{
  "metadata": {
    "timestamp": 1736710415699,
    "page": 46,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjUw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "memcached/memcached",
      "stars": 13650,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.470703125,
          "content": "*.[aois]\n*.gcov\n*.gcda\n*.gcno\n*.tcov\n.cache\n.deps\n.buildbot\nINSTALL\nMakefile\nMakefile.in\nconfig.guess\nconfig.h\nconfig.h.in\nconfig.log\nconfig.status\nconfig.sub\naclocal.m4\ncompile\nautom4te.cache\nconfigure\ndepcomp\ninstall-sh\nmemcached\nmemcached-debug\nmemcached-debug.profile\nmissing\nstamp-h1\ntags\nTAGS\ncscope.out\nmemcached_dtrace.h\n*~\nmemcached-*.tar.gz\nmemcached-*.tar.gz.sha1\n/sizes\n/version.m4\n/version.num\n/testapp\n/timedrun\n/doc/doxy\n/memcached.spec\n.*.swp\n/compile_commands.json\n"
        },
        {
          "name": ".shipit",
          "type": "blob",
          "size": 0.1484375,
          "content": "steps = FindVersion, ChangeVersion, CheckChangeLog, DistTest, Commit, Tag, MakeDist, AddToSVNDir\n\nAddToSVNDir.dir = ../website/dist\nsvn.tagpattern = %v\n"
        },
        {
          "name": "AUTHORS",
          "type": "blob",
          "size": 0.11328125,
          "content": "Anatoly Vorobey <mellon@pobox.com>\nBrad Fitzpatrick <brad@danga.com>\nAlan \"Dormando\" Kasindorf <dormando@rydia.net>\n"
        },
        {
          "name": "BUILD",
          "type": "blob",
          "size": 0.708984375,
          "content": "See below if building the proxy\n\nTo build memcached in your machine from local repo you will have to install\nautotools, automake and libevent. In a debian based system that will look\nlike this\n\nsudo apt-get install autotools-dev\nsudo apt-get install automake\nsudo apt-get install libevent-dev\n\nAfter that you can build memcached binary using automake\n\ncd memcached\n./autogen.sh\n./configure\nmake\nmake test\n\nIt should create the binary in the same folder, which you can run\n\n./memcached\n\nYou can telnet into that memcached to ensure it is up and running\n\ntelnet 127.0.0.1 11211\nstats\n\nIF BUILDING PROXY, AN EXTRA STEP IS NECESSARY:\n\ncd memcached\ncd vendor\n./fetch.sh\ncd ..\n./autogen.sh\n./configure --enable-proxy\nmake\nmake test\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 0.470703125,
          "content": "See: https://github.com/memcached/memcached/wiki/DevelopmentRepos\n\nIt's worth repeating here, that the biggest contribution anyone can make is to\nhelp run new releases! Any feedback we get is greatly appreciated. Hard to\nknow what to work on and what to prioritize if we don't hear from you :)\n\nThe easiest thing to do is to run the latest version on one machine in your\ncluster sometimes. Then when you do need to upgrade, you should also have\nconfidence in a well tested version.\n"
        },
        {
          "name": "COPYING",
          "type": "blob",
          "size": 1.4677734375,
          "content": "Copyright (c) 2003, Danga Interactive, Inc.\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are\nmet:\n\n    * Redistributions of source code must retain the above copyright\nnotice, this list of conditions and the following disclaimer.\n\n    * Redistributions in binary form must reproduce the above\ncopyright notice, this list of conditions and the following disclaimer\nin the documentation and/or other materials provided with the\ndistribution.\n\n    * Neither the name of the Danga Interactive nor the names of its\ncontributors may be used to endorse or promote products derived from\nthis software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n\"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\nLIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\nA PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\nOWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\nSPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\nLIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\nDATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\nTHEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n"
        },
        {
          "name": "ChangeLog",
          "type": "blob",
          "size": 19.2080078125,
          "content": "2016-08-23\n    * ChangeLog moved from Google Code to Github Wiki, it's now available at:\n      https://github.com/memcached/memcached/wiki/ReleaseNotes\n\n2010-10-11\n\t* ChangeLog is no longer being updated.\n\tSee http://code.google.com/p/memcached/wiki/ReleaseNotes\n\n2009-04-10\n\n\t* *critical bugfix*. In 1.2.7 under multithreaded mode, memcached\n\t  would never restart accepting connections after hitting the\n\t  maximum connection limit.\n\n2009-04-02\n\n\t* Fix: Rewrote stat buffer handling (Trond)\n\n2009-03-31\n\n\t* Feature:  -b option for configuring backlog\n\n\t* Feature:  stats settings\n\n\t* Fix: binary stat alignment bug (bug26)\n\n\t* Fix: buffer overflow in stats (bug27)\n\n\t* Fix: recycle memory more aggressively (bug14)\n\n\t* Fix: incr validation (bug31)\n\n\t* Fix: 64-bit incr/decr delta problem (bug21)\n\n\t* Fix: Ascii UDP set (bug36)\n\n\t* Fix: stats slabs' used chunks (bug29)\n\n\t* Fix: stats reset resetting more (bug22)\n\n\t* Misc: More tests, documentation, cleanliness (godliness)\n\n\t* Stable merge (stats, debuggability, listen bugs)\n\n2009-03-11\n\n\t* Protocol:  Binary complete (Dustin, Trond, Toru, etc...)\n\n\t* Performance:  improvements from facebook (Dormando)\n\n\t* Performance:  per-thread stats (Trond)\n\n\t* Performance:  Hash expansion in its own thread (Trond)\n\n\t* Stats:  Lots of new stats (Dustin)\n\n\t* Bug fixes (various)\n\n\tSee the 1.3.2 release notes or revision control for more detail.\n\n2008-09-06\n       * Display error status on listen failures (Dormando)\n\n       * Remove managed instance code. Incomplete/etc. (Dormando)\n\n          * Handle broken IPV6 stacks better (Brian Aker)\n\n          * Generate warnings on setsockopt() failures (Brian Aker)\n\n          * Fix some indentation issues (Brian Aker)\n\n          * UDP/TCP can be disabled by setting their port to zero (Brian Aker)\n\n          * Zero out libevent thread structures before use (Ricky Zhou)\n\n          * New stat: Last accessed time for last evicted item per slab class.\n            (Dormando)\n\n          * Use a dedicated socket accept thread (Facebook)\n\n          * Add -R option. Limit the number of requests processed by a connection\n            at once. Prevents starving other threads if bulk loading. (Facebook)\n\n2008-07-29 [Version 1.2.6 released]\n\n2008-07-24 [Version 1.2.6-rc1 released]\n\n       * Add support for newer automake (Facebook)\n\n       * DTrace support for Solaris/etc (Trond Norbye)\n\n       * LRU tests (Steve Yen)\n\n       * Handle negative length items properly (Dormando)\n\n       * Don't leave stale data after failed set attempts (Dormando)\n\n       * Fix refcount leaks, which would result in OOM's on all sets\n\t       (Dormando)\n\n       * Fix buffer overruns (Dustin Sallings, Tomash Brechko)\n\n       * Fix memory corruption with CAS (Dustin Sallings)\n\n2008-06-11\n\n       * Fix -k to work with -d. (reported by Gary Zhu)\n\n2008-03-02 [Version 1.2.5-rc1 released]\n\n       * Add per-item-class tracking of evictions and OOM errors (dormando)\n\n       * Optimize item_alloc() a little (dormando)\n\n       * Give 'SERVER_ERROR out of memory' errors more context (dormando)\n\n       * Enable usage of large memory pages under solaris\n         (Trond.Norbye@Sun.COM)\n\n       * Enable UDP by default, clean up server socket code\n         (brian@tangent.org)\n\n       * 'noreply' support (Tomash Brechko)\n\n       * IPv6 support, and IPv6 multi-interface support (brian@tangent.org)\n\n       * Add compiler options for Sun Studio compilers with --enable-threads\n\t     (Trond.Norbye@Sun.COM)\n\n       * Add --enable-64bit for mulitarget platforms (Trond.Norbye@Sun.COM)\n\n       * Use gettimeofday(2) instead of time(2).\n\n       * Make -k option work (Tomash Brechko)\n\n\t   * Fix chunk slab alignment (Trond.Norbye@Sun.COM)\n\n2007-12-06 [Version 1.2.4 released]\n\n2007-12-05\n\n       * Fix compilation on panther (JS and Dormando)\n\n\t   * More CAS tests (Chris Goffinet)\n\n\t   * Final fixes for all 1.2.4 features are in, -rc2 sent out.\n\n2007-11-19 [Version 1.2.4-rc1 released]\n\n2007-11-19  Dormando <dormando@rydia.net>\n\n       * Patch series from Tomash Brechko <tomash.brechko@gmail.com>:\n         Minor fixes and optimizations.\n\n       * Patches from Chris, Dustin, and Dormando to fix CAS.\n\n       * Prepping for 1.2.4 release.\n\n2007-11-13  Dormando <dormando@rydia.net>\n\n\t* Adjusted patch from js <ebgssth@gmail.com>: Compile on OS X Panther\n\t  and earlier.\n\n2007-11-12  Steven Grimm  <sgrimm@facebook.com>\n\n\t* Patch from Tomash Brechko <tomash.brechko@gmail.com>: Always send\n\t  \"SERVER_ERROR out of memory\" when memory exhausted.\n\n2007-10-15  Paul Lindner  <lindner@inuus.com>\n\n\t* Patch from David Bremner <bremner@unb.ca> that implements\n\t  a new option \"-a\" which takes an octal permission mask\n\t  (like chmod) sets the permissions on the unix domain socket \n\t  (specified by \"-s\").\n\n2007-10-03 Paul Lindner <lindner@inuus.com>\n\t* Incorporate \"cas\" operation developed by Dustin\n\t  Sallings <dustin@spy.net> This change allows you\n\t  to do atomic changes to an existing key.\n\n\t* Fix for stats.evictions not incrementing\n\t  when exptime == 0 items are kicked off the cache. \n\t  from Jean-Francois BUSTARRET <jfbustarret@wat.tv>.\n\n\t* Fix for do_item_cachedump() which was returning\n\t  an incorrect timestamp.\n\t\n\t* Switch to unsigned 64-bit increment/decrement counters\n\t  from Evan Miller and Dustin Sallings.\n\n\t* Add append command support written by Filipe Laborde.\n\t  Thread safe version plus prepend command from Maxim Dounin\n\t  <mdounin@mdounin.ru>\n\n\t* The memcached-tool script can now display stats.  Patch\n\t  provided by Dan Christian <dchristian@google.com>\n\n\t* Fix for Unix Domain sockets on FreeBSD\n\t  FreeBSD's sendmsg() requires msg_name in msghdr structure \n\t  to be NULL if not used, setting msg_namelen to 0 isn't enough.\n\t  Patch from Maxim Dounin <mdounin@mdounin.ru>\n\n2007-08-21 Paul Lindner <lindner@inuus.com>\n\t* Incorporate increment patch from Evan Miller \n\t  <emiller@imvu.com> to define increment overflow\n\t  behavior.\n\n2007-08-07 Leon Brocard <acme@astray.com>\n\t* Bring the memcached.1 manpage up to date\n\n2007-08-06 Paul Lindner <lindner@inuus.com>\n\t* Fix crash when using -P and -d flags on x86_64\n\t  with latest libevent release.\n\n2007-07-08  Steven Grimm  <sgrimm@facebook.com>\n\n\t* Item stats commands weren't thread-safe; wrap them with locks\n\t  when compiled in multithreaded mode.\n\t* The \"stats items\" command now works again; it broke with the\n\t  introduction of the powers-of-N chunk size change.\n\n2007-07-06 [Version 1.2.3 released]\n\n2007-06-19  Paul Lindner  <lindner@mirth.inuus.com>\n\n\t* Solaris portability fixes from Trond Norbye\n\n2007-05-29  Paul Lindner  <lindner@mirth.inuus.com>\n\n\t* Properly document evictions statistic value\n\n2007-05-10  Paul Lindner  <lindner@inuus.com>\n\n\t* Flesh out tests for unix domain sockets and binary data.\n\t* Update rpm spec file to run tests\n\n2007-05-07  Paul Lindner  <lindner@inuus.com>\n\n\t* Fix compilation bug on freebsd 6.x (and maybe others)\n\t* Update RPM spec file per redhat bugzilla #238994\n\t* Move unistd.h to memcached.h to get rid of warnings\n\t* Add string.h to thread.c to get correctly prototyped strerror()\n\n2007-05-04  Paul Lindner  <lindner@inuus.com>\n\n\t* Add fedora/redhat style init script and RPM spec file\n\n2007-05-12 [Version 1.2.2 released]\n\n2007-04-16  Steven Grimm  <sgrimm@facebook.com>\n\n\t* Command tokenizer performance and cleanliness improvement.\n\t  Patch contributed by Paolo Borelli <paolo.borelli@gmail.com>.\n\n2007-04-16  Paul Lindner  <lindner@inuus.com>\n\n\t* Add notes to README about MacOS, libevent and kqueue.\n\n\t* Windows Patch integration -- part 1, warnings elimination.\n\n2007-04-12  Paul Lindner  <lindner@mirth.inuus.com>\n\n\t* Allow changes to the verbosity level of the server with a new\n\t  \"verbosity\" command and some compiler cleanups. \n          Patch contributed by Paolo Borelli <paolo.borelli@gmail.com>.\n\n2007-04-08  Paul Lindner  <lindner@inuus.com>\n\n\t* Add cleanup patch from \"Tim Yardley\" <liquid@haveheart.com> to\n\t  clean up source spacing issues, fix -Wall warnings, add some\n\t  null checks, adds asserts at the top of each function for any\n\t  use of conn *c without checking to see if c is NULL first.\n\n        * Also adjust clean-whitespace.pl to clean *.ac files.  Add\n          script to test-suite to test for tabs.\n\n2007-04-04  Paul Lindner  <lindner@inuus.com>\n\n\t* Add clarification of flush_all in the protocol docs\n\t  from Elizabeth Mattijsen <liz@dijkmat.nl>\n\n2007-03-31  Paul Lindner  <lindner@inuus.com>\n\n\t* Add patch from Eli Bingham <eli@pandora.com> to \n\t  re-enable the -n switch to memcached.\n\n2007-03-20  Paul Lindner  <lindner@inuus.com>\n\t* Add patch to collect eviction statistics from\n          Jean-Francois BUSTARRET <jfbustarret@wat.tv>.\n\n        * Updated docs, added new test cases for t/stats.t\n\n2007-03-18  Paul Lindner  <lindner@inuus.com>\n\n\t* Add more test cases using larger buffer sizes up to and greater\n\t  than 1MB.\n\n\t* Remove unused parameter to item_size_ok()\n\n\t* Use a single printf() in usage()\n\n\t* Add a failing test for conforming with maximum connections.\n\n2007-03-17\n\t* crash fix from Thomas van Gulick <thomas@partyflock.nl> in\n\t  conn_shrink(), passing &ptr, instead of ptr to realloc().\n\n2007-03-05  Paul Lindner  <lindner@inuus.com>\n\t* Fix a number of places where (s)printf calls were using unsigned\n\t  or signed formats that did not match their arguments.\n\n\t* Add support for stdbool.h and stdint.h to use the bool and\n\t  uint8_t types.\n\n\t* Major refactoring - move API calls for assoc/items/slabs to\n\t  their own individual header files.  Add appropriate const and\n\t  static declarations as appropriate.\n\t\n\t* Avoid type-punning.  Do a more efficient realloc inside the\n\t  conn_shrink routine.\n\n        * Fix overflow bug where uninitialized access to slabclass caused\n\t  size-0 mallocs during slab preallocation.\n\n\t* Use EXIT_SUCCESS/EXIT_FAILURE constants.\n\n\t* Convert some sprintf calls to snprintf to protect against\n\t  buffer overflows.\n\n\t* Explicitly compare against NULL or zero in many places.\n\n2007-03-05\n\t* Steven Grimm <sgrimm@facebook.com>: Per-object-type stats collection\n\t  support. Specify the object type delimiter with the -D command line\n\t  option. Turn stats gathering on and off with \"stats detail on\" and\n\t  \"stats detail off\". Dump the per-object-type details with\n\t  \"stats detail dump\".\n\n2007-03-01\n\t* Steven Grimm <sgrimm@facebook.com>: Fix an off-by-one error in the\n\t  multithreaded version's message passing code.\n\n2006-12-23\n\t* fix expirations of items set with absolute expiration times in\n\t  the past, before the server's start time.  bug was introduced in\n\t  1.2.0 with rel_time_t.  Thanks to Adam Dixon\n\t  <adamtdixon@gmail.com> for the bug report and test case!\n\n2006-11-26\n\t* Steven Grimm <sgrimm@facebook.com>: Performance improvements:\n\t  \n\t  Dynamic sizing of hashtable to reduce collisions on very large\n\t  caches and conserve memory on small caches.\n\n\t  Only reposition items in the LRU queue once a minute, to reduce\n\t  overhead of accessing extremely frequently-used items.\n\n\t  Stop listening for new connections until an existing one closes\n\t  if we run out of available file descriptors.\n\n\t  Command parser refactoring: Add a single-pass tokenizer to cut\n\t  down on string scanning.  Split the command processing into\n\t  separate functions for easier profiling and better readability.\n\t  Pass key lengths along with the keys in all API functions that\n\t  need keys, to avoid needing to call strlen() repeatedly.\n\n2006-11-25\n\t* Steve Peters <steve@fisharerojo.org>: OpenBSD has a malloc.h,\n\tbut warns to use stdlib.h instead\n\n2006-11-22\n\t* Steven Grimm <sgrimm@facebook.com>: Add support for multithreaded\n\t  execution. Run configure with \"--enable-threads\" to enable. See\n\t  doc/threads.txt for details.\n\n2006-11-13\n\t* Iain Wade <iwade@optusnet.com.au>: Fix for UDP responses on non-\"get\"\n\t commands.\n\n2006-10-15\n\t* Steven Grimm <sgrimm@facebook.com>: Dynamic sizing of hashtable to\n\t  reduce collisions on very large caches and conserve memory on\n\t  small caches.\n\n2006-10-13\n\t* Steven Grimm <sgrimm@facebook.com>: New faster hash function.\n\n2006-09-20\n\n\t* don't listen on UDP by default; more clear message when UDP port in use\n\n2006-09-09\n\t* release 1.2.0 (along with 1.1.13, which is the more tested branch)\n\n\tnobody has run 1.2.0 in production, to my knowledge.  facebook has run\n\ttheir pre-merge-with-trunk version, but bugs were discovered (and fixed)\n\tafter the merge.  there might be more.  you've been warned.  :)\n\n2006-09-04\n\t* improved autoconf libevent detection, from the Tor project.\n\n2006-09-03\n\t* test suite and lot of expiration, delete, flush_all, etc corner\n\t  case bugs fixed (Brad Fitzpatrick)\n\n2006-09-02\n\t* Nathan Neulinger <nneul@umr.edu>: fix breakage in expiration code\n\t  causing expiration times to not be processed correctly.\n\n2006-08-21\n\t* Nathan Neulinger <nneul@umr.edu>: fix incompatibilities with\n\t  unix domain socket support and the UDP code and clean up stale \n\t  sockets\n\n2006-08-20\n\t* Nathan Neulinger <nneul@umr.edu>: unix domain socket support\n\n2006-05-03\n\t* Steven Grimm <sgrimm@facebook.com>:  big bunch of changes:\n\t  big CPU reduction work, UDP-based interface, increased memory\n\t  efficiency.  (intertwined patch, committed all together)\n\t  <https://lists.danga.com/pipermail/memcached/2006-May/002164.html>\n\t  or see svn commit logs\n\n2006-04-30\n\t* River Tarnell:  autoconf work for Solaris 10.  Brad:\n\tmerge and verify it works on Nexenta.\n\n2006-03-04\n\t* avva: bucket/generation patch (old, but Brad's just finally\n\tcommitting it)\n\n2006-01-01\n\t* Brad Fitzpatrick <brad@danga.com>:  allocate 1 slab per class\n\ton start-up, to avoid confusing users with out-of-memory errors\n\tlater.  this is 18 MB of allocation on start, unless max memory\n\tallowed with -m is lower, in which case only the smaller slab\n\tclasses are allocated.\n\n2005-08-09\n\t* Elizabeth Mattijsen <liz@dijkmat.nl>: needed a way to flush all\n\tmemcached backend servers, but not at exactly the same time (to\n\treduce load peaks), I've added some simple functionality to the\n\tmemcached protocol in the \"flush_all\" command that allows you to\n\tspecify a time at which the flush will actually occur (instead of\n\talways at the moment the \"flush_all\" command is received).\n\n2005-05-25\n\t* patch from Peter van Dijk <peter@nextgear.nl> to make\n\t  stderr unbuffered, for running under daemontools\n\n2005-04-04\n\t* patch from Don MacAskill <don@smugmug.com> 'flush_all' doesn't\n\tseem to work properly.  Basically, if you try to add a key which\n\tis present, but expired, the store fails but the old key is no\n\tlonger expired.\n\n\t* release 1.1.12\n\n2005-01-14\n\t* Date: Thu, 18 Nov 2004 15:25:59 -0600\n\t  From: David Phillips <electrum@gmail.com>\n\tHere is a patch to configure.ac and Makefile.am to put the man page in\n\tthe correct location.  Trying to install the man page from a\n\tsubdirectory results in the subdirectory being used in the install\n\tpath (it tries to install to doc/memcached.1).  This is the correct\n\tthing to  do:\n\n\t- create a Makefile.am in the doc directory that installs the man page\n\t  with man_MANS\n\t- modify Makefile.am in the base directory to reference the doc\n  \t  directory using SUBDIRS\n\t- modify the AC_CONFIG_FILES macro in configure.ac to output the \n\t  Makefile in doc\n\n\t\n2005-01-14\n\t* pidfile saving support from Lisa Seelye <lisa@gentoo.org>, sent\n\t  Jan 13, 2005\n\n2005-01-14\n\t* don't delete libevent events that haven't been added (the deltimer)\n\t  patch from Ted Schundler <tschundler@gmail.com>\n\n2004-12-10\n\t* document -M and -r in manpage (Doug Porter <dsp@dsp.name>)\n\n2004-07-22\n\t* fix buffer overflow in items.c with 250 byte keys along with\n\t  other info on the same line going into a 256 byte char[].\n\t  thanks to Andrei Nigmatulin <anight@monamour.ru>\n\t\n2004-06-15\n\t* immediate deletes weren't being unlinked a few seconds,\n\t  preventing \"add\" commands to the same key in that time period.\n\t  thanks to Michael Alan Dorman <mdorman@debian.org> for the\n\t  bug report and demo script.\n\t\n2004-04-30\n\t* released 1.1.11\n\n2004-04-24\n\t* Avva: Add a new command line option: -r , to maximize core file\n\tlimit.\n\n2004-03-31\n\t* Avva: Use getrlimit and setrlimit to set limits for number of\n\tsimultaneously open file descriptors. Get the current limits and\n\ttry to raise them if they're not enough for the specified (or the\n\tdefault) setting of max connections.\n\t\n2004-02-24\n\t* Adds a '-M' flag to turn off tossing items from the cache.\n\t  (Jason Titus <jtitus@postini.com>)\n\n2004-02-19 (Evan)\n\t* Install manpage on \"make install\", etc.\n\n2003-12-30 (Brad)\n\t* remove static build stuff.  interferes with PAM setuid stuff\n\t  and was only included as a possible fix with the old memory\n\t  allocator.  really shouldn't make a difference.\n\t* add Jay Bonci's Debian scripts and manpage\n\t* release version 1.1.10\n\n2003-12-01 (Avva)\n\t* New command: flush_all, causes all existing items to\n\t  be invalidated immediately (without deleting them from\n\t  memory, merely causing memcached to no longer return them).\n2003-10-23\n\t* Shift init code around to fix daemon mode on FreeBSD,\n\t* and drop root only after creating the server socket (to\n\t* allow the use of privileged ports)\n\t* version 1.1.10pre\n\n2003-10-09\n\t* BSD compile fixes from Ryan T. Dean\n\t* version 1.1.9\n\t\n2003-09-29\n\t* ignore SIGPIPE at start instead of crashing in rare cases it\n\t  comes up.  no other code had to be modified, since everything\n\t  else is already dead-connection-aware.  (avva)\n\t\n2003-09-09 (Avva, Lisa Marie Seelye <lisa@gentoo.org>)\n\t* setuid support\n\t\n2003-09-05 (Avva)\n\t* accept all new connections in the same event (so we work with ET epoll)\n\t* mark all items as clsid=0 after slab page reassignment to please future\n\t  asserts (on the road to making slab page reassignment work fully)\n\n2003-08-12 (Brad Fitzpatrick)\n\t* use TCP_CORK on Linux or TCP_PUSH on BSD\n\t* only use TCP_NODELAY when we don't have alternatives\n\t\n2003-08-10\n\t* disable Nagel's Algorithm (TCP_NODELAY) for better performance (avva)\n\n2003-08-10\n\t* support multiple levels of verbosity (-vv)\n\n2003-08-10  (Evan Martin)\n\t* Makefile.am: debug, optimization, and static flags are controlled\n\t  by the configure script.\n\t* configure.ac:\n\t  - allow specifying libevent directory with --with-libevent=DIR\n\t  - check for malloc.h (unavailable on BSDs)\n\t  - check for socklen_t (unavailable on OSX)\n\t* assoc.c, items.c, slabs.c:  Remove some unused headers.\n\t* memcached.c:  allow for nonexistence of malloc.h; #define a POSIX\n\t  macro to import mlockall flags.\n\n2003-07-29\n\t* version 1.1.7\n\t* big bug fix: item exptime 0 meant expire immediately, not never\n\t* version 1.1.8\n\n2003-07-22\n\t* make 'delete' take second arg, of time to refuse new add/replace\n\t* set/add/replace/delete can all take abs or delta time (delta can't\n\t  be larger than a month)\n\n2003-07-21\n\t* added doc/protocol.txt\n\n2003-07-01\n\t* report CPU usage in stats\n\t \n2003-06-30\n\t* version 1.1.6\n\t* fix a number of obscure bugs\n\t* more stats reporting\n\t\n2003-06-10\n\t* removing use of Judy; use a hash.  (judy caused memory fragmentation)\n\t* shrink some structures\n\t* security improvements\n\t* version 1.1.0\n\t\n2003-06-18\n\t* changing maxsize back to an unsigned int\n\t\n2003-06-16\n\t* adding PHP support\n\t* added CONTRIBUTORS file\n\t* version 1.0.4\n\t\n2003-06-15\n\t* forgot to distribute website/api (still learning auto*)\n\t* version 1.0.3\n\t\n2003-06-15\n\t* update to version 1.0.2\n\t* autoconf/automake fixes for older versions\n\t* make stats report version number\n\t* change license from GPL to BSD\n\t\nFri, 13 Jun 2003 10:05:51 -0700  Evan Martin  <martine@danga.com>\n\n\t* configure.ac, autogen.sh, Makefile.am:  Use autotools.\n\t* items.c, memcached.c:  #include <time.h> for time(),\n\t  printf time_t as %lu (is this correct?),\n\t  minor warnings fixes.\n\n"
        },
        {
          "name": "Doxyfile",
          "type": "blob",
          "size": 61.3037109375,
          "content": "# Doxyfile 1.5.8\n\n# This file describes the settings to be used by the documentation system\n# doxygen (www.doxygen.org) for a project\n#\n# All text after a hash (#) is considered a comment and will be ignored\n# The format is:\n#       TAG = value [value, ...]\n# For lists items can also be appended using:\n#       TAG += value [value, ...]\n# Values that contain spaces should be placed between quotes (\" \")\n\n#---------------------------------------------------------------------------\n# Project related configuration options\n#---------------------------------------------------------------------------\n\n# This tag specifies the encoding used for all characters in the config file\n# that follow. The default is UTF-8 which is also the encoding used for all\n# text before the first occurrence of this tag. Doxygen uses libiconv (or the\n# iconv built into libc) for the transcoding. See\n# http://www.gnu.org/software/libiconv for the list of possible encodings.\n\nDOXYFILE_ENCODING      = UTF-8\n\n# The PROJECT_NAME tag is a single word (or a sequence of words surrounded\n# by quotes) that should identify the project.\n\nPROJECT_NAME           = memcached\n\n# The PROJECT_NUMBER tag can be used to enter a project or revision number.\n# This could be handy for archiving the generated documentation or\n# if some version control system is used.\n\nPROJECT_NUMBER         =\n\n# The OUTPUT_DIRECTORY tag is used to specify the (relative or absolute)\n# base path where the generated documentation will be put.\n# If a relative path is entered, it will be relative to the location\n# where doxygen was started. If left blank the current directory will be used.\n\nOUTPUT_DIRECTORY       = doc/doxy\n\n# If the CREATE_SUBDIRS tag is set to YES, then doxygen will create\n# 4096 sub-directories (in 2 levels) under the output directory of each output\n# format and will distribute the generated files over these directories.\n# Enabling this option can be useful when feeding doxygen a huge amount of\n# source files, where putting all generated files in the same directory would\n# otherwise cause performance problems for the file system.\n\nCREATE_SUBDIRS         = NO\n\n# The OUTPUT_LANGUAGE tag is used to specify the language in which all\n# documentation generated by doxygen is written. Doxygen will use this\n# information to generate all constant output in the proper language.\n# The default language is English, other supported languages are:\n# Afrikaans, Arabic, Brazilian, Catalan, Chinese, Chinese-Traditional,\n# Croatian, Czech, Danish, Dutch, Farsi, Finnish, French, German, Greek,\n# Hungarian, Italian, Japanese, Japanese-en (Japanese with English messages),\n# Korean, Korean-en, Lithuanian, Norwegian, Macedonian, Persian, Polish,\n# Portuguese, Romanian, Russian, Serbian, Serbian-Cyrillic, Slovak, Slovene,\n# Spanish, Swedish, and Ukrainian.\n\nOUTPUT_LANGUAGE        = English\n\n# If the BRIEF_MEMBER_DESC tag is set to YES (the default) Doxygen will\n# include brief member descriptions after the members that are listed in\n# the file and class documentation (similar to JavaDoc).\n# Set to NO to disable this.\n\nBRIEF_MEMBER_DESC      = YES\n\n# If the REPEAT_BRIEF tag is set to YES (the default) Doxygen will prepend\n# the brief description of a member or function before the detailed description.\n# Note: if both HIDE_UNDOC_MEMBERS and BRIEF_MEMBER_DESC are set to NO, the\n# brief descriptions will be completely suppressed.\n\nREPEAT_BRIEF           = YES\n\n# This tag implements a quasi-intelligent brief description abbreviator\n# that is used to form the text in various listings. Each string\n# in this list, if found as the leading text of the brief description, will be\n# stripped from the text and the result after processing the whole list, is\n# used as the annotated text. Otherwise, the brief description is used as-is.\n# If left blank, the following values are used (\"$name\" is automatically\n# replaced with the name of the entity): \"The $name class\" \"The $name widget\"\n# \"The $name file\" \"is\" \"provides\" \"specifies\" \"contains\"\n# \"represents\" \"a\" \"an\" \"the\"\n\nABBREVIATE_BRIEF       =\n\n# If the ALWAYS_DETAILED_SEC and REPEAT_BRIEF tags are both set to YES then\n# Doxygen will generate a detailed section even if there is only a brief\n# description.\n\nALWAYS_DETAILED_SEC    = NO\n\n# If the INLINE_INHERITED_MEMB tag is set to YES, doxygen will show all\n# inherited members of a class in the documentation of that class as if those\n# members were ordinary class members. Constructors, destructors and assignment\n# operators of the base classes will not be shown.\n\nINLINE_INHERITED_MEMB  = NO\n\n# If the FULL_PATH_NAMES tag is set to YES then Doxygen will prepend the full\n# path before files name in the file list and in the header files. If set\n# to NO the shortest path that makes the file name unique will be used.\n\nFULL_PATH_NAMES        = YES\n\n# If the FULL_PATH_NAMES tag is set to YES then the STRIP_FROM_PATH tag\n# can be used to strip a user-defined part of the path. Stripping is\n# only done if one of the specified strings matches the left-hand part of\n# the path. The tag can be used to show relative paths in the file list.\n# If left blank the directory from which doxygen is run is used as the\n# path to strip.\n\nSTRIP_FROM_PATH        =\n\n# The STRIP_FROM_INC_PATH tag can be used to strip a user-defined part of\n# the path mentioned in the documentation of a class, which tells\n# the reader which header file to include in order to use a class.\n# If left blank only the name of the header file containing the class\n# definition is used. Otherwise one should specify the include paths that\n# are normally passed to the compiler using the -I flag.\n\nSTRIP_FROM_INC_PATH    =\n\n# If the SHORT_NAMES tag is set to YES, doxygen will generate much shorter\n# (but less readable) file names. This can be useful is your file systems\n# doesn't support long names like on DOS, Mac, or CD-ROM.\n\nSHORT_NAMES            = NO\n\n# If the JAVADOC_AUTOBRIEF tag is set to YES then Doxygen\n# will interpret the first line (until the first dot) of a JavaDoc-style\n# comment as the brief description. If set to NO, the JavaDoc\n# comments will behave just like regular Qt-style comments\n# (thus requiring an explicit @brief command for a brief description.)\n\nJAVADOC_AUTOBRIEF      = YES\n\n# If the QT_AUTOBRIEF tag is set to YES then Doxygen will\n# interpret the first line (until the first dot) of a Qt-style\n# comment as the brief description. If set to NO, the comments\n# will behave just like regular Qt-style comments (thus requiring\n# an explicit \\brief command for a brief description.)\n\nQT_AUTOBRIEF           = NO\n\n# The MULTILINE_CPP_IS_BRIEF tag can be set to YES to make Doxygen\n# treat a multi-line C++ special comment block (i.e. a block of //! or ///\n# comments) as a brief description. This used to be the default behaviour.\n# The new default is to treat a multi-line C++ comment block as a detailed\n# description. Set this tag to YES if you prefer the old behaviour instead.\n\nMULTILINE_CPP_IS_BRIEF = NO\n\n# If the INHERIT_DOCS tag is set to YES (the default) then an undocumented\n# member inherits the documentation from any documented member that it\n# re-implements.\n\nINHERIT_DOCS           = YES\n\n# If the SEPARATE_MEMBER_PAGES tag is set to YES, then doxygen will produce\n# a new page for each member. If set to NO, the documentation of a member will\n# be part of the file/class/namespace that contains it.\n\nSEPARATE_MEMBER_PAGES  = NO\n\n# The TAB_SIZE tag can be used to set the number of spaces in a tab.\n# Doxygen uses this value to replace tabs by spaces in code fragments.\n\nTAB_SIZE               = 8\n\n# This tag can be used to specify a number of aliases that acts\n# as commands in the documentation. An alias has the form \"name=value\".\n# For example adding \"sideeffect=\\par Side Effects:\\n\" will allow you to\n# put the command \\sideeffect (or @sideeffect) in the documentation, which\n# will result in a user-defined paragraph with heading \"Side Effects:\".\n# You can put \\n's in the value part of an alias to insert newlines.\n\nALIASES                =\n\n# Set the OPTIMIZE_OUTPUT_FOR_C tag to YES if your project consists of C\n# sources only. Doxygen will then generate output that is more tailored for C.\n# For instance, some of the names that are used will be different. The list\n# of all members will be omitted, etc.\n\nOPTIMIZE_OUTPUT_FOR_C  = NO\n\n# Set the OPTIMIZE_OUTPUT_JAVA tag to YES if your project consists of Java\n# sources only. Doxygen will then generate output that is more tailored for\n# Java. For instance, namespaces will be presented as packages, qualified\n# scopes will look different, etc.\n\nOPTIMIZE_OUTPUT_JAVA   = NO\n\n# Set the OPTIMIZE_FOR_FORTRAN tag to YES if your project consists of Fortran\n# sources only. Doxygen will then generate output that is more tailored for\n# Fortran.\n\nOPTIMIZE_FOR_FORTRAN   = NO\n\n# Set the OPTIMIZE_OUTPUT_VHDL tag to YES if your project consists of VHDL\n# sources. Doxygen will then generate output that is tailored for\n# VHDL.\n\nOPTIMIZE_OUTPUT_VHDL   = NO\n\n# Doxygen selects the parser to use depending on the extension of the files it parses.\n# With this tag you can assign which parser to use for a given extension.\n# Doxygen has a built-in mapping, but you can override or extend it using this tag.\n# The format is ext=language, where ext is a file extension, and language is one of\n# the parsers supported by doxygen: IDL, Java, JavaScript, C#, C, C++, D, PHP,\n# Objective-C, Python, Fortran, VHDL, C, C++. For instance to make doxygen treat\n# .inc files as Fortran files (default is PHP), and .f files as C (default is Fortran),\n# use: inc=Fortran f=C\n\nEXTENSION_MAPPING      =\n\n# If you use STL classes (i.e. std::string, std::vector, etc.) but do not want\n# to include (a tag file for) the STL sources as input, then you should\n# set this tag to YES in order to let doxygen match functions declarations and\n# definitions whose arguments contain STL classes (e.g. func(std::string); v.s.\n# func(std::string) {}). This also make the inheritance and collaboration\n# diagrams that involve STL classes more complete and accurate.\n\nBUILTIN_STL_SUPPORT    = NO\n\n# If you use Microsoft's C++/CLI language, you should set this option to YES to\n# enable parsing support.\n\nCPP_CLI_SUPPORT        = NO\n\n# Set the SIP_SUPPORT tag to YES if your project consists of sip sources only.\n# Doxygen will parse them like normal C++ but will assume all classes use public\n# instead of private inheritance when no explicit protection keyword is present.\n\nSIP_SUPPORT            = NO\n\n# For Microsoft's IDL there are propget and propput attributes to indicate getter\n# and setter methods for a property. Setting this option to YES (the default)\n# will make doxygen to replace the get and set methods by a property in the\n# documentation. This will only work if the methods are indeed getting or\n# setting a simple type. If this is not the case, or you want to show the\n# methods anyway, you should set this option to NO.\n\nIDL_PROPERTY_SUPPORT   = YES\n\n# If member grouping is used in the documentation and the DISTRIBUTE_GROUP_DOC\n# tag is set to YES, then doxygen will reuse the documentation of the first\n# member in the group (if any) for the other members of the group. By default\n# all members of a group must be documented explicitly.\n\nDISTRIBUTE_GROUP_DOC   = NO\n\n# Set the SUBGROUPING tag to YES (the default) to allow class member groups of\n# the same type (for instance a group of public functions) to be put as a\n# subgroup of that type (e.g. under the Public Functions section). Set it to\n# NO to prevent subgrouping. Alternatively, this can be done per class using\n# the \\nosubgrouping command.\n\nSUBGROUPING            = YES\n\n# When TYPEDEF_HIDES_STRUCT is enabled, a typedef of a struct, union, or enum\n# is documented as struct, union, or enum with the name of the typedef. So\n# typedef struct TypeS {} TypeT, will appear in the documentation as a struct\n# with name TypeT. When disabled the typedef will appear as a member of a file,\n# namespace, or class. And the struct will be named TypeS. This can typically\n# be useful for C code in case the coding convention dictates that all compound\n# types are typedef'ed and only the typedef is referenced, never the tag name.\n\nTYPEDEF_HIDES_STRUCT   = NO\n\n# The SYMBOL_CACHE_SIZE determines the size of the internal cache use to\n# determine which symbols to keep in memory and which to flush to disk.\n# When the cache is full, less often used symbols will be written to disk.\n# For small to medium size projects (<1000 input files) the default value is\n# probably good enough. For larger projects a too small cache size can cause\n# doxygen to be busy swapping symbols to and from disk most of the time\n# causing a significant performance penality.\n# If the system has enough physical memory increasing the cache will improve the\n# performance by keeping more symbols in memory. Note that the value works on\n# a logarithmic scale so increasing the size by one will roughly double the\n# memory usage. The cache size is given by this formula:\n# 2^(16+SYMBOL_CACHE_SIZE). The valid range is 0..9, the default is 0,\n# corresponding to a cache size of 2^16 = 65536 symbols\n\nSYMBOL_CACHE_SIZE      = 0\n\n#---------------------------------------------------------------------------\n# Build related configuration options\n#---------------------------------------------------------------------------\n\n# If the EXTRACT_ALL tag is set to YES doxygen will assume all entities in\n# documentation are documented, even if no documentation was available.\n# Private class members and static file members will be hidden unless\n# the EXTRACT_PRIVATE and EXTRACT_STATIC tags are set to YES\n\nEXTRACT_ALL            = YES\n\n# If the EXTRACT_PRIVATE tag is set to YES all private members of a class\n# will be included in the documentation.\n\nEXTRACT_PRIVATE        = NO\n\n# If the EXTRACT_STATIC tag is set to YES all static members of a file\n# will be included in the documentation.\n\nEXTRACT_STATIC         = NO\n\n# If the EXTRACT_LOCAL_CLASSES tag is set to YES classes (and structs)\n# defined locally in source files will be included in the documentation.\n# If set to NO only classes defined in header files are included.\n\nEXTRACT_LOCAL_CLASSES  = YES\n\n# This flag is only useful for Objective-C code. When set to YES local\n# methods, which are defined in the implementation section but not in\n# the interface are included in the documentation.\n# If set to NO (the default) only methods in the interface are included.\n\nEXTRACT_LOCAL_METHODS  = NO\n\n# If this flag is set to YES, the members of anonymous namespaces will be\n# extracted and appear in the documentation as a namespace called\n# 'anonymous_namespace{file}', where file will be replaced with the base\n# name of the file that contains the anonymous namespace. By default\n# anonymous namespace are hidden.\n\nEXTRACT_ANON_NSPACES   = NO\n\n# If the HIDE_UNDOC_MEMBERS tag is set to YES, Doxygen will hide all\n# undocumented members of documented classes, files or namespaces.\n# If set to NO (the default) these members will be included in the\n# various overviews, but no documentation section is generated.\n# This option has no effect if EXTRACT_ALL is enabled.\n\nHIDE_UNDOC_MEMBERS     = NO\n\n# If the HIDE_UNDOC_CLASSES tag is set to YES, Doxygen will hide all\n# undocumented classes that are normally visible in the class hierarchy.\n# If set to NO (the default) these classes will be included in the various\n# overviews. This option has no effect if EXTRACT_ALL is enabled.\n\nHIDE_UNDOC_CLASSES     = NO\n\n# If the HIDE_FRIEND_COMPOUNDS tag is set to YES, Doxygen will hide all\n# friend (class|struct|union) declarations.\n# If set to NO (the default) these declarations will be included in the\n# documentation.\n\nHIDE_FRIEND_COMPOUNDS  = NO\n\n# If the HIDE_IN_BODY_DOCS tag is set to YES, Doxygen will hide any\n# documentation blocks found inside the body of a function.\n# If set to NO (the default) these blocks will be appended to the\n# function's detailed documentation block.\n\nHIDE_IN_BODY_DOCS      = NO\n\n# The INTERNAL_DOCS tag determines if documentation\n# that is typed after a \\internal command is included. If the tag is set\n# to NO (the default) then the documentation will be excluded.\n# Set it to YES to include the internal documentation.\n\nINTERNAL_DOCS          = NO\n\n# If the CASE_SENSE_NAMES tag is set to NO then Doxygen will only generate\n# file names in lower-case letters. If set to YES upper-case letters are also\n# allowed. This is useful if you have classes or files whose names only differ\n# in case and if your file system supports case sensitive file names. Windows\n# and Mac users are advised to set this option to NO.\n\nCASE_SENSE_NAMES       = NO\n\n# If the HIDE_SCOPE_NAMES tag is set to NO (the default) then Doxygen\n# will show members with their full class and namespace scopes in the\n# documentation. If set to YES the scope will be hidden.\n\nHIDE_SCOPE_NAMES       = NO\n\n# If the SHOW_INCLUDE_FILES tag is set to YES (the default) then Doxygen\n# will put a list of the files that are included by a file in the documentation\n# of that file.\n\nSHOW_INCLUDE_FILES     = YES\n\n# If the INLINE_INFO tag is set to YES (the default) then a tag [inline]\n# is inserted in the documentation for inline members.\n\nINLINE_INFO            = YES\n\n# If the SORT_MEMBER_DOCS tag is set to YES (the default) then doxygen\n# will sort the (detailed) documentation of file and class members\n# alphabetically by member name. If set to NO the members will appear in\n# declaration order.\n\nSORT_MEMBER_DOCS       = YES\n\n# If the SORT_BRIEF_DOCS tag is set to YES then doxygen will sort the\n# brief documentation of file, namespace and class members alphabetically\n# by member name. If set to NO (the default) the members will appear in\n# declaration order.\n\nSORT_BRIEF_DOCS        = NO\n\n# If the SORT_GROUP_NAMES tag is set to YES then doxygen will sort the\n# hierarchy of group names into alphabetical order. If set to NO (the default)\n# the group names will appear in their defined order.\n\nSORT_GROUP_NAMES       = NO\n\n# If the SORT_BY_SCOPE_NAME tag is set to YES, the class list will be\n# sorted by fully-qualified names, including namespaces. If set to\n# NO (the default), the class list will be sorted only by class name,\n# not including the namespace part.\n# Note: This option is not very useful if HIDE_SCOPE_NAMES is set to YES.\n# Note: This option applies only to the class list, not to the\n# alphabetical list.\n\nSORT_BY_SCOPE_NAME     = NO\n\n# The GENERATE_TODOLIST tag can be used to enable (YES) or\n# disable (NO) the todo list. This list is created by putting \\todo\n# commands in the documentation.\n\nGENERATE_TODOLIST      = YES\n\n# The GENERATE_TESTLIST tag can be used to enable (YES) or\n# disable (NO) the test list. This list is created by putting \\test\n# commands in the documentation.\n\nGENERATE_TESTLIST      = YES\n\n# The GENERATE_BUGLIST tag can be used to enable (YES) or\n# disable (NO) the bug list. This list is created by putting \\bug\n# commands in the documentation.\n\nGENERATE_BUGLIST       = YES\n\n# The GENERATE_DEPRECATEDLIST tag can be used to enable (YES) or\n# disable (NO) the deprecated list. This list is created by putting\n# \\deprecated commands in the documentation.\n\nGENERATE_DEPRECATEDLIST= YES\n\n# The ENABLED_SECTIONS tag can be used to enable conditional\n# documentation sections, marked by \\if sectionname ... \\endif.\n\nENABLED_SECTIONS       =\n\n# The MAX_INITIALIZER_LINES tag determines the maximum number of lines\n# the initial value of a variable or define consists of for it to appear in\n# the documentation. If the initializer consists of more lines than specified\n# here it will be hidden. Use a value of 0 to hide initializers completely.\n# The appearance of the initializer of individual variables and defines in the\n# documentation can be controlled using \\showinitializer or \\hideinitializer\n# command in the documentation regardless of this setting.\n\nMAX_INITIALIZER_LINES  = 30\n\n# Set the SHOW_USED_FILES tag to NO to disable the list of files generated\n# at the bottom of the documentation of classes and structs. If set to YES the\n# list will mention the files that were used to generate the documentation.\n\nSHOW_USED_FILES        = YES\n\n# If the sources in your project are distributed over multiple directories\n# then setting the SHOW_DIRECTORIES tag to YES will show the directory hierarchy\n# in the documentation. The default is NO.\n\nSHOW_DIRECTORIES       = NO\n\n# Set the SHOW_FILES tag to NO to disable the generation of the Files page.\n# This will remove the Files entry from the Quick Index and from the\n# Folder Tree View (if specified). The default is YES.\n\nSHOW_FILES             = YES\n\n# Set the SHOW_NAMESPACES tag to NO to disable the generation of the\n# Namespaces page.\n# This will remove the Namespaces entry from the Quick Index\n# and from the Folder Tree View (if specified). The default is YES.\n\nSHOW_NAMESPACES        = YES\n\n# The FILE_VERSION_FILTER tag can be used to specify a program or script that\n# doxygen should invoke to get the current version for each file (typically from\n# the version control system). Doxygen will invoke the program by executing (via\n# popen()) the command <command> <input-file>, where <command> is the value of\n# the FILE_VERSION_FILTER tag, and <input-file> is the name of an input file\n# provided by doxygen. Whatever the program writes to standard output\n# is used as the file version. See the manual for examples.\n\nFILE_VERSION_FILTER    =\n\n# The LAYOUT_FILE tag can be used to specify a layout file which will be parsed by\n# doxygen. The layout file controls the global structure of the generated output files\n# in an output format independent way. The create the layout file that represents\n# doxygen's defaults, run doxygen with the -l option. You can optionally specify a\n# file name after the option, if omitted DoxygenLayout.xml will be used as the name\n# of the layout file.\n\nLAYOUT_FILE            =\n\n#---------------------------------------------------------------------------\n# configuration options related to warning and progress messages\n#---------------------------------------------------------------------------\n\n# The QUIET tag can be used to turn on/off the messages that are generated\n# by doxygen. Possible values are YES and NO. If left blank NO is used.\n\nQUIET                  = NO\n\n# The WARNINGS tag can be used to turn on/off the warning messages that are\n# generated by doxygen. Possible values are YES and NO. If left blank\n# NO is used.\n\nWARNINGS               = YES\n\n# If WARN_IF_UNDOCUMENTED is set to YES, then doxygen will generate warnings\n# for undocumented members. If EXTRACT_ALL is set to YES then this flag will\n# automatically be disabled.\n\nWARN_IF_UNDOCUMENTED   = YES\n\n# If WARN_IF_DOC_ERROR is set to YES, doxygen will generate warnings for\n# potential errors in the documentation, such as not documenting some\n# parameters in a documented function, or documenting parameters that\n# don't exist or using markup commands wrongly.\n\nWARN_IF_DOC_ERROR      = YES\n\n# This WARN_NO_PARAMDOC option can be enabled to get warnings for\n# functions that are documented, but have no documentation for their parameters\n# or return value. If set to NO (the default) doxygen will only warn about\n# wrong or incomplete parameter documentation, but not about the absence of\n# documentation.\n\nWARN_NO_PARAMDOC       = NO\n\n# The WARN_FORMAT tag determines the format of the warning messages that\n# doxygen can produce. The string should contain the $file, $line, and $text\n# tags, which will be replaced by the file and line number from which the\n# warning originated and the warning text. Optionally the format may contain\n# $version, which will be replaced by the version of the file (if it could\n# be obtained via FILE_VERSION_FILTER)\n\nWARN_FORMAT            = \"$file:$line: $text\"\n\n# The WARN_LOGFILE tag can be used to specify a file to which warning\n# and error messages should be written. If left blank the output is written\n# to stderr.\n\nWARN_LOGFILE           =\n\n#---------------------------------------------------------------------------\n# configuration options related to the input files\n#---------------------------------------------------------------------------\n\n# The INPUT tag can be used to specify the files and/or directories that contain\n# documented source files. You may enter file names like \"myfile.cpp\" or\n# directories like \"/usr/src/myproject\". Separate the files or directories\n# with spaces.\n\nINPUT                  = .\n\n# This tag can be used to specify the character encoding of the source files\n# that doxygen parses. Internally doxygen uses the UTF-8 encoding, which is\n# also the default input encoding. Doxygen uses libiconv (or the iconv built\n# into libc) for the transcoding. See http://www.gnu.org/software/libiconv for\n# the list of possible encodings.\n\nINPUT_ENCODING         = UTF-8\n\n# If the value of the INPUT tag contains directories, you can use the\n# FILE_PATTERNS tag to specify one or more wildcard pattern (like *.cpp\n# and *.h) to filter out the source-files in the directories. If left\n# blank the following patterns are tested:\n# *.c *.cc *.cxx *.cpp *.c++ *.java *.ii *.ixx *.ipp *.i++ *.inl *.h *.hh *.hxx\n# *.hpp *.h++ *.idl *.odl *.cs *.php *.php3 *.inc *.m *.mm *.py *.f90\n\nFILE_PATTERNS          = *.h *.c\n\n# The RECURSIVE tag can be used to turn specify whether or not subdirectories\n# should be searched for input files as well. Possible values are YES and NO.\n# If left blank NO is used.\n\nRECURSIVE              = NO\n\n# The EXCLUDE tag can be used to specify files and/or directories that should\n# excluded from the INPUT source files. This way you can easily exclude a\n# subdirectory from a directory tree whose root is specified with the INPUT tag.\n\nEXCLUDE                =\n\n# The EXCLUDE_SYMLINKS tag can be used select whether or not files or\n# directories that are symbolic links (a Unix filesystem feature) are excluded\n# from the input.\n\nEXCLUDE_SYMLINKS       = NO\n\n# If the value of the INPUT tag contains directories, you can use the\n# EXCLUDE_PATTERNS tag to specify one or more wildcard patterns to exclude\n# certain files from those directories. Note that the wildcards are matched\n# against the file with absolute path, so to exclude all test directories\n# for example use the pattern */test/*\n\nEXCLUDE_PATTERNS       = testapp.c\n\n# The EXCLUDE_SYMBOLS tag can be used to specify one or more symbol names\n# (namespaces, classes, functions, etc.) that should be excluded from the\n# output. The symbol name can be a fully qualified name, a word, or if the\n# wildcard * is used, a substring. Examples: ANamespace, AClass,\n# AClass::ANamespace, ANamespace::*Test\n\nEXCLUDE_SYMBOLS        =\n\n# The EXAMPLE_PATH tag can be used to specify one or more files or\n# directories that contain example code fragments that are included (see\n# the \\include command).\n\nEXAMPLE_PATH           =\n\n# If the value of the EXAMPLE_PATH tag contains directories, you can use the\n# EXAMPLE_PATTERNS tag to specify one or more wildcard pattern (like *.cpp\n# and *.h) to filter out the source-files in the directories. If left\n# blank all files are included.\n\nEXAMPLE_PATTERNS       =\n\n# If the EXAMPLE_RECURSIVE tag is set to YES then subdirectories will be\n# searched for input files to be used with the \\include or \\dontinclude\n# commands irrespective of the value of the RECURSIVE tag.\n# Possible values are YES and NO. If left blank NO is used.\n\nEXAMPLE_RECURSIVE      = NO\n\n# The IMAGE_PATH tag can be used to specify one or more files or\n# directories that contain image that are included in the documentation (see\n# the \\image command).\n\nIMAGE_PATH             =\n\n# The INPUT_FILTER tag can be used to specify a program that doxygen should\n# invoke to filter for each input file. Doxygen will invoke the filter program\n# by executing (via popen()) the command <filter> <input-file>, where <filter>\n# is the value of the INPUT_FILTER tag, and <input-file> is the name of an\n# input file. Doxygen will then use the output that the filter program writes\n# to standard output.\n# If FILTER_PATTERNS is specified, this tag will be\n# ignored.\n\nINPUT_FILTER           =\n\n# The FILTER_PATTERNS tag can be used to specify filters on a per file pattern\n# basis.\n# Doxygen will compare the file name with each pattern and apply the\n# filter if there is a match.\n# The filters are a list of the form:\n# pattern=filter (like *.cpp=my_cpp_filter). See INPUT_FILTER for further\n# info on how filters are used. If FILTER_PATTERNS is empty, INPUT_FILTER\n# is applied to all files.\n\nFILTER_PATTERNS        =\n\n# If the FILTER_SOURCE_FILES tag is set to YES, the input filter (if set using\n# INPUT_FILTER) will be used to filter the input files when producing source\n# files to browse (i.e. when SOURCE_BROWSER is set to YES).\n\nFILTER_SOURCE_FILES    = NO\n\n#---------------------------------------------------------------------------\n# configuration options related to source browsing\n#---------------------------------------------------------------------------\n\n# If the SOURCE_BROWSER tag is set to YES then a list of source files will\n# be generated. Documented entities will be cross-referenced with these sources.\n# Note: To get rid of all source code in the generated output, make sure also\n# VERBATIM_HEADERS is set to NO.\n\nSOURCE_BROWSER         = NO\n\n# Setting the INLINE_SOURCES tag to YES will include the body\n# of functions and classes directly in the documentation.\n\nINLINE_SOURCES         = NO\n\n# Setting the STRIP_CODE_COMMENTS tag to YES (the default) will instruct\n# doxygen to hide any special comment blocks from generated source code\n# fragments. Normal C and C++ comments will always remain visible.\n\nSTRIP_CODE_COMMENTS    = YES\n\n# If the REFERENCED_BY_RELATION tag is set to YES\n# then for each documented function all documented\n# functions referencing it will be listed.\n\nREFERENCED_BY_RELATION = NO\n\n# If the REFERENCES_RELATION tag is set to YES\n# then for each documented function all documented entities\n# called/used by that function will be listed.\n\nREFERENCES_RELATION    = NO\n\n# If the REFERENCES_LINK_SOURCE tag is set to YES (the default)\n# and SOURCE_BROWSER tag is set to YES, then the hyperlinks from\n# functions in REFERENCES_RELATION and REFERENCED_BY_RELATION lists will\n# link to the source code.\n# Otherwise they will link to the documentation.\n\nREFERENCES_LINK_SOURCE = YES\n\n# If the USE_HTAGS tag is set to YES then the references to source code\n# will point to the HTML generated by the htags(1) tool instead of doxygen\n# built-in source browser. The htags tool is part of GNU's global source\n# tagging system (see http://www.gnu.org/software/global/global.html). You\n# will need version 4.8.6 or higher.\n\nUSE_HTAGS              = NO\n\n# If the VERBATIM_HEADERS tag is set to YES (the default) then Doxygen\n# will generate a verbatim copy of the header file for each class for\n# which an include is specified. Set to NO to disable this.\n\nVERBATIM_HEADERS       = YES\n\n#---------------------------------------------------------------------------\n# configuration options related to the alphabetical class index\n#---------------------------------------------------------------------------\n\n# If the ALPHABETICAL_INDEX tag is set to YES, an alphabetical index\n# of all compounds will be generated. Enable this if the project\n# contains a lot of classes, structs, unions or interfaces.\n\nALPHABETICAL_INDEX     = NO\n\n# If the alphabetical index is enabled (see ALPHABETICAL_INDEX) then\n# the COLS_IN_ALPHA_INDEX tag can be used to specify the number of columns\n# in which this list will be split (can be a number in the range [1..20])\n\nCOLS_IN_ALPHA_INDEX    = 5\n\n# In case all classes in a project start with a common prefix, all\n# classes will be put under the same header in the alphabetical index.\n# The IGNORE_PREFIX tag can be used to specify one or more prefixes that\n# should be ignored while generating the index headers.\n\nIGNORE_PREFIX          =\n\n#---------------------------------------------------------------------------\n# configuration options related to the HTML output\n#---------------------------------------------------------------------------\n\n# If the GENERATE_HTML tag is set to YES (the default) Doxygen will\n# generate HTML output.\n\nGENERATE_HTML          = YES\n\n# The HTML_OUTPUT tag is used to specify where the HTML docs will be put.\n# If a relative path is entered the value of OUTPUT_DIRECTORY will be\n# put in front of it. If left blank `html' will be used as the default path.\n\nHTML_OUTPUT            = html\n\n# The HTML_FILE_EXTENSION tag can be used to specify the file extension for\n# each generated HTML page (for example: .htm,.php,.asp). If it is left blank\n# doxygen will generate files with .html extension.\n\nHTML_FILE_EXTENSION    = .html\n\n# The HTML_HEADER tag can be used to specify a personal HTML header for\n# each generated HTML page. If it is left blank doxygen will generate a\n# standard header.\n\nHTML_HEADER            =\n\n# The HTML_FOOTER tag can be used to specify a personal HTML footer for\n# each generated HTML page. If it is left blank doxygen will generate a\n# standard footer.\n\nHTML_FOOTER            =\n\n# The HTML_STYLESHEET tag can be used to specify a user-defined cascading\n# style sheet that is used by each HTML page. It can be used to\n# fine-tune the look of the HTML output. If the tag is left blank doxygen\n# will generate a default style sheet. Note that doxygen will try to copy\n# the style sheet file to the HTML output directory, so don't put your own\n# stylesheet in the HTML output directory as well, or it will be erased!\n\nHTML_STYLESHEET        =\n\n# If the HTML_ALIGN_MEMBERS tag is set to YES, the members of classes,\n# files or namespaces will be aligned in HTML using tables. If set to\n# NO a bullet list will be used.\n\nHTML_ALIGN_MEMBERS     = YES\n\n# If the HTML_DYNAMIC_SECTIONS tag is set to YES then the generated HTML\n# documentation will contain sections that can be hidden and shown after the\n# page has loaded. For this to work a browser that supports\n# JavaScript and DHTML is required (for instance Mozilla 1.0+, Firefox\n# Netscape 6.0+, Internet explorer 5.0+, Konqueror, or Safari).\n\nHTML_DYNAMIC_SECTIONS  = NO\n\n# If the GENERATE_DOCSET tag is set to YES, additional index files\n# will be generated that can be used as input for Apple's Xcode 3\n# integrated development environment, introduced with OSX 10.5 (Leopard).\n# To create a documentation set, doxygen will generate a Makefile in the\n# HTML output directory. Running make will produce the docset in that\n# directory and running \"make install\" will install the docset in\n# ~/Library/Developer/Shared/Documentation/DocSets so that Xcode will find\n# it at startup.\n# See http://developer.apple.com/tools/creatingdocsetswithdoxygen.html for more information.\n\nGENERATE_DOCSET        = NO\n\n# When GENERATE_DOCSET tag is set to YES, this tag determines the name of the\n# feed. A documentation feed provides an umbrella under which multiple\n# documentation sets from a single provider (such as a company or product suite)\n# can be grouped.\n\nDOCSET_FEEDNAME        = \"Doxygen generated docs\"\n\n# When GENERATE_DOCSET tag is set to YES, this tag specifies a string that\n# should uniquely identify the documentation set bundle. This should be a\n# reverse domain-name style string, e.g. com.mycompany.MyDocSet. Doxygen\n# will append .docset to the name.\n\nDOCSET_BUNDLE_ID       = org.doxygen.Project\n\n# If the GENERATE_HTMLHELP tag is set to YES, additional index files\n# will be generated that can be used as input for tools like the\n# Microsoft HTML help workshop to generate a compiled HTML help file (.chm)\n# of the generated HTML documentation.\n\nGENERATE_HTMLHELP      = NO\n\n# If the GENERATE_HTMLHELP tag is set to YES, the CHM_FILE tag can\n# be used to specify the file name of the resulting .chm file. You\n# can add a path in front of the file if the result should not be\n# written to the html output directory.\n\nCHM_FILE               =\n\n# If the GENERATE_HTMLHELP tag is set to YES, the HHC_LOCATION tag can\n# be used to specify the location (absolute path including file name) of\n# the HTML help compiler (hhc.exe). If non-empty doxygen will try to run\n# the HTML help compiler on the generated index.hhp.\n\nHHC_LOCATION           =\n\n# If the GENERATE_HTMLHELP tag is set to YES, the GENERATE_CHI flag\n# controls if a separate .chi index file is generated (YES) or that\n# it should be included in the master .chm file (NO).\n\nGENERATE_CHI           = NO\n\n# If the GENERATE_HTMLHELP tag is set to YES, the CHM_INDEX_ENCODING\n# is used to encode HtmlHelp index (hhk), content (hhc) and project file\n# content.\n\nCHM_INDEX_ENCODING     =\n\n# If the GENERATE_HTMLHELP tag is set to YES, the BINARY_TOC flag\n# controls whether a binary table of contents is generated (YES) or a\n# normal table of contents (NO) in the .chm file.\n\nBINARY_TOC             = NO\n\n# The TOC_EXPAND flag can be set to YES to add extra items for group members\n# to the contents of the HTML help documentation and to the tree view.\n\nTOC_EXPAND             = NO\n\n# If the GENERATE_QHP tag is set to YES and both QHP_NAMESPACE and QHP_VIRTUAL_FOLDER\n# are set, an additional index file will be generated that can be used as input for\n# Qt's qhelpgenerator to generate a Qt Compressed Help (.qch) of the generated\n# HTML documentation.\n\nGENERATE_QHP           = NO\n\n# If the QHG_LOCATION tag is specified, the QCH_FILE tag can\n# be used to specify the file name of the resulting .qch file.\n# The path specified is relative to the HTML output folder.\n\nQCH_FILE               =\n\n# The QHP_NAMESPACE tag specifies the namespace to use when generating\n# Qt Help Project output. For more information please see\n# http://doc.trolltech.com/qthelpproject.html#namespace\n\nQHP_NAMESPACE          =\n\n# The QHP_VIRTUAL_FOLDER tag specifies the namespace to use when generating\n# Qt Help Project output. For more information please see\n# http://doc.trolltech.com/qthelpproject.html#virtual-folders\n\nQHP_VIRTUAL_FOLDER     = doc\n\n# If QHP_CUST_FILTER_NAME is set, it specifies the name of a custom filter to add.\n# For more information please see\n# http://doc.trolltech.com/qthelpproject.html#custom-filters\n\nQHP_CUST_FILTER_NAME   =\n\n# The QHP_CUST_FILT_ATTRS tag specifies the list of the attributes of the custom filter to add.For more information please see\n# <a href=\"http://doc.trolltech.com/qthelpproject.html#custom-filters\">Qt Help Project / Custom Filters</a>.\n\nQHP_CUST_FILTER_ATTRS  =\n\n# The QHP_SECT_FILTER_ATTRS tag specifies the list of the attributes this project's\n# filter section matches.\n# <a href=\"http://doc.trolltech.com/qthelpproject.html#filter-attributes\">Qt Help Project / Filter Attributes</a>.\n\nQHP_SECT_FILTER_ATTRS  =\n\n# If the GENERATE_QHP tag is set to YES, the QHG_LOCATION tag can\n# be used to specify the location of Qt's qhelpgenerator.\n# If non-empty doxygen will try to run qhelpgenerator on the generated\n# .qhp file.\n\nQHG_LOCATION           =\n\n# The DISABLE_INDEX tag can be used to turn on/off the condensed index at\n# top of each HTML page. The value NO (the default) enables the index and\n# the value YES disables it.\n\nDISABLE_INDEX          = NO\n\n# This tag can be used to set the number of enum values (range [1..20])\n# that doxygen will group on one line in the generated HTML documentation.\n\nENUM_VALUES_PER_LINE   = 4\n\n# The GENERATE_TREEVIEW tag is used to specify whether a tree-like index\n# structure should be generated to display hierarchical information.\n# If the tag value is set to FRAME, a side panel will be generated\n# containing a tree-like index structure (just like the one that\n# is generated for HTML Help). For this to work a browser that supports\n# JavaScript, DHTML, CSS and frames is required (for instance Mozilla 1.0+,\n# Netscape 6.0+, Internet explorer 5.0+, or Konqueror). Windows users are\n# probably better off using the HTML help feature. Other possible values\n# for this tag are: HIERARCHIES, which will generate the Groups, Directories,\n# and Class Hierarchy pages using a tree view instead of an ordered list;\n# ALL, which combines the behavior of FRAME and HIERARCHIES; and NONE, which\n# disables this behavior completely. For backwards compatibility with previous\n# releases of Doxygen, the values YES and NO are equivalent to FRAME and NONE\n# respectively.\n\nGENERATE_TREEVIEW      = NONE\n\n# If the treeview is enabled (see GENERATE_TREEVIEW) then this tag can be\n# used to set the initial width (in pixels) of the frame in which the tree\n# is shown.\n\nTREEVIEW_WIDTH         = 250\n\n# Use this tag to change the font size of Latex formulas included\n# as images in the HTML documentation. The default is 10. Note that\n# when you change the font size after a successful doxygen run you need\n# to manually remove any form_*.png images from the HTML output directory\n# to force them to be regenerated.\n\nFORMULA_FONTSIZE       = 10\n\n#---------------------------------------------------------------------------\n# configuration options related to the LaTeX output\n#---------------------------------------------------------------------------\n\n# If the GENERATE_LATEX tag is set to YES (the default) Doxygen will\n# generate Latex output.\n\nGENERATE_LATEX         = NO\n\n# The LATEX_OUTPUT tag is used to specify where the LaTeX docs will be put.\n# If a relative path is entered the value of OUTPUT_DIRECTORY will be\n# put in front of it. If left blank `latex' will be used as the default path.\n\nLATEX_OUTPUT           = latex\n\n# The LATEX_CMD_NAME tag can be used to specify the LaTeX command name to be\n# invoked. If left blank `latex' will be used as the default command name.\n\nLATEX_CMD_NAME         = latex\n\n# The MAKEINDEX_CMD_NAME tag can be used to specify the command name to\n# generate index for LaTeX. If left blank `makeindex' will be used as the\n# default command name.\n\nMAKEINDEX_CMD_NAME     = makeindex\n\n# If the COMPACT_LATEX tag is set to YES Doxygen generates more compact\n# LaTeX documents. This may be useful for small projects and may help to\n# save some trees in general.\n\nCOMPACT_LATEX          = NO\n\n# The PAPER_TYPE tag can be used to set the paper type that is used\n# by the printer. Possible values are: a4, a4wide, letter, legal and\n# executive. If left blank a4wide will be used.\n\nPAPER_TYPE             = a4wide\n\n# The EXTRA_PACKAGES tag can be to specify one or more names of LaTeX\n# packages that should be included in the LaTeX output.\n\nEXTRA_PACKAGES         =\n\n# The LATEX_HEADER tag can be used to specify a personal LaTeX header for\n# the generated latex document. The header should contain everything until\n# the first chapter. If it is left blank doxygen will generate a\n# standard header. Notice: only use this tag if you know what you are doing!\n\nLATEX_HEADER           =\n\n# If the PDF_HYPERLINKS tag is set to YES, the LaTeX that is generated\n# is prepared for conversion to pdf (using ps2pdf). The pdf file will\n# contain links (just like the HTML output) instead of page references\n# This makes the output suitable for online browsing using a pdf viewer.\n\nPDF_HYPERLINKS         = YES\n\n# If the USE_PDFLATEX tag is set to YES, pdflatex will be used instead of\n# plain latex in the generated Makefile. Set this option to YES to get a\n# higher quality PDF documentation.\n\nUSE_PDFLATEX           = YES\n\n# If the LATEX_BATCHMODE tag is set to YES, doxygen will add the \\\\batchmode.\n# command to the generated LaTeX files. This will instruct LaTeX to keep\n# running if errors occur, instead of asking the user for help.\n# This option is also used when generating formulas in HTML.\n\nLATEX_BATCHMODE        = NO\n\n# If LATEX_HIDE_INDICES is set to YES then doxygen will not\n# include the index chapters (such as File Index, Compound Index, etc.)\n# in the output.\n\nLATEX_HIDE_INDICES     = NO\n\n#---------------------------------------------------------------------------\n# configuration options related to the RTF output\n#---------------------------------------------------------------------------\n\n# If the GENERATE_RTF tag is set to YES Doxygen will generate RTF output\n# The RTF output is optimized for Word 97 and may not look very pretty with\n# other RTF readers or editors.\n\nGENERATE_RTF           = NO\n\n# The RTF_OUTPUT tag is used to specify where the RTF docs will be put.\n# If a relative path is entered the value of OUTPUT_DIRECTORY will be\n# put in front of it. If left blank `rtf' will be used as the default path.\n\nRTF_OUTPUT             = rtf\n\n# If the COMPACT_RTF tag is set to YES Doxygen generates more compact\n# RTF documents. This may be useful for small projects and may help to\n# save some trees in general.\n\nCOMPACT_RTF            = NO\n\n# If the RTF_HYPERLINKS tag is set to YES, the RTF that is generated\n# will contain hyperlink fields. The RTF file will\n# contain links (just like the HTML output) instead of page references.\n# This makes the output suitable for online browsing using WORD or other\n# programs which support those fields.\n# Note: wordpad (write) and others do not support links.\n\nRTF_HYPERLINKS         = NO\n\n# Load stylesheet definitions from file. Syntax is similar to doxygen's\n# config file, i.e. a series of assignments. You only have to provide\n# replacements, missing definitions are set to their default value.\n\nRTF_STYLESHEET_FILE    =\n\n# Set optional variables used in the generation of an rtf document.\n# Syntax is similar to doxygen's config file.\n\nRTF_EXTENSIONS_FILE    =\n\n#---------------------------------------------------------------------------\n# configuration options related to the man page output\n#---------------------------------------------------------------------------\n\n# If the GENERATE_MAN tag is set to YES (the default) Doxygen will\n# generate man pages\n\nGENERATE_MAN           = NO\n\n# The MAN_OUTPUT tag is used to specify where the man pages will be put.\n# If a relative path is entered the value of OUTPUT_DIRECTORY will be\n# put in front of it. If left blank `man' will be used as the default path.\n\nMAN_OUTPUT             = man\n\n# The MAN_EXTENSION tag determines the extension that is added to\n# the generated man pages (default is the subroutine's section .3)\n\nMAN_EXTENSION          = .3\n\n# If the MAN_LINKS tag is set to YES and Doxygen generates man output,\n# then it will generate one additional man file for each entity\n# documented in the real man page(s). These additional files\n# only source the real man page, but without them the man command\n# would be unable to find the correct page. The default is NO.\n\nMAN_LINKS              = NO\n\n#---------------------------------------------------------------------------\n# configuration options related to the XML output\n#---------------------------------------------------------------------------\n\n# If the GENERATE_XML tag is set to YES Doxygen will\n# generate an XML file that captures the structure of\n# the code including all documentation.\n\nGENERATE_XML           = NO\n\n# The XML_OUTPUT tag is used to specify where the XML pages will be put.\n# If a relative path is entered the value of OUTPUT_DIRECTORY will be\n# put in front of it. If left blank `xml' will be used as the default path.\n\nXML_OUTPUT             = xml\n\n# The XML_SCHEMA tag can be used to specify an XML schema,\n# which can be used by a validating XML parser to check the\n# syntax of the XML files.\n\nXML_SCHEMA             =\n\n# The XML_DTD tag can be used to specify an XML DTD,\n# which can be used by a validating XML parser to check the\n# syntax of the XML files.\n\nXML_DTD                =\n\n# If the XML_PROGRAMLISTING tag is set to YES Doxygen will\n# dump the program listings (including syntax highlighting\n# and cross-referencing information) to the XML output. Note that\n# enabling this will significantly increase the size of the XML output.\n\nXML_PROGRAMLISTING     = YES\n\n#---------------------------------------------------------------------------\n# configuration options for the AutoGen Definitions output\n#---------------------------------------------------------------------------\n\n# If the GENERATE_AUTOGEN_DEF tag is set to YES Doxygen will\n# generate an AutoGen Definitions (see autogen.sf.net) file\n# that captures the structure of the code including all\n# documentation. Note that this feature is still experimental\n# and incomplete at the moment.\n\nGENERATE_AUTOGEN_DEF   = NO\n\n#---------------------------------------------------------------------------\n# configuration options related to the Perl module output\n#---------------------------------------------------------------------------\n\n# If the GENERATE_PERLMOD tag is set to YES Doxygen will\n# generate a Perl module file that captures the structure of\n# the code including all documentation. Note that this\n# feature is still experimental and incomplete at the\n# moment.\n\nGENERATE_PERLMOD       = NO\n\n# If the PERLMOD_LATEX tag is set to YES Doxygen will generate\n# the necessary Makefile rules, Perl scripts and LaTeX code to be able\n# to generate PDF and DVI output from the Perl module output.\n\nPERLMOD_LATEX          = NO\n\n# If the PERLMOD_PRETTY tag is set to YES the Perl module output will be\n# nicely formatted so it can be parsed by a human reader.\n# This is useful\n# if you want to understand what is going on.\n# On the other hand, if this\n# tag is set to NO the size of the Perl module output will be much smaller\n# and Perl will parse it just the same.\n\nPERLMOD_PRETTY         = YES\n\n# The names of the make variables in the generated doxyrules.make file\n# are prefixed with the string contained in PERLMOD_MAKEVAR_PREFIX.\n# This is useful so different doxyrules.make files included by the same\n# Makefile don't overwrite each other's variables.\n\nPERLMOD_MAKEVAR_PREFIX =\n\n#---------------------------------------------------------------------------\n# Configuration options related to the preprocessor\n#---------------------------------------------------------------------------\n\n# If the ENABLE_PREPROCESSING tag is set to YES (the default) Doxygen will\n# evaluate all C-preprocessor directives found in the sources and include\n# files.\n\nENABLE_PREPROCESSING   = YES\n\n# If the MACRO_EXPANSION tag is set to YES Doxygen will expand all macro\n# names in the source code. If set to NO (the default) only conditional\n# compilation will be performed. Macro expansion can be done in a controlled\n# way by setting EXPAND_ONLY_PREDEF to YES.\n\nMACRO_EXPANSION        = NO\n\n# If the EXPAND_ONLY_PREDEF and MACRO_EXPANSION tags are both set to YES\n# then the macro expansion is limited to the macros specified with the\n# PREDEFINED and EXPAND_AS_DEFINED tags.\n\nEXPAND_ONLY_PREDEF     = NO\n\n# If the SEARCH_INCLUDES tag is set to YES (the default) the includes files\n# in the INCLUDE_PATH (see below) will be search if a #include is found.\n\nSEARCH_INCLUDES        = YES\n\n# The INCLUDE_PATH tag can be used to specify one or more directories that\n# contain include files that are not input files but should be processed by\n# the preprocessor.\n\nINCLUDE_PATH           =\n\n# You can use the INCLUDE_FILE_PATTERNS tag to specify one or more wildcard\n# patterns (like *.h and *.hpp) to filter out the header-files in the\n# directories. If left blank, the patterns specified with FILE_PATTERNS will\n# be used.\n\nINCLUDE_FILE_PATTERNS  =\n\n# The PREDEFINED tag can be used to specify one or more macro names that\n# are defined before the preprocessor is started (similar to the -D option of\n# gcc). The argument of the tag is a list of macros of the form: name\n# or name=definition (no spaces). If the definition and the = are\n# omitted =1 is assumed. To prevent a macro definition from being\n# undefined via #undef or recursively expanded use the := operator\n# instead of the = operator.\n\nPREDEFINED             =\n\n# If the MACRO_EXPANSION and EXPAND_ONLY_PREDEF tags are set to YES then\n# this tag can be used to specify a list of macro names that should be expanded.\n# The macro definition that is found in the sources will be used.\n# Use the PREDEFINED tag if you want to use a different macro definition.\n\nEXPAND_AS_DEFINED      =\n\n# If the SKIP_FUNCTION_MACROS tag is set to YES (the default) then\n# doxygen's preprocessor will remove all function-like macros that are alone\n# on a line, have an all uppercase name, and do not end with a semicolon. Such\n# function macros are typically used for boiler-plate code, and will confuse\n# the parser if not removed.\n\nSKIP_FUNCTION_MACROS   = YES\n\n#---------------------------------------------------------------------------\n# Configuration::additions related to external references\n#---------------------------------------------------------------------------\n\n# The TAGFILES option can be used to specify one or more tagfiles.\n# Optionally an initial location of the external documentation\n# can be added for each tagfile. The format of a tag file without\n# this location is as follows:\n#\n# TAGFILES = file1 file2 ...\n# Adding location for the tag files is done as follows:\n#\n# TAGFILES = file1=loc1 \"file2 = loc2\" ...\n# where \"loc1\" and \"loc2\" can be relative or absolute paths or\n# URLs. If a location is present for each tag, the installdox tool\n# does not have to be run to correct the links.\n# Note that each tag file must have a unique name\n# (where the name does NOT include the path)\n# If a tag file is not located in the directory in which doxygen\n# is run, you must also specify the path to the tagfile here.\n\nTAGFILES               =\n\n# When a file name is specified after GENERATE_TAGFILE, doxygen will create\n# a tag file that is based on the input files it reads.\n\nGENERATE_TAGFILE       =\n\n# If the ALLEXTERNALS tag is set to YES all external classes will be listed\n# in the class index. If set to NO only the inherited external classes\n# will be listed.\n\nALLEXTERNALS           = NO\n\n# If the EXTERNAL_GROUPS tag is set to YES all external groups will be listed\n# in the modules index. If set to NO, only the current project's groups will\n# be listed.\n\nEXTERNAL_GROUPS        = YES\n\n# The PERL_PATH should be the absolute path and name of the perl script\n# interpreter (i.e. the result of `which perl').\n\nPERL_PATH              = /usr/bin/perl\n\n#---------------------------------------------------------------------------\n# Configuration options related to the dot tool\n#---------------------------------------------------------------------------\n\n# If the CLASS_DIAGRAMS tag is set to YES (the default) Doxygen will\n# generate a inheritance diagram (in HTML, RTF and LaTeX) for classes with base\n# or super classes. Setting the tag to NO turns the diagrams off. Note that\n# this option is superseded by the HAVE_DOT option below. This is only a\n# fallback. It is recommended to install and use dot, since it yields more\n# powerful graphs.\n\nCLASS_DIAGRAMS         = YES\n\n# You can define message sequence charts within doxygen comments using the \\msc\n# command. Doxygen will then run the mscgen tool (see\n# http://www.mcternan.me.uk/mscgen/) to produce the chart and insert it in the\n# documentation. The MSCGEN_PATH tag allows you to specify the directory where\n# the mscgen tool resides. If left empty the tool is assumed to be found in the\n# default search path.\n\nMSCGEN_PATH            =\n\n# If set to YES, the inheritance and collaboration graphs will hide\n# inheritance and usage relations if the target is undocumented\n# or is not a class.\n\nHIDE_UNDOC_RELATIONS   = YES\n\n# If you set the HAVE_DOT tag to YES then doxygen will assume the dot tool is\n# available from the path. This tool is part of Graphviz, a graph visualization\n# toolkit from AT&T and Lucent Bell Labs. The other options in this section\n# have no effect if this option is set to NO (the default)\n\nHAVE_DOT               = YES\n\n# By default doxygen will write a font called FreeSans.ttf to the output\n# directory and reference it in all dot files that doxygen generates. This\n# font does not include all possible unicode characters however, so when you need\n# these (or just want a differently looking font) you can specify the font name\n# using DOT_FONTNAME. You need need to make sure dot is able to find the font,\n# which can be done by putting it in a standard location or by setting the\n# DOTFONTPATH environment variable or by setting DOT_FONTPATH to the directory\n# containing the font.\n\nDOT_FONTNAME           = FreeSans\n\n# The DOT_FONTSIZE tag can be used to set the size of the font of dot graphs.\n# The default size is 10pt.\n\nDOT_FONTSIZE           = 10\n\n# By default doxygen will tell dot to use the output directory to look for the\n# FreeSans.ttf font (which doxygen will put there itself). If you specify a\n# different font using DOT_FONTNAME you can set the path where dot\n# can find it using this tag.\n\nDOT_FONTPATH           =\n\n# If the CLASS_GRAPH and HAVE_DOT tags are set to YES then doxygen\n# will generate a graph for each documented class showing the direct and\n# indirect inheritance relations. Setting this tag to YES will force the\n# the CLASS_DIAGRAMS tag to NO.\n\nCLASS_GRAPH            = YES\n\n# If the COLLABORATION_GRAPH and HAVE_DOT tags are set to YES then doxygen\n# will generate a graph for each documented class showing the direct and\n# indirect implementation dependencies (inheritance, containment, and\n# class references variables) of the class with other documented classes.\n\nCOLLABORATION_GRAPH    = YES\n\n# If the GROUP_GRAPHS and HAVE_DOT tags are set to YES then doxygen\n# will generate a graph for groups, showing the direct groups dependencies\n\nGROUP_GRAPHS           = YES\n\n# If the UML_LOOK tag is set to YES doxygen will generate inheritance and\n# collaboration diagrams in a style similar to the OMG's Unified Modeling\n# Language.\n\nUML_LOOK               = NO\n\n# If set to YES, the inheritance and collaboration graphs will show the\n# relations between templates and their instances.\n\nTEMPLATE_RELATIONS     = NO\n\n# If the ENABLE_PREPROCESSING, SEARCH_INCLUDES, INCLUDE_GRAPH, and HAVE_DOT\n# tags are set to YES then doxygen will generate a graph for each documented\n# file showing the direct and indirect include dependencies of the file with\n# other documented files.\n\nINCLUDE_GRAPH          = YES\n\n# If the ENABLE_PREPROCESSING, SEARCH_INCLUDES, INCLUDED_BY_GRAPH, and\n# HAVE_DOT tags are set to YES then doxygen will generate a graph for each\n# documented header file showing the documented files that directly or\n# indirectly include this file.\n\nINCLUDED_BY_GRAPH      = YES\n\n# If the CALL_GRAPH and HAVE_DOT options are set to YES then\n# doxygen will generate a call dependency graph for every global function\n# or class method. Note that enabling this option will significantly increase\n# the time of a run. So in most cases it will be better to enable call graphs\n# for selected functions only using the \\callgraph command.\n\nCALL_GRAPH             = YES\n\n# If the CALLER_GRAPH and HAVE_DOT tags are set to YES then\n# doxygen will generate a caller dependency graph for every global function\n# or class method. Note that enabling this option will significantly increase\n# the time of a run. So in most cases it will be better to enable caller\n# graphs for selected functions only using the \\callergraph command.\n\nCALLER_GRAPH           = YES\n\n# If the GRAPHICAL_HIERARCHY and HAVE_DOT tags are set to YES then doxygen\n# will graphical hierarchy of all classes instead of a textual one.\n\nGRAPHICAL_HIERARCHY    = YES\n\n# If the DIRECTORY_GRAPH, SHOW_DIRECTORIES and HAVE_DOT tags are set to YES\n# then doxygen will show the dependencies a directory has on other directories\n# in a graphical way. The dependency relations are determined by the #include\n# relations between the files in the directories.\n\nDIRECTORY_GRAPH        = YES\n\n# The DOT_IMAGE_FORMAT tag can be used to set the image format of the images\n# generated by dot. Possible values are png, jpg, or gif\n# If left blank png will be used.\n\nDOT_IMAGE_FORMAT       = png\n\n# The tag DOT_PATH can be used to specify the path where the dot tool can be\n# found. If left blank, it is assumed the dot tool can be found in the path.\n\nDOT_PATH               =\n\n# The DOTFILE_DIRS tag can be used to specify one or more directories that\n# contain dot files that are included in the documentation (see the\n# \\dotfile command).\n\nDOTFILE_DIRS           =\n\n# The DOT_GRAPH_MAX_NODES tag can be used to set the maximum number of\n# nodes that will be shown in the graph. If the number of nodes in a graph\n# becomes larger than this value, doxygen will truncate the graph, which is\n# visualized by representing a node as a red box. Note that doxygen if the\n# number of direct children of the root node in a graph is already larger than\n# DOT_GRAPH_MAX_NODES then the graph will not be shown at all. Also note\n# that the size of a graph can be further restricted by MAX_DOT_GRAPH_DEPTH.\n\nDOT_GRAPH_MAX_NODES    = 50\n\n# The MAX_DOT_GRAPH_DEPTH tag can be used to set the maximum depth of the\n# graphs generated by dot. A depth value of 3 means that only nodes reachable\n# from the root by following a path via at most 3 edges will be shown. Nodes\n# that lay further from the root node will be omitted. Note that setting this\n# option to 1 or 2 may greatly reduce the computation time needed for large\n# code bases. Also note that the size of a graph can be further restricted by\n# DOT_GRAPH_MAX_NODES. Using a depth of 0 means no depth restriction.\n\nMAX_DOT_GRAPH_DEPTH    = 0\n\n# Set the DOT_TRANSPARENT tag to YES to generate images with a transparent\n# background. This is disabled by default, because dot on Windows does not\n# seem to support this out of the box. Warning: Depending on the platform used,\n# enabling this option may lead to badly anti-aliased labels on the edges of\n# a graph (i.e. they become hard to read).\n\nDOT_TRANSPARENT        = NO\n\n# Set the DOT_MULTI_TARGETS tag to YES allow dot to generate multiple output\n# files in one run (i.e. multiple -o and -T options on the command line). This\n# makes dot run faster, but since only newer versions of dot (>1.8.10)\n# support this, this feature is disabled by default.\n\nDOT_MULTI_TARGETS      = NO\n\n# If the GENERATE_LEGEND tag is set to YES (the default) Doxygen will\n# generate a legend page explaining the meaning of the various boxes and\n# arrows in the dot generated graphs.\n\nGENERATE_LEGEND        = YES\n\n# If the DOT_CLEANUP tag is set to YES (the default) Doxygen will\n# remove the intermediate dot files that are used to generate\n# the various graphs.\n\nDOT_CLEANUP            = YES\n\n#---------------------------------------------------------------------------\n# Options related to the search engine\n#---------------------------------------------------------------------------\n\n# The SEARCHENGINE tag specifies whether or not a search engine should be\n# used. If set to NO the values of all tags below this one will be ignored.\n\nSEARCHENGINE           = NO\n"
        },
        {
          "name": "HACKING",
          "type": "blob",
          "size": 2.1982421875,
          "content": "* Hacking Memcached\n\n* Prerequisites\n\n - autoconf\n - automake\n - autotools\n - libevent\n\n* Getting Started\n\nAfter checking out a git repository, you must first run autogen.sh\nonce in order to create the configure script.\n\nNext, run the configure script and start doing builds.\n\nIE:\n    ./autogen.sh && ./configure && make && make test\n\n* Setting up Git\n\nThough not required, there are a couple of things you can add to git\nto help development.\n\n** Pre Commit Hook\n\nThe pre-commit hook can be used to ensure that your tree passes tests\nbefore allowing a commit.  To do so, add the following to\n.git/hooks/pre-commit (which must be executable):\n\n    #!/bin/sh\n    make test\n\n** Post Commit Hook\n\nBecause the version number changes on each commit, it's good to use a\npost commit hook to update the version number after each commit so as\nto keep the reporting accurate.  To do so, add the following to\n.git/hooks/post-commit (which must be executable)\n\n    #!/bin/sh\n    ./version.sh\n\n** Running memcached in gdb for tests.\n\nBy default `make test` will spawn a memcached daemon for each test.\nThis doesn't let you easily drop into gdb or run verbosely.\n\nIf you export the environment variable\nT_MEMD_USE_DAEMON=\"127.0.0.1:11211\" the tests will use an existing\ndaemon at that address.\n\n* Debugging seccomp issues\n\nIf new functionality fails when built with seccomp / drop privileges\nsupport, it can be debugged in one of two ways:\n\nRun the memcached via strace. For example:\n\n    strace -o /tmp/memcache.strace -f -- ./memcached\n    less /tmp/memcache.strace\n\nAnd look for calls which failed due to access restriction. They will\nshow up with result: \"-1 (errno 13)\". Then add them to linux_priv.c.\n\nAlternatively, change the definition in linux_priv.c to:\n\n    #define DENY_ACTION SCMP_ACT_TRAP\n\nand the process will crash with a coredump on all policy violations.\nIn strace output those can be seen as:\n\n    SIGSYS {si_signo=SIGSYS, si_code=SYS_SECCOMP,\n    si_call_addr=0x358a443454d, si_syscall=__NR_write,\n    si_arch=AUDIT_ARCH_X86_64} ---\n\nIn that output, the si_syscall shows which operation has been\nblocked. In this case that's `write()`.\n\n* Sending patches\n\nSee current instructions at https://github.com/memcached/memcached/wiki/DevelopmentRepos\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.4677734375,
          "content": "Copyright (c) 2003, Danga Interactive, Inc.\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are\nmet:\n\n    * Redistributions of source code must retain the above copyright\nnotice, this list of conditions and the following disclaimer.\n\n    * Redistributions in binary form must reproduce the above\ncopyright notice, this list of conditions and the following disclaimer\nin the documentation and/or other materials provided with the\ndistribution.\n\n    * Neither the name of the Danga Interactive nor the names of its\ncontributors may be used to endorse or promote products derived from\nthis software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n\"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\nLIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\nA PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\nOWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\nSPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\nLIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\nDATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\nTHEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n"
        },
        {
          "name": "LICENSE.bipbuffer",
          "type": "blob",
          "size": 1.4375,
          "content": "Copyright (c) 2011, Willem-Hendrik Thiart\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n    * Redistributions of source code must retain the above copyright\n      notice, this list of conditions and the following disclaimer.\n    * Redistributions in binary form must reproduce the above copyright\n      notice, this list of conditions and the following disclaimer in the\n      documentation and/or other materials provided with the distribution.\n    * The names of its contributors may not be used to endorse or promote\n      products derived from this software without specific prior written\n      permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\nANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL WILLEM-HENDRIK THIART BE LIABLE FOR ANY\nDIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\nLOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\nON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n"
        },
        {
          "name": "LICENSE.itoa_ljust",
          "type": "blob",
          "size": 1.73046875,
          "content": "Copyright (c) 2016, Arturo Martin-de-Nicolas\narturomdn@gmail.com\nhttps://github.com/amdn/itoa_ljust/\nAll rights reserved.\n\nThis implementation is loosely based on the structure of FastInt32ToBufferLeft\nin:\n\nProtocol Buffers - Google's data interchange format\nCopyright 2008 Google Inc.  All rights reserved.\nhttps://developers.google.com/protocol-buffers/\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are\nmet:\n\n* Redistributions of source code must retain the above copyright\nnotice, this list of conditions and the following disclaimer.\n\n* Redistributions in binary form must reproduce the above\ncopyright notice, this list of conditions and the following disclaimer\nin the documentation and/or other materials provided with the\ndistribution.\n\n* Neither the name of Google Inc. nor the names of its\ncontributors may be used to endorse or promote products derived from\nthis software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n\"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\nLIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\nA PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\nOWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\nSPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\nLIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\nDATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\nTHEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n"
        },
        {
          "name": "Makefile.am",
          "type": "blob",
          "size": 6.599609375,
          "content": "bin_PROGRAMS = memcached\npkginclude_HEADERS = protocol_binary.h xxhash.h\nnoinst_PROGRAMS = memcached-debug sizes testapp timedrun\n\nBUILT_SOURCES=\n\ntestapp_SOURCES = testapp.c util.c util.h stats_prefix.c stats_prefix.h jenkins_hash.c murmur3_hash.c hash.h cache.c crc32c.c\n\ntimedrun_SOURCES = timedrun.c\n\nmemcached_SOURCES = memcached.c memcached.h \\\n                    hash.c hash.h \\\n                    jenkins_hash.c jenkins_hash.h \\\n                    murmur3_hash.c murmur3_hash.h \\\n                    queue.h \\\n                    slabs.c slabs.h \\\n                    items.c items.h \\\n                    assoc.c assoc.h \\\n                    thread.c daemon.c \\\n                    stats_prefix.c stats_prefix.h \\\n                    util.c util.h \\\n                    trace.h cache.c cache.h sasl_defs.h \\\n                    bipbuffer.c bipbuffer.h \\\n                    base64.c base64.h \\\n                    logger.c logger.h \\\n                    crawler.c crawler.h \\\n                    itoa_ljust.c itoa_ljust.h \\\n                    slab_automove.c slab_automove.h \\\n                    slabs_mover.c slabs_mover.h \\\n                    authfile.c authfile.h \\\n                    restart.c restart.h \\\n                    proto_text.c proto_text.h \\\n                    proto_bin.c proto_bin.h\n\nif BUILD_SOLARIS_PRIVS\nmemcached_SOURCES += solaris_priv.c\nendif\n\nif BUILD_LINUX_PRIVS\nmemcached_SOURCES += linux_priv.c\nendif\n\nif BUILD_OPENBSD_PRIVS\nmemcached_SOURCES += openbsd_priv.c\nendif\n\nif BUILD_FREEBSD_PRIVS\nmemcached_SOURCES += freebsd_priv.c\nendif\n\nif BUILD_DARWIN_PRIVS\nmemcached_SOURCES += darwin_priv.c\nendif\n\nif ENABLE_SASL\nmemcached_SOURCES += sasl_defs.c\nendif\n\nif ENABLE_PROXY\nmemcached_SOURCES += proto_proxy.c proto_proxy.h vendor/mcmc/mcmc.h \\\n\t\t\t\t\t proxy_xxhash.c proxy.h \\\n\t\t\t\t\t proxy_ustats.c \\\n\t\t\t\t\t proxy_ratelim.c \\\n\t\t\t\t\t proxy_jump_hash.c proxy_request.c \\\n\t\t\t\t\t proxy_result.c proxy_inspector.c \\\n\t\t\t\t\t proxy_mutator.c \\\n\t\t\t\t\t proxy_network.c proxy_lua.c \\\n\t\t\t\t\t proxy_luafgen.c \\\n\t\t\t\t\t proxy_config.c proxy_ring_hash.c \\\n\t\t\t\t\t proxy_internal.c \\\n\t\t\t\t\t proxy_tls.c proxy_tls.h \\\n\t\t\t\t\t md5.c md5.h \\\n\t\t\t\t\t vendor/routelib/routelib.h\nendif\n\nif ENABLE_EXTSTORE\nmemcached_SOURCES += extstore.c extstore.h \\\n                     crc32c.c crc32c.h \\\n                     storage.c storage.h \\\n                     slab_automove_extstore.c slab_automove_extstore.h\nendif\n\nif ENABLE_TLS\nmemcached_SOURCES += tls.c tls.h\nendif\n\nmemcached_debug_SOURCES = $(memcached_SOURCES)\nmemcached_CPPFLAGS = -DNDEBUG\nmemcached_debug_LDADD = @PROFILER_LDFLAGS@\nmemcached_debug_CFLAGS = @PROFILER_FLAGS@\n\nmemcached_LDADD =\nmemcached_LDFLAGS =\nmemcached_debug_LDFLAGS =\nmemcached_DEPENDENCIES =\nmemcached_debug_DEPENDENCIES =\nCLEANFILES=\n\nif BUILD_LINUX_PRIVS\nmemcached_LDADD += -lseccomp\nmemcached_debug_LDADD += -lseccomp\nendif\n\nif BUILD_DTRACE\nBUILT_SOURCES += memcached_dtrace.h\nCLEANFILES += memcached_dtrace.h\nendif\n\nif DTRACE_INSTRUMENT_OBJ\nmemcached_LDADD += memcached_dtrace.o\nmemcached_DEPENDENCIES += memcached_dtrace.o\nmemcached_debug_LDADD += memcached_debug_dtrace.o\nmemcached_debug_DEPENDENCIES += memcached_debug_dtrace.o\nCLEANFILES += memcached_dtrace.o memcached_debug_dtrace.o\nendif\n\nif ENABLE_PROXY\nmemcached_LDADD += vendor/lua/src/liblua.a vendor/mcmc/mcmc.o\nmemcached_debug_LDADD += vendor/lua/src/liblua.a vendor/mcmc/mcmc.o\nmemcached_LDFLAGS += -rdynamic\nmemcached_debug_LDFLAGS += -rdynamic\nendif\n\nif ENABLE_PROXY_URING\nmemcached_LDADD += vendor/liburing/src/liburing.a\nmemcached_debug_LDADD += vendor/liburing/src/liburing.a\nendif\n\nmemcached_debug_CFLAGS += -DMEMCACHED_DEBUG\n\n# build fails on Darwin with const signature replacements.\nif DARWIN\nmemcached_dtrace.h: memcached_dtrace.d\n\t${DTRACE} -h -s memcached_dtrace.d\nelse\nmemcached_dtrace.h: memcached_dtrace.d\n\t${DTRACE} -h -s memcached_dtrace.d\n\tsed -e 's,void \\*,const void \\*,' memcached_dtrace.h | \\\n\tsed -e 's,char \\*,const char \\*,g' | tr '\\t' ' ' > mmc_dtrace.tmp\n\tmv mmc_dtrace.tmp memcached_dtrace.h\nendif\n\nmemcached_dtrace.o: $(memcached_OBJECTS)\n\t$(DTRACE) $(DTRACEFLAGS) -G -o memcached_dtrace.o -s ${srcdir}/memcached_dtrace.d $(memcached_OBJECTS)\n\nmemcached_debug_dtrace.o: $(memcached_debug_OBJECTS)\n\t$(DTRACE) $(DTRACEFLAGS) -G -o memcached_debug_dtrace.o -s ${srcdir}/memcached_dtrace.d $(memcached_debug_OBJECTS)\n\n\nSUBDIRS = doc\nDIST_DIRS = scripts\nEXTRA_DIST = doc scripts t memcached.spec memcached_dtrace.d version.m4 README.md LICENSE.bipbuffer\nEXTRA_DIST += vendor/Makefile vendor/lua/doc/* vendor/lua/Makefile vendor/lua/README\nEXTRA_DIST += vendor/lua/src/*.c vendor/lua/src/*.h vendor/lua/src/Makefile\nEXTRA_DIST += vendor/mcmc/LICENSE vendor/mcmc/Makefile vendor/mcmc/README.md vendor/mcmc/*.c vendor/mcmc/*.h\nEXTRA_DIST += vendor/routelib/*.h vendor/routelib/*.lua\n\nif ENABLE_PROXY\nSUBDIRS += vendor\nendif\n\nMOSTLYCLEANFILES = *.gcov *.gcno *.gcda *.tcov\n\nif ENABLE_TLS\ntest_tls:\n\t$(MAKE) SSL_TEST=1 test\n\ntest_basic_tls:\n\t@if test $(SSL_TEST)1 != 1; then \\\n\t  echo \"Running basic tests with TLS\"; \\\n\t  $(builddir)/testapp && \\\n\t  prove $(srcdir)/t/binary.t $(srcdir)/t/getset.t $(srcdir)/t/ssl* && \\\n\t  echo \"Finished running basic TLS tests\"; \\\n\telse \\\n\t  echo \"Set SSL_TEST=1 to enable TLS tests\"; \\\n\tfi\nendif\n\ntest:\tmemcached-debug sizes testapp\n\t$(builddir)/sizes\n\t$(builddir)/testapp\nif ENABLE_TLS\n\t@if test $(SSL_TEST)1 = 1; then \\\n          $(MAKE) SSL_TEST=1  test_basic_tls; \\\n\tfi\nendif\n\t@if test -n \"${PARALLEL}\"; then \\\n\t  prove $(srcdir)/t -j ${PARALLEL}; \\\n\telse \\\n\t  prove $(srcdir)/t; \\\n\tfi\n\t@if test `basename $(PROFILER)` = \"gcov\"; then \\\n\t  for file in memcached_debug-*.gc??; do \\\n\t    mv -f $$file `echo $$file | sed 's/memcached_debug-//'`; \\\n\t  done && \\\n\t  for file in *.gcda; do \\\n\t    srcfile=`echo $$file | sed 's/.gcda/.c/'`; \\\n\t    if test -n \"`echo $(memcached_debug_SOURCES) | grep $$srcfile`\"; then \\\n\t      echo `$(PROFILER) $$srcfile` | sed 's/'$$srcfile':.*//'; \\\n\t    fi \\\n\t  done \\\n\telif test `basename $(PROFILER)` = \"tcov\"; then \\\n\t  files=`grep SRCFILE memcached-debug.profile/tcovd | sed 's/SRCFILE://' | sort | uniq` && \\\n\t  $(PROFILER) -x memcached-debug.profile $$files 2>&1; \\\n\t  for file in *.tcov; do \\\n\t    srcfile=`echo $$file | sed 's/.tcov//'`; \\\n\t    if test -n \"`echo $(memcached_debug_SOURCES) | grep $$srcfile`\"; then \\\n\t      echo $$srcfile : `grep 'Percent of the file executed' $$file`; \\\n\t    fi \\\n\t  done \\\n\telse :; fi\n\ndocs:\n\t(cat Doxyfile ; echo \"PROJECT_NUMBER=`cat version.num`\") | doxygen -\n\ndist-hook:\n\trm -f $(distdir)/*/*~ $(distdir)/t/lib/*~ $(distdir)/*~\n\nmaintainer-clean-local:\n\t-rm Makefile.in\n\t-rm aclocal.m4\n\t-rm config.guess\n\t-rm config.sub\n\t-rm depcomp\n\t-rm install-sh\n\t-rm ltmain.sh\n\t-rm missing\n\t-rm configure\n\t-rm config.log\n\t-rm config.status\n"
        },
        {
          "name": "NEWS",
          "type": "blob",
          "size": 0.0224609375,
          "content": "https://memcached.org/\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 1.751953125,
          "content": "# Memcached\n\nMemcached is a high performance multithreaded event-based key/value cache\nstore intended to be used in a distributed system.\n\nSee: https://memcached.org/about\n\nA fun story explaining usage: https://memcached.org/tutorial\n\nIf you're having trouble, try the wiki: https://memcached.org/wiki\n\nIf you're trying to troubleshoot odd behavior or timeouts, see:\nhttps://memcached.org/timeouts\n\nhttps://memcached.org/ is a good resource in general. Please use the mailing\nlist to ask questions, github issues aren't seen by everyone!\n\n## Dependencies\n\n* libevent - https://www.monkey.org/~provos/libevent/ (libevent-dev)\n* libseccomp (optional, experimental, linux) - enables process restrictions for\n  better security. Tested only on x86-64 architectures.\n* openssl (optional) - enables TLS support. need relatively up to date\n  version. pkg-config is needed to find openssl dependencies (such as -lz).\n\n## Environment\n\nBe warned that the -k (mlockall) option to memcached might be\ndangerous when using a large cache. Just make sure the memcached machines\ndon't swap.  memcached does non-blocking network I/O, but not disk.  (it\nshould never go to disk, or you've lost the whole point of it)\n\n## Build status\n\nSee https://build.memcached.org/ for multi-platform regression testing status.\n\n## Bug reports\n\nFeel free to use the issue tracker on github.\n\n**If you are reporting a security bug** please contact a maintainer privately.\nWe follow responsible disclosure: we handle reports privately, prepare a\npatch, allow notifications to vendor lists. Then we push a fix release and your\nbug can be posted publicly with credit in our release notes and commit\nhistory.\n\n## Website\n\n* https://www.memcached.org\n\n## Contributing\n\nSee https://github.com/memcached/memcached/wiki/DevelopmentRepos\n"
        },
        {
          "name": "assoc.c",
          "type": "blob",
          "size": 11.23046875,
          "content": "/* -*- Mode: C; tab-width: 4; c-basic-offset: 4; indent-tabs-mode: nil -*- */\n/*\n * Hash table\n *\n * The hash function used here is by Bob Jenkins, 1996:\n *    <http://burtleburtle.net/bob/hash/doobs.html>\n *       \"By Bob Jenkins, 1996.  bob_jenkins@burtleburtle.net.\n *       You may use this code any way you wish, private, educational,\n *       or commercial.  It's free.\"\n *\n * The rest of the file is licensed under the BSD license.  See LICENSE.\n */\n\n#include \"memcached.h\"\n#include <sys/stat.h>\n#include <sys/socket.h>\n#include <sys/resource.h>\n#include <signal.h>\n#include <fcntl.h>\n#include <netinet/in.h>\n#include <errno.h>\n#include <stdlib.h>\n#include <stdio.h>\n#include <string.h>\n#include <assert.h>\n#include <pthread.h>\n\nstatic pthread_cond_t maintenance_cond = PTHREAD_COND_INITIALIZER;\nstatic pthread_mutex_t maintenance_lock = PTHREAD_MUTEX_INITIALIZER;\n\n/* how many powers of 2's worth of buckets we use */\nunsigned int hashpower = HASHPOWER_DEFAULT;\n\n#define hashsize(n) ((uint64_t)1<<(n))\n#define hashmask(n) (hashsize(n)-1)\n\n/* Main hash table. This is where we look except during expansion. */\nstatic item** primary_hashtable = 0;\n\n/*\n * Previous hash table. During expansion, we look here for keys that haven't\n * been moved over to the primary yet.\n */\nstatic item** old_hashtable = 0;\n\n/* Flag: Are we in the middle of expanding now? */\nstatic bool expanding = false;\n\n/*\n * During expansion we migrate values with bucket granularity; this is how\n * far we've gotten so far. Ranges from 0 .. hashsize(hashpower - 1) - 1.\n */\nstatic uint64_t expand_bucket = 0;\n\nvoid assoc_init(const int hashtable_init) {\n    if (hashtable_init) {\n        hashpower = hashtable_init;\n    }\n    primary_hashtable = calloc(hashsize(hashpower), sizeof(void *));\n    if (! primary_hashtable) {\n        fprintf(stderr, \"Failed to init hashtable.\\n\");\n        exit(EXIT_FAILURE);\n    }\n    STATS_LOCK();\n    stats_state.hash_power_level = hashpower;\n    stats_state.hash_bytes = hashsize(hashpower) * sizeof(void *);\n    STATS_UNLOCK();\n}\n\nitem *assoc_find(const char *key, const size_t nkey, const uint32_t hv) {\n    item *it;\n    uint64_t oldbucket;\n\n    if (expanding &&\n        (oldbucket = (hv & hashmask(hashpower - 1))) >= expand_bucket)\n    {\n        it = old_hashtable[oldbucket];\n    } else {\n        it = primary_hashtable[hv & hashmask(hashpower)];\n    }\n\n    item *ret = NULL;\n#ifdef ENABLE_DTRACE\n    int depth = 0;\n#endif\n    while (it) {\n        if ((nkey == it->nkey) && (memcmp(key, ITEM_key(it), nkey) == 0)) {\n            ret = it;\n            break;\n        }\n        it = it->h_next;\n#ifdef ENABLE_DTRACE\n        ++depth;\n#endif\n    }\n    MEMCACHED_ASSOC_FIND(key, nkey, depth);\n    return ret;\n}\n\n/* returns the address of the item pointer before the key.  if *item == 0,\n   the item wasn't found */\n\nstatic item** _hashitem_before (const char *key, const size_t nkey, const uint32_t hv) {\n    item **pos;\n    uint64_t oldbucket;\n\n    if (expanding &&\n        (oldbucket = (hv & hashmask(hashpower - 1))) >= expand_bucket)\n    {\n        pos = &old_hashtable[oldbucket];\n    } else {\n        pos = &primary_hashtable[hv & hashmask(hashpower)];\n    }\n\n    while (*pos && ((nkey != (*pos)->nkey) || memcmp(key, ITEM_key(*pos), nkey))) {\n        pos = &(*pos)->h_next;\n    }\n    return pos;\n}\n\n/* grows the hashtable to the next power of 2. */\nstatic void assoc_expand(void) {\n    old_hashtable = primary_hashtable;\n\n    primary_hashtable = calloc(hashsize(hashpower + 1), sizeof(void *));\n    if (primary_hashtable) {\n        if (settings.verbose > 1)\n            fprintf(stderr, \"Hash table expansion starting\\n\");\n        hashpower++;\n        expanding = true;\n        expand_bucket = 0;\n        STATS_LOCK();\n        stats_state.hash_power_level = hashpower;\n        stats_state.hash_bytes += hashsize(hashpower) * sizeof(void *);\n        stats_state.hash_is_expanding = true;\n        STATS_UNLOCK();\n    } else {\n        primary_hashtable = old_hashtable;\n        /* Bad news, but we can keep running. */\n    }\n}\n\nvoid assoc_start_expand(uint64_t curr_items) {\n    if (pthread_mutex_trylock(&maintenance_lock) == 0) {\n        if (curr_items > (hashsize(hashpower) * 3) / 2 && hashpower < HASHPOWER_MAX) {\n            pthread_cond_signal(&maintenance_cond);\n        }\n        pthread_mutex_unlock(&maintenance_lock);\n    }\n}\n\n/* Note: this isn't an assoc_update.  The key must not already exist to call this */\nint assoc_insert(item *it, const uint32_t hv) {\n    uint64_t oldbucket;\n\n//    assert(assoc_find(ITEM_key(it), it->nkey) == 0);  /* shouldn't have duplicately named things defined */\n\n    if (expanding &&\n        (oldbucket = (hv & hashmask(hashpower - 1))) >= expand_bucket)\n    {\n        it->h_next = old_hashtable[oldbucket];\n        old_hashtable[oldbucket] = it;\n    } else {\n        it->h_next = primary_hashtable[hv & hashmask(hashpower)];\n        primary_hashtable[hv & hashmask(hashpower)] = it;\n    }\n\n    MEMCACHED_ASSOC_INSERT(ITEM_key(it), it->nkey);\n    return 1;\n}\n\nvoid assoc_delete(const char *key, const size_t nkey, const uint32_t hv) {\n    item **before = _hashitem_before(key, nkey, hv);\n\n    if (*before) {\n        item *nxt;\n        /* The DTrace probe cannot be triggered as the last instruction\n         * due to possible tail-optimization by the compiler\n         */\n        MEMCACHED_ASSOC_DELETE(key, nkey);\n        nxt = (*before)->h_next;\n        (*before)->h_next = 0;   /* probably pointless, but whatever. */\n        *before = nxt;\n        return;\n    }\n    /* Note:  we never actually get here.  the callers don't delete things\n       they can't find. */\n    assert(*before != 0);\n}\n\n\nstatic volatile int do_run_maintenance_thread = 1;\n\n#define DEFAULT_HASH_BULK_MOVE 1\nint hash_bulk_move = DEFAULT_HASH_BULK_MOVE;\n\nstatic void *assoc_maintenance_thread(void *arg) {\n\n    mutex_lock(&maintenance_lock);\n    while (do_run_maintenance_thread) {\n        int ii = 0;\n\n        /* There is only one expansion thread, so no need to global lock. */\n        for (ii = 0; ii < hash_bulk_move && expanding; ++ii) {\n            item *it, *next;\n            uint64_t bucket;\n            void *item_lock = NULL;\n\n            /* bucket = hv & hashmask(hashpower) =>the bucket of hash table\n             * is the lowest N bits of the hv, and the bucket of item_locks is\n             *  also the lowest M bits of hv, and N is greater than M.\n             *  So we can process expanding with only one item_lock. cool! */\n            if ((item_lock = item_trylock(expand_bucket))) {\n                    for (it = old_hashtable[expand_bucket]; NULL != it; it = next) {\n                        next = it->h_next;\n                        bucket = hash(ITEM_key(it), it->nkey) & hashmask(hashpower);\n                        it->h_next = primary_hashtable[bucket];\n                        primary_hashtable[bucket] = it;\n                    }\n\n                    old_hashtable[expand_bucket] = NULL;\n\n                    expand_bucket++;\n                    if (expand_bucket == hashsize(hashpower - 1)) {\n                        expanding = false;\n                        free(old_hashtable);\n                        STATS_LOCK();\n                        stats_state.hash_bytes -= hashsize(hashpower - 1) * sizeof(void *);\n                        stats_state.hash_is_expanding = false;\n                        STATS_UNLOCK();\n                        if (settings.verbose > 1)\n                            fprintf(stderr, \"Hash table expansion done\\n\");\n                    }\n\n            } else {\n                usleep(10*1000 - 1);\n            }\n\n            if (item_lock) {\n                item_trylock_unlock(item_lock);\n                item_lock = NULL;\n            }\n        }\n\n        if (!expanding) {\n            /* We are done expanding.. just wait for next invocation */\n            pthread_cond_wait(&maintenance_cond, &maintenance_lock);\n            /* assoc_expand() swaps out the hash table entirely, so we need\n             * all threads to not hold any references related to the hash\n             * table while this happens.\n             * This is instead of a more complex, possibly slower algorithm to\n             * allow dynamic hash table expansion without causing significant\n             * wait times.\n             */\n            if (do_run_maintenance_thread) {\n                pause_threads(PAUSE_ALL_THREADS);\n                assoc_expand();\n                pause_threads(RESUME_ALL_THREADS);\n            }\n        }\n    }\n    mutex_unlock(&maintenance_lock);\n    return NULL;\n}\n\nstatic pthread_t maintenance_tid;\n\nint start_assoc_maintenance_thread(void) {\n    int ret;\n    char *env = getenv(\"MEMCACHED_HASH_BULK_MOVE\");\n    if (env != NULL) {\n        hash_bulk_move = atoi(env);\n        if (hash_bulk_move == 0) {\n            hash_bulk_move = DEFAULT_HASH_BULK_MOVE;\n        }\n    }\n\n    if ((ret = pthread_create(&maintenance_tid, NULL,\n                              assoc_maintenance_thread, NULL)) != 0) {\n        fprintf(stderr, \"Can't create thread: %s\\n\", strerror(ret));\n        return -1;\n    }\n    thread_setname(maintenance_tid, \"mc-assocmaint\");\n    return 0;\n}\n\nvoid stop_assoc_maintenance_thread(void) {\n    mutex_lock(&maintenance_lock);\n    do_run_maintenance_thread = 0;\n    pthread_cond_signal(&maintenance_cond);\n    mutex_unlock(&maintenance_lock);\n\n    /* Wait for the maintenance thread to stop */\n    pthread_join(maintenance_tid, NULL);\n}\n\nstruct assoc_iterator {\n    uint64_t bucket;\n    item *it;\n    item *next;\n    bool bucket_locked;\n};\n\nvoid *assoc_get_iterator(void) {\n    struct assoc_iterator *iter = calloc(1, sizeof(struct assoc_iterator));\n    if (iter == NULL) {\n        return NULL;\n    }\n    // this will hang the caller while a hash table expansion is running.\n    if (mutex_trylock(&maintenance_lock) == 0) {\n        return iter;\n    } else {\n        return NULL;\n    }\n}\n\nbool assoc_iterate(void *iterp, item **it) {\n    struct assoc_iterator *iter = (struct assoc_iterator *) iterp;\n    *it = NULL;\n    // - if locked bucket and next, update next and return\n    if (iter->bucket_locked) {\n        if (iter->next != NULL) {\n            iter->it = iter->next;\n            iter->next = iter->it->h_next;\n            *it = iter->it;\n        } else {\n            // unlock previous bucket, if any\n            item_unlock(iter->bucket);\n            // iterate the bucket post since it starts at 0.\n            iter->bucket++;\n            iter->bucket_locked = false;\n            *it = NULL;\n        }\n        return true;\n    }\n\n    // - loop until we hit the end or find something.\n    if (iter->bucket != hashsize(hashpower)) {\n        // - lock next bucket\n        item_lock(iter->bucket);\n        iter->bucket_locked = true;\n        // - only check the primary hash table since expand is blocked.\n        iter->it = primary_hashtable[iter->bucket];\n        if (iter->it != NULL) {\n            // - set it, next and return\n            iter->next = iter->it->h_next;\n            *it = iter->it;\n        } else {\n            // - nothing found in this bucket, try next.\n            item_unlock(iter->bucket);\n            iter->bucket_locked = false;\n            iter->bucket++;\n        }\n    } else {\n        return false;\n    }\n\n    return true;\n}\n\nvoid assoc_iterate_final(void *iterp) {\n    struct assoc_iterator *iter = (struct assoc_iterator *) iterp;\n    if (iter->bucket_locked) {\n        item_unlock(iter->bucket);\n    }\n    mutex_unlock(&maintenance_lock);\n    free(iter);\n}\n"
        },
        {
          "name": "assoc.h",
          "type": "blob",
          "size": 0.55078125,
          "content": "/* associative array */\nvoid assoc_init(const int hashpower_init);\n\nitem *assoc_find(const char *key, const size_t nkey, const uint32_t hv);\nint assoc_insert(item *item, const uint32_t hv);\nvoid assoc_delete(const char *key, const size_t nkey, const uint32_t hv);\n\nint start_assoc_maintenance_thread(void);\nvoid stop_assoc_maintenance_thread(void);\nvoid assoc_start_expand(uint64_t curr_items);\n\n/* walk functions */\nvoid *assoc_get_iterator(void);\nbool assoc_iterate(void *iterp, item **it);\nvoid assoc_iterate_final(void *iterp);\n\nextern unsigned int hashpower;\n"
        },
        {
          "name": "authfile.c",
          "type": "blob",
          "size": 3.498046875,
          "content": "/* -*- Mode: C; tab-width: 4; c-basic-offset: 4; indent-tabs-mode: nil -*- */\n#include <stdio.h>\n#include <stdlib.h>\n#include <stdbool.h>\n#include <string.h>\n#include <sys/types.h>\n#include <sys/stat.h>\n#include <unistd.h>\n#include <inttypes.h>\n\n#include \"authfile.h\"\n#include \"util.h\"\n\n// TODO: frontend needs a refactor so this can avoid global objects.\n\n#define MAX_ENTRY_LEN 256\n// Not supposed to be a huge database!\n#define MAX_ENTRIES 8\n\ntypedef struct auth_entry {\n    char *user;\n    size_t ulen;\n    char *pass;\n    size_t plen;\n} auth_t;\n\nauth_t main_auth_entries[MAX_ENTRIES];\nint entry_cnt = 0;\nchar *main_auth_data = NULL;\n\nenum authfile_ret authfile_load(const char *file) {\n    struct stat sb;\n    char *auth_data = NULL;\n    auth_t auth_entries[MAX_ENTRIES];\n\n    FILE *pwfile = fopen(file, \"r\");\n    if (pwfile == NULL) {\n        return AUTHFILE_OPENFAIL;\n    } else if (fstat(fileno(pwfile), &sb)) {\n        fclose(pwfile);\n        return AUTHFILE_STATFAIL;\n    }\n\n    auth_data = calloc(1, sb.st_size + 2);\n\n    char *auth_cur = auth_data;\n    // fgets will stop at EOF or a newline, reading at most one bytes less\n    // than the size limit. If a user supplies a file without an ending\n    // newline we will end up chopping the last character of the password.\n    char *auth_end = auth_data + sb.st_size + 1;\n    auth_t *entry_cur = auth_entries;\n    int used = 0;\n\n    while ((fgets(auth_cur, auth_end - auth_cur < MAX_ENTRY_LEN ? auth_end - auth_cur : MAX_ENTRY_LEN, pwfile)) != NULL) {\n        int x;\n        int found = 0;\n\n        for (x = 0; x < MAX_ENTRY_LEN; x++) {\n            if (!found) {\n                if (auth_cur[x] == '\\0') {\n                    // The username is malformed - this is either the end of the file or a null byte.\n                    break;\n                } else if (auth_cur[x] == ':') {\n                    entry_cur->user = auth_cur;\n                    entry_cur->ulen = x;\n                    entry_cur->pass = &auth_cur[x+1];\n                    found = 1;\n                }\n            } else {\n                // Find end of password.\n                if (auth_cur[x] == '\\n' ||\n                    auth_cur[x] == '\\r' ||\n                    auth_cur[x] == '\\0') {\n                    entry_cur->plen = x - (entry_cur->ulen + 1);\n                    break;\n                }\n            }\n        }\n\n        // malformed line.\n        if (!found) {\n            (void)fclose(pwfile);\n            free(auth_data);\n            return AUTHFILE_MALFORMED;\n        }\n\n        // FIXME: no silent truncation.\n        if (++used == MAX_ENTRIES) {\n            break;\n        }\n        // EOF\n        if (auth_cur[x] == '\\0')\n            break;\n\n        auth_cur += x;\n        entry_cur++;\n    }\n\n    // swap the main pointer out now, so if there's an error reloading we\n    // don't break the existing authentication.\n    if (main_auth_data != NULL) {\n        free(main_auth_data);\n    }\n\n    entry_cnt = used;\n    main_auth_data = auth_data;\n    memcpy(main_auth_entries, auth_entries, sizeof(auth_entries));\n\n    (void)fclose(pwfile);\n\n    return AUTHFILE_OK;\n}\n\n// if only loading the file could be this short...\nint authfile_check(const char *user, const char *pass) {\n    size_t ulen = strlen(user);\n    size_t plen = strlen(pass);\n\n    for (int x = 0; x < entry_cnt; x++) {\n        auth_t *e = &main_auth_entries[x];\n        if (ulen == e->ulen && plen == e->plen &&\n            safe_memcmp(user, e->user, e->ulen) &&\n            safe_memcmp(pass, e->pass, e->plen)) {\n            return 1;\n        }\n    }\n\n    return 0;\n}\n"
        },
        {
          "name": "authfile.h",
          "type": "blob",
          "size": 0.3662109375,
          "content": "#ifndef AUTHFILE_H\n#define AUTHFILE_H\n\nenum authfile_ret {\n    AUTHFILE_OK = 0,\n    AUTHFILE_OOM,\n    AUTHFILE_STATFAIL, // not likely, but just to be sure\n    AUTHFILE_OPENFAIL,\n    AUTHFILE_MALFORMED,\n};\n\n// FIXME: mc_authfile or something?\nenum authfile_ret authfile_load(const char *file);\nint authfile_check(const char *user, const char *pass);\n\n#endif /* AUTHFILE_H */\n"
        },
        {
          "name": "autogen.sh",
          "type": "blob",
          "size": 1.154296875,
          "content": "#!/bin/sh\n\n# Get the initial version.\nperl version.pl\n\ndie() {\n    echo \"$@\"\n    exit 1\n}\n\n# Try to locate a program by using which, and verify that the file is an\n# executable\nlocate_binary() {\n  for f in $@\n  do\n    file=`which $f 2>/dev/null | grep -v '^no '`\n    if test -n \"$file\" -a -x \"$file\"; then\n      echo $file\n      return 0\n    fi\n  done\n\n  echo \"\"\n  return 1\n}\n\necho \"aclocal...\"\nif test x$ACLOCAL = x; then\n  ACLOCAL=`locate_binary aclocal-1.14 aclocal-1.13 aclocal-1.12 aclocal-1.11 aclocal-1.10 aclocal-1.9 aclocal19 aclocal-1.7 aclocal17 aclocal-1.5 aclocal15 aclocal`\n  if test x$ACLOCAL = x; then\n    die \"Did not find a supported aclocal\"\n  fi\nfi\n$ACLOCAL || exit 1\n\necho \"autoheader...\"\nAUTOHEADER=${AUTOHEADER:-autoheader}\n$AUTOHEADER || exit 1\n\necho \"automake...\"\nif test x$AUTOMAKE = x; then\n  AUTOMAKE=`locate_binary automake-1.14 automake-1.13 automake-1.12 automake-1.11 automake-1.10 automake-1.9 automake-1.7 automake`\n  if test x$AUTOMAKE = x; then\n    die \"Did not find a supported automake\"\n  fi\nfi\n$AUTOMAKE --foreign --add-missing || $AUTOMAKE --gnu --add-missing || exit 1\n\necho \"autoconf...\"\nAUTOCONF=${AUTOCONF:-autoconf}\n$AUTOCONF || exit 1\n\n"
        },
        {
          "name": "base64.c",
          "type": "blob",
          "size": 6.576171875,
          "content": "/*\n * Base64 encoding/decoding (RFC1341)\n * Copyright (c) 2005-2011, Jouni Malinen <j@w1.fi>\n * Modified by Dormando\n *\n * This software may be distributed under the terms of the BSD license.\n * Original license included below:\n *\nLicense\n-------\n\nThis software may be distributed, used, and modified under the terms of\nBSD license:\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are\nmet:\n\n1. Redistributions of source code must retain the above copyright\n   notice, this list of conditions and the following disclaimer.\n\n2. Redistributions in binary form must reproduce the above copyright\n   notice, this list of conditions and the following disclaimer in the\n   documentation and/or other materials provided with the distribution.\n\n3. Neither the name(s) of the above-listed copyright holder(s) nor the\n   names of its contributors may be used to endorse or promote products\n   derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n\"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\nLIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\nA PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\nOWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\nSPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\nLIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\nDATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\nTHEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n */\n\n/* Changes from original code:\n * - decode table is pre-generated\n * - no line splitting on encoder\n * - output buffers are passed in instead of malloc'ed\n * - returns encoded/decoded length instead of pointer.\n */\n\n#include <stddef.h>\n#include \"base64.h\"\n\nstatic const unsigned char base64_table[65] =\n    \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/\";\n\n/* Original decode function generated the table every time. I used the code to\n * print this table and pre-generate it instead.\n */\nstatic const unsigned char dtable[256] = {\n128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,\n128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,\n128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,\n62, 128, 128, 128, 63, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 128, 128, 128, 0, 128, 128, 128, 0, 1, 2, 3, 4, 5,\n6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,\n20, 21, 22, 23, 24, 25, 128, 128, 128, 128, 128, 128, 26, 27,\n28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41,\n42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 128, 128, 128, 128,\n128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,\n128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,\n128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,\n128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,\n128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,\n128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,\n128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,\n128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,\n128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,\n128, 128, 128\n};\n\n/**\n * base64_encode - Base64 encode\n * @src: Data to be encoded\n * @len: Length of the data to be encoded\n * @out: output uffer\n * @out_len: length of output buffer\n * Returns: Number of actual bytes encoded into the buffer\n * or 0 on failure\n *\n * Output buffer is nul terminated to make it easier to use as a C string.\n * The nul terminator is * not included in the return length.\n */\nsize_t base64_encode(const unsigned char *src, size_t len,\n                  unsigned char *out, size_t out_len)\n{\n    unsigned char *pos;\n    const unsigned char *end, *in;\n    size_t olen;\n\n    olen = len * 4 / 3 + 4; /* 3-byte blocks to 4-byte */\n    olen += olen / 72; /* line feeds */\n    olen++; /* nul termination */\n    if (olen < len) {\n        return 0; /* integer overflow */\n    }\n    if (olen > out_len) {\n        return 0; /* not enough space in output buffer */\n    }\n    if (out == NULL) {\n        return 0;\n    }\n\n    end = src + len;\n    in = src;\n    pos = out;\n    while (end - in >= 3) {\n        *pos++ = base64_table[in[0] >> 2];\n        *pos++ = base64_table[((in[0] & 0x03) << 4) | (in[1] >> 4)];\n        *pos++ = base64_table[((in[1] & 0x0f) << 2) | (in[2] >> 6)];\n        *pos++ = base64_table[in[2] & 0x3f];\n        in += 3;\n    }\n\n    if (end - in) {\n        *pos++ = base64_table[in[0] >> 2];\n        if (end - in == 1) {\n            *pos++ = base64_table[(in[0] & 0x03) << 4];\n            *pos++ = '=';\n        } else {\n            *pos++ = base64_table[((in[0] & 0x03) << 4) |\n                          (in[1] >> 4)];\n            *pos++ = base64_table[(in[1] & 0x0f) << 2];\n        }\n        *pos++ = '=';\n    }\n\n    *pos = '\\0';\n    return pos - out;\n}\n\n\n/**\n * base64_decode - Base64 decode\n * @src: Data to be decoded\n * @len: Length of the data to be decoded\n * @out: Output buffer to decode into\n * @out_len: Length of output buffer\n * Returns: Length of encoded data, or 0 on failure\n */\nsize_t base64_decode(const unsigned char *src, size_t len,\n                  unsigned char *out, size_t out_len)\n{\n    unsigned char *pos, block[4], tmp;\n    size_t i, count, olen;\n    int pad = 0;\n\n    count = 0;\n    for (i = 0; i < len; i++) {\n        if (dtable[src[i]] != 0x80)\n            count++;\n    }\n\n    if (count == 0 || count % 4)\n        return 0;\n\n    olen = count / 4 * 3;\n    if (olen > out_len) {\n        return 0;\n    }\n    pos = out;\n    if (out == NULL) {\n        return 0;\n    }\n\n    count = 0;\n    for (i = 0; i < len; i++) {\n        tmp = dtable[src[i]];\n        if (tmp == 0x80)\n            continue;\n\n        if (src[i] == '=')\n            pad++;\n        block[count] = tmp;\n        count++;\n        if (count == 4) {\n            *pos++ = (block[0] << 2) | (block[1] >> 4);\n            *pos++ = (block[1] << 4) | (block[2] >> 2);\n            *pos++ = (block[2] << 6) | block[3];\n            count = 0;\n            if (pad) {\n                if (pad == 1)\n                    pos--;\n                else if (pad == 2)\n                    pos -= 2;\n                else {\n                    /* Invalid padding */\n                    return 0;\n                }\n                break;\n            }\n        }\n    }\n\n    return pos - out;\n}\n"
        },
        {
          "name": "base64.h",
          "type": "blob",
          "size": 0.4755859375,
          "content": "/*\n * Base64 encoding/decoding (RFC1341)\n * Copyright (c) 2005, Jouni Malinen <j@w1.fi>\n *\n * This software may be distributed under the terms of the BSD license.\n * See base64.c for more details\n */\n\n#ifndef BASE64_H\n#define BASE64_H\n\nsize_t base64_encode(const unsigned char *src, size_t len,\n                  unsigned char *out, size_t out_len);\nsize_t base64_decode(const unsigned char *src, size_t len,\n                  unsigned char *out, size_t out_len);\n\n#endif /* BASE64_H */\n"
        },
        {
          "name": "bipbuffer.c",
          "type": "blob",
          "size": 3.67578125,
          "content": "/**\n * Copyright (c) 2011, Willem-Hendrik Thiart\n * Use of this source code is governed by a BSD-style license that can be\n * found in the LICENSE.bipbuffer file.\n *\n * @file\n * @author  Willem Thiart himself@willemthiart.com\n */\n\n#include \"stdio.h\"\n#include <stdlib.h>\n\n/* for memcpy */\n#include <string.h>\n\n#include \"bipbuffer.h\"\n\nstatic size_t bipbuf_sizeof(const unsigned int size)\n{\n    return sizeof(bipbuf_t) + size;\n}\n\nint bipbuf_unused(const bipbuf_t* me)\n{\n    if (1 == me->b_inuse)\n        /* distance between region B and region A */\n        return me->a_start - me->b_end;\n    else\n        return me->size - me->a_end;\n}\n\nint bipbuf_size(const bipbuf_t* me)\n{\n    return me->size;\n}\n\nint bipbuf_used(const bipbuf_t* me)\n{\n    return (me->a_end - me->a_start) + me->b_end;\n}\n\nvoid bipbuf_init(bipbuf_t* me, const unsigned int size)\n{\n    me->a_start = me->a_end = me->b_end = 0;\n    me->size = size;\n    me->b_inuse = 0;\n}\n\nbipbuf_t *bipbuf_new(const unsigned int size)\n{\n    bipbuf_t *me = malloc(bipbuf_sizeof(size));\n    if (!me)\n        return NULL;\n    bipbuf_init(me, size);\n    return me;\n}\n\nvoid bipbuf_free(bipbuf_t* me)\n{\n    free(me);\n}\n\nint bipbuf_is_empty(const bipbuf_t* me)\n{\n    return me->a_start == me->a_end;\n}\n\n/* find out if we should turn on region B\n * ie. is the distance from A to buffer's end less than B to A? */\nstatic void __check_for_switch_to_b(bipbuf_t* me)\n{\n    if (me->size - me->a_end < me->a_start - me->b_end)\n        me->b_inuse = 1;\n}\n\n/* TODO: DOCUMENT THESE TWO FUNCTIONS */\nunsigned char *bipbuf_request(bipbuf_t* me, const int size)\n{\n    if (bipbuf_unused(me) < size)\n        return 0;\n    if (1 == me->b_inuse)\n    {\n        return (unsigned char *)me->data + me->b_end;\n    }\n    else\n    {\n        return (unsigned char *)me->data + me->a_end;\n    }\n}\n\nint bipbuf_push(bipbuf_t* me, const int size)\n{\n    if (bipbuf_unused(me) < size)\n        return 0;\n\n    if (1 == me->b_inuse)\n    {\n        me->b_end += size;\n    }\n    else\n    {\n        me->a_end += size;\n    }\n\n    __check_for_switch_to_b(me);\n    return size;\n}\n\nint bipbuf_offer(bipbuf_t* me, const unsigned char *data, const int size)\n{\n    /* not enough space */\n    if (bipbuf_unused(me) < size)\n        return 0;\n\n    if (1 == me->b_inuse)\n    {\n        memcpy(me->data + me->b_end, data, size);\n        me->b_end += size;\n    }\n    else\n    {\n        memcpy(me->data + me->a_end, data, size);\n        me->a_end += size;\n    }\n\n    __check_for_switch_to_b(me);\n    return size;\n}\n\nunsigned char *bipbuf_peek(const bipbuf_t* me, const unsigned int size)\n{\n    /* make sure we can actually peek at this data */\n    if (me->size < me->a_start + size)\n        return NULL;\n\n    if (bipbuf_is_empty(me))\n        return NULL;\n\n    return (unsigned char *)me->data + me->a_start;\n}\n\nunsigned char *bipbuf_peek_all(const bipbuf_t* me, unsigned int *size)\n{\n    if (bipbuf_is_empty(me))\n        return NULL;\n\n    *size = me->a_end - me->a_start;\n    return (unsigned char*)me->data + me->a_start;\n}\n\nunsigned char *bipbuf_poll(bipbuf_t* me, const unsigned int size)\n{\n    if (bipbuf_is_empty(me))\n        return NULL;\n\n    /* make sure we can actually poll this data */\n    if (me->size < me->a_start + size)\n        return NULL;\n\n    void *end = me->data + me->a_start;\n    me->a_start += size;\n\n    /* we seem to be empty.. */\n    if (me->a_start == me->a_end)\n    {\n        /* replace a with region b */\n        if (1 == me->b_inuse)\n        {\n            me->a_start = 0;\n            me->a_end = me->b_end;\n            me->b_end = me->b_inuse = 0;\n        }\n        else\n            /* safely move cursor back to the start because we are empty */\n            me->a_start = me->a_end = 0;\n    }\n\n    __check_for_switch_to_b(me);\n    return end;\n}\n"
        },
        {
          "name": "bipbuffer.h",
          "type": "blob",
          "size": 2.068359375,
          "content": "#ifndef BIPBUFFER_H\n#define BIPBUFFER_H\n\ntypedef struct\n{\n    unsigned long int size;\n\n    /* region A */\n    unsigned int a_start, a_end;\n\n    /* region B */\n    unsigned int b_end;\n\n    /* is B inuse? */\n    int b_inuse;\n\n    unsigned char data[];\n} bipbuf_t;\n\n/**\n * Create a new bip buffer.\n *\n * malloc()s space\n *\n * @param[in] size The size of the buffer */\nbipbuf_t *bipbuf_new(const unsigned int size);\n\n/**\n * Initialise a bip buffer. Use memory provided by user.\n *\n * No malloc()s are performed.\n *\n * @param[in] size The size of the array */\nvoid bipbuf_init(bipbuf_t* me, const unsigned int size);\n\n/**\n * Free the bip buffer */\nvoid bipbuf_free(bipbuf_t *me);\n\n/* TODO: DOCUMENTATION */\nunsigned char *bipbuf_request(bipbuf_t* me, const int size);\nint bipbuf_push(bipbuf_t* me, const int size);\n\n/**\n * @param[in] data The data to be offered to the buffer\n * @param[in] size The size of the data to be offered\n * @return number of bytes offered */\nint bipbuf_offer(bipbuf_t *me, const unsigned char *data, const int size);\n\n/**\n * Look at data. Don't move cursor\n *\n * @param[in] len The length of the data to be peeked\n * @return data on success, NULL if we can't peek at this much data */\nunsigned char *bipbuf_peek(const bipbuf_t* me, const unsigned int len);\n\n/**\n * Look at data. Don't move cursor\n *\n * @param[in] len The length of the data returned\n * @return data on success, NULL if nothing available */\nunsigned char *bipbuf_peek_all(const bipbuf_t* me, unsigned int *len);\n\n/**\n * Get pointer to data to read. Move the cursor on.\n *\n * @param[in] len The length of the data to be polled\n * @return pointer to data, NULL if we can't poll this much data */\nunsigned char *bipbuf_poll(bipbuf_t* me, const unsigned int size);\n\n/**\n * @return the size of the bipbuffer */\nint bipbuf_size(const bipbuf_t* me);\n\n/**\n * @return 1 if buffer is empty; 0 otherwise */\nint bipbuf_is_empty(const bipbuf_t* me);\n\n/**\n * @return how much space we have assigned */\nint bipbuf_used(const bipbuf_t* cb);\n\n/**\n * @return bytes of unused space */\nint bipbuf_unused(const bipbuf_t* me);\n\n#endif /* BIPBUFFER_H */\n"
        },
        {
          "name": "cache.c",
          "type": "blob",
          "size": 3.287109375,
          "content": "/* -*- Mode: C; tab-width: 4; c-basic-offset: 4; indent-tabs-mode: nil -*- */\n#include <stdlib.h>\n#include <string.h>\n#include <inttypes.h>\n#include <assert.h>\n\n#ifndef NDEBUG\n#include <signal.h>\n#endif\n\n#include \"cache.h\"\n\n#ifndef NDEBUG\nconst uint64_t redzone_pattern = 0xdeadbeefcafedeed;\nint cache_error = 0;\n#endif\n\ncache_t* cache_create(const char *name, size_t bufsize, size_t align) {\n    cache_t* ret = calloc(1, sizeof(cache_t));\n    char* nm = strdup(name);\n    if (ret == NULL || nm == NULL ||\n        pthread_mutex_init(&ret->mutex, NULL) == -1) {\n        free(ret);\n        free(nm);\n        return NULL;\n    }\n\n    ret->name = nm;\n    STAILQ_INIT(&ret->head);\n\n#ifndef NDEBUG\n    ret->bufsize = bufsize + 2 * sizeof(redzone_pattern);\n#else\n    ret->bufsize = bufsize;\n#endif\n    assert(ret->bufsize >= sizeof(struct cache_free_s));\n\n    return ret;\n}\n\nvoid cache_set_limit(cache_t *cache, int limit) {\n    pthread_mutex_lock(&cache->mutex);\n    cache->limit = limit;\n    pthread_mutex_unlock(&cache->mutex);\n}\n\nstatic inline void* get_object(void *ptr) {\n#ifndef NDEBUG\n    uint64_t *pre = ptr;\n    return pre + 1;\n#else\n    return ptr;\n#endif\n}\n\nvoid cache_destroy(cache_t *cache) {\n    while (!STAILQ_EMPTY(&cache->head)) {\n        struct cache_free_s *o = STAILQ_FIRST(&cache->head);\n        STAILQ_REMOVE_HEAD(&cache->head, c_next);\n        free(o);\n    }\n    free(cache->name);\n    pthread_mutex_destroy(&cache->mutex);\n    free(cache);\n}\n\nvoid* cache_alloc(cache_t *cache) {\n    void *ret;\n    pthread_mutex_lock(&cache->mutex);\n    ret = do_cache_alloc(cache);\n    pthread_mutex_unlock(&cache->mutex);\n    return ret;\n}\n\nvoid* do_cache_alloc(cache_t *cache) {\n    void *ret;\n    void *object;\n    if (cache->freecurr > 0) {\n        ret = STAILQ_FIRST(&cache->head);\n        STAILQ_REMOVE_HEAD(&cache->head, c_next);\n        object = get_object(ret);\n        cache->freecurr--;\n    } else if (cache->limit == 0 || cache->total < cache->limit) {\n        object = ret = malloc(cache->bufsize);\n        if (ret != NULL) {\n            object = get_object(ret);\n\n            cache->total++;\n        }\n    } else {\n        object = NULL;\n    }\n\n#ifndef NDEBUG\n    if (object != NULL) {\n        /* add a simple form of buffer-check */\n        uint64_t *pre = ret;\n        *pre = redzone_pattern;\n        ret = pre+1;\n        memcpy(((char*)ret) + cache->bufsize - (2 * sizeof(redzone_pattern)),\n               &redzone_pattern, sizeof(redzone_pattern));\n    }\n#endif\n\n    return object;\n}\n\nvoid cache_free(cache_t *cache, void *ptr) {\n    pthread_mutex_lock(&cache->mutex);\n    do_cache_free(cache, ptr);\n    pthread_mutex_unlock(&cache->mutex);\n}\n\nvoid do_cache_free(cache_t *cache, void *ptr) {\n#ifndef NDEBUG\n    /* validate redzone... */\n    if (memcmp(((char*)ptr) + cache->bufsize - (2 * sizeof(redzone_pattern)),\n               &redzone_pattern, sizeof(redzone_pattern)) != 0) {\n        raise(SIGABRT);\n        cache_error = 1;\n        return;\n    }\n    uint64_t *pre = ptr;\n    --pre;\n    if (*pre != redzone_pattern) {\n        raise(SIGABRT);\n        cache_error = -1;\n        return;\n    }\n    ptr = pre;\n#endif\n    if (cache->limit != 0 && cache->limit < cache->total) {\n        free(ptr);\n        cache->total--;\n    } else {\n        STAILQ_INSERT_HEAD(&cache->head, (struct cache_free_s *)ptr, c_next);\n        cache->freecurr++;\n    }\n}\n\n"
        },
        {
          "name": "cache.h",
          "type": "blob",
          "size": 3.3974609375,
          "content": "/* -*- Mode: C; tab-width: 4; c-basic-offset: 4; indent-tabs-mode: nil -*- */\n#ifndef CACHE_H\n#define CACHE_H\n#include <pthread.h>\n#include \"queue.h\"\n\n#ifndef NDEBUG\n/* may be used for debug purposes */\nextern int cache_error;\n#endif\n\nstruct cache_free_s {\n    STAILQ_ENTRY(cache_free_s) c_next;\n};\n\n//typedef STAILQ_HEAD(cache_head_s, cache_free_s) cache_head_t;\n/**\n * Definition of the structure to keep track of the internal details of\n * the cache allocator. Touching any of these variables results in\n * undefined behavior.\n */\ntypedef struct {\n    /** Mutex to protect access to the structure */\n    pthread_mutex_t mutex;\n    /** Name of the cache objects in this cache (provided by the caller) */\n    char *name;\n    /** freelist of available buffers */\n    STAILQ_HEAD(cache_head, cache_free_s) head;\n    /** The size of each element in this cache */\n    size_t bufsize;\n    /** The capacity of the list of elements */\n    int freetotal;\n    /** Total malloc'ed objects */\n    int total;\n    /** The current number of free elements */\n    int freecurr;\n    /** A limit on the total number of elements */\n    int limit;\n} cache_t;\n\n/**\n * Create an object cache.\n *\n * The object cache will let you allocate objects of the same size. It is fully\n * MT safe, so you may allocate objects from multiple threads without having to\n * do any synchronization in the application code.\n *\n * @param name the name of the object cache. This name may be used for debug purposes\n *             and may help you track down what kind of object you have problems with\n *             (buffer overruns, leakage etc)\n * @param bufsize the size of each object in the cache\n * @param align the alignment requirements of the objects in the cache.\n * @param constructor the function to be called to initialize memory when we need\n *                    to allocate more memory from the os.\n * @param destructor the function to be called before we release the memory back\n *                   to the os.\n * @return a handle to an object cache if successful, NULL otherwise.\n */\ncache_t* cache_create(const char* name, size_t bufsize, size_t align);\n\n/**\n * Destroy an object cache.\n *\n * Destroy and invalidate an object cache. You should return all buffers allocated\n * with cache_alloc by using cache_free before calling this function. Not doing\n * so results in undefined behavior (the buffers may or may not be invalidated)\n *\n * @param handle the handle to the object cache to destroy.\n */\nvoid cache_destroy(cache_t* handle);\n/**\n * Allocate an object from the cache.\n *\n * @param handle the handle to the object cache to allocate from\n * @return a pointer to an initialized object from the cache, or NULL if\n *         the allocation cannot be satisfied.\n */\nvoid* cache_alloc(cache_t* handle);\nvoid* do_cache_alloc(cache_t* handle);\n/**\n * Return an object back to the cache.\n *\n * The caller should return the object in an initialized state so that\n * the object may be returned in an expected state from cache_alloc.\n *\n * @param handle handle to the object cache to return the object to\n * @param ptr pointer to the object to return.\n */\nvoid cache_free(cache_t* handle, void* ptr);\nvoid do_cache_free(cache_t* handle, void* ptr);\n/**\n * Set or adjust a limit for the number of objects to malloc\n *\n * @param handle handle to the object cache to adjust\n * @param limit the number of objects to cache before returning NULL\n */\nvoid cache_set_limit(cache_t* handle, int limit);\n\n#endif\n"
        },
        {
          "name": "configure.ac",
          "type": "blob",
          "size": 26.4892578125,
          "content": "AC_PREREQ(2.52)\nm4_include([version.m4])\nm4_include([m4/c99-backport.m4])\nAC_INIT([memcached], [VERSION_NUMBER], [memcached@googlegroups.com])\nAC_CANONICAL_HOST\nAC_CONFIG_SRCDIR([memcached.c])\nAM_INIT_AUTOMAKE([foreign])\nAM_CONFIG_HEADER([config.h])\n\nAC_PROG_CC\n\nis_darwin=no\n\ncase \"${host_os}\" in\n   darwin*)\n   is_darwin=yes\n   ;;\nesac\n\nAM_CONDITIONAL([DARWIN], [test \"$is_darwin\" = \"yes\"])\n\ndnl **********************************************************************\ndnl DETECT_ICC ([ACTION-IF-YES], [ACTION-IF-NO])\ndnl\ndnl check if this is the Intel ICC compiler, and if so run the ACTION-IF-YES\ndnl sets the $ICC variable to \"yes\" or \"no\"\ndnl **********************************************************************\nAC_DEFUN([DETECT_ICC],\n[\n    ICC=\"no\"\n    AC_MSG_CHECKING([for icc in use])\n    if test \"$GCC\" = \"yes\"; then\n       dnl check if this is icc acting as gcc in disguise\n       AC_EGREP_CPP([^__INTEL_COMPILER], [__INTEL_COMPILER],\n         AC_MSG_RESULT([no])\n         [$2],\n         AC_MSG_RESULT([yes])\n         [$1]\n         ICC=\"yes\")\n    else\n       AC_MSG_RESULT([no])\n       [$2]\n    fi\n])\n\nDETECT_ICC([], [])\n\ndnl **********************************************************************\ndnl DETECT_CLANG ([ACTION-IF-YES], [ACTION-IF-NO])\ndnl\ndnl check if compiler is clang, and if so run the ACTION-IF-YES sets the\ndnl $CLANG variable to \"yes\" or \"no\"\ndnl **********************************************************************\nAC_DEFUN([DETECT_CLANG],\n[\n    AC_MSG_CHECKING([for clang in use])\n    AC_COMPILE_IFELSE(\n    [AC_LANG_PROGRAM([], [[\n    #ifndef __clang__\n           not clang\n    #endif\n    ]])],\n    [CLANG=yes], [CLANG=no])\n    AC_MSG_RESULT([$CLANG])\n    AS_IF([test \"$CLANG\" = \"yes\"],[$1],[$2])\n])\nDETECT_CLANG([],[])\n\ndnl **********************************************************************\ndnl DETECT_SUNCC ([ACTION-IF-YES], [ACTION-IF-NO])\ndnl\ndnl check if this is the Sun Studio compiler, and if so run the ACTION-IF-YES\ndnl sets the $SUNCC variable to \"yes\" or \"no\"\ndnl **********************************************************************\nAC_DEFUN([DETECT_SUNCC],\n[\n    AC_CHECK_DECL([__SUNPRO_C], [SUNCC=\"yes\"], [SUNCC=\"no\"])\n    AS_IF(test \"x$SUNCC\" = \"xyes\", [$1], [$2])\n\n])\n\nDETECT_SUNCC([CFLAGS=\"-mt $CFLAGS\"], [])\nAS_IF([test \"$ICC\" = \"yes\" -o \"$GCC\" = \"yes\"],\n[\n    AS_IF(test \"$CLANG\" = \"no\",[CFLAGS=\"$CFLAGS -pthread\"])\n])\n\ndnl clang will error .arch_extension crc32 assembler directives to allow\ndnl assembling crc instructions without this\nAS_IF(test \"$CLANG\" = \"yes\",[CFLAGS=\"$CFLAGS -Wno-language-extension-token\"])\n\nif test \"$ICC\" = \"no\"; then\n   AC_PROG_CC_C99\nfi\n\nAM_PROG_CC_C_O\nAC_PROG_INSTALL\n\nAC_ARG_ENABLE(extstore,\n  [AS_HELP_STRING([--disable-extstore], [Disable external storage (extstore)])])\n\nAC_ARG_ENABLE(seccomp,\n  [AS_HELP_STRING([--enable-seccomp],[Enable seccomp restrictions EXPERIMENTAL])])\n\nAC_ARG_ENABLE(sasl,\n  [AS_HELP_STRING([--enable-sasl],[Enable SASL authentication])])\n\nAC_ARG_ENABLE(sasl_pwdb,\n  [AS_HELP_STRING([--enable-sasl-pwdb],[Enable plaintext password db])])\n\nAS_IF([test \"x$enable_sasl_pwdb\" = \"xyes\"],\n      [enable_sasl=yes ])\n\nAC_ARG_ENABLE(tls,\n  [AS_HELP_STRING([--enable-tls], [Enable Transport Layer Security EXPERIMENTAL ])])\n\n\nAC_ARG_ENABLE(asan,\n  [AS_HELP_STRING([--enable-asan], [Compile with ASAN EXPERIMENTAL ])])\n\nAC_ARG_ENABLE(static,\n  [AS_HELP_STRING([--enable-static], [Compile a statically linked binary])])\n\nAC_ARG_ENABLE(unix_socket,\n  [AS_HELP_STRING([--disable-unix-socket], [Disable unix domain socket])])\n\nAC_ARG_ENABLE(proxy,\n  [AS_HELP_STRING([--enable-proxy], [Enable proxy code EXPERIMENTAL])])\n\nAC_ARG_ENABLE(proxy-uring,\n  [AS_HELP_STRING([--enable-proxy-uring], [Enable proxy io_uring code EXPERIMENTAL])])\n\nAC_ARG_ENABLE(proxy-tls,\n  [AS_HELP_STRING([--enable-proxy-tls], [Enable proxy io_uring code EXPERIMENTAL])])\n\nAC_ARG_ENABLE(werror,\n  [AS_HELP_STRING([--enable-werror], [Enable -Werror])])\n\nAC_ARG_ENABLE(large-client-flags,\n  [AS_HELP_STRING([--enable-large-client-flags], [Change client flags from 32bit to 64bit EXPERIMENTAL])])\n\ndnl **********************************************************************\ndnl DETECT_SASL_CB_GETCONF\ndnl\ndnl check if we can use SASL_CB_GETCONF\ndnl **********************************************************************\nAC_DEFUN([AC_C_DETECT_SASL_CB_GETCONF],\n[\n    AC_CACHE_CHECK([for SASL_CB_GETCONF],\n        [ac_cv_c_sasl_cb_getconf],\n        [AC_TRY_COMPILE(\n            [\n#include <sasl/sasl.h>\n            ], [\nunsigned long val = SASL_CB_GETCONF;\n            ],\n            [ ac_cv_c_sasl_cb_getconf=yes ],\n            [ ac_cv_c_sasl_cb_getconf=no ])\n        ])\n    AS_IF([test \"$ac_cv_c_sasl_cb_getconf\" = \"yes\"],\n          [AC_DEFINE([HAVE_SASL_CB_GETCONF], 1,\n                     [Set to nonzero if your SASL implementation supports SASL_CB_GETCONF])])\n])\n\ndnl **********************************************************************\ndnl DETECT_SASL_CB_GETCONFPATH\ndnl\ndnl check if we can use SASL_CB_GETCONFPATH\ndnl **********************************************************************\nAC_DEFUN([AC_C_DETECT_SASL_CB_GETCONFPATH],\n[\n    AC_CACHE_CHECK([for SASL_CB_GETCONFPATH],\n        [ac_cv_c_sasl_cb_getconfpath],\n        [AC_TRY_COMPILE(\n            [\n#include <sasl/sasl.h>\n            ], [\nunsigned long val = SASL_CB_GETCONFPATH;\n            ],\n            [ ac_cv_c_sasl_cb_getconfpath=yes ],\n            [ ac_cv_c_sasl_cb_getconfpath=no ])\n        ])\n    AS_IF([test \"$ac_cv_c_sasl_cb_getconfpath\" = \"yes\"],\n          [AC_DEFINE([HAVE_SASL_CB_GETCONFPATH], 1,\n                     [Set to nonzero if your SASL implementation supports SASL_CB_GETCONFPATH])])\n])\n\nAC_CHECK_HEADERS([sasl/sasl.h])\nif test \"x$enable_sasl\" = \"xyes\"; then\n  AC_C_DETECT_SASL_CB_GETCONF\n  AC_C_DETECT_SASL_CB_GETCONFPATH\n  AC_DEFINE([ENABLE_SASL],1,[Set to nonzero if you want to include SASL])\n  AC_SEARCH_LIBS([sasl_server_init], [sasl2 sasl], [],\n    [\n      AC_MSG_ERROR([Failed to locate the library containing sasl_server_init])\n    ])\n\n  AS_IF([test \"x$enable_sasl_pwdb\" = \"xyes\"],\n        [AC_DEFINE([ENABLE_SASL_PWDB], 1,\n                   [Set to nonzero if you want to enable a SASL pwdb])])\nfi\n\nAC_ARG_ENABLE(dtrace,\n  [AS_HELP_STRING([--enable-dtrace],[Enable dtrace probes])])\nif test \"x$enable_dtrace\" = \"xyes\"; then\n  AC_PATH_PROG([DTRACE], [dtrace], \"no\", [/usr/sbin:$PATH])\n  if test \"x$DTRACE\" != \"xno\"; then\n    AC_DEFINE([ENABLE_DTRACE],1,[Set to nonzero if you want to include DTRACE])\n    build_dtrace=yes\n    $DTRACE -h -o conftest.h -s memcached_dtrace.d 2>/dev/zero\n    if test $? -eq 0\n    then\n        dtrace_instrument_obj=yes\n        rm conftest.h\n        # on Mac probe id are generated with $\n        if test \"$is_darwin\" = \"yes\"; then\n          CFLAGS=\"$CFLAGS -Wno-dollar-in-identifier-extension\"\n        fi\n    fi\n\n    if test \"`which tr`\" = \"/usr/ucb/tr\"; then\n        AC_MSG_ERROR([Please remove /usr/ucb from your path. See man standards for more info])\n    fi\n  else\n    AC_MSG_ERROR([Need dtrace binary and OS support.])\n  fi\nfi\n\nif test \"x$enable_extstore\" != \"xno\"; then\n    AC_DEFINE([EXTSTORE],1,[Set to nonzero if you want to enable extstore])\nfi\n\nif test \"x$enable_tls\" = \"xyes\"; then\n    AC_DEFINE([TLS],1,[Set to nonzero if you want to enable TLS])\nfi\n\nif test \"x$enable_asan\" = \"xyes\"; then\n    AC_DEFINE([ASAN],1,[Set to nonzero if you want to compile using ASAN])\nfi\n\nif test \"x$enable_static\" = \"xyes\"; then\n    AC_DEFINE([STATIC],1,[Set to nonzero if you want to compile a statically linked binary])\nfi\n\nif test \"x$enable_unix_socket\" = \"xno\"; then\n    AC_DEFINE([DISABLE_UNIX_SOCKET],1,[Set to nonzero if you want to disable unix domain socket])\nfi\n\nif test \"x$enable_proxy\" = \"xyes\"; then\n    AC_DEFINE([PROXY],1,[Set to nonzero if you want to enable proxy code])\n    CPPFLAGS=\"-Ivendor/lua/src -Ivendor/liburing/src/include $CPPFLAGS\"\n    dnl lua needs math lib.\n    LIBS=\"$LIBS -lm -ldl\"\nfi\n\nif test \"x$enable_proxy_uring\" = \"xyes\"; then\n    AC_DEFINE([HAVE_LIBURING],1,[Set to nonzero if you want to enable proxy uring handling])\n    CPPFLAGS=\"-Ivendor/liburing/src/include $CPPFLAGS\"\nfi\n\nif test \"x$enable_proxy_tls\" = \"xyes\"; then\n    if test \"x$enable_tls\" != \"xyes\"; then\n        AC_MSG_ERROR([--enable-proxy-tls requires --enable-tls])\n    else\n        AC_DEFINE([PROXY_TLS],1,[Set to nonzero if you want to enable proxy backend tls support])\n    fi\nfi\n\nif test \"x$enable_large_client_flags\" = \"xyes\"; then\n    AC_DEFINE([LARGE_CLIENT_FLAGS],1,[Set to nonzero if you want 64bit client flags])\nfi\n\nAM_CONDITIONAL([BUILD_DTRACE],[test \"$build_dtrace\" = \"yes\"])\nAM_CONDITIONAL([DTRACE_INSTRUMENT_OBJ],[test \"$dtrace_instrument_obj\" = \"yes\"])\nAM_CONDITIONAL([ENABLE_SASL],[test \"$enable_sasl\" = \"yes\"])\nAM_CONDITIONAL([ENABLE_EXTSTORE],[test \"$enable_extstore\" != \"no\"])\nAM_CONDITIONAL([ENABLE_ARM_CRC32],[test \"$enable_arm_crc32\" = \"yes\"])\nAM_CONDITIONAL([ENABLE_TLS],[test \"$enable_tls\" = \"yes\"])\nAM_CONDITIONAL([ENABLE_ASAN],[test \"$enable_asan\" = \"yes\"])\nAM_CONDITIONAL([ENABLE_STATIC],[test \"$enable_static\" = \"yes\"])\nAM_CONDITIONAL([DISABLE_UNIX_SOCKET],[test \"$enable_unix_socket\" = \"no\"])\nAM_CONDITIONAL([ENABLE_PROXY],[test \"$enable_proxy\" = \"yes\"])\nAM_CONDITIONAL([ENABLE_PROXY_URING],[test \"$enable_proxy_uring\" = \"yes\"])\nAM_CONDITIONAL([ENABLE_PROXY_TLS],[test \"$enable_proxy_tls\" = \"yes\"])\nAM_CONDITIONAL([LARGE_CLIENT_FLAGS],[test \"$enable_large_client_flags\" = \"yes\"])\n\n\nAC_SUBST(DTRACE)\nAC_SUBST(DTRACEFLAGS)\nAC_SUBST(ENABLE_SASL)\nAC_SUBST(PROFILER_LDFLAGS)\n\nAC_ARG_ENABLE(coverage,\n  [AS_HELP_STRING([--disable-coverage],[Disable code coverage])])\n\nif test \"x$enable_coverage\" != \"xno\"; then\n   if test \"$GCC\" = \"yes\" -a \"$ICC\" != \"yes\" -a \"$CLANG\" != \"yes\"\n   then\n      CFLAGS=\"$CFLAGS -pthread\"\n      AC_PATH_PROG([PROFILER], [gcov], \"no\", [$PATH])\n      if test \"x$PROFILER\" != \"xno\"; then\n         # Issue 97: The existence of gcov doesn't mean we have -lgcov\n         AC_CHECK_LIB(gcov, main,\n                    [\n                      PROFILER_FLAGS=\"-fprofile-arcs -ftest-coverage\"\n                      PROFILER_LDFLAGS=\"-lgcov\"\n                    ], [\n                      PROFILER_FLAGS=\n                      PROFILER_LDFLAGS=\n                    ])\n      fi\n   elif test \"$SUNCC\" = \"yes\"\n   then\n      AC_PATH_PROG([PROFILER], [tcov], \"no\", [$PATH])\n      if test \"x$PROFILER\" != \"xno\"; then\n         PROFILER_FLAGS=-xprofile=tcov\n      fi\n   elif test \"x$CLANG\" != \"xno\"\n   then\n      AC_PATH_PROG([PROFILER], [gcov], \"no\", [$PATH])\n      if test \"x$PROFILER\" != \"xno\"\n      then\n          PROFILER_FLAGS=\"-fprofile-arcs -ftest-coverage\"\n          PROFILER_LDFLAGS=\n      fi\n   fi\nfi\nAC_SUBST(PROFILER_FLAGS)\n\n\nAC_ARG_ENABLE(64bit,\n  [AS_HELP_STRING([--enable-64bit],[build 64bit version])])\nif test \"x$enable_64bit\" = \"xyes\"\nthen\n    org_cflags=$CFLAGS\n    CFLAGS=-m64\n    AC_RUN_IFELSE(\n      [AC_LANG_PROGRAM([], [dnl\nreturn sizeof(void*) == 8 ? 0 : 1;\n      ])\n    ],[\n      CFLAGS=\"-m64 $org_cflags\"\n    ],[\n      AC_MSG_ERROR([Don't know how to build a 64-bit object.])\n    ],[\n       dnl cross compile\n       AC_MSG_WARN([Assuming no extra CFLAGS are required for cross-compiling 64bit version.])\n    ])\nfi\n\ndnl Check if data pointer is 64bit or not\nAC_CHECK_SIZEOF([void *])\n\n# Issue 213: Search for clock_gettime to help people linking\n#            with a static version of libevent\nAC_SEARCH_LIBS(clock_gettime, rt)\n# Issue 214: Search for the network libraries _before_ searching\n#            for libevent (to help people linking with static libevent)\nAC_SEARCH_LIBS(socket, socket)\nAC_SEARCH_LIBS(gethostbyname, nsl)\n\ntrylibeventdir=\"\"\nAC_ARG_WITH(libevent,\n       [  --with-libevent=PATH     Specify path to libevent installation ],\n       [\n                if test \"x$withval\" != \"xno\" ; then\n                        trylibeventdir=$withval\n                fi\n       ]\n)\n\ndnl ------------------------------------------------------\ndnl libevent detection.  swiped from Tor.  modified a bit.\n\nLIBEVENT_URL=https://www.monkey.org/~provos/libevent/\n\nAC_CACHE_CHECK([for libevent directory], ac_cv_libevent_dir, [\n  saved_LIBS=\"$LIBS\"\n  saved_LDFLAGS=\"$LDFLAGS\"\n  saved_CPPFLAGS=\"$CPPFLAGS\"\n  le_found=no\n  for ledir in $trylibeventdir \"\" $prefix /usr/local ; do\n    LDFLAGS=\"$saved_LDFLAGS\"\n    LIBS=\"-levent $saved_LIBS\"\n\n    # Skip the directory if it isn't there.\n    if test ! -z \"$ledir\" -a ! -d \"$ledir\" ; then\n       continue;\n    fi\n    if test ! -z \"$ledir\" ; then\n      if test -d \"$ledir/lib\" ; then\n        LDFLAGS=\"-L$ledir/lib $LDFLAGS\"\n      else\n        LDFLAGS=\"-L$ledir $LDFLAGS\"\n      fi\n      if test -d \"$ledir/include\" ; then\n        CPPFLAGS=\"-I$ledir/include $CPPFLAGS\"\n      else\n        CPPFLAGS=\"-I$ledir $CPPFLAGS\"\n      fi\n    fi\n    # Can I compile and link it?\n    AC_TRY_LINK([#include <sys/time.h>\n#include <sys/types.h>\n#include <event.h>], [ event_init(); ],\n       [ libevent_linked=yes ], [ libevent_linked=no ])\n    if test $libevent_linked = yes; then\n       if test ! -z \"$ledir\" ; then\n         ac_cv_libevent_dir=$ledir\n         _myos=`echo $target_os | cut -f 1 -d .`\n         AS_IF(test \"$SUNCC\" = \"yes\" -o \"x$_myos\" = \"xsolaris2\",\n               [saved_LDFLAGS=\"$saved_LDFLAGS -Wl,-R$ledir/lib\"],\n               [AS_IF(test \"$GCC\" = \"yes\",\n                     [saved_LDFLAGS=\"$saved_LDFLAGS -Wl,-rpath,$ledir/lib\"])])\n       else\n         ac_cv_libevent_dir=\"(system)\"\n       fi\n       le_found=yes\n       break\n    fi\n  done\n  LIBS=\"$saved_LIBS\"\n  LDFLAGS=\"$saved_LDFLAGS\"\n  CPPFLAGS=\"$saved_CPPFLAGS\"\n  if test $le_found = no ; then\n    AC_MSG_ERROR([libevent is required.  You can get it from $LIBEVENT_URL\n\n      If it's already installed, specify its path using --with-libevent=/dir/\n])\n  fi\n])\nLIBS=\"-levent $LIBS\"\nif test $ac_cv_libevent_dir != \"(system)\"; then\n  if test -d \"$ac_cv_libevent_dir/lib\" ; then\n    LDFLAGS=\"-L$ac_cv_libevent_dir/lib $LDFLAGS\"\n    le_libdir=\"$ac_cv_libevent_dir/lib\"\n  else\n    LDFLAGS=\"-L$ac_cv_libevent_dir $LDFLAGS\"\n    le_libdir=\"$ac_cv_libevent_dir\"\n  fi\n  if test -d \"$ac_cv_libevent_dir/include\" ; then\n    CPPFLAGS=\"-I$ac_cv_libevent_dir/include $CPPFLAGS\"\n  else\n    CPPFLAGS=\"-I$ac_cv_libevent_dir $CPPFLAGS\"\n  fi\nfi\n\nAC_RUN_IFELSE(\n   [AC_LANG_PROGRAM([\n#include <event.h>\n   ], [dnl\nconst char *ver = event_get_version();\nreturn (ver != NULL && *ver != '1') ? 0 : 1;\n   ])\n   ],[\n     AC_DEFINE(HAVE_LIBEVENT_NEW, 1, [linked to libevent])\n   ],\n   [\n     AC_MSG_ERROR([libevent2 is required])\n   ],\n   []\n[])\n\ntrylibssldir=\"\"\nAC_ARG_WITH(libssl,\n       [  --with-libssl=PATH     Specify path to libssl installation ],\n       [\n                if test \"x$withval\" != \"xno\" ; then\n                        trylibssldir=$withval\n                fi\n       ]\n)\n\ndnl ----------------------------------------------------------------------------\ndnl libssl detection.  swiped from libevent.  modified for openssl detection.\n\nPKG_PROG_PKG_CONFIG\nOPENSSL_URL=https://www.openssl.org/\nif test \"x$enable_tls\" = \"xyes\"; then\n  PKG_CHECK_MODULES(OPENSSL, openssl, [LIBS=\"$LIBS $OPENSSL_LIBS\" CFLAGS=\"$CFLAGS $OPENSSL_CFLAGS\"], [\n    AC_CACHE_CHECK([for libssl directory], ac_cv_libssl_dir, [\n      saved_LIBS=\"$LIBS\"\n      saved_LDFLAGS=\"$LDFLAGS\"\n      saved_CPPFLAGS=\"$CPPFLAGS\"\n      le_found=no\n      for ledir in $trylibssldir \"\" $prefix /usr/local ; do\n        LDFLAGS=\"$saved_LDFLAGS\"\n        LIBS=\"-lssl -lcrypto $saved_LIBS\"\n\n        # Skip the directory if it isn't there.\n        if test ! -z \"$ledir\" -a ! -d \"$ledir\" ; then\n          continue;\n        fi\n        if test ! -z \"$ledir\" ; then\n          if test -d \"$ledir/lib\" ; then\n            LDFLAGS=\"-L$ledir/lib $LDFLAGS\"\n          else\n            LDFLAGS=\"-L$ledir $LDFLAGS\"\n          fi\n          if test -d \"$ledir/include\" ; then\n            CPPFLAGS=\"-I$ledir/include $CPPFLAGS\"\n          else\n            CPPFLAGS=\"-I$ledir $CPPFLAGS\"\n          fi\n        fi\n        # Can I compile and link it?\n        AC_TRY_LINK([#include <sys/time.h>\n    #include <sys/types.h>\n    #include <assert.h>\n    #include <openssl/ssl.h>], [ SSL_CTX* ssl_ctx = SSL_CTX_new(TLS_server_method());\n                                assert(OPENSSL_VERSION_NUMBER >= 0x10101000L);],\n           [ libssl_linked=yes ], [ libssl_linked=no ])\n        if test $libssl_linked = yes; then\n          if test ! -z \"$ledir\" ; then\n            ac_cv_libssl_dir=$ledir\n            _myos=`echo $target_os | cut -f 1 -d .`\n            AS_IF(test \"$SUNCC\" = \"yes\" -o \"x$_myos\" = \"xsolaris2\",\n                  [saved_LDFLAGS=\"$saved_LDFLAGS -Wl,-R$ledir/lib\"],\n                  [AS_IF(test \"$GCC\" = \"yes\",\n                        [saved_LDFLAGS=\"$saved_LDFLAGS -Wl,-rpath,$ledir/lib\"])])\n           else\n             ac_cv_libssl_dir=\"(system)\"\n           fi\n           le_found=yes\n           break\n        fi\n      done\n      LIBS=\"$saved_LIBS\"\n      LDFLAGS=\"$saved_LDFLAGS\"\n      CPPFLAGS=\"$saved_CPPFLAGS\"\n      if test $le_found = no ; then\n        AC_MSG_ERROR([libssl (at least version 1.1.0) is required.  You can get it from $OPENSSL_URL\n\n          If it's already installed, specify its path using --with-libssl=/dir/\n    ])\n      fi\n    ])\n    LIBS=\"-lssl -lcrypto $LIBS\"\n    if test $ac_cv_libssl_dir != \"(system)\"; then\n      if test -d \"$ac_cv_libssl_dir/lib\" ; then\n        LDFLAGS=\"-L$ac_cv_libssl_dir/lib $LDFLAGS\"\n        le_libdir=\"$ac_cv_libssl_dir/lib\"\n      else\n        LDFLAGS=\"-L$ac_cv_libssl_dir $LDFLAGS\"\n        le_libdir=\"$ac_cv_libssl_dir\"\n      fi\n      if test -d \"$ac_cv_libssl_dir/include\" ; then\n        CPPFLAGS=\"-I$ac_cv_libssl_dir/include $CPPFLAGS\"\n      else\n        CPPFLAGS=\"-I$ac_cv_libssl_dir $CPPFLAGS\"\n      fi\n    fi\n  ])\nfi\n\nif test \"x$enable_static\" = \"xyes\"; then\n  LIBS=\"$LIBS -ldl\"\n  LDFLAGS=\"-static $LDFLAGS\"\nfi\n\ndnl ----------------------------------------------------------------------------\n\nAC_SEARCH_LIBS(gethugepagesizes, hugetlbfs)\n\nAC_HEADER_STDBOOL\nAH_BOTTOM([#if HAVE_STDBOOL_H\n#include <stdbool.h>\n#else\n#define bool char\n#define false 0\n#define true 1\n#endif ])\n\nAC_CHECK_HEADERS([inttypes.h])\nAH_BOTTOM([#ifdef HAVE_INTTYPES_H\n#include <inttypes.h>\n#endif\n])\nAC_CHECK_HEADERS([sys/auxv.h])\n\ndnl **********************************************************************\ndnl Figure out if this system has the stupid sasl_callback_ft\ndnl **********************************************************************\n\nAC_DEFUN([AC_HAVE_SASL_CALLBACK_FT],\n[AC_CACHE_CHECK(for sasl_callback_ft, ac_cv_has_sasl_callback_ft,\n[\n  AC_TRY_COMPILE([\n    #ifdef HAVE_SASL_SASL_H\n    #include <sasl/sasl.h>\n    #include <sasl/saslplug.h>\n    #endif\n  ],[\n    sasl_callback_ft a_callback;\n  ],[\n    ac_cv_has_sasl_callback_ft=yes\n  ],[\n    ac_cv_has_sasl_callback_ft=no\n  ])\n])\nif test $ac_cv_has_sasl_callback_ft = yes; then\n  AC_DEFINE(HAVE_SASL_CALLBACK_FT, 1, [we have sasl_callback_ft])\nfi\n])\n\nAC_HAVE_SASL_CALLBACK_FT\n\ndnl **********************************************************************\ndnl DETECT_UINT64_SUPPORT\ndnl\ndnl check if we can use a uint64_t\ndnl **********************************************************************\nAC_DEFUN([AC_C_DETECT_UINT64_SUPPORT],\n[\n    AC_CACHE_CHECK([for print macros for integers (C99 section 7.8.1)],\n        [ac_cv_c_uint64_support],\n        [AC_TRY_COMPILE(\n            [\n#ifdef HAVE_INTTYPES_H\n#include <inttypes.h>\n#endif\n#include <stdio.h>\n            ], [\n  uint64_t val = 0;\n  fprintf(stderr, \"%\" PRIu64 \"\\n\", val);\n            ],\n            [ ac_cv_c_uint64_support=yes ],\n            [ ac_cv_c_uint64_support=no ])\n        ])\n])\n\nAC_C_DETECT_UINT64_SUPPORT\nAS_IF([test \"x$ac_cv_c_uint64_support\" = \"xno\"],\n      [AC_MSG_WARN([\n\nFailed to use print macros (PRIu) as defined in C99 section 7.8.1.\n\n])])\n\nAC_C_CONST\n\ndnl From licq: Copyright (c) 2000 Dirk Mueller\ndnl Check if the type socklen_t is defined anywhere\nAC_DEFUN([AC_C_SOCKLEN_T],\n[AC_CACHE_CHECK(for socklen_t, ac_cv_c_socklen_t,\n[\n  AC_TRY_COMPILE([\n    #include <sys/types.h>\n    #include <sys/socket.h>\n  ],[\n    socklen_t foo;\n  ],[\n    ac_cv_c_socklen_t=yes\n  ],[\n    ac_cv_c_socklen_t=no\n  ])\n])\nif test $ac_cv_c_socklen_t = no; then\n  AC_DEFINE(socklen_t, int, [define to int if socklen_t not available])\nfi\n])\n\nAC_C_SOCKLEN_T\n\ndnl Check if we're a little-endian or a big-endian system, needed by hash code\nAC_C_BIGENDIAN(\n  [AC_DEFINE(ENDIAN_BIG, 1, [machine is bigendian])],\n  [AC_DEFINE(ENDIAN_LITTLE, 1, [machine is littleendian])],\n  [AC_MSG_ERROR([Cannot detect endianness. Must pass ac_cv_c_bigendian={yes,no} to configure.])])\n\nAC_DEFUN([AC_C_HTONLL],\n[\n    AC_MSG_CHECKING([for htonll])\n    have_htoll=\"no\"\n    AC_TRY_LINK([\n#include <sys/types.h>\n#include <netinet/in.h>\n#ifdef HAVE_INTTYPES_H\n#include <inttypes.h> */\n#endif\n       ], [\n          return htonll(0);\n       ], [\n          have_htoll=\"yes\"\n          AC_DEFINE([HAVE_HTONLL], [1], [Have ntohll])\n    ], [\n          have_htoll=\"no\"\n    ])\n\n    AC_MSG_RESULT([$have_htoll])\n])\n\nAC_C_HTONLL\n\ndnl Check whether the user's system supports pthread\nAC_SEARCH_LIBS(pthread_create, pthread)\nif test \"x$ac_cv_search_pthread_create\" = \"xno\"; then\n  AC_MSG_ERROR([Can't enable threads without the POSIX thread library.])\nfi\n\nAC_CHECK_FUNCS(mlockall)\nAC_CHECK_FUNCS(getpagesizes)\nAC_CHECK_FUNCS(sysconf)\nAC_CHECK_FUNCS(memcntl)\nAC_CHECK_FUNCS(clock_gettime)\nAC_CHECK_FUNCS(preadv)\nAC_CHECK_FUNCS(pread)\nAC_CHECK_FUNCS(eventfd)\nAC_CHECK_FUNCS([pthread_setname_np],[AC_DEFINE(HAVE_PTHREAD_SETNAME_NP, 1, [Define to 1 if support pthread_setname_np])])\nAC_CHECK_FUNCS([accept4], [AC_DEFINE(HAVE_ACCEPT4, 1, [Define to 1 if support accept4])])\nAC_CHECK_FUNCS([getopt_long], [AC_DEFINE(HAVE_GETOPT_LONG, 1, [Define to 1 if support getopt_long])])\n\ndnl Need to disable opt for alignment check. GCC is too clever and turns this\ndnl into wide stores and no cmp under O2.\nAC_DEFUN([AC_C_ALIGNMENT],\n[AC_CACHE_CHECK(for alignment, ac_cv_c_alignment,\n[\n  AC_RUN_IFELSE(\n    [AC_LANG_PROGRAM([\n#include <stdlib.h>\n#include <inttypes.h>\n#pragma GCC optimize (\"O0\")\n    ], [\n       char *buf = malloc(32);\n\n       uint64_t *ptr = (uint64_t*)(buf+2);\n       // catch sigbus, etc.\n       *ptr = 0x1;\n\n       // catch unaligned word access (ARM cpus)\n#ifdef ENDIAN_BIG\n#define ALIGNMENT 0x02030405\n#else\n#define ALIGNMENT 0x05040302\n#endif\n       *(buf + 0) = 1;\n       *(buf + 1) = 2;\n       *(buf + 2) = 3;\n       *(buf + 3) = 4;\n       *(buf + 4) = 5;\n       int* i = (int*)(buf+1);\n       return (ALIGNMENT == *i) ? 0 : 1;\n    ])\n  ],[\n    ac_cv_c_alignment=none\n  ],[\n    ac_cv_c_alignment=need\n  ],[\n    dnl cross compile\n    ac_cv_c_alignment=maybe\n  ])\n])\nAS_IF([test $ac_cv_c_alignment = need],\n  [AC_DEFINE(NEED_ALIGN, 1, [Machine need alignment])])\nAS_IF([test $ac_cv_c_alignment = maybe],\n  [AC_MSG_WARN([Assuming aligned access is required when cross-compiling])\n   AC_DEFINE(NEED_ALIGN, 1, [Machine need alignment])])\n])\n\nAC_C_ALIGNMENT\n\ndnl Check for our specific usage of GCC atomics.\ndnl These were added in 4.1.2, but 32bit OS's may lack shorts and 4.1.2\ndnl lacks testable defines.\nhave_gcc_atomics=no\nAC_MSG_CHECKING(for GCC atomics)\nAC_TRY_LINK([],[\n  unsigned short a;\n  unsigned short b;\n  b = __sync_add_and_fetch(&a, 1);\n  b = __sync_sub_and_fetch(&a, 2);\n  ],[have_gcc_atomics=yes\n  AC_DEFINE(HAVE_GCC_ATOMICS, 1, [GCC Atomics available])])\nAC_MSG_RESULT($have_gcc_atomics)\n\ndnl Check for usage of 64bit atomics\ndnl 32bit systems shouldn't have these.\nhave_gcc_64atomics=no\nAC_MSG_CHECKING(for GCC 64bit atomics)\nAC_TRY_LINK([#include <inttypes.h>\n   ],[\n  uint64_t a;\n  uint64_t b;\n  b = __sync_add_and_fetch(&a, 1);\n  b = __sync_sub_and_fetch(&a, 2);\n  ],[have_gcc_64atomics=yes\n  AC_DEFINE(HAVE_GCC_64ATOMICS, 1, [GCC 64bit Atomics available])])\nAC_MSG_RESULT($have_gcc_64atomics)\n\ndnl Check for the requirements for running memcached with less privileges\ndnl than the default privilege set. On Solaris we need setppriv and priv.h\ndnl If you want to add support for other platforms you should check for\ndnl your requirements, define HAVE_DROP_PRIVILEGES, and make sure you add\ndnl the source file containing the implementation into memcached_SOURCE\ndnl in Makefile.am\nAC_CHECK_FUNCS(setppriv, [\n   AC_CHECK_HEADER(priv.h, [\n      AC_DEFINE([HAVE_DROP_PRIVILEGES], 1,\n         [Define this if you have an implementation of drop_privileges()])\n      build_solaris_privs=yes\n   ], [])\n],[])\n\nAS_IF([test \"x$enable_seccomp\" = \"xyes\" ], [\n   AC_CHECK_LIB(seccomp, seccomp_rule_add, [\n      AC_DEFINE([HAVE_DROP_PRIVILEGES], 1,\n         [Define this if you have an implementation of drop_privileges()])\n      build_linux_privs=yes\n      AC_DEFINE([HAVE_DROP_WORKER_PRIVILEGES], 1,\n         [Define this if you have an implementation of drop_worker_privileges()])\n      build_linux_privs=yes\n   ], [])\n])\n\nAC_CHECK_FUNCS(pledge, [\n   AC_CHECK_HEADER(unistd.h, [\n      AC_DEFINE([HAVE_DROP_PRIVILEGES], 1,\n         [Define this if you have an implementation of drop_privileges()])\n      build_openbsd_privs=yes\n   ], [])\n],[])\n\nAC_CHECK_FUNCS(cap_enter, [\n   AC_CHECK_HEADER(sys/capsicum.h, [\n      AC_DEFINE([HAVE_DROP_PRIVILEGES], 1,\n         [Define this if you have an implementation of drop_privileges()])\n      build_freebsd_privs=yes\n   ], [])\n],[])\n\nAC_CHECK_FUNCS(sandbox_init, [\n   AC_CHECK_HEADER(sandbox.h, [\n      AC_DEFINE([HAVE_DROP_PRIVILEGES], 1,\n         [Define this if you have an implementation of drop_privileges()])\n      build_darwin_privs=yes\n   ], [])\n],[])\n\n\n\nAM_CONDITIONAL([BUILD_SOLARIS_PRIVS],[test \"$build_solaris_privs\" = \"yes\"])\nAM_CONDITIONAL([BUILD_LINUX_PRIVS],[test \"$build_linux_privs\" = \"yes\"])\nAM_CONDITIONAL([BUILD_OPENBSD_PRIVS],[test \"$build_openbsd_privs\" = \"yes\"])\nAM_CONDITIONAL([BUILD_FREEBSD_PRIVS],[test \"$build_freebsd_privs\" = \"yes\"])\nAM_CONDITIONAL([BUILD_DARWIN_PRIVS],[test \"$build_darwin_privs\" = \"yes\"])\n\nAC_ARG_ENABLE(docs,\n  [AS_HELP_STRING([--disable-docs],[Disable documentation generation])])\n\nAC_PATH_PROG([XML2RFC], [xml2rfc], \"no\")\nAC_PATH_PROG([XSLTPROC], [xsltproc], \"no\")\n\nAM_CONDITIONAL([BUILD_SPECIFICATIONS],\n               [test \"x$enable_docs\" != \"xno\" -a \"x$XML2RFC\" != \"xno\" -a \"x$XSLTPROC\" != \"xno\"])\n\n\nif test \"x$enable_werror\" = \"xyes\"; then\n   CFLAGS=\"$CFLAGS -Werror\"\nfi\n\ndnl Let the compiler be a bit more picky. Please note that you cannot\ndnl specify these flags to the compiler before AC_CHECK_FUNCS, because\ndnl the test program will generate a compilation warning and hence fail\ndnl to detect the function ;-)\nif test \"$ICC\" = \"yes\"\nthen\n   dnl ICC trying to be gcc.\n   CFLAGS=\"$CFLAGS -diag-disable 187 -Wall\"\n   AC_DEFINE([_GNU_SOURCE],[1],[make sure IOV_MAX is defined])\nelif test \"$GCC\" = \"yes\"\nthen\n  GCC_VERSION=`$CC -dumpversion`\n  CFLAGS=\"$CFLAGS -Wall -pedantic -Wmissing-prototypes -Wmissing-declarations -Wredundant-decls\"\n  if test \"x$enable_asan\" = \"xyes\"; then\n    CFLAGS=\"$CFLAGS -fsanitize=address\"\n  fi\n  case $GCC_VERSION in\n    4.4.*)\n    CFLAGS=\"$CFLAGS -fno-strict-aliasing\"\n    ;;\n  esac\n  AC_DEFINE([_GNU_SOURCE],[1],[make sure IOV_MAX is defined])\nelif test \"$SUNCC\" = \"yes\"\nthen\n  CFLAGS=\"$CFLAGS -errfmt=error -errwarn -errshort=tags\"\nfi\n\nAC_CONFIG_FILES(Makefile doc/Makefile)\nAC_OUTPUT\n"
        },
        {
          "name": "crawler.c",
          "type": "blob",
          "size": 28.955078125,
          "content": "/*  Copyright 2016 Netflix.\n *\n *  Use and distribution licensed under the BSD license.  See\n *  the LICENSE file for full text.\n */\n\n/* -*- Mode: C; tab-width: 4; c-basic-offset: 4; indent-tabs-mode: nil -*- */\n#include \"memcached.h\"\n#include \"storage.h\"\n#include <sys/stat.h>\n#include <sys/socket.h>\n#include <sys/resource.h>\n#include <fcntl.h>\n#include <netinet/in.h>\n#include <errno.h>\n#include <stdlib.h>\n#include <stdio.h>\n#include <signal.h>\n#include <string.h>\n#include <time.h>\n#include <assert.h>\n#include <unistd.h>\n#include <poll.h>\n\n#include \"base64.h\"\n\n#define LARGEST_ID POWER_LARGEST\n\ntypedef struct {\n    void *c; /* original connection structure. still with source thread attached. */\n    int sfd; /* client fd. */\n    int buflen;\n    int bufused;\n    char *buf; /* output buffer */\n} crawler_client_t;\n\ntypedef struct _crawler_module_t crawler_module_t;\n\ntypedef void (*crawler_eval_func)(crawler_module_t *cm, item *it, uint32_t hv, int slab_cls);\ntypedef int (*crawler_init_func)(crawler_module_t *cm, void *data); // TODO: init args?\ntypedef void (*crawler_deinit_func)(crawler_module_t *cm); // TODO: extra args?\ntypedef void (*crawler_doneclass_func)(crawler_module_t *cm, int slab_cls);\ntypedef void (*crawler_finalize_func)(crawler_module_t *cm);\n\ntypedef struct {\n    crawler_init_func init; /* run before crawl starts */\n    crawler_eval_func eval; /* runs on an item. */\n    crawler_doneclass_func doneclass; /* runs once per sub-crawler completion. */\n    crawler_finalize_func finalize; /* runs once when all sub-crawlers are done. */\n    bool needs_lock; /* whether or not we need the LRU lock held when eval is called */\n    bool needs_client; /* whether or not to grab onto the remote client */\n} crawler_module_reg_t;\n\nstruct _crawler_module_t {\n    void *data; /* opaque data pointer */\n    crawler_client_t c;\n    crawler_module_reg_t *mod;\n    int status; /* flags/code/etc for internal module usage */\n};\n\nstatic int crawler_expired_init(crawler_module_t *cm, void *data);\nstatic void crawler_expired_doneclass(crawler_module_t *cm, int slab_cls);\nstatic void crawler_expired_finalize(crawler_module_t *cm);\nstatic void crawler_expired_eval(crawler_module_t *cm, item *search, uint32_t hv, int i);\n\ncrawler_module_reg_t crawler_expired_mod = {\n    .init = crawler_expired_init,\n    .eval = crawler_expired_eval,\n    .doneclass = crawler_expired_doneclass,\n    .finalize = crawler_expired_finalize,\n    .needs_lock = true,\n    .needs_client = false,\n};\n\nstatic int crawler_metadump_init(crawler_module_t *cm, void *data);\nstatic void crawler_metadump_eval(crawler_module_t *cm, item *search, uint32_t hv, int i);\nstatic void crawler_metadump_finalize(crawler_module_t *cm);\n\ncrawler_module_reg_t crawler_metadump_mod = {\n    .init = crawler_metadump_init,\n    .eval = crawler_metadump_eval,\n    .doneclass = NULL,\n    .finalize = crawler_metadump_finalize,\n    .needs_lock = false,\n    .needs_client = true,\n};\n\nstatic int crawler_mgdump_init(crawler_module_t *cm, void *data);\nstatic void crawler_mgdump_eval(crawler_module_t *cm, item *search, uint32_t hv, int i);\nstatic void crawler_mgdump_finalize(crawler_module_t *cm);\n\ncrawler_module_reg_t crawler_mgdump_mod = {\n    .init = crawler_mgdump_init,\n    .eval = crawler_mgdump_eval,\n    .doneclass = NULL,\n    .finalize = crawler_mgdump_finalize,\n    .needs_lock = false,\n    .needs_client = true,\n};\n\ncrawler_module_reg_t *crawler_mod_regs[4] = {\n    &crawler_expired_mod,\n    &crawler_expired_mod,\n    &crawler_metadump_mod,\n    &crawler_mgdump_mod,\n};\n\nstatic int lru_crawler_write(crawler_client_t *c);\ncrawler_module_t active_crawler_mod;\nenum crawler_run_type active_crawler_type;\n\nstatic crawler crawlers[LARGEST_ID];\n\nstatic int crawler_count = 0;\nstatic volatile int do_run_lru_crawler_thread = 0;\nstatic int lru_crawler_initialized = 0;\nstatic pthread_mutex_t lru_crawler_lock = PTHREAD_MUTEX_INITIALIZER;\nstatic pthread_cond_t  lru_crawler_cond = PTHREAD_COND_INITIALIZER;\n#ifdef EXTSTORE\n/* TODO: pass this around */\nstatic void *storage;\n#endif\n\n/* Will crawl all slab classes a minimum of once per hour */\n#define MAX_MAINTCRAWL_WAIT 60 * 60\n\n/*** LRU CRAWLER THREAD ***/\n\n#define LRU_CRAWLER_MINBUFSPACE 8192\n\nstatic void lru_crawler_close_client(crawler_client_t *c) {\n    //fprintf(stderr, \"CRAWLER: Closing client\\n\");\n    sidethread_conn_close(c->c);\n    c->c = NULL;\n    free(c->buf);\n    c->buf = NULL;\n}\n\nstatic void lru_crawler_release_client(crawler_client_t *c) {\n    //fprintf(stderr, \"CRAWLER: Closing client\\n\");\n    redispatch_conn(c->c);\n    c->c = NULL;\n    free(c->buf);\n    c->buf = NULL;\n}\n\nstatic int lru_crawler_expand_buf(crawler_client_t *c) {\n    c->buflen *= 2;\n    char *nb = realloc(c->buf, c->buflen);\n    if (nb == NULL) {\n        return -1;\n    }\n    c->buf = nb;\n    return 0;\n}\n\nstatic int crawler_expired_init(crawler_module_t *cm, void *data) {\n    struct crawler_expired_data *d;\n    if (data != NULL) {\n        d = data;\n        d->is_external = true;\n        cm->data = data;\n    } else {\n        // allocate data.\n        d = calloc(1, sizeof(struct crawler_expired_data));\n        if (d == NULL) {\n            return -1;\n        }\n        // init lock.\n        pthread_mutex_init(&d->lock, NULL);\n        d->is_external = false;\n        d->start_time = current_time;\n\n        cm->data = d;\n    }\n    pthread_mutex_lock(&d->lock);\n    memset(&d->crawlerstats, 0, sizeof(crawlerstats_t) * POWER_LARGEST);\n    for (int x = 0; x < POWER_LARGEST; x++) {\n        d->crawlerstats[x].start_time = current_time;\n        d->crawlerstats[x].run_complete = false;\n    }\n    pthread_mutex_unlock(&d->lock);\n    return 0;\n}\n\nstatic void crawler_expired_doneclass(crawler_module_t *cm, int slab_cls) {\n    struct crawler_expired_data *d = (struct crawler_expired_data *) cm->data;\n    pthread_mutex_lock(&d->lock);\n    d->crawlerstats[slab_cls].end_time = current_time;\n    d->crawlerstats[slab_cls].run_complete = true;\n    pthread_mutex_unlock(&d->lock);\n}\n\nstatic void crawler_expired_finalize(crawler_module_t *cm) {\n    struct crawler_expired_data *d = (struct crawler_expired_data *) cm->data;\n    pthread_mutex_lock(&d->lock);\n    d->end_time = current_time;\n    d->crawl_complete = true;\n    pthread_mutex_unlock(&d->lock);\n\n    if (!d->is_external) {\n        free(d);\n    }\n}\n\n/* I pulled this out to make the main thread clearer, but it reaches into the\n * main thread's values too much. Should rethink again.\n */\nstatic void crawler_expired_eval(crawler_module_t *cm, item *search, uint32_t hv, int i) {\n    struct crawler_expired_data *d = (struct crawler_expired_data *) cm->data;\n    pthread_mutex_lock(&d->lock);\n    crawlerstats_t *s = &d->crawlerstats[i];\n    int is_flushed = item_is_flushed(search);\n#ifdef EXTSTORE\n    bool is_valid = true;\n    if (search->it_flags & ITEM_HDR) {\n        is_valid = storage_validate_item(storage, search);\n    }\n#endif\n    if ((search->exptime != 0 && search->exptime < current_time)\n        || is_flushed\n#ifdef EXTSTORE\n        || !is_valid\n#endif\n        ) {\n        crawlers[i].reclaimed++;\n        s->reclaimed++;\n\n        if (settings.verbose > 1) {\n            int ii;\n            char *key = ITEM_key(search);\n            fprintf(stderr, \"LRU crawler found an expired item (flags: %d, slab: %d): \",\n                search->it_flags, search->slabs_clsid);\n            for (ii = 0; ii < search->nkey; ++ii) {\n                fprintf(stderr, \"%c\", key[ii]);\n            }\n            fprintf(stderr, \"\\n\");\n        }\n        if ((search->it_flags & ITEM_FETCHED) == 0 && !is_flushed) {\n            crawlers[i].unfetched++;\n        }\n#ifdef EXTSTORE\n        STORAGE_delete(storage, search);\n#endif\n        do_item_unlink_nolock(search, hv);\n        do_item_remove(search);\n    } else {\n        s->seen++;\n        refcount_decr(search);\n        if (search->exptime == 0) {\n            s->noexp++;\n        } else if (search->exptime - current_time > 3599) {\n            s->ttl_hourplus++;\n        } else {\n            rel_time_t ttl_remain = search->exptime - current_time;\n            int bucket = ttl_remain / 60;\n            if (bucket <= 60) {\n                s->histo[bucket]++;\n            }\n        }\n    }\n    pthread_mutex_unlock(&d->lock);\n}\n\nstatic int crawler_metadump_init(crawler_module_t *cm, void *data) {\n    cm->status = 0;\n    return 0;\n}\n\nstatic void crawler_metadump_eval(crawler_module_t *cm, item *it, uint32_t hv, int i) {\n    char keybuf[KEY_MAX_URI_ENCODED_LENGTH];\n    int is_flushed = item_is_flushed(it);\n    /* Ignore expired content. */\n    if ((it->exptime != 0 && it->exptime < current_time)\n        || is_flushed) {\n        refcount_decr(it);\n        return;\n    }\n    client_flags_t flags;\n    FLAGS_CONV(it, flags);\n    // TODO: uriencode directly into the buffer.\n    uriencode(ITEM_key(it), keybuf, it->nkey, KEY_MAX_URI_ENCODED_LENGTH);\n    int total = snprintf(cm->c.buf + cm->c.bufused, 4096,\n            \"key=%s exp=%ld la=%llu cas=%llu fetch=%s cls=%u size=%lu flags=%llu\\n\",\n            keybuf,\n            (it->exptime == 0) ? -1 : (long)(it->exptime + process_started),\n            (unsigned long long)(it->time + process_started),\n            (unsigned long long)ITEM_get_cas(it),\n            (it->it_flags & ITEM_FETCHED) ? \"yes\" : \"no\",\n            ITEM_clsid(it),\n            (unsigned long) ITEM_ntotal(it),\n            (unsigned long long)flags);\n    refcount_decr(it);\n    // TODO: some way of tracking the errors. these should be impossible given\n    // the space requirements.\n    if (total >= LRU_CRAWLER_MINBUFSPACE - 1 || total <= 0) {\n        // Failed to write, don't push it.\n        return;\n    }\n    cm->c.bufused += total;\n}\n\nstatic void crawler_metadump_finalize(crawler_module_t *cm) {\n    if (cm->c.c != NULL) {\n        // flush any pending data.\n        if (lru_crawler_write(&cm->c) == 0) {\n            // Only nonzero status right now means we were locked\n            if (cm->status != 0) {\n                const char *errstr = \"ERROR locked try again later\\r\\n\";\n                size_t errlen = strlen(errstr);\n                memcpy(cm->c.buf, errstr, errlen);\n                cm->c.bufused += errlen;\n            } else {\n                memcpy(cm->c.buf, \"END\\r\\n\", 5);\n                cm->c.bufused += 5;\n            }\n        }\n    }\n}\n\nstatic int crawler_mgdump_init(crawler_module_t *cm, void *data) {\n    cm->status = 0;\n    return 0;\n}\n\nstatic void crawler_mgdump_eval(crawler_module_t *cm, item *it, uint32_t hv, int i) {\n    int is_flushed = item_is_flushed(it);\n    /* Ignore expired content. */\n    if ((it->exptime != 0 && it->exptime < current_time)\n        || is_flushed) {\n        refcount_decr(it);\n        return;\n    }\n\n    char *p = cm->c.buf + cm->c.bufused; // buffer offset.\n    char *start = p;\n    memcpy(p, \"mg \", 3);\n    p += 3;\n    if (it->it_flags & ITEM_KEY_BINARY) {\n        p += base64_encode((unsigned char *) ITEM_key(it), it->nkey, (unsigned char*) p, LRU_CRAWLER_MINBUFSPACE/2);\n        memcpy(p, \" b\\r\\n\", 4);\n        p += 4;\n    } else {\n        memcpy(p, ITEM_key(it), it->nkey);\n        p += it->nkey;\n        memcpy(p, \"\\r\\n\", 2);\n        p += 2;\n    }\n    int total = p - start;\n\n    refcount_decr(it);\n    cm->c.bufused += total;\n}\n\nstatic void crawler_mgdump_finalize(crawler_module_t *cm) {\n    if (cm->c.c != NULL) {\n        // flush any pending data.\n        if (lru_crawler_write(&cm->c) == 0) {\n            // Only nonzero status right now means we were locked\n            if (cm->status != 0) {\n                const char *errstr = \"ERROR locked try again later\\r\\n\";\n                size_t errlen = strlen(errstr);\n                memcpy(cm->c.buf, errstr, errlen);\n                cm->c.bufused += errlen;\n            } else {\n                memcpy(cm->c.buf, \"EN\\r\\n\", 4);\n                cm->c.bufused += 4;\n            }\n        }\n    }\n}\n\n// write the whole buffer out to the client socket.\nstatic int lru_crawler_write(crawler_client_t *c) {\n    unsigned int data_size = c->bufused;\n    unsigned int sent = 0;\n    struct pollfd to_poll[1];\n    to_poll[0].fd = c->sfd;\n    to_poll[0].events = POLLOUT;\n\n    if (c->c == NULL) return -1;\n    if (data_size == 0) return 0;\n\n    while (sent < data_size) {\n        int ret = poll(to_poll, 1, 1000);\n\n        if (ret < 0) {\n            // fatal.\n            lru_crawler_close_client(c);\n            return -1;\n        }\n\n        if (ret == 0) return 0;\n\n        // check if socket was closed on us.\n        if (to_poll[0].revents & POLLIN) {\n            char buf[1];\n            int res = ((conn*)c->c)->read(c->c, buf, 1);\n            if (res == 0 || (res == -1 && (errno != EAGAIN && errno != EWOULDBLOCK))) {\n                lru_crawler_close_client(c);\n                return -1;\n            }\n        }\n\n        if (to_poll[0].revents & (POLLHUP|POLLERR)) {\n            // got socket hangup.\n            lru_crawler_close_client(c);\n            return -1;\n        } else if (to_poll[0].revents & POLLOUT) {\n            // socket is writeable.\n            int total = ((conn*)c->c)->write(c->c, c->buf + sent, data_size - sent);\n            if (total == -1) {\n                if (errno != EAGAIN && errno != EWOULDBLOCK) {\n                    lru_crawler_close_client(c);\n                    return -1;\n                }\n            } else if (total == 0) {\n                lru_crawler_close_client(c);\n                return -1;\n            }\n            sent += total;\n        }\n    } // while\n\n    // write buffer now empty.\n    c->bufused = 0;\n\n    return 0;\n}\n\nstatic void lru_crawler_class_done(int i) {\n    crawlers[i].it_flags = 0;\n    crawler_count--;\n    do_item_unlinktail_q((item *)&crawlers[i]);\n    do_item_stats_add_crawl(i, crawlers[i].reclaimed,\n            crawlers[i].unfetched, crawlers[i].checked);\n    pthread_mutex_unlock(&lru_locks[i]);\n    if (active_crawler_mod.mod->doneclass != NULL)\n        active_crawler_mod.mod->doneclass(&active_crawler_mod, i);\n}\n\n// ensure we build the buffer a little bit to cut down on poll/write syscalls.\n#define MIN_ITEMS_PER_WRITE 16\nstatic void item_crawl_hash(void) {\n    // get iterator from assoc. can hang for a long time.\n    // - blocks hash expansion\n    void *iter = assoc_get_iterator();\n    int crawls_persleep = settings.crawls_persleep;\n    item *it = NULL;\n    int items = 0;\n\n    // Could not get the iterator: probably locked due to hash expansion.\n    if (iter == NULL) {\n        active_crawler_mod.status = 1;\n        return;\n    }\n\n    // loop while iterator returns something\n    // - iterator func handles bucket-walking\n    // - iterator returns with bucket locked.\n    while (assoc_iterate(iter, &it)) {\n        // if iterator returns true but no item, we're inbetween buckets and\n        // can do cleanup work without holding an item lock.\n        if (it == NULL) {\n            if (active_crawler_mod.c.c != NULL) {\n                if (items > MIN_ITEMS_PER_WRITE) {\n                    int ret = lru_crawler_write(&active_crawler_mod.c);\n                    items = 0;\n                    if (ret != 0) {\n                        // fail out and finalize.\n                        break;\n                    }\n                }\n            } else if (active_crawler_mod.mod->needs_client) {\n                // fail out and finalize.\n                break;\n            }\n\n            // - sleep bits from orig loop\n            if (crawls_persleep <= 0 && settings.lru_crawler_sleep) {\n                pthread_mutex_unlock(&lru_crawler_lock);\n                usleep(settings.lru_crawler_sleep);\n                pthread_mutex_lock(&lru_crawler_lock);\n                crawls_persleep = settings.crawls_persleep;\n            } else if (!settings.lru_crawler_sleep) {\n                // TODO: only cycle lock every N?\n                pthread_mutex_unlock(&lru_crawler_lock);\n                pthread_mutex_lock(&lru_crawler_lock);\n            }\n            continue;\n        }\n\n        // double check that the item isn't in a transitional state.\n        if (refcount_incr(it) < 2) {\n            refcount_decr(it);\n            continue;\n        }\n\n        // We're presently holding an item lock, so we cannot flush the\n        // buffer to the network socket as the syscall is both slow and could\n        // hang waiting for POLLOUT. Instead we must expand the buffer.\n        if (active_crawler_mod.c.c != NULL) {\n            crawler_client_t *c = &active_crawler_mod.c;\n            if (c->buflen - c->bufused < LRU_CRAWLER_MINBUFSPACE) {\n                if (lru_crawler_expand_buf(c) != 0) {\n                    // failed to expand buffer, stop.\n                    break;\n                }\n            }\n        }\n        // FIXME: missing hv and i are fine for metadump eval, but not fine\n        // for expire eval.\n        active_crawler_mod.mod->eval(&active_crawler_mod, it, 0, 0);\n        crawls_persleep--;\n        items++;\n    }\n\n    // must finalize or we leave the hash table expansion blocked.\n    assoc_iterate_final(iter);\n    return;\n}\n\nstatic void *item_crawler_thread(void *arg) {\n    int i;\n    int crawls_persleep = settings.crawls_persleep;\n\n    pthread_mutex_lock(&lru_crawler_lock);\n    pthread_cond_signal(&lru_crawler_cond);\n    settings.lru_crawler = true;\n    if (settings.verbose > 2)\n        fprintf(stderr, \"Starting LRU crawler background thread\\n\");\n    while (do_run_lru_crawler_thread) {\n    pthread_cond_wait(&lru_crawler_cond, &lru_crawler_lock);\n\n    if (crawler_count == -1) {\n        item_crawl_hash();\n        crawler_count = 0;\n    } else {\n    while (crawler_count) {\n        item *search = NULL;\n        void *hold_lock = NULL;\n\n        for (i = POWER_SMALLEST; i < LARGEST_ID; i++) {\n            if (crawlers[i].it_flags != 1) {\n                continue;\n            }\n\n            if (active_crawler_mod.c.c != NULL) {\n                crawler_client_t *c = &active_crawler_mod.c;\n                if (c->buflen - c->bufused < LRU_CRAWLER_MINBUFSPACE) {\n                    int ret = lru_crawler_write(c);\n                    if (ret != 0) {\n                        lru_crawler_class_done(i);\n                        continue;\n                    }\n                }\n            } else if (active_crawler_mod.mod->needs_client) {\n                lru_crawler_class_done(i);\n                continue;\n            }\n            pthread_mutex_lock(&lru_locks[i]);\n            search = do_item_crawl_q((item *)&crawlers[i]);\n            if (search == NULL ||\n                (crawlers[i].remaining && --crawlers[i].remaining < 1)) {\n                if (settings.verbose > 2)\n                    fprintf(stderr, \"Nothing left to crawl for %d\\n\", i);\n                lru_crawler_class_done(i);\n                continue;\n            }\n            uint32_t hv = hash(ITEM_key(search), search->nkey);\n            /* Attempt to hash item lock the \"search\" item. If locked, no\n             * other callers can incr the refcount\n             */\n            if ((hold_lock = item_trylock(hv)) == NULL) {\n                pthread_mutex_unlock(&lru_locks[i]);\n                continue;\n            }\n            /* Now see if the item is refcount locked */\n            if (refcount_incr(search) != 2) {\n                refcount_decr(search);\n                if (hold_lock)\n                    item_trylock_unlock(hold_lock);\n                pthread_mutex_unlock(&lru_locks[i]);\n                continue;\n            }\n\n            crawlers[i].checked++;\n            /* Frees the item or decrements the refcount. */\n            /* Interface for this could improve: do the free/decr here\n             * instead? */\n            if (!active_crawler_mod.mod->needs_lock) {\n                pthread_mutex_unlock(&lru_locks[i]);\n            }\n\n            active_crawler_mod.mod->eval(&active_crawler_mod, search, hv, i);\n\n            if (hold_lock)\n                item_trylock_unlock(hold_lock);\n            if (active_crawler_mod.mod->needs_lock) {\n                pthread_mutex_unlock(&lru_locks[i]);\n            }\n\n            if (crawls_persleep-- <= 0 && settings.lru_crawler_sleep) {\n                pthread_mutex_unlock(&lru_crawler_lock);\n                usleep(settings.lru_crawler_sleep);\n                pthread_mutex_lock(&lru_crawler_lock);\n                crawls_persleep = settings.crawls_persleep;\n            } else if (!settings.lru_crawler_sleep) {\n                // TODO: only cycle lock every N?\n                pthread_mutex_unlock(&lru_crawler_lock);\n                pthread_mutex_lock(&lru_crawler_lock);\n            }\n        }\n    } // while\n    } // if crawler_count\n\n    if (active_crawler_mod.mod != NULL) {\n        if (active_crawler_mod.mod->finalize != NULL)\n            active_crawler_mod.mod->finalize(&active_crawler_mod);\n        while (active_crawler_mod.c.c != NULL && active_crawler_mod.c.bufused != 0) {\n            lru_crawler_write(&active_crawler_mod.c);\n        }\n        // Double checking in case the client closed during the poll\n        if (active_crawler_mod.c.c != NULL) {\n            lru_crawler_release_client(&active_crawler_mod.c);\n        }\n        active_crawler_mod.mod = NULL;\n    }\n\n    if (settings.verbose > 2)\n        fprintf(stderr, \"LRU crawler thread sleeping\\n\");\n\n    STATS_LOCK();\n    stats_state.lru_crawler_running = false;\n    STATS_UNLOCK();\n    }\n    pthread_mutex_unlock(&lru_crawler_lock);\n    if (settings.verbose > 2)\n        fprintf(stderr, \"LRU crawler thread stopping\\n\");\n    settings.lru_crawler = false;\n\n    return NULL;\n}\n\nstatic pthread_t item_crawler_tid;\n\nint stop_item_crawler_thread(bool wait) {\n    int ret;\n    pthread_mutex_lock(&lru_crawler_lock);\n    if (do_run_lru_crawler_thread == 0) {\n        pthread_mutex_unlock(&lru_crawler_lock);\n        return 0;\n    }\n    do_run_lru_crawler_thread = 0;\n    pthread_cond_signal(&lru_crawler_cond);\n    pthread_mutex_unlock(&lru_crawler_lock);\n    if (wait && (ret = pthread_join(item_crawler_tid, NULL)) != 0) {\n        fprintf(stderr, \"Failed to stop LRU crawler thread: %s\\n\", strerror(ret));\n        return -1;\n    }\n    return 0;\n}\n\n/* Lock dance to \"block\" until thread is waiting on its condition:\n * caller locks mtx. caller spawns thread.\n * thread blocks on mutex.\n * caller waits on condition, releases lock.\n * thread gets lock, sends signal.\n * caller can't wait, as thread has lock.\n * thread waits on condition, releases lock\n * caller wakes on condition, gets lock.\n * caller immediately releases lock.\n * thread is now safely waiting on condition before the caller returns.\n */\nint start_item_crawler_thread(void) {\n    int ret;\n\n    if (settings.lru_crawler)\n        return -1;\n    pthread_mutex_lock(&lru_crawler_lock);\n    do_run_lru_crawler_thread = 1;\n    if ((ret = pthread_create(&item_crawler_tid, NULL,\n        item_crawler_thread, NULL)) != 0) {\n        fprintf(stderr, \"Can't create LRU crawler thread: %s\\n\",\n            strerror(ret));\n        pthread_mutex_unlock(&lru_crawler_lock);\n        return -1;\n    }\n    thread_setname(item_crawler_tid, \"mc-itemcrawler\");\n    /* Avoid returning until the crawler has actually started */\n    pthread_cond_wait(&lru_crawler_cond, &lru_crawler_lock);\n    pthread_mutex_unlock(&lru_crawler_lock);\n\n    return 0;\n}\n\n/* 'remaining' is passed in so the LRU maintainer thread can scrub the whole\n * LRU every time.\n */\nstatic int do_lru_crawler_start(uint32_t id, uint32_t remaining) {\n    uint32_t sid = id;\n    int starts = 0;\n\n    pthread_mutex_lock(&lru_locks[sid]);\n    if (crawlers[sid].it_flags == 0) {\n        if (settings.verbose > 2)\n            fprintf(stderr, \"Kicking LRU crawler off for LRU %u\\n\", sid);\n        crawlers[sid].nbytes = 0;\n        crawlers[sid].nkey = 0;\n        crawlers[sid].it_flags = 1; /* For a crawler, this means enabled. */\n        crawlers[sid].next = 0;\n        crawlers[sid].prev = 0;\n        crawlers[sid].time = 0;\n        if (remaining == LRU_CRAWLER_CAP_REMAINING) {\n            remaining = do_get_lru_size(sid);\n        }\n        /* Values for remaining:\n         * remaining = 0\n         * - scan all elements, until a NULL is reached\n         * - if empty, NULL is reached right away\n         * remaining = n + 1\n         * - first n elements are parsed (or until a NULL is reached)\n         */\n        if (remaining) remaining++;\n        crawlers[sid].remaining = remaining;\n        crawlers[sid].slabs_clsid = sid;\n        crawlers[sid].reclaimed = 0;\n        crawlers[sid].unfetched = 0;\n        crawlers[sid].checked = 0;\n        do_item_linktail_q((item *)&crawlers[sid]);\n        crawler_count++;\n        starts++;\n    }\n    pthread_mutex_unlock(&lru_locks[sid]);\n    return starts;\n}\n\nstatic int lru_crawler_set_client(crawler_module_t *cm, void *c, const int sfd) {\n    crawler_client_t *crawlc = &cm->c;\n    if (crawlc->c != NULL) {\n        return -1;\n    }\n    crawlc->c = c;\n    crawlc->sfd = sfd;\n\n    size_t size = LRU_CRAWLER_MINBUFSPACE * 16;\n    crawlc->buf = malloc(size);\n\n    if (crawlc->buf == NULL) {\n        return -2;\n    }\n    crawlc->buflen = size;\n    crawlc->bufused = 0;\n    return 0;\n}\n\nint lru_crawler_start(uint8_t *ids, uint32_t remaining,\n                             const enum crawler_run_type type, void *data,\n                             void *c, const int sfd) {\n    int starts = 0;\n    bool is_running;\n    static rel_time_t block_ae_until = 0;\n    pthread_mutex_lock(&lru_crawler_lock);\n    STATS_LOCK();\n    is_running = stats_state.lru_crawler_running;\n    STATS_UNLOCK();\n    if (do_run_lru_crawler_thread == 0) {\n        pthread_mutex_unlock(&lru_crawler_lock);\n        return -2;\n    }\n\n    if (is_running &&\n            !(type == CRAWLER_AUTOEXPIRE && active_crawler_type == CRAWLER_AUTOEXPIRE)) {\n        pthread_mutex_unlock(&lru_crawler_lock);\n        block_ae_until = current_time + 60;\n        return -1;\n    }\n\n    if (type == CRAWLER_AUTOEXPIRE && block_ae_until > current_time) {\n        pthread_mutex_unlock(&lru_crawler_lock);\n        return -1;\n    }\n\n    /* hash table walk only supported with metadump for now. */\n    if (ids == NULL && type != CRAWLER_METADUMP && type != CRAWLER_MGDUMP) {\n        pthread_mutex_unlock(&lru_crawler_lock);\n        return -2;\n    }\n\n    /* Configure the module */\n    if (!is_running) {\n        assert(crawler_mod_regs[type] != NULL);\n        active_crawler_mod.mod = crawler_mod_regs[type];\n        active_crawler_type = type;\n        if (active_crawler_mod.mod->init != NULL) {\n            active_crawler_mod.mod->init(&active_crawler_mod, data);\n        }\n        if (active_crawler_mod.mod->needs_client) {\n            if (c == NULL || sfd == 0) {\n                pthread_mutex_unlock(&lru_crawler_lock);\n                return -2;\n            }\n            if (lru_crawler_set_client(&active_crawler_mod, c, sfd) != 0) {\n                pthread_mutex_unlock(&lru_crawler_lock);\n                return -2;\n            }\n        }\n    }\n\n    if (ids == NULL) {\n        /* NULL ids means to walk the hash table instead. */\n        starts = 1;\n        /* FIXME: hack to signal hash mode to the crawler thread.\n         * Something more clear would be nice.\n         */\n        crawler_count = -1;\n    } else {\n        /* we allow the autocrawler to restart sub-LRU's before completion */\n        for (int sid = POWER_SMALLEST; sid < POWER_LARGEST; sid++) {\n            if (ids[sid])\n                starts += do_lru_crawler_start(sid, remaining);\n        }\n    }\n    if (starts) {\n        STATS_LOCK();\n        stats_state.lru_crawler_running = true;\n        stats.lru_crawler_starts++;\n        STATS_UNLOCK();\n        pthread_cond_signal(&lru_crawler_cond);\n    }\n    pthread_mutex_unlock(&lru_crawler_lock);\n    return starts;\n}\n\n/*\n * Also only clear the crawlerstats once per sid.\n */\nenum crawler_result_type lru_crawler_crawl(char *slabs, const enum crawler_run_type type,\n        void *c, const int sfd, unsigned int remaining) {\n    char *b = NULL;\n    uint32_t sid = 0;\n    int starts = 0;\n    uint8_t tocrawl[POWER_LARGEST];\n    bool hash_crawl = false;\n\n    /* FIXME: I added this while debugging. Don't think it's needed? */\n    memset(tocrawl, 0, sizeof(uint8_t) * POWER_LARGEST);\n    if (strcmp(slabs, \"all\") == 0) {\n        for (sid = 0; sid < POWER_LARGEST; sid++) {\n            tocrawl[sid] = 1;\n        }\n    } else if (strcmp(slabs, \"hash\") == 0) {\n        hash_crawl = true;\n    } else {\n        for (char *p = strtok_r(slabs, \",\", &b);\n             p != NULL;\n             p = strtok_r(NULL, \",\", &b)) {\n\n            if (!safe_strtoul(p, &sid) || sid < POWER_SMALLEST\n                    || sid >= MAX_NUMBER_OF_SLAB_CLASSES) {\n                return CRAWLER_BADCLASS;\n            }\n            tocrawl[sid | TEMP_LRU] = 1;\n            tocrawl[sid | HOT_LRU] = 1;\n            tocrawl[sid | WARM_LRU] = 1;\n            tocrawl[sid | COLD_LRU] = 1;\n        }\n    }\n\n    starts = lru_crawler_start(hash_crawl ? NULL : tocrawl, remaining, type, NULL, c, sfd);\n    if (starts == -1) {\n        return CRAWLER_RUNNING;\n    } else if (starts == -2) {\n        return CRAWLER_ERROR; /* FIXME: not very helpful. */\n    } else if (starts) {\n        return CRAWLER_OK;\n    } else {\n        return CRAWLER_NOTSTARTED;\n    }\n}\n\n/* If we hold this lock, crawler can't wake up or move */\nvoid lru_crawler_pause(void) {\n    pthread_mutex_lock(&lru_crawler_lock);\n}\n\nvoid lru_crawler_resume(void) {\n    pthread_mutex_unlock(&lru_crawler_lock);\n}\n\nint init_lru_crawler(void *arg) {\n    if (lru_crawler_initialized == 0) {\n#ifdef EXTSTORE\n        storage = arg;\n#endif\n        active_crawler_mod.c.c = NULL;\n        active_crawler_mod.mod = NULL;\n        active_crawler_mod.data = NULL;\n        lru_crawler_initialized = 1;\n    }\n    return 0;\n}\n"
        },
        {
          "name": "crawler.h",
          "type": "blob",
          "size": 1.2900390625,
          "content": "#ifndef CRAWLER_H\n#define CRAWLER_H\n\n#define LRU_CRAWLER_CAP_REMAINING -1\n\ntypedef struct {\n    uint64_t histo[61];\n    uint64_t ttl_hourplus;\n    uint64_t noexp;\n    uint64_t reclaimed;\n    uint64_t seen;\n    rel_time_t start_time;\n    rel_time_t end_time;\n    bool run_complete;\n} crawlerstats_t;\n\nstruct crawler_expired_data {\n    pthread_mutex_t lock;\n    crawlerstats_t crawlerstats[POWER_LARGEST];\n    /* redundant with crawlerstats_t so we can get overall start/stop/done */\n    rel_time_t start_time;\n    rel_time_t end_time;\n    bool crawl_complete;\n    bool is_external; /* whether this was an alloc local or remote to the module. */\n};\n\nenum crawler_result_type {\n    CRAWLER_OK=0, CRAWLER_RUNNING, CRAWLER_BADCLASS, CRAWLER_NOTSTARTED, CRAWLER_ERROR\n};\nint start_item_crawler_thread(void);\n#define CRAWLER_WAIT true\n#define CRAWLER_NOWAIT false\nint stop_item_crawler_thread(bool wait);\nint init_lru_crawler(void *arg);\nenum crawler_result_type lru_crawler_crawl(char *slabs, enum crawler_run_type,\n        void *c, const int sfd, unsigned int remaining);\nint lru_crawler_start(uint8_t *ids, uint32_t remaining,\n                             const enum crawler_run_type type, void *data,\n                             void *c, const int sfd);\nvoid lru_crawler_pause(void);\nvoid lru_crawler_resume(void);\n\n#endif\n"
        },
        {
          "name": "crc32c.c",
          "type": "blob",
          "size": 17.453125,
          "content": "/* crc32c.c -- compute CRC-32C using the Intel crc32 instruction\n * Copyright (C) 2013, 2015, 2021 Mark Adler\n * Version 1.4  31 May 2021  Mark Adler\n */\n\n/*\n  This software is provided 'as-is', without any express or implied\n  warranty.  In no event will the author be held liable for any damages\n  arising from the use of this software.\n\n  Permission is granted to anyone to use this software for any purpose,\n  including commercial applications, and to alter it and redistribute it\n  freely, subject to the following restrictions:\n\n  1. The origin of this software must not be misrepresented; you must not\n     claim that you wrote the original software. If you use this software\n     in a product, an acknowledgment in the product documentation would be\n     appreciated but is not required.\n  2. Altered source versions must be plainly marked as such, and must not be\n     misrepresented as being the original software.\n  3. This notice may not be removed or altered from any source distribution.\n\n  Mark Adler\n  madler@alumni.caltech.edu\n */\n\n/* Use hardware CRC instruction on Intel SSE 4.2 processors.  This computes a\n   CRC-32C, *not* the CRC-32 used by Ethernet and zip, gzip, etc.  A software\n   version is provided as a fall-back, as well as for speed comparisons. */\n\n/* Version history:\n   1.0  10 Feb 2013  First version\n   1.1   1 Aug 2013  Correct comments on why three crc instructions in parallel\n   1.2   1 Nov 2015  Add const qualifier to avoid compiler warning\n                     Load entire input into memory (test code)\n                     Argument gives number of times to repeat (test code)\n                     Argument < 0 forces software implementation (test code)\n   1.3  31 Dec 2015  Check for Intel architecture using compiler macro\n                     Support big-endian processors in software calculation\n                     Add header for external use\n   1.4  31 May 2021  Correct register constraints on assembly instructions\n */\n\n#include <pthread.h>\n#include \"crc32c.h\"\n\ncrc_func crc32c;\n\n/* CRC-32C (iSCSI) polynomial in reversed bit order. */\n#define POLY 0x82f63b78\n\nuint32_t crc32c_sw_little(uint32_t crc, void const *buf, size_t len);\nuint32_t crc32c_sw_big(uint32_t crc, void const *buf, size_t len);\n#ifdef __x86_64__\n\n/* Hardware CRC-32C for Intel and compatible processors. */\n\n/* Multiply a matrix times a vector over the Galois field of two elements,\n   GF(2).  Each element is a bit in an unsigned integer.  mat must have at\n   least as many entries as the power of two for most significant one bit in\n   vec. */\nstatic inline uint32_t gf2_matrix_times(uint32_t *mat, uint32_t vec) {\n    uint32_t sum = 0;\n    while (vec) {\n        if (vec & 1)\n            sum ^= *mat;\n        vec >>= 1;\n        mat++;\n    }\n    return sum;\n}\n\n/* Multiply a matrix by itself over GF(2).  Both mat and square must have 32\n   rows. */\nstatic inline void gf2_matrix_square(uint32_t *square, uint32_t *mat) {\n    for (unsigned n = 0; n < 32; n++)\n        square[n] = gf2_matrix_times(mat, mat[n]);\n}\n\n/* Construct an operator to apply len zeros to a crc.  len must be a power of\n   two.  If len is not a power of two, then the result is the same as for the\n   largest power of two less than len.  The result for len == 0 is the same as\n   for len == 1.  A version of this routine could be easily written for any\n   len, but that is not needed for this application. */\nstatic void crc32c_zeros_op(uint32_t *even, size_t len) {\n    uint32_t odd[32];       /* odd-power-of-two zeros operator */\n\n    /* put operator for one zero bit in odd */\n    odd[0] = POLY;              /* CRC-32C polynomial */\n    uint32_t row = 1;\n    for (unsigned n = 1; n < 32; n++) {\n        odd[n] = row;\n        row <<= 1;\n    }\n\n    /* put operator for two zero bits in even */\n    gf2_matrix_square(even, odd);\n\n    /* put operator for four zero bits in odd */\n    gf2_matrix_square(odd, even);\n\n    /* first square will put the operator for one zero byte (eight zero bits),\n       in even -- next square puts operator for two zero bytes in odd, and so\n       on, until len has been rotated down to zero */\n    do {\n        gf2_matrix_square(even, odd);\n        len >>= 1;\n        if (len == 0)\n            return;\n        gf2_matrix_square(odd, even);\n        len >>= 1;\n    } while (len);\n\n    /* answer ended up in odd -- copy to even */\n    for (unsigned n = 0; n < 32; n++)\n        even[n] = odd[n];\n}\n\n/* Take a length and build four lookup tables for applying the zeros operator\n   for that length, byte-by-byte on the operand. */\nstatic void crc32c_zeros(uint32_t zeros[][256], size_t len) {\n    uint32_t op[32];\n\n    crc32c_zeros_op(op, len);\n    for (unsigned n = 0; n < 256; n++) {\n        zeros[0][n] = gf2_matrix_times(op, n);\n        zeros[1][n] = gf2_matrix_times(op, n << 8);\n        zeros[2][n] = gf2_matrix_times(op, n << 16);\n        zeros[3][n] = gf2_matrix_times(op, n << 24);\n    }\n}\n\n/* Apply the zeros operator table to crc. */\nstatic inline uint32_t crc32c_shift(uint32_t zeros[][256], uint32_t crc) {\n    return zeros[0][crc & 0xff] ^ zeros[1][(crc >> 8) & 0xff] ^\n           zeros[2][(crc >> 16) & 0xff] ^ zeros[3][crc >> 24];\n}\n\n/* Block sizes for three-way parallel crc computation.  LONG and SHORT must\n   both be powers of two.  The associated string constants must be set\n   accordingly, for use in constructing the assembler instructions. */\n#define LONG 8192\n#define LONGx1 \"8192\"\n#define LONGx2 \"16384\"\n#define SHORT 256\n#define SHORTx1 \"256\"\n#define SHORTx2 \"512\"\n\n/* Tables for hardware crc that shift a crc by LONG and SHORT zeros. */\nstatic pthread_once_t crc32c_once_hw = PTHREAD_ONCE_INIT;\nstatic uint32_t crc32c_long[4][256];\nstatic uint32_t crc32c_short[4][256];\n\n/* Initialize tables for shifting crcs. */\nstatic void crc32c_init_hw(void) {\n    crc32c_zeros(crc32c_long, LONG);\n    crc32c_zeros(crc32c_short, SHORT);\n}\n\n/* Compute CRC-32C using the Intel hardware instruction. */\nstatic uint32_t crc32c_hw(uint32_t crc, void const *buf, size_t len) {\n    /* populate shift tables the first time through */\n    pthread_once(&crc32c_once_hw, crc32c_init_hw);\n\n    /* pre-process the crc */\n    crc = ~crc;\n    uint64_t crc0 = crc;            /* 64-bits for crc32q instruction */\n\n    /* compute the crc for up to seven leading bytes to bring the data pointer\n       to an eight-byte boundary */\n    unsigned char const *next = buf;\n    while (len && ((uintptr_t)next & 7) != 0) {\n        __asm__(\"crc32b\\t\" \"(%1), %0\"\n                : \"+r\"(crc0)\n                : \"r\"(next), \"m\"(*next));\n        next++;\n        len--;\n    }\n\n    /* compute the crc on sets of LONG*3 bytes, executing three independent crc\n       instructions, each on LONG bytes -- this is optimized for the Nehalem,\n       Westmere, Sandy Bridge, and Ivy Bridge architectures, which have a\n       throughput of one crc per cycle, but a latency of three cycles */\n    while (len >= LONG*3) {\n        uint64_t crc1 = 0;\n        uint64_t crc2 = 0;\n        unsigned char const * const end = next + LONG;\n        do {\n            __asm__(\"crc32q\\t\" \"(%3), %0\\n\\t\"\n                    \"crc32q\\t\" LONGx1 \"(%3), %1\\n\\t\"\n                    \"crc32q\\t\" LONGx2 \"(%3), %2\"\n                    : \"+r\"(crc0), \"+r\"(crc1), \"+r\"(crc2)\n                    : \"r\"(next), \"m\"(*next));\n            next += 8;\n        } while (next < end);\n        crc0 = crc32c_shift(crc32c_long, crc0) ^ crc1;\n        crc0 = crc32c_shift(crc32c_long, crc0) ^ crc2;\n        next += LONG*2;\n        len -= LONG*3;\n    }\n\n    /* do the same thing, but now on SHORT*3 blocks for the remaining data less\n       than a LONG*3 block */\n    while (len >= SHORT*3) {\n        uint64_t crc1 = 0;\n        uint64_t crc2 = 0;\n        unsigned char const * const end = next + SHORT;\n        do {\n            __asm__(\"crc32q\\t\" \"(%3), %0\\n\\t\"\n                    \"crc32q\\t\" SHORTx1 \"(%3), %1\\n\\t\"\n                    \"crc32q\\t\" SHORTx2 \"(%3), %2\"\n                    : \"+r\"(crc0), \"+r\"(crc1), \"+r\"(crc2)\n                    : \"r\"(next), \"m\"(*next));\n            next += 8;\n        } while (next < end);\n        crc0 = crc32c_shift(crc32c_short, crc0) ^ crc1;\n        crc0 = crc32c_shift(crc32c_short, crc0) ^ crc2;\n        next += SHORT*2;\n        len -= SHORT*3;\n    }\n\n    /* compute the crc on the remaining eight-byte units less than a SHORT*3\n       block */\n    {\n        unsigned char const * const end = next + (len - (len & 7));\n        while (next < end) {\n            __asm__(\"crc32q\\t\" \"(%1), %0\"\n                    : \"+r\"(crc0)\n                    : \"r\"(next), \"m\"(*next));\n            next += 8;\n        }\n        len &= 7;\n    }\n\n    /* compute the crc for up to seven trailing bytes */\n    while (len) {\n        __asm__(\"crc32b\\t\" \"(%1), %0\"\n                : \"+r\"(crc0)\n                : \"r\"(next), \"m\"(*next));\n        next++;\n        len--;\n    }\n\n    /* return a post-processed crc */\n    return ~crc0;\n}\n\n/* Check for SSE 4.2.  SSE 4.2 was first supported in Nehalem processors\n   introduced in November, 2008.  This does not check for the existence of the\n   cpuid instruction itself, which was introduced on the 486SL in 1992, so this\n   will fail on earlier x86 processors.  cpuid works on all Pentium and later\n   processors. */\n#define SSE42(have) \\\n    do { \\\n        uint32_t eax, ecx; \\\n        eax = 1; \\\n        __asm__(\"cpuid\" \\\n                : \"=c\"(ecx) \\\n                : \"a\"(eax) \\\n                : \"%ebx\", \"%edx\"); \\\n        (have) = (ecx >> 20) & 1; \\\n    } while (0)\n\n/* Compute a CRC-32C.  If the crc32 instruction is available, use the hardware\n   version.  Otherwise, use the software version. */\nvoid crc32c_init(void) {\n    int sse42;\n\n    SSE42(sse42);\n    if (sse42) {\n        crc32c = crc32c_hw;\n    } else {\n        crc32c = crc32c_sw;\n    }\n}\n\n#elif defined(__aarch64__) && (defined(__linux__) || defined(__APPLE__))\n#if defined(__linux__) && defined(HAVE_SYS_AUX_H)\n#include <sys/auxv.h>\n#elif defined(__APPLE__)\n#include <sys/sysctl.h>\n#endif\n\n#if defined(HWCAP_CRC32)\nstatic inline uint32_t crc32cx(uint32_t crc, const uint64_t data)\n{\n        asm(\".arch_extension crc\\n\"\n        \"crc32cx %w0, %w0, %x1\" : \"+r\" (crc) : \"r\" (data));\n        return crc;\n}\n\nstatic inline uint32_t crc32cb(uint32_t crc, const uint8_t data)\n{\n        asm(\".arch_extension crc\\n\"\n            \"crc32cb %w0, %w0, %w1\" : \"+r\" (crc) : \"r\" (data));\n        return crc;\n}\n\nstatic uint32_t crc32c_hw(uint32_t crc, void const *buf, size_t len) {\n    crc = ~crc;\n    unsigned char const *next = buf;\n\n    while (((uintptr_t)next & 7) && len > 0) {\n        crc = crc32cb(crc, *(uint8_t *)next);\n        next++;\n        len--;\n    }\n\n    while (len >= 64) {\n        uint64_t *next8 = (uint64_t *)next;\n        crc = crc32cx(crc, next8[0]);\n        crc = crc32cx(crc, next8[1]);\n        crc = crc32cx(crc, next8[2]);\n        crc = crc32cx(crc, next8[3]);\n        crc = crc32cx(crc, next8[4]);\n        crc = crc32cx(crc, next8[5]);\n        crc = crc32cx(crc, next8[6]);\n        crc = crc32cx(crc, next8[7]);\n        next += 64;\n        len -= 64;\n    }\n\n    while (len >= 8) {\n        crc = crc32cx(crc, *(uint64_t *)next);\n        next += 8;\n        len -= 8;\n    }\n\n    while (len > 0) {\n        crc = crc32cb(crc, *(uint8_t *)next);\n        next++;\n        len--;\n    }\n\n    return ~crc;\n}\n\nvoid crc32c_init(void) {\n#if defined(__linux__)\n    uint64_t auxv = getauxval(AT_HWCAP);\n\n    crc32c = crc32c_sw;\n    if (auxv & HWCAP_CRC32)\n        crc32c = crc32c_hw;\n#elif defined(__APPLE__)\n    int armv8_crc32;\n    size_t size = sizeof(armv8_crc32);\n\n    if (sysctlbyname(\"hw.optional.armv8_crc32\", &armv8_crc32, &size, NULL, 0) == 0 &&\n       armv8_crc32 == 1)\n        crc32c = crc32c_hw;\n#endif\n}\n#else /* no hw crc32 on arm64 system supported? old compiler/libc/kernel? */\nvoid crc32c_init(void) {\n    crc32c = crc32c_sw;\n}\n#endif\n#else /* !__x86_64__i && !__aarch64__ */\nvoid crc32c_init(void) {\n    crc32c = crc32c_sw;\n}\n\n#endif\n\n/* Construct table for software CRC-32C little-endian calculation. */\nstatic pthread_once_t crc32c_once_little = PTHREAD_ONCE_INIT;\nstatic uint32_t crc32c_table_little[8][256];\nstatic void crc32c_init_sw_little(void) {\n    for (unsigned n = 0; n < 256; n++) {\n        uint32_t crc = n;\n        crc = crc & 1 ? (crc >> 1) ^ POLY : crc >> 1;\n        crc = crc & 1 ? (crc >> 1) ^ POLY : crc >> 1;\n        crc = crc & 1 ? (crc >> 1) ^ POLY : crc >> 1;\n        crc = crc & 1 ? (crc >> 1) ^ POLY : crc >> 1;\n        crc = crc & 1 ? (crc >> 1) ^ POLY : crc >> 1;\n        crc = crc & 1 ? (crc >> 1) ^ POLY : crc >> 1;\n        crc = crc & 1 ? (crc >> 1) ^ POLY : crc >> 1;\n        crc = crc & 1 ? (crc >> 1) ^ POLY : crc >> 1;\n        crc32c_table_little[0][n] = crc;\n    }\n    for (unsigned n = 0; n < 256; n++) {\n        uint32_t crc = crc32c_table_little[0][n];\n        for (unsigned k = 1; k < 8; k++) {\n            crc = crc32c_table_little[0][crc & 0xff] ^ (crc >> 8);\n            crc32c_table_little[k][n] = crc;\n        }\n    }\n}\n\n/* Compute a CRC-32C in software assuming a little-endian architecture,\n   constructing the required table if that hasn't already been done. */\nuint32_t crc32c_sw_little(uint32_t crc, void const *buf, size_t len) {\n    unsigned char const *next = buf;\n\n    pthread_once(&crc32c_once_little, crc32c_init_sw_little);\n    crc = ~crc;\n    while (len && ((uintptr_t)next & 7) != 0) {\n        crc = crc32c_table_little[0][(crc ^ *next++) & 0xff] ^ (crc >> 8);\n        len--;\n    }\n    if (len >= 8) {\n        uint64_t crcw = crc;\n        do {\n            crcw ^= *(uint64_t const *)next;\n            crcw = crc32c_table_little[7][crcw & 0xff] ^\n                   crc32c_table_little[6][(crcw >> 8) & 0xff] ^\n                   crc32c_table_little[5][(crcw >> 16) & 0xff] ^\n                   crc32c_table_little[4][(crcw >> 24) & 0xff] ^\n                   crc32c_table_little[3][(crcw >> 32) & 0xff] ^\n                   crc32c_table_little[2][(crcw >> 40) & 0xff] ^\n                   crc32c_table_little[1][(crcw >> 48) & 0xff] ^\n                   crc32c_table_little[0][crcw >> 56];\n            next += 8;\n            len -= 8;\n        } while (len >= 8);\n        crc = crcw;\n    }\n    while (len) {\n        crc = crc32c_table_little[0][(crc ^ *next++) & 0xff] ^ (crc >> 8);\n        len--;\n    }\n    return ~crc;\n}\n\n/* Swap the bytes in a uint64_t.  (Only for big-endian.) */\n#if defined(__has_builtin) || (defined(__GNUC__) && \\\n    (__GNUC__ > 4 || (__GNUC__ == 4 && __GNUC_MINOR__ >= 3)))\n#  define swap __builtin_bswap64\n#else\nstatic inline uint64_t swap(uint64_t x) {\n    x = ((x << 8) & 0xff00ff00ff00ff00) | ((x >> 8) & 0xff00ff00ff00ff);\n    x = ((x << 16) & 0xffff0000ffff0000) | ((x >> 16) & 0xffff0000ffff);\n    return (x << 32) | (x >> 32);\n}\n#endif\n\n/* Construct tables for software CRC-32C big-endian calculation. */\nstatic pthread_once_t crc32c_once_big = PTHREAD_ONCE_INIT;\nstatic uint32_t crc32c_table_big_byte[256];\nstatic uint64_t crc32c_table_big[8][256];\nstatic void crc32c_init_sw_big(void) {\n    for (unsigned n = 0; n < 256; n++) {\n        uint32_t crc = n;\n        crc = crc & 1 ? (crc >> 1) ^ POLY : crc >> 1;\n        crc = crc & 1 ? (crc >> 1) ^ POLY : crc >> 1;\n        crc = crc & 1 ? (crc >> 1) ^ POLY : crc >> 1;\n        crc = crc & 1 ? (crc >> 1) ^ POLY : crc >> 1;\n        crc = crc & 1 ? (crc >> 1) ^ POLY : crc >> 1;\n        crc = crc & 1 ? (crc >> 1) ^ POLY : crc >> 1;\n        crc = crc & 1 ? (crc >> 1) ^ POLY : crc >> 1;\n        crc = crc & 1 ? (crc >> 1) ^ POLY : crc >> 1;\n        crc32c_table_big_byte[n] = crc;\n    }\n    for (unsigned n = 0; n < 256; n++) {\n        uint32_t crc = crc32c_table_big_byte[n];\n        crc32c_table_big[0][n] = swap(crc);\n        for (unsigned k = 1; k < 8; k++) {\n            crc = crc32c_table_big_byte[crc & 0xff] ^ (crc >> 8);\n            crc32c_table_big[k][n] = swap(crc);\n        }\n    }\n}\n\n/* Compute a CRC-32C in software assuming a big-endian architecture,\n   constructing the required tables if that hasn't already been done. */\nuint32_t crc32c_sw_big(uint32_t crc, void const *buf, size_t len) {\n    unsigned char const *next = buf;\n\n    pthread_once(&crc32c_once_big, crc32c_init_sw_big);\n    crc = ~crc;\n    while (len && ((uintptr_t)next & 7) != 0) {\n        crc = crc32c_table_big_byte[(crc ^ *next++) & 0xff] ^ (crc >> 8);\n        len--;\n    }\n    if (len >= 8) {\n        uint64_t crcw = swap(crc);\n        do {\n            crcw ^= *(uint64_t const *)next;\n            crcw = crc32c_table_big[0][crcw & 0xff] ^\n                   crc32c_table_big[1][(crcw >> 8) & 0xff] ^\n                   crc32c_table_big[2][(crcw >> 16) & 0xff] ^\n                   crc32c_table_big[3][(crcw >> 24) & 0xff] ^\n                   crc32c_table_big[4][(crcw >> 32) & 0xff] ^\n                   crc32c_table_big[5][(crcw >> 40) & 0xff] ^\n                   crc32c_table_big[6][(crcw >> 48) & 0xff] ^\n                   crc32c_table_big[7][(crcw >> 56)];\n            next += 8;\n            len -= 8;\n        } while (len >= 8);\n        crc = swap(crcw);\n    }\n    while (len) {\n        crc = crc32c_table_big_byte[(crc ^ *next++) & 0xff] ^ (crc >> 8);\n        len--;\n    }\n    return ~crc;\n}\n\n/* Table-driven software CRC-32C.  This is about 15 times slower than using the\n   hardware instructions.  Determine the endianess of the processor and proceed\n   accordingly.  Ideally the endianess will be determined at compile time, in\n   which case the unused functions and tables for the other endianess will be\n   removed by the optimizer.  If not, then the proper routines and tables will\n   be used, even if the endianess is changed mid-stream.  (Yes, there are\n   processors that permit that -- go figure.) */\nuint32_t crc32c_sw(uint32_t crc, void const *buf, size_t len) {\n    static int const little = 1;\n    if (*(char const *)&little)\n        return crc32c_sw_little(crc, buf, len);\n    else\n        return crc32c_sw_big(crc, buf, len);\n}\n"
        },
        {
          "name": "crc32c.h",
          "type": "blob",
          "size": 0.751953125,
          "content": "#ifndef CRC32C_H\n#define    CRC32C_H\n\n\n// crc32c.h -- header for crc32c.c\n// Copyright (C) 2015 Mark Adler\n// See crc32c.c for the license.\n\n#include <stdint.h>\n\n// Return the CRC-32C of buf[0..len-1] given the starting CRC crc.  This can be\n// used to calculate the CRC of a sequence of bytes a chunk at a time, using\n// the previously returned crc in the next call.  The first call must be with\n// crc == 0.  crc32c() uses the Intel crc32 hardware instruction if available.\ntypedef uint32_t (*crc_func)(uint32_t crc, const void *buf, size_t len);\nextern crc_func crc32c;\n\nvoid crc32c_init(void);\n\n// Expose a prototype for the crc32c software variant simply for testing purposes\nuint32_t crc32c_sw(uint32_t crc, void const *buf, size_t len);\n\n#endif    /* CRC32C_H */\n"
        },
        {
          "name": "daemon.c",
          "type": "blob",
          "size": 3.0224609375,
          "content": "/*    $Header: /cvsroot/wikipedia/willow/src/bin/willow/daemon.c,v 1.1 2005/05/02 19:15:21 kateturner Exp $    */\n/*    $NetBSD: daemon.c,v 1.9 2003/08/07 16:42:46 agc Exp $    */\n/*-\n * Copyright (c) 1990, 1993\n *    The Regents of the University of California.  All rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions\n * are met:\n * 1. Redistributions of source code must retain the above copyright\n *    notice, this list of conditions and the following disclaimer.\n * 2. Redistributions in binary form must reproduce the above copyright\n *    notice, this list of conditions and the following disclaimer in the\n *    documentation and/or other materials provided with the distribution.\n * 3. Neither the name of the University nor the names of its contributors\n *    may be used to endorse or promote products derived from this software\n *    without specific prior written permission.\n *\n * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND\n * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE\n * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS\n * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)\n * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT\n * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY\n * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF\n * SUCH DAMAGE.\n */\n\n#if defined __SUNPRO_C || defined __DECC || defined __HP_cc\n# pragma ident \"@(#)$Header: /cvsroot/wikipedia/willow/src/bin/willow/daemon.c,v 1.1 2005/05/02 19:15:21 kateturner Exp $\"\n# pragma ident \"$NetBSD: daemon.c,v 1.9 2003/08/07 16:42:46 agc Exp $\"\n#endif\n\n#include <fcntl.h>\n#include <stdio.h>\n#include <stdlib.h>\n#include <unistd.h>\n\n#include \"memcached.h\"\n\nint daemonize(int nochdir, int noclose)\n{\n    int fd;\n\n    switch (fork()) {\n    case -1:\n        return (-1);\n    case 0:\n        break;\n    default:\n        _exit(EXIT_SUCCESS);\n    }\n\n    if (setsid() == -1)\n        return (-1);\n\n    if (nochdir == 0) {\n        if(chdir(\"/\") != 0) {\n            perror(\"chdir\");\n            return (-1);\n        }\n    }\n\n    if (noclose == 0 && (fd = open(\"/dev/null\", O_RDWR, 0)) != -1) {\n        if(dup2(fd, STDIN_FILENO) < 0) {\n            perror(\"dup2 stdin\");\n            goto err_cleanup;\n        }\n        if(dup2(fd, STDOUT_FILENO) < 0) {\n            perror(\"dup2 stdout\");\n            goto err_cleanup;\n        }\n        if(dup2(fd, STDERR_FILENO) < 0) {\n            perror(\"dup2 stderr\");\n            goto err_cleanup;\n        }\n\n        if(close(fd) < 0) {\n            perror(\"close\");\n            return (-1);\n        }\n    }\n    return (0);\n\n    err_cleanup:\n        close(fd);\n        return (-1);\n}\n"
        },
        {
          "name": "darwin_priv.c",
          "type": "blob",
          "size": 0.6962890625,
          "content": "#include <stdlib.h>\n#include <stdio.h>\n#include <string.h>\n#include <sandbox.h>\n#include \"memcached.h\"\n\n#pragma clang diagnostic push\n#pragma clang diagnostic ignored \"-Wdeprecated-declarations\"\n/*\n * the sandbox api is marked deprecated, however still used\n * by couple of major softwares/libraries like openssh\n */\nvoid drop_privileges(void) {\n    extern char *__progname;\n    char *error = NULL;\n\n    if (sandbox_init(kSBXProfileNoInternet, SANDBOX_NAMED, &error) < 0) {\n        fprintf(stderr, \"%s: sandbox_init: %s\\n\", __progname, error);\n        sandbox_free_error(error);\n        exit(EXIT_FAILURE);\n    }\n}\n\n#pragma clang diagnostic pop\n\nvoid setup_privilege_violations_handler(void) {\n   // not needed\n}\n"
        },
        {
          "name": "devtools",
          "type": "tree",
          "content": null
        },
        {
          "name": "doc",
          "type": "tree",
          "content": null
        },
        {
          "name": "docker-compose.yml",
          "type": "blob",
          "size": 0.51171875,
          "content": "version: '3'\nservices:\n    alpine:\n        build:\n            context: .\n            dockerfile: devtools/Dockerfile.alpine\n    ubuntu:\n        build:\n            context: .\n            dockerfile: devtools/Dockerfile.ubuntu\n    debian:\n        build:\n            context: .\n            dockerfile: devtools/Dockerfile.debian\n    arch:\n        build:\n            context: .\n            dockerfile: devtools/Dockerfile.arch\n    fedora:\n        build:\n            context: .\n            dockerfile: devtools/Dockerfile.fedora\n"
        },
        {
          "name": "extstore.c",
          "type": "blob",
          "size": 31.015625,
          "content": "/* -*- Mode: C; tab-width: 4; c-basic-offset: 4; indent-tabs-mode: nil -*- */\n\n#include \"config.h\"\n// FIXME: config.h?\n#include <stdint.h>\n#include <stdbool.h>\n// end FIXME\n#include <stdlib.h>\n#include <limits.h>\n#include <pthread.h>\n#include <sys/types.h>\n#include <sys/stat.h>\n#include <sys/uio.h>\n#include <fcntl.h>\n#include <unistd.h>\n#include <stdio.h>\n#include <string.h>\n#include <assert.h>\n#include \"extstore.h\"\n\n// TODO: better if an init option turns this on/off.\n#ifdef EXTSTORE_DEBUG\n#define E_DEBUG(...) \\\n    do { \\\n        fprintf(stderr, __VA_ARGS__); \\\n    } while (0)\n#else\n#define E_DEBUG(...)\n#endif\n\n#define STAT_L(e) pthread_mutex_lock(&e->stats_mutex);\n#define STAT_UL(e) pthread_mutex_unlock(&e->stats_mutex);\n#define STAT_INCR(e, stat, amount) { \\\n    pthread_mutex_lock(&e->stats_mutex); \\\n    e->stats.stat += amount; \\\n    pthread_mutex_unlock(&e->stats_mutex); \\\n}\n\n#define STAT_DECR(e, stat, amount) { \\\n    pthread_mutex_lock(&e->stats_mutex); \\\n    e->stats.stat -= amount; \\\n    pthread_mutex_unlock(&e->stats_mutex); \\\n}\n\ntypedef struct __store_wbuf {\n    struct __store_wbuf *next;\n    char *buf;\n    char *buf_pos;\n    unsigned int free;\n    unsigned int size;\n    unsigned int offset; /* offset into page this write starts at */\n    bool full; /* done writing to this page */\n    bool flushed; /* whether wbuf has been flushed to disk */\n} _store_wbuf;\n\ntypedef struct _store_page {\n    pthread_mutex_t mutex; /* Need to be held for most operations */\n    uint64_t obj_count; /* _delete can decrease post-closing */\n    uint64_t bytes_used; /* _delete can decrease post-closing */\n    uint64_t offset; /* starting address of page within fd */\n    unsigned int version;\n    unsigned int refcount;\n    unsigned int allocated;\n    unsigned int written; /* item offsets can be past written if wbuf not flushed */\n    unsigned int bucket; /* which bucket the page is linked into */\n    unsigned int free_bucket; /* which bucket this page returns to when freed */\n    int fd;\n    unsigned short id;\n    bool active; /* actively being written to */\n    bool closed; /* closed and draining before free */\n    bool free; /* on freelist */\n    _store_wbuf *wbuf; /* currently active wbuf from the stack */\n    struct _store_page *next;\n} store_page;\n\ntypedef struct store_engine store_engine;\ntypedef struct {\n    pthread_mutex_t mutex;\n    pthread_cond_t cond;\n    obj_io *queue;\n    obj_io *queue_tail;\n    store_engine *e;\n    unsigned int depth; // queue depth\n} store_io_thread;\n\n// sub-struct for maintenance related tasks.\nstruct store_maint {\n    pthread_mutex_t mutex;\n};\n\nstruct store_engine {\n    pthread_mutex_t mutex; /* covers internal stacks and variables */\n    store_page *pages; /* directly addressable page list */\n    _store_wbuf *wbuf_stack; /* wbuf freelist */\n    obj_io *io_stack; /* IO's to use with submitting wbuf's */\n    store_io_thread *io_threads;\n    store_io_thread *bg_thread; /* dedicated thread for write submit / compact ops */\n    store_page **page_buckets; /* stack of pages currently allocated to each bucket */\n    store_page **free_page_buckets; /* stack of use-case isolated free pages */\n    size_t page_size;\n    unsigned int version; /* global version counter */\n    unsigned int last_io_thread; /* round robin the IO threads */\n    unsigned int io_threadcount; /* count of IO threads */\n    unsigned int page_count;\n    unsigned int page_free; /* unallocated pages */\n    unsigned int page_bucketcount; /* count of potential page buckets */\n    unsigned int free_page_bucketcount; /* count of free page buckets */\n    unsigned int io_depth; /* FIXME: Might cache into thr struct */\n    pthread_mutex_t stats_mutex;\n    struct extstore_stats stats;\n    struct store_maint maint;\n};\n\n// FIXME: code is duplicated from thread.c since extstore.c doesn't pull in\n// the memcached ecosystem. worth starting a cross-utility header with static\n// definitions/macros?\n// keeping a minimal func here for now.\n#define THR_NAME_MAXLEN 16\nstatic void thread_setname(pthread_t thread, const char *name) {\nassert(strlen(name) < THR_NAME_MAXLEN);\n#if defined(__linux__) && defined(HAVE_PTHREAD_SETNAME_NP)\npthread_setname_np(thread, name);\n#endif\n}\n#undef THR_NAME_MAXLEN\n\nstatic _store_wbuf *wbuf_new(size_t size) {\n    _store_wbuf *b = calloc(1, sizeof(_store_wbuf));\n    if (b == NULL)\n        return NULL;\n    b->buf = calloc(size, sizeof(char));\n    if (b->buf == NULL) {\n        free(b);\n        return NULL;\n    }\n    b->buf_pos = b->buf;\n    b->free = size;\n    b->size = size;\n    return b;\n}\n\nstatic store_io_thread *_get_io_thread(store_engine *e) {\n    int tid = -1;\n    long long int low = LLONG_MAX;\n    pthread_mutex_lock(&e->mutex);\n    // find smallest queue. ignoring lock since being wrong isn't fatal.\n    // TODO: if average queue depth can be quickly tracked, can break as soon\n    // as we see a thread that's less than average, and start from last_io_thread\n    for (int x = 0; x < e->io_threadcount; x++) {\n        if (e->io_threads[x].depth == 0) {\n            tid = x;\n            break;\n        } else if (e->io_threads[x].depth < low) {\n                tid = x;\n            low = e->io_threads[x].depth;\n        }\n    }\n    pthread_mutex_unlock(&e->mutex);\n\n    return &e->io_threads[tid];\n}\n\nstatic uint64_t _next_version(store_engine *e) {\n    return e->version++;\n}\n// internal only method for freeing a page up\nstatic void _free_page(store_engine *e, store_page *p);\n\nstatic void *extstore_io_thread(void *arg);\n\n/* Copies stats internal to engine and computes any derived values */\nvoid extstore_get_stats(void *ptr, struct extstore_stats *st) {\n    store_engine *e = (store_engine *)ptr;\n    STAT_L(e);\n    memcpy(st, &e->stats, sizeof(struct extstore_stats));\n    STAT_UL(e);\n\n    // grab pages_free/pages_used\n    pthread_mutex_lock(&e->mutex);\n    st->pages_free = e->page_free;\n    st->pages_used = e->page_count - e->page_free;\n    pthread_mutex_unlock(&e->mutex);\n    st->io_queue = 0;\n    for (int x = 0; x < e->io_threadcount; x++) {\n        pthread_mutex_lock(&e->io_threads[x].mutex);\n        st->io_queue += e->io_threads[x].depth;\n        pthread_mutex_unlock(&e->io_threads[x].mutex);\n    }\n    // calculate bytes_fragmented.\n    // note that open and yet-filled pages count against fragmentation.\n    st->bytes_fragmented = st->pages_used * e->page_size -\n        st->bytes_used;\n}\n\nvoid extstore_get_page_data(void *ptr, struct extstore_stats *st) {\n    store_engine *e = (store_engine *)ptr;\n    pthread_mutex_lock(&e->maint.mutex);\n    struct extstore_page_data *pd = st->page_data;\n\n    for (int i = 0; i < e->page_count; i++) {\n        store_page *p = &e->pages[i];\n        pthread_mutex_lock(&p->mutex);\n\n        pd[p->id].free_bucket = p->free_bucket;\n        pd[p->id].version = p->version;\n        pd[p->id].bytes_used = p->bytes_used;\n        if (p->active) {\n            pd[p->id].active = true;\n        }\n        if (p->active || p->free) {\n            pthread_mutex_unlock(&p->mutex);\n            continue;\n        }\n        if (p->obj_count > 0 && !p->closed) {\n            pd[p->id].bucket = p->bucket;\n        }\n        if ((p->obj_count == 0 || p->closed) && p->refcount == 0) {\n            _free_page(e, p);\n        }\n        pthread_mutex_unlock(&p->mutex);\n    }\n\n    pthread_mutex_unlock(&e->maint.mutex);\n}\n\nconst char *extstore_err(enum extstore_res res) {\n    const char *rv = \"unknown error\";\n    switch (res) {\n        case EXTSTORE_INIT_BAD_WBUF_SIZE:\n            rv = \"page_size must be divisible by wbuf_size\";\n            break;\n        case EXTSTORE_INIT_NEED_MORE_WBUF:\n            rv = \"wbuf_count must be >= page_buckets\";\n            break;\n        case EXTSTORE_INIT_NEED_MORE_BUCKETS:\n            rv = \"page_buckets must be > 0\";\n            break;\n        case EXTSTORE_INIT_PAGE_WBUF_ALIGNMENT:\n            rv = \"page_size and wbuf_size must be divisible by 1024*1024*2\";\n            break;\n        case EXTSTORE_INIT_TOO_MANY_PAGES:\n            rv = \"page_count must total to < 65536. Increase page_size or lower path sizes\";\n            break;\n        case EXTSTORE_INIT_OOM:\n            rv = \"failed calloc for engine\";\n            break;\n        case EXTSTORE_INIT_OPEN_FAIL:\n            rv = \"failed to open file\";\n            break;\n        case EXTSTORE_INIT_THREAD_FAIL:\n            break;\n    }\n    return rv;\n}\n\n// TODO: #define's for DEFAULT_BUCKET, FREE_VERSION, etc\nvoid *extstore_init(struct extstore_conf_file *fh, struct extstore_conf *cf,\n        enum extstore_res *res) {\n    int i;\n    struct extstore_conf_file *f = NULL;\n    pthread_t thread;\n\n    if (cf->page_size % cf->wbuf_size != 0) {\n        *res = EXTSTORE_INIT_BAD_WBUF_SIZE;\n        return NULL;\n    }\n    // Should ensure at least one write buffer per potential page\n    if (cf->page_buckets > cf->wbuf_count) {\n        *res = EXTSTORE_INIT_NEED_MORE_WBUF;\n        return NULL;\n    }\n    if (cf->page_buckets < 1) {\n        *res = EXTSTORE_INIT_NEED_MORE_BUCKETS;\n        return NULL;\n    }\n\n    // TODO: More intelligence around alignment of flash erasure block sizes\n    if (cf->page_size % (1024 * 1024 * 2) != 0 ||\n        cf->wbuf_size % (1024 * 1024 * 2) != 0) {\n        *res = EXTSTORE_INIT_PAGE_WBUF_ALIGNMENT;\n        return NULL;\n    }\n\n    store_engine *e = calloc(1, sizeof(store_engine));\n    if (e == NULL) {\n        *res = EXTSTORE_INIT_OOM;\n        return NULL;\n    }\n\n    e->page_size = cf->page_size;\n    uint64_t temp_page_count = 0;\n    for (f = fh; f != NULL; f = f->next) {\n        f->fd = open(f->file, O_RDWR | O_CREAT, 0644);\n        if (f->fd < 0) {\n            *res = EXTSTORE_INIT_OPEN_FAIL;\n#ifdef EXTSTORE_DEBUG\n            perror(\"extstore open\");\n#endif\n            free(e);\n            return NULL;\n        }\n        // use an fcntl lock to help avoid double starting.\n        struct flock lock;\n        lock.l_type = F_WRLCK;\n        lock.l_start = 0;\n        lock.l_whence = SEEK_SET;\n        lock.l_len = 0;\n        if (fcntl(f->fd, F_SETLK, &lock) < 0) {\n            *res = EXTSTORE_INIT_OPEN_FAIL;\n            free(e);\n            return NULL;\n        }\n        if (ftruncate(f->fd, 0) < 0) {\n            *res = EXTSTORE_INIT_OPEN_FAIL;\n            free(e);\n            return NULL;\n        }\n\n        temp_page_count += f->page_count;\n        f->offset = 0;\n    }\n\n    if (temp_page_count >= UINT16_MAX) {\n        *res = EXTSTORE_INIT_TOO_MANY_PAGES;\n        free(e);\n        return NULL;\n    }\n    e->page_count = temp_page_count;\n\n    e->pages = calloc(e->page_count, sizeof(store_page));\n    if (e->pages == NULL) {\n        *res = EXTSTORE_INIT_OOM;\n        // FIXME: loop-close. make error label\n        free(e);\n        return NULL;\n    }\n\n    // interleave the pages between devices\n    f = NULL; // start at the first device.\n    for (i = 0; i < e->page_count; i++) {\n        // find next device with available pages\n        while (1) {\n            // restart the loop\n            if (f == NULL || f->next == NULL) {\n                f = fh;\n            } else {\n                f = f->next;\n            }\n            if (f->page_count) {\n                f->page_count--;\n                break;\n            }\n        }\n        pthread_mutex_init(&e->pages[i].mutex, NULL);\n        e->pages[i].id = i;\n        e->pages[i].fd = f->fd;\n        e->pages[i].free_bucket = f->free_bucket;\n        e->pages[i].offset = f->offset;\n        e->pages[i].free = true;\n        f->offset += e->page_size;\n    }\n\n    // free page buckets allows the app to organize devices by use case\n    e->free_page_buckets = calloc(cf->page_buckets, sizeof(store_page *));\n    e->free_page_bucketcount = cf->page_buckets;\n\n    e->page_free = e->page_count;\n    for (i = e->page_count-1; i >= 0; i--) {\n        int fb = e->pages[i].free_bucket;\n        e->pages[i].next = e->free_page_buckets[fb];\n        e->free_page_buckets[fb] = &e->pages[i];\n    }\n\n    // 0 is magic \"page is freed\" version\n    e->version = 1;\n\n    // scratch data for stats. TODO: malloc failure handle\n    e->stats.page_data =\n        calloc(e->page_count, sizeof(struct extstore_page_data));\n    e->stats.page_count = e->page_count;\n    e->stats.page_size = e->page_size;\n\n    // page buckets lazily have pages assigned into them\n    e->page_buckets = calloc(cf->page_buckets, sizeof(store_page *));\n    e->page_bucketcount = cf->page_buckets;\n\n    // allocate write buffers\n    // also IO's to use for shipping to IO thread\n    for (i = 0; i < cf->wbuf_count; i++) {\n        _store_wbuf *w = wbuf_new(cf->wbuf_size);\n        obj_io *io = calloc(1, sizeof(obj_io));\n        /* TODO: on error, loop again and free stack. */\n        w->next = e->wbuf_stack;\n        e->wbuf_stack = w;\n        io->next = e->io_stack;\n        e->io_stack = io;\n    }\n\n    pthread_mutex_init(&e->mutex, NULL);\n    pthread_mutex_init(&e->stats_mutex, NULL);\n    pthread_mutex_init(&e->maint.mutex, NULL);\n\n    e->io_depth = cf->io_depth;\n\n    // spawn threads\n    e->io_threads = calloc(cf->io_threadcount, sizeof(store_io_thread));\n    for (i = 0; i < cf->io_threadcount; i++) {\n        pthread_mutex_init(&e->io_threads[i].mutex, NULL);\n        pthread_cond_init(&e->io_threads[i].cond, NULL);\n        e->io_threads[i].e = e;\n        // FIXME: error handling\n        pthread_create(&thread, NULL, extstore_io_thread, &e->io_threads[i]);\n        thread_setname(thread, \"mc-ext-io\");\n    }\n    e->io_threadcount = cf->io_threadcount;\n\n    // dedicated IO thread for certain non-hotpath functions.\n    e->bg_thread = calloc(1, sizeof(store_io_thread));\n    e->bg_thread->e = e;\n    pthread_mutex_init(&e->bg_thread->mutex, NULL);\n    pthread_cond_init(&e->bg_thread->cond, NULL);\n    pthread_create(&thread, NULL, extstore_io_thread, e->bg_thread);\n    thread_setname(thread, \"mc-ext-bgio\");\n\n    return (void *)e;\n}\n\n// Call without *e locked, not a fast function.\nstatic void _evict_page(store_engine *e, unsigned int bucket,\n        unsigned int free_bucket) {\n    struct extstore_stats st;\n    st.page_data = calloc(e->page_count, sizeof(struct extstore_page_data));\n    extstore_get_page_data(e, &st);\n    uint64_t low_version = ULLONG_MAX;\n    unsigned int low_page = 0;\n\n    // find lowest version of anything in free_bucket OR 0\n    // unless free_bucket is 0\n    for (int i = 0; i < e->page_count; i++) {\n        // must belong to 0 or the requested free_bucket\n        if (st.page_data[i].free_bucket &&\n            st.page_data[i].free_bucket != free_bucket) {\n            continue;\n        }\n\n        // found a free page, don't evict.\n        if (st.page_data[i].version == 0) {\n            low_version = ULLONG_MAX;\n            break;\n        }\n\n        // find the lowest version.\n        if (!st.page_data[i].active &&\n                st.page_data[i].version < low_version) {\n            low_page = i;\n            low_version = st.page_data[i].version;\n        }\n    }\n\n    if (low_version != ULLONG_MAX) {\n        extstore_evict_page(e, low_page, low_version);\n    }\n}\n\n// call with *e locked\nstatic store_page *_allocate_page(store_engine *e, unsigned int bucket,\n        unsigned int free_bucket) {\n    E_DEBUG(\"EXTSTORE: allocating new page [bucket:%u]\\n\", bucket);\n    assert(!e->page_buckets[bucket] || e->page_buckets[bucket]->allocated == e->page_size);\n    store_page *tmp = NULL;\n    if (e->free_page_buckets[free_bucket] != NULL) {\n        assert(e->page_free > 0);\n        tmp = e->free_page_buckets[free_bucket];\n        e->free_page_buckets[free_bucket] = tmp->next;\n    } else if (e->free_page_buckets[0] != NULL) {\n        // fall back to default bucket.\n        assert(e->page_free > 0);\n        tmp = e->free_page_buckets[0];\n        e->free_page_buckets[0] = tmp->next;\n    }\n    if (tmp != NULL) {\n        tmp->next = e->page_buckets[bucket];\n        e->page_buckets[bucket] = tmp;\n        tmp->active = true;\n        tmp->free = false;\n        tmp->closed = false;\n        tmp->version = _next_version(e);\n        tmp->bucket = bucket;\n        e->page_free--;\n        STAT_INCR(e, page_allocs, 1);\n    }\n\n    if (tmp)\n        E_DEBUG(\"EXTSTORE: got page %u [free:%u]\\n\", tmp->id, e->page_free);\n    return tmp;\n}\n\n// call with *p locked. locks *e\nstatic void _allocate_wbuf(store_engine *e, store_page *p) {\n    _store_wbuf *wbuf = NULL;\n    assert(!p->wbuf);\n    pthread_mutex_lock(&e->mutex);\n    if (e->wbuf_stack) {\n        wbuf = e->wbuf_stack;\n        e->wbuf_stack = wbuf->next;\n        wbuf->next = 0;\n    }\n    pthread_mutex_unlock(&e->mutex);\n    if (wbuf) {\n        wbuf->offset = p->allocated;\n        p->allocated += wbuf->size;\n        wbuf->free = wbuf->size;\n        wbuf->buf_pos = wbuf->buf;\n        wbuf->full = false;\n        wbuf->flushed = false;\n\n        p->wbuf = wbuf;\n    }\n}\n\n/* callback after wbuf is flushed. can only remove wbuf's from the head onward\n * if successfully flushed, which complicates this routine. each callback\n * attempts to free the wbuf stack, which is finally done when the head wbuf's\n * callback happens.\n * It's rare flushes would happen out of order.\n */\nstatic void _wbuf_cb(void *ep, obj_io *io, int ret) {\n    store_engine *e = (store_engine *)ep;\n    store_page *p = &e->pages[io->page_id];\n    _store_wbuf *w = (_store_wbuf *) io->data;\n\n    // TODO: Examine return code. Not entirely sure how to handle errors.\n    // Naive first-pass should probably cause the page to close/free.\n    w->flushed = true;\n    pthread_mutex_lock(&p->mutex);\n    assert(p->wbuf != NULL && p->wbuf == w);\n    assert(p->written == w->offset);\n    p->written += w->size;\n    p->wbuf = NULL;\n\n    if (p->written == e->page_size)\n        p->active = false;\n\n    // return the wbuf\n    pthread_mutex_lock(&e->mutex);\n    w->next = e->wbuf_stack;\n    e->wbuf_stack = w;\n    // also return the IO we just used.\n    io->next = e->io_stack;\n    e->io_stack = io;\n    pthread_mutex_unlock(&e->mutex);\n    pthread_mutex_unlock(&p->mutex);\n}\n\n/* Wraps pages current wbuf in an io and submits to IO thread.\n * Called with p locked, locks e.\n */\nstatic void _submit_wbuf(store_engine *e, store_page *p) {\n    _store_wbuf *w;\n    pthread_mutex_lock(&e->mutex);\n    obj_io *io = e->io_stack;\n    e->io_stack = io->next;\n    pthread_mutex_unlock(&e->mutex);\n    w = p->wbuf;\n\n    // zero out the end of the wbuf to allow blind readback of data.\n    memset(w->buf + (w->size - w->free), 0, w->free);\n\n    io->next = NULL;\n    io->mode = OBJ_IO_WRITE;\n    io->page_id = p->id;\n    io->data = w;\n    io->offset = w->offset;\n    io->len = w->size;\n    io->buf = w->buf;\n    io->cb = _wbuf_cb;\n\n    extstore_submit_bg(e, io);\n}\n\n/* engine write function; takes engine, item_io.\n * fast fail if no available write buffer (flushing)\n * lock engine context, find active page, unlock\n * if page full, submit page/buffer to io thread.\n *\n * write is designed to be flaky; if page full, caller must try again to get\n * new page. best if used from a background thread that can harmlessly retry.\n */\n\nint extstore_write_request(void *ptr, unsigned int bucket,\n        unsigned int free_bucket, obj_io *io) {\n    store_engine *e = (store_engine *)ptr;\n    store_page *p;\n    int ret = -1;\n    if (bucket >= e->page_bucketcount)\n        return ret;\n\n    pthread_mutex_lock(&e->mutex);\n    p = e->page_buckets[bucket];\n    if (!p) {\n        p = _allocate_page(e, bucket, free_bucket);\n    }\n    pthread_mutex_unlock(&e->mutex);\n    if (!p) {\n        _evict_page(e, bucket, free_bucket);\n        return ret;\n    }\n\n    pthread_mutex_lock(&p->mutex);\n\n    // FIXME: can't null out page_buckets!!!\n    // page is full, clear bucket and retry later.\n    if (!p->active ||\n            ((!p->wbuf || p->wbuf->full) && p->allocated >= e->page_size)) {\n        pthread_mutex_unlock(&p->mutex);\n        pthread_mutex_lock(&e->mutex);\n        store_page *temp_p = _allocate_page(e, bucket, free_bucket);\n        pthread_mutex_unlock(&e->mutex);\n        if (!temp_p) {\n            _evict_page(e, bucket, free_bucket);\n        }\n        return ret;\n    }\n\n    // if io won't fit, submit IO for wbuf and find new one.\n    if (p->wbuf && p->wbuf->free < io->len && !p->wbuf->full) {\n        _submit_wbuf(e, p);\n        p->wbuf->full = true;\n    }\n\n    if (!p->wbuf && p->allocated < e->page_size) {\n        _allocate_wbuf(e, p);\n    }\n\n    // hand over buffer for caller to copy into\n    // leaves p locked.\n    if (p->wbuf && !p->wbuf->full && p->wbuf->free >= io->len) {\n        io->buf = p->wbuf->buf_pos;\n        io->page_id = p->id;\n        return 0;\n    }\n\n    pthread_mutex_unlock(&p->mutex);\n    // p->written is incremented post-wbuf flush\n    return ret;\n}\n\n/* _must_ be called after a successful write_request.\n * fills the rest of io structure.\n */\nvoid extstore_write(void *ptr, obj_io *io) {\n    store_engine *e = (store_engine *)ptr;\n    store_page *p = &e->pages[io->page_id];\n\n    io->offset = p->wbuf->offset + (p->wbuf->size - p->wbuf->free);\n    io->page_version = p->version;\n    p->wbuf->buf_pos += io->len;\n    p->wbuf->free -= io->len;\n    p->bytes_used += io->len;\n    p->obj_count++;\n    STAT_L(e);\n    e->stats.bytes_written += io->len;\n    e->stats.bytes_used += io->len;\n    e->stats.objects_written++;\n    e->stats.objects_used++;\n    STAT_UL(e);\n\n    pthread_mutex_unlock(&p->mutex);\n}\n\n/* engine submit function; takes engine, item_io stack.\n * lock io_thread context and add stack\n * signal io thread to wake.\n * return success.\n */\nstatic int _extstore_submit(void *ptr, obj_io *io, store_io_thread *t) {\n    unsigned int depth = 0;\n    obj_io *tio = io;\n    obj_io *tail = NULL;\n    while (tio != NULL) {\n        tail = tio; // keep updating potential tail.\n        depth++;\n        tio = tio->next;\n    }\n\n    pthread_mutex_lock(&t->mutex);\n\n    t->depth += depth;\n    if (t->queue == NULL) {\n        t->queue = io;\n        t->queue_tail = tail;\n    } else {\n        // Have to put the *io stack at the end of current queue.\n        assert(tail->next == NULL);\n        assert(t->queue_tail->next == NULL);\n        t->queue_tail->next = io;\n        t->queue_tail = tail;\n    }\n\n    pthread_mutex_unlock(&t->mutex);\n\n    //pthread_mutex_lock(&t->mutex);\n    pthread_cond_signal(&t->cond);\n    //pthread_mutex_unlock(&t->mutex);\n    return 0;\n}\n\nint extstore_submit(void *ptr, obj_io *io) {\n    store_engine *e = (store_engine *)ptr;\n    store_io_thread *t = _get_io_thread(e);\n    return _extstore_submit(ptr, io, t);\n}\n\nint extstore_submit_bg(void *ptr, obj_io *io) {\n    store_engine *e = (store_engine *)ptr;\n    store_io_thread *t = e->bg_thread;\n    return _extstore_submit(ptr, io, t);\n}\n\n/* engine note delete function: takes engine, page id, size?\n * note that an item in this page is no longer valid\n */\nint extstore_delete(void *ptr, unsigned int page_id, uint64_t page_version,\n        unsigned int count, unsigned int bytes) {\n    store_engine *e = (store_engine *)ptr;\n    // FIXME: validate page_id in bounds\n    store_page *p = &e->pages[page_id];\n    int ret = 0;\n\n    pthread_mutex_lock(&p->mutex);\n    if (!p->closed && p->version == page_version) {\n        if (p->bytes_used >= bytes) {\n            p->bytes_used -= bytes;\n        } else {\n            p->bytes_used = 0;\n        }\n\n        if (p->obj_count >= count) {\n            p->obj_count -= count;\n        } else {\n            p->obj_count = 0; // caller has bad accounting?\n        }\n        STAT_L(e);\n        e->stats.bytes_used -= bytes;\n        e->stats.objects_used -= count;\n        STAT_UL(e);\n\n        if (p->obj_count == 0 && p->refcount == 0 && !p->active) {\n            _free_page(e, p);\n        }\n    } else {\n        ret = -1;\n    }\n    pthread_mutex_unlock(&p->mutex);\n    return ret;\n}\n\nint extstore_check(void *ptr, unsigned int page_id, uint64_t page_version) {\n    store_engine *e = (store_engine *)ptr;\n    store_page *p = &e->pages[page_id];\n    int ret = 0;\n\n    pthread_mutex_lock(&p->mutex);\n    if (p->version != page_version)\n        ret = -1;\n    pthread_mutex_unlock(&p->mutex);\n    return ret;\n}\n\n/* allows a compactor to say \"we're done with this page, kill it.\" */\nvoid extstore_close_page(void *ptr, unsigned int page_id, uint64_t page_version) {\n    store_engine *e = (store_engine *)ptr;\n    store_page *p = &e->pages[page_id];\n\n    pthread_mutex_lock(&p->mutex);\n    if (!p->closed && !p->active && p->version == page_version) {\n        p->closed = true;\n        if (p->refcount == 0) {\n            _free_page(e, p);\n        }\n    }\n    pthread_mutex_unlock(&p->mutex);\n}\n\n/* signal that we've forcefully ejected rather than gracefully closed */\nvoid extstore_evict_page(void *ptr, unsigned int page_id, uint64_t page_version) {\n    store_engine *e = (store_engine *)ptr;\n    store_page *p = &e->pages[page_id];\n\n    pthread_mutex_lock(&p->mutex);\n    if (!p->closed && !p->active && p->version == page_version) {\n        E_DEBUG(\"EXTSTORE: evicting page [%d] [v: %llu]\\n\",\n                p->id, (unsigned long long) p->version);\n\n        p->closed = true;\n        STAT_L(e);\n        e->stats.page_evictions++;\n        e->stats.objects_evicted += p->obj_count;\n        e->stats.bytes_evicted += p->bytes_used;\n        STAT_UL(e);\n        if (p->refcount == 0) {\n            _free_page(e, p);\n        }\n    }\n    pthread_mutex_unlock(&p->mutex);\n}\n\n/* Finds an attached wbuf that can satisfy the read.\n * Since wbufs can potentially be flushed to disk out of order, they are only\n * removed as the head of the list successfully flushes to disk.\n */\n// call with *p locked\n// FIXME: protect from reading past wbuf\nstatic inline int _read_from_wbuf(store_page *p, obj_io *io) {\n    _store_wbuf *wbuf = p->wbuf;\n    assert(wbuf != NULL);\n    assert(io->offset < p->written + wbuf->size);\n    if (io->iov == NULL) {\n        memcpy(io->buf, wbuf->buf + (io->offset - wbuf->offset), io->len);\n    } else {\n        int x;\n        unsigned int off = io->offset - wbuf->offset;\n        // need to loop fill iovecs\n        for (x = 0; x < io->iovcnt; x++) {\n            struct iovec *iov = &io->iov[x];\n            memcpy(iov->iov_base, wbuf->buf + off, iov->iov_len);\n            off += iov->iov_len;\n        }\n    }\n    return io->len;\n}\n\n/* engine IO thread; takes engine context\n * manage writes/reads\n * runs IO callbacks inline after each IO\n */\n// FIXME: protect from reading past page\nstatic void *extstore_io_thread(void *arg) {\n    store_io_thread *me = (store_io_thread *)arg;\n    store_engine *e = me->e;\n    while (1) {\n        obj_io *io_stack = NULL;\n        pthread_mutex_lock(&me->mutex);\n        if (me->queue == NULL) {\n            pthread_cond_wait(&me->cond, &me->mutex);\n        }\n\n        // Pull and disconnect a batch from the queue\n        // Chew small batches from the queue so the IO thread picker can keep\n        // the IO queue depth even, instead of piling on threads one at a time\n        // as they gobble a queue.\n        if (me->queue != NULL) {\n            int i;\n            obj_io *end = NULL;\n            io_stack = me->queue;\n            end = io_stack;\n            for (i = 1; i < e->io_depth; i++) {\n                if (end->next) {\n                    end = end->next;\n                } else {\n                    me->queue_tail = end->next;\n                    break;\n                }\n            }\n            me->depth -= i;\n            me->queue = end->next;\n            end->next = NULL;\n        }\n        pthread_mutex_unlock(&me->mutex);\n\n        obj_io *cur_io = io_stack;\n        while (cur_io) {\n            // We need to note next before the callback in case the obj_io\n            // gets reused.\n            obj_io *next = cur_io->next;\n            int ret = 0;\n            int do_op = 1;\n            store_page *p = &e->pages[cur_io->page_id];\n            // TODO: loop if not enough bytes were read/written.\n            switch (cur_io->mode) {\n                case OBJ_IO_READ:\n                    // Page is currently open. deal if read is past the end.\n                    pthread_mutex_lock(&p->mutex);\n                    if (!p->free && !p->closed && p->version == cur_io->page_version) {\n                        if (p->active && cur_io->offset >= p->written) {\n                            ret = _read_from_wbuf(p, cur_io);\n                            do_op = 0;\n                        } else {\n                            p->refcount++;\n                        }\n                        STAT_L(e);\n                        e->stats.bytes_read += cur_io->len;\n                        e->stats.objects_read++;\n                        STAT_UL(e);\n                    } else {\n                        do_op = 0;\n                        ret = -2; // TODO: enum in IO for status?\n                    }\n                    pthread_mutex_unlock(&p->mutex);\n                    if (do_op) {\n#if !defined(HAVE_PREAD) || !defined(HAVE_PREADV)\n                        // TODO: lseek offset is natively 64-bit on OS X, but\n                        // perhaps not on all platforms? Else use lseek64()\n                        ret = lseek(p->fd, p->offset + cur_io->offset, SEEK_SET);\n                        if (ret >= 0) {\n                            if (cur_io->iov == NULL) {\n                                ret = read(p->fd, cur_io->buf, cur_io->len);\n                            } else {\n                                ret = readv(p->fd, cur_io->iov, cur_io->iovcnt);\n                            }\n                        }\n#else\n                        if (cur_io->iov == NULL) {\n                            ret = pread(p->fd, cur_io->buf, cur_io->len, p->offset + cur_io->offset);\n                        } else {\n                            ret = preadv(p->fd, cur_io->iov, cur_io->iovcnt, p->offset + cur_io->offset);\n                        }\n#endif\n                    }\n                    break;\n                case OBJ_IO_WRITE:\n                    do_op = 0;\n                    // FIXME: Should hold refcount during write. doesn't\n                    // currently matter since page can't free while active.\n                    ret = pwrite(p->fd, cur_io->buf, cur_io->len, p->offset + cur_io->offset);\n                    break;\n            }\n            if (ret == 0) {\n                E_DEBUG(\"read returned nothing\\n\");\n            }\n\n#ifdef EXTSTORE_DEBUG\n            if (ret == -1) {\n                perror(\"read/write op failed\");\n            }\n#endif\n            cur_io->cb(e, cur_io, ret);\n            if (do_op) {\n                pthread_mutex_lock(&p->mutex);\n                p->refcount--;\n                pthread_mutex_unlock(&p->mutex);\n            }\n            cur_io = next;\n        }\n    }\n\n    return NULL;\n}\n\n// call with *p locked.\nstatic void _free_page(store_engine *e, store_page *p) {\n    store_page *tmp = NULL;\n    store_page *prev = NULL;\n    E_DEBUG(\"EXTSTORE: freeing page %u\\n\", p->id);\n    STAT_L(e);\n    e->stats.objects_used -= p->obj_count;\n    e->stats.bytes_used -= p->bytes_used;\n    e->stats.page_reclaims++;\n    STAT_UL(e);\n    pthread_mutex_lock(&e->mutex);\n    // unlink page from bucket list\n    tmp = e->page_buckets[p->bucket];\n    while (tmp) {\n        if (tmp == p) {\n            if (prev) {\n                prev->next = tmp->next;\n            } else {\n                e->page_buckets[p->bucket] = tmp->next;\n            }\n            tmp->next = NULL;\n            break;\n        }\n        prev = tmp;\n        tmp = tmp->next;\n    }\n    // reset most values\n    p->version = 0;\n    p->obj_count = 0;\n    p->bytes_used = 0;\n    p->allocated = 0;\n    p->written = 0;\n    p->bucket = 0;\n    p->active = false;\n    p->closed = false;\n    p->free = true;\n    // add to page stack\n    p->next = e->free_page_buckets[p->free_bucket];\n    e->free_page_buckets[p->free_bucket] = p;\n    e->page_free++;\n    E_DEBUG(\"EXTSTORE: pages free %u\\n\", e->page_free);\n    pthread_mutex_unlock(&e->mutex);\n}\n"
        },
        {
          "name": "extstore.h",
          "type": "blob",
          "size": 4.654296875,
          "content": "#ifndef EXTSTORE_H\n#define EXTSTORE_H\n\n/* A safe-to-read dataset for determining compaction.\n * id is the array index.\n */\nstruct extstore_page_data {\n    uint64_t version;\n    uint64_t bytes_used;\n    unsigned int bucket;\n    unsigned int free_bucket;\n    bool active; // page is actively being written to; ignore it except for tallying.\n};\n\n/* Pages can have objects deleted from them at any time. This creates holes\n * that can't be reused until the page is either evicted or all objects are\n * deleted.\n * bytes_fragmented is the total bytes for all of these holes.\n * It is the size of all used pages minus each page's bytes_used value.\n */\nstruct extstore_stats {\n    uint64_t page_allocs;\n    uint64_t page_count; /* total page count */\n    uint64_t page_evictions;\n    uint64_t page_reclaims;\n    uint64_t page_size; /* size in bytes per page (supplied by caller) */\n    uint64_t pages_free; /* currently unallocated/unused pages */\n    uint64_t pages_used;\n    uint64_t objects_evicted;\n    uint64_t objects_read;\n    uint64_t objects_written;\n    uint64_t objects_used; /* total number of objects stored */\n    uint64_t bytes_evicted;\n    uint64_t bytes_written;\n    uint64_t bytes_read; /* wbuf - read -> bytes read from storage */\n    uint64_t bytes_used; /* total number of bytes stored */\n    uint64_t bytes_fragmented; /* see above comment */\n    uint64_t io_queue;\n    struct extstore_page_data *page_data;\n};\n\n// TODO: Temporary configuration structure. A \"real\" library should have an\n// extstore_set(enum, void *ptr) which hides the implementation.\n// this is plenty for quick development.\nstruct extstore_conf {\n    unsigned int page_size; // ideally 64-256M in size\n    unsigned int page_count;\n    unsigned int page_buckets; // number of different writeable pages\n    unsigned int free_page_buckets; // buckets of dedicated pages (see code)\n    unsigned int wbuf_size; // must divide cleanly into page_size\n    unsigned int wbuf_count; // this might get locked to \"2 per active page\"\n    unsigned int io_threadcount;\n    unsigned int io_depth; // with normal I/O, hits locks less. req'd for AIO\n};\n\nstruct extstore_conf_file {\n    unsigned int page_count;\n    char *file;\n    int fd; // internal usage\n    uint64_t offset; // internal usage\n    unsigned int bucket; // free page bucket\n    unsigned int free_bucket; // specialized free bucket\n    struct extstore_conf_file *next;\n};\n\nenum obj_io_mode {\n    OBJ_IO_READ = 0,\n    OBJ_IO_WRITE,\n};\n\ntypedef struct _obj_io obj_io;\ntypedef void (*obj_io_cb)(void *e, obj_io *io, int ret);\n\n/* An object for both reads and writes to the storage engine.\n * Once an IO is submitted, ->next may be changed by the IO thread. It is not\n * safe to further modify the IO stack until the entire request is completed.\n */\nstruct _obj_io {\n    void *data; /* user supplied data pointer */\n    struct _obj_io *next;\n    char *buf;  /* buffer of data to read or write to */\n    struct iovec *iov; /* alternatively, use this iovec */\n    unsigned int iovcnt; /* number of IOV's */\n    unsigned int page_version;     /* page version for read mode */\n    unsigned int len;     /* for both modes */\n    unsigned int offset;  /* for read mode */\n    unsigned short page_id; /* for read mode */\n    enum obj_io_mode mode;\n    /* callback pointers? */\n    obj_io_cb cb;\n};\n\nenum extstore_res {\n    EXTSTORE_INIT_BAD_WBUF_SIZE = 1,\n    EXTSTORE_INIT_NEED_MORE_WBUF,\n    EXTSTORE_INIT_NEED_MORE_BUCKETS,\n    EXTSTORE_INIT_PAGE_WBUF_ALIGNMENT,\n    EXTSTORE_INIT_TOO_MANY_PAGES,\n    EXTSTORE_INIT_OOM,\n    EXTSTORE_INIT_OPEN_FAIL,\n    EXTSTORE_INIT_THREAD_FAIL\n};\n\nconst char *extstore_err(enum extstore_res res);\nvoid *extstore_init(struct extstore_conf_file *fh, struct extstore_conf *cf, enum extstore_res *res);\nint extstore_write_request(void *ptr, unsigned int bucket, unsigned int free_bucket, obj_io *io);\nvoid extstore_write(void *ptr, obj_io *io);\nint extstore_submit(void *ptr, obj_io *io);\nint extstore_submit_bg(void *ptr, obj_io *io);\n/* count are the number of objects being removed, bytes are the original\n * length of those objects. Bytes is optional but you can't track\n * fragmentation without it.\n */\nint extstore_check(void *ptr, unsigned int page_id, uint64_t page_version);\nint extstore_delete(void *ptr, unsigned int page_id, uint64_t page_version, unsigned int count, unsigned int bytes);\nvoid extstore_get_stats(void *ptr, struct extstore_stats *st);\n/* add page data array to a stats structure.\n * caller must allocate its stats.page_data memory first.\n */\nvoid extstore_get_page_data(void *ptr, struct extstore_stats *st);\nvoid extstore_close_page(void *ptr, unsigned int page_id, uint64_t page_version);\nvoid extstore_evict_page(void *ptr, unsigned int page_id, uint64_t page_version);\n\n#endif\n"
        },
        {
          "name": "freebsd_priv.c",
          "type": "blob",
          "size": 1.345703125,
          "content": "#include <sys/capsicum.h>\n#include <errno.h>\n#include <stdlib.h>\n#include <stdio.h>\n#include <string.h>\n#include <unistd.h>\n#include \"memcached.h\"\n\n/*\n * dropping privileges is entering in capability mode\n * in FreeBSD vocabulary.\n */\nvoid drop_privileges() {\n    cap_rights_t wd, rd;\n\n    if (cap_rights_init(&wd, CAP_WRITE, CAP_READ) == NULL) {\n        fprintf(stderr, \"cap_rights_init write protection failed: %s\\n\", strerror(errno));\n        exit(EXIT_FAILURE);\n    }\n\n    if (cap_rights_init(&rd, CAP_FCNTL, CAP_READ, CAP_EVENT) == NULL) {\n        fprintf(stderr, \"cap_rights_init read protection failed: %s\\n\", strerror(errno));\n        exit(EXIT_FAILURE);\n    }\n\n    if (cap_rights_limit(STDIN_FILENO, &rd) != 0) {\n        fprintf(stderr, \"cap_rights_limit stdin failed: %s\\n\", strerror(errno));\n        exit(EXIT_FAILURE);\n    }\n\n    if (cap_rights_limit(STDOUT_FILENO, &wd) != 0) {\n        fprintf(stderr, \"cap_rights_limit stdout failed: %s\\n\", strerror(errno));\n        exit(EXIT_FAILURE);\n    }\n\n    if (cap_rights_limit(STDERR_FILENO, &wd) != 0) {\n        fprintf(stderr, \"cap_rights_limit stderr failed: %s\\n\", strerror(errno));\n        exit(EXIT_FAILURE);\n    }\n\n    if (cap_enter() != 0) {\n        fprintf(stderr, \"cap_enter failed: %s\\n\", strerror(errno));\n        exit(EXIT_FAILURE);\n    }\n}\n\nvoid setup_privilege_violations_handler(void) {\n   // not needed\n}\n"
        },
        {
          "name": "globals.c",
          "type": "blob",
          "size": 0.8203125,
          "content": "#include \"memcached.h\"\n\n/*\n * This file contains global variables shared across the rest of the\n * memcached codebase.  These were originally in memcached.c but had\n * to be removed to make the rest of the object files linkable into\n * the test infrastructure.\n *\n */\n\n/*\n * We keep the current time of day in a global variable that's updated by a\n * timer event. This saves us a bunch of time() system calls (we really only\n * need to get the time once a second, whereas there can be tens of thousands\n * of requests a second) and allows us to use server-start-relative timestamps\n * rather than absolute UNIX timestamps, a space savings on systems where\n * sizeof(time_t) > sizeof(unsigned int).\n */\nvolatile rel_time_t current_time;\n\n/** exported globals **/\nstruct stats stats;\nstruct stats_state stats_state;\nstruct settings settings;\n"
        },
        {
          "name": "hash.c",
          "type": "blob",
          "size": 0.8447265625,
          "content": "/* -*- Mode: C; tab-width: 4; c-basic-offset: 4; indent-tabs-mode: nil -*- */\n\n#include \"memcached.h\"\n#include \"jenkins_hash.h\"\n#include \"murmur3_hash.h\"\n#define XXH_INLINE_ALL // modifier for xxh3's include below\n#include \"xxhash.h\"\n\nhash_func hash;\n\nstatic uint32_t XXH3_hash(const void *key, size_t length) {\n    return (uint32_t)XXH3_64bits(key, length);\n}\n\nint hash_init(enum hashfunc_type type) {\n    switch(type) {\n        case JENKINS_HASH:\n            hash = jenkins_hash;\n            settings.hash_algorithm = \"jenkins\";\n            break;\n        case MURMUR3_HASH:\n            hash = MurmurHash3_x86_32;\n            settings.hash_algorithm = \"murmur3\";\n            break;\n        case XXH3_HASH:\n            hash = XXH3_hash;\n            settings.hash_algorithm = \"xxh3\";\n            break;\n        default:\n            return -1;\n    }\n    return 0;\n}\n"
        },
        {
          "name": "hash.h",
          "type": "blob",
          "size": 0.2490234375,
          "content": "#ifndef HASH_H\n#define    HASH_H\n\ntypedef uint32_t (*hash_func)(const void *key, size_t length);\nextern hash_func hash;\n\nenum hashfunc_type {\n    JENKINS_HASH=0, MURMUR3_HASH, XXH3_HASH\n};\n\nint hash_init(enum hashfunc_type type);\n\n#endif    /* HASH_H */\n\n"
        },
        {
          "name": "items.c",
          "type": "blob",
          "size": 61.0048828125,
          "content": "/* -*- Mode: C; tab-width: 4; c-basic-offset: 4; indent-tabs-mode: nil -*- */\n#include \"memcached.h\"\n#include \"bipbuffer.h\"\n#include \"storage.h\"\n#include \"slabs_mover.h\"\n#include <sys/stat.h>\n#include <sys/socket.h>\n#include <sys/resource.h>\n#include <fcntl.h>\n#include <netinet/in.h>\n#include <stdlib.h>\n#include <stdio.h>\n#include <string.h>\n#include <time.h>\n#include <assert.h>\n#include <unistd.h>\n#include <poll.h>\n\n/* Forward Declarations */\nstatic void item_link_q(item *it);\nstatic void item_unlink_q(item *it);\n\nstatic unsigned int lru_type_map[4] = {HOT_LRU, WARM_LRU, COLD_LRU, TEMP_LRU};\n\n#define LARGEST_ID POWER_LARGEST\ntypedef struct {\n    uint64_t evicted;\n    uint64_t evicted_nonzero;\n    uint64_t reclaimed;\n    uint64_t outofmemory;\n    uint64_t tailrepairs;\n    uint64_t expired_unfetched; /* items reclaimed but never touched */\n    uint64_t evicted_unfetched; /* items evicted but never touched */\n    uint64_t evicted_active; /* items evicted that should have been shuffled */\n    uint64_t crawler_reclaimed;\n    uint64_t crawler_items_checked;\n    uint64_t lrutail_reflocked;\n    uint64_t moves_to_cold;\n    uint64_t moves_to_warm;\n    uint64_t moves_within_lru;\n    uint64_t direct_reclaims;\n    uint64_t hits_to_hot;\n    uint64_t hits_to_warm;\n    uint64_t hits_to_cold;\n    uint64_t hits_to_temp;\n    uint64_t mem_requested;\n    rel_time_t evicted_time;\n} itemstats_t;\n\nstatic item *heads[LARGEST_ID];\nstatic item *tails[LARGEST_ID];\nstatic itemstats_t itemstats[LARGEST_ID];\nstatic unsigned int sizes[LARGEST_ID];\nstatic uint64_t sizes_bytes[LARGEST_ID];\nstatic unsigned int *stats_sizes_hist = NULL;\nstatic int stats_sizes_buckets = 0;\nstatic uint64_t cas_id = 1;\n\nstatic volatile int do_run_lru_maintainer_thread = 0;\nstatic pthread_mutex_t lru_maintainer_lock = PTHREAD_MUTEX_INITIALIZER;\nstatic pthread_mutex_t cas_id_lock = PTHREAD_MUTEX_INITIALIZER;\n\nvoid item_stats_reset(void) {\n    int i;\n    for (i = 0; i < LARGEST_ID; i++) {\n        pthread_mutex_lock(&lru_locks[i]);\n        memset(&itemstats[i], 0, sizeof(itemstats_t));\n        pthread_mutex_unlock(&lru_locks[i]);\n    }\n}\n\n/* called with class lru lock held */\nvoid do_item_stats_add_crawl(const int i, const uint64_t reclaimed,\n        const uint64_t unfetched, const uint64_t checked) {\n    itemstats[i].crawler_reclaimed += reclaimed;\n    itemstats[i].expired_unfetched += unfetched;\n    itemstats[i].crawler_items_checked += checked;\n}\n\ntypedef struct _lru_bump_buf {\n    struct _lru_bump_buf *prev;\n    struct _lru_bump_buf *next;\n    pthread_mutex_t mutex;\n    bipbuf_t *buf;\n    uint64_t dropped;\n} lru_bump_buf;\n\ntypedef struct {\n    item *it;\n    uint32_t hv;\n} lru_bump_entry;\n\nstatic lru_bump_buf *bump_buf_head = NULL;\nstatic lru_bump_buf *bump_buf_tail = NULL;\nstatic pthread_mutex_t bump_buf_lock = PTHREAD_MUTEX_INITIALIZER;\n/* TODO: tunable? Need bench results */\n#define LRU_BUMP_BUF_SIZE 8192\n\nstatic bool lru_bump_async(lru_bump_buf *b, item *it, uint32_t hv);\nstatic uint64_t lru_total_bumps_dropped(void);\n\n/* Get the next CAS id for a new item. */\n/* TODO: refactor some atomics for this. */\nuint64_t get_cas_id(void) {\n    pthread_mutex_lock(&cas_id_lock);\n    uint64_t next_id = ++cas_id;\n    pthread_mutex_unlock(&cas_id_lock);\n    return next_id;\n}\n\nvoid set_cas_id(uint64_t new_cas) {\n    pthread_mutex_lock(&cas_id_lock);\n    cas_id = new_cas;\n    pthread_mutex_unlock(&cas_id_lock);\n}\n\nint item_is_flushed(item *it) {\n    rel_time_t oldest_live = settings.oldest_live;\n    if (it->time <= oldest_live && oldest_live <= current_time)\n        return 1;\n\n    return 0;\n}\n\n/* must be locked before call */\nunsigned int do_get_lru_size(uint32_t id) {\n    return sizes[id];\n}\n\n/* Enable this for reference-count debugging. */\n#if 0\n# define DEBUG_REFCNT(it,op) \\\n                fprintf(stderr, \"item %x refcnt(%c) %d %c%c%c\\n\", \\\n                        it, op, it->refcount, \\\n                        (it->it_flags & ITEM_LINKED) ? 'L' : ' ', \\\n                        (it->it_flags & ITEM_SLABBED) ? 'S' : ' ')\n#else\n# define DEBUG_REFCNT(it,op) while(0)\n#endif\n\n/**\n * Generates the variable-sized part of the header for an object.\n *\n * nkey    - The length of the key\n * flags   - key flags\n * nbytes  - Number of bytes to hold value and addition CRLF terminator\n * suffix  - Buffer for the \"VALUE\" line suffix (flags, size).\n * nsuffix - The length of the suffix is stored here.\n *\n * Returns the total size of the header.\n */\nstatic size_t item_make_header(const uint8_t nkey, const client_flags_t flags, const int nbytes,\n                     char *suffix, uint8_t *nsuffix) {\n    if (flags == 0) {\n        *nsuffix = 0;\n    } else {\n        *nsuffix = sizeof(flags);\n    }\n    return sizeof(item) + nkey + *nsuffix + nbytes;\n}\n\nitem *do_item_alloc_pull(const size_t ntotal, const unsigned int id) {\n    item *it = NULL;\n    int i;\n    /* If no memory is available, attempt a direct LRU juggle/eviction */\n    /* This is a race in order to simplify lru_pull_tail; in cases where\n     * locked items are on the tail, you want them to fall out and cause\n     * occasional OOM's, rather than internally work around them.\n     * This also gives one fewer code path for slab alloc/free\n     */\n    for (i = 0; i < 10; i++) {\n        /* Try to reclaim memory first */\n        if (!settings.lru_segmented) {\n            lru_pull_tail(id, COLD_LRU, 0, 0, 0, NULL);\n        }\n        it = slabs_alloc(id, 0);\n\n        if (it == NULL) {\n            // We send '0' in for \"total_bytes\" as this routine is always\n            // pulling to evict, or forcing HOT -> COLD migration.\n            // As of this writing, total_bytes isn't at all used with COLD_LRU.\n            if (lru_pull_tail(id, COLD_LRU, 0, LRU_PULL_EVICT, 0, NULL) <= 0) {\n                if (settings.lru_segmented) {\n                    lru_pull_tail(id, HOT_LRU, 0, 0, 0, NULL);\n                } else {\n                    break;\n                }\n            }\n        } else {\n            break;\n        }\n    }\n\n    if (i > 0) {\n        pthread_mutex_lock(&lru_locks[id]);\n        itemstats[id].direct_reclaims += i;\n        pthread_mutex_unlock(&lru_locks[id]);\n    }\n\n    return it;\n}\n\n/* Chain another chunk onto this chunk. */\n/* slab mover: if it finds a chunk without ITEM_CHUNK flag, and no ITEM_LINKED\n * flag, it counts as busy and skips.\n * I think it might still not be safe to do linking outside of the slab lock\n */\nitem_chunk *do_item_alloc_chunk(item_chunk *ch, const size_t bytes_remain) {\n    // TODO: Should be a cleaner way of finding real size with slabber calls\n    size_t size = bytes_remain + sizeof(item_chunk);\n    if (size > settings.slab_chunk_size_max)\n        size = settings.slab_chunk_size_max;\n    unsigned int id = slabs_clsid(size);\n\n    item_chunk *nch = (item_chunk *) do_item_alloc_pull(size, id);\n    if (nch == NULL) {\n        // The final chunk in a large item will attempt to be a more\n        // appropriately sized chunk to minimize memory overhead. However, if\n        // there's no memory available in the lower slab classes we fail the\n        // SET. In these cases as a fallback we ensure we attempt to evict a\n        // max-size item and reuse a large chunk.\n        if (size == settings.slab_chunk_size_max) {\n            return NULL;\n        } else {\n            size = settings.slab_chunk_size_max;\n            id = slabs_clsid(size);\n            nch = (item_chunk *) do_item_alloc_pull(size, id);\n\n            if (nch == NULL)\n                return NULL;\n        }\n    }\n\n    // link in.\n    // ITEM_CHUNK[ED] bits need to be protected by the slabs lock.\n    slabs_mlock();\n    nch->head = ch->head;\n    ch->next = nch;\n    nch->prev = ch;\n    nch->next = 0;\n    nch->used = 0;\n    nch->slabs_clsid = id;\n    nch->size = size - sizeof(item_chunk);\n    nch->it_flags |= ITEM_CHUNK;\n    slabs_munlock();\n    return nch;\n}\n\nitem *do_item_alloc(const char *key, const size_t nkey, const client_flags_t flags,\n                    const rel_time_t exptime, const int nbytes) {\n    uint8_t nsuffix;\n    item *it = NULL;\n    char suffix[40];\n    // Avoid potential underflows.\n    if (nbytes < 2)\n        return 0;\n\n    size_t ntotal = item_make_header(nkey + 1, flags, nbytes, suffix, &nsuffix);\n    if (settings.use_cas) {\n        ntotal += sizeof(uint64_t);\n    }\n\n    unsigned int id = slabs_clsid(ntotal);\n    unsigned int hdr_id = 0;\n    if (id == 0)\n        return 0;\n\n    /* This is a large item. Allocate a header object now, lazily allocate\n     *  chunks while reading the upload.\n     */\n    if (ntotal > settings.slab_chunk_size_max) {\n        /* We still link this item into the LRU for the larger slab class, but\n         * we're pulling a header from an entirely different slab class. The\n         * free routines handle large items specifically.\n         */\n        int htotal = nkey + 1 + nsuffix + sizeof(item) + sizeof(item_chunk);\n        if (settings.use_cas) {\n            htotal += sizeof(uint64_t);\n        }\n#ifdef NEED_ALIGN\n        // header chunk needs to be padded on some systems\n        int remain = htotal % 8;\n        if (remain != 0) {\n            htotal += 8 - remain;\n        }\n#endif\n        hdr_id = slabs_clsid(htotal);\n        it = do_item_alloc_pull(htotal, hdr_id);\n        /* setting ITEM_CHUNKED is fine here because we aren't LINKED yet. */\n        if (it != NULL)\n            it->it_flags |= ITEM_CHUNKED;\n    } else {\n        it = do_item_alloc_pull(ntotal, id);\n    }\n\n    if (it == NULL) {\n        pthread_mutex_lock(&lru_locks[id]);\n        itemstats[id].outofmemory++;\n        pthread_mutex_unlock(&lru_locks[id]);\n        return NULL;\n    }\n\n    assert(it->it_flags == 0 || it->it_flags == ITEM_CHUNKED);\n    //assert(it != heads[id]);\n\n    /* Refcount is seeded to 1 by slabs_alloc() */\n    it->next = it->prev = 0;\n\n    /* Items are initially loaded into the HOT_LRU. This is '0' but I want at\n     * least a note here. Compiler (hopefully?) optimizes this out.\n     */\n    if (settings.temp_lru &&\n            exptime - current_time <= settings.temporary_ttl) {\n        id |= TEMP_LRU;\n    } else if (settings.lru_segmented) {\n        id |= HOT_LRU;\n    } else {\n        /* There is only COLD in compat-mode */\n        id |= COLD_LRU;\n    }\n    it->slabs_clsid = id;\n\n    DEBUG_REFCNT(it, '*');\n    it->it_flags |= settings.use_cas ? ITEM_CAS : 0;\n    it->it_flags |= nsuffix != 0 ? ITEM_CFLAGS : 0;\n    it->nkey = nkey;\n    it->nbytes = nbytes;\n    memcpy(ITEM_key(it), key, nkey);\n    it->exptime = exptime;\n    if (nsuffix > 0) {\n        memcpy(ITEM_suffix(it), &flags, sizeof(flags));\n    }\n\n    /* Initialize internal chunk. */\n    if (it->it_flags & ITEM_CHUNKED) {\n        item_chunk *chunk = (item_chunk *) ITEM_schunk(it);\n\n        chunk->next = 0;\n        chunk->prev = 0;\n        chunk->used = 0;\n        chunk->size = 0;\n        chunk->head = it;\n        chunk->orig_clsid = hdr_id;\n    }\n    it->h_next = 0;\n\n    return it;\n}\n\nvoid item_free(item *it) {\n    unsigned int clsid;\n    assert((it->it_flags & ITEM_LINKED) == 0);\n    assert(it != heads[it->slabs_clsid]);\n    assert(it != tails[it->slabs_clsid]);\n    assert(it->refcount == 0);\n\n    /* so slab size changer can tell later if item is already free or not */\n    clsid = ITEM_clsid(it);\n    DEBUG_REFCNT(it, 'F');\n    slabs_free(it, clsid);\n}\n\n/**\n * Returns true if an item will fit in the cache (its size does not exceed\n * the maximum for a cache entry.)\n */\nbool item_size_ok(const size_t nkey, const client_flags_t flags, const int nbytes) {\n    char prefix[40];\n    uint8_t nsuffix;\n    if (nbytes < 2)\n        return false;\n\n    size_t ntotal = item_make_header(nkey + 1, flags, nbytes,\n                                     prefix, &nsuffix);\n    if (settings.use_cas) {\n        ntotal += sizeof(uint64_t);\n    }\n\n    return slabs_clsid(ntotal) != 0;\n}\n\n/* fixing stats/references during warm start */\nvoid do_item_link_fixup(item *it) {\n    item **head, **tail;\n    int ntotal = ITEM_ntotal(it);\n    uint32_t hv = hash(ITEM_key(it), it->nkey);\n    assoc_insert(it, hv);\n\n    head = &heads[it->slabs_clsid];\n    tail = &tails[it->slabs_clsid];\n    if (it->prev == 0 && *head == 0) *head = it;\n    if (it->next == 0 && *tail == 0) *tail = it;\n    sizes[it->slabs_clsid]++;\n    sizes_bytes[it->slabs_clsid] += ntotal;\n\n    STATS_LOCK();\n    stats_state.curr_bytes += ntotal;\n    stats_state.curr_items += 1;\n    stats.total_items += 1;\n    STATS_UNLOCK();\n\n    item_stats_sizes_add(it);\n\n    return;\n}\n\nstatic void do_item_link_q(item *it) { /* item is the new head */\n    item **head, **tail;\n    assert((it->it_flags & ITEM_SLABBED) == 0);\n\n    head = &heads[it->slabs_clsid];\n    tail = &tails[it->slabs_clsid];\n    assert(it != *head);\n    assert((*head && *tail) || (*head == 0 && *tail == 0));\n    it->prev = 0;\n    it->next = *head;\n    if (it->next) it->next->prev = it;\n    *head = it;\n    if (*tail == 0) *tail = it;\n    sizes[it->slabs_clsid]++;\n#ifdef EXTSTORE\n    if (it->it_flags & ITEM_HDR) {\n        sizes_bytes[it->slabs_clsid] += (ITEM_ntotal(it) - it->nbytes) + sizeof(item_hdr);\n    } else {\n        sizes_bytes[it->slabs_clsid] += ITEM_ntotal(it);\n    }\n#else\n    sizes_bytes[it->slabs_clsid] += ITEM_ntotal(it);\n#endif\n\n    return;\n}\n\nstatic void item_link_q(item *it) {\n    pthread_mutex_lock(&lru_locks[it->slabs_clsid]);\n    do_item_link_q(it);\n    pthread_mutex_unlock(&lru_locks[it->slabs_clsid]);\n}\n\nstatic void item_link_q_warm(item *it) {\n    pthread_mutex_lock(&lru_locks[it->slabs_clsid]);\n    do_item_link_q(it);\n    itemstats[it->slabs_clsid].moves_to_warm++;\n    pthread_mutex_unlock(&lru_locks[it->slabs_clsid]);\n}\n\nstatic void do_item_unlink_q(item *it) {\n    item **head, **tail;\n    head = &heads[it->slabs_clsid];\n    tail = &tails[it->slabs_clsid];\n\n    if (*head == it) {\n        assert(it->prev == 0);\n        *head = it->next;\n    }\n    if (*tail == it) {\n        assert(it->next == 0);\n        *tail = it->prev;\n    }\n    assert(it->next != it);\n    assert(it->prev != it);\n\n    if (it->next) it->next->prev = it->prev;\n    if (it->prev) it->prev->next = it->next;\n    sizes[it->slabs_clsid]--;\n#ifdef EXTSTORE\n    if (it->it_flags & ITEM_HDR) {\n        sizes_bytes[it->slabs_clsid] -= (ITEM_ntotal(it) - it->nbytes) + sizeof(item_hdr);\n    } else {\n        sizes_bytes[it->slabs_clsid] -= ITEM_ntotal(it);\n    }\n#else\n    sizes_bytes[it->slabs_clsid] -= ITEM_ntotal(it);\n#endif\n\n    return;\n}\n\nstatic void item_unlink_q(item *it) {\n    pthread_mutex_lock(&lru_locks[it->slabs_clsid]);\n    do_item_unlink_q(it);\n    pthread_mutex_unlock(&lru_locks[it->slabs_clsid]);\n}\n\nint do_item_link(item *it, const uint32_t hv, const uint64_t cas) {\n    MEMCACHED_ITEM_LINK(ITEM_key(it), it->nkey, it->nbytes);\n    assert((it->it_flags & (ITEM_LINKED|ITEM_SLABBED)) == 0);\n    it->it_flags |= ITEM_LINKED;\n    it->time = current_time;\n\n    STATS_LOCK();\n    stats_state.curr_bytes += ITEM_ntotal(it);\n    stats_state.curr_items += 1;\n    stats.total_items += 1;\n    STATS_UNLOCK();\n\n    /* Allocate a new CAS ID on link. */\n    ITEM_set_cas(it, cas);\n    assoc_insert(it, hv);\n    item_link_q(it);\n    refcount_incr(it);\n    item_stats_sizes_add(it);\n\n    return 1;\n}\n\nvoid do_item_unlink(item *it, const uint32_t hv) {\n    MEMCACHED_ITEM_UNLINK(ITEM_key(it), it->nkey, it->nbytes);\n    if ((it->it_flags & ITEM_LINKED) != 0) {\n        it->it_flags &= ~ITEM_LINKED;\n        STATS_LOCK();\n        stats_state.curr_bytes -= ITEM_ntotal(it);\n        stats_state.curr_items -= 1;\n        STATS_UNLOCK();\n        item_stats_sizes_remove(it);\n        assoc_delete(ITEM_key(it), it->nkey, hv);\n        item_unlink_q(it);\n        do_item_remove(it);\n    }\n}\n\n/* FIXME: Is it necessary to keep this copy/pasted code? */\nvoid do_item_unlink_nolock(item *it, const uint32_t hv) {\n    MEMCACHED_ITEM_UNLINK(ITEM_key(it), it->nkey, it->nbytes);\n    if ((it->it_flags & ITEM_LINKED) != 0) {\n        it->it_flags &= ~ITEM_LINKED;\n        STATS_LOCK();\n        stats_state.curr_bytes -= ITEM_ntotal(it);\n        stats_state.curr_items -= 1;\n        STATS_UNLOCK();\n        item_stats_sizes_remove(it);\n        assoc_delete(ITEM_key(it), it->nkey, hv);\n        do_item_unlink_q(it);\n        do_item_remove(it);\n    }\n}\n\nvoid do_item_remove(item *it) {\n    MEMCACHED_ITEM_REMOVE(ITEM_key(it), it->nkey, it->nbytes);\n    assert((it->it_flags & ITEM_SLABBED) == 0);\n    assert(it->refcount > 0);\n\n    if (refcount_decr(it) == 0) {\n        item_free(it);\n    }\n}\n\n/* Bump the last accessed time, or relink if we're in compat mode */\nvoid do_item_update(item *it) {\n    MEMCACHED_ITEM_UPDATE(ITEM_key(it), it->nkey, it->nbytes);\n\n    /* Hits to COLD_LRU immediately move to WARM. */\n    if (settings.lru_segmented) {\n        assert((it->it_flags & ITEM_SLABBED) == 0);\n        if ((it->it_flags & ITEM_LINKED) != 0) {\n            if (ITEM_lruid(it) == COLD_LRU && (it->it_flags & ITEM_ACTIVE)) {\n                it->time = current_time;\n                item_unlink_q(it);\n                it->slabs_clsid = ITEM_clsid(it);\n                it->slabs_clsid |= WARM_LRU;\n                it->it_flags &= ~ITEM_ACTIVE;\n                item_link_q_warm(it);\n            } else {\n                it->time = current_time;\n            }\n        }\n    } else if (it->time < current_time - ITEM_UPDATE_INTERVAL) {\n        assert((it->it_flags & ITEM_SLABBED) == 0);\n\n        if ((it->it_flags & ITEM_LINKED) != 0) {\n            it->time = current_time;\n            item_unlink_q(it);\n            item_link_q(it);\n        }\n    }\n}\n\nint do_item_replace(item *it, item *new_it, const uint32_t hv, const uint64_t cas) {\n    MEMCACHED_ITEM_REPLACE(ITEM_key(it), it->nkey, it->nbytes,\n                           ITEM_key(new_it), new_it->nkey, new_it->nbytes);\n    assert((it->it_flags & ITEM_SLABBED) == 0);\n\n    do_item_unlink(it, hv);\n    return do_item_link(new_it, hv, cas);\n}\n\nvoid item_flush_expired(void) {\n    int i;\n    item *iter, *next;\n    if (settings.oldest_live == 0)\n        return;\n    for (i = 0; i < LARGEST_ID; i++) {\n        /* The LRU is sorted in decreasing time order, and an item's timestamp\n         * is never newer than its last access time, so we only need to walk\n         * back until we hit an item older than the oldest_live time.\n         * The oldest_live checking will auto-expire the remaining items.\n         */\n        pthread_mutex_lock(&lru_locks[i]);\n        for (iter = heads[i]; iter != NULL; iter = next) {\n            void *hold_lock = NULL;\n            next = iter->next;\n            if (iter->time == 0 && iter->nkey == 0 && iter->it_flags == 1) {\n                continue; // crawler item.\n            }\n            uint32_t hv = hash(ITEM_key(iter), iter->nkey);\n            // if we can't lock the item, just give up.\n            // we can't block here because the lock order is inverted.\n            if ((hold_lock = item_trylock(hv)) == NULL) {\n                continue;\n            }\n\n            if (iter->time >= settings.oldest_live) {\n                // note: not sure why SLABBED check is here. linked and slabbed\n                // are mutually exclusive, but this can't hurt and I don't\n                // want to validate it right now.\n                if ((iter->it_flags & ITEM_SLABBED) == 0) {\n                    STORAGE_delete(ext_storage, iter);\n                    // nolock version because we hold the LRU lock already.\n                    do_item_unlink_nolock(iter, hash(ITEM_key(iter), iter->nkey));\n                }\n                item_trylock_unlock(hold_lock);\n            } else {\n                /* We've hit the first old item. Continue to the next queue. */\n                item_trylock_unlock(hold_lock);\n                break;\n            }\n        }\n        pthread_mutex_unlock(&lru_locks[i]);\n    }\n}\n\n/*@null@*/\n/* This is walking the line of violating lock order, but I think it's safe.\n * If the LRU lock is held, an item in the LRU cannot be wiped and freed.\n * The data could possibly be overwritten, but this is only accessing the\n * headers.\n * It may not be the best idea to leave it like this, but for now it's safe.\n */\nchar *item_cachedump(const unsigned int slabs_clsid, const unsigned int limit, unsigned int *bytes) {\n    unsigned int memlimit = 2 * 1024 * 1024;   /* 2MB max response size */\n    char *buffer;\n    unsigned int bufcurr;\n    item *it;\n    unsigned int len;\n    unsigned int shown = 0;\n    char key_temp[KEY_MAX_LENGTH + 1];\n    char temp[512];\n    unsigned int id = slabs_clsid;\n    id |= COLD_LRU;\n\n    pthread_mutex_lock(&lru_locks[id]);\n    it = heads[id];\n\n    buffer = malloc((size_t)memlimit);\n    if (buffer == 0) {\n        pthread_mutex_unlock(&lru_locks[id]);\n        return NULL;\n    }\n    bufcurr = 0;\n\n    while (it != NULL && (limit == 0 || shown < limit)) {\n        assert(it->nkey <= KEY_MAX_LENGTH);\n        // protect from printing binary keys.\n        if ((it->nbytes == 0 && it->nkey == 0) || (it->it_flags & ITEM_KEY_BINARY)) {\n            it = it->next;\n            continue;\n        }\n        /* Copy the key since it may not be null-terminated in the struct */\n        strncpy(key_temp, ITEM_key(it), it->nkey);\n        key_temp[it->nkey] = 0x00; /* terminate */\n        len = snprintf(temp, sizeof(temp), \"ITEM %s [%d b; %llu s]\\r\\n\",\n                       key_temp, it->nbytes - 2,\n                       it->exptime == 0 ? 0 :\n                       (unsigned long long)it->exptime + process_started);\n        if (bufcurr + len + 6 > memlimit)  /* 6 is END\\r\\n\\0 */\n            break;\n        memcpy(buffer + bufcurr, temp, len);\n        bufcurr += len;\n        shown++;\n        it = it->next;\n    }\n\n    memcpy(buffer + bufcurr, \"END\\r\\n\", 6);\n    bufcurr += 5;\n\n    *bytes = bufcurr;\n    pthread_mutex_unlock(&lru_locks[id]);\n    return buffer;\n}\n\n/* With refactoring of the various stats code the automover shouldn't need a\n * custom function here.\n */\nvoid fill_item_stats_automove(item_stats_automove *am) {\n    int n;\n    for (n = 0; n < MAX_NUMBER_OF_SLAB_CLASSES; n++) {\n        item_stats_automove *cur = &am[n];\n\n        // outofmemory records into HOT\n        int i = n | HOT_LRU;\n        pthread_mutex_lock(&lru_locks[i]);\n        cur->outofmemory = itemstats[i].outofmemory;\n        pthread_mutex_unlock(&lru_locks[i]);\n\n        // evictions and tail age are from COLD\n        i = n | COLD_LRU;\n        pthread_mutex_lock(&lru_locks[i]);\n        cur->evicted = itemstats[i].evicted;\n        if (!tails[i]) {\n            cur->age = 0;\n        } else if (tails[i]->nbytes == 0 && tails[i]->nkey == 0 && tails[i]->it_flags == 1) {\n            /* it's a crawler, check previous entry */\n            if (tails[i]->prev) {\n               cur->age = current_time - tails[i]->prev->time;\n            } else {\n               cur->age = 0;\n            }\n        } else {\n            cur->age = current_time - tails[i]->time;\n        }\n        pthread_mutex_unlock(&lru_locks[i]);\n     }\n}\n\nvoid item_stats_totals(ADD_STAT add_stats, void *c) {\n    itemstats_t totals;\n    memset(&totals, 0, sizeof(itemstats_t));\n    int n;\n    for (n = 0; n < MAX_NUMBER_OF_SLAB_CLASSES; n++) {\n        int x;\n        int i;\n        for (x = 0; x < 4; x++) {\n            i = n | lru_type_map[x];\n            pthread_mutex_lock(&lru_locks[i]);\n            totals.evicted += itemstats[i].evicted;\n            totals.reclaimed += itemstats[i].reclaimed;\n            totals.expired_unfetched += itemstats[i].expired_unfetched;\n            totals.evicted_unfetched += itemstats[i].evicted_unfetched;\n            totals.evicted_active += itemstats[i].evicted_active;\n            totals.crawler_reclaimed += itemstats[i].crawler_reclaimed;\n            totals.crawler_items_checked += itemstats[i].crawler_items_checked;\n            totals.lrutail_reflocked += itemstats[i].lrutail_reflocked;\n            totals.moves_to_cold += itemstats[i].moves_to_cold;\n            totals.moves_to_warm += itemstats[i].moves_to_warm;\n            totals.moves_within_lru += itemstats[i].moves_within_lru;\n            totals.direct_reclaims += itemstats[i].direct_reclaims;\n            pthread_mutex_unlock(&lru_locks[i]);\n        }\n    }\n    APPEND_STAT(\"expired_unfetched\", \"%llu\",\n                (unsigned long long)totals.expired_unfetched);\n    APPEND_STAT(\"evicted_unfetched\", \"%llu\",\n                (unsigned long long)totals.evicted_unfetched);\n    if (settings.lru_maintainer_thread) {\n        APPEND_STAT(\"evicted_active\", \"%llu\",\n                    (unsigned long long)totals.evicted_active);\n    }\n    APPEND_STAT(\"evictions\", \"%llu\",\n                (unsigned long long)totals.evicted);\n    APPEND_STAT(\"reclaimed\", \"%llu\",\n                (unsigned long long)totals.reclaimed);\n    APPEND_STAT(\"crawler_reclaimed\", \"%llu\",\n                (unsigned long long)totals.crawler_reclaimed);\n    APPEND_STAT(\"crawler_items_checked\", \"%llu\",\n                (unsigned long long)totals.crawler_items_checked);\n    APPEND_STAT(\"lrutail_reflocked\", \"%llu\",\n                (unsigned long long)totals.lrutail_reflocked);\n    if (settings.lru_maintainer_thread) {\n        APPEND_STAT(\"moves_to_cold\", \"%llu\",\n                    (unsigned long long)totals.moves_to_cold);\n        APPEND_STAT(\"moves_to_warm\", \"%llu\",\n                    (unsigned long long)totals.moves_to_warm);\n        APPEND_STAT(\"moves_within_lru\", \"%llu\",\n                    (unsigned long long)totals.moves_within_lru);\n        APPEND_STAT(\"direct_reclaims\", \"%llu\",\n                    (unsigned long long)totals.direct_reclaims);\n        APPEND_STAT(\"lru_bumps_dropped\", \"%llu\",\n                    (unsigned long long)lru_total_bumps_dropped());\n    }\n}\n\nvoid item_stats(ADD_STAT add_stats, void *c) {\n    struct thread_stats thread_stats;\n    threadlocal_stats_aggregate(&thread_stats);\n    itemstats_t totals;\n    int n;\n    for (n = 0; n < MAX_NUMBER_OF_SLAB_CLASSES; n++) {\n        memset(&totals, 0, sizeof(itemstats_t));\n        int x;\n        int i;\n        unsigned int size = 0;\n        unsigned int age  = 0;\n        unsigned int age_hot = 0;\n        unsigned int age_warm = 0;\n        unsigned int lru_size_map[4];\n        const char *fmt = \"items:%d:%s\";\n        char key_str[STAT_KEY_LEN];\n        char val_str[STAT_VAL_LEN];\n        int klen = 0, vlen = 0;\n        for (x = 0; x < 4; x++) {\n            i = n | lru_type_map[x];\n            pthread_mutex_lock(&lru_locks[i]);\n            totals.evicted += itemstats[i].evicted;\n            totals.evicted_nonzero += itemstats[i].evicted_nonzero;\n            totals.reclaimed += itemstats[i].reclaimed;\n            totals.outofmemory += itemstats[i].outofmemory;\n            totals.tailrepairs += itemstats[i].tailrepairs;\n            totals.expired_unfetched += itemstats[i].expired_unfetched;\n            totals.evicted_unfetched += itemstats[i].evicted_unfetched;\n            totals.evicted_active += itemstats[i].evicted_active;\n            totals.crawler_reclaimed += itemstats[i].crawler_reclaimed;\n            totals.crawler_items_checked += itemstats[i].crawler_items_checked;\n            totals.lrutail_reflocked += itemstats[i].lrutail_reflocked;\n            totals.moves_to_cold += itemstats[i].moves_to_cold;\n            totals.moves_to_warm += itemstats[i].moves_to_warm;\n            totals.moves_within_lru += itemstats[i].moves_within_lru;\n            totals.direct_reclaims += itemstats[i].direct_reclaims;\n            totals.mem_requested += sizes_bytes[i];\n            size += sizes[i];\n            lru_size_map[x] = sizes[i];\n            if (lru_type_map[x] == COLD_LRU && tails[i] != NULL) {\n                age = current_time - tails[i]->time;\n            } else if (lru_type_map[x] == HOT_LRU && tails[i] != NULL) {\n                age_hot = current_time - tails[i]->time;\n            } else if (lru_type_map[x] == WARM_LRU && tails[i] != NULL) {\n                age_warm = current_time - tails[i]->time;\n            }\n            if (lru_type_map[x] == COLD_LRU)\n                totals.evicted_time = itemstats[i].evicted_time;\n            switch (lru_type_map[x]) {\n                case HOT_LRU:\n                    totals.hits_to_hot = thread_stats.lru_hits[i];\n                    break;\n                case WARM_LRU:\n                    totals.hits_to_warm = thread_stats.lru_hits[i];\n                    break;\n                case COLD_LRU:\n                    totals.hits_to_cold = thread_stats.lru_hits[i];\n                    break;\n                case TEMP_LRU:\n                    totals.hits_to_temp = thread_stats.lru_hits[i];\n                    break;\n            }\n            pthread_mutex_unlock(&lru_locks[i]);\n        }\n        if (size == 0)\n            continue;\n        APPEND_NUM_FMT_STAT(fmt, n, \"number\", \"%u\", size);\n        if (settings.lru_maintainer_thread) {\n            APPEND_NUM_FMT_STAT(fmt, n, \"number_hot\", \"%u\", lru_size_map[0]);\n            APPEND_NUM_FMT_STAT(fmt, n, \"number_warm\", \"%u\", lru_size_map[1]);\n            APPEND_NUM_FMT_STAT(fmt, n, \"number_cold\", \"%u\", lru_size_map[2]);\n            if (settings.temp_lru) {\n                APPEND_NUM_FMT_STAT(fmt, n, \"number_temp\", \"%u\", lru_size_map[3]);\n            }\n            APPEND_NUM_FMT_STAT(fmt, n, \"age_hot\", \"%u\", age_hot);\n            APPEND_NUM_FMT_STAT(fmt, n, \"age_warm\", \"%u\", age_warm);\n        }\n        APPEND_NUM_FMT_STAT(fmt, n, \"age\", \"%u\", age);\n        APPEND_NUM_FMT_STAT(fmt, n, \"mem_requested\", \"%llu\", (unsigned long long)totals.mem_requested);\n        APPEND_NUM_FMT_STAT(fmt, n, \"evicted\",\n                            \"%llu\", (unsigned long long)totals.evicted);\n        APPEND_NUM_FMT_STAT(fmt, n, \"evicted_nonzero\",\n                            \"%llu\", (unsigned long long)totals.evicted_nonzero);\n        APPEND_NUM_FMT_STAT(fmt, n, \"evicted_time\",\n                            \"%u\", totals.evicted_time);\n        APPEND_NUM_FMT_STAT(fmt, n, \"outofmemory\",\n                            \"%llu\", (unsigned long long)totals.outofmemory);\n        APPEND_NUM_FMT_STAT(fmt, n, \"tailrepairs\",\n                            \"%llu\", (unsigned long long)totals.tailrepairs);\n        APPEND_NUM_FMT_STAT(fmt, n, \"reclaimed\",\n                            \"%llu\", (unsigned long long)totals.reclaimed);\n        APPEND_NUM_FMT_STAT(fmt, n, \"expired_unfetched\",\n                            \"%llu\", (unsigned long long)totals.expired_unfetched);\n        APPEND_NUM_FMT_STAT(fmt, n, \"evicted_unfetched\",\n                            \"%llu\", (unsigned long long)totals.evicted_unfetched);\n        if (settings.lru_maintainer_thread) {\n            APPEND_NUM_FMT_STAT(fmt, n, \"evicted_active\",\n                                \"%llu\", (unsigned long long)totals.evicted_active);\n        }\n        APPEND_NUM_FMT_STAT(fmt, n, \"crawler_reclaimed\",\n                            \"%llu\", (unsigned long long)totals.crawler_reclaimed);\n        APPEND_NUM_FMT_STAT(fmt, n, \"crawler_items_checked\",\n                            \"%llu\", (unsigned long long)totals.crawler_items_checked);\n        APPEND_NUM_FMT_STAT(fmt, n, \"lrutail_reflocked\",\n                            \"%llu\", (unsigned long long)totals.lrutail_reflocked);\n        if (settings.lru_maintainer_thread) {\n            APPEND_NUM_FMT_STAT(fmt, n, \"moves_to_cold\",\n                                \"%llu\", (unsigned long long)totals.moves_to_cold);\n            APPEND_NUM_FMT_STAT(fmt, n, \"moves_to_warm\",\n                                \"%llu\", (unsigned long long)totals.moves_to_warm);\n            APPEND_NUM_FMT_STAT(fmt, n, \"moves_within_lru\",\n                                \"%llu\", (unsigned long long)totals.moves_within_lru);\n            APPEND_NUM_FMT_STAT(fmt, n, \"direct_reclaims\",\n                                \"%llu\", (unsigned long long)totals.direct_reclaims);\n            APPEND_NUM_FMT_STAT(fmt, n, \"hits_to_hot\",\n                                \"%llu\", (unsigned long long)totals.hits_to_hot);\n\n            APPEND_NUM_FMT_STAT(fmt, n, \"hits_to_warm\",\n                                \"%llu\", (unsigned long long)totals.hits_to_warm);\n\n            APPEND_NUM_FMT_STAT(fmt, n, \"hits_to_cold\",\n                                \"%llu\", (unsigned long long)totals.hits_to_cold);\n\n            APPEND_NUM_FMT_STAT(fmt, n, \"hits_to_temp\",\n                                \"%llu\", (unsigned long long)totals.hits_to_temp);\n\n        }\n    }\n\n    /* getting here means both ascii and binary terminators fit */\n    add_stats(NULL, 0, NULL, 0, c);\n}\n\nbool item_stats_sizes_status(void) {\n    bool ret = false;\n    if (stats_sizes_hist != NULL)\n        ret = true;\n    return ret;\n}\n\nvoid item_stats_sizes_init(void) {\n    if (stats_sizes_hist != NULL)\n        return;\n    stats_sizes_buckets = settings.item_size_max / 32 + 1;\n    stats_sizes_hist = calloc(stats_sizes_buckets, sizeof(int));\n}\n\nvoid item_stats_sizes_add(item *it) {\n    if (stats_sizes_hist == NULL)\n        return;\n    int ntotal = ITEM_ntotal(it);\n    int bucket = ntotal / 32;\n    if ((ntotal % 32) != 0) bucket++;\n    if (bucket < stats_sizes_buckets) stats_sizes_hist[bucket]++;\n}\n\n/* I think there's no way for this to be accurate without using the CAS value.\n * Since items getting their time value bumped will pass this validation.\n */\nvoid item_stats_sizes_remove(item *it) {\n    if (stats_sizes_hist == NULL)\n        return;\n    int ntotal = ITEM_ntotal(it);\n    int bucket = ntotal / 32;\n    if ((ntotal % 32) != 0) bucket++;\n    if (bucket < stats_sizes_buckets) stats_sizes_hist[bucket]--;\n}\n\n/** dumps out a list of objects of each size, with granularity of 32 bytes */\n/*@null@*/\n/* Locks are correct based on a technicality. Holds LRU lock while doing the\n * work, so items can't go invalid, and it's only looking at header sizes\n * which don't change.\n */\nvoid item_stats_sizes(ADD_STAT add_stats, void *c) {\n    if (stats_sizes_hist != NULL) {\n        int i;\n        for (i = 0; i < stats_sizes_buckets; i++) {\n            if (stats_sizes_hist[i] != 0) {\n                char key[12];\n                snprintf(key, sizeof(key), \"%d\", i * 32);\n                APPEND_STAT(key, \"%u\", stats_sizes_hist[i]);\n            }\n        }\n    } else {\n        APPEND_STAT(\"sizes_status\", \"disabled\", \"\");\n    }\n\n    add_stats(NULL, 0, NULL, 0, c);\n}\n\n/** wrapper around assoc_find which does the lazy expiration logic */\nitem *do_item_get(const char *key, const size_t nkey, const uint32_t hv, LIBEVENT_THREAD *t, const bool do_update) {\n    item *it = assoc_find(key, nkey, hv);\n    if (it != NULL) {\n        refcount_incr(it);\n    }\n    int was_found = 0;\n\n    if (settings.verbose > 2) {\n        int ii;\n        if (it == NULL) {\n            fprintf(stderr, \"> NOT FOUND \");\n        } else if (was_found) {\n            fprintf(stderr, \"> FOUND KEY \");\n        }\n        for (ii = 0; ii < nkey; ++ii) {\n            fprintf(stderr, \"%c\", key[ii]);\n        }\n    }\n\n    if (it != NULL) {\n        was_found = 1;\n        if (item_is_flushed(it)) {\n            do_item_unlink(it, hv);\n            STORAGE_delete(t->storage, it);\n            do_item_remove(it);\n            it = NULL;\n            pthread_mutex_lock(&t->stats.mutex);\n            t->stats.get_flushed++;\n            pthread_mutex_unlock(&t->stats.mutex);\n            if (settings.verbose > 2) {\n                fprintf(stderr, \" -nuked by flush\");\n            }\n            was_found = 2;\n        } else if (it->exptime != 0 && it->exptime <= current_time) {\n            do_item_unlink(it, hv);\n            STORAGE_delete(t->storage, it);\n            do_item_remove(it);\n            it = NULL;\n            pthread_mutex_lock(&t->stats.mutex);\n            t->stats.get_expired++;\n            pthread_mutex_unlock(&t->stats.mutex);\n            if (settings.verbose > 2) {\n                fprintf(stderr, \" -nuked by expire\");\n            }\n            was_found = 3;\n        } else {\n            if (do_update) {\n                do_item_bump(t, it, hv);\n            }\n            DEBUG_REFCNT(it, '+');\n        }\n    }\n\n    if (settings.verbose > 2)\n        fprintf(stderr, \"\\n\");\n    /* For now this is in addition to the above verbose logging. */\n    LOGGER_LOG(t->l, LOG_FETCHERS, LOGGER_ITEM_GET, NULL, was_found, key,\n               nkey, (it) ? it->nbytes : 0, (it) ? ITEM_clsid(it) : 0, t->cur_sfd);\n\n    return it;\n}\n\n// Requires lock held for item.\n// Split out of do_item_get() to allow mget functions to look through header\n// data before losing state modified via the bump function.\nvoid do_item_bump(LIBEVENT_THREAD *t, item *it, const uint32_t hv) {\n    /* We update the hit markers only during fetches.\n     * An item needs to be hit twice overall to be considered\n     * ACTIVE, but only needs a single hit to maintain activity\n     * afterward.\n     * FETCHED tells if an item has ever been active.\n     */\n    if (settings.lru_segmented) {\n        if ((it->it_flags & ITEM_ACTIVE) == 0) {\n            if ((it->it_flags & ITEM_FETCHED) == 0) {\n                it->it_flags |= ITEM_FETCHED;\n            } else {\n                it->it_flags |= ITEM_ACTIVE;\n                if (ITEM_lruid(it) != COLD_LRU) {\n                    it->time = current_time; // only need to bump time.\n                } else if (!lru_bump_async(t->lru_bump_buf, it, hv)) {\n                    // add flag before async bump to avoid race.\n                    it->it_flags &= ~ITEM_ACTIVE;\n                }\n            }\n        }\n    } else {\n        it->it_flags |= ITEM_FETCHED;\n        do_item_update(it);\n    }\n}\n\nitem *do_item_touch(const char *key, size_t nkey, uint32_t exptime,\n                    const uint32_t hv, LIBEVENT_THREAD *t) {\n    item *it = do_item_get(key, nkey, hv, t, DO_UPDATE);\n    if (it != NULL) {\n        it->exptime = exptime;\n    }\n    return it;\n}\n\n/*** LRU MAINTENANCE THREAD ***/\n\n/* Returns number of items remove, expired, or evicted.\n * Callable from worker threads or the LRU maintainer thread */\nint lru_pull_tail(const int orig_id, const int cur_lru,\n        const uint64_t total_bytes, const uint8_t flags, const rel_time_t max_age,\n        struct lru_pull_tail_return *ret_it) {\n    item *it = NULL;\n    int id = orig_id;\n    int removed = 0;\n    if (id == 0)\n        return 0;\n\n    int tries = 5;\n    item *search;\n    item *next_it;\n    void *hold_lock = NULL;\n    unsigned int move_to_lru = 0;\n    uint64_t limit = 0;\n\n    id |= cur_lru;\n    pthread_mutex_lock(&lru_locks[id]);\n    search = tails[id];\n    /* We walk up *only* for locked items, and if bottom is expired. */\n    for (; tries > 0 && search != NULL; tries--, search=next_it) {\n        /* we might relink search mid-loop, so search->prev isn't reliable */\n        next_it = search->prev;\n        if (search->nbytes == 0 && search->nkey == 0 && search->it_flags == 1) {\n            /* We are a crawler, ignore it. */\n            if (flags & LRU_PULL_CRAWL_BLOCKS) {\n                pthread_mutex_unlock(&lru_locks[id]);\n                return 0;\n            }\n            tries++;\n            continue;\n        }\n        uint32_t hv = hash(ITEM_key(search), search->nkey);\n        /* Attempt to hash item lock the \"search\" item. If locked, no\n         * other callers can incr the refcount. Also skip ourselves. */\n        if ((hold_lock = item_trylock(hv)) == NULL)\n            continue;\n        /* Now see if the item is refcount locked */\n        if (refcount_incr(search) != 2) {\n            /* Note pathological case with ref'ed items in tail.\n             * Can still unlink the item, but it won't be reusable yet */\n            itemstats[id].lrutail_reflocked++;\n            /* In case of refcount leaks, enable for quick workaround. */\n            /* WARNING: This can cause terrible corruption */\n            if (settings.tail_repair_time &&\n                    search->time + settings.tail_repair_time < current_time) {\n                itemstats[id].tailrepairs++;\n                search->refcount = 1;\n                /* This will call item_remove -> item_free since refcnt is 1 */\n                STORAGE_delete(ext_storage, search);\n                do_item_unlink_nolock(search, hv);\n                item_trylock_unlock(hold_lock);\n                continue;\n            }\n        }\n\n        /* Expired or flushed */\n        if ((search->exptime != 0 && search->exptime < current_time)\n            || item_is_flushed(search)) {\n            itemstats[id].reclaimed++;\n            if ((search->it_flags & ITEM_FETCHED) == 0) {\n                itemstats[id].expired_unfetched++;\n            }\n            /* refcnt 2 -> 1 */\n            do_item_unlink_nolock(search, hv);\n            STORAGE_delete(ext_storage, search);\n            /* refcnt 1 -> 0 -> item_free */\n            do_item_remove(search);\n            item_trylock_unlock(hold_lock);\n            removed++;\n\n            /* If all we're finding are expired, can keep going */\n            continue;\n        }\n\n        /* If we're HOT_LRU or WARM_LRU and over size limit, send to COLD_LRU.\n         * If we're COLD_LRU, send to WARM_LRU unless we need to evict\n         */\n        switch (cur_lru) {\n            case HOT_LRU:\n                limit = total_bytes * settings.hot_lru_pct / 100;\n            case WARM_LRU:\n                if (limit == 0)\n                    limit = total_bytes * settings.warm_lru_pct / 100;\n                /* Rescue ACTIVE items aggressively */\n                if ((search->it_flags & ITEM_ACTIVE) != 0) {\n                    search->it_flags &= ~ITEM_ACTIVE;\n                    removed++;\n                    if (cur_lru == WARM_LRU) {\n                        itemstats[id].moves_within_lru++;\n                        do_item_unlink_q(search);\n                        do_item_link_q(search);\n                        do_item_remove(search);\n                        item_trylock_unlock(hold_lock);\n                    } else {\n                        /* Active HOT_LRU items flow to WARM */\n                        itemstats[id].moves_to_warm++;\n                        move_to_lru = WARM_LRU;\n                        do_item_unlink_q(search);\n                        it = search;\n                    }\n                } else if (sizes_bytes[id] > limit ||\n                           current_time - search->time > max_age) {\n                    itemstats[id].moves_to_cold++;\n                    move_to_lru = COLD_LRU;\n                    do_item_unlink_q(search);\n                    it = search;\n                    removed++;\n                    break;\n                } else {\n                    /* Don't want to move to COLD, not active, bail out */\n                    it = search;\n                }\n                break;\n            case COLD_LRU:\n                it = search; /* No matter what, we're stopping */\n                if (flags & LRU_PULL_EVICT) {\n                    if (settings.evict_to_free == 0) {\n                        /* Don't think we need a counter for this. It'll OOM.  */\n                        break;\n                    }\n                    itemstats[id].evicted++;\n                    itemstats[id].evicted_time = current_time - search->time;\n                    if (search->exptime != 0)\n                        itemstats[id].evicted_nonzero++;\n                    if ((search->it_flags & ITEM_FETCHED) == 0) {\n                        itemstats[id].evicted_unfetched++;\n                    }\n                    if ((search->it_flags & ITEM_ACTIVE)) {\n                        itemstats[id].evicted_active++;\n                    }\n                    LOGGER_LOG(NULL, LOG_EVICTIONS, LOGGER_EVICTION, search);\n                    STORAGE_delete(ext_storage, search);\n                    do_item_unlink_nolock(search, hv);\n                    removed++;\n                    if (settings.slab_automove == 2) {\n                        slabs_reassign(settings.slab_rebal, -1, orig_id, SLABS_REASSIGN_ALLOW_EVICTIONS);\n                    }\n                } else if (flags & LRU_PULL_RETURN_ITEM) {\n                    /* Keep a reference to this item and return it. */\n                    ret_it->it = it;\n                    ret_it->hv = hv;\n                } else if ((search->it_flags & ITEM_ACTIVE) != 0\n                        && settings.lru_segmented) {\n                    itemstats[id].moves_to_warm++;\n                    search->it_flags &= ~ITEM_ACTIVE;\n                    move_to_lru = WARM_LRU;\n                    do_item_unlink_q(search);\n                    removed++;\n                }\n                break;\n            case TEMP_LRU:\n                it = search; /* Kill the loop. Parent only interested in reclaims */\n                break;\n        }\n        if (it != NULL)\n            break;\n    }\n\n    pthread_mutex_unlock(&lru_locks[id]);\n\n    if (it != NULL) {\n        if (move_to_lru) {\n            it->slabs_clsid = ITEM_clsid(it);\n            it->slabs_clsid |= move_to_lru;\n            item_link_q(it);\n        }\n        if ((flags & LRU_PULL_RETURN_ITEM) == 0) {\n            do_item_remove(it);\n            item_trylock_unlock(hold_lock);\n        }\n    }\n\n    return removed;\n}\n\n\n/* TODO: Third place this code needs to be deduped */\nstatic void lru_bump_buf_link_q(lru_bump_buf *b) {\n    pthread_mutex_lock(&bump_buf_lock);\n    assert(b != bump_buf_head);\n\n    b->prev = 0;\n    b->next = bump_buf_head;\n    if (b->next) b->next->prev = b;\n    bump_buf_head = b;\n    if (bump_buf_tail == 0) bump_buf_tail = b;\n    pthread_mutex_unlock(&bump_buf_lock);\n    return;\n}\n\nvoid *item_lru_bump_buf_create(void) {\n    lru_bump_buf *b = calloc(1, sizeof(lru_bump_buf));\n    if (b == NULL) {\n        return NULL;\n    }\n\n    b->buf = bipbuf_new(sizeof(lru_bump_entry) * LRU_BUMP_BUF_SIZE);\n    if (b->buf == NULL) {\n        free(b);\n        return NULL;\n    }\n\n    pthread_mutex_init(&b->mutex, NULL);\n\n    lru_bump_buf_link_q(b);\n    return b;\n}\n\nstatic bool lru_bump_async(lru_bump_buf *b, item *it, uint32_t hv) {\n    bool ret = true;\n    refcount_incr(it);\n    pthread_mutex_lock(&b->mutex);\n    lru_bump_entry *be = (lru_bump_entry *) bipbuf_request(b->buf, sizeof(lru_bump_entry));\n    if (be != NULL) {\n        be->it = it;\n        be->hv = hv;\n        if (bipbuf_push(b->buf, sizeof(lru_bump_entry)) == 0) {\n            ret = false;\n            b->dropped++;\n        }\n    } else {\n        ret = false;\n        b->dropped++;\n    }\n    if (!ret) {\n        refcount_decr(it);\n    }\n    pthread_mutex_unlock(&b->mutex);\n    return ret;\n}\n\n/* TODO: Might be worth a micro-optimization of having bump buffers link\n * themselves back into the central queue when queue goes from zero to\n * non-zero, then remove from list if zero more than N times.\n * If very few hits on cold this would avoid extra memory barriers from LRU\n * maintainer thread. If many hits, they'll just stay in the list.\n */\nstatic bool lru_maintainer_bumps(void) {\n    lru_bump_buf *b;\n    lru_bump_entry *be;\n    unsigned int size;\n    unsigned int todo;\n    bool bumped = false;\n    pthread_mutex_lock(&bump_buf_lock);\n    for (b = bump_buf_head; b != NULL; b=b->next) {\n        pthread_mutex_lock(&b->mutex);\n        be = (lru_bump_entry *) bipbuf_peek_all(b->buf, &size);\n        pthread_mutex_unlock(&b->mutex);\n\n        if (be == NULL) {\n            continue;\n        }\n        todo = size;\n        bumped = true;\n\n        while (todo) {\n            item_lock(be->hv);\n            do_item_update(be->it);\n            do_item_remove(be->it);\n            item_unlock(be->hv);\n            be++;\n            todo -= sizeof(lru_bump_entry);\n        }\n\n        pthread_mutex_lock(&b->mutex);\n        be = (lru_bump_entry *) bipbuf_poll(b->buf, size);\n        pthread_mutex_unlock(&b->mutex);\n    }\n    pthread_mutex_unlock(&bump_buf_lock);\n    return bumped;\n}\n\nstatic uint64_t lru_total_bumps_dropped(void) {\n    uint64_t total = 0;\n    lru_bump_buf *b;\n    pthread_mutex_lock(&bump_buf_lock);\n    for (b = bump_buf_head; b != NULL; b=b->next) {\n        pthread_mutex_lock(&b->mutex);\n        total += b->dropped;\n        pthread_mutex_unlock(&b->mutex);\n    }\n    pthread_mutex_unlock(&bump_buf_lock);\n    return total;\n}\n\n/* Loop up to N times:\n * If too many items are in HOT_LRU, push to COLD_LRU\n * If too many items are in WARM_LRU, push to COLD_LRU\n * If too many items are in COLD_LRU, poke COLD_LRU tail\n * 1000 loops with 1ms min sleep gives us under 1m items shifted/sec. The\n * locks can't handle much more than that. Leaving a TODO for how to\n * autoadjust in the future.\n */\nstatic int lru_maintainer_juggle(const int slabs_clsid) {\n    int i;\n    int did_moves = 0;\n    uint64_t total_bytes = 0;\n    unsigned int chunks_perslab = 0;\n    //unsigned int chunks_free = 0;\n    /* TODO: if free_chunks below high watermark, increase aggressiveness */\n    slabs_available_chunks(slabs_clsid, NULL,\n            &chunks_perslab);\n    if (settings.temp_lru) {\n        /* Only looking for reclaims. Run before we size the LRU. */\n        for (i = 0; i < 500; i++) {\n            if (lru_pull_tail(slabs_clsid, TEMP_LRU, 0, 0, 0, NULL) <= 0) {\n                break;\n            } else {\n                did_moves++;\n            }\n        }\n    }\n\n    rel_time_t cold_age = 0;\n    rel_time_t hot_age = 0;\n    rel_time_t warm_age = 0;\n    /* If LRU is in flat mode, force items to drain into COLD via max age of 0 */\n    if (settings.lru_segmented) {\n        pthread_mutex_lock(&lru_locks[slabs_clsid|COLD_LRU]);\n        if (tails[slabs_clsid|COLD_LRU]) {\n            cold_age = current_time - tails[slabs_clsid|COLD_LRU]->time;\n        }\n        // Also build up total_bytes for the classes.\n        total_bytes += sizes_bytes[slabs_clsid|COLD_LRU];\n        pthread_mutex_unlock(&lru_locks[slabs_clsid|COLD_LRU]);\n\n        hot_age = cold_age * settings.hot_max_factor;\n        warm_age = cold_age * settings.warm_max_factor;\n\n        // total_bytes doesn't have to be exact. cache it for the juggles.\n        pthread_mutex_lock(&lru_locks[slabs_clsid|HOT_LRU]);\n        total_bytes += sizes_bytes[slabs_clsid|HOT_LRU];\n        pthread_mutex_unlock(&lru_locks[slabs_clsid|HOT_LRU]);\n\n        pthread_mutex_lock(&lru_locks[slabs_clsid|WARM_LRU]);\n        total_bytes += sizes_bytes[slabs_clsid|WARM_LRU];\n        pthread_mutex_unlock(&lru_locks[slabs_clsid|WARM_LRU]);\n    }\n\n    /* Juggle HOT/WARM up to N times */\n    for (i = 0; i < 500; i++) {\n        int do_more = 0;\n        if (lru_pull_tail(slabs_clsid, HOT_LRU, total_bytes, LRU_PULL_CRAWL_BLOCKS, hot_age, NULL) ||\n            lru_pull_tail(slabs_clsid, WARM_LRU, total_bytes, LRU_PULL_CRAWL_BLOCKS, warm_age, NULL)) {\n            do_more++;\n        }\n        if (settings.lru_segmented) {\n            do_more += lru_pull_tail(slabs_clsid, COLD_LRU, total_bytes, LRU_PULL_CRAWL_BLOCKS, 0, NULL);\n        }\n        if (do_more == 0)\n            break;\n        did_moves++;\n    }\n    return did_moves;\n}\n\n/* Will crawl all slab classes a minimum of once per hour */\n#define MAX_MAINTCRAWL_WAIT 60 * 60\n\n/* Hoping user input will improve this function. This is all a wild guess.\n * Operation: Kicks crawler for each slab id. Crawlers take some statistics as\n * to items with nonzero expirations. It then buckets how many items will\n * expire per minute for the next hour.\n * This function checks the results of a run, and if it things more than 1% of\n * expirable objects are ready to go, kick the crawler again to reap.\n * It will also kick the crawler once per minute regardless, waiting a minute\n * longer for each time it has no work to do, up to an hour wait time.\n * The latter is to avoid newly started daemons from waiting too long before\n * retrying a crawl.\n */\nstatic void lru_maintainer_crawler_check(struct crawler_expired_data *cdata, logger *l) {\n    int i;\n    static rel_time_t next_crawls[POWER_LARGEST];\n    static rel_time_t next_crawl_wait[POWER_LARGEST];\n    uint8_t todo[POWER_LARGEST];\n    memset(todo, 0, sizeof(uint8_t) * POWER_LARGEST);\n    bool do_run = false;\n    unsigned int tocrawl_limit = 0;\n\n    // TODO: If not segmented LRU, skip non-cold\n    for (i = POWER_SMALLEST; i < POWER_LARGEST; i++) {\n        crawlerstats_t *s = &cdata->crawlerstats[i];\n        /* We've not successfully kicked off a crawl yet. */\n        if (s->run_complete) {\n            char *lru_name = \"na\";\n            pthread_mutex_lock(&cdata->lock);\n            int x;\n            /* Should we crawl again? */\n            uint64_t possible_reclaims = s->seen - s->noexp;\n            uint64_t available_reclaims = 0;\n            /* Need to think we can free at least 1% of the items before\n             * crawling. */\n            /* FIXME: Configurable? */\n            uint64_t low_watermark = (possible_reclaims / 100) + 1;\n            rel_time_t since_run = current_time - s->end_time;\n            /* Don't bother if the payoff is too low. */\n            for (x = 0; x < 60; x++) {\n                available_reclaims += s->histo[x];\n                if (available_reclaims > low_watermark) {\n                    if (next_crawl_wait[i] < (x * 60)) {\n                        next_crawl_wait[i] += 60;\n                    } else if (next_crawl_wait[i] >= 60) {\n                        next_crawl_wait[i] -= 60;\n                    }\n                    break;\n                }\n            }\n\n            if (available_reclaims == 0) {\n                next_crawl_wait[i] += 60;\n            }\n\n            if (next_crawl_wait[i] > MAX_MAINTCRAWL_WAIT) {\n                next_crawl_wait[i] = MAX_MAINTCRAWL_WAIT;\n            }\n\n            next_crawls[i] = current_time + next_crawl_wait[i] + 5;\n            switch (GET_LRU(i)) {\n                case HOT_LRU:\n                    lru_name = \"hot\";\n                    break;\n                case WARM_LRU:\n                    lru_name = \"warm\";\n                    break;\n                case COLD_LRU:\n                    lru_name = \"cold\";\n                    break;\n                case TEMP_LRU:\n                    lru_name = \"temp\";\n                    break;\n            }\n            LOGGER_LOG(l, LOG_SYSEVENTS, LOGGER_CRAWLER_STATUS, NULL,\n                    CLEAR_LRU(i),\n                    lru_name,\n                    (unsigned long long)low_watermark,\n                    (unsigned long long)available_reclaims,\n                    (unsigned int)since_run,\n                    next_crawls[i] - current_time,\n                    s->end_time - s->start_time,\n                    s->seen,\n                    s->reclaimed);\n            // Got our calculation, avoid running until next actual run.\n            s->run_complete = false;\n            pthread_mutex_unlock(&cdata->lock);\n        }\n        if (current_time > next_crawls[i]) {\n            pthread_mutex_lock(&lru_locks[i]);\n            if (sizes[i] > tocrawl_limit) {\n                tocrawl_limit = sizes[i];\n            }\n            pthread_mutex_unlock(&lru_locks[i]);\n            todo[i] = 1;\n            do_run = true;\n            next_crawls[i] = current_time + 5; // minimum retry wait.\n        }\n    }\n    if (do_run) {\n        if (settings.lru_crawler_tocrawl && settings.lru_crawler_tocrawl < tocrawl_limit) {\n            tocrawl_limit = settings.lru_crawler_tocrawl;\n        }\n        lru_crawler_start(todo, tocrawl_limit, CRAWLER_AUTOEXPIRE, cdata, NULL, 0);\n    }\n}\n\nstatic pthread_t lru_maintainer_tid;\n\n#define MAX_LRU_MAINTAINER_SLEEP (1000000-1)\n#define MIN_LRU_MAINTAINER_SLEEP 1000\n\nstatic void *lru_maintainer_thread(void *arg) {\n    int i;\n    useconds_t to_sleep = MIN_LRU_MAINTAINER_SLEEP;\n    useconds_t last_sleep = MIN_LRU_MAINTAINER_SLEEP;\n    rel_time_t last_crawler_check = 0;\n    useconds_t next_juggles[MAX_NUMBER_OF_SLAB_CLASSES] = {0};\n    useconds_t backoff_juggles[MAX_NUMBER_OF_SLAB_CLASSES] = {0};\n    struct crawler_expired_data *cdata =\n        calloc(1, sizeof(struct crawler_expired_data));\n    if (cdata == NULL) {\n        fprintf(stderr, \"Failed to allocate crawler data for LRU maintainer thread\\n\");\n        abort();\n    }\n    pthread_mutex_init(&cdata->lock, NULL);\n    cdata->crawl_complete = true; // kick off the crawler.\n    logger *l = logger_create();\n    if (l == NULL) {\n        fprintf(stderr, \"Failed to allocate logger for LRU maintainer thread\\n\");\n        abort();\n    }\n\n    pthread_mutex_lock(&lru_maintainer_lock);\n    if (settings.verbose > 2)\n        fprintf(stderr, \"Starting LRU maintainer background thread\\n\");\n    while (do_run_lru_maintainer_thread) {\n        pthread_mutex_unlock(&lru_maintainer_lock);\n        if (to_sleep)\n            usleep(to_sleep);\n        pthread_mutex_lock(&lru_maintainer_lock);\n        /* A sleep of zero counts as a minimum of a 1ms wait */\n        last_sleep = to_sleep > 1000 ? to_sleep : 1000;\n        to_sleep = MAX_LRU_MAINTAINER_SLEEP;\n\n        STATS_LOCK();\n        stats.lru_maintainer_juggles++;\n        STATS_UNLOCK();\n\n        /* Each slab class gets its own sleep to avoid hammering locks */\n        for (i = POWER_SMALLEST; i < MAX_NUMBER_OF_SLAB_CLASSES; i++) {\n            next_juggles[i] = next_juggles[i] > last_sleep ? next_juggles[i] - last_sleep : 0;\n\n            if (next_juggles[i] > 0) {\n                // Sleep the thread just for the minimum amount (or not at all)\n                if (next_juggles[i] < to_sleep)\n                    to_sleep = next_juggles[i];\n                continue;\n            }\n\n            int did_moves = lru_maintainer_juggle(i);\n            if (did_moves == 0) {\n                if (backoff_juggles[i] != 0) {\n                    backoff_juggles[i] += backoff_juggles[i] / 8;\n                } else {\n                    backoff_juggles[i] = MIN_LRU_MAINTAINER_SLEEP;\n                }\n                if (backoff_juggles[i] > MAX_LRU_MAINTAINER_SLEEP)\n                    backoff_juggles[i] = MAX_LRU_MAINTAINER_SLEEP;\n            } else if (backoff_juggles[i] > 0) {\n                backoff_juggles[i] /= 2;\n                if (backoff_juggles[i] < MIN_LRU_MAINTAINER_SLEEP) {\n                    backoff_juggles[i] = 0;\n                }\n            }\n            next_juggles[i] = backoff_juggles[i];\n            if (next_juggles[i] < to_sleep)\n                to_sleep = next_juggles[i];\n        }\n\n        /* Minimize the sleep if we had async LRU bumps to process */\n        if (settings.lru_segmented && lru_maintainer_bumps() && to_sleep > 1000) {\n            to_sleep = 1000;\n        }\n\n        /* Once per second at most */\n        if (settings.lru_crawler && last_crawler_check != current_time) {\n            lru_maintainer_crawler_check(cdata, l);\n            last_crawler_check = current_time;\n        }\n    }\n    pthread_mutex_unlock(&lru_maintainer_lock);\n    // LRU crawler *must* be stopped.\n    free(cdata);\n    if (settings.verbose > 2)\n        fprintf(stderr, \"LRU maintainer thread stopping\\n\");\n\n    return NULL;\n}\n\nint stop_lru_maintainer_thread(void) {\n    int ret;\n    pthread_mutex_lock(&lru_maintainer_lock);\n    /* LRU thread is a sleep loop, will die on its own */\n    do_run_lru_maintainer_thread = 0;\n    pthread_mutex_unlock(&lru_maintainer_lock);\n    if ((ret = pthread_join(lru_maintainer_tid, NULL)) != 0) {\n        fprintf(stderr, \"Failed to stop LRU maintainer thread: %s\\n\", strerror(ret));\n        return -1;\n    }\n    settings.lru_maintainer_thread = false;\n    return 0;\n}\n\nint start_lru_maintainer_thread(void *arg) {\n    int ret;\n\n    pthread_mutex_lock(&lru_maintainer_lock);\n    do_run_lru_maintainer_thread = 1;\n    settings.lru_maintainer_thread = true;\n    if ((ret = pthread_create(&lru_maintainer_tid, NULL,\n        lru_maintainer_thread, arg)) != 0) {\n        fprintf(stderr, \"Can't create LRU maintainer thread: %s\\n\",\n            strerror(ret));\n        pthread_mutex_unlock(&lru_maintainer_lock);\n        return -1;\n    }\n    thread_setname(lru_maintainer_tid, \"mc-lrumaint\");\n    pthread_mutex_unlock(&lru_maintainer_lock);\n\n    return 0;\n}\n\n/* If we hold this lock, crawler can't wake up or move */\nvoid lru_maintainer_pause(void) {\n    pthread_mutex_lock(&lru_maintainer_lock);\n}\n\nvoid lru_maintainer_resume(void) {\n    pthread_mutex_unlock(&lru_maintainer_lock);\n}\n\n/* Tail linkers and crawler for the LRU crawler. */\nvoid do_item_linktail_q(item *it) { /* item is the new tail */\n    item **head, **tail;\n    assert(it->it_flags == 1);\n    assert(it->nbytes == 0);\n\n    head = &heads[it->slabs_clsid];\n    tail = &tails[it->slabs_clsid];\n    //assert(*tail != 0);\n    assert(it != *tail);\n    assert((*head && *tail) || (*head == 0 && *tail == 0));\n    it->prev = *tail;\n    it->next = 0;\n    if (it->prev) {\n        assert(it->prev->next == 0);\n        it->prev->next = it;\n    }\n    *tail = it;\n    if (*head == 0) *head = it;\n    return;\n}\n\nvoid do_item_unlinktail_q(item *it) {\n    item **head, **tail;\n    head = &heads[it->slabs_clsid];\n    tail = &tails[it->slabs_clsid];\n\n    if (*head == it) {\n        assert(it->prev == 0);\n        *head = it->next;\n    }\n    if (*tail == it) {\n        assert(it->next == 0);\n        *tail = it->prev;\n    }\n    assert(it->next != it);\n    assert(it->prev != it);\n\n    if (it->next) it->next->prev = it->prev;\n    if (it->prev) it->prev->next = it->next;\n    return;\n}\n\n/* This is too convoluted, but it's a difficult shuffle. Try to rewrite it\n * more clearly. */\nitem *do_item_crawl_q(item *it) {\n    item **head, **tail;\n    assert(it->it_flags == 1);\n    assert(it->nbytes == 0);\n    head = &heads[it->slabs_clsid];\n    tail = &tails[it->slabs_clsid];\n\n    /* We've hit the head, pop off */\n    if (it->prev == 0) {\n        assert(*head == it);\n        if (it->next) {\n            *head = it->next;\n            assert(it->next->prev == it);\n            it->next->prev = 0;\n        }\n        return NULL; /* Done */\n    }\n\n    /* Swing ourselves in front of the next item */\n    /* NB: If there is a prev, we can't be the head */\n    assert(it->prev != it);\n    if (it->prev) {\n        if (*head == it->prev) {\n            /* Prev was the head, now we're the head */\n            *head = it;\n        }\n        if (*tail == it) {\n            /* We are the tail, now they are the tail */\n            *tail = it->prev;\n        }\n        assert(it->next != it);\n        if (it->next) {\n            assert(it->prev->next == it);\n            it->prev->next = it->next;\n            it->next->prev = it->prev;\n        } else {\n            /* Tail. Move this above? */\n            it->prev->next = 0;\n        }\n        /* prev->prev's next is it->prev */\n        it->next = it->prev;\n        it->prev = it->next->prev;\n        it->next->prev = it;\n        /* New it->prev now, if we're not at the head. */\n        if (it->prev) {\n            it->prev->next = it;\n        }\n    }\n    assert(it->next != it);\n    assert(it->prev != it);\n\n    return it->next; /* success */\n}\n"
        },
        {
          "name": "items.h",
          "type": "blob",
          "size": 2.984375,
          "content": "#define HOT_LRU 0\n#define WARM_LRU 64\n#define COLD_LRU 128\n#define TEMP_LRU 192\n\n#define CLEAR_LRU(id) (id & ~(3<<6))\n#define GET_LRU(id) (id & (3<<6))\n\n/* See items.c */\nuint64_t get_cas_id(void);\nvoid set_cas_id(uint64_t new_cas);\n\n/*@null@*/\nitem *do_item_alloc(const char *key, const size_t nkey, const client_flags_t flags, const rel_time_t exptime, const int nbytes);\nitem_chunk *do_item_alloc_chunk(item_chunk *ch, const size_t bytes_remain);\nitem *do_item_alloc_pull(const size_t ntotal, const unsigned int id);\nvoid item_free(item *it);\nbool item_size_ok(const size_t nkey, const client_flags_t flags, const int nbytes);\n\nint  do_item_link(item *it, const uint32_t hv, const uint64_t cas);     /** may fail if transgresses limits */\nvoid do_item_unlink(item *it, const uint32_t hv);\nvoid do_item_unlink_nolock(item *it, const uint32_t hv);\nvoid do_item_remove(item *it);\nvoid do_item_update(item *it);   /** update LRU time to current and reposition */\nvoid do_item_update_nolock(item *it);\nint  do_item_replace(item *it, item *new_it, const uint32_t hv, const uint64_t cas);\nvoid do_item_link_fixup(item *it);\n\nint item_is_flushed(item *it);\nunsigned int do_get_lru_size(uint32_t id);\n\nvoid do_item_linktail_q(item *it);\nvoid do_item_unlinktail_q(item *it);\nitem *do_item_crawl_q(item *it);\n\nvoid *item_lru_bump_buf_create(void);\n\n#define LRU_PULL_EVICT 1\n#define LRU_PULL_CRAWL_BLOCKS 2\n#define LRU_PULL_RETURN_ITEM 4 /* fill info struct if available */\n\nstruct lru_pull_tail_return {\n    item *it;\n    uint32_t hv;\n};\n\nint lru_pull_tail(const int orig_id, const int cur_lru,\n        const uint64_t total_bytes, const uint8_t flags, const rel_time_t max_age,\n        struct lru_pull_tail_return *ret_it);\nvoid item_flush_expired(void);\n\n/*@null@*/\nchar *item_cachedump(const unsigned int slabs_clsid, const unsigned int limit, unsigned int *bytes);\nvoid item_stats(ADD_STAT add_stats, void *c);\nvoid do_item_stats_add_crawl(const int i, const uint64_t reclaimed,\n        const uint64_t unfetched, const uint64_t checked);\nvoid item_stats_totals(ADD_STAT add_stats, void *c);\n/*@null@*/\nvoid item_stats_sizes(ADD_STAT add_stats, void *c);\nvoid item_stats_sizes_init(void);\nvoid item_stats_sizes_add(item *it);\nvoid item_stats_sizes_remove(item *it);\nbool item_stats_sizes_status(void);\n\n/* stats getter for slab automover */\ntypedef struct {\n    int64_t evicted;\n    int64_t outofmemory;\n    uint32_t age;\n} item_stats_automove;\nvoid fill_item_stats_automove(item_stats_automove *am);\n\nitem *do_item_get(const char *key, const size_t nkey, const uint32_t hv, LIBEVENT_THREAD *t, const bool do_update);\nitem *do_item_touch(const char *key, const size_t nkey, uint32_t exptime, const uint32_t hv, LIBEVENT_THREAD *t);\nvoid do_item_bump(LIBEVENT_THREAD *t, item *it, const uint32_t hv);\nvoid item_stats_reset(void);\nextern pthread_mutex_t lru_locks[POWER_LARGEST];\n\nint start_lru_maintainer_thread(void *arg);\nint stop_lru_maintainer_thread(void);\nvoid lru_maintainer_pause(void);\nvoid lru_maintainer_resume(void);\n\nvoid *lru_bump_buf_create(void);\n"
        },
        {
          "name": "itoa_ljust.c",
          "type": "blob",
          "size": 5.119140625,
          "content": "//=== itoa_ljust.cpp - Fast integer to ascii conversion           --*- C++ -*-//\n//\n// Substantially simplified (and slightly faster) version\n// based on the following functions in Google's protocol buffers:\n//\n//    FastInt32ToBufferLeft()\n//    FastUInt32ToBufferLeft()\n//    FastInt64ToBufferLeft()\n//    FastUInt64ToBufferLeft()\n//\n// Differences:\n//    1) Greatly simplified\n//    2) Avoids GOTO statements - uses \"switch\" instead and relies on\n//       compiler constant folding and propagation for high performance\n//    3) Avoids unary minus of signed types - undefined behavior if value\n//       is INT_MIN in platforms using two's complement representation\n//    4) Uses memcpy to store 2 digits at a time - lets the compiler\n//       generate a 2-byte load/store in platforms that support\n//       unaligned access, this is faster (and less code) than explicitly\n//       loading and storing each byte\n//\n// Copyright (c) 2016 Arturo Martin-de-Nicolas\n// arturomdn@gmail.com\n// https://github.com/amdn/itoa_ljust/\n//\n// Released under the BSD 3-Clause License, see Google's original copyright\n// and license below.\n//===----------------------------------------------------------------------===//\n\n// Protocol Buffers - Google's data interchange format\n// Copyright 2008 Google Inc.  All rights reserved.\n// https://developers.google.com/protocol-buffers/\n//\n// Redistribution and use in source and binary forms, with or without\n// modification, are permitted provided that the following conditions are\n// met:\n//\n//     * Redistributions of source code must retain the above copyright\n// notice, this list of conditions and the following disclaimer.\n//     * Redistributions in binary form must reproduce the above\n// copyright notice, this list of conditions and the following disclaimer\n// in the documentation and/or other materials provided with the\n// distribution.\n//     * Neither the name of Google Inc. nor the names of its\n// contributors may be used to endorse or promote products derived from\n// this software without specific prior written permission.\n//\n// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n// \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n// LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n// A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n// OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n// LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n// DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n// THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n// OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n//===----------------------------------------------------------------------===//\n\n#include \"itoa_ljust.h\"\n#include <string.h>\n\nstatic const char lut[201] =\n    \"0001020304050607080910111213141516171819\"\n    \"2021222324252627282930313233343536373839\"\n    \"4041424344454647484950515253545556575859\"\n    \"6061626364656667686970717273747576777879\"\n    \"8081828384858687888990919293949596979899\";\n\n#define dd(u) ((const uint16_t)(lut[u]))\n\nstatic inline char* out2(const int d, char* p) {\n    memcpy(p, &((uint16_t *)lut)[d], 2);\n    return p + 2;\n}\n\nstatic inline char* out1(const char in, char* p) {\n    memcpy(p, &in, 1);\n    return p + 1;\n}\n\nstatic inline int digits( uint32_t u, unsigned k, int* d, char** p, int n ) {\n    if (u < k*10) {\n        *d = u / k;\n        *p = out1('0'+*d, *p);\n        --n;\n    }\n    return n;\n}\n\nstatic inline char* itoa(uint32_t u, char* p, int d, int n) {\n    switch(n) {\n    case 10: d  = u / 100000000; p = out2( d, p );\n    case  9: u -= d * 100000000;\n    case  8: d  = u /   1000000; p = out2( d, p );\n    case  7: u -= d *   1000000;\n    case  6: d  = u /     10000; p = out2( d, p );\n    case  5: u -= d *     10000;\n    case  4: d  = u /       100; p = out2( d, p );\n    case  3: u -= d *       100;\n    case  2: d  = u /         1; p = out2( d, p );\n    case  1: ;\n    }\n    *p = '\\0';\n    return p;\n}\n\nchar* itoa_u32(uint32_t u, char* p) {\n    int d = 0,n;\n         if (u >=100000000) n = digits(u, 100000000, &d, &p, 10);\n    else if (u <       100) n = digits(u,         1, &d, &p,  2);\n    else if (u <     10000) n = digits(u,       100, &d, &p,  4);\n    else if (u <   1000000) n = digits(u,     10000, &d, &p,  6);\n    else                    n = digits(u,   1000000, &d, &p,  8);\n    return itoa( u, p, d, n );\n}\n\nchar* itoa_32(int32_t i, char* p) {\n    uint32_t u = i;\n    if (i < 0) {\n        *p++ = '-';\n        u = -u;\n    }\n    return itoa_u32(u, p);\n}\n\nchar* itoa_u64(uint64_t u, char* p) {\n    int d;\n\n    uint32_t lower = (uint32_t)u;\n    if (lower == u) return itoa_u32(lower, p);\n\n    uint64_t upper = u / 1000000000;\n    p = itoa_u64(upper, p);\n    lower = u - (upper * 1000000000);\n    d = lower / 100000000;\n    p = out1('0'+d,p);\n    return itoa( lower, p, d, 9 );\n}\n\nchar* itoa_64(int64_t i, char* p) {\n    uint64_t u = i;\n    if (i < 0) {\n        *p++ = '-';\n        u = -u;\n    }\n    return itoa_u64(u, p);\n}\n"
        },
        {
          "name": "itoa_ljust.h",
          "type": "blob",
          "size": 0.802734375,
          "content": "#ifndef ITOA_LJUST_H\n#define ITOA_LJUST_H\n\n//=== itoa_ljust.h - Fast integer to ascii conversion\n//\n// Fast and simple integer to ASCII conversion:\n//\n//   - 32 and 64-bit integers\n//   - signed and unsigned\n//   - user supplied buffer must be large enough for all decimal digits\n//     in value plus minus sign if negative\n//   - left-justified\n//   - NUL terminated\n//   - return value is pointer to NUL terminator\n//\n// Copyright (c) 2016 Arturo Martin-de-Nicolas\n// arturomdn@gmail.com\n// https://github.com/amdn/itoa_ljust/\n//===----------------------------------------------------------------------===//\n\n#include <stdint.h>\n\nchar* itoa_u32(uint32_t u, char* buffer);\nchar* itoa_32( int32_t i, char* buffer);\nchar* itoa_u64(uint64_t u, char* buffer);\nchar* itoa_64( int64_t i, char* buffer);\n\n#endif // ITOA_LJUST_H\n"
        },
        {
          "name": "jenkins_hash.c",
          "type": "blob",
          "size": 14.3642578125,
          "content": "/* -*- Mode: C; tab-width: 4; c-basic-offset: 4; indent-tabs-mode: nil -*- */\n/*\n * Hash table\n *\n * The hash function used here is by Bob Jenkins, 1996:\n *    <http://burtleburtle.net/bob/hash/doobs.html>\n *       \"By Bob Jenkins, 1996.  bob_jenkins@burtleburtle.net.\n *       You may use this code any way you wish, private, educational,\n *       or commercial.  It's free.\"\n *\n */\n#include \"memcached.h\"\n#include \"jenkins_hash.h\"\n\n/*\n * Since the hash function does bit manipulation, it needs to know\n * whether it's big or little-endian. ENDIAN_LITTLE and ENDIAN_BIG\n * are set in the configure script.\n */\n#if ENDIAN_BIG == 1\n# define HASH_LITTLE_ENDIAN 0\n# define HASH_BIG_ENDIAN 1\n#else\n# if ENDIAN_LITTLE == 1\n#  define HASH_LITTLE_ENDIAN 1\n#  define HASH_BIG_ENDIAN 0\n# else\n#  define HASH_LITTLE_ENDIAN 0\n#  define HASH_BIG_ENDIAN 0\n# endif\n#endif\n\n#define rot(x,k) (((x)<<(k)) ^ ((x)>>(32-(k))))\n\n/*\n-------------------------------------------------------------------------------\nmix -- mix 3 32-bit values reversibly.\n\nThis is reversible, so any information in (a,b,c) before mix() is\nstill in (a,b,c) after mix().\n\nIf four pairs of (a,b,c) inputs are run through mix(), or through\nmix() in reverse, there are at least 32 bits of the output that\nare sometimes the same for one pair and different for another pair.\nThis was tested for:\n* pairs that differed by one bit, by two bits, in any combination\n  of top bits of (a,b,c), or in any combination of bottom bits of\n  (a,b,c).\n* \"differ\" is defined as +, -, ^, or ~^.  For + and -, I transformed\n  the output delta to a Gray code (a^(a>>1)) so a string of 1's (as\n  is commonly produced by subtraction) look like a single 1-bit\n  difference.\n* the base values were pseudorandom, all zero but one bit set, or\n  all zero plus a counter that starts at zero.\n\nSome k values for my \"a-=c; a^=rot(c,k); c+=b;\" arrangement that\nsatisfy this are\n    4  6  8 16 19  4\n    9 15  3 18 27 15\n   14  9  3  7 17  3\nWell, \"9 15 3 18 27 15\" didn't quite get 32 bits diffing\nfor \"differ\" defined as + with a one-bit base and a two-bit delta.  I\nused http://burtleburtle.net/bob/hash/avalanche.html to choose\nthe operations, constants, and arrangements of the variables.\n\nThis does not achieve avalanche.  There are input bits of (a,b,c)\nthat fail to affect some output bits of (a,b,c), especially of a.  The\nmost thoroughly mixed value is c, but it doesn't really even achieve\navalanche in c.\n\nThis allows some parallelism.  Read-after-writes are good at doubling\nthe number of bits affected, so the goal of mixing pulls in the opposite\ndirection as the goal of parallelism.  I did what I could.  Rotates\nseem to cost as much as shifts on every machine I could lay my hands\non, and rotates are much kinder to the top and bottom bits, so I used\nrotates.\n-------------------------------------------------------------------------------\n*/\n#define mix(a,b,c) \\\n{ \\\n  a -= c;  a ^= rot(c, 4);  c += b; \\\n  b -= a;  b ^= rot(a, 6);  a += c; \\\n  c -= b;  c ^= rot(b, 8);  b += a; \\\n  a -= c;  a ^= rot(c,16);  c += b; \\\n  b -= a;  b ^= rot(a,19);  a += c; \\\n  c -= b;  c ^= rot(b, 4);  b += a; \\\n}\n\n/*\n-------------------------------------------------------------------------------\nfinal -- final mixing of 3 32-bit values (a,b,c) into c\n\nPairs of (a,b,c) values differing in only a few bits will usually\nproduce values of c that look totally different.  This was tested for\n* pairs that differed by one bit, by two bits, in any combination\n  of top bits of (a,b,c), or in any combination of bottom bits of\n  (a,b,c).\n* \"differ\" is defined as +, -, ^, or ~^.  For + and -, I transformed\n  the output delta to a Gray code (a^(a>>1)) so a string of 1's (as\n  is commonly produced by subtraction) look like a single 1-bit\n  difference.\n* the base values were pseudorandom, all zero but one bit set, or\n  all zero plus a counter that starts at zero.\n\nThese constants passed:\n 14 11 25 16 4 14 24\n 12 14 25 16 4 14 24\nand these came close:\n  4  8 15 26 3 22 24\n 10  8 15 26 3 22 24\n 11  8 15 26 3 22 24\n-------------------------------------------------------------------------------\n*/\n#define final(a,b,c) \\\n{ \\\n  c ^= b; c -= rot(b,14); \\\n  a ^= c; a -= rot(c,11); \\\n  b ^= a; b -= rot(a,25); \\\n  c ^= b; c -= rot(b,16); \\\n  a ^= c; a -= rot(c,4);  \\\n  b ^= a; b -= rot(a,14); \\\n  c ^= b; c -= rot(b,24); \\\n}\n\n#if HASH_LITTLE_ENDIAN == 1\nuint32_t jenkins_hash(\n  const void *key,       /* the key to hash */\n  size_t      length)    /* length of the key */\n{\n  uint32_t a,b,c;                                          /* internal state */\n  union { const void *ptr; size_t i; } u;     /* needed for Mac Powerbook G4 */\n\n  /* Set up the internal state */\n  a = b = c = 0xdeadbeef + ((uint32_t)length) + 0;\n\n  u.ptr = key;\n  if (HASH_LITTLE_ENDIAN && ((u.i & 0x3) == 0)) {\n    const uint32_t *k = key;                           /* read 32-bit chunks */\n#ifdef VALGRIND\n    const uint8_t  *k8;\n#endif /* ifdef VALGRIND */\n\n    /*------ all but last block: aligned reads and affect 32 bits of (a,b,c) */\n    while (length > 12)\n    {\n      a += k[0];\n      b += k[1];\n      c += k[2];\n      mix(a,b,c);\n      length -= 12;\n      k += 3;\n    }\n\n    /*----------------------------- handle the last (probably partial) block */\n    /*\n     * \"k[2]&0xffffff\" actually reads beyond the end of the string, but\n     * then masks off the part it's not allowed to read.  Because the\n     * string is aligned, the masked-off tail is in the same word as the\n     * rest of the string.  Every machine with memory protection I've seen\n     * does it on word boundaries, so is OK with this.  But VALGRIND will\n     * still catch it and complain.  The masking trick does make the hash\n     * noticeably faster for short strings (like English words).\n     */\n#ifndef VALGRIND\n\n    switch(length)\n    {\n    case 12: c+=k[2]; b+=k[1]; a+=k[0]; break;\n    case 11: c+=k[2]&0xffffff; b+=k[1]; a+=k[0]; break;\n    case 10: c+=k[2]&0xffff; b+=k[1]; a+=k[0]; break;\n    case 9 : c+=k[2]&0xff; b+=k[1]; a+=k[0]; break;\n    case 8 : b+=k[1]; a+=k[0]; break;\n    case 7 : b+=k[1]&0xffffff; a+=k[0]; break;\n    case 6 : b+=k[1]&0xffff; a+=k[0]; break;\n    case 5 : b+=k[1]&0xff; a+=k[0]; break;\n    case 4 : a+=k[0]; break;\n    case 3 : a+=k[0]&0xffffff; break;\n    case 2 : a+=k[0]&0xffff; break;\n    case 1 : a+=k[0]&0xff; break;\n    case 0 : return c;  /* zero length strings require no mixing */\n    }\n\n#else /* make valgrind happy */\n\n    k8 = (const uint8_t *)k;\n    switch(length)\n    {\n    case 12: c+=k[2]; b+=k[1]; a+=k[0]; break;\n    case 11: c+=((uint32_t)k8[10])<<16;  /* fall through */\n    case 10: c+=((uint32_t)k8[9])<<8;    /* fall through */\n    case 9 : c+=k8[8];                   /* fall through */\n    case 8 : b+=k[1]; a+=k[0]; break;\n    case 7 : b+=((uint32_t)k8[6])<<16;   /* fall through */\n    case 6 : b+=((uint32_t)k8[5])<<8;    /* fall through */\n    case 5 : b+=k8[4];                   /* fall through */\n    case 4 : a+=k[0]; break;\n    case 3 : a+=((uint32_t)k8[2])<<16;   /* fall through */\n    case 2 : a+=((uint32_t)k8[1])<<8;    /* fall through */\n    case 1 : a+=k8[0]; break;\n    case 0 : return c;  /* zero length strings require no mixing */\n    }\n\n#endif /* !valgrind */\n\n  } else if (HASH_LITTLE_ENDIAN && ((u.i & 0x1) == 0)) {\n    const uint16_t *k = key;                           /* read 16-bit chunks */\n    const uint8_t  *k8;\n\n    /*--------------- all but last block: aligned reads and different mixing */\n    while (length > 12)\n    {\n      a += k[0] + (((uint32_t)k[1])<<16);\n      b += k[2] + (((uint32_t)k[3])<<16);\n      c += k[4] + (((uint32_t)k[5])<<16);\n      mix(a,b,c);\n      length -= 12;\n      k += 6;\n    }\n\n    /*----------------------------- handle the last (probably partial) block */\n    k8 = (const uint8_t *)k;\n    switch(length)\n    {\n    case 12: c+=k[4]+(((uint32_t)k[5])<<16);\n             b+=k[2]+(((uint32_t)k[3])<<16);\n             a+=k[0]+(((uint32_t)k[1])<<16);\n             break;\n    case 11: c+=((uint32_t)k8[10])<<16;     /* @fallthrough */\n    case 10: c+=k[4];                       /* @fallthrough@ */\n             b+=k[2]+(((uint32_t)k[3])<<16);\n             a+=k[0]+(((uint32_t)k[1])<<16);\n             break;\n    case 9 : c+=k8[8];                      /* @fallthrough */\n    case 8 : b+=k[2]+(((uint32_t)k[3])<<16);\n             a+=k[0]+(((uint32_t)k[1])<<16);\n             break;\n    case 7 : b+=((uint32_t)k8[6])<<16;      /* @fallthrough */\n    case 6 : b+=k[2];\n             a+=k[0]+(((uint32_t)k[1])<<16);\n             break;\n    case 5 : b+=k8[4];                      /* @fallthrough */\n    case 4 : a+=k[0]+(((uint32_t)k[1])<<16);\n             break;\n    case 3 : a+=((uint32_t)k8[2])<<16;      /* @fallthrough */\n    case 2 : a+=k[0];\n             break;\n    case 1 : a+=k8[0];\n             break;\n    case 0 : return c;  /* zero length strings require no mixing */\n    }\n\n  } else {                        /* need to read the key one byte at a time */\n    const uint8_t *k = key;\n\n    /*--------------- all but the last block: affect some 32 bits of (a,b,c) */\n    while (length > 12)\n    {\n      a += k[0];\n      a += ((uint32_t)k[1])<<8;\n      a += ((uint32_t)k[2])<<16;\n      a += ((uint32_t)k[3])<<24;\n      b += k[4];\n      b += ((uint32_t)k[5])<<8;\n      b += ((uint32_t)k[6])<<16;\n      b += ((uint32_t)k[7])<<24;\n      c += k[8];\n      c += ((uint32_t)k[9])<<8;\n      c += ((uint32_t)k[10])<<16;\n      c += ((uint32_t)k[11])<<24;\n      mix(a,b,c);\n      length -= 12;\n      k += 12;\n    }\n\n    /*-------------------------------- last block: affect all 32 bits of (c) */\n    switch(length)                   /* all the case statements fall through */\n    {\n    case 12: c+=((uint32_t)k[11])<<24;\n    case 11: c+=((uint32_t)k[10])<<16;\n    case 10: c+=((uint32_t)k[9])<<8;\n    case 9 : c+=k[8];\n    case 8 : b+=((uint32_t)k[7])<<24;\n    case 7 : b+=((uint32_t)k[6])<<16;\n    case 6 : b+=((uint32_t)k[5])<<8;\n    case 5 : b+=k[4];\n    case 4 : a+=((uint32_t)k[3])<<24;\n    case 3 : a+=((uint32_t)k[2])<<16;\n    case 2 : a+=((uint32_t)k[1])<<8;\n    case 1 : a+=k[0];\n             break;\n    case 0 : return c;  /* zero length strings require no mixing */\n    }\n  }\n\n  final(a,b,c);\n  return c;             /* zero length strings require no mixing */\n}\n\n#elif HASH_BIG_ENDIAN == 1\n/*\n * hashbig():\n * This is the same as hashword() on big-endian machines.  It is different\n * from hashlittle() on all machines.  hashbig() takes advantage of\n * big-endian byte ordering.\n */\nuint32_t jenkins_hash( const void *key, size_t length)\n{\n  uint32_t a,b,c;\n  union { const void *ptr; size_t i; } u; /* to cast key to (size_t) happily */\n\n  /* Set up the internal state */\n  a = b = c = 0xdeadbeef + ((uint32_t)length) + 0;\n\n  u.ptr = key;\n  if (HASH_BIG_ENDIAN && ((u.i & 0x3) == 0)) {\n    const uint32_t *k = key;                           /* read 32-bit chunks */\n#ifdef VALGRIND\n    const uint8_t  *k8;\n#endif /* ifdef VALGRIND */\n\n    /*------ all but last block: aligned reads and affect 32 bits of (a,b,c) */\n    while (length > 12)\n    {\n      a += k[0];\n      b += k[1];\n      c += k[2];\n      mix(a,b,c);\n      length -= 12;\n      k += 3;\n    }\n\n    /*----------------------------- handle the last (probably partial) block */\n    /*\n     * \"k[2]<<8\" actually reads beyond the end of the string, but\n     * then shifts out the part it's not allowed to read.  Because the\n     * string is aligned, the illegal read is in the same word as the\n     * rest of the string.  Every machine with memory protection I've seen\n     * does it on word boundaries, so is OK with this.  But VALGRIND will\n     * still catch it and complain.  The masking trick does make the hash\n     * noticeably faster for short strings (like English words).\n     */\n#ifndef VALGRIND\n\n    switch(length)\n    {\n    case 12: c+=k[2]; b+=k[1]; a+=k[0]; break;\n    case 11: c+=k[2]&0xffffff00; b+=k[1]; a+=k[0]; break;\n    case 10: c+=k[2]&0xffff0000; b+=k[1]; a+=k[0]; break;\n    case 9 : c+=k[2]&0xff000000; b+=k[1]; a+=k[0]; break;\n    case 8 : b+=k[1]; a+=k[0]; break;\n    case 7 : b+=k[1]&0xffffff00; a+=k[0]; break;\n    case 6 : b+=k[1]&0xffff0000; a+=k[0]; break;\n    case 5 : b+=k[1]&0xff000000; a+=k[0]; break;\n    case 4 : a+=k[0]; break;\n    case 3 : a+=k[0]&0xffffff00; break;\n    case 2 : a+=k[0]&0xffff0000; break;\n    case 1 : a+=k[0]&0xff000000; break;\n    case 0 : return c;              /* zero length strings require no mixing */\n    }\n\n#else  /* make valgrind happy */\n\n    k8 = (const uint8_t *)k;\n    switch(length)                   /* all the case statements fall through */\n    {\n    case 12: c+=k[2]; b+=k[1]; a+=k[0]; break;\n    case 11: c+=((uint32_t)k8[10])<<8;  /* fall through */\n    case 10: c+=((uint32_t)k8[9])<<16;  /* fall through */\n    case 9 : c+=((uint32_t)k8[8])<<24;  /* fall through */\n    case 8 : b+=k[1]; a+=k[0]; break;\n    case 7 : b+=((uint32_t)k8[6])<<8;   /* fall through */\n    case 6 : b+=((uint32_t)k8[5])<<16;  /* fall through */\n    case 5 : b+=((uint32_t)k8[4])<<24;  /* fall through */\n    case 4 : a+=k[0]; break;\n    case 3 : a+=((uint32_t)k8[2])<<8;   /* fall through */\n    case 2 : a+=((uint32_t)k8[1])<<16;  /* fall through */\n    case 1 : a+=((uint32_t)k8[0])<<24; break;\n    case 0 : return c;\n    }\n\n#endif /* !VALGRIND */\n\n  } else {                        /* need to read the key one byte at a time */\n    const uint8_t *k = key;\n\n    /*--------------- all but the last block: affect some 32 bits of (a,b,c) */\n    while (length > 12)\n    {\n      a += ((uint32_t)k[0])<<24;\n      a += ((uint32_t)k[1])<<16;\n      a += ((uint32_t)k[2])<<8;\n      a += ((uint32_t)k[3]);\n      b += ((uint32_t)k[4])<<24;\n      b += ((uint32_t)k[5])<<16;\n      b += ((uint32_t)k[6])<<8;\n      b += ((uint32_t)k[7]);\n      c += ((uint32_t)k[8])<<24;\n      c += ((uint32_t)k[9])<<16;\n      c += ((uint32_t)k[10])<<8;\n      c += ((uint32_t)k[11]);\n      mix(a,b,c);\n      length -= 12;\n      k += 12;\n    }\n\n    /*-------------------------------- last block: affect all 32 bits of (c) */\n    switch(length)                   /* all the case statements fall through */\n    {\n    case 12: c+=k[11];\n    case 11: c+=((uint32_t)k[10])<<8;\n    case 10: c+=((uint32_t)k[9])<<16;\n    case 9 : c+=((uint32_t)k[8])<<24;\n    case 8 : b+=k[7];\n    case 7 : b+=((uint32_t)k[6])<<8;\n    case 6 : b+=((uint32_t)k[5])<<16;\n    case 5 : b+=((uint32_t)k[4])<<24;\n    case 4 : a+=k[3];\n    case 3 : a+=((uint32_t)k[2])<<8;\n    case 2 : a+=((uint32_t)k[1])<<16;\n    case 1 : a+=((uint32_t)k[0])<<24;\n             break;\n    case 0 : return c;\n    }\n  }\n\n  final(a,b,c);\n  return c;\n}\n#else /* HASH_XXX_ENDIAN == 1 */\n#error Must define HASH_BIG_ENDIAN or HASH_LITTLE_ENDIAN\n#endif /* HASH_XXX_ENDIAN == 1 */\n"
        },
        {
          "name": "jenkins_hash.h",
          "type": "blob",
          "size": 0.2080078125,
          "content": "#ifndef JENKINS_HASH_H\n#define    JENKINS_HASH_H\n\n#ifdef    __cplusplus\nextern \"C\" {\n#endif\n\nuint32_t jenkins_hash(const void *key, size_t length);\n\n#ifdef    __cplusplus\n}\n#endif\n\n#endif    /* JENKINS_HASH_H */\n\n"
        },
        {
          "name": "linux_priv.c",
          "type": "blob",
          "size": 9.1591796875,
          "content": "#include \"config.h\"\n#include <seccomp.h>\n#include <termios.h>\n#include <errno.h>\n#include <stdlib.h>\n#include <sys/ioctl.h>\n#include <signal.h>\n#include <string.h>\n#include \"memcached.h\"\n\nstatic char *kill_msg;\n// Make sure to preserve the ??? position in the string, or correct the offsets\n// in the syssig handler.\n#define KILL_MSG_STR \"Seccomp policy failure. Caught syscall ???. This is \" \\\n    \"either an exploit attempt, or your system is not supported yet.\\n\"\n\nstatic void handle_syssig(int signum, siginfo_t *si, void *thread_context) {\n#if defined(si_syscall)\n    int syscall_no = si->si_syscall;\n#else\n    // If system has no support for si_syscal, the information may not be\n    // precise.\n    int syscall_no = si->si_value.sival_int;\n#endif\n\n    // Replace the characters in the kill message with the syscall number. We\n    // can't safely printf (even write is not really valid, but we're crashing\n    // anyway).\n\n    kill_msg[39] = (syscall_no / 100) % 10 + '0';\n    kill_msg[40] = (syscall_no / 10) % 10 + '0';\n    kill_msg[41] = syscall_no % 10 + '0';\n    if (write(2, kill_msg, strlen(kill_msg)) == -1) {\n        // An error occurred, but we can't do anything about it here. This check\n        // is mostly to avoid the \"ignoring return value of 'write'\" error on\n        // distributions with broken gcc (no \"ignore via cast to void\" support).\n    }\n\n    // We can't use the nice exit() version because it causes at_exit handlers\n    // to be looked up and run. We can't take any locks while handling the\n    // signal, so _exit() is the only thing to do safely.\n    _exit(EXIT_FAILURE);\n}\n\nstatic const struct sigaction act = {\n    .sa_sigaction = handle_syssig,\n    .sa_flags = SA_SIGINFO,\n};\n\nvoid setup_privilege_violations_handler(void) {\n    kill_msg = malloc(strlen(KILL_MSG_STR)+1);\n    strncpy(kill_msg, KILL_MSG_STR, strlen(KILL_MSG_STR)+1);\n\n    sigaction(SIGSYS, &act, NULL);\n}\n\n// If anything crosses the policy, kill the process.\n#define DENY_ACTION SCMP_ACT_TRAP\n\nvoid drop_privileges(void) {\n    scmp_filter_ctx ctx = seccomp_init(DENY_ACTION);\n    if (ctx == NULL) {\n        return;\n    }\n\n    int rc = 0;\n    rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(sigreturn), 0);\n    rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(rt_sigreturn), 0);\n    rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(rt_sigprocmask), 0);\n    rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(rt_sigaction), 0);\n    rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(futex), 0);\n    rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(epoll_wait), 0);\n    rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(epoll_pwait), 0);\n    rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(epoll_ctl), 0);\n    rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(accept4), 0);\n    rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(accept), 0);\n    rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(write), 0);\n    rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(writev), 0);\n    rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(fstat), 0);\n    rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(close), 0);\n    rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(mmap), 0);\n    rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(munmap), 0);\n    rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(shmctl), 0);\n    rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(exit), 0);\n    rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(exit_group), 0);\n    rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(brk), 0);\n    rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(ioctl), 1, SCMP_A1(SCMP_CMP_EQ, TIOCGWINSZ));\n    rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(ioctl), 1, SCMP_A1(SCMP_CMP_EQ, TCGETS));\n    rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(msync), 0);\n    rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(umask), 0);\n\n#if defined(HAVE_CLOCK_GETTIME) && defined(CLOCK_MONOTONIC)\n    rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(clock_gettime), 0);\n#endif\n    rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(gettimeofday), 0);\n\n#ifdef MEMCACHED_DEBUG\n    rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(open), 0);\n    rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(fcntl), 0);\n    rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(read), 0);\n    rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(readv), 0);\n    rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(lseek), 0);\n    rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(getpid), 0);\n\n    if (settings.relaxed_privileges) {\n        rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(openat), 0);\n        rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(mkdir), 0);\n        rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(access), 0);\n    }\n#endif\n\n    if (rc != 0) {\n        goto fail;\n    }\n\n    rc = seccomp_load(ctx);\n    if (rc < 0) {\n        goto fail;\n    }\n\n    seccomp_release(ctx);\n    return;\n\nfail:\n    seccomp_release(ctx);\n    fprintf(stderr, \"Failed to set a seccomp profile on the main thread\\n\");\n    exit(EXIT_FAILURE);\n}\n\nvoid drop_worker_privileges(void) {\n    scmp_filter_ctx ctx = seccomp_init(DENY_ACTION);\n    if (ctx == NULL) {\n        return;\n    }\n\n    int rc = 0;\n    rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(sigreturn), 0);\n    rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(rt_sigreturn), 0);\n    rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(rt_sigprocmask), 0);\n    rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(rt_sigaction), 0);\n    rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(futex), 0);\n    rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(epoll_wait), 0);\n    rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(epoll_pwait), 0);\n    rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(epoll_ctl), 0);\n    rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(poll), 0);\n    rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(read), 0);\n    rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(readv), 0);\n    rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(mprotect), 0);\n    rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(getpeername), 0);\n    rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(close), 0);\n    rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(sendmsg), 0);\n    rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(getrusage), 0);\n    rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(mmap), 0);\n    rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(mremap), 0);\n    rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(munmap), 0);\n    rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(recvfrom), 0);\n    rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(brk), 0);\n    rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(ioctl), 1, SCMP_A1(SCMP_CMP_EQ, TIOCGWINSZ));\n    rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(msync), 0);\n\n    // for spawning the LRU crawler\n    rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(clone), 0);\n    rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(set_robust_list), 0);\n    rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(madvise), 0);\n    rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(exit), 0);\n\n    // stat\n    rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(getsockname), 0);\n    rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(getpid), 0);\n\n    if (settings.shutdown_command) {\n        rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(tgkill), 0);\n        rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(tkill), 0);\n        rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(exit_group), 0);\n        rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(fstat), 0);\n        rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(getpid), 0);\n        rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(gettid), 0);\n    }\n\n    if (settings.relaxed_privileges) {\n        rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(openat), 0);\n        rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(mkdir), 0);\n        rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(access), 0);\n        rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(open), 0);\n        rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(fcntl), 0);\n        rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(lseek), 0);\n        rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(write), 0);\n        rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(writev), 0);\n        rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(socket), 0);\n    } else {\n        // stdout\n        rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(write), 1, SCMP_A0(SCMP_CMP_EQ, 1));\n        rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(writev), 1, SCMP_A0(SCMP_CMP_EQ, 1));\n        // stderr\n        rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(write), 1, SCMP_A0(SCMP_CMP_EQ, 2));\n        rc |= seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(writev), 1, SCMP_A0(SCMP_CMP_EQ, 2));\n    }\n\n    if (rc != 0) {\n        goto fail;\n    }\n\n    rc = seccomp_load(ctx);\n    if (rc < 0) {\n        goto fail;\n    }\n\n    seccomp_release(ctx);\n    return;\n\nfail:\n    seccomp_release(ctx);\n    fprintf(stderr, \"Failed to set a seccomp profile on a worker thread\\n\");\n    exit(EXIT_FAILURE);\n}\n"
        },
        {
          "name": "logger.c",
          "type": "blob",
          "size": 38.28515625,
          "content": "/* -*- Mode: C; tab-width: 4; c-basic-offset: 4; indent-tabs-mode: nil -*- */\n\n#include <arpa/inet.h>\n#include <stdlib.h>\n#include <stdio.h>\n#include <string.h>\n#include <errno.h>\n#include <poll.h>\n#include <ctype.h>\n#include <stdarg.h>\n\n#if defined(__sun)\n#include <atomic.h>\n#endif\n\n#include \"memcached.h\"\n#include \"bipbuffer.h\"\n\n#ifdef LOGGER_DEBUG\n#define L_DEBUG(...) \\\n    do { \\\n        fprintf(stderr, __VA_ARGS__); \\\n    } while (0)\n#else\n#define L_DEBUG(...)\n#endif\n\n\n/* TODO: put this in a struct and ditch the global vars. */\nstatic logger *logger_stack_head = NULL;\nstatic logger *logger_stack_tail = NULL;\nstatic unsigned int logger_count = 0;\nstatic volatile int do_run_logger_thread = 1;\nstatic pthread_t logger_tid;\npthread_mutex_t logger_stack_lock = PTHREAD_MUTEX_INITIALIZER;\npthread_cond_t logger_stack_cond = PTHREAD_COND_INITIALIZER;\n\npthread_key_t logger_key;\n\n#if !defined(HAVE_GCC_64ATOMICS) && !defined(__sun)\npthread_mutex_t logger_atomics_mutex = PTHREAD_MUTEX_INITIALIZER;\n#endif\n\n#define WATCHER_LIMIT 20\nlogger_watcher *watchers[20];\nstruct pollfd watchers_pollfds[20];\nint watcher_count = 0;\n\n#define WATCHER_ALL -1\nstatic int logger_thread_poll_watchers(int force_poll, int watcher);\n\n/* helpers for logger_log */\n\nstatic void _logger_log_text(logentry *e, const entry_details *d, const void *entry, va_list ap) {\n    int reqlen = d->reqlen;\n    int total = vsnprintf((char *) e->data, reqlen, d->format, ap);\n    if (total <= 0) {\n        fprintf(stderr, \"LOGGER: Failed to vsnprintf a text entry: (total) %d\\n\", total);\n    }\n    e->size = total + 1; // null byte\n}\n\nstatic void _logger_log_evictions(logentry *e, const entry_details *d, const void *entry, va_list ap) {\n    item *it = (item *)entry;\n    struct logentry_eviction *le = (struct logentry_eviction *) e->data;\n\n    le->exptime = (it->exptime > 0) ? (long long int)(it->exptime - current_time) : (long long int) -1;\n    le->latime = current_time - it->time;\n    le->it_flags = it->it_flags;\n    le->nkey = it->nkey;\n    le->nbytes = it->nbytes;\n    le->clsid = ITEM_clsid(it);\n    memcpy(le->key, ITEM_key(it), it->nkey);\n    e->size = sizeof(struct logentry_eviction) + le->nkey;\n}\n#ifdef EXTSTORE\nstatic void _logger_log_ext_write(logentry *e, const entry_details *d, const void *entry, va_list ap) {\n    item *it = (item *)entry;\n    int ew_bucket = va_arg(ap, int);\n\n    struct logentry_ext_write *le = (struct logentry_ext_write *) e->data;\n    le->exptime = (it->exptime > 0) ? (long long int)(it->exptime - current_time) : (long long int) -1;\n    le->latime = current_time - it->time;\n    le->it_flags = it->it_flags;\n    le->nkey = it->nkey;\n    le->clsid = ITEM_clsid(it);\n    le->bucket = (uint8_t)ew_bucket;\n    memcpy(le->key, ITEM_key(it), it->nkey);\n    e->size = sizeof(struct logentry_ext_write) + le->nkey;\n}\n#endif\n// 0 == nf, 1 == found. 2 == flushed. 3 == expired.\n// might be useful to store/print the flags an item has?\n// could also collapse this and above code into an \"item status\" struct. wait\n// for more endpoints to be written before making it generic, though.\nstatic void _logger_log_item_get(logentry *e, const entry_details *d, const void *entry, va_list ap) {\n    int was_found = va_arg(ap, int);\n    char *key = va_arg(ap, char *);\n    int nkey = va_arg(ap, int);\n    int nbytes = va_arg(ap, int);\n    uint8_t clsid = va_arg(ap, int);\n    int sfd = va_arg(ap, int);\n\n    struct logentry_item_get *le = (struct logentry_item_get *) e->data;\n    le->was_found = was_found;\n    le->nkey = nkey;\n    le->nbytes = nbytes;\n    le->clsid = clsid;\n    memcpy(le->key, key, nkey);\n    le->sfd = sfd;\n    e->size = sizeof(struct logentry_item_get) + nkey;\n}\n\nstatic void _logger_log_item_store(logentry *e, const entry_details *d, const void *entry, va_list ap) {\n    enum store_item_type status = va_arg(ap, enum store_item_type);\n    int comm = va_arg(ap, int);\n    char *key = va_arg(ap, char *);\n    int nkey = va_arg(ap, int);\n    int nbytes = va_arg(ap, int);\n    rel_time_t ttl = va_arg(ap, rel_time_t);\n    uint8_t clsid = va_arg(ap, int);\n    int sfd = va_arg(ap, int);\n\n    struct logentry_item_store *le = (struct logentry_item_store *) e->data;\n    le->status = status;\n    le->cmd = comm;\n    le->nkey = nkey;\n    le->nbytes = nbytes;\n    le->clsid = clsid;\n    if (ttl != 0) {\n        le->ttl = ttl - current_time;\n    } else {\n        le->ttl = 0;\n    }\n    memcpy(le->key, key, nkey);\n    le->sfd = sfd;\n    e->size = sizeof(struct logentry_item_store) + nkey;\n}\n\nstatic void _logger_log_item_deleted(logentry *e, const entry_details *d, const void *entry, va_list ap) {\n    item *it = (item *)entry;\n    int comm = va_arg(ap, int);\n    struct logentry_deletion *le = (struct logentry_deletion *) e->data;\n    le->nkey = it->nkey;\n    le->cmd = comm;\n    le->nbytes = it->nbytes;\n    le->clsid = ITEM_clsid(it);\n    memcpy(le->key, ITEM_key(it), it->nkey);\n    e->size = sizeof(struct logentry_deletion) + le->nkey;\n}\n\nstatic void _logger_log_conn_event(logentry *e, const entry_details *d, const void *entry, va_list ap) {\n    struct sockaddr_in6 *addr = va_arg(ap, struct sockaddr_in6 *);\n    socklen_t addrlen = va_arg(ap, socklen_t);\n    enum network_transport transport = va_arg(ap, enum network_transport);\n    enum close_reasons reason = va_arg(ap, enum close_reasons);\n    int sfd = va_arg(ap, int);\n\n    struct logentry_conn_event *le = (struct logentry_conn_event *) e->data;\n\n    memcpy(&le->addr, addr, addrlen);\n    le->sfd = sfd;\n    le->transport = transport;\n    le->reason = reason;\n    e->size = sizeof(struct logentry_conn_event);\n}\n\n/*************************\n * Util functions used by the logger background thread\n *************************/\n\nstatic int _logger_util_addr_endpoint(struct sockaddr_in6 *addr, char *rip,\n        size_t riplen, unsigned short *rport) {\n    memset(rip, 0, riplen);\n    *rport = 0;\n\n    switch (addr->sin6_family) {\n        case AF_INET:\n            inet_ntop(AF_INET, &((struct sockaddr_in *) addr)->sin_addr,\n                    rip, riplen - 1);\n            *rport = ntohs(((struct sockaddr_in *) addr)->sin_port);\n            break;\n        case AF_INET6:\n            inet_ntop(AF_INET6, &((struct sockaddr_in6 *) addr)->sin6_addr,\n                    rip, riplen - 1);\n            *rport = ntohs(((struct sockaddr_in6 *) addr)->sin6_port);\n            break;\n#ifndef DISABLE_UNIX_SOCKET\n        // Connections on Unix socket transports have c->request_addr zeroed out.\n        case AF_UNSPEC:\n        case AF_UNIX:\n            strncpy(rip, \"unix\", strlen(\"unix\") + 1);\n            break;\n#endif // #ifndef DISABLE_UNIX_SOCKET\n    }\n\n    return 0;\n}\n\n/*************************\n * Logger background thread functions. Aggregates per-worker buffers and\n * writes to any watchers.\n *************************/\n\n#define LOGGER_PARSE_SCRATCH 4096\n\nstatic int _logger_parse_text(logentry *e, char *scratch) {\n    return snprintf(scratch, LOGGER_PARSE_SCRATCH, \"ts=%lld.%d gid=%llu %s\\n\",\n            (long long int)e->tv.tv_sec, (int)e->tv.tv_usec,\n            (unsigned long long) e->gid, (char *) e->data);\n}\n\nstatic int _logger_parse_ise(logentry *e, char *scratch) {\n    int total;\n    const char *cmd = \"na\";\n    char keybuf[KEY_MAX_URI_ENCODED_LENGTH];\n    struct logentry_item_store *le = (struct logentry_item_store *) e->data;\n    const char * const status_map[] = {\n        \"not_stored\", \"stored\", \"exists\", \"not_found\", \"too_large\", \"no_memory\" };\n    const char * const cmd_map[] = {\n        \"null\", \"add\", \"set\", \"replace\", \"append\", \"prepend\", \"cas\", \"append\", \"prepend\" };\n\n    if (le->cmd <= 8)\n        cmd = cmd_map[le->cmd];\n\n    uriencode(le->key, keybuf, le->nkey, KEY_MAX_URI_ENCODED_LENGTH);\n    total = snprintf(scratch, LOGGER_PARSE_SCRATCH,\n            \"ts=%lld.%d gid=%llu type=item_store key=%s status=%s cmd=%s ttl=%u clsid=%u cfd=%d size=%d\\n\",\n            (long long int)e->tv.tv_sec, (int)e->tv.tv_usec, (unsigned long long) e->gid,\n            keybuf, status_map[le->status], cmd, le->ttl, le->clsid, le->sfd,\n            le->nbytes > 0 ? le->nbytes - 2 : 0); // CLRF\n    return total;\n}\n\nstatic int _logger_parse_ige(logentry *e, char *scratch) {\n    int total;\n    struct logentry_item_get *le = (struct logentry_item_get *) e->data;\n    char keybuf[KEY_MAX_URI_ENCODED_LENGTH];\n    const char * const was_found_map[] = {\n        \"not_found\", \"found\", \"flushed\", \"expired\" };\n\n    uriencode(le->key, keybuf, le->nkey, KEY_MAX_URI_ENCODED_LENGTH);\n    total = snprintf(scratch, LOGGER_PARSE_SCRATCH,\n            \"ts=%lld.%d gid=%llu type=item_get key=%s status=%s clsid=%u cfd=%d size=%d\\n\",\n            (long long int)e->tv.tv_sec, (int)e->tv.tv_usec, (unsigned long long) e->gid,\n            keybuf, was_found_map[le->was_found], le->clsid, le->sfd,\n            le->nbytes > 0 ? le->nbytes - 2 : 0); // CLRF\n    return total;\n}\n\nstatic int _logger_parse_ee(logentry *e, char *scratch) {\n    int total;\n    char keybuf[KEY_MAX_URI_ENCODED_LENGTH];\n    struct logentry_eviction *le = (struct logentry_eviction *) e->data;\n    uriencode(le->key, keybuf, le->nkey, KEY_MAX_URI_ENCODED_LENGTH);\n    total = snprintf(scratch, LOGGER_PARSE_SCRATCH,\n            \"ts=%lld.%d gid=%llu type=eviction key=%s fetch=%s ttl=%lld la=%d clsid=%u size=%d\\n\",\n            (long long int)e->tv.tv_sec, (int)e->tv.tv_usec, (unsigned long long) e->gid,\n            keybuf, (le->it_flags & ITEM_FETCHED) ? \"yes\" : \"no\",\n            (long long int)le->exptime, le->latime, le->clsid,\n            le->nbytes > 0 ? le->nbytes - 2 : 0); // CLRF\n\n    return total;\n}\n\nstatic int _logger_parse_ide(logentry *e, char *scratch) {\n    int total;\n    const char *cmd = \"na\";\n    const char * const cmd_map[] = {\n            \"null\", \"delete\", \"md\" };\n    char keybuf[KEY_MAX_URI_ENCODED_LENGTH];\n    struct logentry_deletion *le = (struct logentry_deletion *) e->data;\n    uriencode(le->key, keybuf, le->nkey, KEY_MAX_URI_ENCODED_LENGTH);\n\n    if (le->cmd <= 2)\n        cmd = cmd_map[le->cmd];\n\n    total = snprintf(scratch, LOGGER_PARSE_SCRATCH,\n                     \"ts=%d.%d gid=%llu type=deleted key=%s cmd=%s clsid=%u size=%d\\n\",\n                     (int)e->tv.tv_sec, (int)e->tv.tv_usec, (unsigned long long) e->gid,\n                     keybuf, cmd, le->clsid,\n                     le->nbytes > 0 ? le->nbytes - 2 : 0); // CLRF\n    return total;\n}\n\n#ifdef EXTSTORE\nstatic int _logger_parse_extw(logentry *e, char *scratch) {\n    int total;\n    char keybuf[KEY_MAX_URI_ENCODED_LENGTH];\n    struct logentry_ext_write *le = (struct logentry_ext_write *) e->data;\n    uriencode(le->key, keybuf, le->nkey, KEY_MAX_URI_ENCODED_LENGTH);\n    total = snprintf(scratch, LOGGER_PARSE_SCRATCH,\n            \"ts=%lld.%d gid=%llu type=extwrite key=%s fetch=%s ttl=%lld la=%d clsid=%u bucket=%u\\n\",\n            (long long int)e->tv.tv_sec, (int)e->tv.tv_usec, (unsigned long long) e->gid,\n            keybuf, (le->it_flags & ITEM_FETCHED) ? \"yes\" : \"no\",\n            (long long int)le->exptime, le->latime, le->clsid, le->bucket);\n\n    return total;\n}\n#endif\n\nstatic int _logger_parse_cne(logentry *e, char *scratch) {\n    int total;\n    unsigned short rport = 0;\n    char rip[64];\n    struct logentry_conn_event *le = (struct logentry_conn_event *) e->data;\n    const char * const transport_map[] = { \"local\", \"tcp\", \"udp\" };\n\n    _logger_util_addr_endpoint(&le->addr, rip, sizeof(rip), &rport);\n\n    total = snprintf(scratch, LOGGER_PARSE_SCRATCH,\n            \"ts=%lld.%d gid=%llu type=conn_new rip=%s rport=%hu transport=%s cfd=%d\\n\",\n            (long long int) e->tv.tv_sec, (int) e->tv.tv_usec, (unsigned long long) e->gid,\n            rip, rport, transport_map[le->transport], le->sfd);\n\n    return total;\n}\n\nstatic int _logger_parse_cce(logentry *e, char *scratch) {\n    int total;\n    unsigned short rport = 0;\n    char rip[64];\n    struct logentry_conn_event *le = (struct logentry_conn_event *) e->data;\n    const char * const transport_map[] = { \"local\", \"tcp\", \"udp\" };\n    const char * const reason_map[] = { \"error\", \"normal\", \"idle_timeout\", \"shutdown\" };\n\n    _logger_util_addr_endpoint(&le->addr, rip, sizeof(rip), &rport);\n\n    total = snprintf(scratch, LOGGER_PARSE_SCRATCH,\n            \"ts=%lld.%d gid=%llu type=conn_close rip=%s rport=%hu transport=%s reason=%s cfd=%d\\n\",\n            (long long int) e->tv.tv_sec, (int) e->tv.tv_usec, (unsigned long long) e->gid,\n            rip, rport, transport_map[le->transport],\n            reason_map[le->reason], le->sfd);\n\n    return total;\n}\n\n#ifdef PROXY\n// TODO (v2): the length caps here are all magic numbers. Haven't thought of\n// something yet that I like better.\n// Should at least make a define to the max log len (1024) and do some math\n// here.\nstatic void _logger_log_proxy_req(logentry *e, const entry_details *d, const void *entry, va_list ap) {\n    char *req = va_arg(ap, char *);\n    int reqlen = va_arg(ap, uint32_t);\n    long elapsed = va_arg(ap, long);\n    unsigned short type = va_arg(ap, int);\n    unsigned short code = va_arg(ap, int);\n    int status = va_arg(ap, int);\n    int conn_fd = va_arg(ap, int);\n    char *detail = va_arg(ap, char *);\n    int dlen = va_arg(ap, int);\n    char *be_name = va_arg(ap, char *);\n    char *be_port = va_arg(ap, char *);\n\n    struct logentry_proxy_req *le = (void *)e->data;\n    le->type = type;\n    le->code = code;\n    le->status = status;\n    le->conn_fd = conn_fd;\n    le->dlen = dlen;\n    le->elapsed = elapsed;\n    if (be_name && be_port) {\n        le->be_namelen = strlen(be_name);\n        le->be_portlen = strlen(be_port);\n    } else {\n        le->be_namelen = 0;\n        le->be_portlen = 0;\n    }\n    char *data = le->data;\n    if (req[reqlen-2] == '\\r') {\n        reqlen -= 2;\n    } else {\n        reqlen--;\n    }\n    if (reqlen > 300) {\n        reqlen = 300;\n    }\n    if (dlen > 150) {\n        dlen = 150;\n    }\n    // be_namelen and be_portlen can't be longer than 255+6\n    le->reqlen = reqlen;\n    memcpy(data, req, reqlen);\n    data += reqlen;\n    memcpy(data, detail, dlen);\n    data += dlen;\n    memcpy(data, be_name, le->be_namelen);\n    data += le->be_namelen;\n    memcpy(data, be_port, le->be_portlen);\n    e->size = sizeof(struct logentry_proxy_req) + reqlen + dlen + le->be_namelen + le->be_portlen;\n}\n\nstatic int _logger_parse_prx_req(logentry *e, char *scratch) {\n    int total;\n    struct logentry_proxy_req *le = (void *)e->data;\n\n    total = snprintf(scratch, LOGGER_PARSE_SCRATCH,\n            \"ts=%lld.%d gid=%llu type=proxy_req elapsed=%lu type=%d code=%d status=%d cfd=%d be=%.*s:%.*s detail=%.*s req=%.*s\\n\",\n            (long long int) e->tv.tv_sec, (int) e->tv.tv_usec, (unsigned long long) e->gid,\n            le->elapsed, le->type, le->code, le->status, le->conn_fd,\n            (int)le->be_namelen, le->data+le->reqlen+le->dlen,\n            (int)le->be_portlen, le->data+le->reqlen+le->dlen+le->be_namelen, // fml.\n            (int)le->dlen, le->data+le->reqlen, (int)le->reqlen, le->data\n            );\n    return total;\n}\n\n#define MAX_RBUF_READ 100\nstatic void _logger_log_proxy_errbe(logentry *e, const entry_details *d, const void *entry, va_list ap) {\n    char *errmsg = va_arg(ap, char *);\n    char *be_name = va_arg(ap, char *);\n    char *be_port = va_arg(ap, char *);\n    char *be_label = va_arg(ap, char *);\n    int be_depth = va_arg(ap, int);\n    char *be_rbuf = va_arg(ap, char *);\n    int be_rbuflen = va_arg(ap, int);\n    int be_retry = va_arg(ap, int);\n\n    struct logentry_proxy_errbe *le = (void *)e->data;\n    le->be_depth = be_depth;\n    le->retry = be_retry;\n    le->errlen = strlen(errmsg);\n    if (be_name && be_port) {\n        le->be_namelen = strlen(be_name);\n        le->be_portlen = strlen(be_port);\n    }\n\n    if (be_label) {\n        le->be_labellen = strlen(be_label);\n    }\n\n    le->be_rbuflen = be_rbuflen;\n    if (be_rbuflen > MAX_RBUF_READ) {\n        le->be_rbuflen = MAX_RBUF_READ;\n    }\n\n    char *data = le->data;\n    memcpy(data, errmsg, le->errlen);\n    data += le->errlen;\n    memcpy(data, be_name, le->be_namelen);\n    data += le->be_namelen;\n    memcpy(data, be_port, le->be_portlen);\n    data += le->be_portlen;\n    memcpy(data, be_label, le->be_labellen);\n    data += le->be_labellen;\n    memcpy(data, be_rbuf, le->be_rbuflen);\n    data += le->be_rbuflen;\n\n    e->size = sizeof(struct logentry_proxy_errbe) + (data - le->data);\n}\n\nstatic int _logger_parse_prx_errbe(logentry *e, char *scratch) {\n    int total;\n    char rbuf[MAX_RBUF_READ * 3]; // x 3 for worst case URI encoding.\n    struct logentry_proxy_errbe *le = (void *)e->data;\n    char *data = le->data;\n    char *errmsg = data;\n    data += le->errlen;\n    char *be_name = data;\n    data += le->be_namelen;\n    char *be_port = data;\n    data += le->be_portlen;\n    char *be_label = data;\n    data += le->be_labellen;\n    char *be_rbuf = data;\n\n    uriencode(be_rbuf, rbuf, le->be_rbuflen, MAX_RBUF_READ * 3);\n    if (le->retry) {\n        total = snprintf(scratch, LOGGER_PARSE_SCRATCH,\n                \"ts=%lld.%d gid=%llu type=proxy_backend error=%.*s name=%.*s port=%.*s label=%.*s retry=%d\\n\",\n                (long long int)e->tv.tv_sec, (int)e->tv.tv_usec, (unsigned long long) e->gid,\n                (int)le->errlen, errmsg, (int)le->be_namelen, be_name,\n                (int)le->be_portlen, be_port, (int)le->be_labellen, be_label, le->retry);\n    } else {\n        total = snprintf(scratch, LOGGER_PARSE_SCRATCH,\n                \"ts=%lld.%d gid=%llu type=proxy_backend error=%.*s name=%.*s port=%.*s label=%.*s depth=%d rbuf=%s\\n\",\n                (long long int)e->tv.tv_sec, (int)e->tv.tv_usec, (unsigned long long) e->gid,\n                (int)le->errlen, errmsg, (int)le->be_namelen, be_name,\n                (int)le->be_portlen, be_port, (int)le->be_labellen, be_label, le->be_depth, rbuf);\n    }\n\n    return total;\n}\n#endif\n\n/* Should this go somewhere else? */\nstatic const entry_details default_entries[] = {\n    [LOGGER_ASCII_CMD] = {512, LOG_RAWCMDS, _logger_log_text, _logger_parse_text, \"<%d %s\"},\n    [LOGGER_EVICTION] = {512, LOG_EVICTIONS, _logger_log_evictions, _logger_parse_ee, NULL},\n    [LOGGER_ITEM_GET] = {512, LOG_FETCHERS, _logger_log_item_get, _logger_parse_ige, NULL},\n    [LOGGER_ITEM_STORE] = {512, LOG_MUTATIONS, _logger_log_item_store, _logger_parse_ise, NULL},\n    [LOGGER_CRAWLER_STATUS] = {512, LOG_SYSEVENTS, _logger_log_text, _logger_parse_text,\n        \"type=lru_crawler crawler=%d lru=%s low_mark=%llu next_reclaims=%llu since_run=%u next_run=%d elapsed=%u examined=%llu reclaimed=%llu\"\n    },\n    [LOGGER_SLAB_MOVE] = {512, LOG_SYSEVENTS, _logger_log_text, _logger_parse_text,\n        \"type=slab_move src=%d dst=%d state=%s\"\n    },\n    [LOGGER_CONNECTION_NEW] = {512, LOG_CONNEVENTS, _logger_log_conn_event, _logger_parse_cne, NULL},\n    [LOGGER_CONNECTION_CLOSE] = {512, LOG_CONNEVENTS, _logger_log_conn_event, _logger_parse_cce, NULL},\n    [LOGGER_CONNECTION_ERROR] = {512, LOG_CONNEVENTS, _logger_log_text, _logger_parse_text,\n        \"type=connerr fd=%d msg=%s\"\n    },\n    [LOGGER_CONNECTION_TLSERROR] = {512, LOG_CONNEVENTS, _logger_log_text, _logger_parse_text,\n        \"type=conntlserr fd=%d msg=%s\"\n    },\n    [LOGGER_DELETIONS] = {512, LOG_DELETIONS, _logger_log_item_deleted, _logger_parse_ide, NULL},\n#ifdef EXTSTORE\n    [LOGGER_EXTSTORE_WRITE] = {512, LOG_EVICTIONS, _logger_log_ext_write, _logger_parse_extw, NULL},\n    [LOGGER_COMPACT_START] = {512, LOG_SYSEVENTS, _logger_log_text, _logger_parse_text,\n        \"type=compact_start id=%lu version=%llu\"\n    },\n    [LOGGER_COMPACT_ABORT] = {512, LOG_SYSEVENTS, _logger_log_text, _logger_parse_text,\n        \"type=compact_abort id=%lu\"\n    },\n    [LOGGER_COMPACT_READ_START] = {512, LOG_SYSEVENTS, _logger_log_text, _logger_parse_text,\n        \"type=compact_read_start id=%lu offset=%llu\"\n    },\n    [LOGGER_COMPACT_READ_END] = {512, LOG_SYSEVENTS, _logger_log_text, _logger_parse_text,\n        \"type=compact_read_end id=%lu offset=%llu rescues=%lu lost=%lu skipped=%lu\"\n    },\n    [LOGGER_COMPACT_END] = {512, LOG_SYSEVENTS, _logger_log_text, _logger_parse_text,\n        \"type=compact_end id=%lu\"\n    },\n    [LOGGER_COMPACT_FRAGINFO] = {512, LOG_SYSEVENTS, _logger_log_text, _logger_parse_text,\n        \"type=compact_fraginfo ratio=%.2f bytes=%lu\"\n    },\n#endif\n#ifdef PROXY\n    [LOGGER_PROXY_CONFIG] = {512, LOG_PROXYEVENTS, _logger_log_text, _logger_parse_text,\n        \"type=proxy_conf status=%s\"\n    },\n    [LOGGER_PROXY_REQ] = {1024, LOG_PROXYREQS, _logger_log_proxy_req, _logger_parse_prx_req, NULL},\n    [LOGGER_PROXY_ERROR] = {512, LOG_PROXYEVENTS, _logger_log_text, _logger_parse_text,\n        \"type=proxy_error msg=%s\"\n    },\n    [LOGGER_PROXY_USER] = {512, LOG_PROXYUSER, _logger_log_text, _logger_parse_text,\n        \"type=proxy_user msg=%s\"\n    },\n    [LOGGER_PROXY_BE_ERROR] = {512, LOG_PROXYEVENTS, _logger_log_proxy_errbe, _logger_parse_prx_errbe,\n        NULL\n    },\n\n#endif\n};\n\n/*************************\n * Util functions shared between bg thread and workers\n *************************/\n\n/* Logger GID's can be used by watchers to put logs back into strict order\n */\nstatic uint64_t logger_gid = 0;\nuint64_t logger_get_gid(void) {\n#ifdef HAVE_GCC_64ATOMICS\n    return __sync_add_and_fetch(&logger_gid, 1);\n#elif defined(__sun)\n    return atomic_inc_64_nv(&logger_gid);\n#else\n    mutex_lock(&logger_atomics_mutex);\n    uint64_t res = ++logger_gid;\n    mutex_unlock(&logger_atomics_mutex);\n    return res;\n#endif\n}\n\nvoid logger_set_gid(uint64_t gid) {\n#ifdef HAVE_GCC_64ATOMICS\n    __sync_add_and_fetch(&logger_gid, gid);\n#elif defined(__sun)\n    atomic_add_64(&logger_gid);\n#else\n    mutex_lock(&logger_atomics_mutex);\n    logger_gid = gid;\n    mutex_unlock(&logger_atomics_mutex);\n#endif\n}\n\n/* TODO: genericize lists. would be nice to import queue.h if the impact is\n * studied... otherwise can just write a local one.\n */\n/* Add to the list of threads with a logger object */\nstatic void logger_link_q(logger *l) {\n    pthread_mutex_lock(&logger_stack_lock);\n    assert(l != logger_stack_head);\n\n    l->prev = 0;\n    l->next = logger_stack_head;\n    if (l->next) l->next->prev = l;\n    logger_stack_head = l;\n    if (logger_stack_tail == 0) logger_stack_tail = l;\n    logger_count++;\n    pthread_mutex_unlock(&logger_stack_lock);\n    return;\n}\n\n/* Remove from the list of threads with a logger object */\n/*static void logger_unlink_q(logger *l) {\n    pthread_mutex_lock(&logger_stack_lock);\n    if (logger_stack_head == l) {\n        assert(l->prev == 0);\n        logger_stack_head = l->next;\n    }\n    if (logger_stack_tail == l) {\n        assert(l->next == 0);\n        logger_stack_tail = l->prev;\n    }\n    assert(l->next != l);\n    assert(l->prev != l);\n\n    if (l->next) l->next->prev = l->prev;\n    if (l->prev) l->prev->next = l->next;\n    logger_count--;\n    pthread_mutex_unlock(&logger_stack_lock);\n    return;\n}*/\n\n/* Called with logger stack locked.\n * Iterates over every watcher collecting enabled flags.\n */\nstatic void logger_set_flags(void) {\n    logger *l = NULL;\n    int x = 0;\n    uint16_t f = 0; /* logger eflags */\n\n    for (x = 0; x < WATCHER_LIMIT; x++) {\n        logger_watcher *w = watchers[x];\n        if (w == NULL)\n            continue;\n\n        f |= w->eflags;\n    }\n    for (l = logger_stack_head; l != NULL; l=l->next) {\n        pthread_mutex_lock(&l->mutex);\n        l->eflags = f;\n        pthread_mutex_unlock(&l->mutex);\n    }\n    return;\n}\n\n/* Completes rendering of log line. */\nstatic enum logger_parse_entry_ret logger_thread_parse_entry(logentry *e, struct logger_stats *ls,\n        char *scratch, int *scratch_len) {\n    int total = 0;\n    const entry_details *d = &default_entries[e->event];\n    assert(d->parse_cb != NULL);\n    total = d->parse_cb(e, scratch);\n\n    if (total >= LOGGER_PARSE_SCRATCH || total <= 0) {\n        L_DEBUG(\"LOGGER: Failed to flatten log entry!\\n\");\n        return LOGGER_PARSE_ENTRY_FAILED;\n    } else {\n        *scratch_len = total;\n    }\n\n    return LOGGER_PARSE_ENTRY_OK;\n}\n\n/* Writes flattened entry to available watchers */\nstatic void logger_thread_write_entry(logentry *e, struct logger_stats *ls,\n        char *scratch, int scratch_len) {\n    int x, total;\n    /* Write the line into available watchers with matching flags */\n    for (x = 0; x < WATCHER_LIMIT; x++) {\n        logger_watcher *w = watchers[x];\n        char *skip_scr = NULL;\n        if (w == NULL || (e->eflags & w->eflags) == 0 || (e->gid < w->min_gid))\n            continue;\n\n         /* Avoid poll()'ing constantly when buffer is full by resetting a\n         * flag periodically.\n         */\n        while (!w->failed_flush &&\n                (skip_scr = (char *) bipbuf_request(w->buf, scratch_len + 128)) == NULL) {\n            if (logger_thread_poll_watchers(0, x) <= 0) {\n                L_DEBUG(\"LOGGER: Watcher had no free space for line of size (%d)\\n\", scratch_len + 128);\n                w->failed_flush = true;\n            }\n        }\n\n        if (w->failed_flush) {\n            L_DEBUG(\"LOGGER: Fast skipped for watcher [%d] due to failed_flush\\n\", w->sfd);\n            w->skipped++;\n            ls->watcher_skipped++;\n            continue;\n        }\n\n        if (w->skipped > 0) {\n            total = snprintf(skip_scr, 128, \"skipped=%llu\\n\", (unsigned long long) w->skipped);\n            if (total >= 128 || total <= 0) {\n                L_DEBUG(\"LOGGER: Failed to flatten skipped message into watcher [%d]\\n\", w->sfd);\n                w->skipped++;\n                ls->watcher_skipped++;\n                continue;\n            }\n            bipbuf_push(w->buf, total);\n            w->skipped = 0;\n        }\n        /* Can't fail because bipbuf_request succeeded. */\n        bipbuf_offer(w->buf, (unsigned char *) scratch, scratch_len);\n        ls->watcher_sent++;\n    }\n}\n\n/* Called with logger stack locked.\n * Releases every chunk associated with a watcher and closes the connection.\n * We can't presently send a connection back to the worker for further\n * processing.\n */\nstatic void logger_thread_close_watcher(logger_watcher *w) {\n    L_DEBUG(\"LOGGER: Closing dead watcher\\n\");\n    watchers[w->id] = NULL;\n    sidethread_conn_close(w->c);\n    watcher_count--;\n    bipbuf_free(w->buf);\n    free(w);\n    logger_set_flags();\n}\n\n/* Reads a particular worker thread's available bipbuf bytes. Parses each log\n * entry into the watcher buffers.\n */\nstatic int logger_thread_read(logger *l, struct logger_stats *ls) {\n    unsigned int size;\n    unsigned int pos = 0;\n    unsigned char *data;\n    char scratch[LOGGER_PARSE_SCRATCH];\n    logentry *e;\n    pthread_mutex_lock(&l->mutex);\n    data = bipbuf_peek_all(l->buf, &size);\n    pthread_mutex_unlock(&l->mutex);\n\n    if (data == NULL) {\n        return 0;\n    }\n    L_DEBUG(\"LOGGER: Got %d bytes from bipbuffer\\n\", size);\n\n    /* parse buffer */\n    while (pos < size && watcher_count > 0) {\n        enum logger_parse_entry_ret ret;\n        int scratch_len = 0;\n        e = (logentry *) (data + pos);\n        ret = logger_thread_parse_entry(e, ls, scratch, &scratch_len);\n        if (ret != LOGGER_PARSE_ENTRY_OK) {\n            /* TODO: stats counter */\n            fprintf(stderr, \"LOGGER: Failed to parse log entry\\n\");\n        } else {\n            logger_thread_write_entry(e, ls, scratch, scratch_len);\n        }\n        pos += sizeof(logentry) + e->size + e->pad;\n    }\n    assert(pos <= size);\n\n    pthread_mutex_lock(&l->mutex);\n    data = bipbuf_poll(l->buf, size);\n    ls->worker_written += l->written;\n    ls->worker_dropped += l->dropped;\n    l->written = 0;\n    l->dropped = 0;\n    pthread_mutex_unlock(&l->mutex);\n    if (data == NULL) {\n        fprintf(stderr, \"LOGGER: unexpectedly couldn't advance buf pointer\\n\");\n        assert(0);\n    }\n    return size; /* maybe the count of objects iterated? */\n}\n\n/* Since the event loop code isn't reusable without a refactor, and we have a\n * limited number of potential watchers, we run our own poll loop.\n * This calls poll() unnecessarily during write flushes, should be possible to\n * micro-optimize later.\n *\n * This flushes buffers attached to watchers, iterating through the bytes set\n * to each worker. Also checks for readability in case client connection was\n * closed.\n *\n * Allows a specific watcher to be flushed (if buf full)\n */\nstatic int logger_thread_poll_watchers(int force_poll, int watcher) {\n    int x;\n    int nfd = 0;\n    unsigned char *data;\n    unsigned int data_size = 0;\n    int flushed = 0;\n\n    for (x = 0; x < WATCHER_LIMIT; x++) {\n        logger_watcher *w = watchers[x];\n        if (w == NULL || (watcher != WATCHER_ALL && x != watcher))\n            continue;\n\n        data = bipbuf_peek_all(w->buf, &data_size);\n        if (data != NULL) {\n            watchers_pollfds[nfd].fd = w->sfd;\n            watchers_pollfds[nfd].events = POLLOUT;\n            nfd++;\n        } else if (force_poll) {\n            watchers_pollfds[nfd].fd = w->sfd;\n            watchers_pollfds[nfd].events = POLLIN;\n            nfd++;\n        }\n        /* This gets set after a call to poll, and should be used to gate on\n         * calling poll again.\n         */\n        w->failed_flush = false;\n    }\n\n    if (nfd == 0)\n        return 0;\n\n    //L_DEBUG(\"LOGGER: calling poll() [data_size: %d]\\n\", data_size);\n    int ret = poll(watchers_pollfds, nfd, 0);\n\n    if (ret < 0) {\n        perror(\"something failed with logger thread watcher fd polling\");\n        return -1;\n    }\n\n    nfd = 0;\n    for (x = 0; x < WATCHER_LIMIT; x++) {\n        logger_watcher *w = watchers[x];\n        if (w == NULL || (watcher != WATCHER_ALL && x != watcher))\n            continue;\n\n        data_size = 0;\n        /* Early detection of a disconnect. Otherwise we have to wait until\n         * the next write\n         */\n        if (watchers_pollfds[nfd].revents & POLLIN) {\n            char buf[1];\n            int res = ((conn*)w->c)->read(w->c, buf, 1);\n            if (res == 0 || (res == -1 && (errno != EAGAIN && errno != EWOULDBLOCK))) {\n                L_DEBUG(\"LOGGER: watcher closed remotely\\n\");\n                logger_thread_close_watcher(w);\n                nfd++;\n                continue;\n            }\n        }\n        if ((data = bipbuf_peek_all(w->buf, &data_size)) != NULL) {\n            if (watchers_pollfds[nfd].revents & (POLLHUP|POLLERR)) {\n                L_DEBUG(\"LOGGER: watcher closed during poll() call\\n\");\n                logger_thread_close_watcher(w);\n            } else if (watchers_pollfds[nfd].revents & POLLOUT) {\n                int total = 0;\n\n                /* We can write a bit. */\n                switch (w->t) {\n                    case LOGGER_WATCHER_STDERR:\n                        total = fwrite(data, 1, data_size, stderr);\n                        break;\n                    case LOGGER_WATCHER_CLIENT:\n                        total = ((conn*)w->c)->write(w->c, data, data_size);\n                        break;\n                }\n\n                L_DEBUG(\"LOGGER: poll() wrote %d to %d (data_size: %d) (bipbuf_used: %d)\\n\", total, w->sfd,\n                        data_size, bipbuf_used(w->buf));\n                if (total == -1) {\n                    if (errno != EAGAIN && errno != EWOULDBLOCK) {\n                        logger_thread_close_watcher(w);\n                    }\n                    L_DEBUG(\"LOGGER: watcher hit EAGAIN\\n\");\n                } else if (total == 0) {\n                    logger_thread_close_watcher(w);\n                } else {\n                    bipbuf_poll(w->buf, total);\n                    flushed += total;\n                }\n            }\n        }\n        nfd++;\n    }\n    return flushed;\n}\n\nstatic void logger_thread_flush_stats(struct logger_stats *ls) {\n    STATS_LOCK();\n    stats.log_worker_dropped  += ls->worker_dropped;\n    stats.log_worker_written  += ls->worker_written;\n    stats.log_watcher_skipped += ls->watcher_skipped;\n    stats.log_watcher_sent    += ls->watcher_sent;\n    stats_state.log_watchers   = ls->watcher_count;\n    STATS_UNLOCK();\n}\n\n#define MAX_LOGGER_SLEEP 1000000\n#define MIN_LOGGER_SLEEP 1000\n\n/* Primary logger thread routine */\nstatic void *logger_thread(void *arg) {\n    useconds_t to_sleep = MIN_LOGGER_SLEEP;\n    L_DEBUG(\"LOGGER: Starting logger thread\\n\");\n    // TODO: If we ever have item references in the logger code, will need to\n    // ensure everything is dequeued before stopping the thread.\n    while (do_run_logger_thread) {\n        int found_logs = 0;\n        logger *l;\n        struct logger_stats ls;\n        memset(&ls, 0, sizeof(struct logger_stats));\n\n        /* only sleep if we're *above* the minimum */\n        if (to_sleep > MIN_LOGGER_SLEEP)\n            usleep(to_sleep);\n\n        /* Call function to iterate each logger. */\n        pthread_mutex_lock(&logger_stack_lock);\n        if (watcher_count == 0) {\n            // Not bothering to loop on the condition here since it's fine to\n            // walk through with zero watchers.\n            pthread_cond_wait(&logger_stack_cond, &logger_stack_lock);\n        }\n        for (l = logger_stack_head; l != NULL; l=l->next) {\n            /* lock logger, call function to manipulate it */\n            found_logs += logger_thread_read(l, &ls);\n        }\n\n        logger_thread_poll_watchers(1, WATCHER_ALL);\n\n        /* capture the current count within mutual exclusion of the lock */\n        ls.watcher_count = watcher_count;\n\n        pthread_mutex_unlock(&logger_stack_lock);\n\n        /* TODO: abstract into a function and share with lru_crawler */\n        if (!found_logs) {\n            if (to_sleep < MAX_LOGGER_SLEEP)\n                to_sleep += to_sleep / 8;\n            if (to_sleep > MAX_LOGGER_SLEEP)\n                to_sleep = MAX_LOGGER_SLEEP;\n        } else {\n            to_sleep /= 2;\n            if (to_sleep < MIN_LOGGER_SLEEP)\n                to_sleep = MIN_LOGGER_SLEEP;\n        }\n        logger_thread_flush_stats(&ls);\n    }\n\n    return NULL;\n}\n\nstatic int start_logger_thread(void) {\n    int ret;\n    do_run_logger_thread = 1;\n    if ((ret = pthread_create(&logger_tid, NULL,\n                              logger_thread, NULL)) != 0) {\n        fprintf(stderr, \"Can't start logger thread: %s\\n\", strerror(ret));\n        return -1;\n    }\n    thread_setname(logger_tid, \"mc-log\");\n    return 0;\n}\n\nstatic int stop_logger_thread(void) {\n    // Guarantees that the logger thread is waiting on 'logger_stack_cond'\n    // before we signal it.\n    pthread_mutex_lock(&logger_stack_lock);\n    do_run_logger_thread = 0;\n    pthread_cond_signal(&logger_stack_cond);\n    pthread_mutex_unlock(&logger_stack_lock);\n    pthread_join(logger_tid, NULL);\n    return 0;\n}\n\n/*************************\n * Public functions for submitting logs and starting loggers from workers.\n *************************/\n\n/* Global logger thread start/init */\nvoid logger_init(void) {\n    /* TODO: auto destructor when threads exit */\n    /* TODO: error handling */\n\n    /* init stack for iterating loggers */\n    logger_stack_head = 0;\n    logger_stack_tail = 0;\n    pthread_key_create(&logger_key, NULL);\n\n    if (start_logger_thread() != 0) {\n        abort();\n    }\n\n    /* This is what adding a STDERR watcher looks like. should replace old\n     * \"verbose\" settings. */\n    //logger_add_watcher(NULL, 0);\n    return;\n}\n\nvoid logger_stop(void) {\n    stop_logger_thread();\n}\n\n/* called *from* the thread using a logger.\n * initializes the per-thread bipbuf, links it into the list of loggers\n */\nlogger *logger_create(void) {\n    L_DEBUG(\"LOGGER: Creating and linking new logger instance\\n\");\n    logger *l = calloc(1, sizeof(logger));\n    if (l == NULL) {\n        return NULL;\n    }\n\n    l->buf = bipbuf_new(settings.logger_buf_size);\n    if (l->buf == NULL) {\n        free(l);\n        return NULL;\n    }\n\n    l->entry_map = default_entries;\n\n    pthread_mutex_init(&l->mutex, NULL);\n    pthread_setspecific(logger_key, l);\n\n    /* add to list of loggers */\n    logger_link_q(l);\n    return l;\n}\n\n/* Public function for logging an entry.\n * Tries to encapsulate as much of the formatting as possible to simplify the\n * caller's code.\n */\nenum logger_ret_type logger_log(logger *l, const enum log_entry_type event, const void *entry, ...) {\n    bipbuf_t *buf = l->buf;\n    bool nospace = false;\n    va_list ap;\n    logentry *e;\n\n    const entry_details *d = &l->entry_map[event];\n    int reqlen = d->reqlen;\n\n    pthread_mutex_lock(&l->mutex);\n    /* Request a maximum length of data to write to */\n    e = (logentry *) bipbuf_request(buf, (sizeof(logentry) + reqlen));\n    if (e == NULL) {\n        l->dropped++;\n        pthread_mutex_unlock(&l->mutex);\n        return LOGGER_RET_NOSPACE;\n    }\n    e->event = event;\n    e->pad = 0;\n    e->gid = logger_get_gid();\n    /* TODO: Could pass this down as an argument now that we're using\n     * LOGGER_LOG() macro.\n     */\n    e->eflags = d->eflags;\n    /* Noting time isn't optional. A feature may be added to avoid rendering\n     * time and/or gid to a logger.\n     */\n    gettimeofday(&e->tv, NULL);\n\n    va_start(ap, entry);\n    d->log_cb(e, d, entry, ap);\n    va_end(ap);\n\n#ifdef NEED_ALIGN\n    /* Need to ensure *next* request is aligned. */\n    if (sizeof(logentry) + e->size % 8 != 0) {\n        e->pad = 8 - (sizeof(logentry) + e->size % 8);\n    }\n#endif\n\n    /* Push pointer forward by the actual amount required */\n    if (bipbuf_push(buf, (sizeof(logentry) + e->size + e->pad)) == 0) {\n        fprintf(stderr, \"LOGGER: Failed to bipbuf push a text entry\\n\");\n        pthread_mutex_unlock(&l->mutex);\n        return LOGGER_RET_ERR;\n    }\n    l->written++;\n    L_DEBUG(\"LOGGER: Requested %d bytes, wrote %lu bytes\\n\", reqlen,\n            (sizeof(logentry) + e->size));\n\n    pthread_mutex_unlock(&l->mutex);\n\n    if (nospace) {\n        return LOGGER_RET_NOSPACE;\n    } else {\n        return LOGGER_RET_OK;\n    }\n}\n\n/* Passes a client connection socket from a primary worker thread to the\n * logger thread. Caller *must* event_del() the client before handing it over.\n * Presently there's no way to hand the client back to the worker thread.\n */\nenum logger_add_watcher_ret logger_add_watcher(void *c, const int sfd, uint16_t f) {\n    int x;\n    logger_watcher *w = NULL;\n    pthread_mutex_lock(&logger_stack_lock);\n    if (watcher_count >= WATCHER_LIMIT) {\n        pthread_mutex_unlock(&logger_stack_lock);\n        return LOGGER_ADD_WATCHER_TOO_MANY;\n    }\n\n    for (x = 0; x < WATCHER_LIMIT-1; x++) {\n        if (watchers[x] == NULL)\n            break;\n    }\n\n    w = calloc(1, sizeof(logger_watcher));\n    if (w == NULL) {\n        pthread_mutex_unlock(&logger_stack_lock);\n        return LOGGER_ADD_WATCHER_FAILED;\n    }\n    w->c = c;\n    w->sfd = sfd;\n    if (sfd == 0 && c == NULL) {\n        w->t = LOGGER_WATCHER_STDERR;\n    } else {\n        w->t = LOGGER_WATCHER_CLIENT;\n    }\n    w->id = x;\n    w->eflags = f;\n    w->min_gid = logger_get_gid();\n    w->buf = bipbuf_new(settings.logger_watcher_buf_size);\n    if (w->buf == NULL) {\n        free(w);\n        pthread_mutex_unlock(&logger_stack_lock);\n        return LOGGER_ADD_WATCHER_FAILED;\n    }\n    bipbuf_offer(w->buf, (unsigned char *) \"OK\\r\\n\", 4);\n\n    watchers[x] = w;\n    watcher_count++;\n    /* Update what flags the global logs will watch */\n    logger_set_flags();\n    pthread_cond_signal(&logger_stack_cond);\n\n    pthread_mutex_unlock(&logger_stack_lock);\n    return LOGGER_ADD_WATCHER_OK;\n}\n"
        },
        {
          "name": "logger.h",
          "type": "blob",
          "size": 6.216796875,
          "content": "/* logging functions */\n#ifndef LOGGER_H\n#define LOGGER_H\n\n#include \"bipbuffer.h\"\n\n/* TODO: starttime tunable */\n#define LOGGER_BUF_SIZE 1024 * 64\n#define LOGGER_WATCHER_BUF_SIZE 1024 * 256\n#define LOGGER_ENTRY_MAX_SIZE 2048\n#define GET_LOGGER() ((logger *) pthread_getspecific(logger_key));\n\n/* Inlined from memcached.h - should go into sub header */\ntypedef unsigned int rel_time_t;\n\nenum log_entry_type {\n    LOGGER_ASCII_CMD = 0,\n    LOGGER_EVICTION,\n    LOGGER_ITEM_GET,\n    LOGGER_ITEM_STORE,\n    LOGGER_CRAWLER_STATUS,\n    LOGGER_SLAB_MOVE,\n    LOGGER_CONNECTION_NEW,\n    LOGGER_CONNECTION_CLOSE,\n    LOGGER_CONNECTION_ERROR,\n    LOGGER_CONNECTION_TLSERROR,\n    LOGGER_DELETIONS,\n#ifdef EXTSTORE\n    LOGGER_EXTSTORE_WRITE,\n    LOGGER_COMPACT_START,\n    LOGGER_COMPACT_ABORT,\n    LOGGER_COMPACT_READ_START,\n    LOGGER_COMPACT_READ_END,\n    LOGGER_COMPACT_END,\n    LOGGER_COMPACT_FRAGINFO,\n#endif\n#ifdef PROXY\n    LOGGER_PROXY_CONFIG,\n    LOGGER_PROXY_RAW,\n    LOGGER_PROXY_ERROR,\n    LOGGER_PROXY_USER,\n    LOGGER_PROXY_REQ,\n    LOGGER_PROXY_BE_ERROR,\n#endif\n};\n\nenum logger_ret_type {\n    LOGGER_RET_OK = 0,\n    LOGGER_RET_NOSPACE,\n    LOGGER_RET_ERR\n};\n\nenum logger_parse_entry_ret {\n    LOGGER_PARSE_ENTRY_OK = 0,\n    LOGGER_PARSE_ENTRY_FULLBUF,\n    LOGGER_PARSE_ENTRY_FAILED\n};\n\ntypedef struct _logentry logentry;\ntypedef struct _entry_details entry_details;\n\ntypedef void (*entry_log_cb)(logentry *e, const entry_details *d, const void *entry, va_list ap);\ntypedef int (*entry_parse_cb)(logentry *e, char *scratch);\n\nstruct _entry_details {\n    int reqlen;\n    uint16_t eflags;\n    entry_log_cb log_cb;\n    entry_parse_cb parse_cb;\n    char *format;\n};\n\n/* log entry intermediary structures */\nstruct logentry_eviction {\n    long long int exptime;\n    int nbytes;\n    uint32_t latime;\n    uint16_t it_flags;\n    uint8_t nkey;\n    uint8_t clsid;\n    char key[];\n};\n#ifdef EXTSTORE\nstruct logentry_ext_write {\n    long long int exptime;\n    uint32_t latime;\n    uint16_t it_flags;\n    uint8_t nkey;\n    uint8_t clsid;\n    uint8_t bucket;\n    char key[];\n};\n#endif\nstruct logentry_item_get {\n    uint8_t was_found;\n    uint8_t nkey;\n    uint8_t clsid;\n    int nbytes;\n    int sfd;\n    char key[];\n};\n\nstruct logentry_item_store {\n    int status;\n    int cmd;\n    rel_time_t ttl;\n    uint8_t nkey;\n    uint8_t clsid;\n    int nbytes;\n    int sfd;\n    char key[];\n};\n\nstruct logentry_deletion {\n    int nbytes;\n    int cmd;\n    uint8_t nkey;\n    uint8_t clsid;\n    char key[];\n};\n\nstruct logentry_conn_event {\n    int transport;\n    int reason;\n    int sfd;\n    struct sockaddr_in6 addr;\n};\n#ifdef PROXY\nstruct logentry_proxy_req {\n    unsigned short type;\n    unsigned short code;\n    int status;\n    int conn_fd;\n    uint32_t reqlen;\n    size_t dlen;\n    size_t be_namelen;\n    size_t be_portlen;\n    long elapsed;\n    char data[];\n};\n\nstruct logentry_proxy_errbe {\n    size_t errlen;\n    size_t be_namelen;\n    size_t be_portlen;\n    size_t be_labellen;\n    size_t be_rbuflen;\n    int be_depth;\n    int retry;\n    char data[];\n};\n#endif\n/* end intermediary structures */\n\n/* WARNING: cuddled items aren't compatible with warm restart. more code\n * necessary to ensure log streams are all flushed/processed before stopping\n */\nstruct _logentry {\n    enum log_entry_type event;\n    uint8_t pad;\n    uint16_t eflags;\n    uint64_t gid;\n    struct timeval tv; /* not monotonic! */\n    int size;\n    union {\n        char end;\n    } data[];\n};\n\n#define LOG_SYSEVENTS  (1<<1) /* threads start/stop/working */\n#define LOG_FETCHERS   (1<<2) /* get/gets/etc */\n#define LOG_MUTATIONS  (1<<3) /* set/append/incr/etc */\n#define LOG_SYSERRORS  (1<<4) /* malloc/etc errors */\n#define LOG_CONNEVENTS (1<<5) /* new client, closed, etc */\n#define LOG_EVICTIONS  (1<<6) /* details of evicted items */\n#define LOG_STRICT     (1<<7) /* block worker instead of drop */\n#define LOG_RAWCMDS    (1<<9) /* raw ascii commands */\n#define LOG_PROXYREQS  (1<<10) /* command logs from proxy */\n#define LOG_PROXYEVENTS (1<<11) /* error log stream from proxy */\n#define LOG_PROXYUSER (1<<12) /* user generated logs from proxy */\n#define LOG_DELETIONS (1<<13) /* see whats deleted */\n\ntypedef struct _logger {\n    struct _logger *prev;\n    struct _logger *next;\n    pthread_mutex_t mutex; /* guard for this + *buf */\n    uint64_t written; /* entries written to the buffer */\n    uint64_t dropped; /* entries dropped */\n    uint64_t blocked; /* times blocked instead of dropped */\n    uint16_t fetcher_ratio; /* log one out of every N fetches */\n    uint16_t mutation_ratio; /* log one out of every N mutations */\n    uint16_t eflags; /* flags this logger should log */\n    bipbuf_t *buf;\n    const entry_details *entry_map;\n} logger;\n\nenum logger_watcher_type {\n    LOGGER_WATCHER_STDERR = 0,\n    LOGGER_WATCHER_CLIENT = 1\n};\n\ntypedef struct  {\n    void *c; /* original connection structure. still with source thread attached */\n    int sfd; /* client fd */\n    int id; /* id number for watcher list */\n    uint64_t skipped; /* lines skipped since last successful print */\n    uint64_t min_gid; /* don't show log entries older than this GID */\n    bool failed_flush; /* recently failed to write out (EAGAIN), wait before retry */\n    enum logger_watcher_type t; /* stderr, client, syslog, etc */\n    uint16_t eflags; /* flags we are interested in */\n    bipbuf_t *buf; /* per-watcher output buffer */\n} logger_watcher;\n\n\nstruct logger_stats {\n    uint64_t worker_dropped;\n    uint64_t worker_written;\n    uint64_t watcher_skipped;\n    uint64_t watcher_sent;\n    uint64_t watcher_count;\n};\n\nextern pthread_key_t logger_key;\n\n/* public functions */\n\nvoid logger_init(void);\nvoid logger_stop(void);\nlogger *logger_create(void);\n\n#define LOGGER_LOG(l, flag, type, ...) \\\n    do { \\\n        logger *myl = l; \\\n        if (l == NULL) \\\n            myl = GET_LOGGER(); \\\n        if (myl->eflags & flag) \\\n            logger_log(myl, type, __VA_ARGS__); \\\n    } while (0)\n\nenum logger_ret_type logger_log(logger *l, const enum log_entry_type event, const void *entry, ...);\n\nenum logger_add_watcher_ret {\n    LOGGER_ADD_WATCHER_TOO_MANY = 0,\n    LOGGER_ADD_WATCHER_OK,\n    LOGGER_ADD_WATCHER_FAILED\n};\n\nenum logger_add_watcher_ret logger_add_watcher(void *c, const int sfd, uint16_t f);\n\n/* functions used by restart system */\nuint64_t logger_get_gid(void);\nvoid logger_set_gid(uint64_t gid);\n\n#endif\n"
        },
        {
          "name": "m4",
          "type": "tree",
          "content": null
        },
        {
          "name": "md5.c",
          "type": "blob",
          "size": 12.142578125,
          "content": "/*\n  Copyright (C) 1999, 2000, 2002 Aladdin Enterprises.  All rights reserved.\n\n  This software is provided 'as-is', without any express or implied\n  warranty.  In no event will the authors be held liable for any damages\n  arising from the use of this software.\n\n  Permission is granted to anyone to use this software for any purpose,\n  including commercial applications, and to alter it and redistribute it\n  freely, subject to the following restrictions:\n\n  1. The origin of this software must not be misrepresented; you must not\n     claim that you wrote the original software. If you use this software\n     in a product, an acknowledgment in the product documentation would be\n     appreciated but is not required.\n  2. Altered source versions must be plainly marked as such, and must not be\n     misrepresented as being the original software.\n  3. This notice may not be removed or altered from any source distribution.\n\n  L. Peter Deutsch\n  ghost@aladdin.com\n\n */\n/* $Id: md5.c,v 1.6 2002/04/13 19:20:28 lpd Exp $ */\n/*\n  Independent implementation of MD5 (RFC 1321).\n\n  This code implements the MD5 Algorithm defined in RFC 1321, whose\n  text is available at\n\thttp://www.ietf.org/rfc/rfc1321.txt\n  The code is derived from the text of the RFC, including the test suite\n  (section A.5) but excluding the rest of Appendix A.  It does not include\n  any code or documentation that is identified in the RFC as being\n  copyrighted.\n\n  The original and principal author of md5.c is L. Peter Deutsch\n  <ghost@aladdin.com>.  Other authors are noted in the change history\n  that follows (in reverse chronological order):\n\n  2002-04-13 lpd Clarified derivation from RFC 1321; now handles byte order\n\teither statically or dynamically; added missing #include <string.h>\n\tin library.\n  2002-03-11 lpd Corrected argument list for main(), and added int return\n\ttype, in test program and T value program.\n  2002-02-21 lpd Added missing #include <stdio.h> in test program.\n  2000-07-03 lpd Patched to eliminate warnings about \"constant is\n\tunsigned in ANSI C, signed in traditional\"; made test program\n\tself-checking.\n  1999-11-04 lpd Edited comments slightly for automatic TOC extraction.\n  1999-10-18 lpd Fixed typo in header comment (ansi2knr rather than md5).\n  1999-05-03 lpd Original version.\n */\n\n#include \"md5.h\"\n#include <string.h>\n\n#undef BYTE_ORDER\t/* 1 = big-endian, -1 = little-endian, 0 = unknown */\n#ifdef ARCH_IS_BIG_ENDIAN\n#  define BYTE_ORDER (ARCH_IS_BIG_ENDIAN ? 1 : -1)\n#else\n#  define BYTE_ORDER 0\n#endif\n\n#define T_MASK ((md5_word_t)~0)\n#define T1 /* 0xd76aa478 */ (T_MASK ^ 0x28955b87)\n#define T2 /* 0xe8c7b756 */ (T_MASK ^ 0x173848a9)\n#define T3    0x242070db\n#define T4 /* 0xc1bdceee */ (T_MASK ^ 0x3e423111)\n#define T5 /* 0xf57c0faf */ (T_MASK ^ 0x0a83f050)\n#define T6    0x4787c62a\n#define T7 /* 0xa8304613 */ (T_MASK ^ 0x57cfb9ec)\n#define T8 /* 0xfd469501 */ (T_MASK ^ 0x02b96afe)\n#define T9    0x698098d8\n#define T10 /* 0x8b44f7af */ (T_MASK ^ 0x74bb0850)\n#define T11 /* 0xffff5bb1 */ (T_MASK ^ 0x0000a44e)\n#define T12 /* 0x895cd7be */ (T_MASK ^ 0x76a32841)\n#define T13    0x6b901122\n#define T14 /* 0xfd987193 */ (T_MASK ^ 0x02678e6c)\n#define T15 /* 0xa679438e */ (T_MASK ^ 0x5986bc71)\n#define T16    0x49b40821\n#define T17 /* 0xf61e2562 */ (T_MASK ^ 0x09e1da9d)\n#define T18 /* 0xc040b340 */ (T_MASK ^ 0x3fbf4cbf)\n#define T19    0x265e5a51\n#define T20 /* 0xe9b6c7aa */ (T_MASK ^ 0x16493855)\n#define T21 /* 0xd62f105d */ (T_MASK ^ 0x29d0efa2)\n#define T22    0x02441453\n#define T23 /* 0xd8a1e681 */ (T_MASK ^ 0x275e197e)\n#define T24 /* 0xe7d3fbc8 */ (T_MASK ^ 0x182c0437)\n#define T25    0x21e1cde6\n#define T26 /* 0xc33707d6 */ (T_MASK ^ 0x3cc8f829)\n#define T27 /* 0xf4d50d87 */ (T_MASK ^ 0x0b2af278)\n#define T28    0x455a14ed\n#define T29 /* 0xa9e3e905 */ (T_MASK ^ 0x561c16fa)\n#define T30 /* 0xfcefa3f8 */ (T_MASK ^ 0x03105c07)\n#define T31    0x676f02d9\n#define T32 /* 0x8d2a4c8a */ (T_MASK ^ 0x72d5b375)\n#define T33 /* 0xfffa3942 */ (T_MASK ^ 0x0005c6bd)\n#define T34 /* 0x8771f681 */ (T_MASK ^ 0x788e097e)\n#define T35    0x6d9d6122\n#define T36 /* 0xfde5380c */ (T_MASK ^ 0x021ac7f3)\n#define T37 /* 0xa4beea44 */ (T_MASK ^ 0x5b4115bb)\n#define T38    0x4bdecfa9\n#define T39 /* 0xf6bb4b60 */ (T_MASK ^ 0x0944b49f)\n#define T40 /* 0xbebfbc70 */ (T_MASK ^ 0x4140438f)\n#define T41    0x289b7ec6\n#define T42 /* 0xeaa127fa */ (T_MASK ^ 0x155ed805)\n#define T43 /* 0xd4ef3085 */ (T_MASK ^ 0x2b10cf7a)\n#define T44    0x04881d05\n#define T45 /* 0xd9d4d039 */ (T_MASK ^ 0x262b2fc6)\n#define T46 /* 0xe6db99e5 */ (T_MASK ^ 0x1924661a)\n#define T47    0x1fa27cf8\n#define T48 /* 0xc4ac5665 */ (T_MASK ^ 0x3b53a99a)\n#define T49 /* 0xf4292244 */ (T_MASK ^ 0x0bd6ddbb)\n#define T50    0x432aff97\n#define T51 /* 0xab9423a7 */ (T_MASK ^ 0x546bdc58)\n#define T52 /* 0xfc93a039 */ (T_MASK ^ 0x036c5fc6)\n#define T53    0x655b59c3\n#define T54 /* 0x8f0ccc92 */ (T_MASK ^ 0x70f3336d)\n#define T55 /* 0xffeff47d */ (T_MASK ^ 0x00100b82)\n#define T56 /* 0x85845dd1 */ (T_MASK ^ 0x7a7ba22e)\n#define T57    0x6fa87e4f\n#define T58 /* 0xfe2ce6e0 */ (T_MASK ^ 0x01d3191f)\n#define T59 /* 0xa3014314 */ (T_MASK ^ 0x5cfebceb)\n#define T60    0x4e0811a1\n#define T61 /* 0xf7537e82 */ (T_MASK ^ 0x08ac817d)\n#define T62 /* 0xbd3af235 */ (T_MASK ^ 0x42c50dca)\n#define T63    0x2ad7d2bb\n#define T64 /* 0xeb86d391 */ (T_MASK ^ 0x14792c6e)\n\n\nstatic void\nmd5_process(md5_state_t *pms, const md5_byte_t *data /*[64]*/)\n{\n    md5_word_t\n\ta = pms->abcd[0], b = pms->abcd[1],\n\tc = pms->abcd[2], d = pms->abcd[3];\n    md5_word_t t;\n#if BYTE_ORDER > 0\n    /* Define storage only for big-endian CPUs. */\n    md5_word_t X[16];\n#else\n    /* Define storage for little-endian or both types of CPUs. */\n    md5_word_t xbuf[16];\n    const md5_word_t *X;\n#endif\n\n    {\n#if BYTE_ORDER == 0\n\t/*\n\t * Determine dynamically whether this is a big-endian or\n\t * little-endian machine, since we can use a more efficient\n\t * algorithm on the latter.\n\t */\n\tstatic const int w = 1;\n\n\tif (*((const md5_byte_t *)&w)) /* dynamic little-endian */\n#endif\n#if BYTE_ORDER <= 0\t\t/* little-endian */\n\t{\n\t    /*\n\t     * On little-endian machines, we can process properly aligned\n\t     * data without copying it.\n\t     */\n\t    if (!((data - (const md5_byte_t *)0) & 3)) {\n\t\t/* data are properly aligned */\n\t\tX = (const md5_word_t *)data;\n\t    } else {\n\t\t/* not aligned */\n\t\tmemcpy(xbuf, data, 64);\n\t\tX = xbuf;\n\t    }\n\t}\n#endif\n#if BYTE_ORDER == 0\n\telse\t\t\t/* dynamic big-endian */\n#endif\n#if BYTE_ORDER >= 0\t\t/* big-endian */\n\t{\n\t    /*\n\t     * On big-endian machines, we must arrange the bytes in the\n\t     * right order.\n\t     */\n\t    const md5_byte_t *xp = data;\n\t    int i;\n\n#  if BYTE_ORDER == 0\n\t    X = xbuf;\t\t/* (dynamic only) */\n#  else\n#    define xbuf X\t\t/* (static only) */\n#  endif\n\t    for (i = 0; i < 16; ++i, xp += 4)\n\t\txbuf[i] = xp[0] + (xp[1] << 8) + (xp[2] << 16) + (xp[3] << 24);\n\t}\n#endif\n    }\n\n#define ROTATE_LEFT(x, n) (((x) << (n)) | ((x) >> (32 - (n))))\n\n    /* Round 1. */\n    /* Let [abcd k s i] denote the operation\n       a = b + ((a + F(b,c,d) + X[k] + T[i]) <<< s). */\n#define F(x, y, z) (((x) & (y)) | (~(x) & (z)))\n#define SET(a, b, c, d, k, s, Ti)\\\n  t = a + F(b,c,d) + X[k] + Ti;\\\n  a = ROTATE_LEFT(t, s) + b\n    /* Do the following 16 operations. */\n    SET(a, b, c, d,  0,  7,  T1);\n    SET(d, a, b, c,  1, 12,  T2);\n    SET(c, d, a, b,  2, 17,  T3);\n    SET(b, c, d, a,  3, 22,  T4);\n    SET(a, b, c, d,  4,  7,  T5);\n    SET(d, a, b, c,  5, 12,  T6);\n    SET(c, d, a, b,  6, 17,  T7);\n    SET(b, c, d, a,  7, 22,  T8);\n    SET(a, b, c, d,  8,  7,  T9);\n    SET(d, a, b, c,  9, 12, T10);\n    SET(c, d, a, b, 10, 17, T11);\n    SET(b, c, d, a, 11, 22, T12);\n    SET(a, b, c, d, 12,  7, T13);\n    SET(d, a, b, c, 13, 12, T14);\n    SET(c, d, a, b, 14, 17, T15);\n    SET(b, c, d, a, 15, 22, T16);\n#undef SET\n\n     /* Round 2. */\n     /* Let [abcd k s i] denote the operation\n          a = b + ((a + G(b,c,d) + X[k] + T[i]) <<< s). */\n#define G(x, y, z) (((x) & (z)) | ((y) & ~(z)))\n#define SET(a, b, c, d, k, s, Ti)\\\n  t = a + G(b,c,d) + X[k] + Ti;\\\n  a = ROTATE_LEFT(t, s) + b\n     /* Do the following 16 operations. */\n    SET(a, b, c, d,  1,  5, T17);\n    SET(d, a, b, c,  6,  9, T18);\n    SET(c, d, a, b, 11, 14, T19);\n    SET(b, c, d, a,  0, 20, T20);\n    SET(a, b, c, d,  5,  5, T21);\n    SET(d, a, b, c, 10,  9, T22);\n    SET(c, d, a, b, 15, 14, T23);\n    SET(b, c, d, a,  4, 20, T24);\n    SET(a, b, c, d,  9,  5, T25);\n    SET(d, a, b, c, 14,  9, T26);\n    SET(c, d, a, b,  3, 14, T27);\n    SET(b, c, d, a,  8, 20, T28);\n    SET(a, b, c, d, 13,  5, T29);\n    SET(d, a, b, c,  2,  9, T30);\n    SET(c, d, a, b,  7, 14, T31);\n    SET(b, c, d, a, 12, 20, T32);\n#undef SET\n\n     /* Round 3. */\n     /* Let [abcd k s t] denote the operation\n          a = b + ((a + H(b,c,d) + X[k] + T[i]) <<< s). */\n#define H(x, y, z) ((x) ^ (y) ^ (z))\n#define SET(a, b, c, d, k, s, Ti)\\\n  t = a + H(b,c,d) + X[k] + Ti;\\\n  a = ROTATE_LEFT(t, s) + b\n     /* Do the following 16 operations. */\n    SET(a, b, c, d,  5,  4, T33);\n    SET(d, a, b, c,  8, 11, T34);\n    SET(c, d, a, b, 11, 16, T35);\n    SET(b, c, d, a, 14, 23, T36);\n    SET(a, b, c, d,  1,  4, T37);\n    SET(d, a, b, c,  4, 11, T38);\n    SET(c, d, a, b,  7, 16, T39);\n    SET(b, c, d, a, 10, 23, T40);\n    SET(a, b, c, d, 13,  4, T41);\n    SET(d, a, b, c,  0, 11, T42);\n    SET(c, d, a, b,  3, 16, T43);\n    SET(b, c, d, a,  6, 23, T44);\n    SET(a, b, c, d,  9,  4, T45);\n    SET(d, a, b, c, 12, 11, T46);\n    SET(c, d, a, b, 15, 16, T47);\n    SET(b, c, d, a,  2, 23, T48);\n#undef SET\n\n     /* Round 4. */\n     /* Let [abcd k s t] denote the operation\n          a = b + ((a + I(b,c,d) + X[k] + T[i]) <<< s). */\n#define I(x, y, z) ((y) ^ ((x) | ~(z)))\n#define SET(a, b, c, d, k, s, Ti)\\\n  t = a + I(b,c,d) + X[k] + Ti;\\\n  a = ROTATE_LEFT(t, s) + b\n     /* Do the following 16 operations. */\n    SET(a, b, c, d,  0,  6, T49);\n    SET(d, a, b, c,  7, 10, T50);\n    SET(c, d, a, b, 14, 15, T51);\n    SET(b, c, d, a,  5, 21, T52);\n    SET(a, b, c, d, 12,  6, T53);\n    SET(d, a, b, c,  3, 10, T54);\n    SET(c, d, a, b, 10, 15, T55);\n    SET(b, c, d, a,  1, 21, T56);\n    SET(a, b, c, d,  8,  6, T57);\n    SET(d, a, b, c, 15, 10, T58);\n    SET(c, d, a, b,  6, 15, T59);\n    SET(b, c, d, a, 13, 21, T60);\n    SET(a, b, c, d,  4,  6, T61);\n    SET(d, a, b, c, 11, 10, T62);\n    SET(c, d, a, b,  2, 15, T63);\n    SET(b, c, d, a,  9, 21, T64);\n#undef SET\n\n     /* Then perform the following additions. (That is increment each\n        of the four registers by the value it had before this block\n        was started.) */\n    pms->abcd[0] += a;\n    pms->abcd[1] += b;\n    pms->abcd[2] += c;\n    pms->abcd[3] += d;\n}\n\nvoid\nmd5_init(md5_state_t *pms)\n{\n    pms->count[0] = pms->count[1] = 0;\n    pms->abcd[0] = 0x67452301;\n    pms->abcd[1] = /*0xefcdab89*/ T_MASK ^ 0x10325476;\n    pms->abcd[2] = /*0x98badcfe*/ T_MASK ^ 0x67452301;\n    pms->abcd[3] = 0x10325476;\n}\n\nvoid\nmd5_append(md5_state_t *pms, const md5_byte_t *data, int nbytes)\n{\n    const md5_byte_t *p = data;\n    int left = nbytes;\n    int offset = (pms->count[0] >> 3) & 63;\n    md5_word_t nbits = (md5_word_t)(nbytes << 3);\n\n    if (nbytes <= 0)\n\treturn;\n\n    /* Update the message length. */\n    pms->count[1] += nbytes >> 29;\n    pms->count[0] += nbits;\n    if (pms->count[0] < nbits)\n\tpms->count[1]++;\n\n    /* Process an initial partial block. */\n    if (offset) {\n\tint copy = (offset + nbytes > 64 ? 64 - offset : nbytes);\n\n\tmemcpy(pms->buf + offset, p, copy);\n\tif (offset + copy < 64)\n\t    return;\n\tp += copy;\n\tleft -= copy;\n\tmd5_process(pms, pms->buf);\n    }\n\n    /* Process full blocks. */\n    for (; left >= 64; p += 64, left -= 64)\n\tmd5_process(pms, p);\n\n    /* Process a final partial block. */\n    if (left)\n\tmemcpy(pms->buf, p, left);\n}\n\nvoid\nmd5_finish(md5_state_t *pms, md5_byte_t digest[16])\n{\n    static const md5_byte_t pad[64] = {\n\t0x80, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n    };\n    md5_byte_t data[8];\n    int i;\n\n    /* Save the length before padding. */\n    for (i = 0; i < 8; ++i)\n\tdata[i] = (md5_byte_t)(pms->count[i >> 2] >> ((i & 3) << 3));\n    /* Pad to 56 bytes mod 64. */\n    md5_append(pms, pad, ((55 - (pms->count[0] >> 3)) & 63) + 1);\n    /* Append the length. */\n    md5_append(pms, data, 8);\n    for (i = 0; i < 16; ++i)\n\tdigest[i] = (md5_byte_t)(pms->abcd[i >> 2] >> ((i & 3) << 3));\n}\n"
        },
        {
          "name": "md5.h",
          "type": "blob",
          "size": 3.43359375,
          "content": "/*\n  Copyright (C) 1999, 2002 Aladdin Enterprises.  All rights reserved.\n\n  This software is provided 'as-is', without any express or implied\n  warranty.  In no event will the authors be held liable for any damages\n  arising from the use of this software.\n\n  Permission is granted to anyone to use this software for any purpose,\n  including commercial applications, and to alter it and redistribute it\n  freely, subject to the following restrictions:\n\n  1. The origin of this software must not be misrepresented; you must not\n     claim that you wrote the original software. If you use this software\n     in a product, an acknowledgment in the product documentation would be\n     appreciated but is not required.\n  2. Altered source versions must be plainly marked as such, and must not be\n     misrepresented as being the original software.\n  3. This notice may not be removed or altered from any source distribution.\n\n  L. Peter Deutsch\n  ghost@aladdin.com\n\n */\n/* $Id: md5.h,v 1.4 2002/04/13 19:20:28 lpd Exp $ */\n/*\n  Independent implementation of MD5 (RFC 1321).\n\n  This code implements the MD5 Algorithm defined in RFC 1321, whose\n  text is available at\n\thttp://www.ietf.org/rfc/rfc1321.txt\n  The code is derived from the text of the RFC, including the test suite\n  (section A.5) but excluding the rest of Appendix A.  It does not include\n  any code or documentation that is identified in the RFC as being\n  copyrighted.\n\n  The original and principal author of md5.h is L. Peter Deutsch\n  <ghost@aladdin.com>.  Other authors are noted in the change history\n  that follows (in reverse chronological order):\n\n  2002-04-13 lpd Removed support for non-ANSI compilers; removed\n\treferences to Ghostscript; clarified derivation from RFC 1321;\n\tnow handles byte order either statically or dynamically.\n  1999-11-04 lpd Edited comments slightly for automatic TOC extraction.\n  1999-10-18 lpd Fixed typo in header comment (ansi2knr rather than md5);\n\tadded conditionalization for C++ compilation from Martin\n\tPurschke <purschke@bnl.gov>.\n  1999-05-03 lpd Original version.\n */\n\n#ifndef md5_INCLUDED\n#  define md5_INCLUDED\n\n/*\n * This package supports both compile-time and run-time determination of CPU\n * byte order.  If ARCH_IS_BIG_ENDIAN is defined as 0, the code will be\n * compiled to run only on little-endian CPUs; if ARCH_IS_BIG_ENDIAN is\n * defined as non-zero, the code will be compiled to run only on big-endian\n * CPUs; if ARCH_IS_BIG_ENDIAN is not defined, the code will be compiled to\n * run on either big- or little-endian CPUs, but will run slightly less\n * efficiently on either one than if ARCH_IS_BIG_ENDIAN is defined.\n */\n\ntypedef unsigned char md5_byte_t; /* 8-bit byte */\ntypedef unsigned int md5_word_t; /* 32-bit word */\n\n/* Define the state of the MD5 Algorithm. */\ntypedef struct md5_state_s {\n    md5_word_t count[2];\t/* message length in bits, lsw first */\n    md5_word_t abcd[4];\t\t/* digest buffer */\n    md5_byte_t buf[64];\t\t/* accumulate block */\n} md5_state_t;\n\n#ifdef __cplusplus\nextern \"C\" \n{\n#endif\n\n/* Initialize the algorithm. */\n\n#ifdef WIN32\n_declspec(dllexport)\n#endif\nvoid md5_init(md5_state_t *pms);\n\n/* Append a string to the message. */\n#ifdef WIN32\n_declspec(dllexport)\n#endif\nvoid md5_append(md5_state_t *pms, const md5_byte_t *data, int nbytes);\n\n/* Finish the message and return the digest. */\n#ifdef WIN32\n_declspec(dllexport)\n#endif\nvoid md5_finish(md5_state_t *pms, md5_byte_t digest[16]);\n\n#ifdef __cplusplus\n}  /* end extern \"C\" */\n#endif\n\n#endif /* md5_INCLUDED */\n"
        },
        {
          "name": "memcached.c",
          "type": "blob",
          "size": 220.439453125,
          "content": "/* -*- Mode: C; tab-width: 4; c-basic-offset: 4; indent-tabs-mode: nil -*- */\n/*\n *  memcached - memory caching daemon\n *\n *       https://www.memcached.org/\n *\n *  Copyright 2003 Danga Interactive, Inc.  All rights reserved.\n *\n *  Use and distribution licensed under the BSD license.  See\n *  the LICENSE file for full text.\n *\n *  Authors:\n *      Anatoly Vorobey <mellon@pobox.com>\n *      Brad Fitzpatrick <brad@danga.com>\n */\n#include \"memcached.h\"\n#include \"storage.h\"\n#include \"authfile.h\"\n#include \"restart.h\"\n#include \"slabs_mover.h\"\n#include <sys/stat.h>\n#include <sys/socket.h>\n#include <sys/un.h>\n#include <signal.h>\n#include <sys/param.h>\n#include <sys/resource.h>\n#include <sys/uio.h>\n#include <ctype.h>\n#include <stdarg.h>\n\n/* some POSIX systems need the following definition\n * to get mlockall flags out of sys/mman.h.  */\n#ifndef _P1003_1B_VISIBLE\n#define _P1003_1B_VISIBLE\n#endif\n#include <pwd.h>\n#include <sys/mman.h>\n#include <fcntl.h>\n#include <netinet/tcp.h>\n#include <arpa/inet.h>\n#include <errno.h>\n#include <stdlib.h>\n#include <stdio.h>\n#include <string.h>\n#include <time.h>\n#include <assert.h>\n#include <sysexits.h>\n#include <stddef.h>\n\n#ifdef HAVE_GETOPT_LONG\n#include <getopt.h>\n#endif\n\n#include \"tls.h\"\n\n#include \"proto_text.h\"\n#include \"proto_bin.h\"\n#include \"proto_proxy.h\"\n\n#if defined(__FreeBSD__)\n#include <sys/sysctl.h>\n#endif\n\n/*\n * forward declarations\n */\nstatic void drive_machine(conn *c);\nstatic int new_socket(struct addrinfo *ai);\nstatic ssize_t tcp_read(conn *arg, void *buf, size_t count);\nstatic ssize_t tcp_sendmsg(conn *arg, struct msghdr *msg, int flags);\nstatic ssize_t tcp_write(conn *arg, void *buf, size_t count);\n\nenum try_read_result {\n    READ_DATA_RECEIVED,\n    READ_NO_DATA_RECEIVED,\n    READ_ERROR,            /** an error occurred (on the socket) (or client closed connection) */\n    READ_MEMORY_ERROR      /** failed to allocate more memory */\n};\n\nstatic int try_read_command_negotiate(conn *c);\nstatic int try_read_command_udp(conn *c);\n\nstatic enum try_read_result try_read_network(conn *c);\nstatic enum try_read_result try_read_udp(conn *c);\n\nstatic int start_conn_timeout_thread(void);\n\n/* stats */\nstatic void stats_init(void);\nstatic void conn_to_str(const conn *c, char *addr, char *svr_addr);\n\n/* defaults */\nstatic void settings_init(void);\n\n/* event handling, network IO */\nstatic void event_handler(const evutil_socket_t fd, const short which, void *arg);\nstatic void conn_close(conn *c);\nstatic void conn_init(void);\nstatic bool update_event(conn *c, const int new_flags);\nstatic void complete_nread(conn *c);\n\nstatic void conn_free(conn *c);\n\n/** exported globals **/\nstruct stats stats;\nstruct stats_state stats_state;\nstruct settings settings;\ntime_t process_started;     /* when the process was started */\nconn **conns;\n\n#ifdef EXTSTORE\n/* hoping this is temporary; I'd prefer to cut globals, but will complete this\n * battle another day.\n */\nvoid *ext_storage = NULL;\n#endif\n/** file scope variables **/\nstatic conn *listen_conn = NULL;\nstatic int max_fds;\nstatic struct event_base *main_base;\n\nenum transmit_result {\n    TRANSMIT_COMPLETE,   /** All done writing. */\n    TRANSMIT_INCOMPLETE, /** More data remaining to write. */\n    TRANSMIT_SOFT_ERROR, /** Can't write any more right now. */\n    TRANSMIT_HARD_ERROR  /** Can't write (c->state is set to conn_closing) */\n};\n\n/* Default methods to read from/ write to a socket */\nssize_t tcp_read(conn *c, void *buf, size_t count) {\n    assert (c != NULL);\n    return read(c->sfd, buf, count);\n}\n\nssize_t tcp_sendmsg(conn *c, struct msghdr *msg, int flags) {\n    assert (c != NULL);\n    return sendmsg(c->sfd, msg, flags);\n}\n\nssize_t tcp_write(conn *c, void *buf, size_t count) {\n    assert (c != NULL);\n    return write(c->sfd, buf, count);\n}\n\nstatic enum transmit_result transmit(conn *c);\n\n/* This reduces the latency without adding lots of extra wiring to be able to\n * notify the listener thread of when to listen again.\n * Also, the clock timer could be broken out into its own thread and we\n * can block the listener via a condition.\n */\nstatic volatile bool allow_new_conns = true;\nstatic int stop_main_loop = NOT_STOP;\nstatic struct event maxconnsevent;\nstatic void maxconns_handler(const evutil_socket_t fd, const short which, void *arg) {\n    struct timeval t = {.tv_sec = 0, .tv_usec = 10000};\n\n    if (fd == -42 || allow_new_conns == false) {\n        /* reschedule in 10ms if we need to keep polling */\n        evtimer_set(&maxconnsevent, maxconns_handler, 0);\n        event_base_set(main_base, &maxconnsevent);\n        evtimer_add(&maxconnsevent, &t);\n    } else {\n        evtimer_del(&maxconnsevent);\n        accept_new_conns(true);\n    }\n}\n\n/*\n * given time value that's either unix time or delta from current unix time, return\n * unix time. Use the fact that delta can't exceed one month (and real time value can't\n * be that low).\n */\nrel_time_t realtime(const time_t exptime) {\n    /* no. of seconds in 30 days - largest possible delta exptime */\n\n    if (exptime == 0) return 0; /* 0 means never expire */\n\n    if (exptime > REALTIME_MAXDELTA) {\n        /* if item expiration is at/before the server started, give it an\n           expiration time of 1 second after the server started.\n           (because 0 means don't expire).  without this, we'd\n           underflow and wrap around to some large value way in the\n           future, effectively making items expiring in the past\n           really expiring never */\n        if (exptime <= process_started)\n            return (rel_time_t)1;\n        return (rel_time_t)(exptime - process_started);\n    } else {\n        return (rel_time_t)(exptime + current_time);\n    }\n}\n\nstatic void stats_init(void) {\n    memset(&stats, 0, sizeof(struct stats));\n    memset(&stats_state, 0, sizeof(struct stats_state));\n    stats_state.accepting_conns = true; /* assuming we start in this state. */\n\n    /* make the time we started always be 2 seconds before we really\n       did, so time(0) - time.started is never zero.  if so, things\n       like 'settings.oldest_live' which act as booleans as well as\n       values are now false in boolean context... */\n    process_started = time(0) - ITEM_UPDATE_INTERVAL - 2;\n    stats_prefix_init(settings.prefix_delimiter);\n}\n\nvoid stats_reset(void) {\n    STATS_LOCK();\n    memset(&stats, 0, sizeof(struct stats));\n    stats_prefix_clear();\n    STATS_UNLOCK();\n    threadlocal_stats_reset();\n    item_stats_reset();\n}\n\nstatic void settings_init(void) {\n    settings.use_cas = true;\n    settings.access = 0700;\n    settings.port = 11211;\n    settings.udpport = 0;\n    ssl_init_settings();\n    /* By default this string should be NULL for getaddrinfo() */\n    settings.inter = NULL;\n    settings.maxbytes = 64 * 1024 * 1024; /* default is 64MB */\n    settings.maxconns = 1024;         /* to limit connections-related memory to about 5MB */\n    settings.verbose = 0;\n    settings.oldest_live = 0;\n    settings.evict_to_free = 1;       /* push old items out of cache when memory runs out */\n    settings.socketpath = NULL;       /* by default, not using a unix socket */\n    settings.auth_file = NULL;        /* by default, not using ASCII authentication tokens */\n    settings.factor = 1.25;\n    settings.chunk_size = 48;         /* space for a modest key and value */\n    settings.num_threads = 4;         /* N workers */\n    settings.num_threads_per_udp = 0;\n    settings.prefix_delimiter = ':';\n    settings.detail_enabled = 0;\n    settings.reqs_per_event = 20;\n    settings.backlog = 1024;\n    settings.binding_protocol = negotiating_prot;\n    settings.item_size_max = 1024 * 1024; /* The famous 1MB upper limit. */\n    settings.slab_page_size = 1024 * 1024; /* chunks are split from 1MB pages. */\n    settings.slab_chunk_size_max = settings.slab_page_size / 2;\n    settings.sasl = false;\n    settings.maxconns_fast = true;\n    settings.lru_crawler = false;\n    settings.lru_crawler_sleep = 100;\n    settings.lru_crawler_tocrawl = 0;\n    settings.lru_maintainer_thread = false;\n    settings.lru_segmented = true;\n    settings.hot_lru_pct = 20;\n    settings.warm_lru_pct = 40;\n    settings.hot_max_factor = 0.2;\n    settings.warm_max_factor = 2.0;\n    settings.temp_lru = false;\n    settings.temporary_ttl = 61;\n    settings.idle_timeout = 0; /* disabled */\n    settings.hashpower_init = 0;\n    settings.slab_reassign = true;\n    settings.slab_automove = 1;\n    settings.slab_automove_version = 0;\n    settings.slab_automove_ratio = 0.8;\n    settings.slab_automove_window = 10;\n    settings.shutdown_command = false;\n    settings.tail_repair_time = TAIL_REPAIR_TIME_DEFAULT;\n    settings.flush_enabled = true;\n    settings.dump_enabled = true;\n    settings.crawls_persleep = 1000;\n    settings.logger_watcher_buf_size = LOGGER_WATCHER_BUF_SIZE;\n    settings.logger_buf_size = LOGGER_BUF_SIZE;\n    settings.drop_privileges = false;\n    settings.watch_enabled = true;\n    settings.read_buf_mem_limit = 0;\n#ifdef MEMCACHED_DEBUG\n    settings.relaxed_privileges = false;\n#endif\n    settings.num_napi_ids = 0;\n    settings.memory_file = NULL;\n#ifdef SOCK_COOKIE_ID\n    settings.sock_cookie_id = 0;\n#endif\n}\n\nextern pthread_mutex_t conn_lock;\n\n/* Connection timeout thread bits */\nstatic pthread_t conn_timeout_tid;\nstatic int do_run_conn_timeout_thread;\nstatic pthread_cond_t conn_timeout_cond = PTHREAD_COND_INITIALIZER;\nstatic pthread_mutex_t conn_timeout_lock = PTHREAD_MUTEX_INITIALIZER;\n\n#define CONNS_PER_SLICE 100\nstatic void *conn_timeout_thread(void *arg) {\n    int i;\n    conn *c;\n    rel_time_t oldest_last_cmd;\n    int sleep_time;\n    int sleep_slice = max_fds / CONNS_PER_SLICE;\n    if (sleep_slice == 0)\n        sleep_slice = CONNS_PER_SLICE;\n\n    useconds_t timeslice = 1000000 / sleep_slice;\n\n    mutex_lock(&conn_timeout_lock);\n    while(do_run_conn_timeout_thread) {\n        if (settings.verbose > 2)\n            fprintf(stderr, \"idle timeout thread at top of connection list\\n\");\n\n        oldest_last_cmd = current_time;\n\n        for (i = 0; i < max_fds; i++) {\n            if ((i % CONNS_PER_SLICE) == 0) {\n                if (settings.verbose > 2)\n                    fprintf(stderr, \"idle timeout thread sleeping for %ulus\\n\",\n                        (unsigned int)timeslice);\n                usleep(timeslice);\n            }\n\n            if (!conns[i])\n                continue;\n\n            c = conns[i];\n\n            if (!IS_TCP(c->transport))\n                continue;\n\n            if (c->state != conn_new_cmd && c->state != conn_read)\n                continue;\n\n            if ((current_time - c->last_cmd_time) > settings.idle_timeout) {\n                timeout_conn(c);\n            } else {\n                if (c->last_cmd_time < oldest_last_cmd)\n                    oldest_last_cmd = c->last_cmd_time;\n            }\n        }\n\n        /* This is the soonest we could have another connection time out */\n        sleep_time = settings.idle_timeout - (current_time - oldest_last_cmd) + 1;\n        if (sleep_time <= 0)\n            sleep_time = 1;\n\n        if (settings.verbose > 2)\n            fprintf(stderr,\n                    \"idle timeout thread finished pass, sleeping for %ds\\n\",\n                    sleep_time);\n\n        struct timeval now;\n        struct timespec to_sleep;\n        gettimeofday(&now, NULL);\n        to_sleep.tv_sec = now.tv_sec + sleep_time;\n        to_sleep.tv_nsec = 0;\n\n        pthread_cond_timedwait(&conn_timeout_cond, &conn_timeout_lock, &to_sleep);\n    }\n\n    mutex_unlock(&conn_timeout_lock);\n    return NULL;\n}\n\nstatic int start_conn_timeout_thread(void) {\n    int ret;\n\n    if (settings.idle_timeout == 0)\n        return -1;\n\n    do_run_conn_timeout_thread = 1;\n    if ((ret = pthread_create(&conn_timeout_tid, NULL,\n        conn_timeout_thread, NULL)) != 0) {\n        fprintf(stderr, \"Can't create idle connection timeout thread: %s\\n\",\n            strerror(ret));\n        return -1;\n    }\n    thread_setname(conn_timeout_tid, \"mc-idletimeout\");\n\n    return 0;\n}\n\nint stop_conn_timeout_thread(void) {\n    if (!do_run_conn_timeout_thread)\n        return -1;\n    mutex_lock(&conn_timeout_lock);\n    do_run_conn_timeout_thread = 0;\n    pthread_cond_signal(&conn_timeout_cond);\n    mutex_unlock(&conn_timeout_lock);\n    pthread_join(conn_timeout_tid, NULL);\n    return 0;\n}\n\n/*\n * read buffer cache helper functions\n */\nstatic void rbuf_release(conn *c) {\n    if (c->rbuf != NULL && c->rbytes == 0 && !IS_UDP(c->transport)) {\n        if (c->rbuf_malloced) {\n            free(c->rbuf);\n            c->rbuf_malloced = false;\n        } else {\n            do_cache_free(c->thread->rbuf_cache, c->rbuf);\n        }\n        c->rsize = 0;\n        c->rbuf = NULL;\n        c->rcurr = NULL;\n    }\n}\n\nstatic bool rbuf_alloc(conn *c) {\n    if (c->rbuf == NULL) {\n        c->rbuf = do_cache_alloc(c->thread->rbuf_cache);\n        if (!c->rbuf) {\n            THR_STATS_LOCK(c->thread);\n            c->thread->stats.read_buf_oom++;\n            THR_STATS_UNLOCK(c->thread);\n            return false;\n        }\n        c->rsize = READ_BUFFER_SIZE;\n        c->rcurr = c->rbuf;\n    }\n    return true;\n}\n\n// Just for handling huge ASCII multigets.\n// The previous system was essentially the same; realloc'ing until big enough,\n// then realloc'ing back down after the request finished.\nbool rbuf_switch_to_malloc(conn *c) {\n    // Might as well start with x2 and work from there.\n    size_t size = c->rsize * 2;\n    char *tmp = malloc(size);\n    if (!tmp)\n        return false;\n\n    memcpy(tmp, c->rcurr, c->rbytes);\n    do_cache_free(c->thread->rbuf_cache, c->rbuf);\n\n    c->rcurr = c->rbuf = tmp;\n    c->rsize = size;\n    c->rbuf_malloced = true;\n    return true;\n}\n\n/*\n * Initializes the connections array. We don't actually allocate connection\n * structures until they're needed, so as to avoid wasting memory when the\n * maximum connection count is much higher than the actual number of\n * connections.\n *\n * This does end up wasting a few pointers' worth of memory for FDs that are\n * used for things other than connections, but that's worth it in exchange for\n * being able to directly index the conns array by FD.\n */\nstatic void conn_init(void) {\n    /* We're unlikely to see an FD much higher than maxconns. */\n    int next_fd = dup(1);\n    if (next_fd < 0) {\n        perror(\"Failed to duplicate file descriptor\\n\");\n        exit(1);\n    }\n    int headroom = 10;      /* account for extra unexpected open FDs */\n    struct rlimit rl;\n\n    max_fds = settings.maxconns + headroom + next_fd;\n\n    /* But if possible, get the actual highest FD we can possibly ever see. */\n    if (getrlimit(RLIMIT_NOFILE, &rl) == 0) {\n        max_fds = rl.rlim_max;\n    } else {\n        fprintf(stderr, \"Failed to query maximum file descriptor; \"\n                        \"falling back to maxconns\\n\");\n    }\n\n    close(next_fd);\n\n    if ((conns = calloc(max_fds, sizeof(conn *))) == NULL) {\n        fprintf(stderr, \"Failed to allocate connection structures\\n\");\n        /* This is unrecoverable so bail out early. */\n        exit(1);\n    }\n}\n\nstatic const char *prot_text(enum protocol prot) {\n    char *rv = \"unknown\";\n    switch(prot) {\n        case ascii_prot:\n            rv = \"ascii\";\n            break;\n        case binary_prot:\n            rv = \"binary\";\n            break;\n        case negotiating_prot:\n            rv = \"auto-negotiate\";\n            break;\n#ifdef PROXY\n        case proxy_prot:\n            rv = \"proxy\";\n            break;\n#endif\n    }\n    return rv;\n}\n\nvoid conn_close_idle(conn *c) {\n    if (settings.idle_timeout > 0 &&\n        (current_time - c->last_cmd_time) > settings.idle_timeout) {\n        if (c->state != conn_new_cmd && c->state != conn_read) {\n            if (settings.verbose > 1)\n                fprintf(stderr,\n                    \"fd %d wants to timeout, but isn't in read state\", c->sfd);\n            return;\n        }\n\n        if (settings.verbose > 1)\n            fprintf(stderr, \"Closing idle fd %d\\n\", c->sfd);\n\n        pthread_mutex_lock(&c->thread->stats.mutex);\n        c->thread->stats.idle_kicks++;\n        pthread_mutex_unlock(&c->thread->stats.mutex);\n\n        c->close_reason = IDLE_TIMEOUT_CLOSE;\n\n        conn_set_state(c, conn_closing);\n        drive_machine(c);\n    }\n}\n\nstatic void _conn_event_readd(conn *c) {\n    c->ev_flags = EV_READ | EV_PERSIST;\n    event_set(&c->event, c->sfd, c->ev_flags, event_handler, (void *)c);\n    event_base_set(c->thread->base, &c->event);\n\n    // TODO: call conn_cleanup/fail/etc\n    if (event_add(&c->event, 0) == -1) {\n        perror(\"event_add\");\n    }\n}\n\n/* bring conn back from a sidethread. could have had its event base moved. */\nvoid conn_worker_readd(conn *c) {\n    if (c->io_queues_submitted) { // TODO: ensure this is safe?\n        c->io_queues_submitted--;\n        // If we're still waiting for other queues to return, don't re-add the\n        // connection yet.\n        if (c->io_queues_submitted != 0) {\n            return;\n        }\n    }\n\n    switch (c->state) {\n        case conn_closing:\n            drive_machine(c);\n            break;\n        case conn_io_pending:\n            // The event listener was removed as more data showed up while\n            // waiting for the async response.\n            _conn_event_readd(c);\n            // Explicit fall-through.\n        case conn_io_queue:\n            conn_set_state(c, conn_io_resume);\n            // machine will know how to return based on secondary state.\n            drive_machine(c);\n            break;\n        default:\n            event_del(&c->event);\n            _conn_event_readd(c);\n            conn_set_state(c, conn_new_cmd);\n    }\n\n}\n\nvoid thread_io_queue_add(LIBEVENT_THREAD *t, int type, void *ctx, io_queue_stack_cb cb) {\n    io_queue_cb_t *q = t->io_queues;\n    while (q->type != IO_QUEUE_NONE) {\n        q++;\n    }\n    q->type = type;\n    q->ctx = ctx;\n    q->submit_cb = cb;\n    return;\n}\n\nvoid conn_io_queue_setup(conn *c) {\n    io_queue_cb_t *qcb = c->thread->io_queues;\n    io_queue_t *q = c->io_queues;\n    while (qcb->type != IO_QUEUE_NONE) {\n        q->type = qcb->type;\n        q->ctx = qcb->ctx;\n        q->stack_ctx = NULL;\n        q->count = 0;\n        qcb++;\n        q++;\n    }\n}\n\n// To be called from conn_release_items to ensure the stack ptrs are reset.\nstatic void conn_io_queue_reset(conn *c) {\n    for (io_queue_t *q = c->io_queues; q->type != IO_QUEUE_NONE; q++) {\n        assert(q->count == 0);\n        q->stack_ctx = NULL;\n    }\n}\n\nio_queue_cb_t *thread_io_queue_get(LIBEVENT_THREAD *t, int type) {\n    io_queue_cb_t *q = t->io_queues;\n    while (q->type != IO_QUEUE_NONE) {\n        if (q->type == type) {\n            return q;\n        }\n        q++;\n    }\n    return NULL;\n}\n\nio_queue_t *conn_io_queue_get(conn *c, int type) {\n    io_queue_t *q = c->io_queues;\n    while (q->type != IO_QUEUE_NONE) {\n        if (q->type == type) {\n            return q;\n        }\n        q++;\n    }\n    return NULL;\n}\n\n// called to return a single IO object to the original worker thread.\nvoid conn_io_queue_return(io_pending_t *io) {\n    io->return_cb(io);\n}\n\nconn *conn_new(const int sfd, enum conn_states init_state,\n                const int event_flags,\n                const int read_buffer_size, enum network_transport transport,\n                struct event_base *base, void *ssl, uint64_t conntag,\n                enum protocol bproto) {\n    conn *c;\n\n    assert(sfd >= 0 && sfd < max_fds);\n    c = conns[sfd];\n\n    if (NULL == c) {\n        if (!(c = (conn *)calloc(1, sizeof(conn)))) {\n            STATS_LOCK();\n            stats.malloc_fails++;\n            STATS_UNLOCK();\n            fprintf(stderr, \"Failed to allocate connection object\\n\");\n            return NULL;\n        }\n        MEMCACHED_CONN_CREATE(c);\n        c->read = NULL;\n        c->sendmsg = NULL;\n        c->write = NULL;\n        c->rbuf = NULL;\n\n        c->rsize = read_buffer_size;\n\n        // UDP connections use a persistent static buffer.\n        if (c->rsize) {\n            c->rbuf = (char *)malloc((size_t)c->rsize);\n        }\n\n        if (c->rsize && c->rbuf == NULL) {\n            conn_free(c);\n            STATS_LOCK();\n            stats.malloc_fails++;\n            STATS_UNLOCK();\n            fprintf(stderr, \"Failed to allocate buffers for connection\\n\");\n            return NULL;\n        }\n\n\n        STATS_LOCK();\n        stats_state.conn_structs++;\n        STATS_UNLOCK();\n\n        c->sfd = sfd;\n        conns[sfd] = c;\n    }\n\n    c->transport = transport;\n    c->protocol = bproto;\n    c->tag = conntag;\n\n    /* unix socket mode doesn't need this, so zeroed out.  but why\n     * is this done for every command?  presumably for UDP\n     * mode.  */\n    if (!settings.socketpath) {\n        c->request_addr_size = sizeof(c->request_addr);\n    } else {\n        c->request_addr_size = 0;\n    }\n\n    if (transport == tcp_transport && init_state == conn_new_cmd) {\n        if (getpeername(sfd, (struct sockaddr *) &c->request_addr,\n                        &c->request_addr_size)) {\n            perror(\"getpeername\");\n            memset(&c->request_addr, 0, sizeof(c->request_addr));\n        }\n    }\n\n    if (init_state == conn_new_cmd) {\n        LOGGER_LOG(NULL, LOG_CONNEVENTS, LOGGER_CONNECTION_NEW, NULL,\n                &c->request_addr, c->request_addr_size, c->transport, 0, sfd);\n    }\n\n    if (settings.verbose > 1) {\n        if (init_state == conn_listening) {\n            fprintf(stderr, \"<%d server listening (%s)\\n\", sfd,\n                prot_text(c->protocol));\n        } else if (IS_UDP(transport)) {\n            fprintf(stderr, \"<%d server listening (udp)\\n\", sfd);\n        } else if (c->protocol == negotiating_prot) {\n            fprintf(stderr, \"<%d new auto-negotiating client connection\\n\",\n                    sfd);\n        } else if (c->protocol == ascii_prot) {\n            fprintf(stderr, \"<%d new ascii client connection.\\n\", sfd);\n        } else if (c->protocol == binary_prot) {\n            fprintf(stderr, \"<%d new binary client connection.\\n\", sfd);\n#ifdef PROXY\n        } else if (c->protocol == proxy_prot) {\n            fprintf(stderr, \"<%d new proxy client connection.\\n\", sfd);\n#endif\n        } else {\n            fprintf(stderr, \"<%d new unknown (%d) client connection\\n\",\n                sfd, c->protocol);\n            assert(false);\n        }\n    }\n\n    c->state = init_state;\n    c->rlbytes = 0;\n    c->cmd = -1;\n    c->rbytes = 0;\n    c->rcurr = c->rbuf;\n    c->ritem = 0;\n    c->rbuf_malloced = false;\n    c->item_malloced = false;\n    c->sasl_started = false;\n    c->set_stale = false;\n    c->mset_res = false;\n    c->close_after_write = false;\n    c->last_cmd_time = current_time; /* initialize for idle kicker */\n    // wipe all queues.\n    memset(c->io_queues, 0, sizeof(c->io_queues));\n    c->io_queues_submitted = 0;\n\n    c->item = 0;\n    c->ssl = NULL;\n#ifdef TLS\n    c->ssl_wbuf = NULL;\n#endif\n\n    c->noreply = false;\n\n    if (ssl) {\n        // musn't get here without ssl enabled.\n        assert(settings.ssl_enabled);\n        ssl_init_conn(c, ssl);\n        c->ssl_enabled = true;\n    } else {\n        c->read = tcp_read;\n        c->sendmsg = tcp_sendmsg;\n        c->write = tcp_write;\n        c->ssl_enabled = false;\n    }\n\n    if (IS_UDP(transport)) {\n        c->try_read_command = try_read_command_udp;\n    } else {\n        switch (c->protocol) {\n            case ascii_prot:\n                if (settings.auth_file == NULL) {\n                    c->authenticated = true;\n                    c->try_read_command = try_read_command_ascii;\n                } else {\n                    c->authenticated = false;\n                    c->try_read_command = try_read_command_asciiauth;\n                }\n                break;\n            case binary_prot:\n                // binprot handles its own authentication via SASL parsing.\n                c->authenticated = false;\n                c->try_read_command = try_read_command_binary;\n                break;\n            case negotiating_prot:\n                c->try_read_command = try_read_command_negotiate;\n                break;\n#ifdef PROXY\n            case proxy_prot:\n                c->try_read_command = try_read_command_proxy;\n                break;\n#endif\n        }\n    }\n\n    event_set(&c->event, sfd, event_flags, event_handler, (void *)c);\n    event_base_set(base, &c->event);\n    c->ev_flags = event_flags;\n\n    if (event_add(&c->event, 0) == -1) {\n        perror(\"event_add\");\n        return NULL;\n    }\n\n    STATS_LOCK();\n    stats_state.curr_conns++;\n    stats.total_conns++;\n    STATS_UNLOCK();\n\n    MEMCACHED_CONN_ALLOCATE(c->sfd);\n\n    return c;\n}\n\nvoid conn_release_items(conn *c) {\n    assert(c != NULL);\n\n    if (c->item) {\n        if (c->item_malloced) {\n            free(c->item);\n            c->item_malloced = false;\n        } else {\n            item_remove(c->item);\n        }\n        c->item = 0;\n    }\n\n    // Cull any unsent responses.\n    if (c->resp_head) {\n        mc_resp *resp = c->resp_head;\n        // r_f() handles the chain maintenance.\n        while (resp) {\n            // temporary by default. hide behind a debug flag in the future:\n            // double free detection. Transmit loops can drop out early, but\n            // here we could infinite loop.\n            if (resp->free) {\n                fprintf(stderr, \"ERROR: double free detected during conn_release_items(): [%d] [%s]\\n\",\n                        c->sfd, c->protocol == binary_prot ? \"binary\" : \"ascii\");\n                // Since this is a critical failure, just leak the memory.\n                // If these errors are seen, an abort() can be used instead.\n                c->resp_head = NULL;\n                c->resp = NULL;\n                break;\n            }\n            resp = resp_finish(c, resp);\n        }\n        conn_io_queue_reset(c);\n    }\n}\n\nstatic void conn_cleanup(conn *c) {\n    assert(c != NULL);\n\n    conn_release_items(c);\n#ifdef PROXY\n    if (c->proxy_rctx) {\n        proxy_cleanup_conn(c);\n    }\n#endif\n    if (c->sasl_conn) {\n        assert(settings.sasl);\n        sasl_dispose(&c->sasl_conn);\n        c->sasl_conn = NULL;\n    }\n\n    if (IS_UDP(c->transport)) {\n        conn_set_state(c, conn_read);\n    }\n}\n\n/*\n * Frees a connection.\n */\nvoid conn_free(conn *c) {\n    if (c) {\n        assert(c != NULL);\n        assert(c->sfd >= 0 && c->sfd < max_fds);\n\n        MEMCACHED_CONN_DESTROY(c);\n        conns[c->sfd] = NULL;\n        if (c->rbuf)\n            free(c->rbuf);\n#ifdef TLS\n        if (c->ssl_wbuf)\n            c->ssl_wbuf = NULL;\n#endif\n\n        free(c);\n    }\n}\n\nstatic void conn_close(conn *c) {\n    assert(c != NULL);\n\n    if (c->thread) {\n        LOGGER_LOG(c->thread->l, LOG_CONNEVENTS, LOGGER_CONNECTION_CLOSE, NULL,\n                &c->request_addr, c->request_addr_size, c->transport,\n                c->close_reason, c->sfd);\n    }\n\n    /* delete the event, the socket and the conn */\n    event_del(&c->event);\n\n    if (settings.verbose > 1)\n        fprintf(stderr, \"<%d connection closed.\\n\", c->sfd);\n\n    conn_cleanup(c);\n\n    // force release of read buffer.\n    if (c->thread) {\n        c->rbytes = 0;\n        rbuf_release(c);\n    }\n\n    MEMCACHED_CONN_RELEASE(c->sfd);\n    conn_set_state(c, conn_closed);\n    if (c->ssl_enabled) {\n        ssl_conn_close(c->ssl);\n    }\n    close(c->sfd);\n    c->close_reason = 0;\n    pthread_mutex_lock(&conn_lock);\n    allow_new_conns = true;\n    pthread_mutex_unlock(&conn_lock);\n\n    STATS_LOCK();\n    stats_state.curr_conns--;\n    STATS_UNLOCK();\n\n    return;\n}\n\n// Since some connections might be off on side threads and some are managed as\n// listeners we need to walk through them all from a central point.\n// Must be called with all worker threads hung or in the process of closing.\nvoid conn_close_all(void) {\n    int i;\n    for (i = 0; i < max_fds; i++) {\n        if (conns[i] && conns[i]->state != conn_closed) {\n            conn_close(conns[i]);\n        }\n    }\n}\n\n/**\n * Convert a state name to a human readable form.\n */\nstatic const char *state_text(enum conn_states state) {\n    const char* const statenames[] = { \"conn_listening\",\n                                       \"conn_new_cmd\",\n                                       \"conn_waiting\",\n                                       \"conn_read\",\n                                       \"conn_parse_cmd\",\n                                       \"conn_write\",\n                                       \"conn_nread\",\n                                       \"conn_swallow\",\n                                       \"conn_closing\",\n                                       \"conn_mwrite\",\n                                       \"conn_closed\",\n                                       \"conn_watch\",\n                                       \"conn_io_queue\",\n                                       \"conn_io_resume\",\n                                       \"conn_io_pending\" };\n    return statenames[state];\n}\n\n/*\n * Sets a connection's current state in the state machine. Any special\n * processing that needs to happen on certain state transitions can\n * happen here.\n */\nvoid conn_set_state(conn *c, enum conn_states state) {\n    assert(c != NULL);\n    assert(state >= conn_listening && state < conn_max_state);\n\n    if (state != c->state) {\n        if (settings.verbose > 2) {\n            fprintf(stderr, \"%d: going from %s to %s\\n\",\n                    c->sfd, state_text(c->state),\n                    state_text(state));\n        }\n\n        if (state == conn_write || state == conn_mwrite) {\n            MEMCACHED_PROCESS_COMMAND_END(c->sfd, c->resp->wbuf, c->resp->wbytes);\n        }\n        c->state = state;\n    }\n}\n\n/*\n * response object helper functions\n */\nvoid resp_reset(mc_resp *resp) {\n    if (resp->item) {\n        item_remove(resp->item);\n        resp->item = NULL;\n    }\n    if (resp->write_and_free) {\n#ifdef PROXY\n        if (resp->proxy_res) {\n            LIBEVENT_THREAD *t = resp->bundle->thread;\n            pthread_mutex_lock(&t->proxy_limit_lock);\n            t->proxy_buffer_memory_used -= resp->wbytes;\n            pthread_mutex_unlock(&t->proxy_limit_lock);\n        }\n#endif\n        free(resp->write_and_free);\n        resp->write_and_free = NULL;\n    }\n    resp->wbytes = 0;\n    resp->tosend = 0;\n    resp->iovcnt = 0;\n    resp->chunked_data_iov = 0;\n    resp->chunked_total = 0;\n    resp->skip = false;\n}\n\nvoid resp_add_iov(mc_resp *resp, const void *buf, int len) {\n    assert(resp->iovcnt < MC_RESP_IOVCOUNT);\n    int x = resp->iovcnt;\n    resp->iov[x].iov_base = (void *)buf;\n    resp->iov[x].iov_len = len;\n    resp->iovcnt++;\n    resp->tosend += len;\n}\n\n// Notes that an IOV should be handled as a chunked item header.\n// TODO: I'm hoping this isn't a permanent abstraction while I learn what the\n// API should be.\nvoid resp_add_chunked_iov(mc_resp *resp, const void *buf, int len) {\n    resp->chunked_data_iov = resp->iovcnt;\n    resp->chunked_total = len;\n    resp_add_iov(resp, buf, len);\n}\n\n// resp_allocate and resp_free are a wrapper around read buffers which makes\n// read buffers the only network memory to track.\n// Normally this would be too excessive. In this case it allows end users to\n// track a single memory limit for ephemeral connection buffers.\n// Fancy bit twiddling tricks are avoided to help keep this straightforward.\nstatic mc_resp* resp_allocate(conn *c) {\n    LIBEVENT_THREAD *th = c->thread;\n    mc_resp *resp = NULL;\n    mc_resp_bundle *b = th->open_bundle;\n\n    if (b != NULL) {\n        for (int i = 0; i < MAX_RESP_PER_BUNDLE; i++) {\n            // loop around starting from the most likely to be free\n            int x = (i + b->next_check) % MAX_RESP_PER_BUNDLE;\n            if (b->r[x].free) {\n                resp = &b->r[x];\n                b->next_check = x+1;\n                break;\n            }\n        }\n\n        if (resp != NULL) {\n            b->refcount++;\n            memset(resp, 0, sizeof(*resp));\n            resp->free = false; // redundant, for clarity.\n            resp->bundle = b;\n            if (b->refcount == MAX_RESP_PER_BUNDLE) {\n                assert(b->prev == NULL);\n                // We only allocate off the head. Assign new head.\n                th->open_bundle = b->next;\n                // Remove ourselves from the list.\n                if (b->next) {\n                    b->next->prev = 0;\n                    b->next = 0;\n                }\n            }\n        }\n    }\n\n    if (resp == NULL) {\n        assert(th->open_bundle == NULL);\n        b = do_cache_alloc(th->rbuf_cache);\n        if (b) {\n            THR_STATS_LOCK(th);\n            th->stats.response_obj_bytes += READ_BUFFER_SIZE;\n            THR_STATS_UNLOCK(th);\n            b->next_check = 1;\n            b->refcount = 1;\n            for (int i = 0; i < MAX_RESP_PER_BUNDLE; i++) {\n                b->r[i].free = true;\n            }\n            b->next = 0;\n            b->prev = 0;\n            b->thread = th;\n            th->open_bundle = b;\n            resp = &b->r[0];\n            memset(resp, 0, sizeof(*resp));\n            resp->free = false; // redundant. for clarity.\n            resp->bundle = b;\n        } else {\n            return NULL;\n        }\n    }\n\n    return resp;\n}\n\nvoid resp_free(LIBEVENT_THREAD *th, mc_resp *resp) {\n    mc_resp_bundle *b = resp->bundle;\n\n    resp->free = true;\n    b->refcount--;\n    if (b->refcount == 0) {\n        if (b == th->open_bundle && b->next == 0) {\n            // This is the final bundle. Just hold and reuse to skip init loop\n            assert(b->prev == 0);\n            b->next_check = 0;\n        } else {\n            // Assert that we're either in the list or at the head.\n            assert((b->next || b->prev) || b == th->open_bundle);\n\n            // unlink from list.\n            mc_resp_bundle **head = &th->open_bundle;\n            if (*head == b) *head = b->next;\n            // Not tracking the tail.\n            assert(b->next != b && b->prev != b);\n\n            if (b->next) b->next->prev = b->prev;\n            if (b->prev) b->prev->next = b->next;\n\n            // Now completely done with this buffer.\n            do_cache_free(th->rbuf_cache, b);\n            THR_STATS_LOCK(th);\n            th->stats.response_obj_bytes -= READ_BUFFER_SIZE;\n            THR_STATS_UNLOCK(th);\n        }\n    } else {\n        mc_resp_bundle **head = &th->open_bundle;\n        // NOTE: since we're not tracking tail, latest free ends up in head.\n        if (b == th->open_bundle || (b->prev || b->next)) {\n            // If we're already linked, leave it in place to save CPU.\n        } else {\n            // Non-zero refcount, need to link into the freelist.\n            b->prev = 0;\n            b->next = *head;\n            if (b->next) b->next->prev = b;\n            *head = b;\n        }\n\n    }\n    THR_STATS_LOCK(th);\n    th->stats.response_obj_count--;\n    THR_STATS_UNLOCK(th);\n}\n\nbool resp_start(conn *c) {\n    mc_resp *resp = resp_allocate(c);\n    if (!resp) {\n        THR_STATS_LOCK(c->thread);\n        c->thread->stats.response_obj_oom++;\n        THR_STATS_UNLOCK(c->thread);\n        return false;\n    }\n\n    // handling the stats counters here to simplify testing\n    THR_STATS_LOCK(c->thread);\n    c->thread->stats.response_obj_count++;\n    THR_STATS_UNLOCK(c->thread);\n\n    if (!c->resp_head) {\n        c->resp_head = resp;\n    }\n    if (!c->resp) {\n        c->resp = resp;\n    } else {\n        c->resp->next = resp;\n        c->resp = resp;\n    }\n    if (IS_UDP(c->transport)) {\n        // need to hold on to some data for async responses.\n        c->resp->request_id = c->request_id;\n        c->resp->request_addr = c->request_addr;\n        c->resp->request_addr_size = c->request_addr_size;\n    }\n    return true;\n}\n\nmc_resp *resp_start_unlinked(conn *c) {\n    mc_resp *resp = resp_allocate(c);\n    if (!resp) {\n        THR_STATS_LOCK(c->thread);\n        c->thread->stats.response_obj_oom++;\n        THR_STATS_UNLOCK(c->thread);\n        return false;\n    }\n\n    // handling the stats counters here to simplify testing\n    THR_STATS_LOCK(c->thread);\n    c->thread->stats.response_obj_count++;\n    THR_STATS_UNLOCK(c->thread);\n\n    if (IS_UDP(c->transport)) {\n        // need to hold on to some data for async responses.\n        c->resp->request_id = c->request_id;\n        c->resp->request_addr = c->request_addr;\n        c->resp->request_addr_size = c->request_addr_size;\n    }\n\n    return resp;\n}\n\n// returns next response in chain.\nmc_resp* resp_finish(conn *c, mc_resp *resp) {\n    mc_resp *next = resp->next;\n    if (resp->item) {\n        // TODO: cache hash value in resp obj?\n        item_remove(resp->item);\n        resp->item = NULL;\n    }\n    if (resp->write_and_free) {\n#ifdef PROXY\n        if (resp->proxy_res) {\n            LIBEVENT_THREAD *t = resp->bundle->thread;\n            pthread_mutex_lock(&t->proxy_limit_lock);\n            t->proxy_buffer_memory_used -= resp->wbytes;\n            pthread_mutex_unlock(&t->proxy_limit_lock);\n        }\n#endif\n        free(resp->write_and_free);\n    }\n    if (resp->io_pending) {\n        io_pending_t *io = resp->io_pending;\n        // If we had a pending IO, tell it to internally clean up then return\n        // the main object back to our thread cache.\n        io->finalize_cb(io);\n        do_cache_free(c->thread->io_cache, io);\n        resp->io_pending = NULL;\n    }\n    if (c->resp_head == resp) {\n        c->resp_head = next;\n    }\n    if (c->resp == resp) {\n        c->resp = NULL;\n    }\n    resp_free(c->thread, resp);\n    return next;\n}\n\n// tells if connection has a depth of response objects to process.\nbool resp_has_stack(conn *c) {\n    return c->resp_head->next != NULL ? true : false;\n}\n\nvoid out_string(conn *c, const char *str) {\n    size_t len;\n    assert(c != NULL);\n    mc_resp *resp = c->resp;\n\n    // if response was original filled with something, but we're now writing\n    // out an error or similar, have to reset the object first.\n    // TODO: since this is often redundant with allocation, how many callers\n    // are actually requiring it be reset? Can we fast test by just looking at\n    // tosend and reset if nonzero?\n    resp_reset(resp);\n\n    if (c->noreply) {\n        // TODO: just invalidate the response since nothing's been attempted\n        // to send yet?\n        resp->skip = true;\n        if (settings.verbose > 1)\n            fprintf(stderr, \">%d NOREPLY %s\\n\", c->sfd, str);\n        conn_set_state(c, conn_new_cmd);\n        return;\n    }\n\n    if (settings.verbose > 1)\n        fprintf(stderr, \">%d %s\\n\", c->sfd, str);\n\n    // Fill response object with static string.\n\n    len = strlen(str);\n    if ((len + 2) > WRITE_BUFFER_SIZE) {\n        /* ought to be always enough. just fail for simplicity */\n        str = \"SERVER_ERROR output line too long\";\n        len = strlen(str);\n    }\n\n    memcpy(resp->wbuf, str, len);\n    memcpy(resp->wbuf + len, \"\\r\\n\", 2);\n    resp_add_iov(resp, resp->wbuf, len + 2);\n\n    conn_set_state(c, conn_new_cmd);\n    return;\n}\n\n// For metaget-style ASCII commands. Ignores noreply, ensuring clients see\n// protocol level errors.\nvoid out_errstring(conn *c, const char *str) {\n    c->noreply = false;\n    out_string(c, str);\n}\n\n/*\n * Outputs a protocol-specific \"out of memory\" error. For ASCII clients,\n * this is equivalent to out_string().\n */\nvoid out_of_memory(conn *c, char *ascii_error) {\n    const static char error_prefix[] = \"SERVER_ERROR \";\n    const static int error_prefix_len = sizeof(error_prefix) - 1;\n\n    if (c->protocol == binary_prot) {\n        /* Strip off the generic error prefix; it's irrelevant in binary */\n        if (!strncmp(ascii_error, error_prefix, error_prefix_len)) {\n            ascii_error += error_prefix_len;\n        }\n        write_bin_error(c, PROTOCOL_BINARY_RESPONSE_ENOMEM, ascii_error, 0);\n    } else {\n        out_string(c, ascii_error);\n    }\n}\n\nstatic void append_bin_stats(const char *key, const uint16_t klen,\n                             const char *val, const uint32_t vlen,\n                             conn *c) {\n    char *buf = c->stats.buffer + c->stats.offset;\n    uint32_t bodylen = klen + vlen;\n    protocol_binary_response_header header = {\n        .response.magic = (uint8_t)PROTOCOL_BINARY_RES,\n        .response.opcode = PROTOCOL_BINARY_CMD_STAT,\n        .response.keylen = (uint16_t)htons(klen),\n        .response.datatype = (uint8_t)PROTOCOL_BINARY_RAW_BYTES,\n        .response.bodylen = htonl(bodylen),\n        .response.opaque = c->opaque\n    };\n\n    memcpy(buf, header.bytes, sizeof(header.response));\n    buf += sizeof(header.response);\n\n    if (klen > 0) {\n        memcpy(buf, key, klen);\n        buf += klen;\n\n        if (vlen > 0) {\n            memcpy(buf, val, vlen);\n        }\n    }\n\n    c->stats.offset += sizeof(header.response) + bodylen;\n}\n\nstatic void append_ascii_stats(const char *key, const uint16_t klen,\n                               const char *val, const uint32_t vlen,\n                               conn *c) {\n    char *pos = c->stats.buffer + c->stats.offset;\n    uint32_t nbytes = 0;\n    int remaining = c->stats.size - c->stats.offset;\n    int room = remaining - 1;\n\n    if (klen == 0 && vlen == 0) {\n        nbytes = snprintf(pos, room, \"END\\r\\n\");\n    } else if (vlen == 0) {\n        nbytes = snprintf(pos, room, \"STAT %s\\r\\n\", key);\n    } else {\n        nbytes = snprintf(pos, room, \"STAT %s %s\\r\\n\", key, val);\n    }\n\n    c->stats.offset += nbytes;\n}\n\nstatic bool grow_stats_buf(conn *c, size_t needed) {\n    size_t nsize = c->stats.size;\n    size_t available = nsize - c->stats.offset;\n    bool rv = true;\n\n    /* Special case: No buffer -- need to allocate fresh */\n    if (c->stats.buffer == NULL) {\n        nsize = 1024;\n        available = c->stats.size = c->stats.offset = 0;\n    }\n\n    while (needed > available) {\n        assert(nsize > 0);\n        nsize = nsize << 1;\n        available = nsize - c->stats.offset;\n    }\n\n    if (nsize != c->stats.size) {\n        char *ptr = realloc(c->stats.buffer, nsize);\n        if (ptr) {\n            c->stats.buffer = ptr;\n            c->stats.size = nsize;\n        } else {\n            STATS_LOCK();\n            stats.malloc_fails++;\n            STATS_UNLOCK();\n            rv = false;\n        }\n    }\n\n    return rv;\n}\n\nvoid append_stats(const char *key, const uint16_t klen,\n                  const char *val, const uint32_t vlen,\n                  const void *cookie)\n{\n    /* value without a key is invalid */\n    if (klen == 0 && vlen > 0) {\n        return;\n    }\n\n    conn *c = (conn*)cookie;\n\n    if (c->protocol == binary_prot) {\n        size_t needed = vlen + klen + sizeof(protocol_binary_response_header);\n        if (!grow_stats_buf(c, needed)) {\n            return;\n        }\n        append_bin_stats(key, klen, val, vlen, c);\n    } else {\n        size_t needed = vlen + klen + 10; // 10 == \"STAT = \\r\\n\"\n        if (!grow_stats_buf(c, needed)) {\n            return;\n        }\n        append_ascii_stats(key, klen, val, vlen, c);\n    }\n\n    assert(c->stats.offset <= c->stats.size);\n}\n\nstatic void reset_cmd_handler(conn *c) {\n    c->cmd = -1;\n    c->substate = bin_no_state;\n    if (c->item != NULL) {\n        // TODO: Any other way to get here?\n        // SASL auth was mistakenly using it. Nothing else should?\n        if (c->item_malloced) {\n            free(c->item);\n            c->item_malloced = false;\n        } else {\n            item_remove(c->item);\n        }\n        c->item = NULL;\n    }\n    if (c->rbytes > 0) {\n        conn_set_state(c, conn_parse_cmd);\n    } else if (c->resp_head) {\n        conn_set_state(c, conn_mwrite);\n    } else {\n        conn_set_state(c, conn_waiting);\n    }\n}\n\nstatic void complete_nread(conn *c) {\n    assert(c != NULL);\n#ifdef PROXY\n    assert(c->protocol == ascii_prot\n           || c->protocol == binary_prot\n           || c->protocol == proxy_prot);\n#else\n    assert(c->protocol == ascii_prot\n           || c->protocol == binary_prot);\n#endif\n    if (c->protocol == ascii_prot) {\n        complete_nread_ascii(c);\n    } else if (c->protocol == binary_prot) {\n        complete_nread_binary(c);\n#ifdef PROXY\n    } else if (c->protocol == proxy_prot) {\n        complete_nread_proxy(c);\n#endif\n    }\n}\n\n/* Destination must always be chunked */\n/* This should be part of item.c */\nstatic int _store_item_copy_chunks(item *d_it, item *s_it, const int len) {\n    item_chunk *dch = (item_chunk *) ITEM_schunk(d_it);\n    /* Advance dch until we find free space */\n    while (dch->size == dch->used) {\n        if (dch->next) {\n            dch = dch->next;\n        } else {\n            break;\n        }\n    }\n\n    if (s_it->it_flags & ITEM_CHUNKED) {\n        int remain = len;\n        item_chunk *sch = (item_chunk *) ITEM_schunk(s_it);\n        int copied = 0;\n        /* Fills dch's to capacity, not straight copy sch in case data is\n         * being added or removed (ie append/prepend)\n         */\n        while (sch && dch && remain) {\n            assert(dch->used <= dch->size);\n            int todo = (dch->size - dch->used < sch->used - copied)\n                ? dch->size - dch->used : sch->used - copied;\n            if (remain < todo)\n                todo = remain;\n            memcpy(dch->data + dch->used, sch->data + copied, todo);\n            dch->used += todo;\n            copied += todo;\n            remain -= todo;\n            assert(dch->used <= dch->size);\n            if (dch->size == dch->used) {\n                item_chunk *tch = do_item_alloc_chunk(dch, remain);\n                if (tch) {\n                    dch = tch;\n                } else {\n                    return -1;\n                }\n            }\n            assert(copied <= sch->used);\n            if (copied == sch->used) {\n                copied = 0;\n                sch = sch->next;\n            }\n        }\n        /* assert that the destination had enough space for the source */\n        assert(remain == 0);\n    } else {\n        int done = 0;\n        /* Fill dch's via a non-chunked item. */\n        while (len > done && dch) {\n            int todo = (dch->size - dch->used < len - done)\n                ? dch->size - dch->used : len - done;\n            //assert(dch->size - dch->used != 0);\n            memcpy(dch->data + dch->used, ITEM_data(s_it) + done, todo);\n            done += todo;\n            dch->used += todo;\n            assert(dch->used <= dch->size);\n            if (dch->size == dch->used) {\n                item_chunk *tch = do_item_alloc_chunk(dch, len - done);\n                if (tch) {\n                    dch = tch;\n                } else {\n                    return -1;\n                }\n            }\n        }\n        assert(len == done);\n    }\n    return 0;\n}\n\nstatic int _store_item_copy_data(int comm, item *old_it, item *new_it, item *add_it) {\n    if (comm == NREAD_APPEND || comm == NREAD_APPENDVIV) {\n        if (new_it->it_flags & ITEM_CHUNKED) {\n            if (_store_item_copy_chunks(new_it, old_it, old_it->nbytes - 2) == -1 ||\n                _store_item_copy_chunks(new_it, add_it, add_it->nbytes) == -1) {\n                return -1;\n            }\n        } else {\n            memcpy(ITEM_data(new_it), ITEM_data(old_it), old_it->nbytes);\n            memcpy(ITEM_data(new_it) + old_it->nbytes - 2 /* CRLF */, ITEM_data(add_it), add_it->nbytes);\n        }\n    } else {\n        /* NREAD_PREPEND */\n        if (new_it->it_flags & ITEM_CHUNKED) {\n            if (_store_item_copy_chunks(new_it, add_it, add_it->nbytes - 2) == -1 ||\n                _store_item_copy_chunks(new_it, old_it, old_it->nbytes) == -1) {\n                return -1;\n            }\n        } else {\n            memcpy(ITEM_data(new_it), ITEM_data(add_it), add_it->nbytes);\n            memcpy(ITEM_data(new_it) + add_it->nbytes - 2 /* CRLF */, ITEM_data(old_it), old_it->nbytes);\n        }\n    }\n    return 0;\n}\n\n/*\n * Stores an item in the cache according to the semantics of one of the set\n * commands. Protected by the item lock.\n *\n * Returns the state of storage.\n */\nenum store_item_type do_store_item(item *it, int comm, LIBEVENT_THREAD *t, const uint32_t hv, int *nbytes, uint64_t *cas, uint64_t cas_in, bool cas_stale) {\n    char *key = ITEM_key(it);\n    item *old_it = do_item_get(key, it->nkey, hv, t, DONT_UPDATE);\n    enum store_item_type stored = NOT_STORED;\n\n    enum cas_result { CAS_NONE, CAS_MATCH, CAS_BADVAL, CAS_STALE, CAS_MISS };\n\n    item *new_it = NULL;\n    client_flags_t flags;\n\n    /* Do the CAS test up front so we can apply to all store modes */\n    enum cas_result cas_res = CAS_NONE;\n\n    bool do_store = false;\n    if (old_it != NULL) {\n        // Most of the CAS work requires something to compare to.\n        uint64_t it_cas = ITEM_get_cas(it);\n        uint64_t old_cas = ITEM_get_cas(old_it);\n        if (it_cas == 0) {\n            cas_res = CAS_NONE;\n        } else if (it_cas == old_cas) {\n            cas_res = CAS_MATCH;\n        } else if (cas_stale && it_cas < old_cas) {\n            cas_res = CAS_STALE;\n        } else {\n            cas_res = CAS_BADVAL;\n        }\n\n        switch (comm) {\n            case NREAD_ADD:\n                /* add only adds a nonexistent item, but promote to head of LRU */\n                do_item_update(old_it);\n                break;\n            case NREAD_CAS:\n                if (cas_res == CAS_MATCH) {\n                    // cas validates\n                    // it and old_it may belong to different classes.\n                    // I'm updating the stats for the one that's getting pushed out\n                    pthread_mutex_lock(&t->stats.mutex);\n                    t->stats.slab_stats[ITEM_clsid(old_it)].cas_hits++;\n                    pthread_mutex_unlock(&t->stats.mutex);\n                    do_store = true;\n                } else if (cas_res == CAS_STALE) {\n                    // if we're allowed to set a stale value, CAS must be lower than\n                    // the current item's CAS.\n                    // This replaces the value, but should preserve TTL, and stale\n                    // item marker bit + token sent if exists.\n                    it->exptime = old_it->exptime;\n                    it->it_flags |= ITEM_STALE;\n                    if (old_it->it_flags & ITEM_TOKEN_SENT) {\n                        it->it_flags |= ITEM_TOKEN_SENT;\n                    }\n\n                    pthread_mutex_lock(&t->stats.mutex);\n                    t->stats.slab_stats[ITEM_clsid(old_it)].cas_hits++;\n                    pthread_mutex_unlock(&t->stats.mutex);\n                    do_store = true;\n                } else {\n                    // NONE or BADVAL are the same for CAS cmd\n                    pthread_mutex_lock(&t->stats.mutex);\n                    t->stats.slab_stats[ITEM_clsid(old_it)].cas_badval++;\n                    pthread_mutex_unlock(&t->stats.mutex);\n\n                    if (settings.verbose > 1) {\n                        fprintf(stderr, \"CAS:  failure: expected %llu, got %llu\\n\",\n                                (unsigned long long)ITEM_get_cas(old_it),\n                                (unsigned long long)ITEM_get_cas(it));\n                    }\n                    stored = EXISTS;\n                }\n                break;\n            case NREAD_APPEND:\n            case NREAD_PREPEND:\n            case NREAD_APPENDVIV:\n            case NREAD_PREPENDVIV:\n                if (cas_res != CAS_NONE && cas_res != CAS_MATCH) {\n                    stored = EXISTS;\n                    break;\n                }\n#ifdef EXTSTORE\n                if ((old_it->it_flags & ITEM_HDR) != 0) {\n                    /* block append/prepend from working with extstore-d items.\n                     * leave response code to NOT_STORED default */\n                    break;\n                }\n#endif\n                /* we have it and old_it here - alloc memory to hold both */\n                FLAGS_CONV(old_it, flags);\n                new_it = do_item_alloc(key, it->nkey, flags, old_it->exptime, it->nbytes + old_it->nbytes - 2 /* CRLF */);\n\n                // OOM trying to copy.\n                if (new_it == NULL)\n                    break;\n                /* copy data from it and old_it to new_it */\n                if (_store_item_copy_data(comm, old_it, new_it, it) == -1) {\n                    // failed data copy\n                    break;\n                } else {\n                    // refcount of new_it is 1 here. will end up 2 after link.\n                    // it's original ref is managed outside of this function\n                    it = new_it;\n                    do_store = true;\n                    // Upstream final object size for meta\n                    if (nbytes != NULL) {\n                        *nbytes = it->nbytes;\n                    }\n                }\n                break;\n            case NREAD_REPLACE:\n            case NREAD_SET:\n                do_store = true;\n                break;\n        }\n\n        if (do_store) {\n            STORAGE_delete(t->storage, old_it);\n            item_replace(old_it, it, hv, cas_in);\n            stored = STORED;\n        }\n\n        do_item_remove(old_it);         /* release our reference */\n        if (new_it != NULL) {\n            // append/prepend end up with an extra reference for new_it.\n            do_item_remove(new_it);\n        }\n    } else {\n        /* No pre-existing item to replace or compare to. */\n        if (ITEM_get_cas(it) != 0) {\n            /* Asked for a CAS match but nothing to compare it to. */\n            cas_res = CAS_MISS;\n        }\n\n        switch (comm) {\n            case NREAD_ADD:\n            case NREAD_SET:\n            case NREAD_APPENDVIV:\n            case NREAD_PREPENDVIV:\n                do_store = true;\n                break;\n            case NREAD_CAS:\n                // LRU expired\n                stored = NOT_FOUND;\n                pthread_mutex_lock(&t->stats.mutex);\n                t->stats.cas_misses++;\n                pthread_mutex_unlock(&t->stats.mutex);\n                break;\n            case NREAD_REPLACE:\n            case NREAD_APPEND:\n            case NREAD_PREPEND:\n                /* Requires an existing item. */\n                break;\n        }\n\n        if (do_store) {\n            do_item_link(it, hv, cas_in);\n            stored = STORED;\n        }\n    }\n\n    if (stored == STORED && cas != NULL) {\n        *cas = ITEM_get_cas(it);\n    }\n    LOGGER_LOG(t->l, LOG_MUTATIONS, LOGGER_ITEM_STORE, NULL,\n            stored, comm, ITEM_key(it), it->nkey, it->nbytes, it->exptime,\n            ITEM_clsid(it), t->cur_sfd);\n\n    return stored;\n}\n\n/* set up a connection to write a buffer then free it, used for stats */\nvoid write_and_free(conn *c, char *buf, int bytes) {\n    if (buf) {\n        mc_resp *resp = c->resp;\n        resp->write_and_free = buf;\n        resp_add_iov(resp, buf, bytes);\n        conn_set_state(c, conn_new_cmd);\n    } else {\n        out_of_memory(c, \"SERVER_ERROR out of memory writing stats\");\n    }\n}\n\nvoid append_stat(const char *name, ADD_STAT add_stats, conn *c,\n                 const char *fmt, ...) {\n    char val_str[STAT_VAL_LEN];\n    int vlen;\n    va_list ap;\n\n    assert(name);\n    assert(add_stats);\n    assert(c);\n    assert(fmt);\n\n    va_start(ap, fmt);\n    vlen = vsnprintf(val_str, sizeof(val_str) - 1, fmt, ap);\n    va_end(ap);\n\n    add_stats(name, strlen(name), val_str, vlen, c);\n}\n\n/* return server specific stats only */\nvoid server_stats(ADD_STAT add_stats, void *c) {\n    pid_t pid = getpid();\n    rel_time_t now = current_time;\n\n    struct thread_stats thread_stats;\n    threadlocal_stats_aggregate(&thread_stats);\n    struct slab_stats slab_stats;\n    slab_stats_aggregate(&thread_stats, &slab_stats);\n#ifndef WIN32\n    struct rusage usage;\n    getrusage(RUSAGE_SELF, &usage);\n#endif /* !WIN32 */\n\n    STATS_LOCK();\n\n    APPEND_STAT(\"pid\", \"%lu\", (long)pid);\n    APPEND_STAT(\"uptime\", \"%u\", now - ITEM_UPDATE_INTERVAL);\n    APPEND_STAT(\"time\", \"%ld\", now + (long)process_started);\n    APPEND_STAT(\"version\", \"%s\", VERSION);\n    APPEND_STAT(\"libevent\", \"%s\", event_get_version());\n    APPEND_STAT(\"pointer_size\", \"%d\", (int)(8 * sizeof(void *)));\n\n#ifndef WIN32\n    append_stat(\"rusage_user\", add_stats, c, \"%ld.%06ld\",\n                (long)usage.ru_utime.tv_sec,\n                (long)usage.ru_utime.tv_usec);\n    append_stat(\"rusage_system\", add_stats, c, \"%ld.%06ld\",\n                (long)usage.ru_stime.tv_sec,\n                (long)usage.ru_stime.tv_usec);\n#endif /* !WIN32 */\n\n    APPEND_STAT(\"max_connections\", \"%d\", settings.maxconns);\n    APPEND_STAT(\"curr_connections\", \"%llu\", (unsigned long long)stats_state.curr_conns - 1);\n    APPEND_STAT(\"total_connections\", \"%llu\", (unsigned long long)stats.total_conns);\n    if (settings.maxconns_fast) {\n        APPEND_STAT(\"rejected_connections\", \"%llu\", (unsigned long long)stats.rejected_conns);\n    }\n    APPEND_STAT(\"connection_structures\", \"%u\", stats_state.conn_structs);\n    APPEND_STAT(\"response_obj_oom\", \"%llu\", (unsigned long long)thread_stats.response_obj_oom);\n    APPEND_STAT(\"response_obj_count\", \"%llu\", (unsigned long long)thread_stats.response_obj_count);\n    APPEND_STAT(\"response_obj_bytes\", \"%llu\", (unsigned long long)thread_stats.response_obj_bytes);\n    APPEND_STAT(\"read_buf_count\", \"%llu\", (unsigned long long)thread_stats.read_buf_count);\n    APPEND_STAT(\"read_buf_bytes\", \"%llu\", (unsigned long long)thread_stats.read_buf_bytes);\n    APPEND_STAT(\"read_buf_bytes_free\", \"%llu\", (unsigned long long)thread_stats.read_buf_bytes_free);\n    APPEND_STAT(\"read_buf_oom\", \"%llu\", (unsigned long long)thread_stats.read_buf_oom);\n    APPEND_STAT(\"reserved_fds\", \"%u\", stats_state.reserved_fds);\n#ifdef PROXY\n    if (settings.proxy_enabled) {\n        APPEND_STAT(\"proxy_conn_requests\", \"%llu\", (unsigned long long)thread_stats.proxy_conn_requests);\n        APPEND_STAT(\"proxy_conn_errors\", \"%llu\", (unsigned long long)thread_stats.proxy_conn_errors);\n        APPEND_STAT(\"proxy_conn_oom\", \"%llu\", (unsigned long long)thread_stats.proxy_conn_oom);\n        APPEND_STAT(\"proxy_req_active\", \"%llu\", (unsigned long long)thread_stats.proxy_req_active);\n    }\n#endif\n    APPEND_STAT(\"cmd_get\", \"%llu\", (unsigned long long)thread_stats.get_cmds);\n    APPEND_STAT(\"cmd_set\", \"%llu\", (unsigned long long)slab_stats.set_cmds);\n    APPEND_STAT(\"cmd_flush\", \"%llu\", (unsigned long long)thread_stats.flush_cmds);\n    APPEND_STAT(\"cmd_touch\", \"%llu\", (unsigned long long)thread_stats.touch_cmds);\n    APPEND_STAT(\"cmd_meta\", \"%llu\", (unsigned long long)thread_stats.meta_cmds);\n    APPEND_STAT(\"get_hits\", \"%llu\", (unsigned long long)slab_stats.get_hits);\n    APPEND_STAT(\"get_misses\", \"%llu\", (unsigned long long)thread_stats.get_misses);\n    APPEND_STAT(\"get_expired\", \"%llu\", (unsigned long long)thread_stats.get_expired);\n    APPEND_STAT(\"get_flushed\", \"%llu\", (unsigned long long)thread_stats.get_flushed);\n#ifdef EXTSTORE\n    if (ext_storage) {\n        APPEND_STAT(\"get_extstore\", \"%llu\", (unsigned long long)thread_stats.get_extstore);\n        APPEND_STAT(\"get_aborted_extstore\", \"%llu\", (unsigned long long)thread_stats.get_aborted_extstore);\n        APPEND_STAT(\"get_oom_extstore\", \"%llu\", (unsigned long long)thread_stats.get_oom_extstore);\n        APPEND_STAT(\"recache_from_extstore\", \"%llu\", (unsigned long long)thread_stats.recache_from_extstore);\n        APPEND_STAT(\"miss_from_extstore\", \"%llu\", (unsigned long long)thread_stats.miss_from_extstore);\n        APPEND_STAT(\"badcrc_from_extstore\", \"%llu\", (unsigned long long)thread_stats.badcrc_from_extstore);\n    }\n#endif\n    APPEND_STAT(\"delete_misses\", \"%llu\", (unsigned long long)thread_stats.delete_misses);\n    APPEND_STAT(\"delete_hits\", \"%llu\", (unsigned long long)slab_stats.delete_hits);\n    APPEND_STAT(\"incr_misses\", \"%llu\", (unsigned long long)thread_stats.incr_misses);\n    APPEND_STAT(\"incr_hits\", \"%llu\", (unsigned long long)slab_stats.incr_hits);\n    APPEND_STAT(\"decr_misses\", \"%llu\", (unsigned long long)thread_stats.decr_misses);\n    APPEND_STAT(\"decr_hits\", \"%llu\", (unsigned long long)slab_stats.decr_hits);\n    APPEND_STAT(\"cas_misses\", \"%llu\", (unsigned long long)thread_stats.cas_misses);\n    APPEND_STAT(\"cas_hits\", \"%llu\", (unsigned long long)slab_stats.cas_hits);\n    APPEND_STAT(\"cas_badval\", \"%llu\", (unsigned long long)slab_stats.cas_badval);\n    APPEND_STAT(\"touch_hits\", \"%llu\", (unsigned long long)slab_stats.touch_hits);\n    APPEND_STAT(\"touch_misses\", \"%llu\", (unsigned long long)thread_stats.touch_misses);\n    APPEND_STAT(\"store_too_large\", \"%llu\", (unsigned long long)thread_stats.store_too_large);\n    APPEND_STAT(\"store_no_memory\", \"%llu\", (unsigned long long)thread_stats.store_no_memory);\n    APPEND_STAT(\"auth_cmds\", \"%llu\", (unsigned long long)thread_stats.auth_cmds);\n    APPEND_STAT(\"auth_errors\", \"%llu\", (unsigned long long)thread_stats.auth_errors);\n    if (settings.idle_timeout) {\n        APPEND_STAT(\"idle_kicks\", \"%llu\", (unsigned long long)thread_stats.idle_kicks);\n    }\n    APPEND_STAT(\"bytes_read\", \"%llu\", (unsigned long long)thread_stats.bytes_read);\n    APPEND_STAT(\"bytes_written\", \"%llu\", (unsigned long long)thread_stats.bytes_written);\n    APPEND_STAT(\"limit_maxbytes\", \"%llu\", (unsigned long long)settings.maxbytes);\n    APPEND_STAT(\"accepting_conns\", \"%u\", stats_state.accepting_conns);\n    APPEND_STAT(\"listen_disabled_num\", \"%llu\", (unsigned long long)stats.listen_disabled_num);\n    APPEND_STAT(\"time_in_listen_disabled_us\", \"%llu\", stats.time_in_listen_disabled_us);\n    APPEND_STAT(\"threads\", \"%d\", settings.num_threads);\n    APPEND_STAT(\"conn_yields\", \"%llu\", (unsigned long long)thread_stats.conn_yields);\n    APPEND_STAT(\"hash_power_level\", \"%u\", stats_state.hash_power_level);\n    APPEND_STAT(\"hash_bytes\", \"%llu\", (unsigned long long)stats_state.hash_bytes);\n    APPEND_STAT(\"hash_is_expanding\", \"%u\", stats_state.hash_is_expanding);\n    if (settings.slab_reassign) {\n        APPEND_STAT(\"slab_reassign_rescues\", \"%llu\", stats.slab_reassign_rescues);\n        APPEND_STAT(\"slab_reassign_chunk_rescues\", \"%llu\", stats.slab_reassign_chunk_rescues);\n        APPEND_STAT(\"slab_reassign_inline_reclaim\", \"%llu\", stats.slab_reassign_inline_reclaim);\n        APPEND_STAT(\"slab_reassign_busy_items\", \"%llu\", stats.slab_reassign_busy_items);\n        APPEND_STAT(\"slab_reassign_busy_deletes\", \"%llu\", stats.slab_reassign_busy_deletes);\n        APPEND_STAT(\"slab_reassign_busy_nomem\", \"%llu\", stats.slab_reassign_busy_nomem);\n        APPEND_STAT(\"slab_reassign_running\", \"%u\", stats_state.slab_reassign_running);\n        APPEND_STAT(\"slabs_moved\", \"%llu\", stats.slabs_moved);\n    }\n    if (settings.lru_crawler) {\n        APPEND_STAT(\"lru_crawler_running\", \"%u\", stats_state.lru_crawler_running);\n        APPEND_STAT(\"lru_crawler_starts\", \"%u\", stats.lru_crawler_starts);\n    }\n    if (settings.lru_maintainer_thread) {\n        APPEND_STAT(\"lru_maintainer_juggles\", \"%llu\", (unsigned long long)stats.lru_maintainer_juggles);\n    }\n    APPEND_STAT(\"malloc_fails\", \"%llu\",\n                (unsigned long long)stats.malloc_fails);\n    APPEND_STAT(\"log_worker_dropped\", \"%llu\", (unsigned long long)stats.log_worker_dropped);\n    APPEND_STAT(\"log_worker_written\", \"%llu\", (unsigned long long)stats.log_worker_written);\n    APPEND_STAT(\"log_watcher_skipped\", \"%llu\", (unsigned long long)stats.log_watcher_skipped);\n    APPEND_STAT(\"log_watcher_sent\", \"%llu\", (unsigned long long)stats.log_watcher_sent);\n    APPEND_STAT(\"log_watchers\", \"%llu\", (unsigned long long)stats_state.log_watchers);\n    STATS_UNLOCK();\n#ifdef EXTSTORE\n    storage_stats(add_stats, c);\n#endif\n#ifdef PROXY\n    proxy_stats(settings.proxy_ctx, add_stats, c);\n#endif\n#ifdef TLS\n    if (settings.ssl_enabled) {\n        if (settings.ssl_session_cache) {\n            APPEND_STAT(\"ssl_new_sessions\", \"%llu\", (unsigned long long)stats.ssl_new_sessions);\n        }\n        APPEND_STAT(\"ssl_handshake_errors\", \"%llu\", (unsigned long long)stats.ssl_handshake_errors);\n        APPEND_STAT(\"ssl_proto_errors\", \"%llu\", (unsigned long long)stats.ssl_proto_errors);\n        APPEND_STAT(\"time_since_server_cert_refresh\", \"%u\", now - settings.ssl_last_cert_refresh_time);\n    }\n#endif\n    APPEND_STAT(\"unexpected_napi_ids\", \"%llu\", (unsigned long long)stats.unexpected_napi_ids);\n    APPEND_STAT(\"round_robin_fallback\", \"%llu\", (unsigned long long)stats.round_robin_fallback);\n}\n\nvoid process_stat_settings(ADD_STAT add_stats, void *c) {\n    assert(add_stats);\n    APPEND_STAT(\"maxbytes\", \"%llu\", (unsigned long long)settings.maxbytes);\n    APPEND_STAT(\"maxconns\", \"%d\", settings.maxconns);\n    APPEND_STAT(\"tcpport\", \"%d\", settings.port);\n    APPEND_STAT(\"udpport\", \"%d\", settings.udpport);\n    APPEND_STAT(\"inter\", \"%s\", settings.inter ? settings.inter : \"NULL\");\n    APPEND_STAT(\"verbosity\", \"%d\", settings.verbose);\n    APPEND_STAT(\"oldest\", \"%lu\", (unsigned long)settings.oldest_live);\n    APPEND_STAT(\"evictions\", \"%s\", settings.evict_to_free ? \"on\" : \"off\");\n    APPEND_STAT(\"domain_socket\", \"%s\",\n                settings.socketpath ? settings.socketpath : \"NULL\");\n    APPEND_STAT(\"umask\", \"%o\", settings.access);\n    APPEND_STAT(\"shutdown_command\", \"%s\",\n                settings.shutdown_command ? \"yes\" : \"no\");\n    APPEND_STAT(\"growth_factor\", \"%.2f\", settings.factor);\n    APPEND_STAT(\"chunk_size\", \"%d\", settings.chunk_size);\n    APPEND_STAT(\"num_threads\", \"%d\", settings.num_threads);\n    APPEND_STAT(\"num_threads_per_udp\", \"%d\", settings.num_threads_per_udp);\n    APPEND_STAT(\"stat_key_prefix\", \"%c\", settings.prefix_delimiter);\n    APPEND_STAT(\"detail_enabled\", \"%s\",\n                settings.detail_enabled ? \"yes\" : \"no\");\n    APPEND_STAT(\"reqs_per_event\", \"%d\", settings.reqs_per_event);\n    APPEND_STAT(\"cas_enabled\", \"%s\", settings.use_cas ? \"yes\" : \"no\");\n    APPEND_STAT(\"tcp_backlog\", \"%d\", settings.backlog);\n    APPEND_STAT(\"binding_protocol\", \"%s\",\n                prot_text(settings.binding_protocol));\n    APPEND_STAT(\"auth_enabled_sasl\", \"%s\", settings.sasl ? \"yes\" : \"no\");\n    APPEND_STAT(\"auth_enabled_ascii\", \"%s\", settings.auth_file ? settings.auth_file : \"no\");\n    APPEND_STAT(\"item_size_max\", \"%d\", settings.item_size_max);\n    APPEND_STAT(\"maxconns_fast\", \"%s\", settings.maxconns_fast ? \"yes\" : \"no\");\n    APPEND_STAT(\"hashpower_init\", \"%d\", settings.hashpower_init);\n    APPEND_STAT(\"slab_reassign\", \"%s\", settings.slab_reassign ? \"yes\" : \"no\");\n    APPEND_STAT(\"slab_automove\", \"%d\", settings.slab_automove);\n    APPEND_STAT(\"slab_automove_ratio\", \"%.2f\", settings.slab_automove_ratio);\n    APPEND_STAT(\"slab_automove_window\", \"%u\", settings.slab_automove_window);\n    APPEND_STAT(\"slab_chunk_max\", \"%d\", settings.slab_chunk_size_max);\n    APPEND_STAT(\"lru_crawler\", \"%s\", settings.lru_crawler ? \"yes\" : \"no\");\n    APPEND_STAT(\"lru_crawler_sleep\", \"%d\", settings.lru_crawler_sleep);\n    APPEND_STAT(\"lru_crawler_tocrawl\", \"%lu\", (unsigned long)settings.lru_crawler_tocrawl);\n    APPEND_STAT(\"tail_repair_time\", \"%d\", settings.tail_repair_time);\n    APPEND_STAT(\"flush_enabled\", \"%s\", settings.flush_enabled ? \"yes\" : \"no\");\n    APPEND_STAT(\"dump_enabled\", \"%s\", settings.dump_enabled ? \"yes\" : \"no\");\n    APPEND_STAT(\"hash_algorithm\", \"%s\", settings.hash_algorithm);\n    APPEND_STAT(\"lru_maintainer_thread\", \"%s\", settings.lru_maintainer_thread ? \"yes\" : \"no\");\n    APPEND_STAT(\"lru_segmented\", \"%s\", settings.lru_segmented ? \"yes\" : \"no\");\n    APPEND_STAT(\"hot_lru_pct\", \"%d\", settings.hot_lru_pct);\n    APPEND_STAT(\"warm_lru_pct\", \"%d\", settings.warm_lru_pct);\n    APPEND_STAT(\"hot_max_factor\", \"%.2f\", settings.hot_max_factor);\n    APPEND_STAT(\"warm_max_factor\", \"%.2f\", settings.warm_max_factor);\n    APPEND_STAT(\"temp_lru\", \"%s\", settings.temp_lru ? \"yes\" : \"no\");\n    APPEND_STAT(\"temporary_ttl\", \"%u\", settings.temporary_ttl);\n    APPEND_STAT(\"idle_timeout\", \"%d\", settings.idle_timeout);\n    APPEND_STAT(\"watcher_logbuf_size\", \"%u\", settings.logger_watcher_buf_size);\n    APPEND_STAT(\"worker_logbuf_size\", \"%u\", settings.logger_buf_size);\n    APPEND_STAT(\"read_buf_mem_limit\", \"%u\", settings.read_buf_mem_limit);\n    APPEND_STAT(\"track_sizes\", \"%s\", item_stats_sizes_status() ? \"yes\" : \"no\");\n    APPEND_STAT(\"inline_ascii_response\", \"%s\", \"no\"); // setting is dead, cannot be yes.\n#ifdef HAVE_DROP_PRIVILEGES\n    APPEND_STAT(\"drop_privileges\", \"%s\", settings.drop_privileges ? \"yes\" : \"no\");\n#endif\n#ifdef EXTSTORE\n    APPEND_STAT(\"ext_item_size\", \"%u\", settings.ext_item_size);\n    APPEND_STAT(\"ext_item_age\", \"%u\", settings.ext_item_age);\n    APPEND_STAT(\"ext_low_ttl\", \"%u\", settings.ext_low_ttl);\n    APPEND_STAT(\"ext_recache_rate\", \"%u\", settings.ext_recache_rate);\n    APPEND_STAT(\"ext_wbuf_size\", \"%u\", settings.ext_wbuf_size);\n    APPEND_STAT(\"ext_compact_under\", \"%u\", settings.ext_compact_under);\n    APPEND_STAT(\"ext_drop_under\", \"%u\", settings.ext_drop_under);\n    APPEND_STAT(\"ext_max_sleep\", \"%u\", settings.ext_max_sleep);\n    APPEND_STAT(\"ext_max_frag\", \"%.2f\", settings.ext_max_frag);\n    APPEND_STAT(\"slab_automove_freeratio\", \"%.3f\", settings.slab_automove_freeratio);\n    APPEND_STAT(\"ext_drop_unread\", \"%s\", settings.ext_drop_unread ? \"yes\" : \"no\");\n#endif\n#ifdef TLS\n    APPEND_STAT(\"ssl_enabled\", \"%s\", settings.ssl_enabled ? \"yes\" : \"no\");\n    APPEND_STAT(\"ssl_chain_cert\", \"%s\", settings.ssl_chain_cert);\n    APPEND_STAT(\"ssl_key\", \"%s\", settings.ssl_key);\n    APPEND_STAT(\"ssl_verify_mode\", \"%d\", settings.ssl_verify_mode);\n    APPEND_STAT(\"ssl_keyformat\", \"%d\", settings.ssl_keyformat);\n    APPEND_STAT(\"ssl_ciphers\", \"%s\", settings.ssl_ciphers ? settings.ssl_ciphers : \"NULL\");\n    APPEND_STAT(\"ssl_ca_cert\", \"%s\", settings.ssl_ca_cert ? settings.ssl_ca_cert : \"NULL\");\n    APPEND_STAT(\"ssl_wbuf_size\", \"%u\", settings.ssl_wbuf_size);\n    APPEND_STAT(\"ssl_session_cache\", \"%s\", settings.ssl_session_cache ? \"yes\" : \"no\");\n    APPEND_STAT(\"ssl_kernel_tls\", \"%s\", settings.ssl_kernel_tls ? \"yes\" : \"no\");\n    APPEND_STAT(\"ssl_min_version\", \"%s\", ssl_proto_text(settings.ssl_min_version));\n#endif\n#ifdef PROXY\n    APPEND_STAT(\"proxy_enabled\", \"%s\", settings.proxy_enabled ? \"yes\" : \"no\");\n    APPEND_STAT(\"proxy_uring_enabled\", \"%s\", settings.proxy_uring ? \"yes\" : \"no\");\n#endif\n    APPEND_STAT(\"num_napi_ids\", \"%s\", settings.num_napi_ids);\n    APPEND_STAT(\"memory_file\", \"%s\", settings.memory_file);\n    APPEND_STAT(\"client_flags_size\", \"%d\", sizeof(client_flags_t));\n}\n\nstatic int nz_strcmp(int nzlength, const char *nz, const char *z) {\n    int zlength=strlen(z);\n    return (zlength == nzlength) && (strncmp(nz, z, zlength) == 0) ? 0 : -1;\n}\n\nbool get_stats(const char *stat_type, int nkey, ADD_STAT add_stats, void *c) {\n    bool ret = true;\n\n    if (add_stats != NULL) {\n        if (!stat_type) {\n            /* prepare general statistics for the engine */\n            STATS_LOCK();\n            APPEND_STAT(\"bytes\", \"%llu\", (unsigned long long)stats_state.curr_bytes);\n            APPEND_STAT(\"curr_items\", \"%llu\", (unsigned long long)stats_state.curr_items);\n            APPEND_STAT(\"total_items\", \"%llu\", (unsigned long long)stats.total_items);\n            STATS_UNLOCK();\n            APPEND_STAT(\"slab_global_page_pool\", \"%u\", global_page_pool_size(NULL));\n            item_stats_totals(add_stats, c);\n        } else if (nz_strcmp(nkey, stat_type, \"items\") == 0) {\n            item_stats(add_stats, c);\n        } else if (nz_strcmp(nkey, stat_type, \"slabs\") == 0) {\n            slabs_stats(add_stats, c);\n        } else if (nz_strcmp(nkey, stat_type, \"sizes\") == 0) {\n            item_stats_sizes(add_stats, c);\n        } else {\n            ret = false;\n        }\n    } else {\n        ret = false;\n    }\n\n    return ret;\n}\n\nstatic inline void get_conn_text(const conn *c, const int af,\n                char* addr, struct sockaddr *sock_addr) {\n    char addr_text[MAXPATHLEN];\n    addr_text[0] = '\\0';\n    const char *protoname = \"?\";\n    unsigned short port = 0;\n\n    switch (af) {\n        case AF_INET:\n            (void) inet_ntop(af,\n                    &((struct sockaddr_in *)sock_addr)->sin_addr,\n                    addr_text,\n                    sizeof(addr_text) - 1);\n            port = ntohs(((struct sockaddr_in *)sock_addr)->sin_port);\n            protoname = IS_UDP(c->transport) ? \"udp\" : \"tcp\";\n            break;\n\n        case AF_INET6:\n            addr_text[0] = '[';\n            addr_text[1] = '\\0';\n            if (inet_ntop(af,\n                    &((struct sockaddr_in6 *)sock_addr)->sin6_addr,\n                    addr_text + 1,\n                    sizeof(addr_text) - 2)) {\n                strncat(addr_text, \"]\", 2);\n            }\n            port = ntohs(((struct sockaddr_in6 *)sock_addr)->sin6_port);\n            protoname = IS_UDP(c->transport) ? \"udp6\" : \"tcp6\";\n            break;\n\n#ifndef DISABLE_UNIX_SOCKET\n        case AF_UNIX:\n        {\n            size_t pathlen = 0;\n            // this strncpy call originally could piss off an address\n            // sanitizer; we supplied the size of the dest buf as a limiter,\n            // but optimized versions of strncpy could read past the end of\n            // *src while looking for a null terminator. Since buf and\n            // sun_path here are both on the stack they could even overlap,\n            // which is \"undefined\". In all OSS versions of strncpy I could\n            // find this has no effect; it'll still only copy until the first null\n            // terminator is found. Thus it's possible to get the OS to\n            // examine past the end of sun_path but it's unclear to me if this\n            // can cause any actual problem.\n            //\n            // We need a safe_strncpy util function but I'll punt on figuring\n            // that out for now.\n            pathlen = sizeof(((struct sockaddr_un *)sock_addr)->sun_path);\n            if (MAXPATHLEN <= pathlen) {\n                pathlen = MAXPATHLEN - 1;\n            }\n            strncpy(addr_text,\n                    ((struct sockaddr_un *)sock_addr)->sun_path,\n                    pathlen);\n            addr_text[pathlen] = '\\0';\n            protoname = \"unix\";\n        }\n            break;\n#endif /* #ifndef DISABLE_UNIX_SOCKET */\n    }\n\n    if (strlen(addr_text) < 2) {\n        /* Most likely this is a connected UNIX-domain client which\n         * has no peer socket address, but there's no portable way\n         * to tell for sure.\n         */\n        snprintf(addr_text, MAXPATHLEN, \"<AF %d>\", af);\n    }\n\n    if (port) {\n        snprintf(addr, MAXPATHLEN + 11, \"%s:%s:%u\", protoname, addr_text, port);\n    } else {\n        snprintf(addr, MAXPATHLEN + 11, \"%s:%s\", protoname, addr_text);\n    }\n}\n\nstatic void conn_to_str(const conn *c, char *addr, char *svr_addr) {\n    if (!c) {\n        memcpy(addr, \"<null>\", 6);\n    } else if (c->state == conn_closed) {\n        memcpy(addr, \"<closed>\", 8);\n    } else {\n        struct sockaddr_in6 local_addr;\n        struct sockaddr *sock_addr = (void *)&c->request_addr;\n\n        /* For listen ports and idle UDP ports, show listen address */\n        if (c->state == conn_listening ||\n                (IS_UDP(c->transport) &&\n                 c->state == conn_read)) {\n            memset(&local_addr, 0, sizeof(local_addr));\n            socklen_t local_addr_len = sizeof(local_addr);\n\n            if (getsockname(c->sfd,\n                        (struct sockaddr *)&local_addr,\n                        &local_addr_len) == 0) {\n                sock_addr = (struct sockaddr *)&local_addr;\n            }\n        }\n        get_conn_text(c, sock_addr->sa_family, addr, sock_addr);\n\n        if (c->state != conn_listening && !(IS_UDP(c->transport) &&\n                 c->state == conn_read)) {\n            struct sockaddr_storage svr_sock_addr;\n            memset(&svr_sock_addr, 0, sizeof(svr_sock_addr));\n            socklen_t svr_addr_len = sizeof(svr_sock_addr);\n            getsockname(c->sfd, (struct sockaddr *)&svr_sock_addr, &svr_addr_len);\n            get_conn_text(c, svr_sock_addr.ss_family, svr_addr, (struct sockaddr *)&svr_sock_addr);\n        }\n    }\n}\n\nstatic char *conn_queue_to_str(const conn *c, io_queue_t *q) {\n    if (q->type == IO_QUEUE_EXTSTORE) {\n        return \"queue_extstore\";\n    } else if (q->type == IO_QUEUE_PROXY) {\n        return \"queue_proxy\";\n    } else {\n        return \"queue_unknown\";\n    }\n}\n\nvoid process_stats_conns(ADD_STAT add_stats, void *c) {\n    int i;\n    char key_str[STAT_KEY_LEN];\n    char val_str[STAT_VAL_LEN];\n    size_t extras_len = sizeof(\":unix:\") + sizeof(\"65535\");\n    char addr[MAXPATHLEN + extras_len];\n    char svr_addr[MAXPATHLEN + extras_len];\n    memset(addr, 0, sizeof(addr));\n    memset(svr_addr, 0, sizeof(svr_addr));\n    int klen = 0, vlen = 0;\n\n    assert(add_stats);\n\n    for (i = 0; i < max_fds; i++) {\n        if (conns[i]) {\n            /* This is safe to do unlocked because conns are never freed; the\n             * worst that'll happen will be a minor inconsistency in the\n             * output -- not worth the complexity of the locking that'd be\n             * required to prevent it.\n             */\n            if (IS_UDP(conns[i]->transport)) {\n                APPEND_NUM_STAT(i, \"UDP\", \"%s\", \"UDP\");\n            }\n            if (conns[i]->state != conn_closed) {\n                conn *sc = conns[i];\n                conn_to_str(sc, addr, svr_addr);\n\n                APPEND_NUM_STAT(i, \"addr\", \"%s\", addr);\n                if (sc->state != conn_listening &&\n                    !(IS_UDP(sc->transport) && sc->state == conn_read)) {\n                    APPEND_NUM_STAT(i, \"listen_addr\", \"%s\", svr_addr);\n                }\n                APPEND_NUM_STAT(i, \"state\", \"%s\",\n                        state_text(sc->state));\n                if (sc->io_queues_submitted) {\n                    APPEND_NUM_STAT(i, \"queues_waiting\", \"%d\", sc->io_queues_submitted);\n                    for (io_queue_t *q = sc->io_queues; q->type != IO_QUEUE_NONE; q++) {\n                        if (q->count) {\n                            const char *qname = conn_queue_to_str(sc, q);\n                            APPEND_NUM_STAT(i, qname, \"%d\", q->count);\n                        }\n                    }\n                }\n                APPEND_NUM_STAT(i, \"secs_since_last_cmd\", \"%d\",\n                        current_time - sc->last_cmd_time);\n            }\n        }\n    }\n}\n\n#define IT_REFCOUNT_LIMIT 60000\nitem* limited_get(const char *key, size_t nkey, LIBEVENT_THREAD *t, uint32_t exptime, bool should_touch, bool do_update, bool *overflow) {\n    item *it;\n    if (should_touch) {\n        it = item_touch(key, nkey, exptime, t);\n    } else {\n        it = item_get(key, nkey, t, do_update);\n    }\n    if (it && it->refcount > IT_REFCOUNT_LIMIT) {\n        item_remove(it);\n        it = NULL;\n        *overflow = true;\n    } else {\n        *overflow = false;\n    }\n    return it;\n}\n\n// Semantics are different than limited_get; since the item is returned\n// locked, caller can directly change what it needs.\n// though it might eventually be a better interface to sink it all into\n// items.c.\nitem* limited_get_locked(const char *key, size_t nkey, LIBEVENT_THREAD *t, bool do_update, uint32_t *hv, bool *overflow) {\n    item *it;\n    it = item_get_locked(key, nkey, t, do_update, hv);\n    if (it && it->refcount > IT_REFCOUNT_LIMIT) {\n        do_item_remove(it);\n        it = NULL;\n        item_unlock(*hv);\n        *overflow = true;\n    } else {\n        *overflow = false;\n    }\n    return it;\n}\n\n/*\n * adds a delta value to a numeric item.\n *\n * c     connection requesting the operation\n * it    item to adjust\n * incr  true to increment value, false to decrement\n * delta amount to adjust value by\n * buf   buffer for response string\n *\n * returns a response string to send back to the client.\n */\nenum delta_result_type do_add_delta(LIBEVENT_THREAD *t, const char *key, const size_t nkey,\n                                    const bool incr, const int64_t delta,\n                                    char *buf, uint64_t *cas,\n                                    const uint32_t hv,\n                                    item **it_ret) {\n    char *ptr;\n    uint64_t value;\n    int res;\n    item *it;\n\n    it = do_item_get(key, nkey, hv, t, DONT_UPDATE);\n    if (!it) {\n        return DELTA_ITEM_NOT_FOUND;\n    }\n\n    /* Can't delta zero byte values. 2-byte are the \"\\r\\n\" */\n    /* Also can't delta for chunked items. Too large to be a number */\n#ifdef EXTSTORE\n    if (it->nbytes <= 2 || (it->it_flags & (ITEM_CHUNKED|ITEM_HDR)) != 0) {\n#else\n    if (it->nbytes <= 2 || (it->it_flags & (ITEM_CHUNKED)) != 0) {\n#endif\n        do_item_remove(it);\n        return NON_NUMERIC;\n    }\n\n    if (cas != NULL && *cas != 0 && ITEM_get_cas(it) != *cas) {\n        do_item_remove(it);\n        return DELTA_ITEM_CAS_MISMATCH;\n    }\n\n    ptr = ITEM_data(it);\n\n    if (!safe_strtoull(ptr, &value)) {\n        do_item_remove(it);\n        return NON_NUMERIC;\n    }\n\n    if (incr) {\n        value += delta;\n        //MEMCACHED_COMMAND_INCR(c->sfd, ITEM_key(it), it->nkey, value);\n    } else {\n        if(delta > value) {\n            value = 0;\n        } else {\n            value -= delta;\n        }\n        //MEMCACHED_COMMAND_DECR(c->sfd, ITEM_key(it), it->nkey, value);\n    }\n\n    pthread_mutex_lock(&t->stats.mutex);\n    if (incr) {\n        t->stats.slab_stats[ITEM_clsid(it)].incr_hits++;\n    } else {\n        t->stats.slab_stats[ITEM_clsid(it)].decr_hits++;\n    }\n    pthread_mutex_unlock(&t->stats.mutex);\n\n    itoa_u64(value, buf);\n    res = strlen(buf);\n    /* refcount == 2 means we are the only ones holding the item, and it is\n     * linked. We hold the item's lock in this function, so refcount cannot\n     * increase. */\n    if (res + 2 <= it->nbytes && it->refcount == 2) { /* replace in-place */\n        /* When changing the value without replacing the item, we\n           need to update the CAS on the existing item. */\n        /* We also need to fiddle it in the sizes tracker in case the tracking\n         * was enabled at runtime, since it relies on the CAS value to know\n         * whether to remove an item or not. */\n        item_stats_sizes_remove(it);\n        ITEM_set_cas(it, (settings.use_cas) ? get_cas_id() : 0);\n        item_stats_sizes_add(it);\n        memcpy(ITEM_data(it), buf, res);\n        memset(ITEM_data(it) + res, ' ', it->nbytes - res - 2);\n        do_item_update(it);\n    } else if (it->refcount > 1) {\n        item *new_it;\n        client_flags_t flags;\n        FLAGS_CONV(it, flags);\n        new_it = do_item_alloc(ITEM_key(it), it->nkey, flags, it->exptime, res + 2);\n        if (new_it == 0) {\n            do_item_remove(it);\n            return EOM;\n        }\n        memcpy(ITEM_data(new_it), buf, res);\n        memcpy(ITEM_data(new_it) + res, \"\\r\\n\", 2);\n        item_replace(it, new_it, hv, (settings.use_cas) ? get_cas_id() : 0);\n        // Overwrite the older item's CAS with our new CAS since we're\n        // returning the CAS of the old item below.\n        ITEM_set_cas(it, (settings.use_cas) ? ITEM_get_cas(new_it) : 0);\n        do_item_remove(new_it);       /* release our reference */\n    } else {\n        /* Should never get here. This means we somehow fetched an unlinked\n         * item. TODO: Add a counter? */\n        if (settings.verbose) {\n            fprintf(stderr, \"Tried to do incr/decr on invalid item\\n\");\n        }\n        if (it->refcount == 1)\n            do_item_remove(it);\n        return DELTA_ITEM_NOT_FOUND;\n    }\n\n    if (cas) {\n        *cas = ITEM_get_cas(it);    /* swap the incoming CAS value */\n    }\n    if (it_ret != NULL) {\n        *it_ret = it;\n    } else {\n        do_item_remove(it);         /* release our reference */\n    }\n    return OK;\n}\n\nstatic int try_read_command_negotiate(conn *c) {\n    assert(c != NULL);\n    assert(c->protocol == negotiating_prot);\n    assert(c->rcurr <= (c->rbuf + c->rsize));\n    assert(c->rbytes > 0);\n\n    if ((unsigned char)c->rbuf[0] == (unsigned char)PROTOCOL_BINARY_REQ) {\n        c->protocol = binary_prot;\n        c->try_read_command = try_read_command_binary;\n    } else {\n        // authentication doesn't work with negotiated protocol.\n        c->protocol = ascii_prot;\n        c->try_read_command = try_read_command_ascii;\n    }\n\n    if (settings.verbose > 1) {\n        fprintf(stderr, \"%d: Client using the %s protocol\\n\", c->sfd,\n                prot_text(c->protocol));\n    }\n\n    return c->try_read_command(c);\n}\n\nstatic int try_read_command_udp(conn *c) {\n    assert(c != NULL);\n    assert(c->rcurr <= (c->rbuf + c->rsize));\n    assert(c->rbytes > 0);\n\n    if ((unsigned char)c->rbuf[0] == (unsigned char)PROTOCOL_BINARY_REQ) {\n        c->protocol = binary_prot;\n        return try_read_command_binary(c);\n    } else {\n        c->protocol = ascii_prot;\n        return try_read_command_ascii(c);\n    }\n}\n\n/*\n * read a UDP request.\n */\nstatic enum try_read_result try_read_udp(conn *c) {\n    int res;\n\n    assert(c != NULL);\n\n    c->request_addr_size = sizeof(c->request_addr);\n    res = recvfrom(c->sfd, c->rbuf, c->rsize,\n                   0, (struct sockaddr *)&c->request_addr,\n                   &c->request_addr_size);\n    if (res > 8) {\n        unsigned char *buf = (unsigned char *)c->rbuf;\n        pthread_mutex_lock(&c->thread->stats.mutex);\n        c->thread->stats.bytes_read += res;\n        pthread_mutex_unlock(&c->thread->stats.mutex);\n\n        /* Beginning of UDP packet is the request ID; save it. */\n        c->request_id = buf[0] * 256 + buf[1];\n\n        /* If this is a multi-packet request, drop it. */\n        if (buf[4] != 0 || buf[5] != 1) {\n            return READ_NO_DATA_RECEIVED;\n        }\n\n        /* Don't care about any of the rest of the header. */\n        res -= 8;\n        memmove(c->rbuf, c->rbuf + 8, res);\n\n        c->rbytes = res;\n        c->rcurr = c->rbuf;\n        return READ_DATA_RECEIVED;\n    }\n    return READ_NO_DATA_RECEIVED;\n}\n\n/*\n * read from network as much as we can, handle buffer overflow and connection\n * close.\n * before reading, move the remaining incomplete fragment of a command\n * (if any) to the beginning of the buffer.\n *\n * To protect us from someone flooding a connection with bogus data causing\n * the connection to eat up all available memory, break out and start looking\n * at the data I've got after a number of reallocs...\n *\n * @return enum try_read_result\n */\nstatic enum try_read_result try_read_network(conn *c) {\n    enum try_read_result gotdata = READ_NO_DATA_RECEIVED;\n    int res;\n    int num_allocs = 0;\n    assert(c != NULL);\n\n    if (c->rcurr != c->rbuf) {\n        if (c->rbytes > 0) /* otherwise there's nothing to copy */\n            memmove(c->rbuf, c->rcurr, c->rbytes);\n        c->rcurr = c->rbuf;\n    }\n\n    while (1) {\n        // TODO: move to rbuf_* func?\n        if (c->rbytes >= c->rsize && c->rbuf_malloced) {\n            if (num_allocs == 4) {\n                return gotdata;\n            }\n            ++num_allocs;\n            char *new_rbuf = realloc(c->rbuf, c->rsize * 2);\n            if (!new_rbuf) {\n                STATS_LOCK();\n                stats.malloc_fails++;\n                STATS_UNLOCK();\n                if (settings.verbose > 0) {\n                    fprintf(stderr, \"Couldn't realloc input buffer\\n\");\n                }\n                c->rbytes = 0; /* ignore what we read */\n                out_of_memory(c, \"SERVER_ERROR out of memory reading request\");\n                c->close_after_write = true;\n                return READ_MEMORY_ERROR;\n            }\n            c->rcurr = c->rbuf = new_rbuf;\n            c->rsize *= 2;\n        }\n\n        int avail = c->rsize - c->rbytes;\n        res = c->read(c, c->rbuf + c->rbytes, avail);\n        if (res > 0) {\n            pthread_mutex_lock(&c->thread->stats.mutex);\n            c->thread->stats.bytes_read += res;\n            pthread_mutex_unlock(&c->thread->stats.mutex);\n            gotdata = READ_DATA_RECEIVED;\n            c->rbytes += res;\n            if (res == avail && c->rbuf_malloced) {\n                // Resize rbuf and try a few times if huge ascii multiget.\n                continue;\n            } else {\n                break;\n            }\n        }\n        if (res == 0) {\n            c->close_reason = NORMAL_CLOSE;\n            return READ_ERROR;\n        }\n        if (res == -1) {\n            if (errno == EAGAIN || errno == EWOULDBLOCK) {\n                break;\n            }\n            return READ_ERROR;\n        }\n    }\n    return gotdata;\n}\n\nstatic bool update_event(conn *c, const int new_flags) {\n    assert(c != NULL);\n\n    struct event_base *base = c->event.ev_base;\n    if (c->ev_flags == new_flags)\n        return true;\n    if (event_del(&c->event) == -1) return false;\n    event_set(&c->event, c->sfd, new_flags, event_handler, (void *)c);\n    event_base_set(base, &c->event);\n    c->ev_flags = new_flags;\n    if (event_add(&c->event, 0) == -1) return false;\n    return true;\n}\n\n/*\n * Sets whether we are listening for new connections or not.\n */\nvoid do_accept_new_conns(const bool do_accept) {\n    conn *next;\n\n    for (next = listen_conn; next; next = next->next) {\n        if (do_accept) {\n            update_event(next, EV_READ | EV_PERSIST);\n            if (listen(next->sfd, settings.backlog) != 0) {\n                perror(\"listen\");\n            }\n        }\n        else {\n            update_event(next, 0);\n            if (listen(next->sfd, 0) != 0) {\n                perror(\"listen\");\n            }\n        }\n    }\n\n    if (do_accept) {\n        struct timeval maxconns_exited;\n        uint64_t elapsed_us;\n        gettimeofday(&maxconns_exited,NULL);\n        STATS_LOCK();\n        elapsed_us =\n            (maxconns_exited.tv_sec - stats.maxconns_entered.tv_sec) * 1000000\n            + (maxconns_exited.tv_usec - stats.maxconns_entered.tv_usec);\n        stats.time_in_listen_disabled_us += elapsed_us;\n        stats_state.accepting_conns = true;\n        STATS_UNLOCK();\n    } else {\n        STATS_LOCK();\n        stats_state.accepting_conns = false;\n        gettimeofday(&stats.maxconns_entered,NULL);\n        stats.listen_disabled_num++;\n        STATS_UNLOCK();\n        allow_new_conns = false;\n        maxconns_handler(-42, 0, 0);\n    }\n}\n\n#define TRANSMIT_ONE_RESP true\n#define TRANSMIT_ALL_RESP false\nstatic int _transmit_pre(conn *c, struct iovec *iovs, int iovused, bool one_resp) {\n    mc_resp *resp = c->resp_head;\n    while (resp && iovused + resp->iovcnt < IOV_MAX-1) {\n        if (resp->skip) {\n            // Don't actually unchain the resp obj here since it's singly-linked.\n            // Just let the post function handle it linearly.\n            resp = resp->next;\n            continue;\n        }\n        if (resp->chunked_data_iov) {\n            // Handle chunked items specially.\n            // They spend much more time in send so we can be a bit wasteful\n            // in rebuilding iovecs for them.\n            item_chunk *ch = (item_chunk *)ITEM_schunk((item *)resp->iov[resp->chunked_data_iov].iov_base);\n            int x;\n            for (x = 0; x < resp->iovcnt; x++) {\n                // This iov is tracking how far we've copied so far.\n                if (x == resp->chunked_data_iov) {\n                    int done = resp->chunked_total - resp->iov[x].iov_len;\n                    // Start from the len to allow binprot to cut the \\r\\n\n                    int todo = resp->iov[x].iov_len;\n                    while (ch && todo > 0 && iovused < IOV_MAX-1) {\n                        int skip = 0;\n                        if (!ch->used) {\n                            ch = ch->next;\n                            continue;\n                        }\n                        // Skip parts we've already sent.\n                        if (done >= ch->used) {\n                            done -= ch->used;\n                            ch = ch->next;\n                            continue;\n                        } else if (done) {\n                            skip = done;\n                            done = 0;\n                        }\n                        iovs[iovused].iov_base = ch->data + skip;\n                        // Stupid binary protocol makes this go negative.\n                        iovs[iovused].iov_len = ch->used - skip > todo ? todo : ch->used - skip;\n                        iovused++;\n                        todo -= ch->used - skip;\n                        ch = ch->next;\n                    }\n                } else {\n                    iovs[iovused].iov_base = resp->iov[x].iov_base;\n                    iovs[iovused].iov_len = resp->iov[x].iov_len;\n                    iovused++;\n                }\n                if (iovused >= IOV_MAX-1)\n                    break;\n            }\n        } else {\n            memcpy(&iovs[iovused], resp->iov, sizeof(struct iovec)*resp->iovcnt);\n            iovused += resp->iovcnt;\n        }\n\n        // done looking at first response, walk down the chain.\n        resp = resp->next;\n        // used for UDP mode: UDP cannot send multiple responses per packet.\n        if (one_resp)\n            break;\n    }\n    return iovused;\n}\n\n/*\n * Decrements and completes responses based on how much data was transmitted.\n * Takes the connection and current result bytes.\n */\nstatic void _transmit_post(conn *c, ssize_t res) {\n    // We've written some of the data. Remove the completed\n    // responses from the list of pending writes.\n    mc_resp *resp = c->resp_head;\n    while (resp) {\n        int x;\n        if (resp->skip) {\n            resp = resp_finish(c, resp);\n            continue;\n        }\n\n        // fastpath check. all small responses should cut here.\n        if (res >= resp->tosend) {\n            res -= resp->tosend;\n            resp = resp_finish(c, resp);\n            continue;\n        }\n\n        // it's fine to re-check iov's that were zeroed out before.\n        for (x = 0; x < resp->iovcnt; x++) {\n            struct iovec *iov = &resp->iov[x];\n            if (res >= iov->iov_len) {\n                resp->tosend -= iov->iov_len;\n                res -= iov->iov_len;\n                iov->iov_len = 0;\n            } else {\n                // Dumb special case for chunked items. Currently tracking\n                // where to inject the chunked item via iov_base.\n                // Extra not-great since chunked items can't be the first\n                // index, so we have to check for non-zero c_d_iov first.\n                if (!resp->chunked_data_iov || x != resp->chunked_data_iov) {\n                    iov->iov_base = (char *)iov->iov_base + res;\n                }\n                iov->iov_len -= res;\n                resp->tosend -= res;\n                res = 0;\n                break;\n            }\n        }\n\n        // are we done with this response object?\n        if (resp->tosend == 0) {\n            resp = resp_finish(c, resp);\n        } else {\n            // Jammed up here. This is the new head.\n            break;\n        }\n    }\n}\n\n/*\n * Transmit the next chunk of data from our list of msgbuf structures.\n *\n * Returns:\n *   TRANSMIT_COMPLETE   All done writing.\n *   TRANSMIT_INCOMPLETE More data remaining to write.\n *   TRANSMIT_SOFT_ERROR Can't write any more right now.\n *   TRANSMIT_HARD_ERROR Can't write (c->state is set to conn_closing)\n */\nstatic enum transmit_result transmit(conn *c) {\n    assert(c != NULL);\n    struct iovec iovs[IOV_MAX];\n    struct msghdr msg;\n    int iovused = 0;\n\n    // init the msg.\n    memset(&msg, 0, sizeof(struct msghdr));\n    msg.msg_iov = iovs;\n\n    iovused = _transmit_pre(c, iovs, iovused, TRANSMIT_ALL_RESP);\n    if (iovused == 0) {\n        // Avoid the syscall if we're only handling a noreply.\n        // Return the response object.\n        _transmit_post(c, 0);\n        return TRANSMIT_COMPLETE;\n    }\n\n    // Alright, send.\n    ssize_t res;\n    msg.msg_iovlen = iovused;\n    res = c->sendmsg(c, &msg, 0);\n    if (res >= 0) {\n        pthread_mutex_lock(&c->thread->stats.mutex);\n        c->thread->stats.bytes_written += res;\n        pthread_mutex_unlock(&c->thread->stats.mutex);\n\n        // Decrement any partial IOV's and complete any finished resp's.\n        _transmit_post(c, res);\n\n        if (c->resp_head) {\n            return TRANSMIT_INCOMPLETE;\n        } else {\n            return TRANSMIT_COMPLETE;\n        }\n    }\n\n    if (res == -1 && (errno == EAGAIN || errno == EWOULDBLOCK)) {\n        if (!update_event(c, EV_WRITE | EV_PERSIST)) {\n            if (settings.verbose > 0)\n                fprintf(stderr, \"Couldn't update event\\n\");\n            conn_set_state(c, conn_closing);\n            return TRANSMIT_HARD_ERROR;\n        }\n        return TRANSMIT_SOFT_ERROR;\n    }\n    /* if res == -1 and error is not EAGAIN or EWOULDBLOCK,\n       we have a real error, on which we close the connection */\n    if (settings.verbose > 0)\n        perror(\"Failed to write, and not due to blocking\");\n\n    conn_set_state(c, conn_closing);\n    return TRANSMIT_HARD_ERROR;\n}\n\nstatic void build_udp_header(unsigned char *hdr, mc_resp *resp) {\n    // We need to communicate the total number of packets\n    // If this isn't set, it's the first time this response is building a udp\n    // header, so \"tosend\" must be static.\n    if (!resp->udp_total) {\n        uint32_t total;\n        total = resp->tosend / UDP_DATA_SIZE;\n        if (resp->tosend % UDP_DATA_SIZE)\n            total++;\n        // The spec doesn't really say what we should do here. It's _probably_\n        // better to bail out?\n        if (total > USHRT_MAX) {\n            total = USHRT_MAX;\n        }\n        resp->udp_total = total;\n    }\n\n    // TODO: why wasn't this hto*'s and casts?\n    // this ends up sending UDP hdr data specifically in host byte order.\n    *hdr++ = resp->request_id / 256;\n    *hdr++ = resp->request_id % 256;\n    *hdr++ = resp->udp_sequence / 256;\n    *hdr++ = resp->udp_sequence % 256;\n    *hdr++ = resp->udp_total / 256;\n    *hdr++ = resp->udp_total % 256;\n    *hdr++ = 0;\n    *hdr++ = 0;\n    resp->udp_sequence++;\n}\n\n/*\n * UDP specific transmit function. Uses its own function rather than check\n * IS_UDP() five times. If we ever implement sendmmsg or similar support they\n * will diverge even more.\n * Does not use TLS.\n *\n * Returns:\n *   TRANSMIT_COMPLETE   All done writing.\n *   TRANSMIT_INCOMPLETE More data remaining to write.\n *   TRANSMIT_SOFT_ERROR Can't write any more right now.\n *   TRANSMIT_HARD_ERROR Can't write (c->state is set to conn_closing)\n */\nstatic enum transmit_result transmit_udp(conn *c) {\n    assert(c != NULL);\n    struct iovec iovs[IOV_MAX];\n    struct msghdr msg;\n    mc_resp *resp;\n    int iovused = 0;\n    unsigned char udp_hdr[UDP_HEADER_SIZE];\n\n    // We only send one UDP packet per call (ugh), so we can only operate on a\n    // single response at a time.\n    resp = c->resp_head;\n\n    if (!resp) {\n        return TRANSMIT_COMPLETE;\n    }\n\n    if (resp->skip) {\n        resp = resp_finish(c, resp);\n        return TRANSMIT_INCOMPLETE;\n    }\n\n    // clear the message and initialize it.\n    memset(&msg, 0, sizeof(struct msghdr));\n    msg.msg_iov = iovs;\n\n    // the UDP source to return to.\n    msg.msg_name = &resp->request_addr;\n    msg.msg_namelen = resp->request_addr_size;\n\n    // First IOV is the custom UDP header.\n    iovs[0].iov_base = (void *)udp_hdr;\n    iovs[0].iov_len = UDP_HEADER_SIZE;\n    build_udp_header(udp_hdr, resp);\n    iovused++;\n\n    // Fill the IOV's the standard way.\n    // TODO: might get a small speedup if we let it break early with a length\n    // limit.\n    iovused = _transmit_pre(c, iovs, iovused, TRANSMIT_ONE_RESP);\n\n    // Clip the IOV's to the max UDP packet size.\n    // If we add support for send_mmsg, this can be where we split msg's.\n    {\n        int x = 0;\n        int len = 0;\n        for (x = 0; x < iovused; x++) {\n            if (len + iovs[x].iov_len >= UDP_MAX_PAYLOAD_SIZE) {\n                iovs[x].iov_len = UDP_MAX_PAYLOAD_SIZE - len;\n                x++;\n                break;\n            } else {\n                len += iovs[x].iov_len;\n            }\n        }\n        iovused = x;\n    }\n\n    ssize_t res;\n    msg.msg_iovlen = iovused;\n    // NOTE: uses system sendmsg since we have no support for indirect UDP.\n    res = sendmsg(c->sfd, &msg, 0);\n    if (res >= 0) {\n        pthread_mutex_lock(&c->thread->stats.mutex);\n        c->thread->stats.bytes_written += res;\n        pthread_mutex_unlock(&c->thread->stats.mutex);\n\n        // Ignore the header size from forwarding the IOV's\n        res -= UDP_HEADER_SIZE;\n\n        // Decrement any partial IOV's and complete any finished resp's.\n        _transmit_post(c, res);\n\n        if (c->resp_head) {\n            return TRANSMIT_INCOMPLETE;\n        } else {\n            return TRANSMIT_COMPLETE;\n        }\n    }\n\n    if (res == -1 && (errno == EAGAIN || errno == EWOULDBLOCK)) {\n        if (!update_event(c, EV_WRITE | EV_PERSIST)) {\n            if (settings.verbose > 0)\n                fprintf(stderr, \"Couldn't update event\\n\");\n            conn_set_state(c, conn_closing);\n            return TRANSMIT_HARD_ERROR;\n        }\n        return TRANSMIT_SOFT_ERROR;\n    }\n    /* if res == -1 and error is not EAGAIN or EWOULDBLOCK,\n       we have a real error, on which we close the connection */\n    if (settings.verbose > 0)\n        perror(\"Failed to write, and not due to blocking\");\n\n    conn_set_state(c, conn_read);\n    return TRANSMIT_HARD_ERROR;\n}\n\n\n/* Does a looped read to fill data chunks */\n/* TODO: restrict number of times this can loop.\n * Also, benchmark using readv's.\n */\nstatic int read_into_chunked_item(conn *c) {\n    int total = 0;\n    int res;\n    assert(c->rcurr != c->ritem);\n\n    while (c->rlbytes > 0) {\n        item_chunk *ch = (item_chunk *)c->ritem;\n        if (ch->size == ch->used) {\n            // FIXME: ch->next is currently always 0. remove this?\n            if (ch->next) {\n                c->ritem = (char *) ch->next;\n            } else {\n                /* Allocate next chunk. Binary protocol needs 2b for \\r\\n */\n                c->ritem = (char *) do_item_alloc_chunk(ch, c->rlbytes +\n                       ((c->protocol == binary_prot) ? 2 : 0));\n                if (!c->ritem) {\n                    // We failed an allocation. Let caller handle cleanup.\n                    total = -2;\n                    break;\n                }\n                // ritem has new chunk, restart the loop.\n                continue;\n                //assert(c->rlbytes == 0);\n            }\n        }\n\n        int unused = ch->size - ch->used;\n        /* first check if we have leftovers in the conn_read buffer */\n        if (c->rbytes > 0) {\n            total = 0;\n            int tocopy = c->rbytes > c->rlbytes ? c->rlbytes : c->rbytes;\n            tocopy = tocopy > unused ? unused : tocopy;\n            if (c->ritem != c->rcurr) {\n                memmove(ch->data + ch->used, c->rcurr, tocopy);\n            }\n            total += tocopy;\n            c->rlbytes -= tocopy;\n            c->rcurr += tocopy;\n            c->rbytes -= tocopy;\n            ch->used += tocopy;\n            if (c->rlbytes == 0) {\n                break;\n            }\n        } else {\n            /*  now try reading from the socket */\n            res = c->read(c, ch->data + ch->used,\n                    (unused > c->rlbytes ? c->rlbytes : unused));\n            if (res > 0) {\n                pthread_mutex_lock(&c->thread->stats.mutex);\n                c->thread->stats.bytes_read += res;\n                pthread_mutex_unlock(&c->thread->stats.mutex);\n                ch->used += res;\n                total += res;\n                c->rlbytes -= res;\n            } else {\n                /* Reset total to the latest result so caller can handle it */\n                total = res;\n                break;\n            }\n        }\n    }\n\n    /* At some point I will be able to ditch the \\r\\n from item storage and\n       remove all of these kludges.\n       The above binprot check ensures inline space for \\r\\n, but if we do\n       exactly enough allocs there will be no additional chunk for \\r\\n.\n     */\n    if (c->rlbytes == 0 && c->protocol == binary_prot && total >= 0) {\n        item_chunk *ch = (item_chunk *)c->ritem;\n        if (ch->size - ch->used < 2) {\n            c->ritem = (char *) do_item_alloc_chunk(ch, 2);\n            if (!c->ritem) {\n                total = -2;\n            }\n        }\n    }\n    return total;\n}\n\nstatic void drive_machine(conn *c) {\n    bool stop = false;\n    int sfd;\n    socklen_t addrlen;\n    struct sockaddr_storage addr;\n    int nreqs = settings.reqs_per_event;\n    int res;\n    const char *str;\n#ifdef HAVE_ACCEPT4\n    static int  use_accept4 = 1;\n#else\n    static int  use_accept4 = 0;\n#endif\n\n    assert(c != NULL);\n\n    while (!stop) {\n\n        switch(c->state) {\n        case conn_listening:\n            addrlen = sizeof(addr);\n#ifdef HAVE_ACCEPT4\n            if (use_accept4) {\n                sfd = accept4(c->sfd, (struct sockaddr *)&addr, &addrlen, SOCK_NONBLOCK);\n            } else {\n                sfd = accept(c->sfd, (struct sockaddr *)&addr, &addrlen);\n            }\n#else\n            sfd = accept(c->sfd, (struct sockaddr *)&addr, &addrlen);\n#endif\n            if (sfd == -1) {\n                if (use_accept4 && errno == ENOSYS) {\n                    use_accept4 = 0;\n                    continue;\n                }\n                perror(use_accept4 ? \"accept4()\" : \"accept()\");\n                if (errno == EAGAIN || errno == EWOULDBLOCK) {\n                    /* these are transient, so don't log anything */\n                    stop = true;\n                } else if (errno == EMFILE) {\n                    if (settings.verbose > 0)\n                        fprintf(stderr, \"Too many open connections\\n\");\n                    accept_new_conns(false);\n                    stop = true;\n                } else {\n                    perror(\"accept()\");\n                    stop = true;\n                }\n                break;\n            }\n            if (!use_accept4) {\n                if (fcntl(sfd, F_SETFL, fcntl(sfd, F_GETFL) | O_NONBLOCK) < 0) {\n                    perror(\"setting O_NONBLOCK\");\n                    close(sfd);\n                    break;\n                }\n            }\n\n            bool reject;\n            if (settings.maxconns_fast) {\n                reject = sfd >= settings.maxconns - 1;\n                if (reject) {\n                    STATS_LOCK();\n                    stats.rejected_conns++;\n                    STATS_UNLOCK();\n                }\n            } else {\n                reject = false;\n            }\n\n            if (reject) {\n                str = \"ERROR Too many open connections\\r\\n\";\n                res = write(sfd, str, strlen(str));\n                close(sfd);\n            } else {\n                // run accept routine if ssl is compiled + enabled\n                bool fail = false;\n                void *ssl_v = ssl_accept(c, sfd, &fail);\n                if (fail) {\n                    close(sfd);\n                    break;\n                }\n\n                dispatch_conn_new(sfd, conn_new_cmd, EV_READ | EV_PERSIST,\n                                     READ_BUFFER_CACHED, c->transport, ssl_v, c->tag, c->protocol);\n            }\n\n            stop = true;\n            break;\n\n        case conn_waiting:\n            rbuf_release(c);\n            if (!update_event(c, EV_READ | EV_PERSIST)) {\n                if (settings.verbose > 0)\n                    fprintf(stderr, \"Couldn't update event\\n\");\n                conn_set_state(c, conn_closing);\n                break;\n            }\n\n            conn_set_state(c, conn_read);\n            stop = true;\n            break;\n\n        case conn_read:\n            if (!IS_UDP(c->transport)) {\n                // Assign a read buffer if necessary.\n                if (!rbuf_alloc(c)) {\n                    // TODO: Some way to allow for temporary failures.\n                    conn_set_state(c, conn_closing);\n                    break;\n                }\n                res = try_read_network(c);\n            } else {\n                // UDP connections always have a static buffer.\n                res = try_read_udp(c);\n            }\n\n            switch (res) {\n            case READ_NO_DATA_RECEIVED:\n                conn_set_state(c, conn_waiting);\n                break;\n            case READ_DATA_RECEIVED:\n                conn_set_state(c, conn_parse_cmd);\n                break;\n            case READ_ERROR:\n                conn_set_state(c, conn_closing);\n                break;\n            case READ_MEMORY_ERROR: /* Failed to allocate more memory */\n                /* State already set by try_read_network */\n                break;\n            }\n            break;\n\n        case conn_parse_cmd:\n            c->noreply = false;\n            if (c->try_read_command(c) == 0) {\n                /* we need more data! */\n                if (c->resp_head) {\n                    // Buffered responses waiting, flush in the meantime.\n                    conn_set_state(c, conn_mwrite);\n                } else {\n                    conn_set_state(c, conn_waiting);\n                }\n            }\n\n            break;\n\n        case conn_new_cmd:\n            /* Only process nreqs at a time to avoid starving other\n               connections */\n\n            --nreqs;\n            if (nreqs >= 0) {\n                reset_cmd_handler(c);\n            } else if (c->resp_head) {\n                // flush response pipe on yield.\n                conn_set_state(c, conn_mwrite);\n            } else {\n                pthread_mutex_lock(&c->thread->stats.mutex);\n                c->thread->stats.conn_yields++;\n                pthread_mutex_unlock(&c->thread->stats.mutex);\n                if (c->rbytes > 0) {\n                    /* We have already read in data into the input buffer,\n                       so libevent will most likely not signal read events\n                       on the socket (unless more data is available. As a\n                       hack we should just put in a request to write data,\n                       because that should be possible ;-)\n                    */\n                    if (!update_event(c, EV_WRITE | EV_PERSIST)) {\n                        if (settings.verbose > 0)\n                            fprintf(stderr, \"Couldn't update event\\n\");\n                        conn_set_state(c, conn_closing);\n                        break;\n                    }\n                }\n                stop = true;\n            }\n            break;\n\n        case conn_nread:\n            if (c->rlbytes == 0) {\n                complete_nread(c);\n                break;\n            }\n\n            /* Check if rbytes < 0, to prevent crash */\n            if (c->rlbytes < 0) {\n                if (settings.verbose) {\n                    fprintf(stderr, \"Invalid rlbytes to read: len %d\\n\", c->rlbytes);\n                }\n                conn_set_state(c, conn_closing);\n                break;\n            }\n\n            if (c->item_malloced || ((((item *)c->item)->it_flags & ITEM_CHUNKED) == 0) ) {\n                /* first check if we have leftovers in the conn_read buffer */\n                if (c->rbytes > 0) {\n                    int tocopy = c->rbytes > c->rlbytes ? c->rlbytes : c->rbytes;\n                    memmove(c->ritem, c->rcurr, tocopy);\n                    c->ritem += tocopy;\n                    c->rlbytes -= tocopy;\n                    c->rcurr += tocopy;\n                    c->rbytes -= tocopy;\n                    if (c->rlbytes == 0) {\n                        break;\n                    }\n                }\n\n                /*  now try reading from the socket */\n                res = c->read(c, c->ritem, c->rlbytes);\n                if (res > 0) {\n                    pthread_mutex_lock(&c->thread->stats.mutex);\n                    c->thread->stats.bytes_read += res;\n                    pthread_mutex_unlock(&c->thread->stats.mutex);\n                    if (c->rcurr == c->ritem) {\n                        c->rcurr += res;\n                    }\n                    c->ritem += res;\n                    c->rlbytes -= res;\n                    break;\n                }\n            } else {\n                res = read_into_chunked_item(c);\n                if (res > 0)\n                    break;\n            }\n\n            if (res == 0) { /* end of stream */\n                c->close_reason = NORMAL_CLOSE;\n                conn_set_state(c, conn_closing);\n                break;\n            }\n\n            if (res == -1 && (errno == EAGAIN || errno == EWOULDBLOCK)) {\n                if (!update_event(c, EV_READ | EV_PERSIST)) {\n                    if (settings.verbose > 0)\n                        fprintf(stderr, \"Couldn't update event\\n\");\n                    conn_set_state(c, conn_closing);\n                    break;\n                }\n                stop = true;\n                break;\n            }\n\n            /* Memory allocation failure */\n            if (res == -2) {\n                out_of_memory(c, \"SERVER_ERROR Out of memory during read\");\n                c->sbytes = c->rlbytes;\n                conn_set_state(c, conn_swallow);\n                // Ensure this flag gets cleared. It gets killed on conn_new()\n                // so any conn_closing is fine, calling complete_nread is\n                // fine. This swallow semms to be the only other case.\n                c->set_stale = false;\n                c->mset_res = false;\n                break;\n            }\n            /* otherwise we have a real error, on which we close the connection */\n            if (settings.verbose > 0) {\n                fprintf(stderr, \"Failed to read, and not due to blocking:\\n\"\n                        \"errno: %d %s \\n\"\n                        \"rcurr=%p ritem=%p rbuf=%p rlbytes=%d rsize=%d\\n\",\n                        errno, strerror(errno),\n                        (void *)c->rcurr, (void *)c->ritem, (void *)c->rbuf,\n                        (int)c->rlbytes, (int)c->rsize);\n            }\n            conn_set_state(c, conn_closing);\n            break;\n\n        case conn_swallow:\n            /* we are reading sbytes and throwing them away */\n            if (c->sbytes <= 0) {\n                conn_set_state(c, conn_new_cmd);\n                break;\n            }\n\n            /* first check if we have leftovers in the conn_read buffer */\n            if (c->rbytes > 0) {\n                int tocopy = c->rbytes > c->sbytes ? c->sbytes : c->rbytes;\n                c->sbytes -= tocopy;\n                c->rcurr += tocopy;\n                c->rbytes -= tocopy;\n                break;\n            }\n\n            /*  now try reading from the socket */\n            res = c->read(c, c->rbuf, c->rsize > c->sbytes ? c->sbytes : c->rsize);\n            if (res > 0) {\n                pthread_mutex_lock(&c->thread->stats.mutex);\n                c->thread->stats.bytes_read += res;\n                pthread_mutex_unlock(&c->thread->stats.mutex);\n                c->sbytes -= res;\n                break;\n            }\n            if (res == 0) { /* end of stream */\n                c->close_reason = NORMAL_CLOSE;\n                conn_set_state(c, conn_closing);\n                break;\n            }\n            if (res == -1 && (errno == EAGAIN || errno == EWOULDBLOCK)) {\n                if (!update_event(c, EV_READ | EV_PERSIST)) {\n                    if (settings.verbose > 0)\n                        fprintf(stderr, \"Couldn't update event\\n\");\n                    conn_set_state(c, conn_closing);\n                    break;\n                }\n                stop = true;\n                break;\n            }\n            /* otherwise we have a real error, on which we close the connection */\n            if (settings.verbose > 0)\n                fprintf(stderr, \"Failed to read, and not due to blocking\\n\");\n            conn_set_state(c, conn_closing);\n            break;\n\n        case conn_write:\n        case conn_mwrite:\n            /* have side IO's that must process before transmit() can run.\n             * remove the connection from the worker thread and dispatch the\n             * IO queue\n             */\n            assert(c->io_queues_submitted == 0);\n\n            for (io_queue_t *q = c->io_queues; q->type != IO_QUEUE_NONE; q++) {\n                if (q->stack_ctx != NULL) {\n                    io_queue_cb_t *qcb = thread_io_queue_get(c->thread, q->type);\n                    qcb->submit_cb(q);\n                    c->io_queues_submitted++;\n                }\n            }\n            if (c->io_queues_submitted != 0) {\n                conn_set_state(c, conn_io_queue);\n\n                stop = true;\n                break;\n            }\n\n            switch (!IS_UDP(c->transport) ? transmit(c) : transmit_udp(c)) {\n            case TRANSMIT_COMPLETE:\n                if (c->state == conn_mwrite) {\n                    // Free up IO wraps and any half-uploaded items.\n                    conn_release_items(c);\n                    conn_set_state(c, conn_new_cmd);\n                    if (c->close_after_write) {\n                        conn_set_state(c, conn_closing);\n                    }\n                } else {\n                    if (settings.verbose > 0)\n                        fprintf(stderr, \"Unexpected state %d\\n\", c->state);\n                    conn_set_state(c, conn_closing);\n                }\n                break;\n\n            case TRANSMIT_INCOMPLETE:\n            case TRANSMIT_HARD_ERROR:\n                break;                   /* Continue in state machine. */\n\n            case TRANSMIT_SOFT_ERROR:\n                stop = true;\n                break;\n            }\n            break;\n\n        case conn_closing:\n            if (IS_UDP(c->transport))\n                conn_cleanup(c);\n            else\n                conn_close(c);\n            stop = true;\n            break;\n\n        case conn_closed:\n            /* This only happens if dormando is an idiot. */\n            abort();\n            break;\n\n        case conn_watch:\n            /* We handed off our connection to the logger thread. */\n            stop = true;\n            break;\n        case conn_io_queue:\n            /* Woke up while waiting for an async return, but not ready. */\n            event_del(&c->event);\n            conn_set_state(c, conn_io_pending);\n            stop = true;\n            break;\n        case conn_io_pending:\n            /* Should not be reachable */\n            assert(false);\n            break;\n        case conn_io_resume:\n            /* Complete our queued IO's from within the worker thread. */\n            conn_set_state(c, conn_mwrite);\n            break;\n        case conn_max_state:\n            assert(false);\n            break;\n        }\n    }\n\n    return;\n}\n\nvoid event_handler(const evutil_socket_t fd, const short which, void *arg) {\n    conn *c;\n\n    c = (conn *)arg;\n    assert(c != NULL);\n\n    c->which = which;\n\n    /* sanity */\n    if (fd != c->sfd) {\n        if (settings.verbose > 0)\n            fprintf(stderr, \"Catastrophic: event fd doesn't match conn fd!\\n\");\n        conn_close(c);\n        return;\n    }\n\n    drive_machine(c);\n\n    /* wait for next event */\n    return;\n}\n\nstatic int new_socket(struct addrinfo *ai) {\n    int sfd;\n    int flags;\n\n    if ((sfd = socket(ai->ai_family, ai->ai_socktype, ai->ai_protocol)) == -1) {\n        return -1;\n    }\n\n    if ((flags = fcntl(sfd, F_GETFL, 0)) < 0 ||\n        fcntl(sfd, F_SETFL, flags | O_NONBLOCK) < 0) {\n        perror(\"setting O_NONBLOCK\");\n        close(sfd);\n        return -1;\n    }\n    return sfd;\n}\n\n\n/*\n * Sets a socket's send buffer size to the maximum allowed by the system.\n */\nstatic void maximize_sndbuf(const int sfd) {\n    socklen_t intsize = sizeof(int);\n    int last_good = 0;\n    int min, max, avg;\n    int old_size;\n\n    /* Start with the default size. */\n#ifdef _WIN32\n    if (getsockopt((SOCKET)sfd, SOL_SOCKET, SO_SNDBUF, (char *)&old_size, &intsize) != 0) {\n#else\n    if (getsockopt(sfd, SOL_SOCKET, SO_SNDBUF, &old_size, &intsize) != 0) {\n#endif /* #ifdef _WIN32 */\n        if (settings.verbose > 0)\n            perror(\"getsockopt(SO_SNDBUF)\");\n        return;\n    }\n\n    /* Binary-search for the real maximum. */\n    min = old_size;\n    max = MAX_SENDBUF_SIZE;\n\n    while (min <= max) {\n        avg = ((unsigned int)(min + max)) / 2;\n        if (setsockopt(sfd, SOL_SOCKET, SO_SNDBUF, (void *)&avg, intsize) == 0) {\n            last_good = avg;\n            min = avg + 1;\n        } else {\n            max = avg - 1;\n        }\n    }\n\n    if (settings.verbose > 1)\n        fprintf(stderr, \"<%d send buffer was %d, now %d\\n\", sfd, old_size, last_good);\n}\n\n/**\n * Create a socket and bind it to a specific port number\n * @param interface the interface to bind to\n * @param port the port number to bind to\n * @param transport the transport protocol (TCP / UDP)\n * @param portnumber_file A filepointer to write the port numbers to\n *        when they are successfully added to the list of ports we\n *        listen on.\n */\nstatic int server_socket(const char *interface,\n                         int port,\n                         enum network_transport transport,\n                         FILE *portnumber_file, uint8_t ssl_enabled,\n                         uint64_t conntag,\n                         enum protocol bproto) {\n    int sfd;\n    struct linger ling = {0, 0};\n    struct addrinfo *ai;\n    struct addrinfo *next;\n    struct addrinfo hints = { .ai_flags = AI_PASSIVE,\n                              .ai_family = AF_UNSPEC };\n    char port_buf[NI_MAXSERV];\n    int error;\n    int success = 0;\n    int flags =1;\n\n    hints.ai_socktype = IS_UDP(transport) ? SOCK_DGRAM : SOCK_STREAM;\n\n    if (port == -1) {\n        port = 0;\n    }\n    snprintf(port_buf, sizeof(port_buf), \"%d\", port);\n    error= getaddrinfo(interface, port_buf, &hints, &ai);\n    if (error != 0) {\n        if (error != EAI_SYSTEM)\n          fprintf(stderr, \"getaddrinfo(): %s\\n\", gai_strerror(error));\n        else\n          perror(\"getaddrinfo()\");\n        return 1;\n    }\n\n    for (next= ai; next; next= next->ai_next) {\n        conn *listen_conn_add;\n        if ((sfd = new_socket(next)) == -1) {\n            /* getaddrinfo can return \"junk\" addresses,\n             * we make sure at least one works before erroring.\n             */\n            if (errno == EMFILE) {\n                /* ...unless we're out of fds */\n                perror(\"server_socket\");\n                exit(EX_OSERR);\n            }\n            continue;\n        }\n\n        if (settings.num_napi_ids) {\n            socklen_t len = sizeof(socklen_t);\n            int napi_id;\n            error = getsockopt(sfd, SOL_SOCKET, SO_INCOMING_NAPI_ID, &napi_id, &len);\n            if (error != 0) {\n                fprintf(stderr, \"-N <num_napi_ids> option not supported\\n\");\n                exit(EXIT_FAILURE);\n            }\n        }\n\n#ifdef IPV6_V6ONLY\n        if (next->ai_family == AF_INET6) {\n            error = setsockopt(sfd, IPPROTO_IPV6, IPV6_V6ONLY, (char *) &flags, sizeof(flags));\n            if (error != 0) {\n                perror(\"setsockopt\");\n                close(sfd);\n                continue;\n            }\n        }\n#endif\n#ifdef SOCK_COOKIE_ID\n        if (settings.sock_cookie_id != 0) {\n            error = setsockopt(sfd, SOL_SOCKET, SOCK_COOKIE_ID, (void *)&settings.sock_cookie_id, sizeof(uint32_t));\n            if (error != 0)\n                perror(\"setsockopt\");\n        }\n#endif\n\n        setsockopt(sfd, SOL_SOCKET, SO_REUSEADDR, (void *)&flags, sizeof(flags));\n        if (IS_UDP(transport)) {\n            maximize_sndbuf(sfd);\n        } else {\n            error = setsockopt(sfd, SOL_SOCKET, SO_KEEPALIVE, (void *)&flags, sizeof(flags));\n            if (error != 0)\n                perror(\"setsockopt\");\n\n            error = setsockopt(sfd, SOL_SOCKET, SO_LINGER, (void *)&ling, sizeof(ling));\n            if (error != 0)\n                perror(\"setsockopt\");\n\n            error = setsockopt(sfd, IPPROTO_TCP, TCP_NODELAY, (void *)&flags, sizeof(flags));\n            if (error != 0)\n                perror(\"setsockopt\");\n        }\n\n        if (bind(sfd, next->ai_addr, next->ai_addrlen) == -1) {\n            if (errno != EADDRINUSE) {\n                perror(\"bind()\");\n                close(sfd);\n                freeaddrinfo(ai);\n                return 1;\n            }\n            close(sfd);\n            continue;\n        } else {\n            success++;\n            if (!IS_UDP(transport) && listen(sfd, settings.backlog) == -1) {\n                perror(\"listen()\");\n                close(sfd);\n                freeaddrinfo(ai);\n                return 1;\n            }\n            if (portnumber_file != NULL &&\n                (next->ai_addr->sa_family == AF_INET ||\n                 next->ai_addr->sa_family == AF_INET6)) {\n                union {\n                    struct sockaddr_in in;\n                    struct sockaddr_in6 in6;\n                } my_sockaddr;\n                socklen_t len = sizeof(my_sockaddr);\n                if (getsockname(sfd, (struct sockaddr*)&my_sockaddr, &len)==0) {\n                    if (next->ai_addr->sa_family == AF_INET) {\n                        fprintf(portnumber_file, \"%s INET: %u\\n\",\n                                IS_UDP(transport) ? \"UDP\" : \"TCP\",\n                                ntohs(my_sockaddr.in.sin_port));\n                    } else {\n                        fprintf(portnumber_file, \"%s INET6: %u\\n\",\n                                IS_UDP(transport) ? \"UDP\" : \"TCP\",\n                                ntohs(my_sockaddr.in6.sin6_port));\n                    }\n                }\n            }\n        }\n\n        if (IS_UDP(transport)) {\n            int c;\n\n            for (c = 0; c < settings.num_threads_per_udp; c++) {\n                /* Allocate one UDP file descriptor per worker thread;\n                 * this allows \"stats conns\" to separately list multiple\n                 * parallel UDP requests in progress.\n                 *\n                 * The dispatch code round-robins new connection requests\n                 * among threads, so this is guaranteed to assign one\n                 * FD to each thread.\n                 */\n                int per_thread_fd;\n                if (c == 0) {\n                    per_thread_fd = sfd;\n                } else {\n                    per_thread_fd = dup(sfd);\n                    if (per_thread_fd < 0) {\n                        perror(\"Failed to duplicate file descriptor\");\n                        exit(EXIT_FAILURE);\n                    }\n                }\n                dispatch_conn_new(per_thread_fd, conn_read,\n                                  EV_READ | EV_PERSIST,\n                                  UDP_READ_BUFFER_SIZE, transport, NULL, conntag, bproto);\n            }\n        } else {\n            if (!(listen_conn_add = conn_new(sfd, conn_listening,\n                                             EV_READ | EV_PERSIST, 1,\n                                             transport, main_base, NULL, conntag, bproto))) {\n                fprintf(stderr, \"failed to create listening connection\\n\");\n                exit(EXIT_FAILURE);\n            }\n#ifdef TLS\n            listen_conn_add->ssl_enabled = ssl_enabled;\n#else\n            assert(ssl_enabled == false);\n#endif\n            listen_conn_add->next = listen_conn;\n            listen_conn = listen_conn_add;\n        }\n    }\n\n    freeaddrinfo(ai);\n\n    /* Return zero iff we detected no errors in starting up connections */\n    return success == 0;\n}\n\nstatic int server_sockets(int port, enum network_transport transport,\n                          FILE *portnumber_file) {\n    uint8_t ssl_enabled = MC_SSL_DISABLED;\n\n    const char *notls = \"notls\";\n    const char *btls = \"btls\";\n    const char *mtls = \"mtls\";\n    if (settings.ssl_enabled) {\n        ssl_enabled = MC_SSL_ENABLED_DEFAULT;\n    }\n\n    if (settings.inter == NULL) {\n        return server_socket(settings.inter, port, transport, portnumber_file, ssl_enabled, 0, settings.binding_protocol);\n    } else {\n        // tokenize them and bind to each one of them..\n        char *b;\n        int ret = 0;\n        char *list = strdup(settings.inter);\n\n        if (list == NULL) {\n            fprintf(stderr, \"Failed to allocate memory for parsing server interface string\\n\");\n            return 1;\n        }\n        // If we encounter any failure, preserve the first errno for the caller.\n        int errno_save = 0;\n        for (char *p = strtok_r(list, \";,\", &b);\n            p != NULL;\n            p = strtok_r(NULL, \";,\", &b)) {\n            uint64_t conntag = 0;\n            int the_port = port;\n            if (settings.ssl_enabled) {\n                ssl_enabled = MC_SSL_ENABLED_DEFAULT;\n            }\n            // \"notls\" option is valid only when memcached is run with SSL enabled.\n            if (strncmp(p, notls, strlen(notls)) == 0) {\n                if (!settings.ssl_enabled) {\n                    fprintf(stderr, \"'notls' option is valid only when SSL is enabled\\n\");\n                    free(list);\n                    return 1;\n                }\n                ssl_enabled = MC_SSL_DISABLED;\n                p += strlen(notls) + 1;\n            } else if (strncmp(p, btls, strlen(btls)) == 0) {\n                 if (!settings.ssl_enabled) {\n                    fprintf(stderr, \"'btls' option is valid only when SSL is enabled\\n\");\n                    free(list);\n                    return 1;\n                }\n                ssl_enabled = MC_SSL_ENABLED_NOPEER;\n                p += strlen(btls) + 1;\n            } else if (strncmp(p, mtls, strlen(mtls)) == 0) {\n                if (!settings.ssl_enabled) {\n                    fprintf(stderr, \"'otls' option is valid only when SSL is enabled\\n\");\n                    free(list);\n                    return 1;\n                }\n                ssl_enabled = MC_SSL_ENABLED_PEER;\n                p += strlen(mtls) + 1;\n            }\n\n            // Allow forcing the protocol of this listener.\n            const char *protostr = \"proto\";\n            enum protocol bproto = settings.binding_protocol;\n            if (strncmp(p, protostr, strlen(protostr)) == 0) {\n                p += strlen(protostr);\n                if (*p == '[') {\n                    char *e = strchr(p, ']');\n                    if (e == NULL) {\n                        fprintf(stderr, \"Invalid protocol spec: \\\"%s\\\"\\n\", p);\n                        free(list);\n                        return 1;\n                    }\n                    char *st = ++p; // skip '[';\n                    *e = '\\0';\n                    size_t len = e - st;\n                    p = ++e; // skip ']'\n                    p++; // skip an assumed ':'\n\n                    if (strncmp(st, \"ascii\", len) == 0) {\n                        bproto = ascii_prot;\n                    } else if (strncmp(st, \"binary\", len) == 0) {\n                        bproto = binary_prot;\n                    } else if (strncmp(st, \"negotiating\", len) == 0) {\n                        bproto = negotiating_prot;\n                    } else if (strncmp(st, \"proxy\", len) == 0) {\n#ifdef PROXY\n                        if (settings.proxy_enabled) {\n                            bproto = proxy_prot;\n                        } else {\n                            fprintf(stderr, \"Proxy must be enabled to use: \\\"%s\\\"\\n\", list);\n                            free(list);\n                            return 1;\n                        }\n#else\n                        fprintf(stderr, \"Server not built with proxy: \\\"%s\\\"\\n\", list);\n                        free(list);\n                        return 1;\n#endif\n                    }\n                }\n            }\n\n            const char *tagstr = \"tag\";\n            if (strncmp(p, tagstr, strlen(tagstr)) == 0) {\n                p += strlen(tagstr);\n                // NOTE: should probably retire the [] dumbassery. those're\n                // shell characters.\n                if (*p == '[' || *p == '_') {\n                    char *e = strchr(p, ']');\n                    if (e == NULL) {\n                        e = strchr(p+1, '_');\n                    }\n                    if (e == NULL) {\n                        fprintf(stderr, \"Invalid tag in socket config: \\\"%s\\\"\\n\", p);\n                        free(list);\n                        return 1;\n                    }\n                    char *st = ++p; // skip '['\n                    *e = '\\0';\n                    size_t len = e - st;\n                    p = ++e; // skip ']'\n                    p++; // skip an assumed ':'\n\n                    // validate the tag and copy it in.\n                    if (len > 8 || len < 1) {\n                        fprintf(stderr, \"Listener tags must be between 1 and 8 characters: \\\"%s\\\"\\n\", st);\n                        free(list);\n                        return 1;\n                    }\n\n                    // C programmers love turning string comparisons into\n                    // integer comparisons.\n                    memcpy(&conntag, st, len);\n                }\n            }\n\n            char *h = NULL;\n            if (*p == '[') {\n                // expecting it to be an IPv6 address enclosed in []\n                // i.e. RFC3986 style recommended by RFC5952\n                char *e = strchr(p, ']');\n                if (e == NULL) {\n                    fprintf(stderr, \"Invalid IPV6 address: \\\"%s\\\"\", p);\n                    free(list);\n                    return 1;\n                }\n                h = ++p; // skip the opening '['\n                *e = '\\0';\n                p = ++e; // skip the closing ']'\n            }\n\n            char *s = strchr(p, ':');\n            if (s != NULL) {\n                // If no more semicolons - attempt to treat as port number.\n                // Otherwise the only valid option is an unenclosed IPv6 without port, until\n                // of course there was an RFC3986 IPv6 address previously specified -\n                // in such a case there is no good option, will just send it to fail as port number.\n                if (strchr(s + 1, ':') == NULL || h != NULL) {\n                    *s = '\\0';\n                    ++s;\n                    if (!safe_strtol(s, &the_port)) {\n                        fprintf(stderr, \"Invalid port number: \\\"%s\\\"\\n\", s);\n                        free(list);\n                        return 1;\n                    }\n                }\n            }\n\n            if (h != NULL)\n                p = h;\n\n            if (strcmp(p, \"*\") == 0) {\n                p = NULL;\n            }\n            ret |= server_socket(p, the_port, transport, portnumber_file, ssl_enabled, conntag, bproto);\n            if (ret != 0 && errno_save == 0) errno_save = errno;\n        }\n        free(list);\n        errno = errno_save;\n        return ret;\n    }\n}\n\n#ifndef DISABLE_UNIX_SOCKET\nstatic int new_socket_unix(void) {\n    int sfd;\n    int flags;\n\n    if ((sfd = socket(AF_UNIX, SOCK_STREAM, 0)) == -1) {\n        perror(\"socket()\");\n        return -1;\n    }\n\n    if ((flags = fcntl(sfd, F_GETFL, 0)) < 0 ||\n        fcntl(sfd, F_SETFL, flags | O_NONBLOCK) < 0) {\n        perror(\"setting O_NONBLOCK\");\n        close(sfd);\n        return -1;\n    }\n    return sfd;\n}\n\nstatic int server_socket_unix(const char *path, int access_mask) {\n    int sfd;\n    struct linger ling = {0, 0};\n    struct sockaddr_un addr;\n    struct stat tstat;\n    int flags =1;\n    int old_umask;\n\n    if (!path) {\n        return 1;\n    }\n\n    if ((sfd = new_socket_unix()) == -1) {\n        return 1;\n    }\n\n    /*\n     * Clean up a previous socket file if we left it around\n     */\n    if (lstat(path, &tstat) == 0) {\n        if (S_ISSOCK(tstat.st_mode))\n            unlink(path);\n    }\n\n    setsockopt(sfd, SOL_SOCKET, SO_REUSEADDR, (void *)&flags, sizeof(flags));\n    setsockopt(sfd, SOL_SOCKET, SO_KEEPALIVE, (void *)&flags, sizeof(flags));\n    setsockopt(sfd, SOL_SOCKET, SO_LINGER, (void *)&ling, sizeof(ling));\n\n    /*\n     * the memset call clears nonstandard fields in some implementations\n     * that otherwise mess things up.\n     */\n    memset(&addr, 0, sizeof(addr));\n\n    addr.sun_family = AF_UNIX;\n    strncpy(addr.sun_path, path, sizeof(addr.sun_path) - 1);\n    assert(strcmp(addr.sun_path, path) == 0);\n    old_umask = umask( ~(access_mask&0777));\n    if (bind(sfd, (struct sockaddr *)&addr, sizeof(addr)) == -1) {\n        perror(\"bind()\");\n        close(sfd);\n        umask(old_umask);\n        return 1;\n    }\n    umask(old_umask);\n    if (listen(sfd, settings.backlog) == -1) {\n        perror(\"listen()\");\n        close(sfd);\n        return 1;\n    }\n    if (!(listen_conn = conn_new(sfd, conn_listening,\n                                 EV_READ | EV_PERSIST, 1,\n                                 local_transport, main_base, NULL, 0, settings.binding_protocol))) {\n        fprintf(stderr, \"failed to create listening connection\\n\");\n        exit(EXIT_FAILURE);\n    }\n\n    return 0;\n}\n#else\n#define server_socket_unix(path, access_mask)   -1\n#endif /* #ifndef DISABLE_UNIX_SOCKET */\n\n/*\n * We keep the current time of day in a global variable that's updated by a\n * timer event. This saves us a bunch of time() system calls (we really only\n * need to get the time once a second, whereas there can be tens of thousands\n * of requests a second) and allows us to use server-start-relative timestamps\n * rather than absolute UNIX timestamps, a space savings on systems where\n * sizeof(time_t) > sizeof(unsigned int).\n */\nvolatile rel_time_t current_time;\nstatic struct event clockevent;\n#ifdef MEMCACHED_DEBUG\nvolatile bool is_paused;\nvolatile int64_t delta;\n#endif\n#if defined(HAVE_CLOCK_GETTIME) && defined(CLOCK_MONOTONIC)\nstatic bool monotonic = false;\nstatic int64_t monotonic_start;\n#endif\n\n/* libevent uses a monotonic clock when available for event scheduling. Aside\n * from jitter, simply ticking our internal timer here is accurate enough.\n * Note that users who are setting explicit dates for expiration times *must*\n * ensure their clocks are correct before starting memcached. */\nstatic void clock_handler(const evutil_socket_t fd, const short which, void *arg) {\n    struct timeval t = {.tv_sec = 1, .tv_usec = 0};\n    static bool initialized = false;\n\n    if (initialized) {\n        /* only delete the event if it's actually there. */\n        evtimer_del(&clockevent);\n    } else {\n        initialized = true;\n    }\n\n    // While we're here, check for hash table expansion.\n    // This function should be quick to avoid delaying the timer.\n    assoc_start_expand(stats_state.curr_items);\n    // also, if HUP'ed we need to do some maintenance.\n    // for now that's just the authfile reload.\n    if (settings.sig_hup) {\n        settings.sig_hup = false;\n\n        authfile_load(settings.auth_file);\n#ifdef PROXY\n        if (settings.proxy_ctx) {\n            proxy_start_reload(settings.proxy_ctx);\n        }\n#endif\n    }\n\n    evtimer_set(&clockevent, clock_handler, 0);\n    event_base_set(main_base, &clockevent);\n    evtimer_add(&clockevent, &t);\n\n#ifdef MEMCACHED_DEBUG\n    if (is_paused) return;\n#endif\n\n#if defined(HAVE_CLOCK_GETTIME) && defined(CLOCK_MONOTONIC)\n    if (monotonic) {\n        struct timespec ts;\n        if (clock_gettime(CLOCK_MONOTONIC, &ts) == -1)\n            return;\n#ifdef MEMCACHED_DEBUG\n        current_time = (rel_time_t) (ts.tv_sec - monotonic_start + delta);\n#else\n        current_time = (rel_time_t) (ts.tv_sec - monotonic_start);\n#endif\n        return;\n    }\n#endif\n    {\n        struct timeval tv;\n        gettimeofday(&tv, NULL);\n#ifdef MEMCACHED_DEBUG\n        current_time = (rel_time_t) (tv.tv_sec - process_started + delta);\n#else\n        current_time = (rel_time_t) (tv.tv_sec - process_started);\n#endif\n    }\n}\n\nstatic const char* flag_enabled_disabled(bool flag) {\n    return (flag ? \"enabled\" : \"disabled\");\n}\n\nvoid verify_default(const char* param, bool condition) {\n    if (!condition) {\n        printf(\"Default value of [%s] has changed.\"\n            \" Modify the help text and default value check.\\n\", param);\n        exit(EXIT_FAILURE);\n    }\n}\n\nstatic void usage(void) {\n    printf(PACKAGE \" \" VERSION \"\\n\");\n    printf(\"-p, --port=<num>          TCP port to listen on (default: %d)\\n\"\n           \"-U, --udp-port=<num>      UDP port to listen on (default: %d, off)\\n\",\n           settings.port, settings.udpport);\n#ifndef DISABLE_UNIX_SOCKET\n    printf(\"-s, --unix-socket=<file>  UNIX socket to listen on (disables network support)\\n\");\n    printf(\"-a, --unix-mask=<mask>    access mask for UNIX socket, in octal (default: %o)\\n\",\n            settings.access);\n#endif /* #ifndef DISABLE_UNIX_SOCKET */\n    printf(\"-A, --enable-shutdown     enable ascii \\\"shutdown\\\" command\\n\");\n    printf(\"-l, --listen=<addr>       interface to listen on (default: INADDR_ANY)\\n\");\n#ifdef TLS\n    printf(\"                          if TLS/SSL is enabled, 'notls' prefix can be used to\\n\"\n           \"                          disable for specific listeners (-l notls:<ip>:<port>) \\n\");\n#endif\n    printf(\"-d, --daemon              run as a daemon\\n\"\n           \"-r, --enable-coredumps    maximize core file limit\\n\"\n           \"-u, --user=<user>         assume identity of <username> (only when run as root)\\n\"\n           \"-m, --memory-limit=<num>  item memory in megabytes (default: %lu)\\n\"\n           \"-M, --disable-evictions   return error on memory exhausted instead of evicting\\n\"\n           \"-c, --conn-limit=<num>    max simultaneous connections (default: %d)\\n\"\n           \"-k, --lock-memory         lock down all paged memory\\n\"\n           \"-v, --verbose             verbose (print errors/warnings while in event loop)\\n\"\n           \"-vv                       very verbose (also print client commands/responses)\\n\"\n           \"-vvv                      extremely verbose (internal state transitions)\\n\"\n           \"-h, --help                print this help and exit\\n\"\n           \"-i, --license             print memcached and libevent license\\n\"\n           \"-V, --version             print version and exit\\n\"\n           \"-P, --pidfile=<file>      save PID in <file>, only used with -d option\\n\"\n           \"-f, --slab-growth-factor=<num> chunk size growth factor (default: %2.2f)\\n\"\n           \"-n, --slab-min-size=<bytes> min space used for key+value+flags (default: %d)\\n\",\n           (unsigned long) settings.maxbytes / (1 << 20),\n           settings.maxconns, settings.factor, settings.chunk_size);\n    verify_default(\"udp-port\",settings.udpport == 0);\n    printf(\"-L, --enable-largepages  try to use large memory pages (if available)\\n\");\n    printf(\"-D <char>     Use <char> as the delimiter between key prefixes and IDs.\\n\"\n           \"              This is used for per-prefix stats reporting. The default is\\n\"\n           \"              \\\"%c\\\" (colon). If this option is specified, stats collection\\n\"\n           \"              is turned on automatically; if not, then it may be turned on\\n\"\n           \"              by sending the \\\"stats detail on\\\" command to the server.\\n\",\n           settings.prefix_delimiter);\n    printf(\"-t, --threads=<num>       number of threads to use (default: %d)\\n\", settings.num_threads);\n    printf(\"-R, --max-reqs-per-event  maximum number of requests per event, limits the\\n\"\n           \"                          requests processed per connection to prevent \\n\"\n           \"                          starvation (default: %d)\\n\", settings.reqs_per_event);\n    printf(\"-C, --disable-cas         disable use of CAS\\n\");\n    printf(\"-b, --listen-backlog=<num> set the backlog queue limit (default: %d)\\n\", settings.backlog);\n    printf(\"-B, --protocol=<name>     protocol - one of ascii, binary, or auto (default: %s)\\n\",\n           prot_text(settings.binding_protocol));\n    printf(\"-I, --max-item-size=<num> adjusts max item size\\n\"\n           \"                          (default: %dm, min: %dk, max: %dm)\\n\",\n           settings.item_size_max/ (1 << 20), ITEM_SIZE_MAX_LOWER_LIMIT / (1 << 10),  ITEM_SIZE_MAX_UPPER_LIMIT / (1 << 20));\n#ifdef ENABLE_SASL\n    printf(\"-S, --enable-sasl         turn on Sasl authentication\\n\");\n#endif\n    printf(\"-F, --disable-flush-all   disable flush_all command\\n\");\n    printf(\"-X, --disable-dumping     disable stats cachedump and lru_crawler metadump\\n\");\n    printf(\"-W  --disable-watch       disable watch commands (live logging)\\n\");\n    printf(\"-Y, --auth-file=<file>    (EXPERIMENTAL) enable ASCII protocol authentication. format:\\n\"\n           \"                          user:pass\\\\nuser2:pass2\\\\n\\n\");\n    printf(\"-e, --memory-file=<file>  (EXPERIMENTAL) mmap a file for item memory.\\n\"\n           \"                          use only in ram disks or persistent memory mounts!\\n\"\n           \"                          enables restartable cache (stop with SIGUSR1)\\n\");\n#ifdef TLS\n    printf(\"-Z, --enable-ssl          enable TLS/SSL\\n\");\n#endif\n    printf(\"-o, --extended            comma separated list of extended options\\n\"\n           \"                          most options have a 'no_' prefix to disable\\n\"\n           \"   - maxconns_fast:       immediately close new connections after limit (default: %s)\\n\"\n           \"   - hashpower:           an integer multiplier for how large the hash\\n\"\n           \"                          table should be. normally grows at runtime. (default starts at: %d)\\n\"\n           \"                          set based on \\\"STAT hash_power_level\\\"\\n\"\n           \"   - tail_repair_time:    time in seconds for how long to wait before\\n\"\n           \"                          forcefully killing LRU tail item.\\n\"\n           \"                          disabled by default; very dangerous option.\\n\"\n           \"   - hash_algorithm:      the hash table algorithm\\n\"\n           \"                          default is murmur3 hash. options: jenkins, murmur3, xxh3\\n\"\n           \"   - no_lru_crawler:      disable LRU Crawler background thread.\\n\"\n           \"   - lru_crawler_sleep:   microseconds to sleep between items\\n\"\n           \"                          default is %d.\\n\"\n           \"   - lru_crawler_tocrawl: max items to crawl per slab per run\\n\"\n           \"                          default is %u (unlimited)\\n\",\n           flag_enabled_disabled(settings.maxconns_fast), settings.hashpower_init,\n           settings.lru_crawler_sleep, settings.lru_crawler_tocrawl);\n    printf(\"   - read_buf_mem_limit:  limit in megabytes for connection read/response buffers.\\n\"\n           \"                          do not adjust unless you have high (20k+) conn. limits.\\n\"\n           \"                          0 means unlimited (default: %u)\\n\",\n           settings.read_buf_mem_limit);\n    verify_default(\"read_buf_mem_limit\", settings.read_buf_mem_limit == 0);\n    printf(\"   - no_lru_maintainer:   disable new LRU system + background thread.\\n\"\n           \"   - hot_lru_pct:         pct of slab memory to reserve for hot lru.\\n\"\n           \"                          (requires lru_maintainer, default pct: %d)\\n\"\n           \"   - warm_lru_pct:        pct of slab memory to reserve for warm lru.\\n\"\n           \"                          (requires lru_maintainer, default pct: %d)\\n\"\n           \"   - hot_max_factor:      items idle > cold lru age * drop from hot lru. (default: %.2f)\\n\"\n           \"   - warm_max_factor:     items idle > cold lru age * this drop from warm. (default: %.2f)\\n\"\n           \"   - temporary_ttl:       TTL's below get separate LRU, can't be evicted.\\n\"\n           \"                          (requires lru_maintainer, default: %d)\\n\"\n           \"   - idle_timeout:        timeout for idle connections. (default: %d, no timeout)\\n\",\n           settings.hot_lru_pct, settings.warm_lru_pct, settings.hot_max_factor, settings.warm_max_factor,\n           settings.temporary_ttl, settings.idle_timeout);\n    printf(\"   - slab_chunk_max:      (EXPERIMENTAL) maximum slab size in kilobytes. use extreme care. (default: %d)\\n\"\n           \"   - watcher_logbuf_size: size in kilobytes of per-watcher write buffer. (default: %u)\\n\"\n           \"   - worker_logbuf_size:  size in kilobytes of per-worker-thread buffer\\n\"\n           \"                          read by background thread, then written to watchers. (default: %u)\\n\"\n           \"   - track_sizes:         enable dynamic reports for 'stats sizes' command.\\n\"\n           \"                          note that counts for each size are approximate.\\n\"\n           \"   - no_hashexpand:       disables hash table expansion (dangerous)\\n\"\n           \"   - modern:              enables options which will be default in future.\\n\"\n           \"                          currently: nothing\\n\"\n           \"   - no_modern:           uses defaults of previous major version (1.4.x)\\n\",\n           settings.slab_chunk_size_max / (1 << 10), settings.logger_watcher_buf_size / (1 << 10),\n           settings.logger_buf_size / (1 << 10));\n    verify_default(\"tail_repair_time\", settings.tail_repair_time == TAIL_REPAIR_TIME_DEFAULT);\n    verify_default(\"lru_crawler_tocrawl\", settings.lru_crawler_tocrawl == 0);\n    verify_default(\"idle_timeout\", settings.idle_timeout == 0);\n#ifdef HAVE_DROP_PRIVILEGES\n    printf(\"   - drop_privileges:     enable dropping extra syscall privileges\\n\"\n           \"   - no_drop_privileges:  disable drop_privileges in case it causes issues with\\n\"\n           \"                          some customisation.\\n\"\n           \"                          (default is no_drop_privileges)\\n\");\n    verify_default(\"drop_privileges\", !settings.drop_privileges);\n#ifdef MEMCACHED_DEBUG\n    printf(\"   - relaxed_privileges:  running tests requires extra privileges. (default: %s)\\n\",\n           flag_enabled_disabled(settings.relaxed_privileges));\n#endif\n#endif\n#ifdef SOCK_COOKIE_ID\n    printf(\"   - sock_cookie_id:      attributes an ID to a socket for ip filtering/firewalls \\n\");\n#endif\n#ifdef EXTSTORE\n    printf(\"\\n   - External storage (ext_*) related options (see: https://memcached.org/extstore)\\n\");\n    printf(\"   - ext_path:            file to write to for external storage.\\n\"\n           \"                          ie: ext_path=/mnt/d1/extstore:1G\\n\"\n           \"   - ext_page_size:       size in megabytes of storage pages. (default: %u)\\n\"\n           \"   - ext_wbuf_size:       size in megabytes of page write buffers. (default: %u)\\n\"\n           \"   - ext_threads:         number of IO threads to run. (default: %u)\\n\"\n           \"   - ext_item_size:       store items larger than this (bytes, default %u)\\n\"\n           \"   - ext_item_age:        store items idle at least this long (seconds, default: no age limit)\\n\"\n           \"   - ext_low_ttl:         consider TTLs lower than this specially (default: %u)\\n\"\n           \"   - ext_drop_unread:     don't re-write unread values during compaction (default: %s)\\n\"\n           \"   - ext_recache_rate:    recache an item every N accesses (default: %u)\\n\"\n           \"   - ext_compact_under:   compact when fewer than this many free pages\\n\"\n           \"                          (default: 1 percent of the assigned storage)\\n\"\n           \"   - ext_drop_under:      drop COLD items when fewer than this many free pages\\n\"\n           \"                          (default: 1/4th of the assigned storage)\\n\"\n           \"   - ext_max_frag:        only defrag pages if they are less full than this pct-wise (default: %.2f)\\n\"\n           \"   - ext_max_sleep:       max sleep time of background threads in us (default: %u)\\n\"\n           \"   - slab_automove_freeratio: ratio of memory to hold free as buffer.\\n\"\n           \"                          (see doc/storage.txt for more info, default: %.3f)\\n\",\n           settings.ext_page_size / (1 << 20), settings.ext_wbuf_size / (1 << 20), settings.ext_io_threadcount,\n           settings.ext_item_size, settings.ext_low_ttl,\n           flag_enabled_disabled(settings.ext_drop_unread), settings.ext_recache_rate,\n           settings.ext_max_frag, settings.ext_max_sleep, settings.slab_automove_freeratio);\n    verify_default(\"ext_item_age\", settings.ext_item_age == UINT_MAX);\n#endif\n#ifdef PROXY\n    printf(\"   - proxy_config:        path to lua library file. separate with ':' for multiple files\\n\"\n           \"                          use proxy_config=routelib to use built-in library\\n\"\n            );\n    printf(\"   - proxy_arg:           argument string (file path) to pass to proxy config\\n\"\n            );\n#endif\n    ssl_help();\n    printf(\"-N, --napi_ids            number of napi ids. see doc/napi_ids.txt for more details\\n\");\n    return;\n}\n\nstatic void usage_license(void) {\n    printf(PACKAGE \" \" VERSION \"\\n\\n\");\n    printf(\n    \"Copyright (c) 2003, Danga Interactive, Inc. <http://www.danga.com/>\\n\"\n    \"All rights reserved.\\n\"\n    \"\\n\"\n    \"Redistribution and use in source and binary forms, with or without\\n\"\n    \"modification, are permitted provided that the following conditions are\\n\"\n    \"met:\\n\"\n    \"\\n\"\n    \"    * Redistributions of source code must retain the above copyright\\n\"\n    \"notice, this list of conditions and the following disclaimer.\\n\"\n    \"\\n\"\n    \"    * Redistributions in binary form must reproduce the above\\n\"\n    \"copyright notice, this list of conditions and the following disclaimer\\n\"\n    \"in the documentation and/or other materials provided with the\\n\"\n    \"distribution.\\n\"\n    \"\\n\"\n    \"    * Neither the name of the Danga Interactive nor the names of its\\n\"\n    \"contributors may be used to endorse or promote products derived from\\n\"\n    \"this software without specific prior written permission.\\n\"\n    \"\\n\"\n    \"THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\\n\"\n    \"\\\"AS IS\\\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\\n\"\n    \"LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\\n\"\n    \"A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\\n\"\n    \"OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\\n\"\n    \"SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\\n\"\n    \"LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\\n\"\n    \"DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\\n\"\n    \"THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\\n\"\n    \"(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\\n\"\n    \"OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\n\"\n    );\n\n    return;\n}\n\nstatic void save_pid(const char *pid_file) {\n    FILE *fp;\n    if (access(pid_file, F_OK) == 0) {\n        if ((fp = fopen(pid_file, \"r\")) != NULL) {\n            char buffer[1024];\n            if (fgets(buffer, sizeof(buffer), fp) != NULL) {\n                unsigned int pid;\n                if (safe_strtoul(buffer, &pid) && kill((pid_t)pid, 0) == 0) {\n                    fprintf(stderr, \"WARNING: The pid file contained the following (running) pid: %u\\n\", pid);\n                }\n            }\n            fclose(fp);\n        }\n    }\n\n    /* Create the pid file first with a temporary name, then\n     * atomically move the file to the real name to avoid a race with\n     * another process opening the file to read the pid, but finding\n     * it empty.\n     */\n    char tmp_pid_file[1024];\n    snprintf(tmp_pid_file, sizeof(tmp_pid_file), \"%s.tmp\", pid_file);\n\n    if ((fp = fopen(tmp_pid_file, \"w\")) == NULL) {\n        vperror(\"Could not open the pid file %s for writing\", tmp_pid_file);\n        return;\n    }\n\n    fprintf(fp,\"%ld\\n\", (long)getpid());\n    if (fclose(fp) == -1) {\n        vperror(\"Could not close the pid file %s\", tmp_pid_file);\n    }\n\n    if (rename(tmp_pid_file, pid_file) != 0) {\n        vperror(\"Could not rename the pid file from %s to %s\",\n                tmp_pid_file, pid_file);\n    }\n}\n\nstatic void remove_pidfile(const char *pid_file) {\n  if (pid_file == NULL)\n      return;\n\n  if (unlink(pid_file) != 0) {\n      vperror(\"Could not remove the pid file %s\", pid_file);\n  }\n\n}\n\nstatic void sig_handler(const int sig) {\n    stop_main_loop = EXIT_NORMALLY;\n}\n\nstatic void sighup_handler(const int sig) {\n    settings.sig_hup = true;\n}\n\nstatic void sig_usrhandler(const int sig) {\n    stop_main_loop = GRACE_STOP;\n}\n\n/*\n * On systems that supports multiple page sizes we may reduce the\n * number of TLB-misses by using the biggest available page size\n */\nstatic int enable_large_pages(void) {\n#if defined(HAVE_GETPAGESIZES) && defined(HAVE_MEMCNTL)\n    int ret = -1;\n    size_t sizes[32];\n    int avail = getpagesizes(sizes, 32);\n    if (avail != -1) {\n        size_t max = sizes[0];\n        struct memcntl_mha arg = {0};\n        int ii;\n\n        for (ii = 1; ii < avail; ++ii) {\n            if (max < sizes[ii]) {\n                max = sizes[ii];\n            }\n        }\n\n        arg.mha_flags   = 0;\n        arg.mha_pagesize = max;\n        arg.mha_cmd = MHA_MAPSIZE_BSSBRK;\n\n        if (memcntl(0, 0, MC_HAT_ADVISE, (caddr_t)&arg, 0, 0) == -1) {\n            fprintf(stderr, \"Failed to set large pages: %s\\n\",\n                    strerror(errno));\n            fprintf(stderr, \"Will use default page size\\n\");\n        } else {\n            ret = 0;\n        }\n    } else {\n        fprintf(stderr, \"Failed to get supported pagesizes: %s\\n\",\n                strerror(errno));\n        fprintf(stderr, \"Will use default page size\\n\");\n    }\n\n    return ret;\n#elif defined(__linux__) && defined(MADV_HUGEPAGE)\n    /* check if transparent hugepages is compiled into the kernel */\n    /* RH based systems possibly uses a different path */\n    static const char *mm_thp_paths[] = {\n        \"/sys/kernel/mm/transparent_hugepage/enabled\",\n        \"/sys/kernel/mm/redhat_transparent_hugepage/enabled\",\n        NULL\n    };\n\n    char thpb[128] = {0};\n    int pfd = -1;\n    for (const char **p = mm_thp_paths; *p; p++) {\n        if ((pfd = open(*p, O_RDONLY)) != -1)\n            break;\n    }\n\n    if (pfd == -1) {\n        fprintf(stderr, \"Transparent huge pages support not detected.\\n\");\n        fprintf(stderr, \"Will use default page size.\\n\");\n        return -1;\n    }\n    ssize_t rd = read(pfd, thpb, sizeof(thpb));\n    close(pfd);\n    if (rd <= 0) {\n        fprintf(stderr, \"Transparent huge pages could not read the configuration.\\n\");\n        fprintf(stderr, \"Will use default page size.\\n\");\n        return -1;\n    }\n    thpb[rd] = 0;\n    if (strstr(thpb, \"[never]\")) {\n        fprintf(stderr, \"Transparent huge pages support disabled.\\n\");\n        fprintf(stderr, \"Will use default page size.\\n\");\n        return -1;\n    }\n    return 0;\n#elif defined(__FreeBSD__)\n    int spages;\n    size_t spagesl = sizeof(spages);\n\n    if (sysctlbyname(\"vm.pmap.pg_ps_enabled\", &spages,\n    &spagesl, NULL, 0) != 0) {\n        fprintf(stderr, \"Could not evaluate the presence of superpages features.\");\n        return -1;\n    }\n    if (spages != 1) {\n        fprintf(stderr, \"Superpages support not detected.\\n\");\n        fprintf(stderr, \"Will use default page size.\\n\");\n        return -1;\n    }\n    return 0;\n#else\n    return -1;\n#endif\n}\n\n/**\n * Do basic sanity check of the runtime environment\n * @return true if no errors found, false if we can't use this env\n */\nstatic bool sanitycheck(void) {\n    /* One of our biggest problems is old and bogus libevents */\n    const char *ever = event_get_version();\n    if (ever != NULL) {\n        if (strncmp(ever, \"1.\", 2) == 0) {\n            fprintf(stderr, \"You are using libevent %s.\\nPlease upgrade to 2.x\"\n                        \" or newer\\n\", event_get_version());\n            return false;\n        }\n    }\n\n    return true;\n}\n\nstatic bool _parse_slab_sizes(char *s, uint32_t *slab_sizes) {\n    char *b = NULL;\n    uint32_t size = 0;\n    int i = 0;\n    uint32_t last_size = 0;\n\n    if (strlen(s) < 1)\n        return false;\n\n    for (char *p = strtok_r(s, \"-\", &b);\n         p != NULL;\n         p = strtok_r(NULL, \"-\", &b)) {\n        if (!safe_strtoul(p, &size) || size < settings.chunk_size\n             || size > settings.slab_chunk_size_max) {\n            fprintf(stderr, \"slab size %u is out of valid range\\n\", size);\n            return false;\n        }\n        if (last_size >= size) {\n            fprintf(stderr, \"slab size %u cannot be lower than or equal to a previous class size\\n\", size);\n            return false;\n        }\n        if (size <= last_size + CHUNK_ALIGN_BYTES) {\n            fprintf(stderr, \"slab size %u must be at least %d bytes larger than previous class\\n\",\n                    size, CHUNK_ALIGN_BYTES);\n            return false;\n        }\n        slab_sizes[i++] = size;\n        last_size = size;\n        if (i >= MAX_NUMBER_OF_SLAB_CLASSES-1) {\n            fprintf(stderr, \"too many slab classes specified\\n\");\n            return false;\n        }\n    }\n\n    slab_sizes[i] = 0;\n    return true;\n}\n\nstruct _mc_meta_data {\n    void *mmap_base;\n    uint64_t old_base;\n    char *slab_config; // string containing either factor or custom slab list.\n    int64_t time_delta;\n    uint64_t process_started;\n    uint32_t current_time;\n};\n\n// We need to remember a combination of configuration settings and global\n// state for restart viability and resumption of internal services.\n// Compared to the number of tunables and state values, relatively little\n// does need to be remembered.\n// Time is the hardest; we have to assume the sys clock is correct and re-sync for\n// the lost time after restart.\nstatic int _mc_meta_save_cb(const char *tag, void *ctx, void *data) {\n    struct _mc_meta_data *meta = (struct _mc_meta_data *)data;\n\n    // Settings to remember.\n    // TODO: should get a version of version which is numeric, else\n    // comparisons for compat reasons are difficult.\n    // it may be possible to punt on this for now; since we can test for the\n    // absence of another key... such as the new numeric version.\n    //restart_set_kv(ctx, \"version\", \"%s\", VERSION);\n    // We hold the original factor or subopts _string_\n    // it can be directly compared without roundtripping through floats or\n    // serializing/deserializing the long options list.\n    restart_set_kv(ctx, \"slab_config\", \"%s\", meta->slab_config);\n    restart_set_kv(ctx, \"maxbytes\", \"%llu\", (unsigned long long) settings.maxbytes);\n    restart_set_kv(ctx, \"chunk_size\", \"%d\", settings.chunk_size);\n    restart_set_kv(ctx, \"item_size_max\", \"%d\", settings.item_size_max);\n    restart_set_kv(ctx, \"slab_chunk_size_max\", \"%d\", settings.slab_chunk_size_max);\n    restart_set_kv(ctx, \"slab_page_size\", \"%d\", settings.slab_page_size);\n    restart_set_kv(ctx, \"use_cas\", \"%s\", settings.use_cas ? \"true\" : \"false\");\n    restart_set_kv(ctx, \"slab_reassign\", \"%s\", settings.slab_reassign ? \"true\" : \"false\");\n\n    // Online state to remember.\n\n    // current time is tough. we need to rely on the clock being correct to\n    // pull the delta between stop and start times. we also need to know the\n    // delta between start time and now to restore monotonic clocks.\n    // for non-monotonic clocks (some OS?), process_started is the only\n    // important one.\n    restart_set_kv(ctx, \"current_time\", \"%u\", current_time);\n    // types are great until... this. some systems time_t could be big, but\n    // I'm assuming never negative.\n    restart_set_kv(ctx, \"process_started\", \"%llu\", (unsigned long long) process_started);\n    {\n        struct timeval tv;\n        gettimeofday(&tv, NULL);\n        restart_set_kv(ctx, \"stop_time\", \"%lu\", tv.tv_sec);\n    }\n\n    // Might as well just fetch the next CAS value to use than tightly\n    // coupling the internal variable into the restart system.\n    restart_set_kv(ctx, \"current_cas\", \"%llu\", (unsigned long long) get_cas_id());\n    restart_set_kv(ctx, \"logger_gid\", \"%llu\", logger_get_gid());\n    restart_set_kv(ctx, \"hashpower\", \"%u\", stats_state.hash_power_level);\n    // NOTE: oldest_live is a rel_time_t, which aliases for unsigned int.\n    // should future proof this with a 64bit upcast, or fetch value from a\n    // converter function/macro?\n    restart_set_kv(ctx, \"oldest_live\", \"%u\", settings.oldest_live);\n    // TODO: use uintptr_t etc? is it portable enough?\n    restart_set_kv(ctx, \"mmap_oldbase\", \"%p\", meta->mmap_base);\n\n    return 0;\n}\n\n// We must see at least this number of checked lines. Else empty/missing lines\n// could cause a false-positive.\n// TODO: Once crc32'ing of the metadata file is done this could be ensured better by\n// the restart module itself (crc32 + count of lines must match on the\n// backend)\n#define RESTART_REQUIRED_META 16\n\n// With this callback we make a decision on if the current configuration\n// matches up enough to allow reusing the cache.\n// We also re-load important runtime information.\nstatic int _mc_meta_load_cb(const char *tag, void *ctx, void *data) {\n    struct _mc_meta_data *meta = (struct _mc_meta_data *)data;\n    char *key;\n    char *val;\n    int reuse_mmap = 0;\n    meta->process_started = 0;\n    meta->time_delta = 0;\n    meta->current_time = 0;\n    int lines_seen = 0;\n\n    // TODO: not sure this is any better than just doing an if/else tree with\n    // strcmp's...\n    enum {\n        R_MMAP_OLDBASE = 0,\n        R_MAXBYTES,\n        R_CHUNK_SIZE,\n        R_ITEM_SIZE_MAX,\n        R_SLAB_CHUNK_SIZE_MAX,\n        R_SLAB_PAGE_SIZE,\n        R_SLAB_CONFIG,\n        R_USE_CAS,\n        R_SLAB_REASSIGN,\n        R_CURRENT_CAS,\n        R_OLDEST_LIVE,\n        R_LOGGER_GID,\n        R_CURRENT_TIME,\n        R_STOP_TIME,\n        R_PROCESS_STARTED,\n        R_HASHPOWER,\n    };\n\n    const char *opts[] = {\n        [R_MMAP_OLDBASE] = \"mmap_oldbase\",\n        [R_MAXBYTES] = \"maxbytes\",\n        [R_CHUNK_SIZE] = \"chunk_size\",\n        [R_ITEM_SIZE_MAX] = \"item_size_max\",\n        [R_SLAB_CHUNK_SIZE_MAX] = \"slab_chunk_size_max\",\n        [R_SLAB_PAGE_SIZE] = \"slab_page_size\",\n        [R_SLAB_CONFIG] = \"slab_config\",\n        [R_USE_CAS] = \"use_cas\",\n        [R_SLAB_REASSIGN] = \"slab_reassign\",\n        [R_CURRENT_CAS] = \"current_cas\",\n        [R_OLDEST_LIVE] = \"oldest_live\",\n        [R_LOGGER_GID] = \"logger_gid\",\n        [R_CURRENT_TIME] = \"current_time\",\n        [R_STOP_TIME] = \"stop_time\",\n        [R_PROCESS_STARTED] = \"process_started\",\n        [R_HASHPOWER] = \"hashpower\",\n        NULL\n    };\n\n    while (restart_get_kv(ctx, &key, &val) == RESTART_OK) {\n        int type = 0;\n        int32_t val_int = 0;\n        uint32_t val_uint = 0;\n        int64_t bigval_int = 0;\n        uint64_t bigval_uint = 0;\n\n        while (opts[type] != NULL && strcmp(key, opts[type]) != 0) {\n            type++;\n        }\n        if (opts[type] == NULL) {\n            fprintf(stderr, \"[restart] unknown/unhandled key: %s\\n\", key);\n            continue;\n        }\n        lines_seen++;\n\n        // helper for any boolean checkers.\n        bool val_bool = false;\n        bool is_bool = true;\n        if (strcmp(val, \"false\") == 0) {\n            val_bool = false;\n        } else if (strcmp(val, \"true\") == 0) {\n            val_bool = true;\n        } else {\n            is_bool = false;\n        }\n\n        switch (type) {\n        case R_MMAP_OLDBASE:\n            if (!safe_strtoull_hex(val, &meta->old_base)) {\n                fprintf(stderr, \"[restart] failed to parse %s: %s\\n\", key, val);\n                reuse_mmap = -1;\n            }\n            break;\n        case R_MAXBYTES:\n            if (!safe_strtoll(val, &bigval_int) || settings.maxbytes != bigval_int) {\n                reuse_mmap = -1;\n            }\n            break;\n        case R_CHUNK_SIZE:\n            if (!safe_strtol(val, &val_int) || settings.chunk_size != val_int) {\n                reuse_mmap = -1;\n            }\n            break;\n        case R_ITEM_SIZE_MAX:\n            if (!safe_strtol(val, &val_int) || settings.item_size_max != val_int) {\n                reuse_mmap = -1;\n            }\n            break;\n        case R_SLAB_CHUNK_SIZE_MAX:\n            if (!safe_strtol(val, &val_int) || settings.slab_chunk_size_max != val_int) {\n                reuse_mmap = -1;\n            }\n            break;\n        case R_SLAB_PAGE_SIZE:\n            if (!safe_strtol(val, &val_int) || settings.slab_page_size != val_int) {\n                reuse_mmap = -1;\n            }\n            break;\n        case R_SLAB_CONFIG:\n            if (strcmp(val, meta->slab_config) != 0) {\n                reuse_mmap = -1;\n            }\n            break;\n        case R_USE_CAS:\n            if (!is_bool || settings.use_cas != val_bool) {\n                reuse_mmap = -1;\n            }\n            break;\n        case R_SLAB_REASSIGN:\n            if (!is_bool || settings.slab_reassign != val_bool) {\n                reuse_mmap = -1;\n            }\n            break;\n        case R_CURRENT_CAS:\n            // FIXME: do we need to fail if these values _aren't_ found?\n            if (!safe_strtoull(val, &bigval_uint)) {\n                reuse_mmap = -1;\n            } else {\n                set_cas_id(bigval_uint);\n            }\n            break;\n        case R_OLDEST_LIVE:\n            if (!safe_strtoul(val, &val_uint)) {\n                reuse_mmap = -1;\n            } else {\n                settings.oldest_live = val_uint;\n            }\n            break;\n        case R_LOGGER_GID:\n            if (!safe_strtoull(val, &bigval_uint)) {\n                reuse_mmap = -1;\n            } else {\n                logger_set_gid(bigval_uint);\n            }\n            break;\n        case R_PROCESS_STARTED:\n            if (!safe_strtoull(val, &bigval_uint)) {\n                reuse_mmap = -1;\n            } else {\n                meta->process_started = bigval_uint;\n            }\n            break;\n        case R_CURRENT_TIME:\n            if (!safe_strtoul(val, &val_uint)) {\n                reuse_mmap = -1;\n            } else {\n                meta->current_time = val_uint;\n            }\n            break;\n        case R_STOP_TIME:\n            if (!safe_strtoll(val, &bigval_int)) {\n                reuse_mmap = -1;\n            } else {\n                struct timeval t;\n                gettimeofday(&t, NULL);\n                meta->time_delta = t.tv_sec - bigval_int;\n                // clock has done something crazy.\n                // there are _lots_ of ways the clock can go wrong here, but\n                // this is a safe sanity check since there's nothing else we\n                // can realistically do.\n                if (meta->time_delta <= 0) {\n                    reuse_mmap = -1;\n                }\n            }\n            break;\n        case R_HASHPOWER:\n            if (!safe_strtoul(val, &val_uint)) {\n                reuse_mmap = -1;\n            } else {\n                settings.hashpower_init = val_uint;\n            }\n            break;\n        default:\n            fprintf(stderr, \"[restart] unhandled key: %s\\n\", key);\n        }\n\n        if (reuse_mmap != 0) {\n            fprintf(stderr, \"[restart] restart incompatible due to setting for [%s] [old value: %s]\\n\", key, val);\n            break;\n        }\n    }\n\n    if (lines_seen < RESTART_REQUIRED_META) {\n        fprintf(stderr, \"[restart] missing some metadata lines\\n\");\n        reuse_mmap = -1;\n    }\n\n    return reuse_mmap;\n}\n\nint main (int argc, char **argv) {\n    int c;\n    bool lock_memory = false;\n    bool do_daemonize = false;\n    bool preallocate = false;\n    int maxcore = 0;\n    char *username = NULL;\n    char *pid_file = NULL;\n    struct passwd *pw;\n    struct rlimit rlim;\n    char *buf;\n    char unit = '\\0';\n    int size_max = 0;\n    int retval = EXIT_SUCCESS;\n    bool protocol_specified = false;\n    bool tcp_specified = false;\n    bool udp_specified = false;\n    bool start_lru_maintainer = true;\n    bool start_lru_crawler = true;\n    bool start_assoc_maint = true;\n    enum hashfunc_type hash_type = MURMUR3_HASH;\n    uint32_t tocrawl;\n    uint32_t slab_sizes[MAX_NUMBER_OF_SLAB_CLASSES];\n    bool use_slab_sizes = false;\n    char *slab_sizes_unparsed = NULL;\n    bool slab_chunk_size_changed = false;\n    // struct for restart code. Initialized up here so we can curry\n    // important settings to save or validate.\n    struct _mc_meta_data *meta = malloc(sizeof(struct _mc_meta_data));\n    meta->slab_config = NULL;\n    char *subopts, *subopts_orig;\n    char *subopts_value;\n    enum {\n        MAXCONNS_FAST = 0,\n        HASHPOWER_INIT,\n        NO_HASHEXPAND,\n        SLAB_REASSIGN,\n        SLAB_AUTOMOVE,\n        SLAB_AUTOMOVE_RATIO,\n        SLAB_AUTOMOVE_WINDOW,\n        TAIL_REPAIR_TIME,\n        HASH_ALGORITHM,\n        LRU_CRAWLER,\n        LRU_CRAWLER_SLEEP,\n        LRU_CRAWLER_TOCRAWL,\n        LRU_MAINTAINER,\n        HOT_LRU_PCT,\n        WARM_LRU_PCT,\n        HOT_MAX_FACTOR,\n        WARM_MAX_FACTOR,\n        TEMPORARY_TTL,\n        IDLE_TIMEOUT,\n        WATCHER_LOGBUF_SIZE,\n        WORKER_LOGBUF_SIZE,\n        SLAB_SIZES,\n        SLAB_CHUNK_MAX,\n        TRACK_SIZES,\n        NO_INLINE_ASCII_RESP,\n        MODERN,\n        NO_MODERN,\n        NO_CHUNKED_ITEMS,\n        NO_SLAB_REASSIGN,\n        NO_SLAB_AUTOMOVE,\n        NO_MAXCONNS_FAST,\n        INLINE_ASCII_RESP,\n        NO_LRU_CRAWLER,\n        NO_LRU_MAINTAINER,\n        NO_DROP_PRIVILEGES,\n        DROP_PRIVILEGES,\n        RESP_OBJ_MEM_LIMIT,\n        READ_BUF_MEM_LIMIT,\n#ifdef TLS\n        SSL_CERT,\n        SSL_KEY,\n        SSL_VERIFY_MODE,\n        SSL_KEYFORM,\n        SSL_CIPHERS,\n        SSL_CA_CERT,\n        SSL_WBUF_SIZE,\n        SSL_SESSION_CACHE,\n        SSL_KERNEL_TLS,\n        SSL_MIN_VERSION,\n#endif\n#ifdef PROXY\n        PROXY_CONFIG,\n        PROXY_ARG,\n        PROXY_URING,\n        PROXY_MEMPROFILE,\n#endif\n#ifdef MEMCACHED_DEBUG\n        RELAXED_PRIVILEGES,\n#endif\n#ifdef SOCK_COOKIE_ID\n        COOKIE_ID,\n#endif\n    };\n    char *const subopts_tokens[] = {\n        [MAXCONNS_FAST] = \"maxconns_fast\",\n        [HASHPOWER_INIT] = \"hashpower\",\n        [NO_HASHEXPAND] = \"no_hashexpand\",\n        [SLAB_REASSIGN] = \"slab_reassign\",\n        [SLAB_AUTOMOVE] = \"slab_automove\",\n        [SLAB_AUTOMOVE_RATIO] = \"slab_automove_ratio\",\n        [SLAB_AUTOMOVE_WINDOW] = \"slab_automove_window\",\n        [TAIL_REPAIR_TIME] = \"tail_repair_time\",\n        [HASH_ALGORITHM] = \"hash_algorithm\",\n        [LRU_CRAWLER] = \"lru_crawler\",\n        [LRU_CRAWLER_SLEEP] = \"lru_crawler_sleep\",\n        [LRU_CRAWLER_TOCRAWL] = \"lru_crawler_tocrawl\",\n        [LRU_MAINTAINER] = \"lru_maintainer\",\n        [HOT_LRU_PCT] = \"hot_lru_pct\",\n        [WARM_LRU_PCT] = \"warm_lru_pct\",\n        [HOT_MAX_FACTOR] = \"hot_max_factor\",\n        [WARM_MAX_FACTOR] = \"warm_max_factor\",\n        [TEMPORARY_TTL] = \"temporary_ttl\",\n        [IDLE_TIMEOUT] = \"idle_timeout\",\n        [WATCHER_LOGBUF_SIZE] = \"watcher_logbuf_size\",\n        [WORKER_LOGBUF_SIZE] = \"worker_logbuf_size\",\n        [SLAB_SIZES] = \"slab_sizes\",\n        [SLAB_CHUNK_MAX] = \"slab_chunk_max\",\n        [TRACK_SIZES] = \"track_sizes\",\n        [NO_INLINE_ASCII_RESP] = \"no_inline_ascii_resp\",\n        [MODERN] = \"modern\",\n        [NO_MODERN] = \"no_modern\",\n        [NO_CHUNKED_ITEMS] = \"no_chunked_items\",\n        [NO_SLAB_REASSIGN] = \"no_slab_reassign\",\n        [NO_SLAB_AUTOMOVE] = \"no_slab_automove\",\n        [NO_MAXCONNS_FAST] = \"no_maxconns_fast\",\n        [INLINE_ASCII_RESP] = \"inline_ascii_resp\",\n        [NO_LRU_CRAWLER] = \"no_lru_crawler\",\n        [NO_LRU_MAINTAINER] = \"no_lru_maintainer\",\n        [NO_DROP_PRIVILEGES] = \"no_drop_privileges\",\n        [DROP_PRIVILEGES] = \"drop_privileges\",\n        [RESP_OBJ_MEM_LIMIT] = \"resp_obj_mem_limit\",\n        [READ_BUF_MEM_LIMIT] = \"read_buf_mem_limit\",\n#ifdef TLS\n        [SSL_CERT] = \"ssl_chain_cert\",\n        [SSL_KEY] = \"ssl_key\",\n        [SSL_VERIFY_MODE] = \"ssl_verify_mode\",\n        [SSL_KEYFORM] = \"ssl_keyformat\",\n        [SSL_CIPHERS] = \"ssl_ciphers\",\n        [SSL_CA_CERT] = \"ssl_ca_cert\",\n        [SSL_WBUF_SIZE] = \"ssl_wbuf_size\",\n        [SSL_SESSION_CACHE] = \"ssl_session_cache\",\n        [SSL_KERNEL_TLS] = \"ssl_kernel_tls\",\n        [SSL_MIN_VERSION] = \"ssl_min_version\",\n#endif\n#ifdef PROXY\n        [PROXY_CONFIG] = \"proxy_config\",\n        [PROXY_ARG] = \"proxy_arg\",\n        [PROXY_URING] = \"proxy_uring\",\n        [PROXY_MEMPROFILE] = \"proxy_memprofile\",\n#endif\n#ifdef MEMCACHED_DEBUG\n        [RELAXED_PRIVILEGES] = \"relaxed_privileges\",\n#endif\n#ifdef SOCK_COOKIE_ID\n        [COOKIE_ID] = \"sock_cookie_id\",\n#endif\n        NULL\n    };\n\n    if (!sanitycheck()) {\n        free(meta);\n        return EX_OSERR;\n    }\n\n    /* handle SIGINT, SIGTERM */\n    signal(SIGINT, sig_handler);\n    signal(SIGTERM, sig_handler);\n    signal(SIGHUP, sighup_handler);\n    signal(SIGUSR1, sig_usrhandler);\n\n    /* init settings */\n    settings_init();\n    verify_default(\"hash_algorithm\", hash_type == MURMUR3_HASH);\n    void *storage = NULL;\n#ifdef EXTSTORE\n    void *storage_cf = storage_init_config(&settings);\n    bool storage_enabled = false;\n    if (storage_cf == NULL) {\n        fprintf(stderr, \"failed to allocate extstore config\\n\");\n        return 1;\n    }\n#endif\n\n    /* set stderr non-buffering (for running under, say, daemontools) */\n    setbuf(stderr, NULL);\n\n    char *shortopts =\n          \"a:\"  /* access mask for unix socket */\n          \"A\"   /* enable admin shutdown command */\n          \"Z\"   /* enable SSL */\n          \"p:\"  /* TCP port number to listen on */\n          \"s:\"  /* unix socket path to listen on */\n          \"U:\"  /* UDP port number to listen on */\n          \"m:\"  /* max memory to use for items in megabytes */\n          \"M\"   /* return error on memory exhausted */\n          \"c:\"  /* max simultaneous connections */\n          \"k\"   /* lock down all paged memory */\n          \"hiV\" /* help, licence info, version */\n          \"r\"   /* maximize core file limit */\n          \"v\"   /* verbose */\n          \"d\"   /* daemon mode */\n          \"l:\"  /* interface to listen on */\n          \"u:\"  /* user identity to run as */\n          \"P:\"  /* save PID in file */\n          \"f:\"  /* factor? */\n          \"n:\"  /* minimum space allocated for key+value+flags */\n          \"t:\"  /* threads */\n          \"D:\"  /* prefix delimiter? */\n          \"L\"   /* Large memory pages */\n          \"R:\"  /* max requests per event */\n          \"C\"   /* Disable use of CAS */\n          \"b:\"  /* backlog queue limit */\n          \"B:\"  /* Binding protocol */\n          \"I:\"  /* Max item size */\n          \"S\"   /* Sasl ON */\n          \"F\"   /* Disable flush_all */\n          \"X\"   /* Disable dump commands */\n          \"W\"   /* Disable watch commands */\n          \"Y:\"   /* Enable token auth */\n          \"e:\"  /* mmap path for external item memory */\n          \"o:\"  /* Extended generic options */\n          \"N:\"  /* NAPI ID based thread selection */\n          ;\n\n    /* process arguments */\n#ifdef HAVE_GETOPT_LONG\n    const struct option longopts[] = {\n        {\"unix-mask\", required_argument, 0, 'a'},\n        {\"enable-shutdown\", no_argument, 0, 'A'},\n        {\"enable-ssl\", no_argument, 0, 'Z'},\n        {\"port\", required_argument, 0, 'p'},\n        {\"unix-socket\", required_argument, 0, 's'},\n        {\"udp-port\", required_argument, 0, 'U'},\n        {\"memory-limit\", required_argument, 0, 'm'},\n        {\"disable-evictions\", no_argument, 0, 'M'},\n        {\"conn-limit\", required_argument, 0, 'c'},\n        {\"lock-memory\", no_argument, 0, 'k'},\n        {\"help\", no_argument, 0, 'h'},\n        {\"license\", no_argument, 0, 'i'},\n        {\"version\", no_argument, 0, 'V'},\n        {\"enable-coredumps\", no_argument, 0, 'r'},\n        {\"verbose\", optional_argument, 0, 'v'},\n        {\"daemon\", no_argument, 0, 'd'},\n        {\"listen\", required_argument, 0, 'l'},\n        {\"user\", required_argument, 0, 'u'},\n        {\"pidfile\", required_argument, 0, 'P'},\n        {\"slab-growth-factor\", required_argument, 0, 'f'},\n        {\"slab-min-size\", required_argument, 0, 'n'},\n        {\"threads\", required_argument, 0, 't'},\n        {\"enable-largepages\", no_argument, 0, 'L'},\n        {\"max-reqs-per-event\", required_argument, 0, 'R'},\n        {\"disable-cas\", no_argument, 0, 'C'},\n        {\"listen-backlog\", required_argument, 0, 'b'},\n        {\"protocol\", required_argument, 0, 'B'},\n        {\"max-item-size\", required_argument, 0, 'I'},\n        {\"enable-sasl\", no_argument, 0, 'S'},\n        {\"disable-flush-all\", no_argument, 0, 'F'},\n        {\"disable-dumping\", no_argument, 0, 'X'},\n        {\"disable-watch\", no_argument, 0, 'W'},\n        {\"auth-file\", required_argument, 0, 'Y'},\n        {\"memory-file\", required_argument, 0, 'e'},\n        {\"extended\", required_argument, 0, 'o'},\n        {\"napi-ids\", required_argument, 0, 'N'},\n        {0, 0, 0, 0}\n    };\n    int optindex;\n    while (-1 != (c = getopt_long(argc, argv, shortopts,\n                    longopts, &optindex))) {\n#else\n    while (-1 != (c = getopt(argc, argv, shortopts))) {\n#endif\n        if (optarg) {\n            while(isspace(optarg[0])) {\n                optarg++;\n            }\n        }\n        switch (c) {\n        case 'A':\n            /* enables \"shutdown\" command */\n            settings.shutdown_command = true;\n            break;\n        case 'Z':\n            /* enable secure communication*/\n#ifdef TLS\n            settings.ssl_enabled = true;\n#else\n            fprintf(stderr, \"This server is not built with TLS support.\\n\");\n            exit(EX_USAGE);\n#endif\n            break;\n        case 'a':\n#ifndef DISABLE_UNIX_SOCKET\n            /* access for unix domain socket, as octal mask (like chmod)*/\n            settings.access= strtol(optarg,NULL,8);\n#else\n            fprintf(stderr, \"This server is not built with unix socket support.\\n\");\n            exit(EX_USAGE);\n#endif /* #ifndef DISABLE_UNIX_SOCKET */\n            break;\n        case 'U':\n            settings.udpport = atoi(optarg);\n            udp_specified = true;\n            break;\n        case 'p':\n            settings.port = atoi(optarg);\n            tcp_specified = true;\n            break;\n        case 's':\n#ifndef DISABLE_UNIX_SOCKET\n            settings.socketpath = optarg;\n#else\n            fprintf(stderr, \"This server is not built with unix socket support.\\n\");\n            exit(EX_USAGE);\n#endif /* #ifndef DISABLE_UNIX_SOCKET */\n            break;\n        case 'm':\n            settings.maxbytes = ((size_t)atoi(optarg)) * 1024 * 1024;\n            break;\n        case 'M':\n            settings.evict_to_free = 0;\n            break;\n        case 'c':\n            settings.maxconns = atoi(optarg);\n            if (settings.maxconns <= 0) {\n                fprintf(stderr, \"Maximum connections must be greater than 0\\n\");\n                return 1;\n            }\n            break;\n        case 'h':\n            usage();\n            exit(EXIT_SUCCESS);\n        case 'i':\n            usage_license();\n            exit(EXIT_SUCCESS);\n        case 'V':\n            printf(PACKAGE \" \" VERSION \"\\n\");\n            exit(EXIT_SUCCESS);\n        case 'k':\n            lock_memory = true;\n            break;\n        case 'v':\n            settings.verbose++;\n            break;\n        case 'l':\n            if (settings.inter != NULL) {\n                if (strstr(settings.inter, optarg) != NULL) {\n                    break;\n                }\n                size_t len = strlen(settings.inter) + strlen(optarg) + 2;\n                char *p = malloc(len);\n                if (p == NULL) {\n                    fprintf(stderr, \"Failed to allocate memory\\n\");\n                    return 1;\n                }\n                snprintf(p, len, \"%s,%s\", settings.inter, optarg);\n                free(settings.inter);\n                settings.inter = p;\n            } else {\n                settings.inter= strdup(optarg);\n            }\n            break;\n        case 'd':\n            do_daemonize = true;\n            break;\n        case 'r':\n            maxcore = 1;\n            break;\n        case 'R':\n            settings.reqs_per_event = atoi(optarg);\n            if (settings.reqs_per_event <= 0) {\n                fprintf(stderr, \"Number of requests per event must be greater than 0\\n\");\n                return 1;\n            }\n            break;\n        case 'u':\n            username = optarg;\n            break;\n        case 'P':\n            pid_file = optarg;\n            break;\n        case 'e':\n            settings.memory_file = optarg;\n            break;\n        case 'f':\n            settings.factor = atof(optarg);\n            if (settings.factor <= 1.0) {\n                fprintf(stderr, \"Factor must be greater than 1\\n\");\n                return 1;\n            }\n            meta->slab_config = strdup(optarg);\n            break;\n        case 'n':\n            settings.chunk_size = atoi(optarg);\n            if (settings.chunk_size <= 0) {\n                fprintf(stderr, \"Chunk size must be greater than 0\\n\");\n                return 1;\n            }\n            break;\n        case 't':\n            settings.num_threads = atoi(optarg);\n            if (settings.num_threads <= 0) {\n                fprintf(stderr, \"Number of threads must be greater than 0\\n\");\n                return 1;\n            }\n            /* There're other problems when you get above 64 threads.\n             * In the future we should portably detect # of cores for the\n             * default.\n             */\n            if (settings.num_threads > 64) {\n                fprintf(stderr, \"WARNING: Setting a high number of worker\"\n                                \"threads is not recommended.\\n\"\n                                \" Set this value to the number of cores in\"\n                                \" your machine or less.\\n\");\n            }\n            break;\n        case 'D':\n            if (! optarg || ! optarg[0]) {\n                fprintf(stderr, \"No delimiter specified\\n\");\n                return 1;\n            }\n            settings.prefix_delimiter = optarg[0];\n            settings.detail_enabled = 1;\n            break;\n        case 'L' :\n            if (enable_large_pages() == 0) {\n                preallocate = true;\n            } else {\n                fprintf(stderr, \"Cannot enable large pages on this system\\n\"\n                    \"(There is no support as of this version)\\n\");\n                return 1;\n            }\n            break;\n        case 'C' :\n            settings.use_cas = false;\n            break;\n        case 'b' :\n            settings.backlog = atoi(optarg);\n            break;\n        case 'B':\n            protocol_specified = true;\n            if (strcmp(optarg, \"auto\") == 0) {\n                settings.binding_protocol = negotiating_prot;\n            } else if (strcmp(optarg, \"binary\") == 0) {\n                settings.binding_protocol = binary_prot;\n            } else if (strcmp(optarg, \"ascii\") == 0) {\n                settings.binding_protocol = ascii_prot;\n            } else {\n                fprintf(stderr, \"Invalid value for binding protocol: %s\\n\"\n                        \" -- should be one of auto, binary, or ascii\\n\", optarg);\n                exit(EX_USAGE);\n            }\n            break;\n        case 'I':\n            buf = strdup(optarg);\n            unit = buf[strlen(buf)-1];\n            if (unit == 'k' || unit == 'm' ||\n                unit == 'K' || unit == 'M') {\n                buf[strlen(buf)-1] = '\\0';\n                size_max = atoi(buf);\n                if (unit == 'k' || unit == 'K')\n                    size_max *= 1024;\n                if (unit == 'm' || unit == 'M')\n                    size_max *= 1024 * 1024;\n                settings.item_size_max = size_max;\n            } else {\n                settings.item_size_max = atoi(buf);\n            }\n            free(buf);\n            break;\n        case 'S': /* set Sasl authentication to true. Default is false */\n#ifndef ENABLE_SASL\n            fprintf(stderr, \"This server is not built with SASL support.\\n\");\n            exit(EX_USAGE);\n#endif\n            settings.sasl = true;\n            break;\n       case 'F' :\n            settings.flush_enabled = false;\n            break;\n       case 'X' :\n            settings.dump_enabled = false;\n            break;\n       case 'W' :\n            settings.watch_enabled = false;\n            break;\n       case 'Y' :\n            // dupe the file path now just in case the options get mangled.\n            settings.auth_file = strdup(optarg);\n            break;\n       case 'N':\n            settings.num_napi_ids = atoi(optarg);\n            if (settings.num_napi_ids <= 0) {\n                fprintf(stderr, \"Maximum number of NAPI IDs must be greater than 0\\n\");\n                return 1;\n            }\n            break;\n        case 'o': /* It's sub-opts time! */\n            subopts_orig = subopts = strdup(optarg); /* getsubopt() changes the original args */\n\n            while (*subopts != '\\0') {\n            // BSD getsubopt (at least) has undefined behavior on -1, so\n            // if we want to retry the getsubopt call in submodules we\n            // need an extra layer of string copies.\n            char *subopts_temp_o = NULL;\n            char *subopts_temp = subopts_temp_o = strdup(subopts);\n\n            switch (getsubopt(&subopts, subopts_tokens, &subopts_value)) {\n            case MAXCONNS_FAST:\n                settings.maxconns_fast = true;\n                break;\n            case HASHPOWER_INIT:\n                if (subopts_value == NULL) {\n                    fprintf(stderr, \"Missing numeric argument for hashpower\\n\");\n                    return 1;\n                }\n                settings.hashpower_init = atoi(subopts_value);\n                if (settings.hashpower_init < 12) {\n                    fprintf(stderr, \"Initial hashtable multiplier of %d is too low\\n\",\n                        settings.hashpower_init);\n                    return 1;\n                } else if (settings.hashpower_init > 32) {\n                    fprintf(stderr, \"Initial hashtable multiplier of %d is too high\\n\"\n                        \"Choose a value based on \\\"STAT hash_power_level\\\" from a running instance\\n\",\n                        settings.hashpower_init);\n                    return 1;\n                }\n                break;\n            case NO_HASHEXPAND:\n                start_assoc_maint = false;\n                break;\n            case SLAB_REASSIGN:\n                settings.slab_reassign = true;\n                break;\n            case SLAB_AUTOMOVE:\n                if (subopts_value == NULL) {\n                    settings.slab_automove = 1;\n                    break;\n                }\n                settings.slab_automove = atoi(subopts_value);\n                if (settings.slab_automove < 0 || settings.slab_automove > 2) {\n                    fprintf(stderr, \"slab_automove must be between 0 and 2\\n\");\n                    return 1;\n                }\n                break;\n            case SLAB_AUTOMOVE_RATIO:\n                if (subopts_value == NULL) {\n                    fprintf(stderr, \"Missing slab_automove_ratio argument\\n\");\n                    return 1;\n                }\n                settings.slab_automove_ratio = atof(subopts_value);\n                if (settings.slab_automove_ratio <= 0 || settings.slab_automove_ratio > 1) {\n                    fprintf(stderr, \"slab_automove_ratio must be > 0 and < 1\\n\");\n                    return 1;\n                }\n                break;\n            case SLAB_AUTOMOVE_WINDOW:\n                if (subopts_value == NULL) {\n                    fprintf(stderr, \"Missing slab_automove_window argument\\n\");\n                    return 1;\n                }\n                settings.slab_automove_window = atoi(subopts_value);\n                if (settings.slab_automove_window < 3) {\n                    fprintf(stderr, \"slab_automove_window must be > 2\\n\");\n                    return 1;\n                }\n                break;\n            case TAIL_REPAIR_TIME:\n                if (subopts_value == NULL) {\n                    fprintf(stderr, \"Missing numeric argument for tail_repair_time\\n\");\n                    return 1;\n                }\n                settings.tail_repair_time = atoi(subopts_value);\n                if (settings.tail_repair_time < 10) {\n                    fprintf(stderr, \"Cannot set tail_repair_time to less than 10 seconds\\n\");\n                    return 1;\n                }\n                break;\n            case HASH_ALGORITHM:\n                if (subopts_value == NULL) {\n                    fprintf(stderr, \"Missing hash_algorithm argument\\n\");\n                    return 1;\n                };\n                if (strcmp(subopts_value, \"jenkins\") == 0) {\n                    hash_type = JENKINS_HASH;\n                } else if (strcmp(subopts_value, \"murmur3\") == 0) {\n                    hash_type = MURMUR3_HASH;\n                } else if (strcmp(subopts_value, \"xxh3\") == 0) {\n                    hash_type = XXH3_HASH;\n                } else {\n                    fprintf(stderr, \"Unknown hash_algorithm option (jenkins, murmur3, xxh3)\\n\");\n                    return 1;\n                }\n                break;\n            case LRU_CRAWLER:\n                start_lru_crawler = true;\n                break;\n            case LRU_CRAWLER_SLEEP:\n                if (subopts_value == NULL) {\n                    fprintf(stderr, \"Missing lru_crawler_sleep value\\n\");\n                    return 1;\n                }\n                settings.lru_crawler_sleep = atoi(subopts_value);\n                if (settings.lru_crawler_sleep > 1000000 || settings.lru_crawler_sleep < 0) {\n                    fprintf(stderr, \"LRU crawler sleep must be between 0 and 1 second\\n\");\n                    return 1;\n                }\n                break;\n            case LRU_CRAWLER_TOCRAWL:\n                if (subopts_value == NULL) {\n                    fprintf(stderr, \"Missing lru_crawler_tocrawl value\\n\");\n                    return 1;\n                }\n                if (!safe_strtoul(subopts_value, &tocrawl)) {\n                    fprintf(stderr, \"lru_crawler_tocrawl takes a numeric 32bit value\\n\");\n                    return 1;\n                }\n                settings.lru_crawler_tocrawl = tocrawl;\n                break;\n            case LRU_MAINTAINER:\n                start_lru_maintainer = true;\n                settings.lru_segmented = true;\n                break;\n            case HOT_LRU_PCT:\n                if (subopts_value == NULL) {\n                    fprintf(stderr, \"Missing hot_lru_pct argument\\n\");\n                    return 1;\n                }\n                settings.hot_lru_pct = atoi(subopts_value);\n                if (settings.hot_lru_pct < 1 || settings.hot_lru_pct >= 80) {\n                    fprintf(stderr, \"hot_lru_pct must be > 1 and < 80\\n\");\n                    return 1;\n                }\n                break;\n            case WARM_LRU_PCT:\n                if (subopts_value == NULL) {\n                    fprintf(stderr, \"Missing warm_lru_pct argument\\n\");\n                    return 1;\n                }\n                settings.warm_lru_pct = atoi(subopts_value);\n                if (settings.warm_lru_pct < 1 || settings.warm_lru_pct >= 80) {\n                    fprintf(stderr, \"warm_lru_pct must be > 1 and < 80\\n\");\n                    return 1;\n                }\n                break;\n            case HOT_MAX_FACTOR:\n                if (subopts_value == NULL) {\n                    fprintf(stderr, \"Missing hot_max_factor argument\\n\");\n                    return 1;\n                }\n                settings.hot_max_factor = atof(subopts_value);\n                if (settings.hot_max_factor <= 0) {\n                    fprintf(stderr, \"hot_max_factor must be > 0\\n\");\n                    return 1;\n                }\n                break;\n            case WARM_MAX_FACTOR:\n                if (subopts_value == NULL) {\n                    fprintf(stderr, \"Missing warm_max_factor argument\\n\");\n                    return 1;\n                }\n                settings.warm_max_factor = atof(subopts_value);\n                if (settings.warm_max_factor <= 0) {\n                    fprintf(stderr, \"warm_max_factor must be > 0\\n\");\n                    return 1;\n                }\n                break;\n            case TEMPORARY_TTL:\n                if (subopts_value == NULL) {\n                    fprintf(stderr, \"Missing temporary_ttl argument\\n\");\n                    return 1;\n                }\n                settings.temp_lru = true;\n                settings.temporary_ttl = atoi(subopts_value);\n                break;\n            case IDLE_TIMEOUT:\n                if (subopts_value == NULL) {\n                    fprintf(stderr, \"Missing numeric argument for idle_timeout\\n\");\n                    return 1;\n                }\n                settings.idle_timeout = atoi(subopts_value);\n                break;\n            case WATCHER_LOGBUF_SIZE:\n                if (subopts_value == NULL) {\n                    fprintf(stderr, \"Missing watcher_logbuf_size argument\\n\");\n                    return 1;\n                }\n                if (!safe_strtoul(subopts_value, &settings.logger_watcher_buf_size)) {\n                    fprintf(stderr, \"could not parse argument to watcher_logbuf_size\\n\");\n                    return 1;\n                }\n                settings.logger_watcher_buf_size *= 1024; /* kilobytes */\n                break;\n            case WORKER_LOGBUF_SIZE:\n                if (subopts_value == NULL) {\n                    fprintf(stderr, \"Missing worker_logbuf_size argument\\n\");\n                    return 1;\n                }\n                if (!safe_strtoul(subopts_value, &settings.logger_buf_size)) {\n                    fprintf(stderr, \"could not parse argument to worker_logbuf_size\\n\");\n                    return 1;\n                }\n                settings.logger_buf_size *= 1024; /* kilobytes */\n            case SLAB_SIZES:\n                slab_sizes_unparsed = strdup(subopts_value);\n                break;\n            case SLAB_CHUNK_MAX:\n                if (subopts_value == NULL) {\n                    fprintf(stderr, \"Missing slab_chunk_max argument\\n\");\n                    return 1;\n                }\n                if (!safe_strtol(subopts_value, &settings.slab_chunk_size_max)) {\n                    fprintf(stderr, \"could not parse argument to slab_chunk_max\\n\");\n                    return 1;\n                }\n                if (settings.slab_chunk_size_max <= 0) {\n                    fprintf(stderr, \"slab_chunk_max must be >= 0\\n\");\n                    return 1;\n                }\n                if (settings.slab_chunk_size_max > (1 << 10)) {\n                    fprintf(stderr, \"slab_chunk_max must be specified in kilobytes.\\n\");\n                    return 1;\n                }\n                settings.slab_chunk_size_max *= (1 << 10);\n                slab_chunk_size_changed = true;\n                break;\n            case TRACK_SIZES:\n                item_stats_sizes_init();\n                break;\n            case NO_INLINE_ASCII_RESP:\n                break;\n            case INLINE_ASCII_RESP:\n                break;\n            case NO_CHUNKED_ITEMS:\n                settings.slab_chunk_size_max = settings.slab_page_size;\n                break;\n            case NO_SLAB_REASSIGN:\n                settings.slab_reassign = false;\n                break;\n            case NO_SLAB_AUTOMOVE:\n                settings.slab_automove = 0;\n                break;\n            case NO_MAXCONNS_FAST:\n                settings.maxconns_fast = false;\n                break;\n            case NO_LRU_CRAWLER:\n                settings.lru_crawler = false;\n                start_lru_crawler = false;\n                break;\n            case NO_LRU_MAINTAINER:\n                start_lru_maintainer = false;\n                settings.lru_segmented = false;\n                break;\n#ifdef TLS\n            case SSL_CERT:\n                if (subopts_value == NULL) {\n                    fprintf(stderr, \"Missing ssl_chain_cert argument\\n\");\n                    return 1;\n                }\n                settings.ssl_chain_cert = strdup(subopts_value);\n                break;\n            case SSL_KEY:\n                if (subopts_value == NULL) {\n                    fprintf(stderr, \"Missing ssl_key argument\\n\");\n                    return 1;\n                }\n                settings.ssl_key = strdup(subopts_value);\n                break;\n            case SSL_VERIFY_MODE:\n            {\n                if (subopts_value == NULL) {\n                    fprintf(stderr, \"Missing ssl_verify_mode argument\\n\");\n                    return 1;\n                }\n                int verify  = 0;\n                if (!safe_strtol(subopts_value, &verify)) {\n                    fprintf(stderr, \"could not parse argument to ssl_verify_mode\\n\");\n                    return 1;\n                }\n                if (!ssl_set_verify_mode(verify)) {\n                    fprintf(stderr, \"Invalid ssl_verify_mode. Use help to see valid options.\\n\");\n                    return 1;\n                }\n                break;\n            }\n            case SSL_KEYFORM:\n                if (subopts_value == NULL) {\n                    fprintf(stderr, \"Missing ssl_keyformat argument\\n\");\n                    return 1;\n                }\n                if (!safe_strtol(subopts_value, &settings.ssl_keyformat)) {\n                    fprintf(stderr, \"could not parse argument to ssl_keyformat\\n\");\n                    return 1;\n                }\n                break;\n            case SSL_CIPHERS:\n                if (subopts_value == NULL) {\n                    fprintf(stderr, \"Missing ssl_ciphers argument\\n\");\n                    return 1;\n                }\n                settings.ssl_ciphers = strdup(subopts_value);\n                break;\n            case SSL_CA_CERT:\n                if (subopts_value == NULL) {\n                    fprintf(stderr, \"Missing ssl_ca_cert argument\\n\");\n                    return 1;\n                }\n                settings.ssl_ca_cert = strdup(subopts_value);\n                break;\n            case SSL_WBUF_SIZE:\n                if (subopts_value == NULL) {\n                    fprintf(stderr, \"Missing ssl_wbuf_size argument\\n\");\n                    return 1;\n                }\n                if (!safe_strtoul(subopts_value, &settings.ssl_wbuf_size)) {\n                    fprintf(stderr, \"could not parse argument to ssl_wbuf_size\\n\");\n                    return 1;\n                }\n                settings.ssl_wbuf_size *= 1024; /* kilobytes */\n                break;\n            case SSL_SESSION_CACHE:\n                settings.ssl_session_cache = true;\n                break;\n            case SSL_KERNEL_TLS:\n                settings.ssl_kernel_tls = true;\n                break;\n            case SSL_MIN_VERSION: {\n                int min_version;\n                if (subopts_value == NULL) {\n                    fprintf(stderr, \"Missing ssl_min_version argument\\n\");\n                    return 1;\n                }\n                if (!safe_strtol(subopts_value, &min_version)) {\n                    fprintf(stderr, \"could not parse argument to ssl_min_version\\n\");\n                    return 1;\n                }\n                if (!ssl_set_min_version(min_version)) {\n                    fprintf(stderr, \"Invalid ssl_min_version. Use help to see valid options.\\n\");\n                    return 1;\n                }\n                break;\n            }\n#endif\n            case MODERN:\n                /* currently no new defaults */\n                break;\n            case NO_MODERN:\n                if (!slab_chunk_size_changed) {\n                    settings.slab_chunk_size_max = settings.slab_page_size;\n                }\n                settings.slab_reassign = false;\n                settings.slab_automove = 0;\n                settings.maxconns_fast = false;\n                settings.lru_segmented = false;\n                hash_type = JENKINS_HASH;\n                start_lru_crawler = false;\n                start_lru_maintainer = false;\n                break;\n            case NO_DROP_PRIVILEGES:\n                settings.drop_privileges = false;\n                break;\n            case DROP_PRIVILEGES:\n                settings.drop_privileges = true;\n                break;\n            case RESP_OBJ_MEM_LIMIT:\n                // TODO: Remove at some point in the future.\n                fprintf(stderr, \"DEPRECATED: resp_obj_mem_limit no longer used. See read_buf_mem_limit,\\n\");\n                break;\n            case READ_BUF_MEM_LIMIT:\n                if (subopts_value == NULL) {\n                    fprintf(stderr, \"Missing read_buf_mem_limit argument\\n\");\n                    return 1;\n                }\n                if (!safe_strtoul(subopts_value, &settings.read_buf_mem_limit)) {\n                    fprintf(stderr, \"could not parse argument to read_buf_mem_limit\\n\");\n                    return 1;\n                }\n                settings.read_buf_mem_limit *= 1024 * 1024; /* megabytes */\n                break;\n#ifdef PROXY\n            case PROXY_CONFIG:\n                if (subopts_value == NULL) {\n                    fprintf(stderr, \"Missing proxy_config file argument\\n\");\n                    return 1;\n                }\n                if (protocol_specified) {\n                    fprintf(stderr, \"Cannot specify a protocol with proxy mode enabled\\n\");\n                    return 1;\n                }\n                settings.proxy_startfile = strdup(subopts_value);\n                settings.proxy_enabled = true;\n                settings.binding_protocol = proxy_prot;\n                protocol_specified = true;\n                break;\n            case PROXY_ARG:\n                settings.proxy_startarg = strdup(subopts_value);\n                break;\n            case PROXY_URING:\n                settings.proxy_uring = true;\n                break;\n            case PROXY_MEMPROFILE:\n                settings.proxy_memprofile = true;\n                break;\n#endif\n#ifdef MEMCACHED_DEBUG\n            case RELAXED_PRIVILEGES:\n                settings.relaxed_privileges = true;\n                break;\n#endif\n#ifdef SOCK_COOKIE_ID\n            case COOKIE_ID:\n                (void)safe_strtoul(subopts_value, &settings.sock_cookie_id);\n                break;\n#endif\n            default:\n#ifdef EXTSTORE\n                // TODO: differentiating response code.\n                if (storage_read_config(storage_cf, &subopts_temp)) {\n                    return 1;\n                }\n#else\n                printf(\"Illegal suboption \\\"%s\\\"\\n\", subopts_temp);\n                return 1;\n#endif\n            } // switch\n            if (subopts_temp_o) {\n                free(subopts_temp_o);\n            }\n\n            } // while\n            free(subopts_orig);\n            break;\n        default:\n            fprintf(stderr, \"Illegal argument \\\"%c\\\"\\n\", c);\n            return 1;\n        }\n    }\n\n    if (settings.num_napi_ids > settings.num_threads) {\n        fprintf(stderr, \"Number of napi_ids(%d) cannot be greater than number of threads(%d)\\n\",\n                settings.num_napi_ids, settings.num_threads);\n        exit(EX_USAGE);\n    }\n\n    if (settings.item_size_max < ITEM_SIZE_MAX_LOWER_LIMIT) {\n        fprintf(stderr, \"Item max size cannot be less than 1024 bytes.\\n\");\n        exit(EX_USAGE);\n    }\n    if (settings.item_size_max > (settings.maxbytes / 2)) {\n        fprintf(stderr, \"Cannot set item size limit higher than 1/2 of memory max.\\n\");\n        exit(EX_USAGE);\n    }\n    if (settings.item_size_max > (ITEM_SIZE_MAX_UPPER_LIMIT)) {\n        fprintf(stderr, \"Cannot set item size limit higher than a gigabyte.\\n\");\n        exit(EX_USAGE);\n    }\n    if (settings.item_size_max > 1024 * 1024) {\n        if (!slab_chunk_size_changed) {\n            // Ideal new default is 16k, but needs stitching.\n            settings.slab_chunk_size_max = settings.slab_page_size / 2;\n        }\n    }\n\n    if (settings.slab_chunk_size_max > settings.item_size_max) {\n        fprintf(stderr, \"slab_chunk_max (bytes: %d) cannot be larger than -I (item_size_max %d)\\n\",\n                settings.slab_chunk_size_max, settings.item_size_max);\n        exit(EX_USAGE);\n    }\n\n    if (settings.item_size_max % settings.slab_chunk_size_max != 0) {\n        fprintf(stderr, \"-I (item_size_max: %d) must be evenly divisible by slab_chunk_max (bytes: %d)\\n\",\n                settings.item_size_max, settings.slab_chunk_size_max);\n        exit(EX_USAGE);\n    }\n\n    if (settings.slab_page_size % settings.slab_chunk_size_max != 0) {\n        fprintf(stderr, \"slab_chunk_max (bytes: %d) must divide evenly into %d (slab_page_size)\\n\",\n                settings.slab_chunk_size_max, settings.slab_page_size);\n        exit(EX_USAGE);\n    }\n#ifdef EXTSTORE\n    switch (storage_check_config(storage_cf)) {\n        case 0:\n            storage_enabled = true;\n            break;\n        case 1:\n            exit(EX_USAGE);\n            break;\n    }\n#endif\n    // Reserve this for the new default. If factor size hasn't changed, use\n    // new default.\n    /*if (settings.slab_chunk_size_max == 16384 && settings.factor == 1.25) {\n        settings.factor = 1.08;\n    }*/\n\n    if (slab_sizes_unparsed != NULL) {\n        // want the unedited string for restart code.\n        char *temp = strdup(slab_sizes_unparsed);\n        if (_parse_slab_sizes(slab_sizes_unparsed, slab_sizes)) {\n            use_slab_sizes = true;\n            if (meta->slab_config) {\n                free(meta->slab_config);\n            }\n            meta->slab_config = temp;\n        } else {\n            exit(EX_USAGE);\n        }\n    } else if (!meta->slab_config) {\n        // using the default factor.\n        meta->slab_config = \"1.25\";\n    }\n\n    if (settings.hot_lru_pct + settings.warm_lru_pct > 80) {\n        fprintf(stderr, \"hot_lru_pct + warm_lru_pct cannot be more than 80%% combined\\n\");\n        exit(EX_USAGE);\n    }\n\n    if (settings.temp_lru && !start_lru_maintainer) {\n        fprintf(stderr, \"temporary_ttl requires lru_maintainer to be enabled\\n\");\n        exit(EX_USAGE);\n    }\n\n    if (hash_init(hash_type) != 0) {\n        fprintf(stderr, \"Failed to initialize hash_algorithm!\\n\");\n        exit(EX_USAGE);\n    }\n\n    /*\n     * Use one workerthread to serve each UDP port if the user specified\n     * multiple ports\n     */\n    if (settings.inter != NULL && strchr(settings.inter, ',')) {\n        settings.num_threads_per_udp = 1;\n    } else {\n        settings.num_threads_per_udp = settings.num_threads;\n    }\n\n    if (settings.sasl) {\n        if (!protocol_specified) {\n            settings.binding_protocol = binary_prot;\n        } else {\n            if (settings.binding_protocol != binary_prot) {\n                fprintf(stderr, \"ERROR: You cannot allow the ASCII protocol while using SASL.\\n\");\n                exit(EX_USAGE);\n            }\n        }\n\n        if (settings.udpport) {\n            fprintf(stderr, \"ERROR: Cannot enable UDP while using binary SASL authentication.\\n\");\n            exit(EX_USAGE);\n        }\n    }\n\n    if (settings.auth_file) {\n        if (!protocol_specified) {\n            settings.binding_protocol = ascii_prot;\n        } else {\n            if (settings.binding_protocol != ascii_prot) {\n                fprintf(stderr, \"ERROR: You cannot allow the BINARY protocol while using ascii authentication tokens.\\n\");\n                exit(EX_USAGE);\n            }\n        }\n    }\n\n    if (udp_specified && settings.udpport != 0 && !tcp_specified) {\n        settings.port = settings.udpport;\n    }\n\n    if (settings.port > 65535) {\n        fprintf(stderr, \"ERROR: Invalid port number %d.\\n\", settings.port);\n        exit(EX_USAGE);\n    }\n\n\n    /*\n     * Setup SSL if enabled\n     */\n    if (settings.ssl_enabled) {\n        if (!settings.port) {\n            fprintf(stderr, \"ERROR: You cannot enable SSL without a TCP port.\\n\");\n            exit(EX_USAGE);\n        }\n        // Initiate the SSL context.\n        ssl_init();\n    }\n\n    if (maxcore != 0) {\n        struct rlimit rlim_new;\n        /*\n         * First try raising to infinity; if that fails, try bringing\n         * the soft limit to the hard.\n         */\n        if (getrlimit(RLIMIT_CORE, &rlim) == 0) {\n            rlim_new.rlim_cur = rlim_new.rlim_max = RLIM_INFINITY;\n            if (setrlimit(RLIMIT_CORE, &rlim_new)!= 0) {\n                /* failed. try raising just to the old max */\n                rlim_new.rlim_cur = rlim_new.rlim_max = rlim.rlim_max;\n                (void)setrlimit(RLIMIT_CORE, &rlim_new);\n            }\n        }\n        /*\n         * getrlimit again to see what we ended up with. Only fail if\n         * the soft limit ends up 0, because then no core files will be\n         * created at all.\n         */\n\n        if ((getrlimit(RLIMIT_CORE, &rlim) != 0) || rlim.rlim_cur == 0) {\n            fprintf(stderr, \"failed to ensure corefile creation\\n\");\n            exit(EX_OSERR);\n        }\n    }\n\n    /*\n     * If needed, increase rlimits to allow as many connections\n     * as needed.\n     */\n\n    if (getrlimit(RLIMIT_NOFILE, &rlim) != 0) {\n        fprintf(stderr, \"failed to getrlimit number of files\\n\");\n        exit(EX_OSERR);\n    } else {\n        rlim.rlim_cur = settings.maxconns;\n        rlim.rlim_max = settings.maxconns;\n        if (setrlimit(RLIMIT_NOFILE, &rlim) != 0) {\n#ifndef MEMCACHED_DEBUG\n            fprintf(stderr, \"failed to set rlimit for open files. Try starting as root or requesting smaller maxconns value.\\n\");\n            exit(EX_OSERR);\n#endif\n        }\n    }\n\n    /* lose root privileges if we have them */\n    if (getuid() == 0 || geteuid() == 0) {\n        if (username == 0 || *username == '\\0') {\n            fprintf(stderr, \"can't run as root without the -u switch\\n\");\n            exit(EX_USAGE);\n        }\n        if ((pw = getpwnam(username)) == 0) {\n            fprintf(stderr, \"can't find the user %s to switch to\\n\", username);\n            exit(EX_NOUSER);\n        }\n        if (setgroups(0, NULL) < 0) {\n            /* setgroups may fail with EPERM, indicating we are already in a\n             * minimally-privileged state. In that case we continue. For all\n             * other failure codes we exit.\n             *\n             * Note that errno is stored here because fprintf may change it.\n             */\n            bool should_exit = errno != EPERM;\n            fprintf(stderr, \"failed to drop supplementary groups: %s\\n\",\n                    strerror(errno));\n            if (should_exit) {\n                exit(EX_OSERR);\n            }\n        }\n        if (setgid(pw->pw_gid) < 0 || setuid(pw->pw_uid) < 0) {\n            fprintf(stderr, \"failed to assume identity of user %s\\n\", username);\n            exit(EX_OSERR);\n        }\n    }\n\n    /* Initialize Sasl if -S was specified */\n    if (settings.sasl) {\n        init_sasl();\n    }\n\n    /* daemonize if requested */\n    /* if we want to ensure our ability to dump core, don't chdir to / */\n    if (do_daemonize) {\n        if (daemonize(maxcore, settings.verbose) == -1) {\n            fprintf(stderr, \"failed to daemon() in order to daemonize\\n\");\n            exit(EXIT_FAILURE);\n        }\n    }\n\n    /* lock paged memory if needed */\n    if (lock_memory) {\n#ifdef HAVE_MLOCKALL\n        int res = mlockall(MCL_CURRENT | MCL_FUTURE);\n        if (res != 0) {\n            fprintf(stderr, \"warning: -k invalid, mlockall() failed: %s\\n\",\n                    strerror(errno));\n        }\n#else\n        fprintf(stderr, \"warning: -k invalid, mlockall() not supported on this platform.  proceeding without.\\n\");\n#endif\n    }\n\n    /* initialize main thread libevent instance */\n#if defined(LIBEVENT_VERSION_NUMBER) && LIBEVENT_VERSION_NUMBER >= 0x02000101\n    /* If libevent version is larger/equal to 2.0.2-alpha, use newer version */\n    struct event_config *ev_config;\n    ev_config = event_config_new();\n    event_config_set_flag(ev_config, EVENT_BASE_FLAG_NOLOCK);\n    main_base = event_base_new_with_config(ev_config);\n    event_config_free(ev_config);\n#else\n    /* Otherwise, use older API */\n    main_base = event_init();\n#endif\n\n    /* Load initial auth file if required */\n    if (settings.auth_file) {\n        if (settings.udpport) {\n            fprintf(stderr, \"Cannot use UDP with ascii authentication enabled (-U 0 to disable)\\n\");\n            exit(EX_USAGE);\n        }\n\n        switch (authfile_load(settings.auth_file)) {\n            case AUTHFILE_STATFAIL:\n                vperror(\"Could not stat authfile [%s], error %s\", settings.auth_file\n                                                            , strerror(errno));\n                exit(EXIT_FAILURE);\n                break;\n            case AUTHFILE_OPENFAIL:\n                vperror(\"Could not open authfile [%s] for reading, error %s\", settings.auth_file\n                                                                           , strerror(errno));\n                exit(EXIT_FAILURE);\n                break;\n            case AUTHFILE_OOM:\n                fprintf(stderr, \"Out of memory reading password file: %s\", settings.auth_file);\n                exit(EXIT_FAILURE);\n                break;\n            case AUTHFILE_MALFORMED:\n                fprintf(stderr, \"Authfile [%s] has a malformed entry. Should be 'user:password'\", settings.auth_file);\n                exit(EXIT_FAILURE);\n                break;\n            case AUTHFILE_OK:\n                break;\n        }\n    }\n\n    /* initialize other stuff */\n    stats_init();\n    logger_init();\n    logger_create(); // main process logger\n    conn_init();\n    bool reuse_mem = false;\n    void *mem_base = NULL;\n    bool prefill = false;\n    if (settings.memory_file != NULL) {\n        preallocate = true;\n        // Easier to manage memory if we prefill the global pool when reusing.\n        prefill = true;\n        restart_register(\"main\", _mc_meta_load_cb, _mc_meta_save_cb, meta);\n        reuse_mem = restart_mmap_open(settings.maxbytes,\n                        settings.memory_file,\n                        &mem_base);\n        // The \"save\" callback gets called when we're closing out the mmap,\n        // but we don't know what the mmap_base is until after we call open.\n        // So we pass the struct above but have to fill it in here so the\n        // data's available during the save routine.\n        meta->mmap_base = mem_base;\n        // Also, the callbacks for load() run before _open returns, so we\n        // should have the old base in 'meta' as of here.\n    }\n    // Initialize the hash table _after_ checking restart metadata.\n    // We override the hash table start argument with what was live\n    // previously, to avoid filling a huge set of items into a tiny hash\n    // table.\n    assoc_init(settings.hashpower_init);\n#ifdef EXTSTORE\n    if (storage_enabled && reuse_mem) {\n        fprintf(stderr, \"[restart] memory restart with extstore not presently supported.\\n\");\n        reuse_mem = false;\n    }\n#endif\n    slabs_init(settings.maxbytes, settings.factor, preallocate,\n            use_slab_sizes ? slab_sizes : NULL, mem_base, reuse_mem);\n#ifdef EXTSTORE\n    if (storage_enabled) {\n        storage = storage_init(storage_cf);\n        if (storage == NULL) {\n            exit(EXIT_FAILURE);\n        }\n        ext_storage = storage;\n        /* page mover algorithm for extstore needs memory prefilled */\n        prefill = true;\n    }\n#endif\n\n    if (settings.drop_privileges) {\n        setup_privilege_violations_handler();\n    }\n\n    if (prefill)\n        slabs_prefill_global();\n    /* In restartable mode and we've decided to issue a fixup on memory */\n    if (settings.memory_file != NULL && reuse_mem) {\n        mc_ptr_t old_base = meta->old_base;\n        assert(old_base == meta->old_base);\n\n        // should've pulled in process_started from meta file.\n        process_started = meta->process_started;\n        // TODO: must be a more canonical way of serializing/deserializing\n        // pointers? passing through uint64_t should work, and we're not\n        // annotating the pointer with anything, but it's still slightly\n        // insane.\n        restart_fixup((void *)old_base);\n    }\n    /*\n     * ignore SIGPIPE signals; we can use errno == EPIPE if we\n     * need that information\n     */\n    if (signal(SIGPIPE, SIG_IGN) == SIG_ERR) {\n        perror(\"failed to ignore SIGPIPE; sigaction\");\n        exit(EX_OSERR);\n    }\n    /* start up worker threads if MT mode */\n#ifdef PROXY\n    if (settings.proxy_enabled) {\n        settings.proxy_ctx = proxy_init(settings.proxy_uring, settings.proxy_memprofile);\n    }\n#endif\n#ifdef EXTSTORE\n    memcached_thread_init(settings.num_threads, storage);\n    init_lru_crawler(storage);\n#else\n    memcached_thread_init(settings.num_threads, NULL);\n    init_lru_crawler(NULL);\n#endif\n\n#ifdef PROXY\n    if (settings.proxy_enabled) {\n        if (proxy_first_confload(settings.proxy_ctx) != 0) {\n            exit(EXIT_FAILURE);\n        }\n    }\n#endif\n\n    if (start_assoc_maint && start_assoc_maintenance_thread() == -1) {\n        exit(EXIT_FAILURE);\n    }\n    if (start_lru_crawler && start_item_crawler_thread() != 0) {\n        fprintf(stderr, \"Failed to enable LRU crawler thread\\n\");\n        exit(EXIT_FAILURE);\n    }\n#ifdef EXTSTORE\n    if (storage && start_storage_compact_thread(storage) != 0) {\n        fprintf(stderr, \"Failed to start storage compaction thread\\n\");\n        exit(EXIT_FAILURE);\n    }\n    if (storage && start_storage_write_thread(storage) != 0) {\n        fprintf(stderr, \"Failed to start storage writer thread\\n\");\n        exit(EXIT_FAILURE);\n    }\n\n    if (start_lru_maintainer && start_lru_maintainer_thread(storage) != 0) {\n#else\n    if (start_lru_maintainer && start_lru_maintainer_thread(NULL) != 0) {\n#endif\n        fprintf(stderr, \"Failed to enable LRU maintainer thread\\n\");\n        free(meta);\n        return 1;\n    }\n\n    if (settings.slab_reassign) {\n        settings.slab_rebal = start_slab_maintenance_thread(storage);\n        if (!settings.slab_rebal) {\n            exit(EXIT_FAILURE);\n        }\n    }\n\n    if (settings.idle_timeout && start_conn_timeout_thread() == -1) {\n        exit(EXIT_FAILURE);\n    }\n\n    /* initialise clock event */\n#if defined(HAVE_CLOCK_GETTIME) && defined(CLOCK_MONOTONIC)\n    {\n        struct timespec ts;\n        if (clock_gettime(CLOCK_MONOTONIC, &ts) == 0) {\n            monotonic = true;\n            monotonic_start = ts.tv_sec;\n            // Monotonic clock needs special handling for restarts.\n            // We get a start time at an arbitrary place, so we need to\n            // restore the original time delta, which is always \"now\" - _start\n            if (reuse_mem) {\n                // the running timespan at stop time + the time we think we\n                // were stopped.\n                monotonic_start -= meta->current_time + meta->time_delta;\n            } else {\n                monotonic_start -= ITEM_UPDATE_INTERVAL + 2;\n            }\n        }\n    }\n#endif\n    clock_handler(0, 0, 0);\n\n    /* create unix mode sockets after dropping privileges */\n    if (settings.socketpath != NULL) {\n        errno = 0;\n        if (server_socket_unix(settings.socketpath,settings.access)) {\n            vperror(\"failed to listen on UNIX socket: %s\", settings.socketpath);\n            exit(EX_OSERR);\n        }\n    }\n\n    /* create the listening socket, bind it, and init */\n    if (settings.socketpath == NULL) {\n        const char *portnumber_filename = getenv(\"MEMCACHED_PORT_FILENAME\");\n        char *temp_portnumber_filename = NULL;\n        size_t len;\n        FILE *portnumber_file = NULL;\n\n        if (portnumber_filename != NULL) {\n            len = strlen(portnumber_filename)+4+1;\n            temp_portnumber_filename = malloc(len);\n            if (temp_portnumber_filename == NULL) {\n                vperror(\"Failed to allocate memory for portnumber file\");\n                exit(EX_OSERR);\n            }\n            snprintf(temp_portnumber_filename,\n                     len,\n                     \"%s.lck\", portnumber_filename);\n\n            portnumber_file = fopen(temp_portnumber_filename, \"a\");\n            if (portnumber_file == NULL) {\n                fprintf(stderr, \"Failed to open \\\"%s\\\": %s\\n\",\n                        temp_portnumber_filename, strerror(errno));\n            }\n        }\n\n        errno = 0;\n        if (settings.port && server_sockets(settings.port, tcp_transport,\n                                           portnumber_file)) {\n            if (settings.inter == NULL) {\n                vperror(\"failed to listen on TCP port %d\", settings.port);\n            } else {\n                vperror(\"failed to listen on one of interface(s) %s\", settings.inter);\n            }\n            exit(EX_OSERR);\n        }\n\n        /*\n         * initialization order: first create the listening sockets\n         * (may need root on low ports), then drop root if needed,\n         * then daemonize if needed, then init libevent (in some cases\n         * descriptors created by libevent wouldn't survive forking).\n         */\n\n        /* create the UDP listening socket and bind it */\n        errno = 0;\n        if (settings.udpport && server_sockets(settings.udpport, udp_transport,\n                                              portnumber_file)) {\n            if (settings.inter == NULL) {\n                vperror(\"failed to listen on UDP port %d\", settings.udpport);\n            } else {\n                vperror(\"failed to listen on one of interface(s) %s\", settings.inter);\n            }\n            exit(EX_OSERR);\n        }\n\n        if (portnumber_file) {\n            fclose(portnumber_file);\n            rename(temp_portnumber_filename, portnumber_filename);\n        }\n        if (temp_portnumber_filename)\n            free(temp_portnumber_filename);\n    }\n\n    /* Give the sockets a moment to open. I know this is dumb, but the error\n     * is only an advisory.\n     */\n    usleep(1000);\n    if (stats_state.curr_conns + stats_state.reserved_fds >= settings.maxconns - 1) {\n        fprintf(stderr, \"Maxconns setting is too low, use -c to increase.\\n\");\n        exit(EXIT_FAILURE);\n    }\n\n    if (pid_file != NULL) {\n        save_pid(pid_file);\n    }\n\n    /* Drop privileges no longer needed */\n    if (settings.drop_privileges) {\n        drop_privileges();\n    }\n\n    /* Initialize the uriencode lookup table. */\n    uriencode_init();\n\n    /* enter the event loop */\n    while (!stop_main_loop) {\n        if (event_base_loop(main_base, EVLOOP_ONCE) != 0) {\n            retval = EXIT_FAILURE;\n            break;\n        }\n    }\n\n    switch (stop_main_loop) {\n        case GRACE_STOP:\n            fprintf(stderr, \"Gracefully stopping\\n\");\n        break;\n        case EXIT_NORMALLY:\n            // Don't need to print anything to STDERR for a normal shutdown except\n            // if we want to.\n\n            if (settings.verbose) {\n                fprintf(stderr, \"Exiting normally\\n\");\n            }\n\n        break;\n        default:\n            fprintf(stderr, \"Exiting on error\\n\");\n        break;\n    }\n\n    if (stop_main_loop == GRACE_STOP) {\n        stop_threads();\n        if (settings.memory_file != NULL) {\n            restart_mmap_close();\n        }\n    }\n\n    /* remove the PID file if we're a daemon */\n    if (do_daemonize)\n        remove_pidfile(pid_file);\n    /* Clean up strdup() call for bind() address */\n    if (settings.inter)\n      free(settings.inter);\n\n    /* cleanup base */\n    event_base_free(main_base);\n\n    free(meta);\n\n    return retval;\n}\n"
        },
        {
          "name": "memcached.h",
          "type": "blob",
          "size": 39.9970703125,
          "content": "/* -*- Mode: C; tab-width: 4; c-basic-offset: 4; indent-tabs-mode: nil -*- */\n\n/** \\file\n * The main memcached header holding commonly used data\n * structures and function prototypes.\n */\n\n#ifdef HAVE_CONFIG_H\n#include \"config.h\"\n#endif\n\n#include <sys/types.h>\n#include <sys/socket.h>\n#include <sys/time.h>\n#include <netinet/in.h>\n#include <event.h>\n#include <netdb.h>\n#include <pthread.h>\n#include <unistd.h>\n#include <assert.h>\n#include <grp.h>\n#include <signal.h>\n/* need this to get IOV_MAX on some platforms. */\n#ifndef __need_IOV_MAX\n#define __need_IOV_MAX\n#endif\n#include <limits.h>\n/* FreeBSD 4.x doesn't have IOV_MAX exposed. */\n#ifndef IOV_MAX\n#if defined(__FreeBSD__) || defined(__APPLE__) || defined(__GNU__)\n# define IOV_MAX 1024\n/* GNU/Hurd don't set MAXPATHLEN\n * http://www.gnu.org/software/hurd/hurd/porting/guidelines.html#PATH_MAX_tt_MAX_PATH_tt_MAXPATHL */\n#ifndef MAXPATHLEN\n#define MAXPATHLEN 4096\n#endif\n#endif\n#endif\n\n#if defined(__linux__)\n# define SOCK_COOKIE_ID SO_MARK\n#elif defined(__FreeBSD__)\n# define SOCK_COOKIE_ID SO_USER_COOKIE\n#elif defined(__OpenBSD__)\n# define SOCK_COOKIE_ID SO_RTABLE\n#endif\n\n#include \"itoa_ljust.h\"\n#include \"slabs_mover.h\"\n#include \"protocol_binary.h\"\n#include \"cache.h\"\n#include \"logger.h\"\n#include \"queue.h\"\n#include \"util.h\"\n\n#ifdef EXTSTORE\n#include \"crc32c.h\"\n#endif\n\n#include \"sasl_defs.h\"\n\n/* for NAPI pinning feature */\n#ifndef SO_INCOMING_NAPI_ID\n#define SO_INCOMING_NAPI_ID 56\n#endif\n\n/** Maximum length of a key. */\n#define KEY_MAX_LENGTH 250\n\n/** Maximum length of a uri encoded key. */\n#define KEY_MAX_URI_ENCODED_LENGTH (KEY_MAX_LENGTH  * 3 + 1)\n\n/** Size of an incr buf. */\n#define INCR_MAX_STORAGE_LEN 24\n\n#define WRITE_BUFFER_SIZE 1024\n#define READ_BUFFER_SIZE 16384\n#define READ_BUFFER_CACHED 0\n#define UDP_READ_BUFFER_SIZE 65536\n#define UDP_MAX_PAYLOAD_SIZE 1400\n#define UDP_HEADER_SIZE 8\n#define UDP_DATA_SIZE 1392 // UDP_MAX_PAYLOAD_SIZE - UDP_HEADER_SIZE\n#define MAX_SENDBUF_SIZE (256 * 1024 * 1024)\n\n/* Binary protocol stuff */\n#define BIN_MAX_EXTLEN 20 // length of the _incr command is currently the longest.\n\n/* Initial power multiplier for the hash table */\n#define HASHPOWER_DEFAULT 16\n#define HASHPOWER_MAX 32\n\n/* Abstract the size of an item's client flag suffix */\n#ifdef LARGE_CLIENT_FLAGS\ntypedef uint64_t client_flags_t;\n#define safe_strtoflags safe_strtoull\n#else\ntypedef uint32_t client_flags_t;\n#define safe_strtoflags safe_strtoul\n#endif\n\n/*\n * We only reposition items in the LRU queue if they haven't been repositioned\n * in this many seconds. That saves us from churning on frequently-accessed\n * items.\n */\n#define ITEM_UPDATE_INTERVAL 60\n\n/*\n * Valid range of the maximum size of an item, in bytes.\n */\n#define ITEM_SIZE_MAX_LOWER_LIMIT 1024\n#define ITEM_SIZE_MAX_UPPER_LIMIT 1024 * 1024 * 1024\n\n/* Slab sizing definitions. */\n#define POWER_SMALLEST 1\n#define POWER_LARGEST  256 /* actual cap is 255 */\n#define SLAB_GLOBAL_PAGE_POOL 0 /* magic slab class for storing pages for reassignment */\n#define CHUNK_ALIGN_BYTES 8\n/* slab class max is a 6-bit number, -1. */\n#define MAX_NUMBER_OF_SLAB_CLASSES (63 + 1)\n\n/** How long an object can reasonably be assumed to be locked before\n    harvesting it on a low memory condition. Default: disabled. */\n#define TAIL_REPAIR_TIME_DEFAULT 0\n\n/* warning: don't use these macros with a function, as it evals its arg twice */\n#define ITEM_get_cas(i) (((i)->it_flags & ITEM_CAS) ? \\\n        (i)->data->cas : (uint64_t)0)\n\n#define ITEM_set_cas(i,v) { \\\n    if ((i)->it_flags & ITEM_CAS) { \\\n        (i)->data->cas = v; \\\n    } \\\n}\n\n#define ITEM_key(item) (((char*)&((item)->data)) \\\n         + (((item)->it_flags & ITEM_CAS) ? sizeof(uint64_t) : 0))\n\n#define ITEM_suffix(item) ((char*) &((item)->data) + (item)->nkey + 1 \\\n         + (((item)->it_flags & ITEM_CAS) ? sizeof(uint64_t) : 0))\n\n#define ITEM_data(item) ((char*) &((item)->data) + (item)->nkey + 1 \\\n         + (((item)->it_flags & ITEM_CFLAGS) ? sizeof(client_flags_t) : 0) \\\n         + (((item)->it_flags & ITEM_CAS) ? sizeof(uint64_t) : 0))\n\n#define ITEM_ntotal(item) (sizeof(struct _stritem) + (item)->nkey + 1 \\\n         + (item)->nbytes \\\n         + (((item)->it_flags & ITEM_CFLAGS) ? sizeof(client_flags_t) : 0) \\\n         + (((item)->it_flags & ITEM_CAS) ? sizeof(uint64_t) : 0))\n\n#define ITEM_clsid(item) ((item)->slabs_clsid & ~(3<<6))\n#define ITEM_lruid(item) ((item)->slabs_clsid & (3<<6))\n\n#define STAT_KEY_LEN 128\n#define STAT_VAL_LEN 128\n\n/** Append a simple stat with a stat name, value format and value */\n#define APPEND_STAT(name, fmt, val) \\\n    append_stat(name, add_stats, c, fmt, val);\n\n/** Append an indexed stat with a stat name (with format), value format\n    and value */\n#define APPEND_NUM_FMT_STAT(name_fmt, num, name, fmt, val)          \\\n    klen = snprintf(key_str, STAT_KEY_LEN, name_fmt, num, name);    \\\n    vlen = snprintf(val_str, STAT_VAL_LEN, fmt, val);               \\\n    add_stats(key_str, klen, val_str, vlen, c);\n\n/** Common APPEND_NUM_FMT_STAT format. */\n#define APPEND_NUM_STAT(num, name, fmt, val) \\\n    APPEND_NUM_FMT_STAT(\"%d:%s\", num, name, fmt, val)\n\n/** Item client flag conversion */\n#define FLAGS_CONV(it, flag) { \\\n    if ((it)->it_flags & ITEM_CFLAGS) { \\\n        flag = *((client_flags_t *)ITEM_suffix((it))); \\\n    } else { \\\n        flag = 0; \\\n    } \\\n}\n\n#define FLAGS_SIZE(item) (((item)->it_flags & ITEM_CFLAGS) ? sizeof(client_flags_t) : 0)\n\n/**\n * Callback for any function producing stats.\n *\n * @param key the stat's key\n * @param klen length of the key\n * @param val the stat's value in an ascii form (e.g. text form of a number)\n * @param vlen length of the value\n * @parm cookie magic callback cookie\n */\ntypedef void (*ADD_STAT)(const char *key, const uint16_t klen,\n                         const char *val, const uint32_t vlen,\n                         const void *cookie);\n\n/*\n * NOTE: If you modify this table you _MUST_ update the function state_text\n */\n/**\n * Possible states of a connection.\n */\nenum conn_states {\n    conn_listening,  /**< the socket which listens for connections */\n    conn_new_cmd,    /**< Prepare connection for next command */\n    conn_waiting,    /**< waiting for a readable socket */\n    conn_read,       /**< reading in a command line */\n    conn_parse_cmd,  /**< try to parse a command from the input buffer */\n    conn_write,      /**< writing out a simple response */\n    conn_nread,      /**< reading in a fixed number of bytes */\n    conn_swallow,    /**< swallowing unnecessary bytes w/o storing */\n    conn_closing,    /**< closing this connection */\n    conn_mwrite,     /**< writing out many items sequentially */\n    conn_closed,     /**< connection is closed */\n    conn_watch,      /**< held by the logger thread as a watcher */\n    conn_io_queue,   /**< wait on async. process to get response object */\n    conn_io_resume,  /**< ready to resume mwrite after async work */\n    conn_io_pending, /**< got woken up while waiting for async work */\n    conn_max_state   /**< Max state value (used for assertion) */\n};\n\nenum bin_substates {\n    bin_no_state,\n    bin_reading_set_header,\n    bin_reading_cas_header,\n    bin_read_set_value,\n    bin_reading_get_key,\n    bin_reading_stat,\n    bin_reading_del_header,\n    bin_reading_incr_header,\n    bin_read_flush_exptime,\n    bin_reading_sasl_auth,\n    bin_reading_sasl_auth_data,\n    bin_reading_touch_key,\n};\n\nenum protocol {\n    ascii_prot = 3, /* arbitrary value. */\n    binary_prot,\n    negotiating_prot, /* Discovering the protocol */\n#ifdef PROXY\n    proxy_prot,\n#endif\n};\n\nenum network_transport {\n    local_transport, /* Unix sockets*/\n    tcp_transport,\n    udp_transport\n};\n\nenum pause_thread_types {\n    PAUSE_WORKER_THREADS = 0,\n    PAUSE_ALL_THREADS,\n    RESUME_ALL_THREADS,\n    RESUME_WORKER_THREADS\n};\n\nenum stop_reasons {\n    NOT_STOP,\n    GRACE_STOP,\n    EXIT_NORMALLY\n};\n\nenum close_reasons {\n    ERROR_CLOSE,\n    NORMAL_CLOSE,\n    IDLE_TIMEOUT_CLOSE,\n    SHUTDOWN_CLOSE,\n};\n\n#define IS_TCP(x) (x == tcp_transport)\n#define IS_UDP(x) (x == udp_transport)\n\n#define NREAD_ADD 1\n#define NREAD_SET 2\n#define NREAD_REPLACE 3\n#define NREAD_APPEND 4\n#define NREAD_PREPEND 5\n#define NREAD_CAS 6\n#define NREAD_APPENDVIV 7 // specific to meta\n#define NREAD_PREPENDVIV 8 // specific to meta\n\n#define CAS_ALLOW_STALE true\n#define CAS_NO_STALE false\n\n#define LOG_TYPE_DELETE 1\n#define LOG_TYPE_META_DELETE 2\n\nenum store_item_type {\n    NOT_STORED=0, STORED, EXISTS, NOT_FOUND, TOO_LARGE, NO_MEMORY\n};\n\nenum delta_result_type {\n    OK, NON_NUMERIC, EOM, DELTA_ITEM_NOT_FOUND, DELTA_ITEM_CAS_MISMATCH\n};\n\n/** Time relative to server start. Smaller than time_t on 64-bit systems. */\n// TODO: Move to sub-header. needed in logger.h\n//typedef unsigned int rel_time_t;\n\n/** Use X macros to avoid iterating over the stats fields during reset and\n * aggregation. No longer have to add new stats in 3+ places.\n */\n\n#define SLAB_STATS_FIELDS \\\n    X(set_cmds) \\\n    X(get_hits) \\\n    X(touch_hits) \\\n    X(delete_hits) \\\n    X(cas_hits) \\\n    X(cas_badval) \\\n    X(incr_hits) \\\n    X(decr_hits)\n\n/** Stats stored per slab (and per thread). */\nstruct slab_stats {\n#define X(name) uint64_t    name;\n    SLAB_STATS_FIELDS\n#undef X\n};\n\n#define THREAD_STATS_FIELDS \\\n    X(get_cmds) \\\n    X(get_misses) \\\n    X(get_expired) \\\n    X(get_flushed) \\\n    X(touch_cmds) \\\n    X(touch_misses) \\\n    X(delete_misses) \\\n    X(incr_misses) \\\n    X(decr_misses) \\\n    X(cas_misses) \\\n    X(meta_cmds) \\\n    X(bytes_read) \\\n    X(bytes_written) \\\n    X(flush_cmds) \\\n    X(conn_yields) /* # of yields for connections (-R option)*/ \\\n    X(auth_cmds) \\\n    X(auth_errors) \\\n    X(idle_kicks) /* idle connections killed */ \\\n    X(response_obj_oom) \\\n    X(response_obj_count) \\\n    X(response_obj_bytes) \\\n    X(read_buf_oom) \\\n    X(store_too_large) \\\n    X(store_no_memory)\n\n#ifdef EXTSTORE\n#define EXTSTORE_THREAD_STATS_FIELDS \\\n    X(get_extstore) \\\n    X(get_aborted_extstore) \\\n    X(get_oom_extstore) \\\n    X(recache_from_extstore) \\\n    X(miss_from_extstore) \\\n    X(badcrc_from_extstore)\n#endif\n\n#ifdef PROXY\n#define PROXY_THREAD_STATS_FIELDS \\\n    X(proxy_conn_requests) \\\n    X(proxy_conn_errors) \\\n    X(proxy_conn_oom) \\\n    X(proxy_req_active)\n#endif\n\n/**\n * Stats stored per-thread.\n */\nstruct thread_stats {\n    pthread_mutex_t   mutex;\n#define X(name) uint64_t    name;\n    THREAD_STATS_FIELDS\n#ifdef EXTSTORE\n    EXTSTORE_THREAD_STATS_FIELDS\n#endif\n#ifdef PROXY\n    PROXY_THREAD_STATS_FIELDS\n#endif\n#undef X\n    struct slab_stats slab_stats[MAX_NUMBER_OF_SLAB_CLASSES];\n    uint64_t lru_hits[POWER_LARGEST];\n    uint64_t read_buf_count;\n    uint64_t read_buf_bytes;\n    uint64_t read_buf_bytes_free;\n};\n\n/**\n * Global stats. Only resettable stats should go into this structure.\n */\nstruct stats {\n    uint64_t      total_items;\n    uint64_t      total_conns;\n    uint64_t      rejected_conns;\n    uint64_t      malloc_fails;\n    uint64_t      listen_disabled_num;\n    uint64_t      slabs_moved;       /* times slabs were moved around */\n    uint64_t      slab_reassign_rescues; /* items rescued during slab move */\n    uint64_t      slab_reassign_inline_reclaim; /* valid items lost during slab move */\n    uint64_t      slab_reassign_chunk_rescues; /* chunked-item chunks recovered */\n    uint64_t      slab_reassign_busy_items; /* valid temporarily unmovable */\n    uint64_t      slab_reassign_busy_deletes; /* refcounted items killed */\n    uint64_t      slab_reassign_busy_nomem; /* valid items lost during slab move */\n    uint64_t      lru_crawler_starts; /* Number of item crawlers kicked off */\n    uint64_t      lru_maintainer_juggles; /* number of LRU bg pokes */\n    uint64_t      time_in_listen_disabled_us;  /* elapsed time in microseconds while server unable to process new connections */\n    uint64_t      log_worker_dropped; /* logs dropped by worker threads */\n    uint64_t      log_worker_written; /* logs written by worker threads */\n    uint64_t      log_watcher_skipped; /* logs watchers missed */\n    uint64_t      log_watcher_sent; /* logs sent to watcher buffers */\n#ifdef EXTSTORE\n    uint64_t      extstore_compact_lost; /* items lost because they were locked */\n    uint64_t      extstore_compact_rescues; /* items re-written during compaction */\n    uint64_t      extstore_compact_skipped; /* unhit items skipped during compaction */\n    uint64_t      extstore_compact_resc_cold; /* items re-written during compaction */\n    uint64_t      extstore_compact_resc_old; /* items re-written during compaction */\n#endif\n#ifdef TLS\n    uint64_t      ssl_proto_errors; /* TLS failures during SSL_read() and SSL_write() calls */\n    uint64_t      ssl_handshake_errors; /* TLS failures at accept/handshake time */\n    uint64_t      ssl_new_sessions; /* successfully negotiated new (non-reused) TLS sessions */\n#endif\n    struct timeval maxconns_entered;  /* last time maxconns entered */\n    uint64_t      unexpected_napi_ids;  /* see doc/napi_ids.txt */\n    uint64_t      round_robin_fallback; /* see doc/napi_ids.txt */\n};\n\n/**\n * Global \"state\" stats. Reflects state that shouldn't be wiped ever.\n * Ordered for some cache line locality for commonly updated counters.\n */\nstruct stats_state {\n    uint64_t      curr_items;\n    uint64_t      curr_bytes;\n    uint64_t      curr_conns;\n    uint64_t      hash_bytes;       /* size used for hash tables */\n    float         extstore_memory_pressure; /* when extstore might memory evict */\n    unsigned int  conn_structs;\n    unsigned int  reserved_fds;\n    unsigned int  hash_power_level; /* Better hope it's not over 9000 */\n    unsigned int  log_watchers; /* number of currently active watchers */\n    bool          hash_is_expanding; /* If the hash table is being expanded */\n    bool          accepting_conns;  /* whether we are currently accepting */\n    bool          slab_reassign_running; /* slab reassign in progress */\n    bool          lru_crawler_running; /* crawl in progress */\n};\n\n#define MAX_VERBOSITY_LEVEL 2\n\n/* When adding a setting, be sure to update process_stat_settings */\n/**\n * Globally accessible settings as derived from the commandline.\n */\nstruct settings {\n    size_t maxbytes;\n    int maxconns;\n    int port;\n    int udpport;\n    char *inter;\n    int verbose;\n    rel_time_t oldest_live; /* ignore existing items older than this */\n    int evict_to_free;\n    char *socketpath;   /* path to unix socket if using local socket */\n    char *auth_file;    /* path to user authentication file */\n    int access;  /* access mask (a la chmod) for unix domain socket */\n    double factor;          /* chunk size growth factor */\n    int chunk_size;\n    int num_threads;        /* number of worker (without dispatcher) libevent threads to run */\n    int num_threads_per_udp; /* number of worker threads serving each udp socket */\n    char prefix_delimiter;  /* character that marks a key prefix (for stats) */\n    int detail_enabled;     /* nonzero if we're collecting detailed stats */\n    int reqs_per_event;     /* Maximum number of io to process on each\n                               io-event. */\n    bool use_cas;\n    enum protocol binding_protocol;\n    int backlog;\n    int item_size_max;        /* Maximum item size */\n    int slab_chunk_size_max;  /* Upper end for chunks within slab pages. */\n    int slab_page_size;     /* Slab's page units. */\n    volatile sig_atomic_t sig_hup;  /* a HUP signal was received but not yet handled */\n    bool sasl;              /* SASL on/off */\n    bool maxconns_fast;     /* Whether or not to early close connections */\n    bool lru_crawler;        /* Whether or not to enable the autocrawler thread */\n    bool lru_maintainer_thread; /* LRU maintainer background thread */\n    bool lru_segmented;     /* Use split or flat LRU's */\n    bool slab_reassign;     /* Whether or not slab reassignment is allowed */\n    bool ssl_enabled; /* indicates whether SSL is enabled */\n    int slab_automove;     /* Whether or not to automatically move slabs */\n    unsigned int slab_automove_version; /* bump if AM config args change */\n    double slab_automove_ratio; /* youngest must be within pct of oldest */\n    double slab_automove_freeratio; /* % of memory to hold free as buffer */\n    unsigned int slab_automove_window; /* window mover for algorithm */\n    int hashpower_init;     /* Starting hash power level */\n    bool shutdown_command; /* allow shutdown command */\n    int tail_repair_time;   /* LRU tail refcount leak repair time */\n    bool flush_enabled;     /* flush_all enabled */\n    bool dump_enabled;      /* whether cachedump/metadump commands work */\n    char *hash_algorithm;     /* Hash algorithm in use */\n    int lru_crawler_sleep;  /* Microsecond sleep between items */\n    uint32_t lru_crawler_tocrawl; /* Number of items to crawl per run */\n    int hot_lru_pct; /* percentage of slab space for HOT_LRU */\n    int warm_lru_pct; /* percentage of slab space for WARM_LRU */\n    double hot_max_factor; /* HOT tail age relative to COLD tail */\n    double warm_max_factor; /* WARM tail age relative to COLD tail */\n    int crawls_persleep; /* Number of LRU crawls to run before sleeping */\n    bool temp_lru; /* TTL < temporary_ttl uses TEMP_LRU */\n    uint32_t temporary_ttl; /* temporary LRU threshold */\n    int idle_timeout;       /* Number of seconds to let connections idle */\n    unsigned int logger_watcher_buf_size; /* size of logger's per-watcher buffer */\n    unsigned int logger_buf_size; /* size of per-thread logger buffer */\n    unsigned int read_buf_mem_limit; /* total megabytes allowable for net buffers */\n    bool drop_privileges;   /* Whether or not to drop unnecessary process privileges */\n    bool watch_enabled; /* allows watch commands to be dropped */\n    bool relaxed_privileges;   /* Relax process restrictions when running testapp */\n    struct slab_rebal_thread *slab_rebal; /* struct for page mover thread */\n#ifdef EXTSTORE\n    unsigned int ext_io_threadcount; /* number of IO threads to run. */\n    unsigned int ext_page_size; /* size in megabytes of storage pages. */\n    unsigned int ext_item_size; /* minimum size of items to store externally */\n    unsigned int ext_item_age; /* max age of tail item before storing ext. */\n    unsigned int ext_low_ttl; /* remaining TTL below this uses own pages */\n    unsigned int ext_recache_rate; /* counter++ % recache_rate == 0 > recache */\n    unsigned int ext_wbuf_size; /* read only note for the engine */\n    unsigned int ext_compact_under; /* when fewer than this many pages, compact */\n    unsigned int ext_drop_under; /* when fewer than this many pages, drop COLD items */\n    unsigned int ext_max_sleep; /* maximum sleep time for extstore bg threads, in us */\n    double ext_max_frag; /* ideal maximum page fragmentation */\n    bool ext_drop_unread; /* skip unread items during compaction */\n    /* start flushing to extstore after memory below this */\n    unsigned int ext_global_pool_min;\n#endif\n#ifdef TLS\n    void *ssl_ctx; /* holds the SSL server context which has the server certificate */\n    char *ssl_chain_cert; /* path to the server SSL chain certificate */\n    char *ssl_key; /* path to the server key */\n    int ssl_verify_mode; /* client certificate verify mode */\n    int ssl_keyformat; /* key format , default is PEM */\n    char *ssl_ciphers; /* list of SSL ciphers */\n    char *ssl_ca_cert; /* certificate with CAs. */\n    rel_time_t ssl_last_cert_refresh_time; /* time of the last server certificate refresh */\n    unsigned int ssl_wbuf_size; /* size of the write buffer used by ssl_sendmsg method */\n    bool ssl_session_cache; /* enable SSL server session caching */\n    bool ssl_kernel_tls; /* enable server kTLS */\n    int ssl_min_version; /* minimum SSL protocol version to accept */\n#endif\n    int num_napi_ids;   /* maximum number of NAPI IDs */\n    char *memory_file;  /* warm restart memory file path */\n#ifdef PROXY\n    bool proxy_enabled;\n    bool proxy_uring; /* if the proxy should use io_uring */\n    bool proxy_memprofile; /* output detail of lua allocations */\n    char *proxy_startfile; /* lua file to run when workers start */\n    char *proxy_startarg; /* string argument to pass to proxy */\n    void *proxy_ctx; /* proxy's state context */\n#endif\n#ifdef SOCK_COOKIE_ID\n    uint32_t sock_cookie_id;\n#endif\n};\n\nextern struct stats stats;\nextern struct stats_state stats_state;\nextern time_t process_started;\nextern struct settings settings;\n\n#define ITEM_LINKED 1\n#define ITEM_CAS 2\n\n/* temp */\n#define ITEM_SLABBED 4\n\n/* Item was fetched at least once in its lifetime */\n#define ITEM_FETCHED 8\n/* Appended on fetch, removed on LRU shuffling */\n#define ITEM_ACTIVE 16\n/* If an item's storage are chained chunks. */\n#define ITEM_CHUNKED 32\n#define ITEM_CHUNK 64\n/* ITEM_data bulk is external to item */\n#define ITEM_HDR 128\n/* additional 4 bytes for item client flags */\n#define ITEM_CFLAGS 256\n/* item has sent out a token already */\n#define ITEM_TOKEN_SENT 512\n/* reserved, in case tokens should be a 2-bit count in future */\n#define ITEM_TOKEN_RESERVED 1024\n/* if item has been marked as a stale value */\n#define ITEM_STALE 2048\n/* if item key was sent in binary */\n#define ITEM_KEY_BINARY 4096\n\n/**\n * Structure for storing items within memcached.\n */\ntypedef struct _stritem {\n    /* Protected by LRU locks */\n    struct _stritem *next;\n    struct _stritem *prev;\n    /* Rest are protected by an item lock */\n    struct _stritem *h_next;    /* hash chain next */\n    rel_time_t      time;       /* least recent access */\n    rel_time_t      exptime;    /* expire time */\n    int             nbytes;     /* size of data */\n    unsigned short  refcount;\n    uint16_t        it_flags;   /* ITEM_* above */\n    uint8_t         slabs_clsid;/* which slab class we're in */\n    uint8_t         nkey;       /* key length, w/terminating null and padding */\n    /* this odd type prevents type-punning issues when we do\n     * the little shuffle to save space when not using CAS. */\n    union {\n        uint64_t cas;\n        char end;\n    } data[];\n    /* if it_flags & ITEM_CAS we have 8 bytes CAS */\n    /* then null-terminated key */\n    /* then \" flags length\\r\\n\" (no terminating null) */\n    /* then data with terminating \\r\\n (no terminating null; it's binary!) */\n} item;\n\n// TODO: If we eventually want user loaded modules, we can't use an enum :(\nenum crawler_run_type {\n    CRAWLER_AUTOEXPIRE=0, CRAWLER_EXPIRED, CRAWLER_METADUMP, CRAWLER_MGDUMP\n};\n\ntypedef struct {\n    struct _stritem *next;\n    struct _stritem *prev;\n    struct _stritem *h_next;    /* hash chain next */\n    rel_time_t      time;       /* least recent access */\n    rel_time_t      exptime;    /* expire time */\n    int             nbytes;     /* size of data */\n    unsigned short  refcount;\n    uint16_t        it_flags;   /* ITEM_* above */\n    uint8_t         slabs_clsid;/* which slab class we're in */\n    uint8_t         nkey;       /* key length, w/terminating null and padding */\n    uint32_t        remaining;  /* Max keys to crawl per slab per invocation */\n    uint64_t        reclaimed;  /* items reclaimed during this crawl. */\n    uint64_t        unfetched;  /* items reclaimed unfetched during this crawl. */\n    uint64_t        checked;    /* items examined during this crawl. */\n} crawler;\n\n/* Header when an item is actually a chunk of another item. */\ntypedef struct _strchunk {\n    struct _strchunk *next;     /* points within its own chain. */\n    struct _strchunk *prev;     /* can potentially point to the head. */\n    struct _stritem  *head;     /* always points to the owner chunk */\n    int              size;      /* available chunk space in bytes */\n    int              used;      /* chunk space used */\n    int              nbytes;    /* used. */\n    unsigned short   refcount;  /* used? */\n    uint16_t         it_flags;  /* ITEM_* above. */\n    uint8_t          slabs_clsid; /* Same as above. */\n    uint8_t          orig_clsid; /* For obj hdr chunks slabs_clsid is fake. */\n    char data[];\n} item_chunk;\n\n#ifdef NEED_ALIGN\nstatic inline char *ITEM_schunk(item *it) {\n    int offset = it->nkey + 1\n        + ((it->it_flags & ITEM_CFLAGS) ? sizeof(client_flags_t) : 0)\n        + ((it->it_flags & ITEM_CAS) ? sizeof(uint64_t) : 0);\n    int remain = offset % 8;\n    if (remain != 0) {\n        offset += 8 - remain;\n    }\n    return ((char *) &(it->data)) + offset;\n}\n#else\n#define ITEM_schunk(item) ((char*) &((item)->data) + (item)->nkey + 1 \\\n         + (((item)->it_flags & ITEM_CFLAGS) ? sizeof(client_flags_t) : 0) \\\n         + (((item)->it_flags & ITEM_CAS) ? sizeof(uint64_t) : 0))\n#endif\n\n#ifdef EXTSTORE\ntypedef struct {\n    unsigned int page_version; /* from IO header */\n    unsigned int offset; /* from IO header */\n    unsigned short page_id; /* from IO header */\n} item_hdr;\n#endif\n\n#define IO_QUEUE_COUNT 3\n\n#define IO_QUEUE_NONE 0\n#define IO_QUEUE_EXTSTORE 1\n#define IO_QUEUE_PROXY 2\n\ntypedef STAILQ_HEAD(iop_head_s, _io_pending_t) iop_head_t;\ntypedef struct _io_pending_t io_pending_t;\ntypedef struct io_queue_s io_queue_t;\ntypedef void (*io_queue_stack_cb)(io_queue_t *q);\ntypedef void (*io_queue_cb)(io_pending_t *pending);\n// This structure used to be passed between threads, but is now owned entirely\n// by the worker threads.\n// IO pending objects are created and stacked into this structure. They are\n// then sent off to remote threads.\n// The objects are returned one at a time to the worker threads, and this\n// structure is then consulted to see when to resume the worker.\nstruct io_queue_s {\n    void *ctx; // duplicated from io_queue_cb_t\n    void *stack_ctx; // module-specific context to be batch-submitted\n    int count; // ios to process before returning. only accessed by queue processor once submitted\n    int type; // duplicated from io_queue_cb_t\n};\n\ntypedef struct io_queue_cb_s {\n    void *ctx; // untouched ptr for specific context\n    io_queue_stack_cb submit_cb; // callback given a full stack of pending IO's at once.\n    int type;\n} io_queue_cb_t;\n\nstruct thread_notify {\n    struct event notify_event;  /* listen event for notify pipe or eventfd */\n#ifdef HAVE_EVENTFD\n    int notify_event_fd;        /* notify counter */\n#else\n    int notify_receive_fd;      /* receiving end of notify pipe */\n    int notify_send_fd;         /* sending end of notify pipe */\n#endif\n};\n\ntypedef struct _mc_resp_bundle mc_resp_bundle;\ntypedef struct {\n    pthread_t thread_id;        /* unique ID of this thread */\n    struct event_base *base;    /* libevent handle this thread uses */\n    struct thread_notify n;     /* for thread notification */\n    struct thread_notify ion;   /* for thread IO object notification */\n    pthread_mutex_t ion_lock;   /* mutex for ion_head */\n    iop_head_t ion_head;        /* queue for IO object return */\n    int cur_sfd;                /* client fd for logging commands */\n    int thread_baseid;          /* which \"number\" thread this is for data offsets */\n    struct thread_stats stats;  /* Stats generated by this thread */\n    io_queue_cb_t io_queues[IO_QUEUE_COUNT];\n    struct conn_queue *ev_queue; /* Worker/conn event queue */\n    cache_t *rbuf_cache;        /* static-sized read buffers */\n    mc_resp_bundle *open_bundle;\n    cache_t *io_cache;          /* IO objects */\n#ifdef EXTSTORE\n    void *storage;              /* data object for storage system */\n#endif\n    logger *l;                  /* logger buffer */\n    void *lru_bump_buf;         /* async LRU bump buffer */\n#ifdef TLS\n    char   *ssl_wbuf;\n#endif\n    int napi_id;                /* napi id associated with this thread */\n#ifdef PROXY\n    void *proxy_ctx; // proxy global context\n    void *L; // lua VM\n    void *proxy_hooks;\n    void *proxy_user_stats;\n    void *proxy_int_stats;\n    void *proxy_event_thread; // worker threads can also be proxy IO threads\n    struct event *proxy_gc_timer; // periodic GC pushing.\n    pthread_mutex_t proxy_limit_lock;\n    int proxy_vm_extra_kb;\n    int proxy_vm_last_kb;\n    unsigned int proxy_vm_negative_delta;\n    int proxy_vm_gcrunning;\n    int proxy_vm_gcpokemem;\n    uint64_t proxy_active_req_limit;\n    uint64_t proxy_buffer_memory_limit; // protected by limit_lock\n    uint64_t proxy_buffer_memory_used; // protected by limit_lock\n    uint32_t proxy_rng[4]; // fast per-thread rng for lua.\n    // TODO: add ctx object so we can attach to queue.\n#endif\n} LIBEVENT_THREAD;\n\n/**\n * Response objects\n */\n#define MC_RESP_IOVCOUNT 4\ntypedef struct _mc_resp {\n    mc_resp_bundle *bundle; // ptr back to bundle\n    struct _mc_resp *next; // choo choo.\n    int wbytes; // bytes to write out of wbuf: might be able to nuke this.\n    int tosend; // total bytes to send for this response\n    void *write_and_free; /** free this memory after finishing writing */\n    io_pending_t *io_pending; /* pending IO descriptor for this response */\n\n    item *item; /* item associated with this response object, with reference held */\n    struct iovec iov[MC_RESP_IOVCOUNT]; /* built-in iovecs to simplify network code */\n    int chunked_total; /* total amount of chunked item data to send. */\n    uint8_t iovcnt;\n    uint8_t chunked_data_iov; /* this iov is a pointer to chunked data header */\n\n    /* instruct transmit to skip this response object. used by storage engines\n     * to asynchronously kill an object that was queued to write\n     */\n    bool skip;\n    bool free; // double free detection.\n#ifdef PROXY\n    bool proxy_res; // we're handling a proxied response buffer.\n#endif\n    // UDP bits. Copied in from the client.\n    uint16_t    request_id; /* Incoming UDP request ID, if this is a UDP \"connection\" */\n    uint16_t    udp_sequence; /* packet counter when transmitting result */\n    uint16_t    udp_total; /* total number of packets in sequence */\n    struct sockaddr_in6 request_addr; /* udp: Who sent this request */\n    socklen_t request_addr_size;\n\n    char wbuf[WRITE_BUFFER_SIZE];\n} mc_resp;\n\n#define MAX_RESP_PER_BUNDLE ((READ_BUFFER_SIZE - sizeof(mc_resp_bundle)) / sizeof(mc_resp))\nstruct _mc_resp_bundle {\n    uint8_t refcount;\n    uint8_t next_check; // next object to check on assignment.\n    LIBEVENT_THREAD *thread;\n    struct _mc_resp_bundle *next;\n    struct _mc_resp_bundle *prev;\n    mc_resp r[];\n};\n\ntypedef struct conn conn;\n\nstruct _io_pending_t {\n    int io_queue_type; // matches one of IO_QUEUE_*\n    LIBEVENT_THREAD *thread;\n    conn *c;\n    mc_resp *resp; // associated response object\n    io_queue_cb return_cb; // called on worker thread.\n    io_queue_cb finalize_cb; // called back on the worker thread.\n    STAILQ_ENTRY(_io_pending_t) iop_next; // queue chain.\n    char data[120];\n};\n\n/**\n * The structure representing a connection into memcached.\n */\nstruct conn {\n    sasl_conn_t *sasl_conn;\n    int    sfd;\n    bool sasl_started;\n    bool authenticated;\n    bool set_stale;\n    bool mset_res; /** uses mset format for return code */\n    bool close_after_write; /** flush write then move to close connection */\n    bool rbuf_malloced; /** read buffer was malloc'ed for ascii mget, needs free() */\n    bool item_malloced; /** item for conn_nread state is a temporary malloc */\n    uint8_t ssl_enabled;\n    void    *ssl;\n#ifdef TLS\n    char   *ssl_wbuf;\n#endif\n    enum conn_states  state;\n    enum bin_substates substate;\n    rel_time_t last_cmd_time;\n    struct event event;\n    short  ev_flags;\n    short  which;   /** which events were just triggered */\n\n    char   *rbuf;   /** buffer to read commands into */\n    char   *rcurr;  /** but if we parsed some already, this is where we stopped */\n    int    rsize;   /** total allocated size of rbuf */\n    int    rbytes;  /** how much data, starting from rcur, do we have unparsed */\n\n    mc_resp *resp; // tail response.\n    mc_resp *resp_head; // first response in current stack.\n    char   *ritem;  /** when we read in an item's value, it goes here */\n    int    rlbytes;\n\n    /**\n     * item is used to hold an item structure created after reading the command\n     * line of set/add/replace commands, but before we finished reading the actual\n     * data. The data is read into ITEM_data(item) to avoid extra copying.\n     */\n\n    void   *item;     /* for commands set/add/replace  */\n\n    /* data for the swallow state */\n    int    sbytes;    /* how many bytes to swallow */\n\n    int io_queues_submitted; /* see notes on io_queue_t */\n    io_queue_t io_queues[IO_QUEUE_COUNT]; /* set of deferred IO queues. */\n#ifdef PROXY\n    void *proxy_rctx; /* pointer to active request context */\n#endif\n#ifdef EXTSTORE\n    unsigned int recache_counter;\n#endif\n    enum protocol protocol;   /* which protocol this connection speaks */\n    enum network_transport transport; /* what transport is used by this connection */\n    enum close_reasons close_reason; /* reason for transition into conn_closing */\n\n    /* data for UDP clients */\n    int    request_id; /* Incoming UDP request ID, if this is a UDP \"connection\" */\n    struct sockaddr_in6 request_addr; /* udp: Who sent the most recent request */\n    socklen_t request_addr_size;\n\n    bool   noreply;   /* True if the reply should not be sent. */\n    /* current stats command */\n    struct {\n        char *buffer;\n        size_t size;\n        size_t offset;\n    } stats;\n\n    /* Binary protocol stuff */\n    /* This is where the binary header goes */\n    protocol_binary_request_header binary_header;\n    uint64_t cas; /* the cas to return */\n    uint64_t tag; /* listener stocket tag */\n    short cmd; /* current command being processed */\n    int opaque;\n    int keylen;\n    conn   *next;     /* Used for generating a list of conn structures */\n    LIBEVENT_THREAD *thread; /* Pointer to the thread object serving this connection */\n    int (*try_read_command)(conn *c); /* pointer for top level input parser */\n    ssize_t (*read)(conn  *c, void *buf, size_t count);\n    ssize_t (*sendmsg)(conn *c, struct msghdr *msg, int flags);\n    ssize_t (*write)(conn *c, void *buf, size_t count);\n};\n\n/* array of conn structures, indexed by file descriptor */\nextern conn **conns;\n\n/* current time of day (updated periodically) */\nextern volatile rel_time_t current_time;\n\n#ifdef MEMCACHED_DEBUG\nextern volatile bool is_paused;\nextern volatile int64_t delta;\n#endif\n\n#ifdef EXTSTORE\nextern void *ext_storage;\n#endif\n/*\n * Functions\n */\nvoid verify_default(const char* param, bool condition);\nvoid do_accept_new_conns(const bool do_accept);\nenum delta_result_type do_add_delta(LIBEVENT_THREAD *t, const char *key,\n                                    const size_t nkey, const bool incr,\n                                    const int64_t delta, char *buf,\n                                    uint64_t *cas, const uint32_t hv,\n                                    item **it_ret);\nenum store_item_type do_store_item(item *item, int comm, LIBEVENT_THREAD *t, const uint32_t hv, int *nbytes, uint64_t *cas, const uint64_t cas_in, bool cas_stale);\nvoid thread_io_queue_add(LIBEVENT_THREAD *t, int type, void *ctx, io_queue_stack_cb cb);\nvoid conn_io_queue_setup(conn *c);\nio_queue_t *conn_io_queue_get(conn *c, int type);\nio_queue_cb_t *thread_io_queue_get(LIBEVENT_THREAD *t, int type);\nvoid conn_io_queue_return(io_pending_t *io);\nconn *conn_new(const int sfd, const enum conn_states init_state, const int event_flags, const int read_buffer_size,\n    enum network_transport transport, struct event_base *base, void *ssl, uint64_t conntag, enum protocol bproto);\n\nvoid conn_worker_readd(conn *c);\nextern int daemonize(int nochdir, int noclose);\n\n#define mutex_lock(x) pthread_mutex_lock(x)\n#define mutex_trylock(x) pthread_mutex_trylock(x)\n#define mutex_unlock(x) pthread_mutex_unlock(x)\n\n#include \"stats_prefix.h\"\n#include \"slabs.h\"\n#include \"assoc.h\"\n#include \"items.h\"\n#include \"crawler.h\"\n#include \"trace.h\"\n#include \"hash.h\"\n\n/*\n * Functions such as the libevent-related calls that need to do cross-thread\n * communication in multithreaded mode (rather than actually doing the work\n * in the current thread) are called via \"dispatch_\" frontends, which are\n * also #define-d to directly call the underlying code in singlethreaded mode.\n */\nvoid memcached_thread_init(int nthreads, void *arg);\nvoid redispatch_conn(conn *c);\nvoid timeout_conn(conn *c);\n#ifdef PROXY\nvoid proxy_reload_notify(LIBEVENT_THREAD *t);\n#endif\nvoid return_io_pending(io_pending_t *io);\nvoid dispatch_conn_new(int sfd, enum conn_states init_state, int event_flags, int read_buffer_size,\n    enum network_transport transport, void *ssl, uint64_t conntag, enum protocol bproto);\nvoid sidethread_conn_close(conn *c);\n\n/* Lock wrappers for cache functions that are called from main loop. */\nenum delta_result_type add_delta(LIBEVENT_THREAD *t, const char *key,\n                                 const size_t nkey, bool incr,\n                                 const int64_t delta, char *buf,\n                                 uint64_t *cas);\nvoid accept_new_conns(const bool do_accept);\nvoid  conn_close_idle(conn *c);\nvoid  conn_close_all(void);\nitem *item_alloc(const char *key, size_t nkey, client_flags_t flags, rel_time_t exptime, int nbytes);\n#define DO_UPDATE true\n#define DONT_UPDATE false\nitem *item_get(const char *key, const size_t nkey, LIBEVENT_THREAD *t, const bool do_update);\nitem *item_get_locked(const char *key, const size_t nkey, LIBEVENT_THREAD *t, const bool do_update, uint32_t *hv);\nitem *item_touch(const char *key, const size_t nkey, uint32_t exptime, LIBEVENT_THREAD *t);\nint   item_link(item *it);\nvoid  item_remove(item *it);\nint   item_replace(item *it, item *new_it, const uint32_t hv, const uint64_t cas_in);\nvoid  item_unlink(item *it);\n\nvoid item_lock(uint32_t hv);\nvoid *item_trylock(uint32_t hv);\nvoid item_trylock_unlock(void *arg);\nvoid item_unlock(uint32_t hv);\nvoid pause_threads(enum pause_thread_types type);\nvoid stop_threads(void);\nint stop_conn_timeout_thread(void);\n#define refcount_incr(it) ++(it->refcount)\n#define refcount_decr(it) --(it->refcount)\nvoid STATS_LOCK(void);\nvoid STATS_UNLOCK(void);\n#define THR_STATS_LOCK(t) pthread_mutex_lock(&t->stats.mutex)\n#define THR_STATS_UNLOCK(t) pthread_mutex_unlock(&t->stats.mutex)\nvoid threadlocal_stats_reset(void);\nvoid threadlocal_stats_aggregate(struct thread_stats *stats);\nvoid slab_stats_aggregate(struct thread_stats *stats, struct slab_stats *out);\nvoid thread_setname(pthread_t thread, const char *name);\nLIBEVENT_THREAD *get_worker_thread(int id);\n\n/* Stat processing functions */\nvoid append_stat(const char *name, ADD_STAT add_stats, conn *c,\n                 const char *fmt, ...);\n\nenum store_item_type store_item(item *item, int comm, LIBEVENT_THREAD *t, int *nbytes, uint64_t *cas, const uint64_t cas_in, bool cas_stale);\n\n/* Protocol related code */\nvoid out_string(conn *c, const char *str);\n#define REALTIME_MAXDELTA 60*60*24*30\n/* Negative exptimes can underflow and end up immortal. realtime() will\n   immediately expire values that are greater than REALTIME_MAXDELTA, but less\n   than process_started, so lets aim for that. */\n#define EXPTIME_TO_POSITIVE_TIME(exptime) (exptime < 0) ? \\\n        REALTIME_MAXDELTA + 1 : exptime\nrel_time_t realtime(const time_t exptime);\nitem* limited_get(const char *key, size_t nkey, LIBEVENT_THREAD *t, uint32_t exptime, bool should_touch, bool do_update, bool *overflow);\nitem* limited_get_locked(const char *key, size_t nkey, LIBEVENT_THREAD *t, bool do_update, uint32_t *hv, bool *overflow);\n// Read/Response object handlers.\nvoid resp_reset(mc_resp *resp);\nvoid resp_add_iov(mc_resp *resp, const void *buf, int len);\nvoid resp_add_chunked_iov(mc_resp *resp, const void *buf, int len);\nbool resp_start(conn *c);\nmc_resp *resp_start_unlinked(conn *c);\nmc_resp* resp_finish(conn *c, mc_resp *resp);\nvoid resp_free(LIBEVENT_THREAD *th, mc_resp *resp);\nbool resp_has_stack(conn *c);\nbool rbuf_switch_to_malloc(conn *c);\nvoid conn_release_items(conn *c);\nvoid conn_set_state(conn *c, enum conn_states state);\nvoid out_of_memory(conn *c, char *ascii_error);\nvoid out_errstring(conn *c, const char *str);\nvoid write_and_free(conn *c, char *buf, int bytes);\nvoid server_stats(ADD_STAT add_stats, void *c);\nvoid append_stats(const char *key, const uint16_t klen,\n                  const char *val, const uint32_t vlen,\n                  const void *cookie);\n/** Return a datum for stats in binary protocol */\nbool get_stats(const char *stat_type, int nkey, ADD_STAT add_stats, void *c);\nvoid stats_reset(void);\nvoid process_stat_settings(ADD_STAT add_stats, void *c);\nvoid process_stats_conns(ADD_STAT add_stats, void *c);\n\n#if HAVE_DROP_PRIVILEGES\nextern void setup_privilege_violations_handler(void);\nextern void drop_privileges(void);\n#else\n#define setup_privilege_violations_handler()\n#define drop_privileges()\n#endif\n\n#if HAVE_DROP_WORKER_PRIVILEGES\nextern void drop_worker_privileges(void);\n#else\n#define drop_worker_privileges()\n#endif\n\n/* If supported, give compiler hints for branch prediction. */\n#if !defined(__GNUC__) || (__GNUC__ == 2 && __GNUC_MINOR__ < 96)\n#define __builtin_expect(x, expected_value) (x)\n#endif\n\n#define likely(x)       __builtin_expect((x),1)\n#define unlikely(x)     __builtin_expect((x),0)\n"
        },
        {
          "name": "memcached.spec.in",
          "type": "blob",
          "size": 5.673828125,
          "content": "%bcond_with extstore\n%bcond_with seccomp\n%bcond_with sasl\n%bcond_with sasl_pwdb\n%bcond_with dtrace\n%bcond_with 64bit\n%bcond_without option_checking\n%bcond_without coverage\n%bcond_without docs\n\n# Set with_systemd on distros that use it, so we can install the service\n# file, otherwise the sysvinit script will be installed\n%if 0%{?fedora} >= 14 || 0%{?rhel} >= 7 || 0%{?suse_version} >= 1210\n%global with_systemd 1\nBuildRequires: systemd-units\n\n# Disable some systemd safety features on OSes without a new enough systemd\n# (new enough is systemd >= 233)\n%if 0%{?fedora} < 26 || 0%{?rhel} > 0\n%global safer_systemd 0\n%else\n%global safer_systemd 1\n%endif\n\n%else\n%global with_systemd 0\n%endif\n\nName:           memcached\nVersion:        @VERSION@\nRelease:        @RELEASE@%{?dist}\nSummary:        High Performance, Distributed Memory Object Cache\n\nGroup:          System Environment/Daemons\nLicense:        BSD\nURL:            https://memcached.org\nSource0:        https://memcached.org/files/%{name}-%{version}.tar.gz\nSource1:        memcached.sysconfig\nSource2:        memcached.service\nSource3:        memcached@.service\nBuildRoot:      %{_tmppath}/%{name}-%{version}-%{release}-root-%(%{__id_u} -n)\n\nBuildRequires:  libevent-devel\nBuildRequires:  perl(Test::More)\nBuildRequires:  /usr/bin/prove\nRequires: initscripts\n%if %{with_systemd}\nRequires(post):   systemd-units\nRequires(preun):  systemd-units\nRequires(postun): systemd-units\n%else\nRequires(post): /sbin/chkconfig\nRequires(preun): /sbin/chkconfig, /sbin/service\nRequires(postun): /sbin/service\n%endif\n\n%description\nmemcached is a high-performance, distributed memory object caching\nsystem, generic in nature, but intended for use in speeding up dynamic\nweb applications by alleviating database load.\n\n%prep\n%setup -q -n %{name}-%{version}\n\n\n%build\n%configure \\\n  %{?with_extstore:--enable-extstore} \\\n  %{?with_seccomp:--enable-seccomp} \\\n  %{?with_sasl:--enable-sasl} \\\n  %{?with_sasl_pwdb:--enable-pwdb} \\\n  %{?with_dtrace:--enable-dtrace} \\\n  %{?with_64bit:--enable-64bit} \\\n  %{!?with_option_checking:--disable-option-checking}\n  %{!?with_coverage:--disable-coverage} \\\n  %{!?with_docs:--disable-docs}\n\nmake %{?_smp_mflags}\n\n\n%check\nmake test\n\n\n%install\nrm -rf %{buildroot}\nmake install DESTDIR=%{buildroot}\n\n# remove memcached-debug\nrm -f %{buildroot}/%{_bindir}/%{name}-debug\n\n# Perl script for monitoring memcached\ninstall -Dp -m0755 scripts/memcached-tool %{buildroot}%{_bindir}/%{name}-tool\n\n# Init script\n%if %{with_systemd}\ninstall -Dp -m0755 scripts/memcached.service %{buildroot}%{_unitdir}/%{name}.service\ninstall -Dp -m0755 scripts/memcached@.service %{buildroot}%{_unitdir}/%{name}@.service\n\nif [ %{safer_systemd} -gt 0 ]; then\n    sed -e 's/^##safer##//g' -i %{buildroot}%{_unitdir}/%{name}.service %{buildroot}%{_unitdir}/%{name}@.service\nelse\n    sed -e 's/^##safer##/#/g' -i %{buildroot}%{_unitdir}/%{name}.service %{buildroot}%{_unitdir}/%{name}@.service\nfi\n%else\ninstall -Dp -m0755 scripts/memcached.sysv %{buildroot}%{_initrddir}/%{name}\n%endif\n\n# Default configs\ninstall -Dp -m0644 scripts/memcached.sysconfig %{buildroot}%{_sysconfdir}/sysconfig/%{name}\n\n# pid directory\nmkdir -p %{buildroot}/%{_localstatedir}/run/%{name}\n\n\n%clean\nrm -rf %{buildroot}\n\n\n%post\nif [ $1 -eq 1 ]; then\n    # Initial install\n%if %{with_systemd}\n    /bin/systemctl daemon-reload >/dev/null 2>&1 || :\n%else\n    /sbin/chkconfig --add %{name}\n%endif\nfi\n\n\n%preun\nif [ \"$1\" = 0 ] ; then\n    # Removal, not upgrade\n%if %{with_systemd}\n    /bin/systemctl --no-reload disable %{name}.service > /dev/null 2>&1 || :\n    /bin/systemctl --no-reload disable %{name}@\\*.service > /dev/null 2>&1 || :\n    /bin/systemctl stop %{name}.service > /dev/null 2>&1 || :\n    /bin/systemctl stop %{name}@\\*.service > /dev/null 2>&1 || :\n%else\n    /sbin/service %{name} stop > /dev/null 2>&1 || :\n    /sbin/chkconfig --del %{name}\n%endif\nfi\n\nexit 0\n\n\n%postun\n%if %{with_systemd}\n    /bin/systemctl daemon-reload >/dev/null 2>&1 || :\n%endif\n\n# Don't auto-restart memcached on upgrade -- let user control when cache flushes\n# if [ \"$1\" -ge 1 ]; then\n#    # upgrade, not install\n#    %if %{with_systemd}\n#        /bin/systemctl try-restart %{name}.service\n#        /bin/systemctl try-restart %{name}@\\*.service\n#    %else\n#        /sbin/service %named condrestart 2>/dev/null || :\n#    %endif\n#fi\n\nexit 0\n\n\n%files\n%defattr(-,root,root,-)\n%doc AUTHORS ChangeLog COPYING NEWS README.md doc/CONTRIBUTORS doc/*.txt\n%config(noreplace) %{_sysconfdir}/sysconfig/%{name}\n\n%dir %attr(750,nobody,nobody) %{_localstatedir}/run/%{name}\n%{_bindir}/%{name}-tool\n%{_bindir}/%{name}\n%{_mandir}/man1/%{name}.1*\n%{_includedir}/%{name}\n\n%if %{with_systemd}\n%{_unitdir}/%{name}.service\n%{_unitdir}/%{name}@.service\n%else\n%{_initrddir}/%{name}\n%endif\n\n%changelog\n* Wed Jul  5 2017 J. Grizzard <jg-github@lupine.org> - 1.4.39\n- Add systemd-aware build\n- Add both static and instanced versions of memcached unit files\n\n* Mon Nov  2 2009 Dormando <dormando@rydia.net> - 1.4.3-1\n- Fix autogen more.\n\n* Sat Aug 29 2009 Dustin Sallings <dustin@spy.net> - 1.4.1-1\n- Autogenerate the version number from tags.\n\n* Wed Jul  4 2007 Paul Lindner <lindner@inuus.com> - 1.2.2-5\n- Use /var/run/memcached/ directory to hold PID file\n\n* Sat May 12 2007 Paul Lindner <lindner@inuus.com> - 1.2.2-4\n- Remove tabs from spec file, rpmlint reports no more errors\n\n* Thu May 10 2007 Paul Lindner <lindner@inuus.com> - 1.2.2-3\n- Enable build-time regression tests\n- add dependency on initscripts\n- remove memcached-debug (not needed in dist)\n- above suggestions from Bernard Johnson\n\n* Mon May  7 2007 Paul Lindner <lindner@inuus.com> - 1.2.2-2\n- Tidiness improvements suggested by Ruben Kerkhof in bugzilla #238994\n\n* Fri May  4 2007 Paul Lindner <lindner@inuus.com> - 1.2.2-1\n- Initial spec file created via rpmdev-newspec\n"
        },
        {
          "name": "memcached_dtrace.d",
          "type": "blob",
          "size": 10.35546875,
          "content": "/*\n * Copyright (c) <2008>, Sun Microsystems, Inc.\n * All rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n *     * Redistributions of source code must retain the above copyright\n *       notice, this list of conditions and the following disclaimer.\n *     * Redistributions in binary form must reproduce the above copyright\n *       notice, this list of conditions and the following disclaimer in the\n *       documentation and/or other materials provided with the distribution.\n *     * Neither the name of the  nor the\n *       names of its contributors may be used to endorse or promote products\n *       derived from this software without specific prior written permission.\n *\n * THIS SOFTWARE IS PROVIDED BY SUN MICROSYSTEMS, INC. ``AS IS'' AND ANY\n * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n * DISCLAIMED. IN NO EVENT SHALL SUN MICROSYSTEMS, INC. BE LIABLE FOR ANY\n * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF\n * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n */\nprovider memcached {\n   /**\n    * Fired when a connection object is allocated from the connection pool.\n    * @param connid the connection id\n    */\n   probe conn__allocate(int connid);\n\n   /**\n    * Fired when a connection object is released back to the connection pool.\n    * @param connid the connection id\n    */\n   probe conn__release(int connid);\n\n   /**\n    * Fired when a new connection object is created (there are no more\n    * connection objects in the connection pool).\n    * @param ptr pointer to the connection object\n    */\n   probe conn__create(void *ptr);\n\n   /**\n    * Fired when a connection object is destroyed (\"released back to\n    * the memory subsystem\").\n    * @param ptr pointer to the connection object\n    */\n   probe conn__destroy(void *ptr);\n\n   /**\n    * Fired when a connection is dispatched from the \"main thread\" to a\n    * worker thread.\n    * @param connid the connection id\n    * @param threadid the thread id\n    */\n   probe conn__dispatch(int connid, int64_t threadid);\n\n   /**\n    * Allocate memory from the slab allocator.\n    * @param slabclass the allocation will be fulfilled in this class\n    * @param slabsize the size of each item in this class\n    * @param ptr pointer to allocated memory\n    */\n   probe slabs__allocate(int slabclass, int slabsize, void* ptr);\n\n   /**\n    * Failed to allocate memory (out of memory).\n    * @param slabclass the class that failed to fulfill the request\n    */\n   probe slabs__allocate__failed(int slabclass);\n\n   /**\n    * Fired when a slab class attempts to allocate more space.\n    * @param slabclass class that needs more memory\n    */\n   probe slabs__slabclass__allocate(int slabclass);\n\n   /**\n    * Failed to allocate memory (out of memory).\n    * @param slabclass the class that failed grab more memory\n    */\n   probe slabs__slabclass__allocate__failed(int slabclass);\n\n   /**\n    * Release memory.\n    * @param size the size of the memory\n    * @param slabclass the class the memory belongs to\n    * @param ptr pointer to the memory to release\n    */\n   probe slabs__free(int slabclass, void* ptr);\n\n   /**\n    * Fired when the when we have searched the hash table for a named key.\n    * These two elements provide an insight in how well the hash function\n    * functions. Long traversals are a sign of a less optimal function,\n    * wasting cpu capacity.\n    *\n    * @param key the key searched for\n    * @param keylen length of the key\n    * @param depth the depth in the list of hash table\n    */\n   probe assoc__find(const char *key, int keylen, int depth);\n\n   /**\n    * Fired when a new item has been inserted.\n    * @param key the key just inserted\n    * @param keylen length of the key\n    */\n   probe assoc__insert(const char *key, int keylen);\n\n   /**\n    * Fired when a new item has been removed.\n    * @param key the key just deleted\n    * @param keylen length of the key\n    */\n   probe assoc__delete(const char *key, int keylen);\n\n   /**\n    * Fired when an item is linked into the cache.\n    * @param key the items key\n    * @param keylen length of the key\n    * @param size the size of the data\n    */\n   probe item__link(const char *key, int keylen, int size);\n\n   /**\n    * Fired when an item is deleted.\n    * @param key the items key\n    * @param keylen length of the key\n    * @param size the size of the data\n    */\n   probe item__unlink(const char *key, int keylen, int size);\n\n   /**\n    * Fired when the refcount for an item is reduced.\n    * @param key the items key\n    * @param keylen length of the key\n    * @param size the size of the data\n    */\n   probe item__remove(const char *key, int keylen, int size);\n\n   /**\n    * Fired when the \"last refenced\" time is updated.\n    * @param key the items key\n    * @param keylen length of the key\n    * @param size the size of the data\n    */\n   probe item__update(const char *key, int keylen, int size);\n\n   /**\n    * Fired when an item is replaced with another item.\n    * @param oldkey the key of the item to replace\n    * @param oldkeylen the length of the old key\n    * @param oldsize the size of the old item\n    * @param newkey the key of the new item\n    * @param newkeylen the length of the new key\n    * @param newsize the size of the new item\n    */\n   probe item__replace(const char *oldkey, int oldkeylen, int oldsize,\n                       const char *newkey, int newkeylen, int newsize);\n\n   /**\n    * Fired when the processing of a command starts.\n    * @param connid the connection id\n    * @param request the incoming request\n    * @param size the size of the request\n    */\n   probe process__command__start(int connid, const void *request, int size);\n\n   /**\n    * Fired when the processing of a command is done.\n    * @param connid the connection id\n    * @param response the response to send back to the client\n    * @param size the size of the response\n    */\n   probe process__command__end(int connid, const void *response, int size);\n\n   /**\n    * Fired for a get-command\n    * @param connid connection id\n    * @param key requested key\n    * @param keylen length of the key\n    * @param size size of the key's data (or signed int -1 if not found)\n    * @param casid the casid for the item\n    */\n   probe command__get(int connid, const char *key, int keylen, int size, int64_t casid);\n\n   /**\n    * Fired for an add-command.\n    * @param connid connection id\n    * @param key requested key\n    * @param keylen length of the key\n    * @param size the new size of the key's data (or signed int -1 if\n    *             not found)\n    * @param casid the casid for the item\n    */\n   probe command__add(int connid, const char *key, int keylen, int size, int64_t casid);\n\n   /**\n    * Fired for a set-command.\n    * @param connid connection id\n    * @param key requested key\n    * @param keylen length of the key\n    * @param size the new size of the key's data (or signed int -1 if\n    *             not found)\n    * @param casid the casid for the item\n    */\n   probe command__set(int connid, const char *key, int keylen, int size, int64_t casid);\n\n   /**\n    * Fired for a replace-command.\n    * @param connid connection id\n    * @param key requested key\n    * @param keylen length of the key\n    * @param size the new size of the key's data (or signed int -1 if\n    *             not found)\n    * @param casid the casid for the item\n    */\n   probe command__replace(int connid, const char *key, int keylen, int size, int64_t casid);\n\n   /**\n    * Fired for a prepend-command.\n    * @param connid connection id\n    * @param key requested key\n    * @param keylen length of the key\n    * @param size the new size of the key's data (or signed int -1 if\n    *             not found)\n    * @param casid the casid for the item\n    */\n   probe command__prepend(int connid, const char *key, int keylen, int size, int64_t casid);\n\n   /**\n    * Fired for an append-command.\n    * @param connid connection id\n    * @param key requested key\n    * @param keylen length of the key\n    * @param size the new size of the key's data (or signed int -1 if\n    *             not found)\n    * @param casid the casid for the item\n    */\n   probe command__append(int connid, const char *key, int keylen, int size, int64_t casid);\n\n   /**\n    * Fired for an touch-command.\n    * @param connid connection id\n    * @param key requested key\n    * @param keylen length of the key\n    * @param size the new size of the key's data (or signed int -1 if\n    *             not found)\n    * @param casid the casid for the item\n    */\n   probe command__touch(int connid, const char *key, int keylen, int size, int64_t casid);\n\n   /**\n    * Fired for a cas-command.\n    * @param connid connection id\n    * @param key requested key\n    * @param keylen length of the key\n    * @param size size of the key's data (or signed int -1 if not found)\n    * @param casid the cas id requested\n    */\n   probe command__cas(int connid, const char *key, int keylen, int size, int64_t casid);\n\n   /**\n    * Fired for an incr command.\n    * @param connid connection id\n    * @param key the requested key\n    * @param keylen length of the key\n    * @param val the new value\n    */\n   probe command__incr(int connid, const char *key, int keylen, int64_t val);\n\n   /**\n    * Fired for a decr command.\n    * @param connid connection id\n    * @param key the requested key\n    * @param keylen length of the key\n    * @param val the new value\n    */\n   probe command__decr(int connid, const char *key, int keylen, int64_t val);\n\n   /**\n    * Fired for a delete command.\n    * @param connid connection id\n    * @param key the requested key\n    * @param keylen length of the key\n    */\n   probe command__delete(int connid, const char *key, int keylen);\n\n};\n\n#pragma D attributes Unstable/Unstable/Common provider memcached provider\n#pragma D attributes Private/Private/Common provider memcached module\n#pragma D attributes Private/Private/Common provider memcached function\n#pragma D attributes Unstable/Unstable/Common provider memcached name\n#pragma D attributes Unstable/Unstable/Common provider memcached args\n"
        },
        {
          "name": "murmur3_hash.c",
          "type": "blob",
          "size": 2.759765625,
          "content": "//-----------------------------------------------------------------------------\n// MurmurHash3 was written by Austin Appleby, and is placed in the public\n// domain. The author hereby disclaims copyright to this source code.\n\n// Note - The x86 and x64 versions do _not_ produce the same results, as the\n// algorithms are optimized for their respective platforms. You can still\n// compile and run any of them on any platform, but your performance with the\n// non-native version will be less than optimal.\n\n#include \"murmur3_hash.h\"\n\n//-----------------------------------------------------------------------------\n// Platform-specific functions and macros\n\n// Microsoft Visual Studio\n\n#if defined(_MSC_VER)\n\n#define FORCE_INLINE    __forceinline\n\n#include <stdlib.h>\n\n#define ROTL32(x,y)    _rotl(x,y)\n\n#define BIG_CONSTANT(x) (x)\n\n// Other compilers\n\n#else    // defined(_MSC_VER)\n\n#define    FORCE_INLINE inline __attribute__((always_inline))\n\nstatic inline uint32_t rotl32 ( uint32_t x, int8_t r )\n{\n  return (x << r) | (x >> (32 - r));\n}\n\n#define    ROTL32(x,y)    rotl32(x,y)\n\n#define BIG_CONSTANT(x) (x##LLU)\n\n#endif // !defined(_MSC_VER)\n\n//-----------------------------------------------------------------------------\n// Block read - if your platform needs to do endian-swapping or can only\n// handle aligned reads, do the conversion here\n\nstatic FORCE_INLINE uint32_t getblock32 ( const uint32_t * p, int i )\n{\n  return p[i];\n}\n\n//-----------------------------------------------------------------------------\n// Finalization mix - force all bits of a hash block to avalanche\n\nstatic FORCE_INLINE uint32_t fmix32 ( uint32_t h )\n{\n  h ^= h >> 16;\n  h *= 0x85ebca6b;\n  h ^= h >> 13;\n  h *= 0xc2b2ae35;\n  h ^= h >> 16;\n\n  return h;\n}\n\n//-----------------------------------------------------------------------------\n\n/* Definition modified slightly from the public domain interface (no seed +\n * return value */\nuint32_t MurmurHash3_x86_32 ( const void * key, size_t length)\n{\n  const uint8_t * data = (const uint8_t*)key;\n  const int nblocks = length / 4;\n\n  uint32_t h1 = 0;\n\n  uint32_t c1 = 0xcc9e2d51;\n  uint32_t c2 = 0x1b873593;\n\n  //----------\n  // body\n\n  const uint32_t * blocks = (const uint32_t *)(data + nblocks*4);\n\n  for(int i = -nblocks; i; i++)\n  {\n    uint32_t k1 = getblock32(blocks,i);\n\n    k1 *= c1;\n    k1 = ROTL32(k1,15);\n    k1 *= c2;\n\n    h1 ^= k1;\n    h1 = ROTL32(h1,13);\n    h1 = h1*5+0xe6546b64;\n  }\n\n  //----------\n  // tail\n\n  const uint8_t * tail = (const uint8_t*)(data + nblocks*4);\n\n  uint32_t k1 = 0;\n\n  switch(length & 3)\n  {\n  case 3: k1 ^= tail[2] << 16;\n  case 2: k1 ^= tail[1] << 8;\n  case 1: k1 ^= tail[0];\n          k1 *= c1; k1 = ROTL32(k1,15); k1 *= c2; h1 ^= k1;\n  };\n\n  //----------\n  // finalization\n\n  h1 ^= length;\n\n  h1 = fmix32(h1);\n\n  //*(uint32_t*)out = h1;\n  return h1;\n}\n\n"
        },
        {
          "name": "murmur3_hash.h",
          "type": "blob",
          "size": 0.6650390625,
          "content": "//-----------------------------------------------------------------------------\n// MurmurHash3 was written by Austin Appleby, and is placed in the public\n// domain. The author hereby disclaims copyright to this source code.\n\n#ifndef MURMURHASH3_H\n#define MURMURHASH3_H\n\n//-----------------------------------------------------------------------------\n// Platform-specific functions and macros\n#include <stdint.h>\n#include <stddef.h>\n\n//-----------------------------------------------------------------------------\n\nuint32_t MurmurHash3_x86_32(const void *key, size_t length);\n\n//-----------------------------------------------------------------------------\n\n#endif // MURMURHASH3_H\n"
        },
        {
          "name": "openbsd_priv.c",
          "type": "blob",
          "size": 0.8994140625,
          "content": "#include <errno.h>\n#include <stdlib.h>\n#include <stdio.h>\n#include <string.h>\n#include <unistd.h>\n#include \"memcached.h\"\n\n/*\n * this section of code will drop all (OpenBSD) privileges including\n * those normally granted to all userland process (basic privileges). The\n * effect of this is that after running this code, the process will not able\n * to fork(), exec(), etc.  See pledge(2) for more information.\n */\nvoid drop_privileges() {\n    extern char *__progname;\n\n    if (settings.socketpath != NULL) {\n       if (pledge(\"stdio unix\", NULL) == -1) {\n          fprintf(stderr, \"%s: pledge: %s\\n\", __progname, strerror(errno));\n          exit(EXIT_FAILURE);\n       }\n    } else {\n       if (pledge(\"stdio inet\", NULL) == -1) {\n          fprintf(stderr, \"%s: pledge: %s\\n\", __progname, strerror(errno));\n          exit(EXIT_FAILURE);\n       }\n     }\n}\n\nvoid setup_privilege_violations_handler(void) {\n   // not needed\n}\n"
        },
        {
          "name": "proto_bin.c",
          "type": "blob",
          "size": 42.83984375,
          "content": "/* -*- Mode: C; tab-width: 4; c-basic-offset: 4; indent-tabs-mode: nil -*- */\n/*\n * Functions for handling the binary protocol.\n * NOTE: The binary protocol is deprecated as of 1.6.0.\n */\n\n#include \"memcached.h\"\n#include \"proto_bin.h\"\n#include \"storage.h\"\n#include <string.h>\n#include <stdlib.h>\n\n/** binprot handlers **/\nstatic void process_bin_flush(conn *c, char *extbuf);\nstatic void process_bin_append_prepend(conn *c);\nstatic void process_bin_update(conn *c, char *extbuf);\nstatic void process_bin_get_or_touch(conn *c, char *extbuf);\nstatic void process_bin_delete(conn *c);\nstatic void complete_incr_bin(conn *c, char *extbuf);\nstatic void process_bin_stat(conn *c);\nstatic void process_bin_sasl_auth(conn *c);\nstatic void dispatch_bin_command(conn *c, char *extbuf);\nstatic void complete_update_bin(conn *c);\nstatic void process_bin_complete_sasl_auth(conn *c);\n\nstatic void write_bin_miss_response(conn *c, char *key, size_t nkey);\n\nvoid complete_nread_binary(conn *c) {\n    assert(c != NULL);\n    assert(c->cmd >= 0);\n\n    switch(c->substate) {\n    case bin_read_set_value:\n        complete_update_bin(c);\n        break;\n    case bin_reading_sasl_auth_data:\n        process_bin_complete_sasl_auth(c);\n        if (c->item) {\n            do_item_remove(c->item);\n            c->item = NULL;\n        }\n        break;\n    default:\n        fprintf(stderr, \"Not handling substate %d\\n\", c->substate);\n        assert(0);\n    }\n}\n\nint try_read_command_binary(conn *c) {\n    /* Do we have the complete packet header? */\n    if (c->rbytes < sizeof(c->binary_header)) {\n        /* need more data! */\n        return 0;\n    } else {\n        memcpy(&c->binary_header, c->rcurr, sizeof(c->binary_header));\n        protocol_binary_request_header* req;\n        req = &c->binary_header;\n\n        if (settings.verbose > 1) {\n            /* Dump the packet before we convert it to host order */\n            int ii;\n            fprintf(stderr, \"<%d Read binary protocol data:\", c->sfd);\n            for (ii = 0; ii < sizeof(req->bytes); ++ii) {\n                if (ii % 4 == 0) {\n                    fprintf(stderr, \"\\n<%d   \", c->sfd);\n                }\n                fprintf(stderr, \" 0x%02x\", req->bytes[ii]);\n            }\n            fprintf(stderr, \"\\n\");\n        }\n\n        c->binary_header = *req;\n        c->binary_header.request.keylen = ntohs(req->request.keylen);\n        c->binary_header.request.bodylen = ntohl(req->request.bodylen);\n        c->binary_header.request.cas = ntohll(req->request.cas);\n\n        if (c->binary_header.request.magic != PROTOCOL_BINARY_REQ) {\n            if (settings.verbose) {\n                fprintf(stderr, \"Invalid magic:  %x\\n\",\n                        c->binary_header.request.magic);\n            }\n            conn_set_state(c, conn_closing);\n            return -1;\n        }\n\n        uint8_t extlen = c->binary_header.request.extlen;\n        uint16_t keylen = c->binary_header.request.keylen;\n        if (c->rbytes < keylen + extlen + sizeof(c->binary_header)) {\n            // Still need more bytes. Let try_read_network() realign the\n            // read-buffer and fetch more data as necessary.\n            return 0;\n        }\n\n        if (!resp_start(c)) {\n            conn_set_state(c, conn_closing);\n            return -1;\n        }\n\n        c->cmd = c->binary_header.request.opcode;\n        c->keylen = c->binary_header.request.keylen;\n        c->opaque = c->binary_header.request.opaque;\n        /* clear the returned cas value */\n        c->cas = 0;\n\n        c->last_cmd_time = current_time;\n        // sigh. binprot has no \"largest possible extlen\" define, and I don't\n        // want to refactor a ton of code either. Header is only ever used out\n        // of c->binary_header, but the extlen stuff is used for the latter\n        // bytes. Just wastes 24 bytes on the stack this way.\n\n        // +4 need to be here because extbuf is used for protocol_binary_request_incr\n        // and its member message is alligned to 48 bytes intead of 44\n        char extbuf[sizeof(c->binary_header) + BIN_MAX_EXTLEN+4];\n        memcpy(extbuf + sizeof(c->binary_header), c->rcurr + sizeof(c->binary_header),\n                extlen > BIN_MAX_EXTLEN ? BIN_MAX_EXTLEN : extlen);\n        c->rbytes -= sizeof(c->binary_header) + extlen + keylen;\n        c->rcurr += sizeof(c->binary_header) + extlen + keylen;\n\n        dispatch_bin_command(c, extbuf);\n    }\n\n    return 1;\n}\n\n/**\n * get a pointer to the key in this request\n */\nstatic char* binary_get_key(conn *c) {\n    return c->rcurr - (c->binary_header.request.keylen);\n}\n\nstatic void add_bin_header(conn *c, uint16_t err, uint8_t hdr_len, uint16_t key_len, uint32_t body_len) {\n    protocol_binary_response_header* header;\n    mc_resp *resp = c->resp;\n\n    assert(c);\n\n    resp_reset(resp);\n\n    header = (protocol_binary_response_header *)resp->wbuf;\n\n    header->response.magic = (uint8_t)PROTOCOL_BINARY_RES;\n    header->response.opcode = c->binary_header.request.opcode;\n    header->response.keylen = (uint16_t)htons(key_len);\n\n    header->response.extlen = (uint8_t)hdr_len;\n    header->response.datatype = (uint8_t)PROTOCOL_BINARY_RAW_BYTES;\n    header->response.status = (uint16_t)htons(err);\n\n    header->response.bodylen = htonl(body_len);\n    header->response.opaque = c->opaque;\n    header->response.cas = htonll(c->cas);\n\n    if (settings.verbose > 1) {\n        int ii;\n        fprintf(stderr, \">%d Writing bin response:\", c->sfd);\n        for (ii = 0; ii < sizeof(header->bytes); ++ii) {\n            if (ii % 4 == 0) {\n                fprintf(stderr, \"\\n>%d  \", c->sfd);\n            }\n            fprintf(stderr, \" 0x%02x\", header->bytes[ii]);\n        }\n        fprintf(stderr, \"\\n\");\n    }\n\n    resp->wbytes = sizeof(header->response);\n    resp_add_iov(resp, resp->wbuf, resp->wbytes);\n}\n\n\n/**\n * Writes a binary error response. If errstr is supplied, it is used as the\n * error text; otherwise a generic description of the error status code is\n * included.\n */\nvoid write_bin_error(conn *c, protocol_binary_response_status err,\n                            const char *errstr, int swallow) {\n    size_t len;\n\n    if (!errstr) {\n        switch (err) {\n        case PROTOCOL_BINARY_RESPONSE_ENOMEM:\n            errstr = \"Out of memory\";\n            break;\n        case PROTOCOL_BINARY_RESPONSE_UNKNOWN_COMMAND:\n            errstr = \"Unknown command\";\n            break;\n        case PROTOCOL_BINARY_RESPONSE_KEY_ENOENT:\n            errstr = \"Not found\";\n            break;\n        case PROTOCOL_BINARY_RESPONSE_EINVAL:\n            errstr = \"Invalid arguments\";\n            break;\n        case PROTOCOL_BINARY_RESPONSE_KEY_EEXISTS:\n            errstr = \"Data exists for key.\";\n            break;\n        case PROTOCOL_BINARY_RESPONSE_E2BIG:\n            errstr = \"Too large.\";\n            break;\n        case PROTOCOL_BINARY_RESPONSE_DELTA_BADVAL:\n            errstr = \"Non-numeric server-side value for incr or decr\";\n            break;\n        case PROTOCOL_BINARY_RESPONSE_NOT_STORED:\n            errstr = \"Not stored.\";\n            break;\n        case PROTOCOL_BINARY_RESPONSE_AUTH_ERROR:\n            errstr = \"Auth failure.\";\n            break;\n        default:\n            assert(false);\n            errstr = \"UNHANDLED ERROR\";\n            fprintf(stderr, \">%d UNHANDLED ERROR: %d\\n\", c->sfd, err);\n        }\n    }\n\n    if (settings.verbose > 1) {\n        fprintf(stderr, \">%d Writing an error: %s\\n\", c->sfd, errstr);\n    }\n\n    len = strlen(errstr);\n    add_bin_header(c, err, 0, 0, len);\n    if (len > 0) {\n        resp_add_iov(c->resp, errstr, len);\n    }\n    if (swallow > 0) {\n        c->sbytes = swallow;\n        conn_set_state(c, conn_swallow);\n    } else {\n        conn_set_state(c, conn_mwrite);\n    }\n}\n\n/* Just write an error message and disconnect the client */\nstatic void handle_binary_protocol_error(conn *c) {\n    write_bin_error(c, PROTOCOL_BINARY_RESPONSE_EINVAL, NULL, 0);\n    if (settings.verbose) {\n        fprintf(stderr, \"Protocol error (opcode %02x), close connection %d\\n\",\n                c->binary_header.request.opcode, c->sfd);\n    }\n    c->close_after_write = true;\n}\n\n/* Form and send a response to a command over the binary protocol */\nstatic void write_bin_response(conn *c, void *d, int hlen, int keylen, int dlen) {\n    if (!c->noreply || c->cmd == PROTOCOL_BINARY_CMD_GET ||\n        c->cmd == PROTOCOL_BINARY_CMD_GETK) {\n        add_bin_header(c, 0, hlen, keylen, dlen);\n        mc_resp *resp = c->resp;\n        if (dlen > 0) {\n            resp_add_iov(resp, d, dlen);\n        }\n    }\n\n    conn_set_state(c, conn_new_cmd);\n}\n\nstatic void complete_incr_bin(conn *c, char *extbuf) {\n    item *it;\n    char *key;\n    size_t nkey;\n    /* Weird magic in add_delta forces me to pad here */\n    char tmpbuf[INCR_MAX_STORAGE_LEN];\n    uint64_t cas = 0;\n\n    assert(c != NULL);\n    protocol_binary_response_incr* rsp = (protocol_binary_response_incr*)c->resp->wbuf;\n    protocol_binary_request_incr* req = (void *)extbuf;\n\n    //assert(c->wsize >= sizeof(*rsp));\n\n    /* fix byteorder in the request */\n    req->message.body.delta = ntohll(req->message.body.delta);\n    req->message.body.initial = ntohll(req->message.body.initial);\n    req->message.body.expiration = ntohl(req->message.body.expiration);\n    key = binary_get_key(c);\n    nkey = c->binary_header.request.keylen;\n\n    if (settings.verbose > 1) {\n        int i;\n        fprintf(stderr, \"incr \");\n\n        for (i = 0; i < nkey; i++) {\n            fprintf(stderr, \"%c\", key[i]);\n        }\n        fprintf(stderr, \" %lld, %llu, %d\\n\",\n                (long long)req->message.body.delta,\n                (long long)req->message.body.initial,\n                req->message.body.expiration);\n    }\n\n    if (c->binary_header.request.cas != 0) {\n        cas = c->binary_header.request.cas;\n    }\n    switch(add_delta(c->thread, key, nkey, c->cmd == PROTOCOL_BINARY_CMD_INCREMENT,\n                     req->message.body.delta, tmpbuf,\n                     &cas)) {\n    case OK:\n        rsp->message.body.value = htonll(strtoull(tmpbuf, NULL, 10));\n        if (cas) {\n            c->cas = cas;\n        }\n        write_bin_response(c, &rsp->message.body, 0, 0,\n                           sizeof(rsp->message.body.value));\n        break;\n    case NON_NUMERIC:\n        write_bin_error(c, PROTOCOL_BINARY_RESPONSE_DELTA_BADVAL, NULL, 0);\n        break;\n    case EOM:\n        out_of_memory(c, \"SERVER_ERROR Out of memory incrementing value\");\n        break;\n    case DELTA_ITEM_NOT_FOUND:\n        if (req->message.body.expiration != 0xffffffff) {\n            /* Save some room for the response */\n            rsp->message.body.value = htonll(req->message.body.initial);\n\n            snprintf(tmpbuf, INCR_MAX_STORAGE_LEN, \"%llu\",\n                (unsigned long long)req->message.body.initial);\n            int res = strlen(tmpbuf);\n            it = item_alloc(key, nkey, 0, realtime(req->message.body.expiration),\n                            res + 2);\n\n            if (it != NULL) {\n                uint64_t cas = 0;\n                memcpy(ITEM_data(it), tmpbuf, res);\n                memcpy(ITEM_data(it) + res, \"\\r\\n\", 2);\n                c->thread->cur_sfd = c->sfd; // for store_item logging.\n\n                if (store_item(it, NREAD_ADD, c->thread, NULL, &cas, (settings.use_cas) ? get_cas_id() : 0, CAS_NO_STALE)) {\n                    c->cas = cas;\n                    write_bin_response(c, &rsp->message.body, 0, 0, sizeof(rsp->message.body.value));\n                } else {\n                    write_bin_error(c, PROTOCOL_BINARY_RESPONSE_NOT_STORED,\n                                    NULL, 0);\n                }\n                item_remove(it);         /* release our reference */\n            } else {\n                out_of_memory(c,\n                        \"SERVER_ERROR Out of memory allocating new item\");\n            }\n        } else {\n            pthread_mutex_lock(&c->thread->stats.mutex);\n            if (c->cmd == PROTOCOL_BINARY_CMD_INCREMENT) {\n                c->thread->stats.incr_misses++;\n            } else {\n                c->thread->stats.decr_misses++;\n            }\n            pthread_mutex_unlock(&c->thread->stats.mutex);\n\n            write_bin_error(c, PROTOCOL_BINARY_RESPONSE_KEY_ENOENT, NULL, 0);\n        }\n        break;\n    case DELTA_ITEM_CAS_MISMATCH:\n        write_bin_error(c, PROTOCOL_BINARY_RESPONSE_KEY_EEXISTS, NULL, 0);\n        break;\n    }\n}\n\nstatic void complete_update_bin(conn *c) {\n    protocol_binary_response_status eno = PROTOCOL_BINARY_RESPONSE_EINVAL;\n    enum store_item_type ret = NOT_STORED;\n    assert(c != NULL);\n\n    item *it = c->item;\n    pthread_mutex_lock(&c->thread->stats.mutex);\n    c->thread->stats.slab_stats[ITEM_clsid(it)].set_cmds++;\n    pthread_mutex_unlock(&c->thread->stats.mutex);\n\n    /* We don't actually receive the trailing two characters in the bin\n     * protocol, so we're going to just set them here */\n    if ((it->it_flags & ITEM_CHUNKED) == 0) {\n        *(ITEM_data(it) + it->nbytes - 2) = '\\r';\n        *(ITEM_data(it) + it->nbytes - 1) = '\\n';\n    } else {\n        assert(c->ritem);\n        item_chunk *ch = (item_chunk *) c->ritem;\n        if (ch->size == ch->used)\n            ch = ch->next;\n        assert(ch->size - ch->used >= 2);\n        ch->data[ch->used] = '\\r';\n        ch->data[ch->used + 1] = '\\n';\n        ch->used += 2;\n    }\n\n    uint64_t cas = 0;\n    c->thread->cur_sfd = c->sfd; // for store_item logging.\n    ret = store_item(it, c->cmd, c->thread, NULL, &cas, (settings.use_cas) ? get_cas_id() : 0, CAS_NO_STALE);\n    c->cas = cas;\n\n#ifdef ENABLE_DTRACE\n    switch (c->cmd) {\n    case NREAD_ADD:\n        MEMCACHED_COMMAND_ADD(c->sfd, ITEM_key(it), it->nkey,\n                              (ret == STORED) ? it->nbytes : -1, cas);\n        break;\n    case NREAD_REPLACE:\n        MEMCACHED_COMMAND_REPLACE(c->sfd, ITEM_key(it), it->nkey,\n                                  (ret == STORED) ? it->nbytes : -1, cas);\n        break;\n    case NREAD_APPEND:\n        MEMCACHED_COMMAND_APPEND(c->sfd, ITEM_key(it), it->nkey,\n                                 (ret == STORED) ? it->nbytes : -1, cas);\n        break;\n    case NREAD_PREPEND:\n        MEMCACHED_COMMAND_PREPEND(c->sfd, ITEM_key(it), it->nkey,\n                                 (ret == STORED) ? it->nbytes : -1, cas);\n        break;\n    case NREAD_SET:\n        MEMCACHED_COMMAND_SET(c->sfd, ITEM_key(it), it->nkey,\n                              (ret == STORED) ? it->nbytes : -1, cas);\n        break;\n    }\n#endif\n\n    switch (ret) {\n    case STORED:\n        /* Stored */\n        write_bin_response(c, NULL, 0, 0, 0);\n        break;\n    case EXISTS:\n        write_bin_error(c, PROTOCOL_BINARY_RESPONSE_KEY_EEXISTS, NULL, 0);\n        break;\n    case NOT_FOUND:\n        write_bin_error(c, PROTOCOL_BINARY_RESPONSE_KEY_ENOENT, NULL, 0);\n        break;\n    case NOT_STORED:\n    case TOO_LARGE:\n    case NO_MEMORY:\n        if (c->cmd == NREAD_ADD) {\n            eno = PROTOCOL_BINARY_RESPONSE_KEY_EEXISTS;\n        } else if(c->cmd == NREAD_REPLACE) {\n            eno = PROTOCOL_BINARY_RESPONSE_KEY_ENOENT;\n        } else {\n            eno = PROTOCOL_BINARY_RESPONSE_NOT_STORED;\n        }\n        write_bin_error(c, eno, NULL, 0);\n    }\n\n    item_remove(c->item);       /* release the c->item reference */\n    c->item = 0;\n}\n\nstatic void write_bin_miss_response(conn *c, char *key, size_t nkey) {\n    if (nkey) {\n        add_bin_header(c, PROTOCOL_BINARY_RESPONSE_KEY_ENOENT,\n                0, nkey, nkey);\n        char *ofs = c->resp->wbuf + sizeof(protocol_binary_response_header);\n        memcpy(ofs, key, nkey);\n        resp_add_iov(c->resp, ofs, nkey);\n        conn_set_state(c, conn_new_cmd);\n    } else {\n        write_bin_error(c, PROTOCOL_BINARY_RESPONSE_KEY_ENOENT,\n                        NULL, 0);\n    }\n}\n\nstatic void process_bin_get_or_touch(conn *c, char *extbuf) {\n    item *it;\n\n    protocol_binary_response_get* rsp = (protocol_binary_response_get*)c->resp->wbuf;\n    char* key = binary_get_key(c);\n    size_t nkey = c->binary_header.request.keylen;\n    int should_touch = (c->cmd == PROTOCOL_BINARY_CMD_TOUCH ||\n                        c->cmd == PROTOCOL_BINARY_CMD_GAT ||\n                        c->cmd == PROTOCOL_BINARY_CMD_GATK);\n    int should_return_key = (c->cmd == PROTOCOL_BINARY_CMD_GETK ||\n                             c->cmd == PROTOCOL_BINARY_CMD_GATK);\n    int should_return_value = (c->cmd != PROTOCOL_BINARY_CMD_TOUCH);\n    bool failed = false;\n\n    if (settings.verbose > 1) {\n        fprintf(stderr, \"<%d %s \", c->sfd, should_touch ? \"TOUCH\" : \"GET\");\n        if (fwrite(key, 1, nkey, stderr)) {}\n        fputc('\\n', stderr);\n    }\n\n    if (should_touch) {\n        protocol_binary_request_touch *t = (void *)extbuf;\n        time_t exptime = ntohl(t->message.body.expiration);\n\n        it = item_touch(key, nkey, realtime(exptime), c->thread);\n    } else {\n        it = item_get(key, nkey, c->thread, DO_UPDATE);\n    }\n\n    if (it) {\n        /* the length has two unnecessary bytes (\"\\r\\n\") */\n        uint16_t keylen = 0;\n        uint32_t bodylen = sizeof(rsp->message.body) + (it->nbytes - 2);\n\n        pthread_mutex_lock(&c->thread->stats.mutex);\n        if (should_touch) {\n            c->thread->stats.touch_cmds++;\n            c->thread->stats.slab_stats[ITEM_clsid(it)].touch_hits++;\n        } else {\n            c->thread->stats.get_cmds++;\n            c->thread->stats.lru_hits[it->slabs_clsid]++;\n        }\n        pthread_mutex_unlock(&c->thread->stats.mutex);\n\n        if (should_touch) {\n            MEMCACHED_COMMAND_TOUCH(c->sfd, ITEM_key(it), it->nkey,\n                                    it->nbytes, ITEM_get_cas(it));\n        } else {\n            MEMCACHED_COMMAND_GET(c->sfd, ITEM_key(it), it->nkey,\n                                  it->nbytes, ITEM_get_cas(it));\n        }\n\n        if (c->cmd == PROTOCOL_BINARY_CMD_TOUCH) {\n            bodylen -= it->nbytes - 2;\n        } else if (should_return_key) {\n            bodylen += nkey;\n            keylen = nkey;\n        }\n\n        add_bin_header(c, 0, sizeof(rsp->message.body), keylen, bodylen);\n        rsp->message.header.response.cas = htonll(ITEM_get_cas(it));\n\n        // add the flags\n        FLAGS_CONV(it, rsp->message.body.flags);\n        rsp->message.body.flags = htonl(rsp->message.body.flags);\n        resp_add_iov(c->resp, &rsp->message.body, sizeof(rsp->message.body));\n\n        if (should_return_key) {\n            resp_add_iov(c->resp, ITEM_key(it), nkey);\n        }\n\n        if (should_return_value) {\n            /* Add the data minus the CRLF */\n#ifdef EXTSTORE\n            if (it->it_flags & ITEM_HDR) {\n                if (storage_get_item(c, it, c->resp) != 0) {\n                    pthread_mutex_lock(&c->thread->stats.mutex);\n                    c->thread->stats.get_oom_extstore++;\n                    pthread_mutex_unlock(&c->thread->stats.mutex);\n\n                    failed = true;\n                }\n            } else if ((it->it_flags & ITEM_CHUNKED) == 0) {\n                resp_add_iov(c->resp, ITEM_data(it), it->nbytes - 2);\n            } else {\n                // Allow transmit handler to find the item and expand iov's\n                resp_add_chunked_iov(c->resp, it, it->nbytes - 2);\n            }\n#else\n            if ((it->it_flags & ITEM_CHUNKED) == 0) {\n                resp_add_iov(c->resp, ITEM_data(it), it->nbytes - 2);\n            } else {\n                resp_add_chunked_iov(c->resp, it, it->nbytes - 2);\n            }\n#endif\n        }\n\n        if (!failed) {\n            conn_set_state(c, conn_new_cmd);\n            /* Remember this command so we can garbage collect it later */\n#ifdef EXTSTORE\n            if ((it->it_flags & ITEM_HDR) != 0 && should_return_value) {\n                // Only have extstore clean if header and returning value.\n                c->resp->item = NULL;\n            } else {\n                c->resp->item = it;\n            }\n#else\n            c->resp->item = it;\n#endif\n        } else {\n            item_remove(it);\n        }\n    } else {\n        failed = true;\n    }\n\n    if (failed) {\n        pthread_mutex_lock(&c->thread->stats.mutex);\n        if (should_touch) {\n            c->thread->stats.touch_cmds++;\n            c->thread->stats.touch_misses++;\n        } else {\n            c->thread->stats.get_cmds++;\n            c->thread->stats.get_misses++;\n        }\n        pthread_mutex_unlock(&c->thread->stats.mutex);\n\n        if (should_touch) {\n            MEMCACHED_COMMAND_TOUCH(c->sfd, key, nkey, -1, 0);\n        } else {\n            MEMCACHED_COMMAND_GET(c->sfd, key, nkey, -1, 0);\n        }\n\n        if (c->noreply) {\n            conn_set_state(c, conn_new_cmd);\n        } else {\n            if (should_return_key) {\n                write_bin_miss_response(c, key, nkey);\n            } else {\n                write_bin_miss_response(c, NULL, 0);\n            }\n        }\n    }\n\n    if (settings.detail_enabled) {\n        stats_prefix_record_get(key, nkey, NULL != it);\n    }\n}\n\nstatic void process_bin_stat(conn *c) {\n    char *subcommand = binary_get_key(c);\n    size_t nkey = c->binary_header.request.keylen;\n\n    if (settings.verbose > 1) {\n        int ii;\n        fprintf(stderr, \"<%d STATS \", c->sfd);\n        for (ii = 0; ii < nkey; ++ii) {\n            fprintf(stderr, \"%c\", subcommand[ii]);\n        }\n        fprintf(stderr, \"\\n\");\n    }\n\n    if (nkey == 0) {\n        /* request all statistics */\n        server_stats(&append_stats, c);\n        (void)get_stats(NULL, 0, &append_stats, c);\n    } else if (strncmp(subcommand, \"reset\", 5) == 0) {\n        stats_reset();\n    } else if (strncmp(subcommand, \"settings\", 8) == 0) {\n        process_stat_settings(&append_stats, c);\n    } else if (strncmp(subcommand, \"detail\", 6) == 0) {\n        char *subcmd_pos = subcommand + 6;\n        if (strncmp(subcmd_pos, \" dump\", 5) == 0) {\n            int len;\n            char *dump_buf = stats_prefix_dump(&len);\n            if (dump_buf == NULL || len <= 0) {\n                out_of_memory(c, \"SERVER_ERROR Out of memory generating stats\");\n                if (dump_buf != NULL)\n                    free(dump_buf);\n                return;\n            } else {\n                append_stats(\"detailed\", strlen(\"detailed\"), dump_buf, len, c);\n                free(dump_buf);\n            }\n        } else if (strncmp(subcmd_pos, \" on\", 3) == 0) {\n            settings.detail_enabled = 1;\n        } else if (strncmp(subcmd_pos, \" off\", 4) == 0) {\n            settings.detail_enabled = 0;\n        } else {\n            write_bin_error(c, PROTOCOL_BINARY_RESPONSE_KEY_ENOENT, NULL, 0);\n            return;\n        }\n    } else {\n        if (get_stats(subcommand, nkey, &append_stats, c)) {\n            if (c->stats.buffer == NULL) {\n                out_of_memory(c, \"SERVER_ERROR Out of memory generating stats\");\n            } else {\n                write_and_free(c, c->stats.buffer, c->stats.offset);\n                c->stats.buffer = NULL;\n            }\n        } else {\n            write_bin_error(c, PROTOCOL_BINARY_RESPONSE_KEY_ENOENT, NULL, 0);\n        }\n\n        return;\n    }\n\n    /* Append termination package and start the transfer */\n    append_stats(NULL, 0, NULL, 0, c);\n    if (c->stats.buffer == NULL) {\n        out_of_memory(c, \"SERVER_ERROR Out of memory preparing to send stats\");\n    } else {\n        write_and_free(c, c->stats.buffer, c->stats.offset);\n        c->stats.buffer = NULL;\n    }\n}\n\nstatic void init_sasl_conn(conn *c) {\n    assert(c);\n    /* should something else be returned? */\n    if (!settings.sasl)\n        return;\n\n    c->authenticated = false;\n\n    if (!c->sasl_conn) {\n        int result=sasl_server_new(\"memcached\",\n                                   NULL,\n                                   my_sasl_hostname[0] ? my_sasl_hostname : NULL,\n                                   NULL, NULL,\n                                   NULL, 0, &c->sasl_conn);\n        if (result != SASL_OK) {\n            if (settings.verbose) {\n                fprintf(stderr, \"Failed to initialize SASL conn.\\n\");\n            }\n            c->sasl_conn = NULL;\n        }\n    }\n}\n\nstatic void bin_list_sasl_mechs(conn *c) {\n    // Guard against a disabled SASL.\n    if (!settings.sasl) {\n        write_bin_error(c, PROTOCOL_BINARY_RESPONSE_UNKNOWN_COMMAND, NULL,\n                        c->binary_header.request.bodylen\n                        - c->binary_header.request.keylen);\n        return;\n    }\n\n    init_sasl_conn(c);\n    const char *result_string = NULL;\n    unsigned int string_length = 0;\n    int result=sasl_listmech(c->sasl_conn, NULL,\n                             \"\",   /* What to prepend the string with */\n                             \" \",  /* What to separate mechanisms with */\n                             \"\",   /* What to append to the string */\n                             &result_string, &string_length,\n                             NULL);\n    if (result != SASL_OK) {\n        /* Perhaps there's a better error for this... */\n        if (settings.verbose) {\n            fprintf(stderr, \"Failed to list SASL mechanisms.\\n\");\n        }\n        write_bin_error(c, PROTOCOL_BINARY_RESPONSE_AUTH_ERROR, NULL, 0);\n        return;\n    }\n    write_bin_response(c, (char*)result_string, 0, 0, string_length);\n}\n\nstatic void process_bin_sasl_auth(conn *c) {\n    // Guard for handling disabled SASL on the server.\n    if (!settings.sasl) {\n        write_bin_error(c, PROTOCOL_BINARY_RESPONSE_UNKNOWN_COMMAND, NULL,\n                        c->binary_header.request.bodylen\n                        - c->binary_header.request.keylen);\n        return;\n    }\n\n    assert(c->binary_header.request.extlen == 0);\n\n    uint16_t nkey = c->binary_header.request.keylen;\n    int vlen = c->binary_header.request.bodylen - nkey;\n\n    if (nkey > MAX_SASL_MECH_LEN) {\n        write_bin_error(c, PROTOCOL_BINARY_RESPONSE_EINVAL, NULL, vlen);\n        conn_set_state(c, conn_swallow);\n        return;\n    }\n\n    char *key = binary_get_key(c);\n    assert(key);\n\n    item *it = item_alloc(key, nkey, 0, 0, vlen+2);\n\n    /* Can't use a chunked item for SASL authentication. */\n    if (it == 0 || (it->it_flags & ITEM_CHUNKED)) {\n        write_bin_error(c, PROTOCOL_BINARY_RESPONSE_ENOMEM, NULL, vlen);\n        conn_set_state(c, conn_swallow);\n        if (it) {\n            do_item_remove(it);\n        }\n        return;\n    }\n\n    c->item = it;\n    c->ritem = ITEM_data(it);\n    c->rlbytes = vlen;\n    conn_set_state(c, conn_nread);\n    c->substate = bin_reading_sasl_auth_data;\n}\n\nstatic void process_bin_complete_sasl_auth(conn *c) {\n    assert(settings.sasl);\n    const char *out = NULL;\n    unsigned int outlen = 0;\n\n    assert(c->item);\n    init_sasl_conn(c);\n\n    uint16_t nkey = c->binary_header.request.keylen;\n    int vlen = c->binary_header.request.bodylen - nkey;\n\n    if (nkey > ((item*) c->item)->nkey) {\n        write_bin_error(c, PROTOCOL_BINARY_RESPONSE_EINVAL, NULL, vlen);\n        conn_set_state(c, conn_swallow);\n        return;\n    }\n\n    char mech[nkey+1];\n    memcpy(mech, ITEM_key((item*)c->item), nkey);\n    mech[nkey] = 0x00;\n\n    if (settings.verbose)\n        fprintf(stderr, \"mech:  ``%s'' with %d bytes of data\\n\", mech, vlen);\n\n    const char *challenge = vlen == 0 ? NULL : ITEM_data((item*) c->item);\n\n    if (vlen > ((item*) c->item)->nbytes) {\n        write_bin_error(c, PROTOCOL_BINARY_RESPONSE_EINVAL, NULL, vlen);\n        conn_set_state(c, conn_swallow);\n        return;\n    }\n\n    int result=-1;\n\n    switch (c->cmd) {\n    case PROTOCOL_BINARY_CMD_SASL_AUTH:\n        result = sasl_server_start(c->sasl_conn, mech,\n                                   challenge, vlen,\n                                   &out, &outlen);\n        c->sasl_started = (result == SASL_OK || result == SASL_CONTINUE);\n        break;\n    case PROTOCOL_BINARY_CMD_SASL_STEP:\n        if (!c->sasl_started) {\n            if (settings.verbose) {\n                fprintf(stderr, \"%d: SASL_STEP called but sasl_server_start \"\n                        \"not called for this connection!\\n\", c->sfd);\n            }\n            break;\n        }\n        result = sasl_server_step(c->sasl_conn,\n                                  challenge, vlen,\n                                  &out, &outlen);\n        break;\n    default:\n        assert(false); /* CMD should be one of the above */\n        /* This code is pretty much impossible, but makes the compiler\n           happier */\n        if (settings.verbose) {\n            fprintf(stderr, \"Unhandled command %d with challenge %s\\n\",\n                    c->cmd, challenge);\n        }\n        break;\n    }\n\n    if (settings.verbose) {\n        fprintf(stderr, \"sasl result code:  %d\\n\", result);\n    }\n\n    switch(result) {\n    case SASL_OK:\n        c->authenticated = true;\n        write_bin_response(c, \"Authenticated\", 0, 0, strlen(\"Authenticated\"));\n        pthread_mutex_lock(&c->thread->stats.mutex);\n        c->thread->stats.auth_cmds++;\n        pthread_mutex_unlock(&c->thread->stats.mutex);\n        break;\n    case SASL_CONTINUE:\n        add_bin_header(c, PROTOCOL_BINARY_RESPONSE_AUTH_CONTINUE, 0, 0, outlen);\n        if (outlen > 0) {\n            resp_add_iov(c->resp, out, outlen);\n        }\n        // Immediately flush our write.\n        conn_set_state(c, conn_mwrite);\n        break;\n    default:\n        if (settings.verbose)\n            fprintf(stderr, \"Unknown sasl response:  %d\\n\", result);\n        write_bin_error(c, PROTOCOL_BINARY_RESPONSE_AUTH_ERROR, NULL, 0);\n        pthread_mutex_lock(&c->thread->stats.mutex);\n        c->thread->stats.auth_cmds++;\n        c->thread->stats.auth_errors++;\n        pthread_mutex_unlock(&c->thread->stats.mutex);\n    }\n}\n\nstatic bool authenticated(conn *c) {\n    assert(settings.sasl);\n    bool rv = false;\n\n    switch (c->cmd) {\n    case PROTOCOL_BINARY_CMD_SASL_LIST_MECHS: /* FALLTHROUGH */\n    case PROTOCOL_BINARY_CMD_SASL_AUTH:       /* FALLTHROUGH */\n    case PROTOCOL_BINARY_CMD_SASL_STEP:       /* FALLTHROUGH */\n    case PROTOCOL_BINARY_CMD_VERSION:         /* FALLTHROUGH */\n        rv = true;\n        break;\n    default:\n        rv = c->authenticated;\n    }\n\n    if (settings.verbose > 1) {\n        fprintf(stderr, \"authenticated() in cmd 0x%02x is %s\\n\",\n                c->cmd, rv ? \"true\" : \"false\");\n    }\n\n    return rv;\n}\n\nstatic void dispatch_bin_command(conn *c, char *extbuf) {\n    int protocol_error = 0;\n\n    uint8_t extlen = c->binary_header.request.extlen;\n    uint16_t keylen = c->binary_header.request.keylen;\n    uint32_t bodylen = c->binary_header.request.bodylen;\n    c->thread->cur_sfd = c->sfd; // cuddle sfd for logging.\n\n    if (keylen > bodylen || keylen + extlen > bodylen) {\n        write_bin_error(c, PROTOCOL_BINARY_RESPONSE_UNKNOWN_COMMAND, NULL, 0);\n        c->close_after_write = true;\n        return;\n    }\n\n    if (settings.sasl && !authenticated(c)) {\n        write_bin_error(c, PROTOCOL_BINARY_RESPONSE_AUTH_ERROR, NULL, 0);\n        c->close_after_write = true;\n        return;\n    }\n\n    MEMCACHED_PROCESS_COMMAND_START(c->sfd, c->rcurr, c->rbytes);\n    c->noreply = true;\n\n    /* binprot supports 16bit keys, but internals are still 8bit */\n    if (keylen > KEY_MAX_LENGTH) {\n        handle_binary_protocol_error(c);\n        return;\n    }\n\n    switch (c->cmd) {\n    case PROTOCOL_BINARY_CMD_SETQ:\n        c->cmd = PROTOCOL_BINARY_CMD_SET;\n        break;\n    case PROTOCOL_BINARY_CMD_ADDQ:\n        c->cmd = PROTOCOL_BINARY_CMD_ADD;\n        break;\n    case PROTOCOL_BINARY_CMD_REPLACEQ:\n        c->cmd = PROTOCOL_BINARY_CMD_REPLACE;\n        break;\n    case PROTOCOL_BINARY_CMD_DELETEQ:\n        c->cmd = PROTOCOL_BINARY_CMD_DELETE;\n        break;\n    case PROTOCOL_BINARY_CMD_INCREMENTQ:\n        c->cmd = PROTOCOL_BINARY_CMD_INCREMENT;\n        break;\n    case PROTOCOL_BINARY_CMD_DECREMENTQ:\n        c->cmd = PROTOCOL_BINARY_CMD_DECREMENT;\n        break;\n    case PROTOCOL_BINARY_CMD_QUITQ:\n        c->cmd = PROTOCOL_BINARY_CMD_QUIT;\n        break;\n    case PROTOCOL_BINARY_CMD_FLUSHQ:\n        c->cmd = PROTOCOL_BINARY_CMD_FLUSH;\n        break;\n    case PROTOCOL_BINARY_CMD_APPENDQ:\n        c->cmd = PROTOCOL_BINARY_CMD_APPEND;\n        break;\n    case PROTOCOL_BINARY_CMD_PREPENDQ:\n        c->cmd = PROTOCOL_BINARY_CMD_PREPEND;\n        break;\n    case PROTOCOL_BINARY_CMD_GETQ:\n        c->cmd = PROTOCOL_BINARY_CMD_GET;\n        break;\n    case PROTOCOL_BINARY_CMD_GETKQ:\n        c->cmd = PROTOCOL_BINARY_CMD_GETK;\n        break;\n    case PROTOCOL_BINARY_CMD_GATQ:\n        c->cmd = PROTOCOL_BINARY_CMD_GAT;\n        break;\n    case PROTOCOL_BINARY_CMD_GATKQ:\n        c->cmd = PROTOCOL_BINARY_CMD_GATK;\n        break;\n    default:\n        c->noreply = false;\n    }\n\n    switch (c->cmd) {\n        case PROTOCOL_BINARY_CMD_VERSION:\n            if (extlen == 0 && keylen == 0 && bodylen == 0) {\n                write_bin_response(c, VERSION, 0, 0, strlen(VERSION));\n            } else {\n                protocol_error = 1;\n            }\n            break;\n        case PROTOCOL_BINARY_CMD_FLUSH:\n            if (keylen == 0 && bodylen == extlen && (extlen == 0 || extlen == 4)) {\n                process_bin_flush(c, extbuf);\n            } else {\n                protocol_error = 1;\n            }\n            break;\n        case PROTOCOL_BINARY_CMD_NOOP:\n            if (extlen == 0 && keylen == 0 && bodylen == 0) {\n                write_bin_response(c, NULL, 0, 0, 0);\n                // NOOP forces pipeline flush.\n                conn_set_state(c, conn_mwrite);\n            } else {\n                protocol_error = 1;\n            }\n            break;\n        case PROTOCOL_BINARY_CMD_SET: /* FALLTHROUGH */\n        case PROTOCOL_BINARY_CMD_ADD: /* FALLTHROUGH */\n        case PROTOCOL_BINARY_CMD_REPLACE:\n            if (extlen == 8 && keylen != 0 && bodylen >= (keylen + 8)) {\n                process_bin_update(c, extbuf);\n            } else {\n                protocol_error = 1;\n            }\n            break;\n        case PROTOCOL_BINARY_CMD_GETQ:  /* FALLTHROUGH */\n        case PROTOCOL_BINARY_CMD_GET:   /* FALLTHROUGH */\n        case PROTOCOL_BINARY_CMD_GETKQ: /* FALLTHROUGH */\n        case PROTOCOL_BINARY_CMD_GETK:\n            if (extlen == 0 && bodylen == keylen && keylen > 0) {\n                process_bin_get_or_touch(c, extbuf);\n            } else {\n                protocol_error = 1;\n            }\n            break;\n        case PROTOCOL_BINARY_CMD_DELETE:\n            if (keylen > 0 && extlen == 0 && bodylen == keylen) {\n                process_bin_delete(c);\n            } else {\n                protocol_error = 1;\n            }\n            break;\n        case PROTOCOL_BINARY_CMD_INCREMENT:\n        case PROTOCOL_BINARY_CMD_DECREMENT:\n            if (keylen > 0 && extlen == 20 && bodylen == (keylen + extlen)) {\n                complete_incr_bin(c, extbuf);\n            } else {\n                protocol_error = 1;\n            }\n            break;\n        case PROTOCOL_BINARY_CMD_APPEND:\n        case PROTOCOL_BINARY_CMD_PREPEND:\n            if (keylen > 0 && extlen == 0) {\n                process_bin_append_prepend(c);\n            } else {\n                protocol_error = 1;\n            }\n            break;\n        case PROTOCOL_BINARY_CMD_STAT:\n            if (extlen == 0) {\n                process_bin_stat(c);\n            } else {\n                protocol_error = 1;\n            }\n            break;\n        case PROTOCOL_BINARY_CMD_QUIT:\n            if (keylen == 0 && extlen == 0 && bodylen == 0) {\n                write_bin_response(c, NULL, 0, 0, 0);\n                conn_set_state(c, conn_mwrite);\n                c->close_after_write = true;\n                c->close_reason = NORMAL_CLOSE;\n            } else {\n                protocol_error = 1;\n            }\n            break;\n        case PROTOCOL_BINARY_CMD_SASL_LIST_MECHS:\n            if (extlen == 0 && keylen == 0 && bodylen == 0) {\n                bin_list_sasl_mechs(c);\n            } else {\n                protocol_error = 1;\n            }\n            break;\n        case PROTOCOL_BINARY_CMD_SASL_AUTH:\n        case PROTOCOL_BINARY_CMD_SASL_STEP:\n            if (extlen == 0 && keylen != 0) {\n                process_bin_sasl_auth(c);\n            } else {\n                protocol_error = 1;\n            }\n            break;\n        case PROTOCOL_BINARY_CMD_TOUCH:\n        case PROTOCOL_BINARY_CMD_GAT:\n        case PROTOCOL_BINARY_CMD_GATQ:\n        case PROTOCOL_BINARY_CMD_GATK:\n        case PROTOCOL_BINARY_CMD_GATKQ:\n            if (extlen == 4 && keylen != 0) {\n                process_bin_get_or_touch(c, extbuf);\n            } else {\n                protocol_error = 1;\n            }\n            break;\n        default:\n            write_bin_error(c, PROTOCOL_BINARY_RESPONSE_UNKNOWN_COMMAND, NULL,\n                            bodylen);\n    }\n\n    if (protocol_error)\n        handle_binary_protocol_error(c);\n}\n\nstatic void process_bin_update(conn *c, char *extbuf) {\n    char *key;\n    uint16_t nkey;\n    int vlen;\n    item *it;\n    protocol_binary_request_set* req = (void *)extbuf;\n\n    assert(c != NULL);\n\n    key = binary_get_key(c);\n    nkey = c->binary_header.request.keylen;\n\n    /* fix byteorder in the request */\n    req->message.body.flags = ntohl(req->message.body.flags);\n    req->message.body.expiration = ntohl(req->message.body.expiration);\n\n    vlen = c->binary_header.request.bodylen - (nkey + c->binary_header.request.extlen);\n\n    if (settings.verbose > 1) {\n        int ii;\n        if (c->cmd == PROTOCOL_BINARY_CMD_ADD) {\n            fprintf(stderr, \"<%d ADD \", c->sfd);\n        } else if (c->cmd == PROTOCOL_BINARY_CMD_SET) {\n            fprintf(stderr, \"<%d SET \", c->sfd);\n        } else {\n            fprintf(stderr, \"<%d REPLACE \", c->sfd);\n        }\n        for (ii = 0; ii < nkey; ++ii) {\n            fprintf(stderr, \"%c\", key[ii]);\n        }\n\n        fprintf(stderr, \" Value len is %d\", vlen);\n        fprintf(stderr, \"\\n\");\n    }\n\n    if (settings.detail_enabled) {\n        stats_prefix_record_set(key, nkey);\n    }\n\n    it = item_alloc(key, nkey, req->message.body.flags,\n            realtime(req->message.body.expiration), vlen+2);\n\n    if (it == 0) {\n        enum store_item_type status;\n        if (! item_size_ok(nkey, req->message.body.flags, vlen + 2)) {\n            write_bin_error(c, PROTOCOL_BINARY_RESPONSE_E2BIG, NULL, vlen);\n            status = TOO_LARGE;\n        } else {\n            out_of_memory(c, \"SERVER_ERROR Out of memory allocating item\");\n            /* This error generating method eats the swallow value. Add here. */\n            c->sbytes = vlen;\n            status = NO_MEMORY;\n        }\n        /* FIXME: losing c->cmd since it's translated below. refactor? */\n        LOGGER_LOG(c->thread->l, LOG_MUTATIONS, LOGGER_ITEM_STORE,\n                NULL, status, 0, key, nkey, req->message.body.expiration,\n                ITEM_clsid(it), c->sfd);\n\n        /* Avoid stale data persisting in cache because we failed alloc.\n         * Unacceptable for SET. Anywhere else too? */\n        if (c->cmd == PROTOCOL_BINARY_CMD_SET) {\n            it = item_get(key, nkey, c->thread, DONT_UPDATE);\n            if (it) {\n                item_unlink(it);\n                STORAGE_delete(c->thread->storage, it);\n                item_remove(it);\n            }\n        }\n\n        /* swallow the data line */\n        conn_set_state(c, conn_swallow);\n        return;\n    }\n\n    ITEM_set_cas(it, c->binary_header.request.cas);\n\n    switch (c->cmd) {\n        case PROTOCOL_BINARY_CMD_ADD:\n            c->cmd = NREAD_ADD;\n            break;\n        case PROTOCOL_BINARY_CMD_SET:\n            c->cmd = NREAD_SET;\n            break;\n        case PROTOCOL_BINARY_CMD_REPLACE:\n            c->cmd = NREAD_REPLACE;\n            break;\n        default:\n            assert(0);\n    }\n\n    if (ITEM_get_cas(it) != 0) {\n        c->cmd = NREAD_CAS;\n    }\n\n    c->item = it;\n#ifdef NEED_ALIGN\n    if (it->it_flags & ITEM_CHUNKED) {\n        c->ritem = ITEM_schunk(it);\n    } else {\n        c->ritem = ITEM_data(it);\n    }\n#else\n    c->ritem = ITEM_data(it);\n#endif\n    c->rlbytes = vlen;\n    conn_set_state(c, conn_nread);\n    c->substate = bin_read_set_value;\n}\n\nstatic void process_bin_append_prepend(conn *c) {\n    char *key;\n    uint16_t nkey;\n    int vlen;\n    item *it;\n\n    assert(c != NULL);\n\n    key = binary_get_key(c);\n    nkey = c->binary_header.request.keylen;\n    vlen = c->binary_header.request.bodylen - nkey;\n\n    if (settings.verbose > 1) {\n        fprintf(stderr, \"Value len is %d\\n\", vlen);\n    }\n\n    if (settings.detail_enabled) {\n        stats_prefix_record_set(key, nkey);\n    }\n\n    it = item_alloc(key, nkey, 0, 0, vlen+2);\n\n    if (it == 0) {\n        if (! item_size_ok(nkey, 0, vlen + 2)) {\n            write_bin_error(c, PROTOCOL_BINARY_RESPONSE_E2BIG, NULL, vlen);\n        } else {\n            out_of_memory(c, \"SERVER_ERROR Out of memory allocating item\");\n            /* OOM calls eat the swallow value. Add here. */\n            c->sbytes = vlen;\n        }\n        /* swallow the data line */\n        conn_set_state(c, conn_swallow);\n        return;\n    }\n\n    ITEM_set_cas(it, c->binary_header.request.cas);\n\n    switch (c->cmd) {\n        case PROTOCOL_BINARY_CMD_APPEND:\n            c->cmd = NREAD_APPEND;\n            break;\n        case PROTOCOL_BINARY_CMD_PREPEND:\n            c->cmd = NREAD_PREPEND;\n            break;\n        default:\n            assert(0);\n    }\n\n    c->item = it;\n#ifdef NEED_ALIGN\n    if (it->it_flags & ITEM_CHUNKED) {\n        c->ritem = ITEM_schunk(it);\n    } else {\n        c->ritem = ITEM_data(it);\n    }\n#else\n    c->ritem = ITEM_data(it);\n#endif\n    c->rlbytes = vlen;\n    conn_set_state(c, conn_nread);\n    c->substate = bin_read_set_value;\n}\n\nstatic void process_bin_flush(conn *c, char *extbuf) {\n    time_t exptime = 0;\n    protocol_binary_request_flush* req = (void *)extbuf;\n    rel_time_t new_oldest = 0;\n\n    if (!settings.flush_enabled) {\n      // flush_all is not allowed but we log it on stats\n      write_bin_error(c, PROTOCOL_BINARY_RESPONSE_AUTH_ERROR, NULL, 0);\n      return;\n    }\n\n    if (c->binary_header.request.extlen == sizeof(req->message.body)) {\n        exptime = ntohl(req->message.body.expiration);\n    }\n\n    if (exptime > 0) {\n        new_oldest = realtime(exptime) - 1;\n    } else {\n        new_oldest = current_time - 1;\n    }\n    settings.oldest_live = new_oldest;\n    item_flush_expired();\n\n    pthread_mutex_lock(&c->thread->stats.mutex);\n    c->thread->stats.flush_cmds++;\n    pthread_mutex_unlock(&c->thread->stats.mutex);\n\n    write_bin_response(c, NULL, 0, 0, 0);\n}\n\nstatic void process_bin_delete(conn *c) {\n    item *it;\n    uint32_t hv;\n\n    assert(c != NULL);\n    char* key = binary_get_key(c);\n    size_t nkey = c->binary_header.request.keylen;\n\n    if (settings.verbose > 1) {\n        int ii;\n        fprintf(stderr, \"Deleting \");\n        for (ii = 0; ii < nkey; ++ii) {\n            fprintf(stderr, \"%c\", key[ii]);\n        }\n        fprintf(stderr, \"\\n\");\n    }\n\n    if (settings.detail_enabled) {\n        stats_prefix_record_delete(key, nkey);\n    }\n\n    it = item_get_locked(key, nkey, c->thread, DONT_UPDATE, &hv);\n    if (it) {\n        uint64_t cas = c->binary_header.request.cas;\n        if (cas == 0 || cas == ITEM_get_cas(it)) {\n            MEMCACHED_COMMAND_DELETE(c->sfd, ITEM_key(it), it->nkey);\n            pthread_mutex_lock(&c->thread->stats.mutex);\n            c->thread->stats.slab_stats[ITEM_clsid(it)].delete_hits++;\n            pthread_mutex_unlock(&c->thread->stats.mutex);\n            do_item_unlink(it, hv);\n            STORAGE_delete(c->thread->storage, it);\n            write_bin_response(c, NULL, 0, 0, 0);\n        } else {\n            write_bin_error(c, PROTOCOL_BINARY_RESPONSE_KEY_EEXISTS, NULL, 0);\n        }\n        do_item_remove(it);      /* release our reference */\n    } else {\n        write_bin_error(c, PROTOCOL_BINARY_RESPONSE_KEY_ENOENT, NULL, 0);\n        pthread_mutex_lock(&c->thread->stats.mutex);\n        c->thread->stats.delete_misses++;\n        pthread_mutex_unlock(&c->thread->stats.mutex);\n    }\n    item_unlock(hv);\n}\n\n\n"
        },
        {
          "name": "proto_bin.h",
          "type": "blob",
          "size": 0.27734375,
          "content": "#ifndef PROTO_BIN_H\n#define PROTO_BIN_H\n\n/* binary protocol handlers */\nint try_read_command_binary(conn *c);\nvoid complete_nread_binary(conn *c);\nvoid write_bin_error(conn *c, protocol_binary_response_status err,\n                            const char *errstr, int swallow);\n\n#endif\n"
        },
        {
          "name": "proto_proxy.c",
          "type": "blob",
          "size": 61.5830078125,
          "content": "/* -*- Mode: C; tab-width: 4; c-basic-offset: 4; indent-tabs-mode: nil -*- */\n/*\n * Functions for handling the proxy layer. wraps text protocols\n *\n * NOTE: many lua functions generate pointers via \"lua_newuserdatauv\" or\n * similar. Normal memory checking isn't done as lua will throw a high level\n * error if malloc fails. Must keep this in mind while allocating data so any\n * manually malloc'ed information gets freed properly.\n */\n\n#include \"proxy.h\"\n\n#define PROCESS_MULTIGET true\n#define PROCESS_NORMAL false\n#define PROXY_GC_BACKGROUND_SECONDS 4\n#define PROXY_GC_DEFAULT_RATIO 2.0\nstatic void proxy_process_command(conn *c, char *command, size_t cmdlen, bool multiget);\nstatic void *mcp_profile_alloc(void *ud, void *ptr, size_t osize, size_t nsize);\n\n/******** EXTERNAL FUNCTIONS ******/\n// functions starting with _ are breakouts for the public functions.\n\nstatic inline void _proxy_advance_lastkb(lua_State *L, LIBEVENT_THREAD *t) {\n    int new_kb = lua_gc(L, LUA_GCCOUNT);\n    // We need to slew the increase in \"gc pause\" because the lua GC actually\n    // needs to run twice to free a userdata: once to run the _gc's and again\n    // to actually clean up the object.\n    // Meaning we will continually increase in size.\n    if (new_kb > t->proxy_vm_last_kb) {\n        new_kb = t->proxy_vm_last_kb + (new_kb - t->proxy_vm_last_kb) * 0.50;\n    }\n\n    // remove the memory freed during this cycle so we can kick off the GC\n    // early if we're very aggressively making garbage.\n    // carry our negative delta forward so a huge reclaim can push for a\n    // couple cycles.\n    if (t->proxy_vm_negative_delta >= new_kb) {\n        t->proxy_vm_negative_delta -= new_kb;\n        new_kb = 1;\n    } else {\n        new_kb -= t->proxy_vm_negative_delta;\n        t->proxy_vm_negative_delta = 0;\n    }\n\n    t->proxy_vm_last_kb = new_kb;\n}\n\n// The lua GC is paused while running requests. Run it manually inbetween\n// processing network events.\nvoid proxy_gc_poke(LIBEVENT_THREAD *t) {\n    lua_State *L = t->L;\n    proxy_ctx_t *ctx = t->proxy_ctx;\n    float ratio = ctx->tunables.gc_ratio;\n    struct proxy_int_stats *is = t->proxy_int_stats;\n    int vm_kb = lua_gc(L, LUA_GCCOUNT) + t->proxy_vm_extra_kb;\n    WSTAT_L(t);\n    is->vm_memory_kb = vm_kb;\n    WSTAT_UL(t);\n\n    // equivalent of luagc \"pause\" value\n    int last = t->proxy_vm_last_kb;\n    if (t->proxy_vm_gcrunning <= 0 && vm_kb > last * ratio) {\n        t->proxy_vm_gcrunning = 1;\n        //fprintf(stderr, \"PROXYGC: proxy_gc_poke START [cur: %d - last: %d - ratio: %f]\\n\", vm_kb, last, ratio);\n    }\n\n    // We configure small GC \"steps\" then increase the number of times we run\n    // a step based on current memory usage.\n    if (t->proxy_vm_gcrunning > 0) {\n        t->proxy_vm_gcpokemem = 0;\n        int loops = t->proxy_vm_gcrunning;\n        int done = 0;\n        /*fprintf(stderr, \"PROXYGC: proxy_gc_poke [cur: %d - last: %d - loops: %d]\\n\",\n            vm_kb,\n            t->proxy_vm_last_kb,\n            loops);*/\n        while (loops-- && !done) {\n            // reset counters once full GC cycle has completed\n            done = lua_gc(L, LUA_GCSTEP, 0);\n        }\n\n        int vm_kb_after = lua_gc(L, LUA_GCCOUNT);\n        int vm_kb_clean = vm_kb - t->proxy_vm_extra_kb;\n        if (vm_kb_clean > vm_kb_after) {\n            // track the amount of memory freed during the GC cycle.\n            t->proxy_vm_negative_delta += vm_kb_clean - vm_kb_after;\n        }\n\n        if (done) {\n            _proxy_advance_lastkb(L, t);\n            t->proxy_vm_extra_kb = 0;\n            t->proxy_vm_gcrunning = 0;\n            WSTAT_L(t);\n            is->vm_gc_runs++;\n            WSTAT_UL(t);\n            //fprintf(stderr, \"PROXYGC: proxy_gc_poke COMPLETE [cur: %d next: %d]\\n\", lua_gc(L, LUA_GCCOUNT), t->proxy_vm_last_kb);\n        } else if ((last*ratio) * (1 + t->proxy_vm_gcrunning*0.20) < vm_kb) {\n            // increase the aggressiveness by memory bloat level.\n            t->proxy_vm_gcrunning++;\n            //fprintf(stderr, \"PROXYGC: proxy_gc_poke INCREASING AGGRESSIVENESS [cur: %d - aggro: %d]\\n\", t->proxy_vm_last_kb, t->proxy_vm_gcrunning);\n        }\n    }\n}\n\n// every couple seconds we force-run one GC step.\n// this is needed until after API1 is retired and pool objects are no longer\n// managed by the GC.\n// We use a negative value so a \"timer poke\" GC run doesn't cause requests to\n// suddenly aggressively run the GC.\nstatic void proxy_gc_timerpoke(evutil_socket_t fd, short event, void *arg) {\n    LIBEVENT_THREAD *t = arg;\n    struct timeval next = { PROXY_GC_BACKGROUND_SECONDS, 0 };\n    evtimer_add(t->proxy_gc_timer, &next);\n    // if GC ran recently, don't do anything.\n    // also if memory changed recently, don't do anything.\n    int curmem = lua_gc(t->L, LUA_GCCOUNT);\n    if (t->proxy_vm_gcpokemem == 0 || curmem != t->proxy_vm_gcpokemem) {\n        t->proxy_vm_gcpokemem = curmem;\n        return;\n    }\n\n    // if we weren't told to skip and there's otherwise no GC running, start a\n    // GC run.\n    if (t->proxy_vm_gcrunning == 0) {\n        t->proxy_vm_gcrunning = -1;\n    }\n\n    // only advance GC if we're doing our own timer run.\n    if (t->proxy_vm_gcrunning == -1) {\n        if (lua_gc(t->L, LUA_GCSTEP, 0)) {\n            // don't advance last_kb: let the main algo decide when to run.\n            t->proxy_vm_gcrunning = 0;\n            t->proxy_vm_gcpokemem = 0;\n        } else {\n            // only continue running if memory stays where we expect it.\n            t->proxy_vm_gcpokemem = lua_gc(t->L, LUA_GCCOUNT);\n        }\n    }\n}\n\nbool proxy_bufmem_checkadd(LIBEVENT_THREAD *t, int len) {\n    bool oom = false;\n    pthread_mutex_lock(&t->proxy_limit_lock);\n    if (t->proxy_buffer_memory_used > t->proxy_buffer_memory_limit) {\n        oom = true;\n    } else {\n        t->proxy_buffer_memory_used += len;\n    }\n    pthread_mutex_unlock(&t->proxy_limit_lock);\n    return oom;\n}\n\n// see also: process_extstore_stats()\nvoid proxy_stats(void *arg, ADD_STAT add_stats, void *c) {\n    if (arg == NULL) {\n       return;\n    }\n    proxy_ctx_t *ctx = arg;\n\n    STAT_L(ctx);\n    APPEND_STAT(\"proxy_config_reloads\", \"%llu\", (unsigned long long)ctx->global_stats.config_reloads);\n    APPEND_STAT(\"proxy_config_reload_fails\", \"%llu\", (unsigned long long)ctx->global_stats.config_reload_fails);\n    APPEND_STAT(\"proxy_config_cron_runs\", \"%llu\", (unsigned long long)ctx->global_stats.config_cron_runs);\n    APPEND_STAT(\"proxy_config_cron_fails\", \"%llu\", (unsigned long long)ctx->global_stats.config_cron_fails);\n    APPEND_STAT(\"proxy_backend_total\", \"%llu\", (unsigned long long)ctx->global_stats.backend_total);\n    APPEND_STAT(\"proxy_backend_marked_bad\", \"%llu\", (unsigned long long)ctx->global_stats.backend_marked_bad);\n    APPEND_STAT(\"proxy_backend_failed\", \"%llu\", (unsigned long long)ctx->global_stats.backend_failed);\n    APPEND_STAT(\"proxy_request_failed_depth\", \"%llu\", (unsigned long long)ctx->global_stats.request_failed_depth);\n    STAT_UL(ctx);\n}\n\nvoid process_proxy_stats(void *arg, ADD_STAT add_stats, void *c) {\n    char key_str[STAT_KEY_LEN];\n    struct proxy_int_stats istats = {0};\n    uint64_t req_limit = 0;\n    uint64_t buffer_memory_limit = 0;\n    uint64_t buffer_memory_used = 0;\n\n    if (!arg) {\n        return;\n    }\n    proxy_ctx_t *ctx = arg;\n    STAT_L(ctx);\n    req_limit = ctx->active_req_limit;\n    buffer_memory_limit = ctx->buffer_memory_limit;\n\n    // prepare aggregated counters.\n    struct proxy_user_stats_entry *us = ctx->user_stats;\n    int stats_num = ctx->user_stats_num;\n    uint64_t counters[stats_num];\n    memset(counters, 0, sizeof(counters));\n\n    // TODO (v3): more globals to remove and/or change API method.\n    // aggregate worker thread counters.\n    for (int x = 0; x < settings.num_threads; x++) {\n        LIBEVENT_THREAD *t = get_worker_thread(x);\n        struct proxy_user_stats *tus = t->proxy_user_stats;\n        struct proxy_int_stats *is = t->proxy_int_stats;\n        WSTAT_L(t);\n        for (int i = 0; i < CMD_FINAL; i++) {\n            istats.counters[i] += is->counters[i];\n        }\n        istats.vm_gc_runs += is->vm_gc_runs;\n        istats.vm_memory_kb += is->vm_memory_kb;\n        if (tus && tus->num_stats >= stats_num) {\n            for (int i = 0; i < stats_num; i++) {\n                counters[i] += tus->counters[i];\n            }\n        }\n        WSTAT_UL(t);\n        pthread_mutex_lock(&t->proxy_limit_lock);\n        buffer_memory_used += t->proxy_buffer_memory_used;\n        pthread_mutex_unlock(&t->proxy_limit_lock);\n    }\n\n    // return all of the user generated stats\n    if (ctx->user_stats_namebuf) {\n        char vbuf[INCR_MAX_STORAGE_LEN];\n        char *e = NULL; // ptr into vbuf\n        const char *pfx = \"user_\";\n        const size_t pfxlen = strlen(pfx);\n        for (int x = 0; x < stats_num; x++) {\n            if (us[x].cname) {\n                char *name = ctx->user_stats_namebuf + us[x].cname;\n                size_t nlen = strlen(name);\n                if (nlen > STAT_KEY_LEN-6) {\n                    // impossible, but for paranoia.\n                    nlen = STAT_KEY_LEN-6;\n                }\n                // avoiding an snprintf call for some performance (\"user_%s\")\n                memcpy(key_str, pfx, pfxlen);\n                memcpy(key_str+pfxlen, name, nlen);\n                key_str[pfxlen+nlen] = '\\0';\n\n                // APPEND_STAT() calls another snprintf, which calls our\n                // add_stats argument. Lets skip yet another snprintf with\n                // some unrolling.\n                e = itoa_u64(counters[x], vbuf);\n                *(e+1) = '\\0';\n                add_stats(key_str, pfxlen+nlen, vbuf, e-vbuf, c);\n            }\n        }\n    }\n\n    STAT_UL(ctx);\n\n    if (buffer_memory_limit == UINT64_MAX) {\n        buffer_memory_limit = 0;\n    } else {\n        buffer_memory_limit *= settings.num_threads;\n    }\n    if (req_limit == UINT64_MAX) {\n        req_limit = 0;\n    } else {\n        req_limit *= settings.num_threads;\n    }\n\n    // return proxy counters\n    APPEND_STAT(\"active_req_limit\", \"%llu\", (unsigned long long)req_limit);\n    APPEND_STAT(\"buffer_memory_limit\", \"%llu\", (unsigned long long)buffer_memory_limit);\n    APPEND_STAT(\"buffer_memory_used\", \"%llu\", (unsigned long long)buffer_memory_used);\n    APPEND_STAT(\"vm_gc_runs\", \"%llu\", (unsigned long long)istats.vm_gc_runs);\n    APPEND_STAT(\"vm_memory_kb\", \"%llu\", (unsigned long long)istats.vm_memory_kb);\n    APPEND_STAT(\"cmd_mg\", \"%llu\", (unsigned long long)istats.counters[CMD_MG]);\n    APPEND_STAT(\"cmd_ms\", \"%llu\", (unsigned long long)istats.counters[CMD_MS]);\n    APPEND_STAT(\"cmd_md\", \"%llu\", (unsigned long long)istats.counters[CMD_MD]);\n    APPEND_STAT(\"cmd_mn\", \"%llu\", (unsigned long long)istats.counters[CMD_MN]);\n    APPEND_STAT(\"cmd_ma\", \"%llu\", (unsigned long long)istats.counters[CMD_MA]);\n    APPEND_STAT(\"cmd_me\", \"%llu\", (unsigned long long)istats.counters[CMD_ME]);\n    APPEND_STAT(\"cmd_get\", \"%llu\", (unsigned long long)istats.counters[CMD_GET]);\n    APPEND_STAT(\"cmd_gat\", \"%llu\", (unsigned long long)istats.counters[CMD_GAT]);\n    APPEND_STAT(\"cmd_set\", \"%llu\", (unsigned long long)istats.counters[CMD_SET]);\n    APPEND_STAT(\"cmd_add\", \"%llu\", (unsigned long long)istats.counters[CMD_ADD]);\n    APPEND_STAT(\"cmd_cas\", \"%llu\", (unsigned long long)istats.counters[CMD_CAS]);\n    APPEND_STAT(\"cmd_gets\", \"%llu\", (unsigned long long)istats.counters[CMD_GETS]);\n    APPEND_STAT(\"cmd_gats\", \"%llu\", (unsigned long long)istats.counters[CMD_GATS]);\n    APPEND_STAT(\"cmd_incr\", \"%llu\", (unsigned long long)istats.counters[CMD_INCR]);\n    APPEND_STAT(\"cmd_decr\", \"%llu\", (unsigned long long)istats.counters[CMD_DECR]);\n    APPEND_STAT(\"cmd_touch\", \"%llu\", (unsigned long long)istats.counters[CMD_TOUCH]);\n    APPEND_STAT(\"cmd_append\", \"%llu\", (unsigned long long)istats.counters[CMD_APPEND]);\n    APPEND_STAT(\"cmd_prepend\", \"%llu\", (unsigned long long)istats.counters[CMD_PREPEND]);\n    APPEND_STAT(\"cmd_delete\", \"%llu\", (unsigned long long)istats.counters[CMD_DELETE]);\n    APPEND_STAT(\"cmd_replace\", \"%llu\", (unsigned long long)istats.counters[CMD_REPLACE]);\n}\n\nvoid process_proxy_funcstats(void *arg, ADD_STAT add_stats, void *c) {\n    char key_str[STAT_KEY_LEN];\n    if (!arg) {\n        return;\n    }\n    proxy_ctx_t *ctx = arg;\n    lua_State *L = ctx->proxy_sharedvm;\n    pthread_mutex_lock(&ctx->sharedvm_lock);\n\n    // iterate all of the named function slots\n    lua_pushnil(L);\n    while (lua_next(L, SHAREDVM_FGEN_IDX) != 0) {\n        int n = lua_tointeger(L, -1);\n        lua_pop(L, 1); // drop the value, leave the key.\n        if (n != 0) {\n            // reuse the key. make a copy since rawget will pop it.\n            lua_pushvalue(L, -1);\n            lua_rawget(L, SHAREDVM_FGENSLOT_IDX);\n            int slots = lua_tointeger(L, -1);\n            lua_pop(L, 1); // drop the slot count.\n\n            // now grab the name key.\n            const char *name = lua_tostring(L, -1);\n            snprintf(key_str, STAT_KEY_LEN-1, \"funcs_%s\", name);\n            APPEND_STAT(key_str, \"%d\", n);\n            snprintf(key_str, STAT_KEY_LEN-1, \"slots_%s\", name);\n            APPEND_STAT(key_str, \"%d\", slots);\n        } else {\n            // TODO: It is safe to delete keys here. Slightly complex so low\n            // priority.\n        }\n    }\n\n    pthread_mutex_unlock(&ctx->sharedvm_lock);\n}\n\nvoid process_proxy_bestats(void *arg, ADD_STAT add_stats, void *c) {\n    char key_str[STAT_KEY_LEN];\n    if (!arg) {\n        return;\n    }\n    proxy_ctx_t *ctx = arg;\n    lua_State *L = ctx->proxy_sharedvm;\n    pthread_mutex_lock(&ctx->sharedvm_lock);\n\n    // iterate all of the listed backends\n    lua_pushnil(L);\n    while (lua_next(L, SHAREDVM_BACKEND_IDX) != 0) {\n        int n = lua_tointeger(L, -1);\n        lua_pop(L, 1); // drop the value, leave the key.\n        if (n != 0) {\n            // now grab the name key.\n            const char *name = lua_tostring(L, -1);\n            snprintf(key_str, STAT_KEY_LEN-1, \"bad_%s\", name);\n            APPEND_STAT(key_str, \"%d\", n);\n        } else {\n            // delete keys of backends that are no longer bad or no longer\n            // exist to keep the table small.\n            const char *name = lua_tostring(L, -1);\n            lua_pushnil(L);\n            lua_setfield(L, SHAREDVM_BACKEND_IDX, name);\n        }\n    }\n\n    pthread_mutex_unlock(&ctx->sharedvm_lock);\n}\n\n// start the centralized lua state and config thread.\nvoid *proxy_init(bool use_uring, bool proxy_memprofile) {\n    proxy_ctx_t *ctx = calloc(1, sizeof(proxy_ctx_t));\n    ctx->use_uring = use_uring;\n    ctx->memprofile = proxy_memprofile;\n\n    pthread_mutex_init(&ctx->config_lock, NULL);\n    pthread_cond_init(&ctx->config_cond, NULL);\n    pthread_mutex_init(&ctx->worker_lock, NULL);\n    pthread_cond_init(&ctx->worker_cond, NULL);\n    pthread_mutex_init(&ctx->manager_lock, NULL);\n    pthread_cond_init(&ctx->manager_cond, NULL);\n    pthread_mutex_init(&ctx->stats_lock, NULL);\n\n    ctx->active_req_limit = UINT64_MAX;\n    ctx->buffer_memory_limit = UINT64_MAX;\n\n    // FIXME (v2): default defines.\n    ctx->tunables.tcp_keepalive = false;\n    ctx->tunables.backend_failure_limit = 3;\n    ctx->tunables.connect.tv_sec = 5;\n    ctx->tunables.retry.tv_sec = 3;\n    ctx->tunables.read.tv_sec = 3;\n    ctx->tunables.flap_backoff_ramp = 1.5;\n    ctx->tunables.flap_backoff_max = 3600;\n    ctx->tunables.gc_ratio = PROXY_GC_DEFAULT_RATIO;\n    ctx->tunables.backend_depth_limit = 0;\n    ctx->tunables.max_ustats = MAX_USTATS_DEFAULT;\n    ctx->tunables.use_iothread = false;\n    ctx->tunables.use_tls = false;\n\n    STAILQ_INIT(&ctx->manager_head);\n    lua_State *L = NULL;\n    if (ctx->memprofile) {\n        struct mcp_memprofile *prof = calloc(1, sizeof(struct mcp_memprofile));\n        prof->id = ctx->memprofile_thread_counter++;\n        L = lua_newstate(mcp_profile_alloc, prof);\n    } else {\n        L = luaL_newstate();\n    }\n    ctx->proxy_state = L;\n    luaL_openlibs(L);\n    // NOTE: might need to differentiate the libs yes?\n    proxy_register_libs(ctx, NULL, L);\n    // Create the cron table.\n    lua_newtable(L);\n    ctx->cron_ref = luaL_ref(L, LUA_REGISTRYINDEX);\n    ctx->cron_next = INT_MAX;\n\n    // set up the shared state VM. Used by short-lock events (counters/state)\n    // for global visibility.\n    pthread_mutex_init(&ctx->sharedvm_lock, NULL);\n    ctx->proxy_sharedvm = luaL_newstate();\n    luaL_openlibs(ctx->proxy_sharedvm);\n    // we keep info tables in the top level stack so we don't have to\n    // constantly fetch them from registry.\n    lua_newtable(ctx->proxy_sharedvm); // fgen count\n    lua_newtable(ctx->proxy_sharedvm); // fgen slot count\n    lua_newtable(ctx->proxy_sharedvm); // backend down status\n\n    // Create/start the IO thread, which we need before servers\n    // start getting created.\n    proxy_event_thread_t *t = calloc(1, sizeof(proxy_event_thread_t));\n    ctx->proxy_io_thread = t;\n    proxy_init_event_thread(t, ctx, NULL);\n\n    pthread_create(&t->thread_id, NULL, proxy_event_thread, t);\n    thread_setname(t->thread_id, \"mc-prx-io\");\n\n    _start_proxy_config_threads(ctx);\n    return ctx;\n}\n\n// Initialize the VM for an individual worker thread.\nvoid proxy_thread_init(void *ctx, LIBEVENT_THREAD *thr) {\n    assert(ctx != NULL);\n    assert(thr != NULL);\n\n    // Create the hook table.\n    thr->proxy_hooks = calloc(CMD_SIZE, sizeof(struct proxy_hook));\n    if (thr->proxy_hooks == NULL) {\n        fprintf(stderr, \"Failed to allocate proxy hooks\\n\");\n        exit(EXIT_FAILURE);\n    }\n    thr->proxy_int_stats = calloc(1, sizeof(struct proxy_int_stats));\n    if (thr->proxy_int_stats == NULL) {\n        fprintf(stderr, \"Failed to allocate proxy thread stats\\n\");\n        exit(EXIT_FAILURE);\n    }\n    pthread_mutex_init(&thr->proxy_limit_lock, NULL);\n    thr->proxy_ctx = ctx;\n\n    // Initialize the lua state.\n    proxy_ctx_t *pctx = ctx;\n    lua_State *L = NULL;\n    if (pctx->memprofile) {\n        struct mcp_memprofile *prof = calloc(1, sizeof(struct mcp_memprofile));\n        prof->id = pctx->memprofile_thread_counter++;\n        L = lua_newstate(mcp_profile_alloc, prof);\n    } else {\n        L = luaL_newstate();\n    }\n\n    // With smaller requests the default incremental collector appears to\n    // never complete. With this simple tuning (def-1, def, def) it seems\n    // fine.\n    // We can't use GCGEN until we manage pools with reference counting, as\n    // they may never hit GC and thus never release their connection\n    // resources.\n    lua_gc(L, LUA_GCINC, 199, 100, 12);\n    lua_gc(L, LUA_GCSTOP); // handle GC on our own schedule.\n    thr->L = L;\n    luaL_openlibs(L);\n    proxy_register_libs(ctx, thr, L);\n    // TODO: srand on time? do we need to bother?\n    for (int x = 0; x < 3; x++) {\n        thr->proxy_rng[x] = rand();\n    }\n\n    // init our internal GC checker.\n    thr->proxy_vm_last_kb = lua_gc(L, LUA_GCCOUNT);\n    assert(thr->proxy_vm_last_kb != 0);\n    thr->proxy_gc_timer = evtimer_new(thr->base, proxy_gc_timerpoke, thr);\n    // kick off the timer loop.\n    proxy_gc_timerpoke(0, 0, thr);\n\n    // Create a proxy event thread structure to piggyback on the worker.\n    proxy_event_thread_t *t = calloc(1, sizeof(proxy_event_thread_t));\n    thr->proxy_event_thread = t;\n    proxy_init_event_thread(t, ctx, thr->base);\n}\n\n// ctx_stack is a stack of io_pending_proxy_t's.\n// head of q->s_ctx is the \"newest\" request so we must push into the head\n// of the next queue, as requests are dequeued from the head\nvoid proxy_submit_cb(io_queue_t *q) {\n    proxy_event_thread_t *e = ((proxy_ctx_t *)q->ctx)->proxy_io_thread;\n    io_pending_proxy_t *p = q->stack_ctx;\n    io_head_t head;\n    be_head_t w_head; // worker local stack.\n    STAILQ_INIT(&head);\n    STAILQ_INIT(&w_head);\n\n    // NOTE: responses get returned in the correct order no matter what, since\n    // mc_resp's are linked.\n    // we just need to ensure stuff is parsed off the backend in the correct\n    // order.\n    // So we can do with a single list here, but we need to repair the list as\n    // responses are parsed. (in the req_remaining-- section)\n    // TODO (v2):\n    // - except we can't do that because the deferred IO stack isn't\n    // compatible with queue.h.\n    // So for now we build the secondary list with an STAILQ, which\n    // can be transplanted/etc.\n    while (p) {\n        mcp_backend_t *be;\n        P_DEBUG(\"%s: queueing req for backend: %p\\n\", __func__, (void *)p);\n        if (p->qcount_incr) {\n            // funny workaround: async IOP's don't count toward\n            // resuming a connection, only the completion of the async\n            // condition.\n            q->count++;\n        }\n\n        if (p->background) {\n            P_DEBUG(\"%s: fast-returning background object: %p\\n\", __func__, (void *)p);\n            // intercept background requests\n            // this call cannot recurse if we're on the worker thread,\n            // since the worker thread has to finish executing this\n            // function in order to pick up the returned IO.\n            return_io_pending((io_pending_t *)p);\n            p = p->next;\n            continue;\n        }\n        be = p->backend;\n\n        if (be->use_io_thread) {\n            STAILQ_INSERT_HEAD(&head, p, io_next);\n        } else {\n            // emulate some of handler_dequeue()\n            STAILQ_INSERT_HEAD(&be->io_head, p, io_next);\n            assert(be->depth > -1);\n            be->depth++;\n            if (!be->stacked) {\n                be->stacked = true;\n                STAILQ_INSERT_TAIL(&w_head, be, be_next);\n            }\n        }\n\n        p = p->next;\n    }\n\n    // clear out the submit queue so we can re-queue new IO's inline.\n    q->stack_ctx = NULL;\n\n    if (!STAILQ_EMPTY(&head)) {\n        bool do_notify = false;\n        P_DEBUG(\"%s: submitting queue to IO thread\\n\", __func__);\n        // Transfer request stack to event thread.\n        pthread_mutex_lock(&e->mutex);\n        if (STAILQ_EMPTY(&e->io_head_in)) {\n            do_notify = true;\n        }\n        STAILQ_CONCAT(&e->io_head_in, &head);\n        // No point in holding the lock since we're not doing a cond signal.\n        pthread_mutex_unlock(&e->mutex);\n\n        if (do_notify) {\n        // Signal to check queue.\n#ifdef USE_EVENTFD\n        uint64_t u = 1;\n        // TODO (v2): check result? is it ever possible to get a short write/failure\n        // for an eventfd?\n        if (write(e->event_fd, &u, sizeof(uint64_t)) != sizeof(uint64_t)) {\n            assert(1 == 0);\n        }\n#else\n        if (write(e->notify_send_fd, \"w\", 1) <= 0) {\n            assert(1 == 0);\n        }\n#endif\n        }\n    }\n\n    if (!STAILQ_EMPTY(&w_head)) {\n        P_DEBUG(\"%s: running inline worker queue\\n\", __func__);\n        // emulating proxy_event_handler\n        proxy_run_backend_queue(&w_head);\n    }\n    return;\n}\n\n// This function handles return processing for the \"old style\" API:\n// currently just `mcp.internal()`\nvoid proxy_return_rctx_cb(io_pending_t *pending) {\n    io_pending_proxy_t *p = (io_pending_proxy_t *)pending;\n    if (p->client_resp && p->client_resp->blen) {\n        // FIXME: workaround for buffer memory being external to objects.\n        // can't run 0 since that means something special (run the GC)\n        unsigned int kb = p->client_resp->blen / 1000;\n        p->thread->proxy_vm_extra_kb += kb > 0 ? kb : 1;\n    }\n\n    mcp_rcontext_t *rctx = p->rctx;\n    lua_rotate(rctx->Lc, 1, 1);\n    lua_settop(rctx->Lc, 1);\n    // hold the resp for a minute.\n    mc_resp *resp = rctx->resp;\n\n    proxy_run_rcontext(rctx);\n    mcp_funcgen_return_rctx(rctx);\n\n    io_queue_t *q = conn_io_queue_get(p->c, p->io_queue_type);\n    // Detatch the iop from the mc_resp and free it here.\n    conn *c = p->c;\n    if (p->io_type != IO_PENDING_TYPE_EXTSTORE) {\n        // if we're doing an extstore subrequest, the iop needs to live until\n        // resp's ->finish_cb is called.\n        resp->io_pending = NULL;\n        do_cache_free(p->thread->io_cache, p);\n    }\n\n    q->count--;\n    if (q->count == 0) {\n        // call re-add directly since we're already in the worker thread.\n        conn_worker_readd(c);\n    }\n}\n\n// This is called if resp_finish is called while an iop exists on the\n// resp.\n// so we need to release our iop and rctx.\n// - This can't happen unless we're doing extstore fetches.\n// - the request context is freed before connection processing resumes.\nvoid proxy_finalize_rctx_cb(io_pending_t *pending) {\n    io_pending_proxy_t *p = (io_pending_proxy_t *)pending;\n\n    if (p->io_type == IO_PENDING_TYPE_EXTSTORE) {\n        if (p->hdr_it) {\n            // TODO: lock once, worst case this hashes/locks twice.\n            if (p->miss) {\n                item_unlink(p->hdr_it);\n            }\n            item_remove(p->hdr_it);\n        }\n    }\n}\n\nint try_read_command_proxy(conn *c) {\n    char *el, *cont;\n\n    if (c->rbytes == 0)\n        return 0;\n\n    el = memchr(c->rcurr, '\\n', c->rbytes);\n    if (!el) {\n        if (c->rbytes > 1024) {\n            /*\n             * We didn't have a '\\n' in the first k. This _has_ to be a\n             * large multiget, if not we should just nuke the connection.\n             */\n            char *ptr = c->rcurr;\n            while (*ptr == ' ') { /* ignore leading whitespaces */\n                ++ptr;\n            }\n\n            if (ptr - c->rcurr > 100 ||\n                (strncmp(ptr, \"get \", 4) && strncmp(ptr, \"gets \", 5))) {\n\n                conn_set_state(c, conn_closing);\n                return 1;\n            }\n\n            // ASCII multigets are unbound, so our fixed size rbuf may not\n            // work for this particular workload... For backcompat we'll use a\n            // malloc/realloc/free routine just for this.\n            if (!c->rbuf_malloced) {\n                if (!rbuf_switch_to_malloc(c)) {\n                    conn_set_state(c, conn_closing);\n                    return 1;\n                }\n            }\n        }\n\n        return 0;\n    }\n    cont = el + 1;\n\n    assert(cont <= (c->rcurr + c->rbytes));\n\n    c->last_cmd_time = current_time;\n    proxy_process_command(c, c->rcurr, cont - c->rcurr, PROCESS_NORMAL);\n\n    c->rbytes -= (cont - c->rcurr);\n    c->rcurr = cont;\n\n    assert(c->rcurr <= (c->rbuf + c->rsize));\n\n    return 1;\n\n}\n\n// Called when a connection is closed while in nread state reading a set\n// Must only be called with an active coroutine.\nvoid proxy_cleanup_conn(conn *c) {\n    assert(c->proxy_rctx);\n    mcp_rcontext_t *rctx = c->proxy_rctx;\n    assert(rctx->pending_reqs == 1);\n    rctx->pending_reqs = 0;\n\n    mcp_funcgen_return_rctx(rctx);\n    c->proxy_rctx = NULL;\n}\n\n// we buffered a SET of some kind.\nvoid complete_nread_proxy(conn *c) {\n    assert(c != NULL);\n\n    LIBEVENT_THREAD *thr = c->thread;\n    lua_State *L = thr->L;\n\n    if (c->proxy_rctx == NULL) {\n        complete_nread_ascii(c);\n        return;\n    }\n\n    conn_set_state(c, conn_new_cmd);\n\n    assert(c->proxy_rctx);\n    mcp_rcontext_t *rctx = c->proxy_rctx;\n    mcp_request_t *rq = rctx->request;\n\n    if (strncmp((char *)c->item + rq->pr.vlen - 2, \"\\r\\n\", 2) != 0) {\n        lua_settop(L, 0); // clear anything remaining on the main thread.\n        // FIXME (v2): need to set noreply false if mset_res, but that's kind\n        // of a weird hack to begin with. Evaluate how to best do that here.\n        out_string(c, \"CLIENT_ERROR bad data chunk\");\n        rctx->pending_reqs--;\n        mcp_funcgen_return_rctx(rctx);\n        return;\n    }\n\n    // We move ownership of the c->item buffer from the connection to the\n    // request object here. Else we can double free if the conn closes while\n    // inside nread.\n    rq->pr.vbuf = c->item;\n    c->item = NULL;\n    c->item_malloced = false;\n    c->proxy_rctx = NULL;\n    pthread_mutex_lock(&thr->proxy_limit_lock);\n    thr->proxy_buffer_memory_used += rq->pr.vlen;\n    pthread_mutex_unlock(&thr->proxy_limit_lock);\n\n    proxy_run_rcontext(rctx);\n    mcp_funcgen_return_rctx(rctx);\n\n    lua_settop(L, 0); // clear anything remaining on the main thread.\n\n    return;\n}\n\n// Simple error wrapper for common failures.\n// lua_error() is a jump so this function never returns\n// for clarity add a 'return' after calls to this.\nvoid proxy_lua_error(lua_State *L, const char *s) {\n    lua_pushstring(L, s);\n    lua_error(L);\n}\n\n// Need a custom function so we can prefix lua strings easily.\nvoid proxy_out_errstring(mc_resp *resp, char *type, const char *str) {\n    size_t len;\n    size_t prefix_len = strlen(type);\n\n    assert(resp != NULL);\n\n    resp_reset(resp);\n    // avoid noreply since we're throwing important errors.\n\n    // Fill response object with static string.\n    len = strlen(str);\n    if ((len + prefix_len + 2) > WRITE_BUFFER_SIZE) {\n        /* ought to be always enough. just fail for simplicity */\n        str = \"SERVER_ERROR output line too long\";\n        len = strlen(str);\n    }\n\n    char *w = resp->wbuf;\n    memcpy(w, type, prefix_len);\n    w += prefix_len;\n\n    memcpy(w, str, len);\n    w += len;\n\n    memcpy(w, \"\\r\\n\", 2);\n    resp_add_iov(resp, resp->wbuf, len + prefix_len + 2);\n    return;\n}\n\n// NOTE: See notes in mcp_queue_io; the secondary problem with setting the\n// noreply mode from the response object is that the proxy can return strings\n// manually, so we have no way to obey what the original request wanted in\n// that case.\nstatic void _set_noreply_mode(mc_resp *resp, mcp_resp_t *r) {\n    switch (r->mode) {\n        case RESP_MODE_NORMAL:\n            break;\n        case RESP_MODE_NOREPLY:\n            // ascii noreply only threw egregious errors to client\n            if (r->status == MCMC_OK) {\n                resp->skip = true;\n            }\n            break;\n        case RESP_MODE_METAQUIET:\n            if (r->resp.code == MCMC_CODE_END) {\n                resp->skip = true;\n            } else if (r->cmd != CMD_MG && r->resp.code == MCMC_CODE_OK) {\n                // FIXME (v2): mcmc's parser needs to help us out a bit more\n                // here.\n                // This is a broken case in the protocol though; quiet mode\n                // ignores HD for mutations but not get.\n                resp->skip = true;\n            }\n            break;\n        default:\n            assert(1 == 0);\n    }\n}\n\nstatic void _proxy_run_rcontext_queues(mcp_rcontext_t *rctx) {\n    for (int x = 0; x < rctx->fgen->max_queues; x++) {\n        mcp_run_rcontext_handle(rctx, x);\n    }\n}\n\nstatic void _proxy_run_tresp_to_resp(mc_resp *tresp, mc_resp *resp) {\n    // The internal cache handler has created a resp we want to swap in\n    // here. It would be fastest to swap *resp's position in the\n    // link but if the set is deep this would instead be slow, so\n    // we copy over details from this temporary resp instead.\n\n    // So far all we fill is the wbuf and some iov's? so just copy\n    // that + the UDP info?\n    memcpy(resp->wbuf, tresp->wbuf, tresp->iov[0].iov_len);\n    resp->tosend = 0;\n    for (int x = 0; x < tresp->iovcnt; x++) {\n        resp->iov[x] = tresp->iov[x];\n        resp->tosend += tresp->iov[x].iov_len;\n    }\n    // resp->iov[x].iov_base needs to be updated if it's\n    // pointing within its wbuf.\n    // FIXME: This is too fragile. we need to be able to\n    // inherit details and swap resp objects around.\n    if (tresp->iov[0].iov_base == tresp->wbuf) {\n        resp->iov[0].iov_base = resp->wbuf;\n    }\n    resp->iovcnt = tresp->iovcnt;\n    resp->chunked_total = tresp->chunked_total;\n    resp->chunked_data_iov = tresp->chunked_data_iov;\n    // copy UDP headers...\n    resp->request_id = tresp->request_id;\n    resp->udp_sequence = tresp->udp_sequence;\n    resp->udp_total = tresp->udp_total;\n    resp->request_addr = tresp->request_addr;\n    resp->request_addr_size = tresp->request_addr_size;\n    resp->item = tresp->item; // will be populated if not extstore fetch\n    tresp->item = NULL; // move ownership of the item to resp from tresp\n    resp->skip = tresp->skip;\n}\n\n// HACK NOTES:\n// These are self-notes for dormando mostly.\n// The IO queue system does not work well with the proxy, as we need to:\n// - only increment q->count during the submit phase\n//   - .. because a resumed coroutine can queue more data.\n//   - and we will never hit q->count == 0\n//   - .. and then never resume the main connection. (conn_worker_readd)\n//   - which will never submit the new sub-requests\n// - need to only increment q->count once per stack of requests coming from a\n//   resp.\n//\n// For RQU backed requests (new API) there isn't an easy place to test for\n// \"the first request\", because:\n// - The connection queue is a stack of _all_ requests pending on this\n// connection, and many requests can arrive in one batch.\n//   - Thus we cannot simply check if there are items in the queue\n// - RQU's can be recursive, so we have to loop back to the parent to check to\n//   see if we're the first queue or not.\n//\n// This hack workaround exists so I can fix the IO queue subsystem as a change\n// independent of the RCTX change, as the IO queue touches everything and\n// scares the shit out of me. It's much easier to make changes to it in\n// isolation, when all existing systems are currently working and testable.\n//\n// Description of the hack:\n// - in mcp_queue_io: roll up rctx to parent, and if we are the first IO to queue\n// since the rcontext started, set p->qcounr_incr = true\n// Later in submit_cb:\n// - q->count++ if p->qcount_incr.\n//\n// Finally, in proxy_return_rqu_cb:\n// - If parent completed non-yielded work, q->count-- to allow conn\n// resumption.\n// - At bottom of rqu_cb(), flush any IO queues for the connection in case we\n// re-queued work.\nint proxy_run_rcontext(mcp_rcontext_t *rctx) {\n    int nresults = 0;\n    lua_State *Lc = rctx->Lc;\n    assert(rctx->lua_narg != 0);\n    int cores = lua_resume(Lc, NULL, rctx->lua_narg, &nresults);\n    rctx->lua_narg = 1; // reset to default since not-default is uncommon.\n    size_t rlen = 0;\n    mc_resp *resp = rctx->resp;\n\n    if (cores == LUA_OK) {\n        // don't touch the result object if we were a sub-context.\n        if (!rctx->parent) {\n            int type = lua_type(Lc, 1);\n            mcp_resp_t *r = NULL;\n            P_DEBUG(\"%s: coroutine completed. return type: %d\\n\", __func__, type);\n            if (type == LUA_TUSERDATA && (r = luaL_testudata(Lc, 1, \"mcp.response\")) != NULL) {\n                _set_noreply_mode(resp, r);\n                if (r->status != MCMC_OK && r->resp.type != MCMC_RESP_ERRMSG) {\n                    proxy_out_errstring(resp, PROXY_SERVER_ERROR, \"backend failure\");\n                } else if (r->cresp) {\n                    mc_resp *tresp = r->cresp;\n\n                    _proxy_run_tresp_to_resp(tresp, resp);\n                    // we let the mcp_resp gc handler free up tresp and any\n                    // associated io_pending's of its own later.\n                } else if (r->buf) {\n                    // response set from C.\n                    resp->write_and_free = r->buf;\n                    resp_add_iov(resp, r->buf, r->blen);\n                    // stash the length to later remove from memory tracking\n                    resp->wbytes = r->blen + r->extra;\n                    resp->proxy_res = true;\n                    r->buf = NULL;\n                } else {\n                    // Empty response: used for ascii multiget emulation.\n                }\n\n            } else if (type == LUA_TSTRING) {\n                // response is a raw string from lua.\n                const char *s = lua_tolstring(Lc, 1, &rlen);\n                size_t l = rlen > WRITE_BUFFER_SIZE ? WRITE_BUFFER_SIZE : rlen;\n                memcpy(resp->wbuf, s, l);\n                resp_add_iov(resp, resp->wbuf, l);\n                lua_pop(Lc, 1);\n            } else {\n                proxy_out_errstring(resp, PROXY_SERVER_ERROR, \"bad response\");\n            }\n        }\n\n        rctx->pending_reqs--;\n    } else if (cores == LUA_YIELD) {\n        int yield_type = lua_tointeger(Lc, -1);\n        P_DEBUG(\"%s: coroutine yielded. return type: %d\\n\", __func__, yield_type);\n        assert(yield_type != 0);\n        lua_pop(Lc, 1);\n\n        int res = 0;\n        switch (yield_type) {\n            case MCP_YIELD_INTERNAL:\n                // stack should be: rq, res\n                if (rctx->parent) {\n                    LOGGER_LOG(NULL, LOG_PROXYEVENTS, LOGGER_PROXY_ERROR, NULL, \"cannot run mcp.internal from a sub request\");\n                    rctx->pending_reqs--;\n                    return LUA_ERRRUN;\n                } else {\n                    res = mcplib_internal_run(rctx);\n                    if (res == 0) {\n                        // stack should still be: rq, res\n                        // TODO: turn this function into a for loop that re-runs on\n                        // certain status codes, to avoid recursive depth here.\n                        // or maybe... a goto? :P\n                        proxy_run_rcontext(rctx);\n                    } else if (res > 0) {\n                        // internal run queued for extstore.\n                    } else {\n                        assert(res < 0);\n                        proxy_out_errstring(resp, PROXY_SERVER_ERROR, \"bad request\");\n                    }\n                }\n                break;\n            case MCP_YIELD_WAITCOND:\n            case MCP_YIELD_WAITHANDLE:\n                // Even if we're in WAITHANDLE, we want to dispatch any queued\n                // requests, so we still need to iterate the full set of qslots.\n                _proxy_run_rcontext_queues(rctx);\n                break;\n            case MCP_YIELD_SLEEP:\n                // Pause coroutine and do nothing. Alarm will resume.\n                break;\n            default:\n                abort();\n        }\n\n    } else {\n        // Log the error where it happens, then the parent will handle a\n        // result object normally.\n        P_DEBUG(\"%s: Failed to run coroutine: %s\\n\", __func__, lua_tostring(Lc, -1));\n        LOGGER_LOG(NULL, LOG_PROXYEVENTS, LOGGER_PROXY_ERROR, NULL, lua_tostring(Lc, -1));\n        if (!rctx->parent) {\n            proxy_out_errstring(resp, PROXY_SERVER_ERROR, \"lua failure\");\n        }\n        rctx->pending_reqs--;\n    }\n\n    return cores;\n}\n\n// basically any data before the first key.\n// max is like 15ish plus spaces. we can be more strict about how many spaces\n// to expect because any client spamming space is being deliberately stupid\n// anyway.\n#define MAX_CMD_PREFIX 20\n\nstatic void proxy_process_command(conn *c, char *command, size_t cmdlen, bool multiget) {\n    assert(c != NULL);\n    LIBEVENT_THREAD *thr = c->thread;\n    struct proxy_hook *hooks = thr->proxy_hooks;\n    lua_State *L = thr->L;\n    proxy_ctx_t *ctx = thr->proxy_ctx;\n    mcp_parser_t pr = {0};\n\n    // Avoid doing resp_start() here, instead do it a bit later or as-needed.\n    // This allows us to hop over to the internal text protocol parser, which\n    // also calls resp_start().\n    // Tighter integration later should obviate the need for this, it is not a\n    // permanent solution.\n    int ret = process_request(&pr, command, cmdlen);\n    if (ret != 0) {\n        WSTAT_INCR(c->thread, proxy_conn_errors, 1);\n        if (!resp_start(c)) {\n            conn_set_state(c, conn_closing);\n            return;\n        }\n        proxy_out_errstring(c->resp, PROXY_CLIENT_ERROR, \"parsing request\");\n        if (ret == -2) {\n            // Kill connection on more critical parse failure.\n            conn_set_state(c, conn_closing);\n        }\n        return;\n    }\n\n    struct proxy_hook *hook = &hooks[pr.command];\n    struct proxy_hook_ref hook_ref = hook->ref;\n    // if client came from a tagged listener, scan for a more specific hook.\n    // TODO: (v2) avoiding a hash table lookup here, but maybe some other\n    // datastructure would suffice. for 4-8 tags this is perfectly fast.\n    if (c->tag && hook->tagged) {\n        struct proxy_hook_tagged *pht = hook->tagged;\n        while (pht->ref.lua_ref) {\n            if (c->tag == pht->tag) {\n                hook_ref = pht->ref;\n                break;\n            }\n            pht++;\n        }\n    }\n\n    if (!hook_ref.lua_ref) {\n        // need to pass our command string into the internal handler.\n        // to minimize the code change, this means allowing it to tokenize the\n        // full command. The proxy's indirect parser should be built out to\n        // become common code for both proxy and ascii handlers.\n        // For now this means we have to null-terminate the command string,\n        // then call into text protocol handler.\n        // FIXME (v2): use a ptr or something; don't like this code.\n        if (cmdlen > 1 && command[cmdlen-2] == '\\r') {\n            command[cmdlen-2] = '\\0';\n        } else {\n            command[cmdlen-1] = '\\0';\n        }\n        // lets nread_proxy know we're in ascii mode.\n        c->proxy_rctx = NULL;\n        process_command_ascii(c, command);\n        return;\n    }\n\n    // If ascii multiget, we turn this into a self-calling loop :(\n    // create new request with next key, call this func again, then advance\n    // original string.\n    // might be better to split this function; the below bits turn into a\n    // function call, then we don't re-process the above bits in the same way?\n    // The way this is detected/passed on is very fragile.\n    if (!multiget && pr.cmd_type == CMD_TYPE_GET && pr.has_space) {\n        uint32_t keyoff = pr.tokens[pr.keytoken];\n        while (pr.klen != 0) {\n            char temp[KEY_MAX_LENGTH + MAX_CMD_PREFIX + 30];\n            char *cur = temp;\n            // Core daemon can abort the entire command if one key is bad, but\n            // we cannot from the proxy. Instead we have to inject errors into\n            // the stream. This should, thankfully, be rare at least.\n            if (pr.tokens[pr.keytoken] > MAX_CMD_PREFIX) {\n                if (!resp_start(c)) {\n                    conn_set_state(c, conn_closing);\n                    return;\n                }\n                proxy_out_errstring(c->resp, PROXY_CLIENT_ERROR, \"malformed request\");\n            } else if (pr.klen > KEY_MAX_LENGTH) {\n                if (!resp_start(c)) {\n                    conn_set_state(c, conn_closing);\n                    return;\n                }\n                proxy_out_errstring(c->resp, PROXY_CLIENT_ERROR, \"key too long\");\n            } else {\n                // copy original request up until the original key token.\n                memcpy(cur, pr.request, pr.tokens[pr.keytoken]);\n                cur += pr.tokens[pr.keytoken];\n\n                // now copy in our \"current\" key.\n                memcpy(cur, &pr.request[keyoff], pr.klen);\n                cur += pr.klen;\n\n                memcpy(cur, \"\\r\\n\", 2);\n                cur += 2;\n\n                *cur = '\\0';\n                P_DEBUG(\"%s: new multiget sub request: %s [%u/%u]\\n\", __func__, temp, keyoff, pr.klen);\n                proxy_process_command(c, temp, cur - temp, PROCESS_MULTIGET);\n            }\n\n            // now advance to the next key.\n            keyoff = _process_request_next_key(&pr);\n        }\n\n        if (!resp_start(c)) {\n            conn_set_state(c, conn_closing);\n            return;\n        }\n\n        // The above recursions should have created c->resp's in dispatch\n        // order.\n        // So now we add another one at the end to create the capping END\n        // string.\n        memcpy(c->resp->wbuf, ENDSTR, ENDLEN);\n        resp_add_iov(c->resp, c->resp->wbuf, ENDLEN);\n\n        return;\n    }\n\n    // We test the command length all the way down here because multigets can\n    // be very long, and they're chopped up by now.\n    if (cmdlen >= MCP_REQUEST_MAXLEN) {\n        WSTAT_INCR(c->thread, proxy_conn_errors, 1);\n        if (!resp_start(c)) {\n            conn_set_state(c, conn_closing);\n            return;\n        }\n        proxy_out_errstring(c->resp, PROXY_CLIENT_ERROR, \"request too long\");\n        conn_set_state(c, conn_closing);\n        return;\n    }\n\n    if (!resp_start(c)) {\n        conn_set_state(c, conn_closing);\n        return;\n    }\n\n    // Count requests handled by proxy vs local.\n    // Also batch the counts down this far so we can lock once for the active\n    // counter instead of twice.\n    struct proxy_int_stats *istats = c->thread->proxy_int_stats;\n    uint64_t active_reqs = 0;\n    WSTAT_L(c->thread);\n    istats->counters[pr.command]++;\n    c->thread->stats.proxy_conn_requests++;\n    active_reqs = c->thread->stats.proxy_req_active;\n    WSTAT_UL(c->thread);\n\n    if (active_reqs >= ctx->active_req_limit) {\n        proxy_out_errstring(c->resp, PROXY_SERVER_ERROR, \"active request limit reached\");\n        if (pr.vlen != 0) {\n            c->sbytes = pr.vlen;\n            conn_set_state(c, conn_swallow);\n        }\n        return;\n    }\n\n    // hook is owned by a function generator.\n    mcp_rcontext_t *rctx = mcp_funcgen_start(L, hook_ref.ctx, &pr);\n    if (rctx == NULL) {\n        proxy_out_errstring(c->resp, PROXY_SERVER_ERROR, \"lua start failure\");\n        if (pr.vlen != 0) {\n            c->sbytes = pr.vlen;\n            conn_set_state(c, conn_swallow);\n        }\n        return;\n    }\n\n    mcp_set_request(&pr, rctx->request, command, cmdlen);\n    rctx->request->ascii_multiget = multiget;\n    rctx->c = c;\n    rctx->conn_fd = c->sfd;\n    rctx->pending_reqs++; // seed counter with the \"main\" request\n    // remember the top level mc_resp, because further requests on the\n    // same connection will replace c->resp.\n    rctx->resp = c->resp;\n\n    // for the very first call we need to place:\n    // - rctx->function_ref + rctx->request_ref\n    // I _think_ here is the right place to do that?\n    lua_rawgeti(rctx->Lc, LUA_REGISTRYINDEX, rctx->function_ref);\n    lua_rawgeti(rctx->Lc, LUA_REGISTRYINDEX, rctx->request_ref);\n\n    if (pr.vlen != 0) {\n        c->item = NULL;\n        // Need to add the used memory later due to needing an extra callback\n        // handler on error during nread.\n        bool oom = proxy_bufmem_checkadd(c->thread, 0);\n\n        // relying on temporary malloc's not having fragmentation\n        if (!oom) {\n            c->item = malloc(pr.vlen);\n        }\n        if (c->item == NULL) {\n            // return the RCTX\n            rctx->pending_reqs--;\n            mcp_funcgen_return_rctx(rctx);\n            // normal cleanup\n            lua_settop(L, 0);\n            proxy_out_errstring(c->resp, PROXY_SERVER_ERROR, \"out of memory\");\n            c->sbytes = pr.vlen;\n            conn_set_state(c, conn_swallow);\n            return;\n        }\n        c->item_malloced = true;\n        c->ritem = c->item;\n        c->rlbytes = pr.vlen;\n\n        // remember the request context for later.\n        c->proxy_rctx = rctx;\n\n        conn_set_state(c, conn_nread);\n        return;\n    }\n\n    proxy_run_rcontext(rctx);\n    mcp_funcgen_return_rctx(rctx);\n\n    lua_settop(L, 0); // clear any junk from the main thread.\n}\n\nmcp_resp_t *mcp_prep_bare_resobj(lua_State *L, LIBEVENT_THREAD *t) {\n    mcp_resp_t *r = lua_newuserdatauv(L, sizeof(mcp_resp_t), 0);\n    // FIXME (v2): is this memset still necessary? I was using it for\n    // debugging.\n    memset(r, 0, sizeof(mcp_resp_t));\n    r->thread = t;\n    assert(r->thread != NULL);\n    gettimeofday(&r->start, NULL);\n\n    luaL_getmetatable(L, \"mcp.response\");\n    lua_setmetatable(L, -2);\n\n    return r;\n}\n\nvoid mcp_set_resobj(mcp_resp_t *r, mcp_request_t *rq, mcp_backend_t *be, LIBEVENT_THREAD *t) {\n    memset(r, 0, sizeof(mcp_resp_t));\n    r->buf = NULL;\n    r->blen = 0;\n    r->thread = t;\n    assert(r->thread != NULL);\n    gettimeofday(&r->start, NULL);\n    // Set noreply mode.\n    // TODO (v2): the response \"inherits\" the request's noreply mode, which isn't\n    // strictly correct; we should inherit based on the request that spawned\n    // the coroutine but the structure doesn't allow that yet.\n    // Should also be able to settle this exact mode from the parser so we\n    // don't have to re-branch here.\n    if (rq->pr.noreply) {\n        if (rq->pr.cmd_type == CMD_TYPE_META) {\n            r->mode = RESP_MODE_METAQUIET;\n            for (int x = 2; x < rq->pr.ntokens; x++) {\n                if (rq->request[rq->pr.tokens[x]] == 'q') {\n                    rq->request[rq->pr.tokens[x]] = ' ';\n                }\n            }\n        } else {\n            r->mode = RESP_MODE_NOREPLY;\n            rq->request[rq->pr.reqlen - 3] = 'Y';\n        }\n    } else {\n        r->mode = RESP_MODE_NORMAL;\n    }\n\n    r->cmd = rq->pr.command;\n\n    strncpy(r->be_name, be->name, MAX_NAMELEN+1);\n    strncpy(r->be_port, be->port, MAX_PORTLEN+1);\n\n}\n\nmcp_resp_t *mcp_prep_resobj(lua_State *L, mcp_request_t *rq, mcp_backend_t *be, LIBEVENT_THREAD *t) {\n    mcp_resp_t *r = lua_newuserdatauv(L, sizeof(mcp_resp_t), 0);\n    mcp_set_resobj(r, rq, be, t);\n\n    luaL_getmetatable(L, \"mcp.response\");\n    lua_setmetatable(L, -2);\n\n    return r;\n}\n\nvoid mcp_resp_set_elapsed(mcp_resp_t *r) {\n    struct timeval end;\n    // stamp the elapsed time into the response object.\n    gettimeofday(&end, NULL);\n    r->elapsed = (end.tv_sec - r->start.tv_sec) * 1000000 +\n        (end.tv_usec - r->start.tv_usec);\n}\n\n// Used for any cases where we're queueing requests to the IO subsystem.\n// NOTE: it's not currently possible to limit the memory used by the IO\n// object cache. So this check is redundant, and any callers may proceed\n// as though it is successful.\nio_pending_proxy_t *mcp_queue_rctx_io(mcp_rcontext_t *rctx, mcp_request_t *rq, mcp_backend_t *be, mcp_resp_t *r) {\n    conn *c = rctx->c;\n    io_queue_t *q = conn_io_queue_get(c, IO_QUEUE_PROXY);\n    io_pending_proxy_t *p = do_cache_alloc(c->thread->io_cache);\n    if (p == NULL) {\n        WSTAT_INCR(c->thread, proxy_conn_oom, 1);\n        proxy_lua_error(rctx->Lc, \"out of memory allocating from IO cache\");\n        // NOTE: the error call above jumps to an error handler, so this does\n        // not actually return.\n        return NULL;\n    }\n\n    // this is a re-cast structure, so assert that we never outsize it.\n    assert(sizeof(io_pending_t) >= sizeof(io_pending_proxy_t));\n    memset(p, 0, sizeof(io_pending_proxy_t));\n    // set up back references.\n    p->io_queue_type = IO_QUEUE_PROXY;\n    p->thread = c->thread;\n    p->c = c;\n    p->client_resp = r;\n    p->flushed = false;\n    p->return_cb = NULL;\n    p->finalize_cb = proxy_finalize_rctx_cb;\n\n    // pass along the request context for resumption.\n    p->rctx = rctx;\n\n    if (rq) {\n        p->ascii_multiget = rq->ascii_multiget;\n        // The direct backend object. Lc is holding the reference in the stack\n        p->backend = be;\n\n        mcp_request_attach(rq, p);\n    }\n\n    // HACK\n    // find the parent rctx\n    while (rctx->parent) {\n        rctx = rctx->parent;\n    }\n    // Hack to enforce the first iop increments client IO queue counter.\n    if (!rctx->first_queue) {\n        rctx->first_queue = true;\n        p->qcount_incr = true;\n    }\n    // END HACK\n\n    // link into the batch chain.\n    p->next = q->stack_ctx;\n    q->stack_ctx = p;\n    P_DEBUG(\"%s: queued\\n\", __func__);\n\n    return p;\n}\n\n// DO NOT call this method frequently! globally locked!\nvoid mcp_sharedvm_delta(proxy_ctx_t *ctx, int tidx, const char *name, int delta) {\n    lua_State *L = ctx->proxy_sharedvm;\n    pthread_mutex_lock(&ctx->sharedvm_lock);\n\n    if (lua_getfield(L, tidx, name) == LUA_TNIL) {\n        lua_pop(L, 1);\n        lua_pushinteger(L, delta);\n        lua_setfield(L, tidx, name);\n    } else {\n        lua_pushinteger(L, delta);\n        lua_arith(L, LUA_OPADD);\n        lua_setfield(L, tidx, name);\n    }\n\n    pthread_mutex_unlock(&ctx->sharedvm_lock);\n}\n\nvoid mcp_sharedvm_remove(proxy_ctx_t *ctx, int tidx, const char *name) {\n    lua_State *L = ctx->proxy_sharedvm;\n    pthread_mutex_lock(&ctx->sharedvm_lock);\n\n    lua_pushnil(L);\n    lua_setfield(L, tidx, name);\n\n    pthread_mutex_unlock(&ctx->sharedvm_lock);\n}\n\n// Global object support code.\n// Global objects are created in the configuration VM, and referenced in\n// worker VMs via proxy objects that refer back to memory in the\n// configuration VM.\n// We manage reference counts: once all remote proxy objects are collected, we\n// signal the config thread to remove a final reference and collect garbage to\n// remove the global object.\n\nstatic void mcp_gobj_enqueue(proxy_ctx_t *ctx, struct mcp_globalobj_s *g) {\n    pthread_mutex_lock(&ctx->manager_lock);\n    STAILQ_INSERT_TAIL(&ctx->manager_head, g, next);\n    pthread_cond_signal(&ctx->manager_cond);\n    pthread_mutex_unlock(&ctx->manager_lock);\n}\n\n// References the object, initializing the self-reference if necessary.\n// Call from config thread, with global object on top of stack.\nvoid mcp_gobj_ref(lua_State *L, struct mcp_globalobj_s *g) {\n    pthread_mutex_lock(&g->lock);\n    if (g->self_ref == 0) {\n        // Initialization requires a small dance:\n        // - store a negative of our ref, increase refcount an extra time\n        // - then link and signal the manager thread as though we were GC'ing\n        // the object.\n        // - the manager thread will later acknowledge the initialization of\n        // this global object and negate the self_ref again\n        // - this prevents an unused proxy object from causing the global\n        // object to be reaped early while we are still copying it to worker\n        // threads, as the manager thread will block waiting for the config\n        // thread to finish its reload work.\n        g->self_ref = -luaL_ref(L, LUA_REGISTRYINDEX);\n        g->refcount++;\n        proxy_ctx_t *ctx = PROXY_GET_CTX(L);\n        mcp_gobj_enqueue(ctx, g);\n    } else {\n        lua_pop(L, 1); // drop the reference we didn't end up using.\n    }\n    g->refcount++;\n    pthread_mutex_unlock(&g->lock);\n}\n\nvoid mcp_gobj_unref(proxy_ctx_t *ctx, struct mcp_globalobj_s *g) {\n    pthread_mutex_lock(&g->lock);\n    g->refcount--;\n    if (g->refcount == 0) {\n        mcp_gobj_enqueue(ctx, g);\n    }\n    pthread_mutex_unlock(&g->lock);\n}\n\nvoid mcp_gobj_finalize(struct mcp_globalobj_s *g) {\n    pthread_mutex_destroy(&g->lock);\n}\n\nstatic void *mcp_profile_alloc(void *ud, void *ptr, size_t osize,\n                                            size_t nsize) {\n    struct mcp_memprofile *prof = ud;\n    struct timespec now;\n    clock_gettime(CLOCK_MONOTONIC, &now);\n    enum mcp_memprofile_types t = mcp_memp_free;\n    if (ptr == NULL) {\n        switch (osize) {\n            case LUA_TSTRING:\n                t = mcp_memp_string;\n                //fprintf(stderr, \"alloc string: %ld\\n\", nsize);\n                break;\n            case LUA_TTABLE:\n                t = mcp_memp_table;\n                //fprintf(stderr, \"alloc table: %ld\\n\", nsize);\n                break;\n            case LUA_TFUNCTION:\n                t = mcp_memp_func;\n                //fprintf(stderr, \"alloc func: %ld\\n\", nsize);\n                break;\n            case LUA_TUSERDATA:\n                t = mcp_memp_userdata;\n                //fprintf(stderr, \"alloc userdata: %ld\\n\", nsize);\n                break;\n            case LUA_TTHREAD:\n                t = mcp_memp_thread;\n                //fprintf(stderr, \"alloc thread: %ld\\n\", nsize);\n                break;\n            default:\n                t = mcp_memp_default;\n                //fprintf(stderr, \"alloc osize: %ld nsize: %ld\\n\", osize, nsize);\n        }\n        prof->allocs[t]++;\n        prof->alloc_bytes[t] += nsize;\n    } else {\n        if (nsize != 0) {\n            prof->allocs[mcp_memp_realloc]++;\n            prof->alloc_bytes[mcp_memp_realloc] += nsize;\n        } else {\n            prof->allocs[mcp_memp_free]++;\n            prof->alloc_bytes[mcp_memp_free] += osize;\n        }\n        //fprintf(stderr, \"realloc: osize: %ld nsize: %ld\\n\", osize, nsize);\n    }\n\n    if (now.tv_sec != prof->last_status.tv_sec) {\n        prof->last_status.tv_sec = now.tv_sec;\n        fprintf(stderr, \"MEMPROF[%d]:\\tstring[%llu][%llu] table[%llu][%llu] func[%llu][%llu] udata[%llu][%llu] thr[%llu][%llu] def[%llu][%llu] realloc[%llu][%llu] free[%llu][%llu]\\n\",\n                prof->id,\n                (unsigned long long)prof->allocs[1],\n                (unsigned long long)prof->alloc_bytes[1],\n                (unsigned long long)prof->allocs[2],\n                (unsigned long long)prof->alloc_bytes[2],\n                (unsigned long long)prof->allocs[3],\n                (unsigned long long)prof->alloc_bytes[3],\n                (unsigned long long)prof->allocs[4],\n                (unsigned long long)prof->alloc_bytes[4],\n                (unsigned long long)prof->allocs[5],\n                (unsigned long long)prof->alloc_bytes[5],\n                (unsigned long long)prof->allocs[6],\n                (unsigned long long)prof->alloc_bytes[6],\n                (unsigned long long)prof->allocs[7],\n                (unsigned long long)prof->alloc_bytes[7],\n                (unsigned long long)prof->allocs[0],\n                (unsigned long long)prof->alloc_bytes[0]);\n        for (int x = 0; x < 8; x++) {\n            prof->allocs[x] = 0;\n            prof->alloc_bytes[x] = 0;\n        }\n    }\n\n    if (nsize == 0) {\n        free(ptr);\n        return NULL;\n    } else {\n        return realloc(ptr, nsize);\n    }\n}\n\n// Common lua debug command.\n__attribute__((unused)) void dump_stack(lua_State *L, const char *msg) {\n    int top = lua_gettop(L);\n    int i = 1;\n    fprintf(stderr, \"--TOP OF STACK [%d] | %s\\n\", top, msg);\n    for (; i < top + 1; i++) {\n        int type = lua_type(L, i);\n        void *udata = NULL;\n        // lets find the metatable of this userdata to identify it.\n        if (lua_getmetatable(L, i) != 0) {\n            lua_pushstring(L, \"__name\");\n            if (lua_rawget(L, -2) != LUA_TNIL) {\n                if (type == LUA_TUSERDATA) {\n                    udata = lua_touserdata(L, i);\n                }\n                fprintf(stderr, \"--|%d| [%s] (%s) [ptr: %p]\\n\", i, lua_typename(L, type), lua_tostring(L, -1), udata);\n                lua_pop(L, 2);\n                continue;\n            }\n            lua_pop(L, 2);\n        }\n        if (type == LUA_TSTRING) {\n            fprintf(stderr, \"--|%d| [%s] | %s\\n\", i, lua_typename(L, type), lua_tostring(L, i));\n        } else {\n            if (type == LUA_TUSERDATA) {\n                udata = lua_touserdata(L, i);\n            }\n            fprintf(stderr, \"--|%d| [%s] [ptr: %p]\\n\", i, lua_typename(L, type), udata);\n        }\n    }\n    fprintf(stderr, \"-----------------\\n\");\n}\n\n// Not very pretty, but helped.\n// Nice to haves:\n// - summarize counts for each metatable (easy enough to do from logging)\n// - use a less noisy stack dump instead of calling dump_stack()\n__attribute__((unused)) void dump_registry(lua_State *L, const char *msg) {\n    int ref_size = lua_rawlen(L, LUA_REGISTRYINDEX);\n    fprintf(stderr, \"--LUA REGISTRY TABLE [%d] | %s\\n\", ref_size, msg);\n    // walk registry\n    int ridx = lua_absindex(L, LUA_REGISTRYINDEX);\n    int udata = 0;\n    int number = 0;\n    int string = 0;\n    int function = 0;\n    int table = 0;\n    lua_pushnil(L);\n    while (lua_next(L, ridx) != 0) {\n        dump_stack(L, \"===registry entry===\");\n        int type = lua_type(L, -1);\n        if (type == LUA_TUSERDATA) {\n            udata++;\n        } else if (type == LUA_TNUMBER) {\n            number++;\n        } else if (type == LUA_TSTRING) {\n            string++;\n        } else if (type == LUA_TFUNCTION) {\n            function++;\n        } else if (type == LUA_TTABLE) {\n            table++;\n        }\n        lua_pop(L, 1); // drop value\n    }\n    fprintf(stderr, \"SUMMARY:\\n\\n\");\n    fprintf(stderr, \"### UDATA\\t[%d]\\n\", udata);\n    fprintf(stderr, \"### NUMBER\\t[%d]\\n\", number);\n    fprintf(stderr, \"### STRING\\t[%d]\\n\", string);\n    fprintf(stderr, \"### FUNCTION\\t[%d]\\n\", function );\n    fprintf(stderr, \"### TABLE\\t[%d]\\n\", table);\n    fprintf(stderr, \"-----------------\\n\");\n}\n\n// Searches for a function generator with a specific name attached.\n// Adding breakpoints on the print lines lets you inspect the fgen and its\n// slots.\n__attribute__((unused)) void dump_funcgen(lua_State *L, const char *name, const char *msg) {\n    int ref_size = lua_rawlen(L, LUA_REGISTRYINDEX);\n    fprintf(stderr, \"--LUA FUNCGEN FINDER [%d] | %s\\n\", ref_size, msg);\n    // walk registry\n    int ridx = lua_absindex(L, LUA_REGISTRYINDEX);\n    lua_pushnil(L);\n    while (lua_next(L, ridx) != 0) {\n        int type = lua_type(L, -1);\n        if (type == LUA_TUSERDATA) {\n            mcp_funcgen_t *f = luaL_testudata(L, -1, \"mcp.funcgen\");\n            if (f != NULL && strcmp(name, f->name) == 0) {\n                fprintf(stderr, \"===found funcgen [%s] [%p]===\\n\", f->name, (void *)f);\n                lua_getiuservalue(L, -1, 1);\n                int tidx = lua_absindex(L, -1);\n                lua_pushnil(L);\n                while (lua_next(L, tidx) != 0) {\n                    mcp_rcontext_t *rctx = lua_touserdata(L, -1);\n                    if (rctx != NULL) {\n                        fprintf(stderr, \"-- slot: [%p]\\n\", (void *)rctx);\n                    }\n                    lua_pop(L, 1); // drop value\n                }\n                lua_pop(L, 1); // drop slot table\n            }\n        }\n        lua_pop(L, 1); // drop value\n    }\n    fprintf(stderr, \"-----------------\\n\");\n}\n\nstatic void dump_pool_info(mcp_pool_t *p) {\n    fprintf(stderr, \"--pool: [%s] size: [%d] be_total: [%d] rc: [%d] io: [%d]\\n\",\n            p->beprefix, p->pool_size, p->pool_be_total, p->g.refcount, p->use_iothread);\n\n    for (int x = 0; x < p->pool_be_total; x++) {\n        mcp_backend_t *be = p->pool[x].be;\n        // Dumb: pool_be_total is wrong if pool is using iothread. Why?\n        if (be != NULL) {\n            fprintf(stderr, \"  --be[%d] label: [%s] name: [%s] conns: [%d] depth: [%d]\\n\",\n                    x, be->label, be->name, be->conncount, be->depth);\n            for (int i = 0; i < be->conncount; i++) {\n                struct mcp_backendconn_s *bec = &be->be[i];\n                fprintf(stderr, \"    --bec[%d] bad: [%d] failcnt: [%d] depth: [%d] state: [%d] can_write[%d] write_event[%d]\\n\",\n                        i, bec->bad, bec->failed_count, bec->depth, bec->state, bec->can_write, event_pending(&bec->timeout_event, EV_WRITE, NULL));\n            }\n        }\n    }\n    fprintf(stderr, \"=======\\n\");\n}\n\n// Dumps some info about pools.\n// If given the config thread, it should find the main pools\n// If given a worker thread, it will look for the pool proxy objects and find\n// the main pools that way.\n__attribute__((unused)) void dump_pools(lua_State *L, const char *msg) {\n    int ref_size = lua_rawlen(L, LUA_REGISTRYINDEX);\n    fprintf(stderr, \"--LUA POOL DUMPER [%d] | %s\\n\", ref_size, msg);\n    // walk registry\n    int ridx = lua_absindex(L, LUA_REGISTRYINDEX);\n    lua_pushnil(L);\n    while (lua_next(L, ridx) != 0) {\n        int type = lua_type(L, -1);\n        if (type == LUA_TUSERDATA) {\n            mcp_pool_t *p = luaL_testudata(L, -1, \"mcp.pool\");\n            if (p != NULL) {\n                dump_pool_info(p);\n            } else {\n                mcp_pool_proxy_t *pp = luaL_testudata(L, -1, \"mcp.pool_proxy\");\n                if (pp != NULL) {\n                    dump_pool_info(pp->main);\n                }\n            }\n        }\n        lua_pop(L, 1); // drop value\n    }\n    fprintf(stderr, \"-----------------\\n\");\n\n}\n"
        },
        {
          "name": "proto_proxy.h",
          "type": "blob",
          "size": 1.0107421875,
          "content": "#ifndef PROTO_PROXY_H\n#define PROTO_PROXY_H\n\nvoid proxy_stats(void *arg, ADD_STAT add_stats, void *c);\nvoid process_proxy_stats(void *arg, ADD_STAT add_stats, void *c);\nvoid process_proxy_funcstats(void *arg, ADD_STAT add_stats, void *c);\nvoid process_proxy_bestats(void *arg, ADD_STAT add_stats, void *c);\n\n/* proxy mode handlers */\nint try_read_command_proxy(conn *c);\nvoid complete_nread_proxy(conn *c);\nvoid proxy_cleanup_conn(conn *c);\nvoid proxy_thread_init(void *ctx, LIBEVENT_THREAD *thr);\nvoid *proxy_init(bool proxy_uring, bool proxy_memprofile);\n// TODO: need better names or a better interface for these. can be confusing\n// to reason about the order.\nvoid proxy_start_reload(void *arg);\nint proxy_first_confload(void *arg);\nint proxy_load_config(void *arg);\nvoid proxy_worker_reload(void *arg, LIBEVENT_THREAD *thr);\nvoid proxy_gc_poke(LIBEVENT_THREAD *t);\n\nvoid proxy_submit_cb(io_queue_t *q);\nvoid proxy_complete_cb(io_queue_t *q);\n\n/* lua */\nint proxy_register_libs(void *ctx, LIBEVENT_THREAD *t, void *state);\n\n#endif\n"
        },
        {
          "name": "proto_text.c",
          "type": "blob",
          "size": 100.228515625,
          "content": "/* -*- Mode: C; tab-width: 4; c-basic-offset: 4; indent-tabs-mode: nil -*- */\n/*\n * Functions for handling the text related protocols, original and meta.\n */\n\n#include \"memcached.h\"\n#include \"proto_text.h\"\n// FIXME: only for process_proxy_stats()\n// - some better/different structure for stats subcommands\n// would remove this abstraction leak.\n#include \"proto_proxy.h\"\n#include \"authfile.h\"\n#include \"storage.h\"\n#include \"base64.h\"\n#include \"tls.h\"\n#include <string.h>\n#include <stdlib.h>\n\n#define META_SPACE(p) { \\\n    *p = ' '; \\\n    p++; \\\n}\n\n#define META_CHAR(p, c) { \\\n    *p = ' '; \\\n    *(p+1) = c; \\\n    p += 2; \\\n}\n\n// NOTE: being a little casual with the write buffer.\n// the buffer needs to be sized that the longest possible meta response will\n// fit. Here we allow the key to fill up to half the write buffer, in case\n// something terrible has gone wrong.\n#define META_KEY(p, key, nkey, bin) { \\\n    META_CHAR(p, 'k'); \\\n    if (!bin) { \\\n        memcpy(p, key, nkey); \\\n        p += nkey; \\\n    } else { \\\n        p += base64_encode((unsigned char *) key, nkey, (unsigned char *)p, WRITE_BUFFER_SIZE / 2); \\\n        *p = ' '; \\\n        *(p+1) = 'b'; \\\n        p += 2; \\\n    } \\\n}\n\ntypedef struct token_s {\n    char *value;\n    size_t length;\n} token_t;\n\nstatic void _finalize_mset(conn *c, int nbytes, enum store_item_type ret, uint64_t cas) {\n    mc_resp *resp = c->resp;\n    item *it = c->item;\n    conn_set_state(c, conn_new_cmd);\n\n    // information about the response line has been stashed in wbuf.\n    char *p = resp->wbuf + resp->wbytes;\n    char *end = p; // end of the stashed data portion.\n\n    switch (ret) {\n    case STORED:\n      memcpy(p, \"HD\", 2);\n      // Only place noreply is used for meta cmds is a nominal response.\n      if (c->noreply) {\n          resp->skip = true;\n      }\n      break;\n    case EXISTS:\n      memcpy(p, \"EX\", 2);\n      break;\n    case NOT_FOUND:\n      memcpy(p, \"NF\", 2);\n      break;\n    case NOT_STORED:\n      memcpy(p, \"NS\", 2);\n      break;\n    default:\n      c->noreply = false;\n      out_string(c, \"SERVER_ERROR Unhandled storage type.\");\n      return;\n    }\n    p += 2;\n\n    for (char *fp = resp->wbuf; fp < end; fp++) {\n        switch (*fp) {\n            case 'O':\n                // Copy stashed opaque.\n                META_SPACE(p);\n                while (fp < end && *fp != ' ') {\n                    *p = *fp;\n                    p++;\n                    fp++;\n                }\n                break;\n            case 'k':\n                // Encode the key here instead of earlier to minimize copying.\n                META_KEY(p, ITEM_key(it), it->nkey, (it->it_flags & ITEM_KEY_BINARY));\n                break;\n            case 'c':\n                // We don't have the CAS until this point, which is why we\n                // generate this line so late.\n                META_CHAR(p, 'c');\n                p = itoa_u64(cas, p);\n                break;\n            case 's':\n                // Get final item size, ie from append/prepend\n                META_CHAR(p, 's');\n                // If the size changed during append/prepend\n                if (nbytes != 0) {\n                    p = itoa_u32(nbytes-2, p);\n                } else {\n                    p = itoa_u32(it->nbytes-2, p);\n                }\n                break;\n            default:\n                break;\n        }\n    }\n\n    memcpy(p, \"\\r\\n\", 2);\n    p += 2;\n    // we're offset into wbuf, but good convention to track wbytes.\n    resp->wbytes = p - resp->wbuf;\n    resp_add_iov(resp, end, p - end);\n}\n\n/*\n * we get here after reading the value in set/add/replace commands. The command\n * has been stored in c->cmd, and the item is ready in c->item.\n */\nvoid complete_nread_ascii(conn *c) {\n    assert(c != NULL);\n\n    item *it = c->item;\n    int comm = c->cmd;\n    enum store_item_type ret;\n    bool is_valid = false;\n    int nbytes = 0;\n\n    pthread_mutex_lock(&c->thread->stats.mutex);\n    c->thread->stats.slab_stats[ITEM_clsid(it)].set_cmds++;\n    pthread_mutex_unlock(&c->thread->stats.mutex);\n\n    if ((it->it_flags & ITEM_CHUNKED) == 0) {\n        if (strncmp(ITEM_data(it) + it->nbytes - 2, \"\\r\\n\", 2) == 0) {\n            is_valid = true;\n        }\n    } else {\n        char buf[2];\n        /* should point to the final item chunk */\n        item_chunk *ch = (item_chunk *) c->ritem;\n        assert(ch->used != 0);\n        /* :( We need to look at the last two bytes. This could span two\n         * chunks.\n         */\n        if (ch->used > 1) {\n            buf[0] = ch->data[ch->used - 2];\n            buf[1] = ch->data[ch->used - 1];\n        } else {\n            assert(ch->prev);\n            assert(ch->used == 1);\n            buf[0] = ch->prev->data[ch->prev->used - 1];\n            buf[1] = ch->data[ch->used - 1];\n        }\n        if (strncmp(buf, \"\\r\\n\", 2) == 0) {\n            is_valid = true;\n        } else {\n            assert(1 == 0);\n        }\n    }\n\n    if (!is_valid) {\n        // metaset mode always returns errors.\n        if (c->mset_res) {\n            c->noreply = false;\n        }\n        out_string(c, \"CLIENT_ERROR bad data chunk\");\n    } else {\n      uint64_t cas = 0;\n      c->thread->cur_sfd = c->sfd; // cuddle sfd for logging.\n      ret = store_item(it, comm, c->thread, &nbytes, &cas, c->cas ? c->cas : get_cas_id(), c->set_stale);\n      c->cas = 0;\n\n#ifdef ENABLE_DTRACE\n      switch (c->cmd) {\n      case NREAD_ADD:\n          MEMCACHED_COMMAND_ADD(c->sfd, ITEM_key(it), it->nkey,\n                                (ret == 1) ? it->nbytes : -1, cas);\n          break;\n      case NREAD_REPLACE:\n          MEMCACHED_COMMAND_REPLACE(c->sfd, ITEM_key(it), it->nkey,\n                                    (ret == 1) ? it->nbytes : -1, cas);\n          break;\n      case NREAD_APPEND:\n          MEMCACHED_COMMAND_APPEND(c->sfd, ITEM_key(it), it->nkey,\n                                   (ret == 1) ? it->nbytes : -1, cas);\n          break;\n      case NREAD_PREPEND:\n          MEMCACHED_COMMAND_PREPEND(c->sfd, ITEM_key(it), it->nkey,\n                                    (ret == 1) ? it->nbytes : -1, cas);\n          break;\n      case NREAD_SET:\n          MEMCACHED_COMMAND_SET(c->sfd, ITEM_key(it), it->nkey,\n                                (ret == 1) ? it->nbytes : -1, cas);\n          break;\n      case NREAD_CAS:\n          MEMCACHED_COMMAND_CAS(c->sfd, ITEM_key(it), it->nkey, it->nbytes,\n                                cas);\n          break;\n      }\n#endif\n\n      if (c->mset_res) {\n          _finalize_mset(c, nbytes, ret, cas);\n      } else {\n          switch (ret) {\n          case STORED:\n              out_string(c, \"STORED\");\n              break;\n          case EXISTS:\n              out_string(c, \"EXISTS\");\n              break;\n          case NOT_FOUND:\n              out_string(c, \"NOT_FOUND\");\n              break;\n          case NOT_STORED:\n              out_string(c, \"NOT_STORED\");\n              break;\n          default:\n              out_string(c, \"SERVER_ERROR Unhandled storage type.\");\n          }\n      }\n\n    }\n\n    c->set_stale = false; /* force flag to be off just in case */\n    c->mset_res = false;\n    item_remove(c->item);       /* release the c->item reference */\n    c->item = 0;\n}\n\n#define COMMAND_TOKEN 0\n#define SUBCOMMAND_TOKEN 1\n#define KEY_TOKEN 1\n\n#define MAX_TOKENS 24\n\n#define WANT_TOKENS(ntokens, min, max) \\\n    do { \\\n        if ((min != -1 && ntokens < min) || (max != -1 && ntokens > max)) { \\\n            out_string(c, \"ERROR\"); \\\n            return; \\\n        } \\\n    } while (0)\n\n#define WANT_TOKENS_OR(ntokens, a, b) \\\n    do { \\\n        if (ntokens != a && ntokens != b) { \\\n            out_string(c, \"ERROR\"); \\\n            return; \\\n        } \\\n    } while (0)\n\n#define WANT_TOKENS_MIN(ntokens, min) \\\n    do { \\\n        if (ntokens < min) { \\\n            out_string(c, \"ERROR\"); \\\n            return; \\\n        } \\\n    } while (0)\n\n/*\n * Tokenize the command string by replacing whitespace with '\\0' and update\n * the token array tokens with pointer to start of each token and length.\n * Returns total number of tokens.  The last valid token is the terminal\n * token (value points to the first unprocessed character of the string and\n * length zero).\n *\n * Usage example:\n *\n *  while(tokenize_command(command, ncommand, tokens, max_tokens) > 0) {\n *      for(int ix = 0; tokens[ix].length != 0; ix++) {\n *          ...\n *      }\n *      ncommand = tokens[ix].value - command;\n *      command  = tokens[ix].value;\n *   }\n */\nstatic size_t tokenize_command(char *command, token_t *tokens, const size_t max_tokens) {\n    char *s, *e;\n    size_t ntokens = 0;\n    assert(command != NULL && tokens != NULL && max_tokens > 1);\n    size_t len = strlen(command);\n    unsigned int i = 0;\n\n    s = e = command;\n    for (i = 0; i < len; i++) {\n        if (*e == ' ') {\n            if (s != e) {\n                tokens[ntokens].value = s;\n                tokens[ntokens].length = e - s;\n                ntokens++;\n                *e = '\\0';\n                if (ntokens == max_tokens - 1) {\n                    e++;\n                    s = e; /* so we don't add an extra token */\n                    break;\n                }\n            }\n            s = e + 1;\n        }\n        e++;\n    }\n\n    if (s != e) {\n        tokens[ntokens].value = s;\n        tokens[ntokens].length = e - s;\n        ntokens++;\n    }\n\n    /*\n     * If we scanned the whole string, the terminal value pointer is null,\n     * otherwise it is the first unprocessed character.\n     */\n    tokens[ntokens].value =  *e == '\\0' ? NULL : e;\n    tokens[ntokens].length = 0;\n    ntokens++;\n\n    return ntokens;\n}\n\nint try_read_command_asciiauth(conn *c) {\n    token_t tokens[MAX_TOKENS];\n    size_t ntokens;\n    char *cont = NULL;\n\n    // TODO: move to another function.\n    if (!c->sasl_started) {\n        char *el;\n        uint32_t size = 0;\n\n        // impossible for the auth command to be this short.\n        if (c->rbytes < 2)\n            return 0;\n\n        el = memchr(c->rcurr, '\\n', c->rbytes);\n\n        // If no newline after 1k, getting junk data, close out.\n        if (!el) {\n            if (c->rbytes > 2048) {\n                conn_set_state(c, conn_closing);\n                return 1;\n            }\n            return 0;\n        }\n\n        // Looking for: \"set foo 0 0 N\\r\\nuser pass\\r\\n\"\n        // key, flags, and ttl are ignored. N is used to see if we have the rest.\n\n        // so tokenize doesn't walk past into the value.\n        // it's fine to leave the \\r in, as strtoul will stop at it.\n        *el = '\\0';\n\n        ntokens = tokenize_command(c->rcurr, tokens, MAX_TOKENS);\n        // ensure the buffer is consumed.\n        c->rbytes -= (el - c->rcurr) + 1;\n        c->rcurr += (el - c->rcurr) + 1;\n\n        // final token is a NULL ender, so we have one more than expected.\n        if (ntokens < 6\n                || strcmp(tokens[0].value, \"set\") != 0\n                || !safe_strtoul(tokens[4].value, &size)) {\n            if (!c->resp) {\n                if (!resp_start(c)) {\n                    conn_set_state(c, conn_closing);\n                    return 1;\n                }\n            }\n            out_string(c, \"CLIENT_ERROR unauthenticated\");\n            return 1;\n        }\n\n        // we don't actually care about the key at all; it can be anything.\n        // we do care about the size of the remaining read.\n        c->rlbytes = size + 2;\n\n        c->sasl_started = true; // reuse from binprot sasl, but not sasl :)\n    }\n\n    if (c->rbytes < c->rlbytes) {\n        // need more bytes.\n        return 0;\n    }\n\n    // Going to respond at this point, so attach a response object.\n    if (!c->resp) {\n        if (!resp_start(c)) {\n            conn_set_state(c, conn_closing);\n            return 1;\n        }\n    }\n\n    cont = c->rcurr;\n    // advance buffer. no matter what we're stopping.\n    c->rbytes -= c->rlbytes;\n    c->rcurr += c->rlbytes;\n    c->sasl_started = false;\n\n    // must end with \\r\\n\n    // NB: I thought ASCII sets also worked with just \\n, but according to\n    // complete_nread_ascii only \\r\\n is valid.\n    if (strncmp(cont + c->rlbytes - 2, \"\\r\\n\", 2) != 0) {\n        out_string(c, \"CLIENT_ERROR bad command line termination\");\n        return 1;\n    }\n\n    // payload should be \"user pass\", so we can use the tokenizer.\n    cont[c->rlbytes - 2] = '\\0';\n    ntokens = tokenize_command(cont, tokens, MAX_TOKENS);\n\n    if (ntokens < 3) {\n        out_string(c, \"CLIENT_ERROR bad authentication token format\");\n        return 1;\n    }\n\n    if (authfile_check(tokens[0].value, tokens[1].value) == 1) {\n        out_string(c, \"STORED\");\n        c->authenticated = true;\n        c->try_read_command = try_read_command_ascii;\n        pthread_mutex_lock(&c->thread->stats.mutex);\n        c->thread->stats.auth_cmds++;\n        pthread_mutex_unlock(&c->thread->stats.mutex);\n    } else {\n        out_string(c, \"CLIENT_ERROR authentication failure\");\n        pthread_mutex_lock(&c->thread->stats.mutex);\n        c->thread->stats.auth_cmds++;\n        c->thread->stats.auth_errors++;\n        pthread_mutex_unlock(&c->thread->stats.mutex);\n    }\n\n    return 1;\n}\n\nint try_read_command_ascii(conn *c) {\n    char *el, *cont;\n\n    if (c->rbytes == 0)\n        return 0;\n\n    el = memchr(c->rcurr, '\\n', c->rbytes);\n    if (!el) {\n        if (c->rbytes > 2048) {\n            /*\n             * We didn't have a '\\n' in the first few k. This _has_ to be a\n             * large multiget, if not we should just nuke the connection.\n             */\n            char *ptr = c->rcurr;\n            while (*ptr == ' ') { /* ignore leading whitespaces */\n                ++ptr;\n            }\n\n            if (ptr - c->rcurr > 100 ||\n                (strncmp(ptr, \"get \", 4) && strncmp(ptr, \"gets \", 5))) {\n\n                conn_set_state(c, conn_closing);\n                return 1;\n            }\n\n            // ASCII multigets are unbound, so our fixed size rbuf may not\n            // work for this particular workload... For backcompat we'll use a\n            // malloc/realloc/free routine just for this.\n            if (!c->rbuf_malloced) {\n                if (!rbuf_switch_to_malloc(c)) {\n                    conn_set_state(c, conn_closing);\n                    return 1;\n                }\n            }\n        }\n\n        return 0;\n    }\n    cont = el + 1;\n    if ((el - c->rcurr) > 1 && *(el - 1) == '\\r') {\n        el--;\n    }\n    *el = '\\0';\n\n    assert(cont <= (c->rcurr + c->rbytes));\n\n    c->last_cmd_time = current_time;\n    process_command_ascii(c, c->rcurr);\n\n    c->rbytes -= (cont - c->rcurr);\n    c->rcurr = cont;\n\n    assert(c->rcurr <= (c->rbuf + c->rsize));\n\n    return 1;\n}\n\n\nstatic inline bool set_noreply_maybe(conn *c, token_t *tokens, size_t ntokens)\n{\n    int noreply_index = ntokens - 2;\n\n    /*\n      NOTE: this function is not the first place where we are going to\n      send the reply.  We could send it instead from process_command()\n      if the request line has wrong number of tokens.  However parsing\n      malformed line for \"noreply\" option is not reliable anyway, so\n      it can't be helped.\n    */\n    if (tokens[noreply_index].value\n        && strcmp(tokens[noreply_index].value, \"noreply\") == 0) {\n        c->noreply = true;\n    }\n    return c->noreply;\n}\n\n/* client flags == 0 means use no storage for client flags */\nstatic inline int make_ascii_get_suffix(char *suffix, item *it, bool return_cas, int nbytes) {\n    char *p = suffix;\n    *p = ' ';\n    p++;\n    if (FLAGS_SIZE(it) == 0) {\n        *p = '0';\n        p++;\n    } else {\n        p = itoa_u64(*((client_flags_t *) ITEM_suffix(it)), p);\n    }\n    *p = ' ';\n    p = itoa_u32(nbytes-2, p+1);\n\n    if (return_cas) {\n        *p = ' ';\n        p = itoa_u64(ITEM_get_cas(it), p+1);\n    }\n\n    *p = '\\r';\n    *(p+1) = '\\n';\n    *(p+2) = '\\0';\n    return (p - suffix) + 2;\n}\n\n/* ntokens is overwritten here... shrug.. */\nstatic inline void process_get_command(conn *c, token_t *tokens, size_t ntokens, bool return_cas, bool should_touch) {\n    char *key;\n    size_t nkey;\n    item *it;\n    token_t *key_token = &tokens[KEY_TOKEN];\n    int32_t exptime_int = 0;\n    rel_time_t exptime = 0;\n    bool fail_length = false;\n    assert(c != NULL);\n    mc_resp *resp = c->resp;\n\n    if (should_touch) {\n        // For get and touch commands, use first token as exptime\n        if (!safe_strtol(tokens[1].value, &exptime_int)) {\n            out_string(c, \"CLIENT_ERROR invalid exptime argument\");\n            return;\n        }\n        key_token++;\n        exptime = realtime(EXPTIME_TO_POSITIVE_TIME(exptime_int));\n    }\n\n    do {\n        while(key_token->length != 0) {\n            bool overflow; // not used here.\n            key = key_token->value;\n            nkey = key_token->length;\n\n            if (nkey > KEY_MAX_LENGTH) {\n                fail_length = true;\n                goto stop;\n            }\n\n            it = limited_get(key, nkey, c->thread, exptime, should_touch, DO_UPDATE, &overflow);\n            if (settings.detail_enabled) {\n                stats_prefix_record_get(key, nkey, NULL != it);\n            }\n            if (it) {\n                /*\n                 * Construct the response. Each hit adds three elements to the\n                 * outgoing data list:\n                 *   \"VALUE \"\n                 *   key\n                 *   \" \" + flags + \" \" + data length + \"\\r\\n\" + data (with \\r\\n)\n                 */\n\n                {\n                  MEMCACHED_COMMAND_GET(c->sfd, ITEM_key(it), it->nkey,\n                                        it->nbytes, ITEM_get_cas(it));\n                  int nbytes = it->nbytes;\n                  char *p = resp->wbuf;\n                  memcpy(p, \"VALUE \", 6);\n                  p += 6;\n                  memcpy(p, ITEM_key(it), it->nkey);\n                  p += it->nkey;\n                  p += make_ascii_get_suffix(p, it, return_cas, nbytes);\n                  resp_add_iov(resp, resp->wbuf, p - resp->wbuf);\n\n#ifdef EXTSTORE\n                  if (it->it_flags & ITEM_HDR) {\n                      if (storage_get_item(c, it, resp) != 0) {\n                          pthread_mutex_lock(&c->thread->stats.mutex);\n                          c->thread->stats.get_oom_extstore++;\n                          pthread_mutex_unlock(&c->thread->stats.mutex);\n\n                          item_remove(it);\n                          goto stop;\n                      }\n                  } else if ((it->it_flags & ITEM_CHUNKED) == 0) {\n                      resp_add_iov(resp, ITEM_data(it), it->nbytes);\n                  } else {\n                      resp_add_chunked_iov(resp, it, it->nbytes);\n                  }\n#else\n                  if ((it->it_flags & ITEM_CHUNKED) == 0) {\n                      resp_add_iov(resp, ITEM_data(it), it->nbytes);\n                  } else {\n                      resp_add_chunked_iov(resp, it, it->nbytes);\n                  }\n#endif\n                }\n\n                if (settings.verbose > 1) {\n                    int ii;\n                    fprintf(stderr, \">%d sending key \", c->sfd);\n                    for (ii = 0; ii < it->nkey; ++ii) {\n                        fprintf(stderr, \"%c\", key[ii]);\n                    }\n                    fprintf(stderr, \"\\n\");\n                }\n\n                /* item_get() has incremented it->refcount for us */\n                pthread_mutex_lock(&c->thread->stats.mutex);\n                if (should_touch) {\n                    c->thread->stats.touch_cmds++;\n                    c->thread->stats.slab_stats[ITEM_clsid(it)].touch_hits++;\n                } else {\n                    c->thread->stats.lru_hits[it->slabs_clsid]++;\n                    c->thread->stats.get_cmds++;\n                }\n                pthread_mutex_unlock(&c->thread->stats.mutex);\n#ifdef EXTSTORE\n                /* If ITEM_HDR, an io_wrap owns the reference. */\n                if ((it->it_flags & ITEM_HDR) == 0) {\n                    resp->item = it;\n                }\n#else\n                resp->item = it;\n#endif\n            } else {\n                pthread_mutex_lock(&c->thread->stats.mutex);\n                if (should_touch) {\n                    c->thread->stats.touch_cmds++;\n                    c->thread->stats.touch_misses++;\n                } else {\n                    c->thread->stats.get_misses++;\n                    c->thread->stats.get_cmds++;\n                }\n                MEMCACHED_COMMAND_GET(c->sfd, key, nkey, -1, 0);\n                pthread_mutex_unlock(&c->thread->stats.mutex);\n            }\n\n            key_token++;\n            if (key_token->length != 0) {\n                if (!resp_start(c)) {\n                    goto stop;\n                }\n                resp = c->resp;\n            }\n        }\n\n        /*\n         * If the command string hasn't been fully processed, get the next set\n         * of tokens.\n         */\n        if (key_token->value != NULL) {\n            ntokens = tokenize_command(key_token->value, tokens, MAX_TOKENS);\n            key_token = tokens;\n            if (!resp_start(c)) {\n                goto stop;\n            }\n            resp = c->resp;\n        }\n    } while(key_token->value != NULL);\nstop:\n\n    if (settings.verbose > 1)\n        fprintf(stderr, \">%d END\\n\", c->sfd);\n\n    /*\n        If the loop was terminated because of out-of-memory, it is not\n        reliable to add END\\r\\n to the buffer, because it might not end\n        in \\r\\n. So we send SERVER_ERROR instead.\n    */\n    if (key_token->value != NULL) {\n        // Kill any stacked responses we had.\n        conn_release_items(c);\n        // Start a new response object for the error message.\n        if (!resp_start(c)) {\n            // severe out of memory error.\n            conn_set_state(c, conn_closing);\n            return;\n        }\n        if (fail_length) {\n            out_string(c, \"CLIENT_ERROR bad command line format\");\n        } else {\n            out_of_memory(c, \"SERVER_ERROR out of memory writing get response\");\n        }\n    } else {\n        // Tag the end token onto the most recent response object.\n        resp_add_iov(resp, \"END\\r\\n\", 5);\n        conn_set_state(c, conn_mwrite);\n    }\n}\n\ninline static void process_stats_detail(conn *c, const char *command) {\n    assert(c != NULL);\n\n    if (strcmp(command, \"on\") == 0) {\n        settings.detail_enabled = 1;\n        out_string(c, \"OK\");\n    }\n    else if (strcmp(command, \"off\") == 0) {\n        settings.detail_enabled = 0;\n        out_string(c, \"OK\");\n    }\n    else if (strcmp(command, \"dump\") == 0) {\n        int len;\n        char *stats = stats_prefix_dump(&len);\n        write_and_free(c, stats, len);\n    }\n    else {\n        out_string(c, \"CLIENT_ERROR usage: stats detail on|off|dump\");\n    }\n}\n\nstatic void process_stat(conn *c, token_t *tokens, const size_t ntokens) {\n    const char *subcommand = tokens[SUBCOMMAND_TOKEN].value;\n    assert(c != NULL);\n\n    if (ntokens < 2) {\n        out_string(c, \"CLIENT_ERROR bad command line\");\n        return;\n    }\n\n    if (ntokens == 2) {\n        server_stats(&append_stats, c);\n        (void)get_stats(NULL, 0, &append_stats, c);\n    } else if (strcmp(subcommand, \"reset\") == 0) {\n        stats_reset();\n        out_string(c, \"RESET\");\n        return;\n    } else if (strcmp(subcommand, \"detail\") == 0) {\n        /* NOTE: how to tackle detail with binary? */\n        if (ntokens < 4)\n            process_stats_detail(c, \"\");  /* outputs the error message */\n        else\n            process_stats_detail(c, tokens[2].value);\n        /* Output already generated */\n        return;\n    } else if (strcmp(subcommand, \"settings\") == 0) {\n        process_stat_settings(&append_stats, c);\n    } else if (strcmp(subcommand, \"cachedump\") == 0) {\n        char *buf;\n        unsigned int bytes, id, limit = 0;\n\n        if (!settings.dump_enabled) {\n            out_string(c, \"CLIENT_ERROR stats cachedump not allowed\");\n            return;\n        }\n\n        if (ntokens < 5) {\n            out_string(c, \"CLIENT_ERROR bad command line\");\n            return;\n        }\n\n        if (!safe_strtoul(tokens[2].value, &id) ||\n            !safe_strtoul(tokens[3].value, &limit)) {\n            out_string(c, \"CLIENT_ERROR bad command line format\");\n            return;\n        }\n\n        if (id >= MAX_NUMBER_OF_SLAB_CLASSES) {\n            out_string(c, \"CLIENT_ERROR Illegal slab id\");\n            return;\n        }\n\n        buf = item_cachedump(id, limit, &bytes);\n        write_and_free(c, buf, bytes);\n        return;\n    } else if (strcmp(subcommand, \"conns\") == 0) {\n        process_stats_conns(&append_stats, c);\n#ifdef EXTSTORE\n    } else if (strcmp(subcommand, \"extstore\") == 0) {\n        process_extstore_stats(&append_stats, c);\n#endif\n#ifdef PROXY\n    } else if (strcmp(subcommand, \"proxy\") == 0) {\n        process_proxy_stats(settings.proxy_ctx, &append_stats, c);\n    } else if (strcmp(subcommand, \"proxyfuncs\") == 0) {\n        process_proxy_funcstats(settings.proxy_ctx, &append_stats, c);\n    } else if (strcmp(subcommand, \"proxybe\") == 0) {\n        process_proxy_bestats(settings.proxy_ctx, &append_stats, c);\n#endif\n    } else {\n        /* getting here means that the subcommand is either engine specific or\n           is invalid. query the engine and see. */\n        if (get_stats(subcommand, strlen(subcommand), &append_stats, c)) {\n            if (c->stats.buffer == NULL) {\n                out_of_memory(c, \"SERVER_ERROR out of memory writing stats\");\n            } else {\n                write_and_free(c, c->stats.buffer, c->stats.offset);\n                c->stats.buffer = NULL;\n            }\n        } else {\n            out_string(c, \"ERROR\");\n        }\n        return;\n    }\n\n    /* append terminator and start the transfer */\n    append_stats(NULL, 0, NULL, 0, c);\n\n    if (c->stats.buffer == NULL) {\n        out_of_memory(c, \"SERVER_ERROR out of memory writing stats\");\n    } else {\n        write_and_free(c, c->stats.buffer, c->stats.offset);\n        c->stats.buffer = NULL;\n    }\n}\n\n// slow snprintf for debugging purposes.\nstatic void process_meta_command(conn *c, token_t *tokens, const size_t ntokens) {\n    assert(c != NULL);\n\n    if (ntokens < 3 || tokens[KEY_TOKEN].length > KEY_MAX_LENGTH) {\n        out_string(c, \"CLIENT_ERROR bad command line format\");\n        return;\n    }\n\n    char *key = tokens[KEY_TOKEN].value;\n    size_t nkey = tokens[KEY_TOKEN].length;\n\n    if (ntokens >= 4 && tokens[2].length == 1 && tokens[2].value[0] == 'b') {\n        size_t ret = base64_decode((unsigned char *)key, nkey,\n                    (unsigned char *)key, nkey);\n        if (ret == 0) {\n            // failed to decode.\n            out_string(c, \"CLIENT_ERROR bad command line format\");\n            return;\n        }\n        nkey = ret;\n    }\n\n    bool overflow; // not used here.\n    item *it = limited_get(key, nkey, c->thread, 0, false, DONT_UPDATE, &overflow);\n    if (it) {\n        mc_resp *resp = c->resp;\n        size_t total = 0;\n        size_t ret;\n        // similar to out_string().\n        memcpy(resp->wbuf, \"ME \", 3);\n        total += 3;\n        if (it->it_flags & ITEM_KEY_BINARY) {\n            // re-encode from memory rather than copy the original key;\n            // to help give confidence that what in memory is what we asked\n            // for.\n            total += base64_encode((unsigned char *) ITEM_key(it), it->nkey, (unsigned char *)resp->wbuf + total, WRITE_BUFFER_SIZE - total);\n        } else {\n            memcpy(resp->wbuf + total, ITEM_key(it), it->nkey);\n            total += it->nkey;\n        }\n        resp->wbuf[total] = ' ';\n        total++;\n\n        ret = snprintf(resp->wbuf + total, WRITE_BUFFER_SIZE - (it->nkey + 12),\n                \"exp=%d la=%llu cas=%llu fetch=%s cls=%u size=%lu\\r\\n\",\n                (it->exptime == 0) ? -1 : (it->exptime - current_time),\n                (unsigned long long)(current_time - it->time),\n                (unsigned long long)ITEM_get_cas(it),\n                (it->it_flags & ITEM_FETCHED) ? \"yes\" : \"no\",\n                ITEM_clsid(it),\n                (unsigned long) ITEM_ntotal(it));\n\n        item_remove(it);\n        resp->wbytes = total + ret;\n        resp_add_iov(resp, resp->wbuf, resp->wbytes);\n        conn_set_state(c, conn_new_cmd);\n    } else {\n        out_string(c, \"EN\");\n    }\n    pthread_mutex_lock(&c->thread->stats.mutex);\n    c->thread->stats.meta_cmds++;\n    pthread_mutex_unlock(&c->thread->stats.mutex);\n}\n\n#define MFLAG_MAX_OPT_LENGTH 20\n#define MFLAG_MAX_OPAQUE_LENGTH 32\n\nstruct _meta_flags {\n    unsigned int has_error :1; // flipped if we found an error during parsing.\n    unsigned int no_update :1;\n    unsigned int locked :1;\n    unsigned int vivify :1;\n    unsigned int la :1;\n    unsigned int hit :1;\n    unsigned int value :1;\n    unsigned int set_stale :1;\n    unsigned int no_reply :1;\n    unsigned int has_cas :1;\n    unsigned int has_cas_in :1;\n    unsigned int new_ttl :1;\n    unsigned int key_binary:1;\n    unsigned int remove_val:1;\n    char mode; // single character mode switch, common to ms/ma\n    rel_time_t exptime;\n    rel_time_t autoviv_exptime;\n    rel_time_t recache_time;\n    client_flags_t client_flags;\n    uint64_t req_cas_id;\n    uint64_t cas_id_in; // client supplied next-CAS\n    uint64_t delta; // ma\n    uint64_t initial; // ma\n};\n\nstatic int _meta_flag_preparse(token_t *tokens, const size_t start,\n        struct _meta_flags *of, char **errstr) {\n    unsigned int i;\n    size_t ret;\n    int32_t tmp_int;\n    uint8_t seen[127] = {0};\n    // Start just past the key token. Look at first character of each token.\n    for (i = start; tokens[i].length != 0; i++) {\n        uint8_t o = (uint8_t)tokens[i].value[0];\n        // zero out repeat flags so we don't over-parse for return data.\n        if (o >= 127 || seen[o] != 0) {\n            *errstr = \"CLIENT_ERROR duplicate flag\";\n            return -1;\n        }\n        seen[o] = 1;\n        switch (o) {\n            // base64 decode the key in-place, as the binary should always be\n            // shorter and the conversion code buffers bytes.\n            case 'b':\n                ret = base64_decode((unsigned char *)tokens[KEY_TOKEN].value, tokens[KEY_TOKEN].length,\n                            (unsigned char *)tokens[KEY_TOKEN].value, tokens[KEY_TOKEN].length);\n                if (ret == 0) {\n                    // Failed to decode\n                    *errstr = \"CLIENT_ERROR error decoding key\";\n                    of->has_error = 1;\n                }\n                tokens[KEY_TOKEN].length = ret;\n                of->key_binary = 1;\n                break;\n            /* Negative exptimes can underflow and end up immortal. realtime() will\n               immediately expire values that are greater than REALTIME_MAXDELTA, but less\n               than process_started, so lets aim for that. */\n            case 'N':\n                of->locked = 1;\n                of->vivify = 1;\n                if (!safe_strtol(tokens[i].value+1, &tmp_int)) {\n                    *errstr = \"CLIENT_ERROR bad token in command line format\";\n                    of->has_error = 1;\n                } else {\n                    of->autoviv_exptime = realtime(EXPTIME_TO_POSITIVE_TIME(tmp_int));\n                }\n                break;\n            case 'T':\n                of->locked = 1;\n                if (!safe_strtol(tokens[i].value+1, &tmp_int)) {\n                    *errstr = \"CLIENT_ERROR bad token in command line format\";\n                    of->has_error = 1;\n                } else {\n                    of->exptime = realtime(EXPTIME_TO_POSITIVE_TIME(tmp_int));\n                    of->new_ttl = true;\n                }\n                break;\n            case 'R':\n                of->locked = 1;\n                if (!safe_strtol(tokens[i].value+1, &tmp_int)) {\n                    *errstr = \"CLIENT_ERROR bad token in command line format\";\n                    of->has_error = 1;\n                } else {\n                    of->recache_time = realtime(EXPTIME_TO_POSITIVE_TIME(tmp_int));\n                }\n                break;\n            case 'l':\n                of->la = 1;\n                of->locked = 1; // need locked to delay LRU bump\n                break;\n            case 'O':\n            case 'P':\n            case 'L':\n                break;\n            case 'k': // known but no special handling\n            case 's':\n            case 't':\n            case 'c':\n            case 'f':\n                break;\n            case 'v':\n                of->value = 1;\n                break;\n            case 'h':\n                of->locked = 1; // need locked to delay LRU bump\n                break;\n            case 'u':\n                of->no_update = 1;\n                break;\n            case 'q':\n                of->no_reply = 1;\n                break;\n            case 'x':\n                of->remove_val = 1;\n                break;\n            // mset-related.\n            case 'F':\n                if (!safe_strtoflags(tokens[i].value+1, &of->client_flags)) {\n                    of->has_error = true;\n                }\n                break;\n            case 'C': // mset, mdelete, marithmetic\n                if (!safe_strtoull(tokens[i].value+1, &of->req_cas_id)) {\n                    *errstr = \"CLIENT_ERROR bad token in command line format\";\n                    of->has_error = true;\n                } else {\n                    of->has_cas = true;\n                }\n                break;\n            case 'E': // ms, md, ma\n                if (!safe_strtoull(tokens[i].value+1, &of->cas_id_in)) {\n                    *errstr = \"CLIENT_ERROR bad token in command line format\";\n                    of->has_error = true;\n                } else {\n                    of->has_cas_in = true;\n                }\n                break;\n            case 'M': // mset and marithmetic mode switch\n                if (tokens[i].length != 2) {\n                    *errstr = \"CLIENT_ERROR incorrect length for M token\";\n                    of->has_error = 1;\n                } else {\n                    of->mode = tokens[i].value[1];\n                }\n                break;\n            case 'J': // marithmetic initial value\n                if (!safe_strtoull(tokens[i].value+1, &of->initial)) {\n                    *errstr = \"CLIENT_ERROR invalid numeric initial value\";\n                    of->has_error = 1;\n                }\n                break;\n            case 'D': // marithmetic delta value\n                if (!safe_strtoull(tokens[i].value+1, &of->delta)) {\n                    *errstr = \"CLIENT_ERROR invalid numeric delta value\";\n                    of->has_error = 1;\n                }\n                break;\n            case 'I':\n                of->set_stale = 1;\n                break;\n            default: // unknown flag, bail.\n                *errstr = \"CLIENT_ERROR invalid flag\";\n                return -1;\n        }\n    }\n\n    return of->has_error ? -1 : 0;\n}\n\nstatic void process_mget_command(conn *c, token_t *tokens, const size_t ntokens) {\n    char *key;\n    size_t nkey;\n    item *it;\n    unsigned int i = 0;\n    struct _meta_flags of = {0}; // option bitflags.\n    uint32_t hv; // cached hash value for unlocking an item.\n    bool failed = false;\n    bool item_created = false;\n    bool won_token = false;\n    bool ttl_set = false;\n    char *errstr = \"CLIENT_ERROR bad command line format\";\n    assert(c != NULL);\n    mc_resp *resp = c->resp;\n    char *p = resp->wbuf;\n\n    WANT_TOKENS_MIN(ntokens, 3);\n\n    // FIXME: do we move this check to after preparse?\n    if (tokens[KEY_TOKEN].length > KEY_MAX_LENGTH) {\n        out_errstring(c, \"CLIENT_ERROR bad command line format\");\n        return;\n    }\n\n    // NOTE: final token has length == 0.\n    // KEY_TOKEN == 1. 0 is command.\n\n    if (ntokens > MFLAG_MAX_OPT_LENGTH) {\n        // TODO: ensure the command tokenizer gives us at least this many\n        out_errstring(c, \"CLIENT_ERROR options flags are too long\");\n        return;\n    }\n\n    // scrubs duplicated options and sets flags for how to load the item.\n    // we pass in the first token that should be a flag.\n    if (_meta_flag_preparse(tokens, 2, &of, &errstr) != 0) {\n        out_errstring(c, errstr);\n        return;\n    }\n    c->noreply = of.no_reply;\n\n    // Grab key and length after meta preparsing in case it was decoded.\n    key = tokens[KEY_TOKEN].value;\n    nkey = tokens[KEY_TOKEN].length;\n\n    // TODO: need to indicate if the item was overflowed or not?\n    // I think we do, since an overflow shouldn't trigger an alloc/replace.\n    bool overflow = false;\n    if (!of.locked) {\n        it = limited_get(key, nkey, c->thread, 0, false, !of.no_update, &overflow);\n    } else {\n        // If we had to lock the item, we're doing our own bump later.\n        it = limited_get_locked(key, nkey, c->thread, DONT_UPDATE, &hv, &overflow);\n    }\n\n    // Since we're a new protocol, we can actually inform users that refcount\n    // overflow is happening by straight up throwing an error.\n    // We definitely don't want to re-autovivify by accident.\n    if (overflow) {\n        assert(it == NULL);\n        out_errstring(c, \"SERVER_ERROR refcount overflow during fetch\");\n        return;\n    }\n\n    if (it == NULL && of.vivify) {\n        // Fill in the exptime during parsing later.\n        it = item_alloc(key, nkey, 0, realtime(0), 2);\n        // We don't actually need any of do_store_item's logic:\n        // - already fetched and missed an existing item.\n        // - lock is still held.\n        // - not append/prepend/replace\n        // - not testing CAS\n        if (it != NULL) {\n            // I look forward to the day I get rid of this :)\n            memcpy(ITEM_data(it), \"\\r\\n\", 2);\n            // NOTE: This initializes the CAS value.\n            do_item_link(it, hv, of.has_cas_in ? of.cas_id_in : get_cas_id());\n            item_created = true;\n        }\n    }\n\n    // don't have to check result of add_iov() since the iov size defaults are\n    // enough.\n    if (it) {\n        if (of.value) {\n            memcpy(p, \"VA \", 3);\n            p = itoa_u32(it->nbytes-2, p+3);\n        } else {\n            memcpy(p, \"HD\", 2);\n            p += 2;\n        }\n\n        for (i = KEY_TOKEN+1; i < ntokens-1; i++) {\n            switch (tokens[i].value[0]) {\n                case 'T':\n                    ttl_set = true;\n                    it->exptime = of.exptime;\n                    break;\n                case 'N':\n                    if (item_created) {\n                        it->exptime = of.autoviv_exptime;\n                        won_token = true;\n                    }\n                    break;\n                case 'R':\n                    // If we haven't autovivified and supplied token is less\n                    // than current TTL, mark a win.\n                    if ((it->it_flags & ITEM_TOKEN_SENT) == 0\n                            && !item_created\n                            && it->exptime != 0\n                            && it->exptime < of.recache_time) {\n                        won_token = true;\n                    }\n                    break;\n                case 's':\n                    META_CHAR(p, 's');\n                    p = itoa_u32(it->nbytes-2, p);\n                    break;\n                case 't':\n                    // TTL remaining as of this request.\n                    // needs to be relative because server clocks may not be in sync.\n                    META_CHAR(p, 't');\n                    if (it->exptime == 0) {\n                        *p = '-';\n                        *(p+1) = '1';\n                        p += 2;\n                    } else {\n                        p = itoa_u32(it->exptime - current_time, p);\n                    }\n                    break;\n                case 'c':\n                    META_CHAR(p, 'c');\n                    p = itoa_u64(ITEM_get_cas(it), p);\n                    break;\n                case 'f':\n                    META_CHAR(p, 'f');\n                    if (FLAGS_SIZE(it) == 0) {\n                        *p = '0';\n                        p++;\n                    } else {\n                        p = itoa_u64(*((client_flags_t *) ITEM_suffix(it)), p);\n                    }\n                    break;\n                case 'l':\n                    META_CHAR(p, 'l');\n                    p = itoa_u32(current_time - it->time, p);\n                    break;\n                case 'h':\n                    META_CHAR(p, 'h');\n                    if (it->it_flags & ITEM_FETCHED) {\n                        *p = '1';\n                    } else {\n                        *p = '0';\n                    }\n                    p++;\n                    break;\n                case 'O':\n                    if (tokens[i].length > MFLAG_MAX_OPAQUE_LENGTH) {\n                        errstr = \"CLIENT_ERROR opaque token too long\";\n                        goto error;\n                    }\n                    META_SPACE(p);\n                    memcpy(p, tokens[i].value, tokens[i].length);\n                    p += tokens[i].length;\n                    break;\n                case 'k':\n                    META_KEY(p, ITEM_key(it), it->nkey, (it->it_flags & ITEM_KEY_BINARY));\n                    break;\n            }\n        }\n\n        // Has this item already sent a token?\n        // Important to do this here so we don't send W with Z.\n        // Isn't critical, but easier for client authors to understand.\n        if (it->it_flags & ITEM_TOKEN_SENT) {\n            META_CHAR(p, 'Z');\n        }\n        if (it->it_flags & ITEM_STALE) {\n            META_CHAR(p, 'X');\n            // FIXME: think hard about this. is this a default, or a flag?\n            if ((it->it_flags & ITEM_TOKEN_SENT) == 0) {\n                // If we're stale but no token already sent, now send one.\n                won_token = true;\n            }\n        }\n\n        if (won_token) {\n            // Mark a win into the flag buffer.\n            META_CHAR(p, 'W');\n            it->it_flags |= ITEM_TOKEN_SENT;\n        }\n\n        *p = '\\r';\n        *(p+1) = '\\n';\n        *(p+2) = '\\0';\n        p += 2;\n        // finally, chain in the buffer.\n        resp_add_iov(resp, resp->wbuf, p - resp->wbuf);\n\n        if (of.value) {\n#ifdef EXTSTORE\n            if (it->it_flags & ITEM_HDR) {\n                if (storage_get_item(c, it, resp) != 0) {\n                    pthread_mutex_lock(&c->thread->stats.mutex);\n                    c->thread->stats.get_oom_extstore++;\n                    pthread_mutex_unlock(&c->thread->stats.mutex);\n\n                    failed = true;\n                }\n            } else if ((it->it_flags & ITEM_CHUNKED) == 0) {\n                resp_add_iov(resp, ITEM_data(it), it->nbytes);\n            } else {\n                resp_add_chunked_iov(resp, it, it->nbytes);\n            }\n#else\n            if ((it->it_flags & ITEM_CHUNKED) == 0) {\n                resp_add_iov(resp, ITEM_data(it), it->nbytes);\n            } else {\n                resp_add_chunked_iov(resp, it, it->nbytes);\n            }\n#endif\n        }\n\n        // need to hold the ref at least because of the key above.\n#ifdef EXTSTORE\n        if (!failed) {\n            if ((it->it_flags & ITEM_HDR) != 0 && of.value) {\n                // Only have extstore clean if header and returning value.\n                resp->item = NULL;\n            } else {\n                resp->item = it;\n            }\n        } else {\n            // Failed to set up extstore fetch.\n            if (of.locked) {\n                do_item_remove(it);\n            } else {\n                item_remove(it);\n            }\n        }\n#else\n        resp->item = it;\n#endif\n    } else {\n        failed = true;\n    }\n\n    if (of.locked) {\n        // Delayed bump so we could get fetched/last access time pre-update.\n        if (!of.no_update && it != NULL) {\n            do_item_bump(c->thread, it, hv);\n        }\n        item_unlock(hv);\n    }\n\n    // we count this command as a normal one if we've gotten this far.\n    // TODO: for autovivify case, miss never happens. Is this okay?\n    if (!failed) {\n        pthread_mutex_lock(&c->thread->stats.mutex);\n        if (ttl_set) {\n            c->thread->stats.touch_cmds++;\n            c->thread->stats.slab_stats[ITEM_clsid(it)].touch_hits++;\n        } else {\n            c->thread->stats.lru_hits[it->slabs_clsid]++;\n            c->thread->stats.get_cmds++;\n        }\n        pthread_mutex_unlock(&c->thread->stats.mutex);\n\n        conn_set_state(c, conn_new_cmd);\n    } else {\n        pthread_mutex_lock(&c->thread->stats.mutex);\n        if (ttl_set) {\n            c->thread->stats.touch_cmds++;\n            c->thread->stats.touch_misses++;\n        } else {\n            c->thread->stats.get_misses++;\n            c->thread->stats.get_cmds++;\n        }\n        MEMCACHED_COMMAND_GET(c->sfd, key, nkey, -1, 0);\n        pthread_mutex_unlock(&c->thread->stats.mutex);\n\n        // This gets elided in noreply mode.\n        if (c->noreply)\n            resp->skip = true;\n        memcpy(p, \"EN\", 2);\n        p += 2;\n        for (i = KEY_TOKEN+1; i < ntokens-1; i++) {\n            switch (tokens[i].value[0]) {\n                // TODO: macro perhaps?\n                case 'O':\n                    if (tokens[i].length > MFLAG_MAX_OPAQUE_LENGTH) {\n                        errstr = \"CLIENT_ERROR opaque token too long\";\n                        goto error;\n                    }\n                    META_SPACE(p);\n                    memcpy(p, tokens[i].value, tokens[i].length);\n                    p += tokens[i].length;\n                    break;\n                case 'k':\n                    META_KEY(p, key, nkey, of.key_binary);\n                    break;\n            }\n        }\n        resp->wbytes = p - resp->wbuf;\n        memcpy(resp->wbuf + resp->wbytes, \"\\r\\n\", 2);\n        resp->wbytes += 2;\n        resp_add_iov(resp, resp->wbuf, resp->wbytes);\n        conn_set_state(c, conn_new_cmd);\n    }\n    return;\nerror:\n    if (it) {\n        do_item_remove(it);\n        if (of.locked) {\n            item_unlock(hv);\n        }\n    }\n    out_errstring(c, errstr);\n}\n\nstatic void process_mset_command(conn *c, token_t *tokens, const size_t ntokens) {\n    char *key;\n    size_t nkey;\n    item *it;\n    int i;\n    short comm = NREAD_SET;\n    struct _meta_flags of = {0}; // option bitflags.\n    char *errstr = \"CLIENT_ERROR bad command line format\";\n    uint32_t hv; // cached hash value.\n    int vlen = 0; // value from data line.\n    assert(c != NULL);\n    mc_resp *resp = c->resp;\n    char *p = resp->wbuf;\n    rel_time_t exptime = 0;\n\n    WANT_TOKENS_MIN(ntokens, 3);\n\n    // TODO: most of this is identical to mget.\n    if (tokens[KEY_TOKEN].length > KEY_MAX_LENGTH) {\n        out_errstring(c, \"CLIENT_ERROR bad command line format\");\n        return;\n    }\n\n    if (ntokens == 3) {\n        out_errstring(c, \"CLIENT_ERROR bad command line format\");\n        return;\n    }\n\n    if (ntokens > MFLAG_MAX_OPT_LENGTH) {\n        out_errstring(c, \"CLIENT_ERROR options flags too long\");\n        return;\n    }\n\n    // We note tokens into the front of the write buffer, so we can create the\n    // final buffer in complete_nread_ascii.\n    p = resp->wbuf;\n\n    if (!safe_strtol(tokens[KEY_TOKEN + 1].value, (int32_t*)&vlen)) {\n        out_errstring(c, \"CLIENT_ERROR bad command line format\");\n        return;\n    }\n\n    if (vlen < 0 || vlen > (INT_MAX - 2)) {\n        out_errstring(c, \"CLIENT_ERROR bad command line format\");\n        return;\n    }\n    vlen += 2;\n\n    // We need to at least try to get the size to properly slurp bad bytes\n    // after an error.\n    // we pass in the first token that should be a flag.\n    if (_meta_flag_preparse(tokens, 3, &of, &errstr) != 0) {\n        goto error;\n    }\n\n    key = tokens[KEY_TOKEN].value;\n    nkey = tokens[KEY_TOKEN].length;\n\n    // Set noreply after tokens are understood.\n    c->noreply = of.no_reply;\n    // Set cas return value\n    c->cas = of.has_cas_in ? of.cas_id_in : get_cas_id();\n    exptime = of.exptime;\n\n    bool has_error = false;\n    for (i = KEY_TOKEN+1; i < ntokens-1; i++) {\n        switch (tokens[i].value[0]) {\n            // TODO: macro perhaps?\n            case 'O':\n                if (tokens[i].length > MFLAG_MAX_OPAQUE_LENGTH) {\n                    errstr = \"CLIENT_ERROR opaque token too long\";\n                    has_error = true;\n                    break;\n                }\n                META_SPACE(p);\n                memcpy(p, tokens[i].value, tokens[i].length);\n                p += tokens[i].length;\n                break;\n            case 'k':\n                META_CHAR(p, 'k');\n                break;\n            case 'c':\n                // need to set the cas value post-assignment.\n                META_CHAR(p, 'c');\n                break;\n            case 's':\n                // get the final size post-fill\n                META_CHAR(p, 's');\n                break;\n        }\n    }\n\n    // \"mode switch\" to alternative commands\n    switch (of.mode) {\n        case 0:\n            break; // no mode supplied.\n        case 'E': // Add...\n            comm = NREAD_ADD;\n            break;\n        case 'A': // Append.\n            if (of.vivify) {\n                comm = NREAD_APPENDVIV;\n                exptime = of.autoviv_exptime;\n            } else {\n                comm = NREAD_APPEND;\n            }\n            break;\n        case 'P': // Prepend.\n            if (of.vivify) {\n                comm = NREAD_PREPENDVIV;\n                exptime = of.autoviv_exptime;\n            } else {\n                comm = NREAD_PREPEND;\n            }\n            break;\n        case 'R': // Replace.\n            comm = NREAD_REPLACE;\n            break;\n        case 'S': // Set. Default.\n            comm = NREAD_SET;\n            break;\n        default:\n            errstr = \"CLIENT_ERROR invalid mode for ms M token\";\n            goto error;\n    }\n\n    // The item storage function doesn't exactly map to mset.\n    // If a CAS value is supplied, upgrade default SET mode to CAS mode.\n    // Also allows REPLACE to work, as REPLACE + CAS works the same as CAS.\n    // add-with-cas works the same as add; but could only LRU bump if match..\n    // APPEND/PREPEND allow a simplified CAS check.\n    if (of.has_cas && (comm == NREAD_SET || comm == NREAD_REPLACE)) {\n        comm = NREAD_CAS;\n    }\n\n    // We attempt to process as much as we can in hopes of getting a valid and\n    // adjusted vlen, or else the data swallowed after error will be for 0b.\n    if (has_error)\n        goto error;\n\n    it = item_alloc(key, nkey, of.client_flags, exptime, vlen);\n\n    if (it == 0) {\n        enum store_item_type status;\n        // TODO: These could be normalized codes (TL and OM). Need to\n        // reorganize the output stuff a bit though.\n        if (! item_size_ok(nkey, of.client_flags, vlen)) {\n            errstr = \"SERVER_ERROR object too large for cache\";\n            status = TOO_LARGE;\n            pthread_mutex_lock(&c->thread->stats.mutex);\n            c->thread->stats.store_too_large++;\n            pthread_mutex_unlock(&c->thread->stats.mutex);\n        } else {\n            errstr = \"SERVER_ERROR out of memory storing object\";\n            status = NO_MEMORY;\n            pthread_mutex_lock(&c->thread->stats.mutex);\n            c->thread->stats.store_no_memory++;\n            pthread_mutex_unlock(&c->thread->stats.mutex);\n        }\n        // FIXME: LOGGER_LOG specific to mset, include options.\n        LOGGER_LOG(c->thread->l, LOG_MUTATIONS, LOGGER_ITEM_STORE,\n                NULL, status, comm, key, nkey, 0, 0);\n\n        /* Avoid stale data persisting in cache because we failed alloc. */\n        // NOTE: only if SET mode?\n        it = item_get_locked(key, nkey, c->thread, DONT_UPDATE, &hv);\n        if (it) {\n            do_item_unlink(it, hv);\n            STORAGE_delete(c->thread->storage, it);\n            do_item_remove(it);\n        }\n        item_unlock(hv);\n\n        goto error;\n    }\n    ITEM_set_cas(it, of.req_cas_id);\n\n    c->item = it;\n#ifdef NEED_ALIGN\n    if (it->it_flags & ITEM_CHUNKED) {\n        c->ritem = ITEM_schunk(it);\n    } else {\n        c->ritem = ITEM_data(it);\n    }\n#else\n    c->ritem = ITEM_data(it);\n#endif\n    c->rlbytes = it->nbytes;\n    c->cmd = comm;\n\n    // Prevent printing back the key in meta commands as garbage.\n    if (of.key_binary) {\n        it->it_flags |= ITEM_KEY_BINARY;\n    }\n\n    if (of.set_stale && comm == NREAD_CAS) {\n        c->set_stale = true;\n    }\n    resp->wbytes = p - resp->wbuf;\n    // we don't set up the iov here, instead after complete_nread_ascii when\n    // we have the full status code and item data.\n    c->mset_res = true;\n    conn_set_state(c, conn_nread);\n    return;\nerror:\n    /* swallow the data line */\n    c->sbytes = vlen;\n\n    // Note: no errors possible after the item was successfully allocated.\n    // So we're just looking at dumping error codes and returning.\n    out_errstring(c, errstr);\n    // TODO: pass state in? else switching twice meh.\n    conn_set_state(c, conn_swallow);\n}\n\nstatic void process_mdelete_command(conn *c, token_t *tokens, const size_t ntokens) {\n    char *key;\n    size_t nkey;\n    item *it = NULL;\n    int i;\n    uint32_t hv = 0;\n    struct _meta_flags of = {0}; // option bitflags.\n    char *errstr = \"CLIENT_ERROR bad command line format\";\n    assert(c != NULL);\n    mc_resp *resp = c->resp;\n    // reserve bytes for status code\n    char *p = resp->wbuf + 2;\n\n    WANT_TOKENS_MIN(ntokens, 3);\n\n    // TODO: most of this is identical to mget.\n    if (tokens[KEY_TOKEN].length > KEY_MAX_LENGTH) {\n        out_string(c, \"CLIENT_ERROR bad command line format\");\n        return;\n    }\n\n    if (ntokens > MFLAG_MAX_OPT_LENGTH) {\n        out_string(c, \"CLIENT_ERROR options flags too long\");\n        return;\n    }\n\n    // scrubs duplicated options and sets flags for how to load the item.\n    // we pass in the first token that should be a flag.\n    // FIXME: not using the preparse errstr?\n    if (_meta_flag_preparse(tokens, 2, &of, &errstr) != 0) {\n        out_errstring(c, \"CLIENT_ERROR invalid or duplicate flag\");\n        return;\n    }\n    assert(c != NULL);\n    c->noreply = of.no_reply;\n\n    key = tokens[KEY_TOKEN].value;\n    nkey = tokens[KEY_TOKEN].length;\n\n    for (i = KEY_TOKEN+1; i < ntokens-1; i++) {\n        switch (tokens[i].value[0]) {\n            // TODO: macro perhaps?\n            case 'O':\n                if (tokens[i].length > MFLAG_MAX_OPAQUE_LENGTH) {\n                    errstr = \"CLIENT_ERROR opaque token too long\";\n                    goto error;\n                }\n                META_SPACE(p);\n                memcpy(p, tokens[i].value, tokens[i].length);\n                p += tokens[i].length;\n                break;\n            case 'k':\n                META_KEY(p, key, nkey, of.key_binary);\n                break;\n        }\n    }\n\n    it = item_get_locked(key, nkey, c->thread, DONT_UPDATE, &hv);\n    if (it) {\n        MEMCACHED_COMMAND_DELETE(c->sfd, ITEM_key(it), it->nkey);\n\n        // allow only deleting/marking if a CAS value matches.\n        if (of.has_cas && ITEM_get_cas(it) != of.req_cas_id) {\n            pthread_mutex_lock(&c->thread->stats.mutex);\n            c->thread->stats.delete_misses++;\n            pthread_mutex_unlock(&c->thread->stats.mutex);\n\n            memcpy(resp->wbuf, \"EX\", 2);\n            goto cleanup;\n        }\n\n        // If requested, create a new empty tombstone item.\n        if (of.remove_val) {\n            item *new_it = item_alloc(key, nkey, of.client_flags, of.exptime, 2);\n            if (new_it != NULL) {\n                memcpy(ITEM_data(new_it), \"\\r\\n\", 2);\n                if (do_store_item(new_it, NREAD_SET, c->thread, hv, NULL, NULL,\n                            of.has_cas_in ? of.cas_id_in : ITEM_get_cas(it), CAS_NO_STALE)) {\n                    do_item_remove(it);\n                    it = new_it;\n                } else {\n                    do_item_remove(new_it);\n                    memcpy(resp->wbuf, \"NS\", 2);\n                    goto cleanup;\n                }\n            } else {\n                errstr = \"SERVER_ERROR out of memory\";\n                goto error;\n            }\n        }\n\n        // If we're to set this item as stale, we don't actually want to\n        // delete it. We mark the stale bit, bump CAS, and update exptime if\n        // we were supplied a new TTL.\n        if (of.set_stale) {\n            if (of.new_ttl) {\n                it->exptime = of.exptime;\n            }\n            it->it_flags |= ITEM_STALE;\n            // Also need to remove TOKEN_SENT, so next client can win.\n            it->it_flags &= ~ITEM_TOKEN_SENT;\n\n            ITEM_set_cas(it, of.has_cas_in ? of.cas_id_in : get_cas_id());\n            // Clients can noreply nominal responses.\n            if (c->noreply)\n                resp->skip = true;\n\n            memcpy(resp->wbuf, \"HD\", 2);\n        } else {\n            pthread_mutex_lock(&c->thread->stats.mutex);\n            c->thread->stats.slab_stats[ITEM_clsid(it)].delete_hits++;\n            pthread_mutex_unlock(&c->thread->stats.mutex);\n\n            LOGGER_LOG(NULL, LOG_DELETIONS, LOGGER_DELETIONS, it, LOG_TYPE_META_DELETE);\n            if (!of.remove_val) {\n                do_item_unlink(it, hv);\n                STORAGE_delete(c->thread->storage, it);\n            }\n            if (c->noreply)\n                resp->skip = true;\n            memcpy(resp->wbuf, \"HD\", 2);\n        }\n        goto cleanup;\n    } else {\n        pthread_mutex_lock(&c->thread->stats.mutex);\n        c->thread->stats.delete_misses++;\n        pthread_mutex_unlock(&c->thread->stats.mutex);\n\n        memcpy(resp->wbuf, \"NF\", 2);\n        goto cleanup;\n    }\ncleanup:\n    if (it) {\n        do_item_remove(it);\n    }\n    // Item is always returned locked, even if missing.\n    item_unlock(hv);\n    resp->wbytes = p - resp->wbuf;\n    memcpy(resp->wbuf + resp->wbytes, \"\\r\\n\", 2);\n    resp->wbytes += 2;\n    resp_add_iov(resp, resp->wbuf, resp->wbytes);\n    conn_set_state(c, conn_new_cmd);\n    return;\nerror:\n    // cleanup if an error happens after we fetched an item.\n    if (it) {\n        do_item_remove(it);\n        item_unlock(hv);\n    }\n    out_errstring(c, errstr);\n}\n\nstatic void process_marithmetic_command(conn *c, token_t *tokens, const size_t ntokens) {\n    char *key;\n    size_t nkey;\n    int i;\n    struct _meta_flags of = {0}; // option bitflags.\n    char *errstr = \"CLIENT_ERROR bad command line format\";\n    assert(c != NULL);\n    mc_resp *resp = c->resp;\n    // no reservation (like del/set) since we post-process the status line.\n    char *p = resp->wbuf;\n\n    // If no argument supplied, incr or decr by one.\n    of.delta = 1;\n    of.initial = 0; // redundant, for clarity.\n    bool incr = true; // default mode is to increment.\n    bool locked = false;\n    uint32_t hv = 0;\n    item *it = NULL; // item returned by do_add_delta.\n\n    WANT_TOKENS_MIN(ntokens, 3);\n\n    // TODO: most of this is identical to mget.\n    if (tokens[KEY_TOKEN].length > KEY_MAX_LENGTH) {\n        out_string(c, \"CLIENT_ERROR bad command line format\");\n        return;\n    }\n\n    if (ntokens > MFLAG_MAX_OPT_LENGTH) {\n        out_string(c, \"CLIENT_ERROR options flags too long\");\n        return;\n    }\n\n    // scrubs duplicated options and sets flags for how to load the item.\n    // we pass in the first token that should be a flag.\n    if (_meta_flag_preparse(tokens, 2, &of, &errstr) != 0) {\n        out_errstring(c, \"CLIENT_ERROR invalid or duplicate flag\");\n        return;\n    }\n    assert(c != NULL);\n    c->noreply = of.no_reply;\n\n    key = tokens[KEY_TOKEN].value;\n    nkey = tokens[KEY_TOKEN].length;\n\n    // \"mode switch\" to alternative commands\n    switch (of.mode) {\n        case 0: // no switch supplied.\n            break;\n        case 'I': // Incr (default)\n        case '+':\n            incr = true;\n            break;\n        case 'D': // Decr.\n        case '-':\n            incr = false;\n            break;\n        default:\n            errstr = \"CLIENT_ERROR invalid mode for ma M token\";\n            goto error;\n            break;\n    }\n\n    // take hash value and manually lock item... hold lock during store phase\n    // on miss and avoid recalculating the hash multiple times.\n    hv = hash(key, nkey);\n    item_lock(hv);\n    locked = true;\n    char tmpbuf[INCR_MAX_STORAGE_LEN];\n\n    // return a referenced item if it exists, so we can modify it here, rather\n    // than adding even more parameters to do_add_delta.\n    bool item_created = false;\n    switch(do_add_delta(c->thread, key, nkey, incr, of.delta, tmpbuf, &of.req_cas_id, hv, &it)) {\n    case OK:\n        if (c->noreply)\n            resp->skip = true;\n        // *it was filled, set the status below.\n        if (of.has_cas_in) {\n            // override the CAS. slightly inefficient but fixing that can wait\n            // until the next time do_add_delta is changed.\n            ITEM_set_cas(it, of.cas_id_in);\n        }\n        break;\n    case NON_NUMERIC:\n        errstr = \"CLIENT_ERROR cannot increment or decrement non-numeric value\";\n        goto error;\n        break;\n    case EOM:\n        errstr = \"SERVER_ERROR out of memory\";\n        goto error;\n        break;\n    case DELTA_ITEM_NOT_FOUND:\n        if (of.vivify) {\n            itoa_u64(of.initial, tmpbuf);\n            int vlen = strlen(tmpbuf);\n\n            it = item_alloc(key, nkey, 0, 0, vlen+2);\n            if (it != NULL) {\n                memcpy(ITEM_data(it), tmpbuf, vlen);\n                memcpy(ITEM_data(it) + vlen, \"\\r\\n\", 2);\n                if (do_store_item(it, NREAD_ADD, c->thread, hv, NULL, NULL,\n                            of.has_cas_in ? of.cas_id_in : get_cas_id(), CAS_NO_STALE)) {\n                    item_created = true;\n                } else {\n                    // Not sure how we can get here if we're holding the lock.\n                    memcpy(resp->wbuf, \"NS\", 2);\n                }\n            } else {\n                errstr = \"SERVER_ERROR Out of memory allocating new item\";\n                goto error;\n            }\n        } else {\n            pthread_mutex_lock(&c->thread->stats.mutex);\n            if (incr) {\n                c->thread->stats.incr_misses++;\n            } else {\n                c->thread->stats.decr_misses++;\n            }\n            pthread_mutex_unlock(&c->thread->stats.mutex);\n            // won't have a valid it here.\n            memcpy(p, \"NF\", 2);\n            p += 2;\n        }\n        break;\n    case DELTA_ITEM_CAS_MISMATCH:\n        // also returns without a valid it.\n        memcpy(p, \"EX\", 2);\n        p += 2;\n        break;\n    }\n\n    // final loop\n    // allows building the response with information after vivifying from a\n    // miss, or returning a new CAS value after add_delta().\n    if (it) {\n        size_t vlen = strlen(tmpbuf);\n        if (of.value) {\n            memcpy(p, \"VA \", 3);\n            p = itoa_u32(vlen, p+3);\n        } else {\n            memcpy(p, \"HD\", 2);\n            p += 2;\n        }\n\n        for (i = KEY_TOKEN+1; i < ntokens-1; i++) {\n            switch (tokens[i].value[0]) {\n                case 'c':\n                    META_CHAR(p, 'c');\n                    p = itoa_u64(ITEM_get_cas(it), p);\n                    break;\n                case 't':\n                    META_CHAR(p, 't');\n                    if (it->exptime == 0) {\n                        *p = '-';\n                        *(p+1) = '1';\n                        p += 2;\n                    } else {\n                        p = itoa_u32(it->exptime - current_time, p);\n                    }\n                    break;\n                case 'T':\n                    it->exptime = of.exptime;\n                    break;\n                case 'N':\n                    if (item_created) {\n                        it->exptime = of.autoviv_exptime;\n                    }\n                    break;\n                // TODO: macro perhaps?\n                case 'O':\n                    if (tokens[i].length > MFLAG_MAX_OPAQUE_LENGTH) {\n                        errstr = \"CLIENT_ERROR opaque token too long\";\n                        goto error;\n                    }\n                    META_SPACE(p);\n                    memcpy(p, tokens[i].value, tokens[i].length);\n                    p += tokens[i].length;\n                    break;\n                case 'k':\n                    META_KEY(p, key, nkey, of.key_binary);\n                    break;\n            }\n        }\n\n        if (of.value) {\n            *p = '\\r';\n            *(p+1) = '\\n';\n            p += 2;\n            memcpy(p, tmpbuf, vlen);\n            p += vlen;\n        }\n\n        do_item_remove(it);\n    } else {\n        // No item to handle. still need to return opaque/key tokens\n        for (i = KEY_TOKEN+1; i < ntokens-1; i++) {\n            switch (tokens[i].value[0]) {\n                // TODO: macro perhaps?\n                case 'O':\n                    if (tokens[i].length > MFLAG_MAX_OPAQUE_LENGTH) {\n                        errstr = \"CLIENT_ERROR opaque token too long\";\n                        goto error;\n                    }\n                    META_SPACE(p);\n                    memcpy(p, tokens[i].value, tokens[i].length);\n                    p += tokens[i].length;\n                    break;\n                case 'k':\n                    META_KEY(p, key, nkey, of.key_binary);\n                    break;\n            }\n        }\n    }\n\n    item_unlock(hv);\n\n    resp->wbytes = p - resp->wbuf;\n    memcpy(resp->wbuf + resp->wbytes, \"\\r\\n\", 2);\n    resp->wbytes += 2;\n    resp_add_iov(resp, resp->wbuf, resp->wbytes);\n    conn_set_state(c, conn_new_cmd);\n    return;\nerror:\n    if (it != NULL)\n        do_item_remove(it);\n    if (locked)\n        item_unlock(hv);\n    out_errstring(c, errstr);\n}\n\n\nstatic void process_update_command(conn *c, token_t *tokens, const size_t ntokens, int comm, bool handle_cas) {\n    char *key;\n    size_t nkey;\n    client_flags_t flags;\n    int32_t exptime_int = 0;\n    rel_time_t exptime = 0;\n    int vlen;\n    uint64_t req_cas_id=0;\n    item *it;\n\n    assert(c != NULL);\n\n    set_noreply_maybe(c, tokens, ntokens);\n\n    if (tokens[KEY_TOKEN].length > KEY_MAX_LENGTH) {\n        out_string(c, \"CLIENT_ERROR bad command line format\");\n        return;\n    }\n\n    key = tokens[KEY_TOKEN].value;\n    nkey = tokens[KEY_TOKEN].length;\n\n    if (! (safe_strtoflags(tokens[2].value, &flags)\n           && safe_strtol(tokens[3].value, &exptime_int)\n           && safe_strtol(tokens[4].value, (int32_t *)&vlen))) {\n        out_string(c, \"CLIENT_ERROR bad command line format\");\n        return;\n    }\n\n    exptime = realtime(EXPTIME_TO_POSITIVE_TIME(exptime_int));\n\n    // does cas value exist?\n    if (handle_cas) {\n        if (!safe_strtoull(tokens[5].value, &req_cas_id)) {\n            out_string(c, \"CLIENT_ERROR bad command line format\");\n            return;\n        }\n    }\n\n    if (vlen < 0 || vlen > (INT_MAX - 2)) {\n        out_string(c, \"CLIENT_ERROR bad command line format\");\n        return;\n    }\n    vlen += 2;\n\n    if (settings.detail_enabled) {\n        stats_prefix_record_set(key, nkey);\n    }\n\n    it = item_alloc(key, nkey, flags, exptime, vlen);\n\n    if (it == 0) {\n        enum store_item_type status;\n        if (! item_size_ok(nkey, flags, vlen)) {\n            out_string(c, \"SERVER_ERROR object too large for cache\");\n            status = TOO_LARGE;\n            pthread_mutex_lock(&c->thread->stats.mutex);\n            c->thread->stats.store_too_large++;\n            pthread_mutex_unlock(&c->thread->stats.mutex);\n        } else {\n            out_of_memory(c, \"SERVER_ERROR out of memory storing object\");\n            status = NO_MEMORY;\n            pthread_mutex_lock(&c->thread->stats.mutex);\n            c->thread->stats.store_no_memory++;\n            pthread_mutex_unlock(&c->thread->stats.mutex);\n        }\n        LOGGER_LOG(c->thread->l, LOG_MUTATIONS, LOGGER_ITEM_STORE,\n                NULL, status, comm, key, nkey, 0, 0, c->sfd);\n        /* swallow the data line */\n        conn_set_state(c, conn_swallow);\n        c->sbytes = vlen;\n\n        /* Avoid stale data persisting in cache because we failed alloc.\n         * Unacceptable for SET. Anywhere else too? */\n        if (comm == NREAD_SET) {\n            it = item_get(key, nkey, c->thread, DONT_UPDATE);\n            if (it) {\n                item_unlink(it);\n                STORAGE_delete(c->thread->storage, it);\n                item_remove(it);\n            }\n        }\n\n        return;\n    }\n    ITEM_set_cas(it, req_cas_id);\n\n    c->item = it;\n#ifdef NEED_ALIGN\n    if (it->it_flags & ITEM_CHUNKED) {\n        c->ritem = ITEM_schunk(it);\n    } else {\n        c->ritem = ITEM_data(it);\n    }\n#else\n    c->ritem = ITEM_data(it);\n#endif\n    c->rlbytes = it->nbytes;\n    c->cmd = comm;\n    conn_set_state(c, conn_nread);\n}\n\nstatic void process_touch_command(conn *c, token_t *tokens, const size_t ntokens) {\n    char *key;\n    size_t nkey;\n    int32_t exptime_int = 0;\n    rel_time_t exptime = 0;\n    item *it;\n\n    assert(c != NULL);\n\n    set_noreply_maybe(c, tokens, ntokens);\n\n    if (tokens[KEY_TOKEN].length > KEY_MAX_LENGTH) {\n        out_string(c, \"CLIENT_ERROR bad command line format\");\n        return;\n    }\n\n    key = tokens[KEY_TOKEN].value;\n    nkey = tokens[KEY_TOKEN].length;\n\n    if (!safe_strtol(tokens[2].value, &exptime_int)) {\n        out_string(c, \"CLIENT_ERROR invalid exptime argument\");\n        return;\n    }\n\n    exptime = realtime(EXPTIME_TO_POSITIVE_TIME(exptime_int));\n    it = item_touch(key, nkey, exptime, c->thread);\n    if (it) {\n        pthread_mutex_lock(&c->thread->stats.mutex);\n        c->thread->stats.touch_cmds++;\n        c->thread->stats.slab_stats[ITEM_clsid(it)].touch_hits++;\n        pthread_mutex_unlock(&c->thread->stats.mutex);\n\n        out_string(c, \"TOUCHED\");\n        item_remove(it);\n    } else {\n        pthread_mutex_lock(&c->thread->stats.mutex);\n        c->thread->stats.touch_cmds++;\n        c->thread->stats.touch_misses++;\n        pthread_mutex_unlock(&c->thread->stats.mutex);\n\n        out_string(c, \"NOT_FOUND\");\n    }\n}\n\nstatic void process_arithmetic_command(conn *c, token_t *tokens, const size_t ntokens, const bool incr) {\n    char temp[INCR_MAX_STORAGE_LEN];\n    uint64_t delta;\n    char *key;\n    size_t nkey;\n\n    assert(c != NULL);\n\n    set_noreply_maybe(c, tokens, ntokens);\n\n    if (tokens[KEY_TOKEN].length > KEY_MAX_LENGTH) {\n        out_string(c, \"CLIENT_ERROR bad command line format\");\n        return;\n    }\n\n    key = tokens[KEY_TOKEN].value;\n    nkey = tokens[KEY_TOKEN].length;\n\n    if (!safe_strtoull(tokens[2].value, &delta)) {\n        out_string(c, \"CLIENT_ERROR invalid numeric delta argument\");\n        return;\n    }\n\n    switch(add_delta(c->thread, key, nkey, incr, delta, temp, NULL)) {\n    case OK:\n        out_string(c, temp);\n        break;\n    case NON_NUMERIC:\n        out_string(c, \"CLIENT_ERROR cannot increment or decrement non-numeric value\");\n        break;\n    case EOM:\n        out_of_memory(c, \"SERVER_ERROR out of memory\");\n        break;\n    case DELTA_ITEM_NOT_FOUND:\n        pthread_mutex_lock(&c->thread->stats.mutex);\n        if (incr) {\n            c->thread->stats.incr_misses++;\n        } else {\n            c->thread->stats.decr_misses++;\n        }\n        pthread_mutex_unlock(&c->thread->stats.mutex);\n\n        out_string(c, \"NOT_FOUND\");\n        break;\n    case DELTA_ITEM_CAS_MISMATCH:\n        break; /* Should never get here */\n    }\n}\n\n\nstatic void process_delete_command(conn *c, token_t *tokens, const size_t ntokens) {\n    char *key;\n    size_t nkey;\n    item *it;\n    uint32_t hv;\n\n    assert(c != NULL);\n\n    if (ntokens > 3) {\n        bool hold_is_zero = strcmp(tokens[KEY_TOKEN+1].value, \"0\") == 0;\n        bool sets_noreply = set_noreply_maybe(c, tokens, ntokens);\n        bool valid = (ntokens == 4 && (hold_is_zero || sets_noreply))\n            || (ntokens == 5 && hold_is_zero && sets_noreply);\n        if (!valid) {\n            out_string(c, \"CLIENT_ERROR bad command line format.  \"\n                       \"Usage: delete <key> [noreply]\");\n            return;\n        }\n    }\n\n\n    key = tokens[KEY_TOKEN].value;\n    nkey = tokens[KEY_TOKEN].length;\n\n    if(nkey > KEY_MAX_LENGTH) {\n        out_string(c, \"CLIENT_ERROR bad command line format\");\n        return;\n    }\n\n    if (settings.detail_enabled) {\n        stats_prefix_record_delete(key, nkey);\n    }\n\n    it = item_get_locked(key, nkey, c->thread, DONT_UPDATE, &hv);\n    if (it) {\n        MEMCACHED_COMMAND_DELETE(c->sfd, ITEM_key(it), it->nkey);\n\n        pthread_mutex_lock(&c->thread->stats.mutex);\n        c->thread->stats.slab_stats[ITEM_clsid(it)].delete_hits++;\n        pthread_mutex_unlock(&c->thread->stats.mutex);\n        LOGGER_LOG(NULL, LOG_DELETIONS, LOGGER_DELETIONS, it, LOG_TYPE_DELETE);\n        do_item_unlink(it, hv);\n        STORAGE_delete(c->thread->storage, it);\n        do_item_remove(it);      /* release our reference */\n        out_string(c, \"DELETED\");\n    } else {\n        pthread_mutex_lock(&c->thread->stats.mutex);\n        c->thread->stats.delete_misses++;\n        pthread_mutex_unlock(&c->thread->stats.mutex);\n\n        out_string(c, \"NOT_FOUND\");\n    }\n    item_unlock(hv);\n}\n\nstatic void process_verbosity_command(conn *c, token_t *tokens, const size_t ntokens) {\n    unsigned int level;\n\n    assert(c != NULL);\n\n    set_noreply_maybe(c, tokens, ntokens);\n\n    if (!safe_strtoul(tokens[1].value, (uint32_t*)&level)) {\n        out_string(c, \"CLIENT_ERROR bad command line format\");\n        return;\n    }\n    settings.verbose = level > MAX_VERBOSITY_LEVEL ? MAX_VERBOSITY_LEVEL : level;\n    out_string(c, \"OK\");\n    return;\n}\n\n#ifdef MEMCACHED_DEBUG\nstatic void process_misbehave_command(conn *c) {\n    int allowed = 0;\n\n    // try opening new TCP socket\n    int i = socket(AF_INET, SOCK_STREAM, 0);\n    if (i != -1) {\n        allowed++;\n        close(i);\n    }\n\n    // try executing new commands\n    i = system(\"sleep 0\");\n    if (i != -1) {\n        allowed++;\n    }\n\n    if (allowed) {\n        out_string(c, \"ERROR\");\n    } else {\n        out_string(c, \"OK\");\n    }\n}\n\nstatic void process_debugtime_command(conn *c, token_t *tokens, const size_t ntokens) {\n    if (strcmp(tokens[1].value, \"p\") == 0) {\n        if (!is_paused) {\n            is_paused = true;\n        }\n    } else if (strcmp(tokens[1].value, \"r\") == 0) {\n        if (is_paused) {\n            is_paused = false;\n        }\n    } else {\n        int64_t time_delta = 0;\n        if (!safe_strtoll(tokens[1].value, &time_delta)) {\n            out_string(c, \"ERROR\");\n            return;\n        }\n        delta += time_delta;\n        current_time += delta;\n    }\n    out_string(c, \"OK\");\n}\n\nstatic void process_debugitem_command(conn *c, token_t *tokens, const size_t ntokens) {\n    if (strcmp(tokens[1].value, \"lock\") == 0) {\n        uint32_t hv = hash(tokens[2].value, tokens[2].length);\n        item_lock(hv);\n    } else if (strcmp(tokens[1].value, \"unlock\") == 0) {\n        uint32_t hv = hash(tokens[2].value, tokens[2].length);\n        item_unlock(hv);\n    } else if (strcmp(tokens[1].value, \"ref\") == 0) {\n        // intentionally leak a reference.\n        item *it = item_get(tokens[2].value, tokens[2].length, c->thread, DONT_UPDATE);\n        if (it == NULL) {\n            out_string(c, \"MISS\");\n            return;\n        }\n    } else if (strcmp(tokens[1].value, \"unref\") == 0) {\n        // double unlink. debugger must have already ref'ed it or this\n        // underflows.\n        item *it = item_get(tokens[2].value, tokens[2].length, c->thread, DONT_UPDATE);\n        if (it == NULL) {\n            out_string(c, \"MISS\");\n            return;\n        }\n        do_item_remove(it);\n        do_item_remove(it);\n    } else {\n        out_string(c, \"ERROR\");\n        return;\n    }\n    out_string(c, \"OK\");\n}\n#endif\n\nstatic void process_slabs_automove_command(conn *c, token_t *tokens, const size_t ntokens) {\n    unsigned int level;\n    double ratio;\n\n    assert(c != NULL);\n\n    set_noreply_maybe(c, tokens, ntokens);\n\n    if (strcmp(tokens[2].value, \"ratio\") == 0) {\n        if (ntokens < 5 || !safe_strtod(tokens[3].value, &ratio)) {\n            out_string(c, \"ERROR\");\n            return;\n        }\n        // TODO: settings needs an overhaul... no locks/etc.\n        settings.slab_automove_ratio = ratio;\n        settings.slab_automove_version++;\n    } else if (strcmp(tokens[2].value, \"freeratio\") == 0) {\n        if (ntokens < 5 || !safe_strtod(tokens[3].value, &ratio)) {\n            out_string(c, \"ERROR\");\n            return;\n        }\n        settings.slab_automove_freeratio = ratio;\n        settings.slab_automove_version++;\n    } else if (strcmp(tokens[2].value, \"window\") == 0) {\n        if (ntokens < 5 || !safe_strtoul(tokens[3].value, (uint32_t*)&level)) {\n            out_string(c, \"CLIENT_ERROR bad command line format\");\n            return;\n        }\n\n        settings.slab_automove_window = level;\n        settings.slab_automove_version++;\n    } else {\n        if (!safe_strtoul(tokens[2].value, (uint32_t*)&level)) {\n            out_string(c, \"CLIENT_ERROR bad command line format\");\n            return;\n        }\n        if (level == 0) {\n            settings.slab_automove = 0;\n        } else if (level == 1 || level == 2) {\n            settings.slab_automove = level;\n        } else {\n            out_string(c, \"ERROR\");\n            return;\n        }\n    }\n    out_string(c, \"OK\");\n    return;\n}\n\n/* TODO: decide on syntax for sampling? */\nstatic void process_watch_command(conn *c, token_t *tokens, const size_t ntokens) {\n    uint16_t f = 0;\n    int x;\n    assert(c != NULL);\n\n    set_noreply_maybe(c, tokens, ntokens);\n    if (!settings.watch_enabled) {\n        out_string(c, \"CLIENT_ERROR watch commands not allowed\");\n        return;\n    }\n\n    if (resp_has_stack(c)) {\n        out_string(c, \"ERROR cannot pipeline other commands before watch\");\n        return;\n    }\n\n    if (ntokens > 2) {\n        for (x = COMMAND_TOKEN + 1; x < ntokens - 1; x++) {\n            if ((strcmp(tokens[x].value, \"rawcmds\") == 0)) {\n                f |= LOG_RAWCMDS;\n            } else if ((strcmp(tokens[x].value, \"evictions\") == 0)) {\n                f |= LOG_EVICTIONS;\n            } else if ((strcmp(tokens[x].value, \"fetchers\") == 0)) {\n                f |= LOG_FETCHERS;\n            } else if ((strcmp(tokens[x].value, \"mutations\") == 0)) {\n                f |= LOG_MUTATIONS;\n            } else if ((strcmp(tokens[x].value, \"sysevents\") == 0)) {\n                f |= LOG_SYSEVENTS;\n            } else if ((strcmp(tokens[x].value, \"connevents\") == 0)) {\n                f |= LOG_CONNEVENTS;\n            } else if ((strcmp(tokens[x].value, \"proxyreqs\") == 0)) {\n                f |= LOG_PROXYREQS;\n            } else if ((strcmp(tokens[x].value, \"proxyevents\") == 0)) {\n                f |= LOG_PROXYEVENTS;\n            } else if ((strcmp(tokens[x].value, \"proxyuser\") == 0)) {\n                f |= LOG_PROXYUSER;\n            } else if ((strcmp(tokens[x].value, \"deletions\") == 0)) {\n                f |= LOG_DELETIONS;\n            } else {\n                out_string(c, \"ERROR\");\n                return;\n            }\n        }\n    } else {\n        f |= LOG_FETCHERS;\n    }\n\n    switch(logger_add_watcher(c, c->sfd, f)) {\n        case LOGGER_ADD_WATCHER_TOO_MANY:\n            out_string(c, \"WATCHER_TOO_MANY log watcher limit reached\");\n            break;\n        case LOGGER_ADD_WATCHER_FAILED:\n            out_string(c, \"WATCHER_FAILED failed to add log watcher\");\n            break;\n        case LOGGER_ADD_WATCHER_OK:\n            conn_set_state(c, conn_watch);\n            event_del(&c->event);\n            break;\n    }\n}\n\nstatic void process_memlimit_command(conn *c, token_t *tokens, const size_t ntokens) {\n    uint32_t memlimit;\n    assert(c != NULL);\n\n    set_noreply_maybe(c, tokens, ntokens);\n\n    if (!safe_strtoul(tokens[1].value, &memlimit)) {\n        out_string(c, \"ERROR\");\n    } else {\n        if (memlimit < 8) {\n            out_string(c, \"MEMLIMIT_TOO_SMALL cannot set maxbytes to less than 8m\");\n        } else {\n            if (memlimit > 1000000000) {\n                out_string(c, \"MEMLIMIT_ADJUST_FAILED input value is megabytes not bytes\");\n            } else if (slabs_adjust_mem_limit((size_t) memlimit * 1024 * 1024)) {\n                if (settings.verbose > 0) {\n                    fprintf(stderr, \"maxbytes adjusted to %llum\\n\", (unsigned long long)memlimit);\n                }\n\n                out_string(c, \"OK\");\n            } else {\n                out_string(c, \"MEMLIMIT_ADJUST_FAILED out of bounds or unable to adjust\");\n            }\n        }\n    }\n}\n\nstatic void process_lru_command(conn *c, token_t *tokens, const size_t ntokens) {\n    uint32_t pct_hot;\n    uint32_t pct_warm;\n    double hot_factor;\n    int32_t ttl;\n    double factor;\n\n    set_noreply_maybe(c, tokens, ntokens);\n\n    if (strcmp(tokens[1].value, \"tune\") == 0 && ntokens >= 7) {\n        if (!safe_strtoul(tokens[2].value, &pct_hot) ||\n            !safe_strtoul(tokens[3].value, &pct_warm) ||\n            !safe_strtod(tokens[4].value, &hot_factor) ||\n            !safe_strtod(tokens[5].value, &factor)) {\n            out_string(c, \"ERROR\");\n        } else {\n            if (pct_hot + pct_warm > 80) {\n                out_string(c, \"ERROR hot and warm pcts must not exceed 80\");\n            } else if (factor <= 0 || hot_factor <= 0) {\n                out_string(c, \"ERROR hot/warm age factors must be greater than 0\");\n            } else {\n                settings.hot_lru_pct = pct_hot;\n                settings.warm_lru_pct = pct_warm;\n                settings.hot_max_factor = hot_factor;\n                settings.warm_max_factor = factor;\n                out_string(c, \"OK\");\n            }\n        }\n    } else if (strcmp(tokens[1].value, \"mode\") == 0 && ntokens >= 4 &&\n               settings.lru_maintainer_thread) {\n        if (strcmp(tokens[2].value, \"flat\") == 0) {\n            settings.lru_segmented = false;\n            out_string(c, \"OK\");\n        } else if (strcmp(tokens[2].value, \"segmented\") == 0) {\n            settings.lru_segmented = true;\n            out_string(c, \"OK\");\n        } else {\n            out_string(c, \"ERROR\");\n        }\n    } else if (strcmp(tokens[1].value, \"temp_ttl\") == 0 && ntokens >= 4 &&\n               settings.lru_maintainer_thread) {\n        if (!safe_strtol(tokens[2].value, &ttl)) {\n            out_string(c, \"ERROR\");\n        } else {\n            if (ttl < 0) {\n                settings.temp_lru = false;\n            } else {\n                settings.temp_lru = true;\n                settings.temporary_ttl = ttl;\n            }\n            out_string(c, \"OK\");\n        }\n    } else {\n        out_string(c, \"ERROR\");\n    }\n}\n#ifdef EXTSTORE\nstatic void process_extstore_command(conn *c, token_t *tokens, const size_t ntokens) {\n    set_noreply_maybe(c, tokens, ntokens);\n    bool ok = true;\n    if (ntokens < 4) {\n        ok = false;\n    } else if (strcmp(tokens[1].value, \"free_memchunks\") == 0 && ntokens > 4) {\n        // setting is deprecated and ignored, but accepted for backcompat\n        unsigned int clsid = 0;\n        unsigned int limit = 0;\n        if (!safe_strtoul(tokens[2].value, &clsid) ||\n                !safe_strtoul(tokens[3].value, &limit)) {\n            ok = false;\n        } else {\n            if (clsid < MAX_NUMBER_OF_SLAB_CLASSES) {\n                ok = true;\n            } else {\n                ok = false;\n            }\n        }\n    } else if (strcmp(tokens[1].value, \"item_size\") == 0) {\n        if (safe_strtoul(tokens[2].value, &settings.ext_item_size)) {\n            settings.slab_automove_version++;\n        } else {\n            ok = false;\n        }\n    } else if (strcmp(tokens[1].value, \"item_age\") == 0) {\n        if (!safe_strtoul(tokens[2].value, &settings.ext_item_age))\n            ok = false;\n    } else if (strcmp(tokens[1].value, \"low_ttl\") == 0) {\n        if (!safe_strtoul(tokens[2].value, &settings.ext_low_ttl))\n            ok = false;\n    } else if (strcmp(tokens[1].value, \"recache_rate\") == 0) {\n        if (!safe_strtoul(tokens[2].value, &settings.ext_recache_rate))\n            ok = false;\n    } else if (strcmp(tokens[1].value, \"compact_under\") == 0) {\n        if (!safe_strtoul(tokens[2].value, &settings.ext_compact_under))\n            ok = false;\n    } else if (strcmp(tokens[1].value, \"drop_under\") == 0) {\n        if (!safe_strtoul(tokens[2].value, &settings.ext_drop_under))\n            ok = false;\n    } else if (strcmp(tokens[1].value, \"max_sleep\") == 0) {\n        if (!safe_strtoul(tokens[2].value, &settings.ext_max_sleep))\n            ok = false;\n    } else if (strcmp(tokens[1].value, \"max_frag\") == 0) {\n        if (!safe_strtod(tokens[2].value, &settings.ext_max_frag))\n            ok = false;\n    } else if (strcmp(tokens[1].value, \"drop_unread\") == 0) {\n        unsigned int v;\n        if (!safe_strtoul(tokens[2].value, &v)) {\n            ok = false;\n        } else {\n            settings.ext_drop_unread = v == 0 ? false : true;\n        }\n    } else {\n        ok = false;\n    }\n    if (!ok) {\n        out_string(c, \"ERROR\");\n    } else {\n        out_string(c, \"OK\");\n    }\n}\n#endif\nstatic void process_flush_all_command(conn *c, token_t *tokens, const size_t ntokens) {\n    int32_t exptime = 0;\n    rel_time_t new_oldest = 0;\n\n    set_noreply_maybe(c, tokens, ntokens);\n\n    pthread_mutex_lock(&c->thread->stats.mutex);\n    c->thread->stats.flush_cmds++;\n    pthread_mutex_unlock(&c->thread->stats.mutex);\n\n    if (!settings.flush_enabled) {\n        // flush_all is not allowed but we log it on stats\n        out_string(c, \"CLIENT_ERROR flush_all not allowed\");\n        return;\n    }\n\n    if (ntokens != (c->noreply ? 3 : 2)) {\n        if (!safe_strtol(tokens[1].value, &exptime)) {\n            out_string(c, \"CLIENT_ERROR invalid exptime argument\");\n            return;\n        }\n    }\n\n    /*\n      If exptime is zero realtime() would return zero too, and\n      realtime(exptime) - 1 would overflow to the max unsigned\n      value.  So we process exptime == 0 the same way we do when\n      no delay is given at all.\n    */\n    if (exptime > 0) {\n        new_oldest = realtime(exptime) - 1;\n    } else { /* exptime == 0 */\n        new_oldest = current_time - 1;\n    }\n\n    settings.oldest_live = new_oldest;\n    item_flush_expired();\n    out_string(c, \"OK\");\n}\n\nstatic void process_version_command(conn *c) {\n    out_string(c, \"VERSION \" VERSION);\n}\n\nstatic void process_quit_command(conn *c) {\n    conn_set_state(c, conn_mwrite);\n    c->close_after_write = true;\n    c->close_reason = NORMAL_CLOSE;\n}\n\nstatic void process_shutdown_command(conn *c, token_t *tokens, const size_t ntokens) {\n    if (!settings.shutdown_command) {\n        out_string(c, \"ERROR: shutdown not enabled\");\n        return;\n    }\n\n    if (ntokens == 2) {\n        c->close_reason = SHUTDOWN_CLOSE;\n        conn_set_state(c, conn_closing);\n        raise(SIGINT);\n    } else if (ntokens == 3 && strcmp(tokens[SUBCOMMAND_TOKEN].value, \"graceful\") == 0) {\n        c->close_reason = SHUTDOWN_CLOSE;\n        conn_set_state(c, conn_closing);\n        raise(SIGUSR1);\n    } else {\n        out_string(c, \"CLIENT_ERROR invalid shutdown mode\");\n    }\n}\n\nstatic void process_slabs_command(conn *c, token_t *tokens, const size_t ntokens) {\n    if (ntokens == 5 && strcmp(tokens[COMMAND_TOKEN + 1].value, \"reassign\") == 0) {\n        int src, dst, rv;\n\n        if (settings.slab_reassign == false) {\n            out_string(c, \"CLIENT_ERROR slab reassignment disabled\");\n            return;\n        }\n\n        if (! (safe_strtol(tokens[2].value, (int32_t*)&src)\n               && safe_strtol(tokens[3].value, (int32_t*)&dst))) {\n            out_string(c, \"CLIENT_ERROR bad command line format\");\n            return;\n        }\n\n        rv = slabs_reassign(settings.slab_rebal, src, dst, SLABS_REASSIGN_ALLOW_EVICTIONS);\n        switch (rv) {\n        case REASSIGN_OK:\n            out_string(c, \"OK\");\n            break;\n        case REASSIGN_RUNNING:\n            out_string(c, \"BUSY currently processing reassign request\");\n            break;\n        case REASSIGN_BADCLASS:\n            out_string(c, \"BADCLASS invalid src or dst class id\");\n            break;\n        case REASSIGN_NOSPARE:\n            out_string(c, \"NOSPARE source class has no spare pages\");\n            break;\n        case REASSIGN_SRC_DST_SAME:\n            out_string(c, \"SAME src and dst class are identical\");\n            break;\n        }\n        return;\n    } else if (ntokens >= 4 &&\n        (strcmp(tokens[COMMAND_TOKEN + 1].value, \"automove\") == 0)) {\n        process_slabs_automove_command(c, tokens, ntokens);\n    } else {\n        out_string(c, \"ERROR\");\n    }\n}\n\nstatic void process_lru_crawler_command(conn *c, token_t *tokens, const size_t ntokens) {\n    if (ntokens == 4 && strcmp(tokens[COMMAND_TOKEN + 1].value, \"crawl\") == 0) {\n        int rv;\n        if (settings.lru_crawler == false) {\n            out_string(c, \"CLIENT_ERROR lru crawler disabled\");\n            return;\n        }\n\n        rv = lru_crawler_crawl(tokens[2].value, CRAWLER_EXPIRED, NULL, 0,\n                settings.lru_crawler_tocrawl);\n        switch(rv) {\n        case CRAWLER_OK:\n            out_string(c, \"OK\");\n            break;\n        case CRAWLER_RUNNING:\n            out_string(c, \"BUSY currently processing crawler request\");\n            break;\n        case CRAWLER_BADCLASS:\n            out_string(c, \"BADCLASS invalid class id\");\n            break;\n        case CRAWLER_NOTSTARTED:\n            out_string(c, \"NOTSTARTED no items to crawl\");\n            break;\n        case CRAWLER_ERROR:\n            out_string(c, \"ERROR an unknown error happened\");\n            break;\n        }\n        return;\n    } else if (ntokens == 4 && strcmp(tokens[COMMAND_TOKEN + 1].value, \"metadump\") == 0) {\n        if (settings.lru_crawler == false) {\n            out_string(c, \"CLIENT_ERROR lru crawler disabled\");\n            return;\n        }\n        if (!settings.dump_enabled) {\n            out_string(c, \"ERROR metadump not allowed\");\n            return;\n        }\n        if (resp_has_stack(c)) {\n            out_string(c, \"ERROR cannot pipeline other commands before metadump\");\n            return;\n        }\n\n        int rv = lru_crawler_crawl(tokens[2].value, CRAWLER_METADUMP,\n                c, c->sfd, LRU_CRAWLER_CAP_REMAINING);\n        switch(rv) {\n            case CRAWLER_OK:\n                // TODO: documentation says this string is returned, but\n                // it never was before. We never switch to conn_write so\n                // this o_s call never worked. Need to talk to users and\n                // decide if removing the OK from docs is fine.\n                //out_string(c, \"OK\");\n                // TODO: Don't reuse conn_watch here.\n                conn_set_state(c, conn_watch);\n                event_del(&c->event);\n                break;\n            case CRAWLER_RUNNING:\n                out_string(c, \"BUSY currently processing crawler request\");\n                break;\n            case CRAWLER_BADCLASS:\n                out_string(c, \"BADCLASS invalid class id\");\n                break;\n            case CRAWLER_NOTSTARTED:\n                out_string(c, \"NOTSTARTED no items to crawl\");\n                break;\n            case CRAWLER_ERROR:\n                out_string(c, \"ERROR an unknown error happened\");\n                break;\n        }\n        return;\n    } else if (ntokens == 4 && strcmp(tokens[COMMAND_TOKEN + 1].value, \"mgdump\") == 0) {\n        if (settings.lru_crawler == false) {\n            out_string(c, \"CLIENT_ERROR lru crawler disabled\");\n            return;\n        }\n        if (!settings.dump_enabled) {\n            out_string(c, \"ERROR key dump not allowed\");\n            return;\n        }\n        if (resp_has_stack(c)) {\n            out_string(c, \"ERROR cannot pipeline other commands before mgdump\");\n            return;\n        }\n\n        int rv = lru_crawler_crawl(tokens[2].value, CRAWLER_MGDUMP,\n                c, c->sfd, LRU_CRAWLER_CAP_REMAINING);\n        switch(rv) {\n            case CRAWLER_OK:\n                conn_set_state(c, conn_watch);\n                event_del(&c->event);\n                break;\n            case CRAWLER_RUNNING:\n                out_string(c, \"BUSY currently processing crawler request\");\n                break;\n            case CRAWLER_BADCLASS:\n                out_string(c, \"BADCLASS invalid class id\");\n                break;\n            case CRAWLER_NOTSTARTED:\n                out_string(c, \"NOTSTARTED no items to crawl\");\n                break;\n            case CRAWLER_ERROR:\n                out_string(c, \"ERROR an unknown error happened\");\n                break;\n        }\n        return;\n    } else if (ntokens == 4 && strcmp(tokens[COMMAND_TOKEN + 1].value, \"tocrawl\") == 0) {\n        uint32_t tocrawl;\n         if (!safe_strtoul(tokens[2].value, &tocrawl)) {\n            out_string(c, \"CLIENT_ERROR bad command line format\");\n            return;\n        }\n        settings.lru_crawler_tocrawl = tocrawl;\n        out_string(c, \"OK\");\n        return;\n    } else if (ntokens == 4 && strcmp(tokens[COMMAND_TOKEN + 1].value, \"sleep\") == 0) {\n        uint32_t tosleep;\n        if (!safe_strtoul(tokens[2].value, &tosleep)) {\n            out_string(c, \"CLIENT_ERROR bad command line format\");\n            return;\n        }\n        if (tosleep > 1000000) {\n            out_string(c, \"CLIENT_ERROR sleep must be one second or less\");\n            return;\n        }\n        settings.lru_crawler_sleep = tosleep;\n        out_string(c, \"OK\");\n        return;\n    } else if (ntokens == 3) {\n        if ((strcmp(tokens[COMMAND_TOKEN + 1].value, \"enable\") == 0)) {\n            if (start_item_crawler_thread() == 0) {\n                out_string(c, \"OK\");\n            } else {\n                out_string(c, \"ERROR failed to start lru crawler thread\");\n            }\n        } else if ((strcmp(tokens[COMMAND_TOKEN + 1].value, \"disable\") == 0)) {\n            if (stop_item_crawler_thread(CRAWLER_NOWAIT) == 0) {\n                out_string(c, \"OK\");\n            } else {\n                out_string(c, \"ERROR failed to stop lru crawler thread\");\n            }\n        } else {\n            out_string(c, \"ERROR\");\n        }\n        return;\n    } else {\n        out_string(c, \"ERROR\");\n    }\n}\n#ifdef TLS\nstatic void process_refresh_certs_command(conn *c, token_t *tokens, const size_t ntokens) {\n    set_noreply_maybe(c, tokens, ntokens);\n    char *errmsg = NULL;\n    if (refresh_certs(&errmsg)) {\n        out_string(c, \"OK\");\n    } else {\n        write_and_free(c, errmsg, strlen(errmsg));\n    }\n    return;\n}\n#endif\n\n// TODO: pipelined commands are incompatible with shifting connections to a\n// side thread. Given this only happens in two instances (watch and\n// lru_crawler metadump) it should be fine for things to bail. It _should_ be\n// unusual for these commands.\n// This is hard to fix since tokenize_command() mutilates the read buffer, so\n// we can't drop out and back in again.\n// Leaving this note here to spend more time on a fix when necessary, or if an\n// opportunity becomes obvious.\nvoid process_command_ascii(conn *c, char *command) {\n\n    token_t tokens[MAX_TOKENS];\n    size_t ntokens;\n    int comm;\n\n    assert(c != NULL);\n\n    MEMCACHED_PROCESS_COMMAND_START(c->sfd, c->rcurr, c->rbytes);\n\n    if (settings.verbose > 1)\n        fprintf(stderr, \"<%d %s\\n\", c->sfd, command);\n\n    /*\n     * for commands set/add/replace, we build an item and read the data\n     * directly into it, then continue in nread_complete().\n     */\n\n    // Prep the response object for this query.\n    if (!resp_start(c)) {\n        conn_set_state(c, conn_closing);\n        return;\n    }\n\n    c->thread->cur_sfd = c->sfd; // cuddle sfd for logging.\n    ntokens = tokenize_command(command, tokens, MAX_TOKENS);\n    // All commands need a minimum of two tokens: cmd and NULL finalizer\n    // There are also no valid commands shorter than two bytes.\n    if (ntokens < 2 || tokens[COMMAND_TOKEN].length < 2) {\n        out_string(c, \"ERROR\");\n        return;\n    }\n\n    // Meta commands are all 2-char in length.\n    char first = tokens[COMMAND_TOKEN].value[0];\n    if (first == 'm' && tokens[COMMAND_TOKEN].length == 2) {\n        switch (tokens[COMMAND_TOKEN].value[1]) {\n            case 'g':\n                process_mget_command(c, tokens, ntokens);\n                break;\n            case 's':\n                process_mset_command(c, tokens, ntokens);\n                break;\n            case 'd':\n                process_mdelete_command(c, tokens, ntokens);\n                break;\n            case 'n':\n                out_string(c, \"MN\");\n                // mn command forces immediate writeback flush.\n                conn_set_state(c, conn_mwrite);\n                break;\n            case 'a':\n                process_marithmetic_command(c, tokens, ntokens);\n                break;\n            case 'e':\n                process_meta_command(c, tokens, ntokens);\n                break;\n            default:\n                out_string(c, \"ERROR\");\n                break;\n        }\n    } else if (first == 'g') {\n        // Various get commands are very common.\n        WANT_TOKENS_MIN(ntokens, 3);\n        if (strcmp(tokens[COMMAND_TOKEN].value, \"get\") == 0) {\n\n            process_get_command(c, tokens, ntokens, false, false);\n        } else if (strcmp(tokens[COMMAND_TOKEN].value, \"gets\") == 0) {\n\n            process_get_command(c, tokens, ntokens, true, false);\n        } else if (strcmp(tokens[COMMAND_TOKEN].value, \"gat\") == 0) {\n\n            process_get_command(c, tokens, ntokens, false, true);\n        } else if (strcmp(tokens[COMMAND_TOKEN].value, \"gats\") == 0) {\n\n            process_get_command(c, tokens, ntokens, true, true);\n        } else {\n            out_string(c, \"ERROR\");\n        }\n    } else if (first == 's') {\n        if (strcmp(tokens[COMMAND_TOKEN].value, \"set\") == 0 && (comm = NREAD_SET)) {\n\n            WANT_TOKENS_OR(ntokens, 6, 7);\n            process_update_command(c, tokens, ntokens, comm, false);\n        } else if (strcmp(tokens[COMMAND_TOKEN].value, \"stats\") == 0) {\n\n            process_stat(c, tokens, ntokens);\n        } else if (strcmp(tokens[COMMAND_TOKEN].value, \"shutdown\") == 0) {\n\n            process_shutdown_command(c, tokens, ntokens);\n        } else if (strcmp(tokens[COMMAND_TOKEN].value, \"slabs\") == 0) {\n\n            process_slabs_command(c, tokens, ntokens);\n        } else {\n            out_string(c, \"ERROR\");\n        }\n    } else if (first == 'a') {\n        if ((strcmp(tokens[COMMAND_TOKEN].value, \"add\") == 0 && (comm = NREAD_ADD)) ||\n            (strcmp(tokens[COMMAND_TOKEN].value, \"append\") == 0 && (comm = NREAD_APPEND)) ) {\n\n            WANT_TOKENS_OR(ntokens, 6, 7);\n            process_update_command(c, tokens, ntokens, comm, false);\n        } else {\n            out_string(c, \"ERROR\");\n        }\n    } else if (first == 'c') {\n        if (strcmp(tokens[COMMAND_TOKEN].value, \"cas\") == 0 && (comm = NREAD_CAS)) {\n\n            WANT_TOKENS_OR(ntokens, 7, 8);\n            process_update_command(c, tokens, ntokens, comm, true);\n        } else if (strcmp(tokens[COMMAND_TOKEN].value, \"cache_memlimit\") == 0) {\n\n            WANT_TOKENS_OR(ntokens, 3, 4);\n            process_memlimit_command(c, tokens, ntokens);\n        } else {\n            out_string(c, \"ERROR\");\n        }\n    } else if (first == 'i') {\n        if (strcmp(tokens[COMMAND_TOKEN].value, \"incr\") == 0) {\n\n            WANT_TOKENS_OR(ntokens, 4, 5);\n            process_arithmetic_command(c, tokens, ntokens, 1);\n        } else {\n            out_string(c, \"ERROR\");\n        }\n    } else if (first == 'd') {\n        if (strcmp(tokens[COMMAND_TOKEN].value, \"delete\") == 0) {\n\n            WANT_TOKENS(ntokens, 3, 5);\n            process_delete_command(c, tokens, ntokens);\n        } else if (strcmp(tokens[COMMAND_TOKEN].value, \"decr\") == 0) {\n\n            WANT_TOKENS_OR(ntokens, 4, 5);\n            process_arithmetic_command(c, tokens, ntokens, 0);\n#ifdef MEMCACHED_DEBUG\n        } else if (strcmp(tokens[COMMAND_TOKEN].value, \"debugtime\") == 0) {\n            WANT_TOKENS_MIN(ntokens, 2);\n            process_debugtime_command(c, tokens, ntokens);\n        } else if (strcmp(tokens[COMMAND_TOKEN].value, \"debugitem\") == 0) {\n            WANT_TOKENS_MIN(ntokens, 2);\n            process_debugitem_command(c, tokens, ntokens);\n#endif\n        } else {\n            out_string(c, \"ERROR\");\n        }\n    } else if (first == 't') {\n        if (strcmp(tokens[COMMAND_TOKEN].value, \"touch\") == 0) {\n\n            WANT_TOKENS_OR(ntokens, 4, 5);\n            process_touch_command(c, tokens, ntokens);\n        } else {\n            out_string(c, \"ERROR\");\n        }\n    } else if (\n                (strcmp(tokens[COMMAND_TOKEN].value, \"replace\") == 0 && (comm = NREAD_REPLACE)) ||\n                (strcmp(tokens[COMMAND_TOKEN].value, \"prepend\") == 0 && (comm = NREAD_PREPEND)) ) {\n\n        WANT_TOKENS_OR(ntokens, 6, 7);\n        process_update_command(c, tokens, ntokens, comm, false);\n\n    } else if (strcmp(tokens[COMMAND_TOKEN].value, \"bget\") == 0) {\n        // ancient \"binary get\" command which isn't in any documentation, was\n        // removed > 10 years ago, etc. Keeping for compatibility reasons but\n        // we should look deeper into client code and remove this.\n        WANT_TOKENS_MIN(ntokens, 3);\n        process_get_command(c, tokens, ntokens, false, false);\n\n    } else if (strcmp(tokens[COMMAND_TOKEN].value, \"flush_all\") == 0) {\n\n        WANT_TOKENS(ntokens, 2, 4);\n        process_flush_all_command(c, tokens, ntokens);\n\n    } else if (strcmp(tokens[COMMAND_TOKEN].value, \"version\") == 0) {\n\n        process_version_command(c);\n\n    } else if (strcmp(tokens[COMMAND_TOKEN].value, \"quit\") == 0) {\n\n        process_quit_command(c);\n\n    } else if (strcmp(tokens[COMMAND_TOKEN].value, \"lru_crawler\") == 0) {\n\n        process_lru_crawler_command(c, tokens, ntokens);\n\n    } else if (strcmp(tokens[COMMAND_TOKEN].value, \"watch\") == 0) {\n\n        process_watch_command(c, tokens, ntokens);\n\n    } else if (strcmp(tokens[COMMAND_TOKEN].value, \"verbosity\") == 0) {\n        WANT_TOKENS_OR(ntokens, 3, 4);\n        process_verbosity_command(c, tokens, ntokens);\n    } else if (strcmp(tokens[COMMAND_TOKEN].value, \"lru\") == 0) {\n        WANT_TOKENS_MIN(ntokens, 3);\n        process_lru_command(c, tokens, ntokens);\n#ifdef MEMCACHED_DEBUG\n    // commands which exist only for testing the memcached's security protection\n    } else if (strcmp(tokens[COMMAND_TOKEN].value, \"misbehave\") == 0) {\n        process_misbehave_command(c);\n#endif\n#ifdef EXTSTORE\n    } else if (strcmp(tokens[COMMAND_TOKEN].value, \"extstore\") == 0) {\n        WANT_TOKENS_MIN(ntokens, 3);\n        process_extstore_command(c, tokens, ntokens);\n#endif\n#ifdef TLS\n    } else if (strcmp(tokens[COMMAND_TOKEN].value, \"refresh_certs\") == 0) {\n        process_refresh_certs_command(c, tokens, ntokens);\n#endif\n    } else {\n        if (strncmp(tokens[ntokens - 2].value, \"HTTP/\", 5) == 0) {\n            conn_set_state(c, conn_closing);\n        } else {\n            out_string(c, \"ERROR\");\n        }\n    }\n    return;\n}\n\n\n"
        },
        {
          "name": "proto_text.h",
          "type": "blob",
          "size": 0.240234375,
          "content": "#ifndef PROTO_TEXT_H\n#define PROTO_TEXT_H\n\n/* text protocol handlers */\nvoid complete_nread_ascii(conn *c);\nint try_read_command_asciiauth(conn *c);\nint try_read_command_ascii(conn *c);\nvoid process_command_ascii(conn *c, char *command);\n\n#endif\n"
        },
        {
          "name": "protocol_binary.h",
          "type": "blob",
          "size": 16.1376953125,
          "content": "/*\n * Copyright (c) <2008>, Sun Microsystems, Inc.\n * All rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n *     * Redistributions of source code must retain the above copyright\n *       notice, this list of conditions and the following disclaimer.\n *     * Redistributions in binary form must reproduce the above copyright\n *       notice, this list of conditions and the following disclaimer in the\n *       documentation and/or other materials provided with the distribution.\n *     * Neither the name of the  nor the\n *       names of its contributors may be used to endorse or promote products\n *       derived from this software without specific prior written permission.\n *\n * THIS SOFTWARE IS PROVIDED BY SUN MICROSYSTEMS, INC. ``AS IS'' AND ANY\n * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n * DISCLAIMED. IN NO EVENT SHALL SUN MICROSYSTEMS, INC. BE LIABLE FOR ANY\n * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF\n * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n */\n/*\n * Summary: Constants used by to implement the binary protocol.\n *\n * Copy: See Copyright for the status of this software.\n *\n * Author: Trond Norbye <trond.norbye@sun.com>\n */\n\n#ifndef PROTOCOL_BINARY_H\n#define PROTOCOL_BINARY_H\n\n/**\n * This file contains definitions of the constants and packet formats\n * defined in the binary specification. Please note that you _MUST_ remember\n * to convert each multibyte field to / from network byte order to / from\n * host order.\n */\n#ifdef __cplusplus\nextern \"C\"\n{\n#endif\n\n    /**\n     * Definition of the legal \"magic\" values used in a packet.\n     * See section 3.1 Magic byte\n     */\n    typedef enum {\n        PROTOCOL_BINARY_REQ = 0x80,\n        PROTOCOL_BINARY_RES = 0x81\n    } protocol_binary_magic;\n\n    /**\n     * Definition of the valid response status numbers.\n     * See section 3.2 Response Status\n     */\n    typedef enum {\n        PROTOCOL_BINARY_RESPONSE_SUCCESS = 0x00,\n        PROTOCOL_BINARY_RESPONSE_KEY_ENOENT = 0x01,\n        PROTOCOL_BINARY_RESPONSE_KEY_EEXISTS = 0x02,\n        PROTOCOL_BINARY_RESPONSE_E2BIG = 0x03,\n        PROTOCOL_BINARY_RESPONSE_EINVAL = 0x04,\n        PROTOCOL_BINARY_RESPONSE_NOT_STORED = 0x05,\n        PROTOCOL_BINARY_RESPONSE_DELTA_BADVAL = 0x06,\n        PROTOCOL_BINARY_RESPONSE_AUTH_ERROR = 0x20,\n        PROTOCOL_BINARY_RESPONSE_AUTH_CONTINUE = 0x21,\n        PROTOCOL_BINARY_RESPONSE_UNKNOWN_COMMAND = 0x81,\n        PROTOCOL_BINARY_RESPONSE_ENOMEM = 0x82\n    } protocol_binary_response_status;\n\n    /**\n     * Definition of the different command opcodes.\n     * See section 3.3 Command Opcodes\n     */\n    typedef enum {\n        PROTOCOL_BINARY_CMD_GET = 0x00,\n        PROTOCOL_BINARY_CMD_SET = 0x01,\n        PROTOCOL_BINARY_CMD_ADD = 0x02,\n        PROTOCOL_BINARY_CMD_REPLACE = 0x03,\n        PROTOCOL_BINARY_CMD_DELETE = 0x04,\n        PROTOCOL_BINARY_CMD_INCREMENT = 0x05,\n        PROTOCOL_BINARY_CMD_DECREMENT = 0x06,\n        PROTOCOL_BINARY_CMD_QUIT = 0x07,\n        PROTOCOL_BINARY_CMD_FLUSH = 0x08,\n        PROTOCOL_BINARY_CMD_GETQ = 0x09,\n        PROTOCOL_BINARY_CMD_NOOP = 0x0a,\n        PROTOCOL_BINARY_CMD_VERSION = 0x0b,\n        PROTOCOL_BINARY_CMD_GETK = 0x0c,\n        PROTOCOL_BINARY_CMD_GETKQ = 0x0d,\n        PROTOCOL_BINARY_CMD_APPEND = 0x0e,\n        PROTOCOL_BINARY_CMD_PREPEND = 0x0f,\n        PROTOCOL_BINARY_CMD_STAT = 0x10,\n        PROTOCOL_BINARY_CMD_SETQ = 0x11,\n        PROTOCOL_BINARY_CMD_ADDQ = 0x12,\n        PROTOCOL_BINARY_CMD_REPLACEQ = 0x13,\n        PROTOCOL_BINARY_CMD_DELETEQ = 0x14,\n        PROTOCOL_BINARY_CMD_INCREMENTQ = 0x15,\n        PROTOCOL_BINARY_CMD_DECREMENTQ = 0x16,\n        PROTOCOL_BINARY_CMD_QUITQ = 0x17,\n        PROTOCOL_BINARY_CMD_FLUSHQ = 0x18,\n        PROTOCOL_BINARY_CMD_APPENDQ = 0x19,\n        PROTOCOL_BINARY_CMD_PREPENDQ = 0x1a,\n        PROTOCOL_BINARY_CMD_TOUCH = 0x1c,\n        PROTOCOL_BINARY_CMD_GAT = 0x1d,\n        PROTOCOL_BINARY_CMD_GATQ = 0x1e,\n        PROTOCOL_BINARY_CMD_GATK = 0x23,\n        PROTOCOL_BINARY_CMD_GATKQ = 0x24,\n\n        PROTOCOL_BINARY_CMD_SASL_LIST_MECHS = 0x20,\n        PROTOCOL_BINARY_CMD_SASL_AUTH = 0x21,\n        PROTOCOL_BINARY_CMD_SASL_STEP = 0x22,\n\n        /* These commands are used for range operations and exist within\n         * this header for use in other projects.  Range operations are\n         * not expected to be implemented in the memcached server itself.\n         */\n        PROTOCOL_BINARY_CMD_RGET      = 0x30,\n        PROTOCOL_BINARY_CMD_RSET      = 0x31,\n        PROTOCOL_BINARY_CMD_RSETQ     = 0x32,\n        PROTOCOL_BINARY_CMD_RAPPEND   = 0x33,\n        PROTOCOL_BINARY_CMD_RAPPENDQ  = 0x34,\n        PROTOCOL_BINARY_CMD_RPREPEND  = 0x35,\n        PROTOCOL_BINARY_CMD_RPREPENDQ = 0x36,\n        PROTOCOL_BINARY_CMD_RDELETE   = 0x37,\n        PROTOCOL_BINARY_CMD_RDELETEQ  = 0x38,\n        PROTOCOL_BINARY_CMD_RINCR     = 0x39,\n        PROTOCOL_BINARY_CMD_RINCRQ    = 0x3a,\n        PROTOCOL_BINARY_CMD_RDECR     = 0x3b,\n        PROTOCOL_BINARY_CMD_RDECRQ    = 0x3c\n        /* End Range operations */\n\n    } protocol_binary_command;\n\n    /**\n     * Definition of the data types in the packet\n     * See section 3.4 Data Types\n     */\n    typedef enum {\n        PROTOCOL_BINARY_RAW_BYTES = 0x00\n    } protocol_binary_datatypes;\n\n    /**\n     * Definition of the header structure for a request packet.\n     * See section 2\n     */\n    typedef union {\n        struct {\n            uint8_t magic;\n            uint8_t opcode;\n            uint16_t keylen;\n            uint8_t extlen;\n            uint8_t datatype;\n            uint16_t reserved;\n            uint32_t bodylen;\n            uint32_t opaque;\n            uint64_t cas;\n        } request;\n        uint8_t bytes[24];\n    } protocol_binary_request_header;\n\n    /**\n     * Definition of the header structure for a response packet.\n     * See section 2\n     */\n    typedef union {\n        struct {\n            uint8_t magic;\n            uint8_t opcode;\n            uint16_t keylen;\n            uint8_t extlen;\n            uint8_t datatype;\n            uint16_t status;\n            uint32_t bodylen;\n            uint32_t opaque;\n            uint64_t cas;\n        } response;\n        uint8_t bytes[24];\n    } protocol_binary_response_header;\n\n    /**\n     * Definition of a request-packet containing no extras\n     */\n    typedef union {\n        struct {\n            protocol_binary_request_header header;\n        } message;\n        uint8_t bytes[sizeof(protocol_binary_request_header)];\n    } protocol_binary_request_no_extras;\n\n    /**\n     * Definition of a response-packet containing no extras\n     */\n    typedef union {\n        struct {\n            protocol_binary_response_header header;\n        } message;\n        uint8_t bytes[sizeof(protocol_binary_response_header)];\n    } protocol_binary_response_no_extras;\n\n    /**\n     * Definition of the packet used by the get, getq, getk and getkq command.\n     * See section 4\n     */\n    typedef protocol_binary_request_no_extras protocol_binary_request_get;\n    typedef protocol_binary_request_no_extras protocol_binary_request_getq;\n    typedef protocol_binary_request_no_extras protocol_binary_request_getk;\n    typedef protocol_binary_request_no_extras protocol_binary_request_getkq;\n\n    /**\n     * Definition of the packet returned from a successful get, getq, getk and\n     * getkq.\n     * See section 4\n     */\n    typedef union {\n        struct {\n            protocol_binary_response_header header;\n            struct {\n                uint32_t flags;\n            } body;\n        } message;\n        uint8_t bytes[sizeof(protocol_binary_response_header) + 4];\n    } protocol_binary_response_get;\n\n    typedef protocol_binary_response_get protocol_binary_response_getq;\n    typedef protocol_binary_response_get protocol_binary_response_getk;\n    typedef protocol_binary_response_get protocol_binary_response_getkq;\n\n    /**\n     * Definition of the packet used by the delete command\n     * See section 4\n     */\n    typedef protocol_binary_request_no_extras protocol_binary_request_delete;\n\n    /**\n     * Definition of the packet returned by the delete command\n     * See section 4\n     */\n    typedef protocol_binary_response_no_extras protocol_binary_response_delete;\n\n    /**\n     * Definition of the packet used by the flush command\n     * See section 4\n     * Please note that the expiration field is optional, so remember to see\n     * check the header.bodysize to see if it is present.\n     */\n    typedef union {\n        struct {\n            protocol_binary_request_header header;\n            struct {\n                uint32_t expiration;\n            } body;\n        } message;\n        uint8_t bytes[sizeof(protocol_binary_request_header) + 4];\n    } protocol_binary_request_flush;\n\n    /**\n     * Definition of the packet returned by the flush command\n     * See section 4\n     */\n    typedef protocol_binary_response_no_extras protocol_binary_response_flush;\n\n    /**\n     * Definition of the packet used by set, add and replace\n     * See section 4\n     */\n    typedef union {\n        struct {\n            protocol_binary_request_header header;\n            struct {\n                uint32_t flags;\n                uint32_t expiration;\n            } body;\n        } message;\n        uint8_t bytes[sizeof(protocol_binary_request_header) + 8];\n    } protocol_binary_request_set;\n    typedef protocol_binary_request_set protocol_binary_request_add;\n    typedef protocol_binary_request_set protocol_binary_request_replace;\n\n    /**\n     * Definition of the packet returned by set, add and replace\n     * See section 4\n     */\n    typedef protocol_binary_response_no_extras protocol_binary_response_set;\n    typedef protocol_binary_response_no_extras protocol_binary_response_add;\n    typedef protocol_binary_response_no_extras protocol_binary_response_replace;\n\n    /**\n     * Definition of the noop packet\n     * See section 4\n     */\n    typedef protocol_binary_request_no_extras protocol_binary_request_noop;\n\n    /**\n     * Definition of the packet returned by the noop command\n     * See section 4\n     */\n    typedef protocol_binary_response_no_extras protocol_binary_response_noop;\n\n    /**\n     * Definition of the structure used by the increment and decrement\n     * command.\n     * See section 4\n     */\n    typedef union {\n        struct {\n            protocol_binary_request_header header;\n            struct {\n                uint64_t delta;\n                uint64_t initial;\n                uint32_t expiration;\n            } body;\n        } message;\n        uint8_t bytes[sizeof(protocol_binary_request_header) + 20];\n    } protocol_binary_request_incr;\n    typedef protocol_binary_request_incr protocol_binary_request_decr;\n\n    /**\n     * Definition of the response from an incr or decr command\n     * command.\n     * See section 4\n     */\n    typedef union {\n        struct {\n            protocol_binary_response_header header;\n            struct {\n                uint64_t value;\n            } body;\n        } message;\n        uint8_t bytes[sizeof(protocol_binary_response_header) + 8];\n    } protocol_binary_response_incr;\n    typedef protocol_binary_response_incr protocol_binary_response_decr;\n\n    /**\n     * Definition of the quit\n     * See section 4\n     */\n    typedef protocol_binary_request_no_extras protocol_binary_request_quit;\n\n    /**\n     * Definition of the packet returned by the quit command\n     * See section 4\n     */\n    typedef protocol_binary_response_no_extras protocol_binary_response_quit;\n\n    /**\n     * Definition of the packet used by append and prepend command\n     * See section 4\n     */\n    typedef protocol_binary_request_no_extras protocol_binary_request_append;\n    typedef protocol_binary_request_no_extras protocol_binary_request_prepend;\n\n    /**\n     * Definition of the packet returned from a successful append or prepend\n     * See section 4\n     */\n    typedef protocol_binary_response_no_extras protocol_binary_response_append;\n    typedef protocol_binary_response_no_extras protocol_binary_response_prepend;\n\n    /**\n     * Definition of the packet used by the version command\n     * See section 4\n     */\n    typedef protocol_binary_request_no_extras protocol_binary_request_version;\n\n    /**\n     * Definition of the packet returned from a successful version command\n     * See section 4\n     */\n    typedef protocol_binary_response_no_extras protocol_binary_response_version;\n\n\n    /**\n     * Definition of the packet used by the stats command.\n     * See section 4\n     */\n    typedef protocol_binary_request_no_extras protocol_binary_request_stats;\n\n    /**\n     * Definition of the packet returned from a successful stats command\n     * See section 4\n     */\n    typedef protocol_binary_response_no_extras protocol_binary_response_stats;\n\n    /**\n     * Definition of the packet used by the touch command.\n     */\n    typedef union {\n        struct {\n            protocol_binary_request_header header;\n            struct {\n                uint32_t expiration;\n            } body;\n        } message;\n        uint8_t bytes[sizeof(protocol_binary_request_header) + 4];\n    } protocol_binary_request_touch;\n\n    /**\n     * Definition of the packet returned from the touch command\n     */\n    typedef protocol_binary_response_no_extras protocol_binary_response_touch;\n\n    /**\n     * Definition of the packet used by the GAT(Q) command.\n     */\n    typedef union {\n        struct {\n            protocol_binary_request_header header;\n            struct {\n                uint32_t expiration;\n            } body;\n        } message;\n        uint8_t bytes[sizeof(protocol_binary_request_header) + 4];\n    } protocol_binary_request_gat;\n\n    typedef protocol_binary_request_gat protocol_binary_request_gatq;\n    typedef protocol_binary_request_gat protocol_binary_request_gatk;\n    typedef protocol_binary_request_gat protocol_binary_request_gatkq;\n\n    /**\n     * Definition of the packet returned from the GAT(Q)\n     */\n    typedef protocol_binary_response_get protocol_binary_response_gat;\n    typedef protocol_binary_response_get protocol_binary_response_gatq;\n    typedef protocol_binary_response_get protocol_binary_response_gatk;\n    typedef protocol_binary_response_get protocol_binary_response_gatkq;\n\n    /**\n     * Definition of a request for a range operation.\n     * See http://code.google.com/p/memcached/wiki/RangeOps\n     *\n     * These types are used for range operations and exist within\n     * this header for use in other projects.  Range operations are\n     * not expected to be implemented in the memcached server itself.\n     */\n    typedef union {\n        struct {\n            protocol_binary_response_header header;\n            struct {\n                uint16_t size;\n                uint8_t  reserved;\n                uint8_t  flags;\n                uint32_t max_results;\n            } body;\n        } message;\n        uint8_t bytes[sizeof(protocol_binary_request_header) + 4];\n    } protocol_binary_request_rangeop;\n\n    typedef protocol_binary_request_rangeop protocol_binary_request_rget;\n    typedef protocol_binary_request_rangeop protocol_binary_request_rset;\n    typedef protocol_binary_request_rangeop protocol_binary_request_rsetq;\n    typedef protocol_binary_request_rangeop protocol_binary_request_rappend;\n    typedef protocol_binary_request_rangeop protocol_binary_request_rappendq;\n    typedef protocol_binary_request_rangeop protocol_binary_request_rprepend;\n    typedef protocol_binary_request_rangeop protocol_binary_request_rprependq;\n    typedef protocol_binary_request_rangeop protocol_binary_request_rdelete;\n    typedef protocol_binary_request_rangeop protocol_binary_request_rdeleteq;\n    typedef protocol_binary_request_rangeop protocol_binary_request_rincr;\n    typedef protocol_binary_request_rangeop protocol_binary_request_rincrq;\n    typedef protocol_binary_request_rangeop protocol_binary_request_rdecr;\n    typedef protocol_binary_request_rangeop protocol_binary_request_rdecrq;\n\n#ifdef __cplusplus\n}\n#endif\n#endif /* PROTOCOL_BINARY_H */\n"
        },
        {
          "name": "proxy.h",
          "type": "blob",
          "size": 30.8828125,
          "content": "#ifndef PROXY_H\n#define PROXY_H\n\n#include \"memcached.h\"\n#include \"extstore.h\"\n#include <string.h>\n#include <stdlib.h>\n#include <ctype.h>\n#include <errno.h>\n\n#include <lua.h>\n#include <lualib.h>\n#include <lauxlib.h>\n\n#include \"config.h\"\n\n#if defined(__linux__)\n#define USE_EVENTFD 1\n#include <sys/eventfd.h>\n#endif\n\n#ifdef HAVE_LIBURING\n#include <liburing.h>\n#include <poll.h> // POLLOUT for liburing.\n#define PRING_QUEUE_SQ_ENTRIES 2048\n#define PRING_QUEUE_CQ_ENTRIES 16384\n#endif\n\n#include \"proto_proxy.h\"\n#include \"proto_text.h\"\n#include \"queue.h\"\n#define XXH_INLINE_ALL // modifier for xxh3's include below\n#include \"xxhash.h\"\n\n#ifdef PROXY_DEBUG\n#define P_DEBUG(...) \\\n    do { \\\n        fprintf(stderr, __VA_ARGS__); \\\n    } while (0)\n#else\n#define P_DEBUG(...)\n#endif\n\n#define WSTAT_L(t) pthread_mutex_lock(&t->stats.mutex);\n#define WSTAT_UL(t) pthread_mutex_unlock(&t->stats.mutex);\n#define WSTAT_INCR(t, stat, amount) { \\\n    pthread_mutex_lock(&t->stats.mutex); \\\n    t->stats.stat += amount; \\\n    pthread_mutex_unlock(&t->stats.mutex); \\\n}\n#define WSTAT_DECR(t, stat, amount) { \\\n    pthread_mutex_lock(&t->stats.mutex); \\\n    t->stats.stat -= amount; \\\n    pthread_mutex_unlock(&t->stats.mutex); \\\n}\n#define STAT_L(ctx) pthread_mutex_lock(&ctx->stats_lock);\n#define STAT_UL(ctx) pthread_mutex_unlock(&ctx->stats_lock);\n#define STAT_INCR(ctx, stat, amount) { \\\n        pthread_mutex_lock(&ctx->stats_lock); \\\n        ctx->global_stats.stat += amount; \\\n        pthread_mutex_unlock(&ctx->stats_lock); \\\n}\n\n#define STAT_DECR(ctx, stat, amount) { \\\n        pthread_mutex_lock(&ctx->stats_lock); \\\n        ctx->global_stats.stat -= amount; \\\n        pthread_mutex_unlock(&ctx->stats_lock); \\\n}\n\n// FIXME (v2): do include dir properly.\n#include \"vendor/mcmc/mcmc.h\"\n\nenum mcp_memprofile_types {\n    mcp_memp_free = 0,\n    mcp_memp_string,\n    mcp_memp_table,\n    mcp_memp_func,\n    mcp_memp_userdata,\n    mcp_memp_thread,\n    mcp_memp_default,\n    mcp_memp_realloc,\n};\n\nstruct mcp_memprofile {\n    struct timespec last_status; // for per-second prints on status\n    int id;\n    uint64_t allocs[8];\n    uint64_t alloc_bytes[8];\n};\n\n// for various time conversion functions\n#define NANOSECONDS(x) ((x) * 1E9 + 0.5)\n#define MICROSECONDS(x) ((x) * 1E6 + 0.5)\n\n// Note: value created from thin air. Could be shorter.\n#define MCP_REQUEST_MAXLEN KEY_MAX_LENGTH * 2\n\n#define ENDSTR \"END\\r\\n\"\n#define ENDLEN sizeof(ENDSTR)-1\n\n#define MCP_BACKEND_UPVALUE 1\n\n#define MCP_YIELD_INTERNAL 1\n#define MCP_YIELD_WAITCOND 2\n#define MCP_YIELD_WAITHANDLE 3\n#define MCP_YIELD_SLEEP 4\n\n#define SHAREDVM_FGEN_IDX 1\n#define SHAREDVM_FGENSLOT_IDX 2\n#define SHAREDVM_BACKEND_IDX 3\n\n// all possible commands.\n#define CMD_FIELDS \\\n    X(CMD_MG) \\\n    X(CMD_MS) \\\n    X(CMD_MD) \\\n    X(CMD_MN) \\\n    X(CMD_MA) \\\n    X(CMD_ME) \\\n    X(CMD_GET) \\\n    X(CMD_GAT) \\\n    X(CMD_SET) \\\n    X(CMD_ADD) \\\n    X(CMD_CAS) \\\n    X(CMD_GETS) \\\n    X(CMD_GATS) \\\n    X(CMD_INCR) \\\n    X(CMD_DECR) \\\n    X(CMD_TOUCH) \\\n    X(CMD_APPEND) \\\n    X(CMD_DELETE) \\\n    X(CMD_REPLACE) \\\n    X(CMD_PREPEND) \\\n    X(CMD_END_STORAGE) \\\n    X(CMD_QUIT) \\\n    X(CMD_STATS) \\\n    X(CMD_SLABS) \\\n    X(CMD_WATCH) \\\n    X(CMD_LRU) \\\n    X(CMD_VERSION) \\\n    X(CMD_SHUTDOWN) \\\n    X(CMD_EXTSTORE) \\\n    X(CMD_FLUSH_ALL) \\\n    X(CMD_VERBOSITY) \\\n    X(CMD_LRU_CRAWLER) \\\n    X(CMD_REFRESH_CERTS) \\\n    X(CMD_CACHE_MEMLIMIT)\n\n#define X(name) name,\nenum proxy_defines {\n    P_OK = 0,\n    CMD_FIELDS\n    CMD_SIZE, // used to define array size for command hooks.\n    CMD_ANY, // override _all_ commands\n    CMD_ANY_STORAGE, // override commands specific to key storage.\n    CMD_FINAL, // end cap for convenience.\n};\n#undef X\n\n// certain classes of ascii commands have similar parsing (ie;\n// get/gets/gat/gats). Use types so we don't have to test a ton of them.\nenum proxy_cmd_types {\n    CMD_TYPE_GENERIC = 0,\n    CMD_TYPE_GET, // get/gets/gat/gats\n    CMD_TYPE_META, // m*'s.\n};\n\ntypedef struct _io_pending_proxy_t io_pending_proxy_t;\ntypedef struct proxy_event_thread_s proxy_event_thread_t;\n\n#ifdef HAVE_LIBURING\n// TODO: pass in cqe->res instead of cqe?\ntypedef void (*proxy_event_cb)(void *udata, struct io_uring_cqe *cqe);\ntypedef struct {\n    void *udata;\n    proxy_event_cb cb;\n    bool set; // NOTE: not sure if necessary if code structured properly\n} proxy_event_t;\n\nvoid *proxy_event_thread_ur(void *arg);\n#endif\n\n// Note: This ends up wasting a few counters, but simplifies the rest of the\n// process for handling internal worker stats.\nstruct proxy_int_stats {\n    uint64_t vm_gc_runs;\n    uint64_t vm_memory_kb;\n    uint64_t counters[CMD_FINAL];\n};\n\nstruct proxy_user_stats {\n    int num_stats; // number of stats, for sizing various arrays\n    uint64_t *counters; // array of counters.\n};\n\nstruct proxy_user_stats_entry {\n    char *name;\n    unsigned int cname; // offset into compact name buffer\n    bool reset; // counter must reset this cycle\n};\n\nstruct proxy_global_stats {\n    uint64_t config_reloads;\n    uint64_t config_reload_fails;\n    uint64_t config_cron_runs;\n    uint64_t config_cron_fails;\n    uint64_t backend_total;\n    uint64_t backend_marked_bad; // backend set to autofail\n    uint64_t backend_failed; // an error caused a backend reset\n    uint64_t request_failed_depth; // requests fast-failed due to be depth\n};\n\nstruct proxy_tunables {\n    struct timeval connect;\n    struct timeval retry; // wait time before retrying a dead backend\n    struct timeval read;\n    struct timeval flap; // need to stay connected this long or it's flapping\n    float flap_backoff_ramp; // factorial for retry time\n    float gc_ratio; // how much lua VM growth to allow before running GC\n    uint32_t flap_backoff_max; // don't backoff longer than this.\n    int backend_depth_limit; // requests fast fail once depth over this limit\n    int backend_failure_limit;\n    int max_ustats; // limit the ustats index.\n    bool tcp_keepalive;\n    bool use_iothread; // default for using the bg io thread.\n    bool use_tls; // whether or not be should use TLS\n    bool down; // backend is forced into a down/bad state.\n};\n\ntypedef STAILQ_HEAD(globalobj_head_s, mcp_globalobj_s) globalobj_head_t;\ntypedef struct {\n    lua_State *proxy_state; // main configuration vm\n    proxy_event_thread_t *proxy_io_thread;\n    uint64_t active_req_limit; // max total in-flight requests\n    uint64_t buffer_memory_limit; // max bytes for send/receive buffers.\n#ifdef PROXY_TLS\n    void *tls_ctx;\n#endif\n    int user_stats_num; // highest seen stat index\n    struct proxy_user_stats_entry *user_stats;\n    char *user_stats_namebuf; // compact linear buffer for stat names\n    struct proxy_tunables tunables; // NOTE: updates covered by stats_lock\n    struct proxy_global_stats global_stats;\n    // less frequently used entries down here.\n    void *proxy_code;\n    lua_State *proxy_sharedvm; // sub VM for short-lock global events/data\n    pthread_mutex_t stats_lock; // used for rare global counters\n    pthread_mutex_t config_lock;\n    pthread_cond_t config_cond;\n    pthread_t config_tid;\n    pthread_mutex_t worker_lock;\n    pthread_cond_t worker_cond;\n    pthread_t manager_tid; // deallocation management thread\n    pthread_mutex_t manager_lock;\n    pthread_cond_t manager_cond;\n    pthread_mutex_t sharedvm_lock; // protect statevm above\n    globalobj_head_t manager_head; // stack for pool deallocation.\n    int config_generation; // counter tracking config reloads\n    int cron_ref; // reference to lua cron table\n    int cron_next; // next cron to sleep to / execute\n    bool worker_done; // signal variable for the worker lock/cond system.\n    bool worker_failed; // covered by worker_lock as well.\n    bool use_uring; // use IO_URING for backend connections.\n    bool loading; // bool indicating an active config load.\n    bool memprofile; // indicate if we want to profile lua memory.\n    uint8_t memprofile_thread_counter;\n} proxy_ctx_t;\n\n#define PROXY_GET_THR_CTX(L) ((*(LIBEVENT_THREAD **)lua_getextraspace(L))->proxy_ctx)\n#define PROXY_GET_THR(L) (*(LIBEVENT_THREAD **)lua_getextraspace(L))\n// Operations from the config VM don't have a libevent thread.\n#define PROXY_GET_CTX(L) (*(proxy_ctx_t **)lua_getextraspace(L))\n\nstruct proxy_hook_ref {\n    int lua_ref;\n    void *ctx; // if we're a generator based function.\n};\n\nstruct proxy_hook_tagged {\n    uint64_t tag;\n    struct proxy_hook_ref ref;\n};\n\nstruct proxy_hook {\n    struct proxy_hook_ref ref;\n    int tagcount;\n    struct proxy_hook_tagged *tagged; // array of possible tagged hooks.\n};\n\n// TODO (v2): some hash functions (crc?) might require initializers. If we run into\n// any the interface might need expanding.\ntypedef uint64_t (*key_hash_func)(const void *key, size_t len, uint64_t seed);\nstruct proxy_hash_func {\n    key_hash_func func;\n};\ntypedef const char *(*key_hash_filter_func)(const char *conf, const char *key, size_t klen, size_t *newlen);\ntypedef uint32_t (*hash_selector_func)(uint64_t hash, void *ctx);\nstruct proxy_hash_caller {\n    hash_selector_func selector_func;\n    void *ctx;\n};\n\nenum mcp_backend_states {\n    mcp_backend_read = 0, // waiting to read any response\n    mcp_backend_parse, // have some buffered data to check\n    mcp_backend_read_end, // looking for an \"END\" marker for GET\n    mcp_backend_want_read, // read more data to complete command\n    mcp_backend_next, // advance to the next IO\n    mcp_backend_next_close, // complete current request, then close socket\n};\n\ntypedef struct mcp_cron_s mcp_cron_t;\ntypedef struct mcp_backend_wrap_s mcp_backend_wrap_t;\ntypedef struct mcp_backend_label_s mcp_backend_label_t;\ntypedef struct mcp_backend_s mcp_backend_t;\ntypedef struct mcp_request_s mcp_request_t;\ntypedef struct mcp_parser_s mcp_parser_t;\ntypedef struct mcp_rcontext_s mcp_rcontext_t;\ntypedef struct mcp_funcgen_s mcp_funcgen_t;\n\n#define PARSER_MAX_TOKENS 24\n\nstruct mcp_parser_meta_s {\n    uint64_t flags;\n};\n\n// Note that we must use offsets into request for tokens,\n// as *request can change between parsing and later accessors.\nstruct mcp_parser_s {\n    const char *request;\n    void *vbuf; // temporary buffer for holding value lengths.\n    uint8_t command;\n    uint8_t cmd_type; // command class.\n    uint8_t ntokens;\n    uint8_t keytoken; // because GAT. sigh. also cmds without a key.\n    uint32_t parsed; // how far into the request we parsed already\n    uint32_t reqlen; // full length of request buffer.\n    uint32_t endlen; // index to the start of \\r\\n or \\n\n    int vlen;\n    uint32_t klen; // length of key.\n    uint16_t tokens[PARSER_MAX_TOKENS]; // offsets for start of each token\n    bool has_space; // a space was found after the last byte parsed.\n    bool noreply; // if quiet/noreply mode is set.\n    union {\n        struct mcp_parser_meta_s meta;\n    } t;\n};\n\n#define MCP_PARSER_KEY(pr) (&pr.request[pr.tokens[pr.keytoken]])\n\n#define MAX_REQ_TOKENS 2\nstruct mcp_request_s {\n    mcp_parser_t pr; // non-lua-specific parser handling.\n    bool ascii_multiget; // ascii multiget mode. (hide errors/END)\n    char request[];\n};\n\nstruct mcp_cron_s {\n    uint32_t gen;\n    uint32_t next;\n    uint32_t every;\n    bool repeat;\n};\n\ntypedef STAILQ_HEAD(io_head_s, _io_pending_proxy_t) io_head_t;\n#define MAX_LABELLEN 512\n#define MAX_NAMELEN 255\n#define MAX_PORTLEN 6\n// TODO (v2): IOV_MAX tends to be 1000+ which would allow for more batching but we\n// don't have a good temporary space and don't want to malloc/free on every\n// write. transmit() uses the stack but we can't do that for uring's use case.\n#if MEMCACHED_DEBUG\n#define BE_IOV_MAX 128 // let bench tests trigger max condition easily\n#elif (IOV_MAX > 1024)\n#define BE_IOV_MAX 1024\n#else\n#define BE_IOV_MAX IOV_MAX\n#endif\n// lua descriptor object: passed to pools, which create wrappers.\nstruct mcp_backend_label_s {\n    char name[MAX_NAMELEN+1];\n    char port[MAX_PORTLEN+1];\n    char label[MAX_LABELLEN+1];\n    size_t llen; // cache label length for small speedup in pool creation.\n    int conncount; // number of sockets to make.\n    struct proxy_tunables tunables;\n};\n\n// lua object wrapper meant to own a malloc'ed conn structure\n// when this object is created, it ships its connection to the real owner\n// (worker, IO thread, etc)\n// when this object is garbage collected, it ships a notice to the owner\n// thread to stop using and free the backend conn memory.\nstruct mcp_backend_wrap_s {\n    mcp_backend_t *be;\n};\n\nstruct mcp_backendconn_s {\n    mcp_backend_t *be_parent; // find the wrapper.\n    int self; // our index into the parent array.\n    int depth; // total number of requests in queue\n    int pending_read; // number of requests written to socket, pending read.\n    int failed_count; // number of fails (timeouts) in a row\n    int flap_count; // number of times we've \"flapped\" into bad state.\n    proxy_event_thread_t *event_thread; // event thread owning this backend.\n    void *client; // mcmc client\n#ifdef PROXY_TLS\n    void *ssl;\n#endif\n    io_head_t io_head; // stack of requests.\n    io_pending_proxy_t *io_next; // next request to write.\n    char *rbuf; // statically allocated read buffer.\n    size_t rbufused; // currently active bytes in the buffer\n    struct event main_event; // libevent: changes role, mostly for main read events\n    struct event write_event; // libevent: only used when socket wbuf full\n    struct event timeout_event; // libevent: alarm for pending reads\n    struct proxy_tunables tunables;\n    struct timeval last_failed; // time the backend was last reset.\n    enum mcp_backend_states state; // readback state machine\n    int connect_flags; // flags to pass to mcmc_connect\n    bool connecting; // in the process of an asynch connection.\n    bool validating; // in process of validating a new backend connection.\n    bool can_write; // recently got a WANT_WRITE or are connecting.\n    bool bad; // timed out, marked as bad.\n#ifndef PROXY_TLS\n    bool ssl;\n#endif\n    struct iovec write_iovs[BE_IOV_MAX]; // iovs to stage batched writes\n};\n\n// TODO: move depth and flags to a second top level array so we can make index\n// decisions from fewer memory stalls.\nstruct mcp_backend_s {\n    int conncount; // total number of connections managed.\n    int depth; // temporary depth counter for io_head\n    bool transferred; // if beconn has been shipped to owner thread.\n    bool use_io_thread; // note if this backend is worker-local or not.\n    bool stacked; // if backend already queued for syscalls.\n    STAILQ_ENTRY(mcp_backend_s) beconn_next; // stack for connecting conns\n    STAILQ_ENTRY(mcp_backend_s) be_next; // stack for backends\n    io_head_t io_head; // stack of inbound requests.\n    char name[MAX_NAMELEN+1];\n    char port[MAX_PORTLEN+1];\n    char label[MAX_LABELLEN+1];\n    struct proxy_tunables tunables; // this gets copied a few times for speed.\n    struct mcp_backendconn_s be[];\n};\ntypedef STAILQ_HEAD(be_head_s, mcp_backend_s) be_head_t;\ntypedef STAILQ_HEAD(beconn_head_s, mcp_backend_s) beconn_head_t;\n\nstruct proxy_event_thread_s {\n    pthread_t thread_id;\n    struct event_base *base;\n    struct event notify_event; // listen event for the notify pipe/eventfd.\n    struct event beconn_event; // listener for backends in connect state\n#ifdef HAVE_LIBURING\n    struct io_uring ring;\n    proxy_event_t ur_notify_event; // listen on eventfd.\n    proxy_event_t ur_benotify_event; // listen on eventfd for backend connections.\n    eventfd_t event_counter;\n    eventfd_t beevent_counter;\n    bool use_uring;\n#endif\n#ifdef PROXY_TLS\n    char *tls_wbuf;\n    size_t tls_wbuf_size;\n#endif\n    pthread_mutex_t mutex; // covers stack.\n    pthread_cond_t cond; // condition to wait on while stack drains.\n    io_head_t io_head_in; // inbound requests to process.\n    be_head_t be_head; // stack of backends for processing.\n    beconn_head_t beconn_head_in; // stack of backends for connection processing.\n#ifdef USE_EVENTFD\n    int event_fd; // for request ingestion\n    int be_event_fd; // for backend ingestion\n#else\n    int notify_receive_fd;\n    int notify_send_fd;\n    int be_notify_receive_fd;\n    int be_notify_send_fd;\n#endif\n    proxy_ctx_t *ctx; // main context.\n};\n\nenum mcp_resp_mode {\n    RESP_MODE_NORMAL = 0,\n    RESP_MODE_NOREPLY,\n    RESP_MODE_METAQUIET\n};\n\n#define RESP_CMD_MAX 8\ntypedef struct {\n    mcmc_resp_t resp;\n    mcmc_tokenizer_t tok; // optional tokenization of res\n    char *buf; // response line + potentially value.\n    mc_resp *cresp; // client mc_resp object during extstore fetches.\n    LIBEVENT_THREAD *thread; // cresp's owner thread needed for extstore cleanup.\n    unsigned int blen; // total size of the value to read.\n    struct timeval start; // time this object was created.\n    long elapsed; // time elapsed once handled.\n    int status; // status code from mcmc_read()\n    int bread; // amount of bytes read into value so far.\n    uint8_t cmd; // from parser (pr.command)\n    uint8_t extra; // ascii multiget hack for memory accounting. extra blen.\n    enum mcp_resp_mode mode; // reply mode (for noreply fixing)\n    char be_name[MAX_NAMELEN+1];\n    char be_port[MAX_PORTLEN+1];\n} mcp_resp_t;\n\n// re-cast an io_pending_t into this more descriptive structure.\n// the first few items _must_ match the original struct.\n#define IO_PENDING_TYPE_PROXY 0\n#define IO_PENDING_TYPE_EXTSTORE 1\nstruct _io_pending_proxy_t {\n    int io_queue_type;\n    LIBEVENT_THREAD *thread;\n    conn *c;\n    mc_resp *resp;\n    io_queue_cb return_cb; // called on worker thread.\n    io_queue_cb finalize_cb; // called back on the worker thread.\n    STAILQ_ENTRY(io_pending_t) iop_next; // queue chain.\n    // original struct ends here\n\n    mcp_rcontext_t *rctx; // pointer to request context.\n    int queue_handle; // queue slot to return this result to\n    bool ascii_multiget; // passed on from mcp_r_t\n    uint8_t io_type; // extstore IO or backend IO\n    union {\n        // extstore IO.\n        struct {\n            obj_io eio;\n            item *hdr_it;\n            mc_resp *tresp; // temporary mc_resp for storage to fill.\n            int gettype;\n            int iovec_data;\n            bool miss;\n            bool badcrc;\n            bool active;\n        };\n        // backend request IO\n        struct {\n            // FIXME: use top level next chain\n            struct _io_pending_proxy_t *next; // stack for IO submission\n            STAILQ_ENTRY(_io_pending_proxy_t) io_next; // stack for backends\n            mcp_backend_t *backend; // backend server to request from\n            struct iovec iov[2]; // request string + tail buffer\n            int iovcnt; // 1 or 2...\n            unsigned int iovbytes; // total bytes in the iovec\n            mcp_resp_t *client_resp; // reference (currently pointing to a lua object)\n            bool flushed; // whether we've fully written this request to a backend.\n            bool background; // dummy IO for backgrounded awaits\n            bool qcount_incr; // HACK.\n        };\n    };\n};\n\nstruct mcp_globalobj_s {\n    pthread_mutex_t lock; // protects refcount/object.\n    STAILQ_ENTRY(mcp_globalobj_s) next;\n    int refcount;\n    int self_ref;\n};\n\n// Note: does *be have to be a sub-struct? how stable are userdata pointers?\n// https://stackoverflow.com/questions/38718475/lifetime-of-lua-userdata-pointers\n// - says no.\ntypedef struct {\n    int ref; // luaL_ref reference of backend_wrap_t obj.\n    mcp_backend_t *be;\n} mcp_pool_be_t;\n\n#define KEY_HASH_FILTER_MAX 5\ntypedef struct mcp_pool_s mcp_pool_t;\nstruct mcp_pool_s {\n    struct proxy_hash_caller phc;\n    key_hash_filter_func key_filter;\n    key_hash_func key_hasher;\n    proxy_ctx_t *ctx; // main context.\n    char key_filter_conf[KEY_HASH_FILTER_MAX+1];\n    struct mcp_globalobj_s g;\n    char beprefix[MAX_LABELLEN+1]; // TODO: should probably be shorter.\n    uint64_t hash_seed; // calculated from a string.\n    int pool_size;\n    int pool_be_total; // can be different from pool size for worker IO\n    int phc_ref;\n    bool use_iothread;\n    mcp_pool_be_t pool[];\n};\n\ntypedef struct {\n    mcp_pool_t *main; // ptr to original\n    mcp_pool_be_t *pool; // ptr to main->pool starting offset for owner thread.\n} mcp_pool_proxy_t;\n\n// utils\nbool proxy_bufmem_checkadd(LIBEVENT_THREAD *t, int len);\nvoid mcp_sharedvm_delta(proxy_ctx_t *ctx, int tidx, const char *name, int delta);\nvoid mcp_sharedvm_remove(proxy_ctx_t *ctx, int tidx, const char *name);\n\nvoid mcp_gobj_ref(lua_State *L, struct mcp_globalobj_s *g);\nvoid mcp_gobj_unref(proxy_ctx_t *ctx, struct mcp_globalobj_s *g);\nvoid mcp_gobj_finalize(struct mcp_globalobj_s *g);\n\n// networking interface\nvoid proxy_init_event_thread(proxy_event_thread_t *t, proxy_ctx_t *ctx, struct event_base *base);\nvoid *proxy_event_thread(void *arg);\nvoid proxy_run_backend_queue(be_head_t *head);\nstruct mcp_backendconn_s *proxy_choose_beconn(mcp_backend_t *be);\nmcp_resp_t *mcp_prep_resobj(lua_State *L, mcp_request_t *rq, mcp_backend_t *be, LIBEVENT_THREAD *t);\nmcp_resp_t *mcp_prep_bare_resobj(lua_State *L, LIBEVENT_THREAD *t);\nvoid mcp_resp_set_elapsed(mcp_resp_t *r);\nio_pending_proxy_t *mcp_queue_rctx_io(mcp_rcontext_t *rctx, mcp_request_t *rq, mcp_backend_t *be, mcp_resp_t *r);\n\n// internal request interface\nint mcplib_internal(lua_State *L);\nint mcplib_internal_run(mcp_rcontext_t *rctx);\n\n// user stats interface\n#define MAX_USTATS_DEFAULT 1024\nint mcplib_add_stat(lua_State *L);\nint mcplib_stat(lua_State *L);\nsize_t _process_request_next_key(mcp_parser_t *pr);\nint process_request(mcp_parser_t *pr, const char *command, size_t cmdlen);\nmcp_request_t *mcp_new_request(lua_State *L, mcp_parser_t *pr, const char *command, size_t cmdlen);\nvoid mcp_set_request(mcp_parser_t *pr, mcp_request_t *r, const char *command, size_t cmdlen);\n\n// rate limit interfaces\nint mcplib_ratelim_tbf(lua_State *L);\nint mcplib_ratelim_tbf_call(lua_State *L);\nint mcplib_ratelim_global_tbf(lua_State *L);\nint mcplib_ratelim_proxy_tbf_call(lua_State *L);\nint mcp_ratelim_proxy_tbf(lua_State *from, lua_State *to);\nint mcplib_ratelim_global_tbf_gc(lua_State *L);\nint mcplib_ratelim_proxy_tbf_gc(lua_State *L);\n\n// request function generator interface\nvoid proxy_return_rctx_cb(io_pending_t *pending);\nvoid proxy_finalize_rctx_cb(io_pending_t *pending);\n\nenum mcp_rqueue_e {\n    QWAIT_IDLE = 0,\n    QWAIT_ANY,\n    QWAIT_OK,\n    QWAIT_GOOD,\n    QWAIT_FASTGOOD,\n    QWAIT_HANDLE,\n    QWAIT_SLEEP,\n};\n\n#define FGEN_NAME_MAXLEN 80\nstruct mcp_funcgen_s {\n    LIBEVENT_THREAD *thread; // worker thread that created this funcgen.\n    int generator_ref; // reference to the generator function.\n    int self_ref; // self-reference if we're attached anywhere\n    int argument_ref; // reference to an argument to pass to generator\n    int max_queues; // how many queue slots rctx's have\n    int uobj_queues; // how many extra queue slots for object storage we have\n    unsigned int refcount; // reference counter\n    unsigned int total; // total contexts managed\n    unsigned int free; // free contexts\n    unsigned int free_max; // size of list below.\n    unsigned int free_pressure; // \"pressure\" for when to early release rctx\n    bool closed; // the hook holding this fgen has been replaced\n    bool ready; // if we're locked down or not.\n    bool is_router; // if this fgen is actually a router object.\n    struct timespec free_waiter; // must be \"too free\" for this much time\n    mcp_rcontext_t **list;\n    struct mcp_rqueue_s *queue_list;\n    char name[FGEN_NAME_MAXLEN+1]; // string name for the generator.\n};\n\nenum mcp_funcgen_router_e {\n    FGEN_ROUTER_NONE = 0,\n    FGEN_ROUTER_CMDMAP,\n    FGEN_ROUTER_SHORTSEP,\n    FGEN_ROUTER_LONGSEP,\n    FGEN_ROUTER_ANCHORSM,\n    FGEN_ROUTER_ANCHORBIG,\n};\n\nstruct mcp_router_long_s {\n    char start[KEY_HASH_FILTER_MAX+1];\n    char stop[KEY_HASH_FILTER_MAX+1];\n};\n\n// To simplify the attach/start code we wrap a funcgen with the router\n// structure. This allows us to have a larger router structure without\n// bloating the fgen object itself, and still benefit from letting funcgen\n// new/cleanup handle most of the memory management.\nstruct mcp_funcgen_router {\n    mcp_funcgen_t fgen_self;\n    enum mcp_funcgen_router_e type;\n    union {\n        char sep;\n        char lsep[KEY_HASH_FILTER_MAX+1];\n        char anchorsm[2]; // short anchored mode.\n        struct mcp_router_long_s big;\n    } conf;\n    int map_ref;\n    mcp_funcgen_t *def_fgen; // default route\n    mcp_funcgen_t *cmap[CMD_END_STORAGE]; // fallback command map\n};\n\n#define RQUEUE_TYPE_NONE 0\n#define RQUEUE_TYPE_POOL 1\n#define RQUEUE_TYPE_FGEN 2\n#define RQUEUE_TYPE_UOBJ 3 // user tracked object types past this point\n#define RQUEUE_TYPE_UOBJ_REQ 4\n#define RQUEUE_TYPE_UOBJ_RES 5\n#define RQUEUE_ASSIGNED (1<<0)\n#define RQUEUE_R_RESUME (1<<1)\n#define RQUEUE_R_GOOD (1<<3)\n#define RQUEUE_R_OK (1<<4)\n#define RQUEUE_R_ANY (1<<5)\n#define RQUEUE_R_ERROR (1<<7)\n#define RQUEUE_UOBJ_MAX UINT8_MAX\n\nenum mcp_rqueue_state {\n    RQUEUE_IDLE = 0,\n    RQUEUE_QUEUED,\n    RQUEUE_ACTIVE,\n    RQUEUE_COMPLETE,\n    RQUEUE_WAITED\n};\n\nstruct mcp_rqueue_s {\n    int obj_ref; // reference to pool/func/etc object\n    int cb_ref; // if a lua callback was specified\n    int req_ref; // reference to associated request object.\n    int res_ref; // reference to lua response object.\n    void *obj; // direct pointer to the object for fast access.\n    mcp_request_t *rq; // request set to this slot\n    mcp_resp_t *res_obj; // pointer to result object\n    enum mcp_rqueue_state state; // queued/active/etc\n    uint8_t obj_type; // what the obj_ref actually is.\n    uint8_t flags; // bit flags for various states\n};\n\nstruct mcp_rcontext_s {\n    int self_ref; // reference to our own object\n    int request_ref; // top level request for this context.\n    int function_ref; // ref to the created route function.\n    int coroutine_ref; // ref to our encompassing coroutine.\n    int pending_reqs; // pending requests and sub-requests\n    unsigned int wait_count;\n    unsigned int wait_done; // TODO: change these variables to uint8's\n    int wait_handle; // waiting on a specific queue slot\n    int parent_handle; // queue slot in parent rctx\n    int conn_fd; // fd of the originating client, as *c can become invalid\n    enum mcp_rqueue_e wait_mode;\n    uint8_t lua_narg; // number of responses to push when yield resuming.\n    uint8_t uobj_count; // number of extra tracked req/res objects.\n    bool first_queue; // HACK\n    lua_State *Lc; // coroutine thread pointer.\n    mcp_request_t *request; // ptr to the above reference.\n    mcp_rcontext_t *parent; // parent rctx in the call graph\n    conn *c; // associated client object.\n    mc_resp *resp; // top level response object to fill.\n    mcp_funcgen_t *fgen; // parent function generator context.\n    struct event timeout_event; // for *_wait_timeout() and sleep() calls\n    struct mcp_rqueue_s qslots[]; // queueable slots.\n};\n\n#define mcp_is_flag_invalid(f) (f < 65 || f > 122)\n\nvoid mcp_run_rcontext_handle(mcp_rcontext_t *rctx, int handle);\nvoid mcp_process_rctx_wait(mcp_rcontext_t *rctx, int handle);\nint mcp_process_rqueue_return(mcp_rcontext_t *rctx, int handle, mcp_resp_t *res);\nint mcplib_rcontext_handle_set_cb(lua_State *L);\nint mcplib_rcontext_enqueue(lua_State *L);\nint mcplib_rcontext_wait_cond(lua_State *L);\nint mcplib_rcontext_wait_handle(lua_State *L);\nint mcplib_rcontext_enqueue_and_wait(lua_State *L);\nint mcplib_rcontext_res_good(lua_State *L);\nint mcplib_rcontext_res_any(lua_State *L);\nint mcplib_rcontext_res_ok(lua_State *L);\nint mcplib_rcontext_result(lua_State *L);\nint mcplib_rcontext_cfd(lua_State *L);\nint mcplib_rcontext_tls_peer_cn(lua_State *L);\nint mcplib_rcontext_request_new(lua_State *L);\nint mcplib_rcontext_response_new(lua_State *L);\nint mcplib_rcontext_sleep(lua_State *L);\nint mcplib_funcgenbare_new(lua_State *L);\nint mcplib_funcgen_new(lua_State *L);\nint mcplib_funcgen_new_handle(lua_State *L);\nint mcplib_funcgen_ready(lua_State *L);\nint mcplib_router_new(lua_State *L);\nmcp_rcontext_t *mcp_funcgen_start(lua_State *L, mcp_funcgen_t *fgen, mcp_parser_t *pr);\nmcp_rcontext_t *mcp_funcgen_get_rctx(lua_State *L, int fgen_ref, mcp_funcgen_t *fgen);\nvoid mcp_funcgen_return_rctx(mcp_rcontext_t *rctx);\nint mcplib_funcgen_gc(lua_State *L);\nvoid mcp_funcgen_reference(lua_State *L);\nvoid mcp_funcgen_dereference(lua_State *L, mcp_funcgen_t *fgen);\nvoid mcp_rcontext_push_rqu_res(lua_State *L, mcp_rcontext_t *rctx, int handle);\n\n\nint mcplib_factory_command_new(lua_State *L);\n\n// request interface\nint mcplib_request(lua_State *L);\nint mcplib_request_command(lua_State *L);\nint mcplib_request_key(lua_State *L);\nint mcplib_request_ltrimkey(lua_State *L);\nint mcplib_request_rtrimkey(lua_State *L);\nint mcplib_request_token(lua_State *L);\nint mcplib_request_token_int(lua_State *L);\nint mcplib_request_ntokens(lua_State *L);\nint mcplib_request_has_flag(lua_State *L);\nint mcplib_request_flag_token(lua_State *L);\nint mcplib_request_flag_token_int(lua_State *L);\nint mcplib_request_flag_add(lua_State *L);\nint mcplib_request_flag_set(lua_State *L);\nint mcplib_request_flag_replace(lua_State *L);\nint mcplib_request_flag_del(lua_State *L);\nint mcplib_request_gc(lua_State *L);\nint mcplib_request_match_res(lua_State *L);\nvoid mcp_request_cleanup(LIBEVENT_THREAD *t, mcp_request_t *rq);\n\n// response interface\nint mcplib_response_elapsed(lua_State *L);\nint mcplib_response_ok(lua_State *L);\nint mcplib_response_hit(lua_State *L);\nint mcplib_response_vlen(lua_State *L);\nint mcplib_response_code(lua_State *L);\nint mcplib_response_line(lua_State *L);\nint mcplib_response_flag_blank(lua_State *L);\n\n// inspector interface\nint mcplib_req_inspector_new(lua_State *L);\nint mcplib_res_inspector_new(lua_State *L);\nint mcplib_inspector_gc(lua_State *L);\nint mcplib_inspector_call(lua_State *L);\n\n// mutator interface\nint mcplib_req_mutator_new(lua_State *L);\nint mcplib_res_mutator_new(lua_State *L);\nint mcplib_mutator_gc(lua_State *L);\nint mcplib_mutator_call(lua_State *L);\n\nvoid mcp_response_cleanup(LIBEVENT_THREAD *t, mcp_resp_t *r);\nvoid mcp_set_resobj(mcp_resp_t *r, mcp_request_t *rq, mcp_backend_t *be, LIBEVENT_THREAD *t);\nint mcplib_response_gc(lua_State *L);\nint mcplib_response_close(lua_State *L);\n\nint mcplib_open_dist_jump_hash(lua_State *L);\nint mcplib_open_dist_ring_hash(lua_State *L);\n\nint proxy_run_rcontext(mcp_rcontext_t *rctx);\nmcp_backend_t *mcplib_pool_proxy_call_helper(mcp_pool_proxy_t *pp, const char *key, size_t len);\nvoid mcp_request_attach(mcp_request_t *rq, io_pending_proxy_t *p);\nint mcp_request_render(mcp_request_t *rq, int idx, char flag, const char *tok, size_t len);\nint mcp_request_append(mcp_request_t *rq, const char flag, const char *tok, size_t len);\nint mcp_request_find_flag_index(mcp_request_t *rq, const char flag);\nint mcp_request_find_flag_token(mcp_request_t *rq, const char flag, const char **token, size_t *len);\nint mcp_request_find_flag_tokenint64(mcp_request_t *rq, const char flag, int64_t *token);\nvoid proxy_lua_error(lua_State *L, const char *s);\n#define proxy_lua_ferror(L, fmt, ...) \\\n    do { \\\n        lua_pushfstring(L, fmt, __VA_ARGS__); \\\n        lua_error(L); \\\n    } while (0)\n\n#define PROXY_SERVER_ERROR \"SERVER_ERROR \"\n#define PROXY_CLIENT_ERROR \"CLIENT_ERROR \"\nvoid proxy_out_errstring(mc_resp *resp, char *type, const char *str);\nint _start_proxy_config_threads(proxy_ctx_t *ctx);\nint proxy_thread_loadconf(proxy_ctx_t *ctx, LIBEVENT_THREAD *thr);\n\n// TODO (v2): more .h files, perhaps?\nint mcplib_open_hash_xxhash(lua_State *L);\n\n__attribute__((unused)) void dump_stack(lua_State *L, const char *msg);\n__attribute__((unused)) void dump_registry(lua_State *L, const char *msg);\n__attribute__((unused)) void dump_funcgen(lua_State *L, const char *name, const char *msg);\n__attribute__((unused)) void dump_pools(lua_State *L, const char *msg);\n#endif\n"
        },
        {
          "name": "proxy_config.c",
          "type": "blob",
          "size": 25.3203125,
          "content": "/* -*- Mode: C; tab-width: 4; c-basic-offset: 4; indent-tabs-mode: nil -*- */\n// Functions related to the configuration management threads and VM\n// TODO (v2): move worker thread related code back out of here.\n\n#include \"proxy.h\"\n#include \"vendor/routelib/routelib.h\"\n\n// not using queue.h becuase those require specific storage for HEAD.\n// it's not possible to have the HEAD simply be in the proxy context because\n// it would need to know the offset into this private structure.\n// This might be doable but the problem is too trivial to spend time on it.\n#define MCP_LUAFILE_SIZE 16384\nstruct _mcp_luafile {\n    size_t size;\n    size_t used;\n    bool loaded; // flip this to false before each load use\n    char *buf;\n    char *fname; // filename to load\n    struct _mcp_luafile *next;\n};\n\nstatic int _dump_helper(lua_State *L, const void *p, size_t sz, void *ud) {\n    (void)L;\n    struct _mcp_luafile *db = ud;\n    if (db->used + sz > db->size) {\n        // increase by blocks instead of doubling to avoid memory waste\n        db->size += MCP_LUAFILE_SIZE;\n        char *nb = realloc(db->buf, db->size);\n        if (nb == NULL) {\n            return -1;\n        }\n        db->buf = nb;\n    }\n    memcpy(db->buf + db->used, (const char *)p, sz);\n    db->used += sz;\n    return 0;\n}\n\nstatic const char * _load_helper(lua_State *L, void *data, size_t *size) {\n    (void)L;\n    struct _mcp_luafile *db = data;\n    if (db->loaded) {\n        *size = 0;\n        return NULL;\n    }\n    *size = db->used;\n    db->loaded = true;\n    return db->buf;\n}\n\nvoid proxy_start_reload(void *arg) {\n    proxy_ctx_t *ctx = arg;\n    if (pthread_mutex_trylock(&ctx->config_lock) == 0) {\n        ctx->loading = true;\n        pthread_cond_signal(&ctx->config_cond);\n        pthread_mutex_unlock(&ctx->config_lock);\n    }\n}\n\nint proxy_first_confload(void *arg) {\n    proxy_ctx_t *ctx = arg;\n    pthread_mutex_lock(&ctx->config_lock);\n    ctx->loading = true;\n    pthread_cond_signal(&ctx->config_cond);\n    pthread_mutex_unlock(&ctx->config_lock);\n\n    while (1) {\n        bool stop = false;\n        pthread_mutex_lock(&ctx->config_lock);\n        if (!ctx->loading) {\n            stop = true;\n        }\n        pthread_mutex_unlock(&ctx->config_lock);\n        if (stop)\n            break;\n    }\n    int fails = 0;\n    STAT_L(ctx);\n    fails = ctx->global_stats.config_reload_fails;\n    STAT_UL(ctx);\n    if (fails) {\n        return -1;\n    }\n\n    return 0;\n}\n\n// Manages a queue of inbound objects destined to be deallocated.\nstatic void *_proxy_manager_thread(void *arg) {\n    proxy_ctx_t *ctx = arg;\n    globalobj_head_t head;\n\n    pthread_mutex_lock(&ctx->manager_lock);\n    while (1) {\n        STAILQ_INIT(&head);\n        while (STAILQ_EMPTY(&ctx->manager_head)) {\n            pthread_cond_wait(&ctx->manager_cond, &ctx->manager_lock);\n        }\n\n        // pull dealloc queue into local queue.\n        STAILQ_CONCAT(&head, &ctx->manager_head);\n        pthread_mutex_unlock(&ctx->manager_lock);\n\n        // Config lock is required for using config VM.\n        pthread_mutex_lock(&ctx->config_lock);\n        lua_State *L = ctx->proxy_state;\n        struct mcp_globalobj_s *g;\n        STAILQ_FOREACH(g, &head, next) {\n            // we let the object _gc() handle backend/etc references\n            pthread_mutex_lock(&g->lock);\n            assert(g->self_ref != -1);\n            // See comment on mcp_gobj_ref()\n            if (g->self_ref < -1) {\n                g->refcount--;\n                g->self_ref = -g->self_ref;\n            }\n            assert(g->self_ref > 0 || g->refcount == 0);\n            if (g->refcount == 0) {\n                luaL_unref(L, LUA_REGISTRYINDEX, g->self_ref);\n                g->self_ref = -1;\n            }\n            pthread_mutex_unlock(&g->lock);\n        }\n        // force lua garbage collection so any resources close out quickly.\n        lua_gc(L, LUA_GCCOLLECT);\n        // twice because objects with garbage collector handlers are only\n        // marked on the first collection cycle.\n        lua_gc(L, LUA_GCCOLLECT);\n        // must hold this lock while interacting with the config VM.\n        pthread_mutex_unlock(&ctx->config_lock);\n\n        // done.\n        pthread_mutex_lock(&ctx->manager_lock);\n    }\n\n    return NULL;\n}\n\n// TODO: only run routine if something changed.\n// This compacts all of the names for proxy user stats into a linear buffer,\n// which can save considerable CPU when emitting a large number of stats. It\n// also saves some total memory by having one linear buffer instead of many\n// potentially small aligned allocations.\nstatic void proxy_config_stats_prep(proxy_ctx_t *ctx) {\n    char *oldnamebuf = ctx->user_stats_namebuf;\n    struct proxy_user_stats_entry *entries = ctx->user_stats;\n    size_t namelen = 0;\n\n    STAT_L(ctx);\n    // find size of new compact name buffer\n    for (int x = 0; x < ctx->user_stats_num; x++) {\n        if (entries[x].name) {\n            namelen += strlen(entries[x].name) + 1; // null byte\n        } else if (entries[x].cname) {\n            char *name = oldnamebuf + entries[x].cname;\n            namelen += strlen(name) + 1;\n        }\n    }\n    // start one byte into the cname buffer so we can do faster checks on if a\n    // name exists or not. so extend the buffer by one byte.\n    namelen++;\n\n    char *namebuf = calloc(1, namelen);\n    // copy names into the compact buffer\n    char *p = namebuf + 1;\n    for (int x = 0; x < ctx->user_stats_num; x++) {\n        struct proxy_user_stats_entry *e = &entries[x];\n        char *newname = NULL;\n        if (e->name) {\n            // skip blank names.\n            if (e->name[0]) {\n                newname = e->name;\n            }\n        } else if (e->cname) {\n            // else re-copy from old buffer\n            newname = oldnamebuf + e->cname;\n        }\n\n        if (newname) {\n            // set the buffer offset for this name\n            e->cname = p - namebuf;\n            // copy in the name\n            size_t nlen = strlen(newname);\n            memcpy(p, newname, nlen);\n            p += nlen;\n            *p = '\\0'; // add null byte\n            p++;\n        } else {\n            // name is blank or doesn't exist, ensure we skip it.\n            e->cname = 0;\n        }\n\n        if (e->name) {\n            // now get rid of the name buffer.\n            free(e->name);\n            e->name = NULL;\n        }\n    }\n\n    ctx->user_stats_namebuf = namebuf;\n    if (oldnamebuf) {\n        free(oldnamebuf);\n    }\n    STAT_UL(ctx);\n}\n\nstatic void proxy_config_reload(proxy_ctx_t *ctx) {\n    LOGGER_LOG(NULL, LOG_PROXYEVENTS, LOGGER_PROXY_CONFIG, NULL, \"start\");\n    STAT_INCR(ctx, config_reloads, 1);\n    // gen. used for tracking object lifecycles over time.\n    // ie: ensuring old things are unloaded.\n    ctx->config_generation++;\n    lua_State *L = ctx->proxy_state;\n    lua_settop(L, 0); // clear off any crud that could have been left on the stack.\n\n    // The main stages of config reload are:\n    // - load and execute the config file\n    // - run mcp_config_pools()\n    // - for each worker:\n    //   - copy and execute new lua code\n    //   - copy selector table\n    //   - run mcp_config_routes()\n\n    if (proxy_load_config(ctx) != 0) {\n        // Failed to load. log and wait for a retry.\n        STAT_INCR(ctx, config_reload_fails, 1);\n        LOGGER_LOG(NULL, LOG_PROXYEVENTS, LOGGER_PROXY_CONFIG, NULL, \"failed\");\n        return;\n    }\n\n    proxy_config_stats_prep(ctx);\n\n    // TODO (v2): create a temporary VM to test-load the worker code into.\n    // failing to load partway through the worker VM reloads can be\n    // critically bad if we're not careful about references.\n    // IE: the config VM _must_ hold references to selectors and backends\n    // as long as they exist in any worker for any reason.\n\n    for (int x = 0; x < settings.num_threads; x++) {\n        LIBEVENT_THREAD *thr = get_worker_thread(x);\n\n        pthread_mutex_lock(&ctx->worker_lock);\n        ctx->worker_done = false;\n        ctx->worker_failed = false;\n        proxy_reload_notify(thr);\n        while (!ctx->worker_done) {\n            // in case of spurious wakeup.\n            pthread_cond_wait(&ctx->worker_cond, &ctx->worker_lock);\n        }\n        pthread_mutex_unlock(&ctx->worker_lock);\n\n        // Code load bailed.\n        if (ctx->worker_failed) {\n            STAT_INCR(ctx, config_reload_fails, 1);\n            LOGGER_LOG(NULL, LOG_PROXYEVENTS, LOGGER_PROXY_CONFIG, NULL, \"failed\");\n            return;\n        }\n    }\n\n    // Need to clear the reset flag for the stats system after pushing the new\n    // config to each worker.\n    STAT_L(ctx);\n    for (int x = 0; x < ctx->user_stats_num; x++) {\n        ctx->user_stats[x].reset = false;\n    }\n    STAT_UL(ctx);\n\n    lua_pop(ctx->proxy_state, 1); // drop config_pools return value\n    LOGGER_LOG(NULL, LOG_PROXYEVENTS, LOGGER_PROXY_CONFIG, NULL, \"done\");\n}\n\n// Very basic scheduler. Unsorted because we don't expect a huge list of\n// functions to run.\nstatic void proxy_run_crons(proxy_ctx_t *ctx) {\n    lua_State *L = ctx->proxy_state;\n    assert(lua_gettop(L) == 0);\n    assert(ctx->cron_ref);\n    struct timespec now;\n\n    // Fetch the cron table. Created on startup so must exist.\n    lua_rawgeti(L, LUA_REGISTRYINDEX, ctx->cron_ref);\n\n    clock_gettime(CLOCK_REALTIME, &now);\n    if (ctx->cron_next <= now.tv_sec) {\n        ctx->cron_next = INT_MAX;\n    } else {\n        // no crons ready.\n        return;\n    }\n\n    // Loop the cron entries.\n    lua_pushnil(L);\n    while (lua_next(L, 1) != 0) {\n        const char *key = lua_tostring(L, -2);\n        mcp_cron_t *ce = lua_touserdata(L, -1);\n        int idx = lua_absindex(L, -1);\n\n        // check generation.\n        if (ctx->config_generation != ce->gen) {\n            // remove entry.\n            lua_pushnil(L);\n            lua_setfield(L, 1, key);\n        } else if (ce->next <= now.tv_sec) {\n            // grab func and execute it\n            lua_getiuservalue(L, idx, 1);\n            // no arguments or return values\n            int res = lua_pcall(L, 0, 0, 0);\n            STAT_INCR(ctx, config_cron_runs, 1);\n            if (res != LUA_OK) {\n                LOGGER_LOG(NULL, LOG_PROXYEVENTS, LOGGER_PROXY_ERROR, NULL, lua_tostring(L, -1));\n                STAT_INCR(ctx, config_cron_fails, 1);\n                lua_pop(L, 1); // drop error.\n            }\n\n            if (ce->repeat) {\n                ce->next = now.tv_sec + ce->every;\n                // if rescheduled, check next against ctx. update if sooner\n                if (ctx->cron_next > ce->next) {\n                    ctx->cron_next = ce->next;\n                }\n            } else {\n                // non-repeating cron. delete entry.\n                lua_pushnil(L);\n                lua_setfield(L, 1, key);\n            }\n        } else {\n            // not scheduled to run now, but check if we're next.\n            if (ctx->cron_next > ce->next) {\n                ctx->cron_next = ce->next;\n            }\n        }\n\n        lua_pop(L, 1); // drop value so we can loop.\n    }\n\n    lua_pop(L, 1); // drop cron table.\n}\n\n// Thread handling the configuration reload sequence.\n// TODO (v2): get a logger instance.\n// TODO (v2): making this \"safer\" will require a few phases of work.\n// 1) JFDI\n// 2) \"test VM\" -> from config thread, test the worker reload portion.\n// 3) \"unit testing\" -> from same temporary worker VM, execute set of\n// integration tests that must pass.\n// 4) run update on each worker, collecting new mcp.attach() hooks.\n//    Once every worker has successfully executed and set new hooks, roll\n//    through a _second_ time to actually swap the hook structures and unref\n//    the old structures where marked dirty.\nstatic void *_proxy_config_thread(void *arg) {\n    proxy_ctx_t *ctx = arg;\n    struct timespec wait = {0};\n\n    logger_create();\n    pthread_mutex_lock(&ctx->config_lock);\n    pthread_cond_signal(&ctx->config_cond);\n    while (1) {\n        ctx->loading = false;\n\n        // cron only thinks in whole seconds.\n        wait.tv_sec = ctx->cron_next;\n        pthread_cond_timedwait(&ctx->config_cond, &ctx->config_lock, &wait);\n\n        proxy_run_crons(ctx);\n\n        if (ctx->loading) {\n            proxy_config_reload(ctx);\n        }\n    }\n\n    return NULL;\n}\n\nint _start_proxy_config_threads(proxy_ctx_t *ctx) {\n    int ret;\n\n    pthread_mutex_lock(&ctx->config_lock);\n    if ((ret = pthread_create(&ctx->config_tid, NULL,\n                    _proxy_config_thread, ctx)) != 0) {\n        fprintf(stderr, \"Failed to start proxy configuration thread: %s\\n\",\n                strerror(ret));\n        pthread_mutex_unlock(&ctx->config_lock);\n        return -1;\n    }\n    thread_setname(ctx->config_tid, \"mc-prx-config\");\n    // Avoid returning until the config thread has actually started.\n    pthread_cond_wait(&ctx->config_cond, &ctx->config_lock);\n    pthread_mutex_unlock(&ctx->config_lock);\n\n    pthread_mutex_lock(&ctx->manager_lock);\n    if ((ret = pthread_create(&ctx->manager_tid, NULL,\n                    _proxy_manager_thread, ctx)) != 0) {\n        fprintf(stderr, \"Failed to start proxy manager thread: %s\\n\",\n                strerror(ret));\n        pthread_mutex_unlock(&ctx->manager_lock);\n        return -1;\n    }\n    thread_setname(ctx->manager_tid, \"mc-prx-manager\");\n    pthread_mutex_unlock(&ctx->manager_lock);\n\n    return 0;\n}\n\n// this splits a list of lua startfiles into independent data chunk buffers\n// we call this once the first time we start so we can use mallocs without\n// having to armor against runtime malloc failures... as much.\nstatic int proxy_init_startfiles(proxy_ctx_t *ctx, const char *files) {\n    char *flist = strdup(settings.proxy_startfile);\n    if (flist == NULL) {\n        fprintf(stderr, \"ERROR: failed to allocate memory for parsing proxy_startfile\\n\");\n        return -1;\n    }\n\n    char *b;\n    for (const char *p = strtok_r(flist, \":\", &b);\n            p != NULL;\n            p = strtok_r(NULL, \":\", &b)) {\n        struct _mcp_luafile *db = calloc(sizeof(struct _mcp_luafile), 1);\n        if (db == NULL) {\n            fprintf(stderr, \"ERROR: failed to allocate memory for parsing proxy_startfile\\n\");\n            return -1;\n        }\n        db->size = MCP_LUAFILE_SIZE;\n        db->buf = calloc(db->size, 1);\n        db->fname = strdup(p);\n        if (db->buf == NULL || db->fname == NULL) {\n            fprintf(stderr, \"ERROR: failed to allocate memory while parsing proxy_startfile\\n\");\n            return -1;\n        }\n\n        // put new file at tail\n        if (ctx->proxy_code == NULL) {\n            ctx->proxy_code = db;\n        } else {\n            struct _mcp_luafile *list = ctx->proxy_code;\n            while (list->next) {\n                list = list->next;\n            }\n            assert(list->next == NULL);\n            list->next = db;\n        }\n    }\n\n    free(flist);\n    return 0;\n}\n\nstatic int proxy_load_files(proxy_ctx_t *ctx) {\n    lua_State *L = ctx->proxy_state;\n    struct _mcp_luafile *db = ctx->proxy_code;\n    assert(db);\n\n    while (db) {\n        int res;\n        // clear the buffer for reuse.\n        memset(db->buf, 0, db->size);\n        db->used = 0;\n\n        if (strcmp(db->fname, \"routelib\") == 0) {\n            res = luaL_loadbuffer(L, routelib_lua, routelib_lua_len, \"routelib\");\n        } else {\n            res = luaL_loadfile(L, db->fname);\n        }\n        if (res != LUA_OK) {\n            fprintf(stderr, \"ERROR: Failed to load proxy_startfile: %s\\n\", lua_tostring(L, -1));\n            return -1;\n        }\n        // LUA_OK, LUA_ERRSYNTAX, LUA_ERRMEM, LUA_ERRFILE\n\n        // Now we need to dump the compiled code into bytecode.\n        // This will then get loaded into worker threads.\n        lua_dump(L, _dump_helper, db, 0);\n        // 0 means no error.\n\n        // now we complete the data load by calling the function.\n        res = lua_pcall(L, 0, LUA_MULTRET, 0);\n        if (res != LUA_OK) {\n            fprintf(stderr, \"ERROR: Failed to load data into lua config state: %s\\n\", lua_tostring(L, -1));\n            exit(EXIT_FAILURE);\n        }\n\n        db = db->next;\n    }\n\n    return 0;\n}\n\nint proxy_load_config(void *arg) {\n    proxy_ctx_t *ctx = arg;\n    lua_State *L = ctx->proxy_state;\n    int res = 0;\n\n    if (ctx->proxy_code == NULL) {\n        res = proxy_init_startfiles(ctx, settings.proxy_startfile);\n        if (res != 0) {\n            return res;\n        }\n    }\n\n    // load each of the data files in order.\n    res = proxy_load_files(ctx);\n\n    // call the mcp_config_pools function to get the central backends.\n    lua_getglobal(L, \"mcp_config_pools\");\n\n    if (lua_isnil(L, -1)) {\n        fprintf(stderr, \"ERROR: Configuration file missing 'mcp_config_pools' function\\n\");\n        exit(EXIT_FAILURE);\n    }\n    lua_pushnil(L); // no \"old\" config yet.\n    if (lua_pcall(L, 1, 1, 0) != LUA_OK) {\n        fprintf(stderr, \"ERROR: Failed to execute mcp_config_pools: %s\\n\", lua_tostring(L, -1));\n        exit(EXIT_FAILURE);\n    }\n\n    // result is our main config.\n    return 0;\n}\n\nstatic int _copy_pool(lua_State *from, lua_State *to, LIBEVENT_THREAD *thr) {\n    // from, -3 should have the userdata.\n    mcp_pool_t *p = luaL_checkudata(from, -3, \"mcp.pool\");\n    size_t size = sizeof(mcp_pool_proxy_t);\n    mcp_pool_proxy_t *pp = lua_newuserdatauv(to, size, 0);\n    luaL_setmetatable(to, \"mcp.pool_proxy\");\n\n    pp->main = p;\n    if (p->use_iothread) {\n        pp->pool = p->pool;\n    } else {\n        // allow 0 indexing for backends when unique to each worker thread\n        pp->pool = &p->pool[thr->thread_baseid * p->pool_size];\n    }\n    lua_pushvalue(from, -3); // dupe pool for referencing\n    mcp_gobj_ref(from, &p->g); // pops obj copy\n    return 0;\n}\n\nstatic void _copy_config_table(lua_State *from, lua_State *to, LIBEVENT_THREAD *thr);\n// (from, -1) is the source value\n// should end with (to, -1) being the new value.\nstatic void _copy_config_table(lua_State *from, lua_State *to, LIBEVENT_THREAD *thr) {\n    int type = lua_type(from, -1);\n    bool found = false;\n    luaL_checkstack(from, 4, \"configuration error: table recursion too deep\");\n    luaL_checkstack(to, 4, \"configuration error: table recursion too deep\");\n    switch (type) {\n        case LUA_TNIL:\n            lua_pushnil(to);\n            break;\n        case LUA_TUSERDATA:\n            // see dump_stack() - check if it's something we handle.\n            if (lua_getmetatable(from, -1) != 0) {\n                lua_pushstring(from, \"__name\");\n                if (lua_rawget(from, -2) != LUA_TNIL) {\n                    const char *name = lua_tostring(from, -1);\n                    if (strcmp(name, \"mcp.pool\") == 0) {\n                        _copy_pool(from, to, thr);\n                        found = true;\n                    } else if (strcmp(name, \"mcp.ratelim_global_tbf\") == 0) {\n                        mcp_ratelim_proxy_tbf(from, to);\n                        found = true;\n                    }\n                }\n                lua_pop(from, 2);\n            }\n            if (!found) {\n                proxy_lua_error(from, \"unhandled userdata type in configuration table\\n\");\n            }\n            break;\n        case LUA_TNUMBER:\n            if (lua_isinteger(from, -1)) {\n                lua_pushinteger(to, lua_tointeger(from, -1));\n            } else {\n                lua_pushnumber(to, lua_tonumber(from, -1));\n            }\n            break;\n        case LUA_TSTRING:\n            lua_pushlstring(to, lua_tostring(from, -1), lua_rawlen(from, -1));\n            break;\n        case LUA_TBOOLEAN:\n            lua_pushboolean(to, lua_toboolean(from, -1));\n            break;\n        case LUA_TTABLE:\n            // TODO (v2): copy the metatable first?\n            // TODO (v2): size narr/nrec from old table and use createtable to\n            // pre-allocate.\n            lua_newtable(to); // throw new table on worker\n            int t = lua_absindex(from, -1); // static index of table to copy.\n            int nt = lua_absindex(to, -1); // static index of new table.\n            lua_pushnil(from); // start iterator for main\n            while (lua_next(from, t) != 0) {\n                // (key, -2), (val, -1)\n                int keytype = lua_type(from, -2);\n                // to intentionally limit complexity and allow for future\n                // optimizations we restrict what types may be used as keys\n                // for sub-tables.\n                switch (keytype) {\n                    case LUA_TSTRING:\n                        // to[l]string converts the actual key in the table\n                        // into a string, so we must not do that unless it\n                        // already is one.\n                        lua_pushlstring(to, lua_tostring(from, -2), lua_rawlen(from, -2));\n                        break;\n                    case LUA_TNUMBER:\n                        if (lua_isinteger(from, -2)) {\n                            lua_pushinteger(to, lua_tointeger(from, -2));\n                        } else {\n                            lua_pushnumber(to, lua_tonumber(from, -2));\n                        }\n                        break;\n                    default:\n                        proxy_lua_error(from, \"configuration table keys must be strings or numbers\");\n                }\n                // lua_settable(to, n) - n being the table\n                // takes -2 key -1 value, pops both.\n                // use lua_absindex(L, -1) and so to convert easier?\n                _copy_config_table(from, to, thr); // push next value.\n                lua_settable(to, nt);\n                lua_pop(from, 1); // drop value, keep key.\n            }\n            // top of from is now the original table.\n            // top of to should be the new table.\n            break;\n        default:\n            proxy_lua_error(from, \"unhandled data type in configuration table\\n\");\n    }\n}\n\n// Run from proxy worker to coordinate code reload.\n// config_lock must be held first.\nvoid proxy_worker_reload(void *arg, LIBEVENT_THREAD *thr) {\n    proxy_ctx_t *ctx = arg;\n    pthread_mutex_lock(&ctx->worker_lock);\n    if (proxy_thread_loadconf(ctx, thr) != 0) {\n        ctx->worker_failed = true;\n    }\n    ctx->worker_done = true;\n    pthread_cond_signal(&ctx->worker_cond);\n    pthread_mutex_unlock(&ctx->worker_lock);\n}\n\n// FIXME (v2): need to test how to recover from an actual error here. error message\n// needs to go somewhere useful, counters added, etc.\nint proxy_thread_loadconf(proxy_ctx_t *ctx, LIBEVENT_THREAD *thr) {\n    lua_State *L = thr->L;\n    // load the precompiled config functions.\n\n    struct _mcp_luafile *db = ctx->proxy_code;\n    while (db) {\n        db->loaded = false;\n        int res = lua_load(L, _load_helper, db, \"config\", NULL);\n        if (res != LUA_OK) {\n            fprintf(stderr, \"Failed to load data into worker thread: %s\\n\", lua_tostring(L, -1));\n            return -1;\n        }\n\n        res = lua_pcall(L, 0, LUA_MULTRET, 0);\n        if (res != LUA_OK) {\n            // FIXME (v2): don't exit here!\n            fprintf(stderr, \"Failed to load data into worker thread: %s\\n\", lua_tostring(L, -1));\n            return -1;\n        }\n\n        db = db->next;\n    }\n\n    lua_getglobal(L, \"mcp_config_routes\");\n    // create deepcopy of argument to pass into mcp_config_routes.\n    // FIXME (v2): to avoid lua SIGABRT'ing on errors we need to protect the call\n    // normal pattern:\n    // lua_pushcfunction(L, &_copy_config_table);\n    // lua_pushlightuserdata(L, &L2);\n    // res = la_pcall(L, etc);\n    // ... but since this is cross-VM we could get errors from not the\n    // protected VM, breaking setjmp/etc.\n    // for this part of the code we should override lua_atpanic(),\n    // allowing us to specifically recover and bail.\n    // However, again, this will require the next version of the config reload\n    // code since we are re-using the VM's and a panic can leave us in a\n    // broken state.\n    // If the setjump/longjump combos are compatible a pcall for from and\n    // atpanic for to might work best, since the config VM is/should be long\n    // running and worker VM's should be rotated.\n    _copy_config_table(ctx->proxy_state, L, thr);\n\n    // copied value is in front of route function, now call it.\n    if (lua_pcall(L, 1, 0, 0) != LUA_OK) {\n        fprintf(stderr, \"Failed to execute mcp_config_routes: %s\\n\", lua_tostring(L, -1));\n        return -1;\n    }\n\n    // update user stats\n    STAT_L(ctx);\n    struct proxy_user_stats_entry *us = ctx->user_stats;\n    int stats_num = ctx->user_stats_num;\n    struct proxy_user_stats *tus = NULL;\n    if (stats_num != 0) {\n        pthread_mutex_lock(&thr->stats.mutex);\n        if (thr->proxy_user_stats == NULL) {\n            tus = calloc(1, sizeof(struct proxy_user_stats));\n            thr->proxy_user_stats = tus;\n        } else {\n            tus = thr->proxy_user_stats;\n        }\n\n        // originally this was a realloc routine but it felt fragile.\n        // that might still be a better idea; still need to zero out the end.\n        uint64_t *counters = calloc(stats_num, sizeof(uint64_t));\n\n        // note that num_stats can _only_ grow in size.\n        if (tus->counters) {\n            // pull in old counters, if the names didn't change.\n            for (int x = 0; x < tus->num_stats; x++) {\n                if (us[x].reset) {\n                    counters[x] = 0;\n                } else {\n                    counters[x] = tus->counters[x];\n                }\n            }\n            assert(tus->num_stats <= stats_num);\n            free(tus->counters);\n        }\n        tus->counters = counters;\n        tus->num_stats = stats_num;\n\n        pthread_mutex_unlock(&thr->stats.mutex);\n    }\n    // also grab the concurrent request limit\n    thr->proxy_active_req_limit = ctx->active_req_limit;\n    STAT_UL(ctx);\n\n    // update limit counter(s)\n    pthread_mutex_lock(&thr->proxy_limit_lock);\n    thr->proxy_buffer_memory_limit = ctx->buffer_memory_limit;\n    pthread_mutex_unlock(&thr->proxy_limit_lock);\n\n    return 0;\n}\n\n\n"
        },
        {
          "name": "proxy_inspector.c",
          "type": "blob",
          "size": 21.3701171875,
          "content": "/* -*- Mode: C; tab-width: 4; c-basic-offset: 4; indent-tabs-mode: nil -*- */\n\n#include \"proxy.h\"\n\nenum mcp_ins_type {\n    INS_REQ = 1,\n    INS_RES,\n};\n\nenum mcp_ins_steptype {\n    mcp_ins_step_none = 0,\n    mcp_ins_step_sepkey,\n    mcp_ins_step_keybegin,\n    mcp_ins_step_keyis,\n    mcp_ins_step_hasflag,\n    mcp_ins_step_flagtoken,\n    mcp_ins_step_flagint,\n    mcp_ins_step_flagis,\n    mcp_ins_step_final, // not used.\n};\n\n// START STEP STRUCTS\n\nstruct mcp_ins_sepkey {\n    char sep;\n    int pos;\n    int mapref;\n};\n\nstruct mcp_ins_string {\n    unsigned int str; // arena offset for match string.\n    unsigned int len;\n};\n\nstruct mcp_ins_flag {\n    uint64_t bit; // flag converted for bitmask test\n    char f;\n};\n\n// TODO: it might make more sense to flatten the structs into the ins_step\n// struct. It wouldn't take much more space if we can be careful with\n// alignment.\nstruct mcp_ins_flagstr {\n    unsigned int str;\n    unsigned int len;\n    uint64_t bit; // flag bit\n    char f;\n};\n\nstruct mcp_ins_step {\n    enum mcp_ins_steptype type;\n    union {\n        struct mcp_ins_sepkey sepkey;\n        struct mcp_ins_string string;\n        struct mcp_ins_flag flag;\n        struct mcp_ins_flagstr flagstr;\n    } c;\n};\n\n// END STEP STRUCTS\n\nstruct mcp_inspector {\n    enum mcp_ins_type type;\n    int scount;\n    unsigned int aused; // arena memory used\n    unsigned int rcount; // number of results to expect\n    char *arena; // string/data storage for steps\n    struct mcp_ins_step steps[];\n};\n\n// PRIVATE INTERFACE\n\n#define res_buf(r) (r->cresp ? r->cresp->iov[0].iov_base : r->buf)\n\n// COMMON ARG HANDLERS\n\n// multiple step types only take 'flag' as an argument.\nstatic int mcp_inspector_flag_c_g(lua_State *L, int tidx) {\n    if (lua_getfield(L, tidx, \"flag\") != LUA_TNIL) {\n        size_t len = 0;\n        const char *flag = lua_tolstring(L, -1, &len);\n        if (len != 1) {\n            proxy_lua_ferror(L, \"inspector step %d: 'flag' must be a single character\", tidx);\n        }\n        if (mcp_is_flag_invalid(flag[0])) {\n            proxy_lua_ferror(L, \"inspect step %d: 'flag' must be alphanumeric\", tidx);\n        }\n    } else {\n        proxy_lua_ferror(L, \"inspector step %d: must provide 'flag' argument\", tidx);\n    }\n    lua_pop(L, 1); // val or nil\n    return 0;\n}\n\nstatic int mcp_inspector_flag_i_g(lua_State *L, int tidx, int sc, struct mcp_inspector *ins) {\n    struct mcp_ins_step *s = &ins->steps[sc];\n    struct mcp_ins_flag *c = &s->c.flag;\n\n    if (lua_getfield(L, tidx, \"flag\") != LUA_TNIL) {\n        const char *flag = lua_tostring(L, -1);\n        c->f = flag[0];\n        c->bit = (uint64_t)1 << (c->f - 65);\n    }\n    lua_pop(L, 1); // val or nil\n\n    return 0;\n}\n\nstatic int mcp_inspector_string_c_g(lua_State *L, int tidx) {\n    size_t len = 0;\n\n    if (lua_getfield(L, tidx, \"str\") != LUA_TNIL) {\n        lua_tolstring(L, -1, &len);\n        if (len < 1) {\n            proxy_lua_ferror(L, \"inspector step %d: 'str' must have nonzero length\", tidx);\n        }\n    } else {\n        proxy_lua_ferror(L, \"inspector step %d: must provide 'str' argument\", tidx);\n    }\n    lua_pop(L, 1); // val or nil\n\n    return len;\n}\n\nstatic int mcp_inspector_string_i_g(lua_State *L, int tidx, int sc, struct mcp_inspector *ins) {\n    struct mcp_ins_step *s = &ins->steps[sc];\n    struct mcp_ins_string *c = &s->c.string;\n    size_t len = 0;\n\n    // store our match string in the arena space that we reserved before.\n    if (lua_getfield(L, tidx, \"str\") != LUA_TNIL) {\n        const char *str = lua_tolstring(L, -1, &len);\n        c->str = ins->aused;\n        c->len = len;\n        char *a = ins->arena + ins->aused;\n        memcpy(a, str, len);\n        ins->aused += len;\n    }\n    lua_pop(L, 1); // val or nil\n\n    return len;\n}\n\n// END COMMMON ARG HANDLERS\n\nstatic int mcp_inspector_sepkey_c(lua_State *L, int tidx) {\n    if (lua_getfield(L, tidx, \"sep\") != LUA_TNIL) {\n        size_t len = 0;\n        lua_tolstring(L, -1, &len);\n        if (len != 1) {\n            proxy_lua_ferror(L, \"inspector step %d: separator must be one character\", tidx);\n        }\n    }\n    lua_pop(L, 1); // val or nil\n\n    if (lua_getfield(L, tidx, \"pos\") != LUA_TNIL) {\n        luaL_checktype(L, -1, LUA_TNUMBER);\n    }\n    lua_pop(L, 1); // val or nil\n\n    if (lua_getfield(L, tidx, \"map\") != LUA_TNIL) {\n        luaL_checktype(L, -1, LUA_TTABLE);\n    }\n    lua_pop(L, 1); // val or nil\n\n    return 0;\n}\n\n// initializer. arguments already checked, so just fill out the slot.\nstatic int mcp_inspector_sepkey_i(lua_State *L, int tidx, int sc, struct mcp_inspector *ins) {\n    struct mcp_ins_step *s = &ins->steps[sc];\n    struct mcp_ins_sepkey *c = &s->c.sepkey;\n\n    if (lua_getfield(L, tidx, \"sep\") != LUA_TNIL) {\n        const char *sep = lua_tostring(L, -1);\n        c->sep = sep[0];\n    } else {\n        // default separator\n        c->sep = '/';\n    }\n    lua_pop(L, 1); // val or nil\n\n    if (lua_getfield(L, tidx, \"pos\") != LUA_TNIL) {\n        c->pos = lua_tointeger(L, -1);\n    } else {\n        c->pos = 1;\n    }\n    lua_pop(L, 1);\n\n    if (lua_getfield(L, tidx, \"map\") != LUA_TNIL) {\n        c->mapref = luaL_ref(L, LUA_REGISTRYINDEX);\n    } else {\n        c->mapref = 0;\n        lua_pop(L, 1);\n    }\n    // ref was popped\n\n    return 0;\n}\n\n// TODO: abstract out the token-position-finder\nstatic int mcp_inspector_sepkey_r(lua_State *L, struct mcp_inspector *ins, struct mcp_ins_step *s, void *arg) {\n    mcp_request_t *rq = arg;\n    struct mcp_ins_sepkey *c = &s->c.sepkey;\n\n    const char *key = MCP_PARSER_KEY(rq->pr);\n    const char *end = key + rq->pr.klen;\n    char sep = c->sep;\n    int pos = c->pos;\n\n    // skip initial separators\n    while (key != end) {\n        if (*key == sep) {\n            key++;\n        } else {\n            break;\n        }\n    }\n    const char *token = key;\n    int tlen = 0;\n\n    while (key != end) {\n        if (*key == sep) {\n            // measure token length and stop if at position.\n            if (--pos == 0) {\n                tlen = key - token;\n                break;\n            } else {\n                // NOTE: this could point past the end of the key, but unless\n                // it's the token we want we won't look at it.\n                token = key+1;\n            }\n        }\n        key++;\n    }\n\n    // either the separator was never found, or we ended before finding\n    // another one, which gives us an end token.\n    if (pos == 1) {\n        tlen = key - token;\n    }\n\n    // now have *token and tlen\n    if (tlen != 0) {\n        if (c->mapref) {\n            // look up this string against the map.\n            // NOTE: this still ends up creating a garbage string. However,\n            // since the map is internal we can optimize this later by moving\n            // the map lookup op to C.\n            lua_rawgeti(L, LUA_REGISTRYINDEX, c->mapref);\n            lua_pushlstring(L, token, tlen);\n            lua_rawget(L, -2); // pops string.\n            lua_remove(L, -2); // removes map, shifts lookup result down.\n            // stack should be clean: just the result.\n        } else {\n            // no map, return the actual token.\n            lua_pushlstring(L, token, tlen);\n        }\n    } else {\n        lua_pushnil(L); // not found.\n    }\n\n    return 1;\n}\n\nstatic int mcp_inspector_keybegin_r(lua_State *L, struct mcp_inspector *ins, struct mcp_ins_step *s, void *arg) {\n    mcp_request_t *rq = arg;\n    struct mcp_ins_string *c = &s->c.string;\n\n    const char *key = MCP_PARSER_KEY(rq->pr);\n    int klen = rq->pr.klen;\n    const char *str = ins->arena + c->str;\n\n    if (c->len < klen && strncmp(key, str, c->len) == 0) {\n        lua_pushboolean(L, 1);\n    } else {\n        lua_pushboolean(L, 0);\n    }\n\n    return 1;\n}\n\nstatic int mcp_inspector_keyis_r(lua_State *L, struct mcp_inspector *ins, struct mcp_ins_step *s, void *arg) {\n    mcp_request_t *rq = arg;\n    struct mcp_ins_string *c = &s->c.string;\n\n    const char *key = MCP_PARSER_KEY(rq->pr);\n    int klen = rq->pr.klen;\n    const char *str = ins->arena + c->str;\n\n    if (c->len == klen && strncmp(key, str, c->len) == 0) {\n        lua_pushboolean(L, 1);\n    } else {\n        lua_pushboolean(L, 0);\n    }\n\n    return 1;\n}\n\nstatic int mcp_inspector_hasflag_r(lua_State *L, struct mcp_inspector *ins, struct mcp_ins_step *s, void *arg) {\n    struct mcp_ins_flag *c = &s->c.flag;\n    if (ins->type == INS_REQ) {\n        mcp_request_t *rq = arg;\n        // requests should always be tokenized, so we can just check the bit.\n        if (rq->pr.t.meta.flags & c->bit) {\n            lua_pushboolean(L, 1);\n        } else {\n            lua_pushboolean(L, 0);\n        }\n    } else {\n        mcp_resp_t *res = arg;\n        if (res->resp.type == MCMC_RESP_META) {\n            // result object may not be tokenized. this will do so if not\n            // already. any future hits agains the same object will use the\n            // cached tokenizer struct.\n            mcmc_tokenize_res(res_buf(res), res->resp.reslen, &res->tok);\n            if (mcmc_token_has_flag_bit(&res->tok, c->bit) == MCMC_OK) {\n                lua_pushboolean(L, 1);\n            } else {\n                lua_pushboolean(L, 0);\n            }\n        } else {\n            proxy_lua_error(L, \"inspector error: response is not meta protocol\");\n        }\n    }\n    return 1;\n}\n\n// This mirrors `bool, (str|nil) = r:flag_token(\"T\")`\nstatic int mcp_inspector_flagtoken_r(lua_State *L, struct mcp_inspector *ins, struct mcp_ins_step *s, void *arg) {\n    struct mcp_ins_flag *c = &s->c.flag;\n    if (ins->type == INS_REQ) {\n        mcp_request_t *rq = arg;\n\n        if (rq->pr.t.meta.flags & c->bit) {\n            lua_pushboolean(L, 1); // flag exists\n            const char *tok = NULL;\n            size_t tlen = 0;\n            mcp_request_find_flag_token(rq, c->f, &tok, &tlen);\n            lua_pushlstring(L, tok, tlen); // flag's token\n            return 2;\n        }\n    } else {\n        mcp_resp_t *res = arg;\n        if (res->resp.type == MCMC_RESP_META) {\n            mcmc_tokenize_res(res_buf(res), res->resp.reslen, &res->tok);\n            if (mcmc_token_has_flag_bit(&res->tok, c->bit) == MCMC_OK) {\n                lua_pushboolean(L, 1); // flag exists\n                int tlen = 0;\n                const char *tok = mcmc_token_get_flag(res_buf(res), &res->tok, c->f, &tlen);\n                lua_pushlstring(L, tok, tlen); // flag's token\n                return 2;\n            }\n        }\n    }\n    lua_pushboolean(L, 0);\n    lua_pushnil(L);\n\n    return 2;\n}\n\n// TODO: flaguint variant?\n// still stuck as signed in lua but would reject signed tokens\nstatic int mcp_inspector_flagint_r(lua_State *L, struct mcp_inspector *ins, struct mcp_ins_step *s, void *arg) {\n    struct mcp_ins_flag *c = &s->c.flag;\n    if (ins->type == INS_REQ) {\n        mcp_request_t *rq = arg;\n\n        if (rq->pr.t.meta.flags & c->bit) {\n            lua_pushboolean(L, 1); // flag exists\n            int64_t tok = 0;\n            if (mcp_request_find_flag_tokenint64(rq, c->f, &tok) == 0) {\n                lua_pushinteger(L, tok);\n            } else {\n                lua_pushnil(L);\n            }\n            return 2;\n        }\n    } else {\n        mcp_resp_t *res = arg;\n        if (res->resp.type == MCMC_RESP_META) {\n            mcmc_tokenize_res(res_buf(res), res->resp.reslen, &res->tok);\n            if (mcmc_token_has_flag_bit(&res->tok, c->bit) == MCMC_OK) {\n                lua_pushboolean(L, 1); // flag exists\n                int64_t tok = 0;\n                if (mcmc_token_get_flag_64(res_buf(res), &res->tok, c->f, &tok) == MCMC_OK) {\n                    lua_pushinteger(L, tok);\n                } else {\n                    lua_pushnil(L); // token couldn't be converted\n                }\n                return 2;\n            }\n        }\n    }\n    lua_pushboolean(L, 0);\n    lua_pushnil(L);\n\n    return 2;\n}\n\nstatic int mcp_inspector_flagstr_c(lua_State *L, int tidx) {\n    mcp_inspector_flag_c_g(L, tidx);\n    int size = mcp_inspector_string_c_g(L, tidx);\n    return size;\n}\n\nstatic int mcp_inspector_flagstr_i(lua_State *L, int tidx, int sc, struct mcp_inspector *ins) {\n    // TODO: if we never use mcp_ins_step we can remove it and just pass parts\n    // of the relevant structs down into these functions.\n    struct mcp_ins_step *s = &ins->steps[sc];\n    struct mcp_ins_flagstr *c = &s->c.flagstr;\n    size_t len = 0;\n\n    if (lua_getfield(L, tidx, \"flag\") != LUA_TNIL) {\n        const char *flag = lua_tostring(L, -1);\n        c->f = flag[0];\n        c->bit = (uint64_t)1 << (c->f - 65);\n    }\n    lua_pop(L, 1); // val or nil\n\n    if (lua_getfield(L, tidx, \"str\") != LUA_TNIL) {\n        const char *str = lua_tolstring(L, -1, &len);\n        c->str = ins->aused;\n        c->len = len;\n        char *a = ins->arena + ins->aused;\n        memcpy(a, str, len);\n        ins->aused += len;\n    }\n    lua_pop(L, 1); // val or nil\n\n    return len;\n}\n\n// FIXME: size_t vs int consistency for tlen would shorten the code.\nstatic int mcp_inspector_flagis_r(lua_State *L, struct mcp_inspector *ins, struct mcp_ins_step *s, void *arg) {\n    struct mcp_ins_flagstr *c = &s->c.flagstr;\n    const char *str = ins->arena + c->str;\n    if (ins->type == INS_REQ) {\n        mcp_request_t *rq = arg;\n\n        if (rq->pr.t.meta.flags & c->bit) {\n            lua_pushboolean(L, 1); // flag exists\n            const char *tok = NULL;\n            size_t tlen = 0;\n            mcp_request_find_flag_token(rq, c->f, &tok, &tlen);\n            if (tlen == c->len && strncmp(tok, str, c->len) == 0) {\n                lua_pushboolean(L, 1);\n            } else {\n                lua_pushboolean(L, 0);\n            }\n            return 2;\n        }\n    } else {\n        mcp_resp_t *res = arg;\n        if (res->resp.type == MCMC_RESP_META) {\n            mcmc_tokenize_res(res_buf(res), res->resp.reslen, &res->tok);\n            if (mcmc_token_has_flag_bit(&res->tok, c->bit) == MCMC_OK) {\n                lua_pushboolean(L, 1); // flag exists\n                int tlen = 0;\n                const char *tok = mcmc_token_get_flag(res_buf(res), &res->tok, c->f, &tlen);\n                if (tlen == c->len && strncmp(tok, str, c->len) == 0) {\n                    lua_pushboolean(L, 1);\n                } else {\n                    lua_pushboolean(L, 0);\n                }\n                return 2;\n            }\n        }\n    }\n    lua_pushboolean(L, 0);\n    lua_pushnil(L);\n\n    return 2;\n}\n\n// END STEPS\n\ntypedef int (*mcp_ins_c)(lua_State *L, int tidx);\ntypedef int (*mcp_ins_i)(lua_State *L, int tidx, int sc, struct mcp_inspector *ins);\ntypedef int (*mcp_ins_r)(lua_State *L, struct mcp_inspector *ins, struct mcp_ins_step *s, void *arg);\n\nstruct mcp_ins_entry {\n    const char *s; // string name\n    mcp_ins_c c;\n    mcp_ins_i i;\n    mcp_ins_r r;\n    unsigned int t; // allowed object types\n    int n; // number of results to expect\n};\n\nstatic const struct mcp_ins_entry mcp_ins_entries[] = {\n    [mcp_ins_step_none] = {NULL, NULL, NULL, NULL, 0, 0},\n    [mcp_ins_step_sepkey] = {\"sepkey\", mcp_inspector_sepkey_c, mcp_inspector_sepkey_i, mcp_inspector_sepkey_r, INS_REQ, 1},\n    [mcp_ins_step_keybegin] = {\"keybegin\", mcp_inspector_string_c_g, mcp_inspector_string_i_g, mcp_inspector_keybegin_r, INS_REQ, 1},\n    [mcp_ins_step_keyis] = {\"keyis\", mcp_inspector_string_c_g, mcp_inspector_string_i_g, mcp_inspector_keyis_r, INS_REQ, 1},\n    [mcp_ins_step_hasflag] = {\"hasflag\", mcp_inspector_flag_c_g, mcp_inspector_flag_i_g, mcp_inspector_hasflag_r, INS_REQ|INS_RES, 1},\n    [mcp_ins_step_flagtoken] = {\"flagtoken\", mcp_inspector_flag_c_g, mcp_inspector_flag_i_g, mcp_inspector_flagtoken_r, INS_REQ|INS_RES, 2},\n    [mcp_ins_step_flagint] = {\"flagint\", mcp_inspector_flag_c_g, mcp_inspector_flag_i_g, mcp_inspector_flagint_r, INS_REQ|INS_RES, 2},\n    [mcp_ins_step_flagis] = {\"flagis\", mcp_inspector_flagstr_c, mcp_inspector_flagstr_i, mcp_inspector_flagis_r, INS_REQ|INS_RES, 2},\n};\n\n// call with type string on top\nstatic enum mcp_ins_steptype mcp_inspector_steptype(lua_State *L) {\n    const char *type = luaL_checkstring(L, -1);\n    for (int x = 0; x < mcp_ins_step_final; x++) {\n        const struct mcp_ins_entry *e = &mcp_ins_entries[x];\n        if (e->s && strcmp(type, e->s) == 0) {\n            return x;\n        }\n    }\n    return mcp_ins_step_none;\n}\n\n// - arguments given as list of tables:\n//   { t = \"type\", arg = \"bar\", etc },\n//   { etc }\n//   - can take table-of-tables via: mcp.req_inspector_new(table.unpack(args))\n// NOTES:\n// - can we inline necessary strings/etc via extra allocated memory?\n// - can we get mcp.inspector metatable into the upvalue of the _call and _gc\n// funcs for fast-compare?\nstatic int mcp_inspector_new(lua_State *L, enum mcp_ins_type type) {\n    int argc = lua_gettop(L);\n    size_t size = 0;\n    int scount = 0;\n\n    // loop argument tables once for validation and pre-calculations.\n    for (int x = 1; x <= argc; x++) {\n        luaL_checktype(L, x, LUA_TTABLE);\n        if (lua_getfield(L, x, \"t\") != LUA_TNIL) {\n            enum mcp_ins_steptype st = mcp_inspector_steptype(L);\n            const struct mcp_ins_entry *e = &mcp_ins_entries[st];\n            if (!(e->t & type)) {\n                proxy_lua_ferror(L, \"inspector step %d: step incompatible with inspector type\", x);\n            }\n            if ((st == mcp_ins_step_none) || e->c == NULL) {\n                proxy_lua_ferror(L, \"inspector step %d: unknown step type\", x);\n            }\n            size += e->c(L, x);\n        }\n        lua_pop(L, 1); // drop 't' or nil\n        scount++;\n    }\n\n    // we now know the size and number of steps. allocate some flat memory.\n\n    // TODO: we need memory for steps + arbitrary step data. (ie; string stems\n    // and the like)\n    // - now: single extra malloc, divvy out the buffer as requested\n    // - later: if alignment of the flexible step array can be reliably\n    // determined (C11 alignas or etc), inline memory can be used instead.\n    size_t extsize = sizeof(struct mcp_ins_step) * scount;\n    struct mcp_inspector *ins = lua_newuserdatauv(L, sizeof(*ins) + extsize, 1);\n    memset(ins, 0, sizeof(*ins));\n\n    ins->arena = malloc(size);\n    if (ins->arena == NULL) {\n        proxy_lua_error(L, \"mcp.req_inspector_new: failed to allocate memory\");\n    }\n\n    luaL_setmetatable(L, \"mcp.inspector\");\n    switch (type) {\n        case INS_REQ:\n            luaL_getmetatable(L, \"mcp.request\");\n            break;\n        case INS_RES:\n            luaL_getmetatable(L, \"mcp.response\");\n            break;\n    }\n    // set metatable to the upvalue for a fast comparison during __call\n    lua_setiuservalue(L, -2, 1);\n    ins->type = type;\n\n    // loop the arg tables again to fill in the steps\n    // skip checks since we did that during the first loop.\n    scount = 0;\n    for (int x = 1; x <= argc; x++) {\n        if (lua_getfield(L, x, \"t\") != LUA_TNIL) {\n            enum mcp_ins_steptype st = mcp_inspector_steptype(L);\n            ins->steps[scount].type = st;\n            mcp_ins_entries[st].i(L, x, scount, ins);\n            ins->rcount += mcp_ins_entries[st].n;\n        }\n        lua_pop(L, 1); // drop t or nil\n        scount++;\n    }\n\n    if (size != ins->aused) {\n        proxy_lua_error(L, \"inspector failed to properly initialize, memory not filled correctly\");\n    }\n    ins->scount = scount;\n\n    return 1;\n}\n\nstatic int mcp_ins_run(lua_State *L, struct mcp_inspector *ins, void *arg) {\n    int ret = 0;\n\n    for (int x = 0; x < ins->scount; x++) {\n        struct mcp_ins_step *s = &ins->steps[x];\n        assert(s->type != mcp_ins_step_none);\n        ret += mcp_ins_entries[s->type].r(L, ins, s, arg);\n    }\n\n    return ret;\n}\n\n// PUBLIC INTERFACE\n\nint mcplib_req_inspector_new(lua_State *L) {\n    return mcp_inspector_new(L, INS_REQ);\n}\n\nint mcplib_res_inspector_new(lua_State *L) {\n    return mcp_inspector_new(L, INS_RES);\n}\n\n// walk each step and free references/memory/etc\nint mcplib_inspector_gc(lua_State *L) {\n    struct mcp_inspector *ins = lua_touserdata(L, 1);\n\n    if (ins->arena) {\n        free(ins->arena);\n        ins->arena = NULL;\n    }\n\n    for (int x = 0; x < ins->scount; x++) {\n        struct mcp_ins_step *s = &ins->steps[x];\n        switch (s->type) {\n            case mcp_ins_step_sepkey:\n                if (s->c.sepkey.mapref) {\n                    luaL_unref(L, LUA_REGISTRYINDEX, s->c.sepkey.mapref);\n                    s->c.sepkey.mapref = 0;\n                }\n                break;\n            case mcp_ins_step_keybegin:\n            case mcp_ins_step_keyis:\n            case mcp_ins_step_hasflag:\n            case mcp_ins_step_flagtoken:\n            case mcp_ins_step_flagint:\n            case mcp_ins_step_flagis:\n            case mcp_ins_step_none:\n            case mcp_ins_step_final:\n                break;\n        }\n    }\n\n    return 0;\n}\n\n// - iterate steps, call function callbacks with as context arg\n// TODO:\n// - second arg _may_ be a table: in which case we fill the results into this\n//   table rather than return them directly.\n//   - do this via a different run function that pops each step result?\nint mcplib_inspector_call(lua_State *L) {\n    // since we're here from a __call, assume the type is correct.\n    struct mcp_inspector *ins = lua_touserdata(L, 1);\n    luaL_checktype(L, 2, LUA_TUSERDATA);\n    if (lua_checkstack(L, ins->rcount) == 0) {\n        proxy_lua_error(L, \"inspector ran out of stack space for results\");\n    }\n\n    // luaL_checkudata() is slow. Trying a new method here where we pull the\n    // metatable from a reference then compare it against the meta table of\n    // the argument object.\n    lua_getmetatable(L, 2); // put arg metatable on stack\n    lua_getiuservalue(L, 1, 1); // put stashed metatable on stack\n    luaL_argcheck(L, lua_rawequal(L, -1, -2), 2,\n            \"invalid argument to inspector object\");\n    lua_pop(L, 2); // toss both metatables\n\n    // we're valid now. run the steps\n    void *arg = lua_touserdata(L, 2);\n    return mcp_ins_run(L, ins, arg);\n}\n"
        },
        {
          "name": "proxy_internal.c",
          "type": "blob",
          "size": 57.2001953125,
          "content": "/* -*- Mode: C; tab-width: 4; c-basic-offset: 4; indent-tabs-mode: nil -*- */\n// Functions related to local command execution.\n\n#include \"proxy.h\"\n#include \"storage.h\"\n\n#define PROXY_STORAGE_GET 0\n#define PROXY_STORAGE_MG 1\n#define _DO_CAS true\n#define _NO_CAS false\n#define _DO_TOUCH true\n#define _NO_TOUCH false\n\nstatic int _store_item_copy_from_buf(item *d_it, char *buf, const int len) {\n    if (d_it->it_flags & ITEM_CHUNKED) {\n        item_chunk *dch = (item_chunk *) ITEM_schunk(d_it);\n        int done = 0;\n        // Fill dch's via a flat data buffer\n        while (len > done && dch) {\n            int todo = (dch->size - dch->used < len - done)\n                ? dch->size - dch->used : len - done;\n            memcpy(dch->data + dch->used, buf + done, todo);\n            done += todo;\n            dch->used += todo;\n            assert(dch->used <= dch->size);\n\n            if (dch->size == dch->used) {\n                item_chunk *tch = do_item_alloc_chunk(dch, len - done);\n                if (tch) {\n                    dch = tch;\n                } else {\n                    return -1;\n                }\n            }\n        }\n        assert(len == done);\n    } else {\n        memcpy(ITEM_data(d_it), buf, len);\n    }\n\n    return 0;\n}\n\n// TODO (v2): out_string() needs to change to just take a *resp, but I don't\n// want to do the huge refactor in this change series. So for now we have a\n// custom out_string().\nstatic void pout_string(mc_resp *resp, const char *str) {\n    size_t len;\n    bool skip = resp->skip;\n    assert(resp != NULL);\n\n    // if response was original filled with something, but we're now writing\n    // out an error or similar, have to reset the object first.\n    resp_reset(resp);\n\n    // We blank the response \"just in case\", but if we're not intending on\n    // sending it lets not rewrite it.\n    if (skip) {\n        resp->skip = true;\n        return;\n    }\n\n    // Fill response object with static string.\n\n    len = strlen(str);\n    if ((len + 2) > WRITE_BUFFER_SIZE) {\n        /* ought to be always enough. just fail for simplicity */\n        str = \"SERVER_ERROR output line too long\";\n        len = strlen(str);\n    }\n\n    memcpy(resp->wbuf, str, len);\n    memcpy(resp->wbuf + len, \"\\r\\n\", 2);\n    resp_add_iov(resp, resp->wbuf, len + 2);\n\n    return;\n}\n\n// For meta commands error strings override the quiet flag.\nstatic void pout_errstring(mc_resp *resp, const char *str) {\n    resp->skip = false;\n    pout_string(resp, str);\n}\n\n#ifdef EXTSTORE\nstatic void _storage_get_item_cb(void *e, obj_io *eio, int ret) {\n    io_pending_proxy_t *io = (io_pending_proxy_t *)eio->data;\n    assert(io->active == true);\n    mc_resp *resp = io->tresp;\n    item *read_it = (item *)eio->buf;\n    bool miss = false;\n\n    if (ret < 1) {\n        miss = true;\n    } else {\n        uint32_t crc2;\n        uint32_t crc = (uint32_t) read_it->exptime;\n        crc2 = crc32c(0, (char *)read_it+STORE_OFFSET, eio->len-STORE_OFFSET);\n\n        if (crc != crc2) {\n            miss = true;\n            io->badcrc = true;\n        }\n    }\n\n    if (miss && !resp->skip) {\n        resp->iovcnt = 1;\n        if (io->gettype == PROXY_STORAGE_GET) {\n            if (io->ascii_multiget) {\n                resp->iov[0].iov_len = 0;\n                resp->iov[0].iov_base = \"\";\n                resp->tosend = 0;\n            } else {\n                resp->iov[0].iov_len = 5;\n                resp->iov[0].iov_base = \"END\\r\\n\";\n                resp->tosend = 5;\n            }\n        } else if (io->gettype == PROXY_STORAGE_MG) {\n            resp->iov[0].iov_len = 4;\n            resp->iov[0].iov_base = \"EN\\r\\n\";\n            resp->tosend = 5;\n        } else {\n            assert(1 == 0);\n        }\n    }\n\n    if (!miss) {\n        resp->iov[io->iovec_data].iov_base = ITEM_data(read_it);\n    }\n    io->miss = miss;\n    io->active = false;\n\n    // in proxy mode we tend to return IO's as they happen so we can keep\n    // latency down more.\n    return_io_pending((io_pending_t *)io);\n}\n\n// TODO (v2): if the item is smaller than resp->wbuf[] shouldn't we just read\n// directly into there? item only necessary for recache.\nstatic int proxy_storage_get(LIBEVENT_THREAD *t, item *it, mc_resp *resp,\n        int type) {\n#ifdef NEED_ALIGN\n    item_hdr hdr;\n    memcpy(&hdr, ITEM_data(it), sizeof(hdr));\n#else\n    item_hdr *hdr = (item_hdr *)ITEM_data(it);\n#endif\n    size_t ntotal = ITEM_ntotal(it);\n\n    io_pending_proxy_t *io = do_cache_alloc(t->io_cache);\n    // this is a re-cast structure, so assert that we never outsize it.\n    assert(sizeof(io_pending_t) >= sizeof(io_pending_proxy_t));\n    memset(io, 0, sizeof(io_pending_proxy_t));\n    io->active = true;\n    // io_pending owns the reference for this object now.\n    io->hdr_it = it;\n    io->tresp = resp; // our mc_resp is a temporary object.\n    io->io_queue_type = IO_QUEUE_EXTSTORE;\n    io->io_type = IO_PENDING_TYPE_EXTSTORE; // proxy specific sub-type.\n    io->gettype = type;\n    io->thread = t;\n    io->return_cb = proxy_return_rctx_cb;\n    io->finalize_cb = proxy_finalize_rctx_cb;\n    obj_io *eio = &io->eio;\n\n    eio->buf = malloc(ntotal);\n    if (eio->buf == NULL) {\n        do_cache_free(t->io_cache, io);\n        return -1;\n    }\n\n    io->iovec_data = resp->iovcnt;\n    resp_add_iov(resp, \"\", it->nbytes);\n\n    // We can't bail out anymore, so mc_resp owns the IO from here.\n    resp->io_pending = (io_pending_t *)io;\n\n    // reference ourselves for the callback.\n    eio->data = (void *)io;\n\n    // Now, fill in io->io based on what was in our header.\n#ifdef NEED_ALIGN\n    eio->page_version = hdr.page_version;\n    eio->page_id = hdr.page_id;\n    eio->offset = hdr.offset;\n#else\n    eio->page_version = hdr->page_version;\n    eio->page_id = hdr->page_id;\n    eio->offset = hdr->offset;\n#endif\n    eio->len = ntotal;\n    eio->mode = OBJ_IO_READ;\n    eio->cb = _storage_get_item_cb;\n\n    pthread_mutex_lock(&t->stats.mutex);\n    t->stats.get_extstore++;\n    pthread_mutex_unlock(&t->stats.mutex);\n\n    return 0;\n}\n#endif // EXTSTORE\n\n/* client flags == 0 means use no storage for client flags */\nstatic inline int make_ascii_get_suffix(char *suffix, item *it, bool return_cas, int nbytes) {\n    char *p = suffix;\n    *p = ' ';\n    p++;\n    if (FLAGS_SIZE(it) == 0) {\n        *p = '0';\n        p++;\n    } else {\n        p = itoa_u64(*((client_flags_t *) ITEM_suffix(it)), p);\n    }\n    *p = ' ';\n    p = itoa_u32(nbytes-2, p+1);\n\n    if (return_cas) {\n        *p = ' ';\n        p = itoa_u64(ITEM_get_cas(it), p+1);\n    }\n\n    *p = '\\r';\n    *(p+1) = '\\n';\n    *(p+2) = '\\0';\n    return (p - suffix) + 2;\n}\n\nstatic void process_get_cmd(LIBEVENT_THREAD *t, mcp_parser_t *pr, mc_resp *resp, bool return_cas, bool should_touch) {\n    const char *key = &pr->request[pr->tokens[pr->keytoken]];\n    int nkey = pr->klen;\n    rel_time_t exptime = 0;\n    bool overflow = false; // unused.\n\n    if (nkey > KEY_MAX_LENGTH) {\n        pout_string(resp, \"CLIENT_ERROR bad command line format\");\n        return;\n    }\n\n    item *it = limited_get(key, nkey, t, exptime, should_touch, DO_UPDATE, &overflow);\n    if (it) {\n      int nbytes = it->nbytes;;\n      nbytes = it->nbytes;\n      char *p = resp->wbuf;\n      memcpy(p, \"VALUE \", 6);\n      p += 6;\n      memcpy(p, ITEM_key(it), it->nkey);\n      p += it->nkey;\n      p += make_ascii_get_suffix(p, it, return_cas, nbytes);\n      resp_add_iov(resp, resp->wbuf, p - resp->wbuf);\n\n#ifdef EXTSTORE\n      if (it->it_flags & ITEM_HDR) {\n          if (proxy_storage_get(t, it, resp, PROXY_STORAGE_GET) != 0) {\n              pthread_mutex_lock(&t->stats.mutex);\n              t->stats.get_oom_extstore++;\n              pthread_mutex_unlock(&t->stats.mutex);\n\n              item_remove(it);\n              proxy_out_errstring(resp, PROXY_SERVER_ERROR, \"out of memory writing get response\");\n              return;\n          }\n      } else if ((it->it_flags & ITEM_CHUNKED) == 0) {\n          resp_add_iov(resp, ITEM_data(it), it->nbytes);\n      } else {\n          resp_add_chunked_iov(resp, it, it->nbytes);\n      }\n#else\n      if ((it->it_flags & ITEM_CHUNKED) == 0) {\n          resp_add_iov(resp, ITEM_data(it), it->nbytes);\n      } else {\n          resp_add_chunked_iov(resp, it, it->nbytes);\n      }\n#endif\n\n        /* item_get() has incremented it->refcount for us */\n        pthread_mutex_lock(&t->stats.mutex);\n        if (should_touch) {\n            t->stats.touch_cmds++;\n            t->stats.slab_stats[ITEM_clsid(it)].touch_hits++;\n        } else {\n            t->stats.lru_hits[it->slabs_clsid]++;\n            t->stats.get_cmds++;\n        }\n        pthread_mutex_unlock(&t->stats.mutex);\n#ifdef EXTSTORE\n        /* If ITEM_HDR, an io_wrap owns the reference. */\n        if ((it->it_flags & ITEM_HDR) == 0) {\n            resp->item = it;\n        }\n#else\n        resp->item = it;\n#endif\n    } else {\n        pthread_mutex_lock(&t->stats.mutex);\n        if (should_touch) {\n            t->stats.touch_cmds++;\n            t->stats.touch_misses++;\n        } else {\n            t->stats.get_misses++;\n            t->stats.get_cmds++;\n        }\n        pthread_mutex_unlock(&t->stats.mutex);\n    }\n\n    resp_add_iov(resp, \"END\\r\\n\", 5);\n    return;\n}\n\nstatic void process_update_cmd(LIBEVENT_THREAD *t, mcp_parser_t *pr, mc_resp *resp, int comm, bool handle_cas) {\n    const char *key = &pr->request[pr->tokens[pr->keytoken]];\n    size_t nkey = pr->klen;\n    client_flags_t flags;\n    int32_t exptime_int = 0;\n    rel_time_t exptime = 0;\n    uint64_t req_cas_id = 0;\n    item *it;\n\n    assert(resp != NULL);\n\n    if (nkey > KEY_MAX_LENGTH) {\n        pout_string(resp, \"CLIENT_ERROR bad command line format\");\n        return;\n    }\n\n    // TODO (v2): these safe_str* functions operate on C _strings_, but these\n    // tokens simply end with a space or carriage return/newline, so we either\n    // need custom functions or validate harder that these calls won't bite us\n    // later.\n    if (! (safe_strtoflags(&pr->request[pr->tokens[2]], &flags)\n           && safe_strtol(&pr->request[pr->tokens[3]], &exptime_int))) {\n        pout_string(resp, \"CLIENT_ERROR bad command line format\");\n        return;\n    }\n\n    exptime = realtime(EXPTIME_TO_POSITIVE_TIME(exptime_int));\n\n    // does cas value exist?\n    if (handle_cas) {\n        if (!safe_strtoull(&pr->request[pr->tokens[5]], &req_cas_id)) {\n            pout_string(resp, \"CLIENT_ERROR bad command line format\");\n            return;\n        }\n    }\n\n    // vlen is validated from the main parser.\n\n    if (settings.detail_enabled) {\n        stats_prefix_record_set(key, nkey);\n    }\n\n    it = item_alloc(key, nkey, flags, exptime, pr->vlen);\n\n    if (it == 0) {\n        //enum store_item_type status;\n        if (! item_size_ok(nkey, flags, pr->vlen)) {\n            pout_string(resp, \"SERVER_ERROR object too large for cache\");\n            //status = TOO_LARGE;\n            pthread_mutex_lock(&t->stats.mutex);\n            t->stats.store_too_large++;\n            pthread_mutex_unlock(&t->stats.mutex);\n        } else {\n            pout_string(resp, \"SERVER_ERROR out of memory storing object\");\n            //status = NO_MEMORY;\n            pthread_mutex_lock(&t->stats.mutex);\n            t->stats.store_no_memory++;\n            pthread_mutex_unlock(&t->stats.mutex);\n        }\n        //LOGGER_LOG(c->thread->l, LOG_MUTATIONS, LOGGER_ITEM_STORE,\n        //        NULL, status, comm, key, nkey, 0, 0, c->sfd);\n\n        /* Avoid stale data persisting in cache because we failed alloc.\n         * Unacceptable for SET. Anywhere else too? */\n        if (comm == NREAD_SET) {\n            it = item_get(key, nkey, t, DONT_UPDATE);\n            if (it) {\n                item_unlink(it);\n                STORAGE_delete(t->storage, it);\n                item_remove(it);\n            }\n        }\n\n        return;\n    }\n    ITEM_set_cas(it, req_cas_id);\n\n    pthread_mutex_lock(&t->stats.mutex);\n    t->stats.slab_stats[ITEM_clsid(it)].set_cmds++;\n    pthread_mutex_unlock(&t->stats.mutex);\n\n    // complete_nread_proxy() does the data chunk check so all we need to do\n    // is copy the data.\n    if (_store_item_copy_from_buf(it, pr->vbuf, it->nbytes) != 0) {\n        pout_string(resp, \"SERVER_ERROR out of memory storing object\");\n        item_remove(it);\n        return;\n    }\n\n    int ret = store_item(it, comm, t, NULL, NULL, (settings.use_cas) ? get_cas_id() : 0, CAS_NO_STALE);\n    switch (ret) {\n    case STORED:\n      pout_string(resp, \"STORED\");\n      break;\n    case EXISTS:\n      pout_string(resp, \"EXISTS\");\n      break;\n    case NOT_FOUND:\n      pout_string(resp, \"NOT_FOUND\");\n      break;\n    case NOT_STORED:\n      pout_string(resp, \"NOT_STORED\");\n      break;\n    default:\n      pout_string(resp, \"SERVER_ERROR Unhandled storage type.\");\n    }\n\n    // We don't need to hold a reference since the item was fully read.\n    item_remove(it);\n}\n\nstatic void process_arithmetic_cmd(LIBEVENT_THREAD *t, mcp_parser_t *pr, mc_resp *resp, const bool incr) {\n    char temp[INCR_MAX_STORAGE_LEN];\n    uint64_t delta;\n    const char *key = &pr->request[pr->tokens[pr->keytoken]];\n    size_t nkey = pr->klen;\n\n    assert(t != NULL);\n\n    if (nkey > KEY_MAX_LENGTH) {\n        pout_string(resp, \"CLIENT_ERROR bad command line format\");\n        return;\n    }\n\n    if (!safe_strtoull(&pr->request[pr->tokens[2]], &delta)) {\n        pout_string(resp, \"CLIENT_ERROR invalid numeric delta argument\");\n        return;\n    }\n\n    switch(add_delta(t, key, nkey, incr, delta, temp, NULL)) {\n    case OK:\n        pout_string(resp, temp);\n        break;\n    case NON_NUMERIC:\n        pout_string(resp, \"CLIENT_ERROR cannot increment or decrement non-numeric value\");\n        break;\n    case EOM:\n        pout_string(resp, \"SERVER_ERROR out of memory\");\n        break;\n    case DELTA_ITEM_NOT_FOUND:\n        pthread_mutex_lock(&t->stats.mutex);\n        if (incr) {\n            t->stats.incr_misses++;\n        } else {\n            t->stats.decr_misses++;\n        }\n        pthread_mutex_unlock(&t->stats.mutex);\n\n        pout_string(resp, \"NOT_FOUND\");\n        break;\n    case DELTA_ITEM_CAS_MISMATCH:\n        break; /* Should never get here */\n    }\n}\n\nstatic void process_delete_cmd(LIBEVENT_THREAD *t, mcp_parser_t *pr, mc_resp *resp) {\n    const char *key = &pr->request[pr->tokens[pr->keytoken]];\n    size_t nkey = pr->klen;\n    item *it;\n    uint32_t hv;\n\n    assert(t != NULL);\n\n    // NOTE: removed a compatibility bodge from a decade ago.\n    // delete used to take a \"delay\" argument, which was removed, but some\n    // ancient php clients always sent a 0 argument, which would then fail.\n    // It's been long enough that I don't want to carry this forward into the\n    // new parser.\n\n    if (nkey > KEY_MAX_LENGTH) {\n        pout_string(resp, \"CLIENT_ERROR bad command line format\");\n        return;\n    }\n\n    it = item_get_locked(key, nkey, t, DONT_UPDATE, &hv);\n    if (it) {\n        //MEMCACHED_COMMAND_DELETE(c->sfd, ITEM_key(it), it->nkey);\n\n        pthread_mutex_lock(&t->stats.mutex);\n        t->stats.slab_stats[ITEM_clsid(it)].delete_hits++;\n        pthread_mutex_unlock(&t->stats.mutex);\n        LOGGER_LOG(NULL, LOG_DELETIONS, LOGGER_DELETIONS, it, LOG_TYPE_DELETE);\n        do_item_unlink(it, hv);\n        STORAGE_delete(t->storage, it);\n        do_item_remove(it);      /* release our reference */\n        pout_string(resp, \"DELETED\");\n    } else {\n        pthread_mutex_lock(&t->stats.mutex);\n        t->stats.delete_misses++;\n        pthread_mutex_unlock(&t->stats.mutex);\n\n        pout_string(resp, \"NOT_FOUND\");\n    }\n    item_unlock(hv);\n}\n\nstatic void process_touch_cmd(LIBEVENT_THREAD *t, mcp_parser_t *pr, mc_resp *resp) {\n    const char *key = &pr->request[pr->tokens[pr->keytoken]];\n    size_t nkey = pr->klen;\n    int32_t exptime_int = 0;\n    rel_time_t exptime = 0;\n    item *it;\n\n    assert(t != NULL);\n\n    if (nkey > KEY_MAX_LENGTH) {\n        pout_string(resp, \"CLIENT_ERROR bad command line format\");\n        return;\n    }\n\n    if (!safe_strtol(&pr->request[pr->tokens[2]], &exptime_int)) {\n        pout_string(resp, \"CLIENT_ERROR invalid exptime argument\");\n        return;\n    }\n\n    exptime = realtime(EXPTIME_TO_POSITIVE_TIME(exptime_int));\n    it = item_touch(key, nkey, exptime, t);\n    if (it) {\n        pthread_mutex_lock(&t->stats.mutex);\n        t->stats.touch_cmds++;\n        t->stats.slab_stats[ITEM_clsid(it)].touch_hits++;\n        pthread_mutex_unlock(&t->stats.mutex);\n\n        pout_string(resp, \"TOUCHED\");\n        item_remove(it);\n    } else {\n        pthread_mutex_lock(&t->stats.mutex);\n        t->stats.touch_cmds++;\n        t->stats.touch_misses++;\n        pthread_mutex_unlock(&t->stats.mutex);\n\n        pout_string(resp, \"NOT_FOUND\");\n    }\n}\n\n/*** meta command handlers ***/\n\n// FIXME: macro or public interface, this is copypasted.\nstatic int _process_token_len(mcp_parser_t *pr, size_t token) {\n  const char *s = pr->request + pr->tokens[token];\n  const char *e = pr->request + pr->tokens[token+1];\n  // start of next token is after any space delimiters, so back those out.\n  while (*(e-1) == ' ') {\n      e--;\n  }\n  return e - s;\n}\n\n#define META_SPACE(p) { \\\n    *p = ' '; \\\n    p++; \\\n}\n\n#define META_CHAR(p, c) { \\\n    *p = ' '; \\\n    *(p+1) = c; \\\n    p += 2; \\\n}\n\n// FIXME: binary key support.\n#define META_KEY(p, key, nkey, bin) { \\\n    META_CHAR(p, 'k'); \\\n    memcpy(p, key, nkey); \\\n    p += nkey; \\\n}\n\n#define MFLAG_MAX_OPT_LENGTH 20\n#define MFLAG_MAX_OPAQUE_LENGTH 32\n\nstruct _meta_flags {\n    unsigned int has_error :1; // flipped if we found an error during parsing.\n    unsigned int no_update :1;\n    unsigned int locked :1;\n    unsigned int vivify :1;\n    unsigned int la :1;\n    unsigned int hit :1;\n    unsigned int value :1;\n    unsigned int set_stale :1;\n    unsigned int no_reply :1;\n    unsigned int has_cas :1;\n    unsigned int has_cas_in :1;\n    unsigned int new_ttl :1;\n    unsigned int key_binary:1;\n    unsigned int remove_val:1;\n    char mode; // single character mode switch, common to ms/ma\n    rel_time_t exptime;\n    rel_time_t autoviv_exptime;\n    rel_time_t recache_time;\n    client_flags_t client_flags;\n    uint64_t req_cas_id;\n    uint64_t cas_id_in; // client supplied next-CAS\n    uint64_t delta; // ma\n    uint64_t initial; // ma\n};\n\nstatic int _meta_flag_preparse(mcp_parser_t *pr, const size_t start,\n        struct _meta_flags *of, char **errstr) {\n    unsigned int i;\n    //size_t ret;\n    int32_t tmp_int;\n    uint8_t seen[127] = {0};\n    // Start just past the key token. Look at first character of each token.\n    for (i = start; i < pr->ntokens; i++) {\n        uint8_t o = (uint8_t)pr->request[pr->tokens[i]];\n        // zero out repeat flags so we don't over-parse for return data.\n        if (o >= 127 || seen[o] != 0) {\n            *errstr = \"CLIENT_ERROR duplicate flag\";\n            return -1;\n        }\n        seen[o] = 1;\n        switch (o) {\n            // base64 decode the key in-place, as the binary should always be\n            // shorter and the conversion code buffers bytes.\n            // TODO: we need temporary space for the binary key decode since\n            // request should be const.\n            /*case 'b':\n                ret = base64_decode((unsigned char *)tokens[KEY_TOKEN].value, tokens[KEY_TOKEN].length,\n                            (unsigned char *)tokens[KEY_TOKEN].value, tokens[KEY_TOKEN].length);\n                if (ret == 0) {\n                    // Failed to decode\n                    *errstr = \"CLIENT_ERROR error decoding key\";\n                    of->has_error = 1;\n                }\n                tokens[KEY_TOKEN].length = ret;\n                of->key_binary = 1;\n                break;*/\n            /* Negative exptimes can underflow and end up immortal. realtime() will\n               immediately expire values that are greater than REALTIME_MAXDELTA, but less\n               than process_started, so lets aim for that. */\n            case 'N':\n                of->locked = 1;\n                of->vivify = 1;\n                if (!safe_strtol(&pr->request[pr->tokens[i]+1], &tmp_int)) {\n                    *errstr = \"CLIENT_ERROR bad token in command line format\";\n                    of->has_error = 1;\n                } else {\n                    of->autoviv_exptime = realtime(EXPTIME_TO_POSITIVE_TIME(tmp_int));\n                }\n                break;\n            case 'T':\n                of->locked = 1;\n                if (!safe_strtol(&pr->request[pr->tokens[i]+1], &tmp_int)) {\n                    *errstr = \"CLIENT_ERROR bad token in command line format\";\n                    of->has_error = 1;\n                } else {\n                    of->exptime = realtime(EXPTIME_TO_POSITIVE_TIME(tmp_int));\n                    of->new_ttl = true;\n                }\n                break;\n            case 'R':\n                of->locked = 1;\n                if (!safe_strtol(&pr->request[pr->tokens[i]+1], &tmp_int)) {\n                    *errstr = \"CLIENT_ERROR bad token in command line format\";\n                    of->has_error = 1;\n                } else {\n                    of->recache_time = realtime(EXPTIME_TO_POSITIVE_TIME(tmp_int));\n                }\n                break;\n            case 'l':\n                of->la = 1;\n                of->locked = 1; // need locked to delay LRU bump\n                break;\n            case 'O':\n            case 'P':\n            case 'L':\n                break;\n            case 'k': // known but no special handling\n            case 's':\n            case 't':\n            case 'c':\n            case 'f':\n                break;\n            case 'v':\n                of->value = 1;\n                break;\n            case 'h':\n                of->locked = 1; // need locked to delay LRU bump\n                break;\n            case 'u':\n                of->no_update = 1;\n                break;\n            case 'q':\n                of->no_reply = 1;\n                break;\n            case 'x':\n                of->remove_val = 1;\n                break;\n            // mset-related.\n            case 'F':\n                if (!safe_strtoflags(&pr->request[pr->tokens[i]+1], &of->client_flags)) {\n                    of->has_error = true;\n                }\n                break;\n            case 'C': // mset, mdelete, marithmetic\n                if (!safe_strtoull(&pr->request[pr->tokens[i]+1], &of->req_cas_id)) {\n                    *errstr = \"CLIENT_ERROR bad token in command line format\";\n                    of->has_error = true;\n                } else {\n                    of->has_cas = true;\n                }\n                break;\n            case 'E': // ms, md, ma\n                if (!safe_strtoull(&pr->request[pr->tokens[i]+1], &of->cas_id_in)) {\n                    *errstr = \"CLIENT_ERROR bad token in command line format\";\n                    of->has_error = true;\n                } else {\n                    of->has_cas_in = true;\n                }\n                break;\n            case 'M': // mset and marithmetic mode switch\n                // FIXME: this used to error if the token isn't a single byte.\n                // It probably should still?\n                of->mode = pr->request[pr->tokens[i]+1];\n                break;\n            case 'J': // marithmetic initial value\n                if (!safe_strtoull(&pr->request[pr->tokens[i]+1], &of->initial)) {\n                    *errstr = \"CLIENT_ERROR invalid numeric initial value\";\n                    of->has_error = 1;\n                }\n                break;\n            case 'D': // marithmetic delta value\n                if (!safe_strtoull(&pr->request[pr->tokens[i]+1], &of->delta)) {\n                    *errstr = \"CLIENT_ERROR invalid numeric delta value\";\n                    of->has_error = 1;\n                }\n                break;\n            case 'I':\n                of->set_stale = 1;\n                break;\n            default: // unknown flag, bail.\n                *errstr = \"CLIENT_ERROR invalid flag\";\n                return -1;\n        }\n    }\n\n    return of->has_error ? -1 : 0;\n}\n\nstatic void process_mget_cmd(LIBEVENT_THREAD *t, mcp_parser_t *pr, mc_resp *resp) {\n    const char *key = &pr->request[pr->tokens[pr->keytoken]];\n    size_t nkey = pr->klen;\n    item *it;\n    unsigned int i = 0;\n    struct _meta_flags of = {0}; // option bitflags.\n    uint32_t hv; // cached hash value for unlocking an item.\n    bool failed = false;\n    bool item_created = false;\n    bool won_token = false;\n    bool ttl_set = false;\n    char *errstr = \"CLIENT_ERROR bad command line format\";\n    assert(t != NULL);\n    char *p = resp->wbuf;\n    int tlen = 0;\n\n    // FIXME: still needed?\n    //WANT_TOKENS_MIN(ntokens, 3);\n\n    if (nkey > KEY_MAX_LENGTH) {\n        pout_string(resp, \"CLIENT_ERROR bad command line format\");\n        return;\n    }\n\n    if (pr->ntokens > MFLAG_MAX_OPT_LENGTH) {\n        // TODO: ensure the command tokenizer gives us at least this many\n        pout_errstring(resp, \"CLIENT_ERROR options flags are too long\");\n        return;\n    }\n\n    // scrubs duplicated options and sets flags for how to load the item.\n    // we pass in the first token that should be a flag.\n    if (_meta_flag_preparse(pr, 2, &of, &errstr) != 0) {\n        pout_errstring(resp, errstr);\n        return;\n    }\n\n    bool overflow = false;\n    if (!of.locked) {\n        it = limited_get(key, nkey, t, 0, false, !of.no_update, &overflow);\n    } else {\n        // If we had to lock the item, we're doing our own bump later.\n        it = limited_get_locked(key, nkey, t, DONT_UPDATE, &hv, &overflow);\n    }\n\n    // Since we're a new protocol, we can actually inform users that refcount\n    // overflow is happening by straight up throwing an error.\n    // We definitely don't want to re-autovivify by accident.\n    if (overflow) {\n        assert(it == NULL);\n        pout_errstring(resp, \"SERVER_ERROR refcount overflow during fetch\");\n        return;\n    }\n\n    if (it == NULL && of.vivify) {\n        // Fill in the exptime during parsing later.\n        it = item_alloc(key, nkey, 0, realtime(0), 2);\n        // We don't actually need any of do_store_item's logic:\n        // - already fetched and missed an existing item.\n        // - lock is still held.\n        // - not append/prepend/replace\n        // - not testing CAS\n        if (it != NULL) {\n            // I look forward to the day I get rid of this :)\n            memcpy(ITEM_data(it), \"\\r\\n\", 2);\n            // NOTE: This initializes the CAS value.\n            do_item_link(it, hv, of.has_cas_in ? of.cas_id_in : get_cas_id());\n            item_created = true;\n        }\n    }\n\n    // don't have to check result of add_iov() since the iov size defaults are\n    // enough.\n    if (it) {\n        if (of.value) {\n            memcpy(p, \"VA \", 3);\n            p = itoa_u32(it->nbytes-2, p+3);\n        } else {\n            memcpy(p, \"HD\", 2);\n            p += 2;\n        }\n\n        for (i = pr->keytoken+1; i < pr->ntokens; i++) {\n            switch (pr->request[pr->tokens[i]]) {\n                case 'T':\n                    ttl_set = true;\n                    it->exptime = of.exptime;\n                    break;\n                case 'N':\n                    if (item_created) {\n                        it->exptime = of.autoviv_exptime;\n                        won_token = true;\n                    }\n                    break;\n                case 'R':\n                    // If we haven't autovivified and supplied token is less\n                    // than current TTL, mark a win.\n                    if ((it->it_flags & ITEM_TOKEN_SENT) == 0\n                            && !item_created\n                            && it->exptime != 0\n                            && it->exptime < of.recache_time) {\n                        won_token = true;\n                    }\n                    break;\n                case 's':\n                    META_CHAR(p, 's');\n                    p = itoa_u32(it->nbytes-2, p);\n                    break;\n                case 't':\n                    // TTL remaining as of this request.\n                    // needs to be relative because server clocks may not be in sync.\n                    META_CHAR(p, 't');\n                    if (it->exptime == 0) {\n                        *p = '-';\n                        *(p+1) = '1';\n                        p += 2;\n                    } else {\n                        p = itoa_u32(it->exptime - current_time, p);\n                    }\n                    break;\n                case 'c':\n                    META_CHAR(p, 'c');\n                    p = itoa_u64(ITEM_get_cas(it), p);\n                    break;\n                case 'f':\n                    META_CHAR(p, 'f');\n                    if (FLAGS_SIZE(it) == 0) {\n                        *p = '0';\n                        p++;\n                    } else {\n                        p = itoa_u64(*((client_flags_t *) ITEM_suffix(it)), p);\n                    }\n                    break;\n                case 'l':\n                    META_CHAR(p, 'l');\n                    p = itoa_u32(current_time - it->time, p);\n                    break;\n                case 'h':\n                    META_CHAR(p, 'h');\n                    if (it->it_flags & ITEM_FETCHED) {\n                        *p = '1';\n                    } else {\n                        *p = '0';\n                    }\n                    p++;\n                    break;\n                case 'O':\n                    tlen = _process_token_len(pr, i);\n                    if (tlen > MFLAG_MAX_OPAQUE_LENGTH) {\n                        errstr = \"CLIENT_ERROR opaque token too long\";\n                        goto error;\n                    }\n                    META_SPACE(p);\n                    memcpy(p, &pr->request[pr->tokens[i]], tlen);\n                    p += tlen;\n                    break;\n                case 'k':\n                    META_KEY(p, ITEM_key(it), it->nkey, (it->it_flags & ITEM_KEY_BINARY));\n                    break;\n            }\n        }\n\n        // Has this item already sent a token?\n        // Important to do this here so we don't send W with Z.\n        // Isn't critical, but easier for client authors to understand.\n        if (it->it_flags & ITEM_TOKEN_SENT) {\n            META_CHAR(p, 'Z');\n        }\n        if (it->it_flags & ITEM_STALE) {\n            META_CHAR(p, 'X');\n            // FIXME: think hard about this. is this a default, or a flag?\n            if ((it->it_flags & ITEM_TOKEN_SENT) == 0) {\n                // If we're stale but no token already sent, now send one.\n                won_token = true;\n            }\n        }\n\n        if (won_token) {\n            // Mark a win into the flag buffer.\n            META_CHAR(p, 'W');\n            it->it_flags |= ITEM_TOKEN_SENT;\n        }\n\n        *p = '\\r';\n        *(p+1) = '\\n';\n        *(p+2) = '\\0';\n        p += 2;\n        // finally, chain in the buffer.\n        resp_add_iov(resp, resp->wbuf, p - resp->wbuf);\n\n        if (of.value) {\n#ifdef EXTSTORE\n            if (it->it_flags & ITEM_HDR) {\n                if (proxy_storage_get(t, it, resp, PROXY_STORAGE_MG) != 0) {\n                    pthread_mutex_lock(&t->stats.mutex);\n                    t->stats.get_oom_extstore++;\n                    pthread_mutex_unlock(&t->stats.mutex);\n\n                    failed = true;\n                }\n            } else if ((it->it_flags & ITEM_CHUNKED) == 0) {\n                resp_add_iov(resp, ITEM_data(it), it->nbytes);\n            } else {\n                resp_add_chunked_iov(resp, it, it->nbytes);\n            }\n#else\n            if ((it->it_flags & ITEM_CHUNKED) == 0) {\n                resp_add_iov(resp, ITEM_data(it), it->nbytes);\n            } else {\n                resp_add_chunked_iov(resp, it, it->nbytes);\n            }\n#endif\n        }\n\n        // need to hold the ref at least because of the key above.\n#ifdef EXTSTORE\n        if (!failed) {\n            if ((it->it_flags & ITEM_HDR) != 0 && of.value) {\n                // Only have extstore clean if header and returning value.\n                resp->item = NULL;\n            } else {\n                resp->item = it;\n            }\n        } else {\n            // Failed to set up extstore fetch.\n            if (of.locked) {\n                do_item_remove(it);\n            } else {\n                item_remove(it);\n            }\n        }\n#else\n        resp->item = it;\n#endif\n    } else {\n        failed = true;\n    }\n\n    if (of.locked) {\n        // Delayed bump so we could get fetched/last access time pre-update.\n        if (!of.no_update && it != NULL) {\n            do_item_bump(t, it, hv);\n        }\n        item_unlock(hv);\n    }\n\n    // we count this command as a normal one if we've gotten this far.\n    // TODO: for autovivify case, miss never happens. Is this okay?\n    if (!failed) {\n        pthread_mutex_lock(&t->stats.mutex);\n        if (ttl_set) {\n            t->stats.touch_cmds++;\n            t->stats.slab_stats[ITEM_clsid(it)].touch_hits++;\n        } else {\n            t->stats.lru_hits[it->slabs_clsid]++;\n            t->stats.get_cmds++;\n        }\n        pthread_mutex_unlock(&t->stats.mutex);\n    } else {\n        pthread_mutex_lock(&t->stats.mutex);\n        if (ttl_set) {\n            t->stats.touch_cmds++;\n            t->stats.touch_misses++;\n        } else {\n            t->stats.get_misses++;\n            t->stats.get_cmds++;\n        }\n        pthread_mutex_unlock(&t->stats.mutex);\n\n        // This gets elided in noreply mode.\n        if (of.no_reply)\n            resp->skip = true;\n        memcpy(p, \"EN\", 2);\n        p += 2;\n        for (i = pr->keytoken+1; i < pr->ntokens; i++) {\n            switch (pr->request[pr->tokens[i]]) {\n                // TODO: macro perhaps?\n                case 'O':\n                    tlen = _process_token_len(pr, i);\n                    if (tlen > MFLAG_MAX_OPAQUE_LENGTH) {\n                        errstr = \"CLIENT_ERROR opaque token too long\";\n                        goto error;\n                    }\n                    META_SPACE(p);\n                    memcpy(p, &pr->request[pr->tokens[i]], tlen);\n                    p += tlen;\n                    break;\n                case 'k':\n                    META_KEY(p, key, nkey, of.key_binary);\n                    break;\n            }\n        }\n        resp->wbytes = p - resp->wbuf;\n        memcpy(resp->wbuf + resp->wbytes, \"\\r\\n\", 2);\n        resp->wbytes += 2;\n        resp_add_iov(resp, resp->wbuf, resp->wbytes);\n    }\n    return;\nerror:\n    if (it) {\n        do_item_remove(it);\n        if (of.locked) {\n            item_unlock(hv);\n        }\n    }\n    pout_errstring(resp, errstr);\n}\n\nstatic void process_mset_cmd(LIBEVENT_THREAD *t, mcp_parser_t *pr, mc_resp *resp) {\n    const char *key = &pr->request[pr->tokens[pr->keytoken]];\n    size_t nkey = pr->klen;\n\n    item *it;\n    int i;\n    short comm = NREAD_SET;\n    struct _meta_flags of = {0}; // option bitflags.\n    char *errstr = \"CLIENT_ERROR bad command line format\";\n    uint32_t hv; // cached hash value.\n    int vlen = pr->vlen; // value from data line.\n    assert(t != NULL);\n    char *p = resp->wbuf;\n    int tlen = 0;\n\n    //WANT_TOKENS_MIN(ntokens, 3);\n\n    if (nkey > KEY_MAX_LENGTH || pr->ntokens < 3) {\n        pout_string(resp, \"CLIENT_ERROR bad command line format\");\n        return;\n    }\n\n    if (pr->ntokens > MFLAG_MAX_OPT_LENGTH) {\n        // TODO: ensure the command tokenizer gives us at least this many\n        pout_errstring(resp, \"CLIENT_ERROR options flags are too long\");\n        return;\n    }\n\n    // We need to at least try to get the size to properly slurp bad bytes\n    // after an error.\n    // we pass in the first token that should be a flag.\n    if (_meta_flag_preparse(pr, 3, &of, &errstr) != 0) {\n        goto error;\n    }\n\n    rel_time_t exptime = of.exptime;\n    // \"mode switch\" to alternative commands\n    switch (of.mode) {\n        case 0:\n            break; // no mode supplied.\n        case 'E': // Add...\n            comm = NREAD_ADD;\n            break;\n        case 'A': // Append.\n            if (of.vivify) {\n                comm = NREAD_APPENDVIV;\n                exptime = of.autoviv_exptime;\n            } else {\n                comm = NREAD_APPEND;\n            }\n            break;\n        case 'P': // Prepend.\n            if (of.vivify) {\n                comm = NREAD_PREPENDVIV;\n                exptime = of.autoviv_exptime;\n            } else {\n                comm = NREAD_PREPEND;\n            }\n            break;\n        case 'R': // Replace.\n            comm = NREAD_REPLACE;\n            break;\n        case 'S': // Set. Default.\n            comm = NREAD_SET;\n            break;\n        default:\n            errstr = \"CLIENT_ERROR invalid mode for ms M token\";\n            goto error;\n    }\n\n    // The item storage function doesn't exactly map to mset.\n    // If a CAS value is supplied, upgrade default SET mode to CAS mode.\n    // Also allows REPLACE to work, as REPLACE + CAS works the same as CAS.\n    // add-with-cas works the same as add; but could only LRU bump if match..\n    // APPEND/PREPEND allow a simplified CAS check.\n    if (of.has_cas && (comm == NREAD_SET || comm == NREAD_REPLACE)) {\n        comm = NREAD_CAS;\n    }\n\n    it = item_alloc(key, nkey, of.client_flags, exptime, vlen);\n\n    if (it == 0) {\n        if (! item_size_ok(nkey, of.client_flags, vlen)) {\n            errstr = \"SERVER_ERROR object too large for cache\";\n            pthread_mutex_lock(&t->stats.mutex);\n            t->stats.store_too_large++;\n            pthread_mutex_unlock(&t->stats.mutex);\n        } else {\n            errstr = \"SERVER_ERROR out of memory storing object\";\n            pthread_mutex_lock(&t->stats.mutex);\n            t->stats.store_no_memory++;\n            pthread_mutex_unlock(&t->stats.mutex);\n        }\n\n        /* Avoid stale data persisting in cache because we failed alloc. */\n        // NOTE: only if SET mode?\n        it = item_get_locked(key, nkey, t, DONT_UPDATE, &hv);\n        if (it) {\n            do_item_unlink(it, hv);\n            STORAGE_delete(t->storage, it);\n            do_item_remove(it);\n        }\n        item_unlock(hv);\n\n        goto error;\n    }\n    ITEM_set_cas(it, of.req_cas_id);\n\n    // data should already be read into the request.\n\n    // Prevent printing back the key in meta commands as garbage.\n    if (of.key_binary) {\n        it->it_flags |= ITEM_KEY_BINARY;\n    }\n\n    bool set_stale = CAS_NO_STALE;\n    if (of.set_stale && comm == NREAD_CAS) {\n        set_stale = CAS_ALLOW_STALE;\n    }\n    resp->wbytes = p - resp->wbuf;\n\n    pthread_mutex_lock(&t->stats.mutex);\n    t->stats.slab_stats[ITEM_clsid(it)].set_cmds++;\n    pthread_mutex_unlock(&t->stats.mutex);\n\n    // complete_nread_proxy() does the data chunk check so all we need to do\n    // is copy the data.\n    if (_store_item_copy_from_buf(it, pr->vbuf, it->nbytes) != 0) {\n        pout_string(resp, \"SERVER_ERROR out of memory storing object\");\n        item_remove(it);\n        return;\n    }\n\n    uint64_t cas = 0;\n    int nbytes = 0;\n    int ret = store_item(it, comm, t, &nbytes, &cas, of.has_cas_in ? of.cas_id_in : get_cas_id(), set_stale);\n    switch (ret) {\n        case STORED:\n          memcpy(p, \"HD\", 2);\n          // Only place noreply is used for meta cmds is a nominal response.\n          if (of.no_reply) {\n              resp->skip = true;\n          }\n          break;\n        case EXISTS:\n          memcpy(p, \"EX\", 2);\n          break;\n        case NOT_FOUND:\n          memcpy(p, \"NF\", 2);\n          break;\n        case NOT_STORED:\n          memcpy(p, \"NS\", 2);\n          break;\n        default:\n          pout_errstring(resp, \"SERVER_ERROR Unhandled storage type.\");\n          return;\n\n    }\n    p += 2;\n\n    for (i = pr->keytoken+1; i < pr->ntokens; i++) {\n        switch (pr->request[pr->tokens[i]]) {\n            case 'O':\n                tlen = _process_token_len(pr, i);\n                if (tlen > MFLAG_MAX_OPAQUE_LENGTH) {\n                    errstr = \"CLIENT_ERROR opaque token too long\";\n                    goto error;\n                }\n                META_SPACE(p);\n                memcpy(p, &pr->request[pr->tokens[i]], tlen);\n                p += tlen;\n                break;\n            case 'k':\n                META_KEY(p, ITEM_key(it), it->nkey, (it->it_flags & ITEM_KEY_BINARY));\n                break;\n            case 'c':\n                META_CHAR(p, 'c');\n                p = itoa_u64(cas, p);\n                break;\n            case 's':\n                // Get final item size, ie from append/prepend\n                META_CHAR(p, 's');\n                // If the size changed during append/prepend\n                if (nbytes != 0) {\n                    p = itoa_u32(nbytes-2, p);\n                } else {\n                    p = itoa_u32(it->nbytes-2, p);\n                }\n                break;\n        }\n    }\n\n    // We don't need to free pr->vbuf as that is owned by *rq\n    // either way, there's no c->item or resp->item reference right now.\n\n    memcpy(p, \"\\r\\n\", 2);\n    p += 2;\n    // we're offset into wbuf, but good convention to track wbytes.\n    resp->wbytes = p - resp->wbuf;\n    resp_add_iov(resp, resp->wbuf, resp->wbytes);\n\n    item_remove(it);\n\n    return;\nerror:\n    // Note: no errors possible after the item was successfully allocated.\n    // So we're just looking at dumping error codes and returning.\n    pout_errstring(resp, errstr);\n}\n\nstatic void process_mdelete_cmd(LIBEVENT_THREAD *t, mcp_parser_t *pr, mc_resp *resp) {\n    const char *key = &pr->request[pr->tokens[pr->keytoken]];\n    size_t nkey = pr->klen;\n    item *it = NULL;\n    int i;\n    uint32_t hv;\n    struct _meta_flags of = {0}; // option bitflags.\n    char *errstr = \"CLIENT_ERROR bad command line format\";\n    assert(t != NULL);\n    // reserve bytes for status code\n    char *p = resp->wbuf + 2;\n    int tlen = 0;\n\n    //WANT_TOKENS_MIN(ntokens, 3);\n\n    if (nkey > KEY_MAX_LENGTH) {\n        pout_string(resp, \"CLIENT_ERROR bad command line format\");\n        return;\n    }\n\n    if (pr->ntokens > MFLAG_MAX_OPT_LENGTH) {\n        // TODO: ensure the command tokenizer gives us at least this many\n        pout_errstring(resp, \"CLIENT_ERROR options flags are too long\");\n        return;\n    }\n\n    // scrubs duplicated options and sets flags for how to load the item.\n    // we pass in the first token that should be a flag.\n    // FIXME: not using the preparse errstr?\n    if (_meta_flag_preparse(pr, 2, &of, &errstr) != 0) {\n        pout_errstring(resp, \"CLIENT_ERROR invalid or duplicate flag\");\n        return;\n    }\n\n    for (i = pr->keytoken+1; i < pr->ntokens; i++) {\n        switch (pr->request[pr->tokens[i]]) {\n            // TODO: macro perhaps?\n            case 'O':\n                tlen = _process_token_len(pr, i);\n                if (tlen > MFLAG_MAX_OPAQUE_LENGTH) {\n                    errstr = \"CLIENT_ERROR opaque token too long\";\n                    goto error;\n                }\n                META_SPACE(p);\n                memcpy(p, &pr->request[pr->tokens[i]], tlen);\n                p += tlen;\n                break;\n            case 'k':\n                META_KEY(p, key, nkey, of.key_binary);\n                break;\n        }\n    }\n\n    it = item_get_locked(key, nkey, t, DONT_UPDATE, &hv);\n    if (it) {\n        // allow only deleting/marking if a CAS value matches.\n        if (of.has_cas && ITEM_get_cas(it) != of.req_cas_id) {\n            pthread_mutex_lock(&t->stats.mutex);\n            t->stats.delete_misses++;\n            pthread_mutex_unlock(&t->stats.mutex);\n\n            memcpy(resp->wbuf, \"EX\", 2);\n            goto cleanup;\n        }\n\n        // If requested, create a new empty tombstone item.\n        if (of.remove_val) {\n            item *new_it = item_alloc(key, nkey, of.client_flags, of.exptime, 2);\n            if (new_it != NULL) {\n                memcpy(ITEM_data(new_it), \"\\r\\n\", 2);\n                if (do_store_item(new_it, NREAD_SET, t, hv, NULL, NULL,\n                            of.has_cas_in ? of.cas_id_in : ITEM_get_cas(it), CAS_NO_STALE)) {\n                    do_item_remove(it);\n                    it = new_it;\n                } else {\n                    do_item_remove(new_it);\n                    memcpy(resp->wbuf, \"NS\", 2);\n                    goto cleanup;\n                }\n            } else {\n                errstr = \"SERVER_ERROR out of memory\";\n                goto error;\n            }\n        }\n\n        // If we're to set this item as stale, we don't actually want to\n        // delete it. We mark the stale bit, bump CAS, and update exptime if\n        // we were supplied a new TTL.\n        if (of.set_stale) {\n            if (of.new_ttl) {\n                it->exptime = of.exptime;\n            }\n            it->it_flags |= ITEM_STALE;\n            // Also need to remove TOKEN_SENT, so next client can win.\n            it->it_flags &= ~ITEM_TOKEN_SENT;\n\n            ITEM_set_cas(it, of.has_cas_in ? of.cas_id_in : get_cas_id());\n            if (of.no_reply)\n                resp->skip = true;\n\n            memcpy(resp->wbuf, \"HD\", 2);\n        } else {\n            pthread_mutex_lock(&t->stats.mutex);\n            t->stats.slab_stats[ITEM_clsid(it)].delete_hits++;\n            pthread_mutex_unlock(&t->stats.mutex);\n            LOGGER_LOG(NULL, LOG_DELETIONS, LOGGER_DELETIONS, it, LOG_TYPE_META_DELETE);\n\n            if (!of.remove_val) {\n                do_item_unlink(it, hv);\n                STORAGE_delete(t->storage, it);\n            }\n            if (of.no_reply)\n                resp->skip = true;\n            memcpy(resp->wbuf, \"HD\", 2);\n        }\n        goto cleanup;\n    } else {\n        pthread_mutex_lock(&t->stats.mutex);\n        t->stats.delete_misses++;\n        pthread_mutex_unlock(&t->stats.mutex);\n\n        memcpy(resp->wbuf, \"NF\", 2);\n        goto cleanup;\n    }\ncleanup:\n    if (it) {\n        do_item_remove(it);\n    }\n    // Item is always returned locked, even if missing.\n    item_unlock(hv);\n    resp->wbytes = p - resp->wbuf;\n    memcpy(resp->wbuf + resp->wbytes, \"\\r\\n\", 2);\n    resp->wbytes += 2;\n    resp_add_iov(resp, resp->wbuf, resp->wbytes);\n    //conn_set_state(c, conn_new_cmd);\n    return;\nerror:\n    // cleanup if an error happens after we fetched an item.\n    if (it) {\n        do_item_remove(it);\n        item_unlock(hv);\n    }\n    pout_errstring(resp, errstr);\n}\n\nstatic void process_marithmetic_cmd(LIBEVENT_THREAD *t, mcp_parser_t *pr, mc_resp *resp) {\n    const char *key = &pr->request[pr->tokens[pr->keytoken]];\n    size_t nkey = pr->klen;\n    int i;\n    struct _meta_flags of = {0}; // option bitflags.\n    char *errstr = \"CLIENT_ERROR bad command line format\";\n    assert(t != NULL);\n    // no reservation (like del/set) since we post-process the status line.\n    char *p = resp->wbuf;\n    int tlen = 0;\n\n    // If no argument supplied, incr or decr by one.\n    of.delta = 1;\n    of.initial = 0; // redundant, for clarity.\n    bool incr = true; // default mode is to increment.\n    bool locked = false;\n    uint32_t hv = 0;\n    item *it = NULL; // item returned by do_add_delta.\n\n    //WANT_TOKENS_MIN(ntokens, 3);\n\n    if (nkey > KEY_MAX_LENGTH) {\n        pout_string(resp, \"CLIENT_ERROR bad command line format\");\n        return;\n    }\n\n    if (pr->ntokens > MFLAG_MAX_OPT_LENGTH) {\n        // TODO: ensure the command tokenizer gives us at least this many\n        pout_errstring(resp, \"CLIENT_ERROR options flags are too long\");\n        return;\n    }\n\n    // scrubs duplicated options and sets flags for how to load the item.\n    // we pass in the first token that should be a flag.\n    if (_meta_flag_preparse(pr, 2, &of, &errstr) != 0) {\n        pout_errstring(resp, \"CLIENT_ERROR invalid or duplicate flag\");\n        return;\n    }\n    //c->noreply = of.no_reply;\n\n    // \"mode switch\" to alternative commands\n    switch (of.mode) {\n        case 0: // no switch supplied.\n            break;\n        case 'I': // Incr (default)\n        case '+':\n            incr = true;\n            break;\n        case 'D': // Decr.\n        case '-':\n            incr = false;\n            break;\n        default:\n            errstr = \"CLIENT_ERROR invalid mode for ma M token\";\n            goto error;\n            break;\n    }\n\n    // take hash value and manually lock item... hold lock during store phase\n    // on miss and avoid recalculating the hash multiple times.\n    hv = hash(key, nkey);\n    item_lock(hv);\n    locked = true;\n    char tmpbuf[INCR_MAX_STORAGE_LEN];\n\n    // return a referenced item if it exists, so we can modify it here, rather\n    // than adding even more parameters to do_add_delta.\n    bool item_created = false;\n    uint64_t cas = 0;\n    switch(do_add_delta(t, key, nkey, incr, of.delta, tmpbuf, &of.req_cas_id, hv, &it)) {\n    case OK:\n        //if (c->noreply)\n        //    resp->skip = true;\n        // *it was filled, set the status below.\n        if (of.has_cas_in) {\n            // override the CAS. slightly inefficient but fixing that can wait\n            // until the next time do_add_delta is changed.\n            ITEM_set_cas(it, of.cas_id_in);\n        }\n        cas = ITEM_get_cas(it);\n        break;\n    case NON_NUMERIC:\n        errstr = \"CLIENT_ERROR cannot increment or decrement non-numeric value\";\n        goto error;\n        break;\n    case EOM:\n        errstr = \"SERVER_ERROR out of memory\";\n        goto error;\n        break;\n    case DELTA_ITEM_NOT_FOUND:\n        if (of.vivify) {\n            itoa_u64(of.initial, tmpbuf);\n            int vlen = strlen(tmpbuf);\n\n            it = item_alloc(key, nkey, 0, 0, vlen+2);\n            if (it != NULL) {\n                memcpy(ITEM_data(it), tmpbuf, vlen);\n                memcpy(ITEM_data(it) + vlen, \"\\r\\n\", 2);\n                if (do_store_item(it, NREAD_ADD, t, hv, NULL, &cas,\n                            of.has_cas_in ? of.cas_id_in : get_cas_id(), CAS_NO_STALE)) {\n                    item_created = true;\n                } else {\n                    // Not sure how we can get here if we're holding the lock.\n                    memcpy(resp->wbuf, \"NS\", 2);\n                }\n            } else {\n                errstr = \"SERVER_ERROR Out of memory allocating new item\";\n                goto error;\n            }\n        } else {\n            pthread_mutex_lock(&t->stats.mutex);\n            if (incr) {\n                t->stats.incr_misses++;\n            } else {\n                t->stats.decr_misses++;\n            }\n            pthread_mutex_unlock(&t->stats.mutex);\n            // won't have a valid it here.\n            memcpy(p, \"NF\", 2);\n            p += 2;\n        }\n        break;\n    case DELTA_ITEM_CAS_MISMATCH:\n        // also returns without a valid it.\n        memcpy(p, \"EX\", 2);\n        p += 2;\n        break;\n    }\n\n    // final loop\n    // allows building the response with information after vivifying from a\n    // miss, or returning a new CAS value after add_delta().\n    if (it) {\n        size_t vlen = strlen(tmpbuf);\n        if (of.value) {\n            memcpy(p, \"VA \", 3);\n            p = itoa_u32(vlen, p+3);\n        } else {\n            memcpy(p, \"HD\", 2);\n            p += 2;\n        }\n\n        for (i = pr->keytoken+1; i < pr->ntokens; i++) {\n            switch (pr->request[pr->tokens[i]]) {\n                case 'c':\n                    META_CHAR(p, 'c');\n                    p = itoa_u64(cas, p);\n                    break;\n                case 't':\n                    META_CHAR(p, 't');\n                    if (it->exptime == 0) {\n                        *p = '-';\n                        *(p+1) = '1';\n                        p += 2;\n                    } else {\n                        p = itoa_u32(it->exptime - current_time, p);\n                    }\n                    break;\n                case 'T':\n                    it->exptime = of.exptime;\n                    break;\n                case 'N':\n                    if (item_created) {\n                        it->exptime = of.autoviv_exptime;\n                    }\n                    break;\n                case 'O':\n                    tlen = _process_token_len(pr, i);\n                    if (tlen > MFLAG_MAX_OPAQUE_LENGTH) {\n                        errstr = \"CLIENT_ERROR opaque token too long\";\n                        goto error;\n                    }\n                    META_SPACE(p);\n                    memcpy(p, &pr->request[pr->tokens[i]], tlen);\n                    p += tlen;\n                    break;\n                case 'k':\n                    META_KEY(p, key, nkey, of.key_binary);\n                    break;\n            }\n        }\n\n        if (of.value) {\n            *p = '\\r';\n            *(p+1) = '\\n';\n            p += 2;\n            memcpy(p, tmpbuf, vlen);\n            p += vlen;\n        }\n\n        do_item_remove(it);\n    } else {\n        // No item to handle. still need to return opaque/key tokens\n        for (i = pr->keytoken+1; i < pr->ntokens; i++) {\n            switch (pr->request[pr->tokens[i]]) {\n                case 'O':\n                    tlen = _process_token_len(pr, i);\n                    if (tlen > MFLAG_MAX_OPAQUE_LENGTH) {\n                        errstr = \"CLIENT_ERROR opaque token too long\";\n                        goto error;\n                    }\n                    META_SPACE(p);\n                    memcpy(p, &pr->request[pr->tokens[i]], tlen);\n                    p += tlen;\n                    break;\n                case 'k':\n                    META_KEY(p, key, nkey, of.key_binary);\n                    break;\n            }\n        }\n    }\n\n    item_unlock(hv);\n\n    resp->wbytes = p - resp->wbuf;\n    memcpy(resp->wbuf + resp->wbytes, \"\\r\\n\", 2);\n    resp->wbytes += 2;\n    resp_add_iov(resp, resp->wbuf, resp->wbytes);\n    return;\nerror:\n    if (it != NULL)\n        do_item_remove(it);\n    if (locked)\n        item_unlock(hv);\n    pout_errstring(resp, errstr);\n}\n\n/*** Lua and internal handler ***/\n\nint mcplib_internal(lua_State *L) {\n    luaL_checkudata(L, 1, \"mcp.request\");\n    mcp_resp_t *r = lua_newuserdatauv(L, sizeof(mcp_resp_t), 0);\n    memset(r, 0, sizeof(mcp_resp_t));\n    luaL_getmetatable(L, \"mcp.response\");\n    lua_setmetatable(L, -2);\n\n    lua_pushinteger(L, MCP_YIELD_INTERNAL);\n    return lua_yield(L, 2);\n}\n\n// we're pretending to be p_c_ascii(), but reusing our already tokenized code.\n// the text parser should eventually move to the new tokenizer and we can\n// merge all of this code together.\nint mcplib_internal_run(mcp_rcontext_t *rctx) {\n    lua_State *L = rctx->Lc;\n    mcp_request_t *rq = luaL_checkudata(L, 1, \"mcp.request\");\n    mcp_resp_t *r = luaL_checkudata(L, 2, \"mcp.response\");\n    mc_resp *resp = resp_start_unlinked(rctx->c);\n    LIBEVENT_THREAD *t = rctx->c->thread;\n    mcp_parser_t *pr = &rq->pr;\n    if (resp == NULL) {\n        return -1;\n    }\n\n    // TODO: meta no-op isn't handled here. haven't decided how yet.\n    switch (rq->pr.command) {\n        case CMD_MG:\n            process_mget_cmd(t, pr, resp);\n            break;\n        case CMD_MS:\n            process_mset_cmd(t, pr, resp);\n            break;\n        case CMD_MD:\n            process_mdelete_cmd(t, pr, resp);\n            break;\n        case CMD_MA:\n            process_marithmetic_cmd(t, pr, resp);\n            break;\n        case CMD_GET:\n            process_get_cmd(t, pr, resp, _NO_CAS, _NO_TOUCH);\n            break;\n        case CMD_GETS:\n            process_get_cmd(t, pr, resp, _DO_CAS, _NO_TOUCH);\n            break;\n        case CMD_GAT:\n            process_get_cmd(t, pr, resp, _NO_CAS, _DO_TOUCH);\n            break;\n        case CMD_GATS:\n            process_get_cmd(t, pr, resp, _DO_CAS, _DO_TOUCH);\n            break;\n        case CMD_SET:\n            process_update_cmd(t, pr, resp, NREAD_SET, _NO_CAS);\n            break;\n        case CMD_ADD:\n            process_update_cmd(t, pr, resp, NREAD_ADD, _NO_CAS);\n            break;\n        case CMD_APPEND:\n            process_update_cmd(t, pr, resp, NREAD_APPEND, _NO_CAS);\n            break;\n        case CMD_PREPEND:\n            process_update_cmd(t, pr, resp, NREAD_PREPEND, _NO_CAS);\n            break;\n        case CMD_CAS:\n            process_update_cmd(t, pr, resp, NREAD_CAS, _DO_CAS);\n            break;\n        case CMD_REPLACE:\n            process_update_cmd(t, pr, resp, NREAD_REPLACE, _DO_CAS);\n            break;\n        case CMD_INCR:\n            process_arithmetic_cmd(t, pr, resp, true);\n            break;\n        case CMD_DECR:\n            process_arithmetic_cmd(t, pr, resp, false);\n            break;\n        case CMD_DELETE:\n            process_delete_cmd(t, pr, resp);\n            break;\n        case CMD_TOUCH:\n            process_touch_cmd(t, pr, resp);\n            break;\n        default:\n            resp_free(t, resp);\n            return -1;\n    }\n\n    // TODO: I'd like to shortcut the parsing here, but if we want the resp\n    // object to have full support (ie: resp:line()/etc) it might be necessary\n    // to still do a full parsing. It might be possible to\n    // wrap the main commands with something that decorates r->resp directly\n    // instead of going through a parser to save some CPU.\n    // Either way this is a lot less code.\n    mcmc_parse_buf(resp->iov[0].iov_base, resp->iov[0].iov_len, &r->resp);\n\n    if (rq->ascii_multiget) {\n        if (r->resp.type == MCMC_RESP_GET) {\n            // Blank out the END. Bad hack.\n            resp->iovcnt--;\n        } else if (r->resp.type == MCMC_RESP_END) {\n            resp->skip = true;\n        }\n    }\n\n    // in case someone logs this response it should make sense.\n    memcpy(r->be_name, \"internal\", strlen(\"internal\"));\n    memcpy(r->be_port, \"0\", 1);\n\n    // TODO: r-> will need status/code/mode copied from resp.\n    r->cresp = resp;\n    r->thread = t;\n    r->cmd = rq->pr.command;\n    // Always return OK from here as this is signalling an internal error.\n    r->status = MCMC_OK;\n\n    // resp object is associated with the\n    // response object, which is about a\n    // kilobyte.\n    t->proxy_vm_extra_kb++;\n\n    if (resp->io_pending) {\n        // TODO (v2): here we move the IO from the temporary resp to the top\n        // resp, but this feels kludgy so I'm leaving an explicit note to find\n        // a better way to do this.\n        rctx->resp->io_pending = resp->io_pending;\n        resp->io_pending = NULL;\n\n        // Add io object to extstore submission queue.\n        io_queue_t *q = conn_io_queue_get(rctx->c, IO_QUEUE_EXTSTORE);\n        io_pending_proxy_t *io = (io_pending_proxy_t *)rctx->resp->io_pending;\n\n        io->eio.next = q->stack_ctx;\n        q->stack_ctx = &io->eio;\n        assert(q->count >= 0);\n        q->count++;\n\n        io->rctx = rctx;\n        io->c = rctx->c;\n        io->ascii_multiget = rq->ascii_multiget;\n        // mark the buffer into the mcp_resp for freeing later.\n        r->buf = io->eio.buf;\n        return 1;\n    }\n\n    return 0;\n}\n"
        },
        {
          "name": "proxy_jump_hash.c",
          "type": "blob",
          "size": 1.3935546875,
          "content": "/* -*- Mode: C; tab-width: 4; c-basic-offset: 4; indent-tabs-mode: nil -*- */\n\n#include \"proxy.h\"\n\ntypedef struct {\n    struct proxy_hash_caller phc; // passed back to proxy API\n    unsigned int buckets;\n} mcplib_jump_hash_t;\n\nstatic uint32_t mcplib_dist_jump_hash_get_server(uint64_t hash, void *ctx) {\n    mcplib_jump_hash_t *jh = ctx;\n\n    int64_t b = -1, j = 0;\n    while (j < jh->buckets) {\n        b = j;\n        hash = hash * 2862933555777941757ULL + 1;\n        j = (b + 1) * ((double)(1LL << 31) / (double)((hash >> 33) + 1));\n    }\n    return b;\n}\n\n// stack = [pool, option]\nstatic int mcplib_dist_jump_hash_new(lua_State *L) {\n    luaL_checktype(L, 1, LUA_TTABLE);\n    lua_Unsigned buckets = lua_rawlen(L, 1);\n\n    mcplib_jump_hash_t *jh = lua_newuserdatauv(L, sizeof(mcplib_jump_hash_t), 0);\n\n    // don't need to loop through the table at all, just need its length.\n    // could optimize startup time by adding hints to the module for how to\n    // format pool (ie; just a total count or the full table)\n    jh->buckets = buckets;\n    jh->phc.ctx = jh;\n    jh->phc.selector_func = mcplib_dist_jump_hash_get_server;\n\n    lua_pushlightuserdata(L, &jh->phc);\n\n    // - return [UD, lightuserdata]\n    return 2;\n}\n\nint mcplib_open_dist_jump_hash(lua_State *L) {\n    const struct luaL_Reg jump_f[] = {\n        {\"new\", mcplib_dist_jump_hash_new},\n        {NULL, NULL},\n    };\n\n    luaL_newlib(L, jump_f);\n\n    return 1;\n}\n"
        },
        {
          "name": "proxy_lua.c",
          "type": "blob",
          "size": 60.962890625,
          "content": "/* -*- Mode: C; tab-width: 4; c-basic-offset: 4; indent-tabs-mode: nil -*- */\n\n#include \"proxy.h\"\n#include \"proxy_tls.h\"\n#include \"storage.h\" // for stats call\n\n// func prototype example:\n// static int fname (lua_State *L)\n// normal library open:\n// int luaopen_mcp(lua_State *L) { }\n\nstruct _mcplib_statctx_s {\n    lua_State *L;\n};\n\nstatic void _mcplib_append_stats(const char *key, const uint16_t klen,\n                  const char *val, const uint32_t vlen,\n                  const void *cookie) {\n    // k + v == 0 means END, but we don't use END for this lua API.\n    if (klen == 0) {\n        return;\n    }\n\n    // cookie -> struct\n    const struct _mcplib_statctx_s *c = cookie;\n    lua_State *L = c->L;\n    // table should always be on the top.\n    lua_pushlstring(L, key, klen);\n    lua_pushlstring(L, val, vlen);\n    lua_rawset(L, -3);\n}\n\nstatic void _mcplib_append_section_stats(const char *key, const uint16_t klen,\n                  const char *val, const uint32_t vlen,\n                  const void *cookie) {\n    char stat[STAT_KEY_LEN];\n    long section = 0;\n    if (klen == 0) {\n        return;\n    }\n\n    const struct _mcplib_statctx_s *c = cookie;\n    lua_State *L = c->L;\n    // table must be at the top when this function is called.\n    int tidx = lua_absindex(L, -1);\n\n    // NOTE: sscanf is not great, especially with numerics due to UD for out\n    // of range data. It is safe to use here because we're generating the\n    // strings, and we don't use this function on anything that has user\n    // defined data (ie; stats proxy). Otherwise sscanf saves a lot of code so\n    // we use it here.\n    if (sscanf(key, \"items:%ld:%s\", &section, stat) == 2\n            || sscanf(key, \"%ld:%s\", &section, stat) == 2) {\n        // stats [items, slabs, conns]\n        if (lua_rawgeti(L, tidx, section) == LUA_TNIL) {\n            lua_pop(L, 1); // drop the nil\n            // no sub-section table yet, create one.\n            lua_newtable(L);\n            lua_pushvalue(L, -1); // copy the table\n            lua_rawseti(L, tidx, section); // remember the table\n            // now top of stack is the table.\n        }\n\n        lua_pushstring(L, stat);\n        lua_pushlstring(L, val, vlen);\n        lua_rawset(L, -3); // put key/val into sub-table\n        lua_pop(L, 1); // pop sub-table.\n    } else {\n        // normal stat counter.\n        lua_pushlstring(L, key, klen);\n        lua_pushlstring(L, val, vlen);\n        lua_rawset(L, tidx);\n    }\n}\n\n// reimplementation of proto_text.c:process_stat()\nstatic int mcplib_server_stats(lua_State *L) {\n    int argc = lua_gettop(L);\n    proxy_ctx_t *ctx = PROXY_GET_CTX(L);\n    lua_newtable(L); // the table to return.\n    struct _mcplib_statctx_s c = {\n        L,\n    };\n\n    if (argc == 0 || lua_isnil(L, 1)) {\n        server_stats(&_mcplib_append_stats, &c);\n        get_stats(NULL, 0, &_mcplib_append_stats, &c);\n    } else {\n        const char *cmd = luaL_checkstring(L, 1);\n        if (strcmp(cmd, \"settings\") == 0) {\n            process_stat_settings(&_mcplib_append_stats, &c);\n        } else if (strcmp(cmd, \"conns\") == 0) {\n            process_stats_conns(&_mcplib_append_section_stats, &c);\n#ifdef EXTSTORE\n        } else if (strcmp(cmd, \"extstore\") == 0) {\n            process_extstore_stats(&_mcplib_append_stats, &c);\n#endif\n        } else if (strcmp(cmd, \"proxy\") == 0) {\n            process_proxy_stats(ctx, &_mcplib_append_stats, &c);\n        } else if (strcmp(cmd, \"proxyfuncs\") == 0) {\n            process_proxy_funcstats(ctx, &_mcplib_append_stats, &c);\n        } else if (strcmp(cmd, \"proxybe\") == 0) {\n            process_proxy_bestats(ctx, &_mcplib_append_stats, &c);\n        } else {\n            if (get_stats(cmd, strlen(cmd), &_mcplib_append_section_stats, &c)) {\n                // all good.\n            } else {\n                // unknown command.\n                proxy_lua_error(L, \"unknown subcommand passed to server_stats\");\n            }\n        }\n    }\n\n    // return the table.\n    return 1;\n}\n\nstatic lua_Integer _mcplib_backend_get_waittime(lua_Number secondsf) {\n    lua_Integer secondsi = (lua_Integer) secondsf;\n    lua_Number subseconds = secondsf - secondsi;\n    if (subseconds >= 0.5) {\n        // Yes, I know this rounding is probably wrong. it's close enough.\n        // Rounding functions have tricky portability and whole-integer\n        // rounding is at least simpler to reason about.\n        secondsi++;\n    }\n    if (secondsi < 1) {\n        secondsi = 1;\n    }\n    return secondsi;\n}\n\n// take string, table as arg:\n// name, { every =, rerun = false, func = f }\n// repeat defaults to true\nstatic int mcplib_register_cron(lua_State *L) {\n    proxy_ctx_t *ctx = PROXY_GET_CTX(L);\n    const char *name = luaL_checkstring(L, 1);\n    luaL_checktype(L, 2, LUA_TTABLE);\n\n    // reserve an upvalue for storing the function.\n    mcp_cron_t *ce = lua_newuserdatauv(L, sizeof(mcp_cron_t), 1);\n    memset(ce, 0, sizeof(*ce));\n\n    // default repeat.\n    ce->repeat = true;\n    // sync config generation.\n    ce->gen = ctx->config_generation;\n\n    if (lua_getfield(L, 2, \"func\") != LUA_TNIL) {\n        luaL_checktype(L, -1, LUA_TFUNCTION);\n        lua_setiuservalue(L, 3, 1); // pop value\n    } else {\n        proxy_lua_error(L, \"proxy cron entry missing 'func' field\");\n        return 0;\n    }\n\n    if (lua_getfield(L, 2, \"rerun\") != LUA_TNIL) {\n        int rerun = lua_toboolean(L, -1);\n        if (!rerun) {\n            ce->repeat = false;\n        }\n    }\n    lua_pop(L, 1); // pop val or nil\n\n    // TODO: set a limit on 'every' so we don't have to worry about\n    // underflows. a year? a month?\n    if (lua_getfield(L, 2, \"every\") != LUA_TNIL) {\n        luaL_checktype(L, -1, LUA_TNUMBER);\n        int every = lua_tointeger(L, -1);\n        if (every < 1) {\n            proxy_lua_error(L, \"proxy cron entry 'every' must be > 0\");\n            return 0;\n        }\n        ce->every = every;\n    } else {\n        proxy_lua_error(L, \"proxy cron entry missing 'every' field\");\n        return 0;\n    }\n    lua_pop(L, 1); // pop val or nil\n\n    // schedule the next cron run\n    struct timespec now;\n    clock_gettime(CLOCK_REALTIME, &now);\n    ce->next = now.tv_sec + ce->every;\n    // we may adjust ce->next shortly, so don't update global yet.\n\n    // valid cron entry, now place into cron table.\n    lua_rawgeti(L, LUA_REGISTRYINDEX, ctx->cron_ref);\n\n    // first, check if a cron of this name already exists.\n    // if so and the 'every' field matches, inherit its 'next' field\n    // so we don't perpetually reschedule all crons.\n    if (lua_getfield(L, -1, name) != LUA_TNIL) {\n        mcp_cron_t *oldce = lua_touserdata(L, -1);\n        if (ce->every == oldce->every) {\n            ce->next = oldce->next;\n        }\n    }\n    lua_pop(L, 1); // drop val/nil\n\n    lua_pushvalue(L, 3); // duplicate cron entry\n    lua_setfield(L, -2, name); // pop duplicate cron entry\n    lua_pop(L, 1); // drop cron table\n\n    // update central cron sleep.\n    if (ctx->cron_next > ce->next) {\n        ctx->cron_next = ce->next;\n    }\n\n    return 0;\n}\n\n// just set ctx->loading = true\n// called from config thread, so config_lock must be held, so it's safe to\n// modify protected ctx contents.\nstatic int mcplib_schedule_config_reload(lua_State *L) {\n    proxy_ctx_t *ctx = PROXY_GET_CTX(L);\n    ctx->loading = true;\n    return 0;\n}\n\nstatic int mcplib_time_real_millis(lua_State *L) {\n    struct timespec now;\n    clock_gettime(CLOCK_REALTIME, &now);\n    lua_Integer t = now.tv_nsec / 1000000 + now.tv_sec * 1000;\n    lua_pushinteger(L, t);\n    return 1;\n}\n\nstatic int mcplib_time_mono_millis(lua_State *L) {\n    struct timespec now;\n    clock_gettime(CLOCK_MONOTONIC, &now);\n    lua_Integer t = now.tv_nsec / 1000000 + now.tv_sec * 1000;\n    lua_pushinteger(L, t);\n    return 1;\n}\n\n// end util funcs.\n\n// NOTE: backends are global objects owned by pool objects.\n// Each pool has a \"proxy pool object\" distributed to each worker VM.\n// proxy pool objects are held at the same time as any request exists on a\n// backend, in the coroutine stack during yield()\n// To free a backend: All proxies for a pool are collected, then the central\n// pool is collected, which releases backend references, which allows backend\n// to be collected.\nstatic int mcplib_backend_wrap_gc(lua_State *L) {\n    mcp_backend_wrap_t *bew = luaL_checkudata(L, -1, \"mcp.backendwrap\");\n    proxy_ctx_t *ctx = PROXY_GET_CTX(L);\n\n    if (bew->be != NULL) {\n        mcp_backend_t *be = bew->be;\n        // TODO (v3): technically a race where a backend could be created,\n        // queued, but not picked up before being gc'ed again. In practice\n        // this is impossible but at some point we should close the loop here.\n        // Since we're running in the config thread it could just busy poll\n        // until the connection was picked up.\n        assert(be->transferred);\n        // There has to be at least one connection, and the event_thread will\n        // always be the same.\n        proxy_event_thread_t *e = be->be[0].event_thread;\n        pthread_mutex_lock(&e->mutex);\n        STAILQ_INSERT_TAIL(&e->beconn_head_in, be, beconn_next);\n        pthread_mutex_unlock(&e->mutex);\n\n        // Signal to check queue.\n#ifdef USE_EVENTFD\n        uint64_t u = 1;\n        // TODO (v2): check result? is it ever possible to get a short write/failure\n        // for an eventfd?\n        if (write(e->be_event_fd, &u, sizeof(uint64_t)) != sizeof(uint64_t)) {\n            assert(1 == 0);\n        }\n#else\n        if (write(e->be_notify_send_fd, \"w\", 1) <= 0) {\n            assert(1 == 0);\n        }\n#endif\n    }\n\n    STAT_DECR(ctx, backend_total, 1);\n\n    return 0;\n}\n\nstatic int mcplib_backend_gc(lua_State *L) {\n    return 0; // no-op.\n}\n\n// backend label object; given to pools which then find or create backend\n// objects as necessary.\n// allow optionally passing a table of arguments for extended options:\n// { label = \"etc\", \"host\" = \"127.0.0.1\", port = \"11211\",\n//   readtimeout = 0.5, connecttimeout = 1, retrytime = 3,\n//   failurelimit = 3, tcpkeepalive = false }\nstatic int mcplib_backend(lua_State *L) {\n    size_t llen = 0;\n    size_t nlen = 0;\n    size_t plen = 0;\n    proxy_ctx_t *ctx = PROXY_GET_CTX(L);\n    mcp_backend_label_t *be = lua_newuserdatauv(L, sizeof(mcp_backend_label_t), 0);\n    memset(be, 0, sizeof(*be));\n    const char *label;\n    const char *name;\n    const char *port;\n    // copy global defaults for tunables.\n    memcpy(&be->tunables, &ctx->tunables, sizeof(be->tunables));\n    be->conncount = 1; // one connection per backend as default.\n\n    if (lua_istable(L, 1)) {\n\n        // We don't pop the label/host/port strings so lua won't change them\n        // until after the function call.\n        if (lua_getfield(L, 1, \"label\") != LUA_TNIL) {\n            label = luaL_checklstring(L, -1, &llen);\n        } else {\n            proxy_lua_error(L, \"backend must have a label argument\");\n            return 0;\n        }\n\n        if (lua_getfield(L, 1, \"host\") != LUA_TNIL) {\n            name = luaL_checklstring(L, -1, &nlen);\n        } else {\n            proxy_lua_error(L, \"backend must have a host argument\");\n            return 0;\n        }\n\n        // TODO: allow a default port.\n        if (lua_getfield(L, 1, \"port\") != LUA_TNIL) {\n            port = luaL_checklstring(L, -1, &plen);\n        } else {\n            proxy_lua_error(L, \"backend must have a port argument\");\n            return 0;\n        }\n\n        if (lua_getfield(L, 1, \"tcpkeepalive\") != LUA_TNIL) {\n            be->tunables.tcp_keepalive = lua_toboolean(L, -1);\n        }\n        lua_pop(L, 1);\n\n        if (lua_getfield(L, 1, \"tls\") != LUA_TNIL) {\n            be->tunables.use_tls = lua_toboolean(L, -1);\n        }\n        lua_pop(L, 1);\n\n        if (lua_getfield(L, 1, \"failurelimit\") != LUA_TNIL) {\n            int limit = luaL_checkinteger(L, -1);\n            if (limit < 0) {\n                proxy_lua_error(L, \"failurelimit must be >= 0\");\n                return 0;\n            }\n\n            be->tunables.backend_failure_limit = limit;\n        }\n        lua_pop(L, 1);\n\n        if (lua_getfield(L, 1, \"depthlimit\") != LUA_TNIL) {\n            int limit = luaL_checkinteger(L, -1);\n            if (limit < 0) {\n                proxy_lua_error(L, \"depthlimit must be >= 0\");\n                return 0;\n            }\n\n            be->tunables.backend_depth_limit = limit;\n        }\n        lua_pop(L, 1);\n\n        if (lua_getfield(L, 1, \"connecttimeout\") != LUA_TNIL) {\n            lua_Number secondsf = luaL_checknumber(L, -1);\n            lua_Integer secondsi = (lua_Integer) secondsf;\n            lua_Number subseconds = secondsf - secondsi;\n\n            be->tunables.connect.tv_sec = secondsi;\n            be->tunables.connect.tv_usec = MICROSECONDS(subseconds);\n        }\n        lua_pop(L, 1);\n\n        // TODO (v2): print deprecation warning.\n        if (lua_getfield(L, 1, \"retrytimeout\") != LUA_TNIL) {\n            be->tunables.retry.tv_sec =\n                _mcplib_backend_get_waittime(luaL_checknumber(L, -1));\n        }\n        lua_pop(L, 1);\n\n        if (lua_getfield(L, 1, \"retrywaittime\") != LUA_TNIL) {\n            be->tunables.retry.tv_sec =\n                _mcplib_backend_get_waittime(luaL_checknumber(L, -1));\n        }\n        lua_pop(L, 1);\n\n        if (lua_getfield(L, 1, \"retrytimeout\") != LUA_TNIL) {\n            lua_Number secondsf = luaL_checknumber(L, -1);\n            lua_Integer secondsi = (lua_Integer) secondsf;\n            lua_Number subseconds = secondsf - secondsi;\n\n            be->tunables.retry.tv_sec = secondsi;\n            be->tunables.retry.tv_usec = MICROSECONDS(subseconds);\n        }\n        lua_pop(L, 1);\n\n        if (lua_getfield(L, 1, \"readtimeout\") != LUA_TNIL) {\n            lua_Number secondsf = luaL_checknumber(L, -1);\n            lua_Integer secondsi = (lua_Integer) secondsf;\n            lua_Number subseconds = secondsf - secondsi;\n\n            be->tunables.read.tv_sec = secondsi;\n            be->tunables.read.tv_usec = MICROSECONDS(subseconds);\n        }\n        lua_pop(L, 1);\n\n        if (lua_getfield(L, 1, \"down\") != LUA_TNIL) {\n            int down = lua_toboolean(L, -1);\n            be->tunables.down = down;\n        }\n        lua_pop(L, 1);\n\n        if (lua_getfield(L, 1, \"flaptime\") != LUA_TNIL) {\n            lua_Number secondsf = luaL_checknumber(L, -1);\n            lua_Integer secondsi = (lua_Integer) secondsf;\n            lua_Number subseconds = secondsf - secondsi;\n\n            be->tunables.flap.tv_sec = secondsi;\n            be->tunables.flap.tv_usec = MICROSECONDS(subseconds);\n        }\n        lua_pop(L, 1);\n\n        if (lua_getfield(L, 1, \"flapbackofframp\") != LUA_TNIL) {\n            float ramp = luaL_checknumber(L, -1);\n            if (ramp <= 1.1) {\n                ramp = 1.1;\n            }\n            be->tunables.flap_backoff_ramp = ramp;\n        }\n        lua_pop(L, 1);\n\n        if (lua_getfield(L, 1, \"flapbackoffmax\") != LUA_TNIL) {\n            luaL_checknumber(L, -1);\n            uint32_t max = lua_tointeger(L, -1);\n            be->tunables.flap_backoff_max = max;\n        }\n        lua_pop(L, 1);\n\n        if (lua_getfield(L, 1, \"connections\") != LUA_TNIL) {\n            int c = luaL_checkinteger(L, -1);\n            if (c <= 0) {\n                proxy_lua_error(L, \"backend connections argument must be >= 0\");\n                return 0;\n            } else if (c > 8) {\n                proxy_lua_error(L, \"backend connections argument must be <= 8\");\n                return 0;\n            }\n\n            be->conncount = c;\n        }\n        lua_pop(L, 1);\n\n    } else {\n        label = luaL_checklstring(L, 1, &llen);\n        name = luaL_checklstring(L, 2, &nlen);\n        port = luaL_checklstring(L, 3, &plen);\n    }\n\n    if (llen > MAX_LABELLEN-1) {\n        proxy_lua_error(L, \"backend label too long\");\n        return 0;\n    }\n\n    if (nlen > MAX_NAMELEN-1) {\n        proxy_lua_error(L, \"backend name too long\");\n        return 0;\n    }\n\n    if (plen > MAX_PORTLEN-1) {\n        proxy_lua_error(L, \"backend port too long\");\n        return 0;\n    }\n\n    memcpy(be->label, label, llen);\n    be->label[llen] = '\\0';\n    memcpy(be->name, name, nlen);\n    be->name[nlen] = '\\0';\n    memcpy(be->port, port, plen);\n    be->port[plen] = '\\0';\n    be->llen = llen;\n    if (lua_istable(L, 1)) {\n        lua_pop(L, 3); // drop label, name, port.\n    }\n    luaL_getmetatable(L, \"mcp.backend\");\n    lua_setmetatable(L, -2); // set metatable to userdata.\n\n    return 1; // return be object.\n}\n\n// Called with the cache label at top of the stack.\nstatic mcp_backend_wrap_t *_mcplib_backend_checkcache(lua_State *L, mcp_backend_label_t *bel) {\n    // first check our reference table to compare.\n    // Note: The upvalue won't be found unless we're running from a function with it\n    // set as an upvalue.\n    int ret = lua_gettable(L, lua_upvalueindex(MCP_BACKEND_UPVALUE));\n    if (ret != LUA_TNIL) {\n        mcp_backend_wrap_t *be_orig = luaL_checkudata(L, -1, \"mcp.backendwrap\");\n        if (strncmp(be_orig->be->name, bel->name, MAX_NAMELEN) == 0\n                && strncmp(be_orig->be->port, bel->port, MAX_PORTLEN) == 0\n                && be_orig->be->conncount == bel->conncount\n                && memcmp(&be_orig->be->tunables, &bel->tunables, sizeof(bel->tunables)) == 0) {\n            // backend is the same, return it.\n            return be_orig;\n        } else {\n            // backend not the same, pop from stack and make new one.\n            lua_pop(L, 1);\n        }\n    } else {\n        lua_pop(L, 1); // pop the nil.\n    }\n\n    return NULL;\n}\n\nstatic mcp_backend_wrap_t *_mcplib_make_backendconn(lua_State *L, mcp_backend_label_t *bel,\n        proxy_event_thread_t *e) {\n    proxy_ctx_t *ctx = PROXY_GET_CTX(L);\n\n    mcp_backend_wrap_t *bew = lua_newuserdatauv(L, sizeof(mcp_backend_wrap_t), 0);\n    luaL_getmetatable(L, \"mcp.backendwrap\");\n    lua_setmetatable(L, -2); // set metatable to userdata.\n\n    mcp_backend_t *be = calloc(1, sizeof(mcp_backend_t) + sizeof(struct mcp_backendconn_s) * bel->conncount);\n    if (be == NULL) {\n        proxy_lua_error(L, \"out of memory allocating backend connection\");\n        return NULL;\n    }\n    bew->be = be;\n\n    strncpy(be->name, bel->name, MAX_NAMELEN+1);\n    strncpy(be->port, bel->port, MAX_PORTLEN+1);\n    strncpy(be->label, bel->label, MAX_LABELLEN+1);\n    memcpy(&be->tunables, &bel->tunables, sizeof(bel->tunables));\n    be->conncount = bel->conncount;\n    STAILQ_INIT(&be->io_head);\n\n    for (int x = 0; x < bel->conncount; x++) {\n        struct mcp_backendconn_s *bec = &be->be[x];\n        bec->be_parent = be;\n        memcpy(&bec->tunables, &bel->tunables, sizeof(bel->tunables));\n        STAILQ_INIT(&bec->io_head);\n        bec->state = mcp_backend_read;\n\n        // this leaves a permanent buffer on the backend, which is fine\n        // unless you have billions of backends.\n        // we can later optimize for pulling buffers from idle backends.\n        bec->rbuf = malloc(READ_BUFFER_SIZE);\n        if (bec->rbuf == NULL) {\n            proxy_lua_error(L, \"out of memory allocating backend\");\n            return NULL;\n        }\n\n        // initialize the client\n        bec->client = malloc(mcmc_size(MCMC_OPTION_BLANK));\n        if (bec->client == NULL) {\n            proxy_lua_error(L, \"out of memory allocating backend\");\n            return NULL;\n        }\n        // TODO (v2): no way to change the TCP_KEEPALIVE state post-construction.\n        // This is a trivial fix if we ensure a backend's owning event thread is\n        // set before it can be used in the proxy, as it would have access to the\n        // tunables structure. _reset_bad_backend() may not have its event thread\n        // set 100% of the time and I don't want to introduce a crash right now,\n        // so I'm writing this overly long comment. :)\n        int flags = MCMC_OPTION_NONBLOCK;\n        STAT_L(ctx);\n        if (ctx->tunables.tcp_keepalive) {\n            flags |= MCMC_OPTION_TCP_KEEPALIVE;\n        }\n        STAT_UL(ctx);\n        bec->connect_flags = flags;\n\n        // FIXME: remove ifdef via an initialized checker? or\n        // mcp_tls_backend_init response code?\n#ifdef PROXY_TLS\n        if (be->tunables.use_tls && !ctx->tls_ctx) {\n            proxy_lua_error(L, \"TLS requested but not initialized: call mcp.init_tls()\");\n            return NULL;\n        }\n#endif\n        mcp_tls_backend_init(ctx, bec);\n\n        bec->event_thread = e;\n    }\n    pthread_mutex_lock(&e->mutex);\n    STAILQ_INSERT_TAIL(&e->beconn_head_in, be, beconn_next);\n    pthread_mutex_unlock(&e->mutex);\n\n    // Signal to check queue.\n#ifdef USE_EVENTFD\n    uint64_t u = 1;\n    // TODO (v2): check result? is it ever possible to get a short write/failure\n    // for an eventfd?\n    if (write(e->be_event_fd, &u, sizeof(uint64_t)) != sizeof(uint64_t)) {\n        assert(1 == 0);\n    }\n#else\n    if (write(e->be_notify_send_fd, \"w\", 1) <= 0) {\n        assert(1 == 0);\n    }\n#endif\n\n    lua_pushvalue(L, -2); // push the label string back to the top.\n    // Add this new backend connection to the object cache.\n    lua_pushvalue(L, -2); // copy the backend reference to the top.\n    // set our new backend wrapper object into the reference table.\n    lua_settable(L, lua_upvalueindex(MCP_BACKEND_UPVALUE));\n    // stack is back to having backend on the top.\n\n    STAT_INCR(ctx, backend_total, 1);\n\n    return bew;\n}\n\nstatic int mcplib_pool_gc(lua_State *L) {\n    mcp_pool_t *p = luaL_checkudata(L, -1, \"mcp.pool\");\n\n    mcp_gobj_finalize(&p->g);\n\n    luaL_unref(L, LUA_REGISTRYINDEX, p->phc_ref);\n\n    for (int x = 0; x < p->pool_be_total; x++) {\n        if (p->pool[x].ref) {\n            luaL_unref(L, LUA_REGISTRYINDEX, p->pool[x].ref);\n        }\n    }\n\n    return 0;\n}\n\n// Looks for a short string in a key to separate which part gets hashed vs\n// sent to the backend node.\n// ie: \"foo:bar|#|restofkey\" - only \"foo:bar\" gets hashed.\nstatic const char *mcp_key_hash_filter_stop(const char *conf, const char *key, size_t klen, size_t *newlen) {\n    char temp[KEY_MAX_LENGTH+1];\n    *newlen = klen;\n    if (klen > KEY_MAX_LENGTH) {\n        // Hedging against potential bugs.\n        return key;\n    }\n\n    memcpy(temp, key, klen);\n    temp[klen+1] = '\\0';\n\n    // TODO (v2): memmem would avoid the temp key and memcpy here, but it's\n    // not technically portable. An easy improvement would be to detect\n    // memmem() in `configure` and only use strstr/copy as a fallback.\n    // Since keys are short it's unlikely this would be a major performance\n    // win.\n    char *found = strstr(temp, conf);\n\n    if (found) {\n        *newlen = found - temp;\n    }\n\n    // hash stop can't change where keys start.\n    return key;\n}\n\n// Takes a two character \"tag\", ie; \"{}\", or \"$$\", searches string for the\n// first then second character. Only hashes the portion within these tags.\n// *conf _must_ be two characters.\nstatic const char *mcp_key_hash_filter_tag(const char *conf, const char *key, size_t klen, size_t *newlen) {\n    *newlen = klen;\n\n    const char *t1 = memchr(key, conf[0], klen);\n    if (t1) {\n        size_t remain = klen - (t1 - key);\n        // must be at least one character inbetween the tags to hash.\n        if (remain > 1) {\n            const char *t2 = memchr(t1, conf[1], remain);\n\n            if (t2) {\n                *newlen = t2 - t1 - 1;\n                return t1+1;\n            }\n        }\n    }\n\n    return key;\n}\n\nstatic void _mcplib_pool_dist(lua_State *L, mcp_pool_t *p) {\n    luaL_checktype(L, -1, LUA_TTABLE);\n    if (lua_getfield(L, -1, \"new\") != LUA_TFUNCTION) {\n        proxy_lua_error(L, \"key distribution object missing 'new' function\");\n        return;\n    }\n\n    // - now create the copy pool table\n    lua_createtable(L, p->pool_size, 0); // give the new pool table a sizing hint.\n    for (int x = 1; x <= p->pool_size; x++) {\n        mcp_backend_t *be = p->pool[x-1].be;\n        lua_createtable(L, 0, 4);\n        // stack = [p, h, f, optN, newpool, backend]\n        // the key should be fine for id? maybe don't need to duplicate\n        // this?\n        lua_pushinteger(L, x);\n        lua_setfield(L, -2, \"id\");\n        // we don't use the hostname for ketama hashing\n        // so passing ip for hostname is fine\n        lua_pushstring(L, be->name);\n        lua_setfield(L, -2, \"addr\");\n        lua_pushstring(L, be->port);\n        lua_setfield(L, -2, \"port\");\n\n        // set the backend table into the new pool table.\n        lua_rawseti(L, -2, x);\n    }\n\n    // we can either use lua_insert() or possibly _rotate to shift\n    // things into the right place, but simplest is to just copy the\n    // option arg to the end of the stack.\n    lua_pushvalue(L, 2);\n    //   - stack should be: pool, opts, func, pooltable, opts\n\n    // call the dist new function.\n    int res = lua_pcall(L, 2, 2, 0);\n\n    if (res != LUA_OK) {\n        lua_error(L); // error should be on the stack already.\n        return;\n    }\n\n    // -1 is lightuserdata ptr to the struct (which must be owned by the\n    // userdata), which is later used for internal calls.\n    struct proxy_hash_caller *phc;\n\n    luaL_checktype(L, -1, LUA_TLIGHTUSERDATA);\n    luaL_checktype(L, -2, LUA_TUSERDATA);\n    phc = lua_touserdata(L, -1);\n    memcpy(&p->phc, phc, sizeof(*phc));\n    lua_pop(L, 1);\n    // -2 was userdata we need to hold a reference to\n    p->phc_ref = luaL_ref(L, LUA_REGISTRYINDEX);\n    // UD now popped from stack.\n}\n\n// in the proxy object, we can alias a ptr to the pool to where it needs to be\n// based on worker number or io_thread right?\nstatic void _mcplib_pool_make_be_loop(lua_State *L, mcp_pool_t *p, int offset, proxy_event_thread_t *t) {\n    // remember lua arrays are 1 indexed.\n    for (int x = 1; x <= p->pool_size; x++) {\n        mcp_pool_be_t *s = &p->pool[x-1 + (offset * p->pool_size)];\n        lua_geti(L, 1, x); // get next server into the stack.\n        // If we bail here, the pool _gc() should handle releasing any backend\n        // references we made so far.\n        mcp_backend_label_t *bel = luaL_checkudata(L, -1, \"mcp.backend\");\n\n        // check label for pre-existing backend conn/wrapper\n        // TODO (v2): there're native ways of \"from C make lua strings\"\n        int toconcat = 1;\n        if (p->beprefix[0] != '\\0') {\n            lua_pushstring(L, p->beprefix);\n            toconcat++;\n        }\n        if (p->use_iothread) {\n            lua_pushstring(L, \":io:\");\n            toconcat++;\n        } else {\n            lua_pushstring(L, \":w\");\n            lua_pushinteger(L, offset);\n            lua_pushstring(L, \":\");\n            toconcat += 3;\n        }\n        lua_pushlstring(L, bel->label, bel->llen);\n        lua_concat(L, toconcat);\n\n        lua_pushvalue(L, -1); // copy the label string for the create method.\n        mcp_backend_wrap_t *bew = _mcplib_backend_checkcache(L, bel);\n        if (bew == NULL) {\n            bew = _mcplib_make_backendconn(L, bel, t);\n        }\n        s->be = bew->be; // unwrap the backend connection for direct ref.\n        bew->be->use_io_thread = p->use_iothread;\n\n        // If found from cache or made above, the backend wrapper is on the\n        // top of the stack, so we can now take its reference.\n        // The wrapper abstraction allows the be memory to be owned by its\n        // destination thread (IO thread/etc).\n\n        s->ref = luaL_ref(L, LUA_REGISTRYINDEX); // references and pops object.\n        lua_pop(L, 1); // pop the mcp.backend label object.\n        lua_pop(L, 1); // drop extra label copy.\n    }\n}\n\n// call with table of backends in 1\nstatic void _mcplib_pool_make_be(lua_State *L, mcp_pool_t *p) {\n    if (p->use_iothread) {\n        proxy_ctx_t *ctx = PROXY_GET_CTX(L);\n        _mcplib_pool_make_be_loop(L, p, 0, ctx->proxy_io_thread);\n    } else {\n        // TODO (v3) globals.\n        for (int n = 0; n < settings.num_threads; n++) {\n            LIBEVENT_THREAD *t = get_worker_thread(n);\n            _mcplib_pool_make_be_loop(L, p, t->thread_baseid, t->proxy_event_thread);\n        }\n    }\n}\n\n// p = mcp.pool(backends, { dist = f, hashfilter = f, seed = \"a\", hash = f })\nstatic int mcplib_pool(lua_State *L) {\n    proxy_ctx_t *ctx = PROXY_GET_CTX(L);\n    int argc = lua_gettop(L);\n    luaL_checktype(L, 1, LUA_TTABLE);\n    int n = luaL_len(L, 1); // get length of array table\n    int workers = settings.num_threads; // TODO (v3): globals usage.\n\n    size_t plen = sizeof(mcp_pool_t) + (sizeof(mcp_pool_be_t) * n * workers);\n    mcp_pool_t *p = lua_newuserdatauv(L, plen, 0);\n    // Zero the memory before use, so we can realibly use __gc to clean up\n    memset(p, 0, plen);\n    p->pool_size = n;\n    p->pool_be_total = n * workers;\n    p->use_iothread = ctx->tunables.use_iothread;\n    // TODO (v2): Nicer if this is fetched from mcp.default_key_hash\n    p->key_hasher = XXH3_64bits_withSeed;\n    pthread_mutex_init(&p->g.lock, NULL);\n    p->ctx = PROXY_GET_CTX(L);\n\n    luaL_setmetatable(L, \"mcp.pool\");\n\n    // Allow passing an ignored nil as a second argument. Makes the lua easier\n    int type = lua_type(L, 2);\n    if (argc == 1 || type == LUA_TNIL) {\n        _mcplib_pool_make_be(L, p);\n        lua_getglobal(L, \"mcp\");\n        // TODO (v2): decide on a mcp.default_dist and use that instead\n        if (lua_getfield(L, -1, \"dist_jump_hash\") != LUA_TNIL) {\n            _mcplib_pool_dist(L, p);\n            lua_pop(L, 1); // pop \"dist_jump_hash\" value.\n        } else {\n            lua_pop(L, 1);\n        }\n        lua_pop(L, 1); // pop \"mcp\"\n        return 1;\n    }\n\n    // Supplied with an options table. We inspect this table to decorate the\n    // pool, then pass it along to the a constructor if necessary.\n    luaL_checktype(L, 2, LUA_TTABLE);\n\n    if (lua_getfield(L, 2, \"iothread\") != LUA_TNIL) {\n        luaL_checktype(L, -1, LUA_TBOOLEAN);\n        int use_iothread = lua_toboolean(L, -1);\n        if (use_iothread) {\n            p->use_iothread = true;\n        } else {\n            p->use_iothread = false;\n        }\n        lua_pop(L, 1); // remove value.\n    } else {\n        lua_pop(L, 1); // pop the nil.\n    }\n\n    if (lua_getfield(L, 2, \"beprefix\") != LUA_TNIL) {\n        luaL_checktype(L, -1, LUA_TSTRING);\n        size_t len = 0;\n        const char *bepfx = lua_tolstring(L, -1, &len);\n        memcpy(p->beprefix, bepfx, len);\n        p->beprefix[len+1] = '\\0';\n        lua_pop(L, 1); // pop beprefix string.\n    } else {\n        lua_pop(L, 1); // pop the nil.\n    }\n    _mcplib_pool_make_be(L, p);\n\n    // stack: backends, options, mcp.pool\n    if (lua_getfield(L, 2, \"dist\") != LUA_TNIL) {\n        // overriding the distribution function.\n        _mcplib_pool_dist(L, p);\n        lua_pop(L, 1); // remove the dist table from stack.\n    } else {\n        lua_pop(L, 1); // pop the nil.\n\n        // use the default dist if not specified with an override table.\n        lua_getglobal(L, \"mcp\");\n        // TODO (v2): decide on a mcp.default_dist and use that instead\n        if (lua_getfield(L, -1, \"dist_jump_hash\") != LUA_TNIL) {\n            _mcplib_pool_dist(L, p);\n            lua_pop(L, 1); // pop \"dist_jump_hash\" value.\n        } else {\n            lua_pop(L, 1);\n        }\n        lua_pop(L, 1); // pop \"mcp\"\n    }\n\n    if (lua_getfield(L, 2, \"filter\") != LUA_TNIL) {\n        luaL_checktype(L, -1, LUA_TSTRING);\n        const char *f_type = lua_tostring(L, -1);\n        if (strcmp(f_type, \"stop\") == 0) {\n            p->key_filter = mcp_key_hash_filter_stop;\n        } else if (strcmp(f_type, \"tags\") == 0) {\n            p->key_filter = mcp_key_hash_filter_tag;\n        } else {\n            proxy_lua_ferror(L, \"unknown hash filter specified: %s\\n\", f_type);\n        }\n\n        lua_pop(L, 1); // pops \"filter\" value.\n\n        if (lua_getfield(L, 2, \"filter_conf\") == LUA_TSTRING) {\n            size_t len = 0;\n            const char *conf = lua_tolstring(L, -1, &len);\n            if (len < 2 || len > KEY_HASH_FILTER_MAX) {\n                proxy_lua_ferror(L, \"hash filter conf must be between 2 and %d characters\", KEY_HASH_FILTER_MAX);\n            }\n\n            memcpy(p->key_filter_conf, conf, len);\n            p->key_filter_conf[len+1] = '\\0';\n        } else {\n            proxy_lua_error(L, \"hash filter requires 'filter_conf' string\");\n        }\n        lua_pop(L, 1); // pops \"filter_conf\" value.\n    } else {\n        lua_pop(L, 1); // pop the nil.\n    }\n\n    if (lua_getfield(L, 2, \"hash\") != LUA_TNIL) {\n        luaL_checktype(L, -1, LUA_TLIGHTUSERDATA);\n        struct proxy_hash_func *phf = lua_touserdata(L, -1);\n        p->key_hasher = phf->func;\n        lua_pop(L, 1);\n    } else {\n        lua_pop(L, 1); // pop the nil.\n    }\n\n    if (lua_getfield(L, 2, \"seed\") != LUA_TNIL) {\n        luaL_checktype(L, -1, LUA_TSTRING);\n        size_t seedlen;\n        const char *seedstr = lua_tolstring(L, -1, &seedlen);\n        // Note: the custom hasher for a dist may be \"weird\" in some cases, so\n        // we use a standard hash method for the seed here.\n        // I'm open to changing this (ie; mcp.pool_seed_hasher = etc)\n        p->hash_seed = XXH3_64bits(seedstr, seedlen);\n\n        lua_pop(L, 1);\n    } else {\n        lua_pop(L, 1); // pop the nil.\n    }\n\n    if (p->phc.selector_func == NULL) {\n        proxy_lua_error(L, \"cannot create pool missing 'dist' argument\");\n    }\n\n    return 1;\n}\n\nstatic int mcplib_pool_proxy_gc(lua_State *L) {\n    mcp_pool_proxy_t *pp = luaL_checkudata(L, -1, \"mcp.pool_proxy\");\n    mcp_pool_t *p = pp->main;\n    pthread_mutex_lock(&p->g.lock);\n    p->g.refcount--;\n    if (p->g.refcount == 0) {\n        proxy_ctx_t *ctx = p->ctx;\n        pthread_mutex_lock(&ctx->manager_lock);\n        STAILQ_INSERT_TAIL(&ctx->manager_head, &p->g, next);\n        pthread_cond_signal(&ctx->manager_cond);\n        pthread_mutex_unlock(&ctx->manager_lock);\n    }\n    pthread_mutex_unlock(&p->g.lock);\n\n    return 0;\n}\n\nmcp_backend_t *mcplib_pool_proxy_call_helper(mcp_pool_proxy_t *pp, const char *key, size_t len) {\n    mcp_pool_t *p = pp->main;\n    if (p->key_filter) {\n        key = p->key_filter(p->key_filter_conf, key, len, &len);\n        P_DEBUG(\"%s: filtered key for hashing (%.*s)\\n\", __func__, (int)len, key);\n    }\n    uint64_t hash = p->key_hasher(key, len, p->hash_seed);\n    uint32_t lookup = p->phc.selector_func(hash, p->phc.ctx);\n\n    assert(p->phc.ctx != NULL);\n    if (lookup >= p->pool_size) {\n        return NULL;\n    }\n\n    return pp->pool[lookup].be;\n}\n\nstatic int mcplib_backend_use_iothread(lua_State *L) {\n    luaL_checktype(L, -1, LUA_TBOOLEAN);\n    int state = lua_toboolean(L, -1);\n    proxy_ctx_t *ctx = PROXY_GET_CTX(L);\n\n    STAT_L(ctx);\n    ctx->tunables.use_iothread = state;\n    STAT_UL(ctx);\n\n    return 0;\n}\n\nstatic int mcplib_backend_use_tls(lua_State *L) {\n    luaL_checktype(L, -1, LUA_TBOOLEAN);\n    int state = lua_toboolean(L, -1);\n    proxy_ctx_t *ctx = PROXY_GET_CTX(L);\n#ifndef PROXY_TLS\n    if (state == 1) {\n        proxy_lua_error(L, \"cannot set mcp.backend_use_tls: TLS support not compiled\");\n    }\n#endif\n    STAT_L(ctx);\n    ctx->tunables.use_tls = state;\n    STAT_UL(ctx);\n\n    return 0;\n}\n\n// TODO: error checking.\nstatic int mcplib_init_tls(lua_State *L) {\n#ifndef PROXY_TLS\n    proxy_lua_error(L, \"cannot run mcp.init_tls: TLS support not compiled\");\n#else\n    proxy_ctx_t *ctx = PROXY_GET_CTX(L);\n    mcp_tls_init(ctx);\n#endif\n\n    return 0;\n}\n\nstatic int mcplib_tcp_keepalive(lua_State *L) {\n    luaL_checktype(L, -1, LUA_TBOOLEAN);\n    int state = lua_toboolean(L, -1);\n    proxy_ctx_t *ctx = PROXY_GET_CTX(L);\n\n    STAT_L(ctx);\n    ctx->tunables.tcp_keepalive = state;\n    STAT_UL(ctx);\n\n    return 0;\n}\n\nstatic int mcplib_backend_failure_limit(lua_State *L) {\n    int limit = luaL_checkinteger(L, -1);\n    proxy_ctx_t *ctx = PROXY_GET_CTX(L);\n\n    if (limit < 0) {\n        proxy_lua_error(L, \"backend_failure_limit must be >= 0\");\n        return 0;\n    }\n\n    STAT_L(ctx);\n    ctx->tunables.backend_failure_limit = limit;\n    STAT_UL(ctx);\n\n    return 0;\n}\n\nstatic int mcplib_backend_depth_limit(lua_State *L) {\n    int limit = luaL_checkinteger(L, -1);\n    proxy_ctx_t *ctx = PROXY_GET_CTX(L);\n\n    if (limit < 0) {\n        proxy_lua_error(L, \"backend_depth_limit must be >= 0\");\n        return 0;\n    }\n\n    STAT_L(ctx);\n    ctx->tunables.backend_depth_limit = limit;\n    STAT_UL(ctx);\n\n    return 0;\n}\n\nstatic int mcplib_backend_connect_timeout(lua_State *L) {\n    lua_Number secondsf = luaL_checknumber(L, -1);\n    lua_Integer secondsi = (lua_Integer) secondsf;\n    lua_Number subseconds = secondsf - secondsi;\n    proxy_ctx_t *ctx = PROXY_GET_CTX(L);\n\n    STAT_L(ctx);\n    ctx->tunables.connect.tv_sec = secondsi;\n    ctx->tunables.connect.tv_usec = MICROSECONDS(subseconds);\n    STAT_UL(ctx);\n\n    return 0;\n}\n\nstatic int mcplib_backend_retry_waittime(lua_State *L) {\n    lua_Number secondsf = luaL_checknumber(L, -1);\n    proxy_ctx_t *ctx = PROXY_GET_CTX(L);\n    lua_Integer secondsi = _mcplib_backend_get_waittime(secondsf);\n\n    STAT_L(ctx);\n    ctx->tunables.retry.tv_sec = secondsi;\n    ctx->tunables.retry.tv_usec = 0;\n    STAT_UL(ctx);\n\n    return 0;\n}\n\n// TODO (v2): deprecation notice print when using this function.\nstatic int mcplib_backend_retry_timeout(lua_State *L) {\n    return mcplib_backend_retry_waittime(L);\n}\n\nstatic int mcplib_backend_read_timeout(lua_State *L) {\n    lua_Number secondsf = luaL_checknumber(L, -1);\n    lua_Integer secondsi = (lua_Integer) secondsf;\n    lua_Number subseconds = secondsf - secondsi;\n    proxy_ctx_t *ctx = PROXY_GET_CTX(L);\n\n    STAT_L(ctx);\n    ctx->tunables.read.tv_sec = secondsi;\n    ctx->tunables.read.tv_usec = MICROSECONDS(subseconds);\n    STAT_UL(ctx);\n\n    return 0;\n}\n\nstatic int mcplib_backend_flap_time(lua_State *L) {\n    lua_Number secondsf = luaL_checknumber(L, -1);\n    lua_Integer secondsi = (lua_Integer) secondsf;\n    lua_Number subseconds = secondsf - secondsi;\n    proxy_ctx_t *ctx = PROXY_GET_CTX(L);\n\n    STAT_L(ctx);\n    ctx->tunables.flap.tv_sec = secondsi;\n    ctx->tunables.flap.tv_usec = MICROSECONDS(subseconds);\n    STAT_UL(ctx);\n\n    return 0;\n}\n\nstatic int mcplib_backend_flap_backoff_ramp(lua_State *L) {\n    float factor = luaL_checknumber(L, -1);\n    proxy_ctx_t *ctx = PROXY_GET_CTX(L);\n    if (factor <= 1.1) {\n        factor = 1.1;\n    }\n\n    STAT_L(ctx);\n    ctx->tunables.flap_backoff_ramp = factor;\n    STAT_UL(ctx);\n\n    return 0;\n}\n\nstatic int mcplib_backend_flap_backoff_max(lua_State *L) {\n    luaL_checknumber(L, -1);\n    uint32_t max = lua_tointeger(L, -1);\n    proxy_ctx_t *ctx = PROXY_GET_CTX(L);\n\n    STAT_L(ctx);\n    ctx->tunables.flap_backoff_max = max;\n    STAT_UL(ctx);\n\n    return 0;\n}\n\nstatic int mcplib_luagc_ratio(lua_State *L) {\n    float ratio = luaL_checknumber(L, -1);\n    proxy_ctx_t *ctx = PROXY_GET_CTX(L);\n    if (ratio < 1.1) {\n        ratio = 1.1;\n    }\n\n    STAT_L(ctx);\n    ctx->tunables.gc_ratio = ratio;\n    STAT_UL(ctx);\n\n    return 0;\n}\n\nstatic int mcplib_stat_limit(lua_State *L) {\n    proxy_ctx_t *ctx = PROXY_GET_CTX(L);\n    int limit = luaL_checkinteger(L, -1);\n\n    if (limit == 0) {\n        limit = MAX_USTATS_DEFAULT;\n    }\n    if (limit > MAX_USTATS_DEFAULT) {\n        fprintf(stderr, \"PROXY WARNING: setting ustats limit above default may cause performance problems\\n\");\n    }\n\n    // lock isn't necessary as this is only used from the config thread.\n    // keeping the lock call for code consistency.\n    STAT_L(ctx);\n    ctx->tunables.max_ustats = limit;\n    STAT_UL(ctx);\n    return 0;\n}\n\nstatic int mcplib_active_req_limit(lua_State *L) {\n    proxy_ctx_t *ctx = PROXY_GET_CTX(L);\n    uint64_t limit = luaL_checkinteger(L, -1);\n\n    if (limit == 0) {\n        limit = UINT64_MAX;\n    } else {\n        // FIXME: global\n        int tcount = settings.num_threads;\n        // The actual limit is per-worker-thread, so divide it up.\n        if (limit > tcount * 2) {\n            limit /= tcount;\n        }\n    }\n\n    STAT_L(ctx);\n    ctx->active_req_limit = limit;\n    STAT_UL(ctx);\n\n    return 0;\n}\n\n// limit specified in kilobytes\nstatic int mcplib_buffer_memory_limit(lua_State *L) {\n    proxy_ctx_t *ctx = PROXY_GET_CTX(L);\n    uint64_t limit = luaL_checkinteger(L, -1);\n\n    if (limit == 0) {\n        limit = UINT64_MAX;\n    } else {\n        limit *= 1024;\n\n        int tcount = settings.num_threads;\n        if (limit > tcount * 2) {\n            limit /= tcount;\n        }\n    }\n    ctx->buffer_memory_limit = limit;\n\n    return 0;\n}\n\n// mcp.attach(mcp.HOOK_NAME, function)\n// fill hook structure: if lua function, use luaL_ref() to store the func\nstatic int mcplib_attach(lua_State *L) {\n    // Pull the original worker thread out of the shared mcplib upvalue.\n    LIBEVENT_THREAD *t = PROXY_GET_THR(L);\n\n    int hook = luaL_checkinteger(L, 1);\n    // pushvalue to dupe func and etc.\n    // can leave original func on stack afterward because it'll get cleared.\n    int loop_end = 0;\n    int loop_start = 1;\n    if (hook == CMD_ANY) {\n        // if CMD_ANY we need individually set loop 1 to CMD_SIZE.\n        loop_end = CMD_SIZE;\n    } else if (hook == CMD_ANY_STORAGE) {\n        // if CMD_ANY_STORAGE we only override get/set/etc.\n        loop_end = CMD_END_STORAGE;\n    } else {\n        loop_start = hook;\n        loop_end = hook + 1;\n    }\n\n    mcp_funcgen_t *fgen = NULL;\n    if (lua_isfunction(L, 2)) {\n        // create a funcgen with null generator that calls this function\n        lua_pushvalue(L, 2); // function must be at top of stack.\n        mcplib_funcgenbare_new(L); // convert it into a function generator.\n        fgen = luaL_checkudata(L, -1, \"mcp.funcgen\"); // set our pointer ref.\n        lua_replace(L, 2); // move the function generator over the input\n                           // function. necessary for alignment with the rest\n                           // of the code.\n        lua_pop(L, 1); // drop the extra generator function reference.\n    } else if ((fgen = luaL_testudata(L, 2, \"mcp.funcgen\")) != NULL) {\n        // good\n    } else {\n        proxy_lua_error(L, \"mcp.attach: must pass a function\");\n        return 0;\n    }\n\n    if (fgen->closed) {\n        proxy_lua_error(L, \"mcp.attach: cannot use a previously replaced function\");\n        return 0;\n    }\n\n    {\n        struct proxy_hook *hooks = t->proxy_hooks;\n        uint64_t tag = 0; // listener socket tag\n\n        if (lua_isstring(L, 3)) {\n            size_t len;\n            const char *stag = lua_tolstring(L, 3, &len);\n            if (len < 1 || len > 8) {\n                proxy_lua_error(L, \"mcp.attach: tag must be 1 to 8 characters\");\n                return 0;\n            }\n            memcpy(&tag, stag, len);\n        }\n\n        for (int x = loop_start; x < loop_end; x++) {\n            struct proxy_hook *h = &hooks[x];\n            if (x == CMD_MN) {\n                // disallow overriding MN so client pipeline flushes work.\n                // need to add flush support before allowing override\n                continue;\n            }\n            lua_pushvalue(L, 2); // duplicate the ref.\n            struct proxy_hook_ref *href = &h->ref;\n\n            if (tag) {\n                // listener was tagged. use the extended hook structure.\n                struct proxy_hook_tagged *pht = h->tagged;\n\n                if (h->tagcount == 0) {\n                    pht = calloc(1, sizeof(struct proxy_hook_tagged));\n                    if (pht == NULL) {\n                        proxy_lua_error(L, \"mcp.attach: failure allocating tagged hooks\");\n                        return 0;\n                    }\n                    h->tagcount = 1;\n                    h->tagged = pht;\n                }\n\n                bool found = false;\n                for (int x = 0; x < h->tagcount; x++) {\n                    if (pht->tag == tag || pht->tag == 0) {\n                        found = true;\n                        break;\n                    }\n                    pht++;\n                }\n\n                // need to resize the array to fit the new tag.\n                if (!found) {\n                    struct proxy_hook_tagged *temp = realloc(h->tagged, sizeof(struct proxy_hook_tagged) * (h->tagcount+1));\n                    if (!temp) {\n                        proxy_lua_error(L, \"mcp.attach: failure to resize tagged hooks\");\n                        return 0;\n                    }\n                    pht = &temp[h->tagcount];\n                    memset(pht, 0, sizeof(*pht));\n                    h->tagcount++;\n                    h->tagged = temp;\n                }\n\n                href = &pht->ref;\n                pht->tag = tag;\n            }\n\n            // now assign our hook reference.\n            if (href->lua_ref) {\n                // Found existing tagged hook.\n                luaL_unref(L, LUA_REGISTRYINDEX, href->lua_ref);\n                mcp_funcgen_dereference(L, href->ctx);\n            }\n\n            lua_pushvalue(L, -1); // duplicate the funcgen\n            mcp_funcgen_reference(L);\n            href->lua_ref = luaL_ref(L, LUA_REGISTRYINDEX);\n            href->ctx = fgen;\n            assert(href->lua_ref != 0);\n        }\n    }\n\n    return 0;\n}\n\n/*** START lua interface to logger ***/\n\n// user logger specific to the config thread\nstatic int mcplib_ct_log(lua_State *L) {\n    const char *msg = luaL_checkstring(L, -1);\n    // The only difference is we pull the logger from thread local storage.\n    LOGGER_LOG(NULL, LOG_PROXYUSER, LOGGER_PROXY_USER, NULL, msg);\n    return 0;\n}\n\nstatic int mcplib_log(lua_State *L) {\n    LIBEVENT_THREAD *t = PROXY_GET_THR(L);\n    const char *msg = luaL_checkstring(L, -1);\n    LOGGER_LOG(t->l, LOG_PROXYUSER, LOGGER_PROXY_USER, NULL, msg);\n    return 0;\n}\n\n// (request, resp, \"detail\")\nstatic int mcplib_log_req(lua_State *L) {\n    LIBEVENT_THREAD *t = PROXY_GET_THR(L);\n    logger *l = t->l;\n    // Not using the LOGGER_LOG macro so we can avoid as much overhead as\n    // possible when logging is disabled.\n    if (! (l->eflags & LOG_PROXYREQS)) {\n        return 0;\n    }\n    int rtype = 0;\n    int rcode = 0;\n    int rstatus = 0;\n    long elapsed = 0;\n    char *rname = NULL;\n    char *rport = NULL;\n\n    mcp_request_t *rq = luaL_checkudata(L, 1, \"mcp.request\");\n    int type = lua_type(L, 2);\n    if (type == LUA_TUSERDATA) {\n        mcp_resp_t *rs = luaL_checkudata(L, 2, \"mcp.response\");\n        rtype = rs->resp.type;\n        rcode = rs->resp.code;\n        rstatus = rs->status;\n        rname = rs->be_name;\n        rport = rs->be_port;\n        elapsed = rs->elapsed;\n    }\n    size_t dlen = 0;\n    const char *detail = luaL_optlstring(L, 3, NULL, &dlen);\n    int cfd = luaL_optinteger(L, 4, 0);\n\n    logger_log(l, LOGGER_PROXY_REQ, NULL, rq->pr.request, rq->pr.reqlen, elapsed, rtype, rcode, rstatus, cfd, detail, dlen, rname, rport);\n\n    return 0;\n}\n\nstatic inline uint32_t _mcp_rotl(const uint32_t x, int k) {\n    return (x << k) | (x >> (32 - k));\n}\n\n// xoroshiro128++ 32bit version.\nstatic uint32_t _mcp_nextrand(uint32_t *s) {\n    const uint32_t result = _mcp_rotl(s[0] + s[3], 7) + s[0];\n\n    const uint32_t t = s[1] << 9;\n\n    s[2] ^= s[0];\n    s[3] ^= s[1];\n    s[1] ^= s[2];\n    s[0] ^= s[3];\n\n    s[2] ^= t;\n\n    s[3] = _mcp_rotl(s[3], 11);\n\n    return result;\n}\n\n\n// (milliseconds, sample_rate, allerrors, request, resp, \"detail\")\nstatic int mcplib_log_reqsample(lua_State *L) {\n    LIBEVENT_THREAD *t = PROXY_GET_THR(L);\n    logger *l = t->l;\n    // Not using the LOGGER_LOG macro so we can avoid as much overhead as\n    // possible when logging is disabled.\n    if (! (l->eflags & LOG_PROXYREQS)) {\n        return 0;\n    }\n    int rtype = 0;\n    int rcode = 0;\n    int rstatus = 0;\n    long elapsed = 0;\n    char *rname = NULL;\n    char *rport = NULL;\n\n    int ms = luaL_checkinteger(L, 1);\n    int rate = luaL_checkinteger(L, 2);\n    int allerr = lua_toboolean(L, 3);\n    mcp_request_t *rq = luaL_checkudata(L, 4, \"mcp.request\");\n    int type = lua_type(L, 5);\n    if (type == LUA_TUSERDATA) {\n        mcp_resp_t *rs = luaL_checkudata(L, 5, \"mcp.response\");\n        rtype = rs->resp.type;\n        rcode = rs->resp.code;\n        rstatus = rs->status;\n        rname = rs->be_name;\n        rport = rs->be_port;\n        elapsed = rs->elapsed;\n    }\n    size_t dlen = 0;\n    const char *detail = luaL_optlstring(L, 6, NULL, &dlen);\n    int cfd = luaL_optinteger(L, 7, 0);\n\n    bool do_log = false;\n    if (allerr && rstatus != MCMC_OK) {\n        do_log = true;\n    } else if (ms > 0 && elapsed > ms * 1000) {\n        do_log = true;\n    } else if (rate > 0) {\n        // slightly biased random-to-rate without adding a loop, which is\n        // completely fine for this use case.\n        uint32_t rnd = (uint64_t)_mcp_nextrand(t->proxy_rng) * (uint64_t)rate >> 32;\n        if (rnd == 0) {\n            do_log = true;\n        }\n    }\n\n    if (do_log) {\n        logger_log(l, LOGGER_PROXY_REQ, NULL, rq->pr.request, rq->pr.reqlen, elapsed, rtype, rcode, rstatus, cfd, detail, dlen, rname, rport);\n    }\n\n    return 0;\n}\n\n// TODO: slowsample\n// _err versions?\n\n/*** END lua interface to logger ***/\n\nstatic void proxy_register_defines(lua_State *L) {\n#define X(x) \\\n    lua_pushinteger(L, x); \\\n    lua_setfield(L, -2, #x);\n#define Y(x, l) \\\n    lua_pushinteger(L, x); \\\n    lua_setfield(L, -2, l);\n\n    X(MCMC_CODE_STORED);\n    X(MCMC_CODE_EXISTS);\n    X(MCMC_CODE_DELETED);\n    X(MCMC_CODE_TOUCHED);\n    X(MCMC_CODE_VERSION);\n    X(MCMC_CODE_NOT_FOUND);\n    X(MCMC_CODE_NOT_STORED);\n    X(MCMC_CODE_OK);\n    X(MCMC_CODE_NOP);\n    X(MCMC_CODE_END);\n    X(MCMC_CODE_ERROR);\n    X(MCMC_CODE_CLIENT_ERROR);\n    X(MCMC_CODE_SERVER_ERROR);\n    X(MCMC_ERR);\n    X(P_OK);\n    X(CMD_ANY);\n    X(CMD_ANY_STORAGE);\n    Y(QWAIT_ANY, \"WAIT_ANY\");\n    Y(QWAIT_OK, \"WAIT_OK\");\n    Y(QWAIT_GOOD, \"WAIT_GOOD\");\n    Y(QWAIT_FASTGOOD, \"WAIT_FASTGOOD\");\n    Y(RQUEUE_R_GOOD, \"RES_GOOD\");\n    Y(RQUEUE_R_OK, \"RES_OK\");\n    Y(RQUEUE_R_ANY, \"RES_ANY\");\n    CMD_FIELDS\n#undef X\n#undef Y\n\n    lua_pushboolean(L, 1);\n    lua_setfield(L, -2, \"WAIT_RESUME\");\n}\n\n// TODO: low priority malloc error handling.\nstatic void proxy_register_startarg(lua_State *L) {\n    int idx = lua_absindex(L, -1); // remember 'mcp' table.\n    if (settings.proxy_startarg == NULL) {\n        // no argument given.\n        lua_pushboolean(L, 0);\n        lua_setfield(L, idx, \"start_arg\");\n        return;\n    }\n\n    char *sarg = strdup(settings.proxy_startarg);\n    if (strchr(sarg, ':') == NULL) {\n        // just upload the string\n        lua_pushstring(L, sarg);\n    } else {\n        // split into a table and set that instead.\n        lua_newtable(L);\n        int nidx = lua_absindex(L, -1);\n        char *b = NULL;\n        for (char *p = strtok_r(sarg, \":\", &b);\n                p != NULL;\n                p = strtok_r(NULL, \":\", &b)) {\n            char *e = NULL;\n            char *name = strtok_r(p, \"_\", &e);\n            lua_pushstring(L, name); // table -> key\n            char *value = strtok_r(NULL, \"_\", &e);\n            if (value == NULL) {\n                lua_pushboolean(L, 1); // table -> key -> True\n            } else {\n                lua_pushstring(L, value); // table -> key -> value\n            }\n            lua_settable(L, nidx);\n        }\n    }\n    free(sarg);\n    lua_setfield(L, idx, \"start_arg\");\n}\n\n// Creates and returns the top level \"mcp\" module\nint proxy_register_libs(void *ctx, LIBEVENT_THREAD *t, void *state) {\n    lua_State *L = state;\n\n    const struct luaL_Reg mcplib_backend_m[] = {\n        {\"__gc\", mcplib_backend_gc},\n        {NULL, NULL}\n    };\n\n    const struct luaL_Reg mcplib_backend_wrap_m[] = {\n        {\"__gc\", mcplib_backend_wrap_gc},\n        {NULL, NULL}\n    };\n\n    const struct luaL_Reg mcplib_request_m[] = {\n        {\"command\", mcplib_request_command},\n        {\"key\", mcplib_request_key},\n        {\"ltrimkey\", mcplib_request_ltrimkey},\n        {\"rtrimkey\", mcplib_request_rtrimkey},\n        {\"token\", mcplib_request_token},\n        {\"token_int\", mcplib_request_token_int},\n        {\"ntokens\", mcplib_request_ntokens},\n        {\"has_flag\", mcplib_request_has_flag},\n        {\"flag_token\", mcplib_request_flag_token},\n        {\"flag_token_int\", mcplib_request_flag_token_int},\n        {\"flag_add\", mcplib_request_flag_add},\n        {\"flag_set\", mcplib_request_flag_set},\n        {\"flag_replace\", mcplib_request_flag_replace},\n        {\"flag_del\", mcplib_request_flag_del},\n        {\"match_res\", mcplib_request_match_res},\n        {\"__tostring\", NULL},\n        {\"__gc\", mcplib_request_gc},\n        {NULL, NULL}\n    };\n\n    const struct luaL_Reg mcplib_response_m[] = {\n        {\"ok\", mcplib_response_ok},\n        {\"hit\", mcplib_response_hit},\n        {\"vlen\", mcplib_response_vlen},\n        {\"code\", mcplib_response_code},\n        {\"line\", mcplib_response_line},\n        {\"flag_blank\", mcplib_response_flag_blank},\n        {\"elapsed\", mcplib_response_elapsed},\n        {\"__gc\", mcplib_response_gc},\n        {\"__close\", mcplib_response_close},\n        {\"close\", mcplib_response_close},\n        {NULL, NULL}\n    };\n\n    const struct luaL_Reg mcplib_pool_m[] = {\n        {\"__gc\", mcplib_pool_gc},\n        {NULL, NULL}\n    };\n\n    const struct luaL_Reg mcplib_pool_proxy_m[] = {\n        {\"__gc\", mcplib_pool_proxy_gc},\n        {NULL, NULL}\n    };\n\n    const struct luaL_Reg mcplib_ratelim_tbf_m[] = {\n        {\"__call\", mcplib_ratelim_tbf_call},\n        {NULL, NULL}\n    };\n\n    const struct luaL_Reg mcplib_ratelim_global_tbf_m[] = {\n        {\"__gc\", mcplib_ratelim_global_tbf_gc},\n        {NULL, NULL}\n    };\n\n    const struct luaL_Reg mcplib_ratelim_proxy_tbf_m[] = {\n        {\"__call\", mcplib_ratelim_proxy_tbf_call},\n        {\"__gc\", mcplib_ratelim_proxy_tbf_gc},\n        {NULL, NULL}\n    };\n\n    const struct luaL_Reg mcplib_rcontext_m[] = {\n        {\"handle_set_cb\", mcplib_rcontext_handle_set_cb},\n        {\"enqueue\", mcplib_rcontext_enqueue},\n        {\"wait_cond\", mcplib_rcontext_wait_cond},\n        {\"enqueue_and_wait\", mcplib_rcontext_enqueue_and_wait},\n        {\"wait_handle\", mcplib_rcontext_wait_handle},\n        {\"res_good\", mcplib_rcontext_res_good},\n        {\"res_ok\", mcplib_rcontext_res_ok},\n        {\"res_any\", mcplib_rcontext_res_any},\n        {\"result\", mcplib_rcontext_result},\n        {\"cfd\", mcplib_rcontext_cfd},\n        {\"tls_peer_cn\", mcplib_rcontext_tls_peer_cn},\n        {\"request_new\", mcplib_rcontext_request_new},\n        {\"response_new\", mcplib_rcontext_response_new},\n        //{\"sleep\", mcplib_rcontext_sleep}, see comments on function\n        {NULL, NULL}\n    };\n\n    const struct luaL_Reg mcplib_funcgen_m[] = {\n        {\"__gc\", mcplib_funcgen_gc},\n        {\"new_handle\", mcplib_funcgen_new_handle},\n        {\"ready\", mcplib_funcgen_ready},\n        {NULL, NULL}\n    };\n\n    const struct luaL_Reg mcplib_inspector_m[] = {\n        {\"__gc\", mcplib_inspector_gc},\n        {\"__call\", mcplib_inspector_call},\n        {NULL, NULL},\n    };\n\n    const struct luaL_Reg mcplib_mutator_m[] = {\n        {\"__gc\", mcplib_mutator_gc},\n        {\"__call\", mcplib_mutator_call},\n        {NULL, NULL},\n    };\n\n    const struct luaL_Reg mcplib_f_config [] = {\n        {\"pool\", mcplib_pool},\n        {\"backend\", mcplib_backend},\n        {\"add_stat\", mcplib_add_stat},\n        {\"ratelim_global_tbf\", mcplib_ratelim_global_tbf},\n        {\"luagc_ratio\", mcplib_luagc_ratio},\n        {\"stat_limit\", mcplib_stat_limit},\n        {\"backend_connect_timeout\", mcplib_backend_connect_timeout},\n        {\"backend_retry_timeout\", mcplib_backend_retry_timeout},\n        {\"backend_retry_waittime\", mcplib_backend_retry_waittime},\n        {\"backend_read_timeout\", mcplib_backend_read_timeout},\n        {\"backend_failure_limit\", mcplib_backend_failure_limit},\n        {\"backend_depth_limit\", mcplib_backend_depth_limit},\n        {\"backend_flap_time\", mcplib_backend_flap_time},\n        {\"backend_flap_backoff_ramp\", mcplib_backend_flap_backoff_ramp},\n        {\"backend_flap_backoff_max\", mcplib_backend_flap_backoff_max},\n        {\"backend_use_iothread\", mcplib_backend_use_iothread},\n        {\"backend_use_tls\", mcplib_backend_use_tls},\n        {\"init_tls\", mcplib_init_tls},\n        {\"tcp_keepalive\", mcplib_tcp_keepalive},\n        {\"active_req_limit\", mcplib_active_req_limit},\n        {\"buffer_memory_limit\", mcplib_buffer_memory_limit},\n        {\"schedule_config_reload\", mcplib_schedule_config_reload},\n        {\"register_cron\", mcplib_register_cron},\n        {\"server_stats\", mcplib_server_stats},\n        {\"log\", mcplib_ct_log},\n        {NULL, NULL}\n    };\n\n    const struct luaL_Reg mcplib_f_routes [] = {\n        {\"internal\", mcplib_internal},\n        {\"attach\", mcplib_attach},\n        {\"funcgen_new\", mcplib_funcgen_new},\n        {\"router_new\", mcplib_router_new},\n        {\"log\", mcplib_log},\n        {\"log_req\", mcplib_log_req},\n        {\"log_reqsample\", mcplib_log_reqsample},\n        {\"stat\", mcplib_stat},\n        {\"request\", mcplib_request},\n        {\"ratelim_tbf\", mcplib_ratelim_tbf},\n        {\"req_inspector_new\", mcplib_req_inspector_new},\n        {\"res_inspector_new\", mcplib_res_inspector_new},\n        {\"req_mutator_new\", mcplib_req_mutator_new},\n        {\"res_mutator_new\", mcplib_res_mutator_new},\n        {\"time_real_millis\", mcplib_time_real_millis},\n        {\"time_mono_millis\", mcplib_time_mono_millis},\n        {NULL, NULL}\n    };\n    // VM's have void* extra space in the VM by default for fast-access to a\n    // context pointer like this. In some cases upvalues are inaccessible (ie;\n    // GC's) but we still need access to the proxy global context.\n    void **extra = lua_getextraspace(L);\n\n    if (t != NULL) {\n        // If thread VM, extra is the libevent thread\n        *extra = t;\n        luaL_newmetatable(L, \"mcp.request\");\n        lua_pushvalue(L, -1); // duplicate metatable.\n        lua_setfield(L, -2, \"__index\"); // mt.__index = mt\n        luaL_setfuncs(L, mcplib_request_m, 0); // register methods\n        lua_pop(L, 1);\n\n        luaL_newmetatable(L, \"mcp.response\");\n        lua_pushvalue(L, -1); // duplicate metatable.\n        lua_setfield(L, -2, \"__index\"); // mt.__index = mt\n        luaL_setfuncs(L, mcplib_response_m, 0); // register methods\n        lua_pop(L, 1);\n\n        luaL_newmetatable(L, \"mcp.pool_proxy\");\n        lua_pushvalue(L, -1); // duplicate metatable.\n        lua_setfield(L, -2, \"__index\"); // mt.__index = mt\n        luaL_setfuncs(L, mcplib_pool_proxy_m, 0); // register methods\n        lua_pop(L, 1); // drop the hash selector metatable\n\n        luaL_newmetatable(L, \"mcp.ratelim_tbf\");\n        lua_pushvalue(L, -1); // duplicate metatable.\n        lua_setfield(L, -2, \"__index\"); // mt.__index = mt\n        luaL_setfuncs(L, mcplib_ratelim_tbf_m, 0); // register methods\n        lua_pop(L, 1);\n\n        luaL_newmetatable(L, \"mcp.ratelim_proxy_tbf\");\n        lua_pushvalue(L, -1); // duplicate metatable.\n        lua_setfield(L, -2, \"__index\"); // mt.__index = mt\n        luaL_setfuncs(L, mcplib_ratelim_proxy_tbf_m, 0); // register methods\n        lua_pop(L, 1);\n\n        luaL_newmetatable(L, \"mcp.inspector\");\n        lua_pushvalue(L, -1); // duplicate metatable.\n        lua_setfield(L, -2, \"__index\"); // mt.__index = mt\n        luaL_setfuncs(L, mcplib_inspector_m, 0); // register methods\n        lua_pop(L, 1);\n\n        luaL_newmetatable(L, \"mcp.mutator\");\n        lua_pushvalue(L, -1); // duplicate metatable.\n        lua_setfield(L, -2, \"__index\"); // mt.__index = mt\n        luaL_setfuncs(L, mcplib_mutator_m, 0); // register methods\n        lua_pop(L, 1);\n\n        luaL_newmetatable(L, \"mcp.rcontext\");\n        lua_pushvalue(L, -1); // duplicate metatable.\n        lua_setfield(L, -2, \"__index\"); // mt.__index = mt\n        luaL_setfuncs(L, mcplib_rcontext_m, 0); // register methods\n        lua_pop(L, 1);\n\n        luaL_newmetatable(L, \"mcp.funcgen\");\n        lua_pushvalue(L, -1); // duplicate metatable.\n        lua_setfield(L, -2, \"__index\"); // mt.__index = mt\n        luaL_setfuncs(L, mcplib_funcgen_m, 0); // register methods\n        lua_pop(L, 1);\n\n        // marks a special C-compatible route function.\n        luaL_newmetatable(L, \"mcp.rfunc\");\n        lua_pop(L, 1);\n\n        // function generator userdata.\n        luaL_newmetatable(L, \"mcp.funcgen\");\n        lua_pop(L, 1);\n\n        luaL_newlibtable(L, mcplib_f_routes);\n    } else {\n        // Change the extra space override for the configuration VM to just point\n        // straight to ctx.\n        *extra = ctx;\n\n        luaL_newmetatable(L, \"mcp.backend\");\n        lua_pushvalue(L, -1); // duplicate metatable.\n        lua_setfield(L, -2, \"__index\"); // mt.__index = mt\n        luaL_setfuncs(L, mcplib_backend_m, 0); // register methods\n        lua_pop(L, 1);\n\n        luaL_newmetatable(L, \"mcp.backendwrap\");\n        lua_pushvalue(L, -1); // duplicate metatable.\n        lua_setfield(L, -2, \"__index\"); // mt.__index = mt\n        luaL_setfuncs(L, mcplib_backend_wrap_m, 0); // register methods\n        lua_pop(L, 1);\n\n        luaL_newmetatable(L, \"mcp.pool\");\n        lua_pushvalue(L, -1); // duplicate metatable.\n        lua_setfield(L, -2, \"__index\"); // mt.__index = mt\n        luaL_setfuncs(L, mcplib_pool_m, 0); // register methods\n        lua_pop(L, 1); // drop the hash selector metatable\n\n        luaL_newmetatable(L, \"mcp.ratelim_global_tbf\");\n        lua_pushvalue(L, -1); // duplicate metatable.\n        lua_setfield(L, -2, \"__index\"); // mt.__index = mt\n        luaL_setfuncs(L, mcplib_ratelim_global_tbf_m, 0); // register methods\n        lua_pop(L, 1);\n\n        luaL_newlibtable(L, mcplib_f_config);\n    }\n\n    // create main library table.\n    //luaL_newlib(L, mcplib_f);\n    // TODO (v2): luaL_newlibtable() just pre-allocs the exact number of things\n    // here.\n    // can replace with createtable and add the num. of the constant\n    // definitions.\n    proxy_register_defines(L);\n\n    mcplib_open_hash_xxhash(L);\n    lua_setfield(L, -2, \"hash_xxhash\");\n    // hash function for selectors.\n    // have to wrap the function in a struct because function pointers aren't\n    // pointer pointers :)\n    mcplib_open_dist_jump_hash(L);\n    lua_setfield(L, -2, \"dist_jump_hash\");\n    mcplib_open_dist_ring_hash(L);\n    lua_setfield(L, -2, \"dist_ring_hash\");\n\n    // create weak table for storing backends by label.\n    lua_newtable(L); // {}\n    lua_newtable(L); // {}, {} for metatable\n    lua_pushstring(L, \"v\"); // {}, {}, \"v\" for weak values.\n    lua_setfield(L, -2, \"__mode\"); // {}, {__mode = \"v\"}\n    lua_setmetatable(L, -2); // {__mt = {__mode = \"v\"} }\n\n    if (t != NULL) {\n        luaL_setfuncs(L, mcplib_f_routes, 1); // store upvalues.\n    } else {\n        luaL_setfuncs(L, mcplib_f_config, 1); // store upvalues.\n    }\n\n    // every VM gets a copy of the start arguments to work with.\n    proxy_register_startarg(L);\n\n    lua_setglobal(L, \"mcp\"); // set the lib table to mcp global.\n    return 1;\n}\n"
        },
        {
          "name": "proxy_luafgen.c",
          "type": "blob",
          "size": 69.5029296875,
          "content": "/* -*- Mode: C; tab-width: 4; c-basic-offset: 4; indent-tabs-mode: nil -*- */\n\n#include \"proxy.h\"\n#ifdef TLS\n#include \"tls.h\"\n#endif\n\nstatic mcp_funcgen_t *mcp_funcgen_route(lua_State *L, mcp_funcgen_t *fgen, mcp_parser_t *pr);\nstatic int mcp_funcgen_router_cleanup(lua_State *L, mcp_funcgen_t *fgen);\nstatic void _mcplib_funcgen_cache(mcp_funcgen_t *fgen, mcp_rcontext_t *rctx);\nstatic void mcp_funcgen_cleanup(lua_State *L, mcp_funcgen_t *fgen);\nstatic void mcp_resume_rctx_from_cb(mcp_rcontext_t *rctx);\nstatic void proxy_return_rqu_cb(io_pending_t *pending);\n\nstatic inline void _mcp_queue_hack(conn *c) {\n    if (c) {\n        // HACK\n        // see notes above proxy_run_rcontext.\n        // in case the above resume calls queued new work, we have to submit\n        // it to the backend handling system here.\n        for (io_queue_t *q = c->io_queues; q->type != IO_QUEUE_NONE; q++) {\n            if (q->stack_ctx != NULL) {\n                io_queue_cb_t *qcb = thread_io_queue_get(c->thread, q->type);\n                qcb->submit_cb(q);\n            }\n        }\n    }\n}\n\n// If we're GC'ed but not closed, it means it was created but never\n// attached to a function, so ensure everything is closed properly.\nint mcplib_funcgen_gc(lua_State *L) {\n    mcp_funcgen_t *fgen = luaL_checkudata(L, -1, \"mcp.funcgen\");\n    if (fgen->closed) {\n        return 0;\n    }\n    assert(fgen->self_ref == 0);\n\n    mcp_funcgen_cleanup(L, fgen);\n    fgen->closed = true;\n    return 0;\n}\n\n// handler for *_wait_*() variants and sleep calls\nstatic void mcp_funcgen_wait_handler(const int fd, const short which, void *arg) {\n    mcp_rcontext_t *rctx = arg;\n\n    // if we were in waiting: reset wait mode, push wait_done + boolean true\n    // if we were in sleep: reset wait mode.\n    // immediately resume.\n    lua_settop(rctx->Lc, 0);\n    rctx->wait_count = 0;\n    rctx->lua_narg = 2;\n    if (rctx->wait_mode == QWAIT_HANDLE) {\n        // if timed out then we shouldn't have a result. just push nil.\n        lua_pushnil(rctx->Lc);\n    } else if (rctx->wait_mode == QWAIT_SLEEP) {\n        // no extra arg.\n        rctx->lua_narg = 1;\n    } else {\n        // how many results were processed\n        lua_pushinteger(rctx->Lc, rctx->wait_done);\n    }\n    // \"timed out\"\n    lua_pushboolean(rctx->Lc, 1);\n\n    rctx->wait_mode = QWAIT_IDLE;\n\n    mcp_resume_rctx_from_cb(rctx);\n}\n\n// For describing functions which generate functions which can execute\n// requests.\n// These \"generator functions\" handle pre-allocating and creating a memory\n// heirarchy, allowing dynamic runtimes at high speed.\n\n// must be called with fgen on top of stack in fgen->thread->L\nstatic void mcp_rcontext_cleanup(lua_State *L, mcp_funcgen_t *fgen, mcp_rcontext_t *rctx, int fgen_idx) {\n    luaL_unref(L, LUA_REGISTRYINDEX, rctx->coroutine_ref);\n    luaL_unref(L, LUA_REGISTRYINDEX, rctx->function_ref);\n    if (rctx->request_ref) {\n        luaL_unref(L, LUA_REGISTRYINDEX, rctx->request_ref);\n    }\n    assert(rctx->pending_reqs == 0);\n\n    // cleanup of request queue entries. recurse funcgen cleanup.\n    for (int x = 0; x < fgen->max_queues; x++) {\n        struct mcp_rqueue_s *rqu = &rctx->qslots[x];\n        if (rqu->obj_type == RQUEUE_TYPE_POOL) {\n            // nothing to do.\n        } else if (rqu->obj_type == RQUEUE_TYPE_FGEN) {\n            // don't need to recurse, just free the subrctx.\n            mcp_rcontext_t *subrctx = rqu->obj;\n            lua_rawgeti(L, LUA_REGISTRYINDEX, subrctx->fgen->self_ref);\n            mcp_rcontext_cleanup(L, subrctx->fgen, subrctx, lua_absindex(L, -1));\n            lua_pop(L, 1); // drop subrctx fgen\n        } else if (rqu->obj_type != RQUEUE_TYPE_NONE) {\n            assert(1 == 0);\n        }\n\n        if (rqu->res_ref) {\n            luaL_unref(L, LUA_REGISTRYINDEX, rqu->res_ref);\n            rqu->res_ref = 0;\n        }\n\n        if (rqu->cb_ref) {\n            luaL_unref(L, LUA_REGISTRYINDEX, rqu->cb_ref);\n            rqu->cb_ref = 0;\n        }\n    }\n\n    // look for rctx-local objects.\n    if (rctx->uobj_count) {\n        int lim = fgen->max_queues + rctx->uobj_count;\n        for (int x = fgen->max_queues; x < lim; x++) {\n            struct mcp_rqueue_s *rqu = &rctx->qslots[x];\n            // Don't need to look at the type:\n            // - slot has to be freed (thus cleaned up) before getting here\n            // - any uobj is ref'ed into obj_ref\n            luaL_unref(L, LUA_REGISTRYINDEX, rqu->obj_ref);\n            rqu->obj_ref = 0;\n        }\n    }\n\n    // nuke alarm if set.\n    // should only be paranoia here, but just in case.\n    if (event_pending(&rctx->timeout_event, EV_TIMEOUT, NULL)) {\n        event_del(&rctx->timeout_event);\n    }\n\n    lua_getiuservalue(L, fgen_idx, 1);\n    luaL_unref(L, -1, rctx->self_ref);\n    rctx->self_ref = 0;\n    lua_pop(L, 1); // drop freelist table\n\n    fgen->total--;\n    LIBEVENT_THREAD *t = PROXY_GET_THR(L);\n    // Fake an allocation when we free slots as they are long running data.\n    // This tricks the GC into running and freeing them.\n    t->proxy_vm_extra_kb += 2;\n    mcp_sharedvm_delta(t->proxy_ctx, SHAREDVM_FGENSLOT_IDX, fgen->name, -1);\n}\n\n// TODO: switch from an array to a STAILQ so we can avoid the memory\n// management and error handling.\n// Realistically it's impossible for these to error so we're safe for now.\n#ifdef MEMCACHED_DEBUG\n// require fewer test rounds for unit tests.\n#define FGEN_FREE_PRESSURE_MAX 100\n#define FGEN_FREE_PRESSURE_DROP 10\n#define FGEN_FREE_WAIT 0\n#else\n#define FGEN_FREE_PRESSURE_MAX 5000\n#define FGEN_FREE_PRESSURE_DROP 200\n#define FGEN_FREE_WAIT 60 // seconds.\n#endif\nstatic void _mcplib_funcgen_cache(mcp_funcgen_t *fgen, mcp_rcontext_t *rctx) {\n    bool do_cache = true;\n    // Easing algorithm to decide when to \"early free\" rctx slots:\n    // - If we recently allocated a slot, reset pressure.\n    // - Each time an rctx is freed and more than half of available rctx's are\n    // free, increase pressure.\n    // - If free rctx are less than half of total, reduce pressure.\n    // - If pressure is too high, immediately free the rctx, then drop the\n    // pressure slightly.\n    // - If pressure is too high, and has been for more than FGEN_FREE_WAIT\n    // seconds, immediately free the rctx, then drop the pressure slightly.\n    //\n    // This should allow bursty traffic to avoid spinning on alloc/frees,\n    // while one-time bursts will slowly free slots back down to a min of 1.\n    if (fgen->free > fgen->total/2 - 1) {\n        if (fgen->free_pressure++ > FGEN_FREE_PRESSURE_MAX) {\n            struct timespec now;\n            clock_gettime(CLOCK_REALTIME, &now);\n            if (fgen->free_waiter.tv_sec == 0) {\n                fgen->free_waiter.tv_sec = now.tv_sec + FGEN_FREE_WAIT;\n            }\n\n            if (now.tv_sec >= fgen->free_waiter.tv_sec) {\n                do_cache = false;\n            }\n            // check again in a little while.\n            fgen->free_pressure -= FGEN_FREE_PRESSURE_DROP;\n        }\n    } else {\n        fgen->free_pressure >>= 1;\n        // must be too-free for a full wait period before releasing.\n        fgen->free_waiter.tv_sec = 0;\n    }\n\n    if (do_cache) {\n        if (fgen->free + 1 >= fgen->free_max) {\n            int x = fgen->free_max;\n            fgen->free_max *= 2;\n            fgen->list = realloc(fgen->list, fgen->free_max * sizeof(mcp_rcontext_t *));\n            for (; x < fgen->free_max; x++) {\n                fgen->list[x] = NULL;\n            }\n        }\n        fgen->list[fgen->free] = rctx;\n        fgen->free++;\n    } else {\n        // do not cache the rctx\n        assert(fgen->self_ref);\n        lua_State *L = fgen->thread->L;\n        lua_rawgeti(L, LUA_REGISTRYINDEX, fgen->self_ref);\n        mcp_rcontext_cleanup(L, fgen, rctx, lua_absindex(L, -1));\n        lua_pop(L, 1); // drop fgen\n    }\n\n    // we're closed and every outstanding request slot has been\n    // returned.\n    if (fgen->closed && fgen->free == fgen->total) {\n        mcp_funcgen_cleanup(fgen->thread->L, fgen);\n    }\n}\n\n// call with stack: mcp.funcgen -2, function -1\nstatic int _mcplib_funcgen_gencall(lua_State *L) {\n    mcp_funcgen_t *fgen = luaL_checkudata(L, -2, \"mcp.funcgen\");\n    int fgen_idx = lua_absindex(L, -2);\n    // create the ctx object.\n    int total_queues = fgen->max_queues + fgen->uobj_queues;\n    size_t rctx_len = sizeof(mcp_rcontext_t) + sizeof(struct mcp_rqueue_s) * total_queues;\n    mcp_rcontext_t *rc = lua_newuserdatauv(L, rctx_len, 0);\n    memset(rc, 0, rctx_len);\n\n    luaL_getmetatable(L, \"mcp.rcontext\");\n    lua_setmetatable(L, -2);\n    // allow the rctx to reference the function generator.\n    rc->fgen = fgen;\n    rc->lua_narg = 1;\n\n    // initialize the queue slots based on the fgen parent\n    for (int x = 0; x < fgen->max_queues; x++) {\n        struct mcp_rqueue_s *frqu = &fgen->queue_list[x];\n        struct mcp_rqueue_s *rqu = &rc->qslots[x];\n        rqu->obj_type = frqu->obj_type;\n        if (frqu->obj_type == RQUEUE_TYPE_POOL) {\n            rqu->obj_ref = 0;\n            rqu->obj = frqu->obj;\n            mcp_resp_t *r = mcp_prep_bare_resobj(L, fgen->thread);\n            rqu->res_ref = luaL_ref(L, LUA_REGISTRYINDEX);\n            rqu->res_obj = r;\n        } else if (frqu->obj_type == RQUEUE_TYPE_FGEN) {\n            // owner funcgen already holds the subfgen reference, so here we're just\n            // grabbing a subrctx to pin into the slot.\n            mcp_funcgen_t *fg = frqu->obj;\n            mcp_rcontext_t *subrctx = mcp_funcgen_get_rctx(L, fg->self_ref, fg);\n            if (subrctx == NULL) {\n                proxy_lua_error(L, \"failed to generate request slot during queue_assign()\");\n            }\n\n            // if this rctx ever had a request object assigned to it, we can get\n            // rid of it. we're pinning the subrctx in here and don't want\n            // to waste memory.\n            if (subrctx->request_ref) {\n                luaL_unref(L, LUA_REGISTRYINDEX, subrctx->request_ref);\n                subrctx->request_ref = 0;\n                subrctx->request = NULL;\n            }\n\n            // link the new rctx into this chain; we'll hold onto it until the\n            // parent de-allocates.\n            subrctx->parent = rc;\n            subrctx->parent_handle = x;\n            rqu->obj = subrctx;\n        }\n    }\n\n    // copy the rcontext reference\n    lua_pushvalue(L, -1);\n\n    // issue a rotation so one rcontext is now below genfunc, and one rcontext\n    // is on the top.\n    // right shift: gf, rc1, rc2 -> rc2, gf, rc1\n    lua_rotate(L, -3, 1);\n\n    // current stack should be func, mcp.rcontext.\n    int call_argnum = 1;\n    // stack will be func, rctx, arg if there is an arg.\n    if (fgen->argument_ref) {\n        lua_rawgeti(L, LUA_REGISTRYINDEX, fgen->argument_ref);\n        call_argnum++;\n    }\n\n    // can throw an error upstream.\n    lua_call(L, call_argnum, 1);\n\n    // we should have a top level function as a result.\n    if (!lua_isfunction(L, -1)) {\n        proxy_lua_error(L, \"function generator didn't return a function\");\n        return 0;\n    }\n    // can't fail past this point.\n\n    // pop the returned function.\n    rc->function_ref = luaL_ref(L, LUA_REGISTRYINDEX);\n\n    // link the rcontext into the function generator.\n    fgen->total++;\n\n    lua_getiuservalue(L, fgen_idx, 1); // get the reference table.\n    // rc, t -> t, rc\n    lua_rotate(L, -2, 1);\n    rc->self_ref = luaL_ref(L, -2); // pop rcontext\n    lua_pop(L, 1); // pop ref table.\n\n    _mcplib_funcgen_cache(fgen, rc);\n\n    // associate a coroutine thread with this context.\n    rc->Lc = lua_newthread(L);\n    assert(rc->Lc);\n    rc->coroutine_ref = luaL_ref(L, LUA_REGISTRYINDEX);\n\n    // increment the slot counter\n    LIBEVENT_THREAD *t = PROXY_GET_THR(L);\n    mcp_sharedvm_delta(t->proxy_ctx, SHAREDVM_FGENSLOT_IDX, fgen->name, 1);\n\n    event_assign(&rc->timeout_event, t->base, -1, EV_TIMEOUT, mcp_funcgen_wait_handler, rc);\n\n    // return the fgen.\n    // FIXME: just return 0? need to adjust caller to not mis-ref the\n    // generator function.\n    return 1;\n}\n\nstatic void _mcp_funcgen_return_rctx(mcp_rcontext_t *rctx) {\n    mcp_funcgen_t *fgen = rctx->fgen;\n    assert(rctx->pending_reqs == 0);\n    int res = lua_resetthread(rctx->Lc);\n    if (res != LUA_OK) {\n        // TODO: I was under the impression it was possible to reuse a\n        // coroutine from an error state, but it seems like this only works if\n        // the routine landed in LUA_YIELD or LUA_OK\n        // Leaving a note here to triple check this or if my memory was wrong.\n        // Instead for now we throw away the coroutine if it was involved in\n        // an error. Realistically this shouldn't be normal so it might not\n        // matter anyway.\n        lua_State *L = fgen->thread->L;\n        luaL_unref(L, LUA_REGISTRYINDEX, rctx->coroutine_ref);\n        rctx->Lc = lua_newthread(L);\n        assert(rctx->Lc);\n        rctx->coroutine_ref = luaL_ref(L, LUA_REGISTRYINDEX);\n    } else {\n        lua_settop(rctx->Lc, 0);\n    }\n    rctx->wait_mode = QWAIT_IDLE;\n    rctx->resp = NULL;\n    rctx->first_queue = false; // HACK\n    if (rctx->request) {\n        mcp_request_cleanup(fgen->thread, rctx->request);\n    }\n\n    // nuke alarm if set.\n    if (event_pending(&rctx->timeout_event, EV_TIMEOUT, NULL)) {\n        event_del(&rctx->timeout_event);\n    }\n\n    // reset each rqu.\n    for (int x = 0; x < fgen->max_queues; x++) {\n        struct mcp_rqueue_s *rqu = &rctx->qslots[x];\n        if (rqu->res_ref) {\n            if (rqu->res_obj) {\n                // using a persistent object.\n                mcp_response_cleanup(fgen->thread, rqu->res_obj);\n            } else {\n                // temporary error object\n                luaL_unref(rctx->Lc, LUA_REGISTRYINDEX, rqu->res_ref);\n                rqu->res_ref = 0;\n            }\n        }\n        if (rqu->req_ref) {\n            luaL_unref(rctx->Lc, LUA_REGISTRYINDEX, rqu->req_ref);\n            rqu->req_ref = 0;\n        }\n        assert(rqu->state != RQUEUE_ACTIVE);\n        rqu->state = RQUEUE_IDLE;\n        rqu->flags = 0;\n        rqu->rq = NULL;\n        if (rqu->obj_type == RQUEUE_TYPE_FGEN) {\n            _mcp_funcgen_return_rctx(rqu->obj);\n        }\n    }\n\n    // look for rctx-local objects.\n    if (rctx->uobj_count) {\n        int lim = fgen->max_queues + rctx->uobj_count;\n        for (int x = fgen->max_queues; x < lim; x++) {\n            struct mcp_rqueue_s *rqu = &rctx->qslots[x];\n            if (rqu->obj_type == RQUEUE_TYPE_UOBJ_REQ) {\n                mcp_request_t *rq = rqu->obj;\n                mcp_request_cleanup(fgen->thread, rq);\n            } else if (rqu->obj_type == RQUEUE_TYPE_UOBJ_RES) {\n                mcp_resp_t *rs = rqu->obj;\n                mcp_response_cleanup(fgen->thread, rs);\n            } else {\n                // no known type. only crash the debug binary.\n                assert(1 == 0);\n            }\n        }\n    }\n}\n\n// TODO: check rctx->awaiting before returning?\n// TODO: separate the \"cleanup\" portion from the \"Return to cache\" portion, so\n// we can call that directly for subrctx's\nvoid mcp_funcgen_return_rctx(mcp_rcontext_t *rctx) {\n    mcp_funcgen_t *fgen = rctx->fgen;\n    if (rctx->pending_reqs != 0) {\n        // not ready to return to cache yet.\n        return;\n    }\n    if (rctx->parent) {\n        // Important: we need to hold the parent request reference until this\n        // subrctx is fully depleted of outstanding requests itself.\n        rctx->parent->pending_reqs--;\n        assert(rctx->parent->pending_reqs > -1);\n        if (rctx->parent->pending_reqs == 0) {\n            mcp_funcgen_return_rctx(rctx->parent);\n        }\n        return;\n    }\n    WSTAT_DECR(rctx->fgen->thread, proxy_req_active, 1);\n    _mcp_funcgen_return_rctx(rctx);\n    _mcplib_funcgen_cache(fgen, rctx);\n}\n\nmcp_rcontext_t *mcp_funcgen_get_rctx(lua_State *L, int fgen_ref, mcp_funcgen_t *fgen) {\n    mcp_rcontext_t *rctx = NULL;\n    // nothing left in slot cache, generate a new function.\n    if (fgen->free == 0) {\n        // reset free pressure so we try to keep the rctx cached\n        fgen->free_pressure = 0;\n        fgen->free_waiter.tv_sec = 0;\n        // TODO (perf): pre-create this c closure somewhere hidden.\n        lua_pushcclosure(L, _mcplib_funcgen_gencall, 0);\n        // pull in the funcgen object\n        lua_rawgeti(L, LUA_REGISTRYINDEX, fgen_ref);\n        // then generator function\n        lua_rawgeti(L, LUA_REGISTRYINDEX, fgen->generator_ref);\n        // then generate a new function slot.\n        int res = lua_pcall(L, 2, 1, 0);\n        if (res != LUA_OK) {\n            LOGGER_LOG(NULL, LOG_PROXYEVENTS, LOGGER_PROXY_ERROR, NULL, lua_tostring(L, -1));\n            lua_settop(L, 0);\n            return NULL;\n        }\n        lua_pop(L, 1); // drop the extra funcgen\n    } else {\n        P_DEBUG(\"%s: serving from cache\\n\", __func__);\n    }\n\n    rctx = fgen->list[fgen->free-1];\n    fgen->list[fgen->free-1] = NULL;\n    fgen->free--;\n\n    // on non-error, return the response object upward.\n    return rctx;\n}\n\nmcp_rcontext_t *mcp_funcgen_start(lua_State *L, mcp_funcgen_t *fgen, mcp_parser_t *pr) {\n    if (fgen->is_router) {\n        fgen = mcp_funcgen_route(L, fgen, pr);\n        if (fgen == NULL) {\n            return NULL;\n        }\n    }\n    // fgen->self_ref must be valid because we cannot start a function that\n    // hasn't been referenced anywhere.\n    mcp_rcontext_t *rctx = mcp_funcgen_get_rctx(L, fgen->self_ref, fgen);\n\n    if (rctx == NULL) {\n        return NULL;\n    }\n\n    // only top level rctx's can have a request object assigned to them.\n    // so we create them late here, in the start function.\n    // Note that we can _technically_ fail with an OOM here, but we've not set\n    // up lua in a way that OOM's are possible.\n    if (rctx->request_ref == 0) {\n        mcp_request_t *rq = lua_newuserdatauv(L, sizeof(mcp_request_t) + MCP_REQUEST_MAXLEN, 0);\n        memset(rq, 0, sizeof(mcp_request_t));\n        luaL_getmetatable(L, \"mcp.request\");\n        lua_setmetatable(L, -2);\n\n        rctx->request_ref = luaL_ref(L, LUA_REGISTRYINDEX); // pop the request\n        rctx->request = rq;\n    }\n\n    // TODO: could probably move a few more lines from proto_proxy into here,\n    // but that's splitting hairs.\n    WSTAT_INCR(fgen->thread, proxy_req_active, 1);\n    return rctx;\n}\n\n// calling either with self_ref set, or with fgen in stack -1 (ie; from GC\n// function without ever being attached to anything)\nstatic void mcp_funcgen_cleanup(lua_State *L, mcp_funcgen_t *fgen) {\n    int fgen_idx = 0;\n    lua_checkstack(L, 5); // paranoia. this can recurse from a router.\n    // pull the fgen into the stack.\n    if (fgen->self_ref) {\n        // pull self onto the stack and hold until the end of the func.\n        lua_rawgeti(L, LUA_REGISTRYINDEX, fgen->self_ref);\n        fgen_idx = lua_absindex(L, -1); // remember fgen offset\n        // remove the C reference to the fgen\n        luaL_unref(L, LUA_REGISTRYINDEX, fgen->self_ref);\n        fgen->self_ref = 0;\n    } else if (fgen->closed) {\n        // we've already cleaned up, probably redundant call from _gc()\n        return;\n    } else {\n        // not closed, no self-ref, so must be unattached and coming from GC\n        fgen_idx = lua_absindex(L, -1);\n    }\n\n    if (fgen->is_router) {\n        // we're actually a \"router\", send this out for cleanup.\n        mcp_funcgen_router_cleanup(L, fgen);\n    }\n\n    // decrement the slot counter\n    LIBEVENT_THREAD *t = PROXY_GET_THR(L);\n    mcp_sharedvm_delta(t->proxy_ctx, SHAREDVM_FGEN_IDX, fgen->name, -1);\n\n    // Walk every request context and issue cleanup.\n    for (int x = 0; x < fgen->free_max; x++) {\n        mcp_rcontext_t *rctx = fgen->list[x];\n        if (rctx == NULL) {\n            continue;\n        }\n        mcp_rcontext_cleanup(L, fgen, rctx, fgen_idx);\n    }\n\n    if (fgen->argument_ref) {\n        luaL_unref(L, LUA_REGISTRYINDEX, fgen->argument_ref);\n        fgen->argument_ref = 0;\n    }\n\n    if (fgen->generator_ref) {\n        luaL_unref(L, LUA_REGISTRYINDEX, fgen->generator_ref);\n        fgen->generator_ref = 0;\n    }\n\n    if (fgen->queue_list) {\n        for (int x = 0; x < fgen->max_queues; x++) {\n            struct mcp_rqueue_s *rqu = &fgen->queue_list[x];\n            if (rqu->obj_type == RQUEUE_TYPE_POOL) {\n                // just the obj_ref\n                luaL_unref(L, LUA_REGISTRYINDEX, rqu->obj_ref);\n            } else if (rqu->obj_type == RQUEUE_TYPE_FGEN) {\n                // don't need to recurse, just deref.\n                mcp_funcgen_t *subfgen = rqu->obj;\n                mcp_funcgen_dereference(L, subfgen);\n            } else if (rqu->obj_type != RQUEUE_TYPE_NONE) {\n                assert(1 == 0);\n            }\n        }\n        free(fgen->queue_list);\n    }\n\n    free(fgen->list);\n    fgen->list = NULL;\n    lua_pop(L, 1); // drop funcgen reference\n}\n\n// Must be called with the function generator at on top of stack\n// Pops the value from the stack.\nvoid mcp_funcgen_reference(lua_State *L) {\n    mcp_funcgen_t *fgen = luaL_checkudata(L, -1, \"mcp.funcgen\");\n    if (fgen->self_ref) {\n        fgen->refcount++;\n        lua_pop(L, 1); // ensure we drop the extra value.\n    } else {\n        fgen->self_ref = luaL_ref(L, LUA_REGISTRYINDEX);\n        fgen->refcount = 1;\n    }\n    P_DEBUG(\"%s: funcgen referenced: %d\\n\", __func__, fgen->refcount);\n}\n\nvoid mcp_funcgen_dereference(lua_State *L, mcp_funcgen_t *fgen) {\n    assert(fgen->refcount > 0);\n    fgen->refcount--;\n    P_DEBUG(\"%s: funcgen dereferenced: %d\\n\", __func__, fgen->refcount);\n    if (fgen->refcount == 0) {\n        fgen->closed = true;\n\n        P_DEBUG(\"%s: funcgen cleaning up\\n\", __func__);\n        if (fgen->free == fgen->total) {\n            mcp_funcgen_cleanup(L, fgen);\n        }\n    }\n}\n\n// All we need to do here is copy the function reference we've stashed into\n// the C closure's upvalue and return it.\nstatic int _mcplib_funcgenbare_generator(lua_State *L) {\n    lua_pushvalue(L, lua_upvalueindex(1));\n    return 1;\n}\n\n// helper function to create a function generator with a \"default\" function.\n// the function passed in here is a standard 'function(r) etc end' prototype,\n// which we want to always return instead of calling a real generator\n// function.\nint mcplib_funcgenbare_new(lua_State *L) {\n    if (!lua_isfunction(L, -1)) {\n        proxy_lua_error(L, \"Must pass a function to mcp.funcgenbare_new\");\n        return 0;\n    }\n\n    // Pops the function into the upvalue of this C closure function.\n    lua_pushcclosure(L, _mcplib_funcgenbare_generator, 1);\n    // FIXME: not urgent, but this function chain isn't stack balanced, and its caller has\n    // to drop an extra reference.\n    // Need to re-audit and decide if we still need this pushvalue here or if\n    // we can drop the pop from the caller and leave this function balanced.\n    lua_pushvalue(L, -1);\n    int gen_ref = luaL_ref(L, LUA_REGISTRYINDEX);\n\n    // Pass our fakeish generator function down the line.\n    mcplib_funcgen_new(L);\n\n    mcp_funcgen_t *fgen = lua_touserdata(L, -1);\n    strncpy(fgen->name, \"anonymous\", FGEN_NAME_MAXLEN);\n    mcp_sharedvm_delta(fgen->thread->proxy_ctx, SHAREDVM_FGEN_IDX, fgen->name, 1);\n\n    fgen->generator_ref = gen_ref;\n    fgen->ready = true;\n    return 1;\n}\n\n#define FGEN_DEFAULT_FREELIST_SIZE 8\nint mcplib_funcgen_new(lua_State *L) {\n    LIBEVENT_THREAD *t = PROXY_GET_THR(L);\n\n    mcp_funcgen_t *fgen = lua_newuserdatauv(L, sizeof(mcp_funcgen_t), 2);\n    memset(fgen, 0, sizeof(mcp_funcgen_t));\n    fgen->thread = t;\n    fgen->free_max = FGEN_DEFAULT_FREELIST_SIZE;\n    fgen->list = calloc(fgen->free_max, sizeof(mcp_rcontext_t *));\n\n    luaL_getmetatable(L, \"mcp.funcgen\");\n    lua_setmetatable(L, -2);\n\n    // the table we will use to hold references to rctx's\n    lua_createtable(L, 8, 0);\n    // set our table into the uservalue 1 of fgen (idx -2)\n    // pops the table.\n    lua_setiuservalue(L, -2, 1);\n\n    return 1;\n}\n\nint mcplib_funcgen_new_handle(lua_State *L) {\n    mcp_funcgen_t *fgen = lua_touserdata(L, 1);\n    mcp_pool_proxy_t *pp = NULL;\n    mcp_funcgen_t *fg = NULL;\n\n    if (fgen->ready) {\n        proxy_lua_error(L, \"cannot modify function generator after calling ready\");\n        return 0;\n    }\n\n    if ((pp = luaL_testudata(L, 2, \"mcp.pool_proxy\")) != NULL) {\n        // good.\n    } else if ((fg = luaL_testudata(L, 2, \"mcp.funcgen\")) != NULL) {\n        if (fg->is_router) {\n            proxy_lua_error(L, \"cannot assign a router to a handle in new_handle\");\n            return 0;\n        }\n        if (fg->closed) {\n            proxy_lua_error(L, \"cannot use a replaced function in new_handle\");\n            return 0;\n        }\n    } else {\n        proxy_lua_error(L, \"invalid argument to new_handle\");\n        return 0;\n    }\n\n    fgen->max_queues++;\n    if (fgen->queue_list == NULL) {\n        fgen->queue_list = malloc(sizeof(struct mcp_rqueue_s));\n    } else {\n        fgen->queue_list = realloc(fgen->queue_list, fgen->max_queues * sizeof(struct mcp_rqueue_s));\n    }\n    if (fgen->queue_list == NULL) {\n        proxy_lua_error(L, \"failed to realloc queue list during new_handle()\");\n        return 0;\n    }\n\n    struct mcp_rqueue_s *rqu = &fgen->queue_list[fgen->max_queues-1];\n    memset(rqu, 0, sizeof(*rqu));\n\n    if (pp) {\n        // pops pp from the stack\n        rqu->obj_ref = luaL_ref(L, LUA_REGISTRYINDEX);\n        rqu->obj_type = RQUEUE_TYPE_POOL;\n        rqu->obj = pp;\n    } else {\n        // pops the fgen from the stack.\n        mcp_funcgen_reference(L);\n        rqu->obj_type = RQUEUE_TYPE_FGEN;\n        rqu->obj = fg;\n    }\n\n    lua_pushinteger(L, fgen->max_queues-1);\n    return 1;\n}\n\nint mcplib_funcgen_ready(lua_State *L) {\n    mcp_funcgen_t *fgen = lua_touserdata(L, 1);\n    luaL_checktype(L, 2, LUA_TTABLE);\n\n    if (fgen->ready) {\n        proxy_lua_error(L, \"cannot modify function generator after calling ready\");\n        return 0;\n    }\n\n    if (lua_getfield(L, 2, \"f\") != LUA_TFUNCTION) {\n        proxy_lua_error(L, \"Must specify generator function ('f') to fgen:ready\");\n        return 0;\n    }\n    fgen->generator_ref = luaL_ref(L, LUA_REGISTRYINDEX);\n\n    if (lua_getfield(L, 2, \"a\") != LUA_TNIL) {\n        fgen->argument_ref = luaL_ref(L, LUA_REGISTRYINDEX);\n    } else {\n        lua_pop(L, 1);\n    }\n\n    if (lua_getfield(L, 2, \"n\") == LUA_TSTRING) {\n        size_t len = 0;\n        const char *name = lua_tolstring(L, -1, &len);\n        strncpy(fgen->name, name, FGEN_NAME_MAXLEN);\n    } else {\n        strncpy(fgen->name, \"anonymous\", FGEN_NAME_MAXLEN);\n        lua_pop(L, 1);\n    }\n\n    if (lua_getfield(L, 2, \"u\") == LUA_TNUMBER) {\n        int uobj_queues = luaL_checkinteger(L, -1);\n        if (uobj_queues < 1 || uobj_queues > RQUEUE_UOBJ_MAX) {\n            proxy_lua_ferror(L, \"user obj ('u') in fgen:ready must be between 1 and %d\", RQUEUE_UOBJ_MAX);\n            return 0;\n        }\n        fgen->uobj_queues = uobj_queues;\n    }\n    lua_pop(L, 1);\n\n    // now we test the generator function and create the first slot.\n    lua_pushvalue(L, 1); // copy the funcgen to pass into gencall\n    lua_rawgeti(L, LUA_REGISTRYINDEX, fgen->generator_ref); // for gencall\n    _mcplib_funcgen_gencall(L);\n    lua_pop(L, 1); // drop extra funcgen ref.\n\n    // add us to the global state\n    mcp_sharedvm_delta(fgen->thread->proxy_ctx, SHAREDVM_FGEN_IDX, fgen->name, 1);\n\n    fgen->ready = true;\n    return 1;\n}\n\n// Handlers for request contexts\n\nint mcplib_rcontext_handle_set_cb(lua_State *L) {\n    mcp_rcontext_t *rctx = lua_touserdata(L, 1);\n    luaL_checktype(L, 2, LUA_TNUMBER);\n    luaL_checktype(L, 3, LUA_TFUNCTION);\n\n    int handle = lua_tointeger(L, 2);\n    if (handle < 0 || handle >= rctx->fgen->max_queues) {\n        proxy_lua_error(L, \"invalid handle passed to queue_set_cb\");\n        return 0;\n    }\n\n    struct mcp_rqueue_s *rqu = &rctx->qslots[handle];\n    if (rqu->cb_ref) {\n        luaL_unref(L, LUA_REGISTRYINDEX, rqu->cb_ref);\n    }\n    rqu->cb_ref = luaL_ref(L, LUA_REGISTRYINDEX);\n\n    return 0;\n}\n\n// call with request object on top of stack.\n// pops the request object\n// FIXME: callers are doing a pushvalue(L, 2) and then in here we're also\n// pushvalue(L, 2)\n// Think this should just document as needing the request object top of stack\n// and xmove without the extra push bits.\nstatic void _mcplib_rcontext_queue(lua_State *L, mcp_rcontext_t *rctx, mcp_request_t *rq, int handle) {\n    if (handle < 0 || handle >= rctx->fgen->max_queues) {\n        proxy_lua_error(L, \"attempted to enqueue an invalid handle\");\n        return;\n    }\n    struct mcp_rqueue_s *rqu = &rctx->qslots[handle];\n\n    if (rqu->state != RQUEUE_IDLE) {\n        lua_pop(L, 1);\n        return;\n    }\n\n    // If we're queueing to an fgen, arm the coroutine while we have the\n    // objects handy. Else this requires roundtripping a luaL_ref/luaL_unref\n    // later.\n    if (rqu->obj_type == RQUEUE_TYPE_FGEN) {\n        mcp_rcontext_t *subrctx = rqu->obj;\n        lua_pushvalue(L, 2); // duplicate the request obj\n        lua_rawgeti(subrctx->Lc, LUA_REGISTRYINDEX, subrctx->function_ref);\n        lua_xmove(L, subrctx->Lc, 1); // move the requet object.\n        subrctx->pending_reqs++;\n    }\n\n    // hold the request reference.\n    rqu->req_ref = luaL_ref(L, LUA_REGISTRYINDEX);\n\n    rqu->state = RQUEUE_QUEUED;\n    rqu->rq = rq;\n}\n\n// first arg is rcontext\n// then a request object\n// then either a handle (integer) or array style table of handles\nint mcplib_rcontext_enqueue(lua_State *L) {\n    mcp_rcontext_t *rctx = lua_touserdata(L, 1);\n    mcp_request_t *rq = luaL_checkudata(L, 2, \"mcp.request\");\n\n    if (rctx->wait_mode != QWAIT_IDLE) {\n        proxy_lua_error(L, \"enqueue: cannot enqueue new requests while in a wait\");\n        return 0;\n    }\n\n    if (!rq->pr.keytoken) {\n        proxy_lua_error(L, \"cannot queue requests without a key\");\n        return 0;\n    }\n\n    int type = lua_type(L, 3);\n    if (type == LUA_TNUMBER) {\n        int handle = lua_tointeger(L, 3);\n\n        lua_pushvalue(L, 2);\n        _mcplib_rcontext_queue(L, rctx, rq, handle);\n    } else if (type == LUA_TTABLE) {\n        unsigned int len = lua_rawlen(L, 3);\n        for (int x = 0; x < len; x++) {\n            type = lua_rawgeti(L, 3, x+1);\n            if (type != LUA_TNUMBER) {\n                proxy_lua_error(L, \"invalid handle passed to queue via array table\");\n                return 0;\n            }\n\n            int handle = lua_tointeger(L, 4);\n            lua_pop(L, 1);\n\n            lua_pushvalue(L, 2);\n            _mcplib_rcontext_queue(L, rctx, rq, handle);\n        }\n    } else {\n        proxy_lua_error(L, \"must pass a handle or a table to queue\");\n        return 0;\n    }\n\n    return 0;\n}\n\n// TODO: pre-generate a result object into sub-rctx's that we can pull up for\n// this, instead of allocating outside of a protected call.\nstatic void _mcp_resume_rctx_process_error(mcp_rcontext_t *rctx, struct mcp_rqueue_s *rqu) {\n    // we have an error. need to mark the error into the parent rqu\n    rqu->flags |= RQUEUE_R_ERROR|RQUEUE_R_ANY;\n    mcp_resp_t *r = mcp_prep_bare_resobj(rctx->Lc, rctx->fgen->thread);\n    r->status = MCMC_ERR;\n    r->resp.code = MCMC_CODE_SERVER_ERROR;\n    assert(rqu->res_ref == 0);\n    rqu->res_ref = luaL_ref(rctx->Lc, LUA_REGISTRYINDEX);\n    mcp_process_rqueue_return(rctx->parent, rctx->parent_handle, r);\n    if (rctx->parent->wait_count) {\n        mcp_process_rctx_wait(rctx->parent, rctx->parent_handle);\n    }\n}\n\nstatic void _mcp_start_rctx_process_error(mcp_rcontext_t *rctx, struct mcp_rqueue_s *rqu) {\n    // we have an error. need to mark the error into the parent rqu\n    rqu->flags |= RQUEUE_R_ERROR|RQUEUE_R_ANY;\n    mcp_resp_t *r = mcp_prep_bare_resobj(rctx->Lc, rctx->fgen->thread);\n    r->status = MCMC_ERR;\n    r->resp.code = MCMC_CODE_SERVER_ERROR;\n    assert(rqu->res_ref == 0);\n    rqu->res_ref = luaL_ref(rctx->Lc, LUA_REGISTRYINDEX);\n\n    // queue an IO to return later.\n    io_pending_proxy_t *p = mcp_queue_rctx_io(rctx->parent, NULL, NULL, r);\n    p->return_cb = proxy_return_rqu_cb;\n    p->queue_handle = rctx->parent_handle;\n    p->background = true;\n}\n\nstatic void mcp_start_subrctx(mcp_rcontext_t *rctx) {\n    int res = proxy_run_rcontext(rctx);\n    struct mcp_rqueue_s *rqu = &rctx->parent->qslots[rctx->parent_handle];\n    if (res == LUA_OK) {\n        int type = lua_type(rctx->Lc, 1);\n        mcp_resp_t *r = NULL;\n        if (type == LUA_TUSERDATA && (r = luaL_testudata(rctx->Lc, 1, \"mcp.response\")) != NULL) {\n            // move stack result object into parent rctx rqu slot.\n            assert(rqu->res_ref == 0);\n            rqu->res_ref = luaL_ref(rctx->Lc, LUA_REGISTRYINDEX);\n\n            io_pending_proxy_t *p = mcp_queue_rctx_io(rctx->parent, NULL, NULL, r);\n            p->return_cb = proxy_return_rqu_cb;\n            p->queue_handle = rctx->parent_handle;\n            // TODO: change name of property to fast-return once mcp.await is\n            // retired.\n            p->background = true;\n        } else if (type == LUA_TSTRING) {\n            // TODO: wrap with a resobj and parse it.\n            // for now we bypass the rqueue process handling\n            // meaning no callbacks/etc.\n            assert(rqu->res_ref == 0);\n            rqu->res_ref = luaL_ref(rctx->Lc, LUA_REGISTRYINDEX);\n            rqu->flags |= RQUEUE_R_ANY;\n            rqu->state = RQUEUE_COMPLETE;\n            io_pending_proxy_t *p = mcp_queue_rctx_io(rctx->parent, NULL, NULL, NULL);\n            p->return_cb = proxy_return_rqu_cb;\n            p->queue_handle = rctx->parent_handle;\n            p->background = true;\n        } else {\n            // generate a generic object with an error.\n            _mcp_start_rctx_process_error(rctx, rqu);\n        }\n    } else if (res == LUA_YIELD) {\n        // normal.\n    } else {\n        lua_pop(rctx->Lc, 1); // drop the error message.\n        _mcp_start_rctx_process_error(rctx, rqu);\n    }\n}\n\nstatic void mcp_resume_rctx_from_cb(mcp_rcontext_t *rctx) {\n    int res = proxy_run_rcontext(rctx);\n    if (rctx->parent) {\n        struct mcp_rqueue_s *rqu = &rctx->parent->qslots[rctx->parent_handle];\n        if (res == LUA_OK) {\n            mcp_rcontext_t *parent = rctx->parent;\n            int handle = rctx->parent_handle;\n            int type = lua_type(rctx->Lc, 1);\n            mcp_resp_t *r = NULL;\n            if (type == LUA_TUSERDATA && (r = luaL_testudata(rctx->Lc, 1, \"mcp.response\")) != NULL) {\n                // move stack result object into parent rctx rqu slot.\n                assert(rqu->res_ref == 0);\n                rqu->res_ref = luaL_ref(rctx->Lc, LUA_REGISTRYINDEX);\n                mcp_process_rqueue_return(rctx->parent, rctx->parent_handle, r);\n            } else if (type == LUA_TSTRING) {\n                // TODO: wrap with a resobj and parse it.\n                // for now we bypass the rqueue process handling\n                // meaning no callbacks/etc.\n                assert(rqu->res_ref == 0);\n                rqu->res_ref = luaL_ref(rctx->Lc, LUA_REGISTRYINDEX);\n                rqu->flags |= RQUEUE_R_ANY;\n                rqu->state = RQUEUE_COMPLETE;\n            } else {\n                // generate a generic object with an error.\n                _mcp_resume_rctx_process_error(rctx, rqu);\n            }\n\n            // return ourself before telling the parent to wait.\n            mcp_funcgen_return_rctx(rctx);\n            if (parent->wait_count) {\n                mcp_process_rctx_wait(parent, handle);\n            }\n        } else if (res == LUA_YIELD) {\n            // normal.\n            _mcp_queue_hack(rctx->c);\n        } else {\n            lua_pop(rctx->Lc, 1); // drop the error message.\n            _mcp_resume_rctx_process_error(rctx, rqu);\n            mcp_funcgen_return_rctx(rctx);\n        }\n    } else {\n        // Parent rctx has returned either a response or error to its top\n        // level resp object and is now complete.\n        // HACK\n        // see notes in proxy_run_rcontext()\n        // NOTE: this function is called from rqu_cb(), which pushes the\n        // submission loop. This code below can call drive_machine(), which\n        // calls submission loop if stuff is queued.\n        // Would remove redundancy if we can signal up to rqu_cb() and either\n        // q->count-- or do the inline submission at that level.\n        if (res != LUA_YIELD) {\n            mcp_funcgen_return_rctx(rctx);\n            io_queue_t *q = conn_io_queue_get(rctx->c, IO_QUEUE_PROXY);\n            q->count--;\n            if (q->count == 0) {\n                // call re-add directly since we're already in the worker thread.\n                conn_worker_readd(rctx->c);\n            }\n        } else if (res == LUA_YIELD) {\n            _mcp_queue_hack(rctx->c);\n        }\n    }\n}\n\n// This \"Dummy\" IO immediately resumes the yielded function, without a result\n// attached.\nstatic void proxy_return_rqu_dummy_cb(io_pending_t *pending) {\n    io_pending_proxy_t *p = (io_pending_proxy_t *)pending;\n    mcp_rcontext_t *rctx = p->rctx;\n\n    rctx->pending_reqs--;\n    assert(rctx->pending_reqs > -1);\n\n    lua_settop(rctx->Lc, 0);\n    lua_pushinteger(rctx->Lc, 0); // return a \"0\" done count to the function.\n    mcp_resume_rctx_from_cb(rctx);\n\n    do_cache_free(p->thread->io_cache, p);\n}\n\nvoid mcp_process_rctx_wait(mcp_rcontext_t *rctx, int handle) {\n    struct mcp_rqueue_s *rqu = &rctx->qslots[handle];\n    int status = rqu->flags;\n    assert(rqu->state == RQUEUE_COMPLETE);\n    // waiting for some IO's to complete before continuing.\n    // meaning if we \"match good\" here, we can resume.\n    // we can also resume if we are in wait mode but pending_reqs is down\n    // to 1.\n    switch (rctx->wait_mode) {\n        case QWAIT_IDLE:\n            // should be impossible to get here.\n            // TODO: find a better path for throwing real errors from these\n            // side cases. would feel better long term.\n            abort();\n            break;\n        case QWAIT_GOOD:\n            if (status & RQUEUE_R_GOOD) {\n                rctx->wait_done++;\n                rqu->state = RQUEUE_WAITED;\n            }\n            break;\n        case QWAIT_OK:\n            if (status & (RQUEUE_R_GOOD|RQUEUE_R_OK)) {\n                rctx->wait_done++;\n                rqu->state = RQUEUE_WAITED;\n            }\n            break;\n        case QWAIT_ANY:\n            rctx->wait_done++;\n            rqu->state = RQUEUE_WAITED;\n            break;\n        case QWAIT_FASTGOOD:\n            if (status & RQUEUE_R_GOOD) {\n                rctx->wait_done++;\n                rqu->state = RQUEUE_WAITED;\n                // resume early if \"good\"\n                status |= RQUEUE_R_RESUME;\n            } else if (status & RQUEUE_R_OK) {\n                // count but don't resume early if \"ok\"\n                rctx->wait_done++;\n                rqu->state = RQUEUE_WAITED;\n            }\n            break;\n        case QWAIT_HANDLE:\n            // waiting for a specific handle to return\n            if (handle == rctx->wait_handle) {\n                rctx->wait_done++;\n                rqu->state = RQUEUE_WAITED;\n            }\n            break;\n        case QWAIT_SLEEP:\n            assert(1 == 0); // should not get here.\n            break;\n    }\n\n    assert(rctx->pending_reqs != 0);\n    if ((status & RQUEUE_R_RESUME) || rctx->wait_done == rctx->wait_count || rctx->pending_reqs == 1) {\n        // ran out of stuff to wait for. time to resume.\n        // TODO: can we do the settop at the yield? nothing we need to\n        // keep in the stack in this mode.\n        lua_settop(rctx->Lc, 0);\n        rctx->wait_count = 0;\n        if (rctx->wait_mode == QWAIT_HANDLE) {\n            mcp_rcontext_push_rqu_res(rctx->Lc, rctx, handle);\n        } else {\n            lua_pushinteger(rctx->Lc, rctx->wait_done);\n        }\n        rctx->wait_mode = QWAIT_IDLE;\n\n        // nuke alarm if set.\n        if (event_pending(&rctx->timeout_event, EV_TIMEOUT, NULL)) {\n            event_del(&rctx->timeout_event);\n        }\n\n        mcp_resume_rctx_from_cb(rctx);\n    }\n}\n\n// sets the slot's return status code, to be used for filtering responses\n// later.\n// if a callback was set, execute it now.\nint mcp_process_rqueue_return(mcp_rcontext_t *rctx, int handle, mcp_resp_t *res) {\n    struct mcp_rqueue_s *rqu = &rctx->qslots[handle];\n    uint8_t flag = RQUEUE_R_ANY;\n\n    assert(rqu->state == RQUEUE_ACTIVE);\n    rqu->state = RQUEUE_COMPLETE;\n    if (res->status == MCMC_OK) {\n        if (res->resp.code != MCMC_CODE_END) {\n            flag = RQUEUE_R_GOOD;\n        } else {\n            flag = RQUEUE_R_OK;\n        }\n    }\n\n    if (rqu->cb_ref) {\n        lua_settop(rctx->Lc, 0);\n        lua_rawgeti(rctx->Lc, LUA_REGISTRYINDEX, rqu->cb_ref);\n        lua_rawgeti(rctx->Lc, LUA_REGISTRYINDEX, rqu->res_ref);\n        lua_rawgeti(rctx->Lc, LUA_REGISTRYINDEX, rqu->req_ref);\n        if (lua_pcall(rctx->Lc, 2, 2, 0) != LUA_OK) {\n            LOGGER_LOG(NULL, LOG_PROXYEVENTS, LOGGER_PROXY_ERROR, NULL, lua_tostring(rctx->Lc, -1));\n        } else if (lua_isinteger(rctx->Lc, 1)) {\n            // allow overriding the result flag from the callback.\n            enum mcp_rqueue_e mode = lua_tointeger(rctx->Lc, 1);\n            switch (mode) {\n                case QWAIT_GOOD:\n                    flag = RQUEUE_R_GOOD;\n                    break;\n                case QWAIT_OK:\n                    flag = RQUEUE_R_OK;\n                    break;\n                case QWAIT_ANY:\n                    break;\n                default:\n                    // ANY\n                    break;\n            }\n\n            // if second result return shortcut status code\n            if (lua_toboolean(rctx->Lc, 2)) {\n                flag |= RQUEUE_R_RESUME;\n            }\n        }\n        lua_settop(rctx->Lc, 0); // FIXME: This might not be necessary.\n                                 // we settop _before_ calling cb's and\n                                 // _before_ setting up for a coro resume.\n    }\n    rqu->flags |= flag;\n    return rqu->flags;\n}\n\n// specific function for queue-based returns.\nstatic void proxy_return_rqu_cb(io_pending_t *pending) {\n    io_pending_proxy_t *p = (io_pending_proxy_t *)pending;\n    mcp_rcontext_t *rctx = p->rctx;\n\n    if (p->client_resp) {\n        mcp_process_rqueue_return(rctx, p->queue_handle, p->client_resp);\n    }\n    rctx->pending_reqs--;\n    assert(rctx->pending_reqs > -1);\n\n    if (rctx->wait_count) {\n        mcp_process_rctx_wait(rctx, p->queue_handle);\n    } else {\n        mcp_funcgen_return_rctx(rctx);\n    }\n\n    do_cache_free(p->thread->io_cache, p);\n}\n\nvoid mcp_run_rcontext_handle(mcp_rcontext_t *rctx, int handle) {\n    struct mcp_rqueue_s *rqu = NULL;\n    rqu = &rctx->qslots[handle];\n\n    if (rqu->state == RQUEUE_QUEUED) {\n        rqu->state = RQUEUE_ACTIVE;\n        if (rqu->obj_type == RQUEUE_TYPE_POOL) {\n            mcp_request_t *rq = rqu->rq;\n            mcp_backend_t *be = mcplib_pool_proxy_call_helper(rqu->obj, MCP_PARSER_KEY(rq->pr), rq->pr.klen);\n            // FIXME: queue requires conn because we're stacking objects\n            // into the connection for later submission, which means we\n            // absolutely cannot queue things once *c becomes invalid.\n            // need to assert/block this from happening.\n            mcp_set_resobj(rqu->res_obj, rq, be, rctx->fgen->thread);\n            io_pending_proxy_t *p = mcp_queue_rctx_io(rctx, rq, be, rqu->res_obj);\n            p->return_cb = proxy_return_rqu_cb;\n            p->queue_handle = handle;\n            rctx->pending_reqs++;\n        } else if (rqu->obj_type == RQUEUE_TYPE_FGEN) {\n            // TODO: NULL the ->c post-return?\n            mcp_rcontext_t *subrctx = rqu->obj;\n            subrctx->c = rctx->c;\n            rctx->pending_reqs++;\n            mcp_start_subrctx(subrctx);\n        } else {\n            assert(1==0);\n        }\n    } else if (rqu->state == RQUEUE_COMPLETE && rctx->wait_count) {\n        // The slot was previously completed from an earlier dispatch, but we\n        // haven't \"waited\" on it yet.\n        mcp_process_rctx_wait(rctx, handle);\n    }\n}\n\nstatic inline void _mcplib_set_rctx_alarm(lua_State *L, mcp_rcontext_t *rctx, int arg) {\n    int isnum = 0;\n    lua_Number secondsf = lua_tonumberx(L, arg, &isnum);\n    if (!isnum) {\n        proxy_lua_error(L, \"timeout argument to wait or sleep must be a number\");\n        return;\n    }\n    int pending = event_pending(&rctx->timeout_event, EV_TIMEOUT, NULL);\n    if ((pending & (EV_TIMEOUT)) == 0) {\n        struct timeval tv = { .tv_sec = 0, .tv_usec = 0 };\n        lua_Integer secondsi = (lua_Integer) secondsf;\n        lua_Number subseconds = secondsf - secondsi;\n\n        tv.tv_sec = secondsi;\n        tv.tv_usec = MICROSECONDS(subseconds);\n        event_add(&rctx->timeout_event, &tv);\n    }\n}\n\n// TODO: one more function to wait on a list of handles? to queue and wait on\n// a list of handles? expand wait_cond()\n\nstatic inline int _mcplib_rcontext_wait_prep(lua_State *L, mcp_rcontext_t *rctx, int argc) {\n    int mode = QWAIT_ANY;\n    int wait = 0;\n\n    if (rctx->wait_mode != QWAIT_IDLE) {\n        proxy_lua_error(L, \"wait_cond: cannot call while already in wait mode\");\n        return 0;\n    }\n\n    if (argc < 2) {\n        proxy_lua_error(L, \"must pass at least count to wait_cond\");\n        return 0;\n    }\n\n    int isnum = 0;\n    wait = lua_tointegerx(L, 2, &isnum);\n    if (!isnum || wait < 0) {\n        proxy_lua_error(L, \"wait count for wait_cond must be a positive integer\");\n        return 0;\n    }\n\n    if (argc > 2) {\n        mode = lua_tointeger(L, 3);\n    }\n\n    switch (mode) {\n        case QWAIT_ANY:\n        case QWAIT_OK:\n        case QWAIT_GOOD:\n        case QWAIT_FASTGOOD:\n            break;\n        default:\n            proxy_lua_error(L, \"invalid mode sent to wait_cond\");\n            return 0;\n    }\n\n    rctx->wait_count = wait;\n    rctx->wait_done = 0;\n    rctx->wait_mode = mode;\n\n    return 0;\n}\n\n// takes num, filter mode\nint mcplib_rcontext_wait_cond(lua_State *L) {\n    int argc = lua_gettop(L);\n    mcp_rcontext_t *rctx = lua_touserdata(L, 1);\n\n    _mcplib_rcontext_wait_prep(L, rctx, argc);\n\n    // waiting for none, meaning just execute the queues.\n    if (rctx->wait_count == 0) {\n        io_pending_proxy_t *p = mcp_queue_rctx_io(rctx, NULL, NULL, NULL);\n        p->return_cb = proxy_return_rqu_dummy_cb;\n        p->background = true;\n        rctx->pending_reqs++;\n        rctx->wait_mode = QWAIT_IDLE; // not actually waiting.\n    } else if (argc > 3) {\n        // optional wait timeout. does not cancel existing request!\n        _mcplib_set_rctx_alarm(L, rctx, 4);\n    }\n\n    lua_pushinteger(L, MCP_YIELD_WAITCOND);\n    return lua_yield(L, 1);\n}\n\nint mcplib_rcontext_enqueue_and_wait(lua_State *L) {\n    mcp_rcontext_t *rctx = lua_touserdata(L, 1);\n    mcp_request_t *rq = luaL_checkudata(L, 2, \"mcp.request\");\n    int isnum = 0;\n    int handle = lua_tointegerx(L, 3, &isnum);\n\n    if (rctx->wait_mode != QWAIT_IDLE) {\n        proxy_lua_error(L, \"wait_cond: cannot call while already in wait mode\");\n        return 0;\n    }\n\n    if (!rq->pr.keytoken) {\n        proxy_lua_error(L, \"cannot queue requests without a key\");\n        return 0;\n    }\n\n    if (!isnum) {\n        proxy_lua_error(L, \"invalid handle passed to enqueue_and_wait\");\n        return 0;\n    }\n\n    // queue up this handle and yield for the direct wait.\n    lua_pushvalue(L, 2);\n    _mcplib_rcontext_queue(L, rctx, rq, handle);\n\n    if (lua_gettop(L) > 3) {\n        _mcplib_set_rctx_alarm(L, rctx, 4);\n    }\n\n    rctx->wait_done = 0;\n    rctx->wait_count = 1;\n    rctx->wait_mode = QWAIT_HANDLE;\n    rctx->wait_handle = handle;\n\n    lua_pushinteger(L, MCP_YIELD_WAITHANDLE);\n    return lua_yield(L, 1);\n}\n\nint mcplib_rcontext_wait_handle(lua_State *L) {\n    mcp_rcontext_t *rctx = lua_touserdata(L, 1);\n    int isnum = 0;\n    int handle = lua_tointegerx(L, 2, &isnum);\n\n    if (rctx->wait_mode != QWAIT_IDLE) {\n        proxy_lua_error(L, \"wait: cannot call while already in wait mode\");\n        return 0;\n    }\n\n    if (!isnum || handle < 0 || handle >= rctx->fgen->max_queues) {\n        proxy_lua_error(L, \"invalid handle passed to wait_handle\");\n        return 0;\n    }\n\n    struct mcp_rqueue_s *rqu = &rctx->qslots[handle];\n    if (rqu->state == RQUEUE_IDLE) {\n        proxy_lua_error(L, \"wait_handle called on unqueued handle\");\n        return 0;\n    }\n\n    if (lua_gettop(L) > 2) {\n        _mcplib_set_rctx_alarm(L, rctx, 3);\n    }\n\n    rctx->wait_done = 0;\n    rctx->wait_count = 1;\n    rctx->wait_mode = QWAIT_HANDLE;\n    rctx->wait_handle = handle;\n\n    lua_pushinteger(L, MCP_YIELD_WAITHANDLE);\n    return lua_yield(L, 1);\n}\n\n// TODO: This is disabled due to issues with the IO subsystem. Fixing this\n// requires retiring of API1 to allow refactoring or some extra roundtrip\n// work.\n// - if rctx:sleep() is called, the coroutine is suspended.\n// - once resumed after the timeout, we may later suspend again and make\n// backend requests\n// - once the coroutine is completed, we need to check if the owning client\n// conn is ready to be resumed\n// - BUG: we can only get into the \"conn is in IO queue wait\" state if a\n// sub-IO was created and submitted somewhere.\n// - This means either rctx:sleep() needs to be implemented by submitting a\n// dummy IO req (as other code does)\n// - OR we need to refactor the IO system so the dummies aren't required\n// anymore.\n//\n// If a dummy is used, we would have to implement this as:\n// - immediately submit a dummy IO if sleep is called.\n// - this allows the IO system to move the connection into the right state\n// - will immediately circle back then set an alarm for the sleep timeout\n// - once the sleep resumes, run code as normal. resumption should work\n// properly since we've entered the correct state originally.\n//\n// This adds a lot of CPU overhead to sleep, which should be fine given the\n// nature of the function, but also adds a lot of code and increases the\n// chances of bugs. So I'm leaving it out until this can be implemented more\n// simply.\nint mcplib_rcontext_sleep(lua_State *L) {\n    mcp_rcontext_t *rctx = lua_touserdata(L, 1);\n    if (rctx->wait_mode != QWAIT_IDLE) {\n        proxy_lua_error(L, \"sleep: cannot call while already in wait mode\");\n        return 0;\n    };\n\n    _mcplib_set_rctx_alarm(L, rctx, 2);\n    rctx->wait_mode = QWAIT_SLEEP;\n\n    lua_pushinteger(L, MCP_YIELD_SLEEP);\n    return lua_yield(L, 1);\n}\n\nstatic inline struct mcp_rqueue_s *_mcplib_rcontext_checkhandle(lua_State *L) {\n    mcp_rcontext_t *rctx = lua_touserdata(L, 1);\n    int isnum = 0;\n    int handle = lua_tointegerx(L, 2, &isnum);\n    if (!isnum || handle < 0 || handle >= rctx->fgen->max_queues) {\n        proxy_lua_error(L, \"invalid queue handle passed to :good/:ok:/:any\");\n        return NULL;\n    }\n\n    struct mcp_rqueue_s *rqu = &rctx->qslots[handle];\n    return rqu;\n}\n\nint mcplib_rcontext_res_good(lua_State *L) {\n    struct mcp_rqueue_s *rqu = _mcplib_rcontext_checkhandle(L);\n    if (rqu->flags & RQUEUE_R_GOOD) {\n        lua_rawgeti(L, LUA_REGISTRYINDEX, rqu->res_ref);\n    } else {\n        lua_pushnil(L);\n    }\n    return 1;\n}\n\nint mcplib_rcontext_res_ok(lua_State *L) {\n    struct mcp_rqueue_s *rqu = _mcplib_rcontext_checkhandle(L);\n    if (rqu->flags & (RQUEUE_R_OK|RQUEUE_R_GOOD)) {\n        lua_rawgeti(L, LUA_REGISTRYINDEX, rqu->res_ref);\n    } else {\n        lua_pushnil(L);\n    }\n    return 1;\n}\n\nint mcplib_rcontext_res_any(lua_State *L) {\n    struct mcp_rqueue_s *rqu = _mcplib_rcontext_checkhandle(L);\n    if (rqu->flags & (RQUEUE_R_ANY|RQUEUE_R_OK|RQUEUE_R_GOOD)) {\n        lua_rawgeti(L, LUA_REGISTRYINDEX, rqu->res_ref);\n    } else {\n        // Shouldn't be possible to get here, unless you're asking about a\n        // queue that was never armed or hasn't completed yet.\n        lua_pushnil(L);\n    }\n    return 1;\n}\n\n// returns res, RES_GOOD|OK|ANY\nint mcplib_rcontext_result(lua_State *L) {\n    struct mcp_rqueue_s *rqu = _mcplib_rcontext_checkhandle(L);\n    if (rqu->flags & (RQUEUE_R_ANY|RQUEUE_R_OK|RQUEUE_R_GOOD)) {\n        lua_rawgeti(L, LUA_REGISTRYINDEX, rqu->res_ref);\n        // mask away any other queue flags.\n        lua_pushinteger(L, rqu->flags & (RQUEUE_R_ANY|RQUEUE_R_OK|RQUEUE_R_GOOD));\n    } else {\n        lua_pushnil(L);\n        lua_pushnil(L);\n    }\n\n    return 2;\n}\n\nint mcplib_rcontext_cfd(lua_State *L) {\n    mcp_rcontext_t *rctx = lua_touserdata(L, 1);\n    lua_pushinteger(L, rctx->conn_fd);\n    return 1;\n}\n\n// Must not call this if rctx has returned result to client already.\nint mcplib_rcontext_tls_peer_cn(lua_State *L) {\n    mcp_rcontext_t *rctx = lua_touserdata(L, 1);\n    if (!rctx->c) {\n        lua_pushnil(L);\n        return 1;\n    }\n\n#ifdef TLS\n    int len = 0;\n    const unsigned char *cn = ssl_get_peer_cn(rctx->c, &len);\n    if (cn) {\n        lua_pushlstring(L, (const char *)cn, len);\n    } else {\n        lua_pushnil(L);\n    }\n#else\n    lua_pushnil(L);\n#endif\n    return 1;\n}\n\n// call with uobj on top of stack\nstatic void _mcplib_rcontext_ref_uobj(lua_State *L, mcp_rcontext_t *rctx, void *obj, int otype) {\n    lua_pushvalue(L, -1); // dupe rq for the rqueue slot\n    struct mcp_rqueue_s *rqu = &rctx->qslots[rctx->fgen->max_queues + rctx->uobj_count];\n    rctx->uobj_count++;\n    // hold the request reference into the rctx for memory management.\n    rqu->obj_ref = luaL_ref(L, LUA_REGISTRYINDEX);\n    rqu->obj_type = otype;\n    rqu->obj = obj;\n}\n\n// Creates request object that's tracked by request context so we can call\n// cleanup routines post-run.\nint mcplib_rcontext_request_new(lua_State *L) {\n    mcp_rcontext_t *rctx = lua_touserdata(L, 1);\n    if (rctx->uobj_count == rctx->fgen->uobj_queues) {\n        proxy_lua_error(L, \"rctx request new: object count limit reached\");\n        return 0;\n    }\n\n    // create new request object\n    mcp_parser_t pr = {0};\n    mcp_request_t *rq = mcp_new_request(L, &pr, \" \", 1);\n\n    _mcplib_rcontext_ref_uobj(L, rctx, rq, RQUEUE_TYPE_UOBJ_REQ);\n    return 1;\n}\n\nint mcplib_rcontext_response_new(lua_State *L) {\n    mcp_rcontext_t *rctx = lua_touserdata(L, 1);\n    if (rctx->uobj_count == rctx->fgen->uobj_queues) {\n        proxy_lua_error(L, \"rctx request new: object count limit reached\");\n        return 0;\n    }\n\n    mcp_resp_t *r = lua_newuserdatauv(L, sizeof(mcp_resp_t), 0);\n    memset(r, 0, sizeof(mcp_resp_t));\n    luaL_getmetatable(L, \"mcp.response\");\n    lua_setmetatable(L, -2);\n\n    _mcplib_rcontext_ref_uobj(L, rctx, r, RQUEUE_TYPE_UOBJ_RES);\n    return 1;\n}\n\n// the supplied handle must be valid.\nvoid mcp_rcontext_push_rqu_res(lua_State *L, mcp_rcontext_t *rctx, int handle) {\n    struct mcp_rqueue_s *rqu = &rctx->qslots[handle];\n    lua_rawgeti(L, LUA_REGISTRYINDEX, rqu->res_ref);\n}\n\n/*\n * Specialized router funcgen.\n * For routing a key across a map of possible function generators, we use a\n * specialized function generator. This is to keep the attach and start code\n * consistent, as they only need to think about function generators.\n * It also keeps the cleanup code consistent, as when a \"router\" funcgen is\n * replaced by mcp.attach() during a reload, we can immediately dereference\n * all of the route fgens, rather than have to wait for GC.\n *\n * Another upside is when we're starting a new request, we can immediately\n * swap out the top level fgen object, rather than force all routes to be\n * processed as sub-funcs, which is a tiny bit slower and disallows custom\n * request object sizes.\n *\n * The downside is this will appear to be bolted onto the side of the existing\n * structs rather than be its own object, like I initially wanted.\n */\n\nstatic inline const char *_mcp_router_shortsep(const char *key, const int klen, const char needle, size_t *len) {\n    const char *end = NULL;\n    const char *lookup = NULL;\n\n    end = memchr(key, needle, klen);\n    if (end == NULL) {\n        lookup = key;\n    } else {\n        lookup = key;\n        *len = end - key;\n    }\n\n    return lookup;\n}\n\n// we take some liberties here because we know needle and key can't be zero\n// this isn't the most hyper optimized search but prefixes and separators\n// should both be short.\nstatic inline const char *_mcp_router_longsep(const char *key, const int klen, const char *needle, size_t *len) {\n    const char *end = NULL;\n    const char *lookup = key;\n    size_t nlen = strlen(needle);\n\n    end = memchr(key, needle[0], klen);\n    if (end == NULL) {\n        // definitely no needle in this haystack.\n        return key;\n    }\n\n    // find the last possible position\n    const char *last = key + (klen - nlen);\n\n    while (end <= last) {\n        if (*end == needle[0] && memcmp(end, needle, nlen) == 0) {\n            lookup = key;\n            *len = end - key;\n            break;\n        }\n        end++;\n    }\n\n    return lookup;\n}\n\nstatic inline const char *_mcp_router_anchorsm(const char *key, const int klen, const char *needle, size_t *len) {\n    // check the first byte anchor.\n    if (key[0] != needle[0]) {\n        return NULL;\n    }\n\n    // rest is same as shortsep.\n    return _mcp_router_shortsep(key+1, klen-1, needle[1], len);\n}\n\nstatic inline const char *_mcp_router_anchorbig(const char *key, const int klen, const struct mcp_router_long_s *conf, size_t *len) {\n    // check long anchored prefix.\n    size_t slen = strlen(conf->start);\n    // check for start len+2 to avoid sending a zero byte haystack to longsep\n    if (slen+2 > klen || memcmp(key, conf->start, slen) != 0) {\n        return NULL;\n    }\n\n    // rest is same as longsep\n    return _mcp_router_longsep(key+slen, klen-slen, conf->stop, len);\n}\n\nstatic inline mcp_funcgen_t *_mcp_funcgen_route_fallback(struct mcp_funcgen_router *fr, int cmd) {\n    if (fr->cmap[cmd]) {\n        return fr->cmap[cmd];\n    }\n    return fr->def_fgen;\n}\n\nstatic mcp_funcgen_t *mcp_funcgen_route(lua_State *L, mcp_funcgen_t *fgen, mcp_parser_t *pr) {\n    struct mcp_funcgen_router *fr = (struct mcp_funcgen_router *)fgen;\n    if (pr->klen == 0) {\n        return NULL;\n    }\n    const char *key = &pr->request[pr->tokens[pr->keytoken]];\n    const char *lookup = NULL;\n    size_t lookuplen = 0;\n    switch(fr->type) {\n        case FGEN_ROUTER_NONE:\n            break;\n        case FGEN_ROUTER_CMDMAP:\n            // short circuit if all we can do is cmap and default.\n            return _mcp_funcgen_route_fallback(fr, pr->command);\n            break;\n        case FGEN_ROUTER_SHORTSEP:\n            lookup = _mcp_router_shortsep(key, pr->klen, fr->conf.sep, &lookuplen);\n            break;\n        case FGEN_ROUTER_LONGSEP:\n            lookup = _mcp_router_longsep(key, pr->klen, fr->conf.lsep, &lookuplen);\n            break;\n        case FGEN_ROUTER_ANCHORSM:\n            lookup = _mcp_router_anchorsm(key, pr->klen, fr->conf.anchorsm, &lookuplen);\n            break;\n        case FGEN_ROUTER_ANCHORBIG:\n            lookup = _mcp_router_anchorbig(key, pr->klen, &fr->conf.big, &lookuplen);\n            break;\n    }\n\n    if (lookuplen == 0) {\n        return _mcp_funcgen_route_fallback(fr, pr->command);\n    }\n\n    // hoping the lua short string cache helps us avoid allocations at least.\n    // since this lookup code is internal to the router object we can optimize\n    // this later and remove the lua bits.\n    lua_rawgeti(L, LUA_REGISTRYINDEX, fr->map_ref);\n    lua_pushlstring(L, lookup, lookuplen);\n    lua_rawget(L, -2); // pops key, returns value\n    if (lua_isnil(L, -1)) {\n        lua_pop(L, 2); // drop nil and map.\n        return _mcp_funcgen_route_fallback(fr, pr->command);\n    } else {\n        int type = lua_type(L, -1);\n        if (type == LUA_TUSERDATA) {\n            mcp_funcgen_t *nfgen = lua_touserdata(L, -1);\n            lua_pop(L, 2); // drop fgen and map.\n            return nfgen;\n        } else if (type == LUA_TTABLE) {\n            lua_rawgeti(L, -1, pr->command);\n            // If nil, check CMD_ANY_STORAGE index for a cmap default\n            if (lua_isnil(L, -1)) {\n                lua_pop(L, 1); // drop nil.\n                // check if we have a local-default\n                lua_rawgeti(L, -1, CMD_ANY_STORAGE);\n                if (lua_isnil(L, -1)) {\n                    lua_pop(L, 3); // drop map, cmd map, nil\n                    return _mcp_funcgen_route_fallback(fr, pr->command);\n                } else {\n                    mcp_funcgen_t *nfgen = lua_touserdata(L, -1);\n                    lua_pop(L, 3); // drop map, cmd map, fgen\n                    return nfgen;\n                }\n            }\n            mcp_funcgen_t *nfgen = lua_touserdata(L, -1);\n            lua_pop(L, 3); // drop fgen, cmd map, map\n            return nfgen;\n        } else {\n            return _mcp_funcgen_route_fallback(fr, pr->command);\n        }\n    }\n}\n\n// called from mcp_funcgen_cleanup if necessary.\nstatic int mcp_funcgen_router_cleanup(lua_State *L, mcp_funcgen_t *fgen) {\n    struct mcp_funcgen_router *fr = (struct mcp_funcgen_router *)fgen;\n    if (fr->map_ref) {\n        lua_rawgeti(L, LUA_REGISTRYINDEX, fr->map_ref);\n\n        // walk the map, de-ref any funcgens found.\n        int tidx = lua_absindex(L, -1);\n        lua_pushnil(L);\n        while (lua_next(L, tidx) != 0) {\n            int type = lua_type(L, -1);\n            if (type == LUA_TUSERDATA) {\n                mcp_funcgen_t *mfgen = lua_touserdata(L, -1);\n                mcp_funcgen_dereference(L, mfgen);\n                lua_pop(L, 1);\n            } else if (type == LUA_TTABLE) {\n                int midx = lua_absindex(L, -1);\n                lua_pushnil(L);\n                while (lua_next(L, midx) != 0) {\n                    mcp_funcgen_t *mfgen = lua_touserdata(L, -1);\n                    mcp_funcgen_dereference(L, mfgen);\n                    lua_pop(L, 1); // drop value\n                }\n                lua_pop(L, 1); // drop command map table\n            }\n        }\n\n        lua_pop(L, 1); // drop the table.\n        luaL_unref(L, LUA_REGISTRYINDEX, fr->map_ref);\n        fr->map_ref = 0;\n    }\n\n    // release any command map entries.\n    for (int x = 0; x < CMD_END_STORAGE; x++) {\n        if (fr->cmap[x]) {\n            mcp_funcgen_dereference(L, fr->cmap[x]);\n            fr->cmap[x] = NULL;\n        }\n    }\n\n    if (fr->def_fgen) {\n        mcp_funcgen_dereference(L, fr->def_fgen);\n        fr->def_fgen = NULL;\n    }\n\n    return 0;\n}\n\n// Note: the string should be safe to use after popping it here, because we\n// were fetching it from a table, but I might consider copying it into a\n// buffer from the caller first.\nstatic const char *_mcplib_router_new_check(lua_State *L, const char *arg, size_t *len) {\n    int type = lua_getfield(L, 1, arg);\n    if (type == LUA_TSTRING) {\n        const char *sep = lua_tolstring(L, -1, len);\n        if (*len == 0) {\n            proxy_lua_ferror(L, \"must pass a non-zero length string to %s in mcp.router_new\", arg);\n        } else if (*len > KEY_HASH_FILTER_MAX) {\n            proxy_lua_ferror(L, \"%s is too long in mcp.router_new\", arg);\n        }\n        lua_pop(L, 1); // drop key\n        return sep;\n    } else if (type != LUA_TNIL) {\n        proxy_lua_ferror(L, \"must pass a string to %s in mcp.router_new\", arg);\n    }\n    return NULL;\n}\n\nstatic void _mcplib_router_new_cmapcheck(lua_State *L) {\n    int tidx = lua_absindex(L, -1);\n    lua_pushnil(L); // init next table key.\n    while (lua_next(L, tidx) != 0) {\n        if (!lua_isinteger(L, -2)) {\n            proxy_lua_error(L, \"Non integer key in router command map in router_new\");\n        }\n        int cmd = lua_tointeger(L, -2);\n        if ((cmd <= 0 || cmd >= CMD_END_STORAGE) && cmd != CMD_ANY_STORAGE) {\n            proxy_lua_error(L, \"Bad command in router command map in router_new\");\n        }\n        luaL_checkudata(L, -1, \"mcp.funcgen\");\n        lua_pop(L, 1); // drop val, keep key.\n    }\n}\n\nstatic size_t _mcplib_router_new_mapcheck(lua_State *L) {\n    size_t route_count = 0;\n    if (!lua_istable(L, -1)) {\n        proxy_lua_error(L, \"Must pass a table to map argument of router_new\");\n    }\n    // walk map table, get size count.\n    lua_pushnil(L); // init table key.\n    while (lua_next(L, 2) != 0) {\n        int type = lua_type(L, -1);\n        if (type == LUA_TUSERDATA) {\n            luaL_checkudata(L, -1, \"mcp.funcgen\");\n        } else if (type == LUA_TTABLE) {\n            // If table, it's a command map, poke in and validate.\n            _mcplib_router_new_cmapcheck(L);\n        } else {\n            proxy_lua_error(L, \"unhandled data in router_new map\");\n        }\n        route_count++;\n        lua_pop(L, 1); // drop val, keep key.\n    }\n\n    return route_count;\n}\n\n// reads the configuration for the router based on the mode.\nstatic void _mcplib_router_new_mode(lua_State *L, struct mcp_funcgen_router *fr) {\n    const char *type = lua_tostring(L, -1);\n    size_t len = 0;\n    const char *sep = NULL;\n\n    // change internal type based on length of separator\n    if (strcmp(type, \"prefix\") == 0) {\n        sep = _mcplib_router_new_check(L, \"stop\", &len);\n        if (sep == NULL) {\n            // defaults\n            fr->type = FGEN_ROUTER_SHORTSEP;\n            fr->conf.sep = '/';\n        } else if (len == 1) {\n            // optimized shortsep case.\n            fr->type = FGEN_ROUTER_SHORTSEP;\n            fr->conf.sep = sep[0];\n        } else {\n            // len is long.\n            fr->type = FGEN_ROUTER_LONGSEP;\n            memcpy(fr->conf.lsep, sep, len);\n            fr->conf.lsep[len] = '\\0'; // cap it.\n        }\n    } else if (strcmp(type, \"anchor\") == 0) {\n        size_t elen = 0; // stop len.\n        const char *usep = _mcplib_router_new_check(L, \"stop\", &elen);\n        sep = _mcplib_router_new_check(L, \"start\", &len);\n        if (sep == NULL && usep == NULL) {\n            // no arguments, use a default.\n            fr->type = FGEN_ROUTER_ANCHORSM;\n            fr->conf.anchorsm[0] = '/';\n            fr->conf.anchorsm[1] = '/';\n        } else if (sep == NULL || usep == NULL) {\n            // reduce the combinatorial space because I'm lazy.\n            proxy_lua_error(L, \"must specify start and stop if mode is anchor in mcp.router_new\");\n        } else if (len == 1 && elen == 1) {\n            fr->type = FGEN_ROUTER_ANCHORSM;\n            fr->conf.anchorsm[0] = sep[0];\n            fr->conf.anchorsm[1] = usep[0];\n        } else {\n            fr->type = FGEN_ROUTER_ANCHORBIG;\n            memcpy(fr->conf.big.start, sep, len);\n            memcpy(fr->conf.big.stop, usep, elen);\n            fr->conf.big.start[len] = '\\0';\n            fr->conf.big.stop[elen] = '\\0';\n        }\n    } else {\n        proxy_lua_error(L, \"unknown type passed to mcp.router_new\");\n    }\n}\n\n// FIXME: error if map or cmap not passed in?\nint mcplib_router_new(lua_State *L) {\n    struct mcp_funcgen_router fr = {0};\n    size_t route_count = 0;\n    bool has_map = false;\n\n    if (!lua_istable(L, 1)) {\n        proxy_lua_error(L, \"Must pass a table of arguments to mcp.router_new\");\n    }\n\n    if (lua_getfield(L, 1, \"map\") != LUA_TNIL) {\n        route_count = _mcplib_router_new_mapcheck(L);\n        has_map = true;\n    }\n    lua_pop(L, 1); // drop map or nil\n\n    if (lua_getfield(L, 1, \"cmap\") != LUA_TNIL) {\n        if (!lua_istable(L, -1)) {\n            proxy_lua_error(L, \"Must pass a table to cmap argument of mcp.router_new\");\n        }\n        _mcplib_router_new_cmapcheck(L);\n    } else {\n        if (!has_map) {\n            proxy_lua_error(L, \"Must pass map and/or cmap to mcp.router_new\");\n        }\n    }\n    lua_pop(L, 1);\n\n    fr.fgen_self.is_router = true;\n\n    // config:\n    // { mode = \"anchor\", start = \"/\", stop = \"/\" }\n    // { mode = \"prefix\", stop = \"/\" }\n    if (has_map) {\n        // default to a short prefix type with a single byte separator.\n        fr.type = FGEN_ROUTER_SHORTSEP;\n        fr.conf.sep = '/';\n\n        if (lua_getfield(L, 1, \"mode\") == LUA_TSTRING) {\n            _mcplib_router_new_mode(L, &fr);\n        }\n        lua_pop(L, 1); // drop mode or nil.\n    } else {\n        // pure command map router.\n        fr.type = FGEN_ROUTER_CMDMAP;\n    }\n\n    struct mcp_funcgen_router *router = lua_newuserdatauv(L, sizeof(struct mcp_funcgen_router), 0);\n    memset(router, 0, sizeof(*router));\n    mcp_funcgen_t *fgen = &router->fgen_self;\n\n    luaL_getmetatable(L, \"mcp.funcgen\");\n    lua_setmetatable(L, -2);\n\n    int type = lua_getfield(L, 1, \"default\");\n    if (type == LUA_TUSERDATA) {\n        fr.def_fgen = luaL_checkudata(L, -1, \"mcp.funcgen\");\n        mcp_funcgen_reference(L); // pops the funcgen.\n    } else {\n        lua_pop(L, 1);\n    }\n\n    memcpy(router, &fr, sizeof(struct mcp_funcgen_router));\n    strncpy(fgen->name, \"mcp_router\", FGEN_NAME_MAXLEN);\n\n    if (has_map) {\n        // walk map table again, funcgen_ref everyone.\n        lua_createtable(L, 0, route_count);\n        lua_pushvalue(L, -1); // dupe table ref for a moment.\n        router->map_ref = luaL_ref(L, LUA_REGISTRYINDEX); // pops extra map\n\n        int mymap = lua_absindex(L, -1);\n        lua_getfield(L, 1, \"map\");\n        int argmap = lua_absindex(L, -1);\n        lua_pushnil(L); // seed walk of the passed in map\n\n        while (lua_next(L, argmap) != 0) {\n            // types are already validated.\n            int type = lua_type(L, -1);\n            if (type == LUA_TUSERDATA) {\n                // first lets reference the function generator.\n                lua_pushvalue(L, -1); // duplicate value.\n                mcp_funcgen_reference(L); // pops the funcgen after referencing.\n\n                // duplicate key.\n                lua_pushvalue(L, -2);\n                // move key underneath value\n                lua_insert(L, -2); // take top (key) and move it down one.\n                // now key, key, value\n                lua_rawset(L, mymap); // pops k, v into our internal table.\n            } else if (type == LUA_TTABLE) {\n                int tidx = lua_absindex(L, -1); // idx of our command map table.\n                lua_createtable(L, CMD_END_STORAGE, 0);\n                int midx = lua_absindex(L, -1); // idx of our new command map.\n                lua_pushnil(L); // seed the iterator\n                while (lua_next(L, tidx) != 0) {\n                    lua_pushvalue(L, -1); // duplicate value.\n                    mcp_funcgen_reference(L); // pop funcgen.\n\n                    lua_pushvalue(L, -2); // dupe key.\n                    lua_insert(L, -2); // move key down one.\n                    lua_rawset(L, midx); // set to new map table.\n                }\n\n                // -1: new command map\n                // -2: input command map\n                // -3: key\n                lua_pushvalue(L, -3); // dupe key\n                lua_insert(L, -2); // move key down below new cmd map\n                lua_rawset(L, mymap); // pop key, new map into main map.\n                lua_pop(L, 1); // drop input table.\n            }\n        }\n\n        lua_pop(L, 2); // drop argmap, mymap.\n    }\n\n    // process a command map directly into our internal table.\n    if (lua_getfield(L, 1, \"cmap\") != LUA_TNIL) {\n        int tidx = lua_absindex(L, -1); // idx of our command map table.\n        lua_pushnil(L); // seed the iterator\n        while (lua_next(L, tidx) != 0) {\n            int cmd = lua_tointeger(L, -2);\n            mcp_funcgen_t *cfgen = lua_touserdata(L, -1);\n            mcp_funcgen_reference(L); // pop funcgen.\n            router->cmap[cmd] = cfgen;\n        }\n    }\n    lua_pop(L, 1);\n\n    LIBEVENT_THREAD *t = PROXY_GET_THR(L);\n    fgen->thread = t;\n    mcp_sharedvm_delta(t->proxy_ctx, SHAREDVM_FGEN_IDX, \"mcp_router\", 1);\n\n    return 1;\n}\n"
        },
        {
          "name": "proxy_mutator.c",
          "type": "blob",
          "size": 33.9462890625,
          "content": "/* -*- Mode: C; tab-width: 4; c-basic-offset: 4; indent-tabs-mode: nil -*- */\n\n#include \"proxy.h\"\n\n/*\n * !!!WARNING!!!\n * This is an experimental interface and is not to be used in production until\n * after this warning has been removed.\n * The req/res mutator system is currently an experimental draft, merged to\n * allow code experiments and further information gathering before completing\n * the interface.\n *\n */\n\n// space or \\r\\n\n#define MIN_BUF_SPACE 2\n\nenum mcp_mut_type {\n    MUT_REQ = 1,\n    MUT_RES,\n};\n\nenum mcp_mut_steptype {\n    mcp_mut_step_none = 0,\n    mcp_mut_step_cmdset,\n    mcp_mut_step_cmdcopy,\n    mcp_mut_step_keycopy,\n    mcp_mut_step_keyset,\n    mcp_mut_step_rescodeset,\n    mcp_mut_step_rescodecopy,\n    mcp_mut_step_reserr,\n    mcp_mut_step_flagset,\n    mcp_mut_step_flagcopy,\n    mcp_mut_step_valcopy,\n    mcp_mut_step_final, // not used.\n};\n\n// START STEP STRUCTS\n\n// struct forward declarations for entry/step function pointers\nstruct mcp_mut_step;\nstruct mcp_mutator;\nstruct mcp_mut_run;\nstruct mcp_mut_part;\n\ntypedef int (*mcp_mut_c)(lua_State *L, int tidx);\ntypedef int (*mcp_mut_i)(lua_State *L, int tidx, int sc, struct mcp_mutator *mut);\ntypedef int (*mcp_mut_n)(struct mcp_mut_run *run, struct mcp_mut_step *s, struct mcp_mut_part *p);\ntypedef void (*mcp_mut_r)(struct mcp_mut_run *run, struct mcp_mut_step *s, struct mcp_mut_part *p);\n\nstruct mcp_mut_entry {\n    const char *s; // string name\n    mcp_mut_c c; // argument checker\n    mcp_mut_i i; // argument initializer\n    mcp_mut_n n; // runtime length totaller\n    mcp_mut_r r; // runtime assembly\n    unsigned int t; // allowed object types\n    int rc; // number of results to expect\n};\n\nstruct mcp_mut_string {\n    unsigned int str; // arena offset for match string.\n    unsigned int len;\n};\n\nstruct mcp_mut_flag {\n    uint64_t bit; // flag converted for bitmask test\n    char f;\n};\n\n#define RESERR_ERROR 1\n#define RESERR_ERROR_STR \"ERROR\"\n#define RESERR_CLIENT 2\n#define RESERR_CLIENT_STR \"CLIENT_ERROR\"\n#define RESERR_SERVER 3\n#define RESERR_SERVER_STR \"SERVER_ERROR\"\n\nstruct mcp_mut_flagval {\n    struct mcp_mut_flag flag;\n    struct mcp_mut_string str;\n};\n\nstruct mcp_mut_step {\n    enum mcp_mut_steptype type;\n    unsigned int idx; // common: input argument position\n    mcp_mut_n n; // totaller function\n    mcp_mut_r r; // data copy function\n    union {\n        struct mcp_mut_string string;\n        struct mcp_mut_flag flag;\n        struct mcp_mut_flagval flagval;\n    } c;\n};\n\n// END STEP STRUCTS\n\nstruct mcp_mutator {\n    enum mcp_mut_type type;\n    int scount;\n    unsigned int aused; // arena memory used\n    unsigned int rcount; // number of results to expect\n    char *arena; // string/data storage for steps\n    struct mcp_mut_step steps[];\n};\n\n// scratch space for steps between total and execution stages.\nstruct mcp_mut_part {\n    const char *src;\n    size_t slen;\n};\n\n// stack scratch variables for mutation execution\nstruct mcp_mut_run {\n    lua_State *L;\n    struct mcp_mutator *mut;\n    void *arg;\n    char *numbuf; // stack space for rendering numerics\n    char *d_pos; // current offset to the write string.\n\n    const char *vbuf; // buffer or ptr if a value is being attached\n    size_t vlen; // length of the actual value buffer.\n};\n\n#define mut_step_c(n) static int mcp_mutator_##n##_c(lua_State *L, int tidx)\n#define mut_step_i(n) static int mcp_mutator_##n##_i(lua_State *L, int tidx, int sc, struct mcp_mutator *mut)\n#define mut_step_r(n) static void mcp_mutator_##n##_r(struct mcp_mut_run *run, struct mcp_mut_step *s, struct mcp_mut_part *p)\n#define mut_step_n(n) static int mcp_mutator_##n##_n(struct mcp_mut_run *run, struct mcp_mut_step *s, struct mcp_mut_part *p)\n\n// PRIVATE INTERFACE\n\n// COMMON ARG HANDLERS\n\nstatic void _mut_check_idx(lua_State *L, int tidx) {\n    if (lua_getfield(L, tidx, \"idx\") != LUA_TNIL) {\n        luaL_checktype(L, -1, LUA_TNUMBER);\n        int isnum = 0;\n        lua_Integer i = lua_tointegerx(L, -1, &isnum);\n        if (!isnum) {\n            proxy_lua_ferror(L, \"mutator step %d: must provide 'idx' argument as an integer\", tidx);\n        }\n        if (i < 1) {\n            proxy_lua_ferror(L, \"mutator step %d: 'idx' argument must be greater than 0\", tidx);\n        }\n    } else {\n        proxy_lua_ferror(L, \"mutator step %d: must provide 'idx' argument\", tidx);\n    }\n    lua_pop(L, 1);\n}\n\nstatic size_t _mut_check_strlen(lua_State *L, int tidx, const char *n) {\n    size_t len = 0;\n\n    if (lua_getfield(L, tidx, n) != LUA_TNIL) {\n        lua_tolstring(L, -1, &len);\n    } else {\n        proxy_lua_ferror(L, \"mutator step %d: must provide '%s' argument\", tidx, n);\n    }\n    lua_pop(L, 1);\n\n    return len;\n}\n\nstatic void _mut_check_flag(lua_State *L, int tidx) {\n    if (lua_getfield(L, tidx, \"flag\") != LUA_TNIL) {\n        size_t len = 0;\n        const char *flag = lua_tolstring(L, -1, &len);\n        if (len != 1) {\n            proxy_lua_ferror(L, \"mutator step %d: 'flag' must be a single character\", tidx);\n        }\n        if (mcp_is_flag_invalid(flag[0])) {\n            proxy_lua_ferror(L, \"mutator step %d: 'flag' must be alphanumeric\", tidx);\n        }\n    } else {\n        proxy_lua_ferror(L, \"mutator step %d: must provide 'flag' argument\", tidx);\n    }\n    lua_pop(L, 1); // val or nil\n}\n\nstatic void _mut_init_flag(lua_State *L, int tidx, struct mcp_mut_flag *c) {\n    if (lua_getfield(L, tidx, \"flag\") != LUA_TNIL) {\n        const char *flag = lua_tostring(L, -1);\n        c->f = flag[0];\n        c->bit = (uint64_t)1 << (c->f - 65);\n    }\n    lua_pop(L, 1); // val or nil\n}\n\n// mutator must be at position 1. index to check at idx.\n// pulls metatables and compares them to see if we have a request or response\n// object. then fills the appropriate pointer and returns.\nstatic inline void _mut_checkudata(lua_State *L, unsigned int idx, mcp_request_t **srq, mcp_resp_t **srs) {\n    lua_getmetatable(L, idx);\n    lua_getiuservalue(L, 1, MUT_REQ);\n    if (lua_rawequal(L, -1, -2)) {\n        lua_pop(L, 2);\n        *srq = lua_touserdata(L, idx);\n        return;\n    }\n    lua_getiuservalue(L, 1, MUT_RES);\n    if (lua_rawequal(L, -1, -3)) {\n        *srs = lua_touserdata(L, idx);\n    }\n    lua_pop(L, 3);\n}\n\n// END COMMMON ARG HANDLERS\n\n// START STEPS\n\nmut_step_c(cmdset) {\n    size_t len = 0;\n\n    if (lua_getfield(L, tidx, \"cmd\") != LUA_TNIL) {\n        lua_tolstring(L, -1, &len);\n        // TODO: know either exact max length or parse for valid commands\n        // this small sanity check should help against user error for now.\n        if (len > 20) {\n            proxy_lua_ferror(L, \"mutator step %d: 'cmd' too long\", tidx);\n        }\n    } else {\n        proxy_lua_ferror(L, \"mutator step %d: must provide 'cmd' argument\", tidx);\n    }\n    lua_pop(L, 1);\n\n    return len;\n}\n\nmut_step_i(cmdset) {\n    struct mcp_mut_step *s = &mut->steps[sc];\n    struct mcp_mut_string *c = &s->c.string;\n    size_t len = 0;\n\n    if (lua_getfield(L, tidx, \"cmd\") != LUA_TNIL) {\n        const char *cmd = lua_tolstring(L, -1, &len);\n        c->str = mut->aused;\n        c->len = len;\n        char *a = mut->arena + mut->aused;\n        memcpy(a, cmd, len);\n        mut->aused += len;\n    }\n    lua_pop(L, 1);\n\n    return 0;\n}\n\nmut_step_n(cmdset) {\n    struct mcp_mut_string *c = &s->c.string;\n    return c->len;\n}\n\nmut_step_r(cmdset) {\n    struct mcp_mutator *mut = run->mut;\n    struct mcp_mut_string *c = &s->c.string;\n\n    const char *str = mut->arena + c->str;\n\n    memcpy(run->d_pos, str, c->len);\n    run->d_pos += c->len;\n}\n\nmut_step_c(cmdcopy) {\n    _mut_check_idx(L, tidx);\n    return 0;\n}\n\nmut_step_i(cmdcopy) {\n    struct mcp_mut_step *s = &mut->steps[sc];\n    if (lua_getfield(L, tidx, \"idx\") != LUA_TNIL) {\n        s->idx = lua_tointeger(L, -1);\n    }\n    lua_pop(L, 1);\n\n    return 0;\n}\n\nmut_step_n(cmdcopy) {\n    lua_State *L = run->L;\n    unsigned idx = s->idx;\n    mcp_request_t *srq = NULL;\n    mcp_resp_t *srs = NULL;\n\n    switch (lua_type(L, idx)) {\n        case LUA_TSTRING:\n            p->src = lua_tolstring(L, idx, &p->slen);\n            break;\n        case LUA_TUSERDATA:\n            _mut_checkudata(L, idx, &srq, &srs);\n            p->slen = 0;\n            if (srq) {\n                // command must be at the start\n                const char *cmd = srq->pr.request;\n                // command ends at the first token\n                int clen = srq->pr.tokens[1];\n                if (cmd[clen] == ' ') {\n                    clen--;\n                }\n                p->src = cmd;\n                p->slen = clen;\n            } else {\n                // can only use a request object\n                return -1;\n            }\n            break;\n        default:\n            // ints/etc unsupported\n            return -1;\n    }\n\n    return p->slen;\n}\n\nmut_step_r(cmdcopy) {\n    memcpy(run->d_pos, p->src, p->slen);\n    run->d_pos += p->slen;\n}\n\n// TODO: validate a cmd is already slated to be set\n// NOTE: we might need to know the integer CMD because key position can move\n// with the stupid GAT command.\nmut_step_c(keycopy) {\n    _mut_check_idx(L, tidx);\n    return 0;\n}\n\nmut_step_i(keycopy) {\n    struct mcp_mut_step *s = &mut->steps[sc];\n    if (lua_getfield(L, tidx, \"idx\") != LUA_TNIL) {\n        s->idx = lua_tointeger(L, -1);\n    }\n    lua_pop(L, 1);\n    return 0;\n}\n\nmut_step_n(keycopy) {\n    lua_State *L = run->L;\n    unsigned idx = s->idx;\n    mcp_request_t *srq = NULL;\n    mcp_resp_t *srs = NULL;\n\n    switch (lua_type(L, idx)) {\n        case LUA_TSTRING:\n            p->src = lua_tolstring(L, idx, &p->slen);\n            break;\n        case LUA_TUSERDATA:\n            _mut_checkudata(L, idx, &srq, &srs);\n            if (srq) {\n                p->src = MCP_PARSER_KEY(srq->pr);\n                p->slen = srq->pr.klen;\n            } else {\n                // TODO: if a result object:\n                // - if meta, and 'k' flag, copy flag token.\n                return -1;\n            }\n            break;\n        default:\n            // ints/etc unsupported\n            return -1;\n    }\n\n    return p->slen;\n}\n\nmut_step_r(keycopy) {\n    memcpy(run->d_pos, p->src, p->slen);\n    run->d_pos += p->slen;\n}\n\nmut_step_c(keyset) {\n    size_t len = 0;\n\n    if (lua_getfield(L, tidx, \"val\") != LUA_TNIL) {\n        lua_tolstring(L, -1, &len);\n        if (len < 1) {\n            proxy_lua_ferror(L, \"mutator step %d: 'val' must have nonzero length\", tidx);\n        }\n    } else {\n        proxy_lua_ferror(L, \"mutator step %d: must provide 'val' argument\", tidx);\n    }\n    lua_pop(L, 1); // val or nil\n\n    return len;\n}\n\nmut_step_i(keyset) {\n    struct mcp_mut_step *s = &mut->steps[sc];\n    struct mcp_mut_string *c = &s->c.string;\n    size_t len = 0;\n\n    // store our match string in the arena space that we reserved before.\n    if (lua_getfield(L, tidx, \"val\") != LUA_TNIL) {\n        const char *str = lua_tolstring(L, -1, &len);\n        c->str = mut->aused;\n        c->len = len;\n        char *a = mut->arena + mut->aused;\n        memcpy(a, str, len);\n        mut->aused += len;\n    }\n    lua_pop(L, 1); // val or nil\n\n    return len;\n}\n\nmut_step_n(keyset) {\n    struct mcp_mut_string *c = &s->c.string;\n    return c->len;\n}\n\nmut_step_r(keyset) {\n    struct mcp_mutator *mut = run->mut;\n    struct mcp_mut_string *c = &s->c.string;\n\n    const char *str = mut->arena + c->str;\n\n    memcpy(run->d_pos, str, c->len);\n    run->d_pos += c->len;\n}\n\n// TODO: pre-validate that it's an accepted code?\nmut_step_c(rescodeset) {\n    return _mut_check_strlen(L, tidx, \"val\");\n}\n\nmut_step_i(rescodeset) {\n    struct mcp_mut_step *s = &mut->steps[sc];\n    struct mcp_mut_string *c = &s->c.string;\n    size_t len = 0;\n\n    if (lua_getfield(L, tidx, \"val\") != LUA_TNIL) {\n        const char *str = lua_tolstring(L, -1, &len);\n        c->str = mut->aused;\n        c->len = len;\n        char *a = mut->arena + mut->aused;\n        memcpy(a, str, len);\n        mut->aused += len;\n    }\n    lua_pop(L, 1);\n\n    return 0;\n}\n\nmut_step_n(rescodeset) {\n    struct mcp_mut_string *c = &s->c.string;\n    return c->len;\n}\n\nmut_step_r(rescodeset) {\n    struct mcp_mutator *mut = run->mut;\n    struct mcp_mut_string *c = &s->c.string;\n\n    const char *str = mut->arena + c->str;\n\n    memcpy(run->d_pos, str, c->len);\n    run->d_pos += c->len;\n}\n\nmut_step_c(rescodecopy) {\n    _mut_check_idx(L, tidx);\n    return 0;\n}\n\nmut_step_i(rescodecopy) {\n    struct mcp_mut_step *s = &mut->steps[sc];\n    if (lua_getfield(L, tidx, \"idx\") != LUA_TNIL) {\n        s->idx = lua_tointeger(L, -1);\n    }\n    lua_pop(L, 1);\n\n    return 0;\n}\n\nmut_step_n(rescodecopy) {\n    lua_State *L = run->L;\n    unsigned idx = s->idx;\n    mcp_request_t *srq = NULL;\n    mcp_resp_t *srs = NULL;\n\n    switch (lua_type(L, idx)) {\n        case LUA_TSTRING:\n            p->src = lua_tolstring(L, idx, &p->slen);\n            break;\n        case LUA_TUSERDATA:\n            _mut_checkudata(L, idx, &srq, &srs);\n            if (srs) {\n                // can't recover the exact code from the mcmc resp object, so lets make\n                // sure it's tokenized and copy the first token.\n                if (srs->resp.type == MCMC_RESP_META) {\n                    mcmc_tokenize_res(srs->buf, srs->resp.reslen, &srs->tok);\n                } else {\n                    // FIXME: only supports meta responses\n                    return -1;\n                }\n                int len = 0;\n                p->src = mcmc_token_get(srs->buf, &srs->tok, 0, &len);\n                p->slen = len;\n                if (len < 2) {\n                    return -1;\n                }\n            } else {\n                return -1;\n            }\n            break;\n        default:\n            // ints/etc unsupported\n            return -1;\n    }\n\n    return p->slen;\n}\n\nmut_step_r(rescodecopy) {\n    memcpy(run->d_pos, p->src, p->slen);\n    run->d_pos += p->slen;\n}\n\n// TODO: can be no other steps after an error is set.\n// TODO: if idx given instead of msg copy an input string.\nmut_step_c(reserr) {\n    size_t total = 0;\n    char *code = NULL;\n\n    // FIXME: add code length to len\n    if (lua_getfield(L, tidx, \"code\") != LUA_TNIL) {\n        const char *val = lua_tostring(L, -1);\n\n        if (strcmp(val, \"error\") == 0) {\n            code = RESERR_ERROR_STR;\n        } else if (strcmp(val, \"server\") == 0) {\n            code = RESERR_SERVER_STR;\n        } else if (strcmp(val, \"client\") == 0) {\n            code = RESERR_CLIENT_STR;\n        } else {\n            proxy_lua_ferror(L, \"mutator step %d: code must be 'error', server', or 'client'\", tidx);\n        }\n\n        total += strlen(code);\n    } else {\n        proxy_lua_ferror(L, \"mutator step %d: must provide 'code' argument\", tidx);\n    }\n    lua_pop(L, 1);\n\n    if (lua_getfield(L, tidx, \"msg\") != LUA_TNIL) {\n        size_t len = 0;\n        lua_tolstring(L, -1, &len);\n        if (len < 1) {\n            proxy_lua_ferror(L, \"mutator step %d: 'msg' must be a nonzero length string\", tidx);\n        }\n        total += len + 1; // room for space between code and msg\n    }\n    lua_pop(L, 1);\n\n    return total;\n}\n\nmut_step_i(reserr) {\n    struct mcp_mut_step *s = &mut->steps[sc];\n    struct mcp_mut_string *c = &s->c.string;\n    size_t len = 0;\n    const char *code = NULL;\n\n    char *a = mut->arena + mut->aused;\n    if (lua_getfield(L, tidx, \"code\") != LUA_TNIL) {\n        const char *val = lua_tostring(L, -1);\n        if (strcmp(val, \"error\") == 0) {\n            code = RESERR_ERROR_STR;\n        } else if (strcmp(val, \"server\") == 0) {\n            code = RESERR_SERVER_STR;\n        } else if (strcmp(val, \"client\") == 0) {\n            code = RESERR_CLIENT_STR;\n        } else {\n            // shouldn't be possible\n            proxy_lua_ferror(L, \"mutator step %d: code must be 'error', server', or 'client'\", tidx);\n        }\n\n        // Start our string here.\n        c->str = mut->aused;\n        size_t clen = strlen(code);\n        memcpy(a, code, clen);\n        a += clen;\n        *a = ' ';\n        a++;\n        clen++;\n        mut->aused += clen;\n        c->len += clen;\n    } else {\n        proxy_lua_ferror(L, \"mutator step %d: must provide 'code' argument\", tidx);\n    }\n    lua_pop(L, 1);\n\n    if (lua_getfield(L, tidx, \"msg\") != LUA_TNIL) {\n        // Extend the string with a msg if supplied.\n        const char *str = lua_tolstring(L, -1, &len);\n        c->len += len;\n        memcpy(a, str, len);\n        mut->aused += len;\n    } else {\n        // TODO: note no error msg\n    }\n    lua_pop(L, 1);\n\n    return c->len;\n}\n\nmut_step_n(reserr) {\n    struct mcp_mut_string *c = &s->c.string;\n    return c->len;\n}\n\nmut_step_r(reserr) {\n    struct mcp_mutator *mut = run->mut;\n    struct mcp_mut_string *c = &s->c.string;\n\n    const char *str = mut->arena + c->str;\n    int len = c->len;\n\n    // set error code first\n    memcpy(run->d_pos, str, len);\n    run->d_pos += len;\n}\n\n// TODO: track which flags we've already set and error on dupes\nmut_step_c(flagset) {\n    _mut_check_flag(L, tidx);\n    size_t len = 0;\n\n    int vtype = lua_getfield(L, tidx, \"val\");\n    if (vtype == LUA_TNUMBER || vtype == LUA_TSTRING) {\n        // this converts the arg into a string for us.\n        lua_tolstring(L, -1, &len);\n    } else if (vtype != LUA_TNIL) {\n        proxy_lua_ferror(L, \"mutator step %d: unsupported type for 'val'\", tidx);\n    }\n    lua_pop(L, 1);\n\n    return len;\n}\n\nmut_step_i(flagset) {\n    struct mcp_mut_step *s = &mut->steps[sc];\n    struct mcp_mut_flagval *c = &s->c.flagval;\n    size_t len = 0;\n\n    _mut_init_flag(L, tidx, &c->flag);\n    if (lua_getfield(L, tidx, \"val\") != LUA_TNIL) {\n        const char *str = lua_tolstring(L, -1, &len);\n        c->str.str = mut->aused;\n        c->str.len = len;\n        char *a = mut->arena + mut->aused;\n        memcpy(a, str, len);\n        mut->aused += len;\n    }\n    lua_pop(L, 1);\n\n    return len;\n}\n\nmut_step_n(flagset) {\n    struct mcp_mut_flagval *c = &s->c.flagval;\n    return c->str.len + 1; // room for flag\n}\n\n// FIXME: triple check that this is actually the same code for req vs res?\n// seems like it is.\nmut_step_r(flagset) {\n    struct mcp_mutator *mut = run->mut;\n    struct mcp_mut_flagval *c = &s->c.flagval;\n\n    const char *str = mut->arena + c->str.str;\n    int len = c->str.len;\n\n    *(run->d_pos) = c->flag.f;\n    run->d_pos++;\n    if (len > 0) {\n        memcpy(run->d_pos, str, len);\n        run->d_pos += len;\n    }\n}\n\nmut_step_c(flagcopy) {\n    _mut_check_flag(L, tidx);\n    _mut_check_idx(L, tidx);\n    return 0;\n}\n\n// TODO: maybe: optional default val if copy source missing\nmut_step_i(flagcopy) {\n    struct mcp_mut_step *s = &mut->steps[sc];\n    struct mcp_mut_flag *c = &s->c.flag;\n\n    _mut_init_flag(L, tidx, c);\n\n    if (lua_getfield(L, tidx, \"idx\") != LUA_TNIL) {\n        s->idx = lua_tointeger(L, -1);\n    }\n    lua_pop(L, 1);\n\n    return 0;\n}\n\nmut_step_n(flagcopy) {\n    struct mcp_mut_flag *c = &s->c.flag;\n\n    lua_State *L = run->L;\n    unsigned int idx = s->idx;\n    mcp_request_t *srq = NULL;\n    mcp_resp_t *srs = NULL;\n\n    switch (lua_type(L, idx)) {\n        case LUA_TSTRING:\n            p->src = lua_tolstring(L, idx, &p->slen);\n            break;\n        case LUA_TNUMBER:\n            // TODO: unimplemented\n            return -1;\n            break;\n        case LUA_TUSERDATA:\n            _mut_checkudata(L, idx, &srq, &srs);\n            p->slen = 0;\n            if (srq) {\n                if (srq->pr.cmd_type != CMD_TYPE_META) {\n                    return -1;\n                }\n                if (srq->pr.t.meta.flags & c->bit) {\n                    const char *tok = NULL;\n                    size_t tlen = 0;\n                    mcp_request_find_flag_token(srq, c->f, &tok, &tlen);\n\n                    if (tlen > 0) {\n                        p->src = tok;\n                        p->slen = tlen;\n                    }\n                }\n            } else if (srs) {\n                if (srs->resp.type != MCMC_RESP_META) {\n                    // FIXME: error, can't copy flag from non-meta res\n                    return -1;\n                }\n                mcmc_tokenize_res(srs->buf, srs->resp.reslen, &srs->tok);\n                if (mcmc_token_has_flag_bit(&srs->tok, c->bit) == MCMC_OK) {\n                    // flag exists, so copy that in.\n                    // realistically this func is only used if we're also copying a\n                    // token, so we look for that too.\n                    // could add an option to avoid checking for a token?\n                    int len = 0;\n                    const char *tok = mcmc_token_get_flag(srs->buf, &srs->tok, c->f, &len);\n\n                    if (len > 0) {\n                        p->src = tok;\n                        p->slen = len;\n                    }\n                }\n            } else {\n                return -1;\n            }\n            break;\n        default:\n            return -1;\n    }\n\n    return p->slen;\n}\n\nmut_step_r(flagcopy) {\n    struct mcp_mut_flag *c = &s->c.flag;\n    *(run->d_pos) = c->f;\n    run->d_pos++;\n    if (p->slen) {\n        memcpy(run->d_pos, p->src, p->slen);\n        run->d_pos += p->slen;\n    }\n}\n\n// TODO: check that the value hasn't been set yet.\nmut_step_c(valcopy) {\n    _mut_check_idx(L, tidx);\n\n    return 0;\n}\n\nmut_step_i(valcopy) {\n    struct mcp_mut_step *s = &mut->steps[sc];\n\n    if (lua_getfield(L, tidx, \"idx\") != LUA_TNIL) {\n        s->idx = lua_tointeger(L, -1);\n    }\n    lua_pop(L, 1);\n\n    return 0;\n}\n\nmut_step_n(valcopy) {\n    lua_State *L = run->L;\n    unsigned int idx = s->idx;\n    // extract the string + length we need to copy\n    mcp_request_t *srq = NULL;\n    mcp_resp_t *srs = NULL;\n\n    switch (lua_type(L, idx)) {\n        case LUA_TSTRING:\n            run->vbuf = lua_tolstring(L, idx, &run->vlen);\n            break;\n        case LUA_TNUMBER:\n            // TODO: unimplemented\n            break;\n        case LUA_TUSERDATA:\n            _mut_checkudata(L, idx, &srq, &srs);\n            if (srq) {\n                if (srq->pr.vbuf) {\n                    run->vbuf = srq->pr.vbuf;\n                    run->vlen = srq->pr.vlen;\n                }\n            } else if (srs) {\n                // TODO: need to locally parse the result to get the actual val\n                // offsets until later refactoring takes place.\n            } else {\n                return -1;\n            }\n            break;\n        default:\n            return -1;\n    }\n\n    // count the number of digits in vlen to reserve space.\n    //\n    // oddly algorithms to count digits and write digits are similar (outside\n    // of hyper optimization via bit math), so just reuse the same code here.\n    // if I can ever get arena allocations to work and remove the pre-calc\n    // steps this is moot anyway.\n    char temp[22];\n    const char *e = itoa_u64(run->vlen, temp);\n\n    return e - temp;\n}\n\n// print the vlen into the buffer\n// we remove the \\r\\n from the protocol length\nmut_step_r(valcopy) {\n    run->d_pos = itoa_u64(run->vlen-2, run->d_pos);\n}\n\n// END STEPS\n\nstatic const struct mcp_mut_entry mcp_mut_entries[] = {\n    [mcp_mut_step_none] = {NULL, NULL, NULL, NULL, NULL, 0, 0},\n    [mcp_mut_step_cmdset] = {\"cmdset\", mcp_mutator_cmdset_c, mcp_mutator_cmdset_i, mcp_mutator_cmdset_n, mcp_mutator_cmdset_r, MUT_REQ, 0},\n    [mcp_mut_step_cmdcopy] = {\"cmdcopy\", mcp_mutator_cmdcopy_c, mcp_mutator_cmdcopy_i, mcp_mutator_cmdcopy_n, mcp_mutator_cmdcopy_r, MUT_REQ, 0},\n    [mcp_mut_step_keycopy] = {\"keycopy\", mcp_mutator_keycopy_c, mcp_mutator_keycopy_i, mcp_mutator_keycopy_n, mcp_mutator_keycopy_r, MUT_REQ, 0},\n    [mcp_mut_step_keyset] = {\"keyset\", mcp_mutator_keyset_c, mcp_mutator_keyset_i, mcp_mutator_keyset_n, mcp_mutator_keyset_r, MUT_REQ, 0},\n    [mcp_mut_step_rescodeset] = {\"rescodeset\", mcp_mutator_rescodeset_c, mcp_mutator_rescodeset_i, mcp_mutator_rescodeset_n, mcp_mutator_rescodeset_r, MUT_RES, 0},\n    [mcp_mut_step_rescodecopy] = {\"rescodecopy\", mcp_mutator_rescodecopy_c, mcp_mutator_rescodecopy_i, mcp_mutator_rescodecopy_n, mcp_mutator_rescodecopy_r, MUT_RES, 0},\n    [mcp_mut_step_reserr] = {\"reserr\", mcp_mutator_reserr_c, mcp_mutator_reserr_i, mcp_mutator_reserr_n, mcp_mutator_reserr_r, MUT_RES, 0},\n    [mcp_mut_step_flagset] = {\"flagset\", mcp_mutator_flagset_c, mcp_mutator_flagset_i, mcp_mutator_flagset_n, mcp_mutator_flagset_r, MUT_REQ|MUT_RES, 0},\n    [mcp_mut_step_flagcopy] = {\"flagcopy\", mcp_mutator_flagcopy_c, mcp_mutator_flagcopy_i, mcp_mutator_flagcopy_n, mcp_mutator_flagcopy_r, MUT_REQ|MUT_RES, 0},\n    [mcp_mut_step_valcopy] = {\"valcopy\", mcp_mutator_valcopy_c, mcp_mutator_valcopy_i, mcp_mutator_valcopy_n, mcp_mutator_valcopy_r, MUT_REQ|MUT_RES, 0},\n    [mcp_mut_step_final] = {NULL, NULL, NULL, NULL, NULL, 0, 0},\n};\n\n// call with type string on top\nstatic enum mcp_mut_steptype mcp_mutator_steptype(lua_State *L) {\n    const char *type = luaL_checkstring(L, -1);\n    for (int x = 0; x < mcp_mut_step_final; x++) {\n        const struct mcp_mut_entry *e = &mcp_mut_entries[x];\n        if (e->s && strcmp(type, e->s) == 0) {\n            return x;\n        }\n    }\n    return mcp_mut_step_none;\n}\n\nstatic int mcp_mutator_new(lua_State *L, enum mcp_mut_type type) {\n    int argc = lua_gettop(L);\n    size_t size = 0;\n    int scount = 0;\n\n    // loop argument tables once for validation and pre-calculations.\n    for (int x = 1; x <= argc; x++) {\n        luaL_checktype(L, x, LUA_TTABLE);\n        if (lua_getfield(L, x, \"t\") != LUA_TNIL) {\n            enum mcp_mut_steptype st = mcp_mutator_steptype(L);\n            const struct mcp_mut_entry *e = &mcp_mut_entries[st];\n            if (!(e->t & type)) {\n                proxy_lua_ferror(L, \"mutator step %d: step incompatible with mutator type\", x);\n            }\n            if ((st == mcp_mut_step_none) || e->c == NULL) {\n                proxy_lua_ferror(L, \"mutator step %d: unknown step type\", x);\n            }\n            size += e->c(L, x);\n        }\n        lua_pop(L, 1); // drop 't' or nil\n        scount++;\n    }\n\n    // we now know the size and number of steps. allocate some flat memory.\n\n    size_t extsize = sizeof(struct mcp_mut_step) * scount;\n    struct mcp_mutator *mut = lua_newuserdatauv(L, sizeof(*mut) + extsize, 2);\n    memset(mut, 0, sizeof(*mut) + extsize);\n\n    mut->arena = malloc(size);\n    if (mut->arena == NULL) {\n        proxy_lua_error(L, \"mutator_new: failed to allocate memory\");\n    }\n    luaL_setmetatable(L, \"mcp.mutator\");\n    mut->type = type;\n\n    // Cache both request and result metatables for arg validation.\n    // Since a req mutator can take a res argument and vice versa\n    luaL_getmetatable(L, \"mcp.request\");\n    lua_setiuservalue(L, -2, MUT_REQ);\n    luaL_getmetatable(L, \"mcp.response\");\n    lua_setiuservalue(L, -2, MUT_RES);\n\n    // loop the arg tables again to fill in the steps\n    // skip checks since we did that during the first loop.\n    scount = 0;\n    for (int x = 1; x <= argc; x++) {\n        if (lua_getfield(L, x, \"t\") != LUA_TNIL) {\n            enum mcp_mut_steptype st = mcp_mutator_steptype(L);\n            mut->steps[scount].type = st;\n            mcp_mut_entries[st].i(L, x, scount, mut);\n            mut->rcount += mcp_mut_entries[st].rc;\n\n            // copy function pointers into the step so we don't have to skip\n            // around the much larger mcp_mut_entries at runtime.\n            mut->steps[scount].n = mcp_mut_entries[st].n;\n            mut->steps[scount].r = mcp_mut_entries[st].r;\n            // actual args are \"self, dst, args\". start user idx's at 3\n            mut->steps[scount].idx += 2;\n        }\n        lua_pop(L, 1); // drop t or nil\n        scount++;\n    }\n\n    if (size != mut->aused) {\n        proxy_lua_error(L, \"mutator failed to properly initialize, memory not filled correctly\");\n    }\n    mut->scount = scount;\n\n    return 1;\n}\n\nstatic inline int _mcp_mut_run_total(struct mcp_mut_run *run, struct mcp_mut_part *parts) {\n    int total = 0;\n    struct mcp_mutator *mut = run->mut;\n    for (int x = 0; x < mut->scount; x++) {\n        struct mcp_mut_step *s = &mut->steps[x];\n        assert(s->type != mcp_mut_step_none);\n        int len = s->n(run, s, &parts[x]);\n        if (len < 0) {\n            return -1;\n        } else {\n            total += len;\n        }\n    }\n    // account for spaces between \"steps\" and \\r\\n\n    // note that steps that themselves can make multiple tokens _must_ account\n    // for this on their own.\n    total += mut->scount + MIN_BUF_SPACE;\n\n    return total;\n}\n\nstatic inline int _mcp_mut_run_assemble(struct mcp_mut_run *run, struct mcp_mut_part *parts) {\n    struct mcp_mutator *mut = run->mut;\n    // assemble final req/res\n    for (int x = 0; x < mut->scount; x++) {\n        struct mcp_mut_step *s = &mut->steps[x];\n        assert(s->type != mcp_mut_step_none);\n        // Error handling is pushed to the totalling phase.\n        // In this phase we should just be copying data and cannot fail.\n        s->r(run, s, &parts[x]);\n\n        *(run->d_pos) = ' ';\n        run->d_pos++;\n    }\n\n    // TODO: any cases where we need to check if the final char is a space or\n    // not?\n    // add the \\r\\n after all steps\n    *(run->d_pos-1) = '\\r';\n    *(run->d_pos) = '\\n';\n    run->d_pos++;\n\n    return 0;\n}\n\nstatic int mcp_mut_run(struct mcp_mut_run *run) {\n    struct mcp_mutator *mut = run->mut;\n    LIBEVENT_THREAD *t = PROXY_GET_THR(run->L);\n    int ret = 0;\n    struct mcp_mut_part parts[mut->scount];\n\n    // first accumulate the length tally\n    // FIXME: noticed off-by-one's sometimes.\n    // maybe add a debug assert to verify the written total (d_pos - etc)\n    // matches total?\n    // This isn't critical so long as total is > actual, which it has been\n    int total = _mcp_mut_run_total(run, parts);\n    if (total < 0) {\n        lua_pushboolean(run->L, 0);\n        return 1;\n    }\n\n    // ensure space and/or allocate memory then seed our destination pointer.\n    if (mut->type == MUT_REQ) {\n        mcp_request_t *rq = run->arg;\n        if (rq->pr.vbuf) {\n            // FIXME: maybe NULL rq->pr.request in cleanup phase and test that\n            // instead? this check will only fire if req had a vbuf.\n            proxy_lua_error(run->L, \"mutator: request has already been rendered\");\n            return 0;\n        }\n        // future.. should be able to dynamically assign request buffer.\n        if (total > MCP_REQUEST_MAXLEN) {\n            proxy_lua_error(run->L, \"mutator: new request is too long\");\n            return 0;\n        }\n        run->d_pos = rq->request;\n\n        _mcp_mut_run_assemble(run, parts);\n\n        if (process_request(&rq->pr, rq->request, run->d_pos - rq->request) != 0) {\n            proxy_lua_error(run->L, \"mutator: failed to parse new request\");\n            return 0;\n        }\n\n        if (run->vbuf) {\n            rq->pr.vbuf = malloc(run->vlen);\n            if (rq->pr.vbuf == NULL) {\n                proxy_lua_error(run->L, \"mutator: failed to allocate value buffer\");\n                return 0;\n            }\n            pthread_mutex_lock(&t->proxy_limit_lock);\n            t->proxy_buffer_memory_used += rq->pr.vlen;\n            pthread_mutex_unlock(&t->proxy_limit_lock);\n\n            rq->pr.vlen = run->vlen;\n            memcpy(rq->pr.vbuf, run->vbuf, run->vlen);\n        }\n    } else {\n        mcp_resp_t *rs = run->arg;\n        if (rs->buf) {\n            proxy_lua_error(run->L, \"mutator: result has already been rendered\");\n            return 0;\n        }\n\n        // value is inlined in result buffers. future intention to allow more\n        // complex objects so we can refcount values.\n        rs->buf = malloc(total + run->vlen);\n        if (rs->buf == NULL) {\n            proxy_lua_error(run->L, \"mutator: failed to allocate result buffer\");\n            return 0;\n        }\n        run->d_pos = rs->buf;\n\n        _mcp_mut_run_assemble(run, parts);\n\n        rs->tok.ntokens = 0; // TODO: handler from mcmc?\n        rs->status = mcmc_parse_buf(rs->buf, run->d_pos - rs->buf, &rs->resp);\n        if (rs->resp.type == MCMC_RESP_FAIL) {\n            proxy_lua_error(run->L, \"mutator: failed to parse new result\");\n            return 0;\n        }\n\n        // results are sequential buffers, copy the value in.\n        if (run->vbuf) {\n            memcpy(run->d_pos, run->vbuf, run->vlen);\n            run->d_pos += run->vlen;\n        }\n\n        rs->blen = run->d_pos - rs->buf;\n        // NOTE: We increment but don't check the memory limits here. Any\n        // incoming request or incoming response will also check the memory\n        // limits, and just doing an increment here removes some potential\n        // error handling. Requests that are already started should be allowed\n        // to complete to minimize impact of hitting memory limits.\n        pthread_mutex_lock(&t->proxy_limit_lock);\n        t->proxy_buffer_memory_used += rs->blen;\n        pthread_mutex_unlock(&t->proxy_limit_lock);\n    }\n\n    return ret;\n}\n\n// PUBLIC INTERFACE\n\nint mcplib_req_mutator_new(lua_State *L) {\n    return mcp_mutator_new(L, MUT_REQ);\n}\n\nint mcplib_res_mutator_new(lua_State *L) {\n    return mcp_mutator_new(L, MUT_RES);\n}\n\n// walk each step and free references/memory/etc\nint mcplib_mutator_gc(lua_State *L) {\n    struct mcp_mutator *mut = lua_touserdata(L, 1);\n\n    if (mut->arena) {\n        free(mut->arena);\n        mut->arena = NULL;\n    }\n\n    // NOTE: leaving commented until I run into something that actually has\n    // something extra to GC\n    /*for (int x = 0; x < mut->scount; x++) {\n        struct mcp_mut_step *s = &mut->steps[x];\n        switch (s->type) {\n            case mcp_mut_step_none:\n            case mcp_mut_step_cmdset:\n            case mcp_mut_step_cmdcopy:\n            case mcp_mut_step_keycopy:\n            case mcp_mut_step_keyset:\n            case mcp_mut_step_rescodeset:\n            case mcp_mut_step_rescodecopy:\n            case mcp_mut_step_reserr:\n            case mcp_mut_step_flagset:\n            case mcp_mut_step_flagcopy:\n            case mcp_mut_step_valcopy:\n            case mcp_mut_step_final:\n                break;\n        }\n    }*/\n\n    return 0;\n}\n\nint mcplib_mutator_call(lua_State *L) {\n    // since we're here from a __call, assume the type is correct.\n    struct mcp_mutator *mut = lua_touserdata(L, 1);\n    luaL_checktype(L, 2, LUA_TUSERDATA);\n    if (lua_checkstack(L, mut->rcount + 3) == 0) {\n        proxy_lua_error(L, \"mutator ran out of stack space for results\");\n    }\n\n    lua_getmetatable(L, 2); // put dest obj arg metatable on stack\n    lua_getiuservalue(L, 1, mut->type); // put stashed metatable on stack\n    luaL_argcheck(L, lua_rawequal(L, -1, -2), 2,\n            \"invalid argument to mutator object\");\n    lua_pop(L, 2); // toss both metatables\n\n\n    // we're valid now.\n    void *arg = lua_touserdata(L, 2);\n    // stack scratch space so we can avoid modifying the mut struct\n    struct mcp_mut_run run = {L, mut, arg, NULL, NULL, 0};\n    // TODO: numbuf space\n    int ret = mcp_mut_run(&run);\n\n    return ret;\n}\n"
        },
        {
          "name": "proxy_network.c",
          "type": "blob",
          "size": 53.4013671875,
          "content": "/* -*- Mode: C; tab-width: 4; c-basic-offset: 4; indent-tabs-mode: nil -*- */\n// Functions related to the backend handler thread.\n\n#include \"proxy.h\"\n#include \"proxy_tls.h\"\n\nenum proxy_be_failures {\n    P_BE_FAIL_TIMEOUT = 0,\n    P_BE_FAIL_DISCONNECTED,\n    P_BE_FAIL_CONNECTING,\n    P_BE_FAIL_CONNTIMEOUT,\n    P_BE_FAIL_READVALIDATE,\n    P_BE_FAIL_BADVALIDATE,\n    P_BE_FAIL_WRITING,\n    P_BE_FAIL_READING,\n    P_BE_FAIL_PARSING,\n    P_BE_FAIL_CLOSED,\n    P_BE_FAIL_UNHANDLEDRES,\n    P_BE_FAIL_OOM,\n    P_BE_FAIL_ENDSYNC,\n    P_BE_FAIL_TRAILINGDATA,\n    P_BE_FAIL_INVALIDPROTOCOL,\n};\n\nconst char *proxy_be_failure_text[] = {\n    [P_BE_FAIL_TIMEOUT] = \"timeout\",\n    [P_BE_FAIL_DISCONNECTED] = \"disconnected\",\n    [P_BE_FAIL_CONNECTING] = \"connecting\",\n    [P_BE_FAIL_CONNTIMEOUT] = \"conntimeout\",\n    [P_BE_FAIL_READVALIDATE] = \"readvalidate\",\n    [P_BE_FAIL_BADVALIDATE] = \"badvalidate\",\n    [P_BE_FAIL_WRITING] = \"writing\",\n    [P_BE_FAIL_READING] = \"reading\",\n    [P_BE_FAIL_PARSING] = \"parsing\",\n    [P_BE_FAIL_CLOSED] = \"closedsock\",\n    [P_BE_FAIL_UNHANDLEDRES] = \"unhandledres\",\n    [P_BE_FAIL_OOM] = \"outofmemory\",\n    [P_BE_FAIL_ENDSYNC] = \"missingend\",\n    [P_BE_FAIL_TRAILINGDATA] = \"trailingdata\",\n    [P_BE_FAIL_INVALIDPROTOCOL] = \"invalidprotocol\",\n    NULL\n};\n\nstatic void proxy_backend_handler(const int fd, const short which, void *arg);\nstatic void proxy_backend_tls_handler(const int fd, const short which, void *arg);\nstatic void proxy_beconn_handler(const int fd, const short which, void *arg);\nstatic void proxy_beconn_tls_handler(const int fd, const short which, void *arg);\nstatic void proxy_event_handler(evutil_socket_t fd, short which, void *arg);\nstatic void proxy_event_beconn(evutil_socket_t fd, short which, void *arg);\nstatic int _prep_pending_write(struct mcp_backendconn_s *be);\nstatic void _post_pending_write(struct mcp_backendconn_s *be, ssize_t sent);\nstatic int _flush_pending_write(struct mcp_backendconn_s *be);\nstatic int _flush_pending_tls_write(struct mcp_backendconn_s *be);\nstatic void _cleanup_backend(mcp_backend_t *be);\nstatic void _reset_bad_backend(struct mcp_backendconn_s *be, enum proxy_be_failures err);\nstatic void _set_main_event(struct mcp_backendconn_s *be, struct event_base *base, int flags, struct timeval *t, event_callback_fn callback);\nstatic void _stop_main_event(struct mcp_backendconn_s *be);\nstatic void _start_write_event(struct mcp_backendconn_s *be);\nstatic void _stop_write_event(struct mcp_backendconn_s *be);\nstatic void _start_timeout_event(struct mcp_backendconn_s *be);\nstatic void _stop_timeout_event(struct mcp_backendconn_s *be);\nstatic int proxy_backend_drive_machine(struct mcp_backendconn_s *be);\n\n/* Helper routines common to io_uring and libevent modes */\n\n// TODO (v3): doing an inline syscall here, not ideal for uring mode.\n// leaving for now since this should be extremely uncommon.\nstatic int _beconn_send_validate(struct mcp_backendconn_s *be) {\n    const char *str = \"version\\r\\n\";\n    const ssize_t len = strlen(str);\n\n    ssize_t res = write(mcmc_fd(be->client), str, len);\n\n    if (res == -1) {\n        return -1;\n    }\n\n    // I'm making an opinionated statement that we should be able to write\n    // \"version\\r\\n\" into a fresh socket without hitting EAGAIN.\n    if (res < len) {\n        return -1;\n    }\n\n    return 1;\n}\n\nstatic int _proxy_beconn_checkconnect(struct mcp_backendconn_s *be) {\n    int err = 0;\n    // We were connecting, now ensure we're properly connected.\n    if (mcmc_check_nonblock_connect(be->client, &err) != MCMC_OK) {\n        P_DEBUG(\"%s: backend failed to connect (%s:%s)\\n\", __func__, be->be_parent->name, be->be_parent->port);\n        // kick the bad backend, clear the queue, retry later.\n        // FIXME (v2): if a connect fails, anything currently in the queue\n        // should be safe to hold up until their timeout.\n        _reset_bad_backend(be, P_BE_FAIL_CONNECTING);\n        return -1;\n    }\n    P_DEBUG(\"%s: backend connected [fd: %d] (%s:%s)\\n\", __func__, mcmc_fd(be->client), be->be_parent->name, be->be_parent->port);\n    be->connecting = false;\n    be->state = mcp_backend_read;\n\n    // seed the failure time for the flap check.\n    gettimeofday(&be->last_failed, NULL);\n\n    be->validating = true;\n    // TODO: make validation optional.\n\n    return 0;\n}\n\n// Use a simple heuristic to choose a backend connection socket out of a list\n// of sockets.\nstruct mcp_backendconn_s *proxy_choose_beconn(mcp_backend_t *be) {\n    struct mcp_backendconn_s *bec = &be->be[0];\n    if (be->conncount != 1) {\n        int depth = INT_MAX;\n        // TODO: to computationally limit + ensure each connection stays\n        // somewhat warm:\n        // - remember idx of last conn used.\n        // - if next idx has a lower depth, use that one instead\n        // - tick idx (and reset if necessary)\n        // else under low loads only the first conn will ever get used (which\n        // is normally good; but sometimes bad if using stateful firewalls)\n        for (int x = 0; x < be->conncount; x++) {\n            struct mcp_backendconn_s *bec_i = &be->be[x];\n            if (bec_i->bad) {\n                continue;\n            }\n            if (bec_i->depth == 0) {\n                bec = bec_i;\n                break;\n            } else if (bec_i->depth < depth) {\n                depth = bec_i->depth;\n                bec = bec_i;\n            }\n        }\n    }\n\n    return bec;\n}\n\nstatic void _proxy_event_handler_dequeue(proxy_event_thread_t *t) {\n    io_head_t head;\n\n    STAILQ_INIT(&head);\n    STAILQ_INIT(&t->be_head);\n\n    // Pull the entire stack of inbound into local queue.\n    pthread_mutex_lock(&t->mutex);\n    STAILQ_CONCAT(&head, &t->io_head_in);\n    pthread_mutex_unlock(&t->mutex);\n\n    while (!STAILQ_EMPTY(&head)) {\n        io_pending_proxy_t *io = STAILQ_FIRST(&head);\n        io->flushed = false;\n\n        // _no_ mutex on backends. they are owned by the event thread.\n        STAILQ_REMOVE_HEAD(&head, io_next);\n        // paranoia about moving items between lists.\n        io->io_next.stqe_next = NULL;\n\n        mcp_backend_t *be = io->backend;\n        STAILQ_INSERT_TAIL(&be->io_head, io, io_next);\n        assert(be->depth > -1);\n        be->depth++;\n        if (!be->stacked) {\n            be->stacked = true;\n            STAILQ_INSERT_TAIL(&t->be_head, be, be_next);\n        }\n    }\n}\n\nstatic void _cleanup_backend(mcp_backend_t *be) {\n    for (int x = 0; x < be->conncount; x++) {\n        struct mcp_backendconn_s *bec = &be->be[x];\n        // remove any pending events.\n        if (!be->tunables.down) {\n            int pending = event_pending(&bec->main_event, EV_READ|EV_WRITE|EV_TIMEOUT, NULL);\n            if (pending != 0) {\n                event_del(&bec->main_event); // an error to call event_del() without event.\n            }\n            pending = event_pending(&bec->write_event, EV_READ|EV_WRITE|EV_TIMEOUT, NULL);\n            if (pending != 0) {\n                event_del(&bec->write_event); // an error to call event_del() without event.\n            }\n            pending = event_pending(&bec->timeout_event, EV_TIMEOUT, NULL);\n            if (pending != 0) {\n                event_del(&bec->timeout_event); // an error to call event_del() without event.\n            }\n\n            // - assert on empty queue\n            assert(STAILQ_EMPTY(&bec->io_head));\n\n            mcp_tls_shutdown(bec);\n            mcmc_disconnect(bec->client);\n\n            if (bec->bad) {\n                mcp_sharedvm_delta(bec->event_thread->ctx, SHAREDVM_BACKEND_IDX,\n                    bec->be_parent->label, -1);\n            }\n        }\n        // - free be->client\n        free(bec->client);\n        // - free be->rbuf\n        free(bec->rbuf);\n    }\n    // free once parent has had all connections closed off.\n    free(be);\n}\n\nstatic void _setup_backend(mcp_backend_t *be) {\n    for (int x = 0; x < be->conncount; x++) {\n        struct mcp_backendconn_s *bec = &be->be[x];\n        if (be->tunables.down) {\n            // backend is \"forced\" into a bad state. never connect or\n            // otherwise attempt to use it.\n            be->be[x].bad = true;\n            continue;\n        }\n        // assign the initial events to the backend, so we don't have to\n        // constantly check if they were initialized yet elsewhere.\n        // note these events will not fire until event_add() is called.\n        int status = mcmc_connect(bec->client, be->name, be->port, bec->connect_flags);\n        event_callback_fn _beconn_handler = &proxy_beconn_handler;\n        event_callback_fn _backend_handler = &proxy_backend_handler;\n        if (be->tunables.use_tls) {\n            _beconn_handler = &proxy_beconn_tls_handler;\n            _backend_handler = &proxy_backend_tls_handler;\n        }\n        event_assign(&bec->main_event, bec->event_thread->base, mcmc_fd(bec->client), EV_WRITE|EV_TIMEOUT, _beconn_handler, bec);\n        event_assign(&bec->write_event, bec->event_thread->base, mcmc_fd(bec->client), EV_WRITE|EV_TIMEOUT, _backend_handler, bec);\n        event_assign(&bec->timeout_event, bec->event_thread->base, -1, EV_TIMEOUT, _backend_handler, bec);\n\n        if (status == MCMC_CONNECTING || status == MCMC_CONNECTED) {\n            // if we're already connected for some reason, still push it\n            // through the connection handler to keep the code unified. It\n            // will auto-wake because the socket is writeable.\n            bec->connecting = true;\n            bec->can_write = false;\n            // kick off the event we intialized above.\n            event_add(&bec->main_event, &bec->tunables.connect);\n        } else {\n            _reset_bad_backend(bec, P_BE_FAIL_CONNECTING);\n        }\n    }\n}\n\n// event handler for injecting backends for processing\n// currently just for initiating connections the first time.\nstatic void proxy_event_beconn(evutil_socket_t fd, short which, void *arg) {\n    proxy_event_thread_t *t = arg;\n\n#ifdef USE_EVENTFD\n    uint64_t u;\n    if (read(fd, &u, sizeof(uint64_t)) != sizeof(uint64_t)) {\n        // Temporary error or wasn't actually ready to read somehow.\n        return;\n    }\n#else\n    char buf[1];\n    if (read(fd, buf, 1) != 1) {\n        P_DEBUG(\"%s: pipe read failed\\n\", __func__);\n        return;\n    }\n#endif\n\n    beconn_head_t head;\n\n    STAILQ_INIT(&head);\n    pthread_mutex_lock(&t->mutex);\n    STAILQ_CONCAT(&head, &t->beconn_head_in);\n    pthread_mutex_unlock(&t->mutex);\n\n    // Think we should reuse this code path for manually instructing backends\n    // to disable/etc but not coding for that generically. We just need to\n    // check the state of the backend when it reaches here or some flags at\n    // least.\n    // FIXME: another ->stacked flag?\n    // Either that or remove the STAILQ code and just using an array of\n    // ptr's.\n    mcp_backend_t *be = NULL;\n    // be can be freed by the loop, so can't use STAILQ_FOREACH.\n    while (!STAILQ_EMPTY(&head)) {\n        be = STAILQ_FIRST(&head);\n        STAILQ_REMOVE_HEAD(&head, beconn_next);\n        if (be->transferred) {\n            // If this object was already transferred here, we're being\n            // signalled to clean it up and free.\n            _cleanup_backend(be);\n        } else {\n            be->transferred = true;\n            _setup_backend(be);\n        }\n    }\n}\n\nstatic void _proxy_flush_backend_queue(mcp_backend_t *be) {\n    io_pending_proxy_t *io = NULL;\n    P_DEBUG(\"%s: fast failing request to bad backend (%s:%s) depth: %d\\n\", __func__, be->name, be->port, be->depth);\n\n    while (!STAILQ_EMPTY(&be->io_head)) {\n        io = STAILQ_FIRST(&be->io_head);\n        STAILQ_REMOVE_HEAD(&be->io_head, io_next);\n        mcp_resp_set_elapsed(io->client_resp);\n        io->client_resp->status = MCMC_ERR;\n        io->client_resp->resp.code = MCMC_CODE_SERVER_ERROR;\n        be->depth--;\n        assert(be->depth > -1);\n        return_io_pending((io_pending_t *)io);\n    }\n}\n\nvoid proxy_run_backend_queue(be_head_t *head) {\n    mcp_backend_t *be;\n    STAILQ_FOREACH(be, head, be_next) {\n        be->stacked = false;\n        int flags = 0;\n        struct mcp_backendconn_s *bec = proxy_choose_beconn(be);\n\n        int limit = be->tunables.backend_depth_limit;\n        if (bec->bad) {\n            // TODO: another counter for fast fails?\n            _proxy_flush_backend_queue(be);\n            continue;\n        } else if (limit && bec->depth > limit) {\n            proxy_ctx_t *ctx = bec->event_thread->ctx;\n            STAT_INCR(ctx, request_failed_depth, be->depth);\n            _proxy_flush_backend_queue(be);\n            continue;\n        }\n\n        // drop new requests onto end of conn's io-head, reset the backend one.\n        STAILQ_CONCAT(&bec->io_head, &be->io_head);\n        bec->depth += be->depth;\n        be->depth = 0;\n\n        if (bec->connecting || bec->validating) {\n            P_DEBUG(\"%s: deferring IO pending connecting (%s:%s)\\n\", __func__, be->name, be->port);\n        } else {\n            if (!bec->ssl) {\n                flags = _flush_pending_write(bec);\n            } else {\n                flags = _flush_pending_tls_write(bec);\n            }\n\n            if (flags == -1) {\n                _reset_bad_backend(bec, P_BE_FAIL_WRITING);\n            } else if (flags & EV_WRITE) {\n                // only get here because we need to kick off the write handler\n                _start_write_event(bec);\n            }\n\n            if (bec->pending_read) {\n                _start_timeout_event(bec);\n            }\n\n        }\n    }\n}\n\n// event handler for executing backend requests\nstatic void proxy_event_handler(evutil_socket_t fd, short which, void *arg) {\n    proxy_event_thread_t *t = arg;\n\n#ifdef USE_EVENTFD\n    uint64_t u;\n    if (read(fd, &u, sizeof(uint64_t)) != sizeof(uint64_t)) {\n        // Temporary error or wasn't actually ready to read somehow.\n        return;\n    }\n#else\n    char buf[1];\n    // TODO (v2): This is a lot more fatal than it should be. can it fail? can\n    // it blow up the server?\n    // TODO (v2): a cross-platform method of speeding this up would be nice. With\n    // event fds we can queue N events and wakeup once here.\n    // If we're pulling one byte out of the pipe at a time here it'll just\n    // wake us up too often.\n    // If the pipe is O_NONBLOCK then maybe just a larger read would work?\n    if (read(fd, buf, 1) != 1) {\n        P_DEBUG(\"%s: pipe read failed\\n\", __func__);\n        return;\n    }\n#endif\n\n    _proxy_event_handler_dequeue(t);\n\n    // Re-walk each backend and check set event as required.\n    proxy_run_backend_queue(&t->be_head);\n}\n\nvoid *proxy_event_thread(void *arg) {\n    proxy_event_thread_t *t = arg;\n\n    logger_create(); // TODO (v2): add logger ptr to structure\n    event_base_loop(t->base, 0);\n    event_base_free(t->base);\n\n    // TODO (v2): join bt threads, free array.\n\n    return NULL;\n}\n\nstatic void _set_main_event(struct mcp_backendconn_s *be, struct event_base *base, int flags, struct timeval *t, event_callback_fn callback) {\n    int pending = event_pending(&be->main_event, EV_READ|EV_WRITE|EV_TIMEOUT, NULL);\n    if (pending != 0) {\n        event_del(&be->main_event); // replace existing event.\n    }\n\n    int fd = mcmc_fd(be->client);\n    if (fd == 0) {\n        fd = -1; // need to pass -1 to event assign if we're not operating on\n                 // a connection.\n    }\n    event_assign(&be->main_event, base, fd,\n            flags, callback, be);\n    event_add(&be->main_event, t);\n}\n\nstatic void _stop_main_event(struct mcp_backendconn_s *be) {\n    event_del(&be->main_event);\n}\n\nstatic void _start_write_event(struct mcp_backendconn_s *be) {\n    int pending = event_pending(&be->write_event, EV_WRITE|EV_TIMEOUT, NULL);\n    if (pending != 0) {\n        return;\n    }\n    // FIXME: wasn't there a write timeout?\n    event_add(&be->write_event, &be->tunables.read);\n}\n\nstatic void _stop_write_event(struct mcp_backendconn_s *be) {\n    event_del(&be->write_event);\n}\n\n// handle the read timeouts with a side event, so we can stick with a\n// persistent listener (optimization + catch disconnects faster)\nstatic void _start_timeout_event(struct mcp_backendconn_s *be) {\n    int pending = event_pending(&be->timeout_event, EV_TIMEOUT, NULL);\n    if (pending != 0) {\n        return;\n    }\n    event_add(&be->timeout_event, &be->tunables.read);\n}\n\nstatic void _stop_timeout_event(struct mcp_backendconn_s *be) {\n    int pending = event_pending(&be->timeout_event, EV_TIMEOUT, NULL);\n    if (pending == 0) {\n        return;\n    }\n    event_del(&be->timeout_event);\n}\n\nstatic void _drive_machine_next(struct mcp_backendconn_s *be, io_pending_proxy_t *p) {\n    // set the head here. when we break the head will be correct.\n    STAILQ_REMOVE_HEAD(&be->io_head, io_next);\n    be->depth--;\n    assert(p != be->io_next); // don't remove what we need to flush.\n    assert(be->depth > -1);\n    be->pending_read--;\n    assert(be->pending_read > -1);\n\n    mcp_resp_set_elapsed(p->client_resp);\n    // have to do the q->count-- and == 0 and redispatch_conn()\n    // stuff here. The moment we call return_io here we\n    // don't own *p anymore.\n    return_io_pending((io_pending_t *)p);\n    be->state = mcp_backend_read;\n}\n\n// NOTES:\n// - mcp_backend_read: grab req_stack_head, do things\n// read -> next, want_read -> next | read_end, etc.\nstatic int proxy_backend_drive_machine(struct mcp_backendconn_s *be) {\n    bool stop = false;\n    io_pending_proxy_t *p = NULL;\n    int flags = 0;\n\n    p = STAILQ_FIRST(&be->io_head);\n    if (p == NULL) {\n        // got a read event, but nothing was queued.\n        // probably means a disconnect event.\n        // TODO (v2): could probably confirm this by attempting to read the\n        // socket, getsockopt, or something else simply for logging or\n        // statistical purposes.\n        // In this case we know it's going to be a close so error.\n        flags = P_BE_FAIL_CLOSED;\n        P_DEBUG(\"%s: read event but nothing in IO queue\\n\", __func__);\n        return flags;\n    }\n\n    while (!stop) {\n        mcp_resp_t *r;\n\n    switch(be->state) {\n        case mcp_backend_read:\n            assert(p != NULL);\n            // FIXME: remove the _read state?\n            be->state = mcp_backend_parse;\n            break;\n        case mcp_backend_parse:\n            r = p->client_resp;\n            r->status = mcmc_parse_buf(be->rbuf, be->rbufused, &r->resp);\n\n            // Quick check if we need more data.\n            if (r->resp.code == MCMC_WANT_READ) {\n                return 0;\n            }\n\n            // we actually don't care about anything but the value length\n            // TODO (v2): if vlen != vlen_read, pull an item and copy the data.\n            int extra_space = 0;\n            // if all goes well, move to the next request.\n            be->state = mcp_backend_next;\n            switch (r->resp.type) {\n                case MCMC_RESP_GET:\n                    // We're in GET mode. we only support one key per\n                    // GET in the proxy backends, so we need to later check\n                    // for an END.\n                    extra_space = ENDLEN;\n                    be->state = mcp_backend_read_end;\n                    break;\n                case MCMC_RESP_END:\n                    // this is a MISS from a GET request\n                    // or final handler from a STAT request.\n                    assert(r->resp.vlen == 0);\n                    if (p->ascii_multiget) {\n                        // Ascii multiget hack mode; consume END's\n                        be->rbufused -= r->resp.reslen;\n                        if (be->rbufused > 0) {\n                            memmove(be->rbuf, be->rbuf+r->resp.reslen, be->rbufused);\n                        }\n\n                        be->state = mcp_backend_next;\n                        continue;\n                    }\n                    break;\n                case MCMC_RESP_META:\n                    // we can handle meta responses easily since they're self\n                    // contained.\n                    break;\n                case MCMC_RESP_GENERIC:\n                case MCMC_RESP_NUMERIC:\n                    break;\n                case MCMC_RESP_ERRMSG: // received an error message\n                    if (r->resp.code != MCMC_CODE_SERVER_ERROR) {\n                        // Non server errors are protocol errors; can't trust\n                        // the connection anymore.\n                        be->state = mcp_backend_next_close;\n                    }\n                    break;\n                case MCMC_RESP_FAIL:\n                    P_DEBUG(\"%s: mcmc_read failed [%d]\\n\", __func__, r->status);\n                    flags = P_BE_FAIL_PARSING;\n                    stop = true;\n                    break;\n                // TODO (v2): No-op response?\n                default:\n                    P_DEBUG(\"%s: Unhandled response from backend: %d\\n\", __func__, r->resp.type);\n                    // unhandled :(\n                    flags = P_BE_FAIL_UNHANDLEDRES;\n                    stop = true;\n                    break;\n            }\n\n            // r->resp.reslen + r->resp.vlen is the total length of the response.\n            // TODO (v2): need to associate a buffer with this response...\n            // for now we simply malloc, but reusable buffers should be used\n\n            r->blen = r->resp.reslen + r->resp.vlen;\n            {\n                bool oom = proxy_bufmem_checkadd(r->thread, r->blen + extra_space);\n\n                if (oom) {\n                    flags = P_BE_FAIL_OOM;\n                    // need to zero out blen so we don't over-decrement later\n                    r->blen = 0;\n                    stop = true;\n                    break;\n                }\n            }\n            r->buf = malloc(r->blen + extra_space);\n            if (r->buf == NULL) {\n                // Enforce accounting.\n                pthread_mutex_lock(&r->thread->proxy_limit_lock);\n                r->thread->proxy_buffer_memory_used -= r->blen + extra_space;\n                pthread_mutex_unlock(&r->thread->proxy_limit_lock);\n\n                flags = P_BE_FAIL_OOM;\n                r->blen = 0;\n                stop = true;\n                break;\n            }\n\n            P_DEBUG(\"%s: r->status: %d, r->bread: %d, r->vlen: %lu\\n\", __func__, r->status, r->bread, r->resp.vlen);\n            if (r->resp.vlen != r->resp.vlen_read) {\n                // shouldn't be possible to have excess in buffer\n                // if we're dealing with a partial value.\n                assert(be->rbufused == r->resp.reslen+r->resp.vlen_read);\n                P_DEBUG(\"%s: got a short read, moving to want_read\\n\", __func__);\n                // copy the partial and advance mcmc's buffer digestion.\n                memcpy(r->buf, be->rbuf, r->resp.reslen + r->resp.vlen_read);\n                r->bread = r->resp.reslen + r->resp.vlen_read;\n                be->rbufused = 0;\n                be->state = mcp_backend_want_read;\n                flags = 0;\n                stop = true;\n                break;\n            } else {\n                // mcmc's already counted the value as read if it fit in\n                // the original buffer...\n                memcpy(r->buf, be->rbuf, r->resp.reslen+r->resp.vlen_read);\n            }\n\n            // had a response, advance the buffer.\n            be->rbufused -= r->resp.reslen + r->resp.vlen_read;\n            if (be->rbufused > 0) {\n                memmove(be->rbuf, be->rbuf+r->resp.reslen+r->resp.vlen_read, be->rbufused);\n            }\n\n            break;\n        case mcp_backend_read_end:\n            r = p->client_resp;\n            // we need to ensure the next data in the stream is \"END\\r\\n\"\n            // if not, the stack is desynced and we lose it.\n\n            if (be->rbufused >= ENDLEN) {\n                if (memcmp(be->rbuf, ENDSTR, ENDLEN) != 0) {\n                    flags = P_BE_FAIL_ENDSYNC;\n                    stop = true;\n                    break;\n                } else {\n                    // response is good.\n                    // FIXME (v2): copy what the server actually sent?\n                    if (!p->ascii_multiget) {\n                        // sigh... if part of a multiget we need to eat the END\n                        // markers down here.\n                        memcpy(r->buf+r->blen, ENDSTR, ENDLEN);\n                        r->blen += 5;\n                    } else {\n                        r->extra = 5;\n                    }\n\n                    // advance buffer\n                    be->rbufused -= ENDLEN;\n                    if (be->rbufused > 0) {\n                        memmove(be->rbuf, be->rbuf+ENDLEN, be->rbufused);\n                    }\n                }\n            } else {\n                flags = 0;\n                stop = true;\n                break;\n            }\n\n            be->state = mcp_backend_next;\n\n            break;\n        case mcp_backend_want_read:\n            // Continuing a read from earlier\n            r = p->client_resp;\n            // take bread input and see if we're done reading the value,\n            // else advance, set buffers, return next.\n            P_DEBUG(\"%s: [want_read] r->bread: %d vlen: %lu\\n\", __func__, r->bread, r->resp.vlen);\n            assert(be->rbufused != 0);\n            size_t tocopy = be->rbufused < r->blen - r->bread ?\n                be->rbufused : r->blen - r->bread;\n            memcpy(r->buf+r->bread, be->rbuf, tocopy);\n            r->bread += tocopy;\n\n            if (r->bread >= r->blen) {\n                // all done copying data.\n                if (r->resp.type == MCMC_RESP_GET) {\n                    be->state = mcp_backend_read_end;\n                } else {\n                    be->state = mcp_backend_next;\n                }\n\n                // shuffle remaining buffer.\n                be->rbufused -= tocopy;\n                if (be->rbufused > 0) {\n                    memmove(be->rbuf, be->rbuf+tocopy, be->rbufused);\n                }\n            } else {\n                assert(tocopy == be->rbufused);\n                // signal to caller to issue a read.\n                be->rbufused = 0;\n                flags = 0;\n                stop = true;\n            }\n\n            break;\n        case mcp_backend_next:\n            _drive_machine_next(be, p);\n\n            if (STAILQ_EMPTY(&be->io_head)) {\n                stop = true;\n                // if there're no pending requests, the read buffer\n                // should also be empty.\n                if (be->rbufused > 0) {\n                    flags = P_BE_FAIL_TRAILINGDATA;\n                }\n                break;\n            } else {\n                p = STAILQ_FIRST(&be->io_head);\n            }\n\n            // if leftover, keep processing IO's.\n            // if no more data in buffer, need to re-set stack head and re-set\n            // event.\n            P_DEBUG(\"%s: [next] remain: %lu\\n\", __func__, be->rbufused);\n            if (be->rbufused != 0) {\n                // data trailing in the buffer, for a different request.\n                be->state = mcp_backend_parse;\n            } else {\n                // need to read more data, buffer is empty.\n                stop = true;\n            }\n\n            break;\n        case mcp_backend_next_close:\n            // we advance and return the current IO, then kill the conn.\n            _drive_machine_next(be, p);\n            stop = true;\n            flags = P_BE_FAIL_INVALIDPROTOCOL;\n\n            break;\n        default:\n            // TODO (v2): at some point (after v1?) this should attempt to recover,\n            // though we should only get here from memory corruption and\n            // bailing may be the right thing to do.\n            fprintf(stderr, \"%s: invalid backend state: %d\\n\", __func__, be->state);\n            assert(false);\n    } // switch\n    } // while\n\n    return flags;\n}\n\nstatic void _backend_reconnect(struct mcp_backendconn_s *be) {\n    int status = mcmc_connect(be->client, be->be_parent->name, be->be_parent->port, be->connect_flags);\n    if (status == MCMC_CONNECTED) {\n        // TODO (v2): unexpected but lets let it be here.\n        be->connecting = false;\n        be->can_write = true;\n    } else if (status == MCMC_CONNECTING) {\n        be->connecting = true;\n        be->can_write = false;\n    } else {\n        // failed to immediately re-establish the connection.\n        // need to put the BE into a bad/retry state.\n        be->connecting = false;\n        be->can_write = true;\n    }\n    // re-create the write handler for the new file descriptor.\n    // the main event will be re-assigned after this call.\n    event_callback_fn _backend_handler = &proxy_backend_handler;\n    if (be->be_parent->tunables.use_tls) {\n        _backend_handler = &proxy_backend_tls_handler;\n    }\n    event_assign(&be->write_event, be->event_thread->base, mcmc_fd(be->client), EV_WRITE|EV_TIMEOUT, _backend_handler, be);\n    // do not need to re-assign the timer event because it's not tied to fd\n}\n\n// All we need to do here is schedule the backend to attempt to connect again.\nstatic void proxy_backend_retry_handler(const int fd, const short which, void *arg) {\n    struct mcp_backendconn_s *be = arg;\n    assert(which & EV_TIMEOUT);\n    struct timeval tmp_time = be->tunables.connect;\n    _backend_reconnect(be);\n    event_callback_fn _backend_handler = &proxy_beconn_handler;\n    if (be->be_parent->tunables.use_tls) {\n        _backend_handler = &proxy_beconn_tls_handler;\n    }\n    _set_main_event(be, be->event_thread->base, EV_WRITE, &tmp_time, _backend_handler);\n}\n\n// must be called after _reset_bad_backend(), so the backend is currently\n// clear.\n// TODO (v2): extra counter for \"backend connect tries\" so it's still possible\n// to see dead backends exist\nstatic void _backend_reschedule(struct mcp_backendconn_s *be) {\n    bool failed = false;\n    struct timeval tmp_time = {0};\n    long int retry_time = be->tunables.retry.tv_sec;\n    char *badtext = \"markedbad\";\n    if (be->flap_count > be->tunables.backend_failure_limit) {\n        // reduce retry frequency to avoid noise.\n        float backoff = retry_time;\n        for (int x = 0; x < be->flap_count; x++) {\n            backoff *= be->tunables.flap_backoff_ramp;\n        }\n        retry_time = (uint32_t)backoff;\n\n        if (retry_time > be->tunables.flap_backoff_max) {\n            retry_time = be->tunables.flap_backoff_max;\n        }\n        badtext = \"markedbadflap\";\n        failed = true;\n    } else if (be->failed_count > be->tunables.backend_failure_limit) {\n        failed = true;\n    }\n    tmp_time.tv_sec = retry_time;\n\n    if (failed) {\n        if (!be->bad) {\n            P_DEBUG(\"%s: marking backend as bad\\n\", __func__);\n            STAT_INCR(be->event_thread->ctx, backend_marked_bad, 1);\n            mcp_sharedvm_delta(be->event_thread->ctx, SHAREDVM_BACKEND_IDX,\n                    be->be_parent->label, 1);\n            LOGGER_LOG(NULL, LOG_PROXYEVENTS, LOGGER_PROXY_BE_ERROR, NULL, badtext, be->be_parent->name, be->be_parent->port, be->be_parent->label, 0, NULL, 0, retry_time);\n        }\n        be->bad = true;\n       _set_main_event(be, be->event_thread->base, EV_TIMEOUT, &tmp_time, proxy_backend_retry_handler);\n    } else {\n        struct timeval tmp_time = be->tunables.connect;\n        STAT_INCR(be->event_thread->ctx, backend_failed, 1);\n        _backend_reconnect(be);\n        event_callback_fn _backend_handler = &proxy_beconn_handler;\n        if (be->be_parent->tunables.use_tls) {\n            _backend_handler = &proxy_beconn_tls_handler;\n        }\n        _set_main_event(be, be->event_thread->base, EV_WRITE, &tmp_time, _backend_handler);\n    }\n}\n\nstatic void _backend_flap_check(struct mcp_backendconn_s *be, enum proxy_be_failures err) {\n    struct timeval now;\n    struct timeval *flap = &be->tunables.flap;\n\n    switch (err) {\n        case P_BE_FAIL_TIMEOUT:\n        case P_BE_FAIL_DISCONNECTED:\n        case P_BE_FAIL_WRITING:\n        case P_BE_FAIL_READING:\n            if (flap->tv_sec != 0 || flap->tv_usec != 0) {\n                struct timeval delta = {0};\n                int64_t subsec = 0;\n                gettimeofday(&now, NULL);\n                delta.tv_sec = now.tv_sec - be->last_failed.tv_sec;\n                subsec = now.tv_usec - be->last_failed.tv_usec;\n                if (subsec < 0) {\n                    // tv_usec is specced as \"at least\" [-1, 1000000]\n                    // so to guarantee lower negatives we need this temp var.\n                    delta.tv_sec--;\n                    subsec += 1000000;\n                    delta.tv_usec = subsec;\n                }\n\n                if (flap->tv_sec < delta.tv_sec ||\n                    (flap->tv_sec == delta.tv_sec && flap->tv_usec < delta.tv_usec)) {\n                    // delta is larger than our flap range. reset the flap counter.\n                    be->flap_count = 0;\n                } else {\n                    // seems like we flapped again.\n                    be->flap_count++;\n                }\n                be->last_failed = now;\n            }\n            break;\n        default:\n            // only perform a flap check on network related errors.\n            break;\n    }\n}\n\n// TODO (v2): add a second argument for assigning a specific error to all pending\n// IO's (ie; timeout).\n// The backend has gotten into a bad state (timed out, protocol desync, or\n// some other supposedly unrecoverable error: purge the queue and\n// cycle the socket.\n// Note that some types of errors may not require flushing the queue and\n// should be fixed as they're figured out.\n// _must_ be called from within the event thread.\nstatic void _reset_bad_backend(struct mcp_backendconn_s *be, enum proxy_be_failures err) {\n    io_pending_proxy_t *io = NULL;\n    P_DEBUG(\"%s: resetting bad backend: [fd: %d] %s\\n\", __func__, mcmc_fd(be->client), proxy_be_failure_text[err]);\n    // Can't use STAILQ_FOREACH() since r_io_p() free's the current\n    // io. STAILQ_FOREACH_SAFE maybe?\n    int depth = be->depth;\n    while (!STAILQ_EMPTY(&be->io_head)) {\n        io = STAILQ_FIRST(&be->io_head);\n        STAILQ_REMOVE_HEAD(&be->io_head, io_next);\n        // TODO (v2): Unsure if this is the best way of surfacing errors to lua,\n        // but will do for V1.\n        mcp_resp_set_elapsed(io->client_resp);\n        io->client_resp->status = MCMC_ERR;\n        io->client_resp->resp.code = MCMC_CODE_SERVER_ERROR;\n        be->depth--;\n        assert(be->depth > -1);\n        return_io_pending((io_pending_t *)io);\n    }\n\n    STAILQ_INIT(&be->io_head);\n    be->io_next = NULL; // also reset the write offset.\n\n    // Only log if we don't already know it's messed up.\n    if (!be->bad) {\n        LOGGER_LOG(NULL, LOG_PROXYEVENTS, LOGGER_PROXY_BE_ERROR, NULL, proxy_be_failure_text[err], be->be_parent->name, be->be_parent->port, be->be_parent->label, depth, be->rbuf, be->rbufused, 0);\n    }\n\n    // reset buffer to blank state.\n    be->rbufused = 0;\n    be->pending_read = 0;\n    // clear events so the reconnect handler can re-arm them with a few fd.\n    _stop_write_event(be);\n    _stop_main_event(be);\n    _stop_timeout_event(be);\n    mcp_tls_shutdown(be);\n    mcmc_disconnect(be->client);\n    // we leave the main event alone, because be_failed() always overwrites.\n\n    // check failure counters and schedule a retry.\n    be->failed_count++;\n    _backend_flap_check(be, err);\n    _backend_reschedule(be);\n}\n\nstatic int _prep_pending_write(struct mcp_backendconn_s *be) {\n    struct iovec *iovs = be->write_iovs;\n    io_pending_proxy_t *io = NULL;\n    int iovused = 0;\n    if (be->io_next == NULL) {\n        // separate pointer for how far into the list we've flushed.\n        io = STAILQ_FIRST(&be->io_head);\n    } else {\n        io = be->io_next;\n    }\n    assert(io != NULL);\n    for (; io; io = STAILQ_NEXT(io, io_next)) {\n        // TODO (v2): paranoia for now, but this check should never fire\n        if (io->flushed)\n            continue;\n\n        if (io->iovcnt + iovused > BE_IOV_MAX) {\n            // We will need to keep writing later.\n            break;\n        }\n\n        memcpy(&iovs[iovused], io->iov, sizeof(struct iovec)*io->iovcnt);\n        iovused += io->iovcnt;\n    }\n    return iovused;\n}\n\n// returns true if any pending writes were fully flushed.\nstatic void _post_pending_write(struct mcp_backendconn_s *be, ssize_t sent) {\n    io_pending_proxy_t *io = be->io_next;\n    if (io == NULL) {\n        io = STAILQ_FIRST(&be->io_head);\n    }\n\n    for (; io; io = STAILQ_NEXT(io, io_next)) {\n        bool flushed = true;\n        if (io->flushed)\n            continue;\n        if (sent >= io->iovbytes) {\n            // short circuit for common case.\n            sent -= io->iovbytes;\n        } else {\n            io->iovbytes -= sent;\n            for (int x = 0; x < io->iovcnt; x++) {\n                struct iovec *iov = &io->iov[x];\n                if (sent >= iov->iov_len) {\n                    sent -= iov->iov_len;\n                    iov->iov_len = 0;\n                } else {\n                    iov->iov_len -= sent;\n                    iov->iov_base = (char *)iov->iov_base + sent;\n                    sent = 0;\n                    flushed = false;\n                    break;\n                }\n            }\n        }\n        io->flushed = flushed;\n        if (flushed) {\n            be->pending_read++;\n        }\n\n        if (sent <= 0) {\n            // really shouldn't be negative, though.\n            assert(sent >= 0);\n            break;\n        }\n    } // for\n\n    // resume the flush from this point.\n    if (io != NULL) {\n        if (!io->flushed) {\n            be->io_next = io;\n        } else {\n            // Check for incomplete list because we hit the iovcnt limit.\n            io_pending_proxy_t *nio = STAILQ_NEXT(io, io_next);\n            if (nio != NULL && !nio->flushed) {\n                be->io_next = nio;\n            } else {\n                be->io_next = NULL;\n            }\n        }\n    } else {\n        be->io_next = NULL;\n    }\n}\n\nstatic int _flush_pending_write(struct mcp_backendconn_s *be) {\n    int flags = 0;\n    // Allow us to be called with an empty stack to prevent dev errors.\n    if (STAILQ_EMPTY(&be->io_head)) {\n        return 0;\n    }\n\n    int iovcnt = _prep_pending_write(be);\n\n    ssize_t sent = writev(mcmc_fd(be->client), be->write_iovs, iovcnt);\n    if (sent > 0) {\n        _post_pending_write(be, sent);\n        // still have unflushed pending IO's, check for write and re-loop.\n        if (be->io_next) {\n            be->can_write = false;\n            flags |= EV_WRITE;\n        }\n    } else if (sent == -1) {\n        if (errno == EAGAIN || errno == EWOULDBLOCK) {\n            be->can_write = false;\n            flags |= EV_WRITE;\n        } else {\n            flags = -1;\n        }\n    }\n\n    return flags;\n}\n\nstatic int _flush_pending_tls_write(struct mcp_backendconn_s *be) {\n    int flags = 0;\n    // Allow us to be called with an empty stack to prevent dev errors.\n    if (STAILQ_EMPTY(&be->io_head)) {\n        return 0;\n    }\n\n    int iovcnt = _prep_pending_write(be);\n\n    int sent = mcp_tls_writev(be, iovcnt);\n    if (sent > 0) {\n        _post_pending_write(be, sent);\n        // FIXME: can _post_pending_write do this and return EV_WRITE?\n        // still have unflushed pending IO's, check for write and re-loop.\n        if (be->io_next) {\n            be->can_write = false;\n            flags |= EV_WRITE;\n        }\n    } else if (sent == MCP_TLS_NEEDIO) {\n        // want io\n        be->can_write = false;\n        flags |= EV_WRITE;\n    } else if (sent == MCP_TLS_ERR) {\n        // hard error from tls\n        flags = -1;\n    }\n\n    return flags;\n}\n\n\nstatic void proxy_bevalidate_tls_handler(const int fd, const short which, void *arg) {\n    assert(arg != NULL);\n    struct mcp_backendconn_s *be = arg;\n    int flags = EV_TIMEOUT;\n    struct timeval tmp_time = be->tunables.read;\n\n    if (which & EV_TIMEOUT) {\n        P_DEBUG(\"%s: backend timed out while connecting [fd: %d]\\n\", __func__, mcmc_fd(be->client));\n        if (be->connecting) {\n            _reset_bad_backend(be, P_BE_FAIL_CONNTIMEOUT);\n        } else {\n            _reset_bad_backend(be, P_BE_FAIL_READVALIDATE);\n        }\n        return;\n    }\n\n    if (which & EV_READ) {\n        int read = mcp_tls_read(be);\n\n        if (read > 0) {\n            mcmc_resp_t r;\n\n            int status = mcmc_parse_buf(be->rbuf, be->rbufused, &r);\n            if (status == MCMC_ERR) {\n                // Needed more data for a version line, somehow. I feel like\n                // this should set off some alarms, but it is possible.\n                if (r.code == MCMC_WANT_READ) {\n                    _set_main_event(be, be->event_thread->base, EV_READ, &tmp_time, proxy_bevalidate_tls_handler);\n                    return;\n                }\n\n                _reset_bad_backend(be, P_BE_FAIL_READVALIDATE);\n                return;\n            }\n\n            if (r.code != MCMC_CODE_VERSION) {\n                _reset_bad_backend(be, P_BE_FAIL_BADVALIDATE);\n                return;\n            }\n\n            be->validating = false;\n            be->rbufused = 0;\n        } else if (read == 0) {\n            // not connected or error.\n            _reset_bad_backend(be, P_BE_FAIL_DISCONNECTED);\n            return;\n        } else if (read == MCP_TLS_NEEDIO) {\n            // try again failure.\n            _set_main_event(be, be->event_thread->base, EV_READ, &tmp_time, proxy_bevalidate_tls_handler);\n            return;\n        } else if (read == MCP_TLS_ERR) {\n            // hard failure.\n            _reset_bad_backend(be, P_BE_FAIL_READING);\n            return;\n        }\n\n        // Passed validation, don't need to re-read, flush any pending writes.\n        int res = _flush_pending_tls_write(be);\n        if (res == -1) {\n            _reset_bad_backend(be, P_BE_FAIL_WRITING);\n            return;\n        }\n        if (flags & EV_WRITE) {\n            _start_write_event(be);\n        }\n        if (be->pending_read) {\n            _start_timeout_event(be);\n        }\n    }\n\n    // switch to the primary persistent read event.\n    if (!be->validating) {\n        _set_main_event(be, be->event_thread->base, EV_READ|EV_PERSIST, NULL, proxy_backend_tls_handler);\n\n        // we're happily validated and switching to normal processing, so\n        // _now_ the backend is no longer \"bad\".\n        // If we reset the failed count earlier we then can fail the\n        // validation loop indefinitely without ever being marked bad.\n        if (be->bad) {\n            // was bad, need to mark as no longer bad in shared space.\n            mcp_sharedvm_delta(be->event_thread->ctx, SHAREDVM_BACKEND_IDX,\n                    be->be_parent->label, -1);\n        }\n        be->bad = false;\n        be->failed_count = 0;\n    }\n}\n\n// Libevent handler when we're in TLS mode. Unfortunately the code is\n// different enough to warrant its own function.\nstatic void proxy_beconn_tls_handler(const int fd, const short which, void *arg) {\n    assert(arg != NULL);\n    struct mcp_backendconn_s *be = arg;\n    //int flags = EV_TIMEOUT;\n    struct timeval tmp_time = be->tunables.read;\n\n    if (which & EV_TIMEOUT) {\n        P_DEBUG(\"%s: backend timed out while connecting [fd: %d]\\n\", __func__, mcmc_fd(be->client));\n        if (be->connecting) {\n            _reset_bad_backend(be, P_BE_FAIL_CONNTIMEOUT);\n        } else {\n            _reset_bad_backend(be, P_BE_FAIL_READVALIDATE);\n        }\n        return;\n    }\n\n    if (which & EV_WRITE) {\n        be->can_write = true;\n\n        if (be->connecting) {\n            if (_proxy_beconn_checkconnect(be) == -1) {\n                return;\n            }\n            // TODO: check return code.\n            mcp_tls_connect(be);\n            // fall through to handshake attempt.\n        }\n    }\n\n    assert(be->validating);\n    int ret = mcp_tls_handshake(be);\n    if (ret == MCP_TLS_NEEDIO) {\n        // Need to try again.\n        _set_main_event(be, be->event_thread->base, EV_READ, &tmp_time, proxy_beconn_tls_handler);\n        return;\n    } else if (ret == 1) {\n        // handshake complete.\n        if (mcp_tls_send_validate(be) != MCP_TLS_OK) {\n            _reset_bad_backend(be, P_BE_FAIL_BADVALIDATE);\n            return;\n        }\n\n        // switch to another handler for the final stage.\n        _set_main_event(be, be->event_thread->base, EV_READ, &tmp_time, proxy_bevalidate_tls_handler);\n    } else if (ret < 0) {\n        // FIXME: FAIL_HANDSHAKE\n        _reset_bad_backend(be, P_BE_FAIL_BADVALIDATE);\n        return;\n    }\n}\n\n// Libevent handler for backends in a connecting state.\nstatic void proxy_beconn_handler(const int fd, const short which, void *arg) {\n    assert(arg != NULL);\n    struct mcp_backendconn_s *be = arg;\n    int flags = EV_TIMEOUT;\n    struct timeval tmp_time = be->tunables.read;\n\n    if (which & EV_TIMEOUT) {\n        P_DEBUG(\"%s: backend timed out while connecting [fd: %d]\\n\", __func__, mcmc_fd(be->client));\n        if (be->connecting) {\n            _reset_bad_backend(be, P_BE_FAIL_CONNTIMEOUT);\n        } else {\n            _reset_bad_backend(be, P_BE_FAIL_READVALIDATE);\n        }\n        return;\n    }\n\n    if (which & EV_WRITE) {\n        be->can_write = true;\n\n        if (be->connecting) {\n            if (_proxy_beconn_checkconnect(be) == -1) {\n                return;\n            }\n            if (_beconn_send_validate(be) == -1) {\n                _reset_bad_backend(be, P_BE_FAIL_BADVALIDATE);\n                return;\n            }\n            _set_main_event(be, be->event_thread->base, EV_READ, &tmp_time, proxy_beconn_handler);\n        }\n\n        // TODO: currently never taken, until validation is made optional.\n        if (!be->validating) {\n            int res = _flush_pending_write(be);\n            if (res == -1) {\n                _reset_bad_backend(be, P_BE_FAIL_WRITING);\n                return;\n            }\n            flags |= res;\n            // FIXME: set write event?\n        }\n    }\n\n    if (which & EV_READ) {\n        assert(be->validating);\n\n        int read = recv(mcmc_fd(be->client), be->rbuf + be->rbufused, READ_BUFFER_SIZE - be->rbufused, 0);\n        if (read > 0) {\n            mcmc_resp_t r;\n            be->rbufused += read;\n\n            int status = mcmc_parse_buf(be->rbuf, be->rbufused, &r);\n            if (status == MCMC_ERR) {\n                // Needed more data for a version line, somehow. I feel like\n                // this should set off some alarms, but it is possible.\n                if (r.code == MCMC_WANT_READ) {\n                    _set_main_event(be, be->event_thread->base, EV_READ, &tmp_time, proxy_beconn_handler);\n                    return;\n                }\n\n                _reset_bad_backend(be, P_BE_FAIL_READVALIDATE);\n                return;\n            }\n\n            if (r.code != MCMC_CODE_VERSION) {\n                _reset_bad_backend(be, P_BE_FAIL_BADVALIDATE);\n                return;\n            }\n\n            be->validating = false;\n            be->rbufused = 0;\n        } else if (read == 0) {\n            // not connected or error.\n            _reset_bad_backend(be, P_BE_FAIL_DISCONNECTED);\n            return;\n        } else if (read == -1) {\n            // sit on epoll again.\n            if (errno != EAGAIN && errno != EWOULDBLOCK) {\n                _reset_bad_backend(be, P_BE_FAIL_READING);\n                return;\n            }\n            _set_main_event(be, be->event_thread->base, EV_READ, &tmp_time, proxy_beconn_handler);\n            return;\n        }\n\n        // Passed validation, don't need to re-read, flush any pending writes.\n        int res = _flush_pending_write(be);\n        if (res == -1) {\n            _reset_bad_backend(be, P_BE_FAIL_WRITING);\n            return;\n        }\n        if (res & EV_WRITE) {\n            _start_write_event(be);\n        }\n        if (be->pending_read) {\n            _start_timeout_event(be);\n        }\n    }\n\n    // switch to the primary persistent read event.\n    if (!be->validating) {\n        _set_main_event(be, be->event_thread->base, EV_READ|EV_PERSIST, NULL, proxy_backend_handler);\n\n        // we're happily validated and switching to normal processing, so\n        // _now_ the backend is no longer \"bad\".\n        // If we reset the failed count earlier we then can fail the\n        // validation loop indefinitely without ever being marked bad.\n        if (be->bad) {\n            // was bad, need to mark as no longer bad in shared space.\n            mcp_sharedvm_delta(be->event_thread->ctx, SHAREDVM_BACKEND_IDX,\n                    be->be_parent->label, -1);\n        }\n        be->bad = false;\n        be->failed_count = 0;\n    }\n}\n\nstatic void proxy_backend_tls_handler(const int fd, const short which, void *arg) {\n    struct mcp_backendconn_s *be = arg;\n\n    if (which & EV_TIMEOUT) {\n        P_DEBUG(\"%s: timeout received, killing backend queue\\n\", __func__);\n        _reset_bad_backend(be, P_BE_FAIL_TIMEOUT);\n        return;\n    }\n\n    if (which & EV_WRITE) {\n        be->can_write = true;\n        int res = _flush_pending_tls_write(be);\n        if (res == -1) {\n            _reset_bad_backend(be, P_BE_FAIL_WRITING);\n            return;\n        }\n        if (res & EV_WRITE) {\n            _start_write_event(be);\n        }\n    }\n\n    if (which & EV_READ) {\n        // got a read event, always kill the pending read timer.\n        _stop_timeout_event(be);\n        // We do the syscall here before diving into the state machine to allow a\n        // common code path for io_uring/epoll/tls/etc\n        int read = mcp_tls_read(be);\n        if (read > 0) {\n            int res = proxy_backend_drive_machine(be);\n            if (res != 0) {\n                _reset_bad_backend(be, res);\n                return;\n            }\n        } else if (read == 0) {\n            // not connected or error.\n            _reset_bad_backend(be, P_BE_FAIL_DISCONNECTED);\n            return;\n        } else if (read == MCP_TLS_NEEDIO) {\n            // sit on epoll again.\n            return;\n        } else if (read == MCP_TLS_ERR) {\n            _reset_bad_backend(be, P_BE_FAIL_READING);\n            return;\n        }\n\n#ifdef PROXY_DEBUG\n        if (!STAILQ_EMPTY(&be->io_head)) {\n            P_DEBUG(\"backend has leftover IOs: %d\\n\", be->depth);\n        }\n#endif\n    }\n\n    if (be->pending_read) {\n        _start_timeout_event(be);\n    }\n}\n\n// The libevent backend callback handler.\n// If we end up resetting a backend, it will get put back into a connecting\n// state.\nstatic void proxy_backend_handler(const int fd, const short which, void *arg) {\n    struct mcp_backendconn_s *be = arg;\n\n    if (which & EV_TIMEOUT) {\n        P_DEBUG(\"%s: timeout received, killing backend queue\\n\", __func__);\n        _reset_bad_backend(be, P_BE_FAIL_TIMEOUT);\n        return;\n    }\n\n    if (which & EV_WRITE) {\n        be->can_write = true;\n        int res = _flush_pending_write(be);\n        if (res == -1) {\n            _reset_bad_backend(be, P_BE_FAIL_WRITING);\n            return;\n        }\n        if (res & EV_WRITE) {\n            _start_write_event(be);\n        }\n    }\n\n    if (which & EV_READ) {\n        // got a read event, always kill the pending read timer.\n        _stop_timeout_event(be);\n        // We do the syscall here before diving into the state machine to allow a\n        // common code path for io_uring/epoll\n        int read = recv(mcmc_fd(be->client), be->rbuf + be->rbufused,\n                    READ_BUFFER_SIZE - be->rbufused, 0);\n        if (read > 0) {\n            be->rbufused += read;\n            int res = proxy_backend_drive_machine(be);\n            if (res != 0) {\n                _reset_bad_backend(be, res);\n                return;\n            }\n        } else if (read == 0) {\n            // not connected or error.\n            _reset_bad_backend(be, P_BE_FAIL_DISCONNECTED);\n            return;\n        } else if (read == -1) {\n            // sit on epoll again.\n            if (errno != EAGAIN && errno != EWOULDBLOCK) {\n                _reset_bad_backend(be, P_BE_FAIL_READING);\n                return;\n            }\n        }\n\n#ifdef PROXY_DEBUG\n        if (!STAILQ_EMPTY(&be->io_head)) {\n            P_DEBUG(\"backend has leftover IOs: %d\\n\", be->depth);\n        }\n#endif\n    }\n\n    if (be->pending_read) {\n        _start_timeout_event(be);\n    }\n}\n\nvoid proxy_init_event_thread(proxy_event_thread_t *t, proxy_ctx_t *ctx, struct event_base *base) {\n    t->ctx = ctx;\n#ifdef USE_EVENTFD\n    t->event_fd = eventfd(0, EFD_NONBLOCK);\n    if (t->event_fd == -1) {\n        perror(\"failed to create backend notify eventfd\");\n        exit(1);\n    }\n    t->be_event_fd = eventfd(0, EFD_NONBLOCK);\n    if (t->be_event_fd == -1) {\n        perror(\"failed to create backend notify eventfd\");\n        exit(1);\n    }\n#else\n    int fds[2];\n    if (pipe(fds)) {\n        perror(\"can't create proxy backend notify pipe\");\n        exit(1);\n    }\n\n    t->notify_receive_fd = fds[0];\n    t->notify_send_fd = fds[1];\n\n    if (pipe(fds)) {\n        perror(\"can't create proxy backend connection notify pipe\");\n        exit(1);\n    }\n    t->be_notify_receive_fd = fds[0];\n    t->be_notify_send_fd = fds[1];\n#endif\n\n    // incoming request queue.\n    STAILQ_INIT(&t->io_head_in);\n    STAILQ_INIT(&t->beconn_head_in);\n    pthread_mutex_init(&t->mutex, NULL);\n    pthread_cond_init(&t->cond, NULL);\n\n    // initialize the event system.\n\n#ifdef HAVE_LIBURING\n    if (t->ctx->use_uring) {\n        fprintf(stderr, \"Sorry, io_uring not supported right now\\n\");\n        abort();\n    }\n#endif\n\n    if (base == NULL) {\n        struct event_config *ev_config;\n        ev_config = event_config_new();\n        event_config_set_flag(ev_config, EVENT_BASE_FLAG_NOLOCK);\n        t->base = event_base_new_with_config(ev_config);\n        event_config_free(ev_config);\n        if (! t->base) {\n            fprintf(stderr, \"Can't allocate event base\\n\");\n            exit(1);\n        }\n    } else {\n        // reusing an event base from a worker thread.\n        t->base = base;\n    }\n\n    // listen for notifications.\n    // NULL was thread_libevent_process\n    // FIXME (v2): use modern format? (event_assign)\n#ifdef USE_EVENTFD\n    event_set(&t->notify_event, t->event_fd,\n          EV_READ | EV_PERSIST, proxy_event_handler, t);\n    event_set(&t->beconn_event, t->be_event_fd,\n          EV_READ | EV_PERSIST, proxy_event_beconn, t);\n#else\n    event_set(&t->notify_event, t->notify_receive_fd,\n          EV_READ | EV_PERSIST, proxy_event_handler, t);\n    event_set(&t->beconn_event, t->be_notify_receive_fd,\n          EV_READ | EV_PERSIST, proxy_event_beconn, t);\n#endif\n\n    event_base_set(t->base, &t->notify_event);\n    if (event_add(&t->notify_event, 0) == -1) {\n        fprintf(stderr, \"Can't monitor libevent notify pipe\\n\");\n        exit(1);\n    }\n    event_base_set(t->base, &t->beconn_event);\n    if (event_add(&t->beconn_event, 0) == -1) {\n        fprintf(stderr, \"Can't monitor libevent notify pipe\\n\");\n        exit(1);\n    }\n}\n\n\n"
        },
        {
          "name": "proxy_ratelim.c",
          "type": "blob",
          "size": 5.1748046875,
          "content": "/* -*- Mode: C; tab-width: 4; c-basic-offset: 4; indent-tabs-mode: nil -*- */\n\n#include \"proxy.h\"\n\n// No GC necessary.\nstruct mcp_ratelim_tbf {\n    uint32_t bucket;\n    uint32_t limit;\n    uint32_t fill_rate; // tokens to add per tick rate\n    uint32_t tick_rate; // in milliseconds\n    int64_t last_update; // time in milliseconds\n};\n\nstruct mcp_ratelim_global_tbf {\n    struct mcp_globalobj_s g;\n    struct mcp_ratelim_tbf tbf;\n};\n\n#define TIMEVAL_TO_MILLIS(n) (n.tv_usec / 1000 + n.tv_sec * (uint64_t)1000)\n\n// global config VM object GC\nint mcplib_ratelim_global_tbf_gc(lua_State *L) {\n    struct mcp_ratelim_global_tbf *lim = luaL_checkudata(L, 1, \"mcp.ratelim_global_tbf\");\n    assert(lim->g.refcount == 0);\n    mcp_gobj_finalize(&lim->g);\n\n    // no other memory to directly free, just kill the mutex.\n    return 0;\n}\n\n// worker thread proxy object GC\nint mcplib_ratelim_proxy_tbf_gc(lua_State *L) {\n    struct mcp_ratelim_global_tbf **lim_p = luaL_checkudata(L, 1, \"mcp.ratelim_proxy_tbf\");\n    struct mcp_ratelim_global_tbf *lim = *lim_p;\n    proxy_ctx_t *ctx = PROXY_GET_THR_CTX(L);\n    mcp_gobj_unref(ctx, &lim->g);\n\n    return 0;\n}\n\nint mcp_ratelim_proxy_tbf(lua_State *from, lua_State *to) {\n    // from, -3 should have the userdata.\n    struct mcp_ratelim_global_tbf *lim = luaL_checkudata(from, -3, \"mcp.ratelim_global_tbf\");\n    struct mcp_ratelim_global_tbf **lim_p = lua_newuserdatauv(to, sizeof(struct mcp_ratelim_global_tbf *), 0);\n    luaL_setmetatable(to, \"mcp.ratelim_proxy_tbf\");\n\n    *lim_p = lim;\n    lua_pushvalue(from, -3); // copy ratelim obj to ref below\n    mcp_gobj_ref(from, &lim->g); // pops obj copy\n\n    return 0;\n}\n\nstatic lua_Integer _tbf_check(lua_State *L, char *key) {\n    lua_Integer n = 0;\n    if (lua_getfield(L, 1, key) != LUA_TNIL) {\n        n = lua_tointeger(L, -1);\n        if (n < 0 || n > UINT_MAX-1) {\n            proxy_lua_error(L, \"mcp.ratelim_tbf: arguments must be unsigned 32 bit integer\");\n        }\n    }\n    lua_pop(L, 1); // pops value or nil.\n    return n;\n}\n\nstatic void _setup_tbf(lua_State *L, struct mcp_ratelim_tbf *lim) {\n    struct timeval now;\n    luaL_checktype(L, 1, LUA_TTABLE);\n    lim->limit = _tbf_check(L, \"limit\");\n    lim->fill_rate = _tbf_check(L, \"fillrate\");\n    lim->tick_rate = _tbf_check(L, \"tickrate\");\n\n    // seed the token bucket filter.\n    lim->bucket = lim->limit;\n    gettimeofday(&now, NULL);\n    lim->last_update = TIMEVAL_TO_MILLIS(now);\n}\n\nint mcplib_ratelim_tbf(lua_State *L) {\n    struct mcp_ratelim_tbf *lim = lua_newuserdatauv(L, sizeof(*lim), 0);\n    memset(lim, 0, sizeof(*lim));\n    luaL_setmetatable(L, \"mcp.ratelim_tbf\");\n\n    _setup_tbf(L, lim);\n    return 1;\n}\n\nint mcplib_ratelim_global_tbf(lua_State *L) {\n    struct mcp_ratelim_global_tbf *lim = lua_newuserdatauv(L, sizeof(*lim), 0);\n    memset(lim, 0, sizeof(*lim));\n    // TODO: during next refactor, add \"globalobj init\" phase, which probably\n    // just does this.\n    pthread_mutex_init(&lim->g.lock, NULL);\n    luaL_setmetatable(L, \"mcp.ratelim_global_tbf\");\n\n    _setup_tbf(L, &lim->tbf);\n    return 1;\n}\n\nstatic int _update_tbf(struct mcp_ratelim_tbf *lim, int take, uint64_t now) {\n    uint64_t delta = 0;\n    delta = now - lim->last_update;\n\n    if (delta > lim->tick_rate) {\n        // find how many ticks to add to the bucket.\n        uint32_t toadd = delta / lim->tick_rate;\n        // advance time up to the most recent tick.\n        lim->last_update += toadd * lim->tick_rate;\n        // add tokens to the bucket\n        lim->bucket += toadd * lim->fill_rate;\n        if (lim->bucket > lim->limit) {\n            lim->bucket = lim->limit;\n        }\n    }\n\n    if (lim->bucket > take) {\n        lim->bucket -= take;\n        return 1;\n    } else {\n        return 0;\n    }\n}\n\nint mcplib_ratelim_tbf_call(lua_State *L) {\n    struct mcp_ratelim_tbf *lim = luaL_checkudata(L, 1, \"mcp.ratelim_tbf\");\n    luaL_checktype(L, 2, LUA_TNUMBER);\n    int take = lua_tointeger(L, 2);\n    struct timeval now;\n    uint64_t now_millis = 0;\n\n    gettimeofday(&now, NULL);\n    now_millis = TIMEVAL_TO_MILLIS(now);\n    lua_pushboolean(L, _update_tbf(lim, take, now_millis));\n\n    return 1;\n}\n\n// NOTE: it should be possible to run a TBF using atomics, in the future when\n// we start to support C11 atomics.\n// Flip the concept of checking the time, updating, then subtracting the take\n// to:\n// - how much \"time elapsed\" is necessary for the take requested\n// - atomically load the old time\n// - if not enough time delta between old time and now, return false\n// - else atomically swap the update time with the new time\n//   - compare and update the oldtime to newtime\n// - not sure how much perf this buys you. would have to test.\nint mcplib_ratelim_proxy_tbf_call(lua_State *L) {\n    struct mcp_ratelim_global_tbf **lim_p = luaL_checkudata(L, 1, \"mcp.ratelim_proxy_tbf\");\n    // line was kinda long / hard to read.\n    struct mcp_ratelim_global_tbf *lim = *lim_p;\n    struct timeval now;\n    luaL_checktype(L, 2, LUA_TNUMBER);\n    int take = lua_tointeger(L, 2);\n    gettimeofday(&now, NULL);\n    uint64_t now_millis = 0;\n    now_millis = TIMEVAL_TO_MILLIS(now);\n\n    pthread_mutex_lock(&lim->g.lock);\n    int res = _update_tbf(&lim->tbf, take, now_millis);\n    pthread_mutex_unlock(&lim->g.lock);\n\n    lua_pushboolean(L, res);\n    return 1;\n}\n"
        },
        {
          "name": "proxy_request.c",
          "type": "blob",
          "size": 38.4306640625,
          "content": "/* -*- Mode: C; tab-width: 4; c-basic-offset: 4; indent-tabs-mode: nil -*- */\n\n#include \"proxy.h\"\n\n#define PARSER_MAXLEN USHRT_MAX-1\n\n// Find the starting offsets of each token; ignoring length.\n// This creates a fast small (<= cacheline) index into the request,\n// where we later scan or directly feed data into API's.\nstatic int _process_tokenize(mcp_parser_t *pr, const size_t max) {\n    const char *s = pr->request;\n    int len = pr->endlen;\n\n    // since multigets can be huge, we can't purely judge reqlen against this\n    // limit, but we also can't index past it since the tokens are shorts.\n    if (len > PARSER_MAXLEN) {\n        len = PARSER_MAXLEN;\n    }\n    const char *end = s + len;\n    int curtoken = 0;\n\n    int state = 0;\n    while (s != end) {\n        switch (state) {\n            case 0:\n                // scanning for first non-space to find a token.\n                if (*s != ' ') {\n                    pr->tokens[curtoken] = s - pr->request;\n                    if (++curtoken == max) {\n                        s++;\n                        state = 2;\n                        break;\n                    }\n                    state = 1;\n                }\n                s++;\n                break;\n            case 1:\n                // advance over a token\n                if (*s != ' ') {\n                    s++;\n                } else {\n                    state = 0;\n                }\n                break;\n            case 2:\n                // hit max tokens before end of the line.\n                // keep advancing so we can place endcap token.\n                if (*s == ' ') {\n                    goto endloop;\n                }\n                s++;\n                break;\n        }\n    }\nendloop:\n\n    // endcap token so we can quickly find the length of any token by looking\n    // at the next one.\n    pr->tokens[curtoken] = s - pr->request;\n    pr->ntokens = curtoken;\n    P_DEBUG(\"%s: cur_tokens: %d\\n\", __func__, curtoken);\n\n    return 0;\n}\n\nstatic int _process_token_len(mcp_parser_t *pr, size_t token) {\n    const char *s = pr->request + pr->tokens[token];\n    const char *e = pr->request + pr->tokens[token+1];\n    // start of next token is after any space delimiters, so back those out.\n    while (*(e-1) == ' ') {\n        e--;\n    }\n    return e - s;\n}\n\nstatic int _process_request_key(mcp_parser_t *pr) {\n    pr->klen = _process_token_len(pr, pr->keytoken);\n    // advance the parser in case of multikey.\n    pr->parsed = pr->tokens[pr->keytoken] + pr->klen + 1;\n\n    if (pr->request[pr->parsed-1] == ' ') {\n        P_DEBUG(\"%s: request_key found extra space\\n\", __func__);\n        pr->has_space = true;\n    } else {\n        pr->has_space = false;\n    }\n    return 0;\n}\n\n// Just for ascii multiget: search for next \"key\" beyond where we stopped\n// tokenizing before.\n// Returns the offset for the next key.\nsize_t _process_request_next_key(mcp_parser_t *pr) {\n    const char *cur = pr->request + pr->parsed;\n    int remain = pr->endlen - pr->parsed;\n\n    // chew off any leading whitespace.\n    while (remain) {\n        if (*cur == ' ') {\n            remain--;\n            cur++;\n            pr->parsed++;\n        } else {\n            break;\n        }\n    }\n\n    const char *s = memchr(cur, ' ', remain);\n    if (s != NULL) {\n        pr->klen = s - cur;\n        pr->parsed += s - cur;\n    } else {\n        pr->klen = remain;\n        pr->parsed += remain;\n    }\n\n    return cur - pr->request;\n}\n\n// for fast testing of existence of meta flags.\n// meta has all flags as final tokens\nstatic int _process_request_metaflags(mcp_parser_t *pr, int token) {\n    if (pr->ntokens <= token) {\n        pr->t.meta.flags = 0; // no flags found.\n        return 0;\n    }\n    const char *cur = pr->request + pr->tokens[token];\n    const char *end = pr->request + pr->endlen;\n\n    // We blindly convert flags into bits, since the range of possible\n    // flags is deliberately < 64.\n    int state = 0;\n    while (cur != end) {\n        switch (state) {\n            case 0:\n                if (*cur == ' ') {\n                    cur++;\n                } else {\n                    if (*cur < 65 || *cur > 122) {\n                        return -1;\n                    }\n                    P_DEBUG(\"%s: setting meta flag: %d\\n\", __func__, *cur - 65);\n                    pr->t.meta.flags |= (uint64_t)1 << (*cur - 65);\n                    state = 1;\n                }\n                break;\n            case 1:\n                if (*cur != ' ') {\n                    cur++;\n                } else {\n                    state = 0;\n                }\n                break;\n        }\n    }\n\n    // not too great hack for noreply detection: this can be flattened out\n    // once a few other contexts are fixed and we detect the noreply from the\n    // coroutine start instead.\n    if (pr->t.meta.flags & ((uint64_t)1 << 48)) {\n        pr->noreply = true;\n    }\n\n    return 0;\n}\n\n// All meta commands are of form: \"cm key f l a g S100\"\nstatic int _process_request_meta(mcp_parser_t *pr) {\n    _process_tokenize(pr, PARSER_MAX_TOKENS);\n    if (pr->ntokens < 2) {\n        P_DEBUG(\"%s: not enough tokens for meta command: %d\\n\", __func__, pr->ntokens);\n        return -1;\n    }\n    pr->keytoken = 1;\n    _process_request_key(pr);\n\n    // pass the first flag token.\n    return _process_request_metaflags(pr, 2);\n}\n\n// ms <key> <datalen> <flags>*\\r\\n\nstatic int _process_request_mset(mcp_parser_t *pr) {\n    _process_tokenize(pr, PARSER_MAX_TOKENS);\n    if (pr->ntokens < 3) {\n        P_DEBUG(\"%s: not enough tokens for meta set command: %d\\n\", __func__, pr->ntokens);\n        return -1;\n    }\n    pr->keytoken = 1;\n    _process_request_key(pr);\n\n    const char *cur = pr->request + pr->tokens[2];\n\n    errno = 0;\n    char *n = NULL;\n    int vlen = strtol(cur, &n, 10);\n    if ((errno == ERANGE) || (cur == n)) {\n        return -1;\n    }\n\n    if (vlen < 0 || vlen > (INT_MAX - 2)) {\n       return -1;\n    }\n    vlen += 2;\n\n    pr->vlen = vlen;\n\n    // pass the first flag token\n    return _process_request_metaflags(pr, 3);\n}\n\n// gat[s] <exptime> <key>*\\r\\n\nstatic int _process_request_gat(mcp_parser_t *pr) {\n    _process_tokenize(pr, 3);\n    if (pr->ntokens < 3) {\n        P_DEBUG(\"%s: not enough tokens for GAT: %d\\n\", __func__, pr->ntokens);\n        return -1;\n    }\n\n    pr->keytoken = 2;\n    _process_request_key(pr);\n    return 0;\n}\n\n#define NOREPLYSTR \"noreply\"\n#define NOREPLYLEN sizeof(NOREPLYSTR)-1\n// given a tokenized parser for a normal ASCII command, checks for noreply\n// mode.\nstatic int _process_request_noreply(mcp_parser_t *pr) {\n    if (pr->tokens[pr->ntokens] - pr->tokens[pr->ntokens-1] >= NOREPLYLEN\n            && strncmp(NOREPLYSTR, pr->request + pr->tokens[pr->ntokens-1], NOREPLYLEN) == 0) {\n        pr->noreply = true;\n    }\n    return 0;\n}\n\n// we need t find the bytes supplied immediately so we can read the request\n// from the client properly.\n// set <key> <flags> <exptime> <bytes> [noreply]\\r\\n\nstatic int _process_request_storage(mcp_parser_t *pr, size_t max) {\n    _process_tokenize(pr, max);\n    if (pr->ntokens < 5) {\n        P_DEBUG(\"%s: not enough tokens to storage command: %d\\n\", __func__, pr->ntokens);\n        return -1;\n    }\n    pr->keytoken = 1;\n    _process_request_key(pr);\n\n    errno = 0;\n    char *n = NULL;\n    const char *cur = pr->request + pr->tokens[4];\n\n    int vlen = strtol(cur, &n, 10);\n    if ((errno == ERANGE) || (cur == n)) {\n        return -1;\n    }\n\n    if (vlen < 0 || vlen > (INT_MAX - 2)) {\n       return -1;\n    }\n    vlen += 2;\n\n    pr->vlen = vlen;\n\n    return _process_request_noreply(pr);\n}\n\n// common request with key: <cmd> <key> <args>\nstatic int _process_request_simple(mcp_parser_t *pr, const int min, const int max) {\n    _process_tokenize(pr, max);\n    if (pr->ntokens < min) {\n        P_DEBUG(\"%s: not enough tokens for simple request: %d\\n\", __func__, pr->ntokens);\n        return -1;\n    }\n    pr->keytoken = 1; // second token is usually the key... stupid GAT.\n\n    _process_request_key(pr);\n    return _process_request_noreply(pr);\n}\n\n// TODO: return code ENUM with error types.\n// FIXME: the mcp_parser_t bits have ended up being more fragile than I hoped.\n// careful zero'ing is required. revisit?\n// I think this mostly refers to recursive work (maybe just multiget?)\n// Is a parser object run throgh process_request() twice, ever?\nint process_request(mcp_parser_t *pr, const char *command, size_t cmdlen) {\n    // we want to \"parse in place\" as much as possible, which allows us to\n    // forward an unmodified request without having to rebuild it.\n\n    const char *cm = command;\n    size_t cl = 0;\n    // min command length is 2, plus the \"\\r\\n\"\n    if (cmdlen < 4) {\n        return -1;\n    }\n\n    // Commands can end with bare '\\n's. Depressingly I intended to be strict\n    // with a \\r\\n requirement but never did this and need backcompat.\n    // In this case we _know_ \\n is at cmdlen because we can't enter this\n    // function otherwise.\n    if (cm[cmdlen-2] == '\\r') {\n        pr->endlen = cmdlen - 2;\n    } else {\n        pr->endlen = cmdlen - 1;\n    }\n\n    const char *s = memchr(command, ' ', pr->endlen);\n    if (s != NULL) {\n        cl = s - command;\n    } else {\n        cl = pr->endlen;\n    }\n    pr->keytoken = 0;\n    pr->has_space = false;\n    pr->parsed = cl;\n    pr->request = command;\n    pr->reqlen = cmdlen;\n    int token_max = PARSER_MAX_TOKENS;\n\n    int cmd = -1;\n    int type = CMD_TYPE_GENERIC;\n    int ret = 0;\n\n    switch (cl) {\n        case 0:\n        case 1:\n            // falls through with cmd as -1. should error.\n            break;\n        case 2:\n            if (cm[0] == 'm') {\n                type = CMD_TYPE_META;\n                switch (cm[1]) {\n                    case 'g':\n                        cmd = CMD_MG;\n                        ret = _process_request_meta(pr);\n                        break;\n                    case 's':\n                        cmd = CMD_MS;\n                        ret = _process_request_mset(pr);\n                        break;\n                    case 'd':\n                        cmd = CMD_MD;\n                        ret = _process_request_meta(pr);\n                        break;\n                    case 'n':\n                        // TODO: do we route/handle NOP's at all?\n                        // they should simply reflect to the client.\n                        cmd = CMD_MN;\n                        break;\n                    case 'a':\n                        cmd = CMD_MA;\n                        ret = _process_request_meta(pr);\n                        break;\n                    case 'e':\n                        cmd = CMD_ME;\n                        // TODO: not much special processing here; binary keys\n                        ret = _process_request_meta(pr);\n                        break;\n                }\n            }\n            break;\n        case 3:\n            if (cm[0] == 'g') {\n                if (cm[1] == 'e' && cm[2] == 't') {\n                    cmd = CMD_GET;\n                    type = CMD_TYPE_GET;\n                    token_max = 2; // don't chew through multigets.\n                    ret = _process_request_simple(pr, 2, 2);\n                }\n                if (cm[1] == 'a' && cm[2] == 't') {\n                    type = CMD_TYPE_GET;\n                    cmd = CMD_GAT;\n                    token_max = 2; // don't chew through multigets.\n                    ret = _process_request_gat(pr);\n                }\n            } else if (cm[0] == 's' && cm[1] == 'e' && cm[2] == 't') {\n                cmd = CMD_SET;\n                ret = _process_request_storage(pr, token_max);\n            } else if (cm[0] == 'a' && cm[1] == 'd' && cm[2] == 'd') {\n                cmd = CMD_ADD;\n                ret = _process_request_storage(pr, token_max);\n            } else if (cm[0] == 'c' && cm[1] == 'a' && cm[2] == 's') {\n                cmd = CMD_CAS;\n                ret = _process_request_storage(pr, token_max);\n            }\n            break;\n        case 4:\n            if (strncmp(cm, \"gets\", 4) == 0) {\n                cmd = CMD_GETS;\n                type = CMD_TYPE_GET;\n                token_max = 2; // don't chew through multigets.\n                ret = _process_request_simple(pr, 2, 2);\n            } else if (strncmp(cm, \"incr\", 4) == 0) {\n                cmd = CMD_INCR;\n                ret = _process_request_simple(pr, 3, 4);\n            } else if (strncmp(cm, \"decr\", 4) == 0) {\n                cmd = CMD_DECR;\n                ret = _process_request_simple(pr, 3, 4);\n            } else if (strncmp(cm, \"gats\", 4) == 0) {\n                cmd = CMD_GATS;\n                type = CMD_TYPE_GET;\n                ret = _process_request_gat(pr);\n            } else if (strncmp(cm, \"quit\", 4) == 0) {\n                cmd = CMD_QUIT;\n            }\n            break;\n        case 5:\n            if (strncmp(cm, \"touch\", 5) == 0) {\n                cmd = CMD_TOUCH;\n                ret = _process_request_simple(pr, 3, 4);\n            } else if (strncmp(cm, \"stats\", 5) == 0) {\n                cmd = CMD_STATS;\n                // Don't process a key; fetch via arguments.\n                _process_tokenize(pr, token_max);\n            } else if (strncmp(cm, \"watch\", 5) == 0) {\n                cmd = CMD_WATCH;\n                _process_tokenize(pr, token_max);\n            }\n            break;\n        case 6:\n            if (strncmp(cm, \"delete\", 6) == 0) {\n                cmd = CMD_DELETE;\n                ret = _process_request_simple(pr, 2, 4);\n            } else if (strncmp(cm, \"append\", 6) == 0) {\n                cmd = CMD_APPEND;\n                ret = _process_request_storage(pr, token_max);\n            }\n            break;\n        case 7:\n            if (strncmp(cm, \"replace\", 7) == 0) {\n                cmd = CMD_REPLACE;\n                ret = _process_request_storage(pr, token_max);\n            } else if (strncmp(cm, \"prepend\", 7) == 0) {\n                cmd = CMD_PREPEND;\n                ret = _process_request_storage(pr, token_max);\n            } else if (strncmp(cm, \"version\", 7) == 0) {\n                cmd = CMD_VERSION;\n                _process_tokenize(pr, token_max);\n            }\n            break;\n    }\n\n    // TODO: log more specific error code.\n    if (cmd == -1 || ret != 0) {\n        return -1;\n    }\n\n    pr->command = cmd;\n    pr->cmd_type = type;\n\n    return 0;\n}\n\n// FIXME (v2): any reason to pass in command/cmdlen separately?\nmcp_request_t *mcp_new_request(lua_State *L, mcp_parser_t *pr, const char *command, size_t cmdlen) {\n    mcp_request_t *rq = lua_newuserdatauv(L, sizeof(mcp_request_t) + MCP_REQUEST_MAXLEN, 0);\n    // TODO (v2): memset only the non-data part? as the rest gets memcpy'd\n    // over.\n    memset(rq, 0, sizeof(mcp_request_t));\n    memcpy(&rq->pr, pr, sizeof(*pr));\n\n    memcpy(rq->request, command, cmdlen);\n    rq->pr.request = rq->request;\n    rq->pr.reqlen = cmdlen;\n\n    luaL_getmetatable(L, \"mcp.request\");\n    lua_setmetatable(L, -2);\n\n    // at this point we should know if we have to bounce through _nread to\n    // get item data or not.\n    return rq;\n}\n\n// fill a preallocated request object.\nvoid mcp_set_request(mcp_parser_t *pr, mcp_request_t *rq, const char *command, size_t cmdlen) {\n    memset(rq, 0, sizeof(mcp_request_t));\n    memcpy(&rq->pr, pr, sizeof(*pr));\n\n    memcpy(rq->request, command, cmdlen);\n    rq->pr.request = rq->request;\n    rq->pr.reqlen = cmdlen;\n}\n\n// Replaces a token inside a request and re-parses.\n// Note that this has some optimization opportunities. Delaying until\n// required.\n// We should not guarantee order when updating meta flags, which would allow\n// blanking tokens and appending new ones.\n// TODO (v2): much of the length is the key, avoid copying it.\nint mcp_request_render(mcp_request_t *rq, int idx, const char flag, const char *tok, size_t len) {\n    char temp[MCP_REQUEST_MAXLEN+1];\n    char *p = temp;\n    mcp_parser_t *pr = &rq->pr;\n\n    if (pr->reqlen + len + 2 > MCP_REQUEST_MAXLEN) {\n        return -1;\n    }\n    // Cannot add/append tokens yet.\n    if (idx >= pr->ntokens) {\n        return -1;\n    }\n\n    memcpy(p, pr->request, pr->tokens[idx]);\n    p += pr->tokens[idx];\n\n    if (flag) {\n        *p = flag;\n        p++;\n    }\n    if (tok) {\n        memcpy(p, tok, len);\n        p += len;\n    }\n\n    // Add a space and copy more tokens if there were more.\n    if (idx+1 < pr->ntokens) {\n        if (flag || len != 0) {\n            // Only pre-space if not deleting the token.\n            *p = ' ';\n            p++;\n        }\n        memcpy(p, &pr->request[pr->tokens[idx+1]], pr->tokens[pr->ntokens] - pr->tokens[idx+1]);\n        p += pr->tokens[pr->ntokens] - pr->tokens[idx+1];\n    } else {\n        // If we removed something from the end we might've left some spaces.\n        while (*(p-1) == ' ') {\n            p--;\n        }\n    }\n\n    memcpy(p, \"\\r\\n\\0\", 3);\n    p += 2;\n\n    memcpy(rq->request, temp, p - temp);\n\n    // Hold the vlen/vbuf and restore after re-parsing. Since we can only edit\n    // the command line, not the value here, we would otherwise allow sending\n    // arbitrary memory over the network if someone modifies a SET.\n    void *vbuf = pr->vbuf;\n    int vlen = pr->vlen;\n\n    memset(pr, 0, sizeof(mcp_parser_t)); // TODO: required?\n    int ret = process_request(pr, rq->request, p - temp);\n    if (ret != 0) {\n        return ret;\n    }\n    pr->vbuf = vbuf;\n    pr->vlen = vlen;\n    return 0;\n}\n\nint mcp_request_append(mcp_request_t *rq, const char flag, const char *tok, size_t len) {\n    mcp_parser_t *pr = &rq->pr;\n    const char *start = pr->request;\n    char *p = (char *)pr->request + pr->reqlen - 2; // start at the \\r\n    assert(*p == '\\r');\n\n    if (pr->reqlen + len + 2 > MCP_REQUEST_MAXLEN) {\n        return -1;\n    }\n\n    *p = ' ';\n    p++;\n\n    if (flag) {\n        *p = flag;\n        p++;\n    }\n    if (tok) {\n        memcpy(p, tok, len);\n        p += len;\n    }\n\n    memcpy(p, \"\\r\\n\\0\", 3);\n    p += 2;\n\n    // See note on mcp_request_render()\n    void *vbuf = pr->vbuf;\n    int vlen = pr->vlen;\n\n    memset(pr, 0, sizeof(mcp_parser_t)); // TODO: required?\n    int ret = process_request(pr, rq->request, p - start);\n    if (ret != 0) {\n        return ret;\n    }\n    pr->vbuf = vbuf;\n    pr->vlen = vlen;\n\n    return 0;\n}\n\nvoid mcp_request_attach(mcp_request_t *rq, io_pending_proxy_t *p) {\n    mcp_parser_t *pr = &rq->pr;\n    char *r = (char *) pr->request;\n    size_t len = pr->reqlen;\n\n    // The stringified request. This is also referencing into the coroutine\n    // stack, which should be safe from gc.\n    p->iov[0].iov_base = r;\n    p->iov[0].iov_len = len;\n    p->iovcnt = 1;\n    p->iovbytes = len;\n    if (pr->vlen != 0) {\n        p->iov[1].iov_base = pr->vbuf;\n        p->iov[1].iov_len = pr->vlen;\n        p->iovcnt = 2;\n        p->iovbytes += pr->vlen;\n    }\n}\n\n// second argument is optional, for building set requests.\n// TODO: append the \\r\\n for the VAL?\nint mcplib_request(lua_State *L) {\n    LIBEVENT_THREAD *t = PROXY_GET_THR(L);\n    size_t len = 0;\n    size_t vlen = 0;\n    mcp_parser_t pr = {0};\n    const char *cmd = luaL_checklstring(L, 1, &len);\n    const char *val = NULL;\n    int type = lua_type(L, 2);\n    if (type == LUA_TSTRING) {\n        val = luaL_optlstring(L, 2, NULL, &vlen);\n        if (vlen < 2 || memcmp(val+vlen-2, \"\\r\\n\", 2) != 0) {\n            proxy_lua_error(L, \"value passed to mcp.request must end with \\\\r\\\\n\");\n        }\n    } else if (type == LUA_TUSERDATA) {\n        // vlen for requests and responses include the \"\\r\\n\" already.\n        mcp_resp_t *r = luaL_testudata(L, 2, \"mcp.response\");\n        if (r != NULL) {\n            if (r->resp.value) {\n                val = r->resp.value;\n                vlen = r->resp.vlen_read; // paranoia, so we can't overread into memory.\n            }\n        } else {\n            mcp_request_t *rq = luaL_testudata(L, 2, \"mcp.request\");\n            if (rq->pr.vbuf) {\n                val = rq->pr.vbuf;\n                vlen = rq->pr.vlen;\n            }\n        }\n    }\n\n    if (len > MCP_REQUEST_MAXLEN) {\n        proxy_lua_error(L, \"request length too long\");\n        return 0;\n    }\n\n    // FIXME (v2): if we inline the userdata we can avoid memcpy'ing the parser\n    // structure from the stack? but causes some code duplication.\n    if (process_request(&pr, cmd, len) != 0) {\n        proxy_lua_error(L, \"failed to parse request\");\n        return 0;\n    }\n    mcp_request_t *rq = mcp_new_request(L, &pr, cmd, len);\n\n    if (val != NULL) {\n        rq->pr.vlen = vlen;\n        rq->pr.vbuf = malloc(vlen);\n        if (rq->pr.vbuf == NULL) {\n            // Note: without *c we can't tick the appropriate counter.\n            // However, in practice raw malloc's are nearly never going to\n            // fail.\n            // TODO(v2): we can stack values into the request objects or use\n            // the slabber memory, so this isn't necessary anyway.\n            proxy_lua_error(L, \"failed to allocate value memory for request object\");\n        }\n        memcpy(rq->pr.vbuf, val, vlen);\n        // Note: Not enforcing the memory limit here is deliberate:\n        // - if we're over the memory limit, it'll get caught very soon after\n        // this, but we won't be causing some lua to bail mid-flight, which is\n        // more graceful to the end user.\n        pthread_mutex_lock(&t->proxy_limit_lock);\n        t->proxy_buffer_memory_used += rq->pr.vlen;\n        pthread_mutex_unlock(&t->proxy_limit_lock);\n    }\n\n    // rq is now created, parsed, and on the stack.\n    return 1;\n}\n\nint mcplib_request_key(lua_State *L) {\n    mcp_request_t *rq = luaL_checkudata(L, -1, \"mcp.request\");\n    lua_pushlstring(L, MCP_PARSER_KEY(rq->pr), rq->pr.klen);\n    return 1;\n}\n\n// NOTE: I've mixed up const/non-const strings in the request. During parsing\n// we want it to be const, but after that's done the request is no longer\n// const. It might be better to just remove the const higher up the chain, but\n// I'd rather not. So for now these functions will be dumping the const to\n// modify the string.\nint mcplib_request_ltrimkey(lua_State *L) {\n    mcp_request_t *rq = luaL_checkudata(L, -2, \"mcp.request\");\n    int totrim = luaL_checkinteger(L, -1);\n    char *key = (char *) MCP_PARSER_KEY(rq->pr);\n\n    if (totrim > rq->pr.klen) {\n        proxy_lua_error(L, \"ltrimkey cannot zero out key\");\n        return 0;\n    } else {\n        memset(key, ' ', totrim);\n        rq->pr.klen -= totrim;\n        rq->pr.tokens[rq->pr.keytoken] += totrim;\n    }\n    return 1;\n}\n\nint mcplib_request_rtrimkey(lua_State *L) {\n    mcp_request_t *rq = luaL_checkudata(L, -2, \"mcp.request\");\n    int totrim = luaL_checkinteger(L, -1);\n    char *key = (char *) MCP_PARSER_KEY(rq->pr);\n\n    if (totrim > rq->pr.klen) {\n        proxy_lua_error(L, \"rtrimkey cannot zero out key\");\n        return 0;\n    } else {\n        memset(key + (rq->pr.klen - totrim), ' ', totrim);\n        rq->pr.klen -= totrim;\n        // don't need to change the key token.\n    }\n    return 1;\n}\n\n// Virtual table operations on the request.\nint mcplib_request_token(lua_State *L) {\n    mcp_request_t *rq = luaL_checkudata(L, 1, \"mcp.request\");\n    int argc = lua_gettop(L);\n\n    if (argc == 1) {\n        lua_pushnil(L);\n        return 1;\n    }\n\n    int token = luaL_checkinteger(L, 2);\n\n    if (token < 1 || token > rq->pr.ntokens) {\n        // maybe an error?\n        lua_pushnil(L);\n        return 1;\n    }\n\n    size_t vlen = 0;\n    if (argc > 2) {\n        // overwriting a token.\n        size_t newlen = 0;\n        const char *newtok = lua_tolstring(L, 3, &newlen);\n        if (mcp_request_render(rq, token-1, 0, newtok, newlen) != 0) {\n            proxy_lua_error(L, \"token(): request malformed after edit\");\n            return 0;\n        }\n        return 0;\n    } else {\n        // fetching a token.\n        const char *start = rq->pr.request + rq->pr.tokens[token-1];\n        vlen = _process_token_len(&rq->pr, token-1);\n\n        P_DEBUG(\"%s: pushing token of len: %lu\\n\", __func__, vlen);\n        lua_pushlstring(L, start, vlen);\n        return 1;\n    }\n\n    return 0;\n}\n\n// Fetch only.\nint mcplib_request_token_int(lua_State *L) {\n    mcp_request_t *rq = luaL_checkudata(L, 1, \"mcp.request\");\n    int argc = lua_gettop(L);\n\n    if (argc == 1) {\n        lua_pushnil(L);\n        return 1;\n    }\n\n    int x = luaL_checkinteger(L, 2);\n\n    if (x < 1 || x > rq->pr.ntokens) {\n        // maybe an error?\n        lua_pushnil(L);\n        return 1;\n    }\n\n    size_t vlen = 0;\n    // fetching a token.\n    const char *s = rq->pr.request + rq->pr.tokens[x-1];\n    vlen = _process_token_len(&rq->pr, x-1);\n    // do a funny dance to safely strtol the token.\n    // TODO: use tokenizer based tokto when merged.\n    char temp[22];\n    int tocopy = vlen > 22 ? 21 : vlen;\n    memcpy(temp, s, tocopy);\n    temp[vlen] = '\\0';\n    int64_t token = 0;\n    if (safe_strtoll(temp, &token)) {\n        lua_pushinteger(L, token);\n    } else {\n        lua_pushnil(L);\n    }\n\n    return 1;\n}\n\nint mcplib_request_ntokens(lua_State *L) {\n    mcp_request_t *rq = luaL_checkudata(L, 1, \"mcp.request\");\n    lua_pushinteger(L, rq->pr.ntokens);\n    return 1;\n}\n\nint mcplib_request_command(lua_State *L) {\n    mcp_request_t *rq = luaL_checkudata(L, -1, \"mcp.request\");\n    lua_pushinteger(L, rq->pr.command);\n    return 1;\n}\n\nint mcplib_request_has_flag(lua_State *L) {\n    mcp_request_t *rq = luaL_checkudata(L, 1, \"mcp.request\");\n    size_t len = 0;\n    const char *flagstr = luaL_checklstring(L, 2, &len);\n    if (len != 1) {\n        proxy_lua_error(L, \"has_flag(): meta flag must be a single character\");\n        return 0;\n    }\n    if (flagstr[0] < 65 || flagstr[0] > 122) {\n        proxy_lua_error(L, \"has_flag(): invalid flag, must be A-Z,a-z\");\n        return 0;\n    }\n    uint64_t flagbit = (uint64_t)1 << (flagstr[0] - 65);\n    if (rq->pr.t.meta.flags & flagbit) {\n        lua_pushboolean(L, 1);\n    } else {\n        lua_pushboolean(L, 0);\n    }\n\n    return 1;\n}\n\n// req:flag_token(\"F\") -> (bool, nil|token)\n// req:flag_token(\"O\", \"Onewopauqe\") -> (bool, oldtoken)\nint mcplib_request_flag_token(lua_State *L) {\n    mcp_request_t *rq = luaL_checkudata(L, 1, \"mcp.request\");\n    size_t len = 0;\n    const char *flagstr = luaL_checklstring(L, 2, &len);\n    bool replace = false;\n    if (len != 1) {\n        proxy_lua_error(L, \"has_flag(): meta flag must be a single character\");\n        return 0;\n    }\n    if (flagstr[0] < 65 || flagstr[0] > 122) {\n        proxy_lua_error(L, \"has_flag(): invalid flag, must be A-Z,a-z\");\n        return 0;\n    }\n    if (lua_isstring(L, 3)) {\n        // overwriting a flag/token with the third argument.\n        replace = true;\n    }\n    uint64_t flagbit = (uint64_t)1 << (flagstr[0] - 65);\n\n    int ret = 1;\n    if (rq->pr.t.meta.flags & flagbit) {\n        // The flag definitely exists, but sadly we need to scan for the\n        // actual flag to see if it has a token.\n        lua_pushboolean(L, 1);\n        for (int x = rq->pr.keytoken+1; x < rq->pr.ntokens; x++) {\n            const char *s = rq->pr.request + rq->pr.tokens[x];\n            if (s[0] == flagstr[0]) {\n                size_t vlen = _process_token_len(&rq->pr, x);\n                if (vlen > 1) {\n                    // strip the flag off the token and return.\n                    lua_pushlstring(L, s+1, vlen-1);\n                    ret = 2;\n                }\n\n                // Have something to replace the flag/token with.\n                if (replace) {\n                    size_t newlen = 0;\n                    const char *newtok = lua_tolstring(L, 3, &newlen);\n                    if (mcp_request_render(rq, x, 0, newtok, newlen) != 0) {\n                        proxy_lua_error(L, \"token(): request malformed after edit\");\n                        return 0;\n                    }\n                }\n                break;\n            }\n        }\n    } else {\n        lua_pushboolean(L, 0);\n    }\n\n    return ret;\n}\n\n// returns bool, int\n// bool results if flag exists or not\n// if int conversion fails, int is nil\nint mcplib_request_flag_token_int(lua_State *L) {\n    mcp_request_t *rq = luaL_checkudata(L, 1, \"mcp.request\");\n    size_t len = 0;\n    const char *flagstr = luaL_checklstring(L, 2, &len);\n    if (len != 1) {\n        proxy_lua_error(L, \"has_flag(): meta flag must be a single character\");\n        return 0;\n    }\n    if (flagstr[0] < 65 || flagstr[0] > 122) {\n        proxy_lua_error(L, \"has_flag(): invalid flag, must be A-Z,a-z\");\n        return 0;\n    }\n\n    uint64_t flagbit = (uint64_t)1 << (flagstr[0] - 65);\n\n    int ret = 1;\n    if (rq->pr.t.meta.flags & flagbit) {\n        lua_pushboolean(L, 1);\n        for (int x = rq->pr.keytoken+1; x < rq->pr.ntokens; x++) {\n            const char *s = rq->pr.request + rq->pr.tokens[x];\n            if (s[0] == flagstr[0]) {\n                size_t vlen = _process_token_len(&rq->pr, x);\n                if (vlen > 1) {\n                    // do a funny dance to safely strtol the token.\n                    // TODO: use tokenizer based tokto when merged.\n                    char temp[22];\n                    int tocopy = vlen > 22 ? 21 : vlen-1;\n                    memcpy(temp, s+1, tocopy);\n                    temp[vlen-1] = '\\0';\n                    int64_t token = 0;\n                    if (safe_strtoll(temp, &token)) {\n                        lua_pushinteger(L, token);\n                    } else {\n                        lua_pushnil(L);\n                    }\n                    ret = 2;\n                }\n\n                break;\n            }\n        }\n    } else {\n        lua_pushboolean(L, 0);\n    }\n\n    return ret;\n}\n\n// these functions take token as string or number\n// if number, internally convert it to avoid creating garbage\nstatic inline char _mcp_request_get_arg_flag(lua_State *L, int idx) {\n    size_t len = 0;\n    const char *flagstr = luaL_checklstring(L, idx, &len);\n\n    if (len != 1) {\n        proxy_lua_error(L, \"request: meta flag must be a single character\");\n        return 0;\n    }\n    if (flagstr[0] < 65 || flagstr[0] > 122) {\n        proxy_lua_error(L, \"request: invalid flag, must be A-Z,a-z\");\n        return 0;\n    }\n\n    return flagstr[0];\n}\n\n// *tostring must be large enough to hold a 64bit number as a string.\nstatic inline const char * _mcp_request_check_flag_token(lua_State *L, int idx, char *tostring, size_t *tlen) {\n    const char *token = NULL;\n    *tlen = 0;\n    if (lua_isstring(L, idx)) {\n        token = lua_tolstring(L, idx, tlen);\n    } else if (lua_isnumber(L, idx)) {\n        int isnum = 0;\n        lua_Integer n = lua_tointegerx(L, idx, &isnum);\n        if (isnum) {\n            char *end = itoa_64(n, tostring);\n            token = tostring;\n            *tlen = end - tostring;\n        } else {\n            proxy_lua_error(L, \"request: invalid flag argument\");\n            return NULL;\n        }\n    } else if (lua_isnoneornil(L, idx)) {\n        // no token, just add the flag.\n    } else {\n        proxy_lua_error(L, \"request: invalid flag argument\");\n        return NULL;\n    }\n\n    return token;\n}\n\n// req:flag_add(\"F\", token) -> (bool)\n// if token is \"example\", appends \"Fexample\" to request\nint mcplib_request_flag_add(lua_State *L) {\n    mcp_request_t *rq = luaL_checkudata(L, 1, \"mcp.request\");\n    char flag = _mcp_request_get_arg_flag(L, 2);\n    char tostring[30];\n\n    uint64_t flagbit = (uint64_t)1 << (flag - 65);\n    if (rq->pr.t.meta.flags & flagbit) {\n        // fail, flag already exists.\n        lua_pushboolean(L, 0);\n        return 1;\n    }\n\n    size_t tlen = 0;\n    const char *token = _mcp_request_check_flag_token(L, 3, tostring, &tlen);\n\n    if (mcp_request_append(rq, flag, token, tlen) == 0) {\n        lua_pushboolean(L, 1);\n    } else {\n        lua_pushboolean(L, 0);\n    }\n\n    return 1;\n}\n\n// req:flag_set(\"F\", token) -> (bool) [overwrites if exists]\n// if token is \"example\", appends \"Fexample\" to request\nint mcplib_request_flag_set(lua_State *L) {\n    mcp_request_t *rq = luaL_checkudata(L, 1, \"mcp.request\");\n    char flag = _mcp_request_get_arg_flag(L, 2);\n    char tostring[30];\n\n    int x = mcp_request_find_flag_index(rq, flag);\n    size_t tlen = 0;\n    const char *token = _mcp_request_check_flag_token(L, 3, tostring, &tlen);\n\n    if (x > 0) {\n        // TODO: do nothing if:\n        // flag exists in request, without token, and we're not setting a\n        // token.\n        if (mcp_request_render(rq, x, flag, token, tlen) != 0) {\n            lua_pushboolean(L, 0);\n            return 1;\n        }\n    } else {\n        if (mcp_request_append(rq, flag, token, tlen) != 0) {\n            lua_pushboolean(L, 0);\n            return 1;\n        }\n    }\n\n    lua_pushboolean(L, 1);\n    return 1;\n}\n\n// allows replacing a flag with a different flag\n// req:flag_replace(\"F\", \"N\", token) -> (bool)\n// if token is \"example\", appends \"Nexample\" to request\nint mcplib_request_flag_replace(lua_State *L) {\n    mcp_request_t *rq = luaL_checkudata(L, 1, \"mcp.request\");\n    char flag = _mcp_request_get_arg_flag(L, 2);\n    char newflag = _mcp_request_get_arg_flag(L, 3);\n    char tostring[30];\n\n    int x = mcp_request_find_flag_index(rq, flag);\n    size_t tlen = 0;\n    const char *token = _mcp_request_check_flag_token(L, 4, tostring, &tlen);\n\n    if (x > 0) {\n        if (mcp_request_render(rq, x, newflag, token, tlen) != 0) {\n            lua_pushboolean(L, 0);\n            return 1;\n        }\n    } else {\n        if (mcp_request_append(rq, newflag, token, tlen) != 0) {\n            lua_pushboolean(L, 0);\n            return 1;\n        }\n    }\n\n    lua_pushboolean(L, 1);\n    return 1;\n}\n\n// req:flag_del(\"F\") -> (bool)\n// remove a flag if exists\nint mcplib_request_flag_del(lua_State *L) {\n    mcp_request_t *rq = luaL_checkudata(L, 1, \"mcp.request\");\n    char flag = _mcp_request_get_arg_flag(L, 2);\n\n    int x = mcp_request_find_flag_index(rq, flag);\n\n    if (x > 0) {\n        if (mcp_request_render(rq, x, 0, NULL, 0) != 0) {\n            lua_pushboolean(L, 0);\n            return 1;\n        }\n    } else {\n        // nothing there, didn't delete anything.\n        lua_pushboolean(L, 0);\n        return 1;\n    }\n\n    lua_pushboolean(L, 1);\n    return 1;\n}\n\n// local match, token = req:match_res(res)\n// checks if req has `k` or `O`. If so, checks response for `K` or `O`\n// returns true, nil if matches\n// returns false, res token if not match.\n//\nint mcplib_request_match_res(lua_State *L) {\n    mcp_request_t *rq = luaL_checkudata(L, 1, \"mcp.request\");\n    mcp_resp_t *rs = luaL_checkudata(L, 2, \"mcp.response\");\n\n    const char *opaque_token = NULL;\n    size_t opaque_len = 0;\n    mcmc_resp_t reresp;\n\n    // requests all have keys. check for an opaque.\n    mcp_request_find_flag_token(rq, 'O', &opaque_token, &opaque_len);\n    mcmc_parse_buf(rs->buf, rs->blen, &reresp);\n\n    // scan the response line for tokens, since we don't have a reciprocal API\n    // yet. When we do this code will be replaced with a function call like\n    // the above.\n    const char *p = reresp.rline;\n    const char *e = p + reresp.rlen;\n    if (!p) {\n        // happens if the result line is blank (ie; 'HD\\r\\n')\n        lua_pushboolean(L, 0);\n        lua_pushnil(L);\n        return 2;\n    }\n\n    int matched = 0;\n    while (p != e) {\n        if (*p == ' ') {\n            p++;\n        } else if (*p == 'k' || *p == 'O') {\n            const char *rq_token = NULL;\n            int rq_len = 0;\n            if (*p == 'k') {\n                rq_token = MCP_PARSER_KEY(rq->pr);\n                rq_len = rq->pr.klen;\n            } else if (*p == 'O') {\n                rq_token = opaque_token;\n                rq_len = opaque_len;\n            }\n            if (rq_token == NULL) {\n                lua_pushboolean(L, 0);\n                lua_pushnil(L);\n                return 2;\n            }\n\n            p++; // skip flag and start comparing token\n            const char *rs_token = p;\n\n            // find end of token\n            while (p != e && !isspace(*p)) {\n                p++;\n            }\n\n            int rs_len = p - rs_token;\n            if (rq_len != rs_len || memcmp(rq_token, rs_token, rs_len) != 0) {\n                // FAIL, keys aren't the same length or don't match.\n                lua_pushboolean(L, 0);\n                lua_pushlstring(L, rs_token, rs_len);\n                return 2;\n            } else {\n                matched = 1;\n            }\n        } else {\n            // skip token\n            while (p != e && *p != ' ') {\n                p++;\n            }\n        }\n    }\n\n    lua_pushboolean(L, matched);\n    lua_pushnil(L);\n    return 2;\n}\n\nvoid mcp_request_cleanup(LIBEVENT_THREAD *t, mcp_request_t *rq) {\n    // During nread c->item is the malloc'ed buffer. not yet put into\n    // rq->buf - this gets freed because we've also set c->item_malloced if\n    // the connection closes before finishing nread.\n    if (rq->pr.vbuf != NULL) {\n        pthread_mutex_lock(&t->proxy_limit_lock);\n        t->proxy_buffer_memory_used -= rq->pr.vlen;\n        pthread_mutex_unlock(&t->proxy_limit_lock);\n        free(rq->pr.vbuf);\n        // need to ensure we NULL this out now, since we can call the cleanup\n        // routine independent of GC, and a later GC would double-free.\n        rq->pr.vbuf = NULL;\n    }\n}\n\nint mcplib_request_gc(lua_State *L) {\n    LIBEVENT_THREAD *t = PROXY_GET_THR(L);\n    mcp_request_t *rq = luaL_checkudata(L, -1, \"mcp.request\");\n    mcp_request_cleanup(t, rq);\n\n    return 0;\n}\n\nstatic int _mcp_request_find_flag(mcp_request_t *rq, const char flag) {\n    uint64_t flagbit = (uint64_t)1 << (flag - 65);\n    if (rq->pr.t.meta.flags & flagbit) {\n        for (int x = rq->pr.keytoken+1; x < rq->pr.ntokens; x++) {\n            const char *s = rq->pr.request + rq->pr.tokens[x];\n            if (s[0] == flag) {\n                return x;\n            }\n        }\n    }\n    return -1;\n}\n\nint mcp_request_find_flag_index(mcp_request_t *rq, const char flag) {\n    int x = _mcp_request_find_flag(rq, flag);\n    return x;\n}\n\nint mcp_request_find_flag_token(mcp_request_t *rq, const char flag, const char **token, size_t *len) {\n    int x = _mcp_request_find_flag(rq, flag);\n    if (x > 0) {\n        size_t tlen = _process_token_len(&rq->pr, x);\n        if (tlen > 1) {\n            *token = rq->pr.request + rq->pr.tokens[x] +1;\n        } else {\n            *token = NULL;\n        }\n        *len = tlen-1;\n    }\n    return x;\n}\n\n// FIXME: temporary copypasta accessor until request objects can be moved to\n// mcmc tokenizer.\nint mcp_request_find_flag_tokenint64(mcp_request_t *rq, const char flag, int64_t *token) {\n    for (int x = rq->pr.keytoken+1; x < rq->pr.ntokens; x++) {\n        const char *s = rq->pr.request + rq->pr.tokens[x];\n        if (s[0] == flag) {\n            size_t vlen = _process_token_len(&rq->pr, x);\n            if (vlen > 1) {\n                // do a funny dance to safely strtol the token.\n                char temp[22];\n                int tocopy = vlen > 22 ? 21 : vlen-1;\n                memcpy(temp, s+1, tocopy);\n                temp[vlen-1] = '\\0';\n                if (safe_strtoll(temp, token)) {\n                    return 0;\n                } else {\n                    return -1;\n                }\n            }\n\n            break;\n        }\n    }\n\n    return -1;\n}\n\n// TODO (v2): check what lua does when it calls a function with a string argument\n// stored from a table/similar (ie; the prefix check code).\n// If it's not copying anything, we can add request-side functions to do most\n// forms of matching and avoid copying the key to lua space.\n"
        },
        {
          "name": "proxy_result.c",
          "type": "blob",
          "size": 4.4873046875,
          "content": "/* -*- Mode: C; tab-width: 4; c-basic-offset: 4; indent-tabs-mode: nil -*- */\n\n#include \"proxy.h\"\n\nint mcplib_response_elapsed(lua_State *L) {\n    mcp_resp_t *r = luaL_checkudata(L, -1, \"mcp.response\");\n    lua_pushinteger(L, r->elapsed);\n    return 1;\n}\n\n// resp:ok()\nint mcplib_response_ok(lua_State *L) {\n    mcp_resp_t *r = luaL_checkudata(L, -1, \"mcp.response\");\n\n    if (r->status == MCMC_OK) {\n        lua_pushboolean(L, 1);\n    } else {\n        lua_pushboolean(L, 0);\n    }\n\n    return 1;\n}\n\nint mcplib_response_hit(lua_State *L) {\n    mcp_resp_t *r = luaL_checkudata(L, -1, \"mcp.response\");\n\n    if (r->status == MCMC_OK && r->resp.code != MCMC_CODE_END) {\n        lua_pushboolean(L, 1);\n    } else {\n        lua_pushboolean(L, 0);\n    }\n\n    return 1;\n}\n\n// Caller needs to discern if a vlen is 0 because of a failed response or an\n// OK response that was actually zero. So we always return an integer value\n// here.\nint mcplib_response_vlen(lua_State *L) {\n    mcp_resp_t *r = luaL_checkudata(L, -1, \"mcp.response\");\n\n    // We do remove the \"\\r\\n\" from the value length, so if you're actually\n    // processing the value nothing breaks.\n    if (r->resp.vlen >= 2) {\n        lua_pushinteger(L, r->resp.vlen-2);\n    } else {\n        lua_pushinteger(L, 0);\n    }\n\n    return 1;\n}\n\n// Refer to MCMC_CODE_* defines.\nint mcplib_response_code(lua_State *L) {\n    mcp_resp_t *r = luaL_checkudata(L, -1, \"mcp.response\");\n\n    lua_pushinteger(L, r->resp.code);\n\n    return 1;\n}\n\n// Get the unparsed response line for handling in lua.\nint mcplib_response_line(lua_State *L) {\n    mcp_resp_t *r = luaL_checkudata(L, -1, \"mcp.response\");\n\n    if (r->resp.rline != NULL) {\n        lua_pushlstring(L, r->resp.rline, r->resp.rlen);\n    } else {\n        lua_pushnil(L);\n    }\n\n    return 1;\n}\n\nint mcplib_response_flag_blank(lua_State *L) {\n    mcp_resp_t *r = luaL_checkudata(L, 1, \"mcp.response\");\n    mcmc_resp_t reresp;\n    size_t len = 0;\n    const char *flagstr = luaL_checklstring(L, 2, &len);\n\n    if (len != 1) {\n        proxy_lua_error(L, \"request: meta flag must be a single character\");\n        return 0;\n    }\n    if (flagstr[0] < 65 || flagstr[0] > 122) {\n        proxy_lua_error(L, \"request: invalid flag, must be A-Z,a-z\");\n        return 0;\n    }\n\n    mcmc_parse_buf(r->buf, r->blen, &reresp);\n\n    if (reresp.type == MCMC_RESP_META) {\n        // r->resp.rline is the start of the meta line... rlen is the length.\n        // we cast away the const and do evil here.\n        char *pos = (char *) reresp.rline;\n        size_t rlen = reresp.rlen;\n\n        char *end = pos + rlen;\n        char flag = flagstr[0];\n\n        while (pos != end) {\n            // either flag is at the start of the line or it has a space\n            // immediately before it.\n            if (*pos == flag && (pos == reresp.rline || *(pos-1) == ' ')) {\n                while (pos != end && !isspace(*pos)) {\n                    *pos = ' ';\n                    pos++;\n                }\n                lua_pushboolean(L, 1); // found and blanked.\n                return 1;\n            } else {\n                pos++;\n            }\n        }\n\n        // TODO: blank out r->tok?\n    }\n    lua_pushboolean(L, 0); // not found or not done.\n    return 1;\n}\n\nvoid mcp_response_cleanup(LIBEVENT_THREAD *t, mcp_resp_t *r) {\n    // On error/similar we might be holding the read buffer.\n    // If the buf is handed off to mc_resp for return, this pointer is NULL\n    if (r->buf != NULL) {\n        pthread_mutex_lock(&t->proxy_limit_lock);\n        t->proxy_buffer_memory_used -= r->blen + r->extra;\n        pthread_mutex_unlock(&t->proxy_limit_lock);\n\n        free(r->buf);\n        r->buf = NULL;\n    }\n    r->tok.ntokens = 0;\n\n    // release our temporary mc_resp sub-object.\n    if (r->cresp != NULL) {\n        mc_resp *cresp = r->cresp;\n        assert(r->thread != NULL);\n        if (cresp->item) {\n            item_remove(cresp->item);\n            cresp->item = NULL;\n        }\n        resp_free(r->thread, cresp);\n        r->cresp = NULL;\n    }\n}\n\nint mcplib_response_gc(lua_State *L) {\n    LIBEVENT_THREAD *t = PROXY_GET_THR(L);\n    mcp_resp_t *r = luaL_checkudata(L, -1, \"mcp.response\");\n    mcp_response_cleanup(t, r);\n\n    return 0;\n}\n\n// Note that this can be called multiple times for a single object, as opposed\n// to _gc. The cleanup routine is armored against repeat accesses by NULL'ing\n// th efields it checks.\nint mcplib_response_close(lua_State *L) {\n    LIBEVENT_THREAD *t = PROXY_GET_THR(L);\n    mcp_resp_t *r = luaL_checkudata(L, 1, \"mcp.response\");\n    mcp_response_cleanup(t, r);\n\n    return 0;\n}\n\n\n"
        },
        {
          "name": "proxy_ring_hash.c",
          "type": "blob",
          "size": 12.7509765625,
          "content": "/* -*- Mode: C; tab-width: 4; c-basic-offset: 4; indent-tabs-mode: nil -*- */\n/*\n* Copyright (c) 2022, Cache Forge LLC, All rights reserved.\n* Alan Kasindorf <alan@cacheforge.com>\n* Copyright (c) 2007, Last.fm, All rights reserved.\n* Richard Jones <rj@last.fm>\n* Christian Muehlhaeuser <muesli@gmail.com>\n*\n* Redistribution and use in source and binary forms, with or without\n* modification, are permitted provided that the following conditions are met:\n*     * Redistributions of source code must retain the above copyright\n*       notice, this list of conditions and the following disclaimer.\n*     * Redistributions in binary form must reproduce the above copyright\n*       notice, this list of conditions and the following disclaimer in the\n*       documentation and/or other materials provided with the distribution.\n*     * Neither the name of the Last.fm Limited nor the\n*       names of its contributors may be used to endorse or promote products\n*       derived from this software without specific prior written permission.\n*\n* THIS SOFTWARE IS PROVIDED BY Last.fm ``AS IS'' AND ANY\n* EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n* WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n* DISCLAIMED. IN NO EVENT SHALL Last.fm BE LIABLE FOR ANY\n* DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n* (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n* LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n* ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n* (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n* SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n*/\n\n#include \"proxy.h\"\n#include \"md5.h\"\n\n#define DEFAULT_BUCKET_SIZE 160\n\ntypedef struct {\n    unsigned int point; // continuum point.\n    unsigned int id; // server id.\n} cpoint;\n\ntypedef struct {\n    struct proxy_hash_caller phc; // passed back to the proxy API.\n    unsigned int total_buckets;\n    cpoint continuum[]; // points to server ids.\n} ketama_t;\n\nstatic uint64_t ketama_key_hasher(const void *key, size_t len, uint64_t seed);\nstatic struct proxy_hash_func ketama_key_hash = {\n    ketama_key_hasher,\n};\n\n/* FROM ketama.c */\nstatic void ketama_md5_digest( char* inString, unsigned char md5pword[16] )\n{\n    md5_state_t md5state;\n\n    md5_init( &md5state );\n    md5_append( &md5state, (unsigned char *)inString, strlen( inString ) );\n    md5_finish( &md5state, md5pword );\n}\n\nstatic int ketama_compare(const void *p1, const void *p2) {\n    const cpoint *a = p1;\n    const cpoint *b = p2;\n\n    return (a->point < b->point) ? -1 : ((a->point > b->point) ? 1 : 0);\n}\n\nstatic uint64_t ketama_key_hasher(const void *key, size_t len, uint64_t seed) {\n    // NOTE: seed is ignored!\n    // embedding the md5 bits since key is specified with a length here.\n    md5_state_t md5state;\n    unsigned char digest[16];\n\n    md5_init(&md5state);\n    md5_append(&md5state, (unsigned char *)key, len);\n    md5_finish(&md5state, digest);\n\n    // mix the hash down (from ketama_hashi)\n    unsigned int h = (unsigned int)(( digest[3] << 24 )\n                        | ( digest[2] << 16 )\n                        | ( digest[1] <<  8 )\n                        |   digest[0] );\n    return h;\n}\n\n// Note: must return lookupas as zero-indexed.\nstatic uint32_t ketama_get_server(uint64_t hash, void *ctx) {\n    ketama_t *kt = (ketama_t *)ctx;\n    unsigned int h = hash;\n    int highp = kt->total_buckets;\n    int lowp = 0, midp;\n    unsigned int midval, midval1;\n\n    // divide and conquer array search to find server with next biggest\n    // point after what this key hashes to\n    while ( 1 )\n    {\n        midp = (int)( ( lowp+highp ) / 2 );\n\n        if ( midp == kt->total_buckets )\n            return kt->continuum[0].id-1; // if at the end, roll back to zeroth\n\n        midval = kt->continuum[midp].point;\n        midval1 = midp == 0 ? 0 : kt->continuum[midp-1].point;\n\n        if ( h <= midval && h > midval1 )\n            return kt->continuum[midp].id-1;\n\n        if ( midval < h )\n            lowp = midp + 1;\n        else\n            highp = midp - 1;\n\n        if ( lowp > highp )\n            return kt->continuum[0].id-1;\n    }\n}\n/* END FROM ketama.c */\n\n// not much to be done about this without making the interface unusable.\n#define MODE_DEFAULT 0 // uses xxhash\n#define MODE_KETAMA 1 // uses md5\n#define MODE_TWEMPROXY 2 // technically \"libmemcached\" ?\n#define MODE_EVCACHE 3 // not sure why this has a funny string init.\n\n// Not sure the hash algo used here matters all that much given the low number\n// of points... but it might be better to let it be overrideable.\nstatic void _add_server_default(ketama_t *kt, size_t hashstring_size, const char **parts,\n        lua_Integer bucket_size, lua_Integer id, unsigned int *cont) {\n    char *hashstring = malloc(hashstring_size);\n\n    for (int k = 0; k < bucket_size; k++) {\n        size_t len = snprintf(hashstring, hashstring_size, \"%s:%s-%d\", parts[0], parts[1], k);\n        kt->continuum[*cont].point = (unsigned int) XXH3_64bits(hashstring, len);\n        kt->continuum[*cont].id = id;\n    }\n\n    free(hashstring);\n}\n\nstatic void _add_server_ketama(ketama_t *kt, size_t hashstring_size, const char **parts,\n        lua_Integer bucket_size, lua_Integer id, unsigned int *cont) {\n    char *hashstring = malloc(hashstring_size);\n\n    for (int k = 0; k < bucket_size / 4; k++) {\n        unsigned char digest[16];\n\n        // - create hashing string for ketama\n        snprintf(hashstring, hashstring_size, \"%s:%s-%d\", parts[0], parts[1], k);\n\n        // - md5() hash it\n        // mostly from ketama.c\n        ketama_md5_digest(hashstring, digest);\n\n        /* Use successive 4-bytes from hash as numbers\n         * for the points on the circle: */\n        for(int h = 0; h < 4; h++ )\n        {\n            kt->continuum[*cont].point = ( digest[3+h*4] << 24 )\n                                  | ( digest[2+h*4] << 16 )\n                                  | ( digest[1+h*4] <<  8 )\n                                  |   digest[h*4];\n            kt->continuum[*cont].id = id;\n            (*cont)++;\n        }\n\n    }\n\n    free(hashstring);\n}\n\nstatic void _add_server_twemproxy(ketama_t *kt, size_t hashstring_size, const char **parts,\n        lua_Integer bucket_size, lua_Integer id, unsigned int *cont) {\n    char *hashstring = malloc(hashstring_size);\n\n    for (int k = 0; k < bucket_size / 4; k++) {\n        unsigned char digest[16];\n\n        // - create hashing string for ketama\n        if (strcmp(parts[1], \"11211\") == 0) {\n            // twemproxy sources libmemcached as removing the default port\n            // from the string if found.\n            snprintf(hashstring, hashstring_size, \"%s-%d\", parts[0], k);\n        } else {\n            snprintf(hashstring, hashstring_size, \"%s:%s-%d\", parts[0], parts[1], k);\n        }\n\n        // - md5() hash it\n        // mostly from ketama.c\n        ketama_md5_digest(hashstring, digest);\n\n        /* Use successive 4-bytes from hash as numbers\n         * for the points on the circle: */\n        for(int h = 0; h < 4; h++ )\n        {\n            kt->continuum[*cont].point = ( digest[3+h*4] << 24 )\n                                  | ( digest[2+h*4] << 16 )\n                                  | ( digest[1+h*4] <<  8 )\n                                  |   digest[h*4];\n            kt->continuum[*cont].id = id;\n            (*cont)++;\n        }\n\n    }\n\n    free(hashstring);\n}\n\nstatic void _add_server_evcache(ketama_t *kt, size_t hashstring_size, const char **parts,\n        lua_Integer bucket_size, lua_Integer id, unsigned int *cont) {\n    char *hashstring = malloc(hashstring_size);\n\n    for (int k = 0; k < bucket_size / 4; k++) {\n        unsigned char digest[16];\n\n        // - create hashing string for ketama\n        snprintf(hashstring, hashstring_size, \"%s/%s:%s-%d\", parts[0], parts[0], parts[1], k);\n        // - md5() hash it\n        // mostly from ketama.c\n        ketama_md5_digest(hashstring, digest);\n\n        /* Use successive 4-bytes from hash as numbers\n         * for the points on the circle: */\n        for(int h = 0; h < 4; h++ )\n        {\n            kt->continuum[*cont].point = ( digest[3+h*4] << 24 )\n                                  | ( digest[2+h*4] << 16 )\n                                  | ( digest[1+h*4] <<  8 )\n                                  |   digest[h*4];\n            kt->continuum[*cont].id = id;\n            (*cont)++;\n        }\n\n    }\n\n    free(hashstring);\n}\n\n#define PARTS 2\n// stack = [pool, option]\nstatic int ketama_new(lua_State *L) {\n    lua_Integer bucket_size = DEFAULT_BUCKET_SIZE;\n    const char *parts[PARTS];\n    size_t partlens[PARTS];\n    int makemode = 0;\n\n    // check for UA_TTABLE at 1\n    luaL_checktype(L, 1, LUA_TTABLE);\n    // get number of servers in pool.\n    // NOTE: rawlen skips metatable redirection. if we care; lua_len instead.\n    lua_Unsigned total = lua_rawlen(L, 1);\n\n    // check for optional input\n    int argc = lua_gettop(L);\n    if (argc > 1) {\n        luaL_checktype(L, 2, LUA_TTABLE);\n        if (lua_getfield(L, 2, \"omode\") != LUA_TNIL) {\n            luaL_checktype(L, -1, LUA_TSTRING);\n            const char *mode = lua_tostring(L, -1);\n            if (strcmp(mode, \"default\") == 0) {\n                makemode = MODE_DEFAULT;\n            } else if (strcmp(mode, \"ketama\") == 0) {\n                makemode = MODE_KETAMA;\n            } else if (strcmp(mode, \"twemproxy\") == 0) {\n                makemode = MODE_TWEMPROXY;\n            } else if (strcmp(mode, \"evcache\") == 0) {\n                makemode = MODE_EVCACHE;\n            } else {\n                lua_pushstring(L, \"ring_hash: bad omode argument\");\n                lua_error(L);\n            }\n        }\n        lua_pop(L, 1); // pops the nil or mode\n\n        if (lua_getfield(L, 2, \"obuckets\") != LUA_TNIL) {\n          int success = 0;\n          bucket_size = lua_tointegerx(L, -1, &success);\n          if (!success || bucket_size < 1) {\n              lua_pushstring(L, \"ring_hash: option argument must be a positive integer\");\n              lua_error(L);\n          }\n        }\n        lua_pop(L, 1);\n    }\n\n    // newuserdatauv() sized for pool*\n    size_t size = sizeof(ketama_t) + sizeof(cpoint) * (total * bucket_size);\n    ketama_t *kt = lua_newuserdatauv(L, size, 0);\n    // TODO: check *kt.\n    kt->total_buckets = bucket_size * total;\n\n    // loop over pool\n    unsigned int cont = 0;\n    lua_pushnil(L); // start the pool iterator\n    while (lua_next(L, 1) != 0) {\n        // key is -2, value is -1.\n        // value is another table. need to query it to get what we need for\n        // the hash.\n        // hash string is: hostname/ipaddr:port-repitition\n        // TODO: bother doing error checking?\n        lua_getfield(L, -1, \"id\");\n        lua_Integer id = lua_tointeger(L, -1);\n        lua_pop(L, 1);\n\n        // FIXME: we need to do the lua_pop after string assembly to be safe.\n        lua_getfield(L, -1, \"addr\");\n        parts[0] = lua_tolstring(L, -1, &partlens[0]);\n        lua_pop(L, 1);\n        lua_getfield(L, -1, \"port\");\n        parts[1] = lua_tolstring(L, -1, &partlens[1]);\n        lua_pop(L, 1);\n\n        size_t hashstring_size = 0;\n        for (int x = 0; x < PARTS; x++) {\n            hashstring_size += partlens[x];\n        }\n\n        // We have up to 3 delimiters in the final hashstring and an index\n        // 16 bytes is plenty to accomodate this requirement.\n        hashstring_size += 16;\n\n        switch (makemode) {\n            case MODE_DEFAULT:\n                _add_server_default(kt, hashstring_size, parts, bucket_size, id, &cont);\n                break;\n            case MODE_KETAMA:\n                _add_server_ketama(kt, hashstring_size, parts, bucket_size, id, &cont);\n                break;\n            case MODE_TWEMPROXY:\n                _add_server_twemproxy(kt, hashstring_size, parts, bucket_size, id, &cont);\n                break;\n            case MODE_EVCACHE:\n                // EVCache uses the ipaddress couple of times, we need to factor that in\n                // when calculating the hashstring_size\n                hashstring_size += partlens[0];\n                _add_server_evcache(kt, hashstring_size, parts, bucket_size, id, &cont);\n                break;\n        }\n\n        lua_pop(L, 1); // remove value, leave key for next iteration.\n    }\n\n    // - qsort the points\n    qsort( &kt->continuum, cont, sizeof(cpoint), ketama_compare);\n\n    // set the hash/fetch function and the context ptr.\n    kt->phc.ctx = kt;\n    kt->phc.selector_func = ketama_get_server;\n\n    // - add a pushlightuserdata for the sub-struct with func/ctx.\n    lua_pushlightuserdata(L, &kt->phc);\n    // - return [UD, lightuserdata]\n    return 2;\n}\n\nint mcplib_open_dist_ring_hash(lua_State *L) {\n    const struct luaL_Reg ketama_f[] = {\n        {\"new\", ketama_new},\n        {NULL, NULL},\n    };\n\n    luaL_newlib(L, ketama_f);\n    lua_pushlightuserdata(L, &ketama_key_hash);\n    lua_setfield(L, -2, \"hash\");\n\n    return 1;\n}\n"
        },
        {
          "name": "proxy_tls.c",
          "type": "blob",
          "size": 7.6279296875,
          "content": "/* -*- Mode: C; tab-width: 4; c-basic-offset: 4; indent-tabs-mode: nil -*- */\n\n#include \"proxy.h\"\n#include \"proxy_tls.h\"\n#ifdef PROXY_TLS\n#include <openssl/ssl.h>\n#include <openssl/err.h>\n\n/* Notes on ERR_clear_error() and friends:\n * - Errors from SSL calls leave errors on a thread-local \"error stack\"\n * - If an error is received from an SSL call, the stack needs to be inspected\n *   and cleared.\n * - The error stack _must_ be clear before any SSL_get_error() calls, as it\n *   may return garbage.\n * - There may be _multiple_ errors queued after one SSL call, so just\n *   checking the top level does not clear it.\n * - ERR_clear_error() is not \"free\", so we would prefer to avoid calling it\n *   before hotpath calls. Thus, we should ensure it's called _after_ any\n *   hotpath call that receives any kind of error.\n * - We should also call it _before_ any non-hotpath SSL calls (such as\n *   SSL_connect()) for defense against bugs in our code or OpenSSL.\n */\n\nint mcp_tls_init(proxy_ctx_t *ctx) {\n    if (ctx->tls_ctx) {\n        return MCP_TLS_OK;\n    }\n\n    // TODO: check for OpenSSL 1.1+ ? should be elsewhere in the code.\n    SSL_CTX *tctx = SSL_CTX_new(TLS_client_method());\n    if (tctx == NULL) {\n        return MCP_TLS_ERR;\n    }\n\n    // TODO: make configurable like main cache server\n    SSL_CTX_set_min_proto_version(tctx, TLS1_3_VERSION);\n    // reduce memory consumption of idle backends.\n    SSL_CTX_set_mode(tctx, SSL_MODE_RELEASE_BUFFERS);\n\n    ctx->tls_ctx = tctx;\n    return 0;\n}\n\nint mcp_tls_backend_init(proxy_ctx_t *ctx, struct mcp_backendconn_s *be) {\n    if (!be->be_parent->tunables.use_tls) {\n        return MCP_TLS_OK;\n    }\n\n    SSL *ssl = SSL_new(ctx->tls_ctx);\n    if (ssl == NULL) {\n        return MCP_TLS_ERR;\n    }\n\n    be->ssl = ssl;\n    // SSL_set_fd() will free a pre-existing BIO and allocate a new one\n    // so we set any file descriptor at connect time instead.\n\n    return MCP_TLS_OK;\n}\n\nint mcp_tls_shutdown(struct mcp_backendconn_s *be) {\n    if (!be->ssl) {\n        return MCP_TLS_OK;\n    }\n\n    // TODO: This may need to be called multiple times to \"properly\" shutdown\n    // a session. However we only ever call this when a backend is dead or not\n    // in used anymore. Unclear if checking for WANT_READ|WRITE is worth\n    // doing.\n    SSL_shutdown(be->ssl);\n\n    return MCP_TLS_OK;\n}\n\nint mcp_tls_cleanup(struct mcp_backendconn_s *be) {\n    if (!be->ssl) {\n        return MCP_TLS_OK;\n    }\n\n    SSL_free(be->ssl);\n    be->ssl = NULL;\n    return MCP_TLS_OK;\n}\n\n// Contrary to the name of this function, the underlying tcp socket must\n// already be connected.\nint mcp_tls_connect(struct mcp_backendconn_s *be) {\n    // TODO: check return code. can fail if BIO fails to alloc.\n    SSL_set_fd(be->ssl, mcmc_fd(be->client));\n\n    // TODO:\n    // if the backend is changing TLS version or some similar issue, we will\n    // be unable to reconnect as the SSL object \"Caches\" some information\n    // about the previous request (why doesn't clear work then???)\n    // This will normally be fine, but we should detect severe errors here and\n    // decide if we should free and re-alloc the SSL object.\n    // Allocating the SSL object can be pretty slow, so we should at least\n    // attempt to do not do this.\n    // Related: https://github.com/openssl/openssl/issues/20286\n    SSL_clear(be->ssl);\n    ERR_clear_error();\n    int n = SSL_connect(be->ssl);\n    int ret = MCP_TLS_OK;\n    // TODO: complete error handling.\n    if (n == 1) {\n        // Successfully established and handshake complete.\n        return ret;\n    }\n\n    int err = SSL_get_error(be->ssl, n);\n    if (n == 0) {\n        // Not successsful, but shut down normally.\n        ERR_clear_error();\n        ret = MCP_TLS_ERR;\n    } else if (n < 0) {\n        // Not successful. Check for temporary error.\n        if (err == SSL_ERROR_WANT_READ ||\n            err == SSL_ERROR_WANT_WRITE) {\n            ret = MCP_TLS_OK;\n        } else {\n            ret = MCP_TLS_ERR;\n        }\n        ERR_clear_error();\n    }\n\n    return ret;\n}\n\nint mcp_tls_handshake(struct mcp_backendconn_s *be) {\n    if (SSL_is_init_finished(be->ssl)) {\n        return MCP_TLS_OK;\n    }\n\n    // Non hot path, so clear errors before running.\n    ERR_clear_error();\n    int n = SSL_do_handshake(be->ssl);\n    if (n == 1) {\n        return MCP_TLS_OK;\n    }\n\n    int err = SSL_get_error(be->ssl, n);\n    // TODO: realistically we'll only ever get WANT_READ here, since OpenSSL\n    // is handling the fd and it will have written a small number of bytes.\n    // leaving this note just in case.\n    if (err == SSL_ERROR_WANT_READ ||\n        err == SSL_ERROR_WANT_WRITE) {\n        // So far as I can tell there would be an error on the queue here.\n        ERR_clear_error();\n        return MCP_TLS_NEEDIO;\n    } else {\n        // TODO: can get the full error message and give to the caller to log\n        // to proxyevents?\n        ERR_clear_error();\n        return MCP_TLS_ERR;\n    }\n}\n\nint mcp_tls_send_validate(struct mcp_backendconn_s *be) {\n    const char *str = \"version\\r\\n\";\n    const size_t len = strlen(str);\n\n    // Non hot path, clear errors.\n    ERR_clear_error();\n    int n = SSL_write(be->ssl, str, len);\n\n    // TODO: more detailed error checking.\n    if (n < 0 || n != len) {\n        ERR_clear_error();\n        return MCP_TLS_ERR;\n    }\n\n    return MCP_TLS_OK;\n}\n\nint mcp_tls_read(struct mcp_backendconn_s *be) {\n    int n = SSL_read(be->ssl, be->rbuf + be->rbufused, READ_BUFFER_SIZE - be->rbufused);\n\n    if (n < 0) {\n        int err = SSL_get_error(be->ssl, n);\n        if (err == SSL_ERROR_WANT_WRITE ||\n            err == SSL_ERROR_WANT_READ) {\n            ERR_clear_error();\n            return MCP_TLS_NEEDIO;\n        } else {\n            // TODO: log detailed error.\n            ERR_clear_error();\n            return MCP_TLS_ERR;\n        }\n    } else {\n        be->rbufused += n;\n        return n;\n    }\n\n    return 0;\n}\n\n// TODO: option.\n#define TLS_WBUF_SIZE 16 * 1024\n\n// We cache the temporary write buffer on the be's event thread.\n// This is actually required when retrying ops (WANT_WRITE/etc) unless\n// MOVING_BUFFERS flag is set in OpenSSL.\nint mcp_tls_writev(struct mcp_backendconn_s *be, int iovcnt) {\n    proxy_event_thread_t *et = be->event_thread;\n    // TODO: move this to event thread init to remove branch and move error\n    // handling to startup time.\n    // Actually we won't know if TLS is in use until a backend shows up and\n    // tries to write... so I'm not sure where to move this. TLS compiled in\n    // but not used would waste memory.\n    // Maybe can at least mark it unlikely()?\n    if (et->tls_wbuf_size == 0) {\n        et->tls_wbuf_size = TLS_WBUF_SIZE;\n        et->tls_wbuf = malloc(et->tls_wbuf_size);\n        if (et->tls_wbuf == NULL) {\n            return MCP_TLS_ERR;\n        }\n    }\n    size_t remain = et->tls_wbuf_size;\n    char *b = et->tls_wbuf;\n\n    // OpenSSL has no writev or TCP_CORK equivalent, so we have to pre-mempcy\n    // the iov's here.\n    for (int i = 0; i < iovcnt; i++) {\n        size_t len = be->write_iovs[i].iov_len;\n        size_t to_copy = len < remain ? len : remain;\n\n        memcpy(b, (char *)be->write_iovs[i].iov_base, to_copy);\n        remain -= to_copy;\n        b += to_copy;\n        if (remain == 0)\n            break;\n    }\n\n    int n = SSL_write(be->ssl, et->tls_wbuf, b - et->tls_wbuf);\n    if (n < 0) {\n        int err = SSL_get_error(be->ssl, n);\n        if (err == SSL_ERROR_WANT_WRITE ||\n            err == SSL_ERROR_WANT_READ) {\n            ERR_clear_error();\n            return MCP_TLS_NEEDIO;\n        }\n        ERR_clear_error();\n        return MCP_TLS_ERR;\n    }\n\n    return n;\n}\n\n#else // PROXY_TLS\n\nint mcp_tls_writev(struct mcp_backendconn_s *be, int iovcnt) {\n    (void)be;\n    (void)iovcnt;\n    return 0;\n}\n\n#endif // PROXY_TLS\n"
        },
        {
          "name": "proxy_tls.h",
          "type": "blob",
          "size": 1.3564453125,
          "content": "#ifndef PROXY_TLS_H\n#define PROXY_TLS_H\n\n// Attempt to reduce ifdef soup within the larger code files by blanking out\n// or swapping these specialized functions.\n// I'm not being super smart about this: if usage of a function leads to a\n// compile error just adjust this as necessary. This is a bit less typing than\n// leaving the header empty and redefining everything in the .c file, but if\n// the balance changes we should switch to always doing that.\n\nenum mcp_tls_ret {\n    MCP_TLS_OK = 1,\n    MCP_TLS_NEEDIO = -1,\n    MCP_TLS_ERR = -2,\n};\n\n#ifdef PROXY_TLS\nint mcp_tls_init(proxy_ctx_t *ctx);\nint mcp_tls_backend_init(proxy_ctx_t *ctx, struct mcp_backendconn_s *be);\nint mcp_tls_shutdown(struct mcp_backendconn_s *be);\nint mcp_tls_cleanup(struct mcp_backendconn_s *be);\nint mcp_tls_connect(struct mcp_backendconn_s *be);\nint mcp_tls_handshake(struct mcp_backendconn_s *be);\nint mcp_tls_send_validate(struct mcp_backendconn_s *be);\nint mcp_tls_read(struct mcp_backendconn_s *be);\nint mcp_tls_writev(struct mcp_backendconn_s *be, int iovcnt);\n#else\n#define mcp_tls_init(ctx)\n#define mcp_tls_backend_init(ctx, be)\n#define mcp_tls_shutdown(be);\n#define mcp_tls_cleanup(be);\n#define mcp_tls_connect(be)\n#define mcp_tls_handshake(be) 0\n#define mcp_tls_send_validate(be) 0\n#define mcp_tls_read(be) 0\nint mcp_tls_writev(struct mcp_backendconn_s *be, int iovcnt);\n#endif // PROXY_TLS\n\n#endif\n"
        },
        {
          "name": "proxy_ustats.c",
          "type": "blob",
          "size": 3.1611328125,
          "content": "/* -*- Mode: C; tab-width: 4; c-basic-offset: 4; indent-tabs-mode: nil -*- */\n\n#include \"proxy.h\"\n\n// mcp.add_stat(index, name)\n// creates a custom lua stats counter\nint mcplib_add_stat(lua_State *L) {\n    int idx = luaL_checkinteger(L, -2);\n    const char *name = luaL_checkstring(L, -1);\n    proxy_ctx_t *ctx = PROXY_GET_CTX(L);\n\n    if (idx < 1) {\n        proxy_lua_error(L, \"stat index must be 1 or higher\");\n        return 0;\n    }\n    if (idx > ctx->tunables.max_ustats) {\n        proxy_lua_ferror(L, \"stat index must be %d or less\", ctx->tunables.max_ustats);\n        return 0;\n    }\n    // max name length? avoids errors if something huge gets thrown in.\n    if (strlen(name) > STAT_KEY_LEN - 6) {\n        // we prepend \"user_\" to the output. + null byte.\n        proxy_lua_ferror(L, \"stat name too long: %s\\n\", name);\n        return 0;\n    }\n    // restrict characters, at least no spaces/newlines.\n    for (int x = 0; x < strlen(name); x++) {\n        if (isspace(name[x])) {\n            proxy_lua_error(L, \"stat cannot contain spaces or newlines\");\n            return 0;\n        }\n    }\n\n    STAT_L(ctx);\n    int stats_num = ctx->user_stats_num;\n    struct proxy_user_stats_entry *entries = ctx->user_stats;\n\n    // if num_stats is 0 we need to init sizes.\n    // TODO (v2): malloc fail checking. (should be rare/impossible)\n    if (stats_num < idx) {\n        struct proxy_user_stats_entry *nentries = calloc(idx, sizeof(*entries));\n        // funny realloc; start with zeroed memory and copy in original.\n        if (entries) {\n            memcpy(nentries, entries, sizeof(*entries) * stats_num);\n            free(entries);\n        }\n        ctx->user_stats = nentries;\n        ctx->user_stats_num = idx;\n        entries = nentries;\n    }\n\n    idx--; // real slot start as 0.\n    if (entries[idx].name != NULL) {\n        // If name changed, we have to reset the counter in the slot.\n        // Also only free/strdup the string if it's changed.\n        if (strcmp(name, entries[idx].name) != 0) {\n            entries[idx].reset = true;\n            free(entries[idx].name);\n            entries[idx].name = strdup(name);\n        }\n        // else the stat name didn't change, so don't do anything.\n    } else if (entries[idx].cname) {\n        char *oldname = ctx->user_stats_namebuf + entries[idx].cname;\n        if (strcmp(name, oldname) != 0) {\n            entries[idx].reset = true;\n            entries[idx].name = strdup(name);\n        }\n    } else {\n        entries[idx].name = strdup(name);\n    }\n    STAT_UL(ctx);\n\n    return 0;\n}\n\nint mcplib_stat(lua_State *L) {\n    LIBEVENT_THREAD *t = PROXY_GET_THR(L);\n    if (t == NULL) {\n        proxy_lua_error(L, \"stat must be called from router handlers\");\n        return 0;\n    }\n\n    struct proxy_user_stats *tus = t->proxy_user_stats;\n    if (tus == NULL) {\n        proxy_lua_error(L, \"no stats counters initialized\");\n        return 0;\n    }\n\n    int idx = luaL_checkinteger(L, -2);\n    int change = luaL_checkinteger(L, -1);\n\n    if (idx < 1 || idx > tus->num_stats) {\n        proxy_lua_error(L, \"stat index out of range\");\n        return 0;\n    }\n\n    idx--; // actual array is 0 indexed.\n    WSTAT_L(t);\n    tus->counters[idx] += change;\n    WSTAT_UL(t);\n\n    return 0;\n}\n"
        },
        {
          "name": "proxy_xxhash.c",
          "type": "blob",
          "size": 0.2861328125,
          "content": "/* -*- Mode: C; tab-width: 4; c-basic-offset: 4; indent-tabs-mode: nil -*- */\n\n#include \"proxy.h\"\n\nstatic struct proxy_hash_func mcplib_hash_xxhash = {\n    XXH3_64bits_withSeed,\n};\n\nint mcplib_open_hash_xxhash(lua_State *L) {\n    lua_pushlightuserdata(L, &mcplib_hash_xxhash);\n    return 1;\n}\n"
        },
        {
          "name": "queue.h",
          "type": "blob",
          "size": 28.6982421875,
          "content": "// grabbed from https://raw.githubusercontent.com/freebsd/freebsd-src/master/sys/sys/queue.h\n// on jan 17th, 2021.\n/*-\n * SPDX-License-Identifier: BSD-3-Clause\n *\n * Copyright (c) 1991, 1993\n *\tThe Regents of the University of California.  All rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions\n * are met:\n * 1. Redistributions of source code must retain the above copyright\n *    notice, this list of conditions and the following disclaimer.\n * 2. Redistributions in binary form must reproduce the above copyright\n *    notice, this list of conditions and the following disclaimer in the\n *    documentation and/or other materials provided with the distribution.\n * 3. Neither the name of the University nor the names of its contributors\n *    may be used to endorse or promote products derived from this software\n *    without specific prior written permission.\n *\n * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND\n * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE\n * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS\n * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)\n * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT\n * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY\n * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF\n * SUCH DAMAGE.\n *\n *\t@(#)queue.h\t8.5 (Berkeley) 8/20/94\n * $FreeBSD$\n */\n\n#ifndef _SYS_QUEUE_H_\n#define\t_SYS_QUEUE_H_\n\n/*\n * This file defines four types of data structures: singly-linked lists,\n * singly-linked tail queues, lists and tail queues.\n *\n * A singly-linked list is headed by a single forward pointer. The elements\n * are singly linked for minimum space and pointer manipulation overhead at\n * the expense of O(n) removal for arbitrary elements. New elements can be\n * added to the list after an existing element or at the head of the list.\n * Elements being removed from the head of the list should use the explicit\n * macro for this purpose for optimum efficiency. A singly-linked list may\n * only be traversed in the forward direction.  Singly-linked lists are ideal\n * for applications with large datasets and few or no removals or for\n * implementing a LIFO queue.\n *\n * A singly-linked tail queue is headed by a pair of pointers, one to the\n * head of the list and the other to the tail of the list. The elements are\n * singly linked for minimum space and pointer manipulation overhead at the\n * expense of O(n) removal for arbitrary elements. New elements can be added\n * to the list after an existing element, at the head of the list, or at the\n * end of the list. Elements being removed from the head of the tail queue\n * should use the explicit macro for this purpose for optimum efficiency.\n * A singly-linked tail queue may only be traversed in the forward direction.\n * Singly-linked tail queues are ideal for applications with large datasets\n * and few or no removals or for implementing a FIFO queue.\n *\n * A list is headed by a single forward pointer (or an array of forward\n * pointers for a hash table header). The elements are doubly linked\n * so that an arbitrary element can be removed without a need to\n * traverse the list. New elements can be added to the list before\n * or after an existing element or at the head of the list. A list\n * may be traversed in either direction.\n *\n * A tail queue is headed by a pair of pointers, one to the head of the\n * list and the other to the tail of the list. The elements are doubly\n * linked so that an arbitrary element can be removed without a need to\n * traverse the list. New elements can be added to the list before or\n * after an existing element, at the head of the list, or at the end of\n * the list. A tail queue may be traversed in either direction.\n *\n * For details on the use of these macros, see the queue(3) manual page.\n *\n * Below is a summary of implemented functions where:\n *  +  means the macro is available\n *  -  means the macro is not available\n *  s  means the macro is available but is slow (runs in O(n) time)\n *\n *\t\t\t\tSLIST\tLIST\tSTAILQ\tTAILQ\n * _HEAD\t\t\t+\t+\t+\t+\n * _CLASS_HEAD\t\t\t+\t+\t+\t+\n * _HEAD_INITIALIZER\t\t+\t+\t+\t+\n * _ENTRY\t\t\t+\t+\t+\t+\n * _CLASS_ENTRY\t\t\t+\t+\t+\t+\n * _INIT\t\t\t+\t+\t+\t+\n * _EMPTY\t\t\t+\t+\t+\t+\n * _FIRST\t\t\t+\t+\t+\t+\n * _NEXT\t\t\t+\t+\t+\t+\n * _PREV\t\t\t-\t+\t-\t+\n * _LAST\t\t\t-\t-\t+\t+\n * _LAST_FAST\t\t\t-\t-\t-\t+\n * _FOREACH\t\t\t+\t+\t+\t+\n * _FOREACH_FROM\t\t+\t+\t+\t+\n * _FOREACH_SAFE\t\t+\t+\t+\t+\n * _FOREACH_FROM_SAFE\t\t+\t+\t+\t+\n * _FOREACH_REVERSE\t\t-\t-\t-\t+\n * _FOREACH_REVERSE_FROM\t-\t-\t-\t+\n * _FOREACH_REVERSE_SAFE\t-\t-\t-\t+\n * _FOREACH_REVERSE_FROM_SAFE\t-\t-\t-\t+\n * _INSERT_HEAD\t\t\t+\t+\t+\t+\n * _INSERT_BEFORE\t\t-\t+\t-\t+\n * _INSERT_AFTER\t\t+\t+\t+\t+\n * _INSERT_TAIL\t\t\t-\t-\t+\t+\n * _CONCAT\t\t\ts\ts\t+\t+\n * _REMOVE_AFTER\t\t+\t-\t+\t-\n * _REMOVE_HEAD\t\t\t+\t-\t+\t-\n * _REMOVE\t\t\ts\t+\ts\t+\n * _SWAP\t\t\t+\t+\t+\t+\n *\n */\n#ifdef QUEUE_MACRO_DEBUG\n#warn Use QUEUE_MACRO_DEBUG_TRACE and/or QUEUE_MACRO_DEBUG_TRASH\n#define\tQUEUE_MACRO_DEBUG_TRACE\n#define\tQUEUE_MACRO_DEBUG_TRASH\n#endif\n\n#ifdef QUEUE_MACRO_DEBUG_TRACE\n/* Store the last 2 places the queue element or head was altered */\nstruct qm_trace {\n\tunsigned long\t lastline;\n\tunsigned long\t prevline;\n\tconst char\t*lastfile;\n\tconst char\t*prevfile;\n};\n\n#define\tTRACEBUF\tstruct qm_trace trace;\n#define\tTRACEBUF_INITIALIZER\t{ __LINE__, 0, __FILE__, NULL } ,\n\n#define\tQMD_TRACE_HEAD(head) do {\t\t\t\t\t\\\n\t(head)->trace.prevline = (head)->trace.lastline;\t\t\\\n\t(head)->trace.prevfile = (head)->trace.lastfile;\t\t\\\n\t(head)->trace.lastline = __LINE__;\t\t\t\t\\\n\t(head)->trace.lastfile = __FILE__;\t\t\t\t\\\n} while (0)\n\n#define\tQMD_TRACE_ELEM(elem) do {\t\t\t\t\t\\\n\t(elem)->trace.prevline = (elem)->trace.lastline;\t\t\\\n\t(elem)->trace.prevfile = (elem)->trace.lastfile;\t\t\\\n\t(elem)->trace.lastline = __LINE__;\t\t\t\t\\\n\t(elem)->trace.lastfile = __FILE__;\t\t\t\t\\\n} while (0)\n\n#else\t/* !QUEUE_MACRO_DEBUG_TRACE */\n#define\tQMD_TRACE_ELEM(elem)\n#define\tQMD_TRACE_HEAD(head)\n#define\tTRACEBUF\n#define\tTRACEBUF_INITIALIZER\n#endif\t/* QUEUE_MACRO_DEBUG_TRACE */\n\n#ifdef QUEUE_MACRO_DEBUG_TRASH\n#define\tQMD_SAVELINK(name, link)\tvoid **name = (void *)&(link)\n#define\tTRASHIT(x)\t\tdo {(x) = (void *)-1;} while (0)\n#define\tQMD_IS_TRASHED(x)\t((x) == (void *)(intptr_t)-1)\n#else\t/* !QUEUE_MACRO_DEBUG_TRASH */\n#define\tQMD_SAVELINK(name, link)\n#define\tTRASHIT(x)\n#define\tQMD_IS_TRASHED(x)\t0\n#endif\t/* QUEUE_MACRO_DEBUG_TRASH */\n\n#ifdef __cplusplus\n/*\n * In C++ there can be structure lists and class lists:\n */\n#define\tQUEUE_TYPEOF(type) type\n#else\n#define\tQUEUE_TYPEOF(type) struct type\n#endif\n\n/*\n * Singly-linked List declarations.\n */\n#define\tSLIST_HEAD(name, type)\t\t\t\t\t\t\\\nstruct name {\t\t\t\t\t\t\t\t\\\n\tstruct type *slh_first;\t/* first element */\t\t\t\\\n}\n\n#define\tSLIST_CLASS_HEAD(name, type)\t\t\t\t\t\\\nstruct name {\t\t\t\t\t\t\t\t\\\n\tclass type *slh_first;\t/* first element */\t\t\t\\\n}\n\n#define\tSLIST_HEAD_INITIALIZER(head)\t\t\t\t\t\\\n\t{ NULL }\n\n#define\tSLIST_ENTRY(type)\t\t\t\t\t\t\\\nstruct {\t\t\t\t\t\t\t\t\\\n\tstruct type *sle_next;\t/* next element */\t\t\t\\\n}\n\n#define\tSLIST_CLASS_ENTRY(type)\t\t\t\t\t\t\\\nstruct {\t\t\t\t\t\t\t\t\\\n\tclass type *sle_next;\t\t/* next element */\t\t\\\n}\n\n/*\n * Singly-linked List functions.\n */\n#if (defined(_KERNEL) && defined(INVARIANTS))\n#define\tQMD_SLIST_CHECK_PREVPTR(prevp, elm) do {\t\t\t\\\n\tif (*(prevp) != (elm))\t\t\t\t\t\t\\\n\t\tpanic(\"Bad prevptr *(%p) == %p != %p\",\t\t\t\\\n\t\t    (prevp), *(prevp), (elm));\t\t\t\t\\\n} while (0)\n#else\n#define\tQMD_SLIST_CHECK_PREVPTR(prevp, elm)\n#endif\n\n#define SLIST_CONCAT(head1, head2, type, field) do {\t\t\t\\\n\tQUEUE_TYPEOF(type) *curelm = SLIST_FIRST(head1);\t\t\\\n\tif (curelm == NULL) {\t\t\t\t\t\t\\\n\t\tif ((SLIST_FIRST(head1) = SLIST_FIRST(head2)) != NULL)\t\\\n\t\t\tSLIST_INIT(head2);\t\t\t\t\\\n\t} else if (SLIST_FIRST(head2) != NULL) {\t\t\t\\\n\t\twhile (SLIST_NEXT(curelm, field) != NULL)\t\t\\\n\t\t\tcurelm = SLIST_NEXT(curelm, field);\t\t\\\n\t\tSLIST_NEXT(curelm, field) = SLIST_FIRST(head2);\t\t\\\n\t\tSLIST_INIT(head2);\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n} while (0)\n\n#define\tSLIST_EMPTY(head)\t((head)->slh_first == NULL)\n\n#define\tSLIST_FIRST(head)\t((head)->slh_first)\n\n#define\tSLIST_FOREACH(var, head, field)\t\t\t\t\t\\\n\tfor ((var) = SLIST_FIRST((head));\t\t\t\t\\\n\t    (var);\t\t\t\t\t\t\t\\\n\t    (var) = SLIST_NEXT((var), field))\n\n#define\tSLIST_FOREACH_FROM(var, head, field)\t\t\t\t\\\n\tfor ((var) = ((var) ? (var) : SLIST_FIRST((head)));\t\t\\\n\t    (var);\t\t\t\t\t\t\t\\\n\t    (var) = SLIST_NEXT((var), field))\n\n#define\tSLIST_FOREACH_SAFE(var, head, field, tvar)\t\t\t\\\n\tfor ((var) = SLIST_FIRST((head));\t\t\t\t\\\n\t    (var) && ((tvar) = SLIST_NEXT((var), field), 1);\t\t\\\n\t    (var) = (tvar))\n\n#define\tSLIST_FOREACH_FROM_SAFE(var, head, field, tvar)\t\t\t\\\n\tfor ((var) = ((var) ? (var) : SLIST_FIRST((head)));\t\t\\\n\t    (var) && ((tvar) = SLIST_NEXT((var), field), 1);\t\t\\\n\t    (var) = (tvar))\n\n#define\tSLIST_FOREACH_PREVPTR(var, varp, head, field)\t\t\t\\\n\tfor ((varp) = &SLIST_FIRST((head));\t\t\t\t\\\n\t    ((var) = *(varp)) != NULL;\t\t\t\t\t\\\n\t    (varp) = &SLIST_NEXT((var), field))\n\n#define\tSLIST_INIT(head) do {\t\t\t\t\t\t\\\n\tSLIST_FIRST((head)) = NULL;\t\t\t\t\t\\\n} while (0)\n\n#define\tSLIST_INSERT_AFTER(slistelm, elm, field) do {\t\t\t\\\n\tSLIST_NEXT((elm), field) = SLIST_NEXT((slistelm), field);\t\\\n\tSLIST_NEXT((slistelm), field) = (elm);\t\t\t\t\\\n} while (0)\n\n#define\tSLIST_INSERT_HEAD(head, elm, field) do {\t\t\t\\\n\tSLIST_NEXT((elm), field) = SLIST_FIRST((head));\t\t\t\\\n\tSLIST_FIRST((head)) = (elm);\t\t\t\t\t\\\n} while (0)\n\n#define\tSLIST_NEXT(elm, field)\t((elm)->field.sle_next)\n\n#define\tSLIST_REMOVE(head, elm, type, field) do {\t\t\t\\\n\tQMD_SAVELINK(oldnext, (elm)->field.sle_next);\t\t\t\\\n\tif (SLIST_FIRST((head)) == (elm)) {\t\t\t\t\\\n\t\tSLIST_REMOVE_HEAD((head), field);\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\telse {\t\t\t\t\t\t\t\t\\\n\t\tQUEUE_TYPEOF(type) *curelm = SLIST_FIRST(head);\t\t\\\n\t\twhile (SLIST_NEXT(curelm, field) != (elm))\t\t\\\n\t\t\tcurelm = SLIST_NEXT(curelm, field);\t\t\\\n\t\tSLIST_REMOVE_AFTER(curelm, field);\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\tTRASHIT(*oldnext);\t\t\t\t\t\t\\\n} while (0)\n\n#define SLIST_REMOVE_AFTER(elm, field) do {\t\t\t\t\\\n\tSLIST_NEXT(elm, field) =\t\t\t\t\t\\\n\t    SLIST_NEXT(SLIST_NEXT(elm, field), field);\t\t\t\\\n} while (0)\n\n#define\tSLIST_REMOVE_HEAD(head, field) do {\t\t\t\t\\\n\tSLIST_FIRST((head)) = SLIST_NEXT(SLIST_FIRST((head)), field);\t\\\n} while (0)\n\n#define\tSLIST_REMOVE_PREVPTR(prevp, elm, field) do {\t\t\t\\\n\tQMD_SLIST_CHECK_PREVPTR(prevp, elm);\t\t\t\t\\\n\t*(prevp) = SLIST_NEXT(elm, field);\t\t\t\t\\\n\tTRASHIT((elm)->field.sle_next);\t\t\t\t\t\\\n} while (0)\n\n#define SLIST_SWAP(head1, head2, type) do {\t\t\t\t\\\n\tQUEUE_TYPEOF(type) *swap_first = SLIST_FIRST(head1);\t\t\\\n\tSLIST_FIRST(head1) = SLIST_FIRST(head2);\t\t\t\\\n\tSLIST_FIRST(head2) = swap_first;\t\t\t\t\\\n} while (0)\n\n/*\n * Singly-linked Tail queue declarations.\n */\n#define\tSTAILQ_HEAD(name, type)\t\t\t\t\t\t\\\nstruct name {\t\t\t\t\t\t\t\t\\\n\tstruct type *stqh_first;/* first element */\t\t\t\\\n\tstruct type **stqh_last;/* addr of last next element */\t\t\\\n}\n\n#define\tSTAILQ_CLASS_HEAD(name, type)\t\t\t\t\t\\\nstruct name {\t\t\t\t\t\t\t\t\\\n\tclass type *stqh_first;\t/* first element */\t\t\t\\\n\tclass type **stqh_last;\t/* addr of last next element */\t\t\\\n}\n\n#define\tSTAILQ_HEAD_INITIALIZER(head)\t\t\t\t\t\\\n\t{ NULL, &(head).stqh_first }\n\n#define\tSTAILQ_ENTRY(type)\t\t\t\t\t\t\\\nstruct {\t\t\t\t\t\t\t\t\\\n\tstruct type *stqe_next;\t/* next element */\t\t\t\\\n}\n\n#define\tSTAILQ_CLASS_ENTRY(type)\t\t\t\t\t\\\nstruct {\t\t\t\t\t\t\t\t\\\n\tclass type *stqe_next;\t/* next element */\t\t\t\\\n}\n\n/*\n * Singly-linked Tail queue functions.\n */\n#define\tSTAILQ_CONCAT(head1, head2) do {\t\t\t\t\\\n\tif (!STAILQ_EMPTY((head2))) {\t\t\t\t\t\\\n\t\t*(head1)->stqh_last = (head2)->stqh_first;\t\t\\\n\t\t(head1)->stqh_last = (head2)->stqh_last;\t\t\\\n\t\tSTAILQ_INIT((head2));\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n} while (0)\n\n#define\tSTAILQ_EMPTY(head)\t((head)->stqh_first == NULL)\n\n#define\tSTAILQ_FIRST(head)\t((head)->stqh_first)\n\n#define\tSTAILQ_FOREACH(var, head, field)\t\t\t\t\\\n\tfor((var) = STAILQ_FIRST((head));\t\t\t\t\\\n\t   (var);\t\t\t\t\t\t\t\\\n\t   (var) = STAILQ_NEXT((var), field))\n\n#define\tSTAILQ_FOREACH_FROM(var, head, field)\t\t\t\t\\\n\tfor ((var) = ((var) ? (var) : STAILQ_FIRST((head)));\t\t\\\n\t   (var);\t\t\t\t\t\t\t\\\n\t   (var) = STAILQ_NEXT((var), field))\n\n#define\tSTAILQ_FOREACH_SAFE(var, head, field, tvar)\t\t\t\\\n\tfor ((var) = STAILQ_FIRST((head));\t\t\t\t\\\n\t    (var) && ((tvar) = STAILQ_NEXT((var), field), 1);\t\t\\\n\t    (var) = (tvar))\n\n#define\tSTAILQ_FOREACH_FROM_SAFE(var, head, field, tvar)\t\t\\\n\tfor ((var) = ((var) ? (var) : STAILQ_FIRST((head)));\t\t\\\n\t    (var) && ((tvar) = STAILQ_NEXT((var), field), 1);\t\t\\\n\t    (var) = (tvar))\n\n#define\tSTAILQ_INIT(head) do {\t\t\t\t\t\t\\\n\tSTAILQ_FIRST((head)) = NULL;\t\t\t\t\t\\\n\t(head)->stqh_last = &STAILQ_FIRST((head));\t\t\t\\\n} while (0)\n\n#define\tSTAILQ_INSERT_AFTER(head, tqelm, elm, field) do {\t\t\\\n\tif ((STAILQ_NEXT((elm), field) = STAILQ_NEXT((tqelm), field)) == NULL)\\\n\t\t(head)->stqh_last = &STAILQ_NEXT((elm), field);\t\t\\\n\tSTAILQ_NEXT((tqelm), field) = (elm);\t\t\t\t\\\n} while (0)\n\n#define\tSTAILQ_INSERT_HEAD(head, elm, field) do {\t\t\t\\\n\tif ((STAILQ_NEXT((elm), field) = STAILQ_FIRST((head))) == NULL)\t\\\n\t\t(head)->stqh_last = &STAILQ_NEXT((elm), field);\t\t\\\n\tSTAILQ_FIRST((head)) = (elm);\t\t\t\t\t\\\n} while (0)\n\n#define\tSTAILQ_INSERT_TAIL(head, elm, field) do {\t\t\t\\\n\tSTAILQ_NEXT((elm), field) = NULL;\t\t\t\t\\\n\t*(head)->stqh_last = (elm);\t\t\t\t\t\\\n\t(head)->stqh_last = &STAILQ_NEXT((elm), field);\t\t\t\\\n} while (0)\n\n#define\tSTAILQ_LAST(head, type, field)\t\t\t\t\\\n\t(STAILQ_EMPTY((head)) ? NULL :\t\t\t\t\\\n\t    __containerof((head)->stqh_last,\t\t\t\\\n\t    QUEUE_TYPEOF(type), field.stqe_next))\n\n#define\tSTAILQ_NEXT(elm, field)\t((elm)->field.stqe_next)\n\n#define\tSTAILQ_REMOVE(head, elm, type, field) do {\t\t\t\\\n\tQMD_SAVELINK(oldnext, (elm)->field.stqe_next);\t\t\t\\\n\tif (STAILQ_FIRST((head)) == (elm)) {\t\t\t\t\\\n\t\tSTAILQ_REMOVE_HEAD((head), field);\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\telse {\t\t\t\t\t\t\t\t\\\n\t\tQUEUE_TYPEOF(type) *curelm = STAILQ_FIRST(head);\t\\\n\t\twhile (STAILQ_NEXT(curelm, field) != (elm))\t\t\\\n\t\t\tcurelm = STAILQ_NEXT(curelm, field);\t\t\\\n\t\tSTAILQ_REMOVE_AFTER(head, curelm, field);\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\tTRASHIT(*oldnext);\t\t\t\t\t\t\\\n} while (0)\n\n#define STAILQ_REMOVE_AFTER(head, elm, field) do {\t\t\t\\\n\tif ((STAILQ_NEXT(elm, field) =\t\t\t\t\t\\\n\t     STAILQ_NEXT(STAILQ_NEXT(elm, field), field)) == NULL)\t\\\n\t\t(head)->stqh_last = &STAILQ_NEXT((elm), field);\t\t\\\n} while (0)\n\n#define\tSTAILQ_REMOVE_HEAD(head, field) do {\t\t\t\t\\\n\tif ((STAILQ_FIRST((head)) =\t\t\t\t\t\\\n\t     STAILQ_NEXT(STAILQ_FIRST((head)), field)) == NULL)\t\t\\\n\t\t(head)->stqh_last = &STAILQ_FIRST((head));\t\t\\\n} while (0)\n\n#define STAILQ_SWAP(head1, head2, type) do {\t\t\t\t\\\n\tQUEUE_TYPEOF(type) *swap_first = STAILQ_FIRST(head1);\t\t\\\n\tQUEUE_TYPEOF(type) **swap_last = (head1)->stqh_last;\t\t\\\n\tSTAILQ_FIRST(head1) = STAILQ_FIRST(head2);\t\t\t\\\n\t(head1)->stqh_last = (head2)->stqh_last;\t\t\t\\\n\tSTAILQ_FIRST(head2) = swap_first;\t\t\t\t\\\n\t(head2)->stqh_last = swap_last;\t\t\t\t\t\\\n\tif (STAILQ_EMPTY(head1))\t\t\t\t\t\\\n\t\t(head1)->stqh_last = &STAILQ_FIRST(head1);\t\t\\\n\tif (STAILQ_EMPTY(head2))\t\t\t\t\t\\\n\t\t(head2)->stqh_last = &STAILQ_FIRST(head2);\t\t\\\n} while (0)\n\n/*\n * List declarations.\n */\n#define\tLIST_HEAD(name, type)\t\t\t\t\t\t\\\nstruct name {\t\t\t\t\t\t\t\t\\\n\tstruct type *lh_first;\t/* first element */\t\t\t\\\n}\n\n#define\tLIST_CLASS_HEAD(name, type)\t\t\t\t\t\\\nstruct name {\t\t\t\t\t\t\t\t\\\n\tclass type *lh_first;\t/* first element */\t\t\t\\\n}\n\n#define\tLIST_HEAD_INITIALIZER(head)\t\t\t\t\t\\\n\t{ NULL }\n\n#define\tLIST_ENTRY(type)\t\t\t\t\t\t\\\nstruct {\t\t\t\t\t\t\t\t\\\n\tstruct type *le_next;\t/* next element */\t\t\t\\\n\tstruct type **le_prev;\t/* address of previous next element */\t\\\n}\n\n#define\tLIST_CLASS_ENTRY(type)\t\t\t\t\t\t\\\nstruct {\t\t\t\t\t\t\t\t\\\n\tclass type *le_next;\t/* next element */\t\t\t\\\n\tclass type **le_prev;\t/* address of previous next element */\t\\\n}\n\n/*\n * List functions.\n */\n\n#if (defined(_KERNEL) && defined(INVARIANTS))\n/*\n * QMD_LIST_CHECK_HEAD(LIST_HEAD *head, LIST_ENTRY NAME)\n *\n * If the list is non-empty, validates that the first element of the list\n * points back at 'head.'\n */\n#define\tQMD_LIST_CHECK_HEAD(head, field) do {\t\t\t\t\\\n\tif (LIST_FIRST((head)) != NULL &&\t\t\t\t\\\n\t    LIST_FIRST((head))->field.le_prev !=\t\t\t\\\n\t     &LIST_FIRST((head)))\t\t\t\t\t\\\n\t\tpanic(\"Bad list head %p first->prev != head\", (head));\t\\\n} while (0)\n\n/*\n * QMD_LIST_CHECK_NEXT(TYPE *elm, LIST_ENTRY NAME)\n *\n * If an element follows 'elm' in the list, validates that the next element\n * points back at 'elm.'\n */\n#define\tQMD_LIST_CHECK_NEXT(elm, field) do {\t\t\t\t\\\n\tif (LIST_NEXT((elm), field) != NULL &&\t\t\t\t\\\n\t    LIST_NEXT((elm), field)->field.le_prev !=\t\t\t\\\n\t     &((elm)->field.le_next))\t\t\t\t\t\\\n\t     \tpanic(\"Bad link elm %p next->prev != elm\", (elm));\t\\\n} while (0)\n\n/*\n * QMD_LIST_CHECK_PREV(TYPE *elm, LIST_ENTRY NAME)\n *\n * Validates that the previous element (or head of the list) points to 'elm.'\n */\n#define\tQMD_LIST_CHECK_PREV(elm, field) do {\t\t\t\t\\\n\tif (*(elm)->field.le_prev != (elm))\t\t\t\t\\\n\t\tpanic(\"Bad link elm %p prev->next != elm\", (elm));\t\\\n} while (0)\n#else\n#define\tQMD_LIST_CHECK_HEAD(head, field)\n#define\tQMD_LIST_CHECK_NEXT(elm, field)\n#define\tQMD_LIST_CHECK_PREV(elm, field)\n#endif /* (_KERNEL && INVARIANTS) */\n\n#define LIST_CONCAT(head1, head2, type, field) do {\t\t\t      \\\n\tQUEUE_TYPEOF(type) *curelm = LIST_FIRST(head1);\t\t\t      \\\n\tif (curelm == NULL) {\t\t\t\t\t\t      \\\n\t\tif ((LIST_FIRST(head1) = LIST_FIRST(head2)) != NULL) {\t      \\\n\t\t\tLIST_FIRST(head2)->field.le_prev =\t\t      \\\n\t\t\t    &LIST_FIRST((head1));\t\t\t      \\\n\t\t\tLIST_INIT(head2);\t\t\t\t      \\\n\t\t}\t\t\t\t\t\t\t      \\\n\t} else if (LIST_FIRST(head2) != NULL) {\t\t\t\t      \\\n\t\twhile (LIST_NEXT(curelm, field) != NULL)\t\t      \\\n\t\t\tcurelm = LIST_NEXT(curelm, field);\t\t      \\\n\t\tLIST_NEXT(curelm, field) = LIST_FIRST(head2);\t\t      \\\n\t\tLIST_FIRST(head2)->field.le_prev = &LIST_NEXT(curelm, field); \\\n\t\tLIST_INIT(head2);\t\t\t\t\t      \\\n\t}\t\t\t\t\t\t\t\t      \\\n} while (0)\n\n#define\tLIST_EMPTY(head)\t((head)->lh_first == NULL)\n\n#define\tLIST_FIRST(head)\t((head)->lh_first)\n\n#define\tLIST_FOREACH(var, head, field)\t\t\t\t\t\\\n\tfor ((var) = LIST_FIRST((head));\t\t\t\t\\\n\t    (var);\t\t\t\t\t\t\t\\\n\t    (var) = LIST_NEXT((var), field))\n\n#define\tLIST_FOREACH_FROM(var, head, field)\t\t\t\t\\\n\tfor ((var) = ((var) ? (var) : LIST_FIRST((head)));\t\t\\\n\t    (var);\t\t\t\t\t\t\t\\\n\t    (var) = LIST_NEXT((var), field))\n\n#define\tLIST_FOREACH_SAFE(var, head, field, tvar)\t\t\t\\\n\tfor ((var) = LIST_FIRST((head));\t\t\t\t\\\n\t    (var) && ((tvar) = LIST_NEXT((var), field), 1);\t\t\\\n\t    (var) = (tvar))\n\n#define\tLIST_FOREACH_FROM_SAFE(var, head, field, tvar)\t\t\t\\\n\tfor ((var) = ((var) ? (var) : LIST_FIRST((head)));\t\t\\\n\t    (var) && ((tvar) = LIST_NEXT((var), field), 1);\t\t\\\n\t    (var) = (tvar))\n\n#define\tLIST_INIT(head) do {\t\t\t\t\t\t\\\n\tLIST_FIRST((head)) = NULL;\t\t\t\t\t\\\n} while (0)\n\n#define\tLIST_INSERT_AFTER(listelm, elm, field) do {\t\t\t\\\n\tQMD_LIST_CHECK_NEXT(listelm, field);\t\t\t\t\\\n\tif ((LIST_NEXT((elm), field) = LIST_NEXT((listelm), field)) != NULL)\\\n\t\tLIST_NEXT((listelm), field)->field.le_prev =\t\t\\\n\t\t    &LIST_NEXT((elm), field);\t\t\t\t\\\n\tLIST_NEXT((listelm), field) = (elm);\t\t\t\t\\\n\t(elm)->field.le_prev = &LIST_NEXT((listelm), field);\t\t\\\n} while (0)\n\n#define\tLIST_INSERT_BEFORE(listelm, elm, field) do {\t\t\t\\\n\tQMD_LIST_CHECK_PREV(listelm, field);\t\t\t\t\\\n\t(elm)->field.le_prev = (listelm)->field.le_prev;\t\t\\\n\tLIST_NEXT((elm), field) = (listelm);\t\t\t\t\\\n\t*(listelm)->field.le_prev = (elm);\t\t\t\t\\\n\t(listelm)->field.le_prev = &LIST_NEXT((elm), field);\t\t\\\n} while (0)\n\n#define\tLIST_INSERT_HEAD(head, elm, field) do {\t\t\t\t\\\n\tQMD_LIST_CHECK_HEAD((head), field);\t\t\t\t\\\n\tif ((LIST_NEXT((elm), field) = LIST_FIRST((head))) != NULL)\t\\\n\t\tLIST_FIRST((head))->field.le_prev = &LIST_NEXT((elm), field);\\\n\tLIST_FIRST((head)) = (elm);\t\t\t\t\t\\\n\t(elm)->field.le_prev = &LIST_FIRST((head));\t\t\t\\\n} while (0)\n\n#define\tLIST_NEXT(elm, field)\t((elm)->field.le_next)\n\n#define\tLIST_PREV(elm, head, type, field)\t\t\t\\\n\t((elm)->field.le_prev == &LIST_FIRST((head)) ? NULL :\t\\\n\t    __containerof((elm)->field.le_prev,\t\t\t\\\n\t    QUEUE_TYPEOF(type), field.le_next))\n\n#define\tLIST_REMOVE(elm, field) do {\t\t\t\t\t\\\n\tQMD_SAVELINK(oldnext, (elm)->field.le_next);\t\t\t\\\n\tQMD_SAVELINK(oldprev, (elm)->field.le_prev);\t\t\t\\\n\tQMD_LIST_CHECK_NEXT(elm, field);\t\t\t\t\\\n\tQMD_LIST_CHECK_PREV(elm, field);\t\t\t\t\\\n\tif (LIST_NEXT((elm), field) != NULL)\t\t\t\t\\\n\t\tLIST_NEXT((elm), field)->field.le_prev = \t\t\\\n\t\t    (elm)->field.le_prev;\t\t\t\t\\\n\t*(elm)->field.le_prev = LIST_NEXT((elm), field);\t\t\\\n\tTRASHIT(*oldnext);\t\t\t\t\t\t\\\n\tTRASHIT(*oldprev);\t\t\t\t\t\t\\\n} while (0)\n\n#define LIST_SWAP(head1, head2, type, field) do {\t\t\t\\\n\tQUEUE_TYPEOF(type) *swap_tmp = LIST_FIRST(head1);\t\t\\\n\tLIST_FIRST((head1)) = LIST_FIRST((head2));\t\t\t\\\n\tLIST_FIRST((head2)) = swap_tmp;\t\t\t\t\t\\\n\tif ((swap_tmp = LIST_FIRST((head1))) != NULL)\t\t\t\\\n\t\tswap_tmp->field.le_prev = &LIST_FIRST((head1));\t\t\\\n\tif ((swap_tmp = LIST_FIRST((head2))) != NULL)\t\t\t\\\n\t\tswap_tmp->field.le_prev = &LIST_FIRST((head2));\t\t\\\n} while (0)\n\n/*\n * Tail queue declarations.\n */\n#define\tTAILQ_HEAD(name, type)\t\t\t\t\t\t\\\nstruct name {\t\t\t\t\t\t\t\t\\\n\tstruct type *tqh_first;\t/* first element */\t\t\t\\\n\tstruct type **tqh_last;\t/* addr of last next element */\t\t\\\n\tTRACEBUF\t\t\t\t\t\t\t\\\n}\n\n#define\tTAILQ_CLASS_HEAD(name, type)\t\t\t\t\t\\\nstruct name {\t\t\t\t\t\t\t\t\\\n\tclass type *tqh_first;\t/* first element */\t\t\t\\\n\tclass type **tqh_last;\t/* addr of last next element */\t\t\\\n\tTRACEBUF\t\t\t\t\t\t\t\\\n}\n\n#define\tTAILQ_HEAD_INITIALIZER(head)\t\t\t\t\t\\\n\t{ NULL, &(head).tqh_first, TRACEBUF_INITIALIZER }\n\n#define\tTAILQ_ENTRY(type)\t\t\t\t\t\t\\\nstruct {\t\t\t\t\t\t\t\t\\\n\tstruct type *tqe_next;\t/* next element */\t\t\t\\\n\tstruct type **tqe_prev;\t/* address of previous next element */\t\\\n\tTRACEBUF\t\t\t\t\t\t\t\\\n}\n\n#define\tTAILQ_CLASS_ENTRY(type)\t\t\t\t\t\t\\\nstruct {\t\t\t\t\t\t\t\t\\\n\tclass type *tqe_next;\t/* next element */\t\t\t\\\n\tclass type **tqe_prev;\t/* address of previous next element */\t\\\n\tTRACEBUF\t\t\t\t\t\t\t\\\n}\n\n/*\n * Tail queue functions.\n */\n#if (defined(_KERNEL) && defined(INVARIANTS))\n/*\n * QMD_TAILQ_CHECK_HEAD(TAILQ_HEAD *head, TAILQ_ENTRY NAME)\n *\n * If the tailq is non-empty, validates that the first element of the tailq\n * points back at 'head.'\n */\n#define\tQMD_TAILQ_CHECK_HEAD(head, field) do {\t\t\t\t\\\n\tif (!TAILQ_EMPTY(head) &&\t\t\t\t\t\\\n\t    TAILQ_FIRST((head))->field.tqe_prev !=\t\t\t\\\n\t     &TAILQ_FIRST((head)))\t\t\t\t\t\\\n\t\tpanic(\"Bad tailq head %p first->prev != head\", (head));\t\\\n} while (0)\n\n/*\n * QMD_TAILQ_CHECK_TAIL(TAILQ_HEAD *head, TAILQ_ENTRY NAME)\n *\n * Validates that the tail of the tailq is a pointer to pointer to NULL.\n */\n#define\tQMD_TAILQ_CHECK_TAIL(head, field) do {\t\t\t\t\\\n\tif (*(head)->tqh_last != NULL)\t\t\t\t\t\\\n\t    \tpanic(\"Bad tailq NEXT(%p->tqh_last) != NULL\", (head)); \t\\\n} while (0)\n\n/*\n * QMD_TAILQ_CHECK_NEXT(TYPE *elm, TAILQ_ENTRY NAME)\n *\n * If an element follows 'elm' in the tailq, validates that the next element\n * points back at 'elm.'\n */\n#define\tQMD_TAILQ_CHECK_NEXT(elm, field) do {\t\t\t\t\\\n\tif (TAILQ_NEXT((elm), field) != NULL &&\t\t\t\t\\\n\t    TAILQ_NEXT((elm), field)->field.tqe_prev !=\t\t\t\\\n\t     &((elm)->field.tqe_next))\t\t\t\t\t\\\n\t\tpanic(\"Bad link elm %p next->prev != elm\", (elm));\t\\\n} while (0)\n\n/*\n * QMD_TAILQ_CHECK_PREV(TYPE *elm, TAILQ_ENTRY NAME)\n *\n * Validates that the previous element (or head of the tailq) points to 'elm.'\n */\n#define\tQMD_TAILQ_CHECK_PREV(elm, field) do {\t\t\t\t\\\n\tif (*(elm)->field.tqe_prev != (elm))\t\t\t\t\\\n\t\tpanic(\"Bad link elm %p prev->next != elm\", (elm));\t\\\n} while (0)\n#else\n#define\tQMD_TAILQ_CHECK_HEAD(head, field)\n#define\tQMD_TAILQ_CHECK_TAIL(head, headname)\n#define\tQMD_TAILQ_CHECK_NEXT(elm, field)\n#define\tQMD_TAILQ_CHECK_PREV(elm, field)\n#endif /* (_KERNEL && INVARIANTS) */\n\n#define\tTAILQ_CONCAT(head1, head2, field) do {\t\t\t\t\\\n\tif (!TAILQ_EMPTY(head2)) {\t\t\t\t\t\\\n\t\t*(head1)->tqh_last = (head2)->tqh_first;\t\t\\\n\t\t(head2)->tqh_first->field.tqe_prev = (head1)->tqh_last;\t\\\n\t\t(head1)->tqh_last = (head2)->tqh_last;\t\t\t\\\n\t\tTAILQ_INIT((head2));\t\t\t\t\t\\\n\t\tQMD_TRACE_HEAD(head1);\t\t\t\t\t\\\n\t\tQMD_TRACE_HEAD(head2);\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n} while (0)\n\n#define\tTAILQ_EMPTY(head)\t((head)->tqh_first == NULL)\n\n#define\tTAILQ_FIRST(head)\t((head)->tqh_first)\n\n#define\tTAILQ_FOREACH(var, head, field)\t\t\t\t\t\\\n\tfor ((var) = TAILQ_FIRST((head));\t\t\t\t\\\n\t    (var);\t\t\t\t\t\t\t\\\n\t    (var) = TAILQ_NEXT((var), field))\n\n#define\tTAILQ_FOREACH_FROM(var, head, field)\t\t\t\t\\\n\tfor ((var) = ((var) ? (var) : TAILQ_FIRST((head)));\t\t\\\n\t    (var);\t\t\t\t\t\t\t\\\n\t    (var) = TAILQ_NEXT((var), field))\n\n#define\tTAILQ_FOREACH_SAFE(var, head, field, tvar)\t\t\t\\\n\tfor ((var) = TAILQ_FIRST((head));\t\t\t\t\\\n\t    (var) && ((tvar) = TAILQ_NEXT((var), field), 1);\t\t\\\n\t    (var) = (tvar))\n\n#define\tTAILQ_FOREACH_FROM_SAFE(var, head, field, tvar)\t\t\t\\\n\tfor ((var) = ((var) ? (var) : TAILQ_FIRST((head)));\t\t\\\n\t    (var) && ((tvar) = TAILQ_NEXT((var), field), 1);\t\t\\\n\t    (var) = (tvar))\n\n#define\tTAILQ_FOREACH_REVERSE(var, head, headname, field)\t\t\\\n\tfor ((var) = TAILQ_LAST((head), headname);\t\t\t\\\n\t    (var);\t\t\t\t\t\t\t\\\n\t    (var) = TAILQ_PREV((var), headname, field))\n\n#define\tTAILQ_FOREACH_REVERSE_FROM(var, head, headname, field)\t\t\\\n\tfor ((var) = ((var) ? (var) : TAILQ_LAST((head), headname));\t\\\n\t    (var);\t\t\t\t\t\t\t\\\n\t    (var) = TAILQ_PREV((var), headname, field))\n\n#define\tTAILQ_FOREACH_REVERSE_SAFE(var, head, headname, field, tvar)\t\\\n\tfor ((var) = TAILQ_LAST((head), headname);\t\t\t\\\n\t    (var) && ((tvar) = TAILQ_PREV((var), headname, field), 1);\t\\\n\t    (var) = (tvar))\n\n#define\tTAILQ_FOREACH_REVERSE_FROM_SAFE(var, head, headname, field, tvar) \\\n\tfor ((var) = ((var) ? (var) : TAILQ_LAST((head), headname));\t\\\n\t    (var) && ((tvar) = TAILQ_PREV((var), headname, field), 1);\t\\\n\t    (var) = (tvar))\n\n#define\tTAILQ_INIT(head) do {\t\t\t\t\t\t\\\n\tTAILQ_FIRST((head)) = NULL;\t\t\t\t\t\\\n\t(head)->tqh_last = &TAILQ_FIRST((head));\t\t\t\\\n\tQMD_TRACE_HEAD(head);\t\t\t\t\t\t\\\n} while (0)\n\n#define\tTAILQ_INSERT_AFTER(head, listelm, elm, field) do {\t\t\\\n\tQMD_TAILQ_CHECK_NEXT(listelm, field);\t\t\t\t\\\n\tif ((TAILQ_NEXT((elm), field) = TAILQ_NEXT((listelm), field)) != NULL)\\\n\t\tTAILQ_NEXT((elm), field)->field.tqe_prev = \t\t\\\n\t\t    &TAILQ_NEXT((elm), field);\t\t\t\t\\\n\telse {\t\t\t\t\t\t\t\t\\\n\t\t(head)->tqh_last = &TAILQ_NEXT((elm), field);\t\t\\\n\t\tQMD_TRACE_HEAD(head);\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\tTAILQ_NEXT((listelm), field) = (elm);\t\t\t\t\\\n\t(elm)->field.tqe_prev = &TAILQ_NEXT((listelm), field);\t\t\\\n\tQMD_TRACE_ELEM(&(elm)->field);\t\t\t\t\t\\\n\tQMD_TRACE_ELEM(&(listelm)->field);\t\t\t\t\\\n} while (0)\n\n#define\tTAILQ_INSERT_BEFORE(listelm, elm, field) do {\t\t\t\\\n\tQMD_TAILQ_CHECK_PREV(listelm, field);\t\t\t\t\\\n\t(elm)->field.tqe_prev = (listelm)->field.tqe_prev;\t\t\\\n\tTAILQ_NEXT((elm), field) = (listelm);\t\t\t\t\\\n\t*(listelm)->field.tqe_prev = (elm);\t\t\t\t\\\n\t(listelm)->field.tqe_prev = &TAILQ_NEXT((elm), field);\t\t\\\n\tQMD_TRACE_ELEM(&(elm)->field);\t\t\t\t\t\\\n\tQMD_TRACE_ELEM(&(listelm)->field);\t\t\t\t\\\n} while (0)\n\n#define\tTAILQ_INSERT_HEAD(head, elm, field) do {\t\t\t\\\n\tQMD_TAILQ_CHECK_HEAD(head, field);\t\t\t\t\\\n\tif ((TAILQ_NEXT((elm), field) = TAILQ_FIRST((head))) != NULL)\t\\\n\t\tTAILQ_FIRST((head))->field.tqe_prev =\t\t\t\\\n\t\t    &TAILQ_NEXT((elm), field);\t\t\t\t\\\n\telse\t\t\t\t\t\t\t\t\\\n\t\t(head)->tqh_last = &TAILQ_NEXT((elm), field);\t\t\\\n\tTAILQ_FIRST((head)) = (elm);\t\t\t\t\t\\\n\t(elm)->field.tqe_prev = &TAILQ_FIRST((head));\t\t\t\\\n\tQMD_TRACE_HEAD(head);\t\t\t\t\t\t\\\n\tQMD_TRACE_ELEM(&(elm)->field);\t\t\t\t\t\\\n} while (0)\n\n#define\tTAILQ_INSERT_TAIL(head, elm, field) do {\t\t\t\\\n\tQMD_TAILQ_CHECK_TAIL(head, field);\t\t\t\t\\\n\tTAILQ_NEXT((elm), field) = NULL;\t\t\t\t\\\n\t(elm)->field.tqe_prev = (head)->tqh_last;\t\t\t\\\n\t*(head)->tqh_last = (elm);\t\t\t\t\t\\\n\t(head)->tqh_last = &TAILQ_NEXT((elm), field);\t\t\t\\\n\tQMD_TRACE_HEAD(head);\t\t\t\t\t\t\\\n\tQMD_TRACE_ELEM(&(elm)->field);\t\t\t\t\t\\\n} while (0)\n\n#define\tTAILQ_LAST(head, headname)\t\t\t\t\t\\\n\t(*(((struct headname *)((head)->tqh_last))->tqh_last))\n\n/*\n * The FAST function is fast in that it causes no data access other\n * then the access to the head. The standard LAST function above\n * will cause a data access of both the element you want and\n * the previous element. FAST is very useful for instances when\n * you may want to prefetch the last data element.\n */\n#define\tTAILQ_LAST_FAST(head, type, field)\t\t\t\\\n    (TAILQ_EMPTY(head) ? NULL : __containerof((head)->tqh_last, QUEUE_TYPEOF(type), field.tqe_next))\n\n#define\tTAILQ_NEXT(elm, field) ((elm)->field.tqe_next)\n\n#define\tTAILQ_PREV(elm, headname, field)\t\t\t\t\\\n\t(*(((struct headname *)((elm)->field.tqe_prev))->tqh_last))\n\n#define\tTAILQ_PREV_FAST(elm, head, type, field)\t\t\t\t\\\n    ((elm)->field.tqe_prev == &(head)->tqh_first ? NULL :\t\t\\\n     __containerof((elm)->field.tqe_prev, QUEUE_TYPEOF(type), field.tqe_next))\n\n#define\tTAILQ_REMOVE(head, elm, field) do {\t\t\t\t\\\n\tQMD_SAVELINK(oldnext, (elm)->field.tqe_next);\t\t\t\\\n\tQMD_SAVELINK(oldprev, (elm)->field.tqe_prev);\t\t\t\\\n\tQMD_TAILQ_CHECK_NEXT(elm, field);\t\t\t\t\\\n\tQMD_TAILQ_CHECK_PREV(elm, field);\t\t\t\t\\\n\tif ((TAILQ_NEXT((elm), field)) != NULL)\t\t\t\t\\\n\t\tTAILQ_NEXT((elm), field)->field.tqe_prev = \t\t\\\n\t\t    (elm)->field.tqe_prev;\t\t\t\t\\\n\telse {\t\t\t\t\t\t\t\t\\\n\t\t(head)->tqh_last = (elm)->field.tqe_prev;\t\t\\\n\t\tQMD_TRACE_HEAD(head);\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\t*(elm)->field.tqe_prev = TAILQ_NEXT((elm), field);\t\t\\\n\tTRASHIT(*oldnext);\t\t\t\t\t\t\\\n\tTRASHIT(*oldprev);\t\t\t\t\t\t\\\n\tQMD_TRACE_ELEM(&(elm)->field);\t\t\t\t\t\\\n} while (0)\n\n#define TAILQ_SWAP(head1, head2, type, field) do {\t\t\t\\\n\tQUEUE_TYPEOF(type) *swap_first = (head1)->tqh_first;\t\t\\\n\tQUEUE_TYPEOF(type) **swap_last = (head1)->tqh_last;\t\t\\\n\t(head1)->tqh_first = (head2)->tqh_first;\t\t\t\\\n\t(head1)->tqh_last = (head2)->tqh_last;\t\t\t\t\\\n\t(head2)->tqh_first = swap_first;\t\t\t\t\\\n\t(head2)->tqh_last = swap_last;\t\t\t\t\t\\\n\tif ((swap_first = (head1)->tqh_first) != NULL)\t\t\t\\\n\t\tswap_first->field.tqe_prev = &(head1)->tqh_first;\t\\\n\telse\t\t\t\t\t\t\t\t\\\n\t\t(head1)->tqh_last = &(head1)->tqh_first;\t\t\\\n\tif ((swap_first = (head2)->tqh_first) != NULL)\t\t\t\\\n\t\tswap_first->field.tqe_prev = &(head2)->tqh_first;\t\\\n\telse\t\t\t\t\t\t\t\t\\\n\t\t(head2)->tqh_last = &(head2)->tqh_first;\t\t\\\n} while (0)\n\n#endif /* !_SYS_QUEUE_H_ */\n"
        },
        {
          "name": "restart.c",
          "type": "blob",
          "size": 13.732421875,
          "content": "#include \"memcached.h\"\n\n#include \"restart.h\"\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <sys/mman.h>\n#include <sys/types.h>\n#include <sys/stat.h>\n#include <fcntl.h>\n#include <string.h>\n\ntypedef struct _restart_data_cb restart_data_cb;\n\nstruct _restart_data_cb {\n    void *data; // user supplied opaque data.\n    struct _restart_data_cb *next; // callbacks are ordered stack\n    restart_check_cb ccb;\n    restart_save_cb scb;\n    char tag[RESTART_TAG_MAXLEN];\n};\n\n// TODO: struct to hand back to caller.\nstatic int mmap_fd = 0;\nstatic void *mmap_base = NULL;\nstatic size_t slabmem_limit = 0;\nchar *memory_file = NULL;\n\nstatic restart_data_cb *cb_stack = NULL;\n\n// Allows submodules and engines to have independent check and save metadata\n// routines for the restart code.\nvoid restart_register(const char *tag, restart_check_cb ccb, restart_save_cb scb, void *data) {\n    restart_data_cb *cb = calloc(1, sizeof(restart_data_cb));\n    if (cb == NULL) {\n        fprintf(stderr, \"[restart] failed to allocate callback register\\n\");\n        abort();\n    }\n\n    // Handle first time call initialization inline so we don't need separate\n    // API call.\n    if (cb_stack == NULL) {\n        cb_stack = cb;\n    } else {\n        // Ensure we fire the callbacks in registration order.\n        // Someday I'll get a queue.h overhaul.\n        restart_data_cb *finder = cb_stack;\n        while (finder->next != NULL) {\n            finder = finder->next;\n        }\n        finder->next = cb;\n    }\n\n    safe_strcpy(cb->tag, tag, RESTART_TAG_MAXLEN);\n    cb->data = data;\n    cb->ccb = *ccb;\n    cb->scb = *scb;\n}\n\ntypedef struct {\n    FILE *f;\n    restart_data_cb *cb;\n    char *line;\n    bool done;\n} restart_cb_ctx;\n\n// TODO: error string from cb?\n// - look for final line with checksum\n// - checksum entire file (up until final line)\n// - seek to start\n\nstatic int restart_check(const char *file) {\n    // metadata is kept in a separate file.\n    size_t flen = strlen(file);\n    const char *ext = \".meta\";\n    char *metafile = calloc(1, flen + strlen(ext) + 1);\n    if (metafile == NULL) {\n        // probably in a really bad position if we hit here, so don't start.\n        fprintf(stderr, \"[restart] failed to allocate memory for restart check\\n\");\n        abort();\n    }\n    memcpy(metafile, file, flen);\n    memcpy(metafile+flen, ext, strlen(ext));\n\n    FILE *f = fopen(metafile, \"r\");\n    if (f == NULL) {\n        fprintf(stderr, \"[restart] no metadata save file, starting with a clean cache\\n\");\n        free(metafile);\n        return -1;\n    }\n\n    restart_cb_ctx ctx;\n\n    ctx.f = f;\n    ctx.cb = NULL;\n    ctx.line = NULL;\n    ctx.done = false;\n    if (restart_get_kv(&ctx, NULL, NULL) != RESTART_DONE) {\n        // First line must be a tag, so read it in and set up the proper\n        // callback here.\n        fprintf(stderr, \"[restart] corrupt metadata file\\n\");\n        // TODO: this should probably just return -1 and skip the reuse.\n        abort();\n    }\n    if (ctx.cb == NULL) {\n        fprintf(stderr, \"[restart] Failed to read a tag from metadata file\\n\");\n        abort();\n    }\n\n    // loop call the callback, check result code.\n    bool failed = false;\n    while (!ctx.done) {\n        restart_data_cb *cb = ctx.cb;\n        if (cb->ccb(cb->tag, &ctx, cb->data) != 0) {\n            failed = true;\n            break;\n        }\n    }\n\n    if (ctx.line)\n        free(ctx.line);\n\n    fclose(f);\n\n    unlink(metafile);\n    free(metafile);\n\n    if (failed) {\n        fprintf(stderr, \"[restart] failed to validate metadata, starting with a clean cache\\n\");\n        return -1;\n    } else {\n        return 0;\n    }\n}\n\n// This function advances the file read while being called directly from the\n// callback.\n// The control inversion here (callback calling in which might change the next\n// callback) allows the callbacks to set up proper loops or sequences for\n// reading data back, avoiding an event model.\nenum restart_get_kv_ret restart_get_kv(void *ctx, char **key, char **val) {\n    char *line = NULL;\n    size_t len = 0;\n    restart_data_cb *cb = NULL;\n    restart_cb_ctx *c = (restart_cb_ctx *) ctx;\n    // free previous line.\n    // we could just pass it into getline, but it can randomly realloc so we'd\n    // have to re-assign it into the structure anyway.\n    if (c->line != NULL) {\n        free(c->line);\n        c->line = NULL;\n    }\n\n    if (getline(&line, &len, c->f) != -1) {\n        // First char is an indicator:\n        // T for TAG, changing the callback we use.\n        // K for key/value, to ship to the active callback.\n        char *p = line;\n        while (*p != '\\n') {\n            p++;\n        }\n        *p = '\\0';\n\n        if (line[0] == 'T') {\n            cb = cb_stack;\n            while (cb != NULL) {\n                // NOTE: len is allocated size, not line len. need to chomp \\n\n                if (strcmp(cb->tag, line+1) == 0) {\n                    break;\n                }\n                cb = cb->next;\n            }\n            if (cb == NULL) {\n                fprintf(stderr, \"[restart] internal handler for metadata tag not found: %s:\\n\", line+1);\n                return RESTART_NOTAG;\n            }\n            c->cb = cb;\n        } else if (line[0] == 'K') {\n            char *p = line+1; // start just ahead of the token.\n            // tokenize the string and return the pointers?\n            if (key != NULL) {\n                *key = p;\n            }\n\n            // turn key into a normal NULL terminated string.\n            while (*p != ' ' && (p - line < len)) {\n                p++;\n            }\n            *p = '\\0';\n            p++;\n\n            // value _should_ run until where the newline was, which is \\0 now\n            if (val != NULL) {\n                *val = p;\n            }\n            c->line = line;\n\n            return RESTART_OK;\n        } else {\n            // FIXME: proper error chain.\n            fprintf(stderr, \"[restart] invalid metadata line:\\n\\n%s\\n\", line);\n            free(line);\n            return RESTART_BADLINE;\n        }\n    } else {\n        // EOF or error in read.\n        c->done = true;\n    }\n\n    return RESTART_DONE;\n}\n\n// TODO:\n// - rolling checksum along with the writes.\n// - write final line + checksum + byte count or w/e.\n\nstatic int restart_save(const char *file) {\n    // metadata is kept in a separate file.\n    // FIXME: function.\n    size_t flen = strlen(file);\n    const char *ext = \".meta\";\n    size_t extlen = strlen(ext);\n    char *metafile = calloc(1, flen + extlen + 1);\n    if (metafile == NULL) {\n        fprintf(stderr, \"[restart] failed to allocate memory during metadata save\\n\");\n        return -1;\n    }\n    memcpy(metafile, file, flen);\n    memcpy(metafile+flen, ext, extlen);\n\n    // restrictive permissions for the metadata file.\n    // TODO: also for the mmap file eh? :P\n    mode_t oldmask = umask(~(S_IRUSR | S_IWUSR));\n    FILE *f = fopen(metafile, \"w\");\n    umask(oldmask);\n    if (f == NULL) {\n        // FIXME: correct error handling.\n        free(metafile);\n        perror(\"failed to write metadata file\");\n        return -1;\n    }\n\n    restart_data_cb *cb = cb_stack;\n    restart_cb_ctx ctx;\n    ctx.f = f;\n    while (cb != NULL) {\n        // Plugins/engines in the metadata file are separated by tag lines.\n        fprintf(f, \"T%s\\n\", cb->tag);\n        if (cb->scb(cb->tag, &ctx, cb->data) != 0) {\n            fclose(f);\n            free(metafile);\n            return -1;\n        }\n\n        cb = cb->next;\n    }\n\n    fclose(f);\n    free(metafile);\n\n    return 0;\n}\n\n// Keys and values must not contain spaces or newlines.\n// Could offer an interface that uriencodes values for the caller, however\n// nothing currently would use it, so add when necessary.\n#define SET_VAL_MAX 4096\nvoid restart_set_kv(void *ctx, const char *key, const char *fmt, ...) {\n    va_list ap;\n    restart_cb_ctx *c = (restart_cb_ctx *) ctx;\n    char valbuf[SET_VAL_MAX];\n\n    va_start(ap, fmt);\n    int vlen = vsnprintf(valbuf, SET_VAL_MAX-1, fmt, ap);\n    va_end(ap);\n    // This is heavy handed. We need to protect against corrupt data as much\n    // as possible. The buffer is large and these values are currently small,\n    // it will take a significant mistake to land here.\n    if (vlen >= SET_VAL_MAX) {\n        fprintf(stderr, \"[restart] fatal error while saving metadata state, value too long for: %s %s\",\n                key, valbuf);\n        abort();\n    }\n\n    fprintf(c->f, \"K%s %s\\n\", key, valbuf);\n    // TODO: update crc32c\n}\n\nstatic long _find_pagesize(void) {\n#if defined(HAVE_SYSCONF) && defined(_SC_PAGESIZE)\n    return sysconf(_SC_PAGESIZE);\n#else\n    // A good guess.\n    return 4096;\n#endif\n}\n\nbool restart_mmap_open(const size_t limit, const char *file, void **mem_base) {\n    bool reuse_mmap = true;\n\n    long pagesize = _find_pagesize();\n    memory_file = strdup(file);\n    mmap_fd = open(file, O_RDWR|O_CREAT, S_IRWXU);\n    if (mmap_fd == -1) {\n        perror(\"failed to open file for mmap\");\n        abort();\n    }\n    if (ftruncate(mmap_fd, limit) != 0) {\n        perror(\"ftruncate failed\");\n        abort();\n    }\n    /* Allocate everything in a big chunk with malloc */\n    if (limit % pagesize) {\n        // This is a sanity check; shouldn't ever be possible since we\n        // increase memory by whole megabytes.\n        fprintf(stderr, \"[restart] memory limit not divisible evenly by pagesize (please report bug)\\n\");\n        abort();\n    }\n    mmap_base = mmap(NULL, limit, PROT_READ|PROT_WRITE, MAP_SHARED, mmap_fd, 0);\n    if (mmap_base == MAP_FAILED) {\n        perror(\"failed to mmap, aborting\");\n        abort();\n    }\n    // Set the limit before calling check_mmap, so we can find the meta page..\n    slabmem_limit = limit;\n    if (restart_check(file) != 0) {\n        reuse_mmap = false;\n    }\n    *mem_base = mmap_base;\n\n    return reuse_mmap;\n}\n\n/* Gracefully stop/close the shared memory segment */\nvoid restart_mmap_close(void) {\n    msync(mmap_base, slabmem_limit, MS_SYNC);\n\n    if (restart_save(memory_file) != 0) {\n        fprintf(stderr, \"[restart] failed to save metadata\");\n    }\n\n    if (munmap(mmap_base, slabmem_limit) != 0) {\n        perror(\"[restart] failed to munmap shared memory\");\n    } else if (close(mmap_fd) != 0) {\n        perror(\"[restart] failed to close shared memory fd\");\n    }\n\n    free(memory_file);\n}\n\n// given memory base, quickly walk memory and do pointer fixup.\n// do this once on startup to avoid having to do pointer fixup on every\n// reference from hash table or LRU.\nunsigned int restart_fixup(void *orig_addr) {\n    struct timeval tv;\n    uint64_t checked = 0;\n    const unsigned int page_size = settings.slab_page_size;\n    unsigned int page_remain = page_size;\n\n    gettimeofday(&tv, NULL);\n    if (settings.verbose > 0) {\n        fprintf(stderr, \"[restart] original memory base: [%p] new base: [%p]\\n\", orig_addr, mmap_base);\n        fprintf(stderr, \"[restart] recovery start [%d.%d]\\n\", (int)tv.tv_sec, (int)tv.tv_usec);\n    }\n\n    // since chunks don't align with pages, we have to also track page size.\n    while (checked < slabmem_limit) {\n        //fprintf(stderr, \"checked: %lu\\n\", checked);\n        item *it = (item *)((char *)mmap_base + checked);\n\n        int size = slabs_fixup((char *)mmap_base + checked,\n                checked % settings.slab_page_size);\n        //fprintf(stderr, \"id: %d, size: %d remain: %u\\n\", it->slabs_clsid, size, page_remain);\n        // slabber gobbled an entire page, skip and move on.\n        if (size == -1) {\n            assert(page_remain % page_size == 0);\n            assert(page_remain == page_size);\n            checked += page_remain;\n            page_remain = page_size;\n            continue;\n        }\n\n        if (it->it_flags & ITEM_LINKED) {\n            // fixup next/prev links while on LRU.\n            if (it->next) {\n                it->next = (item *)((mc_ptr_t)it->next - (mc_ptr_t)orig_addr);\n                it->next = (item *)((mc_ptr_t)it->next + (mc_ptr_t)mmap_base);\n            }\n            if (it->prev) {\n                it->prev = (item *)((mc_ptr_t)it->prev - (mc_ptr_t)orig_addr);\n                it->prev = (item *)((mc_ptr_t)it->prev + (mc_ptr_t)mmap_base);\n            }\n\n            //fprintf(stderr, \"item was linked\\n\");\n            do_item_link_fixup(it);\n        }\n\n        if (it->it_flags & (ITEM_CHUNKED|ITEM_CHUNK)) {\n            item_chunk *ch;\n            if (it->it_flags & ITEM_CHUNKED) {\n                ch = (item_chunk *) ITEM_schunk(it);\n                // Sigh. Chunked items are a hack; the clsid is the clsid of\n                // the full object (always the largest slab class) rather than\n                // the actual chunk.\n                // I bet this is fixable :(\n                size = slabs_size(ch->orig_clsid);\n                //fprintf(stderr, \"fixing chunked item header [%d]\\n\", size);\n            } else {\n                //fprintf(stderr, \"fixing item chunk [%d]\\n\", size);\n                ch = (item_chunk *) it;\n            }\n            if (ch->next) {\n                ch->next = (item_chunk *)((mc_ptr_t)ch->next - (mc_ptr_t)orig_addr);\n                ch->next = (item_chunk *)((mc_ptr_t)ch->next + (mc_ptr_t)mmap_base);\n            }\n            if (ch->prev) {\n                ch->prev = (item_chunk *)((mc_ptr_t)ch->prev - (mc_ptr_t)orig_addr);\n                ch->prev = (item_chunk *)((mc_ptr_t)ch->prev + (mc_ptr_t)mmap_base);\n            }\n            if (ch->head) {\n                ch->head = (item *)((mc_ptr_t)ch->head - (mc_ptr_t)orig_addr);\n                ch->head = (item *)((mc_ptr_t)ch->head + (mc_ptr_t)mmap_base);\n            }\n        }\n\n        // next chunk\n        checked += size;\n        page_remain -= size;\n        if (size > page_remain) {\n            //fprintf(stderr, \"doot %d\\n\", page_remain);\n            checked += page_remain;\n            page_remain = settings.slab_page_size;\n        }\n        //assert(checked != 3145728);\n    }\n\n    if (settings.verbose > 0) {\n        gettimeofday(&tv, NULL);\n        fprintf(stderr, \"[restart] recovery end [%d.%d]\\n\", (int)tv.tv_sec, (int)tv.tv_usec);\n    }\n\n    return 0;\n}\n"
        },
        {
          "name": "restart.h",
          "type": "blob",
          "size": 0.8388671875,
          "content": "#ifndef RESTART_H\n#define RESTART_H\n\n#define RESTART_TAG_MAXLEN 255\n\n// Track the pointer size for restart fiddling.\n#if SIZEOF_VOID_P == 8\n    typedef uint64_t mc_ptr_t;\n#else\n    typedef uint32_t mc_ptr_t;\n#endif\n\nenum restart_get_kv_ret {\n    RESTART_OK=0, RESTART_NOTAG, RESTART_BADLINE, RESTART_DONE\n};\n\ntypedef int (*restart_check_cb)(const char *tag, void *ctx, void *data);\ntypedef int (*restart_save_cb)(const char *tag, void *ctx, void *data);\nvoid restart_register(const char *tag, restart_check_cb ccb, restart_save_cb scb, void *data);\n\nvoid restart_set_kv(void *ctx, const char *key, const char *fmt, ...);\nenum restart_get_kv_ret restart_get_kv(void *ctx, char **key, char **val);\n\nbool restart_mmap_open(const size_t limit, const char *file, void **mem_base);\nvoid restart_mmap_close(void);\nunsigned int restart_fixup(void *old_base);\n\n#endif\n"
        },
        {
          "name": "sasl_defs.c",
          "type": "blob",
          "size": 6.0576171875,
          "content": "/* -*- Mode: C; tab-width: 4; c-basic-offset: 4; indent-tabs-mode: nil -*- */\n#include \"memcached.h\"\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <sasl/saslplug.h>\n\nchar my_sasl_hostname[1025];\n\n#if defined(HAVE_SASL_CB_GETCONF) || defined(HAVE_SASL_CB_GETCONFPATH)\n/* The locations we may search for a SASL config file if the user didn't\n * specify one in the environment variable SASL_CONF_PATH\n */\nconst char * const locations[] = {\n    \"/etc/sasl/memcached.conf\",\n    \"/etc/sasl2/memcached.conf\",\n    NULL\n};\n\n/* If the element of locations is file, locations_dir_path stores the\n * directory path of these elements */\nconst char *const locations_dir_path[] = {\n    \"/etc/sasl\",\n    \"/etc/sasl2\",\n    NULL\n};\n\n/* If the element of locations is directory, locations_file_path stores\n * the actual configure file which used by sasl, when GETCONFPATH is\n * enabled */\nconst char *const locations_file_path[] = {\n    \"/etc/sasl/memcached.conf/memcached.conf\",\n    \"/etc/sasl2/memcached.conf/memcached.conf\",\n    NULL\n};\n#endif\n\n#ifndef HAVE_SASL_CALLBACK_FT\ntypedef int (*sasl_callback_ft)(void);\n#endif\n\n#ifdef ENABLE_SASL_PWDB\n#define MAX_ENTRY_LEN 256\n\nstatic const char *memcached_sasl_pwdb;\n\nstatic int sasl_server_userdb_checkpass(sasl_conn_t *conn,\n                                        void *context,\n                                        const char *user,\n                                        const char *pass,\n                                        unsigned passlen,\n                                        struct propctx *propctx)\n{\n    size_t unmlen = strlen(user);\n    if ((passlen + unmlen) > (MAX_ENTRY_LEN - 4)) {\n        fprintf(stderr,\n                \"WARNING: Failed to authenticate <%s> due to too long password (%d)\\n\",\n                user, passlen);\n        return SASL_NOAUTHZ;\n    }\n\n    FILE *pwfile = fopen(memcached_sasl_pwdb, \"r\");\n    if (pwfile == NULL) {\n        if (settings.verbose) {\n            vperror(\"WARNING: Failed to open sasl database <%s>\",\n                    memcached_sasl_pwdb);\n        }\n        return SASL_NOAUTHZ;\n    }\n\n    char buffer[MAX_ENTRY_LEN];\n    bool ok = false;\n\n    while ((fgets(buffer, sizeof(buffer), pwfile)) != NULL) {\n        if (memcmp(user, buffer, unmlen) == 0 && buffer[unmlen] == ':') {\n            /* This is the correct user */\n            ++unmlen;\n            if (memcmp(pass, buffer + unmlen, passlen) == 0 &&\n                (buffer[unmlen + passlen] == ':' || /* Additional tokens */\n                 buffer[unmlen + passlen] == '\\n' || /* end of line */\n                 buffer[unmlen + passlen] == '\\r'|| /* dos format? */\n                 buffer[unmlen + passlen] == '\\0')) { /* line truncated */\n                ok = true;\n            }\n\n            break;\n        }\n    }\n    (void)fclose(pwfile);\n    if (ok) {\n        return SASL_OK;\n    }\n\n    if (settings.verbose) {\n        fprintf(stderr, \"INFO: User <%s> failed to authenticate\\n\", user);\n    }\n\n    return SASL_NOAUTHZ;\n}\n#endif\n\n#if defined(HAVE_SASL_CB_GETCONF) || defined(HAVE_SASL_CB_GETCONFPATH)\nstatic int sasl_getconf(void *context, const char **path)\n{\n    *path = getenv(\"SASL_CONF_PATH\");\n\n    if (*path == NULL) {\n#if defined(HAVE_SASL_CB_GETCONF)\n        for (int i = 0; locations[i] != NULL; ++i) {\n            if (access(locations[i], F_OK) == 0) {\n                *path = locations[i];\n                break;\n            }\n        }\n#elif defined(HAVE_SASL_CB_GETCONFPATH)\n        for (int i = 0; locations[i] != NULL; ++i) {\n            if (access(locations_file_path[i], F_OK) == 0) {\n                *path = locations[i];\n                break;\n            } else if (access(locations[i], F_OK) == 0) {\n                *path = locations_dir_path[i];\n                break;\n            }\n        }\n#endif\n    }\n\n    if (settings.verbose) {\n        if (*path != NULL) {\n            fprintf(stderr, \"Reading configuration from: <%s>\\n\", *path);\n        } else {\n            fprintf(stderr, \"Failed to locate a config path\\n\");\n        }\n\n    }\n\n    return (*path != NULL) ? SASL_OK : SASL_FAIL;\n}\n#endif\n\nstatic int sasl_log(void *context, int level, const char *message)\n{\n    bool log = true;\n\n    switch (level) {\n    case SASL_LOG_NONE:\n        log = false;\n        break;\n    case SASL_LOG_PASS:\n    case SASL_LOG_TRACE:\n    case SASL_LOG_DEBUG:\n    case SASL_LOG_NOTE:\n        if (settings.verbose < 2) {\n            log = false;\n        }\n        break;\n    case SASL_LOG_WARN:\n    case SASL_LOG_FAIL:\n        if (settings.verbose < 1) {\n            log = false;\n        }\n        break;\n    default:\n        /* This is an error */\n        ;\n    }\n\n    if (log) {\n        fprintf(stderr, \"SASL (severity %d): %s\\n\", level, message);\n    }\n\n    return SASL_OK;\n}\n\nstatic sasl_callback_t sasl_callbacks[] = {\n#ifdef ENABLE_SASL_PWDB\n   { SASL_CB_SERVER_USERDB_CHECKPASS, (sasl_callback_ft)sasl_server_userdb_checkpass, NULL },\n#endif\n\n   { SASL_CB_LOG, (sasl_callback_ft)sasl_log, NULL },\n\n#ifdef HAVE_SASL_CB_GETCONF\n   { SASL_CB_GETCONF, sasl_getconf, NULL },\n#else\n#ifdef HAVE_SASL_CB_GETCONFPATH\n   { SASL_CB_GETCONFPATH, (sasl_callback_ft)sasl_getconf, NULL },\n#endif\n#endif\n\n   { SASL_CB_LIST_END, NULL, NULL }\n};\n\nvoid init_sasl(void) {\n#ifdef ENABLE_SASL_PWDB\n    memcached_sasl_pwdb = getenv(\"MEMCACHED_SASL_PWDB\");\n    if (memcached_sasl_pwdb == NULL) {\n       if (settings.verbose) {\n          fprintf(stderr,\n                  \"INFO: MEMCACHED_SASL_PWDB not specified. \"\n                  \"Internal passwd database disabled\\n\");\n       }\n       sasl_callbacks[0].id = SASL_CB_LIST_END;\n       sasl_callbacks[0].proc = NULL;\n    }\n#endif\n\n    memset(my_sasl_hostname, 0, sizeof(my_sasl_hostname));\n    if (gethostname(my_sasl_hostname, sizeof(my_sasl_hostname)-1) == -1) {\n        if (settings.verbose) {\n            fprintf(stderr, \"Error discovering hostname for SASL\\n\");\n        }\n        my_sasl_hostname[0] = '\\0';\n    }\n\n    if (sasl_server_init(sasl_callbacks, \"memcached\") != SASL_OK) {\n        fprintf(stderr, \"Error initializing sasl.\\n\");\n        exit(EXIT_FAILURE);\n    } else {\n        if (settings.verbose) {\n            fprintf(stderr, \"Initialized SASL.\\n\");\n        }\n    }\n}\n"
        },
        {
          "name": "sasl_defs.h",
          "type": "blob",
          "size": 0.6767578125,
          "content": "#ifndef SASL_DEFS_H\n#define SASL_DEFS_H 1\n\n// Longest one I could find was ``9798-U-RSA-SHA1-ENC''\n#define MAX_SASL_MECH_LEN 32\n\n#if defined(HAVE_SASL_SASL_H) && defined(ENABLE_SASL)\n\n#include <sasl/sasl.h>\nvoid init_sasl(void);\n\nextern char my_sasl_hostname[1025];\n\n#else /* End of SASL support */\n\ntypedef void* sasl_conn_t;\n\n#define init_sasl() {}\n#define sasl_dispose(x) {}\n#define sasl_server_new(a, b, c, d, e, f, g, h) 1\n#define sasl_listmech(a, b, c, d, e, f, g, h) 1\n#define sasl_server_start(a, b, c, d, e, f) 1\n#define sasl_server_step(a, b, c, d, e) 1\n#define sasl_getprop(a, b, c) {}\n\n#define SASL_OK 0\n#define SASL_CONTINUE -1\n\n#endif /* sasl compat */\n\n#endif /* SASL_DEFS_H */\n"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "sizes.c",
          "type": "blob",
          "size": 1.1474609375,
          "content": "#include <stdio.h>\n\n#include \"memcached.h\"\n\nstatic void display(const char *name, size_t size) {\n    printf(\"%s\\t%d\\n\", name, (int)size);\n}\n\nint main(int argc, char **argv) {\n\n    display(\"Slab Stats\", sizeof(struct slab_stats));\n    display(\"Thread stats\",\n            sizeof(struct thread_stats)\n            - (MAX_NUMBER_OF_SLAB_CLASSES * sizeof(struct slab_stats)));\n    display(\"Global stats\", sizeof(struct stats));\n    display(\"Settings\", sizeof(struct settings));\n    display(\"Item (no cas)\", sizeof(item));\n    display(\"Item (cas)\", sizeof(item) + sizeof(uint64_t));\n#ifdef EXTSTORE\n    display(\"extstore header\", sizeof(item_hdr));\n#endif\n    display(\"Libevent thread\",\n            sizeof(LIBEVENT_THREAD) - sizeof(struct thread_stats));\n    display(\"Connection\", sizeof(conn));\n    display(\"Response object\", sizeof(mc_resp));\n    display(\"Response bundle\", sizeof(mc_resp_bundle));\n    display(\"Response objects per bundle\", MAX_RESP_PER_BUNDLE);\n\n    printf(\"----------------------------------------\\n\");\n\n    display(\"libevent thread cumulative\", sizeof(LIBEVENT_THREAD));\n    display(\"Thread stats cumulative\\t\", sizeof(struct thread_stats));\n\n    return 0;\n}\n"
        },
        {
          "name": "slab_automove.c",
          "type": "blob",
          "size": 5.5927734375,
          "content": "/*  Copyright 2017 Facebook.\n *\n *  Use and distribution licensed under the BSD license.  See\n *  the LICENSE file for full text.\n */\n\n/* -*- Mode: C; tab-width: 4; c-basic-offset: 4; indent-tabs-mode: nil -*- */\n#include \"memcached.h\"\n#include \"slab_automove.h\"\n#include <stdlib.h>\n#include <string.h>\n\n#define MIN_PAGES_FOR_SOURCE 2\n#define MIN_PAGES_FOR_RECLAIM 2.5\n\nstruct window_data {\n    uint64_t age;\n    uint64_t dirty;\n    float evicted_ratio;\n    uint64_t evicted_seen; // if evictions were seen at all this window\n};\n\ntypedef struct {\n    struct window_data *window_data;\n    uint32_t window_size;\n    uint32_t window_cur;\n    double max_age_ratio;\n    item_stats_automove iam_before[MAX_NUMBER_OF_SLAB_CLASSES];\n    item_stats_automove iam_after[MAX_NUMBER_OF_SLAB_CLASSES];\n    slab_stats_automove sam_before[MAX_NUMBER_OF_SLAB_CLASSES];\n    slab_stats_automove sam_after[MAX_NUMBER_OF_SLAB_CLASSES];\n} slab_automove;\n\nvoid *slab_automove_init(struct settings *settings) {\n    uint32_t window_size = settings->slab_automove_window;\n    double max_age_ratio = settings->slab_automove_ratio;\n    slab_automove *a = calloc(1, sizeof(slab_automove));\n    if (a == NULL)\n        return NULL;\n    a->window_data = calloc(window_size * MAX_NUMBER_OF_SLAB_CLASSES, sizeof(struct window_data));\n    a->window_size = window_size;\n    a->max_age_ratio = max_age_ratio;\n    if (a->window_data == NULL) {\n        free(a);\n        return NULL;\n    }\n\n    // do a dry run to fill the before structs\n    fill_item_stats_automove(a->iam_before);\n    fill_slab_stats_automove(a->sam_before);\n\n    return (void *)a;\n}\n\nvoid slab_automove_free(void *arg) {\n    slab_automove *a = (slab_automove *)arg;\n    free(a->window_data);\n    free(a);\n}\n\nstatic void window_sum(struct window_data *wd, struct window_data *w, uint32_t size) {\n    int x;\n    for (x = 0; x < size; x++) {\n        struct window_data *d = &wd[x];\n        w->age += d->age;\n        w->dirty += d->dirty;\n        w->evicted_ratio += d->evicted_ratio;\n        w->evicted_seen += d->evicted_seen;\n    }\n}\n\n// TODO: if oldest is dirty, find next oldest.\n// still need to base ratio off of absolute age\nvoid slab_automove_run(void *arg, int *src, int *dst) {\n    slab_automove *a = (slab_automove *)arg;\n    int n;\n    struct window_data w_sum;\n    int oldest = -1;\n    uint64_t oldest_age = 0;\n    int youngest = -1;\n    uint64_t youngest_age = ~0;\n    bool youngest_evicting = false;\n    *src = -1;\n    *dst = -1;\n\n    // fill after structs\n    fill_item_stats_automove(a->iam_after);\n    fill_slab_stats_automove(a->sam_after);\n    // Loop once to get total_evicted for this window.\n    uint64_t evicted_total = 0;\n    for (n = POWER_SMALLEST; n < MAX_NUMBER_OF_SLAB_CLASSES; n++) {\n        evicted_total += a->iam_after[n].evicted - a->iam_before[n].evicted;\n    }\n    a->window_cur++;\n\n    // iterate slabs\n    for (n = POWER_SMALLEST; n < MAX_NUMBER_OF_SLAB_CLASSES; n++) {\n        int w_offset = n * a->window_size;\n        struct window_data *wd = &a->window_data[w_offset + (a->window_cur % a->window_size)];\n        memset(wd, 0, sizeof(struct window_data));\n\n        // if page delta, or evicted delta, mark window dirty\n        // (or outofmemory)\n        uint64_t evicted_delta = a->iam_after[n].evicted - a->iam_before[n].evicted;\n        if (evicted_delta > 0) {\n            // FIXME: the python script is using floats. we have ints.\n            wd->evicted_ratio = (float) evicted_delta / evicted_total;\n            wd->evicted_seen = 1;\n            wd->dirty = 1;\n        }\n\n        if (a->iam_after[n].outofmemory - a->iam_before[n].outofmemory > 0) {\n            wd->dirty = 1;\n        }\n        if (a->sam_after[n].total_pages - a->sam_before[n].total_pages > 0) {\n            wd->dirty = 1;\n        }\n\n        // set age into window\n        wd->age = a->iam_after[n].age;\n\n        // summarize the window-up-to-now.\n        memset(&w_sum, 0, sizeof(struct window_data));\n        window_sum(&a->window_data[w_offset], &w_sum, a->window_size);\n\n        // grab age as average of window total\n        uint64_t age = w_sum.age / a->window_size;\n\n        // if > N free chunks and not dirty, make decision.\n        if (a->sam_after[n].free_chunks > a->sam_after[n].chunks_per_page * MIN_PAGES_FOR_RECLAIM) {\n            if (w_sum.dirty == 0) {\n                *src = n;\n                *dst = 0;\n                youngest = oldest = -1;\n                break;\n            }\n        }\n\n        // if oldest and have enough pages, is oldest\n        if (age > oldest_age && a->sam_after[n].total_pages > MIN_PAGES_FOR_SOURCE) {\n            oldest = n;\n            oldest_age = age;\n        }\n\n        // grab evicted count from window\n        // if > half the window and youngest, mark as youngest\n        // or, if more than 25% of total evictions in the window.\n        if (age < youngest_age && (w_sum.evicted_seen > a->window_size / 2\n                    || w_sum.evicted_ratio / a->window_size > 0.25)) {\n            youngest = n;\n            youngest_age = age;\n            youngest_evicting = wd->evicted_seen ? true : false;\n        }\n    }\n\n    memcpy(a->iam_before, a->iam_after,\n            sizeof(item_stats_automove) * MAX_NUMBER_OF_SLAB_CLASSES);\n    memcpy(a->sam_before, a->sam_after,\n            sizeof(slab_stats_automove) * MAX_NUMBER_OF_SLAB_CLASSES);\n    // if we have a youngest and oldest, and oldest is outside the ratio,\n    // also, only make decisions if window has filled once.\n    if (youngest != -1 && oldest != -1 && a->window_cur > a->window_size) {\n        if (youngest_age < ((double)oldest_age * a->max_age_ratio) && youngest_evicting) {\n            *src = oldest;\n            *dst = youngest;\n        }\n    }\n    return;\n}\n"
        },
        {
          "name": "slab_automove.h",
          "type": "blob",
          "size": 0.5546875,
          "content": "#ifndef SLAB_AUTOMOVE_H\n#define SLAB_AUTOMOVE_H\n\n/* default automove functions */\nvoid *slab_automove_init(struct settings *settings);\nvoid slab_automove_free(void *arg);\nvoid slab_automove_run(void *arg, int *src, int *dst);\n\ntypedef void *(*slab_automove_init_func)(struct settings *settings);\ntypedef void (*slab_automove_free_func)(void *arg);\ntypedef void (*slab_automove_run_func)(void *arg, int *src, int *dst);\n\ntypedef struct {\n    slab_automove_init_func init;\n    slab_automove_free_func free;\n    slab_automove_run_func run;\n} slab_automove_reg_t;\n\n#endif\n"
        },
        {
          "name": "slab_automove_extstore.c",
          "type": "blob",
          "size": 8.3505859375,
          "content": "/*  Copyright 2017 Facebook.\n *\n *  Use and distribution licensed under the BSD license.  See\n *  the LICENSE file for full text.\n */\n\n/* -*- Mode: C; tab-width: 4; c-basic-offset: 4; indent-tabs-mode: nil -*- */\n#include \"memcached.h\"\n#include \"slab_automove_extstore.h\"\n#include <stdlib.h>\n#include <string.h>\n\n#define MIN_PAGES_FOR_SOURCE 2\n\nstruct window_data {\n    uint64_t age;\n    uint64_t dirty;\n    uint64_t evicted;\n    unsigned int excess_free;\n    unsigned int relaxed;\n};\n\n// TODO: use ptrs for before/after to cut the memcpy\n// after reach run and save some cpu.\ntypedef struct {\n    struct window_data *window_data;\n    struct settings *settings;\n    uint32_t window_size;\n    uint32_t window_cur;\n    uint32_t item_size;\n    double max_age_ratio;\n    double free_ratio;\n    bool pool_filled_once;\n    unsigned int global_pool_watermark;\n    item_stats_automove iam_before[MAX_NUMBER_OF_SLAB_CLASSES];\n    item_stats_automove iam_after[MAX_NUMBER_OF_SLAB_CLASSES];\n    slab_stats_automove sam_before[MAX_NUMBER_OF_SLAB_CLASSES];\n    slab_stats_automove sam_after[MAX_NUMBER_OF_SLAB_CLASSES];\n} slab_automove;\n\nvoid *slab_automove_extstore_init(struct settings *settings) {\n    uint32_t window_size = settings->slab_automove_window;\n    double max_age_ratio = settings->slab_automove_ratio;\n    slab_automove *a = calloc(1, sizeof(slab_automove));\n    if (a == NULL)\n        return NULL;\n    a->window_data = calloc(window_size * MAX_NUMBER_OF_SLAB_CLASSES, sizeof(struct window_data));\n    a->window_size = window_size;\n    a->max_age_ratio = max_age_ratio;\n    a->free_ratio = settings->slab_automove_freeratio;\n    a->item_size = settings->ext_item_size;\n    a->settings = settings;\n    a->pool_filled_once = false;\n    if (a->window_data == NULL) {\n        if (a->window_data)\n            free(a->window_data);\n        free(a);\n        return NULL;\n    }\n\n    // do a dry run to fill the before structs\n    fill_item_stats_automove(a->iam_before);\n    fill_slab_stats_automove(a->sam_before);\n\n    return (void *)a;\n}\n\nvoid slab_automove_extstore_free(void *arg) {\n    slab_automove *a = (slab_automove *)arg;\n    free(a->window_data);\n    free(a);\n}\n\nstatic void window_sum(struct window_data *wd, struct window_data *w,\n        uint32_t size) {\n    for (int x = 0; x < size; x++) {\n        struct window_data *d = &wd[x];\n        w->age += d->age;\n        w->dirty += d->dirty;\n        w->evicted += d->evicted;\n        w->excess_free += d->excess_free;\n        w->relaxed += d->relaxed;\n    }\n}\n\nstatic int global_pool_check(slab_automove *a, unsigned int *count) {\n    bool mem_limit_reached;\n    unsigned int free = a->global_pool_watermark;\n    *count = global_page_pool_size(&mem_limit_reached);\n    if (!mem_limit_reached)\n        return 0;\n    if (*count < free) {\n        a->pool_filled_once = true;\n        return 1;\n    } else {\n        a->pool_filled_once = true;\n    }\n    return 0;\n}\n\n/* A percentage of memory is configured to be held \"free\" as buffers for the\n * external storage system.\n * % of global memory is desired in the global page pool\n * each slab class has a % of free chunks desired based on how much memory is\n * currently in the class. This allows time for extstore to flush data when\n * spikes or waves of set data arrive.\n * The global page pool reserve acts as a secondary buffer for any slab class,\n * which helps absorb shifts in which class is active.\n */\nstatic void memcheck(slab_automove *a) {\n    unsigned int total_pages = 0;\n\n    // FIXME: is there a cached counter for total pages alloced?\n    // technically we only really need to do this once as the pages are\n    // prefilled and ratio isn't a runtime change.\n    for (int n = 1; n < MAX_NUMBER_OF_SLAB_CLASSES; n++) {\n        slab_stats_automove *sam = &a->sam_after[n];\n        total_pages += sam->total_pages;\n    }\n    // always update what remains in the global page pool\n    total_pages += a->sam_after[0].total_pages;\n    a->global_pool_watermark = total_pages * a->free_ratio;\n    if (a->global_pool_watermark < 2)\n        a->global_pool_watermark = 2;\n    settings.ext_global_pool_min = a->global_pool_watermark;\n}\n\nstatic struct window_data *get_window_data(slab_automove *a, int class) {\n    int w_offset = class * a->window_size;\n    return &a->window_data[w_offset + (a->window_cur % a->window_size)];\n}\n\nvoid slab_automove_extstore_run(void *arg, int *src, int *dst) {\n    slab_automove *a = (slab_automove *)arg;\n    int n;\n    struct window_data w_sum;\n    int oldest = -1;\n    uint64_t oldest_age = 0;\n    bool too_free = false;\n    *src = -1;\n    *dst = -1;\n\n    // calculate how much memory pressure extstore is under.\n    // 100% means we need to evict item headers.\n    unsigned int total_low_pages = 0;\n    unsigned int total_high_pages = 0;\n\n    unsigned int global_count = 0;\n    int global_low = global_pool_check(a, &global_count);\n    // fill after structs\n    fill_item_stats_automove(a->iam_after);\n    fill_slab_stats_automove(a->sam_after);\n    a->window_cur++;\n\n    memcheck(a);\n\n    // iterate slabs\n    for (n = POWER_SMALLEST; n < MAX_NUMBER_OF_SLAB_CLASSES; n++) {\n        bool small_slab = a->sam_before[n].chunk_size < a->item_size\n            ? true : false;\n        struct window_data *wd = get_window_data(a, n);\n        int w_offset = n * a->window_size;\n        memset(wd, 0, sizeof(struct window_data));\n        unsigned int free_target = a->sam_after[n].chunks_per_page * MIN_PAGES_FOR_SOURCE;\n\n        if (small_slab) {\n            total_low_pages += a->sam_after[n].total_pages;\n        } else {\n            unsigned int pages = a->sam_after[n].total_pages;\n            // only include potentially movable pages\n            if (pages > MIN_PAGES_FOR_SOURCE) {\n                total_high_pages += a->sam_after[n].total_pages;\n            }\n        }\n\n        // if page delta, oom, or evicted delta, mark window dirty\n        // classes marked dirty cannot donate memory back to global pool.\n        if (small_slab) {\n            if (a->iam_after[n].evicted - a->iam_before[n].evicted > 0 ||\n                a->iam_after[n].outofmemory - a->iam_before[n].outofmemory > 0) {\n                wd->evicted = 1;\n                wd->dirty = 1;\n            }\n            if (a->sam_after[n].total_pages - a->sam_before[n].total_pages > 0) {\n                wd->dirty = 1;\n            }\n        }\n\n        // reclaim excessively free memory to global after a full window\n        if (a->sam_after[n].free_chunks > free_target) {\n            wd->excess_free = 1;\n        }\n\n        // set age into window\n        wd->age = a->iam_after[n].age;\n\n        // summarize the window-up-to-now.\n        memset(&w_sum, 0, sizeof(struct window_data));\n        window_sum(&a->window_data[w_offset], &w_sum, a->window_size);\n\n        // If global page pool is nearly empty we need to force a move\n        // from any possible source. Otherwise avoid moving from this class if\n        // it appears dirty.\n        if (w_sum.dirty != 0 && global_count != 0) {\n            continue;\n        }\n\n        // if > N free chunks, reclaim memory\n        // small slab classes aren't age balanced and rely more on global\n        if (w_sum.excess_free >= a->window_size) {\n            *src = n;\n            *dst = 0;\n            too_free = true;\n        }\n\n        // large slabs should push to extstore if we try to evict from them.\n        // so we can be aggressive there if the global pool is low.\n        if (!small_slab) {\n            // grab age as average of window total\n            uint64_t age = w_sum.age / a->window_size;\n            // if oldest and have enough pages, is oldest\n            if (age > oldest_age\n                    && a->sam_after[n].total_pages > MIN_PAGES_FOR_SOURCE) {\n                oldest = n;\n                oldest_age = age;\n            }\n        }\n    }\n\n    // update the pressure calculation.\n    float total_pages = total_low_pages + total_high_pages + global_count;\n    float memory_pressure = (total_low_pages / total_pages) * 100;\n    STATS_LOCK();\n    stats_state.extstore_memory_pressure = memory_pressure;\n    STATS_UNLOCK();\n\n    memcpy(a->iam_before, a->iam_after,\n            sizeof(item_stats_automove) * MAX_NUMBER_OF_SLAB_CLASSES);\n    memcpy(a->sam_before, a->sam_after,\n            sizeof(slab_stats_automove) * MAX_NUMBER_OF_SLAB_CLASSES);\n    // only make decisions if window has filled once.\n    if (a->window_cur < a->window_size)\n        return;\n\n    if (!too_free && global_low && oldest != -1) {\n        *src = oldest;\n        *dst = 0;\n    }\n    return;\n}\n"
        },
        {
          "name": "slab_automove_extstore.h",
          "type": "blob",
          "size": 0.240234375,
          "content": "#ifndef SLAB_AUTOMOVE_EXTSTORE_H\n#define SLAB_AUTOMOVE_EXTSTORE_H\n\nvoid *slab_automove_extstore_init(struct settings *settings);\nvoid slab_automove_extstore_free(void *arg);\nvoid slab_automove_extstore_run(void *arg, int *src, int *dst);\n\n#endif\n"
        },
        {
          "name": "slabs.c",
          "type": "blob",
          "size": 25.4140625,
          "content": "/* -*- Mode: C; tab-width: 4; c-basic-offset: 4; indent-tabs-mode: nil -*- */\n/*\n * Slabs memory allocation, based on powers-of-N. Slabs are up to 1MB in size\n * and are divided into chunks. The chunk sizes start off at the size of the\n * \"item\" structure plus space for a small key and value. They increase by\n * a multiplier factor from there, up to half the maximum slab size.\n */\n#include \"memcached.h\"\n#include <sys/mman.h>\n#include <sys/stat.h>\n#include <sys/socket.h>\n#include <sys/resource.h>\n#include <fcntl.h>\n#include <netinet/in.h>\n#include <stdlib.h>\n#include <stdio.h>\n#include <string.h>\n#include <assert.h>\n#include <pthread.h>\n\n//#define DEBUG_SLAB_MOVER\n/* powers-of-N allocation structures */\n\ntypedef struct {\n    uint32_t size;      /* sizes of items */\n    uint32_t perslab;   /* how many items per slab */\n\n    void *slots;           /* list of item ptrs */\n    unsigned int sl_curr;   /* total free items in list */\n\n    unsigned int slabs;     /* how many slabs were allocated for this class */\n\n    void **slab_list;       /* array of slab pointers */\n    unsigned int list_size; /* size of prev array */\n} slabclass_t;\n\nstatic slabclass_t slabclass[MAX_NUMBER_OF_SLAB_CLASSES];\nstatic size_t mem_limit = 0;\nstatic size_t mem_malloced = 0;\n/* If the memory limit has been hit once. Used as a hint to decide when to\n * early-wake the LRU maintenance thread */\nstatic bool mem_limit_reached = false;\nstatic int power_largest;\n\nstatic void *mem_base = NULL;\nstatic void *mem_current = NULL;\nstatic size_t mem_avail = 0;\n/**\n * Access to the slab allocator is protected by this lock\n */\nstatic pthread_mutex_t slabs_lock = PTHREAD_MUTEX_INITIALIZER;\n\n/*\n * Forward Declarations\n */\nstatic int do_grow_slab_list(const unsigned int id);\nstatic int do_slabs_newslab(const unsigned int id);\nstatic void *memory_allocate(size_t size);\nstatic void do_slabs_free(void *ptr, unsigned int id);\n\n/* Preallocate as many slab pages as possible (called from slabs_init)\n   on start-up, so users don't get confused out-of-memory errors when\n   they do have free (in-slab) space, but no space to make new slabs.\n   if maxslabs is 18 (POWER_LARGEST - POWER_SMALLEST + 1), then all\n   slab types can be made.  if max memory is less than 18 MB, only the\n   smaller ones will be made.  */\nstatic void slabs_preallocate (const unsigned int maxslabs);\n\n/*\n * Figures out which slab class (chunk size) is required to store an item of\n * a given size.\n *\n * Given object size, return id to use when allocating/freeing memory for object\n * 0 means error: can't store such a large object\n */\n\nunsigned int slabs_clsid(const size_t size) {\n    int res = POWER_SMALLEST;\n\n    if (size == 0 || size > settings.item_size_max)\n        return 0;\n    while (size > slabclass[res].size)\n        if (res++ == power_largest)     /* won't fit in the biggest slab */\n            return power_largest;\n    return res;\n}\n\nunsigned int slabs_size(const int clsid) {\n    return slabclass[clsid].size;\n}\n\n// TODO: could this work with the restartable memory?\n// Docs say hugepages only work with private shm allocs.\n/* Function split out for better error path handling */\nstatic void * alloc_large_chunk(const size_t limit)\n{\n    void *ptr = NULL;\n#if defined(__linux__) && defined(MADV_HUGEPAGE)\n    size_t pagesize = 0;\n    FILE *fp;\n    int ret;\n\n    /* Get the size of huge pages */\n    fp = fopen(\"/proc/meminfo\", \"r\");\n    if (fp != NULL) {\n        char buf[64];\n\n        while ((fgets(buf, sizeof(buf), fp)))\n            if (!strncmp(buf, \"Hugepagesize:\", 13)) {\n                ret = sscanf(buf + 13, \"%zu\\n\", &pagesize);\n\n                /* meminfo huge page size is in KiBs */\n                pagesize <<= 10;\n            }\n        fclose(fp);\n    }\n\n    if (!pagesize) {\n        fprintf(stderr, \"Failed to get supported huge page size\\n\");\n        return NULL;\n    }\n\n    if (settings.verbose > 1)\n        fprintf(stderr, \"huge page size: %zu\\n\", pagesize);\n\n    /* This works because glibc simply uses mmap when the alignment is\n     * above a certain limit. */\n    ret = posix_memalign(&ptr, pagesize, limit);\n    if (ret != 0) {\n        fprintf(stderr, \"Failed to get aligned memory chunk: %d\\n\", ret);\n        return NULL;\n    }\n\n    ret = madvise(ptr, limit, MADV_HUGEPAGE);\n    if (ret < 0) {\n        fprintf(stderr, \"Failed to set transparent hugepage hint: %d\\n\", ret);\n        free(ptr);\n        ptr = NULL;\n    }\n#elif defined(__FreeBSD__)\n    size_t align = (sizeof(size_t) * 8 - (__builtin_clzl(4095)));\n    ptr = mmap(NULL, limit, PROT_READ | PROT_WRITE, MAP_SHARED | MAP_ANON | MAP_ALIGNED(align) | MAP_ALIGNED_SUPER, -1, 0);\n    if (ptr == MAP_FAILED) {\n        fprintf(stderr, \"Failed to set super pages\\n\");\n        ptr = NULL;\n    }\n#else\n    ptr = malloc(limit);\n#endif\n    return ptr;\n}\n\nunsigned int slabs_fixup(char *chunk, const int border) {\n    slabclass_t *p;\n    item *it = (item *)chunk;\n    int id = ITEM_clsid(it);\n\n    // memory isn't used yet. shunt to global pool.\n    // (which must be 0)\n    if (id == 0) {\n        //assert(border == 0);\n        p = &slabclass[0];\n        do_grow_slab_list(0);\n        p->slab_list[p->slabs++] = (char*)chunk;\n        return -1;\n    }\n    p = &slabclass[id];\n\n    // if we're on a page border, add the slab to slab class\n    if (border == 0) {\n        do_grow_slab_list(id);\n        p->slab_list[p->slabs++] = chunk;\n    }\n\n    // increase free count if ITEM_SLABBED\n    if (it->it_flags == ITEM_SLABBED) {\n        // if ITEM_SLABBED re-stack on freelist.\n        // don't have to run pointer fixups.\n        it->prev = 0;\n        it->next = p->slots;\n        if (it->next) it->next->prev = it;\n        p->slots = it;\n\n        p->sl_curr++;\n        //fprintf(stderr, \"replacing into freelist\\n\");\n    }\n\n    return p->size;\n}\n\n/**\n * Determines the chunk sizes and initializes the slab class descriptors\n * accordingly.\n */\nvoid slabs_init(const size_t limit, const double factor, const bool prealloc, const uint32_t *slab_sizes, void *mem_base_external, bool reuse_mem) {\n    int i = POWER_SMALLEST - 1;\n    unsigned int size = sizeof(item) + settings.chunk_size;\n\n    /* Some platforms use runtime transparent hugepages. If for any reason\n     * the initial allocation fails, the required settings do not persist\n     * for remaining allocations. As such it makes little sense to do slab\n     * preallocation. */\n    bool __attribute__ ((unused)) do_slab_prealloc = false;\n\n    mem_limit = limit;\n\n    if (prealloc && mem_base_external == NULL) {\n        mem_base = alloc_large_chunk(mem_limit);\n        if (mem_base) {\n            do_slab_prealloc = true;\n            mem_current = mem_base;\n            mem_avail = mem_limit;\n        } else {\n            fprintf(stderr, \"Warning: Failed to allocate requested memory in\"\n                    \" one large chunk.\\nWill allocate in smaller chunks\\n\");\n        }\n    } else if (prealloc && mem_base_external != NULL) {\n        // Can't (yet) mix hugepages with mmap allocations, so separate the\n        // logic from above. Reusable memory also force-preallocates memory\n        // pages into the global pool, which requires turning mem_* variables.\n        do_slab_prealloc = true;\n        mem_base = mem_base_external;\n        // _current shouldn't be used in this case, but we set it to where it\n        // should be anyway.\n        if (reuse_mem) {\n            mem_current = ((char*)mem_base) + mem_limit;\n            mem_avail = 0;\n        } else {\n            mem_current = mem_base;\n            mem_avail = mem_limit;\n        }\n    }\n\n    memset(slabclass, 0, sizeof(slabclass));\n\n    while (++i < MAX_NUMBER_OF_SLAB_CLASSES-1) {\n        if (slab_sizes != NULL) {\n            if (slab_sizes[i-1] == 0)\n                break;\n            size = slab_sizes[i-1];\n        } else if (size >= settings.slab_chunk_size_max / factor) {\n            break;\n        }\n        /* Make sure items are always n-byte aligned */\n        if (size % CHUNK_ALIGN_BYTES)\n            size += CHUNK_ALIGN_BYTES - (size % CHUNK_ALIGN_BYTES);\n\n        slabclass[i].size = size;\n        slabclass[i].perslab = settings.slab_page_size / slabclass[i].size;\n        if (slab_sizes == NULL)\n            size *= factor;\n        if (settings.verbose > 1) {\n            fprintf(stderr, \"slab class %3d: chunk size %9u perslab %7u\\n\",\n                    i, slabclass[i].size, slabclass[i].perslab);\n        }\n    }\n\n    power_largest = i;\n    slabclass[power_largest].size = settings.slab_chunk_size_max;\n    slabclass[power_largest].perslab = settings.slab_page_size / settings.slab_chunk_size_max;\n    if (settings.verbose > 1) {\n        fprintf(stderr, \"slab class %3d: chunk size %9u perslab %7u\\n\",\n                i, slabclass[i].size, slabclass[i].perslab);\n    }\n\n    /* for the test suite:  faking of how much we've already malloc'd */\n    {\n        char *t_initial_malloc = getenv(\"T_MEMD_INITIAL_MALLOC\");\n        if (t_initial_malloc) {\n            int64_t env_malloced;\n            if (safe_strtoll((const char *)t_initial_malloc, &env_malloced)) {\n                mem_malloced = (size_t)env_malloced;\n            }\n        }\n\n    }\n\n    if (do_slab_prealloc) {\n        if (!reuse_mem) {\n            slabs_preallocate(power_largest);\n        }\n    }\n}\n\nvoid slabs_prefill_global(void) {\n    void *ptr;\n    slabclass_t *p = &slabclass[0];\n    int len = settings.slab_page_size;\n\n    while (mem_malloced < mem_limit\n            && (ptr = memory_allocate(len)) != NULL) {\n        do_grow_slab_list(0);\n        // Ensure the front header is zero'd to avoid confusing restart code.\n        // It's probably good enough to cast it and just zero slabs_clsid, but\n        // this is extra paranoid.\n        memset(ptr, 0, sizeof(item));\n        p->slab_list[p->slabs++] = ptr;\n    }\n    mem_limit_reached = true;\n}\n\nstatic void slabs_preallocate(const unsigned int maxslabs) {\n    int i;\n    unsigned int prealloc = 0;\n\n    /* pre-allocate a 1MB slab in every size class so people don't get\n       confused by non-intuitive \"SERVER_ERROR out of memory\"\n       messages.  this is the most common question on the mailing\n       list.  if you really don't want this, you can rebuild without\n       these three lines.  */\n\n    for (i = POWER_SMALLEST; i < MAX_NUMBER_OF_SLAB_CLASSES; i++) {\n        if (++prealloc > maxslabs)\n            break;\n        if (do_slabs_newslab(i) == 0) {\n            fprintf(stderr, \"Error while preallocating slab memory!\\n\"\n                \"If using -L or other prealloc options, max memory must be \"\n                \"at least %d megabytes.\\n\", power_largest);\n            exit(1);\n        }\n    }\n}\n\nstatic int do_grow_slab_list(const unsigned int id) {\n    if (id > power_largest)\n        return 0;\n\n    slabclass_t *p = &slabclass[id];\n    if (p->slabs == p->list_size) {\n        size_t new_size =  (p->list_size != 0) ? p->list_size * 2 : 16;\n        void *new_list = realloc(p->slab_list, new_size * sizeof(void *));\n        if (new_list == 0) return 0;\n        p->list_size = new_size;\n        p->slab_list = new_list;\n    }\n    return 1;\n}\n\nint slabs_grow_slab_list(const unsigned int id) {\n    int ret = 0;\n    pthread_mutex_lock(&slabs_lock);\n    ret = do_grow_slab_list(id);\n    pthread_mutex_unlock(&slabs_lock);\n    return ret;\n}\n\nstatic void split_slab_page_into_freelist(char *ptr, const unsigned int id) {\n    slabclass_t *p = &slabclass[id];\n    int x;\n    for (x = 0; x < p->perslab; x++) {\n        do_slabs_free(ptr, id);\n        ptr += p->size;\n    }\n}\n\n/* Fast FIFO queue */\nstatic void *get_page_from_global_pool(void) {\n    slabclass_t *p = &slabclass[SLAB_GLOBAL_PAGE_POOL];\n    if (p->slabs < 1) {\n        return NULL;\n    }\n    char *ret = p->slab_list[p->slabs - 1];\n    p->slabs--;\n    return ret;\n}\n\nstatic int do_slabs_newslab(const unsigned int id) {\n    slabclass_t *p = &slabclass[id];\n    slabclass_t *g = &slabclass[SLAB_GLOBAL_PAGE_POOL];\n    int len = (settings.slab_reassign || settings.slab_chunk_size_max != settings.slab_page_size)\n        ? settings.slab_page_size\n        : p->size * p->perslab;\n    char *ptr;\n\n    if ((mem_limit && mem_malloced + len > mem_limit && p->slabs > 0\n         && g->slabs == 0)) {\n        mem_limit_reached = true;\n        MEMCACHED_SLABS_SLABCLASS_ALLOCATE_FAILED(id);\n        return 0;\n    }\n\n    if ((do_grow_slab_list(id) == 0) ||\n        (((ptr = get_page_from_global_pool()) == NULL) &&\n        ((ptr = memory_allocate((size_t)len)) == 0))) {\n\n        MEMCACHED_SLABS_SLABCLASS_ALLOCATE_FAILED(id);\n        return 0;\n    }\n\n    // Always wipe the memory at this stage: in restart mode the mmap memory\n    // could be unused, yet still full of data. Better for usability if we're\n    // wiping memory as it's being pulled out of the global pool instead of\n    // blocking startup all at once.\n    memset(ptr, 0, (size_t)len);\n    split_slab_page_into_freelist(ptr, id);\n\n    p->slab_list[p->slabs++] = ptr;\n    MEMCACHED_SLABS_SLABCLASS_ALLOCATE(id);\n\n    return 1;\n}\n\n/*@null@*/\nstatic void *do_slabs_alloc(unsigned int id,\n        unsigned int flags) {\n    slabclass_t *p;\n    void *ret = NULL;\n    item *it = NULL;\n\n    if (id < POWER_SMALLEST || id > power_largest) {\n        MEMCACHED_SLABS_ALLOCATE_FAILED(id);\n        return NULL;\n    }\n    p = &slabclass[id];\n    assert(p->sl_curr == 0 || (((item *)p->slots)->it_flags & ITEM_SLABBED));\n\n    /* fail unless we have space at the end of a recently allocated page,\n       we have something on our freelist, or we could allocate a new page */\n    if (p->sl_curr == 0 && flags != SLABS_ALLOC_NO_NEWPAGE) {\n        do_slabs_newslab(id);\n    }\n\n    if (p->sl_curr != 0) {\n        /* return off our freelist */\n        it = (item *)p->slots;\n        p->slots = it->next;\n        if (it->next) it->next->prev = 0;\n        /* Kill flag and initialize refcount here for lock safety in slab\n         * mover's freeness detection. */\n        it->it_flags &= ~ITEM_SLABBED;\n        it->refcount = 1;\n        p->sl_curr--;\n        ret = (void *)it;\n    } else {\n        ret = NULL;\n    }\n\n    if (ret) {\n        MEMCACHED_SLABS_ALLOCATE(id, p->size, ret);\n    } else {\n        MEMCACHED_SLABS_ALLOCATE_FAILED(id);\n    }\n\n    return ret;\n}\n\nstatic void do_slabs_free_chunked(item *it) {\n    item_chunk *chunk = (item_chunk *) ITEM_schunk(it);\n    slabclass_t *p;\n\n    it->it_flags = ITEM_SLABBED;\n    // FIXME: refresh on how this works?\n    //it->slabs_clsid = 0;\n    it->prev = 0;\n    // header object's original classid is stored in chunk.\n    p = &slabclass[chunk->orig_clsid];\n    // original class id needs to be set on free memory.\n    it->slabs_clsid = chunk->orig_clsid;\n    if (chunk->next) {\n        chunk = chunk->next;\n        chunk->prev = 0;\n    } else {\n        // header with no attached chunk\n        chunk = NULL;\n    }\n\n    // return the header object.\n    // TODO: This is in three places, here and in do_slabs_free().\n    it->prev = 0;\n    it->next = p->slots;\n    if (it->next) it->next->prev = it;\n    p->slots = it;\n    p->sl_curr++;\n\n    item_chunk *next_chunk;\n    while (chunk) {\n        assert(chunk->it_flags == ITEM_CHUNK);\n        chunk->it_flags = ITEM_SLABBED;\n        p = &slabclass[chunk->slabs_clsid];\n        next_chunk = chunk->next;\n\n        chunk->prev = 0;\n        chunk->next = p->slots;\n        if (chunk->next) chunk->next->prev = chunk;\n        p->slots = chunk;\n        p->sl_curr++;\n\n        chunk = next_chunk;\n    }\n\n    return;\n}\n\nstatic void do_slabs_free(void *ptr, unsigned int id) {\n    slabclass_t *p;\n    item *it;\n\n    assert(id >= POWER_SMALLEST && id <= power_largest);\n    if (id < POWER_SMALLEST || id > power_largest)\n        return;\n\n    MEMCACHED_SLABS_FREE(id, ptr);\n    p = &slabclass[id];\n\n    it = (item *)ptr;\n    if ((it->it_flags & ITEM_CHUNKED) == 0) {\n        it->it_flags = ITEM_SLABBED;\n        it->slabs_clsid = id;\n        it->prev = 0;\n        it->next = p->slots;\n        if (it->next) it->next->prev = it;\n        p->slots = it;\n\n        p->sl_curr++;\n    } else {\n        do_slabs_free_chunked(it);\n    }\n    return;\n}\n\n/* With refactoring of the various stats code the automover won't need a\n * custom function here.\n */\nvoid fill_slab_stats_automove(slab_stats_automove *am) {\n    int n;\n    pthread_mutex_lock(&slabs_lock);\n    for (n = 0; n < MAX_NUMBER_OF_SLAB_CLASSES; n++) {\n        slabclass_t *p = &slabclass[n];\n        slab_stats_automove *cur = &am[n];\n        cur->chunks_per_page = p->perslab;\n        cur->free_chunks = p->sl_curr;\n        cur->total_pages = p->slabs;\n        cur->chunk_size = p->size;\n    }\n    pthread_mutex_unlock(&slabs_lock);\n}\n\n/* TODO: slabs_available_chunks should grow up to encompass this.\n * mem_flag is redundant with the other function.\n */\nunsigned int global_page_pool_size(bool *mem_flag) {\n    unsigned int ret = 0;\n    pthread_mutex_lock(&slabs_lock);\n    if (mem_flag != NULL)\n        *mem_flag = mem_malloced >= mem_limit ? true : false;\n    ret = slabclass[SLAB_GLOBAL_PAGE_POOL].slabs;\n    pthread_mutex_unlock(&slabs_lock);\n    return ret;\n}\n\n/*@null@*/\nstatic void do_slabs_stats(ADD_STAT add_stats, void *c) {\n    int i, total;\n    /* Get the per-thread stats which contain some interesting aggregates */\n    struct thread_stats thread_stats;\n    threadlocal_stats_aggregate(&thread_stats);\n\n    total = 0;\n    for(i = POWER_SMALLEST; i <= power_largest; i++) {\n        slabclass_t *p = &slabclass[i];\n        if (p->slabs != 0) {\n            uint32_t perslab, slabs;\n            slabs = p->slabs;\n            perslab = p->perslab;\n\n            char key_str[STAT_KEY_LEN];\n            char val_str[STAT_VAL_LEN];\n            int klen = 0, vlen = 0;\n\n            APPEND_NUM_STAT(i, \"chunk_size\", \"%u\", p->size);\n            APPEND_NUM_STAT(i, \"chunks_per_page\", \"%u\", perslab);\n            APPEND_NUM_STAT(i, \"total_pages\", \"%u\", slabs);\n            APPEND_NUM_STAT(i, \"total_chunks\", \"%u\", slabs * perslab);\n            APPEND_NUM_STAT(i, \"used_chunks\", \"%u\",\n                            slabs*perslab - p->sl_curr);\n            APPEND_NUM_STAT(i, \"free_chunks\", \"%u\", p->sl_curr);\n            /* Stat is dead, but displaying zero instead of removing it. */\n            APPEND_NUM_STAT(i, \"free_chunks_end\", \"%u\", 0);\n            APPEND_NUM_STAT(i, \"get_hits\", \"%llu\",\n                    (unsigned long long)thread_stats.slab_stats[i].get_hits);\n            APPEND_NUM_STAT(i, \"cmd_set\", \"%llu\",\n                    (unsigned long long)thread_stats.slab_stats[i].set_cmds);\n            APPEND_NUM_STAT(i, \"delete_hits\", \"%llu\",\n                    (unsigned long long)thread_stats.slab_stats[i].delete_hits);\n            APPEND_NUM_STAT(i, \"incr_hits\", \"%llu\",\n                    (unsigned long long)thread_stats.slab_stats[i].incr_hits);\n            APPEND_NUM_STAT(i, \"decr_hits\", \"%llu\",\n                    (unsigned long long)thread_stats.slab_stats[i].decr_hits);\n            APPEND_NUM_STAT(i, \"cas_hits\", \"%llu\",\n                    (unsigned long long)thread_stats.slab_stats[i].cas_hits);\n            APPEND_NUM_STAT(i, \"cas_badval\", \"%llu\",\n                    (unsigned long long)thread_stats.slab_stats[i].cas_badval);\n            APPEND_NUM_STAT(i, \"touch_hits\", \"%llu\",\n                    (unsigned long long)thread_stats.slab_stats[i].touch_hits);\n            total++;\n        }\n    }\n\n    /* add overall slab stats and append terminator */\n\n    APPEND_STAT(\"active_slabs\", \"%d\", total);\n    APPEND_STAT(\"total_malloced\", \"%llu\", (unsigned long long)mem_malloced);\n    add_stats(NULL, 0, NULL, 0, c);\n}\n\nstatic void *memory_allocate(size_t size) {\n    void *ret;\n\n    if (mem_base == NULL) {\n        /* We are not using a preallocated large memory chunk */\n        ret = malloc(size);\n    } else {\n        ret = mem_current;\n\n        if (size > mem_avail) {\n            return NULL;\n        }\n\n        /* mem_current pointer _must_ be aligned!!! */\n        if (size % CHUNK_ALIGN_BYTES) {\n            size += CHUNK_ALIGN_BYTES - (size % CHUNK_ALIGN_BYTES);\n        }\n\n        mem_current = ((char*)mem_current) + size;\n        if (size < mem_avail) {\n            mem_avail -= size;\n        } else {\n            mem_avail = 0;\n        }\n    }\n    mem_malloced += size;\n\n    return ret;\n}\n\n/* Must only be used if all pages are item_size_max */\nstatic void memory_release(void) {\n    void *p = NULL;\n    if (mem_base != NULL)\n        return;\n\n    if (!settings.slab_reassign)\n        return;\n\n    while (mem_malloced > mem_limit &&\n            (p = get_page_from_global_pool()) != NULL) {\n        free(p);\n        mem_malloced -= settings.slab_page_size;\n    }\n}\n\nvoid *slabs_alloc(unsigned int id, unsigned int flags) {\n    void *ret;\n\n    pthread_mutex_lock(&slabs_lock);\n    ret = do_slabs_alloc(id, flags);\n    pthread_mutex_unlock(&slabs_lock);\n    return ret;\n}\n\nvoid slabs_free(void *ptr, unsigned int id) {\n    pthread_mutex_lock(&slabs_lock);\n    do_slabs_free(ptr, id);\n    pthread_mutex_unlock(&slabs_lock);\n}\n\nvoid slabs_stats(ADD_STAT add_stats, void *c) {\n    pthread_mutex_lock(&slabs_lock);\n    do_slabs_stats(add_stats, c);\n    pthread_mutex_unlock(&slabs_lock);\n}\n\nstatic bool do_slabs_adjust_mem_limit(size_t new_mem_limit) {\n    /* Cannot adjust memory limit at runtime if prealloc'ed */\n    if (mem_base != NULL)\n        return false;\n    settings.maxbytes = new_mem_limit;\n    mem_limit = new_mem_limit;\n    mem_limit_reached = false; /* Will reset on next alloc */\n    memory_release(); /* free what might already be in the global pool */\n    return true;\n}\n\nbool slabs_adjust_mem_limit(size_t new_mem_limit) {\n    bool ret;\n    pthread_mutex_lock(&slabs_lock);\n    ret = do_slabs_adjust_mem_limit(new_mem_limit);\n    pthread_mutex_unlock(&slabs_lock);\n    return ret;\n}\n\nunsigned int slabs_available_chunks(const unsigned int id, bool *mem_flag,\n        unsigned int *chunks_perslab) {\n    unsigned int ret;\n    slabclass_t *p;\n\n    pthread_mutex_lock(&slabs_lock);\n    p = &slabclass[id];\n    ret = p->sl_curr;\n    if (mem_flag != NULL)\n        *mem_flag = mem_malloced >= mem_limit ? true : false;\n    if (chunks_perslab != NULL)\n        *chunks_perslab = p->perslab;\n    pthread_mutex_unlock(&slabs_lock);\n    return ret;\n}\n\nvoid *slabs_peek_page(const unsigned int id, uint32_t *size, uint32_t *perslab) {\n    slabclass_t *s_cls;\n    void *page = NULL;\n    if (id > power_largest) {\n        return NULL;\n    }\n    pthread_mutex_lock(&slabs_lock);\n    s_cls = &slabclass[id];\n    if (s_cls->slabs < 2) {\n        pthread_mutex_unlock(&slabs_lock);\n        return NULL;\n    }\n    *size = s_cls->size;\n    *perslab = s_cls->perslab;\n\n    page = s_cls->slab_list[0];\n\n    pthread_mutex_unlock(&slabs_lock);\n\n    return page;\n}\n\n/* detaches item/chunk from freelist.\n * for use with page mover.\n * lock _must_ be held.\n */\nvoid do_slabs_unlink_free_chunk(const unsigned int id, item *it) {\n    slabclass_t *s_cls = &slabclass[id];\n    /* Ensure this was on the freelist and nothing else. */\n    assert(it->it_flags == ITEM_SLABBED);\n    if (s_cls->slots == it) {\n        s_cls->slots = it->next;\n    }\n    if (it->next) it->next->prev = it->prev;\n    if (it->prev) it->prev->next = it->next;\n    s_cls->sl_curr--;\n}\n\nvoid slabs_finalize_page_move(const unsigned int sid, const unsigned int did, void *page) {\n    pthread_mutex_lock(&slabs_lock);\n    slabclass_t *s_cls = &slabclass[sid];\n    slabclass_t *d_cls = &slabclass[did];\n\n    s_cls->slabs--;\n    for (int x = 0; x < s_cls->slabs; x++) {\n        s_cls->slab_list[x] = s_cls->slab_list[x+1];\n    }\n\n    // FIXME: it's nearly impossible for this to fail, and error handling here\n    // is gnarly since we'll have to just put the page back where we got it\n    // from.\n    // For now we won't handle the error, and a subsequent commit should\n    // remove the need to resize the slab list.\n    do_grow_slab_list(did);\n    d_cls->slab_list[d_cls->slabs++] = page;\n    /* Don't need to split the page into chunks if we're just storing it */\n    if (did > SLAB_GLOBAL_PAGE_POOL) {\n        memset(page, 0, (size_t)settings.slab_page_size);\n        split_slab_page_into_freelist(page, did);\n    } else if (did == SLAB_GLOBAL_PAGE_POOL) {\n        /* memset just enough to signal restart handler to skip */\n        memset(page, 0, sizeof(item));\n        /* mem_malloc'ed might be higher than mem_limit. */\n        mem_limit_reached = false;\n        memory_release();\n    }\n\n    pthread_mutex_unlock(&slabs_lock);\n}\n/* Iterate at most once through the slab classes and pick a \"random\" source.\n * I like this better than calling rand() since rand() is slow enough that we\n * can just check all of the classes once instead.\n */\nint slabs_pick_any_for_reassign(const unsigned int did) {\n    pthread_mutex_lock(&slabs_lock);\n    static int cur = POWER_SMALLEST - 1;\n    int tries = MAX_NUMBER_OF_SLAB_CLASSES - POWER_SMALLEST + 1;\n    for (; tries > 0; tries--) {\n        cur++;\n        if (cur > MAX_NUMBER_OF_SLAB_CLASSES)\n            cur = POWER_SMALLEST;\n        if (cur == did)\n            continue;\n        if (slabclass[cur].slabs > 1) {\n            pthread_mutex_unlock(&slabs_lock);\n            return cur;\n        }\n    }\n    pthread_mutex_unlock(&slabs_lock);\n    return -1;\n}\n\nint slabs_page_count(const unsigned int id) {\n    int ret;\n    pthread_mutex_lock(&slabs_lock);\n    ret = slabclass[id].slabs;\n    pthread_mutex_unlock(&slabs_lock);\n    return ret;\n}\n\nint slabs_locked_callback(slabs_cb cb, void *arg) {\n    int ret = 0;\n    pthread_mutex_lock(&slabs_lock);\n    ret = cb(arg);\n    pthread_mutex_unlock(&slabs_lock);\n\n    return ret;\n}\n\n/* The slabber system could avoid needing to understand much, if anything,\n * about items if callbacks were strategically used. Due to how the slab mover\n * works, certain flag bits can only be adjusted while holding the slabs lock.\n * Using these functions, isolate sections of code needing this and turn them\n * into callbacks when an interface becomes more obvious.\n */\nvoid slabs_mlock(void) {\n    pthread_mutex_lock(&slabs_lock);\n}\n\nvoid slabs_munlock(void) {\n    pthread_mutex_unlock(&slabs_lock);\n}\n"
        },
        {
          "name": "slabs.h",
          "type": "blob",
          "size": 2.3603515625,
          "content": "/* slabs memory allocation */\n#ifndef SLABS_H\n#define SLABS_H\n\n/** Init the subsystem. 1st argument is the limit on no. of bytes to allocate,\n    0 if no limit. 2nd argument is the growth factor; each slab will use a chunk\n    size equal to the previous slab's chunk size times this factor.\n    3rd argument specifies if the slab allocator should allocate all memory\n    up front (if true), or allocate memory in chunks as it is needed (if false)\n*/\nvoid slabs_init(const size_t limit, const double factor, const bool prealloc, const uint32_t *slab_sizes, void *mem_base_external, bool reuse_mem);\n\n/** Call only during init. Pre-allocates all available memory */\nvoid slabs_prefill_global(void);\n\n/**\n * Given object size, return id to use when allocating/freeing memory for object\n * 0 means error: can't store such a large object\n */\n\nunsigned int slabs_clsid(const size_t size);\nunsigned int slabs_size(const int clsid);\n\n/** Allocate object of given length. 0 on error */ /*@null@*/\n#define SLABS_ALLOC_NO_NEWPAGE 1\nvoid *slabs_alloc(unsigned int id, unsigned int flags);\n\n/** Free previously allocated object */\nvoid slabs_free(void *ptr, unsigned int id);\n\n/** Adjust global memory limit up or down */\nbool slabs_adjust_mem_limit(size_t new_mem_limit);\n\ntypedef struct {\n    unsigned int chunks_per_page;\n    unsigned int chunk_size;\n    long int free_chunks;\n    long int total_pages;\n} slab_stats_automove;\nvoid fill_slab_stats_automove(slab_stats_automove *am);\nunsigned int global_page_pool_size(bool *mem_flag);\n\n/** Fill buffer with stats */ /*@null@*/\nvoid slabs_stats(ADD_STAT add_stats, void *c);\n\n/* Hints as to freespace in slab class */\nunsigned int slabs_available_chunks(unsigned int id, bool *mem_flag, unsigned int *chunks_perslab);\n\nvoid slabs_mlock(void);\nvoid slabs_munlock(void);\n\n/* utilities for page moving */\nvoid *slabs_peek_page(const unsigned int id, uint32_t *size, uint32_t *perslab);\nvoid do_slabs_unlink_free_chunk(const unsigned int id, item *it);\nvoid slabs_finalize_page_move(const unsigned int sid, const unsigned int did, void *page);\nint slabs_pick_any_for_reassign(const unsigned int did);\nint slabs_page_count(const unsigned int id);\n\ntypedef int (*slabs_cb)(void *arg);\nint slabs_locked_callback(slabs_cb cb, void *arg);\nint slabs_grow_slab_list(const unsigned int id);\n\n/* Fixup for restartable code. */\nunsigned int slabs_fixup(char *chunk, const int border);\n\n#endif\n"
        },
        {
          "name": "slabs_mover.c",
          "type": "blob",
          "size": 26.3271484375,
          "content": "/* -*- Mode: C; tab-width: 4; c-basic-offset: 4; indent-tabs-mode: nil -*- */\n#include \"memcached.h\"\n#include \"slabs_mover.h\"\n#include \"slab_automove.h\"\n#ifdef EXTSTORE\n#include \"slab_automove_extstore.h\"\n#endif\n#include \"storage.h\"\n#include <sys/mman.h>\n#include <sys/stat.h>\n#include <sys/socket.h>\n#include <sys/resource.h>\n#include <fcntl.h>\n#include <netinet/in.h>\n#include <stdlib.h>\n#include <stdio.h>\n#include <string.h>\n#include <assert.h>\n#include <pthread.h>\n\nstruct slab_rebalance {\n    void *slab_start;\n    void *slab_end;\n    void *slab_pos;\n    unsigned int s_clsid;\n    unsigned int d_clsid;\n    uint32_t cls_size;\n    uint32_t busy_items;\n    uint32_t rescues;\n    uint32_t inline_reclaim;\n    uint32_t chunk_rescues;\n    uint32_t busy_nomem;\n    uint32_t busy_deletes;\n    uint32_t busy_loops;\n    uint8_t done;\n    uint8_t *completed;\n};\n\nstruct slab_rebal_thread {\n    bool run_thread;\n    bool allow_evictions; // global pool empty or manual page move\n    void *storage; // extstore instance.\n    item *new_it; // memory for swapping out valid items.\n    // TODO: logger instance.\n    pthread_mutex_t lock;\n    pthread_cond_t cond;\n    pthread_t tid;\n    logger *l;\n    unsigned int am_version; // re-generate am object if version changes\n    struct timespec am_last; // last time automover algo ran\n    slab_automove_reg_t *sam; // active algorithm module\n    void *active_am; // automover memory\n    struct slab_rebalance rebal;\n};\n\nenum move_status {\n    MOVE_PASS=0, MOVE_FROM_SLAB, MOVE_FROM_LRU, MOVE_BUSY,\n    MOVE_BUSY_UPLOADING, MOVE_BUSY_ACTIVE, MOVE_BUSY_FLOATING, MOVE_LOCKED\n};\n\nstatic slab_automove_reg_t slab_automove_default = {\n    .init = slab_automove_init,\n    .free = slab_automove_free,\n    .run = slab_automove_run\n};\n#ifdef EXTSTORE\nstatic slab_automove_reg_t slab_automove_extstore = {\n    .init = slab_automove_extstore_init,\n    .free = slab_automove_extstore_free,\n    .run = slab_automove_extstore_run\n};\n#endif\n\n// the sleep inbetween loops is short, so 1000 loops is < 1s\n#define SLAB_MOVE_MAX_LOOPS 5000\nstatic enum reassign_result_type do_slabs_reassign(struct slab_rebal_thread *t, int src, int dst, int flags);\n\nstatic int slab_rebalance_start(struct slab_rebal_thread *t) {\n    uint32_t size;\n    uint32_t perslab;\n    bool mem_limit_reached;\n\n    if (t->rebal.s_clsid == t->rebal.d_clsid) {\n        return -1;\n    }\n\n    // check once at the start of a page move if the global pool is full.\n    // since we're the only thing that can put memory back into the global\n    // pool, this can't change until we complete.\n    // unless the user changes the memory limit manually, which should be\n    // rare.\n    unsigned int global = global_page_pool_size(&mem_limit_reached);\n    if (mem_limit_reached && global == 0) {\n        t->allow_evictions = true;\n    }\n\n    void *page = slabs_peek_page(t->rebal.s_clsid, &size, &perslab);\n\n    // Bit-vector to keep track of completed chunks\n    t->rebal.completed = (uint8_t*)calloc(perslab,sizeof(uint8_t));\n    if (!t->rebal.completed) {\n        return -1;\n    }\n\n    /* Always kill the first available slab page as it is most likely to\n     * contain the oldest items\n     */\n    t->rebal.slab_start = t->rebal.slab_pos = page;\n    t->rebal.slab_end   = (char *)page + (size * perslab);\n    t->rebal.done       = 0;\n    t->rebal.cls_size   = size;\n    // Don't need to do chunk move work if page is in global pool.\n    if (t->rebal.s_clsid == SLAB_GLOBAL_PAGE_POOL) {\n        t->rebal.done = 1;\n    }\n\n    // FIXME: remove this. query the structure from outside to see if we're\n    // running.\n    STATS_LOCK();\n    stats_state.slab_reassign_running = true;\n    STATS_UNLOCK();\n\n    return 0;\n}\n\nstatic void *slab_rebalance_alloc(struct slab_rebal_thread *t, unsigned int id) {\n    item *new_it = NULL;\n\n    // We will either wipe the whole page if unused, or run out of memory in\n    // the page and return NULL.\n    while (1) {\n        new_it = slabs_alloc(id, SLABS_ALLOC_NO_NEWPAGE);\n        if (new_it == NULL) {\n            break;\n        }\n        /* check that memory isn't within the range to clear */\n        if ((void *)new_it >= t->rebal.slab_start\n            && (void *)new_it < t->rebal.slab_end) {\n            /* Pulled something we intend to free. Mark it as freed since\n             * we've already done the work of unlinking it from the freelist.\n             */\n            new_it->refcount = 0;\n            new_it->it_flags = ITEM_SLABBED|ITEM_FETCHED;\n#ifdef DEBUG_SLAB_MOVER\n            memcpy(ITEM_key(new_it), \"deadbeef\", 8);\n#endif\n            new_it = NULL;\n            t->rebal.inline_reclaim++;\n        } else {\n            break;\n        }\n    }\n    return new_it;\n}\n\n// To call move, we first need a free chunk of memory.\nstatic void slab_rebalance_prep(struct slab_rebal_thread *t) {\n    unsigned int s_clsid = t->rebal.s_clsid;\n    if (t->new_it) {\n        // move didn't use the memory from the last loop.\n        return;\n    }\n\n    t->new_it = slab_rebalance_alloc(t, s_clsid);\n    // we could free the entire page in the above alloc call, but not get any\n    // other memory to work with.\n    // We try to busy-loop the page mover at least a few times in this case,\n    // so it will pick up on all of the memory being freed already.\n    if (t->new_it == NULL && t->allow_evictions) {\n        // global is empty and memory limit is reached. we have to evict\n        // memory to move forward.\n        for (int x = 0; x < 10; x++) {\n            if (lru_pull_tail(s_clsid, COLD_LRU, 0, LRU_PULL_EVICT, 0, NULL) <= 0) {\n                if (settings.lru_segmented) {\n                    lru_pull_tail(s_clsid, HOT_LRU, 0, 0, 0, NULL);\n                }\n            }\n            t->new_it = slab_rebalance_alloc(t, s_clsid);\n            if (t->new_it != NULL) {\n                break;\n            }\n        }\n    }\n}\n\n/* refcount == 0 is safe since nobody can incr while item_lock is held.\n * refcount != 0 is impossible since flags/etc can be modified in other\n * threads. instead, note we found a busy one and bail.\n * NOTE: This is checking it_flags outside of an item lock. I believe this\n * works since it_flags is 8 bits, and we're only ever comparing a single bit\n * regardless. ITEM_SLABBED bit will always be correct since we're holding the\n * lock which modifies that bit. ITEM_LINKED won't exist if we're between an\n * item having ITEM_SLABBED removed, and the key hasn't been added to the item\n * yet. The memory barrier from the slabs lock should order the key write and the\n * flags to the item?\n * If ITEM_LINKED did exist and was just removed, but we still see it, that's\n * still safe since it will have a valid key, which we then lock, and then\n * recheck everything.\n * This may not be safe on all platforms; If not, slabs_alloc() will need to\n * seed the item key while holding slabs_lock.\n */\n\nstruct _locked_st {\n    item *it;\n    item_chunk *ch;\n    void *hold_lock; // held lock from trylock.\n    uint32_t hv;\n    unsigned int s_clsid;\n    unsigned int d_clsid;\n};\n\n// called while slabs lock is held so we can safely inspect a chunk of memory\n// and do an inverted item lock.\nstatic int _slabs_locked_cb(void *arg) {\n    struct _locked_st *a = arg;\n    int status = MOVE_PASS;\n    item *it = a->it;\n\n    if (it->it_flags & ITEM_CHUNK) {\n        /* This chunk is a chained part of a larger item. */\n        a->ch = (item_chunk *) it;\n        /* Instead, we use the head chunk to find the item and effectively\n         * lock the entire structure. If a chunk has ITEM_CHUNK flag, its\n         * head cannot be slabbed, so the normal routine is safe. */\n        it = a->ch->head;\n        assert(it->it_flags & ITEM_CHUNKED);\n    }\n\n    /* ITEM_FETCHED when ITEM_SLABBED is overloaded to mean we've cleared\n     * the chunk for move. Only these two flags should exist.\n     */\n    // TODO: bad failure mode if MOVE_PASS and we decide to skip later\n    // but the item is actually alive for whatever reason.\n    // default to MOVE_BUSY and set MOVE_PASS explicitly if the item is S|F?\n    if (it->it_flags != (ITEM_SLABBED|ITEM_FETCHED)) {\n        int refcount = 0;\n\n        /* ITEM_SLABBED can only be added/removed under the slabs_lock */\n        if (it->it_flags & ITEM_SLABBED) {\n            assert(a->ch == NULL);\n            // must unlink our free item while slab lock held.\n            // since we don't have an item lock to cover us.\n            do_slabs_unlink_free_chunk(a->s_clsid, it);\n            it->it_flags = 0; // fix flags outside of the slab lock.\n            status = MOVE_FROM_SLAB;\n        } else if ((it->it_flags & ITEM_LINKED) != 0) {\n            /* If it doesn't have ITEM_SLABBED, the item could be in any\n             * state on its way to being freed or written to. If no\n             * ITEM_SLABBED, but it's had ITEM_LINKED, it must be active\n             * and have the key written to it already.\n             */\n            a->hv = hash(ITEM_key(it), it->nkey);\n            if ((a->hold_lock = item_trylock(a->hv)) == NULL) {\n                status = MOVE_LOCKED;\n            } else {\n                bool is_linked = (it->it_flags & ITEM_LINKED);\n                refcount = refcount_incr(it);\n                if (refcount == 2) { /* item is linked but not busy */\n                    /* Double check ITEM_LINKED flag here, since we're\n                     * past a memory barrier from the mutex. */\n                    if (is_linked) {\n                        status = MOVE_FROM_LRU;\n                    } else {\n                        /* refcount == 1 + !ITEM_LINKED means the item is being\n                         * uploaded to, or was just unlinked but hasn't been freed\n                         * yet. Let it bleed off on its own and try again later */\n                        status = MOVE_BUSY_UPLOADING;\n                    }\n                } else if (refcount > 2 && is_linked) {\n                    status = MOVE_BUSY_ACTIVE;\n                } else {\n                    status = MOVE_BUSY;\n                }\n            }\n        } else {\n            /* See above comment. No ITEM_SLABBED or ITEM_LINKED. Mark\n             * busy and wait for item to complete its upload. */\n            status = MOVE_BUSY_FLOATING;\n        }\n    }\n\n    return status;\n}\n\nstatic void slab_rebalance_rescue(struct slab_rebal_thread *t, struct _locked_st *a) {\n    int cls_size = t->rebal.cls_size;\n    item *it = a->it;\n    item_chunk *ch = a->ch;\n    item *new_it = t->new_it;\n\n    if (ch == NULL) {\n        assert((new_it->it_flags & ITEM_CHUNKED) == 0);\n        /* if free memory, memcpy. clear prev/next/h_bucket */\n        memcpy(new_it, it, cls_size);\n        new_it->prev = 0;\n        new_it->next = 0;\n        new_it->h_next = 0;\n        /* These are definitely required. else fails assert */\n        new_it->it_flags &= ~ITEM_LINKED;\n        new_it->refcount = 0;\n        do_item_replace(it, new_it, a->hv, ITEM_get_cas(it));\n        /* Need to walk the chunks and repoint head  */\n        if (new_it->it_flags & ITEM_CHUNKED) {\n            item_chunk *fch = (item_chunk *) ITEM_schunk(new_it);\n            fch->next->prev = fch;\n            while (fch) {\n                fch->head = new_it;\n                fch = fch->next;\n            }\n        }\n        it->refcount = 0;\n        it->it_flags = ITEM_SLABBED|ITEM_FETCHED;\n#ifdef DEBUG_SLAB_MOVER\n        memcpy(ITEM_key(it), \"deadbeef\", 8);\n#endif\n        t->rebal.rescues++;\n    } else {\n        item_chunk *nch = (item_chunk *) new_it;\n        /* Chunks always have head chunk (the main it) */\n        ch->prev->next = nch;\n        if (ch->next)\n            ch->next->prev = nch;\n        memcpy(nch, ch, ch->used + sizeof(item_chunk));\n        ch->refcount = 0;\n        ch->it_flags = ITEM_SLABBED|ITEM_FETCHED;\n        t->rebal.chunk_rescues++;\n#ifdef DEBUG_SLAB_MOVER\n        memcpy(ITEM_key((item *)ch), \"deadbeef\", 8);\n#endif\n        refcount_decr(it);\n    }\n\n    // we've used the temporary memory.\n    t->new_it = NULL;\n}\n\n// TODO: in order to rescue active chunked items we need to first do more work\n// on chunked items:\n// - individual chunks need to be refcounted, with refcounts protected by item\n// lock. then they can be swapped out an released on refcount reduction\n// - for chunked item headers I don't know how off-hand.\nstatic int slab_rebalance_active_rescue(struct slab_rebal_thread *t, struct _locked_st *a) {\n    int cls_size = t->rebal.cls_size;\n    item *it = a->it;\n    item_chunk *ch = a->ch;\n    item *new_it = t->new_it;\n\n    // Can only rescue active non-chunked items right now.\n    if (ch == NULL && (it->it_flags & ITEM_CHUNKED) == 0) {\n        memcpy(new_it, it, cls_size);\n        new_it->prev = 0;\n        new_it->next = 0;\n        new_it->h_next = 0;\n\n        new_it->it_flags &= ~ITEM_LINKED;\n        new_it->refcount = 0;\n        do_item_replace(it, new_it, a->hv, ITEM_get_cas(it));\n        t->rebal.rescues++;\n\n        // old it is now unlinked. can't immediately rescue item.\n        t->new_it = NULL;\n        return 0;\n    } else {\n        // else if chunked, check if we've been busy-waiting too long and\n        // delete the item.\n        if (t->rebal.busy_loops > SLAB_MOVE_MAX_LOOPS) {\n            // TODO: add indicator for source of eviction\n            LOGGER_LOG(t->l, LOG_EVICTIONS, LOGGER_EVICTION, it);\n            STORAGE_delete(t->storage, it);\n            do_item_unlink(it, a->hv);\n            t->rebal.busy_deletes++;\n        }\n    }\n\n    // failed to rescue busy item.\n    return 1;\n}\n\n// try to free up a chunk of memory, if not already free.\n// if we have memory available outside of the source page, rescue any valid\n// items.\n// we still attempt to move data even if no memory is available for a rescue,\n// in case the item is already free, expired, busy, etc.\nstatic int slab_rebalance_move(struct slab_rebal_thread *t) {\n    uint32_t was_busy = t->rebal.busy_items;\n    struct _locked_st cbarg;\n    memset(&cbarg, 0, sizeof(cbarg));\n\n    // the offset to check if completed or not\n    int offset = ((char*)t->rebal.slab_pos-(char*)t->rebal.slab_start)/(t->rebal.cls_size);\n\n    // skip acquiring the slabs lock for items we've already fully processed.\n    if (t->rebal.completed[offset] == 0) {\n        item *it;\n        cbarg.it = it = t->rebal.slab_pos;\n        cbarg.s_clsid = t->rebal.s_clsid;\n        cbarg.d_clsid = t->rebal.d_clsid;\n        // it is returned _locked_ if successful. _must_ unlock it!\n        int status = slabs_locked_callback(_slabs_locked_cb, &cbarg);\n\n        item_chunk *ch = cbarg.ch;\n        if (ch) {\n            // swap item under examination to the chunk head if we're\n            // attempting to move a chunk within a larger item.\n            cbarg.it = it = ch->head;\n        }\n        switch (status) {\n            case MOVE_BUSY_ACTIVE:\n            case MOVE_FROM_LRU:\n                /* Lock order is LRU locks -> slabs_lock. unlink uses LRU lock.\n                 * We only need to hold the slabs_lock while initially looking\n                 * at an item, and at this point we have an exclusive refcount\n                 * (2) + the item is locked. Drop slabs lock, drop item to\n                 * refcount 1 (just our own, then fall through and wipe it\n                 */\n                /* Check if expired or flushed */\n                if ((it->exptime != 0 && it->exptime < current_time)\n                    || item_is_flushed(it)) {\n                    /* Expired, don't save. */\n                    STORAGE_delete(t->storage, it);\n                    if (!ch && (it->it_flags & ITEM_CHUNKED) == 0) {\n                        do_item_unlink(it, cbarg.hv);\n                        if (it->refcount == 1) {\n                            it->it_flags = ITEM_SLABBED|ITEM_FETCHED;\n                            it->refcount = 0;\n#ifdef DEBUG_SLAB_MOVER\n                            memcpy(ITEM_key(it), \"deadbeef\", 8);\n#endif\n                            t->rebal.completed[offset] = 1;\n                        } else {\n                            // expired, but busy.\n                            do_item_remove(it);\n                            t->rebal.busy_items++;\n                        }\n                    } else {\n                        // chunked: unlink it and grab on next loop\n                        // same code regardless of refcount\n                        do_item_unlink(it, cbarg.hv);\n                        do_item_remove(it);\n                        t->rebal.busy_items++;\n                    }\n                } else {\n                    // we should try to rescue the item.\n                    if (t->new_it == NULL) {\n                        // we don't actually have memory: need to mark as busy\n                        // and try again in a future loop.\n                        t->rebal.busy_items++;\n                        t->rebal.busy_nomem++;\n                        refcount_decr(it);\n                    } else {\n                        if (it->refcount == 2) {\n                            slab_rebalance_rescue(t, &cbarg);\n                            t->rebal.completed[offset] = 1;\n                        } else {\n                            assert(it->refcount > 2);\n                            // need to wait for ref'ed owners to free *it\n                            slab_rebalance_active_rescue(t, &cbarg);\n                            t->rebal.busy_items++;\n                            do_item_remove(it);\n                        }\n                    }\n                }\n\n                item_trylock_unlock(cbarg.hold_lock);\n                break;\n            case MOVE_FROM_SLAB:\n                it->refcount = 0;\n                it->it_flags = ITEM_SLABBED|ITEM_FETCHED;\n#ifdef DEBUG_SLAB_MOVER\n                memcpy(ITEM_key(it), \"deadbeef\", 8);\n#endif\n                t->rebal.completed[offset] = 1;\n                break;\n            case MOVE_BUSY:\n            case MOVE_BUSY_UPLOADING:\n                assert(it->refcount != 0);\n                t->rebal.busy_items++;\n                refcount_decr(it);\n                item_trylock_unlock(cbarg.hold_lock);\n                break;\n            case MOVE_LOCKED:\n            case MOVE_BUSY_FLOATING:\n                t->rebal.busy_items++;\n                break;\n            case MOVE_PASS:\n                // already freed and unlinked, probably during an alloc\n                t->rebal.completed[offset] = 1;\n                break;\n        }\n\n    }\n\n    t->rebal.slab_pos = (char *)t->rebal.slab_pos + t->rebal.cls_size;\n\n    if (t->rebal.slab_pos >= t->rebal.slab_end) {\n        /* Some items were busy, start again from the top */\n        if (t->rebal.busy_items) {\n            t->rebal.slab_pos = t->rebal.slab_start;\n            STATS_LOCK();\n            stats.slab_reassign_busy_items += t->rebal.busy_items;\n            STATS_UNLOCK();\n            t->rebal.busy_items = 0;\n            t->rebal.busy_loops++;\n        } else {\n            t->rebal.done++;\n        }\n    }\n\n    return (t->rebal.busy_items != was_busy) ? 1 : 0;\n}\n\nstatic void slab_rebalance_finish(struct slab_rebal_thread *t) {\n#ifdef DEBUG_SLAB_MOVER\n    /* If the algorithm is broken, live items can sneak in. */\n    slab_rebal.slab_pos = slab_rebal.slab_start;\n    while (1) {\n        item *it = slab_rebal.slab_pos;\n        assert(it->it_flags == (ITEM_SLABBED|ITEM_FETCHED));\n        assert(memcmp(ITEM_key(it), \"deadbeef\", 8) == 0);\n        it->it_flags = ITEM_SLABBED|ITEM_FETCHED;\n        slab_rebal.slab_pos = (char *)slab_rebal.slab_pos + slab_rebal.cls_size;\n        if (slab_rebal.slab_pos >= slab_rebal.slab_end)\n            break;\n    }\n#endif\n\n    // release any temporary memory we didn't end up using.\n    if (t->new_it) {\n        slabs_free(t->new_it, t->rebal.s_clsid);\n        t->new_it = NULL;\n    }\n\n    /* At this point the stolen slab is completely clear.\n     * We always kill the \"first\"/\"oldest\" slab page in the slab_list, so\n     * shuffle the page list backwards and decrement.\n     */\n    slabs_finalize_page_move(t->rebal.s_clsid, t->rebal.d_clsid,\n            t->rebal.slab_start);\n\n    STATS_LOCK();\n    stats.slabs_moved++;\n    stats.slab_reassign_rescues += t->rebal.rescues;\n    stats.slab_reassign_inline_reclaim += t->rebal.inline_reclaim;\n    stats.slab_reassign_chunk_rescues += t->rebal.chunk_rescues;\n    stats.slab_reassign_busy_deletes += t->rebal.busy_deletes;\n    stats.slab_reassign_busy_nomem += t->rebal.busy_nomem;\n    stats_state.slab_reassign_running = false;\n    STATS_UNLOCK();\n\n    free(t->rebal.completed);\n    memset(&t->rebal, 0, sizeof(t->rebal));\n    t->allow_evictions = false;\n}\n\nstatic int slab_rebalance_check_automove(struct slab_rebal_thread *t,\n        struct timespec *now) {\n    int src, dst;\n    if (settings.slab_automove == 0) {\n        // not enabled\n        return 0;\n    }\n\n    if (t->am_last.tv_sec == now->tv_sec) {\n        // run once per second-ish.\n        return 0;\n    }\n\n    if (settings.slab_automove_version != t->am_version) {\n        void *am_new = t->sam->init(&settings);\n        // only replace if we successfully re-init'ed\n        if (am_new) {\n            t->sam->free(t->active_am);\n            t->active_am = am_new;\n        }\n        t->am_version = settings.slab_automove_version;\n    }\n\n    t->sam->run(t->active_am, &src, &dst);\n    if (src != -1 && dst != -1) {\n        // rebalancer lock already held, call directly.\n        const char *msg = \"ok\";\n        if (do_slabs_reassign(t, src, dst, 0) != REASSIGN_OK) {\n            msg = \"fail\";\n        }\n        LOGGER_LOG(t->l, LOG_SYSEVENTS, LOGGER_SLAB_MOVE, NULL, src, dst, msg);\n        if (dst != 0) {\n            // if not reclaiming to global, rate limit to one per second.\n            t->am_last.tv_sec = now->tv_sec;\n        }\n        // run the thread since we're moving a page.\n        return 1;\n    }\n\n    return 0;\n}\n\n/* Slab mover thread.\n * Sits waiting for a condition to jump off and shovel some memory about\n */\n// TODO: add back the \"max busy loops\" and bail the page move\nstatic void *slab_rebalance_thread(void *arg) {\n    struct slab_rebal_thread *t = arg;\n    struct slab_rebalance *r = &t->rebal;\n    int backoff_timer = 1;\n    int backoff_max = 1000;\n    // create logger in thread for setspecific\n    t->l = logger_create();\n    /* Go into cond_wait with the mutex held */\n    mutex_lock(&t->lock);\n\n    /* Must finish moving page before stopping */\n    while (t->run_thread) {\n        // are we running a rebalance?\n        if (r->s_clsid != 0 || r->d_clsid != 0) {\n            // do we need to kick it off?\n            if (r->slab_start == NULL) {\n                if (slab_rebalance_start(t) < 0) {\n                    r->s_clsid = 0;\n                    r->d_clsid = 0;\n                    continue;\n                }\n            }\n\n            if (r->done) {\n                slab_rebalance_finish(t);\n            } else {\n                // attempt to get some prepared memory\n                slab_rebalance_prep(t);\n                // attempt to free up memory in a page\n                if (slab_rebalance_move(t)) {\n                    /* Stuck waiting for some items to unlock, so slow down a bit\n                     * to give them a chance to free up */\n                    usleep(backoff_timer);\n                    backoff_timer = backoff_timer * 2;\n                    if (backoff_timer > backoff_max)\n                        backoff_timer = backoff_max;\n                } else {\n                    backoff_timer = 1;\n                }\n            }\n        } else {\n            struct timespec now;\n            clock_gettime(CLOCK_REALTIME, &now);\n            if (slab_rebalance_check_automove(t, &now) == 0) {\n                // standard delay\n                now.tv_sec++;\n                // wait for signal to start another move.\n                pthread_cond_timedwait(&t->cond, &t->lock, &now);\n            } // else don't wait, run again immediately.\n        }\n    }\n\n    // TODO: cancel in-flight slab page move\n    mutex_unlock(&t->lock);\n    return NULL;\n}\n\nstatic enum reassign_result_type do_slabs_reassign(struct slab_rebal_thread *t, int src, int dst, int flags) {\n    if (src == dst)\n        return REASSIGN_SRC_DST_SAME;\n\n    /* Special indicator to choose ourselves. */\n    if (src == -1) {\n        src = slabs_pick_any_for_reassign(dst);\n        /* TODO: If we end up back at -1, return a new error type */\n    }\n\n    if (src < SLAB_GLOBAL_PAGE_POOL || src > MAX_NUMBER_OF_SLAB_CLASSES||\n        dst < SLAB_GLOBAL_PAGE_POOL || dst > MAX_NUMBER_OF_SLAB_CLASSES)\n        return REASSIGN_BADCLASS;\n\n    if (slabs_page_count(src) < 2) {\n        return REASSIGN_NOSPARE;\n    }\n\n    t->rebal.s_clsid = src;\n    t->rebal.d_clsid = dst;\n    if (flags & SLABS_REASSIGN_ALLOW_EVICTIONS) {\n        t->allow_evictions = true;\n    }\n\n    pthread_cond_signal(&t->cond);\n\n    return REASSIGN_OK;\n}\n\nenum reassign_result_type slabs_reassign(struct slab_rebal_thread *t, int src, int dst, int flags) {\n    enum reassign_result_type ret;\n    if (pthread_mutex_trylock(&t->lock) != 0) {\n        return REASSIGN_RUNNING;\n    }\n    ret = do_slabs_reassign(t, src, dst, flags);\n    pthread_mutex_unlock(&t->lock);\n    return ret;\n}\n\n/* If we hold this lock, rebalancer can't wake up or move */\nvoid slab_maintenance_pause(struct slab_rebal_thread *t) {\n    pthread_mutex_lock(&t->lock);\n}\n\nvoid slab_maintenance_resume(struct slab_rebal_thread *t) {\n    pthread_mutex_unlock(&t->lock);\n}\n\nstruct slab_rebal_thread *start_slab_maintenance_thread(void *storage) {\n    int ret;\n    struct slab_rebal_thread *t = calloc(1, sizeof(*t));\n    if (t == NULL)\n        return NULL;\n\n    pthread_mutex_init(&t->lock, NULL);\n    pthread_cond_init(&t->cond, NULL);\n    t->run_thread = true;\n    if (storage) {\n#ifdef EXTSTORE\n        t->storage = storage;\n        t->sam = &slab_automove_extstore;\n#endif\n    } else {\n        t->sam = &slab_automove_default;\n    }\n    t->active_am = t->sam->init(&settings);\n    if (t->active_am == NULL) {\n        fprintf(stderr, \"Can't create slab rebalancer thread: failed to allocate automover memory\\n\");\n        return NULL;\n    }\n\n    if ((ret = pthread_create(&t->tid, NULL,\n                              slab_rebalance_thread, t)) != 0) {\n        fprintf(stderr, \"Can't create slab rebalancer thread: %s\\n\", strerror(ret));\n        return NULL;\n    }\n    thread_setname(t->tid, \"mc-slabmaint\");\n    return t;\n}\n\n/* The maintenance thread is on a sleep/loop cycle, so it should join after a\n * short wait */\nvoid stop_slab_maintenance_thread(struct slab_rebal_thread *t) {\n    pthread_mutex_lock(&t->lock);\n    t->run_thread = false;\n    pthread_cond_signal(&t->cond);\n    pthread_mutex_unlock(&t->lock);\n\n    /* Wait for the maintenance thread to stop */\n    pthread_join(t->tid, NULL);\n\n    pthread_mutex_destroy(&t->lock);\n    pthread_cond_destroy(&t->cond);\n    if (t->rebal.completed) {\n        free(t->rebal.completed);\n    }\n    t->sam->free(t->active_am);\n    free(t);\n\n    // TODO: there is no logger_destroy() yet.\n}\n"
        },
        {
          "name": "slabs_mover.h",
          "type": "blob",
          "size": 0.609375,
          "content": "#ifndef SLABS_MOVER_H\n#define SLABS_MOVER_H\n\nstruct slab_rebal_thread;\nstruct slab_rebal_thread *start_slab_maintenance_thread(void *storage);\nvoid stop_slab_maintenance_thread(struct slab_rebal_thread *t);\n\nenum reassign_result_type {\n    REASSIGN_OK=0, REASSIGN_RUNNING, REASSIGN_BADCLASS, REASSIGN_NOSPARE,\n    REASSIGN_SRC_DST_SAME\n};\n\n#define SLABS_REASSIGN_ALLOW_EVICTIONS 1\nenum reassign_result_type slabs_reassign(struct slab_rebal_thread *t, int src, int dst, int flags);\n\nvoid slab_maintenance_pause(struct slab_rebal_thread *t);\nvoid slab_maintenance_resume(struct slab_rebal_thread *t);\n\n#endif // SLABS_MOVER_H\n"
        },
        {
          "name": "solaris_priv.c",
          "type": "blob",
          "size": 1.2958984375,
          "content": "#include <stdlib.h>\n#include <priv.h>\n#include <stdio.h>\n#include \"memcached.h\"\n\n/*\n * this section of code will drop all (Solaris) privileges including\n * those normally granted to all userland process (basic privileges). The\n * effect of this is that after running this code, the process will not able\n * to fork(), exec(), etc.  See privileges(5) for more information.\n */\nvoid drop_privileges(void) {\n   priv_set_t *privs = priv_str_to_set(\"basic\", \",\", NULL);\n\n   if (privs == NULL) {\n      perror(\"priv_str_to_set\");\n      exit(EXIT_FAILURE);\n   }\n\n   (void)priv_delset(privs, PRIV_FILE_LINK_ANY);\n   (void)priv_delset(privs, PRIV_PROC_EXEC);\n   (void)priv_delset(privs, PRIV_PROC_FORK);\n   (void)priv_delset(privs, PRIV_PROC_INFO);\n   (void)priv_delset(privs, PRIV_PROC_SESSION);\n\n   if (setppriv(PRIV_SET, PRIV_PERMITTED, privs) != 0) {\n      perror(\"setppriv(PRIV_SET, PRIV_PERMITTED)\");\n      exit(EXIT_FAILURE);\n   }\n\n   priv_emptyset(privs);\n\n   if (setppriv(PRIV_SET, PRIV_INHERITABLE, privs) != 0) {\n      perror(\"setppriv(PRIV_SET, PRIV_INHERITABLE)\");\n      exit(EXIT_FAILURE);\n   }\n\n   if (setppriv(PRIV_SET, PRIV_LIMIT, privs) != 0) {\n      perror(\"setppriv(PRIV_SET, PRIV_LIMIT)\");\n      exit(EXIT_FAILURE);\n   }\n\n   priv_freeset(privs);\n}\n\nvoid setup_privilege_violations_handler(void) {\n   // not needed\n}\n"
        },
        {
          "name": "stats_prefix.c",
          "type": "blob",
          "size": 4.3369140625,
          "content": "/* -*- Mode: C; tab-width: 4; c-basic-offset: 4; indent-tabs-mode: nil -*- */\n/* Author: Steven Grimm <sgrimm@facebook.com> */\n#include \"memcached.h\"\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <assert.h>\n\n/* Hash table that uses the global hash function */\nstatic PREFIX_STATS *prefix_stats[PREFIX_HASH_SIZE];\n\nstatic char prefix_delimiter;\nstatic int num_prefixes = 0;\nstatic int total_prefix_size = 0;\n\nvoid stats_prefix_init(char delimiter) {\n    prefix_delimiter = delimiter;\n    memset(prefix_stats, 0, sizeof(prefix_stats));\n}\n\nvoid stats_prefix_clear(void) {\n    int i;\n\n    for (i = 0; i < PREFIX_HASH_SIZE; i++) {\n        PREFIX_STATS *cur, *next;\n        for (cur = prefix_stats[i]; cur != NULL; cur = next) {\n            next = cur->next;\n            free(cur->prefix);\n            free(cur);\n        }\n        prefix_stats[i] = NULL;\n    }\n    num_prefixes = 0;\n    total_prefix_size = 0;\n}\n\nPREFIX_STATS *stats_prefix_find(const char *key, const size_t nkey) {\n    PREFIX_STATS *pfs;\n    uint32_t hashval;\n    size_t length;\n    bool bailout = true;\n\n    assert(key != NULL);\n\n    for (length = 0; length < nkey && key[length] != '\\0'; length++) {\n        if (key[length] == prefix_delimiter) {\n            bailout = false;\n            break;\n        }\n    }\n\n    if (bailout) {\n        return NULL;\n    }\n\n    hashval = hash(key, length) % PREFIX_HASH_SIZE;\n\n    for (pfs = prefix_stats[hashval]; NULL != pfs; pfs = pfs->next) {\n        if (strncmp(pfs->prefix, key, length) == 0)\n            return pfs;\n    }\n\n    pfs = calloc(sizeof(PREFIX_STATS), 1);\n    if (NULL == pfs) {\n        perror(\"Can't allocate space for stats structure: calloc\");\n        return NULL;\n    }\n\n    pfs->prefix = malloc(length + 1);\n    if (NULL == pfs->prefix) {\n        perror(\"Can't allocate space for copy of prefix: malloc\");\n        free(pfs);\n        return NULL;\n    }\n\n    strncpy(pfs->prefix, key, length);\n    pfs->prefix[length] = '\\0';      /* because strncpy() sucks */\n    pfs->prefix_len = length;\n\n    pfs->next = prefix_stats[hashval];\n    prefix_stats[hashval] = pfs;\n\n    num_prefixes++;\n    total_prefix_size += length;\n\n    return pfs;\n}\n\nvoid stats_prefix_record_get(const char *key, const size_t nkey, const bool is_hit) {\n    PREFIX_STATS *pfs;\n\n    STATS_LOCK();\n    pfs = stats_prefix_find(key, nkey);\n    if (NULL != pfs) {\n        pfs->num_gets++;\n        if (is_hit) {\n            pfs->num_hits++;\n        }\n    }\n    STATS_UNLOCK();\n}\n\nvoid stats_prefix_record_delete(const char *key, const size_t nkey) {\n    PREFIX_STATS *pfs;\n\n    STATS_LOCK();\n    pfs = stats_prefix_find(key, nkey);\n    if (NULL != pfs) {\n        pfs->num_deletes++;\n    }\n    STATS_UNLOCK();\n}\n\nvoid stats_prefix_record_set(const char *key, const size_t nkey) {\n    PREFIX_STATS *pfs;\n\n    STATS_LOCK();\n    pfs = stats_prefix_find(key, nkey);\n    if (NULL != pfs) {\n        pfs->num_sets++;\n    }\n    STATS_UNLOCK();\n}\n\nchar *stats_prefix_dump(int *length) {\n    const char *format = \"PREFIX %s get %llu hit %llu set %llu del %llu\\r\\n\";\n    PREFIX_STATS *pfs;\n    char *buf;\n    int i, pos;\n    size_t size = 0, written = 0;\n#ifndef NDEBUG\n    size_t total_written = 0;\n#endif\n    /*\n     * Figure out how big the buffer needs to be. This is the sum of the\n     * lengths of the prefixes themselves, plus the size of one copy of\n     * the per-prefix output with 20-digit values for all the counts,\n     * plus space for the \"END\" at the end.\n     */\n    STATS_LOCK();\n    size = strlen(format) + total_prefix_size +\n           num_prefixes * (strlen(format) - 2 /* %s */\n                           + 4 * (20 - 4)) /* %llu replaced by 20-digit num */\n                           + sizeof(\"END\\r\\n\");\n    buf = malloc(size);\n    if (NULL == buf) {\n        perror(\"Can't allocate stats response: malloc\");\n        STATS_UNLOCK();\n        return NULL;\n    }\n\n    pos = 0;\n    for (i = 0; i < PREFIX_HASH_SIZE; i++) {\n        for (pfs = prefix_stats[i]; NULL != pfs; pfs = pfs->next) {\n            written = snprintf(buf + pos, size-pos, format,\n                           pfs->prefix, pfs->num_gets, pfs->num_hits,\n                           pfs->num_sets, pfs->num_deletes);\n            pos += written;\n#ifndef NDEBUG\n            total_written += written;\n            assert(total_written < size);\n#endif\n        }\n    }\n\n    STATS_UNLOCK();\n    memcpy(buf + pos, \"END\\r\\n\", 6);\n\n    *length = pos + 5;\n    return buf;\n}\n"
        },
        {
          "name": "stats_prefix.h",
          "type": "blob",
          "size": 1.8076171875,
          "content": "#ifndef STATS_PREFIX_H\n#define STATS_PREFIX_H\n\n/* The stats prefix subsystem stores detailed statistics for each key prefix.\n * Simple statistics like total number of GETS are stored by the Stats\n * subsystem defined elsewhere.\n *\n * Suppose the prefix delimiter is \":\", then \"user:123\" and \"user:456\" both\n * have the same prefix \"user\".\n */\n\n\n/* Initialize the stats prefix subsystem. Should be called once before other\n * functions are called. The global hash initialization should be done before\n * using this subsystem.\n */\nvoid stats_prefix_init(char prefix_delimiter);\n\n/* Clear previously collected stats. Requires you to have the acquired\n * the STATS_LOCK() first.\n */\nvoid stats_prefix_clear(void);\n\n/* Record a GET for a key */\nvoid stats_prefix_record_get(const char *key, const size_t nkey, const bool is_hit);\n\n/* Record a DELETE for a key */\nvoid stats_prefix_record_delete(const char *key, const size_t nkey);\n\n/* Record a SET for a key */\nvoid stats_prefix_record_set(const char *key, const size_t nkey);\n\n/* Return the collected stats in a textual for suitable for writing to a client.\n * The size of the output text is stored in the length parameter.\n * Returns NULL on error\n */\nchar *stats_prefix_dump(int *length);\n\n/* Visible for testing */\n#define PREFIX_HASH_SIZE 256\ntypedef struct _prefix_stats PREFIX_STATS;\nstruct _prefix_stats {\n    char *prefix;\n    size_t prefix_len;\n    uint64_t num_gets;\n    uint64_t num_sets;\n    uint64_t num_deletes;\n    uint64_t num_hits;\n    PREFIX_STATS *next;\n};\n\n/* Return the PREFIX_STATS structure for the specified key, creating it if\n * it does not already exist. Returns NULL if the key does not contain\n * prefix delimiter, or if there was an error. Requires you to have acquired\n * STATS_LOCK() first.\n */\nPREFIX_STATS *stats_prefix_find(const char *key, const size_t nkey);\n\n#endif\n"
        },
        {
          "name": "storage.c",
          "type": "blob",
          "size": 57.2099609375,
          "content": "/* -*- Mode: C; tab-width: 4; c-basic-offset: 4; indent-tabs-mode: nil -*- */\n#include \"memcached.h\"\n#ifdef EXTSTORE\n\n#include \"storage.h\"\n#include \"extstore.h\"\n#include <stdlib.h>\n#include <stdio.h>\n#include <stddef.h>\n#include <string.h>\n#include <limits.h>\n#include <ctype.h>\n\n#define PAGE_BUCKET_DEFAULT 0\n#define PAGE_BUCKET_COMPACT 1\n#define PAGE_BUCKET_CHUNKED 2\n#define PAGE_BUCKET_LOWTTL  3\n#define PAGE_BUCKET_COLDCOMPACT 4\n#define PAGE_BUCKET_OLD     5\n// Not another bucket; this is the total number of buckets.\n#define PAGE_BUCKET_COUNT   6\n\n/*\n * API functions\n */\nstatic void storage_finalize_cb(io_pending_t *pending);\nstatic void storage_return_cb(io_pending_t *pending);\n\n// re-cast an io_pending_t into this more descriptive structure.\n// the first few items _must_ match the original struct.\ntypedef struct _io_pending_storage_t {\n    int io_queue_type;\n    LIBEVENT_THREAD *thread;\n    conn *c;\n    mc_resp *resp;\n    io_queue_cb return_cb;    // called on worker thread.\n    io_queue_cb finalize_cb;  // called back on the worker thread.\n    STAILQ_ENTRY(io_pending_t) iop_next; // queue chain.\n                              /* original struct ends here */\n    item *hdr_it;             /* original header item. */\n    obj_io io_ctx;            /* embedded extstore IO header */\n    unsigned int iovec_data;  /* specific index of data iovec */\n    bool noreply;             /* whether the response had noreply set */\n    bool miss;                /* signal a miss to unlink hdr_it */\n    bool badcrc;              /* signal a crc failure */\n    bool active;              /* tells if IO was dispatched or not */\n} io_pending_storage_t;\n\nstatic pthread_t storage_compact_tid;\nstatic pthread_mutex_t storage_compact_plock;\nstatic pthread_cond_t storage_compact_cond;\n\n// Only call this if item has ITEM_HDR\nbool storage_validate_item(void *e, item *it) {\n    item_hdr *hdr = (item_hdr *)ITEM_data(it);\n    if (extstore_check(e, hdr->page_id, hdr->page_version) != 0) {\n        return false;\n    } else {\n        return true;\n    }\n}\n\nvoid storage_delete(void *e, item *it) {\n    if (it->it_flags & ITEM_HDR) {\n        item_hdr *hdr = (item_hdr *)ITEM_data(it);\n        extstore_delete(e, hdr->page_id, hdr->page_version,\n                1, ITEM_ntotal(it));\n    }\n}\n\n// Function for the extra stats called from a protocol.\n// NOTE: This either needs a name change or a wrapper, perhaps?\n// it's defined here to reduce exposure of extstore.h to the rest of memcached\n// but feels a little off being defined here.\n// At very least maybe \"process_storage_stats\" in line with making this more\n// of a generic wrapper module.\nvoid process_extstore_stats(ADD_STAT add_stats, void *c) {\n    int i;\n    char key_str[STAT_KEY_LEN];\n    char val_str[STAT_VAL_LEN];\n    int klen = 0, vlen = 0;\n    struct extstore_stats st;\n\n    assert(add_stats);\n\n    void *storage = ext_storage;\n    if (storage == NULL) {\n        return;\n    }\n    extstore_get_stats(storage, &st);\n    st.page_data = calloc(st.page_count, sizeof(struct extstore_page_data));\n    extstore_get_page_data(storage, &st);\n\n    for (i = 0; i < st.page_count; i++) {\n        APPEND_NUM_STAT(i, \"version\", \"%llu\",\n                (unsigned long long) st.page_data[i].version);\n        APPEND_NUM_STAT(i, \"bytes\", \"%llu\",\n                (unsigned long long) st.page_data[i].bytes_used);\n        APPEND_NUM_STAT(i, \"bucket\", \"%u\",\n                st.page_data[i].bucket);\n        APPEND_NUM_STAT(i, \"free_bucket\", \"%u\",\n                st.page_data[i].free_bucket);\n    }\n\n    free(st.page_data);\n}\n\n// Additional storage stats for the main stats output.\nvoid storage_stats(ADD_STAT add_stats, void *c) {\n    struct extstore_stats st;\n    if (ext_storage) {\n        STATS_LOCK();\n        APPEND_STAT(\"extstore_memory_pressure\", \"%.2f\", stats_state.extstore_memory_pressure);\n        APPEND_STAT(\"extstore_compact_lost\", \"%llu\", (unsigned long long)stats.extstore_compact_lost);\n        APPEND_STAT(\"extstore_compact_rescues\", \"%llu\", (unsigned long long)stats.extstore_compact_rescues);\n        APPEND_STAT(\"extstore_compact_resc_cold\", \"%llu\", (unsigned long long)stats.extstore_compact_resc_cold);\n        APPEND_STAT(\"extstore_compact_resc_old\", \"%llu\", (unsigned long long)stats.extstore_compact_resc_old);\n        APPEND_STAT(\"extstore_compact_skipped\", \"%llu\", (unsigned long long)stats.extstore_compact_skipped);\n        STATS_UNLOCK();\n        extstore_get_stats(ext_storage, &st);\n        APPEND_STAT(\"extstore_page_allocs\", \"%llu\", (unsigned long long)st.page_allocs);\n        APPEND_STAT(\"extstore_page_evictions\", \"%llu\", (unsigned long long)st.page_evictions);\n        APPEND_STAT(\"extstore_page_reclaims\", \"%llu\", (unsigned long long)st.page_reclaims);\n        APPEND_STAT(\"extstore_pages_free\", \"%llu\", (unsigned long long)st.pages_free);\n        APPEND_STAT(\"extstore_pages_used\", \"%llu\", (unsigned long long)st.pages_used);\n        APPEND_STAT(\"extstore_objects_evicted\", \"%llu\", (unsigned long long)st.objects_evicted);\n        APPEND_STAT(\"extstore_objects_read\", \"%llu\", (unsigned long long)st.objects_read);\n        APPEND_STAT(\"extstore_objects_written\", \"%llu\", (unsigned long long)st.objects_written);\n        APPEND_STAT(\"extstore_objects_used\", \"%llu\", (unsigned long long)st.objects_used);\n        APPEND_STAT(\"extstore_bytes_evicted\", \"%llu\", (unsigned long long)st.bytes_evicted);\n        APPEND_STAT(\"extstore_bytes_written\", \"%llu\", (unsigned long long)st.bytes_written);\n        APPEND_STAT(\"extstore_bytes_read\", \"%llu\", (unsigned long long)st.bytes_read);\n        APPEND_STAT(\"extstore_bytes_used\", \"%llu\", (unsigned long long)st.bytes_used);\n        APPEND_STAT(\"extstore_bytes_fragmented\", \"%llu\", (unsigned long long)st.bytes_fragmented);\n        APPEND_STAT(\"extstore_limit_maxbytes\", \"%llu\", (unsigned long long)(st.page_count * st.page_size));\n        APPEND_STAT(\"extstore_io_queue\", \"%llu\", (unsigned long long)(st.io_queue));\n    }\n\n}\n\n// This callback runs in the IO thread.\n// TODO: Some or all of this should move to the\n// io_pending's callback back in the worker thread.\n// It might make sense to keep the crc32c check here though.\nstatic void _storage_get_item_cb(void *e, obj_io *io, int ret) {\n    // FIXME: assumes success\n    io_pending_storage_t *p = (io_pending_storage_t *)io->data;\n    mc_resp *resp = p->resp;\n    conn *c = p->c;\n    assert(p->active == true);\n    item *read_it = (item *)io->buf;\n    bool miss = false;\n\n    // TODO: How to do counters for hit/misses?\n    if (ret < 1) {\n        miss = true;\n    } else {\n        uint32_t crc2;\n        uint32_t crc = (uint32_t) read_it->exptime;\n        int x;\n        // item is chunked, crc the iov's\n        if (io->iov != NULL) {\n            // first iov is the header, which we don't use beyond crc\n            crc2 = crc32c(0, (char *)io->iov[0].iov_base+STORE_OFFSET, io->iov[0].iov_len-STORE_OFFSET);\n            // make sure it's not sent. hack :(\n            io->iov[0].iov_len = 0;\n            for (x = 1; x < io->iovcnt; x++) {\n                crc2 = crc32c(crc2, (char *)io->iov[x].iov_base, io->iov[x].iov_len);\n            }\n        } else {\n            crc2 = crc32c(0, (char *)read_it+STORE_OFFSET, io->len-STORE_OFFSET);\n        }\n\n        if (crc != crc2) {\n            miss = true;\n            p->badcrc = true;\n        }\n    }\n\n    if (miss) {\n        if (p->noreply) {\n            // In all GET cases, noreply means we send nothing back.\n            resp->skip = true;\n        } else {\n            // TODO: This should be movable to the worker thread.\n            // Convert the binprot response into a miss response.\n            // The header requires knowing a bunch of stateful crap, so rather\n            // than simply writing out a \"new\" miss response we mangle what's\n            // already there.\n            if (c->protocol == binary_prot) {\n                protocol_binary_response_header *header =\n                    (protocol_binary_response_header *)resp->wbuf;\n\n                // cut the extra nbytes off of the body_len\n                uint32_t body_len = ntohl(header->response.bodylen);\n                uint8_t hdr_len = header->response.extlen;\n                body_len -= resp->iov[p->iovec_data].iov_len + hdr_len;\n                resp->tosend -= resp->iov[p->iovec_data].iov_len + hdr_len;\n                header->response.extlen = 0;\n                header->response.status = (uint16_t)htons(PROTOCOL_BINARY_RESPONSE_KEY_ENOENT);\n                header->response.bodylen = htonl(body_len);\n\n                // truncate the data response.\n                resp->iov[p->iovec_data].iov_len = 0;\n                // wipe the extlen iov... wish it was just a flat buffer.\n                resp->iov[p->iovec_data-1].iov_len = 0;\n                resp->chunked_data_iov = 0;\n            } else {\n                int i;\n                // Meta commands have EN status lines for miss, rather than\n                // END as a trailer as per normal ascii.\n                if (resp->iov[0].iov_len >= 3\n                        && memcmp(resp->iov[0].iov_base, \"VA \", 3) == 0) {\n                    // TODO: These miss translators should use specific callback\n                    // functions attached to the io wrap. This is weird :(\n                    resp->iovcnt = 1;\n                    resp->iov[0].iov_len = 4;\n                    resp->iov[0].iov_base = \"EN\\r\\n\";\n                    resp->tosend = 4;\n                } else {\n                    // Wipe the iovecs up through our data injection.\n                    // Allows trailers to be returned (END)\n                    for (i = 0; i <= p->iovec_data; i++) {\n                        resp->tosend -= resp->iov[i].iov_len;\n                        resp->iov[i].iov_len = 0;\n                        resp->iov[i].iov_base = NULL;\n                    }\n                }\n                resp->chunked_total = 0;\n                resp->chunked_data_iov = 0;\n            }\n        }\n        p->miss = true;\n    } else {\n        assert(read_it->slabs_clsid != 0);\n        // TODO: should always use it instead of ITEM_data to kill more\n        // chunked special casing.\n        if ((read_it->it_flags & ITEM_CHUNKED) == 0) {\n            resp->iov[p->iovec_data].iov_base = ITEM_data(read_it);\n        }\n        p->miss = false;\n    }\n\n    p->active = false;\n    //assert(c->io_wrapleft >= 0);\n\n    return_io_pending((io_pending_t *)p);\n}\n\nint storage_get_item(conn *c, item *it, mc_resp *resp) {\n#ifdef NEED_ALIGN\n    item_hdr hdr;\n    memcpy(&hdr, ITEM_data(it), sizeof(hdr));\n#else\n    item_hdr *hdr = (item_hdr *)ITEM_data(it);\n#endif\n    io_queue_t *q = conn_io_queue_get(c, IO_QUEUE_EXTSTORE);\n    size_t ntotal = ITEM_ntotal(it);\n    unsigned int clsid = slabs_clsid(ntotal);\n    item *new_it;\n    bool chunked = false;\n    if (ntotal > settings.slab_chunk_size_max) {\n        // Pull a chunked item header.\n        client_flags_t flags;\n        FLAGS_CONV(it, flags);\n        new_it = item_alloc(ITEM_key(it), it->nkey, flags, it->exptime, it->nbytes);\n        assert(new_it == NULL || (new_it->it_flags & ITEM_CHUNKED));\n        chunked = true;\n    } else {\n        new_it = do_item_alloc_pull(ntotal, clsid);\n    }\n    if (new_it == NULL)\n        return -1;\n    // so we can free the chunk on a miss\n    new_it->slabs_clsid = clsid;\n\n    io_pending_storage_t *p = do_cache_alloc(c->thread->io_cache);\n    // this is a re-cast structure, so assert that we never outsize it.\n    assert(sizeof(io_pending_t) >= sizeof(io_pending_storage_t));\n    memset(p, 0, sizeof(io_pending_storage_t));\n    p->active = true;\n    p->miss = false;\n    p->badcrc = false;\n    p->noreply = c->noreply;\n    p->thread = c->thread;\n    p->return_cb = storage_return_cb;\n    p->finalize_cb = storage_finalize_cb;\n    // io_pending owns the reference for this object now.\n    p->hdr_it = it;\n    p->resp = resp;\n    p->io_queue_type = IO_QUEUE_EXTSTORE;\n    obj_io *eio = &p->io_ctx;\n\n    // FIXME: error handling.\n    if (chunked) {\n        unsigned int ciovcnt = 0;\n        size_t remain = new_it->nbytes;\n        item_chunk *chunk = (item_chunk *) ITEM_schunk(new_it);\n        // TODO: This might make sense as a _global_ cache vs a per-thread.\n        // but we still can't load objects requiring > IOV_MAX iovs.\n        // In the meantime, these objects are rare/slow enough that\n        // malloc/freeing a statically sized object won't cause us much pain.\n        eio->iov = malloc(sizeof(struct iovec) * IOV_MAX);\n        if (eio->iov == NULL) {\n            item_remove(new_it);\n            do_cache_free(c->thread->io_cache, p);\n            return -1;\n        }\n\n        // fill the header so we can get the full data + crc back.\n        eio->iov[0].iov_base = new_it;\n        eio->iov[0].iov_len = ITEM_ntotal(new_it) - new_it->nbytes;\n        ciovcnt++;\n\n        while (remain > 0) {\n            chunk = do_item_alloc_chunk(chunk, remain);\n            // FIXME: _pure evil_, silently erroring if item is too large.\n            if (chunk == NULL || ciovcnt > IOV_MAX-1) {\n                item_remove(new_it);\n                free(eio->iov);\n                // TODO: wrapper function for freeing up an io wrap?\n                eio->iov = NULL;\n                do_cache_free(c->thread->io_cache, p);\n                return -1;\n            }\n            eio->iov[ciovcnt].iov_base = chunk->data;\n            eio->iov[ciovcnt].iov_len = (remain < chunk->size) ? remain : chunk->size;\n            chunk->used = (remain < chunk->size) ? remain : chunk->size;\n            remain -= chunk->size;\n            ciovcnt++;\n        }\n\n        eio->iovcnt = ciovcnt;\n    }\n\n    // Chunked or non chunked we reserve a response iov here.\n    p->iovec_data = resp->iovcnt;\n    int iovtotal = (c->protocol == binary_prot) ? it->nbytes - 2 : it->nbytes;\n    if (chunked) {\n        resp_add_chunked_iov(resp, new_it, iovtotal);\n    } else {\n        resp_add_iov(resp, \"\", iovtotal);\n    }\n\n    // We can't bail out anymore, so mc_resp owns the IO from here.\n    resp->io_pending = (io_pending_t *)p;\n\n    eio->buf = (void *)new_it;\n    p->c = c;\n\n    // We need to stack the sub-struct IO's together for submission.\n    eio->next = q->stack_ctx;\n    q->stack_ctx = eio;\n\n    // No need to stack the io_pending's together as they live on mc_resp's.\n    assert(q->count >= 0);\n    q->count++;\n    // reference ourselves for the callback.\n    eio->data = (void *)p;\n\n    // Now, fill in io->io based on what was in our header.\n#ifdef NEED_ALIGN\n    eio->page_version = hdr.page_version;\n    eio->page_id = hdr.page_id;\n    eio->offset = hdr.offset;\n#else\n    eio->page_version = hdr->page_version;\n    eio->page_id = hdr->page_id;\n    eio->offset = hdr->offset;\n#endif\n    eio->len = ntotal;\n    eio->mode = OBJ_IO_READ;\n    eio->cb = _storage_get_item_cb;\n\n    // FIXME: This stat needs to move to reflect # of flash hits vs misses\n    // for now it's a good gauge on how often we request out to flash at\n    // least.\n    pthread_mutex_lock(&c->thread->stats.mutex);\n    c->thread->stats.get_extstore++;\n    pthread_mutex_unlock(&c->thread->stats.mutex);\n\n    return 0;\n}\n\nvoid storage_submit_cb(io_queue_t *q) {\n    // Don't need to do anything special for extstore.\n    extstore_submit(q->ctx, q->stack_ctx);\n\n    // need to reset the stack for next use.\n    q->stack_ctx = NULL;\n}\n\n// Runs locally in worker thread.\nstatic void recache_or_free(io_pending_t *pending) {\n    // re-cast to our specific struct.\n    io_pending_storage_t *p = (io_pending_storage_t *)pending;\n\n    conn *c = p->c;\n    obj_io *io = &p->io_ctx;\n    assert(io != NULL);\n    item *it = (item *)io->buf;\n    assert(c != NULL);\n    bool do_free = true;\n    if (p->active) {\n        // If request never dispatched, free the read buffer but leave the\n        // item header alone.\n        do_free = false;\n        size_t ntotal = ITEM_ntotal(p->hdr_it);\n        slabs_free(it, slabs_clsid(ntotal));\n\n        io_queue_t *q = conn_io_queue_get(c, p->io_queue_type);\n        q->count--;\n        assert(q->count >= 0);\n        pthread_mutex_lock(&c->thread->stats.mutex);\n        c->thread->stats.get_aborted_extstore++;\n        pthread_mutex_unlock(&c->thread->stats.mutex);\n    } else if (p->miss) {\n        // If request was ultimately a miss, unlink the header.\n        do_free = false;\n        size_t ntotal = ITEM_ntotal(p->hdr_it);\n        item_unlink(p->hdr_it);\n        slabs_free(it, slabs_clsid(ntotal));\n        pthread_mutex_lock(&c->thread->stats.mutex);\n        c->thread->stats.miss_from_extstore++;\n        if (p->badcrc)\n            c->thread->stats.badcrc_from_extstore++;\n        pthread_mutex_unlock(&c->thread->stats.mutex);\n    } else if (settings.ext_recache_rate) {\n        // hashvalue is cuddled during store\n        uint32_t hv = (uint32_t)it->time;\n        // opt to throw away rather than wait on a lock.\n        void *hold_lock = item_trylock(hv);\n        if (hold_lock != NULL) {\n            item *h_it = p->hdr_it;\n            uint8_t flags = ITEM_LINKED|ITEM_FETCHED|ITEM_ACTIVE;\n            // Item must be recently hit at least twice to recache.\n            if (((h_it->it_flags & flags) == flags) &&\n                    h_it->time > current_time - ITEM_UPDATE_INTERVAL &&\n                    c->recache_counter++ % settings.ext_recache_rate == 0) {\n                do_free = false;\n                // In case it's been updated.\n                it->exptime = h_it->exptime;\n                it->it_flags &= ~ITEM_LINKED;\n                it->refcount = 0;\n                it->h_next = NULL; // might not be necessary.\n                STORAGE_delete(c->thread->storage, h_it);\n                item_replace(h_it, it, hv, ITEM_get_cas(h_it));\n                pthread_mutex_lock(&c->thread->stats.mutex);\n                c->thread->stats.recache_from_extstore++;\n                pthread_mutex_unlock(&c->thread->stats.mutex);\n            }\n        }\n        if (hold_lock)\n            item_trylock_unlock(hold_lock);\n    }\n    if (do_free)\n        slabs_free(it, ITEM_clsid(it));\n\n    p->io_ctx.buf = NULL;\n    p->io_ctx.next = NULL;\n    p->active = false;\n\n    // TODO: reuse lock and/or hv.\n    item_remove(p->hdr_it);\n}\n\n// Called after an IO has been returned to the worker thread.\nstatic void storage_return_cb(io_pending_t *pending) {\n    io_queue_t *q = conn_io_queue_get(pending->c, pending->io_queue_type);\n    q->count--;\n    if (q->count == 0) {\n        conn_worker_readd(pending->c);\n    }\n}\n\n// Called after responses have been transmitted. Need to free up related data.\nstatic void storage_finalize_cb(io_pending_t *pending) {\n    recache_or_free(pending);\n    io_pending_storage_t *p = (io_pending_storage_t *)pending;\n    obj_io *io = &p->io_ctx;\n    // malloc'ed iovec list used for chunked extstore fetches.\n    if (io->iov) {\n        free(io->iov);\n        io->iov = NULL;\n    }\n    // don't need to free the main context, since it's embedded.\n}\n\n/*\n * WRITE FLUSH THREAD\n */\n\nstatic int storage_write(void *storage, const int clsid, const int item_age) {\n    int did_moves = 0;\n    struct lru_pull_tail_return it_info;\n\n    it_info.it = NULL;\n    lru_pull_tail(clsid, COLD_LRU, 0, LRU_PULL_RETURN_ITEM, 0, &it_info);\n    /* Item is locked, and we have a reference to it. */\n    if (it_info.it == NULL) {\n        return did_moves;\n    }\n\n    obj_io io;\n    item *it = it_info.it;\n    /* First, storage for the header object */\n    size_t orig_ntotal = ITEM_ntotal(it);\n    client_flags_t flags;\n    if ((it->it_flags & ITEM_HDR) == 0 &&\n            (item_age == 0 || current_time - it->time > item_age)) {\n        FLAGS_CONV(it, flags);\n        item *hdr_it = do_item_alloc(ITEM_key(it), it->nkey, flags, it->exptime, sizeof(item_hdr));\n        /* Run the storage write understanding the start of the item is dirty.\n         * We will fill it (time/exptime/etc) from the header item on read.\n         */\n        if (hdr_it != NULL) {\n            int bucket = (it->it_flags & ITEM_CHUNKED) ?\n                PAGE_BUCKET_CHUNKED : PAGE_BUCKET_DEFAULT;\n            // Compress soon to expire items into similar pages.\n            if (it->exptime - current_time < settings.ext_low_ttl) {\n                bucket = PAGE_BUCKET_LOWTTL;\n            }\n            hdr_it->it_flags |= ITEM_HDR;\n            io.len = orig_ntotal;\n            io.mode = OBJ_IO_WRITE;\n            // NOTE: when the item is read back in, the slab mover\n            // may see it. Important to have refcount>=2 or ~ITEM_LINKED\n            assert(it->refcount >= 2);\n            // NOTE: write bucket vs free page bucket will disambiguate once\n            // lowttl feature is better understood.\n            if (extstore_write_request(storage, bucket, bucket, &io) == 0) {\n                // cuddle the hash value into the time field so we don't have\n                // to recalculate it.\n                item *buf_it = (item *) io.buf;\n                buf_it->time = it_info.hv;\n                // copy from past the headers + time headers.\n                // TODO: should be in items.c\n                if (it->it_flags & ITEM_CHUNKED) {\n                    // Need to loop through the item and copy\n                    item_chunk *sch = (item_chunk *) ITEM_schunk(it);\n                    int remain = orig_ntotal;\n                    int copied = 0;\n                    // copy original header\n                    int hdrtotal = ITEM_ntotal(it) - it->nbytes;\n                    memcpy((char *)io.buf+STORE_OFFSET, (char *)it+STORE_OFFSET, hdrtotal - STORE_OFFSET);\n                    copied = hdrtotal;\n                    // copy data in like it were one large object.\n                    while (sch && remain) {\n                        assert(remain >= sch->used);\n                        memcpy((char *)io.buf+copied, sch->data, sch->used);\n                        // FIXME: use one variable?\n                        remain -= sch->used;\n                        copied += sch->used;\n                        sch = sch->next;\n                    }\n                } else {\n                    memcpy((char *)io.buf+STORE_OFFSET, (char *)it+STORE_OFFSET, io.len-STORE_OFFSET);\n                }\n                // crc what we copied so we can do it sequentially.\n                buf_it->it_flags &= ~ITEM_LINKED;\n                buf_it->exptime = crc32c(0, (char*)io.buf+STORE_OFFSET, orig_ntotal-STORE_OFFSET);\n                extstore_write(storage, &io);\n                item_hdr *hdr = (item_hdr *) ITEM_data(hdr_it);\n                hdr->page_version = io.page_version;\n                hdr->page_id = io.page_id;\n                hdr->offset  = io.offset;\n                // overload nbytes for the header it\n                hdr_it->nbytes = it->nbytes;\n                /* success! Now we need to fill relevant data into the new\n                 * header and replace. Most of this requires the item lock\n                 */\n                /* CAS gets set while linking. Copy post-replace */\n                item_replace(it, hdr_it, it_info.hv, ITEM_get_cas(it));\n                do_item_remove(hdr_it);\n                did_moves = 1;\n                LOGGER_LOG(NULL, LOG_EVICTIONS, LOGGER_EXTSTORE_WRITE, it, bucket);\n            } else {\n                /* Failed to write for some reason, can't continue. */\n                slabs_free(hdr_it, ITEM_clsid(hdr_it));\n            }\n        }\n    }\n    do_item_remove(it);\n    item_unlock(it_info.hv);\n    return did_moves;\n}\n\nstatic pthread_t storage_write_tid;\nstatic pthread_mutex_t storage_write_plock;\n#define WRITE_SLEEP_MIN 200\n\nstatic void *storage_write_thread(void *arg) {\n    void *storage = arg;\n    // NOTE: ignoring overflow since that would take years of uptime in a\n    // specific load pattern of never going to sleep.\n    unsigned int backoff[MAX_NUMBER_OF_SLAB_CLASSES] = {0};\n    unsigned int counter = 0;\n    useconds_t to_sleep = WRITE_SLEEP_MIN;\n    logger *l = logger_create();\n    if (l == NULL) {\n        fprintf(stderr, \"Failed to allocate logger for storage compaction thread\\n\");\n        abort();\n    }\n\n    pthread_mutex_lock(&storage_write_plock);\n    // The compaction checker is CPU intensive, so we do a loose fudging to\n    // only activate it once every \"slab page size\" worth of bytes written.\n    // I was calling the compact checker once per run through this main loop,\n    // but we can end up doing lots of short loops without sleeping and end up\n    // calling the compact checker pretty frequently.\n    int check_compact = settings.slab_page_size;\n\n    while (1) {\n        // cache per-loop to avoid calls to the slabs_clsid() search loop\n        int min_class = slabs_clsid(settings.ext_item_size);\n        unsigned int global_pages = global_page_pool_size(NULL);\n        bool do_sleep = true;\n        int target_pages = 0;\n        if (global_pages < settings.ext_global_pool_min) {\n            target_pages = settings.ext_global_pool_min - global_pages;\n        }\n        counter++;\n        if (to_sleep > settings.ext_max_sleep)\n            to_sleep = settings.ext_max_sleep;\n\n        // the largest items have the least overhead from going to disk.\n        for (int x = MAX_NUMBER_OF_SLAB_CLASSES-1; x > 0; x--) {\n            bool did_move = false;\n            bool mem_limit_reached = false;\n            unsigned int chunks_free;\n            int item_age;\n\n            if (min_class > x || (backoff[x] && (counter % backoff[x] != 0))) {\n                continue;\n            }\n\n            // Avoid extra slab lock calls during heavy writing.\n            unsigned int chunks_perpage = 0;\n            chunks_free = slabs_available_chunks(x, &mem_limit_reached,\n                    &chunks_perpage);\n\n            if (chunks_perpage == 0) {\n                // no slab class here, skip.\n                continue;\n            }\n            unsigned int target = chunks_perpage * target_pages;\n            // Loose estimate for cutting the calls to compacter\n            unsigned int chunk_size = settings.slab_page_size / chunks_perpage;\n\n            // storage_write() will fail and cut loop after filling write buffer.\n            while (1) {\n                // if we are low on chunks and no spare, push out early.\n                if (chunks_free < target) {\n                    item_age = 0;\n                } else {\n                    item_age = settings.ext_item_age;\n                }\n                if (storage_write(storage, x, item_age)) {\n                    chunks_free++; // Allow stopping if we've done enough this loop\n                    check_compact -= chunk_size;\n                    // occasionally kick the compact checker.\n                    if (check_compact < 0) {\n                        pthread_cond_signal(&storage_compact_cond);\n                        check_compact = settings.slab_page_size;\n                    }\n                    did_move = true;\n                    do_sleep = false;\n                    if (to_sleep > WRITE_SLEEP_MIN)\n                        to_sleep /= 2;\n                } else {\n                    break;\n                }\n            }\n\n            if (!did_move) {\n                backoff[x]++;\n            } else {\n                backoff[x] = 1;\n            }\n        }\n\n        // flip lock so we can be paused or stopped\n        pthread_mutex_unlock(&storage_write_plock);\n        if (do_sleep) {\n            // Only do backoffs on other slab classes if we're actively\n            // flushing at least one class.\n            for (int x = 0; x < MAX_NUMBER_OF_SLAB_CLASSES; x++) {\n                backoff[x] = 1;\n            }\n\n            // call the compact checker occasionally even if we're just\n            // sleeping.\n            check_compact -= to_sleep * 10;\n            if (check_compact < 0) {\n                pthread_cond_signal(&storage_compact_cond);\n                check_compact = settings.slab_page_size;\n            }\n\n            usleep(to_sleep);\n            to_sleep++;\n        }\n        pthread_mutex_lock(&storage_write_plock);\n    }\n    return NULL;\n}\n\n// TODO\n// logger needs logger_destroy() to exist/work before this is safe.\n/*int stop_storage_write_thread(void) {\n    int ret;\n    pthread_mutex_lock(&lru_maintainer_lock);\n    do_run_lru_maintainer_thread = 0;\n    pthread_mutex_unlock(&lru_maintainer_lock);\n    // WAKEUP SIGNAL\n    if ((ret = pthread_join(lru_maintainer_tid, NULL)) != 0) {\n        fprintf(stderr, \"Failed to stop LRU maintainer thread: %s\\n\", strerror(ret));\n        return -1;\n    }\n    settings.lru_maintainer_thread = false;\n    return 0;\n}*/\n\nvoid storage_write_pause(void) {\n    pthread_mutex_lock(&storage_write_plock);\n}\n\nvoid storage_write_resume(void) {\n    pthread_mutex_unlock(&storage_write_plock);\n}\n\nint start_storage_write_thread(void *arg) {\n    int ret;\n\n    pthread_mutex_init(&storage_write_plock, NULL);\n    if ((ret = pthread_create(&storage_write_tid, NULL,\n        storage_write_thread, arg)) != 0) {\n        fprintf(stderr, \"Can't create storage_write thread: %s\\n\",\n            strerror(ret));\n        return -1;\n    }\n    thread_setname(storage_write_tid, \"mc-ext-write\");\n\n    return 0;\n}\n\n/*** COMPACTOR ***/\ntypedef struct __storage_buk {\n    unsigned int bucket;\n    unsigned int low_page;\n    unsigned int lowest_page;\n    uint64_t low_version;\n    uint64_t lowest_version;\n    unsigned int pages_free;\n    unsigned int pages_used;\n    unsigned int pages_total;\n    unsigned int bytes_fragmented; // fragmented bytes for low page\n    bool do_compact; // indicate this bucket should do a compaction.\n    bool do_compact_drop;\n} _storage_buk;\n\nstruct _compact_flags {\n    unsigned int drop_unread : 1;\n    unsigned int has_coldcompact : 1;\n    unsigned int has_old : 1;\n    unsigned int use_old : 1;\n};\n\n/* Fetch stats from the external storage system and decide to compact.\n */\nstatic int storage_compact_check(void *storage, logger *l,\n        uint32_t *page_id, uint64_t *page_version,\n        uint64_t *page_size, struct _compact_flags *flags) {\n    struct extstore_stats st;\n    _storage_buk buckets[PAGE_BUCKET_COUNT];\n    _storage_buk *buk = NULL;\n    uint64_t frag_limit;\n    extstore_get_stats(storage, &st);\n    if (st.pages_used == 0)\n        return 0;\n\n    for (int x = 0; x < PAGE_BUCKET_COUNT; x++) {\n        memset(&buckets[x], 0, sizeof(_storage_buk));\n        buckets[x].low_version = ULLONG_MAX;\n        buckets[x].lowest_version = ULLONG_MAX;\n    }\n    flags->drop_unread = 0;\n\n    frag_limit = st.page_size * settings.ext_max_frag;\n    LOGGER_LOG(l, LOG_SYSEVENTS, LOGGER_COMPACT_FRAGINFO,\n            NULL, settings.ext_max_frag, frag_limit);\n    st.page_data = calloc(st.page_count, sizeof(struct extstore_page_data));\n    extstore_get_page_data(storage, &st);\n\n    // find either the most fragmented page or the lowest version.\n    for (int x = 0; x < st.page_count; x++) {\n        buk = &buckets[st.page_data[x].free_bucket];\n        buk->pages_total++;\n        if (st.page_data[x].version == 0) {\n            buk->pages_free++;\n            // free pages don't contribute after this point.\n            continue;\n        } else {\n            buk->pages_used++;\n        }\n\n        // skip pages actively being used.\n        if (st.page_data[x].active) {\n            continue;\n        }\n\n        if (st.page_data[x].version < buk->lowest_version) {\n            buk->lowest_page = x;\n            buk->lowest_version = st.page_data[x].version;\n        }\n        // track the most fragmented page.\n        unsigned int frag = st.page_size - st.page_data[x].bytes_used;\n        if (st.page_data[x].bytes_used < frag_limit && frag > buk->bytes_fragmented) {\n            buk->low_page = x;\n            buk->low_version = st.page_data[x].version;\n            buk->bytes_fragmented = frag;\n        }\n    }\n    *page_size = st.page_size;\n    free(st.page_data);\n\n    buk = &buckets[PAGE_BUCKET_COLDCOMPACT];\n    if (buk->pages_total != 0) {\n        flags->has_coldcompact = 1;\n        if (buk->pages_free == 0 && buk->lowest_version != ULLONG_MAX) {\n            extstore_evict_page(storage, buk->lowest_page, buk->lowest_version);\n            return 0;\n        }\n    }\n\n    buk = &buckets[PAGE_BUCKET_OLD];\n    if (buk->pages_total != 0) {\n        flags->has_old = 1;\n        if (buk->pages_free == 0 && buk->lowest_version != ULLONG_MAX) {\n            extstore_evict_page(storage, buk->lowest_page, buk->lowest_version);\n            return 0;\n        }\n    }\n\n    for (int x = 0; x < PAGE_BUCKET_COUNT; x++) {\n        buk = &buckets[x];\n        assert(buk->pages_total == (buk->pages_used + buk->pages_free));\n        unsigned int pages_total = buk->pages_total;\n        // only process buckets which have dedicated pages assigned.\n        // LOWTTL skips compaction.\n        if (pages_total == 0 || x == PAGE_BUCKET_LOWTTL)\n            continue;\n\n        if (buk->pages_free < settings.ext_compact_under) {\n            if (buk->low_version != ULLONG_MAX) {\n                // found a normally defraggable page.\n                *page_id = buk->low_page;\n                *page_version = buk->low_version;\n                return 1;\n            } else if (buk->pages_free < settings.ext_drop_under\n                    && buk->lowest_version != ULLONG_MAX) {\n\n                if (x == PAGE_BUCKET_COLDCOMPACT || x == PAGE_BUCKET_OLD) {\n                    // this freeing technique doesn't apply to these buckets.\n                    // instead these buckets are eviction or normal\n                    // defragmentation only.\n                    continue;\n                }\n                // Nothing defraggable. Check for other usable conditions.\n                if (settings.ext_drop_unread) {\n                    flags->drop_unread = 1;\n                }\n\n                // If OLD and/or COLDCOMPACT pages exist we should always have\n                // one free page in those buckets, so we can always attempt to\n                // defrag into them.\n                // If only COLDCOMPACT exists this will attempt to segment off\n                // parts of a page that haven't been used.\n                // If OLD exists everything else in this \"oldest page\" goes\n                // into the OLD stream.\n                if (flags->drop_unread || flags->has_coldcompact || flags->has_old) {\n                    // only actually use the old flag if we can't compact.\n                    flags->use_old = flags->has_old;\n                    *page_id = buk->lowest_page;\n                    *page_version = buk->lowest_version;\n                    return 1;\n                }\n            }\n        }\n    }\n\n    return 0;\n}\n\n#define MIN_STORAGE_COMPACT_SLEEP 1000\n\nstruct storage_compact_wrap {\n    obj_io io;\n    pthread_mutex_t lock; // gates the bools.\n    bool done;\n    bool submitted;\n    bool miss; // version flipped out from under us\n};\n\nstatic void storage_compact_readback(void *storage, logger *l,\n        struct _compact_flags flags, char *readback_buf,\n        uint32_t page_id, uint64_t page_version, uint32_t page_offset, uint64_t read_size) {\n    uint64_t offset = 0;\n    unsigned int rescues = 0;\n    unsigned int lost = 0;\n    unsigned int skipped = 0;\n    unsigned int rescue_cold = 0;\n    unsigned int rescue_old = 0;\n\n    while (offset < read_size) {\n        item *hdr_it = NULL;\n        item_hdr *hdr = NULL;\n        item *it = (item *)(readback_buf+offset);\n        unsigned int ntotal;\n        // probably zeroed out junk at the end of the wbuf\n        if (it->nkey == 0) {\n            break;\n        }\n\n        ntotal = ITEM_ntotal(it);\n        uint32_t hv = (uint32_t)it->time;\n        item_lock(hv);\n        // We don't have a conn and don't need to do most of do_item_get\n        hdr_it = assoc_find(ITEM_key(it), it->nkey, hv);\n        if (hdr_it != NULL) {\n            bool do_write = false;\n            int bucket = flags.use_old ? PAGE_BUCKET_OLD : PAGE_BUCKET_COMPACT;\n            refcount_incr(hdr_it);\n\n            // Check validity but don't bother removing it.\n            if ((hdr_it->it_flags & ITEM_HDR) && !item_is_flushed(hdr_it) &&\n                   (hdr_it->exptime == 0 || hdr_it->exptime > current_time)) {\n                hdr = (item_hdr *)ITEM_data(hdr_it);\n                if (hdr->page_id == page_id && hdr->page_version == page_version\n                        && hdr->offset == (int)offset + page_offset) {\n                    // Item header is still completely valid.\n                    extstore_delete(storage, page_id, page_version, 1, ntotal);\n                    // special case inactive items.\n                    do_write = true;\n                    if (GET_LRU(hdr_it->slabs_clsid) == COLD_LRU) {\n                        if (flags.has_coldcompact) {\n                            // Write the cold items to a different stream.\n                            bucket = PAGE_BUCKET_COLDCOMPACT;\n                        } else if (flags.drop_unread) {\n                            do_write = false;\n                            skipped++;\n                        }\n                    }\n                }\n            }\n\n            if (do_write) {\n                bool do_update = false;\n                int tries;\n                obj_io io;\n                io.len = ntotal;\n                io.mode = OBJ_IO_WRITE;\n                for (tries = 10; tries > 0; tries--) {\n                    if (extstore_write_request(storage, bucket, bucket, &io) == 0) {\n                        memcpy(io.buf, it, io.len);\n                        extstore_write(storage, &io);\n                        do_update = true;\n                        break;\n                    } else {\n                        usleep(1000);\n                    }\n                }\n\n                if (do_update) {\n                    bool rescued = false;\n                    if (it->refcount == 2) {\n                        hdr->page_version = io.page_version;\n                        hdr->page_id = io.page_id;\n                        hdr->offset = io.offset;\n                        rescued = true;\n                    } else {\n                        // re-alloc and replace header.\n                        client_flags_t flags;\n                        FLAGS_CONV(hdr_it, flags);\n                        item *new_it = do_item_alloc(ITEM_key(hdr_it), hdr_it->nkey, flags, hdr_it->exptime, sizeof(item_hdr));\n                        if (new_it) {\n                            // need to preserve the original item flags, but we\n                            // start unlinked, with linked being added during\n                            // item_replace below.\n                            new_it->it_flags = hdr_it->it_flags & (~ITEM_LINKED);\n                            new_it->time = hdr_it->time;\n                            new_it->nbytes = hdr_it->nbytes;\n\n                            // copy the hdr data.\n                            item_hdr *new_hdr = (item_hdr *) ITEM_data(new_it);\n                            new_hdr->page_version = io.page_version;\n                            new_hdr->page_id = io.page_id;\n                            new_hdr->offset = io.offset;\n\n                            // replace the item in the hash table.\n                            item_replace(hdr_it, new_it, hv, ITEM_get_cas(hdr_it));\n                            do_item_remove(new_it); // release our reference.\n                            rescued = true;\n                        } else {\n                            lost++;\n                        }\n                    }\n\n                    if (rescued) {\n                        rescues++;\n                        if (bucket == PAGE_BUCKET_COLDCOMPACT) {\n                            rescue_cold++;\n                        } else if (bucket == PAGE_BUCKET_OLD) {\n                            rescue_old++;\n                        }\n                    }\n                } else {\n                    lost++;\n                }\n            }\n\n            do_item_remove(hdr_it);\n        }\n\n        item_unlock(hv);\n        offset += ntotal;\n        if (read_size - offset < sizeof(struct _stritem))\n            break;\n    }\n\n    STATS_LOCK();\n    stats.extstore_compact_lost += lost;\n    stats.extstore_compact_rescues += rescues;\n    stats.extstore_compact_skipped += skipped;\n    stats.extstore_compact_resc_cold += rescue_cold;\n    stats.extstore_compact_resc_old += rescue_old;\n    STATS_UNLOCK();\n    LOGGER_LOG(l, LOG_SYSEVENTS, LOGGER_COMPACT_READ_END,\n            NULL, page_id, offset, rescues, lost, skipped);\n}\n\n// wrap lock is held while waiting for this callback, preventing caller thread\n// from fast-looping.\nstatic void _storage_compact_cb(void *e, obj_io *io, int ret) {\n    struct storage_compact_wrap *wrap = (struct storage_compact_wrap *)io->data;\n    assert(wrap->submitted == true);\n\n    if (ret < 1) {\n        wrap->miss = true;\n    }\n    wrap->done = true;\n\n    pthread_mutex_unlock(&wrap->lock);\n}\n\n// TODO: hoist the storage bits from lru_maintainer_thread in here.\n// would be nice if they could avoid hammering the same locks though?\n// I guess it's only COLD. that's probably fine.\nstatic void *storage_compact_thread(void *arg) {\n    void *storage = arg;\n    bool compacting = false;\n    uint64_t page_version = 0;\n    uint64_t page_size = 0;\n    uint32_t page_offset = 0;\n    uint32_t page_id = 0;\n    struct _compact_flags flags;\n    char *readback_buf = NULL;\n    struct storage_compact_wrap wrap;\n    memset(&flags, 0, sizeof(flags));\n\n    logger *l = logger_create();\n    if (l == NULL) {\n        fprintf(stderr, \"Failed to allocate logger for storage compaction thread\\n\");\n        abort();\n    }\n\n    readback_buf = malloc(settings.ext_wbuf_size);\n    if (readback_buf == NULL) {\n        fprintf(stderr, \"Failed to allocate readback buffer for storage compaction thread\\n\");\n        abort();\n    }\n\n    pthread_mutex_init(&wrap.lock, NULL);\n    wrap.done = false;\n    wrap.submitted = false;\n    wrap.io.data = &wrap;\n    wrap.io.iov = NULL;\n    wrap.io.buf = (void *)readback_buf;\n\n    wrap.io.len = settings.ext_wbuf_size;\n    wrap.io.mode = OBJ_IO_READ;\n    wrap.io.cb = _storage_compact_cb;\n    pthread_mutex_lock(&storage_compact_plock);\n\n    while (1) {\n        if (!compacting && storage_compact_check(storage, l,\n                    &page_id, &page_version, &page_size, &flags)) {\n            page_offset = 0;\n            compacting = true;\n            LOGGER_LOG(l, LOG_SYSEVENTS, LOGGER_COMPACT_START,\n                    NULL, page_id, page_version);\n        } else {\n            pthread_cond_wait(&storage_compact_cond, &storage_compact_plock);\n        }\n\n        while (compacting) {\n            pthread_mutex_lock(&wrap.lock);\n            if (page_offset < page_size && !wrap.done && !wrap.submitted) {\n                wrap.io.page_version = page_version;\n                wrap.io.page_id = page_id;\n                wrap.io.offset = page_offset;\n                // FIXME: should be smarter about io->next (unlink at use?)\n                wrap.io.next = NULL;\n                wrap.submitted = true;\n                wrap.miss = false;\n\n                extstore_submit_bg(storage, &wrap.io);\n            } else if (wrap.miss) {\n                LOGGER_LOG(l, LOG_SYSEVENTS, LOGGER_COMPACT_ABORT,\n                        NULL, page_id);\n                wrap.done = false;\n                wrap.submitted = false;\n                compacting = false;\n            } else if (wrap.submitted && wrap.done) {\n                LOGGER_LOG(l, LOG_SYSEVENTS, LOGGER_COMPACT_READ_START,\n                        NULL, page_id, page_offset);\n                storage_compact_readback(storage, l, flags,\n                        readback_buf, page_id, page_version, page_offset,\n                        settings.ext_wbuf_size);\n                page_offset += settings.ext_wbuf_size;\n                wrap.done = false;\n                wrap.submitted = false;\n            } else if (page_offset >= page_size) {\n                compacting = false;\n                wrap.done = false;\n                wrap.submitted = false;\n                extstore_close_page(storage, page_id, page_version);\n                LOGGER_LOG(l, LOG_SYSEVENTS, LOGGER_COMPACT_END,\n                        NULL, page_id);\n                // short cooling period between defragmentation runs.\n                usleep(MIN_STORAGE_COMPACT_SLEEP);\n            }\n            pthread_mutex_unlock(&wrap.lock);\n        }\n    }\n    free(readback_buf);\n\n    return NULL;\n}\n\n// TODO\n// logger needs logger_destroy() to exist/work before this is safe.\n/*int stop_storage_compact_thread(void) {\n    int ret;\n    pthread_mutex_lock(&lru_maintainer_lock);\n    do_run_lru_maintainer_thread = 0;\n    pthread_mutex_unlock(&lru_maintainer_lock);\n    if ((ret = pthread_join(lru_maintainer_tid, NULL)) != 0) {\n        fprintf(stderr, \"Failed to stop LRU maintainer thread: %s\\n\", strerror(ret));\n        return -1;\n    }\n    settings.lru_maintainer_thread = false;\n    return 0;\n}*/\n\nvoid storage_compact_pause(void) {\n    pthread_mutex_lock(&storage_compact_plock);\n}\n\nvoid storage_compact_resume(void) {\n    pthread_mutex_unlock(&storage_compact_plock);\n}\n\nint start_storage_compact_thread(void *arg) {\n    int ret;\n\n    pthread_mutex_init(&storage_compact_plock, NULL);\n    pthread_cond_init(&storage_compact_cond, NULL);\n    if ((ret = pthread_create(&storage_compact_tid, NULL,\n        storage_compact_thread, arg)) != 0) {\n        fprintf(stderr, \"Can't create storage_compact thread: %s\\n\",\n            strerror(ret));\n        return -1;\n    }\n    thread_setname(storage_compact_tid, \"mc-ext-compact\");\n\n    return 0;\n}\n\n/*** UTILITY ***/\n// /path/to/file:100G:bucket1\n// FIXME: Modifies argument. copy instead?\nstruct extstore_conf_file *storage_conf_parse(char *arg, unsigned int page_size) {\n    struct extstore_conf_file *cf = NULL;\n    char *b = NULL;\n    char *p = strtok_r(arg, \":\", &b);\n    char unit = 0;\n    uint64_t multiplier = 0;\n    int base_size = 0;\n    if (p == NULL)\n        goto error;\n    // First arg is the filepath.\n    cf = calloc(1, sizeof(struct extstore_conf_file));\n    cf->file = strdup(p);\n\n    p = strtok_r(NULL, \":\", &b);\n    if (p == NULL) {\n        fprintf(stderr, \"must supply size to ext_path, ie: ext_path=/f/e:64m (M|G|T|P supported)\\n\");\n        goto error;\n    }\n    unit = tolower(p[strlen(p)-1]);\n    p[strlen(p)-1] = '\\0';\n    // sigh.\n    switch (unit) {\n        case 'm':\n            multiplier = 1024 * 1024;\n            break;\n        case 'g':\n            multiplier = 1024 * 1024 * 1024;\n            break;\n        case 't':\n            multiplier = 1024 * 1024;\n            multiplier *= 1024 * 1024;\n            break;\n        case 'p':\n            multiplier = 1024 * 1024;\n            multiplier *= 1024 * 1024 * 1024;\n            break;\n        default:\n            fprintf(stderr, \"must supply size to ext_path, ie: ext_path=/f/e:64m (M|G|T|P supported)\\n\");\n            goto error;\n    }\n    base_size = atoi(p);\n    multiplier *= base_size;\n    // page_count is nearest-but-not-larger-than pages * psize\n    cf->page_count = multiplier / page_size;\n    assert(page_size * cf->page_count <= multiplier);\n    if (cf->page_count == 0) {\n        fprintf(stderr, \"supplied ext_path has zero size, cannot use\\n\");\n        goto error;\n    }\n\n    // final token would be a default free bucket\n    p = strtok_r(NULL, \":\", &b);\n    // TODO: We reuse the original DEFINES for now,\n    // but if lowttl gets split up this needs to be its own set.\n    if (p != NULL) {\n        if (strcmp(p, \"compact\") == 0) {\n            cf->free_bucket = PAGE_BUCKET_COMPACT;\n        } else if (strcmp(p, \"lowttl\") == 0) {\n            cf->free_bucket = PAGE_BUCKET_LOWTTL;\n        } else if (strcmp(p, \"chunked\") == 0) {\n            cf->free_bucket = PAGE_BUCKET_CHUNKED;\n        } else if (strcmp(p, \"default\") == 0) {\n            cf->free_bucket = PAGE_BUCKET_DEFAULT;\n        } else if (strcmp(p, \"coldcompact\") == 0) {\n            cf->free_bucket = PAGE_BUCKET_COLDCOMPACT;\n        } else if (strcmp(p, \"old\") == 0) {\n            cf->free_bucket = PAGE_BUCKET_OLD;\n        } else {\n            fprintf(stderr, \"Unknown extstore bucket: %s\\n\", p);\n            goto error;\n        }\n    } else {\n        // TODO: is this necessary?\n        cf->free_bucket = PAGE_BUCKET_DEFAULT;\n    }\n\n    return cf;\nerror:\n    if (cf) {\n        if (cf->file)\n            free(cf->file);\n        free(cf);\n    }\n    return NULL;\n}\n\nstruct storage_settings {\n    struct extstore_conf_file *storage_file;\n    struct extstore_conf ext_cf;\n};\n\nvoid *storage_init_config(struct settings *s) {\n    struct storage_settings *cf = calloc(1, sizeof(struct storage_settings));\n\n    s->ext_item_size = 512;\n    s->ext_item_age = UINT_MAX;\n    s->ext_low_ttl = 0;\n    s->ext_recache_rate = 2000;\n    s->ext_max_frag = 0.8;\n    s->ext_drop_unread = false;\n    s->ext_wbuf_size = 1024 * 1024 * 4;\n    s->ext_compact_under = 0;\n    s->ext_drop_under = 0;\n    s->ext_max_sleep = 1000000;\n    s->slab_automove_freeratio = 0.01;\n    s->ext_page_size = 1024 * 1024 * 64;\n    s->ext_io_threadcount = 1;\n    cf->ext_cf.page_size = settings.ext_page_size;\n    cf->ext_cf.wbuf_size = settings.ext_wbuf_size;\n    cf->ext_cf.io_threadcount = settings.ext_io_threadcount;\n    cf->ext_cf.io_depth = 1;\n    cf->ext_cf.page_buckets = PAGE_BUCKET_COUNT;\n    cf->ext_cf.wbuf_count = cf->ext_cf.page_buckets;\n\n    return cf;\n}\n\n// TODO: pass settings struct?\nint storage_read_config(void *conf, char **subopt) {\n    struct storage_settings *cf = conf;\n    struct extstore_conf *ext_cf = &cf->ext_cf;\n    char *subopts_value;\n\n    enum {\n        EXT_PAGE_SIZE,\n        EXT_WBUF_SIZE,\n        EXT_THREADS,\n        EXT_IO_DEPTH,\n        EXT_PATH,\n        EXT_ITEM_SIZE,\n        EXT_ITEM_AGE,\n        EXT_LOW_TTL,\n        EXT_RECACHE_RATE,\n        EXT_COMPACT_UNDER,\n        EXT_DROP_UNDER,\n        EXT_MAX_SLEEP,\n        EXT_MAX_FRAG,\n        EXT_DROP_UNREAD,\n        SLAB_AUTOMOVE_FREERATIO, // FIXME: move this back?\n    };\n\n    char *const subopts_tokens[] = {\n        [EXT_PAGE_SIZE] = \"ext_page_size\",\n        [EXT_WBUF_SIZE] = \"ext_wbuf_size\",\n        [EXT_THREADS] = \"ext_threads\",\n        [EXT_IO_DEPTH] = \"ext_io_depth\",\n        [EXT_PATH] = \"ext_path\",\n        [EXT_ITEM_SIZE] = \"ext_item_size\",\n        [EXT_ITEM_AGE] = \"ext_item_age\",\n        [EXT_LOW_TTL] = \"ext_low_ttl\",\n        [EXT_RECACHE_RATE] = \"ext_recache_rate\",\n        [EXT_COMPACT_UNDER] = \"ext_compact_under\",\n        [EXT_DROP_UNDER] = \"ext_drop_under\",\n        [EXT_MAX_SLEEP] = \"ext_max_sleep\",\n        [EXT_MAX_FRAG] = \"ext_max_frag\",\n        [EXT_DROP_UNREAD] = \"ext_drop_unread\",\n        [SLAB_AUTOMOVE_FREERATIO] = \"slab_automove_freeratio\",\n        NULL\n    };\n\n    switch (getsubopt(subopt, subopts_tokens, &subopts_value)) {\n        case EXT_PAGE_SIZE:\n            if (cf->storage_file) {\n                fprintf(stderr, \"Must specify ext_page_size before any ext_path arguments\\n\");\n                return 1;\n            }\n            if (subopts_value == NULL) {\n                fprintf(stderr, \"Missing ext_page_size argument\\n\");\n                return 1;\n            }\n            if (!safe_strtoul(subopts_value, &ext_cf->page_size)) {\n                fprintf(stderr, \"could not parse argument to ext_page_size\\n\");\n                return 1;\n            }\n            ext_cf->page_size *= 1024 * 1024; /* megabytes */\n            break;\n        case EXT_WBUF_SIZE:\n            if (subopts_value == NULL) {\n                fprintf(stderr, \"Missing ext_wbuf_size argument\\n\");\n                return 1;\n            }\n            if (!safe_strtoul(subopts_value, &ext_cf->wbuf_size)) {\n                fprintf(stderr, \"could not parse argument to ext_wbuf_size\\n\");\n                return 1;\n            }\n            ext_cf->wbuf_size *= 1024 * 1024; /* megabytes */\n            settings.ext_wbuf_size = ext_cf->wbuf_size;\n            break;\n        case EXT_THREADS:\n            if (subopts_value == NULL) {\n                fprintf(stderr, \"Missing ext_threads argument\\n\");\n                return 1;\n            }\n            if (!safe_strtoul(subopts_value, &ext_cf->io_threadcount)) {\n                fprintf(stderr, \"could not parse argument to ext_threads\\n\");\n                return 1;\n            }\n            break;\n        case EXT_IO_DEPTH:\n            if (subopts_value == NULL) {\n                fprintf(stderr, \"Missing ext_io_depth argument\\n\");\n                return 1;\n            }\n            if (!safe_strtoul(subopts_value, &ext_cf->io_depth)) {\n                fprintf(stderr, \"could not parse argument to ext_io_depth\\n\");\n                return 1;\n            }\n            break;\n        case EXT_ITEM_SIZE:\n            if (subopts_value == NULL) {\n                fprintf(stderr, \"Missing ext_item_size argument\\n\");\n                return 1;\n            }\n            if (!safe_strtoul(subopts_value, &settings.ext_item_size)) {\n                fprintf(stderr, \"could not parse argument to ext_item_size\\n\");\n                return 1;\n            }\n            break;\n        case EXT_ITEM_AGE:\n            if (subopts_value == NULL) {\n                fprintf(stderr, \"Missing ext_item_age argument\\n\");\n                return 1;\n            }\n            if (!safe_strtoul(subopts_value, &settings.ext_item_age)) {\n                fprintf(stderr, \"could not parse argument to ext_item_age\\n\");\n                return 1;\n            }\n            break;\n        case EXT_LOW_TTL:\n            if (subopts_value == NULL) {\n                fprintf(stderr, \"Missing ext_low_ttl argument\\n\");\n                return 1;\n            }\n            if (!safe_strtoul(subopts_value, &settings.ext_low_ttl)) {\n                fprintf(stderr, \"could not parse argument to ext_low_ttl\\n\");\n                return 1;\n            }\n            break;\n        case EXT_RECACHE_RATE:\n            if (subopts_value == NULL) {\n                fprintf(stderr, \"Missing ext_recache_rate argument\\n\");\n                return 1;\n            }\n            if (!safe_strtoul(subopts_value, &settings.ext_recache_rate)) {\n                fprintf(stderr, \"could not parse argument to ext_recache_rate\\n\");\n                return 1;\n            }\n            break;\n        case EXT_COMPACT_UNDER:\n            if (subopts_value == NULL) {\n                fprintf(stderr, \"Missing ext_compact_under argument\\n\");\n                return 1;\n            }\n            if (!safe_strtoul(subopts_value, &settings.ext_compact_under)) {\n                fprintf(stderr, \"could not parse argument to ext_compact_under\\n\");\n                return 1;\n            }\n            break;\n        case EXT_DROP_UNDER:\n            if (subopts_value == NULL) {\n                fprintf(stderr, \"Missing ext_drop_under argument\\n\");\n                return 1;\n            }\n            if (!safe_strtoul(subopts_value, &settings.ext_drop_under)) {\n                fprintf(stderr, \"could not parse argument to ext_drop_under\\n\");\n                return 1;\n            }\n            break;\n        case EXT_MAX_SLEEP:\n            if (subopts_value == NULL) {\n                fprintf(stderr, \"Missing ext_max_sleep argument\\n\");\n                return 1;\n            }\n            if (!safe_strtoul(subopts_value, &settings.ext_max_sleep)) {\n                fprintf(stderr, \"could not parse argument to ext_max_sleep\\n\");\n                return 1;\n            }\n            break;\n        case EXT_MAX_FRAG:\n            if (subopts_value == NULL) {\n                fprintf(stderr, \"Missing ext_max_frag argument\\n\");\n                return 1;\n            }\n            if (!safe_strtod(subopts_value, &settings.ext_max_frag)) {\n                fprintf(stderr, \"could not parse argument to ext_max_frag\\n\");\n                return 1;\n            }\n            break;\n        case SLAB_AUTOMOVE_FREERATIO:\n            if (subopts_value == NULL) {\n                fprintf(stderr, \"Missing slab_automove_freeratio argument\\n\");\n                return 1;\n            }\n            if (!safe_strtod(subopts_value, &settings.slab_automove_freeratio)) {\n                fprintf(stderr, \"could not parse argument to slab_automove_freeratio\\n\");\n                return 1;\n            }\n            break;\n        case EXT_DROP_UNREAD:\n            settings.ext_drop_unread = true;\n            break;\n        case EXT_PATH:\n            if (subopts_value) {\n                struct extstore_conf_file *tmp = storage_conf_parse(subopts_value, ext_cf->page_size);\n                if (tmp == NULL) {\n                    fprintf(stderr, \"failed to parse ext_path argument\\n\");\n                    return 1;\n                }\n                if (cf->storage_file != NULL) {\n                    tmp->next = cf->storage_file;\n                }\n                cf->storage_file = tmp;\n            } else {\n                fprintf(stderr, \"missing argument to ext_path, ie: ext_path=/d/file:5G\\n\");\n                return 1;\n            }\n            break;\n        default:\n            fprintf(stderr, \"Illegal suboption \\\"%s\\\"\\n\", subopts_value);\n            return 1;\n    }\n\n    return 0;\n}\n\nint storage_check_config(void *conf) {\n    struct storage_settings *cf = conf;\n    struct extstore_conf *ext_cf = &cf->ext_cf;\n\n    if (cf->storage_file) {\n        if (settings.item_size_max > ext_cf->wbuf_size) {\n            fprintf(stderr, \"-I (item_size_max: %d) cannot be larger than ext_wbuf_size: %d\\n\",\n                settings.item_size_max, ext_cf->wbuf_size);\n            return 1;\n        }\n\n        if (settings.udpport) {\n            fprintf(stderr, \"Cannot use UDP with extstore enabled (-U 0 to disable)\\n\");\n            return 1;\n        }\n\n        return 0;\n    }\n\n    return 2;\n}\n\nvoid *storage_init(void *conf) {\n    struct storage_settings *cf = conf;\n    struct extstore_conf *ext_cf = &cf->ext_cf;\n\n    enum extstore_res eres;\n    void *storage = NULL;\n    if (settings.ext_compact_under == 0) {\n        // If changing the default fraction, change the help text as well.\n        settings.ext_compact_under = cf->storage_file->page_count * 0.01;\n        settings.ext_drop_under = cf->storage_file->page_count * 0.01;\n        if (settings.ext_compact_under < 1) {\n            settings.ext_compact_under = 1;\n        }\n        if (settings.ext_drop_under < 1) {\n            settings.ext_drop_under = 1;\n        }\n    }\n    crc32c_init();\n\n    settings.ext_global_pool_min = 0;\n    storage = extstore_init(cf->storage_file, ext_cf, &eres);\n    if (storage == NULL) {\n        fprintf(stderr, \"Failed to initialize external storage: %s\\n\",\n                extstore_err(eres));\n        if (eres == EXTSTORE_INIT_OPEN_FAIL) {\n            perror(\"extstore open\");\n        }\n        return NULL;\n    }\n\n    return storage;\n}\n\n#endif\n"
        },
        {
          "name": "storage.h",
          "type": "blob",
          "size": 1.1142578125,
          "content": "#ifndef STORAGE_H\n#define STORAGE_H\n\nvoid storage_delete(void *e, item *it);\n#ifdef EXTSTORE\n#define STORAGE_delete(e, it) \\\n    do { \\\n        storage_delete(e, it); \\\n    } while (0)\n#else\n#define STORAGE_delete(...)\n#endif\n\n// API.\nvoid storage_stats(ADD_STAT add_stats, void *c);\nvoid process_extstore_stats(ADD_STAT add_stats, void *c);\nbool storage_validate_item(void *e, item *it);\nint storage_get_item(conn *c, item *it, mc_resp *resp);\n\n// callback for the IO queue subsystem.\nvoid storage_submit_cb(io_queue_t *q);\n\n// Thread functions.\nint start_storage_write_thread(void *arg);\nvoid storage_write_pause(void);\nvoid storage_write_resume(void);\nint start_storage_compact_thread(void *arg);\nvoid storage_compact_pause(void);\nvoid storage_compact_resume(void);\n\n// Init functions.\nstruct extstore_conf_file *storage_conf_parse(char *arg, unsigned int page_size);\nvoid *storage_init_config(struct settings *s);\nint storage_read_config(void *conf, char **subopt);\nint storage_check_config(void *conf);\nvoid *storage_init(void *conf);\n\n// Ignore pointers and header bits from the CRC\n#define STORE_OFFSET offsetof(item, nbytes)\n\n#endif\n"
        },
        {
          "name": "t",
          "type": "tree",
          "content": null
        },
        {
          "name": "testapp.c",
          "type": "blob",
          "size": 77.423828125,
          "content": "/* -*- Mode: C; tab-width: 4; c-basic-offset: 4; indent-tabs-mode: nil -*- */\n#undef NDEBUG\n#include <pthread.h>\n#include <sys/types.h>\n#include <sys/socket.h>\n#include <sys/wait.h>\n#include <netdb.h>\n#include <arpa/inet.h>\n#include <netinet/in.h>\n#include <netinet/tcp.h>\n#include <signal.h>\n#include <stdio.h>\n#include <stdlib.h>\n#include <errno.h>\n#include <assert.h>\n#include <string.h>\n#include <unistd.h>\n#include <netinet/in.h>\n#include <fcntl.h>\n\n#include \"config.h\"\n#include \"cache.h\"\n#include \"crc32c.h\"\n#include \"hash.h\"\n#include \"jenkins_hash.h\"\n#include \"stats_prefix.h\"\n#include \"util.h\"\n#include \"protocol_binary.h\"\n#ifdef TLS\n#include <openssl/ssl.h>\n#endif\n\n#define TMP_TEMPLATE \"/tmp/test_file.XXXXXXX\"\n\nenum test_return { TEST_SKIP, TEST_PASS, TEST_FAIL };\n\nstruct conn {\n    int sock;\n#ifdef TLS\n    SSL_CTX   *ssl_ctx;\n    SSL    *ssl;\n#endif\n    ssize_t (*read)(struct conn  *c, void *buf, size_t count);\n    ssize_t (*write)(struct conn *c, const void *buf, size_t count);\n};\n\nhash_func hash;\n\nstatic ssize_t tcp_read(struct conn *c, void *buf, size_t count);\nstatic ssize_t tcp_write(struct conn *c, const void *buf, size_t count);\n#ifdef TLS\nstatic ssize_t ssl_read(struct conn *c, void *buf, size_t count);\nstatic ssize_t ssl_write(struct conn *c, const void *buf, size_t count);\n#endif\n\nssize_t tcp_read(struct conn *c, void *buf, size_t count) {\n    assert(c != NULL);\n    return read(c->sock, buf, count);\n}\n\nssize_t tcp_write(struct conn *c, const void *buf, size_t count) {\n    assert(c != NULL);\n    return write(c->sock, buf, count);\n}\n#ifdef TLS\nssize_t ssl_read(struct conn *c, void *buf, size_t count) {\n    assert(c != NULL);\n    return SSL_read(c->ssl, buf, count);\n}\n\nssize_t ssl_write(struct conn *c, const void *buf, size_t count) {\n    assert(c != NULL);\n    return SSL_write(c->ssl, buf, count);\n}\n#endif\n\nstatic pid_t server_pid;\nstatic in_port_t port;\nstatic struct conn *con = NULL;\nstatic bool allow_closed_read = false;\nstatic bool enable_ssl = false;\n\nstatic void close_conn(void) {\n    if (con == NULL) return;\n#ifdef TLS\n    if (con->ssl) {\n        SSL_shutdown(con->ssl);\n        SSL_free(con->ssl);\n    }\n    if (con->ssl_ctx)\n        SSL_CTX_free(con->ssl_ctx);\n#endif\n    if (con->sock > 0) close(con->sock);\n    free(con);\n    con = NULL;\n}\n\nstatic enum test_return cache_create_test(void)\n{\n    cache_t *cache = cache_create(\"test\", sizeof(uint32_t), sizeof(char*));\n    assert(cache != NULL);\n    cache_destroy(cache);\n    return TEST_PASS;\n}\n\nstatic enum test_return cache_reuse_test(void)\n{\n    int ii;\n    cache_t *cache = cache_create(\"test\", sizeof(uint32_t), sizeof(char*));\n    if (cache == NULL) {\n        return TEST_FAIL;\n    }\n    char *ptr = cache_alloc(cache);\n    cache_free(cache, ptr);\n    for (ii = 0; ii < 100; ++ii) {\n        char *p = cache_alloc(cache);\n        assert(p == ptr);\n        cache_free(cache, ptr);\n    }\n    cache_destroy(cache);\n    return TEST_PASS;\n}\n\n\nstatic enum test_return cache_bulkalloc(size_t datasize)\n{\n    cache_t *cache = cache_create(\"test\", datasize, sizeof(char*));\n    if (cache == NULL) {\n        return TEST_FAIL;\n    }\n#define ITERATIONS 1024\n    void *ptr[ITERATIONS];\n\n    for (int ii = 0; ii < ITERATIONS; ++ii) {\n        ptr[ii] = cache_alloc(cache);\n        assert(ptr[ii] != 0);\n        memset(ptr[ii], 0xff, datasize);\n    }\n\n    for (int ii = 0; ii < ITERATIONS; ++ii) {\n        cache_free(cache, ptr[ii]);\n    }\n\n#undef ITERATIONS\n    cache_destroy(cache);\n    return TEST_PASS;\n}\n\nstatic enum test_return test_issue_161(void)\n{\n    enum test_return ret = cache_bulkalloc(1);\n    if (ret == TEST_PASS) {\n        ret = cache_bulkalloc(512);\n    }\n\n    return ret;\n}\n\nstatic enum test_return cache_redzone_test(void)\n{\n#ifndef HAVE_UMEM_H\n    cache_t *cache = cache_create(\"test\", sizeof(uint32_t), sizeof(char*));\n\n    if (cache == NULL) {\n        return TEST_FAIL;\n    }\n    /* Ignore SIGABRT */\n    struct sigaction old_action;\n    struct sigaction action = { .sa_handler = SIG_IGN, .sa_flags = 0};\n    sigemptyset(&action.sa_mask);\n    sigaction(SIGABRT, &action, &old_action);\n\n    /* check memory debug.. */\n    char *p = cache_alloc(cache);\n    char old = *(p - 1);\n    *(p - 1) = 0;\n    cache_free(cache, p);\n    assert(cache_error == -1);\n    *(p - 1) = old;\n\n    p[sizeof(uint32_t)] = 0;\n    cache_free(cache, p);\n    assert(cache_error == 1);\n\n    /* restore signal handler */\n    sigaction(SIGABRT, &old_action, NULL);\n\n    cache_destroy(cache);\n\n    return TEST_PASS;\n#else\n    return TEST_SKIP;\n#endif\n}\n\nstatic enum test_return cache_limit_revised_downward_test(void)\n{\n    int limit = 10, allocated_num = limit + 1, i;\n    char ** alloc_objs = calloc(allocated_num, sizeof(char *));\n\n    cache_t *cache = cache_create(\"test\", sizeof(uint32_t), sizeof(char*));\n    assert(cache != NULL);\n\n    /* cache->limit is 0 and we can allocate limit+1 items */\n    for (i = 0; i < allocated_num; i++) {\n        alloc_objs[i] = cache_alloc(cache);\n        assert(alloc_objs[i] != NULL);\n    }\n    assert(cache->total == allocated_num);\n\n    /* revised downward cache->limit */\n    cache_set_limit(cache, limit);\n\n    /* If we free one item, the cache->total should decreased by one*/\n    cache_free(cache, alloc_objs[0]);\n\n    assert(cache->total == allocated_num-1);\n    cache_destroy(cache);\n\n    free(alloc_objs);\n\n    return TEST_PASS;\n}\n\nstatic enum test_return test_stats_prefix_find(void) {\n    PREFIX_STATS *pfs1, *pfs2;\n\n    stats_prefix_clear();\n    pfs1 = stats_prefix_find(\"abc\", 3);\n    assert(pfs1 == NULL);\n    pfs1 = stats_prefix_find(\"abc|\", 4);\n    assert(pfs1 == NULL);\n\n    pfs1 = stats_prefix_find(\"abc:\", 4);\n    assert(pfs1 != NULL);\n    assert(0ULL == (pfs1->num_gets + pfs1->num_sets + pfs1->num_deletes + pfs1->num_hits));\n    pfs2 = stats_prefix_find(\"abc:\", 4);\n    assert(pfs1 == pfs2);\n    pfs2 = stats_prefix_find(\"abc:d\", 5);\n    assert(pfs1 == pfs2);\n    pfs2 = stats_prefix_find(\"xyz123:\", 6);\n    assert(pfs1 != pfs2);\n    pfs2 = stats_prefix_find(\"ab:\", 3);\n    assert(pfs1 != pfs2);\n    return TEST_PASS;\n}\n\nstatic enum test_return test_stats_prefix_record_get(void) {\n    PREFIX_STATS *pfs;\n    stats_prefix_clear();\n\n    stats_prefix_record_get(\"abc:123\", 7, false);\n    pfs = stats_prefix_find(\"abc:123\", 7);\n    if (pfs == NULL) {\n        return TEST_FAIL;\n    }\n    assert(1 == pfs->num_gets);\n    assert(0 == pfs->num_hits);\n    stats_prefix_record_get(\"abc:456\", 7, false);\n    assert(2 == pfs->num_gets);\n    assert(0 == pfs->num_hits);\n    stats_prefix_record_get(\"abc:456\", 7, true);\n    assert(3 == pfs->num_gets);\n    assert(1 == pfs->num_hits);\n    stats_prefix_record_get(\"def:\", 4, true);\n    assert(3 == pfs->num_gets);\n    assert(1 == pfs->num_hits);\n    return TEST_PASS;\n}\n\nstatic enum test_return test_stats_prefix_record_delete(void) {\n    PREFIX_STATS *pfs;\n    stats_prefix_clear();\n\n    stats_prefix_record_delete(\"abc:123\", 7);\n    pfs = stats_prefix_find(\"abc:123\", 7);\n    if (pfs == NULL) {\n        return TEST_FAIL;\n    }\n    assert(0 == pfs->num_gets);\n    assert(0 == pfs->num_hits);\n    assert(1 == pfs->num_deletes);\n    assert(0 == pfs->num_sets);\n    stats_prefix_record_delete(\"def:\", 4);\n    assert(1 == pfs->num_deletes);\n    return TEST_PASS;\n}\n\nstatic enum test_return test_stats_prefix_record_set(void) {\n    PREFIX_STATS *pfs;\n    stats_prefix_clear();\n\n    stats_prefix_record_set(\"abc:123\", 7);\n    pfs = stats_prefix_find(\"abc:123\", 7);\n    if (pfs == NULL) {\n        return TEST_FAIL;\n    }\n    assert(0 == pfs->num_gets);\n    assert(0 == pfs->num_hits);\n    assert(0 == pfs->num_deletes);\n    assert(1 == pfs->num_sets);\n    stats_prefix_record_delete(\"def:\", 4);\n    assert(1 == pfs->num_sets);\n    return TEST_PASS;\n}\n\nstatic enum test_return test_stats_prefix_dump(void) {\n    int hashval = hash(\"abc\", 3) % PREFIX_HASH_SIZE;\n    char tmp[500];\n    char *buf;\n    const char *expected;\n    int keynum;\n    int length;\n\n    stats_prefix_clear();\n\n    assert(strcmp(\"END\\r\\n\", (buf = stats_prefix_dump(&length))) == 0);\n    assert(5 == length);\n    stats_prefix_record_set(\"abc:123\", 7);\n    free(buf);\n    expected = \"PREFIX abc get 0 hit 0 set 1 del 0\\r\\nEND\\r\\n\";\n    assert(strcmp(expected, (buf = stats_prefix_dump(&length))) == 0);\n    assert(strlen(expected) == length);\n    stats_prefix_record_get(\"abc:123\", 7, false);\n    free(buf);\n    expected = \"PREFIX abc get 1 hit 0 set 1 del 0\\r\\nEND\\r\\n\";\n    assert(strcmp(expected, (buf = stats_prefix_dump(&length))) == 0);\n    assert(strlen(expected) == length);\n    stats_prefix_record_get(\"abc:123\", 7, true);\n    free(buf);\n    expected = \"PREFIX abc get 2 hit 1 set 1 del 0\\r\\nEND\\r\\n\";\n    assert(strcmp(expected, (buf = stats_prefix_dump(&length))) == 0);\n    assert(strlen(expected) == length);\n    stats_prefix_record_delete(\"abc:123\", 7);\n    free(buf);\n    expected = \"PREFIX abc get 2 hit 1 set 1 del 1\\r\\nEND\\r\\n\";\n    assert(strcmp(expected, (buf = stats_prefix_dump(&length))) == 0);\n    assert(strlen(expected) == length);\n\n    stats_prefix_record_delete(\"def:123\", 7);\n    free(buf);\n    /* NOTE: Prefixes can be dumped in any order, so we verify that\n       each expected line is present in the string. */\n    buf = stats_prefix_dump(&length);\n    assert(strstr(buf, \"PREFIX abc get 2 hit 1 set 1 del 1\\r\\n\") != NULL);\n    assert(strstr(buf, \"PREFIX def get 0 hit 0 set 0 del 1\\r\\n\") != NULL);\n    assert(strstr(buf, \"END\\r\\n\") != NULL);\n    free(buf);\n\n    /* Find a key that hashes to the same bucket as \"abc\" */\n    bool found_match = false;\n    for (keynum = 0; keynum < PREFIX_HASH_SIZE * 100; keynum++) {\n        snprintf(tmp, sizeof(tmp), \"%d:\", keynum);\n        /* -1 because only the prefix portion is used when hashing */\n        if (hashval == hash(tmp, strlen(tmp) - 1) % PREFIX_HASH_SIZE) {\n            found_match = true;\n            break;\n        }\n    }\n    assert(found_match);\n    stats_prefix_record_set(tmp, strlen(tmp));\n    buf = stats_prefix_dump(&length);\n    assert(strstr(buf, \"PREFIX abc get 2 hit 1 set 1 del 1\\r\\n\") != NULL);\n    assert(strstr(buf, \"PREFIX def get 0 hit 0 set 0 del 1\\r\\n\") != NULL);\n    assert(strstr(buf, \"END\\r\\n\") != NULL);\n    snprintf(tmp, sizeof(tmp), \"PREFIX %d get 0 hit 0 set 1 del 0\\r\\n\", keynum);\n    assert(strstr(buf, tmp) != NULL);\n    free(buf);\n\n    /* Marking the end of these tests */\n    stats_prefix_clear();\n\n    return TEST_PASS;\n}\n\nstatic enum test_return test_safe_strtoul(void) {\n    uint32_t val;\n    assert(safe_strtoul(\"123\", &val));\n    assert(val == 123);\n    assert(safe_strtoul(\"+123\", &val));\n    assert(val == 123);\n    assert(!safe_strtoul(\"\", &val));  // empty\n    assert(!safe_strtoul(\"123BOGUS\", &val));  // non-numeric\n    assert(!safe_strtoul(\" issue221\", &val));  // non-numeric\n    /* Not sure what it does, but this works with ICC :/\n       assert(!safe_strtoul(\"92837498237498237498029383\", &val)); // out of range\n    */\n\n    // extremes:\n    assert(safe_strtoul(\"4294967295\", &val)); // 2**32 - 1\n    assert(val == 4294967295L);\n    /* This actually works on 64-bit ubuntu\n       assert(!safe_strtoul(\"4294967296\", &val)); // 2**32\n    */\n    assert(!safe_strtoul(\"-1\", &val));  // negative\n\n    // huge number, with a negative sign _past_ the value\n    if (sizeof(unsigned long) > 4) {\n        assert(safe_strtoul(\"18446744073709551615\\r\\nextrastring-morestring\", &val));\n    } else {\n        assert(safe_strtoul(\"4294967295\\r\\nextrastring-morestring\", &val));\n    }\n\n    return TEST_PASS;\n}\n\n\nstatic enum test_return test_safe_strtoull(void) {\n    uint64_t val;\n    assert(safe_strtoull(\"123\", &val));\n    assert(val == 123);\n    assert(safe_strtoull(\"+123\", &val));\n    assert(val == 123);\n    assert(!safe_strtoull(\"\", &val));  // empty\n    assert(!safe_strtoull(\"123BOGUS\", &val));  // non-numeric\n    assert(!safe_strtoull(\"92837498237498237498029383\", &val)); // out of range\n    assert(!safe_strtoull(\" issue221\", &val));  // non-numeric\n\n    // extremes:\n    assert(safe_strtoull(\"18446744073709551615\", &val)); // 2**64 - 1\n    assert(val == 18446744073709551615ULL);\n    assert(!safe_strtoull(\"18446744073709551616\", &val)); // 2**64\n    assert(!safe_strtoull(\"-1\", &val));  // negative\n\n    // huge number, with a negative sign _past_ the value\n    assert(safe_strtoull(\"18446744073709551615\\r\\nextrastring-morestring\", &val));\n    return TEST_PASS;\n}\n\nstatic enum test_return test_safe_strtoll(void) {\n    int64_t val;\n    assert(safe_strtoll(\"123\", &val));\n    assert(val == 123);\n    assert(safe_strtoll(\"+123\", &val));\n    assert(val == 123);\n    assert(safe_strtoll(\"-123\", &val));\n    assert(val == -123);\n    assert(!safe_strtoll(\"\", &val));  // empty\n    assert(!safe_strtoll(\"123BOGUS\", &val));  // non-numeric\n    assert(!safe_strtoll(\"92837498237498237498029383\", &val)); // out of range\n    assert(!safe_strtoll(\" issue221\", &val));  // non-numeric\n\n    // extremes:\n    assert(!safe_strtoll(\"18446744073709551615\", &val)); // 2**64 - 1\n    assert(safe_strtoll(\"9223372036854775807\", &val)); // 2**63 - 1\n    assert(val == 9223372036854775807LL);\n    /*\n      assert(safe_strtoll(\"-9223372036854775808\", &val)); // -2**63\n      assert(val == -9223372036854775808LL);\n    */\n    assert(!safe_strtoll(\"-9223372036854775809\", &val)); // -2**63 - 1\n\n    // We'll allow space to terminate the string.  And leading space.\n    assert(safe_strtoll(\" 123 foo\", &val));\n    assert(val == 123);\n    return TEST_PASS;\n}\n\nstatic enum test_return test_safe_strtol(void) {\n    int32_t val;\n    assert(safe_strtol(\"123\", &val));\n    assert(val == 123);\n    assert(safe_strtol(\"+123\", &val));\n    assert(val == 123);\n    assert(safe_strtol(\"-123\", &val));\n    assert(val == -123);\n    assert(!safe_strtol(\"\", &val));  // empty\n    assert(!safe_strtol(\"123BOGUS\", &val));  // non-numeric\n    assert(!safe_strtol(\"92837498237498237498029383\", &val)); // out of range\n    assert(!safe_strtol(\" issue221\", &val));  // non-numeric\n\n    // extremes:\n    /* This actually works on 64-bit ubuntu\n       assert(!safe_strtol(\"2147483648\", &val)); // (expt 2.0 31.0)\n    */\n    assert(safe_strtol(\"2147483647\", &val)); // (- (expt 2.0 31) 1)\n    assert(val == 2147483647L);\n    /* This actually works on 64-bit ubuntu\n       assert(!safe_strtol(\"-2147483649\", &val)); // (- (expt -2.0 31) 1)\n    */\n\n    // We'll allow space to terminate the string.  And leading space.\n    assert(safe_strtol(\" 123 foo\", &val));\n    assert(val == 123);\n    return TEST_PASS;\n}\n\n/**\n * Function to start the server and let it listen on a random port\n *\n * @param port_out where to store the TCP port number the server is\n *                 listening on\n * @param daemon set to true if you want to run the memcached server\n *               as a daemon process\n * @return the pid of the memcached server\n */\nstatic pid_t start_server(in_port_t *port_out, bool daemon, int timeout) {\n    char environment[80];\n    snprintf(environment, sizeof(environment),\n             \"MEMCACHED_PORT_FILENAME=/tmp/ports.%lu\", (long)getpid());\n    char *filename= environment + strlen(\"MEMCACHED_PORT_FILENAME=\");\n    char pid_file[80];\n    snprintf(pid_file, sizeof(pid_file), \"/tmp/pid.%lu\", (long)getpid());\n\n    remove(filename);\n    remove(pid_file);\n\n#ifdef __sun\n    /* I want to name the corefiles differently so that they don't\n       overwrite each other\n    */\n    char coreadm[128];\n    snprintf(coreadm, sizeof(coreadm),\n             \"coreadm -p core.%%f.%%p %lu\", (unsigned long)getpid());\n    system(coreadm);\n#endif\n\n    pid_t pid = fork();\n    assert(pid != -1);\n    if (pid == 0) {\n        /* Child */\n        char *argv[24];\n        int arg = 0;\n        char tmo[24];\n        snprintf(tmo, sizeof(tmo), \"%u\", timeout);\n\n        putenv(environment);\n#ifdef __sun\n        putenv(\"LD_PRELOAD=watchmalloc.so.1\");\n        putenv(\"MALLOC_DEBUG=WATCH\");\n#endif\n\n        if (!daemon) {\n            argv[arg++] = \"./timedrun\";\n            argv[arg++] = tmo;\n        }\n        argv[arg++] = \"./memcached-debug\";\n        argv[arg++] = \"-A\";\n        argv[arg++] = \"-p\";\n        argv[arg++] = \"-1\";\n        argv[arg++] = \"-U\";\n        argv[arg++] = \"0\";\n#ifdef TLS\n        if (enable_ssl) {\n            argv[arg++] = \"-Z\";\n            argv[arg++] = \"-o\";\n            argv[arg++] = \"ssl_chain_cert=t/server_crt.pem\";\n            argv[arg++] = \"-o\";\n            argv[arg++] = \"ssl_key=t/server_key.pem\";\n        }\n#endif\n        /* Handle rpmbuild and the like doing this as root */\n        if (getuid() == 0) {\n            argv[arg++] = \"-u\";\n            argv[arg++] = \"root\";\n        }\n        if (daemon) {\n            argv[arg++] = \"-d\";\n            argv[arg++] = \"-P\";\n            argv[arg++] = pid_file;\n        }\n#ifdef MESSAGE_DEBUG\n         argv[arg++] = \"-vvv\";\n#endif\n#ifdef HAVE_DROP_PRIVILEGES\n        argv[arg++] = \"-o\";\n        argv[arg++] = \"relaxed_privileges\";\n#endif\n        argv[arg++] = NULL;\n        assert(execv(argv[0], argv) != -1);\n    }\n\n    /* Yeah just let us \"busy-wait\" for the file to be created ;-) */\n    useconds_t wait_timeout = 1000000 * 10;\n    useconds_t wait = 1000;\n    while (access(filename, F_OK) == -1 && wait_timeout > 0) {\n        usleep(wait);\n        wait_timeout -= (wait > wait_timeout ? wait_timeout : wait);\n    }\n\n    if (access(filename, F_OK) == -1) {\n        fprintf(stderr, \"Failed to start the memcached server.\\n\");\n        assert(false);\n    }\n\n    FILE *fp = fopen(filename, \"r\");\n    if (fp == NULL) {\n        fprintf(stderr, \"Failed to open the file containing port numbers: %s\\n\",\n                strerror(errno));\n        assert(false);\n    }\n\n    *port_out = (in_port_t)-1;\n    char buffer[80];\n    while ((fgets(buffer, sizeof(buffer), fp)) != NULL) {\n        if (strncmp(buffer, \"TCP INET: \", 10) == 0) {\n            int32_t val;\n            assert(safe_strtol(buffer + 10, &val));\n            *port_out = (in_port_t)val;\n        }\n    }\n    fclose(fp);\n    assert(remove(filename) == 0);\n\n    if (daemon) {\n        /* loop and wait for the pid file.. There is a potential race\n         * condition that the server just created the file but isn't\n         * finished writing the content, so we loop a few times\n         * reading as well */\n        while (access(pid_file, F_OK) == -1) {\n            usleep(10);\n        }\n\n        fp = fopen(pid_file, \"r\");\n        if (fp == NULL) {\n            fprintf(stderr, \"Failed to open pid file: %s\\n\",\n                    strerror(errno));\n            assert(false);\n        }\n\n        /* Avoid race by retrying 20 times */\n        for (int x = 0; x < 20 && fgets(buffer, sizeof(buffer), fp) == NULL; x++) {\n            usleep(10);\n        }\n        fclose(fp);\n\n        int32_t val;\n        assert(safe_strtol(buffer, &val));\n        pid = (pid_t)val;\n    }\n\n    return pid;\n}\n\nstatic enum test_return test_issue_44(void) {\n    in_port_t port;\n    pid_t pid = start_server(&port, true, 600);\n    assert(kill(pid, SIGHUP) == 0);\n    sleep(1);\n    assert(kill(pid, SIGTERM) == 0);\n\n    return TEST_PASS;\n}\n\nstatic struct addrinfo *lookuphost(const char *hostname, in_port_t port)\n{\n    struct addrinfo *ai = 0;\n    struct addrinfo hints = { .ai_family = AF_UNSPEC,\n                              .ai_protocol = IPPROTO_TCP,\n                              .ai_socktype = SOCK_STREAM };\n    char service[NI_MAXSERV];\n    int error;\n\n    (void)snprintf(service, NI_MAXSERV, \"%d\", port);\n    if ((error = getaddrinfo(hostname, service, &hints, &ai)) != 0) {\n       if (error != EAI_SYSTEM) {\n          fprintf(stderr, \"getaddrinfo(): %s\\n\", gai_strerror(error));\n       } else {\n          perror(\"getaddrinfo()\");\n       }\n    }\n\n    return ai;\n}\n\nstatic struct conn *connect_server(const char *hostname, in_port_t port,\n                            bool nonblock, const bool ssl)\n{\n    struct conn *c;\n    if (!(c = (struct conn *)calloc(1, sizeof(struct conn)))) {\n        fprintf(stderr, \"Failed to allocate the client connection: %s\\n\",\n                strerror(errno));\n        return NULL;\n    }\n\n    struct addrinfo *ai = lookuphost(hostname, port);\n    int sock = -1;\n    if (ai != NULL) {\n       if ((sock = socket(ai->ai_family, ai->ai_socktype,\n                          ai->ai_protocol)) != -1) {\n          if (connect(sock, ai->ai_addr, ai->ai_addrlen) == -1) {\n             fprintf(stderr, \"Failed to connect socket: %s\\n\",\n                     strerror(errno));\n             close(sock);\n             sock = -1;\n          } else if (nonblock) {\n              int flags = fcntl(sock, F_GETFL, 0);\n              if (flags < 0 || fcntl(sock, F_SETFL, flags | O_NONBLOCK) < 0) {\n                  fprintf(stderr, \"Failed to enable nonblocking mode: %s\\n\",\n                          strerror(errno));\n                  close(sock);\n                  sock = -1;\n              }\n          }\n       } else {\n          fprintf(stderr, \"Failed to create socket: %s\\n\", strerror(errno));\n       }\n\n       freeaddrinfo(ai);\n    }\n    c->sock = sock;\n#ifdef TLS\n    if (sock > 0 && ssl) {\n        c->ssl_ctx = SSL_CTX_new(SSLv23_client_method());\n        if (c->ssl_ctx == NULL) {\n            fprintf(stderr, \"Failed to create the SSL context: %s\\n\",\n                strerror(errno));\n            close(sock);\n            sock = -1;\n        }\n        c->ssl = SSL_new(c->ssl_ctx);\n        if (c->ssl == NULL) {\n            fprintf(stderr, \"Failed to create the SSL object: %s\\n\",\n                strerror(errno));\n            close(sock);\n            sock = -1;\n        }\n        SSL_set_fd (c->ssl, c->sock);\n        int ret = SSL_connect(c->ssl);\n        if (ret < 0) {\n            int err = SSL_get_error(c->ssl, ret);\n            if (err == SSL_ERROR_SYSCALL || err == SSL_ERROR_SSL) {\n                fprintf(stderr, \"SSL connection failed with error code : %s\\n\",\n                    strerror(errno));\n                close(sock);\n                sock = -1;\n            }\n        }\n        c->read = ssl_read;\n        c->write = ssl_write;\n    } else\n#endif\n    {\n        c->read = tcp_read;\n        c->write = tcp_write;\n    }\n    return c;\n}\n\nstatic enum test_return test_vperror(void) {\n    int rv = 0;\n    int oldstderr = dup(STDERR_FILENO);\n    assert(oldstderr >= 0);\n    char tmpl[sizeof(TMP_TEMPLATE)+1];\n    strncpy(tmpl, TMP_TEMPLATE, sizeof(TMP_TEMPLATE)+1);\n\n    int newfile = mkstemp(tmpl);\n    assert(newfile > 0);\n    rv = dup2(newfile, STDERR_FILENO);\n    assert(rv == STDERR_FILENO);\n    rv = close(newfile);\n    assert(rv == 0);\n\n    errno = EIO;\n    vperror(\"Old McDonald had a farm.  %s\", \"EI EIO\");\n\n    /* Restore stderr */\n    rv = dup2(oldstderr, STDERR_FILENO);\n    assert(rv == STDERR_FILENO);\n\n\n    /* Go read the file */\n    char buf[80] = { 0 };\n    FILE *efile = fopen(tmpl, \"r\");\n    assert(efile);\n    char *prv = fgets(buf, sizeof(buf), efile);\n    assert(prv);\n    fclose(efile);\n\n    unlink(tmpl);\n\n    char expected[80] = { 0 };\n    snprintf(expected, sizeof(expected),\n             \"Old McDonald had a farm.  EI EIO: %s\\n\", strerror(EIO));\n\n    /*\n    fprintf(stderr,\n            \"\\nExpected:  ``%s''\"\n            \"\\nGot:       ``%s''\\n\", expected, buf);\n    */\n\n    return strcmp(expected, buf) == 0 ? TEST_PASS : TEST_FAIL;\n}\n\nstatic void send_ascii_command(const char *buf) {\n    off_t offset = 0;\n    const char* ptr = buf;\n    size_t len = strlen(buf);\n\n    do {\n        ssize_t nw = con->write((void*)con, ptr + offset, len - offset);\n        if (nw == -1) {\n            if (errno != EINTR) {\n                fprintf(stderr, \"Failed to write: %s\\n\", strerror(errno));\n                abort();\n            }\n        } else {\n            offset += nw;\n        }\n    } while (offset < len);\n}\n\n/*\n * This is a dead slow single byte read, but it should only read out\n * _one_ response and I don't have an input buffer... The current\n * implementation only supports single-line responses, so if you want to use\n * it for get commands you need to implement that first ;-)\n */\nstatic void read_ascii_response(char *buffer, size_t size) {\n    off_t offset = 0;\n    bool need_more = true;\n    do {\n        ssize_t nr = con->read(con, buffer + offset, 1);\n        if (nr == -1) {\n            if (errno != EINTR) {\n                fprintf(stderr, \"Failed to read: %s\\n\", strerror(errno));\n                abort();\n            }\n        } else {\n            assert(nr == 1);\n            if (buffer[offset] == '\\n') {\n                need_more = false;\n                buffer[offset + 1] = '\\0';\n            }\n            offset += nr;\n            assert(offset + 1 < size);\n        }\n    } while (need_more);\n}\n\nstatic enum test_return test_issue_92(void) {\n    char buffer[1024];\n\n    close_conn();\n    con = connect_server(\"127.0.0.1\", port, false, enable_ssl);\n    assert(con);\n\n    send_ascii_command(\"stats cachedump 1 0 0\\r\\n\");\n\n    read_ascii_response(buffer, sizeof(buffer));\n    assert(strncmp(buffer, \"END\", strlen(\"END\")) == 0);\n\n    send_ascii_command(\"stats cachedump 200 0 0\\r\\n\");\n    read_ascii_response(buffer, sizeof(buffer));\n    assert(strncmp(buffer, \"CLIENT_ERROR\", strlen(\"CLIENT_ERROR\")) == 0);\n\n    close_conn();\n    con = connect_server(\"127.0.0.1\", port, false, enable_ssl);\n    assert(con);\n    return TEST_PASS;\n}\n\nstatic enum test_return test_crc32c(void) {\n    uint32_t crc_hw, crc_sw;\n\n    char buffer[256];\n    for (int x = 0; x < 256; x++)\n        buffer[x] = x;\n\n    /* Compare hardware to software implementation */\n    crc_hw = crc32c(0, buffer, 256);\n    crc_sw = crc32c_sw(0, buffer, 256);\n    assert(crc_hw == 0x9c44184b);\n    assert(crc_sw == 0x9c44184b);\n\n    /* Test that passing a CRC in also works */\n    crc_hw = crc32c(crc_hw, buffer, 256);\n    crc_sw = crc32c_sw(crc_sw, buffer, 256);\n    assert(crc_hw == 0xae10ee5a);\n    assert(crc_sw == 0xae10ee5a);\n\n    /* Test odd offsets/sizes */\n    crc_hw = crc32c(crc_hw, buffer + 1, 256 - 2);\n    crc_sw = crc32c_sw(crc_sw, buffer + 1, 256 - 2);\n    assert(crc_hw == 0xed37b906);\n    assert(crc_sw == 0xed37b906);\n\n    return TEST_PASS;\n}\n\nstatic enum test_return test_issue_102(void) {\n    char buffer[4096];\n    memset(buffer, ' ', sizeof(buffer));\n    buffer[sizeof(buffer) - 1] = '\\0';\n\n    close_conn();\n    con = connect_server(\"127.0.0.1\", port, false, enable_ssl);\n    assert(con);\n\n    send_ascii_command(buffer);\n    /* verify that the server closed the connection */\n    assert(con->read(con, buffer, sizeof(buffer)) == 0);\n\n    close_conn();\n    con = connect_server(\"127.0.0.1\", port, false, enable_ssl);\n    assert(con);\n\n    snprintf(buffer, sizeof(buffer), \"gets \");\n    size_t offset = 5;\n    while (offset < 4000) {\n        offset += snprintf(buffer + offset, sizeof(buffer) - offset,\n                           \"%010u \", (unsigned int)offset);\n    }\n\n    send_ascii_command(buffer);\n    usleep(250);\n\n    send_ascii_command(\"\\r\\n\");\n    char rsp[80];\n    read_ascii_response(rsp, sizeof(rsp));\n    assert(strncmp(rsp, \"END\", strlen(\"END\")) == 0);\n    buffer[3]= ' ';\n    send_ascii_command(buffer);\n    usleep(250);\n    send_ascii_command(\"\\r\\n\");\n    read_ascii_response(rsp, sizeof(rsp));\n    assert(strncmp(rsp, \"END\", strlen(\"END\")) == 0);\n\n    memset(buffer, ' ', sizeof(buffer));\n    int len = snprintf(buffer + 101, sizeof(buffer) - 101, \"gets foo\");\n    buffer[101 + len] = ' ';\n    buffer[sizeof(buffer) - 1] = '\\0';\n    send_ascii_command(buffer);\n    /* verify that the server closed the connection */\n    assert(con->read(con, buffer, sizeof(buffer)) == 0);\n\n    close_conn();\n    con = connect_server(\"127.0.0.1\", port, false, enable_ssl);\n    assert(con);\n\n    return TEST_PASS;\n}\n\nstatic enum test_return start_memcached_server(void) {\n    server_pid = start_server(&port, false, 600);\n    close_conn();\n    con = connect_server(\"127.0.0.1\", port, false, enable_ssl);\n    assert(con);\n    return TEST_PASS;\n}\n\nstatic enum test_return stop_memcached_server(void) {\n    close_conn();\n    if (server_pid != -1) {\n        assert(kill(server_pid, SIGTERM) == 0);\n    }\n\n    return TEST_PASS;\n}\n\nstatic enum test_return shutdown_memcached_server(void) {\n    char buffer[1024];\n\n    close_conn();\n    con = connect_server(\"127.0.0.1\", port, false, enable_ssl);\n    assert(con);\n\n    send_ascii_command(\"shutdown\\r\\n\");\n    /* verify that the server closed the connection */\n    assert(con->read(con, buffer, sizeof(buffer)) == 0);\n\n    close_conn();\n\n    /* We set server_pid to -1 so that we don't later call kill() */\n    if (kill(server_pid, 0) == 0) {\n        server_pid = -1;\n    }\n\n    return TEST_PASS;\n}\n\nstatic void safe_send(const void* buf, size_t len, bool hickup)\n{\n    off_t offset = 0;\n    const char* ptr = buf;\n#ifdef MESSAGE_DEBUG\n    uint8_t val = *ptr;\n    assert(val == (uint8_t)0x80);\n    fprintf(stderr, \"About to send %lu bytes:\", (unsigned long)len);\n    for (int ii = 0; ii < len; ++ii) {\n        if (ii % 4 == 0) {\n            fprintf(stderr, \"\\n   \");\n        }\n        val = *(ptr + ii);\n        fprintf(stderr, \" 0x%02x\", val);\n    }\n    fprintf(stderr, \"\\n\");\n    usleep(500);\n#endif\n\n    do {\n        size_t num_bytes = len - offset;\n        if (hickup) {\n            if (num_bytes > 1024) {\n                num_bytes = (rand() % 1023) + 1;\n            }\n        }\n        ssize_t nw = con->write(con, ptr + offset, num_bytes);\n        if (nw == -1) {\n            if (errno != EINTR) {\n                fprintf(stderr, \"Failed to write: %s\\n\", strerror(errno));\n                abort();\n            }\n        } else {\n            if (hickup) {\n                usleep(100);\n            }\n            offset += nw;\n        }\n    } while (offset < len);\n}\n\nstatic bool safe_recv(void *buf, size_t len) {\n    if (len == 0) {\n        return true;\n    }\n    off_t offset = 0;\n    do {\n        ssize_t nr = con->read(con, ((char*)buf) + offset, len - offset);\n        if (nr == -1) {\n            if (errno != EINTR) {\n                fprintf(stderr, \"Failed to read: %s\\n\", strerror(errno));\n                abort();\n            }\n        } else {\n            if (nr == 0 && allow_closed_read) {\n                return false;\n            }\n            assert(nr != 0);\n            offset += nr;\n        }\n    } while (offset < len);\n\n    return true;\n}\n\nstatic bool safe_recv_packet(void *buf, size_t size) {\n    protocol_binary_response_no_extras *response = buf;\n    assert(size > sizeof(*response));\n    if (!safe_recv(response, sizeof(*response))) {\n        return false;\n    }\n    response->message.header.response.keylen = ntohs(response->message.header.response.keylen);\n    response->message.header.response.status = ntohs(response->message.header.response.status);\n    response->message.header.response.bodylen = ntohl(response->message.header.response.bodylen);\n\n    size_t len = sizeof(*response);\n\n    char *ptr = buf;\n    ptr += len;\n    if (!safe_recv(ptr, response->message.header.response.bodylen)) {\n        return false;\n    }\n\n#ifdef MESSAGE_DEBUG\n    usleep(500);\n    ptr = buf;\n    len += response->message.header.response.bodylen;\n    uint8_t val = *ptr;\n    assert(val == (uint8_t)0x81);\n    fprintf(stderr, \"Received %lu bytes:\", (unsigned long)len);\n    for (int ii = 0; ii < len; ++ii) {\n        if (ii % 4 == 0) {\n            fprintf(stderr, \"\\n   \");\n        }\n        val = *(ptr + ii);\n        fprintf(stderr, \" 0x%02x\", val);\n    }\n    fprintf(stderr, \"\\n\");\n#endif\n    return true;\n}\n\nstatic off_t storage_command(char*buf,\n                             size_t bufsz,\n                             uint8_t cmd,\n                             const void* key,\n                             size_t keylen,\n                             const void* dta,\n                             size_t dtalen,\n                             uint32_t flags,\n                             uint32_t exp) {\n    /* all of the storage commands use the same command layout */\n    protocol_binary_request_set *request = (void*)buf;\n    assert(bufsz > sizeof(*request) + keylen + dtalen);\n\n    memset(request, 0, sizeof(*request));\n    request->message.header.request.magic = PROTOCOL_BINARY_REQ;\n    request->message.header.request.opcode = cmd;\n    request->message.header.request.keylen = htons(keylen);\n    request->message.header.request.extlen = 8;\n    request->message.header.request.bodylen = htonl(keylen + 8 + dtalen);\n    request->message.header.request.opaque = 0xdeadbeef;\n    request->message.body.flags = flags;\n    request->message.body.expiration = exp;\n\n    off_t key_offset = sizeof(protocol_binary_request_no_extras) + 8;\n\n    memcpy(buf + key_offset, key, keylen);\n    if (dta != NULL) {\n        memcpy(buf + key_offset + keylen, dta, dtalen);\n    }\n\n    return key_offset + keylen + dtalen;\n}\n\nstatic off_t ext_command(char* buf,\n                         size_t bufsz,\n                         uint8_t cmd,\n                         const void* ext,\n                         size_t extlen,\n                         const void* key,\n                         size_t keylen,\n                         const void* dta,\n                         size_t dtalen) {\n    protocol_binary_request_no_extras *request = (void*)buf;\n    assert(bufsz > sizeof(*request) + extlen + keylen + dtalen);\n\n    memset(request, 0, sizeof(*request));\n    request->message.header.request.magic = PROTOCOL_BINARY_REQ;\n    request->message.header.request.opcode = cmd;\n    request->message.header.request.extlen = extlen;\n    request->message.header.request.keylen = htons(keylen);\n    request->message.header.request.bodylen = htonl(extlen + keylen + dtalen);\n    request->message.header.request.opaque = 0xdeadbeef;\n\n    off_t ext_offset = sizeof(protocol_binary_request_no_extras);\n    off_t key_offset = ext_offset + extlen;\n    off_t dta_offset = key_offset + keylen;\n\n    if (ext != NULL) {\n        memcpy(buf + ext_offset, ext, extlen);\n    }\n    if (key != NULL) {\n        memcpy(buf + key_offset, key, keylen);\n    }\n    if (dta != NULL) {\n        memcpy(buf + dta_offset, dta, dtalen);\n    }\n\n    return sizeof(*request) + extlen + keylen + dtalen;\n}\n\nstatic off_t raw_command(char* buf,\n                         size_t bufsz,\n                         uint8_t cmd,\n                         const void* key,\n                         size_t keylen,\n                         const void* dta,\n                         size_t dtalen) {\n    /* all of the storage commands use the same command layout */\n    return ext_command(buf, bufsz, cmd, NULL, 0, key, keylen, dta, dtalen);\n}\n\nstatic off_t flush_command(char* buf, size_t bufsz, uint8_t cmd, uint32_t exptime, bool use_extra) {\n    protocol_binary_request_flush *request = (void*)buf;\n    assert(bufsz > sizeof(*request));\n\n    memset(request, 0, sizeof(*request));\n    request->message.header.request.magic = PROTOCOL_BINARY_REQ;\n    request->message.header.request.opcode = cmd;\n\n    off_t size = sizeof(protocol_binary_request_no_extras);\n    if (use_extra) {\n        request->message.header.request.extlen = 4;\n        request->message.body.expiration = htonl(exptime);\n        request->message.header.request.bodylen = htonl(4);\n        size += 4;\n    }\n\n    request->message.header.request.opaque = 0xdeadbeef;\n\n    return size;\n}\n\n\nstatic off_t touch_command(char* buf,\n                           size_t bufsz,\n                           uint8_t cmd,\n                           const void* key,\n                           size_t keylen,\n                           uint32_t exptime) {\n    protocol_binary_request_touch *request = (void*)buf;\n    assert(bufsz > sizeof(*request));\n\n    memset(request, 0, sizeof(*request));\n    request->message.header.request.magic = PROTOCOL_BINARY_REQ;\n    request->message.header.request.opcode = cmd;\n\n    request->message.header.request.keylen = htons(keylen);\n    request->message.header.request.extlen = 4;\n    request->message.body.expiration = htonl(exptime);\n    request->message.header.request.bodylen = htonl(keylen + 4);\n\n    request->message.header.request.opaque = 0xdeadbeef;\n\n    off_t key_offset = sizeof(protocol_binary_request_no_extras) + 4;\n\n    memcpy(buf + key_offset, key, keylen);\n    return sizeof(protocol_binary_request_no_extras) + 4 + keylen;\n}\n\nstatic off_t arithmetic_command(char* buf,\n                                size_t bufsz,\n                                uint8_t cmd,\n                                const void* key,\n                                size_t keylen,\n                                uint64_t delta,\n                                uint64_t initial,\n                                uint32_t exp) {\n    protocol_binary_request_incr *request = (void*)buf;\n    assert(bufsz > sizeof(*request) + keylen);\n\n    memset(request, 0, sizeof(*request));\n    request->message.header.request.magic = PROTOCOL_BINARY_REQ;\n    request->message.header.request.opcode = cmd;\n    request->message.header.request.keylen = htons(keylen);\n    request->message.header.request.extlen = 20;\n    request->message.header.request.bodylen = htonl(keylen + 20);\n    request->message.header.request.opaque = 0xdeadbeef;\n    request->message.body.delta = htonll(delta);\n    request->message.body.initial = htonll(initial);\n    request->message.body.expiration = htonl(exp);\n\n    off_t key_offset = sizeof(protocol_binary_request_no_extras) + 20;\n\n    memcpy(buf + key_offset, key, keylen);\n    return key_offset + keylen;\n}\n\nstatic void validate_response_header(protocol_binary_response_no_extras *response,\n                                     uint8_t cmd, uint16_t status)\n{\n    assert(response->message.header.response.magic == PROTOCOL_BINARY_RES);\n    assert(response->message.header.response.opcode == cmd);\n    assert(response->message.header.response.datatype == PROTOCOL_BINARY_RAW_BYTES);\n    assert(response->message.header.response.status == status);\n    assert(response->message.header.response.opaque == 0xdeadbeef);\n\n    if (status == PROTOCOL_BINARY_RESPONSE_SUCCESS) {\n        switch (cmd) {\n        case PROTOCOL_BINARY_CMD_ADDQ:\n        case PROTOCOL_BINARY_CMD_APPENDQ:\n        case PROTOCOL_BINARY_CMD_DECREMENTQ:\n        case PROTOCOL_BINARY_CMD_DELETEQ:\n        case PROTOCOL_BINARY_CMD_FLUSHQ:\n        case PROTOCOL_BINARY_CMD_INCREMENTQ:\n        case PROTOCOL_BINARY_CMD_PREPENDQ:\n        case PROTOCOL_BINARY_CMD_QUITQ:\n        case PROTOCOL_BINARY_CMD_REPLACEQ:\n        case PROTOCOL_BINARY_CMD_SETQ:\n            assert(\"Quiet command shouldn't return on success\" == NULL);\n        default:\n            break;\n        }\n\n        switch (cmd) {\n        case PROTOCOL_BINARY_CMD_ADD:\n        case PROTOCOL_BINARY_CMD_REPLACE:\n        case PROTOCOL_BINARY_CMD_SET:\n        case PROTOCOL_BINARY_CMD_APPEND:\n        case PROTOCOL_BINARY_CMD_PREPEND:\n            assert(response->message.header.response.keylen == 0);\n            assert(response->message.header.response.extlen == 0);\n            assert(response->message.header.response.bodylen == 0);\n            assert(response->message.header.response.cas != 0);\n            break;\n        case PROTOCOL_BINARY_CMD_FLUSH:\n        case PROTOCOL_BINARY_CMD_NOOP:\n        case PROTOCOL_BINARY_CMD_QUIT:\n        case PROTOCOL_BINARY_CMD_DELETE:\n            assert(response->message.header.response.keylen == 0);\n            assert(response->message.header.response.extlen == 0);\n            assert(response->message.header.response.bodylen == 0);\n            assert(response->message.header.response.cas == 0);\n            break;\n\n        case PROTOCOL_BINARY_CMD_DECREMENT:\n        case PROTOCOL_BINARY_CMD_INCREMENT:\n            assert(response->message.header.response.keylen == 0);\n            assert(response->message.header.response.extlen == 0);\n            assert(response->message.header.response.bodylen == 8);\n            assert(response->message.header.response.cas != 0);\n            break;\n\n        case PROTOCOL_BINARY_CMD_STAT:\n            assert(response->message.header.response.extlen == 0);\n            /* key and value exists in all packets except in the terminating */\n            assert(response->message.header.response.cas == 0);\n            break;\n\n        case PROTOCOL_BINARY_CMD_VERSION:\n            assert(response->message.header.response.keylen == 0);\n            assert(response->message.header.response.extlen == 0);\n            assert(response->message.header.response.bodylen != 0);\n            assert(response->message.header.response.cas == 0);\n            break;\n\n        case PROTOCOL_BINARY_CMD_GET:\n        case PROTOCOL_BINARY_CMD_GETQ:\n        case PROTOCOL_BINARY_CMD_GAT:\n        case PROTOCOL_BINARY_CMD_GATQ:\n            assert(response->message.header.response.keylen == 0);\n            assert(response->message.header.response.extlen == 4);\n            assert(response->message.header.response.cas != 0);\n            break;\n\n        case PROTOCOL_BINARY_CMD_GETK:\n        case PROTOCOL_BINARY_CMD_GETKQ:\n        case PROTOCOL_BINARY_CMD_GATK:\n        case PROTOCOL_BINARY_CMD_GATKQ:\n            assert(response->message.header.response.keylen != 0);\n            assert(response->message.header.response.extlen == 4);\n            assert(response->message.header.response.cas != 0);\n            break;\n\n        default:\n            /* Undefined command code */\n            break;\n        }\n    } else {\n        assert(response->message.header.response.cas == 0);\n        assert(response->message.header.response.extlen == 0);\n        if (cmd != PROTOCOL_BINARY_CMD_GETK &&\n            cmd != PROTOCOL_BINARY_CMD_GATK) {\n            assert(response->message.header.response.keylen == 0);\n        }\n    }\n}\n\nstatic enum test_return test_binary_noop(void) {\n    union {\n        protocol_binary_request_no_extras request;\n        protocol_binary_response_no_extras response;\n        char bytes[1024];\n    } buffer;\n\n    size_t len = raw_command(buffer.bytes, sizeof(buffer.bytes),\n                             PROTOCOL_BINARY_CMD_NOOP,\n                             NULL, 0, NULL, 0);\n\n    safe_send(buffer.bytes, len, false);\n    safe_recv_packet(buffer.bytes, sizeof(buffer.bytes));\n    validate_response_header(&buffer.response, PROTOCOL_BINARY_CMD_NOOP,\n                             PROTOCOL_BINARY_RESPONSE_SUCCESS);\n\n    return TEST_PASS;\n}\n\nstatic enum test_return test_binary_quit_impl(uint8_t cmd) {\n    union {\n        protocol_binary_request_no_extras request;\n        protocol_binary_response_no_extras response;\n        char bytes[1024];\n    } buffer;\n    size_t len = raw_command(buffer.bytes, sizeof(buffer.bytes),\n                             cmd, NULL, 0, NULL, 0);\n\n    safe_send(buffer.bytes, len, false);\n    if (cmd == PROTOCOL_BINARY_CMD_QUIT) {\n        safe_recv_packet(buffer.bytes, sizeof(buffer.bytes));\n        validate_response_header(&buffer.response, PROTOCOL_BINARY_CMD_QUIT,\n                                 PROTOCOL_BINARY_RESPONSE_SUCCESS);\n    }\n\n    /* Socket should be closed now, read should return 0 */\n    assert(con->read(con, buffer.bytes, sizeof(buffer.bytes)) == 0);\n    close_conn();\n    con = connect_server(\"127.0.0.1\", port, false, enable_ssl);\n    assert(con);\n\n    return TEST_PASS;\n}\n\nstatic enum test_return test_binary_quit(void) {\n    return test_binary_quit_impl(PROTOCOL_BINARY_CMD_QUIT);\n}\n\nstatic enum test_return test_binary_quitq(void) {\n    return test_binary_quit_impl(PROTOCOL_BINARY_CMD_QUITQ);\n}\n\nstatic enum test_return test_binary_set_impl(const char *key, uint8_t cmd) {\n    union {\n        protocol_binary_request_no_extras request;\n        protocol_binary_response_no_extras response;\n        char bytes[1024];\n    } send, receive;\n    uint64_t value = 0xdeadbeefdeadcafe;\n    size_t len = storage_command(send.bytes, sizeof(send.bytes), cmd,\n                                 key, strlen(key), &value, sizeof(value),\n                                 0, 0);\n\n    /* Set should work over and over again */\n    int ii;\n    for (ii = 0; ii < 10; ++ii) {\n        safe_send(send.bytes, len, false);\n        if (cmd == PROTOCOL_BINARY_CMD_SET) {\n            safe_recv_packet(receive.bytes, sizeof(receive.bytes));\n            validate_response_header(&receive.response, cmd,\n                                     PROTOCOL_BINARY_RESPONSE_SUCCESS);\n        }\n    }\n\n    if (cmd == PROTOCOL_BINARY_CMD_SETQ) {\n        return test_binary_noop();\n    }\n\n    send.request.message.header.request.cas = receive.response.message.header.response.cas;\n    safe_send(send.bytes, len, false);\n    if (cmd == PROTOCOL_BINARY_CMD_SET) {\n        safe_recv_packet(receive.bytes, sizeof(receive.bytes));\n        validate_response_header(&receive.response, cmd,\n                                 PROTOCOL_BINARY_RESPONSE_SUCCESS);\n        assert(receive.response.message.header.response.cas != send.request.message.header.request.cas);\n    } else {\n        return test_binary_noop();\n    }\n\n    return TEST_PASS;\n}\n\nstatic enum test_return test_binary_set(void) {\n    return test_binary_set_impl(\"test_binary_set\", PROTOCOL_BINARY_CMD_SET);\n}\n\nstatic enum test_return test_binary_setq(void) {\n    return test_binary_set_impl(\"test_binary_setq\", PROTOCOL_BINARY_CMD_SETQ);\n}\n\n\nstatic enum test_return test_binary_add_impl(const char *key, uint8_t cmd) {\n    uint64_t value = 0xdeadbeefdeadcafe;\n    union {\n        protocol_binary_request_no_extras request;\n        protocol_binary_response_no_extras response;\n        char bytes[1024];\n    } send, receive;\n    size_t len = storage_command(send.bytes, sizeof(send.bytes), cmd, key,\n                                 strlen(key), &value, sizeof(value),\n                                 0, 0);\n\n    /* Add should only work the first time */\n    int ii;\n    for (ii = 0; ii < 10; ++ii) {\n        safe_send(send.bytes, len, false);\n        if (ii == 0) {\n            if (cmd == PROTOCOL_BINARY_CMD_ADD) {\n                safe_recv_packet(receive.bytes, sizeof(receive.bytes));\n                validate_response_header(&receive.response, cmd,\n                                         PROTOCOL_BINARY_RESPONSE_SUCCESS);\n            }\n        } else {\n            safe_recv_packet(receive.bytes, sizeof(receive.bytes));\n            validate_response_header(&receive.response, cmd,\n                                     PROTOCOL_BINARY_RESPONSE_KEY_EEXISTS);\n        }\n    }\n\n    return TEST_PASS;\n}\n\nstatic enum test_return test_binary_add(void) {\n    return test_binary_add_impl(\"test_binary_add\", PROTOCOL_BINARY_CMD_ADD);\n}\n\nstatic enum test_return test_binary_addq(void) {\n    return test_binary_add_impl(\"test_binary_addq\", PROTOCOL_BINARY_CMD_ADDQ);\n}\n\nstatic enum test_return test_binary_replace_impl(const char* key, uint8_t cmd) {\n    uint64_t value = 0xdeadbeefdeadcafe;\n    union {\n        protocol_binary_request_no_extras request;\n        protocol_binary_response_no_extras response;\n        char bytes[1024];\n    } send, receive;\n    size_t len = storage_command(send.bytes, sizeof(send.bytes), cmd,\n                                 key, strlen(key), &value, sizeof(value),\n                                 0, 0);\n    safe_send(send.bytes, len, false);\n    safe_recv_packet(receive.bytes, sizeof(receive.bytes));\n    validate_response_header(&receive.response, cmd,\n                             PROTOCOL_BINARY_RESPONSE_KEY_ENOENT);\n    len = storage_command(send.bytes, sizeof(send.bytes),\n                          PROTOCOL_BINARY_CMD_ADD,\n                          key, strlen(key), &value, sizeof(value), 0, 0);\n    safe_send(send.bytes, len, false);\n    safe_recv_packet(receive.bytes, sizeof(receive.bytes));\n    validate_response_header(&receive.response, PROTOCOL_BINARY_CMD_ADD,\n                             PROTOCOL_BINARY_RESPONSE_SUCCESS);\n\n    len = storage_command(send.bytes, sizeof(send.bytes), cmd,\n                          key, strlen(key), &value, sizeof(value), 0, 0);\n    int ii;\n    for (ii = 0; ii < 10; ++ii) {\n        safe_send(send.bytes, len, false);\n        if (cmd == PROTOCOL_BINARY_CMD_REPLACE) {\n            safe_recv_packet(receive.bytes, sizeof(receive.bytes));\n            validate_response_header(&receive.response,\n                                     PROTOCOL_BINARY_CMD_REPLACE,\n                                     PROTOCOL_BINARY_RESPONSE_SUCCESS);\n        }\n    }\n\n    if (cmd == PROTOCOL_BINARY_CMD_REPLACEQ) {\n        test_binary_noop();\n    }\n\n    return TEST_PASS;\n}\n\nstatic enum test_return test_binary_replace(void) {\n    return test_binary_replace_impl(\"test_binary_replace\",\n                                    PROTOCOL_BINARY_CMD_REPLACE);\n}\n\nstatic enum test_return test_binary_replaceq(void) {\n    return test_binary_replace_impl(\"test_binary_replaceq\",\n                                    PROTOCOL_BINARY_CMD_REPLACEQ);\n}\n\nstatic enum test_return test_binary_delete_impl(const char *key, uint8_t cmd) {\n    union {\n        protocol_binary_request_no_extras request;\n        protocol_binary_response_no_extras response;\n        char bytes[1024];\n    } send, receive;\n    size_t len = raw_command(send.bytes, sizeof(send.bytes), cmd,\n                             key, strlen(key), NULL, 0);\n\n    safe_send(send.bytes, len, false);\n    safe_recv_packet(receive.bytes, sizeof(receive.bytes));\n    validate_response_header(&receive.response, cmd,\n                             PROTOCOL_BINARY_RESPONSE_KEY_ENOENT);\n    len = storage_command(send.bytes, sizeof(send.bytes),\n                          PROTOCOL_BINARY_CMD_ADD,\n                          key, strlen(key), NULL, 0, 0, 0);\n    safe_send(send.bytes, len, false);\n    safe_recv_packet(receive.bytes, sizeof(receive.bytes));\n    validate_response_header(&receive.response, PROTOCOL_BINARY_CMD_ADD,\n                             PROTOCOL_BINARY_RESPONSE_SUCCESS);\n\n    len = raw_command(send.bytes, sizeof(send.bytes),\n                      cmd, key, strlen(key), NULL, 0);\n    safe_send(send.bytes, len, false);\n\n    if (cmd == PROTOCOL_BINARY_CMD_DELETE) {\n        safe_recv_packet(receive.bytes, sizeof(receive.bytes));\n        validate_response_header(&receive.response, PROTOCOL_BINARY_CMD_DELETE,\n                                 PROTOCOL_BINARY_RESPONSE_SUCCESS);\n    }\n\n    safe_send(send.bytes, len, false);\n    safe_recv_packet(receive.bytes, sizeof(receive.bytes));\n    validate_response_header(&receive.response, cmd,\n                             PROTOCOL_BINARY_RESPONSE_KEY_ENOENT);\n\n    return TEST_PASS;\n}\n\nstatic enum test_return test_binary_delete(void) {\n    return test_binary_delete_impl(\"test_binary_delete\",\n                                   PROTOCOL_BINARY_CMD_DELETE);\n}\n\nstatic enum test_return test_binary_deleteq(void) {\n    return test_binary_delete_impl(\"test_binary_deleteq\",\n                                   PROTOCOL_BINARY_CMD_DELETEQ);\n}\n\nstatic enum test_return test_binary_get_impl(const char *key, uint8_t cmd) {\n    union {\n        protocol_binary_request_no_extras request;\n        protocol_binary_response_no_extras response;\n        char bytes[1024];\n    } send, receive;\n\n    uint32_t expiration = htonl(3600);\n    size_t extlen = 0;\n    if (cmd == PROTOCOL_BINARY_CMD_GAT || cmd == PROTOCOL_BINARY_CMD_GATK)\n        extlen = sizeof(expiration);\n\n    size_t len = ext_command(send.bytes, sizeof(send.bytes), cmd,\n                             extlen ? &expiration : NULL, extlen,\n                             key, strlen(key), NULL, 0);\n\n    safe_send(send.bytes, len, false);\n    safe_recv_packet(receive.bytes, sizeof(receive.bytes));\n    validate_response_header(&receive.response, cmd,\n                             PROTOCOL_BINARY_RESPONSE_KEY_ENOENT);\n\n    len = storage_command(send.bytes, sizeof(send.bytes),\n                          PROTOCOL_BINARY_CMD_ADD,\n                          key, strlen(key), NULL, 0,\n                          0, 0);\n    safe_send(send.bytes, len, false);\n    safe_recv_packet(receive.bytes, sizeof(receive.bytes));\n    validate_response_header(&receive.response, PROTOCOL_BINARY_CMD_ADD,\n                             PROTOCOL_BINARY_RESPONSE_SUCCESS);\n\n    /* run a little pipeline test ;-) */\n    len = 0;\n    int ii;\n    for (ii = 0; ii < 10; ++ii) {\n        union {\n            protocol_binary_request_no_extras request;\n            char bytes[1024];\n        } temp;\n        size_t l = ext_command(temp.bytes, sizeof(temp.bytes), cmd,\n                               extlen ? &expiration : NULL, extlen,\n                               key, strlen(key), NULL, 0);\n        memcpy(send.bytes + len, temp.bytes, l);\n        len += l;\n    }\n\n    safe_send(send.bytes, len, false);\n    for (ii = 0; ii < 10; ++ii) {\n        safe_recv_packet(receive.bytes, sizeof(receive.bytes));\n        validate_response_header(&receive.response, cmd,\n                                 PROTOCOL_BINARY_RESPONSE_SUCCESS);\n    }\n\n    return TEST_PASS;\n}\n\nstatic enum test_return test_binary_get(void) {\n    return test_binary_get_impl(\"test_binary_get\", PROTOCOL_BINARY_CMD_GET);\n}\n\nstatic enum test_return test_binary_getk(void) {\n    return test_binary_get_impl(\"test_binary_getk\", PROTOCOL_BINARY_CMD_GETK);\n}\n\nstatic enum test_return test_binary_gat(void) {\n    return test_binary_get_impl(\"test_binary_gat\", PROTOCOL_BINARY_CMD_GAT);\n}\n\nstatic enum test_return test_binary_gatk(void) {\n    return test_binary_get_impl(\"test_binary_gatk\", PROTOCOL_BINARY_CMD_GATK);\n}\n\nstatic enum test_return test_binary_getq_impl(const char *key, uint8_t cmd) {\n    const char *missing = \"test_binary_getq_missing\";\n    union {\n        protocol_binary_request_no_extras request;\n        protocol_binary_response_no_extras response;\n        char bytes[1024];\n    } send, temp, receive;\n\n    uint32_t expiration = htonl(3600);\n    size_t extlen = 0;\n    if (cmd == PROTOCOL_BINARY_CMD_GATQ || cmd == PROTOCOL_BINARY_CMD_GATKQ)\n        extlen = sizeof(expiration);\n\n    size_t len = storage_command(send.bytes, sizeof(send.bytes),\n                                 PROTOCOL_BINARY_CMD_ADD,\n                                 key, strlen(key), NULL, 0,\n                                 0, 0);\n    size_t len2 = ext_command(temp.bytes, sizeof(temp.bytes), cmd,\n                              extlen ? &expiration : NULL, extlen,\n                              missing, strlen(missing), NULL, 0);\n    /* I need to change the first opaque so that I can separate the two\n     * return packets */\n    temp.request.message.header.request.opaque = 0xfeedface;\n    memcpy(send.bytes + len, temp.bytes, len2);\n    len += len2;\n\n    len2 = ext_command(temp.bytes, sizeof(temp.bytes), cmd,\n                       extlen ? &expiration : NULL, extlen,\n                       key, strlen(key), NULL, 0);\n    memcpy(send.bytes + len, temp.bytes, len2);\n    len += len2;\n\n    safe_send(send.bytes, len, false);\n    safe_recv_packet(receive.bytes, sizeof(receive.bytes));\n    validate_response_header(&receive.response, PROTOCOL_BINARY_CMD_ADD,\n                             PROTOCOL_BINARY_RESPONSE_SUCCESS);\n    /* The first GETQ shouldn't return anything */\n    safe_recv_packet(receive.bytes, sizeof(receive.bytes));\n    validate_response_header(&receive.response, cmd,\n                             PROTOCOL_BINARY_RESPONSE_SUCCESS);\n\n    return TEST_PASS;\n}\n\nstatic enum test_return test_binary_getq(void) {\n    return test_binary_getq_impl(\"test_binary_getq\", PROTOCOL_BINARY_CMD_GETQ);\n}\n\nstatic enum test_return test_binary_getkq(void) {\n    return test_binary_getq_impl(\"test_binary_getkq\", PROTOCOL_BINARY_CMD_GETKQ);\n}\n\nstatic enum test_return test_binary_gatq(void) {\n    return test_binary_getq_impl(\"test_binary_gatq\", PROTOCOL_BINARY_CMD_GATQ);\n}\n\nstatic enum test_return test_binary_gatkq(void) {\n    return test_binary_getq_impl(\"test_binary_gatkq\", PROTOCOL_BINARY_CMD_GATKQ);\n}\n\nstatic enum test_return test_binary_incr_impl(const char* key, uint8_t cmd) {\n    union {\n        protocol_binary_request_no_extras request;\n        protocol_binary_response_no_extras response_header;\n        protocol_binary_response_incr response;\n        char bytes[1024];\n    } send, receive;\n    size_t len = arithmetic_command(send.bytes, sizeof(send.bytes), cmd,\n                                    key, strlen(key), 1, 0, 0);\n\n    int ii;\n    for (ii = 0; ii < 10; ++ii) {\n        safe_send(send.bytes, len, false);\n        if (cmd == PROTOCOL_BINARY_CMD_INCREMENT) {\n            safe_recv_packet(receive.bytes, sizeof(receive.bytes));\n            validate_response_header(&receive.response_header, cmd,\n                                     PROTOCOL_BINARY_RESPONSE_SUCCESS);\n            assert(ntohll(receive.response.message.body.value) == ii);\n        }\n    }\n\n    if (cmd == PROTOCOL_BINARY_CMD_INCREMENTQ) {\n        test_binary_noop();\n    }\n    return TEST_PASS;\n}\n\nstatic enum test_return test_binary_incr(void) {\n    return test_binary_incr_impl(\"test_binary_incr\",\n                                 PROTOCOL_BINARY_CMD_INCREMENT);\n}\n\nstatic enum test_return test_binary_incrq(void) {\n    return test_binary_incr_impl(\"test_binary_incrq\",\n                                 PROTOCOL_BINARY_CMD_INCREMENTQ);\n}\n\nstatic enum test_return test_binary_decr_impl(const char* key, uint8_t cmd) {\n    union {\n        protocol_binary_request_no_extras request;\n        protocol_binary_response_no_extras response_header;\n        protocol_binary_response_decr response;\n        char bytes[1024];\n    } send, receive;\n    size_t len = arithmetic_command(send.bytes, sizeof(send.bytes), cmd,\n                                    key, strlen(key), 1, 9, 0);\n\n    int ii;\n    for (ii = 9; ii >= 0; --ii) {\n        safe_send(send.bytes, len, false);\n        if (cmd == PROTOCOL_BINARY_CMD_DECREMENT) {\n            safe_recv_packet(receive.bytes, sizeof(receive.bytes));\n            validate_response_header(&receive.response_header, cmd,\n                                     PROTOCOL_BINARY_RESPONSE_SUCCESS);\n            assert(ntohll(receive.response.message.body.value) == ii);\n        }\n    }\n\n    /* decr on 0 should not wrap */\n    safe_send(send.bytes, len, false);\n    if (cmd == PROTOCOL_BINARY_CMD_DECREMENT) {\n        safe_recv_packet(receive.bytes, sizeof(receive.bytes));\n        validate_response_header(&receive.response_header, cmd,\n                                 PROTOCOL_BINARY_RESPONSE_SUCCESS);\n        assert(ntohll(receive.response.message.body.value) == 0);\n    } else {\n        test_binary_noop();\n    }\n\n    return TEST_PASS;\n}\n\nstatic enum test_return test_binary_decr(void) {\n    return test_binary_decr_impl(\"test_binary_decr\",\n                                 PROTOCOL_BINARY_CMD_DECREMENT);\n}\n\nstatic enum test_return test_binary_decrq(void) {\n    return test_binary_decr_impl(\"test_binary_decrq\",\n                                 PROTOCOL_BINARY_CMD_DECREMENTQ);\n}\n\nstatic enum test_return test_binary_version(void) {\n    union {\n        protocol_binary_request_no_extras request;\n        protocol_binary_response_no_extras response;\n        char bytes[1024];\n    } buffer;\n\n    size_t len = raw_command(buffer.bytes, sizeof(buffer.bytes),\n                             PROTOCOL_BINARY_CMD_VERSION,\n                             NULL, 0, NULL, 0);\n\n    safe_send(buffer.bytes, len, false);\n    safe_recv_packet(buffer.bytes, sizeof(buffer.bytes));\n    validate_response_header(&buffer.response, PROTOCOL_BINARY_CMD_VERSION,\n                             PROTOCOL_BINARY_RESPONSE_SUCCESS);\n\n    return TEST_PASS;\n}\n\nstatic enum test_return test_binary_flush_impl(const char *key, uint8_t cmd) {\n    union {\n        protocol_binary_request_no_extras request;\n        protocol_binary_response_no_extras response;\n        char bytes[1024];\n    } send, receive;\n\n    size_t len = storage_command(send.bytes, sizeof(send.bytes),\n                                 PROTOCOL_BINARY_CMD_ADD,\n                                 key, strlen(key), NULL, 0, 0, 0);\n    safe_send(send.bytes, len, false);\n    safe_recv_packet(receive.bytes, sizeof(receive.bytes));\n    validate_response_header(&receive.response, PROTOCOL_BINARY_CMD_ADD,\n                             PROTOCOL_BINARY_RESPONSE_SUCCESS);\n\n    len = flush_command(send.bytes, sizeof(send.bytes), cmd, 2, true);\n    safe_send(send.bytes, len, false);\n    if (cmd == PROTOCOL_BINARY_CMD_FLUSH) {\n        safe_recv_packet(receive.bytes, sizeof(receive.bytes));\n        validate_response_header(&receive.response, cmd,\n                                 PROTOCOL_BINARY_RESPONSE_SUCCESS);\n    }\n\n    len = raw_command(send.bytes, sizeof(send.bytes), PROTOCOL_BINARY_CMD_GET,\n                      key, strlen(key), NULL, 0);\n    safe_send(send.bytes, len, false);\n    safe_recv_packet(receive.bytes, sizeof(receive.bytes));\n    validate_response_header(&receive.response, PROTOCOL_BINARY_CMD_GET,\n                             PROTOCOL_BINARY_RESPONSE_SUCCESS);\n\n    sleep(2);\n    safe_send(send.bytes, len, false);\n    safe_recv_packet(receive.bytes, sizeof(receive.bytes));\n    validate_response_header(&receive.response, PROTOCOL_BINARY_CMD_GET,\n                             PROTOCOL_BINARY_RESPONSE_KEY_ENOENT);\n\n    int ii;\n    for (ii = 0; ii < 2; ++ii) {\n        len = storage_command(send.bytes, sizeof(send.bytes),\n                              PROTOCOL_BINARY_CMD_ADD,\n                              key, strlen(key), NULL, 0, 0, 0);\n        safe_send(send.bytes, len, false);\n        safe_recv_packet(receive.bytes, sizeof(receive.bytes));\n        validate_response_header(&receive.response, PROTOCOL_BINARY_CMD_ADD,\n                                 PROTOCOL_BINARY_RESPONSE_SUCCESS);\n\n        len = flush_command(send.bytes, sizeof(send.bytes), cmd, 0, ii == 0);\n        safe_send(send.bytes, len, false);\n        if (cmd == PROTOCOL_BINARY_CMD_FLUSH) {\n            safe_recv_packet(receive.bytes, sizeof(receive.bytes));\n            validate_response_header(&receive.response, cmd,\n                                     PROTOCOL_BINARY_RESPONSE_SUCCESS);\n        }\n\n        len = raw_command(send.bytes, sizeof(send.bytes),\n                          PROTOCOL_BINARY_CMD_GET,\n                          key, strlen(key), NULL, 0);\n        safe_send(send.bytes, len, false);\n        safe_recv_packet(receive.bytes, sizeof(receive.bytes));\n        validate_response_header(&receive.response, PROTOCOL_BINARY_CMD_GET,\n                                 PROTOCOL_BINARY_RESPONSE_KEY_ENOENT);\n    }\n\n    return TEST_PASS;\n}\n\nstatic enum test_return test_binary_flush(void) {\n    return test_binary_flush_impl(\"test_binary_flush\",\n                                  PROTOCOL_BINARY_CMD_FLUSH);\n}\n\nstatic enum test_return test_binary_flushq(void) {\n    return test_binary_flush_impl(\"test_binary_flushq\",\n                                  PROTOCOL_BINARY_CMD_FLUSHQ);\n}\n\nstatic enum test_return test_binary_concat_impl(const char *key, uint8_t cmd) {\n    union {\n        protocol_binary_request_no_extras request;\n        protocol_binary_response_no_extras response;\n        char bytes[1024];\n    } send, receive;\n    const char *value = \"world\";\n\n    size_t len = raw_command(send.bytes, sizeof(send.bytes), cmd,\n                              key, strlen(key), value, strlen(value));\n\n\n    safe_send(send.bytes, len, false);\n    safe_recv_packet(receive.bytes, sizeof(receive.bytes));\n    validate_response_header(&receive.response, cmd,\n                             PROTOCOL_BINARY_RESPONSE_NOT_STORED);\n\n    len = storage_command(send.bytes, sizeof(send.bytes),\n                          PROTOCOL_BINARY_CMD_ADD,\n                          key, strlen(key), value, strlen(value), 0, 0);\n    safe_send(send.bytes, len, false);\n    safe_recv_packet(receive.bytes, sizeof(receive.bytes));\n    validate_response_header(&receive.response, PROTOCOL_BINARY_CMD_ADD,\n                             PROTOCOL_BINARY_RESPONSE_SUCCESS);\n\n    len = raw_command(send.bytes, sizeof(send.bytes), cmd,\n                      key, strlen(key), value, strlen(value));\n    safe_send(send.bytes, len, false);\n\n    if (cmd == PROTOCOL_BINARY_CMD_APPEND || cmd == PROTOCOL_BINARY_CMD_PREPEND) {\n        safe_recv_packet(receive.bytes, sizeof(receive.bytes));\n        validate_response_header(&receive.response, cmd,\n                                 PROTOCOL_BINARY_RESPONSE_SUCCESS);\n    } else {\n        len = raw_command(send.bytes, sizeof(send.bytes), PROTOCOL_BINARY_CMD_NOOP,\n                          NULL, 0, NULL, 0);\n        safe_send(send.bytes, len, false);\n        safe_recv_packet(receive.bytes, sizeof(receive.bytes));\n        validate_response_header(&receive.response, PROTOCOL_BINARY_CMD_NOOP,\n                                 PROTOCOL_BINARY_RESPONSE_SUCCESS);\n    }\n\n    len = raw_command(send.bytes, sizeof(send.bytes), PROTOCOL_BINARY_CMD_GETK,\n                      key, strlen(key), NULL, 0);\n\n    safe_send(send.bytes, len, false);\n    safe_recv_packet(receive.bytes, sizeof(receive.bytes));\n    validate_response_header(&receive.response, PROTOCOL_BINARY_CMD_GETK,\n                             PROTOCOL_BINARY_RESPONSE_SUCCESS);\n\n    assert(receive.response.message.header.response.keylen == strlen(key));\n    assert(receive.response.message.header.response.bodylen == (strlen(key) + 2*strlen(value) + 4));\n\n    char *ptr = receive.bytes;\n    ptr += sizeof(receive.response);\n    ptr += 4;\n\n    assert(memcmp(ptr, key, strlen(key)) == 0);\n    ptr += strlen(key);\n    assert(memcmp(ptr, value, strlen(value)) == 0);\n    ptr += strlen(value);\n    assert(memcmp(ptr, value, strlen(value)) == 0);\n\n    return TEST_PASS;\n}\n\nstatic enum test_return test_binary_append(void) {\n    return test_binary_concat_impl(\"test_binary_append\",\n                                   PROTOCOL_BINARY_CMD_APPEND);\n}\n\nstatic enum test_return test_binary_prepend(void) {\n    return test_binary_concat_impl(\"test_binary_prepend\",\n                                   PROTOCOL_BINARY_CMD_PREPEND);\n}\n\nstatic enum test_return test_binary_appendq(void) {\n    return test_binary_concat_impl(\"test_binary_appendq\",\n                                   PROTOCOL_BINARY_CMD_APPENDQ);\n}\n\nstatic enum test_return test_binary_prependq(void) {\n    return test_binary_concat_impl(\"test_binary_prependq\",\n                                   PROTOCOL_BINARY_CMD_PREPENDQ);\n}\n\nstatic enum test_return test_binary_stat(void) {\n    union {\n        protocol_binary_request_no_extras request;\n        protocol_binary_response_no_extras response;\n        char bytes[1024];\n    } buffer;\n\n    size_t len = raw_command(buffer.bytes, sizeof(buffer.bytes),\n                             PROTOCOL_BINARY_CMD_STAT,\n                             NULL, 0, NULL, 0);\n\n    safe_send(buffer.bytes, len, false);\n    do {\n        safe_recv_packet(buffer.bytes, sizeof(buffer.bytes));\n        validate_response_header(&buffer.response, PROTOCOL_BINARY_CMD_STAT,\n                                 PROTOCOL_BINARY_RESPONSE_SUCCESS);\n    } while (buffer.response.message.header.response.keylen != 0);\n\n    return TEST_PASS;\n}\n\nstatic enum test_return test_binary_illegal(void) {\n    uint8_t cmd = 0x25;\n    while (cmd != 0x00) {\n        union {\n            protocol_binary_request_no_extras request;\n            protocol_binary_response_no_extras response;\n            char bytes[1024];\n        } buffer;\n        size_t len = raw_command(buffer.bytes, sizeof(buffer.bytes),\n                                 cmd, NULL, 0, NULL, 0);\n        safe_send(buffer.bytes, len, false);\n        safe_recv_packet(buffer.bytes, sizeof(buffer.bytes));\n        validate_response_header(&buffer.response, cmd,\n                                 PROTOCOL_BINARY_RESPONSE_UNKNOWN_COMMAND);\n        ++cmd;\n    }\n\n    return TEST_PASS;\n}\n\nvolatile bool hickup_thread_running;\n\nstatic void *binary_hickup_recv_verification_thread(void *arg) {\n    protocol_binary_response_no_extras *response = malloc(65*1024);\n    if (response != NULL) {\n        while (safe_recv_packet(response, 65*1024)) {\n            /* Just validate the packet format */\n            validate_response_header(response,\n                                     response->message.header.response.opcode,\n                                     response->message.header.response.status);\n        }\n        free(response);\n    }\n    hickup_thread_running = false;\n    allow_closed_read = false;\n    return NULL;\n}\n\nstatic enum test_return test_binary_pipeline_hickup_chunk(void *buffer, size_t buffersize) {\n    off_t offset = 0;\n    char *key[256] = { NULL };\n    uint64_t value = 0xfeedfacedeadbeef;\n\n    while (hickup_thread_running &&\n           offset + sizeof(protocol_binary_request_no_extras) < buffersize) {\n        union {\n            protocol_binary_request_no_extras request;\n            char bytes[65 * 1024];\n        } command;\n        uint8_t cmd = (uint8_t)(rand() & 0xff);\n        size_t len;\n        size_t keylen = (rand() % 250) + 1;\n\n        switch (cmd) {\n        case PROTOCOL_BINARY_CMD_ADD:\n        case PROTOCOL_BINARY_CMD_ADDQ:\n        case PROTOCOL_BINARY_CMD_REPLACE:\n        case PROTOCOL_BINARY_CMD_REPLACEQ:\n        case PROTOCOL_BINARY_CMD_SET:\n        case PROTOCOL_BINARY_CMD_SETQ:\n            len = storage_command(command.bytes, sizeof(command.bytes), cmd,\n                                  key, keylen , &value, sizeof(value),\n                                  0, 0);\n            break;\n        case PROTOCOL_BINARY_CMD_APPEND:\n        case PROTOCOL_BINARY_CMD_APPENDQ:\n        case PROTOCOL_BINARY_CMD_PREPEND:\n        case PROTOCOL_BINARY_CMD_PREPENDQ:\n            len = raw_command(command.bytes, sizeof(command.bytes), cmd,\n                              key, keylen, &value, sizeof(value));\n            break;\n        case PROTOCOL_BINARY_CMD_FLUSH:\n        case PROTOCOL_BINARY_CMD_FLUSHQ:\n            len = raw_command(command.bytes, sizeof(command.bytes), cmd,\n                              NULL, 0, NULL, 0);\n            break;\n        case PROTOCOL_BINARY_CMD_NOOP:\n            len = raw_command(command.bytes, sizeof(command.bytes), cmd,\n                              NULL, 0, NULL, 0);\n            break;\n        case PROTOCOL_BINARY_CMD_DELETE:\n        case PROTOCOL_BINARY_CMD_DELETEQ:\n            len = raw_command(command.bytes, sizeof(command.bytes), cmd,\n                             key, keylen, NULL, 0);\n            break;\n        case PROTOCOL_BINARY_CMD_DECREMENT:\n        case PROTOCOL_BINARY_CMD_DECREMENTQ:\n        case PROTOCOL_BINARY_CMD_INCREMENT:\n        case PROTOCOL_BINARY_CMD_INCREMENTQ:\n            len = arithmetic_command(command.bytes, sizeof(command.bytes), cmd,\n                                     key, keylen, 1, 0, 0);\n            break;\n        case PROTOCOL_BINARY_CMD_VERSION:\n            len = raw_command(command.bytes, sizeof(command.bytes),\n                             PROTOCOL_BINARY_CMD_VERSION,\n                             NULL, 0, NULL, 0);\n            break;\n        case PROTOCOL_BINARY_CMD_GET:\n        case PROTOCOL_BINARY_CMD_GETK:\n        case PROTOCOL_BINARY_CMD_GETKQ:\n        case PROTOCOL_BINARY_CMD_GETQ:\n            len = raw_command(command.bytes, sizeof(command.bytes), cmd,\n                             key, keylen, NULL, 0);\n            break;\n\n        case PROTOCOL_BINARY_CMD_TOUCH:\n        case PROTOCOL_BINARY_CMD_GAT:\n        case PROTOCOL_BINARY_CMD_GATQ:\n        case PROTOCOL_BINARY_CMD_GATK:\n        case PROTOCOL_BINARY_CMD_GATKQ:\n            len = touch_command(command.bytes, sizeof(command.bytes), cmd,\n                                key, keylen, 10);\n            break;\n\n        case PROTOCOL_BINARY_CMD_STAT:\n            len = raw_command(command.bytes, sizeof(command.bytes),\n                              PROTOCOL_BINARY_CMD_STAT,\n                              NULL, 0, NULL, 0);\n            break;\n\n        case PROTOCOL_BINARY_CMD_SASL_LIST_MECHS:\n        case PROTOCOL_BINARY_CMD_SASL_AUTH:\n        case PROTOCOL_BINARY_CMD_SASL_STEP:\n            /* Ignoring SASL */\n        case PROTOCOL_BINARY_CMD_QUITQ:\n        case PROTOCOL_BINARY_CMD_QUIT:\n            /* I don't want to pass on the quit commands ;-) */\n            cmd |= 0xf0;\n            /* FALLTHROUGH */\n        default:\n            len = raw_command(command.bytes, sizeof(command.bytes),\n                              cmd, NULL, 0, NULL, 0);\n        }\n\n        if ((len + offset) < buffersize) {\n            memcpy(((char*)buffer) + offset, command.bytes, len);\n            offset += len;\n        } else {\n            break;\n        }\n    }\n    safe_send(buffer, offset, true);\n\n    return TEST_PASS;\n}\n\nstatic enum test_return test_binary_pipeline_hickup(void)\n{\n    size_t buffersize = 65 * 1024;\n    void *buffer = malloc(buffersize);\n    int ii;\n\n    pthread_t tid;\n    int ret;\n    allow_closed_read = true;\n    hickup_thread_running = true;\n    if ((ret = pthread_create(&tid, NULL,\n                              binary_hickup_recv_verification_thread, NULL)) != 0) {\n        fprintf(stderr, \"Can't create thread: %s\\n\", strerror(ret));\n        free(buffer);\n        return TEST_FAIL;\n    }\n\n    /* Allow the thread to start */\n    usleep(250);\n\n    srand((int)time(NULL));\n    for (ii = 0; ii < 2; ++ii) {\n        test_binary_pipeline_hickup_chunk(buffer, buffersize);\n    }\n\n    /* send quitq to shut down the read thread ;-) */\n    size_t len = raw_command(buffer, buffersize, PROTOCOL_BINARY_CMD_QUITQ,\n                             NULL, 0, NULL, 0);\n    safe_send(buffer, len, false);\n\n    pthread_join(tid, NULL);\n    free(buffer);\n    return TEST_PASS;\n}\n\n\nstatic enum test_return test_issue_101(void) {\n    enum { max = 2 };\n    enum test_return ret = TEST_PASS;\n    struct conn *conns[max];\n    int ii = 0;\n    pid_t child = 0;\n\n    if (getenv(\"SKIP_TEST_101\") != NULL) {\n        return TEST_SKIP;\n    }\n\n    const char *command = \"stats\\r\\nstats\\r\\nstats\\r\\nstats\\r\\nstats\\r\\n\";\n    size_t cmdlen = strlen(command);\n\n    server_pid = start_server(&port, false, 1000);\n\n    for (ii = 0; ii < max; ++ii) {\n        conns[ii] = NULL;\n        conns[ii] = connect_server(\"127.0.0.1\", port, true, enable_ssl);\n        assert(conns[ii]);\n        assert(conns[ii]->sock > 0);\n    }\n\n    /* Send command on the connection until it blocks */\n    for (ii = 0; ii < max; ++ii) {\n        bool more = true;\n        do {\n            ssize_t err = conns[ii]->write(conns[ii], command, cmdlen);\n            if (err == -1) {\n                switch (errno) {\n                case EINTR:\n                    break;\n                case ENOMEM:\n                case EWOULDBLOCK:\n                    more = false;\n                    break;\n                default:\n                    ret = TEST_FAIL;\n                    goto cleanup;\n                }\n            }\n        } while (more);\n    }\n\n    child = fork();\n    if (child == (pid_t)-1) {\n        abort();\n    } else if (child > 0) {\n        int stat;\n        pid_t c;\n        while ((c = waitpid(child, &stat, 0)) == (pid_t)-1 && errno == EINTR);\n        assert(c == child);\n        assert(stat == 0);\n    } else {\n        con = connect_server(\"127.0.0.1\", port, false, enable_ssl);\n        assert(con);\n        ret = test_binary_noop();\n        close_conn();\n        exit(0);\n    }\n\n cleanup:\n    /* close all connections */\n    for (ii = 0; ii < max; ++ii) {\n        struct conn *c = conns[ii];\n        if (c == NULL) continue;\n#ifdef TLS\n        if (c->ssl) {\n            SSL_shutdown(c->ssl);\n            SSL_free(c->ssl);\n        }\n        if (c->ssl_ctx)\n            SSL_CTX_free(c->ssl_ctx);\n#endif\n        if (c->sock > 0) close(c->sock);\n        free(conns[ii]);\n        conns[ii] = NULL;\n    }\n\n    assert(kill(server_pid, SIGTERM) == 0);\n\n    return ret;\n}\n\ntypedef enum test_return (*TEST_FUNC)(void);\nstruct testcase {\n    const char *description;\n    TEST_FUNC function;\n};\n\nstruct testcase testcases[] = {\n    { \"cache_create\", cache_create_test },\n    { \"cache_reuse\", cache_reuse_test },\n    { \"cache_redzone\", cache_redzone_test },\n    { \"cache_limit_revised_downward\", cache_limit_revised_downward_test },\n    { \"stats_prefix_find\", test_stats_prefix_find },\n    { \"stats_prefix_record_get\", test_stats_prefix_record_get },\n    { \"stats_prefix_record_delete\", test_stats_prefix_record_delete },\n    { \"stats_prefix_record_set\", test_stats_prefix_record_set },\n    { \"stats_prefix_dump\", test_stats_prefix_dump },\n    { \"issue_161\", test_issue_161 },\n    { \"strtol\", test_safe_strtol },\n    { \"strtoll\", test_safe_strtoll },\n    { \"strtoul\", test_safe_strtoul },\n    { \"strtoull\", test_safe_strtoull },\n    { \"issue_44\", test_issue_44 },\n    { \"vperror\", test_vperror },\n    { \"issue_101\", test_issue_101 },\n    { \"crc32c\", test_crc32c },\n    /* The following tests all run towards the same server */\n    { \"start_server\", start_memcached_server },\n    { \"issue_92\", test_issue_92 },\n    { \"issue_102\", test_issue_102 },\n    { \"binary_noop\", test_binary_noop },\n    { \"binary_quit\", test_binary_quit },\n    { \"binary_quitq\", test_binary_quitq },\n    { \"binary_set\", test_binary_set },\n    { \"binary_setq\", test_binary_setq },\n    { \"binary_add\", test_binary_add },\n    { \"binary_addq\", test_binary_addq },\n    { \"binary_replace\", test_binary_replace },\n    { \"binary_replaceq\", test_binary_replaceq },\n    { \"binary_delete\", test_binary_delete },\n    { \"binary_deleteq\", test_binary_deleteq },\n    { \"binary_get\", test_binary_get },\n    { \"binary_getq\", test_binary_getq },\n    { \"binary_getk\", test_binary_getk },\n    { \"binary_getkq\", test_binary_getkq },\n    { \"binary_gat\", test_binary_gat },\n    { \"binary_gatq\", test_binary_gatq },\n    { \"binary_gatk\", test_binary_gatk },\n    { \"binary_gatkq\", test_binary_gatkq },\n    { \"binary_incr\", test_binary_incr },\n    { \"binary_incrq\", test_binary_incrq },\n    { \"binary_decr\", test_binary_decr },\n    { \"binary_decrq\", test_binary_decrq },\n    { \"binary_version\", test_binary_version },\n    { \"binary_flush\", test_binary_flush },\n    { \"binary_flushq\", test_binary_flushq },\n    { \"binary_append\", test_binary_append },\n    { \"binary_appendq\", test_binary_appendq },\n    { \"binary_prepend\", test_binary_prepend },\n    { \"binary_prependq\", test_binary_prependq },\n    { \"binary_stat\", test_binary_stat },\n    { \"binary_illegal\", test_binary_illegal },\n    { \"binary_pipeline_hickup\", test_binary_pipeline_hickup },\n    { \"shutdown\", shutdown_memcached_server },\n    { \"stop_server\", stop_memcached_server },\n    { NULL, NULL }\n};\n\n/* Stub out function defined in memcached.c */\nvoid STATS_LOCK(void);\nvoid STATS_UNLOCK(void);\nvoid STATS_LOCK(void)\n{}\nvoid STATS_UNLOCK(void)\n{}\n\nint main(int argc, char **argv)\n{\n    int exitcode = 0;\n    int ii = 0, num_cases = 0;\n#ifdef TLS\n    if (getenv(\"SSL_TEST\") != NULL) {\n        SSLeay_add_ssl_algorithms();\n        SSL_load_error_strings();\n        enable_ssl = true;\n    }\n#endif\n    /* Initialized directly instead of using hash_init to avoid pulling in\n       the definition of settings struct from memcached.h */\n    hash = jenkins_hash;\n    stats_prefix_init(':');\n\n    crc32c_init();\n\n    for (num_cases = 0; testcases[num_cases].description; num_cases++) {\n        /* Just counting */\n    }\n\n    printf(\"1..%d\\n\", num_cases);\n\n    for (ii = 0; testcases[ii].description != NULL; ++ii) {\n        fflush(stdout);\n#ifndef DEBUG\n        /* the test program shouldn't run longer than 10 minutes... */\n        alarm(600);\n#endif\n        enum test_return ret = testcases[ii].function();\n        if (ret == TEST_SKIP) {\n            fprintf(stdout, \"ok # SKIP %d - %s\\n\", ii + 1, testcases[ii].description);\n        } else if (ret == TEST_PASS) {\n            fprintf(stdout, \"ok %d - %s\\n\", ii + 1, testcases[ii].description);\n        } else {\n            fprintf(stdout, \"not ok %d - %s\\n\", ii + 1, testcases[ii].description);\n            exitcode = 1;\n        }\n        fflush(stdout);\n    }\n\n    return exitcode;\n}\n"
        },
        {
          "name": "thread.c",
          "type": "blob",
          "size": 33.888671875,
          "content": "/* -*- Mode: C; tab-width: 4; c-basic-offset: 4; indent-tabs-mode: nil -*- */\n/*\n * Thread management for memcached.\n */\n#include \"memcached.h\"\n#ifdef EXTSTORE\n#include \"storage.h\"\n#endif\n#ifdef HAVE_EVENTFD\n#include <sys/eventfd.h>\n#endif\n#ifdef PROXY\n#include \"proto_proxy.h\"\n#endif\n#include <assert.h>\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <pthread.h>\n\n#include \"queue.h\"\n#include \"tls.h\"\n\n#ifdef __sun\n#include <atomic.h>\n#endif\n\n#define ITEMS_PER_ALLOC 64\n\n/* An item in the connection queue. */\nenum conn_queue_item_modes {\n    queue_new_conn,   /* brand new connection. */\n    queue_pause,      /* pause thread */\n    queue_timeout,    /* socket sfd timed out */\n    queue_redispatch, /* return conn from side thread */\n    queue_stop,       /* exit thread */\n#ifdef PROXY\n    queue_proxy_reload, /* signal proxy to reload worker VM */\n#endif\n};\ntypedef struct conn_queue_item CQ_ITEM;\nstruct conn_queue_item {\n    int               sfd;\n    enum conn_states  init_state;\n    int               event_flags;\n    int               read_buffer_size;\n    enum network_transport     transport;\n    enum conn_queue_item_modes mode;\n    conn *c;\n    void    *ssl;\n    uint64_t conntag;\n    enum protocol bproto;\n    io_pending_t *io; // IO when used for deferred IO handling.\n    STAILQ_ENTRY(conn_queue_item) i_next;\n};\n\n/* A connection queue. */\ntypedef struct conn_queue CQ;\nstruct conn_queue {\n    STAILQ_HEAD(conn_ev_head, conn_queue_item) head;\n    pthread_mutex_t lock;\n    cache_t *cache; /* freelisted objects */\n};\n\n/* Locks for cache LRU operations */\npthread_mutex_t lru_locks[POWER_LARGEST];\n\n/* Connection lock around accepting new connections */\npthread_mutex_t conn_lock = PTHREAD_MUTEX_INITIALIZER;\n\n#if !defined(HAVE_GCC_ATOMICS) && !defined(__sun)\npthread_mutex_t atomics_mutex = PTHREAD_MUTEX_INITIALIZER;\n#endif\n\n/* Lock for global stats */\nstatic pthread_mutex_t stats_lock = PTHREAD_MUTEX_INITIALIZER;\n\n/* Lock to cause worker threads to hang up after being woken */\nstatic pthread_mutex_t worker_hang_lock;\n\nstatic pthread_mutex_t *item_locks;\n/* size of the item lock hash table */\nstatic uint32_t item_lock_count;\nstatic unsigned int item_lock_hashpower;\n#define hashsize(n) ((unsigned long int)1<<(n))\n#define hashmask(n) (hashsize(n)-1)\n\n/*\n * Each libevent instance has a wakeup pipe, which other threads\n * can use to signal that they've put a new connection on its queue.\n */\nstatic LIBEVENT_THREAD *threads;\n\n/*\n * Number of worker threads that have finished setting themselves up.\n */\nstatic int init_count = 0;\nstatic pthread_mutex_t init_lock;\nstatic pthread_cond_t init_cond;\n\nstatic void notify_worker(LIBEVENT_THREAD *t, CQ_ITEM *item);\nstatic void notify_worker_fd(LIBEVENT_THREAD *t, int sfd, enum conn_queue_item_modes mode);\nstatic CQ_ITEM *cqi_new(CQ *cq);\nstatic void cq_push(CQ *cq, CQ_ITEM *item);\n\nstatic void thread_libevent_process(evutil_socket_t fd, short which, void *arg);\nstatic void thread_libevent_ionotify(evutil_socket_t fd, short which, void *arg);\n\n/* item_lock() must be held for an item before any modifications to either its\n * associated hash bucket, or the structure itself.\n * LRU modifications must hold the item lock, and the LRU lock.\n * LRU's accessing items must item_trylock() before modifying an item.\n * Items accessible from an LRU must not be freed or modified\n * without first locking and removing from the LRU.\n */\n\nvoid item_lock(uint32_t hv) {\n    mutex_lock(&item_locks[hv & hashmask(item_lock_hashpower)]);\n}\n\nvoid *item_trylock(uint32_t hv) {\n    pthread_mutex_t *lock = &item_locks[hv & hashmask(item_lock_hashpower)];\n    if (pthread_mutex_trylock(lock) == 0) {\n        return lock;\n    }\n    return NULL;\n}\n\nvoid item_trylock_unlock(void *lock) {\n    mutex_unlock((pthread_mutex_t *) lock);\n}\n\nvoid item_unlock(uint32_t hv) {\n    mutex_unlock(&item_locks[hv & hashmask(item_lock_hashpower)]);\n}\n\nstatic void wait_for_thread_registration(int nthreads) {\n    while (init_count < nthreads) {\n        pthread_cond_wait(&init_cond, &init_lock);\n    }\n}\n\nstatic void register_thread_initialized(void) {\n    pthread_mutex_lock(&init_lock);\n    init_count++;\n    pthread_cond_signal(&init_cond);\n    pthread_mutex_unlock(&init_lock);\n    /* Force worker threads to pile up if someone wants us to */\n    pthread_mutex_lock(&worker_hang_lock);\n    pthread_mutex_unlock(&worker_hang_lock);\n}\n\n/* Must not be called with any deeper locks held */\nvoid pause_threads(enum pause_thread_types type) {\n    int i;\n    bool pause_workers = false;\n\n    switch (type) {\n        case PAUSE_ALL_THREADS:\n            slab_maintenance_pause(settings.slab_rebal);\n            lru_maintainer_pause();\n            lru_crawler_pause();\n#ifdef EXTSTORE\n            storage_compact_pause();\n            storage_write_pause();\n#endif\n        case PAUSE_WORKER_THREADS:\n            pause_workers = true;\n            pthread_mutex_lock(&worker_hang_lock);\n            break;\n        case RESUME_ALL_THREADS:\n            slab_maintenance_resume(settings.slab_rebal);\n            lru_maintainer_resume();\n            lru_crawler_resume();\n#ifdef EXTSTORE\n            storage_compact_resume();\n            storage_write_resume();\n#endif\n        case RESUME_WORKER_THREADS:\n            pthread_mutex_unlock(&worker_hang_lock);\n            break;\n        default:\n            fprintf(stderr, \"Unknown lock type: %d\\n\", type);\n            assert(1 == 0);\n            break;\n    }\n\n    /* Only send a message if we have one. */\n    if (!pause_workers) {\n        return;\n    }\n\n    pthread_mutex_lock(&init_lock);\n    init_count = 0;\n    for (i = 0; i < settings.num_threads; i++) {\n        notify_worker_fd(&threads[i], 0, queue_pause);\n    }\n    wait_for_thread_registration(settings.num_threads);\n    pthread_mutex_unlock(&init_lock);\n}\n\n// MUST not be called with any deeper locks held\n// MUST be called only by parent thread\n// Note: listener thread is the \"main\" event base, which has exited its\n// loop in order to call this function.\nvoid stop_threads(void) {\n    int i;\n\n    // assoc can call pause_threads(), so we have to stop it first.\n    stop_assoc_maintenance_thread();\n    if (settings.verbose > 0)\n        fprintf(stderr, \"stopped assoc\\n\");\n\n    if (settings.verbose > 0)\n        fprintf(stderr, \"asking workers to stop\\n\");\n\n    pthread_mutex_lock(&worker_hang_lock);\n    pthread_mutex_lock(&init_lock);\n    init_count = 0;\n    for (i = 0; i < settings.num_threads; i++) {\n        notify_worker_fd(&threads[i], 0, queue_stop);\n    }\n    wait_for_thread_registration(settings.num_threads);\n    pthread_mutex_unlock(&init_lock);\n\n    // All of the workers are hung but haven't done cleanup yet.\n\n    if (settings.verbose > 0)\n        fprintf(stderr, \"asking background threads to stop\\n\");\n\n    // stop each side thread.\n    // TODO: Verify these all work if the threads are already stopped\n    stop_item_crawler_thread(CRAWLER_WAIT);\n    if (settings.verbose > 0)\n        fprintf(stderr, \"stopped lru crawler\\n\");\n    if (settings.lru_maintainer_thread) {\n        stop_lru_maintainer_thread();\n        if (settings.verbose > 0)\n            fprintf(stderr, \"stopped maintainer\\n\");\n    }\n    if (settings.slab_reassign) {\n        stop_slab_maintenance_thread(settings.slab_rebal);\n        if (settings.verbose > 0)\n            fprintf(stderr, \"stopped slab mover\\n\");\n    }\n    logger_stop();\n    if (settings.verbose > 0)\n        fprintf(stderr, \"stopped logger thread\\n\");\n    stop_conn_timeout_thread();\n    if (settings.verbose > 0)\n        fprintf(stderr, \"stopped idle timeout thread\\n\");\n\n    // Close all connections then let the workers finally exit.\n    if (settings.verbose > 0)\n        fprintf(stderr, \"closing connections\\n\");\n    conn_close_all();\n    pthread_mutex_unlock(&worker_hang_lock);\n    if (settings.verbose > 0)\n        fprintf(stderr, \"reaping worker threads\\n\");\n    for (i = 0; i < settings.num_threads; i++) {\n        pthread_join(threads[i].thread_id, NULL);\n    }\n\n    if (settings.verbose > 0)\n        fprintf(stderr, \"all background threads stopped\\n\");\n\n    // At this point, every background thread must be stopped.\n}\n\n/*\n * Initializes a connection queue.\n */\nstatic void cq_init(CQ *cq) {\n    pthread_mutex_init(&cq->lock, NULL);\n    STAILQ_INIT(&cq->head);\n    cq->cache = cache_create(\"cq\", sizeof(CQ_ITEM), sizeof(char *));\n    if (cq->cache == NULL) {\n        fprintf(stderr, \"Failed to create connection queue cache\\n\");\n        exit(EXIT_FAILURE);\n    }\n}\n\n/*\n * Looks for an item on a connection queue, but doesn't block if there isn't\n * one.\n * Returns the item, or NULL if no item is available\n */\nstatic CQ_ITEM *cq_pop(CQ *cq) {\n    CQ_ITEM *item;\n\n    pthread_mutex_lock(&cq->lock);\n    item = STAILQ_FIRST(&cq->head);\n    if (item != NULL) {\n        STAILQ_REMOVE_HEAD(&cq->head, i_next);\n    }\n    pthread_mutex_unlock(&cq->lock);\n\n    return item;\n}\n\n/*\n * Adds an item to a connection queue.\n */\nstatic void cq_push(CQ *cq, CQ_ITEM *item) {\n    pthread_mutex_lock(&cq->lock);\n    STAILQ_INSERT_TAIL(&cq->head, item, i_next);\n    pthread_mutex_unlock(&cq->lock);\n}\n\n/*\n * Returns a fresh connection queue item.\n */\nstatic CQ_ITEM *cqi_new(CQ *cq) {\n    CQ_ITEM *item = cache_alloc(cq->cache);\n    if (item == NULL) {\n        STATS_LOCK();\n        stats.malloc_fails++;\n        STATS_UNLOCK();\n    }\n    return item;\n}\n\n/*\n * Frees a connection queue item (adds it to the freelist.)\n */\nstatic void cqi_free(CQ *cq, CQ_ITEM *item) {\n    cache_free(cq->cache, item);\n}\n\n// TODO: Skip notify if queue wasn't empty?\n// - Requires cq_push() returning a \"was empty\" flag\n// - Requires event handling loop to pop the entire queue and work from that\n// instead of the ev_count work there now.\n// In testing this does result in a large performance uptick, but unclear how\n// much that will transfer from a synthetic benchmark.\nstatic void notify_worker(LIBEVENT_THREAD *t, CQ_ITEM *item) {\n    cq_push(t->ev_queue, item);\n#ifdef HAVE_EVENTFD\n    uint64_t u = 1;\n    if (write(t->n.notify_event_fd, &u, sizeof(uint64_t)) != sizeof(uint64_t)) {\n        perror(\"failed writing to worker eventfd\");\n        /* TODO: This is a fatal problem. Can it ever happen temporarily? */\n    }\n#else\n    char buf[1] = \"c\";\n    if (write(t->n.notify_send_fd, buf, 1) != 1) {\n        perror(\"Failed writing to notify pipe\");\n        /* TODO: This is a fatal problem. Can it ever happen temporarily? */\n    }\n#endif\n}\n\n// NOTE: An external func that takes a conn *c might be cleaner overall.\nstatic void notify_worker_fd(LIBEVENT_THREAD *t, int sfd, enum conn_queue_item_modes mode) {\n    CQ_ITEM *item;\n    while ( (item = cqi_new(t->ev_queue)) == NULL ) {\n        // NOTE: most callers of this function cannot fail, but mallocs in\n        // theory can fail. Small mallocs essentially never do without also\n        // killing the process. Syscalls can also fail but the original code\n        // never handled this either.\n        // As a compromise, I'm leaving this note and this loop: This alloc\n        // cannot fail, but pre-allocating the data is too much code in an\n        // area I want to keep more lean. If this CQ business becomes a more\n        // generic queue I'll reconsider.\n    }\n\n    item->mode = mode;\n    item->sfd = sfd;\n    notify_worker(t, item);\n}\n\n/*\n * Creates a worker thread.\n */\nstatic void create_worker(void *(*func)(void *), void *arg) {\n    pthread_attr_t  attr;\n    int             ret;\n\n    pthread_attr_init(&attr);\n\n    if ((ret = pthread_create(&((LIBEVENT_THREAD*)arg)->thread_id, &attr, func, arg)) != 0) {\n        fprintf(stderr, \"Can't create thread: %s\\n\",\n                strerror(ret));\n        exit(1);\n    }\n\n    thread_setname(((LIBEVENT_THREAD*)arg)->thread_id, \"mc-worker\");\n}\n\n/*\n * Sets whether or not we accept new connections.\n */\nvoid accept_new_conns(const bool do_accept) {\n    pthread_mutex_lock(&conn_lock);\n    do_accept_new_conns(do_accept);\n    pthread_mutex_unlock(&conn_lock);\n}\n/****************************** LIBEVENT THREADS *****************************/\n\nstatic void setup_thread_notify(LIBEVENT_THREAD *me, struct thread_notify *tn,\n        void(*cb)(int, short, void *)) {\n#ifdef HAVE_EVENTFD\n    event_set(&tn->notify_event, tn->notify_event_fd,\n              EV_READ | EV_PERSIST, cb, me);\n#else\n    event_set(&tn->notify_event, tn->notify_receive_fd,\n              EV_READ | EV_PERSIST, cb, me);\n#endif\n    event_base_set(me->base, &tn->notify_event);\n\n    if (event_add(&tn->notify_event, 0) == -1) {\n        fprintf(stderr, \"Can't monitor libevent notify pipe\\n\");\n        exit(1);\n    }\n}\n\n/*\n * Set up a thread's information.\n */\nstatic void setup_thread(LIBEVENT_THREAD *me) {\n#if defined(LIBEVENT_VERSION_NUMBER) && LIBEVENT_VERSION_NUMBER >= 0x02000101\n    struct event_config *ev_config;\n    ev_config = event_config_new();\n    event_config_set_flag(ev_config, EVENT_BASE_FLAG_NOLOCK);\n    me->base = event_base_new_with_config(ev_config);\n    event_config_free(ev_config);\n#else\n    me->base = event_init();\n#endif\n\n    if (! me->base) {\n        fprintf(stderr, \"Can't allocate event base\\n\");\n        exit(1);\n    }\n\n    /* Listen for notifications from other threads */\n    setup_thread_notify(me, &me->n, thread_libevent_process);\n    setup_thread_notify(me, &me->ion, thread_libevent_ionotify);\n    pthread_mutex_init(&me->ion_lock, NULL);\n    STAILQ_INIT(&me->ion_head);\n\n    me->ev_queue = malloc(sizeof(struct conn_queue));\n    if (me->ev_queue == NULL) {\n        perror(\"Failed to allocate memory for connection queue\");\n        exit(EXIT_FAILURE);\n    }\n    cq_init(me->ev_queue);\n\n    if (pthread_mutex_init(&me->stats.mutex, NULL) != 0) {\n        perror(\"Failed to initialize mutex\");\n        exit(EXIT_FAILURE);\n    }\n\n    me->rbuf_cache = cache_create(\"rbuf\", READ_BUFFER_SIZE, sizeof(char *));\n    if (me->rbuf_cache == NULL) {\n        fprintf(stderr, \"Failed to create read buffer cache\\n\");\n        exit(EXIT_FAILURE);\n    }\n    // Note: we were cleanly passing in num_threads before, but this now\n    // relies on settings globals too much.\n    if (settings.read_buf_mem_limit) {\n        int limit = settings.read_buf_mem_limit / settings.num_threads;\n        if (limit < READ_BUFFER_SIZE) {\n            limit = 1;\n        } else {\n            limit = limit / READ_BUFFER_SIZE;\n        }\n        cache_set_limit(me->rbuf_cache, limit);\n    }\n\n    me->io_cache = cache_create(\"io\", sizeof(io_pending_t), sizeof(char*));\n    if (me->io_cache == NULL) {\n        fprintf(stderr, \"Failed to create IO object cache\\n\");\n        exit(EXIT_FAILURE);\n    }\n#ifdef TLS\n    if (settings.ssl_enabled) {\n        me->ssl_wbuf = (char *)malloc((size_t)settings.ssl_wbuf_size);\n        if (me->ssl_wbuf == NULL) {\n            fprintf(stderr, \"Failed to allocate the SSL write buffer\\n\");\n            exit(EXIT_FAILURE);\n        }\n    }\n#endif\n#ifdef EXTSTORE\n    // me->storage is set just before this function is called.\n    if (me->storage) {\n        thread_io_queue_add(me, IO_QUEUE_EXTSTORE, me->storage,\n            storage_submit_cb);\n    }\n#endif\n#ifdef PROXY\n    thread_io_queue_add(me, IO_QUEUE_PROXY, settings.proxy_ctx, proxy_submit_cb);\n\n    // TODO: maybe register hooks to be called here from sub-packages? ie;\n    // extstore, TLS, proxy.\n    if (settings.proxy_enabled) {\n        proxy_thread_init(settings.proxy_ctx, me);\n    }\n#endif\n    thread_io_queue_add(me, IO_QUEUE_NONE, NULL, NULL);\n}\n\n/*\n * Worker thread: main event loop\n */\nstatic void *worker_libevent(void *arg) {\n    LIBEVENT_THREAD *me = arg;\n\n    /* Any per-thread setup can happen here; memcached_thread_init() will block until\n     * all threads have finished initializing.\n     */\n    me->l = logger_create();\n    me->lru_bump_buf = item_lru_bump_buf_create();\n    if (me->l == NULL || me->lru_bump_buf == NULL) {\n        abort();\n    }\n\n    if (settings.drop_privileges) {\n        drop_worker_privileges();\n    }\n\n    register_thread_initialized();\n#ifdef PROXY\n    while (!event_base_got_exit(me->base)) {\n        event_base_loop(me->base, EVLOOP_ONCE);\n        if (me->proxy_ctx) {\n            proxy_gc_poke(me);\n        }\n    }\n#else\n    event_base_loop(me->base, 0);\n#endif\n    // same mechanism used to watch for all threads exiting.\n    register_thread_initialized();\n\n    event_base_free(me->base);\n    return NULL;\n}\n\n// Syscalls can be expensive enough that handling a few of them once here can\n// save both throughput and overall latency.\n#define MAX_PIPE_EVENTS 32\n\n// dedicated worker thread notify system for IO objects.\nstatic void thread_libevent_ionotify(evutil_socket_t fd, short which, void *arg) {\n    LIBEVENT_THREAD *me = arg;\n    uint64_t ev_count = 0;\n    iop_head_t head;\n\n    STAILQ_INIT(&head);\n#ifdef HAVE_EVENTFD\n    if (read(fd, &ev_count, sizeof(uint64_t)) != sizeof(uint64_t)) {\n        if (settings.verbose > 0)\n            fprintf(stderr, \"Can't read from libevent pipe\\n\");\n        return;\n    }\n#else\n    char buf[MAX_PIPE_EVENTS];\n\n    ev_count = read(fd, buf, MAX_PIPE_EVENTS);\n    if (ev_count == 0) {\n        if (settings.verbose > 0)\n            fprintf(stderr, \"Can't read from libevent pipe\\n\");\n        return;\n    }\n#endif\n\n    // pull entire queue and zero the thread head.\n    // need to do this after reading a syscall as we are only guaranteed to\n    // get syscalls if the queue is empty.\n    pthread_mutex_lock(&me->ion_lock);\n    STAILQ_CONCAT(&head, &me->ion_head);\n    pthread_mutex_unlock(&me->ion_lock);\n\n    while (!STAILQ_EMPTY(&head)) {\n        io_pending_t *io = STAILQ_FIRST(&head);\n        STAILQ_REMOVE_HEAD(&head, iop_next);\n        conn_io_queue_return(io);\n    }\n}\n\n/*\n * Processes an incoming \"connection event\" item. This is called when\n * input arrives on the libevent wakeup pipe.\n */\nstatic void thread_libevent_process(evutil_socket_t fd, short which, void *arg) {\n    LIBEVENT_THREAD *me = arg;\n    CQ_ITEM *item;\n    conn *c;\n    uint64_t ev_count = 0; // max number of events to loop through this run.\n#ifdef HAVE_EVENTFD\n    // NOTE: unlike pipe we aren't limiting the number of events per read.\n    // However we do limit the number of queue pulls to what the count was at\n    // the time of this function firing.\n    if (read(fd, &ev_count, sizeof(uint64_t)) != sizeof(uint64_t)) {\n        if (settings.verbose > 0)\n            fprintf(stderr, \"Can't read from libevent pipe\\n\");\n        return;\n    }\n#else\n    char buf[MAX_PIPE_EVENTS];\n\n    ev_count = read(fd, buf, MAX_PIPE_EVENTS);\n    if (ev_count == 0) {\n        if (settings.verbose > 0)\n            fprintf(stderr, \"Can't read from libevent pipe\\n\");\n        return;\n    }\n#endif\n\n    for (int x = 0; x < ev_count; x++) {\n        item = cq_pop(me->ev_queue);\n        if (item == NULL) {\n            return;\n        }\n\n        switch (item->mode) {\n            case queue_new_conn:\n                c = conn_new(item->sfd, item->init_state, item->event_flags,\n                                   item->read_buffer_size, item->transport,\n                                   me->base, item->ssl, item->conntag, item->bproto);\n                if (c == NULL) {\n                    if (IS_UDP(item->transport)) {\n                        fprintf(stderr, \"Can't listen for events on UDP socket\\n\");\n                        exit(1);\n                    } else {\n                        if (settings.verbose > 0) {\n                            fprintf(stderr, \"Can't listen for events on fd %d\\n\",\n                                item->sfd);\n                        }\n                        if (item->ssl) {\n                            ssl_conn_close(item->ssl);\n                            item->ssl = NULL;\n                        }\n                        close(item->sfd);\n                    }\n                } else {\n                    c->thread = me;\n                    conn_io_queue_setup(c);\n#ifdef TLS\n                    if (settings.ssl_enabled && c->ssl != NULL) {\n                        assert(c->thread && c->thread->ssl_wbuf);\n                        c->ssl_wbuf = c->thread->ssl_wbuf;\n                    }\n#endif\n                }\n                break;\n            case queue_pause:\n                /* we were told to pause and report in */\n                register_thread_initialized();\n                break;\n            case queue_timeout:\n                /* a client socket timed out */\n                conn_close_idle(conns[item->sfd]);\n                break;\n            case queue_redispatch:\n                /* a side thread redispatched a client connection */\n                conn_worker_readd(conns[item->sfd]);\n                break;\n            case queue_stop:\n                /* asked to stop */\n                event_base_loopexit(me->base, NULL);\n                break;\n#ifdef PROXY\n            case queue_proxy_reload:\n                proxy_worker_reload(settings.proxy_ctx, me);\n                break;\n#endif\n        }\n\n        cqi_free(me->ev_queue, item);\n    }\n}\n\n// Interface is slightly different on various platforms.\n// On linux, at least, the len limit is 16 bytes.\n#define THR_NAME_MAXLEN 16\nvoid thread_setname(pthread_t thread, const char *name) {\nassert(strlen(name) < THR_NAME_MAXLEN);\n#if defined(__linux__) && defined(HAVE_PTHREAD_SETNAME_NP)\npthread_setname_np(thread, name);\n#endif\n}\n#undef THR_NAME_MAXLEN\n\n// NOTE: need better encapsulation.\n// used by the proxy module to iterate the worker threads.\nLIBEVENT_THREAD *get_worker_thread(int id) {\n    return &threads[id];\n}\n\n/* Which thread we assigned a connection to most recently. */\nstatic int last_thread = -1;\n\n/* Last thread we assigned to a connection based on napi_id */\nstatic int last_thread_by_napi_id = -1;\n\nstatic LIBEVENT_THREAD *select_thread_round_robin(void)\n{\n    int tid = (last_thread + 1) % settings.num_threads;\n\n    last_thread = tid;\n\n    return threads + tid;\n}\n\nstatic void reset_threads_napi_id(void)\n{\n    LIBEVENT_THREAD *thread;\n    int i;\n\n    for (i = 0; i < settings.num_threads; i++) {\n         thread = threads + i;\n         thread->napi_id = 0;\n    }\n\n    last_thread_by_napi_id = -1;\n}\n\n/* Select a worker thread based on the NAPI ID of an incoming connection\n * request. NAPI ID is a globally unique ID that identifies a NIC RX queue\n * on which a flow is received.\n */\nstatic LIBEVENT_THREAD *select_thread_by_napi_id(int sfd)\n{\n    LIBEVENT_THREAD *thread;\n    int napi_id, err, i;\n    socklen_t len;\n    int tid = -1;\n\n    len = sizeof(socklen_t);\n    err = getsockopt(sfd, SOL_SOCKET, SO_INCOMING_NAPI_ID, &napi_id, &len);\n    if ((err == -1) || (napi_id == 0)) {\n        STATS_LOCK();\n        stats.round_robin_fallback++;\n        STATS_UNLOCK();\n        return select_thread_round_robin();\n    }\n\nselect:\n    for (i = 0; i < settings.num_threads; i++) {\n         thread = threads + i;\n         if (last_thread_by_napi_id < i) {\n             thread->napi_id = napi_id;\n             last_thread_by_napi_id = i;\n             tid = i;\n             break;\n         }\n         if (thread->napi_id == napi_id) {\n             tid = i;\n             break;\n         }\n    }\n\n    if (tid == -1) {\n        STATS_LOCK();\n        stats.unexpected_napi_ids++;\n        STATS_UNLOCK();\n        reset_threads_napi_id();\n        goto select;\n    }\n\n    return threads + tid;\n}\n\n/*\n * Dispatches a new connection to another thread. This is only ever called\n * from the main thread, either during initialization (for UDP) or because\n * of an incoming connection.\n */\nvoid dispatch_conn_new(int sfd, enum conn_states init_state, int event_flags,\n                       int read_buffer_size, enum network_transport transport, void *ssl,\n                       uint64_t conntag, enum protocol bproto) {\n    CQ_ITEM *item = NULL;\n    LIBEVENT_THREAD *thread;\n\n    if (!settings.num_napi_ids)\n        thread = select_thread_round_robin();\n    else\n        thread = select_thread_by_napi_id(sfd);\n\n    item = cqi_new(thread->ev_queue);\n    if (item == NULL) {\n        close(sfd);\n        /* given that malloc failed this may also fail, but let's try */\n        fprintf(stderr, \"Failed to allocate memory for connection object\\n\");\n        return;\n    }\n\n    item->sfd = sfd;\n    item->init_state = init_state;\n    item->event_flags = event_flags;\n    item->read_buffer_size = read_buffer_size;\n    item->transport = transport;\n    item->mode = queue_new_conn;\n    item->ssl = ssl;\n    item->conntag = conntag;\n    item->bproto = bproto;\n\n    MEMCACHED_CONN_DISPATCH(sfd, (int64_t)thread->thread_id);\n    notify_worker(thread, item);\n}\n\n/*\n * Re-dispatches a connection back to the original thread. Can be called from\n * any side thread borrowing a connection.\n */\nvoid redispatch_conn(conn *c) {\n    notify_worker_fd(c->thread, c->sfd, queue_redispatch);\n}\n\nvoid timeout_conn(conn *c) {\n    notify_worker_fd(c->thread, c->sfd, queue_timeout);\n}\n#ifdef PROXY\nvoid proxy_reload_notify(LIBEVENT_THREAD *t) {\n    notify_worker_fd(t, 0, queue_proxy_reload);\n}\n#endif\n\nvoid return_io_pending(io_pending_t *io) {\n    bool do_notify = false;\n    LIBEVENT_THREAD *t = io->thread;\n    pthread_mutex_lock(&t->ion_lock);\n    if (STAILQ_EMPTY(&t->ion_head)) {\n        do_notify = true;\n    }\n    STAILQ_INSERT_TAIL(&t->ion_head, io, iop_next);\n    pthread_mutex_unlock(&t->ion_lock);\n\n    // skip the syscall if there was already data in the queue, as it's\n    // already been notified.\n    if (do_notify) {\n#ifdef HAVE_EVENTFD\n        uint64_t u = 1;\n        if (write(t->ion.notify_event_fd, &u, sizeof(uint64_t)) != sizeof(uint64_t)) {\n            perror(\"failed writing to worker eventfd\");\n            /* TODO: This is a fatal problem. Can it ever happen temporarily? */\n        }\n#else\n        char buf[1] = \"c\";\n        if (write(t->ion.notify_send_fd, buf, 1) != 1) {\n            perror(\"Failed writing to notify pipe\");\n            /* TODO: This is a fatal problem. Can it ever happen temporarily? */\n        }\n#endif\n    }\n}\n\n/* This misses the allow_new_conns flag :( */\nvoid sidethread_conn_close(conn *c) {\n    if (settings.verbose > 1)\n        fprintf(stderr, \"<%d connection closing from side thread.\\n\", c->sfd);\n\n    c->state = conn_closing;\n    // redispatch will see closing flag and properly close connection.\n    redispatch_conn(c);\n    return;\n}\n\n/********************************* ITEM ACCESS *******************************/\n\n/*\n * Allocates a new item.\n */\nitem *item_alloc(const char *key, size_t nkey, client_flags_t flags, rel_time_t exptime, int nbytes) {\n    item *it;\n    /* do_item_alloc handles its own locks */\n    it = do_item_alloc(key, nkey, flags, exptime, nbytes);\n    return it;\n}\n\n/*\n * Returns an item if it hasn't been marked as expired,\n * lazy-expiring as needed.\n */\nitem *item_get(const char *key, const size_t nkey, LIBEVENT_THREAD *t, const bool do_update) {\n    item *it;\n    uint32_t hv;\n    hv = hash(key, nkey);\n    item_lock(hv);\n    it = do_item_get(key, nkey, hv, t, do_update);\n    item_unlock(hv);\n    return it;\n}\n\n// returns an item with the item lock held.\n// lock will still be held even if return is NULL, allowing caller to replace\n// an item atomically if desired.\nitem *item_get_locked(const char *key, const size_t nkey, LIBEVENT_THREAD *t, const bool do_update, uint32_t *hv) {\n    item *it;\n    *hv = hash(key, nkey);\n    item_lock(*hv);\n    it = do_item_get(key, nkey, *hv, t, do_update);\n    return it;\n}\n\nitem *item_touch(const char *key, size_t nkey, uint32_t exptime, LIBEVENT_THREAD *t) {\n    item *it;\n    uint32_t hv;\n    hv = hash(key, nkey);\n    item_lock(hv);\n    it = do_item_touch(key, nkey, exptime, hv, t);\n    item_unlock(hv);\n    return it;\n}\n\n/*\n * Decrements the reference count on an item and adds it to the freelist if\n * needed.\n */\nvoid item_remove(item *item) {\n    uint32_t hv;\n    hv = hash(ITEM_key(item), item->nkey);\n\n    item_lock(hv);\n    do_item_remove(item);\n    item_unlock(hv);\n}\n\n/*\n * Replaces one item with another in the hashtable.\n * Unprotected by a mutex lock since the core server does not require\n * it to be thread-safe.\n */\nint item_replace(item *old_it, item *new_it, const uint32_t hv, const uint64_t cas_in) {\n    return do_item_replace(old_it, new_it, hv, cas_in);\n}\n\n/*\n * Unlinks an item from the LRU and hashtable.\n */\nvoid item_unlink(item *item) {\n    uint32_t hv;\n    hv = hash(ITEM_key(item), item->nkey);\n    item_lock(hv);\n    do_item_unlink(item, hv);\n    item_unlock(hv);\n}\n\n/*\n * Does arithmetic on a numeric item value.\n */\nenum delta_result_type add_delta(LIBEVENT_THREAD *t, const char *key,\n                                 const size_t nkey, bool incr,\n                                 const int64_t delta, char *buf,\n                                 uint64_t *cas) {\n    enum delta_result_type ret;\n    uint32_t hv;\n\n    hv = hash(key, nkey);\n    item_lock(hv);\n    ret = do_add_delta(t, key, nkey, incr, delta, buf, cas, hv, NULL);\n    item_unlock(hv);\n    return ret;\n}\n\n/*\n * Stores an item in the cache (high level, obeys set/add/replace semantics)\n */\nenum store_item_type store_item(item *item, int comm, LIBEVENT_THREAD *t, int *nbytes, uint64_t *cas, const uint64_t cas_in, bool cas_stale) {\n    enum store_item_type ret;\n    uint32_t hv;\n\n    hv = hash(ITEM_key(item), item->nkey);\n    item_lock(hv);\n    ret = do_store_item(item, comm, t, hv, nbytes, cas, cas_in, cas_stale);\n    item_unlock(hv);\n    return ret;\n}\n\n/******************************* GLOBAL STATS ******************************/\n\nvoid STATS_LOCK(void) {\n    pthread_mutex_lock(&stats_lock);\n}\n\nvoid STATS_UNLOCK(void) {\n    pthread_mutex_unlock(&stats_lock);\n}\n\nvoid threadlocal_stats_reset(void) {\n    int ii;\n    for (ii = 0; ii < settings.num_threads; ++ii) {\n        pthread_mutex_lock(&threads[ii].stats.mutex);\n#define X(name) threads[ii].stats.name = 0;\n        THREAD_STATS_FIELDS\n#ifdef EXTSTORE\n        EXTSTORE_THREAD_STATS_FIELDS\n#endif\n#ifdef PROXY\n        PROXY_THREAD_STATS_FIELDS\n#endif\n#undef X\n\n        memset(&threads[ii].stats.slab_stats, 0,\n                sizeof(threads[ii].stats.slab_stats));\n        memset(&threads[ii].stats.lru_hits, 0,\n                sizeof(uint64_t) * POWER_LARGEST);\n\n        pthread_mutex_unlock(&threads[ii].stats.mutex);\n    }\n}\n\nvoid threadlocal_stats_aggregate(struct thread_stats *stats) {\n    int ii, sid;\n\n    /* The struct has a mutex, but we can safely set the whole thing\n     * to zero since it is unused when aggregating. */\n    memset(stats, 0, sizeof(*stats));\n\n    for (ii = 0; ii < settings.num_threads; ++ii) {\n        pthread_mutex_lock(&threads[ii].stats.mutex);\n#define X(name) stats->name += threads[ii].stats.name;\n        THREAD_STATS_FIELDS\n#ifdef EXTSTORE\n        EXTSTORE_THREAD_STATS_FIELDS\n#endif\n#ifdef PROXY\n        PROXY_THREAD_STATS_FIELDS\n#endif\n#undef X\n\n        for (sid = 0; sid < MAX_NUMBER_OF_SLAB_CLASSES; sid++) {\n#define X(name) stats->slab_stats[sid].name += \\\n            threads[ii].stats.slab_stats[sid].name;\n            SLAB_STATS_FIELDS\n#undef X\n        }\n\n        for (sid = 0; sid < POWER_LARGEST; sid++) {\n            stats->lru_hits[sid] +=\n                threads[ii].stats.lru_hits[sid];\n            stats->slab_stats[CLEAR_LRU(sid)].get_hits +=\n                threads[ii].stats.lru_hits[sid];\n        }\n\n        stats->read_buf_count += threads[ii].rbuf_cache->total;\n        stats->read_buf_bytes += threads[ii].rbuf_cache->total * READ_BUFFER_SIZE;\n        stats->read_buf_bytes_free += threads[ii].rbuf_cache->freecurr * READ_BUFFER_SIZE;\n        pthread_mutex_unlock(&threads[ii].stats.mutex);\n    }\n}\n\nvoid slab_stats_aggregate(struct thread_stats *stats, struct slab_stats *out) {\n    int sid;\n\n    memset(out, 0, sizeof(*out));\n\n    for (sid = 0; sid < MAX_NUMBER_OF_SLAB_CLASSES; sid++) {\n#define X(name) out->name += stats->slab_stats[sid].name;\n        SLAB_STATS_FIELDS\n#undef X\n    }\n}\n\nstatic void memcached_thread_notify_init(struct thread_notify *tn) {\n#ifdef HAVE_EVENTFD\n        tn->notify_event_fd = eventfd(0, EFD_NONBLOCK);\n        if (tn->notify_event_fd == -1) {\n            perror(\"failed creating eventfd for worker thread\");\n            exit(1);\n        }\n#else\n        int fds[2];\n        if (pipe(fds)) {\n            perror(\"Can't create notify pipe\");\n            exit(1);\n        }\n\n        tn->notify_receive_fd = fds[0];\n        tn->notify_send_fd = fds[1];\n#endif\n}\n\n/*\n * Initializes the thread subsystem, creating various worker threads.\n *\n * nthreads  Number of worker event handler threads to spawn\n */\nvoid memcached_thread_init(int nthreads, void *arg) {\n    int         i;\n    int         power;\n\n    for (i = 0; i < POWER_LARGEST; i++) {\n        pthread_mutex_init(&lru_locks[i], NULL);\n    }\n    pthread_mutex_init(&worker_hang_lock, NULL);\n\n    pthread_mutex_init(&init_lock, NULL);\n    pthread_cond_init(&init_cond, NULL);\n\n    /* Want a wide lock table, but don't waste memory */\n    if (nthreads < 3) {\n        power = 10;\n    } else if (nthreads < 4) {\n        power = 11;\n    } else if (nthreads < 5) {\n        power = 12;\n    } else if (nthreads <= 10) {\n        power = 13;\n    } else if (nthreads <= 20) {\n        power = 14;\n    } else {\n        /* 32k buckets. just under the hashpower default. */\n        power = 15;\n    }\n\n    if (power >= hashpower) {\n        fprintf(stderr, \"Hash table power size (%d) cannot be equal to or less than item lock table (%d)\\n\", hashpower, power);\n        fprintf(stderr, \"Item lock table grows with `-t N` (worker threadcount)\\n\");\n        fprintf(stderr, \"Hash table grows with `-o hashpower=N` \\n\");\n        exit(1);\n    }\n\n    item_lock_count = hashsize(power);\n    item_lock_hashpower = power;\n\n    item_locks = calloc(item_lock_count, sizeof(pthread_mutex_t));\n    if (! item_locks) {\n        perror(\"Can't allocate item locks\");\n        exit(1);\n    }\n    for (i = 0; i < item_lock_count; i++) {\n        pthread_mutex_init(&item_locks[i], NULL);\n    }\n\n    threads = calloc(nthreads, sizeof(LIBEVENT_THREAD));\n    if (! threads) {\n        perror(\"Can't allocate thread descriptors\");\n        exit(1);\n    }\n\n    for (i = 0; i < nthreads; i++) {\n        memcached_thread_notify_init(&threads[i].n);\n        memcached_thread_notify_init(&threads[i].ion);\n#ifdef EXTSTORE\n        threads[i].storage = arg;\n#endif\n        threads[i].thread_baseid = i;\n        setup_thread(&threads[i]);\n        /* Reserve three fds for the libevent base, and two for the pipe */\n        stats_state.reserved_fds += 5;\n    }\n\n    /* Create threads after we've done all the libevent setup. */\n    for (i = 0; i < nthreads; i++) {\n        create_worker(worker_libevent, &threads[i]);\n    }\n\n    /* Wait for all the threads to set themselves up before returning. */\n    pthread_mutex_lock(&init_lock);\n    wait_for_thread_registration(nthreads);\n    pthread_mutex_unlock(&init_lock);\n}\n\n"
        },
        {
          "name": "timedrun.c",
          "type": "blob",
          "size": 3,
          "content": "#include <stdio.h>\n#include <stdlib.h>\n#include <unistd.h>\n#include <string.h>\n#include <signal.h>\n#include <sys/wait.h>\n#include <sysexits.h>\n\n#include <assert.h>\n\nvolatile sig_atomic_t caught_sig = 0;\n\nstatic void signal_handler(int which)\n{\n    caught_sig = which;\n}\n\nstatic int wait_for_process(pid_t pid)\n{\n    int rv = EX_SOFTWARE;\n    int status = 0;\n    int i = 0;\n    struct sigaction sig_handler;\n\n    memset(&sig_handler, 0, sizeof(struct sigaction));\n    sig_handler.sa_handler = signal_handler;\n    sig_handler.sa_flags = 0;\n\n    sigaction(SIGALRM, &sig_handler, NULL);\n    sigaction(SIGHUP, &sig_handler, NULL);\n    sigaction(SIGINT, &sig_handler, NULL);\n    sigaction(SIGUSR1, &sig_handler, NULL);\n    sigaction(SIGTERM, &sig_handler, NULL);\n    sigaction(SIGPIPE, &sig_handler, NULL);\n\n    /* Loop forever waiting for the process to quit */\n    for (i = 0; ;i++) {\n        pid_t p = waitpid(pid, &status, 0);\n        if (p == pid) {\n            /* child exited.  Let's get out of here */\n            rv = WIFEXITED(status) ?\n                WEXITSTATUS(status) :\n                (0x80 | WTERMSIG(status));\n            break;\n        } else {\n            int sig = 0;\n            /* pass along SIGHUP gracefully */\n            if (caught_sig == SIGHUP) {\n                i = 0;\n                int sig = caught_sig;\n                if (kill(pid, sig) < 0) {\n                    /* Kill failed.  Must have lost the process. :/ */\n                    perror(\"lost child when trying to kill\");\n                }\n                continue;\n            }\n            switch (i) {\n            case 0:\n                /* On the first iteration, pass the signal through */\n                sig = caught_sig > 0 ? caught_sig : SIGTERM;\n                if (caught_sig == SIGALRM) {\n                   fprintf(stderr, \"Timeout.. killing the process\\n\");\n                }\n                break;\n            case 1:\n                sig = SIGTERM;\n                break;\n            default:\n                sig = SIGKILL;\n                break;\n            }\n            if (kill(pid, sig) < 0) {\n                /* Kill failed.  Must have lost the process. :/ */\n                perror(\"lost child when trying to kill\");\n            }\n            /* Wait up to 5 seconds for the pid */\n            alarm(5);\n        }\n    }\n    return rv;\n}\n\nstatic int spawn_and_wait(char **argv)\n{\n    int rv = EX_SOFTWARE;\n    pid_t pid = fork();\n\n    switch (pid) {\n    case -1:\n        perror(\"fork\");\n        rv = EX_OSERR;\n        break; /* NOTREACHED */\n    case 0:\n        execvp(argv[0], argv);\n        perror(\"exec\");\n        rv = EX_SOFTWARE;\n        break; /* NOTREACHED */\n    default:\n        rv = wait_for_process(pid);\n    }\n    return rv;\n}\n\nstatic void usage(void) {\n    fprintf(stderr, \"./timedrun <naptime in sec> args...\\n\");\n    exit(-1);\n}\n\nint main(int argc, char **argv)\n{\n    int naptime = 0;\n    if (argc < 3)\n        usage();\n\n    naptime = atoi(argv[1]);\n    assert(naptime > 0 && naptime < 1800);\n\n    alarm(naptime);\n\n    return spawn_and_wait(argv+2);\n}\n"
        },
        {
          "name": "tls.c",
          "type": "blob",
          "size": 20.2275390625,
          "content": "#include \"memcached.h\"\n\n#ifdef TLS\n\n#include \"tls.h\"\n#include <string.h>\n#include <sysexits.h>\n#include <sys/param.h>\n#include <openssl/ssl.h>\n#include <openssl/err.h>\n\n/* constant session ID context for application-level SSL session scoping.\n * used in server-side SSL session caching, when enabled. */\n#define SESSION_ID_CONTEXT \"memcached\"\n\n#ifndef MAXPATHLEN\n#define MAXPATHLEN 4096\n#endif\n\nstatic ssize_t ssl_read(conn *c, void *buf, size_t count);\nstatic ssize_t ssl_sendmsg(conn *c, struct msghdr *msg, int flags);\nstatic ssize_t ssl_write(conn *c, void *buf, size_t count);\nstatic void print_ssl_error(char *buff, size_t len);\nstatic void ssl_callback(const SSL *s, int where, int ret);\nstatic int ssl_new_session_callback(SSL *s, SSL_SESSION *sess);\n\nstatic pthread_mutex_t ssl_ctx_lock = PTHREAD_MUTEX_INITIALIZER;\n\nconst unsigned ERROR_MSG_SIZE = 64;\nconst size_t SSL_ERROR_MSG_SIZE = 256;\n\nstatic void SSL_LOCK(void) {\n    pthread_mutex_lock(&(ssl_ctx_lock));\n}\n\nstatic void SSL_UNLOCK(void) {\n    pthread_mutex_unlock(&(ssl_ctx_lock));\n}\n\nvoid *ssl_accept(conn *c, int sfd, bool *fail) {\n    SSL *ssl = NULL;\n    if (c->ssl_enabled) {\n        assert(IS_TCP(c->transport) && settings.ssl_enabled);\n\n        if (settings.ssl_ctx == NULL) {\n            if (settings.verbose) {\n                fprintf(stderr, \"SSL context is not initialized\\n\");\n            }\n            *fail = true;\n            return NULL;\n        }\n        SSL_LOCK();\n        ssl = SSL_new(settings.ssl_ctx);\n        SSL_UNLOCK();\n        if (ssl == NULL) {\n            if (settings.verbose) {\n                fprintf(stderr, \"Failed to created the SSL object\\n\");\n            }\n            *fail = true;\n            ERR_clear_error();\n            return NULL;\n        }\n        SSL_set_fd(ssl, sfd);\n\n        if (c->ssl_enabled == MC_SSL_ENABLED_NOPEER) {\n            // Don't enforce peer certs for this socket.\n            SSL_set_verify(ssl, SSL_VERIFY_NONE, NULL);\n        } else if (c->ssl_enabled == MC_SSL_ENABLED_PEER) {\n            // Force peer validation for this socket.\n            SSL_set_verify(ssl, SSL_VERIFY_PEER|SSL_VERIFY_FAIL_IF_NO_PEER_CERT, NULL);\n        }\n\n        ERR_clear_error();\n        int ret = SSL_accept(ssl);\n        if (ret <= 0) {\n            int err = SSL_get_error(ssl, ret);\n            if (err == SSL_ERROR_WANT_WRITE || err == SSL_ERROR_WANT_READ) {\n                // we're actually fine, let the worker thread continue.\n                ERR_clear_error();\n            } else {\n                // TODO: ship full error to log stream? conn events?\n                // SYSCALL specifically means we need to check errno/strerror.\n                // Else we need to look at the main error stack.\n                if (err == SSL_ERROR_SYSCALL) {\n                    LOGGER_LOG(NULL, LOG_CONNEVENTS, LOGGER_CONNECTION_TLSERROR,\n                            NULL, c->sfd, strerror(errno));\n                } else {\n                    char ssl_err[SSL_ERROR_MSG_SIZE];\n                    // OpenSSL internal error. One or more, but lets only care about\n                    // the top error for now.\n                    print_ssl_error(ssl_err, SSL_ERROR_MSG_SIZE);\n                    LOGGER_LOG(NULL, LOG_CONNEVENTS, LOGGER_CONNECTION_TLSERROR,\n                            NULL, c->sfd, ssl_err);\n                }\n                ERR_clear_error();\n                SSL_free(ssl);\n                STATS_LOCK();\n                stats.ssl_handshake_errors++;\n                STATS_UNLOCK();\n                *fail = true;\n                return NULL;\n            }\n        }\n    }\n\n    return ssl;\n}\n\n/*\n * Note on setting errno in the follow functions:\n * We either have to refactor callers of read/write/sendmsg to take an error\n * flag to find out if we're in an EAGAIN state, or we ensure the errno is set\n * properly before returning from our TLS call. We do this because it's\n * _possible_ for OpenSSL to do something weird and land with an errno that\n * doesn't match the WANT_READ|WRITE state.\n *\n * Also: we _might_ have to communicate from these calls if we need to wait on\n * reads or write. Since I haven't yet proved that's even possible I'll save\n * that for a future refactor.\n */\n\n// TODO: add int offset, and find the nth NID here.\n// or different function that accepts a string, then does etc?\n// Caller _must immediately_ use the string and not store the pointer.\nconst unsigned char *ssl_get_peer_cn(conn *c, int *len) {\n    if (!c->ssl) {\n        return NULL;\n    }\n\n    // can't use get0 to avoid getting a reference since that requires 3.0.0+\n    X509 *cert = SSL_get_peer_certificate(c->ssl);\n    if (cert == NULL) {\n        return NULL;\n    }\n    X509_NAME *name = X509_get_subject_name(cert);\n    if (name == NULL) {\n        X509_free(cert);\n        return NULL;\n    }\n\n    int r = X509_NAME_get_index_by_NID(name, NID_commonName, -1);\n    if (r == -1) {\n        X509_free(cert);\n        return NULL;\n    }\n    ASN1_STRING *asn1 = X509_NAME_ENTRY_get_data(X509_NAME_get_entry(name, r));\n\n    if (asn1 == NULL) {\n        X509_free(cert);\n        return NULL;\n    }\n    *len = ASN1_STRING_length(asn1);\n    X509_free(cert);\n    return ASN1_STRING_get0_data(asn1);\n}\n\n/*\n * Reads decrypted data from the underlying BIO read buffers,\n * which reads from the socket.\n */\nstatic ssize_t ssl_read(conn *c, void *buf, size_t count) {\n    assert (c != NULL);\n    /* TODO : document the state machine interactions for SSL_read with\n        non-blocking sockets/ SSL re-negotiations\n    */\n\n    ssize_t ret = SSL_read(c->ssl, buf, count);\n    if (ret <= 0) {\n        int err = SSL_get_error(c->ssl, ret);\n        if (err == SSL_ERROR_WANT_WRITE || err == SSL_ERROR_WANT_READ) {\n            errno = EAGAIN;\n        } else if (err == SSL_ERROR_ZERO_RETURN) {\n            // TLS session is closed... let the caller move this along.\n            return 0;\n        } else if (err == SSL_ERROR_SYSCALL) {\n            // need to rely on errno to find out what happened\n            LOGGER_LOG(c->thread->l, LOG_CONNEVENTS, LOGGER_CONNECTION_TLSERROR,\n                    NULL, c->sfd, strerror(errno));\n        } else if (ret != 0) {\n            char ssl_err[SSL_ERROR_MSG_SIZE];\n            // OpenSSL internal error. One or more, but lets only care about\n            // the top error for now.\n            print_ssl_error(ssl_err, SSL_ERROR_MSG_SIZE);\n            LOGGER_LOG(c->thread->l, LOG_CONNEVENTS, LOGGER_CONNECTION_TLSERROR,\n                    NULL, c->sfd, ssl_err);\n            STATS_LOCK();\n            stats.ssl_proto_errors++;\n            STATS_UNLOCK();\n        }\n        ERR_clear_error();\n    }\n\n    return ret;\n}\n\n/*\n * Writes data to the underlying BIO write buffers,\n * which encrypt and write them to the socket.\n */\nstatic ssize_t ssl_write(conn *c, void *buf, size_t count) {\n    assert (c != NULL);\n\n    ssize_t ret = SSL_write(c->ssl, buf, count);\n    if (ret <= 0) {\n        int err = SSL_get_error(c->ssl, ret);\n        if (err == SSL_ERROR_WANT_WRITE || err == SSL_ERROR_WANT_READ) {\n            errno = EAGAIN;\n        } else if (err == SSL_ERROR_ZERO_RETURN) {\n            // TLS session is closed... let the caller move this along.\n            return 0;\n        } else if (err == SSL_ERROR_SYSCALL) {\n            // need to rely on errno to find out what happened\n            LOGGER_LOG(c->thread->l, LOG_CONNEVENTS, LOGGER_CONNECTION_TLSERROR,\n                    NULL, c->sfd, strerror(errno));\n        } else if (ret != 0) {\n            char ssl_err[SSL_ERROR_MSG_SIZE];\n            // OpenSSL internal error. One or more, but lets only care about\n            // the top error for now.\n            print_ssl_error(ssl_err, SSL_ERROR_MSG_SIZE);\n            LOGGER_LOG(c->thread->l, LOG_CONNEVENTS, LOGGER_CONNECTION_TLSERROR,\n                    NULL, c->sfd, ssl_err);\n            STATS_LOCK();\n            stats.ssl_proto_errors++;\n            STATS_UNLOCK();\n        }\n        ERR_clear_error();\n    }\n    return ret;\n}\n\n/*\n * SSL sendmsg implementation. Perform a SSL_write.\n */\nstatic ssize_t ssl_sendmsg(conn *c, struct msghdr *msg, int flags) {\n    assert (c != NULL);\n    size_t buf_remain = settings.ssl_wbuf_size;\n    size_t bytes = 0;\n    size_t to_copy;\n    int i;\n\n    // ssl_wbuf is pointing to the buffer allocated in the worker thread.\n    assert(c->ssl_wbuf);\n    // TODO: allocate a fix buffer in crawler/logger if they start using\n    // the sendmsg method. Also, set c->ssl_wbuf  when the side thread\n    // start owning the connection and reset the pointer in\n    // conn_worker_readd.\n    // Currently this connection would not be served by a different thread\n    // than the one it's assigned.\n    assert(pthread_equal(c->thread->thread_id, pthread_self()) != 0);\n\n    char *bp = c->ssl_wbuf;\n    for (i = 0; i < msg->msg_iovlen; i++) {\n        size_t len = msg->msg_iov[i].iov_len;\n        to_copy = len < buf_remain ? len : buf_remain;\n\n        memcpy(bp + bytes, (void*)msg->msg_iov[i].iov_base, to_copy);\n        buf_remain -= to_copy;\n        bytes += to_copy;\n        if (buf_remain == 0)\n            break;\n    }\n    /* TODO : document the state machine interactions for SSL_write with\n        non-blocking sockets/ SSL re-negotiations\n    */\n    return ssl_write(c, c->ssl_wbuf, bytes);\n}\n\n/*\n * Prints an SSL error into the buff, if there's any.\n */\nstatic void print_ssl_error(char *buff, size_t len) {\n    unsigned long err;\n    if ((err = ERR_get_error()) != 0) {\n        ERR_error_string_n(err, buff, len);\n    }\n}\n\n/*\n * Loads server certificates to the SSL context and validate them.\n * @return whether certificates are successfully loaded and verified or not.\n * @param error_msg contains the error when unsuccessful.\n */\nstatic bool load_server_certificates(char **errmsg) {\n    bool success = false;\n\n    const size_t CRLF_NULLCHAR_LEN = 3;\n    char *error_msg = malloc(MAXPATHLEN + ERROR_MSG_SIZE +\n        SSL_ERROR_MSG_SIZE);\n    size_t errmax = MAXPATHLEN + ERROR_MSG_SIZE + SSL_ERROR_MSG_SIZE -\n        CRLF_NULLCHAR_LEN;\n\n    if (error_msg == NULL) {\n        *errmsg = NULL;\n        return false;\n    }\n\n    if (settings.ssl_ctx == NULL) {\n        snprintf(error_msg, errmax, \"Error TLS not enabled\\r\\n\");\n        *errmsg = error_msg;\n        return false;\n    }\n\n    char *ssl_err_msg = malloc(SSL_ERROR_MSG_SIZE);\n    if (ssl_err_msg == NULL) {\n        free(error_msg);\n        *errmsg = NULL;\n        return false;\n    }\n    bzero(ssl_err_msg, SSL_ERROR_MSG_SIZE);\n    size_t err_msg_size = 0;\n\n    SSL_LOCK();\n    if (!SSL_CTX_use_certificate_chain_file(settings.ssl_ctx,\n        settings.ssl_chain_cert)) {\n        print_ssl_error(ssl_err_msg, SSL_ERROR_MSG_SIZE);\n        err_msg_size = snprintf(error_msg, errmax, \"Error loading the certificate chain: \"\n            \"%s : %s\", settings.ssl_chain_cert, ssl_err_msg);\n    } else if (!SSL_CTX_use_PrivateKey_file(settings.ssl_ctx, settings.ssl_key,\n                                        settings.ssl_keyformat)) {\n        print_ssl_error(ssl_err_msg, SSL_ERROR_MSG_SIZE);\n        err_msg_size = snprintf(error_msg, errmax, \"Error loading the key: %s : %s\",\n            settings.ssl_key, ssl_err_msg);\n    } else if (!SSL_CTX_check_private_key(settings.ssl_ctx)) {\n        print_ssl_error(ssl_err_msg, SSL_ERROR_MSG_SIZE);\n        err_msg_size = snprintf(error_msg, errmax, \"Error validating the certificate: %s\",\n            ssl_err_msg);\n    } else if (settings.ssl_ca_cert) {\n        if (!SSL_CTX_load_verify_locations(settings.ssl_ctx,\n          settings.ssl_ca_cert, NULL)) {\n            print_ssl_error(ssl_err_msg, SSL_ERROR_MSG_SIZE);\n            err_msg_size = snprintf(error_msg, errmax,\n              \"Error loading the CA certificate: %s : %s\",\n              settings.ssl_ca_cert, ssl_err_msg);\n        } else {\n            SSL_CTX_set_client_CA_list(settings.ssl_ctx,\n              SSL_load_client_CA_file(settings.ssl_ca_cert));\n            success = true;\n        }\n    } else {\n        success = true;\n    }\n    SSL_UNLOCK();\n    free(ssl_err_msg);\n    if (success) {\n        settings.ssl_last_cert_refresh_time = current_time;\n        free(error_msg);\n    } else {\n        *errmsg = error_msg;\n        error_msg += (err_msg_size >= errmax ? errmax - 1: err_msg_size);\n        snprintf(error_msg, CRLF_NULLCHAR_LEN, \"\\r\\n\");\n        // Print if there are more errors and drain the queue.\n        ERR_print_errors_fp(stderr);\n    }\n    return success;\n}\n\nvoid ssl_conn_close(void *ssl_in) {\n    SSL *ssl = ssl_in;\n    SSL_shutdown(ssl);\n    SSL_free(ssl);\n}\n\nvoid ssl_init_conn(conn *c, void *ssl_in) {\n    if (ssl_in) {\n        SSL *ssl = ssl_in;\n        c->ssl = (SSL*)ssl;\n        c->read = ssl_read;\n        c->sendmsg = ssl_sendmsg;\n        c->write = ssl_write;\n        SSL_set_info_callback(c->ssl, ssl_callback);\n    }\n}\n\nvoid ssl_init_settings(void) {\n    settings.ssl_enabled = false;\n    settings.ssl_ctx = NULL;\n    settings.ssl_chain_cert = NULL;\n    settings.ssl_key = NULL;\n    settings.ssl_verify_mode = SSL_VERIFY_NONE;\n    settings.ssl_keyformat = SSL_FILETYPE_PEM;\n    settings.ssl_ciphers = NULL;\n    settings.ssl_ca_cert = NULL;\n    settings.ssl_last_cert_refresh_time = current_time;\n    settings.ssl_wbuf_size = 16 * 1024; // default is 16KB (SSL max frame size is 17KB)\n    settings.ssl_session_cache = false;\n    settings.ssl_kernel_tls = false;\n    settings.ssl_min_version = TLS1_2_VERSION;\n}\n\n/*\n * Verify SSL settings and initiates the SSL context.\n */\nint ssl_init(void) {\n    assert(settings.ssl_enabled);\n\n    OPENSSL_init_ssl(0, NULL);\n\n    // SSL context for the process. All connections will share one\n    // process level context.\n    settings.ssl_ctx = SSL_CTX_new(TLS_server_method());\n\n    SSL_CTX_set_min_proto_version(settings.ssl_ctx, settings.ssl_min_version);\n\n    // The server certificate, private key and validations.\n    char *error_msg;\n    if (!load_server_certificates(&error_msg)) {\n        fprintf(stderr, \"%s\", error_msg);\n        free(error_msg);\n        exit(EX_USAGE);\n    }\n\n    // The verification mode of client certificate, default is SSL_VERIFY_PEER.\n    SSL_CTX_set_verify(settings.ssl_ctx, settings.ssl_verify_mode, NULL);\n    if (settings.ssl_ciphers && !SSL_CTX_set_cipher_list(settings.ssl_ctx,\n                                                    settings.ssl_ciphers)) {\n        fprintf(stderr, \"Error setting the provided cipher(s): %s\\n\",\n                settings.ssl_ciphers);\n        exit(EX_USAGE);\n    }\n\n    // Optional session caching; default disabled.\n    if (settings.ssl_session_cache) {\n        SSL_CTX_sess_set_new_cb(settings.ssl_ctx, ssl_new_session_callback);\n        SSL_CTX_set_session_cache_mode(settings.ssl_ctx, SSL_SESS_CACHE_SERVER);\n        SSL_CTX_set_session_id_context(settings.ssl_ctx,\n                                       (const unsigned char *) SESSION_ID_CONTEXT,\n                                       strlen(SESSION_ID_CONTEXT));\n    } else {\n        SSL_CTX_set_session_cache_mode(settings.ssl_ctx, SSL_SESS_CACHE_OFF);\n    }\n\n    // Optional kernel TLS offload; default disabled.\n    if (settings.ssl_kernel_tls) {\n#if defined(SSL_OP_ENABLE_KTLS)\n        SSL_CTX_set_options(settings.ssl_ctx, SSL_OP_ENABLE_KTLS);\n#else\n        fprintf(stderr, \"Kernel TLS offload is not available\\n\");\n        exit(EX_USAGE);\n#endif\n    }\n\n#ifdef SSL_OP_NO_RENEGOTIATION\n    // Disable TLS re-negotiation if SSL_OP_NO_RENEGOTIATION is defined for\n    // openssl 1.1.0h or above\n    SSL_CTX_set_options(settings.ssl_ctx, SSL_OP_NO_RENEGOTIATION);\n#endif\n\n    // Release TLS read/write buffers of idle connections\n    SSL_CTX_set_mode(settings.ssl_ctx, SSL_MODE_RELEASE_BUFFERS);\n\n    return 0;\n}\n\n/*\n * This method is registered with each SSL connection and abort the SSL session\n * if a client initiates a renegotiation for openssl versions before 1.1.0h.\n * For openssl 1.1.0h and above, TLS re-negotiation is disabled by setting the\n * SSL_OP_NO_RENEGOTIATION option in SSL_CTX_set_options.\n */\nvoid ssl_callback(const SSL *s, int where, int ret) {\n    // useful for debugging.\n    // fprintf(stderr, \"WHERE: %d RET: %d CODE: %s LONG: %s\\n\", where, ret, SSL_state_string(s), SSL_state_string_long(s));\n#ifndef SSL_OP_NO_RENEGOTIATION\n    SSL* ssl = (SSL*)s;\n    if (SSL_in_before(ssl)) {\n        fprintf(stderr, \"%d: SSL renegotiation is not supported, \"\n                \"closing the connection\\n\", SSL_get_fd(ssl));\n        SSL_set_shutdown(ssl, SSL_SENT_SHUTDOWN | SSL_RECEIVED_SHUTDOWN);\n        return;\n    }\n#endif\n}\n\n/*\n * This method is invoked with every new successfully negotiated SSL session,\n * when server-side session caching is enabled. Note that this method is not\n * invoked when a session is reused.\n */\nint ssl_new_session_callback(SSL *s, SSL_SESSION *sess) {\n    STATS_LOCK();\n    stats.ssl_new_sessions++;\n    STATS_UNLOCK();\n\n    return 0;\n}\n\nbool refresh_certs(char **errmsg) {\n    return load_server_certificates(errmsg);\n}\n\nvoid ssl_help(void) {\n    printf(\"   - ssl_chain_cert:      certificate chain file in PEM format\\n\"\n           \"   - ssl_key:             private key, if not part of the -ssl_chain_cert\\n\"\n           \"   - ssl_keyformat:       private key format (PEM, DER or ENGINE) (default: PEM)\\n\");\n    printf(\"   - ssl_verify_mode:     peer certificate verification mode, default is 0(None).\\n\"\n           \"                          valid values are 0(None), 1(Request), 2(Require)\\n\"\n           \"                          or 3(Once)\\n\");\n    printf(\"   - ssl_ciphers:         specify cipher list to be used\\n\"\n           \"   - ssl_ca_cert:         PEM format file of acceptable client CA's\\n\"\n           \"   - ssl_wbuf_size:       size in kilobytes of per-connection SSL output buffer\\n\"\n           \"                          (default: %u)\\n\", settings.ssl_wbuf_size / (1 << 10));\n    printf(\"   - ssl_session_cache:   enable server-side SSL session cache, to support session\\n\"\n           \"                          resumption\\n\"\n           \"   - ssl_kernel_tls:      enable kernel TLS offload\\n\"\n           \"   - ssl_min_version:     minimum protocol version to accept (default: %s)\\n\",\n           ssl_proto_text(settings.ssl_min_version));\n#if defined(TLS1_3_VERSION)\n    printf(\"                          valid values are 0(%s), 1(%s), 2(%s), or 3(%s).\\n\",\n           ssl_proto_text(TLS1_VERSION), ssl_proto_text(TLS1_1_VERSION),\n           ssl_proto_text(TLS1_2_VERSION), ssl_proto_text(TLS1_3_VERSION));\n#else\n    printf(\"                          valid values are 0(%s), 1(%s), or 2(%s).\\n\",\n           ssl_proto_text(TLS1_VERSION), ssl_proto_text(TLS1_1_VERSION),\n           ssl_proto_text(TLS1_2_VERSION));\n#endif\n    verify_default(\"ssl_keyformat\", settings.ssl_keyformat == SSL_FILETYPE_PEM);\n    verify_default(\"ssl_verify_mode\", settings.ssl_verify_mode == SSL_VERIFY_NONE);\n    verify_default(\"ssl_min_version\", settings.ssl_min_version == TLS1_2_VERSION);\n}\n\nconst char *ssl_proto_text(int version) {\n    switch (version) {\n        case TLS1_VERSION:\n            return \"tlsv1.0\";\n        case TLS1_1_VERSION:\n            return \"tlsv1.1\";\n        case TLS1_2_VERSION:\n            return \"tlsv1.2\";\n#if defined(TLS1_3_VERSION)\n        case TLS1_3_VERSION:\n            return \"tlsv1.3\";\n#endif\n        default:\n            return \"unknown\";\n    }\n}\n\n// TODO: would be nice to pull the entire set of startup option parsing into\n// here like we do with extstore. To save time I'm only pulling subsection\n// that require openssl headers to start.\nbool ssl_set_verify_mode(int verify) {\n    switch(verify) {\n        case 0:\n            settings.ssl_verify_mode = SSL_VERIFY_NONE;\n            break;\n        case 1:\n            settings.ssl_verify_mode = SSL_VERIFY_PEER;\n            break;\n        case 2:\n            settings.ssl_verify_mode = SSL_VERIFY_PEER |\n                                        SSL_VERIFY_FAIL_IF_NO_PEER_CERT;\n            break;\n        case 3:\n            settings.ssl_verify_mode = SSL_VERIFY_PEER |\n                                        SSL_VERIFY_FAIL_IF_NO_PEER_CERT |\n                                        SSL_VERIFY_CLIENT_ONCE;\n            break;\n        default:\n            return false;\n    }\n    return true;\n}\n\nbool ssl_set_min_version(int version) {\n    switch (version) {\n        case 0:\n            settings.ssl_min_version = TLS1_VERSION;\n            break;\n        case 1:\n            settings.ssl_min_version = TLS1_1_VERSION;\n            break;\n        case 2:\n            settings.ssl_min_version = TLS1_2_VERSION;\n            break;\n#if defined(TLS1_3_VERSION)\n        case 3:\n            settings.ssl_min_version = TLS1_3_VERSION;\n            break;\n#endif\n        default:\n            return false;\n    }\n    return true;\n}\n\n#endif // ifdef TLS\n"
        },
        {
          "name": "tls.h",
          "type": "blob",
          "size": 0.7392578125,
          "content": "#ifndef TLS_H\n#define TLS_H\n\n#define MC_SSL_DISABLED 0\n#define MC_SSL_ENABLED_DEFAULT 1\n#define MC_SSL_ENABLED_NOPEER 2\n#define MC_SSL_ENABLED_PEER 3\n\n#ifdef TLS\nvoid *ssl_accept(conn *c, int sfd, bool *fail);\nconst unsigned char *ssl_get_peer_cn(conn *c, int *len);\nint ssl_init(void);\nvoid ssl_init_settings(void);\nvoid ssl_init_conn(conn *c, void *ssl);\nvoid ssl_conn_close(void *ssl_in);\nbool refresh_certs(char **errmsg);\nvoid ssl_help(void);\nbool ssl_set_verify_mode(int verify);\nbool ssl_set_min_version(int version);\nconst char *ssl_proto_text(int version);\n#else\n#define ssl_init(void)\n#define ssl_init_conn(c, ssl)\n#define ssl_init_settings(void)\n#define ssl_conn_close(ssl)\n#define ssl_accept(c, sfd, fail) NULL\n#define ssl_help()\n#endif\n\n#endif\n"
        },
        {
          "name": "trace.h",
          "type": "blob",
          "size": 3.0751953125,
          "content": "#ifndef TRACE_H\n#define TRACE_H\n\n#ifdef ENABLE_DTRACE\n#include \"memcached_dtrace.h\"\n#else\n#define MEMCACHED_ASSOC_DELETE(arg0, arg1)\n#define MEMCACHED_ASSOC_DELETE_ENABLED() (0)\n#define MEMCACHED_ASSOC_FIND(arg0, arg1, arg2)\n#define MEMCACHED_ASSOC_FIND_ENABLED() (0)\n#define MEMCACHED_ASSOC_INSERT(arg0, arg1)\n#define MEMCACHED_ASSOC_INSERT_ENABLED() (0)\n#define MEMCACHED_COMMAND_ADD(arg0, arg1, arg2, arg3, arg4)\n#define MEMCACHED_COMMAND_ADD_ENABLED() (0)\n#define MEMCACHED_COMMAND_APPEND(arg0, arg1, arg2, arg3, arg4)\n#define MEMCACHED_COMMAND_APPEND_ENABLED() (0)\n#define MEMCACHED_COMMAND_CAS(arg0, arg1, arg2, arg3, arg4)\n#define MEMCACHED_COMMAND_CAS_ENABLED() (0)\n#define MEMCACHED_COMMAND_DECR(arg0, arg1, arg2, arg3)\n#define MEMCACHED_COMMAND_DECR_ENABLED() (0)\n#define MEMCACHED_COMMAND_DELETE(arg0, arg1, arg2)\n#define MEMCACHED_COMMAND_DELETE_ENABLED() (0)\n#define MEMCACHED_COMMAND_GET(arg0, arg1, arg2, arg3, arg4)\n#define MEMCACHED_COMMAND_GET_ENABLED() (0)\n#define MEMCACHED_COMMAND_TOUCH(arg0, arg1, arg2, arg3, arg4)\n#define MEMCACHED_COMMAND_TOUCH_ENABLED() (0)\n#define MEMCACHED_COMMAND_INCR(arg0, arg1, arg2, arg3)\n#define MEMCACHED_COMMAND_INCR_ENABLED() (0)\n#define MEMCACHED_COMMAND_PREPEND(arg0, arg1, arg2, arg3, arg4)\n#define MEMCACHED_COMMAND_PREPEND_ENABLED() (0)\n#define MEMCACHED_COMMAND_REPLACE(arg0, arg1, arg2, arg3, arg4)\n#define MEMCACHED_COMMAND_REPLACE_ENABLED() (0)\n#define MEMCACHED_COMMAND_SET(arg0, arg1, arg2, arg3, arg4)\n#define MEMCACHED_COMMAND_SET_ENABLED() (0)\n#define MEMCACHED_CONN_ALLOCATE(arg0)\n#define MEMCACHED_CONN_ALLOCATE_ENABLED() (0)\n#define MEMCACHED_CONN_CREATE(arg0)\n#define MEMCACHED_CONN_CREATE_ENABLED() (0)\n#define MEMCACHED_CONN_DESTROY(arg0)\n#define MEMCACHED_CONN_DESTROY_ENABLED() (0)\n#define MEMCACHED_CONN_DISPATCH(arg0, arg1)\n#define MEMCACHED_CONN_DISPATCH_ENABLED() (0)\n#define MEMCACHED_CONN_RELEASE(arg0)\n#define MEMCACHED_CONN_RELEASE_ENABLED() (0)\n#define MEMCACHED_ITEM_LINK(arg0, arg1, arg2)\n#define MEMCACHED_ITEM_LINK_ENABLED() (0)\n#define MEMCACHED_ITEM_REMOVE(arg0, arg1, arg2)\n#define MEMCACHED_ITEM_REMOVE_ENABLED() (0)\n#define MEMCACHED_ITEM_REPLACE(arg0, arg1, arg2, arg3, arg4, arg5)\n#define MEMCACHED_ITEM_REPLACE_ENABLED() (0)\n#define MEMCACHED_ITEM_UNLINK(arg0, arg1, arg2)\n#define MEMCACHED_ITEM_UNLINK_ENABLED() (0)\n#define MEMCACHED_ITEM_UPDATE(arg0, arg1, arg2)\n#define MEMCACHED_ITEM_UPDATE_ENABLED() (0)\n#define MEMCACHED_PROCESS_COMMAND_END(arg0, arg1, arg2)\n#define MEMCACHED_PROCESS_COMMAND_END_ENABLED() (0)\n#define MEMCACHED_PROCESS_COMMAND_START(arg0, arg1, arg2)\n#define MEMCACHED_PROCESS_COMMAND_START_ENABLED() (0)\n#define MEMCACHED_SLABS_ALLOCATE(arg0, arg1, arg2)\n#define MEMCACHED_SLABS_ALLOCATE_ENABLED() (0)\n#define MEMCACHED_SLABS_ALLOCATE_FAILED(arg0)\n#define MEMCACHED_SLABS_ALLOCATE_FAILED_ENABLED() (0)\n#define MEMCACHED_SLABS_FREE(arg0, arg1)\n#define MEMCACHED_SLABS_FREE_ENABLED() (0)\n#define MEMCACHED_SLABS_SLABCLASS_ALLOCATE(arg0)\n#define MEMCACHED_SLABS_SLABCLASS_ALLOCATE_ENABLED() (0)\n#define MEMCACHED_SLABS_SLABCLASS_ALLOCATE_FAILED(arg0)\n#define MEMCACHED_SLABS_SLABCLASS_ALLOCATE_FAILED_ENABLED() (0)\n#endif\n\n#endif\n"
        },
        {
          "name": "util.c",
          "type": "blob",
          "size": 6.44921875,
          "content": "#include <stdio.h>\n#include <assert.h>\n#include <ctype.h>\n#include <errno.h>\n#include <string.h>\n#include <stdlib.h>\n#include <stdarg.h>\n\n#include \"memcached.h\"\n\nstatic char *uriencode_map[256];\nstatic char uriencode_str[768];\n\nvoid uriencode_init(void) {\n    int x;\n    char *str = uriencode_str;\n    for (x = 0; x < 256; x++) {\n        if (isalnum(x) || x == '-' || x == '.' || x == '_' || x == '~') {\n            uriencode_map[x] = NULL;\n        } else {\n            snprintf(str, 4, \"%%%02hhX\", (unsigned char)x);\n            uriencode_map[x] = str;\n            str += 3; /* lobbing off the \\0 is fine */\n        }\n    }\n}\n\nbool uriencode(const char *src, char *dst, const size_t srclen, const size_t dstlen) {\n    int x;\n    size_t d = 0;\n    for (x = 0; x < srclen; x++) {\n        if (d + 4 > dstlen)\n            return false;\n        if (uriencode_map[(unsigned char) src[x]] != NULL) {\n            memcpy(&dst[d], uriencode_map[(unsigned char) src[x]], 3);\n            d += 3;\n        } else {\n            dst[d] = src[x];\n            d++;\n        }\n    }\n    dst[d] = '\\0';\n    return true;\n}\n\n/* Avoid warnings on solaris, where isspace() is an index into an array, and gcc uses signed chars */\n#define xisspace(c) isspace((unsigned char)c)\n\nbool safe_strtoull(const char *str, uint64_t *out) {\n    assert(out != NULL);\n    errno = 0;\n    *out = 0;\n    char *endptr;\n    unsigned long long ull = strtoull(str, &endptr, 10);\n    if ((errno == ERANGE) || (str == endptr)) {\n        return false;\n    }\n\n    if (xisspace(*endptr) || (*endptr == '\\0' && endptr != str)) {\n        if ((long long) ull < 0) {\n            /* only check for negative signs in the uncommon case when\n             * the unsigned number is so big that it's negative as a\n             * signed number. */\n            if (memchr(str, '-', endptr - str) != NULL) {\n                return false;\n            }\n        }\n        *out = ull;\n        return true;\n    }\n    return false;\n}\n\n/* Could macro this. Decided to keep this unrolled for safety rather than add\n * the base parameter to all callers. Very few places need to parse a number\n * outside of base 10, currently exactly once, so splitting this up should\n * help avoid typo bugs.\n */\nbool safe_strtoull_hex(const char *str, uint64_t *out) {\n    assert(out != NULL);\n    errno = 0;\n    *out = 0;\n    char *endptr;\n    unsigned long long ull = strtoull(str, &endptr, 16);\n    if ((errno == ERANGE) || (str == endptr)) {\n        return false;\n    }\n\n    if (xisspace(*endptr) || (*endptr == '\\0' && endptr != str)) {\n        if ((long long) ull < 0) {\n            /* only check for negative signs in the uncommon case when\n             * the unsigned number is so big that it's negative as a\n             * signed number. */\n            if (memchr(str, '-', endptr - str) != NULL) {\n                return false;\n            }\n        }\n        *out = ull;\n        return true;\n    }\n    return false;\n}\n\nbool safe_strtoll(const char *str, int64_t *out) {\n    assert(out != NULL);\n    errno = 0;\n    *out = 0;\n    char *endptr;\n    long long ll = strtoll(str, &endptr, 10);\n    if ((errno == ERANGE) || (str == endptr)) {\n        return false;\n    }\n\n    if (xisspace(*endptr) || (*endptr == '\\0' && endptr != str)) {\n        *out = ll;\n        return true;\n    }\n    return false;\n}\n\nbool safe_strtoul(const char *str, uint32_t *out) {\n    char *endptr = NULL;\n    unsigned long l = 0;\n    assert(out);\n    assert(str);\n    *out = 0;\n    errno = 0;\n\n    l = strtoul(str, &endptr, 10);\n    if ((errno == ERANGE) || (str == endptr)) {\n        return false;\n    }\n\n    if (xisspace(*endptr) || (*endptr == '\\0' && endptr != str)) {\n        if ((long) l < 0) {\n            /* only check for negative signs in the uncommon case when\n             * the unsigned number is so big that it's negative as a\n             * signed number. */\n            if (memchr(str, '-', endptr - str) != NULL) {\n                return false;\n            }\n        }\n        *out = l;\n        return true;\n    }\n\n    return false;\n}\n\nbool safe_strtol(const char *str, int32_t *out) {\n    assert(out != NULL);\n    errno = 0;\n    *out = 0;\n    char *endptr;\n    long l = strtol(str, &endptr, 10);\n    if ((errno == ERANGE) || (str == endptr)) {\n        return false;\n    }\n\n    if (xisspace(*endptr) || (*endptr == '\\0' && endptr != str)) {\n        *out = l;\n        return true;\n    }\n    return false;\n}\n\nbool safe_strtod(const char *str, double *out) {\n    assert(out != NULL);\n    errno = 0;\n    *out = 0;\n    char *endptr;\n    double d = strtod(str, &endptr);\n    if ((errno == ERANGE) || (str == endptr)) {\n        return false;\n    }\n\n    if (xisspace(*endptr) || (*endptr == '\\0' && endptr != str)) {\n        *out = d;\n        return true;\n    }\n    return false;\n}\n\n// slow, safe function for copying null terminated buffers.\n// ensures null terminator set on destination buffer. copies at most dstmax-1\n// non-null bytes.\n// Explicitly avoids over-reading src while looking for the null byte.\n// returns true if src was fully copied.\n// returns false if src was truncated into dst.\nbool safe_strcpy(char *dst, const char *src, const size_t dstmax) {\n   size_t x;\n\n   for (x = 0; x < dstmax - 1 && src[x] != '\\0'; x++) {\n        dst[x] = src[x];\n   }\n\n   dst[x] = '\\0';\n\n   if (src[x] == '\\0') {\n       return true;\n   } else {\n       return false;\n   }\n}\n\nbool safe_memcmp(const void *a, const void *b, size_t len) {\n    const volatile unsigned char *ua = (const volatile unsigned char *)a;\n    const volatile unsigned char *ub = (const volatile unsigned char *)b;\n    int delta = 0;\n    size_t x;\n\n    for (x = 0; x < len; x++) {\n        delta |= ua[x] ^ ub[x];\n    }\n\n    if (delta == 0) {\n        return true;\n    } else {\n        return false;\n    }\n}\n\nvoid vperror(const char *fmt, ...) {\n    int old_errno = errno;\n    char buf[1024];\n    va_list ap;\n\n    va_start(ap, fmt);\n    if (vsnprintf(buf, sizeof(buf), fmt, ap) == -1) {\n        buf[sizeof(buf) - 1] = '\\0';\n    }\n    va_end(ap);\n\n    errno = old_errno;\n\n    perror(buf);\n}\n\n#ifndef HAVE_HTONLL\nstatic uint64_t mc_swap64(uint64_t in) {\n#ifdef ENDIAN_LITTLE\n    /* Little endian, flip the bytes around until someone makes a faster/better\n    * way to do this. */\n    int64_t rv = 0;\n    int i = 0;\n     for(i = 0; i<8; i++) {\n        rv = (rv << 8) | (in & 0xff);\n        in >>= 8;\n     }\n    return rv;\n#else\n    /* big-endian machines don't need byte swapping */\n    return in;\n#endif\n}\n\nuint64_t ntohll(uint64_t val) {\n   return mc_swap64(val);\n}\n\nuint64_t htonll(uint64_t val) {\n   return mc_swap64(val);\n}\n#endif\n\n"
        },
        {
          "name": "util.h",
          "type": "blob",
          "size": 1.2998046875,
          "content": "#include \"config.h\"\n\n/* fast-enough functions for uriencoding strings. */\nvoid uriencode_init(void);\nbool uriencode(const char *src, char *dst, const size_t srclen, const size_t dstlen);\n\n/*\n * Wrappers around strtoull/strtoll that are safer and easier to\n * use.  For tests and assumptions, see internal_tests.c.\n *\n * str   a NULL-terminated base decimal 10 unsigned integer\n * out   out parameter, if conversion succeeded\n *\n * returns true if conversion succeeded.\n */\nbool safe_strtoull(const char *str, uint64_t *out);\nbool safe_strtoull_hex(const char *str, uint64_t *out);\nbool safe_strtoll(const char *str, int64_t *out);\nbool safe_strtoul(const char *str, uint32_t *out);\nbool safe_strtol(const char *str, int32_t *out);\nbool safe_strtod(const char *str, double *out);\nbool safe_strcpy(char *dst, const char *src, const size_t dstmax);\nbool safe_memcmp(const void *a, const void *b, size_t len);\n\n#ifndef HAVE_HTONLL\nextern uint64_t htonll(uint64_t);\nextern uint64_t ntohll(uint64_t);\n#endif\n\n#ifdef __GCC\n# define __gcc_attribute__ __attribute__\n#else\n# define __gcc_attribute__(x)\n#endif\n\n/**\n * Vararg variant of perror that makes for more useful error messages\n * when reporting with parameters.\n *\n * @param fmt a printf format\n */\nvoid vperror(const char *fmt, ...)\n    __gcc_attribute__ ((format (printf, 1, 2)));\n"
        },
        {
          "name": "vendor",
          "type": "tree",
          "content": null
        },
        {
          "name": "version.pl",
          "type": "blob",
          "size": 1.412109375,
          "content": "#!/usr/bin/perl\n# If you think this is stupid/overkill, blame dormando\n\nuse warnings;\nuse strict;\n\nmy $version = `git describe`;\nchomp $version;\n# Test the various versions.\n#my $version = 'foob';\n#my $version = '1.4.2-30-gf966dba';\n#my $version = '1.4.3-rc1';\n#my $version = '1.4.3';\nunless ($version =~ m/^\\d+\\.\\d+\\.\\d+/) {\n    write_file('version.m4', \"m4_define([VERSION_NUMBER], [UNKNOWN])\\n\");\n    exit;\n}\n\n$version =~ s/-/_/g;\nwrite_file('version.m4', \"m4_define([VERSION_NUMBER], [$version])\\n\");\nmy ($VERSION, $FULLVERSION, $RELEASE);\n\nif ($version =~ m/^(\\d+\\.\\d+\\.\\d+)_rc(\\d+)$/) {\n    $VERSION = $1;\n    $FULLVERSION = $version;\n    $RELEASE = '0.1.rc' . $2;\n} elsif ($version =~ m/^(\\d+\\.\\d+\\.\\d+)_(.+)$/) {\n    $VERSION = $1;\n    $FULLVERSION = $version;\n    $RELEASE = '1.' . $2;\n} elsif ($version =~ m/^(\\d+\\.\\d+\\.\\d+)$/) {\n    $VERSION = $1;\n    $FULLVERSION = $version;\n    $RELEASE = '1';\n}\n\nmy $spec = read_file('memcached.spec.in');\n$spec =~ s/\\@VERSION\\@/$VERSION/gm;\n$spec =~ s/\\@FULLVERSION\\@/$FULLVERSION/gm;\n$spec =~ s/\\@RELEASE\\@/$RELEASE/gm;\n\nwrite_file('memcached.spec', $spec);\n\nsub write_file {\n    my $file = shift;\n    my $data = shift;\n    open(my $fh, \"> $file\") or die \"Can't open $file: $!\";\n    print $fh $data;\n    close($fh);\n}\n\nsub read_file {\n    my $file = shift;\n    local $/ = undef;\n    open(my $fh, \"< $file\") or die \"Can't open $file: $!\";\n    my $data = <$fh>;\n    close($fh);\n    return $data;\n}\n"
        },
        {
          "name": "version.sh",
          "type": "blob",
          "size": 0.291015625,
          "content": "#!/bin/sh\n\nif git describe | sed s/-/_/g > version.num.tmp\nthen\n    mv version.num.tmp version.num\n    echo \"m4_define([VERSION_NUMBER], [`tr -d '\\n' < version.num`])\" \\\n        > version.m4\n    sed s/@VERSION@/`cat version.num`/ < memcached.spec.in > memcached.spec\nelse\n    rm version.num.tmp\nfi\n"
        },
        {
          "name": "xxhash.h",
          "type": "blob",
          "size": 194.30078125,
          "content": "/*\n * xxHash - Extremely Fast Hash algorithm\n * Header File\n * Copyright (C) 2012-2020 Yann Collet\n *\n * BSD 2-Clause License (https://www.opensource.org/licenses/bsd-license.php)\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are\n * met:\n *\n *    * Redistributions of source code must retain the above copyright\n *      notice, this list of conditions and the following disclaimer.\n *    * Redistributions in binary form must reproduce the above\n *      copyright notice, this list of conditions and the following disclaimer\n *      in the documentation and/or other materials provided with the\n *      distribution.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n * \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n *\n * You can contact the author at:\n *   - xxHash homepage: https://www.xxhash.com\n *   - xxHash source repository: https://github.com/Cyan4973/xxHash\n */\n/*!\n * @mainpage xxHash\n *\n * @file xxhash.h\n * xxHash prototypes and implementation\n */\n/* TODO: update */\n/* Notice extracted from xxHash homepage:\n\nxxHash is an extremely fast hash algorithm, running at RAM speed limits.\nIt also successfully passes all tests from the SMHasher suite.\n\nComparison (single thread, Windows Seven 32 bits, using SMHasher on a Core 2 Duo @3GHz)\n\nName            Speed       Q.Score   Author\nxxHash          5.4 GB/s     10\nCrapWow         3.2 GB/s      2       Andrew\nMurmurHash 3a   2.7 GB/s     10       Austin Appleby\nSpookyHash      2.0 GB/s     10       Bob Jenkins\nSBox            1.4 GB/s      9       Bret Mulvey\nLookup3         1.2 GB/s      9       Bob Jenkins\nSuperFastHash   1.2 GB/s      1       Paul Hsieh\nCityHash64      1.05 GB/s    10       Pike & Alakuijala\nFNV             0.55 GB/s     5       Fowler, Noll, Vo\nCRC32           0.43 GB/s     9\nMD5-32          0.33 GB/s    10       Ronald L. Rivest\nSHA1-32         0.28 GB/s    10\n\nQ.Score is a measure of quality of the hash function.\nIt depends on successfully passing SMHasher test set.\n10 is a perfect score.\n\nNote: SMHasher's CRC32 implementation is not the fastest one.\nOther speed-oriented implementations can be faster,\nespecially in combination with PCLMUL instruction:\nhttps://fastcompression.blogspot.com/2019/03/presenting-xxh3.html?showComment=1552696407071#c3490092340461170735\n\nA 64-bit version, named XXH64, is available since r35.\nIt offers much better speed, but for 64-bit applications only.\nName     Speed on 64 bits    Speed on 32 bits\nXXH64       13.8 GB/s            1.9 GB/s\nXXH32        6.8 GB/s            6.0 GB/s\n*/\n\n#if defined (__cplusplus)\nextern \"C\" {\n#endif\n\n/* ****************************\n *  INLINE mode\n ******************************/\n/*!\n * XXH_INLINE_ALL (and XXH_PRIVATE_API)\n * Use these build macros to inline xxhash into the target unit.\n * Inlining improves performance on small inputs, especially when the length is\n * expressed as a compile-time constant:\n *\n *      https://fastcompression.blogspot.com/2018/03/xxhash-for-small-keys-impressive-power.html\n *\n * It also keeps xxHash symbols private to the unit, so they are not exported.\n *\n * Usage:\n *     #define XXH_INLINE_ALL\n *     #include \"xxhash.h\"\n *\n * Do not compile and link xxhash.o as a separate object, as it is not useful.\n */\n#if (defined(XXH_INLINE_ALL) || defined(XXH_PRIVATE_API)) \\\n    && !defined(XXH_INLINE_ALL_31684351384)\n   /* this section should be traversed only once */\n#  define XXH_INLINE_ALL_31684351384\n   /* give access to the advanced API, required to compile implementations */\n#  undef XXH_STATIC_LINKING_ONLY   /* avoid macro redef */\n#  define XXH_STATIC_LINKING_ONLY\n   /* make all functions private */\n#  undef XXH_PUBLIC_API\n#  if defined(__GNUC__)\n#    define XXH_PUBLIC_API static __inline __attribute__((unused))\n#  elif defined (__cplusplus) || (defined (__STDC_VERSION__) && (__STDC_VERSION__ >= 199901L) /* C99 */)\n#    define XXH_PUBLIC_API static inline\n#  elif defined(_MSC_VER)\n#    define XXH_PUBLIC_API static __inline\n#  else\n     /* note: this version may generate warnings for unused static functions */\n#    define XXH_PUBLIC_API static\n#  endif\n\n   /*\n    * This part deals with the special case where a unit wants to inline xxHash,\n    * but \"xxhash.h\" has previously been included without XXH_INLINE_ALL, such\n    * as part of some previously included *.h header file.\n    * Without further action, the new include would just be ignored,\n    * and functions would effectively _not_ be inlined (silent failure).\n    * The following macros solve this situation by prefixing all inlined names,\n    * avoiding naming collision with previous inclusions.\n    */\n#  ifdef XXH_NAMESPACE\n#    error \"XXH_INLINE_ALL with XXH_NAMESPACE is not supported\"\n     /*\n      * Note: Alternative: #undef all symbols (it's a pretty large list).\n      * Without #error: it compiles, but functions are actually not inlined.\n      */\n#  endif\n#  define XXH_NAMESPACE XXH_INLINE_\n   /*\n    * Some identifiers (enums, type names) are not symbols, but they must\n    * still be renamed to avoid redeclaration.\n    * Alternative solution: do not redeclare them.\n    * However, this requires some #ifdefs, and is a more dispersed action.\n    * Meanwhile, renaming can be achieved in a single block\n    */\n#  define XXH_IPREF(Id)   XXH_INLINE_ ## Id\n#  define XXH_OK XXH_IPREF(XXH_OK)\n#  define XXH_ERROR XXH_IPREF(XXH_ERROR)\n#  define XXH_errorcode XXH_IPREF(XXH_errorcode)\n#  define XXH32_canonical_t  XXH_IPREF(XXH32_canonical_t)\n#  define XXH64_canonical_t  XXH_IPREF(XXH64_canonical_t)\n#  define XXH128_canonical_t XXH_IPREF(XXH128_canonical_t)\n#  define XXH32_state_s XXH_IPREF(XXH32_state_s)\n#  define XXH32_state_t XXH_IPREF(XXH32_state_t)\n#  define XXH64_state_s XXH_IPREF(XXH64_state_s)\n#  define XXH64_state_t XXH_IPREF(XXH64_state_t)\n#  define XXH3_state_s  XXH_IPREF(XXH3_state_s)\n#  define XXH3_state_t  XXH_IPREF(XXH3_state_t)\n#  define XXH128_hash_t XXH_IPREF(XXH128_hash_t)\n   /* Ensure the header is parsed again, even if it was previously included */\n#  undef XXHASH_H_5627135585666179\n#  undef XXHASH_H_STATIC_13879238742\n#endif /* XXH_INLINE_ALL || XXH_PRIVATE_API */\n\n\n\n/* ****************************************************************\n *  Stable API\n *****************************************************************/\n#ifndef XXHASH_H_5627135585666179\n#define XXHASH_H_5627135585666179 1\n\n\n/*!\n * @defgroup public Public API\n * Contains details on the public xxHash functions.\n * @{\n */\n/* specific declaration modes for Windows */\n#if !defined(XXH_INLINE_ALL) && !defined(XXH_PRIVATE_API)\n#  if defined(WIN32) && defined(_MSC_VER) && (defined(XXH_IMPORT) || defined(XXH_EXPORT))\n#    ifdef XXH_EXPORT\n#      define XXH_PUBLIC_API __declspec(dllexport)\n#    elif XXH_IMPORT\n#      define XXH_PUBLIC_API __declspec(dllimport)\n#    endif\n#  else\n#    define XXH_PUBLIC_API   /* do nothing */\n#  endif\n#endif\n\n#ifdef XXH_DOXYGEN\n/*!\n * @brief Emulate a namespace by transparently prefixing all symbols.\n *\n * If you want to include _and expose_ xxHash functions from within your own\n * library, but also want to avoid symbol collisions with other libraries which\n * may also include xxHash, you can use XXH_NAMESPACE to automatically prefix\n * any public symbol from xxhash library with the value of XXH_NAMESPACE\n * (therefore, avoid empty or numeric values).\n *\n * Note that no change is required within the calling program as long as it\n * includes `xxhash.h`: Regular symbol names will be automatically translated\n * by this header.\n */\n#  define XXH_NAMESPACE /* YOUR NAME HERE */\n#  undef XXH_NAMESPACE\n#endif\n\n#ifdef XXH_NAMESPACE\n#  define XXH_CAT(A,B) A##B\n#  define XXH_NAME2(A,B) XXH_CAT(A,B)\n#  define XXH_versionNumber XXH_NAME2(XXH_NAMESPACE, XXH_versionNumber)\n/* XXH32 */\n#  define XXH32 XXH_NAME2(XXH_NAMESPACE, XXH32)\n#  define XXH32_createState XXH_NAME2(XXH_NAMESPACE, XXH32_createState)\n#  define XXH32_freeState XXH_NAME2(XXH_NAMESPACE, XXH32_freeState)\n#  define XXH32_reset XXH_NAME2(XXH_NAMESPACE, XXH32_reset)\n#  define XXH32_update XXH_NAME2(XXH_NAMESPACE, XXH32_update)\n#  define XXH32_digest XXH_NAME2(XXH_NAMESPACE, XXH32_digest)\n#  define XXH32_copyState XXH_NAME2(XXH_NAMESPACE, XXH32_copyState)\n#  define XXH32_canonicalFromHash XXH_NAME2(XXH_NAMESPACE, XXH32_canonicalFromHash)\n#  define XXH32_hashFromCanonical XXH_NAME2(XXH_NAMESPACE, XXH32_hashFromCanonical)\n/* XXH64 */\n#  define XXH64 XXH_NAME2(XXH_NAMESPACE, XXH64)\n#  define XXH64_createState XXH_NAME2(XXH_NAMESPACE, XXH64_createState)\n#  define XXH64_freeState XXH_NAME2(XXH_NAMESPACE, XXH64_freeState)\n#  define XXH64_reset XXH_NAME2(XXH_NAMESPACE, XXH64_reset)\n#  define XXH64_update XXH_NAME2(XXH_NAMESPACE, XXH64_update)\n#  define XXH64_digest XXH_NAME2(XXH_NAMESPACE, XXH64_digest)\n#  define XXH64_copyState XXH_NAME2(XXH_NAMESPACE, XXH64_copyState)\n#  define XXH64_canonicalFromHash XXH_NAME2(XXH_NAMESPACE, XXH64_canonicalFromHash)\n#  define XXH64_hashFromCanonical XXH_NAME2(XXH_NAMESPACE, XXH64_hashFromCanonical)\n/* XXH3_64bits */\n#  define XXH3_64bits XXH_NAME2(XXH_NAMESPACE, XXH3_64bits)\n#  define XXH3_64bits_withSecret XXH_NAME2(XXH_NAMESPACE, XXH3_64bits_withSecret)\n#  define XXH3_64bits_withSeed XXH_NAME2(XXH_NAMESPACE, XXH3_64bits_withSeed)\n#  define XXH3_createState XXH_NAME2(XXH_NAMESPACE, XXH3_createState)\n#  define XXH3_freeState XXH_NAME2(XXH_NAMESPACE, XXH3_freeState)\n#  define XXH3_copyState XXH_NAME2(XXH_NAMESPACE, XXH3_copyState)\n#  define XXH3_64bits_reset XXH_NAME2(XXH_NAMESPACE, XXH3_64bits_reset)\n#  define XXH3_64bits_reset_withSeed XXH_NAME2(XXH_NAMESPACE, XXH3_64bits_reset_withSeed)\n#  define XXH3_64bits_reset_withSecret XXH_NAME2(XXH_NAMESPACE, XXH3_64bits_reset_withSecret)\n#  define XXH3_64bits_update XXH_NAME2(XXH_NAMESPACE, XXH3_64bits_update)\n#  define XXH3_64bits_digest XXH_NAME2(XXH_NAMESPACE, XXH3_64bits_digest)\n#  define XXH3_generateSecret XXH_NAME2(XXH_NAMESPACE, XXH3_generateSecret)\n/* XXH3_128bits */\n#  define XXH128 XXH_NAME2(XXH_NAMESPACE, XXH128)\n#  define XXH3_128bits XXH_NAME2(XXH_NAMESPACE, XXH3_128bits)\n#  define XXH3_128bits_withSeed XXH_NAME2(XXH_NAMESPACE, XXH3_128bits_withSeed)\n#  define XXH3_128bits_withSecret XXH_NAME2(XXH_NAMESPACE, XXH3_128bits_withSecret)\n#  define XXH3_128bits_reset XXH_NAME2(XXH_NAMESPACE, XXH3_128bits_reset)\n#  define XXH3_128bits_reset_withSeed XXH_NAME2(XXH_NAMESPACE, XXH3_128bits_reset_withSeed)\n#  define XXH3_128bits_reset_withSecret XXH_NAME2(XXH_NAMESPACE, XXH3_128bits_reset_withSecret)\n#  define XXH3_128bits_update XXH_NAME2(XXH_NAMESPACE, XXH3_128bits_update)\n#  define XXH3_128bits_digest XXH_NAME2(XXH_NAMESPACE, XXH3_128bits_digest)\n#  define XXH128_isEqual XXH_NAME2(XXH_NAMESPACE, XXH128_isEqual)\n#  define XXH128_cmp     XXH_NAME2(XXH_NAMESPACE, XXH128_cmp)\n#  define XXH128_canonicalFromHash XXH_NAME2(XXH_NAMESPACE, XXH128_canonicalFromHash)\n#  define XXH128_hashFromCanonical XXH_NAME2(XXH_NAMESPACE, XXH128_hashFromCanonical)\n#endif\n\n\n/* *************************************\n*  Version\n***************************************/\n#define XXH_VERSION_MAJOR    0\n#define XXH_VERSION_MINOR    8\n#define XXH_VERSION_RELEASE  0\n#define XXH_VERSION_NUMBER  (XXH_VERSION_MAJOR *100*100 + XXH_VERSION_MINOR *100 + XXH_VERSION_RELEASE)\n\n/*!\n * @brief Obtains the xxHash version.\n *\n * This is only useful when xxHash is compiled as a shared library, as it is\n * independent of the version defined in the header.\n *\n * @return `XXH_VERSION_NUMBER` as of when the function was compiled.\n */\nXXH_PUBLIC_API unsigned XXH_versionNumber (void);\n\n\n/* ****************************\n*  Definitions\n******************************/\n#include <stddef.h>   /* size_t */\ntypedef enum { XXH_OK=0, XXH_ERROR } XXH_errorcode;\n\n\n/*-**********************************************************************\n*  32-bit hash\n************************************************************************/\n#if defined(XXH_DOXYGEN) /* Don't show <stdint.h> include */\n/*!\n * @brief An unsigned 32-bit integer.\n *\n * Not necessarily defined to `uint32_t` but functionally equivalent.\n */\ntypedef uint32_t XXH32_hash_t;\n#elif !defined (__VMS) \\\n  && (defined (__cplusplus) \\\n  || (defined (__STDC_VERSION__) && (__STDC_VERSION__ >= 199901L) /* C99 */) )\n#   include <stdint.h>\n    typedef uint32_t XXH32_hash_t;\n#else\n#   include <limits.h>\n#   if UINT_MAX == 0xFFFFFFFFUL\n      typedef unsigned int XXH32_hash_t;\n#   else\n#     if ULONG_MAX == 0xFFFFFFFFUL\n        typedef unsigned long XXH32_hash_t;\n#     else\n#       error \"unsupported platform: need a 32-bit type\"\n#     endif\n#   endif\n#endif\n\n/*!\n * @}\n *\n * @defgroup xxh32_family XXH32 family\n * @ingroup public\n * Contains functions used in the classic 32-bit xxHash algorithm.\n *\n * @note\n *   XXH32 is considered rather weak by today's standards.\n *   The @ref xxh3_family provides competitive speed for both 32-bit and 64-bit\n *   systems, and offers true 64/128 bit hash results. It provides a superior\n *   level of dispersion, and greatly reduces the risks of collisions.\n *\n * @see @ref xxh64_family, @ref xxh3_family : Other xxHash families\n * @see @ref xxh32_impl for implementation details\n * @{\n */\n\n/*!\n * @brief Calculates the 32-bit hash of @p input using xxHash32.\n *\n * Speed on Core 2 Duo @ 3 GHz (single thread, SMHasher benchmark): 5.4 GB/s\n *\n * @param input The block of data to be hashed, at least @p length bytes in size.\n * @param length The length of @p input, in bytes.\n * @param seed The 32-bit seed to alter the hash's output predictably.\n *\n * @pre\n *   The memory between @p input and @p input + @p length must be valid,\n *   readable, contiguous memory. However, if @p length is `0`, @p input may be\n *   `NULL`. In C++, this also must be *TriviallyCopyable*.\n *\n * @return The calculated 32-bit hash value.\n *\n * @see\n *    XXH64(), XXH3_64bits_withSeed(), XXH3_128bits_withSeed(), XXH128():\n *    Direct equivalents for the other variants of xxHash.\n * @see\n *    XXH32_createState(), XXH32_update(), XXH32_digest(): Streaming version.\n */\nXXH_PUBLIC_API XXH32_hash_t XXH32 (const void* input, size_t length, XXH32_hash_t seed);\n\n/*!\n * Streaming functions generate the xxHash value from an incremental input.\n * This method is slower than single-call functions, due to state management.\n * For small inputs, prefer `XXH32()` and `XXH64()`, which are better optimized.\n *\n * An XXH state must first be allocated using `XXH*_createState()`.\n *\n * Start a new hash by initializing the state with a seed using `XXH*_reset()`.\n *\n * Then, feed the hash state by calling `XXH*_update()` as many times as necessary.\n *\n * The function returns an error code, with 0 meaning OK, and any other value\n * meaning there is an error.\n *\n * Finally, a hash value can be produced anytime, by using `XXH*_digest()`.\n * This function returns the nn-bits hash as an int or long long.\n *\n * It's still possible to continue inserting input into the hash state after a\n * digest, and generate new hash values later on by invoking `XXH*_digest()`.\n *\n * When done, release the state using `XXH*_freeState()`.\n *\n * Example code for incrementally hashing a file:\n * @code{.c}\n *    #include <stdio.h>\n *    #include <xxhash.h>\n *    #define BUFFER_SIZE 256\n *\n *    // Note: XXH64 and XXH3 use the same interface.\n *    XXH32_hash_t\n *    hashFile(FILE* stream)\n *    {\n *        XXH32_state_t* state;\n *        unsigned char buf[BUFFER_SIZE];\n *        size_t amt;\n *        XXH32_hash_t hash;\n *\n *        state = XXH32_createState();       // Create a state\n *        assert(state != NULL);             // Error check here\n *        XXH32_reset(state, 0xbaad5eed);    // Reset state with our seed\n *        while ((amt = fread(buf, 1, sizeof(buf), stream)) != 0) {\n *            XXH32_update(state, buf, amt); // Hash the file in chunks\n *        }\n *        hash = XXH32_digest(state);        // Finalize the hash\n *        XXH32_freeState(state);            // Clean up\n *        return hash;\n *    }\n * @endcode\n */\n\n/*!\n * @typedef struct XXH32_state_s XXH32_state_t\n * @brief The opaque state struct for the XXH32 streaming API.\n *\n * @see XXH32_state_s for details.\n */\ntypedef struct XXH32_state_s XXH32_state_t;\n\n/*!\n * @brief Allocates an @ref XXH32_state_t.\n *\n * Must be freed with XXH32_freeState().\n * @return An allocated XXH32_state_t on success, `NULL` on failure.\n */\nXXH_PUBLIC_API XXH32_state_t* XXH32_createState(void);\n/*!\n * @brief Frees an @ref XXH32_state_t.\n *\n * Must be allocated with XXH32_createState().\n * @param statePtr A pointer to an @ref XXH32_state_t allocated with @ref XXH32_createState().\n * @return XXH_OK.\n */\nXXH_PUBLIC_API XXH_errorcode  XXH32_freeState(XXH32_state_t* statePtr);\n/*!\n * @brief Copies one @ref XXH32_state_t to another.\n *\n * @param dst_state The state to copy to.\n * @param src_state The state to copy from.\n * @pre\n *   @p dst_state and @p src_state must not be `NULL` and must not overlap.\n */\nXXH_PUBLIC_API void XXH32_copyState(XXH32_state_t* dst_state, const XXH32_state_t* src_state);\n\n/*!\n * @brief Resets an @ref XXH32_state_t to begin a new hash.\n *\n * This function resets and seeds a state. Call it before @ref XXH32_update().\n *\n * @param statePtr The state struct to reset.\n * @param seed The 32-bit seed to alter the hash result predictably.\n *\n * @pre\n *   @p statePtr must not be `NULL`.\n *\n * @return @ref XXH_OK on success, @ref XXH_ERROR on failure.\n */\nXXH_PUBLIC_API XXH_errorcode XXH32_reset  (XXH32_state_t* statePtr, XXH32_hash_t seed);\n\n/*!\n * @brief Consumes a block of @p input to an @ref XXH32_state_t.\n *\n * Call this to incrementally consume blocks of data.\n *\n * @param statePtr The state struct to update.\n * @param input The block of data to be hashed, at least @p length bytes in size.\n * @param length The length of @p input, in bytes.\n *\n * @pre\n *   @p statePtr must not be `NULL`.\n * @pre\n *   The memory between @p input and @p input + @p length must be valid,\n *   readable, contiguous memory. However, if @p length is `0`, @p input may be\n *   `NULL`. In C++, this also must be *TriviallyCopyable*.\n *\n * @return @ref XXH_OK on success, @ref XXH_ERROR on failure.\n */\nXXH_PUBLIC_API XXH_errorcode XXH32_update (XXH32_state_t* statePtr, const void* input, size_t length);\n\n/*!\n * @brief Returns the calculated hash value from an @ref XXH32_state_t.\n *\n * @note\n *   Calling XXH32_digest() will not affect @p statePtr, so you can update,\n *   digest, and update again.\n *\n * @param statePtr The state struct to calculate the hash from.\n *\n * @pre\n *  @p statePtr must not be `NULL`.\n *\n * @return The calculated xxHash32 value from that state.\n */\nXXH_PUBLIC_API XXH32_hash_t  XXH32_digest (const XXH32_state_t* statePtr);\n\n/*******   Canonical representation   *******/\n\n/*\n * The default return values from XXH functions are unsigned 32 and 64 bit\n * integers.\n * This the simplest and fastest format for further post-processing.\n *\n * However, this leaves open the question of what is the order on the byte level,\n * since little and big endian conventions will store the same number differently.\n *\n * The canonical representation settles this issue by mandating big-endian\n * convention, the same convention as human-readable numbers (large digits first).\n *\n * When writing hash values to storage, sending them over a network, or printing\n * them, it's highly recommended to use the canonical representation to ensure\n * portability across a wider range of systems, present and future.\n *\n * The following functions allow transformation of hash values to and from\n * canonical format.\n */\n\n/*!\n * @brief Canonical (big endian) representation of @ref XXH32_hash_t.\n */\ntypedef struct {\n    unsigned char digest[4]; /*!< Hash bytes, big endian */\n} XXH32_canonical_t;\n\n/*!\n * @brief Converts an @ref XXH32_hash_t to a big endian @ref XXH32_canonical_t.\n *\n * @param dst The @ref XXH32_canonical_t pointer to be stored to.\n * @param hash The @ref XXH32_hash_t to be converted.\n *\n * @pre\n *   @p dst must not be `NULL`.\n */\nXXH_PUBLIC_API void XXH32_canonicalFromHash(XXH32_canonical_t* dst, XXH32_hash_t hash);\n\n/*!\n * @brief Converts an @ref XXH32_canonical_t to a native @ref XXH32_hash_t.\n *\n * @param src The @ref XXH32_canonical_t to convert.\n *\n * @pre\n *   @p src must not be `NULL`.\n *\n * @return The converted hash.\n */\nXXH_PUBLIC_API XXH32_hash_t XXH32_hashFromCanonical(const XXH32_canonical_t* src);\n\n\n/*!\n * @}\n * @ingroup public\n * @{\n */\n\n#ifndef XXH_NO_LONG_LONG\n/*-**********************************************************************\n*  64-bit hash\n************************************************************************/\n#if defined(XXH_DOXYGEN) /* don't include <stdint.h> */\n/*!\n * @brief An unsigned 64-bit integer.\n *\n * Not necessarily defined to `uint64_t` but functionally equivalent.\n */\ntypedef uint64_t XXH64_hash_t;\n#elif !defined (__VMS) \\\n  && (defined (__cplusplus) \\\n  || (defined (__STDC_VERSION__) && (__STDC_VERSION__ >= 199901L) /* C99 */) )\n#  include <stdint.h>\n   typedef uint64_t XXH64_hash_t;\n#else\n#  include <limits.h>\n#  if defined(__LP64__) && ULONG_MAX == 0xFFFFFFFFFFFFFFFFULL\n     /* LP64 ABI says uint64_t is unsigned long */\n     typedef unsigned long XXH64_hash_t;\n#  else\n     /* the following type must have a width of 64-bit */\n     typedef unsigned long long XXH64_hash_t;\n#  endif\n#endif\n\n/*!\n * @}\n *\n * @defgroup xxh64_family XXH64 family\n * @ingroup public\n * @{\n * Contains functions used in the classic 64-bit xxHash algorithm.\n *\n * @note\n *   XXH3 provides competitive speed for both 32-bit and 64-bit systems,\n *   and offers true 64/128 bit hash results. It provides a superior level of\n *   dispersion, and greatly reduces the risks of collisions.\n */\n\n\n/*!\n * @brief Calculates the 64-bit hash of @p input using xxHash64.\n *\n * This function usually runs faster on 64-bit systems, but slower on 32-bit\n * systems (see benchmark).\n *\n * @param input The block of data to be hashed, at least @p length bytes in size.\n * @param length The length of @p input, in bytes.\n * @param seed The 64-bit seed to alter the hash's output predictably.\n *\n * @pre\n *   The memory between @p input and @p input + @p length must be valid,\n *   readable, contiguous memory. However, if @p length is `0`, @p input may be\n *   `NULL`. In C++, this also must be *TriviallyCopyable*.\n *\n * @return The calculated 64-bit hash.\n *\n * @see\n *    XXH32(), XXH3_64bits_withSeed(), XXH3_128bits_withSeed(), XXH128():\n *    Direct equivalents for the other variants of xxHash.\n * @see\n *    XXH64_createState(), XXH64_update(), XXH64_digest(): Streaming version.\n */\nXXH_PUBLIC_API XXH64_hash_t XXH64(const void* input, size_t length, XXH64_hash_t seed);\n\n/*******   Streaming   *******/\n/*!\n * @brief The opaque state struct for the XXH64 streaming API.\n *\n * @see XXH64_state_s for details.\n */\ntypedef struct XXH64_state_s XXH64_state_t;   /* incomplete type */\nXXH_PUBLIC_API XXH64_state_t* XXH64_createState(void);\nXXH_PUBLIC_API XXH_errorcode  XXH64_freeState(XXH64_state_t* statePtr);\nXXH_PUBLIC_API void XXH64_copyState(XXH64_state_t* dst_state, const XXH64_state_t* src_state);\n\nXXH_PUBLIC_API XXH_errorcode XXH64_reset  (XXH64_state_t* statePtr, XXH64_hash_t seed);\nXXH_PUBLIC_API XXH_errorcode XXH64_update (XXH64_state_t* statePtr, const void* input, size_t length);\nXXH_PUBLIC_API XXH64_hash_t  XXH64_digest (const XXH64_state_t* statePtr);\n\n/*******   Canonical representation   *******/\ntypedef struct { unsigned char digest[sizeof(XXH64_hash_t)]; } XXH64_canonical_t;\nXXH_PUBLIC_API void XXH64_canonicalFromHash(XXH64_canonical_t* dst, XXH64_hash_t hash);\nXXH_PUBLIC_API XXH64_hash_t XXH64_hashFromCanonical(const XXH64_canonical_t* src);\n\n/*!\n * @}\n * ************************************************************************\n * @defgroup xxh3_family XXH3 family\n * @ingroup public\n * @{\n *\n * XXH3 is a more recent hash algorithm featuring:\n *  - Improved speed for both small and large inputs\n *  - True 64-bit and 128-bit outputs\n *  - SIMD acceleration\n *  - Improved 32-bit viability\n *\n * Speed analysis methodology is explained here:\n *\n *    https://fastcompression.blogspot.com/2019/03/presenting-xxh3.html\n *\n * Compared to XXH64, expect XXH3 to run approximately\n * ~2x faster on large inputs and >3x faster on small ones,\n * exact differences vary depending on platform.\n *\n * XXH3's speed benefits greatly from SIMD and 64-bit arithmetic,\n * but does not require it.\n * Any 32-bit and 64-bit targets that can run XXH32 smoothly\n * can run XXH3 at competitive speeds, even without vector support.\n * Further details are explained in the implementation.\n *\n * Optimized implementations are provided for AVX512, AVX2, SSE2, NEON, POWER8,\n * ZVector and scalar targets. This can be controlled via the XXH_VECTOR macro.\n *\n * XXH3 implementation is portable:\n * it has a generic C90 formulation that can be compiled on any platform,\n * all implementations generage exactly the same hash value on all platforms.\n * Starting from v0.8.0, it's also labelled \"stable\", meaning that\n * any future version will also generate the same hash value.\n *\n * XXH3 offers 2 variants, _64bits and _128bits.\n *\n * When only 64 bits are needed, prefer invoking the _64bits variant, as it\n * reduces the amount of mixing, resulting in faster speed on small inputs.\n * It's also generally simpler to manipulate a scalar return type than a struct.\n *\n * The API supports one-shot hashing, streaming mode, and custom secrets.\n */\n\n/*-**********************************************************************\n*  XXH3 64-bit variant\n************************************************************************/\n\n/* XXH3_64bits():\n * default 64-bit variant, using default secret and default seed of 0.\n * It's the fastest variant. */\nXXH_PUBLIC_API XXH64_hash_t XXH3_64bits(const void* data, size_t len);\n\n/*\n * XXH3_64bits_withSeed():\n * This variant generates a custom secret on the fly\n * based on default secret altered using the `seed` value.\n * While this operation is decently fast, note that it's not completely free.\n * Note: seed==0 produces the same results as XXH3_64bits().\n */\nXXH_PUBLIC_API XXH64_hash_t XXH3_64bits_withSeed(const void* data, size_t len, XXH64_hash_t seed);\n\n/*!\n * The bare minimum size for a custom secret.\n *\n * @see\n *  XXH3_64bits_withSecret(), XXH3_64bits_reset_withSecret(),\n *  XXH3_128bits_withSecret(), XXH3_128bits_reset_withSecret().\n */\n#define XXH3_SECRET_SIZE_MIN 136\n\n/*\n * XXH3_64bits_withSecret():\n * It's possible to provide any blob of bytes as a \"secret\" to generate the hash.\n * This makes it more difficult for an external actor to prepare an intentional collision.\n * The main condition is that secretSize *must* be large enough (>= XXH3_SECRET_SIZE_MIN).\n * However, the quality of produced hash values depends on secret's entropy.\n * Technically, the secret must look like a bunch of random bytes.\n * Avoid \"trivial\" or structured data such as repeated sequences or a text document.\n * Whenever unsure about the \"randomness\" of the blob of bytes,\n * consider relabelling it as a \"custom seed\" instead,\n * and employ \"XXH3_generateSecret()\" (see below)\n * to generate a high entropy secret derived from the custom seed.\n */\nXXH_PUBLIC_API XXH64_hash_t XXH3_64bits_withSecret(const void* data, size_t len, const void* secret, size_t secretSize);\n\n\n/*******   Streaming   *******/\n/*\n * Streaming requires state maintenance.\n * This operation costs memory and CPU.\n * As a consequence, streaming is slower than one-shot hashing.\n * For better performance, prefer one-shot functions whenever applicable.\n */\n\n/*!\n * @brief The state struct for the XXH3 streaming API.\n *\n * @see XXH3_state_s for details.\n */\ntypedef struct XXH3_state_s XXH3_state_t;\nXXH_PUBLIC_API XXH3_state_t* XXH3_createState(void);\nXXH_PUBLIC_API XXH_errorcode XXH3_freeState(XXH3_state_t* statePtr);\nXXH_PUBLIC_API void XXH3_copyState(XXH3_state_t* dst_state, const XXH3_state_t* src_state);\n\n/*\n * XXH3_64bits_reset():\n * Initialize with default parameters.\n * digest will be equivalent to `XXH3_64bits()`.\n */\nXXH_PUBLIC_API XXH_errorcode XXH3_64bits_reset(XXH3_state_t* statePtr);\n/*\n * XXH3_64bits_reset_withSeed():\n * Generate a custom secret from `seed`, and store it into `statePtr`.\n * digest will be equivalent to `XXH3_64bits_withSeed()`.\n */\nXXH_PUBLIC_API XXH_errorcode XXH3_64bits_reset_withSeed(XXH3_state_t* statePtr, XXH64_hash_t seed);\n/*\n * XXH3_64bits_reset_withSecret():\n * `secret` is referenced, it _must outlive_ the hash streaming session.\n * Similar to one-shot API, `secretSize` must be >= `XXH3_SECRET_SIZE_MIN`,\n * and the quality of produced hash values depends on secret's entropy\n * (secret's content should look like a bunch of random bytes).\n * When in doubt about the randomness of a candidate `secret`,\n * consider employing `XXH3_generateSecret()` instead (see below).\n */\nXXH_PUBLIC_API XXH_errorcode XXH3_64bits_reset_withSecret(XXH3_state_t* statePtr, const void* secret, size_t secretSize);\n\nXXH_PUBLIC_API XXH_errorcode XXH3_64bits_update (XXH3_state_t* statePtr, const void* input, size_t length);\nXXH_PUBLIC_API XXH64_hash_t  XXH3_64bits_digest (const XXH3_state_t* statePtr);\n\n/* note : canonical representation of XXH3 is the same as XXH64\n * since they both produce XXH64_hash_t values */\n\n\n/*-**********************************************************************\n*  XXH3 128-bit variant\n************************************************************************/\n\n/*!\n * @brief The return value from 128-bit hashes.\n *\n * Stored in little endian order, although the fields themselves are in native\n * endianness.\n */\ntypedef struct {\n    XXH64_hash_t low64;   /*!< `value & 0xFFFFFFFFFFFFFFFF` */\n    XXH64_hash_t high64;  /*!< `value >> 64` */\n} XXH128_hash_t;\n\nXXH_PUBLIC_API XXH128_hash_t XXH3_128bits(const void* data, size_t len);\nXXH_PUBLIC_API XXH128_hash_t XXH3_128bits_withSeed(const void* data, size_t len, XXH64_hash_t seed);\nXXH_PUBLIC_API XXH128_hash_t XXH3_128bits_withSecret(const void* data, size_t len, const void* secret, size_t secretSize);\n\n/*******   Streaming   *******/\n/*\n * Streaming requires state maintenance.\n * This operation costs memory and CPU.\n * As a consequence, streaming is slower than one-shot hashing.\n * For better performance, prefer one-shot functions whenever applicable.\n *\n * XXH3_128bits uses the same XXH3_state_t as XXH3_64bits().\n * Use already declared XXH3_createState() and XXH3_freeState().\n *\n * All reset and streaming functions have same meaning as their 64-bit counterpart.\n */\n\nXXH_PUBLIC_API XXH_errorcode XXH3_128bits_reset(XXH3_state_t* statePtr);\nXXH_PUBLIC_API XXH_errorcode XXH3_128bits_reset_withSeed(XXH3_state_t* statePtr, XXH64_hash_t seed);\nXXH_PUBLIC_API XXH_errorcode XXH3_128bits_reset_withSecret(XXH3_state_t* statePtr, const void* secret, size_t secretSize);\n\nXXH_PUBLIC_API XXH_errorcode XXH3_128bits_update (XXH3_state_t* statePtr, const void* input, size_t length);\nXXH_PUBLIC_API XXH128_hash_t XXH3_128bits_digest (const XXH3_state_t* statePtr);\n\n/* Following helper functions make it possible to compare XXH128_hast_t values.\n * Since XXH128_hash_t is a structure, this capability is not offered by the language.\n * Note: For better performance, these functions can be inlined using XXH_INLINE_ALL */\n\n/*!\n * XXH128_isEqual():\n * Return: 1 if `h1` and `h2` are equal, 0 if they are not.\n */\nXXH_PUBLIC_API int XXH128_isEqual(XXH128_hash_t h1, XXH128_hash_t h2);\n\n/*!\n * XXH128_cmp():\n *\n * This comparator is compatible with stdlib's `qsort()`/`bsearch()`.\n *\n * return: >0 if *h128_1  > *h128_2\n *         =0 if *h128_1 == *h128_2\n *         <0 if *h128_1  < *h128_2\n */\nXXH_PUBLIC_API int XXH128_cmp(const void* h128_1, const void* h128_2);\n\n\n/*******   Canonical representation   *******/\ntypedef struct { unsigned char digest[sizeof(XXH128_hash_t)]; } XXH128_canonical_t;\nXXH_PUBLIC_API void XXH128_canonicalFromHash(XXH128_canonical_t* dst, XXH128_hash_t hash);\nXXH_PUBLIC_API XXH128_hash_t XXH128_hashFromCanonical(const XXH128_canonical_t* src);\n\n\n#endif  /* XXH_NO_LONG_LONG */\n\n/*!\n * @}\n */\n#endif /* XXHASH_H_5627135585666179 */\n\n\n\n#if defined(XXH_STATIC_LINKING_ONLY) && !defined(XXHASH_H_STATIC_13879238742)\n#define XXHASH_H_STATIC_13879238742\n/* ****************************************************************************\n * This section contains declarations which are not guaranteed to remain stable.\n * They may change in future versions, becoming incompatible with a different\n * version of the library.\n * These declarations should only be used with static linking.\n * Never use them in association with dynamic linking!\n ***************************************************************************** */\n\n/*\n * These definitions are only present to allow static allocation\n * of XXH states, on stack or in a struct, for example.\n * Never **ever** access their members directly.\n */\n\n/*!\n * @internal\n * @brief Structure for XXH32 streaming API.\n *\n * @note This is only defined when @ref XXH_STATIC_LINKING_ONLY,\n * @ref XXH_INLINE_ALL, or @ref XXH_IMPLEMENTATION is defined. Otherwise it is\n * an opaque type. This allows fields to safely be changed.\n *\n * Typedef'd to @ref XXH32_state_t.\n * Do not access the members of this struct directly.\n * @see XXH64_state_s, XXH3_state_s\n */\nstruct XXH32_state_s {\n   XXH32_hash_t total_len_32; /*!< Total length hashed, modulo 2^32 */\n   XXH32_hash_t large_len;    /*!< Whether the hash is >= 16 (handles @ref total_len_32 overflow) */\n   XXH32_hash_t v1;           /*!< First accumulator lane */\n   XXH32_hash_t v2;           /*!< Second accumulator lane */\n   XXH32_hash_t v3;           /*!< Third accumulator lane */\n   XXH32_hash_t v4;           /*!< Fourth accumulator lane */\n   XXH32_hash_t mem32[4];     /*!< Internal buffer for partial reads. Treated as unsigned char[16]. */\n   XXH32_hash_t memsize;      /*!< Amount of data in @ref mem32 */\n   XXH32_hash_t reserved;     /*!< Reserved field. Do not read or write to it, it may be removed. */\n};   /* typedef'd to XXH32_state_t */\n\n\n#ifndef XXH_NO_LONG_LONG  /* defined when there is no 64-bit support */\n\n/*!\n * @internal\n * @brief Structure for XXH64 streaming API.\n *\n * @note This is only defined when @ref XXH_STATIC_LINKING_ONLY,\n * @ref XXH_INLINE_ALL, or @ref XXH_IMPLEMENTATION is defined. Otherwise it is\n * an opaque type. This allows fields to safely be changed.\n *\n * Typedef'd to @ref XXH64_state_t.\n * Do not access the members of this struct directly.\n * @see XXH32_state_s, XXH3_state_s\n */\nstruct XXH64_state_s {\n   XXH64_hash_t total_len;    /*!< Total length hashed. This is always 64-bit. */\n   XXH64_hash_t v1;           /*!< First accumulator lane */\n   XXH64_hash_t v2;           /*!< Second accumulator lane */\n   XXH64_hash_t v3;           /*!< Third accumulator lane */\n   XXH64_hash_t v4;           /*!< Fourth accumulator lane */\n   XXH64_hash_t mem64[4];     /*!< Internal buffer for partial reads. Treated as unsigned char[32]. */\n   XXH32_hash_t memsize;      /*!< Amount of data in @ref mem64 */\n   XXH32_hash_t reserved32;   /*!< Reserved field, needed for padding anyways*/\n   XXH64_hash_t reserved64;   /*!< Reserved field. Do not read or write to it, it may be removed. */\n};   /* typedef'd to XXH64_state_t */\n\n#if defined (__STDC_VERSION__) && (__STDC_VERSION__ >= 201112L)   /* C11+ */\n#  include <stdalign.h>\n#  define XXH_ALIGN(n)      alignas(n)\n#elif defined(__GNUC__)\n#  define XXH_ALIGN(n)      __attribute__ ((aligned(n)))\n#elif defined(_MSC_VER)\n#  define XXH_ALIGN(n)      __declspec(align(n))\n#else\n#  define XXH_ALIGN(n)   /* disabled */\n#endif\n\n/* Old GCC versions only accept the attribute after the type in structures. */\n#if !(defined(__STDC_VERSION__) && (__STDC_VERSION__ >= 201112L))   /* C11+ */ \\\n    && defined(__GNUC__)\n#   define XXH_ALIGN_MEMBER(align, type) type XXH_ALIGN(align)\n#else\n#   define XXH_ALIGN_MEMBER(align, type) XXH_ALIGN(align) type\n#endif\n\n/*!\n * @brief The size of the internal XXH3 buffer.\n *\n * This is the optimal update size for incremental hashing.\n *\n * @see XXH3_64b_update(), XXH3_128b_update().\n */\n#define XXH3_INTERNALBUFFER_SIZE 256\n\n/*!\n * @brief Default size of the secret buffer (and @ref XXH3_kSecret).\n *\n * This is the size used in @ref XXH3_kSecret and the seeded functions.\n *\n * Not to be confused with @ref XXH3_SECRET_SIZE_MIN.\n */\n#define XXH3_SECRET_DEFAULT_SIZE 192\n\n/*!\n * @internal\n * @brief Structure for XXH3 streaming API.\n *\n * @note This is only defined when @ref XXH_STATIC_LINKING_ONLY,\n * @ref XXH_INLINE_ALL, or @ref XXH_IMPLEMENTATION is defined. Otherwise it is\n * an opaque type. This allows fields to safely be changed.\n *\n * @note **This structure has a strict alignment requirement of 64 bytes.** Do\n * not allocate this with `malloc()` or `new`, it will not be sufficiently\n * aligned. Use @ref XXH3_createState() and @ref XXH3_freeState(), or stack\n * allocation.\n *\n * Typedef'd to @ref XXH3_state_t.\n * Do not access the members of this struct directly.\n *\n * @see XXH3_INITSTATE() for stack initialization.\n * @see XXH3_createState(), XXH3_freeState().\n * @see XXH32_state_s, XXH64_state_s\n */\nstruct XXH3_state_s {\n   XXH_ALIGN_MEMBER(64, XXH64_hash_t acc[8]);\n       /*!< The 8 accumulators. Similar to `vN` in @ref XXH32_state_s::v1 and @ref XXH64_state_s */\n   XXH_ALIGN_MEMBER(64, unsigned char customSecret[XXH3_SECRET_DEFAULT_SIZE]);\n       /*!< Used to store a custom secret generated from a seed. */\n   XXH_ALIGN_MEMBER(64, unsigned char buffer[XXH3_INTERNALBUFFER_SIZE]);\n       /*!< The internal buffer. @see XXH32_state_s::mem32 */\n   XXH32_hash_t bufferedSize;\n       /*!< The amount of memory in @ref buffer, @see XXH32_state_s::memsize */\n   XXH32_hash_t reserved32;\n       /*!< Reserved field. Needed for padding on 64-bit. */\n   size_t nbStripesSoFar;\n       /*!< Number or stripes processed. */\n   XXH64_hash_t totalLen;\n       /*!< Total length hashed. 64-bit even on 32-bit targets. */\n   size_t nbStripesPerBlock;\n       /*!< Number of stripes per block. */\n   size_t secretLimit;\n       /*!< Size of @ref customSecret or @ref extSecret */\n   XXH64_hash_t seed;\n       /*!< Seed for _withSeed variants. Must be zero otherwise, @see XXH3_INITSTATE() */\n   XXH64_hash_t reserved64;\n       /*!< Reserved field. */\n   const unsigned char* extSecret;\n       /*!< Reference to an external secret for the _withSecret variants, NULL\n        *   for other variants. */\n   /* note: there may be some padding at the end due to alignment on 64 bytes */\n}; /* typedef'd to XXH3_state_t */\n\n#undef XXH_ALIGN_MEMBER\n\n/*!\n * @brief Initializes a stack-allocated `XXH3_state_s`.\n *\n * When the @ref XXH3_state_t structure is merely emplaced on stack,\n * it should be initialized with XXH3_INITSTATE() or a memset()\n * in case its first reset uses XXH3_NNbits_reset_withSeed().\n * This init can be omitted if the first reset uses default or _withSecret mode.\n * This operation isn't necessary when the state is created with XXH3_createState().\n * Note that this doesn't prepare the state for a streaming operation,\n * it's still necessary to use XXH3_NNbits_reset*() afterwards.\n */\n#define XXH3_INITSTATE(XXH3_state_ptr)   { (XXH3_state_ptr)->seed = 0; }\n\n\n/* ===   Experimental API   === */\n/* Symbols defined below must be considered tied to a specific library version. */\n\n/*\n * XXH3_generateSecret():\n *\n * Derive a high-entropy secret from any user-defined content, named customSeed.\n * The generated secret can be used in combination with `*_withSecret()` functions.\n * The `_withSecret()` variants are useful to provide a higher level of protection than 64-bit seed,\n * as it becomes much more difficult for an external actor to guess how to impact the calculation logic.\n *\n * The function accepts as input a custom seed of any length and any content,\n * and derives from it a high-entropy secret of length XXH3_SECRET_DEFAULT_SIZE\n * into an already allocated buffer secretBuffer.\n * The generated secret is _always_ XXH_SECRET_DEFAULT_SIZE bytes long.\n *\n * The generated secret can then be used with any `*_withSecret()` variant.\n * Functions `XXH3_128bits_withSecret()`, `XXH3_64bits_withSecret()`,\n * `XXH3_128bits_reset_withSecret()` and `XXH3_64bits_reset_withSecret()`\n * are part of this list. They all accept a `secret` parameter\n * which must be very long for implementation reasons (>= XXH3_SECRET_SIZE_MIN)\n * _and_ feature very high entropy (consist of random-looking bytes).\n * These conditions can be a high bar to meet, so\n * this function can be used to generate a secret of proper quality.\n *\n * customSeed can be anything. It can have any size, even small ones,\n * and its content can be anything, even stupidly \"low entropy\" source such as a bunch of zeroes.\n * The resulting `secret` will nonetheless provide all expected qualities.\n *\n * Supplying NULL as the customSeed copies the default secret into `secretBuffer`.\n * When customSeedSize > 0, supplying NULL as customSeed is undefined behavior.\n */\nXXH_PUBLIC_API void XXH3_generateSecret(void* secretBuffer, const void* customSeed, size_t customSeedSize);\n\n\n/* simple short-cut to pre-selected XXH3_128bits variant */\nXXH_PUBLIC_API XXH128_hash_t XXH128(const void* data, size_t len, XXH64_hash_t seed);\n\n\n#endif  /* XXH_NO_LONG_LONG */\n#if defined(XXH_INLINE_ALL) || defined(XXH_PRIVATE_API)\n#  define XXH_IMPLEMENTATION\n#endif\n\n#endif  /* defined(XXH_STATIC_LINKING_ONLY) && !defined(XXHASH_H_STATIC_13879238742) */\n\n\n/* ======================================================================== */\n/* ======================================================================== */\n/* ======================================================================== */\n\n\n/*-**********************************************************************\n * xxHash implementation\n *-**********************************************************************\n * xxHash's implementation used to be hosted inside xxhash.c.\n *\n * However, inlining requires implementation to be visible to the compiler,\n * hence be included alongside the header.\n * Previously, implementation was hosted inside xxhash.c,\n * which was then #included when inlining was activated.\n * This construction created issues with a few build and install systems,\n * as it required xxhash.c to be stored in /include directory.\n *\n * xxHash implementation is now directly integrated within xxhash.h.\n * As a consequence, xxhash.c is no longer needed in /include.\n *\n * xxhash.c is still available and is still useful.\n * In a \"normal\" setup, when xxhash is not inlined,\n * xxhash.h only exposes the prototypes and public symbols,\n * while xxhash.c can be built into an object file xxhash.o\n * which can then be linked into the final binary.\n ************************************************************************/\n\n#if ( defined(XXH_INLINE_ALL) || defined(XXH_PRIVATE_API) \\\n   || defined(XXH_IMPLEMENTATION) ) && !defined(XXH_IMPLEM_13a8737387)\n#  define XXH_IMPLEM_13a8737387\n\n/* *************************************\n*  Tuning parameters\n***************************************/\n\n/*!\n * @defgroup tuning Tuning parameters\n * @{\n *\n * Various macros to control xxHash's behavior.\n */\n#ifdef XXH_DOXYGEN\n/*!\n * @brief Define this to disable 64-bit code.\n *\n * Useful if only using the @ref xxh32_family and you have a strict C90 compiler.\n */\n#  define XXH_NO_LONG_LONG\n#  undef XXH_NO_LONG_LONG /* don't actually */\n/*!\n * @brief Controls how unaligned memory is accessed.\n *\n * By default, access to unaligned memory is controlled by `memcpy()`, which is\n * safe and portable.\n *\n * Unfortunately, on some target/compiler combinations, the generated assembly\n * is sub-optimal.\n *\n * The below switch allow selection of a different access method\n * in the search for improved performance.\n *\n * @par Possible options:\n *\n *  - `XXH_FORCE_MEMORY_ACCESS=0` (default): `memcpy`\n *   @par\n *     Use `memcpy()`. Safe and portable. Note that most modern compilers will\n *     eliminate the function call and treat it as an unaligned access.\n *\n *  - `XXH_FORCE_MEMORY_ACCESS=1`: `__attribute__((packed))`\n *   @par\n *     Depends on compiler extensions and is therefore not portable.\n *     This method is safe _if_ your compiler supports it,\n *     and *generally* as fast or faster than `memcpy`.\n *\n *  - `XXH_FORCE_MEMORY_ACCESS=2`: Direct cast\n *  @par\n *     Casts directly and dereferences. This method doesn't depend on the\n *     compiler, but it violates the C standard as it directly dereferences an\n *     unaligned pointer. It can generate buggy code on targets which do not\n *     support unaligned memory accesses, but in some circumstances, it's the\n *     only known way to get the most performance.\n *\n *  - `XXH_FORCE_MEMORY_ACCESS=3`: Byteshift\n *  @par\n *     Also portable. This can generate the best code on old compilers which don't\n *     inline small `memcpy()` calls, and it might also be faster on big-endian\n *     systems which lack a native byteswap instruction. However, some compilers\n *     will emit literal byteshifts even if the target supports unaligned access.\n *  .\n *\n * @warning\n *   Methods 1 and 2 rely on implementation-defined behavior. Use these with\n *   care, as what works on one compiler/platform/optimization level may cause\n *   another to read garbage data or even crash.\n *\n * See https://stackoverflow.com/a/32095106/646947 for details.\n *\n * Prefer these methods in priority order (0 > 3 > 1 > 2)\n */\n#  define XXH_FORCE_MEMORY_ACCESS 0\n/*!\n * @def XXH_ACCEPT_NULL_INPUT_POINTER\n * @brief Whether to add explicit `NULL` checks.\n *\n * If the input pointer is `NULL` and the length is non-zero, xxHash's default\n * behavior is to dereference it, triggering a segfault.\n *\n * When this macro is enabled, xxHash actively checks the input for a null pointer.\n * If it is, the result for null input pointers is the same as a zero-length input.\n */\n#  define XXH_ACCEPT_NULL_INPUT_POINTER 0\n/*!\n * @def XXH_FORCE_ALIGN_CHECK\n * @brief If defined to non-zero, adds a special path for aligned inputs (XXH32()\n * and XXH64() only).\n *\n * This is an important performance trick for architectures without decent\n * unaligned memory access performance.\n *\n * It checks for input alignment, and when conditions are met, uses a \"fast\n * path\" employing direct 32-bit/64-bit reads, resulting in _dramatically\n * faster_ read speed.\n *\n * The check costs one initial branch per hash, which is generally negligible,\n * but not zero.\n *\n * Moreover, it's not useful to generate an additional code path if memory\n * access uses the same instruction for both aligned and unaligned\n * addresses (e.g. x86 and aarch64).\n *\n * In these cases, the alignment check can be removed by setting this macro to 0.\n * Then the code will always use unaligned memory access.\n * Align check is automatically disabled on x86, x64 & arm64,\n * which are platforms known to offer good unaligned memory accesses performance.\n *\n * This option does not affect XXH3 (only XXH32 and XXH64).\n */\n#  define XXH_FORCE_ALIGN_CHECK 0\n\n/*!\n * @def XXH_NO_INLINE_HINTS\n * @brief When non-zero, sets all functions to `static`.\n *\n * By default, xxHash tries to force the compiler to inline almost all internal\n * functions.\n *\n * This can usually improve performance due to reduced jumping and improved\n * constant folding, but significantly increases the size of the binary which\n * might not be favorable.\n *\n * Additionally, sometimes the forced inlining can be detrimental to performance,\n * depending on the architecture.\n *\n * XXH_NO_INLINE_HINTS marks all internal functions as static, giving the\n * compiler full control on whether to inline or not.\n *\n * When not optimizing (-O0), optimizing for size (-Os, -Oz), or using\n * -fno-inline with GCC or Clang, this will automatically be defined.\n */\n#  define XXH_NO_INLINE_HINTS 0\n\n/*!\n * @def XXH_REROLL\n * @brief Whether to reroll `XXH32_finalize` and `XXH64_finalize`.\n *\n * For performance, `XXH32_finalize` and `XXH64_finalize` use an unrolled loop\n * in the form of a switch statement.\n *\n * This is not always desirable, as it generates larger code, and depending on\n * the architecture, may even be slower\n *\n * This is automatically defined with `-Os`/`-Oz` on GCC and Clang.\n */\n#  define XXH_REROLL 0\n\n/*!\n * @internal\n * @brief Redefines old internal names.\n *\n * For compatibility with code that uses xxHash's internals before the names\n * were changed to improve namespacing. There is no other reason to use this.\n */\n#  define XXH_OLD_NAMES\n#  undef XXH_OLD_NAMES /* don't actually use, it is ugly. */\n#endif /* XXH_DOXYGEN */\n/*!\n * @}\n */\n\n#ifndef XXH_FORCE_MEMORY_ACCESS   /* can be defined externally, on command line for example */\n   /* prefer __packed__ structures (method 1) for gcc on armv7 and armv8 */\n#  if !defined(__clang__) && ( \\\n    (defined(__INTEL_COMPILER) && !defined(_WIN32)) || \\\n    (defined(__GNUC__) && (defined(__ARM_ARCH) && __ARM_ARCH >= 7)) )\n#    define XXH_FORCE_MEMORY_ACCESS 1\n#  endif\n#endif\n\n#ifndef XXH_ACCEPT_NULL_INPUT_POINTER   /* can be defined externally */\n#  define XXH_ACCEPT_NULL_INPUT_POINTER 0\n#endif\n\n#ifndef XXH_FORCE_ALIGN_CHECK  /* can be defined externally */\n#  if defined(__i386)  || defined(__x86_64__) || defined(__aarch64__) \\\n   || defined(_M_IX86) || defined(_M_X64)     || defined(_M_ARM64) /* visual */\n#    define XXH_FORCE_ALIGN_CHECK 0\n#  else\n#    define XXH_FORCE_ALIGN_CHECK 1\n#  endif\n#endif\n\n#ifndef XXH_NO_INLINE_HINTS\n#  if defined(__OPTIMIZE_SIZE__) /* -Os, -Oz */ \\\n   || defined(__NO_INLINE__)     /* -O0, -fno-inline */\n#    define XXH_NO_INLINE_HINTS 1\n#  else\n#    define XXH_NO_INLINE_HINTS 0\n#  endif\n#endif\n\n#ifndef XXH_REROLL\n#  if defined(__OPTIMIZE_SIZE__)\n#    define XXH_REROLL 1\n#  else\n#    define XXH_REROLL 0\n#  endif\n#endif\n\n/*!\n * @defgroup impl Implementation\n * @{\n */\n\n\n/* *************************************\n*  Includes & Memory related functions\n***************************************/\n/*\n * Modify the local functions below should you wish to use\n * different memory routines for malloc() and free()\n */\n#include <stdlib.h>\n\n/*!\n * @internal\n * @brief Modify this function to use a different routine than malloc().\n */\nstatic void* XXH_malloc(size_t s) { return malloc(s); }\n\n/*!\n * @internal\n * @brief Modify this function to use a different routine than free().\n */\nstatic void XXH_free(void* p) { free(p); }\n\n#include <string.h>\n\n/*!\n * @internal\n * @brief Modify this function to use a different routine than memcpy().\n */\nstatic void* XXH_memcpy(void* dest, const void* src, size_t size)\n{\n    return memcpy(dest,src,size);\n}\n\n#include <limits.h>   /* ULLONG_MAX */\n\n\n/* *************************************\n*  Compiler Specific Options\n***************************************/\n#ifdef _MSC_VER /* Visual Studio warning fix */\n#  pragma warning(disable : 4127) /* disable: C4127: conditional expression is constant */\n#endif\n\n#if XXH_NO_INLINE_HINTS  /* disable inlining hints */\n#  if defined(__GNUC__)\n#    define XXH_FORCE_INLINE static __attribute__((unused))\n#  else\n#    define XXH_FORCE_INLINE static\n#  endif\n#  define XXH_NO_INLINE static\n/* enable inlining hints */\n#elif defined(_MSC_VER)  /* Visual Studio */\n#  define XXH_FORCE_INLINE static __forceinline\n#  define XXH_NO_INLINE static __declspec(noinline)\n#elif defined(__GNUC__)\n#  define XXH_FORCE_INLINE static __inline__ __attribute__((always_inline, unused))\n#  define XXH_NO_INLINE static __attribute__((noinline))\n#elif defined (__cplusplus) \\\n  || (defined (__STDC_VERSION__) && (__STDC_VERSION__ >= 199901L))   /* C99 */\n#  define XXH_FORCE_INLINE static inline\n#  define XXH_NO_INLINE static\n#else\n#  define XXH_FORCE_INLINE static\n#  define XXH_NO_INLINE static\n#endif\n\n\n\n/* *************************************\n*  Debug\n***************************************/\n/*!\n * @ingroup tuning\n * @def XXH_DEBUGLEVEL\n * @brief Sets the debugging level.\n *\n * XXH_DEBUGLEVEL is expected to be defined externally, typically via the\n * compiler's command line options. The value must be a number.\n */\n#ifndef XXH_DEBUGLEVEL\n#  ifdef DEBUGLEVEL /* backwards compat */\n#    define XXH_DEBUGLEVEL DEBUGLEVEL\n#  else\n#    define XXH_DEBUGLEVEL 0\n#  endif\n#endif\n\n#if (XXH_DEBUGLEVEL>=1)\n#  include <assert.h>   /* note: can still be disabled with NDEBUG */\n#  define XXH_ASSERT(c)   assert(c)\n#else\n#  define XXH_ASSERT(c)   ((void)0)\n#endif\n\n/* note: use after variable declarations */\n#define XXH_STATIC_ASSERT(c)  do { enum { XXH_sa = 1/(int)(!!(c)) }; } while (0)\n\n/*!\n * @internal\n * @def XXH_COMPILER_GUARD(var)\n * @brief Used to prevent unwanted optimizations for @p var.\n *\n * It uses an empty GCC inline assembly statement with a register constraint\n * which forces @p var into a general purpose register (eg eax, ebx, ecx\n * on x86) and marks it as modified.\n *\n * This is used in a few places to avoid unwanted autovectorization (e.g.\n * XXH32_round()). All vectorization we want is explicit via intrinsics,\n * and _usually_ isn't wanted elsewhere.\n *\n * We also use it to prevent unwanted constant folding for AArch64 in\n * XXH3_initCustomSecret_scalar().\n */\n#ifdef __GNUC__\n#  define XXH_COMPILER_GUARD(var) __asm__ __volatile__(\"\" : \"+r\" (var))\n#else\n#  define XXH_COMPILER_GUARD(var) ((void)0)\n#endif\n\n/* *************************************\n*  Basic Types\n***************************************/\n#if !defined (__VMS) \\\n && (defined (__cplusplus) \\\n || (defined (__STDC_VERSION__) && (__STDC_VERSION__ >= 199901L) /* C99 */) )\n# include <stdint.h>\n  typedef uint8_t xxh_u8;\n#else\n  typedef unsigned char xxh_u8;\n#endif\ntypedef XXH32_hash_t xxh_u32;\n\n#ifdef XXH_OLD_NAMES\n#  define BYTE xxh_u8\n#  define U8   xxh_u8\n#  define U32  xxh_u32\n#endif\n\n/* ***   Memory access   *** */\n\n/*!\n * @internal\n * @fn xxh_u32 XXH_read32(const void* ptr)\n * @brief Reads an unaligned 32-bit integer from @p ptr in native endianness.\n *\n * Affected by @ref XXH_FORCE_MEMORY_ACCESS.\n *\n * @param ptr The pointer to read from.\n * @return The 32-bit native endian integer from the bytes at @p ptr.\n */\n\n/*!\n * @internal\n * @fn xxh_u32 XXH_readLE32(const void* ptr)\n * @brief Reads an unaligned 32-bit little endian integer from @p ptr.\n *\n * Affected by @ref XXH_FORCE_MEMORY_ACCESS.\n *\n * @param ptr The pointer to read from.\n * @return The 32-bit little endian integer from the bytes at @p ptr.\n */\n\n/*!\n * @internal\n * @fn xxh_u32 XXH_readBE32(const void* ptr)\n * @brief Reads an unaligned 32-bit big endian integer from @p ptr.\n *\n * Affected by @ref XXH_FORCE_MEMORY_ACCESS.\n *\n * @param ptr The pointer to read from.\n * @return The 32-bit big endian integer from the bytes at @p ptr.\n */\n\n/*!\n * @internal\n * @fn xxh_u32 XXH_readLE32_align(const void* ptr, XXH_alignment align)\n * @brief Like @ref XXH_readLE32(), but has an option for aligned reads.\n *\n * Affected by @ref XXH_FORCE_MEMORY_ACCESS.\n * Note that when @ref XXH_FORCE_ALIGN_CHECK == 0, the @p align parameter is\n * always @ref XXH_alignment::XXH_unaligned.\n *\n * @param ptr The pointer to read from.\n * @param align Whether @p ptr is aligned.\n * @pre\n *   If @p align == @ref XXH_alignment::XXH_aligned, @p ptr must be 4 byte\n *   aligned.\n * @return The 32-bit little endian integer from the bytes at @p ptr.\n */\n\n#if (defined(XXH_FORCE_MEMORY_ACCESS) && (XXH_FORCE_MEMORY_ACCESS==3))\n/*\n * Manual byteshift. Best for old compilers which don't inline memcpy.\n * We actually directly use XXH_readLE32 and XXH_readBE32.\n */\n#elif (defined(XXH_FORCE_MEMORY_ACCESS) && (XXH_FORCE_MEMORY_ACCESS==2))\n\n/*\n * Force direct memory access. Only works on CPU which support unaligned memory\n * access in hardware.\n */\nstatic xxh_u32 XXH_read32(const void* memPtr) { return *(const xxh_u32*) memPtr; }\n\n#elif (defined(XXH_FORCE_MEMORY_ACCESS) && (XXH_FORCE_MEMORY_ACCESS==1))\n\n/*\n * __pack instructions are safer but compiler specific, hence potentially\n * problematic for some compilers.\n *\n * Currently only defined for GCC and ICC.\n */\n#ifdef XXH_OLD_NAMES\ntypedef union { xxh_u32 u32; } __attribute__((packed)) unalign;\n#endif\nstatic xxh_u32 XXH_read32(const void* ptr)\n{\n    typedef union { xxh_u32 u32; } __attribute__((packed)) xxh_unalign;\n    return ((const xxh_unalign*)ptr)->u32;\n}\n\n#else\n\n/*\n * Portable and safe solution. Generally efficient.\n * see: https://stackoverflow.com/a/32095106/646947\n */\nstatic xxh_u32 XXH_read32(const void* memPtr)\n{\n    xxh_u32 val;\n    memcpy(&val, memPtr, sizeof(val));\n    return val;\n}\n\n#endif   /* XXH_FORCE_DIRECT_MEMORY_ACCESS */\n\n\n/* ***   Endianness   *** */\ntypedef enum { XXH_bigEndian=0, XXH_littleEndian=1 } XXH_endianess;\n\n/*!\n * @ingroup tuning\n * @def XXH_CPU_LITTLE_ENDIAN\n * @brief Whether the target is little endian.\n *\n * Defined to 1 if the target is little endian, or 0 if it is big endian.\n * It can be defined externally, for example on the compiler command line.\n *\n * If it is not defined, a runtime check (which is usually constant folded)\n * is used instead.\n *\n * @note\n *   This is not necessarily defined to an integer constant.\n *\n * @see XXH_isLittleEndian() for the runtime check.\n */\n#ifndef XXH_CPU_LITTLE_ENDIAN\n/*\n * Try to detect endianness automatically, to avoid the nonstandard behavior\n * in `XXH_isLittleEndian()`\n */\n#  if defined(_WIN32) /* Windows is always little endian */ \\\n     || defined(__LITTLE_ENDIAN__) \\\n     || (defined(__BYTE_ORDER__) && __BYTE_ORDER__ == __ORDER_LITTLE_ENDIAN__)\n#    define XXH_CPU_LITTLE_ENDIAN 1\n#  elif defined(__BIG_ENDIAN__) \\\n     || (defined(__BYTE_ORDER__) && __BYTE_ORDER__ == __ORDER_BIG_ENDIAN__)\n#    define XXH_CPU_LITTLE_ENDIAN 0\n#  else\n/*!\n * @internal\n * @brief Runtime check for @ref XXH_CPU_LITTLE_ENDIAN.\n *\n * Most compilers will constant fold this.\n */\nstatic int XXH_isLittleEndian(void)\n{\n    /*\n     * Portable and well-defined behavior.\n     * Don't use static: it is detrimental to performance.\n     */\n    const union { xxh_u32 u; xxh_u8 c[4]; } one = { 1 };\n    return one.c[0];\n}\n#   define XXH_CPU_LITTLE_ENDIAN   XXH_isLittleEndian()\n#  endif\n#endif\n\n\n\n\n/* ****************************************\n*  Compiler-specific Functions and Macros\n******************************************/\n#define XXH_GCC_VERSION (__GNUC__ * 100 + __GNUC_MINOR__)\n\n#ifdef __has_builtin\n#  define XXH_HAS_BUILTIN(x) __has_builtin(x)\n#else\n#  define XXH_HAS_BUILTIN(x) 0\n#endif\n\n/*!\n * @internal\n * @def XXH_rotl32(x,r)\n * @brief 32-bit rotate left.\n *\n * @param x The 32-bit integer to be rotated.\n * @param r The number of bits to rotate.\n * @pre\n *   @p r > 0 && @p r < 32\n * @note\n *   @p x and @p r may be evaluated multiple times.\n * @return The rotated result.\n */\n#if !defined(NO_CLANG_BUILTIN) && XXH_HAS_BUILTIN(__builtin_rotateleft32) \\\n                               && XXH_HAS_BUILTIN(__builtin_rotateleft64)\n#  define XXH_rotl32 __builtin_rotateleft32\n#  define XXH_rotl64 __builtin_rotateleft64\n/* Note: although _rotl exists for minGW (GCC under windows), performance seems poor */\n#elif defined(_MSC_VER)\n#  define XXH_rotl32(x,r) _rotl(x,r)\n#  define XXH_rotl64(x,r) _rotl64(x,r)\n#else\n#  define XXH_rotl32(x,r) (((x) << (r)) | ((x) >> (32 - (r))))\n#  define XXH_rotl64(x,r) (((x) << (r)) | ((x) >> (64 - (r))))\n#endif\n\n/*!\n * @internal\n * @fn xxh_u32 XXH_swap32(xxh_u32 x)\n * @brief A 32-bit byteswap.\n *\n * @param x The 32-bit integer to byteswap.\n * @return @p x, byteswapped.\n */\n#if defined(_MSC_VER)     /* Visual Studio */\n#  define XXH_swap32 _byteswap_ulong\n#elif XXH_GCC_VERSION >= 403\n#  define XXH_swap32 __builtin_bswap32\n#else\nstatic xxh_u32 XXH_swap32 (xxh_u32 x)\n{\n    return  ((x << 24) & 0xff000000 ) |\n            ((x <<  8) & 0x00ff0000 ) |\n            ((x >>  8) & 0x0000ff00 ) |\n            ((x >> 24) & 0x000000ff );\n}\n#endif\n\n\n/* ***************************\n*  Memory reads\n*****************************/\n\n/*!\n * @internal\n * @brief Enum to indicate whether a pointer is aligned.\n */\ntypedef enum {\n    XXH_aligned,  /*!< Aligned */\n    XXH_unaligned /*!< Possibly unaligned */\n} XXH_alignment;\n\n/*\n * XXH_FORCE_MEMORY_ACCESS==3 is an endian-independent byteshift load.\n *\n * This is ideal for older compilers which don't inline memcpy.\n */\n#if (defined(XXH_FORCE_MEMORY_ACCESS) && (XXH_FORCE_MEMORY_ACCESS==3))\n\nXXH_FORCE_INLINE xxh_u32 XXH_readLE32(const void* memPtr)\n{\n    const xxh_u8* bytePtr = (const xxh_u8 *)memPtr;\n    return bytePtr[0]\n         | ((xxh_u32)bytePtr[1] << 8)\n         | ((xxh_u32)bytePtr[2] << 16)\n         | ((xxh_u32)bytePtr[3] << 24);\n}\n\nXXH_FORCE_INLINE xxh_u32 XXH_readBE32(const void* memPtr)\n{\n    const xxh_u8* bytePtr = (const xxh_u8 *)memPtr;\n    return bytePtr[3]\n         | ((xxh_u32)bytePtr[2] << 8)\n         | ((xxh_u32)bytePtr[1] << 16)\n         | ((xxh_u32)bytePtr[0] << 24);\n}\n\n#else\nXXH_FORCE_INLINE xxh_u32 XXH_readLE32(const void* ptr)\n{\n    return XXH_CPU_LITTLE_ENDIAN ? XXH_read32(ptr) : XXH_swap32(XXH_read32(ptr));\n}\n\nstatic xxh_u32 XXH_readBE32(const void* ptr)\n{\n    return XXH_CPU_LITTLE_ENDIAN ? XXH_swap32(XXH_read32(ptr)) : XXH_read32(ptr);\n}\n#endif\n\nXXH_FORCE_INLINE xxh_u32\nXXH_readLE32_align(const void* ptr, XXH_alignment align)\n{\n    if (align==XXH_unaligned) {\n        return XXH_readLE32(ptr);\n    } else {\n        return XXH_CPU_LITTLE_ENDIAN ? *(const xxh_u32*)ptr : XXH_swap32(*(const xxh_u32*)ptr);\n    }\n}\n\n\n/* *************************************\n*  Misc\n***************************************/\n/*! @ingroup public */\nXXH_PUBLIC_API unsigned XXH_versionNumber (void) { return XXH_VERSION_NUMBER; }\n\n\n/* *******************************************************************\n*  32-bit hash functions\n*********************************************************************/\n/*!\n * @}\n * @defgroup xxh32_impl XXH32 implementation\n * @ingroup impl\n * @{\n */\n /* #define instead of static const, to be used as initializers */\n#define XXH_PRIME32_1  0x9E3779B1U  /*!< 0b10011110001101110111100110110001 */\n#define XXH_PRIME32_2  0x85EBCA77U  /*!< 0b10000101111010111100101001110111 */\n#define XXH_PRIME32_3  0xC2B2AE3DU  /*!< 0b11000010101100101010111000111101 */\n#define XXH_PRIME32_4  0x27D4EB2FU  /*!< 0b00100111110101001110101100101111 */\n#define XXH_PRIME32_5  0x165667B1U  /*!< 0b00010110010101100110011110110001 */\n\n#ifdef XXH_OLD_NAMES\n#  define PRIME32_1 XXH_PRIME32_1\n#  define PRIME32_2 XXH_PRIME32_2\n#  define PRIME32_3 XXH_PRIME32_3\n#  define PRIME32_4 XXH_PRIME32_4\n#  define PRIME32_5 XXH_PRIME32_5\n#endif\n\n/*!\n * @internal\n * @brief Normal stripe processing routine.\n *\n * This shuffles the bits so that any bit from @p input impacts several bits in\n * @p acc.\n *\n * @param acc The accumulator lane.\n * @param input The stripe of input to mix.\n * @return The mixed accumulator lane.\n */\nstatic xxh_u32 XXH32_round(xxh_u32 acc, xxh_u32 input)\n{\n    acc += input * XXH_PRIME32_2;\n    acc  = XXH_rotl32(acc, 13);\n    acc *= XXH_PRIME32_1;\n#if (defined(__SSE4_1__) || defined(__aarch64__)) && !defined(XXH_ENABLE_AUTOVECTORIZE)\n    /*\n     * UGLY HACK:\n     * A compiler fence is the only thing that prevents GCC and Clang from\n     * autovectorizing the XXH32 loop (pragmas and attributes don't work for some\n     * reason) without globally disabling SSE4.1.\n     *\n     * The reason we want to avoid vectorization is because despite working on\n     * 4 integers at a time, there are multiple factors slowing XXH32 down on\n     * SSE4:\n     * - There's a ridiculous amount of lag from pmulld (10 cycles of latency on\n     *   newer chips!) making it slightly slower to multiply four integers at\n     *   once compared to four integers independently. Even when pmulld was\n     *   fastest, Sandy/Ivy Bridge, it is still not worth it to go into SSE\n     *   just to multiply unless doing a long operation.\n     *\n     * - Four instructions are required to rotate,\n     *      movqda tmp,  v // not required with VEX encoding\n     *      pslld  tmp, 13 // tmp <<= 13\n     *      psrld  v,   19 // x >>= 19\n     *      por    v,  tmp // x |= tmp\n     *   compared to one for scalar:\n     *      roll   v, 13    // reliably fast across the board\n     *      shldl  v, v, 13 // Sandy Bridge and later prefer this for some reason\n     *\n     * - Instruction level parallelism is actually more beneficial here because\n     *   the SIMD actually serializes this operation: While v1 is rotating, v2\n     *   can load data, while v3 can multiply. SSE forces them to operate\n     *   together.\n     *\n     * This is also enabled on AArch64, as Clang autovectorizes it incorrectly\n     * and it is pointless writing a NEON implementation that is basically the\n     * same speed as scalar for XXH32.\n     */\n    XXH_COMPILER_GUARD(acc);\n#endif\n    return acc;\n}\n\n/*!\n * @internal\n * @brief Mixes all bits to finalize the hash.\n *\n * The final mix ensures that all input bits have a chance to impact any bit in\n * the output digest, resulting in an unbiased distribution.\n *\n * @param h32 The hash to avalanche.\n * @return The avalanched hash.\n */\nstatic xxh_u32 XXH32_avalanche(xxh_u32 h32)\n{\n    h32 ^= h32 >> 15;\n    h32 *= XXH_PRIME32_2;\n    h32 ^= h32 >> 13;\n    h32 *= XXH_PRIME32_3;\n    h32 ^= h32 >> 16;\n    return(h32);\n}\n\n#define XXH_get32bits(p) XXH_readLE32_align(p, align)\n\n/*!\n * @internal\n * @brief Processes the last 0-15 bytes of @p ptr.\n *\n * There may be up to 15 bytes remaining to consume from the input.\n * This final stage will digest them to ensure that all input bytes are present\n * in the final mix.\n *\n * @param h32 The hash to finalize.\n * @param ptr The pointer to the remaining input.\n * @param len The remaining length, modulo 16.\n * @param align Whether @p ptr is aligned.\n * @return The finalized hash.\n */\nstatic xxh_u32\nXXH32_finalize(xxh_u32 h32, const xxh_u8* ptr, size_t len, XXH_alignment align)\n{\n#define XXH_PROCESS1 do {                           \\\n    h32 += (*ptr++) * XXH_PRIME32_5;                \\\n    h32 = XXH_rotl32(h32, 11) * XXH_PRIME32_1;      \\\n} while (0)\n\n#define XXH_PROCESS4 do {                           \\\n    h32 += XXH_get32bits(ptr) * XXH_PRIME32_3;      \\\n    ptr += 4;                                   \\\n    h32  = XXH_rotl32(h32, 17) * XXH_PRIME32_4;     \\\n} while (0)\n\n    /* Compact rerolled version */\n    if (XXH_REROLL) {\n        len &= 15;\n        while (len >= 4) {\n            XXH_PROCESS4;\n            len -= 4;\n        }\n        while (len > 0) {\n            XXH_PROCESS1;\n            --len;\n        }\n        return XXH32_avalanche(h32);\n    } else {\n         switch(len&15) /* or switch(bEnd - p) */ {\n           case 12:      XXH_PROCESS4;\n                         /* fallthrough */\n           case 8:       XXH_PROCESS4;\n                         /* fallthrough */\n           case 4:       XXH_PROCESS4;\n                         return XXH32_avalanche(h32);\n\n           case 13:      XXH_PROCESS4;\n                         /* fallthrough */\n           case 9:       XXH_PROCESS4;\n                         /* fallthrough */\n           case 5:       XXH_PROCESS4;\n                         XXH_PROCESS1;\n                         return XXH32_avalanche(h32);\n\n           case 14:      XXH_PROCESS4;\n                         /* fallthrough */\n           case 10:      XXH_PROCESS4;\n                         /* fallthrough */\n           case 6:       XXH_PROCESS4;\n                         XXH_PROCESS1;\n                         XXH_PROCESS1;\n                         return XXH32_avalanche(h32);\n\n           case 15:      XXH_PROCESS4;\n                         /* fallthrough */\n           case 11:      XXH_PROCESS4;\n                         /* fallthrough */\n           case 7:       XXH_PROCESS4;\n                         /* fallthrough */\n           case 3:       XXH_PROCESS1;\n                         /* fallthrough */\n           case 2:       XXH_PROCESS1;\n                         /* fallthrough */\n           case 1:       XXH_PROCESS1;\n                         /* fallthrough */\n           case 0:       return XXH32_avalanche(h32);\n        }\n        XXH_ASSERT(0);\n        return h32;   /* reaching this point is deemed impossible */\n    }\n}\n\n#ifdef XXH_OLD_NAMES\n#  define PROCESS1 XXH_PROCESS1\n#  define PROCESS4 XXH_PROCESS4\n#else\n#  undef XXH_PROCESS1\n#  undef XXH_PROCESS4\n#endif\n\n/*!\n * @internal\n * @brief The implementation for @ref XXH32().\n *\n * @param input, len, seed Directly passed from @ref XXH32().\n * @param align Whether @p input is aligned.\n * @return The calculated hash.\n */\nXXH_FORCE_INLINE xxh_u32\nXXH32_endian_align(const xxh_u8* input, size_t len, xxh_u32 seed, XXH_alignment align)\n{\n    const xxh_u8* bEnd = input + len;\n    xxh_u32 h32;\n\n#if defined(XXH_ACCEPT_NULL_INPUT_POINTER) && (XXH_ACCEPT_NULL_INPUT_POINTER>=1)\n    if (input==NULL) {\n        len=0;\n        bEnd=input=(const xxh_u8*)(size_t)16;\n    }\n#endif\n\n    if (len>=16) {\n        const xxh_u8* const limit = bEnd - 15;\n        xxh_u32 v1 = seed + XXH_PRIME32_1 + XXH_PRIME32_2;\n        xxh_u32 v2 = seed + XXH_PRIME32_2;\n        xxh_u32 v3 = seed + 0;\n        xxh_u32 v4 = seed - XXH_PRIME32_1;\n\n        do {\n            v1 = XXH32_round(v1, XXH_get32bits(input)); input += 4;\n            v2 = XXH32_round(v2, XXH_get32bits(input)); input += 4;\n            v3 = XXH32_round(v3, XXH_get32bits(input)); input += 4;\n            v4 = XXH32_round(v4, XXH_get32bits(input)); input += 4;\n        } while (input < limit);\n\n        h32 = XXH_rotl32(v1, 1)  + XXH_rotl32(v2, 7)\n            + XXH_rotl32(v3, 12) + XXH_rotl32(v4, 18);\n    } else {\n        h32  = seed + XXH_PRIME32_5;\n    }\n\n    h32 += (xxh_u32)len;\n\n    return XXH32_finalize(h32, input, len&15, align);\n}\n\n/*! @ingroup xxh32_family */\nXXH_PUBLIC_API XXH32_hash_t XXH32 (const void* input, size_t len, XXH32_hash_t seed)\n{\n#if 0\n    /* Simple version, good for code maintenance, but unfortunately slow for small inputs */\n    XXH32_state_t state;\n    XXH32_reset(&state, seed);\n    XXH32_update(&state, (const xxh_u8*)input, len);\n    return XXH32_digest(&state);\n#else\n    if (XXH_FORCE_ALIGN_CHECK) {\n        if ((((size_t)input) & 3) == 0) {   /* Input is 4-bytes aligned, leverage the speed benefit */\n            return XXH32_endian_align((const xxh_u8*)input, len, seed, XXH_aligned);\n    }   }\n\n    return XXH32_endian_align((const xxh_u8*)input, len, seed, XXH_unaligned);\n#endif\n}\n\n\n\n/*******   Hash streaming   *******/\n/*!\n * @ingroup xxh32_family\n */\nXXH_PUBLIC_API XXH32_state_t* XXH32_createState(void)\n{\n    return (XXH32_state_t*)XXH_malloc(sizeof(XXH32_state_t));\n}\n/*! @ingroup xxh32_family */\nXXH_PUBLIC_API XXH_errorcode XXH32_freeState(XXH32_state_t* statePtr)\n{\n    XXH_free(statePtr);\n    return XXH_OK;\n}\n\n/*! @ingroup xxh32_family */\nXXH_PUBLIC_API void XXH32_copyState(XXH32_state_t* dstState, const XXH32_state_t* srcState)\n{\n    memcpy(dstState, srcState, sizeof(*dstState));\n}\n\n/*! @ingroup xxh32_family */\nXXH_PUBLIC_API XXH_errorcode XXH32_reset(XXH32_state_t* statePtr, XXH32_hash_t seed)\n{\n    XXH32_state_t state;   /* using a local state to memcpy() in order to avoid strict-aliasing warnings */\n    memset(&state, 0, sizeof(state));\n    state.v1 = seed + XXH_PRIME32_1 + XXH_PRIME32_2;\n    state.v2 = seed + XXH_PRIME32_2;\n    state.v3 = seed + 0;\n    state.v4 = seed - XXH_PRIME32_1;\n    /* do not write into reserved, planned to be removed in a future version */\n    memcpy(statePtr, &state, sizeof(state) - sizeof(state.reserved));\n    return XXH_OK;\n}\n\n\n/*! @ingroup xxh32_family */\nXXH_PUBLIC_API XXH_errorcode\nXXH32_update(XXH32_state_t* state, const void* input, size_t len)\n{\n    if (input==NULL)\n#if defined(XXH_ACCEPT_NULL_INPUT_POINTER) && (XXH_ACCEPT_NULL_INPUT_POINTER>=1)\n        return XXH_OK;\n#else\n        return XXH_ERROR;\n#endif\n\n    {   const xxh_u8* p = (const xxh_u8*)input;\n        const xxh_u8* const bEnd = p + len;\n\n        state->total_len_32 += (XXH32_hash_t)len;\n        state->large_len |= (XXH32_hash_t)((len>=16) | (state->total_len_32>=16));\n\n        if (state->memsize + len < 16)  {   /* fill in tmp buffer */\n            XXH_memcpy((xxh_u8*)(state->mem32) + state->memsize, input, len);\n            state->memsize += (XXH32_hash_t)len;\n            return XXH_OK;\n        }\n\n        if (state->memsize) {   /* some data left from previous update */\n            XXH_memcpy((xxh_u8*)(state->mem32) + state->memsize, input, 16-state->memsize);\n            {   const xxh_u32* p32 = state->mem32;\n                state->v1 = XXH32_round(state->v1, XXH_readLE32(p32)); p32++;\n                state->v2 = XXH32_round(state->v2, XXH_readLE32(p32)); p32++;\n                state->v3 = XXH32_round(state->v3, XXH_readLE32(p32)); p32++;\n                state->v4 = XXH32_round(state->v4, XXH_readLE32(p32));\n            }\n            p += 16-state->memsize;\n            state->memsize = 0;\n        }\n\n        if (p <= bEnd-16) {\n            const xxh_u8* const limit = bEnd - 16;\n            xxh_u32 v1 = state->v1;\n            xxh_u32 v2 = state->v2;\n            xxh_u32 v3 = state->v3;\n            xxh_u32 v4 = state->v4;\n\n            do {\n                v1 = XXH32_round(v1, XXH_readLE32(p)); p+=4;\n                v2 = XXH32_round(v2, XXH_readLE32(p)); p+=4;\n                v3 = XXH32_round(v3, XXH_readLE32(p)); p+=4;\n                v4 = XXH32_round(v4, XXH_readLE32(p)); p+=4;\n            } while (p<=limit);\n\n            state->v1 = v1;\n            state->v2 = v2;\n            state->v3 = v3;\n            state->v4 = v4;\n        }\n\n        if (p < bEnd) {\n            XXH_memcpy(state->mem32, p, (size_t)(bEnd-p));\n            state->memsize = (unsigned)(bEnd-p);\n        }\n    }\n\n    return XXH_OK;\n}\n\n\n/*! @ingroup xxh32_family */\nXXH_PUBLIC_API XXH32_hash_t XXH32_digest(const XXH32_state_t* state)\n{\n    xxh_u32 h32;\n\n    if (state->large_len) {\n        h32 = XXH_rotl32(state->v1, 1)\n            + XXH_rotl32(state->v2, 7)\n            + XXH_rotl32(state->v3, 12)\n            + XXH_rotl32(state->v4, 18);\n    } else {\n        h32 = state->v3 /* == seed */ + XXH_PRIME32_5;\n    }\n\n    h32 += state->total_len_32;\n\n    return XXH32_finalize(h32, (const xxh_u8*)state->mem32, state->memsize, XXH_aligned);\n}\n\n\n/*******   Canonical representation   *******/\n\n/*!\n * @ingroup xxh32_family\n * The default return values from XXH functions are unsigned 32 and 64 bit\n * integers.\n *\n * The canonical representation uses big endian convention, the same convention\n * as human-readable numbers (large digits first).\n *\n * This way, hash values can be written into a file or buffer, remaining\n * comparable across different systems.\n *\n * The following functions allow transformation of hash values to and from their\n * canonical format.\n */\nXXH_PUBLIC_API void XXH32_canonicalFromHash(XXH32_canonical_t* dst, XXH32_hash_t hash)\n{\n    XXH_STATIC_ASSERT(sizeof(XXH32_canonical_t) == sizeof(XXH32_hash_t));\n    if (XXH_CPU_LITTLE_ENDIAN) hash = XXH_swap32(hash);\n    memcpy(dst, &hash, sizeof(*dst));\n}\n/*! @ingroup xxh32_family */\nXXH_PUBLIC_API XXH32_hash_t XXH32_hashFromCanonical(const XXH32_canonical_t* src)\n{\n    return XXH_readBE32(src);\n}\n\n\n#ifndef XXH_NO_LONG_LONG\n\n/* *******************************************************************\n*  64-bit hash functions\n*********************************************************************/\n/*!\n * @}\n * @ingroup impl\n * @{\n */\n/*******   Memory access   *******/\n\ntypedef XXH64_hash_t xxh_u64;\n\n#ifdef XXH_OLD_NAMES\n#  define U64 xxh_u64\n#endif\n\n#if (defined(XXH_FORCE_MEMORY_ACCESS) && (XXH_FORCE_MEMORY_ACCESS==3))\n/*\n * Manual byteshift. Best for old compilers which don't inline memcpy.\n * We actually directly use XXH_readLE64 and XXH_readBE64.\n */\n#elif (defined(XXH_FORCE_MEMORY_ACCESS) && (XXH_FORCE_MEMORY_ACCESS==2))\n\n/* Force direct memory access. Only works on CPU which support unaligned memory access in hardware */\nstatic xxh_u64 XXH_read64(const void* memPtr)\n{\n    return *(const xxh_u64*) memPtr;\n}\n\n#elif (defined(XXH_FORCE_MEMORY_ACCESS) && (XXH_FORCE_MEMORY_ACCESS==1))\n\n/*\n * __pack instructions are safer, but compiler specific, hence potentially\n * problematic for some compilers.\n *\n * Currently only defined for GCC and ICC.\n */\n#ifdef XXH_OLD_NAMES\ntypedef union { xxh_u32 u32; xxh_u64 u64; } __attribute__((packed)) unalign64;\n#endif\nstatic xxh_u64 XXH_read64(const void* ptr)\n{\n    typedef union { xxh_u32 u32; xxh_u64 u64; } __attribute__((packed)) xxh_unalign64;\n    return ((const xxh_unalign64*)ptr)->u64;\n}\n\n#else\n\n/*\n * Portable and safe solution. Generally efficient.\n * see: https://stackoverflow.com/a/32095106/646947\n */\nstatic xxh_u64 XXH_read64(const void* memPtr)\n{\n    xxh_u64 val;\n    memcpy(&val, memPtr, sizeof(val));\n    return val;\n}\n\n#endif   /* XXH_FORCE_DIRECT_MEMORY_ACCESS */\n\n#if defined(_MSC_VER)     /* Visual Studio */\n#  define XXH_swap64 _byteswap_uint64\n#elif XXH_GCC_VERSION >= 403\n#  define XXH_swap64 __builtin_bswap64\n#else\nstatic xxh_u64 XXH_swap64(xxh_u64 x)\n{\n    return  ((x << 56) & 0xff00000000000000ULL) |\n            ((x << 40) & 0x00ff000000000000ULL) |\n            ((x << 24) & 0x0000ff0000000000ULL) |\n            ((x << 8)  & 0x000000ff00000000ULL) |\n            ((x >> 8)  & 0x00000000ff000000ULL) |\n            ((x >> 24) & 0x0000000000ff0000ULL) |\n            ((x >> 40) & 0x000000000000ff00ULL) |\n            ((x >> 56) & 0x00000000000000ffULL);\n}\n#endif\n\n\n/* XXH_FORCE_MEMORY_ACCESS==3 is an endian-independent byteshift load. */\n#if (defined(XXH_FORCE_MEMORY_ACCESS) && (XXH_FORCE_MEMORY_ACCESS==3))\n\nXXH_FORCE_INLINE xxh_u64 XXH_readLE64(const void* memPtr)\n{\n    const xxh_u8* bytePtr = (const xxh_u8 *)memPtr;\n    return bytePtr[0]\n         | ((xxh_u64)bytePtr[1] << 8)\n         | ((xxh_u64)bytePtr[2] << 16)\n         | ((xxh_u64)bytePtr[3] << 24)\n         | ((xxh_u64)bytePtr[4] << 32)\n         | ((xxh_u64)bytePtr[5] << 40)\n         | ((xxh_u64)bytePtr[6] << 48)\n         | ((xxh_u64)bytePtr[7] << 56);\n}\n\nXXH_FORCE_INLINE xxh_u64 XXH_readBE64(const void* memPtr)\n{\n    const xxh_u8* bytePtr = (const xxh_u8 *)memPtr;\n    return bytePtr[7]\n         | ((xxh_u64)bytePtr[6] << 8)\n         | ((xxh_u64)bytePtr[5] << 16)\n         | ((xxh_u64)bytePtr[4] << 24)\n         | ((xxh_u64)bytePtr[3] << 32)\n         | ((xxh_u64)bytePtr[2] << 40)\n         | ((xxh_u64)bytePtr[1] << 48)\n         | ((xxh_u64)bytePtr[0] << 56);\n}\n\n#else\nXXH_FORCE_INLINE xxh_u64 XXH_readLE64(const void* ptr)\n{\n    return XXH_CPU_LITTLE_ENDIAN ? XXH_read64(ptr) : XXH_swap64(XXH_read64(ptr));\n}\n\nstatic xxh_u64 XXH_readBE64(const void* ptr)\n{\n    return XXH_CPU_LITTLE_ENDIAN ? XXH_swap64(XXH_read64(ptr)) : XXH_read64(ptr);\n}\n#endif\n\nXXH_FORCE_INLINE xxh_u64\nXXH_readLE64_align(const void* ptr, XXH_alignment align)\n{\n    if (align==XXH_unaligned)\n        return XXH_readLE64(ptr);\n    else\n        return XXH_CPU_LITTLE_ENDIAN ? *(const xxh_u64*)ptr : XXH_swap64(*(const xxh_u64*)ptr);\n}\n\n\n/*******   xxh64   *******/\n/*!\n * @}\n * @defgroup xxh64_impl XXH64 implementation\n * @ingroup impl\n * @{\n */\n/* #define rather that static const, to be used as initializers */\n#define XXH_PRIME64_1  0x9E3779B185EBCA87ULL  /*!< 0b1001111000110111011110011011000110000101111010111100101010000111 */\n#define XXH_PRIME64_2  0xC2B2AE3D27D4EB4FULL  /*!< 0b1100001010110010101011100011110100100111110101001110101101001111 */\n#define XXH_PRIME64_3  0x165667B19E3779F9ULL  /*!< 0b0001011001010110011001111011000110011110001101110111100111111001 */\n#define XXH_PRIME64_4  0x85EBCA77C2B2AE63ULL  /*!< 0b1000010111101011110010100111011111000010101100101010111001100011 */\n#define XXH_PRIME64_5  0x27D4EB2F165667C5ULL  /*!< 0b0010011111010100111010110010111100010110010101100110011111000101 */\n\n#ifdef XXH_OLD_NAMES\n#  define PRIME64_1 XXH_PRIME64_1\n#  define PRIME64_2 XXH_PRIME64_2\n#  define PRIME64_3 XXH_PRIME64_3\n#  define PRIME64_4 XXH_PRIME64_4\n#  define PRIME64_5 XXH_PRIME64_5\n#endif\n\nstatic xxh_u64 XXH64_round(xxh_u64 acc, xxh_u64 input)\n{\n    acc += input * XXH_PRIME64_2;\n    acc  = XXH_rotl64(acc, 31);\n    acc *= XXH_PRIME64_1;\n    return acc;\n}\n\nstatic xxh_u64 XXH64_mergeRound(xxh_u64 acc, xxh_u64 val)\n{\n    val  = XXH64_round(0, val);\n    acc ^= val;\n    acc  = acc * XXH_PRIME64_1 + XXH_PRIME64_4;\n    return acc;\n}\n\nstatic xxh_u64 XXH64_avalanche(xxh_u64 h64)\n{\n    h64 ^= h64 >> 33;\n    h64 *= XXH_PRIME64_2;\n    h64 ^= h64 >> 29;\n    h64 *= XXH_PRIME64_3;\n    h64 ^= h64 >> 32;\n    return h64;\n}\n\n\n#define XXH_get64bits(p) XXH_readLE64_align(p, align)\n\nstatic xxh_u64\nXXH64_finalize(xxh_u64 h64, const xxh_u8* ptr, size_t len, XXH_alignment align)\n{\n    len &= 31;\n    while (len >= 8) {\n        xxh_u64 const k1 = XXH64_round(0, XXH_get64bits(ptr));\n        ptr += 8;\n        h64 ^= k1;\n        h64  = XXH_rotl64(h64,27) * XXH_PRIME64_1 + XXH_PRIME64_4;\n        len -= 8;\n    }\n    if (len >= 4) {\n        h64 ^= (xxh_u64)(XXH_get32bits(ptr)) * XXH_PRIME64_1;\n        ptr += 4;\n        h64 = XXH_rotl64(h64, 23) * XXH_PRIME64_2 + XXH_PRIME64_3;\n        len -= 4;\n    }\n    while (len > 0) {\n        h64 ^= (*ptr++) * XXH_PRIME64_5;\n        h64 = XXH_rotl64(h64, 11) * XXH_PRIME64_1;\n        --len;\n    }\n    return  XXH64_avalanche(h64);\n}\n\n#ifdef XXH_OLD_NAMES\n#  define PROCESS1_64 XXH_PROCESS1_64\n#  define PROCESS4_64 XXH_PROCESS4_64\n#  define PROCESS8_64 XXH_PROCESS8_64\n#else\n#  undef XXH_PROCESS1_64\n#  undef XXH_PROCESS4_64\n#  undef XXH_PROCESS8_64\n#endif\n\nXXH_FORCE_INLINE xxh_u64\nXXH64_endian_align(const xxh_u8* input, size_t len, xxh_u64 seed, XXH_alignment align)\n{\n    const xxh_u8* bEnd = input + len;\n    xxh_u64 h64;\n\n#if defined(XXH_ACCEPT_NULL_INPUT_POINTER) && (XXH_ACCEPT_NULL_INPUT_POINTER>=1)\n    if (input==NULL) {\n        len=0;\n        bEnd=input=(const xxh_u8*)(size_t)32;\n    }\n#endif\n\n    if (len>=32) {\n        const xxh_u8* const limit = bEnd - 32;\n        xxh_u64 v1 = seed + XXH_PRIME64_1 + XXH_PRIME64_2;\n        xxh_u64 v2 = seed + XXH_PRIME64_2;\n        xxh_u64 v3 = seed + 0;\n        xxh_u64 v4 = seed - XXH_PRIME64_1;\n\n        do {\n            v1 = XXH64_round(v1, XXH_get64bits(input)); input+=8;\n            v2 = XXH64_round(v2, XXH_get64bits(input)); input+=8;\n            v3 = XXH64_round(v3, XXH_get64bits(input)); input+=8;\n            v4 = XXH64_round(v4, XXH_get64bits(input)); input+=8;\n        } while (input<=limit);\n\n        h64 = XXH_rotl64(v1, 1) + XXH_rotl64(v2, 7) + XXH_rotl64(v3, 12) + XXH_rotl64(v4, 18);\n        h64 = XXH64_mergeRound(h64, v1);\n        h64 = XXH64_mergeRound(h64, v2);\n        h64 = XXH64_mergeRound(h64, v3);\n        h64 = XXH64_mergeRound(h64, v4);\n\n    } else {\n        h64  = seed + XXH_PRIME64_5;\n    }\n\n    h64 += (xxh_u64) len;\n\n    return XXH64_finalize(h64, input, len, align);\n}\n\n\n/*! @ingroup xxh64_family */\nXXH_PUBLIC_API XXH64_hash_t XXH64 (const void* input, size_t len, XXH64_hash_t seed)\n{\n#if 0\n    /* Simple version, good for code maintenance, but unfortunately slow for small inputs */\n    XXH64_state_t state;\n    XXH64_reset(&state, seed);\n    XXH64_update(&state, (const xxh_u8*)input, len);\n    return XXH64_digest(&state);\n#else\n    if (XXH_FORCE_ALIGN_CHECK) {\n        if ((((size_t)input) & 7)==0) {  /* Input is aligned, let's leverage the speed advantage */\n            return XXH64_endian_align((const xxh_u8*)input, len, seed, XXH_aligned);\n    }   }\n\n    return XXH64_endian_align((const xxh_u8*)input, len, seed, XXH_unaligned);\n\n#endif\n}\n\n/*******   Hash Streaming   *******/\n\n/*! @ingroup xxh64_family*/\nXXH_PUBLIC_API XXH64_state_t* XXH64_createState(void)\n{\n    return (XXH64_state_t*)XXH_malloc(sizeof(XXH64_state_t));\n}\n/*! @ingroup xxh64_family */\nXXH_PUBLIC_API XXH_errorcode XXH64_freeState(XXH64_state_t* statePtr)\n{\n    XXH_free(statePtr);\n    return XXH_OK;\n}\n\n/*! @ingroup xxh64_family */\nXXH_PUBLIC_API void XXH64_copyState(XXH64_state_t* dstState, const XXH64_state_t* srcState)\n{\n    memcpy(dstState, srcState, sizeof(*dstState));\n}\n\n/*! @ingroup xxh64_family */\nXXH_PUBLIC_API XXH_errorcode XXH64_reset(XXH64_state_t* statePtr, XXH64_hash_t seed)\n{\n    XXH64_state_t state;   /* use a local state to memcpy() in order to avoid strict-aliasing warnings */\n    memset(&state, 0, sizeof(state));\n    state.v1 = seed + XXH_PRIME64_1 + XXH_PRIME64_2;\n    state.v2 = seed + XXH_PRIME64_2;\n    state.v3 = seed + 0;\n    state.v4 = seed - XXH_PRIME64_1;\n     /* do not write into reserved64, might be removed in a future version */\n    memcpy(statePtr, &state, sizeof(state) - sizeof(state.reserved64));\n    return XXH_OK;\n}\n\n/*! @ingroup xxh64_family */\nXXH_PUBLIC_API XXH_errorcode\nXXH64_update (XXH64_state_t* state, const void* input, size_t len)\n{\n    if (input==NULL)\n#if defined(XXH_ACCEPT_NULL_INPUT_POINTER) && (XXH_ACCEPT_NULL_INPUT_POINTER>=1)\n        return XXH_OK;\n#else\n        return XXH_ERROR;\n#endif\n\n    {   const xxh_u8* p = (const xxh_u8*)input;\n        const xxh_u8* const bEnd = p + len;\n\n        state->total_len += len;\n\n        if (state->memsize + len < 32) {  /* fill in tmp buffer */\n            XXH_memcpy(((xxh_u8*)state->mem64) + state->memsize, input, len);\n            state->memsize += (xxh_u32)len;\n            return XXH_OK;\n        }\n\n        if (state->memsize) {   /* tmp buffer is full */\n            XXH_memcpy(((xxh_u8*)state->mem64) + state->memsize, input, 32-state->memsize);\n            state->v1 = XXH64_round(state->v1, XXH_readLE64(state->mem64+0));\n            state->v2 = XXH64_round(state->v2, XXH_readLE64(state->mem64+1));\n            state->v3 = XXH64_round(state->v3, XXH_readLE64(state->mem64+2));\n            state->v4 = XXH64_round(state->v4, XXH_readLE64(state->mem64+3));\n            p += 32 - state->memsize;\n            state->memsize = 0;\n        }\n\n        if (p+32 <= bEnd) {\n            const xxh_u8* const limit = bEnd - 32;\n            xxh_u64 v1 = state->v1;\n            xxh_u64 v2 = state->v2;\n            xxh_u64 v3 = state->v3;\n            xxh_u64 v4 = state->v4;\n\n            do {\n                v1 = XXH64_round(v1, XXH_readLE64(p)); p+=8;\n                v2 = XXH64_round(v2, XXH_readLE64(p)); p+=8;\n                v3 = XXH64_round(v3, XXH_readLE64(p)); p+=8;\n                v4 = XXH64_round(v4, XXH_readLE64(p)); p+=8;\n            } while (p<=limit);\n\n            state->v1 = v1;\n            state->v2 = v2;\n            state->v3 = v3;\n            state->v4 = v4;\n        }\n\n        if (p < bEnd) {\n            XXH_memcpy(state->mem64, p, (size_t)(bEnd-p));\n            state->memsize = (unsigned)(bEnd-p);\n        }\n    }\n\n    return XXH_OK;\n}\n\n\n/*! @ingroup xxh64_family */\nXXH_PUBLIC_API XXH64_hash_t XXH64_digest(const XXH64_state_t* state)\n{\n    xxh_u64 h64;\n\n    if (state->total_len >= 32) {\n        xxh_u64 const v1 = state->v1;\n        xxh_u64 const v2 = state->v2;\n        xxh_u64 const v3 = state->v3;\n        xxh_u64 const v4 = state->v4;\n\n        h64 = XXH_rotl64(v1, 1) + XXH_rotl64(v2, 7) + XXH_rotl64(v3, 12) + XXH_rotl64(v4, 18);\n        h64 = XXH64_mergeRound(h64, v1);\n        h64 = XXH64_mergeRound(h64, v2);\n        h64 = XXH64_mergeRound(h64, v3);\n        h64 = XXH64_mergeRound(h64, v4);\n    } else {\n        h64  = state->v3 /*seed*/ + XXH_PRIME64_5;\n    }\n\n    h64 += (xxh_u64) state->total_len;\n\n    return XXH64_finalize(h64, (const xxh_u8*)state->mem64, (size_t)state->total_len, XXH_aligned);\n}\n\n\n/******* Canonical representation   *******/\n\n/*! @ingroup xxh64_family */\nXXH_PUBLIC_API void XXH64_canonicalFromHash(XXH64_canonical_t* dst, XXH64_hash_t hash)\n{\n    XXH_STATIC_ASSERT(sizeof(XXH64_canonical_t) == sizeof(XXH64_hash_t));\n    if (XXH_CPU_LITTLE_ENDIAN) hash = XXH_swap64(hash);\n    memcpy(dst, &hash, sizeof(*dst));\n}\n\n/*! @ingroup xxh64_family */\nXXH_PUBLIC_API XXH64_hash_t XXH64_hashFromCanonical(const XXH64_canonical_t* src)\n{\n    return XXH_readBE64(src);\n}\n\n#ifndef XXH_NO_XXH3\n\n/* *********************************************************************\n*  XXH3\n*  New generation hash designed for speed on small keys and vectorization\n************************************************************************ */\n/*!\n * @}\n * @defgroup xxh3_impl XXH3 implementation\n * @ingroup impl\n * @{\n */\n\n/* ===   Compiler specifics   === */\n\n#if ((defined(sun) || defined(__sun)) && __cplusplus) /* Solaris includes __STDC_VERSION__ with C++. Tested with GCC 5.5 */\n#  define XXH_RESTRICT /* disable */\n#elif defined (__STDC_VERSION__) && __STDC_VERSION__ >= 199901L   /* >= C99 */\n#  define XXH_RESTRICT   restrict\n#else\n/* Note: it might be useful to define __restrict or __restrict__ for some C++ compilers */\n#  define XXH_RESTRICT   /* disable */\n#endif\n\n#if (defined(__GNUC__) && (__GNUC__ >= 3))  \\\n  || (defined(__INTEL_COMPILER) && (__INTEL_COMPILER >= 800)) \\\n  || defined(__clang__)\n#    define XXH_likely(x) __builtin_expect(x, 1)\n#    define XXH_unlikely(x) __builtin_expect(x, 0)\n#else\n#    define XXH_likely(x) (x)\n#    define XXH_unlikely(x) (x)\n#endif\n\n#if defined(__GNUC__)\n#  if defined(__AVX2__)\n#    include <immintrin.h>\n#  elif defined(__SSE2__)\n#    include <emmintrin.h>\n#  elif defined(__ARM_NEON__) || defined(__ARM_NEON)\n#    define inline __inline__  /* circumvent a clang bug */\n#    include <arm_neon.h>\n#    undef inline\n#  endif\n#elif defined(_MSC_VER)\n#  include <intrin.h>\n#endif\n\n/*\n * One goal of XXH3 is to make it fast on both 32-bit and 64-bit, while\n * remaining a true 64-bit/128-bit hash function.\n *\n * This is done by prioritizing a subset of 64-bit operations that can be\n * emulated without too many steps on the average 32-bit machine.\n *\n * For example, these two lines seem similar, and run equally fast on 64-bit:\n *\n *   xxh_u64 x;\n *   x ^= (x >> 47); // good\n *   x ^= (x >> 13); // bad\n *\n * However, to a 32-bit machine, there is a major difference.\n *\n * x ^= (x >> 47) looks like this:\n *\n *   x.lo ^= (x.hi >> (47 - 32));\n *\n * while x ^= (x >> 13) looks like this:\n *\n *   // note: funnel shifts are not usually cheap.\n *   x.lo ^= (x.lo >> 13) | (x.hi << (32 - 13));\n *   x.hi ^= (x.hi >> 13);\n *\n * The first one is significantly faster than the second, simply because the\n * shift is larger than 32. This means:\n *  - All the bits we need are in the upper 32 bits, so we can ignore the lower\n *    32 bits in the shift.\n *  - The shift result will always fit in the lower 32 bits, and therefore,\n *    we can ignore the upper 32 bits in the xor.\n *\n * Thanks to this optimization, XXH3 only requires these features to be efficient:\n *\n *  - Usable unaligned access\n *  - A 32-bit or 64-bit ALU\n *      - If 32-bit, a decent ADC instruction\n *  - A 32 or 64-bit multiply with a 64-bit result\n *  - For the 128-bit variant, a decent byteswap helps short inputs.\n *\n * The first two are already required by XXH32, and almost all 32-bit and 64-bit\n * platforms which can run XXH32 can run XXH3 efficiently.\n *\n * Thumb-1, the classic 16-bit only subset of ARM's instruction set, is one\n * notable exception.\n *\n * First of all, Thumb-1 lacks support for the UMULL instruction which\n * performs the important long multiply. This means numerous __aeabi_lmul\n * calls.\n *\n * Second of all, the 8 functional registers are just not enough.\n * Setup for __aeabi_lmul, byteshift loads, pointers, and all arithmetic need\n * Lo registers, and this shuffling results in thousands more MOVs than A32.\n *\n * A32 and T32 don't have this limitation. They can access all 14 registers,\n * do a 32->64 multiply with UMULL, and the flexible operand allowing free\n * shifts is helpful, too.\n *\n * Therefore, we do a quick sanity check.\n *\n * If compiling Thumb-1 for a target which supports ARM instructions, we will\n * emit a warning, as it is not a \"sane\" platform to compile for.\n *\n * Usually, if this happens, it is because of an accident and you probably need\n * to specify -march, as you likely meant to compile for a newer architecture.\n *\n * Credit: large sections of the vectorial and asm source code paths\n *         have been contributed by @easyaspi314\n */\n#if defined(__thumb__) && !defined(__thumb2__) && defined(__ARM_ARCH_ISA_ARM)\n#   warning \"XXH3 is highly inefficient without ARM or Thumb-2.\"\n#endif\n\n/* ==========================================\n * Vectorization detection\n * ========================================== */\n\n#ifdef XXH_DOXYGEN\n/*!\n * @ingroup tuning\n * @brief Overrides the vectorization implementation chosen for XXH3.\n *\n * Can be defined to 0 to disable SIMD or any of the values mentioned in\n * @ref XXH_VECTOR_TYPE.\n *\n * If this is not defined, it uses predefined macros to determine the best\n * implementation.\n */\n#  define XXH_VECTOR XXH_SCALAR\n/*!\n * @ingroup tuning\n * @brief Possible values for @ref XXH_VECTOR.\n *\n * Note that these are actually implemented as macros.\n *\n * If this is not defined, it is detected automatically.\n * @ref XXH_X86DISPATCH overrides this.\n */\nenum XXH_VECTOR_TYPE /* fake enum */ {\n    XXH_SCALAR = 0,  /*!< Portable scalar version */\n    XXH_SSE2   = 1,  /*!<\n                      * SSE2 for Pentium 4, Opteron, all x86_64.\n                      *\n                      * @note SSE2 is also guaranteed on Windows 10, macOS, and\n                      * Android x86.\n                      */\n    XXH_AVX2   = 2,  /*!< AVX2 for Haswell and Bulldozer */\n    XXH_AVX512 = 3,  /*!< AVX512 for Skylake and Icelake */\n    XXH_NEON   = 4,  /*!< NEON for most ARMv7-A and all AArch64 */\n    XXH_VSX    = 5,  /*!< VSX and ZVector for POWER8/z13 (64-bit) */\n};\n/*!\n * @ingroup tuning\n * @brief Selects the minimum alignment for XXH3's accumulators.\n *\n * When using SIMD, this should match the alignment reqired for said vector\n * type, so, for example, 32 for AVX2.\n *\n * Default: Auto detected.\n */\n#  define XXH_ACC_ALIGN 8\n#endif\n\n/* Actual definition */\n#ifndef XXH_DOXYGEN\n#  define XXH_SCALAR 0\n#  define XXH_SSE2   1\n#  define XXH_AVX2   2\n#  define XXH_AVX512 3\n#  define XXH_NEON   4\n#  define XXH_VSX    5\n#endif\n\n#ifndef XXH_VECTOR    /* can be defined on command line */\n#  if defined(__AVX512F__)\n#    define XXH_VECTOR XXH_AVX512\n#  elif defined(__AVX2__)\n#    define XXH_VECTOR XXH_AVX2\n#  elif defined(__SSE2__) || defined(_M_AMD64) || defined(_M_X64) || (defined(_M_IX86_FP) && (_M_IX86_FP == 2))\n#    define XXH_VECTOR XXH_SSE2\n#  elif defined(__GNUC__) /* msvc support maybe later */ \\\n  && (defined(__ARM_NEON__) || defined(__ARM_NEON)) \\\n  && (defined(__LITTLE_ENDIAN__) /* We only support little endian NEON */ \\\n    || (defined(__BYTE_ORDER__) && __BYTE_ORDER__ == __ORDER_LITTLE_ENDIAN__))\n#    define XXH_VECTOR XXH_NEON\n#  elif (defined(__PPC64__) && defined(__POWER8_VECTOR__)) \\\n     || (defined(__s390x__) && defined(__VEC__)) \\\n     && defined(__GNUC__) /* TODO: IBM XL */\n#    define XXH_VECTOR XXH_VSX\n#  else\n#    define XXH_VECTOR XXH_SCALAR\n#  endif\n#endif\n\n/*\n * Controls the alignment of the accumulator,\n * for compatibility with aligned vector loads, which are usually faster.\n */\n#ifndef XXH_ACC_ALIGN\n#  if defined(XXH_X86DISPATCH)\n#     define XXH_ACC_ALIGN 64  /* for compatibility with avx512 */\n#  elif XXH_VECTOR == XXH_SCALAR  /* scalar */\n#     define XXH_ACC_ALIGN 8\n#  elif XXH_VECTOR == XXH_SSE2  /* sse2 */\n#     define XXH_ACC_ALIGN 16\n#  elif XXH_VECTOR == XXH_AVX2  /* avx2 */\n#     define XXH_ACC_ALIGN 32\n#  elif XXH_VECTOR == XXH_NEON  /* neon */\n#     define XXH_ACC_ALIGN 16\n#  elif XXH_VECTOR == XXH_VSX   /* vsx */\n#     define XXH_ACC_ALIGN 16\n#  elif XXH_VECTOR == XXH_AVX512  /* avx512 */\n#     define XXH_ACC_ALIGN 64\n#  endif\n#endif\n\n#if defined(XXH_X86DISPATCH) || XXH_VECTOR == XXH_SSE2 \\\n    || XXH_VECTOR == XXH_AVX2 || XXH_VECTOR == XXH_AVX512\n#  define XXH_SEC_ALIGN XXH_ACC_ALIGN\n#else\n#  define XXH_SEC_ALIGN 8\n#endif\n\n/*\n * UGLY HACK:\n * GCC usually generates the best code with -O3 for xxHash.\n *\n * However, when targeting AVX2, it is overzealous in its unrolling resulting\n * in code roughly 3/4 the speed of Clang.\n *\n * There are other issues, such as GCC splitting _mm256_loadu_si256 into\n * _mm_loadu_si128 + _mm256_inserti128_si256. This is an optimization which\n * only applies to Sandy and Ivy Bridge... which don't even support AVX2.\n *\n * That is why when compiling the AVX2 version, it is recommended to use either\n *   -O2 -mavx2 -march=haswell\n * or\n *   -O2 -mavx2 -mno-avx256-split-unaligned-load\n * for decent performance, or to use Clang instead.\n *\n * Fortunately, we can control the first one with a pragma that forces GCC into\n * -O2, but the other one we can't control without \"failed to inline always\n * inline function due to target mismatch\" warnings.\n */\n#if XXH_VECTOR == XXH_AVX2 /* AVX2 */ \\\n  && defined(__GNUC__) && !defined(__clang__) /* GCC, not Clang */ \\\n  && defined(__OPTIMIZE__) && !defined(__OPTIMIZE_SIZE__) /* respect -O0 and -Os */\n#  pragma GCC push_options\n#  pragma GCC optimize(\"-O2\")\n#endif\n\n\n#if XXH_VECTOR == XXH_NEON\n/*\n * NEON's setup for vmlal_u32 is a little more complicated than it is on\n * SSE2, AVX2, and VSX.\n *\n * While PMULUDQ and VMULEUW both perform a mask, VMLAL.U32 performs an upcast.\n *\n * To do the same operation, the 128-bit 'Q' register needs to be split into\n * two 64-bit 'D' registers, performing this operation::\n *\n *   [                a                 |                 b                ]\n *            |              '---------. .--------'                |\n *            |                         x                          |\n *            |              .---------' '--------.                |\n *   [ a & 0xFFFFFFFF | b & 0xFFFFFFFF ],[    a >> 32     |     b >> 32    ]\n *\n * Due to significant changes in aarch64, the fastest method for aarch64 is\n * completely different than the fastest method for ARMv7-A.\n *\n * ARMv7-A treats D registers as unions overlaying Q registers, so modifying\n * D11 will modify the high half of Q5. This is similar to how modifying AH\n * will only affect bits 8-15 of AX on x86.\n *\n * VZIP takes two registers, and puts even lanes in one register and odd lanes\n * in the other.\n *\n * On ARMv7-A, this strangely modifies both parameters in place instead of\n * taking the usual 3-operand form.\n *\n * Therefore, if we want to do this, we can simply use a D-form VZIP.32 on the\n * lower and upper halves of the Q register to end up with the high and low\n * halves where we want - all in one instruction.\n *\n *   vzip.32   d10, d11       @ d10 = { d10[0], d11[0] }; d11 = { d10[1], d11[1] }\n *\n * Unfortunately we need inline assembly for this: Instructions modifying two\n * registers at once is not possible in GCC or Clang's IR, and they have to\n * create a copy.\n *\n * aarch64 requires a different approach.\n *\n * In order to make it easier to write a decent compiler for aarch64, many\n * quirks were removed, such as conditional execution.\n *\n * NEON was also affected by this.\n *\n * aarch64 cannot access the high bits of a Q-form register, and writes to a\n * D-form register zero the high bits, similar to how writes to W-form scalar\n * registers (or DWORD registers on x86_64) work.\n *\n * The formerly free vget_high intrinsics now require a vext (with a few\n * exceptions)\n *\n * Additionally, VZIP was replaced by ZIP1 and ZIP2, which are the equivalent\n * of PUNPCKL* and PUNPCKH* in SSE, respectively, in order to only modify one\n * operand.\n *\n * The equivalent of the VZIP.32 on the lower and upper halves would be this\n * mess:\n *\n *   ext     v2.4s, v0.4s, v0.4s, #2 // v2 = { v0[2], v0[3], v0[0], v0[1] }\n *   zip1    v1.2s, v0.2s, v2.2s     // v1 = { v0[0], v2[0] }\n *   zip2    v0.2s, v0.2s, v1.2s     // v0 = { v0[1], v2[1] }\n *\n * Instead, we use a literal downcast, vmovn_u64 (XTN), and vshrn_n_u64 (SHRN):\n *\n *   shrn    v1.2s, v0.2d, #32  // v1 = (uint32x2_t)(v0 >> 32);\n *   xtn     v0.2s, v0.2d       // v0 = (uint32x2_t)(v0 & 0xFFFFFFFF);\n *\n * This is available on ARMv7-A, but is less efficient than a single VZIP.32.\n */\n\n/*!\n * Function-like macro:\n * void XXH_SPLIT_IN_PLACE(uint64x2_t &in, uint32x2_t &outLo, uint32x2_t &outHi)\n * {\n *     outLo = (uint32x2_t)(in & 0xFFFFFFFF);\n *     outHi = (uint32x2_t)(in >> 32);\n *     in = UNDEFINED;\n * }\n */\n# if !defined(XXH_NO_VZIP_HACK) /* define to disable */ \\\n   && defined(__GNUC__) \\\n   && !defined(__aarch64__) && !defined(__arm64__)\n#  define XXH_SPLIT_IN_PLACE(in, outLo, outHi)                                              \\\n    do {                                                                                    \\\n      /* Undocumented GCC/Clang operand modifier: %e0 = lower D half, %f0 = upper D half */ \\\n      /* https://github.com/gcc-mirror/gcc/blob/38cf91e5/gcc/config/arm/arm.c#L22486 */     \\\n      /* https://github.com/llvm-mirror/llvm/blob/2c4ca683/lib/Target/ARM/ARMAsmPrinter.cpp#L399 */ \\\n      __asm__(\"vzip.32  %e0, %f0\" : \"+w\" (in));                                             \\\n      (outLo) = vget_low_u32 (vreinterpretq_u32_u64(in));                                   \\\n      (outHi) = vget_high_u32(vreinterpretq_u32_u64(in));                                   \\\n   } while (0)\n# else\n#  define XXH_SPLIT_IN_PLACE(in, outLo, outHi)                                            \\\n    do {                                                                                  \\\n      (outLo) = vmovn_u64    (in);                                                        \\\n      (outHi) = vshrn_n_u64  ((in), 32);                                                  \\\n    } while (0)\n# endif\n#endif  /* XXH_VECTOR == XXH_NEON */\n\n/*\n * VSX and Z Vector helpers.\n *\n * This is very messy, and any pull requests to clean this up are welcome.\n *\n * There are a lot of problems with supporting VSX and s390x, due to\n * inconsistent intrinsics, spotty coverage, and multiple endiannesses.\n */\n#if XXH_VECTOR == XXH_VSX\n#  if defined(__s390x__)\n#    include <s390intrin.h>\n#  else\n/* gcc's altivec.h can have the unwanted consequence to unconditionally\n * #define bool, vector, and pixel keywords,\n * with bad consequences for programs already using these keywords for other purposes.\n * The paragraph defining these macros is skipped when __APPLE_ALTIVEC__ is defined.\n * __APPLE_ALTIVEC__ is _generally_ defined automatically by the compiler,\n * but it seems that, in some cases, it isn't.\n * Force the build macro to be defined, so that keywords are not altered.\n */\n#    if defined(__GNUC__) && !defined(__APPLE_ALTIVEC__)\n#      define __APPLE_ALTIVEC__\n#    endif\n#    include <altivec.h>\n#  endif\n\ntypedef __vector unsigned long long xxh_u64x2;\ntypedef __vector unsigned char xxh_u8x16;\ntypedef __vector unsigned xxh_u32x4;\n\n# ifndef XXH_VSX_BE\n#  if defined(__BIG_ENDIAN__) \\\n  || (defined(__BYTE_ORDER__) && __BYTE_ORDER__ == __ORDER_BIG_ENDIAN__)\n#    define XXH_VSX_BE 1\n#  elif defined(__VEC_ELEMENT_REG_ORDER__) && __VEC_ELEMENT_REG_ORDER__ == __ORDER_BIG_ENDIAN__\n#    warning \"-maltivec=be is not recommended. Please use native endianness.\"\n#    define XXH_VSX_BE 1\n#  else\n#    define XXH_VSX_BE 0\n#  endif\n# endif /* !defined(XXH_VSX_BE) */\n\n# if XXH_VSX_BE\n#  if defined(__POWER9_VECTOR__) || (defined(__clang__) && defined(__s390x__))\n#    define XXH_vec_revb vec_revb\n#  else\n/*!\n * A polyfill for POWER9's vec_revb().\n */\nXXH_FORCE_INLINE xxh_u64x2 XXH_vec_revb(xxh_u64x2 val)\n{\n    xxh_u8x16 const vByteSwap = { 0x07, 0x06, 0x05, 0x04, 0x03, 0x02, 0x01, 0x00,\n                                  0x0F, 0x0E, 0x0D, 0x0C, 0x0B, 0x0A, 0x09, 0x08 };\n    return vec_perm(val, val, vByteSwap);\n}\n#  endif\n# endif /* XXH_VSX_BE */\n\n/*!\n * Performs an unaligned vector load and byte swaps it on big endian.\n */\nXXH_FORCE_INLINE xxh_u64x2 XXH_vec_loadu(const void *ptr)\n{\n    xxh_u64x2 ret;\n    memcpy(&ret, ptr, sizeof(xxh_u64x2));\n# if XXH_VSX_BE\n    ret = XXH_vec_revb(ret);\n# endif\n    return ret;\n}\n\n/*\n * vec_mulo and vec_mule are very problematic intrinsics on PowerPC\n *\n * These intrinsics weren't added until GCC 8, despite existing for a while,\n * and they are endian dependent. Also, their meaning swap depending on version.\n * */\n# if defined(__s390x__)\n /* s390x is always big endian, no issue on this platform */\n#  define XXH_vec_mulo vec_mulo\n#  define XXH_vec_mule vec_mule\n# elif defined(__clang__) && XXH_HAS_BUILTIN(__builtin_altivec_vmuleuw)\n/* Clang has a better way to control this, we can just use the builtin which doesn't swap. */\n#  define XXH_vec_mulo __builtin_altivec_vmulouw\n#  define XXH_vec_mule __builtin_altivec_vmuleuw\n# else\n/* gcc needs inline assembly */\n/* Adapted from https://github.com/google/highwayhash/blob/master/highwayhash/hh_vsx.h. */\nXXH_FORCE_INLINE xxh_u64x2 XXH_vec_mulo(xxh_u32x4 a, xxh_u32x4 b)\n{\n    xxh_u64x2 result;\n    __asm__(\"vmulouw %0, %1, %2\" : \"=v\" (result) : \"v\" (a), \"v\" (b));\n    return result;\n}\nXXH_FORCE_INLINE xxh_u64x2 XXH_vec_mule(xxh_u32x4 a, xxh_u32x4 b)\n{\n    xxh_u64x2 result;\n    __asm__(\"vmuleuw %0, %1, %2\" : \"=v\" (result) : \"v\" (a), \"v\" (b));\n    return result;\n}\n# endif /* XXH_vec_mulo, XXH_vec_mule */\n#endif /* XXH_VECTOR == XXH_VSX */\n\n\n/* prefetch\n * can be disabled, by declaring XXH_NO_PREFETCH build macro */\n#if defined(XXH_NO_PREFETCH)\n#  define XXH_PREFETCH(ptr)  (void)(ptr)  /* disabled */\n#else\n#  if defined(_MSC_VER) && (defined(_M_X64) || defined(_M_IX86))  /* _mm_prefetch() not defined outside of x86/x64 */\n#    include <mmintrin.h>   /* https://msdn.microsoft.com/fr-fr/library/84szxsww(v=vs.90).aspx */\n#    define XXH_PREFETCH(ptr)  _mm_prefetch((const char*)(ptr), _MM_HINT_T0)\n#  elif defined(__GNUC__) && ( (__GNUC__ >= 4) || ( (__GNUC__ == 3) && (__GNUC_MINOR__ >= 1) ) )\n#    define XXH_PREFETCH(ptr)  __builtin_prefetch((ptr), 0 /* rw==read */, 3 /* locality */)\n#  else\n#    define XXH_PREFETCH(ptr) (void)(ptr)  /* disabled */\n#  endif\n#endif  /* XXH_NO_PREFETCH */\n\n\n/* ==========================================\n * XXH3 default settings\n * ========================================== */\n\n#define XXH_SECRET_DEFAULT_SIZE 192   /* minimum XXH3_SECRET_SIZE_MIN */\n\n#if (XXH_SECRET_DEFAULT_SIZE < XXH3_SECRET_SIZE_MIN)\n#  error \"default keyset is not large enough\"\n#endif\n\n/*! Pseudorandom secret taken directly from FARSH. */\nXXH_ALIGN(64) static const xxh_u8 XXH3_kSecret[XXH_SECRET_DEFAULT_SIZE] = {\n    0xb8, 0xfe, 0x6c, 0x39, 0x23, 0xa4, 0x4b, 0xbe, 0x7c, 0x01, 0x81, 0x2c, 0xf7, 0x21, 0xad, 0x1c,\n    0xde, 0xd4, 0x6d, 0xe9, 0x83, 0x90, 0x97, 0xdb, 0x72, 0x40, 0xa4, 0xa4, 0xb7, 0xb3, 0x67, 0x1f,\n    0xcb, 0x79, 0xe6, 0x4e, 0xcc, 0xc0, 0xe5, 0x78, 0x82, 0x5a, 0xd0, 0x7d, 0xcc, 0xff, 0x72, 0x21,\n    0xb8, 0x08, 0x46, 0x74, 0xf7, 0x43, 0x24, 0x8e, 0xe0, 0x35, 0x90, 0xe6, 0x81, 0x3a, 0x26, 0x4c,\n    0x3c, 0x28, 0x52, 0xbb, 0x91, 0xc3, 0x00, 0xcb, 0x88, 0xd0, 0x65, 0x8b, 0x1b, 0x53, 0x2e, 0xa3,\n    0x71, 0x64, 0x48, 0x97, 0xa2, 0x0d, 0xf9, 0x4e, 0x38, 0x19, 0xef, 0x46, 0xa9, 0xde, 0xac, 0xd8,\n    0xa8, 0xfa, 0x76, 0x3f, 0xe3, 0x9c, 0x34, 0x3f, 0xf9, 0xdc, 0xbb, 0xc7, 0xc7, 0x0b, 0x4f, 0x1d,\n    0x8a, 0x51, 0xe0, 0x4b, 0xcd, 0xb4, 0x59, 0x31, 0xc8, 0x9f, 0x7e, 0xc9, 0xd9, 0x78, 0x73, 0x64,\n    0xea, 0xc5, 0xac, 0x83, 0x34, 0xd3, 0xeb, 0xc3, 0xc5, 0x81, 0xa0, 0xff, 0xfa, 0x13, 0x63, 0xeb,\n    0x17, 0x0d, 0xdd, 0x51, 0xb7, 0xf0, 0xda, 0x49, 0xd3, 0x16, 0x55, 0x26, 0x29, 0xd4, 0x68, 0x9e,\n    0x2b, 0x16, 0xbe, 0x58, 0x7d, 0x47, 0xa1, 0xfc, 0x8f, 0xf8, 0xb8, 0xd1, 0x7a, 0xd0, 0x31, 0xce,\n    0x45, 0xcb, 0x3a, 0x8f, 0x95, 0x16, 0x04, 0x28, 0xaf, 0xd7, 0xfb, 0xca, 0xbb, 0x4b, 0x40, 0x7e,\n};\n\n\n#ifdef XXH_OLD_NAMES\n#  define kSecret XXH3_kSecret\n#endif\n\n#ifdef XXH_DOXYGEN\n/*!\n * @brief Calculates a 32-bit to 64-bit long multiply.\n *\n * Implemented as a macro.\n *\n * Wraps `__emulu` on MSVC x86 because it tends to call `__allmul` when it doesn't\n * need to (but it shouldn't need to anyways, it is about 7 instructions to do\n * a 64x64 multiply...). Since we know that this will _always_ emit `MULL`, we\n * use that instead of the normal method.\n *\n * If you are compiling for platforms like Thumb-1 and don't have a better option,\n * you may also want to write your own long multiply routine here.\n *\n * @param x, y Numbers to be multiplied\n * @return 64-bit product of the low 32 bits of @p x and @p y.\n */\nXXH_FORCE_INLINE xxh_u64\nXXH_mult32to64(xxh_u64 x, xxh_u64 y)\n{\n   return (x & 0xFFFFFFFF) * (y & 0xFFFFFFFF);\n}\n#elif defined(_MSC_VER) && defined(_M_IX86)\n#    include <intrin.h>\n#    define XXH_mult32to64(x, y) __emulu((unsigned)(x), (unsigned)(y))\n#else\n/*\n * Downcast + upcast is usually better than masking on older compilers like\n * GCC 4.2 (especially 32-bit ones), all without affecting newer compilers.\n *\n * The other method, (x & 0xFFFFFFFF) * (y & 0xFFFFFFFF), will AND both operands\n * and perform a full 64x64 multiply -- entirely redundant on 32-bit.\n */\n#    define XXH_mult32to64(x, y) ((xxh_u64)(xxh_u32)(x) * (xxh_u64)(xxh_u32)(y))\n#endif\n\n/*!\n * @brief Calculates a 64->128-bit long multiply.\n *\n * Uses `__uint128_t` and `_umul128` if available, otherwise uses a scalar\n * version.\n *\n * @param lhs, rhs The 64-bit integers to be multiplied\n * @return The 128-bit result represented in an @ref XXH128_hash_t.\n */\nstatic XXH128_hash_t\nXXH_mult64to128(xxh_u64 lhs, xxh_u64 rhs)\n{\n    /*\n     * GCC/Clang __uint128_t method.\n     *\n     * On most 64-bit targets, GCC and Clang define a __uint128_t type.\n     * This is usually the best way as it usually uses a native long 64-bit\n     * multiply, such as MULQ on x86_64 or MUL + UMULH on aarch64.\n     *\n     * Usually.\n     *\n     * Despite being a 32-bit platform, Clang (and emscripten) define this type\n     * despite not having the arithmetic for it. This results in a laggy\n     * compiler builtin call which calculates a full 128-bit multiply.\n     * In that case it is best to use the portable one.\n     * https://github.com/Cyan4973/xxHash/issues/211#issuecomment-515575677\n     */\n#if defined(__GNUC__) && !defined(__wasm__) \\\n    && defined(__SIZEOF_INT128__) \\\n    || (defined(_INTEGRAL_MAX_BITS) && _INTEGRAL_MAX_BITS >= 128)\n\n    __uint128_t const product = (__uint128_t)lhs * (__uint128_t)rhs;\n    XXH128_hash_t r128;\n    r128.low64  = (xxh_u64)(product);\n    r128.high64 = (xxh_u64)(product >> 64);\n    return r128;\n\n    /*\n     * MSVC for x64's _umul128 method.\n     *\n     * xxh_u64 _umul128(xxh_u64 Multiplier, xxh_u64 Multiplicand, xxh_u64 *HighProduct);\n     *\n     * This compiles to single operand MUL on x64.\n     */\n#elif defined(_M_X64) || defined(_M_IA64)\n\n#ifndef _MSC_VER\n#   pragma intrinsic(_umul128)\n#endif\n    xxh_u64 product_high;\n    xxh_u64 const product_low = _umul128(lhs, rhs, &product_high);\n    XXH128_hash_t r128;\n    r128.low64  = product_low;\n    r128.high64 = product_high;\n    return r128;\n\n#else\n    /*\n     * Portable scalar method. Optimized for 32-bit and 64-bit ALUs.\n     *\n     * This is a fast and simple grade school multiply, which is shown below\n     * with base 10 arithmetic instead of base 0x100000000.\n     *\n     *           9 3 // D2 lhs = 93\n     *         x 7 5 // D2 rhs = 75\n     *     ----------\n     *           1 5 // D2 lo_lo = (93 % 10) * (75 % 10) = 15\n     *         4 5 | // D2 hi_lo = (93 / 10) * (75 % 10) = 45\n     *         2 1 | // D2 lo_hi = (93 % 10) * (75 / 10) = 21\n     *     + 6 3 | | // D2 hi_hi = (93 / 10) * (75 / 10) = 63\n     *     ---------\n     *         2 7 | // D2 cross = (15 / 10) + (45 % 10) + 21 = 27\n     *     + 6 7 | | // D2 upper = (27 / 10) + (45 / 10) + 63 = 67\n     *     ---------\n     *       6 9 7 5 // D4 res = (27 * 10) + (15 % 10) + (67 * 100) = 6975\n     *\n     * The reasons for adding the products like this are:\n     *  1. It avoids manual carry tracking. Just like how\n     *     (9 * 9) + 9 + 9 = 99, the same applies with this for UINT64_MAX.\n     *     This avoids a lot of complexity.\n     *\n     *  2. It hints for, and on Clang, compiles to, the powerful UMAAL\n     *     instruction available in ARM's Digital Signal Processing extension\n     *     in 32-bit ARMv6 and later, which is shown below:\n     *\n     *         void UMAAL(xxh_u32 *RdLo, xxh_u32 *RdHi, xxh_u32 Rn, xxh_u32 Rm)\n     *         {\n     *             xxh_u64 product = (xxh_u64)*RdLo * (xxh_u64)*RdHi + Rn + Rm;\n     *             *RdLo = (xxh_u32)(product & 0xFFFFFFFF);\n     *             *RdHi = (xxh_u32)(product >> 32);\n     *         }\n     *\n     *     This instruction was designed for efficient long multiplication, and\n     *     allows this to be calculated in only 4 instructions at speeds\n     *     comparable to some 64-bit ALUs.\n     *\n     *  3. It isn't terrible on other platforms. Usually this will be a couple\n     *     of 32-bit ADD/ADCs.\n     */\n\n    /* First calculate all of the cross products. */\n    xxh_u64 const lo_lo = XXH_mult32to64(lhs & 0xFFFFFFFF, rhs & 0xFFFFFFFF);\n    xxh_u64 const hi_lo = XXH_mult32to64(lhs >> 32,        rhs & 0xFFFFFFFF);\n    xxh_u64 const lo_hi = XXH_mult32to64(lhs & 0xFFFFFFFF, rhs >> 32);\n    xxh_u64 const hi_hi = XXH_mult32to64(lhs >> 32,        rhs >> 32);\n\n    /* Now add the products together. These will never overflow. */\n    xxh_u64 const cross = (lo_lo >> 32) + (hi_lo & 0xFFFFFFFF) + lo_hi;\n    xxh_u64 const upper = (hi_lo >> 32) + (cross >> 32)        + hi_hi;\n    xxh_u64 const lower = (cross << 32) | (lo_lo & 0xFFFFFFFF);\n\n    XXH128_hash_t r128;\n    r128.low64  = lower;\n    r128.high64 = upper;\n    return r128;\n#endif\n}\n\n/*!\n * @brief Calculates a 64-bit to 128-bit multiply, then XOR folds it.\n *\n * The reason for the separate function is to prevent passing too many structs\n * around by value. This will hopefully inline the multiply, but we don't force it.\n *\n * @param lhs, rhs The 64-bit integers to multiply\n * @return The low 64 bits of the product XOR'd by the high 64 bits.\n * @see XXH_mult64to128()\n */\nstatic xxh_u64\nXXH3_mul128_fold64(xxh_u64 lhs, xxh_u64 rhs)\n{\n    XXH128_hash_t product = XXH_mult64to128(lhs, rhs);\n    return product.low64 ^ product.high64;\n}\n\n/*! Seems to produce slightly better code on GCC for some reason. */\nXXH_FORCE_INLINE xxh_u64 XXH_xorshift64(xxh_u64 v64, int shift)\n{\n    XXH_ASSERT(0 <= shift && shift < 64);\n    return v64 ^ (v64 >> shift);\n}\n\n/*\n * This is a fast avalanche stage,\n * suitable when input bits are already partially mixed\n */\nstatic XXH64_hash_t XXH3_avalanche(xxh_u64 h64)\n{\n    h64 = XXH_xorshift64(h64, 37);\n    h64 *= 0x165667919E3779F9ULL;\n    h64 = XXH_xorshift64(h64, 32);\n    return h64;\n}\n\n/*\n * This is a stronger avalanche,\n * inspired by Pelle Evensen's rrmxmx\n * preferable when input has not been previously mixed\n */\nstatic XXH64_hash_t XXH3_rrmxmx(xxh_u64 h64, xxh_u64 len)\n{\n    /* this mix is inspired by Pelle Evensen's rrmxmx */\n    h64 ^= XXH_rotl64(h64, 49) ^ XXH_rotl64(h64, 24);\n    h64 *= 0x9FB21C651E98DF25ULL;\n    h64 ^= (h64 >> 35) + len ;\n    h64 *= 0x9FB21C651E98DF25ULL;\n    return XXH_xorshift64(h64, 28);\n}\n\n\n/* ==========================================\n * Short keys\n * ==========================================\n * One of the shortcomings of XXH32 and XXH64 was that their performance was\n * sub-optimal on short lengths. It used an iterative algorithm which strongly\n * favored lengths that were a multiple of 4 or 8.\n *\n * Instead of iterating over individual inputs, we use a set of single shot\n * functions which piece together a range of lengths and operate in constant time.\n *\n * Additionally, the number of multiplies has been significantly reduced. This\n * reduces latency, especially when emulating 64-bit multiplies on 32-bit.\n *\n * Depending on the platform, this may or may not be faster than XXH32, but it\n * is almost guaranteed to be faster than XXH64.\n */\n\n/*\n * At very short lengths, there isn't enough input to fully hide secrets, or use\n * the entire secret.\n *\n * There is also only a limited amount of mixing we can do before significantly\n * impacting performance.\n *\n * Therefore, we use different sections of the secret and always mix two secret\n * samples with an XOR. This should have no effect on performance on the\n * seedless or withSeed variants because everything _should_ be constant folded\n * by modern compilers.\n *\n * The XOR mixing hides individual parts of the secret and increases entropy.\n *\n * This adds an extra layer of strength for custom secrets.\n */\nXXH_FORCE_INLINE XXH64_hash_t\nXXH3_len_1to3_64b(const xxh_u8* input, size_t len, const xxh_u8* secret, XXH64_hash_t seed)\n{\n    XXH_ASSERT(input != NULL);\n    XXH_ASSERT(1 <= len && len <= 3);\n    XXH_ASSERT(secret != NULL);\n    /*\n     * len = 1: combined = { input[0], 0x01, input[0], input[0] }\n     * len = 2: combined = { input[1], 0x02, input[0], input[1] }\n     * len = 3: combined = { input[2], 0x03, input[0], input[1] }\n     */\n    {   xxh_u8  const c1 = input[0];\n        xxh_u8  const c2 = input[len >> 1];\n        xxh_u8  const c3 = input[len - 1];\n        xxh_u32 const combined = ((xxh_u32)c1 << 16) | ((xxh_u32)c2  << 24)\n                               | ((xxh_u32)c3 <<  0) | ((xxh_u32)len << 8);\n        xxh_u64 const bitflip = (XXH_readLE32(secret) ^ XXH_readLE32(secret+4)) + seed;\n        xxh_u64 const keyed = (xxh_u64)combined ^ bitflip;\n        return XXH64_avalanche(keyed);\n    }\n}\n\nXXH_FORCE_INLINE XXH64_hash_t\nXXH3_len_4to8_64b(const xxh_u8* input, size_t len, const xxh_u8* secret, XXH64_hash_t seed)\n{\n    XXH_ASSERT(input != NULL);\n    XXH_ASSERT(secret != NULL);\n    XXH_ASSERT(4 <= len && len <= 8);\n    seed ^= (xxh_u64)XXH_swap32((xxh_u32)seed) << 32;\n    {   xxh_u32 const input1 = XXH_readLE32(input);\n        xxh_u32 const input2 = XXH_readLE32(input + len - 4);\n        xxh_u64 const bitflip = (XXH_readLE64(secret+8) ^ XXH_readLE64(secret+16)) - seed;\n        xxh_u64 const input64 = input2 + (((xxh_u64)input1) << 32);\n        xxh_u64 const keyed = input64 ^ bitflip;\n        return XXH3_rrmxmx(keyed, len);\n    }\n}\n\nXXH_FORCE_INLINE XXH64_hash_t\nXXH3_len_9to16_64b(const xxh_u8* input, size_t len, const xxh_u8* secret, XXH64_hash_t seed)\n{\n    XXH_ASSERT(input != NULL);\n    XXH_ASSERT(secret != NULL);\n    XXH_ASSERT(8 <= len && len <= 16);\n    {   xxh_u64 const bitflip1 = (XXH_readLE64(secret+24) ^ XXH_readLE64(secret+32)) + seed;\n        xxh_u64 const bitflip2 = (XXH_readLE64(secret+40) ^ XXH_readLE64(secret+48)) - seed;\n        xxh_u64 const input_lo = XXH_readLE64(input)           ^ bitflip1;\n        xxh_u64 const input_hi = XXH_readLE64(input + len - 8) ^ bitflip2;\n        xxh_u64 const acc = len\n                          + XXH_swap64(input_lo) + input_hi\n                          + XXH3_mul128_fold64(input_lo, input_hi);\n        return XXH3_avalanche(acc);\n    }\n}\n\nXXH_FORCE_INLINE XXH64_hash_t\nXXH3_len_0to16_64b(const xxh_u8* input, size_t len, const xxh_u8* secret, XXH64_hash_t seed)\n{\n    XXH_ASSERT(len <= 16);\n    {   if (XXH_likely(len >  8)) return XXH3_len_9to16_64b(input, len, secret, seed);\n        if (XXH_likely(len >= 4)) return XXH3_len_4to8_64b(input, len, secret, seed);\n        if (len) return XXH3_len_1to3_64b(input, len, secret, seed);\n        return XXH64_avalanche(seed ^ (XXH_readLE64(secret+56) ^ XXH_readLE64(secret+64)));\n    }\n}\n\n/*\n * DISCLAIMER: There are known *seed-dependent* multicollisions here due to\n * multiplication by zero, affecting hashes of lengths 17 to 240.\n *\n * However, they are very unlikely.\n *\n * Keep this in mind when using the unseeded XXH3_64bits() variant: As with all\n * unseeded non-cryptographic hashes, it does not attempt to defend itself\n * against specially crafted inputs, only random inputs.\n *\n * Compared to classic UMAC where a 1 in 2^31 chance of 4 consecutive bytes\n * cancelling out the secret is taken an arbitrary number of times (addressed\n * in XXH3_accumulate_512), this collision is very unlikely with random inputs\n * and/or proper seeding:\n *\n * This only has a 1 in 2^63 chance of 8 consecutive bytes cancelling out, in a\n * function that is only called up to 16 times per hash with up to 240 bytes of\n * input.\n *\n * This is not too bad for a non-cryptographic hash function, especially with\n * only 64 bit outputs.\n *\n * The 128-bit variant (which trades some speed for strength) is NOT affected\n * by this, although it is always a good idea to use a proper seed if you care\n * about strength.\n */\nXXH_FORCE_INLINE xxh_u64 XXH3_mix16B(const xxh_u8* XXH_RESTRICT input,\n                                     const xxh_u8* XXH_RESTRICT secret, xxh_u64 seed64)\n{\n#if defined(__GNUC__) && !defined(__clang__) /* GCC, not Clang */ \\\n  && defined(__i386__) && defined(__SSE2__)  /* x86 + SSE2 */ \\\n  && !defined(XXH_ENABLE_AUTOVECTORIZE)      /* Define to disable like XXH32 hack */\n    /*\n     * UGLY HACK:\n     * GCC for x86 tends to autovectorize the 128-bit multiply, resulting in\n     * slower code.\n     *\n     * By forcing seed64 into a register, we disrupt the cost model and\n     * cause it to scalarize. See `XXH32_round()`\n     *\n     * FIXME: Clang's output is still _much_ faster -- On an AMD Ryzen 3600,\n     * XXH3_64bits @ len=240 runs at 4.6 GB/s with Clang 9, but 3.3 GB/s on\n     * GCC 9.2, despite both emitting scalar code.\n     *\n     * GCC generates much better scalar code than Clang for the rest of XXH3,\n     * which is why finding a more optimal codepath is an interest.\n     */\n    XXH_COMPILER_GUARD(seed64);\n#endif\n    {   xxh_u64 const input_lo = XXH_readLE64(input);\n        xxh_u64 const input_hi = XXH_readLE64(input+8);\n        return XXH3_mul128_fold64(\n            input_lo ^ (XXH_readLE64(secret)   + seed64),\n            input_hi ^ (XXH_readLE64(secret+8) - seed64)\n        );\n    }\n}\n\n/* For mid range keys, XXH3 uses a Mum-hash variant. */\nXXH_FORCE_INLINE XXH64_hash_t\nXXH3_len_17to128_64b(const xxh_u8* XXH_RESTRICT input, size_t len,\n                     const xxh_u8* XXH_RESTRICT secret, size_t secretSize,\n                     XXH64_hash_t seed)\n{\n    XXH_ASSERT(secretSize >= XXH3_SECRET_SIZE_MIN); (void)secretSize;\n    XXH_ASSERT(16 < len && len <= 128);\n\n    {   xxh_u64 acc = len * XXH_PRIME64_1;\n        if (len > 32) {\n            if (len > 64) {\n                if (len > 96) {\n                    acc += XXH3_mix16B(input+48, secret+96, seed);\n                    acc += XXH3_mix16B(input+len-64, secret+112, seed);\n                }\n                acc += XXH3_mix16B(input+32, secret+64, seed);\n                acc += XXH3_mix16B(input+len-48, secret+80, seed);\n            }\n            acc += XXH3_mix16B(input+16, secret+32, seed);\n            acc += XXH3_mix16B(input+len-32, secret+48, seed);\n        }\n        acc += XXH3_mix16B(input+0, secret+0, seed);\n        acc += XXH3_mix16B(input+len-16, secret+16, seed);\n\n        return XXH3_avalanche(acc);\n    }\n}\n\n#define XXH3_MIDSIZE_MAX 240\n\nXXH_NO_INLINE XXH64_hash_t\nXXH3_len_129to240_64b(const xxh_u8* XXH_RESTRICT input, size_t len,\n                      const xxh_u8* XXH_RESTRICT secret, size_t secretSize,\n                      XXH64_hash_t seed)\n{\n    XXH_ASSERT(secretSize >= XXH3_SECRET_SIZE_MIN); (void)secretSize;\n    XXH_ASSERT(128 < len && len <= XXH3_MIDSIZE_MAX);\n\n    #define XXH3_MIDSIZE_STARTOFFSET 3\n    #define XXH3_MIDSIZE_LASTOFFSET  17\n\n    {   xxh_u64 acc = len * XXH_PRIME64_1;\n        int const nbRounds = (int)len / 16;\n        int i;\n        for (i=0; i<8; i++) {\n            acc += XXH3_mix16B(input+(16*i), secret+(16*i), seed);\n        }\n        acc = XXH3_avalanche(acc);\n        XXH_ASSERT(nbRounds >= 8);\n#if defined(__clang__)                                /* Clang */ \\\n    && (defined(__ARM_NEON) || defined(__ARM_NEON__)) /* NEON */ \\\n    && !defined(XXH_ENABLE_AUTOVECTORIZE)             /* Define to disable */\n        /*\n         * UGLY HACK:\n         * Clang for ARMv7-A tries to vectorize this loop, similar to GCC x86.\n         * In everywhere else, it uses scalar code.\n         *\n         * For 64->128-bit multiplies, even if the NEON was 100% optimal, it\n         * would still be slower than UMAAL (see XXH_mult64to128).\n         *\n         * Unfortunately, Clang doesn't handle the long multiplies properly and\n         * converts them to the nonexistent \"vmulq_u64\" intrinsic, which is then\n         * scalarized into an ugly mess of VMOV.32 instructions.\n         *\n         * This mess is difficult to avoid without turning autovectorization\n         * off completely, but they are usually relatively minor and/or not\n         * worth it to fix.\n         *\n         * This loop is the easiest to fix, as unlike XXH32, this pragma\n         * _actually works_ because it is a loop vectorization instead of an\n         * SLP vectorization.\n         */\n        #pragma clang loop vectorize(disable)\n#endif\n        for (i=8 ; i < nbRounds; i++) {\n            acc += XXH3_mix16B(input+(16*i), secret+(16*(i-8)) + XXH3_MIDSIZE_STARTOFFSET, seed);\n        }\n        /* last bytes */\n        acc += XXH3_mix16B(input + len - 16, secret + XXH3_SECRET_SIZE_MIN - XXH3_MIDSIZE_LASTOFFSET, seed);\n        return XXH3_avalanche(acc);\n    }\n}\n\n\n/* =======     Long Keys     ======= */\n\n#define XXH_STRIPE_LEN 64\n#define XXH_SECRET_CONSUME_RATE 8   /* nb of secret bytes consumed at each accumulation */\n#define XXH_ACC_NB (XXH_STRIPE_LEN / sizeof(xxh_u64))\n\n#ifdef XXH_OLD_NAMES\n#  define STRIPE_LEN XXH_STRIPE_LEN\n#  define ACC_NB XXH_ACC_NB\n#endif\n\nXXH_FORCE_INLINE void XXH_writeLE64(void* dst, xxh_u64 v64)\n{\n    if (!XXH_CPU_LITTLE_ENDIAN) v64 = XXH_swap64(v64);\n    memcpy(dst, &v64, sizeof(v64));\n}\n\n/* Several intrinsic functions below are supposed to accept __int64 as argument,\n * as documented in https://software.intel.com/sites/landingpage/IntrinsicsGuide/ .\n * However, several environments do not define __int64 type,\n * requiring a workaround.\n */\n#if !defined (__VMS) \\\n  && (defined (__cplusplus) \\\n  || (defined (__STDC_VERSION__) && (__STDC_VERSION__ >= 199901L) /* C99 */) )\n    typedef int64_t xxh_i64;\n#else\n    /* the following type must have a width of 64-bit */\n    typedef long long xxh_i64;\n#endif\n\n/*\n * XXH3_accumulate_512 is the tightest loop for long inputs, and it is the most optimized.\n *\n * It is a hardened version of UMAC, based off of FARSH's implementation.\n *\n * This was chosen because it adapts quite well to 32-bit, 64-bit, and SIMD\n * implementations, and it is ridiculously fast.\n *\n * We harden it by mixing the original input to the accumulators as well as the product.\n *\n * This means that in the (relatively likely) case of a multiply by zero, the\n * original input is preserved.\n *\n * On 128-bit inputs, we swap 64-bit pairs when we add the input to improve\n * cross-pollination, as otherwise the upper and lower halves would be\n * essentially independent.\n *\n * This doesn't matter on 64-bit hashes since they all get merged together in\n * the end, so we skip the extra step.\n *\n * Both XXH3_64bits and XXH3_128bits use this subroutine.\n */\n\n#if (XXH_VECTOR == XXH_AVX512) \\\n     || (defined(XXH_DISPATCH_AVX512) && XXH_DISPATCH_AVX512 != 0)\n\n#ifndef XXH_TARGET_AVX512\n# define XXH_TARGET_AVX512  /* disable attribute target */\n#endif\n\nXXH_FORCE_INLINE XXH_TARGET_AVX512 void\nXXH3_accumulate_512_avx512(void* XXH_RESTRICT acc,\n                     const void* XXH_RESTRICT input,\n                     const void* XXH_RESTRICT secret)\n{\n    XXH_ALIGN(64) __m512i* const xacc = (__m512i *) acc;\n    XXH_ASSERT((((size_t)acc) & 63) == 0);\n    XXH_STATIC_ASSERT(XXH_STRIPE_LEN == sizeof(__m512i));\n\n    {\n        /* data_vec    = input[0]; */\n        __m512i const data_vec    = _mm512_loadu_si512   (input);\n        /* key_vec     = secret[0]; */\n        __m512i const key_vec     = _mm512_loadu_si512   (secret);\n        /* data_key    = data_vec ^ key_vec; */\n        __m512i const data_key    = _mm512_xor_si512     (data_vec, key_vec);\n        /* data_key_lo = data_key >> 32; */\n        __m512i const data_key_lo = _mm512_shuffle_epi32 (data_key, (_MM_PERM_ENUM)_MM_SHUFFLE(0, 3, 0, 1));\n        /* product     = (data_key & 0xffffffff) * (data_key_lo & 0xffffffff); */\n        __m512i const product     = _mm512_mul_epu32     (data_key, data_key_lo);\n        /* xacc[0] += swap(data_vec); */\n        __m512i const data_swap = _mm512_shuffle_epi32(data_vec, (_MM_PERM_ENUM)_MM_SHUFFLE(1, 0, 3, 2));\n        __m512i const sum       = _mm512_add_epi64(*xacc, data_swap);\n        /* xacc[0] += product; */\n        *xacc = _mm512_add_epi64(product, sum);\n    }\n}\n\n/*\n * XXH3_scrambleAcc: Scrambles the accumulators to improve mixing.\n *\n * Multiplication isn't perfect, as explained by Google in HighwayHash:\n *\n *  // Multiplication mixes/scrambles bytes 0-7 of the 64-bit result to\n *  // varying degrees. In descending order of goodness, bytes\n *  // 3 4 2 5 1 6 0 7 have quality 228 224 164 160 100 96 36 32.\n *  // As expected, the upper and lower bytes are much worse.\n *\n * Source: https://github.com/google/highwayhash/blob/0aaf66b/highwayhash/hh_avx2.h#L291\n *\n * Since our algorithm uses a pseudorandom secret to add some variance into the\n * mix, we don't need to (or want to) mix as often or as much as HighwayHash does.\n *\n * This isn't as tight as XXH3_accumulate, but still written in SIMD to avoid\n * extraction.\n *\n * Both XXH3_64bits and XXH3_128bits use this subroutine.\n */\n\nXXH_FORCE_INLINE XXH_TARGET_AVX512 void\nXXH3_scrambleAcc_avx512(void* XXH_RESTRICT acc, const void* XXH_RESTRICT secret)\n{\n    XXH_ASSERT((((size_t)acc) & 63) == 0);\n    XXH_STATIC_ASSERT(XXH_STRIPE_LEN == sizeof(__m512i));\n    {   XXH_ALIGN(64) __m512i* const xacc = (__m512i*) acc;\n        const __m512i prime32 = _mm512_set1_epi32((int)XXH_PRIME32_1);\n\n        /* xacc[0] ^= (xacc[0] >> 47) */\n        __m512i const acc_vec     = *xacc;\n        __m512i const shifted     = _mm512_srli_epi64    (acc_vec, 47);\n        __m512i const data_vec    = _mm512_xor_si512     (acc_vec, shifted);\n        /* xacc[0] ^= secret; */\n        __m512i const key_vec     = _mm512_loadu_si512   (secret);\n        __m512i const data_key    = _mm512_xor_si512     (data_vec, key_vec);\n\n        /* xacc[0] *= XXH_PRIME32_1; */\n        __m512i const data_key_hi = _mm512_shuffle_epi32 (data_key, (_MM_PERM_ENUM)_MM_SHUFFLE(0, 3, 0, 1));\n        __m512i const prod_lo     = _mm512_mul_epu32     (data_key, prime32);\n        __m512i const prod_hi     = _mm512_mul_epu32     (data_key_hi, prime32);\n        *xacc = _mm512_add_epi64(prod_lo, _mm512_slli_epi64(prod_hi, 32));\n    }\n}\n\nXXH_FORCE_INLINE XXH_TARGET_AVX512 void\nXXH3_initCustomSecret_avx512(void* XXH_RESTRICT customSecret, xxh_u64 seed64)\n{\n    XXH_STATIC_ASSERT((XXH_SECRET_DEFAULT_SIZE & 63) == 0);\n    XXH_STATIC_ASSERT(XXH_SEC_ALIGN == 64);\n    XXH_ASSERT(((size_t)customSecret & 63) == 0);\n    (void)(&XXH_writeLE64);\n    {   int const nbRounds = XXH_SECRET_DEFAULT_SIZE / sizeof(__m512i);\n        __m512i const seed = _mm512_mask_set1_epi64(_mm512_set1_epi64((xxh_i64)seed64), 0xAA, -(xxh_i64)seed64);\n\n        XXH_ALIGN(64) const __m512i* const src  = (const __m512i*) XXH3_kSecret;\n        XXH_ALIGN(64)       __m512i* const dest = (      __m512i*) customSecret;\n        int i;\n        for (i=0; i < nbRounds; ++i) {\n            /* GCC has a bug, _mm512_stream_load_si512 accepts 'void*', not 'void const*',\n             * this will warn \"discards const qualifier\". */\n            union {\n                XXH_ALIGN(64) const __m512i* cp;\n                XXH_ALIGN(64) void* p;\n            } remote_const_void;\n            remote_const_void.cp = src + i;\n            dest[i] = _mm512_add_epi64(_mm512_stream_load_si512(remote_const_void.p), seed);\n    }   }\n}\n\n#endif\n\n#if (XXH_VECTOR == XXH_AVX2) \\\n    || (defined(XXH_DISPATCH_AVX2) && XXH_DISPATCH_AVX2 != 0)\n\n#ifndef XXH_TARGET_AVX2\n# define XXH_TARGET_AVX2  /* disable attribute target */\n#endif\n\nXXH_FORCE_INLINE XXH_TARGET_AVX2 void\nXXH3_accumulate_512_avx2( void* XXH_RESTRICT acc,\n                    const void* XXH_RESTRICT input,\n                    const void* XXH_RESTRICT secret)\n{\n    XXH_ASSERT((((size_t)acc) & 31) == 0);\n    {   XXH_ALIGN(32) __m256i* const xacc    =       (__m256i *) acc;\n        /* Unaligned. This is mainly for pointer arithmetic, and because\n         * _mm256_loadu_si256 requires  a const __m256i * pointer for some reason. */\n        const         __m256i* const xinput  = (const __m256i *) input;\n        /* Unaligned. This is mainly for pointer arithmetic, and because\n         * _mm256_loadu_si256 requires a const __m256i * pointer for some reason. */\n        const         __m256i* const xsecret = (const __m256i *) secret;\n\n        size_t i;\n        for (i=0; i < XXH_STRIPE_LEN/sizeof(__m256i); i++) {\n            /* data_vec    = xinput[i]; */\n            __m256i const data_vec    = _mm256_loadu_si256    (xinput+i);\n            /* key_vec     = xsecret[i]; */\n            __m256i const key_vec     = _mm256_loadu_si256   (xsecret+i);\n            /* data_key    = data_vec ^ key_vec; */\n            __m256i const data_key    = _mm256_xor_si256     (data_vec, key_vec);\n            /* data_key_lo = data_key >> 32; */\n            __m256i const data_key_lo = _mm256_shuffle_epi32 (data_key, _MM_SHUFFLE(0, 3, 0, 1));\n            /* product     = (data_key & 0xffffffff) * (data_key_lo & 0xffffffff); */\n            __m256i const product     = _mm256_mul_epu32     (data_key, data_key_lo);\n            /* xacc[i] += swap(data_vec); */\n            __m256i const data_swap = _mm256_shuffle_epi32(data_vec, _MM_SHUFFLE(1, 0, 3, 2));\n            __m256i const sum       = _mm256_add_epi64(xacc[i], data_swap);\n            /* xacc[i] += product; */\n            xacc[i] = _mm256_add_epi64(product, sum);\n    }   }\n}\n\nXXH_FORCE_INLINE XXH_TARGET_AVX2 void\nXXH3_scrambleAcc_avx2(void* XXH_RESTRICT acc, const void* XXH_RESTRICT secret)\n{\n    XXH_ASSERT((((size_t)acc) & 31) == 0);\n    {   XXH_ALIGN(32) __m256i* const xacc = (__m256i*) acc;\n        /* Unaligned. This is mainly for pointer arithmetic, and because\n         * _mm256_loadu_si256 requires a const __m256i * pointer for some reason. */\n        const         __m256i* const xsecret = (const __m256i *) secret;\n        const __m256i prime32 = _mm256_set1_epi32((int)XXH_PRIME32_1);\n\n        size_t i;\n        for (i=0; i < XXH_STRIPE_LEN/sizeof(__m256i); i++) {\n            /* xacc[i] ^= (xacc[i] >> 47) */\n            __m256i const acc_vec     = xacc[i];\n            __m256i const shifted     = _mm256_srli_epi64    (acc_vec, 47);\n            __m256i const data_vec    = _mm256_xor_si256     (acc_vec, shifted);\n            /* xacc[i] ^= xsecret; */\n            __m256i const key_vec     = _mm256_loadu_si256   (xsecret+i);\n            __m256i const data_key    = _mm256_xor_si256     (data_vec, key_vec);\n\n            /* xacc[i] *= XXH_PRIME32_1; */\n            __m256i const data_key_hi = _mm256_shuffle_epi32 (data_key, _MM_SHUFFLE(0, 3, 0, 1));\n            __m256i const prod_lo     = _mm256_mul_epu32     (data_key, prime32);\n            __m256i const prod_hi     = _mm256_mul_epu32     (data_key_hi, prime32);\n            xacc[i] = _mm256_add_epi64(prod_lo, _mm256_slli_epi64(prod_hi, 32));\n        }\n    }\n}\n\nXXH_FORCE_INLINE XXH_TARGET_AVX2 void XXH3_initCustomSecret_avx2(void* XXH_RESTRICT customSecret, xxh_u64 seed64)\n{\n    XXH_STATIC_ASSERT((XXH_SECRET_DEFAULT_SIZE & 31) == 0);\n    XXH_STATIC_ASSERT((XXH_SECRET_DEFAULT_SIZE / sizeof(__m256i)) == 6);\n    XXH_STATIC_ASSERT(XXH_SEC_ALIGN <= 64);\n    (void)(&XXH_writeLE64);\n    XXH_PREFETCH(customSecret);\n    {   __m256i const seed = _mm256_set_epi64x(-(xxh_i64)seed64, (xxh_i64)seed64, -(xxh_i64)seed64, (xxh_i64)seed64);\n\n        XXH_ALIGN(64) const __m256i* const src  = (const __m256i*) XXH3_kSecret;\n        XXH_ALIGN(64)       __m256i*       dest = (      __m256i*) customSecret;\n\n#       if defined(__GNUC__) || defined(__clang__)\n        /*\n         * On GCC & Clang, marking 'dest' as modified will cause the compiler:\n         *   - do not extract the secret from sse registers in the internal loop\n         *   - use less common registers, and avoid pushing these reg into stack\n         */\n        XXH_COMPILER_GUARD(dest);\n#       endif\n\n        /* GCC -O2 need unroll loop manually */\n        dest[0] = _mm256_add_epi64(_mm256_stream_load_si256(src+0), seed);\n        dest[1] = _mm256_add_epi64(_mm256_stream_load_si256(src+1), seed);\n        dest[2] = _mm256_add_epi64(_mm256_stream_load_si256(src+2), seed);\n        dest[3] = _mm256_add_epi64(_mm256_stream_load_si256(src+3), seed);\n        dest[4] = _mm256_add_epi64(_mm256_stream_load_si256(src+4), seed);\n        dest[5] = _mm256_add_epi64(_mm256_stream_load_si256(src+5), seed);\n    }\n}\n\n#endif\n\n/* x86dispatch always generates SSE2 */\n#if (XXH_VECTOR == XXH_SSE2) || defined(XXH_X86DISPATCH)\n\n#ifndef XXH_TARGET_SSE2\n# define XXH_TARGET_SSE2  /* disable attribute target */\n#endif\n\nXXH_FORCE_INLINE XXH_TARGET_SSE2 void\nXXH3_accumulate_512_sse2( void* XXH_RESTRICT acc,\n                    const void* XXH_RESTRICT input,\n                    const void* XXH_RESTRICT secret)\n{\n    /* SSE2 is just a half-scale version of the AVX2 version. */\n    XXH_ASSERT((((size_t)acc) & 15) == 0);\n    {   XXH_ALIGN(16) __m128i* const xacc    =       (__m128i *) acc;\n        /* Unaligned. This is mainly for pointer arithmetic, and because\n         * _mm_loadu_si128 requires a const __m128i * pointer for some reason. */\n        const         __m128i* const xinput  = (const __m128i *) input;\n        /* Unaligned. This is mainly for pointer arithmetic, and because\n         * _mm_loadu_si128 requires a const __m128i * pointer for some reason. */\n        const         __m128i* const xsecret = (const __m128i *) secret;\n\n        size_t i;\n        for (i=0; i < XXH_STRIPE_LEN/sizeof(__m128i); i++) {\n            /* data_vec    = xinput[i]; */\n            __m128i const data_vec    = _mm_loadu_si128   (xinput+i);\n            /* key_vec     = xsecret[i]; */\n            __m128i const key_vec     = _mm_loadu_si128   (xsecret+i);\n            /* data_key    = data_vec ^ key_vec; */\n            __m128i const data_key    = _mm_xor_si128     (data_vec, key_vec);\n            /* data_key_lo = data_key >> 32; */\n            __m128i const data_key_lo = _mm_shuffle_epi32 (data_key, _MM_SHUFFLE(0, 3, 0, 1));\n            /* product     = (data_key & 0xffffffff) * (data_key_lo & 0xffffffff); */\n            __m128i const product     = _mm_mul_epu32     (data_key, data_key_lo);\n            /* xacc[i] += swap(data_vec); */\n            __m128i const data_swap = _mm_shuffle_epi32(data_vec, _MM_SHUFFLE(1,0,3,2));\n            __m128i const sum       = _mm_add_epi64(xacc[i], data_swap);\n            /* xacc[i] += product; */\n            xacc[i] = _mm_add_epi64(product, sum);\n    }   }\n}\n\nXXH_FORCE_INLINE XXH_TARGET_SSE2 void\nXXH3_scrambleAcc_sse2(void* XXH_RESTRICT acc, const void* XXH_RESTRICT secret)\n{\n    XXH_ASSERT((((size_t)acc) & 15) == 0);\n    {   XXH_ALIGN(16) __m128i* const xacc = (__m128i*) acc;\n        /* Unaligned. This is mainly for pointer arithmetic, and because\n         * _mm_loadu_si128 requires a const __m128i * pointer for some reason. */\n        const         __m128i* const xsecret = (const __m128i *) secret;\n        const __m128i prime32 = _mm_set1_epi32((int)XXH_PRIME32_1);\n\n        size_t i;\n        for (i=0; i < XXH_STRIPE_LEN/sizeof(__m128i); i++) {\n            /* xacc[i] ^= (xacc[i] >> 47) */\n            __m128i const acc_vec     = xacc[i];\n            __m128i const shifted     = _mm_srli_epi64    (acc_vec, 47);\n            __m128i const data_vec    = _mm_xor_si128     (acc_vec, shifted);\n            /* xacc[i] ^= xsecret[i]; */\n            __m128i const key_vec     = _mm_loadu_si128   (xsecret+i);\n            __m128i const data_key    = _mm_xor_si128     (data_vec, key_vec);\n\n            /* xacc[i] *= XXH_PRIME32_1; */\n            __m128i const data_key_hi = _mm_shuffle_epi32 (data_key, _MM_SHUFFLE(0, 3, 0, 1));\n            __m128i const prod_lo     = _mm_mul_epu32     (data_key, prime32);\n            __m128i const prod_hi     = _mm_mul_epu32     (data_key_hi, prime32);\n            xacc[i] = _mm_add_epi64(prod_lo, _mm_slli_epi64(prod_hi, 32));\n        }\n    }\n}\n\nXXH_FORCE_INLINE XXH_TARGET_SSE2 void XXH3_initCustomSecret_sse2(void* XXH_RESTRICT customSecret, xxh_u64 seed64)\n{\n    XXH_STATIC_ASSERT((XXH_SECRET_DEFAULT_SIZE & 15) == 0);\n    (void)(&XXH_writeLE64);\n    {   int const nbRounds = XXH_SECRET_DEFAULT_SIZE / sizeof(__m128i);\n\n#       if defined(_MSC_VER) && defined(_M_IX86) && _MSC_VER < 1900\n        // MSVC 32bit mode does not support _mm_set_epi64x before 2015\n        XXH_ALIGN(16) const xxh_i64 seed64x2[2] = { (xxh_i64)seed64, -(xxh_i64)seed64 };\n        __m128i const seed = _mm_load_si128((__m128i const*)seed64x2);\n#       else\n        __m128i const seed = _mm_set_epi64x(-(xxh_i64)seed64, (xxh_i64)seed64);\n#       endif\n        int i;\n\n        XXH_ALIGN(64)        const float* const src  = (float const*) XXH3_kSecret;\n        XXH_ALIGN(XXH_SEC_ALIGN) __m128i*       dest = (__m128i*) customSecret;\n#       if defined(__GNUC__) || defined(__clang__)\n        /*\n         * On GCC & Clang, marking 'dest' as modified will cause the compiler:\n         *   - do not extract the secret from sse registers in the internal loop\n         *   - use less common registers, and avoid pushing these reg into stack\n         */\n        XXH_COMPILER_GUARD(dest);\n#       endif\n\n        for (i=0; i < nbRounds; ++i) {\n            dest[i] = _mm_add_epi64(_mm_castps_si128(_mm_load_ps(src+i*4)), seed);\n    }   }\n}\n\n#endif\n\n#if (XXH_VECTOR == XXH_NEON)\n\nXXH_FORCE_INLINE void\nXXH3_accumulate_512_neon( void* XXH_RESTRICT acc,\n                    const void* XXH_RESTRICT input,\n                    const void* XXH_RESTRICT secret)\n{\n    XXH_ASSERT((((size_t)acc) & 15) == 0);\n    {\n        XXH_ALIGN(16) uint64x2_t* const xacc = (uint64x2_t *) acc;\n        /* We don't use a uint32x4_t pointer because it causes bus errors on ARMv7. */\n        uint8_t const* const xinput = (const uint8_t *) input;\n        uint8_t const* const xsecret  = (const uint8_t *) secret;\n\n        size_t i;\n        for (i=0; i < XXH_STRIPE_LEN / sizeof(uint64x2_t); i++) {\n            /* data_vec = xinput[i]; */\n            uint8x16_t data_vec    = vld1q_u8(xinput  + (i * 16));\n            /* key_vec  = xsecret[i];  */\n            uint8x16_t key_vec     = vld1q_u8(xsecret + (i * 16));\n            uint64x2_t data_key;\n            uint32x2_t data_key_lo, data_key_hi;\n            /* xacc[i] += swap(data_vec); */\n            uint64x2_t const data64  = vreinterpretq_u64_u8(data_vec);\n            uint64x2_t const swapped = vextq_u64(data64, data64, 1);\n            xacc[i] = vaddq_u64 (xacc[i], swapped);\n            /* data_key = data_vec ^ key_vec; */\n            data_key = vreinterpretq_u64_u8(veorq_u8(data_vec, key_vec));\n            /* data_key_lo = (uint32x2_t) (data_key & 0xFFFFFFFF);\n             * data_key_hi = (uint32x2_t) (data_key >> 32);\n             * data_key = UNDEFINED; */\n            XXH_SPLIT_IN_PLACE(data_key, data_key_lo, data_key_hi);\n            /* xacc[i] += (uint64x2_t) data_key_lo * (uint64x2_t) data_key_hi; */\n            xacc[i] = vmlal_u32 (xacc[i], data_key_lo, data_key_hi);\n\n        }\n    }\n}\n\nXXH_FORCE_INLINE void\nXXH3_scrambleAcc_neon(void* XXH_RESTRICT acc, const void* XXH_RESTRICT secret)\n{\n    XXH_ASSERT((((size_t)acc) & 15) == 0);\n\n    {   uint64x2_t* xacc       = (uint64x2_t*) acc;\n        uint8_t const* xsecret = (uint8_t const*) secret;\n        uint32x2_t prime       = vdup_n_u32 (XXH_PRIME32_1);\n\n        size_t i;\n        for (i=0; i < XXH_STRIPE_LEN/sizeof(uint64x2_t); i++) {\n            /* xacc[i] ^= (xacc[i] >> 47); */\n            uint64x2_t acc_vec  = xacc[i];\n            uint64x2_t shifted  = vshrq_n_u64 (acc_vec, 47);\n            uint64x2_t data_vec = veorq_u64   (acc_vec, shifted);\n\n            /* xacc[i] ^= xsecret[i]; */\n            uint8x16_t key_vec  = vld1q_u8(xsecret + (i * 16));\n            uint64x2_t data_key = veorq_u64(data_vec, vreinterpretq_u64_u8(key_vec));\n\n            /* xacc[i] *= XXH_PRIME32_1 */\n            uint32x2_t data_key_lo, data_key_hi;\n            /* data_key_lo = (uint32x2_t) (xacc[i] & 0xFFFFFFFF);\n             * data_key_hi = (uint32x2_t) (xacc[i] >> 32);\n             * xacc[i] = UNDEFINED; */\n            XXH_SPLIT_IN_PLACE(data_key, data_key_lo, data_key_hi);\n            {   /*\n                 * prod_hi = (data_key >> 32) * XXH_PRIME32_1;\n                 *\n                 * Avoid vmul_u32 + vshll_n_u32 since Clang 6 and 7 will\n                 * incorrectly \"optimize\" this:\n                 *   tmp     = vmul_u32(vmovn_u64(a), vmovn_u64(b));\n                 *   shifted = vshll_n_u32(tmp, 32);\n                 * to this:\n                 *   tmp     = \"vmulq_u64\"(a, b); // no such thing!\n                 *   shifted = vshlq_n_u64(tmp, 32);\n                 *\n                 * However, unlike SSE, Clang lacks a 64-bit multiply routine\n                 * for NEON, and it scalarizes two 64-bit multiplies instead.\n                 *\n                 * vmull_u32 has the same timing as vmul_u32, and it avoids\n                 * this bug completely.\n                 * See https://bugs.llvm.org/show_bug.cgi?id=39967\n                 */\n                uint64x2_t prod_hi = vmull_u32 (data_key_hi, prime);\n                /* xacc[i] = prod_hi << 32; */\n                xacc[i] = vshlq_n_u64(prod_hi, 32);\n                /* xacc[i] += (prod_hi & 0xFFFFFFFF) * XXH_PRIME32_1; */\n                xacc[i] = vmlal_u32(xacc[i], data_key_lo, prime);\n            }\n    }   }\n}\n\n#endif\n\n#if (XXH_VECTOR == XXH_VSX)\n\nXXH_FORCE_INLINE void\nXXH3_accumulate_512_vsx(  void* XXH_RESTRICT acc,\n                    const void* XXH_RESTRICT input,\n                    const void* XXH_RESTRICT secret)\n{\n          xxh_u64x2* const xacc     =       (xxh_u64x2*) acc;    /* presumed aligned */\n    xxh_u64x2 const* const xinput   = (xxh_u64x2 const*) input;   /* no alignment restriction */\n    xxh_u64x2 const* const xsecret  = (xxh_u64x2 const*) secret;    /* no alignment restriction */\n    xxh_u64x2 const v32 = { 32, 32 };\n    size_t i;\n    for (i = 0; i < XXH_STRIPE_LEN / sizeof(xxh_u64x2); i++) {\n        /* data_vec = xinput[i]; */\n        xxh_u64x2 const data_vec = XXH_vec_loadu(xinput + i);\n        /* key_vec = xsecret[i]; */\n        xxh_u64x2 const key_vec  = XXH_vec_loadu(xsecret + i);\n        xxh_u64x2 const data_key = data_vec ^ key_vec;\n        /* shuffled = (data_key << 32) | (data_key >> 32); */\n        xxh_u32x4 const shuffled = (xxh_u32x4)vec_rl(data_key, v32);\n        /* product = ((xxh_u64x2)data_key & 0xFFFFFFFF) * ((xxh_u64x2)shuffled & 0xFFFFFFFF); */\n        xxh_u64x2 const product  = XXH_vec_mulo((xxh_u32x4)data_key, shuffled);\n        xacc[i] += product;\n\n        /* swap high and low halves */\n#ifdef __s390x__\n        xacc[i] += vec_permi(data_vec, data_vec, 2);\n#else\n        xacc[i] += vec_xxpermdi(data_vec, data_vec, 2);\n#endif\n    }\n}\n\nXXH_FORCE_INLINE void\nXXH3_scrambleAcc_vsx(void* XXH_RESTRICT acc, const void* XXH_RESTRICT secret)\n{\n    XXH_ASSERT((((size_t)acc) & 15) == 0);\n\n    {         xxh_u64x2* const xacc    =       (xxh_u64x2*) acc;\n        const xxh_u64x2* const xsecret = (const xxh_u64x2*) secret;\n        /* constants */\n        xxh_u64x2 const v32  = { 32, 32 };\n        xxh_u64x2 const v47 = { 47, 47 };\n        xxh_u32x4 const prime = { XXH_PRIME32_1, XXH_PRIME32_1, XXH_PRIME32_1, XXH_PRIME32_1 };\n        size_t i;\n        for (i = 0; i < XXH_STRIPE_LEN / sizeof(xxh_u64x2); i++) {\n            /* xacc[i] ^= (xacc[i] >> 47); */\n            xxh_u64x2 const acc_vec  = xacc[i];\n            xxh_u64x2 const data_vec = acc_vec ^ (acc_vec >> v47);\n\n            /* xacc[i] ^= xsecret[i]; */\n            xxh_u64x2 const key_vec  = XXH_vec_loadu(xsecret + i);\n            xxh_u64x2 const data_key = data_vec ^ key_vec;\n\n            /* xacc[i] *= XXH_PRIME32_1 */\n            /* prod_lo = ((xxh_u64x2)data_key & 0xFFFFFFFF) * ((xxh_u64x2)prime & 0xFFFFFFFF);  */\n            xxh_u64x2 const prod_even  = XXH_vec_mule((xxh_u32x4)data_key, prime);\n            /* prod_hi = ((xxh_u64x2)data_key >> 32) * ((xxh_u64x2)prime >> 32);  */\n            xxh_u64x2 const prod_odd  = XXH_vec_mulo((xxh_u32x4)data_key, prime);\n            xacc[i] = prod_odd + (prod_even << v32);\n    }   }\n}\n\n#endif\n\n/* scalar variants - universal */\n\nXXH_FORCE_INLINE void\nXXH3_accumulate_512_scalar(void* XXH_RESTRICT acc,\n                     const void* XXH_RESTRICT input,\n                     const void* XXH_RESTRICT secret)\n{\n    XXH_ALIGN(XXH_ACC_ALIGN) xxh_u64* const xacc = (xxh_u64*) acc; /* presumed aligned */\n    const xxh_u8* const xinput  = (const xxh_u8*) input;  /* no alignment restriction */\n    const xxh_u8* const xsecret = (const xxh_u8*) secret;   /* no alignment restriction */\n    size_t i;\n    XXH_ASSERT(((size_t)acc & (XXH_ACC_ALIGN-1)) == 0);\n    for (i=0; i < XXH_ACC_NB; i++) {\n        xxh_u64 const data_val = XXH_readLE64(xinput + 8*i);\n        xxh_u64 const data_key = data_val ^ XXH_readLE64(xsecret + i*8);\n        xacc[i ^ 1] += data_val; /* swap adjacent lanes */\n        xacc[i] += XXH_mult32to64(data_key & 0xFFFFFFFF, data_key >> 32);\n    }\n}\n\nXXH_FORCE_INLINE void\nXXH3_scrambleAcc_scalar(void* XXH_RESTRICT acc, const void* XXH_RESTRICT secret)\n{\n    XXH_ALIGN(XXH_ACC_ALIGN) xxh_u64* const xacc = (xxh_u64*) acc;   /* presumed aligned */\n    const xxh_u8* const xsecret = (const xxh_u8*) secret;   /* no alignment restriction */\n    size_t i;\n    XXH_ASSERT((((size_t)acc) & (XXH_ACC_ALIGN-1)) == 0);\n    for (i=0; i < XXH_ACC_NB; i++) {\n        xxh_u64 const key64 = XXH_readLE64(xsecret + 8*i);\n        xxh_u64 acc64 = xacc[i];\n        acc64 = XXH_xorshift64(acc64, 47);\n        acc64 ^= key64;\n        acc64 *= XXH_PRIME32_1;\n        xacc[i] = acc64;\n    }\n}\n\nXXH_FORCE_INLINE void\nXXH3_initCustomSecret_scalar(void* XXH_RESTRICT customSecret, xxh_u64 seed64)\n{\n    /*\n     * We need a separate pointer for the hack below,\n     * which requires a non-const pointer.\n     * Any decent compiler will optimize this out otherwise.\n     */\n    const xxh_u8* kSecretPtr = XXH3_kSecret;\n    XXH_STATIC_ASSERT((XXH_SECRET_DEFAULT_SIZE & 15) == 0);\n\n#if defined(__clang__) && defined(__aarch64__)\n    /*\n     * UGLY HACK:\n     * Clang generates a bunch of MOV/MOVK pairs for aarch64, and they are\n     * placed sequentially, in order, at the top of the unrolled loop.\n     *\n     * While MOVK is great for generating constants (2 cycles for a 64-bit\n     * constant compared to 4 cycles for LDR), long MOVK chains stall the\n     * integer pipelines:\n     *   I   L   S\n     * MOVK\n     * MOVK\n     * MOVK\n     * MOVK\n     * ADD\n     * SUB      STR\n     *          STR\n     * By forcing loads from memory (as the asm line causes Clang to assume\n     * that XXH3_kSecretPtr has been changed), the pipelines are used more\n     * efficiently:\n     *   I   L   S\n     *      LDR\n     *  ADD LDR\n     *  SUB     STR\n     *          STR\n     * XXH3_64bits_withSeed, len == 256, Snapdragon 835\n     *   without hack: 2654.4 MB/s\n     *   with hack:    3202.9 MB/s\n     */\n    XXH_COMPILER_GUARD(kSecretPtr);\n#endif\n    /*\n     * Note: in debug mode, this overrides the asm optimization\n     * and Clang will emit MOVK chains again.\n     */\n    XXH_ASSERT(kSecretPtr == XXH3_kSecret);\n\n    {   int const nbRounds = XXH_SECRET_DEFAULT_SIZE / 16;\n        int i;\n        for (i=0; i < nbRounds; i++) {\n            /*\n             * The asm hack causes Clang to assume that kSecretPtr aliases with\n             * customSecret, and on aarch64, this prevented LDP from merging two\n             * loads together for free. Putting the loads together before the stores\n             * properly generates LDP.\n             */\n            xxh_u64 lo = XXH_readLE64(kSecretPtr + 16*i)     + seed64;\n            xxh_u64 hi = XXH_readLE64(kSecretPtr + 16*i + 8) - seed64;\n            XXH_writeLE64((xxh_u8*)customSecret + 16*i,     lo);\n            XXH_writeLE64((xxh_u8*)customSecret + 16*i + 8, hi);\n    }   }\n}\n\n\ntypedef void (*XXH3_f_accumulate_512)(void* XXH_RESTRICT, const void*, const void*);\ntypedef void (*XXH3_f_scrambleAcc)(void* XXH_RESTRICT, const void*);\ntypedef void (*XXH3_f_initCustomSecret)(void* XXH_RESTRICT, xxh_u64);\n\n\n#if (XXH_VECTOR == XXH_AVX512)\n\n#define XXH3_accumulate_512 XXH3_accumulate_512_avx512\n#define XXH3_scrambleAcc    XXH3_scrambleAcc_avx512\n#define XXH3_initCustomSecret XXH3_initCustomSecret_avx512\n\n#elif (XXH_VECTOR == XXH_AVX2)\n\n#define XXH3_accumulate_512 XXH3_accumulate_512_avx2\n#define XXH3_scrambleAcc    XXH3_scrambleAcc_avx2\n#define XXH3_initCustomSecret XXH3_initCustomSecret_avx2\n\n#elif (XXH_VECTOR == XXH_SSE2)\n\n#define XXH3_accumulate_512 XXH3_accumulate_512_sse2\n#define XXH3_scrambleAcc    XXH3_scrambleAcc_sse2\n#define XXH3_initCustomSecret XXH3_initCustomSecret_sse2\n\n#elif (XXH_VECTOR == XXH_NEON)\n\n#define XXH3_accumulate_512 XXH3_accumulate_512_neon\n#define XXH3_scrambleAcc    XXH3_scrambleAcc_neon\n#define XXH3_initCustomSecret XXH3_initCustomSecret_scalar\n\n#elif (XXH_VECTOR == XXH_VSX)\n\n#define XXH3_accumulate_512 XXH3_accumulate_512_vsx\n#define XXH3_scrambleAcc    XXH3_scrambleAcc_vsx\n#define XXH3_initCustomSecret XXH3_initCustomSecret_scalar\n\n#else /* scalar */\n\n#define XXH3_accumulate_512 XXH3_accumulate_512_scalar\n#define XXH3_scrambleAcc    XXH3_scrambleAcc_scalar\n#define XXH3_initCustomSecret XXH3_initCustomSecret_scalar\n\n#endif\n\n\n\n#ifndef XXH_PREFETCH_DIST\n#  ifdef __clang__\n#    define XXH_PREFETCH_DIST 320\n#  else\n#    if (XXH_VECTOR == XXH_AVX512)\n#      define XXH_PREFETCH_DIST 512\n#    else\n#      define XXH_PREFETCH_DIST 384\n#    endif\n#  endif  /* __clang__ */\n#endif  /* XXH_PREFETCH_DIST */\n\n/*\n * XXH3_accumulate()\n * Loops over XXH3_accumulate_512().\n * Assumption: nbStripes will not overflow the secret size\n */\nXXH_FORCE_INLINE void\nXXH3_accumulate(     xxh_u64* XXH_RESTRICT acc,\n                const xxh_u8* XXH_RESTRICT input,\n                const xxh_u8* XXH_RESTRICT secret,\n                      size_t nbStripes,\n                      XXH3_f_accumulate_512 f_acc512)\n{\n    size_t n;\n    for (n = 0; n < nbStripes; n++ ) {\n        const xxh_u8* const in = input + n*XXH_STRIPE_LEN;\n        XXH_PREFETCH(in + XXH_PREFETCH_DIST);\n        f_acc512(acc,\n                 in,\n                 secret + n*XXH_SECRET_CONSUME_RATE);\n    }\n}\n\nXXH_FORCE_INLINE void\nXXH3_hashLong_internal_loop(xxh_u64* XXH_RESTRICT acc,\n                      const xxh_u8* XXH_RESTRICT input, size_t len,\n                      const xxh_u8* XXH_RESTRICT secret, size_t secretSize,\n                            XXH3_f_accumulate_512 f_acc512,\n                            XXH3_f_scrambleAcc f_scramble)\n{\n    size_t const nbStripesPerBlock = (secretSize - XXH_STRIPE_LEN) / XXH_SECRET_CONSUME_RATE;\n    size_t const block_len = XXH_STRIPE_LEN * nbStripesPerBlock;\n    size_t const nb_blocks = (len - 1) / block_len;\n\n    size_t n;\n\n    XXH_ASSERT(secretSize >= XXH3_SECRET_SIZE_MIN);\n\n    for (n = 0; n < nb_blocks; n++) {\n        XXH3_accumulate(acc, input + n*block_len, secret, nbStripesPerBlock, f_acc512);\n        f_scramble(acc, secret + secretSize - XXH_STRIPE_LEN);\n    }\n\n    /* last partial block */\n    XXH_ASSERT(len > XXH_STRIPE_LEN);\n    {   size_t const nbStripes = ((len - 1) - (block_len * nb_blocks)) / XXH_STRIPE_LEN;\n        XXH_ASSERT(nbStripes <= (secretSize / XXH_SECRET_CONSUME_RATE));\n        XXH3_accumulate(acc, input + nb_blocks*block_len, secret, nbStripes, f_acc512);\n\n        /* last stripe */\n        {   const xxh_u8* const p = input + len - XXH_STRIPE_LEN;\n#define XXH_SECRET_LASTACC_START 7  /* not aligned on 8, last secret is different from acc & scrambler */\n            f_acc512(acc, p, secret + secretSize - XXH_STRIPE_LEN - XXH_SECRET_LASTACC_START);\n    }   }\n}\n\nXXH_FORCE_INLINE xxh_u64\nXXH3_mix2Accs(const xxh_u64* XXH_RESTRICT acc, const xxh_u8* XXH_RESTRICT secret)\n{\n    return XXH3_mul128_fold64(\n               acc[0] ^ XXH_readLE64(secret),\n               acc[1] ^ XXH_readLE64(secret+8) );\n}\n\nstatic XXH64_hash_t\nXXH3_mergeAccs(const xxh_u64* XXH_RESTRICT acc, const xxh_u8* XXH_RESTRICT secret, xxh_u64 start)\n{\n    xxh_u64 result64 = start;\n    size_t i = 0;\n\n    for (i = 0; i < 4; i++) {\n        result64 += XXH3_mix2Accs(acc+2*i, secret + 16*i);\n#if defined(__clang__)                                /* Clang */ \\\n    && (defined(__arm__) || defined(__thumb__))       /* ARMv7 */ \\\n    && (defined(__ARM_NEON) || defined(__ARM_NEON__)) /* NEON */  \\\n    && !defined(XXH_ENABLE_AUTOVECTORIZE)             /* Define to disable */\n        /*\n         * UGLY HACK:\n         * Prevent autovectorization on Clang ARMv7-a. Exact same problem as\n         * the one in XXH3_len_129to240_64b. Speeds up shorter keys > 240b.\n         * XXH3_64bits, len == 256, Snapdragon 835:\n         *   without hack: 2063.7 MB/s\n         *   with hack:    2560.7 MB/s\n         */\n        XXH_COMPILER_GUARD(result64);\n#endif\n    }\n\n    return XXH3_avalanche(result64);\n}\n\n#define XXH3_INIT_ACC { XXH_PRIME32_3, XXH_PRIME64_1, XXH_PRIME64_2, XXH_PRIME64_3, \\\n                        XXH_PRIME64_4, XXH_PRIME32_2, XXH_PRIME64_5, XXH_PRIME32_1 }\n\nXXH_FORCE_INLINE XXH64_hash_t\nXXH3_hashLong_64b_internal(const void* XXH_RESTRICT input, size_t len,\n                           const void* XXH_RESTRICT secret, size_t secretSize,\n                           XXH3_f_accumulate_512 f_acc512,\n                           XXH3_f_scrambleAcc f_scramble)\n{\n    XXH_ALIGN(XXH_ACC_ALIGN) xxh_u64 acc[XXH_ACC_NB] = XXH3_INIT_ACC;\n\n    XXH3_hashLong_internal_loop(acc, (const xxh_u8*)input, len, (const xxh_u8*)secret, secretSize, f_acc512, f_scramble);\n\n    /* converge into final hash */\n    XXH_STATIC_ASSERT(sizeof(acc) == 64);\n    /* do not align on 8, so that the secret is different from the accumulator */\n#define XXH_SECRET_MERGEACCS_START 11\n    XXH_ASSERT(secretSize >= sizeof(acc) + XXH_SECRET_MERGEACCS_START);\n    return XXH3_mergeAccs(acc, (const xxh_u8*)secret + XXH_SECRET_MERGEACCS_START, (xxh_u64)len * XXH_PRIME64_1);\n}\n\n/*\n * It's important for performance that XXH3_hashLong is not inlined.\n */\nXXH_NO_INLINE XXH64_hash_t\nXXH3_hashLong_64b_withSecret(const void* XXH_RESTRICT input, size_t len,\n                             XXH64_hash_t seed64, const xxh_u8* XXH_RESTRICT secret, size_t secretLen)\n{\n    (void)seed64;\n    return XXH3_hashLong_64b_internal(input, len, secret, secretLen, XXH3_accumulate_512, XXH3_scrambleAcc);\n}\n\n/*\n * It's important for performance that XXH3_hashLong is not inlined.\n * Since the function is not inlined, the compiler may not be able to understand that,\n * in some scenarios, its `secret` argument is actually a compile time constant.\n * This variant enforces that the compiler can detect that,\n * and uses this opportunity to streamline the generated code for better performance.\n */\nXXH_NO_INLINE XXH64_hash_t\nXXH3_hashLong_64b_default(const void* XXH_RESTRICT input, size_t len,\n                          XXH64_hash_t seed64, const xxh_u8* XXH_RESTRICT secret, size_t secretLen)\n{\n    (void)seed64; (void)secret; (void)secretLen;\n    return XXH3_hashLong_64b_internal(input, len, XXH3_kSecret, sizeof(XXH3_kSecret), XXH3_accumulate_512, XXH3_scrambleAcc);\n}\n\n/*\n * XXH3_hashLong_64b_withSeed():\n * Generate a custom key based on alteration of default XXH3_kSecret with the seed,\n * and then use this key for long mode hashing.\n *\n * This operation is decently fast but nonetheless costs a little bit of time.\n * Try to avoid it whenever possible (typically when seed==0).\n *\n * It's important for performance that XXH3_hashLong is not inlined. Not sure\n * why (uop cache maybe?), but the difference is large and easily measurable.\n */\nXXH_FORCE_INLINE XXH64_hash_t\nXXH3_hashLong_64b_withSeed_internal(const void* input, size_t len,\n                                    XXH64_hash_t seed,\n                                    XXH3_f_accumulate_512 f_acc512,\n                                    XXH3_f_scrambleAcc f_scramble,\n                                    XXH3_f_initCustomSecret f_initSec)\n{\n    if (seed == 0)\n        return XXH3_hashLong_64b_internal(input, len,\n                                          XXH3_kSecret, sizeof(XXH3_kSecret),\n                                          f_acc512, f_scramble);\n    {   XXH_ALIGN(XXH_SEC_ALIGN) xxh_u8 secret[XXH_SECRET_DEFAULT_SIZE];\n        f_initSec(secret, seed);\n        return XXH3_hashLong_64b_internal(input, len, secret, sizeof(secret),\n                                          f_acc512, f_scramble);\n    }\n}\n\n/*\n * It's important for performance that XXH3_hashLong is not inlined.\n */\nXXH_NO_INLINE XXH64_hash_t\nXXH3_hashLong_64b_withSeed(const void* input, size_t len,\n                           XXH64_hash_t seed, const xxh_u8* secret, size_t secretLen)\n{\n    (void)secret; (void)secretLen;\n    return XXH3_hashLong_64b_withSeed_internal(input, len, seed,\n                XXH3_accumulate_512, XXH3_scrambleAcc, XXH3_initCustomSecret);\n}\n\n\ntypedef XXH64_hash_t (*XXH3_hashLong64_f)(const void* XXH_RESTRICT, size_t,\n                                          XXH64_hash_t, const xxh_u8* XXH_RESTRICT, size_t);\n\nXXH_FORCE_INLINE XXH64_hash_t\nXXH3_64bits_internal(const void* XXH_RESTRICT input, size_t len,\n                     XXH64_hash_t seed64, const void* XXH_RESTRICT secret, size_t secretLen,\n                     XXH3_hashLong64_f f_hashLong)\n{\n    XXH_ASSERT(secretLen >= XXH3_SECRET_SIZE_MIN);\n    /*\n     * If an action is to be taken if `secretLen` condition is not respected,\n     * it should be done here.\n     * For now, it's a contract pre-condition.\n     * Adding a check and a branch here would cost performance at every hash.\n     * Also, note that function signature doesn't offer room to return an error.\n     */\n    if (len <= 16)\n        return XXH3_len_0to16_64b((const xxh_u8*)input, len, (const xxh_u8*)secret, seed64);\n    if (len <= 128)\n        return XXH3_len_17to128_64b((const xxh_u8*)input, len, (const xxh_u8*)secret, secretLen, seed64);\n    if (len <= XXH3_MIDSIZE_MAX)\n        return XXH3_len_129to240_64b((const xxh_u8*)input, len, (const xxh_u8*)secret, secretLen, seed64);\n    return f_hashLong(input, len, seed64, (const xxh_u8*)secret, secretLen);\n}\n\n\n/* ===   Public entry point   === */\n\n/*! @ingroup xxh3_family */\nXXH_PUBLIC_API XXH64_hash_t XXH3_64bits(const void* input, size_t len)\n{\n    return XXH3_64bits_internal(input, len, 0, XXH3_kSecret, sizeof(XXH3_kSecret), XXH3_hashLong_64b_default);\n}\n\n/*! @ingroup xxh3_family */\nXXH_PUBLIC_API XXH64_hash_t\nXXH3_64bits_withSecret(const void* input, size_t len, const void* secret, size_t secretSize)\n{\n    return XXH3_64bits_internal(input, len, 0, secret, secretSize, XXH3_hashLong_64b_withSecret);\n}\n\n/*! @ingroup xxh3_family */\nXXH_PUBLIC_API XXH64_hash_t\nXXH3_64bits_withSeed(const void* input, size_t len, XXH64_hash_t seed)\n{\n    return XXH3_64bits_internal(input, len, seed, XXH3_kSecret, sizeof(XXH3_kSecret), XXH3_hashLong_64b_withSeed);\n}\n\n\n/* ===   XXH3 streaming   === */\n\n/*\n * Malloc's a pointer that is always aligned to align.\n *\n * This must be freed with `XXH_alignedFree()`.\n *\n * malloc typically guarantees 16 byte alignment on 64-bit systems and 8 byte\n * alignment on 32-bit. This isn't enough for the 32 byte aligned loads in AVX2\n * or on 32-bit, the 16 byte aligned loads in SSE2 and NEON.\n *\n * This underalignment previously caused a rather obvious crash which went\n * completely unnoticed due to XXH3_createState() not actually being tested.\n * Credit to RedSpah for noticing this bug.\n *\n * The alignment is done manually: Functions like posix_memalign or _mm_malloc\n * are avoided: To maintain portability, we would have to write a fallback\n * like this anyways, and besides, testing for the existence of library\n * functions without relying on external build tools is impossible.\n *\n * The method is simple: Overallocate, manually align, and store the offset\n * to the original behind the returned pointer.\n *\n * Align must be a power of 2 and 8 <= align <= 128.\n */\nstatic void* XXH_alignedMalloc(size_t s, size_t align)\n{\n    XXH_ASSERT(align <= 128 && align >= 8); /* range check */\n    XXH_ASSERT((align & (align-1)) == 0);   /* power of 2 */\n    XXH_ASSERT(s != 0 && s < (s + align));  /* empty/overflow */\n    {   /* Overallocate to make room for manual realignment and an offset byte */\n        xxh_u8* base = (xxh_u8*)XXH_malloc(s + align);\n        if (base != NULL) {\n            /*\n             * Get the offset needed to align this pointer.\n             *\n             * Even if the returned pointer is aligned, there will always be\n             * at least one byte to store the offset to the original pointer.\n             */\n            size_t offset = align - ((size_t)base & (align - 1)); /* base % align */\n            /* Add the offset for the now-aligned pointer */\n            xxh_u8* ptr = base + offset;\n\n            XXH_ASSERT((size_t)ptr % align == 0);\n\n            /* Store the offset immediately before the returned pointer. */\n            ptr[-1] = (xxh_u8)offset;\n            return ptr;\n        }\n        return NULL;\n    }\n}\n/*\n * Frees an aligned pointer allocated by XXH_alignedMalloc(). Don't pass\n * normal malloc'd pointers, XXH_alignedMalloc has a specific data layout.\n */\nstatic void XXH_alignedFree(void* p)\n{\n    if (p != NULL) {\n        xxh_u8* ptr = (xxh_u8*)p;\n        /* Get the offset byte we added in XXH_malloc. */\n        xxh_u8 offset = ptr[-1];\n        /* Free the original malloc'd pointer */\n        xxh_u8* base = ptr - offset;\n        XXH_free(base);\n    }\n}\n/*! @ingroup xxh3_family */\nXXH_PUBLIC_API XXH3_state_t* XXH3_createState(void)\n{\n    XXH3_state_t* const state = (XXH3_state_t*)XXH_alignedMalloc(sizeof(XXH3_state_t), 64);\n    if (state==NULL) return NULL;\n    XXH3_INITSTATE(state);\n    return state;\n}\n\n/*! @ingroup xxh3_family */\nXXH_PUBLIC_API XXH_errorcode XXH3_freeState(XXH3_state_t* statePtr)\n{\n    XXH_alignedFree(statePtr);\n    return XXH_OK;\n}\n\n/*! @ingroup xxh3_family */\nXXH_PUBLIC_API void\nXXH3_copyState(XXH3_state_t* dst_state, const XXH3_state_t* src_state)\n{\n    memcpy(dst_state, src_state, sizeof(*dst_state));\n}\n\nstatic void\nXXH3_reset_internal(XXH3_state_t* statePtr,\n                           XXH64_hash_t seed,\n                           const void* secret, size_t secretSize)\n{\n    size_t const initStart = offsetof(XXH3_state_t, bufferedSize);\n    size_t const initLength = offsetof(XXH3_state_t, nbStripesPerBlock) - initStart;\n    XXH_ASSERT(offsetof(XXH3_state_t, nbStripesPerBlock) > initStart);\n    XXH_ASSERT(statePtr != NULL);\n    /* set members from bufferedSize to nbStripesPerBlock (excluded) to 0 */\n    memset((char*)statePtr + initStart, 0, initLength);\n    statePtr->acc[0] = XXH_PRIME32_3;\n    statePtr->acc[1] = XXH_PRIME64_1;\n    statePtr->acc[2] = XXH_PRIME64_2;\n    statePtr->acc[3] = XXH_PRIME64_3;\n    statePtr->acc[4] = XXH_PRIME64_4;\n    statePtr->acc[5] = XXH_PRIME32_2;\n    statePtr->acc[6] = XXH_PRIME64_5;\n    statePtr->acc[7] = XXH_PRIME32_1;\n    statePtr->seed = seed;\n    statePtr->extSecret = (const unsigned char*)secret;\n    XXH_ASSERT(secretSize >= XXH3_SECRET_SIZE_MIN);\n    statePtr->secretLimit = secretSize - XXH_STRIPE_LEN;\n    statePtr->nbStripesPerBlock = statePtr->secretLimit / XXH_SECRET_CONSUME_RATE;\n}\n\n/*! @ingroup xxh3_family */\nXXH_PUBLIC_API XXH_errorcode\nXXH3_64bits_reset(XXH3_state_t* statePtr)\n{\n    if (statePtr == NULL) return XXH_ERROR;\n    XXH3_reset_internal(statePtr, 0, XXH3_kSecret, XXH_SECRET_DEFAULT_SIZE);\n    return XXH_OK;\n}\n\n/*! @ingroup xxh3_family */\nXXH_PUBLIC_API XXH_errorcode\nXXH3_64bits_reset_withSecret(XXH3_state_t* statePtr, const void* secret, size_t secretSize)\n{\n    if (statePtr == NULL) return XXH_ERROR;\n    XXH3_reset_internal(statePtr, 0, secret, secretSize);\n    if (secret == NULL) return XXH_ERROR;\n    if (secretSize < XXH3_SECRET_SIZE_MIN) return XXH_ERROR;\n    return XXH_OK;\n}\n\n/*! @ingroup xxh3_family */\nXXH_PUBLIC_API XXH_errorcode\nXXH3_64bits_reset_withSeed(XXH3_state_t* statePtr, XXH64_hash_t seed)\n{\n    if (statePtr == NULL) return XXH_ERROR;\n    if (seed==0) return XXH3_64bits_reset(statePtr);\n    if (seed != statePtr->seed) XXH3_initCustomSecret(statePtr->customSecret, seed);\n    XXH3_reset_internal(statePtr, seed, NULL, XXH_SECRET_DEFAULT_SIZE);\n    return XXH_OK;\n}\n\n/* Note : when XXH3_consumeStripes() is invoked,\n * there must be a guarantee that at least one more byte must be consumed from input\n * so that the function can blindly consume all stripes using the \"normal\" secret segment */\nXXH_FORCE_INLINE void\nXXH3_consumeStripes(xxh_u64* XXH_RESTRICT acc,\n                    size_t* XXH_RESTRICT nbStripesSoFarPtr, size_t nbStripesPerBlock,\n                    const xxh_u8* XXH_RESTRICT input, size_t nbStripes,\n                    const xxh_u8* XXH_RESTRICT secret, size_t secretLimit,\n                    XXH3_f_accumulate_512 f_acc512,\n                    XXH3_f_scrambleAcc f_scramble)\n{\n    XXH_ASSERT(nbStripes <= nbStripesPerBlock);  /* can handle max 1 scramble per invocation */\n    XXH_ASSERT(*nbStripesSoFarPtr < nbStripesPerBlock);\n    if (nbStripesPerBlock - *nbStripesSoFarPtr <= nbStripes) {\n        /* need a scrambling operation */\n        size_t const nbStripesToEndofBlock = nbStripesPerBlock - *nbStripesSoFarPtr;\n        size_t const nbStripesAfterBlock = nbStripes - nbStripesToEndofBlock;\n        XXH3_accumulate(acc, input, secret + nbStripesSoFarPtr[0] * XXH_SECRET_CONSUME_RATE, nbStripesToEndofBlock, f_acc512);\n        f_scramble(acc, secret + secretLimit);\n        XXH3_accumulate(acc, input + nbStripesToEndofBlock * XXH_STRIPE_LEN, secret, nbStripesAfterBlock, f_acc512);\n        *nbStripesSoFarPtr = nbStripesAfterBlock;\n    } else {\n        XXH3_accumulate(acc, input, secret + nbStripesSoFarPtr[0] * XXH_SECRET_CONSUME_RATE, nbStripes, f_acc512);\n        *nbStripesSoFarPtr += nbStripes;\n    }\n}\n\n/*\n * Both XXH3_64bits_update and XXH3_128bits_update use this routine.\n */\nXXH_FORCE_INLINE XXH_errorcode\nXXH3_update(XXH3_state_t* state,\n            const xxh_u8* input, size_t len,\n            XXH3_f_accumulate_512 f_acc512,\n            XXH3_f_scrambleAcc f_scramble)\n{\n    if (input==NULL)\n#if defined(XXH_ACCEPT_NULL_INPUT_POINTER) && (XXH_ACCEPT_NULL_INPUT_POINTER>=1)\n        return XXH_OK;\n#else\n        return XXH_ERROR;\n#endif\n\n    {   const xxh_u8* const bEnd = input + len;\n        const unsigned char* const secret = (state->extSecret == NULL) ? state->customSecret : state->extSecret;\n\n        state->totalLen += len;\n        XXH_ASSERT(state->bufferedSize <= XXH3_INTERNALBUFFER_SIZE);\n\n        if (state->bufferedSize + len <= XXH3_INTERNALBUFFER_SIZE) {  /* fill in tmp buffer */\n            XXH_memcpy(state->buffer + state->bufferedSize, input, len);\n            state->bufferedSize += (XXH32_hash_t)len;\n            return XXH_OK;\n        }\n        /* total input is now > XXH3_INTERNALBUFFER_SIZE */\n\n        #define XXH3_INTERNALBUFFER_STRIPES (XXH3_INTERNALBUFFER_SIZE / XXH_STRIPE_LEN)\n        XXH_STATIC_ASSERT(XXH3_INTERNALBUFFER_SIZE % XXH_STRIPE_LEN == 0);   /* clean multiple */\n\n        /*\n         * Internal buffer is partially filled (always, except at beginning)\n         * Complete it, then consume it.\n         */\n        if (state->bufferedSize) {\n            size_t const loadSize = XXH3_INTERNALBUFFER_SIZE - state->bufferedSize;\n            XXH_memcpy(state->buffer + state->bufferedSize, input, loadSize);\n            input += loadSize;\n            XXH3_consumeStripes(state->acc,\n                               &state->nbStripesSoFar, state->nbStripesPerBlock,\n                                state->buffer, XXH3_INTERNALBUFFER_STRIPES,\n                                secret, state->secretLimit,\n                                f_acc512, f_scramble);\n            state->bufferedSize = 0;\n        }\n        XXH_ASSERT(input < bEnd);\n\n        /* Consume input by a multiple of internal buffer size */\n        if (input+XXH3_INTERNALBUFFER_SIZE < bEnd) {\n            const xxh_u8* const limit = bEnd - XXH3_INTERNALBUFFER_SIZE;\n            do {\n                XXH3_consumeStripes(state->acc,\n                                   &state->nbStripesSoFar, state->nbStripesPerBlock,\n                                    input, XXH3_INTERNALBUFFER_STRIPES,\n                                    secret, state->secretLimit,\n                                    f_acc512, f_scramble);\n                input += XXH3_INTERNALBUFFER_SIZE;\n            } while (input<limit);\n            /* for last partial stripe */\n            memcpy(state->buffer + sizeof(state->buffer) - XXH_STRIPE_LEN, input - XXH_STRIPE_LEN, XXH_STRIPE_LEN);\n        }\n        XXH_ASSERT(input < bEnd);\n\n        /* Some remaining input (always) : buffer it */\n        XXH_memcpy(state->buffer, input, (size_t)(bEnd-input));\n        state->bufferedSize = (XXH32_hash_t)(bEnd-input);\n    }\n\n    return XXH_OK;\n}\n\n/*! @ingroup xxh3_family */\nXXH_PUBLIC_API XXH_errorcode\nXXH3_64bits_update(XXH3_state_t* state, const void* input, size_t len)\n{\n    return XXH3_update(state, (const xxh_u8*)input, len,\n                       XXH3_accumulate_512, XXH3_scrambleAcc);\n}\n\n\nXXH_FORCE_INLINE void\nXXH3_digest_long (XXH64_hash_t* acc,\n                  const XXH3_state_t* state,\n                  const unsigned char* secret)\n{\n    /*\n     * Digest on a local copy. This way, the state remains unaltered, and it can\n     * continue ingesting more input afterwards.\n     */\n    memcpy(acc, state->acc, sizeof(state->acc));\n    if (state->bufferedSize >= XXH_STRIPE_LEN) {\n        size_t const nbStripes = (state->bufferedSize - 1) / XXH_STRIPE_LEN;\n        size_t nbStripesSoFar = state->nbStripesSoFar;\n        XXH3_consumeStripes(acc,\n                           &nbStripesSoFar, state->nbStripesPerBlock,\n                            state->buffer, nbStripes,\n                            secret, state->secretLimit,\n                            XXH3_accumulate_512, XXH3_scrambleAcc);\n        /* last stripe */\n        XXH3_accumulate_512(acc,\n                            state->buffer + state->bufferedSize - XXH_STRIPE_LEN,\n                            secret + state->secretLimit - XXH_SECRET_LASTACC_START);\n    } else {  /* bufferedSize < XXH_STRIPE_LEN */\n        xxh_u8 lastStripe[XXH_STRIPE_LEN];\n        size_t const catchupSize = XXH_STRIPE_LEN - state->bufferedSize;\n        XXH_ASSERT(state->bufferedSize > 0);  /* there is always some input buffered */\n        memcpy(lastStripe, state->buffer + sizeof(state->buffer) - catchupSize, catchupSize);\n        memcpy(lastStripe + catchupSize, state->buffer, state->bufferedSize);\n        XXH3_accumulate_512(acc,\n                            lastStripe,\n                            secret + state->secretLimit - XXH_SECRET_LASTACC_START);\n    }\n}\n\n/*! @ingroup xxh3_family */\nXXH_PUBLIC_API XXH64_hash_t XXH3_64bits_digest (const XXH3_state_t* state)\n{\n    const unsigned char* const secret = (state->extSecret == NULL) ? state->customSecret : state->extSecret;\n    if (state->totalLen > XXH3_MIDSIZE_MAX) {\n        XXH_ALIGN(XXH_ACC_ALIGN) XXH64_hash_t acc[XXH_ACC_NB];\n        XXH3_digest_long(acc, state, secret);\n        return XXH3_mergeAccs(acc,\n                              secret + XXH_SECRET_MERGEACCS_START,\n                              (xxh_u64)state->totalLen * XXH_PRIME64_1);\n    }\n    /* totalLen <= XXH3_MIDSIZE_MAX: digesting a short input */\n    if (state->seed)\n        return XXH3_64bits_withSeed(state->buffer, (size_t)state->totalLen, state->seed);\n    return XXH3_64bits_withSecret(state->buffer, (size_t)(state->totalLen),\n                                  secret, state->secretLimit + XXH_STRIPE_LEN);\n}\n\n\n#define XXH_MIN(x, y) (((x) > (y)) ? (y) : (x))\n\n/*! @ingroup xxh3_family */\nXXH_PUBLIC_API void\nXXH3_generateSecret(void* secretBuffer, const void* customSeed, size_t customSeedSize)\n{\n    XXH_ASSERT(secretBuffer != NULL);\n    if (customSeedSize == 0) {\n        memcpy(secretBuffer, XXH3_kSecret, XXH_SECRET_DEFAULT_SIZE);\n        return;\n    }\n    XXH_ASSERT(customSeed != NULL);\n\n    {   size_t const segmentSize = sizeof(XXH128_hash_t);\n        size_t const nbSegments = XXH_SECRET_DEFAULT_SIZE / segmentSize;\n        XXH128_canonical_t scrambler;\n        XXH64_hash_t seeds[12];\n        size_t segnb;\n        XXH_ASSERT(nbSegments == 12);\n        XXH_ASSERT(segmentSize * nbSegments == XXH_SECRET_DEFAULT_SIZE); /* exact multiple */\n        XXH128_canonicalFromHash(&scrambler, XXH128(customSeed, customSeedSize, 0));\n\n        /*\n        * Copy customSeed to seeds[], truncating or repeating as necessary.\n        */\n        {   size_t toFill = XXH_MIN(customSeedSize, sizeof(seeds));\n            size_t filled = toFill;\n            memcpy(seeds, customSeed, toFill);\n            while (filled < sizeof(seeds)) {\n                toFill = XXH_MIN(filled, sizeof(seeds) - filled);\n                memcpy((char*)seeds + filled, seeds, toFill);\n                filled += toFill;\n        }   }\n\n        /* generate secret */\n        memcpy(secretBuffer, &scrambler, sizeof(scrambler));\n        for (segnb=1; segnb < nbSegments; segnb++) {\n            size_t const segmentStart = segnb * segmentSize;\n            XXH128_canonical_t segment;\n            XXH128_canonicalFromHash(&segment,\n                XXH128(&scrambler, sizeof(scrambler), XXH_readLE64(seeds + segnb) + segnb) );\n            memcpy((char*)secretBuffer + segmentStart, &segment, sizeof(segment));\n    }   }\n}\n\n\n/* ==========================================\n * XXH3 128 bits (a.k.a XXH128)\n * ==========================================\n * XXH3's 128-bit variant has better mixing and strength than the 64-bit variant,\n * even without counting the significantly larger output size.\n *\n * For example, extra steps are taken to avoid the seed-dependent collisions\n * in 17-240 byte inputs (See XXH3_mix16B and XXH128_mix32B).\n *\n * This strength naturally comes at the cost of some speed, especially on short\n * lengths. Note that longer hashes are about as fast as the 64-bit version\n * due to it using only a slight modification of the 64-bit loop.\n *\n * XXH128 is also more oriented towards 64-bit machines. It is still extremely\n * fast for a _128-bit_ hash on 32-bit (it usually clears XXH64).\n */\n\nXXH_FORCE_INLINE XXH128_hash_t\nXXH3_len_1to3_128b(const xxh_u8* input, size_t len, const xxh_u8* secret, XXH64_hash_t seed)\n{\n    /* A doubled version of 1to3_64b with different constants. */\n    XXH_ASSERT(input != NULL);\n    XXH_ASSERT(1 <= len && len <= 3);\n    XXH_ASSERT(secret != NULL);\n    /*\n     * len = 1: combinedl = { input[0], 0x01, input[0], input[0] }\n     * len = 2: combinedl = { input[1], 0x02, input[0], input[1] }\n     * len = 3: combinedl = { input[2], 0x03, input[0], input[1] }\n     */\n    {   xxh_u8 const c1 = input[0];\n        xxh_u8 const c2 = input[len >> 1];\n        xxh_u8 const c3 = input[len - 1];\n        xxh_u32 const combinedl = ((xxh_u32)c1 <<16) | ((xxh_u32)c2 << 24)\n                                | ((xxh_u32)c3 << 0) | ((xxh_u32)len << 8);\n        xxh_u32 const combinedh = XXH_rotl32(XXH_swap32(combinedl), 13);\n        xxh_u64 const bitflipl = (XXH_readLE32(secret) ^ XXH_readLE32(secret+4)) + seed;\n        xxh_u64 const bitfliph = (XXH_readLE32(secret+8) ^ XXH_readLE32(secret+12)) - seed;\n        xxh_u64 const keyed_lo = (xxh_u64)combinedl ^ bitflipl;\n        xxh_u64 const keyed_hi = (xxh_u64)combinedh ^ bitfliph;\n        XXH128_hash_t h128;\n        h128.low64  = XXH64_avalanche(keyed_lo);\n        h128.high64 = XXH64_avalanche(keyed_hi);\n        return h128;\n    }\n}\n\nXXH_FORCE_INLINE XXH128_hash_t\nXXH3_len_4to8_128b(const xxh_u8* input, size_t len, const xxh_u8* secret, XXH64_hash_t seed)\n{\n    XXH_ASSERT(input != NULL);\n    XXH_ASSERT(secret != NULL);\n    XXH_ASSERT(4 <= len && len <= 8);\n    seed ^= (xxh_u64)XXH_swap32((xxh_u32)seed) << 32;\n    {   xxh_u32 const input_lo = XXH_readLE32(input);\n        xxh_u32 const input_hi = XXH_readLE32(input + len - 4);\n        xxh_u64 const input_64 = input_lo + ((xxh_u64)input_hi << 32);\n        xxh_u64 const bitflip = (XXH_readLE64(secret+16) ^ XXH_readLE64(secret+24)) + seed;\n        xxh_u64 const keyed = input_64 ^ bitflip;\n\n        /* Shift len to the left to ensure it is even, this avoids even multiplies. */\n        XXH128_hash_t m128 = XXH_mult64to128(keyed, XXH_PRIME64_1 + (len << 2));\n\n        m128.high64 += (m128.low64 << 1);\n        m128.low64  ^= (m128.high64 >> 3);\n\n        m128.low64   = XXH_xorshift64(m128.low64, 35);\n        m128.low64  *= 0x9FB21C651E98DF25ULL;\n        m128.low64   = XXH_xorshift64(m128.low64, 28);\n        m128.high64  = XXH3_avalanche(m128.high64);\n        return m128;\n    }\n}\n\nXXH_FORCE_INLINE XXH128_hash_t\nXXH3_len_9to16_128b(const xxh_u8* input, size_t len, const xxh_u8* secret, XXH64_hash_t seed)\n{\n    XXH_ASSERT(input != NULL);\n    XXH_ASSERT(secret != NULL);\n    XXH_ASSERT(9 <= len && len <= 16);\n    {   xxh_u64 const bitflipl = (XXH_readLE64(secret+32) ^ XXH_readLE64(secret+40)) - seed;\n        xxh_u64 const bitfliph = (XXH_readLE64(secret+48) ^ XXH_readLE64(secret+56)) + seed;\n        xxh_u64 const input_lo = XXH_readLE64(input);\n        xxh_u64       input_hi = XXH_readLE64(input + len - 8);\n        XXH128_hash_t m128 = XXH_mult64to128(input_lo ^ input_hi ^ bitflipl, XXH_PRIME64_1);\n        /*\n         * Put len in the middle of m128 to ensure that the length gets mixed to\n         * both the low and high bits in the 128x64 multiply below.\n         */\n        m128.low64 += (xxh_u64)(len - 1) << 54;\n        input_hi   ^= bitfliph;\n        /*\n         * Add the high 32 bits of input_hi to the high 32 bits of m128, then\n         * add the long product of the low 32 bits of input_hi and XXH_PRIME32_2 to\n         * the high 64 bits of m128.\n         *\n         * The best approach to this operation is different on 32-bit and 64-bit.\n         */\n        if (sizeof(void *) < sizeof(xxh_u64)) { /* 32-bit */\n            /*\n             * 32-bit optimized version, which is more readable.\n             *\n             * On 32-bit, it removes an ADC and delays a dependency between the two\n             * halves of m128.high64, but it generates an extra mask on 64-bit.\n             */\n            m128.high64 += (input_hi & 0xFFFFFFFF00000000ULL) + XXH_mult32to64((xxh_u32)input_hi, XXH_PRIME32_2);\n        } else {\n            /*\n             * 64-bit optimized (albeit more confusing) version.\n             *\n             * Uses some properties of addition and multiplication to remove the mask:\n             *\n             * Let:\n             *    a = input_hi.lo = (input_hi & 0x00000000FFFFFFFF)\n             *    b = input_hi.hi = (input_hi & 0xFFFFFFFF00000000)\n             *    c = XXH_PRIME32_2\n             *\n             *    a + (b * c)\n             * Inverse Property: x + y - x == y\n             *    a + (b * (1 + c - 1))\n             * Distributive Property: x * (y + z) == (x * y) + (x * z)\n             *    a + (b * 1) + (b * (c - 1))\n             * Identity Property: x * 1 == x\n             *    a + b + (b * (c - 1))\n             *\n             * Substitute a, b, and c:\n             *    input_hi.hi + input_hi.lo + ((xxh_u64)input_hi.lo * (XXH_PRIME32_2 - 1))\n             *\n             * Since input_hi.hi + input_hi.lo == input_hi, we get this:\n             *    input_hi + ((xxh_u64)input_hi.lo * (XXH_PRIME32_2 - 1))\n             */\n            m128.high64 += input_hi + XXH_mult32to64((xxh_u32)input_hi, XXH_PRIME32_2 - 1);\n        }\n        /* m128 ^= XXH_swap64(m128 >> 64); */\n        m128.low64  ^= XXH_swap64(m128.high64);\n\n        {   /* 128x64 multiply: h128 = m128 * XXH_PRIME64_2; */\n            XXH128_hash_t h128 = XXH_mult64to128(m128.low64, XXH_PRIME64_2);\n            h128.high64 += m128.high64 * XXH_PRIME64_2;\n\n            h128.low64   = XXH3_avalanche(h128.low64);\n            h128.high64  = XXH3_avalanche(h128.high64);\n            return h128;\n    }   }\n}\n\n/*\n * Assumption: `secret` size is >= XXH3_SECRET_SIZE_MIN\n */\nXXH_FORCE_INLINE XXH128_hash_t\nXXH3_len_0to16_128b(const xxh_u8* input, size_t len, const xxh_u8* secret, XXH64_hash_t seed)\n{\n    XXH_ASSERT(len <= 16);\n    {   if (len > 8) return XXH3_len_9to16_128b(input, len, secret, seed);\n        if (len >= 4) return XXH3_len_4to8_128b(input, len, secret, seed);\n        if (len) return XXH3_len_1to3_128b(input, len, secret, seed);\n        {   XXH128_hash_t h128;\n            xxh_u64 const bitflipl = XXH_readLE64(secret+64) ^ XXH_readLE64(secret+72);\n            xxh_u64 const bitfliph = XXH_readLE64(secret+80) ^ XXH_readLE64(secret+88);\n            h128.low64 = XXH64_avalanche(seed ^ bitflipl);\n            h128.high64 = XXH64_avalanche( seed ^ bitfliph);\n            return h128;\n    }   }\n}\n\n/*\n * A bit slower than XXH3_mix16B, but handles multiply by zero better.\n */\nXXH_FORCE_INLINE XXH128_hash_t\nXXH128_mix32B(XXH128_hash_t acc, const xxh_u8* input_1, const xxh_u8* input_2,\n              const xxh_u8* secret, XXH64_hash_t seed)\n{\n    acc.low64  += XXH3_mix16B (input_1, secret+0, seed);\n    acc.low64  ^= XXH_readLE64(input_2) + XXH_readLE64(input_2 + 8);\n    acc.high64 += XXH3_mix16B (input_2, secret+16, seed);\n    acc.high64 ^= XXH_readLE64(input_1) + XXH_readLE64(input_1 + 8);\n    return acc;\n}\n\n\nXXH_FORCE_INLINE XXH128_hash_t\nXXH3_len_17to128_128b(const xxh_u8* XXH_RESTRICT input, size_t len,\n                      const xxh_u8* XXH_RESTRICT secret, size_t secretSize,\n                      XXH64_hash_t seed)\n{\n    XXH_ASSERT(secretSize >= XXH3_SECRET_SIZE_MIN); (void)secretSize;\n    XXH_ASSERT(16 < len && len <= 128);\n\n    {   XXH128_hash_t acc;\n        acc.low64 = len * XXH_PRIME64_1;\n        acc.high64 = 0;\n        if (len > 32) {\n            if (len > 64) {\n                if (len > 96) {\n                    acc = XXH128_mix32B(acc, input+48, input+len-64, secret+96, seed);\n                }\n                acc = XXH128_mix32B(acc, input+32, input+len-48, secret+64, seed);\n            }\n            acc = XXH128_mix32B(acc, input+16, input+len-32, secret+32, seed);\n        }\n        acc = XXH128_mix32B(acc, input, input+len-16, secret, seed);\n        {   XXH128_hash_t h128;\n            h128.low64  = acc.low64 + acc.high64;\n            h128.high64 = (acc.low64    * XXH_PRIME64_1)\n                        + (acc.high64   * XXH_PRIME64_4)\n                        + ((len - seed) * XXH_PRIME64_2);\n            h128.low64  = XXH3_avalanche(h128.low64);\n            h128.high64 = (XXH64_hash_t)0 - XXH3_avalanche(h128.high64);\n            return h128;\n        }\n    }\n}\n\nXXH_NO_INLINE XXH128_hash_t\nXXH3_len_129to240_128b(const xxh_u8* XXH_RESTRICT input, size_t len,\n                       const xxh_u8* XXH_RESTRICT secret, size_t secretSize,\n                       XXH64_hash_t seed)\n{\n    XXH_ASSERT(secretSize >= XXH3_SECRET_SIZE_MIN); (void)secretSize;\n    XXH_ASSERT(128 < len && len <= XXH3_MIDSIZE_MAX);\n\n    {   XXH128_hash_t acc;\n        int const nbRounds = (int)len / 32;\n        int i;\n        acc.low64 = len * XXH_PRIME64_1;\n        acc.high64 = 0;\n        for (i=0; i<4; i++) {\n            acc = XXH128_mix32B(acc,\n                                input  + (32 * i),\n                                input  + (32 * i) + 16,\n                                secret + (32 * i),\n                                seed);\n        }\n        acc.low64 = XXH3_avalanche(acc.low64);\n        acc.high64 = XXH3_avalanche(acc.high64);\n        XXH_ASSERT(nbRounds >= 4);\n        for (i=4 ; i < nbRounds; i++) {\n            acc = XXH128_mix32B(acc,\n                                input + (32 * i),\n                                input + (32 * i) + 16,\n                                secret + XXH3_MIDSIZE_STARTOFFSET + (32 * (i - 4)),\n                                seed);\n        }\n        /* last bytes */\n        acc = XXH128_mix32B(acc,\n                            input + len - 16,\n                            input + len - 32,\n                            secret + XXH3_SECRET_SIZE_MIN - XXH3_MIDSIZE_LASTOFFSET - 16,\n                            0ULL - seed);\n\n        {   XXH128_hash_t h128;\n            h128.low64  = acc.low64 + acc.high64;\n            h128.high64 = (acc.low64    * XXH_PRIME64_1)\n                        + (acc.high64   * XXH_PRIME64_4)\n                        + ((len - seed) * XXH_PRIME64_2);\n            h128.low64  = XXH3_avalanche(h128.low64);\n            h128.high64 = (XXH64_hash_t)0 - XXH3_avalanche(h128.high64);\n            return h128;\n        }\n    }\n}\n\nXXH_FORCE_INLINE XXH128_hash_t\nXXH3_hashLong_128b_internal(const void* XXH_RESTRICT input, size_t len,\n                            const xxh_u8* XXH_RESTRICT secret, size_t secretSize,\n                            XXH3_f_accumulate_512 f_acc512,\n                            XXH3_f_scrambleAcc f_scramble)\n{\n    XXH_ALIGN(XXH_ACC_ALIGN) xxh_u64 acc[XXH_ACC_NB] = XXH3_INIT_ACC;\n\n    XXH3_hashLong_internal_loop(acc, (const xxh_u8*)input, len, secret, secretSize, f_acc512, f_scramble);\n\n    /* converge into final hash */\n    XXH_STATIC_ASSERT(sizeof(acc) == 64);\n    XXH_ASSERT(secretSize >= sizeof(acc) + XXH_SECRET_MERGEACCS_START);\n    {   XXH128_hash_t h128;\n        h128.low64  = XXH3_mergeAccs(acc,\n                                     secret + XXH_SECRET_MERGEACCS_START,\n                                     (xxh_u64)len * XXH_PRIME64_1);\n        h128.high64 = XXH3_mergeAccs(acc,\n                                     secret + secretSize\n                                            - sizeof(acc) - XXH_SECRET_MERGEACCS_START,\n                                     ~((xxh_u64)len * XXH_PRIME64_2));\n        return h128;\n    }\n}\n\n/*\n * It's important for performance that XXH3_hashLong is not inlined.\n */\nXXH_NO_INLINE XXH128_hash_t\nXXH3_hashLong_128b_default(const void* XXH_RESTRICT input, size_t len,\n                           XXH64_hash_t seed64,\n                           const void* XXH_RESTRICT secret, size_t secretLen)\n{\n    (void)seed64; (void)secret; (void)secretLen;\n    return XXH3_hashLong_128b_internal(input, len, XXH3_kSecret, sizeof(XXH3_kSecret),\n                                       XXH3_accumulate_512, XXH3_scrambleAcc);\n}\n\n/*\n * It's important for performance that XXH3_hashLong is not inlined.\n */\nXXH_NO_INLINE XXH128_hash_t\nXXH3_hashLong_128b_withSecret(const void* XXH_RESTRICT input, size_t len,\n                              XXH64_hash_t seed64,\n                              const void* XXH_RESTRICT secret, size_t secretLen)\n{\n    (void)seed64;\n    return XXH3_hashLong_128b_internal(input, len, (const xxh_u8*)secret, secretLen,\n                                       XXH3_accumulate_512, XXH3_scrambleAcc);\n}\n\nXXH_FORCE_INLINE XXH128_hash_t\nXXH3_hashLong_128b_withSeed_internal(const void* XXH_RESTRICT input, size_t len,\n                                XXH64_hash_t seed64,\n                                XXH3_f_accumulate_512 f_acc512,\n                                XXH3_f_scrambleAcc f_scramble,\n                                XXH3_f_initCustomSecret f_initSec)\n{\n    if (seed64 == 0)\n        return XXH3_hashLong_128b_internal(input, len,\n                                           XXH3_kSecret, sizeof(XXH3_kSecret),\n                                           f_acc512, f_scramble);\n    {   XXH_ALIGN(XXH_SEC_ALIGN) xxh_u8 secret[XXH_SECRET_DEFAULT_SIZE];\n        f_initSec(secret, seed64);\n        return XXH3_hashLong_128b_internal(input, len, (const xxh_u8*)secret, sizeof(secret),\n                                           f_acc512, f_scramble);\n    }\n}\n\n/*\n * It's important for performance that XXH3_hashLong is not inlined.\n */\nXXH_NO_INLINE XXH128_hash_t\nXXH3_hashLong_128b_withSeed(const void* input, size_t len,\n                            XXH64_hash_t seed64, const void* XXH_RESTRICT secret, size_t secretLen)\n{\n    (void)secret; (void)secretLen;\n    return XXH3_hashLong_128b_withSeed_internal(input, len, seed64,\n                XXH3_accumulate_512, XXH3_scrambleAcc, XXH3_initCustomSecret);\n}\n\ntypedef XXH128_hash_t (*XXH3_hashLong128_f)(const void* XXH_RESTRICT, size_t,\n                                            XXH64_hash_t, const void* XXH_RESTRICT, size_t);\n\nXXH_FORCE_INLINE XXH128_hash_t\nXXH3_128bits_internal(const void* input, size_t len,\n                      XXH64_hash_t seed64, const void* XXH_RESTRICT secret, size_t secretLen,\n                      XXH3_hashLong128_f f_hl128)\n{\n    XXH_ASSERT(secretLen >= XXH3_SECRET_SIZE_MIN);\n    /*\n     * If an action is to be taken if `secret` conditions are not respected,\n     * it should be done here.\n     * For now, it's a contract pre-condition.\n     * Adding a check and a branch here would cost performance at every hash.\n     */\n    if (len <= 16)\n        return XXH3_len_0to16_128b((const xxh_u8*)input, len, (const xxh_u8*)secret, seed64);\n    if (len <= 128)\n        return XXH3_len_17to128_128b((const xxh_u8*)input, len, (const xxh_u8*)secret, secretLen, seed64);\n    if (len <= XXH3_MIDSIZE_MAX)\n        return XXH3_len_129to240_128b((const xxh_u8*)input, len, (const xxh_u8*)secret, secretLen, seed64);\n    return f_hl128(input, len, seed64, secret, secretLen);\n}\n\n\n/* ===   Public XXH128 API   === */\n\n/*! @ingroup xxh3_family */\nXXH_PUBLIC_API XXH128_hash_t XXH3_128bits(const void* input, size_t len)\n{\n    return XXH3_128bits_internal(input, len, 0,\n                                 XXH3_kSecret, sizeof(XXH3_kSecret),\n                                 XXH3_hashLong_128b_default);\n}\n\n/*! @ingroup xxh3_family */\nXXH_PUBLIC_API XXH128_hash_t\nXXH3_128bits_withSecret(const void* input, size_t len, const void* secret, size_t secretSize)\n{\n    return XXH3_128bits_internal(input, len, 0,\n                                 (const xxh_u8*)secret, secretSize,\n                                 XXH3_hashLong_128b_withSecret);\n}\n\n/*! @ingroup xxh3_family */\nXXH_PUBLIC_API XXH128_hash_t\nXXH3_128bits_withSeed(const void* input, size_t len, XXH64_hash_t seed)\n{\n    return XXH3_128bits_internal(input, len, seed,\n                                 XXH3_kSecret, sizeof(XXH3_kSecret),\n                                 XXH3_hashLong_128b_withSeed);\n}\n\n/*! @ingroup xxh3_family */\nXXH_PUBLIC_API XXH128_hash_t\nXXH128(const void* input, size_t len, XXH64_hash_t seed)\n{\n    return XXH3_128bits_withSeed(input, len, seed);\n}\n\n\n/* ===   XXH3 128-bit streaming   === */\n\n/*\n * All the functions are actually the same as for 64-bit streaming variant.\n * The only difference is the finalization routine.\n */\n\n/*! @ingroup xxh3_family */\nXXH_PUBLIC_API XXH_errorcode\nXXH3_128bits_reset(XXH3_state_t* statePtr)\n{\n    if (statePtr == NULL) return XXH_ERROR;\n    XXH3_reset_internal(statePtr, 0, XXH3_kSecret, XXH_SECRET_DEFAULT_SIZE);\n    return XXH_OK;\n}\n\n/*! @ingroup xxh3_family */\nXXH_PUBLIC_API XXH_errorcode\nXXH3_128bits_reset_withSecret(XXH3_state_t* statePtr, const void* secret, size_t secretSize)\n{\n    if (statePtr == NULL) return XXH_ERROR;\n    XXH3_reset_internal(statePtr, 0, secret, secretSize);\n    if (secret == NULL) return XXH_ERROR;\n    if (secretSize < XXH3_SECRET_SIZE_MIN) return XXH_ERROR;\n    return XXH_OK;\n}\n\n/*! @ingroup xxh3_family */\nXXH_PUBLIC_API XXH_errorcode\nXXH3_128bits_reset_withSeed(XXH3_state_t* statePtr, XXH64_hash_t seed)\n{\n    if (statePtr == NULL) return XXH_ERROR;\n    if (seed==0) return XXH3_128bits_reset(statePtr);\n    if (seed != statePtr->seed) XXH3_initCustomSecret(statePtr->customSecret, seed);\n    XXH3_reset_internal(statePtr, seed, NULL, XXH_SECRET_DEFAULT_SIZE);\n    return XXH_OK;\n}\n\n/*! @ingroup xxh3_family */\nXXH_PUBLIC_API XXH_errorcode\nXXH3_128bits_update(XXH3_state_t* state, const void* input, size_t len)\n{\n    return XXH3_update(state, (const xxh_u8*)input, len,\n                       XXH3_accumulate_512, XXH3_scrambleAcc);\n}\n\n/*! @ingroup xxh3_family */\nXXH_PUBLIC_API XXH128_hash_t XXH3_128bits_digest (const XXH3_state_t* state)\n{\n    const unsigned char* const secret = (state->extSecret == NULL) ? state->customSecret : state->extSecret;\n    if (state->totalLen > XXH3_MIDSIZE_MAX) {\n        XXH_ALIGN(XXH_ACC_ALIGN) XXH64_hash_t acc[XXH_ACC_NB];\n        XXH3_digest_long(acc, state, secret);\n        XXH_ASSERT(state->secretLimit + XXH_STRIPE_LEN >= sizeof(acc) + XXH_SECRET_MERGEACCS_START);\n        {   XXH128_hash_t h128;\n            h128.low64  = XXH3_mergeAccs(acc,\n                                         secret + XXH_SECRET_MERGEACCS_START,\n                                         (xxh_u64)state->totalLen * XXH_PRIME64_1);\n            h128.high64 = XXH3_mergeAccs(acc,\n                                         secret + state->secretLimit + XXH_STRIPE_LEN\n                                                - sizeof(acc) - XXH_SECRET_MERGEACCS_START,\n                                         ~((xxh_u64)state->totalLen * XXH_PRIME64_2));\n            return h128;\n        }\n    }\n    /* len <= XXH3_MIDSIZE_MAX : short code */\n    if (state->seed)\n        return XXH3_128bits_withSeed(state->buffer, (size_t)state->totalLen, state->seed);\n    return XXH3_128bits_withSecret(state->buffer, (size_t)(state->totalLen),\n                                   secret, state->secretLimit + XXH_STRIPE_LEN);\n}\n\n/* 128-bit utility functions */\n\n#include <string.h>   /* memcmp, memcpy */\n\n/* return : 1 is equal, 0 if different */\n/*! @ingroup xxh3_family */\nXXH_PUBLIC_API int XXH128_isEqual(XXH128_hash_t h1, XXH128_hash_t h2)\n{\n    /* note : XXH128_hash_t is compact, it has no padding byte */\n    return !(memcmp(&h1, &h2, sizeof(h1)));\n}\n\n/* This prototype is compatible with stdlib's qsort().\n * return : >0 if *h128_1  > *h128_2\n *          <0 if *h128_1  < *h128_2\n *          =0 if *h128_1 == *h128_2  */\n/*! @ingroup xxh3_family */\nXXH_PUBLIC_API int XXH128_cmp(const void* h128_1, const void* h128_2)\n{\n    XXH128_hash_t const h1 = *(const XXH128_hash_t*)h128_1;\n    XXH128_hash_t const h2 = *(const XXH128_hash_t*)h128_2;\n    int const hcmp = (h1.high64 > h2.high64) - (h2.high64 > h1.high64);\n    /* note : bets that, in most cases, hash values are different */\n    if (hcmp) return hcmp;\n    return (h1.low64 > h2.low64) - (h2.low64 > h1.low64);\n}\n\n\n/*======   Canonical representation   ======*/\n/*! @ingroup xxh3_family */\nXXH_PUBLIC_API void\nXXH128_canonicalFromHash(XXH128_canonical_t* dst, XXH128_hash_t hash)\n{\n    XXH_STATIC_ASSERT(sizeof(XXH128_canonical_t) == sizeof(XXH128_hash_t));\n    if (XXH_CPU_LITTLE_ENDIAN) {\n        hash.high64 = XXH_swap64(hash.high64);\n        hash.low64  = XXH_swap64(hash.low64);\n    }\n    memcpy(dst, &hash.high64, sizeof(hash.high64));\n    memcpy((char*)dst + sizeof(hash.high64), &hash.low64, sizeof(hash.low64));\n}\n\n/*! @ingroup xxh3_family */\nXXH_PUBLIC_API XXH128_hash_t\nXXH128_hashFromCanonical(const XXH128_canonical_t* src)\n{\n    XXH128_hash_t h;\n    h.high64 = XXH_readBE64(src);\n    h.low64  = XXH_readBE64(src->digest + 8);\n    return h;\n}\n\n/* Pop our optimization override from above */\n#if XXH_VECTOR == XXH_AVX2 /* AVX2 */ \\\n  && defined(__GNUC__) && !defined(__clang__) /* GCC, not Clang */ \\\n  && defined(__OPTIMIZE__) && !defined(__OPTIMIZE_SIZE__) /* respect -O0 and -Os */\n#  pragma GCC pop_options\n#endif\n\n#endif  /* XXH_NO_LONG_LONG */\n\n#endif  /* XXH_NO_XXH3 */\n\n/*!\n * @}\n */\n#endif  /* XXH_IMPLEMENTATION */\n\n\n#if defined (__cplusplus)\n}\n#endif\n"
        }
      ]
    }
  ]
}