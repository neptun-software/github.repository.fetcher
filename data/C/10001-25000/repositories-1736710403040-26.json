{
  "metadata": {
    "timestamp": 1736710403040,
    "page": 26,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjMw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "timescale/timescaledb",
      "stars": 18178,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".clang-format",
          "type": "blob",
          "size": 4.380859375,
          "content": "---\nLanguage:        Cpp\n# BasedOnStyle:  LLVM\nAccessModifierOffset: -2\nAlignAfterOpenBracket: Align\nAlignConsecutiveAssignments: false\nAlignConsecutiveDeclarations: false\nAlignEscapedNewlines: Right\nAlignOperands:   true\nAlignTrailingComments: true\nAllowAllParametersOfDeclarationOnNextLine: true\nAllowShortBlocksOnASingleLine: false\nAllowShortCaseLabelsOnASingleLine: false\nAllowShortFunctionsOnASingleLine: All\nAllowShortIfStatementsOnASingleLine: false\nAllowShortLoopsOnASingleLine: false\n# AlwaysBreakAfterDefinitionReturnType: None # option is deprecated\nAlwaysBreakAfterReturnType: AllDefinitions\nAlwaysBreakBeforeMultilineStrings: false\nAlwaysBreakTemplateDeclarations: MultiLine\nBinPackArguments: false\nBinPackParameters: true\nBraceWrapping:\n  AfterCaseLabel: true\n  AfterClass:      true\n  AfterControlStatement: true\n  AfterEnum:       true\n  AfterFunction:   true\n  AfterNamespace:  true\n  AfterObjCDeclaration: true\n  AfterStruct:     true\n  AfterUnion:      true\n  AfterExternBlock: true\n  BeforeCatch:     true\n  BeforeElse:      true\n  IndentBraces:    false\n  SplitEmptyFunction: true\n  SplitEmptyRecord: true\n  SplitEmptyNamespace: true\nBreakBeforeBinaryOperators: None\nBreakBeforeBraces: Custom\nBreakBeforeInheritanceComma: false # N/A C++\nBreakInheritanceList: BeforeColon\nBreakBeforeTernaryOperators: false\nBreakConstructorInitializersBeforeComma: false\nBreakConstructorInitializers: BeforeColon\nBreakAfterJavaFieldAnnotations: false # N/A Java\nBreakStringLiterals: true\nColumnLimit:     100\nCommentPragmas:  '^ TS Pragma:' #For future proofing\nCompactNamespaces: false # N/A c++\nConstructorInitializerAllOnOneLineOrOnePerLine: false # N/A C++\nConstructorInitializerIndentWidth: 40 # N/A C++\nContinuationIndentWidth: 4\nCpp11BracedListStyle: false # see catalog.c array struct assigns for an example\nDerivePointerAlignment: false # always use Right\nDisableFormat:   false # haha\n# ExperimentalAutoDetectBinPacking: false #the docs say not to have this in config file\nFixNamespaceComments: true # N/A C++\nForEachMacros:\n  - foreach\n  - forboth\n  - for_each_cell\n  - for_both_cell\n  - forthree\nIncludeBlocks:   Preserve # separate include blocks will not be merged\nIncludeCategories: # we want to ensure c.h and postgres.h appear first\n  - Regex:            '^<(string|unistd)\\.h>'\n    Priority:        -2\n  - Regex:            '^<postgres\\.h>'\n    Priority:        -1\n  - Regex:            '^(<|\")compat/compat\\.h'\n    Priority:        -2\n  - Regex:            '^\"compat/compat-msvc-enter\\.h'\n    Priority:        -1 \n  - Regex:            '^\"compat/compat-msvc-exit\\.h'\n    Priority:         2000000000\n  - Regex:            '.*'\n    Priority:         1\nIncludeIsMainRegex: '' # filename_<suffix> will be seen as the primary include\nIndentCaseLabels: true\nIndentPPDirectives: None # do not indent preprocessor directives after the '#'\nIndentWidth:     4\nIndentWrappedFunctionNames: false # we do not indent the function name in the declaration\nJavaScriptQuotes: Double # N/A js\nJavaScriptWrapImports: true # N/A js\nKeepEmptyLinesAtTheStartOfBlocks: false\nMacroBlockBegin: '' # regex of macros that behave like '{'\nMacroBlockEnd:   '' # regex of macros that behave like '}'\nMaxEmptyLinesToKeep: 1\nNamespaceIndentation: None # N/A c++\nObjCBinPackProtocolList: Auto # N/A objC\nObjCBlockIndentWidth: 2 # N/A objC\nObjCSpaceAfterProperty: false # N/A objC\nObjCSpaceBeforeProtocolList: true # N/A objC\nPenaltyBreakAssignment: 2\nPenaltyBreakBeforeFirstCallParameter: 10000\nPenaltyBreakComment: 300\nPenaltyBreakFirstLessLess: 120\nPenaltyBreakString: 1000\nPenaltyBreakTemplateDeclaration: 10\nPenaltyExcessCharacter: 1000000\nPenaltyReturnTypeOnItsOwnLine: 60\nPointerAlignment: Right # as in char *foo;\nReflowComments:  true # break up long comments into multiple lines\nSortIncludes: CaseInsensitive # sort includes\nSortUsingDeclarations: false # N/A c++\nSpaceAfterCStyleCast: true\nSpaceAfterTemplateKeyword: false # N/A c++\nSpaceBeforeAssignmentOperators: true\nSpaceBeforeCpp11BracedList: false\nSpaceBeforeCtorInitializerColon: true # N/A c++\nSpaceBeforeInheritanceColon: false # N/A c++\nSpaceBeforeParens: ControlStatements\nSpaceBeforeRangeBasedForLoopColon: true # N/A C++\nSpaceInEmptyParentheses: false\nSpacesBeforeTrailingComments: 1\nSpacesInAngles:  false # N/A c++\nSpacesInContainerLiterals: true # N/A c++\nSpacesInCStyleCastParentheses: false\nSpacesInParentheses: false\nSpacesInSquareBrackets: false\nStandard:        Cpp11\nTabWidth:        4\nUseTab:          Always\n...\n"
        },
        {
          "name": ".codecov.yml",
          "type": "blob",
          "size": 0.296875,
          "content": "# If it says that commit YAML is invalid again,\n# validate it with:\n# curl --data-binary @.codecov.yml https://codecov.io/validate\n# More docs: https://docs.codecov.com/docs/codecov-yaml#repository-yaml\nignore:\n  - \"test/src\"\ncoverage:\n  status:\n    project:\n      default:\n        flags:\n          - pr\n"
        },
        {
          "name": ".dir-locals.el",
          "type": "blob",
          "size": 0.828125,
          "content": ";; see also src/tools/editors/emacs.samples in the PostgreSQL source\n;; tree for more complete settings\n\n((c-mode . ((c-basic-offset . 4)\n            (c-file-style . \"bsd\")\n            (fill-column . 78)\n            (indent-tabs-mode . t)\n            (tab-width . 4)))\n (diff-mode .  ((tab-width . 4)))\n (dsssl-mode . ((indent-tabs-mode . nil)))\n (nxml-mode . ((indent-tabs-mode . nil)))\n (perl-mode . ((perl-indent-level . 4)\n               (perl-continued-statement-offset . 4)\n               (perl-continued-brace-offset . 4)\n               (perl-brace-offset . 0)\n               (perl-brace-imaginary-offset . 0)\n               (perl-label-offset . -2)\n               (indent-tabs-mode . t)\n               (tab-width . 4)))\n (sgml-mode . ((fill-column . 78)\n               (indent-tabs-mode . nil)))\n (sql-mode . ((sql-product . postgres)))\n )\n"
        },
        {
          "name": ".editorconfig",
          "type": "blob",
          "size": 0.2216796875,
          "content": "root = true\n\n[*]\ncharset = utf-8\ntrim_trailing_whitespace = true\ninsert_final_newline = true\nend_of_line = lf\n\n[*.{c,h}]\nindent_style = tab\nindent_size = 4\n\n[*.out]\ntrim_trailing_whitespace = false\ninsert_final_newline = false\n"
        },
        {
          "name": ".git-blame-ignore-revs",
          "type": "blob",
          "size": 1.328125,
          "content": "# This file contains a list of commits that are not likely what you\n# are looking for in a blame, such as mass reformatting or renaming.\n# You can set this file as a default ignore file for blame by running\n# the following command.\n#\n# $ git config blame.ignoreRevsFile .git-blame-ignore-revs\n\n# formatting with pgindent\n32c45b75b27e3f690236a9d1c8d13a025316ad2f\n# Fix formatting to comply with pgindent\n2ec065b53823e50dd1ac1d7cf925ae5f90e293ea\n# Fix formatting issues\n3e42150e3b0402ae865d9a827d8d178568a0d27e\n# Run clang-format on code\n34edba16a9385a4b0353e8e07a19dba98d7e3fb9\n# Run pg_format on SQL files\n3f5872ec61650519e2c5e6fe1dfb60d07696cac7\n# Fix various misspellings\n68aec9593c0f37dddbaa4f2e2b34a9ba3f5b11d9\n# Switch to clang-format-14\n7758f5959c8ed64499ab0e6bb66c30464b11dd81\n# Remove trailing whitespaces from test code\na4356f342f1732857a1d8057f71219b50f1919b2\n# Cosmetic changes to create.c\n230f368f4e5d146ce5f919cc5999b236997befaf\n\n# Adding python and yaml linters\n9133319081aef92705f1405087822fc281d215d4\n44cd71a602ba96029001de6e97a1b44488730080\nf75a51def79796ff7fef58ec950c859fe4e71618\n21a3f8206c0de98932867096637c7d1e3d04d925\n\n# Clang-tidy\nf862212c8ca19b1af56c7608a68f22b7dd0c985e\n05ba1cf22f0dc9232069b566dd23c3edb2cbaee4\necf34132c69e1709cd393eab43b5fcbfd7c201db\ne75274ee7c6eef1dafc9b4f4d9f71e8e88f76813\na3ef0384655d57200e83ad7b13c91a31177b97c1\n\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.4580078125,
          "content": "\\#*#\n.#*\n*~\n**/CMakeFiles/\n**/CMakeCache.txt\n/sql/tests/unit/testoutputs.tmp\n/sql/timescaledb--*.sql\n/sql/updates/*.gen\n/data/\n/src/*.o\n/src/*.so\n/src/*.d\n/.vscode/\n/timescaledb.so\n*.bak\n*.backup\ntypedef.list\n/test/testcluster\n/test/log\n/test/temp_schedule\n/build*\n/update_test\n**/GPATH\n**/GTAGS\n**/GRTAGS\n**/GSYMS\ntags\n/.vs\n/compile_commands.json\n/.DS_Store\n/.clangd\n/.cache\n\n/CMakeSettings.json\n/out/*\n/debug/\n/cmake-build-debug/\n/.idea/\ncoccinelle.diff\n.gdb_history\n"
        },
        {
          "name": ".perltidyrc",
          "type": "blob",
          "size": 0.3525390625,
          "content": "--add-whitespace\n--delete-old-whitespace\n--entab-leading-whitespace=4\n--keep-old-blank-lines=2\n--maximum-line-length=78\n--nooutdent-long-comments\n--nooutdent-long-quotes\n--nospace-for-semicolon\n--opening-brace-on-new-line\n--output-line-ending=unix\n--paren-tightness=2\n--paren-vertical-tightness=2\n--paren-vertical-tightness-closing=2\n--noblanks-before-comments\n"
        },
        {
          "name": ".pull-review",
          "type": "blob",
          "size": 1.96875,
          "content": "---\n# pull-review config version (required)\nversion: 1\n\n# use review requests instead of assignees to assign reviewers to pull requests\nuse_review_requests: true\n\n# maximum number of files to evaluate per pull request (set to 0 for no limit)\nmax_files: 0 # evaluate all files of the PR\n\n# minimum number of reviewers to assign and notify for a pull request\nmin_reviewers: 2\n\n# maximum number of reviewers to assign and notify for a pull request\nmax_reviewers: 2\n\n# maximum number of files per reviewer (set to 0 for no limit)\nmax_files_per_reviewer: 0\n\n# maximum number of lines changed per reviewer (set to 0 for no limit)\nmax_lines_per_reviewer: 0\n\n# if at least a minimum number of reviewers is not found, assign a minimum number of reviewers randomly\nassign_min_reviewers_randomly: true\n\n# if the pull request changes code with fewer than a minimum number of authors, add extra reviewers if possible (set to 0 to disable)\nmin_authors_of_changed_files: 0\n\n# minimum percent of lines authored by a reviewer to require an extra reviewer (set to 0 to disable)\nmin_percent_authorship_for_extra_reviewer: 0\n\n# minimum number of lines that must be changed to add an extra reviewer (set to 0 to disable)\nmin_lines_changed_for_extra_reviewer: 0\n\n# require a user to be listed in the reviewers section in order to be assigned to a pull request\nrequire_notification: false\n\n# list of users and their app-specific usernames\nreviewers:\n  svenklemm: {}\n  mkindahl: {}\n  fabriziomello: {}\n  konskov: {}\n  erimatnor: {}\n  nikkhils: {}\n  akuzm: {}\n  gayyappan: {}\n  jnidzwetzki: {}\n  antekresic: {}\n\n# list of users who will never be notified\nreview_blacklist:\n  - horzsolt\n\n# ignore changes to the test output files.\n# 1. because they usually will have equivalent .sql files\n# 2. because people that modify a test, don't necessarily change the same c-functionality at all\n# so tests should be less important than C files\nfile_blacklist:\n  - tsl/test/expected/*.out\n  - test/expected/*.out\n\nlabel_blacklist:\n  - is-auto-backport\n"
        },
        {
          "name": ".unreleased",
          "type": "tree",
          "content": null
        },
        {
          "name": ".yamllint.yaml",
          "type": "blob",
          "size": 0.0595703125,
          "content": "rules:\n  document-start: disable\n  line-length:\n    max: 210\n"
        },
        {
          "name": "CHANGELOG.md",
          "type": "blob",
          "size": 167.474609375,
          "content": "# TimescaleDB Changelog\n\n**Please note: When updating your database, you should connect using\n`psql` with the `-X` flag to prevent any `.psqlrc` commands from\naccidentally triggering the load of a previous DB version.**\n\n## 2.17.2 (2024-11-06)\n\nThis release contains bug fixes since the 2.17.1 release. We recommend that you\nupgrade at the next available opportunity.\n\n**Bugfixes**\n* #7384 Fix \"negative bitmapset member not allowed\" and performance degradation\non queries to compressed tables with ORDER BY clause matching the order of the\ncompressed data\n* #7388 Use-after-free in vectorized grouping by segmentby columns\n\n**Thanks**\n* @dx034 for reporting an issue with negative bitmapset members due to large OIDs\n\n## 2.17.1 (2024-10-21)\n\nThis release contains performance improvements and bug fixes since\nthe 2.17.0 release. We recommend that you upgrade at the next\navailable opportunity.\n\n**Features**\n* #7360 Add chunk skipping GUC\n\n**Bugfixes**\n* #7335 Change log level used in compression\n* #7342 Fix collation for in-memory tuple filtering\n\n**Thanks**\n* @gmilamjr for reporting an issue with the log level of compression messages\n* @hackbnw for reporting an issue with collation during tuple filtering\n\n## 2.17.0 (2024-10-08)\n\nThis release adds support for PostgreSQL 17, significantly improves the performance of continuous aggregate refreshes,\nand contains performance improvements for analytical queries and delete operations over compressed hypertables.\nWe recommend that you upgrade at the next available opportunity.\n\n**Highlighted features in TimescaleDB v2.17.0**\n\n* Full PostgreSQL 17 support for all existing features. TimescaleDB v2.17 is available for PostgreSQL 14, 15, 16, and 17.\n\n* Significant performance improvements for continuous aggregate policies: continuous aggregate refresh is now using\n`merge` instead of deleting old materialized data and re-inserting.\n\n  This update can decrease dramatically the amount of data that must be written on the continuous aggregate in the\n  presence of a small number of changes, reduce the `i/o` cost of refreshing a continuous aggregate, and generate fewer\n  Write-Ahead Logs (`WAL`).\n  Overall, continuous aggregate policies will be more lightweight, use less system resources, and complete faster.\n\n* Increased performance for real-time analytical queries over compressed hypertables:\n  we are excited to introduce additional Single Instruction, Multiple Data (`SIMD`) vectorization optimization to our\n  engine by supporting vectorized execution for queries that group by using the `segment_by` column(s) and\n  aggregate using the basic aggregate functions (`sum`, `count`, `avg`, `min`, `max`).\n\n  Stay tuned for more to come in follow-up releases! Support for grouping on additional columns, filtered aggregation,\n  vectorized expressions, and `time_bucket` is coming soon.\n\n* Improved performance of deletes on compressed hypertables when a large amount of data is affected.\n\n  This improvement speeds up operations that delete whole segments by skipping the decompression step.\n  It is enabled for all deletes that filter by the `segment_by` column(s).\n\n**PostgreSQL 14 deprecation announcement**\n\n  We will continue supporting PostgreSQL 14 until April 2025. Closer to that time, we will announce the specific\n  version of TimescaleDB in which PostgreSQL 14 support will not be included going forward.\n\n**Features**\n* #6882: Allow delete of full segments on compressed chunks without decompression.\n* #7033: Use `merge` statement on continuous aggregates refresh.\n* #7126: Add functions to show the compression information.\n* #7147: Vectorize partial aggregation for `sum(int4)` with grouping on `segment by` columns.\n* #7204: Track additional extensions in telemetry.\n* #7207: Refactor the `decompress_batches_scan` functions for easier maintenance.\n* #7209: Add a function to drop the `osm` chunk.\n* #7275: Add support for the `returning` clause for `merge`.\n* #7200: Vectorize common aggregate functions like `min`, `max`, `sum`, `avg`, `stddev`, `variance` for compressed columns\n  of arithmetic types, when there is grouping on `segment by` columns or no grouping.\n\n**Bug fixes**\n* #7187: Fix the string literal length for the `compressed_data_info` function.\n* #7191: Fix creating default indexes on chunks when migrating the data.\n* #7195: Fix the `segment by` and `order by` checks when dropping a column from a compressed hypertable.\n* #7201: Use the generic extension description when building `apt` and `rpm` loader packages.\n* #7227: Add an index to the `compression_chunk_size` catalog table.\n* #7229: Fix the foreign key constraints where the index and the constraint column order are different.\n* #7230: Do not propagate the foreign key constraints to the `osm` chunk.\n* #7234: Release the cache after accessing the cache entry.\n* #7258: Force English in the `pg_config` command executed by `cmake` to avoid the unexpected building errors.\n* #7270: Fix the memory leak in compressed DML batch filtering.\n* #7286: Fix the index column check while searching for the index.\n* #7290: Add check for null offset for continuous aggregates built on top of continuous aggregates.\n* #7301: Make foreign key behavior for hypertables consistent.\n* #7318: Fix chunk skipping range filtering.\n* #7320: Set the license specific extension comment in the install script.\n\n**Thanks**\n* @MiguelTubio for reporting and fixing the Windows build error.\n* @posuch for reporting the misleading extension description in the generic loader packages.\n* @snyrkill for discovering and reporting the issue with continuous aggregates built on top of continuous aggregates.\n\n## 2.16.1 (2024-08-06)\n\nThis release contains bug fixes since the 2.16.0 release. We recommend\nthat you upgrade at the next available opportunity.\n\n**Bugfixes**\n* #7182 Fix untier_chunk for hypertables with foreign keys\n\n## 2.16.0 (2024-07-31)\n\nThis release contains significant performance improvements when working with compressed data, extended join\nsupport in continuous aggregates, and the ability to define foreign keys from regular tables towards hypertables.\nWe recommend that you upgrade at the next available opportunity.\n\nIn TimescaleDB v2.16.0 we:\n\n* Introduce multiple performance focused optimizations for data manipulation operations (DML) over compressed chunks.\n\n  Improved upsert performance by more than 100x in some cases and more than 1000x in some update/delete scenarios.\n\n* Add the ability to define chunk skipping indexes on non-partitioning columns of compressed hypertables\n\n  TimescaleDB v2.16.0 extends chunk exclusion to use those skipping (sparse) indexes when queries filter on the relevant columns,\n  and prune chunks that do not include any relevant data for calculating the query response.\n\n* Offer new options for use cases that require foreign keys defined.\n\n  You can now add foreign keys from regular tables towards hypertables. We have also removed\n  some really annoying locks in the reverse direction that blocked access to referenced tables\n  while compression was running.\n\n* Extend Continuous Aggregates to support more types of analytical queries.\n\n  More types of joins are supported, additional equality operators on join clauses, and\n  support for joins between multiple regular tables.\n\n**Highlighted features in this release**\n\n* Improved query performance through chunk exclusion on compressed hypertables.\n\n  You can now define chunk skipping indexes on compressed chunks for any column with one of the following\n  integer data types: `smallint`, `int`, `bigint`, `serial`, `bigserial`, `date`, `timestamp`, `timestamptz`.\n\n  After you call `enable_chunk_skipping` on a column, TimescaleDB tracks the min and max values for\n  that column. TimescaleDB uses that information to exclude chunks for queries that filter on that\n  column, and would not find any data in those chunks.\n\n* Improved upsert performance on compressed hypertables.\n\n  By using index scans to verify constraints during inserts on compressed chunks, TimescaleDB speeds\n  up some ON CONFLICT clauses by more than 100x.\n\n* Improved performance of updates, deletes, and inserts on compressed hypertables.\n\n  By filtering data while accessing the compressed data and before decompressing, TimescaleDB has\n  improved performance for updates and deletes on all types of compressed chunks, as well as inserts\n  into compressed chunks with unique constraints.\n\n  By signaling constraint violations without decompressing, or decompressing only when matching\n  records are found in the case of updates, deletes and upserts, TimescaleDB v2.16.0 speeds\n  up those operations more than 1000x in some update/delete scenarios, and 10x for upserts.\n\n* You can add foreign keys from regular tables to hypertables, with support for all types of cascading options.\n  This is useful for hypertables that partition using sequential IDs, and need to reference those IDs from other tables.\n\n* Lower locking requirements during compression for hypertables with foreign keys\n\n  Advanced foreign key handling removes the need for locking referenced tables when new chunks are compressed.\n  DML is no longer blocked on referenced tables while compression runs on a hypertable.\n\n* Improved support for queries on Continuous Aggregates\n\n  `INNER/LEFT` and `LATERAL` joins are now supported. Plus, you can now join with multiple regular tables,\n  and you can have more than one equality operator on join clauses.\n\n**PostgreSQL 13 support removal announcement**\n\nFollowing the deprecation announcement for PostgreSQL 13 in TimescaleDB v2.13,\nPostgreSQL 13 is no longer supported in TimescaleDB v2.16.\n\nThe Currently supported PostgreSQL major versions are 14, 15 and 16.\n\n**Features**\n* #6880: Add support for the array operators used for compressed DML batch filtering.\n* #6895: Improve the compressed DML expression pushdown.\n* #6897: Add support for replica identity on compressed hypertables.\n* #6918: Remove support for PG13.\n* #6920: Rework compression activity wal markers.\n* #6989: Add support for foreign keys when converting plain tables to hypertables.\n* #7020: Add support for the chunk column statistics tracking.\n* #7048: Add an index scan for INSERT DML decompression.\n* #7075: Reduce decompression on the compressed INSERT.\n* #7101: Reduce decompressions for the compressed UPDATE/DELETE.\n* #7108 Reduce decompressions for INSERTs with UNIQUE constraints\n* #7116 Use DELETE instead of TRUNCATE after compression\n* #7134 Refactor foreign key handling for compressed hypertables\n* #7161 Fix `mergejoin input data is out of order`\n\n**Bugfixes**\n* #6987 Fix REASSIGN OWNED BY for background jobs\n* #7018: Fix `search_path` quoting in the compression defaults function.\n* #7046: Prevent locking for compressed tuples.\n* #7055: Fix the `scankey` for `segment by` columns, where the type `constant` is different to `variable`.\n* #7064: Fix the bug in the default `order by` calculation in compression.\n* #7069: Fix the index column name usage.\n* #7074: Fix the bug in the default `segment by` calculation in compression.\n\n**Thanks**\n* @jledentu For reporting a problem with mergejoin input order\n\n\n## 2.15.3 (2024-07-02)\n\nThis release contains bug fixes since the 2.15.2 release.\nBest practice is to upgrade at the next available opportunity.\n\n**Migrating from self-hosted TimescaleDB v2.14.x and earlier**\n\nAfter you run `ALTER EXTENSION`, you must run [this SQL script](https://github.com/timescale/timescaledb-extras/blob/master/utils/2.15.X-fix_hypertable_foreign_keys.sql). For more details, see the following pull request [#6797](https://github.com/timescale/timescaledb/pull/6797).\n\nIf you are migrating from TimescaleDB v2.15.0, v2.15.1 or v2.15.2, no changes are required.\n\n**Bugfixes**\n* #7035: Fix the error when acquiring a tuple lock on the OSM chunks on the replica.\n* #7061: Fix the handling of multiple unique indexes in a compressed INSERT.\n* #7080: Fix the `corresponding equivalence member not found` error.\n* #7088: Fix the leaks in the DML functions.\n* #7091: Fix ORDER BY/GROUP BY expression not found in targetlist on PG16\n\n**Thanks**\n* @Kazmirchuk for reporting the issue about leaks with the functions in DML.\n\n## 2.15.2 (2024-06-07)\n\nThis release contains bug fixes since the 2.15.1 release. Best\npractice is to upgrade at the next available opportunity.\n\n**Migrating from self-hosted TimescaleDB v2.14.x and earlier**\n\nAfter you run `ALTER EXTENSION`, you must run [this SQL script](https://github.com/timescale/timescaledb-extras/blob/master/utils/2.15.X-fix_hypertable_foreign_keys.sql). For more details, see the following pull request [#6797](https://github.com/timescale/timescaledb/pull/6797).\n\nIf you are migrating from TimescaleDB v2.15.0 or v2.15.1, no changes are required.\n\n**Bugfixes**\n* #6975: Fix sort pushdown for partially compressed chunks.\n* #6976: Fix removal of metadata function and update script.\n* #6978: Fix segfault in `compress_chunk` with a primary space partition.\n* #6993: Disallow hash partitioning on primary column.\n\n**Thanks**\n* @gugu for reporting the issue with catalog corruption due to update.\n* @srieding for reporting an issue with partially compressed chunks and ordering on joined columns.\n\n## 2.15.1 (2024-05-28)\n\nThis release contains bug fixes since the 2.15.0 release.\nBest practice is to upgrade at the next available opportunity.\n\n**Migrating from self-hosted TimescaleDB v2.14.x and earlier**\n\nAfter you run `ALTER EXTENSION`, you must run [this SQL script](https://github.com/timescale/timescaledb-extras/blob/master/utils/2.15.X-fix_hypertable_foreign_keys.sql). For more details, see the following pull request [#6797](https://github.com/timescale/timescaledb/pull/6797).\n\nIf you are migrating from TimescaleDB v2.15.0, no changes are required.\n\n**Bugfixes**\n* #6540: Segmentation fault when you backfill data using COPY into a compressed chunk.\n* #6858: `BEFORE UPDATE` trigger not working correctly.\n* #6908: Fix `time_bucket_gapfill()` with timezone behaviour around daylight savings time (DST) switches.\n* #6911: Fix dropped chunk metadata removal in the update script.\n* #6940: Fix `pg_upgrade` failure by removing `regprocedure` from the catalog table.\n* #6957: Fix then `segfault` in UNION queries that contain ordering on compressed chunks.\n\n**Thanks**\n* @DiAifU, @kiddhombre and @intermittentnrg for reporting the issues with gapfill and daylight saving time.\n* @edgarzamora for reporting the issue with update triggers.\n* @hongquan for reporting the issue with the update script.\n* @iliastsa and @SystemParadox for reporting the issue with COPY into a compressed chunk.\n\n## 2.15.0 (2024-05-08)\n\nThis release contains performance improvements and bug fixes since\nthe 2.14.2 release. Best practice is to upgrade at the next\navailable opportunity.\n\nIn addition, it includes these noteworthy features:\n* Support `time_bucket` with `origin` and/or `offset` on Continuous Aggregate\n* Compression improvements:\n  - Improve expression pushdown\n  - Add minmax sparse indexes when compressing columns with btree indexes\n  - Improve compression setting defaults\n  - Vectorize filters in WHERE clause that contain text equality operators and LIKE expressions\n\n**Deprecation warning**\n* Starting on this release will not be possible to create Continuous Aggregate using `time_bucket_ng` anymore and it will be completely removed on the upcoming releases.\n* Recommend users to [migrate their old Continuous Aggregate format to the new one](https://docs.timescale.com/use-timescale/latest/continuous-aggregates/migrate/) because it support will be completely removed in next releases prevent them to migrate.\n* This is the last release supporting PostgreSQL 13.\n\n**Migrating from self-hosted TimescaleDB v2.14.x and earlier**\n\nAfter you run `ALTER EXTENSION`, you must run [this SQL script](https://github.com/timescale/timescaledb-extras/blob/master/utils/2.15.X-fix_hypertable_foreign_keys.sql). For more details, see the following pull request [#6797](https://github.com/timescale/timescaledb/pull/6797).\n\n**Features**\n* #6382: Support for `time_bucket` with `origin` and `offset` in CAggs.\n* #6696: Improve defaults for compression `segment_by` and `order_by`.\n* #6705: Add sparse minmax indexes for compressed columns that have uncompressed btree indexes.\n* #6754: Allow `DROP CONSTRAINT` on compressed hypertables.\n* #6767: Add metadata table `_timestaledb_internal.bgw_job_stat_history` for tracking job execution history.\n* #6798: Prevent usage of deprecated `time_bucket_ng` in CAgg definition.\n* #6810: Add telemetry for access methods.\n* #6811: Remove no longer relevant `timescaledb.allow_install_without_preload` GUC.\n* #6837: Add migration path for CAggs using `time_bucket_ng`.\n* #6865: Update the watermark when truncating a CAgg.\n\n**Bugfixes**\n* #6617: Fix error in show_chunks.\n* #6621: Remove metadata when dropping chunks.\n* #6677: Fix snapshot usage in CAgg invalidation scanner.\n* #6698: Define meaning of 0 retries for jobs as no retries.\n* #6717: Fix handling of compressed tables with primary or unique index in COPY path.\n* #6726: Fix constify cagg_watermark using window function when querying a CAgg.\n* #6729: Fix NULL start value handling in CAgg refresh.\n* #6732: Fix CAgg migration with custom timezone / date format settings.\n* #6752: Remove custom autovacuum setting from compressed chunks.\n* #6770: Fix plantime chunk exclusion for OSM chunk.\n* #6789: Fix deletes with subqueries and compression.\n* #6796: Fix a crash involving a view on a hypertable.\n* #6797: Fix foreign key constraint handling on compressed hypertables.\n* #6816: Fix handling of chunks with no contraints.\n* #6820: Fix a crash when the ts_hypertable_insert_blocker was called directly.\n* #6849: Use non-orderby compressed metadata in compressed DML.\n* #6867: Clean up compression settings when deleting compressed cagg.\n* #6869: Fix compressed DML with constraints of form value OP column.\n* #6870: Fix bool expression pushdown for queries on compressed chunks.\n\n**Thanks**\n* @brasic for reporting a crash when the ts_hypertable_insert_blocker was called directly.\n* @bvanelli for reporting an issue with the jobs retry count.\n* @djzurawsk for reporting error when dropping chunks.\n* @Dzuzepppe for reporting an issue with DELETEs using subquery on compressed chunk working incorrectly.\n* @hongquan for reporting a `timestamp out of range` error during CAgg migrations.\n* @kevcenteno for reporting an issue with the show_chunks API showing incorrect output when `created_before/created_after` was used with time-partitioned columns.\n* @mahipv for starting working on the job history PR.\n* @rovo89 for reporting constify cagg_watermark not working using window function when querying a CAgg.\n\n## 2.14.2 (2024-02-20)\n\nThis release contains bug fixes since the 2.14.1 release.\nWe recommend that you upgrade at the next available opportunity.\n\n**Bugfixes**\n* #6655 Fix segfault in cagg_validate_query\n* #6660 Fix refresh on empty CAgg with variable bucket\n* #6670 Don't try to compress osm chunks\n\n**Thanks**\n* @kav23alex for reporting a segfault in cagg_validate_query\n\n## 2.14.1 (2024-02-14)\n\nThis release contains bug fixes since the 2.14.0 release.\nWe recommend that you upgrade at the next available opportunity.\n\n**Features**\n* #6630 Add views for per chunk compression settings\n\n**Bugfixes**\n* #6636 Fixes extension update of compressed hypertables with dropped columns\n* #6637 Reset sequence numbers on non-rollup compression\n* #6639 Disable default indexscan for compression\n* #6651 Fix DecompressChunk path generation with per chunk settings\n\n**Thanks**\n* @anajavi for reporting an issue with extension update of compressed hypertables\n\n## 2.14.0 (2024-02-08)\n\nThis release contains performance improvements and bug fixes since \nthe 2.13.1 release. We recommend that you upgrade at the next\navailable opportunity.\n \nIn addition, it includes these noteworthy features:\n\n* Ability to change compression settings on existing compressed hypertables at any time. \nNew compression settings take effect on any new chunks that are compressed after the change.\n* Reduced locking requirements during chunk recompression\n* Limiting tuple decompression during DML operations to avoid decompressing a lot of tuples and causing storage issues (100k limit, configurable)\n* Helper functions for determining compression settings\n* Plan-time chunk exclusion for real-time Continuous Aggregate by constifying the cagg_watermark function call, leading to faster queries using real-time continuous aggregates\n\n**For this release only**, you will need to restart the database before running `ALTER EXTENSION`\n\n**Multi-node support removal announcement**\nFollowing the deprecation announcement for Multi-node in TimescaleDB 2.13,\nMulti-node is no longer supported starting with TimescaleDB 2.14.\n\nTimescaleDB 2.13 is the last version that includes multi-node support. Learn more about it [here](docs/MultiNodeDeprecation.md).\n\nIf you want to migrate from multi-node TimescaleDB to single-node TimescaleDB, read the\n[migration documentation](https://docs.timescale.com/migrate/latest/multi-node-to-timescale-service/).\n\n**Deprecation notice: recompress_chunk procedure**\nTimescaleDB 2.14 is the last version that will include the recompress_chunk procedure. Its\nfunctionality will be replaced by the compress_chunk function, which, starting on TimescaleDB 2.14, \nworks on both uncompressed and partially compressed chunks. \nThe compress_chunk function should be used going forward to fully compress all types of chunks or even recompress \nold fully compressed chunks using new compression settings (through the newly introduced recompress optional parameter).\n\n**Features**\n* #6325 Add plan-time chunk exclusion for real-time CAggs\n* #6360 Remove support for creating Continuous Aggregates with old format\n* #6386 Add functions for determining compression defaults\n* #6410 Remove multinode public API\n* #6440 Allow SQLValueFunction pushdown into compressed scan\n* #6463 Support approximate hypertable size\n* #6513 Make compression settings per chunk\n* #6529 Remove reindex_relation from recompression\n* #6531 Fix if_not_exists behavior for CAgg policy with NULL offsets\n* #6545 Remove restrictions for changing compression settings\n* #6566 Limit tuple decompression during DML operations\n* #6579 Change compress_chunk and decompress_chunk to idempotent version by default\n* #6608 Add LWLock for OSM usage in loader \n* #6609 Deprecate recompress_chunk\n* #6609 Add optional recompress argument to compress_chunk\n\n**Bugfixes**\n* #6541 Inefficient join plans on compressed hypertables.\n* #6491 Enable now() plantime constification with BETWEEN\n* #6494 Fix create_hypertable referenced by fk succeeds\n* #6498 Suboptimal query plans when using time_bucket with query parameters\n* #6507 time_bucket_gapfill with timezones doesn't handle daylight savings \n* #6509 Make extension state available through function\n* #6512 Log extension state changes\n* #6522 Disallow triggers on CAggs\n* #6523 Reduce locking level on compressed chunk index during segmentwise recompression\n* #6531 Fix if_not_exists behavior for CAgg policy with NULL offsets\n* #6571 Fix pathtarget adjustment for MergeAppend paths in aggregation pushdown code \n* #6575 Fix compressed chunk not found during upserts\n* #6592 Fix recompression policy ignoring partially compressed chunks\n* #6610 Ensure qsort comparison function is transitive\n\n**Thanks**\n* @coney21 and @GStechschulte for reporting the problem with inefficient join plans on compressed hypertables.\n* @HollowMan6 for reporting triggers not working on materialized views of\nCAggs\n* @jbx1 for reporting suboptimal query plans when using time_bucket with query parameters\n* @JerkoNikolic for reporting the issue with gapfill and DST\n* @pdipesh02 for working on removing the old Continuous Aggregate format\n* @raymalt and @martinhale for reporting very slow query plans on realtime CAggs queries\n\n## 2.13.1 (2024-01-09)\n\nThis release contains bug fixes since the 2.13.0 release.\nWe recommend that you upgrade at the next available opportunity.\n\n**Bugfixes**\n* #6365 Use numrows_pre_compression in approximate row count\n* #6377 Use processed group clauses in PG16\n* #6384 Change bgw_log_level to use PGC_SUSET\n* #6393 Disable vectorized sum for expressions.\n* #6405 Read CAgg watermark from materialized data\n* #6408 Fix groupby pathkeys for gapfill in PG16\n* #6428 Fix index matching during DML decompression\n* #6439 Fix compressed chunk permission handling on PG16\n* #6443 Fix lost concurrent CAgg updates\n* #6454 Fix unique expression indexes on compressed chunks\n* #6465 Fix use of freed path in decompression sort logic\n\n**Thanks**\n* @MA-MacDonald for reporting an issue with gapfill in PG16\n* @aarondglover for reporting an issue with unique expression indexes on compressed chunks\n* @adriangb for reporting an issue with security barrier views on pg16\n\n## 2.13.0 (2023-11-28)\n\nThis release contains performance improvements, an improved hypertable DDL API\nand bug fixes since the 2.12.2 release. We recommend that you upgrade at the next\navailable opportunity.\n\nIn addition, it includes these noteworthy features:\n\n* Full PostgreSQL 16 support for all existing features\n* Vectorized aggregation execution for sum()\n* Track chunk creation time used in retention/compression policies\n\n**Deprecation notice: Multi-node support**\nTimescaleDB 2.13 is the last version that will include multi-node support. Multi-node\nsupport in 2.13 is available for PostgreSQL 13, 14 and 15. Learn more about it\n[here](docs/MultiNodeDeprecation.md).\n\nIf you want to migrate from multi-node TimescaleDB to single-node TimescaleDB read the\n[migration documentation](https://docs.timescale.com/migrate/latest/multi-node-to-timescale-service/).\n\n**PostgreSQL 13 deprecation announcement**\nWe will continue supporting PostgreSQL 13 until April 2024. Sooner to that time, we will announce the specific version of TimescaleDB in which PostgreSQL 13 support will not be included going forward.\n\n**Starting from TimescaleDB 2.13.0**\n* No Amazon Machine Images (AMI) are published. If you previously used AMI, please \nuse another [installation method](https://docs.timescale.com/self-hosted/latest/install/)\n* Continuous Aggregates are materialized only (non-realtime) by default\n\n**Features**\n* #5575 Add chunk-wise sorted paths for compressed chunks\n* #5761 Simplify hypertable DDL API\n* #5890 Reduce WAL activity by freezing compressed tuples immediately\n* #6050 Vectorized aggregation execution for sum() \n* #6062 Add metadata for chunk creation time\n* #6077 Make Continous Aggregates materialized only (non-realtime) by default\n* #6177 Change show_chunks/drop_chunks using chunk creation time\n* #6178 Show batches/tuples decompressed during DML operations in EXPLAIN output\n* #6185 Keep track of catalog version\n* #6227 Use creation time in retention/compression policy\n* #6307 Add SQL function cagg_validate_query\n\n**Bugfixes**\n* #6188 Add GUC for setting background worker log level\n* #6222 Allow enabling compression on hypertable with unique expression index\n* #6240 Check if worker registration succeeded\n* #6254 Fix exception detail passing in compression_policy_execute\n* #6264 Fix missing bms_del_member result assignment\n* #6275 Fix negative bitmapset member not allowed in compression\n* #6280 Potential data loss when compressing a table with a partial index that matches compression order.\n* #6289 Add support for startup chunk exclusion with aggs \n* #6290 Repair relacl on upgrade\n* #6297 Fix segfault when creating a cagg using a NULL width in time bucket function\n* #6305 Make timescaledb_functions.makeaclitem strict\n* #6332 Fix typmod and collation for segmentby columns\n* #6339 Fix tablespace with constraints\n* #6343 Enable segmentwise recompression in compression policy\n\n**Thanks**\n* @fetchezar for reporting an issue with compression policy error messages\n* @jflambert for reporting the background worker log level issue\n* @torazem for reporting an issue with compression and large oids\n* @fetchezar for reporting an issue in the compression policy\n* @lyp-bobi for reporting an issue with tablespace with constraints\n* @pdipesh02 for contributing to the implementation of the metadata for chunk creation time, \n             the generalized hypertable API, and show_chunks/drop_chunks using chunk creation time\n* @lkshminarayanan for all his work on PG16 support\n\n## 2.12.2 (2023-10-19)\n\nThis release contains bug fixes since the 2.12.1 release.\nWe recommend that you upgrade at the next available opportunity.\n\n**Bugfixes**\n* #6155 Align gapfill bucket generation with time_bucket\n* #6181 Ensure fixed_schedule field is populated\n* #6210 Fix EXPLAIN ANALYZE for compressed DML\n\n## 2.12.1 (2023-10-12)\n\nThis release contains bug fixes since the 2.12.0 release.\nWe recommend that you upgrade at the next available opportunity.\n\n**Bugfixes**\n* #6113 Fix planner distributed table count\n* #6117 Avoid decompressing batches using an empty slot\n* #6123 Fix concurrency errors in OSM API\n* #6142 do not throw an error when deprecation GUC cannot be read\n\n**Thanks**\n* @symbx for reporting a crash when selecting from empty hypertables\n\n## 2.12.0 (2023-09-27)\n\nThis release contains performance improvements for compressed hypertables\nand continuous aggregates and bug fixes since the 2.11.2 release.\nWe recommend that you upgrade at the next available opportunity.\n\nThis release moves all internal functions from the _timescaleb_internal\nschema into the _timescaledb_functions schema. This separates code from\ninternal data objects and improves security by allowing more restrictive\npermissions for the code schema. If you are calling any of those internal\nfunctions you should adjust your code as soon as possible. This version\nalso includes a compatibility layer that allows calling them in the old\nlocation but that layer will be removed in 2.14.0.\n\n**PostgreSQL 12 support removal announcement**\nFollowing the deprecation announcement for PostgreSQL 12 in TimescaleDB 2.10,\nPostgreSQL 12 is not supported starting with TimescaleDB 2.12.\nCurrently supported PostgreSQL major versions are 13, 14 and 15.\nPostgreSQL 16 support will be added with a following TimescaleDB release.\n\n**Features**\n* #5137 Insert into index during chunk compression\n* #5150 MERGE support on hypertables\n* #5515 Make hypertables support replica identity\n* #5586 Index scan support during UPDATE/DELETE on compressed hypertables\n* #5596 Support for partial aggregations at chunk level\n* #5599 Enable ChunkAppend for partially compressed chunks\n* #5655 Improve the number of parallel workers for decompression\n* #5758 Enable altering job schedule type through `alter_job`\n* #5805 Make logrepl markers for (partial) decompressions\n* #5809 Relax invalidation threshold table-level lock to row-level when refreshing a Continuous Aggregate\n* #5839 Support CAgg names in chunk_detailed_size\n* #5852 Make set_chunk_time_interval CAggs aware\n* #5868 Allow ALTER TABLE ... REPLICA IDENTITY (FULL|INDEX) on materialized hypertables (continuous aggregates)\n* #5875 Add job exit status and runtime to log\n* #5909 CREATE INDEX ONLY ON hypertable creates index on chunks\n\n**Bugfixes**\n* #5860 Fix interval calculation for hierarchical CAggs\n* #5894 Check unique indexes when enabling compression\n* #5951 _timescaledb_internal.create_compressed_chunk doesn't account for existing uncompressed rows\n* #5988 Move functions to _timescaledb_functions schema\n* #5788 Chunk_create must add an existing table or fail\n* #5872 Fix duplicates on partially compressed chunk reads\n* #5918 Fix crash in COPY from program returning error\n* #5990 Place data in first/last function in correct mctx \n* #5991 Call eq_func correctly in time_bucket_gapfill\n* #6015 Correct row count in EXPLAIN ANALYZE INSERT .. ON CONFLICT output\n* #6035 Fix server crash on UPDATE of compressed chunk\n* #6044 Fix server crash when using duplicate segmentby column\n* #6045 Fix segfault in set_integer_now_func\n* #6053 Fix approximate_row_count for CAggs\n* #6081 Improve compressed DML datatype handling\n* #6084 Propagate parameter changes to decompress child nodes\n* #6102 Schedule compression policy more often\n\n**Thanks**\n* @ajcanterbury for reporting a problem with lateral joins on compressed chunks \n* @alexanderlaw for reporting multiple server crashes \n* @lukaskirner for reporting a bug with monthly continuous aggregates\n* @mrksngl for reporting a bug with unusual user names\n* @willsbit for reporting a crash in time_bucket_gapfill\n\n## 2.11.2 (2023-08-09)\n\nThis release contains bug fixes since the 2.11.1 release.\nWe recommend that you upgrade at the next available opportunity.\n\n**Features**\n* #5923 Feature flags for TimescaleDB features\n**Bugfixes**\n* #5680 Fix DISTINCT query with JOIN on multiple segmentby columns\n* #5774 Fixed two bugs in decompression sorted merge code \n* #5786 Ensure pg_config --cppflags are passed\n* #5906 Fix quoting owners in sql scripts.\n* #5912 Fix crash in 1-step integer policy creation\n**Thanks**\n* @mrksngl for submitting a PR to fix extension upgrade scripts\n* @ericdevries for reporting an issue with DISTINCT queries using segmentby columns of compressed hypertable\n\n## 2.11.1 (2023-06-29)\n\nThis release contains bug fixes since the 2.11.0 release.\nWe recommend that you upgrade at the next available opportunity.\n\n**Features**\n* #5679 Update the loader to add support for the OSM extension (used for data tiering on [Timescale](https://www.timescale.com/))\n\n**Bugfixes**\n* #5705 Scheduler accidentally getting killed when calling `delete_job`\n* #5742 Fix Result node handling with ConstraintAwareAppend on compressed chunks\n* #5750 Ensure tlist is present in decompress chunk plan \n* #5754 Fixed handling of NULL values in bookend_sfunc \n* #5798 Fixed batch look ahead in compressed sorted merge\n* #5804 Mark cagg_watermark function as PARALLEL RESTRICTED\n* #5807 Copy job config JSONB structure into current MemoryContext\n* #5824 Improve continuous aggregate query chunk exclusion\n\n**Thanks**\n* @JamieD9 for reporting an issue with a wrong result ordering\n* @xvaara for reporting an issue with Result node handling in ConstraintAwareAppend\n\n\n## 2.11.0 (2023-05-12)\n\nThis release contains new features and bug fixes since the 2.10.3 release.\nWe deem it moderate priority for upgrading.\n\nThis release includes these noteworthy features:\n* Support for DML operations on compressed chunks:\n  * UPDATE/DELETE support\n  * Support for unique constraints on compressed chunks\n  * Support for `ON CONFLICT DO UPDATE`\n  * Support for `ON CONFLICT DO NOTHING`\n* Join support for Hierarchical Continuous Aggregates\n* Performance improvements for real-time Hierarchical Continuous Aggregates\n\n**Features**\n* #5212 Allow pushdown of reference table joins\n* #5261 Improve Realtime Continuous Aggregate performance\n* #5252 Improve unique constraint support on compressed hypertables\n* #5339 Support UPDATE/DELETE on compressed hypertables\n* #5344 Enable JOINS for Hierarchical Continuous Aggregates\n* #5361 Add parallel support for partialize_agg()\n* #5417 Refactor and optimize distributed COPY\n* #5454 Add support for ON CONFLICT DO UPDATE for compressed hypertables\n* #5547 Skip Ordered Append when only 1 child node is present\n* #5510 Propagate vacuum/analyze to compressed chunks\n* #5584 Reduce decompression during constraint checking\n* #5530 Optimize compressed chunk resorting\n* #5639 Support sending telemetry event reports\n\n**Bugfixes**\n* #5396 Fix SEGMENTBY columns predicates to be pushed down\n* #5427 Handle user-defined FDW options properly\n* #5442 Decompression may have lost DEFAULT values\n* #5459 Fix issue creating dimensional constraints\n* #5570 Improve interpolate error message on datatype mismatch\n* #5573 Fix unique constraint on compressed tables\n* #5615 Add permission checks to run_job()\n* #5614 Enable run_job() for telemetry job\n* #5578 Fix on-insert decompression after schema changes\n* #5613 Quote username identifier appropriately\n* #5525 Fix tablespace for compressed hypertable and corresponding toast\n* #5642 Fix ALTER TABLE SET with normal tables\n* #5666 Reduce memory usage for distributed analyze\n* #5668 Fix subtransaction resource owner\n\n**Thanks**\n* @kovetskiy and @DZDomi for reporting peformance regression in Realtime Continuous Aggregates\n* @ollz272 for reporting an issue with interpolate error messages\n\n\n## 2.10.3 (2023-04-26)\n\n**Bugfixes**\n* #5583 Fix parameterization in DecompressChunk path generation\n* #5602 Fix broken CAgg with JOIN repair function\n\n\n## 2.10.2 (2023-04-20)\n\n**Bugfixes**\n* #5410 Fix file trailer handling in the COPY fetcher\n* #5446 Add checks for malloc failure in libpq calls\n* #5233 Out of on_proc_exit slots on guc license change\n* #5428 Use consistent snapshots when scanning metadata\n* #5499 Do not segfault on large histogram() parameters\n* #5470 Ensure superuser perms during copy/move chunk\n* #5500 Fix when no FROM clause in continuous aggregate definition\n* #5433 Fix join rte in CAggs with joins\n* #5556 Fix duplicated entries on timescaledb_experimental.policies view\n* #5462 Fix segfault after column drop on compressed table\n* #5543 Copy scheduled_jobs list before sorting it\n* #5497 Allow named time_bucket arguments in Cagg definition\n* #5544 Fix refresh from beginning of Continuous Aggregate with variable time bucket\n* #5558 Use regrole for job owner\n* #5542 Enable indexscan on uncompressed part of partially compressed chunks\n\n**Thanks**\n* @nikolaps for reporting an issue with the COPY fetcher\n* @S-imo-n for reporting the issue on Background Worker Scheduler crash\n* @geezhu for reporting issue on segfault in historgram()\n* @mwahlhuetter for reporting the issue with joins in CAggs\n* @mwahlhuetter for reporting issue with duplicated entries on timescaledb_experimental.policies view\n* @H25E for reporting error refreshing from beginning of a Continuous Aggregate with variable time bucket\n\n\n## 2.10.1 (2023-03-07)\n\nThis release contains bug fixes since the 2.10.0 release.\nWe recommend that you upgrade at the next available opportunity.\n\n**Bugfixes**\n* #5159 Support Continuous Aggregates names in hypertable_(detailed_)size\n* #5226 Fix concurrent locking with chunk_data_node table\n* #5317 Fix some incorrect memory handling\n* #5336 Use NameData and namestrcpy for names\n* #5343 Set PortalContext when starting job\n* #5360 Fix uninitialized bucket_info variable\n* #5362 Make copy fetcher more async\n* #5364 Fix num_chunks inconsistency in hypertables view\n* #5367 Fix column name handling in old-style continuous aggregates\n* #5378 Fix multinode DML HA performance regression\n* #5384 Fix Hierarchical Continuous Aggregates chunk_interval_size\n\n**Thanks**\n* @justinozavala for reporting an issue with PL/Python procedures in the background worker\n* @Medvecrab for discovering an issue with copying NameData when forming heap tuples.\n* @pushpeepkmonroe for discovering an issue in upgrading old-style continuous aggregates with renamed columns\n\n\n## 2.10.0 (2023-02-21)\n\nThis release contains new features and bug fixes since the 2.9.3 release.\nWe deem it moderate priority for upgrading.\n\nThis release includes these noteworthy features:\n* Joins in continuous aggregates\n* Re-architecture of how compression works: ~2x improvement on INSERT rate into compressed chunks.\n* Full PostgreSQL 15 support for all existing features. Support for the newly introduced MERGE command on hypertables will be introduced on a follow-up release.\n\n**PostgreSQL 12 deprecation announcement**\nWe will continue supporting PostgreSQL 12 until July 2023. Sooner to that time, we will announce the specific version of TimescaleDB in which PostgreSQL 12 support will not be included going forward.\n\n**Old format of Continuous Aggregates deprecation announcement**\nTimescaleDB 2.7 introduced a new format for continuous aggregates that improves performance.\nAll instances with Continuous Aggregates using the old format should [migrate to the new format](https://docs.timescale.com/api/latest/continuous-aggregates/cagg_migrate/) by July 2023,\nwhen support for the old format will be removed.\nSooner to that time, we will announce the specific version of TimescaleDB in which support for this feature will not be included going forward.\n\n**Features**\n* #4874 Allow joins in continuous aggregates\n* #4926 Refactor INSERT into compressed chunks\n* #5241 Allow RETURNING clause when inserting into compressed chunks\n* #5245 Manage life-cycle of connections via memory contexts\n* #5246 Make connection establishment interruptible\n* #5253 Make data node command execution interruptible\n* #5262 Extend enabling compression on a continuous aggregrate with 'compress_segmentby' and 'compress_orderby' parameters\n\n**Bugfixes**\n* #5214 Fix use of prepared statement in async module\n* #5218 Add role-level security to job error log\n* #5239 Fix next_start calculation for fixed schedules\n* #5290 Fix enabling compression on continuous aggregates with columns requiring quotation\n\n**Thanks**\n* @henriquegelio for reporting the issue on fixed schedules\n\n\n## 2.9.3 (2023-02-03)\n\nThis release contains bug fixes since the 2.9.2 release and a fix for a security vulnerability (#5259).\nYou can check the security advisory(https://github.com/timescale/timescaledb/security/advisories/GHSA-44jh-j22r-33wq)\nfor more information on the vulnerability and the platforms that are affected.\n\nThis release is high priority for upgrade. We strongly recommend that you upgrade as soon as possible.\n\n**Bugfixes**\n* #4804 Skip bucketing when start or end of refresh job is null\n* #5108 Fix column ordering in compressed table index not following the order of a multi-column segment by definition\n* #5187 Don't enable clang-tidy by default\n* #5255 Fix year not being considered as a multiple of day/month in hierarchical continuous aggregates\n* #5259 Lock down search_path in SPI calls\n\n**Thanks**\n* @ssmoss for reporting issues on continuous aggregates\n* @jaskij for reporting the compliation issue that occurred with clang\n\n\n## 2.9.2 (2023-01-26)\n\nThis release contains bug fixes since the 2.9.1 release.\nWe recommend that you upgrade at the next available opportunity.\n\n**Bugfixes**\n* #5114 Fix issue with deleting data node and dropping the database on multi-node\n* #5133 Fix creating a CAgg on a CAgg where the time column is in a different order of the original hypertable\n* #5152 Fix adding column with NULL constraint to compressed hypertable\n* #5170 Fix CAgg on CAgg variable bucket size validation\n* #5180 Fix default data node availability status on multi-node\n* #5181 Fix ChunkAppend and ConstraintAwareAppend with TidRangeScan child subplan\n* #5193 Fix repartition behavior when attaching data node on multi-node\n\n**Thanks**\n* @salquier-appvizer for reporting error on CAgg on CAgg using different column order on the original hypertable\n* @ikkala for reporting error when adding column with NULL constraint to compressed hypertable\n* @ssmoss, @adbnexxtlab and @ivanzamanov for reporting error on CAgg on CAgg variable bucket size validation\n* @ronnyas for reporting a bug \"Invalid child of chunk\" on specific ctid filtering\n\n## 2.9.1 (2022-12-23)\n\nThis release contains bug fixes since the 2.9.0 release.\nThis release is high priority for upgrade. We strongly recommend that you\nupgrade as soon as possible.\n\n**Bugfixes**\n* #5072 Fix CAgg on CAgg bucket size validation\n* #5101 Fix enabling compression on caggs with renamed columns\n* #5106 Fix building against PG15 on Windows\n* #5117 Fix postgres server restart on background worker exit\n* #5121 Fix privileges for job_errors in update script\n\n## 2.9.0 (2022-12-15)\n\nThis release adds major new features since the 2.8.1 release.\nWe deem it moderate priority for upgrading.\n\nThis release includes these noteworthy features:\n* Hierarchical Continuous Aggregates (aka Continuous Aggregate on top of another Continuous Aggregate)\n* Improve `time_bucket_gapfill` function to allow specifying the timezone to bucket\n* Introduce fixed schedules for background jobs and the ability to check job errors.\n* Use `alter_data_node()` to change the data node configuration. This function introduces the option to configure the availability of the data node.\n\nThis release also includes several bug fixes.\n\n**Features**\n* #4476 Batch rows on access node for distributed COPY\n* #4567 Exponentially backoff when out of background workers\n* #4650 Show warnings when not following best practices\n* #4664 Introduce fixed schedules for background jobs\n* #4668 Hierarchical Continuous Aggregates\n* #4670 Add timezone support to time_bucket_gapfill\n* #4678 Add interface for troubleshooting job failures\n* #4718 Add ability to merge chunks while compressing\n* #4786 Extend the now() optimization to also apply to CURRENT_TIMESTAMP\n* #4820 Support parameterized data node scans in joins\n* #4830 Add function to change configuration of a data nodes\n* #4966 Handle DML activity when datanode is not available\n* #4971 Add function to drop stale chunks on a datanode\n\n**Bugfixes**\n* #4663 Don't error when compression metadata is missing\n* #4673 Fix now() constification for VIEWs\n* #4681 Fix compression_chunk_size primary key\n* #4696 Report warning when enabling compression on hypertable\n* #4745 Fix FK constraint violation error while insert into hypertable which references partitioned table\n* #4756 Improve compression job IO performance\n* #4770 Continue compressing other chunks after an error\n* #4794 Fix degraded performance seen on timescaledb_internal.hypertable_local_size() function\n* #4807 Fix segmentation fault during INSERT into compressed hypertable\n* #4822 Fix missing segmentby compression option in CAGGs\n* #4823 Fix a crash that could occur when using nested user-defined functions with hypertables\n* #4840 Fix performance regressions in the copy code\n* #4860 Block multi-statement DDL command in one query\n* #4898 Fix cagg migration failure when trying to resume\n* #4904 Remove BitmapScan support in DecompressChunk\n* #4906 Fix a performance regression in the query planner by speeding up frozen chunk state checks\n* #4910 Fix a typo in process_compressed_data_out\n* #4918 Cagg migration orphans cagg policy\n* #4941 Restrict usage of the old format (pre 2.7) of continuous aggregates in PostgreSQL 15.\n* #4955 Fix cagg migration for hypertables using timestamp without timezone\n* #4968 Check for interrupts in gapfill main loop\n* #4988 Fix cagg migration crash when refreshing the newly created cagg\n* #5054 Fix segfault after second ANALYZE\n* #5086 Reset baserel cache on invalid hypertable cache\n\n**Thanks**\n* @byazici for reporting a problem with segmentby on compressed caggs\n* @jflambert for reporting a crash with nested user-defined functions.\n* @jvanns for reporting hypertable FK reference to vanilla PostgreSQL partitioned table doesn't seem to work\n* @kou for fixing a typo in process_compressed_data_out\n* @kyrias for reporting a crash when ANALYZE is executed on extended query protocol mode with extension loaded.\n* @tobiasdirksen for requesting Continuous aggregate on top of another continuous aggregate\n* @xima for reporting a bug in Cagg migration\n* @xvaara for helping reproduce a bug with bitmap scans in transparent decompression\n\n## 2.8.1 (2022-10-06)\n\nThis release is a patch release. We recommend that you upgrade at the\nnext available opportunity.\n\n**Bugfixes**\n* #4454 Keep locks after reading job status\n* #4658 Fix error when querying a compressed hypertable with compress_segmentby on an enum column\n* #4671 Fix a possible error while flushing the COPY data\n* #4675 Fix bad TupleTableSlot drop\n* #4676 Fix a deadlock when decompressing chunks and performing SELECTs\n* #4685 Fix chunk exclusion for space partitions in SELECT FOR UPDATE queries\n* #4694 Change parameter names of cagg_migrate procedure\n* #4698 Do not use row-by-row fetcher for parameterized plans\n* #4711 Remove support for procedures as custom checks\n* #4712 Fix assertion failure in constify_now\n* #4713 Fix Continuous Aggregate migration policies\n* #4720 Fix chunk exclusion for prepared statements and dst changes\n* #4726 Fix gapfill function signature\n* #4737 Fix join on time column of compressed chunk\n* #4738 Fix error when waiting for remote COPY to finish\n* #4739 Fix continuous aggregate migrate check constraint\n* #4760 Fix segfault when INNER JOINing hypertables\n* #4767 Fix permission issues on index creation for CAggs\n\n**Thanks**\n* @boxhock and @cocowalla for reporting a segfault when JOINing hypertables\n* @carobme for reporting constraint error during continuous aggregate migration\n* @choisnetm, @dustinsorensen, @jayadevanm and @joeyberkovitz for reporting a problem with JOINs on compressed hypertables\n* @daniel-k for reporting a background worker crash\n* @justinpryzby for reporting an error when compressing very wide tables\n* @maxtwardowski for reporting problems with chunk exclusion and space partitions\n* @yuezhihan for reporting GROUP BY error when having compress_segmentby on an enum column\n\n## 2.8.0 (2022-08-30)\n\nThis release adds major new features since the 2.7.2 release.\nWe deem it moderate priority for upgrading.\n\nThis release includes these noteworthy features:\n\n* time_bucket now supports bucketing by month, year and timezone\n* Improve performance of bulk SELECT and COPY for distributed hypertables\n* 1 step CAgg policy management\n* Migrate Continuous Aggregates to the new format\n\n**Features**\n* #4188 Use COPY protocol in row-by-row fetcher\n* #4307 Mark partialize_agg as parallel safe\n* #4380 Enable chunk exclusion for space dimensions in UPDATE/DELETE\n* #4384 Add schedule_interval to policies\n* #4390 Faster lookup of chunks by point\n* #4393 Support intervals with day component when constifying now()\n* #4397 Support intervals with month component when constifying now()\n* #4405 Support ON CONFLICT ON CONSTRAINT for hypertables\n* #4412 Add telemetry about replication\n* #4415 Drop remote data when detaching data node\n* #4416 Handle TRUNCATE TABLE on chunks\n* #4425 Add parameter check_config to alter_job\n* #4430 Create index on Continuous Aggregates\n* #4439 Allow ORDER BY on continuous aggregates\n* #4443 Add stateful partition mappings\n* #4484 Use non-blocking data node connections for COPY\n* #4495 Support add_dimension() with existing data\n* #4502 Add chunks to baserel cache on chunk exclusion\n* #4545 Add hypertable distributed argument and defaults\n* #4552 Migrate Continuous Aggregates to the new format\n* #4556 Add runtime exclusion for hypertables\n* #4561 Change get_git_commit to return full commit hash\n* #4563 1 step CAgg policy management\n* #4641 Allow bucketing by month, year, century in time_bucket and time_bucket_gapfill\n* #4642 Add timezone support to time_bucket\n\n**Bugfixes**\n* #4359 Create composite index on segmentby columns\n* #4374 Remove constified now() constraints from plan\n* #4416 Handle TRUNCATE TABLE on chunks\n* #4478 Synchronize chunk cache sizes\n* #4486 Adding boolean column with default value doesn't work on compressed table\n* #4512 Fix unaligned pointer access\n* #4519 Throw better error message on incompatible row fetcher settings\n* #4549 Fix dump_meta_data for windows\n* #4553 Fix timescaledb_post_restore GUC handling\n* #4573 Load TSL library on compressed_data_out call\n* #4575 Fix use of `get_partition_hash` and `get_partition_for_key` inside an IMMUTABLE function\n* #4577 Fix segfaults in compression code with corrupt data\n* #4580 Handle default privileges on CAggs properly\n* #4582 Fix assertion in GRANT .. ON ALL TABLES IN SCHEMA\n* #4583 Fix partitioning functions\n* #4589 Fix rename for distributed hypertable\n* #4601 Reset compression sequence when group resets\n* #4611 Fix a potential OOM when loading large data sets into a hypertable\n* #4624 Fix heap buffer overflow\n* #4627 Fix telemetry initialization\n* #4631 Ensure TSL library is loaded on database upgrades\n* #4646 Fix time_bucket_ng origin handling\n* #4647 Fix the error \"SubPlan found with no parent plan\" that occurred if using joins in RETURNING clause.\n\n**Thanks**\n* @AlmiS for reporting error on `get_partition_hash` executed inside an IMMUTABLE function\n* @Creatation for reporting an issue with renaming hypertables\n* @janko for reporting an issue when adding bool column with default value to compressed hypertable\n* @jayadevanm for reporting error of TRUNCATE TABLE on compressed chunk\n* @michaelkitson for reporting permission errors using default privileges on Continuous Aggregates\n* @mwahlhuetter for reporting error in joins in RETURNING clause\n* @ninjaltd and @mrksngl for reporting a potential OOM when loading large data sets into a hypertable\n* @PBudmark for reporting an issue with dump_meta_data.sql on Windows\n* @ssmoss for reporting an issue with time_bucket_ng origin handling\n\n## 2.7.2 (2022-07-26)\n\nThis release is a patch release. We recommend that you upgrade at the\nnext available opportunity.\nAmong other things this release fixes several memory leaks, handling\nof TOASTed values in GapFill and parameter handling in prepared statements.\n\n**Bugfixes**\n* #4517 Fix prepared statement param handling in ChunkAppend\n* #4522 Fix ANALYZE on dist hypertable for a set of nodes\n* #4526 Fix gapfill group comparison for TOASTed values\n* #4527 Handle stats properly for range types\n* #4532 Fix memory leak in function telemetry\n* #4534 Use explicit memory context with hash_create\n* #4538 Fix chunk creation on hypertables with non-default statistics\n\n**Thanks**\n* @3a6u9ka, @bgemmill, @hongquan, @stl-leonid-kalmaev and @victor-sudakov for reporting a memory leak\n* @hleung2021 and @laocaixw  for reporting an issue with parameter handling in prepared statements\n\n## 2.7.1 (2022-07-07)\n\nThis release is a patch release. We recommend that you upgrade at the\nnext available opportunity.\n\n**Bugfixes**\n* #4494 Handle timescaledb versions aptly in multinode\n* #4493 Segfault when executing IMMUTABLE functions\n* #4482 Fix race conditions during chunk (de)compression\n* #4367 Improved buffer management in the copy operator\n* #4375 Don't ask for orderby column if default already set\n* #4400 Use our implementation of `find_em_expr_for_rel` for PG15+\n* #4408 Fix crash during insert into distributed hypertable\n* #4411 Add `shmem_request_hook`\n* #4437 Fix segfault in subscription_exec\n* #4442 Fix perms in copy/move chunk\n* #4450 Retain hypertable ownership on `attach_data_node`\n* #4451 Repair numeric partial state on the fly\n* #4463 Fix empty bytea handlng with distributed tables\n* #4469 Better superuser handling for `move_chunk`\n\n**Features**\n* #4244 Function telemetry\n* #4287 Add internal api for foreign table chunk\n* #4470 Block drop chunk if chunk is in frozen state\n* #4464 Add internal api to associate a hypertable with custom jobs\n\n**Thanks**\n* @xin-hedera Finding bug in empty bytea values for distributed tables\n* @jflambert for reporting a bug with IMMUTABLE functions\n* @nikugogoi for reporting a bug with CTEs and upserts on distributed hypertables\n\n## 2.7.0 (2022-05-24)\n\nThis release adds major new features since the 2.6.1 release.\nWe deem it moderate priority for upgrading.\n\nThis release includes these noteworthy features:\n\n* Optimize continuous aggregate query performance and storage\n* The following query clauses and functions can now be used in a continuous\n  aggregate: FILTER, DISTINCT, ORDER BY as well as [Ordered-Set Aggregate](https://www.postgresql.org/docs/current/functions-aggregate.html#FUNCTIONS-ORDEREDSET-TABLE)\n  and [Hypothetical-Set Aggregate](https://www.postgresql.org/docs/current/functions-aggregate.html#FUNCTIONS-HYPOTHETICAL-TABLE)\n* Optimize now() query planning time\n* Improve COPY insert performance\n* Improve performance of UPDATE/DELETE on PG14 by excluding chunks\n\nThis release also includes several bug fixes.\n\nIf you are upgrading from a previous version and were using compression\nwith a non-default collation on a segmentby-column you should recompress\nthose hypertables.\n\n**Features**\n* #4045 Custom origin's support in CAGGs\n* #4120 Add logging for retention policy\n* #4158 Allow ANALYZE command on a data node directly\n* #4169 Add support for chunk exclusion on DELETE to PG14\n* #4209 Add support for chunk exclusion on UPDATE to PG14\n* #4269 Continuous Aggregates finals form\n* #4301 Add support for bulk inserts in COPY operator\n* #4311 Support non-superuser move chunk operations\n* #4330 Add GUC \"bgw_launcher_poll_time\"\n* #4340 Enable now() usage in plan-time chunk exclusion\n\n**Bugfixes**\n* #3899 Fix segfault in Continuous Aggregates\n* #4225 Fix TRUNCATE error as non-owner on hypertable\n* #4236 Fix potential wrong order of results for compressed hypertable with a non-default collation\n* #4249 Fix option \"timescaledb.create_group_indexes\"\n* #4251 Fix INSERT into compressed chunks with dropped columns\n* #4255 Fix option \"timescaledb.create_group_indexes\"\n* #4259 Fix logic bug in extension update script\n* #4269 Fix bad Continuous Aggregate view definition reported in #4233\n* #4289 Support moving compressed chunks between data nodes\n* #4300 Fix refresh window cap for cagg refresh policy\n* #4315 Fix memory leak in scheduler\n* #4323 Remove printouts from signal handlers\n* #4342 Fix move chunk cleanup logic\n* #4349 Fix crashes in functions using AlterTableInternal\n* #4358 Fix crash and other issues in telemetry reporter\n\n**Thanks**\n* @abrownsword for reporting a bug in the telemetry reporter and testing the fix\n* @jsoref for fixing various misspellings in code, comments and documentation\n* @yalon for reporting an error with ALTER TABLE RENAME on distributed hypertables\n* @zhuizhuhaomeng for reporting and fixing a memory leak in our scheduler\n\n## 2.6.1 (2022-04-11)\nThis release is patch release. We recommend that you upgrade at the next available opportunity.\n\n**Bugfixes**\n* #4121 Fix RENAME TO/SET SCHEMA on distributed hypertable\n* #4122 Fix segfault on INSERT into distributed hypertable\n* #4142 Ignore invalid relid when deleting hypertable\n* #4159 Fix ADD COLUMN IF NOT EXISTS error on compressed hypertable\n* #4161 Fix memory handling during scans\n* #4176 Fix remote EXPLAIN with parameterized queries\n* #4181 Fix spelling errors and omissions\n* #4186 Fix owner change for distributed hypertable\n* #4192 Abort sessions after extension reload\n* #4193 Fix relcache callback handling causing crashes\n* #4199 Remove signal-unsafe calls from signal handlers\n* #4219 Do not modify aggregation state in finalize\n\n**Thanks**\n* @abrownsword for reporting a crash in the telemetry reporter\n* @amalek215 for reporting a segmentation fault when running VACUUM FULL pg_class\n* @daydayup863 for reporting issue with remote explain\n* @krvajal for reporting an error with ADD COLUMN IF NOT EXISTS on compressed hypertables\n\n## 2.6.0 (2022-02-16)\nThis release is medium priority for upgrade. We recommend that you upgrade at the next available opportunity.\n\nThis release adds major new features since the 2.5.2 release, including:\n\n* Compression in continuous aggregates\n* Experimental support for timezones in continuous aggregates\n* Experimental support for monthly buckets in continuous aggregates\n\nThe release also includes several bug fixes. Telemetry reports now include new and more detailed statistics on regular tables and views, compression, distributed hypertables, and continuous aggregates, which will help us improve TimescaleDB.\n\n**Features**\n* #3768 Allow ALTER TABLE ADD COLUMN with DEFAULT on compressed hypertable\n* #3769 Allow ALTER TABLE DROP COLUMN on compressed hypertable\n* #3943 Optimize first/last\n* #3945 Add support for ALTER SCHEMA on multi-node\n* #3949 Add support for DROP SCHEMA on multi-node\n\n**Bugfixes**\n* #3808 Properly handle `max_retries` option\n* #3863 Fix remote transaction heal logic\n* #3869 Fix ALTER SET/DROP NULL constraint on distributed hypertable\n* #3944 Fix segfault in add_compression_policy\n* #3961 Fix crash in EXPLAIN VERBOSE on distributed hypertable\n* #4015 Eliminate float rounding instabilities in interpolate\n* #4019 Update ts_extension_oid in transitioning state\n* #4073 Fix buffer overflow in partition scheme\n* #4180 ALTER TABLE OWNER TO does not work for distributed hypertable\n\n**Improvements**\n* Query planning performance is improved for hypertables with a large number of chunks.\n\n**Thanks**\n* @fvannee for reporting a first/last memory leak\n* @mmouterde for reporting an issue with floats and interpolate\n\n## 2.5.2 (2022-02-09)\n\nThis release contains bug fixes since the 2.5.1 release.\nThis release is high priority for upgrade. We strongly recommend that you\nupgrade as soon as possible.\n\n**Bugfixes**\n* #3900 Improve custom scan node registration\n* #3911 Fix role type deparsing for GRANT command\n* #3918 Fix DataNodeScan plans with one-time filter\n* #3921 Fix segfault on insert into internal compressed table\n* #3938 Fix subtract_integer_from_now on 32-bit platforms and improve error handling\n* #3939 Fix projection handling in time_bucket_gapfill\n* #3948 Avoid double PGclear() in data fetchers\n* #3979 Fix deparsing of index predicates\n* #4020 Fix ALTER TABLE EventTrigger initialization\n* #4024 Fix premature cache release call\n* #4037 Fix status for dropped chunks that have catalog entries\n* #4069 Fix riinfo NULL handling in ANY construct\n* #4071 Fix extension installation privilege escalation (CVE-2022-24128)\n\n**Thanks**\n* @carlocperez for reporting crash with NULL handling in ANY construct\n* @erikhh for reporting an issue with time_bucket_gapfill\n* @kancsuki for reporting drop column and partial index creation not working\n* @mmouterde for reporting an issue with floats and interpolate\n* Pedro Gallegos for reporting a possible privilege escalation during extension installation\n\n## 2.5.1 (2021-12-02)\n\nThis release contains bug fixes since the 2.5.0 release.\nWe deem it medium priority to upgrade.\n\n**Bugfixes**\n* #3706 Test enabling dist compression within a procedure\n* #3734 Rework distributed DDL processing logic\n* #3737 Fix flaky pg_dump\n* #3739 Fix compression policy on tables using INTEGER\n* #3766 Fix segfault in ts_hist_sfunc\n* #3779 Support GRANT/REVOKE on distributed database\n* #3789 Fix time_bucket comparison transformation\n* #3797 Fix DISTINCT ON queries for distributed hyperatbles\n* #3799 Fix error printout on correct security label\n* #3801 Fail size utility functions when data nodes do not respond\n* #3809 Fix NULL pointer evaluation in fill_result_error()\n* #3811 Fix INSERT..SELECT involving dist hypertables\n* #3819 Fix reading garbage value from TSConnectionError\n* #3824 Remove pointers from CAGG lists for 64-bit archs\n* #3846 Eliminate deadlock in recompress chunk policy\n* #3881 Fix SkipScan crash due to pruned unique path\n* #3884 Fix create_distributed_restore_point memory issue\n\n**Thanks**\n* @cbisnett for reporting and fixing a typo in an error message\n* @CaptainCuddleCube for reporting bug on compression policy procedure on tables using INTEGER on time dimension\n* @phemmer for reporting bugs on multi-node\n\n## 2.5.0 (2021-10-28)\n\nThis release adds major new features since the 2.4.2 release.\nWe deem it moderate priority for upgrading.\n\nThis release includes these noteworthy features:\n\n* Continuous Aggregates for Distributed Hypertables\n* Support for PostgreSQL 14\n* Experimental: Support for timezones in `time_bucket_ng()`, including\nthe `origin` argument\n\nThis release also includes several bug fixes.\n\n**Features**\n* #3034 Add support for PostgreSQL 14\n* #3435 Add continuous aggregates for distributed hypertables\n* #3505 Add support for timezones in `time_bucket_ng()`\n\n**Bugfixes**\n* #3580 Fix memory context bug executing TRUNCATE\n* #3592 Allow alter column type on distributed hypertable\n* #3598 Improve evaluation of stable functions such as now() on access node\n* #3618 Fix execution of refresh_caggs from user actions\n* #3625 Add shared dependencies when creating chunk\n* #3626 Fix memory context bug executing TRUNCATE\n* #3627 Schema qualify UDTs in multi-node\n* #3638 Allow owner change of a data node\n* #3654 Fix index attnum mapping in reorder_chunk\n* #3661 Fix SkipScan path generation with constant DISTINCT column\n* #3667 Fix compress_policy for multi txn handling\n* #3673 Fix distributed hypertable DROP within a procedure\n* #3701 Allow anyone to use size utilities on distributed hypertables\n* #3708 Fix crash in get_aggsplit\n* #3709 Fix ordered append pathkey check\n* #3712 Fix GRANT/REVOKE ALL IN SCHEMA handling\n* #3717 Support transparent decompression on individual chunks\n* #3724 Fix inserts into compressed chunks on hypertables with caggs\n* #3727 Fix DirectFunctionCall crash in distributed_exec\n* #3728 Fix SkipScan with varchar column\n* #3733 Fix ANALYZE crash with custom statistics for custom types\n* #3747 Always reset expr context in DecompressChunk\n\n**Thanks**\n* @binakot and @sebvett for reporting an issue with DISTINCT queries\n* @hardikm10, @DavidPavlicek and @pafiti for reporting bugs on TRUNCATE\n* @mjf for reporting an issue with ordered append and JOINs\n* @phemmer for reporting the issues on multinode with aggregate queries and evaluation of now()\n* @abolognino for reporting an issue with INSERTs into compressed hypertables that have cagg\n* @tanglebones for reporting the ANALYZE crash with custom types on multinode\n* @amadeubarbosa and @felipenogueirajack for reporting crash using JSONB column in compressed chunks\n\n## 2.4.2 (2021-09-21)\n\nThis release contains bug fixes since the 2.4.1 release.\nWe deem it high priority to upgrade.\n\n**Bugfixes**\n* #3437 Rename on all continuous aggregate objects\n* #3469 Use signal-safe functions in signal handler\n* #3520 Modify compression job processing logic\n* #3527 Fix time_bucket_ng behaviour with origin argument\n* #3532 Fix bootstrap with regresschecks disabled\n* #3574 Fix failure on job execution by background worker\n* #3590 Call cleanup functions on backend exit\n\n**Thanks**\n* @jankatins for reporting a crash with background workers\n* @LutzWeischerFujitsu for reporting an issue with bootstrap\n\n## 2.4.1 (2021-08-19)\n\nThis release contains bug fixes since the 2.4.0 release.  We deem it\nhigh priority to upgrade.\n\nThe release fixes continous aggregate refresh for postgres 12.8 and\n13.4, a crash with ALTER TABLE commands and a crash with continuous\naggregates with HAVING clause.\n\n**Bugfixes**\n* #3430 Fix havingqual processing for continuous aggregates\n* #3468 Disable tests by default if tools are not found\n* #3462 Fix crash while tracking alter table commands\n* #3489 Fix continuous agg bgw job failure for PG 12.8 and 13.4\n* #3494 Improve error message when adding data nodes\n\n**Thanks**\n* @brianbenns for reporting a segfault with continuous aggregates\n* @usego for reporting an issue with continuous aggregate refresh on PG 13.4\n\n## 2.4.0 (2021-07-29)\n\nThis release adds new experimental features since the 2.3.1 release.\n\nThe experimental features in this release are:\n* APIs for chunk manipulation across data nodes in a distributed\nhypertable setup. This includes the ability to add a data node and move\nchunks to the new data node for cluster rebalancing.\n* The `time_bucket_ng` function, a newer version of `time_bucket`. This\nfunction supports years, months, days, hours, minutes, and seconds.\n\nWere committed to developing these experiments, giving the community\n a chance to provide early feedback and influence the direction of\nTimescaleDBs development. Well travel faster with your input!\n\nPlease create your feedback as a GitHub issue (using the\nexperimental-schema label), describe what you found, and tell us the\nsteps or share the code snip to recreate it.\n\nThis release also includes several bug fixes.\n\nPostgreSQL 11 deprecation announcement\nTimescale is working hard on our next exciting features. To make that\npossible, we require functionality that is available in Postgres 12 and\nabove. Postgres 11 is not supported with TimescaleDB 2.4.\n\n**Experimental Features**\n* #3293 Add timescaledb_experimental schema\n* #3302 Add block_new_chunks and allow_new_chunks API to experimental\nschema. Add chunk based refresh_continuous_aggregate.\n* #3211 Introduce experimental time_bucket_ng function\n* #3366 Allow use of experimental time_bucket_ng function in continuous aggregates\n* #3408 Support for seconds, minutes and hours in time_bucket_ng\n* #3446 Implement cleanup for chunk copy/move.\n\n**Bugfixes**\n* #3401 Fix segfault for RelOptInfo without fdw_private\n* #3411 Verify compressed chunk validity for compressed path\n* #3416 Fix targetlist names for continuous aggregate views\n* #3434 Remove extension check from relcache invalidation callback\n* #3440 Fix remote_tx_heal_data_node to work with only current database\n\n**Thanks**\n* @fvannee for reporting an issue with hypertable expansion in functions\n* @amalek215 for reporting an issue with cache invalidation during pg_class vacuum full\n* @hardikm10 for reporting an issue with inserting into compressed chunks\n* @dberardo-com and @iancmcc for reporting an issue with extension updates after renaming columns of continuous aggregates.\n\n## 2.3.1 (2021-07-05)\n\nThis maintenance release contains bugfixes since the 2.3.0 release. We\ndeem it moderate priority for upgrading. The release introduces the\npossibility of generating downgrade scripts, improves the trigger\nhandling for distributed hypertables, adds some more randomness to\nchunk assignment to avoid thundering herd issues in chunk assignment,\nand fixes some issues in update handling as well as some other bugs.\n\n**Bugfixes**\n* #3279 Add some more randomness to chunk assignment\n* #3288 Fix failed update with parallel workers\n* #3300 Improve trigger handling on distributed hypertables\n* #3304 Remove paths that reference parent relids for compressed chunks\n* #3305 Fix pull_varnos miscomputation of relids set\n* #3310 Generate downgrade script\n* #3314 Fix heap buffer overflow in hypertable expansion\n* #3317 Fix heap buffer overflow in remote connection cache.\n* #3327 Make aggregates in caggs fully qualified\n* #3336 Fix pg_init_privs objsubid handling\n* #3345 Fix SkipScan distinct column identification\n* #3355 Fix heap buffer overflow when renaming compressed hypertable columns.\n* #3367 Improve DecompressChunk qual pushdown\n* #3377 Fix bad use of repalloc\n\n**Thanks**\n* @db-adrian for reporting an issue when accessing cagg view through postgres_fdw\n* @fncaldas and @pgwhalen for reporting an issue accessing caggs when public is not in search_path\n* @fvannee, @mglonnro and @ebreijo for reporting an issue with the upgrade script\n* @fvannee for reporting a performance regression with SkipScan\n\n## 2.3.0 (2021-05-25)\n\nThis release adds major new features since the 2.2.1 release.\nWe deem it moderate priority for upgrading.\n\nThis release adds support for inserting data into compressed chunks\nand improves performance when inserting data into distributed hypertables.\nDistributed hypertables now also support triggers and compression policies.\n\nThe bug fixes in this release address issues related to the handling\nof privileges on compressed hypertables, locking, and triggers with transition tables.\n\n**Features**\n* #3116 Add distributed hypertable compression policies\n* #3162 Use COPY when executing distributed INSERTs\n* #3199 Add GENERATED column support on distributed hypertables\n* #3210 Add trigger support on distributed hypertables\n* #3230 Support for inserts into compressed chunks\n\n**Bugfixes**\n* #3213 Propagate grants to compressed hypertables\n* #3229 Use correct lock mode when updating chunk\n* #3243 Fix assertion failure in decompress_chunk_plan_create\n* #3250 Fix constraint triggers on hypertables\n* #3251 Fix segmentation fault due to incorrect call to chunk_scan_internal\n* #3252 Fix blocking triggers with transition tables\n\n**Thanks**\n* @yyjdelete for reporting a crash with decompress_chunk and identifying the bug in the code\n* @fabriziomello for documenting the prerequisites when compiling against PostgreSQL 13\n\n## 2.2.1 (2021-05-05)\n\nThis maintenance release contains bugfixes since the 2.2.0 release. We\ndeem it high priority for upgrading.\n\nThis release extends Skip Scan to multinode by enabling the pushdown\nof `DISTINCT` to data nodes. It also fixes a number of bugs in the\nimplementation of Skip Scan, in distributed hypertables, in creation\nof indexes, in compression, and in policies.\n\n**Features**\n* #3113 Pushdown \"SELECT DISTINCT\" in multi-node to allow use of Skip\n  Scan\n\n**Bugfixes**\n* #3101 Use commit date in `get_git_commit()`\n* #3102 Fix `REINDEX TABLE` for distributed hypertables\n* #3104 Fix use after free in `add_reorder_policy`\n* #3106 Fix use after free in `chunk_api_get_chunk_stats`\n* #3109 Copy recreated object permissions on update\n* #3111 Fix `CMAKE_BUILD_TYPE` check\n* #3112 Use `%u` to format Oid instead of `%d`\n* #3118 Fix use after free in cache\n* #3123 Fix crash while using `REINDEX TABLE CONCURRENTLY`\n* #3135 Fix SkipScan path generation in `DISTINCT` queries with expressions\n* #3146 Fix SkipScan for IndexPaths without pathkeys\n* #3147 Skip ChunkAppend if AppendPath has no children\n* #3148 Make `SELECT DISTINCT` handle non-var targetlists\n* #3151 Fix `fdw_relinfo_get` assertion failure on `DELETE`\n* #3155 Inherit `CFLAGS` from PostgreSQL\n* #3169 Fix incorrect type cast in compression policy\n* #3183 Fix segfault in calculate_chunk_interval\n* #3185 Fix wrong datatype for integer based retention policy\n\n**Thanks**\n* @Dead2, @dv8472 and @einsibjarni for reporting an issue with multinode queries and views\n* @aelg for reporting an issue with policies on integer-based hypertables\n* @hperez75 for reporting an issue with Skip Scan\n* @nathanloisel for reporting an issue with compression on hypertables with integer-based timestamps\n* @xin-hedera for fixing an issue with compression on hypertables with integer-based timestamps\n\n## 2.2.0 (2021-04-13)\n\nThis release adds major new features since the 2.1.1 release.\nWe deem it moderate priority for upgrading.\n\nThis release adds the Skip Scan optimization, which significantly\nimproves the performance of queries with DISTINCT ON. This\noptimization is not yet available for queries on distributed\nhypertables.\n\nThis release also adds a function to create a distributed\nrestore point, which allows performing a consistent restore of a\nmulti-node cluster from a backup.\n\nThe bug fixes in this release address issues with size and stats\nfunctions, high memory usage in distributed inserts, slow distributed\nORDER BY queries, indexes involving INCLUDE, and single chunk query\nplanning.\n\n**PostgreSQL 11 deprecation announcement**\n\nTimescale is working hard on our next exciting features. To make that\npossible, we require functionality that is unfortunately absent on\nPostgreSQL 11. For this reason, we will continue supporting PostgreSQL\n11 until mid-June 2021. Sooner to that time, we will announce the\nspecific version of TimescaleDB in which PostgreSQL 11 support will\nnot be included going forward.\n\n**Major Features**\n* #2843 Add distributed restore point functionality\n* #3000 SkipScan to speed up SELECT DISTINCT\n\n**Bugfixes**\n* #2989 Refactor and harden size and stats functions\n* #3058 Reduce memory usage for distributed inserts\n* #3067 Fix extremely slow multi-node order by queries\n* #3082 Fix chunk index column name mapping\n* #3083 Keep Append pathkeys in ChunkAppend\n\n**Thanks**\n* @BowenGG for reporting an issue with indexes with INCLUDE\n* @fvannee for reporting an issue with ChunkAppend pathkeys\n* @pedrokost and @RobAtticus for reporting an issue with size\n  functions on empty hypertables\n* @phemmer and @ryanbooz for reporting issues with slow\n  multi-node order by queries\n* @stephane-moreau for reporting an issue with high memory usage during\n  single-transaction inserts on a distributed hypertable.\n\n## 2.1.1 (2021-03-29)\n\nThis maintenance release contains bugfixes since the 2.1.0 release. We\ndeem it high priority for upgrading.\n\nThe bug fixes in this release address issues with CREATE INDEX and\nUPSERT for hypertables, custom jobs, and gapfill queries.\n\nThis release marks TimescaleDB as a trusted extension in PG13, so that\nsuperuser privileges are not required anymore to install the extension.\n\n**Minor features**\n* #2998 Mark timescaledb as trusted extension\n\n**Bugfixes**\n* #2948 Fix off by 4 error in histogram deserialize\n* #2974 Fix index creation for hypertables with dropped columns\n* #2990 Fix segfault in job_config_check for cagg\n* #2987 Fix crash due to txns in emit_log_hook_callback\n* #3042 Commit end transaction for CREATE INDEX\n* #3053 Fix gapfill/hashagg planner interaction\n* #3059 Fix UPSERT on hypertables with columns with defaults\n\n**Thanks**\n* @eloyekunle and @kitwestneat for reporting an issue with UPSERT\n* @jocrau for reporting an issue with index creation\n* @kev009 for fixing a compilation issue\n* @majozv and @pehlert for reporting an issue with time_bucket_gapfill\n\n## 2.1.0 (2021-02-22)\n\nThis release adds major new features since the 2.0.2 release.\nWe deem it moderate priority for upgrading.\n\nThis release adds the long-awaited support for PostgreSQL 13 to TimescaleDB.\nThe minimum required PostgreSQL 13 version is 13.2 due to a security vulnerability\naffecting TimescaleDB functionality present in earlier versions of PostgreSQL 13.\n\nThis release also relaxes some restrictions for compressed hypertables;\nnamely, TimescaleDB now supports adding columns to compressed hypertables\nand renaming columns of compressed hypertables.\n\n**Major Features**\n* #2779 Add support for PostgreSQL 13\n\n**Minor features**\n* #2736 Support adding columns to hypertables with compression enabled\n* #2909 Support renaming columns of hypertables with compression enabled\n\n## 2.0.2 (2021-02-19)\n\nThis maintenance release contains bugfixes since the 2.0.1 release. We\ndeem it high priority for upgrading.\n\nThe bug fixes in this release address issues with joins, the status of\nbackground jobs, and disabling compression. It also includes\nenhancements to continuous aggregates, including improved validation\nof policies and optimizations for faster refreshes when there are a\nlot of invalidations.\n\n**Minor features**\n* #2926 Optimize cagg refresh for small invalidations\n\n**Bugfixes**\n* #2850 Set status for backend in background jobs\n* #2883 Fix join qual propagation for nested joins\n* #2884 Add GUC to control join qual propagation\n* #2885 Fix compressed chunk check when disabling compression\n* #2908 Fix changing column type of clustered hypertables\n* #2942 Validate continuous aggregate policy\n\n**Thanks**\n* @zeeshanshabbir93 for reporting an issue with joins\n* @Antiarchitect for reporting the issue with slow refreshes of\n  continuous aggregates.\n* @diego-hermida for reporting the issue about being unable to disable\n  compression\n* @mtin for reporting the issue about wrong job status\n\n## 1.7.5 (2021-02-12)\n\nThis maintenance release contains bugfixes since the 1.7.4 release.\nMost of these fixes were backported from the 2.0.0 and 2.0.1 releases.\nWe deem it high priority for upgrading for users on TimescaleDB 1.7.4\nor previous versions.\n\nIn particular the fixes contained in this maintenance release address\nissues in continuous aggregates, compression, JOINs with hypertables,\nand when upgrading from previous versions.\n\n**Bugfixes**\n* #2502 Replace check function when updating\n* #2558 Repair dimension slice table on update\n* #2619 Fix segfault in decompress_chunk for chunks with dropped\n  columns\n* #2664 Fix support for complex aggregate expression\n* #2800 Lock dimension slices when creating new chunk\n* #2860 Fix projection in ChunkAppend nodes\n* #2865 Apply volatile function quals at decompresschunk\n* #2851 Fix nested loop joins that involve compressed chunks\n* #2868 Fix corruption in gapfill plan\n* #2883 Fix join qual propagation for nested joins\n* #2885 Fix compressed chunk check when disabling compression\n* #2920 Fix repair in update scripts\n\n**Thanks**\n* @akamensky for reporting several issues including segfaults after\n  version update\n* @alex88 for reporting an issue with joined hypertables\n* @dhodyn for reporting an issue when joining compressed chunks\n* @diego-hermida for reporting an issue with disabling compression\n* @Netskeh for reporting bug on time_bucket problem in continuous\n  aggregates\n* @WarriorOfWire for reporting the bug with gapfill queries not being\n  able to find pathkey item to sort\n* @zeeshanshabbir93 for reporting an issue with joins\n\n## 2.0.1 (2021-01-28)\n\nThis maintenance release contains bugfixes since the 2.0.0 release.\nWe deem it high priority for upgrading.\n\nIn particular the fixes contained in this maintenance release address\nissues in continuous aggregates, compression, JOINs with hypertables\nand when upgrading from previous versions.\n\n**Bugfixes**\n* #2772 Always validate existing database and extension\n* #2780 Fix config enum entries for remote data fetcher\n* #2806 Add check for dropped chunk on update\n* #2828 Improve cagg watermark caching\n* #2838 Fix catalog repair in update script\n* #2842 Do not mark job as started when setting next_start field\n* #2845 Fix continuous aggregate privileges during upgrade\n* #2851 Fix nested loop joins that involve compressed chunks\n* #2860 Fix projection in ChunkAppend nodes\n* #2861 Remove compression stat update from update script\n* #2865 Apply volatile function quals at decompresschunk node\n* #2866 Avoid partitionwise planning of partialize_agg\n* #2868 Fix corruption in gapfill plan\n* #2874 Fix partitionwise agg crash due to uninitialized memory\n\n**Thanks**\n* @alex88 for reporting an issue with joined hypertables\n* @brian-from-quantrocket for reporting an issue with extension update and dropped chunks\n* @dhodyn for reporting an issue when joining compressed chunks\n* @markatosi for reporting a segfault with partitionwise aggregates enabled\n* @PhilippJust for reporting an issue with add_job and initial_start\n* @sgorsh for reporting an issue when using pgAdmin on windows\n* @WarriorOfWire for reporting the bug with gapfill queries not being\n  able to find pathkey item to sort\n\n## 2.0.0 (2020-12-18)\n\nWith this release, we are officially moving TimescaleDB 2.0 to GA,\nconcluding several release candidates.\n\nTimescaleDB 2.0 adds the much-anticipated support for distributed\nhypertables (multi-node TimescaleDB), as well as new features and\nenhancements to core functionality to give users better clarity and\nmore control and flexibility over their data.\n\nMulti-node architecture:  In particular, with TimescaleDB 2.0, users\ncan now create distributed hypertables across multiple instances of\nTimescaleDB, configured so that one instance serves as an access node\nand multiple others as data nodes. All queries for a distributed\nhypertable are issued to the access node, but inserted data and queries\nare pushed down across data nodes for greater scale and performance.\n\nMulti-node TimescaleDB can be self managed or, for easier operation,\nlaunched within Timescale's fully-managed cloud services.\n\nThis release also adds:\n\n* Support for user-defined actions, allowing users to define,\n  customize, and schedule automated tasks, which can be run by the\n  built-in jobs scheduling framework now exposed to users.\n* Significant changes to continuous aggregates, which now separate the\n  view creation from the policy.  Users can now refresh individual\n  regions of the continuous aggregate materialized view, or schedule\n  automated refreshing via  policy.\n* Redesigned informational views, including new (and more general)\n  views for information about hypertable's dimensions and chunks,\n  policies and user-defined actions, as well as support for multi-node\n  TimescaleDB.\n* Moving all formerly enterprise features into our Community Edition,\n  and updating Timescale License, which now provides additional (more\n  permissive) rights to users and developers.\n\nSome of the changes above (e.g., continuous aggregates, updated\ninformational views) do introduce breaking changes to APIs and are not\nbackwards compatible. While the update scripts in TimescaleDB 2.0 will\nupgrade databases running TimescaleDB 1.x automatically, some of these\nAPI and feature changes may require changes to clients and/or upstream\nscripts that rely on the previous APIs.  Before upgrading, we recommend\nreviewing upgrade documentation at docs.timescale.com for more details.\n\n**Major Features**\n\nTimescaleDB 2.0 moves the following major features to GA:\n* #1923 Add support for distributed hypertables\n* #2006 Add support for user-defined actions\n* #2125 #2221 Improve Continuous Aggregate API\n* #2084 #2089 #2098 #2417 Redesign informational views\n* #2435 Move enterprise features to community\n* #2437 Update Timescale License\n\n**Previous Release Candidates**\n\n* #2702 Release Candidate 4 (December 2, 2020)\n* #2637 Release Candidate 3 (November 12, 2020)\n* #2554 Release Candidate 2 (October 20, 2020)\n* #2478 Release Candidate 1 (October 1, 2020)\n\n**Minor Features**\n\nSince the last release candidate 4, there are several minor\nimprovements:\n* #2746 Optimize locking for create chunk API\n* #2705 Block tableoid access on distributed hypertable\n* #2730 Do not allow unique index on compressed hypertables\n* #2764 Bootstrap data nodes with versioned extension\n\n**Bugfixes**\n\nSince the last release candidate 4, there are several bugfixes:\n* #2719 Support disabling compression on distributed hypertables\n* #2742 Fix compression status in chunks view for distributed chunks\n* #2751 Fix crash and cancel when adding data node\n* #2763 Fix check constraint on hypertable metadata table\n\n**Thanks**\n\nThanks to all contributors for the TimescaleDB 2.0 release:\n* @airton-neto for reporting a bug in executing some queries with UNION\n* @nshah14285 for reporting an issue with propagating privileges\n* @kalman5 for reporting an issue with renaming constraints\n* @LbaNeXte for reporting a bug in decompression for queries with\n  subqueries\n* @semtexzv for reporting an issue with continuous aggregates on\n  int-based hypertables\n* @mr-ns for reporting an issue with privileges for creating chunks\n* @cloud-rocket for reporting an issue with setting an owner on\n  continuous aggregate\n* @jocrau for reporting a bug during creating an index with transaction\n  per chunk\n* @fvannee for reporting an issue with custom time types\n* @ArtificialPB for reporting a bug in executing queries with\n  conditional ordering on compressed hypertable\n* @dutchgecko for reporting an issue with continuous aggregate datatype\n  handling\n* @lambdaq for suggesting to improve error message in continuous\n  aggregate creation\n* @francesco11112 for reporting memory issue on COPY\n* @Netskeh for reporting bug on time_bucket problem in continuous\n  aggregates\n* @mr-ns for reporting the issue with CTEs on distributed hypertables\n* @akamensky for reporting an issue with recursive cache invalidation\n* @ryanbooz for reporting slow queries with real-time aggregation on\n  continuous aggregates\n* @cevian for reporting an issue with disabling compression on\n  distributed hypertables\n\n## 2.0.0-rc4 (2020-12-02)\n\nThis release candidate contains bugfixes since the previous release\ncandidate, as well as additional minor features. It improves\nvalidation of configuration changes for background jobs, adds support\nfor gapfill on distributed tables, contains improvements to the memory\nhandling for large COPY, and contains improvements to compression for\ndistributed hypertables.\n\n**Minor Features**\n* #2689 Check configuration in alter_job and add_job\n* #2696 Support gapfill on distributed hypertable\n* #2468 Show more information in get_git_commit\n* #2678 Include user actions into job stats view\n* #2664 Fix support for complex aggregate expression\n* #2672 Add hypertable to continuous aggregates view\n* #2662 Save compression metadata settings on access node\n* #2707 Introduce additional db for data node bootstrapping\n\n**Bugfixes**\n* #2688 Fix crash for concurrent drop and compress chunk\n* #2666 Fix timeout handling in async library\n* #2683 Fix crash in add_job when given NULL interval\n* #2698 Improve memory handling for remote COPY\n* #2555 Set metadata for chunks compressed before 2.0\n\n**Thanks**\n* @francesco11112 for reporting memory issue on COPY\n* @Netskeh for reporting bug on time_bucket problem in continuous\n  aggregates\n\n## 2.0.0-rc3 (2020-11-12)\n\nThis release candidate contains bugfixes since the previous release\ncandidate, as well as additional minor features including support for\n\"user-mapping\" authentication between access/data nodes and an\nexperimental API for refreshing continuous aggregates on individual\nchunks.\n\n**Minor Features**\n* #2627 Add optional user mappings support\n* #2635 Add API to refresh continuous aggregate on chunk\n\n**Bugfixes**\n* #2560 Fix SCHEMA DROP CASCADE with continuous aggregates\n* #2593 Set explicitly all lock parameters in alter_job\n* #2604 Fix chunk creation on hypertables with foreign key constraints\n* #2610 Support analyze of internal compression table\n* #2612 Optimize internal cagg_watermark function\n* #2613 Refresh correct partial during refresh on drop\n* #2617 Fix validation of available extensions on data node\n* #2619 Fix segfault in decompress_chunk for chunks with dropped columns\n* #2620 Fix DROP CASCADE for continuous aggregate\n* #2625 Fix subquery errors when using AsyncAppend\n* #2626 Fix incorrect total_table_pages setting for compressed scan\n* #2628 Stop recursion in cache invalidation\n\n**Thanks**\n* @mr-ns for reporting the issue with CTEs on distributed hypertables\n* @akamensky for reporting an issue with recursive cache invalidation\n* @ryanbooz for reporting slow queries with real-time aggregation on\n  continuous aggregates\n\n## 2.0.0-rc2 (2020-10-21)\n\nThis release candidate contains bugfixes since the previous release candidate.\n\n**Minor Features**\n* #2520 Support non-transactional distibuted_exec\n\n**Bugfixes**\n* #2307 Overflow handling for refresh policy with integer time\n* #2503 Remove error for correct bootstrap of data node\n* #2507 Fix validation logic when adding a new data node\n* #2510 Fix outer join qual propagation\n* #2514 Lock dimension slices when creating new chunk\n* #2515 Add if_attached argument to detach_data_node()\n* #2517 Fix member access within misaligned address in chunk_update_colstats\n* #2525 Fix index creation on hypertables with dropped columns\n* #2543 Pass correct status to lock_job\n* #2544 Assume custom time type range is same as bigint\n* #2563 Fix DecompressChunk path generation\n* #2564 Improve continuous aggregate datatype handling\n* #2568 Change use of ssl_dir GUC\n* #2571 Make errors and messages conform to style guide\n* #2577 Exclude compressed chunks from ANALYZE/VACUUM\n\n## 2.0.0-rc1 (2020-10-06)\n\nThis release adds major new features and bugfixes since the 1.7.4 release.\nWe deem it moderate priority for upgrading.\n\nThis release adds the long-awaited support for distributed hypertables to\nTimescaleDB. With 2.0, users can create distributed hypertables across\nmultiple instances of TimescaleDB, configured so that one instance serves\nas an access node and multiple others as data nodes. All queries for a\ndistributed hypertable are issued to the access node, but inserted data\nand queries are pushed down across data nodes for greater scale and\nperformance.\n\nThis release also adds support for user-defined actions allowing users to\ndefine actions that are run by the TimescaleDB automation framework.\n\nIn addition to these major new features, the 2.0 branch introduces _breaking_ changes\nto APIs and existing features, such as continuous aggregates. These changes are not\nbackwards compatible and might require changes to clients and/or scripts that rely on\nthe previous APIs. Please review our updated documentation and do proper testing to\nensure compatibility with your existing applications.\n\nThe noticeable breaking changes in APIs are:\n- Redefined functions for policies\n- A continuous aggregate is now created with `CREATE MATERIALIZED VIEW`\n  instead of `CREATE VIEW` and automated refreshing requires adding a policy\n  via `add_continuous_aggregate_policy`\n- Redesign of informational views, including new (and more general) views for\n  information about policies and user-defined actions\n\nThis release candidate is upgradable, so if you are on a previous release (e.g., 1.7.4)\nyou can upgrade to the release candidate and later expect to be able to upgrade to the\nfinal 2.0 release. However, please carefully consider your compatibility requirements\n_before_ upgrading.\n\n**Major Features**\n* #1923 Add support for distributed hypertables\n* #2006 Add support for user-defined actions\n* #2435 Move enterprise features to community\n* #2437 Update Timescale License\n\n**Minor Features**\n* #2011 Constify TIMESTAMPTZ OP INTERVAL in constraints\n* #2105 Support moving compressed chunks\n\n**Bugfixes**\n* #1843 Improve handling of \"dropped\" chunks\n* #1886 Change ChunkAppend leader to use worker subplan\n* #2116 Propagate privileges from hypertables to chunks\n* #2263 Fix timestamp overflow in time_bucket optimization\n* #2270 Fix handling of non-reference counted TupleDescs in gapfill\n* #2325 Fix rename constraint/rename index\n* #2370 Fix detection of hypertables in subqueries\n* #2376 Fix caggs width expression handling on int based hypertables\n* #2416 Check insert privileges to create chunk\n* #2428 Allow owner change of continuous aggregate\n* #2436 Propagate grants in continuous aggregates\n\n## 2.0.0-beta6 (2020-09-14)\n\n**For beta releases**, upgrading from an earlier version of the\nextension (including previous beta releases) is not supported.\n\nThis beta release includes breaking changes to APIs. The most\nnotable changes since the beta-5 release are the following, which will\nbe reflected in forthcoming documentation for the 2.0 release.\n\n* Existing information views were reorganized. Retrieving information\nabout sizes and statistics was moved to functions. New views were added\nto expose information, which was previously available only internally.\n* New ability to create custom jobs was added.\n* Continuous aggregate API was redesigned. Its policy creation is separated\nfrom the view creation.\n* compress_chunk_policy and drop_chunk_policy were renamed to compression_policy and\nretention_policy.\n\n## 1.7.4 (2020-09-07)\n\nThis maintenance release contains bugfixes since the 1.7.3 release. We deem it\nhigh priority for upgrading if TimescaleDB is deployed with replicas (synchronous\nor asynchronous).\n\nIn particular the fixes contained in this maintenance release address an issue with\nrunning queries on compressed hypertables on standby nodes.\n\n**Bugfixes**\n* #2340 Remove tuple lock on select path\n\n## 1.7.3 (2020-07-27)\n\nThis maintenance release contains bugfixes since the 1.7.2 release. We deem it high\npriority for upgrading.\n\nIn particular the fixes contained in this maintenance release address issues in compression,\ndrop_chunks and the background worker scheduler.\n\n**Bugfixes**\n* #2059 Improve infering start and stop arguments from gapfill query\n* #2067 Support moving compressed chunks\n* #2068 Apply SET TABLESPACE for compressed chunks\n* #2090 Fix index creation with IF NOT EXISTS for existing indexes\n* #2092 Fix delete on tables involving hypertables with compression\n* #2164 Fix telemetry installed_time format\n* #2184 Fix background worker scheduler memory consumption\n* #2222 Fix `negative bitmapset member not allowed` in decompression\n* #2255 Propagate privileges from hypertables to chunks\n* #2256 Fix segfault in chunk_append with space partitioning\n* #2259 Fix recursion in cache processing\n* #2261 Lock dimension slice tuple when scanning\n\n**Thanks**\n* @akamensky for reporting an issue with drop_chunks and ChunkAppend with space partitioning\n* @dewetburger430 for reporting an issue with setting tablespace for compressed chunks\n* @fvannee for reporting an issue with cache invalidation\n* @nexces for reporting an issue with ChunkAppend on space-partitioned hypertables\n* @PichetGoulu for reporting an issue with index creation and IF NOT EXISTS\n* @prathamesh-sonpatki for contributing a typo fix\n* @sezaru for reporting an issue with background worker scheduler memory consumption\n\n## 1.7.2 (2020-07-07)\n\nThis maintenance release contains bugfixes since the 1.7.1 release. We deem it medium\npriority for upgrading.\n\nIn particular the fixes contained in this maintenance release address bugs in continuous\naggregates, drop_chunks and compression.\n\n**Features**\n* #1877 Add support for fast pruning of inlined functions\n\n**Bugfixes**\n* #1908 Fix drop_chunks with unique constraints when cascade_to_materializations is false\n* #1915 Check for database in extension_current_state\n* #1918 Unify chunk index creation\n* #1932 Change compression locking order\n* #1938 Fix gapfill locf treat_null_as_missing\n* #1982 Check for disabled telemetry earlier\n* #1984 Fix compression bit array left shift count\n* #1997 Add checks for read-only transactions\n* #2002 Reset restoring gucs rather than explicitly setting 'off'\n* #2028 Fix locking in drop_chunks\n* #2031 Enable compression for tables with compound foreign key\n* #2039 Fix segfault in create_trigger_handler\n* #2043 Fix segfault in cagg_update_view_definition\n* #2046 Use index tablespace during chunk creation\n* #2047 Better handling of chunk insert state destruction\n* #2049 Fix handling of PlaceHolderVar in DecompressChunk\n* #2051 Fix tuple concurrently deleted error with multiple continuous aggregates\n\n**Thanks**\n* @akamensky for reporting an issue with telemetry and an issue with drop_chunks\n* @darko408 for reporting an issue with decompression\n* @dmitri191 for reporting an issue with failing background workers\n* @eduardotsj for reporting an issue with indexes not inheriting tablespace settings\n* @fourseventy for reporting an issue with multiple continuous aggregrates\n* @fvannee for contributing optimizations for pruning inlined functions\n* @jflambert for reporting an issue with failing telemetry jobs\n* @nbouscal for reporting an issue with compression jobs locking referenced tables\n* @nicolai6120 for reporting an issue with locf and treat_null_as_missing\n* @nomanor for reporting an issue with expression index with table references\n* @olernov for contributing a fix for compressing tables with compound foreign keys\n* @werjo for reporting an issue with drop_chunks and unique constraints\n\n## 2.0.0-beta5 (2020-06-08)\n\nThis release adds new functionality on distributed hypertables,\nincluding (but not limited to) basic LIMIT pushdown, manual chunk\ncompression, table access methods storage options,  SERIAL columns,\nand altering of the replication factor.\nThis release only supports PG11 and PG12. Thus, PG9.6 and PG10\nare no longer supported.\n\nNote that the 2.0 major release will introduce breaking changes\nto user functions and APIs. In particular, this beta removes the\ncascade parameter from drop_chunks and changes the names of\ncertain GUC parameters. Expect additional breaking changes to be\nintroduced up until the 2.0 release.\n\n**For beta releases**, upgrading from an earlier version of the\nextension (including previous beta releases) is not supported.\n\n**Features**\n\n* #1877 Add support for fast pruning of inlined functions\n* #1922 Cleanup GUC names\n* #1923 Add repartition option on detach/delete_data_node\n* #1923 Allow ALTER TABLE SET on distributed hypertable\n* #1923 Allow SERIAL columns for distributed hypertables\n* #1923 Basic LIMIT push down support\n* #1923 Implement altering replication factor\n* #1923 Support compression on distributed hypertables\n* #1923 Support storage options for distributed hypertables\n* #1941 Change default prefix for distributed tables\n* #1943 Support table access methods for distributed hypertables\n* #1952 Remove cascade option from drop_chunks\n* #1955 Remove support for PG9.6 and PG10\n\n**Bugfixes**\n* #1915 Check for database in extension_current_state\n* #1918 Unify chunk index creation\n* #1923 Fix insert batch size calculation for prepared statements\n* #1923 Fix port conversion issue in add_data_node\n* #1932 Change compression locking order\n* #1938 Fix gapfill locf treat_null_as_missing\n\n**Thanks**\n* @dmitri191 for reporting an issue with failing background workers\n* @fvannee for optimizing pruning of inlined functions\n* @nbouscal for reporting an issue with compression jobs locking referenced tables\n* @nicolai6120 for reporting an issue with locf and treat_null_as_missing\n* @nomanor for reporting an issue with expression index with table references\n\n## 1.7.1 (2020-05-18)\n\nThis maintenance release contains bugfixes since the 1.7.0 release. We deem it medium\npriority for upgrading and high priority for users with multiple continuous aggregates.\n\nIn particular the fixes contained in this maintenance release address bugs in continuous\naggregates with real-time aggregation and PostgreSQL 12 support.\n\n**Bugfixes**\n* #1834 Define strerror() for Windows\n* #1846 Fix segfault on COPY to hypertable\n* #1850 Fix scheduler failure due to bad next_start_time for jobs\n* #1851 Fix hypertable expansion for UNION ALL\n* #1854 Fix reorder policy job to skip compressed chunks\n* #1861 Fix qual pushdown for compressed hypertables where quals have casts\n* #1864 Fix issue with subplan selection in parallel ChunkAppend\n* #1868 Add support for WHERE, HAVING clauses with real time aggregates\n* #1869 Fix real time aggregate support for multiple continuous aggregates\n* #1871 Don't rely on timescaledb.restoring for upgrade\n* #1875 Fix hypertable detection in subqueries\n* #1884 Fix crash on SELECT WHERE NOT with empty table\n\n**Thanks**\n* @airton-neto for reporting an issue with queries over UNIONs of hypertables\n* @dhodyn for reporting an issue with UNION ALL queries\n* @frostwind for reporting an issue with casts in where clauses on compressed hypertables\n* @fvannee for reporting an issue with hypertable detection in inlined SQL functions and an issue with COPY\n* @hgiasac for reporting missing where clause with real time aggregates\n* @louisth for reporting an issue with real-time aggregation and multiple continuous aggregates\n* @michael-sayapin for reporting an issue with INSERTs and WHERE NOT EXISTS\n* @olernov for reporting and fixing an issue with compressed chunks in the reorder policy\n* @pehlert for reporting an issue with pg_upgrade\n\n## 1.7.0 (2020-04-16)\n\nThis release adds major new features and bugfixes since the 1.6.1 release.\nWe deem it moderate priority for upgrading.\n\nThis release adds the long-awaited support for PostgreSQL 12 to TimescaleDB.\n\nThis release also adds a new default behavior when querying continuous\naggregates that we call real-time aggregation. A query on a continuous\naggregate will now combine materialized data with recent data that has\nyet to be materialized.\n\nNote that only newly created continuous aggregates will have this\nreal-time query behavior, although it can be enabled on existing\ncontinuous aggregates with a configuration setting as follows:\n\nALTER VIEW continuous_view_name SET (timescaledb.materialized_only=false);\n\nThis release also moves several data management lifecycle features\nto the Community version of TimescaleDB (from Enterprise), including\ndata reordering and data retention policies.\n\n**Major Features**\n* #1456 Add support for PostgreSQL 12\n* #1685 Add support for real-time aggregation on continuous aggregates\n\n**Bugfixes**\n* #1665 Add ignore_invalidation_older_than to timescaledb_information.continuous_aggregates view\n* #1750 Handle undefined ignore_invalidation_older_than\n* #1757 Restrict watermark to max for continuous aggregates\n* #1769 Add rescan function to CompressChunkDml CustomScan node\n* #1785 Fix last_run_success value in continuous_aggregate_stats view\n* #1801 Include parallel leader in plan execution\n* #1808 Fix ts_hypertable_get_all for compressed tables\n* #1828 Ignore dropped chunks in compressed_chunk_stats\n\n**Licensing changes**\n* Reorder and policies around reorder and drop chunks are now\n  accessible to community users, not just enterprise\n* Gapfill functionality no longer warns about expired license\n\n**Thanks**\n\n* @t0k4rt for reporting an issue with parallel chunk append plans\n* @alxndrdude for reporting an issue when trying to insert into compressed chunks\n* @Olernov for reporting and fixing an issue with show_chunks and drop_chunks for compressed hypertables\n* @mjb512 for reporting an issue with INSERTs in CTEs in cached plans\n* @dmarsh19 for reporting and fixing an issue with dropped chunks in compressed_chunk_stats\n\n## 1.6.1 (2020-03-18)\n\nThis maintenance release contains bugfixes since the 1.6.0 release. We deem it medium\npriority for upgrading.\n\nIn particular the fixes contained in this maintenance release address bugs in continuous\naggregates, time_bucket_gapfill, partial index handling and drop_chunks.\n\n**For this release only**, you will need to restart the database after upgrade before\nrestoring a backup.\n\n**Minor Features**\n* #1666 Support drop_chunks API for continuous aggregates\n* #1711 Change log level for continuous aggregate materialization messages\n\n**Bugfixes**\n* #1630 Print notice for COPY TO on hypertable\n* #1648 Drop chunks from materialized hypertable\n* #1668 Cannot add dimension if hypertable has empty chunks\n* #1673 Fix crash when interrupting create_hypertable\n* #1674 Fix time_bucket_gapfill's interaction with GROUP BY\n* #1686 Fix order by queries on compressed hypertables that have char segment by column\n* #1687 Fix issue with disabling compression when foreign keys are present\n* #1688 Handle many BGW jobs better\n* #1698 Add logic to ignore dropped chunks in hypertable_relation_size\n* #1704 Fix bad plan for continuous aggregate materialization\n* #1709 Prevent starting background workers with NOLOGIN\n* #1713 Fix miscellaneous background worker issues\n* #1715 Fix issue with overly aggressive chunk exclusion in outer joins\n* #1719 Fix restoring/scheduler entrypoint to avoid BGW death\n* #1720 Add scheduler cache invalidations\n* #1727 Fix compressing INTERVAL columns\n* #1728 Handle Sort nodes in ConstraintAwareAppend\n* #1730 Fix partial index handling on hypertables\n* #1739 Use release OpenSSL DLLs for debug builds on Windows\n* #1740 Fix invalidation entries from multiple caggs on same hypertable\n* #1743 Fix continuous aggregate materialization timezone handling\n* #1748 Fix remove_drop_chunks_policy for continuous aggregates\n* #1756 Fix handling of dropped chunks in compression background worker\n\n**Thanks**\n* @RJPhillips01 for reporting an issue with drop chunks.\n* @b4eEx for reporting an issue with disabling compression.\n* @darko408 for reporting an issue with order by on compressed hypertables\n* @mrechte for reporting an issue with compressing INTERVAL columns\n* @tstaehli for reporting an issue with ConstraintAwareAppend\n* @chadshowalter for reporting an issue with partial index on hypertables\n* @geoffreybennett for reporting an issue with create_hypertable when interrupting operations\n* @alxndrdude for reporting an issue with background workers during restore\n* @zcavaliero for reporting and fixing an issue with dropped columns in hypertable_relation_size\n* @ismailakpolat for reporting an issue with cagg materialization on hypertables with TIMESTAMP column\n\n## 1.6.0 (2020-01-14)\n\nThis release adds major new features and bugfixes since the 1.5.1 release.\nWe deem it moderate priority for upgrading.\n\nThe major new feature in this release allows users to keep the aggregated\ndata in a continuous aggregate while dropping the raw data with drop_chunks.\nThis allows users to save storage by keeping only the aggregates.\n\nThe semantics of the refresh_lag parameter for continuous aggregates has\nbeen changed to be relative to the current timestamp instead of the maximum\nvalue in the table. This change requires that an integer_now func be set on\nhypertables with integer-based time columns to use continuous aggregates on\nthis table.\n\nWe added a timescaledb.ignore_invalidation_older_than parameter for continuous\naggregates. This parameter accept a time-interval (e.g. 1 month). If set,\nit limits the amount of time for which to process invalidation. Thus, if\ntimescaledb.ignore_invalidation_older_than = '1 month', then any modifications\nfor data older than 1 month from the current timestamp at modification time may\nnot cause continuous aggregate to be updated. This limits the amount of work\nthat a backfill can trigger. By default, all invalidations are processed.\n\n**Major Features**\n* #1589 Allow drop_chunks while keeping continuous aggregates\n\n**Minor Features**\n* #1568 Add ignore_invalidation_older_than option to continuous aggs\n* #1575 Reorder group-by clause for continuous aggregates\n* #1592 Improve continuous agg user messages\n\n**Bugfixes**\n* #1565 Fix partial select query for continuous aggregate\n* #1591 Fix locf treat_null_as_missing option\n* #1594 Fix error in compression constraint check\n* #1603 Add join info to compressed chunk\n* #1606 Fix constify params during runtime exclusion\n* #1607 Delete compression policy when drop hypertable\n* #1608 Add jobs to timescaledb_information.policy_stats\n* #1609 Fix bug with parent table in decompression\n* #1624 Fix drop_chunks for ApacheOnly\n* #1632 Check for NULL before dereferencing variable\n\n**Thanks**\n* @optijon for reporting an issue with locf treat_null_as_missing option\n* @acarrera42 for reporting an issue with constify params during runtime exclusion\n* @ChristopherZellermann for reporting an issue with the compression constraint check\n* @SimonDelamare for reporting an issue with joining hypertables with compression\n\n## 2.0.0-beta4 (2019-12-19)\n\n**For beta releases**, upgrading from an earlier version of the\nextension (including previous beta releases) is not supported.\n\nThis release includes user experience improvements for managing data\nnodes, more efficient statistics collection for distributed\nhypertables, and miscellaneous fixes and improvements.\n\n## 1.5.1 (2019-11-12)\n\nThis maintenance release contains bugfixes since the 1.5.0 release. We deem it low\npriority for upgrading.\n\nIn particular the fixes contained in this maintenance release address potential\nsegfaults and no other security vulnerabilities. The bugfixes are related to bloom\nindexes and updates from previous versions.\n\n**Bugfixes**\n* #1523 Fix bad SQL updates from previous updates\n* #1526 Fix hypertable model\n* #1530 Set active snapshots in multi-xact index create\n\n**Thanks**\n* @84660320 for reporting an issue with bloom indexes\n* @gumshoes @perhamm @jermudgeon @gmisagm for reporting the issue with updates\n\n## 2.0.0-beta3 (2019-11-05)\n\n**For beta releases**, upgrading from an earlier version of the\nextension (including previous beta releases) is not supported.\n\nThis release improves performance for queries executed on distributed\nhypertables, fixes minor issues and blocks a number of SQL API\nfunctions, which are not supported on distributed hypertables. It also\nadds information about distributed databases in the telemetry.\n\n## 2.0.0-beta2 (2019-10-22)\n\nThis release introduces *distributed hypertables*, a major new\nfeature that allows hypertables to scale out across multiple nodes for\nincreased performance and fault tolerance. Please review the\ndocumentation to learn how to configure and use distributed\nhypertables and what current limitations are.\n\n## 1.5.0 (2019-10-31)\n\nThis release adds major new features and bugfixes since the 1.4.2 release.\nWe deem it moderate priority for upgrading.\n\nThis release adds compression as a major new feature.\nMultiple type-specific compression options are available in this release\n(including DeltaDelta with run-length-encoding for integers and\ntimestamps; Gorilla compression for floats; dictionary-based compression\nfor any data type, but specifically for low-cardinality datasets;\nand other LZ-based techniques). Individual columns can be compressed with\ntype-specific compression algorithms as Postgres' native row-based format\nare rolled up into columnar-like arrays on a per chunk basis.\nThe query planner then handles transparent decompression for compressed\nchunks at execution time.\n\nThis release also adds support for basic data tiering by supporting\nthe migration of chunks between tablespaces, as well as support for\nparallel query coordination to the ChunkAppend node.\nPreviously ChunkAppend would rely on parallel coordination in the\nunderlying scans for parallel plans.\n\nMore information can be found on [our blog](https://blog.timescale.com/blog/building-columnar-compression-in-a-row-oriented-database)\nor in this [tutorial](https://docs.timescale.com/latest/tutorials/compression-tutorial)\n\n**For this release only**, you will need to restart the database before running\n`ALTER EXTENSION`\n\n**Major Features**\n* #1393 Moving chunks between different tablespaces\n* #1433 Make ChunkAppend parallel aware\n* #1434 Introducing native compression, multiple compression algorithms, and hybrid row/columnar projections\n\n**Minor Features**\n* #1471 Allow setting reloptions on chunks\n* #1479 Add next_start option to alter_job_schedule\n* #1481 Add last_successful_finish to bgw_job_stats\n\n**Bugfixes**\n* #1444 Prevent LIMIT pushdown in JOINs\n* #1447 Fix runtime exclusion memory leak\n* #1464 Fix ordered append with expressions in ORDER BY clause with space partitioning\n* #1476 Fix logic for BGW rescheduling\n* #1477 Fix gapfill treat_null_as_missing\n* #1493 Prevent recursion in invalidation processing\n* #1498 Fix overflow in gapfill's interpolate\n* #1499 Fix error for exported_uuid in pg_restore\n* #1503 Fix bug with histogram function in parallel\n\n**Thanks**\n* @dhyun-obsec for reporting an issue with pg_restore\n* @rhaymo for reporting an issue with interpolate\n* @optijon for reporting an issue with locf treat_null_as_missing\n* @fvannee for reporting an issue with runtime exclusion\n* @Lectem for reporting an issue with histograms\n* @rogerdwan for reporting an issue with BGW rescheduling\n* @od0 for reporting an issue with alter_job_schedule\n\n## 1.4.2 (2019-09-11)\n\nThis maintenance release contains bugfixes since the 1.4.1 release. We deem it medium\npriority for upgrading.\n\nIn particular the fixes contained in this maintenance release address 2 potential\nsegfaults and no other security vulnerabilities. The bugfixes are related to\nbackground workers, OUTER JOINs, ordered append on space partitioned hypertables\nand expression indexes.\n\n**Bugfixes**\n* #1327 Fix chunk exclusion with ordered append\n* #1390 Fix deletes of background workers while a job is running\n* #1392 Fix cagg_agg_validate expression handling (segfault)\n* #1408 Fix ChunkAppend space partitioning support for ordered append\n* #1420 Fix OUTER JOIN qual propagation\n* #1422 Fix background worker error handling (segfault)\n* #1424 Fix ChunkAppend LIMIT pushdown\n* #1429 Fix expression index creation\n\n**Thanks**\n* @shahidhk for reporting an issue with OUTER JOINs\n* @cossbow and @xxGL1TCHxx for reporting reporting issues with ChunkAppend and space partitioning\n* @est for reporting an issue with CASE expressions in continuous aggregates\n* @devlucasc for reporting the issue with deleting a background worker while a job is running\n* @ryan-shaw for reporting an issue with expression indexes on hypertables with dropped columns\n\n## 1.4.1 (2019-08-01)\n\nThis maintenance release contains bugfixes since the 1.4.0 release. We deem it medium\npriority for upgrading.\n\nIn particular the fixes contained in this maintenance release address 2 potential\nsegfaults and no other security vulnerabilities. The bugfixes are related to queries\nwith prepared statements, PL/pgSQL functions and interoperability with other extensions.\nMore details below.\n\n**Bugfixes**\n* #1362 Fix ConstraintAwareAppend subquery exclusion\n* #1363 Mark drop_chunks as VOLATILE and not PARALLEL SAFE\n* #1369 Fix ChunkAppend with prepared statements\n* #1373 Only allow PARAM_EXTERN as time_bucket_gapfill arguments\n* #1380 Handle Result nodes gracefully in ChunkAppend\n\n**Thanks**\n* @overhacked for reporting an issue with drop_chunks and parallel queries\n* @fvannee for reporting an issue with ConstraintAwareAppend and subqueries\n* @rrb3942 for reporting a segfault with ChunkAppend and prepared statements\n* @mchesser for reporting a segfault with time_bucket_gapfill and subqueries\n* @lolizeppelin for reporting and helping debug an issue with ChunkAppend and Result nodes\n\n## 1.4.0 (2019-07-18)\n\nThis release contains major new functionality for continuous aggregates\nand adds performance improvements for analytical queries.\n\nIn version 1.3.0 we added support for continuous aggregates which\nwas initially limited to one continuous aggregate per hypertable.\nWith this release, we remove this restriction and allow multiple\ncontinuous aggregates per hypertable.\n\nThis release adds a new custom node ChunkAppend that can perform\nexecution time constraint exclusion and is also used for ordered\nappend. Ordered append no longer requires a LIMIT clause and now\nsupports space partitioning and ordering by time_bucket.\n\n**Major features**\n* #1270 Use ChunkAppend to replace Append nodes\n* #1257 Support for multiple continuous aggregates\n\n**Minor features**\n* #1181 Remove LIMIT clause restriction from ordered append\n* #1273 Propagate quals to joined hypertables\n* #1317 Support time bucket functions in Ordered Append\n* #1331 Add warning message for REFRESH MATERIALIZED VIEW\n* #1332 Add job statistics columns to timescaledb_information.continuous_aggregate_stats view\n* #1326 Add architecture and bit size to telemetry\n\n**Bugfixes**\n* #1288 Do not remove Result nodes with one-time filter\n* #1300 Fix telemetry report return value\n* #1339 Fix continuous agg catalog table insert failure\n* #1344 Update continuous agg bgw job start time\n\n**Thanks**\n* @ik9999 for reporting a bug with continuous aggregates and negative refresh lag\n\n## 1.3.2 (2019-06-24)\n\nThis maintenance release contains bug and security fixes since the 1.3.1 release. We deem it moderate-to-high priority for upgrading.\n\nThis release fixes some security vulnerabilities, specifically related to being able to elevate role-based permissions by database users that already have access to the database.  We strongly recommend that users who rely on role-based permissions upgrade to this release as soon as possible.\n\n**Security Fixes**\n* #1311 Fix role-based permission checking logic\n\n**Bugfixes**\n* #1315 Fix potentially lost invalidations in continuous aggs\n* #1303 Fix handling of types with custom time partitioning\n* #1299 Arm32: Fix Datum to int cast issue\n* #1297 Arm32: Fix crashes due to long handling\n* #1019 Add ARM32 tests on travis\n\n**Thanks**\n* @hedayat for reporting the error with handling of types with custom time partitioning\n\n## 1.3.1 (2019-06-10)\n\nThis maintenance release contains bugfixes since the 1.3.0 release.\nWe deem it low-to-moderate priority for upgrading.\n\nIn particular, the fixes contained in this maintenance release do not address any\nsecurity vulnerabilities, while the only one affecting system stability is related\nto TimescaleDB running on PostgreSQL 11.  More details below.\n\n**Bugfixes**\n* #1220 Fix detecting JOINs for continuous aggs\n* #1221 Fix segfault in VACUUM on PG11\n* #1228 ARM32 Fix: Pass int64 using Int64GetDatum when a Datum is required\n* #1232 Allow Param as time_bucket_gapfill arguments\n* #1236 Stop preventing REFRESH in transaction blocks\n* #1283 Fix constraint exclusion for OUTER JOIN\n\n**Thanks**\n* @od0 for reporting an error with continuous aggs and JOINs\n* @rickbatka for reporting an error when using time_bucket_gapfill in functions\n* @OneMoreSec for reporting the bug with VACUUM\n* @dvdrozdov @od0 @t0k4rt for reporting the issue with REFRESH in transaction blocks\n* @mhagander and @devrimgunduz for suggesting adding a CMAKE flag to control the default telemetry level\n\n## 1.3.0 (2019-05-06)\n\nThis release contains major new functionality that we call continuous aggregates.\n\nAggregate queries which touch large swathes of time-series data can take a long\ntime to compute because the system needs to scan large amounts of data on every\nquery execution. Our continuous aggregates continuously calculate the\nresults of a query in the background and materialize the results. Queries to the\ncontinuous aggregate view are then significantly faster as they do not need to\ntouch the raw data in the hypertable, instead using the pre-computed aggregates\nin the view.\n\nContinuous aggregates are somewhat similar to PostgreSQL materialized\nviews, but unlike a materialized view, continuous\naggregates do not need to be refreshed manually; the view will be refreshed\nautomatically in the background as new data is added, or old data is\nmodified. Additionally, it does not need to re-calculate all of the data on\nevery refresh. Only new and/or invalidated data will be calculated.  Since this\nre-aggregation is automatic, it doesnt add any maintenance burden to your\ndatabase.\n\nOur continuous aggregate approach supports high-ingest rates by avoiding the\nhigh-write amplification associated with trigger-based approaches. Instead,\nwe use invalidation techniques to track what data has changed, and then correct\nthe materialized aggregate the next time that the automated process executes.\n\nMore information can be found on [our docs overview](http://docs.timescale.com/using-timescaledb/continuous-aggregates)\nor in this [tutorial](http://docs.timescale.com/tutorials/continuous-aggs-tutorial).\n\n**Major Features**\n* #1184 Add continuous aggregate functionality\n\n**Minor Features**\n* #1005 Enable creating indexes with one transaction per chunk\n* #1007 Remove hypertable parent from query plans\n* #1038 Infer time_bucket_gapfill arguments from WHERE clause\n* #1062 Make constraint aware append parallel safe\n* #1067 Add treat_null_as_missing option to locf\n* #1112 Add support for window functions to gapfill\n* #1130 Add support for cross datatype chunk exclusion for time types\n* #1134 Add support for partitionwise aggregation\n* #1153 Add time_bucket support to chunk exclusion\n* #1170 Add functions for turning restoring on/off and setting license key\n* #1177 Add transformed time_bucket comparison to quals\n* #1182 Enable optimizing SELECTs within INSERTs\n* #1201 Add telemetry for policies: drop_chunk & reorder\n\n**Bugfixes**\n* #1010 Add session locks to CLUSTER\n* #1115 Fix ordered append optimization for join queries\n* #1123 Fix gapfill with prepared statements\n* #1125 Fix column handling for columns derived from GROUP BY columns\n* #1132 Adjust ordered append path cost\n* #1155 Limit initial max_open_chunks_per_insert to PG_INT16_MAX\n* #1167 Fix postgres.conf ApacheOnly license\n* #1183 Handle NULL in a check constraint name\n* #1195 Fix cascade in scheduled drop chunks\n* #1196 Fix UPSERT with prepared statements\n\n**Thanks**\n* @spickman for reporting a segfault with ordered append and JOINs\n* @comicfans for reporting a performance regression with ordered append\n* @Specter-Y for reporting a segfault with UPSERT and prepared statements\n* @erthalion submitting a bugfix for a segfault with validating check constraints\n\n## 1.2.2 (2019-03-14)\n\nThis release contains bugfixes.\n\n**Bugfixes**\n* #1097 Adjust ordered append plan cost\n* #1079 Stop background worker on ALTER DATABASE SET TABLESPACE and CREATE DATABASE WITH TEMPLATE\n* #1088 Fix ON CONFLICT when using prepared statements and functions\n* #1089 Fix compatibility with extensions that define planner_hook\n* #1057 Fix chunk exclusion constraint type inference\n* #1060 Fix sort_transform optimization\n\n**Thanks**\n* @esatterwhite for reporting a bug when using timescaledb with zombodb\n* @eeeebbbbrrrr for fixing compatibility with extensions that also define planner_hook\n* @naquad for reporting a segfault when using ON conflict in stored procedures\n* @aaronkaplan for reporting an issue with ALTER DATABASE SET TABLESPACE\n* @quetz for reporting an issue with CREATE DATABASE WITH TEMPLATE\n* @nbouscal for reporting an issue with ordered append resulting in bad plans\n\n## 1.2.1 (2019-02-11)\n\nThis release contains bugfixes.\n\n**Notable commits**\n* [2f6b58a] Fix tlist on hypertable inserts inside CTEs\n* [7973b4a] Stop background worker on rename database\n* [32cc645] Fix loading the tsl license in parallel workers\n\n**Thanks**\n\n* @jamessewell for reporting and helping debug a segfault in last() [034a0b0]\n* @piscopoc for reporting a segfault in time_bucket_gapfill [e6c68f8]\n\n## 1.2.0 (2019-01-29)\n\n**This is our first release to include Timescale-Licensed features, in addition to new Apache-2 capabilities.**\n\nWe are excited to be introducing new time-series analytical functions, advanced data lifecycle management capabilities, and improved performance.\n- **Time-series analytical functions**: Users can now use our `time_bucket_gapfill` function, to write complex gapfilling, last object carried forward, and interpolation queries.\n- **Advanced data lifecycle management**: We are introducing scheduled policies, which use our background worker framework to manage time-series data. In this release, we support scheduled `drop_chunks` and `reorder`.\n- **Improved performance**: We added support for ordered appends, which optimize a large range of queries - particularly those that are ordered by time and contain a LIMIT clause. Please note that ordered appends do not support ordering by `time_bucket` at this time.\n- **Postgres 11 Support**: We added beta support for PG11 in 1.1.0. We're happy to announce that our PG11 support is now out of beta, and fully supported.\n\nThis release adds code under a new license, LICENSE_TIMESCALE. This code can be found in `tsl`.\n\n**For this release only**, you will need to restart the database before running\n`ALTER EXTENSION`\n\n**Notable commits**\n\n* [a531733] switch cis state when we switch chunks\n* [5c6b619] Make a copy of the ri_onConflict object in PG11\n* [61e524e] Make slot for upserts be update for every chunk switch\n* [8a7c127] Fix for ExecSlotDescriptor during upserts\n* [fa61613] Change time_bucket_gapfill argument names\n* [01be394] Fix bgw_launcher restart when failing during launcher setup\n* [7b3929e] Add ordered append optimization\n* [a69f84c] Fix signal processing in background workers\n* [47b5b7d] Log which chunks are dropped by background workers\n* [4e1e15f] Add reorder command\n* [2e4bb5d] Recluster and drop chunks scheduling code\n* [ef43e52] Add alter_policy_schedule API function\n* [5ba740e] Add gapfill query support\n* [be7c74c] Add logic for automatic DB maintenance functions\n* [4ff6ac7] Initial Timescale-Licensed-Module and License-Key Implementation\n* [fc42539] Add new top-level licensing information\n* [31e9c5b] Fix time column handling in get_create_command\n* [1b8ceca] Avoid loading twice in parallel workers and load only from $libdir\n* [76d7875] Don't throw errors when extension is loaded but not installed yet\n* [eecd845] Add Timescale License (TSL)\n* [4b42b30] Free ChunkInsertStates when the es_per_tuple_exprcontext is freed\n\n**Thanks**\n\n* @fordred for reporting our docker-run.sh script was out of date\n* @JpWebster for reporting a deadlock between reads an drop_chunks\n* @chickenburgers for reporting an issue with our CMake\n* Dimtrj and Asbjrn D., on slack, for creating a reproducible testcase for an UPSERT bug\n* @skebanga for reporting a loader bug\n\n\n## 1.1.1 (2018-12-20)\n\nThis release contains bugfixes.\n\n**High-level changes**\n* Fix issue when upgrading with pg_upgrade\n* Fix a segfault that sometimes appeared in long COPYs\n* Other bug and stability fixes\n\n**Notable commits**\n\n* [f99b540] Avoid loading twice in parallel workers and load only from $libdir\n* [e310f7d] Don't throw errors when extension is loaded but not installed yet\n* [8498416] Free ChunkInsertStates when the es_per_tuple_exprcontext is freed\n* [937eefe] Set C standard to C11\n\n**Thanks**\n\n* @costigator for reporting the pg_upgrade bug\n* @FireAndIce68 for reporting the parallel workers issue\n* @damirda for reporting the copy bug\n\n## 1.1.0 (2018-12-13)\n\nOur 1.1 release introduces beta support for PG 11, as well as several performance optimizations aimed at improving chunk exclusion for read queries. We are also packaging our new timescale-tune tool (currently in beta) with our Debian and Linux releases. If you encounter any issues with our beta features, please file a Github issue.\n\n**Potential breaking changes**\n- In addition to optimizing first() / last() to utilize indices for non-group-by queries, we adjusted its sorting behavior to match that of PostgreSQLs max() and min() functions. Previously, if the column being sorted had NULL values, a NULL would be returned. First() and last() now instead ignore NULL values.\n\n**Notable Commits**\n\n* [71f3a0c] Fix Datum conversion issues\n* [5aa1eda] Refactor compatibility functions and code to support PG11\n* [e4a4f8e] Add support for functions on open (time) dimensions\n* [ed5067c] Fix interval_from_now_to_internal timestamptz handling\n* [019971c] Optimize FIRST/LAST aggregate functions\n* [83014ee] Implement drop_chunks in C\n* [9a34028] Implement show_chunks in C and have drop_chunks use it\n* [d461959] Add view to show hypertable information\n* [35dee48] Remove version-checking from client-side\n* [5b6a5f4] Change size utility and job functions to STRICT\n* [7e55d91] Add checks for NULL arguments to DDL functions\n* [c1db608] Fix upsert TLE translation when mapping variable numbers\n* [55a378e] Check extension exists for DROP OWNED and DROP EXTENSION\n* [0c8c085] Exclude unneeded chunks for IN/ANY/ALL operators\n* [f27c0a3] Move int time_bucket functions with offset to C\n\n**Thanks**\n* @did-g for some memory improvements\n\n## 1.0.1 (2018-12-05)\n\nThis commit contains bugfixes and optimizations for 1.0.0\n\n**Notable commits**\n\n* [6553aa4] Make a number of size utility functions to `STRICT`\n* [bb1d748] Add checks for NULL arguments to `set_adaptive_chunking`, `set_number_partitions`, `set_chunk_time_interval`, `add_dimension`, and `create_hypertable`\n* [a534ed4] Fix upsert TLE translation when mapping variable numbers\n* [aecd55b] Check extension exists for DROP OWNED and DROP EXTENSION\n\n## 1.0.0 (2018-10-30)\n\n**This is our 1.0 release!**\n\nFor notable commits between 0.12.0/0.12.1 and this final 1.0 release, please see previous entries for the release candidates (rc1, rc2, and rc3).\n\n**Thanks**\nTo all the external contributors who helped us debug the release candidates, as well as anyone who has contributed bug reports, PRs, or feedback on Slack, GitHub, and other channels. All input has been valuable and helped us create the product we have today!\n\n**Potential breaking changes**\n* To better align with the ISO standard so that time bucketing starts each week by default on a Monday (rather than Saturday), the `time_bucket` epoch/origin has been changed from January 1, 2000 to January 3, 2000.  The function now includes an `origin` parameter that can be used to adjust this.\n* Error codes are now prefixed with `TS` instead of the prior `IO` prefix. If you were checking for these error codes by name, please update your code.\n\n\n## 1.0.0-rc3 (2018-10-18)\n\nThis release is our third 1.0 release candidate. We expect to only merge bug fixes between now and our final 1.0 release. This is a big milestone for us and signifies our maturity and enterprise readiness.\n\n**PLEASE NOTE** that release candidate (rc) builds will only be made available via GitHub and Docker, and _not_ on other release channels. Please help us test these release candidates out if you can!\n\n**Potential breaking change**: Starting with rc2, we updated our error codes to be prefixed with `TS` instead of the old `IO` prefix. If you were checking for these error codes by name, please update your checks.\n\n**Notable commits**\n* [f7ba13d] Handle and test tablespace changes to and from the default tablespace\n* [9ccda0d] Start stopped workers on restart message\n* [3e3bb0c] Add bool created to create_hypertable and add_dimension return value\n* [53ff656] Add state machine and polling to launcher\n* [d9b2dfe] Change return value of add_dimension to TABLE\n* [19299cf] Make all time_bucket function STRICT\n* [297d885] Add a version of time_bucket that takes an origin\n* [e74be30] Move time_bucket epoch to a Monday\n* [46564c1] Handle ALTER SCHEMA RENAME properly\n* [a83e283] Change return value of create_hypertable to TABLE\n* [aea7c7e] Add GRANTs to update script for pg_dump to work\n* [119963a] Replace hardcoded bash path in shell scripts\n\n**Thanks**\n* @jesperpedersen for several PRs that help improve documentation and some rough edges\n* @did-g for improvements to our build process\n* @skebanga for reporting an issue with ALTER SCHEMA RENAME\n* @p-alik for suggesting a way to improve our bash scripts' portability\n* @mx781 and @HeikoOnnebrink for reporting an issues with permission GRANTs and ownership when using pg_dump\n\n\n## 1.0.0-rc2 (2018-09-27)\n\nThis release is our second 1.0 release candidate. We expect to only merge bug fixes between now and our final 1.0 release. This is a big milestone for us and signifies our maturity and enterprise readiness.\n\n**PLEASE NOTE** that release candidate (rc) builds will only be made available via GitHub and Docker, and _not_ on other release channels. Please help us test these release candidates out if you can!\n\n**Potential breaking change**: We updated our error codes to be prefixed with `TS` instead of the old `IO` prefix. If you were checking for these error codes by name, please update your checks.\n\n**Notable commits**\n* [b43574f] Switch 'IO' error prefix to 'TS'\n* [9747885] Prefix public C functions with ts_\n* [39510c3] Block unknown alter table commands on  hypertables\n* [2408a83] Add support for ALTER TABLE SET TABLESPACE on hypertables\n* [41d9846] Enclose macro replacement list and arguments in parentheses\n* [cc59d51] Replace macro LEAST_TIMESTAMP by a static function\n* [281f363] Modify telemetry BGW to run every hour the first 12 hours\n* [a09b3ec] Add pg_isolation_regress support to the timescale build system\n* [2c267ba] Handle SIGTERM/SIGINT asynchronously\n* [5377e2d] Fix use-after-free bug for connections in the telemetry BGW\n* [248f662] Fix pg_dump for unprivileged users\n* [193fa4a] Stop background workers when extension is DROP OWNED\n* [625e3fa] Fix negative value handling in int time_bucket\n* [a33224b] Add loader API version function\n* [18b8068] Remove unnecessary index on dimension metadata table\n* [d09405d] Fix adaptive chunking when hypertables have multiple dimensions\n* [a81dc18] Block signals when writing to the log table in tests\n* [d5a6392] Fix adaptive chunking so it chooses correct index\n* [3489cca] Fix sigterm handling in background jobs\n* [2369ae9] Remove !WIN32 for sys/time.h and sys/socket.h, pg provides fills\n* [ebbb4ae] Also add sys/time.h for NetBSD. Fixes #700\n* [1a9ae17] Fix build on FreeBSD wrt sockets\n* [8225cd2] Remove (redefined) macro PG_VERSION and replace with PACKAGE_VERSION\n* [2a07cf9] Release SpinLock even when we're about to Error due to over-decrementing\n* [b2a15b8] Make sure DB schedulers are not decremented if they were never incremented\n* [6731c86] Add support for pre-release version checks\n\n**Thanks**\n* @did-g for an improvement to our macros to make compiliers happy\n* @mx781 and @HeikoOnnebrink for reporting issues with working with pg_dump fully\n* @znbang and @timolson for reporting a bug that was causing telemetry to fail\n* @alanhamlett for reporting an issue with spinlocks when handling SIGTERMs\n* @oldgreen for reporting an issue with building on NetBSD\n* @kev009 for fixing build issues on FreeBSD and NetBSD\n* All the others who have helped us test and used these RCs!\n\n\n## 0.12.1 (2018-09-19)\n\n**High-level changes**\n\n* Fixes for a few issues related to the new scheduler and background worker framework.\n* Fixed bug in adaptive chunking where the incorrect index could be used for determining the current interval.\n* Improved testing, code cleanup, and other housekeeping.\n\n**Notable commits**\n* [0f6f7fc] Fix adaptive chunking so it chooses correct index\n* [3ed79ed] Fix sigterm handling in background jobs\n* [bea098f] Remove !WIN32 for sys/time.h and sys/socket.h, pg provides fills\n* [9f62a1a] Also add sys/time.h for NetBSD. Fixes #700\n* [95a982f] Fix build on FreeBSD wrt sockets\n* [fcb4a79] Remove (redefined) macro PG_VERSION and replace with PACKAGE_VERSION\n* [2634897] Release SpinLock even when we're about to Error due to over-decrementing\n* [1f30dbb] Make sure DB schedulers are not decremented if they were never incremented\n* [f518cd0] Add support for pre-release version checks\n* [acebaea] Don't start schedulers for template databases.\n* [f221a12] Fix use-after-free bug in telemetry test\n* [0dc5bbb] Use pg_config bindir directory for pg executables\n\n**Thanks**\n* @did-g for reporting a use-after-free bug in a test and for improving the robustness of another test\n* @kev009 for fixing build issues on FreeBSD and NetBSD\n\n\n## 1.0.0-rc1 (2018-09-12)\n\nThis release is our 1.0 release candidate. We expect to only merge bug fixes between now and our final 1.0 release. This is a big milestone for us and signifies our maturity and enterprise readiness.\n\n**PLEASE NOTE** that release candidate (rc) builds will only be made available via GitHub and Docker, and _not_ on other release channels. Please help us test these release candidates out if you can!\n\n\n**Notable commits**\n* [acebaea] Don't start schedulers for template databases.\n* [f221a12] Fix use-after-free bug in telemetry test\n* [2092b2a] Fix unused variable warning in Release build\n* [0dc5bbb] Use pg_config bindir directory for pg executables\n\n**Thanks**\n* @did-g for reporting a use-after-free bug in a test and for improving the robustness of another test\n\n\n## 0.12.0 (2018-09-10)\n\n**High-level changes**\n\n*Scheduler framework:* This release introduces a background job framework and scheduler. Each database running within a PostgreSQL instance has a scheduler that schedules recurring jobs from a new jobs table while maintaining statistics that inform the scheduler's policy. Future releases will leverage this scheduler framework for more automated management of data retention, archiving, analytics, and the like.\n\n*Telemetry:* Using this new scheduler framework, TimescaleDB databases now send anonymized usage information to a telemetry server via HTTPS, as well as perform version checking to notify users if a newer version is available. For transparency, a new `get_telemetry_report` function details the exact JSON that is sent, and users may also opt out of this telemetry and version check.\n\n*Continued hardening:* This release addresses several issues around more robust backup and recovery, handling large numbers of chunks, and additional test coverage.\n\n**Notable commits**\n\n* [efab2aa] Fix net lib functionality on Windows and improve error handling\n* [71589c4] Fix issues when OpenSSL is not available\n* [a43cd04] Call the main telemetry function inside BGW executor\n* [faf481b] Add telemetry functionality\n* [45a2b76] Add new Connection and HTTP libraries\n* [b6fe657] Fix max_background_workers guc, errors on EXEC_BACKEND and formatting\n* [5d8c7cc] Add a scheduler for background jobs\n* [55a7141] Implement a cluster-wide launcher for background workers\n* [5bc705f] Update bootstrap to check for cmake and exit if not found\n* [98e56dd] Improve show_indexes test func to be more platform agnostic\n* [b928caa] Note how to recreate templated files\n* [8571e41] Use AttrNumberGetAttrOffset instead of Anum_name - 1 for array indexing\n* [d1710ef] Improve regression test script to cleanup more thoroughly\n* [fc3677f] Reduce number of open chunks per insert\n* [027b7b2] Hide extension symbols by default on Unix platforms\n* [6a3abe5] Fix SubspaceStore to ensure max_open_chunks_per_insert is obeyed\n\n**Thanks**\n\n@EvanCarroll for updates to the bootstrap script to check for cmake\n\n\n## 0.11.0 (2018-08-08)\n\n**High-level changes**\n\n* **Adaptive chunking**: This feature, currently in beta, allows the database to automatically adapt a chunk's time interval, so that users do not need to manually set (and possibly manually change) this interval size. In this release, users can specify either a target chunk data size (in terms of MB or GB), and the chunk's time intervals will be automatically adapted. Alternatively, users can ask the database to just estimate a target size itself based on the platform's available memory and other parameters, and the system will adapt accordingly. This type of automation can simplify initial database test and operations. This feature is default off. Note: The default time interval for non-adaptive chunking has also been changed from 1 month to 1 week.\n* **Continued hardening**: This release addresses a number of less frequently used schema modifications, functions, or constraints. Unsupported functions are safely blocked, while we have added support for a number of new types of table alterations. This release also adds additional test coverage.\n* Support for additional types of time columns, if they are binary compatible (thanks @fvannee!).\n\n**Notable commits**\n\n* [9ba2e81] Fix segfault with custom partition types\n* [7e9bf25] Change default chunk size to one week\n* [506fa18] Add tests for custom types\n* [1d9ade7] add support for other types as timescale column\n* [570f2f8] Validate parameters when creating partition info\n* [148f2da] Use shared_buffers as the available cache memory\n* [e0a15c1] Add additional comments to explain algorithm\n* [d81dccb] Set the default chunk_time_interval to 1 day with adaptive chunking enabled\n* [2e7b32c] Add WARNING when doing min-max heap scan for adaptive chunking\n* [6b452a8] Update adaptive chunk algorithm to handle very small chunks.\n* [9c9cdca] Add support for adaptive chunk sizing\n* [7f8d17d] Handle DEFERRED and VALID options for constraints\n* [0c5c21b] Block using rules with hypertables\n* [37142e9] Block INSERTs on a hypertable's root table\n* [4daf087] Fix some ALTER TABLE corner case bugs on hypertables\n* [122f5f1] Block replica identity usage with hypertables\n* [8bf552e] Block unlogged tables from being used as hypertables\n* [a8c637e] Create aggregate functions only once to avoid dependency issues\n* [a97f2af] Add support for custom hypertable dimension types\n* [dfe026c] Refactor create_hypertable rel access.\n* [ed379c3] Validate existing indexes before adding a new dimension\n* [1f2d276] Fix and improve show_indexes test support function\n* [77b0035] Enforce IMMUTABLE partitioning functions\n* [cbc5e60] Block NO INHERIT constraints on hypertables\n* [e362e9c] Block mixing hypertables with postgres inheritance\n* [011f12b] Add support for CLUSTER ON and SET WITHOUT CLUSTER\n* [e947c6b] Improve handling of column settings\n* [fc4957b] Update statistics on parent table when doing ANALYZE\n* [82942bf] Enable backwards compatibility for loader for 0.9.0 and 0.9.1\n\n**Thanks**\n\n* @Ngalstyan4 and @hjsuh18, our interns, for all of the PRs this summer\n* @fvannee for a PR adding support for binary compatible custom types as a time column\n* @fmacelw for reporting a bug where first() and last() hold reference across extension update\n* @soccerdroid for reporting a corner case bug in ALTER TABLE\n\n\n## 0.10.1 (2018-07-12)\n\n**High-level changes**\n* Improved memory management for long-lived connections.\n* Fixed handling of dropping triggers that would lead to orphaned references in pg_depend.\n* Fixed pruning in CustomScan when the subplan is not a Scan type that caused a crash with LATERALs.\n* Corrected size reporting that was not accurately counting TOAST size\n* Updated error messages that more closely conform to PG style.\n* Corrected handling of table and schema name changes to chunks; TimescaleDB metadata catalogs are now properly updated\n\n**Notable commits**\n* [8b58500] Fix bug where dropping triggers caused dangling references in pg_depend, disallow disabling triggers on hypertables\n* [745b8ab] Fixing CustomScan pruning whenever the subplan is NOT of a Scan type.\n* [67a8a41] Make chunk identifiers formatting safe using format\n* [41af6ff] Fix misreported toast_size in chunk_relation_size funcs\n* [4f2f1a6] Update the error messages to conform with the style guide; Fix tests\n* [3c28f65] Release cache pin memory\n* [abe76fc] Add support for changing chunk schema and name\n\n**Thanks**\n* @mfuterko for updating our error messages to conform with PG error message style\n* @fvannee for reporting a crash when using certain LATERAL joins with aggregates\n* @linba708 for reporting a memory leak with long lived connections\n* @phlsmk for reporting an issue where dropping triggers prevented drop_chunks from working due to orphaned dependencies\n\n\n## 0.10.0 (2018-06-27)\n\n**High-level changes**\n* Planning time improvement (**up to 15x**) when a hypertable has many chunks by only expanding (and taking locks on) chunks that will actually be used in a query, rather than on all chunks (as was the default PostgreSQL behavior).\n* Smarter use of HashAggregate by teaching the planner to better estimate the number of output rows when using time-based grouping.\n* New convenience function for getting the approximate number of rows in a hypertable (`hypertable_approximate_row_count`).\n* Fixed support for installing extension into non-`public` schemas\n* Other bug fixes and refactorings.\n\n**Notable commits**\n* [12bc117] Fix static analyzer warning when checking for index attributes\n* [7d9f49b] Fix missing NULL check when creating default indexes\n* [2e1f3b9] Improve memory allocation during cache lookups\n* [ca6e5ef] Fix upserts on altered tables.\n* [2de6b02] Add optimization to use HashAggregate more often\n* [4b4211f] Fix some external functions when setting a custom schema\n* [b7257fc] Optimize planning times when hypertables have many chunks\n* [c660fcd] Add hypertable_approximate_row_count convenience function\n* [9ce1576] Fix a compilation issue on pre 9.6.3 versions\n\n**Thanks**\n* @viragkothari for suggesting the addition of `hypertable_approximate_row_count` and @fvannee for providing the initial SQL used to build that function\n* 'mintekhab' from Slack for reporting a segfault when using upserts on an altered table\n* @mmouterde for reporting an issue where the extension implicitly expected to be installed in the `public` schema\n* @mfuterko for bringing some potential bugs to our attention via static analysis\n\n## 0.9.2 (2018-05-04)\n\n**High-level changes**\n* Fixed handling of `DISCARD ALL` command when parallel workers are involved, which sometimes caused the extension to complain it was not preloaded\n* User permission bug fix where users locating TRIGGER permissions in a database could not insert data into a hypertable\n* Fixes for some issues with 32-bit architectures\n\n**Notable commits**\n* [256b394] Fix parsing of GrantRoleStmt\n* [b78953b] Fix datum conversion typo\n* [c7283ef] Fix bug with extension loader when DISCARD ALL is executed\n* [fe20e48] Fix chunk creation with user that lacks TRIGGER permission\n\n**Thanks**\n* @gumshoes, @manigandham, @wallies, & @cjrh for reporting a problem where sometimes the extension would appear to not be preloaded when it actually was\n* @thaxy for reporting a permissions issue when user creating a hypertable lacks TRIGGER permission\n* @bertmelis for reporting some bugs with 32-bit architectures\n\n## 0.9.1 (2018-03-26)\n\n**High-level changes**\n* **For this release only**, you will need to restart the database before\nrunning `ALTER EXTENSION`\n* Several edge cases regarding CTEs addressed\n* Updated preloader with better error messaging and fixed edge case\n* ABI compatibility with latest PostgreSQL to help catch any breaking\nchanges\n\n**Notable commits**\n* [40ce037] Fix crash on explain analyze with insert cte\n* [8378beb] Enable hypertable inserts within CTEs\n* [bdfda75] Fix double-loading of extension\n* [01ea77e] Fix EXPLAIN output for ConstraintAwareAppend inside CTE\n* [fc05637] Add no preload error to versioned library.\n* [38f8e0c] Add ABI compatibility tests\n* [744ca09] Fix Cache Pinning for Subtxns\n* [39010db] Move more drops into event trigger system\n* [fc36699] Do not fail add_dimension() on non-empty table with 'if_not_exists'\n\n**Thanks**\n* @The-Alchemist for pointing out broken links in the README\n* @chaintng for pointing out a broken link in the docs\n* @jgranstrom for reporting a edge case crash with UPSERTs in CTEs\n* @saosebastiao for reporting the lack of an error message when the library is not preloaded and trying to delete/modify a hypertable\n* @jbylund for reporting a cache invalidation issue with the preloader\n\n## 0.9.0 (2018-03-05)\n\n**High-level changes**\n* Support for multiple extension versions on different databases in the\nsame PostgreSQL instance. This allows different databases to be updated\nindependently and provides for smoother updates between versions. No\nmore spurious errors in the log as the extension is being\nupdated, and new versions no longer require a restart of the database.\n* Streamlined update process for smaller binary/package sizes\n* Significant refactoring to simplify and improve codebase, including\nimprovements to error handling, security/permissions, and more\n* Corrections to edge-case scenarios involving dropping schemas,\nhypertables, dimensions, and more\n* Correctness improvements through propagating reloptions from main\ntable to chunk tables and blocking `ONLY` commands that try to alter\nhypertables (i.e., changes should be applied to chunks as well)\n* Addition of a `migrate_data` option to `create_hypertable` to allow\nnon-empty tables to be turned into hypertables without separate\ncreation & insertion steps. Note, this option may take a while if the\noriginal table has lots of data\n* Support for `ALTER TABLE RENAME CONSTRAINT`\n* Support for adjusting the number of partitions for a space dimension\n* Improvements to tablespace handling\n\n**Notable commits**\n* [4672719] Fix error in handling of RESET ALL\n* [9399308] Refactor/simplify update scripts and build process\n* [0e79df4] Fix handling of custom SQL-based partitioning functions\n* [f13969e] Fix possible memory safety issue and squash valgrind error.\n* [ef74491] Migrate table data when creating a hypertable\n* [2696582] Move index and constraints drop handling to event trigger\n* [d6baccb] Improve tablespace handling, including blocks for DROP and REVOKE\n* [b9a6f89] Handle DROP SCHEMA for hypertable and chunk schemas\n* [b534a5a] Add test case for adding metadata entries automatically\n* [6adce4c] Handle TRUNCATE without upcall and handle ONLY modifier\n* [71b1124] Delete orphaned dimension slices\n* [fa19a54] Handle deletes on metadata objects via native catalog API\n* [6e011d1] Refactor hypertable-related API functions\n* [5afd39a] Fix locking for serializing chunk creation\n* [6dd2c46] Add check for null in ca_append_rescan to prevent segfault\n* [71962b8] Refactor dimension-related API functions\n* [cc254a9] Fix CREATE EXTENSION IF NOT EXISTS and error messages\n* [d135256] Spread chunk indexes across tablespaces like chunks\n* [e85721a] Block ONLY hypertable on all ALTER TABLE commands.\n* [78d36b5] Handle subtxn for cache pinning\n* [26ef77f] Add subtxn abort logic to process_utility.c\n* [25f3284] Handle cache invalidation during subtxn rollback\n* [264956f] Block DROP NOT NULL on time-partitioned columns.\n* [ad7d361] Better accounting for number of items stored in a subspace\n* [12f92ea] Improve speed of out-of-order inserts\n* [87f055d] Add support for ALTER TABLE RENAME CONSTRAINT.\n* [da8cc79] Add support for multiple extension version in one pg instance\n* [68faddc] Make chunks inherit reloptions set on the hypertable\n* [4df8f28] Add proper permissions handling for associated (chunk) schemas\n* [21efcce] Refactor chunk table creation and unify constraint handling\n\n**Thanks**\n* @Anthares for a request to pass reloptions like fill factor to child chunks\n* @oldgreen for reporting an issue with subtransaction handling\n* @fvannee for a PR that fixed a bug with `ca_append_rescan`\n* @maksm90 for reporting an superfluous index being created in an internal catalog table\n* @Rashid987 for reporting an issue where deleting a chunk, then changing the time interval would not apply the change when a replacement chunk is created\n* RaedA from Slack for reporting compilation issues on Windows between\n0.8.0 and this release\n* @haohello for a request to adjust the number of partitions for a given dimension\n* @LonghronShen and @devereaux for reporting an issue (and submitting a PR) for handling version identification when there is more to the version than just major and minor numbers\n* @carlospeon for reporting an issue with dropping hypertables\n* @gumshoes, @simpod, @jbylund, and @ryan-shaw for testing a pre-release version to verify our new update path works as expected\n* @gumshoes for reporting an issue with `RESET ALL`\n\n## 0.8.0 (2017-12-19)\n\n**High-level changes**\n* TimescaleDB now builds and runs on Windows! Now in addition to using\nDocker, users can choose to build the extension from source and install\non 64-bit Windows\n* Update functions `add_dimension` and `set_chunk_time_interval` to take `INTERVAL` types\n* Improved tablespace management including detaching tablespaces from hypertables and looking up tablespaces associated with a hypertable\n* Reduced memory usage for `INSERT`s with out-of-order data\n* Fixes inserts on 32-bit architectures, in particular ARM\n* Other correctness improvements including preventing attachment of\nPG10 partitions to hypertables, improved handling of space dimensions\nwith one partition, and correctly working with `pg_upgrade`\n* Test and build improvements making those both more robust and easier\nto do\n\n**Notable commits**\n* [26971d2] Make `tablespace_show` function return Name instead of CString\n* [2fe447b] Make TimescaleDB work with pg_upgrade\n* [90c7a6f] Fix logic for one space partition\n* [6cfdd79] Prevent native partitioning attachment of hypertables\n* [438d79d] Fix trigger relcache handling for COPY\n* [cc1ad95] Reduce memory usage for out-of-order inserts\n* [a0f62c5] Improve bootstrap script's robustness\n* [00a096f] Modify tests to make more platform agnostic\n* [0e76b5f] Do not add tablespaces to hypertable objects\n* [176b75e] Add command to show tablespaces attached to a hypertable\n* [6e92383] Add function to detach tablespaces from hypertables\n* [e593876] Refactor tablespace handling\n* [c4a46ac] Add hypertable cache lookup on ID/pkey\n* [f38a578] Fix handling of long constraint names\n* [20c9b28] Unconditionally add pg_config --includedir to src build\n* [12dff61] Fixes insert for 32bit architecture\n* [e44e47e] Update add_dimension to take INTERVAL times\n* [0763e62] Update set_chunk_time_interval to take INTERVAL times\n* [87c4b4f] Fix test generator to work for PG 10.1\n* [51854ac] Fix error message to reflect that drop_chunks can take a DATE interval\n* [66396fb] Add build support for Windows\n* [e1a0e81] Refactor and fix cache invalidation\n\n**Thanks**\n* @oldgreen for reporting an issue where `COPY` was warning of relcache reference leaks\n* @campeterson for pointing out some documentation typos\n* @jwdeitch for the PR to prevent attaching PG10 partitions to hypertables\n* @vjpr and @sztanpet for reporting bugs and suggesting improvements to the bootstrap script\n\n## 0.7.1 (2017-11-29)\n\n**High-level changes**\n* Fix to the migration script for those coming from 0.6.1 (or earlier)\n* Fix edge case in `drop_chunks` when hypertable uses `TIMESTAMP` type\n* Query planning improvements & fixes\n* Permission fixes and support `SET ROLE` functionality\n\n**Notable commits**\n* [717299f] Change time handling in drop_chunks for TIMESTAMP times\n* [d8ec285] Do not append-optimize plans with result relations (DELETE/UPDATE)\n* [30b72ec] Handle empty append plans in ConstraintAwareAppend\n* [b35509b] Permission fixes and allow SET ROLE\n\n**Thanks**\n* @shaneodonnell for reporting a bug with empty append plans in ConstraintAwareAppend\n* @ryan-shaw for reporting a bug with query plans involving result relations and reporting an issue with our 0.6.1 to 0.7.0 migration script\n\n\n## 0.7.0 (2017-11-21)\n\n**Please note: This update may take a long time (minutes, even hours) to\ncomplete, depending on the size of your database**\n\n**High-level changes**\n* **Initial PostgreSQL 10 support**. TimescaleDB now should work on both PostgreSQL 9.6 and 10. As this is our first release supporting PG10, we look forward to community feedback and testing. _Some release channels, like Ubuntu & RPM-based distros will remain on 9.6 for now_\n* Support for `CLUSTER` on hypertables to recursively apply to chunks\n* Improve constraint handling of edge cases for `DATE` and `TIMESTAMP`\n* Fix `range_start` and `range_end` to properly handle the full 32-bit int space\n* Allow users to specify their desired partitioning function\n* Enforce `NOT NULL` constraint on time columns\n* Add testing infrastructure to use Coverity and test PostgreSQL regression tests in TimescaleDB\n* Switch to the CMake build system for better cross-platform support\n* Several other bug fixes, cleanups, and improvements\n\n**Notable commits**\n* [13e1cb5] Add reindex function\n* [6594018] Handle when create_hypertable is invoked on partitioned table\n* [818bdbc] Add coverity testing\n* [5d0cbc1] Recurse CLUSTER command to chunks\n* [9c7191e] Change TIMESTAMP partitioning to be completely tz-independent\n* [741b256] Mark IMMUTABLE functions as PARALLEL SAFE\n* [2ffb30d] Make aggregate serialize and deserialize functions STRICT\n* [c552410] Add build target to run the standard PostgreSQL regression tests\n* [291050b] Change DATE partitioning to be completely tz-independent\n* [ca0968a] Make all partitioning functions take anyelement argument\n* [a4e1e32] Change range_start and range_end semantics\n* [2dfbc82] Fix off-by-one error on range-end\n* [500563f] Add support for PostgreSQL 10\n* [201a948] Check that time dimensions are set as NOT NULL.\n* [4532650] Allow setting partitioning function\n* [4a0a0d8] Fix column type change on plain tables\n* [cf009cc] Avoid string conversion in hash partitioning\n* [8151098] Improve update testing by adding a rerun test\n* [c420c11] Create a catalog entry for constraint-backed indexes\n* [ec746d1] Add ability to run regression test locally\n* [44f9fec] Add analyze to parallel test for stability\n* [9e0422a] Fix bug with pointer assignment after realloc\n* [114fa8d] Refactor functions used to recurse DDL commands to chunks\n* [b1ec4fa] Refactor build system to use CMake\n\n**Thanks**\n* @jgraichen for reporting an issue with `drop_chunks` not accepting `BIGINT`\n* @nathansgreen for reporting an edge case with constraints for `TIMESTAMP`\n* @jonmd for reporting a similar edge case for `DATE`\n* @jwdeitch for a PR to cover an error case in PG10\n\n\n## 0.6.1 (2017-11-07)\n\n**High-level changes**\n\n* Fix several memory bugs that caused segfaults\n* Fix bug when creating expression indexes\n* Plug a memory leak with constraint expressions\n* Several other bug fixes and stability improvements\n\n**Notable commits**\n* [2799075] Fix EXPLAIN for ConstraintAware and MergeAppend\n* [8084594] Use per-chunk memory context for cached chunks\n* [a13d9de] Do not convert tuples on insert unless needed\n* [da09f24] Limit growth of range table during chunk inserts\n* [85dee79] Fix issue with creating expression indexes\n* [844ff7f] Fix memory leak due to constraint expressions.\n* [e90d3ee] Consider precvious CIS state in copy FROM file to rel\n* [56d632f] Fix bug with pointer assignment after realloc\n* [f97d624] Make event trigger creation idempotent\n\n**Thanks**\n* @jwdeitch for submitting a patch to correct behavior in the COPY operation\n* @jgraichen for reporting a bug with expression indexes\n* @zixet for reporting a memory leak\n* @djk447 for reporting a bug in EXPLAIN with ConstraintAware and MergeAppend\n\n## 0.6.0 (2017-10-12)\n\n**High-level changes**\n\n* Fix bugs where hypertable-specific handlers were affecting normal Postgres tables.\n* Make it so that all TimescaleDB commands can run as a normal user rather than a superuser.\n* Updates to the code to make the extension compileable on Windows; future changes will add steps to properly build.\n* Move `time_bucket` functions out of `public` schema (put in schema where extension is).\n* Several other bugs fixes.\n\n**Notable commits**\n* [1d73fb8] Fix bug with extension starting too early.\n* [fd390ec] Fix chunk index attribute mismatch and locking issue\n* [430ed8a] Fix bug with collected commands in index statement.\n* [614c2b7] Fix permissions bugs and run tests as normal user\n* [ce12104] Fix \"ON CONFLICT ON CONSTRAINT\" on plain PostgreSQL tables\n* [4c451e0] Fix rename and reindex bugs when objects are not relations\n* [c3ebc67] Fix permission problems with dropping hypertables and chunks\n* [040e815] Remove truncate and hypertable metadata triggers\n* [5c26328] Fix INSERT on hypertables using sub-selects with aggregates\n* [b57e2bf] Prepare C code for compiling on Windows\n* [a2bad2b] Fix constraint validation on regular tables\n* [fb5717f] Remove explicit schema for time_bucket\n* [04d01ce] Split DDL processing into start and end hooks\n\n**Thanks**\n* @oldgreen for reporting `time_bucket` being incorrectly put in the `public` schema and pointing out permission problems\n* @qlandman for reporting a bug with INSERT using sub-selects with aggregates\n* @min-mwei for reporting a deadlock issue during INSERTs\n* @ryan-shaw for reporting a bug where the extension sometimes used `pg_cache` too soon\n\n## 0.5.0 (2017-09-20)\n\n**High-level changes**\n* Improved support for primary-key, foreign-key, unique, and exclusion constraints.\n* New histogram function added for getting the frequency of a column's values.\n* Add support for using `DATE` as partition column.\n* `chunk_time_interval` now supports `INTERVAL` data types\n* Block several unsupported and/or dangerous operations on hypertables and chunks, including dropping or otherwise altering a chunk directly.\n* Several bug fixes throughout the code.\n\n**Notable commits**\n* [afcb0b1] Fix NULL handling in first/last functions.\n* [d53c705] Add script to dump meta data that can be useful for debugging.\n* [aa904fa] Block adding constraints without a constraint name\n* [a13039f] Fix dump and restore for tables with triggers and constraints\n* [8cf8d3c] Improve the size utils functions.\n* [2767548] Block adding constraints using an existing index\n* [5cee104] Allow chunk_time_interval to be specified as an INTERVAL type\n* [6232f98] Add histogram function.\n* [2380033] Block ALTER TABLE and handle DROP TABLE on chunks\n* [72d6681] Move security checks for ALTER TABLE ALTER COLUMN to C\n* [19d3d89] Handle changing the type of dimension columns correctly.\n* [17c4ba9] Handle ALTER TABLE rename column\n* [66932cf] Forbid relocating extension after install.\n* [d2561cc] Add ability to partition by a date type\n* [48e0a61] Remove triggers from chunk and chunk_constraint\n* [4dcbe61] Add support for hypertable constraints\n\n**Thanks**\n* @raycheung for reporting a segfault in `first`/`last`\n* @meotimdihia, @noyez, and @andrew-blake for reporting issues with `UNQIUE` and other types of constraints\n\n\n## 0.4.2 (2017-09-06)\n\n**High-level changes**\n* Provide scripts for backing up and restoring single hypertables\n\n**Notable commits**\n* [683c078] Add backup/restore scripts for single hypertables\n\n## 0.4.1 (2017-09-04)\n\n**High-level changes**\n* Bug fix for a segmentation fault in the planner\n* Shortcut when constraint-aware append excludes all chunks\n* Fix edge case with negative timestamps when points fell right on the boundary\n* Fix behavior of `time_bucket` for `DATE` types by not converting to `TIMESTAMPTZ`\n* Make the output of `chunk_relation_size` consistent\n\n**Notable commits**\n* [50c8c4c] Fix possible segfault in planner\n* [e49e45c] Fix failure when constraint-aware append excludes all chunks\n* [c3b6fb9] Fix bug with negative dimension values\n* [3c69e4f] Fix semantics of time_bucket on DATE input\n* [0137c92] Fix output order of chunk dimensions and ranges in chunk_relation_size.\n* [645b530] Convert inserted tuples to the chunk's rowtype\n\n**Thanks**\n* @yadid for reporting a segfault (fixed in 50c8c4c)\n* @ryan-shaw for reporting tuples not being correctly converted to a chunk's rowtype (fixed in 645b530)\n* @yuezhihan for reporting GROUP BY error when setting compress_segmentby with an enum column\n\n## 0.4.0 (2017-08-21)\n\n**High-level changes**\n* Exclude chunks when constraints can be constifyed even if they are\nconsidered mutable like `NOW()`.\n* Support for negative values in the dimension range which allows for pre-1970 dates.\n* Improve handling of default chunk times for integral date times by forcing it to be explicit rather than guessing the units of the time.\n* Improve memory usage for long-running `COPY` operations (previously it would grow unbounded).\n* `VACUUM` and `REINDEX` on hypertables now recurse down to chunks.\n\n**Notable commits**\n* [139fe34] Implement constraint-aware appends to exclude chunks at execution time\n* [2a51cf0] Add support for negative values in dimension range\n* [f2d5c3f] Error if add_dimension given both partition number and interval length\n* [f3df02d] Improve handling of non-TIMESTAMP/TZ timestamps\n* [6a5a7eb] Reduce memory usage on long-running COPY operations\n* [953346c] Make VACUUM and REINDEX recurse to chunks\n* [55bfdf7] Release all cache pins when a transaction ends\n\n## 0.3.0 (2017-07-31)\n\n**High-level changes**\n* \"Upserts\" are now supported via normal `ON CONFLICT DO UPDATE`/`ON CONFLICT DO NOTHING` syntax. However, `ON CONFLICT ON CONSTRAINT` is not yet supported.\n* Improved support for user-defined triggers on hypertables. Now handles both INSERT BEFORE and INSERT AFTER triggers, and triggers can be named arbitrarily (before, a \\_0\\_ prefix was required to ensure correct execution priority).\n* `TRUNCATE` on a hypertable now deletes empty chunks.\n\n**Notable commits**\n* [23f9d3c] Add support for upserts (`ON CONFLICT DO UPDATE`)\n* [1f3dcd8] Make `INSERT`s use a custom plan instead of triggers\n* [f23bf58] Remove empty chunks on `TRUNCATE` hypertable.\n\n## 0.2.0 (2017-07-12)\n\n**High-level changes**\n* Users can now define their own triggers on hypertables (except for `INSERT AFTER`)\n* Hypertables can now be renamed or moved to a different schema\n* Utility functions added so you can examine the size hypertables, chunks, and indices\n\n**Notable commits**\n* [83c75fd] Add support for triggers on hypertables for all triggers except `INSERT AFTER`\n* [e0eeeb9] Add hypertable, chunk, and indexes size utils functions.\n* [4d2a65d] Add infrastructure to build update script files.\n* [a5725d9] Add support to rename and change schema on hypertable.\n* [142f58c] Cleanup planner and process utility hooks\n\n## 0.1.0 (2017-06-28)\n\n**IMPORTANT NOTE**\n\nStarting with this release, TimescaleDB will now\nsupport upgrading between extension versions using the typical\n`ALTER EXTENSION` command, unless otherwise noted in future release notes. This\nimportant step should make it easier to test TimescaleDB and be able\nto get the latest benefits from new versions of TimescaleDB. If you\nwere previously using a version with the `-beta` tag, you will need\nto `DROP` any databases currently using TimescaleDB and re-create them\nin order to upgrade to this new version. To backup and migrate data,\nuse `pg_dump` to save the table schemas and `COPY` to write hypertable\ndata to CSV for re-importing after upgrading is complete. We describe\na similar process on [our docs](http://docs.timescale.com/getting-started/setup/migrate-from-postgresql#different-db).\n\n**High-level changes**\n* More refactoring to stabilize and cleanup the code base for supporting upgrades (see above note)\n* Correct handling of ownership and permission propagation for hypertables\n* Multiple bug fixes\n\n**Notable commits**\n* [696cc4c] Provide API for adding hypertable dimensions\n* [97681c2] Fixes permission handling\n* [aca7f32] Fix extension drop handling\n* [9b8a447] Limit the SubspaceStore size; Add documentation.\n* [14ac892] Fix possible segfault\n* [0f4169c] Fix check constraint on dimension table\n* [71c5e78] Fix and refactor tablespace support\n* [5452dc5] Fix partiton functions; bug fixes (including memory)\n* [e75cd7e] Finer grained memory management\n* [3c460f0] Fix partitioning, memory, and tests\n* [fe51d8d] Add native scan for the chunk table\n* [fc68baa] Separate out subspace_store and add it to the hypertable object as well\n* [c8124b8] Use hypercube instead of dimension slice list\n* [f5d7786] Change the semantics of range_end to be exclusive\n* [700c9c8] Refactor insert path in C.\n* [0584c47] Created chunk_get_or_create in sql with an SPI connector in C\n* [7b8de0c] Refactor catalog for new schema and add native data types\n* [d3bdcba] Start refactoring to support any number of partitioning dimensions\n\n## 0.0.12-beta (2017-06-21)\n\n**High-level changes**\n* A major cleanup and refactoring was done to remove legacy code and\ncurrently unused code paths. This change is **backwards incompatible**\nand will require a database to be re-initialized and data re-imported.\nThis refactoring will allow us to provide upgrade paths starting with\nthe next release.\n* `COPY` and `INSERT` commands now return the correct number of rows\n* Default indexes no longer duplicate existing indexes\n* Cleanup of the Docker image and build process\n* Chunks are now time-aligned across partitions\n\n**Notable commits**\n* [3192c8a] Remove Dockerfile and docker.mk\n* [2a01ebc] Ensure that chunks are aligned.\n* [73622bf] Fix default index creation duplication of indexes\n* [c8872fe] Fix command-tag for COPY and INSERT\n* [bfe58b6] Refactor towards supporting version upgrades\n* [db01c84] Make time-bucket function parallel safe\n* [18db11c] Fix timestamp test\n* [97bbb59] Make constraint exclusion work with non-text partition keys\n* [f2b42eb] Fix problems with partitioning logic for padded fields\n* [997029a] if_not_exist flag to create_hypertable now works on hypertables with data as well\n* [347a8bd] Reference the correct column when scanning partition epochs\n* [88a9849] Fix bug with timescaledb.allow_install_without_preload GUC not working\n\n## 0.0.11-beta (2017-05-24)\n\n**High-level changes**\n* New `first(value, time)` and `last(value, time)` aggregates\n* Remove `setup_timescaledb()` function to streamline setup\n* Allow for use cases where restarting the server is not feasible by force loading the library\n* Disable time series optimizations on non-hypertables\n* Add some default indexes for hypertables if they do not exist\n* Add \"if not exists\" flag for `create_hypertable`\n* Several bug fixes and cleanups\n\n**Notable commits**\n* [8ccc8cc] Add if_not_exists flag to create_hypertable()\n* [2bc60c7] Fix time interval field name in hypertable cache entry\n* [4638688] Improve GUC handling\n* [cedcafc] Remove setup_timescaledb() and fix pg_dump/pg_restore.\n* [34ad9a0] Add error when timescaledb library is not preloaded.\n* [fc4ddd6] Fix bug with dropping chunks on tables with indexes\n* [32215ff] Add default indexes for hypertables\n* [b2900f9] Disable query optimization on regular tables (non-hypertables)\n* [f227db4] Fixes command tag return for COPY on hypertables.\n* [eb32081] Fix Invalid database ID error\n* [662be94] Add the first(value, time),last(value, time) aggregates\n* [384a8fb] Add regression tests for deleted unit tests\n* [31ee92a] Remove unit tests and sql/setup\n* [13d3acb] Fix bug with alter table add/drop column if exists\n* [f960c24] Fix bug with querying a row as a composite type\n\n## 0.0.10-beta (2017-05-04)\n\n**High-level changes**\n* New `time_bucket` functions for doing roll-ups on varied intervals\n* Change default partition function (thanks @robin900)\n* Variety of bug fixes\n\n**Notable commits**\n* [1c4868d] Add documentation for chunk_time_interval argument\n* [55fd2f2] Fixes command tag return for `INSERT`s on hypertables.\n* [c3f930f] Add `time_bucket` functions\n* [b128ac2] Fix bug with `INSERT INTO...SELECT`\n* [e20edf8] Add better error checking for index creation.\n* [72f754a] use PostgreSQL's own `hash_any` function as default partfunc (thanks @robin900)\n* [39f4c0f] Remove sample data instructions and point to docs site\n* [9015314] Revised the `get_general_index_definition` function to handle cases where indexes have definitions other than just `CREATE INDEX` (thanks @bricklen)\n"
        },
        {
          "name": "CMakeLists.txt",
          "type": "blob",
          "size": 22.2333984375,
          "content": "cmake_minimum_required(VERSION 3.15)\n\nlist(APPEND CMAKE_MODULE_PATH ${CMAKE_CURRENT_SOURCE_DIR}/cmake)\n\ninclude(CheckCCompilerFlag)\ninclude(CheckSymbolExists)\ninclude(GitCommands)\ninclude(GenerateScripts)\ninclude(CMakeDependentOption)\n\noption(APACHE_ONLY \"only compile apache code\" off)\n# This requires all tests to run. This defaults to OFF but can be enabled to\n# ensure that no tests are skipped because of missing tools.\noption(REQUIRE_ALL_TESTS \"Require all tests to run.\" OFF)\noption(USE_OPENSSL \"Enable use of OpenSSL if available\" ON)\noption(SEND_TELEMETRY_DEFAULT \"The default value for whether to send telemetry\"\n       ON)\noption(\n  USE_TELEMETRY\n  \"Include telemetry functionality in the build. Disabling will exclude all telemetry code from the build.\"\n  ON)\n\noption(REGRESS_CHECKS \"PostgreSQL regress checks through installcheck\" ON)\noption(\n  ENABLE_OPTIMIZER_DEBUG\n  \"Enable OPTIMIZER_DEBUG when building. Requires Postgres server to be built with OPTIMIZER_DEBUG.\"\n  OFF)\noption(ENABLE_DEBUG_UTILS \"Enable debug utilities for the extension.\" ON)\n\n# Option to enable assertions. Note that if we include headers from a PostgreSQL\n# build that has assertions enabled, we might inherit that setting without\n# explicitly enabling assertions via the ASSERTIONS option defined here. Thus,\n# this option is mostly useful to enable assertions when the PostgreSQL we\n# compile against has it disabled.\noption(ASSERTIONS \"Compile with assertion checks (default OFF)\" OFF)\n\n# Function to call pg_config and extract values.\nfunction(GET_PG_CONFIG var)\n  set(_temp)\n\n  # Only call pg_config if the variable didn't already have a value.\n  if(NOT ${var})\n    execute_process(\n      COMMAND ${CMAKE_COMMAND} -E env LC_MESSAGES=C ${PG_CONFIG} ${ARGN}\n      OUTPUT_VARIABLE _temp\n      OUTPUT_STRIP_TRAILING_WHITESPACE)\n  endif()\n\n  # On Windows, fields that are not recorded will be given the value \"not\n  # recorded\", so we translate this into <var>-NOTFOUND to make it undefined.\n  #\n  # It will then also show as, e.g., \"PG_LDFLAGS-NOTFOUND\" in any string\n  # interpolation, making it obvious that it is an undefined CMake variable.\n  if(\"${_temp}\" STREQUAL \"not recorded\")\n    set(_temp ${var}-NOTFOUND)\n  endif()\n\n  set(${var}\n      ${_temp}\n      PARENT_SCOPE)\nendfunction()\n\nconfigure_file(\"version.config\" \"version.config\" COPYONLY)\nfile(READ version.config VERSION_CONFIG)\n\nif(VERSION_CONFIG\n   MATCHES\n   \"(^|.*[^a-z])version[\\t ]*=[\\t ]*([0-9]+\\\\.[0-9]+\\\\.*[0-9]*)(-([a-z]+[0-9]*|dev))?.*\"\n)\n  set(VERSION ${CMAKE_MATCH_2})\n  set(VERSION_MOD ${CMAKE_MATCH_4}) # This is used in config.h\n  if(CMAKE_MATCH_3)\n    set(PROJECT_VERSION_MOD ${CMAKE_MATCH_2}${CMAKE_MATCH_3})\n  else()\n    set(PROJECT_VERSION_MOD ${CMAKE_MATCH_2})\n  endif()\nendif()\n\nif(VERSION_CONFIG\n   MATCHES\n   \".*update_from_version[\\t ]*=[\\t ]*([0-9]+\\\\.[0-9]+\\\\.[0-9]+(-[a-z]+[0-9]*)?).*\"\n)\n  set(UPDATE_FROM_VERSION ${CMAKE_MATCH_1})\nendif()\n\nif(VERSION_CONFIG\n   MATCHES\n   \".*downgrade_to_version[\\t ]*=[\\t ]*([0-9]+\\\\.[0-9]+\\\\.[0-9]+(-[a-z]+[0-9]*)?).*\"\n)\n  set(DOWNGRADE_TO_VERSION ${CMAKE_MATCH_1})\nendif()\n\n# a hack to avoid change of SQL extschema variable\nset(extschema \"@extschema@\")\n\n# Set project name, version, and language. Language needs to be set for compiler\n# checks\nproject(\n  timescaledb\n  VERSION ${VERSION}\n  LANGUAGES C)\n\nif(NOT CMAKE_BUILD_TYPE)\n  # Default to Release builds\n  set(CMAKE_BUILD_TYPE\n      Release\n      CACHE\n        STRING\n        \"Choose the type of build, options are: None Debug Release RelWithDebInfo MinSizeRel\"\n        FORCE)\nendif()\n\nset(SUPPORTED_BUILD_TYPES Debug Release RelWithDebInfo MinSizeRel)\nif(NOT CMAKE_BUILD_TYPE IN_LIST SUPPORTED_BUILD_TYPES)\n  message(\n    FATAL_ERROR \"Bad CMAKE_BUILD_TYPE. Expected one of ${SUPPORTED_BUILD_TYPES}\"\n  )\nendif()\n\nmessage(\n  STATUS\n    \"TimescaleDB version ${PROJECT_VERSION_MOD}. Can be updated from version ${UPDATE_FROM_VERSION}\"\n)\nmessage(STATUS \"Build type is ${CMAKE_BUILD_TYPE}\")\n\nset(PROJECT_INSTALL_METHOD\n    source\n    CACHE STRING \"Specify what install platform this binary\nis built for\")\nmessage(STATUS \"Install method is '${PROJECT_INSTALL_METHOD}'\")\n\n# Build compilation database by default\nset(CMAKE_EXPORT_COMPILE_COMMANDS ON)\n\n# Code coverage is optional and OFF by default\noption(CODECOVERAGE \"Enable code coverage for the build\" OFF)\noption(EXPERIMENTAL \"Skip postgres version compatibility check\" OFF)\n\n# Generate downgrade script\noption(GENERATE_DOWNGRADE_SCRIPT\n       \"Generate downgrade script. Defaults to not generate a downgrade script.\"\n       OFF)\ncmake_dependent_option(\n  GENERATE_OLD_DOWNGRADE_SCRIPTS\n  \"Generate downgrade scripts for old versions. Requires setting GENERATE_DOWNGRADE_SCRIPT to ON. Defaults to OFF.\"\n  OFF\n  \"GENERATE_DOWNGRADE_SCRIPT\"\n  ON)\n\nif(CMAKE_BUILD_TYPE MATCHES Debug)\n  # CMAKE_BUILD_TYPE is set at CMake configuration type. But usage of\n  # CMAKE_C_FLAGS_DEBUG is determined at build time by running cmake --build .\n  # --config Debug (at least on Windows). Therefore, we only set these flags if\n  # the configuration-time CMAKE_BUILD_TYPE is set to Debug. Then Debug enabled\n  # builds will only happen on Windows if both the configuration- and build-time\n  # settings are Debug.\n  set(CMAKE_C_FLAGS_DEBUG \"${CMAKE_C_FLAGS_DEBUG} -DDEBUG=1 -DTS_DEBUG=1\")\nendif(CMAKE_BUILD_TYPE MATCHES Debug)\n\nset(SUPPORTED_COMPILERS \"GNU\" \"Clang\" \"AppleClang\" \"MSVC\")\n\n# Check for a supported compiler\nif(NOT CMAKE_C_COMPILER_ID IN_LIST SUPPORTED_COMPILERS)\n  message(\n    FATAL_ERROR\n      \"Unsupported compiler ${CMAKE_C_COMPILER_ID}. Supported compilers are: ${SUPPORTED_COMPILERS}\"\n  )\nendif()\n\n# Option to treat warnings as errors when compiling (default on for debug\n# builds, off for all other build types)\nif(CMAKE_BUILD_TYPE STREQUAL Debug)\n  option(WARNINGS_AS_ERRORS \"Make compiler warnings into errors (default ON)\"\n         ON)\nelse()\n  option(WARNINGS_AS_ERRORS \"Make compiler warnings into errors (default ON)\"\n         OFF)\nendif()\n\nif(WARNINGS_AS_ERRORS)\n  if(CMAKE_C_COMPILER_ID MATCHES \"GNU|Clang|AppleClang\")\n    add_compile_options(-Werror)\n  elseif(CMAKE_C_COMPILER_ID MATCHES \"MSVC\")\n    add_compile_options(/WX)\n  endif()\nendif(WARNINGS_AS_ERRORS)\n\nif(CMAKE_C_COMPILER_ID MATCHES \"GNU|AppleClang|Clang\")\n  # These two flags generate too many errors currently, but we probably want\n  # these optimizations enabled.\n  #\n  # -fdelete-null-pointer-checks -Wnull-dereference\n\n  # This flag avoid some subtle bugs related to standard conversions, but\n  # currently does not compile because we are using too many implicit\n  # conversions that potentially lose precision.\n  #\n  # -Wconversions\n\n  # These flags are supported on all compilers.\n  add_compile_options(\n    -Wempty-body\n    -Wvla\n    -Wall\n    -Wextra\n    # The SQL function arguments macro PG_FUNCTION_ARGS often inroduces unused\n    # arguments.\n    -Wno-unused-parameter\n    -Wundef\n    -Wmissing-prototypes\n    -Wpointer-arith\n    -Werror=vla\n    -Wendif-labels\n    -fno-strict-aliasing\n    -fno-omit-frame-pointer)\n\n  # These flags are just supported on some of the compilers, so we check them\n  # before adding them.\n  check_c_compiler_flag(-Wno-unused-command-line-argument\n                        CC_SUPPORTS_NO_UNUSED_CLI_ARG)\n  if(CC_SUPPORTS_NO_UNUSED_CLI_ARG)\n    add_compile_options(-Wno-unused-command-line-argument)\n  endif()\n\n  check_c_compiler_flag(-Wno-format-truncation CC_SUPPORTS_NO_FORMAT_TRUNCATION)\n  if(CC_SUPPORTS_NO_FORMAT_TRUNCATION)\n    add_compile_options(-Wno-format-truncation)\n  else()\n    message(STATUS \"Compiler does not support -Wno-format-truncation\")\n  endif()\n\n  check_c_compiler_flag(-Wstringop-truncation CC_STRINGOP_TRUNCATION)\n  if(CC_STRINGOP_TRUNCATION)\n    add_compile_options(-Wno-stringop-truncation)\n  else()\n    message(STATUS \"Compiler does not support -Wno-stringop-truncation\")\n  endif()\n\n  check_c_compiler_flag(-Wimplicit-fallthrough CC_SUPPORTS_IMPLICIT_FALLTHROUGH)\n  if(CC_SUPPORTS_IMPLICIT_FALLTHROUGH)\n    add_compile_options(-Wimplicit-fallthrough)\n  else()\n    message(STATUS \"Compiler does not support -Wimplicit-fallthrough\")\n  endif()\n\n  check_c_compiler_flag(-Wnewline-eof CC_SUPPORTS_NEWLINE_EOF)\n  if(CC_SUPPORTS_NEWLINE_EOF)\n    add_compile_options(-Wnewline-eof)\n  endif()\n\n  # strict overflow check produces false positives on gcc < 8\n  if(CMAKE_COMPILER_IS_GNUCC AND CMAKE_C_COMPILER_VERSION VERSION_LESS 8)\n    add_compile_options(-Wno-strict-overflow)\n  endif()\n\n  # -Wclobbered produces false positives on gcc < 9\n  if(CMAKE_COMPILER_IS_GNUCC AND CMAKE_C_COMPILER_VERSION VERSION_LESS 9)\n    add_compile_options(-Wno-clobbered)\n  endif()\n\n  if(CMAKE_COMPILER_IS_GNUCC)\n    add_compile_options(\n      # Seems to be broken in GCC 11 with designated initializers.\n      -Wno-missing-field-initializers)\n  endif()\n\n  # On UNIX, the compiler needs to support -fvisibility=hidden to hide symbols\n  # by default\n  check_c_compiler_flag(-fvisibility=hidden CC_SUPPORTS_VISIBILITY_HIDDEN)\n\n  if(NOT CC_SUPPORTS_VISIBILITY_HIDDEN)\n    message(\n      FATAL_ERROR\n        \"The compiler ${CMAKE_C_COMPILER_ID} does not support -fvisibility=hidden\"\n    )\n  endif(NOT CC_SUPPORTS_VISIBILITY_HIDDEN)\nendif()\n\n# On Windows, default to only include Release builds so MSBuild.exe 'just works'\nif(WIN32 AND NOT CMAKE_CONFIGURATION_TYPES)\n  set(CMAKE_CONFIGURATION_TYPES\n      Release\n      CACHE\n        STRING\n        \"Semicolon separated list of supported configuration types, only supports Debug, Release, MinSizeRel, and RelWithDebInfo, anything else will be ignored.\"\n        FORCE)\nendif()\n\nmessage(STATUS \"Using compiler ${CMAKE_C_COMPILER_ID}\")\n\nif(ENABLE_OPTIMIZER_DEBUG)\n  message(\n    STATUS\n      \"Enabling OPTIMIZER_DEBUG. Make sure that ${PG_SOURCE_DIR} is installed and built with OPTIMIZER_DEBUG.\"\n  )\n  add_definitions(-DOPTIMIZER_DEBUG)\nendif()\n\n# Search paths for Postgres binaries\nif(WIN32)\n  find_path(\n    PG_PATH postgres.exe\n    PATHS \"C:/PostgreSQL\" \"C:/Program Files/PostgreSQL\"\n    PATH_SUFFIXES bin 14/bin 15/bin 16/bin\n    DOC \"The path to a PostgreSQL installation\")\nelseif(UNIX)\n  find_path(\n    PG_PATH postgres\n    PATHS $ENV{HOME} /opt/local/pgsql /usr/local/pgsql /usr/lib/postgresql\n    PATH_SUFFIXES bin 14/bin 15/bin 16/bin\n    DOC \"The path to a PostgreSQL installation\")\nendif()\n\nfind_program(\n  PG_CONFIG pg_config\n  HINTS ${PG_PATH}\n  PATH_SUFFIXES bin\n  DOC \"The path to the pg_config of the PostgreSQL version to compile against\")\n\nif(NOT PG_CONFIG)\n  message(FATAL_ERROR \"Unable to find 'pg_config'\")\nendif()\n\nfind_package(Git)\n\n# Check PostgreSQL version\nexecute_process(\n  COMMAND ${PG_CONFIG} --version\n  OUTPUT_VARIABLE PG_VERSION_STRING\n  OUTPUT_STRIP_TRAILING_WHITESPACE)\n\nif(NOT ${PG_VERSION_STRING} MATCHES\n   \"^PostgreSQL[ ]+([0-9]+)(\\\\.([0-9]+)|beta|devel|rc[0-9]+)\")\n  message(FATAL_ERROR \"Could not parse PostgreSQL version ${PG_VERSION_STRING}\")\nendif()\n\nset(PG_VERSION_MAJOR ${CMAKE_MATCH_1})\nif(${CMAKE_MATCH_COUNT} GREATER \"2\")\n  set(PG_VERSION_MINOR ${CMAKE_MATCH_3})\nelse()\n  set(PG_VERSION_MINOR 0)\nendif()\nset(PG_VERSION \"${PG_VERSION_MAJOR}.${PG_VERSION_MINOR}\")\n\nmessage(\n  STATUS\n    \"Compiling against PostgreSQL version ${PG_VERSION} using pg_config '${PG_CONFIG}'\"\n)\n\n# Ensure that PostgreSQL version is supported and consistent with src/compat.h\n# version check\nif((${PG_VERSION_MAJOR} LESS \"14\")\n   OR (${PG_VERSION_MAJOR} GREATER \"17\")\n   AND NOT (${EXPERIMENTAL}))\n  message(FATAL_ERROR \"TimescaleDB only supports PostgreSQL 14, 15, 16 and 17\")\nendif()\n\n# Get PostgreSQL configuration from pg_config\nget_pg_config(PG_INCLUDEDIR --includedir)\nget_pg_config(PG_INCLUDEDIR_SERVER --includedir-server)\nget_pg_config(PG_LIBDIR --libdir)\nget_pg_config(PG_PKGLIBDIR --pkglibdir)\nget_pg_config(PG_SHAREDIR --sharedir)\nget_pg_config(PG_BINDIR --bindir)\nget_pg_config(PG_CFLAGS --cflags)\nget_pg_config(PG_CFLAGS_SL --cflags_sl)\nget_pg_config(PG_CPPFLAGS --cppflags)\nget_pg_config(PG_LDFLAGS --ldflags)\nget_pg_config(PG_LIBS --libs)\n\nseparate_arguments(PG_CFLAGS)\nforeach(option ${PG_CFLAGS})\n  if(NOT ${option} MATCHES ^-W)\n    set(filtered \"${filtered} ${option}\")\n  endif()\nendforeach()\nset(PG_CFLAGS \"${filtered} ${PG_CFLAGS_SL}\")\n\nfind_path(\n  PG_SOURCE_DIR src/include/pg_config.h.in\n  HINTS $ENV{HOME} $ENV{HOME}/projects $ENV{HOME}/Projects\n        $ENV{HOME}/development $ENV{HOME}/Development $ENV{HOME}/workspace\n  PATH_SUFFIXES postgres postgresql pgsql\n  DOC \"The path to the PostgreSQL source tree\")\n\noption(PG_SOURCE_INCLUDES \"Add PG source to include directories\" OFF)\n\nif(PG_SOURCE_DIR)\n  message(STATUS \"Found PostgreSQL source in ${PG_SOURCE_DIR}\")\n  if(PG_SOURCE_INCLUDES)\n    # Add the PostgreSQL source dir include directories to the build system\n    # includes BEFORE the installed PG include files. This will allow the LSP\n    # (e.g., clangd) to navigate to the PostgreSQL source instead of the install\n    # path directory that only has the headers.\n    include_directories(BEFORE SYSTEM ${PG_SOURCE_DIR}/src/include)\n  endif(PG_SOURCE_INCLUDES)\nendif(PG_SOURCE_DIR)\n\nset(EXT_CONTROL_FILE ${PROJECT_NAME}.control)\nconfigure_file(${EXT_CONTROL_FILE}.in ${EXT_CONTROL_FILE})\n\ninstall(FILES ${CMAKE_CURRENT_BINARY_DIR}/${EXT_CONTROL_FILE}\n        DESTINATION \"${PG_SHAREDIR}/extension\")\n\nfind_program(\n  CLANG_FORMAT\n  NAMES clang-format-14 clang-format\n  PATHS /usr/bin /usr/local/bin /usr/local/opt/ /usr/local/opt/llvm/bin /opt/bin\n  DOC \"The path to clang-format\")\n\nif(CLANG_FORMAT)\n  execute_process(\n    COMMAND ${CLANG_FORMAT} --version\n    OUTPUT_VARIABLE CLANG_FORMAT_VERSION_OUTPUT\n    OUTPUT_STRIP_TRAILING_WHITESPACE)\n\n  if(NOT ${CLANG_FORMAT_VERSION_OUTPUT} MATCHES\n     \"version[ ]+([0-9]+)\\\\.([0-9]+)(\\\\.([0-9]+))*\")\n    message(\n      FATAL_ERROR\n        \"Could not parse clang-format version ${CLANG_FORMAT_VERSION_OUTPUT}\")\n  endif()\n\n  if((${CMAKE_MATCH_1} LESS \"14\"))\n    message(WARNING \"clang-format version 14 or greater required\")\n    set(CLANG_FORMAT False)\n  endif()\nendif()\n\nif(CLANG_FORMAT)\n  message(STATUS \"Using local clang-format\")\n  add_custom_target(\n    clang-format COMMAND ${CMAKE_COMMAND} -E env CLANG_FORMAT=${CLANG_FORMAT}\n                         ${PROJECT_SOURCE_DIR}/scripts/clang_format_all.sh)\nendif()\n\nfind_program(\n  CMAKE_FORMAT\n  NAMES cmake-format\n  PATHS /usr/bin /usr/local/bin /usr/local/opt/ /usr/local/opt/llvm/bin /opt/bin\n  DOC \"The path to cmake-format\")\n\nif(CMAKE_FORMAT)\n  add_custom_target(\n    cmake-format COMMAND ${CMAKE_COMMAND} -E env CMAKE_FORMAT=${CMAKE_FORMAT}\n                         ${PROJECT_SOURCE_DIR}/scripts/cmake_format_all.sh)\nendif()\n\nfind_program(\n  PERLTIDY\n  NAMES perltidy\n  PATHS /bin /usr/bin /usr/local/bin /usr/local/opt/ /opt/bin\n  DOC \"The path to perltidy\")\n\nif(PERLTIDY)\n  message(STATUS \"Using perltidy ${PERLTIDY}\")\n  add_custom_target(\n    perltidy\n    COMMAND\n      ${CMAKE_COMMAND} -E env PERLTIDY=${PERLTIDY}\n      PERLTIDY_CONFIG=\"${PROJECT_SOURCE_DIR}/.perltidyrc\"\n      ${PROJECT_SOURCE_DIR}/scripts/perltidy_format_all.sh)\nendif()\n\nif(TARGET clang-format\n   OR TARGET cmake-format\n   OR TARGET perltidy)\n  add_custom_target(format)\n  if(TARGET clang-format)\n    add_dependencies(format clang-format)\n  endif()\n  if(TARGET cmake-format)\n    add_dependencies(format cmake-format)\n  endif()\n  if(TARGET perltidy)\n    add_dependencies(format perltidy)\n  endif()\nendif()\n\nif(REGRESS_CHECKS)\n  find_program(PG_REGRESS pg_regress\n               HINTS \"${PG_BINDIR}\" \"${PG_PKGLIBDIR}/pgxs/src/test/regress/\")\n\n  if(NOT PG_REGRESS)\n    message(STATUS \"Regress checks disabled: program 'pg_regress' not found\")\n  endif()\n\n  find_program(\n    PG_ISOLATION_REGRESS\n    NAMES pg_isolation_regress\n    HINTS ${PG_BINDIR} ${PG_PKGLIBDIR}/pgxs/src/test/isolation\n          ${PG_SOURCE_DIR}/src/test/isolation ${BINDIR})\n\n  if(NOT PG_ISOLATION_REGRESS)\n    message(\n      STATUS\n        \"Isolation regress checks disabled: 'pg_isolation_regress' not found\")\n  endif()\nelse()\n  message(STATUS \"Regress checks and isolation checks disabled\")\nendif()\n\n# Linter support via clang-tidy. Enabled when using clang as compiler\noption(LINTER \"Enable linter support using clang-tidy\" OFF)\n\nif(LINTER)\n  find_program(\n    CLANG_TIDY clang-tidy\n    PATHS /usr/bin /usr/local/bin /usr/local/opt/ /usr/local/opt/llvm/bin\n          /opt/bin\n    DOC \"The path to the clang-tidy linter\")\n\n  if(CLANG_TIDY)\n    message(STATUS \"Using clang-tidy ${CLANG_TIDY}\")\n    execute_process(COMMAND ${CLANG_TIDY} --version)\n    string(\n      CONCAT\n        CMAKE_C_CLANG_TIDY\n        \"${CLANG_TIDY}\"\n        \";--checks=clang-diagnostic-*,clang-analyzer-*\"\n        \",-clang-analyzer-security.insecureAPI.DeprecatedOrUnsafeBufferHandling\"\n        \",-clang-analyzer-deadcode.DeadStores\"\n        \",bugprone-*\"\n        \",-bugprone-branch-clone\"\n        \",-bugprone-easily-swappable-parameters\"\n        \",-bugprone-implicit-widening-of-multiplication-result\"\n        \",-bugprone-narrowing-conversions\"\n        \",-bugprone-reserved-identifier\"\n        \",-bugprone-suspicious-include\"\n        \",readability-*\"\n        \",-readability-avoid-const-params-in-decls\"\n        \",-readability-braces-around-statements\"\n        \",-readability-else-after-return\"\n        \",-readability-function-cognitive-complexity\"\n        \",-readability-function-size\"\n        \",-readability-identifier-length\"\n        \",-readability-isolate-declaration\"\n        \",-readability-magic-numbers\"\n        \",-readability-non-const-parameter\"\n        # \";--fix-errors\"\n    )\n    if(WARNINGS_AS_ERRORS)\n      set(CMAKE_C_CLANG_TIDY \"${CMAKE_C_CLANG_TIDY};--warnings-as-errors=*\")\n    else()\n      set(CMAKE_C_CLANG_TIDY \"${CMAKE_C_CLANG_TIDY};--quiet\")\n    endif(WARNINGS_AS_ERRORS)\n  else()\n    message(STATUS \"Install clang-tidy to enable code linting\")\n  endif(CLANG_TIDY)\nendif(LINTER)\n\nif(NOT EXISTS ${PG_INCLUDEDIR}/pg_config.h)\n  message(\n    FATAL_ERROR\n      \"Could not find pg_config.h in ${PG_INCLUDEDIR}. \"\n      \"Make sure PG_PATH points to a valid PostgreSQL installation that includes development headers.\"\n  )\nendif()\n\nfile(READ ${PG_INCLUDEDIR}/pg_config.h PG_CONFIG_H)\nstring(REGEX MATCH \"#define USE_ASSERT_CHECKING 1\" PG_USE_ASSERT_CHECKING\n             ${PG_CONFIG_H})\n\nif(PG_USE_ASSERT_CHECKING AND NOT ASSERTIONS)\n  message(\n    STATUS\n      \"Assertion checks are OFF although enabled in PostgreSQL build (pg_config.h). \"\n      \"The PostgreSQL setting for assertions will take precedence.\")\nelseif(ASSERTIONS)\n  message(STATUS \"Assertion checks are ON\")\n  add_compile_definitions(USE_ASSERT_CHECKING=1)\nelseif(CMAKE_BUILD_TYPE MATCHES Debug)\n  message(\n    \"Assertion checks are OFF in Debug build. Set -DASSERTIONS=ON to enable assertions.\"\n  )\nelse()\n  message(STATUS \"Assertion checks are OFF\")\nendif()\n\n# Check if PostgreSQL has OpenSSL enabled by inspecting pg_config.h. Right now,\n# a Postgres header will redefine an OpenSSL function if Postgres is not\n# installed --with-openssl, so in order for TimescaleDB to compile correctly\n# with OpenSSL, Postgres must also have OpenSSL enabled.\ncheck_symbol_exists(USE_OPENSSL ${PG_INCLUDEDIR}/pg_config.h PG_USE_OPENSSL)\n\nif(USE_OPENSSL AND (NOT PG_USE_OPENSSL))\n  message(\n    FATAL_ERROR\n      \"PostgreSQL was built without OpenSSL support, which TimescaleDB needs for full compatibility. Please rebuild PostgreSQL using `--with-openssl` or if you want to continue without OpenSSL, re-run bootstrap with `-DUSE_OPENSSL=0`\"\n  )\nendif(USE_OPENSSL AND (NOT PG_USE_OPENSSL))\n\n# While we dont link directly against OpenSSL on non-Windows, doing this on\n# Windows causes linker errors. So on Windows we link directly against the\n# OpenSSL libraries.\nif(USE_OPENSSL AND MSVC)\n  # Try to find a local OpenSSL installation\n  find_package(OpenSSL)\n\n  if(NOT OPENSSL_FOUND)\n    message(\n      FATAL_ERROR\n        \"TimescaleDB requires OpenSSL but it wasn't found. If you want to continue without OpenSSL, re-run bootstrap with `-DUSE_OPENSSL=0`\"\n    )\n  endif(NOT OPENSSL_FOUND)\n\n  if(${OPENSSL_VERSION} VERSION_LESS \"1.0\")\n    message(FATAL_ERROR \"TimescaleDB requires OpenSSL version 1.0 or greater\")\n  endif()\n\n  set(_libraries)\n  foreach(_path ${OPENSSL_LIBRARIES})\n    if(EXISTS \"${_path}\")\n      list(APPEND _libraries ${_path})\n    else()\n      # check if a release version of the libraries are available\n      if(CMAKE_BUILD_TYPE STREQUAL \"Debug\" AND MSVC)\n        get_filename_component(_dir ${_path} DIRECTORY)\n        get_filename_component(_name ${_path} NAME_WE)\n        string(REGEX REPLACE \"[Dd]$\" \"\" _fixed ${_name})\n        get_filename_component(_ext ${_path} EXT)\n        set(_new_path \"${_dir}/${_fixed}${_ext}\")\n        if(EXISTS \"${_new_path}\")\n          list(APPEND _libraries ${_new_path})\n        endif()\n      endif()\n    endif()\n  endforeach()\n  set(OPENSSL_LIBRARIES ${_libraries})\n\n  foreach(_path ${OPENSSL_LIBRARIES})\n    message(STATUS \"OpenSSL libraries: ${_path}\")\n  endforeach()\n  message(STATUS \"Using OpenSSL version ${OPENSSL_VERSION}\")\nendif(USE_OPENSSL AND MSVC)\n\nif(CODECOVERAGE)\n  message(STATUS \"Code coverage is enabled.\")\n  # Note that --coverage is synonym for the necessary compiler and linker flags\n  # for the given compiler.  For example, with GCC, --coverage translates to\n  # -fprofile-arcs -ftest-coverage when compiling and -lgcov when linking\n  add_compile_options(--coverage -O0)\n  add_link_options(--coverage)\nendif(CODECOVERAGE)\n\n# TAP test support\noption(TAP_CHECKS \"Enable TAP test support\" ON)\n\nif(TAP_CHECKS)\n  find_package(Perl 5.8)\n\n  if(PERL_FOUND)\n    get_filename_component(PERL_BIN_PATH ${PERL_EXECUTABLE} DIRECTORY)\n\n    find_program(\n      PROVE prove\n      HINTS ${PERL_BIN_PATH}\n      PATHS \"/usr/bin\")\n\n    if(NOT PROVE)\n      message(STATUS \"Not running TAP tests: 'prove' binary not found.\")\n      set(TAP_CHECKS OFF)\n    endif()\n\n    # Check for the IPC::Run module\n    execute_process(\n      COMMAND ${PERL_EXECUTABLE} -MIPC::Run -e \"\"\n      ERROR_QUIET\n      RESULT_VARIABLE PERL_MODULE_STATUS)\n\n    if(PERL_MODULE_STATUS)\n      message(STATUS \"Not running TAP tests: IPC::Run Perl module not found.\")\n      set(TAP_CHECKS OFF)\n    endif()\n  else()\n    message(STATUS \"Not running TAP tests: Perl not found.\")\n    set(TAP_CHECKS OFF)\n  endif()\nendif()\n\nif(UNIX)\n  add_subdirectory(scripts)\nendif(UNIX)\n\nadd_subdirectory(sql)\nadd_subdirectory(test)\nadd_subdirectory(src)\n\nif(NOT APACHE_ONLY)\n  add_subdirectory(tsl)\nendif()\n\nadd_custom_target(licensecheck\n                  COMMAND ${PROJECT_SOURCE_DIR}/scripts/check_license_all.sh)\n\n# This needs to be the last subdirectory so that other targets are already\n# defined\nif(CODECOVERAGE)\n  add_subdirectory(codecov)\nendif()\n\nif(IS_DIRECTORY ${PROJECT_SOURCE_DIR}/.git)\n  configure_file(${PROJECT_SOURCE_DIR}/scripts/githooks/commit_msg.py\n                 ${PROJECT_SOURCE_DIR}/.git/hooks/commit-msg COPYONLY)\nendif()\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 5.7138671875,
          "content": "# Contributing to TimescaleDB\n\nWe appreciate any help the community can provide to make TimescaleDB better!  \n\nYou can help in different ways:\n\n* Open an [issue](https://github.com/timescale/timescaledb/issues) with a\n  bug report, build issue, feature request, suggestion, etc.\n\n* Fork this repository and submit a pull request\n\nFor any particular improvement you want to make, it can be beneficial to\nbegin discussion on the GitHub issues page. This is the best place to\ndiscuss your proposed improvement (and its implementation) with the core\ndevelopment team.\n\nBefore we accept any code contributions, Timescale contributors need to\nsign the [Contributor License Agreement](https://cla-assistant.io/timescale/timescaledb) (CLA). By signing a CLA, we can\nensure that the community is free and confident in its ability to use your\ncontributions.\n\n## Getting and building TimescaleDB\n\nPlease follow our README for [instructions on installing from source](https://github.com/timescale/timescaledb/blob/main/README.md#option-3---from-source).\n\n## Style guide\n\nBefore submitting any contributions, please ensure that it adheres to\nour [Style Guide](docs/StyleGuide.md).\n\n## Code review workflow\n\n* Sign the [Contributor License Agreement](https://cla-assistant.io/timescale/timescaledb) (CLA) if you're a new contributor.\n\n* Develop on your local branch:\n\n    * Fork the repository and create a local feature branch to do work on,\n      ideally on one thing at a time.  Don't mix bug fixes with unrelated\n      feature enhancements or stylistical changes.\n\n    * Hack away. Add tests for non-trivial changes.\n\n    * Run the [test suite](#testing) and make sure everything passes.\n\n    * When committing, be sure to write good commit messages according to [these\n      seven rules](https://chris.beams.io/posts/git-commit/#seven-rules). Doing \n      `git commit` prints a message if any of the rules is violated. \n      Stylistically,\n      we use commit message titles in the imperative tense, e.g., `Add\n      merge-append query optimization for time aggregate`.  In the case of\n      non-trivial changes, include a longer description in the commit message\n      body explaining and detailing the changes.  That is, a commit message\n      should have a short title, followed by a empty line, and then\n      followed by the longer description.\n\n    * When committing, link which GitHub issue of [this \n      repository](https://github.com/timescale/timescaledb/issues) is fixed or \n      closed by the commit with a [linking keyword recognised by \n      GitHub](https://docs.github.com/en/github/managing-your-work-on-github/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword). \n      For example, if the commit fixes bug 123, add a line at the end of the \n      commit message with  `Fixes #123`, if the commit implements feature \n      request 321, add a line at the end of the commit message `Closes #321`.\n      This will be recognized by GitHub. It will close the corresponding issue \n      and place a hyperlink under the number.\n\n* Push your changes to an upstream branch:\n\n    * Make sure that each commit in the pull request will represent a\n      logical change to the code, will compile, and will pass tests.\n\n    * Make sure that the pull request message contains all important \n      information from the commit messages including which issues are\n      fixed and closed. If a pull request contains one commit only, then\n      repeating the commit message is preferred, which is done automatically\n      by GitHub when it creates the pull request.\n\n    * Rebase your local feature branch against main (`git fetch origin`,\n      then `git rebase origin/main`) to make sure you're\n      submitting your changes on top of the newest version of our code.\n\n    * When finalizing your PR (i.e., it has been approved for merging),\n      aim for the fewest number of commits that\n      make sense. That is, squash any \"fix up\" commits into the commit they\n      fix rather than keep them separate. Each commit should represent a\n      clean, logical change and include a descriptive commit message.\n\n    * Push your commit to your upstream feature branch: `git push -u <yourfork> my-feature-branch`\n\n* Create and manage pull request:\n\n    * [Create a pull request using GitHub](https://help.github.com/articles/creating-a-pull-request).\n      If you know a core developer well suited to reviewing your pull\n      request, either mention them (preferably by GitHub name) in the PR's\n      body or [assign them as a reviewer](https://help.github.com/articles/assigning-issues-and-pull-requests-to-other-github-users/).\n\n    * If you get a test failure in the CI, check them under [Github Actions](https://github.com/timescale/timescaledb/actions)\n\n    * Address feedback by amending your commit(s). If your change contains\n      multiple commits, address each piece of feedback by amending that\n      commit to which the particular feedback is aimed.\n\n    * The PR is marked as accepted when the reviewer thinks it's ready to be\n      merged.  Most new contributors aren't allowed to merge themselves; in\n      that case, we'll do it for you.\n\n## Testing\n\nEvery non-trivial change to the code base should be accompanied by a\nrelevant addition to or modification of the test suite.\n\nPlease check that the full test suite (including your test additions\nor changes) passes successfully on your local machine **before you\nopen a pull request**.\n\nIf you are running locally:\n```bash\n# Use Debug build mode for full battery of tests\n./bootstrap -DCMAKE_BUILD_TYPE=Debug\ncd build && make\nmake installcheck\n```\n\nAll submitted pull requests are also automatically\nrun against our test suite via [Github Actions](https://github.com/timescale/timescaledb/actions)\n(that link shows the latest build status of the repository).\n\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 0.7880859375,
          "content": "Source code in this repository is variously licensed under the Apache License\nVersion 2.0, an Apache compatible license, or the Timescale License.\n\nAll source code should have information at the beginning of its respective file\nwhich specifies its licensing information.\n\n* Outside of the \"tsl\" directory, source code in a given file is licensed\n  under the Apache License Version 2.0, unless otherwise noted (e.g., an\n  Apache-compatible license).\n\n* Within the \"tsl\" folder, source code in a given file is licensed under the\n  Timescale License, unless otherwise noted.\n\nWhen built, separate shared object files are generated for the Apache-licensed\nsource code and the Timescale-licensed source code. The shared object binaries\nthat contain `-tsl` in their name are licensed under the Timescale License.\n"
        },
        {
          "name": "LICENSE-APACHE",
          "type": "blob",
          "size": 9.935546875,
          "content": "\n                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n"
        },
        {
          "name": "NOTICE",
          "type": "blob",
          "size": 2.41015625,
          "content": "TimescaleDB (TM)\n\nCopyright (c) 2017-2024  Timescale, Inc. All Rights Reserved.\nCopyright (c) 2016-2017  440 Labs, Inc. dba Timescale. All Rights Reserved.\n\nSource code in this repository is variously licensed under the Apache License\nVersion 2.0, an Apache-compatible license, or the Timescale License. Please see\nLICENSE for more information.\n\n* For a copy of the Apache License Version 2.0, please see LICENSE-APACHE\n  as included in this repository's top-level directory.\n\n* For a copy of the Timescale License, please see LICENSE-TIMESCALE\n  as included in this repository's \"tsl\" directory.\n\n* For a copy of all other Apache-compatible licenses and notices,\n  please see below.\n\n\n========================================================================\nNOTICES\n========================================================================\n\nCertain files in this code base have been modified and/or copied,\neither partially or wholely, from source code from the PostgreSQL\ndatabase management system, which is licensed under the open-source\nPostgreSQL License with the following copyright information.\n\nThe PostgreSQL License\n========================================================================\n\nPostgreSQL Database Management System\n(formerly known as Postgres, then as Postgres95)\n\nPortions Copyright (c) 1996-2022, The PostgreSQL Global Development Group\nPortions Copyright (c) 1994, The Regents of the University of California\n\nPermission to use, copy, modify, and distribute this software and its\ndocumentation for any purpose, without fee, and without a written\nagreement is hereby granted, provided that the above copyright notice\nand this paragraph and the following two paragraphs appear in all\ncopies.\n\nIN NO EVENT SHALL THE UNIVERSITY OF CALIFORNIA BE LIABLE TO ANY PARTY\nFOR DIRECT, INDIRECT, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES,\nINCLUDING LOST PROFITS, ARISING OUT OF THE USE OF THIS SOFTWARE AND\nITS DOCUMENTATION, EVEN IF THE UNIVERSITY OF CALIFORNIA HAS BEEN\nADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\nTHE UNIVERSITY OF CALIFORNIA SPECIFICALLY DISCLAIMS ANY WARRANTIES,\nINCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF\nMERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE SOFTWARE\nPROVIDED HEREUNDER IS ON AN \"AS IS\" BASIS, AND THE UNIVERSITY OF\nCALIFORNIA HAS NO OBLIGATIONS TO PROVIDE MAINTENANCE, SUPPORT,\nUPDATES, ENHANCEMENTS, OR MODIFICATIONS.\n\n========================================================================\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 10.361328125,
          "content": "<div align=center>\n<picture align=center>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://assets.timescale.com/docs/images/timescale-logo-dark-mode.svg\">\n    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://assets.timescale.com/docs/images/timescale-logo-light-mode.svg\">\n    <img alt=\"Timescale logo\" >\n</picture>\n</div>\n\n<div align=center>\n\n<h3>TimescaleDB is a PostgreSQL extension for high-performance real-time analytics on time-series and event data</h3>\n\n[![Docs](https://img.shields.io/badge/Read_the_Timescale_docs-black?style=for-the-badge&logo=readthedocs&logoColor=white)](https://docs.timescale.com/)\n[![SLACK](https://img.shields.io/badge/Ask_the_Timescale_community-black?style=for-the-badge&logo=slack&logoColor=white)](https://timescaledb.slack.com/archives/C4GT3N90X)\n[![Try TimescaleDB for free](https://img.shields.io/badge/Try_Timescale_for_free-black?style=for-the-badge&logo=timescale&logoColor=white)](https://console.cloud.timescale.com/signup)\n\n</div>\n\n## Install TimescaleDB\n\nInstall from a Docker container:\n\n1. Run the TimescaleDB container:\n\n    ```bash\n    docker run -d --name timescaledb -p 5432:5432 -e POSTGRES_PASSWORD=password timescale/timescaledb:latest-pg17\n    ```\n\n1. Connect to a database:\n\n    ```bash\n    docker exec -it timescaledb psql -d \"postgres://postgres:password@localhost/postgres\"\n    ```\n\nSee [other installation options](https://docs.timescale.com/self-hosted/latest/install/) or try [Timescale Cloud](https://docs.timescale.com/getting-started/latest/) for free.\n\n## Create a hypertable\n\nYou create a regular table and then convert it into a hypertable. A hypertable automatically partitions data into chunks based on your configuration.\n\n```sql\n-- Create timescaledb extension\nCREATE EXTENSION IF NOT EXISTS timescaledb;\n\n-- Create a regular SQL table\nCREATE TABLE conditions (\n  time        TIMESTAMPTZ       NOT NULL,\n  location    TEXT              NOT NULL,\n  temperature DOUBLE PRECISION  NULL,\n  humidity    DOUBLE PRECISION  NULL\n);\n\n-- Convert the table into a hypertable that is partitioned by time\nSELECT create_hypertable('conditions', by_range('time'));\n```\n\nSee more:\n\n- [About hypertables](https://docs.timescale.com/use-timescale/latest/hypertables/)\n- [API reference](https://docs.timescale.com/api/latest/hypertable/)\n\n## Enable columnstore\n\nTimescaleDB's hypercore is a hybrid row-columnar store that boosts analytical query performance on your time-series and event data, while reducing data size by more than 90%. This keeps your queries operating at lightning speed and ensures low storage costs as you scale. Data is inserted in row format in the rowstore and converted to columnar format in the columnstore based on your configuration.\n\n- Configure the columnstore on a hypertable:\n\n    ```sql\n    ALTER TABLE conditions SET (\n      timescaledb.compress,\n      timescaledb.compress_segmentby = 'location'\n    );\n    ```\n\n- Create a policy to automatically convert chunks in row format that are older than seven days to chunks in the columnar format:\n\n    ```sql\n    SELECT add_compression_policy('conditions', INTERVAL '7 days');\n    ```\n\nSee more:\n\n- [About columnstore](https://docs.timescale.com/use-timescale/latest/compression/about-compression/)\n- [Enable columnstore manually](https://docs.timescale.com/use-timescale/latest/compression/manual-compression/)\n- [API reference](https://docs.timescale.com/api/latest/compression/)\n\n## Insert and query data\n\nInsert and query data in a hypertable via regular SQL commands. For example:\n\n- Insert data into a hypertable named `conditions`:\n\n    ```sql\n    INSERT INTO conditions\n      VALUES\n        (NOW(), 'office',   70.0, 50.0),\n        (NOW(), 'basement', 66.5, 60.0),\n        (NOW(), 'garage',   77.0, 65.2);\n    ```\n\n- Return the number of entries written to the table conditions in the last 12 hours:\n\n    ```sql\n    SELECT\n      COUNT(*)\n    FROM\n      conditions\n    WHERE\n      time > NOW() - INTERVAL '12 hours';\n    ```\n\nSee more:\n\n- [Query data](https://docs.timescale.com/use-timescale/latest/query-data/)\n- [Write data](https://docs.timescale.com/use-timescale/latest/write-data/)\n\n## Create time buckets\n\nTime buckets enable you to aggregate data in hypertables by time interval and calculate summary values.\n\nFor example, calculate the average daily temperature in a table named `conditions`. The table has a `time` and `temperature` columns:\n\n```sql\nSELECT\n  time_bucket('1 day', time) AS bucket,\n  AVG(temperature) AS avg_temp\nFROM\n  conditions\nGROUP BY\n  bucket\nORDER BY\n  bucket ASC;\n```\n\nSee more:\n\n- [About time buckets](https://docs.timescale.com/use-timescale/latest/time-buckets/about-time-buckets/)\n- [API reference](https://docs.timescale.com/api/latest/hyperfunctions/time_bucket/)\n- [All TimescaleDB features](https://docs.timescale.com/use-timescale/latest/)\n- [Tutorials](https://docs.timescale.com/tutorials/latest/)\n\n## Create continuous aggregates\n\nContinuous aggregates are designed to make queries on very large datasets run faster. They continuously and incrementally refresh a query in the background, so that when you run such query, only the data that has changed needs to be computed, not the entire dataset. This is what makes them different from regular PostgreSQL [materialized views](https://www.postgresql.org/docs/current/rules-materializedviews.html), which cannot be incrementally materialized and have to be rebuilt from scratch every time you want to refresh it.\n\nFor example, create a continuous aggregate view for daily weather data in two simple steps:\n\n1. Create a materialized view:\n\n   ```sql\n   CREATE MATERIALIZED VIEW conditions_summary_daily\n   WITH (timescaledb.continuous) AS\n   SELECT\n     location,\n     time_bucket(INTERVAL '1 day', time) AS bucket,\n     AVG(temperature),\n     MAX(temperature),\n     MIN(temperature)\n   FROM\n     conditions\n   GROUP BY\n     location,\n     bucket;\n   ```\n\n1. Create a policy to refresh the view every hour:\n\n   ```sql\n   SELECT\n     add_continuous_aggregate_policy(\n       'conditions_summary_daily',\n       start_offset => INTERVAL '1 month',\n       end_offset => INTERVAL '1 day',\n       schedule_interval => INTERVAL '1 hour'\n   );\n   ```\nSee more:\n\n- [About continuous aggregates](https://docs.timescale.com/use-timescale/latest/continuous-aggregates/)\n- [API reference](https://docs.timescale.com/api/latest/continuous-aggregates/create_materialized_view/)\n\n## Want TimescaleDB hosted and managed for you? Try Timescale Cloud\n\n[Timescale Cloud](https://docs.timescale.com/getting-started/latest/) is a cloud-based PostgreSQL platform for resource-intensive workloads. We help you build faster, scale further, and stay under budget. A Timescale Cloud service is a single optimized 100% PostgreSQL database instance that you use as is, or extend with capabilities specific to your business needs. The available capabilities are:\n\n- **Time-series and analytics**: PostgreSQL with TimescaleDB. The PostgreSQL you know and love, supercharged with functionality for storing and querying time-series data at scale for analytics and other use cases. Get faster time-based queries with hypertables, continuous aggregates, and columnar storage. Save on storage with native compression, data retention policies, and bottomless data tiering to Amazon S3.\n- **AI and vector**: PostgreSQL with vector extensions. Use PostgreSQL as a vector database with purpose built extensions for building AI applications from start to scale. Get fast and accurate similarity search with the pgvector and pgvectorscale extensions. Create vector embeddings and perform LLM reasoning on your data with the pgai extension.\n- **PostgreSQL**: the trusted industry-standard RDBMS. Ideal for applications requiring strong data consistency, complex relationships, and advanced querying capabilities. Get ACID compliance, extensive SQL support, JSON handling, and extensibility through custom functions, data types, and extensions.\nAll services include all the cloud tooling you'd expect for production use: [automatic backups](https://docs.timescale.com/use-timescale/latest/backup-restore/backup-restore-cloud/), [high availability](https://docs.timescale.com/use-timescale/latest/ha-replicas/), [read replicas](https://docs.timescale.com/use-timescale/latest/ha-replicas/read-scaling/), [data forking](https://docs.timescale.com/use-timescale/latest/services/service-management/#fork-a-service), [connection pooling](https://docs.timescale.com/use-timescale/latest/services/connection-pooling/), [tiered storage](https://docs.timescale.com/use-timescale/latest/data-tiering/), [usage-based storage](https://docs.timescale.com/about/latest/pricing-and-account-management/), and much more.\n\n## Check build status\n\n|Linux/macOS|Linux i386|Windows|Coverity|Code Coverage|OpenSSF|\n|:---:|:---:|:---:|:---:|:---:|:---:|\n|[![Build Status Linux/macOS](https://github.com/timescale/timescaledb/actions/workflows/linux-build-and-test.yaml/badge.svg?branch=main&event=schedule)](https://github.com/timescale/timescaledb/actions/workflows/linux-build-and-test.yaml?query=workflow%3ARegression+branch%3Amain+event%3Aschedule)|[![Build Status Linux i386](https://github.com/timescale/timescaledb/actions/workflows/linux-32bit-build-and-test.yaml/badge.svg?branch=main&event=schedule)](https://github.com/timescale/timescaledb/actions/workflows/linux-32bit-build-and-test.yaml?query=workflow%3ARegression+branch%3Amain+event%3Aschedule)|[![Windows build status](https://github.com/timescale/timescaledb/actions/workflows/windows-build-and-test.yaml/badge.svg?branch=main&event=schedule)](https://github.com/timescale/timescaledb/actions/workflows/windows-build-and-test.yaml?query=workflow%3ARegression+branch%3Amain+event%3Aschedule)|[![Coverity Scan Build Status](https://scan.coverity.com/projects/timescale-timescaledb/badge.svg)](https://scan.coverity.com/projects/timescale-timescaledb)|[![Code Coverage](https://codecov.io/gh/timescale/timescaledb/branch/main/graphs/badge.svg?branch=main)](https://codecov.io/gh/timescale/timescaledb)|[![OpenSSF Best Practices](https://www.bestpractices.dev/projects/8012/badge)](https://www.bestpractices.dev/projects/8012)|\n\n## Get involved\n\nWe welcome contributions to TimescaleDB! See [Contributing](https://github.com/timescale/timescaledb/blob/main/CONTRIBUTING.md) and [Code style guide](https://github.com/timescale/timescaledb/blob/main/docs/StyleGuide.md) for details.\n\n## Learn about Timescale\n\nTimescale is PostgreSQL made powerful. To learn more about the company and its products, visit [timescale.com](https://www.timescale.com).\n\n"
        },
        {
          "name": "SECURITY.md",
          "type": "blob",
          "size": 1.4560546875,
          "content": "# Security Policy\n\nWe aim to keep TimescaleDB safe for everyone. \nPublicly disclosing security bugs in a public forum can put everyone in the Timescale community at risk,\nhowever. Therefore, we ask that people follow the below instructions to report security vulnerability.\nThe entire Timescale community thanks you!\n\n## Supported Versions\n\nThe supported version is always the latest major release available in our repository.\nWe also release regular minor versions with fixes and corrections alongside some new features as well as patchfix releases, that you should keep upgrading to.\nVulnerability fixes are made available as part of these patchfix releases and you can read our list of [Security Advisories](https://github.com/timescale/timescaledb/security/advisories?state=published).\n \nYou can also take a look at our [Support Policy](https://www.timescale.com/legal/support-policy).\n\n\n## Reporting a Vulnerability\n\nIf you find a vulnerability in our software, please email the Timescale Security Team at security@timescale.com.\n\nPlease note that the e-mail address should only be used for reporting undisclosed security vulnerabilities in Timescale products and services. \nRegular bug reports should be submitted as GitHub issues, while other _questions_ around security,\ncompliance, or functionality can be made either through our support (for customers) or\ncommunity channels (e.g., [Timescale Slack](https://slack.timescale.com/), [Forums](https://www.timescale.com/forums), etc.)\n"
        },
        {
          "name": "bootstrap",
          "type": "blob",
          "size": 1.09375,
          "content": "#!/usr/bin/env bash\n\n# This bootstrap scripts set up the build environment for TimescaleDB\n# Any flags will be passed on to CMake, e.g.,\n# ./bootstrap -DCMAKE_BUILD_TYPE=\"Debug\"\n\n## Check to make cmake is installed\nif ! command -v cmake >/dev/null 2>&1; then\n\techo \"cmake is required to build TimescaleDB. Please install via your system's preferred method.\"\n\texit 1\nfi\n\nBUILD_DIR=${BUILD_DIR:-./build}\nBUILD_FORCE_REMOVE=${BUILD_FORCE_REMOVE:-false}\nSRC_DIR=$(dirname $0)\nif [[ ! ${SRC_DIR} == /* ]]; then\n    SRC_DIR=$(pwd)/${SRC_DIR}\nfi\n\nif [ ${BUILD_FORCE_REMOVE} == \"true\" ]; then\n    rm -fr ${BUILD_DIR}\nelif [ -d ${BUILD_DIR} ]; then\n    echo \"Build system already initialized in ${BUILD_DIR}\"\n\n    read -r -n 1 -p \"Do you want to remove it (this is IMMEDIATE and PERMANENT), y/n? \" choice\n    echo \"\"\n    if [ $choice == \"y\" ]; then\n        rm -fr ${BUILD_DIR}\n    else\n        exit\n    fi\nfi\n\nset -e\nset -u\n\nmkdir -p ${BUILD_DIR} && \\\n    cd ${BUILD_DIR} && \\\n    cmake ${SRC_DIR} \"$@\"\n\necho \"TimescaleDB build system initialized in ${BUILD_DIR}. To compile, do:\"\necho -e \"\\033[1mcd ${BUILD_DIR} && make\\033[0m\"\n"
        },
        {
          "name": "bootstrap.bat",
          "type": "blob",
          "size": 0.755859375,
          "content": "@echo off\n:: This bootstrap scripts set up the build environment for TimescaleDB\n:: Any flags will be passed on to CMake, e.g.,\n:: ./bootstrap.bat -DCMAKE_BUILD_TYPE=\"Debug\"\n\n\n:: Get source directory to build from\nset ORIG=%0\nfor %%F in (%ORIG%) do set SRC_DIR=%%~dpF\n\nSET BUILD_DIR=./build\n\nIF EXIST \"%BUILD_DIR%\" (\n\tsetlocal EnableDelayedExpansion\n\tECHO Build system already initialized in %BUILD_DIR%\n\tSET /P resp=\"Do you want to remove it (this is IMMEDIATE and PERMANENT), y/n? \"\n\tIF \"!resp!\" == \"y\" (\n\t\trd /s /q \"%BUILD_DIR%\"\n\t) ELSE (\n\t\tECHO Exiting\n\t\tEXIT\n\t)\n)\n\nmkdir \"%BUILD_DIR%\"\ncd \"%BUILD_DIR%\"\ncmake %SRC_DIR% -A x64 %*\n\nECHO ---\nECHO TimescaleDB build system initialized in %BUILD_DIR%.\nECHO To compile, do:\nECHO     cmake --build %BUILD_DIR% --config Release\n"
        },
        {
          "name": "cmake",
          "type": "tree",
          "content": null
        },
        {
          "name": "coccinelle",
          "type": "tree",
          "content": null
        },
        {
          "name": "codecov",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "sql",
          "type": "tree",
          "content": null
        },
        {
          "name": "src",
          "type": "tree",
          "content": null
        },
        {
          "name": "test",
          "type": "tree",
          "content": null
        },
        {
          "name": "timescaledb.control.in",
          "type": "blob",
          "size": 0.40234375,
          "content": "# timescaledb extension\ncomment = 'Enables scalable inserts and complex queries for time-series data'\ndefault_version = '@PROJECT_VERSION_MOD@'\nmodule_pathname = '$libdir/timescaledb-@PROJECT_VERSION_MOD@'\n#extension cannot be relocatable once installed because it uses multiple schemas and that is forbidden by PG.\n#(though this extension is relocatable during installation).\nrelocatable = false\ntrusted = true\n"
        },
        {
          "name": "tsl",
          "type": "tree",
          "content": null
        },
        {
          "name": "version.config",
          "type": "blob",
          "size": 0.078125,
          "content": "version = 2.18.0-dev\nupdate_from_version = 2.17.2\ndowngrade_to_version = 2.17.2\n"
        }
      ]
    }
  ]
}