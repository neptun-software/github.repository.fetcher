{
  "metadata": {
    "timestamp": 1736710359993,
    "page": 171,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjE4MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "edenhill/kcat",
      "stars": 5483,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".appveyor.yml",
          "type": "blob",
          "size": 0.392578125,
          "content": "version: 1.8.0-pre-{build}\nimage: Visual Studio 2015\nconfiguration: Release\nenvironment:\n  runtime: v140\n  matrix:\n  - platform: x64\n    arch: x64\n  - platform: win32\n    arch: x86\nbefore_build:\n- cmd: nuget restore win32/kcat.sln\nbuild:\n  project: win32\\kcat.sln\n  verbosity: detailed\ntest: off\nartifacts:\n- path: '**\\*.exe'\n  name: Exe\n- path: '**\\*.dll'\n  name: DLLs\n- path: '**\\*.dll'\n  name: DLLs\n"
        },
        {
          "name": ".dir-locals.el",
          "type": "blob",
          "size": 0.041015625,
          "content": "( (c-mode . ((c-file-style . \"linux\"))) )\n"
        },
        {
          "name": ".dockerignore",
          "type": "blob",
          "size": 0.0234375,
          "content": "win32\ntmp-bootstrap\nrpm\n"
        },
        {
          "name": ".doozer.json",
          "type": "blob",
          "size": 0.970703125,
          "content": "{\n  \"targets\": {\n    \"xenial-amd64\": {\n\n      \"buildenv\": \"xenial-amd64\",\n      \"builddeps\": [\n        \"build-essential\",\n        \"python\",\n        \"curl\",\n        \"cmake\",\n        \"pkg-config\",\n        \"autoconf\",\n        \"dh-autoreconf\",\n        \"libcurl4-openssl-dev\",\n        \"libjansson-dev\",\n        \"libidn11-dev\",\n        \"librtmp-dev\",\n        \"libssl-dev\",\n        \"libkrb5-dev\",\n        \"libldap-dev\",\n        \"libz-dev\"\n      ],\n      \"buildcmd\": [\n          \"./bootstrap.sh\"\n      ]\n    },\n\n    \"xenial-i386\": {\n\n      \"buildenv\": \"xenial-i386\",\n      \"builddeps\": [\n        \"build-essential\",\n        \"python\",\n        \"curl\",\n        \"cmake\",\n        \"pkg-config\",\n        \"autoconf\",\n        \"dh-autoreconf\",\n        \"libcurl4-openssl-dev\",\n        \"libjansson-dev\",\n        \"libidn11-dev\",\n        \"librtmp-dev\",\n        \"libssl-dev\",\n        \"libkrb5-dev\",\n        \"libldap-dev\",\n        \"libz-dev\"\n\n      ],\n      \"buildcmd\": [\n          \"./bootstrap.sh\"\n      ]\n    }\n  }\n}\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.1171875,
          "content": "\\#*\n*~\n*.o\n*.d\nkcat\nconfig.cache\nconfig.log*\nconfig.h\nMakefile.config\ntmp-bootstrap\n*.offset\ncore\ncore.*\nvgcore.*\n.idea\n"
        },
        {
          "name": ".travis.yml",
          "type": "blob",
          "size": 0.68359375,
          "content": "language: c\ndist: bionic\nmatrix:\n  include:\n    - name: \"Linux x64 GCC\"\n      os: linux\n      compiler: gcc\n      arch: amd64\n    - name: \"Linux ppc64le Clang\"\n      os: linux\n      arch: ppc64le\n      compiler: clang\n    - name: \"MacOSX x64 Clang\"\n      os: osx\n      compiler: clang\n      env: HOMEBREW_NO_AUTO_UPDATE=1\n\nbefore_install:\n  - if [[ $TRAVIS_OS_NAME == linux ]]; then sudo apt-get update -qq && sudo apt-get install -y libssl-dev libsasl2-dev libcurl4-openssl-dev libjansson-dev ; fi\n  - if [[ $TRAVIS_OS_NAME == osx ]]; then brew install openssl curl jansson ; fi\n\nscript:\n  - ./bootstrap.sh\n  - if [[ $TRAVIS_OS_NAME == osx ]]; then otool -L kcat ; else ldd kcat ; fi\n  - ./kcat -U\n\n"
        },
        {
          "name": "CHANGELOG.md",
          "type": "blob",
          "size": 2.3291015625,
          "content": "# kcat v1.8.0\n\n * Added new mock cluster mode\n   `kcat -M <broker-cnt>` spins up a mock cluster that applications\n   can produce to and consume from.\n * Mixing modes is now prohibited (e.g., `-P .. -C`).\n * Producer: Fix stdin buffering: no messages would be produced\n   until Ctrl-D or at least 1024 bytes had accumulated (#343).\n\n\n\n# kcat v1.7.1\n\nThe edenhill/kcat image has been updated to librdkafka v1.8.2.\n\n\n# kcat v1.7.0\n\n**kafkacat has been renamed to kcat** to adhere to the\nApache Software Foundation's (ASF) trademark policies.\n\n * `kafkacat` is now called `kcat`.\n * Add support for multibyte delimiters to `-D` and `-K` (#140, #280)\n * Add support for `-X partition.assignment.strategy=cooperative-sticky` incremental rebalancing.\n * High-level consumer `-G` now supports exit-on-eof `-e` option (#86)\n * Avro consumer with -J will now emit `key_schema_id` and `value_schema_id`.\n\n## Upgrade considerations\n\n * Please rename any `kafkacat.conf` config files to `kcat.conf`.\n   The old path will continue to work for some time but a warning will be\n   printed.\n * Please rename any `KAFKACAT_CONF` environment variables to `KCAT_CONF`.\n   The old environment variable will continue to work for some time but a\n   warning will be printed.\n\n\n\n# kafkacat v1.6.0\n\n * Transactional Producer support (see below).\n * Honour `-k <key>` when producing files (#197).\n * Honour `-o <offset>` in `-G` high-level consumer mode (#231).\n * Added `-m <seconds>` argument to set metadata/query/transaction timeouts.\n * Allow `schema.registry.url` to be configured in config file and\n   not only by `-r` (#220).\n * Print broker-id message was produced to (if `-v`),\n   or was consumed from (if `-J`).\n\n## Apache Kafka EOS / Transactional Producer support\n\nMessages can now be produced in a single transaction if `-X transactional.id=..`\nis passed to the producer in `-P` mode.\n\nIf kafkacat is terminated through Ctrl-C (or other signal) the transaction\nwill be aborted, while normal termination (due to stdin closing or after reading\nall supplied files) will commit the transaction.\n\n```bash\n$ kafkacat -b $BROKERS -P -t mytopic -X transactional.id=myproducerapp\n% Using transactional producer\nThis is a transactional message\nAnd so is this\n:)\n[Press Ctrl-D]\n% Committing transaction\n% Transaction successfully committed\n```\n\n\n# Older releases\n\nSee https://github.com/edenhill/kcat/releases\n"
        },
        {
          "name": "Dockerfile",
          "type": "blob",
          "size": 0.955078125,
          "content": "FROM alpine:3.10\n\nCOPY . /usr/src/kcat\n\nENV BUILD_DEPS bash make gcc g++ cmake curl pkgconfig python perl bsd-compat-headers zlib-dev zstd-dev zstd-libs lz4-dev openssl-dev curl-dev\n\nENV RUN_DEPS libcurl lz4-libs zstd-libs ca-certificates\n\n# Kerberos requires a default realm to be set in krb5.conf, which we can't\n# do for obvious reasons. So skip it for now.\n#ENV BUILD_DEPS_EXTRA cyrus-sasl-dev\n#ENV RUN_DEPS_EXTRA libsasl heimdal-libs krb5\n\nRUN echo Installing ; \\\n  apk add --no-cache --virtual .dev_pkgs $BUILD_DEPS $BUILD_DEPS_EXTRA && \\\n  apk add --no-cache $RUN_DEPS $RUN_DEPS_EXTRA && \\\n  echo Building && \\\n  cd /usr/src/kcat && \\\n  rm -rf tmp-bootstrap && \\\n  echo \"Source versions:\" && \\\n  grep ^github_download ./bootstrap.sh && \\\n  ./bootstrap.sh --no-install-deps --no-enable-static && \\\n  mv kcat /usr/bin/ && \\\n  echo Cleaning up && \\\n  cd / && \\\n  rm -rf /usr/src/kcat && \\\n  apk del .dev_pkgs && \\\n  rm -rf /var/cache/apk/*\n\nRUN kcat -V\n\nENTRYPOINT [\"kcat\"]\n"
        },
        {
          "name": "Dockerfile.Debian",
          "type": "blob",
          "size": 0.84375,
          "content": "FROM debian:stretch-slim\n\nCOPY . /usr/src/kcat\n\nENV BUILD_DEPS build-essential zlib1g-dev liblz4-dev libssl-dev libsasl2-dev python cmake libcurl4-openssl-dev pkg-config\nENV RUN_DEPS libssl1.1 libsasl2-2 ca-certificates curl\n\nRUN apt-get update && \\\n  DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends $BUILD_DEPS $RUN_DEPS && \\\n  echo \"Building\" && \\\n  cd /usr/src/kcat && \\\n  rm -rf tmp-bootstrap && \\\n  echo \"Source versions:\" && \\\n  grep ^github_download ./bootstrap.sh && \\\n  ./bootstrap.sh --no-enable-static && \\\n  mv kcat /usr/bin/ ; \\\n  echo \"Cleaning up\" ; \\\n  cd / ; \\\n  rm -rf /usr/src/kcat; \\\n  apt-get purge -y --auto-remove $BUILD_DEPS ; \\\n  apt-get clean -y ; \\\n  apt-get autoclean -y ; \\\n  rm /var/log/dpkg.log /var/log/alternatives.log /var/log/apt/*.log; \\\n  rm -rf /var/lib/apt/lists/*\n\nRUN kcat -V\n\nENTRYPOINT [\"kcat\"]\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.3232421875,
          "content": "librdkafka - Apache Kafka C driver library\n\nCopyright (c) 2012, Magnus Edenhill\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met: \n\n1. Redistributions of source code must retain the above copyright notice,\n   this list of conditions and the following disclaimer. \n2. Redistributions in binary form must reproduce the above copyright notice,\n   this list of conditions and the following disclaimer in the documentation\n   and/or other materials provided with the distribution. \n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE \nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE \nARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE \nLIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR \nCONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF \nSUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS \nINTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN \nCONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\nARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\nPOSSIBILITY OF SUCH DAMAGE.\n"
        },
        {
          "name": "LICENSE.getdelim",
          "type": "blob",
          "size": 2.3642578125,
          "content": "getdelim.c from newlib with Red Hat's copyright and the following license:\n(1) Red Hat Incorporated\n\nCopyright (c) 1994-2009  Red Hat, Inc. All rights reserved.\n\nThis copyrighted material is made available to anyone wishing to use,\nmodify, copy, or redistribute it subject to the terms and conditions\nof the BSD License.   This program is distributed in the hope that \nit will be useful, but WITHOUT ANY WARRANTY expressed or implied, \nincluding the implied warranties of MERCHANTABILITY or FITNESS FOR \nA PARTICULAR PURPOSE.  A copy of this license is available at \nhttp://www.opensource.org/licenses. Any Red Hat trademarks that are\nincorporated in the source code or documentation are not subject to\nthe BSD License and may only be used or replicated with the express\npermission of Red Hat, Inc.\n\n(2) University of California, Berkeley\n\nCopyright (c) 1981-2000 The Regents of the University of California.\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without modification,\nare permitted provided that the following conditions are met:\n\n    * Redistributions of source code must retain the above copyright notice, \n      this list of conditions and the following disclaimer.\n    * Redistributions in binary form must reproduce the above copyright notice,\n      this list of conditions and the following disclaimer in the documentation\n      and/or other materials provided with the distribution.\n    * Neither the name of the University nor the names of its contributors \n      may be used to endorse or promote products derived from this software \n      without specific prior written permission.\n\n  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" \n  AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED \n  WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. \n  IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, \n  INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT \n  NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR \n  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, \n  WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) \n  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY\n  OF SUCH DAMAGE.\n\n"
        },
        {
          "name": "LICENSE.wingetopt",
          "type": "blob",
          "size": 2.537109375,
          "content": "For the files wingetopt.c wingetopt.h downloaded from https://github.com/alex85k/wingetopt\n\n/*\n * Copyright (c) 2002 Todd C. Miller <Todd.Miller@courtesan.com>\n *\n * Permission to use, copy, modify, and distribute this software for any\n * purpose with or without fee is hereby granted, provided that the above\n * copyright notice and this permission notice appear in all copies.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\" AND THE AUTHOR DISCLAIMS ALL WARRANTIES\n * WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF\n * MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR\n * ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES\n * WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN\n * ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF\n * OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.\n *\n * Sponsored in part by the Defense Advanced Research Projects\n * Agency (DARPA) and Air Force Research Laboratory, Air Force\n * Materiel Command, USAF, under agreement number F39502-99-1-0512.\n */\n/*-\n * Copyright (c) 2000 The NetBSD Foundation, Inc.\n * All rights reserved.\n *\n * This code is derived from software contributed to The NetBSD Foundation\n * by Dieter Baron and Thomas Klausner.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions\n * are met:\n * 1. Redistributions of source code must retain the above copyright\n *    notice, this list of conditions and the following disclaimer.\n * 2. Redistributions in binary form must reproduce the above copyright\n *    notice, this list of conditions and the following disclaimer in the\n *    documentation and/or other materials provided with the distribution.\n *\n * THIS SOFTWARE IS PROVIDED BY THE NETBSD FOUNDATION, INC. AND CONTRIBUTORS\n * ``AS IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED\n * TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE FOUNDATION OR CONTRIBUTORS\n * BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n * POSSIBILITY OF SUCH DAMAGE.\n */\n"
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 1.01171875,
          "content": "include Makefile.config\n\nBIN=\tkcat\n\nSRCS_y=\tkcat.c format.c tools.c input.c\nSRCS_$(ENABLE_JSON) += json.c\nSRCS_$(ENABLE_AVRO) += avro.c\nOBJS=\t$(SRCS_y:.c=.o)\n\n.PHONY:\n\nall: $(BIN) TAGS\n\ninclude mklove/Makefile.base\n\n# librdkafka must be compiled with -gstrict-dwarf, but kcat must not,\n# due to some clang bug on OSX 10.9\nCPPFLAGS := $(subst strict-dwarf,,$(CPPFLAGS))\n\ninstall: bin-install install-man\n\ninstall-man:\n\techo $(INSTALL) -d $$DESTDIR$(man1dir) && \\\n\techo $(INSTALL) kcat.1 $$DESTDIR$(man1dir)\n\n\nclean: bin-clean\n\ntest:\n\t$(MAKE) -C tests\n\nTAGS: .PHONY\n\t@(if which etags >/dev/null 2>&1 ; then \\\n\t\techo \"Using etags to generate $@\" ; \\\n\t\tgit ls-tree -r --name-only HEAD | egrep '\\.(c|cpp|h)$$' | \\\n\t\t\tetags -f $@.tmp - ; \\\n\t\tcmp $@ $@.tmp || mv $@.tmp $@ ; rm -f $@.tmp ; \\\n\t elif which ctags >/dev/null 2>&1 ; then \\\n\t\techo \"Using ctags to generate $@\" ; \\\n\t\tgit ls-tree -r --name-only HEAD | egrep '\\.(c|cpp|h)$$' | \\\n\t\t\tctags -e -f $@.tmp -L- ; \\\n\t\tcmp $@ $@.tmp || mv $@.tmp $@ ; rm -f $@.tmp ; \\\n\tfi)\n\n\n-include $(DEPS)\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 10.806640625,
          "content": "![logo by @dtrapezoid](./resources/kcat_small.png)\n\n# kcat\n\n**kcat is the project formerly known as as kafkacat**\n\nkcat and kafkacat are Copyright (c) 2014-2021 Magnus Edenhill\n\n[https://github.com/edenhill/kcat](https://github.com/edenhill/kcat)\n\n*kcat logo by [@dtrapezoid](https://twitter.com/dtrapezoid)*\n\n\n## What is kcat\n\n**kcat** is a generic non-JVM producer and consumer for Apache Kafka >=0.8,\nthink of it as a netcat for Kafka.\n\nIn **producer** mode kcat reads messages from stdin, delimited with a\nconfigurable delimiter (-D, defaults to newline), and produces them to the\nprovided Kafka cluster (-b), topic (-t) and partition (-p).\n\nIn **consumer** mode kcat reads messages from a topic and partition and\nprints them to stdout using the configured message delimiter.\n\nThere's also support for the Kafka >=0.9 high-level balanced consumer, use\nthe `-G <group>` switch and provide a list of topics to join the group.\n\nkcat also features a Metadata list (-L) mode to display the current\nstate of the Kafka cluster and its topics and partitions.\n\nSupports Avro message deserialization using the Confluent Schema-Registry,\nand generic primitive deserializers (see examples below).\n\nkcat is fast and lightweight; statically linked it is no more than 150Kb.\n\n## What happened to kafkacat?\n\n**kcat is kafkacat**. The kafkacat project was renamed to kcat in August 2021\nto adhere to the Apache Software Foundation's (ASF) trademark policies.\nApart from the name, nothing else was changed.\n\n\n## Try it out with docker\n\n```bash\n# List brokers and topics in cluster\n$ docker run -it --network=host edenhill/kcat:1.7.1 -b YOUR_BROKER -L\n```\n\nSee [Examples](#examples) for usage options, and [Running in Docker](#running-in-docker) for more information on how to properly run docker-based clients with Kafka.\n\n\n## Install\n\n### On recent enough Debian systems:\n\n````\napt-get install kafkacat\n````\n\nOn recent openSUSE systems:\n\n```\nzypper addrepo https://download.opensuse.org/repositories/network:utilities/openSUSE_Factory/network:utilities.repo\nzypper refresh\nzypper install kafkacat\n```\n(see [this page](https://software.opensuse.org/download/package?package=kafkacat&project=network%3Autilities) for instructions to install with openSUSE LEAP)\n\n### On Mac OS X with homebrew installed:\n\n````\nbrew install kcat\n````\n\n### On Fedora\n\n```\n# dnf copr enable bvn13/kcat\n# dnf update\n# dnf install kafkacat\n```\n\nSee [this blog](https://rmoff.net/2020/04/20/how-to-install-kafkacat-on-fedora/) for how to build from sources and install kafkacat/kcat on recent Fedora systems.\n\n\n### Otherwise follow directions below\n\n\n## Requirements\n\n * librdkafka - https://github.com/edenhill/librdkafka\n * libyajl (for JSON support, optional)\n * libavro-c and libserdes (for Avro support, optional. See https://github.com/confluentinc/libserdes)\n\nOn Ubuntu or Debian: `sudo apt-get install librdkafka-dev libyajl-dev`\n\n## Build\n\n    ./configure <usual-configure-options>\n    make\n    sudo make install\n\n### Build for Windows\n\n    cd win32\n    nuget restore\n    msbuild\n\n**NOTE**: Requires `Build Tools for Visual Studio 2017` with components `Windows 8.1 SDK` and `VC++ 2015.3 v14.00 (v140) toolset` to be installed.\n\n## Quick build\n\nThe bootstrap.sh build script will download and build the required dependencies,\nproviding a quick and easy means of building kcat.\nInternet connectivity and wget/curl is required by this script.\nThe resulting kcat binary will be linked statically to avoid runtime\ndependencies.\n**NOTE**: Requires `curl` and `cmake` (for yajl) to be installed.\n\n    ./bootstrap.sh\n\n\n## Configuration\n\nAny librdkafka [configuration](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md)\nproperty can be set on the command line using `-X property=value`, or in\na configuration file specified by `-F <config-file>`.\n\nIf no configuration file was specified with `-F ..` on the command line,\nkcat will try the `$KCAT_CONFIG` or (deprecated) `$KAFKACAT_CONFIG`\nenvironment variable,\nand then the default configuration file `~/.config/kcat.conf` or\nthe (deprecated) `~/.config/kafkacat.conf`.\n\nConfiguration files are optional.\n\n\n## Examples\n\nHigh-level balanced KafkaConsumer: subscribe to topic1 and topic2\n(requires broker >=0.9.0 and librdkafka version >=0.9.1)\n\n    $ kcat -b mybroker -G mygroup topic1 topic2\n\n\nRead messages from stdin, produce to 'syslog' topic with snappy compression\n\n    $ tail -f /var/log/syslog | kcat -b mybroker -t syslog -z snappy\n\n\nRead messages from Kafka 'syslog' topic, print to stdout\n\n    $ kcat -b mybroker -t syslog\n\n\nProduce messages from file (one file is one message)\n\n    $ kcat -P -b mybroker -t filedrop -p 0 myfile1.bin /etc/motd thirdfile.tgz\n\n\nProduce messages transactionally (one single transaction for all messages):\n\n    $ kcat -P -b mybroker -t mytopic -X transactional.id=myproducerapp\n\n\nRead the last 2000 messages from 'syslog' topic, then exit\n\n    $ kcat -C -b mybroker -t syslog -p 0 -o -2000 -e\n\n\nConsume from all partitions from 'syslog' topic\n\n    $ kcat -C -b mybroker -t syslog\n\n\nOutput consumed messages in JSON envelope:\n\n    $ kcat -b mybroker -t syslog -J\n\n\nDecode Avro key (`-s key=avro`), value (`-s value=avro`) or both (`-s avro`) to JSON using schema from the Schema-Registry:\n\n    $ kcat -b mybroker -t ledger -s avro -r http://schema-registry-url:8080\n\n\nDecode Avro message value and extract Avro record's \"age\" field:\n\n    $ kcat -b mybroker -t ledger -s value=avro -r http://schema-registry-url:8080 | jq .payload.age\n\n\nDecode key as 32-bit signed integer and value as 16-bit signed integer followed by an unsigned byte followed by string:\n\n    $ kcat -b mybroker -t mytopic -s key='i$' -s value='hB s'\n\n\n*Hint: see `kcat -h` for all available deserializer options.*\n\n\nOutput consumed messages according to format string:\n\n    $ kcat -b mybroker -t syslog -f 'Topic %t[%p], offset: %o, key: %k, payload: %S bytes: %s\\n'\n\n\nRead the last 100 messages from topic 'syslog' with  librdkafka configuration parameter 'broker.version.fallback' set to '0.8.2.1' :\n\n    $ kcat -C -b mybroker -X broker.version.fallback=0.8.2.1 -t syslog -p 0 -o -100 -e\n\n\nProduce a tombstone (a \"delete\" for compacted topics) for key \"abc\" by providing an empty message value which `-Z` interpretes as NULL:\n\n    $ echo \"abc:\" | kcat -b mybroker -t mytopic -Z -K:\n\n\nProduce with headers:\n\n    $ echo \"hello there\" | kcat -b mybroker -P -t mytopic -H \"header1=header value\" -H \"nullheader\" -H \"emptyheader=\" -H \"header1=duplicateIsOk\"\n\n\nPrint headers in consumer:\n\n    $ kcat -b mybroker -C -t mytopic -f 'Headers: %h: Message value: %s\\n'\n\n\nEnable the idempotent producer, providing exactly-once and strict-ordering\n**producer** guarantees:\n\n    $ kcat -b mybroker -X enable.idempotence=true -P -t mytopic ....\n\n\nConnect to cluster using SSL and SASL PLAIN authentication:\n\n    $ kcat -b mybroker -X security.protocol=SASL_SSL -X sasl.mechanism=PLAIN -X sasl.username=myapikey -X sasl.password=myapisecret ...\n\n\nMetadata listing:\n\n```\n$ kcat -L -b mybroker\nMetadata for all topics (from broker 1: mybroker:9092/1):\n 3 brokers:\n  broker 1 at mybroker:9092\n  broker 2 at mybrokertoo:9092\n  broker 3 at thirdbroker:9092\n 16 topics:\n  topic \"syslog\" with 3 partitions:\n    partition 0, leader 3, replicas: 1,2,3, isrs: 1,2,3\n    partition 1, leader 1, replicas: 1,2,3, isrs: 1,2,3\n    partition 2, leader 1, replicas: 1,2, isrs: 1,2\n  topic \"rdkafkatest1_auto_49f744a4327b1b1e\" with 2 partitions:\n    partition 0, leader 3, replicas: 3, isrs: 3\n    partition 1, leader 1, replicas: 1, isrs: 1\n  topic \"rdkafkatest1_auto_e02f58f2c581cba\" with 2 partitions:\n    partition 0, leader 3, replicas: 3, isrs: 3\n    partition 1, leader 1, replicas: 1, isrs: 1\n  ....\n```\n\n\nJSON metadata listing\n\n    $ kcat -b mybroker -L -J\n\n\nPretty-printed JSON metadata listing\n\n    $ kcat -b mybroker -L -J | jq .\n\n\nQuery offset(s) by timestamp(s)\n\n    $ kcat -b mybroker -Q -t mytopic:3:2389238523 -t mytopic2:0:18921841\n\n\nConsume messages between two timestamps\n\n    $ kcat -b mybroker -C -t mytopic -o s@1568276612443 -o e@1568276617901\n\n\n\n## Running in Docker\n\nThe latest kcat docker image is `edenhill/kcat:1.7.1`, there's\nalso [Confluent's kafkacat docker images on Docker Hub](https://hub.docker.com/r/confluentinc/cp-kafkacat/).\n\nIf you are connecting to Kafka brokers also running on Docker you should specify the network name as part of the `docker run` command using the `--network` parameter. For more details of networking with Kafka and Docker [see this post](https://rmoff.net/2018/08/02/kafka-listeners-explained/).\n\nHere are two short examples of using kcat from Docker. See the [Docker Hub listing](https://hub.docker.com/r/confluentinc/cp-kafkacat/) and [kafkacat docs](https://docs.confluent.io/current/app-development/kafkacat-usage.html) for more details:\n\n**Send messages using [here doc](http://tldp.org/LDP/abs/html/here-docs.html):**\n\n```\ndocker run -it --rm \\\n        edenhill/kcat \\\n                -b kafka-broker:9092 \\\n                -t test \\\n                -K: \\\n                -P <<EOF\n\n1:{\"order_id\":1,\"order_ts\":1534772501276,\"total_amount\":10.50,\"customer_name\":\"Bob Smith\"}\n2:{\"order_id\":2,\"order_ts\":1534772605276,\"total_amount\":3.32,\"customer_name\":\"Sarah Black\"}\n3:{\"order_id\":3,\"order_ts\":1534772742276,\"total_amount\":21.00,\"customer_name\":\"Emma Turner\"}\nEOF\n```\n\n**Consume messages:**\n\n```\ndocker run -it --rm \\\n        edenhill/kcat \\\n           -b kafka-broker:9092 \\\n           -C \\\n           -f '\\nKey (%K bytes): %k\\t\\nValue (%S bytes): %s\\n\\Partition: %p\\tOffset: %o\\n--\\n' \\\n           -t test\n\nKey (1 bytes): 1\nValue (88 bytes): {\"order_id\":1,\"order_ts\":1534772501276,\"total_amount\":10.50,\"customer_name\":\"Bob Smith\"}\nPartition: 0    Offset: 0\n--\n\nKey (1 bytes): 2\nValue (89 bytes): {\"order_id\":2,\"order_ts\":1534772605276,\"total_amount\":3.32,\"customer_name\":\"Sarah Black\"}\nPartition: 0    Offset: 1\n--\n\nKey (1 bytes): 3\nValue (90 bytes): {\"order_id\":3,\"order_ts\":1534772742276,\"total_amount\":21.00,\"customer_name\":\"Emma Turner\"}\nPartition: 0    Offset: 2\n--\n% Reached end of topic test [0] at offset 3\n```\n\n\n## Run a mock Kafka cluster\n\nWith kcat you can spin up an ephemeral in-memory mock Kafka cluster\nthat you you can connect your Kafka applications to for quick\ntesting.\nThe mock cluster supports a reasonable subset of the Kafka\nprotocol, such as:\n\n * Producer\n * Idempotent Producer\n * Transactional Producer\n * Low-level consumer\n * High-level balanced consumer groups with offset commits\n * Topic Metadata and auto creation\n\n\nSpin the cluster by running kcat in the `-M` (for mock) mode:\n\n```bash\n\n# Create mock cluster with 3 brokers\n$ kcat -M 3\n...\nBROKERS=localhost:12345,localhost:46346,localhost:23599\n...\n```\n\nWhile kcat runs, let your Kafka applications connect to the mock cluster\nby configuring them with the `bootstrap.servers` emitted in the `BROKERS`\nline above.\n\nLet kcat run for as long as you need the cluster, then terminate it by\npressing `Ctrl-D`.\n\n\nSince the cluster runs all in memory, with no disk IO, it is quite suitable\nfor performance testing.\n"
        },
        {
          "name": "avro.c",
          "type": "blob",
          "size": 5.3876953125,
          "content": "/*\n * kcat - Apache Kafka consumer and producer\n *\n * Copyright (c) 2019, Magnus Edenhill\n * All rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n *\n * 1. Redistributions of source code must retain the above copyright notice,\n *    this list of conditions and the following disclaimer.\n * 2. Redistributions in binary form must reproduce the above copyright notice,\n *    this list of conditions and the following disclaimer in the documentation\n *    and/or other materials provided with the distribution.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE\n * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n * POSSIBILITY OF SUCH DAMAGE.\n */\n\n#include \"kcat.h\"\n#include <libserdes/serdes-avro.h>\n\nstatic serdes_t *serdes;\nstatic serdes_schema_t *key_schema, *value_schema;\n\n\n/**\n * @brief Load schema from file.\n *\n * @param name_and_path is in the format: \"schema_name:path/to/schema.avsc\"\n *\n * @returns the schema object on success, will KC_FATAL on error.\n */\nstatic serdes_schema_t *load_schema (const char *name, const char *path) {\n        serdes_schema_t *schema;\n        FILE *fp;\n        char *buf;\n        long size;\n        char errstr[256];\n\n        if (!(fp = fopen(path, \"r\")))\n                KC_FATAL(\"Failed to open schema file %s: %s\",\n                         path, strerror(errno));\n\n        fseek(fp, 0, SEEK_END);\n        size = ftell(fp);\n        fseek(fp, 0, SEEK_SET);\n\n        if (size <= 0)\n                KC_FATAL(\"Schema file %s is too small: %ld\", path, size);\n\n        buf = malloc(size+1);\n        if (!buf)\n                KC_FATAL(\"Failed to allocate buffer for %s of %ld bytes: %s\",\n                         path, size, strerror(errno));\n\n        if (fread(buf, size, 1, fp) != 1)\n                KC_FATAL(\"Failed to read schema file %s: %s\",\n                         path, strerror(errno));\n\n        fclose(fp);\n\n        buf[size] = '\\0';\n\n        schema = serdes_schema_add(serdes, name, -1, buf, (int)size,\n                                   errstr, sizeof(errstr));\n        if (!schema)\n                KC_FATAL(\"Failed parse schema file %s: %s\", path, errstr);\n\n        return schema;\n}\n\n\nvoid kc_avro_init (const char *key_schema_name,\n                   const char *key_schema_path,\n                   const char *value_schema_name,\n                   const char *value_schema_path) {\n        char errstr[512];\n\n        serdes = serdes_new(conf.srconf, errstr, sizeof(errstr));\n        if (!serdes)\n                KC_FATAL(\"Failed to create schema-registry client: %s\", errstr);\n        conf.srconf = NULL;\n\n        if (key_schema_path)\n                key_schema = load_schema(key_schema_name,\n                                         key_schema_path);\n\n        if (value_schema_path)\n                value_schema = load_schema(value_schema_name,\n                                           value_schema_path);\n\n}\n\nvoid kc_avro_term (void) {\n        if (value_schema)\n                serdes_schema_destroy(value_schema);\n        if (key_schema)\n                serdes_schema_destroy(key_schema);\n        if (serdes)\n                serdes_destroy(serdes);\n}\n\n\n/**\n * @brief Decodes the schema-id framed Avro blob in \\p data\n *        and encodes it as JSON, which is returned as a newly allocated\n *        buffer which the caller must free.\n *\n * @returns newly allocated JSON string, or NULL on error.\n */\nchar *kc_avro_to_json (const void *data, size_t data_len, int *schema_idp,\n                       char *errstr, size_t errstr_size) {\n        avro_value_t avro;\n        serdes_schema_t *schema;\n        char *json;\n        serdes_err_t err;\n\n        err = serdes_deserialize_avro(serdes, &avro, &schema, data, data_len,\n                                      errstr, errstr_size);\n        if (err) {\n                if (err == SERDES_ERR_FRAMING_INVALID ||\n                    strstr(errstr, \"Invalid CP1 magic byte\")) {\n                        static const char badframing[] =\n                                \": message not produced with \"\n                                \"Schema-Registry Avro framing\";\n                        int len = strlen(errstr);\n\n                        if (len + sizeof(badframing) < errstr_size)\n                                snprintf(errstr+len, errstr_size-len,\n                                         \"%s\", badframing);\n                }\n                return NULL;\n        }\n\n        if (avro_value_to_json(&avro, 1/*one-line*/, &json)) {\n                snprintf(errstr, errstr_size, \"Failed to encode Avro as JSON\");\n                avro_value_decref(&avro);\n                return NULL;\n        }\n\n        if (schema && schema_idp)\n                *schema_idp = serdes_schema_id(schema);\n\n        avro_value_decref(&avro);\n\n        return json;\n}\n"
        },
        {
          "name": "bootstrap.sh",
          "type": "blob",
          "size": 5.1025390625,
          "content": "#!/bin/bash\n#\n# This script provides a quick build alternative:\n# * Dependencies are downloaded and built automatically\n# * kcat is built automatically.\n# * kcat is linked statically to avoid runtime dependencies.\n#\n# While this might not be the preferred method of building kcat, it\n# is the easiest and quickest way.\n#\n\nset -o errexit -o nounset -o pipefail\n\n: \"${LIBRDKAFKA_VERSION:=v1.8.2}\"\n\nlrk_install_deps=\"--install-deps\"\nlrk_static=\"--enable-static\"\n\nfor opt in $*; do\n    case $opt in\n        --no-install-deps)\n            lrk_install_deps=\"\"\n            ;;\n\n        --no-enable-static)\n            lrk_static=\"\"\n            ;;\n\n        *)\n            echo \"Unknown option: $opt\"\n            exit 1\n            ;;\n    esac\n    shift\ndone\n\n\nfunction download {\n    local url=$1\n    local dir=$2\n\n    if [[ -d $dir ]]; then\n        echo \"Directory $dir already exists, not downloading $url\"\n        return 0\n    fi\n\n    echo \"Downloading $url to $dir\"\n    if which wget 2>&1 > /dev/null; then\n        local dl='wget -q -O-'\n    else\n        local dl='curl -s -L'\n    fi\n\n    local tar_args=\n\n    # Newer Mac tar's will try to restore metadata/attrs from\n    # certain tar files (avroc in this case), which fails for whatever reason.\n    if [[ $(uname -s) == \"Darwin\" ]] &&\n           tar --no-mac-metadata -h >/dev/null 2>&1; then\n        tar_args=\"--no-mac-metadata\"\n    fi\n\n    mkdir -p \"$dir\"\n    pushd \"$dir\" > /dev/null\n    ($dl \"$url\" | tar -xz $tar_args -f - --strip-components 1) || exit 1\n    popd > /dev/null\n}\n\n\nfunction github_download {\n    local repo=$1\n    local version=$2\n    local dir=$3\n\n    local url=https://github.com/${repo}/archive/${version}.tar.gz\n\n    download \"$url\" \"$dir\"\n}\n\nfunction build {\n    dir=$1\n    cmds=$2\n\n\n    echo \"Building $dir with commands:\"\n    echo \"$cmds\"\n    pushd $dir > /dev/null\n    set +o errexit\n    eval $cmds\n    ret=$?\n    set -o errexit\n    popd > /dev/null\n\n    if [[ $ret == 0 ]]; then\n        echo \"Build of $dir SUCCEEDED!\"\n    else\n        echo \"Build of $dir FAILED!\"\n        exit 1\n    fi\n\n    # Some projects, such as yajl, puts pkg-config files in share/ rather\n    # than lib/, copy them to the correct location.\n    cp -v $DEST/share/pkgconfig/*.pc \"$DEST/lib/pkgconfig/\" || true\n\n    return $ret\n}\n\nfunction pkg_cfg_lib {\n    pkg=$1\n\n    local libs=$(pkg-config --libs --static $pkg)\n\n    # If pkg-config isnt working try grabbing the library list manually.\n    if [[ -z \"$libs\" ]]; then\n        libs=$(grep ^Libs.private $DEST/lib/pkgconfig/${pkg}.pc | sed -e s'/^Libs.private: //g')\n    fi\n\n    # Since we specify the exact .a files to link further down below\n    # we need to remove the -l<libname> here.\n    libs=$(echo $libs | sed -e \"s/-l${pkg}//g\")\n    echo \" $libs\"\n\n    >&2 echo \"Using $libs for $pkg\"\n}\n\nmkdir -p tmp-bootstrap\npushd tmp-bootstrap > /dev/null\n\nexport DEST=\"$PWD/usr\"\nexport CFLAGS=\"-I$DEST/include\"\nif [[ $(uname -s) == Linux ]]; then\n    export LDFLAGS=\"-L$DEST/lib -Wl,-rpath-link=$DEST/lib\"\nelse\n    export LDFLAGS=\"-L$DEST/lib\"\nfi\nexport PKG_CONFIG_PATH=\"$DEST/lib/pkgconfig\"\n\ngithub_download \"edenhill/librdkafka\" \"$LIBRDKAFKA_VERSION\" \"librdkafka\"\nbuild librdkafka \"([ -f config.h ] || ./configure --prefix=$DEST $lrk_install_deps $lrk_static --disable-lz4-ext) && make -j && make install\" || (echo \"Failed to build librdkafka: bootstrap failed\" ; false)\n\ngithub_download \"edenhill/yajl\" \"edenhill\" \"libyajl\"\nbuild libyajl \"([ -d build ] || ./configure --prefix $DEST) && make install\" || (echo \"Failed to build libyajl: JSON support will probably be disabled\" ; true)\n\ndownload http://www.digip.org/jansson/releases/jansson-2.12.tar.gz libjansson\nbuild libjansson \"([[ -f config.status ]] || ./configure --enable-static --prefix=$DEST) && make && make install\" || (echo \"Failed to build libjansson: AVRO support will probably be disabled\" ; true)\n\ngithub_download \"apache/avro\" \"release-1.8.2\" \"avroc\"\nbuild avroc \"cd lang/c && mkdir -p build && cd build && cmake -DCMAKE_C_FLAGS=\\\"$CFLAGS\\\" -DCMAKE_INSTALL_PREFIX=$DEST .. && make install\" || (echo \"Failed to build Avro C: AVRO support will probably be disabled\" ; true)\n\ngithub_download \"confluentinc/libserdes\" \"master\" \"libserdes\"\nbuild libserdes \"([ -f config.h ] || ./configure  --prefix=$DEST --CFLAGS=-I${DEST}/include --LDFLAGS=-L${DEST}/lib) && make && make install\" || (echo \"Failed to build libserdes: AVRO support will probably be disabled\" ; true)\n\npopd > /dev/null\n\necho \"Building kcat\"\n./configure --clean\nexport CPPFLAGS=\"${CPPFLAGS:-} -I$DEST/include\"\nexport STATIC_LIB_avro=\"$DEST/lib/libavro.a\"\nexport STATIC_LIB_rdkafka=\"$DEST/lib/librdkafka.a\"\nexport STATIC_LIB_serdes=\"$DEST/lib/libserdes.a\"\nexport STATIC_LIB_yajl=\"$DEST/lib/libyajl_s.a\"\nexport STATIC_LIB_jansson=\"$DEST/lib/libjansson.a\"\n\n# libserdes does not have a pkg-config file to point out secondary dependencies\n# when linking statically.\nexport LIBS=\"$(pkg_cfg_lib rdkafka) $(pkg_cfg_lib yajl) $STATIC_LIB_avro $STATIC_LIB_jansson -lcurl\"\n\n# Remove tinycthread from libserdes b/c same code is also in librdkafka.\nar dv $DEST/lib/libserdes.a tinycthread.o\n\n./configure --enable-static --enable-json --enable-avro\nmake\n\necho \"\"\necho \"Success! kcat is now built\"\necho \"\"\n\n./kcat -h\n"
        },
        {
          "name": "configure",
          "type": "blob",
          "size": 4.5546875,
          "content": "#!/usr/bin/env bash\n#\n\nBASHVER=$(expr ${BASH_VERSINFO[0]} \\* 1000 + ${BASH_VERSINFO[1]})\n\nif [ \"$BASHVER\" -lt 3002 ]; then\n    echo \"ERROR: mklove requires bash version 3.2 or later but you are using $BASH_VERSION ($BASHVER)\"\n    echo \"       See https://github.com/edenhill/mklove/issues/15\"\n    exit 1\nfi\n\nMKL_CONFIGURE_ARGS=\"$0 $*\"\n\n# Load base module\nsource mklove/modules/configure.base\n\n# Read some special command line options right away that must be known prior to\n# sourcing modules.\nmkl_in_list \"$*\" \"--no-download\" && MKL_NO_DOWNLOAD=1\n# Disable downloads when --help is used to avoid blocking calls.\nmkl_in_list \"$*\" \"--help\" && MKL_NO_DOWNLOAD=1\nmkl_in_list \"$*\" \"--debug\" && MKL_DEBUG=1\n\n# This is the earliest possible time to check for color support in\n# terminal because mkl_check_terminal_color_support uses mkl_dbg which\n# needs to know if MKL_DEBUG is set\nmkl_check_terminal_color_support\n\n# Delete temporary Makefile and header files on exit.\ntrap \"{ rm -f $MKL_OUTMK $MKL_OUTH; }\" EXIT\n\n\n\n##\n## Load builtin modules\n##\n\n# Builtin options, etc.\nmkl_require builtin\n\n# Host/target support\nmkl_require host\n\n# Compiler detection\nmkl_require cc\n\n\n# Load application provided modules (in current directory), if any.\nfor fname in configure.* ; do\n    if [[ $fname = 'configure.*' ]]; then\n        continue\n    fi\n\n    # Skip temporary files\n    if [[ $fname = *~ ]]; then\n        continue\n    fi\n\n    mkl_require $fname\ndone\n\n\n\n\n##\n## Argument parsing (options)\n##\n##\n\n_SAVE_ARGS=\"$*\"\n\n# Parse arguments\nwhile [[ ! -z $@ ]]; do\n    if [[ $1 != --* ]]; then\n        mkl_err \"Unknown non-option argument: $1\"\n        mkl_usage\n        exit 1\n    fi\n\n    opt=${1#--}\n    shift\n\n    if [[ $opt = *=* ]]; then\n        name=\"${opt%%=*}\"\n        arg=\"${opt#*=}\"\n        eqarg=1\n    else\n        name=\"$opt\"\n        arg=\"\"\n        eqarg=0\n    fi\n\n    safeopt=\"$(mkl_env_esc $name)\"\n\n    if ! mkl_func_exists opt_$safeopt ; then\n        mkl_err \"Unknown option $opt\"\n        mkl_usage\n        exit 1\n    fi\n\n    # Check if this option needs an argument.\n    reqarg=$(mkl_meta_get \"MKL_OPT_ARGS\" \"$(mkl_env_esc $name)\")\n    if [[ ! -z $reqarg ]]; then\n        if [[ $eqarg == 0 && -z $arg ]]; then\n            arg=\"$1\"\n            shift\n\n            if [[ -z $arg && $reqarg != '\\*' ]]; then\n                mkl_err \"Missing argument to option --$name $reqarg\"\n                exit 1\n            fi\n        fi\n    else\n        if [[ ! -z $arg ]]; then\n            mkl_err \"Option --$name expects no argument\"\n            exit 1\n        fi\n        arg=y\n    fi\n\n    case $name in\n        re|reconfigure)\n            oldcmd=$(head -1 config.log | grep '^# configure exec: ' | \\\n                sed -e 's/^\\# configure exec: [^ ]*configure//')\n            echo \"Reconfiguring: $0 $oldcmd\"\n            exec $0 $oldcmd\n            ;;\n\n        list-modules)\n            echo \"Modules loaded:\"\n            for mod in $MKL_MODULES ; do\n                echo \"  $mod\"\n            done\n            exit 0\n            ;;\n\n        list-checks)\n            echo \"Check functions in calling order:\"\n            for mf in $MKL_CHECKS ; do\n                mod=${mf%:*}\n                func=${mf#*:}\n                echo -e \"${MKL_GREEN}From module $mod:$MKL_CLR_RESET\"\n                declare -f $func\n                echo \"\"\n            done\n            exit 0\n            ;;\n\n        update-modules)\n            fails=0\n            echo \"Updating modules\"\n            for mod in $MKL_MODULES ; do\n                echo -n \"Updating $mod...\"\n                if mkl_module_download \"$mod\" > /dev/null ; then\n                    echo -e \"${MKL_GREEN}ok${MKL_CLR_RESET}\"\n                else\n                    echo -e \"${MKL_RED}failed${MKL_CLR_RESET}\"\n                    fails=$(expr $fails + 1)\n                fi\n            done\n            exit $fails\n            ;;\n\n        help)\n            mkl_usage\n            exit 0\n            ;;\n\n        *)\n            opt_$safeopt \"$arg\" || exit 1\n            mkl_var_append MKL_OPTS_SET \"$safeopt\"\n            ;;\n    esac\ndone\n\nif [[ ! -z $MKL_CLEAN ]]; then\n    mkl_clean\n    exit 0\nfi\n\n# Move away previous log file\n[[ -f $MKL_OUTDBG ]] && mv $MKL_OUTDBG ${MKL_OUTDBG}.old\n\n\n# Create output files\necho \"# configure exec: $0 $_SAVE_ARGS\" >> $MKL_OUTDBG\necho \"# On $(date)\" >> $MKL_OUTDBG\n\nrm -f $MKL_OUTMK $MKL_OUTH\n\n\n# Load cache file\nmkl_cache_read\n\n# Run checks\nmkl_checks_run\n\n# Check accumulated failures, will not return on failure.\nmkl_check_fails\n\n# Generate outputs\nmkl_generate\n\n# Summarize what happened\nmkl_summary\n\n# Write cache file\nmkl_cache_write\n\n\necho \"\"\necho \"Now type 'make' to build\"\ntrap - EXIT\nexit 0\n"
        },
        {
          "name": "configure.self",
          "type": "blob",
          "size": 2.421875,
          "content": "#!/bin/bash\n#\n\nmkl_require good_cflags\nmkl_require gitversion as KCAT_VERSION default 1.8.0\n\n\nfunction checks {\n\n    # Check that librdkafka is available, and allow to link it statically.\n    mkl_meta_set \"rdkafka\" \"desc\" \"librdkafka is available at http://github.com/edenhill/librdkafka. To quickly download all dependencies and build kcat try ./bootstrap.sh\"\n    mkl_meta_set \"rdkafka\" \"deb\" \"librdkafka-dev\"\n    # Try static librdkafka first\n    mkl_lib_check --libname=rdkafka-static \"rdkafkastatic\" \"\" disable CC \"-lrdkafka\" \\\n                  \"#include <librdkafka/rdkafka.h>\" ||\n        mkl_lib_check \"rdkafka\" \"\" fail CC \"-lrdkafka\" \\\n                  \"#include <librdkafka/rdkafka.h>\"\n\n    # Make sure rdkafka is new enough.\n    mkl_meta_set \"librdkafkaver\" \"name\" \"librdkafka metadata API\"\n    mkl_meta_set \"librdkafkaver\" \"desc\" \"librdkafka 0.8.4 or later is required for the Metadata API\"\n    mkl_compile_check \"librdkafkaver\" \"\" fail CC \"\" \\\n\"#include <librdkafka/rdkafka.h>\nstruct rd_kafka_metadata foo;\"\n\n    # Enable KafkaConsumer support if librdkafka is new enough\n    mkl_meta_set \"librdkafka_ge_090\" \"name\" \"librdkafka KafkaConsumer support\"\n    mkl_compile_check \"librdkafka_ge_090\" ENABLE_KAFKACONSUMER disable CC \"\" \"\n    #include <librdkafka/rdkafka.h>\n    #if RD_KAFKA_VERSION >= 0x00090000\n    #else\n    #error \\\"rdkafka version < 0.9.0\\\"\n    #endif\"\n\n\n    mkl_meta_set \"yajl\" \"deb\" \"libyajl-dev\"\n    # Check for JSON library (yajl)\n    if [[ $WITH_JSON == y ]] && \\\n        mkl_lib_check \"yajl\" HAVE_YAJL disable CC \"-lyajl\" \\\n        \"#include  <yajl/yajl_version.h>\n#if YAJL_MAJOR >= 2\n#else\n#error \\\"Requires libyajl2\\\"\n#endif\n\"\n    then\n        mkl_allvar_set \"json\" ENABLE_JSON y\n    fi\n\n\n    mkl_meta_set \"avroc\" \"static\" \"libavro.a\"\n    mkl_meta_set \"libserdes\" \"deb\" \"libserdes-dev\"\n    mkl_meta_set \"libserdes\" \"static\" \"libserdes.a\"\n\n    # Check for Avro and Schema-Registry client libs\n    if [[ $WITH_AVRO == y ]] &&\n           mkl_lib_check --libname=avro-c \"avroc\" \"\" disable CC \"-lavro\" \"#include <avro.h>\" &&\n           mkl_lib_check \"serdes\" HAVE_SERDES disable CC \"-lserdes\" \\\n        \"#include <sys/types.h>\n        #include  <stdlib.h>\n        #include  <libserdes/serdes-avro.h>\"; then\n        mkl_allvar_set \"avro\" ENABLE_AVRO y\n    fi\n}\n\n\nmkl_toggle_option \"kcat\" WITH_JSON --enable-json \"JSON support (requires libyajl2)\" y\nmkl_toggle_option \"kcat\" WITH_AVRO --enable-avro \"Avro/Schema-Registry support (requires libserdes)\" y\n"
        },
        {
          "name": "debian",
          "type": "tree",
          "content": null
        },
        {
          "name": "format.c",
          "type": "blob",
          "size": 21.48046875,
          "content": "/*\n * kcat - Apache Kafka consumer and producer\n *\n * Copyright (c) 2015, Magnus Edenhill\n * All rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n *\n * 1. Redistributions of source code must retain the above copyright notice,\n *    this list of conditions and the following disclaimer.\n * 2. Redistributions in binary form must reproduce the above copyright notice,\n *    this list of conditions and the following disclaimer in the documentation\n *    and/or other materials provided with the distribution.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE\n * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n * POSSIBILITY OF SUCH DAMAGE.\n */\n\n#include \"kcat.h\"\n#include \"rdendian.h\"\n\nstatic void fmt_add (fmt_type_t type, const char *str, int len) {\n        if (conf.fmt_cnt == KC_FMT_MAX_SIZE)\n                KC_FATAL(\"Too many formatters & strings (KC_FMT_MAX_SIZE=%i)\",\n                         KC_FMT_MAX_SIZE);\n\n        conf.fmt[conf.fmt_cnt].type = type;\n\n        /* For STR types */\n        if (len) {\n                const char *s;\n                char *d;\n                conf.fmt[conf.fmt_cnt].str = d = malloc(len+1);\n                memcpy(d, str, len);\n                d[len] = '\\0';\n                s = d;\n\n                /* Convert \\.. sequences */\n                while (*s) {\n                        if (*s == '\\\\' && *(s+1)) {\n                                int base = 0;\n                                const char *next;\n                                s++;\n                                switch (*s) {\n                                case 't':\n                                        *d = '\\t';\n                                        break;\n                                case 'n':\n                                        *d = '\\n';\n                                        break;\n                                case 'r':\n                                        *d = '\\r';\n                                        break;\n                                case 'x':\n                                        s++;\n                                        base = 16;\n                                        /* FALLTHRU */\n                                default:\n                                        if (*s >= '0' && *s <= '9') {\n                                                *d = (char)strtoul(\n                                                        s, (char **)&next,\n                                                        base);\n                                                if (next > s)\n                                                        s = next - 1;\n                                        } else {\n                                                *d = *s;\n                                        }\n                                        break;\n                                }\n                        } else {\n                                *d = *s;\n                        }\n                        s++;\n                        d++;\n                }\n\n                *d = '\\0';\n\n                conf.fmt[conf.fmt_cnt].str_len =\n                        strlen(conf.fmt[conf.fmt_cnt].str);\n        }\n\n        conf.fmt_cnt++;\n}\n\n\n/**\n * Parse a format string to create a formatter list.\n */\nvoid fmt_parse (const char *fmt) {\n        const char *s = fmt, *t;\n\n        while (*s) {\n                if ((t = strchr(s, '%'))) {\n                        if (t > s)\n                                fmt_add(KC_FMT_STR, s, (int)(t-s));\n\n                        s = t+1;\n                        switch (*s)\n                        {\n                        case 'o':\n                                fmt_add(KC_FMT_OFFSET, NULL, 0);\n                                break;\n                        case 'k':\n                                fmt_add(KC_FMT_KEY, NULL, 0);\n                                break;\n                        case 'K':\n                                fmt_add(KC_FMT_KEY_LEN, NULL, 0);\n                                break;\n                        case 's':\n                                fmt_add(KC_FMT_PAYLOAD, NULL, 0);\n                                break;\n                        case 'S':\n                                fmt_add(KC_FMT_PAYLOAD_LEN, NULL, 0);\n                                break;\n                        case 'R':\n                                fmt_add(KC_FMT_PAYLOAD_LEN_BINARY, NULL, 0);\n                                break;\n                        case 't':\n                                fmt_add(KC_FMT_TOPIC, NULL, 0);\n                                break;\n                        case 'p':\n                                fmt_add(KC_FMT_PARTITION, NULL, 0);\n                                break;\n                        case 'T':\n                                fmt_add(KC_FMT_TIMESTAMP, NULL, 0);\n                                conf.flags |= CONF_F_APIVERREQ;\n                                break;\n#if HAVE_HEADERS\n                        case 'h':\n                                fmt_add(KC_FMT_HEADERS, NULL, 0);\n                                conf.flags |= CONF_F_APIVERREQ;\n                                break;\n#endif\n                        case '%':\n                                fmt_add(KC_FMT_STR, s, 1);\n                                break;\n                        case '\\0':\n                                KC_FATAL(\"Empty formatter\");\n                                break;\n                        default:\n                                KC_FATAL(\"Unsupported formatter: %%%c\", *s);\n                                break;\n                        }\n                        s++;\n                } else {\n                        fmt_add(KC_FMT_STR, s, strlen(s));\n                        break;\n                }\n\n        }\n}\n\n\n\n\nvoid fmt_init (void) {\n#if ENABLE_JSON\n        if (conf.flags & CONF_F_FMT_JSON)\n                fmt_init_json();\n#endif\n}\n\nvoid fmt_term (void) {\n#if ENABLE_JSON\n        if (conf.flags & CONF_F_FMT_JSON)\n                fmt_term_json();\n#endif\n}\n\n\n#if HAVE_HEADERS\nstatic int print_headers (FILE *fp, const rd_kafka_headers_t *hdrs) {\n        size_t idx = 0;\n        const char *name;\n        const void *value;\n        size_t size;\n\n        while (!rd_kafka_header_get_all(hdrs, idx++, &name, &value, &size)) {\n                fprintf(fp, \"%s%s=\", idx > 1 ? \",\" : \"\", name);\n                if (value && size > 0)\n                        fprintf(fp, \"%.*s\", (int)size, (const char *)value);\n                else if (!value)\n                        fprintf(fp, \"NULL\");\n        }\n        return 1;\n}\n#endif\n\n\n/**\n * @brief Check that the pack-format string is valid.\n */\nvoid pack_check (const char *what, const char *fmt) {\n        const char *f = fmt;\n        static const char *valid = \" <>bBhHiIqQcs$\";\n\n        if (!*fmt)\n                KC_FATAL(\"%s pack-format must not be empty\", what);\n\n        while (*f) {\n                if (!strchr(valid, (int)(*f)))\n                        KC_FATAL(\"Invalid token '%c' in %s pack-format, \"\n                                 \"see -s usage.\",\n                                 (int)(*f), what);\n                f++;\n        }\n}\n\n\n/**\n * @brief Unpack (deserialize) the data at \\p buf using the\n *        pack-format string \\p fmt.\n *        \\p fmt must be a valid pack-format string.\n *        Prints result to \\p fp.\n *\n * Format is inspired by Python's struct.unpack()\n *\n * @returns 0 on success or -1 on error.\n */\nstatic int unpack (FILE *fp, const char *what, const char *fmt,\n                   const char *buf, size_t len,\n                   char *errstr, size_t errstr_size) {\n        const char *b = buf;\n        const char *end = buf+len;\n        const char *f = fmt;\n        enum {\n                big_endian,\n                little_endian\n        } endian = big_endian;\n\n#define endian_swap(val,to_little,to_big)                       \\\n        (endian == big_endian ? to_big(val) : to_little(val))\n\n#define fup_copy(dst,sz) do {                                           \\\n                if ((sz) > remaining) {                                 \\\n                        snprintf(errstr, errstr_size,                   \\\n                                 \"%s truncated, expected %d bytes \"     \\\n                                 \"to unpack %c but only %d bytes remaining\", \\\n                                 what, (int)(sz), (int)(*f),            \\\n                                 (int)remaining);                       \\\n                        return -1;                                      \\\n                }                                                       \\\n                memcpy(dst, b, sz);                                     \\\n                b += (sz);                                              \\\n        } while (0)\n\n        while (*f) {\n                size_t remaining = (int)(end - b);\n\n                switch ((int)*f)\n                {\n                case ' ':\n                        fprintf(fp, \" \");\n                        break;\n                case '<':\n                        endian = little_endian;\n                        break;\n                case '>':\n                        endian = big_endian;\n                        break;\n                case 'b':\n                {\n                        int8_t v;\n                        fup_copy(&v, sizeof(v));\n                        fprintf(fp, \"%d\", (int)v);\n                }\n                break;\n                case 'B':\n                {\n                        uint8_t v;\n                        fup_copy(&v, sizeof(v));\n                        fprintf(fp, \"%u\", (unsigned int)v);\n                }\n                break;\n                case 'h':\n                {\n                        int16_t v;\n                        fup_copy(&v, sizeof(v));\n                        v = endian_swap(v, be16toh, be16toh);\n                        fprintf(fp, \"%hd\", v);\n                }\n                break;\n                case 'H':\n                {\n                        uint16_t v;\n                        fup_copy(&v, sizeof(v));\n                        v = endian_swap(v, be16toh, be16toh);\n                        fprintf(fp, \"%hu\", v);\n                }\n                break;\n                case 'i':\n                {\n                        int32_t v;\n                        fup_copy(&v, sizeof(v));\n                        v = endian_swap(v, be32toh, htobe32);\n                        fprintf(fp, \"%d\", v);\n                }\n                break;\n                case 'I':\n                {\n                        uint32_t v;\n                        fup_copy(&v, sizeof(v));\n                        v = endian_swap(v, be32toh, htobe32);\n                        fprintf(fp, \"%u\", v);\n                }\n                break;\n                case 'q':\n                {\n                        int64_t v;\n                        fup_copy(&v, sizeof(v));\n                        v = endian_swap(v, be64toh, htobe64);\n                        fprintf(fp, \"%\"PRId64, v);\n                }\n                break;\n                case 'Q':\n                {\n                        uint64_t v;\n                        fup_copy(&v, sizeof(v));\n                        v = endian_swap(v, be64toh, htobe64);\n                        fprintf(fp, \"%\"PRIu64, v);\n                }\n                break;\n                case 'c':\n                        fprintf(fp, \"%c\", (int)*b);\n                        b++;\n                        break;\n                case 's':\n                {\n                        fprintf(fp, \"%.*s\", (int)remaining, b);\n                        b += remaining;\n                }\n                break;\n                case '$':\n                {\n                        if (remaining > 0) {\n                                snprintf(errstr, errstr_size,\n                                         \"expected %s end-of-input, \"\n                                         \"but %d bytes remaining\",\n                                         what, (int)remaining);\n                                return -1;\n                        }\n                }\n                break;\n\n                default:\n                        KC_FATAL(\"Invalid pack-format token '%c'\", (int)*f);\n                        break;\n\n                }\n\n                f++;\n        }\n\n        /* Ignore remaining input bytes, if any. */\n        return 0;\n\n#undef endian_swap\n#undef fup_copy\n}\n\n\n\n\n/**\n * Delimited output\n */\nstatic void fmt_msg_output_str (FILE *fp,\n                                const rd_kafka_message_t *rkmessage) {\n        int i;\n        char errstr[256];\n\n        *errstr = '\\0';\n\n        for (i = 0 ; i < conf.fmt_cnt ; i++) {\n                int r = 1;\n                uint32_t belen;\n                const char *what_failed = \"\";\n\n                switch (conf.fmt[i].type)\n                {\n                case KC_FMT_OFFSET:\n                        r = fprintf(fp, \"%\"PRId64, rkmessage->offset);\n                        break;\n\n                case KC_FMT_KEY:\n                        if (rkmessage->key_len) {\n                                if (conf.flags & CONF_F_FMT_AVRO_KEY) {\n#if ENABLE_AVRO\n                                        char *json = kc_avro_to_json(\n                                                rkmessage->key,\n                                                rkmessage->key_len,\n                                                NULL,\n                                                errstr, sizeof(errstr));\n\n                                        if (!json) {\n                                                what_failed =\n                                                        \"Avro/Schema-registry \"\n                                                        \"key deserialization\";\n                                                goto fail;\n                                        }\n\n                                        r = fprintf(fp, \"%s\", json);\n                                        free(json);\n#else\n                                        KC_FATAL(\"NOTREACHED\");\n#endif\n                                } else if (conf.pack[KC_MSG_FIELD_KEY]) {\n                                        if (unpack(fp,\n                                                   \"key\",\n                                                   conf.pack[KC_MSG_FIELD_KEY],\n                                                   rkmessage->key,\n                                                   rkmessage->key_len,\n                                                   errstr, sizeof(errstr)) ==\n                                            -1)\n                                                goto fail;\n                                } else\n                                        r = fwrite(rkmessage->key,\n                                                   rkmessage->key_len, 1, fp);\n\n                        } else if (conf.flags & CONF_F_NULL)\n                                r = fwrite(conf.null_str,\n                                           conf.null_str_len, 1, fp);\n\n                        break;\n\n                case KC_FMT_KEY_LEN:\n                        r = fprintf(fp, \"%zd\",\n                                    /* Use -1 to indicate NULL keys */\n                                    rkmessage->key ? (ssize_t)rkmessage->key_len : -1);\n                        break;\n\n                case KC_FMT_PAYLOAD:\n                        if (rkmessage->len) {\n                                if (conf.flags & CONF_F_FMT_AVRO_VALUE) {\n#if ENABLE_AVRO\n                                        char *json = kc_avro_to_json(\n                                                rkmessage->payload,\n                                                rkmessage->len,\n                                                NULL,\n                                                errstr, sizeof(errstr));\n\n                                        if (!json) {\n                                                what_failed =\n                                                        \"Avro/Schema-registry \"\n                                                        \"message \"\n                                                        \"deserialization\";\n                                                goto fail;\n                                        }\n\n                                        r = fprintf(fp, \"%s\", json);\n                                        free(json);\n#else\n                                        KC_FATAL(\"NOTREACHED\");\n#endif\n                                } else if (conf.pack[KC_MSG_FIELD_VALUE]) {\n                                        if (unpack(fp,\n                                                   \"value\",\n                                                   conf.pack[KC_MSG_FIELD_VALUE],\n                                                   rkmessage->payload,\n                                                   rkmessage->len,\n                                                   errstr, sizeof(errstr)) ==\n                                            -1)\n                                                goto fail;\n                                } else\n                                        r = fwrite(rkmessage->payload,\n                                                   rkmessage->len, 1, fp);\n\n                        } else if (conf.flags & CONF_F_NULL)\n                                r = fwrite(conf.null_str,\n                                           conf.null_str_len, 1, fp);\n                        break;\n\n                case KC_FMT_PAYLOAD_LEN:\n                        r = fprintf(fp, \"%zd\",\n                                    /* Use -1 to indicate NULL messages */\n                                    rkmessage->payload ? (ssize_t)rkmessage->len : -1);\n                        break;\n\n                case KC_FMT_PAYLOAD_LEN_BINARY:\n                        /* Use -1 to indicate NULL messages */\n                        belen = htobe32((uint32_t)(rkmessage->payload ?\n                                                   (ssize_t)rkmessage->len :\n                                                   -1));\n                        r = fwrite(&belen, sizeof(uint32_t), 1, fp);\n                        break;\n\n                case KC_FMT_STR:\n                        r = fwrite(conf.fmt[i].str, conf.fmt[i].str_len, 1, fp);\n                        break;\n\n                case KC_FMT_TOPIC:\n                        r = fprintf(fp, \"%s\",\n                                    rd_kafka_topic_name(rkmessage->rkt));\n                        break;\n\n                case KC_FMT_PARTITION:\n                        r = fprintf(fp, \"%\"PRId32, rkmessage->partition);\n                        break;\n\n                case KC_FMT_TIMESTAMP:\n                {\n#if RD_KAFKA_VERSION >= 0x000902ff\n                        rd_kafka_timestamp_type_t tstype;\n                        r = fprintf(fp, \"%\"PRId64,\n                                    rd_kafka_message_timestamp(rkmessage,\n                                                               &tstype));\n#else\n                        r = fprintf(fp, \"-1\");\n#endif\n                        break;\n                }\n\n                case KC_FMT_HEADERS:\n                {\n#if HAVE_HEADERS\n                        rd_kafka_headers_t *hdrs;\n                        rd_kafka_resp_err_t err;\n\n                        err = rd_kafka_message_headers(rkmessage, &hdrs);\n                        if (err == RD_KAFKA_RESP_ERR__NOENT) {\n                                r = 1; /* Fake it to continue */\n                        } else if (err != RD_KAFKA_RESP_ERR_NO_ERROR) {\n                                what_failed = \"Failed to parse headers\";\n                                snprintf(errstr, sizeof(errstr), \"%s\",\n                                         rd_kafka_err2str(err));\n                                goto fail;\n                        } else {\n                                r = print_headers(fp, hdrs);\n                        }\n#endif\n                        break;\n                }\n                }\n\n\n                if (r < 1)\n                        KC_FATAL(\"Write error for message \"\n                                 \"of %zd bytes in %s [%\"PRId32\"] \"\n                                 \"at offset %\"PRId64\": %s\",\n                                 rkmessage->len,\n                                 rd_kafka_topic_name(rkmessage->rkt),\n                                 rkmessage->partition,\n                                 rkmessage->offset,\n                                 strerror(errno));\n\n                continue;\n\n        fail:\n                KC_ERROR(\"Failed to format message in %s [%\"PRId32\"] \"\n                         \"at offset %\"PRId64\": %s%s%s\",\n                         rd_kafka_topic_name(rkmessage->rkt),\n                         rkmessage->partition,\n                         rkmessage->offset,\n                         what_failed, *what_failed ? \": \" : \"\",\n                         errstr);\n                return;\n        }\n\n}\n\n\n/**\n * Format and output a received message.\n */\nvoid fmt_msg_output (FILE *fp, const rd_kafka_message_t *rkmessage) {\n\n#if ENABLE_JSON\n        if (conf.flags & CONF_F_FMT_JSON)\n                fmt_msg_output_json(fp, rkmessage);\n        else\n#endif\n                fmt_msg_output_str(fp, rkmessage);\n\n}\n"
        },
        {
          "name": "input.c",
          "type": "blob",
          "size": 11.2958984375,
          "content": "/*\n * kcat - Apache Kafka consumer and producer\n *\n * Copyright (c) 2020-2021, Magnus Edenhill\n * All rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n *\n * 1. Redistributions of source code must retain the above copyright notice,\n *    this list of conditions and the following disclaimer.\n * 2. Redistributions in binary form must reproduce the above copyright notice,\n *    this list of conditions and the following disclaimer in the documentation\n *    and/or other materials provided with the distribution.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE\n * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n * POSSIBILITY OF SUCH DAMAGE.\n */\n\n#include \"kcat.h\"\n#include \"input.h\"\n\n#include <sys/types.h>\n#include <stdlib.h>\n#ifndef _MSC_VER\n#include <unistd.h>\n#include <sys/mman.h>\n#endif\n\n#include <assert.h>\n\nvoid buf_destroy (struct buf *b) {\n#ifdef MREMAP_MAYMOVE\n        munmap(b->buf, b->size);\n#else\n        free(b->buf);\n#endif\n\n        free(b);\n}\n\nstatic struct buf *buf_new (void *buf, size_t size) {\n        struct buf *b;\n        b = malloc(sizeof(*b));\n\n        b->buf = buf;\n        b->size = size;\n\n        return b;\n}\n\n\n\n\nvoid inbuf_destroy (struct inbuf *inbuf) {\n\n        if (inbuf->buf) {\n#ifdef MREMAP_MAYMOVE\n                munmap(inbuf->buf, inbuf->size);\n#else\n                free(inbuf->buf);\n#endif\n        }\n}\n\n\n/**\n * @returns the initial allocation size.\n */\nstatic size_t inbuf_get_alloc_size (const struct inbuf *inbuf,\n                                    size_t min_size) {\n        const size_t max_size =\n#ifdef MREMAP_MAYMOVE\n                   4096\n#else\n                   1024\n#endif\n                ;\n\n        if (inbuf->max_size < min_size)\n                KC_FATAL(\"Invalid allocation size: %\"PRIu64,\n                         (uint64_t)min_size);\n\n        return MAX(min_size, max_size);\n}\n\n\nvoid inbuf_free_buf (void *buf, size_t size) {\n#ifdef MREMAP_MAYMOVE\n        munmap(buf, size);\n#else\n        free(buf);\n#endif\n}\n\n\n/**\n * @brief Allocate buffer memory. This memory MUST be freed with\n *        inbuf_free_buf().\n */\nstatic char *inbuf_alloc_buf (size_t size) {\n        void *p;\n\n#ifdef MREMAP_MAYMOVE\n        p = mmap(NULL, size, PROT_READ|PROT_WRITE, MAP_ANONYMOUS, -1, 0);\n        if (!p)\n                KC_FATAL(\"Failed to allocate(map) input buffer of %\"\n                         PRIu64\" bytes: %s\",\n                         size, strerror(errno));\n#else\n        p = malloc(size);\n        if (!p)\n                KC_FATAL(\"Failed to allocate(malloc) input buffer of %\"\n                         PRIu64\" bytes: %s\",\n                         size, strerror(errno));\n#endif\n\n        return (char *)p;\n}\n\n\n/**\n * @brief Grows the input buffer by at least \\p min_size.\n */\nstatic void inbuf_grow (struct inbuf *inbuf, size_t min_size) {\n        size_t new_size;\n        size_t req_size = inbuf->size + min_size;\n        void *p;\n#ifdef MREMAP_MAYMOVE\n        float multiplier = 1.75;\n#else\n        float multiplier = 7.9;  /* Keep down the number of reallocs */\n#endif\n\n        new_size = MIN(inbuf->max_size, (size_t)((float)req_size * multiplier));\n\n        if (new_size < req_size)\n                KC_FATAL(\"Input is too large, maximum size is %\"PRIu64,\n                         inbuf->max_size);\n\n#ifdef MREMAP_MAYMOVE\n        p = mremap((void *)inbuf->buf, inbuf->size, new_size, MREMAP_MAYMOVE);\n        if (p == MAP_FAILED)\n                KC_FATAL(\"Failed to allocate(mremap) input buffer of size \"\n                         \"%\"PRIu64\" bytes: %s\",\n                         (uint64_t)new_size, strerror(errno));\n\n#else\n        p = realloc(inbuf->buf, new_size);\n        if (!p)\n                KC_FATAL(\"Failed to allocate(realloc) input buffer of size \"\n                         \"%\"PRIu64\" bytes: %s\",\n                         (uint64_t)new_size, strerror(errno));\n#endif\n\n        inbuf->buf = (char *)p;\n        inbuf->size = new_size;\n}\n\n\nvoid inbuf_init (struct inbuf *inbuf, size_t max_size,\n                 const char *delim, size_t delim_size) {\n        memset(inbuf, 0, sizeof(*inbuf));\n\n        inbuf->max_size = max_size + delim_size;\n\n        inbuf->delim = delim;\n        inbuf->dsize = delim_size;\n        inbuf->sof = inbuf->dsize - 1;\n\n        inbuf_grow(inbuf, inbuf_get_alloc_size(inbuf, 0));\n}\n\n\n\n\n/**\n * @brief Ensure there's at least \\p min_size bytes remaining.\n */\nstatic size_t inbuf_ensure (struct inbuf *inbuf, size_t min_size) {\n        size_t remaining = inbuf->size - inbuf->len;\n\n        if (remaining > min_size)\n                return remaining;\n\n        inbuf_grow(inbuf, min_size);\n\n        return inbuf->size - inbuf->len;\n}\n\n/**\n * @brief Scan input buffer for delimiter and sets the delimiter offset\n *        in \\p dofp. Updates the scan position.\n *\n * @returns 1 if delimiter is found, else 0.\n */\nstatic int inbuf_scan (struct inbuf *inbuf, size_t *dofp) {\n        const char *dstart = inbuf->delim;\n        const char *dend = inbuf->delim + inbuf->dsize - 1;\n        const char *t;\n\n        if (inbuf->len < inbuf->sof)\n                return 0; /* Input buffer smaller than delimiter. */\n\n        /* Use Boyer-Moore inspired scan by matching the delimiter from\n         * right-to-left. */\n        while (inbuf->sof < inbuf->len &&\n               (t = (const char *)memchr((void *)(inbuf->buf + inbuf->sof),\n                                         (int)*dend,\n                                         inbuf->len - inbuf->sof))) {\n                /* Found delimiter's last byte and we know that there's at\n                 * least the delimiter's size of data to left, so start\n                 * scanning backwards. */\n                const char *p = t;    /* buffer pointer */\n                const char *d = dend; /* delimiter pointer */\n\n                do {\n                        d--;\n                        p--;\n                } while (d >= dstart && *d == *p);\n\n                if (d < dstart) {\n                        /* Found a full delimiter. */\n                        *dofp = (size_t)(p+1 - inbuf->buf);\n                        return 1;\n                }\n\n                /* Incomplete delimiter, scan forward. */\n                inbuf->sof = (size_t)(t - inbuf->buf) + 1;\n        }\n\n        return 0;\n}\n\n/*\nKey1;KeyDel;Value1:MyDilemma:;KeyDel;Value2:MyDilemma:Key3;KeyDel;\na:b:c\nsof=1\nsz=1\ndof=sof-sz\n*/\n\n/**\n * @brief Split input buffer at delimiter offset \\d dof and return it in\n *        \\p outp and \\p out_sizep, and copy the remaining bytes, if any,\n *        to a new buffer in \\p inbuf.\n */\nstatic void inbuf_split (struct inbuf *inbuf, size_t dof,\n                         char **outp, size_t *out_sizep) {\n        size_t nof = dof + inbuf->dsize;\n        size_t remaining = inbuf->len - nof;\n        void *rbuf = NULL;\n        size_t rsize = 0;\n\n        /* Copy right side of buffer (past the delimiter) to a temporary\n         * buffer that will be stored on inbuf when the left side is extracted.\n         */\n        rsize = inbuf_get_alloc_size(inbuf, remaining);\n        rbuf = inbuf_alloc_buf(rsize);\n        if (remaining > 0)\n                memcpy(rbuf, inbuf->buf+nof, remaining);\n\n        /* Shrink the returned buffer to the actual size of the left side. */\n        if (remaining + inbuf->dsize > 4096/2) {\n#ifdef MREMAP_MAYMOVE\n                *outp = mremap((void *)inbuf->buf, inbuf->size, dof,\n                               MREMAP_MAYMOVE);\n                if (!*outp)\n                        KC_FATAL(\"Failed to shrink(mremap) buffer to %\"\n                                 PRIu64\" bytes: %s\",\n                                 (uint64_t)dof, strerror(errno));\n#else\n                *outp = realloc((void *)inbuf->buf, MAX(dof, 16));\n                if (!*outp)\n                        KC_FATAL(\"Failed to shrink(REALLOC) buffer to %\"\n                                 PRIu64\" bytes: %s\",\n                                 (uint64_t)dof, strerror(errno));\n#endif\n        } else {\n                *outp = inbuf->buf;\n        }\n\n\n        *out_sizep = dof;\n\n\n        /* Set up a new (or the remaining) input buffer. */\n        inbuf->buf = rbuf;\n        inbuf->size = rsize;\n        inbuf->len = remaining;\n        inbuf->sof = inbuf->dsize - 1;\n}\n\n\n\n/**\n * @brief Read up to delimiter and then return accumulated data in *inbuf.\n *\n * Call with *inbuf as NULL.\n *\n * @returns 0 on eof/error, else 1 inbuf is valid.\n */\nint inbuf_read_to_delimeter (struct inbuf *inbuf, FILE *fp,\n                             struct buf **outbuf) {\n        int read_size = MIN(1024, inbuf->max_size);\n        int fd = fileno(fp);\n        fd_set readfds;\n\n        /*\n         * 1. Make sure there is enough output buffer room for read_size.\n         * 2. Read up to read_size from input stream.\n         * 3. Scan output buffer from current scan position for delimiter using\n         *    Boyer-Moore (searched right-to-left).\n         * 4. If delimiter is not found, go to 1.\n         * 5. Skip delimiter and copy remaining buffer to new buffer.\n         * 6. Return original buffer to caller.\n         */\n\n        if (!inbuf->buf)\n                return 0;  /* Previous EOF encountered, see below. */\n\n        while (conf.run) {\n                ssize_t r;\n                size_t dof;\n                int delim_found;\n\n                /* Scan for delimiter */\n                delim_found = inbuf_scan(inbuf, &dof);\n\n                if (delim_found) {\n                        char *buf;\n                        size_t size;\n\n                        /* Delimiter found, split and return. */\n                        inbuf_split(inbuf, dof, &buf, &size);\n\n                        *outbuf = buf_new(buf, size);\n                        return 1;\n                }\n\n                inbuf_ensure(inbuf, read_size);\n\n                FD_ZERO(&readfds);\n                FD_SET(fd, &readfds);\n                select(1, &readfds, NULL, NULL, NULL);\n\n                if (FD_ISSET(fd, &readfds))\n                        r = read(fd, inbuf->buf+inbuf->len, read_size);\n                else\n                        r = 0;\n\n                if (r <= 0) {\n                        if (inbuf->len == 0) {\n                                /* EOF with no accumulated data */\n                                inbuf_destroy(inbuf);\n                                return 0;\n                        } else {\n                                /* EOF but we have accumulated data, return what\n                                 * we have. */\n                                dof = inbuf->len;\n                                *outbuf = buf_new(inbuf->buf, inbuf->len);\n                                inbuf->buf = NULL;\n                                return 1;\n                        }\n                }\n\n                inbuf->len += (size_t)r;\n\n        }\n\n        return 0; /* NOTREACHED */\n}\n"
        },
        {
          "name": "input.h",
          "type": "blob",
          "size": 2.0751953125,
          "content": "/*\n * kcat - Apache Kafka consumer and producer\n *\n * Copyright (c) 2020-2021, Magnus Edenhill\n * All rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n *\n * 1. Redistributions of source code must retain the above copyright notice,\n *    this list of conditions and the following disclaimer.\n * 2. Redistributions in binary form must reproduce the above copyright notice,\n *    this list of conditions and the following disclaimer in the documentation\n *    and/or other materials provided with the distribution.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE\n * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n * POSSIBILITY OF SUCH DAMAGE.\n */\n\n#ifndef _INPUT_H_\n#define _INPUT_H_\n\n\nstruct buf {\n        void *buf;\n        size_t size;\n};\n\n\nstruct inbuf {\n        const char *delim;\n        size_t dsize;\n\n        size_t sof;  /**< Scan-offset */\n\n        char *buf;\n        size_t size;  /**< Allocated size of buf */\n        size_t len;   /**< How much of buf is used */\n\n        size_t max_size;  /**< Including dsize */\n};\n\n\nvoid buf_destroy (struct buf *buf);\n\nvoid inbuf_free_buf (void *buf, size_t size);\nvoid inbuf_init (struct inbuf *inbuf, size_t max_size,\n                 const char *delim, size_t delim_size);\nint inbuf_read_to_delimeter (struct inbuf *inbuf, FILE *fp,\n                             struct buf **outbuf);\n\n#endif\n"
        },
        {
          "name": "json.c",
          "type": "blob",
          "size": 14.2333984375,
          "content": "/*\n * kcat - Apache Kafka consumer and producer\n *\n * Copyright (c) 2015, Magnus Edenhill\n * All rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n *\n * 1. Redistributions of source code must retain the above copyright notice,\n *    this list of conditions and the following disclaimer.\n * 2. Redistributions in binary form must reproduce the above copyright notice,\n *    this list of conditions and the following disclaimer in the documentation\n *    and/or other materials provided with the distribution.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE\n * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n * POSSIBILITY OF SUCH DAMAGE.\n */\n\n#include \"kcat.h\"\n\n#include <yajl/yajl_gen.h>\n\n#define JS_STR(G, STR) do {                                             \\\n                const char *_s = (STR);                                 \\\n                yajl_gen_string(G, (const unsigned char *)_s, strlen(_s)); \\\n        } while (0)\n#define JS_INT(G, INT) yajl_gen_integer(g, INT)\n\nvoid fmt_msg_output_json (FILE *fp, const rd_kafka_message_t *rkmessage) {\n        yajl_gen g;\n        const char *topic = rd_kafka_topic_name(rkmessage->rkt);\n        const unsigned char *buf;\n        size_t len;\n\n        g = yajl_gen_alloc(NULL);\n\n        yajl_gen_map_open(g);\n        JS_STR(g, \"topic\");\n        JS_STR(g, topic);\n\n        JS_STR(g, \"partition\");\n        yajl_gen_integer(g, (int)rkmessage->partition);\n\n        JS_STR(g, \"offset\");\n        yajl_gen_integer(g, (long long int)rkmessage->offset);\n\n#if RD_KAFKA_VERSION >= 0x000902ff\n        {\n                rd_kafka_timestamp_type_t tstype;\n                int64_t ts = rd_kafka_message_timestamp(rkmessage, &tstype);\n                if (tstype != RD_KAFKA_TIMESTAMP_NOT_AVAILABLE) {\n                        JS_STR(g, \"tstype\");\n                        if (tstype == RD_KAFKA_TIMESTAMP_CREATE_TIME)\n                                JS_STR(g, \"create\");\n                        else if (tstype == RD_KAFKA_TIMESTAMP_LOG_APPEND_TIME)\n                                JS_STR(g, \"logappend\");\n                        else\n                                JS_STR(g, \"unknown\");\n                        JS_STR(g, \"ts\");\n                        yajl_gen_integer(g, (long long int)ts);\n                }\n        }\n#else\n        JS_STR(g, \"tstype\");\n        JS_STR(g, \"unknown\");\n        JS_STR(g, \"ts\");\n        yajl_gen_integer(g, 0);\n#endif\n\n        JS_STR(g, \"broker\");\n#if RD_KAFKA_VERSION >= 0x010500ff\n        yajl_gen_integer(g, (int)rd_kafka_message_broker_id(rkmessage));\n#else\n        yajl_gen_integer(g, -1);\n#endif\n\n\n#if HAVE_HEADERS\n        {\n                rd_kafka_headers_t *hdrs;\n\n                if (!rd_kafka_message_headers(rkmessage, &hdrs)) {\n                        size_t idx = 0;\n                        const char *name;\n                        const void *value;\n                        size_t size;\n\n                        JS_STR(g, \"headers\");\n                        yajl_gen_array_open(g);\n\n                        while (!rd_kafka_header_get_all(hdrs, idx++, &name,\n                                                        &value, &size)) {\n                                JS_STR(g, name);\n                                if (value)\n                                        yajl_gen_string(g, value, size);\n                                else\n                                        yajl_gen_null(g);\n                        }\n\n                        yajl_gen_array_close(g);\n                }\n        }\n#endif\n\n\n        JS_STR(g, \"key\");\n        if (rkmessage->key) {\n#if ENABLE_AVRO && YAJL_HAS_GEN_VERBATIM\n                if (conf.flags & CONF_F_FMT_AVRO_KEY) {\n                        char errstr[256];\n                        int schema_id = -1;\n                        char *json = kc_avro_to_json(\n                                rkmessage->key,\n                                rkmessage->key_len,\n                                &schema_id,\n                                errstr, sizeof(errstr));\n\n                        if (!json) {\n                                KC_ERROR(\"Failed to deserialize key in \"\n                                         \"message in %s [%\"PRId32\"] at \"\n                                         \"offset %\"PRId64\": %s\",\n                                         rd_kafka_topic_name(rkmessage->rkt),\n                                         rkmessage->partition,\n                                         rkmessage->offset, errstr);\n                                yajl_gen_null(g);\n                                JS_STR(g, \"key_error\");\n                                JS_STR(g, errstr);\n                        } else {\n                                yajl_gen_verbatim(g, json, strlen(json));\n                                JS_STR(g, \"key_schema_id\");\n                                JS_INT(g, schema_id);\n                        }\n                        free(json);\n                } else\n#endif\n                        yajl_gen_string(g,\n                                        (const unsigned char *)rkmessage->key,\n                                        rkmessage->key_len);\n        } else\n                yajl_gen_null(g);\n\n        JS_STR(g, \"payload\");\n        if (rkmessage->payload) {\n#if ENABLE_AVRO && YAJL_HAS_GEN_VERBATIM\n                if (conf.flags & CONF_F_FMT_AVRO_VALUE) {\n                        char errstr[256];\n                        int schema_id = -1;\n                        char *json = kc_avro_to_json(\n                                rkmessage->payload,\n                                rkmessage->len,\n                                &schema_id,\n                                errstr, sizeof(errstr));\n\n                        if (!json) {\n                                KC_ERROR(\"Failed to deserialize value in \"\n                                         \"message in %s [%\"PRId32\"] at \"\n                                         \"offset %\"PRId64\": %s\",\n                                         rd_kafka_topic_name(rkmessage->rkt),\n                                         rkmessage->partition,\n                                         rkmessage->offset, errstr);\n                                yajl_gen_null(g);\n                                JS_STR(g, \"payload_error\");\n                                JS_STR(g, errstr);\n                        } else {\n                                yajl_gen_verbatim(g, json, strlen(json));\n                                JS_STR(g, \"value_schema_id\");\n                                JS_INT(g, schema_id);\n                        }\n\n                        free(json);\n                } else\n#endif\n                        yajl_gen_string(g,\n                                        (const unsigned char *)\n                                        rkmessage->payload,\n                                        rkmessage->len);\n        } else\n                yajl_gen_null(g);\n\n        yajl_gen_map_close(g);\n\n        yajl_gen_get_buf(g, &buf, &len);\n\n        if (fwrite(buf, len, 1, fp) != 1 ||\n            (conf.fmt[0].str_len > 0 &&\n             fwrite(conf.fmt[0].str, conf.fmt[0].str_len, 1, fp) != 1))\n                KC_FATAL(\"Output write error: %s\", strerror(errno));\n\n        yajl_gen_free(g);\n}\n\n\n\n/**\n * Print metadata information\n */\nvoid metadata_print_json (const struct rd_kafka_metadata *metadata,\n                          int32_t controllerid) {\n        yajl_gen g;\n        int i, j, k;\n        const unsigned char *buf;\n        size_t len;\n\n        g = yajl_gen_alloc(NULL);\n\n        yajl_gen_map_open(g);\n\n        JS_STR(g, \"originating_broker\");\n        yajl_gen_map_open(g);\n        JS_STR(g, \"id\");\n        yajl_gen_integer(g, (long long int)metadata->orig_broker_id);\n        JS_STR(g, \"name\");\n        JS_STR(g, metadata->orig_broker_name);\n        yajl_gen_map_close(g);\n\n\n        JS_STR(g, \"query\");\n        yajl_gen_map_open(g);\n        JS_STR(g, \"topic\");\n        JS_STR(g, conf.topic ? : \"*\");\n        yajl_gen_map_close(g);\n\n        JS_STR(g, \"controllerid\");\n        yajl_gen_integer(g, (long long int)controllerid);\n\n        /* Iterate brokers */\n        JS_STR(g, \"brokers\");\n        yajl_gen_array_open(g);\n        for (i = 0 ; i < metadata->broker_cnt ; i++) {\n                int blen = strlen(metadata->brokers[i].host);\n                char *host = alloca(blen+1+5+1);\n                sprintf(host, \"%s:%i\",\n                        metadata->brokers[i].host, metadata->brokers[i].port);\n\n                yajl_gen_map_open(g);\n\n                JS_STR(g, \"id\");\n                yajl_gen_integer(g, (long long int)metadata->brokers[i].id);\n\n                JS_STR(g, \"name\");\n                JS_STR(g, host);\n\n                yajl_gen_map_close(g);\n        }\n        yajl_gen_array_close(g);\n\n        /* Iterate topics */\n        JS_STR(g, \"topics\");\n        yajl_gen_array_open(g);\n        for (i = 0 ; i < metadata->topic_cnt ; i++) {\n                const struct rd_kafka_metadata_topic *t = &metadata->topics[i];\n\n                yajl_gen_map_open(g);\n                JS_STR(g, \"topic\");\n                JS_STR(g, t->topic);\n\n                if (t->err) {\n                        JS_STR(g, \"error\");\n                        JS_STR(g, rd_kafka_err2str(t->err));\n                }\n\n                JS_STR(g, \"partitions\");\n                yajl_gen_array_open(g);\n\n                /* Iterate topic's partitions */\n                for (j = 0 ; j < t->partition_cnt ; j++) {\n                        const struct rd_kafka_metadata_partition *p;\n                        p = &t->partitions[j];\n\n                        yajl_gen_map_open(g);\n\n                        JS_STR(g, \"partition\");\n                        yajl_gen_integer(g, (long long int)p->id);\n\n                        if (p->err) {\n                                JS_STR(g, \"error\");\n                                JS_STR(g, rd_kafka_err2str(p->err));\n                        }\n\n                        JS_STR(g, \"leader\");\n                        yajl_gen_integer(g, (long long int)p->leader);\n\n                        /* Iterate partition's replicas */\n                        JS_STR(g, \"replicas\");\n                        yajl_gen_array_open(g);\n                        for (k = 0 ; k < p->replica_cnt ; k++) {\n                                yajl_gen_map_open(g);\n                                JS_STR(g, \"id\");\n                                yajl_gen_integer(g,\n                                                 (long long int)p->replicas[k]);\n                                yajl_gen_map_close(g);\n                        }\n                        yajl_gen_array_close(g);\n\n\n                        /* Iterate partition's ISRs */\n                        JS_STR(g, \"isrs\");\n                        yajl_gen_array_open(g);\n                        for (k = 0 ; k < p->isr_cnt ; k++) {\n                                yajl_gen_map_open(g);\n                                JS_STR(g, \"id\");\n                                yajl_gen_integer(g, (long long int)p->isrs[k]);\n                                yajl_gen_map_close(g);\n                        }\n                        yajl_gen_array_close(g);\n\n                        yajl_gen_map_close(g);\n\n                }\n                yajl_gen_array_close(g);\n\n                yajl_gen_map_close(g);\n        }\n        yajl_gen_array_close(g);\n\n        yajl_gen_map_close(g);\n\n        yajl_gen_get_buf(g, &buf, &len);\n\n        if (fwrite(buf, len, 1, stdout) != 1)\n                KC_FATAL(\"Output write error: %s\", strerror(errno));\n\n        yajl_gen_free(g);\n}\n\n\n/**\n * @brief Generate (if json_gen is a valid yajl_gen), or print (if json_gen is NULL)\n *        a map of topic+partitions+offsets[+errors]\n *\n * { \"<topic>\": { \"topic\": \"<topic>\",\n *                \"<partition>\": { \"partition\": <partition>, \"offset\": <o>,\n *                                  [\"error\": \"...\"]},\n *                 .. },\n *  .. }\n */\nvoid partition_list_print_json (const rd_kafka_topic_partition_list_t *parts,\n                                void *json_gen) {\n        yajl_gen g = (yajl_gen)json_gen;\n        int i;\n        const char *last_topic = \"\";\n\n        if (!g)\n                g = yajl_gen_alloc(NULL);\n\n        yajl_gen_map_open(g);\n        for (i = 0 ; i < parts->cnt ; i++) {\n                const rd_kafka_topic_partition_t *p = &parts->elems[i];\n                char partstr[16];\n\n                if (strcmp(last_topic, p->topic)) {\n                        if (*last_topic)\n                                yajl_gen_map_close(g); /* topic */\n\n                        JS_STR(g, p->topic);\n                        yajl_gen_map_open(g); /* topic */\n                        JS_STR(g, \"topic\");\n                        JS_STR(g, p->topic);\n                        last_topic = p->topic;\n                }\n\n                snprintf(partstr, sizeof(partstr), \"%\"PRId32, p->partition);\n\n                JS_STR(g, partstr);\n                yajl_gen_map_open(g);\n                JS_STR(g, \"partition\");\n                yajl_gen_integer(g, p->partition);\n                JS_STR(g, \"offset\");\n                yajl_gen_integer(g, p->offset);\n                if (p->err) {\n                        JS_STR(g, \"error\");\n                        JS_STR(g, rd_kafka_err2str(p->err));\n                }\n                yajl_gen_map_close(g);\n\n        }\n\n        if (*last_topic)\n                yajl_gen_map_close(g); /* topic */\n\n        yajl_gen_map_close(g);\n\n\n        if (!json_gen) {\n                const unsigned char *buf;\n                size_t len;\n\n                yajl_gen_get_buf(g, &buf, &len);\n                (void)fwrite(buf, len, 1, stdout);\n                yajl_gen_free(g);\n        }\n}\n\n\n\nvoid fmt_init_json (void) {\n}\n\nvoid fmt_term_json (void) {\n}\n\n\nint json_can_emit_verbatim (void) {\n#if YAJL_HAS_GEN_VERBATIM\n        return 1;\n#else\n        return 0;\n#endif\n}\n"
        },
        {
          "name": "kcat.1",
          "type": "blob",
          "size": 1.3916015625,
          "content": ".Dd $Mdocdate: December 09 2014 $\n.Dt KCAT 1\n.Os\n.Sh NAME\n.Nm kcat\n.Nd generic producer and consumer for Apache Kafka\n.Sh SYNOPSIS\n.Nm\n.Fl C | P | L\n.Fl t Ar topic\n.Op Fl p Ar partition\n.Fl b Ar brokers Op , Ar ...\n.Op Fl D Ar delim\n.Op Fl K Ar delim\n.Op Fl c Ar cnt\n.Op Fl X Ar list\n.Op Fl X Ar prop=val\n.Op Fl X Ar dump\n.Op Fl d Ar dbg Op , Ar ...\n.Op Fl q\n.Op Fl v\n.Op Fl Z\n.Op specific options\n.Nm\n.Fl C\n.Op generic options\n.Op Fl o Ar offset\n.Op Fl e\n.Op Fl O\n.Op Fl u\n.Op Fl J\n.Op Fl f Ar fmtstr\n.Nm\n.Fl P\n.Op generic options\n.Op Fl z Ar snappy | gzip\n.Op Fl p Li -1\n.Op Ar file Op ...\n.Nm\n.Fl L\n.Op generic options\n.Op Fl t Ar topic\n.Sh DESCRIPTION\n.Nm\nis a generic non-JVM producer and consumer for Apache Kafka\n0.8, think of it as a netcat for Kafka.\n.Pp\nIn producer mode (\n.Fl P\n),\n.Nm\nreads messages from stdin, delimited with a configurable\ndelimeter and produces them to the provided Kafka cluster, topic and\npartition. In consumer mode (\n.Fl C\n),\n.Nm\nreads messages from a topic and\npartition and prints them to stdout using the configured message\ndelimiter.\n.Pp\nIf neither\n.Fl P\nor\n.Fl C\nare specified\n.Nm\nattempts to figure out the mode automatically based on stdin/stdout tty types.\n.Pp\n.Nm\nalso features a metadata list mode (\n.Fl L\n), to display the current state of the Kafka cluster and its topics\nand partitions.\n.Sh SEE ALSO\nFor a more extensive help and some simple examples, run\n.Nm\nwith\n.Fl h\nflag.\n"
        },
        {
          "name": "kcat.c",
          "type": "blob",
          "size": 95.6767578125,
          "content": "/*\n * kcat - Apache Kafka consumer and producer\n *\n * Copyright (c) 2014-2021, Magnus Edenhill\n * All rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n *\n * 1. Redistributions of source code must retain the above copyright notice,\n *    this list of conditions and the following disclaimer.\n * 2. Redistributions in binary form must reproduce the above copyright notice,\n *    this list of conditions and the following disclaimer in the documentation\n *    and/or other materials provided with the distribution.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE\n * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n * POSSIBILITY OF SUCH DAMAGE.\n */\n\n#ifndef _MSC_VER\n#include <unistd.h>\n#include <syslog.h>\n#include <sys/time.h>\n#include <sys/mman.h>\n#else\n#pragma comment(lib, \"ws2_32.lib\")\n#include \"win32/wingetopt.h\"\n#include <io.h>\n#endif\n#include <stdio.h>\n#include <stdlib.h>\n#include <stdarg.h>\n#include <signal.h>\n#include <ctype.h>\n\n#include <sys/types.h>\n#include <sys/stat.h>\n#include <fcntl.h>\n\n\n\n#include \"kcat.h\"\n#include \"input.h\"\n\n#if ENABLE_MOCK\n#include <librdkafka/rdkafka_mock.h>\n#endif\n\n#if RD_KAFKA_VERSION >= 0x01040000\n#define ENABLE_TXNS 1\n#endif\n\n#if RD_KAFKA_VERSION >= 0x01060000\n#define ENABLE_INCREMENTAL_ASSIGN 1\n#endif\n\n\nstruct conf conf = {\n        .run = 1,\n        .verbosity = 1,\n        .exitonerror = 1,\n        .partition = RD_KAFKA_PARTITION_UA,\n        .msg_size = 1024*1024,\n        .null_str = \"NULL\",\n        .fixed_key = NULL,\n        .metadata_timeout = 5,\n        .offset = RD_KAFKA_OFFSET_INVALID,\n};\n\nstatic struct stats {\n        uint64_t tx;\n        uint64_t tx_err_q;\n        uint64_t tx_err_dr;\n        uint64_t tx_delivered;\n\n        uint64_t rx;\n} stats;\n\n\n/* Partition's stopped state array */\nint *part_stop = NULL;\n/* Number of partitions that are stopped */\nint part_stop_cnt = 0;\n/* Threshold level (partitions stopped) before exiting */\nint part_stop_thres = 0;\n\n\n\n/**\n * Fatal error: print error and exit\n */\nvoid RD_NORETURN fatal0 (const char *func, int line,\n                         const char *fmt, ...) {\n        va_list ap;\n        char buf[1024];\n\n        va_start(ap, fmt);\n        vsnprintf(buf, sizeof(buf), fmt, ap);\n        va_end(ap);\n\n        KC_INFO(2, \"Fatal error at %s:%i:\\n\", func, line);\n        fprintf(stderr, \"%% ERROR: %s\\n\", buf);\n        exit(1);\n}\n\n/**\n * Print error and exit if needed\n */\nvoid error0 (int exitonerror, const char *func, int line,\n             const char *fmt, ...) {\n        va_list ap;\n        char buf[1024];\n\n        va_start(ap, fmt);\n        vsnprintf(buf, sizeof(buf), fmt, ap);\n        va_end(ap);\n\n        if (exitonerror)\n                KC_INFO(2, \"Error at %s:%i:\\n\", func, line);\n\n        fprintf(stderr, \"%% ERROR: %s%s\\n\",\n                buf, exitonerror ? \": terminating\":\"\");\n\n        if (exitonerror)\n                exit(1);\n}\n\n\n\n/**\n * The delivery report callback is called once per message to\n * report delivery success or failure.\n */\nstatic void dr_msg_cb (rd_kafka_t *rk, const rd_kafka_message_t *rkmessage,\n                       void *opaque) {\n        int32_t broker_id = -1;\n        struct buf *b = rkmessage->_private;\n#if RD_KAFKA_VERSION < 0x01000000\n        static int say_once = 1;\n#endif\n\n        if (b)\n                buf_destroy(b);\n\n        if (rkmessage->err) {\n                KC_INFO(1, \"Delivery failed for message: %s\\n\",\n                        rd_kafka_err2str(rkmessage->err));\n                stats.tx_err_dr++;\n                return;\n        }\n\n#if RD_KAFKA_VERSION >= 0x010500ff\n        broker_id = rd_kafka_message_broker_id(rkmessage);\n#endif\n\n        KC_INFO(3,\n                \"Message delivered to partition %\"PRId32\" (offset %\"PRId64\") \"\n                \"on broker %\"PRId32\"\\n\",\n                rkmessage->partition, rkmessage->offset, broker_id);\n\n#if RD_KAFKA_VERSION < 0x01000000\n        if (rkmessage->offset == 0 && say_once) {\n                KC_INFO(3, \"Enable message offset reporting \"\n                        \"with '-X topic.produce.offset.report=true'\\n\");\n                say_once = 0;\n        }\n#endif\n\n        stats.tx_delivered++;\n}\n\n\n/**\n * Produces a single message, retries on queue congestion, and\n * exits hard on error.\n */\nstatic void produce (void *buf, size_t len,\n                     const void *key, size_t key_len, int msgflags,\n                     void *msg_opaque) {\n        rd_kafka_headers_t *hdrs = NULL;\n\n        /* Headers are freed on successful producev(), pass a copy. */\n        if (conf.headers)\n                hdrs = rd_kafka_headers_copy(conf.headers);\n\n        /* Produce message: keep trying until it succeeds. */\n        do {\n                rd_kafka_resp_err_t err;\n\n                if (!conf.run)\n                        KC_FATAL(\"Program terminated while \"\n                                 \"producing message of %zd bytes\", len);\n\n                err = rd_kafka_producev(\n                        conf.rk,\n                        RD_KAFKA_V_RKT(conf.rkt),\n                        RD_KAFKA_V_PARTITION(conf.partition),\n                        RD_KAFKA_V_MSGFLAGS(msgflags),\n                        RD_KAFKA_V_VALUE(buf, len),\n                        RD_KAFKA_V_KEY(key, key_len),\n                        RD_KAFKA_V_HEADERS(hdrs),\n                        RD_KAFKA_V_OPAQUE(msg_opaque),\n                        RD_KAFKA_V_END);\n\n                if (!err) {\n                        stats.tx++;\n                        break;\n                }\n\n                if (err != RD_KAFKA_RESP_ERR__QUEUE_FULL)\n                        KC_FATAL(\"Failed to produce message (%zd bytes): %s\",\n                                 len, rd_kafka_err2str(err));\n\n                stats.tx_err_q++;\n\n                /* Internal queue full, sleep to allow\n                 * messages to be produced/time out\n                 * before trying again. */\n                rd_kafka_poll(conf.rk, 5);\n        } while (1);\n\n        /* Poll for delivery reports, errors, etc. */\n        rd_kafka_poll(conf.rk, 0);\n}\n\n\n/**\n * Produce contents of file as a single message.\n * Returns the file length on success, else -1.\n */\nstatic ssize_t produce_file (const char *path) {\n        int fd;\n        void *ptr;\n        struct stat st;\n        ssize_t sz;\n        int msgflags = 0;\n\n        if ((fd = _COMPAT(open)(path, O_RDONLY)) == -1) {\n                KC_INFO(1, \"Failed to open %s: %s\\n\", path, strerror(errno));\n                return -1;\n        }\n\n        if (fstat(fd, &st) == -1) {\n                KC_INFO(1, \"Failed to stat %s: %s\\n\", path, strerror(errno));\n                _COMPAT(close)(fd);\n                return -1;\n        }\n\n        if (st.st_size == 0) {\n                KC_INFO(3, \"Skipping empty file %s\\n\", path);\n                _COMPAT(close)(fd);\n                return 0;\n        }\n\n#ifndef _MSC_VER\n        ptr = mmap(NULL, st.st_size, PROT_READ, MAP_PRIVATE, fd, 0);\n        if (ptr == MAP_FAILED) {\n                KC_INFO(1, \"Failed to mmap %s: %s\\n\", path, strerror(errno));\n                _COMPAT(close)(fd);\n                return -1;\n        }\n        sz = st.st_size;\n        msgflags = RD_KAFKA_MSG_F_COPY;\n#else\n        ptr = malloc(st.st_size);\n        if (!ptr) {\n                KC_INFO(1, \"Failed to allocate message for %s: %s\\n\",\n                        path, strerror(errno));\n                _COMPAT(close)(fd);\n                return -1;\n        }\n\n        sz = _read(fd, ptr, st.st_size);\n        if (sz < st.st_size) {\n                KC_INFO(1, \"Read failed for %s (%zd/%zd): %s\\n\",\n                        path, sz, (size_t)st.st_size, sz == -1 ? strerror(errno) :\n                        \"incomplete read\");\n                free(ptr);\n                close(fd);\n                return -1;\n        }\n        msgflags = RD_KAFKA_MSG_F_FREE;\n#endif\n\n        KC_INFO(4, \"Producing file %s (%\"PRIdMAX\" bytes)\\n\",\n                path, (intmax_t)st.st_size);\n        produce(ptr, sz, conf.fixed_key, conf.fixed_key_len, msgflags, NULL);\n\n        _COMPAT(close)(fd);\n\n        if (!(msgflags & RD_KAFKA_MSG_F_FREE)) {\n#ifndef _MSC_VER\n                munmap(ptr, st.st_size);\n#else\n                free(ptr);\n#endif\n        }\n        return sz;\n}\n\n\nstatic char *rd_strnstr (const char *haystack, size_t size,\n                         const char *needle, size_t needle_len) {\n        const char *nend = needle + needle_len - 1;\n        const char *t;\n        size_t of = needle_len - 1;\n\n        while (of < size &&\n               (t = (const char *)memchr((void *)(haystack + of),\n                                         (int)*nend,\n                                         size - of))) {\n                const char *n = nend;\n                const char *p = t;\n\n                do {\n                        n--;\n                        p--;\n                } while (n >= needle && *n == *p);\n\n                if (n < needle)\n                        return (char *)p+1;\n\n                of = (size_t)(t - haystack) + 1;\n        }\n\n        return NULL;\n}\n\n\n/**\n * Run producer, reading messages from 'fp' and producing to kafka.\n * Or if 'pathcnt' is > 0, read messages from files in 'paths' instead.\n */\nstatic void producer_run (FILE *fp, char **paths, int pathcnt) {\n        char    errstr[512];\n        char    tmp[16];\n        size_t  tsize = sizeof(tmp);\n\n        if (rd_kafka_conf_get(conf.rk_conf, \"transactional.id\",\n                              tmp, &tsize) == RD_KAFKA_CONF_OK && tsize > 1) {\n                KC_INFO(1, \"Using transactional producer\\n\");\n                conf.txn = 1;\n        }\n\n        tsize = sizeof(tmp);\n        if (rd_kafka_conf_get(conf.rk_conf, \"message.max.bytes\",\n                              tmp, &tsize) == RD_KAFKA_CONF_OK && tsize > 1) {\n                int msg_max_bytes = atoi(tmp);\n                KC_INFO(3, \"Setting producer input buffer max size to \"\n                        \"message.max.bytes value %d\\n\", msg_max_bytes);\n                conf.msg_size = msg_max_bytes;\n        }\n\n        /* Assign per-message delivery report callback. */\n        rd_kafka_conf_set_dr_msg_cb(conf.rk_conf, dr_msg_cb);\n\n        /* Create producer */\n        if (!(conf.rk = rd_kafka_new(RD_KAFKA_PRODUCER, conf.rk_conf,\n                                     errstr, sizeof(errstr))))\n                KC_FATAL(\"Failed to create producer: %s\", errstr);\n\n        if (!conf.debug && conf.verbosity == 0)\n                rd_kafka_set_log_level(conf.rk, 0);\n\n#if ENABLE_TXNS\n        if (conf.txn) {\n                rd_kafka_error_t *error;\n\n                error = rd_kafka_init_transactions(conf.rk,\n                                                   conf.metadata_timeout*1000);\n                if (error)\n                        KC_FATAL(\"init_transactions(): %s\",\n                                 rd_kafka_error_string(error));\n\n                error = rd_kafka_begin_transaction(conf.rk);\n                if (error)\n                        KC_FATAL(\"begin_transaction(): %s\",\n                                 rd_kafka_error_string(error));\n        }\n#endif\n\n        /* Create topic */\n        if (!(conf.rkt = rd_kafka_topic_new(conf.rk, conf.topic,\n                                            conf.rkt_conf)))\n                KC_FATAL(\"Failed to create topic %s: %s\", conf.topic,\n                         rd_kafka_err2str(rd_kafka_last_error()));\n\n        conf.rk_conf  = NULL;\n        conf.rkt_conf = NULL;\n\n\n        if (pathcnt > 0 && !(conf.flags & CONF_F_LINE)) {\n                int i;\n                int good = 0;\n                /* Read messages from files, each file is its own message. */\n\n                for (i = 0 ; i < pathcnt ; i++)\n                        if (produce_file(paths[i]) != -1)\n                                good++;\n\n                if (!good)\n                        conf.exitcode = 1;\n                else if (good < pathcnt)\n                        KC_INFO(1, \"Failed to produce from %i/%i files\\n\",\n                                pathcnt - good, pathcnt);\n\n        } else {\n                struct inbuf inbuf;\n                struct buf *b;\n                int at_eof = 0;\n\n                inbuf_init(&inbuf, conf.msg_size, conf.delim, conf.delim_size);\n\n                /* Read messages from input, delimited by conf.delim */\n                while (conf.run &&\n                       !(at_eof = !inbuf_read_to_delimeter(&inbuf, fp, &b))) {\n                        int msgflags = 0;\n                        char *buf = b->buf;\n                        char *key = NULL;\n                        size_t key_len = 0;\n                        size_t len = b->size;\n\n                        if (len == 0) {\n                                buf_destroy(b);\n                                continue;\n                        }\n\n                        /* Extract key, if desired and found. */\n                        if (conf.flags & CONF_F_KEY_DELIM) {\n                                char *t;\n                                if ((t = rd_strnstr(buf, len,\n                                                    conf.key_delim,\n                                                    conf.key_delim_size))) {\n                                        key_len = (size_t)(t-buf);\n                                        key     = buf;\n                                        buf     = t + conf.key_delim_size;\n                                        len    -= key_len + conf.key_delim_size;\n\n                                        if (conf.flags & CONF_F_NULL) {\n                                                if (len == 0)\n                                                        buf = NULL;\n                                                if (key_len == 0)\n                                                        key = NULL;\n                                        }\n                                }\n                        }\n\n                        if (!key && conf.fixed_key) {\n                                key = conf.fixed_key;\n                                key_len = conf.fixed_key_len;\n                        }\n\n                        if (len < 1024) {\n                                /* If message is smaller than this arbitrary\n                                 * threshold it will be more effective to\n                                 * copy the data in librdkafka. */\n                                msgflags |= RD_KAFKA_MSG_F_COPY;\n                        }\n\n                        /* Produce message */\n                        produce(buf, len, key, key_len, msgflags,\n                                (msgflags & RD_KAFKA_MSG_F_COPY) ?\n                                NULL : b);\n\n                        if (conf.flags & CONF_F_TEE &&\n                            fwrite(b->buf, b->size, 1, stdout) != 1)\n                                KC_FATAL(\"Tee write error for message \"\n                                         \"of %zd bytes: %s\",\n                                         b->size, strerror(errno));\n\n                        if (msgflags & RD_KAFKA_MSG_F_COPY) {\n                                /* librdkafka made a copy of the input. */\n                                buf_destroy(b);\n                        }\n\n                        /* Enforce -c <cnt> */\n                        if (stats.tx == (uint64_t)conf.msg_cnt)\n                                conf.run = 0;\n                }\n\n                if (conf.run && !at_eof)\n                        KC_FATAL(\"Unable to read message: %s\",\n                                 strerror(errno));\n        }\n\n#if ENABLE_TXNS\n        if (conf.txn) {\n                rd_kafka_error_t *error;\n                const char *what;\n\n                if (conf.term_sig) {\n                        KC_INFO(0,\n                                \"Aborting transaction due to \"\n                                \"termination signal\\n\");\n                        what = \"abort_transaction()\";\n                        error = rd_kafka_abort_transaction(\n                                conf.rk, conf.metadata_timeout * 1000);\n                } else {\n                        KC_INFO(1, \"Committing transaction\\n\");\n                        what = \"commit_transaction()\";\n                        error = rd_kafka_commit_transaction(\n                                conf.rk, conf.metadata_timeout * 1000);\n                        if (!error)\n                                KC_INFO(1,\n                                        \"Transaction successfully committed\\n\");\n                }\n\n                if (error)\n                        KC_FATAL(\"%s: %s\", what, rd_kafka_error_string(error));\n        }\n#endif\n\n\n        /* Wait for all messages to be transmitted */\n        conf.run = 1;\n        while (conf.run && rd_kafka_outq_len(conf.rk))\n                rd_kafka_poll(conf.rk, 50);\n\n        rd_kafka_topic_destroy(conf.rkt);\n        rd_kafka_destroy(conf.rk);\n\n        if (stats.tx_err_dr)\n                conf.exitcode = 1;\n}\n\nstatic void stop_partition (rd_kafka_message_t *rkmessage) {\n        if (!part_stop[rkmessage->partition]) {\n                /* Stop consuming this partition */\n                rd_kafka_consume_stop(rkmessage->rkt,\n                                      rkmessage->partition);\n                part_stop[rkmessage->partition] = 1;\n                part_stop_cnt++;\n                if (part_stop_cnt >= part_stop_thres)\n                        conf.run = 0;\n        }\n}\n\n\n/**\n * @brief Mark partition as not at EOF\n */\nstatic void partition_not_eof (const rd_kafka_message_t *rkmessage) {\n        rd_kafka_topic_partition_t *rktpar;\n\n        if (conf.mode != 'G' || !conf.exit_eof ||\n            !conf.assignment || conf.eof_cnt == 0)\n                return;\n\n        /* Find partition in assignment */\n        rktpar = rd_kafka_topic_partition_list_find(\n                conf.assignment,\n                rd_kafka_topic_name(rkmessage->rkt),\n                rkmessage->partition);\n\n        if (!rktpar || rktpar->err != RD_KAFKA_RESP_ERR__PARTITION_EOF)\n                return;\n\n        rktpar->err = RD_KAFKA_RESP_ERR_NO_ERROR;\n        conf.eof_cnt--;\n}\n\n\n/**\n * @brief Mark partition as at EOF\n */\nstatic void partition_at_eof (rd_kafka_message_t *rkmessage) {\n\n        if (conf.mode == 'C') {\n                /* Store EOF offset.\n                 * If partition is empty and at offset 0,\n                 * store future first message (0). */\n                rd_kafka_offset_store(rkmessage->rkt,\n                                      rkmessage->partition,\n                                      rkmessage->offset == 0 ?\n                                      0 : rkmessage->offset-1);\n                if (conf.exit_eof) {\n                        stop_partition(rkmessage);\n                }\n\n        } else if (conf.mode == 'G' && conf.exit_eof && conf.assignment) {\n                /* Find partition in assignment */\n                rd_kafka_topic_partition_t *rktpar;\n\n                rktpar = rd_kafka_topic_partition_list_find(\n                        conf.assignment,\n                        rd_kafka_topic_name(rkmessage->rkt),\n                        rkmessage->partition);\n\n                if (rktpar && rktpar->err != RD_KAFKA_RESP_ERR__PARTITION_EOF) {\n                        rktpar->err = RD_KAFKA_RESP_ERR__PARTITION_EOF;\n                        conf.eof_cnt++;\n\n                        if (conf.eof_cnt == conf.assignment->cnt)\n                                conf.run = 0;\n                }\n        }\n\n        KC_INFO(1, \"Reached end of topic %s [%\"PRId32\"] \"\n                \"at offset %\"PRId64\"%s\\n\",\n                rd_kafka_topic_name(rkmessage->rkt),\n                rkmessage->partition,\n                rkmessage->offset,\n                !conf.run ? \": exiting\" : \"\");\n}\n\n\n/**\n * Consume callback, called for each message consumed.\n */\nstatic void consume_cb (rd_kafka_message_t *rkmessage, void *opaque) {\n        FILE *fp = opaque;\n\n        if (!conf.run)\n                return;\n\n        if (rkmessage->err) {\n                if (rkmessage->err == RD_KAFKA_RESP_ERR__PARTITION_EOF) {\n                        partition_at_eof(rkmessage);\n                        return;\n                }\n\n                if (rkmessage->rkt)\n                        KC_FATAL(\"Topic %s [%\"PRId32\"] error: %s\",\n                                 rd_kafka_topic_name(rkmessage->rkt),\n                                 rkmessage->partition,\n                                 rd_kafka_message_errstr(rkmessage));\n                else\n                        KC_FATAL(\"Consumer error: %s\",\n                                 rd_kafka_message_errstr(rkmessage));\n\n        } else {\n                partition_not_eof(rkmessage);\n        }\n\n        if (conf.stopts) {\n                int64_t ts = rd_kafka_message_timestamp(rkmessage, NULL);\n                if (ts >= conf.stopts) {\n                        stop_partition(rkmessage);\n                        KC_INFO(1, \"Reached stop timestamp for topic \"\n                                \"%s [%\"PRId32\"] \"\n                                \"at offset %\"PRId64\"%s\\n\",\n                                rd_kafka_topic_name(rkmessage->rkt),\n                                rkmessage->partition,\n                                rkmessage->offset,\n                                !conf.run ? \": exiting\" : \"\");\n                        return;\n                }\n        }\n\n        /* Print message */\n        fmt_msg_output(fp, rkmessage);\n\n        if (conf.mode == 'C') {\n                rd_kafka_offset_store(rkmessage->rkt,\n                                      rkmessage->partition,\n                                      rkmessage->offset);\n        }\n\n        if (++stats.rx == (uint64_t)conf.msg_cnt) {\n                conf.run = 0;\n                rd_kafka_yield(conf.rk);\n        }\n}\n\n\n#if RD_KAFKA_VERSION >= 0x00090000\nstatic void throttle_cb (rd_kafka_t *rk, const char *broker_name,\n                         int32_t broker_id, int throttle_time_ms, void *opaque){\n        KC_INFO(1, \"Broker %s (%\"PRId32\") throttled request for %dms\\n\",\n                broker_name, broker_id, throttle_time_ms);\n}\n#endif\n\n#if ENABLE_KAFKACONSUMER\nstatic void print_partition_list (int is_assigned,\n                                  const rd_kafka_topic_partition_list_t\n                                  *partitions) {\n        int i;\n        for (i = 0 ; i < partitions->cnt ; i++) {\n                fprintf(stderr, \"%s%s [%\"PRId32\"]\",\n                        i > 0 ? \", \":\"\",\n                        partitions->elems[i].topic,\n                        partitions->elems[i].partition);\n        }\n        fprintf(stderr, \"\\n\");\n}\n\n\n#if ENABLE_INCREMENTAL_ASSIGN\nstatic void\nincremental_rebalance_cb (rd_kafka_t *rk, rd_kafka_resp_err_t err,\n                          rd_kafka_topic_partition_list_t *partitions,\n                          void *opaque) {\n        rd_kafka_error_t *error = NULL;\n        int i;\n\n        KC_INFO(1, \"Group %s rebalanced: incremental %s of %d partition(s) \"\n                \"(memberid %s%s, %s rebalance protocol): \",\n                conf.group,\n                err == RD_KAFKA_RESP_ERR__ASSIGN_PARTITIONS ?\n                \"assignment\" : \"revoke\",\n                partitions->cnt,\n                rd_kafka_memberid(rk),\n                rd_kafka_assignment_lost(rk) ? \", assignment lost\" : \"\",\n                rd_kafka_rebalance_protocol(rk));\n\n        switch (err)\n        {\n        case RD_KAFKA_RESP_ERR__ASSIGN_PARTITIONS:\n                if (conf.verbosity >= 1)\n                        print_partition_list(1, partitions);\n\n                if (!conf.assignment)\n                        conf.assignment =\n                                rd_kafka_topic_partition_list_new(\n                                        partitions->cnt);\n\n                for (i = 0 ; i < partitions->cnt ; i++) {\n                        rd_kafka_topic_partition_t *rktpar =\n                                &partitions->elems[i];\n\n                        /* Set start offset from -o .. */\n                        if (conf.offset != RD_KAFKA_OFFSET_INVALID)\n                                rktpar->offset = conf.offset;\n\n                        rktpar->offset = conf.offset;\n\n                        rd_kafka_topic_partition_list_add(conf.assignment,\n                                                          rktpar->topic,\n                                                          rktpar->partition);\n                }\n                error = rd_kafka_incremental_assign(rk, partitions);\n                break;\n\n        case RD_KAFKA_RESP_ERR__REVOKE_PARTITIONS:\n                if (conf.verbosity >= 1)\n                        print_partition_list(1, partitions);\n\n                /* Remove partitions from conf.assignment in reverse order\n                 * to minimize array shrink sizes. */\n                for (i = partitions->cnt - 1 ;\n                     conf.assignment && i >= 0 ;\n                     i--) {\n                        rd_kafka_topic_partition_t *rktpar =\n                                &partitions->elems[i];\n\n                        rd_kafka_topic_partition_list_del(conf.assignment,\n                                                          rktpar->topic,\n                                                          rktpar->partition);\n                }\n\n                error = rd_kafka_incremental_unassign(rk, partitions);\n                break;\n\n        default:\n                KC_INFO(0, \"failed: %s\\n\", rd_kafka_err2str(err));\n                break;\n        }\n\n        if (error) {\n                KC_ERROR(\"Incremental rebalance %s of %d partition(s) \"\n                         \"failed: %s\\n\",\n                         err == RD_KAFKA_RESP_ERR__ASSIGN_PARTITIONS ?\n                         \"assign\" : \"unassign\",\n                         partitions->cnt, rd_kafka_error_string(error));\n                rd_kafka_error_destroy(error);\n        }\n}\n#endif\n\nstatic void rebalance_cb (rd_kafka_t *rk, rd_kafka_resp_err_t err,\n                          rd_kafka_topic_partition_list_t *partitions,\n                          void *opaque) {\n        rd_kafka_resp_err_t ret_err = RD_KAFKA_RESP_ERR_NO_ERROR;\n\n#if ENABLE_INCREMENTAL_ASSIGN\n        if (!strcmp(rd_kafka_rebalance_protocol(rk), \"COOPERATIVE\")) {\n                incremental_rebalance_cb(rk, err, partitions, opaque);\n                return;\n        }\n#endif\n\n        KC_INFO(1, \"Group %s rebalanced (memberid %s): \",\n                conf.group, rd_kafka_memberid(rk));\n\n        switch (err)\n        {\n        case RD_KAFKA_RESP_ERR__ASSIGN_PARTITIONS:\n                if (conf.verbosity >= 1) {\n                        fprintf(stderr, \"assigned: \");\n                        print_partition_list(1, partitions);\n                }\n                if (conf.offset != RD_KAFKA_OFFSET_INVALID) {\n                        int i;\n                        for (i = 0 ; i < partitions->cnt ; i++)\n                                partitions->elems[i].offset = conf.offset;\n                }\n\n                if (conf.assignment)\n                        rd_kafka_topic_partition_list_destroy(conf.assignment);\n                conf.assignment =\n                        rd_kafka_topic_partition_list_copy(partitions);\n\n                ret_err = rd_kafka_assign(rk, partitions);\n                break;\n\n        case RD_KAFKA_RESP_ERR__REVOKE_PARTITIONS:\n                if (conf.verbosity >= 1) {\n                        fprintf(stderr, \"revoked: \");\n                        print_partition_list(1, partitions);\n                }\n\n                if (conf.assignment) {\n                        rd_kafka_topic_partition_list_destroy(conf.assignment);\n                        conf.assignment = NULL;\n                }\n\n                ret_err = rd_kafka_assign(rk, NULL);\n                break;\n\n        default:\n                KC_INFO(0, \"failed: %s\\n\", rd_kafka_err2str(err));\n                break;\n        }\n\n        if (ret_err)\n                KC_ERROR(\"Rebalance %s of %d partition(s) failed: %s\\n\",\n                         err == RD_KAFKA_RESP_ERR__ASSIGN_PARTITIONS ?\n                         \"assign\" : \"unassign\",\n                         partitions->cnt, rd_kafka_err2str(ret_err));\n}\n\n/**\n * Run high-level KafkaConsumer, write messages to 'fp'\n */\nstatic void kafkaconsumer_run (FILE *fp, char *const *topics, int topic_cnt) {\n        char    errstr[512];\n        rd_kafka_resp_err_t err;\n        rd_kafka_topic_partition_list_t *topiclist;\n        int i;\n\n        rd_kafka_conf_set_rebalance_cb(conf.rk_conf, rebalance_cb);\n        rd_kafka_conf_set_default_topic_conf(conf.rk_conf, conf.rkt_conf);\n        conf.rkt_conf = NULL;\n\n        /* Create consumer */\n        if (!(conf.rk = rd_kafka_new(RD_KAFKA_CONSUMER, conf.rk_conf,\n                                     errstr, sizeof(errstr))))\n                KC_FATAL(\"Failed to create consumer: %s\", errstr);\n        conf.rk_conf  = NULL;\n\n        /* Forward main event queue to consumer queue so we can\n         * serve both queues with a single consumer_poll() call. */\n        rd_kafka_poll_set_consumer(conf.rk);\n\n        if (conf.debug)\n                rd_kafka_set_log_level(conf.rk, LOG_DEBUG);\n        else if (conf.verbosity == 0)\n                rd_kafka_set_log_level(conf.rk, 0);\n\n        /* Build subscription set */\n        topiclist = rd_kafka_topic_partition_list_new(topic_cnt);\n        for (i = 0 ; i < topic_cnt ; i++)\n                rd_kafka_topic_partition_list_add(topiclist, topics[i], -1);\n\n        /* Subscribe */\n        if ((err = rd_kafka_subscribe(conf.rk, topiclist)))\n                KC_FATAL(\"Failed to subscribe to %d topics: %s\\n\",\n                         topiclist->cnt, rd_kafka_err2str(err));\n\n        KC_INFO(1, \"Waiting for group rebalance\\n\");\n\n        rd_kafka_topic_partition_list_destroy(topiclist);\n\n        /* Read messages from Kafka, write to 'fp'. */\n        while (conf.run) {\n                rd_kafka_message_t *rkmessage;\n\n                rkmessage = rd_kafka_consumer_poll(conf.rk, 100);\n                if (!rkmessage)\n                        continue;\n\n                consume_cb(rkmessage, fp);\n\n                rd_kafka_message_destroy(rkmessage);\n        }\n\n        if ((err = rd_kafka_consumer_close(conf.rk)))\n                KC_FATAL(\"Failed to close consumer: %s\\n\",\n                         rd_kafka_err2str(err));\n\n        /* Wait for outstanding requests to finish. */\n        conf.run = 1;\n        while (conf.run && rd_kafka_outq_len(conf.rk) > 0)\n                rd_kafka_poll(conf.rk, 50);\n\n        if (conf.assignment)\n                rd_kafka_topic_partition_list_destroy(conf.assignment);\n\n        rd_kafka_destroy(conf.rk);\n}\n#endif\n\n/**\n * Get offsets from conf.startts for consumer_run\n */\nstatic int64_t *get_offsets (rd_kafka_metadata_topic_t *topic) {\n        int i;\n        int64_t *offsets;\n        rd_kafka_resp_err_t err;\n        rd_kafka_topic_partition_list_t *rktparlistp =\n                rd_kafka_topic_partition_list_new(1);\n\n        for (i = 0 ; i < topic->partition_cnt ; i++) {\n                int32_t partition = topic->partitions[i].id;\n\n                /* If -p <part> was specified: skip unwanted partitions */\n                if (conf.partition != RD_KAFKA_PARTITION_UA &&\n                    conf.partition != partition)\n                        continue;\n\n                rd_kafka_topic_partition_list_add(\n                        rktparlistp,\n                        rd_kafka_topic_name(conf.rkt),\n                        partition)->offset = conf.startts;\n\n                if (conf.partition != RD_KAFKA_PARTITION_UA)\n                        break;\n        }\n        err = rd_kafka_offsets_for_times(conf.rk, rktparlistp,\n                                         conf.metadata_timeout * 1000);\n        if (err)\n                KC_FATAL(\"offsets_for_times failed: %s\", rd_kafka_err2str(err));\n\n        offsets = calloc(sizeof(int64_t), topic->partition_cnt);\n        for (i = 0 ; i < rktparlistp->cnt ; i++) {\n                const rd_kafka_topic_partition_t *p = &rktparlistp->elems[i];\n                offsets[p->partition] = p->offset;\n        }\n        rd_kafka_topic_partition_list_destroy(rktparlistp);\n\n        return offsets;\n}\n\n/**\n * Run consumer, consuming messages from Kafka and writing to 'fp'.\n */\nstatic void consumer_run (FILE *fp) {\n        char    errstr[512];\n        rd_kafka_resp_err_t err;\n        const rd_kafka_metadata_t *metadata;\n        int i;\n        int64_t *offsets = NULL;\n        rd_kafka_queue_t *rkqu;\n\n        /* Create consumer */\n        if (!(conf.rk = rd_kafka_new(RD_KAFKA_CONSUMER, conf.rk_conf,\n                                     errstr, sizeof(errstr))))\n                KC_FATAL(\"Failed to create consumer: %s\", errstr);\n\n        if (!conf.debug && conf.verbosity == 0)\n                rd_kafka_set_log_level(conf.rk, 0);\n\n        /* The callback-based consumer API's offset store granularity is\n         * not good enough for us, disable automatic offset store\n         * and do it explicitly per-message in the consume callback instead. */\n        if (rd_kafka_topic_conf_set(conf.rkt_conf,\n                                    \"auto.commit.enable\", \"false\",\n                                    errstr, sizeof(errstr)) != RD_KAFKA_CONF_OK)\n                KC_FATAL(\"%s\", errstr);\n\n        /* Create topic */\n        if (!(conf.rkt = rd_kafka_topic_new(conf.rk, conf.topic,\n                                            conf.rkt_conf)))\n                KC_FATAL(\"Failed to create topic %s: %s\", conf.topic,\n                         rd_kafka_err2str(rd_kafka_last_error()));\n\n        conf.rk_conf  = NULL;\n        conf.rkt_conf = NULL;\n\n\n        /* Query broker for topic + partition information. */\n        if ((err = rd_kafka_metadata(conf.rk, 0, conf.rkt, &metadata,\n                                     conf.metadata_timeout * 1000)))\n                KC_FATAL(\"Failed to query metadata for topic %s: %s\",\n                         rd_kafka_topic_name(conf.rkt), rd_kafka_err2str(err));\n\n        /* Error handling */\n        if (metadata->topic_cnt == 0)\n                KC_FATAL(\"No such topic in cluster: %s\",\n                         rd_kafka_topic_name(conf.rkt));\n\n        if ((err = metadata->topics[0].err))\n                KC_FATAL(\"Topic %s error: %s\",\n                         rd_kafka_topic_name(conf.rkt), rd_kafka_err2str(err));\n\n        if (metadata->topics[0].partition_cnt == 0)\n                KC_FATAL(\"Topic %s has no partitions\",\n                         rd_kafka_topic_name(conf.rkt));\n\n        /* If Exit-at-EOF is enabled, set up array to track EOF\n         * state for each partition. */\n        if (conf.exit_eof || conf.stopts) {\n                part_stop = calloc(sizeof(*part_stop),\n                                   metadata->topics[0].partition_cnt);\n\n                if (conf.partition != RD_KAFKA_PARTITION_UA)\n                        part_stop_thres = 1;\n                else\n                        part_stop_thres = metadata->topics[0].partition_cnt;\n        }\n\n#if RD_KAFKA_VERSION >= 0x00090300\n        if (conf.startts) {\n                offsets = get_offsets(&metadata->topics[0]);\n        }\n#endif\n\n        /* Create a shared queue that combines messages from\n         * all wanted partitions. */\n        rkqu = rd_kafka_queue_new(conf.rk);\n\n        /* Start consuming from all wanted partitions. */\n        for (i = 0 ; i < metadata->topics[0].partition_cnt ; i++) {\n                int32_t partition = metadata->topics[0].partitions[i].id;\n\n                /* If -p <part> was specified: skip unwanted partitions */\n                if (conf.partition != RD_KAFKA_PARTITION_UA &&\n                    conf.partition != partition)\n                        continue;\n\n                /* Start consumer for this partition */\n                if (rd_kafka_consume_start_queue(conf.rkt, partition,\n                                                 offsets ? offsets[i] :\n                                                 (conf.offset ==\n                                                  RD_KAFKA_OFFSET_INVALID ?\n                                                  RD_KAFKA_OFFSET_BEGINNING :\n                                                  conf.offset),\n                                                 rkqu) == -1)\n                        KC_FATAL(\"Failed to start consuming \"\n                                 \"topic %s [%\"PRId32\"]: %s\",\n                                 conf.topic, partition,\n                                 rd_kafka_err2str(rd_kafka_last_error()));\n\n                if (conf.partition != RD_KAFKA_PARTITION_UA)\n                        break;\n        }\n        free(offsets);\n\n        if (conf.partition != RD_KAFKA_PARTITION_UA &&\n            i == metadata->topics[0].partition_cnt)\n                KC_FATAL(\"Topic %s (with partitions 0..%i): \"\n                         \"partition %i does not exist\",\n                         rd_kafka_topic_name(conf.rkt),\n                         metadata->topics[0].partition_cnt-1,\n                         conf.partition);\n\n\n        /* Read messages from Kafka, write to 'fp'. */\n        while (conf.run) {\n                rd_kafka_consume_callback_queue(rkqu, 100,\n                                                consume_cb, fp);\n\n                /* Poll for errors, etc */\n                rd_kafka_poll(conf.rk, 0);\n        }\n\n        /* Stop consuming */\n        for (i = 0 ; i < metadata->topics[0].partition_cnt ; i++) {\n                int32_t partition = metadata->topics[0].partitions[i].id;\n\n                /* If -p <part> was specified: skip unwanted partitions */\n                if (conf.partition != RD_KAFKA_PARTITION_UA &&\n                    conf.partition != partition)\n                        continue;\n\n                /* Dont stop already stopped partitions */\n                if (!part_stop || !part_stop[partition])\n                        rd_kafka_consume_stop(conf.rkt, partition);\n\n                rd_kafka_consume_stop(conf.rkt, partition);\n        }\n\n        /* Destroy shared queue */\n        rd_kafka_queue_destroy(rkqu);\n\n        /* Wait for outstanding requests to finish. */\n        conf.run = 1;\n        while (conf.run && rd_kafka_outq_len(conf.rk) > 0)\n                rd_kafka_poll(conf.rk, 50);\n\n        if (conf.assignment)\n                rd_kafka_topic_partition_list_destroy(conf.assignment);\n\n        rd_kafka_metadata_destroy(metadata);\n        rd_kafka_topic_destroy(conf.rkt);\n        rd_kafka_destroy(conf.rk);\n}\n\n\n#if ENABLE_MOCK\n/**\n * @brief Run mock cluster until stdin is closed.\n */\nstatic void mock_run (void) {\n        rd_kafka_t *rk;\n        rd_kafka_mock_cluster_t *mcluster;\n        const char *bootstraps;\n        char errstr[512];\n        char buf[64];\n\n        if (!(rk = rd_kafka_new(RD_KAFKA_PRODUCER, conf.rk_conf,\n                                errstr, sizeof(errstr))))\n                KC_FATAL(\"Failed to create client instance for \"\n                         \"mock cluster: %s\", errstr);\n\n        mcluster = rd_kafka_mock_cluster_new(rk, conf.mock.broker_cnt);\n        if (!mcluster)\n                KC_FATAL(\"Failed to create mock cluster\");\n\n        bootstraps = rd_kafka_mock_cluster_bootstraps(mcluster);\n\n        KC_INFO(1, \"Mock cluster started with bootstrap.servers=%s\\n\",\n                bootstraps);\n        KC_INFO(1, \"Press Ctrl-C+Enter or Ctrl-D to terminate.\\n\");\n\n        printf(\"BROKERS=%s\\n\", bootstraps);\n\n        while (conf.run && fgets(buf, sizeof(buf), stdin)) {\n                /* nop */\n        }\n\n        KC_INFO(1, \"Terminating mock cluster\\n\");\n\n        rd_kafka_mock_cluster_destroy(mcluster);\n        rd_kafka_destroy(rk);\n}\n#endif\n\n/**\n * Print metadata information\n */\nstatic void metadata_print (const rd_kafka_metadata_t *metadata,\n                            int32_t controllerid) {\n        int i, j, k;\n\n        printf(\"Metadata for %s (from broker %\"PRId32\": %s):\\n\",\n               conf.topic ? conf.topic : \"all topics\",\n               metadata->orig_broker_id, metadata->orig_broker_name);\n\n        /* Iterate brokers */\n        printf(\" %i brokers:\\n\", metadata->broker_cnt);\n        for (i = 0 ; i < metadata->broker_cnt ; i++)\n                printf(\"  broker %\"PRId32\" at %s:%i%s\\n\",\n                       metadata->brokers[i].id,\n                       metadata->brokers[i].host,\n                       metadata->brokers[i].port,\n                       controllerid == metadata->brokers[i].id ?\n                       \" (controller)\" : \"\");\n\n        /* Iterate topics */\n        printf(\" %i topics:\\n\", metadata->topic_cnt);\n        for (i = 0 ; i < metadata->topic_cnt ; i++) {\n                const rd_kafka_metadata_topic_t *t = &metadata->topics[i];\n                printf(\"  topic \\\"%s\\\" with %i partitions:\",\n                       t->topic,\n                       t->partition_cnt);\n                if (t->err) {\n                        printf(\" %s\", rd_kafka_err2str(t->err));\n                        if (t->err == RD_KAFKA_RESP_ERR_LEADER_NOT_AVAILABLE)\n                                printf(\" (try again)\");\n                }\n                printf(\"\\n\");\n\n                /* Iterate topic's partitions */\n                for (j = 0 ; j < t->partition_cnt ; j++) {\n                        const rd_kafka_metadata_partition_t *p;\n                        p = &t->partitions[j];\n                        printf(\"    partition %\"PRId32\", \"\n                               \"leader %\"PRId32\", replicas: \",\n                               p->id, p->leader);\n\n                        /* Iterate partition's replicas */\n                        for (k = 0 ; k < p->replica_cnt ; k++)\n                                printf(\"%s%\"PRId32,\n                                       k > 0 ? \",\":\"\", p->replicas[k]);\n\n                        /* Iterate partition's ISRs */\n                        printf(\", isrs: \");\n                        for (k = 0 ; k < p->isr_cnt ; k++)\n                                printf(\"%s%\"PRId32,\n                                       k > 0 ? \",\":\"\", p->isrs[k]);\n                        if (p->err)\n                                printf(\", %s\\n\", rd_kafka_err2str(p->err));\n                        else\n                                printf(\"\\n\");\n                }\n        }\n}\n\n\n/**\n * Lists metadata\n */\nstatic void metadata_list (void) {\n        char    errstr[512];\n        rd_kafka_resp_err_t err;\n        const rd_kafka_metadata_t *metadata;\n        int32_t controllerid = -1;\n\n        /* Create handle */\n        if (!(conf.rk = rd_kafka_new(RD_KAFKA_PRODUCER, conf.rk_conf,\n                                     errstr, sizeof(errstr))))\n                KC_FATAL(\"Failed to create producer: %s\", errstr);\n\n        if (!conf.debug && conf.verbosity == 0)\n                rd_kafka_set_log_level(conf.rk, 0);\n\n        /* Create topic, if specified */\n        if (conf.topic &&\n            !(conf.rkt = rd_kafka_topic_new(conf.rk, conf.topic,\n                                            conf.rkt_conf)))\n                KC_FATAL(\"Failed to create topic %s: %s\", conf.topic,\n                         rd_kafka_err2str(rd_kafka_last_error()));\n\n        conf.rk_conf  = NULL;\n        conf.rkt_conf = NULL;\n\n\n        /* Fetch metadata */\n        err = rd_kafka_metadata(conf.rk, conf.rkt ? 0 : 1, conf.rkt,\n                                &metadata, conf.metadata_timeout * 1000);\n        if (err != RD_KAFKA_RESP_ERR_NO_ERROR)\n                KC_FATAL(\"Failed to acquire metadata: %s%s\",\n                         rd_kafka_err2str(err),\n                         err == RD_KAFKA_RESP_ERR__TRANSPORT ?\n                         \" (Are the brokers reachable? \"\n                         \"Also try increasing the metadata timeout with \"\n                         \"-m <timeout>?)\" : \"\");\n\n#if HAVE_CONTROLLERID\n        controllerid = rd_kafka_controllerid(conf.rk, 0);\n#endif\n\n        /* Print metadata */\n#if ENABLE_JSON\n        if (conf.flags & CONF_F_FMT_JSON)\n                metadata_print_json(metadata, controllerid);\n        else\n#endif\n                metadata_print(metadata, controllerid);\n\n        rd_kafka_metadata_destroy(metadata);\n\n        if (conf.rkt)\n                rd_kafka_topic_destroy(conf.rkt);\n        rd_kafka_destroy(conf.rk);\n}\n\n\n/**\n * Print usage and exit.\n */\nstatic void RD_NORETURN usage (const char *argv0, int exitcode,\n                               const char *reason,\n                               int version_only) {\n\n        FILE *out = stdout;\n        char features[256];\n        size_t flen;\n        rd_kafka_conf_t *tmpconf;\n\n        if (reason) {\n                out = stderr;\n                fprintf(out, \"Error: %s\\n\\n\", reason);\n        }\n\n        if (!version_only)\n                fprintf(out, \"Usage: %s <options> [file1 file2 .. | topic1 topic2 ..]]\\n\",\n                        argv0);\n\n        /* Create a temporary config object to extract builtin.features */\n        tmpconf = rd_kafka_conf_new();\n        flen = sizeof(features);\n        if (rd_kafka_conf_get(tmpconf, \"builtin.features\",\n                              features, &flen) != RD_KAFKA_CONF_OK)\n                strncpy(features, \"n/a\", sizeof(features));\n        rd_kafka_conf_destroy(tmpconf);\n\n        fprintf(out,\n                \"kcat - Apache Kafka producer and consumer tool\\n\"\n                \"https://github.com/edenhill/kcat\\n\"\n                \"Copyright (c) 2014-2021, Magnus Edenhill\\n\"\n                \"Version %s (%s%slibrdkafka %s builtin.features=%s)\\n\"\n                \"\\n\",\n                KCAT_VERSION,\n                \"\"\n#if ENABLE_JSON\n                \"JSON, \"\n#endif\n#if ENABLE_AVRO\n                \"Avro, \"\n#endif\n#if ENABLE_TXNS\n                \"Transactions, \"\n#endif\n#if ENABLE_INCREMENTAL_ASSIGN\n                \"IncrementalAssign, \"\n#endif\n#if ENABLE_MOCK\n                \"MockCluster, \"\n#endif\n                ,\n#if ENABLE_JSON\n                json_can_emit_verbatim() ? \"JSONVerbatim, \" : \"\",\n#else\n                \"\",\n#endif\n                rd_kafka_version_str(), features\n                );\n\n        if (version_only)\n                exit(exitcode);\n\n        fprintf(out, \"\\n\"\n                \"Mode:\\n\"\n                \"  -P                 Producer\\n\"\n                \"  -C                 Consumer\\n\"\n#if ENABLE_KAFKACONSUMER\n                \"  -G <group-id>      High-level KafkaConsumer (Kafka >=0.9 balanced consumer groups)\\n\"\n#endif\n                \"  -L                 Metadata List\\n\"\n                \"  -Q                 Query mode\\n\"\n#if ENABLE_MOCK\n                \"  -M <broker-cnt>    Start Mock cluster\\n\"\n#endif\n                \"\\n\"\n                \"General options for most modes:\\n\"\n                \"  -t <topic>         Topic to consume from, produce to, \"\n                \"or list\\n\"\n                \"  -p <partition>     Partition\\n\"\n                \"  -b <brokers,..>    Bootstrap broker(s) (host[:port])\\n\"\n                \"  -D <delim>         Message delimiter string:\\n\"\n                \"                     a-z | \\\\r | \\\\n | \\\\t | \\\\xNN ..\\n\"\n                \"                     Default: \\\\n\\n\"\n                \"  -K <delim>         Key delimiter (same format as -D)\\n\"\n                \"  -c <cnt>           Limit message count\\n\"\n                \"  -m <seconds>       Metadata (et.al.) request timeout.\\n\"\n                \"                     This limits how long kcat will block\\n\"\n                \"                     while waiting for initial metadata to be\\n\"\n                \"                     retrieved from the Kafka cluster.\\n\"\n                \"                     It also sets the timeout for the producer's\\n\"\n                \"                     transaction commits, init, aborts, etc.\\n\"\n                \"                     Default: 5 seconds.\\n\"\n                \"  -F <config-file>   Read configuration properties from file,\\n\"\n                \"                     file format is \\\"property=value\\\".\\n\"\n                \"                     The KCAT_CONFIG=path environment can \"\n                \"also be used, but -F takes precedence.\\n\"\n                \"                     The default configuration file is \"\n                \"$HOME/.config/kcat.conf\\n\"\n                \"  -X list            List available librdkafka configuration \"\n                \"properties\\n\"\n                \"  -X prop=val        Set librdkafka configuration property.\\n\"\n                \"                     Properties prefixed with \\\"topic.\\\" are\\n\"\n                \"                     applied as topic properties.\\n\"\n#if ENABLE_AVRO\n                \"  -X schema.registry.prop=val Set libserdes configuration property \"\n                \"for the Avro/Schema-Registry client.\\n\"\n#endif\n                \"  -X dump            Dump configuration and exit.\\n\"\n                \"  -d <dbg1,...>      Enable librdkafka debugging:\\n\"\n                \"                     \" RD_KAFKA_DEBUG_CONTEXTS \"\\n\"\n                \"  -q                 Be quiet (verbosity set to 0)\\n\"\n                \"  -v                 Increase verbosity\\n\"\n                \"  -E                 Do not exit on non-fatal error\\n\"\n                \"  -V                 Print version\\n\"\n                \"  -h                 Print usage help\\n\"\n                \"\\n\"\n                \"Producer options:\\n\"\n                \"  -z snappy|gzip|lz4 Message compression. Default: none\\n\"\n                \"  -p -1              Use random partitioner\\n\"\n                \"  -D <delim>         Delimiter to split input into messages\\n\"\n                \"  -K <delim>         Delimiter to split input key and message\\n\"\n                \"  -k <str>           Use a fixed key for all messages.\\n\"\n                \"                     If combined with -K, per-message keys\\n\"\n                \"                     takes precendence.\\n\"\n                \"  -H <header=value>  Add Message Headers \"\n                \"(may be specified multiple times)\\n\"\n                \"  -l                 Send messages from a file separated by\\n\"\n                \"                     delimiter, as with stdin.\\n\"\n                \"                     (only one file allowed)\\n\"\n                \"  -T                 Output sent messages to stdout, acting like tee.\\n\"\n                \"  -c <cnt>           Exit after producing this number \"\n                \"of messages\\n\"\n                \"  -Z                 Send empty messages as NULL messages\\n\"\n                \"  file1 file2..      Read messages from files.\\n\"\n                \"                     With -l, only one file permitted.\\n\"\n                \"                     Otherwise, the entire file contents will\\n\"\n                \"                     be sent as one single message.\\n\"\n                \"  -X transactional.id=.. Enable transactions and send all\\n\"\n                \"                     messages in a single transaction which\\n\"\n                \"                     is committed when stdin is closed or the\\n\"\n                \"                     input file(s) are fully read.\\n\"\n                \"                     If kcat is terminated through Ctrl-C\\n\"\n                \"                     (et.al) the transaction will be aborted.\\n\"\n                \"\\n\"\n                \"Consumer options:\\n\"\n                \"  -o <offset>        Offset to start consuming from:\\n\"\n                \"                     beginning | end | stored |\\n\"\n                \"                     <value>  (absolute offset) |\\n\"\n                \"                     -<value> (relative offset from end)\\n\"\n#if RD_KAFKA_VERSION >= 0x00090300\n                \"                     s@<value> (timestamp in ms to start at)\\n\"\n                \"                     e@<value> (timestamp in ms to stop at \"\n                \"(not included))\\n\"\n#endif\n                \"  -e                 Exit successfully when last message \"\n                \"received\\n\"\n                \"  -f <fmt..>         Output formatting string, see below.\\n\"\n                \"                     Takes precedence over -D and -K.\\n\"\n#if ENABLE_JSON\n                \"  -J                 Output with JSON envelope\\n\"\n#endif\n                \"  -s key=<serdes>    Deserialize non-NULL keys using <serdes>.\\n\"\n                \"  -s value=<serdes>  Deserialize non-NULL values using <serdes>.\\n\"\n                \"  -s <serdes>        Deserialize non-NULL keys and values using <serdes>.\\n\"\n                \"                     Available deserializers (<serdes>):\\n\"\n                \"                       <pack-str> - A combination of:\\n\"\n                \"                                    <: little-endian,\\n\"\n                \"                                    >: big-endian (recommended),\\n\"\n                \"                                    b: signed 8-bit integer\\n\"\n                \"                                    B: unsigned 8-bit integer\\n\"\n                \"                                    h: signed 16-bit integer\\n\"\n                \"                                    H: unsigned 16-bit integer\\n\"\n                \"                                    i: signed 32-bit integer\\n\"\n                \"                                    I: unsigned 32-bit integer\\n\"\n                \"                                    q: signed 64-bit integer\\n\"\n                \"                                    Q: unsigned 64-bit integer\\n\"\n                \"                                    c: ASCII character\\n\"\n                \"                                    s: remaining data is string\\n\"\n                \"                                    $: match end-of-input (no more bytes remaining or a parse error is raised).\\n\"\n                \"                                       Not including this token skips any\\n\"\n                \"                                       remaining data after the pack-str is\\n\"\n                \"                                       exhausted.\\n\"\n#if ENABLE_AVRO\n                \"                       avro       - Avro-formatted with schema in Schema-Registry (requires -r)\\n\"\n                \"                     E.g.: -s key=i -s value=avro - key is 32-bit integer, value is Avro.\\n\"\n                \"                       or: -s avro - both key and value are Avro-serialized\\n\"\n#endif\n#if ENABLE_AVRO\n                \"  -r <url>           Schema registry URL (when avro deserializer is used with -s)\\n\"\n#endif\n                \"  -D <delim>         Delimiter to separate messages on output\\n\"\n                \"  -K <delim>         Print message keys prefixing the message\\n\"\n                \"                     with specified delimiter.\\n\"\n                \"  -O                 Print message offset using -K delimiter\\n\"\n                \"  -c <cnt>           Exit after consuming this number \"\n                \"of messages\\n\"\n                \"  -Z                 Print NULL values and keys as \\\"%s\\\" \"\n                \"instead of empty.\\n\"\n                \"                     For JSON (-J) the nullstr is always null.\\n\"\n                \"  -u                 Unbuffered output\\n\"\n                \"\\n\"\n                \"Metadata options (-L):\\n\"\n                \"  -t <topic>         Topic to query (optional)\\n\"\n                \"\\n\"\n                \"Query options (-Q):\\n\"\n                \"  -t <t>:<p>:<ts>    Get offset for topic <t>,\\n\"\n                \"                     partition <p>, timestamp <ts>.\\n\"\n                \"                     Timestamp is the number of milliseconds\\n\"\n                \"                     since epoch UTC.\\n\"\n                \"                     Requires broker >= 0.10.0.0 and librdkafka >= 0.9.3.\\n\"\n                \"                     Multiple -t .. are allowed but a partition\\n\"\n                \"                     must only occur once.\\n\"\n                \"\\n\"\n#if ENABLE_MOCK\n                \"Mock cluster options (-M):\\n\"\n                \"  The mock cluster is provided by librdkafka and supports a\\n\"\n                \"  reasonable set of Kafka protocol functionality:\\n\"\n                \"  producing, consuming, consumer groups, transactions, etc.\\n\"\n                \"  When kcat is started with -M .. it will print the mock cluster\\n\"\n                \"  bootstrap.servers to stdout, like so:\\n\"\n                \"    BROKERS=broker1:port,broker2:port,..\\n\"\n                \"  Use this list of brokers as bootstrap.servers in your Kafka application.\\n\"\n                \"  When kcat exits (Ctrl-C, Ctrl-D or when stdin is closed) the\\n\"\n                \"  cluster will be terminated.\\n\"\n                \"\\n\"\n#endif\n                \"\\n\"\n                \"Format string tokens:\\n\"\n                \"  %%s                 Message payload\\n\"\n                \"  %%S                 Message payload length (or -1 for NULL)\\n\"\n                \"  %%R                 Message payload length (or -1 for NULL) serialized\\n\"\n                \"                     as a binary big endian 32-bit signed integer\\n\"\n                \"  %%k                 Message key\\n\"\n                \"  %%K                 Message key length (or -1 for NULL)\\n\"\n#if RD_KAFKA_VERSION >= 0x000902ff\n                \"  %%T                 Message timestamp (milliseconds since epoch UTC)\\n\"\n#endif\n#if HAVE_HEADERS\n                \"  %%h                 Message headers (n=v CSV)\\n\"\n#endif\n                \"  %%t                 Topic\\n\"\n                \"  %%p                 Partition\\n\"\n                \"  %%o                 Message offset\\n\"\n                \"  \\\\n \\\\r \\\\t           Newlines, tab\\n\"\n                \"  \\\\xXX \\\\xNNN         Any ASCII character\\n\"\n                \" Example:\\n\"\n                \"  -f 'Topic %%t [%%p] at offset %%o: key %%k: %%s\\\\n'\\n\"\n                \"\\n\"\n#if ENABLE_JSON\n                \"JSON message envelope (on one line) when consuming with -J:\\n\"\n                \" { \\\"topic\\\": str, \\\"partition\\\": int, \\\"offset\\\": int,\\n\"\n                \"   \\\"tstype\\\": \\\"create|logappend|unknown\\\", \\\"ts\\\": int, \"\n                \"// timestamp in milliseconds since epoch\\n\"\n                \"   \\\"broker\\\": int,\\n\"\n                \"   \\\"headers\\\": { \\\"<name>\\\": str, .. }, // optional\\n\"\n                \"   \\\"key\\\": str|json, \\\"payload\\\": str|json,\\n\"\n                \"   \\\"key_error\\\": str, \\\"payload_error\\\": str, //optional\\n\"\n                \"   \\\"key_schema_id\\\": int, \"\n                \"\\\"value_schema_id\\\": int //optional\\n\"\n                \" }\\n\"\n                \" notes:\\n\"\n                \"   - key_error and payload_error are only included if \"\n                \"deserialization fails.\\n\"\n                \"   - key_schema_id and value_schema_id are included for \"\n                \"successfully deserialized Avro messages.\\n\"\n                \"\\n\"\n#endif\n                \"Consumer mode (writes messages to stdout):\\n\"\n                \"  kcat -b <broker> -t <topic> -p <partition>\\n\"\n                \" or:\\n\"\n                \"  kcat -C -b ...\\n\"\n                \"\\n\"\n#if ENABLE_KAFKACONSUMER\n                \"High-level KafkaConsumer mode:\\n\"\n                \"  kcat -b <broker> -G <group-id> topic1 top2 ^aregex\\\\d+\\n\"\n                \"\\n\"\n#endif\n                \"Producer mode (reads messages from stdin):\\n\"\n                \"  ... | kcat -b <broker> -t <topic> -p <partition>\\n\"\n                \" or:\\n\"\n                \"  kcat -P -b ...\\n\"\n                \"\\n\"\n                \"Metadata listing:\\n\"\n                \"  kcat -L -b <broker> [-t <topic>]\\n\"\n                \"\\n\"\n                \"Query offset by timestamp:\\n\"\n                \"  kcat -Q -b broker -t <topic>:<partition>:<timestamp>\\n\"\n                \"\\n\",\n                conf.null_str\n                );\n        exit(exitcode);\n}\n\n\n/**\n * Terminate by putting out the run flag.\n */\nstatic void term (int sig) {\n        conf.run = 0;\n        conf.term_sig = sig;\n}\n\n\n/**\n * librdkafka error callback\n */\nstatic void error_cb (rd_kafka_t *rk, int err,\n                      const char *reason, void *opaque) {\n#if RD_KAFKA_VERSION >= 0x01000000\n        if (err == RD_KAFKA_RESP_ERR__FATAL) {\n                /* A fatal error has been raised, extract the\n                 * underlying error, and start graceful termination -\n                 * this to make sure producer delivery reports are\n                 * handled before exiting. */\n                char fatal_errstr[512];\n                rd_kafka_resp_err_t fatal_err;\n\n                fatal_err = rd_kafka_fatal_error(rk, fatal_errstr,\n                                                 sizeof(fatal_errstr));\n                KC_INFO(0, \"FATAL CLIENT ERROR: %s: %s: terminating\\n\",\n                        rd_kafka_err2str(fatal_err), fatal_errstr);\n                conf.run = 0;\n\n        } else\n#endif\n                if (err == RD_KAFKA_RESP_ERR__ALL_BROKERS_DOWN) {\n                        KC_ERROR(\"%s: %s\", rd_kafka_err2str(err),\n                                 reason ? reason : \"\");\n                } else {\n                        KC_INFO(1, \"ERROR: %s: %s\\n\", rd_kafka_err2str(err),\n                                reason ? reason : \"\");\n                }\n}\n\n\n/**\n * @brief Parse delimiter string from command line arguments and return\n *        an allocated copy.\n */\nstatic char *parse_delim (const char *instr) {\n        char *str;\n\n        /* Make a copy so we can modify the string. */\n        str = strdup(instr);\n\n        while (1) {\n                size_t skip = 0;\n                char *t;\n\n                if ((t = strstr(str, \"\\\\n\"))) {\n                        *t = '\\n';\n                        skip = 1;\n                } else if ((t = strstr(str, \"\\\\t\"))) {\n                        *t = '\\t';\n                        skip = 1;\n                } else if ((t = strstr(str, \"\\\\x\"))) {\n                        char *end;\n                        int x;\n                        x = strtoul(t+strlen(\"\\\\x\"), &end, 16) & 0xff;\n                        if (end == t+strlen(\"\\\\x\"))\n                                KC_FATAL(\"Delimiter %s expects hex number\", t);\n                        *t = (char)x;\n                        skip = (int)(end - t) - 1;\n                } else\n                        break;\n\n                if (t && skip)\n                        memmove(t+1, t+1+skip, strlen(t+1+skip)+1);\n        }\n\n        return str;\n}\n\n/**\n * @brief Add topic+partition+offset to list, from :-separated string.\n *\n * \"<t>:<p>:<o>\"\n *\n * @remark Will modify \\p str\n */\nstatic void add_topparoff (const char *what,\n                           rd_kafka_topic_partition_list_t *rktparlist,\n                           char *str) {\n        char *s, *t, *e;\n        char *topic;\n        int partition;\n        int64_t offset;\n\n        if (!(s = strchr(str, ':')) ||\n            !(t = strchr(s+1, ':')))\n                KC_FATAL(\"%s: expected \\\"topic:partition:offset_or_timestamp\\\"\", what);\n\n        topic = str;\n        *s = '\\0';\n\n        partition = strtoul(s+1, &e, 0);\n        if (e == s+1)\n                KC_FATAL(\"%s: expected \\\"topic:partition:offset_or_timestamp\\\"\", what);\n\n        offset = strtoll(t+1, &e, 0);\n        if (e == t+1)\n                KC_FATAL(\"%s: expected \\\"topic:partition:offset_or_timestamp\\\"\", what);\n\n        rd_kafka_topic_partition_list_add(rktparlist, topic, partition)->offset = offset;\n}\n\n/**\n * Dump current rdkafka configuration to stdout.\n */\nstatic void conf_dump (void) {\n        const char **arr;\n        size_t cnt;\n        int pass;\n\n        for (pass = 0 ; pass < 2 ; pass++) {\n                int i;\n\n                if (pass == 0) {\n                        arr = rd_kafka_conf_dump(conf.rk_conf, &cnt);\n                        printf(\"# Global config\\n\");\n                } else {\n                        printf(\"# Topic config\\n\");\n                        arr = rd_kafka_topic_conf_dump(conf.rkt_conf, &cnt);\n                }\n\n                for (i = 0 ; i < (int)cnt ; i += 2)\n                        printf(\"%s = %s\\n\",\n                               arr[i], arr[i+1]);\n\n                printf(\"\\n\");\n\n                rd_kafka_conf_dump_free(arr, cnt);\n        }\n}\n\n\n/**\n * @brief Try setting a config property. Provides \"topic.\" fallthru.\n *\n * @remark \\p val may be truncated by this function.\n *\n * @returns -1 on failure or 0 on success.\n */\nstatic int try_conf_set (const char *name, char *val,\n                         char *errstr, size_t errstr_size) {\n        rd_kafka_conf_res_t res = RD_KAFKA_CONF_UNKNOWN;\n        size_t srlen = strlen(\"schema.registry.\");\n\n        /* Pass schema.registry. config to the serdes */\n        if (!strncmp(name, \"schema.registry.\", srlen)) {\n#if ENABLE_AVRO\n                serdes_err_t serr;\n\n                if (!conf.srconf)\n                        conf.srconf = serdes_conf_new(NULL, 0, NULL);\n\n                if (!strcmp(name, \"schema.registry.url\")) {\n                        char *t;\n\n                        /* Trim trailing slashes from URL to avoid 404 */\n                        for (t = val + strlen(val) - 1;\n                             t >= val && *t == '/'; t--)\n                                *t = '\\0';\n\n                        if (!*t) {\n                                snprintf(errstr, errstr_size,\n                                         \"schema.registry.url is empty\");\n                                return -1;\n                        }\n\n                        conf.flags |= CONF_F_SR_URL_SEEN;\n                        srlen = 0;\n                }\n\n                serr = serdes_conf_set(conf.srconf, name+srlen, val,\n                                       errstr, errstr_size);\n                return serr == SERDES_ERR_OK ? 0 : -1;\n#else\n                snprintf(errstr, errstr_size,\n                         \"This build of kcat lacks \"\n                         \"Avro/Schema-Registry support\");\n                return -1;\n#endif\n        }\n\n\n        /* Try \"topic.\" prefixed properties on topic\n         * conf first, and then fall through to global if\n         * it didnt match a topic configuration property. */\n        if (!strncmp(name, \"topic.\", strlen(\"topic.\")))\n                res = rd_kafka_topic_conf_set(conf.rkt_conf,\n                                              name+\n                                              strlen(\"topic.\"),\n                                              val,\n                                              errstr, errstr_size);\n        else\n                /* If no \"topic.\" prefix, try the topic config first. */\n                res = rd_kafka_topic_conf_set(conf.rkt_conf,\n                                              name, val,\n                                              errstr, errstr_size);\n\n        if (res == RD_KAFKA_CONF_UNKNOWN)\n                res = rd_kafka_conf_set(conf.rk_conf, name, val,\n                                        errstr, errstr_size);\n\n        if (res != RD_KAFKA_CONF_OK)\n                return -1;\n\n        if (!strcmp(name, \"metadata.broker.list\") ||\n            !strcmp(name, \"bootstrap.servers\"))\n                conf.flags |= CONF_F_BROKERS_SEEN;\n\n        if (!strcmp(name, \"api.version.request\"))\n                conf.flags |= CONF_F_APIVERREQ_USER;\n\n#if RD_KAFKA_VERSION >= 0x00090000\n        rd_kafka_conf_set_throttle_cb(conf.rk_conf, throttle_cb);\n#endif\n\n\n        return 0;\n}\n\n/**\n * @brief Intercept configuration properties and try to identify\n *        incompatible properties that needs to be converted to librdkafka\n *        configuration properties.\n *\n * @returns -1 on failure, 0 if the property was not handled,\n *          or 1 if it was handled.\n */\nstatic int try_java_conf_set (const char *name, const char *val,\n                              char *errstr, size_t errstr_size) {\n        if (!strcmp(name, \"ssl.endpoint.identification.algorithm\"))\n                return 1; /* SSL server verification:\n                           * not supported by librdkafka: ignore for now */\n\n        if (!strcmp(name, \"sasl.jaas.config\")) {\n                char sasl_user[128], sasl_pass[128];\n                if (sscanf(val,\n                           \"org.apache.kafka.common.security.plain.PlainLoginModule required username=\\\"%[^\\\"]\\\" password=\\\"%[^\\\"]\\\"\",\n                           sasl_user, sasl_pass) == 2) {\n                        if (try_conf_set(\"sasl.username\", sasl_user,\n                                         errstr, errstr_size) == -1 ||\n                            try_conf_set(\"sasl.password\", sasl_pass,\n                                         errstr, errstr_size) == -1)\n                                return -1;\n                        return 1;\n                }\n        }\n\n        return 0;\n}\n\n\n/**\n * @brief Read config file, fail terminally if fatal is true, else\n *        fail silently.\n *\n * @returns 0 on success or -1 on failure (unless fatal is true\n *          in which case the app will have exited).\n */\nstatic int read_conf_file (const char *path, int fatal) {\n        FILE *fp;\n        char buf[512];\n        int line = 0;\n\n        if (!(fp = fopen(path, \"r\"))) {\n                if (fatal)\n                        KC_FATAL(\"Failed to open %s: %s\",\n                                 path, strerror(errno));\n                return -1;\n        }\n\n        KC_INFO(fatal ? 1 : 3, \"Reading configuration from file %s\\n\", path);\n\n        while (fgets(buf, sizeof(buf), fp)) {\n                char *s = buf;\n                char *t;\n                char errstr[512];\n                int r;\n\n                line++;\n\n                /* Left-trim */\n                while (isspace(*s))\n                        s++;\n\n                /* Right-trim and remove newline */\n                t = s + strlen(s) - 1;\n                while (t > s && isspace(*t)) {\n                        *t = 0;\n                        t--;\n                }\n\n                /* Ignore Empty line */\n                if (!*s)\n                        continue;\n\n                /* Ignore comments */\n                if (*s == '#')\n                        continue;\n\n                /* Strip escapes for \\: \\= which can be encountered in\n                 * Java configs (see comment below) */\n                while ((t = strstr(s, \"\\\\:\"))) {\n                        memmove(t, t+1, strlen(t+1)+1); /* overwrite \\: */\n                        *t = ':'; /* reinsert : */\n                }\n                while ((t = strstr(s, \"\\\\=\"))) {\n                        memmove(t, t+1, strlen(t+1)+1); /* overwrite \\= */\n                        *t = '='; /* reinsert : */\n                }\n\n                /* Parse prop=value */\n                if (!(t = strchr(s, '=')) || t == s)\n                        KC_FATAL(\"%s:%d: expected property=value\\n\",\n                                 path, line);\n\n                *t = 0;\n                t++;\n\n                /**\n                 * Attempt to support Java client configuration files,\n                 * such as the ccloud config.\n                 * There are some quirks with unnecessary escaping with \\\n                 * that we remove, as well as parsing special configuration\n                 * properties that don't match librdkafka's.\n                 */\n                r = try_java_conf_set(s, t, errstr, sizeof(errstr));\n                if (r == -1)\n                        KC_FATAL(\"%s:%d: %s (java config conversion)\\n\",\n                                 path, line, errstr);\n                else if (r == 1)\n                        continue; /* Handled */\n\n                if (try_conf_set(s, t, errstr, sizeof(errstr)) == -1)\n                        KC_FATAL(\"%s:%d: %s\\n\", path, line, errstr);\n        }\n\n        fclose(fp);\n\n        return 0;\n}\n\n\n/**\n * @returns the value for environment variable \\p env, or NULL\n *          if it is not set or it is empty.\n */\nstatic const char *kc_getenv (const char *env) {\n        const char *val;\n        if (!(val = getenv(env)) || !*val)\n                return NULL;\n        return val;\n}\n\nstatic void read_default_conf_files (void) {\n        char kpath[512], kpath2[512];\n        const char *home;\n\n        if (!(home = kc_getenv(\"HOME\")))\n                return;\n\n        snprintf(kpath, sizeof(kpath), \"%s/.config/kcat.conf\", home);\n\n        if (read_conf_file(kpath, 0/*not fatal*/) == 0)\n                return;\n\n        snprintf(kpath2, sizeof(kpath2), \"%s/.config/kafkacat.conf\", home);\n\n        if (read_conf_file(kpath2, 0/*not fatal*/) == 0) {\n                KC_INFO(1,\n                        \"Configuration filename kafkacat.conf is \"\n                        \"deprecated!\\n\");\n                KC_INFO(1, \"Rename %s to %s\\n\", kpath2, kpath);\n        }\n}\n\n\n\nstatic int unittest_strnstr (void) {\n        struct {\n                const char *sep;\n                const char *hay;\n                int offset;\n        } exp[] = {\n                { \";Sep;\", \";Sep;Post\", 0 },\n                { \";Sep;\", \";Sep;\", 0 },\n                { \";Sep;\", \"Pre;Sep;Post\", 3 },\n                { \";Sep;\", \"Pre;Sep;\", 3 },\n                { \";Sep;\", \"Pre;SepPost\", -1 },\n                { \";KeyDel;\", \"Key1;KeyDel;Value1\", 4 },\n                { \";\", \"Is The;\", 6 },\n                { NULL },\n        };\n        int i;\n        int fails = 0;\n\n        for (i = 0 ; exp[i].sep ; i++) {\n                const char *t = rd_strnstr(exp[i].hay, strlen(exp[i].hay),\n                                           exp[i].sep, strlen(exp[i].sep));\n                const char *e = exp[i].hay + exp[i].offset;\n                const char *fail = NULL;\n\n                if (exp[i].offset == -1) {\n                        if (t)\n                                fail = \"expected no match\";\n                } else if (!t) {\n                        fail = \"expected match\";\n                } else if (t != e)\n                        fail = \"invalid match\";\n\n                if (!fail)\n                        continue;\n\n                fprintf(stderr,\n                        \"%s: FAILED: for hay %d: \"\n                        \"match is %p '%s' for %p '%s' in %p '%s' \"\n                        \"(want offset %d, not %d): %s\\n\",\n                        __FUNCTION__,\n                        i,\n                        t, t,\n                        exp[i].sep, exp[i].sep,\n                        exp[i].hay, exp[i].hay,\n                        exp[i].offset,\n                        t ? (int)(t - exp[i].hay) : -1,\n                        fail);\n                fails++;\n        }\n\n        return fails;\n}\n\nstatic int unittest_parse_delim (void) {\n        struct {\n                const char *in;\n                const char *exp;\n        } delims[] = {\n                { \"\", \"\" },\n                { \"\\\\n\", \"\\n\" },\n                { \"\\\\t\\\\n\\\\n\", \"\\t\\n\\n\" },\n                { \"\\\\x54!\\\\x45\\\\x53T\", \"T!EST\" },\n                { \"\\\\x30\", \"0\" },\n                { NULL }\n        };\n        int i;\n        int fails = 0;\n\n        for (i = 0 ; delims[i].in ; i++) {\n                char *out = parse_delim(delims[i].in);\n                if (strcmp(out, delims[i].exp))\n                        fprintf(stderr, \"%s: FAILED: \"\n                                \"expected '%s' to return '%s', not '%s'\\n\",\n                                __FUNCTION__, delims[i].in, delims[i].exp, out);\n                free(out);\n        }\n\n        return fails;\n}\n\n\n/**\n * @brief Run unittests\n *\n * @returns the number of failed tests.\n */\nstatic int unittest (void) {\n        int r = 0;\n\n        r += unittest_strnstr();\n        r += unittest_parse_delim();\n\n        return r;\n}\n\n/**\n * @brief Add a single header specified as a command line option.\n *\n * @param inp \"name=value\" or \"name\" formatted header\n */\nstatic void add_header (const char *inp) {\n        const char *t;\n        rd_kafka_resp_err_t err;\n\n        t = strchr(inp, '=');\n        if (t == inp || !*inp)\n                KC_FATAL(\"Expected -H \\\"name=value..\\\" or -H \\\"name\\\"\");\n\n        if (!conf.headers)\n                conf.headers = rd_kafka_headers_new(8);\n\n\n        err = rd_kafka_header_add(conf.headers,\n                                  inp,\n                                  t ? (ssize_t)(t-inp) : -1,\n                                  t ? t+1 : NULL, -1);\n        if (err)\n                KC_FATAL(\"Failed to add header \\\"%s\\\": %s\",\n                         inp, rd_kafka_err2str(err));\n}\n\n\n/**\n * Parse command line arguments\n */\nstatic void argparse (int argc, char **argv,\n                      rd_kafka_topic_partition_list_t **rktparlistp) {\n        char errstr[512];\n        int opt;\n        const char *fmt = NULL;\n        const char *delim = \"\\n\";\n        const char *key_delim = NULL;\n        char tmp_fmt[64];\n        int do_conf_dump = 0;\n        int conf_files_read = 0;\n        int i;\n\n        while ((opt = getopt(argc, argv,\n                             \":PCG:LQM:t:p:b:z:o:eED:K:k:H:Od:qvF:X:c:Tuf:ZlVh\"\n                             \"s:r:Jm:U\")) != -1) {\n                switch (opt) {\n                case 'P':\n                case 'C':\n                case 'L':\n                case 'Q':\n                        if (conf.mode && conf.mode != opt)\n                                KC_FATAL(\"Do not mix modes: -%c seen when \"\n                                         \"-%c already set\",\n                                         (char)opt, conf.mode);\n                        conf.mode = opt;\n                        break;\n#if ENABLE_KAFKACONSUMER\n                case 'G':\n                        if (conf.mode && conf.mode != opt)\n                                KC_FATAL(\"Do not mix modes: -%c seen when \"\n                                         \"-%c already set\",\n                                         (char)opt, conf.mode);\n                        conf.mode = opt;\n                        conf.group = optarg;\n                        if (rd_kafka_conf_set(conf.rk_conf, \"group.id\", optarg,\n                                              errstr, sizeof(errstr)) !=\n                            RD_KAFKA_CONF_OK)\n                                KC_FATAL(\"%s\", errstr);\n                        break;\n#endif\n#if ENABLE_MOCK\n                case 'M':\n                        if (conf.mode && conf.mode != opt)\n                                KC_FATAL(\"Do not mix modes: -%c seen when \"\n                                         \"-%c already set\",\n                                         (char)opt, conf.mode);\n                        conf.mode = opt;\n                        conf.mock.broker_cnt = atoi(optarg);\n                        if (conf.mock.broker_cnt <= 0)\n                                KC_FATAL(\"-M <broker_cnt> expected\");\n                        break;\n#endif\n                case 't':\n                        if (conf.mode == 'Q') {\n                                if (!*rktparlistp)\n                                        *rktparlistp = rd_kafka_topic_partition_list_new(1);\n                                add_topparoff(\"-t\", *rktparlistp, optarg);\n                                conf.flags |= CONF_F_APIVERREQ;\n                        } else\n                                conf.topic = optarg;\n\n                        break;\n                case 'p':\n                        conf.partition = atoi(optarg);\n                        break;\n                case 'b':\n                        conf.brokers = optarg;\n                        conf.flags |= CONF_F_BROKERS_SEEN;\n                        break;\n                case 'z':\n                        if (rd_kafka_conf_set(conf.rk_conf,\n                                              \"compression.codec\", optarg,\n                                              errstr, sizeof(errstr)) !=\n                            RD_KAFKA_CONF_OK)\n                                KC_FATAL(\"%s\", errstr);\n                        break;\n                case 'o':\n                        if (!strcmp(optarg, \"end\"))\n                                conf.offset = RD_KAFKA_OFFSET_END;\n                        else if (!strcmp(optarg, \"beginning\"))\n                                conf.offset = RD_KAFKA_OFFSET_BEGINNING;\n                        else if (!strcmp(optarg, \"stored\"))\n                                conf.offset = RD_KAFKA_OFFSET_STORED;\n#if RD_KAFKA_VERSION >= 0x00090300\n                        else if (!strncmp(optarg, \"s@\", 2)) {\n                                conf.startts = strtoll(optarg+2, NULL, 10);\n                                conf.flags |= CONF_F_APIVERREQ;\n                        } else if (!strncmp(optarg, \"e@\", 2)) {\n                                conf.stopts = strtoll(optarg+2, NULL, 10);\n                                conf.flags |= CONF_F_APIVERREQ;\n                        }\n#endif\n                        else {\n                                conf.offset = strtoll(optarg, NULL, 10);\n                                if (conf.offset < 0)\n                                        conf.offset = RD_KAFKA_OFFSET_TAIL(-conf.offset);\n                        }\n                        break;\n                case 'e':\n                        conf.exit_eof = 1;\n                        break;\n                case 'E':\n                        conf.exitonerror = 0;\n                        break;\n                case 'f':\n                        fmt = optarg;\n                        break;\n                case 'J':\n#if ENABLE_JSON\n                        conf.flags |= CONF_F_FMT_JSON;\n#else\n                        KC_FATAL(\"This build of kcat lacks JSON support\");\n#endif\n                        break;\n\n                case 's':\n                {\n                        int field = -1;\n                        const char *t = optarg;\n\n                        if (!strncmp(t, \"key=\", strlen(\"key=\"))) {\n                                t += strlen(\"key=\");\n                                field = KC_MSG_FIELD_KEY;\n                        } else if (!strncmp(t, \"value=\", strlen(\"value=\"))) {\n                                t += strlen(\"value=\");\n                                field = KC_MSG_FIELD_VALUE;\n                        }\n\n                        if (field == -1 || field == KC_MSG_FIELD_KEY) {\n                                if (strcmp(t, \"avro\"))\n                                        pack_check(\"key\", t);\n                                conf.pack[KC_MSG_FIELD_KEY] = t;\n                        }\n\n                        if (field == -1 || field == KC_MSG_FIELD_VALUE) {\n                                if (strcmp(t, \"avro\"))\n                                        pack_check(\"value\", t);\n                                conf.pack[KC_MSG_FIELD_VALUE] = t;\n                        }\n                }\n                break;\n                case 'r':\n#if ENABLE_AVRO\n                        if (!*optarg)\n                                KC_FATAL(\"-r url must not be empty\");\n\n                        if (try_conf_set(\"schema.registry.url\", optarg,\n                                         errstr, sizeof(errstr)) == -1)\n                                KC_FATAL(\"%s\", errstr);\n#else\n                        KC_FATAL(\"This build of kcat lacks \"\n                                 \"Avro/Schema-Registry support\");\n#endif\n                        break;\n                case 'D':\n                        delim = optarg;\n                        break;\n                case 'K':\n                        key_delim = optarg;\n                        conf.flags |= CONF_F_KEY_DELIM;\n                        break;\n                case 'k':\n                        conf.fixed_key = optarg;\n                        conf.fixed_key_len = (size_t)(strlen(conf.fixed_key));\n                        break;\n                case 'H':\n                        add_header(optarg);\n                        break;\n                case 'l':\n                        conf.flags |= CONF_F_LINE;\n                        break;\n                case 'O':\n                        conf.flags |= CONF_F_OFFSET;\n                        break;\n                case 'c':\n                        conf.msg_cnt = strtoll(optarg, NULL, 10);\n                        break;\n                case 'm':\n                        conf.metadata_timeout = strtoll(optarg, NULL, 10);\n                        break;\n                case 'Z':\n                        conf.flags |= CONF_F_NULL;\n                        conf.null_str_len = strlen(conf.null_str);\n                        break;\n                case 'd':\n                        conf.debug = optarg;\n                        if (rd_kafka_conf_set(conf.rk_conf, \"debug\", conf.debug,\n                                              errstr, sizeof(errstr)) !=\n                            RD_KAFKA_CONF_OK)\n                                KC_FATAL(\"%s\", errstr);\n                        break;\n                case 'q':\n                        conf.verbosity = 0;\n                        break;\n                case 'v':\n                        conf.verbosity++;\n                        break;\n                case 'T':\n                        conf.flags |= CONF_F_TEE;\n                        break;\n                case 'u':\n                        setbuf(stdout, NULL);\n                        break;\n                case 'F':\n                        conf.flags |= CONF_F_NO_CONF_SEARCH;\n                        if (!strcmp(optarg, \"-\") || !strcmp(optarg, \"none\"))\n                                break;\n\n                        read_conf_file(optarg, 1);\n                        conf_files_read++;\n                        break;\n                case 'X':\n                {\n                        char *name, *val;\n\n                        if (!strcmp(optarg, \"list\") ||\n                            !strcmp(optarg, \"help\")) {\n                                rd_kafka_conf_properties_show(stdout);\n                                exit(0);\n                        }\n\n                        if (!strcmp(optarg, \"dump\")) {\n                                do_conf_dump = 1;\n                                continue;\n                        }\n\n                        name = optarg;\n                        if (!(val = strchr(name, '='))) {\n                                fprintf(stderr, \"%% Expected \"\n                                        \"-X property=value, not %s, \"\n                                        \"use -X list to display available \"\n                                        \"properties\\n\", name);\n                                exit(1);\n                        }\n\n                        *val = '\\0';\n                        val++;\n\n                        if (try_conf_set(name, val,\n                                         errstr, sizeof(errstr)) == -1)\n                                KC_FATAL(\"%s\", errstr);\n                }\n                break;\n\n                case 'U':\n                        if (unittest())\n                                exit(1);\n                        else\n                                exit(0);\n                        break;\n\n                case 'V':\n                        usage(argv[0], 0, NULL, 1);\n                        break;\n\n                case 'h':\n                        usage(argv[0], 0, NULL, 0);\n                        break;\n\n                default:\n                        usage(argv[0], 1, \"unknown argument\", 0);\n                        break;\n                }\n        }\n\n\n        if (conf_files_read == 0) {\n                const char *cpath = kc_getenv(\"KCAT_CONFIG\");\n                if (cpath) {\n                        conf.flags |= CONF_F_NO_CONF_SEARCH;\n                        read_conf_file(cpath, 1/*fatal errors*/);\n\n                } else if ((cpath = kc_getenv(\"KAFKACAT_CONFIG\"))) {\n                        KC_INFO(1, \"KAFKA_CONFIG is deprecated!\\n\");\n                        KC_INFO(1, \"Rename KAFKA_CONFIG to KCAT_CONFIG\\n\");\n                        conf.flags |= CONF_F_NO_CONF_SEARCH;\n                        read_conf_file(cpath, 1/*fatal errors*/);\n                }\n        }\n\n        if (!(conf.flags & CONF_F_NO_CONF_SEARCH))\n                read_default_conf_files();\n\n        /* Dump configuration and exit, if so desired. */\n        if (do_conf_dump) {\n                conf_dump();\n                exit(0);\n        }\n\n        if (!(conf.flags & CONF_F_BROKERS_SEEN) && conf.mode != 'M')\n                usage(argv[0], 1, \"-b <broker,..> missing\", 0);\n\n        /* Decide mode if not specified */\n        if (!conf.mode) {\n                if (_COMPAT(isatty)(STDIN_FILENO))\n                        conf.mode = 'C';\n                else\n                        conf.mode = 'P';\n                KC_INFO(1, \"Auto-selecting %s mode (use -P or -C to override)\\n\",\n                        conf.mode == 'C' ? \"Consumer\":\"Producer\");\n        }\n\n\n        if (!strchr(\"GLQM\", conf.mode) && !conf.topic)\n                usage(argv[0], 1, \"-t <topic> missing\", 0);\n        else if (conf.mode == 'Q' && !*rktparlistp)\n                usage(argv[0], 1,\n                      \"-t <topic>:<partition>:<offset_or_timestamp> missing\",\n                      0);\n\n        if (conf.brokers &&\n            rd_kafka_conf_set(conf.rk_conf, \"metadata.broker.list\",\n                              conf.brokers, errstr, sizeof(errstr)) !=\n            RD_KAFKA_CONF_OK)\n                usage(argv[0], 1, errstr, 0);\n\n        rd_kafka_conf_set_error_cb(conf.rk_conf, error_cb);\n\n        fmt_init();\n\n        /*\n         * Verify serdes\n         */\n        for (i = 0 ; i < KC_MSG_FIELD_CNT ; i++) {\n                if (!conf.pack[i])\n                        continue;\n\n                if (!strchr(\"GC\", conf.mode))\n                        KC_FATAL(\"-s serdes only available in the consumer\");\n\n                if (conf.pack[i] && !strcmp(conf.pack[i], \"avro\")) {\n#if !ENABLE_AVRO\n                        KC_FATAL(\"This build of kcat lacks \"\n                                 \"Avro/Schema-Registry support\");\n#endif\n#if ENABLE_JSON\n                        /* Avro is decoded to JSON which needs to be\n                         * written verbatim to the JSON envelope when\n                         * using -J: libyajl does not support this,\n                         * but my own fork of yajl does. */\n                        if (conf.flags & CONF_F_FMT_JSON &&\n                            !json_can_emit_verbatim())\n                                KC_FATAL(\"This build of kcat lacks \"\n                                         \"support for emitting \"\n                                         \"JSON-formatted \"\n                                         \"message keys and values: \"\n                                         \"try without -J or build \"\n                                         \"kcat with yajl from \"\n                                         \"https://github.com/edenhill/yajl\");\n#endif\n\n                        if (i == KC_MSG_FIELD_VALUE)\n                                conf.flags |= CONF_F_FMT_AVRO_VALUE;\n                        else if (i == KC_MSG_FIELD_KEY)\n                                conf.flags |= CONF_F_FMT_AVRO_KEY;\n                        continue;\n                }\n        }\n\n\n        /*\n         * Verify and initialize Avro/SR\n         */\n#if ENABLE_AVRO\n        if (conf.flags & (CONF_F_FMT_AVRO_VALUE|CONF_F_FMT_AVRO_KEY)) {\n\n                if (!(conf.flags & CONF_F_SR_URL_SEEN))\n                        KC_FATAL(\"-s avro requires -r <sr_url>\");\n\n                if (!strchr(\"GC\", conf.mode))\n                        KC_FATAL(\"Avro and Schema-registry support is \"\n                                 \"currently only available in the consumer\");\n\n                /* Initialize Avro/Schema-Registry client */\n                kc_avro_init(NULL, NULL, NULL, NULL);\n        }\n#endif\n\n\n        /* If avro key is to be deserialized, set up an delimiter so that\n         * the key is actually emitted. */\n        if ((conf.flags & CONF_F_FMT_AVRO_KEY) && !key_delim)\n                key_delim = \"\";\n\n        if (key_delim) {\n                conf.key_delim = parse_delim(key_delim);\n                conf.key_delim_size = strlen(conf.key_delim);\n        }\n\n        conf.delim = parse_delim(delim);\n        conf.delim_size = strlen(conf.delim);\n\n        if (strchr(\"GC\", conf.mode)) {\n                /* Must be explicitly enabled for librdkafka >= v1.0.0 */\n                rd_kafka_conf_set(conf.rk_conf, \"enable.partition.eof\", \"true\",\n                                  NULL, 0);\n\n                if (!fmt) {\n                        if ((conf.flags & CONF_F_FMT_JSON)) {\n                                /* For JSON the format string is simply the\n                                 * output object delimiter (e.g., newline). */\n                                fmt = conf.delim;\n                        } else {\n                                if (conf.key_delim)\n                                        snprintf(tmp_fmt, sizeof(tmp_fmt),\n                                                 \"%%k%s%%s%s\",\n                                                 conf.key_delim, conf.delim);\n                                else\n                                        snprintf(tmp_fmt, sizeof(tmp_fmt),\n                                                 \"%%s%s\", conf.delim);\n                                fmt = tmp_fmt;\n                        }\n                }\n\n                fmt_parse(fmt);\n\n        } else if (conf.mode == 'P') {\n                if (conf.delim_size == 0)\n                        KC_FATAL(\"Message delimiter -D must not be empty \"\n                                 \"when producing\");\n        }\n\n        /* Automatically enable API version requests if needed and\n         * user hasn't explicitly configured it (in any way). */\n        if ((conf.flags & (CONF_F_APIVERREQ | CONF_F_APIVERREQ_USER)) ==\n            CONF_F_APIVERREQ) {\n                KC_INFO(2, \"Automatically enabling api.version.request=true\\n\");\n                rd_kafka_conf_set(conf.rk_conf, \"api.version.request\", \"true\",\n                                  NULL, 0);\n        }\n}\n\n\n\n\nint main (int argc, char **argv) {\n#ifdef SIGIO\n        char tmp[16];\n#endif\n        FILE *in = stdin;\n        struct timeval tv;\n        rd_kafka_topic_partition_list_t *rktparlist = NULL;\n\n        /* Certain Docker images don't have kcat as the entry point,\n         * requiring `kcat` to be the first argument. As these images\n         * are fixed the examples get outdated and that first argument\n         * will still be passed to the container and thus kcat,\n         * so remove it here. */\n        if (argc > 1 && (!strcmp(argv[1], \"kcat\") ||\n                         !strcmp(argv[1], \"kafkacat\"))) {\n                if (argc > 2)\n                        memmove(&argv[1], &argv[2], sizeof(*argv) * (argc - 2));\n                argc--;\n        }\n\n        /* Seed rng for random partitioner, jitter, etc. */\n        rd_gettimeofday(&tv, NULL);\n        srand(tv.tv_usec);\n\n        /* Create config containers */\n        conf.rk_conf  = rd_kafka_conf_new();\n        conf.rkt_conf = rd_kafka_topic_conf_new();\n\n        /*\n         * Default config\n         */\n#ifdef SIGIO\n        /* Enable quick termination of librdkafka */\n        snprintf(tmp, sizeof(tmp), \"%i\", SIGIO);\n        rd_kafka_conf_set(conf.rk_conf, \"internal.termination.signal\",\n                          tmp, NULL, 0);\n#endif\n\n        /* Log callback */\n        rd_kafka_conf_set_log_cb(conf.rk_conf, rd_kafka_log_print);\n\n        /* Parse command line arguments */\n        argparse(argc, argv, &rktparlist);\n\n        if (optind < argc) {\n                if (!strchr(\"PG\", conf.mode))\n                        usage(argv[0], 1,\n                              \"file/topic list only allowed in \"\n                              \"producer(-P)/kafkaconsumer(-G) mode\", 0);\n                else if ((conf.flags & CONF_F_LINE) && argc - optind > 1)\n                        KC_FATAL(\"Only one file allowed for line mode (-l)\");\n                else if (conf.flags & CONF_F_LINE) {\n                        in = fopen(argv[optind], \"r\");\n                        if (in == NULL)\n                                KC_FATAL(\"Cannot open %s: %s\", argv[optind],\n                                      strerror(errno));\n                }\n        }\n\n        signal(SIGINT, term);\n        signal(SIGTERM, term);\n#ifdef SIGPIPE\n        signal(SIGPIPE, term);\n#endif\n\n        /* Run according to mode */\n        switch (conf.mode)\n        {\n        case 'C':\n                consumer_run(stdout);\n                break;\n\n#if ENABLE_KAFKACONSUMER\n        case 'G':\n                if (conf.stopts || conf.startts)\n                        KC_FATAL(\"-o ..@ timestamps can't be used \"\n                                 \"with -G mode\\n\");\n                kafkaconsumer_run(stdout, &argv[optind], argc-optind);\n                break;\n#endif\n\n        case 'P':\n                producer_run(in, &argv[optind], argc-optind);\n                break;\n\n        case 'L':\n                metadata_list();\n                break;\n\n        case 'Q':\n                if (!rktparlist)\n                        usage(argv[0], 1,\n                              \"-Q requires one or more \"\n                              \"-t <topic>:<partition>:<timestamp>\", 0);\n\n                query_offsets_by_time(rktparlist);\n\n                rd_kafka_topic_partition_list_destroy(rktparlist);\n                break;\n\n#if ENABLE_MOCK\n        case 'M':\n                mock_run();\n                break;\n#endif\n\n        default:\n                usage(argv[0], 0, NULL, 0);\n                break;\n        }\n\n        if (conf.headers)\n                rd_kafka_headers_destroy(conf.headers);\n\n        if (conf.key_delim)\n                free(conf.key_delim);\n        if (conf.delim)\n                free(conf.delim);\n\n        if (in != stdin)\n                fclose(in);\n\n        rd_kafka_wait_destroyed(5000);\n\n#if ENABLE_AVRO\n        kc_avro_term();\n#endif\n\n        fmt_term();\n\n        exit(conf.exitcode);\n}\n"
        },
        {
          "name": "kcat.h",
          "type": "blob",
          "size": 6.7158203125,
          "content": "/*\n * kcat - Apache Kafka consumer and producer\n *\n * Copyright (c) 2015-2019, Magnus Edenhill\n * All rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n *\n * 1. Redistributions of source code must retain the above copyright notice,\n *    this list of conditions and the following disclaimer.\n * 2. Redistributions in binary form must reproduce the above copyright notice,\n *    this list of conditions and the following disclaimer in the documentation\n *    and/or other materials provided with the distribution.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE\n * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n * POSSIBILITY OF SUCH DAMAGE.\n */\n#pragma once\n\n#include <inttypes.h>\n#include <errno.h>\n#include <string.h>\n#include <stdlib.h>\n\n#include <librdkafka/rdkafka.h>\n\n#include \"rdport.h\"\n\n#ifdef _MSC_VER\n#pragma comment(lib, \"librdkafka.lib\")\n#include \"win32/win32_config.h\"\n#else\n#include \"config.h\"\n#endif\n\n#if ENABLE_AVRO\n#include <libserdes/serdes.h>\n#endif\n\n#ifdef RD_KAFKA_V_HEADER\n#define HAVE_HEADERS 1\n#else\n#define HAVE_HEADERS 0\n#endif\n\n#if RD_KAFKA_VERSION >= 0x000b0500\n#define HAVE_CONTROLLERID 1\n#else\n#define HAVE_CONTROLLERID 0\n#endif\n\n#if RD_KAFKA_VERSION >= 0x01030000\n#define ENABLE_MOCK 1\n#endif\n\n\ntypedef enum {\n        KC_FMT_STR,\n        KC_FMT_OFFSET,\n        KC_FMT_KEY,\n        KC_FMT_KEY_LEN,\n        KC_FMT_PAYLOAD,\n        KC_FMT_PAYLOAD_LEN,\n        KC_FMT_PAYLOAD_LEN_BINARY,\n        KC_FMT_TOPIC,\n        KC_FMT_PARTITION,\n        KC_FMT_TIMESTAMP,\n        KC_FMT_HEADERS\n} fmt_type_t;\n\n#define KC_FMT_MAX_SIZE  128\n\ntypedef enum {\n        KC_MSG_FIELD_VALUE,\n        KC_MSG_FIELD_KEY,\n        KC_MSG_FIELD_CNT\n} kc_msg_field_t;\n\nstruct conf {\n        int     run;\n        int     verbosity;\n        int     exitcode;\n        int     exitonerror;\n        char    mode;\n        int     flags;\n#define CONF_F_FMT_JSON   0x1 /* JSON formatting */\n#define CONF_F_KEY_DELIM  0x2 /* Producer: use key delimiter */\n#define CONF_F_OFFSET     0x4 /* Print offsets */\n#define CONF_F_TEE        0x8 /* Tee output when producing */\n#define CONF_F_NULL       0x10 /* -Z: Send empty messages as NULL */\n#define CONF_F_LINE       0x20 /* Read files in line mode when producing */\n#define CONF_F_APIVERREQ  0x40 /* Enable api.version.request=true */\n#define CONF_F_APIVERREQ_USER 0x80 /* User set api.version.request */\n#define CONF_F_NO_CONF_SEARCH 0x100 /* Disable default config file search */\n#define CONF_F_BROKERS_SEEN   0x200 /* Brokers have been configured */\n#define CONF_F_FMT_AVRO_KEY   0x400 /* Convert key from Avro to JSON */\n#define CONF_F_FMT_AVRO_VALUE 0x800 /* Convert value from Avro to JSON  */\n#define CONF_F_SR_URL_SEEN    0x1000 /* schema.registry.url/-r seen */\n        char   *delim;\n        size_t  delim_size;\n        char   *key_delim;\n        size_t  key_delim_size;\n\n        struct {\n                fmt_type_t type;\n                const char *str;\n                int         str_len;\n        } fmt[KC_FMT_MAX_SIZE];\n        int     fmt_cnt;\n\n        /**< Producer: per-field pack-format, see pack()\n         *   Consumer: per-field unpack-format, see unpack(). */\n        const char *pack[KC_MSG_FIELD_CNT];\n\n        int     msg_size;\n        char   *brokers;\n        char   *topic;\n        int32_t partition;\n        rd_kafka_headers_t *headers;\n        char   *group;\n        char   *fixed_key;\n        int32_t fixed_key_len;\n        int64_t offset;\n#if RD_KAFKA_VERSION >= 0x00090300\n        int64_t startts;\n        int64_t stopts;\n#endif\n        int64_t msg_cnt;\n        int     exit_eof;\n        int     eof_cnt;  /**< Current number of partitions reached EOF */\n        rd_kafka_topic_partition_list_t *assignment; /**< Current -G consumer\n                                                      *   assignment */\n        int     metadata_timeout;\n        char   *null_str;\n        int     null_str_len;\n        int     txn;\n\n        rd_kafka_conf_t       *rk_conf;\n        rd_kafka_topic_conf_t *rkt_conf;\n\n        rd_kafka_t            *rk;\n        rd_kafka_topic_t      *rkt;\n\n        char   *debug;\n\n        int term_sig;  /**< Termination signal */\n\n#if ENABLE_AVRO\n        serdes_conf_t *srconf;\n        char   *schema_registry_url;\n#endif\n\n#if ENABLE_MOCK\n        struct {\n                int broker_cnt;\n        } mock;\n#endif\n};\n\nextern struct conf conf;\n\n\nvoid RD_NORETURN fatal0 (const char *func, int line,\n                         const char *fmt, ...);\n\nvoid error0 (int erroronexit, const char *func, int line,\n             const char *fmt, ...);\n\n#define KC_FATAL(.../*fmt*/)  fatal0(__FUNCTION__, __LINE__, __VA_ARGS__)\n\n#define KC_ERROR(.../*fmt*/)  error0(conf.exitonerror, __FUNCTION__, __LINE__, __VA_ARGS__)\n\n/* Info printout */\n#define KC_INFO(VERBLVL,.../*fmt*/) do {                        \\\n                if (conf.verbosity >= (VERBLVL))                \\\n                        fprintf(stderr, \"%% \" __VA_ARGS__);     \\\n        } while (0)\n\n\n\n/*\n * format.c\n */\nvoid pack_check (const char *what, const char *fmt);\n\nvoid fmt_msg_output (FILE *fp, const rd_kafka_message_t *rkmessage);\n\nvoid fmt_parse (const char *fmt);\n\nvoid fmt_init (void);\nvoid fmt_term (void);\n\n\n\n#if ENABLE_JSON\n/*\n * json.c\n */\nvoid fmt_msg_output_json (FILE *fp, const rd_kafka_message_t *rkmessage);\nvoid metadata_print_json (const struct rd_kafka_metadata *metadata,\n                          int32_t controllerid);\nvoid partition_list_print_json (const rd_kafka_topic_partition_list_t *parts,\n                                void *json_gen);\nvoid fmt_init_json (void);\nvoid fmt_term_json (void);\nint  json_can_emit_verbatim (void);\n#endif\n\n#if ENABLE_AVRO\n/*\n * avro.c\n */\nchar *kc_avro_to_json (const void *data, size_t data_len, int *schema_idp,\n                       char *errstr, size_t errstr_size);\n\nvoid kc_avro_init (const char *key_schema_name,\n                   const char *key_schema_path,\n                   const char *value_schema_name,\n                   const char *value_schema_path);\nvoid kc_avro_term (void);\n#endif\n\n\n/*\n * tools.c\n */\nint query_offsets_by_time (rd_kafka_topic_partition_list_t *offsets);\n"
        },
        {
          "name": "mklove",
          "type": "tree",
          "content": null
        },
        {
          "name": "rdendian.h",
          "type": "blob",
          "size": 4.849609375,
          "content": "/*\n * librdkafka - Apache Kafka C library\n *\n * Copyright (c) 2012-2015 Magnus Edenhill\n * All rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n *\n * 1. Redistributions of source code must retain the above copyright notice,\n *    this list of conditions and the following disclaimer.\n * 2. Redistributions in binary form must reproduce the above copyright notice,\n *    this list of conditions and the following disclaimer in the documentation\n *    and/or other materials provided with the distribution.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE\n * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n * POSSIBILITY OF SUCH DAMAGE.\n */\n#ifndef _RDENDIAN_H_\n#define _RDENDIAN_H_\n\n/**\n * Provides portable endian-swapping macros/functions.\n *\n *   be64toh()\n *   htobe64()\n *   be32toh()\n *   htobe32()\n *   be16toh()\n *   htobe16()\n *   le64toh()\n */\n\n#ifdef __FreeBSD__\n#include <sys/endian.h>\n#elif defined __GLIBC__\n#include <endian.h>\n#ifndef be64toh\n/* Support older glibc (<2.9) which lack be64toh */\n#include <byteswap.h>\n#if __BYTE_ORDER == __BIG_ENDIAN\n#define be16toh(x) (x)\n#define be32toh(x) (x)\n#define be64toh(x) (x)\n#define le64toh(x) __bswap_64 (x)\n#define le32toh(x) __bswap_32 (x)\n#else\n#define be16toh(x) __bswap_16 (x)\n#define be32toh(x) __bswap_32 (x)\n#define be64toh(x) __bswap_64 (x)\n#define le64toh(x) (x)\n#define le32toh(x) (x)\n#endif\n#endif\n\n#elif defined __CYGWIN__\n#include <endian.h>\n#elif defined __BSD__\n#include <sys/endian.h>\n#elif defined __sun\n#include <sys/byteorder.h>\n#include <sys/isa_defs.h>\n#define __LITTLE_ENDIAN 1234\n#define __BIG_ENDIAN 4321\n#ifdef _BIG_ENDIAN\n#define __BYTE_ORDER __BIG_ENDIAN\n#define be64toh(x) (x)\n#define be32toh(x) (x)\n#define be16toh(x) (x)\n#define le16toh(x) ((uint16_t)BSWAP_16(x))\n#define le32toh(x) BSWAP_32(x)\n#define le64toh(x) BSWAP_64(x)\n# else\n#define __BYTE_ORDER __LITTLE_ENDIAN\n#define be64toh(x) BSWAP_64(x)\n#define be32toh(x) ntohl(x)\n#define be16toh(x) ntohs(x)\n#define le16toh(x) (x)\n#define le32toh(x) (x)\n#define le64toh(x) (x)\n#define htole16(x) (x)\n#define htole64(x) (x)\n#endif /* __sun */\n\n#elif defined __APPLE__\n#include <machine/endian.h>\n#include <libkern/OSByteOrder.h>\n#if __DARWIN_BYTE_ORDER == __DARWIN_BIG_ENDIAN\n#define be64toh(x) (x)\n#define be32toh(x) (x)\n#define be16toh(x) (x)\n#define le16toh(x) OSSwapInt16(x)\n#define le32toh(x) OSSwapInt32(x)\n#define le64toh(x) OSSwapInt64(x)\n#else\n#define be64toh(x) OSSwapInt64(x)\n#define be32toh(x) OSSwapInt32(x)\n#define be16toh(x) OSSwapInt16(x)\n#define le16toh(x) (x)\n#define le32toh(x) (x)\n#define le64toh(x) (x)\n#endif\n\n#elif defined(_MSC_VER)\n#include <intrin.h>\n\n#define be64toh(x) _byteswap_uint64(x)\n#define be32toh(x) _byteswap_ulong(x)\n#define be16toh(x) _byteswap_ushort(x)\n#define le16toh(x) (x)\n#define le32toh(x) (x)\n#define le64toh(x) (x)\n\n#elif defined _AIX      /* AIX is always big endian */\n#define be64toh(x) (x)\n#define be32toh(x) (x)\n#define be16toh(x) (x)\n#define le32toh(x)                              \\\n        ((((x) & 0xff) << 24) |                 \\\n         (((x) & 0xff00) << 8) |                \\\n         (((x) & 0xff0000) >> 8) |              \\\n         (((x) & 0xff000000) >> 24))\n#define le64toh(x)                              \\\n        ((((x) & 0x00000000000000ffL) << 56) |  \\\n         (((x) & 0x000000000000ff00L) << 40) |  \\\n         (((x) & 0x0000000000ff0000L) << 24) |  \\\n         (((x) & 0x00000000ff000000L) << 8)  |  \\\n         (((x) & 0x000000ff00000000L) >> 8)  |  \\\n         (((x) & 0x0000ff0000000000L) >> 24) |  \\\n         (((x) & 0x00ff000000000000L) >> 40) |  \\\n         (((x) & 0xff00000000000000L) >> 56))\n#else\n#include <endian.h>\n#endif\n\n\n\n/*\n * On Solaris, be64toh is a function, not a macro, so there's no need to error\n * if it's not defined.\n */\n#if !defined(__sun) && !defined(be64toh)\n#error Missing definition for be64toh\n#endif\n\n#ifndef be32toh\n#define be32toh(x) ntohl(x)\n#endif\n\n#ifndef be16toh\n#define be16toh(x) ntohs(x)\n#endif\n\n#ifndef htobe64\n#define htobe64(x) be64toh(x)\n#endif\n#ifndef htobe32\n#define htobe32(x) be32toh(x)\n#endif\n#ifndef htobe16\n#define htobe16(x) be16toh(x)\n#endif\n\n#ifndef htole32\n#define htole32(x) le32toh(x)\n#endif\n\n#endif /* _RDENDIAN_H_ */\n"
        },
        {
          "name": "rdport.h",
          "type": "blob",
          "size": 2.572265625,
          "content": "/*\n * librdkafka - Apache Kafka C library\n *\n * Copyright (c) 2012-2019 Magnus Edenhill\n * All rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n *\n * 1. Redistributions of source code must retain the above copyright notice,\n *    this list of conditions and the following disclaimer.\n * 2. Redistributions in binary form must reproduce the above copyright notice,\n *    this list of conditions and the following disclaimer in the documentation\n *    and/or other materials provided with the distribution.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE\n * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n * POSSIBILITY OF SUCH DAMAGE.\n */\n\n#pragma once\n\n/**\n * Porting\n */\n\n#ifdef _MSC_VER\n/* Windows win32 native */\n\n#define WIN32_MEAN_AND_LEAN\n#include <Windows.h>\n\n#define RD_NORETURN\n#define RD_UNUSED\n\n/* MSVC loves prefixing POSIX functions with underscore */\n#define _COMPAT(FUNC)  _ ## FUNC\n\n#define STDIN_FILENO 0\n\ntypedef SSIZE_T ssize_t;\n\nssize_t getdelim (char **bufptr, size_t *n,\n                  int delim, FILE *fp);\n\n\n/**\n * @brief gettimeofday() for win32\n */\nstatic RD_UNUSED\nint rd_gettimeofday (struct timeval *tv, struct timezone *tz) {\n        SYSTEMTIME st;\n        FILETIME   ft;\n        ULARGE_INTEGER d;\n\n        GetSystemTime(&st);\n        SystemTimeToFileTime(&st, &ft);\n        d.HighPart = ft.dwHighDateTime;\n        d.LowPart = ft.dwLowDateTime;\n        tv->tv_sec = (long)((d.QuadPart - 116444736000000000llu) / 10000000L);\n        tv->tv_usec = (long)(st.wMilliseconds * 1000);\n\n        return 0;\n}\n\n\n#else\n/* POSIX */\n\n#define RD_NORETURN __attribute__((noreturn))\n#define RD_UNUSED __attribute__((unused))\n\n#define _COMPAT(FUNC) FUNC\n\n#define rd_gettimeofday(tv,tz) gettimeofday(tv,tz)\n#endif\n\n\n#ifndef MAX\n#define MAX(A,B) ((A) > (B) ? (A) : (B))\n#endif\n\n#ifndef MIN\n#define MIN(A,B) ((A) < (B) ? (A) : (B))\n#endif\n"
        },
        {
          "name": "resources",
          "type": "tree",
          "content": null
        },
        {
          "name": "rpm",
          "type": "tree",
          "content": null
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "tools.c",
          "type": "blob",
          "size": 2.9140625,
          "content": "/*\n * kcat - Apache Kafka consumer and producer\n *\n * Copyright (c) 2016, Magnus Edenhill\n * All rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n *\n * 1. Redistributions of source code must retain the above copyright notice,\n *    this list of conditions and the following disclaimer.\n * 2. Redistributions in binary form must reproduce the above copyright notice,\n *    this list of conditions and the following disclaimer in the documentation\n *    and/or other materials provided with the distribution.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE\n * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n * POSSIBILITY OF SUCH DAMAGE.\n */\n\n#include \"kcat.h\"\n\nvoid partition_list_print (rd_kafka_topic_partition_list_t *parts,\n                           void *json_gen) {\n        int i;\n\n        /* Sort by topic+partition */\n        rd_kafka_topic_partition_list_sort(parts, NULL, NULL);\n\n#if ENABLE_JSON\n        if (conf.flags & CONF_F_FMT_JSON) {\n                partition_list_print_json(parts, json_gen);\n                return;\n        }\n#endif\n\n        for (i = 0 ; i < parts->cnt ; i++) {\n                const rd_kafka_topic_partition_t *p = &parts->elems[i];\n                printf(\"%s [%\"PRId32\"] offset %\"PRId64\"%s\",\n                       p->topic, p->partition, p->offset,\n                       !p->err ? \"\\n\": \"\");\n                if (p->err)\n                        printf(\": %s\\n\", rd_kafka_err2str(p->err));\n        }\n}\n\nint query_offsets_by_time (rd_kafka_topic_partition_list_t *offsets) {\n        rd_kafka_resp_err_t err;\n#if RD_KAFKA_VERSION >= 0x00090300\n        char errstr[512];\n\n        if (!(conf.rk = rd_kafka_new(RD_KAFKA_PRODUCER, conf.rk_conf,\n                                     errstr, sizeof(errstr))))\n                KC_FATAL(\"Failed to create producer: %s\", errstr);\n\n        err = rd_kafka_offsets_for_times(conf.rk, offsets,\n                                         conf.metadata_timeout * 1000);\n#else\n        err = RD_KAFKA_RESP_ERR__NOT_IMPLEMENTED;\n#endif\n        if (err)\n                KC_FATAL(\"offsets_for_times failed: %s\", rd_kafka_err2str(err));\n\n        partition_list_print(offsets, NULL);\n\n        rd_kafka_destroy(conf.rk);\n\n        return 0;\n}\n"
        },
        {
          "name": "win32",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}