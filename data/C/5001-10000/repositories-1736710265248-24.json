{
  "metadata": {
    "timestamp": 1736710265248,
    "page": 24,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjMw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "torch/torch7",
      "stars": 9009,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.0068359375,
          "content": "build/\n"
        },
        {
          "name": ".travis.yml",
          "type": "blob",
          "size": 1.7548828125,
          "content": "language: c\ncompiler:\n  - gcc\n  - clang\ncache:\n  directories:\n  - $HOME/OpenBlasInstall\nsudo: false\nenv:\n  - TORCH_LUA_VERSION=LUAJIT21\n  - TORCH_LUA_VERSION=LUA51\n  - TORCH_LUA_VERSION=LUA52\nos:\n  - linux\naddons:\n  apt:\n    packages:\n    - cmake\n    - gfortran\n    - gcc-multilib\n    - gfortran-multilib\n    - liblapack-dev\n    - build-essential\n    - gcc\n    - g++\n    - curl\n    - cmake\n    - libreadline-dev\n    - git-core\n    - libqt4-core\n    - libqt4-gui\n    - libqt4-dev\n    - libjpeg-dev\n    - libpng-dev\n    - ncurses-dev\n    - imagemagick\n    - libzmq3-dev\n    - gfortran\n    - unzip\n    - gnuplot\n    - gnuplot-x11\nbefore_script:\n- export ROOT_TRAVIS_DIR=$(pwd)\n- export INSTALL_PREFIX=~/torch/install\n-  ls $HOME/OpenBlasInstall/lib || (cd /tmp/ && git clone https://github.com/xianyi/OpenBLAS.git -b master && cd OpenBLAS && (make NO_AFFINITY=1 -j$(getconf _NPROCESSORS_ONLN) 2>/dev/null >/dev/null) && make PREFIX=$HOME/OpenBlasInstall install)\n- git clone https://github.com/torch/distro.git ~/torch --recursive\n- cd ~/torch && git submodule update --init --recursive\n- mkdir build && cd build\n- export CMAKE_LIBRARY_PATH=$HOME/OpenBlasInstall/include:$HOME/OpenBlasInstall/lib:$CMAKE_LIBRARY_PATH\n- cmake .. -DCMAKE_INSTALL_PREFIX=\"${INSTALL_PREFIX}\" -DCMAKE_BUILD_TYPE=Release -DWITH_${TORCH_LUA_VERSION}=ON\n- make && make install\n- cd $ROOT_TRAVIS_DIR\n- export LD_LIBRARY_PATH=${INSTALL_PREFIX}/lib:$LD_LIBRARY_PATH\nscript:\n- ${INSTALL_PREFIX}/bin/luarocks make rocks/torch-scm-1.rockspec\n- ${INSTALL_PREFIX}/bin/luarocks install luaffi\n- export PATH=${INSTALL_PREFIX}/bin:$PATH\n- export TESTLUA=$(which luajit lua | head -n 1)\n- ${TESTLUA} -ltorch -e \"t=torch.test(); if t.errors[1] then os.exit(1) end\"\n- cd test\n- ${TESTLUA} test_writeObject.lua\n- ${TESTLUA} test_Tester.lua\n"
        },
        {
          "name": "CMakeLists.txt",
          "type": "blob",
          "size": 3.052734375,
          "content": "IF(APPLE)\n  CMAKE_MINIMUM_REQUIRED(VERSION 2.8.12 FATAL_ERROR)\n  CMAKE_POLICY(VERSION 2.8.12)\nELSE()\n  CMAKE_MINIMUM_REQUIRED(VERSION 2.8 FATAL_ERROR)\n  CMAKE_POLICY(VERSION 2.8)\nENDIF()\n\nSET(CMAKE_MODULE_PATH\n  \"${CMAKE_CURRENT_SOURCE_DIR}/cmake\"\n  \"${CMAKE_MODULE_PATH}\")\n\nIF (NOT MSVC)\n  IF (MINGW)\n    SET(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} -Werror=format\")\n  ELSE()\n    SET(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} -Werror=implicit-function-declaration -Werror=format\")\n  ENDIF(MINGW)\nENDIF(NOT MSVC)\n\n# Flags\n# When using MSVC\nIF(MSVC)\n  # we want to respect the standard, and we are bored of those **** .\n  ADD_DEFINITIONS(-D_CRT_SECURE_NO_DEPRECATE=1)\nENDIF(MSVC)\n\n# OpenMP support?\nSET(WITH_OPENMP ON CACHE BOOL \"OpenMP support if available?\")\nIF (APPLE AND CMAKE_COMPILER_IS_GNUCC)\n  EXEC_PROGRAM (uname ARGS -v  OUTPUT_VARIABLE DARWIN_VERSION)\n  STRING (REGEX MATCH \"[0-9]+\" DARWIN_VERSION ${DARWIN_VERSION})\n  MESSAGE (STATUS \"MAC OS Darwin Version: ${DARWIN_VERSION}\")\n  IF (DARWIN_VERSION GREATER 9)\n    SET(APPLE_OPENMP_SUCKS 1)\n  ENDIF (DARWIN_VERSION GREATER 9)\n  EXECUTE_PROCESS (COMMAND ${CMAKE_C_COMPILER} -dumpversion\n    OUTPUT_VARIABLE GCC_VERSION)\n  IF (APPLE_OPENMP_SUCKS AND GCC_VERSION VERSION_LESS 4.6.2)\n    MESSAGE(STATUS \"Warning: Disabling OpenMP (unstable with this version of GCC)\")\n    MESSAGE(STATUS \" Install GCC >= 4.6.2 or change your OS to enable OpenMP\")\n    SET(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} -Wno-unknown-pragmas\")\n    SET(WITH_OPENMP OFF CACHE BOOL \"OpenMP support if available?\" FORCE)\n  ENDIF ()\nENDIF ()\n\nIF (WITH_OPENMP)\n  FIND_PACKAGE(OpenMP)\n  IF(OPENMP_FOUND)\n    MESSAGE(STATUS \"Compiling with OpenMP support\")\n    SET(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} ${OpenMP_C_FLAGS}\")\n    SET(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} ${OpenMP_CXX_FLAGS}\")\n    SET(CMAKE_EXE_LINKER_FLAGS \"${CMAKE_EXE_LINKER_FLAGS} ${OpenMP_EXE_LINKER_FLAGS}\")\n  ENDIF(OPENMP_FOUND)\nENDIF (WITH_OPENMP)\n\n# Includes\nINCLUDE(TorchPaths)\nINCLUDE(TorchPathsInit)\nINCLUDE(TorchPackage)\nINCLUDE(TorchWrap)\nINCLUDE(TorchExports)\n\n# Torch libraries\nADD_SUBDIRECTORY(lib)\n\nCONFIGURE_FILE(paths.lua.in \"${CMAKE_CURRENT_BINARY_DIR}/paths.lua\")\n\nINCLUDE_DIRECTORIES(BEFORE \"${LUA_INCDIR}\")\nINCLUDE_DIRECTORIES(BEFORE \"${CMAKE_CURRENT_SOURCE_DIR}/lib/TH\")\nINCLUDE_DIRECTORIES(BEFORE \"${CMAKE_CURRENT_BINARY_DIR}/lib/TH\")\nINCLUDE_DIRECTORIES(BEFORE \"${CMAKE_CURRENT_SOURCE_DIR}/lib/luaT\")\nLINK_DIRECTORIES(\"${LUA_LIBDIR}\")\n\nSET(src DiskFile.c File.c MemoryFile.c PipeFile.c Storage.c Tensor.c Timer.c utils.c init.c TensorOperator.c TensorMath.c random.c Generator.c)\nSET(luasrc init.lua File.lua Tensor.lua CmdLine.lua FFInterface.lua Tester.lua TestSuite.lua ${CMAKE_CURRENT_BINARY_DIR}/paths.lua test/test.lua)\n\n# Necessary do generate wrapper\nADD_TORCH_WRAP(tensormathwrap TensorMath.lua)\nADD_TORCH_WRAP(randomwrap random.lua)\n\nADD_TORCH_PACKAGE(torch \"${src}\" \"${luasrc}\")\n\nTARGET_LINK_LIBRARIES(torch luaT TH)\n\nIF(LUALIB)\n  TARGET_LINK_LIBRARIES(torch ${LUALIB})\nENDIF()\n\nINSTALL(FILES \"README.md\" DESTINATION \"${Torch_INSTALL_LUA_PATH_SUBDIR}/torch\")\nINSTALL(DIRECTORY \"doc\" DESTINATION \"${Torch_INSTALL_LUA_PATH_SUBDIR}/torch\")\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 4.8876953125,
          "content": "# Contributing to Torch7 Core (torch7, nn, cutorch, cunn)\n\nThanks a lot! There are plenty of ways you can help!\n\nPlease take a moment to review this document in order to make the contribution\nprocess easy and effective for everyone involved.\n\nFollowing these guidelines helps to communicate that you respect the time of\nthe developers managing and developing this open source project. In return,\nthey should reciprocate that respect in addressing your issue or assessing\npatches and features.\n\n\n## Using the issue tracker\n\nThe [issue tracker](https://github.com/torch/torch7/issues) is\nthe preferred channel for [bug reports](#bugs), [features requests](#features)\nand [submitting pull requests](#pull-requests), but please respect the following\nrestrictions:\n\n* Please **do not** use the issue tracker for personal support requests (use\n  [mailing-list](https://groups.google.com/forum/#!forum/torch7)).\n\n* Please **do not** open issues regarding the code in a torch package \n  outside the core. For example don't open issues about the \n  REPL in the torch7 issue tracker, use the trepl issue tracker for that.\n\n<a name=\"bugs\"></a>\n## Bug reports\n\nA bug is a _demonstrable problem_ that is caused by the code in the repository.\nGood bug reports are extremely helpful - thank you!\n\nGuidelines for bug reports:\n\n1. **Use the GitHub issue search** &mdash; check if the issue has already been\n   reported.\n\n2. **Check if the issue has been fixed** &mdash; try to reproduce it using the\n   latest `master` or development branch in the repository.\n\n3. **Isolate the problem** &mdash; ideally create test case that is within reason,\n   preferably within 100 lines of code.\n\nA good bug report shouldn't leave others needing to chase you up for more\ninformation. Please try to be as detailed as possible in your report. What is\nyour environment? What steps will reproduce the issue? What OS do you\nexperience the problem? What would you expect to be the outcome? All these\ndetails will help people to fix any potential bugs.\n\n<a name=\"features\"></a>\n## Feature requests\n\nFeature requests are welcome to be filed. Torch is community-developed, \nthe maintainers are not exclusive torch developers, so keep that in mind.\nThe purpose of feature requests is for others who are looking to implement\na feature are aware of the interest in the feature.\n\n\n<a name=\"pull-requests\"></a>\n## Pull requests\n\nGood pull requests - patches, improvements, new features - are a fantastic\nhelp. They should remain focused in scope **and avoid containing unrelated\ncommits.**\n\n**Please ask first** before embarking on any significant pull request (e.g.\nimplementing features, refactoring code, porting to a different language),\notherwise you risk spending a lot of time working on something that the\nproject's developers might not want to merge into the project.\n\nPlease adhere to the coding conventions used throughout a project (indentation,\naccurate comments, etc.) and any other requirements (such as test coverage).\n\nAdhering to the following this process is the best way to get your work\nincluded in the project:\n\n1. [Fork](https://help.github.com/articles/fork-a-repo) the project, clone your\n   fork, and configure the remotes:\n\n   ```bash\n   # Clone your fork of the repo into the current directory\n   git clone https://github.com/<your-username>/torch7.git\n   # Navigate to the newly cloned directory\n   cd torch7\n   # Assign the original repo to a remote called \"upstream\"\n   git remote add upstream https://github.com/torch/torch7.git\n   ```\n\n2. If you cloned a while ago, get the latest changes from upstream:\n\n   ```bash\n   git checkout master\n   git pull upstream master\n   ```\n\n3. Create a new topic branch (off the main project development branch) to\n   contain your feature, change, or fix:\n\n   ```bash\n   git checkout -b <topic-branch-name>\n   ```\n\n4. Commit your changes in logical chunks. Please try to adhere to these [git commit\n   message guidelines](http://tbaggery.com/2008/04/19/a-note-about-git-commit-messages.html)\n   . Use Git's [interactive rebase](https://help.github.com/articles/about-git-rebase)\n   feature to tidy up your commits before making them public. This helps us keep the \n   commit history in logical blocks and clean, as torch grows. \n   For example: \n     - If you are adding a new function or a module, keep the module + tests + doc \n       to a single commit unless logically warranted. \n     - If you are fixing a bug, keep the bugfix to a single commit unless logically warranted.\n\n5. Locally merge (or rebase) the upstream development branch into your topic branch:\n\n   ```bash\n   git pull [--rebase] upstream master\n   ```\n\n6. Push your topic branch up to your fork:\n\n   ```bash\n   git push origin <topic-branch-name>\n   ```\n\n7. [Open a Pull Request](https://help.github.com/articles/using-pull-requests/)\n    with a clear title and description.\n\n**IMPORTANT**: By submitting a patch, you agree to allow the project owners to\nlicense your work under the terms of the BSD License.\n"
        },
        {
          "name": "COPYRIGHT.txt",
          "type": "blob",
          "size": 2.0009765625,
          "content": "Copyright (c) 2011-2014 Idiap Research Institute (Ronan Collobert)\nCopyright (c) 2012-2014 Deepmind Technologies (Koray Kavukcuoglu)\nCopyright (c) 2011-2012 NEC Laboratories America (Koray Kavukcuoglu)\nCopyright (c) 2011-2013 NYU (Clement Farabet)\nCopyright (c) 2006-2010 NEC Laboratories America (Ronan Collobert, Leon Bottou, Iain Melvin, Jason Weston)\nCopyright (c) 2006      Idiap Research Institute (Samy Bengio)\nCopyright (c) 2001-2004 Idiap Research Institute (Ronan Collobert, Samy Bengio, Johnny Mariethoz)\n\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n1. Redistributions of source code must retain the above copyright\n   notice, this list of conditions and the following disclaimer.\n\n2. Redistributions in binary form must reproduce the above copyright\n   notice, this list of conditions and the following disclaimer in the\n   documentation and/or other materials provided with the distribution.\n\n3. Neither the names of Deepmind Technologies, NYU, NEC Laboratories America \n   and IDIAP Research Institute nor the names of its contributors may be \n   used to endorse or promote products derived from this software without \n   specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\nARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE\nLIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\nCONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\nSUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\nINTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\nCONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\nARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\nPOSSIBILITY OF SUCH DAMAGE.\n"
        },
        {
          "name": "CmdLine.lua",
          "type": "blob",
          "size": 7.0458984375,
          "content": "local CmdLine = torch.class('torch.CmdLine')\n\nlocal function strip(str)\n   return string.match(str, '%-*(.*)')\nend\n\nlocal function pad(str, sz)\n   return str .. string.rep(' ', sz-#str)\nend\n\nfunction CmdLine:error(msg)\n   print('')\n   io.stderr:write(msg)\n   print('')\n   self:help()\n   os.exit(1)\nend\n\nfunction CmdLine:__readArgument__(params, arg, i, nArgument)\n   local argument = self.arguments[nArgument]\n   local value = arg[i]\n\n   if nArgument > #self.arguments then\n      self:error('invalid argument: ' .. value)\n   end\n   if argument.type and type(value) ~= argument.type then\n      self:error('invalid argument type for argument ' .. argument.key .. ' (should be ' .. argument.type .. ')')\n   end\n   params[strip(argument.key)] = value\n   return 1\nend\n\nfunction CmdLine:__readOption__(params, arg, i)\n   local key = arg[i]\n   local option = self.options[key]\n   if not option then\n      self:error('unknown option ' .. key)\n   end\n\n   if option.type and option.type == 'boolean' then\n      params[strip(key)] = not option.default\n      return 1\n   else\n      local value = arg[i+1]\n      if not value then\n         self:error('missing argument for option ' .. key)\n      end\n      if not option.type or option.type == 'string' then\n      elseif option.type == 'number' then\n         value = tonumber(value)\n      else\n         self:error('unknown required option type ' .. option.type)\n      end\n      if not value then\n         self:error('invalid type for option ' .. key .. ' (should be ' .. option.type .. ')')\n      end\n      params[strip(key)] = value\n      return 2\n   end\nend\n\nfunction CmdLine:__init(argseparator_,keyseparator_)\n   self.argseparator = argseparator_ or ','\n   self.keyseparator = keyseparator_ or '='\n   self.options = {}\n   self.arguments = {}\n   self.helplines = {}\n   self.dateformat = nil\n   self.silentio = false\nend\n\nfunction CmdLine:silent()\n   self.silentio = true\nend\n\nfunction CmdLine:addTime(name, format)\n   format = format or '%Y-%m-%d %H:%M:%S'\n   if type(format) ~= 'string' then\n      error('Argument has to be string')\n   end\n   if name ~= nil then\n      name = '[' .. name .. ']: '\n   else\n      name = ''\n   end\n   self.dateformat = format .. name\nend\n\n\nfunction CmdLine:argument(key, help, _type_)\n   table.insert(self.arguments, {key=key, help=help, type=_type_})\n   table.insert(self.helplines, self.arguments[#self.arguments])\nend\n\nfunction CmdLine:option(key, default, help, _type_)\n   if default == nil then\n      error('option ' .. key .. ' has no default value')\n   end\n   _type_ = _type_ or type(default)\n   if type(default) ~= _type_ then\n      error('option ' .. key .. ' has wrong default type value')\n   end\n   self.options[key] = {key=key, default=default, help=help, type=_type_}\n   table.insert(self.helplines, self.options[key])\nend\n\nfunction CmdLine:default()\n   local params = {}\n   for option,v in pairs(self.options) do\n      params[strip(option)] = v.default\n   end\n   return params\nend\n\nfunction CmdLine:parse(arg)\n   local i = 1\n   local params = self:default()\n\n   local nArgument = 0\n\n   while i <= #arg do\n      if arg[i] == '-help' or arg[i] == '-h' or arg[i] == '--help' then\n         self:help(arg)\n         os.exit(0)\n      end\n\n      if self.options[arg[i]] then\n         i = i + self:__readOption__(params, arg, i)\n      else\n         nArgument = nArgument + 1\n         i = i + self:__readArgument__(params, arg, i, nArgument)\n      end\n   end\n\n   if nArgument ~= #self.arguments then\n      self:error('not enough arguments')\n   end\n\n   return params\nend\n\nfunction CmdLine:string(prefix, params, ignore)\n   local arguments = {}\n   local options = {}\n   prefix = prefix or ''\n\n   for k,v in pairs(params) do\n      if ignore[k] then\n         print('-- ignore option ' .. k)\n      elseif self.options['-' .. k] then\n         if v ~= self.options['-' .. k].default or ignore[k] == false then\n            if type(v) == 'boolean' then\n               if v then\n                  v = 't'\n               else\n                  v = 'f'\n               end\n            end\n            table.insert(options, k .. self.keyseparator .. v)\n            print(k,v,self.options['-' .. k].default)\n        end\n       else\n         local narg\n         for i=1,#self.arguments do\n            if strip(self.arguments[i].key) == k then\n               narg = i\n            end\n         end\n         if narg then\n            arguments[narg] = k .. self.keyseparator .. v\n         else\n            print('WARNING: unknown option/argument: ' .. k .. ' IGNORING for DIRECTORY NAME')\n         end\n      end\n   end\n   table.sort(options)\n   local str = table.concat(arguments, self.argseparator)\n   if str == '' then\n      str = table.concat(options, self.argseparator)\n   else\n      str = str .. self.argseparator .. table.concat(options, self.argseparator)\n   end\n   if str == '' then\n      return prefix\n   else\n      return prefix .. self.argseparator .. str\n   end\nend\n\nlocal oprint = nil\nfunction CmdLine:log(file, params)\n   local f = (io.type(file) == 'file' and file) or io.open(file, 'w')\n   oprint = oprint or print -- get the current print function lazily\n   function print(...)\n      local n = select(\"#\", ...)\n      local arg = {...}\n      if not self.silentio then\n\t oprint(...)\n      end\n      local str = {}\n      if self.dateformat then\n\t table.insert(str, os.date(self.dateformat))\n      end\n      for i=1,n do\n\t table.insert(str,tostring(arg[i]))\n      end\n      table.insert(str,'\\n')\n      f:write(table.concat(str,' '))\n      f:flush()\n   end\n   print('[program started on ' .. os.date() .. ']')\n   print('[command line arguments]')\n   if params then\n      for k,v in pairs(params) do\n         print(k,v)\n      end\n   end\n   print('[----------------------]')\nend\n\nfunction CmdLine:text(txt)\n   txt = txt or ''\n   assert(type(txt) == 'string')\n   table.insert(self.helplines, txt)\nend\n\nfunction CmdLine:help(arg)\n   io.write('Usage: ')\n   if arg then io.write(arg[0] .. ' ') end\n   io.write('[options]')\n   for i=1,#self.arguments do\n      io.write(' <' .. strip(self.arguments[i].key) .. '>')\n   end\n   io.write('\\n')\n\n   -- first pass to compute max length\n   local optsz = 0\n   for _,option in ipairs(self.helplines) do\n      if type(option) == 'table' then\n         if option.default ~= nil then -- it is an option\n            if #option.key > optsz then\n               optsz = #option.key\n            end\n         else -- it is an argument\n            if #strip(option.key)+2 > optsz then\n               optsz = #strip(option.key)+2\n            end\n         end\n      end\n   end\n\n   -- second pass to print\n   for _,option in ipairs(self.helplines) do\n      if type(option) == 'table' then\n         io.write('  ')\n         if option.default ~= nil then -- it is an option\n            io.write(pad(option.key, optsz))\n            if option.help then io.write(' ' .. option.help) end\n            io.write(' [' .. tostring(option.default) .. ']')\n         else -- it is an argument\n            io.write(pad('<' .. strip(option.key) .. '>', optsz))\n            if option.help then io.write(' ' .. option.help) end\n         end\n      else\n         io.write(option) -- just some additional help\n      end\n      io.write('\\n')\n   end\nend\n"
        },
        {
          "name": "DiskFile.c",
          "type": "blob",
          "size": 2.75,
          "content": "#include \"general.h\"\n\nstatic int torch_DiskFile_new(lua_State *L)\n{\n  const char *name = luaL_checkstring(L, 1);\n  const char *mode = luaL_optstring(L, 2, \"r\");\n  int isQuiet = luaT_optboolean(L, 3, 0);\n  THFile *self = THDiskFile_new(name, mode, isQuiet);\n\n  luaT_pushudata(L, self, \"torch.DiskFile\");\n  return 1;\n}\n\nstatic int torch_DiskFile_free(lua_State *L)\n{\n  THFile *self = luaT_checkudata(L, 1, \"torch.DiskFile\");\n  THFile_free(self);\n  return 0;\n}\n\nstatic int torch_DiskFile_isLittleEndianCPU(lua_State *L)\n{\n  lua_pushboolean(L, THDiskFile_isLittleEndianCPU());\n  return 1;\n}\n\nstatic int torch_DiskFile_isBigEndianCPU(lua_State *L)\n{\n  lua_pushboolean(L, !THDiskFile_isLittleEndianCPU());\n  return 1;\n}\n\nstatic int torch_DiskFile_nativeEndianEncoding(lua_State *L)\n{\n  THFile *self = luaT_checkudata(L, 1, \"torch.DiskFile\");\n  THDiskFile_nativeEndianEncoding(self);\n  lua_settop(L, 1);\n  return 1;\n}\n\nstatic int torch_DiskFile_littleEndianEncoding(lua_State *L)\n{\n  THFile *self = luaT_checkudata(L, 1, \"torch.DiskFile\");\n  THDiskFile_littleEndianEncoding(self);\n  lua_settop(L, 1);\n  return 1;\n}\n\nstatic int torch_DiskFile_bigEndianEncoding(lua_State *L)\n{\n  THFile *self = luaT_checkudata(L, 1, \"torch.DiskFile\");\n  THDiskFile_bigEndianEncoding(self);\n  lua_settop(L, 1);\n  return 1;\n}\n\nstatic int torch_DiskFile_longSize(lua_State *L)\n{\n  THFile *self = luaT_checkudata(L, 1, \"torch.DiskFile\");\n  THDiskFile_longSize(self, lua_tointeger(L, 2));\n  lua_settop(L, 1);\n  return 1;\n}\n\nstatic int torch_DiskFile_noBuffer(lua_State *L)\n{\n  THFile *self = luaT_checkudata(L, 1, \"torch.DiskFile\");\n  THDiskFile_noBuffer(self);\n  lua_settop(L, 1);\n  return 1;\n}\n\nstatic int torch_DiskFile___tostring__(lua_State *L)\n{\n  THFile *self = luaT_checkudata(L, 1, \"torch.DiskFile\");\n  lua_pushfstring(L, \"torch.DiskFile on <%s> [status: %s -- mode %c%c]\",\n                  THDiskFile_name(self),\n                  (THFile_isOpened(self) ? \"open\" : \"closed\"),\n                  (THFile_isReadable(self) ? 'r' : ' '),\n                  (THFile_isWritable(self) ? 'w' : ' '));\n\n  return 1;\n}\nstatic const struct luaL_Reg torch_DiskFile__ [] = {\n  {\"isLittleEndianCPU\", torch_DiskFile_isLittleEndianCPU},\n  {\"isBigEndianCPU\", torch_DiskFile_isBigEndianCPU},\n  {\"nativeEndianEncoding\", torch_DiskFile_nativeEndianEncoding},\n  {\"littleEndianEncoding\", torch_DiskFile_littleEndianEncoding},\n  {\"bigEndianEncoding\", torch_DiskFile_bigEndianEncoding},\n  {\"longSize\", torch_DiskFile_longSize},\n  {\"noBuffer\", torch_DiskFile_noBuffer},\n  {\"__tostring__\", torch_DiskFile___tostring__},\n  {NULL, NULL}\n};\n\nvoid torch_DiskFile_init(lua_State *L)\n{\n  luaT_newmetatable(L, \"torch.DiskFile\", \"torch.File\",\n                    torch_DiskFile_new, torch_DiskFile_free, NULL);\n\n  luaT_setfuncs(L, torch_DiskFile__, 0);\n  lua_pop(L, 1);\n}\n"
        },
        {
          "name": "FFInterface.lua",
          "type": "blob",
          "size": 6.5498046875,
          "content": "-- if this causes issues, you may need to:\n-- luarocks remove --force ffi\n-- and follow instructions to install\n-- https://github.com/facebook/luaffifb\nlocal ok, ffi = pcall(require, 'ffi')\n\nlocal function checkArgument(condition, fn, ud, msg, level)\n   local level = level or 3\n   if not condition then\n      error(\"bad argument #\" .. ud .. \" to '\" .. fn .. \"' (\" .. msg .. \")\", level)\n   end\nend\n\nlocal function checkArgumentType(expected, actual, fn, ud, level)\n   local level = level or 3\n   if expected ~= actual then\n      checkArgument(false, fn, ud, expected .. \" expected, got \" .. actual, level + 1)\n   end\nend\n\nif ok then\n\n   local Real2real = {\n      Byte='unsigned char',\n      Char='char',\n      Short='short',\n      Int='int',\n      Long='long',\n      Float='float',\n      Double='double',\n      Half='THHalf'\n   }\n\n   -- Allocator\n   ffi.cdef[[\ntypedef struct THAllocator {\n  void* (*malloc)(void*, ptrdiff_t);\n  void* (*realloc)(void*, void*, ptrdiff_t);\n  void (*free)(void*, void*);\n} THAllocator;\n]]\n\n   -- Half\n   ffi.cdef[[\ntypedef struct {\n  unsigned short x;\n} __THHalf;\ntypedef __THHalf THHalf;\n]]\n\n   -- Storage\n   for Real, real in pairs(Real2real) do\n\n      local cdefs = [[\ntypedef struct THRealStorage\n{\n    real *data;\n    ptrdiff_t size;\n    int refcount;\n    char flag;\n    THAllocator *allocator;\n    void *allocatorContext;\n} THRealStorage;\n]]\n      cdefs = cdefs:gsub('Real', Real):gsub('real', real)\n      ffi.cdef(cdefs)\n\n      local Storage = torch.getmetatable(string.format('torch.%sStorage', Real))\n      local Storage_tt = ffi.typeof('TH' .. Real .. 'Storage**')\n\n      rawset(Storage,\n             \"cdata\",\n             function(self)\n                return Storage_tt(self)[0]\n             end)\n\n      rawset(Storage,\n             \"data\",\n             function(self)\n                return Storage_tt(self)[0].data\n             end)\n   end\n\n   -- Tensor\n   for Real, real in pairs(Real2real) do\n\n      local cdefs = [[\ntypedef struct THRealTensor\n{\n    long *size;\n    long *stride;\n    int nDimension;\n\n    THRealStorage *storage;\n    ptrdiff_t storageOffset;\n    int refcount;\n\n    char flag;\n\n} THRealTensor;\n]]\n      cdefs = cdefs:gsub('Real', Real):gsub('real', real)\n      ffi.cdef(cdefs)\n\n      local Tensor_type = string.format('torch.%sTensor', Real)\n      local Tensor = torch.getmetatable(Tensor_type)\n      local Tensor_tt = ffi.typeof('TH' .. Real .. 'Tensor**')\n\n      rawset(Tensor,\n             \"cdata\",\n             function(self)\n                if not self then return nil; end\n                return Tensor_tt(self)[0]\n             end)\n\n      rawset(Tensor,\n             \"data\",\n             function(self)\n                if not self then return nil; end\n                self = Tensor_tt(self)[0]\n                return self.storage ~= nil and self.storage.data + self.storageOffset or nil\n             end)\n\n      -- faster apply (contiguous case)\n      if Tensor_type ~= 'torch.HalfTensor' then\n         local apply = Tensor.apply\n         rawset(Tensor,\n                \"apply\",\n                function(self, func)\n                   if self:isContiguous() and self.data then\n                      local self_d = self:data()\n                      for i=0,self:nElement()-1 do\n                         local res = func(tonumber(self_d[i])) -- tonumber() required for long...\n                         if res then\n                            self_d[i] = res\n                         end\n                      end\n                      return self\n                   else\n                      return apply(self, func)\n                   end\n                end)\n\n         -- faster map (contiguous case)\n         local map = Tensor.map\n         rawset(Tensor,\n                \"map\",\n                function(self, src, func)\n                   checkArgument(torch.isTensor(src), \"map\", 1, \"tensor expected\")\n                   checkArgumentType(self:type(), src:type(), \"map\", 1)\n\n                   if self:isContiguous() and src:isContiguous() and self.data and src.data then\n                      local self_d = self:data()\n                      local src_d = src:data()\n                      assert(src:nElement() == self:nElement(), 'size mismatch')\n                      for i=0,self:nElement()-1 do\n                         local res = func(tonumber(self_d[i]), tonumber(src_d[i])) -- tonumber() required for long...\n                         if res then\n                            self_d[i] = res\n                         end\n                      end\n                      return self\n                   else\n                      return map(self, src, func)\n                   end\n                end)\n\n         -- faster map2 (contiguous case)\n         local map2 = Tensor.map2\n         rawset(Tensor,\n                \"map2\",\n                function(self, src1, src2, func)\n                   checkArgument(torch.isTensor(src1), \"map\", 1, \"tensor expected\")\n                   checkArgument(torch.isTensor(src2), \"map\", 2, \"tensor expected\")\n                   checkArgumentType(self:type(), src1:type(), \"map\", 1)\n                   checkArgumentType(self:type(), src2:type(), \"map\", 2)\n\n                   if self:isContiguous() and src1:isContiguous() and src2:isContiguous() and self.data and src1.data and src2.data then\n                      local self_d = self:data()\n                     local src1_d = src1:data()\n                      local src2_d = src2:data()\n                      assert(src1:nElement() == self:nElement(), 'size mismatch')\n                      assert(src2:nElement() == self:nElement(), 'size mismatch')\n                      for i=0,self:nElement()-1 do\n                         local res = func(tonumber(self_d[i]), tonumber(src1_d[i]), tonumber(src2_d[i])) -- tonumber() required for long...\n                         if res then\n                            self_d[i] = res\n                         end\n                      end\n                      return self\n                   else\n                      return map2(self, src1, src2, func)\n                   end\n                end)\n             end\n   end\n\n   -- torch.data\n   -- will fail if :data() is not defined\n   function torch.data(self, asnumber)\n      if not self then return nil; end\n      local data = self:data()\n      if asnumber then\n         return ffi.cast('intptr_t', data)\n      else\n         return data\n      end\n   end\n\n   -- torch.cdata\n   -- will fail if :cdata() is not defined\n   function torch.cdata(self, asnumber)\n      if not self then return nil; end\n      local cdata = self:cdata()\n      if asnumber then\n         return ffi.cast('intptr_t', cdata)\n      else\n         return cdata\n      end\n   end\n\nend\n"
        },
        {
          "name": "File.c",
          "type": "blob",
          "size": 8.6552734375,
          "content": "#include \"general.h\"\n#include \"THFile.h\"\n#include \"luaT.h\"\n\n#define IMPLEMENT_TORCH_FILE_FLAG(NAME)                   \\\n  static int torch_File_##NAME(lua_State *L)              \\\n  {                                                       \\\n    THFile *self = luaT_checkudata(L, 1, \"torch.File\");  \\\n    lua_pushboolean(L, THFile_##NAME(self));              \\\n    return 1;                                             \\\n  }\n\nIMPLEMENT_TORCH_FILE_FLAG(isQuiet)\nIMPLEMENT_TORCH_FILE_FLAG(isReadable)\nIMPLEMENT_TORCH_FILE_FLAG(isWritable)\nIMPLEMENT_TORCH_FILE_FLAG(isBinary)\nIMPLEMENT_TORCH_FILE_FLAG(isAutoSpacing)\nIMPLEMENT_TORCH_FILE_FLAG(hasError)\n\n#define IMPLEMENT_TORCH_FILE_FUNC(NAME)                   \\\n  static int torch_File_##NAME(lua_State *L)              \\\n  {                                                       \\\n    THFile *self = luaT_checkudata(L, 1, \"torch.File\");  \\\n    THFile_##NAME(self);                                  \\\n    lua_settop(L, 1);                                     \\\n    return 1;                                             \\\n  }\n\nIMPLEMENT_TORCH_FILE_FUNC(binary)\nIMPLEMENT_TORCH_FILE_FUNC(ascii)\nIMPLEMENT_TORCH_FILE_FUNC(autoSpacing)\nIMPLEMENT_TORCH_FILE_FUNC(noAutoSpacing)\nIMPLEMENT_TORCH_FILE_FUNC(quiet)\nIMPLEMENT_TORCH_FILE_FUNC(pedantic)\nIMPLEMENT_TORCH_FILE_FUNC(clearError)\n\nIMPLEMENT_TORCH_FILE_FUNC(synchronize)\n\nstatic int torch_File_seek(lua_State *L)\n{\n  THFile *self = luaT_checkudata(L, 1, \"torch.File\");\n  ptrdiff_t position = luaL_checkinteger(L, 2)-1;\n  // >= 0 because it has 1 already subtracted\n  THArgCheck(position >= 0, 2, \"position has to be greater than 0!\");\n  THFile_seek(self, (size_t)position);\n  lua_settop(L, 1);\n  return 1;\n}\n\nIMPLEMENT_TORCH_FILE_FUNC(seekEnd)\n\nstatic int torch_File_position(lua_State *L)\n{\n  THFile *self = luaT_checkudata(L, 1, \"torch.File\");\n  lua_pushnumber(L, THFile_position(self)+1);\n  return 1;\n}\n\nIMPLEMENT_TORCH_FILE_FUNC(close)\n\n#define IMPLEMENT_TORCH_FILE_RW(TYPEC, TYPE)                            \\\n  static int torch_File_read##TYPEC(lua_State *L)                       \\\n  {                                                                     \\\n    THFile *self = luaT_checkudata(L, 1, \"torch.File\");                \\\n    int narg = lua_gettop(L);                                           \\\n                                                                        \\\n    if(narg == 1)                                                       \\\n    {                                                                   \\\n      lua_pushnumber(L, THFile_read##TYPEC##Scalar(self));              \\\n      return 1;                                                         \\\n    }                                                                   \\\n    else if(narg == 2)                                                  \\\n    {                                                                   \\\n      if(lua_isnumber(L, 2))                                            \\\n      {                                                                 \\\n        ptrdiff_t size = lua_tonumber(L, 2);                                 \\\n        ptrdiff_t nread;                                                     \\\n                                                                        \\\n        TH##TYPEC##Storage *storage = TH##TYPEC##Storage_newWithSize(size); \\\n        luaT_pushudata(L, storage, \"torch.\" #TYPEC \"Storage\");          \\\n        nread = THFile_read##TYPEC(self, storage);                      \\\n        if(nread != size)                                               \\\n          TH##TYPEC##Storage_resize(storage, nread);                    \\\n        return 1;                                                       \\\n      }                                                                 \\\n      else if(luaT_toudata(L, 2, \"torch.\" #TYPEC \"Storage\"))            \\\n      {                                                                 \\\n        TH##TYPEC##Storage *storage = luaT_toudata(L, 2, \"torch.\" #TYPEC \"Storage\"); \\\n        lua_pushnumber(L, THFile_read##TYPEC(self, storage));           \\\n        return 1;                                                       \\\n      }                                                                 \\\n    }                                                                   \\\n                                                                        \\\n    luaL_error(L, \"nothing, number, or \" #TYPEC \"Storage expected\");    \\\n    return 0;                                                           \\\n  }                                                                     \\\n                                                                        \\\n  static int torch_File_write##TYPEC(lua_State *L)                      \\\n  {                                                                     \\\n    THFile *self = luaT_checkudata(L, 1, \"torch.File\");                \\\n    int narg = lua_gettop(L);                                           \\\n                                                                        \\\n    if(narg == 2)                                                       \\\n    {                                                                   \\\n      if(lua_isnumber(L, 2))                                            \\\n      {                                                                 \\\n        TYPE value = lua_tonumber(L, 2);                                \\\n        THFile_write##TYPEC##Scalar(self, (TYPE)value);                 \\\n        return 0;                                                       \\\n      }                                                                 \\\n      else if(luaT_toudata(L, 2, \"torch.\" #TYPEC \"Storage\"))            \\\n      {                                                                 \\\n        TH##TYPEC##Storage *storage = luaT_toudata(L, 2, \"torch.\" #TYPEC \"Storage\"); \\\n        lua_pushnumber(L, THFile_write##TYPEC(self, storage));          \\\n        return 1;                                                       \\\n      }                                                                 \\\n    }                                                                   \\\n                                                                        \\\n    luaL_error(L, \"number, or \" #TYPEC \"Storage expected\");             \\\n    return 0;                                                           \\\n  }\n\n\nIMPLEMENT_TORCH_FILE_RW(Byte, unsigned char)\nIMPLEMENT_TORCH_FILE_RW(Char, char)\nIMPLEMENT_TORCH_FILE_RW(Short, short)\nIMPLEMENT_TORCH_FILE_RW(Int, int)\nIMPLEMENT_TORCH_FILE_RW(Long, long)\nIMPLEMENT_TORCH_FILE_RW(Float, float)\nIMPLEMENT_TORCH_FILE_RW(Double, double)\n\nstatic int torch_File_readString(lua_State *L)\n{\n  THFile *self = luaT_checkudata(L, 1, \"torch.File\");\n  const char *format = luaL_checkstring(L, 2);\n  char *str;\n  ptrdiff_t size;\n\n  size = THFile_readStringRaw(self, format, &str);\n  lua_pushlstring(L, str, size);\n  THFree(str);\n\n  return 1;\n}\n\nstatic int torch_File_writeString(lua_State *L)\n{\n  THFile *self = luaT_checkudata(L, 1, \"torch.File\");\n  const char *str = NULL;\n  size_t size;\n\n  luaL_checktype(L, 2, LUA_TSTRING);\n  str = lua_tolstring(L, 2, &size);\n  lua_pushnumber(L, THFile_writeStringRaw(self, str, size));\n  return 1;\n}\n\nstatic const struct luaL_Reg torch_File__ [] = {\n  {\"isQuiet\", torch_File_isQuiet},\n  {\"isReadable\", torch_File_isReadable},\n  {\"isWritable\", torch_File_isWritable},\n  {\"isBinary\", torch_File_isBinary},\n  {\"isAutoSpacing\", torch_File_isAutoSpacing},\n  {\"hasError\", torch_File_hasError},\n  {\"binary\", torch_File_binary},\n  {\"ascii\", torch_File_ascii},\n  {\"autoSpacing\", torch_File_autoSpacing},\n  {\"noAutoSpacing\", torch_File_noAutoSpacing},\n  {\"quiet\", torch_File_quiet},\n  {\"pedantic\", torch_File_pedantic},\n  {\"clearError\", torch_File_clearError},\n\n  /* DEBUG: CHECK DISK FREE & READ/WRITE STRING*/\n\n  {\"readByte\", torch_File_readByte},\n  {\"readChar\", torch_File_readChar},\n  {\"readShort\", torch_File_readShort},\n  {\"readInt\", torch_File_readInt},\n  {\"readLong\", torch_File_readLong},\n  {\"readFloat\", torch_File_readFloat},\n  {\"readDouble\", torch_File_readDouble},\n  {\"readString\", torch_File_readString},\n\n  {\"writeByte\", torch_File_writeByte},\n  {\"writeChar\", torch_File_writeChar},\n  {\"writeShort\", torch_File_writeShort},\n  {\"writeInt\", torch_File_writeInt},\n  {\"writeLong\", torch_File_writeLong},\n  {\"writeFloat\", torch_File_writeFloat},\n  {\"writeDouble\", torch_File_writeDouble},\n  {\"writeString\", torch_File_writeString},\n\n  {\"synchronize\", torch_File_synchronize},\n  {\"seek\", torch_File_seek},\n  {\"seekEnd\", torch_File_seekEnd},\n  {\"position\", torch_File_position},\n  {\"close\", torch_File_close},\n\n  {NULL, NULL}\n};\n\nvoid torch_File_init(lua_State *L)\n{\n  luaT_newmetatable(L, \"torch.File\", NULL, NULL, NULL, NULL);\n  luaT_setfuncs(L, torch_File__, 0);\n  lua_pop(L, 1);\n}\n"
        },
        {
          "name": "File.lua",
          "type": "blob",
          "size": 15.6611328125,
          "content": "local File = torch.getmetatable('torch.File')\n\nfunction File:writeBool(value)\n   if value then\n      self:writeInt(1)\n   else\n      self:writeInt(0)\n   end\nend\n\nfunction File:readBool()\n   return (self:readInt() == 1)\nend\n\nlocal TYPE_NIL      = 0\nlocal TYPE_NUMBER   = 1\nlocal TYPE_STRING   = 2\nlocal TYPE_TABLE    = 3\nlocal TYPE_TORCH    = 4\nlocal TYPE_BOOLEAN  = 5\nlocal TYPE_FUNCTION = 6\nlocal TYPE_RECUR_FUNCTION = 8\nlocal LEGACY_TYPE_RECUR_FUNCTION = 7\n\n-- Lua 5.2 compatibility\nlocal loadstring = loadstring or load\n\nfunction File:isWritableObject(object)\n   local typename = type(object)\n   local typeidx\n   if type(object) ~= 'boolean' and not object then\n      typeidx = TYPE_NIL\n   elseif torch.typename(object) and torch.factory(torch.typename(object)) then\n      typeidx = TYPE_TORCH\n   elseif typename == 'table' then\n      typeidx = TYPE_TABLE\n   elseif typename == 'number' then\n      typeidx = TYPE_NUMBER\n   elseif typename == 'string' then\n      typeidx = TYPE_STRING\n   elseif typename == 'boolean' then\n      typeidx = TYPE_BOOLEAN\n   elseif typename == 'function' and pcall(string.dump, object) then\n      typeidx = TYPE_RECUR_FUNCTION\n   end\n   return typeidx\nend\n\nfunction File:referenced(ref)\n   -- we use an environment to keep a record of written objects\n   if not torch.getenv(self).writeObjects then\n      torch.setenv(self, {\n            writeObjects={}, writeObjectsRef={},\n            readObjects={},\n            objectNameStack={},\n            upvalueRefToId={}, upvalueIdToClosure={},\n         })\n   end\n   local env = torch.getenv(self)\n   env.force = not ref\n   torch.setenv(self,env)\n   return self\nend\n\nfunction File:isReferenced()\n   -- if no environment, then no forcing setup yet\n   if not torch.getenv(self).writeObjects then\n      return true\n   end\n   local env = torch.getenv(self)\n   return not env.force\nend\n\nlocal function getmetamethod(obj, name)\n   local func\n   local status\n\n   -- check getmetatable(obj).__name or\n   -- check getmetatable(obj).name\n   status, func = pcall(\n      function()\n         -- note that sometimes the metatable is hidden\n         -- we get it for sure through the torch type system\n         local mt = torch.getmetatable(torch.typename(obj))\n         if mt then\n            return mt['__' .. name] or mt[name]\n         end\n      end\n   )\n   if status and type(func) == 'function' then\n      return func\n   end\nend\n\nlocal UPVALUES_TOKEN = {} -- unique object\nlocal function formatStack(objectNameStack)\n   -- Format object name stack skipping UPVALUES_TOKEN and upvalue index\n   local parts = {}\n   for i, v in ipairs(objectNameStack) do\n      if v ~= UPVALUES_TOKEN and objectNameStack[i-1] ~= UPVALUES_TOKEN then\n         table.insert(parts, v)\n      end\n   end\n   return table.concat(parts, '.')\nend\n\nfunction File:writeObject(object, debugname, hook)\n   -- define a default hook function if not provided\n   hook = hook or function(object) return object end\n   -- we use an environment to keep a record of written objects\n   if not torch.getenv(self).writeObjects then\n      torch.setenv(self, {\n            writeObjects={}, writeObjectsRef={},\n            readObjects={},\n            objectNameStack={},\n            upvalueRefToId={}, upvalueIdToClosure={},\n         })\n   end\n   -- That guy is used for references' book-keeping\n   local sobject = object\n   -- That guy is the object that is actually persisted\n   -- hook(object) can be used to modify the object before writing it to the file.\n   -- Useful for serializing objects under a config\n   -- that we want to deserialize safely under another config.\n   -- (e.g. Cuda to Float tensors, cudnn to nn, ...)\n   object = hook(object)\n   local force = torch.getenv(self).force\n\n   -- if nil object, only write the type and return\n   if type(object) ~= 'boolean' and not object then\n      self:writeInt(TYPE_NIL)\n      return\n   end\n\n   local objectNameStack = torch.getenv(self).objectNameStack\n   table.insert(objectNameStack, debugname or '<?>')\n\n   -- check the type we are dealing with\n   local typeidx = self:isWritableObject(object)\n   if not typeidx then\n      error(string.format('Unwritable object <%s> at %s', type(object), formatStack(objectNameStack)))\n   end\n   self:writeInt(typeidx)\n\n   if typeidx == TYPE_NUMBER then\n      self:writeDouble(object)\n   elseif typeidx == TYPE_BOOLEAN then\n      self:writeBool(object)\n   elseif typeidx == TYPE_STRING then\n      local stringStorage = torch.CharStorage():string(object)\n      self:writeInt(#stringStorage)\n      self:writeChar(stringStorage)\n   elseif typeidx == TYPE_TORCH or typeidx == TYPE_TABLE or  typeidx == TYPE_RECUR_FUNCTION then\n      -- check it exists already (we look at the pointer!)\n      local objects = torch.getenv(self).writeObjects\n      local objectsRef = torch.getenv(self).writeObjectsRef\n      local index = objects[torch.pointer(sobject)]\n\n      if index and (not force) then\n         -- if already exists, write only its index\n         self:writeInt(index)\n      else\n         -- else write the object itself\n         index = objects.nWriteObject or 0\n         index = index + 1\n         if not force then\n            objects[torch.pointer(sobject)] = index\n            objectsRef[object] = index -- we make sure the object is not going to disappear\n         end\n         self:writeInt(index)\n         objects.nWriteObject = index\n         if typeidx == TYPE_RECUR_FUNCTION then\n            local upvalueRefToId = torch.getenv(self).upvalueRefToId\n            -- Unique ID for each ref since lightuserdata are not serializable\n            local nextId = 1\n            for _ in pairs(upvalueRefToId) do nextId=nextId+1 end\n            local upvalues = {}\n            local counter = 0\n            while true do\n               counter = counter + 1\n               local name,value = debug.getupvalue(object, counter)\n               if not name then break end\n               if name == '_ENV' then value = nil end\n               local id=nil\n               -- debug.upvalueid exists only for lua>=5.2 and luajit\n               if debug.upvalueid then\n                  local upvalueRef = debug.upvalueid(object, counter)\n                  if not upvalueRefToId[upvalueRef] then\n                     upvalueRefToId[upvalueRef] = nextId\n                     nextId = nextId + 1\n                  end\n                  id = upvalueRefToId[upvalueRef]\n               end\n               table.insert(upvalues, {name=name, id=id, value=value})\n            end\n            local dumped = string.dump(object)\n            local stringStorage = torch.CharStorage():string(dumped)\n            self:writeInt(#stringStorage)\n            self:writeChar(stringStorage)\n            self:writeObject(upvalues, UPVALUES_TOKEN, hook)\n         elseif typeidx == TYPE_TORCH then\n            local version   = torch.CharStorage():string('V ' .. torch.version(object))\n            local className = torch.CharStorage():string(torch.typename(object))\n            self:writeInt(#version)\n            self:writeChar(version)\n            self:writeInt(#className)\n            self:writeChar(className)\n            local write = getmetamethod(object, 'write')\n            if write then\n               write(object, self)\n            elseif type(object) == 'table' then\n               local var = {}\n               for k,v in pairs(object) do\n                  if self:isWritableObject(v) then\n                     var[k] = v\n                  else\n                     print(string.format('$ Warning: cannot write object field <%s> of <%s> %s', k, torch.typename(object), formatStack(objectNameStack)))\n                  end\n               end\n               self:writeObject(var, torch.typename(object), hook)\n            else\n               error(string.format('<%s> is a non-serializable Torch object %s', torch.typename(object), formatStack(objectNameStack)))\n            end\n         else -- it is a table\n            local size = 0; for k,v in pairs(object) do size = size + 1 end\n            self:writeInt(size)\n            for k,v in pairs(object) do\n               self:writeObject(k, nil, hook)\n               local name = (type(k) == 'string' or type(k) == 'number') and tostring(k) or nil\n               -- special case name for upvalues\n               if objectNameStack[#objectNameStack-1] == UPVALUES_TOKEN and\n                  name == 'value' and type(object.name) == 'string' then\n                  name = object.name\n               end\n               self:writeObject(v, name, hook)\n            end\n         end\n      end\n   else\n      error('Unwritable object')\n   end\n   table.remove(objectNameStack)\nend\n\nfunction File:readObject()\n   -- we use an environment to keep a record of read objects\n   if not torch.getenv(self).writeObjects then\n      torch.setenv(self, {\n            writeObjects={}, writeObjectsRef={},\n            readObjects={},\n            objectNameStack={},\n            upvalueRefToId={}, upvalueIdToClosure={},\n         })\n   end\n\n   local force = torch.getenv(self).force\n\n   -- read the typeidx\n   local typeidx = self:readInt()\n\n   -- is it nil?\n   if typeidx == TYPE_NIL then\n      return nil\n   end\n\n   if typeidx == TYPE_NUMBER then\n      return self:readDouble()\n   elseif typeidx == TYPE_BOOLEAN then\n      return self:readBool()\n   elseif typeidx == TYPE_STRING then\n      local size = self:readInt()\n      return self:readChar(size):string()\n   elseif typeidx == TYPE_FUNCTION then\n       local size = self:readInt()\n       local dumped = self:readChar(size):string()\n       local func, err = loadstring(dumped)\n       if not func then\n          io.stderr:write(string.format('Warning: Failed to load function from bytecode: %s', err))\n       end\n       local upvalues = self:readObject()\n       for index,upvalue in ipairs(upvalues) do\n          debug.setupvalue(func, index, upvalue)\n       end\n       return func\n   elseif typeidx == TYPE_TABLE or typeidx == TYPE_TORCH or typeidx == TYPE_RECUR_FUNCTION or typeidx == LEGACY_TYPE_RECUR_FUNCTION then\n      -- read the index\n      local index = self:readInt()\n\n      -- check it is loaded already\n      local objects = torch.getenv(self).readObjects\n      if objects[index] and not force then\n         return objects[index]\n      end\n\n      -- otherwise read it\n      if typeidx == TYPE_RECUR_FUNCTION or typeidx == LEGACY_TYPE_RECUR_FUNCTION then\n         local size = self:readInt()\n         local dumped = self:readChar(size):string()\n         local func, err = loadstring(dumped)\n         if not func then\n\t    io.stderr:write(string.format('Warning: Failed to load function from bytecode: %s', err))\n         end\n         if not force then\n             objects[index] = func\n         end\n         local upvalueIdToClosure = torch.getenv(self).upvalueIdToClosure\n         local upvalues = self:readObject()\n         for index,upvalue in ipairs(upvalues) do\n            if typeidx == LEGACY_TYPE_RECUR_FUNCTION then\n               debug.setupvalue(func, index, upvalue)\n            elseif upvalue.name == '_ENV' then\n               debug.setupvalue(func, index, _ENV)\n            else\n               debug.setupvalue(func, index, upvalue.value)\n               -- debug.upvaluejoin exists only for lua>=5.2 and luajit\n               if debug.upvaluejoin and upvalue.id then\n                  if upvalueIdToClosure[upvalue.id] then\n                     -- This upvalue is linked to another one\n                     local otherClosure = upvalueIdToClosure[upvalue.id]\n                     debug.upvaluejoin(func, index, otherClosure.func, otherClosure.index)\n                  else\n                     -- Save this closure for next time\n                     upvalueIdToClosure[upvalue.id] = {\n                        func = func,\n                        index = index,\n                     }\n                  end\n               end\n            end\n         end\n         return func\n      elseif typeidx == TYPE_TORCH then\n         local version, className, versionNumber\n         version = self:readChar(self:readInt()):string()\n         versionNumber = tonumber(string.match(version, '^V (.*)$'))\n         if not versionNumber then\n            className = version\n            versionNumber = 0 -- file created before existence of versioning system\n         else\n            className = self:readChar(self:readInt()):string()\n         end\n         if not torch.factory(className) then\n            error(string.format('unknown Torch class <%s>', tostring(className)))\n         end\n         local object = torch.factory(className)(self)\n         if not force then\n             objects[index] = object\n         end\n         local read = getmetamethod(object, 'read')\n         if read then\n            read(object, self, versionNumber)\n         elseif type(object) == 'table' then\n            local var = self:readObject()\n            for k,v in pairs(var) do\n               object[k] = v\n            end\n         else\n            error(string.format('Cannot load object class <%s>', tostring(className)))\n         end\n         return object\n      else -- it is a table\n         local size = self:readInt()\n         local object = {}\n         if not force then\n             objects[index] = object\n         end\n         for i = 1,size do\n            local k = self:readObject()\n            local v = self:readObject()\n            object[k] = v\n         end\n         return object\n      end\n   else\n      error('unknown object')\n   end\nend\n\n-- simple helpers to save/load arbitrary objects/tables\nfunction torch.save(filename, object, mode, referenced)\n   assert(mode == nil or mode == 'binary' or mode == 'ascii', '\"binary\" or \"ascii\" (or nil) expected for mode')\n   assert(referenced == nil or referenced == true or referenced == false, 'true or false (or nil) expected for referenced')\n   mode = mode or 'binary'\n   referenced = referenced == nil and true or referenced\n   local file = torch.DiskFile(filename, 'w')\n   file[mode](file)\n   file:referenced(referenced)\n   file:writeObject(object)\n   file:close()\nend\n\nfunction torch.load(filename, mode, referenced)\n   assert(mode == 'binary' or mode == 'b32' or mode == 'b64' or\n          mode == nil or mode == 'ascii',\n          '\"binary\", \"b32\", \"b64\" or \"ascii\" (or nil) expected for mode')\n   assert(referenced == nil or referenced == true or referenced == false,\n          'true or false (or nil) expected for referenced')\n   local longSize\n   if mode == 'b32' or mode == 'b64' then\n      longSize = tonumber(mode:match('%d+')) / 8\n      mode = 'binary'\n   end\n   mode = mode or 'binary'\n   referenced = referenced == nil and true or referenced\n   local file = torch.DiskFile(filename, 'r')\n   file[mode](file)\n   file:referenced(referenced)\n   if longSize then file:longSize(longSize) end\n   local object = file:readObject()\n   file:close()\n   return object\nend\n\n-- simple helpers to serialize/deserialize arbitrary objects/tables\nfunction torch.serialize(object, mode)\n   local storage = torch.serializeToStorage(object, mode)\n   return storage:string()\nend\n\n-- Serialize to a CharStorage, not a lua string. This avoids\nfunction torch.serializeToStorage(object, mode)\n   mode = mode or 'binary'\n   local f = torch.MemoryFile()\n   f = f[mode](f)\n   f:writeObject(object)\n   local storage = f:storage()\n   -- the storage includes an extra NULL character: get rid of it\n   storage:resize(storage:size()-1)\n   f:close()\n   return storage\nend\n\nfunction torch.deserializeFromStorage(storage, mode)\n   mode = mode or 'binary'\n   local tx = torch.CharTensor(storage)\n   local xp = torch.CharStorage(tx:size(1)+1)\n   local txp = torch.CharTensor(xp)\n   txp:narrow(1,1,tx:size(1)):copy(tx)\n   txp[tx:size(1)+1] = 0\n   local f = torch.MemoryFile(xp)\n   f = f[mode](f)\n   local object = f:readObject()\n   f:close()\n   return object\nend\n\nfunction torch.deserialize(str, mode)\n   local storage = torch.CharStorage():string(str)\n   return torch.deserializeFromStorage(storage, mode)\nend\n\n-- public API (saveobj/loadobj are safe for global import)\ntorch.saveobj = torch.save\ntorch.loadobj = torch.load\n"
        },
        {
          "name": "Generator.c",
          "type": "blob",
          "size": 1.21875,
          "content": "#include <general.h>\n\nint torch_Generator_new(lua_State *L)\n{\n  THGenerator *gen = THGenerator_new();\n  luaT_pushudata(L, gen, torch_Generator);\n  return 1;\n}\n\nint torch_Generator_free(lua_State *L)\n{\n  THGenerator *gen= luaT_checkudata(L, 1, torch_Generator);\n  THGenerator_free(gen);\n  return 0;\n}\n\nstatic int torch_Generator_write(lua_State *L)\n{\n  THGenerator *gen = luaT_checkudata(L, 1, torch_Generator);\n  THFile *file = luaT_checkudata(L, 2, \"torch.File\");\n\n  THFile_writeByteRaw(file, (unsigned char *)gen, sizeof(THGenerator));\n  return 0;\n}\n\nstatic int torch_Generator_read(lua_State *L)\n{\n  THGenerator *gen = luaT_checkudata(L, 1, torch_Generator);\n  THFile *file = luaT_checkudata(L, 2, \"torch.File\");\n\n  THFile_readByteRaw(file, (unsigned char *)gen, sizeof(THGenerator));\n  return 0;\n}\n\n\nstatic const struct luaL_Reg torch_Generator_table_ [] = {\n  {\"write\", torch_Generator_write},\n  {\"read\", torch_Generator_read},\n  {NULL, NULL}\n};\n\n#define torch_Generator_factory torch_Generator_new\n\nvoid torch_Generator_init(lua_State *L)\n{\n  luaT_newmetatable(L, torch_Generator, NULL,\n                    torch_Generator_new, torch_Generator_free, torch_Generator_factory);\n  luaT_setfuncs(L, torch_Generator_table_, 0);\n  lua_pop(L, 1);\n}\n"
        },
        {
          "name": "MemoryFile.c",
          "type": "blob",
          "size": 1.79296875,
          "content": "#include \"general.h\"\n\nstatic int torch_MemoryFile_new(lua_State *L)\n{\n  const char *mode;\n  THCharStorage *storage = luaT_toudata(L, 1, \"torch.CharStorage\");\n  THFile *self;\n\n  if(storage)\n  {\n    mode = luaL_optstring(L, 2, \"rw\");\n    self = THMemoryFile_newWithStorage(storage, mode);\n  }\n  else\n  {\n    mode = luaL_optstring(L, 1, \"rw\");\n    self = THMemoryFile_new(mode);\n  }\n\n  luaT_pushudata(L, self, \"torch.MemoryFile\");\n  return 1;\n}\n\nstatic int torch_MemoryFile_storage(lua_State *L)\n{\n  THFile *self = luaT_checkudata(L, 1, \"torch.MemoryFile\");\n  THCharStorage_retain(THMemoryFile_storage(self));\n  luaT_pushudata(L, THMemoryFile_storage(self), \"torch.CharStorage\");\n  return 1;\n}\n\nstatic int torch_longSize(lua_State *L)\n{\n  THFile *self = luaT_checkudata(L, 1, \"torch.MemoryFile\");\n  THMemoryFile_longSize(self, lua_tointeger(L, 2));\n  lua_settop(L, 1);\n  return 1;\n}\n\nstatic int torch_MemoryFile_free(lua_State *L)\n{\n  THFile *self = luaT_checkudata(L, 1, \"torch.MemoryFile\");\n  THFile_free(self);\n  return 0;\n}\n\nstatic int torch_MemoryFile___tostring__(lua_State *L)\n{\n  THFile *self = luaT_checkudata(L, 1, \"torch.MemoryFile\");\n  lua_pushfstring(L, \"torch.MemoryFile [status: %s -- mode: %c%c]\",\n                  (THFile_isOpened(self) ? \"open\" : \"closed\"),\n                  (THFile_isReadable(self) ? 'r' : ' '),\n                  (THFile_isWritable(self) ? 'w' : ' '));\n  return 1;\n}\n\nstatic const struct luaL_Reg torch_MemoryFile__ [] = {\n  {\"storage\", torch_MemoryFile_storage},\n  {\"longSize\", torch_longSize},\n  {\"__tostring__\", torch_MemoryFile___tostring__},\n  {NULL, NULL}\n};\n\nvoid torch_MemoryFile_init(lua_State *L)\n{\n  luaT_newmetatable(L, \"torch.MemoryFile\", \"torch.File\",\n                    torch_MemoryFile_new, torch_MemoryFile_free, NULL);\n  luaT_setfuncs(L, torch_MemoryFile__, 0);\n  lua_pop(L, 1);\n}\n"
        },
        {
          "name": "PipeFile.c",
          "type": "blob",
          "size": 1.2001953125,
          "content": "#include \"general.h\"\n\nstatic int torch_PipeFile_new(lua_State *L)\n{\n  const char *name = luaL_checkstring(L, 1);\n  const char *mode = luaL_optstring(L, 2, \"r\");\n  int isQuiet = luaT_optboolean(L, 3, 0);\n  THFile *self = THPipeFile_new(name, mode, isQuiet);\n\n  luaT_pushudata(L, self, \"torch.PipeFile\");\n  return 1;\n}\n\nstatic int torch_PipeFile_free(lua_State *L)\n{\n  THFile *self = luaT_checkudata(L, 1, \"torch.PipeFile\");\n  THFile_free(self);\n  return 0;\n}\n\nstatic int torch_PipeFile___tostring__(lua_State *L)\n{\n  THFile *self = luaT_checkudata(L, 1, \"torch.PipeFile\");\n  lua_pushfstring(L, \"torch.PipeFile on <%s> [status: %s -- mode: %c%c]\",\n                  THDiskFile_name(self),\n                  (THFile_isOpened(self) ? \"open\" : \"closed\"),\n                  (THFile_isReadable(self) ? 'r' : ' '),\n                  (THFile_isWritable(self) ? 'w' : ' '));\n  return 1;\n}\n\nstatic const struct luaL_Reg torch_PipeFile__ [] = {\n  {\"__tostring__\", torch_PipeFile___tostring__},\n  {NULL, NULL}\n};\n\nvoid torch_PipeFile_init(lua_State *L)\n{\n  luaT_newmetatable(L, \"torch.PipeFile\", \"torch.DiskFile\",\n                    torch_PipeFile_new, torch_PipeFile_free, NULL);\n  luaT_setfuncs(L, torch_PipeFile__, 0);\n  lua_pop(L, 1);\n}\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 3.7744140625,
          "content": "[![Join the chat at https://gitter.im/torch/torch7](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/torch/torch7?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n[![Build Status](https://travis-ci.org/torch/torch7.svg)](https://travis-ci.org/torch/torch7)\n\n## Development Status\n\nTorch is not in active developement. The functionality provided by the C backend of Torch, which are the TH, THNN, THC, THCUNN libraries is actively extended and re-written in the ATen C++11 library ([source](https://github.com/pytorch/pytorch/tree/master/aten), [mirror](https://github.com/zdevito/ATen/)).\nATen exposes all operators you would expect from torch7, nn, cutorch, and cunn directly in C++11 and includes additional support for sparse tensors and distributed operations. It is to note however that the API and semantics of the backend libraries in Torch-7 are different from the semantice provided by ATen. For example ATen provides numpy-style broadcasting while TH* dont. For information on building the forked Torch-7 libraries in C, refer to [\"The C interface\" in pytorch/aten/src/README.md](https://github.com/pytorch/pytorch/tree/master/aten/src#the-c-interface).\n\n\n## Need help? ##\n\nTorch7 community support can be found at the following locations. As of 2019, the Torch-7 community is close to non-existent.\n\n* Questions, Support, Install issues: [Google groups](https://groups.google.com/forum/#!forum/torch7)\n* Reporting bugs: [torch7](https://github.com/torch/torch7/issues) [nn](https://github.com/torch/nn/issues) [cutorch](https://github.com/torch/cutorch/issues) [cunn](https://github.com/torch/cutorch/issues) [optim](https://github.com/torch/optim/issues) [threads](https://github.com/torch/threads/issues)\n* Hanging out with other developers and users (strictly no install issues, no large blobs of text): [Gitter Chat](https://gitter.im/torch/torch7)\n\n<a name=\"torch.reference.dok\"></a>\n# Torch Package Reference Manual #\n\n__Torch__ is the main package in [Torch7](http://torch.ch) where data\nstructures for multi-dimensional tensors and mathematical operations\nover these are defined. Additionally, it provides many utilities for\naccessing files, serializing objects of arbitrary types and other\nuseful utilities.\n\n<a name=\"torch.overview.dok\"></a>\n## Torch Packages ##\n\n  * Tensor Library\n    * [Tensor](doc/tensor.md) defines the _all powerful_ tensor object that provides multi-dimensional numerical arrays with type templating.\n    * [Mathematical operations](doc/maths.md) that are defined for the tensor object types.\n    * [Storage](doc/storage.md) defines a simple storage interface that controls the underlying storage for any tensor object.\n  * File I/O Interface Library\n    * [File](doc/file.md) is an abstract interface for common file operations.\n    * [Disk File](doc/diskfile.md) defines operations on files stored on disk.\n    * [Memory File](doc/memoryfile.md) defines operations on stored in RAM.\n    * [Pipe File](doc/pipefile.md) defines operations for using piped commands.\n    * [High-Level File operations](doc/serialization.md) defines higher-level serialization functions.\n  * Useful Utilities\n    * [Timer](doc/timer.md) provides functionality for _measuring time_.\n    * [Tester](doc/tester.md) is a generic tester framework.\n    * [CmdLine](doc/cmdline.md) is a command line argument parsing utility.\n    * [Random](doc/random.md) defines a random number generator package with various distributions.\n    * Finally useful [utility](doc/utility.md) functions are provided for easy handling of torch tensor types and class inheritance.\n\n<a name=\"torch.links.dok\"></a>\n## Useful Links ##\n\n  * [Community packages](https://github.com/torch/torch7/wiki/Cheatsheet)\n  * [Torch Blog](http://torch.ch/blog/)\n  * [Torch Slides](https://github.com/soumith/cvpr2015/blob/master/cvpr-torch.pdf)\n\n"
        },
        {
          "name": "ROADMAP.md",
          "type": "blob",
          "size": 10.224609375,
          "content": "\n# Torch Roadmap (August 2015 - March 2016)\n\nThis roadmap document is intended to serve as a loose plan of our vision for Torch in the short term.  \nIt is open to community feedback and contribution and only intends to serve as an initial draft.  \nAfter community feedback, we shall freeze it and work on it.  \n\nThe roadmap focuses on five separate things\n\n- Core development: improving the core technically. Design changes, code refactors, performance, they go here.\n- Documentation and Accessibility: Outlining the changes in documentation, and improving general user and developer documentation in various ways.\n- Versioning and Packaging: Planned and much needed changes to the packaging of Torch are discussed here.\n- Continuous Build Infrastructure: Making our continuous builds more robust, introducing CUDA and OpenCL contbuilds etc.\n- Other improvements\n\n\n## Torch Core Project Development\n\n - New class system:\n   - **[definite]** with no global side-effects (i.e. the class constructor should be scoped into its parent package)\n     Get rid of every statement/system that has a global effect on the environment (torch.setdefaultensortype => dangerous and not clean)\n   - **[needs discussion]** fully serializable (i.e. when deserializing/reloading a model, there shouldn't be a need to load libraries that defined the class originally, like nn; the class definition should be serialized as well: this would remove a lot of backward compatibility hacks that we have to add to class definitions currently\n       - **koray**: I like this, but wouldn't it break backward compatibility?\n\t\t            Currently, whatever we serialize, it is just the data and implementation is defined\n\t\t\t\t\tat load time, so if a bug is fixed (or introduced) you use that.\n\t\t\t\t\tAnd it starts being ambiguous, what if I load a layer from file and\n\t\t\t\t\tcreate a new one and their implementation is inconsistent...)\n - **[definite]** Get rid of non-tensor-related stuff (like serialization) in TH, and move it to lua side\n - **[needs discussion]** OpenMP: Should it stay or go? Is Threads sufficient?\n       - **Ronan**: I really wonder about this guy, especially now that I have been using threads intensively. I am not sure that fine-grine threading is necessary.\n\t   - **koray**: I guess you mean with threading, there is no need for OpenMP, but I disagree.\n\t          Our convolution layer will use multiple threads and then if we run a ReLu over a huge state space, it would become embarrassingly slow.\n\t\t\t  We shouldn't expect everyone to run their experiments in a threading framework. It is more work than necessary sometimes.)\n - **[needs discussion]** Templated C++ in TH Core?\n                    - **Ronan**: Should I cleanup TH core? In the end, I am scared to move to C++, but some iterators based taking a closure could be nice (I have some of those that I could add easily).\n\t\t\t\t\t         I could move to C++ if it was only template + keeping pointers (and not C++11/14/17, because that would limit the number of users that it can reach because of the latest compilers needed etc.).\n - **[definite]** Migrate to a single, better/modern testing support\n              - **koray**: like some aspects of Totem, but should be in core Tester\n - **[definite]** Benchmarking support in Tester\n - **[definite]** Consistent testing scripts across all core projects\n - **[definite]** 'nn' container unified interface between containers and graph\n - **[mostly definite]** Switch to batch only assumption in 'nn'. Right now, the code is unnecessarily complicated for stochastic/batch confusion, we needed extra functions like nInputDims and such.\n - **[needs discussion]** Support named arguments in the constructor for all 'nn' layers.\n - **[definite]** 'rnn' package.\n      - **Soumith**: Nicholas Leonard's seems to be a good one.\n - **[mostly definite]** argcheck for all core functions in torch. Get rid of cwrap's ugliness.\n - **[definite]** improve paths to support more file system operations\n       - **Clement**: could lfs and penlight be made more standard? penlight is a heavy package but provides so much utility\n\t   - **Soumith**: I think penlight is lightweight and provides strong utility, definitely consider dependence.\n - **[definite]** JIT/Lua/FFI/GC:\n   - **koray**: I think Torch should be agnostic to whatever is the backend;\n   - **clement**: yes!\n   - at this point, we need to have all core packages use the regular Lua api (almost the case)\n     - **Ronan**: agreed.\n\n- **[definite]** plan to have standalone FFI?\n  - Facebook releases their puc LUA based FFI package mostly improved by Sam Gross\n  - [needs discussion] **Ronan** improves it a bit more to use Leon's C99 parser\n                         - **Koray**: I am not opposed to Leon's C99 parser, but we should not have the QT like situation where\n\t\t\t\t\t\t       it relies mostly on Leon to maintain it.\n\t\t\t\t\t\t\t   And, still we need to have FFI since there are people and packages that rely on it now.\n- **[definite]** Lua 5.2 migration (I think it's already finished ;) ).\n- **[mostly definite]** Lua 5.3 migration\n- **[mostly definite]** Optionally replace GC by Ref-counting (existing version in luajit-rocks; but completely broken but will need to be fixed)\n- **[needs discussion]** Make OpenCL support more visible under torch/opencl (**Soumith**: Hugh Perkins will maintain it of course ;) ).\n- **[definite]** Split nn into THNN and nn. THNN would be NN package using TH as backend and nn would be the lua layer. THNN can be used as a standalone C library. Same for cunn\n- **[Definite]** CUDA typed tensor support - CudaHalfTensor CudaDoubleTensor etc.\n- **[Definite]** better plotting support\n- **[needs discussion]** UI package that doesn't suck?\n  - **Ronan**: something based on cairo?\n    - **clement**: not sure if this would have much adoption\n    - **Ronan**: yes, it is a worry. I started to do some fancy stuff there, it is not that hard.\n\t         However, I would need quite some time to polish it.\n\t\t\t I think having something fully customizable from lua really \n                         makes a difference (rather than something like Qt, for example). \n  - something based on a web client?\n      - **clement**: i like the idea of itorch but could never easily build it, build process is too big.\n      - **Ronan**: I cannot use something which forces me to use global variables.\n      - **koray**: I think at the end of the day, we need to have both a GUI client and a web based client.\n\t\t   My main problem with web based clients is that I can't easily create \n                   custom displays to play an animation or such.\n\t\t   It is an offline process that I need to generate a movie and then load it in.\n\t\t   This and similar things make it hard to use for me.\n\t\t   Also, I agree, I actually could not install iTorch on my laptop \n                   before cvpr tutorial somehow, it did not want to work :).\n  - **soumith**: I think we should propose a common display API that any interface can implement, \n                 that way the users don't need to change scripts across different UI backends.\n\t         Also, szym/display is a good candidate for the Web UI, ITorch is indeed a bit of a pain to install.\n\n  - Should we endorse iTorch for everyone to use? \n    - **Ronan**: I know **Soumith** likes it, but I am not a big fan. \n    -            Heavy+encourages the use of global variables. Excellent for tutorials, though.\n \t   - This ties to the first question in **Other Questions** section.\n \t   - Can we/community do pull requests on iTorch? ( **Soumith**: Yes )\n \t   - First step would be to leanify dependencies and/or install procedure (**Soumith**: agreed)\n- **[needs discussion]** How about Penlight? It has many crucial things that people use.\n   Should we endorse it, use some things from it? Replicate some things in penlight in torch?\n   - **clement**: upvoting this! we use it extensively.\n   - **Ronan**: I live better with less abstractions, but I can be convinced there.\n          However, I find penlight quite big.\n          There are things like the classes that I do not like as well (because of the way they chose for creating classes).\n- **[needs discussion]** how about Moses? New lean functional package that's pretty useful\n- **[definite]** A style guide\n  - Guidelines are super important:\n    - for Lua: at least impose strict camel case + 3 spaces (no tab)\n    - for C: camel case + use of underscore to represent namespace scoping + 2 spaces\n\n## Documentation + Accessibility\n\n - Tutorials: provide guidelines and basic framework/standard to write and publish tutorials?\n - Universal dataset API\n   - Dataset classes for several popular datasets\n   - high performance, thread support etc.\n   - support CPU and GPU\n - Model Zoo + Training scripts, with training scripts we can highlight Torch's strengths\n  - How do we build a super friendly model zoo? git repo of pre-trained models?\n    - Better documentation support, have a doc server\n \t- Documentation for TH/THC interface and design\n \t- Inline documentation parser\n - doc/shell integration (maybe this is still working but needs redoing?)\n\n## Versioning + Packaging\n - Package owners need to start releasing frequent versions (i.e. torch v7.0.1, 7.0.2, ...)\n - scm packages should become deprecated\n - Packages need to avoid global side effects, and return themselves as simple tables (Lua 5.2 started enforcing this on the C side)\n - Provide standard AMI instances that people can launch (already loosely done by the community). We can load it with many standard+optional packages and/or provide one line option to update to latest.\n\n## Build Infrastructure Requirements\n - Prepare core distro release\n - Professional Continuous build for distro and individual core projects\n - Continuous build for GPU\n \t- continuous build should include testing\n - The distro should be build and tested at every pull into any of the member projects\n - CI for Linux and OSX\n\n## Other Questions?\n - If there is a project that seems good from outside or consortium, how do we endorse/improve/modify that?\n \t- do we put some technical criteria to do that?\n \t- being able to do pull requests?\n\t- Licensing?\n \t- or maybe maintain a list of suggested packages?\n \t- when does existence of a package stop us from developing the same in core torch?\n\t- **Soumith**: I think this should largely be community driven and by popularity. Top starred or watched repos in the ecosystem would be a good start.\n \t\n"
        },
        {
          "name": "Storage.c",
          "type": "blob",
          "size": 0.39453125,
          "content": "#include \"general.h\"\n\n#define torch_Storage_(NAME) TH_CONCAT_4(torch_,Real,Storage_,NAME)\n#define THFile_readRealRaw TH_CONCAT_3(THFile_read, Real, Raw)\n#define THFile_writeRealRaw TH_CONCAT_3(THFile_write, Real, Raw)\n#define torch_Storage TH_CONCAT_STRING_3(torch.,Real,Storage)\n\n#include \"generic/Storage.c\"\n#include \"THGenerateAllTypes.h\"\n\n#include \"generic/Storage.c\"\n#include \"THGenerateHalfType.h\"\n"
        },
        {
          "name": "Tensor.c",
          "type": "blob",
          "size": 0.390625,
          "content": "#include \"general.h\"\n\n#define torch_Storage_(NAME) TH_CONCAT_4(torch_,Real,Storage_,NAME)\n#define torch_Storage TH_CONCAT_STRING_3(torch.,Real,Storage)\n#define torch_Tensor_(NAME) TH_CONCAT_4(torch_,Real,Tensor_,NAME)\n#define torch_Tensor TH_CONCAT_STRING_3(torch.,Real,Tensor)\n\n#include \"generic/Tensor.c\"\n#include \"THGenerateAllTypes.h\"\n\n#include \"generic/Tensor.c\"\n#include \"THGenerateHalfType.h\"\n"
        },
        {
          "name": "Tensor.lua",
          "type": "blob",
          "size": 16.3125,
          "content": "-- additional methods for Storage\nlocal Storage = {}\n\n-- additional methods for Tensor\nlocal Tensor = {}\n\n-- types\nlocal types = {'Byte', 'Char', 'Short', 'Int', 'Long', 'Float', 'Half', 'Double'}\n\n-- Lua 5.2 compatibility\nlocal log10 = math.log10 or function(x) return math.log(x, 10) end\n\n-- tostring() functions for Tensor and Storage\nlocal function Storage__printformat(self)\n   if self:size() == 0 then\n     return \"\", nil, 0\n   end\n   local intMode = true\n   local type = torch.typename(self)\n--   if type == 'torch.FloatStorage' or type == 'torch.DoubleStorage' then\n      for i=1,self:size() do\n         if self[i] ~= math.ceil(self[i]) then\n            intMode = false\n            break\n         end\n      end\n--   end\n   local tensor = torch.DoubleTensor(torch.DoubleStorage(self:size()):copy(self), 1, self:size()):abs()\n   local expMin = tensor:min()\n   if expMin ~= 0 then\n      expMin = math.floor(log10(expMin)) + 1\n   else\n      expMin = 1\n   end\n   local expMax = tensor:max()\n   if expMax ~= 0 then\n      expMax = math.floor(log10(expMax)) + 1\n   else\n      expMax = 1\n   end\n\n   local format\n   local scale\n   local sz\n   if intMode then\n      if expMax > 9 then\n         format = \"%11.4e\"\n         sz = 11\n      else\n         format = \"%SZd\"\n         sz = expMax + 1\n      end\n   else\n      if expMax-expMin > 4 then\n         format = \"%SZ.4e\"\n         sz = 11\n         if math.abs(expMax) > 99 or math.abs(expMin) > 99 then\n            sz = sz + 1\n         end\n      else\n         if expMax > 5 or expMax < 0 then\n            format = \"%SZ.4f\"\n            sz = 7\n            scale = math.pow(10, expMax-1)\n         else\n            format = \"%SZ.4f\"\n            if expMax == 0 then\n               sz = 7\n            else\n               sz = expMax+6\n            end\n         end\n      end\n   end\n   format = string.gsub(format, 'SZ', sz)\n   if scale == 1 then\n      scale = nil\n   end\n   return format, scale, sz\nend\n\nfunction Storage.__tostring__(self)\n   local strt = {}\n   local format,scale = Storage__printformat(self)\n   if format:sub(2,4) == 'nan' then format = '%f' end\n   if scale then\n      table.insert(strt, string.format('%g', scale) .. ' *\\n')\n      for i = 1,self:size() do\n         table.insert(strt, string.format(format, self[i]/scale) .. '\\n')\n      end\n   else\n      for i = 1,self:size() do\n         table.insert(strt, string.format(format, self[i]) .. '\\n')\n      end\n   end\n   table.insert(strt, '[' .. torch.typename(self) .. ' of size ' .. self:size() .. ']\\n')\n   local str = table.concat(strt)\n   return str\nend\n\nfor _,type in ipairs(types) do\n   local metatable = torch.getmetatable('torch.' .. type .. 'Storage')\n   for funcname, func in pairs(Storage) do\n      rawset(metatable, funcname, func)\n   end\nend\n\nlocal function Tensor__printMatrix(self, indent)\n   local format,scale,sz = Storage__printformat(self:storage())\n   if format:sub(2,4) == 'nan' then format = '%f' end\n--   print('format = ' .. format)\n   scale = scale or 1\n   indent = indent or ''\n   local strt = {indent}\n   local nColumnPerLine = math.floor((80-#indent)/(sz+1))\n--   print('sz = ' .. sz .. ' and nColumnPerLine = ' .. nColumnPerLine)\n   local firstColumn = 1\n   local lastColumn = -1\n   while firstColumn <= self:size(2) do\n      if firstColumn + nColumnPerLine - 1 <= self:size(2) then\n         lastColumn = firstColumn + nColumnPerLine - 1\n      else\n         lastColumn = self:size(2)\n      end\n      if nColumnPerLine < self:size(2) then\n         if firstColumn ~= 1 then\n            table.insert(strt, '\\n')\n         end\n         table.insert(strt, 'Columns ' .. firstColumn .. ' to ' .. lastColumn .. '\\n' .. indent)\n      end\n      if scale ~= 1 then\n         table.insert(strt, string.format('%g', scale) .. ' *\\n ' .. indent)\n      end\n      for l=1,self:size(1) do\n         local row = self:select(1, l)\n         for c=firstColumn,lastColumn do\n            table.insert(strt, string.format(format, row[c]/scale))\n            if c == lastColumn then\n               table.insert(strt, '\\n')\n               if l~=self:size(1) then\n                  if scale ~= 1 then\n                     table.insert(strt, indent .. ' ')\n                  else\n                     table.insert(strt, indent)\n                  end\n               end\n            else\n               table.insert(strt, ' ')\n            end\n         end\n      end\n      firstColumn = lastColumn + 1\n   end\n   local str = table.concat(strt)\n   return str\nend\n\nlocal function Tensor__printTensor(self)\n   local counter = torch.LongStorage(self:nDimension()-2)\n   local strt = {''}\n   local finished\n   counter:fill(1)\n   counter[1] = 0\n   while true do\n      for i=1,self:nDimension()-2 do\n         counter[i] = counter[i] + 1\n         if counter[i] > self:size(i) then\n            if i == self:nDimension()-2 then\n               finished = true\n               break\n            end\n            counter[i] = 1\n         else\n            break\n         end\n      end\n      if finished then\n         break\n      end\n--      print(counter)\n      if #strt > 1 then\n         table.insert(strt, '\\n')\n      end\n      table.insert(strt, '(')\n      local tensor = self\n      for i=1,self:nDimension()-2 do\n         tensor = tensor:select(1, counter[i])\n         table.insert(strt, counter[i] .. ',')\n      end\n      table.insert(strt, '.,.) = \\n')\n      table.insert(strt, Tensor__printMatrix(tensor, ' '))\n   end\n   return table.concat(strt)\nend\n\nfunction Tensor.__tostring__(self)\n   local strt = {''}\n   if self:nDimension() == 0 then\n      table.insert(strt, '[' .. torch.typename(self) .. ' with no dimension]\\n')\n   else\n      local tensor = torch.DoubleTensor():resize(self:size()):copy(self)\n      if tensor:nDimension() == 1 then\n         local format,scale,sz = Storage__printformat(tensor:storage())\n         if format:sub(2,4) == 'nan' then format = '%f' end\n         if scale then\n            table.insert(strt, string.format('%g', scale) .. ' *\\n')\n            for i = 1,tensor:size(1) do\n               table.insert(strt, string.format(format, tensor[i]/scale) .. '\\n')\n            end\n         else\n            for i = 1,tensor:size(1) do\n               table.insert(strt, string.format(format, tensor[i]) .. '\\n')\n            end\n         end\n         table.insert(strt, '[' .. torch.typename(self) .. ' of size ' .. tensor:size(1) .. ']\\n')\n      elseif tensor:nDimension() == 2 then\n         table.insert(strt, Tensor__printMatrix(tensor))\n         table.insert(strt, '[' .. torch.typename(self) .. ' of size ' .. tensor:size(1) .. 'x' .. tensor:size(2) .. ']\\n')\n      else\n         table.insert(strt, Tensor__printTensor(tensor))\n         table.insert(strt, '[' .. torch.typename(self) .. ' of size ')\n         for i=1,tensor:nDimension() do\n            table.insert(strt, tensor:size(i))\n            if i ~= tensor:nDimension() then\n               table.insert(strt, 'x')\n            end\n         end\n         table.insert(strt, ']\\n')\n      end\n   end\n   return table.concat(strt)\nend\n\nfunction Tensor.type(self,type)\n   local current = torch.typename(self)\n   if not type then return current end\n   if type ~= current then\n      local new = torch.getmetatable(type).new()\n      if self:nElement() > 0 then\n         new:resize(self:size()):copy(self)\n      end\n      return new\n   else\n      return self\n   end\nend\n\nfunction Tensor.typeAs(self,tensor)\n   return self:type(tensor:type())\nend\n\nfunction Tensor.byte(self)\n   return self:type('torch.ByteTensor')\nend\n\nfunction Tensor.char(self)\n   return self:type('torch.CharTensor')\nend\n\nfunction Tensor.short(self)\n   return self:type('torch.ShortTensor')\nend\n\nfunction Tensor.int(self)\n   return self:type('torch.IntTensor')\nend\n\nfunction Tensor.long(self)\n   return self:type('torch.LongTensor')\nend\n\nfunction Tensor.float(self)\n   return self:type('torch.FloatTensor')\nend\n\nfunction Tensor.double(self)\n   return self:type('torch.DoubleTensor')\nend\n\nfunction Tensor.half(self)\n   return self:type('torch.HalfTensor')\nend\n\nfunction Tensor.real(self)\n   return self:type(torch.getdefaulttensortype())\nend\n\nfunction Tensor.expand(result,tensor,...)\n   -- get sizes\n   local sizes = {...}\n\n   local t = torch.type(tensor)\n   if (t == 'number' or t == 'torch.LongStorage') then\n      table.insert(sizes,1,tensor)\n      tensor = result\n      result = tensor.new()\n   end\n\n   -- check type\n   local size\n   if torch.type(sizes[1])=='torch.LongStorage' then\n      size = sizes[1]\n   else\n      size = torch.LongStorage(#sizes)\n      for i,s in ipairs(sizes) do\n         size[i] = s\n      end\n   end\n\n   -- get dimensions\n   local tensor_dim = tensor:dim()\n   local tensor_stride = tensor:stride()\n   local tensor_size = tensor:size()\n\n   -- check nb of dimensions\n   if #size ~= tensor:dim() then\n      error('the number of dimensions provided must equal tensor:dim()')\n   end\n\n   -- create a new geometry for tensor:\n   for i = 1,tensor_dim do\n      if tensor_size[i] == 1 then\n         tensor_size[i] = size[i]\n         tensor_stride[i] = 0\n      elseif tensor_size[i] ~= size[i] then\n         error('incorrect size: only supporting singleton expansion (size=1)')\n      end\n   end\n\n   -- create new view, with singleton expansion:\n   result:set(tensor:storage(), tensor:storageOffset(),\n                         tensor_size, tensor_stride)\n   return result\nend\ntorch.expand = Tensor.expand\n\nfunction Tensor.expandAs(result,tensor,template)\n   if template then\n      return result:expand(tensor,template:size())\n   end\n   return result:expand(tensor:size())\nend\ntorch.expandAs = Tensor.expandAs\n\nfunction Tensor.repeatTensor(result,tensor,...)\n   -- get sizes\n   local sizes = {...}\n\n   local t = torch.type(tensor)\n   if (t == 'number' or t == 'torch.LongStorage') then\n      table.insert(sizes,1,tensor)\n      tensor = result\n      result = tensor.new()\n   end\n   -- if not contiguous, then force the tensor to be contiguous\n   if not tensor:isContiguous() then tensor = tensor:clone() end\n\n   -- check type\n   local size\n   if torch.type(sizes[1])=='torch.LongStorage' then\n      size = sizes[1]\n   else\n      size = torch.LongStorage(#sizes)\n      for i,s in ipairs(sizes) do\n         size[i] = s\n      end\n   end\n   if size:size() < tensor:dim() then\n      error('Number of dimensions of repeat dims can not be smaller than number of dimensions of tensor')\n   end\n   local xtensor = tensor.new():set(tensor)\n   local xsize = xtensor:size():totable()\n   for i=1,size:size()-tensor:dim() do\n      table.insert(xsize,1,1)\n   end\n   size = torch.DoubleTensor(xsize):cmul(torch.DoubleTensor(size:totable())):long():storage()\n   xtensor:resize(torch.LongStorage(xsize))\n   result:resize(size)\n   local urtensor = result.new(result)\n   for i=1,xtensor:dim() do\n      urtensor = urtensor:unfold(i,xtensor:size(i),xtensor:size(i))\n   end\n   for i=1,urtensor:dim()-xtensor:dim() do\n      table.insert(xsize,1,1)\n   end\n   xtensor:resize(torch.LongStorage(xsize))\n   local xxtensor = xtensor:expandAs(urtensor)\n   urtensor:copy(xxtensor)\n   return result\nend\ntorch.repeatTensor = Tensor.repeatTensor\n\n--- One of the size elements can be -1,\n --- a new LongStorage is then returned.\n --- The length of the unspecified dimension\n --- is inferred from the number of remaining elements.\nlocal function specifyFully(size, nElements)\n    local nCoveredElements = 1\n    local remainingDim = nil\n    local sizes = size:totable()\n    for i = 1, #sizes do\n        local wantedDimSize = sizes[i]\n        if wantedDimSize == -1 then\n            if remainingDim then\n                error(\"Only one of torch.view dimensions can be -1.\")\n            end\n            remainingDim = i\n        else\n            nCoveredElements = nCoveredElements * wantedDimSize\n        end\n    end\n\n    if not remainingDim then\n        return size\n    end\n\n    assert(nElements % nCoveredElements == 0, \"The number of covered elements is not a multiple of all elements.\")\n    local copy = torch.LongStorage(sizes)\n    copy[remainingDim] = nElements / nCoveredElements\n    return copy\nend\n\n-- TODO : This should be implemented in TH and and wrapped.\nfunction Tensor.view(result, src, ...)\n   local size = ...\n   local view, tensor\n   local function istensor(tensor)\n      return torch.typename(tensor) and torch.typename(tensor):find('torch.*Tensor')\n   end\n   local function isstorage(storage)\n      return torch.typename(storage) and torch.typename(storage) == 'torch.LongStorage'\n   end\n   if istensor(result) and istensor(src) and type(size) == 'number' then\n      size = torch.LongStorage{...}\n      view = result\n      tensor = src\n   elseif istensor(result) and istensor(src) and isstorage(size) then\n      size = size\n      view = result\n      tensor = src\n   elseif istensor(result) and isstorage(src) and size == nil then\n      size = src\n      tensor = result\n      view = tensor.new()\n   elseif istensor(result) and type(src) == 'number' then\n      size = {...}\n      table.insert(size,1,src)\n      size = torch.LongStorage(size)\n      tensor = result\n      view = tensor.new()\n   else\n      local t1 = 'torch.Tensor, torch.Tensor, number [, number ]*'\n      local t2 = 'torch.Tensor, torch.Tensor, torch.LongStorage'\n      local t3 = 'torch.Tensor, torch.LongStorage'\n      local t4 = 'torch.Tensor, number [, number ]*'\n      error(string.format('torch.view, expected (%s) or\\n (%s) or\\n (%s)\\n or (%s)', t1, t2, t3, t4))\n   end\n   local origNElement = tensor:nElement()\n   size = specifyFully(size, origNElement)\n\n   assert(tensor:isContiguous(), \"expecting a contiguous tensor\")\n   view:set(tensor:storage(), tensor:storageOffset(), size)\n   if view:nElement() ~= origNElement then\n      local inputSize = table.concat(tensor:size():totable(), \"x\")\n      local outputSize = table.concat(size:totable(), \"x\")\n      error(string.format(\"Wrong size for view. Input size: %s. Output size: %s\",\n      inputSize, outputSize))\n   end\n   return view\nend\ntorch.view = Tensor.view\n\nfunction Tensor.viewAs(result, src, template)\n   if template and torch.typename(template) then\n      return result:view(src, template:size())\n   elseif template == nil then\n      template = src\n      src = result\n      result = src.new()\n      return result:view(src, template:size())\n   else\n      local t1 = 'torch.Tensor, torch.Tensor, torch.LongStorage'\n      local t2 = 'torch.Tensor, torch.LongStorage'\n      error(string.format('expecting (%s) or (%s)', t1, t2))\n   end\nend\ntorch.viewAs = Tensor.viewAs\n\nfunction Tensor.split(result, tensor, splitSize, dim)\n   if torch.type(result) ~= 'table' then\n      dim = splitSize\n      splitSize = tensor\n      tensor = result\n      result = {}\n   else\n      -- empty existing result table before using it\n      for k,v in pairs(result) do\n         result[k] = nil\n      end\n   end\n   dim = dim or 1\n   local start = 1\n   while start <= tensor:size(dim) do\n      local size = math.min(splitSize, tensor:size(dim) - start + 1)\n      local split = tensor:narrow(dim, start, size)\n      table.insert(result, split)\n      start = start + size\n   end\n   return result\nend\ntorch.split = Tensor.split\n\nfunction Tensor.chunk(result, tensor, nChunk, dim)\n   if torch.type(result) ~= 'table' then\n      dim = nChunk\n      nChunk = tensor\n      tensor = result\n      result = {}\n   end\n   dim = dim or 1\n   local splitSize = math.ceil(tensor:size(dim)/nChunk)\n   return torch.split(result, tensor, splitSize, dim)\nend\ntorch.chunk = Tensor.chunk\n\nfunction Tensor.totable(tensor)\n  local result = {}\n  local dim = tensor:dim()\n  if dim == 1 then\n    tensor:apply(function(i) table.insert(result, i) end)\n  elseif dim > 0 then\n    for i = 1, tensor:size(1) do\n      table.insert(result, tensor[i]:totable())\n    end\n  end\n  return result\nend\ntorch.totable = Tensor.totable\n\nfunction Tensor.permute(tensor, ...)\n  local perm = {...}\n  local nDims = tensor:dim()\n  assert(#perm == nDims, 'Invalid permutation')\n  local j\n  for i, p in ipairs(perm) do\n    if p ~= i and p ~= 0 then\n      j = i\n      repeat\n        assert(0 < perm[j] and perm[j] <= nDims, 'Invalid permutation')\n        tensor = tensor:transpose(j, perm[j])\n        j, perm[j] = perm[j], 0\n      until perm[j] == i\n      perm[j] = j\n    end\n  end\n  return tensor\nend\ntorch.permute = Tensor.permute\n\nfor _,type in ipairs(types) do\n   local metatable = torch.getmetatable('torch.' .. type .. 'Tensor')\n   for funcname, func in pairs(Tensor) do\n      if funcname ~= 'totable' or type ~='Half' then\n         rawset(metatable, funcname, func)\n      else\n         local function Tensor__totable(self)\n            local host_tensor = self:float()\n            return self:float():totable()\n         end\n         rawset(torch.getmetatable('torch.HalfTensor'), 'totable', Tensor__totable)\n      end\n   end\nend\n"
        },
        {
          "name": "TensorMath.lua",
          "type": "blob",
          "size": 52.6806640625,
          "content": "local wrap = require 'cwrap'\n\nrequire 'torchcwrap'\n\nlocal interface = wrap.CInterface.new()\nlocal method = wrap.CInterface.new()\nlocal argtypes = wrap.CInterface.argtypes\n\nargtypes['ptrdiff_t'] = wrap.types.ptrdiff_t\n\ninterface:print([[\n#include \"TH.h\"\n#include \"THMath.h\"\n#include \"luaT.h\"\n#include \"utils.h\"\n]])\n\n-- specific to torch: we generate a 'dispatch' function\n-- first we create a helper function\n-- note that it let the \"torch\" table on the stack\ninterface:print([[\nstatic const void* torch_istensortype(lua_State *L, const char *tname)\n{\n  if(!tname)\n    return NULL;\n\n  if(!luaT_pushmetatable(L, tname))\n    return NULL;\n\n  lua_pushstring(L, \"torch\");\n  lua_rawget(L, -2);\n  if(lua_istable(L, -1))\n    return tname;\n  else\n  {\n    lua_pop(L, 2);\n    return NULL;\n  }\n\n  return NULL;\n}\n]])\n\ninterface:print([[\nstatic int torch_isnonemptytable(lua_State *L, int idx)\n{\n  int empty;\n  if (!lua_istable(L, idx)) return 0;\n\n  lua_rawgeti(L, idx, 1);\n  empty = lua_isnil(L, -1);\n  lua_pop(L, 1);\n  return !empty;\n}\n]])\n\n\ninterface:print([[\nstatic const void* torch_istensorarray(lua_State *L, int idx)\n{\n  const char* tname;\n  int tensor_idx;\n  if (!torch_isnonemptytable(L, idx)) return 0;\n\n  lua_checkstack(L, 3);\n  lua_rawgeti(L, idx, 1);\n  tensor_idx = lua_gettop(L);\n  tname = (torch_istensortype(L, luaT_typename(L, -1)));\n  lua_remove(L, tensor_idx);\n  return tname;\n}\n]])\n\ninterface.dispatchregistry = {}\nfunction interface:wrap(name, ...)\n   -- usual stuff\n   wrap.CInterface.wrap(self, name, ...)\n\n   -- dispatch function\n   if not interface.dispatchregistry[name] then\n      interface.dispatchregistry[name] = true\n      table.insert(interface.dispatchregistry, {name=name, wrapname=string.format(\"torch_%s\", name)})\n\n      interface:print(string.gsub([[\nstatic int torch_NAME(lua_State *L)\n{\n  int narg = lua_gettop(L);\n  const void *tname;\n  if(narg >= 1 && (tname = torch_istensortype(L, luaT_typename(L, 1)))) /* first argument is tensor? */\n  {\n  }\n  else if(narg >= 2 && (tname = torch_istensortype(L, luaT_typename(L, 2)))) /* second? */\n  {\n  }\n  else if(narg >= 1 && (tname = torch_istensorarray(L, 1))) /* torch table argument? */\n  {\n  }\n  else if(narg >= 1 && lua_type(L, narg) == LUA_TSTRING\n\t  && (tname = torch_istensortype(L, lua_tostring(L, narg)))) /* do we have a valid tensor type string then? */\n  {\n    lua_remove(L, -2);\n  }\n  else if(!(tname = torch_istensortype(L, torch_getdefaulttensortype(L))))\n    luaL_error(L, \"internal error: the default tensor type does not seem to be an actual tensor\");\n\n  lua_pushstring(L, \"NAME\");\n  lua_rawget(L, -2);\n  if(lua_isfunction(L, -1))\n  {\n    lua_insert(L, 1);\n    lua_pop(L, 2); /* the two tables we put on the stack above */\n    lua_call(L, lua_gettop(L)-1, LUA_MULTRET);\n  }\n  else\n    return luaL_error(L, \"%s does not implement the torch.NAME() function\", tname);\n\n  return lua_gettop(L);\n}\n]], 'NAME', name))\n  end\nend\n\nfunction interface:dispatchregister(name)\n   local txt = self.txt\n   table.insert(txt, string.format('static const struct luaL_Reg %s [] = {', name))\n   for _,reg in ipairs(self.dispatchregistry) do\n      table.insert(txt, string.format('{\"%s\", %s},', reg.name, reg.wrapname))\n   end\n   table.insert(txt, '{NULL, NULL}')\n   table.insert(txt, '};')\n   table.insert(txt, '')\n   self.dispatchregistry = {}\nend\n\ninterface:print('/* WARNING: autogenerated file */')\ninterface:print('')\n\nlocal function wrap(...)\n   local args = {...}\n\n   -- interface\n   interface:wrap(...)\n\n   -- method: we override things possibly in method table field\n   for _,x in ipairs(args) do\n      if type(x) == 'table' then -- ok, now we have a list of args\n         for _, arg in ipairs(x) do\n            if arg.method then\n               for k,v in pairs(arg.method) do\n                  if v == 'nil' then -- special case, we erase the field\n                     arg[k] = nil\n                  else\n                     arg[k] = v\n                  end\n               end\n            end\n         end\n      end\n   end\n   local unpack = unpack or table.unpack\n    method:wrap(unpack(args))\nend\n\nlocal reals = {ByteTensor='unsigned char',\n               CharTensor='char',\n               ShortTensor='short',\n               IntTensor='int',\n               LongTensor='long',\n               FloatTensor='float',\n               HalfTensor='half',\n               DoubleTensor='double'}\n\nlocal accreals = {ByteTensor='long',\n               CharTensor='long',\n               ShortTensor='long',\n               IntTensor='long',\n               LongTensor='long',\n               FloatTensor='double',\n               HalfTensor='float',\n               DoubleTensor='double'}\n\nfor _,Tensor in ipairs({\"ByteTensor\", \"CharTensor\",\n                        \"ShortTensor\", \"IntTensor\", \"LongTensor\",\n                        \"FloatTensor\", \"HalfTensor\", \"DoubleTensor\"}) do\n\n   local real = reals[Tensor]\n   local accreal = accreals[Tensor]\n\n   function interface.luaname2wrapname(self, name)\n      return string.format('torch_%s_%s', Tensor, name)\n   end\n\n   function method.luaname2wrapname(self, name)\n      return string.format('m_torch_%s_%s', Tensor, name)\n   end\n\n   local function cname(name)\n      return string.format('TH%s_%s', Tensor, name)\n   end\n\n   local function lastdim(argn)\n      return function(arg)\n                return string.format(\"TH%s_nDimension(%s)\", Tensor, arg.args[argn]:carg())\n             end\n   end\n\n   local function lastdimarray(argn)\n      return function(arg)\n                return string.format(\"TH%s_nDimension(arg%d_data[0])\", Tensor, arg.args[argn].i)\n             end\n   end\n\n   if Tensor ~= 'HalfTensor' then\n   wrap(\"zero\",\n        cname(\"zero\"),\n        {{name=Tensor, returned=true}})\n\n   wrap(\"fill\",\n        cname(\"fill\"),\n        {{name=Tensor, returned=true},\n         {name=real}})\n\n   wrap(\"zeros\",\n        cname(\"zeros\"),\n        {{name=Tensor, default=true, returned=true, method={default='nil'}},\n         {name=\"LongArg\"}})\n\n   wrap(\"ones\",\n        cname(\"ones\"),\n        {{name=Tensor, default=true, returned=true, method={default='nil'}},\n         {name=\"LongArg\"}})\n\n   wrap(\"reshape\",\n        cname(\"reshape\"),\n        {{name=Tensor, default=true, returned=true},\n         {name=Tensor},\n         {name=\"LongArg\"}})\n\n   wrap(\"gather\",\n        cname(\"gather\"),\n        {{name=Tensor, default=true, returned=true,\n          init=function(arg)\n                  return table.concat(\n                     {\n                        arg.__metatable.init(arg),\n                        string.format(\"THLongStorage* %s_size = THLongTensor_newSizeOf(%s);\", arg:carg(), arg.args[4]:carg()),\n                        string.format(\"TH%s_resize(%s, %s_size, NULL);\", Tensor, arg:carg(), arg:carg()),\n                        string.format(\"THLongStorage_free(%s_size);\", arg:carg())\n                     }, '\\n')\n               end\n         },\n         {name=Tensor},\n         {name=\"index\"},\n         {name=\"IndexTensor\", noreadadd=true}})\n\n   wrap(\"scatter\",\n        cname(\"scatter\"),\n        {{name=Tensor, returned=true},\n         {name=\"index\"},\n         {name=\"IndexTensor\", noreadadd=true},\n         {name=Tensor}},\n        cname(\"scatterFill\"),\n        {{name=Tensor, returned=true},\n         {name=\"index\"},\n         {name=\"IndexTensor\", noreadadd=true},\n         {name=real}})\n\n   wrap(\"dot\",\n        cname(\"dot\"),\n        {{name=Tensor},\n         {name=Tensor},\n         {name=accreal, creturned=true}})\n\n   wrap(\"equal\",\n        cname(\"equal\"),\n        {{name=Tensor},\n         {name=Tensor},\n         {name=\"boolean\", creturned=true}})\n\n   wrap(\"add\",\n        cname(\"add\"),\n        {{name=Tensor, default=true, returned=true, method={default='nil'}},\n         {name=Tensor, method={default=1}},\n         {name=real}},\n        cname(\"cadd\"),\n        {{name=Tensor, default=true, returned=true, method={default='nil'}},\n         {name=Tensor, method={default=1}},\n         {name=real, default=1},\n         {name=Tensor}})\n\n   wrap(\"csub\",\n     cname(\"sub\"),\n     {{name=Tensor, default=true, returned=true, method={default='nil'}},\n       {name=Tensor, method={default=1}},\n       {name=real}},\n     cname(\"csub\"),\n     {{name=Tensor, default=true, returned=true, method={default='nil'}},\n       {name=Tensor, method={default=1}},\n       {name=real, default=1},\n       {name=Tensor}})\n\n   wrap(\"mul\",\n        cname(\"mul\"),\n        {{name=Tensor, default=true, returned=true, method={default='nil'}},\n         {name=Tensor, method={default=1}},\n         {name=real}})\n\n   wrap(\"div\",\n        cname(\"div\"),\n        {{name=Tensor, default=true, returned=true, method={default='nil'}},\n         {name=Tensor, method={default=1}},\n         {name=real}})\n\n   wrap(\"lshift\",\n        cname(\"lshift\"),\n        {{name=Tensor, default=true, returned=true, method={default='nil'}},\n         {name=Tensor, method={default=1}},\n         {name=real}})\n\n   wrap(\"rshift\",\n        cname(\"rshift\"),\n        {{name=Tensor, default=true, returned=true, method={default='nil'}},\n         {name=Tensor, method={default=1}},\n         {name=real}})\n\n   wrap(\"fmod\",\n        cname(\"fmod\"),\n        {{name=Tensor, default=true, returned=true, method={default='nil'}},\n         {name=Tensor, method={default=1}},\n         {name=real}})\n\n   wrap(\"remainder\",\n        cname(\"remainder\"),\n        {{name=Tensor, default=true, returned=true, method={default='nil'}},\n         {name=Tensor, method={default=1}},\n         {name=real}})\n\n   wrap(\"bitand\",\n        cname(\"bitand\"),\n        {{name=Tensor, default=true, returned=true, method={default='nil'}},\n         {name=Tensor, method={default=1}},\n         {name=real}})\n\n   wrap(\"bitor\",\n        cname(\"bitor\"),\n        {{name=Tensor, default=true, returned=true, method={default='nil'}},\n         {name=Tensor, method={default=1}},\n         {name=real}})\n\n   wrap(\"bitxor\",\n        cname(\"bitxor\"),\n        {{name=Tensor, default=true, returned=true, method={default='nil'}},\n         {name=Tensor, method={default=1}},\n         {name=real}})\n\n   -- mod alias\n   wrap(\"mod\",\n        cname(\"fmod\"),\n        {{name=Tensor, default=true, returned=true, method={default='nil'}},\n         {name=Tensor, method={default=1}},\n         {name=real}})\n\n   wrap(\"clamp\",\n        cname(\"clamp\"),\n        {{name=Tensor, default=true, returned=true, method={default='nil'}},\n         {name=Tensor, method={default=1}},\n         {name=real},\n         {name=real}})\n\n\n   wrap(\"match\",\n        cname(\"match\"),\n        {{name=Tensor, default=true, returned=true, method={default='nil'}},\n         {name=Tensor},\n         {name=Tensor},\n         {name=real, default=1}\n        })\n\n   wrap(\"cmul\",\n        cname(\"cmul\"),\n        {{name=Tensor, default=true, returned=true, method={default='nil'}},\n         {name=Tensor, method={default=1}},\n         {name=Tensor}})\n\n   wrap(\"cpow\",\n        cname(\"cpow\"),\n        {{name=Tensor, default=true, returned=true, method={default='nil'}},\n         {name=Tensor, method={default=1}},\n         {name=Tensor}})\n\n   wrap(\"cdiv\",\n        cname(\"cdiv\"),\n        {{name=Tensor, default=true, returned=true, method={default='nil'}},\n         {name=Tensor, method={default=1}},\n         {name=Tensor}})\n\n   wrap(\"clshift\",\n        cname(\"clshift\"),\n        {{name=Tensor, default=true, returned=true, method={default='nil'}},\n         {name=Tensor, method={default=1}},\n         {name=Tensor}})\n\n   wrap(\"crshift\",\n        cname(\"crshift\"),\n        {{name=Tensor, default=true, returned=true, method={default='nil'}},\n         {name=Tensor, method={default=1}},\n         {name=Tensor}})\n\n   wrap(\"cfmod\",\n        cname(\"cfmod\"),\n        {{name=Tensor, default=true, returned=true, method={default='nil'}},\n         {name=Tensor, method={default=1}},\n         {name=Tensor}})\n\n   wrap(\"cremainder\",\n        cname(\"cremainder\"),\n        {{name=Tensor, default=true, returned=true, method={default='nil'}},\n         {name=Tensor, method={default=1}},\n         {name=Tensor}})\n\n   wrap(\"cbitand\",\n        cname(\"cbitand\"),\n        {{name=Tensor, default=true, returned=true, method={default='nil'}},\n         {name=Tensor, method={default=1}},\n         {name=Tensor}})\n\n   wrap(\"cbitor\",\n        cname(\"cbitor\"),\n        {{name=Tensor, default=true, returned=true, method={default='nil'}},\n         {name=Tensor, method={default=1}},\n         {name=Tensor}})\n\n   wrap(\"cbitxor\",\n        cname(\"cbitxor\"),\n        {{name=Tensor, default=true, returned=true, method={default='nil'}},\n         {name=Tensor, method={default=1}},\n         {name=Tensor}})\n\n   -- cmod alias\n   wrap(\"cmod\",\n        cname(\"cfmod\"),\n        {{name=Tensor, default=true, returned=true, method={default='nil'}},\n         {name=Tensor, method={default=1}},\n         {name=Tensor}})\n\n   wrap(\"addcmul\",\n        cname(\"addcmul\"),\n        {{name=Tensor, default=true, returned=true, method={default='nil'}},\n         {name=Tensor, method={default=1}},\n         {name=real, default=1},\n         {name=Tensor},\n         {name=Tensor}})\n\n   wrap(\"addcdiv\",\n        cname(\"addcdiv\"),\n        {{name=Tensor, default=true, returned=true, method={default='nil'}},\n         {name=Tensor, method={default=1}},\n         {name=real, default=1},\n         {name=Tensor},\n         {name=Tensor}})\n\n   wrap(\"mv\",\n        cname(\"addmv\"),\n        {{name=Tensor, default=true, returned=true, method={default='nil'},\n          init=function(arg)\n                  return table.concat(\n                     {\n                        arg.__metatable.init(arg),\n                        string.format(\"TH%s_resize1d(%s, %s->size[0]);\", Tensor, arg:carg(), arg.args[5]:carg())\n                     }, '\\n')\n               end,\n          precall=function(arg)\n                  return table.concat(\n                     {\n                        string.format(\"TH%s_zero(%s);\", Tensor, arg:carg()),\n                        arg.__metatable.precall(arg)\n                     }, '\\n')\n               end,\n       },\n         {name=real, default=0, invisible=true},\n         {name=Tensor, default=1, invisible=true},\n         {name=real, default=1, invisible=true},\n         {name=Tensor, dim=2},\n         {name=Tensor, dim=1}}\n     )\n\n   wrap(\"mm\",\n        cname(\"addmm\"),\n        {{name=Tensor, default=true, returned=true, method={default='nil'},\n          init=function(arg)\n                  return table.concat(\n                     {\n                        arg.__metatable.init(arg),\n                        string.format(\"TH%s_resize2d(%s, %s->size[0], %s->size[1]);\", Tensor, arg:carg(), arg.args[5]:carg(), arg.args[6]:carg())\n                     }, '\\n')\n               end,\n          precall=function(arg)\n                  return table.concat(\n                     {\n                        string.format(\"TH%s_zero(%s);\", Tensor, arg:carg()),\n                        arg.__metatable.precall(arg)\n                     }, '\\n')\n               end,\n       },\n         {name=real, default=0, invisible=true},\n         {name=Tensor, default=1, invisible=true},\n         {name=real, default=1, invisible=true},\n         {name=Tensor, dim=2},\n         {name=Tensor, dim=2}}\n     )\n\n   wrap(\"bmm\",\n        cname(\"baddbmm\"),\n        {{name=Tensor, default=true, returned=true, method={default='nil'},\n          init=function(arg)\n                  return table.concat(\n                     {\n                        arg.__metatable.init(arg),\n                        string.format(\"TH%s_resize3d(%s, %s->size[0], %s->size[1], %s->size[2]);\",\n                                      Tensor, arg:carg(), arg.args[5]:carg(), arg.args[5]:carg(), arg.args[6]:carg())\n                     }, '\\n')\n               end,\n          precall=function(arg)\n                  return table.concat(\n                     {\n                        string.format(\"TH%s_zero(%s);\", Tensor, arg:carg()),\n                        arg.__metatable.precall(arg)\n                     }, '\\n')\n               end,\n       },\n         {name=real, default=0, invisible=true},\n         {name=Tensor, default=1, invisible=true},\n         {name=real, default=1, invisible=true},\n         {name=Tensor, dim=3},\n         {name=Tensor, dim=3}}\n     )\n\n   wrap(\"ger\",\n        cname(\"addr\"),\n        {{name=Tensor, default=true, returned=true, method={default='nil'},\n          init=function(arg)\n                  return table.concat(\n                     {\n                        arg.__metatable.init(arg),\n                        string.format(\"TH%s_resize2d(%s, %s->size[0], %s->size[0]);\", Tensor, arg:carg(), arg.args[5]:carg(), arg.args[6]:carg())\n                     }, '\\n')\n               end,\n          precall=function(arg)\n                     return table.concat(\n                        {\n                           string.format(\"TH%s_zero(%s);\", Tensor, arg:carg()),\n                           arg.__metatable.precall(arg)\n                        }, '\\n')\n                  end\n       },\n        {name=real, default=1, invisible=true},\n        {name=Tensor, default=1, invisible=true},\n        {name=real, default=1, invisible=true},\n        {name=Tensor, dim=1},\n        {name=Tensor, dim=1}}\n     )\n\n   for _,f in ipairs({\n                        {name=\"addmv\",   dim1=1, dim2=2, dim3=1},\n                        {name=\"addmm\",   dim1=2, dim2=2, dim3=2},\n                        {name=\"addr\",    dim1=2, dim2=1, dim3=1},\n                        {name=\"addbmm\",  dim1=2, dim2=3, dim3=3},\n                        {name=\"baddbmm\", dim1=3, dim2=3, dim3=3},\n                     }\n                  ) do\n\n      interface:wrap(f.name,\n                     cname(f.name),\n                     {{name=Tensor, default=true, returned=true},\n                      {name=real, default=1},\n                      {name=Tensor, dim=f.dim1},\n                      {name=real, default=1},\n                      {name=Tensor, dim=f.dim2},\n                      {name=Tensor, dim=f.dim3}})\n\n      -- there is an ambiguity here, hence the more complicated setup\n      method:wrap(f.name,\n                  cname(f.name),\n                  {{name=Tensor, returned=true, dim=f.dim1},\n                   {name=real, default=1, invisible=true},\n                   {name=Tensor, default=1, dim=f.dim1},\n                   {name=real, default=1},\n                   {name=Tensor, dim=f.dim2},\n                   {name=Tensor, dim=f.dim3}},\n                  cname(f.name),\n                  {{name=Tensor, returned=true, dim=f.dim1},\n                   {name=real},\n                   {name=Tensor, default=1, dim=f.dim1},\n                   {name=real},\n                   {name=Tensor, dim=f.dim2},\n                   {name=Tensor, dim=f.dim3}})\n   end\n\n   wrap(\"numel\",\n        cname(\"numel\"),\n        {{name=Tensor},\n         {name=\"ptrdiff_t\", creturned=true}})\n\n   for _,name in ipairs({\"cumsum\", \"cumprod\"}) do\n      wrap(name,\n           cname(name),\n           {{name=Tensor, default=true, returned=true},\n            {name=Tensor},\n            {name=\"index\", default=1}})\n   end\n\n   wrap(\"sum\",\n        cname(\"sumall\"),\n        {{name=Tensor},\n         {name=accreal, creturned=true}},\n        cname(\"sum\"),\n        {{name=Tensor, default=true, returned=true},\n         {name=Tensor},\n         {name=\"index\"},\n         {name=\"boolean\", default=true, invisible=true}})\n\n   wrap(\"prod\",\n        cname(\"prodall\"),\n        {{name=Tensor},\n         {name=accreal, creturned=true}},\n        cname(\"prod\"),\n        {{name=Tensor, default=true, returned=true},\n         {name=Tensor},\n         {name=\"index\"},\n         {name=\"boolean\", default=true, invisible=true}})\n\n   for _,name in ipairs({\"min\", \"max\"}) do\n      wrap(name,\n           cname(name .. \"all\"),\n           {{name=Tensor},\n            {name=real, creturned=true}},\n           cname(name),\n           {{name=Tensor, default=true, returned=true},\n            {name=\"IndexTensor\", default=true, returned=true, noreadadd=true},\n            {name=Tensor},\n            {name=\"index\"},\n            {name=\"boolean\", default=true, invisible=true}})\n   end\n\n   for _,name in ipairs({\"cmin\", \"cmax\"}) do\n      wrap(name,\n           cname(name),\n           {{name=Tensor, default=true, returned=true},\n            {name=Tensor, method={default=1}},\n            {name=Tensor}},\n           cname(name .. \"Value\"),\n           {{name=Tensor, default=true, returned=true},\n            {name=Tensor, method={default=1}},\n            {name=real}})\n   end\n\n   wrap(\"trace\",\n        cname(\"trace\"),\n        {{name=Tensor},\n         {name=accreal, creturned=true}})\n\n   wrap(\"cross\",\n        cname(\"cross\"),\n        {{name=Tensor, default=true, returned=true},\n         {name=Tensor},\n         {name=Tensor},\n         {name=\"index\", default=0}})\n\n   wrap(\"diag\",\n        cname(\"diag\"),\n        {{name=Tensor, default=true, returned=true},\n         {name=Tensor},\n         {name=\"long\", default=0}})\n\n   wrap(\"eye\",\n        cname(\"eye\"),\n        {{name=Tensor, default=true, returned=true, method={default='nil'}},\n         {name=\"long\"},\n         {name=\"long\", default=0}})\n\n   wrap(\"range\",\n        cname(\"range\"),\n        {{name=Tensor, default=true, returned=true, method={default='nil'}},\n         {name=accreal},\n         {name=accreal},\n         {name=accreal, default=1}})\n\n   wrap(\"randperm\",\n        cname(\"randperm\"),\n        {{name=Tensor, default=true, returned=true, method={default='nil'},\n          postcall=function(arg)\n                      return table.concat(\n                         {\n                            arg.__metatable.postcall(arg),\n                            string.format(\"TH%s_add(%s, %s, 1);\", Tensor, arg:carg(), arg:carg())\n                         }, '\\n')\n                   end},\n         {name=\"Generator\", default=true},\n         {name=\"long\"}})\n\n   wrap(\"sort\",\n        cname(\"sort\"),\n        {{name=Tensor, default=true, returned=true},\n         {name=\"IndexTensor\", default=true, returned=true, noreadadd=true},\n         {name=Tensor},\n         {name=\"index\", default=lastdim(3)},\n         {name=\"boolean\", default=0}})\n\nwrap(\"topk\",\n     cname(\"topk\"),\n     {{name=Tensor, default=true, returned=true},\n        {name=\"IndexTensor\", default=true, returned=true, noreadadd=true},\n        {name=Tensor},\n        {name=\"long\", default=1},\n        {name=\"index\", default=lastdim(3)},\n        {name=\"boolean\", default=0},\n        {name=\"boolean\", default=0}})\n\n   wrap(\"kthvalue\",\n        cname(\"kthvalue\"),\n        {{name=Tensor, default=true, returned=true},\n         {name=\"IndexTensor\", default=true, returned=true, noreadadd=true},\n         {name=Tensor},\n         {name=\"long\"},\n         {name=\"index\", default=lastdim(3)},\n         {name=\"boolean\", default=true, invisible=true}})\n\n   wrap(\"mode\",\n       cname(\"mode\"),\n       {{name=Tensor, default=true, returned=true},\n           {name=\"IndexTensor\", default=true, returned=true, noreadadd=true},\n           {name=Tensor},\n           {name=\"index\", default=lastdim(3)},\n           {name=\"boolean\", default=true, invisible=true}})\n\n   wrap(\"median\",\n        cname(\"median\"),\n        {{name=Tensor, default=true, returned=true},\n         {name=\"IndexTensor\", default=true, returned=true, noreadadd=true},\n         {name=Tensor},\n         {name=\"index\", default=lastdim(3)},\n         {name=\"boolean\", default=true, invisible=true}})\n\n   wrap(\"tril\",\n        cname(\"tril\"),\n        {{name=Tensor, default=true, returned=true},\n         {name=Tensor},\n         {name=\"int\", default=0}})\n\n   wrap(\"triu\",\n        cname(\"triu\"),\n        {{name=Tensor, default=true, returned=true},\n         {name=Tensor},\n         {name=\"int\", default=0}})\n\n   wrap(\"cat\",\n        cname(\"cat\"),\n        {{name=Tensor, default=true, returned=true},\n         {name=Tensor},\n         {name=Tensor},\n         {name=\"index\", default=-1}},\n        cname(\"catArray\"),\n        {{name=Tensor, default=true, returned=true},\n         {name=Tensor .. \"Array\"},\n         {name=\"index\", default=-1}})\n\n   if Tensor == 'ByteTensor' then -- we declare this only once\n      interface:print(\n         [[\nstatic long THRandom_random2__(THGenerator *gen, long a, long b)\n{\n  THArgCheck(b >= a, 2, \"upper bound must be larger than lower bound\");\n  return((THRandom_random(gen) % (b+1-a)) + a);\n}\n\nstatic long THRandom_random1__(THGenerator *gen, long b)\n{\n  THArgCheck(b > 0, 1, \"upper bound must be strictly positive\");\n  return(THRandom_random(gen) % b + 1);\n}\n         ]])\n   end\n\n   interface:print(string.gsub(\n                      [[\nstatic void THTensor_random2__(THTensor *self, THGenerator *gen, long a, long b)\n{\n  THArgCheck(b >= a, 2, \"upper bound must be larger than lower bound\");\n  TH_TENSOR_APPLY(real, self, *self_data = ((THRandom_random(gen) % (b+1-a)) + a);)\n}\n\nstatic void THTensor_random1__(THTensor *self, THGenerator *gen, long b)\n{\n  THArgCheck(b > 0, 1, \"upper bound must be strictly positive\");\n  TH_TENSOR_APPLY(real, self, *self_data = (THRandom_random(gen) % b + 1);)\n}\n]], 'Tensor', Tensor):gsub('real', real))\n\n   wrap('random',\n        'THRandom_random2__',\n        {{name='Generator', default=true},\n         {name='long'},\n         {name='long'},\n         {name='long', creturned=true}},\n        'THRandom_random1__',\n        {{name='Generator', default=true},\n         {name='long'},\n         {name='long', creturned=true}},\n        'THRandom_random',\n        {{name='Generator', default=true},\n         {name='long', creturned=true}},\n        cname(\"random2__\"),\n        {{name=Tensor, returned=true},\n         {name='Generator', default=true},\n         {name='long'},\n         {name='long'}},\n        cname(\"random1__\"),\n        {{name=Tensor, returned=true},\n         {name='Generator', default=true},\n         {name='long'}},\n        cname(\"random\"),\n        {{name=Tensor, returned=true},\n         {name='Generator', default=true}})\n\n   wrap(\"geometric\",\n     \"THRandom_geometric\",\n     {{name=\"Generator\", default=true},\n      {name=\"double\"},\n      {name=\"double\", creturned=true}},\n     cname(\"geometric\"),\n     {{name=Tensor, returned=true},\n      {name=\"Generator\", default=true},\n      {name=\"double\"}})\n\n   wrap(\"bernoulli\",\n      \"THRandom_bernoulli\",\n      {{name=\"Generator\", default=true},\n       {name=\"double\", default=0.5},\n       {name=\"double\", creturned=true}},\n      cname(\"bernoulli\"),\n      {{name=Tensor, returned=true},\n       {name=\"Generator\", default=true},\n       {name=\"double\", default=0.5}},\n      cname(\"bernoulli_FloatTensor\"),\n      {{name=Tensor, returned=true},\n       {name=\"Generator\", default=true},\n       {name=\"FloatTensor\"}},\n      cname(\"bernoulli_DoubleTensor\"),\n      {{name=Tensor, returned=true},\n       {name=\"Generator\", default=true},\n       {name=\"DoubleTensor\"}})\n\n   wrap(\"squeeze\",\n        cname(\"squeeze\"),\n        {{name=Tensor, default=true, returned=true, postcall=function(arg)\n                                                                local txt = {}\n                                                                if arg.returned then\n                                                                   table.insert(txt, string.format('if(arg%d->nDimension == 1 && arg%d->size[0] == 1)', arg.i, arg.i)) -- number\n                                                                   table.insert(txt, string.format('lua_pushnumber(L, (lua_Number)(*TH%s_data(arg%d)));', Tensor, arg.i))\n                                                                end\n                                                                return table.concat(txt, '\\n')\n                                                             end},\n         {name=Tensor}},\n        cname(\"squeeze1d\"),\n        {{name=Tensor, default=true, returned=true,\n\n          postcall=\n             function(arg)\n                local txt = {}\n                if arg.returned then\n                   table.insert(txt, string.format('if(!hasdims && arg%d->nDimension == 1 && arg%d->size[0] == 1)', arg.i, arg.i)) -- number\n                   table.insert(txt, string.format('lua_pushnumber(L, (lua_Number)(*TH%s_data(arg%d)));}', Tensor, arg.i))\n                end\n                return table.concat(txt, '\\n')\n             end},\n\n         {name=Tensor,\n\n          precall=\n             function(arg)\n                return string.format('{int hasdims = arg%d->nDimension > 1;', arg.i)\n             end},\n\n         {name=\"index\"}})\n\n   wrap(\"sign\",\n        cname(\"sign\"),\n        {{name=Tensor, default=true, returned=true, method={default='nil'}},\n         {name=Tensor, method={default=1}}})\n\n   wrap(\"conv2\",\n        cname(\"conv2Dmul\"),\n        {{name=Tensor, default=true, returned=true},\n         {name=real, default=0, invisible=true},\n         {name=real, default=1, invisible=true},\n         {name=Tensor, dim=2},\n         {name=Tensor, dim=2},\n         {name=real, default=1, invisible=true},\n         {name=real, default=1, invisible=true},\n         {name='charoption', values={'V', 'F'}, default='V'},\n         {name='charoption', default=\"C\", invisible=true}},\n        cname(\"conv2Dcmul\"),\n        {{name=Tensor, default=true, returned=true},\n         {name=real, default=0, invisible=true},\n         {name=real, default=1, invisible=true},\n         {name=Tensor, dim=3},\n         {name=Tensor, dim=3},\n         {name=real, default=1, invisible=true},\n         {name=real, default=1, invisible=true},\n         {name='charoption', values={'V', 'F'}, default='V'},\n         {name='charoption', default=\"C\", invisible=true}},\n        cname(\"conv2Dmv\"),\n        {{name=Tensor, default=true, returned=true},\n         {name=real, default=0, invisible=true},\n         {name=real, default=1, invisible=true},\n         {name=Tensor, dim=3},\n         {name=Tensor, dim=4},\n         {name=real, default=1, invisible=true},\n         {name=real, default=1, invisible=true},\n         {name='charoption', values={'V', 'F'}, default='V'},\n         {name='charoption', default=\"C\", invisible=true}}\n     )\n\n   wrap(\"xcorr2\",\n        cname(\"conv2Dmul\"),\n        {{name=Tensor, default=true, returned=true},\n         {name=real, default=0, invisible=true},\n         {name=real, default=1, invisible=true},\n         {name=Tensor, dim=2},\n         {name=Tensor, dim=2},\n         {name=real, default=1, invisible=true},\n         {name=real, default=1, invisible=true},\n         {name='charoption', values={'V', 'F'}, default='V'},\n         {name='charoption', default=\"X\", invisible=true}},\n        cname(\"conv2Dcmul\"),\n        {{name=Tensor, default=true, returned=true},\n         {name=real, default=0, invisible=true},\n         {name=real, default=1, invisible=true},\n         {name=Tensor, dim=3},\n         {name=Tensor, dim=3},\n         {name=real, default=1, invisible=true},\n         {name=real, default=1, invisible=true},\n         {name='charoption', values={'V', 'F'}, default='V'},\n         {name='charoption', default=\"X\", invisible=true}},\n        cname(\"conv2Dmv\"),\n        {{name=Tensor, default=true, returned=true},\n         {name=real, default=0, invisible=true},\n         {name=real, default=1, invisible=true},\n         {name=Tensor, dim=3},\n         {name=Tensor, dim=4},\n         {name=real, default=1, invisible=true},\n         {name=real, default=1, invisible=true},\n         {name='charoption', values={'V', 'F'}, default='V'},\n         {name='charoption', default=\"X\", invisible=true}}\n     )\n\n   wrap(\"conv3\",\n        cname(\"conv3Dmul\"),\n        {{name=Tensor, default=true, returned=true},\n         {name=real, default=0, invisible=true},\n         {name=real, default=1, invisible=true},\n         {name=Tensor, dim=3},\n         {name=Tensor, dim=3},\n         {name=real, default=1, invisible=true},\n         {name=real, default=1, invisible=true},\n         {name=real, default=1, invisible=true},\n         {name='charoption', values={'V', 'F'}, default='V'},\n         {name='charoption', default=\"C\", invisible=true}},\n        cname(\"conv3Dcmul\"),\n        {{name=Tensor, default=true, returned=true},\n         {name=real, default=0, invisible=true},\n         {name=real, default=1, invisible=true},\n         {name=Tensor, dim=4},\n         {name=Tensor, dim=4},\n         {name=real, default=1, invisible=true},\n         {name=real, default=1, invisible=true},\n         {name=real, default=1, invisible=true},\n         {name='charoption', values={'V', 'F'}, default='V'},\n         {name='charoption', default=\"C\", invisible=true}},\n        cname(\"conv3Dmv\"),\n        {{name=Tensor, default=true, returned=true},\n         {name=real, default=0, invisible=true},\n         {name=real, default=1, invisible=true},\n         {name=Tensor, dim=4},\n         {name=Tensor, dim=5},\n         {name=real, default=1, invisible=true},\n         {name=real, default=1, invisible=true},\n         {name=real, default=1, invisible=true},\n         {name='charoption', values={'V', 'F'}, default='V'},\n         {name='charoption', default=\"C\", invisible=true}}\n     )\n\n   wrap(\"xcorr3\",\n        cname(\"conv3Dmul\"),\n        {{name=Tensor, default=true, returned=true},\n         {name=real, default=0, invisible=true},\n         {name=real, default=1, invisible=true},\n         {name=Tensor, dim=3},\n         {name=Tensor, dim=3},\n         {name=real, default=1, invisible=true},\n         {name=real, default=1, invisible=true},\n         {name=real, default=1, invisible=true},\n         {name='charoption', values={'V', 'F'}, default='V'},\n         {name='charoption', default=\"X\", invisible=true}},\n        cname(\"conv3Dcmul\"),\n        {{name=Tensor, default=true, returned=true},\n         {name=real, default=0, invisible=true},\n         {name=real, default=1, invisible=true},\n         {name=Tensor, dim=4},\n         {name=Tensor, dim=4},\n         {name=real, default=1, invisible=true},\n         {name=real, default=1, invisible=true},\n         {name=real, default=1, invisible=true},\n         {name='charoption', values={'V', 'F'}, default='V'},\n         {name='charoption', default=\"X\", invisible=true}},\n        cname(\"conv3Dmv\"),\n        {{name=Tensor, default=true, returned=true},\n         {name=real, default=0, invisible=true},\n         {name=real, default=1, invisible=true},\n         {name=Tensor, dim=4},\n         {name=Tensor, dim=5},\n         {name=real, default=1, invisible=true},\n         {name=real, default=1, invisible=true},\n         {name=real, default=1, invisible=true},\n         {name='charoption', values={'V', 'F'}, default='V'},\n         {name='charoption', default=\"X\", invisible=true}}\n     )\n\n   for _,name in pairs({'lt','gt','le','ge','eq','ne'}) do\n      wrap(name,\n           cname(name .. 'Value'),\n           {{name='ByteTensor',default=true, returned=true},\n            {name=Tensor},\n            {name=real}},\n           cname(name .. 'ValueT'),\n           {{name=Tensor, returned=true},\n            {name=Tensor},\n            {name=real}},\n           cname(name .. 'Tensor'),\n           {{name='ByteTensor',default=true, returned=true},\n            {name=Tensor},\n            {name=Tensor}},\n           cname(name .. 'TensorT'),\n           {{name=Tensor, returned=true},\n            {name=Tensor},\n            {name=Tensor}})\n   end\n\n   wrap(\"nonzero\",\n        cname(\"nonzero\"),\n        {{name=\"IndexTensor\", default=true, returned=true},\n         {name=Tensor}})\n  end  -- ~= HalfTensor\n\n   if Tensor == 'ByteTensor' then\n     -- Logical accumulators only apply to ByteTensor\n      for _,name in ipairs({'all', 'any'}) do\n        wrap(name,\n             cname('logical' .. name),\n             {{name=Tensor},\n\t\t{name=\"boolean\", creturned=true}})\n      end\n   end\n\n   if Tensor == 'IntTensor' then\n         wrap(\"abs\",\n              cname(\"abs\"),\n              {{name=Tensor, default=true, returned=true, method={default='nil'}},\n               {name=Tensor, method={default=1}}},\n              \"abs\",\n              {{name=real},\n               {name=real, creturned=true}})\n   elseif Tensor == 'LongTensor' then\n         wrap(\"abs\",\n              cname(\"abs\"),\n              {{name=Tensor, default=true, returned=true, method={default='nil'}},\n               {name=Tensor, method={default=1}}},\n              \"labs\",\n              {{name=real},\n               {name=real, creturned=true}})\n   end\n\n   if Tensor == 'FloatTensor' or Tensor == 'DoubleTensor' then\n\n      wrap(\"mean\",\n           cname(\"meanall\"),\n           {{name=Tensor},\n            {name=accreal, creturned=true}},\n           cname(\"mean\"),\n           {{name=Tensor, default=true, returned=true},\n            {name=Tensor},\n            {name=\"index\"},\n            {name=\"boolean\", default=true, invisible=true}})\n\n      for _,name in ipairs({\"var\", \"std\"}) do\n         wrap(name,\n              cname(name .. \"all\"),\n              {{name=Tensor},\n               {name=\"boolean\", default=false},\n               {name=accreal, creturned=true}\n              },\n              cname(name),\n              {{name=Tensor, default=true, returned=true},\n               {name=Tensor},\n               {name=\"index\"},\n               {name=\"boolean\", default=false},\n               {name=\"boolean\", default=true, invisible=true}})\n      end\n      wrap(\"histc\",\n           cname(\"histc\"),\n           {{name=Tensor, default=true, returned=true},\n            {name=Tensor},\n            {name=\"long\",default=100},\n            {name=\"double\",default=0},\n            {name=\"double\",default=0}})\n\n      wrap(\"bhistc\",\n           cname(\"bhistc\"),\n           {{name=Tensor, default=true, returned=true},\n            {name=Tensor},\n            {name=\"long\",default=100},\n            {name=\"double\",default=0},\n            {name=\"double\",default=0}})\n\n      wrap(\"norm\",\n           cname(\"normall\"),\n           {{name=Tensor},\n            {name=real, default=2},\n            {name=accreal, creturned=true}},\n           cname(\"norm\"),\n           {{name=Tensor, default=true, returned=true},\n            {name=Tensor},\n            {name=real},\n            {name=\"index\"},\n            {name=\"boolean\", default=true, invisible=true}})\n\n      wrap(\"renorm\",\n           cname(\"renorm\"),\n           {{name=Tensor, default=true, returned=true, method={default='nil'}},\n            {name=Tensor, method={default=1}},\n            {name=real},\n            {name=\"index\"},\n            {name=real}})\n\n      wrap(\"dist\",\n           cname(\"dist\"),\n           {{name=Tensor},\n            {name=Tensor},\n            {name=real, default=2},\n            {name=accreal, creturned=true}})\n\n      wrap(\"linspace\",\n           cname(\"linspace\"),\n           {{name=Tensor, default=true, returned=true, method={default='nil'}},\n            {name=real},\n            {name=real},\n            {name=\"long\", default=100}})\n\n      wrap(\"logspace\",\n           cname(\"logspace\"),\n           {{name=Tensor, default=true, returned=true, method={default='nil'}},\n            {name=real},\n            {name=real},\n            {name=\"long\", default=100}})\n\n      for _,name in ipairs({\"log\", \"log1p\", \"exp\",\n                            \"cos\", \"acos\", \"cosh\",\n                            \"sin\", \"asin\", \"sinh\",\n                            \"tan\", \"atan\", \"tanh\",\n                            \"sqrt\", \"round\", \"ceil\",\n                            \"floor\", \"trunc\", }) do\n         wrap(name,\n              cname(name),\n              {{name=Tensor, default=true, returned=true, method={default='nil'}},\n               {name=Tensor, method={default=1}}},\n              name,\n              {{name=real},\n               {name=real, creturned=true}})\n      end\n\n      wrap(\"abs\",\n           cname(\"abs\"),\n           {{name=Tensor, default=true, returned=true, method={default='nil'}},\n            {name=Tensor, method={default=1}}},\n           \"fabs\",\n           {{name=real},\n            {name=real, creturned=true}})\n\n      wrap(\"frac\",\n           cname(\"frac\"),\n           {{name=Tensor, default=true, returned=true, method={default='nil'}},\n            {name=Tensor, method={default=1}}},\n           \"TH_frac\",\n           {{name=real},\n            {name=real, creturned=true}})\n\n      wrap(\"rsqrt\",\n           cname(\"rsqrt\"),\n           {{name=Tensor, default=true, returned=true, method={default='nil'}},\n            {name=Tensor, method={default=1}}},\n           \"TH_rsqrt\",\n           {{name=real},\n            {name=real, creturned=true}})\n\n      wrap(\"sigmoid\",\n           cname(\"sigmoid\"),\n           {{name=Tensor, default=true, returned=true, method={default='nil'}},\n            {name=Tensor, method={default=1}}},\n           \"TH_sigmoid\",\n           {{name=real},\n            {name=real, creturned=true}})\n\n      wrap(\"neg\",\n           cname(\"neg\"),\n           {{name=Tensor, default=true, returned=true, method={default='nil'}},\n            {name=Tensor, method={default=1}}})\n\n      wrap(\"cinv\",\n           cname(\"cinv\"),\n           {{name=Tensor, default=true, returned=true, method={default='nil'}},\n            {name=Tensor, method={default=1}}})\n\n      wrap(\"lerp\",\n           cname(\"lerp\"),\n           {{name=Tensor, default=true, returned=true, method={default='nil'}},\n            {name=Tensor, method={default=1}},\n            {name=Tensor},\n            {name=real}},\n           \"TH_lerp\",\n           {{name=real},\n            {name=real},\n            {name=real},\n            {name=real, creturned=true}})\n\n      wrap(\"atan2\",\n           cname(\"atan2\"),\n           {{name=Tensor, default=true, returned=true, method={default='nil'}},\n            {name=Tensor, method={default=1}},\n            {name=Tensor}},\n           \"atan2\",\n           {{name=real},\n            {name=real},\n            {name=real, creturned=true}})\n\n      wrap(\"pow\",\n           cname(\"pow\"),\n           {{name=Tensor, default=true, returned=true, method={default='nil'}},\n            {name=Tensor, method={default=1}},\n            {name=real}},\n           cname(\"tpow\"),\n           {{name=Tensor, default=true, returned=true, method={default='nil'}},\n            {name=real},\n            {name=Tensor, method={default=1}}},\n           \"pow\",\n           {{name=real},\n            {name=real},\n            {name=real, creturned=true}})\n\n      wrap(\"rand\",\n           cname(\"rand\"),\n           {{name=Tensor, default=true, returned=true, method={default='nil'}},\n            {name='Generator', default=true},\n            {name=\"LongArg\"}})\n\n      wrap(\"randn\",\n           cname(\"randn\"),\n           {{name=Tensor, default=true, returned=true, method={default='nil'}},\n            {name='Generator', default=true},\n            {name=\"LongArg\"}})\n\n      wrap(\"multinomial\",\n           cname(\"multinomial\"),\n           {{name=\"IndexTensor\", default=true, returned=true, method={default='nil'}},\n              {name='Generator', default=true},\n              {name=Tensor},\n              {name=\"int\"},\n              {name=\"boolean\", default=false}})\n      \n      wrap(\"multinomialAliasSetup_\",\n           cname(\"multinomialAliasSetup\"),\n           {{name=Tensor},\n              {name=\"IndexTensor\", default=true, returned=true, method={default='nil'}},\n              {name=Tensor, default=true, returned=true, method={default='nil'}}})\n      \n      wrap(\"multinomialAlias_\",\n           cname(\"multinomialAliasDraw\"),\n           {{name=\"IndexTensor\", default=true, returned=true, method={default='nil'}},\n              {name='Generator', default=true},\n              {name=\"IndexTensor\"},\n              {name=Tensor}\n              })\n      \n      for _,f in ipairs({{name='uniform', a=0, b=1},\n            {name='normal', a=0, b=1},\n            {name='cauchy', a=0, b=1},\n            {name='logNormal', a=1, b=2}}) do\n         \n         wrap(f.name,\n              string.format(\"THRandom_%s\", f.name),\n              {{name='Generator', default=true},\n               {name=\"double\", default=f.a},\n               {name=\"double\", default=f.b},\n               {name=\"double\", creturned=true}},\n              cname(f.name),\n              {{name=Tensor, returned=true},\n               {name='Generator', default=true},\n               {name=real, default=f.a},\n               {name=real, default=f.b}})\n      end\n\n      for _,f in ipairs({{name='exponential'}}) do\n\n         wrap(f.name,\n              string.format(\"THRandom_%s\", f.name),\n              {{name='Generator', default=true},\n               {name=\"double\", default=f.a},\n               {name=\"double\", creturned=true}},\n              cname(f.name),\n              {{name=Tensor, returned=true},\n               {name='Generator', default=true},\n               {name=real, default=f.a}})\n      end\n\n      for _,name in ipairs({\"gesv\",\"gels\"}) do\n         interface:wrap(name,\n                        cname(name),\n                        {{name=Tensor, returned=true},\n                         {name=Tensor, returned=true},\n                         {name=Tensor},\n                         {name=Tensor}},\n                        cname(name),\n                        {{name=Tensor, default=true, returned=true, invisible=true},\n                         {name=Tensor, default=true, returned=true, invisible=true},\n                         {name=Tensor},\n                         {name=Tensor}}\n                     )\n      end\n      interface:wrap(\"trtrs\",\n                     cname(\"trtrs\"),\n                     {{name=Tensor, returned=true},\n                      {name=Tensor, returned=true},\n                      {name=Tensor},\n                      {name=Tensor},\n                      {name='charoption', values={'U', 'L'}, default='U'},  -- uplo\n                      {name='charoption', values={'N', 'T'}, default='N'},  -- trans\n                      {name='charoption', values={'N', 'U'}, default='N'}}, -- diag\n                     cname(\"trtrs\"),\n                     {{name=Tensor, default=true, returned=true, invisible=true},\n                      {name=Tensor, default=true, returned=true, invisible=true},\n                      {name=Tensor},\n                      {name=Tensor},\n                      {name='charoption', values={'U', 'L'}, default='U'},  -- uplo\n                      {name='charoption', values={'N', 'T'}, default='N'},  -- trans\n                      {name='charoption', values={'N', 'U'}, default='N'}}  -- diag\n                  )\n\n      interface:wrap(\"symeig\",\n                     cname(\"syev\"),\n                     {{name=Tensor, returned=true},\n                      {name=Tensor, returned=true},\n                      {name=Tensor},\n                      {name='charoption', values={'N', 'V'}, default='N'},\n                      {name='charoption', values={'U', 'L'}, default='U'}},\n                     cname(\"syev\"),\n                     {{name=Tensor, default=true, returned=true, invisible=true},\n                      {name=Tensor, default=true, returned=true, invisible=true},\n                      {name=Tensor},\n                      {name='charoption', values={'N', 'V'}, default='N'},\n                      {name='charoption', values={'U', 'L'}, default='U'}}\n                  )\n      interface:wrap(\"eig\",\n                     cname(\"geev\"),\n                     {{name=Tensor, returned=true},\n                      {name=Tensor, returned=true},\n                      {name=Tensor},\n                      {name='charoption', values={'N', 'V'}, default='N'}},\n                     cname(\"geev\"),\n                     {{name=Tensor, default=true, returned=true, invisible=true},\n                      {name=Tensor, default=true, returned=true, invisible=true},\n                      {name=Tensor},\n                      {name='charoption', values={'N', 'V'}, default='N'}}\n                  )\n\n      interface:wrap(\"svd\",\n                     cname(\"gesvd\"),\n                     {{name=Tensor, returned=true},\n                      {name=Tensor, returned=true},\n                      {name=Tensor, returned=true},\n                      {name=Tensor},\n                      {name='charoption', values={'A', 'S'}, default='S'}},\n                     cname(\"gesvd\"),\n                     {{name=Tensor, default=true, returned=true, invisible=true},\n                      {name=Tensor, default=true, returned=true, invisible=true},\n                      {name=Tensor, default=true, returned=true, invisible=true},\n                      {name=Tensor},\n                      {name='charoption', values={'A', 'S'}, default='S'}}\n                  )\n      interface:wrap(\"inverse\",\n                     cname(\"getri\"),\n                     {{name=Tensor, returned=true},\n                      {name=Tensor}},\n                     cname(\"getri\"),\n                     {{name=Tensor, default=true, returned=true, invisible=true},\n                      {name=Tensor}}\n                  )\n      interface:wrap(\"potrf\",\n                     cname(\"potrf\"),\n                     {{name=Tensor, returned=true},\n                      {name=Tensor},\n                      {name='charoption', values={'U', 'L'}, default='U'}}, -- uplo\n                     cname(\"potrf\"),\n                     {{name=Tensor, default=true, returned=true, invisible=true},\n                      {name=Tensor},\n                      {name='charoption', values={'U', 'L'}, default='U'}}\n                  )\n      interface:wrap(\"potrs\",\n                     cname(\"potrs\"),\n                     {{name=Tensor, returned=true},\n                      {name=Tensor},\n                      {name=Tensor},\n                      {name='charoption', values={'U', 'L'}, default='U'}}, -- uplo\n                     cname(\"potrs\"),\n                     {{name=Tensor, default=true, returned=true, invisible=true},\n                      {name=Tensor},\n                      {name=Tensor},\n                      {name='charoption', values={'U', 'L'}, default='U'}}\n                  )\n      interface:wrap(\"potri\",\n                     cname(\"potri\"),\n                     {{name=Tensor, returned=true},\n                      {name=Tensor},\n                      {name='charoption', values={'U', 'L'}, default='U'}}, -- uplo\n                     cname(\"potri\"),\n                     {{name=Tensor, default=true, returned=true, invisible=true},\n                      {name=Tensor},\n                      {name='charoption', values={'U', 'L'}, default='U'}} -- uplo\n                    )\n      interface:wrap(\"pstrf\",\n                     cname(\"pstrf\"),\n                     {{name=Tensor, returned=true},\n                      {name='IntTensor', returned=true},\n                      {name=Tensor},\n                      {name='charoption', values={'U', 'L'}, default='U'},  -- uplo\n                      {name=real, default=-1}},\n                     cname(\"pstrf\"),\n                     {{name=Tensor, default=true, returned=true, invisible=true},\n                      {name='IntTensor', default=true, returned=true, invisible=true},\n                      {name=Tensor},\n                      {name='charoption', values={'U', 'L'}, default='U'},  -- uplo\n                      {name=real, default=-1}}\n                  )\n      interface:wrap(\"qr\",\n                     cname(\"qr\"),\n                     {{name=Tensor, returned=true},\n                      {name=Tensor, returned=true},\n                      {name=Tensor}},\n                     cname(\"qr\"),\n                     {{name=Tensor, default=true, returned=true, invisible=true},\n                      {name=Tensor, default=true, returned=true, invisible=true},\n                      {name=Tensor}}\n                  )\n      interface:wrap(\"geqrf\",\n                     cname(\"geqrf\"),\n                     {{name=Tensor, returned=true},\n                      {name=Tensor, returned=true},\n                      {name=Tensor}},\n                     cname(\"geqrf\"),\n                     {{name=Tensor, default=true, returned=true, invisible=true},\n                      {name=Tensor, default=true, returned=true, invisible=true},\n                      {name=Tensor}}\n                  )\n      interface:wrap(\"orgqr\",\n                     cname(\"orgqr\"),\n                     {{name=Tensor, returned=true},\n                      {name=Tensor},\n                      {name=Tensor}},\n                     cname(\"orgqr\"),\n                     {{name=Tensor, default=true, returned=true, invisible=true},\n                      {name=Tensor},\n                      {name=Tensor}}\n                  )\n      interface:wrap(\"ormqr\",\n                     cname(\"ormqr\"),\n                     {{name=Tensor, returned=true},\n                      {name=Tensor},\n                      {name=Tensor},\n                      {name=Tensor},\n                      {name='charoption', values={'L', 'R'}, default='L'},\n                      {name='charoption', values={'N', 'T'}, default='N'}},\n                     cname(\"ormqr\"),\n                     {{name=Tensor, default=true, returned=true, invisible=true},\n                      {name=Tensor},\n                      {name=Tensor},\n                      {name=Tensor},\n                      {name='charoption', values={'L', 'R'}, default='L'},\n                      {name='charoption', values={'N', 'T'}, default='N'}}\n                  )\n   end\n\n   method:register(string.format(\"m_torch_%sMath__\", Tensor))\n   interface:print(method:tostring())\n   method:clearhistory()\n   interface:register(string.format(\"torch_%sMath__\", Tensor))\n\n   interface:print(string.gsub([[\nstatic void torch_TensorMath_init(lua_State *L)\n{\n  luaT_pushmetatable(L, \"torch.Tensor\");\n\n  /* register methods */\n  luaT_setfuncs(L, m_torch_TensorMath__, 0);\n\n  /* register functions into the \"torch\" field of the tensor metaclass */\n  lua_pushstring(L, \"torch\");\n  lua_newtable(L);\n  luaT_setfuncs(L, torch_TensorMath__, 0);\n  lua_rawset(L, -3);\n  lua_pop(L, 1);\n}\n]], 'Tensor', Tensor))\nend\n\ninterface:dispatchregister(\"torch_TensorMath__\")\n\ninterface:print([[\nvoid torch_TensorMath_init(lua_State *L)\n{\n  torch_ByteTensorMath_init(L);\n  torch_CharTensorMath_init(L);\n  torch_ShortTensorMath_init(L);\n  torch_IntTensorMath_init(L);\n  torch_LongTensorMath_init(L);\n  torch_FloatTensorMath_init(L);\n  torch_DoubleTensorMath_init(L);\n  luaT_setfuncs(L, torch_TensorMath__, 0);\n}\n]])\n\nif arg[1] then\n   interface:tofile(arg[1])\nelse\n   print(interface:tostring())\nend\n"
        },
        {
          "name": "TensorOperator.c",
          "type": "blob",
          "size": 0.28515625,
          "content": "#include \"general.h\"\n\n#define torch_TensorOperator_(NAME) TH_CONCAT_4(torch_,Real,TensorOperator_,NAME)\n#define torch_Tensor_id TH_CONCAT_3(torch_,Real,Tensor_id)\n#define torch_Tensor TH_CONCAT_STRING_3(torch.,Real,Tensor)\n\n#include \"generic/TensorOperator.c\"\n#include \"THGenerateAllTypes.h\"\n"
        },
        {
          "name": "TestSuite.lua",
          "type": "blob",
          "size": 0.6865234375,
          "content": "function torch.TestSuite()\n   local obj = {\n      __tests = {},\n      __isTestSuite = true\n   }\n\n   local metatable = {}\n\n   function metatable:__index(key)\n      return self.__tests[key]\n   end\n\n   function metatable:__newindex(key, value)\n      if self.__tests[key] ~= nil then\n         error(\"Test \" .. tostring(key) .. \" is already defined.\")\n      end\n      if type(value) ~= \"function\" then\n         if type(value) == \"table\" then\n            error(\"Nested tables of tests are not supported\")\n         else\n            error(\"Only functions are supported as members of a TestSuite\")\n         end\n      end\n      self.__tests[key] = value\n   end\n\n   setmetatable(obj, metatable)\n\n   return obj\nend\n"
        },
        {
          "name": "Tester.lua",
          "type": "blob",
          "size": 27.6904296875,
          "content": "\n-- Lua 5.2 compatibility\nlocal unpack = unpack or table.unpack\n\nlocal check = {} -- helper functions, defined at the bottom of the file\n\nlocal Tester = torch.class('torch.Tester')\n\nfunction Tester:__init()\n   self.errors = {}\n   self.tests = {}\n   self.warnings = {}\n   self._warningCount = {}\n   self.disabledTests = {}\n   self._currentTestName = ''\n\n   -- To maintain backwards compatibility (at least for a short while),\n   -- disable exact dimension checking of tensors when :assertTensorEq is\n   -- called. Thus {{1}} == {1} when this flag is true.\n   --\n   -- Note that other methods that suppose tensor checking (such as\n   -- :assertGeneralEq) ignore this flag, since previously they didn't\n   -- exist or support tensor equality checks at all, so there is no\n   -- old code that uses these functions and relies on the behaviour.\n   --\n   -- Note also that if the dimension check fails with this flag is true, then\n   -- will show a warning.\n   self._assertTensorEqIgnoresDims = true\nend\n\nfunction Tester:setEarlyAbort(earlyAbort)\n   self.earlyAbort = earlyAbort\nend\n\nfunction Tester:setRethrowErrors(rethrow)\n   self.rethrow = rethrow\nend\n\nfunction Tester:setSummaryOnly(summaryOnly)\n   self.summaryOnly = summaryOnly\nend\n\n-- Add a success to the test.\nfunction Tester:_success()\n   local name = self._currentTestName\n   self.assertionPass[name] = self.assertionPass[name] + 1\n   return true\nend\n\nfunction Tester:_addDebugInfo(message)\n   local ss = debug.traceback('tester', 3) or ''\n   ss = ss:match('.-\\n([^\\n]+\\n[^\\n]+)\\n[^\\n]+xpcall') or ''\n   local name = self._currentTestName\n   return (name ~= '' and name .. '\\n' or '') .. message .. '\\n' .. ss\nend\n\n-- Add a failure to the test.\nfunction Tester:_failure(message)\n   if self.rethrow then error(message, 2) end\n   local name = self._currentTestName\n   self.assertionFail[name] = self.assertionFail[name] + 1\n   self.errors[#self.errors + 1] = self:_addDebugInfo(message)\n   return false\nend\n\n-- Add a warning to the test\nfunction Tester:_warning(message)\n   local name = self._currentTestName\n   self._warningCount[name] = (self._warningCount[name] or 0) + 1\n   self.warnings[#self.warnings + 1] = self:_addDebugInfo(message)\nend\n\n-- Call this during a test run with `condition = true` to log a success, or with\n-- `condition = false` to log a failure (using `message`).\nfunction Tester:_assert_sub(condition, message)\n   if condition then\n      return self:_success()\n   else\n      return self:_failure(message)\n   end\nend\n\nlocal function getMessage(message, ...)\n   assert(next{...} == nil, \"Unexpected arguments passed to test function\")\n   if message then\n      assert(type(message) == 'string', 'message parameter must be a string')\n      if message ~= '' then\n         return message .. '\\n'\n      end\n   end\n   return ''\nend\n\n--[[ Historically, some test functions have accepted both a message and a\ntolerance, and some just a message (e.g., assertTableEq). Now assertTableEq\naccepts both a tolerance and a message, so allow the two arguments to be passed\nin either order to maintain backwards compatibility (and more generally,\nfor convenience). (We still document the ordering as \"tolerance, message\" for\nclarity.) This function also sanitizes them (ensures they are non-nil, etc).\n]]\nlocal function getToleranceAndMessage(defaultTolerance, ...)\n   local args = {...}\n   local message = nil\n   local tolerance = nil\n   for _, a in ipairs(args) do\n      if type(a) == 'string' then\n         if message then\n            error(\"Unexpected string argument; already have message\", a)\n         end\n         message = a .. '\\n'\n      elseif type(a) == 'number' then\n         if tolerance then\n            error(\"Unexpected number argument; already have tolerance\", a)\n         end\n         tolerance = a\n         assert(tolerance >= 0, \"tolerance cannot be negative\")\n      else\n         error(\"Unrecognized argument; should be a tolerance or message\", a)\n      end\n   end\n   message = message or ''\n   tolerance = tolerance or defaultTolerance\n   return tolerance, message\nend\n\nfunction Tester:assert(condition, ...)\n   local message = getMessage(...)\n   if type(condition) ~= 'boolean' then\n      self:_warning(\" :assert should only be used for boolean conditions. \"\n                    .. \"To check for non-nil variables, do this explicitly: \"\n                    .. \"Tester:assert(var ~= nil).\")\n   end\n   return self:_assert_sub(condition,\n                           string.format('%sBOOL violation condition=%s',\n                                         message, tostring(condition)))\nend\n\nfunction Tester:assertGeneralEq(got, expected, ...)\n   return self:_eqOrNeq(got, expected, false, ...)\nend\n\nfunction Tester:eq(got, expected, ...)\n   return self:assertGeneralEq(got, expected, ...)\nend\n\nfunction Tester:assertGeneralNe(got, unexpected, ...)\n   return self:_eqOrNeq(got, unexpected, true, ...)\nend\n\nfunction Tester:ne(got, unexpected, ...)\n   return self:assertGeneralNe(got, unexpected, ...)\nend\n\nfunction Tester:_eqOrNeq(got, expected, negate, ...)\n   local tolerance, message = getToleranceAndMessage(0, ...)\n   local success, subMessage = check.areEq(got, expected, tolerance, negate)\n   subMessage = subMessage or ''\n   return self:_assert_sub(success, message .. subMessage)\nend\n\nfunction Tester:assertlt(a, b, ...)\n   local message = getMessage(...)\n   return self:_assert_sub(a < b,\n                           string.format('%sLT failed: %s >= %s',\n                                         message, tostring(a), tostring(b)))\nend\n\nfunction Tester:assertgt(a, b, ...)\n   local message = getMessage(...)\n   return self:_assert_sub(a > b,\n                           string.format('%sGT failed: %s <= %s',\n                                         message, tostring(a), tostring(b)))\nend\n\nfunction Tester:assertle(a, b, ...)\n   local message = getMessage(...)\n   return self:_assert_sub(a <= b,\n                           string.format('%sLE failed: %s > %s',\n                                         message, tostring(a), tostring(b)))\nend\n\nfunction Tester:assertge(a, b, ...)\n   local message = getMessage(...)\n   return self:_assert_sub(a >= b,\n                           string.format('%sGE failed: %s < %s',\n                                         message, tostring(a), tostring(b)))\nend\n\nfunction Tester:assertalmosteq(a, b, ...)\n   local tolerance, message = getToleranceAndMessage(1e-16, ...)\n   local diff = math.abs(a - b)\n   return self:_assert_sub(\n         diff <= tolerance,\n         string.format(\n               '%sALMOST_EQ failed: %s ~= %s with tolerance=%s',\n               message, tostring(a), tostring(b), tostring(tolerance)))\nend\n\nfunction Tester:asserteq(a, b, ...)\n   local message = getMessage(...)\n   return self:_assert_sub(a == b,\n                           string.format('%sEQ failed: %s ~= %s',\n                                         message, tostring(a), tostring(b)))\nend\n\nfunction Tester:assertne(a, b, ...)\n   local message = getMessage(...)\n   if type(a) == type(b) and type(a) == 'table' or type(a) == 'userdata' then\n      self:_warning(\" :assertne should only be used to compare basic lua \"\n                    .. \"objects (numbers, booleans, etc). Consider using \"\n                    .. \"either :assertGeneralNe or :assert(a ~= b).\")\n   end\n   return self:_assert_sub(a ~= b,\n                           string.format('%sNE failed: %s == %s',\n                                         message, tostring(a), tostring(b)))\nend\n\nfunction Tester:assertTensorEq(ta, tb, ...)\n  return self:_assertTensorEqOrNeq(ta, tb, false, ...)\nend\n\nfunction Tester:assertTensorNe(ta, tb, ...)\n  return self:_assertTensorEqOrNeq(ta, tb, true, ...)\nend\n\nfunction Tester:_assertTensorEqOrNeq(ta, tb, negate, ...)\n   assert(torch.isTensor(ta), \"First argument should be a Tensor\")\n   assert(torch.isTensor(tb), \"Second argument should be a Tensor\")\n\n   local tolerance, message = getToleranceAndMessage(0, ...)\n   local success, subMessage =\n         check.areTensorsEq(ta, tb, tolerance, negate,\n                            self._assertTensorEqIgnoresDims)\n   subMessage = subMessage or ''\n\n   if self._assertTensorEqIgnoresDims and (not negate) and success\n         and not ta:isSameSizeAs(tb) then\n     self:_warning(\"Tensors have the same content but different dimensions. \"\n                   .. \"For backwards compatibility, they are considered equal, \"\n                   .. \"but this may change in the future. Consider using :eq \"\n                   .. \"to check for equality instead.\")\n   end\n\n   return self:_assert_sub(success, message .. subMessage)\nend\n\nfunction Tester:assertTableEq(ta, tb, ...)\n   return self:_assertTableEqOrNeq(ta, tb, false, ...)\nend\n\nfunction Tester:assertTableNe(ta, tb, ...)\n   return self:_assertTableEqOrNeq(ta, tb, true, ...)\nend\n\nfunction Tester:_assertTableEqOrNeq(ta, tb, negate, ...)\n   assert(type(ta) == 'table', \"First argument should be a Table\")\n   assert(type(tb) == 'table', \"Second argument should be a Table\")\n   return self:_eqOrNeq(ta, tb, negate, ...)\nend\n\nfunction Tester:assertError(f, ...)\n   return self:assertErrorObj(f, function() return true end, ...)\nend\n\nfunction Tester:assertNoError(f, ...)\n   local message = getMessage(...)\n   local status, err = pcall(f)\n   return self:_assert_sub(status,\n                           string.format('%sERROR violation: err=%s', message,\n                                         tostring(err)))\nend\n\nfunction Tester:assertErrorMsg(f, errmsg, ...)\n   return self:assertErrorObj(f, function(err) return err == errmsg end, ...)\nend\n\nfunction Tester:assertErrorPattern(f, errPattern, ...)\n   local function errcomp(err)\n      return string.find(err, errPattern) ~= nil\n   end\n   return self:assertErrorObj(f, errcomp, ...)\nend\n\nfunction Tester:assertErrorObj(f, errcomp, ...)\n   local message = getMessage(...)\n   local status, err = pcall(f)\n   return self:_assert_sub((not status) and errcomp(err),\n                           string.format('%sERROR violation: err=%s', message,\n                                         tostring(err)))\nend\n\nfunction Tester:add(f, name)\n   if type(f) == \"table\" then\n      assert(name == nil, \"Name parameter is forbidden for a table of tests, \"\n                          .. \"since its use is ambiguous\")\n      if f.__isTestSuite then\n         f = f.__tests\n      else\n         self:_warning(\"Should use TestSuite rather than plain lua table\")\n      end\n      for i, v in pairs(f) do\n         -- We forbid nested tests because the \"expected\" behaviour when a named\n         -- test is run in the case that the named test is in fact a table of\n         -- tests is not supported. Similar issue with _setUp and _tearDown\n         -- functions inside nested tests.\n         assert(type(v) ~= 'table', \"Nested sets of tests are not supported\")\n         self:add(v, i)\n      end\n      return self\n   end\n\n   assert(type(f) == 'function',\n          \"Only tables of functions and functions supported\")\n\n   if name == '_setUp' then\n      assert(not self._setUp, \"Only one set-up function allowed\")\n      self._setUp = f\n   elseif name == '_tearDown' then\n      assert(not self._tearDown, \"Only one tear-down function allowed\")\n      self._tearDown = f\n   else\n      name = name or 'unknown'\n      if self.tests[name] ~= nil then\n         error('Test with name ' .. name .. ' already exists!')\n      end\n      self.tests[name] = f\n   end\n   return self\nend\n\nfunction Tester:disable(testNames)\n   if type(testNames) == 'string' then\n      testNames = {testNames}\n   end\n   assert(type(testNames) == 'table', \"Expecting name or list for disable\")\n   for _, name in ipairs(testNames) do\n      assert(self.tests[name], \"Unrecognized test '\" .. name .. \"'\")\n      self.disabledTests[name] = true\n   end\n   return self\nend\n\nfunction Tester:run(testNames)\n   local tests = self:_getTests(testNames)\n   self.assertionPass = {}\n   self.assertionFail = {}\n   self.haveWarning = {}\n   self.testError = {}\n   for name in pairs(tests) do\n      self.assertionPass[name] = 0\n      self.assertionFail[name] = 0\n      self.testError[name] = 0\n      self._warningCount[name] = 0\n   end\n   self:_run(tests)\n   self:_report(tests)\n\n   -- Throws an error on test failure/error, so that test script returns\n   -- with nonzero return value.\n   for name in pairs(tests) do\n      assert(self.assertionFail[name] == 0,\n             'An error was found while running tests!')\n      assert(self.testError[name] == 0,\n             'An error was found while running tests!')\n   end\n\n   return 0\nend\n\nlocal function pluralize(num, str)\n   local stem = num .. ' ' .. str\n   if num == 1 then\n      return stem\n   else\n      return stem .. 's'\n   end\nend\n\nlocal NCOLS = 80\nlocal coloured\nlocal enable_colors, c = pcall(require, 'sys.colors')\nif arg and enable_colors then  -- have we been invoked from the commandline?\n   coloured = function(str, colour)\n      return colour .. str .. c.none\n   end\nelse\n   c = {}\n   coloured = function(str)\n      return str\n   end\nend\n\nfunction Tester:_run(tests)\n   local ntests = 0\n   for _ in pairs(tests) do\n      ntests = ntests + 1\n   end\n\n   local ntestsAsString = string.format('%u', ntests)\n   local cfmt = string.format('%%%uu/%u ', ntestsAsString:len(), ntestsAsString)\n   local cfmtlen = ntestsAsString:len() * 2 + 2\n\n   local function bracket(str)\n      return '[' .. str .. ']'\n   end\n\n   io.write('Running ' .. pluralize(ntests, 'test') .. '\\n')\n   local i = 1\n   for name, fn in pairs(tests) do\n      self._currentTestName = name\n\n      -- TODO: compute max length of name and cut it down to size if needed\n      local strinit = coloured(string.format(cfmt, i), c.cyan)\n                      .. self._currentTestName .. ' '\n                      .. string.rep('.',\n                                    NCOLS - 6 - 2 -\n                                    cfmtlen - self._currentTestName:len())\n                      .. ' '\n      io.write(strinit .. bracket(coloured('WAIT', c.cyan)))\n      io.flush()\n\n      local status, message, pass, skip\n      if self.disabledTests[name] then\n         skip = true\n      else\n         skip = false\n         if self._setUp then\n            self._setUp(name)\n         end\n         if self.rethrow then\n            status = true\n            local nerr = #self.errors\n            message = fn()\n            pass = nerr == #self.errors\n         else\n            status, message, pass = self:_pcall(fn)\n         end\n         if self._tearDown then\n            self._tearDown(name)\n         end\n      end\n\n      io.write('\\r')\n      io.write(strinit)\n\n      if skip then\n         io.write(bracket(coloured('SKIP', c.yellow)))\n      elseif not status then\n         self.testError[name] = 1\n         io.write(bracket(coloured('ERROR', c.magenta)))\n      elseif not pass then\n         io.write(bracket(coloured('FAIL', c.red)))\n      else\n         io.write(bracket(coloured('PASS', c.green)))\n         if self._warningCount[name] > 0 then\n            io.write('\\n' .. string.rep(' ', NCOLS - 10))\n            io.write(bracket(coloured('+warning', c.yellow)))\n         end\n      end\n      io.write('\\n')\n      io.flush()\n\n      if self.earlyAbort and (i < ntests) and (not status or not pass)\n            and (not skip) then\n         io.write('Aborting on first error, not all tests have been executed\\n')\n         break\n      end\n\n      i = i + 1\n\n      collectgarbage()\n   end\nend\n\nfunction Tester:_pcall(f)\n   local nerr = #self.errors\n   local stat, result = xpcall(f, debug.traceback)\n   if not stat then\n      self.errors[#self.errors + 1] =\n         self._currentTestName .. '\\n Function call failed\\n' .. result .. '\\n'\n   end\n   return stat, result, stat and (nerr == #self.errors)\nend\n\nfunction Tester:_getTests(testNames)\n   if testNames == nil then\n      return self.tests\n   end\n   if type(testNames) == 'string' then\n      testNames = {testNames}\n   end\n   assert(type(testNames) == 'table',\n          \"Only accept a name or table of test names (or nil for all tests)\")\n\n   local function getMatchingNames(pattern)\n      local matchingNames = {}\n      for name in pairs(self.tests) do\n         if string.match(name, pattern) then\n            table.insert(matchingNames, name)\n         end\n      end\n      return matchingNames\n   end\n\n   local tests = {}\n   for _, pattern in ipairs(testNames) do\n      local matchingNames = getMatchingNames(pattern)\n      assert(#matchingNames > 0, \"Couldn't find test '\" .. pattern .. \"'\")\n      for _, name in ipairs(matchingNames) do\n         tests[name] = self.tests[name]\n      end\n   end\n   return tests\nend\n\nfunction Tester:_report(tests)\n   local ntests = 0\n   local nfailures = 0\n   local nerrors = 0\n   local nskipped = 0\n   local nwarnings = 0\n   self.countasserts = 0\n   for name in pairs(tests) do\n      ntests = ntests + 1\n      self.countasserts = self.countasserts + self.assertionFail[name]\n                          + self.assertionPass[name]\n      if self.assertionFail[name] > 0 then\n         nfailures = nfailures + 1\n      end\n      if self.testError[name] > 0 then\n         nerrors = nerrors + 1\n      end\n      if self._warningCount[name] > 0 then\n         nwarnings = nwarnings + 1\n      end\n      if self.disabledTests[name] then\n         nskipped = nskipped + 1\n      end\n   end\n   if self._warningCount[''] then\n      nwarnings = nwarnings + self._warningCount['']\n   end\n\n   io.write('Completed ' .. pluralize(self.countasserts, 'assert'))\n   io.write(' in ' .. pluralize(ntests, 'test') .. ' with ')\n   io.write(coloured(pluralize(nfailures, 'failure'),\n                     nfailures == 0 and c.green or c.red))\n   io.write(' and ')\n   io.write(coloured(pluralize(nerrors, 'error'),\n                     nerrors == 0 and c.green or c.magenta))\n   if nwarnings > 0 then\n      io.write(' and ')\n      io.write(coloured(pluralize(nwarnings, 'warning'), c.yellow))\n   end\n   if nskipped > 0 then\n      io.write(' and ')\n      io.write(coloured(nskipped .. ' disabled', c.yellow))\n   end\n   io.write('\\n')\n\n   -- Prints off a message separated by -----\n   local haveSection = false\n   local function addSection(text)\n      local function printDashes()\n         io.write(string.rep('-', NCOLS) .. '\\n')\n      end\n      if not haveSection then\n         printDashes()\n         haveSection = true\n      end\n      io.write(text .. '\\n')\n      printDashes()\n   end\n\n   if not self.summaryOnly then\n      for _, v in ipairs(self.errors) do\n         addSection(v)\n      end\n      for _, v in ipairs(self.warnings) do\n         addSection(v)\n      end\n   end\nend\n\n\n--[[ Tests for tensor equality between two tensors of matching sizes and types.\n\nTests whether the maximum element-wise difference between `ta` and `tb` is less\nthan or equal to `tolerance`.\n\nArguments:\n* `ta` (tensor)\n* `tb` (tensor)\n* `tolerance` (number) maximum elementwise difference between `ta` and `tb`.\n* `negate` (boolean) if true, we invert success and failure.\n* `storage` (boolean) if true, we print an error message referring to Storages\n    rather than Tensors.\n\nReturns:\n1. success, boolean that indicates success\n2. failure_message, string or nil\n]]\nfunction check.areSameFormatTensorsEq(ta, tb, tolerance, negate, storage)\n   local function ensureHasAbs(t)\n      -- Byte, Char and Short Tensors don't have abs\n      return t.abs and t or t:double()\n   end\n\n   ta = ensureHasAbs(ta)\n   tb = ensureHasAbs(tb)\n\n   local diff = ta:clone():add(-1, tb):abs()\n   local err = diff:max()\n   local success = err <= tolerance\n   if negate then\n      success = not success\n   end\n\n   local errMessage\n   if not success then\n      local prefix = storage and 'Storage' or 'Tensor'\n      local violation = negate and 'NE(==)' or 'EQ(==)'\n      errMessage = string.format('%s%s violation: max diff=%s, tolerance=%s',\n                                 prefix,\n                                 violation,\n                                 tostring(err),\n                                 tostring(tolerance))\n   end\n\n   return success, errMessage\nend\n\n--[[ Tests for tensor equality.\n\nTests whether the maximum element-wise difference between `ta` and `tb` is less\nthan or equal to `tolerance`.\n\nArguments:\n* `ta` (tensor)\n* `tb` (tensor)\n* `tolerance` (number) maximum elementwise difference between `ta` and `tb`.\n* `negate` (boolean) if negate is true, we invert success and failure.\n* `ignoreTensorDims` (boolean, default false) if true, then tensors of the same\n    size but different dimensions can still be considered equal, e.g.,\n    {{1}} == {1}. For backwards compatibility.\n\nReturns:\n1. success, boolean that indicates success\n2. failure_message, string or nil\n]]\nfunction check.areTensorsEq(ta, tb, tolerance, negate, ignoreTensorDims)\n   ignoreTensorDims = ignoreTensorDims or false\n\n   if not ignoreTensorDims and ta:dim() ~= tb:dim() then\n      return negate, 'The tensors have different dimensions'\n   end\n\n   if ta:type() ~= tb:type() then\n      return negate, 'The tensors have different types'\n   end\n\n   -- If we are comparing two empty tensors, return true.\n   -- This is needed because some functions below cannot be applied to tensors\n   -- of dimension 0.\n   if ta:dim() == 0 and tb:dim() == 0 then\n      return not negate, 'Both tensors are empty'\n   end\n\n   local sameSize\n   if ignoreTensorDims then\n      sameSize = ta:nElement() == tb:nElement()\n   else\n      sameSize = ta:isSameSizeAs(tb)\n   end\n   if not sameSize then\n      return negate, 'The tensors have different sizes'\n   end\n\n   return check.areSameFormatTensorsEq(ta, tb, tolerance, negate, false)\nend\n\nlocal typesMatching = {\n      ['torch.ByteStorage'] = torch.ByteTensor,\n      ['torch.CharStorage'] = torch.CharTensor,\n      ['torch.ShortStorage'] = torch.ShortTensor,\n      ['torch.IntStorage'] = torch.IntTensor,\n      ['torch.LongStorage'] = torch.LongTensor,\n      ['torch.FloatStorage'] = torch.FloatTensor,\n      ['torch.DoubleStorage'] = torch.DoubleTensor,\n      ['torch.HalfStorage'] = torch.HalfTensor,\n}\n\n--[[ Tests for storage equality.\n\nTests whether the maximum element-wise difference between `sa` and `sb` is less\nthan or equal to `tolerance`.\n\nArguments:\n* `sa` (storage)\n* `sb` (storage)\n* `tolerance` (number) maximum elementwise difference between `a` and `b`.\n* `negate` (boolean) if negate is true, we invert success and failure.\n\nReturns:\n1. success, boolean that indicates success\n2. failure_message, string or nil\n]]\nfunction check.areStoragesEq(sa, sb, tolerance, negate)\n   if sa:size() ~= sb:size() then\n      return negate, 'The storages have different sizes'\n   end\n\n   local typeOfsa = torch.type(sa)\n   local typeOfsb = torch.type(sb)\n\n   if typeOfsa ~= typeOfsb then\n      return negate, 'The storages have different types'\n   end\n\n   local ta = typesMatching[typeOfsa](sa)\n   local tb = typesMatching[typeOfsb](sb)\n\n   return check.areSameFormatTensorsEq(ta, tb, tolerance, negate, true)\nend\n\n--[[ Tests for general (deep) equality.\n\nThe types of `got` and `expected` must match.\nTables are compared recursively. Keys and types of the associated values must\nmatch, recursively. Numbers are compared with the given tolerance.\nTorch tensors and storages are compared with the given tolerance on their\nelementwise difference. Other types are compared for strict equality with the\nregular Lua == operator.\n\nArguments:\n* `got`\n* `expected`\n* `tolerance` (number) maximum elementwise difference between `a` and `b`.\n* `negate` (boolean) if negate is true, we invert success and failure.\n\nReturns:\n1. success, boolean that indicates success\n2. failure_message, string or nil\n]]\nfunction check.areEq(got, expected, tolerance, negate)\n   local errMessage\n   if type(got) ~= type(expected) then\n      if not negate then\n         errMessage = 'EQ failed: values have different types (first: '\n                      .. type(got) .. ', second: ' .. type(expected) .. ')'\n      end\n      return negate, errMessage\n   elseif type(got) == 'number' then\n      local diff = math.abs(got - expected)\n      local ok = (diff <= tolerance)\n      if negate then\n         ok = not ok\n      end\n      if not ok then\n         if negate then\n            errMessage = string.format(\"NE failed: %s == %s\",\n                                       tostring(got), tostring(expected))\n         else\n            errMessage = string.format(\"EQ failed: %s ~= %s\",\n                                       tostring(got), tostring(expected))\n         end\n         if tolerance > 0 then\n            errMessage = errMessage .. \" with tolerance=\" .. tostring(tolerance)\n         end\n      end\n      return ok, errMessage\n   elseif type(expected) == \"table\" then\n     return check.areTablesEq(got, expected, tolerance, negate)\n   elseif torch.isTensor(got) then\n     return check.areTensorsEq(got, expected, tolerance, negate)\n   elseif torch.isStorage(got) then\n     return check.areStoragesEq(got, expected, tolerance, negate)\n   else\n     -- Below: we have the same type which is either userdata or a lua type\n     -- which is not a number.\n     local ok = (got == expected)\n     if negate then\n        ok = not ok\n     end\n     if not ok then\n        if negate then\n           errMessage = string.format(\"NE failed: %s (%s) == %s (%s)\",\n                                      tostring(got), type(got),\n                                      tostring(expected), type(expected))\n        else\n           errMessage = string.format(\"EQ failed: %s (%s) ~= %s (%s)\",\n                                      tostring(got), type(got),\n                                      tostring(expected), type(expected))\n        end\n     end\n     return ok, errMessage\n   end\nend\n\n--[[ Tests for (deep) table equality.\n\nTables are compared recursively. Keys and types of the associated values must\nmatch, recursively. Numbers are compared with the given tolerance.\nTorch tensors and storages are compared with the given tolerance on their\nelementwise difference. Other types are compared for strict equality with the\nregular Lua == operator.\n\nArguments:\n* `t1` (table)\n* `t2` (table)\n* `tolerance` (number) maximum elementwise difference between `a` and `b`.\n* `negate` (boolean) if negate is true, we invert success and failure.\n\nReturns:\n1. success, boolean that indicates success\n2. failure_message, string or nil\n]]\nfunction check.areTablesEq(t1, t2, tolerance, negate)\n   -- Implementation detail: Instead of doing a depth-first table comparison\n   -- check (for example, using recursion), let's do a breadth-first search\n   -- using a queue. Why? Because if we have two tables that are quite deep\n   -- (e.g., a gModule from nngraph), then if they are different then it's\n   -- more useful to the user to show how they differ at as-shallow-a-depth\n   -- as possible.\n   local queue = {}\n   queue._head = 1\n   queue._tail = 1\n   function queue.isEmpty()\n      return queue._tail == queue._head\n   end\n   function queue.pop()\n      queue._head = queue._head + 1\n      return queue[queue._head - 1]\n   end\n   function queue.push(value)\n      queue[queue._tail] = value\n      queue._tail = queue._tail + 1\n   end\n\n   queue.push({t1, t2})\n   while not queue.isEmpty() do\n      local location\n      t1, t2, location = unpack(queue.pop())\n\n      local function toSublocation(key)\n         local keyAsString = tostring(key)\n         return (location and location .. \".\" .. keyAsString) or keyAsString\n      end\n\n      for key, value1 in pairs(t1) do\n         local sublocation = toSublocation(key)\n         if t2[key] == nil then\n            return negate, string.format(\n                  \"Entry %s missing in second table (is %s in first)\",\n                  sublocation, tostring(value1))\n         end\n         local value2 = t2[key]\n         if type(value1) == 'table' and type(value2) == 'table' then\n            queue.push({value1, value2, sublocation})\n         else\n            local ok, message = check.areEq(value1, value2, tolerance, false)\n            if not ok then\n               message = 'At table location ' .. sublocation .. ': ' .. message\n               return negate, message\n            end\n         end\n      end\n\n      for key, value2 in pairs(t2) do\n         local sublocation = toSublocation(key)\n         if t1[key] == nil then\n             return negate, string.format(\n                   \"Entry %s missing in first table (is %s in second)\",\n                   sublocation, tostring(value2))\n         end\n      end\n   end\n   return not negate, 'The tables are equal'\nend\n"
        },
        {
          "name": "Timer.c",
          "type": "blob",
          "size": 4.62109375,
          "content": "#include \"general.h\"\n\n#ifdef _WIN32\n\n#include <windows.h>\n#include <assert.h>\n#define TimeType __int64\nstatic __declspec( thread ) TimeType ticksPerSecond = 0;\n\n/*\n * There is an example of getrusage for windows in following link:\n * https://github.com/openvswitch/ovs/blob/master/lib/getrusage-windows.c\n */\n\n#else\n\n#include <sys/time.h>\n#include <sys/resource.h>\n#define TimeType double\n\n#endif\n\ntypedef struct _Timer\n{\n    int isRunning;\n\n    TimeType totalrealtime;\n    TimeType totalusertime;\n    TimeType totalsystime;\n\n    TimeType startrealtime;\n    TimeType startusertime;\n    TimeType startsystime;\n} Timer;\n\nstatic TimeType torch_Timer_realtime()\n{\n#ifdef _WIN32\n  TimeType current;\n  QueryPerformanceCounter(&current);\n  return current;\n#else\n  struct timeval current;\n  gettimeofday(&current, NULL);\n  return (current.tv_sec + current.tv_usec/1000000.0);\n#endif\n}\n\nstatic TimeType torch_Timer_usertime()\n{\n#ifdef _WIN32\n  return torch_Timer_realtime();\n#else\n  struct rusage current;\n  getrusage(RUSAGE_SELF, &current);\n  return (current.ru_utime.tv_sec + current.ru_utime.tv_usec/1000000.0);\n#endif\n}\n\nstatic TimeType torch_Timer_systime()\n{\n#ifdef _WIN32\n  return 0;\n#else\n  struct rusage current;\n  getrusage(RUSAGE_SELF, &current);\n  return (current.ru_stime.tv_sec + current.ru_stime.tv_usec/1000000.0);\n#endif\n}\n\nstatic int torch_Timer_new(lua_State *L)\n{\n#ifdef _WIN32\n  if (ticksPerSecond == 0)\n  {\n    assert(sizeof(LARGE_INTEGER) == sizeof(__int64));\n    QueryPerformanceFrequency(&ticksPerSecond);\n  }\n#endif\n  Timer *timer = luaT_alloc(L, sizeof(Timer));\n  timer->isRunning = 1;\n  timer->totalrealtime = 0;\n  timer->totalusertime = 0;\n  timer->totalsystime = 0;\n  timer->startrealtime = torch_Timer_realtime();\n  timer->startusertime = torch_Timer_usertime();\n  timer->startsystime = torch_Timer_systime();\n  luaT_pushudata(L, timer, \"torch.Timer\");\n  return 1;\n}\n\nstatic int torch_Timer_reset(lua_State *L)\n{\n  Timer *timer = luaT_checkudata(L, 1, \"torch.Timer\");\n  timer->totalrealtime = 0;\n  timer->totalusertime = 0;\n  timer->totalsystime = 0;\n  timer->startrealtime = torch_Timer_realtime();\n  timer->startusertime = torch_Timer_usertime();\n  timer->startsystime = torch_Timer_systime();\n  lua_settop(L, 1);\n  return 1;\n}\n\nstatic int torch_Timer_free(lua_State *L)\n{\n  Timer *timer = luaT_checkudata(L, 1, \"torch.Timer\");\n  luaT_free(L, timer);\n  return 0;\n}\n\nstatic int torch_Timer_stop(lua_State *L)\n{\n  Timer *timer = luaT_checkudata(L, 1, \"torch.Timer\");\n  if(timer->isRunning)  \n  {\n    TimeType realtime = torch_Timer_realtime() - timer->startrealtime;\n    TimeType usertime = torch_Timer_usertime() - timer->startusertime;\n    TimeType systime = torch_Timer_systime() - timer->startsystime;\n    timer->totalrealtime += realtime;\n    timer->totalusertime += usertime;\n    timer->totalsystime += systime;\n    timer->isRunning = 0;\n  }\n  lua_settop(L, 1);\n  return 1;  \n}\n\nstatic int torch_Timer_resume(lua_State *L)\n{\n  Timer *timer = luaT_checkudata(L, 1, \"torch.Timer\");\n  if(!timer->isRunning)\n  {\n    timer->isRunning = 1;\n    timer->startrealtime = torch_Timer_realtime();\n    timer->startusertime = torch_Timer_usertime();\n    timer->startsystime = torch_Timer_systime();\n  }\n  lua_settop(L, 1);\n  return 1;  \n}\n\nstatic int torch_Timer_time(lua_State *L)\n{\n  Timer *timer = luaT_checkudata(L, 1, \"torch.Timer\");\n  double realtime = (timer->isRunning ? (timer->totalrealtime + torch_Timer_realtime() - timer->startrealtime) : timer->totalrealtime);\n  double usertime = (timer->isRunning ? (timer->totalusertime + torch_Timer_usertime() - timer->startusertime) : timer->totalusertime);\n  double systime = (timer->isRunning ? (timer->totalsystime + torch_Timer_systime() - timer->startsystime) : timer->totalsystime);\n#ifdef _WIN32\n  realtime /= ticksPerSecond;\n  usertime /= ticksPerSecond;\n  systime  /= ticksPerSecond;\n#endif\n  lua_createtable(L, 0, 3);\n  lua_pushnumber(L, realtime);\n  lua_setfield(L, -2, \"real\");\n  lua_pushnumber(L, usertime);\n  lua_setfield(L, -2, \"user\");\n  lua_pushnumber(L, systime);\n  lua_setfield(L, -2, \"sys\");\n  return 1;\n}\n\nstatic int torch_Timer___tostring__(lua_State *L)\n{\n  Timer *timer = luaT_checkudata(L, 1, \"torch.Timer\");\n  lua_pushfstring(L, \"torch.Timer [status: %s]\", (timer->isRunning ? \"running\" : \"stopped\"));\n  return 1;\n}\n\nstatic const struct luaL_Reg torch_Timer__ [] = {\n  {\"reset\", torch_Timer_reset},\n  {\"stop\", torch_Timer_stop},\n  {\"resume\", torch_Timer_resume},\n  {\"time\", torch_Timer_time},\n  {\"__tostring__\", torch_Timer___tostring__},\n  {NULL, NULL}\n};\n\nvoid torch_Timer_init(lua_State *L)\n{\n  luaT_newmetatable(L, \"torch.Timer\", NULL, torch_Timer_new, torch_Timer_free, NULL);\n  luaT_setfuncs(L, torch_Timer__, 0);\n  lua_pop(L, 1);\n}\n"
        },
        {
          "name": "cmake",
          "type": "tree",
          "content": null
        },
        {
          "name": "doc",
          "type": "tree",
          "content": null
        },
        {
          "name": "general.h",
          "type": "blob",
          "size": 0.671875,
          "content": "#ifndef TORCH_GENERAL_INC\n#define TORCH_GENERAL_INC\n\n#include <stdlib.h>\n#include <string.h>\n#include <stddef.h>\n\n#include \"luaT.h\"\n#include \"TH.h\"\n\n#if (defined(_MSC_VER) || defined(__MINGW32__))\n\n#define snprintf _snprintf\n#define popen _popen\n#define pclose _pclose\n\n#endif\n\n#if LUA_VERSION_NUM >= 503\n/* one can simply enable LUA_COMPAT_5_2 to be backward compatible.\nHowever, this does not work when we are trying to use system-installed lua,\nhence these redefines\n*/\n#define luaL_optlong(L,n,d)     ((long)luaL_optinteger(L, (n), (d)))\n#define luaL_checklong(L,n)     ((long)luaL_checkinteger(L, (n)))\n#define luaL_checkint(L,n)      ((int)luaL_checkinteger(L, (n)))\n#endif\n\n#endif\n"
        },
        {
          "name": "generic",
          "type": "tree",
          "content": null
        },
        {
          "name": "init.c",
          "type": "blob",
          "size": 2.7626953125,
          "content": "#include \"general.h\"\n#include \"utils.h\"\n\nextern void torch_utils_init(lua_State *L);\nextern void torch_random_init(lua_State *L);\nextern void torch_File_init(lua_State *L);\nextern void torch_DiskFile_init(lua_State *L);\nextern void torch_MemoryFile_init(lua_State *L);\nextern void torch_PipeFile_init(lua_State *L);\nextern void torch_Timer_init(lua_State *L);\n\nextern void torch_ByteStorage_init(lua_State *L);\nextern void torch_CharStorage_init(lua_State *L);\nextern void torch_ShortStorage_init(lua_State *L);\nextern void torch_IntStorage_init(lua_State *L);\nextern void torch_LongStorage_init(lua_State *L);\nextern void torch_FloatStorage_init(lua_State *L);\nextern void torch_DoubleStorage_init(lua_State *L);\nextern void torch_HalfStorage_init(lua_State *L);\n\nextern void torch_ByteTensor_init(lua_State *L);\nextern void torch_CharTensor_init(lua_State *L);\nextern void torch_ShortTensor_init(lua_State *L);\nextern void torch_IntTensor_init(lua_State *L);\nextern void torch_LongTensor_init(lua_State *L);\nextern void torch_FloatTensor_init(lua_State *L);\nextern void torch_DoubleTensor_init(lua_State *L);\nextern void torch_HalfTensor_init(lua_State *L);\n\nextern void torch_ByteTensorOperator_init(lua_State *L);\nextern void torch_CharTensorOperator_init(lua_State *L);\nextern void torch_ShortTensorOperator_init(lua_State *L);\nextern void torch_IntTensorOperator_init(lua_State *L);\nextern void torch_LongTensorOperator_init(lua_State *L);\nextern void torch_FloatTensorOperator_init(lua_State *L);\nextern void torch_DoubleTensorOperator_init(lua_State *L);\n\n\nextern void torch_TensorMath_init(lua_State *L);\n\n\nLUA_EXTERNC DLL_EXPORT int luaopen_libtorch(lua_State *L);\n\nint luaopen_libtorch(lua_State *L)\n{\n\n  lua_newtable(L);\n  lua_pushvalue(L, -1);\n  lua_setglobal(L, \"torch\");\n\n  torch_utils_init(L);\n  torch_File_init(L);\n\n  torch_ByteStorage_init(L);\n  torch_CharStorage_init(L);\n  torch_ShortStorage_init(L);\n  torch_IntStorage_init(L);\n  torch_LongStorage_init(L);\n  torch_FloatStorage_init(L);\n  torch_DoubleStorage_init(L);\n  torch_HalfStorage_init(L);\n\n  torch_ByteTensor_init(L);\n  torch_CharTensor_init(L);\n  torch_ShortTensor_init(L);\n  torch_IntTensor_init(L);\n  torch_LongTensor_init(L);\n  torch_FloatTensor_init(L);\n  torch_DoubleTensor_init(L);\n  torch_HalfTensor_init(L);\n\n  torch_ByteTensorOperator_init(L);\n  torch_CharTensorOperator_init(L);\n  torch_ShortTensorOperator_init(L);\n  torch_IntTensorOperator_init(L);\n  torch_LongTensorOperator_init(L);\n  torch_FloatTensorOperator_init(L);\n  torch_DoubleTensorOperator_init(L);\n\n  torch_Timer_init(L);\n  torch_DiskFile_init(L);\n  torch_PipeFile_init(L);\n  torch_MemoryFile_init(L);\n\n  torch_TensorMath_init(L);\n\n  torch_random_init(L);\n\n  // Create 'torch.Allocator' type.\n  luaT_newmetatable(L, \"torch.Allocator\", NULL, NULL, NULL, NULL);\n\n  return 1;\n}\n"
        },
        {
          "name": "init.lua",
          "type": "blob",
          "size": 5.5546875,
          "content": "-- We are using paths.require to appease mkl\n\n-- Make this work with LuaJIT in Lua 5.2 compatibility mode, which\n-- renames string.gfind (already deprecated in 5.1)\nif not string.gfind then\n   string.gfind = string.gmatch\nend\nif not table.unpack then\n   table.unpack = unpack\nend\n\nrequire \"paths\"\npaths.require \"libtorch\"\n\n-- Keep track of all thread local variables torch.\n-- if a Lua VM is passed to another thread thread local\n-- variables need to be updated.\nfunction torch.updatethreadlocals()\n   torch.updateerrorhandlers()\n   local tracking = torch._heaptracking\n   if tracking == nil then tracking = false end\n   torch.setheaptracking(tracking)\nend\n\n--- package stuff\nfunction torch.packageLuaPath(name)\n   if not name then\n      local ret = string.match(torch.packageLuaPath('torch'), '(.*)/')\n      if not ret then --windows?\n         ret = string.match(torch.packageLuaPath('torch'), '(.*)\\\\')\n      end\n      return ret\n   end\n   for path in string.gmatch(package.path, \"[^;]+\") do\n      path = string.gsub(path, \"%?\", name)\n      local f = io.open(path)\n      if f then\n         f:close()\n         local ret = string.match(path, \"(.*)/\")\n         if not ret then --windows?\n            ret = string.match(path, \"(.*)\\\\\")\n         end\n         return ret\n      end\n   end\nend\n\nlocal function include(file, depth)\n   paths.dofile(file, 3 + (depth or 0))\nend\nrawset(_G, 'include', include)\n\nfunction torch.include(package, file)\n   dofile(torch.packageLuaPath(package) .. '/' .. file)\nend\n\nfunction torch.class(...)\n   local tname, parenttname, module\n   if select('#', ...) == 3\n      and type(select(1, ...)) == 'string'\n      and type(select(2, ...)) == 'string'\n      and type(select(3, ...)) == 'table'\n   then\n      tname = select(1, ...)\n      parenttname = select(2, ...)\n      module = select(3, ...)\n   elseif select('#', ...) == 2\n      and type(select(1, ...)) == 'string'\n      and type(select(2, ...)) == 'string'\n   then\n      tname = select(1, ...)\n      parenttname = select(2, ...)\n   elseif select('#', ...) == 2\n      and type(select(1, ...)) == 'string'\n      and type(select(2, ...)) == 'table'\n   then\n      tname = select(1, ...)\n      module = select(2, ...)\n   elseif select('#', ...) == 1\n      and type(select(1, ...)) == 'string'\n   then\n      tname = select(1, ...)\n   else\n      error('<class name> [<parent class name>] [<module table>] expected')\n   end\n\n   local function constructor(...)\n      local self = {}\n      torch.setmetatable(self, tname)\n      if self.__init then\n         self:__init(...)\n      end\n      return self\n   end\n\n   local function factory()\n      local self = {}\n      torch.setmetatable(self, tname)\n      return self\n   end\n\n   local mt = torch.newmetatable(tname, parenttname, constructor, nil, factory, module)\n   local mpt\n   if parenttname then\n      mpt = torch.getmetatable(parenttname)\n   end\n   return mt, mpt\nend\n\nfunction torch.setdefaulttensortype(typename)\n   assert(type(typename) == 'string', 'string expected')\n   if torch.getconstructortable(typename) then\n      torch.Tensor = torch.getconstructortable(typename)\n      torch.Storage = torch.getconstructortable(torch.typename(torch.Tensor(1):storage()))\n   else\n      error(string.format(\"<%s> is not a string describing a torch object\", typename))\n   end\nend\n\nfunction torch.type(obj)\n   local class = torch.typename(obj)\n   if not class then\n      class = type(obj)\n   end\n   return class\nend\n\n--[[ See if a given object is an instance of the provided torch class. ]]\nfunction torch.isTypeOf(obj, typeSpec)\n   -- typeSpec can be provided as either a string, pattern, or the constructor.\n   -- If the constructor is used, we look in the __typename field of the\n   -- metatable to find a string to compare to.\n   if type(typeSpec) ~= 'string' then\n      typeSpec = getmetatable(typeSpec).__typename\n\t  assert(type(typeSpec) == 'string',\n             \"type must be provided as [regexp] string, or factory\")\n   end\n\n   local mt = getmetatable(obj)\n   while mt do\n      if type(mt) == 'table' and mt.__typename then\n         local match = mt.__typename:match(typeSpec)\n         -- Require full match for non-pattern specs\n         if match and (match ~= typeSpec or match == mt.__typename) then\n            return true\n         end\n      end\n      mt = getmetatable(mt)\n   end\n   return false\nend\n\ntorch.setdefaulttensortype('torch.DoubleTensor')\n\nrequire('torch.Tensor')\nrequire('torch.File')\nrequire('torch.CmdLine')\nrequire('torch.FFInterface')\nrequire('torch.Tester')\nrequire('torch.TestSuite')\nrequire('torch.test')\nfunction torch.totable(obj)\n   if torch.isTensor(obj) or torch.isStorage(obj) then\n      return obj:totable()\n   else\n      error(\"obj must be a Storage or a Tensor\")\n   end\nend\n\nfunction torch.isTensor(obj)\n   local typename = torch.typename(obj)\n   if typename and typename:find('torch.*Tensor') then\n      return true\n   end\n   return false\nend\n\nfunction torch.isStorage(obj)\n   local typename = torch.typename(obj)\n   if typename and typename:find('torch.*Storage') then\n      return true\n   end\n   return false\nend\n-- alias for convenience\ntorch.Tensor.isTensor = torch.isTensor\n\n-- remove this line to disable automatic heap-tracking for garbage collection\ntorch.setheaptracking(true)\n\nfunction torch.multinomialAliasSetup(probs, state)\n   if torch.type(state) == 'table' then \n      state[1], state[2] = torch.multinomialAliasSetup_(probs, state[1], state[2])\n   else\n      state = {}\n      state[1], state[2] = torch.multinomialAliasSetup_(probs)\n   end\n   return state\nend\n\nfunction torch.multinomialAlias(output, state)\n   torch.DoubleTensor.multinomialAlias_(output, state[1], state[2])\n   return output\nend\n\nreturn torch\n"
        },
        {
          "name": "lib",
          "type": "tree",
          "content": null
        },
        {
          "name": "mkdocs.yml",
          "type": "blob",
          "size": 0.7158203125,
          "content": "site_name: torch7\ntheme : simplex\nrepo_url : https://github.com/torch/torch7\nuse_directory_urls : false\nmarkdown_extensions: [extra]\ndocs_dir : doc\npages:\n- [index.md, Home]\n- [tensor.md, Tensor Library, Tensor]\n- [maths.md, Tensor Library, Tensor Math]\n- [storage.md, Tensor Library, Storage]\n- [file.md, File I/O Library, File Interface]\n- [diskfile.md, File I/O Library, Disk File]\n- [memoryfile.md, File I/O Library, Memory File]\n- [pipefile.md, File I/O Library, Pipe File]\n- [serialization.md, File I/O Library, Serialization]\n- [utility.md, Useful Utilities, Class]\n- [timer.md, Useful Utilities, Timer]\n- [tester.md, Useful Utilities, Tester]\n- [cmdline.md, Useful Utilities, CmdLine]\n- [random.md, Useful Utilities, Random]\n"
        },
        {
          "name": "paths.lua.in",
          "type": "blob",
          "size": 0.353515625,
          "content": "local paths = {}\n\npaths.install_prefix = [[@Torch_INSTALL_PREFIX@]]\npaths.install_bin = [[@Torch_INSTALL_BIN@]]\npaths.install_man = [[@Torch_INSTALL_MAN@]]\npaths.install_lib = [[@Torch_INSTALL_LIB@]]\npaths.install_share = [[@Torch_INSTALL_SHARE@]]\npaths.install_include = [[@Torch_INSTALL_INCLUDE@]]\npaths.install_cmake = [[@Torch_INSTALL_CMAKE@]]\n\nreturn paths\n"
        },
        {
          "name": "random.lua",
          "type": "blob",
          "size": 1.33203125,
          "content": "local wrap = require 'cwrap'\n\nrequire 'torchcwrap'\n\nlocal interface = wrap.CInterface.new()\n\ninterface:print(\n   [[\n#include \"luaT.h\"\n#include \"TH.h\"\n\nextern void torch_Generator_init(lua_State *L);\nextern void torch_Generator_new(lua_State *L);\n   ]])\n\nfor _,name in ipairs({\"seed\", \"initialSeed\"}) do\n   interface:wrap(name,\n                  string.format(\"THRandom_%s\",name),\n                  {{name='Generator', default=true},\n                   {name=\"long\", creturned=true}})\nend\n\ninterface:wrap('manualSeed',\n               'THRandom_manualSeed',\n               {{name='Generator', default=true},\n                {name=\"long\"}})\n\ninterface:wrap('getRNGState',\n                'THByteTensor_getRNGState',\n                {{name='Generator', default=true},\n                 {name='ByteTensor',default=true,returned=true,method={default='nil'}}\n                 })\n\ninterface:wrap('setRNGState',\n                'THByteTensor_setRNGState',\n                {{name='Generator', default=true},\n                 {name='ByteTensor',default=true,returned=true,method={default='nil'}}\n                 })\n\ninterface:register(\"random__\")\n                \ninterface:print(\n   [[\nvoid torch_random_init(lua_State *L)\n{\n  torch_Generator_init(L);\n  torch_Generator_new(L);\n  lua_setfield(L, -2, \"_gen\");\n  luaT_setfuncs(L, random__, 0);\n}\n]])\n\ninterface:tofile(arg[1])\n"
        },
        {
          "name": "rocks",
          "type": "tree",
          "content": null
        },
        {
          "name": "test",
          "type": "tree",
          "content": null
        },
        {
          "name": "torchcwrap.lua",
          "type": "blob",
          "size": 18.9736328125,
          "content": "local wrap = require 'cwrap'\nlocal types = wrap.types\n\ntypes.Tensor = {\n\n   helpname = function(arg)\n                 if arg.dim then\n                    return string.format(\"Tensor~%dD\", arg.dim)\n                 else\n                    return \"Tensor\"\n                 end\n            end,\n\n   declare = function(arg)\n                local txt = {}\n                table.insert(txt, string.format(\"THTensor *arg%d = NULL;\", arg.i))\n                if arg.returned then\n                   table.insert(txt, string.format(\"int arg%d_idx = 0;\", arg.i));\n                end\n                return table.concat(txt, '\\n')\n           end,\n\n   check = function(arg, idx)\n              if arg.dim then\n                 return string.format(\"(arg%d = luaT_toudata(L, %d, torch_Tensor)) && (arg%d->nDimension == %d)\", arg.i, idx, arg.i, arg.dim)\n              else\n                 return string.format(\"(arg%d = luaT_toudata(L, %d, torch_Tensor))\", arg.i, idx)\n              end\n         end,\n\n   read = function(arg, idx)\n             if arg.returned then\n                return string.format(\"arg%d_idx = %d;\", arg.i, idx)\n             end\n          end,\n\n   init = function(arg)\n             if type(arg.default) == 'boolean' then\n                return string.format('arg%d = THTensor_(new)();', arg.i)\n             elseif type(arg.default) == 'number' then\n                return string.format('arg%d = %s;', arg.i, arg.args[arg.default]:carg())\n             else\n                error('unknown default tensor type value')\n             end\n          end,\n\n   carg = function(arg)\n             return string.format('arg%d', arg.i)\n          end,\n\n   creturn = function(arg)\n                return string.format('arg%d', arg.i)\n             end,\n\n   precall = function(arg)\n                local txt = {}\n                if arg.default and arg.returned then\n                   table.insert(txt, string.format('if(arg%d_idx)', arg.i)) -- means it was passed as arg\n                   table.insert(txt, string.format('lua_pushvalue(L, arg%d_idx);', arg.i))\n                   table.insert(txt, string.format('else'))\n                   if type(arg.default) == 'boolean' then -- boolean: we did a new()\n                      table.insert(txt, string.format('luaT_pushudata(L, arg%d, torch_Tensor);', arg.i))\n                   else  -- otherwise: point on default tensor --> retain\n                      table.insert(txt, string.format('{'))\n                      table.insert(txt, string.format('THTensor_(retain)(arg%d);', arg.i)) -- so we need a retain\n                      table.insert(txt, string.format('luaT_pushudata(L, arg%d, torch_Tensor);', arg.i))\n                      table.insert(txt, string.format('}'))\n                   end\n                elseif arg.default then\n                   -- we would have to deallocate the beast later if we did a new\n                   -- unlikely anyways, so i do not support it for now\n                   if type(arg.default) == 'boolean' then\n                      error('a tensor cannot be optional if not returned')\n                   end\n                elseif arg.returned then\n                   table.insert(txt, string.format('lua_pushvalue(L, arg%d_idx);', arg.i))\n                end\n                return table.concat(txt, '\\n')\n             end,\n\n   postcall = function(arg)\n                 local txt = {}\n                 if arg.creturned then\n                    -- this next line is actually debatable\n                    table.insert(txt, string.format('THTensor_(retain)(arg%d);', arg.i))\n                    table.insert(txt, string.format('luaT_pushudata(L, arg%d, torch_Tensor);', arg.i))\n                 end\n                 return table.concat(txt, '\\n')\n              end\n}\n\ntypes.Generator = {\n\n   helpname = function(arg)\n                 return \"Generator\"\n              end,\n\n   declare = function(arg)\n                return string.format(\"THGenerator *arg%d = NULL;\", arg.i)\n             end,\n\n   check = function(arg, idx)\n              return string.format(\"(arg%d = luaT_toudata(L, %d, torch_Generator))\", arg.i, idx)\n           end,\n\n   read = function(arg, idx)\n          end,\n\n   init = function(arg)\n             local text = {}\n             -- If no generator is supplied, pull the default out of the torch namespace.\n             table.insert(text, 'lua_getglobal(L,\"torch\");')\n             table.insert(text, string.format('arg%d = luaT_getfieldcheckudata(L, -1, \"_gen\", torch_Generator);', arg.i))\n             table.insert(text, 'lua_pop(L, 2);')\n             return table.concat(text, '\\n')\n          end,\n\n   carg = function(arg)\n             return string.format('arg%d', arg.i)\n          end,\n\n   creturn = function(arg)\n                return string.format('arg%d', arg.i)\n             end,\n\n   precall = function(arg)\n             end,\n\n   postcall = function(arg)\n              end\n}\n\ntypes.IndexTensor = {\n\n   helpname = function(arg)\n               return \"LongTensor\"\n            end,\n\n   declare = function(arg)\n                local txt = {}\n                table.insert(txt, string.format(\"THLongTensor *arg%d = NULL;\", arg.i))\n                if arg.returned then\n                   table.insert(txt, string.format(\"int arg%d_idx = 0;\", arg.i));\n                end\n                return table.concat(txt, '\\n')\n           end,\n\n   check = function(arg, idx)\n              return string.format('(arg%d = luaT_toudata(L, %d, \"torch.LongTensor\"))', arg.i, idx)\n           end,\n\n   read = function(arg, idx)\n             local txt = {}\n             if not arg.noreadadd then\n                table.insert(txt, string.format(\"THLongTensor_add(arg%d, arg%d, -1);\", arg.i, arg.i));\n             end\n             if arg.returned then\n                table.insert(txt, string.format(\"arg%d_idx = %d;\", arg.i, idx))\n             end\n             return table.concat(txt, '\\n')\n          end,\n\n   init = function(arg)\n             return string.format('arg%d = THLongTensor_new();', arg.i)\n          end,\n\n   carg = function(arg)\n             return string.format('arg%d', arg.i)\n          end,\n\n   creturn = function(arg)\n                return string.format('arg%d', arg.i)\n             end,\n\n   precall = function(arg)\n                local txt = {}\n                if arg.default and arg.returned then\n                   table.insert(txt, string.format('if(arg%d_idx)', arg.i)) -- means it was passed as arg\n                   table.insert(txt, string.format('lua_pushvalue(L, arg%d_idx);', arg.i))\n                   table.insert(txt, string.format('else')) -- means we did a new()\n                   table.insert(txt, string.format('luaT_pushudata(L, arg%d, \"torch.LongTensor\");', arg.i))\n                elseif arg.default then\n                   error('a tensor cannot be optional if not returned')\n                elseif arg.returned then\n                   table.insert(txt, string.format('lua_pushvalue(L, arg%d_idx);', arg.i))\n                end\n                return table.concat(txt, '\\n')\n             end,\n\n   postcall = function(arg)\n                 local txt = {}\n                 if arg.creturned or arg.returned then\n                    table.insert(txt, string.format(\"THLongTensor_add(arg%d, arg%d, 1);\", arg.i, arg.i));\n                 end\n                 if arg.creturned then\n                    -- this next line is actually debatable\n                    table.insert(txt, string.format('THLongTensor_retain(arg%d);', arg.i))\n                    table.insert(txt, string.format('luaT_pushudata(L, arg%d, \"torch.LongTensor\");', arg.i))\n                 end\n                 return table.concat(txt, '\\n')\n              end\n}\n\nfor _,typename in ipairs({\"ByteTensor\", \"CharTensor\", \"ShortTensor\", \"IntTensor\", \"LongTensor\",\n                          \"FloatTensor\", \"HalfTensor\", \"DoubleTensor\"}) do\n\n   types[typename] = {\n\n      helpname = function(arg)\n                    if arg.dim then\n                       return string.format('%s~%dD', typename, arg.dim)\n                    else\n                       return typename\n                    end\n                 end,\n\n      declare = function(arg)\n                   local txt = {}\n                   table.insert(txt, string.format(\"TH%s *arg%d = NULL;\", typename, arg.i))\n                   if arg.returned then\n                      table.insert(txt, string.format(\"int arg%d_idx = 0;\", arg.i));\n                   end\n                   return table.concat(txt, '\\n')\n                end,\n\n      check = function(arg, idx)\n                 if arg.dim then\n                    return string.format('(arg%d = luaT_toudata(L, %d, \"torch.%s\")) && (arg%d->nDimension == %d)', arg.i, idx, typename, arg.i, arg.dim)\n                 else\n                    return string.format('(arg%d = luaT_toudata(L, %d, \"torch.%s\"))', arg.i, idx, typename)\n                 end\n              end,\n\n      read = function(arg, idx)\n                if arg.returned then\n                   return string.format(\"arg%d_idx = %d;\", arg.i, idx)\n                end\n             end,\n\n      init = function(arg)\n                if type(arg.default) == 'boolean' then\n                   return string.format('arg%d = TH%s_new();', arg.i, typename)\n                elseif type(arg.default) == 'number' then\n                   return string.format('arg%d = %s;', arg.i, arg.args[arg.default]:carg())\n                else\n                   error('unknown default tensor type value')\n                end\n             end,\n\n      carg = function(arg)\n                return string.format('arg%d', arg.i)\n             end,\n\n      creturn = function(arg)\n                   return string.format('arg%d', arg.i)\n             end,\n\n      precall = function(arg)\n                   local txt = {}\n                   if arg.default and arg.returned then\n                      table.insert(txt, string.format('if(arg%d_idx)', arg.i)) -- means it was passed as arg\n                      table.insert(txt, string.format('lua_pushvalue(L, arg%d_idx);', arg.i))\n                      table.insert(txt, string.format('else'))\n                      if type(arg.default) == 'boolean' then -- boolean: we did a new()\n                         table.insert(txt, string.format('luaT_pushudata(L, arg%d, \"torch.%s\");', arg.i, typename))\n                      else  -- otherwise: point on default tensor --> retain\n                         table.insert(txt, string.format('{'))\n                         table.insert(txt, string.format('TH%s_retain(arg%d);', typename, arg.i)) -- so we need a retain\n                         table.insert(txt, string.format('luaT_pushudata(L, arg%d, \"torch.%s\");', arg.i, typename))\n                         table.insert(txt, string.format('}'))\n                      end\n                   elseif arg.default then\n                      -- we would have to deallocate the beast later if we did a new\n                      -- unlikely anyways, so i do not support it for now\n                      if type(arg.default) == 'boolean' then\n                         error('a tensor cannot be optional if not returned')\n                      end\n                   elseif arg.returned then\n                      table.insert(txt, string.format('lua_pushvalue(L, arg%d_idx);', arg.i))\n                   end\n                   return table.concat(txt, '\\n')\n                end,\n\n      postcall = function(arg)\n                    local txt = {}\n                    if arg.creturned then\n                       -- this next line is actually debatable\n                       table.insert(txt, string.format('TH%s_retain(arg%d);', typename, arg.i))\n                       table.insert(txt, string.format('luaT_pushudata(L, arg%d, \"torch.%s\");', arg.i, typename))\n                    end\n                    return table.concat(txt, '\\n')\n                 end\n   }\n\n   types[typename .. 'Array'] = {\n\n      helpname = function(arg)\n                    return string.format('{%s+}', typename)\n               end,\n\n      declare = function(arg)\n                   local txt = {}\n                   table.insert(txt, string.format('TH%s **arg%d_data = NULL;', typename, arg.i))\n                   table.insert(txt, string.format('long arg%d_size = 0;', arg.i))\n                   table.insert(txt, string.format('int arg%d_i = 0;', arg.i))\n                   return table.concat(txt, '\\n')\n              end,\n\n      check = function(arg, idx)\n                 return string.format('torch_isnonemptytable(L, %d)', idx)\n            end,\n\n      read = function(arg, idx)\n                local txt = {}\n                -- Iterate over the array to find its length, leave elements on stack.\n                table.insert(txt, string.format('do'))\n                table.insert(txt, string.format('{'))\n                table.insert(txt, string.format('  arg%d_size++;', arg.i))\n                table.insert(txt, string.format('  lua_checkstack(L, 1);'))\n                table.insert(txt, string.format('  lua_rawgeti(L, %d, arg%d_size);', idx, arg.i))\n                table.insert(txt, string.format('}'))\n                table.insert(txt, string.format('while (!lua_isnil(L, -1));'))\n                table.insert(txt, string.format('arg%d_size--;', arg.i))\n                -- Pop nil element from stack.\n                table.insert(txt, string.format('lua_pop(L, 1);'))\n                -- Allocate tensor pointers and read values from stack backwards.\n                table.insert(txt, string.format('arg%d_data = (TH%s**)THAlloc(arg%d_size * sizeof(TH%s*));', arg.i, typename, arg.i, typename))\n                table.insert(txt, string.format('for (arg%d_i = arg%d_size - 1; arg%d_i >= 0; arg%d_i--)', arg.i, arg.i, arg.i, arg.i))\n                table.insert(txt, string.format('{'))\n                table.insert(txt, string.format('  if (!(arg%d_data[arg%d_i] = luaT_toudata(L, -1, \"torch.%s\")))', arg.i, arg.i, typename))\n                table.insert(txt, string.format('    luaL_error(L, \"expected %s in tensor array\");', typename))\n                table.insert(txt, string.format('  lua_pop(L, 1);'))\n                table.insert(txt, string.format('}'))\n                table.insert(txt, string.format(''))\n                return table.concat(txt, '\\n')\n             end,\n\n      init = function(arg)\n             end,\n\n      carg = function(arg)\n                return string.format('arg%d_data,arg%d_size', arg.i, arg.i)\n             end,\n\n      creturn = function(arg)\n                   error('TensorArray cannot be returned.')\n                end,\n\n      precall = function(arg)\n                end,\n\n      postcall = function(arg)\n                    return string.format('THFree(arg%d_data);', arg.i)\n                 end\n   }\nend\n\ntypes.LongArg = {\n\n   vararg = true,\n\n   helpname = function(arg)\n               return \"(LongStorage | dim1 [dim2...])\"\n            end,\n\n   declare = function(arg)\n              return string.format(\"THLongStorage *arg%d = NULL;\", arg.i)\n           end,\n\n   init = function(arg)\n             if arg.default then\n                error('LongArg cannot have a default value')\n             end\n          end,\n\n   check = function(arg, idx)\n            return string.format(\"torch_islongargs(L, %d)\", idx)\n         end,\n\n   read = function(arg, idx)\n             return string.format(\"arg%d = torch_checklongargs(L, %d);\", arg.i, idx)\n          end,\n\n   carg = function(arg, idx)\n             return string.format('arg%d', arg.i)\n          end,\n\n   creturn = function(arg, idx)\n                return string.format('arg%d', arg.i)\n             end,\n\n   precall = function(arg)\n                local txt = {}\n                if arg.returned then\n                   table.insert(txt, string.format('luaT_pushudata(L, arg%d, \"torch.LongStorage\");', arg.i))\n                end\n                return table.concat(txt, '\\n')\n             end,\n\n   postcall = function(arg)\n                 local txt = {}\n                 if arg.creturned then\n                    -- this next line is actually debatable\n                    table.insert(txt, string.format('THLongStorage_retain(arg%d);', arg.i))\n                    table.insert(txt, string.format('luaT_pushudata(L, arg%d, \"torch.LongStorage\");', arg.i))\n                 end\n                 if not arg.returned and not arg.creturned then\n                    table.insert(txt, string.format('THLongStorage_free(arg%d);', arg.i))\n                 end\n                 return table.concat(txt, '\\n')\n              end\n}\n\ntypes.charoption = {\n\n   helpname = function(arg)\n                 if arg.values then\n                    return \"(\" .. table.concat(arg.values, '|') .. \")\"\n                 end\n              end,\n\n   declare = function(arg)\n                local txt = {}\n                table.insert(txt, string.format(\"const char *arg%d = NULL;\", arg.i))\n                if arg.default then\n                   table.insert(txt, string.format(\"char arg%d_default = '%s';\", arg.i, arg.default))\n                end\n                return table.concat(txt, '\\n')\n           end,\n\n   init = function(arg)\n             return string.format(\"arg%d = &arg%d_default;\", arg.i, arg.i)\n          end,\n\n   check = function(arg, idx)\n              local txt = {}\n              local txtv = {}\n              table.insert(txt, string.format('(arg%d = lua_tostring(L, %d)) && (', arg.i, idx))\n              for _,value in ipairs(arg.values) do\n                 table.insert(txtv, string.format(\"*arg%d == '%s'\", arg.i, value))\n              end\n              table.insert(txt, table.concat(txtv, ' || '))\n              table.insert(txt, ')')\n              return table.concat(txt, '')\n         end,\n\n   read = function(arg, idx)\n          end,\n\n   carg = function(arg, idx)\n             return string.format('arg%d', arg.i)\n          end,\n\n   creturn = function(arg, idx)\n             end,\n\n   precall = function(arg)\n             end,\n\n   postcall = function(arg)\n              end\n}\n\nfor _,typename in ipairs({\"ptrdiff_t\", \"size_t\"}) do\n  types[typename] =  {\n\n  helpname = function(arg)\n                return typename\n             end,\n\n  declare = function(arg)\n               -- if it is a number we initialize here\n               local default = tonumber(tostring(arg.default)) or 0\n               return string.format(\"%s arg%d = %g;\", typename, arg.i, default)\n            end,\n\n  check = function(arg, idx)\n             return string.format(\"lua_isnumber(L, %d)\", idx)\n          end,\n\n  read = function(arg, idx)\n            return string.format(\"arg%d = (%s)lua_tonumber(L, %d);\", arg.i, typename, idx)\n         end,\n\n  init = function(arg)\n            -- otherwise do it here\n            if arg.default then\n               local default = tostring(arg.default)\n               if not tonumber(default) then\n                  return string.format(\"arg%d = %s;\", arg.i, default)\n               end\n            end\n         end,\n\n  carg = function(arg)\n            return string.format('arg%d', arg.i)\n         end,\n\n  creturn = function(arg)\n               return string.format('arg%d', arg.i)\n            end,\n\n  precall = function(arg)\n               if arg.returned then\n                  return string.format('lua_pushnumber(L, (lua_Number)arg%d);', arg.i)\n               end\n            end,\n\n  postcall = function(arg)\n                if arg.creturned then\n                   return string.format('lua_pushnumber(L, (lua_Number)arg%d);', arg.i)\n                end\n             end\n  }\nend\n"
        },
        {
          "name": "utils.c",
          "type": "blob",
          "size": 5.29296875,
          "content": "#include \"general.h\"\n#include \"utils.h\"\n\n#ifdef WIN32\n# include <time.h>\n#else\n# include <sys/time.h>\n#endif\n\nTHLongStorage* torch_checklongargs(lua_State *L, int index)\n{\n  THLongStorage *storage;\n  int i;\n  int narg = lua_gettop(L)-index+1;\n\n  if(narg == 1 && luaT_toudata(L, index, \"torch.LongStorage\"))\n  {\n    THLongStorage *storagesrc = luaT_toudata(L, index, \"torch.LongStorage\");\n    storage = THLongStorage_newWithSize(storagesrc->size);\n    THLongStorage_copy(storage, storagesrc);\n  }\n  else\n  {\n    storage = THLongStorage_newWithSize(narg);\n    for(i = index; i < index+narg; i++)\n    {\n      if(!lua_isnumber(L, i))\n      {\n        THLongStorage_free(storage);\n        luaL_argerror(L, i, \"number expected\");\n      }\n      THLongStorage_set(storage, i-index, lua_tonumber(L, i));\n    }\n  }\n  return storage;\n}\n\nint torch_islongargs(lua_State *L, int index)\n{\n  int narg = lua_gettop(L)-index+1;\n\n  if(narg == 1 && luaT_toudata(L, index, \"torch.LongStorage\"))\n  {\n    return 1;\n  }\n  else\n  {\n    int i;\n\n    for(i = index; i < index+narg; i++)\n    {\n      if(!lua_isnumber(L, i))\n        return 0;\n    }\n    return 1;\n  }\n  return 0;\n}\n\n#ifdef _WIN32\n#include <windows.h>\n#include <io.h>\nstatic __declspec( thread ) LARGE_INTEGER ticksPerSecond = { 0 };\n#endif\n\nstatic int torch_isatty(lua_State *L)\n{\n  FILE **fp = (FILE **) luaL_checkudata(L, -1, LUA_FILEHANDLE);\n#ifdef _WIN32\n  lua_pushboolean(L, _isatty(_fileno(*fp)));\n#else\n  lua_pushboolean(L, isatty(fileno(*fp)));\n#endif\n  return 1;\n}\n\nstatic double real_time()\n{\n#ifdef _WIN32\n  if (ticksPerSecond.QuadPart == 0)\n  {\n    QueryPerformanceFrequency(&ticksPerSecond);\n  }\n  LARGE_INTEGER current;\n  QueryPerformanceCounter(&current);\n  return (double)(current.QuadPart) / ticksPerSecond.QuadPart;\n#else\n  struct timeval current;\n  gettimeofday(&current, NULL);\n  return (current.tv_sec + current.tv_usec/1000000.0);\n#endif\n}\n\nstatic int torch_lua_tic(lua_State* L)\n{\n  double ttime = real_time();\n  lua_pushnumber(L,ttime);\n  return 1;\n}\n\nstatic int torch_lua_toc(lua_State* L)\n{\n  double toctime = real_time();\n  lua_Number tictime = luaL_checknumber(L,1);\n  lua_pushnumber(L,toctime-tictime);\n  return 1;\n}\n\nstatic int torch_lua_getdefaulttensortype(lua_State *L)\n{\n  const char* tname = torch_getdefaulttensortype(L);\n  if(tname)\n  {\n    lua_pushstring(L, tname);\n    return 1;\n  }\n  return 0;\n}\n\nconst char* torch_getdefaulttensortype(lua_State *L)\n{\n  lua_getglobal(L, \"torch\");\n  if(lua_istable(L, -1))\n  {\n    lua_getfield(L, -1, \"Tensor\");\n    if(lua_istable(L, -1))\n    {\n      if(lua_getmetatable(L, -1))\n      {\n        lua_pushstring(L, \"__index\");\n        lua_rawget(L, -2);\n        if(lua_istable(L, -1))\n        {\n          lua_rawget(L, LUA_REGISTRYINDEX);\n          if(lua_isstring(L, -1))\n          {\n            const char *tname = lua_tostring(L, -1);\n            lua_pop(L, 4);\n            return tname;\n          }\n        }\n        else\n        {\n          lua_pop(L, 4);\n          return NULL;\n        }\n      }\n      else\n      {\n        lua_pop(L, 2);\n        return NULL;\n      }\n    }\n    else\n    {\n      lua_pop(L, 2);\n      return NULL;\n    }\n  }\n  else\n  {\n    lua_pop(L, 1);\n    return NULL;\n  }\n  return NULL;\n}\n\nstatic int torch_getnumthreads(lua_State *L)\n{\n  lua_pushinteger(L, THGetNumThreads());\n  return 1;\n}\n\nstatic int torch_setnumthreads(lua_State *L)\n{\n  THSetNumThreads(luaL_checkint(L, 1));\n  return 0;\n}\n\nstatic int torch_getnumcores(lua_State *L)\n{\n  lua_pushinteger(L, THGetNumCores());\n  return 1;\n}\n\nstatic void luaTorchGCFunction(void *data)\n{\n  lua_State *L = data;\n  lua_gc(L, LUA_GCCOLLECT, 0);\n}\n\nstatic int torch_setheaptracking(lua_State *L)\n{\n  int enabled = luaT_checkboolean(L,1);\n  lua_getglobal(L, \"torch\");\n  lua_pushboolean(L, enabled);\n  lua_setfield(L, -2, \"_heaptracking\");\n  if(enabled) {\n    THSetGCHandler(luaTorchGCFunction, L);\n  } else {\n    THSetGCHandler(NULL, NULL);\n  }\n  return 0;\n}\n\nstatic void luaTorchErrorHandlerFunction(const char *msg, void *data)\n{\n  lua_State *L = data;\n  luaL_error(L, msg);\n}\n\nstatic void luaTorchArgErrorHandlerFunction(int argNumber, const char *msg, void *data)\n{\n  lua_State *L = data;\n  luaL_argcheck(L, 0, argNumber, msg);\n}\n\nstatic int torch_updateerrorhandlers(lua_State *L)\n{\n  THSetErrorHandler(luaTorchErrorHandlerFunction, L);\n  THSetArgErrorHandler(luaTorchArgErrorHandlerFunction, L);\n  return 0;\n}\n\nstatic const struct luaL_Reg torch_utils__ [] = {\n  {\"getdefaulttensortype\", torch_lua_getdefaulttensortype},\n  {\"isatty\", torch_isatty},\n  {\"tic\", torch_lua_tic},\n  {\"toc\", torch_lua_toc},\n  {\"setnumthreads\", torch_setnumthreads},\n  {\"getnumthreads\", torch_getnumthreads},\n  {\"getnumcores\", torch_getnumcores},\n  {\"factory\", luaT_lua_factory},\n  {\"getconstructortable\", luaT_lua_getconstructortable},\n  {\"typename\", luaT_lua_typename},\n  {\"isequal\", luaT_lua_isequal},\n  {\"getenv\", luaT_lua_getenv},\n  {\"setenv\", luaT_lua_setenv},\n  {\"newmetatable\", luaT_lua_newmetatable},\n  {\"setmetatable\", luaT_lua_setmetatable},\n  {\"getmetatable\", luaT_lua_getmetatable},\n  {\"metatype\", luaT_lua_metatype},\n  {\"pushudata\", luaT_lua_pushudata},\n  {\"version\", luaT_lua_version},\n  {\"pointer\", luaT_lua_pointer},\n  {\"setheaptracking\", torch_setheaptracking},\n  {\"updateerrorhandlers\", torch_updateerrorhandlers},\n  {NULL, NULL}\n};\n\nvoid torch_utils_init(lua_State *L)\n{\n  torch_updateerrorhandlers(L);\n  luaT_setfuncs(L, torch_utils__, 0);\n}\n"
        },
        {
          "name": "utils.h",
          "type": "blob",
          "size": 0.662109375,
          "content": "#ifndef TORCH_UTILS_INC\n#define TORCH_UTILS_INC\n\n#include \"luaT.h\"\n#include \"TH.h\"\n\n#include <lua.h>\n#include <lualib.h>\n\n#ifdef _WIN32\n#else\n#include <unistd.h>\n#endif\n\n#ifdef __cplusplus\n# define TORCH_EXTERNC extern \"C\"\n#else\n# define TORCH_EXTERNC extern\n#endif\n\n#ifdef _WIN32\n# ifdef torch_EXPORTS\n#  define TORCH_API TORCH_EXTERNC __declspec(dllexport)\n# else\n#  define TORCH_API TORCH_EXTERNC __declspec(dllimport)\n# endif\n#else\n# define TORCH_API TORCH_EXTERNC\n#endif\n\n\nTORCH_API THLongStorage* torch_checklongargs(lua_State *L, int index);\nTORCH_API int torch_islongargs(lua_State *L, int index);\nTORCH_API const char* torch_getdefaulttensortype(lua_State *L);\n\n#endif\n"
        }
      ]
    }
  ]
}