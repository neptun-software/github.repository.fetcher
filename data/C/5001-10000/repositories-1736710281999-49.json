{
  "metadata": {
    "timestamp": 1736710281999,
    "page": 49,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjUw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "antirez/disque",
      "stars": 8011,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.291015625,
          "content": ".*.swp\n*.o\n*.log\ndump.rdb\ndisque-server\ndisque-check-aof\ndisque\ndoc-tools\nrelease\nmisc/*\nsrc/release.h\nappendonly.aof\nSHORT_TERM_TODO\nrelease.h\nsrc/transfer.sh\nsrc/configs\nredis.ds\nsrc/redis.conf\nsrc/nodes.conf\ndeps/lua/src/lua\ndeps/lua/src/luac\ndeps/lua/src/liblua.a\n.make-*\n.prerequisites\n*.dSYM\n"
        },
        {
          "name": ".travis.yml",
          "type": "blob",
          "size": 0.154296875,
          "content": "language: c\nsudo: false\naddons:\n  apt:\n    packages:\n      - tcl8.5\ncache:\n  directories:\n    - $HOME/.ccache\ninstall: make CC=\"ccache $CC\"\nscript: make test\n"
        },
        {
          "name": "COPYING",
          "type": "blob",
          "size": 1.453125,
          "content": "Copyright (c) 2006-2014, Salvatore Sanfilippo\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\n    * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\n    * Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\n    * Neither the name of Disque nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n"
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 0.1474609375,
          "content": "# Top level makefile, the real shit is at src/Makefile\n\ndefault: all\n\n.DEFAULT:\n\tcd src && $(MAKE) $@\n\ninstall:\n\tcd src && $(MAKE) $@\n\n.PHONY: install\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 70.5009765625,
          "content": "[![Build Status](https://travis-ci.org/antirez/disque.svg)](https://travis-ci.org/antirez/disque)\n\nDisque, an in-memory, distributed job queue\n===\n\nDisque is an ongoing experiment to build a distributed, in-memory, message\nbroker.\nIts goal is to capture the essence of the \"Redis as a jobs queue\" use case,\nwhich is usually implemented using blocking list operations, and move\nit into an ad-hoc, self-contained, scalable, and fault tolerant design, with\nsimple to understand properties and guarantees, but still resembling Redis\nin terms of simplicity, performance, and implementation as a C non-blocking\nnetworked server.\n\nCurrently (2 Jan 2016) the project is in release candidate state. People are\nencouraged to start evaluating it and report bugs and experiences.\n\n**WARNING: This is beta code and may not be suitable for production usage. The API is considered to be stable if not for details that may change in the next release candidates, however it's new code, so handle with care!**\n\nWhat is a message queue?\n---\n\n*Hint: skip this section if you are familiar with message queues.*\n\nYou know how humans use text messages to communicate, right? I could write\nmy wife \"please get the milk at the store\", and she maybe will reply \"Ok message\nreceived, I'll get two bottles on my way home\".\n\nA message queue is the same as human text messages, but for computer programs.\nFor example a web application, when an user subscribes, may send another\nprocess, that handles sending emails, \"please send the confirmation email\nto tom@example.net\".\n\nMessage systems like Disque allow communication between processes using\ndifferent queues. So a process can send a message to a queue with a given\nname, and only processes which fetch messages from this queue will return those\nmessages. Moreover, multiple processes can listen for messages in a given\nqueue, and multiple processes can send messages to the same queue.\n\nThe important part of a message queue is to be able to provide guarantees so\nthat messages are eventually delivered even in the face of failures. So even\nif in theory implementing a message queue is very easy, to write a very\nrobust and scalable one is harder than it may appear.\n\nGive me the details!\n---\n\nDisque is a distributed and fault tolerant message broker, so it works as a\nmiddle layer among processes that want to exchange messages.\n\nProducers add messages that are served to consumers.\nSince message queues are often used in order to process delayed\njobs, Disque often uses the term \"job\" in the API and in the documentation,\nhowever jobs are actually just messages in the form of strings, so Disque\ncan be used for other use cases. In this documentation \"jobs\" and \"messages\"\nare used in an interchangeable way.\n\nJob queues with a producer-consumer model are pretty common, so the devil is\nin the details. A few details about Disque are:\n\nDisque is a **synchronously replicated job queue**. By default when a new job is added, it is replicated to W nodes before the client gets an acknowledgement about the job being added. W-1 nodes can fail and the message will still be delivered.\n\nDisque supports both **at-least-once and at-most-once** delivery semantics. At-least-once delivery semantics is where most effort was spent in the design and implementation, while at-most-once semantics is a trivial result of using a *retry time* set to 0 (which means, never re-queue the message again) and a replication factor of 1 for the message (not strictly needed, but it is useless to have multiple copies of a message around if it will be delivered at most one time). You can have, at the same time, both at-least-once and at-most-once jobs in the same queues and nodes, since this is a per message setting.\n\nDisque at-least-once delivery is designed to **approximate single delivery** when possible, even during certain kinds of failures. This means that while Disque can only guarantee a number of deliveries equal or greater to one, it will try hard to avoid multiple deliveries whenever possible.\n\nDisque is a distributed system where **all nodes have the same role** (aka, it is multi-master). Producers and consumers can attach to whatever node they like, and there is no need for producers and consumers of the same queue to stay connected to the same node. Nodes will automatically exchange messages based on load and client requests.\n\nDisque is Available (it is an eventually consistent AP system in CAP terms): producers and consumers can make progress as long as a single node is reachable.\n\nDisque supports **optional asynchronous commands** that are low latency for the client but provide less guarantees. For example a producer can add a job to a queue with a replication factor of 3, but may want to run away before knowing if the contacted node was really able to replicate it to the specified number of nodes or not. The node will replicate the message in the background in a best effort way.\n\nDisque **automatically re-queues messages that are not acknowledged** as already processed by consumers, after a message-specific retry time. There is no need for consumers to re-queue a message if it was not processed.\n\nDisque uses **explicit acknowledges** in order for a consumer to signal a message as delivered (or, using a different terminology, to signal a job as already processed).\n\nDisque queues only provides **best effort ordering**. Each queue sorts messages based on the job creation time, which is obtained using the *wall clock* of the local node where the message was created (plus an incremental counter for messages created in the same millisecond), so messages created in the same node are normally delivered in the same order they were created. This is not causal ordering since correct ordering is violated in different cases: when messages are re-issued because they are not acknowledged, because of nodes local clock drifts, and when messages are moved to other nodes for load balancing and federation (in this case you end with queues having jobs originated in different nodes with different wall clocks). However all this also means that normally messages are not delivered in random order and usually messages created first are delivered first.\n\nNote that since Disque does not provide strict FIFO semantics, technically speaking it should not be called a *message queue*, and it could better identified as a message broker. However I believe that at this point in the IT industry a *message queue* is often more lightly used to identify a generic broker that may or may not be able to guarantee order in all cases. Given that we document the semantics very clearly, I grant myself the right to call Disque a message queue anyway.\n\nDisque provides the user with fine-grained control for each job **using three time related parameters**, and one replication parameter. For each job, the user can control:\n\n1. The replication factor (how many nodes have a copy).\n2. The delay time (the min time Disque will wait before putting the message in a queue, making the message deliverable).\n3. The retry time (how much time should elapse since the last time the job was queued and without an acknowledge about the job delivery, before the job is re-queued for delivery).\n4. The expire time (how much time should elapse for the job to be deleted regardless of whether it was successfully delivered, i.e. acknowledged, or not).\n\nFinally, Disque supports optional disk persistence, which is not enabled by default, but that can be handy in single data center setups and during restarts.\n\nOther minor features are:\n\n* Ability to block queues.\n* A few statistics about queue activity.\n* Stateless iterators for queues and jobs.\n* Commands to control the visibility of single jobs.\n* Easy resize of the cluster (adding nodes is trivial).\n* Graceful removal of nodes without losing job replicas.\n\nACKs and retries\n---\n\nDisque's implementation of *at-least-once* delivery semantics is designed in\norder to avoid multiple delivery during certain classes of failures. It is not able to guarantee that no multiple deliveries will occur. However there are many *at-least-once* workloads where duplicated deliveries are acceptable (or explicitly handled), but not desirable either. A trivial example is sending emails to users (it is not terrible if an user gets a duplicated email, but is important to avoid it when possible), or doing idempotent operations that are expensive (all the times where it is critical for performance to avoid multiple deliveries).\n\nIn order to avoid multiple deliveries when possible, Disque uses client ACKs. When a consumer processes a message correctly, it should acknowledge this fact to Disque. ACKs are replicated to multiple nodes, and are garbage collected as soon as the system believes it is unlikely that more nodes in the cluster have the job (the ACK refers to) still active. Under memory pressure or under certain failure scenarios, ACKs are eventually discarded.\n\nMore explicitly:\n\n1. A job is replicated to multiple nodes, but usually only *queued* in a single node. There is a difference between having a job in memory, and queueing it for delivery.\n2. Nodes having a copy of a message, if a certain amount of time has elapsed without getting the ACK for the message, will re-queue it. Nodes will run a best-effort protocol to avoid re-queueing the message multiple times.\n3. ACKs are replicated and garbage collected across the cluster so that eventually processed messages are evicted (this happens ASAP if there are no failures nor network partitions).\n\nFor example, if a node having a copy of a job gets partitioned away during the time the job gets acknowledged by the consumer, it is likely that when it returns (in a reasonable amount of time, that is, before the retry time is reached) it will be informed about the ACK and will avoid to re-queue the message. Similarly, jobs can be acknowledged during a partition to just a single available node, and when the partition heals the ACK will be propagated to other nodes that may still have a copy of the message.\n\nSo an ACK is just a **proof of delivery** that is replicated and retained for\nsome time in order to make multiple deliveries less likely to happen in practice.\n\nAs already mentioned, in order to control replication and retries, a Disque job has the following associated properties: number of replicas, delay, retry and expire.\n\nIf a job has a retry time set to 0, it will get queued exactly once (and in this case a replication factor greater than 1 is useless, and signaled as an error to the user), so it will get delivered either a single time or will never get delivered. While jobs can be persisted on disk for safety, queues aren't, so this behavior is guaranteed even when nodes restart after a crash, whatever the persistence configuration is. However when nodes are manually restarted by the sysadmin, for example for upgrades, queues are persisted correctly and reloaded at startup, since the store/load operation is atomic in this case, and there are no race conditions possible (it is not possible that a job was delivered to a client and is persisted on disk as queued at the same time).\n\nFast acknowledges\n---\n\nDisque supports a faster way to acknowledge processed messages, via the\n`FASTACK` command. The normal acknowledge is very expensive from the point of\nview of messages exchanged between nodes, this is what happens during a normal\nacknowledge:\n\n1. The client sends ACKJOB to one node.\n2. The node sends a SETACK message to everybody it believes to have a copy.\n3. The receivers of SETACK reply with GOTACK to confirm.\n4. The node finally sends DELJOB to all the nodes.\n\n*Note: actual garbage collection is more complex in case of failures and is explained in the state machine later. The above is what happens 99% of times.*\n\nIf a message is replicated to 3 nodes, acknowledging requires 1+2+2+2 messages,\nfor the sake of retaining the ack if some nodes may not be reached when the\nmessage is acknowledged. This makes the probability of multiple deliveries of\nthis message less likely.\n\nHowever the alternative **fast ack**, while less reliable, is much faster\nand invovles exchanging less messages. This is how a fast acknowledge works:\n\n1. The client sends `FASTACK` to one node.\n2. The node evicts the job and sends a best effort DELJOB to all the nodes that may have a copy, or to all the cluster if the node was not aware of the job.\n\nIf during a fast acknowledge a node having a copy of the message is not\nreachable, for example because of a network partition, the node will deliver\nthe message again, since it has a non-acknowledged copy of the message and\nthere is nobody able to inform it the message has been acknowledged when the\npartition heals.\n\nIf the network you are using is pretty reliable, and you are very concerned with\nperformance, and multiple deliveries in the context of your applications are\na non issue, then `FASTACK` is probably the way to go.\n\n\nDead letter queue\n---\n\nMany message queues implement a feature called *dead letter queue*. It is\na special queue used in order to accumulate messages that cannot be processed\nfor some reason. Common causes could be:\n\n1. The message was delivered too many times but never correctly processed.\n2. The message time-to-live reached zero before it was processed.\n3. Some worker explicitly asked the system to flag the message as having issues.\n\nThe idea is that the administrator of the system checks (usually via automatic\nsystems) if there is something in the dead letter queue in order to understand\nif there is some software error or other kind of error preventing messages\nfrom being processed as expected.\n\nSince Disque is an in-memory system, the message time-to-live is an important\nproperty. When it is reached, we want messages to go away, since the TTL should\nbe chosen so that after such a time it is no longer meaningful to process\nthe message. In such a system, to use memory and create a queue in response\nto an error or to messages timing out looks like a non optimal idea. Moreover,\ndue to the distributed nature of Disque, dead letters could end up spawning\nmultiple nodes and having duplicated entries in them.\n\nSo Disque uses a different approach. Each node message representation has\ntwo counters: a **nacks counter** and an **additional deliveries** counter.\nThe counters are not consistent among nodes having a copy of the same message,\nthey are just best effort counters that may not increment in some node during\nnetwork partitions.\n\nThe idea of these two counters is that one is incremented every time a worker\nuses the `NACK` command to tell the queue the message was not processed correctly\nand should be put back on the queue ASAP. The other is incremented for every other condition (different than the `NACK` call) that requires a message to be put back\non the queue again. This includes messages that get lost and are enqueued again\nor messages that are enqueued on one side of the partition since the message\nwas processed on the other side and so forth.\n\nUsing the `GETJOB` command with the `WITHCOUNTERS` option, or using the\n`SHOW` command to inspect a job, it is possible to retrieve these two counters\ntogether with the other job information, so if a worker, before processing\na message, sees the counters have values over some application-defined limit, it\ncan notify operations people in multiple ways:\n\n1. It may send an email.\n2. Set a flag in a monitoring system.\n3. Put the message in a special queue (simulating the dead letter feature).\n4. Attempt to process the message and report the stack trace of the error if any.\n\nBasically the exact handling of the feature is up to the application using\nDisque. Note that the counters don't need to be consistent in the face of\nfailures or network partitions: the idea is that eventually if a message has\nissues the counters will get incremented enough times to reach the limit\nselected by the application as a warning threshold.\n\nThe reason for having two distinct counters is that applications may want\nto handle the case of explicit negative acknowledges via `NACK` differently\nthan multiple deliveries because of timeouts or messages getting lost.\n\nDisque and disk persistence\n---\n\nDisque can be operated in-memory only, using synchronous replication as a\ndurability guarantee, or can be operated using the Append Only File where\njobs creations and evictions are logged on disk (with configurable fsync\npolicies) and reloaded at restart.\n\nAOF is recommended especially if you run in a single availability zone\nwhere a mass reboot of all your nodes is possible.\n\nNormally Disque only reloads job data in memory, without populating queues,\nsince unacknowledged jobs are requeued eventually. Moreover, reloading\nqueue data is not safe in the case of at-most-once jobs having the retry value\nset to 0. However a special option is provided in order to reload the full\nstate from the AOF. This is used together with an option that allows shutting\ndown the server just after the AOF is generated from scratch, in order to\nmake it safe even to reload jobs with retry set to 0, since the AOF is generated\nwhile the server no longer accepts commands from clients, so no race condition\nis possible.\n\nEven when running memory-only, Disque is able to dump its memory on disk and reload from disk on controlled restarts, for example in order to upgrade the software.\n\nThis is how to perform a controlled restart, that works whether AOF is enabled\nor not:\n\n1. CONFIG SET aof-enqueue-jobs-once yes\n2. CONFIG REWRITE\n3. SHUTDOWN REWRITE-AOF\n\nAt this point we have a freshly generated AOF on disk, and the server is\nconfigured in order to load the full state only at the next restart\n(`aof-enqueue-jobs-once` is automatically turned off after the restart).\n\nWe can just restart the server with the new software, or in a new server, and\nit will restart with the full state. Note that `aof-enqueue-jobs-once`\nimplies loading the AOF even if AOF support is switched off, so there is\nno need to enable AOF just for the upgrade of an in-memory only server.\n\nJob IDs\n---\n\nDisque jobs are uniquely identified by an ID like the following:\n\n    D-dcb833cf-8YL1NT17e9+wsA/09NqxscQI-05a1\n\nJob IDs are composed of exactly 40 characters and start with the prefix `D-`.\n\nWe can split an ID into multiple parts:\n\n    D- | dcb833cf | 8YL1NT17e9+wsA/09NqxscQI | 05a1\n\n1. `D-` is the prefix.\n2. `dcb833cf` is the first 8 bytes of the node ID where the message was generated.\n3. `8YL1NT17e9+wsA/09NqxscQI` is the 144 bit ID pseudo-random part encoded in base64.\n4. `05a1` is the Job TTL in minutes. Because of it, message IDs can be expired safely even without having the job representation.\n\nIDs are returned by ADDJOB when a job is successfully created, are part of\nthe GETJOB output, and are used in order to acknowledge that a job was\ncorrectly processed by a worker.\n\nPart of the node ID is included in the message so that a worker processing\nmessages for a given queue can easily guess what are the nodes where jobs\nare created, and move directly to these nodes to increase efficiency instead\nof listening for messages in a node that will require to fetch messages from\nother nodes.\n\nOnly 32 bits of the original node ID is included in the message, however\nin a cluster with 100 Disque nodes, the probability of two nodes having\nidentical 32 bit ID prefixes is given by the birthday paradox:\n\n    P(100,2^32) = .000001164\n\nIn case of collisions, the workers may just make a non-efficient choice.\n\nCollisions in the 144 bits random part are believed to be impossible,\nsince it is computed as follows.\n\n    144 bit ID = HIGH_144_BITS_OF_SHA1(seed || counter)\n\nWhere:\n\n* **seed** is a seed generated via `/dev/urandom` at startup.\n* **counter** is a 64 bit counter incremented at every ID generation.\n\nSo there are 22300745198530623141535718272648361505980416 possible IDs,\nselected in a uniform way. While the probability of a collision is non-zero\nmathematically, in practice each ID can be regarded as unique.\n\nThe encoded TTL in minutes has a special property: it is always even for\nat most once jobs (job retry value set to 0), and is always odd otherwise.\nThis changes the encoded TTL precision to 2 minutes, but allows to tell\nif a Job ID is about a job with deliveries guarantees or not.\nNote that this fact does not mean that Disque jobs TTLs have a precision of\ntwo minutes. The TTL field is only used to expire job IDs of jobs a given\nnode does not actually have a copy, search \"dummy ACK\" in this documentation\nfor more information.\n\nSetup\n===\n\nTo play with Disque please do the following:\n\n1. Compile Disque - if you can compile Redis, you can compile Disque, it's the usual \"no external deps\" thing. Just type `make`. Binaries (`disque` and `disque-server`) will end up in the `src` directory.\n2. Run a few Disque nodes on different ports. Create different `disque.conf` files following the example `disque.conf` in the source distribution.\n3. After you have them running, you need to join the cluster. Just select a random node among the nodes you are running, and send the command `CLUSTER MEET <ip> <port>` for every other node in the cluster.\n\n**Please note that you need to open two TCP ports on each node**, the base port of the Disque instance, for example 7711, plus the cluster bus port, which is always at a fixed offset, obtained summing 10000 to the base port, so in the above example, you need to open both 7711 and 17711. Disque uses the base port to communicate with clients and the cluster bus port to communicate with other Disque processes.\n\nTo run a node, just call `./disque-server`.\n\nFor example, if you are running three Disque servers in port 7711, 7712, 7713, in order to join the cluster you should use the `disque` command line tool and run the following commands:\n\n    ./disque -p 7711 cluster meet 127.0.0.1 7712\n    ./disque -p 7711 cluster meet 127.0.0.1 7713\n\nYour cluster should now be ready. You can try to add a job and fetch it back\nin order to test if everything is working:\n\n    ./disque -p 7711\n    127.0.0.1:7711> ADDJOB queue body 0\n    D-dcb833cf-8YL1NT17e9+wsA/09NqxscQI-05a1\n    127.0.0.1:7711> GETJOB FROM queue\n    1) 1) \"queue\"\n       2) \"D-dcb833cf-8YL1NT17e9+wsA/09NqxscQI-05a1\"\n       3) \"body\"\n\nRemember that you can add and get jobs from different nodes as Disque\nis multi master. Also remember that you need to acknowledge jobs otherwise\nthey'll never go away from the server memory (unless the time-to-live is\nreached).\n\nMain API\n===\n\nThe Disque API is composed of a small set of commands, since the system solves a\nsingle very specific problem. The three main commands are:\n\n#### `ADDJOB queue_name job <ms-timeout> [REPLICATE <count>] [DELAY <sec>] [RETRY <sec>] [TTL <sec>] [MAXLEN <count>] [ASYNC]`\n\nAdds a job to the specified queue. Arguments are as follows:\n\n* *queue_name* is the name of the queue, any string, basically. You don't need to create queues, if a queue does not exist, it gets created automatically. If one has no more jobs, it gets removed.\n* *job* is a string representing the job. Disque is job meaning agnostic, for it a job is just a message to deliver. Job max size is 4GB.\n* *ms-timeout* is the command timeout in milliseconds. If no ASYNC is specified, and the replication level specified is not reached in the specified number of milliseconds, the command returns with an error, and the node does a best-effort cleanup, that is, it will try to delete copies of the job across the cluster. However the job may still be delivered later. Note that the actual timeout resolution is 1/10 of second or worse with the default server hz.\n* *REPLICATE count* is the number of nodes the job should be replicated to.\n* *DELAY sec* is the number of seconds that should elapse before the job is queued by any server. By default there is no delay.\n* *RETRY sec* period after which, if no ACK is received, the job is put into the queue again for delivery. If RETRY is 0, the job has at-most-once delivery semantics. The default retry time is 5 minutes, with the exception of jobs having a TTL so small that 10% of TTL is less than 5 minutes. In this case the default RETRY is set to TTL/10 (with a minimum value of 1 second).\n* *TTL sec* is the max job life in seconds. After this time, the job is deleted even if it was not successfully delivered. If not specified, the default TTL is one day.\n* *MAXLEN count* specifies that if there are already *count* messages queued for the specified queue name, the message is refused and an error reported to the client.\n* *ASYNC* asks the server to let the command return ASAP and replicate the job to other nodes in the background. The job gets queued ASAP, while normally the job is put into the queue only when the client gets a positive reply.\n\nThe command returns the Job ID of the added job, assuming ASYNC is specified, or if the job was replicated correctly to the specified number of nodes. Otherwise an error is returned.\n\n#### `GETJOB [NOHANG] [TIMEOUT <ms-timeout>] [COUNT <count>] [WITHCOUNTERS] FROM queue1 queue2 ... queueN`\n\nReturn jobs available in one of the specified queues, or return NULL\nif the timeout is reached. A single job per call is returned unless a count greater than 1 is specified. Jobs are returned as a three-element array containing the queue name, the Job ID, and the job body itself. If jobs are available in multiple queues, queues are processed left to right.\n\nIf there are no jobs for the specified queues, the command blocks, and messages are exchanged with other nodes, in order to move messages about these queues to this node, so that the client can be served.\n\nOptions:\n\n* **NOHANG**: Ask the command to not block even if there are no jobs in all the specified queues. This way the caller can just check if there are available jobs without blocking at all.\n* **WITHCOUNTERS**: Return the best-effort count of NACKs (negative acknowledges) received by this job, and the number of additional deliveries performed for this job. See the *Dead Letters* section for more information.\n\n#### `ACKJOB jobid1 jobid2 ... jobidN`\n\nAcknowledges the execution of one or more jobs via job IDs. The node receiving the ACK will replicate it to multiple nodes and will try to garbage collect both the job and the ACKs from the cluster so that memory can be freed.\n\nA node receiving an ACKJOB command about a job ID it does not know will create\na special empty job, with the state set to \"acknowledged\", called a \"dummy ACK\".\nThe dummy ACK is used in order to retain the acknolwedge during a netsplit if\nthe ACKJOB is sent to a node that does not have a copy of the job. When the\npartition heals, job garbage collection will be attempted.\n\nHowever, since the job ID encodes information about the job being an \"at-most-\nonce\" or an \"at-least-once\" job, the dummy ACK is only created for at-least-\nonce jobs.\n\n#### `FASTACK jobid1 jobid2 ... jobidN`\n\nPerforms a best-effort cluster-wide deletion of the specified job IDs. When the\nnetwork is well connected and there are no node failures, this is equivalent to\n`ACKJOB` but much faster (due to less messages being exchanged), however during\nfailures it is more likely that fast acknowledges will result in multiple\ndeliveries of the same messages.\n\n#### `WORKING jobid`\n\nClaims to be still working with the specified job, and asks Disque to postpone\nthe next time it will deliver the job again. The next delivery is postponed\nfor the job retry time, however the command works in a **best effort** way\nsince there is no way to guarantee during failures that another node in a\ndifferent network partition won't perform a delivery of the same job.\n\nAnother limitation of the `WORKING` command is that it cannot be sent to\nnodes not knowing about this particular job. In such a case the command replies\nwith a `NOJOB` error. Similarly, if the job is already acknowledged an error\nis returned.\n\nNote that the `WORKING` command is refused by Disque nodes if 50% of the job\ntime to live has already elapsed. This limitation makes Disque safer since\nusually the *retry* time is much smaller than the time-to-live of a job, so\nit can't happen that a set of broken workers monopolize a job with `WORKING`\nand never process it. After 50% of the TTL has elapsed, the job will be delivered\nto other workers anyway.\n\nNote that `WORKING` returns the number of seconds you (likely) postponed the\nmessage visibility for other workers (the command basically returns the\n*retry* time of the job), so the worker should make sure to send the next\n`WORKING` command before this time elapses. Moreover, a worker that may want\nto use this interface may fetch the retry value with the `SHOW` command\nwhen starting to process a message, or may simply send a `WORKING` command\nASAP, like in the following example (in pseudo code):\n\n    retry = WORKING(jobid)\n    RESET timer\n    WHILE ... work with the job still not finished ...\n        IF timer reached 80% of the retry time\n            WORKING(jobid)\n            RESET timer\n        END\n    END\n\n#### `NACK <job-id> ... <job-id>`\n\nThe `NACK` command tells Disque to put the job back in the queue ASAP. It\nis very similar to `ENQUEUE` but it increments the job `nacks` counter\ninstead of the `additional-deliveries` counter. The command should be used\nwhen the worker was not able to process a message and wants the message to\nbe put back into the queue in order to be processed again.\n\nOther commands\n===\n\n#### `INFO`\n\nGeneric server information / stats.\n\n#### `HELLO`\n\nReturns hello format version, this node ID, all the nodes IDs, IP addresses,\nports, and priority (lower is better, means a node is more available).\nClients should use this as a handshake command when connecting with a\nDisque node.\n\n#### `QLEN <queue-name>`\n\nReturn the length of the queue.\n\n#### `QSTAT <queue-name>`\n\nShow information about a queue as an array of key-value pairs.\nBelow is an example of the output, however, implementations should not rely\non the order of the fields nor on the existence of the fields listed.\nThey may be (unlikely) removed or more can be (likely) added\nin the future.\n\nIf a queue does not exist, NULL is returned. Note that queues are\nautomatically evicted after some time if empty and without clients blocked\nwaiting for jobs, even if there are active jobs for the queue. So the\nnon existence of a queue does not mean there are not jobs in the node\nor in the whole cluster about this queue. The queue will be immediately\ncreated again when needed to serve requests.\n\nExample output:\n\n```\nQSTAT foo\n 1) \"name\"\n 2) \"foo\"\n 3) \"len\"\n 4) (integer) 56520\n 5) \"age\"\n 6) (integer) 601\n 7) \"idle\"\n 8) (integer) 3\n 9) \"blocked\"\n10) (integer) 50\n11) \"import-from\"\n12) 1) \"dcb833cf8f42fbb7924d92335ff6d67d3cea6e3d\"\n    2) \"4377bdf656040a18d8caf4d9f409746f1f9e6396\"\n13) \"import-rate\"\n14) (integer) 19243\n15) \"jobs-in\"\n16) (integer) 3462847\n17) \"jobs-out\"\n18) (integer) 3389522\n19) \"pause\"\n20) \"none\"\n```\n\nMost fields should be obvious. The `import-from` field shows a list of node\nIDs this node is importing jobs from, for this queue, in order to serve\nclients requests. The `import-rate` is the instantaneous amount of jos/sec\nwe import in order to handle our outgoing traffic (GETJOB commands).\n`blocked` is the number of clients blocked on this queue right now.\n`age` and `idle` are reported in seconds. The `jobs-in` and `-out` counters are\nincremented every time a job is enqueued or dequeued for any reason.\n\n#### `QPEEK <queue-name> <count>`\n\nReturn, without consuming from the queue, *count* jobs. If *count* is positive\nthe specified number of jobs are returned from the oldest to the newest\n(in the same best-effort FIFO order as GETJOB). If *count* is negative the\ncommands changes behavior and shows the *count* newest jobs, from the newest\nfrom the oldest.\n\n#### `ENQUEUE <job-id> ... <job-id>`\n\nQueue jobs if not already queued.\n\n#### `DEQUEUE <job-id> ... <job-id>`\n\nRemove the job from the queue.\n\n#### `DELJOB <job-id> ... <job-id>`\n\nCompletely delete a job from a node.\nNote that this is similar to `FASTACK`, but limited to a single node since\nno `DELJOB` cluster bus message is sent to other nodes.\n\n#### `SHOW <job-id>`\n\nDescribe the job.\n\n#### `QSCAN [COUNT <count>] [BUSYLOOP] [MINLEN <len>] [MAXLEN <len>] [IMPORTRATE <rate>]`\n\nThe command provides an interface to iterate all the existing queues in\nthe local node, providing a cursor in the form of an integer that is passed\nto the next command invocation. During the first call, the cursor must be 0,\nin the next calls the cursor returned in the previous call is used in the\nnext. The iterator guarantees to return all the elements but may return\nduplicated elements.\n\nOptions:\n\n* `COUNT <count>` A hint about how much work to do per iteration.\n* `BUSYLOOP` Block and return all the elements in a busy loop.\n* `MINLEN <count>` Don't return elements with less than `count` jobs queued.\n* `MAXLEN <count>` Don't return elements with more than `count` jobs queued.\n* `IMPORTRATE <rate>` Only return elements with a job import rate (from other nodes) `>=` `rate`.\n\nThe cursor argument can be in any place, the first non matching option\nthat has valid cursor form of an unsigned number will be sensed as a valid\ncursor.\n\n#### `JSCAN [<cursor>] [COUNT <count>] [BUSYLOOP] [QUEUE <queue>] [STATE <state1> STATE <state2> ... STATE <stateN>] [REPLY all|id]`\n\nThe command provides an interface to iterate all the existing jobs in\nthe local node, providing a cursor in the form of an integer that is passed\nto the next command invocation. During the first call the cursor must be 0,\nin the next calls the cursor returned in the previous call is used in the\nnext. The iterator guarantees to return all the elements but may return\nduplicated elements.\n\nOptions:\n\n* `COUNT <count>` A hint about how much work to do per iteration.\n* `BUSYLOOP` Block and return all the elements in a busy loop.\n* `QUEUE <queue>` Return only jobs in the specified queue.\n* `STATE <state>` Return jobs in the specified state. Can be used multiple times for a logical OR.\n* `REPLY <type>` Job reply type. Type can be `all` or `id`. Default is to report just the job ID. If `all` is specified the full job state is returned like for the SHOW command.\n\nThe cursor argument can be in any place, the first non matching option\nthat has valid cursor form of an unsigned number will be sensed as a valid\ncursor.\n\n#### `PAUSE <queue-name> option1 [option2 ... optionN]`\n\nControl the paused state of a queue, possibly broadcasting the command to\nother nodes in the cluster. Disque queues can be paused in both directions,\ninput and output, or both. Pausing a queue makes it unavailable for input\nor output operations. Specifically:\n\nA queue paused in input will have changed behavior in the following ways:\n\n1. ADDJOB returns a `-PAUSED` error for queues paused in input.\n2. The node where the queue is paused, no longer accepts to replicate jobs for this queue when requested by other nodes. Since ADDJOB by default uses synchronous replication, it means that if the queue is paused in enough nodes, adding jobs with a specified level of replication may fail. In general the node where the queue is paused will not create new jobs in the local node about this queue.\n3. The job no longer accepts ENQUEUE messages from other nodes. Those messages are usually used by nodes in out of memory conditions that replicate jobs externally (not holding a copy), in order to put the job in the queue of some random node, among the nodes having a copy of a job.\n4. Active jobs that reach their retry time, are not put back into the queue. Instead their retry timer is updated and the node will try again later.\n\nBasically a queue paused in input never creates new jobs for this queue, and never puts active jobs (jobs for which the node has a copy but are not currently queued) back in the queue, for all the time the queue is paused.\n\nA queue paused in output instead will behave in the following way:\n\n1. GETJOB will block even if there are jobs available in the specified queue, instead of serving the jobs. But GETJOB will unblock if the queue output pause is cleared later.\n2. The node will not provide jobs to other nodes in the context of node federation, for paused queues.\n\nSo a queue paused in output will stop acting as a source of messages for both\nlocal and non local clients.\n\nThe paused state can be set for each queue using the PAUSE command followed\nby options to specify how to change the paused state. Possible options are:\n\n* **in**: pause the queue in input.\n* **out**: pause the queue in output.\n* **all**: pause the queue in input and output (same as specifying both the **in** and **out** options).\n* **none**: clear the paused state in input and output.\n* **state**: just report the current queue state.\n* **bcast**: send a PAUSE command to all the reachable nodes of the cluster to set the same queue in the other nodes to the same state.\n\nThe command always returns the state of the queue after the execution of the specified options, so the return value is one of **in**, **out**, **all**, **none**.\n\nQueues paused in input or output are never evicted to reclaim memory, even if\nthey are empty and inactive for a long time, since otherwise the paused state\nwould be forgotten.\n\nFor example, in order to block output for the queue `myqueue` in all the\ncurrently reachable nodes, the following command should be send to a single node:\n\n    PAUSE myqueue out bcast\n\nTo specify **all** is the same as to specify both **in** and **out**, so the two following\nforms are equivalent:\n\n    PAUSE myqueue in out\n    PAUSE myqueue all\n\nTo just get the current state use:\n\n    PAUSE myqueue state\n    \"none\"\n\nSpecial handling of messages with RETRY set to 0\n===\n\nIn order to provide a coherent API, messages with at-most-once delivery\nsemantics are still retained after being delivered a first time, and should\nbe acknowledged like any other message. Of course, the acknowledge is not\nmandatory, since the message may be lost and there is no way for the receiver\nto get the same message again, since the message is associated with a retry\nvalue of 0.\n\nIn order to avoid non acknowledged messages with retry set to 0 from leaking\ninto Disque and eating all the memory, when the Disque server memory is full\nand starts to evict, it does not just evict acknowledged messages, but also\ncan evict non acknowledged messages having, at the same time, the following\ntwo properties:\n\n1. Their retry is set to 0.\n2. The job was already delivered.\n\nIn theory, acknowledging a job that will never be retried is a waste of time\nand resources, however this design has hidden advantages:\n\n1. The API is exactly the same for all the kinds of jobs.\n2. After the job is delivered, it is still possible to examine it. Observability is a very good property of messaging systems.\n\nHowever, not acknowledging the job does not result in big issues since they\nare evicted eventually during memory pressure.\n\nAdding and removing nodes at runtime\n===\n\nAdding nodes is trivial, and just consists in starting a new node and sending it\na `CLUSTER MEET` command. Assuming the node you just started is located\nat address 192.168.1.10 port 7714, and a random (you can use any) node of\nthe existing cluster is located at 192.168.1.9 port 7711, all you need to do\nis:\n\n    ./disque -h 192.168.1.10 -p 7714 cluster meet 192.168.1.9 7711\n\nNote that you can invert the source and destination arguments and the\nnew node will still join the cluster. It does not matter if it's the old node to\nmeet the new one or the other way around.\n\nIn order to remove a node, it is possible to use the crude way of just\nshutting it down, and then use `CLUSTER FORGET <old-node-id>` in all the\nother nodes in order to remove references to it from the configuration of\nthe other nodes. However this means that, for example, messages that had\na replication factor of 3, and one of the replicas was the node you are\nshutting down, suddenly are left with just 2 replicas even if no *actual*\nfailure happened. Moreover if the node you are removing had messages in\nqueue, you'll need to wait the retry time before the messages will be\nqueued again. For all these reasons, Disque has a better way to remove nodes\nwhich is described in the next section.\n\nGracefully removal of nodes\n===\n\nIn order to empty a node of its content before removing it, it is possible\nto use a feature that puts a node in *leaving* state. To enable this feature\njust contact the node to remove, and use the following command:\n\n    CLUSTER LEAVING yes\n\nThe node will start advertising itself as *leaving*, so in a matter of seconds\nall the cluster will know (if there are partitions, when the partition heals\nall the nodes will eventually be informed), and this is what happens\nwhen the node is in this state:\n\n1. When the node receives `ADDJOB` commands, it performs external replication, like when a node is near the memory limits. This means that it will make sure\nto create the number of replicas of the message in the cluster **without using itself** as a replica. So no new messages are created in the context of a node which is leaving.\n2. The node starts to send `-LEAVING` messages to all clients that use `GETJOB` but would block waiting for jobs. The `-LEAVING` error means the clients should connect to another node. Clients that were already blocked waiting for messages will be unblocked and a `-LEAVING` error will be sent to them as well.\n3. The node no longer sends `NEEDJOBS` messages in the context of Disque federation, so it will never ask other nodes to transfer messages to it.\n4. The node and all the other nodes will advertise it with a bad priority in the `HELLO` command output, so that clients will select a different node.\n5. The node will no longer create *dummy acks* in response to an `ACKJOB` command about a job it does not know.\n\nAll these behavior changes result in the node participating only as a source of messages, so eventually its message count will drop to zero (it is possible to check for this condition using `INFO jobs`). When this happens the node can be stopped and removed from the other nodes tables using `CLUSTER FORGET` as described in the section above.\n\nClient libraries\n===\n\nDisque uses the same protocol as Redis itself. To adapt Redis clients, or to use them directly, should be pretty easy. However note that Disque's default port is 7711 and not 6379.\n\nWhile a vanilla Redis client may work well with Disque, clients should optionally use the following protocol in order to connect with a Disque cluster:\n\n1. The client should be given a number of IP addresses and ports where nodes are located. The client should select random nodes and should try to connect until an available one is found.\n2. On a successful connection the `HELLO` command should be used in order to retrieve the Node ID and other potentially useful information (server version, number of nodes).\n3. If a consumer sees a high message rate received from foreign nodes, it may optionally have logic in order to retrieve messages directly from the nodes where producers are producing the messages for a given topic. The consumer can easily check the source of the messages by checking the Node ID prefix in the messages IDs.\n4. The `GETJOB` command, or other commands, may return a `-LEAVING` error instead of blocking. This error should be considered by the client library as a request to connect to a different node, since the node it is connected to is not able to serve the request since it is leaving the cluster. Nodes in this state have a very high *priority* number published via `HELLO`, so will be unlikely to be picked at the next connection attempt.\n\nThis way producers and consumers will eventually try to minimize node message exchanges whenever possible.\n\nSo basically you could perform basic usage using just a Redis client, however\nthere are already specialized client libraries implementing a more specialized\nAPI on top of Disque:\n\n*C++*\n\n- [disque C++ client](https://github.com/zhengshuxin/acl/tree/master/lib_acl_cpp/samples/disque)\n\n*Common Lisp*\n\n- [cl-disque](https://github.com/CodyReichert/cl-disque)\n\n*Elixir*\n\n- [exdisque](https://github.com/mosic/exdisque)\n\n*Erlang*\n\n- [edisque](https://github.com/nacmartin/edisque)\n\n*Go*\n\n- [disque-go](https://github.com/zencoder/disque-go)\n- [go-disque](https://github.com/EverythingMe/go-disque)\n- [disque](https://github.com/goware/disque)\n\n*Java*\n\n- [jedisque](https://github.com/xetorthio/jedisque)\n- [spinach](https://github.com/mp911de/spinach)\n\n*Node.js*\n\n- [disque.js](https://www.npmjs.com/package/disque.js)\n- [thunk-disque](https://github.com/thunks/thunk-disque)\n- [disqueue-node](https://www.npmjs.com/package/disqueue-node)\n\n*Perl*\n\n- [perl-disque](https://github.com/lovelle/perl-disque)\n\n*PHP*\n\n- [phpque](https://github.com/s12v/phpque) (PHP/HHVM)\n- [disque-php](https://github.com/mariano/disque-php) ([Composer/Packagist](https://packagist.org/packages/mariano/disque-php))\n- [disque-client-php](https://github.com/mavimo/disque-client-php) ([Composer/Packagist](https://packagist.org/packages/mavimo/disque-client))\n- [phloppy](https://github.com/0x20h/phloppy) ([Composer/Packagist](https://packagist.org/packages/0x20h/phloppy))\n\n*Python*\n\n- [disq](https://github.com/ryansb/disq) ([PyPi](https://pypi.python.org/pypi/disq))\n- [pydisque](https://github.com/ybrs/pydisque) ([PyPi](https://pypi.python.org/pypi/pydisque))\n- [django-q](https://github.com/koed00/django-q) ([PyPi](https://pypi.python.org/pypi/django-q))\n\n*Ruby*\n\n- [disque-rb](https://github.com/soveran/disque-rb)\n- [disque_jockey](https://github.com/DevinRiley/disque_jockey)\n- [Disc](https://github.com/pote/disc)\n\n*Rust*\n\n- [disque-rs](https://github.com/seppo0010/disque-rs)\n\n*.NET*\n\n- [Disque.Net](https://github.com/ziyasal/Disque.Net)\n\nImplementation details\n===\n\nJob replication strategy\n---\n\n1. Disque tries to replicate to W-1 (or W during out of memory) reachable nodes, shuffled.\n2. The cluster REPLJOB message is used to replicate a job to multiple nodes, the job is sent together with the list of nodes that may have a copy.\n2. If the required replication is not reached promptly, the job is send to one additional node every 50 milliseconds. When this happens, a new REPLJOB message is also re-sent to each node that may already have a copy, in order to refresh the list of nodes that have a copy.\n3. If the specified synchronous replication timeout is reached, the node that originally received the ADDJOB command from the client gives up and returns an error to the client. When this happens the node performs a best-effort procedure to delete the job from nodes that may have already received a copy of the job.\n\nCluster topology\n---\n\nDisque is a full mesh, with each node connected to each other. Disque performs\ndistributed failure detection via gossip, only in order to adjust the\nreplication strategy (try reachable nodes first when trying to replicate\na message), and in order to inform clients about non reachable nodes when\nthey want the list of nodes they can connect to.\n\nAs Disque is multi-master, the event of nodes failing is not handled in any\nspecial way.\n\nCluster messages\n---\n\nNodes communicate via a set of messages, using the node-to-node message bus.\nA few of the messages are used in order to check that other nodes are\nreachable and to mark nodes as failing. Those messages are PING, PONG and\nFAIL. Since failure detection is only used to adjust the replication strategy\n(talk with reachable nodes first in order to improve latency), the details\nare yet not described. Other messages are more important since they are used\nin order to replicate jobs, re-issue jobs while trying to minimize multiple\ndeliveries, and in order to auto-federate to serve consumers when messages\nare produced in different nodes compared to where consumers are.\n\nThe following is a list of messages and what they do, split by category.\nNote that this is just an informal description, while in the next sections\ndescribing the Disque state machine, there is a more detailed description\nof the behavior caused by message reception, and in what cases they are\ngenerated.\n\nCluster messages related to jobs replication and queueing\n---\n\n* REPLJOB: ask the receiver to replicate a job, that is, to add a copy of the job among the registered jobs in the target node. When a job is accepted, the receiver replies with GOTJOB to the sender. A job may not be accepted if the receiving node is near out of memory. In this case GOTJOB is not sent and the message discarded.\n* GOTJOB: The reply to REPLJOB to confirm the job was replicated.\n* ENQUEUE: Ask a node to put a given job into its queue. This message is used when a job is created by a node that does not want to take a copy, so it asks another node (among the ones that acknowledged the job replication) to queue it for the first time. If this message is lost, after the retry time some node will try to re-queue the message, unless retry is set to zero.\n* WILLQUEUE: This message is sent 500 milliseconds before a job is re-queued to all the nodes that may have a copy of the message, according to the sender table. If some of the receivers already have the job queued, they'll reply with QUEUED in order to prevent the sender to queue the job again (avoid multiple delivery when possible).\n* QUEUED: When a node re-queues a job, it sends QUEUED to all the nodes that may have a copy of the message, so that the other nodes will update the time at which they'll retry to queue the job. Moreover, every node that already has the same job in queue, but with a node ID which is lexicographically smaller than the sending node, will de-queue the message in order to best-effort de-dup messages that may be queued in multiple nodes at the same time.\n\nCluster messages related to ACK propagation and garbage collection\n---\n\n* SETACK: This message is sent to force a node to mark a job as successfully delivered (acknowledged by the worker): the job will no longer be considered active, and will never be re-queued by the receiving node. Also SETACK is send to the sender if the receiver of QUEUED or WILLQUEUE message has the same job marked as acknowledged (successfully delivered) already.\n* GOTACK: This message is sent in order to acknowledge a SETACK message. The receiver can mark a given node that may have a copy of a job, as informed about the fact that the job was acknowledged by the worker. Nodes delete (garbage collect) a message cluster wide when they believe all the nodes that may have a copy are informed about the fact the job was acknowledged.\n* DELJOB: Ask the receiver to remove a job. Is only sent in order to perform garbage collection of jobs by nodes that are sure the job was already delivered correctly. Usually the node sending DELJOB only does that when its sure that all the nodes that may have a copy of the message already marked the message ad delivered, however after some time the job GC may be performed anyway, in order to reclaim memory, and in that case, an otherwise avoidable multiple delivery of a job may happen. The DELJOB message is also used in order to implement *fast acknowledges*.\n\nCluster messages related to nodes federation\n---\n\n* NEEDJOBS(queue,count): The sender asks the receiver to obtain messages for a given queue, possibly *count* messages, but this is only an hit for congestion control and messages optimization, the receiver is free to reply with whatever number of messages. NEEDJOBS messages are delivered in two ways: broadcasted to every node in the cluster from time to time, in order to discover new source nodes for a given queue, or more often, to a set of nodes that recently replies with jobs for a given queue. This latter mechanism is called an *ad hoc* delivery, and is possible since every node remembers for some time the set of nodes that were recent providers of messages for a given queue. In both cases, NEEDJOBS messages are delivered with exponential delays, with the exception of queues that drop to zero-messages and have a positive recent import rate, in this case an ad hoc NEEDJOBS delivery is performed regardless of the last time the message was delivered in order to allow a continuous stream of messages under load.\n\n* YOURJOBS(array of messages): The reply to NEEDJOBS. An array of serialized jobs, usually all about the same queue (but future optimization may allow to send different jobs from different queues). Jobs into YOURJOBS replies are extracted from the local queue, and queued at the receiver node's queue with the same name. So even messages with a retry set to 0 (at most once delivery) still guarantee the safety rule since a given message may be in the source node, on the wire, or already received in the destination node. If a YOURJOBS message is lost, at least once delivery jobs will be re-queued later when the retry time is reached.\n\nDisque state machine\n---\n\nThis section shows the most interesting (as in less obvious) parts of the state machine each Disque node implements. While practically it is a single state machine, it is split in sections. The state machine description uses a convention that is not standard but should look familiar, since it is event driven, made of actions performed upon: message receptions in the form of commands received from clients, messages received from other cluster nodes, timers, and procedure calls.\n\nNote that: `job` is a job object with the following fields:\n\n1. `job.delivered`: A list of nodes that may have this message. This list does not need to be complete, is used for best-effort algorithms.\n2. `job.confirmed`: A list of nodes that confirmed reception of ACK by replying with a GOTJOB message.\n3. `job.id`: The job 48 chars ID.\n4. `job.state`: The job state among: `wait-repl`, `active`, `queued`, `acked`.\n5. `job.replicate`: Replication factor for this job.\n5. `job.qtime`: Time at which we need to re-queue the job.\n\nList fields such as `.delivered` and `.confirmed` support methods like `.size` to get the number of elements.\n\nStates are as follows:\n\n1. `wait-repl`: the job is waiting to be synchronously replicated.\n2. `active`: the job is active, either it reached the replication factor in the originating node, or it was created because the node received an `REPLJOB` message from another node.\n3. `queued`: the job is active and also is pending into a queue in this node.\n4. `acked`: the job is no longer active since a client confirmed the reception using the `ACKJOB` command or another Disque node sent a `SETACK` message for the job.\n\nGeneric functions\n---\n\nPROCEDURE `LOOKUP-JOB(string job-id)`:\n\n1. If job with the specified id is found, returns the corresponding job object.\n2. Otherwise returns NULL.\n\nPROCEDURE `UNREGISTER(object job)`:\n\n1. Delete the job from memory, and if queued, from the queue.\n\nPROCEDURE `ENQUEUE(job)`:\n\n1. If `job.state == queued` return ASAP.\n2. Add `job` into `job.queue`.\n3. Change `job.state` to `queued`.\n\nPROCEDURE `DEQUEUE(job)`:\n\n1. If `job.state != queued` return ASAP.\n2. Remove `job` from `job.queue`.\n3. Change `job.state` to `active`.\n\nON RECV cluster message: `DELJOB(string job.id)`:\n\n1. job = Call `LOOKUP-JOB(job-id)`.\n2. IF `job != NULL` THEN call `UNREGISTER(job)`.\n\nJob replication state machine\n---\n\nThis part of the state machine documents how clients add jobs to the cluster\nand how the cluster replicates jobs across different Disque nodes.\n\nON RECV client command `ADDJOB(string queue-name, string body, integer replicate, integer retry, integer ttl, ...):\n\n1. Create a job object in `wait-repl` state, having as body, ttl, retry, queue name, the specified values.\n2. Send REPLJOB(job.serialized) cluster message to `replicate-1` nodes.\n3. Block the client without replying.\n\nStep 3: We'll reply to the client in step 4 of `GOTJOB` message processing.\n\nON RECV cluster message `REPLJOB(object serialized-job)`:\n\n1. job = Call `LOOKUP-JOB(serialized-job.id)`.\n2. IF `job != NULL` THEN: job.delivered = UNION(job.delivered,serialized-job.delivered). Return ASAP, since we have the job.\n3. Create a job from serialized-job information.\n4. job.state = `active`.\n5. Reply to the sender with `GOTJOB(job.id)`.\n\nStep 1: We may already have the job, since REPLJOB may be duplicated.\n\nStep 2: If we already have the same job, we update the list of jobs that may have a copy of this job, performing the union of the list of nodes we have with the list of nodes in the serialized job.\n\nON RECV cluster message `GOTJOB(object serialized-job)`:\n\n1. job = Call `LOOKUP-JOB(serialized-job.id)`.\n2. IF `job == NULL` OR `job.state != wait-repl` Return ASAP.\n3. Add sender node to `job.confirmed`.\n4. IF `job.confirmed.size == job.replicate` THEN change `job.state` to `active`, call ENQUEUE(job), and reply to the blocked client with `job.id`.\n\nStep 4: As we receive enough confirmations via `GOTJOB` messages, we finally reach the replication factor required by the user and consider the message active.\n\nTIMER, firing every next 50 milliseconds while a job still did not reached the expected replication factor.\n\n1. Select an additional node not already listed in `job.delivered`, call it `node`.\n2. Add `node` to `job.delivered`.\n3. Send REPLJOB(job.serialized) cluster message to each node in `job.delivered`.\n\nStep 3: We send the message to every node again, so that each node will have a chance to update `job.delivered` with the new nodes. It is not required for each node to know the full list of nodes that may have a copy, but doing so improves our approximation of single delivery whenever possible.\n\nJob re-queueing state machine\n---\n\nThis part of the state machine documents how Disque nodes put a given job\nback into the queue after the specified retry time elapsed without the\njob being acknowledged.\n\nTIMER, firing 500 milliseconds before the retry time elapses:\n\n1. Send `WILLQUEUE(job.id)` to every node in `jobs.delivered`.\n\nTIMER, firing when `job.qtime` time is reached.\n\n1. If `job.retry == 0` THEN return ASAP.\n2. Call ENQUEUE(job).\n3. Update `job.qtime` to NOW + job.retry.\n4. Send `QUEUED(job.id)` message to each node in `job.delivered`.\n\nStep 1: At most once jobs never get enqueued again.\n\nStep 3: We'll retry again after the retry period.\n\nON RECV cluster message `WILLQUEUE(string job-id)`:\n\n1. job = Call `LOOKUP-JOB(job-id)`.\n2. IF `job == NULL` THEN return ASAP.\n3. IF `job.state == queued` SEND `QUEUED(job.id)` to `job.delivered`.\n4. IF `job.state == acked` SEND `SETACK(job.id)` to the sender.\n\nStep 3: We broadcast the message since likely the other nodes are going to retry as well.\n\nStep 4: SETACK processing is documented below in the acknowledges section of the state machine description.\n\nON RECV cluster message `QUEUED(string job-id)`:\n\n1. job = Call `LOOKUP-JOB(job-id)`.\n2. IF `job == NULL` THEN return ASAP.\n3. IF `job.state == acked` THEN return ASAP.\n4. IF `job.state == queued` THEN if sender node ID is greater than my node ID call DEQUEUE(job).\n5. Update `job.qtime` setting it to NOW + job.retry.\n\nStep 4: If multiple nodes re-queue the job about at the same time because of race conditions or network partitions that make `WILLQUEUE` not effective, then `QUEUED` forces receiving nodes to dequeue the message if the sender has a greater node ID, lowering the probability of unwanted multiple delivery.\n\nStep 5: Now the message is already queued somewhere else, but the node will retry again after the retry time.\n\nAcknowledged jobs garbage collection state machine\n---\n\nThis part of the state machine is used in order to garbage collect\nacknowledged jobs, when a job finally gets acknowledged by a client.\n\nPROCEDURE `ACK-JOB(job)`:\n\n1. If job state is already `acked`, do nothing and return ASAP.\n2. Change job state to `acked`, dequeue the job if queued, schedule first call to TIMER.\n\nPROCEDURE `START-GC(job)`:\n\n1. Send `SETACK(job.delivered.size)` to each node that is listed in `job.delivered` but is not listed in `job.confirmed`.\n2. IF `job.delivered.size == 0`, THEN send `SETACK(0)` to every node in the cluster.\n\nStep 2: this is an ACK about a job we don’t know. In that case, we can just broadcast the acknowledged hoping somebody knows about the job and replies.\n\nON RECV client command `ACKJOB(string job-id)`:\n\n1. job = Call `LOOKUP-JOB(job-id)`.\n1. if job is `NULL`, ignore the message and return.\n2. Call `ACK-JOB(job)`.\n3. Call `START-GC(job)`.\n\nON RECV cluster message `SETACK(string job-id, integer may-have)`:\n\n1. job = Call `LOOKUP-JOB(job-id)`.\n2. Call ACK-JOB(job) IF job is not `NULL`.\n3. Reply with GOTACK IF `job == NULL OR job.delivered.size <= may-have`.\n4. IF `job != NULL` and `jobs.delivered.size > may-have` THEN call `START-GC(job)`.\n5. IF `may-have == 0 AND job  != NULL`, reply with `GOTACK(1)` and call `START-GC(job)`.\n\nSteps 3 and 4 makes sure that among the reachable nodes that may have a message, garbage collection will be performed by the node that is aware of more nodes that may have a copy.\n\nStep 5 instead is used in order to start a GC attempt if we received a SETACK message from a node just hacking a dummy ACK (an acknowledge about a job it was not aware of).\n\nON RECV cluster message `GOTACK(string job-id, bool known)`:\n\n1. job = Call `LOOKUP-JOB(job-id)`. Return ASAP IF `job == NULL`.\n2. Call `ACK-JOB(job)`.\n3. IF `known == true AND job.delivered.size > 0` THEN add the sender node to `job.delivered`.\n4. IF `(known == true) OR (known == false AND job.delivered.size > 0) OR (known == false AND sender is an element of job.delivered)` THEN add the sender node to `jobs.confirmed`.\n5. IF `job.delivered.size > 0 AND job.delivered.size == job.confirmed.size`, THEN send `DELJOB(job.id)` to every node in the `job.delivered` list and call `UNREGISTER(job)`.\n6. IF `job.delivered == 0 AND known == true`, THEN call `UNREGISTER(job)`.\n7. IF `job.delivered == 0 AND job.confirmed.size == cluster.size` THEN call `UNREGISTER(job)`.\n\nStep 3: If `job.delivered.size` is zero, it means that the node just holds a *dummy ack* for the job. It means the node has an acknowledged job it created on the fly because a client acknowledged (via ACKJOB command) a job it was not aware of.\n\nStep 6: we don't have to hold a dummy acknowledged jobs if there are nodes that have the job already acknowledged.\n\nStep 7: this happens when nobody knows about a job, like when a client acknowledged a wrong job ID.\n\nTIMER, from time to time (exponential backoff with random error), for every acknowledged job in memory:\n\n1. call `START-GC(job)`.\n\nLimitations\n===\n\n* Disque is new code, not tested, and will require quite some time to reach production quality. It is likely very buggy and may contain wrong assumptions or tradeoffs.\n* As long as the software is non stable, the API may change in random ways without prior notification.\n* It is possible that Disque spends too much effort in approximating single delivery during failures. The **fast acknowledge** concept and command makes the user able to opt-out this efforts, but yet I may change the Disque implementation and internals in the future if I see the user base really not caring about multiple deliveries during partitions.\n* There is yet a lot of Redis dead code inside probably that could be removed.\n* Disque was designed a bit in *astronaut mode*, not triggered by an actual use case of mine, but more in response to what I was seeing people doing with Redis as a message queue and with other message queues. However I'm not an expert, if I succeeded to ship something useful for most users, this is kinda of an accomplishment. Otherwise it may just be that Disque is pretty useless.\n* As Redis, Disque is single threaded. While in Redis there are stronger reasons to do so, in Disque there is no manipulation of complex data structures, so maybe in the future it should be moved into a threaded server. We need to see what happens in real use cases in order to understand if it's worth it or not.\n* The number of jobs in a Disque process is limited to the amount of memory available. Again while this in Redis makes sense (IMHO), in Disque there are definitely simple ways in order to circumvent this limitation, like logging messages on disk when the server is out of memory and consuming back the messages when memory pressure is already acceptable. However in general, like in Redis, manipulating data structures in memory is a big advantage from the point of view of the implementation simplicity and the functionality we can provide to users.\n* Disque is completely not optimized for speed, was never profiled so far. I'm currently not aware of the fact it's slow, fast, or average, compared to other messaging solutions. For sure it is not going to have Redis-alike numbers because it does a lot more work at each command. For example when a job is added, it is serialized and transmitted to other `N` servers. There is a lot more message passing between nodes involved, and so forth. The good news is that being totally unoptimized, there is room for improvements.\n* Ability of federation to handle well low and high loads without incurring into congestion or high latency, was not tested well enough. The algorithm is reasonable but may fail short under many load patterns.\n* Amount of tested code path and possible states is not enough.\n\nFAQ\n===\n\nIs Disque part of Redis?\n---\n\nNo, it is a standalone project, however a big part of the Redis networking source code, nodes message bus, libraries, and the client protocol, were reused in this new project. In theory it was possible to extract the common code and release it as a framework to write distributed systems in C. However this is not a perfect solution as well, since the projects are expected to diverge more and more in the future, and to rely on a common foundation was hard. Moreover the initial effort to turn Redis into two different layers: an abstract server, networking stack and cluster bus, and the actual Redis implementation, was a huge effort, ways bigger than writing Disque itself.\n\nHowever while it is a separated project, conceptually Disque is related to Redis, since it tries to solve a Redis use case in a vertical, ad-hoc way.\n\nWho created Disque?\n---\n\nDisque is a side project of Salvatore Sanfilippo, aka @antirez.\n\nThere are chances for this project to be actively developed?\n---\n\nCurrently I consider this just a public alpha: If I see people happy to use it for the right reasons (i.e. it is better in some use cases compared to other message queues) I'll continue the development. Otherwise it was anyway cool to develop it, I had much fun, and I definitely learned new things.\n\nWhat happens when a node runs out of memory?\n---\n\n1. Maxmemory setting is mandatory in Disque, and defaults to 1GB.\n2. When 75% of maxmemory is reached, Disque starts to replicate the new jobs only to external nodes, without taking a local copy, so basically if there is free RAM into other nodes, adding still works.\n3. When 95% of maxmemory is reached, Disque starts to evict data that does not violates the safety guarantees: For instance acknowledged jobs and inactive queues.\n4. When 100% of maxmemory is reached, commands that may result into more memory used are not processed at all and the client is informed with an error.\n\nAre there plans to add the ability to hold more jobs than the physical memory of a single node can handle?\n---\n\nYes. In Disque it should be relatively simple to use the disk when memory is not\navailable, since jobs are immutable and don't need to necessarily exist in\nmemory at a given time.\n\nThere are multiple strategies available. The current idea is that\nwhen an instance is out of memory, jobs are stored into a log file instead\nof memory. As more free memory is available in the instance, on disk jobs\nare loaded.\n\nHowever in order to implement this, there is to observe strong evidence of its\ngeneral usefulness for the user base.\n\nWhen I consume and produce from different nodes, sometimes there is a delay in order for the jobs to reach the consumer, why?\n---\n\nDisque routing is not static, the cluster automatically tries to provide\nmessages to nodes where consumers are attached. When there is an high\nenough traffic (even one message per second is enough) nodes remember other\nnodes that recently were sources for jobs in a given queue, so it is possible\nto aggressively send messages asking for more jobs, every time there are\nconsumers waiting for more messages and the local queue is empty.\n\nHowever when the traffic is very low, informations about recent sources of\nmessages are discarded, and nodes rely on a more generic mechanism in order to\ndiscover other nodes that may have messages in the queues we need them (which\nis also used in high traffic conditions as well, in order to discover new\nsources of messages for a given queue).\n\nFor example imagine a setup with two nodes, A and B.\n\n1. A client attaches to node A and asks for jobs in the queue `myqueue`. Node A has no jobs enqueued, so the client is blocked.\n2. After a few seconds another client produces messages into `myqueue`, but sending them to node B.\n\nDuring step `1` if there was no recent traffic of imported messages for this queue, node A has no idea about who may have messages for the queue `myqueue`. Every other node may have, or none may have. So it starts to broadcast `NEEDJOBS` messages to the whole cluster. However we can't spam the cluster with messages, so if no reply is received after the first broadcast, the next will be sent with a larger delay, and so foth. The delay is exponential, with a maximum value of 30 seconds (this parameters will be configurable in the future, likely).\n\nWhen there is some traffic instead, nodes send `NEEDJOBS` messages ASAP to other nodes that were recent sources of messages. Even when no reply is received, the next `NEEDJOBS` messages will be sent more aggressively to the subset of nodes that had messages in the past, with a delay that starts at 25 milliseconds and has a maximum value of two seconds.\n\nIn order to minimize the latency, `NEEDJOBS` messages are not throttled at all when:\n\n1. A client consumed the last message from a given queue. Source nodes are informed immediately in order to receive messages before the node asks for more.\n2. Blocked clients are served the last message available in the queue.\n\nFor more information, please refer to the file `queue.c`, especially the function `needJobsForQueue` and its callers.\n\nAre messages re-enqueued in the queue tail or head or what?\n---\n\nMessages are put into the queue according to their *creation time* attribute. This means that they are enqueued in a best effort order in the local node queue. Messages that need to be put back into the queue again because their delivery failed are usually (but not always) older than messages already in queue, so they'll likely be among the first to be delivered to workers.\n\nWhat Disque means?\n---\n\nDIStributed QUEue but is also a joke with \"dis\" as negation (like in *dis*order) of the strict concept of queue, since Disque is not able to guarantee the strict ordering you expect from something called *queue*. And because of this tradeof it gains many other interesting things.\n\nCommunity: how to get help and how to help\n===\n\nGet in touch with us in one of the following ways:\n\n1. Post on [Stack Overflow](http://stackoverflow.com) using the `disque` tag. This is the preferred method to get general help about Disque: other users will easily find previous questions so we can incrementally build a knowledge base.\n2. Join the `#disque` IRC channel at **irc.freenode.net**.\n3. Create an Issue or Pull request if your question or issue is about the Disque implementation itself.\n\nThanks\n===\n\nI would like to say thank you to the following persons and companies.\n\n* Pivotal, for allowing me to work on Disque, most in my spare time, but sometimes during work hours. Moreover Pivotal agreed to leave the copyright of the code to me. This is very generous. Thanks Pivotal!\n* Michel Martens and Damian Janowski for providing early feedback about Disque while the project was still private.\n* Everybody who is already writing client libraries, sending pull requests, creating issues in order to move this forward from alpha to something actually usable.\n"
        },
        {
          "name": "deps",
          "type": "tree",
          "content": null
        },
        {
          "name": "disque.conf",
          "type": "blob",
          "size": 17.79296875,
          "content": "# Disque configuration file example\n\n# Note on units: when memory size is needed, it is possible to specify\n# it in the usual form of 1k 5GB 4M and so forth:\n#\n# 1k => 1000 bytes\n# 1kb => 1024 bytes\n# 1m => 1000000 bytes\n# 1mb => 1024*1024 bytes\n# 1g => 1000000000 bytes\n# 1gb => 1024*1024*1024 bytes\n#\n# units are case insensitive so 1GB 1Gb 1gB are all the same.\n\n################################## INCLUDES ###################################\n\n# Include one or more other config files here.  This is useful if you\n# have a standard template that goes to all Disque servers but also need\n# to customize a few per-server settings.  Include files can include\n# other files, so use this wisely.\n#\n# Notice option \"include\" won't be rewritten by command \"CONFIG REWRITE\"\n# from admin or Disque Sentinel. Since Disque always uses the last processed\n# line as value of a configuration directive, you'd better put includes\n# at the beginning of this file to avoid overwriting config change at runtime.\n#\n# If instead you are interested in using includes to override configuration\n# options, it is better to use include as the last line.\n#\n# include /path/to/local.conf\n# include /path/to/other.conf\n\n################################ GENERAL  #####################################\n\n# By default Disque does not run as a daemon. Use 'yes' if you need it.\n# Note that Disque will write a pid file in /var/run/disque.pid when daemonized.\ndaemonize no\n\n# When running daemonized, Disque writes a pid file in /var/run/disque.pid by\n# default. You can specify a custom pid file location here.\npidfile /var/run/disque.pid\n\n# Accept connections on the specified port, default is 7711.\n# If port 0 is specified Disque will not listen on a TCP socket.\nport 7711\n\n# TCP listen() backlog.\n#\n# In high requests-per-second environments you need an high backlog in order\n# to avoid slow clients connections issues. Note that the Linux kernel\n# will silently truncate it to the value of /proc/sys/net/core/somaxconn so\n# make sure to raise both the value of somaxconn and tcp_max_syn_backlog\n# in order to get the desired effect.\ntcp-backlog 511\n\n# By default Disque listens for connections from all the network interfaces\n# available on the server. It is possible to listen to just one or multiple\n# interfaces using the \"bind\" configuration directive, followed by one or\n# more IP addresses.\n#\n# Examples:\n#\n# bind 192.168.1.100 10.0.0.1\n# bind 127.0.0.1\n\n# Specify the path for the Unix socket that will be used to listen for\n# incoming connections. There is no default, so Disque will not listen\n# on a unix socket when not specified.\n#\n# unixsocket /tmp/disque.sock\n# unixsocketperm 700\n\n# Close the connection after a client is idle for N seconds (0 to disable)\ntimeout 0\n\n# TCP keepalive.\n#\n# If non-zero, use SO_KEEPALIVE to send TCP ACKs to clients in absence\n# of communication. This is useful for two reasons:\n#\n# 1) Detect dead peers.\n# 2) Take the connection alive from the point of view of network\n#    equipment in the middle.\n#\n# On Linux, the specified value (in seconds) is the period used to send ACKs.\n# Note that to close the connection the double of the time is needed.\n# On other kernels the period depends on the kernel configuration.\n#\n# A reasonable value for this option is 60 seconds.\ntcp-keepalive 0\n\n# Specify the server verbosity level.\n# This can be one of:\n# debug (a lot of information, useful for development/testing)\n# verbose (many rarely useful info, but not a mess like the debug level)\n# notice (moderately verbose, what you want in production probably)\n# warning (only very important / critical messages are logged)\nloglevel notice\n\n# Specify the log file name. Also the empty string can be used to force\n# Disque to log on the standard output. Note that if you use standard\n# output for logging but daemonize, logs will be sent to /dev/null\nlogfile \"\"\n\n# To enable logging to the system logger, just set 'syslog-enabled' to yes,\n# and optionally update the other syslog parameters to suit your needs.\n# syslog-enabled no\n\n# Specify the syslog identity.\n# syslog-ident disque\n\n# Specify the syslog facility. Must be USER or between LOCAL0-LOCAL7.\n# syslog-facility local0\n\n############################# WORKING DIRECTORY ###############################\n\n# The working directory.\n#\n# Data and state files will be written inside this directory.\n#\n# The Append Only File will also be created inside this directory.\n#\n# Note that you must specify a directory here, not a file name.\ndir ./\n\n################################## SECURITY ###################################\n\n# Require clients to issue AUTH <PASSWORD> before processing any other\n# commands.  This might be useful in environments in which you do not trust\n# others with access to the host running disque-server.\n#\n# This should stay commented out for backward compatibility and because most\n# people do not need auth (e.g. they run their own servers).\n#\n# Warning: since Disque is pretty fast an outside user can try up to\n# 150k passwords per second against a good box. This means that you should\n# use a very strong password otherwise it will be very easy to break.\n#\n# requirepass foobared\n\n# Command renaming.\n#\n# It is possible to change the name of dangerous commands in a shared\n# environment. For instance the CONFIG command may be renamed into something\n# hard to guess so that it will still be available for internal-use tools\n# but not available for general clients.\n#\n# Example:\n#\n# rename-command CONFIG b840fc02d524045429941cc15f59e41cb7be6c52\n#\n# It is also possible to completely kill a command by renaming it into\n# an empty string:\n#\n# rename-command CONFIG \"\"\n#\n# Please note that changing the name of commands that are logged into the\n# AOF file or transmitted to slaves may cause problems.\n\n################################### LIMITS ####################################\n\n# Set the max number of connected clients at the same time. By default\n# this limit is set to 10000 clients, however if the Disque server is not\n# able to configure the process file limit to allow for the specified limit\n# the max number of allowed clients is set to the current file limit\n# minus 32 (as Disque reserves a few file descriptors for internal uses).\n#\n# Once the limit is reached Disque will close all the new connections sending\n# an error 'max number of clients reached'.\n#\n# maxclients 10000\n\n# Don't use more memory than the specified amount of bytes.\n# When the memory limit is reached Disque will try to remove ACKs\n# or other state that can be safely evicted according to the specified\n# policy.\n#\n# If Disque can't free memory according to the policy, or if the policy is\n# set to 'noeviction', Disque will start to reply with errors to commands\n# that would use more memory.\n#\n# maxmemory <bytes>\n\n# MAXMEMORY POLICY: how Disque will select what to remove when maxmemory\n# is reached. You can select among the following behavior:\n#\n# \"noeviction\" -- Nothing is evicted, an error is returned to the user\n#                 on write operations.\n# \"acks\"       -- Disque will try to remove jobs in ACKED state in order to\n#                 reclaim memory.\n#\n# The default is:\n#\n# maxmemory-policy acks\n\n############################## APPEND ONLY MODE ###############################\n\n# The Append Only File is used when you want your server to be able to\n# persist data on disk and recover after a crash.\n\nappendonly no\n\n# The name of the append only file (default: \"appendonly.aof\")\n\nappendfilename \"disque.aof\"\n\n# The fsync() call tells the Operating System to actually write data on disk\n# instead of waiting for more data in the output buffer. Some OS will really flush\n# data on disk, some other OS will just try to do it ASAP.\n#\n# Disque supports three different modes:\n#\n# no: don't fsync, just let the OS flush the data when it wants. Faster.\n# always: fsync after every write to the append only log. Slow, Safest.\n# everysec: fsync only one time every second. Compromise.\n#\n# The default is \"everysec\", as that's usually the right compromise between\n# speed and data safety. It's up to you to understand if you can relax this to\n# \"no\" that will let the operating system flush the output buffer when\n# it wants, for better performances (but if you can live with the idea of\n# some data loss consider the default persistence mode that's snapshotting),\n# or on the contrary, use \"always\" that's very slow but a bit safer than\n# everysec.\n#\n# More details please check the following article:\n# http://antirez.com/post/redis-persistence-demystified.html\n#\n# If unsure, use \"everysec\".\n\n# appendfsync always\nappendfsync everysec\n# appendfsync no\n\n# Automatic rewrite of the append only file.\n# Disque is able to automatically rewrite the log file implicitly calling\n# BGREWRITEAOF when the AOF log size grows by the specified percentage.\n#\n# This is how it works: Disque remembers the size of the AOF file after the\n# latest rewrite (if no rewrite has happened since the restart, the size of\n# the AOF at startup is used).\n#\n# This base size is compared to the current size. If the current size is\n# bigger than the specified percentage, the rewrite is triggered. Also\n# you need to specify a minimal size for the AOF file to be rewritten, this\n# is useful to avoid rewriting the AOF file even if the percentage increase\n# is reached but it is still pretty small.\n#\n# Specify a percentage of zero in order to disable the automatic AOF\n# rewrite feature.\n\nauto-aof-rewrite-percentage 100\nauto-aof-rewrite-min-size 64mb\n\n# An AOF file may be found to be truncated at the end during the Disque\n# startup process, when the AOF data gets loaded back into memory.\n# This may happen when the system where Disque is running\n# crashes, especially when an ext4 filesystem is mounted without the\n# data=ordered option (however this can't happen when Disque itself\n# crashes or aborts but the operating system still works correctly).\n#\n# Disque can either exit with an error when this happens, or load as much\n# data as possible (the default now) and start if the AOF file is found\n# to be truncated at the end. The following option controls this behavior.\n#\n# If aof-load-truncated is set to yes, a truncated AOF file is loaded and\n# the Disque server starts emitting a log to inform the user of the event.\n# Otherwise if the option is set to no, the server aborts with an error\n# and refuses to start. When the option is set to no, the user requires\n# to fix the AOF file using the \"disque-check-aof\" utility before to restart\n# the server.\n#\n# Note that if the AOF file will be found to be corrupted in the middle\n# the server will still exit with an error. This option only applies when\n# Disque will try to read more data from the AOF file but not enough bytes\n# will be found.\naof-load-truncated yes\n\n# Normally when Disque is restarted with AOF enabled, it only loads job\n# data without putting the job into the queue again automatically.\n#\n# This is the default for a few reasons:\n# 1) In the time the server was down, it is possible that another node\n#    put the same job in its queue. Having it enqueued again after a\n#    restart may cause unnecessary additional deliveries.\n# 2) If the job has retry set to 0, reloading the \"queued\" state may\n#    violate the at-moce-once contract.\n# 3) Anyway jobs are requeued automatically if not acknowledged.\n#\n# However during server upgrades to load the full state again after the\n# restart may be a good idea: it's a small controlled downtime. Moreover\n# if before exiting the server rewrites the AOF from scratch, even\n# at-moce-once jobs are safe to put back into the queue.\n#\n# The \"aof-enqueue-jobs-once\" enables reloading the AOF with full queue\n# state for the next restart. Note that it also forces to load the AOF\n# even if the AOF is disabled. Once the AOF is loaded, the configuration\n# file is rewritten automatically to set the option back to 'no'.\n#\n# So in order to upgrade a server, it is possible to use the following\n# sequence of commands:\n#\n# CONFIG SET aof-enqueue-jobs-once yes\n# CONFIG REWRITE\n# SHUTDOWN REWRITE-AOF\n#\n# Then restart the server, that will load the AOF generated synchronously\n# via SHUTDOWN REWRITE-AOF, preserving the full state.\n#\n# Note that using this option when the AOF was not generated with\n# SHUTDOWN REWRITE-AOF is risky if you have at-most-once jobs, since the AOF\n# may not reflect the latest state of the server (the message could be\n# already delivered): use this option only in the way mentioned above.\naof-enqueue-jobs-once no\n\n################################### CLUSTER  ##################################\n\n# Every cluster node has a cluster configuration file. This file is not\n# intended to be edited by hand. It is created and updated by Disque nodes.\n# Every Disque Cluster node requires a different cluster configuration file.\n# Make sure that instances running in the same system do not have\n# overlapping cluster configuration file names.\n#\n# cluster-config-file nodes-7711.conf\n\n# Cluster node timeout is the amount of milliseconds a node must be unreachable\n# for it to be considered in failure state.\n# Most other internal time limits are multiple of the node timeout.\n#\n# cluster-node-timeout 15000\n\n################################## SLOW LOG ###################################\n\n# The Disque Slow Log is a system to log queries that exceeded a specified\n# execution time. The execution time does not include the I/O operations\n# like talking with the client, sending the reply and so forth,\n# but just the time needed to actually execute the command (this is the only\n# stage of command execution where the thread is blocked and can not serve\n# other requests in the meantime).\n#\n# You can configure the slow log with two parameters: one tells Disque\n# what is the execution time, in microseconds, to exceed in order for the\n# command to get logged, and the other parameter is the length of the\n# slow log. When a new command is logged the oldest one is removed from the\n# queue of logged commands.\n\n# The following time is expressed in microseconds, so 1000000 is equivalent\n# to one second. Note that a negative number disables the slow log, while\n# a value of zero forces the logging of every command.\nslowlog-log-slower-than 10000\n\n# There is no limit to this length. Just be aware that it will consume memory.\n# You can reclaim memory used by the slow log with SLOWLOG RESET.\nslowlog-max-len 128\n\n################################ LATENCY MONITOR ##############################\n\n# The Disque latency monitoring subsystem samples different operations\n# at runtime in order to collect data related to possible sources of\n# latency of a Disque instance.\n#\n# Via the LATENCY command this information is available to the user that can\n# print graphs and obtain reports.\n#\n# The system only logs operations that were performed in a time equal or\n# greater than the amount of milliseconds specified via the\n# latency-monitor-threshold configuration directive. When its value is set\n# to zero, the latency monitor is turned off.\n#\n# By default latency monitoring is disabled since it is mostly not needed\n# if you don't have latency issues, and collecting data has a performance\n# impact, that while very small, can be measured under big load. Latency\n# monitoring can easily be enalbed at runtime using the command\n# \"CONFIG SET latency-monitor-threshold <milliseconds>\" if needed.\nlatency-monitor-threshold 0\n\n############################### ADVANCED CONFIG ###############################\n\n# The client output buffer limits can be used to force disconnection of clients\n# that are not reading data from the server fast enough for some reason (a\n# common reason is that a Pub/Sub client can't consume messages as fast as the\n# publisher can produce them).\n#\n# The limit can be set differently for the three different classes of clients:\n#\n# normal -> normal clients including MONITOR clients\n# slave  -> slave clients\n# pubsub -> clients subscribed to at least one pubsub channel or pattern\n#\n# The syntax of every client-output-buffer-limit directive is the following:\n#\n# client-output-buffer-limit <class> <hard limit> <soft limit> <soft seconds>\n#\n# A client is immediately disconnected once the hard limit is reached, or if\n# the soft limit is reached and remains reached for the specified number of\n# seconds (continuously).\n# So for instance if the hard limit is 32 megabytes and the soft limit is\n# 16 megabytes / 10 seconds, the client will get disconnected immediately\n# if the size of the output buffers reach 32 megabytes, but will also get\n# disconnected if the client reaches 16 megabytes and continuously overcomes\n# the limit for 10 seconds.\n#\n# By default normal clients are not limited because they don't receive data\n# without asking (in a push way), but just after a request, so only\n# asynchronous clients may create a scenario where data is requested faster\n# than it can read.\n#\n# Instead there is a default limit for pubsub and slave clients, since\n# subscribers and slaves receive data in a push fashion.\n#\n# Both the hard or the soft limit can be disabled by setting them to zero.\nclient-output-buffer-limit normal 0 0 0\n\n# Disque calls an internal function to perform many background tasks, like\n# closing connections of clients in timeout, purging expired keys that are\n# never requested, and so forth.\n#\n# Not all tasks are performed with the same frequency, but Disque checks for\n# tasks to perform according to the specified \"hz\" value.\n#\n# By default \"hz\" is set to 10. Raising the value will use more CPU when\n# Disque is idle, but at the same time will make Disque more responsive when\n# there are many keys expiring at the same time, and timeouts may be\n# handled with more precision.\n#\n# The range is between 1 and 500, however a value over 100 is usually not\n# a good idea. Most users should use the default of 10 and raise this up to\n# 100 only in environments where very low latency is required.\nhz 10\n\n# When a child rewrites the AOF file, if the following option is enabled\n# the file will be fsync-ed every 32 MB of data generated. This is useful\n# in order to commit the file to the disk more incrementally and avoid\n# big latency spikes.\naof-rewrite-incremental-fsync yes\n"
        },
        {
          "name": "runtest",
          "type": "blob",
          "size": 0.265625,
          "content": "#!/bin/sh\nTCL_VERSIONS=\"8.5 8.6\"\nTCLSH=\"\"\n\nfor VERSION in $TCL_VERSIONS; do\n\tTCL=`which tclsh$VERSION 2>/dev/null` && TCLSH=$TCL\ndone\n\nif [ -z $TCLSH ]\nthen\n    echo \"You need tcl 8.5 or newer in order to run the Disque test\"\n    exit 1\nfi\n$TCLSH tests/cluster/run.tcl $*\n"
        },
        {
          "name": "src",
          "type": "tree",
          "content": null
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}