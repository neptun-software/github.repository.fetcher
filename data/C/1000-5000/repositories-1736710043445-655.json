{
  "metadata": {
    "timestamp": 1736710043445,
    "page": 655,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjY2MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "0voice/ffmpeg_develop_doc",
      "stars": 1992,
      "defaultBranch": "main",
      "files": [
        {
          "name": "3个重点，20个函数分析，浅析FFmpeg转码过程.md",
          "type": "blob",
          "size": 8.9384765625,
          "content": "## **写在前面**\n\n最近在做和转码有关的项目，接触到ffmpeg这个神器。从一开始简单的写脚本直接调用ffmpeg的可执行文件做些转码的工作，到后来需要写程序调用ffmpeg的API。虽然上网搜了别人的demo稍微改改顺利完成了工作，但是对于ffmpeg这个黑盒子，还是有些好奇心和担心（项目中使用不了解的代码总是不那么放心），于是抽空翻了翻ffmpeg的源码，整理成文章给大家分享分享。\n\n由于我并非做音频出身，对于音频一窍不通。ffmpeg整个也非常庞大，所以这篇文章从ffmpeg提供的转码的demo开始，侧重于讲清楚整个输入->转码->输出的流程，并学习ffmpeg如何做到通用和可扩展性。\n\n注：本文基于ffmpeg提供的transcode_aac.c样例。\n\n## **三个重点**\n\n转码的过程是怎么样的？简单来说就是从输入读取数据，解析原来的数据格式，转成目标数据格式，再将最终数据输出。这里就涉及到三个**点**：**数据输入和输出方式**，**数据的编码方式**及**数据的容器格式**（容器是用来区分不同文件的数据类型的，而编码格式则由音视频的压缩算法决定，一般所说的文件格式或者后缀名指的就是文件的容器。对于一种容器，可以包含不同编码格式的一种视频和音频）。\n\nffmpeg是一个非常非常通用的工具，支持非常广的数据输入和输出，包括：hls流，文件，内存等，支持各类数据编码格式，包括：aac，mp3等等，同时支持多种容器格式，包括ts，aac等。另外ffmpeg是通过C语言实现的，如果是C++，我们可以通过继承和多态来实现。定义一个IO的基类，一个Format的基类和一个Codec的基类，具体的输入输出协议继承IO基类实现各自的输入输出方法，具体的容器格式继承Format基类，具体的编码格式继承Codec基类。这篇文章也会简单讲解ffmpeg如何用C语言实现类似C++的继承和多态。\n\n## **基本数据结构**\n\nffmpeg转码中最基本的结构为AVFormatContext和AVCodecContext。AVCodecContext负责编码，AVFormatContext负责IO和容器格式。\n\n我从AVFormatContext类抽离出三个基本的成员iformat，oformat，pb。分别属于AVInputFormat，AVOutputFormat，AVIOContext类。iformat为输入的数据格式，oformat为输出的数据格式，pb则负责输入输出。\n\n![img](https://pic4.zhimg.com/v2-2632063d41df7a34fb60304434a75f1b_b.png)\n\n\n\n我把这三个类的定义抽离了出来简化了下，可以看出AVInputFormat声明了read_packet方法，AVOutputFormat声明了write_packet方法，AVIOContext声明了read_packet, write_packet方法。同时AVInputFormat和AVOutputFormat还有一个成员变量name用以标识该格式的后缀名。\n\n![img](https://pic3.zhimg.com/v2-58239fb65a540dc8b19241d70f7c9e7a_b.png)\n\n\n\n下一节我们会看到Input/OutputForm的read/write packet方法和IOContext的关系。\n\n## **输入函数调用图**\n\n下面是初始化输入的整个过程的函数调用图。\n\n![img](https://pic3.zhimg.com/v2-4b021abefdc7ab53c82833f722bf6e7a_b.png)\n\n\n\n首先从调用open_input_file开始，首先**解析输入的protocol**。avio_open2函数会调用一系列helper函数（ffurl_open，ffio_fdopen）分析输入的协议，设置AVFormatContext的pb变量的read_packet方法。而av_probe_input_buffer2函数则会**分析输入文件的格式**（从文件名解析或输入数据做判断），设置AVFormatContext的iformat的read_packet方法。\n\n![img](https://pic2.zhimg.com/v2-e9a529f86175c4cb2f21aa7f190e1769_b.png)\n\n\n\n两个read_packet有什么关系呢？第二个函数调用图可以看出，iformat的read_packet最终会调用pb的read_packet方法。意思就是**数据本身由pb的read_packet方法来读取，而iformat则会在输入的数据上做些格式相关的解析操作**（比如解析输入数据的头部，提取出输入数据中真正的音频/视频数据，再加以转码）。\n\n## **IO相关代码**\n\n直接看上面的图不太直观，这一节我把源码中各个步骤截图下来进行分析。\n\n转码开始步骤，调用open_input_file函数，传入文件名。\n\n![img](https://pic3.zhimg.com/v2-31edf2063a8c04656557b946311f7492_b.png)\n\navformat_open_input函数会调用init_input()来处理输入文件。\n\n![img](https://pic2.zhimg.com/v2-71305dac46d81e0e44fff17439bbabe9_b.png)\n\ninit_input函数主要做两个事情，一是解析输入协议（如何读取数据？hls流？文件？内存？），二是解析输入数据的格式（输入数据为aac？ts？m4a？）\n\n![img](https://pic4.zhimg.com/v2-d37bf73363d41ea844ac698758a0b353_b.png)\n\navio_open2函数首先调用ffurl_open函数，根据文件名来推断所属的输入协议（URLProtocol）。之后再调用ffio_fdopen设置pb的read_packet方法。\n\n![img](https://pic3.zhimg.com/v2-89e21351616f4b8c65dd386b8a41dc9e_b.png)\n\n\n\n![img](https://pic1.zhimg.com/v2-1e33906ff3c6f718970753d4eda2aaf0_b.png)\n\n\n\n![img](https://pic4.zhimg.com/v2-aeb0dbfcf245564406274ff644d63123_b.png)\n\n\n\n上面几段代码的逻辑为：根据文件名查找对应的URLProtocol->把该URLProtocol赋值给URLContext的prot成员变量->创建AVIOContext实例，赋值给AVFormatContext的pb成员变量。\n\n![img](https://pic3.zhimg.com/v2-2620110211da572988203229d29d0416_b.png)\n\n\n\n![img](https://pic4.zhimg.com/v2-fa4a5834ba7b3709ca9c5af2e1349b37_b.png)\n\n这里设置了AVIOContext实例的read_packet为ffurl_read方法。\n\n![img](https://pic4.zhimg.com/v2-520147188f5f044fed71dd64776a75ff_b.png)\n\nffurl_read方法其实就是调用URLContext的prot（上面赋值的）的url_read方法。通过函数指针去调用具体的URLContext对象的prot成员变量的url_read方法。\n\n![img](https://pic2.zhimg.com/v2-c250ed674ddac54fd9d3ef80b070a729_b.png)\n\n接下来看看解析输入数据格式的代码。av_probe_input_buffer2函数调用av_probe_input_format2函数来推断数据数据的格式。从之前的图我们知道*fmt其实就是&s->iformat。因此这里设置了AVFormatContext的iformat成员变量。\n\n![img](https://pic1.zhimg.com/v2-bfcaa2cb999687df6ddb09c1165f5310_b.png)\n\n\n\n至此AVFormatContext对象的iformat和pb成员变量就设置好了。接下来看看如何读取输入开始转码。\n\nav_read_frame函数调用read_frame_internal函数开始读取数据。\n\n![img](https://pic2.zhimg.com/v2-8dfe519e59eab253c3360e4eed36eef5_b.png)\n\n\n\nread_frame_internal会调用ff_read_packet，后者最终调用的是iformat成员变量的read_packet方法。\n\n![img](https://pic2.zhimg.com/v2-98b6b38ae8793c193d97ea70c1e78e39_b.png)\n\n\n\n![img](https://pic2.zhimg.com/v2-b47fb24bdbf9657b3ac1921dab6d4749_b.png)\n\n\n\n拿aac举例，aac的read_packet方法实际上是ff_raw_read_partial_packet函数。\n\n![img](https://pic3.zhimg.com/v2-bfd1eeaa1f4cd1c5733dc1392782f5d2_b.png)\n\n\n\nff_raw_read_partial_packet会调用ffio_read_partial，后者最终调用的是AVFormatContext的pb成员变量的read_packet方法。而我们知道pb成员的read_packet其实就是ffurl_read，也就是具体输入URLProtocl的read_packet方法。\n\n![img](https://pic2.zhimg.com/v2-e529b8f31ed3f9c136c38197a6f896e5_b.png)\n\n\n\n![img](https://pic2.zhimg.com/v2-c6f53da467f8278936ed779d2182e3dd_b.png)\n\n\n\n至此已经走完了整个输入的流程，输出也是类似的代码，这里就不再赘述。\n\n## **转码函数调用图**\n\n上面关于IO的介绍我从输入的角度进行分析。接下来的转码过程我则从输出的角度进行分析。下图是转码过程的函数调用图（做了简化）。load_encode_and_write调用encode_audio_frame, encode_audio_frame调用avcodec_encode_audio2来做实际的编码工作，最后调用av_write_frame将编码完的数据写入输出。\n\n![img](https://pic4.zhimg.com/v2-673c2b1a1d1db45ad83e93e4149f487f_b.png)\n\n## **转码相关代码**\n\n首先需要设置输出目标编码格式，下面的代码为设置编码格式（aac）的片段：\n\n![img](https://pic1.zhimg.com/v2-b7db4b56403fca052660be8b437e1df4_b.png)\n\n\n\n在这里设置了output_codec_context（AVCodecContext类对象）之后，从前面的函数调用图，我们知道是avcodec_encode_audio2函数执行的转码过程：\n\n![img](https://pic3.zhimg.com/v2-c24e2a95d172b8b5a6006097b565d48e_b.png)\n\n\n\n这里看到调用了avctx（AVCodecContext类对象）的codec（AVCodec类对象）成员变量的encode2方法去做编码操作。\n\n转码这里专业性比较强，我并没有细读，因此这里简单带过。\n\n## **总结**\n\n可以看出ffmpeg大量使用函数指针来实现类似C++的继承/多态的效果。并且ffmpeg具有非常好的扩展性。如果我需要自定义一个新的输入协议，只需要自己定义一个新的URLProtocol对象，实现read_packet方法即可。如果需要自定义一个新的容器格式，只需要定义一个新的AVInputFormat对象，实现read_packet方法即可。如果需要自定义一个新的编码格式，只需要定义一个新的AVCodec对象，实现encode2方法即可。真是非常赞的代码架构设计！\n\n"
        },
        {
          "name": "FFMPEG转码技术在HTML5视频系统中的研究与应用.pdf",
          "type": "blob",
          "size": 13577.765625,
          "content": ""
        },
        {
          "name": "FFMpeg写MP4文件例子分析.c",
          "type": "blob",
          "size": 15.44140625,
          "content": "/**\n这段时间看了FFMpeg提供的例子muxing.c，我略微修改了下源代码，使其生成一个MP4文件，音频使用AAC编码，视频使用H.264编码。代码很简单，我就不做说明了，代码如下。\n\n以后我们继续写如何将DirectShow中采集的音视频数据编码并生成MP4文件。\n*/\n\n/* 5 seconds stream duration */\n#define STREAM_DURATION   5.0\n#define STREAM_FRAME_RATE 25 /* 25 images/s */\n#define STREAM_NB_FRAMES  ((int)(STREAM_DURATION * STREAM_FRAME_RATE))\n#define STREAM_PIX_FMT PIX_FMT_YUV420P /* default pix_fmt */\n\nstatic int sws_flags = SWS_BICUBIC;\n\n/**************************************************************/\n/* audio output */\n\nstatic float t, tincr, tincr2;\nstatic int16_t *samples;\nstatic uint8_t *audio_outbuf;\nstatic int audio_outbuf_size;\nstatic int audio_input_frame_size;\n\n/*\n* add an audio output stream\n*/\nstatic AVStream *add_audio_stream(AVFormatContext *oc, enum CodecID codec_id)\n{\n    AVCodecContext *c;\n    AVStream *st;\n\n    st = avformat_new_stream(oc, NULL);\n    if (!st) {\n        fprintf(stderr, \"Could not alloc stream\\n\");\n        exit(1);\n    }\n    st->id = 1;\n\n    c = st->codec;\n    c->codec_id = codec_id;\n    c->codec_type = AVMEDIA_TYPE_AUDIO;\n\n    /* put sample parameters */\n    c->sample_fmt = AV_SAMPLE_FMT_S16;\n    c->bit_rate = 64000;\n    c->sample_rate = 44100;\n    c->channels = 2;\n\n    // some formats want stream headers to be separate\n    if (oc->oformat->flags & AVFMT_GLOBALHEADER)\n        c->flags |= CODEC_FLAG_GLOBAL_HEADER;\n\n    return st;\n}\n\nstatic void open_audio(AVFormatContext *oc, AVStream *st)\n{\n    AVCodecContext *c;\n    AVCodec *codec;\n\n    c = st->codec;\n\n    /* find the audio encoder */\n    codec = avcodec_find_encoder(c->codec_id);\n    if (!codec) {\n        fprintf(stderr, \"codec not found\\n\");\n        exit(1);\n    }\n\n    /* open it */\n    if (avcodec_open(c, codec) < 0) {\n        fprintf(stderr, \"could not open codec\\n\");\n        exit(1);\n    }\n\n    /* init signal generator */\n    t = 0;\n    tincr = 2 * M_PI * 110.0 / c->sample_rate;\n    /* increment frequency by 110 Hz per second */\n    tincr2 = 2 * M_PI * 110.0 / c->sample_rate / c->sample_rate;\n\n    audio_outbuf_size = 10000;\n    audio_outbuf = (uint8_t *)av_malloc(audio_outbuf_size);\n\n    /* ugly hack for PCM codecs (will be removed ASAP with new PCM\n    support to compute the input frame size in samples */\n    if (c->frame_size <= 1) {\n        audio_input_frame_size = audio_outbuf_size / c->channels;\n        switch(st->codec->codec_id) {\n        case CODEC_ID_PCM_S16LE:\n        case CODEC_ID_PCM_S16BE:\n        case CODEC_ID_PCM_U16LE:\n        case CODEC_ID_PCM_U16BE:\n            audio_input_frame_size >>= 1;\n            break;\n        default:\n            break;\n        }\n    } else {\n        audio_input_frame_size = c->frame_size;\n    }\n    samples = (int16_t *)av_malloc(audio_input_frame_size * 2 * c->channels);\n}\n\n/* prepare a 16 bit dummy audio frame of 'frame_size' samples and\n'nb_channels' channels */\nstatic void get_audio_frame(int16_t *samples, int frame_size, int nb_channels)\n{\n    int j, i, v;\n    int16_t *q;\n\n    q = samples;\n    for (j = 0; j < frame_size; j++) {\n        v = (int)(sin(t) * 10000);\n        for(i = 0; i < nb_channels; i++)\n            *q++ = v;\n        t += tincr;\n        tincr += tincr2;\n    }\n}\n\nstatic void write_audio_frame(AVFormatContext *oc, AVStream *st)\n{\n    AVCodecContext *c;\n    AVPacket pkt;\n    av_init_packet(&pkt);\n\n    c = st->codec;\n\n    get_audio_frame(samples, audio_input_frame_size, c->channels);\n\n    pkt.size = avcodec_encode_audio(c, audio_outbuf, audio_outbuf_size, samples);\n\n    if (c->coded_frame && c->coded_frame->pts != AV_NOPTS_VALUE)\n        pkt.pts= av_rescale_q(c->coded_frame->pts, c->time_base, st->time_base);\n    pkt.flags |= AV_PKT_FLAG_KEY;\n    pkt.stream_index = st->index;\n    pkt.data = audio_outbuf;\n\n    /* write the compressed frame in the media file */\n    if (av_interleaved_write_frame(oc, &pkt) != 0) {\n        fprintf(stderr, \"Error while writing audio frame\\n\");\n        exit(1);\n    }\n}\n\nstatic void close_audio(AVFormatContext *oc, AVStream *st)\n{\n    avcodec_close(st->codec);\n\n    av_free(samples);\n    av_free(audio_outbuf);\n}\n\n/**************************************************************/\n/* video output */\n\nstatic AVFrame *picture, *tmp_picture;\nstatic uint8_t *video_outbuf;\nstatic int frame_count, video_outbuf_size;\n\n/* add a video output stream */\nstatic AVStream *add_video_stream(AVFormatContext *oc, enum CodecID codec_id)\n{\n    AVCodecContext *c;\n    AVStream *st;\n    AVCodec *codec;\n\n    st = avformat_new_stream(oc, NULL);\n    if (!st) {\n        fprintf(stderr, \"Could not alloc stream\\n\");\n        exit(1);\n    }\n\n    c = st->codec;\n\n    /* find the video encoder */\n    codec = avcodec_find_encoder(codec_id);\n    if (!codec) {\n        fprintf(stderr, \"codec not found\\n\");\n        exit(1);\n    }\n    avcodec_get_context_defaults3(c, codec);\n\n    c->codec_id = codec_id;\n\n    /* put sample parameters */\n    c->bit_rate = /*400000*/3000000;\n    /* resolution must be a multiple of two */\n    c->width = /*352*/640;\n    c->height = /*288*/480;\n    /* time base: this is the fundamental unit of time (in seconds) in terms\n    of which frame timestamps are represented. for fixed-fps content,\n    timebase should be 1/framerate and timestamp increments should be\n    identically 1. */\n    c->time_base.den = STREAM_FRAME_RATE;\n    c->time_base.num = 1;\n    c->gop_size = 12; /* emit one intra frame every twelve frames at most */\n    c->pix_fmt = STREAM_PIX_FMT;\n    if (c->codec_id == CODEC_ID_MPEG2VIDEO) {\n        /* just for testing, we also add B frames */\n        c->max_b_frames = 2;\n    }\n    if (c->codec_id == CODEC_ID_MPEG1VIDEO){\n        /* Needed to avoid using macroblocks in which some coeffs overflow.\n        This does not happen with normal video, it just happens here as\n        the motion of the chroma plane does not match the luma plane. */\n        c->mb_decision=2;\n    }\n    // some formats want stream headers to be separate\n    if (oc->oformat->flags & AVFMT_GLOBALHEADER)\n        c->flags |= CODEC_FLAG_GLOBAL_HEADER;\n\n\n\n    return st;\n}\n\nstatic AVFrame *alloc_picture(enum PixelFormat pix_fmt, int width, int height)\n{\n    AVFrame *picture;\n    uint8_t *picture_buf;\n    int size;\n\n    picture = avcodec_alloc_frame();\n    if (!picture)\n        return NULL;\n    size = avpicture_get_size(pix_fmt, width, height);\n    picture_buf = (uint8_t *)av_malloc(size);\n    if (!picture_buf) {\n        av_free(picture);\n        return NULL;\n    }\n    avpicture_fill((AVPicture *)picture, picture_buf,\n        pix_fmt, width, height);\n    return picture;\n}\n\nstatic void open_video(AVFormatContext *oc, AVStream *st)\n{\n    AVCodec *codec;\n    AVCodecContext *c;\n\n    c = st->codec;\n\n    /* find the video encoder */\n    codec = avcodec_find_encoder(c->codec_id);\n    if (!codec) {\n        fprintf(stderr, \"codec not found\\n\");\n        exit(1);\n    }\n\n    /* open the codec */\n    if (avcodec_open(c, codec) < 0) {\n        fprintf(stderr, \"could not open codec\\n\");\n        exit(1);\n    }\n\n    video_outbuf = NULL;\n    if (!(oc->oformat->flags & AVFMT_RAWPICTURE)) {\n        /* allocate output buffer */\n        /* XXX: API change will be done */\n        /* buffers passed into lav* can be allocated any way you prefer,\n        as long as they're aligned enough for the architecture, and\n        they're freed appropriately (such as using av_free for buffers\n        allocated with av_malloc) */\n        video_outbuf_size = 200000;\n        video_outbuf = (uint8_t *)av_malloc(video_outbuf_size);\n    }\n\n    /* allocate the encoded raw picture */\n    picture = alloc_picture(c->pix_fmt, c->width, c->height);\n    if (!picture) {\n        fprintf(stderr, \"Could not allocate picture\\n\");\n        exit(1);\n    }\n\n    /* if the output format is not YUV420P, then a temporary YUV420P\n    picture is needed too. It is then converted to the required\n    output format */\n    tmp_picture = NULL;\n    if (c->pix_fmt != PIX_FMT_YUV420P) {\n        tmp_picture = alloc_picture(PIX_FMT_YUV420P, c->width, c->height);\n        if (!tmp_picture) {\n            fprintf(stderr, \"Could not allocate temporary picture\\n\");\n            exit(1);\n        }\n    }\n}\n\n/* prepare a dummy image */\nstatic void fill_yuv_image(AVFrame *pict, int frame_index, int width, int height)\n{\n    int x, y, i;\n\n    i = frame_index;\n\n    /* Y */\n    for (y = 0; y < height; y++) {\n        for (x = 0; x < width; x++) {\n            pict->data[0][y * pict->linesize[0] + x] = x + y + i * 3;\n        }\n    }\n\n    /* Cb and Cr */\n    for (y = 0; y < height/2; y++) {\n        for (x = 0; x < width/2; x++) {\n            pict->data[1][y * pict->linesize[1] + x] = 128 + y + i * 2;\n            pict->data[2][y * pict->linesize[2] + x] = 64 + x + i * 5;\n        }\n    }\n}\n\nstatic void write_video_frame(AVFormatContext *oc, AVStream *st)\n{\n    int out_size, ret;\n    AVCodecContext *c;\n    static struct SwsContext *img_convert_ctx;\n\n    c = st->codec;\n\n    if (frame_count >= STREAM_NB_FRAMES) {\n        /* no more frame to compress. The codec has a latency of a few\n        frames if using B frames, so we get the last frames by\n        passing the same picture again */\n    } else {\n        if (c->pix_fmt != PIX_FMT_YUV420P) {\n            /* as we only generate a YUV420P picture, we must convert it\n            to the codec pixel format if needed */\n            if (img_convert_ctx == NULL) {\n                img_convert_ctx = sws_getContext(c->width, c->height,\n                    PIX_FMT_YUV420P,\n                    c->width, c->height,\n                    c->pix_fmt,\n                    sws_flags, NULL, NULL, NULL);\n                if (img_convert_ctx == NULL) {\n                    fprintf(stderr, \"Cannot initialize the conversion context\\n\");\n                    exit(1);\n                }\n            }\n            fill_yuv_image(tmp_picture, frame_count, c->width, c->height);\n            sws_scale(img_convert_ctx, tmp_picture->data, tmp_picture->linesize,\n                0, c->height, picture->data, picture->linesize);\n        } else {\n            fill_yuv_image(picture, frame_count, c->width, c->height);\n        }\n    }\n\n\n    if (oc->oformat->flags & AVFMT_RAWPICTURE) {\n        /* raw video case. The API will change slightly in the near\n        future for that. */\n        AVPacket pkt;\n        av_init_packet(&pkt);\n\n        pkt.flags |= AV_PKT_FLAG_KEY;\n        pkt.stream_index = st->index;\n        pkt.data = (uint8_t *)picture;\n        pkt.size = sizeof(AVPicture);\n\n        ret = av_interleaved_write_frame(oc, &pkt);\n    } else {\n        /* encode the image */\n        out_size = avcodec_encode_video(c, video_outbuf, video_outbuf_size, picture);\n        /* if zero size, it means the image was buffered */\n        if (out_size > 0) {\n            AVPacket pkt;\n            av_init_packet(&pkt);\n\n            if (c->coded_frame->pts != AV_NOPTS_VALUE)\n                pkt.pts= av_rescale_q(c->coded_frame->pts, c->time_base, st->time_base);\n            if(c->coded_frame->key_frame)\n                pkt.flags |= AV_PKT_FLAG_KEY;\n            pkt.stream_index = st->index;\n            pkt.data = video_outbuf;\n            pkt.size = out_size;\n\n//            printf(\"pts %d \\n\", c->coded_frame->pts);\n\n            /* write the compressed frame in the media file */\n            ret = av_interleaved_write_frame(oc, &pkt);\n        } else {\n            ret = 0;\n        }\n    }\n    if (ret != 0) {\n        fprintf(stderr, \"Error while writing video frame\\n\");\n        exit(1);\n    }\n    frame_count++;\n}\n\nstatic void close_video(AVFormatContext *oc, AVStream *st)\n{\n    avcodec_close(st->codec);\n    av_free(picture->data[0]);\n    av_free(picture);\n    if (tmp_picture) {\n        av_free(tmp_picture->data[0]);\n        av_free(tmp_picture);\n    }\n    av_free(video_outbuf);\n}\n\n/**************************************************************/\n/* media file output */\n\n\nint main(int argc, char **argv)\n{\n    const char *filename;\n    AVOutputFormat *fmt;\n    AVFormatContext *oc;\n    AVStream *audio_st, *video_st;\n    double audio_pts, video_pts;\n    int i;\n\n    /* initialize libavcodec, and register all codecs and formats */\n    av_register_all();\n\n#if 0\n    if (argc != 2) {\n        printf(\"usage: %s output_file\\n\"\n            \"API example program to output a media file with libavformat.\\n\"\n            \"The output format is automatically guessed according to the file extension.\\n\"\n            \"Raw images can also be output by using '%%d' in the filename\\n\"\n            \"\\n\", argv[0]);\n        return 1;\n    }\n\n    filename = argv[1];\n#endif\n\n//#define RTMP_STREAM\n#ifdef RTMP_STREAM\n    filename = \"rtmp://192.168.0.239/live/livestream\";\n#else\n    filename = \"1.mp4\";\n#endif\n\n    /* allocate the output media context */\n    avformat_alloc_output_context2(&oc, NULL, NULL, filename);\n    if (!oc)\n    {\n        printf(\"Could not deduce output format from file extension: using MPEG.\\n\");\n        avformat_alloc_output_context2(&oc, NULL, /*\"mpeg\"*/\"flv\", filename);\n    }\n\n    if (!oc)\n    {\n        return 1;\n    }\n\n    // 强制指定 264 编码\n    oc->oformat->video_codec = CODEC_ID_H264;\n    oc->oformat->audio_codec = CODEC_ID_AAC;\n\n    fmt = oc->oformat;\n\n    /* add the audio and video streams using the default format codecs\n    and initialize the codecs */\n    video_st = NULL;\n    audio_st = NULL;\n    if (fmt->video_codec != CODEC_ID_NONE) {\n        video_st = add_video_stream(oc, fmt->video_codec);\n    }\n    if (fmt->audio_codec != CODEC_ID_NONE) {\n        audio_st = add_audio_stream(oc, fmt->audio_codec);\n    }\n\n    av_dump_format(oc, 0, filename, 1);\n\n    /* now that all the parameters are set, we can open the audio and\n    video codecs and allocate the necessary encode buffers */\n    if (video_st)\n        open_video(oc, video_st);\n    if (audio_st)\n        open_audio(oc, audio_st);\n\n    /* open the output file, if needed */\n    if (!(fmt->flags & AVFMT_NOFILE)) {\n        if (avio_open(&oc->pb, filename, AVIO_FLAG_WRITE) < 0) {\n            fprintf(stderr, \"Could not open '%s'\\n\", filename);\n            return 1;\n        }\n    }\n\n    /* write the stream header, if any */\n    avformat_write_header(oc, NULL);\n    picture->pts = 0;\n    for(;;)\n    {\n        /* compute current audio and video time */\n        if (audio_st)\n            audio_pts = (double)audio_st->pts.val * audio_st->time_base.num / audio_st->time_base.den;\n        else\n            audio_pts = 0.0;\n\n        if (video_st)\n            video_pts = (double)video_st->pts.val * video_st->time_base.num / video_st->time_base.den;\n        else\n            video_pts = 0.0;\n\n        if ((!audio_st || audio_pts >= STREAM_DURATION) &&\n            (!video_st || video_pts >= STREAM_DURATION))\n            break;\n\n        /* write interleaved audio and video frames */\n        if (!video_st || (video_st && audio_st && audio_pts < video_pts)) {\n            write_audio_frame(oc, audio_st);\n\n        } else {\n            write_video_frame(oc, video_st);\n\n            picture->pts++;\n        }\n    }\n\n    /* write the trailer, if any.  the trailer must be written\n    * before you close the CodecContexts open when you wrote the\n    * header; otherwise write_trailer may try to use memory that\n    * was freed on av_codec_close() */\n    av_write_trailer(oc);\n\n    /* close each codec */\n    if (video_st)\n        close_video(oc, video_st);\n    if (audio_st)\n        close_audio(oc, audio_st);\n\n    /* free the streams */\n    for(i = 0; i < oc->nb_streams; i++) {\n        av_freep(&oc->streams[i]->codec);\n        av_freep(&oc->streams[i]);\n    }\n\n    if (!(fmt->flags & AVFMT_NOFILE)) {\n        /* close the output file */\n        avio_close(oc->pb);\n    }\n\n    /* free the stream */\n    av_free(oc);\n\n    return 0;\n}\n"
        },
        {
          "name": "FFmpeg source code structure AVPacket, AVPacketSideData, AVBufferRef and AVBuffer.md",
          "type": "blob",
          "size": 24.5361328125,
          "content": "AVPacket stores the encoded frame data, which is usually output by demuxer and then transferred to decoder as input, or received from encoder as output and then transferred to muxer.\n\n![img](https://www.fatalerrors.org/images/blog/e39fa076ec6156ba98db984ecab4c8ad.jpg)\n\nFor video, it should usually contain a compressed frame. For audio, it may contain several compressed frames. Encoders allow output of empty packets, no compressed data, and only side data (for example, updating some stream parameters at the end of encoding).\n\nThe semantics of data ownership depends on the buf field. If this value is set, the Packet data is dynamically allocated and valid indefinitely until it is used for AV_ Packet_ The call to unref() reduces the reference count to 0.\n\nIf the buf field is not set, av_packet_ref() will make a copy instead of increasing the reference count.\n\nside data is always generated by av_malloc(), assigned by av_packet_ref() is copied by av_packet_unref() is released.\n\nsizeof(AVPacket) has been abandoned as a part of public ABI. Once AV_ init_ The Packet() function is removed, and the new Packet can only be created by av_packet_alloc(), new fields may be added to the end of the structure.\n\nNext, we will learn about AVPacket, and then we will lead out AVBufferRef and AVPacketSideData from AVPacket structure. Finally, we will lead out AVBuffer from AVBufferRef and AVPacketSideData from AVPacketSideData enumeration.\n\n## 1, AVPacket\n\nlibavcodec/packet.h\n\n```\ntypedef struct AVPacket {\n    /**\n     * A reference to the reference-counted buffer where the packet data is\n     * stored.\n     * May be NULL, then the packet data is not reference-counted.\n     */\n    AVBufferRef *buf;\n    /**\n     * Presentation timestamp in AVStream->time_base units; the time at which\n     * the decompressed packet will be presented to the user.\n     * Can be AV_NOPTS_VALUE if it is not stored in the file.\n     * pts MUST be larger or equal to dts as presentation cannot happen before\n     * decompression, unless one wants to view hex dumps. Some formats misuse\n     * the terms dts and pts/cts to mean something different. Such timestamps\n     * must be converted to true pts/dts before they are stored in AVPacket.\n     */\n    int64_t pts;\n    /**\n     * Decompression timestamp in AVStream->time_base units; the time at which\n     * the packet is decompressed.\n     * Can be AV_NOPTS_VALUE if it is not stored in the file.\n     */\n    int64_t dts;\n    uint8_t *data;\n    int   size;\n    int   stream_index;\n    /**\n     * A combination of AV_PKT_FLAG values\n     */\n    int   flags;\n    /**\n     * Additional packet data that can be provided by the container.\n     * Packet can contain several types of side information.\n     */\n    AVPacketSideData *side_data;\n    int side_data_elems;\n\n    /**\n     * Duration of this packet in AVStream->time_base units, 0 if unknown.\n     * Equals next_pts - this_pts in presentation order.\n     */\n    int64_t duration;\n\n    int64_t pos;                            ///< byte position in stream, -1 if unknown\n\n#if FF_API_CONVERGENCE_DURATION\n    /**\n     * @deprecated Same as the duration field, but as int64_t. This was required\n     * for Matroska subtitles, whose duration values could overflow when the\n     * duration field was still an int.\n     */\n    attribute_deprecated\n    int64_t convergence_duration;\n#endif\n} AVPacket;\n```\n\nHere's what each field means.\n\n| field                        | meaning                                                      |\n| ---------------------------- | ------------------------------------------------------------ |\n| AVBufferRef * buf            | The reference to the reference count buffer that stores the packet data. |\n| int64_t pts                  | Using avstream > time_ The time stamp displayed in the base time base is the time when the unpacked packet is presented to the user. |\n| int64_t dts                  | Using avstream > time_ Base is the time stamp of unpacking, and the time when the packet is unpacked. |\n| uint8_t * data               | The actual data buffer of the packet.                        |\n| int size                     | The actual data size of the packet.                          |\n| int stream_index             | The index of the stream.                                     |\n| int flags                    | AV_ PKT_ A combination of flag values.                       |\n| AVPacketSideData * side_data | Additional data that the container can provide.              |\n| int side_data_elems          | side_ The number of data elements.                           |\n| int64_t duration             | The duration of this packet is avstream > time_ Base, or 0 if unknown. |\n| int64_t pos                  | The byte position in the stream, or - 1 if unknown.          |\n\nHere's AV_ PKT_ The combined values that flag can use.\n\nlibavcodec/packet.h\n\n```\n#define AV_PKT_FLAG_KEY 0x0001 / / key frame\n#define AV_PKT_FLAG_CORRUPT 0x0002 / / corrupt data\n#define AV_PKT_FLAG_DISCARD 0x0004 / / is used to discard packet s that need to remain in a valid decoder state but do not need to be output, and should be discarded after decoding.\n#define AV_ PKT_ FLAG_ Trusted 0x0008 / / packet comes from a trusted source.\n#define AV_PKT_FLAG_DISPOSABLE 0x0010 / / used to indicate a packet containing a frame that can be discarded by the decoder, that is, a non referenced frame.\n```\n\n## 2, AVBufferRef\n\nA reference to a data buffer. The size of this structure is not part of the public ABI and is not intended to be allocated directly.\n\nlibavutil/buffer.h\n\n```\ntypedef struct AVBufferRef {\n     AVBuffer *buffer;\n \n     /**\n      * The data buffer. It is considered writable if and only if\n      * this is the only reference to the buffer, in which case\n      * av_buffer_is_writable() returns 1.\n      */\n     uint8_t *data;\n     /**\n      * Size of data in bytes.\n      */\n #if FF_API_BUFFER_SIZE_T\n     int      size;\n #else\n     size_t   size;\n #endif\n} AVBufferRef;\n```\n\n| field             | meaning                                                      |\n| ----------------- | ------------------------------------------------------------ |\n| AVBuffer *buffer  | A reference count buffer type. It is opaque, which means to use it by reference (AVBufferRef). |\n| uint8_t *data     | Data buffer. If and only if this is the only reference to the buffer, it is considered writable, in which case av_buffer_is_writable() returns 1. |\n| size_t / int size | The size of data in bytes.                                   |\n\n## 3, AVBuffer\n\nA reference count buffer type. Defined in libavutil / buffer_ In internal. H. It is opaque, which means to use it by reference (AVBufferRef).\n\nlibavutil/buffer_internal.h\n\n```\nstruct AVBuffer {\n    uint8_t *data; /**< data described by this buffer */\n    buffer_size_t size; /**< size of data in bytes */\n\n    /**\n     *  number of existing AVBufferRef instances referring to this buffer\n     */\n    atomic_uint refcount;\n\n    /**\n     * a callback for freeing the data\n     */\n    void (*free)(void *opaque, uint8_t *data);\n\n    /**\n     * an opaque pointer, to be used by the freeing callback\n     */\n    void *opaque;\n\n    /**\n     * A combination of AV_BUFFER_FLAG_*\n     */\n    int flags;\n\n    /**\n     * A combination of BUFFER_FLAG_*\n     */\n    int flags_internal;\n};\n```\n\n| field                                     | meaning                                                      |\n| ----------------------------------------- | ------------------------------------------------------------ |\n| uint8_t *data                             | The data described by the buffer.                            |\n| buffer_size_t size                        | The size of data in bytes.                                   |\n| atomic_uint refcount                      | The number of existing AVBufferRef instances that reference this buffer. |\n| void (*free)(void *opaque, uint8_t *data) | Callback used to release data.                               |\n| void *opaque                              | An opaque pointer used by the release callback function.     |\n| int flags                                 | AV_BUFFER_FLAG_ *The combination of the two.                 |\n| int flags_internal                        | BUFFER_FLAG_ *The combination of the two.                    |\n\nAVBuffer is an API for referencing count data buffers.\n\nThere are two core objects AVBuffer and AVBufferRef in this API. AVBuffer represents the data buffer itself; it is opaque and cannot be accessed directly by the caller, but only through AVBufferRef. However, the caller may compare two AVBuffer pointers to check whether two different references describe the same data buffer. AVBufferRef represents a single reference to AVBuffer, which can be operated directly by the caller.\n\nThere are two functions that can create a new AVBuffer with one reference -- av_buffer_alloc() is used to allocate a new buffer, av_buffer_create() is used to wrap an existing array in AVBuffer. From existing references, you can use av_buffer_ref() creates another reference. Using av_buffer_unref() releases a reference (once all references are released, the data is automatically released).\n\nThe Convention between this API and the rest of FFmpeg is that a buffer is considered writable if there is only one reference to it (and it is not marked read-only). AV is provided_ buffer_ is_ Write() function to check if this is true, and av_buffer_make_writable() will automatically create a new writable buffer if necessary.\n\nOf course, nothing prevents the calling code from violating this Convention, but it is only safe if all existing references are under its control.\n\nReference and dereference buffers are thread safe, so they can be used by multiple threads at the same time without any additional locks.\n\nTwo different references to the same buffer can point to different parts of the buffer (for example, their AVBufferRef.data The data will not be equal).\n\n## 4, AVPacketSideData\n\nAdditional Packet data that the container can provide. A Packet can contain several types of side information.\n\nlibavcodec/packet.h\n\n```\ntypedef struct AVPacketSideData {\n    uint8_t *data;\n#if FF_API_BUFFER_SIZE_T\n    int      size;\n#else\n    size_t   size;\n#endif\n    enum AVPacketSideDataType type;\n} AVPacketSideData;\n```\n\n| field                          | meaning                              |\n| ------------------------------ | ------------------------------------ |\n| uint8_t *data                  | Data cache.                          |\n| int / size_t size              | The size of the data cache in bytes. |\n| enum AVPacketSideDataType type | Packet side data type.               |\n\nThe AVPacketSideDataType enumeration defines various side data types.\n\nlibavcodec/packet.h\n\n```\n/**\n * @defgroup lavc_packet AVPacket\n *\n * Types and functions for working with AVPacket.\n * @{\n */\nenum AVPacketSideDataType {\n    /**\n     * An AV_PKT_DATA_PALETTE side data packet contains exactly AVPALETTE_SIZE\n     * bytes worth of palette. This side data signals that a new palette is\n     * present.\n     */\n    AV_PKT_DATA_PALETTE,\n\n    /**\n     * The AV_PKT_DATA_NEW_EXTRADATA is used to notify the codec or the format\n     * that the extradata buffer was changed and the receiving side should\n     * act upon it appropriately. The new extradata is embedded in the side\n     * data buffer and should be immediately used for processing the current\n     * frame or packet.\n     */\n    AV_PKT_DATA_NEW_EXTRADATA,\n\n    /**\n     * An AV_PKT_DATA_PARAM_CHANGE side data packet is laid out as follows:\n     * @code\n     * u32le param_flags\n     * if (param_flags & AV_SIDE_DATA_PARAM_CHANGE_CHANNEL_COUNT)\n     *     s32le channel_count\n     * if (param_flags & AV_SIDE_DATA_PARAM_CHANGE_CHANNEL_LAYOUT)\n     *     u64le channel_layout\n     * if (param_flags & AV_SIDE_DATA_PARAM_CHANGE_SAMPLE_RATE)\n     *     s32le sample_rate\n     * if (param_flags & AV_SIDE_DATA_PARAM_CHANGE_DIMENSIONS)\n     *     s32le width\n     *     s32le height\n     * @endcode\n     */\n    AV_PKT_DATA_PARAM_CHANGE,\n\n    /**\n     * An AV_PKT_DATA_H263_MB_INFO side data packet contains a number of\n     * structures with info about macroblocks relevant to splitting the\n     * packet into smaller packets on macroblock edges (e.g. as for RFC 2190).\n     * That is, it does not necessarily contain info about all macroblocks,\n     * as long as the distance between macroblocks in the info is smaller\n     * than the target payload size.\n     * Each MB info structure is 12 bytes, and is laid out as follows:\n     * @code\n     * u32le bit offset from the start of the packet\n     * u8    current quantizer at the start of the macroblock\n     * u8    GOB number\n     * u16le macroblock address within the GOB\n     * u8    horizontal MV predictor\n     * u8    vertical MV predictor\n     * u8    horizontal MV predictor for block number 3\n     * u8    vertical MV predictor for block number 3\n     * @endcode\n     */\n    AV_PKT_DATA_H263_MB_INFO,\n\n    /**\n     * This side data should be associated with an audio stream and contains\n     * ReplayGain information in form of the AVReplayGain struct.\n     */\n    AV_PKT_DATA_REPLAYGAIN,\n\n    /**\n     * This side data contains a 3x3 transformation matrix describing an affine\n     * transformation that needs to be applied to the decoded video frames for\n     * correct presentation.\n     *\n     * See libavutil/display.h for a detailed description of the data.\n     */\n    AV_PKT_DATA_DISPLAYMATRIX,\n\n    /**\n     * This side data should be associated with a video stream and contains\n     * Stereoscopic 3D information in form of the AVStereo3D struct.\n     */\n    AV_PKT_DATA_STEREO3D,\n\n    /**\n     * This side data should be associated with an audio stream and corresponds\n     * to enum AVAudioServiceType.\n     */\n    AV_PKT_DATA_AUDIO_SERVICE_TYPE,\n\n    /**\n     * This side data contains quality related information from the encoder.\n     * @code\n     * u32le quality factor of the compressed frame. Allowed range is between 1 (good) and FF_LAMBDA_MAX (bad).\n     * u8    picture type\n     * u8    error count\n     * u16   reserved\n     * u64le[error count] sum of squared differences between encoder in and output\n     * @endcode\n     */\n    AV_PKT_DATA_QUALITY_STATS,\n\n    /**\n     * This side data contains an integer value representing the stream index\n     * of a \"fallback\" track.  A fallback track indicates an alternate\n     * track to use when the current track can not be decoded for some reason.\n     * e.g. no decoder available for codec.\n     */\n    AV_PKT_DATA_FALLBACK_TRACK,\n\n    /**\n     * This side data corresponds to the AVCPBProperties struct.\n     */\n    AV_PKT_DATA_CPB_PROPERTIES,\n\n    /**\n     * Recommmends skipping the specified number of samples\n     * @code\n     * u32le number of samples to skip from start of this packet\n     * u32le number of samples to skip from end of this packet\n     * u8    reason for start skip\n     * u8    reason for end   skip (0=padding silence, 1=convergence)\n     * @endcode\n     */\n    AV_PKT_DATA_SKIP_SAMPLES,\n\n    /**\n     * An AV_PKT_DATA_JP_DUALMONO side data packet indicates that\n     * the packet may contain \"dual mono\" audio specific to Japanese DTV\n     * and if it is true, recommends only the selected channel to be used.\n     * @code\n     * u8    selected channels (0=mail/left, 1=sub/right, 2=both)\n     * @endcode\n     */\n    AV_PKT_DATA_JP_DUALMONO,\n\n    /**\n     * A list of zero terminated key/value strings. There is no end marker for\n     * the list, so it is required to rely on the side data size to stop.\n     */\n    AV_PKT_DATA_STRINGS_METADATA,\n\n    /**\n     * Subtitle event position\n     * @code\n     * u32le x1\n     * u32le y1\n     * u32le x2\n     * u32le y2\n     * @endcode\n     */\n    AV_PKT_DATA_SUBTITLE_POSITION,\n\n    /**\n     * Data found in BlockAdditional element of matroska container. There is\n     * no end marker for the data, so it is required to rely on the side data\n     * size to recognize the end. 8 byte id (as found in BlockAddId) followed\n     * by data.\n     */\n    AV_PKT_DATA_MATROSKA_BLOCKADDITIONAL,\n\n    /**\n     * The optional first identifier line of a WebVTT cue.\n     */\n    AV_PKT_DATA_WEBVTT_IDENTIFIER,\n\n    /**\n     * The optional settings (rendering instructions) that immediately\n     * follow the timestamp specifier of a WebVTT cue.\n     */\n    AV_PKT_DATA_WEBVTT_SETTINGS,\n\n    /**\n     * A list of zero terminated key/value strings. There is no end marker for\n     * the list, so it is required to rely on the side data size to stop. This\n     * side data includes updated metadata which appeared in the stream.\n     */\n    AV_PKT_DATA_METADATA_UPDATE,\n\n    /**\n     * MPEGTS stream ID as uint8_t, this is required to pass the stream ID\n     * information from the demuxer to the corresponding muxer.\n     */\n    AV_PKT_DATA_MPEGTS_STREAM_ID,\n\n    /**\n     * Mastering display metadata (based on SMPTE-2086:2014). This metadata\n     * should be associated with a video stream and contains data in the form\n     * of the AVMasteringDisplayMetadata struct.\n     */\n    AV_PKT_DATA_MASTERING_DISPLAY_METADATA,\n\n    /**\n     * This side data should be associated with a video stream and corresponds\n     * to the AVSphericalMapping structure.\n     */\n    AV_PKT_DATA_SPHERICAL,\n\n    /**\n     * Content light level (based on CTA-861.3). This metadata should be\n     * associated with a video stream and contains data in the form of the\n     * AVContentLightMetadata struct.\n     */\n    AV_PKT_DATA_CONTENT_LIGHT_LEVEL,\n\n    /**\n     * ATSC A53 Part 4 Closed Captions. This metadata should be associated with\n     * a video stream. A53 CC bitstream is stored as uint8_t in AVPacketSideData.data.\n     * The number of bytes of CC data is AVPacketSideData.size.\n     */\n    AV_PKT_DATA_A53_CC,\n\n    /**\n     * This side data is encryption initialization data.\n     * The format is not part of ABI, use av_encryption_init_info_* methods to\n     * access.\n     */\n    AV_PKT_DATA_ENCRYPTION_INIT_INFO,\n\n    /**\n     * This side data contains encryption info for how to decrypt the packet.\n     * The format is not part of ABI, use av_encryption_info_* methods to access.\n     */\n    AV_PKT_DATA_ENCRYPTION_INFO,\n\n    /**\n     * Active Format Description data consisting of a single byte as specified\n     * in ETSI TS 101 154 using AVActiveFormatDescription enum.\n     */\n    AV_PKT_DATA_AFD,\n\n    /**\n     * Producer Reference Time data corresponding to the AVProducerReferenceTime struct,\n     * usually exported by some encoders (on demand through the prft flag set in the\n     * AVCodecContext export_side_data field).\n     */\n    AV_PKT_DATA_PRFT,\n\n    /**\n     * ICC profile data consisting of an opaque octet buffer following the\n     * format described by ISO 15076-1.\n     */\n    AV_PKT_DATA_ICC_PROFILE,\n\n    /**\n     * DOVI configuration\n     * ref:\n     * dolby-vision-bitstreams-within-the-iso-base-media-file-format-v2.1.2, section 2.2\n     * dolby-vision-bitstreams-in-mpeg-2-transport-stream-multiplex-v1.2, section 3.3\n     * Tags are stored in struct AVDOVIDecoderConfigurationRecord.\n     */\n    AV_PKT_DATA_DOVI_CONF,\n\n    /**\n     * Timecode which conforms to SMPTE ST 12-1:2014. The data is an array of 4 uint32_t\n     * where the first uint32_t describes how many (1-3) of the other timecodes are used.\n     * The timecode format is described in the documentation of av_timecode_get_smpte_from_framenum()\n     * function in libavutil/timecode.h.\n     */\n    AV_PKT_DATA_S12M_TIMECODE,\n\n    /**\n     * The number of side data types.\n     * This is not part of the public API/ABI in the sense that it may\n     * change when new side data types are added.\n     * This must stay the last enum value.\n     * If its value becomes huge, some code using it\n     * needs to be updated as it assumes it to be smaller than other limits.\n     */\n    AV_PKT_DATA_NB\n};\n```\n\n| type                                   | meaning                                                      |\n| -------------------------------------- | ------------------------------------------------------------ |\n| AV_PKT_DATA_PALETTE                    | Palette, data size by AVPALETTE_SIZE decision.               |\n| AV_PKT_DATA_NEW_EXTRADATA              | Used to inform the codec or format that the extradata buffer has changed, and the receiver should take appropriate measures to do so. The new extradata is embedded in the side data buffer and should be used immediately to process the current frame or packet. |\n| AV_PKT_DATA_PARAM_CHANGE               | The layout is affected by the AVSideDataParamChangeFlags type. |\n| AV_PKT_DATA_H263_MB_INFO               | It contains a lot of structure about macroblock information, which is related to dividing the packet into smaller packets at the edge of macroblock. |\n| AV_PKT_DATA_REPLAYGAIN                 | It is associated with audio stream and contains replay gain information in the form of AVReplayGain structure. |\n| AV_PKT_DATA_DISPLAYMATRIX              | It contains a 3x3 transformation matrix, which describes an affine transformation, which needs to be applied to the decoded video frame to display correctly. |\n| AV_PKT_DATA_STEREO3D                   | It is associated with video stream and contains stereo 3D information in the form of avstereo 3D structure. |\n| AV_PKT_DATA_AUDIO_SERVICE_TYPE         | Associated with an audio stream and corresponding to enum type enum AVAudioServiceType. |\n| AV_PKT_DATA_QUALITY_STATS              | Contains quality related information from the encoder.       |\n| AV_PKT_DATA_FALLBACK_TRACK             | Contains an integer value that represents the stream index of the fallback track. |\n| AV_PKT_DATA_CPB_PROPERTIES             | It corresponds to AVCPBProperties structure.                 |\n| AV_PKT_DATA_SKIP_SAMPLES               | It is recommended to skip the specified number of samples.   |\n| AV_PKT_DATA_JP_DUALMONO                | Indicates that the packet may contain \"dual mono\" audio specific to Japanese DTV. If it is true, it is recommended to use only the selected channel. |\n| AV_PKT_DATA_STRINGS_METADATA           | List of string key value pairs.                              |\n| AV_PKT_DATA_SUBTITLE_POSITION          | The location of the subtitle event.                          |\n| AV_PKT_DATA_MATROSKA_BLOCKADDITIONAL   | The data found in the BlockAdditional element of the matroska container. |\n| AV_PKT_DATA_WEBVTT_IDENTIFIER          | The optional first identifier line of the WebVTT cue.        |\n| AV_PKT_DATA_WEBVTT_SETTINGS            | Optional setting (rendering description) after the timestamp specifier of WebVTT cue. |\n| AV_PKT_DATA_METADATA_UPDATE            | List of string key value pairs. Include update metadata that appears in the stream. |\n| AV_PKT_DATA_MPEGTS_STREAM_ID           | uint8_t type MPEGTS stream ID, which needs to transfer stream ID information from demuxer to corresponding muxer. |\n| AV_PKT_DATA_MASTERING_DISPLAY_METADATA | Mastering display metadata (based on SMPTE-2086:2014), which should be associated with video stream and stored in the form of avmasteringdisplay metadata structure. |\n| AV_PKT_DATA_SPHERICAL                  | It is associated with video stream and corresponds to avspherical mapping structure. |\n| AV_PKT_DATA_CONTENT_LIGHT_LEVEL        | Content light level (based on CTA-861.3). The metadata should be associated with the video stream and stored in the form of AVContentLightMetadata structure. |\n| AV_PKT_DATA_A53_CC                     | ATSC A53 Part 4 Closed Captions.                             |\n| AV_PKT_DATA_ENCRYPTION_INIT_INFO       | Encrypt initialization data.                                 |\n| AV_PKT_DATA_ENCRYPTION_INFO            | Contains encrypted information about how to decrypt a packet. |\n| AV_PKT_DATA_AFD                        | Active Format Description data. Describes the use of AVActiveFormatDescription in ETSI TS 101 154 to enumerate specified data consisting of a single byte. |\n| AV_PKT_DATA_PRFT                       | Producer reference time data corresponds to avproducer reference time structure, which is usually exported by some encoders (by exporting in AVCodecContext)_ side_ The prft tag is set in the data field. |\n| AV_PKT_DATA_ICC_PROFILE                | ICC profile data consisting of opaque eight byte buffers in the format described in ISO 15076-1. |\n| AV_PKT_DATA_DOVI_CONF                  | DOVI configuration.                                          |\n| AV_PKT_DATA_S12M_TIMECODE              | Timecode in accordance with SMPTE ST 12-1:2014.              |\n| AV_PKT_DATA_NB                         | Number of side data types.                                   |\n\nreference material:\n\n1. https://ffmpeg.org/doxygen/trunk/structAVPacket.html\n2. https://ffmpeg.org/doxygen/trunk/structAVBufferRef.html\n3. https://ffmpeg.org/doxygen/trunk/structAVBuffer.html\n4. https://ffmpeg.org/doxygen/trunk/structAVPacketSideData.html\n5. https://ffmpeg.org/doxygen/trunk/group__lavc__packet.html#ga9a80bfcacc586b483a973272800edb97\n"
        },
        {
          "name": "FFmpeg 学习(一)：FFmpeg 简介 .md",
          "type": "blob",
          "size": 3.9677734375,
          "content": "> 本文转载自博客园：https://www.cnblogs.com/renhui/p/6922971.html\n\n# 一、FFmpeg 介绍\n\nFFmpeg是一套可以用来记录、转换数字音频、视频，并能将其转化为流的开源计算机程序。采用LGPL或GPL许可证。它提供了录制、转换以及流化音视频的完整解决方案。它包含了非常先进的音频/视频编解码库。\n\n# 二、FFmpeg 组成\n\n- libavformat：用于各种音视频[封装格式](https://baike.baidu.com/item/封装格式)的生成和解析，包括获取解码所需信息以生成解码上下文结构和读取音视频帧等功能；\n- libavcodec：用于各种类型声音/图像编解码；\n- libavutil：包含一些公共的工具函数；\n- libswscale：用于视频场景比例缩放、色彩映射转换；\n- libpostproc：用于后期效果处理；\n- ffmpeg：该项目提供的一个工具，可用于格式转换、解码或[电视卡](https://baike.baidu.com/item/电视卡)即时编码等；\n- ffsever：一个 HTTP 多媒体即时广播串流服务器；\n- ffplay：是一个简单的播放器，使用ffmpeg 库解析和解码，通过SDL显示；\n\n# 三、FFmpeg包含类库说明\n\n### 2.1 类库说明\n\n- libavformat - 用于各种音视频封装格式的生成和解析，包括获取解码所需信息、读取音视频数据等功能。各种流媒体协议代码(如rtmpproto.c等)以及音视频格式的(解)复用代码(如flvdec.c、flvenc.c等)都位于该目录下。\n- libavcodec - 音视频各种格式的编解码。各种格式的编解码代码(如aacenc.c、aacdec.c等)都位于该目录下。\n- libavutil - 包含一些公共的工具函数的使用库，包括算数运算，字符操作等。\n- libswscale - 提供原始视频的比例缩放、色彩映射转换、图像颜色空间或格式转换的功能。\n- libswresample - 提供音频重采样，采样格式转换和混合等功能。\n- libavfilter - 各种音视频滤波器。\n- libpostproc - 用于后期效果处理，如图像的去块效应等。\n- libavdevice - 用于硬件的音视频采集、加速和显示。\n\n如果您之前没有阅读FFmpeg代码的经验，建议优先阅读libavformat、libavcodec以及libavutil下面的代码，它们提供了音视频开发的最基本功能，应用范围也是最广的。\n\n### 2.2 常用结构\n\nFFmpeg里面最常用的数据结构，按功能可大致分为以下几类(以下代码行数，以branch: origin/release/3.4为准)：\n\n#### 1. 封装格式\n\n- AVFormatContext - 描述了媒体文件的构成及基本信息，是统领全局的基本结构体，贯穿程序始终，很多函数都要用它作为参数；\n- AVInputFormat - 解复用器对象，每种作为输入的封装格式(例如FLV、MP4、TS等)对应一个该结构体，如libavformat/flvdec.c的ff_flv_demuxer；\n- AVOutputFormat - 复用器对象，每种作为输出的封装格式（例如FLV, MP4、TS等）对应一个该结构体，如libavformat/flvenc.c的ff_flv_muxer；\n- AVStream - 用于描述一个视频/音频流的相关数据信息。\n\n#### 2.编解码\n\n- AVCodecContext - 描述编解码器上下文的数据结构，包含了众多编解码器需要的参数信息；\n- AVCodec - 编解码器对象，每种编解码格式(例如H.264、AAC等）对应一个该结构体，如libavcodec/aacdec.c的ff_aac_decoder。每个AVCodecContext中含有一个AVCodec；\n- AVCodecParameters - 编解码参数，每个AVStream中都含有一个AVCodecParameters，用来存放当前流的编解码参数。\n\n#### 3. 网络协议\n\n- AVIOContext - 管理输入输出数据的结构体；\n- URLProtocol - 描述了音视频数据传输所使用的协议，每种传输协议(例如HTTP、RTMP)等，都会对应一个URLProtocol结构，如libavformat/http.c中的ff_http_protocol；\n- URLContext - 封装了协议对象及协议操作对象。\n\n#### 4. 数据存放\n\n- AVPacket - 存放编码后、解码前的压缩数据，即ES数据；\n- AVFrame - 存放编码前、解码后的原始数据，如YUV格式的视频数据或PCM格式的音频数据等；\n"
        },
        {
          "name": "FFmpeg 学习(七)：FFmpeg 学习整理总结.md",
          "type": "blob",
          "size": 5.345703125,
          "content": "# 一、FFmpeg 播放视频的基本流程整理\n\n播放流程: video.avi(Container) -> 打开得到 Video_Stream -> 读取Packet -> 解析到 Frame -> 显示Frame。\n\n- Container：在音视频中的容器，一般指的是一种特定的文件格式(如 AVI/QT )，里面指明了所包含的音视频，字幕等相关信息。\n- Stream：媒体流，指时间轴上的一段连续数据，如一段声音、视频或字幕数据。\n- Packet：Stream中的Raw数据,包含了可以被解码成方便我们最后在应用程序中操作的帧的原始数据。\n- Frame：Stream中的一个数据单元。\n- Codec：编解码器(Code 和 Decode)，如 Divx和 MP3,以帧为单位实现压缩数据和原始数据之间的相互转换。\n\n# 二、FFmpeg 各个结构体及相关方法流程整理\n\n### 1. AVCodec \n\nAVCodec -- 编解码器，采用链表维护，每一个都有其对应的名字、类型、CodecID和对数据进行处理的编解码函数指针。\n\n- avcodec_find_decoder/avcodec_find_encoder ：根据给定的codec id或解码器名称从系统中搜寻并返回一个AVCodec结构的指针\n- avcodec_alloc_context3：根据 AVCodec 分配合适的 AVCodecContext\n- avcodec_open/avcodec_open2/avcodec_close ：根据给定的 AVCodec 打开对应的Codec，并初始化 AVCodecContext/ 关闭Codec\n- avcodec_alloc_frame：分配编解码需要的 AVFrame 结构\n- avcodec_decode_video/avcodec_decode_video2 ：解码一个视频帧，输入数据在AVPacket结构中，输出数据在AVFrame结构中\n- avcodec_decode_audio4：解码一个音频帧。输入数据在AVPacket结构中，输出数据在AVFrame结构中\n- avcodec_encode_video/avcodec_encode_video2 ：编码一个视频帧，输入数据在AVFrame结构中，输出数据在AVPacket结构中 \n\n### 2. AVCodecContext\n\nAVCodecContext -- 和具体媒体数据相关的编解码器上下文，保存AVCodec指针和与codec相关的数据，包含了流中所使用的关于编解码器的所有信息\n\n- codec_name[32]、codec_type(AVMediaType)、codec_id(CodecID)、codec_tag：编解码器的名字、类型(音频/视频/字幕等)、ID(H264/MPEG4等)、FOURC等信息\n- hight/width,coded_width/coded_height： Video的高宽\n- sample_fmt：音频的原始采样格式, 是 SampleFormat 枚举\n- time_base：采用分数(den/num)保存了帧率的信息\n\n### 3. AVFrame\n\n- data/linesize：FFMpeg内部以平面的方式存储原始图像数据，即将图像像素分为多个平面（R/G/B或Y/U/V）数组\n- data数组：其中的指针指向各个像素平面的起始位置，编码时需要用户设置数据\n- linesize数组 ：存放各个存贮各个平面的缓冲区的行宽，编码时需要用户设置数据\n- key_frame：该图像是否是关键帧，由 libavcodec 设置\n- pict_type：该图像的编码类型：Intra(1)/Predicted(2)/Bi-dir(3) 等，默认值是 NONE(0)，其值由libavcodec设置\n- pts：呈现时间，编码时由用户设置\n- quality：从1(最好)到FF_LAMBDA_MAX(256*128-1,最差)，编码时用户设置，默认值是0\n- nterlaced_frame：表明是否是隔行扫描的,编码时用户指定，默认0\n\n### 4. AVFormatContext\n\nAVFormatContext -- 格式转换过程中实现输入和输出功能、保存相关数据的主要结构，描述了一个媒体文件或媒体流的构成和基本信息\n\n- nb_streams/streams ：AVStream结构指针数组, 包含了所有内嵌媒体流的描述，其内部有 AVInputFormat + AVOutputFormat 结构体，来表示输入输出的文件格式\n- avformat_open_input：创建并初始化部分值，但其他一些值(如 mux_rate、key 等)需要手工设置初始值，否则可能出现异常\n- avformat_alloc_output_context2：根据文件的输出格式、扩展名或文件名等分配合适的 AVFormatContext 结构\n\n### 5. AVPacket\n\nAVPacket -- 暂存解码之前的媒体数据（一个音/视频帧、一个字幕包等）及附加信息（解码时间戳、显示时间戳、时长等)，主要用于建立缓冲区并装载数据。\n\n- data/size/pos： 数据缓冲区指针、长度和媒体流中的字节偏移量\n- flags：标志域的组合，1(AV_PKT_FLAG_KEY)表示该数据是一个关键帧, 2(AV_PKT_FLAG_CORRUPT)表示该数据已经损坏\n- destruct：释放数据缓冲区的函数指针，其值可为 [av_destruct_packet]/av_destruct_packet_nofree, 会被 av_free_packet 调用\n\n### 6. AVStream\n\nAVStream -- 描述一个媒体流，其大部分信息可通过 avformat_open_input 根据文件头信息确定，其他信息可通过 avformat_find_stream_info 获取，典型的有 视频流、中英文音频流、中英文字幕流(Subtitle)，可通过 av_new_stream、avformat_new_stream 等创建。\n\n- index：在AVFormatContext中流的索引，其值自动生成(AVFormatContext::streams[index])\n- nb_frames：流内的帧数目\n- time_base：流的时间基准，是一个实数，该流中媒体数据的pts和dts都将以这个时间基准为粒度。通常，使用av_rescale/av_rescale_q可以实现不同时间基准的转换\n- avformat_find_stream_info：获取必要的编解码器参数(如 AVMediaType、CodecID )，设置到 AVFormatContext::streams[i]::codec 中\n- av_read_frame：从多媒体文件或多媒体流中读取媒体数据，获取的数据由 AVPacket 来存放\n- av_seek_frame：改变媒体文件的读写指针来实现对媒体文件的随机访问，通常支持基于时间、文件偏移、帧号(AVSEEK_FLAG_FRAME)的随机访问方式\n\n"
        },
        {
          "name": "FFmpeg 学习(三)：将 FFmpeg 移植到 Android平台.md",
          "type": "blob",
          "size": 6.4521484375,
          "content": "首先需要去FFmpeg的官网http://www.ffmpeg.org/去下载FFmpeg的源码，目前的版本号为FFmpeg3.3(Hilbert)。\n\n下载的文件为压缩包，解压后得到ffmpeg-3.3目录。\n\n**修改ffmpeg-3.3的configure文件：**\n\n```\n# 原来的配置内容：\nSLIBNAME_WITH_MAJOR='$(SLIBNAME).$(LIBMAJOR)'\n\nLIB_INSTALL_EXTRA_CMD='$$(RANLIB)\"$(LIBDIR)/$(LIBNAME)\"'\n\nSLIB_INSTALL_NAME='$(SLIBNAME_WITH_VERSION)'\n\nSLIB_INSTALL_LINKS='$(SLIBNAME_WITH_MAJOR)$(SLIBNAME)'\n\n#替换后的内容：\n\nSLIBNAME_WITH_MAJOR='$(SLIBPREF)$(FULLNAME)-$(LIBMAJOR)$(SLIBSUF)'\n\nLIB_INSTALL_EXTRA_CMD='$$(RANLIB)\"$(LIBDIR)/$(LIBNAME)\"'\n\nSLIB_INSTALL_NAME='$(SLIBNAME_WITH_MAJOR)'\n\nSLIB_INSTALL_LINKS='$(SLIBNAME)'\n```\n\n原因：如果不修改配置，直接进行编译出来的so文件类似libavcodec.so.55.39.101，文件的版本号位于so之后，这样在Android上无法加载，所以需要修改！\n\n**编写build_android.sh脚本文件:**\n\n在编译FFmpeg之前需要进行配置，设置相应的环境变量等。所有的配置选项都在ffmpeg-3.3/configure这个脚本文件中，执行如下命令可查看所有的配置选项：\n\n$ ./configure –help\n\n下面将配置项和环境变量设置写成一个sh脚本文件来运行以便编译出Android平台需要的so文件出来。\n\nbuild_android.sh的内容如下：\n\n```\n#!/bin/bash\nNDK=/Users/renhui/framework/android-ndk-r14b\nSYSROOT=$NDK/platforms/android-9/arch-arm/\nTOOLCHAIN=$NDK/toolchains/arm-linux-androideabi-4.9/prebuilt/darwin-x86_64\n\nfunction build_one\n{\n./configure \\\n    --prefix=$PREFIX \\\n    --enable-shared \\\n    --disable-static \\\n    --disable-doc \\--enable-cross-compile \\\n    --cross-prefix=$TOOLCHAIN/bin/arm-linux-androideabi- \\\n    --target-os=linux \\\n    --arch=arm \\\n    --sysroot=$SYSROOT \\\n    --extra-cflags=\"-Os -fpic $ADDI_CFLAGS\" \\\n    --extra-ldflags=\"$ADDI_LDFLAGS\" \\\n    $ADDITIONAL_CONFIGURE_FLAG\n}\nCPU=arm\nPREFIX=$(pwd)/android/$CPU\nADDI_CFLAGS=\"-marm\"\nbuild_one\n```\n\n需要确定的是NDK,SYSROOT和TOOLCHAIN是否是本地的环境，并确定cross-prefix指向的路径存在。\n\n保存脚本文件后，将脚本的权限提升：\n\n```\nchmod 777 build_android.sh  \n```\n\n然后执行脚本，该脚本会完成对ffmpeg的配置，并生成config.h等配置文件，后面的编译会用到。如果未经过配置直接进行编译会提示无法找到config.h文件等错误。\n\n然后执行下面两个命令：\n\n```\n$make  \n$make install  \n```\n\n至此，会在ffmpeg-3.3目录下生成一个android目录，其/android/arm/lib目录下的so库文件就是能够在Android上运行的so库。\n\n \n\n创建Demo工程，测试上面生成的so文件能否正常使用：\n\n1. 创建一个新的Android工程\n\n2. 在工程根目录下创建jni文件夹 \n\n3. 在jni下创建prebuilt目录，然后：将上面编译成功的so文件放入到该目录下\n\n4. 创建包含native方法的类，先在src下创建cn.renhui包，然后创建FFmpegNative.java类文件。主要包括加载so库文件和一个native测试方法两部分，其内容如下： \n\n   ```\n   package cn.renhui;\n   \n   public class FFmpegNative {\n   \n       static {\n           System.loadLibrary(\"avutil-55\");\n           System.loadLibrary(\"avcodec-57\");\n           System.loadLibrary(\"swresample-2\");\n           System.loadLibrary(\"avformat-57\");\n           System.loadLibrary(\"swscale-4\");\n           System.loadLibrary(\"avfilter-6\");\n           System.loadLibrary(\"avdevice-57\");\n           System.loadLibrary(\"ffmpeg_codec\");\n       }\n   \n       public native int avcodec_find_decoder(int codecID);\n   } \n   ```\n\n1. 用javah创建.头文件: classes目录，执行：javah-jni cn.renhui.FFmpegNative，会在当前目录产生cn_renhui_FFmpegNative.h的C头文件;\n\n1. 根据头文件名，建立相同名字c文件cn_renhui_FFmpegNative.c，在这个源文件中实现头文件中定义的方法，代码如下：\n\n   ```\n   #include \"cn_renhui_FFmpegNative.h\"\n   \n   #ifdef __cplusplus\n   extern \"C\" {\n   #endif\n   \n   JNIEXPORT jint JNICALL Java_cn_renhui_FFmpegNative_avcodec_1find_1decoder\n     (JNIEnv *env, jobject obj, jint codecID)\n   {\n       AVCodec *codec = NULL;\n   \n       /* register all formats and codecs */\n       av_register_all();\n   \n       codec = avcodec_find_decoder(codecID);\n   \n       if (codec != NULL)\n       {\n           return 0;\n       }\n       else\n       {\n           return -1;\n       }\n   }\n   \n   #ifdef __cplusplus\n   }\n   #endif\n   ```\n\n1. 编写Android.mk，内容如下：\n\n   ```\n   LOCAL_PATH := $(call my-dir)\n   \n   include $(CLEAR_VARS)\n   LOCAL_MODULE := avcodec-57-prebuilt\n   LOCAL_SRC_FILES := prebuilt/libavcodec-57.so\n   include $(PREBUILT_SHARED_LIBRARY)\n   \n   include $(CLEAR_VARS)\n   LOCAL_MODULE := avdevice-57-prebuilt\n   LOCAL_SRC_FILES := prebuilt/libavdevice-57.so\n   include $(PREBUILT_SHARED_LIBRARY)\n   \n   include $(CLEAR_VARS)\n   LOCAL_MODULE := avfilter-6-prebuilt\n   LOCAL_SRC_FILES := prebuilt/libavfilter-6.so\n   include $(PREBUILT_SHARED_LIBRARY)\n   \n   include $(CLEAR_VARS)\n   LOCAL_MODULE := avformat-57-prebuilt\n   LOCAL_SRC_FILES := prebuilt/libavformat-57.so\n   include $(PREBUILT_SHARED_LIBRARY)\n   \n   include $(CLEAR_VARS)\n   LOCAL_MODULE :=  avutil-55-prebuilt\n   LOCAL_SRC_FILES := prebuilt/libavutil-55.so\n   include $(PREBUILT_SHARED_LIBRARY)\n   \n   include $(CLEAR_VARS)\n   LOCAL_MODULE :=  avswresample-2-prebuilt\n   LOCAL_SRC_FILES := prebuilt/libswresample-2.so\n   include $(PREBUILT_SHARED_LIBRARY)\n   \n   include $(CLEAR_VARS)\n   LOCAL_MODULE :=  swscale-4-prebuilt\n   LOCAL_SRC_FILES := prebuilt/libswscale-4.so\n   include $(PREBUILT_SHARED_LIBRARY)\n   \n   include $(CLEAR_VARS)\n   \n   LOCAL_MODULE := ffmpeg_codec\n   LOCAL_SRC_FILES := cn_dennishucd_FFmpegNative.c\n   \n   LOCAL_LDLIBS := -llog -ljnigraphics -lz -landroid\n   LOCAL_SHARED_LIBRARIES := avcodec-57-prebuilt avdevice-57-prebuilt avfilter-6-prebuilt avformat-57-prebuilt avutil-55-prebuilt\n   \n   include $(BUILD_SHARED_LIBRARY)\n   ```\n\n1. 编译so文件，执行ndk-build\n\n2. 新建一个Activity，进行测试，测试核心代码：\n\n   ```\n   FFmpegNative ffmpeg = new FFmpegNative();\n   \n   int codecID = 28;\n   \n   int res = ffmpeg.avcodec_find_decoder(codecID);\n   \n   if (res == 0) {\n        tv.setText(\"Success!\");\n   } else {\n        tv.setText(\"Failed!\");\n   }\n   ```\n\n​    28是H264的编解码ID，可以在ffmpeg的源代码中找到，它是枚举类型定义的。在C语言中，可以换算为整型值。这里测试能否找到H264编解码，如果能找到，说明调用ffmpeg的库函数是成功的，这也表明我们编译的so文件是基本可用。\n\n"
        },
        {
          "name": "FFmpeg 学习(二)：Mac下安装FFmpeg.md",
          "type": "blob",
          "size": 2.0703125,
          "content": "> 本文转载自博客园：https://www.cnblogs.com/renhui/p/8458150.html\n\n# 一、安装ffmpeg\n\n分为两种安装方式：\n\n### 1. 命令行安装\n\n```\nbrew install ffmpeg\n```\n\n### 2. 下载压缩包安装\n\n去 http://evermeet.cx/ffmpeg/ 下载7z压缩包，解压缩后，将ffmpeg文件拷贝到一个地方，然后在bash_profile里面配置好环境变量\n\n# 二、安装ffplay\n\n分为两种安装方式：\n\n### 1. 命令行安装\n\n执行下面的命令就可以进行安装操作\n\n```\nbrew install ffmpeg --with-ffplay\n```\n\n> - 注：目前使用此安装方式安装后，执行ffplay会出现command not found的问题，可能是因为SDL的配置问题导致的。\n\n### 2. 下载压缩包安装\n\n去 http://evermeet.cx/ffmpeg/ 下载7z压缩包，解压缩后，将ffplay文件拷贝到一个地方，然后在bash_profile里面配置好环境变量\n\n# 三、附言\n\n在上面我们接触到了命令行安装ffmpeg的方法，除了安装选项 --with-ffplay外还有更多的选项如下：\n\n```xml\n–with-fdk-aac  (Enable the Fraunhofer FDK AAC library)\n–with-ffplay  (Enable FFplay media player)\n–with-freetype  (Build with freetype support)\n–with-frei0r  (Build with frei0r support)\n–with-libass  (Enable ASS/SSA subtitle format)\n–with-libcaca  (Build with libcaca support)\n–with-libvo-aacenc  (Enable VisualOn AAC encoder)\n–with-libvorbis  (Build with libvorbis support)\n–with-libvpx  (Build with libvpx support)\n–with-opencore-amr  (Build with opencore-amr support)\n–with-openjpeg  (Enable JPEG 2000 image format)\n–with-openssl  (Enable SSL support)\n–with-opus  (Build with opus support)\n–with-rtmpdump  (Enable RTMP protocol)\n–with-schroedinger  (Enable Dirac video format)\n–with-speex  (Build with speex support)\n–with-theora  (Build with theora support)\n–with-tools  (Enable additional FFmpeg tools)\n–without-faac  (Build without faac support)\n–without-lame  (Disable MP3 encoder)\n–without-x264  (Disable H.264 encoder)\n–without-xvid  (Disable Xvid MPEG-4 video encoder)\n–devel  (install development version 2.1.1)\n–HEAD  (install HEAD version)\n```\n\n"
        },
        {
          "name": "FFmpeg 学习(五)：FFmpeg 编解码 API 分析.md",
          "type": "blob",
          "size": 9.1005859375,
          "content": "在上一篇文章 FFmpeg学习(四)：FFmpeg API 介绍与通用 API 分析 中，我们简单的讲解了一下FFmpeg 的API基本概念，并分析了一下通用API，本文我们将分析 FFmpeg 在编解码时使用的API。\n\n## 一、FFmpeg 解码 API 分析\n\n\n\n### 1. avformat_open_input 分析\n\n函数 avformat_open_input 会根据所提供的文件路径判断文件的格式，其实就是通过这一步来决定到底是使用哪个Demuxer。\n\n举个例子：如果是flv，那么Demuxer就会使用对应的ff_flv_demuxer，所以对应的关键生命周期的方法read_header、read_packet、read_seek、read_close都会使用该flv的Demuxer中函数指针指定的函数。read_header会将AVStream结构体构造好，以方便后续的步骤继续使用AVStream作为输入参数。\n\n\n\n### 2. avformat_find_stream_info 分析\n\n该方法的作用就是把所有的Stream的MetaData信息填充好。方法内部会先查找对于的解码器，然后打开对应的解码器，紧接着会利用Demuxer中的read_packet函数读取一段数据进行解码，当然，解码的数据越多，分析出来的流信息就越准确，如果是本地资源，那么很快就可以得到准确的信息了。但是对于网络资源来说，则会比较慢，因此该函数有几个参数可以控制读取数据的长度，一个是probe size，一个是max_analyze_duration, 还有一个就是fps_probe_size，这三个参数共同控制解码数据的长度，如果配置的这几个参数的数值越小，那么这个函数执行的时间就会越快，但会导致AVStream结构体里面的信息（视频的宽、高、fps、编码类型）不准确。\n\n\n\n### 3. av_read_frame 分析\n\n该方法读取出来的数据是AVPacket，在FFmpeg的早期版本中开发给开发者的函数其实就是av_read_packet，但是需要开发者自己来处理AVPacket中的数据不能被解码器处理完的情况，即需要把未处理完的压缩数据缓存起来的问题。所以在新版本的FFmpeg中，提供了该函数，用于处理此状况。 该函数的实现首先会委托到Demuxer的read_packet方法中，当然read_packet通过解服用层和协议层的处理后，会将数据返回到这里，在该函数中进行数据缓冲处理。\n\n对于音频流，一个AVPacket可能会包含多个AVFrame，但是对于一个视频流，一个AVPacket只包含一个AVFrame，该函数最终只会返回一个AVPacket结构体。\n\n\n\n### 4. avcodec_decode分析\n\n该方法包含了两部分内容：一部分是解码视频，一部分是解码音频。在上面的函数分析中，我们知道，解码是会委托给对应的解码器来实施的，在打开解码器的时候就找到了对应的解码器的实现，比如对于解码H264来讲，会找到ff_h264_decoder，其中会有对应的生命周期函数的实现，最重要的就是init，decode，close三个方法，分别对应于打开解码器、解码及关闭解码器的操作，而解码过程就是调用decode方法。\n\n\n\n### 5. avformat_close_input 分析\n\n该函数负责释放对应的资源，首先会调用对应的Demuxer中的生命周期read_close方法，然后释放掉，AVFormatContext，最后关闭文件或者远程网络链接。 \n\n\n\n## 二、FFmpeg 编码 API 分析\n\n\n\n### 1. avformat_alloc_output_context2 分析\n\n该函数内部需要调用方法avformat_alloc_context来分配一个AVFormatContext结构体，当然最关键的还是根据上一步注册的Muxer和Demuxer部分（也就是封装格式部分）去找对应的格式。有可能是flv格式、MP4格式、mov格式，甚至是MP3格式等，如果找不到对应的格式（应该是因为在configure选项中没有打开这个格式的开关），那么这里会返回找不到对于的格式的错误提示。在调用API的时候，可以使用av_err2str把返回的整数类型的错误代码转换为肉眼可读的字符串，这是个在调试中非常有用的工具函数。该函数最终会将找出来的格式赋值给AVFormatContext类型的oformat。\n\n\n\n### 2. avio_open2 分析\n\n首先会调用函数ffurl_open，构造出URLContext结构体，这个结构体中包含了URLProtocol（需要去第一步register_protocol中已经注册的协议链表）中去寻找；接着会调用avio_alloc_contex方法，分配出AVIOContext结构体，并将上一步构造出来的URLProtocol传递进来；然后把上一步分配出来的AVIOContext结构体赋值给AVFormatContext属性。\n\n下面就是针对上面的描述总结的结构之间的构架图，各位可以参考此图进行进一步的理解：\n\n![img](https://images2018.cnblogs.com/blog/682616/201807/682616-20180720100812049-1285283157.png)\n\navio_open2的过程也恰好是在上面我们分析avformat_open_input过程的一个逆过程。编码过程和解码过程从逻辑上来讲，也是一个逆过程，所以在FFmpeg实现的过程中，他们也互为逆过程。\n\n\n\n### 3. 编码其他API（步骤）分析\n\n 编码的其他步骤也是解码的一个逆过程，解码过程中的avformat_find_stream_info对应到编码就是avformat_new_stream和avformat_write_header。\n\n- avformat_new_stream函数会将音频流或者视频流的信息填充好，分配出AVStream结构体，在音频流中分配声道、采样率、表示格式、编码器等信息，在视频中分配宽、高、帧率、表示格式、编码器等信息。\n- avformat_write_header函数与解码过程中的read_header恰好是一个逆过程，这里就不多赘述了。\n\n接下来就是编码阶段了：\n\n1. 将手动封装好的AVFrame结构体，作为avcodec_encodec_video方法的输入，然后将其编码成为AVPacket，然后调用av_write_frame方法输出到媒体文件中。\n\n2. av_write_frame 方法会将编码后的AVPacket结构体作为Muxer中的write_packet生命周期方法的输入，write_packet会加上自己封装格式的头信息，然后调用协议层，写到本地文件或者网络服务器上。\n\n3. 最后一步就是av_write_trailer（该函数有一个非常大的坑，如果没执行write_header操作，就直接执行write_trailer操作，程序会直接Carsh掉，所以这两个函数必须成对出现），av_write_trailer会把没有输出的AVPacket全部丢给协议层去做输出，然后会调用Muxer的write_trailer生命周期方法（不同的格式，写出的尾部也不一样）。\n\n \n\n\n\n## 三、FFmpeg 解码 API 超时设置\n\n当视频流地址能打开，但是视频流中并没有流内容的时候，可能会导致整体执行流程阻塞在 avformat_open_input 或者 av_read_frame 方法上。\n\n主要原因就是avformat_open_input 和av_read_frame 这两个方法是阻塞的。\n\nav_read_frame() -> read_frame_internal() -> ff_read_packet() -> s->iformat->read_packet() -> read_from_url() -> ffurl_read() -> retry_transfer_wrapper() (此方法会堵塞)\n\n虽然我们可以通过设置 ic->flags |= AVFMT_FLAG_NONBLOCK; 将操作设置为非阻塞，但这样设置是不推荐的，会导致后续的其他操作出现问题。\n\n一般情况下，我们推荐另外两种机制进行设置：\n\n\n\n### 1. 设置开流的超时时间\n\n 在设置开流超时时间的时候，需要注意 不同的协议设置的方式是不一样的。\n\n```\n方法：timeout --> 单位：（http:ms udp:s)``方法：stimeout --> 单位:（rtsp us）　\n```\n\n设置udp、http 超时的示例代码如下：\n\n```\nAVDictionary* opts = NULL;\nav_dict_set(&opts, \"timeout\", \"3000000\", 0);//单位 如果是http:ms  如果是udp:s\nint ret = avformat_open_input(&ctx, url, NULL, &opts);\n```\n\n \n\n设置rtsp超时的示例代码如下：\n\n```\nAVDictionary* opts = NULL;\nav_dict_set(&opts, \"rtsp_transport\", m_bTcp ? \"tcp\" : \"udp\", 0); //设置tcp or udp，默认一般优先tcp再尝试udp\nav_dict_set(&opts, \"stimeout\", \"3000000\", 0);//单位us 也就是这里设置的是3s\nret = avformat_open_input(&ctx, url, NULL, &opts);\n```\n\n\n\n### 2. 设置interrupt_callback定义返回机制\n\n设置回调，监控read超时情况，回调方法为：\n\n\n\n```\nint64_t lastReadPacktTime;\nstatic int interrupt_cb(void *ctx)\n{\n    int timeout = 3;\n    if (av_gettime() - lastReadPacktTime > timeout * 1000 * 1000)\n    {\n        return -1;\n    }\n    return 0;\n}\n```\n\n\n\n回调函数中返回0则代表ffmpeg继续阻塞直到ffmpeg正常工作为止，否则就代表ffmpeg结束阻塞可以将操纵权交给用户线程并返回错误码。\n\n对指定的 AVFormatContext 进行设置，并在需要调用的设置的时间之前，记录当前的时间，这样在回调的时候就能根据时间差，判断执行相应的逻辑：\n\navformat_open_input 设置方式：\n\n```\ninputContext = avformat_alloc_context();\nlastReadPacktTime = av_gettime();\ninputContext->interrupt_callback.callback = interrupt_cb;\nint ret = avformat_open_input(&inputContext, inputUrl.c_str(), nullptr, nullptr);\n```\n\n \n\nav_read_frame 设置方式：\n\n```\nlastReadPacktTime = av_gettime();\nret = av_read_frame(inputContext, packet);\n```\n\n在实际开发中，只是设计这个机制，很容易出现超时，但如果超时时间设置过程，又容易阻塞线程。一般推荐的方案为：在超时的机制上增加连续读流的时长统计，当连续读流超时超过一定时间时就通知当前读流操作已失败。\n"
        },
        {
          "name": "FFmpeg 学习(六)：FFmpeg 核心模块 libavformat 与 libavcodec 分析.md",
          "type": "blob",
          "size": 1.22265625,
          "content": "# 一、libavformat介绍\n\nlibavformat的主要组成与层次调用关系如下图：\n\n![image](https://user-images.githubusercontent.com/87457873/148345134-91ce7724-18ec-4b1b-823b-12238e9c7a31.png)\n\nAVFromatContext是API层直接接触到的结构体，它会进行格式的封装和解封装，它的数据部分由底层提供，底层使用了AVIOContext，这个AVIOContext实际上就是为普通的I/O增加了一层Buffer缓冲区，再往底层就是URLContext，也就是达到了协议层，协议层的实现由很多，如rtmp、http、hls、file等，这个就是libavformat的内部封装结构了。\n\n# 二、libavcodec介绍\n\nlibavcodec模块的主要组成和数据结构图如下：\n\n ![img](https://images2018.cnblogs.com/blog/682616/201807/682616-20180720180926182-1853199081.png)\n\n对于开发者来说，这个模块我们能接触到的最顶层的数据结构就是AVCodecContext，该结构体包含的就是与实际的编解码有关的部分。\n\n首先AVCodecContext是包含在一个AVStream里面的，即描述了这路流的编码格式是什么，然后利用该编码器或者解码器进行AVPacket与AVFrame之间的转换（实际上就是编码或者解码的过程），这是FFmpeg中最重要的一部分。\n\n"
        },
        {
          "name": "FFmpeg 学习(四)：FFmpeg API 介绍与通用 API 分析.md",
          "type": "blob",
          "size": 4.2841796875,
          "content": "\n## 一、FFmpeg 编解码流程\n\nFFmpeg编解码流程图如下，此图包含了整体的解封装、编解码的基本流程。\n\n![img](https://img2020.cnblogs.com/blog/682616/202104/682616-20210402124122366-1492811886.png)\n\n下面我们要介绍的术语及相关API都是围绕这个流程图展开的。\n\n\n\n## 二、FFmpeg 相关术语\n\n**1. 容器/文件（Container/File）**：即特定格式的多媒体文件，比如MP4，flv，mov等。\n\n**2. 媒体流（Stream）**：表示在时间轴上的一段连续的数据，比如一段声音数据、一段视频数据或者一段字母数据，可以是压缩的，也可以是非压缩的，压缩的数据需要关联特定的编解码器。\n\n**3. 数据帧/数据包（Frame/Packet）**：通常一个媒体流是由大量的数据帧组成的，对于压缩数据，帧对应着编解码器的最小处理单元，分属于不同媒体流的数据帧交错存储与容器之中。\n\n**4. 编解码器**：编解码器是以帧为单位实现压缩数据和原始数据之间的相互转换的。\n\n前面介绍的术语，就是FFmpeg中抽象出来的概念。其中：\n\n**1. AVFormatContext**：就是对容器或者媒体文件层次的抽象。\n\n**2. AVStream**：在文件中（容器里面）包含了多路流（音频流、视频流、字幕流），AVStream 就是对流的抽象。\n\n**3. AVCodecContext 与 AVCodec**：在每一路流中都会描述这路流的编码格式，对编解码器格式以及编解码器的抽象就是AVCodecContext 与 AVCodec。\n\n**4. AVPacket 与 AVFrame**：对于编码器或者解码器的输入输出部分，也就是压缩数据以及原始数据的抽象就是AVPacket与AVFrame。\n\n**5. AVFilte**r：除了编解码之外，对音视频的处理肯定是针对于原始数据的处理，也就是针对AVFrame的处理，使用的就是AVFilter。\n\n\n\n## 三、FFmpeg 通用 API 分析\n\n\n\n### 1. av_register_all 分析 \n\n在最开始编译FFmpeg的时候，我们做了一个configure的配置，其中开启或者关闭了很多选项。configure的配置会生成两个文件：config.mk和config.h。\n\n> config.mk：就是makefile文件需要包含进去的子模块，会作用在编译阶段，帮助开发者编译出正确的库。\n>\n> config.h：作用在运行阶段，主要是确定需要注册那些容器及编解码格式到FFmpeg框架中。\n\n调用 av_register_all 就可以注册config.h里面开发的编解码器，然后会注册所有的Muxer和Demuxer（封装格式），最后注册所有的Protocol（协议）。\n\n这样在configure时开启或者关闭的选项就作用到了运行时，该函数的源码分析设计的源码文件包括：url.c、allformats.c、mux.c、format.c 等文件。已经将这几个源码文件单独提出来了，并放在百度网盘上了，地址：https://pan.baidu.com/s/1p8-ish6oeRTaUs84juQtHg。\n\n\n\n### 2. av_find_codec 分析\n\n这个方法包含了两部分的内容：一部分是寻找解码器，一部分是寻找编码器。其实在av_register_all的函数执行时，就已经把编码器和解码器都存放到一个链表中了。这里寻找编解码器就是从上一步构造的链表中遍历，通过Codec的ID或者name进行条件匹配，最终返回对于的Codec。\n\n\n\n### 3. avcodec_open2 分析\n\n该函数是打开编解码器（Codec）的函数，无论是编码过程还是解码过程，都会用到这个函数。该函数的输入参数有三个：第一个是AVCodecContext，解码过程由FFmpeg引擎填充，编码过程由开发者自己构造，如果想传入私有参数，则为它的priv_data设置参数；第二个参数是上一步通过av_find_codec寻找出来的编解码器（Codec）；第三个参数一般传NULL。\n\n\n\n### 4. avcodec_close 分析\n\n如果理解了avcodec_open，那么对应的close就是一个逆过程，找到对应的实现文件中的close函数指针所只指向的函数，然后该函数会调用对应第三方库的API来关闭掉对应的编码库。\n\n\n\n## 四、总结\n\n本文主要是讲述了FFmpeg的相关术语，并讲解了一下通用的API的分析，不难看出其实FFmpeg所做的事情就是透明化所有的编解码库，用自己的封装来为开发者提供统一的接口。开发者使用不同的编码库时，只需要指明要用哪一个即可，这也充分体现了面向对象编程中的封装特性。\n"
        },
        {
          "name": "FFmpeg 开发之 AVFilter 使用流程总结.md",
          "type": "blob",
          "size": 16.6708984375,
          "content": "在使用FFmpeg开发时，使用AVFilter的流程较为复杂，涉及到的数据结构和函数也比较多，那么使用FFmpeg AVFilter的整体流程是什么样，在其执行过程中都有哪些步骤，需要注意哪些细节？这些都是需要我们整理和总结的。\n\n首先，我们需要引入三个概念结构体：AVFilterGraph 、AVFilterContext、AVFilter。\n\n\n\n## 一、AVFilterGraph 、AVFilterContext、AVFilter\n\n在 FFmpeg 中有多种多样的滤镜，你可以把他们当成一个个小工具，专门用于处理视频和音频数据，以便实现一定的目的。如 overlay 这个滤镜，可以将一个图画覆盖到另一个图画上；transport 这个滤镜可以将图画做旋转等等。\n\n一个 filter 的输出可以作为另一个 filter 的输入，因此多个 filter 可以组织成为一个网状的 filter graph，从而实现更加复杂或者综合的任务。\n\n在 libavfilter 中，我们用类型 AVFilter 来表示一个 filter，每一个 filter 都是经过注册的，其特性是相对固定的。而 AVFilterContext 则表示一个真正的 filter 实例，这和 AVCodec 以及 AVCodecContext 的关系是类似的。\n\nAVFilter 中最重要的特征就是其所需的输入和输出。\n\nAVFilterContext 表示一个 AVFilter 的实例，我们在实际使用 filter 时，就是使用这个结构体。AVFilterContext 在被使用前，它必须是 被初始化的，就是需要对 filter 进行一些选项上的设置，通过初始化告诉 FFmpeg 我们已经做了相关的配置。\n\nAVFilterGraph 表示一个 filter graph，当然它也包含了 filter chain的概念。graph 包含了诸多 filter context 实例，并负责它们之间的 link，graph 会负责创建，保存，释放 这些相关的 filter context 和 link，一般不需要用户进行管理。除此之外，它还有线程特性和最大线程数量的字段，和filter context类似。graph 的操作有：分配一个graph，往graph中添加一个filter context，添加一个 filter graph，对 filter 进行 link 操作，检查内部的link和format是否有效，释放graph等。\n\n\n\n## 二、AVFilter 相关Api使用方法整理\n\n\n\n### 1. AVFilterContext 初始化方法\n\nAVFilterContext 的初始化方式有三种，avfilter_init_str() 和 avfilter_init_dict()、avfilter_graph_create_filter(). \n\n\n\n```\n/*\n 使用提供的参数初始化 filter。\n 参数args：表示用于初始化 filter 的 options。该字符串必须使用 \":\" 来分割各个键值对， 而键值对的形式为 'key=value'。如果不需要设置选项，args为空。 \n 除了这种方式设置选项之外，还可以利用 AVOptions API 直接对 filter 设置选项。\n 返回值：成功返回0，失败返回一个负的错误值\n*/\nint avfilter_init_str(AVFilterContext *ctx, const char *args);\n```\n\n\n\n\n\n```\n/*\n 使用提供的参数初始化filter。\n 参数 options：以 dict 形式提供的 options。\n 返回值：成功返回0，失败返回一个负的错误值\n 注意：这个函数和 avfilter_init_str 函数的功能是一样的，只不过传递的参数形式不同。 但是当传入的 options 中有不被 filter 所支持的参数时，这两个函数的行为是不同： avfilter_init_str 调用会失败，而这个函数则不会失败，它会将不能应用于指定 filter 的 option 通过参数 options 返回，然后继续执行任务。\n*/\nint avfilter_init_dict(AVFilterContext *ctx, AVDictionary **options);\n```\n\n\n\n\n\n```\n/**\n * 创建一个Filter实例（根据args和opaque的参数），并添加到已存在的AVFilterGraph. \n * 如果创建成功*filt_ctx会指向一个创建好的Filter实例，否则会指向NULL. \n * @return 失败返回负数，否则返回大于等于0的数\n */\nint avfilter_graph_create_filter(AVFilterContext **filt_ctx, const AVFilter *filt, const char *name,  　　　　　　　　　　　　　　　　　　const char *args, void *opaque, AVFilterGraph *graph_ctx);\n```\n\n\n\n\n\n### 2. AVFilterGraph 相关的Api\n\nAVFilterGraph 表示一个 filter graph，当然它也包含了 filter chain的概念。graph 包含了诸多 filter context 实例，并负责它们之间的 link，graph 会负责创建，保存，释放 这些相关的 filter context 和 link，一般不需要用户进行管理。\n\ngraph 的操作有：分配一个graph，往graph中添加一个filter context，添加一个 filter graph，对 filter 进行 link 操作，检查内部的link和format是否有效，释放graph等。\n\n根据上述操作，可以列举的方法分别为：\n\n**分配空的filter graph：**\n\n```\n/*\n  分配一个空的 filter graph.\n  成功返回一个 filter graph，失败返回 NULL\n*/\nAVFilterGraph *avfilter_graph_alloc(void);\n```\n\n **创建一个新的filter实例：**\n\n\n\n```\n/*\n    在 filter graph 中创建一个新的 filter 实例。这个创建的实例尚未初始化。\n    详细描述：在 graph 中创建一个名称为 name 的 filter类型的实例。\n    创建失败，返回NULL。创建成功，返回 filter context实例。创建成功后的实例会加入到graph中，\n    可以通过 AVFilterGraph.filters 或者 avfilter_graph_get_filter() 获取。\n*/\nAVFilterContext *avfilter_graph_alloc_filter(AVFilterGraph *graph, const AVFilter *filter, const char *name);                                          \n```\n\n\n\n**返回名字为name的filter context：**\n\n```\n/*\n   返回 graph 中的名为 name 的 filter context。\n*/\nAVFilterContext *avfilter_graph_get_filter(AVFilterGraph *graph, const char *name);\n \n```\n\n**在 filter graph 中创建一个新的 filter context 实例，并使用args和opaque初始化这个filter context：**\n\n\n\n```\n/*\n 在 filter graph 中创建一个新的 filter context 实例，并使用 args 和 opaque 初始化这个实例。\n    \n 参数 filt_ctx：返回成功创建的 filter context\n    \n 返回值：成功返回正数，失败返回负的错误值。\n*/\nint avfilter_graph_create_filter(AVFilterContext **filt_ctx, const AVFilter *filt, const char *name, 　　 　　　　　　　　　　　　　　　　　　　 const char *args, void *opaque,  AVFilterGraph *graph_ctx);     \n```\n\n\n\n **配置 AVFilterGraph 的链接和格式：**\n\n```\n/*\n  检查 graph 的有效性，并配置其中所有的连接和格式。\n  有效则返回 >= 0 的数，否则返回一个负值的 AVERROR.\n */\nint avfilter_graph_config(AVFilterGraph *graphctx, void *log_ctx);\n```\n\n**释放AVFilterGraph：**\n\n```\n/*\n   释放graph，摧毁内部的连接，并将其置为NULL。\n*/\nvoid avfilter_graph_free(AVFilterGraph **graph);\n```\n\n**在一个已经存在的link中插入一个FilterContext：**\n\n\n\n```\n/*\n  在一个已经存在的 link 中间插入一个 filter context。\n  参数filt_srcpad_idx和filt_dstpad_idx：指定filt要连接的输入和输出pad的index。\n  成功返回0.\n*/\nint avfilter_insert_filter(AVFilterLink *link, AVFilterContext *filt, 　　　　　　　　　　　　　　　　　　unsigned filt_srcpad_idx, unsigned filt_dstpad_idx);         \n```\n\n\n\n将字符串描述的filter graph 加入到一个已存在的graph中：\n\n\n\n```\n/*\n    将一个字符串描述的 filter graph 加入到一个已经存在的 graph 中。\n    \n    注意：调用者必须提供 inputs 列表和 outputs 列表。它们在调用这个函数之前必须是已知的。\n    \n    注意：inputs 参数用于描述已经存在的 graph 的输入 pad 列表，也就是说，从新的被创建的 graph 来讲，它们是 output。\n        outputs 参数用于已经存在的 graph 的输出 pad 列表，从新的被创建的 graph 来说，它们是 input。\n        \n    成功返回 >= 0，失败返回负的错误值。\n*/\nint avfilter_graph_parse(AVFilterGraph *graph, const char *filters,\n                         AVFilterInOut *inputs, AVFilterInOut *outputs,\n                         void *log_ctx);                   \n```\n\n\n\n\n\n```\n/*\n    和 avfilter_graph_parse 类似。不同的是 inputs 和 outputs 参数，即做输入参数，也做输出参数。\n        在函数返回时，它们将会保存 graph 中所有的处于 open 状态的 pad。返回的 inout 应该使用 avfilter_inout_free() 释放掉。\n    \n    注意：在字符串描述的 graph 中，第一个 filter 的输入如果没有被一个字符串标识，默认其标识为\"in\"，最后一个 filter 的输出如果没有被标识，默认为\"output\"。\n    \n    intpus：作为输入参数是，用于保存已经存在的graph的open inputs，可以为NULL。\n        作为输出参数，用于保存这个parse函数之后，仍然处于open的inputs，当然如果传入为NULL，则并不输出。\n    outputs：同上。\n*/\nint avfilter_graph_parse_ptr(AVFilterGraph *graph, const char *filters,\n                             AVFilterInOut **inputs, AVFilterInOut **outputs, void *log_ctx);\n```\n\n\n\n\n\n```\n/*\n    和 avfilter_graph_parse_ptr 函数类似，不同的是，inputs 和 outputs 函数不作为输入参数，\n    仅作为输出参数，返回字符串描述的新的被解析的graph在这个parse函数后，仍然处于open状态的inputs和outputs。\n    返回的 inout 应该使用 avfilter_inout_free() 释放掉。\n    \n    成功返回0，失败返回负的错误值。\n*/\nint avfilter_graph_parse2(AVFilterGraph *graph, const char *filters,\n                          AVFilterInOut **inputs, AVFilterInOut **outputs);\n```\n\n\n\n**将graph转换为可读取的字符串描述：**\n\n```\n/*\n  将 graph 转化为可读的字符串描述。\n  参数options：未使用，忽略它。\n*/\nchar *avfilter_graph_dump(AVFilterGraph *graph, const char *options);\n```\n\n\n\n## 三、FFmpeg Filter Buffer 和 BufferSink 相关APi的使用方法整理\n\n Buffer 和 BufferSink 作为 graph 的输入点和输出点来和我们交互，我们仅需要和其进行数据交互即可。其API如下：\n\n\n\n```\n//buffersrc flag\nenum {\n    //不去检测 format 的变化\n    AV_BUFFERSRC_FLAG_NO_CHECK_FORMAT = 1,\n \n    //立刻将 frame 推送到 output\n    AV_BUFFERSRC_FLAG_PUSH = 4,\n \n    //对输入的frame新建一个引用，而非接管引用\n    //如果 frame 是引用计数的，那么对它创建一个新的引用；否则拷贝frame中的数据\n    AV_BUFFERSRC_FLAG_KEEP_REF = 8,\n};\n```\n\n\n\n**向 buffer_src 添加一个Frame：**\n\n\n\n```\n/*\n    向 buffer_src 添加一个 frame。\n    \n    默认情况下，如果 frame 是引用计数的，那么这个函数将会接管其引用并重新设置 frame。\n        但这个行为可以由 flags 来控制。如果 frame 不是引用计数的，那么拷贝该 frame。\n    \n    如果函数返回一个 error，那么 frame 并未被使用。frame为NULL时，表示 EOF。\n    成功返回 >= 0，失败返回负的AVERROR。\n*/\nint av_buffersrc_add_frame_flags(AVFilterContext *buffer_src, AVFrame *frame, int flags);\n```\n\n\n\n**添加一个frame到 src filter：**\n\n```\n/*\n    添加一个 frame 到 src filter。\n    这个函数等同于没有 AV_BUFFERSRC_FLAG_KEEP_REF 的 av_buffersrc_add_frame_flags() 函数。\n */\nint av_buffersrc_add_frame(AVFilterContext *ctx, AVFrame *frame)； \n/*\n   添加一个 frame 到 src filter。\n   这个函数等同于设置了 AV_BUFFERSRC_FLAG_KEEP_REF 的av_buffersrc_add_frame_flags() 函数。\n*/\nint av_buffersrc_write_frame(AVFilterContext *ctx, const AVFrame *frame);\n```\n\n**从sink获取已filtered处理的帧，并放到参数frame中：**\n\n\n\n```\n/*\n    从 sink 中获取已进行 filtered 处理的帧，并将其放到参数 frame 中。\n    \n    参数ctx：指向 buffersink 或 abuffersink 类型的 filter context\n    参数frame：获取到的被处理后的frame，使用后必须使用av_frame_unref() / av_frame_free()释放掉它\n    \n    成功返回非负数，失败返回负的错误值，如 EAGAIN（表示需要新的输入数据来产生filter后的数据）,\n        AVERROR_EOF（表示不会再有新的输入数据）\n */\nint av_buffersink_get_frame_flags(AVFilterContext *ctx, AVFrame *frame, int flags);\n```\n\n\n\n```\n/*\n     同 av_buffersink_get_frame_flags ，不过不能指定 flag。\n */\nint av_buffersink_get_frame(AVFilterContext *ctx, AVFrame *frame)\n/*\n 和 av_buffersink_get_frame 相同，不过这个函数是针对音频的，而且可以指定读取的取样数。此时 ctx 只能指向 abuffersink 类型的 filter context。\n*/\nint av_buffersink_get_samples(AVFilterContext *ctx, AVFrame *frame, int nb_samples);\n```\n\n \n\n\n\n## 四、FFmpeg AVFilter 使用整体流程 \n\n下图就是FFmpeg AVFilter在使用过程中的流程图：\n\n![img](https://img2020.cnblogs.com/blog/682616/202104/682616-20210415141146493-1433896880.jpg)\n\n我们对上图先做下说明，理解下图中每个步骤的关系，然后，才从代码的角度来给出其使用的步骤。\n\n1. 最顶端的AVFilterGraph，这个结构前面介绍过，主要管理加入的过滤器，其中加入的过滤器就是通过函数avfilter_graph_create_filter来创建并加入，这个函数返回是AVFilterContext（其封装了AVFilter的详细参数信息）。\n\n2. buffer和buffersink这两个过滤器是FFMpeg为我们实现好的，buffer表示源，用来向后面的过滤器提供数据输入（其实就是原始的AVFrame）；buffersink过滤器是最终输出的（经过过滤器链处理后的数据AVFrame），其它的诸如filter 1 等过滤器是由avfilter_graph_parse_ptr函数解析外部传入的过滤器描述字符串自动生成的，内部也是通过avfilter_graph_create_filter来创建过滤器的。\n\n3. 上面的buffer、filter 1、filter 2、filter n、buffersink之间是通过avfilter_link函数来进行关联的（通过AVFilterLink结构），这样子过滤器和过滤器之间就通过AVFilterLink进行关联上了，前一个过滤器的输出就是下一个过滤器的输入，注意，除了源和接收过滤器之外，其它的过滤器至少有一个输入和输出，这很好理解，中间的过滤器处理完AVFrame后，得到新的处理后的AVFrame数据，然后把新的AVFrame数据作为下一个过滤器的输入。\n\n4. 过滤器建立完成后，首先我们通过av_buffersrc_add_frame把最原始的AVFrame（没有经过任何过滤器处理的）加入到buffer过滤器的fifo队列。\n\n5. 然后调用buffersink过滤器的av_buffersink_get_frame_flags来获取处理完后的数据帧（这个最终放入buffersink过滤器的AVFrame是通过之前创建的一系列过滤器处理后的数据）。\n\n 使用流程图就介绍到这里，下面结合上面的使用流程图详细说下FFMpeg中使用过滤器的步骤，这个过程我们分为三个部分：过滤器构建、数据加工、资源释放。\n\n\n\n### 1. 过滤器构建：\n\n1）分配AVFilterGraph\n\n```\nAVFilterGraph* graph = avfilter_graph_alloc();\n```\n\n 2）创建过滤器源\n\n```\nchar srcArgs[256] = {0};\nAVFilterContext *srcFilterCtx;\nAVFilter* srcFilter = avfilter_get_by_name(\"buffer\");\navfilter_graph_create_filter(&srcFilterCtx, srcFilter ,\"out_buffer\", srcArgs, NULL, graph);\n```\n\n3）创建接收过滤器\n\n```\nAVFilterContext *sinkFilterCtx;\nAVFilter* sinkFilter = avfilter_get_by_name(\"buffersink\");\navfilter_graph_create_filter(&sinkFilterCtx, sinkFilter,\"in_buffersink\", NULL, NULL, graph);\n```\n\n4）生成源和接收过滤器的输入输出\n\n这里主要是把源和接收过滤器封装给AVFilterInOut结构，使用这个中间结构来把过滤器字符串解析并链接进graph，主要代码如下：\n\n\n\n```\nAVFilterInOut *inputs = avfilter_inout_alloc();\nAVFilterInOut *outputs = avfilter_inout_alloc();\noutputs->name       = av_strdup(\"in\");\noutputs->filter_ctx = srcFilterCtx;\noutputs->pad_idx    = 0;\noutputs->next       = NULL;\ninputs->name        = av_strdup(\"out\");\ninputs->filter_ctx  = sinkFilterCtx;\ninputs->pad_idx     = 0;\ninputs->next        = NULL;\n```\n\n\n\n 这里源对应的AVFilterInOut的name最好定义为in，接收对应的name为out，因为FFMpeg源码里默认会通过这样个name来对默认的输出和输入进行查找。\n\n5）通过解析过滤器字符串添加过滤器\n\n```\nconst *char filtergraph = \"[in1]过滤器名称=参数1:参数2[out1]\";\nint ret = avfilter_graph_parse_ptr(graph, filtergraph, &inputs, &outputs, NULL);\n```\n\n这里过滤器是以字符串形式描述的，其格式为：[in]过滤器名称=参数[out]，过滤器之间用,或;分割，如果过滤器有多个参数，则参数之间用:分割，其中[in]和[out]分别为过滤器的输入和输出，可以有多个。\n\n6）检查过滤器的完整性\n\n```\navfilter_graph_config(graph, NULL);\n```\n\n\n\n###  2. 数据加工\n\n1）向源过滤器加入AVFrame \n\n```\nAVFrame* frame; // 这是解码后获取的数据帧\nint ret = av_buffersrc_add_frame(srcFilterCtx, frame);\n```\n\n2）从buffersink接收处理后的AVFrame\n\n```\nint ret = av_buffersink_get_frame_flags(sinkFilterCtx, frame, 0);\n```\n\n 现在我们就可以使用处理后的AVFrame，比如显示或播放出来。\n\n\n\n### 3.资源释放\n\n使用结束后，调用avfilter_graph_free(&graph);释放掉AVFilterGraph类型的graph。\n"
        },
        {
          "name": "FFmpeg 结构体学习(一)： AVFormatContext 分析.md",
          "type": "blob",
          "size": 3.078125,
          "content": "在 FFmpeg 学习(六)：FFmpeg 核心模块 libavformat 与 libavcodec 分析 中，我们分析了FFmpeg中最重要的两个模块以及重要的结构体之间的关系。\n\n后面的文章，我们先不去继续了解其他模块，先针对在之前的学习中接触到的结构体进行分析，然后在根据功能源码，继续了解FFmpeg。\n\n**AVFormatContext是包含码流参数较多的结构体。本文将会详细分析一下该结构体里每个变量的含义和作用。**\n\n# 一、源码整理\n\n首先我们先看一下结构体AVFormatContext的定义的结构体源码(位于libavformat/avformat.h，本人已经将相关注释翻译成中文，方便大家理解)：\n\nView Code\n\n# 二、AVForamtContext 重点字段\n\n在使用FFMPEG进行开发的时候，AVFormatContext是一个贯穿始终的数据结构，很多函数都要用到它作为参数。它是FFMPEG解封装（flv，mp4，rmvb，avi）功能的结构体。下面看几个主要变量的作用（在这里考虑解码的情况）：\n\n\n\n```\nstruct AVInputFormat *iformat：输入数据的封装格式\nAVIOContext *pb：输入数据的缓存\nunsigned int nb_streams：视音频流的个数\nAVStream **streams：视音频流\nchar filename[1024]：文件名\nint64_t duration：时长（单位：微秒us，转换为秒需要除以1000000）\nint bit_rate：比特率（单位bps，转换为kbps需要除以1000）\nAVDictionary *metadata：元数据\n```\n\n\n\n视频的时长可以转换成HH:MM:SS的形式，示例代码如下：\n\n\n\n```\nAVFormatContext *pFormatCtx;\nCString timelong;\n...\n//duration是以微秒为单位\n//转换成hh:mm:ss形式\nint tns, thh, tmm, tss;\ntns  = (pFormatCtx->duration)/1000000;\nthh  = tns / 3600;\ntmm  = (tns % 3600) / 60;\ntss  = (tns % 60);\ntimelong.Format(\"%02d:%02d:%02d\",thh,tmm,tss);\n```\n\n\n\n视频的原数据（metadata）信息可以通过AVDictionary获取。元数据存储在AVDictionaryEntry结构体中，如下所示：\n\n```\ntypedef struct AVDictionaryEntry {\n    char *key;\n    char *value;\n} AVDictionaryEntry;\n```\n\n每一条元数据分为key和value两个属性。\n\n在ffmpeg中通过av_dict_get()函数获得视频的原数据。\n\n下列代码显示了获取元数据并存入meta字符串变量的过程，注意每一条key和value之间有一个\"\\t:\"，value之后有一个\"\\r\\n\"\n\n\n\n```\n//MetaData------------------------------------------------------------\n//从AVDictionary获得\n//需要用到AVDictionaryEntry对象\n//CString author,copyright,description;\nCString meta=NULL,key,value;\nAVDictionaryEntry *m = NULL;\n//不用一个一个找出来\n/*    m=av_dict_get(pFormatCtx->metadata,\"author\",m,0);\nauthor.Format(\"作者：%s\",m->value);\nm=av_dict_get(pFormatCtx->metadata,\"copyright\",m,0);\ncopyright.Format(\"版权：%s\",m->value);\nm=av_dict_get(pFormatCtx->metadata,\"description\",m,0);\ndescription.Format(\"描述：%s\",m->value);\n*/\n//使用循环读出\n//(需要读取的数据，字段名称，前一条字段（循环时使用），参数)\nwhile(m=av_dict_get(pFormatCtx->metadata,\"\",m,AV_DICT_IGNORE_SUFFIX)){\n    key.Format(m->key);\n    value.Format(m->value);\n    meta+=key+\"\\t:\"+value+\"\\r\\n\" ;\n}\n```\n\n"
        },
        {
          "name": "FFmpeg 结构体学习(七)： AVIOContext 分析.md",
          "type": "blob",
          "size": 8.8134765625,
          "content": "AVIOContext是FFMPEG管理输入输出数据的结构体。下面我们来分析一下该结构体里重要变量的含义和作用。\n\n# 一、源码整理\n\n首先我们先看一下结构体AVIOContext的定义的结构体源码(位于libavformat/avio.h)：\n\n\n\n```\n /**\n * Bytestream IO Context.\n * New fields can be added to the end with minor version bumps.\n * Removal, reordering and changes to existing fields require a major\n * version bump.\n * sizeof(AVIOContext) must not be used outside libav*.\n *\n * @note None of the function pointers in AVIOContext should be called\n *       directly, they should only be set by the client application\n *       when implementing custom I/O. Normally these are set to the\n *       function pointers specified in avio_alloc_context()\n */\ntypedef struct {\n    /**\n     * A class for private options.\n     *\n     * If this AVIOContext is created by avio_open2(), av_class is set and\n     * passes the options down to protocols.\n     *\n     * If this AVIOContext is manually allocated, then av_class may be set by\n     * the caller.\n     *\n     * warning -- this field can be NULL, be sure to not pass this AVIOContext\n     * to any av_opt_* functions in that case.\n     */\n    AVClass *av_class;\n    unsigned char *buffer;  /**< Start of the buffer. */\n    int buffer_size;        /**< Maximum buffer size */\n    unsigned char *buf_ptr; /**< Current position in the buffer */\n    unsigned char *buf_end; /**< End of the data, may be less than\n                                 buffer+buffer_size if the read function returned\n                                 less data than requested, e.g. for streams where\n                                 no more data has been received yet. */\n    void *opaque;           /**< A private pointer, passed to the read/write/seek/...\n                                 functions. */\n    int (*read_packet)(void *opaque, uint8_t *buf, int buf_size);\n    int (*write_packet)(void *opaque, uint8_t *buf, int buf_size);\n    int64_t (*seek)(void *opaque, int64_t offset, int whence);\n    int64_t pos;            /**< position in the file of the current buffer */\n    int must_flush;         /**< true if the next seek should flush */\n    int eof_reached;        /**< true if eof reached */\n    int write_flag;         /**< true if open for writing */\n    int max_packet_size;\n    unsigned long checksum;\n    unsigned char *checksum_ptr;\n    unsigned long (*update_checksum)(unsigned long checksum, const uint8_t *buf, unsigned int size);\n    int error;              /**< contains the error code or 0 if no error happened */\n    /**\n     * Pause or resume playback for network streaming protocols - e.g. MMS.\n     */\n    int (*read_pause)(void *opaque, int pause);\n    /**\n     * Seek to a given timestamp in stream with the specified stream_index.\n     * Needed for some network streaming protocols which don't support seeking\n     * to byte position.\n     */\n    int64_t (*read_seek)(void *opaque, int stream_index,\n                         int64_t timestamp, int flags);\n    /**\n     * A combination of AVIO_SEEKABLE_ flags or 0 when the stream is not seekable.\n     */\n    int seekable;\n \n    /**\n     * max filesize, used to limit allocations\n     * This field is internal to libavformat and access from outside is not allowed.\n     */\n     int64_t maxsize;\n} AVIOContext;\n```\n\n\n\n# 二、AVIOContext 重点字段\n\nAVIOContext中有以下几个变量比较重要：\n\n\n\n```\nunsigned char *buffer：缓存开始位置\n\nint buffer_size：缓存大小（默认32768）\n\nunsigned char *buf_ptr：当前指针读取到的位置\n\nunsigned char *buf_end：缓存结束的位置\n\nvoid *opaque：URLContext结构体\n```\n\n\n\n在解码的情况下，buffer用于存储ffmpeg读入的数据。例如打开一个视频文件的时候，先把数据从硬盘读入buffer，然后在送给解码器用于解码。\n\n其中opaque指向了URLContext。注意，这个结构体并不在FFMPEG提供的头文件中，而是在FFMPEG的源代码中。从FFMPEG源代码中翻出的定义如下所示：\n\n\n\n```\ntypedef struct URLContext {\n    const AVClass *av_class; ///< information for av_log(). Set by url_open().\n    struct URLProtocol *prot;\n    int flags;\n    int is_streamed;  /**< true if streamed (no seek possible), default = false */\n    int max_packet_size;  /**< if non zero, the stream is packetized with this max packet size */\n    void *priv_data;\n    char *filename; /**< specified URL */\n    int is_connected;\n    AVIOInterruptCB interrupt_callback;\n} URLContext;\n```\n\n\n\nURLContext结构体中还有一个结构体URLProtocol。注：每种协议（rtp，rtmp，file等）对应一个URLProtocol。这个结构体也不在FFMPEG提供的头文件中。从FFMPEG源代码中翻出其的定义：\n\n\n\n```\ntypedef struct URLProtocol {\n    const char *name;\n    int (*url_open)(URLContext *h, const char *url, int flags);\n    int (*url_read)(URLContext *h, unsigned char *buf, int size);\n    int (*url_write)(URLContext *h, const unsigned char *buf, int size);\n    int64_t (*url_seek)(URLContext *h, int64_t pos, int whence);\n    int (*url_close)(URLContext *h);\n    struct URLProtocol *next;\n    int (*url_read_pause)(URLContext *h, int pause);\n    int64_t (*url_read_seek)(URLContext *h, int stream_index,\n        int64_t timestamp, int flags);\n    int (*url_get_file_handle)(URLContext *h);\n    int priv_data_size;\n    const AVClass *priv_data_class;\n    int flags;\n    int (*url_check)(URLContext *h, int mask);\n} URLProtocol;\n```\n\n\n\n在这个结构体中，除了一些回调函数接口之外，有一个变量const char *name，该变量存储了协议的名称。每一种输入协议都对应这样一个结构体。\n\n比如说，文件协议中代码如下（file.c）：\n\n\n\n```\nURLProtocol ff_file_protocol = {\n    .name                = \"file\",\n    .url_open            = file_open,\n    .url_read            = file_read,\n    .url_write           = file_write,\n    .url_seek            = file_seek,\n    .url_close           = file_close,\n    .url_get_file_handle = file_get_handle,\n    .url_check           = file_check,\n};\n```\n\n\n\nlibRTMP中代码如下（libRTMP.c）：\n\n\n\n```\nURLProtocol ff_rtmp_protocol = {\n    .name                = \"rtmp\",\n    .url_open            = rtmp_open,\n    .url_read            = rtmp_read,\n    .url_write           = rtmp_write,\n    .url_close           = rtmp_close,\n    .url_read_pause      = rtmp_read_pause,\n    .url_read_seek       = rtmp_read_seek,\n    .url_get_file_handle = rtmp_get_file_handle,\n    .priv_data_size      = sizeof(RTMP),\n    .flags               = URL_PROTOCOL_FLAG_NETWORK,\n};\n```\n\n\n\nudp协议代码如下（udp.c）：\n\n\n\n```\nURLProtocol ff_udp_protocol = {\n    .name                = \"udp\",\n    .url_open            = udp_open,\n    .url_read            = udp_read,\n    .url_write           = udp_write,\n    .url_close           = udp_close,\n    .url_get_file_handle = udp_get_file_handle,\n    .priv_data_size      = sizeof(UDPContext),\n    .flags               = URL_PROTOCOL_FLAG_NETWORK,\n};\n```\n\n\n\n等号右边的函数是完成具体读写功能的函数。可以看一下file协议的几个函数（其实就是读文件，写文件这样的操作）（file.c）：\n\n\n\n```\n/* standard file protocol */\n \nstatic int file_read(URLContext *h, unsigned char *buf, int size)\n{\n    int fd = (intptr_t) h->priv_data;\n    int r = read(fd, buf, size);\n    return (-1 == r)?AVERROR(errno):r;\n}\n \nstatic int file_write(URLContext *h, const unsigned char *buf, int size)\n{\n    int fd = (intptr_t) h->priv_data;\n    int r = write(fd, buf, size);\n    return (-1 == r)?AVERROR(errno):r;\n}\n \nstatic int file_get_handle(URLContext *h)\n{\n    return (intptr_t) h->priv_data;\n}\n \nstatic int file_check(URLContext *h, int mask)\n{\n    struct stat st;\n    int ret = stat(h->filename, &st);\n    if (ret < 0)\n        return AVERROR(errno);\n \n    ret |= st.st_mode&S_IRUSR ? mask&AVIO_FLAG_READ  : 0;\n    ret |= st.st_mode&S_IWUSR ? mask&AVIO_FLAG_WRITE : 0;\n \n    return ret;\n}\n \n#if CONFIG_FILE_PROTOCOL\n \nstatic int file_open(URLContext *h, const char *filename, int flags)\n{\n    int access;\n    int fd;\n \n    av_strstart(filename, \"file:\", &filename);\n \n    if (flags & AVIO_FLAG_WRITE && flags & AVIO_FLAG_READ) {\n        access = O_CREAT | O_TRUNC | O_RDWR;\n    } else if (flags & AVIO_FLAG_WRITE) {\n        access = O_CREAT | O_TRUNC | O_WRONLY;\n    } else {\n        access = O_RDONLY;\n    }\n#ifdef O_BINARY\n    access |= O_BINARY;\n#endif\n    fd = open(filename, access, 0666);\n    if (fd == -1)\n        return AVERROR(errno);\n    h->priv_data = (void *) (intptr_t) fd;\n    return 0;\n}\n \n/* XXX: use llseek */\nstatic int64_t file_seek(URLContext *h, int64_t pos, int whence)\n{\n    int fd = (intptr_t) h->priv_data;\n    if (whence == AVSEEK_SIZE) {\n        struct stat st;\n        int ret = fstat(fd, &st);\n        return ret < 0 ? AVERROR(errno) : st.st_size;\n    }\n    return lseek(fd, pos, whence);\n}\n \nstatic int file_close(URLContext *h)\n{\n    int fd = (intptr_t) h->priv_data;\n    return close(fd);\n}\n```\n\n"
        },
        {
          "name": "FFmpeg 结构体学习(三)： AVPacket 分析.md",
          "type": "blob",
          "size": 3.138671875,
          "content": "AVPacket是存储压缩编码数据相关信息的结构体。下面我们来分析一下该结构体里重要变量的含义和作用。\n\n# 一、源码整理\n\n首先我们先看一下结构体AVPacket的定义的结构体源码(位于libavcodec/avcodec.h)：\n\n\n\n```\ntypedef struct AVPacket {\n    /**\n     * Presentation timestamp in AVStream->time_base units; the time at which\n     * the decompressed packet will be presented to the user.\n     * Can be AV_NOPTS_VALUE if it is not stored in the file.\n     * pts MUST be larger or equal to dts as presentation cannot happen before\n     * decompression, unless one wants to view hex dumps. Some formats misuse\n     * the terms dts and pts/cts to mean something different. Such timestamps\n     * must be converted to true pts/dts before they are stored in AVPacket.\n     */\n    int64_t pts;\n    /**\n     * Decompression timestamp in AVStream->time_base units; the time at which\n     * the packet is decompressed.\n     * Can be AV_NOPTS_VALUE if it is not stored in the file.\n     */\n    int64_t dts;\n    uint8_t *data;\n    int   size;\n    int   stream_index;\n    /**\n     * A combination of AV_PKT_FLAG values\n     */\n    int   flags;\n    /**\n     * Additional packet data that can be provided by the container.\n     * Packet can contain several types of side information.\n     */\n    struct {\n        uint8_t *data;\n        int      size;\n        enum AVPacketSideDataType type;\n    } *side_data;\n    int side_data_elems;\n \n    /**\n     * Duration of this packet in AVStream->time_base units, 0 if unknown.\n     * Equals next_pts - this_pts in presentation order.\n     */\n    int   duration;\n    void  (*destruct)(struct AVPacket *);\n    void  *priv;\n    int64_t pos;                            ///< byte position in stream, -1 if unknown\n \n    /**\n     * Time difference in AVStream->time_base units from the pts of this\n     * packet to the point at which the output from the decoder has converged\n     * independent from the availability of previous frames. That is, the\n     * frames are virtually identical no matter if decoding started from\n     * the very first frame or from this keyframe.\n     * Is AV_NOPTS_VALUE if unknown.\n     * This field is not the display duration of the current packet.\n     * This field has no meaning if the packet does not have AV_PKT_FLAG_KEY\n     * set.\n     *\n     * The purpose of this field is to allow seeking in streams that have no\n     * keyframes in the conventional sense. It corresponds to the\n     * recovery point SEI in H.264 and match_time_delta in NUT. It is also\n     * essential for some types of subtitle streams to ensure that all\n     * subtitles are correctly displayed after seeking.\n     */\n    int64_t convergence_duration;\n} AVPacket;\n```\n\n\n\n# 二、AVPacket 重点字段\n\n\n\n```\nuint8_t *data：压缩编码的数据。\n\nint   size：data的大小\n\nint64_t pts：显示时间戳\n\nint64_t dts：解码时间戳\n\nint   stream_index：标识该AVPacket所属的视频/音频流。\n```\n\n\n\n针对data做一下说明：对于H.264格式来说，在使用FFMPEG进行视音频处理的时候，我们常常可以将得到的AVPacket的data数据直接写成文件，从而得到视音频的码流文件。\n"
        },
        {
          "name": "FFmpeg 结构体学习(二)： AVStream 分析.md",
          "type": "blob",
          "size": 6.1259765625,
          "content": "AVStream是存储每一个视频/音频流信息的结构体。下面我们来分析一下该结构体里重要变量的含义和作用。\n\n# 一、源码整理\n\n首先我们先看一下结构体AVStream的定义的结构体源码(位于libavformat/avformat.h)：\n\n\n\n```\n/**\n * Stream structure.\n * New fields can be added to the end with minor version bumps.\n * Removal, reordering and changes to existing fields require a major\n * version bump.\n * sizeof(AVStream) must not be used outside libav*.\n */\ntypedef struct AVStream {\n    int index;    /**< stream index in AVFormatContext */\n    /**\n     * Format-specific stream ID.\n     * decoding: set by libavformat\n     * encoding: set by the user\n     */\n    int id;\n    AVCodecContext *codec; /**< codec context */\n    /**\n     * Real base framerate of the stream.\n     * This is the lowest framerate with which all timestamps can be\n     * represented accurately (it is the least common multiple of all\n     * framerates in the stream). Note, this value is just a guess!\n     * For example, if the time base is 1/90000 and all frames have either\n     * approximately 3600 or 1800 timer ticks, then r_frame_rate will be 50/1.\n     */\n    AVRational r_frame_rate;\n    void *priv_data;\n \n    /**\n     * encoding: pts generation when outputting stream\n     */\n    struct AVFrac pts;\n \n    /**\n     * This is the fundamental unit of time (in seconds) in terms\n     * of which frame timestamps are represented. For fixed-fps content,\n     * time base should be 1/framerate and timestamp increments should be 1.\n     * decoding: set by libavformat\n     * encoding: set by libavformat in av_write_header\n     */\n    AVRational time_base;\n \n    /**\n     * Decoding: pts of the first frame of the stream in presentation order, in stream time base.\n     * Only set this if you are absolutely 100% sure that the value you set\n     * it to really is the pts of the first frame.\n     * This may be undefined (AV_NOPTS_VALUE).\n     * @note The ASF header does NOT contain a correct start_time the ASF\n     * demuxer must NOT set this.\n     */\n    int64_t start_time;\n \n    /**\n     * Decoding: duration of the stream, in stream time base.\n     * If a source file does not specify a duration, but does specify\n     * a bitrate, this value will be estimated from bitrate and file size.\n     */\n    int64_t duration;\n \n    int64_t nb_frames;                 ///< number of frames in this stream if known or 0\n \n    int disposition; /**< AV_DISPOSITION_* bit field */\n \n    enum AVDiscard discard; ///< Selects which packets can be discarded at will and do not need to be demuxed.\n \n    /**\n     * sample aspect ratio (0 if unknown)\n     * - encoding: Set by user.\n     * - decoding: Set by libavformat.\n     */\n    AVRational sample_aspect_ratio;\n \n    AVDictionary *metadata;\n \n    /**\n     * Average framerate\n     */\n    AVRational avg_frame_rate;\n \n    /**\n     * For streams with AV_DISPOSITION_ATTACHED_PIC disposition, this packet\n     * will contain the attached picture.\n     *\n     * decoding: set by libavformat, must not be modified by the caller.\n     * encoding: unused\n     */\n    AVPacket attached_pic;\n \n    /*****************************************************************\n     * All fields below this line are not part of the public API. They\n     * may not be used outside of libavformat and can be changed and\n     * removed at will.\n     * New public fields should be added right above.\n     *****************************************************************\n     */\n \n    /**\n     * Stream information used internally by av_find_stream_info()\n     */\n#define MAX_STD_TIMEBASES (60*12+5)\n    struct {\n        int64_t last_dts;\n        int64_t duration_gcd;\n        int duration_count;\n        double duration_error[2][2][MAX_STD_TIMEBASES];\n        int64_t codec_info_duration;\n        int nb_decoded_frames;\n        int found_decoder;\n    } *info;\n \n    int pts_wrap_bits; /**< number of bits in pts (used for wrapping control) */\n \n    // Timestamp generation support:\n    /**\n     * Timestamp corresponding to the last dts sync point.\n     *\n     * Initialized when AVCodecParserContext.dts_sync_point >= 0 and\n     * a DTS is received from the underlying container. Otherwise set to\n     * AV_NOPTS_VALUE by default.\n     */\n    int64_t reference_dts;\n    int64_t first_dts;\n    int64_t cur_dts;\n    int64_t last_IP_pts;\n    int last_IP_duration;\n \n    /**\n     * Number of packets to buffer for codec probing\n     */\n#define MAX_PROBE_PACKETS 2500\n    int probe_packets;\n \n    /**\n     * Number of frames that have been demuxed during av_find_stream_info()\n     */\n    int codec_info_nb_frames;\n \n    /**\n     * Stream Identifier\n     * This is the MPEG-TS stream identifier +1\n     * 0 means unknown\n     */\n    int stream_identifier;\n \n    int64_t interleaver_chunk_size;\n    int64_t interleaver_chunk_duration;\n \n    /* av_read_frame() support */\n    enum AVStreamParseType need_parsing;\n    struct AVCodecParserContext *parser;\n \n    /**\n     * last packet in packet_buffer for this stream when muxing.\n     */\n    struct AVPacketList *last_in_packet_buffer;\n    AVProbeData probe_data;\n#define MAX_REORDER_DELAY 16\n    int64_t pts_buffer[MAX_REORDER_DELAY+1];\n \n    AVIndexEntry *index_entries; /**< Only used if the format does not\n                                    support seeking natively. */\n    int nb_index_entries;\n    unsigned int index_entries_allocated_size;\n \n    /**\n     * flag to indicate that probing is requested\n     * NOT PART OF PUBLIC API\n     */\n    int request_probe;\n} AVStream;\n```\n\n\n\n# 二、AVStream 重点字段\n\n```\nint index：标识该视频/音频流\n\nAVCodecContext *codec：指向该视频/音频流的AVCodecContext（它们是一一对应的关系）\n\nAVRational time_base：时基。通过该值可以把PTS，DTS转化为真正的时间。FFMPEG其他结构体中也有这个字段，但是根据我的经验，只有AVStream中的time_base是可用的。PTS*time_base=真正的时间\n\nint64_t duration：该视频/音频流长度\n\nAVDictionary *metadata：元数据信息\n\nAVRational avg_frame_rate：帧率（注：对视频来说，这个挺重要的）\n\nAVPacket attached_pic：附带的图片。比如说一些MP3，AAC音频文件附带的专辑封面。\n```\n\n"
        },
        {
          "name": "FFmpeg 结构体学习(五)： AVCodec 分析.md",
          "type": "blob",
          "size": 10.66796875,
          "content": "AVCodec是存储编解码器信息的结构体。下面我们来分析一下该结构体里重要变量的含义和作用。\n\n# 一、源码整理\n\n首先我们先看一下结构体AVCodec的定义的结构体源码(位于libavcodec/avcodec.h)：\n\n\n\n```\n/* 雷霄骅 \n * 中国传媒大学/数字电视技术 \n * leixiaohua1020@126.com \n * \n */\n /**\n * AVCodec.\n */\ntypedef struct AVCodec {\n    /**\n     * Name of the codec implementation.\n     * The name is globally unique among encoders and among decoders (but an\n     * encoder and a decoder can share the same name).\n     * This is the primary way to find a codec from the user perspective.\n     */\n    const char *name;\n    /**\n     * Descriptive name for the codec, meant to be more human readable than name.\n     * You should use the NULL_IF_CONFIG_SMALL() macro to define it.\n     */\n    const char *long_name;\n    enum AVMediaType type;\n    enum CodecID id;\n    /**\n     * Codec capabilities.\n     * see CODEC_CAP_*\n     */\n    int capabilities;\n    const AVRational *supported_framerates; ///< array of supported framerates, or NULL if any, array is terminated by {0,0}\n    const enum PixelFormat *pix_fmts;       ///< array of supported pixel formats, or NULL if unknown, array is terminated by -1\n    const int *supported_samplerates;       ///< array of supported audio samplerates, or NULL if unknown, array is terminated by 0\n    const enum AVSampleFormat *sample_fmts; ///< array of supported sample formats, or NULL if unknown, array is terminated by -1\n    const uint64_t *channel_layouts;         ///< array of support channel layouts, or NULL if unknown. array is terminated by 0\n    uint8_t max_lowres;                     ///< maximum value for lowres supported by the decoder\n    const AVClass *priv_class;              ///< AVClass for the private context\n    const AVProfile *profiles;              ///< array of recognized profiles, or NULL if unknown, array is terminated by {FF_PROFILE_UNKNOWN}\n \n    /*****************************************************************\n     * No fields below this line are part of the public API. They\n     * may not be used outside of libavcodec and can be changed and\n     * removed at will.\n     * New public fields should be added right above.\n     *****************************************************************\n     */\n    int priv_data_size;\n    struct AVCodec *next;\n    /**\n     * @name Frame-level threading support functions\n     * @{\n     */\n    /**\n     * If defined, called on thread contexts when they are created.\n     * If the codec allocates writable tables in init(), re-allocate them here.\n     * priv_data will be set to a copy of the original.\n     */\n    int (*init_thread_copy)(AVCodecContext *);\n    /**\n     * Copy necessary context variables from a previous thread context to the current one.\n     * If not defined, the next thread will start automatically; otherwise, the codec\n     * must call ff_thread_finish_setup().\n     *\n     * dst and src will (rarely) point to the same context, in which case memcpy should be skipped.\n     */\n    int (*update_thread_context)(AVCodecContext *dst, const AVCodecContext *src);\n    /** @} */\n \n    /**\n     * Private codec-specific defaults.\n     */\n    const AVCodecDefault *defaults;\n \n    /**\n     * Initialize codec static data, called from avcodec_register().\n     */\n    void (*init_static_data)(struct AVCodec *codec);\n \n    int (*init)(AVCodecContext *);\n    int (*encode)(AVCodecContext *, uint8_t *buf, int buf_size, void *data);\n    /**\n     * Encode data to an AVPacket.\n     *\n     * @param      avctx          codec context\n     * @param      avpkt          output AVPacket (may contain a user-provided buffer)\n     * @param[in]  frame          AVFrame containing the raw data to be encoded\n     * @param[out] got_packet_ptr encoder sets to 0 or 1 to indicate that a\n     *                            non-empty packet was returned in avpkt.\n     * @return 0 on success, negative error code on failure\n     */\n    int (*encode2)(AVCodecContext *avctx, AVPacket *avpkt, const AVFrame *frame,\n                   int *got_packet_ptr);\n    int (*decode)(AVCodecContext *, void *outdata, int *outdata_size, AVPacket *avpkt);\n    int (*close)(AVCodecContext *);\n    /**\n     * Flush buffers.\n     * Will be called when seeking\n     */\n    void (*flush)(AVCodecContext *);\n} AVCodec;\n```\n\n\n\n# 二、AVCodec 重点字段\n\n下面说一下最主要的几个变量：\n\n\n\n```\nconst char *name：编解码器的名字，比较短\n\nconst char *long_name：编解码器的名字，全称，比较长\n\nenum AVMediaType type：指明了类型，是视频，音频，还是字幕\n\nenum AVCodecID id：ID，不重复\n\nconst AVRational *supported_framerates：支持的帧率（仅视频）\n\nconst enum AVPixelFormat *pix_fmts：支持的像素格式（仅视频）\n\nconst int *supported_samplerates：支持的采样率（仅音频）\n\nconst enum AVSampleFormat *sample_fmts：支持的采样格式（仅音频）\n\nconst uint64_t *channel_layouts：支持的声道数（仅音频）\n\nint priv_data_size：私有数据的大小\n```\n\n\n\n详细介绍几个变量：\n\n### 1.enum AVMediaType type\n\nAVMediaType定义如下：\n\n\n\n```\nenum AVMediaType {\n    AVMEDIA_TYPE_UNKNOWN = -1,  ///< Usually treated as AVMEDIA_TYPE_DATA\n    AVMEDIA_TYPE_VIDEO,\n    AVMEDIA_TYPE_AUDIO,\n    AVMEDIA_TYPE_DATA,          ///< Opaque data information usually continuous\n    AVMEDIA_TYPE_SUBTITLE,\n    AVMEDIA_TYPE_ATTACHMENT,    ///< Opaque data information usually sparse\n    AVMEDIA_TYPE_NB\n};\n```\n\n\n\n### 2.enum AVCodecID id\n\nAVCodecID定义如下：\n\n\n\n```\nenum AVCodecID {\n    AV_CODEC_ID_NONE,\n \n    /* video codecs */\n    AV_CODEC_ID_MPEG1VIDEO,\n    AV_CODEC_ID_MPEG2VIDEO, ///< preferred ID for MPEG-1/2 video decoding\n    AV_CODEC_ID_MPEG2VIDEO_XVMC,\n    AV_CODEC_ID_H261,\n    AV_CODEC_ID_H263,\n    AV_CODEC_ID_RV10,\n    AV_CODEC_ID_RV20,\n    AV_CODEC_ID_MJPEG,\n    AV_CODEC_ID_MJPEGB,\n    AV_CODEC_ID_LJPEG,\n    AV_CODEC_ID_SP5X,\n    AV_CODEC_ID_JPEGLS,\n    AV_CODEC_ID_MPEG4,\n    AV_CODEC_ID_RAWVIDEO,\n    AV_CODEC_ID_MSMPEG4V1,\n    AV_CODEC_ID_MSMPEG4V2,\n    AV_CODEC_ID_MSMPEG4V3,\n    AV_CODEC_ID_WMV1,\n    AV_CODEC_ID_WMV2,\n    AV_CODEC_ID_H263P,\n    AV_CODEC_ID_H263I,\n    AV_CODEC_ID_FLV1,\n    AV_CODEC_ID_SVQ1,\n    AV_CODEC_ID_SVQ3,\n    AV_CODEC_ID_DVVIDEO,\n    AV_CODEC_ID_HUFFYUV,\n    AV_CODEC_ID_CYUV,\n    AV_CODEC_ID_H264,\n    ...\n}\n```\n\n\n\n### 3.const enum AVPixelFormat *pix_fmts\n\nAVPixelFormat定义如下：\n\n\n\n```\nenum AVPixelFormat {\n    AV_PIX_FMT_NONE = -1,\n    AV_PIX_FMT_YUV420P,   ///< planar YUV 4:2:0, 12bpp, (1 Cr & Cb sample per 2x2 Y samples)\n    AV_PIX_FMT_YUYV422,   ///< packed YUV 4:2:2, 16bpp, Y0 Cb Y1 Cr\n    AV_PIX_FMT_RGB24,     ///< packed RGB 8:8:8, 24bpp, RGBRGB...\n    AV_PIX_FMT_BGR24,     ///< packed RGB 8:8:8, 24bpp, BGRBGR...\n    AV_PIX_FMT_YUV422P,   ///< planar YUV 4:2:2, 16bpp, (1 Cr & Cb sample per 2x1 Y samples)\n    AV_PIX_FMT_YUV444P,   ///< planar YUV 4:4:4, 24bpp, (1 Cr & Cb sample per 1x1 Y samples)\n    AV_PIX_FMT_YUV410P,   ///< planar YUV 4:1:0,  9bpp, (1 Cr & Cb sample per 4x4 Y samples)\n    AV_PIX_FMT_YUV411P,   ///< planar YUV 4:1:1, 12bpp, (1 Cr & Cb sample per 4x1 Y samples)\n    AV_PIX_FMT_GRAY8,     ///<        Y        ,  8bpp\n    AV_PIX_FMT_MONOWHITE, ///<        Y        ,  1bpp, 0 is white, 1 is black, in each byte pixels are ordered from the msb to the lsb\n    AV_PIX_FMT_MONOBLACK, ///<        Y        ,  1bpp, 0 is black, 1 is white, in each byte pixels are ordered from the msb to the lsb\n    AV_PIX_FMT_PAL8,      ///< 8 bit with PIX_FMT_RGB32 palette\n    AV_PIX_FMT_YUVJ420P,  ///< planar YUV 4:2:0, 12bpp, full scale (JPEG), deprecated in favor of PIX_FMT_YUV420P and setting color_range\n    AV_PIX_FMT_YUVJ422P,  ///< planar YUV 4:2:2, 16bpp, full scale (JPEG), deprecated in favor of PIX_FMT_YUV422P and setting color_range\n    AV_PIX_FMT_YUVJ444P,  ///< planar YUV 4:4:4, 24bpp, full scale (JPEG), deprecated in favor of PIX_FMT_YUV444P and setting color_range\n    AV_PIX_FMT_XVMC_MPEG2_MC,///< XVideo Motion Acceleration via common packet passing\n    AV_PIX_FMT_XVMC_MPEG2_IDCT,\n    ...（代码太长，略）\n}\n```\n\n\n\n### 4.const enum AVSampleFormat *sample_fmts\n\n\n\n```\nenum AVSampleFormat {\n    AV_SAMPLE_FMT_NONE = -1,\n    AV_SAMPLE_FMT_U8,          ///< unsigned 8 bits\n    AV_SAMPLE_FMT_S16,         ///< signed 16 bits\n    AV_SAMPLE_FMT_S32,         ///< signed 32 bits\n    AV_SAMPLE_FMT_FLT,         ///< float\n    AV_SAMPLE_FMT_DBL,         ///< double\n \n    AV_SAMPLE_FMT_U8P,         ///< unsigned 8 bits, planar\n    AV_SAMPLE_FMT_S16P,        ///< signed 16 bits, planar\n    AV_SAMPLE_FMT_S32P,        ///< signed 32 bits, planar\n    AV_SAMPLE_FMT_FLTP,        ///< float, planar\n    AV_SAMPLE_FMT_DBLP,        ///< double, planar\n \n    AV_SAMPLE_FMT_NB           ///< Number of sample formats. DO NOT USE if linking dynamically\n};\n```\n\n\n\n 每一个编解码器对应一个该结构体，查看一下ffmpeg的源代码，我们可以看一下H.264解码器的结构体如下所示（h264.c）：\n\n\n\n```\nAVCodec ff_h264_decoder = {\n    .name           = \"h264\",\n    .type           = AVMEDIA_TYPE_VIDEO,\n    .id             = CODEC_ID_H264,\n    .priv_data_size = sizeof(H264Context),\n    .init           = ff_h264_decode_init,\n    .close          = ff_h264_decode_end,\n    .decode         = decode_frame,\n    .capabilities   = /*CODEC_CAP_DRAW_HORIZ_BAND |*/ CODEC_CAP_DR1 | CODEC_CAP_DELAY |\n                      CODEC_CAP_SLICE_THREADS | CODEC_CAP_FRAME_THREADS,\n    .flush= flush_dpb,\n    .long_name = NULL_IF_CONFIG_SMALL(\"H.264 / AVC / MPEG-4 AVC / MPEG-4 part 10\"),\n    .init_thread_copy      = ONLY_IF_THREADS_ENABLED(decode_init_thread_copy),\n    .update_thread_context = ONLY_IF_THREADS_ENABLED(decode_update_thread_context),\n    .profiles = NULL_IF_CONFIG_SMALL(profiles),\n    .priv_class     = &h264_class,\n};\n```\n\n\n\nJPEG2000解码器结构体（j2kdec.c）：\n\n\n\n```\nAVCodec ff_jpeg2000_decoder = {\n    .name           = \"j2k\",\n    .type           = AVMEDIA_TYPE_VIDEO,\n    .id             = CODEC_ID_JPEG2000,\n    .priv_data_size = sizeof(J2kDecoderContext),\n    .init           = j2kdec_init,\n    .close          = decode_end,\n    .decode         = decode_frame,\n    .capabilities = CODEC_CAP_EXPERIMENTAL,\n    .long_name = NULL_IF_CONFIG_SMALL(\"JPEG 2000\"),\n    .pix_fmts =\n        (const enum PixelFormat[]) {PIX_FMT_GRAY8, PIX_FMT_RGB24, PIX_FMT_NONE}\n};\n```\n\n\n\n下面简单介绍一下遍历ffmpeg中的解码器信息的方法（这些解码器以一个链表的形式存储）： \n\n1.注册所有编解码器：av_register_all();\n\n2.声明一个AVCodec类型的指针，比如说AVCodec* first_c;\n\n3.调用av_codec_next()函数，即可获得指向链表下一个解码器的指针，循环往复可以获得所有解码器的信息。注意，如果想要获得指向第一个解码器的指针，则需要将该函数的参数设置为NULL。\n"
        },
        {
          "name": "FFmpeg 结构体学习(八)：FFMPEG中重要结构体之间的关系.md",
          "type": "blob",
          "size": 1.1455078125,
          "content": "FFMPEG中结构体很多。最关键的结构体可以分成以下几类：\n\n### 解协议（http,rtsp,rtmp,mms）\n\nAVIOContext，URLProtocol，URLContext主要存储视音频使用的协议的类型以及状态。URLProtocol存储输入视音频使用的封装格式。每种协议都对应一个URLProtocol结构。\n\n### 解封装（flv,avi,rmvb,mp4）\n\nAVFormatContext主要存储视音频封装格式中包含的信息；AVInputFormat存储输入视音频使用的封装格式。每种视音频封装格式都对应一个AVInputFormat 结构。\n\n### 解码（h264,mpeg2,aac,mp3）\n\n每个AVStream存储一个视频/音频流的相关数据；每个AVStream对应一个AVCodecContext，存储该视频/音频流使用解码方式的相关数据；每个AVCodecContext中对应一个AVCodec，包含该视频/音频对应的解码器。每种解码器都对应一个AVCodec结构。\n\n### 存数据\n\n视频的话，每个结构一般是存一帧；音频可能有好几帧\n\n解码前数据：AVPacket\n\n解码后数据：AVFrame\n\n他们之间的对应关系如下所示：\n\n![img](https://images2018.cnblogs.com/blog/682616/201808/682616-20180818164850060-1975380874.png)\n\n"
        },
        {
          "name": "FFmpeg 结构体学习(六)： AVCodecContext 分析.md",
          "type": "blob",
          "size": 53.4375,
          "content": "AVCodecContext是包含变量较多的结构体（感觉差不多是变量最多的结构体）。下面我们来分析一下该结构体里重要变量的含义和作用。\n\n# 一、源码整理\n\n首先我们先看一下结构体AVCodecContext的定义的结构体源码(位于libavcodec/avcodec.h)：\n\n\n\n```\n/**\n * main external API structure.\n * New fields can be added to the end with minor version bumps.\n * Removal, reordering and changes to existing fields require a major\n * version bump.\n * Please use AVOptions (av_opt* / av_set/get*()) to access these fields from user\n * applications.\n * sizeof(AVCodecContext) must not be used outside libav*.\n */\ntypedef struct AVCodecContext {\n    /**\n     * information on struct for av_log\n     * - set by avcodec_alloc_context3\n     */\n    const AVClass *av_class;\n    int log_level_offset;\n \n    enum AVMediaType codec_type; /* see AVMEDIA_TYPE_xxx */\n    const struct AVCodec  *codec;\n    char             codec_name[32];\n    enum AVCodecID     codec_id; /* see AV_CODEC_ID_xxx */\n \n    /**\n     * fourcc (LSB first, so \"ABCD\" -> ('D'<<24) + ('C'<<16) + ('B'<<8) + 'A').\n     * This is used to work around some encoder bugs.\n     * A demuxer should set this to what is stored in the field used to identify the codec.\n     * If there are multiple such fields in a container then the demuxer should choose the one\n     * which maximizes the information about the used codec.\n     * If the codec tag field in a container is larger than 32 bits then the demuxer should\n     * remap the longer ID to 32 bits with a table or other structure. Alternatively a new\n     * extra_codec_tag + size could be added but for this a clear advantage must be demonstrated\n     * first.\n     * - encoding: Set by user, if not then the default based on codec_id will be used.\n     * - decoding: Set by user, will be converted to uppercase by libavcodec during init.\n     */\n    unsigned int codec_tag;\n \n    /**\n     * fourcc from the AVI stream header (LSB first, so \"ABCD\" -> ('D'<<24) + ('C'<<16) + ('B'<<8) + 'A').\n     * This is used to work around some encoder bugs.\n     * - encoding: unused\n     * - decoding: Set by user, will be converted to uppercase by libavcodec during init.\n     */\n    unsigned int stream_codec_tag;\n \n#if FF_API_SUB_ID\n    /**\n     * @deprecated this field is unused\n     */\n    attribute_deprecated int sub_id;\n#endif\n \n    void *priv_data;\n \n    /**\n     * Private context used for internal data.\n     *\n     * Unlike priv_data, this is not codec-specific. It is used in general\n     * libavcodec functions.\n     */\n    struct AVCodecInternal *internal;\n \n    /**\n     * Private data of the user, can be used to carry app specific stuff.\n     * - encoding: Set by user.\n     * - decoding: Set by user.\n     */\n    void *opaque;\n \n    /**\n     * the average bitrate\n     * - encoding: Set by user; unused for constant quantizer encoding.\n     * - decoding: Set by libavcodec. 0 or some bitrate if this info is available in the stream.\n     */\n    int bit_rate;\n \n    /**\n     * number of bits the bitstream is allowed to diverge from the reference.\n     *           the reference can be CBR (for CBR pass1) or VBR (for pass2)\n     * - encoding: Set by user; unused for constant quantizer encoding.\n     * - decoding: unused\n     */\n    int bit_rate_tolerance;\n \n    /**\n     * Global quality for codecs which cannot change it per frame.\n     * This should be proportional to MPEG-1/2/4 qscale.\n     * - encoding: Set by user.\n     * - decoding: unused\n     */\n    int global_quality;\n \n    /**\n     * - encoding: Set by user.\n     * - decoding: unused\n     */\n    int compression_level;\n#define FF_COMPRESSION_DEFAULT -1\n \n    /**\n     * CODEC_FLAG_*.\n     * - encoding: Set by user.\n     * - decoding: Set by user.\n     */\n    int flags;\n \n    /**\n     * CODEC_FLAG2_*\n     * - encoding: Set by user.\n     * - decoding: Set by user.\n     */\n    int flags2;\n \n    /**\n     * some codecs need / can use extradata like Huffman tables.\n     * mjpeg: Huffman tables\n     * rv10: additional flags\n     * mpeg4: global headers (they can be in the bitstream or here)\n     * The allocated memory should be FF_INPUT_BUFFER_PADDING_SIZE bytes larger\n     * than extradata_size to avoid prolems if it is read with the bitstream reader.\n     * The bytewise contents of extradata must not depend on the architecture or CPU endianness.\n     * - encoding: Set/allocated/freed by libavcodec.\n     * - decoding: Set/allocated/freed by user.\n     */\n    uint8_t *extradata;\n    int extradata_size;\n \n    /**\n     * This is the fundamental unit of time (in seconds) in terms\n     * of which frame timestamps are represented. For fixed-fps content,\n     * timebase should be 1/framerate and timestamp increments should be\n     * identically 1.\n     * - encoding: MUST be set by user.\n     * - decoding: Set by libavcodec.\n     */\n    AVRational time_base;\n \n    /**\n     * For some codecs, the time base is closer to the field rate than the frame rate.\n     * Most notably, H.264 and MPEG-2 specify time_base as half of frame duration\n     * if no telecine is used ...\n     *\n     * Set to time_base ticks per frame. Default 1, e.g., H.264/MPEG-2 set it to 2.\n     */\n    int ticks_per_frame;\n \n    /**\n     * Encoding: Number of frames delay there will be from the encoder input to\n     *           the decoder output. (we assume the decoder matches the spec)\n     * Decoding: Number of frames delay in addition to what a standard decoder\n     *           as specified in the spec would produce.\n     *\n     * Video:\n     *   Number of frames the decoded output will be delayed relative to the\n     *   encoded input.\n     *\n     * Audio:\n     *   For encoding, this is the number of \"priming\" samples added to the\n     *   beginning of the stream. The decoded output will be delayed by this\n     *   many samples relative to the input to the encoder. Note that this\n     *   field is purely informational and does not directly affect the pts\n     *   output by the encoder, which should always be based on the actual\n     *   presentation time, including any delay.\n     *   For decoding, this is the number of samples the decoder needs to\n     *   output before the decoder's output is valid. When seeking, you should\n     *   start decoding this many samples prior to your desired seek point.\n     *\n     * - encoding: Set by libavcodec.\n     * - decoding: Set by libavcodec.\n     */\n    int delay;\n \n \n    /* video only */\n    /**\n     * picture width / height.\n     * - encoding: MUST be set by user.\n     * - decoding: Set by libavcodec.\n     * Note: For compatibility it is possible to set this instead of\n     * coded_width/height before decoding.\n     */\n    int width, height;\n \n    /**\n     * Bitstream width / height, may be different from width/height if lowres enabled.\n     * - encoding: unused\n     * - decoding: Set by user before init if known. Codec should override / dynamically change if needed.\n     */\n    int coded_width, coded_height;\n \n#define FF_ASPECT_EXTENDED 15\n \n    /**\n     * the number of pictures in a group of pictures, or 0 for intra_only\n     * - encoding: Set by user.\n     * - decoding: unused\n     */\n    int gop_size;\n \n    /**\n     * Pixel format, see AV_PIX_FMT_xxx.\n     * May be set by the demuxer if known from headers.\n     * May be overridden by the decoder if it knows better.\n     * - encoding: Set by user.\n     * - decoding: Set by user if known, overridden by libavcodec if known\n     */\n    enum AVPixelFormat pix_fmt;\n \n    /**\n     * Motion estimation algorithm used for video coding.\n     * 1 (zero), 2 (full), 3 (log), 4 (phods), 5 (epzs), 6 (x1), 7 (hex),\n     * 8 (umh), 9 (iter), 10 (tesa) [7, 8, 10 are x264 specific, 9 is snow specific]\n     * - encoding: MUST be set by user.\n     * - decoding: unused\n     */\n    int me_method;\n \n    /**\n     * If non NULL, 'draw_horiz_band' is called by the libavcodec\n     * decoder to draw a horizontal band. It improves cache usage. Not\n     * all codecs can do that. You must check the codec capabilities\n     * beforehand.\n     * When multithreading is used, it may be called from multiple threads\n     * at the same time; threads might draw different parts of the same AVFrame,\n     * or multiple AVFrames, and there is no guarantee that slices will be drawn\n     * in order.\n     * The function is also used by hardware acceleration APIs.\n     * It is called at least once during frame decoding to pass\n     * the data needed for hardware render.\n     * In that mode instead of pixel data, AVFrame points to\n     * a structure specific to the acceleration API. The application\n     * reads the structure and can change some fields to indicate progress\n     * or mark state.\n     * - encoding: unused\n     * - decoding: Set by user.\n     * @param height the height of the slice\n     * @param y the y position of the slice\n     * @param type 1->top field, 2->bottom field, 3->frame\n     * @param offset offset into the AVFrame.data from which the slice should be read\n     */\n    void (*draw_horiz_band)(struct AVCodecContext *s,\n                            const AVFrame *src, int offset[AV_NUM_DATA_POINTERS],\n                            int y, int type, int height);\n \n    /**\n     * callback to negotiate the pixelFormat\n     * @param fmt is the list of formats which are supported by the codec,\n     * it is terminated by -1 as 0 is a valid format, the formats are ordered by quality.\n     * The first is always the native one.\n     * @return the chosen format\n     * - encoding: unused\n     * - decoding: Set by user, if not set the native format will be chosen.\n     */\n    enum AVPixelFormat (*get_format)(struct AVCodecContext *s, const enum AVPixelFormat * fmt);\n \n    /**\n     * maximum number of B-frames between non-B-frames\n     * Note: The output will be delayed by max_b_frames+1 relative to the input.\n     * - encoding: Set by user.\n     * - decoding: unused\n     */\n    int max_b_frames;\n \n    /**\n     * qscale factor between IP and B-frames\n     * If > 0 then the last P-frame quantizer will be used (q= lastp_q*factor+offset).\n     * If < 0 then normal ratecontrol will be done (q= -normal_q*factor+offset).\n     * - encoding: Set by user.\n     * - decoding: unused\n     */\n    float b_quant_factor;\n \n    /** obsolete FIXME remove */\n    int rc_strategy;\n#define FF_RC_STRATEGY_XVID 1\n \n    int b_frame_strategy;\n \n#if FF_API_MPV_GLOBAL_OPTS\n    /**\n     * luma single coefficient elimination threshold\n     * - encoding: Set by user.\n     * - decoding: unused\n     */\n    attribute_deprecated int luma_elim_threshold;\n \n    /**\n     * chroma single coeff elimination threshold\n     * - encoding: Set by user.\n     * - decoding: unused\n     */\n    attribute_deprecated int chroma_elim_threshold;\n#endif\n \n    /**\n     * qscale offset between IP and B-frames\n     * - encoding: Set by user.\n     * - decoding: unused\n     */\n    float b_quant_offset;\n \n    /**\n     * Size of the frame reordering buffer in the decoder.\n     * For MPEG-2 it is 1 IPB or 0 low delay IP.\n     * - encoding: Set by libavcodec.\n     * - decoding: Set by libavcodec.\n     */\n    int has_b_frames;\n \n    /**\n     * 0-> h263 quant 1-> mpeg quant\n     * - encoding: Set by user.\n     * - decoding: unused\n     */\n    int mpeg_quant;\n \n    /**\n     * qscale factor between P and I-frames\n     * If > 0 then the last p frame quantizer will be used (q= lastp_q*factor+offset).\n     * If < 0 then normal ratecontrol will be done (q= -normal_q*factor+offset).\n     * - encoding: Set by user.\n     * - decoding: unused\n     */\n    float i_quant_factor;\n \n    /**\n     * qscale offset between P and I-frames\n     * - encoding: Set by user.\n     * - decoding: unused\n     */\n    float i_quant_offset;\n \n    /**\n     * luminance masking (0-> disabled)\n     * - encoding: Set by user.\n     * - decoding: unused\n     */\n    float lumi_masking;\n \n    /**\n     * temporary complexity masking (0-> disabled)\n     * - encoding: Set by user.\n     * - decoding: unused\n     */\n    float temporal_cplx_masking;\n \n    /**\n     * spatial complexity masking (0-> disabled)\n     * - encoding: Set by user.\n     * - decoding: unused\n     */\n    float spatial_cplx_masking;\n \n    /**\n     * p block masking (0-> disabled)\n     * - encoding: Set by user.\n     * - decoding: unused\n     */\n    float p_masking;\n \n    /**\n     * darkness masking (0-> disabled)\n     * - encoding: Set by user.\n     * - decoding: unused\n     */\n    float dark_masking;\n \n    /**\n     * slice count\n     * - encoding: Set by libavcodec.\n     * - decoding: Set by user (or 0).\n     */\n    int slice_count;\n    /**\n     * prediction method (needed for huffyuv)\n     * - encoding: Set by user.\n     * - decoding: unused\n     */\n     int prediction_method;\n#define FF_PRED_LEFT   0\n#define FF_PRED_PLANE  1\n#define FF_PRED_MEDIAN 2\n \n    /**\n     * slice offsets in the frame in bytes\n     * - encoding: Set/allocated by libavcodec.\n     * - decoding: Set/allocated by user (or NULL).\n     */\n    int *slice_offset;\n \n    /**\n     * sample aspect ratio (0 if unknown)\n     * That is the width of a pixel divided by the height of the pixel.\n     * Numerator and denominator must be relatively prime and smaller than 256 for some video standards.\n     * - encoding: Set by user.\n     * - decoding: Set by libavcodec.\n     */\n    AVRational sample_aspect_ratio;\n \n    /**\n     * motion estimation comparison function\n     * - encoding: Set by user.\n     * - decoding: unused\n     */\n    int me_cmp;\n    /**\n     * subpixel motion estimation comparison function\n     * - encoding: Set by user.\n     * - decoding: unused\n     */\n    int me_sub_cmp;\n    /**\n     * macroblock comparison function (not supported yet)\n     * - encoding: Set by user.\n     * - decoding: unused\n     */\n    int mb_cmp;\n    /**\n     * interlaced DCT comparison function\n     * - encoding: Set by user.\n     * - decoding: unused\n     */\n    int ildct_cmp;\n#define FF_CMP_SAD    0\n#define FF_CMP_SSE    1\n#define FF_CMP_SATD   2\n#define FF_CMP_DCT    3\n#define FF_CMP_PSNR   4\n#define FF_CMP_BIT    5\n#define FF_CMP_RD     6\n#define FF_CMP_ZERO   7\n#define FF_CMP_VSAD   8\n#define FF_CMP_VSSE   9\n#define FF_CMP_NSSE   10\n#define FF_CMP_W53    11\n#define FF_CMP_W97    12\n#define FF_CMP_DCTMAX 13\n#define FF_CMP_DCT264 14\n#define FF_CMP_CHROMA 256\n \n    /**\n     * ME diamond size & shape\n     * - encoding: Set by user.\n     * - decoding: unused\n     */\n    int dia_size;\n \n    /**\n     * amount of previous MV predictors (2a+1 x 2a+1 square)\n     * - encoding: Set by user.\n     * - decoding: unused\n     */\n    int last_predictor_count;\n \n    /**\n     * prepass for motion estimation\n     * - encoding: Set by user.\n     * - decoding: unused\n     */\n    int pre_me;\n \n    /**\n     * motion estimation prepass comparison function\n     * - encoding: Set by user.\n     * - decoding: unused\n     */\n    int me_pre_cmp;\n \n    /**\n     * ME prepass diamond size & shape\n     * - encoding: Set by user.\n     * - decoding: unused\n     */\n    int pre_dia_size;\n \n    /**\n     * subpel ME quality\n     * - encoding: Set by user.\n     * - decoding: unused\n     */\n    int me_subpel_quality;\n \n    /**\n     * DTG active format information (additional aspect ratio\n     * information only used in DVB MPEG-2 transport streams)\n     * 0 if not set.\n     *\n     * - encoding: unused\n     * - decoding: Set by decoder.\n     */\n    int dtg_active_format;\n#define FF_DTG_AFD_SAME         8\n#define FF_DTG_AFD_4_3          9\n#define FF_DTG_AFD_16_9         10\n#define FF_DTG_AFD_14_9         11\n#define FF_DTG_AFD_4_3_SP_14_9  13\n#define FF_DTG_AFD_16_9_SP_14_9 14\n#define FF_DTG_AFD_SP_4_3       15\n \n    /**\n     * maximum motion estimation search range in subpel units\n     * If 0 then no limit.\n     *\n     * - encoding: Set by user.\n     * - decoding: unused\n     */\n    int me_range;\n \n    /**\n     * intra quantizer bias\n     * - encoding: Set by user.\n     * - decoding: unused\n     */\n    int intra_quant_bias;\n#define FF_DEFAULT_QUANT_BIAS 999999\n \n    /**\n     * inter quantizer bias\n     * - encoding: Set by user.\n     * - decoding: unused\n     */\n    int inter_quant_bias;\n \n#if FF_API_COLOR_TABLE_ID\n    /**\n     * color table ID\n     * - encoding: unused\n     * - decoding: Which clrtable should be used for 8bit RGB images.\n     *             Tables have to be stored somewhere. FIXME\n     */\n    attribute_deprecated int color_table_id;\n#endif\n \n    /**\n     * slice flags\n     * - encoding: unused\n     * - decoding: Set by user.\n     */\n    int slice_flags;\n#define SLICE_FLAG_CODED_ORDER    0x0001 ///< draw_horiz_band() is called in coded order instead of display\n#define SLICE_FLAG_ALLOW_FIELD    0x0002 ///< allow draw_horiz_band() with field slices (MPEG2 field pics)\n#define SLICE_FLAG_ALLOW_PLANE    0x0004 ///< allow draw_horiz_band() with 1 component at a time (SVQ1)\n \n    /**\n     * XVideo Motion Acceleration\n     * - encoding: forbidden\n     * - decoding: set by decoder\n     */\n    int xvmc_acceleration;\n \n    /**\n     * macroblock decision mode\n     * - encoding: Set by user.\n     * - decoding: unused\n     */\n    int mb_decision;\n#define FF_MB_DECISION_SIMPLE 0        ///< uses mb_cmp\n#define FF_MB_DECISION_BITS   1        ///< chooses the one which needs the fewest bits\n#define FF_MB_DECISION_RD     2        ///< rate distortion\n \n    /**\n     * custom intra quantization matrix\n     * - encoding: Set by user, can be NULL.\n     * - decoding: Set by libavcodec.\n     */\n    uint16_t *intra_matrix;\n \n    /**\n     * custom inter quantization matrix\n     * - encoding: Set by user, can be NULL.\n     * - decoding: Set by libavcodec.\n     */\n    uint16_t *inter_matrix;\n \n    /**\n     * scene change detection threshold\n     * 0 is default, larger means fewer detected scene changes.\n     * - encoding: Set by user.\n     * - decoding: unused\n     */\n    int scenechange_threshold;\n \n    /**\n     * noise reduction strength\n     * - encoding: Set by user.\n     * - decoding: unused\n     */\n    int noise_reduction;\n \n#if FF_API_INTER_THRESHOLD\n    /**\n     * @deprecated this field is unused\n     */\n    attribute_deprecated int inter_threshold;\n#endif\n \n#if FF_API_MPV_GLOBAL_OPTS\n    /**\n     * @deprecated use mpegvideo private options instead\n     */\n    attribute_deprecated int quantizer_noise_shaping;\n#endif\n \n    /**\n     * Motion estimation threshold below which no motion estimation is\n     * performed, but instead the user specified motion vectors are used.\n     *\n     * - encoding: Set by user.\n     * - decoding: unused\n     */\n    int me_threshold;\n \n    /**\n     * Macroblock threshold below which the user specified macroblock types will be used.\n     * - encoding: Set by user.\n     * - decoding: unused\n     */\n    int mb_threshold;\n \n    /**\n     * precision of the intra DC coefficient - 8\n     * - encoding: Set by user.\n     * - decoding: unused\n     */\n    int intra_dc_precision;\n \n    /**\n     * Number of macroblock rows at the top which are skipped.\n     * - encoding: unused\n     * - decoding: Set by user.\n     */\n    int skip_top;\n \n    /**\n     * Number of macroblock rows at the bottom which are skipped.\n     * - encoding: unused\n     * - decoding: Set by user.\n     */\n    int skip_bottom;\n \n    /**\n     * Border processing masking, raises the quantizer for mbs on the borders\n     * of the picture.\n     * - encoding: Set by user.\n     * - decoding: unused\n     */\n    float border_masking;\n \n    /**\n     * minimum MB lagrange multipler\n     * - encoding: Set by user.\n     * - decoding: unused\n     */\n    int mb_lmin;\n \n    /**\n     * maximum MB lagrange multipler\n     * - encoding: Set by user.\n     * - decoding: unused\n     */\n    int mb_lmax;\n \n    /**\n     *\n     * - encoding: Set by user.\n     * - decoding: unused\n     */\n    int me_penalty_compensation;\n \n    /**\n     *\n     * - encoding: Set by user.\n     * - decoding: unused\n     */\n    int bidir_refine;\n \n    /**\n     *\n     * - encoding: Set by user.\n     * - decoding: unused\n     */\n    int brd_scale;\n \n    /**\n     * minimum GOP size\n     * - encoding: Set by user.\n     * - decoding: unused\n     */\n    int keyint_min;\n \n    /**\n     * number of reference frames\n     * - encoding: Set by user.\n     * - decoding: Set by lavc.\n     */\n    int refs;\n \n    /**\n     * chroma qp offset from luma\n     * - encoding: Set by user.\n     * - decoding: unused\n     */\n    int chromaoffset;\n \n    /**\n     * Multiplied by qscale for each frame and added to scene_change_score.\n     * - encoding: Set by user.\n     * - decoding: unused\n     */\n    int scenechange_factor;\n \n    /**\n     *\n     * Note: Value depends upon the compare function used for fullpel ME.\n     * - encoding: Set by user.\n     * - decoding: unused\n     */\n    int mv0_threshold;\n \n    /**\n     * Adjust sensitivity of b_frame_strategy 1.\n     * - encoding: Set by user.\n     * - decoding: unused\n     */\n    int b_sensitivity;\n \n    /**\n     * Chromaticity coordinates of the source primaries.\n     * - encoding: Set by user\n     * - decoding: Set by libavcodec\n     */\n    enum AVColorPrimaries color_primaries;\n \n    /**\n     * Color Transfer Characteristic.\n     * - encoding: Set by user\n     * - decoding: Set by libavcodec\n     */\n    enum AVColorTransferCharacteristic color_trc;\n \n    /**\n     * YUV colorspace type.\n     * - encoding: Set by user\n     * - decoding: Set by libavcodec\n     */\n    enum AVColorSpace colorspace;\n \n    /**\n     * MPEG vs JPEG YUV range.\n     * - encoding: Set by user\n     * - decoding: Set by libavcodec\n     */\n    enum AVColorRange color_range;\n \n    /**\n     * This defines the location of chroma samples.\n     * - encoding: Set by user\n     * - decoding: Set by libavcodec\n     */\n    enum AVChromaLocation chroma_sample_location;\n \n    /**\n     * Number of slices.\n     * Indicates number of picture subdivisions. Used for parallelized\n     * decoding.\n     * - encoding: Set by user\n     * - decoding: unused\n     */\n    int slices;\n \n    /** Field order\n     * - encoding: set by libavcodec\n     * - decoding: Set by user.\n     */\n    enum AVFieldOrder field_order;\n \n    /* audio only */\n    int sample_rate; ///< samples per second\n    int channels;    ///< number of audio channels\n \n    /**\n     * audio sample format\n     * - encoding: Set by user.\n     * - decoding: Set by libavcodec.\n     */\n    enum AVSampleFormat sample_fmt;  ///< sample format\n \n    /* The following data should not be initialized. */\n    /**\n     * Samples per packet, initialized when calling 'init'.\n     */\n    int frame_size;\n \n    /**\n     * Frame counter, set by libavcodec.\n     *\n     * - decoding: total number of frames returned from the decoder so far.\n     * - encoding: total number of frames passed to the encoder so far.\n     *\n     *   @note the counter is not incremented if encoding/decoding resulted in\n     *   an error.\n     */\n    int frame_number;\n \n    /**\n     * number of bytes per packet if constant and known or 0\n     * Used by some WAV based audio codecs.\n     */\n    int block_align;\n \n    /**\n     * Audio cutoff bandwidth (0 means \"automatic\")\n     * - encoding: Set by user.\n     * - decoding: unused\n     */\n    int cutoff;\n \n#if FF_API_REQUEST_CHANNELS\n    /**\n     * Decoder should decode to this many channels if it can (0 for default)\n     * - encoding: unused\n     * - decoding: Set by user.\n     * @deprecated Deprecated in favor of request_channel_layout.\n     */\n    int request_channels;\n#endif\n \n    /**\n     * Audio channel layout.\n     * - encoding: set by user.\n     * - decoding: set by user, may be overwritten by libavcodec.\n     */\n    uint64_t channel_layout;\n \n    /**\n     * Request decoder to use this channel layout if it can (0 for default)\n     * - encoding: unused\n     * - decoding: Set by user.\n     */\n    uint64_t request_channel_layout;\n \n    /**\n     * Type of service that the audio stream conveys.\n     * - encoding: Set by user.\n     * - decoding: Set by libavcodec.\n     */\n    enum AVAudioServiceType audio_service_type;\n \n    /**\n     * desired sample format\n     * - encoding: Not used.\n     * - decoding: Set by user.\n     * Decoder will decode to this format if it can.\n     */\n    enum AVSampleFormat request_sample_fmt;\n \n    /**\n     * Called at the beginning of each frame to get a buffer for it.\n     *\n     * The function will set AVFrame.data[], AVFrame.linesize[].\n     * AVFrame.extended_data[] must also be set, but it should be the same as\n     * AVFrame.data[] except for planar audio with more channels than can fit\n     * in AVFrame.data[]. In that case, AVFrame.data[] shall still contain as\n     * many data pointers as it can hold.\n     *\n     * if CODEC_CAP_DR1 is not set then get_buffer() must call\n     * avcodec_default_get_buffer() instead of providing buffers allocated by\n     * some other means.\n     *\n     * AVFrame.data[] should be 32- or 16-byte-aligned unless the CPU doesn't\n     * need it. avcodec_default_get_buffer() aligns the output buffer properly,\n     * but if get_buffer() is overridden then alignment considerations should\n     * be taken into account.\n     *\n     * @see avcodec_default_get_buffer()\n     *\n     * Video:\n     *\n     * If pic.reference is set then the frame will be read later by libavcodec.\n     * avcodec_align_dimensions2() should be used to find the required width and\n     * height, as they normally need to be rounded up to the next multiple of 16.\n     *\n     * If frame multithreading is used and thread_safe_callbacks is set,\n     * it may be called from a different thread, but not from more than one at\n     * once. Does not need to be reentrant.\n     *\n     * @see release_buffer(), reget_buffer()\n     * @see avcodec_align_dimensions2()\n     *\n     * Audio:\n     *\n     * Decoders request a buffer of a particular size by setting\n     * AVFrame.nb_samples prior to calling get_buffer(). The decoder may,\n     * however, utilize only part of the buffer by setting AVFrame.nb_samples\n     * to a smaller value in the output frame.\n     *\n     * Decoders cannot use the buffer after returning from\n     * avcodec_decode_audio4(), so they will not call release_buffer(), as it\n     * is assumed to be released immediately upon return.\n     *\n     * As a convenience, av_samples_get_buffer_size() and\n     * av_samples_fill_arrays() in libavutil may be used by custom get_buffer()\n     * functions to find the required data size and to fill data pointers and\n     * linesize. In AVFrame.linesize, only linesize[0] may be set for audio\n     * since all planes must be the same size.\n     *\n     * @see av_samples_get_buffer_size(), av_samples_fill_arrays()\n     *\n     * - encoding: unused\n     * - decoding: Set by libavcodec, user can override.\n     */\n    int (*get_buffer)(struct AVCodecContext *c, AVFrame *pic);\n \n    /**\n     * Called to release buffers which were allocated with get_buffer.\n     * A released buffer can be reused in get_buffer().\n     * pic.data[*] must be set to NULL.\n     * May be called from a different thread if frame multithreading is used,\n     * but not by more than one thread at once, so does not need to be reentrant.\n     * - encoding: unused\n     * - decoding: Set by libavcodec, user can override.\n     */\n    void (*release_buffer)(struct AVCodecContext *c, AVFrame *pic);\n \n    /**\n     * Called at the beginning of a frame to get cr buffer for it.\n     * Buffer type (size, hints) must be the same. libavcodec won't check it.\n     * libavcodec will pass previous buffer in pic, function should return\n     * same buffer or new buffer with old frame \"painted\" into it.\n     * If pic.data[0] == NULL must behave like get_buffer().\n     * if CODEC_CAP_DR1 is not set then reget_buffer() must call\n     * avcodec_default_reget_buffer() instead of providing buffers allocated by\n     * some other means.\n     * - encoding: unused\n     * - decoding: Set by libavcodec, user can override.\n     */\n    int (*reget_buffer)(struct AVCodecContext *c, AVFrame *pic);\n \n \n    /* - encoding parameters */\n    float qcompress;  ///< amount of qscale change between easy & hard scenes (0.0-1.0)\n    float qblur;      ///< amount of qscale smoothing over time (0.0-1.0)\n \n    /**\n     * minimum quantizer\n     * - encoding: Set by user.\n     * - decoding: unused\n     */\n    int qmin;\n \n    /**\n     * maximum quantizer\n     * - encoding: Set by user.\n     * - decoding: unused\n     */\n    int qmax;\n \n    /**\n     * maximum quantizer difference between frames\n     * - encoding: Set by user.\n     * - decoding: unused\n     */\n    int max_qdiff;\n \n    /**\n     * ratecontrol qmin qmax limiting method\n     * 0-> clipping, 1-> use a nice continuous function to limit qscale wthin qmin/qmax.\n     * - encoding: Set by user.\n     * - decoding: unused\n     */\n    float rc_qsquish;\n \n    float rc_qmod_amp;\n    int rc_qmod_freq;\n \n    /**\n     * decoder bitstream buffer size\n     * - encoding: Set by user.\n     * - decoding: unused\n     */\n    int rc_buffer_size;\n \n    /**\n     * ratecontrol override, see RcOverride\n     * - encoding: Allocated/set/freed by user.\n     * - decoding: unused\n     */\n    int rc_override_count;\n    RcOverride *rc_override;\n \n    /**\n     * rate control equation\n     * - encoding: Set by user\n     * - decoding: unused\n     */\n    const char *rc_eq;\n \n    /**\n     * maximum bitrate\n     * - encoding: Set by user.\n     * - decoding: unused\n     */\n    int rc_max_rate;\n \n    /**\n     * minimum bitrate\n     * - encoding: Set by user.\n     * - decoding: unused\n     */\n    int rc_min_rate;\n \n    float rc_buffer_aggressivity;\n \n    /**\n     * initial complexity for pass1 ratecontrol\n     * - encoding: Set by user.\n     * - decoding: unused\n     */\n    float rc_initial_cplx;\n \n    /**\n     * Ratecontrol attempt to use, at maximum, <value> of what can be used without an underflow.\n     * - encoding: Set by user.\n     * - decoding: unused.\n     */\n    float rc_max_available_vbv_use;\n \n    /**\n     * Ratecontrol attempt to use, at least, <value> times the amount needed to prevent a vbv overflow.\n     * - encoding: Set by user.\n     * - decoding: unused.\n     */\n    float rc_min_vbv_overflow_use;\n \n    /**\n     * Number of bits which should be loaded into the rc buffer before decoding starts.\n     * - encoding: Set by user.\n     * - decoding: unused\n     */\n    int rc_initial_buffer_occupancy;\n \n#define FF_CODER_TYPE_VLC       0\n#define FF_CODER_TYPE_AC        1\n#define FF_CODER_TYPE_RAW       2\n#define FF_CODER_TYPE_RLE       3\n#define FF_CODER_TYPE_DEFLATE   4\n    /**\n     * coder type\n     * - encoding: Set by user.\n     * - decoding: unused\n     */\n    int coder_type;\n \n    /**\n     * context model\n     * - encoding: Set by user.\n     * - decoding: unused\n     */\n    int context_model;\n \n    /**\n     * minimum Lagrange multipler\n     * - encoding: Set by user.\n     * - decoding: unused\n     */\n    int lmin;\n \n    /**\n     * maximum Lagrange multipler\n     * - encoding: Set by user.\n     * - decoding: unused\n     */\n    int lmax;\n \n    /**\n     * frame skip threshold\n     * - encoding: Set by user.\n     * - decoding: unused\n     */\n    int frame_skip_threshold;\n \n    /**\n     * frame skip factor\n     * - encoding: Set by user.\n     * - decoding: unused\n     */\n    int frame_skip_factor;\n \n    /**\n     * frame skip exponent\n     * - encoding: Set by user.\n     * - decoding: unused\n     */\n    int frame_skip_exp;\n \n    /**\n     * frame skip comparison function\n     * - encoding: Set by user.\n     * - decoding: unused\n     */\n    int frame_skip_cmp;\n \n    /**\n     * trellis RD quantization\n     * - encoding: Set by user.\n     * - decoding: unused\n     */\n    int trellis;\n \n    /**\n     * - encoding: Set by user.\n     * - decoding: unused\n     */\n    int min_prediction_order;\n \n    /**\n     * - encoding: Set by user.\n     * - decoding: unused\n     */\n    int max_prediction_order;\n \n    /**\n     * GOP timecode frame start number\n     * - encoding: Set by user, in non drop frame format\n     * - decoding: Set by libavcodec (timecode in the 25 bits format, -1 if unset)\n     */\n    int64_t timecode_frame_start;\n \n    /* The RTP callback: This function is called    */\n    /* every time the encoder has a packet to send. */\n    /* It depends on the encoder if the data starts */\n    /* with a Start Code (it should). H.263 does.   */\n    /* mb_nb contains the number of macroblocks     */\n    /* encoded in the RTP payload.                  */\n    void (*rtp_callback)(struct AVCodecContext *avctx, void *data, int size, int mb_nb);\n \n    int rtp_payload_size;   /* The size of the RTP payload: the coder will  */\n                            /* do its best to deliver a chunk with size     */\n                            /* below rtp_payload_size, the chunk will start */\n                            /* with a start code on some codecs like H.263. */\n                            /* This doesn't take account of any particular  */\n                            /* headers inside the transmitted RTP payload.  */\n \n    /* statistics, used for 2-pass encoding */\n    int mv_bits;\n    int header_bits;\n    int i_tex_bits;\n    int p_tex_bits;\n    int i_count;\n    int p_count;\n    int skip_count;\n    int misc_bits;\n \n    /**\n     * number of bits used for the previously encoded frame\n     * - encoding: Set by libavcodec.\n     * - decoding: unused\n     */\n    int frame_bits;\n \n    /**\n     * pass1 encoding statistics output buffer\n     * - encoding: Set by libavcodec.\n     * - decoding: unused\n     */\n    char *stats_out;\n \n    /**\n     * pass2 encoding statistics input buffer\n     * Concatenated stuff from stats_out of pass1 should be placed here.\n     * - encoding: Allocated/set/freed by user.\n     * - decoding: unused\n     */\n    char *stats_in;\n \n    /**\n     * Work around bugs in encoders which sometimes cannot be detected automatically.\n     * - encoding: Set by user\n     * - decoding: Set by user\n     */\n    int workaround_bugs;\n#define FF_BUG_AUTODETECT       1  ///< autodetection\n#define FF_BUG_OLD_MSMPEG4      2\n#define FF_BUG_XVID_ILACE       4\n#define FF_BUG_UMP4             8\n#define FF_BUG_NO_PADDING       16\n#define FF_BUG_AMV              32\n#define FF_BUG_AC_VLC           0  ///< Will be removed, libavcodec can now handle these non-compliant files by default.\n#define FF_BUG_QPEL_CHROMA      64\n#define FF_BUG_STD_QPEL         128\n#define FF_BUG_QPEL_CHROMA2     256\n#define FF_BUG_DIRECT_BLOCKSIZE 512\n#define FF_BUG_EDGE             1024\n#define FF_BUG_HPEL_CHROMA      2048\n#define FF_BUG_DC_CLIP          4096\n#define FF_BUG_MS               8192 ///< Work around various bugs in Microsoft's broken decoders.\n#define FF_BUG_TRUNCATED       16384\n \n    /**\n     * strictly follow the standard (MPEG4, ...).\n     * - encoding: Set by user.\n     * - decoding: Set by user.\n     * Setting this to STRICT or higher means the encoder and decoder will\n     * generally do stupid things, whereas setting it to unofficial or lower\n     * will mean the encoder might produce output that is not supported by all\n     * spec-compliant decoders. Decoders don't differentiate between normal,\n     * unofficial and experimental (that is, they always try to decode things\n     * when they can) unless they are explicitly asked to behave stupidly\n     * (=strictly conform to the specs)\n     */\n    int strict_std_compliance;\n#define FF_COMPLIANCE_VERY_STRICT   2 ///< Strictly conform to an older more strict version of the spec or reference software.\n#define FF_COMPLIANCE_STRICT        1 ///< Strictly conform to all the things in the spec no matter what consequences.\n#define FF_COMPLIANCE_NORMAL        0\n#define FF_COMPLIANCE_UNOFFICIAL   -1 ///< Allow unofficial extensions\n#define FF_COMPLIANCE_EXPERIMENTAL -2 ///< Allow nonstandardized experimental things.\n \n    /**\n     * error concealment flags\n     * - encoding: unused\n     * - decoding: Set by user.\n     */\n    int error_concealment;\n#define FF_EC_GUESS_MVS   1\n#define FF_EC_DEBLOCK     2\n \n    /**\n     * debug\n     * - encoding: Set by user.\n     * - decoding: Set by user.\n     */\n    int debug;\n#define FF_DEBUG_PICT_INFO   1\n#define FF_DEBUG_RC          2\n#define FF_DEBUG_BITSTREAM   4\n#define FF_DEBUG_MB_TYPE     8\n#define FF_DEBUG_QP          16\n#define FF_DEBUG_MV          32\n#define FF_DEBUG_DCT_COEFF   0x00000040\n#define FF_DEBUG_SKIP        0x00000080\n#define FF_DEBUG_STARTCODE   0x00000100\n#define FF_DEBUG_PTS         0x00000200\n#define FF_DEBUG_ER          0x00000400\n#define FF_DEBUG_MMCO        0x00000800\n#define FF_DEBUG_BUGS        0x00001000\n#define FF_DEBUG_VIS_QP      0x00002000\n#define FF_DEBUG_VIS_MB_TYPE 0x00004000\n#define FF_DEBUG_BUFFERS     0x00008000\n#define FF_DEBUG_THREADS     0x00010000\n \n    /**\n     * debug\n     * - encoding: Set by user.\n     * - decoding: Set by user.\n     */\n    int debug_mv;\n#define FF_DEBUG_VIS_MV_P_FOR  0x00000001 //visualize forward predicted MVs of P frames\n#define FF_DEBUG_VIS_MV_B_FOR  0x00000002 //visualize forward predicted MVs of B frames\n#define FF_DEBUG_VIS_MV_B_BACK 0x00000004 //visualize backward predicted MVs of B frames\n \n    /**\n     * Error recognition; may misdetect some more or less valid parts as errors.\n     * - encoding: unused\n     * - decoding: Set by user.\n     */\n    int err_recognition;\n#define AV_EF_CRCCHECK  (1<<0)\n#define AV_EF_BITSTREAM (1<<1)\n#define AV_EF_BUFFER    (1<<2)\n#define AV_EF_EXPLODE   (1<<3)\n \n#define AV_EF_CAREFUL    (1<<16)\n#define AV_EF_COMPLIANT  (1<<17)\n#define AV_EF_AGGRESSIVE (1<<18)\n \n \n    /**\n     * opaque 64bit number (generally a PTS) that will be reordered and\n     * output in AVFrame.reordered_opaque\n     * @deprecated in favor of pkt_pts\n     * - encoding: unused\n     * - decoding: Set by user.\n     */\n    int64_t reordered_opaque;\n \n    /**\n     * Hardware accelerator in use\n     * - encoding: unused.\n     * - decoding: Set by libavcodec\n     */\n    struct AVHWAccel *hwaccel;\n \n    /**\n     * Hardware accelerator context.\n     * For some hardware accelerators, a global context needs to be\n     * provided by the user. In that case, this holds display-dependent\n     * data FFmpeg cannot instantiate itself. Please refer to the\n     * FFmpeg HW accelerator documentation to know how to fill this\n     * is. e.g. for VA API, this is a struct vaapi_context.\n     * - encoding: unused\n     * - decoding: Set by user\n     */\n    void *hwaccel_context;\n \n    /**\n     * error\n     * - encoding: Set by libavcodec if flags&CODEC_FLAG_PSNR.\n     * - decoding: unused\n     */\n    uint64_t error[AV_NUM_DATA_POINTERS];\n \n    /**\n     * DCT algorithm, see FF_DCT_* below\n     * - encoding: Set by user.\n     * - decoding: unused\n     */\n    int dct_algo;\n#define FF_DCT_AUTO    0\n#define FF_DCT_FASTINT 1\n#define FF_DCT_INT     2\n#define FF_DCT_MMX     3\n#define FF_DCT_ALTIVEC 5\n#define FF_DCT_FAAN    6\n \n    /**\n     * IDCT algorithm, see FF_IDCT_* below.\n     * - encoding: Set by user.\n     * - decoding: Set by user.\n     */\n    int idct_algo;\n#define FF_IDCT_AUTO          0\n#define FF_IDCT_INT           1\n#define FF_IDCT_SIMPLE        2\n#define FF_IDCT_SIMPLEMMX     3\n#define FF_IDCT_LIBMPEG2MMX   4\n#define FF_IDCT_MMI           5\n#define FF_IDCT_ARM           7\n#define FF_IDCT_ALTIVEC       8\n#define FF_IDCT_SH4           9\n#define FF_IDCT_SIMPLEARM     10\n#define FF_IDCT_H264          11\n#define FF_IDCT_VP3           12\n#define FF_IDCT_IPP           13\n#define FF_IDCT_XVIDMMX       14\n#define FF_IDCT_CAVS          15\n#define FF_IDCT_SIMPLEARMV5TE 16\n#define FF_IDCT_SIMPLEARMV6   17\n#define FF_IDCT_SIMPLEVIS     18\n#define FF_IDCT_WMV2          19\n#define FF_IDCT_FAAN          20\n#define FF_IDCT_EA            21\n#define FF_IDCT_SIMPLENEON    22\n#define FF_IDCT_SIMPLEALPHA   23\n#define FF_IDCT_BINK          24\n \n#if FF_API_DSP_MASK\n    /**\n     * Unused.\n     * @deprecated use av_set_cpu_flags_mask() instead.\n     */\n    attribute_deprecated unsigned dsp_mask;\n#endif\n \n    /**\n     * bits per sample/pixel from the demuxer (needed for huffyuv).\n     * - encoding: Set by libavcodec.\n     * - decoding: Set by user.\n     */\n     int bits_per_coded_sample;\n \n    /**\n     * Bits per sample/pixel of internal libavcodec pixel/sample format.\n     * - encoding: set by user.\n     * - decoding: set by libavcodec.\n     */\n    int bits_per_raw_sample;\n \n    /**\n     * low resolution decoding, 1-> 1/2 size, 2->1/4 size\n     * - encoding: unused\n     * - decoding: Set by user.\n     */\n     int lowres;\n \n    /**\n     * the picture in the bitstream\n     * - encoding: Set by libavcodec.\n     * - decoding: Set by libavcodec.\n     */\n    AVFrame *coded_frame;\n \n    /**\n     * thread count\n     * is used to decide how many independent tasks should be passed to execute()\n     * - encoding: Set by user.\n     * - decoding: Set by user.\n     */\n    int thread_count;\n \n    /**\n     * Which multithreading methods to use.\n     * Use of FF_THREAD_FRAME will increase decoding delay by one frame per thread,\n     * so clients which cannot provide future frames should not use it.\n     *\n     * - encoding: Set by user, otherwise the default is used.\n     * - decoding: Set by user, otherwise the default is used.\n     */\n    int thread_type;\n#define FF_THREAD_FRAME   1 ///< Decode more than one frame at once\n#define FF_THREAD_SLICE   2 ///< Decode more than one part of a single frame at once\n \n    /**\n     * Which multithreading methods are in use by the codec.\n     * - encoding: Set by libavcodec.\n     * - decoding: Set by libavcodec.\n     */\n    int active_thread_type;\n \n    /**\n     * Set by the client if its custom get_buffer() callback can be called\n     * synchronously from another thread, which allows faster multithreaded decoding.\n     * draw_horiz_band() will be called from other threads regardless of this setting.\n     * Ignored if the default get_buffer() is used.\n     * - encoding: Set by user.\n     * - decoding: Set by user.\n     */\n    int thread_safe_callbacks;\n \n    /**\n     * The codec may call this to execute several independent things.\n     * It will return only after finishing all tasks.\n     * The user may replace this with some multithreaded implementation,\n     * the default implementation will execute the parts serially.\n     * @param count the number of things to execute\n     * - encoding: Set by libavcodec, user can override.\n     * - decoding: Set by libavcodec, user can override.\n     */\n    int (*execute)(struct AVCodecContext *c, int (*func)(struct AVCodecContext *c2, void *arg), void *arg2, int *ret, int count, int size);\n \n    /**\n     * The codec may call this to execute several independent things.\n     * It will return only after finishing all tasks.\n     * The user may replace this with some multithreaded implementation,\n     * the default implementation will execute the parts serially.\n     * Also see avcodec_thread_init and e.g. the --enable-pthread configure option.\n     * @param c context passed also to func\n     * @param count the number of things to execute\n     * @param arg2 argument passed unchanged to func\n     * @param ret return values of executed functions, must have space for \"count\" values. May be NULL.\n     * @param func function that will be called count times, with jobnr from 0 to count-1.\n     *             threadnr will be in the range 0 to c->thread_count-1 < MAX_THREADS and so that no\n     *             two instances of func executing at the same time will have the same threadnr.\n     * @return always 0 currently, but code should handle a future improvement where when any call to func\n     *         returns < 0 no further calls to func may be done and < 0 is returned.\n     * - encoding: Set by libavcodec, user can override.\n     * - decoding: Set by libavcodec, user can override.\n     */\n    int (*execute2)(struct AVCodecContext *c, int (*func)(struct AVCodecContext *c2, void *arg, int jobnr, int threadnr), void *arg2, int *ret, int count);\n \n    /**\n     * thread opaque\n     * Can be used by execute() to store some per AVCodecContext stuff.\n     * - encoding: set by execute()\n     * - decoding: set by execute()\n     */\n    void *thread_opaque;\n \n    /**\n     * noise vs. sse weight for the nsse comparsion function\n     * - encoding: Set by user.\n     * - decoding: unused\n     */\n     int nsse_weight;\n \n    /**\n     * profile\n     * - encoding: Set by user.\n     * - decoding: Set by libavcodec.\n     */\n     int profile;\n#define FF_PROFILE_UNKNOWN -99\n#define FF_PROFILE_RESERVED -100\n \n#define FF_PROFILE_AAC_MAIN 0\n#define FF_PROFILE_AAC_LOW  1\n#define FF_PROFILE_AAC_SSR  2\n#define FF_PROFILE_AAC_LTP  3\n#define FF_PROFILE_AAC_HE   4\n#define FF_PROFILE_AAC_HE_V2 28\n#define FF_PROFILE_AAC_LD   22\n#define FF_PROFILE_AAC_ELD  38\n \n#define FF_PROFILE_DTS         20\n#define FF_PROFILE_DTS_ES      30\n#define FF_PROFILE_DTS_96_24   40\n#define FF_PROFILE_DTS_HD_HRA  50\n#define FF_PROFILE_DTS_HD_MA   60\n \n#define FF_PROFILE_MPEG2_422    0\n#define FF_PROFILE_MPEG2_HIGH   1\n#define FF_PROFILE_MPEG2_SS     2\n#define FF_PROFILE_MPEG2_SNR_SCALABLE  3\n#define FF_PROFILE_MPEG2_MAIN   4\n#define FF_PROFILE_MPEG2_SIMPLE 5\n \n#define FF_PROFILE_H264_CONSTRAINED  (1<<9)  // 8+1; constraint_set1_flag\n#define FF_PROFILE_H264_INTRA        (1<<11) // 8+3; constraint_set3_flag\n \n#define FF_PROFILE_H264_BASELINE             66\n#define FF_PROFILE_H264_CONSTRAINED_BASELINE (66|FF_PROFILE_H264_CONSTRAINED)\n#define FF_PROFILE_H264_MAIN                 77\n#define FF_PROFILE_H264_EXTENDED             88\n#define FF_PROFILE_H264_HIGH                 100\n#define FF_PROFILE_H264_HIGH_10              110\n#define FF_PROFILE_H264_HIGH_10_INTRA        (110|FF_PROFILE_H264_INTRA)\n#define FF_PROFILE_H264_HIGH_422             122\n#define FF_PROFILE_H264_HIGH_422_INTRA       (122|FF_PROFILE_H264_INTRA)\n#define FF_PROFILE_H264_HIGH_444             144\n#define FF_PROFILE_H264_HIGH_444_PREDICTIVE  244\n#define FF_PROFILE_H264_HIGH_444_INTRA       (244|FF_PROFILE_H264_INTRA)\n#define FF_PROFILE_H264_CAVLC_444            44\n \n#define FF_PROFILE_VC1_SIMPLE   0\n#define FF_PROFILE_VC1_MAIN     1\n#define FF_PROFILE_VC1_COMPLEX  2\n#define FF_PROFILE_VC1_ADVANCED 3\n \n#define FF_PROFILE_MPEG4_SIMPLE                     0\n#define FF_PROFILE_MPEG4_SIMPLE_SCALABLE            1\n#define FF_PROFILE_MPEG4_CORE                       2\n#define FF_PROFILE_MPEG4_MAIN                       3\n#define FF_PROFILE_MPEG4_N_BIT                      4\n#define FF_PROFILE_MPEG4_SCALABLE_TEXTURE           5\n#define FF_PROFILE_MPEG4_SIMPLE_FACE_ANIMATION      6\n#define FF_PROFILE_MPEG4_BASIC_ANIMATED_TEXTURE     7\n#define FF_PROFILE_MPEG4_HYBRID                     8\n#define FF_PROFILE_MPEG4_ADVANCED_REAL_TIME         9\n#define FF_PROFILE_MPEG4_CORE_SCALABLE             10\n#define FF_PROFILE_MPEG4_ADVANCED_CODING           11\n#define FF_PROFILE_MPEG4_ADVANCED_CORE             12\n#define FF_PROFILE_MPEG4_ADVANCED_SCALABLE_TEXTURE 13\n#define FF_PROFILE_MPEG4_SIMPLE_STUDIO             14\n#define FF_PROFILE_MPEG4_ADVANCED_SIMPLE           15\n \n    /**\n     * level\n     * - encoding: Set by user.\n     * - decoding: Set by libavcodec.\n     */\n     int level;\n#define FF_LEVEL_UNKNOWN -99\n \n    /**\n     *\n     * - encoding: unused\n     * - decoding: Set by user.\n     */\n    enum AVDiscard skip_loop_filter;\n \n    /**\n     *\n     * - encoding: unused\n     * - decoding: Set by user.\n     */\n    enum AVDiscard skip_idct;\n \n    /**\n     *\n     * - encoding: unused\n     * - decoding: Set by user.\n     */\n    enum AVDiscard skip_frame;\n \n    /**\n     * Header containing style information for text subtitles.\n     * For SUBTITLE_ASS subtitle type, it should contain the whole ASS\n     * [Script Info] and [V4+ Styles] section, plus the [Events] line and\n     * the Format line following. It shouldn't include any Dialogue line.\n     * - encoding: Set/allocated/freed by user (before avcodec_open2())\n     * - decoding: Set/allocated/freed by libavcodec (by avcodec_open2())\n     */\n    uint8_t *subtitle_header;\n    int subtitle_header_size;\n \n    /**\n     * Simulates errors in the bitstream to test error concealment.\n     * - encoding: Set by user.\n     * - decoding: unused\n     */\n    int error_rate;\n \n    /**\n     * Current packet as passed into the decoder, to avoid having\n     * to pass the packet into every function. Currently only valid\n     * inside lavc and get/release_buffer callbacks.\n     * - decoding: set by avcodec_decode_*, read by get_buffer() for setting pkt_pts\n     * - encoding: unused\n     */\n    AVPacket *pkt;\n \n    /**\n     * VBV delay coded in the last frame (in periods of a 27 MHz clock).\n     * Used for compliant TS muxing.\n     * - encoding: Set by libavcodec.\n     * - decoding: unused.\n     */\n    uint64_t vbv_delay;\n \n    /**\n     * Timebase in which pkt_dts/pts and AVPacket.dts/pts are.\n     * Code outside libavcodec should access this field using:\n     * avcodec_set_pkt_timebase(avctx)\n     * - encoding unused.\n     * - decodimg set by user\n     */\n    AVRational pkt_timebase;\n \n    /**\n     * AVCodecDescriptor\n     * Code outside libavcodec should access this field using:\n     * avcodec_get_codec_descriptior(avctx)\n     * - encoding: unused.\n     * - decoding: set by libavcodec.\n     */\n    const AVCodecDescriptor *codec_descriptor;\n \n    /**\n     * Current statistics for PTS correction.\n     * - decoding: maintained and used by libavcodec, not intended to be used by user apps\n     * - encoding: unused\n     */\n    int64_t pts_correction_num_faulty_pts; /// Number of incorrect PTS values so far\n    int64_t pts_correction_num_faulty_dts; /// Number of incorrect DTS values so far\n    int64_t pts_correction_last_pts;       /// PTS of the last frame\n    int64_t pts_correction_last_dts;       /// DTS of the last frame\n} AVCodecContext;\n```\n\n\n\n# 二、AVCodecContext 重点字段\n\n下面挑一些关键的变量来看看（这里只考虑解码）：\n\n\n\n```\nenum AVMediaType codec_type：编解码器的类型（视频，音频...）\n\nstruct AVCodec  *codec：采用的解码器AVCodec（H.264,MPEG2...）\n\nint bit_rate：平均比特率\n\nuint8_t *extradata; int extradata_size：针对特定编码器包含的附加信息（例如对于H.264解码器来说，存储SPS，PPS等）\n\nAVRational time_base：根据该参数，可以把PTS转化为实际的时间（单位为秒s）\n\nint width, height：如果是视频的话，代表宽和高\n\nint refs：运动估计参考帧的个数（H.264的话会有多帧，MPEG2这类的一般就没有了）\n\nint sample_rate：采样率（音频）\n\nint channels：声道数（音频）\n\nenum AVSampleFormat sample_fmt：采样格式\n\nint profile：型（H.264里面就有，其他编码标准应该也有）\n\nint level：级（和profile差不太多）\n```\n\n\n\n在这里需要注意：AVCodecContext中很多的参数是编码的时候使用的，而不是解码的时候使用的。\n\n其实这些参数都比较容易理解。就不多费篇幅了。在这里看一下以下几个参数：\n\n### 1.codec_type\n\n编解码器类型有以下几种：\n\n\n\n```\nenum AVMediaType {\n    AVMEDIA_TYPE_UNKNOWN = -1,  ///< Usually treated as AVMEDIA_TYPE_DATA\n    AVMEDIA_TYPE_VIDEO,\n    AVMEDIA_TYPE_AUDIO,\n    AVMEDIA_TYPE_DATA,          ///< Opaque data information usually continuous\n    AVMEDIA_TYPE_SUBTITLE,\n    AVMEDIA_TYPE_ATTACHMENT,    ///< Opaque data information usually sparse\n    AVMEDIA_TYPE_NB\n};\n```\n\n\n\n### 2.sample_fmt \n\n在FFMPEG中音频采样格式有以下几种：\n\n\n\n```\nenum AVSampleFormat {\n    AV_SAMPLE_FMT_NONE = -1,\n    AV_SAMPLE_FMT_U8,          ///< unsigned 8 bits\n    AV_SAMPLE_FMT_S16,         ///< signed 16 bits\n    AV_SAMPLE_FMT_S32,         ///< signed 32 bits\n    AV_SAMPLE_FMT_FLT,         ///< float\n    AV_SAMPLE_FMT_DBL,         ///< double\n \n    AV_SAMPLE_FMT_U8P,         ///< unsigned 8 bits, planar\n    AV_SAMPLE_FMT_S16P,        ///< signed 16 bits, planar\n    AV_SAMPLE_FMT_S32P,        ///< signed 32 bits, planar\n    AV_SAMPLE_FMT_FLTP,        ///< float, planar\n    AV_SAMPLE_FMT_DBLP,        ///< double, planar\n \n    AV_SAMPLE_FMT_NB           ///< Number of sample formats. DO NOT USE if linking dynamically\n};\n```\n\n\n\n### 3.profile \n\n在FFMPEG中型有以下几种，可以看出AAC，MPEG2，H.264，VC-1，MPEG4都有型的概念。\n\n\n\n```\n#define FF_PROFILE_UNKNOWN -99\n#define FF_PROFILE_RESERVED -100\n \n#define FF_PROFILE_AAC_MAIN 0\n#define FF_PROFILE_AAC_LOW  1\n#define FF_PROFILE_AAC_SSR  2\n#define FF_PROFILE_AAC_LTP  3\n#define FF_PROFILE_AAC_HE   4\n#define FF_PROFILE_AAC_HE_V2 28\n#define FF_PROFILE_AAC_LD   22\n#define FF_PROFILE_AAC_ELD  38\n \n#define FF_PROFILE_DTS         20\n#define FF_PROFILE_DTS_ES      30\n#define FF_PROFILE_DTS_96_24   40\n#define FF_PROFILE_DTS_HD_HRA  50\n#define FF_PROFILE_DTS_HD_MA   60\n \n#define FF_PROFILE_MPEG2_422    0\n#define FF_PROFILE_MPEG2_HIGH   1\n#define FF_PROFILE_MPEG2_SS     2\n#define FF_PROFILE_MPEG2_SNR_SCALABLE  3\n#define FF_PROFILE_MPEG2_MAIN   4\n#define FF_PROFILE_MPEG2_SIMPLE 5\n \n#define FF_PROFILE_H264_CONSTRAINED  (1<<9)  // 8+1; constraint_set1_flag\n#define FF_PROFILE_H264_INTRA        (1<<11) // 8+3; constraint_set3_flag\n \n#define FF_PROFILE_H264_BASELINE             66\n#define FF_PROFILE_H264_CONSTRAINED_BASELINE (66|FF_PROFILE_H264_CONSTRAINED)\n#define FF_PROFILE_H264_MAIN                 77\n#define FF_PROFILE_H264_EXTENDED             88\n#define FF_PROFILE_H264_HIGH                 100\n#define FF_PROFILE_H264_HIGH_10              110\n#define FF_PROFILE_H264_HIGH_10_INTRA        (110|FF_PROFILE_H264_INTRA)\n#define FF_PROFILE_H264_HIGH_422             122\n#define FF_PROFILE_H264_HIGH_422_INTRA       (122|FF_PROFILE_H264_INTRA)\n#define FF_PROFILE_H264_HIGH_444             144\n#define FF_PROFILE_H264_HIGH_444_PREDICTIVE  244\n#define FF_PROFILE_H264_HIGH_444_INTRA       (244|FF_PROFILE_H264_INTRA)\n#define FF_PROFILE_H264_CAVLC_444            44\n \n#define FF_PROFILE_VC1_SIMPLE   0\n#define FF_PROFILE_VC1_MAIN     1\n#define FF_PROFILE_VC1_COMPLEX  2\n#define FF_PROFILE_VC1_ADVANCED 3\n \n#define FF_PROFILE_MPEG4_SIMPLE                     0\n#define FF_PROFILE_MPEG4_SIMPLE_SCALABLE            1\n#define FF_PROFILE_MPEG4_CORE                       2\n#define FF_PROFILE_MPEG4_MAIN                       3\n#define FF_PROFILE_MPEG4_N_BIT                      4\n#define FF_PROFILE_MPEG4_SCALABLE_TEXTURE           5\n#define FF_PROFILE_MPEG4_SIMPLE_FACE_ANIMATION      6\n#define FF_PROFILE_MPEG4_BASIC_ANIMATED_TEXTURE     7\n#define FF_PROFILE_MPEG4_HYBRID                     8\n#define FF_PROFILE_MPEG4_ADVANCED_REAL_TIME         9\n#define FF_PROFILE_MPEG4_CORE_SCALABLE             10\n#define FF_PROFILE_MPEG4_ADVANCED_CODING           11\n#define FF_PROFILE_MPEG4_ADVANCED_CORE             12\n#define FF_PROFILE_MPEG4_ADVANCED_SCALABLE_TEXTURE 13\n#define FF_PROFILE_MPEG4_SIMPLE_STUDIO             14\n#define FF_PROFILE_MPEG4_ADVANCED_SIMPLE           15\n```\n\n"
        },
        {
          "name": "FFmpeg 结构体学习(四)： AVFrame 分析.md",
          "type": "blob",
          "size": 14.970703125,
          "content": "AVFrame是包含码流参数较多的结构体。下面我们来分析一下该结构体里重要变量的含义和作用。\n\n# 一、源码整理\n\n首先我们先看一下结构体AVFrame的定义的结构体源码(位于libavcodec/avcodec.h)：\n\n\n\n```\n/*\n *雷霄骅\n *leixiaohua1020@126.com\n *中国传媒大学/数字电视技术\n */\n/**\n * Audio Video Frame.\n * New fields can be added to the end of AVFRAME with minor version\n * bumps. Similarly fields that are marked as to be only accessed by\n * av_opt_ptr() can be reordered. This allows 2 forks to add fields\n * without breaking compatibility with each other.\n * Removal, reordering and changes in the remaining cases require\n * a major version bump.\n * sizeof(AVFrame) must not be used outside libavcodec.\n */\ntypedef struct AVFrame {\n#define AV_NUM_DATA_POINTERS 8\n    /**图像数据\n     * pointer to the picture/channel planes.\n     * This might be different from the first allocated byte\n     * - encoding: Set by user\n     * - decoding: set by AVCodecContext.get_buffer()\n     */\n    uint8_t *data[AV_NUM_DATA_POINTERS];\n \n    /**\n     * Size, in bytes, of the data for each picture/channel plane.\n     *\n     * For audio, only linesize[0] may be set. For planar audio, each channel\n     * plane must be the same size.\n     *\n     * - encoding: Set by user\n     * - decoding: set by AVCodecContext.get_buffer()\n     */\n    int linesize[AV_NUM_DATA_POINTERS];\n \n    /**\n     * pointers to the data planes/channels.\n     *\n     * For video, this should simply point to data[].\n     *\n     * For planar audio, each channel has a separate data pointer, and\n     * linesize[0] contains the size of each channel buffer.\n     * For packed audio, there is just one data pointer, and linesize[0]\n     * contains the total size of the buffer for all channels.\n     *\n     * Note: Both data and extended_data will always be set by get_buffer(),\n     * but for planar audio with more channels that can fit in data,\n     * extended_data must be used by the decoder in order to access all\n     * channels.\n     *\n     * encoding: unused\n     * decoding: set by AVCodecContext.get_buffer()\n     */\n    uint8_t **extended_data;\n \n    /**宽高\n     * width and height of the video frame\n     * - encoding: unused\n     * - decoding: Read by user.\n     */\n    int width, height;\n \n    /**\n     * number of audio samples (per channel) described by this frame\n     * - encoding: Set by user\n     * - decoding: Set by libavcodec\n     */\n    int nb_samples;\n \n    /**\n     * format of the frame, -1 if unknown or unset\n     * Values correspond to enum AVPixelFormat for video frames,\n     * enum AVSampleFormat for audio)\n     * - encoding: unused\n     * - decoding: Read by user.\n     */\n    int format;\n \n    /**是否是关键帧\n     * 1 -> keyframe, 0-> not\n     * - encoding: Set by libavcodec.\n     * - decoding: Set by libavcodec.\n     */\n    int key_frame;\n \n    /**帧类型（I,B,P）\n     * Picture type of the frame, see ?_TYPE below.\n     * - encoding: Set by libavcodec. for coded_picture (and set by user for input).\n     * - decoding: Set by libavcodec.\n     */\n    enum AVPictureType pict_type;\n \n    /**\n     * pointer to the first allocated byte of the picture. Can be used in get_buffer/release_buffer.\n     * This isn't used by libavcodec unless the default get/release_buffer() is used.\n     * - encoding:\n     * - decoding:\n     */\n    uint8_t *base[AV_NUM_DATA_POINTERS];\n \n    /**\n     * sample aspect ratio for the video frame, 0/1 if unknown/unspecified\n     * - encoding: unused\n     * - decoding: Read by user.\n     */\n    AVRational sample_aspect_ratio;\n \n    /**\n     * presentation timestamp in time_base units (time when frame should be shown to user)\n     * If AV_NOPTS_VALUE then frame_rate = 1/time_base will be assumed.\n     * - encoding: MUST be set by user.\n     * - decoding: Set by libavcodec.\n     */\n    int64_t pts;\n \n    /**\n     * reordered pts from the last AVPacket that has been input into the decoder\n     * - encoding: unused\n     * - decoding: Read by user.\n     */\n    int64_t pkt_pts;\n \n    /**\n     * dts from the last AVPacket that has been input into the decoder\n     * - encoding: unused\n     * - decoding: Read by user.\n     */\n    int64_t pkt_dts;\n \n    /**\n     * picture number in bitstream order\n     * - encoding: set by\n     * - decoding: Set by libavcodec.\n     */\n    int coded_picture_number;\n    /**\n     * picture number in display order\n     * - encoding: set by\n     * - decoding: Set by libavcodec.\n     */\n    int display_picture_number;\n \n    /**\n     * quality (between 1 (good) and FF_LAMBDA_MAX (bad))\n     * - encoding: Set by libavcodec. for coded_picture (and set by user for input).\n     * - decoding: Set by libavcodec.\n     */\n    int quality;\n \n    /**\n     * is this picture used as reference\n     * The values for this are the same as the MpegEncContext.picture_structure\n     * variable, that is 1->top field, 2->bottom field, 3->frame/both fields.\n     * Set to 4 for delayed, non-reference frames.\n     * - encoding: unused\n     * - decoding: Set by libavcodec. (before get_buffer() call)).\n     */\n    int reference;\n \n    /**QP表\n     * QP table\n     * - encoding: unused\n     * - decoding: Set by libavcodec.\n     */\n    int8_t *qscale_table;\n    /**\n     * QP store stride\n     * - encoding: unused\n     * - decoding: Set by libavcodec.\n     */\n    int qstride;\n \n    /**\n     *\n     */\n    int qscale_type;\n \n    /**跳过宏块表\n     * mbskip_table[mb]>=1 if MB didn't change\n     * stride= mb_width = (width+15)>>4\n     * - encoding: unused\n     * - decoding: Set by libavcodec.\n     */\n    uint8_t *mbskip_table;\n \n    /**运动矢量表\n     * motion vector table\n     * @code\n     * example:\n     * int mv_sample_log2= 4 - motion_subsample_log2;\n     * int mb_width= (width+15)>>4;\n     * int mv_stride= (mb_width << mv_sample_log2) + 1;\n     * motion_val[direction][x + y*mv_stride][0->mv_x, 1->mv_y];\n     * @endcode\n     * - encoding: Set by user.\n     * - decoding: Set by libavcodec.\n     */\n    int16_t (*motion_val[2])[2];\n \n    /**宏块类型表\n     * macroblock type table\n     * mb_type_base + mb_width + 2\n     * - encoding: Set by user.\n     * - decoding: Set by libavcodec.\n     */\n    uint32_t *mb_type;\n \n    /**DCT系数\n     * DCT coefficients\n     * - encoding: unused\n     * - decoding: Set by libavcodec.\n     */\n    short *dct_coeff;\n \n    /**参考帧列表\n     * motion reference frame index\n     * the order in which these are stored can depend on the codec.\n     * - encoding: Set by user.\n     * - decoding: Set by libavcodec.\n     */\n    int8_t *ref_index[2];\n \n    /**\n     * for some private data of the user\n     * - encoding: unused\n     * - decoding: Set by user.\n     */\n    void *opaque;\n \n    /**\n     * error\n     * - encoding: Set by libavcodec. if flags&CODEC_FLAG_PSNR.\n     * - decoding: unused\n     */\n    uint64_t error[AV_NUM_DATA_POINTERS];\n \n    /**\n     * type of the buffer (to keep track of who has to deallocate data[*])\n     * - encoding: Set by the one who allocates it.\n     * - decoding: Set by the one who allocates it.\n     * Note: User allocated (direct rendering) & internal buffers cannot coexist currently.\n     */\n    int type;\n \n    /**\n     * When decoding, this signals how much the picture must be delayed.\n     * extra_delay = repeat_pict / (2*fps)\n     * - encoding: unused\n     * - decoding: Set by libavcodec.\n     */\n    int repeat_pict;\n \n    /**\n     * The content of the picture is interlaced.\n     * - encoding: Set by user.\n     * - decoding: Set by libavcodec. (default 0)\n     */\n    int interlaced_frame;\n \n    /**\n     * If the content is interlaced, is top field displayed first.\n     * - encoding: Set by user.\n     * - decoding: Set by libavcodec.\n     */\n    int top_field_first;\n \n    /**\n     * Tell user application that palette has changed from previous frame.\n     * - encoding: ??? (no palette-enabled encoder yet)\n     * - decoding: Set by libavcodec. (default 0).\n     */\n    int palette_has_changed;\n \n    /**\n     * codec suggestion on buffer type if != 0\n     * - encoding: unused\n     * - decoding: Set by libavcodec. (before get_buffer() call)).\n     */\n    int buffer_hints;\n \n    /**\n     * Pan scan.\n     * - encoding: Set by user.\n     * - decoding: Set by libavcodec.\n     */\n    AVPanScan *pan_scan;\n \n    /**\n     * reordered opaque 64bit (generally an integer or a double precision float\n     * PTS but can be anything).\n     * The user sets AVCodecContext.reordered_opaque to represent the input at\n     * that time,\n     * the decoder reorders values as needed and sets AVFrame.reordered_opaque\n     * to exactly one of the values provided by the user through AVCodecContext.reordered_opaque\n     * @deprecated in favor of pkt_pts\n     * - encoding: unused\n     * - decoding: Read by user.\n     */\n    int64_t reordered_opaque;\n \n    /**\n     * hardware accelerator private data (FFmpeg-allocated)\n     * - encoding: unused\n     * - decoding: Set by libavcodec\n     */\n    void *hwaccel_picture_private;\n \n    /**\n     * the AVCodecContext which ff_thread_get_buffer() was last called on\n     * - encoding: Set by libavcodec.\n     * - decoding: Set by libavcodec.\n     */\n    struct AVCodecContext *owner;\n \n    /**\n     * used by multithreading to store frame-specific info\n     * - encoding: Set by libavcodec.\n     * - decoding: Set by libavcodec.\n     */\n    void *thread_opaque;\n \n    /**\n     * log2 of the size of the block which a single vector in motion_val represents:\n     * (4->16x16, 3->8x8, 2-> 4x4, 1-> 2x2)\n     * - encoding: unused\n     * - decoding: Set by libavcodec.\n     */\n    uint8_t motion_subsample_log2;\n \n    /**（音频）采样率\n     * Sample rate of the audio data.\n     *\n     * - encoding: unused\n     * - decoding: read by user\n     */\n    int sample_rate;\n \n    /**\n     * Channel layout of the audio data.\n     *\n     * - encoding: unused\n     * - decoding: read by user.\n     */\n    uint64_t channel_layout;\n \n    /**\n     * frame timestamp estimated using various heuristics, in stream time base\n     * Code outside libavcodec should access this field using:\n     * av_frame_get_best_effort_timestamp(frame)\n     * - encoding: unused\n     * - decoding: set by libavcodec, read by user.\n     */\n    int64_t best_effort_timestamp;\n \n    /**\n     * reordered pos from the last AVPacket that has been input into the decoder\n     * Code outside libavcodec should access this field using:\n     * av_frame_get_pkt_pos(frame)\n     * - encoding: unused\n     * - decoding: Read by user.\n     */\n    int64_t pkt_pos;\n \n    /**\n     * duration of the corresponding packet, expressed in\n     * AVStream->time_base units, 0 if unknown.\n     * Code outside libavcodec should access this field using:\n     * av_frame_get_pkt_duration(frame)\n     * - encoding: unused\n     * - decoding: Read by user.\n     */\n    int64_t pkt_duration;\n \n    /**\n     * metadata.\n     * Code outside libavcodec should access this field using:\n     * av_frame_get_metadata(frame)\n     * - encoding: Set by user.\n     * - decoding: Set by libavcodec.\n     */\n    AVDictionary *metadata;\n \n    /**\n     * decode error flags of the frame, set to a combination of\n     * FF_DECODE_ERROR_xxx flags if the decoder produced a frame, but there\n     * were errors during the decoding.\n     * Code outside libavcodec should access this field using:\n     * av_frame_get_decode_error_flags(frame)\n     * - encoding: unused\n     * - decoding: set by libavcodec, read by user.\n     */\n    int decode_error_flags;\n#define FF_DECODE_ERROR_INVALID_BITSTREAM   1\n#define FF_DECODE_ERROR_MISSING_REFERENCE   2\n \n    /**\n     * number of audio channels, only used for audio.\n     * Code outside libavcodec should access this field using:\n     * av_frame_get_channels(frame)\n     * - encoding: unused\n     * - decoding: Read by user.\n     */\n    int64_t channels;\n} AVFrame;\n```\n\n\n\n# 二、AVFrame 重点字段\n\nAVFrame结构体一般用于存储原始数据（即非压缩数据，例如对视频来说是YUV，RGB，对音频来说是PCM），此外还包含了一些相关的信息。比如说，解码的时候存储了宏块类型表，QP表，运动矢量表等数据。编码的时候也存储了相关的数据。因此在使用FFMPEG进行码流分析的时候，AVFrame是一个很重要的结构体。\n\n下面看几个主要变量的作用（在这里考虑解码的情况）：\n\n\n\n```\nuint8_t *data[AV_NUM_DATA_POINTERS]：解码后原始数据（对视频来说是YUV，RGB，对音频来说是PCM）\n\nint linesize[AV_NUM_DATA_POINTERS]：data中“一行”数据的大小。注意：未必等于图像的宽，一般大于图像的宽。\n\nint width, height：视频帧宽和高（1920x1080,1280x720...）\n\nint nb_samples：音频的一个AVFrame中可能包含多个音频帧，在此标记包含了几个\n\nint format：解码后原始数据类型（YUV420，YUV422，RGB24...）\n\nint key_frame：是否是关键帧\n\nenum AVPictureType pict_type：帧类型（I,B,P...）\n\nAVRational sample_aspect_ratio：宽高比（16:9，4:3...）\n\nint64_t pts：显示时间戳\n\nint coded_picture_number：编码帧序号\n\nint display_picture_number：显示帧序号\n\nint8_t *qscale_table：QP表\n\nuint8_t *mbskip_table：跳过宏块表\n\nint16_t (*motion_val[2])[2]：运动矢量表\n\nuint32_t *mb_type：宏块类型表\n\nshort *dct_coeff：DCT系数，这个没有提取过\n\nint8_t *ref_index[2]：运动估计参考帧列表（貌似H.264这种比较新的标准才会涉及到多参考帧）\n\nint interlaced_frame：是否是隔行扫描\n\nuint8_t motion_subsample_log2：一个宏块中的运动矢量采样个数，取log的\n```\n\n\n\n其他的变量不再一一列举，源代码中都有详细的说明。在这里重点分析一下几个需要一定的理解的变量：\n\n### 1.data[]\n\n对于packed格式的数据（例如RGB24），会存到data[0]里面。\n\n对于planar格式的数据（例如YUV420P），则会分开成data[0]，data[1]，data[2]...（YUV420P中data[0]存Y，data[1]存U，data[2]存V）\n\n### 2.pict_type\n\n包含以下类型：\n\n\n\n```\nenum AVPictureType {\n    AV_PICTURE_TYPE_NONE = 0, ///< Undefined\n    AV_PICTURE_TYPE_I,     ///< Intra\n    AV_PICTURE_TYPE_P,     ///< Predicted\n    AV_PICTURE_TYPE_B,     ///< Bi-dir predicted\n    AV_PICTURE_TYPE_S,     ///< S(GMC)-VOP MPEG4\n    AV_PICTURE_TYPE_SI,    ///< Switching Intra\n    AV_PICTURE_TYPE_SP,    ///< Switching Predicted\n    AV_PICTURE_TYPE_BI,    ///< BI type\n};\n```\n\n\n\n### 3.sample_aspect_ratio\n\n宽高比是一个分数，FFMPEG中用AVRational表达分数：\n\n\n\n```\n/**\n * rational number numerator/denominator\n */\ntypedef struct AVRational{\n    int num; ///< numerator\n    int den; ///< denominator\n} AVRational;\n```\n\n\n\n### 4.qscale_table\n\nQP表指向一块内存，里面存储的是每个宏块的QP值。宏块的标号是从左往右，一行一行的来的。每个宏块对应1个QP。\n\nqscale_table[0]就是第1行第1列宏块的QP值；qscale_table[1]就是第1行第2列宏块的QP值；qscale_table[2]就是第1行第3列宏块的QP值。以此类推...\n\n宏块的个数用下式计算：\n\n注：宏块大小是16x16的。\n\n每行宏块数：\n\n```\nint mb_stride = pCodecCtx->width/16+1\n```\n\n宏块的总数：\n\n```\nint mb_sum = ((pCodecCtx->height+15)>>4)*(pCodecCtx->width/16+1)\n```\n\n"
        },
        {
          "name": "FFmpeg在Android多媒体平台下的编码优化研究.pdf",
          "type": "blob",
          "size": 22670.375,
          "content": ""
        },
        {
          "name": "FFmpeg源代码结构图 - 编码.jpg",
          "type": "blob",
          "size": 1007.6298828125,
          "content": null
        },
        {
          "name": "FFmpeg源码分析：内存管理系统.md",
          "type": "blob",
          "size": 8.5625,
          "content": "FFmpeg有专门的内存管理系统，包括：内存分配、内存拷贝、内存释放。其中内存分配包含分配内存与对齐、内存分配与清零、分配指定大小的内存块、重新分配内存块、快速分配内存、分配指定最大值的内存、分配数组内存、快速分配数组内存、重新分配数组内存。\n\nFFmpeg的内存管理位于libavutil/mem.c，相关函数如下图所示：\n\n![img](https://img-blog.csdnimg.cn/ec570c9e0f8f49fdb21ec2819acee3d9.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5b6Q56aP6K6wNDU2,size_20,color_FFFFFF,t_70,g_se,x_16)\n\n一、内存分配\n1、av_malloc\nav_malloc()内存分配，并且内存对齐，方便系统快速访问内存。代码如下：\n\n```c\nvoid *av_malloc(size_t size)\n{\n    void *ptr = NULL;\nif (size > max_alloc_size)\n    return NULL;\n    #if HAVE_POSIX_MEMALIGN\n    if (size)\n    if (posix_memalign(&ptr, ALIGN, size))\n        ptr = NULL;\n#elif HAVE_ALIGNED_MALLOC\n    ptr = _aligned_malloc(size, ALIGN);\n#elif HAVE_MEMALIGN\n#ifndef __DJGPP__\n    ptr = memalign(ALIGN, size);\n#else\n    ptr = memalign(size, ALIGN);\n#endif\n    /* Why 64?\n     * Indeed, we should align it:\n     *   on  4 for 386\n     *   on 16 for 486\n     *   on 32 for 586, PPro - K6-III\n     *   on 64 for K7 (maybe for P3 too).\n     * Because L1 and L2 caches are aligned on those values.\n     * But I don't want to code such logic here!\n     */\n    /* Why 32?\n     * For AVX ASM. SSE / NEON needs only 16.\n     * Why not larger? Because I did not see a difference in benchmarks ...\n     */\n    /* benchmarks with P3\n     * memalign(64) + 1          3071, 3051, 3032\n     * memalign(64) + 2          3051, 3032, 3041\n     * memalign(64) + 4          2911, 2896, 2915\n     * memalign(64) + 8          2545, 2554, 2550\n     * memalign(64) + 16         2543, 2572, 2563\n     * memalign(64) + 32         2546, 2545, 2571\n     * memalign(64) + 64         2570, 2533, 2558\n     *\n     * BTW, malloc seems to do 8-byte alignment by default here.\n     */\n#else\n    ptr = malloc(size);\n#endif\n    if(!ptr && !size) {\n        size = 1;\n        ptr= av_malloc(1);\n    }\n#if CONFIG_MEMORY_POISONING\n    if (ptr)\n        memset(ptr, FF_MEMORY_POISON, size);\n#endif\n    return ptr;\n}2、av_mallocz\nav_mallocz()是在av_malloc()基础上，调用memset()进行内存清零：\n```\n\n```c\nvoid *av_mallocz(size_t size)\n{\n    void *ptr = av_malloc(size);\n    if (ptr)\n        memset(ptr, 0, size);\n    return ptr;\n}\n```\n\n3、av_malloc_array\nav_malloc_array()先计算数组所需要内存块大小，然后用av_malloc()分配数组内存：\n\n```c\nvoid *av_malloc_array(size_t nmemb, size_t size)\n{\n    size_t result;\n    if (av_size_mult(nmemb, size, &result) < 0)\n        return NULL;\n    return av_malloc(result);\n}\n```\n\n4、av_mallocz_array\nav_mallocz_array()先计算数组所需要内存块大小，然后用av_mallocz()分配数组内存：\n\n```c\nvoid *av_mallocz_array(size_t nmemb, size_t size)\n{\n    size_t result;\n    if (av_size_mult(nmemb, size, &result) < 0)\n        return NULL;\n    return av_mallocz(result);\n}\n```\n\n5、av_calloc\nav_calloc()操作与av_mallocz_array()，先计算内存大小再用av_mallocz()分配内存：\n\n```c\nvoid *av_calloc(size_t nmemb, size_t size)\n{\n    size_t result;\n    if (av_size_mult(nmemb, size, &result) < 0)\n        return NULL;\n    return av_mallocz(result);\n}\n```\n\n6、av_max_alloc\nav_max_alloc()主要是指定分配内存的最大值：\n\n```c\nstatic size_t max_alloc_size= INT_MAX;\n\nvoid av_max_alloc(size_t max)\n{\n    max_alloc_size = max;\n}\n```\n\n在av_malloc()用于判断size是否超出最大值：\n\n```c\nvoid *av_malloc(size_t size)\n{\n    void *ptr = NULL;\nif (size > max_alloc_size)\n    return NULL;\n \n......\n}7、av_realloc\nav_realloc()是对系统的realloc函数进行封装，重新分配内存块：\n```\n\n```c\nvoid *av_realloc(void *ptr, size_t size)\n{\n    if (size > max_alloc_size)\n        return NULL;\n\n#if HAVE_ALIGNED_MALLOC\n    return _aligned_realloc(ptr, size + !size, ALIGN);\n#else\n    return realloc(ptr, size + !size);\n#endif\n}\n```\n\n8、av_realloc_array\nav_realloc_array()先计算内存块大小，然后用av_realloc()重新分配数组内存：\n\n```c\nvoid *av_realloc_array(void *ptr, size_t nmemb, size_t size)\n{\n    size_t result;\n    if (av_size_mult(nmemb, size, &result) < 0)\n        return NULL;\n    return av_realloc(ptr, result);\n}\n```\n\n9、av_fast_realloc\nav_fast_realloc()快速重新分配内存，如果原始内存块足够大直接复用：\n\n```c\nvoid *av_fast_realloc(void *ptr, unsigned int *size, size_t min_size)\n{\n    if (min_size <= *size)\n        return ptr;\nif (min_size > max_alloc_size) {\n    *size = 0;\n    return NULL;\n}\n \nmin_size = FFMIN(max_alloc_size, FFMAX(min_size + min_size / 16 + 32, min_size));\n \nptr = av_realloc(ptr, min_size);\n/* we could set this to the unmodified min_size but this is safer\n * if the user lost the ptr and uses NULL now\n */\nif (!ptr)\n    min_size = 0;\n \n*size = min_size;\n \nreturn ptr;\n}\n```\n\n\n\n10、av_fast_malloc\nav_fast_malloc()快速分配内存：\n\n```c\nvoid av_fast_malloc(void *ptr, unsigned int *size, size_t min_size)\n{\n    ff_fast_malloc(ptr, size, min_size, 0);\n}\n```\n\n其中ff_fast_malloc()代码位于libavutil/mem_internal.h:\n\n\n\n```c\nstatic inline int ff_fast_malloc(void *ptr, unsigned int *size, size_t min_size, int zero_realloc)\n{\n    void *val;\nmemcpy(&val, ptr, sizeof(val));\nif (min_size <= *size) {\n    av_assert0(val || !min_size);\n    return 0;\n}\nmin_size = FFMAX(min_size + min_size / 16 + 32, min_size);\nav_freep(ptr);\nval = zero_realloc ? av_mallocz(min_size) : av_malloc(min_size);\nmemcpy(ptr, &val, sizeof(val));\nif (!val)\n    min_size = 0;\n*size = min_size;\nreturn 1;\n}\n```\n\n\n\n11、av_fast_mallocz\nav_fast_mallocz()快速分配内存，并且内存清零：\n\n```c\nvoid av_fast_mallocz(void *ptr, unsigned int *size, size_t min_size)\n{\n    ff_fast_malloc(ptr, size, min_size, 1);\n}\n```\n\n二、内存拷贝\n1、av_strdup\nav_strdup()用于重新分配内存与拷贝字符串：\n\n```c\nchar *av_strdup(const char *s)\n{\n    char *ptr = NULL;\n    if (s) {\n        size_t len = strlen(s) + 1;\n        ptr = av_realloc(NULL, len);\n        if (ptr)\n            memcpy(ptr, s, len);\n    }\n    return ptr;\n}\n```\n\n2、av_strndup\nav_strndup()用于分配指定大小内存与拷贝字符串，先用memchr()获取有效字符串长度，然后使用av_realloc()重新分配内存，再用memcpy()拷贝字符串：\n\n```c\nchar *av_strndup(const char *s, size_t len)\n{\n    char *ret = NULL, *end;\nif (!s)\n    return NULL;\n \nend = memchr(s, 0, len);\nif (end)\n    len = end - s;\n \nret = av_realloc(NULL, len + 1);\nif (!ret)\n    return NULL;\n \nmemcpy(ret, s, len);\nret[len] = 0;\nreturn ret;\n}\n```\n\n\n\n3、av_memdup\nav_memdup()用于内存分配与内存拷贝，先用av_malloc()分配内存，再用memcpy()拷贝内存：\n\n```c\nvoid *av_memdup(const void *p, size_t size)\n{\n    void *ptr = NULL;\n    if (p) {\n        ptr = av_malloc(size);\n        if (ptr)\n            memcpy(ptr, p, size);\n    }\n    return ptr;\n}\n```\n\n4、av_memcpy_backptr\nav_memcpy_backptr()用于内存拷贝，与系统提供的memcpy()类似，并且考虑16位、24位、32位内存对齐：\n\n```c\nvoid av_memcpy_backptr(uint8_t *dst, int back, int cnt)\n{\n    const uint8_t *src = &dst[-back];\n    if (!back)\n        return;\nif (back == 1) {\n    memset(dst, *src, cnt);\n} else if (back == 2) {\n    fill16(dst, cnt);\n} else if (back == 3) {\n    fill24(dst, cnt);\n} else if (back == 4) {\n    fill32(dst, cnt);\n} else {\n    if (cnt >= 16) {\n        int blocklen = back;\n        while (cnt > blocklen) {\n            memcpy(dst, src, blocklen);\n            dst       += blocklen;\n            cnt       -= blocklen;\n            blocklen <<= 1;\n        }\n        memcpy(dst, src, cnt);\n        return;\n    }\n    if (cnt >= 8) {\n        AV_COPY32U(dst,     src);\n        AV_COPY32U(dst + 4, src + 4);\n        src += 8;\n        dst += 8;\n        cnt -= 8;\n    }\n    if (cnt >= 4) {\n        AV_COPY32U(dst, src);\n        src += 4;\n        dst += 4;\n        cnt -= 4;\n    }\n    if (cnt >= 2) {\n        AV_COPY16U(dst, src);\n        src += 2;\n        dst += 2;\n        cnt -= 2;\n    }\n    if (cnt)\n        *dst = *src;\n}\n```\n\n}\n三、内存释放\n1、av_free\nav_free()用于释放内存块，主要是调用系统free()进行释放。如果宏定义了对齐分配，那么要对齐释放：\n\n```c\nvoid av_free(void *ptr)\n{\n#if HAVE_ALIGNED_MALLOC\n    _aligned_free(ptr);\n#else\n    free(ptr);\n#endif\n}\n```\n\n2、av_freep\nav_freep()用于释放内存指针，先备份内存指针，然后把指针地址清空，再释放内存：\n\n```c\nvoid av_freep(void *arg)\n{\n    void *val;\nmemcpy(&val, arg, sizeof(val));\nmemcpy(arg, &(void *){ NULL }, sizeof(val));\nav_free(val);\n}\n```\n"
        },
        {
          "name": "H.264视频解码优化及DSP实现.caj",
          "type": "blob",
          "size": 1999.578125,
          "content": null
        },
        {
          "name": "Linux上的ffmpeg完全使用指南.md",
          "type": "blob",
          "size": 21.66015625,
          "content": "**[ffmpeg](https://ffmpeg.org/)** 是一个处理媒体文件的命令行工具 (command line based) 。它是一个拥有非常多功能的框架，并且因为他是开源的，很多知名的工具如 VLC，YouTube， iTunes 等等，都是再其之上开发出来的。\n\nffmpeg最吸引我的地方就是它可以用非常简练的方式（通过一两个命令）完成许多的处理任务，当然，作为一个强大的工具，他也有很多较为复杂的使用方式，有些时候甚至可以代替一个完整的视频处理流程。\n\n在这个ffmpeg教程中，我会告诉你如何安装ffmpeg，以及各种使用方法，也会讲解一些复杂的功能。\n\n我会详细的说明各个方面，这样即便你是linux新手也能明白。\n\n我使用的linux是 **Ubuntu 18.04**， 不过下面的命令应该可以在其他的linux发行版中同样适用。\n\n![img](https://eyehere.net/wp-content/uploads/2019/04/image.png)\n\n## 在 Ubuntu 和其他 Linux 系统上安装 ffmpeg\n\n安装 **ffmpeg** 是非常容易的，它是个很流行的程序，所以大多数的linux发行版中您都可以通过包管理器直接安装。\n\n### 在 Ubuntu 上安装 ffmpeg\n\n在 Ubuntu 上，ffmpeg 存在于 “Universe repository”， 所以确保您开启了enable universe repository，然后更新并安装ffmpeg。下面就是您可能需要的命令。\n\n```c\nsudo add-apt-repository universe\nsudo apt update\nsudo apt install ffmpeg\n```\n\n这就OK了，您可以通过下面的命令尝试一下有没有正确安装：\n\n```c\nffmpeg\n```\n\n他会打印出一些ffmpeg的配置和版本信息。\n\n![img](https://eyehere.net/wp-content/uploads/2019/04/image-1.png)\n\n正如上图所示，安装的版本是 3.4.4。不过ffmpeg的最新版本应该是4.1。为了安装4.x的版本，您需要使用ffmpeg ppa， 您可以自己研究一下……\n\n### 在 Arch 系的Linux上安装 ffmpeg\n\n这个也非常简单，用下面的命令就行：\n\n```c\nsudo pacman -S ffmpeg\n```\n\n### 在 Fedora 系的Linux上安装 ffmpeg\n\n使用下面的命令就好了：\n\n```c\nsudo dnf install ffmpeg\n```\n\n## 如何使用 ffmpeg: 基础\n\n**ffmpeg** 安装就绪了，我来讲述一些使用这个强力工具的基本概念。\n\n### 0. ffmpeg 命令\n\n使用 **ffmpeg 命令** 的**基本形式**是:\n\n```c\nffmpeg [全局参数] {[输入文件参数] -i 输入文件地址} ... {[输出文件参数] 输出文件地址} ...\n```\n\n要注意的是，所有的参数仅仅对仅接下来的文件有效（下一个文件得把参数再写一遍）。\n\n所有没有使用 **-i** 指定的文件都被认为是输出文件。 **Ffmpeg** 可以接受多个输入文件并输出到您指定的位置。你也可以将输入输出都指定为同一个文件名，不过这个时候要在输出文件前使用用 **-y** 标记。\n\nNote\n\n*你不应该将输入和输出混淆，先指定输入，再指定输出文件*\n\n### 1. 获得媒体文件的信息\n\n**ffmpeg** 最简单的使用就是用来 **显示文件信息** 。不用给输出，只是简单的写：\n\n```c\nffmpeg -i file_name\n```\n\n视频和音频文件都可以使用：\n\n```c\nffmpeg -i video_file.mp4 \nffmpeg -i audio_file.mp3\n```\n\n![Display information about a media file with ffmpeg](https://eyehere.net/wp-content/uploads/2019/04/image-2.png)\n\n通过ffmpeg查看文件属性\n\n命令会输出很多与您文件无关的信息（ffmpeg本身的信息），虽说这个蛮有用的，你可以使用 **-hide_banner** 来隐藏掉它们:\n\n```c\nffmpeg -i video_file.mp4 -hide_banner \nffmpeg -i audio_file.mp3 -hide_banner\n```\n\n![img](https://eyehere.net/wp-content/uploads/2019/04/image-3.png)\n\n如图所示，现在命令只显示你文件相关的信息了（编码器，数据流等）。\n\n### 2. 转换媒体文件\n\n**ffmpeg** 最让人称道常用的恐怕就是你轻而易举的在不同媒体格式之间进行自由转换了。你是要指明输入和输出文件名就行了， **ffmpeg** 会从后缀名猜测格式，这个方法同时适用于视频和音频文件\n\n下面是一些例子:\n\n```c\nffmpeg -i video_input.mp4 video_output.avi \nffmpeg -i video_input.webm video_output.flv \nffmpeg -i audio_input.mp3 audio_output.ogg \nffmpeg -i audio_input.wav audio_output.flac\n```\n\n你也可以同时指定多个输出后缀：\n\n```c\nffmpeg -i audio_input.wav audio_output_1.mp3 audio_output_2.ogg\n```\n\n这样会同时输出多个文件.\n\n想看支持的格式，可以用：\n\n```c\nffmpeg -formats\n```\n\n同样的，你可以使用 **-hide_banner** 来省略一些程序信息。\n\n你可以在输出文件前使用 **-qscale 0** 来保留原始的视频质量：\n\n```c\nffmpeg -i video_input.wav -qscale 0 video_output.mp4\n```\n\n进一步，你可以指定编码器，使用 **-c:a** (音频) 和 **g-c:v** (视频) 来指定编码器名称，或者写 **copy** 来使用与源文件相同的编码器：\n\n```c\nffmpeg -i video_input.mp4 -c:v copy -c:a libvorbis video_output.avi\n```\n\n**Note:** *这样做会让文件后缀使人困惑，所以请避免这么做。*\n\n### 3. 从视频中抽取音频\n\n为了从视频文件中抽取音频，直接加一个 **-vn** 参数就可以了：\n\n```c\nffmpeg -i video.mp4 -vn audio.mp3\n```\n\n这会让命令复用原有文件的比特率，一般来说，使用 **-ab** (音频比特率)来指定编码比特率是比较好的：\n\n```c\nffmpeg -i video.mp4 -vn -ab 128k audio.mp3\n```\n\n一些常见的比特率有 96k, 128k, 192k, 256k, 320k (mp3也可以使用最高的比特率)。\n\n其他的一些常用的参数比如 **-ar** **(采样率**: 22050, 441000, 48000), **-ac** (**声道数**), **-f** (**音频格式**, 通常会自动识别的). **-ab** 也可以使用 **-b:a** 来替代. 比如：\n\n```c\nffmpeg -i video.mov -vn -ar 44100 -ac 2 -b:a 128k -f mp3 audio.mp3\n```\n\n### 4. 让视频静音\n\n和之前的要求类似，我们可以使用 **-an** 来获得纯视频(之前是 **-vn**).\n\n```c\nffmpeg -i video_input.mp4 -an -video_output.mp4\n```\n\n**Note:** *这个 **-an** 标记会让所有的音频参数无效，因为最后没有音频会产生。*\n\n### 5. 从视频中提取图片\n\n这个功能可能对很多人都挺有用，比如你可能有一些幻灯片，你想从里面提取所有的图片，那么下面这个命令就能帮你：\n\n```c\nffmpeg -i video.mp4 -r 1 -f image2 image-%3d.png\n```\n\n我们来解释一下这个命令：\n\n**-r** 代表了帧率（一秒内导出多少张图像，默认25）， **-f** 代表了输出格式(**image2** 实际上上 image2 序列的意思）。\n\n最后一个参数 (输出文件) 有一个有趣的命名：它使用 **%3d** 来指示输出的图片有三位数字 (000, 001, 等等.)。你也可以用 **%2d** (两位数字) 或者 **%4d** (4位数字) ，只要你愿意，你可以随便实验 一下可以怎么写！\n\n**Note:** *同样也有将图片转变为视频/幻灯片的方式，下面的**高级应用**中会讲到。*\n\n### 6. 更改视频分辨率或长宽比\n\n对 **ffmpeg** 来说又是个简单的任务，你只需要使用 **-s** 参数来缩放视频就行了:\n\n```c\nffmpeg -i video_input.mov -s 1024x576 video_output.mp4\n```\n\n同时，你可能需要使用 **-c:a** 来保证音频编码是正确的:\n\n```c\nffmpeg -i video_input.h264 -s 640x480 -c:a video_output.mov\n```\n\n你也可是使用**-aspect** 来更改长宽比:\n\n```c\nffmpeg -i video_input.mp4 -aspect 4:3 video_output.mp4\n```\n\n**Note:** 在高级应用中还会提到更强大的方法\n\n### 7. 为音频增加封面图片\n\n有个很棒的方法把音频变成视频，全程使用一张图片（比如专辑封面）。当你想往某个网站上传音频，但那个网站又仅接受视频（比如YouTube, Facebook等）的情况下会非常有用。\n\n下面是例子：\n\n```c\nffmpeg -loop 1 -i image.jpg -i audio.wav -c:v libx264 -c:a aac -strict experimental -b:a 192k -shortest output.mp4\n```\n\n只要改一下编码设置 (**-c:v** 是 视频编码， **-c:a** 是音频编码) 和文件的名称就能用了。\n\n**Note:** *如果你使用一个较新的ffmpeg版本（4.x），你就可以不指定 **-strict experimental***\n\n### 8. 为视频增加字幕\n\n另一个常见又很容易实现的要求是给视频增加字母，比如一部外文电源，使用下面的命令：\n\n```c\nffmpeg -i video.mp4 -i subtitles.srt -c:v copy -c:a copy -preset veryfast -c:s mov_text -map 0 -map 1 output.mp4\n```\n\n当然，你可以指定自己的编码器和任何其他的音频视频参数。\n\n### 9. 压缩媒体文件\n\n压缩文件可以极大减少文件的体积，节约存储空间，这对于文件传输尤为重要。通过ffmepg，有好几个方法来压缩文件体积。\n\n**Note:** 文件压缩的太厉害会让文件质量显著降低。\n\n首先，对于音频文件，可以通过降低比特率(使用 **-b:a** 或 **-ab**):\n\n```c\nffmpeg -i audio_input.mp3 -ab 128k audio_output.mp3\nffmpeg -i audio_input.mp3 -b:a 192k audio_output.mp3\n```\n\n再次重申，一些常用的比特率有: 96k, 112k, 128k, 160k, 192k, 256k, 320k.值越大，文件所需要的体积就越大。\n\n对于视频文件，选项就多了，一个简单的方法是通过降低视频比特率 (通过 **-b:v**):\n\n```c\nffmpeg -i video_input.mp4 -b:v 1000k -bufsize 1000k video_output.mp4\n```\n\n**Note:** 视频的比特率和音频是不同的（一般要大得多）。\n\n你也可以使用 **-crf** 参数 (恒定质量因子). 较小的**crf** 意味着较大的码率。同时使用 **libx264** 编码器也有助于减小文件体积。这里有个例子，压缩的不错，质量也不会显著变化：\n\n```c\nffmpeg -i video_input.mp4 -c:v libx264 -crf 28 video_output.mp4\n```\n\n**crf** 设置为20 到 30 是最常见的，不过您也可以尝试一些其他的值。\n\n降低帧率在有些情况下也能有效（不过这往往让视频看起来很卡）:\n\n```c\nffmpeg -i video_input.mp4 -r 24 video_output.mp4\n```\n\n**-r** 指示了帧率 (这里是 **24**)。\n\n你还可以通过压缩音频来降低视频文件的体积，比如设置为立体声或者降低比特率：\n\n```c\nffmpeg -i video_input.mp4 -c:v libx264 -ac 2 -c:a aac -strict -2 -b:a 128k -crf 28 video_output.mp4\n```\n\n**Note:** ***-strict -2** 和 **-ac 2** 是来处理立体声部分的。*\n\n### 10. 裁剪媒体文件（基础）\n\n想要从开头开始剪辑一部分，使用T **-t** 参数来指定一个时间:\n\n```c\nffmpeg -i input_video.mp4 -t 5 output_video.mp4 \nffmpeg -i input_audio.wav -t 00:00:05 output_audio.wav\n```\n\n这个参数对音频和视频都适用，上面两个命令做了类似的事情：保存一段5s的输出文件（文件开头开始算）。上面使用了两种不同的表示时间的方式，一个单纯的数字（描述）或者 **HH:MM:SS** (小时, 分钟, 秒). 第二种方式实际上指示了结束时间。\n\n也可以通过 **-ss** 给出一个开始时间，**-to** 给出结束时间：\n\n```c\nffmpeg -i input_audio.mp3 -ss 00:01:14 output_audio.mp3\nffmpeg -i input_audio.wav -ss 00:00:30 -t 10 output_audio.wav \nffmpeg -i input_video.h264 -ss 00:01:30 -to 00:01:40 output_video.h264 \nffmpeg -i input_audio.ogg -ss 5 output_audio.ogg\n```\n\n可以看到 **开始时间** (**-ss HH:MM:SS**), **持续秒数** (**-t duration**), **结束时间** (**-to HH:MM:SS**), 和**开始秒数** (**-s duration**)的用法.\n\n你可以在媒体文件的任何部分使用这些命令。\n\n## ffmpeg: 高级使用\n\n现在该开始讲述一些高级的特性了（比如截屏等），让我们开始吧。\n\n### 1. 分割媒体文件\n\n前面已经讲述了如何裁剪文件，那么如何分割媒体文件呢？只需要为每个输出文件分别指定开始时间、结束或者持续时间就可以了。\n\n看下面这个例子：\n\n```c\nffmpeg -i video.mp4 -t 00:00:30 video_1.mp4 -ss 00:00:30 video_2.mp4\n```\n\n语法很简单，为第一个文件指定了 **-t 00:00:30** 作为持续时间（第一个部分是原始文件的前30秒内容），然后指定接下来的所有内容作为第二个文件（从第一部分的结束时间开始，也就是 **00:00:30**)。\n\n你可以任意指定多少个部分，尝试一下吧，这个功能真的很厉害，同时它也适用用音频文件。\n\n### 2. 拼接媒体文件\n\n**ffmpeg** 也可以进行相反的动作：把多个文件合在一起。\n\n为了实现这一点，你得用自己顺手的编辑器来创建一个文本文件。\n\n因为我喜欢使用终端，所以这里我用了 **touch** 和 **vim**. 文件名无关紧要，这里我用 **touch** 命令创建 **video_to_join.txt** 文件:\n\n```c\ntouch videos_to_join.txt\n```\n\n现在，使用 **vim** 编辑它:\n\n```c\nvim videos_to_join.txt\n```\n\n你可以使用任何你喜欢的工具，比如nano，gedit等等。\n\n在文件内容中, 输入您想拼接的文件的完整路径（文件会按照顺序拼合在一起），一行一个文件。确保他们拥有相同的后缀名。下面是我的例子：\n\n```c\n/home/ubuntu/Desktop/video_1.mp4\n/home/ubuntu/Desktop/video_2.mp4\n/home/ubuntu/Desktop/video_3.mp4\n```\n\n保存这个文件，同样这个方法适用与任何音频或者视频文件。\n\n然后使用下面的命令：\n\n```c\nffmpeg -f concat -i join.txt output.mp4\n```\n\n**Note:** *使用的输出文件的名称是 **output.mp4**, 因为我的输入文件都是mp4的 。*\n\n这样，你 **videos_to_join.txt** 里的所有文件都会被拼接成一个独立的文件了。\n\n### 3. 将图片转变为视频\n\n这会告诉你如何将图片变成幻灯片秀，同时也会告诉你如何加上音频。\n\n首先我建议您将所有的图片放到一个文件夹下面，我把它们放到了 **my_photos** 里，同时图片的后缀名最好是 **.png** 或者 **.jpg**， 不管选那个，他们应该是同一个后缀名，否则ffmpeg可能会工作的不正常，您可以很方便的把 .png 转变为 .jpg （或者倒过来也行）。\n\n我们这次转换的格式 (**-f**) 应该被设置为 **image2pipe**. 你必须使用使用连词符(**–**)来指明输入。 **image2pipe** 允许你使用管道 (在命令间使用 **|**)的结果而不是文件作为ffmpeg的输入。命令结果便是将所有图片的内容逐个输出，还要注意指明视频编码器是 copy (**-c:v copy**) 以正确使用图片输入：\n\n```c\ncat my_photos/* | ffmpeg -f image2pipe -i - -c:v copy video.mkv\n```\n\n如果你播放这个文件，你可能会觉得只有一部分图片被加入了，事实上所有的图片都在，但是**ffmpeg** 播放它们的时候太快了，默认是23fps，一秒播放了23张图片。\n\n你应该指定帧率 (**-framerate**) :\n\n```c\ncat my_photos/* | ffmpeg -framerate 1 -f image2pipe -i - -c:v copy video.mkv \n```\n\n在这个例子里，把帧率设置为1，也就是每帧（每张图）会显示1秒。\n\n为了加一些声音，可以使用音频文件作为输入 (**-i audo_file**) 并且设定copy音频编码 (**-c:a copy**). 你可以同时为音频和视频设定编码器，在输出文件前设置就可以了。你要计算一下音频文件的长度和图片张数，已确定合适的帧率。比如我的音频文件是22秒，图片有9张，那么帧率应该是 9 / 22 大约0.4，所以我这么输入命令：\n\n```c\ncat my_photos/* | ffmpeg -framerate 0.40 -f image2pipe -i - -i audio.wav -c copy video.mkv\n```\n\n### 4. 录制屏幕\n\n通过 **ffmpeg** 录制屏幕同样没有困难的，将格式(**-f**) 设定为**x11grab**. 他就会抓取你的**XSERVER**. 输入的话可以这是屏幕编号（一般都是**0:0**). 抓取是从左上角开始计算的，可以指定屏幕分辨率 (**-s**). 我的屏幕是 **1920×1080**. 注意屏幕分辨率硬在输入之前指定**t**:\n\n```c\nffmpeg -f x11grab -s 1920x1080 -i :0.0 output.mp4\n```\n\n按 **q** 或者 **CTRL+C** 以结束录制屏幕。\n\n**小技巧:**你可以通过命令获得真实的分辨率而不是写死一个固定的大小**：**\n\n```c\n-s $(xdpyinfo | grep dimensions | awk '{print $2;}')\n```\n\n完整的命令这么写:\n\n```c\nffmpeg -f x11grab -s $(xdpyinfo | grep dimensions | awk '{print $2;}') -i :0.0 output.mp4\n```\n\n### 5. 录制摄像头\n\n从摄像头录制就更简单了，linux上设备都是在/dev中的，比如 **/dev/video0, /dev/video1, etc.**:\n\n```c\nffmpeg -i /dev/video0 output.mkv\n```\n\n同样, **q** 或者 **CTRL+C** 来结束录制。\n\n### 6. 录制声音\n\nLinux上同时是使用 **ALSA** 和 **pulseaudio** 来处理声音的。 **ffmpeg** 可以录制两者，不过我要特别说明 **pulseaudio**, 因为 Debian 系列的发行版默认用了它。命令如下：\n\n在 **pulseaudio**, 你必须强制指定(**-f**) **alsa** 然后指定 **default** 作为输入**t** (**-i default**):\n\n```c\nffmpeg -f alsa -i default output.mp3\n```\n\n**Note:** *在你系统音频设置里，应该能看到默认的录音设备。*\n\n我经常玩吉他，我平时使用一个专业音频设备才能录制声音，当我发现ffmpeg也可以很轻松的录制的时候颇为惊讶。\n\n### 录制小贴士\n\n对于录制任务来说，通常都需要指定编码器以及帧率，之前讲过的参数当然也可以用到这里来！\n\n```c\nffmpeg -i /dev/video0 -f alsa -i default -c:v libx264 -c:a flac -r 30 output.mkv\n```\n\n有时候不直接录音，而是在录屏/录像的时候给一个音频文件，那么可以这么做：\n\n```c\nffmpeg -f x11grab -s $(xdpyinfo | grep dimensions | awk '{print $2;}') -i :0.0 -i audio.wav -c:a copy output.mp4\n```\n\n**Note:** ***ffmpeg** 使用片段录取，所有有时候非常短的录制可能不会保存文件。我建议录地可以稍微长一些（然后后期裁剪），已保证录制的文件成功写到磁盘上。*\n\n## ffmpeg中的过滤器的基本使用\n\n**过滤器** 是 **ffmpeg** 中最为强大的功能。在ffmepg中有数不甚数的过滤器存在，可以满足各种编辑需要。因为过滤器实在太多了，这里只会简单讲述几个常用的。\n\n使用 过滤的基本结构是:\n\n```c\nffmpeg -i input.mp4 -vf \"filter=setting_1=value_1:setting_2=value_2,etc\" output.mp4\nffmpeg -i input.wav -af \"filter=setting_1=value_1:setting_2=value_2,etc\" output.wav\n```\n\n可以指定视频过滤器 (**-vf**, **-filter:v**的简写) 和 音频过滤器 (**-af**, **-filter:a**的简写). 过滤器的内容写到双引号里面 (**“**) 并且可以使用逗号(**,**)连接。你可以使用任意数量的过滤器（我写了个etc代表更多的，这不是做一个真实的过滤器）。\n\n过滤器设定的通常格式是:\n\n```c\nfilter=setting_2=value_2:setting_2=value_2\n```\n\n过滤器不同的值使用冒号分割。\n\n你甚至可以在值里面使用进行数学符号计算。\n\n**Note:** 参考 **[ffmpeg 过滤器手册](https://ffmpeg.org/ffmpeg-filters.html)**查看更多高级用法\n\n这里举几个例子来说明视频和音频的过滤器。\n\n### 1. 视频缩放\n\n这是个简单过滤器，设定里只有 **width** 和 **height**:\n\n```c\nffmpeg -i input.mp4 -vf \"scale=w=800:h=600\" output.mp4\n```\n\n我说过你可以使用数学运算来给值：\n\n```c\nffmpeg -i input.mkv -vf \"scale=w=1/2*in_w:h=1/2*in_h\" output.mkv\n```\n\n很明显，这个命令让输入的尺寸变成了输入尺寸(in_w, in_h)的1/2.\n\n### 2. 视频裁剪\n\n类似缩放，这个设定也有 **width** 和 **height** ，另外可以指定裁剪的原点（默认是视频的中心）\n\n```c\nffmpeg -i input.mp4 -vf \"crop=w=1280:h=720:x=0:y=0\" output.mp4 \nffmpeg -i input.mkv -vf \"crop=w=400:h=400\" output.mkv\n```\n\n第二个命令裁剪原点是视频的中心点（因为我没有给x和y坐标），第一个命令会从左上角开始裁剪 (**x=0:y=0**).\n\n这里也有一个使用数学计算的例子：\n\n```c\nffmpeg -i input.mkv -vf \"crop=w=3/4*in_w:h=3/4*in_h\" output.mkv\n```\n\n这会把视频裁剪剩下原大小的3/4/。\n\n### 3. 视频旋转\n\n你可以指定一个弧度，顺时针旋转视频。为了让计算简单一些，你可以给角度然后乘以 **PI/180**:\n\n```c\nffmpeg -i input.avi -vf \"rotate=90*PI/180\" \nffmpeg -i input.mp4 -vf \"rotate=PI\"\n```\n\n第一个命令将视频顺时针旋转90°，第二个则是上下颠倒了视频（翻转了180°）。\n\n### 4. 音频声道重映射\n\n有的时候，你的音频只有右耳可以听到声音，那么这个功能就很有用了。你可以让声音同时在左右声道出现：\n\n```c\nffmpeg -i input.mp3 -af \"channelmap=1-0|1-1\" output.mp3\n```\n\n这将右声道（1）同时映射到左（0）右（1）两个声道（左边的数字是输入，右边的数字是输出）。\n\n### 5. 更改音量\n\n你可以将音量大小乘以一个实数（可以是整数也可以不是），你只需要给出那个数大小就行了。\n\n```c\nffmpeg -i input.wav -af \"volume=1.5\" output.wav \nffmpeg -i input.ogg -af \"volume=0.75\" output.ogg\n```\n\n第一个将音量变为1.5倍，第二个则让音量变成了原来的1/4那么安静。\n\n### 技巧：更改播放速度\n\n这里会介绍视频（不影响音频）和音频的过滤器。\n\n1. **视频**\n\n视频过滤器是 **setpts** (PTS = presentation time stamp). 这个参数以一种有趣的方式工作，因为我们修改的是PTS，所以较大的数值意味着较慢的播放速度，反之亦然：\n\n```c\nffmpeg -i input.mkv -vf \"setpts=0.5*PTS\" output.mkv \nffmpeg -i input.mp4 -vf \"setpts=2*PTS\" output,mp4\n```\n\n第一个命令让播放速度加倍了，第二个则是让播放速度降低了一半。\n\n**2. 音频**\n\n这里的过滤器是 **atempo**. 这里有个限制，它只接受 **0.5**(半速) 到 **2** (倍速)之间的值。为了越过这个限制，你可以链式使用这个过滤器：\n\n```c\nffmpeg -i input.wav -af \"atempo=0.75\" output.wav \nffmpeg -i input.mp3 -af \"atempo=2.0,atempo=2.0\" ouutput.mp3\n```\n\n第一个命令让音频速度慢了1/4，第二个则是加速到原来的4（2*2)倍。\n\n**Note:** *如果想在同一个命令中同时修改视频和音频的速度，你得查看一下 **[filtergraphs](https://ffmpeg.org/ffmpeg-filters.html#Filtergraph-description)**.*\n\n**小结**\n\n在这个手册中，我讲述了安装、基本的使用、高级的使用和一些过滤器的基础。\n\n我希望这对于一些尝试使用ffmpeg的人，或者希望使用ffmpeg做很多工作的人来说是个有用的资源，ffmepg真的是个多功能又极其好用的工具。\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 39.40234375,
          "content": "# 💯 2024年，最新 ffmpeg 资料整理，项目（调试可用），命令手册，文章，编解码论文，视频讲解，面试题全套资料\n\n</br>\n\n<p align=\"center\">\n<a> <img width=\"70%\" height=\"70%\" src=\"https://ahmadawais.com/wp-content/uploads/2021/05/FFmpeg.jpg\"></a> \n</p>\n\n</br>\n\n本repo搜集整理全网ffmpeg学习资料。\n\n所有数据来源于互联网。所谓取之于互联网，用之于互联网。\n\n如果涉及版权侵犯，请邮件至 wchao_isvip@163.com ，我们将第一时间处理。\n\n如果您对我们的项目表示赞同与支持，欢迎您 [lssues](https://github.com/0voice/ffmpeg_develop_doc/issues) 我们，或者邮件 wchao_isvip@163.com 我们，更加欢迎您 pull requests 加入我们。\n\n感谢您的支持！\n\n<p align=\"center\">\n  <a href=\"https://github.com/0voice/ffmpeg_develop_doc#%E5%85%B3%E6%B3%A8%E5%BE%AE%E4%BF%A1%E5%85%AC%E4%BC%97%E5%8F%B7%E5%90%8E%E5%8F%B0%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84%E5%B8%88%E8%81%94%E7%B3%BB%E6%88%91%E4%BB%AC%E5%85%8D%E8%B4%B9%E8%8E%B7%E5%8F%96%E6%9B%B4%E5%A4%9Affmepg%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99\"><img src=\"https://img.shields.io/badge/微信公众号-green\" alt=\"\"></a>\n  <a href=\"https://www.zhihu.com/people/xiao-zhai-nu-linux\"><img src=\"https://img.shields.io/badge/知乎-blue\" alt=\"\"></a>\n  <a href=\"https://space.bilibili.com/64514973\"><img src=\"https://img.shields.io/badge/bilibili-red\" alt=\"\"></a>\n</p>\n\n- 目录\n  - [@ 开源项目](https://github.com/0voice/ffmpeg_develop_doc#-%E5%BC%80%E6%BA%90%E9%A1%B9%E7%9B%AE)\n  - [@ 典藏文档](https://github.com/0voice/ffmpeg_develop_doc#-%E5%85%B8%E8%97%8F%E6%96%87%E6%A1%A3)\n  - [@ 系列文章](https://github.com/0voice/ffmpeg_develop_doc#-%E6%96%87%E7%AB%A0)\n  - [@ 面试题](https://github.com/0voice/ffmpeg_develop_doc#-%E9%9D%A2%E8%AF%95%E9%A2%98)\n  - [@ 教学视频](https://github.com/0voice/ffmpeg_develop_doc#-%E8%A7%86%E9%A2%91)\n  - [@ 学术论文](https://github.com/0voice/ffmpeg_develop_doc#-%E8%AE%BA%E6%96%87)\n  - [@ 资料下载](https://github.com/0voice/ffmpeg_develop_doc#%E8%81%94%E7%B3%BB%E4%B8%93%E6%A0%8F)\n\n\n## 🏗 开源项目\n\n- [bilibili/ijkplayer](https://github.com/bilibili/ijkplayer): 基于FFmpeg n3.4的Android/iOS视频播放器，支持MediaCodec, VideoToolbox。\n\n- [befovy/fijkplayer](https://github.com/befovy/fijkplayer): ijkplayer for flutter. ijkplayer 的 flutter 封装。 Flutter video/audio player. Flutter media player plugin for android/iOS based on ijkplayer. fijkplayer 是基于 ijkplayer 封装的 flutter 媒体播放器，开箱即用，无需编译 ijkplayer\n\n- [mpv-player/mpv](https://github.com/mpv-player/mpv): 命令行视频播放器\n\n- [CarGuo/GSYVideoPlayer](https://github.com/CarGuo/GSYVideoPlayer): 视频播放器（IJKplayer、ExoPlayer、MediaPlayer），HTTPS，支持弹幕，外挂字幕，支持滤镜、水印、gif截图，片头广告、中间广告，多个同时播放，支持基本的拖动，声音、亮度调节，支持边播边缓存，支持视频自带rotation的旋转（90,270之类），重力旋转与手动旋转的同步支持，支持列表播放 ，列表全屏动画，视频加载速度，列表小窗口支持拖动，动画效果，调整比例，多分辨率切换，支持切换播放器，进度条小窗口预览，列表切换详情页面无缝播放，rtsp、concat、mpeg。\n\n- [mpenkov/ffmpeg-tutorial](https://github.com/mpenkov/ffmpeg-tutorial): 教程，演示如何编写一个基于FFmpeg的视频播放器\n\n- [imoreapps/ffmpeg-avplayer-for-ios-tvos](https://github.com/imoreapps/ffmpeg-avplayer-for-ios-tvos): 一个微小但强大的iOS和Apple TV OS的av播放器框架，是基于FFmpeg库。\n\n- [unosquare/ffmediaelement](https://github.com/unosquare/ffmediaelement): FFME:高级WPF MediaElement(基于FFmpeg)\n\n- [microshow/RxFFmpeg](https://github.com/microshow/RxFFmpeg)：RxFFmpeg 是基于 ( FFmpeg 4.0 + X264 + mp3lame + fdk-aac + opencore-amr + openssl ) 编译的适用于 Android 平台的音视频编辑、视频剪辑的快速处理框架，包含以下功能：视频拼接，转码，压缩，裁剪，片头片尾，分离音视频，变速，添加静态贴纸和gif动态贴纸，添加字幕，添加滤镜，添加背景音乐，加速减速视频，倒放音视频，音频裁剪，变声，混音，图片合成视频，视频解码图片，抖音首页，视频播放器及支持 OpenSSL https 等主流特色功能\n\n- [wang-bin/QtAV](https://github.com/wang-bin/QtAV): 基于Qt和FFmpeg的跨平台多媒体框架,高性能。用户和开发人员友好。支持Android, iOS, Windows商店和桌面。基于Qt和FFmpeg的跨平台高性能音视频播放框架\n\n- [xufuji456/FFmpegAndroid](https://github.com/xufuji456/FFmpegAndroid): android端基于FFmpeg实现音频剪切、拼接、转码、编解码；视频剪切、水印、截图、转码、编解码、转Gif动图；音视频合成与分离，配音；音视频解码、同步与播放；FFmpeg本地推流、H264与RTMP实时推流直播；FFmpeg滤镜：素描、色彩平衡、hue、lut、模糊、九宫格等；歌词解析与显示\n\n- [Zhaoss/WeiXinRecordedDemo](https://github.com/Zhaoss/WeiXinRecordedDemo): 仿微信视频拍摄UI, 基于ffmpeg的视频录制编辑\n\n- [yangjie10930/EpMedia](https://github.com/yangjie10930/EpMedia): Android上基于FFmpeg开发的视频处理框架，简单易用，体积小，帮助使用者快速实现视频处理功能。包含以下功能：剪辑，裁剪，旋转，镜像，合并，分离，变速，添加LOGO，添加滤镜，添加背景音乐，加速减速视频，倒放音视频\n\n- [goldvideo/h265player](https://github.com/goldvideo/h265player): 一套完整的Web版H.265播放器解决方案，非常适合学习交流和实际应用。基于JS码流解封装、WebAssembly(FFmpeg)视频解码，利用Canvas画布投影、AudioContext播放音频。\n\n- [wanliyang1990/wlmusic](https://github.com/wanliyang1990/wlmusic): 基于FFmpeg + OpenSL ES的音频播放SDK。可循环不间断播放短音频；播放raw和assets音频文件；可独立设置音量大小；可实时现在音量分贝大小（用于绘制波形图）；可改变音频播放速度和音调（变速不变调、变调不变速、变速又变调）；可设置播放声道（左声道、右声道和立体声）；可边播边录留住美好音乐；可裁剪指定时间段的音频，制作自己的彩铃；还可以从中获取音频原始PCM数据(可指定采样率)，方便二次开发等。\n\n- [Jackarain/avplayer](https://github.com/Jackarain/avplayer): 一个基于FFmpeg、libtorrent的P2P播放器实现\n\n- [tsingsee/EasyPlayerPro-Win](https://github.com/tsingsee/EasyPlayerPro-Win): EasyPlayerPro是一款免费的全功能流媒体播放器，支持RTSP、RTMP、HTTP、HLS、UDP、RTP、File等多种流媒体协议播放、支持本地文件播放，支持本地抓拍、本地录像、播放旋转、多屏播放、倍数播放等多种功能特性，核心基于ffmpeg，稳定、高效、可靠、可控，支持Windows、Android、iOS三个平台，目前在多家教育、安防、行业型公司，都得到的应用，广受好评！\n\n- [yangfeng1994/FFmpeg-Android](https://github.com/yangfeng1994/FFmpeg-Android): FFmpeg-Android 是基于ffmpeg n4.0-39-gda39990编译运行在android平台的音视频的处理框架， 使用的是ProcessBuilder执行命令行操作， 可实现视频字幕添加、尺寸剪切、添加或去除水印、时长截取、转GIF动图、涂鸦、音频提取、拼接、质量压缩、加减速、涂鸦、 倒放、素描、色彩平衡、模糊、九宫格、添加贴纸、滤镜、分屏、图片合成视频等,音视频合成、截取、拼接，混音、音视频解码，视频特效等等音视频处理...\n\n- [yangjie10930/EpMediaDemo](https://github.com/yangjie10930/EpMediaDemo): 基于FFmpeg开发的视频处理框架，简单易用，体积小，帮助使用者快速实现视频处理功能。包含以下功能：剪辑，裁剪，旋转，镜像，合并，分离，添加LOGO，添加滤镜，添加背景音乐，加速减速视频，倒放音视频。简单的Demo,后面逐渐完善各类功能的使用。\n\n- [qingkouwei/oarplayer](https://github.com/qingkouwei/oarplayer): Android Rtmp播放器,基于MediaCodec与srs-librtmp,不依赖ffmpeg\n\n- [goldvideo/decoder_wasm](https://github.com/goldvideo/decoder_wasm): 借助于WebAssembly技术，基于ffmpeg的H.265解码器。\n\n- [HeZhang1994/video-audio-tools](https://github.com/HeZhang1994/video-audio-tools): To process/edit video and audio with Python+FFmpeg. [简单实用] 基于Python+FFmpeg的视频和音频的处理/剪辑。\n\n- [jordiwang/web-capture](https://github.com/jordiwang/web-capture): 基于 ffmpeg + Webassembly 实现前端视频帧提取\n\n- [ccj659/NDK-FFmpeg-master](https://github.com/ccj659/NDK-FFmpeg-master): Video and audio decoding based with FFmpeg 基于ffmpeg的 视频解码 音频解码.播放等\n\n- [kolyvan/kxmovie](https://github.com/kolyvan/kxmovie):iOS电影播放器使用ffmpeg\n\n- [CainKernel/CainCamera](https://github.com/CainKernel/CainCamera):一个关于美容相机、图像和短视频开发的Android项目\n\n- [mifi/lossless-cut](https://github.com/mifi/lossless-cut): 一个基于FFmpeg的无损剪辑软件\n\n## 📂 典藏文档\n\n- [AAC解码算法原理详解](https://github.com/0voice/ffmpeg_develop_doc/blob/main/%E6%96%87%E6%A1%A3%E5%BA%93/AAC%E8%A7%A3%E7%A0%81%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3.pdf)\n- [FFMPEG教程完美排版](https://github.com/0voice/ffmpeg_develop_doc/blob/main/%E6%96%87%E6%A1%A3%E5%BA%93/FFMPEG%E6%95%99%E7%A8%8B%E5%AE%8C%E7%BE%8E%E6%8E%92%E7%89%88.pdf)\n- [FFMpeg-SDK-开发手册](https://github.com/0voice/ffmpeg_develop_doc/blob/main/%E6%96%87%E6%A1%A3%E5%BA%93/FFMpeg-SDK-%E5%BC%80%E5%8F%91%E6%89%8B%E5%86%8C.pdf)\n- [FFmpeg Basics](https://github.com/0voice/ffmpeg_develop_doc/blob/main/%E6%96%87%E6%A1%A3%E5%BA%93/FFmpeg%20Basics.pdf)\n- [ffmpeg(libav)解码全解析(带书签)](https://github.com/0voice/ffmpeg_develop_doc/blob/main/%E6%96%87%E6%A1%A3%E5%BA%93/ffmpeg(libav)%E8%A7%A3%E7%A0%81%E5%85%A8%E8%A7%A3%E6%9E%90(%E5%B8%A6%E4%B9%A6%E7%AD%BE).pdf)\n- [ffmpeg的tutorial中文版](https://github.com/0voice/ffmpeg_develop_doc/blob/main/%E6%96%87%E6%A1%A3%E5%BA%93/ffmpeg%E7%9A%84tutorial%E4%B8%AD%E6%96%87%E7%89%88.pdf)\n- [ffmpeg中文文档](https://github.com/0voice/ffmpeg_develop_doc/blob/main/%E6%96%87%E6%A1%A3%E5%BA%93/ffmpeg%E7%9A%84%E4%B8%AD%E6%96%87%E6%96%87%E6%A1%A3.pdf)\n- [详解FFMPEG API](https://github.com/0voice/ffmpeg_develop_doc/blob/main/%E6%96%87%E6%A1%A3%E5%BA%93/%E8%AF%A6%E8%A7%A3FFMPEG%20API.pdf)\n- [ffmpeg常用命令参数详解](https://github.com/0voice/ffmpeg_develop_doc/blob/main/ffmpeg%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4.md)\n- [ffmepg整体分析](https://github.com/0voice/ffmpeg_develop_doc/blob/main/ffmepg%E6%95%B4%E4%BD%93%E5%88%86%E6%9E%90.pdf)\n\n## 📃 文章\n\n- [FFmpeg 学习(一)：FFmpeg 简介](https://github.com/0voice/ffmpeg_develop_doc/blob/main/FFmpeg%20%E5%AD%A6%E4%B9%A0(%E4%B8%80)%EF%BC%9AFFmpeg%20%E7%AE%80%E4%BB%8B%20.md)\n- [FFmpeg 学习(二)：Mac下安装FFmpepg](https://github.com/0voice/ffmpeg_develop_doc/blob/main/FFmpeg%20%E5%AD%A6%E4%B9%A0(%E4%BA%8C)%EF%BC%9AMac%E4%B8%8B%E5%AE%89%E8%A3%85FFmpeg.md)\n- [FFmpeg 学习(三)：将 FFmpeg 移植到 Android平台](https://github.com/0voice/ffmpeg_develop_doc/blob/main/FFmpeg%20%E5%AD%A6%E4%B9%A0(%E4%B8%89)%EF%BC%9A%E5%B0%86%20FFmpeg%20%E7%A7%BB%E6%A4%8D%E5%88%B0%20Android%E5%B9%B3%E5%8F%B0.md)\n- [FFmpeg 学习(四)：FFmpeg API 介绍与通用 API 分析](https://github.com/0voice/ffmpeg_develop_doc/blob/main/FFmpeg%20%E5%AD%A6%E4%B9%A0(%E5%9B%9B)%EF%BC%9AFFmpeg%20API%20%E4%BB%8B%E7%BB%8D%E4%B8%8E%E9%80%9A%E7%94%A8%20API%20%E5%88%86%E6%9E%90.md)\n- [FFmpeg 学习(五)：FFmpeg 编解码 API 分析](https://github.com/0voice/ffmpeg_develop_doc/blob/main/FFmpeg%20%E5%AD%A6%E4%B9%A0(%E4%BA%94)%EF%BC%9AFFmpeg%20%E7%BC%96%E8%A7%A3%E7%A0%81%20API%20%E5%88%86%E6%9E%90.md)\n- [FFmpeg 学习(六)：FFmpeg 核心模块 libavformat 与 libavcodec 分析](https://github.com/0voice/ffmpeg_develop_doc/blob/main/FFmpeg%20%E5%AD%A6%E4%B9%A0(%E5%85%AD)%EF%BC%9AFFmpeg%20%E6%A0%B8%E5%BF%83%E6%A8%A1%E5%9D%97%20libavformat%20%E4%B8%8E%20libavcodec%20%E5%88%86%E6%9E%90.md)\n- [FFmpeg 学习(七)：FFmpeg 学习整理总结](https://github.com/0voice/ffmpeg_develop_doc/blob/main/FFmpeg%20%E5%AD%A6%E4%B9%A0(%E4%B8%83)%EF%BC%9AFFmpeg%20%E5%AD%A6%E4%B9%A0%E6%95%B4%E7%90%86%E6%80%BB%E7%BB%93.md)\n\n<br>\n\n- [FFmpeg 结构体学习(一)： AVFormatContext 分析](https://github.com/0voice/ffmpeg_develop_doc/blob/main/FFmpeg%20%E7%BB%93%E6%9E%84%E4%BD%93%E5%AD%A6%E4%B9%A0(%E4%B8%80)%EF%BC%9A%20AVFormatContext%20%E5%88%86%E6%9E%90.md)\n- [FFmpeg 结构体学习(二)： AVStream 分析](https://github.com/0voice/ffmpeg_develop_doc/blob/main/FFmpeg%20%E7%BB%93%E6%9E%84%E4%BD%93%E5%AD%A6%E4%B9%A0(%E4%BA%8C)%EF%BC%9A%20AVStream%20%E5%88%86%E6%9E%90.md)\n- [FFmpeg 结构体学习(三)： AVPacket 分析](https://github.com/0voice/ffmpeg_develop_doc/blob/main/FFmpeg%20%E7%BB%93%E6%9E%84%E4%BD%93%E5%AD%A6%E4%B9%A0(%E4%B8%89)%EF%BC%9A%20AVPacket%20%E5%88%86%E6%9E%90.md)\n- [FFmpeg 结构体学习(四)： AVFrame 分析](https://github.com/0voice/ffmpeg_develop_doc/blob/main/FFmpeg%20%E7%BB%93%E6%9E%84%E4%BD%93%E5%AD%A6%E4%B9%A0(%E5%9B%9B)%EF%BC%9A%20AVFrame%20%E5%88%86%E6%9E%90.md)\n- [FFmpeg 结构体学习(五)： AVCodec 分析](https://github.com/0voice/ffmpeg_develop_doc/blob/main/FFmpeg%20%E7%BB%93%E6%9E%84%E4%BD%93%E5%AD%A6%E4%B9%A0(%E4%BA%94)%EF%BC%9A%20AVCodec%20%E5%88%86%E6%9E%90.md)\n- [FFmpeg 结构体学习(六)： AVCodecContext 分析](https://github.com/0voice/ffmpeg_develop_doc/blob/main/FFmpeg%20%E7%BB%93%E6%9E%84%E4%BD%93%E5%AD%A6%E4%B9%A0(%E5%85%AD)%EF%BC%9A%20AVCodecContext%20%E5%88%86%E6%9E%90.md)\n- [FFmpeg 结构体学习(七)： AVIOContext 分析](https://github.com/0voice/ffmpeg_develop_doc/blob/main/FFmpeg%20%E7%BB%93%E6%9E%84%E4%BD%93%E5%AD%A6%E4%B9%A0(%E4%B8%83)%EF%BC%9A%20AVIOContext%20%E5%88%86%E6%9E%90.md)\n- [FFmpeg 结构体学习(八)：FFMPEG中重要结构体之间的关系](https://github.com/0voice/ffmpeg_develop_doc/blob/main/FFmpeg%20%E7%BB%93%E6%9E%84%E4%BD%93%E5%AD%A6%E4%B9%A0(%E5%85%AB)%EF%BC%9AFFMPEG%E4%B8%AD%E9%87%8D%E8%A6%81%E7%BB%93%E6%9E%84%E4%BD%93%E4%B9%8B%E9%97%B4%E7%9A%84%E5%85%B3%E7%B3%BB.md)\n\n<br>\n\n- [Linux上的ffmpeg完全使用指南](https://github.com/0voice/ffmpeg_develop_doc/blob/main/Linux%E4%B8%8A%E7%9A%84ffmpeg%E5%AE%8C%E5%85%A8%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97.md)\n- [3个重点，20个函数分析，浅析FFmpeg转码过程](https://github.com/0voice/ffmpeg_develop_doc/blob/main/3%E4%B8%AA%E9%87%8D%E7%82%B9%EF%BC%8C20%E4%B8%AA%E5%87%BD%E6%95%B0%E5%88%86%E6%9E%90%EF%BC%8C%E6%B5%85%E6%9E%90FFmpeg%E8%BD%AC%E7%A0%81%E8%BF%87%E7%A8%8B.md)\n\n## 🌅 面试题\n\n##### [1. 为什么巨大的原始视频可以编码成很小的视频呢?这其中的技术是什么呢?](https://github.com/0voice/ffmpeg_develop_doc/blob/main/case_interview/001-README.md#subject_001)\n\n##### [2. 怎么做到直播秒开优化？](https://github.com/0voice/ffmpeg_develop_doc/blob/main/case_interview/001-README.md#subject_002)\n\n##### [3. 直方图在图像处理里面最重要的作用是什么？](https://github.com/0voice/ffmpeg_develop_doc/blob/main/case_interview/001-README.md#subject_003)\n\n##### [4. 数字图像滤波有哪些方法？](https://github.com/0voice/ffmpeg_develop_doc/blob/main/case_interview/001-README.md#subject_004)\n\n##### [5. 图像可以提取的特征有哪些？](https://github.com/0voice/ffmpeg_develop_doc/blob/main/case_interview/001-README.md#subject_005)\n\n##### [6. 衡量图像重建好坏的标准有哪些？怎样计算？](https://github.com/0voice/ffmpeg_develop_doc/blob/main/case_interview/001-README.md#subject_006)\n\n##### [7. AAC和PCM的区别？](https://github.com/0voice/ffmpeg_develop_doc/blob/main/case_interview/001-README.md#subject_007)\n\n##### [8. H264存储的两个形态？](https://github.com/0voice/ffmpeg_develop_doc/blob/main/case_interview/001-README.md#subject_008)\n\n##### [9. FFMPEG：图片如何合成视频？](https://github.com/0voice/ffmpeg_develop_doc/blob/main/case_interview/001-README.md#subject_009)\n\n##### [10. 常见的音视频格式有哪些？](https://github.com/0voice/ffmpeg_develop_doc/blob/main/case_interview/001-README.md#subject_010)\n\n##### [11. 请指出“1080p”的意义？](https://github.com/0voice/ffmpeg_develop_doc/blob/main/case_interview/001-README.md#subject_011)\n\n##### [12. 请解释颜色的本质及其数字记录原理，并说出几个你所知道的色域。](https://github.com/0voice/ffmpeg_develop_doc/blob/main/case_interview/001-README.md#subject_012)\n\n##### [13. 请解释“矢量图”和“位图”的区别？](https://github.com/0voice/ffmpeg_develop_doc/blob/main/case_interview/001-README.md#subject_013)\n\n##### [14. 请从“光圈”“快门速度”“感光度”“白平衡”“景深”中任选2个进行叙述？](https://github.com/0voice/ffmpeg_develop_doc/blob/main/case_interview/001-README.md#subject_014)\n\n##### [15. 视频分量YUV的意义及数字化格式？](https://github.com/0voice/ffmpeg_develop_doc/blob/main/case_interview/001-README.md#subject_015)\n\n##### [16. 在MPEG标准中图像类型有哪些？](https://github.com/0voice/ffmpeg_develop_doc/blob/main/case_interview/001-README.md#subject_016)\n\n##### [17. 列举一些音频编解码常用的实现方案？](https://github.com/0voice/ffmpeg_develop_doc/blob/main/case_interview/001-README.md#subject_017)\n\n##### [18. 请叙述MPEG视频基本码流结构？](https://github.com/0voice/ffmpeg_develop_doc/blob/main/case_interview/001-README.md#subject_018)\n\n##### [19. sps和pps的区别？](https://github.com/0voice/ffmpeg_develop_doc/blob/main/case_interview/001-README.md#subject_019)\n\n##### [20. 请叙述AMR基本码流结构？](https://github.com/0voice/ffmpeg_develop_doc/blob/main/case_interview/001-README.md#subject_020)\n\n##### [21. 预测编码的基本原理是什么？](https://github.com/0voice/ffmpeg_develop_doc/blob/main/case_interview/001-README.md#subject_021)\n\n##### [22. 说一说ffmpeg的数据结构？](https://github.com/0voice/ffmpeg_develop_doc/blob/main/case_interview/001-README.md#subject_022)\n\n##### [23. 说一说AVFormatContext 和 AVInputFormat之间的关系？](https://github.com/0voice/ffmpeg_develop_doc/blob/main/case_interview/001-README.md#subject_023)\n\n##### [24. 说一说AVFormatContext, AVStream和AVCodecContext之间的关系？](https://github.com/0voice/ffmpeg_develop_doc/blob/main/case_interview/001-README.md#subject_024)\n\n##### [25. 说一说视频拼接处理步骤？（细节处理，比如分辨率大小不一，时间处理等等）](https://github.com/0voice/ffmpeg_develop_doc/blob/main/case_interview/001-README.md#subject_025)\n\n##### [26. NV21如何转换成I420？](https://github.com/0voice/ffmpeg_develop_doc/blob/main/case_interview/001-README.md#subject_026)\n\n##### [27. DTS与PTS共同点？](https://github.com/0voice/ffmpeg_develop_doc/blob/main/case_interview/001-README.md#subject_027)\n\n##### [28. 影响视频清晰度的指标有哪些？](https://github.com/0voice/ffmpeg_develop_doc/blob/main/case_interview/001-README.md#subject_028)\n\n##### [29. 编解码处理时遇到什么困难？](https://github.com/0voice/ffmpeg_develop_doc/blob/main/case_interview/001-README.md#subject_029)\n\n##### [30. 如何秒开视频？什么是秒开视频？](https://github.com/0voice/ffmpeg_develop_doc/blob/main/case_interview/001-README.md#subject_030)\n\n##### [31. 如何降低延迟？如何保证流畅性？如何解决卡顿？解决网络抖动？](https://github.com/0voice/ffmpeg_develop_doc/blob/main/case_interview/001-README.md#subject_031)\n\n##### [32. 需要把网络上一段视频存储下来（比如作为mp4 ), 请实现并说出方法（第一个视频需要翻墙才能进）？](https://github.com/0voice/ffmpeg_develop_doc/blob/main/case_interview/001-README.md#subject_032)\n\n##### [33. 需要把网络上一段语音存储下来（比如作为mp3 ), 请实现并说出方法？](https://github.com/0voice/ffmpeg_develop_doc/blob/main/case_interview/001-README.md#subject_033)\n\n##### [34. 为什么要有YUV这种数据出来？（YUV相比RGB来说的优点）](https://github.com/0voice/ffmpeg_develop_doc/blob/main/case_interview/001-README.md#subject_034)\n\n##### [35. H264/H265有什么区别？](https://github.com/0voice/ffmpeg_develop_doc/blob/main/case_interview/001-README.md#subject_035)\n\n##### [36. 视频或者音频传输，你会选择TCP协议还是UDP协议？为什么？](https://github.com/0voice/ffmpeg_develop_doc/blob/main/case_interview/001-README.md#subject_036)\n\n##### [37. 平时说的软解和硬解，具体是什么？](https://github.com/0voice/ffmpeg_develop_doc/blob/main/case_interview/001-README.md#subject_037)\n\n##### [38. 何为直播？何为点播？](https://github.com/0voice/ffmpeg_develop_doc/blob/main/case_interview/001-README.md#subject_038)\n\n##### [39. 简述推流、拉流的工作流程？](https://github.com/0voice/ffmpeg_develop_doc/blob/main/case_interview/001-README.md#subject_039)\n\n##### [40. 如何在直播中I帧间隔设置、与帧率分辨率选定？](https://github.com/0voice/ffmpeg_develop_doc/blob/main/case_interview/001-README.md#subject_040)\n\n##### [41. 直播推流中推I帧与推非I帧区别是什么？](https://github.com/0voice/ffmpeg_develop_doc/blob/main/case_interview/001-README.md#subject_041)\n\n##### [42. 常见的直播协议有哪些？之间有什么区别？](https://github.com/0voice/ffmpeg_develop_doc/blob/main/case_interview/001-README.md#subject_042)\n\n##### [43. 点播中常见的数据传输协议主要有哪些？](https://github.com/0voice/ffmpeg_develop_doc/blob/main/case_interview/001-README.md#subject_043)\n\n##### [44. RTMP、HLS协议各自的默认端口号是？](https://github.com/0voice/ffmpeg_develop_doc/blob/main/case_interview/001-README.md#subject_044)\n\n##### [45. 简述RTMP协议，如何封装RTMP包？](https://github.com/0voice/ffmpeg_develop_doc/blob/main/case_interview/001-README.md#subject_045)\n\n##### [46. m3u8构成是？直播中m3u8、ts如何实时更新？](https://github.com/0voice/ffmpeg_develop_doc/blob/main/case_interview/001-README.md#subject_046)\n\n##### [47. 何为音视频同步，音视频同步是什么标准？](https://github.com/0voice/ffmpeg_develop_doc/blob/main/case_interview/001-README.md#subject_047)\n\n##### [48. 播放器暂停、快进快退、seek、逐帧、变速怎么实现？](https://github.com/0voice/ffmpeg_develop_doc/blob/main/case_interview/001-README.md#subject_048)\n\n##### [49. 说说你平时在播放过程中做的优化工作？](https://github.com/0voice/ffmpeg_develop_doc/blob/main/case_interview/001-README.md#subject_049)\n\n##### [50. 你研究过哪些具体的流媒体服务器，是否做过二次开发？](https://github.com/0voice/ffmpeg_develop_doc/blob/main/case_interview/001-README.md#subject_050)\n\n##### [51. 什么是GOP?](https://github.com/0voice/ffmpeg_develop_doc/blob/main/case_interview/002-README.md#subject_051)\n\n##### [52. 音频测试的测试点,音频时延如何测试?](https://github.com/0voice/ffmpeg_develop_doc/blob/main/case_interview/002-README.md#subject_052)\n\n##### [53. 美颜的实现原理，具体实现步骤?](https://github.com/0voice/ffmpeg_develop_doc/blob/main/case_interview/002-README.md#subject_053)\n\n##### [54. 如何直播APP抓包过来的文件，如何过滤上行，下行，总码率？](https://github.com/0voice/ffmpeg_develop_doc/blob/main/case_interview/002-README.md#subject_054)\n\n##### [55. 如何测试一个美颜挂件？](https://github.com/0voice/ffmpeg_develop_doc/blob/main/case_interview/002-README.md#subject_055)\n\n##### [56. 为什么要用FLV？](https://github.com/0voice/ffmpeg_develop_doc/blob/main/case_interview/002-README.md#subject_056)\n\n##### [57. 如何测试一个美颜挂件？](https://github.com/0voice/ffmpeg_develop_doc/blob/main/case_interview/002-README.md#subject_057)\n\n##### [58. 平常的视频格式？](https://github.com/0voice/ffmpeg_develop_doc/blob/main/case_interview/002-README.md#subject_058)\n\n##### [59. 何为homebrew？你用它安装过什么？常用命令有哪些？](https://github.com/0voice/ffmpeg_develop_doc/blob/main/case_interview/002-README.md#subject_059)\n\n##### [60. RTMP、HLS协议各自的默认端口号是？](https://github.com/0voice/ffmpeg_develop_doc/blob/main/case_interview/002-README.md#subject_060)\n\n## 🧿 视频\n\n### 国外大神\n\nNo.|title\n:------- | :---------------\n1|[如何使用FFMPEG将MP4视频文件转换为GIF](https://www.0voice.com/uiwebsite/audio_video_streaming/video/001-如何使用FFMPEG将MP4视频文件转换为GIF.mp4)\n2|[FFMPEG Introduction & Examples](https://www.0voice.com/uiwebsite/audio_video_streaming/video/002-FFMPEG%20Introduction%20%26%20Examples.mp4)\n3|[Live Streaming with Nginx and FFmpeg](https://www.0voice.com/uiwebsite/audio_video_streaming/video/003-Live%20Streaming%20with%20Nginx%20and%20FFmpeg.mp4)\n4|[Ep2 Ffmpeg Nginx & Nginx-Rtmp-Module Streaming to Server](https://www.0voice.com/uiwebsite/audio_video_streaming/video/004-Ep2%20Ffmpeg%20Nginx%20%26%20Nginx-Rtmp-Module%20Streaming%20to%20Server.mp4)\n5|[Streaming an IP Camera to a Web Browser using FFmpeg](https://www.0voice.com/uiwebsite/audio_video_streaming/video/005-Streaming%20an%20IP%20Camera%20to%20a%20Web%20Browser%20using%20FFmpeg.mp4)\n6|[Easy Screencasting and Webcamming with ffmpeg in Linux](https://www.0voice.com/uiwebsite/audio_video_streaming/video/006-Easy%20Screencasting%20and%20Webcamming%20with%20ffmpeg%20in%20Linux.mp4)\n7|[Streaming an IP Camera to a Web Browser using FFmpeg](https://www.0voice.com/uiwebsite/audio_video_streaming/video/007-Streaming%20an%20IP%20Camera%20to%20a%20Web%20Browser%20using%20FFmpeg.mp4)\n8|[FFMPEG Advanced Techniques Pt2 - Filtergraphs & Timeline](https://www.0voice.com/uiwebsite/audio_video_streaming/video/008-FFMPEG%20Advanced%20Techniques%20Pt2%20-%20Filtergraphs%20%26%20Timeline.mp4)\n9|[Convert HEVCh265 mkv video to AVCh264 mp4 with ffmpeg](https://www.0voice.com/uiwebsite/audio_video_streaming/video/009-Convert%20HEVCh265%20mkv%20video%20to%20AVCh264%20mp4%20with%20ffmpeg.mp4)\n10|[How to add soft subtitles( srt subrip) to mp4 video using ffmpeg](https://www.0voice.com/uiwebsite/audio_video_streaming/video/010-How%20to%20add%20soft%20subtitles(%20srt%20subrip)%20to%20mp4%20video%20using%20ffmpeg.mp4)\n11|[FFmpeg Processing multiple video files by using.bat file](https://www.0voice.com/uiwebsite/audio_video_streaming/video/011-FFmpeg%20Processing%20multiple%20video%20files%20by%20using.bat%20file.mp4)\n12|[Opensource Multimedia Framework -- FFmpeg](https://www.0voice.com/uiwebsite/audio_video_streaming/video/012-Opensource%20Multimedia%20Framework%20--%20FFmpeg.mp4)\n13|[rtsp streaming node js ip camera jsmpeg](https://www.0voice.com/uiwebsite/audio_video_streaming/video/013-rtsp%20streaming%20node%20js%20ip%20camera%20jsmpeg.mp4)\n14|[H.265 RTSP Streaming to VLC + NewTek NDI Integration](https://www.0voice.com/uiwebsite/audio_video_streaming/video/014-H.265%20RTSP%20Streaming%20to%20VLC%20+%20NewTek%20NDI%20Integration.mp4)\n15|[IP camera stream using RTSP and openCV python](https://www.0voice.com/uiwebsite/audio_video_streaming/video/015-IP%20camera%20stream%20using%20RTSP%20and%20openCV%20python.mp4)\n16|[NAT Traversal & RTSP](https://www.0voice.com/uiwebsite/audio_video_streaming/video/016-NAT%20Traversal%20%26%20RTSP.mp4)\n17|[Simple client et serveur de Streaming RTSP MJPEG(JAVA SE)](https://www.0voice.com/uiwebsite/audio_video_streaming/video/017-Simple%20client%20et%20serveur%20de%20Streaming%20RTSP%20MJPEG(JAVA%20SE).mp4)\n18|[Build Your First WebRTC Video Chat App](https://www.0voice.com/uiwebsite/audio_video_streaming/video/018-Build%20Your%20First%20WebRTC%20Video%20Chat%20App.mp4)\n19|[P2P Video Chat with JavaScript/WebRTC](https://www.0voice.com/uiwebsite/audio_video_streaming/video/019-P2P%20Video%20Chat%20with%20JavaScript%20WebRTC.mp4)\n20|[Building a WebRTC app - LIVE](https://www.0voice.com/uiwebsite/audio_video_streaming/video/020-Building%20a%20WebRTC%20app%20-%20LIVE.mp4)\n21|[Zoom vs WebRTC](https://www.0voice.com/uiwebsite/audio_video_streaming/video/021-Zoom%20vs%20WebRTC.mp4)\n22|[Architectures for a kickass WebRTC application](https://www.0voice.com/uiwebsite/audio_video_streaming/video/022-Architectures%20for%20a%20kickass%20WebRTC%20application.mp4)\n23|[(REACT NATIVE) - integrate webRTC](https://www.0voice.com/uiwebsite/audio_video_streaming/video/023-(REACT%20NATIVE)%20-%20integrate%20webRTC.mp4)\n24|[How to build Serverless Video Chat App using Firebase and WebRTC in React](https://www.0voice.com/uiwebsite/audio_video_streaming/video/024-How%20to%20build%20Serverless%20Video%20Chat%20App%20using%20Firebase%20and%20WebRTC%20in%20React.mp4)\n25|[Implementation Lessons using WebRTC in Asterisk](https://www.0voice.com/uiwebsite/audio_video_streaming/video/025-Implementation%20Lessons%20using%20WebRTC%20in%20Asterisk.mp4)\n\n### 国内大佬\n\nNo.|title | 地址\n:------- | :---------------| :---------------\n26|windows ffmpeg命令行环境搭建|[百度网盘](https://pan.baidu.com/s/1eCQ7o3gcuU06k6-ZcXUASQ)  提取码：i3f2\n27|FFMPEG如何查询命令帮助文档|[百度网盘](https://pan.baidu.com/s/1oA2OErmfZZpEEY_wRQrl_A)  提取码：9mqk\n28|ffmpeg音视频处理流程|[百度网盘](https://pan.baidu.com/s/1jSIop6IUtxOwkse7xnCI7Q)  提取码：azx3\n29|ffmpeg命令分类查询|[百度网盘](https://pan.baidu.com/s/1VGwop_lOJozEh_gYpKYkrw)  提取码：odhc\n30|ffplay播放控制|[百度网盘](https://pan.baidu.com/s/1BbKQvJdokQrazoNtYjhA2Q)  提取码：e51s\n31|ffplay命令选项(上)|[百度网盘](https://pan.baidu.com/s/1upOGZQdmXyiZbWO1LBcTCQ)  提取码：n1zx\n32|ffplay命令选项(下)|[百度网盘](https://pan.baidu.com/s/1d55H9PyK1CU9Nfu37NIBhw)  提取码：rtn0\n33|ffplay命令播放媒体|[百度网盘](https://pan.baidu.com/s/1FjJnW8eBZxsKIIdvbh0f-A)  提取码：bs9s\n34|ffplay简单过滤器|[百度网盘](https://pan.baidu.com/s/1YlkCGIMH62Wj0-OTRLxDkA)  提取码：r4rk\n35|ffmpeg命令参数说明|[百度网盘](https://pan.baidu.com/s/1aOL7vXnspVAh-iNYsz_5xA)  提取码：5q18\n36|ffmpeg命令提取音视频数据|[百度网盘](https://pan.baidu.com/s/1Zlv_6a-O9Fj9HFpt9S6Z5g)  提取码：v807\n37|ffmpeg命令提取像素格式和PCM数据|[百度网盘](https://pan.baidu.com/s/1Z1cdwVexIvAiyCQNPA0k3A)  提取码：az9x\n38|ffmpeg命令转封装|[百度网盘](https://pan.baidu.com/s/1TxZpe2RicrGWgZPhi81E2g)  提取码：s7ez\n39|fmpeg命令裁剪和合并视频|[百度网盘](https://pan.baidu.com/s/1W8b_krHc3PzAfoRXneS2Wg)  提取码：6g0g\n40|fmpeg命令图片与视频互转|[百度网盘](https://pan.baidu.com/s/1nHhhA3y8dHneFVfNoY_fHg)  提取码：a3p5\n41|ffmpeg命令视频录制|[百度网盘](https://pan.baidu.com/s/1zGz_P34GHKE5KVt_b8bT3w)  提取码：em7b\n42|ffmpeg命令直播(上)|[百度网盘](https://pan.baidu.com/s/1rtCfJWWaanK6Syk2254h2g)  提取码：ilxz\n43|ffmpeg命令直播(下)|[百度网盘](https://pan.baidu.com/s/1mo7vo4d_ghqrue7gzE0M1g)  提取码：akyr\n44|ffmpeg过滤器-裁剪|[百度网盘](https://pan.baidu.com/s/1vuQLx_ff8ZnlStxX2aOeXA)  提取码：toii\n45|ffmpeg过滤器-文字水印|[百度网盘](https://pan.baidu.com/s/1YilCkZg99xhwEQBwjenWKQ)  提取码：unuu\n46|ffmpeg过滤器-图片水印|[百度网盘](https://pan.baidu.com/s/11VFsXn-c8e9GZ3Wy4M8hAA)  提取码：mw4v\n47|ffmpeg过滤器-画中画|[百度网盘](https://pan.baidu.com/s/1TFiR47qhPTHAzbSQhatEBA)  提取码：c6fc\n48|ffmpeg过滤器-多宫格|[百度网盘](https://pan.baidu.com/s/1Ib73MtuqgaFoECuSrzOApQ)  提取码：aioi\n49|SRS流媒体服务器实战(上)|[百度网盘](https://pan.baidu.com/s/1kZTa5-0kfCcdMiObpJdOfQ)  提取码：4134\n50|SRS流媒体服务器实战(下)|[百度网盘](https://pan.baidu.com/s/1goy3g9rmHc-JmO9VpsCKvg)  提取码：g4be\n51|音视频开发-ffplay.iikplayer、vlc的播放器设计实现|[百度网盘](https://pan.baidu.com/s/1NTT_fzfkWIYy2DX90joAoA)  提取码：1img\n52|音视频成长之路-进阶三部曲|[百度网盘](https://pan.baidu.com/s/1XUTn60ZHTBt63CmQe2vObw)  提取码：4nw3\n53|为什么直播领域也要搞WebRTC-srs4.0|[百度网盘](https://pan.baidu.com/s/1c9dexc7-QglR-0hkvqnUEQ)  提取码：m47a\n54|腾讯课堂直播如何做到低延迟|[百度网盘](https://pan.baidu.com/s/1oRuwvWRyw7YjDAqzMPnZyQ)  提取码：jruh\n55|rtmp2webrtc提出问题-灵魂拷问|[百度网盘](https://pan.baidu.com/s/1cyf0qCYUYKNyfSchyY6aWQ)  提取码：pupp\n\n## 📰 论文\n\n[分布式视频处理系统设计与实现](https://github.com/0voice/ffmpeg_develop_doc/blob/main/%E5%88%86%E5%B8%83%E5%BC%8F%E8%A7%86%E9%A2%91%E5%A4%84%E7%90%86%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0.pdf)\n\n[基于Android的H.264_AVC解码器的设计与实现](https://github.com/0voice/ffmpeg_develop_doc/blob/main/%E5%9F%BA%E4%BA%8EAndroid%E7%9A%84H.264_AVC%E8%A7%A3%E7%A0%81%E5%99%A8%E7%9A%84%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0.pdf)\n\n[基于FFMPEG的视频转换系统](https://github.com/0voice/ffmpeg_develop_doc/blob/main/%E5%9F%BA%E4%BA%8EFFMPEG%E7%9A%84%E8%A7%86%E9%A2%91%E8%BD%AC%E6%8D%A2%E7%B3%BB%E7%BB%9F.pdf)\n\n[基于FFMPEG的跨平台视频编解码研究](https://github.com/0voice/ffmpeg_develop_doc/blob/main/%E5%9F%BA%E4%BA%8EFFMPEG%E7%9A%84%E8%B7%A8%E5%B9%B3%E5%8F%B0%E8%A7%86%E9%A2%91%E7%BC%96%E8%A7%A3%E7%A0%81%E7%A0%94%E7%A9%B6.pdf)\n\n[基于FFMPEG解码的音视频同步实现](https://github.com/0voice/ffmpeg_develop_doc/blob/main/%E5%9F%BA%E4%BA%8EFFMPEG%E8%A7%A3%E7%A0%81%E7%9A%84%E9%9F%B3%E8%A7%86%E9%A2%91%E5%90%8C%E6%AD%A5%E5%AE%9E%E7%8E%B0.pdf)\n\n[基于FFMpeg的稳定应用层组播流媒体直播系统研究](https://github.com/0voice/ffmpeg_develop_doc/blob/main/%E5%9F%BA%E4%BA%8EFFMpeg%E7%9A%84%E7%A8%B3%E5%AE%9A%E5%BA%94%E7%94%A8%E5%B1%82%E7%BB%84%E6%92%AD%E6%B5%81%E5%AA%92%E4%BD%93%E7%9B%B4%E6%92%AD%E7%B3%BB%E7%BB%9F%E7%A0%94%E7%A9%B6.pdf)\n\n[基于FFmpeg和SDL的智能录屏及播放系统](https://github.com/0voice/ffmpeg_develop_doc/blob/main/%E5%9F%BA%E4%BA%8EFFmpeg%E5%92%8CSDL%E7%9A%84%E6%99%BA%E8%83%BD%E5%BD%95%E5%B1%8F%E5%8F%8A%E6%92%AD%E6%94%BE%E7%B3%BB%E7%BB%9F.pdf)\n\n[基于FFmpeg和SDL的视频流播放存储研究综述](https://github.com/0voice/ffmpeg_develop_doc/blob/main/%E5%9F%BA%E4%BA%8EFFmpeg%E5%92%8CSDL%E7%9A%84%E8%A7%86%E9%A2%91%E6%B5%81%E6%92%AD%E6%94%BE%E5%AD%98%E5%82%A8%E7%A0%94%E7%A9%B6%E7%BB%BC%E8%BF%B0.pdf)\n\n[基于FFmpeg的H.264解码器实现](https://github.com/0voice/ffmpeg_develop_doc/blob/main/%E5%9F%BA%E4%BA%8EFFmpeg%E7%9A%84H.264%E8%A7%A3%E7%A0%81%E5%99%A8%E5%AE%9E%E7%8E%B0.pdf)\n\n[基于FFmpeg的网络视频监控系统的设计与实现](https://github.com/0voice/ffmpeg_develop_doc/blob/main/%E5%9F%BA%E4%BA%8EFFmpeg%E7%9A%84%E7%BD%91%E7%BB%9C%E8%A7%86%E9%A2%91%E7%9B%91%E6%8E%A7%E7%B3%BB%E7%BB%9F%E7%9A%84%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0.pdf)\n\n[基于FFmpeg的视频转码与保护系统的设计与实现](https://github.com/0voice/ffmpeg_develop_doc/blob/main/%E5%9F%BA%E4%BA%8EFFmpeg%E7%9A%84%E8%A7%86%E9%A2%91%E8%BD%AC%E7%A0%81%E4%B8%8E%E4%BF%9D%E6%8A%A4%E7%B3%BB%E7%BB%9F%E7%9A%84%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0.pdf)\n\n[基于FFmpeg的高清实时直播系统设计与实现](https://github.com/0voice/ffmpeg_develop_doc/blob/main/%E5%9F%BA%E4%BA%8EFFmpeg%E7%9A%84%E9%AB%98%E6%B8%85%E5%AE%9E%E6%97%B6%E7%9B%B4%E6%92%AD%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0.pdf)\n\n[基于H.264与H.265的低延时视频监控系统的设计与实现](https://github.com/0voice/ffmpeg_develop_doc/blob/main/%E5%9F%BA%E4%BA%8EH.264%E4%B8%8EH.265%E7%9A%84%E4%BD%8E%E5%BB%B6%E6%97%B6%E8%A7%86%E9%A2%91%E7%9B%91%E6%8E%A7%E7%B3%BB%E7%BB%9F%E7%9A%84%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0.pdf)\n\n[基于H.265的无线视频监控系统设计与实现](https://github.com/0voice/ffmpeg_develop_doc/blob/main/%E5%9F%BA%E4%BA%8EH.265%E7%9A%84%E6%97%A0%E7%BA%BF%E8%A7%86%E9%A2%91%E7%9B%91%E6%8E%A7%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0.pdf)\n\n[基于H.265的视频教育系统的设计与实现](https://github.com/0voice/ffmpeg_develop_doc/blob/main/%E5%9F%BA%E4%BA%8EH.265%E7%9A%84%E8%A7%86%E9%A2%91%E6%95%99%E8%82%B2%E7%B3%BB%E7%BB%9F%E7%9A%84%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0.pdf)\n\n[基于Hadoop的视频转码优化的研究](https://github.com/0voice/ffmpeg_develop_doc/blob/main/%E5%9F%BA%E4%BA%8EHadoop%E7%9A%84%E8%A7%86%E9%A2%91%E8%BD%AC%E7%A0%81%E4%BC%98%E5%8C%96%E7%9A%84%E7%A0%94%E7%A9%B6.pdf)\n\n[基于RTMP协议的流媒体系统的设计实现](https://github.com/0voice/ffmpeg_develop_doc/blob/main/%E5%9F%BA%E4%BA%8ERTMP%E5%8D%8F%E8%AE%AE%E7%9A%84%E6%B5%81%E5%AA%92%E4%BD%93%E7%B3%BB%E7%BB%9F%E7%9A%84%E8%AE%BE%E8%AE%A1%E5%AE%9E%E7%8E%B0.pdf)\n\n[基于RTMP的高清流媒体直播点播封装技术的研究与实现](https://github.com/0voice/ffmpeg_develop_doc/blob/main/%E5%9F%BA%E4%BA%8ERTMP%E7%9A%84%E9%AB%98%E6%B8%85%E6%B5%81%E5%AA%92%E4%BD%93%E7%9B%B4%E6%92%AD%E7%82%B9%E6%92%AD%E5%B0%81%E8%A3%85%E6%8A%80%E6%9C%AF%E7%9A%84%E7%A0%94%E7%A9%B6%E4%B8%8E%E5%AE%9E%E7%8E%B0.caj)\n\n[基于RTSP协议的iOS视频播放器的设计与实现](https://github.com/0voice/ffmpeg_develop_doc/blob/main/%E5%9F%BA%E4%BA%8ERTSP%E5%8D%8F%E8%AE%AE%E7%9A%84iOS%E8%A7%86%E9%A2%91%E6%92%AD%E6%94%BE%E5%99%A8%E7%9A%84%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0.pdf)\n\n[基于RTSP协议的多源视音频实时直播系统的设计与实现](https://github.com/0voice/ffmpeg_develop_doc/blob/main/%E5%9F%BA%E4%BA%8ERTSP%E5%8D%8F%E8%AE%AE%E7%9A%84%E5%A4%9A%E6%BA%90%E8%A7%86%E9%9F%B3%E9%A2%91%E5%AE%9E%E6%97%B6%E7%9B%B4%E6%92%AD%E7%B3%BB%E7%BB%9F%E7%9A%84%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0.pdf)\n\n[基于RTSP的H.264实时流媒体传输方案的研究与实现](https://github.com/0voice/ffmpeg_develop_doc/blob/main/%E5%9F%BA%E4%BA%8ERTSP%E7%9A%84H.264%E5%AE%9E%E6%97%B6%E6%B5%81%E5%AA%92%E4%BD%93%E4%BC%A0%E8%BE%93%E6%96%B9%E6%A1%88%E7%9A%84%E7%A0%94%E7%A9%B6%E4%B8%8E%E5%AE%9E%E7%8E%B0.pdf)\n\n[基于RTSP的音视频传输系统研究与实现](https://github.com/0voice/ffmpeg_develop_doc/blob/main/%E5%9F%BA%E4%BA%8ERTSP%E7%9A%84%E9%9F%B3%E8%A7%86%E9%A2%91%E4%BC%A0%E8%BE%93%E7%B3%BB%E7%BB%9F%E7%A0%94%E7%A9%B6%E4%B8%8E%E5%AE%9E%E7%8E%B0.pdf)\n\n[基于TCP传输的嵌入式流媒体播放系统](https://github.com/0voice/ffmpeg_develop_doc/blob/main/%E5%9F%BA%E4%BA%8ETCP%E4%BC%A0%E8%BE%93%E7%9A%84%E5%B5%8C%E5%85%A5%E5%BC%8F%E6%B5%81%E5%AA%92%E4%BD%93%E6%92%AD%E6%94%BE%E7%B3%BB%E7%BB%9F.pdf)\n\n[基于ffmpeg的高性能高清流媒体播放器软件设计](https://github.com/0voice/ffmpeg_develop_doc/blob/main/%E5%9F%BA%E4%BA%8Effmpeg%E7%9A%84%E9%AB%98%E6%80%A7%E8%83%BD%E9%AB%98%E6%B8%85%E6%B5%81%E5%AA%92%E4%BD%93%E6%92%AD%E6%94%BE%E5%99%A8%E8%BD%AF%E4%BB%B6%E8%AE%BE%E8%AE%A1.pdf)\n\n[基于流媒体技术的移动视频直播系统的设计与实现](https://github.com/0voice/ffmpeg_develop_doc/blob/main/%E5%9F%BA%E4%BA%8E%E6%B5%81%E5%AA%92%E4%BD%93%E6%8A%80%E6%9C%AF%E7%9A%84%E7%A7%BB%E5%8A%A8%E8%A7%86%E9%A2%91%E7%9B%B4%E6%92%AD%E7%B3%BB%E7%BB%9F%E7%9A%84%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0.pdf)\n\n[直播聚合平台的设计与实现](https://github.com/0voice/ffmpeg_develop_doc/blob/main/%E7%9B%B4%E6%92%AD%E8%81%9A%E5%90%88%E5%B9%B3%E5%8F%B0%E7%9A%84%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0.pdf)\n\n[音视频信号采集压缩及传输系统的设计与实现](https://github.com/0voice/ffmpeg_develop_doc/blob/main/%E9%9F%B3%E8%A7%86%E9%A2%91%E4%BF%A1%E5%8F%B7%E9%87%87%E9%9B%86%E5%8E%8B%E7%BC%A9%E5%8F%8A%E4%BC%A0%E8%BE%93%E7%B3%BB%E7%BB%9F%E7%9A%84%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0.pdf)\n\n<br/>\n<br/>\n<h3 >零领工作</h3> \n\n---\n\n##### 实时提供，每周发布北京，上海，广州，深圳，杭州，南京，合肥，武汉，长沙，重庆，成都，西安，厦门的c/c++，golang方向的招聘岗位信息。 包含校招，社招，实习岗位， 面经，八股，简历\n\n<img src=\"https://img.0voice.com/public/0e59910091576beaebe20f303357edf7.jpg\" alt=\"零领工作\" style=\"width:300px;height:300px;\">\n\n<br/>\n<br/>\n"
        },
        {
          "name": "case_interview",
          "type": "tree",
          "content": null
        },
        {
          "name": "ffmepg整体分析.pdf",
          "type": "blob",
          "size": 975.9716796875,
          "content": null
        },
        {
          "name": "ffmpeg常用命令.md",
          "type": "blob",
          "size": 35.134765625,
          "content": "# ffmpeg常用命令\n\n## `ffmpeg --help`大概分为6个部分，具体如下：\n\n- ffmpeg信息查询部分\n- 公共操作参数部分\n- 文件主要操作参数部分\n- 视频操作参数部分\n- 音频操作参数部分\n- 字母操作参数部分\n\n## 查看支持的容器格式\n\n```\n# 封装和解封装\nffmpeg -formats\n# 解封装\nffmpeg -demuxers\n# 封装\nffmpeg -muxers\n# 查看FLV封装器的参数支持\nffmpeg -h muxer=flv\n# 查看FLV解封装器的参数支持\nffmpeg -h demuxer=flv\n```\n\n## 查看支持的编解码格式\n\n```\n# 编解码\nffmpeg -codecs\n# 解码\nffmpeg -decoders\n# 编码\nffmpeg -encoders\n# 查看H.264(AVC)的编码参数支持\nffmpeg -h encoder=h264\n# 查看H.264(AVC)的解码参数支持\nffmpeg -h decoder=h264\n```\n\n## 查看支持的滤镜\n\n```\n# 滤镜\nffmpeg -filters\n# 查看colorkey滤镜的参数支持\nffmpeg -h filter=colorkey\n```\n\n## 转码\n\n```\nffmpeg -i WMV9_1280x720.wmv -vcodec mpeg4 -b:v 200 -r 15 -an output.mp4 \n# -i 文件 （后缀名）封装格式\n# -vcodec 视频编码格式\n# -b:v 视频码率\n# -r 视频帧率\n# -an 不包括音频\n```\n\n**a) 通用选项**\n\n- -L license\n- -h 帮助\n- -fromats 显示可用的格式，编解码的，协议的...\n- -f fmt 强迫采用格式fmt\n- -I filename 输入文件\n- -y 覆盖输出文件\n- -t duration 设置纪录时间 hh:mm:ss[.xxx]格式的记录时间也支持\n- -ss position 搜索到指定的时间 [-]hh:mm:ss[.xxx]的格式也支持\n- -title string 设置标题\n- -author string 设置作者\n- -copyright string 设置版权\n- -comment string 设置评论\n- -target type 设置目标文件类型(vcd,svcd,dvd) 所有的格式选项（比特率，编解码以及缓冲区大小）自动设置，只需要输入如下的就可以了：ffmpeg -i myfile.avi -target vcd /tmp/vcd.mpg\n- -hq 激活高质量设置\n- -itsoffset offset 设置以秒为基准的时间偏移，该选项影响所有后面的输入文件。该偏移被加到输入文件的时戳，定义一个正偏移意味着相应的流被延迟了 offset秒。 [-]hh:mm:ss[.xxx]的格式也支持\n\n**b) 视频选项**\n\n- -b bitrate 设置比特率，缺省200kb/s\n- -r fps 设置帧频 缺省25\n- -s size 设置帧大小 格式为WXH 缺省160X128.下面的简写也可以直接使用：\n- Sqcif 128X96 qcif 176X144 cif 252X288 4cif 704X576\n- -aspect aspect 设置横纵比 4:3 16:9 或 1.3333 1.7777\n- -croptop size 设置顶部切除带大小 像素单位\n- -cropbottom size –cropleft size –cropright size\n- -padtop size 设置顶部补齐的大小 像素单位\n- -padbottom size –padleft size –padright size –padcolor color 设置补齐条颜色(hex,6个16进制的数，红:绿:兰排列，比如 000000代表黑色)\n- -vn 不做视频记录\n- -bt tolerance 设置视频码率容忍度kbit/s\n- -maxrate bitrate设置最大视频码率容忍度\n- -minrate bitreate 设置最小视频码率容忍度\n- -bufsize size 设置码率控制缓冲区大小\n- -vcodec codec 强制使用codec编解码方式。如果用copy表示原始编解码数据必须被拷贝。\n- -sameq 使用同样视频质量作为源（VBR）\n- -pass n 选择处理遍数（1或者2）。两遍编码非常有用。第一遍生成统计信息，第二遍生成精确的请求的码率\n- -passlogfile file 选择两遍的纪录文件名为file\n\n**c)高级视频选项**\n\n- -g gop_size 设置图像组大小\n- -intra 仅适用帧内编码\n- -qscale q 使用固定的视频量化标度(VBR)\n- -qmin q 最小视频量化标度(VBR)\n- -qmax q 最大视频量化标度(VBR)\n- -qdiff q 量化标度间最大偏差 (VBR)\n- -qblur blur 视频量化标度柔化(VBR)\n- -qcomp compression 视频量化标度压缩(VBR)\n- -rc_init_cplx complexity 一遍编码的初始复杂度\n- -b_qfactor factor 在p和b帧间的qp因子\n- -i_qfactor factor 在p和i帧间的qp因子\n- -b_qoffset offset 在p和b帧间的qp偏差\n- -i_qoffset offset 在p和i帧间的qp偏差\n- -rc_eq equation 设置码率控制方程 默认tex^qComp\n- -rc_override override 特定间隔下的速率控制重载\n- -me method 设置运动估计的方法 可用方法有 zero phods log x1 epzs(缺省) full\n- -dct_algo algo 设置dct的算法 可用的有 0 FF_DCT_AUTO 缺省的DCT 1 FF_DCT_FASTINT 2 FF_DCT_INT 3 FF_DCT_MMX 4 FF_DCT_MLIB 5 FF_DCT_ALTIVEC\n- -idct_algo algo 设置idct算法。可用的有 0 FF_IDCT_AUTO 缺省的IDCT 1 FF_IDCT_INT 2 FF_IDCT_SIMPLE 3 FF_IDCT_SIMPLEMMX 4 FF_IDCT_LIBMPEG2MMX 5 FF_IDCT_PS2 6 FF_IDCT_MLIB 7 FF_IDCT_ARM 8 FF_IDCT_ALTIVEC 9 FF_IDCT_SH4 10 FF_IDCT_SIMPLEARM\n- -er n 设置错误残留为n 1 FF_ER_CAREFULL 缺省 2 FF_ER_COMPLIANT 3 FF_ER_AGGRESSIVE 4 FF_ER_VERY_AGGRESSIVE\n- -ec bit_mask 设置错误掩蔽为bit_mask,该值为如下值的位掩码 1 FF_EC_GUESS_MVS (default=enabled) 2 FF_EC_DEBLOCK (default=enabled)\n- -bf frames 使用frames B 帧，支持mpeg1,mpeg2,mpeg4\n- -mbd mode 宏块决策 0 FF_MB_DECISION_SIMPLE 使用mb_cmp 1 FF_MB_DECISION_BITS 2 FF_MB_DECISION_RD\n- -4mv 使用4个运动矢量 仅用于mpeg4\n- -part 使用数据划分 仅用于mpeg4\n- -bug param 绕过没有被自动监测到编码器的问题\n- -strict strictness 跟标准的严格性\n- -aic 使能高级帧内编码 h263+\n- -umv 使能无限运动矢量 h263+\n- -deinterlace 不采用交织方法\n- -interlace 强迫交织法编码仅对mpeg2和mpeg4有效。当你的输入是交织的并且你想要保持交织以最小图像损失的时候采用该选项。可选的方法是不交织，但是损失更大\n- -psnr 计算压缩帧的psnr\n- -vstats 输出视频编码统计到vstats_hhmmss.log\n- -vhook module 插入视频处理模块 module 包括了模块名和参数，用空格分开\n\n**D)音频选项**\n\n- -ab bitrate 设置音频码率\n- -ar freq 设置音频采样率\n- -ac channels 设置通道 缺省为1\n- -an 不使能音频纪录\n- -acodec codec 使用codec编解码\n\n**E)音频/视频捕获选项**\n\n- -vd device 设置视频捕获设备。比如/dev/video0\n- -vc channel 设置视频捕获通道 DV1394专用\n- -tvstd standard 设置电视标准 NTSC PAL(SECAM)\n- -dv1394 设置DV1394捕获\n- -av device 设置音频设备 比如/dev/dsp\n\n**F)高级选项**\n\n- -map file:stream 设置输入流映射\n- -debug 打印特定调试信息\n- -benchmark 为基准测试加入时间\n- -hex 倾倒每一个输入包\n- -bitexact 仅使用位精确算法 用于编解码测试\n- -ps size 设置包大小，以bits为单位\n- -re 以本地帧频读数据，主要用于模拟捕获设备\n- -loop 循环输入流（只工作于图像流，用于ffserver测试）\n\n\n# ffprobe常用命令\n\n## `-show_packets` 查看多媒体数据包信息\n\n| 字段          | 说明                                         |\n| :------------ | :------------------------------------------- |\n| codec_type    | 多媒体类型，如视频包、音频包等               |\n| stream_index  | 多媒体的stream索引                           |\n| pts           | 多媒体的显示时间值                           |\n| pts_time      | 根据不同格式计算过后的多媒体的显示时间       |\n| dts           | 多媒体解码时间值                             |\n| dts_time      | 根据不同格式计算过后的多媒体的解码时间       |\n| duration      | 多媒体包占用的时间值                         |\n| duration_time | 根据不同格式计算过后的多媒体包所占用的时间值 |\n| size          | 多媒体包的大小                               |\n| pos           | 多媒体包所在的文件偏移位置                   |\n| flags         | 多媒体包标记，如关键包与非关键包的标记       |\n\n## `-show_format` 查看多媒体的封装格式\n\n| 字段             | 说明                 |\n| :--------------- | :------------------- |\n| filename         | 文件名               |\n| nb_streams       | 媒体中包含的流的个数 |\n| nb_programs      | 节目数               |\n| format_name      | 使用的封装模块的名称 |\n| format_long_name | 封装的完整名称       |\n| start_time       | 媒体文件的起始时间   |\n| duration         | 媒体文件的总时间长度 |\n| size             | 媒体文件的大小       |\n| bit_rate         | 媒体文件的码率       |\n\n## `-show_frames` 查看视频文件中的帧信息\n\n| 属性              | 说明                           | 值       |\n| :---------------- | :----------------------------- | :------- |\n| media_type        | 帧的类型（视频、音频、字幕等） | video    |\n| stream_index      | 帧所在的索引区域               | 0        |\n| key_frame         | 是否为关键帧                   | 1        |\n| pkt_pts           | Frame包的pts                   | 0        |\n| pkt_pts_time      | Frame包的pts的时间显示         | 0.080000 |\n| pkt_dts           | Frame包的dts                   | 80       |\n| pkt_dts_time      | Frame包的dts的时间显示         | 0.080000 |\n| pkt_duration      | Frame包的时长                  | N/A      |\n| pkt_duration_time | Frame包的时长时间显示          | N/A      |\n| pkt_pos           | Frame包所在文件的偏移位置      | 344      |\n| width             | 帧显示的宽度                   | 1280     |\n| height            | 帧显示的高度                   | 714      |\n| pix_fmt           | 帧的图像色彩格式               | yuv420p  |\n| pict_type         | 帧类型                         | I        |\n\n## `-show_streams` 查看多媒体文件中的流信息\n\n| 属性             | 说明                              | 值             |\n| :--------------- | :-------------------------------- | :------------- |\n| index            | 流所在的索引区域                  | 0              |\n| codec_name       | 编码名                            | h264           |\n| codec_long_name  | 编码全名                          | MPEG-4 part 10 |\n| profile          | 编码的profile                     | High           |\n| level            | 编码的level                       | 31             |\n| has_b_frames     | 包含B帧信息                       | 2              |\n| codec_type       | 编码类型                          | video          |\n| codec_time_base  | 编码的时间戳计算基础单位          | 1/50           |\n| pix_fmt          | 图像显示的色彩格式                | yuv420p        |\n| coded_width      | 图像的宽度                        | 1280           |\n| coded_height     | 图像的高度                        | 714            |\n| codec_tag_string | 编码的标签数据                    | [0][0][0][0]   |\n| r_frame_rate     | 实际帧率                          | 25/1           |\n| avg_frame_rate   | 平均帧率                          | 25/1           |\n| time_base        | 时间基数（用来进行timestamp计算） | 1/1000         |\n| bit_rate         | 码率                              | 200000         |\n| max_bit_rate     | 最大码率                          | N/A            |\n| nb_frames        | 帧数                              | N/A            |\n\n## `-printf_format`或`-of` 格式化输出支持XML、INI、JSON、CSV、FLAT等\n\n# ffplay\n\n## 可视化\n\nVisualize information exported by some codecs.\n\nhttp://ffmpeg.org/ffmpeg-all.html#codecview\n\nhttps://trac.ffmpeg.org/wiki/Debug/MacroblocksAndMotionVectors\n\n```\nffmpeg -h filter=codecview\n```\n\n- Visualize forward predicted MVs of all frames using\n\n  ```\n  ffplay -flags2 +export_mvs input.mp4 -vf codecview=mv_type=fp\n  ```\n\n- Visualize multi-directionals MVs of P and B-Frames using\n\n  ```\n  ffplay -flags2 +export_mvs input.mp4 -vf codecview=mv=pf+bf+bb\n  ```\n\n# ffmpeg转封装格式\n\n- 需要知道 **源容器** 和 **目标容器** 的可容纳的编码格式\n\n- 编码格式如果相互兼容，可以用`-c copy`拷贝原有的stream\n\n  `ffmpeg -i input.mp4 -c copy -f flv output.flv`\n\n- 编码格式如果不兼容，需要转化成目标文件支持的编码\n\n  `ffmpeg -i input_ac3.mp4 -vcodec copy -acodec aac -f flv output.flv`\n\n## HLS\n\n1. FFmpeg转HLS举例\n\n   常规的从文件转换HLS直播时：\n\n   ```\n   ffmpeg -re -i input.mp4 -c copy -f hls -bsf:v h264_mp4toannexb output.m3u8\n   # -bsf:v h264_mp4toannexb 作用是将MP4中的H.264数据转换成H.264 AnnexB标准编码，AnnexB标准的编码常见于实时传输流中\n   ```\n\n如果源文件为FLV、TS等可以作为直播传输流的视频，则不需要这个参数。\n\n1. ffmpeg推流上传HLS相关的M3U8以及TS文件\n\n   Nginx配置webdav模块\n\n   ```\n   ffmpeg -re -i input.mp4 -c copy -f hls -hls_time 3 -hls_list_size 0 -method PUT -t 30 http://127.0.0.1/test/output.m3u8\n   ```\n\n## 音视频文件音视频流抽取\n\n1. FFmpeg抽取音视频文件中的AAC音频流\n   `ffmpeg -i input.mp4 -vn -acodec copy output.aac`\n\n2. FFmpeg抽取音视频文件中的H.264视频流\n   `ffmpeg -i input.mp4 -vcodec copy -an output.h264`\n\n3. FFmpeg抽取音视频文件中的H.265视频流(前提文件视频编码格式为hevc)\n\n   ```\n   ffmpeg -i input.mp4 -vcodec copy -an -bsf hevc_mp4toannexb -f hevc output.hevc\n   ```\n\n   # ffmpeg转码\n\n## h264转h265(HEVC)\n\n```\nffmpeg -i input.mp4 -c:v libx265 -vtag hvc1 h265_output.mp4\n```\n\n## aac转MP3(需要安装libmp3lame)\n\n```\nffmpeg -i AVC_high_1280x720_2013.mp4 -vn -acodec libmp3lame -f mp3 out.mp3\n```\n\n## x264\n\n### 安装\n\n```\n$git clone git://git.videolan.org/x264.git\n$cd x264\n$./configure –enable-shared \n$make\n$sudo make install\n```\n\n### 查看\n\n```\nx264 --full help\n```\n\n### 设置编码参数\n\n1. 编码器预设参数设置preset\n   通常通过preset来设置编码的速度，影响清晰度\n   `ffmpeg -i input.mp4 -vcodec libx264 -preset ultrafast -b:v 2000k output.mp4`\n\n2. H.264编码优化参数tune\n   在使用ffmpeg与x264进行H.264直播编码并进行推流时，只用tune参数的zerolatency将会提升效率，因为其降低了因编码导致的延迟。\n\n3. H.264的profile与level设置\n   baseline profile编码的H.264视频不会包含B Slice，而使用main profile、high profile编码出来的视频，均可以包含B Slice\n   `ffmpeg -i input.mp4 -vcodec libx264 -profile:v baseline -level 3.1 -s 352x288 -an -y -t 10 output_baseline.ts`\n   `ffmpeg -i input.mp4 -vcodec libx264 -profile:v high -level 3.1 -s 352x288 -an -y -t 10 output_high.ts`\n\n   查看包含B帧的情况：\n\n   `ffprobe -v quiet -show_frames -select_streams v output_baseline.ts | grep \"pict_type=B\" | wc -l`\n\n   当进行实时流媒体直播时，采用baseline编码相对main或high的profile会更可靠些。\n\n4. 控制场景切花关键帧插入参数 sc_threshold\n   ffmpeg通过-g参数设置以帧数间隔为GOP的长度，但是当遇到场景切换时，例如从一个画面突然变成另一个画面时，会强行插入一个关键帧，这是GOP的间隔将会重新开始，可以通过使用sc_threshold参数进行设定以决定是否在场景切换时插入关键帧。\n   ffmpeg命令控制编码时的GOP大小\n   `ffmpeg -i AVC_high_1280x720_2013.mp4 -c:v libx264 -g 50 -t 60 output.mp4`\n   为了使得GOP的插入更加均匀，使用参数 sc_threshold\n   `ffmpeg -i AVC_high_1280x720_2013.mp4 -c:v libx264 -g 50 -sc_threshold 0 -t 60 -y output.mp4`\n\n5. 设置x264内部参数x264opts\n   去掉B帧\n   `ffmpeg -i input.mp4 -c:v libx264 -x264opts \"bframes=0\" -g 50 -sc_threshold 0 output.mp4`\n   控制I帧、P帧、B帧的频率与规律\n   例如设置GOP中，每2个P帧之间存放3个B帧：\n   `ffmpeg -i input.mp4 -c:v libx264 -x264opts \"bframes=3:b-adapt=0\" -g 50 -sc_threshold 0 output.mp4`\n\n6. CBR 恒定码率设置参数 nal-hrd （固定码率好处，可能是网络传输）\n\n   VBR：可变码率\n\n   CBR：恒定码率\n\n   ABR：平均码率。VBR和CBR混合产物。\n\n   ```\n   ffmpeg -i input.mp4 -c:v libx264 -x264opts \"bframes=10:b-adapt=0\" -b:v 1000k -maxrate 1000k -minrate 1000k -bufsize 50k -nal-hrd cbr -g 50 -sc_threshold 0 output.ts\n   # 设置B帧的个数，并且是每2个P帧之间包含10个B帧\n   # 设置视频码率为 1000 kbit/s\n   # 设置最大码率为 1000 kbit/s\n   # 设置最小码率为 1000 kbit/s\n   # 设置编码的buffer大小为 50KB\n   # 设置 H.264 的编码HRD信号形式为 CBR\n   # 设置每50帧一个GOP\n   # 设置场景切换不强行插入关键帧\n   ```\n\n## MP3/AAC\n\n1. MP3转码\n   `ffmpeg -i INPUT -acodec libmp3lame output.mp3`\n\n2. 参数控制\n\n   ```\n   # -q 控制码率(0~9) 高->低\n   ffmpeg -i input.mp3 -acodec libmp3lame -q:a 8 output.mp3\n   # -b 设置为CBR\n   ffmpeg -i input.mp3 -acodec libmp3lame -b:a 64k output.mp3\n   # -abr 设置为abr编码\n   ffmpeg -i input.mp3 -acodec libmp3lame -b:a 64k -abr 1 output.mp3\n   ```\n\n# ffmpeg流媒体\n\n## ffmpeg发布与录制RTMP流\n\nFFmpeg操作RTMP的参数\n\n| 参数           | 类型   | 说明                                                         |\n| :------------- | :----- | :----------------------------------------------------------- |\n| rtmp_app       | 字符串 | RTMP流发布点，又称为APP                                      |\n| rtmp_buffer    | 整数   | 客户端buffer大小（单位：毫秒），默认为3秒                    |\n| rtmp_conn      | 字符串 | 在RTMP的Connect命令中增加自定义AMF数据                       |\n| rtmp_flashver  | 字符串 | 设置模拟的flashplugin的版本号                                |\n| rtmp_live      | 整数   | 指定RTMP流媒体播放类型，具体如下： - any：直播或点播 - live：直播 - recorded：点播 |\n| rtmp_pageurl   | 字符串 | RTMP在Connect命令中设置的PageURL字段，其为播放时所在的Web页面URL |\n| rtmp_playpath  | 字符串 | RTMP流播放的Stream地址，或者成为**秘钥**，或者成为发布流     |\n| rtmp_subscribe | 字符串 | 直播名称，默认设置为rtmp_playpath的值                        |\n| rtmp_swfhash   | 二进制 | 解压swf文件后的SHA256的hash值                                |\n| rtmp_swfsize   | 整数   | swf文件解压后的大小，用于swf认证                             |\n| rtmp_swfurl    | 字符串 | RTMP的Connect命令中设置的swfURL播放器的URL                   |\n| rtmp_swfverify | 字符串 | 设置swf认证时swf文件的URL地址                                |\n| rtmp_tcurl     | 字符串 | RTMP的Connect命令中设置的tcURL目标发布点地址，一般形如 rtmp://xxx.xxx.xxx/app |\n| rtmp_listen    | 整数   | 开启RTMP服务时所监听的端口                                   |\n| listen         | 整数   | 与rtmp_listen相同                                            |\n| timeout        | 整数   | 监听rtmp端口时设置的超时时间，以秒为单位                     |\n\n1. rtmp_app、rtmp_playpath 参数\n\n   通过rtmp_app、rtmp_playpath参数设置rtmp的推流发布点\n\n   `ffmpeg -re -i AVC_high_1280x720_2013.mp4 -c copy -f flv -rtmp_app live -rtmp_playpath play rtmp://127.0.0.1`\n\n   等价于\n\n   `ffmpeg -re -i AVC_high_1280x720_2013.mp4 -c copy -f flv rtmp://127.0.0.1/live/play`\n\n   ## ffmpeg录制RTSP流\n\nFFmpeg操作RTSP的参数\n\n| 参数                | 类型   | 说明                                                         |\n| :------------------ | :----- | :----------------------------------------------------------- |\n| initial_pause       | 布尔   | 建立连接后暂停播放                                           |\n| rtsp_transport      | 标记   | 设置RTSP传输协议，具体如下： - udp：UDP - tcp：TCP -udp_multicast：UDP多播协议 - http：HTTP隧道 |\n| rtsp_flags          | 标记   | RTSP使用标记，具体如下： - filter_src：只接收指定IP的流 - listen：设置为被动接收模式 - prefer_tcp：TCP亲和模式，如果TCP可用则首选TCP传输 |\n| allowed_media_types | 标记   | 设置允许接收的数据模式（默认全部开启），具体如下： - video：只接收视频 - audio：只接收音频 - data：只接收数据 - subtitle：只接收字幕 |\n| min_port            | 整数   | 设置最小本地UDP端口，默认为5000                              |\n| max_port            | 整数   | 设置最大本地UDP端口，默认为65000                             |\n| timeout             | 整数   | 设置监听端口超时时间                                         |\n| reorder_queue_size  | 整数   | 设置录制数据Buffer的大小                                     |\n| buffer_size         | 整数   | 设置底层传输包Buffer的大小                                   |\n| user-agent          | 字符串 | 用户客户端标识                                               |\n\n1. TCP方式录制RTSP直播流\n\n   ffmpeg默认使用的rtsp拉流方式为UDP，为了避免丢包导致的花屏、绿屏、灰屏、马赛克等问题，将UDP改为TCP传输：\n\n```\nffmpeg -rtsp_transport tcp -i rtsp://127.0.0.1/test.mkv -c copy -f mp4 output.mp4\n```\n\n1. User-Agent设置参数\n\n   ```\n   ffmpeg -user-agent \"Alex-Player\" -i rtsp://input:554/live/1/stream.sdp -c copy -f mp4 -u output.mp4\n   ```\n\n   ## FFmpeg录制HTTP流\n\nFFmpeg操作HTTP的参数\n\n| 参数              | 类型   | 说明                             |\n| :---------------- | :----- | :------------------------------- |\n| seekable          | 布尔   | 设置HTTP连接为可seek操作         |\n| chunked_post      | 布尔   | 使用Chunked模式post数据          |\n| http_proxy        | 字符串 | 设置HTTP代理传输数据             |\n| headers           | 字符串 | 自定义HTTP Header数据            |\n| content_type      | 字符串 | 设置POST的内容类型               |\n| user_agent        | 字符串 | 设置HTTP请求客户端信息           |\n| multiple_requests | 布尔   | HTTP长连接开启                   |\n| post_data         | 二进制 | 设置将要POST的数据               |\n| cookies           | 字符串 | 设置HTTP请求时写代码的Cookies    |\n| icy               | 布尔   | 请求ICY源数据：默认开关          |\n| auth_type         | 整数   | HTTP验证类型设置                 |\n| offset            | 整数   | 初始化HTTP请求时的偏移位置       |\n| method            | 字符串 | 发起HTTP请求时使用的HTTP的方法   |\n| reconnect         | 布尔   | 在EOF之前断开发起重连            |\n| reconnect_at_eof  | 布尔   | 在得到EOF时发起重连              |\n| reply_code        | 整数   | 作为HTTP服务时向客户端反馈状态码 |\n\n## FFmpeg录制和发布TCP与UDP流\n\n略\n\n## FFmpeg推多路流\n\n1. 推流（tee协议输出多路流）\n   `ffmpeg -re -i AVC_high_1280x720_2013.mp4 -vcodec libx264 -acodec aac -map 0 -f flv \"tee:rtmp://127.0.0.1/live/p1|rtmp://127.0.0.1/live/p2\"`\n2. 验证\n   `ffmpeg -i rtmp://127.0.0.1/live/p1 -i rtmp://127.0.0.1/live/p2`\n\n# ffmpeg滤镜使用\n\nhttp://ffmpeg.org/ffmpeg-filters.html\n\n## FFmpeg滤镜Filter描述格式\n\n1. FFmpeg滤镜Filter的参数排列方式\n   [输入流或标记]滤镜参数[临时标记名];[输入流或标记]滤镜参数[临时标记名]…\n   输入两个文件，一个视频，一个图片，将logo进行缩放，然后放在视频的左上角：\n\n   ```\n   ffmpeg -i input.mp4 -i input.jpg -filter_complex \" [1:v] scale=176:144[logo];[0:v][logo]overlay=x=0:y=0\" output.mp4\n   # [0:v]/[1:v]代表第几个输入的视频\n   ```\n\n2. FFmpeg为视频加水印\n\n- `drawtext`滤镜\n\n  ```\n  ffmpeg -h filter=drawtext\n  # 文字水印\n  ffmpeg -i input.mp4 -ss 50 -vf \"drawtext=fontsize=100:fontfile=/usr/share/fonts/truetype/freefont/FreeSerif.ttf :text='Hello World':fontcolor='yellow':x=20:y=20\" output.mp4\n  # 动态日期\n  ffmpeg -i input.mp4 -ss 50 -vf \"drawtext=fontsize=100:fontfile=/usr/share/fonts/truetype/freefont/FreeSerif.ttf :text='%{localtime\\:%Y\\-%m\\-%d %H-%M-%S}':fontcolor='yellow':x=20:y=20\" output.mp4\n  # 闪烁\n  ffmpeg -i input.mp4 -ss 50 -vf \"drawtext=fontsize=100:fontfile=/usr/share/fonts/truetype/freefont/FreeSerif.ttf :text='%{localtime\\:%Y\\-%m\\-%d %H-%M-%S}':fontcolor='yellow':x=20:y=20:enable=lt(mod(t\\,3)\\,1)\" output.mp4\n  ```\n\n- `movie`滤镜\n\n  ```\n  # 图片水印\n  ffmpeg -i input.mp4 -vf \"movie=logo.png[wm];[in][wm]overlay=30:10[out]\" output.mp4\n  # colorkey 半透明\n  ffmpeg -i input.mp4 -ss 55 -vf \"movie=../picture/3d_data.png,colorkey=black:1.0:0.1[wm];[in][wm]overlay=30:10[out]\" output.mp4\n  ```\n\n- `overlay`滤镜\n\n  ```\n  # 画中画\n  ffmpeg -re -i input.mp4 -vf \"movie=sub.mp4,scale=480x320[test];[in][test]overlay[out]\" -vcodec libx264 output.flv\n  # 跑马灯\n  ffmpeg -re -i input.mp4 -vf \"movie=sub.wmv,scale=480x320[test];[in][test]overlay=x='if(gte(t,2), -w+(t-2)*50, NAN)':y=0[out]\" -vcodec libx264 output.flv\n  # 视频多宫格处理\n  ffmpeg -i input1.mp4 -i input2.mp4 -i input3.mp4 -i input4.mp4 -filter_complex \"\n  nullsrc=size=1280x720 [background];\n  [0:v] setpts=PTS-STARTPTS, scale=640x360 [upleft];\n  [1:v] setpts=PTS-STARTPTS, scale=640x360 [upright];\n  [2:v] setpts=PTS-STARTPTS, scale=640x360 [downleft];\n  [3:v] setpts=PTS-STARTPTS, scale=640x360 [downright];\n  [background][upleft] overlay=shortest=1 [background+upleft];\n  [background+upleft][upright] overlay=shortest=1:x=640 [background+up];\n  [background+up][downleft] overlay=shortest=1:y=360 [background+up+downleft];\n  [background+up+downleft][downright] overlay=shortest=1:x=640:y=360\n  \" output.mp4\n  ```\n\n## FFmpeg音频流滤镜操作\n\n1. 双声道合并单声道\n   `fmpeg -i input.mp3 -ac 1 output.mp3`\n\n2. 双声道提取\n\n   - map_channel\n     `ffmpeg -i input.mp3 -map_channel 0.0.0 left.mp3 -map_channel 0.0.1 right.mp3`\n\n   - pan\n     `ffmpeg -i input.mp3 -filter_complex \"[0:0]pan=1c|c0=c0[left];[0:0]pan=1c|c0=c1[right]\" -map \"[left]\" left.mp3 -map \"[right]\" right.mp3`\n\n1. 双声道转双音频流\n\n   ```\n   ffmpeg -i input.mp4 -filter_complex channelsplit=channel_layout=stereo output.mka\n   ffprobe output.mka\n   # 可以看到有两个stream\n   ```\n\n不常用，大多数播放器也只会播放第一个流\n\n1. 单声道转双声道\n   `ffmpeg -i left.aac -ac 2 output.m4a`\n   这样的双声道并不是真正的双声道，而是单声道处理成的多声道，效果不会比原来多声道效果好\n\n2. 两个音频源合并双声道\n   `ffmpeg -i left.mp3 -i right.mp3 -filter_complex \"[0:a][1:a]amerge=inputs=2[aout]\" -map \"[aout]\" output.mka`\n\n3. 多个音频合并为多声道\n\n   ```\n   ffmpeg -i front_left.wav -i front_right.wav -i front_center.wav -i lfe.wav -i back_left.wav -i back_right.wav -filter_complex \"[0:a][1:a][2:a][3:a][4:a][5:a]amerge=inputs=6[aout]\" -map \"[aout]\" output.wav\n   ```\n\n   ## FFmpeg音频音量探测\n\n4. 音频音量获得\n   `ffmpeg -i input.wav -filter_complex volumedetect -f null -`\n\n5. 绘制音频波形\n\n   ```\n   ffmpeg -i input.wav -filter_complex \"showwavespic=s=640x120\" -frames:v 1 output.png\n   # 不通声道的波形图\n   ffmpeg -i input.wav -filter_complex \"showwavespic=s=640x120:split_channels=1\" -frames:v 1 output.png\n   ```\n\n## FFmpeg为视频加字母\n\n1. ASS字母流写入视频流\n   `ffmpeg -i input.mp4 -vf ass=t1.ass -f mp4 output.mp4`\n\n2. ASS字母流写入封装容器\n\n   ```\n   ffmpeg -i input.mp4 -vf ass=t1.ass -acodec copy -vcodec copy -scodec copy output.mp4\n   # 输入的视频文件汇总原本同样带有字幕流，希望使用t1.ass字幕流，通过map写入\n   # 下面命令会分别将第一个输入文件的第一个流和第二个流与第二个输入文件的第一个流写入output.mkv\n   ffmpeg -i input.mp4 -i t1.ass -map 0:0 -map 0:1 -map 1:0 -acodec copy -vcodec copy -scodec copy output.mkv\n   ```\n\n## FFmpeg视频抠图合并\n\n1. chromakey 抠图和背景视频合并的操作\n\n   ```\n   # 查询颜色支持\n   ffmpeg -colors\n   # chromakey滤镜将绿色背景中的人物抠出来，贴到input.mp4为背景的视频中\n   ffmpeg -i input.mp4 -i input_green.mp4 -filter_complex \"[1:v]chromakey=Green:0.1:0.2[ckout];[0:v][ckout]overlay[out]\" -map \"[out]\" output.mp4\n   # FFmpeg中除了有chromakey滤镜外，还有colorkey参数，chromakey滤镜主要用于YUV数据，所以一般来说做绿幕处理更有优势；而colorkey处理纯色均可以，因为colorkey主要用于RGB数据。\n   ```\n\n## FFmpeg 3D视频处理\n\n- ```\n  stereo3d\n  ```\n\n  滤镜\n\n  ```\n  # 黄蓝\n  ffplay -vf \"stereo3d=sbsl:aybd\" AVC_high_1280x720_2013.mp4\n  # 红蓝\n  ffplay -vf \"stereo3d=sbsl:aybg\" AVC_high_1280x720_2013.mp4\n  ```\n\n## FFmpeg定时视频截图\n\n- `vframe`参数截取一张图片\n  `ffmpeg -i input.flv -ss 00:00:7.435 -vframes 1 output.png`\n\n- ```\n  fps\n  ```\n\n  滤镜定时获得图片\n\n  ```\n  # 每隔1秒钟生成一张PNG图片\n  ffmpeg -i input.flv -vf fps=1 out%d.png\n  # 每隔一封中生成一张jpg图片\n  ffmpeg -i input.flv -vf fps=1/60 out%d.jpg\n  # select 按照关键帧截取图片\n  ffmpeg -i input.flv -vf \"select='eq(pict_type,PICT_TYPE_I)'\" -vsync vfr thumb%04d.png\n  ```\n\n## FFmpeg 生成测试源数据\n\n1. 音频测试流\n   lavfi 模拟音频源的abuffer、aevalsrc、anullsrc、flite、anoisesrc、sine滤镜生成音频流\n\n   ```\n   # 白噪声\n   ffmpeg -re -f lavfi -i aevalsrc=\"-2+random(0)\" -t 5 output.mp3\n   # 正弦波\n   ffmpeg -re -f lavfi -i \"sine\" -t 5 output.mp3\n   ```\n\n2. 视频测试流\n   通过FFmpeg模拟多种视频源：allrgb、allyuv、color、haldclutsrc、nullsrc、rgbtestsrc、smptebars、smptehdbars、testsrc、testsrc2、yuvtestsrc\n\n   ```\n   # 生成长度为5.3秒、图像大小为QCIF分辨率、帧率为25fps的视频图像数据，并编码成H.264\n   ffmpeg -re -f lavfi -i testsrc=duration=5.3:size=qcif:rate=25 -vcodec libx264 -r:v 25 output.mp4\n   # 纯红\n   ffmpeg -re -f lavfi -i color=c=red@0.2:s=qcif:r=25 -vcodec libx264 -r:v 25 output.mp4\n   # 随机雪花\n   ffmpeg -re -f lavfi -i \"nullsrc=s=256x256,geq=random(1)*255:128:128\" -vcodec libx264 -r:v 25 output.mp4\n   ```\n\n## FFmpeg对音视频倍速处理\n\n1. `atempo`音频倍速处理\n   取值范围：0.5 ~ 2.0\n\n   ```\n   # 半速处理\n   ffmpeg -i input.wav -filter_complex \"atempo=tempo=0.5\" -acodec aac output.aac\n   ```\n\n2. `setpts`视频倍速处理\n   使用PTS控制播放速度的\n\n   ```\n   # 半速处理\n   ffmpeg -re -i input.mp4 -filter_complex \"setpts=PTS*2\" output.mp4\n   ```\n\n# ffmpeg采集设备\n\n1. Linux下查看设备列表\n   `ffmpeg -h demuxer=fbdev`\n\n2. Linux采集设备fbdev\n   FrameBuffer是一个比较有年份的设备，专门用于图像展示操作，早期的图形界面也是基于FrameBuffer进行绘制的，有时在向外界展示Linux的命令行操作又不希望别人看到你的桌面时，可以通过获取FrameBuffer设备图像数据进行编码后推流或录制：\n\n   ```\n   ffmpeg -framerate 30 -f fbdev -i /dev/fb0 output.mp4\n   # ctrl+alt+F1 进入命令行界面\n   # ctrl+alt+F7 进入图形界面\n   ```\n\n3. Linux采集设备v4l2\n   v4l2主要用来采集摄像头，而摄像头通常支持多种像素格式，有些摄像头还支持直接输出已经编码好的H.264数据\n\n   - 查看参数\n     `ffmpeg -h demuxer=v4l2`\n\n   - 查看v4l2摄像头锁支持的色彩格式及分辨率\n     `ffmpeg -hide_banner -f v4l2 -list_formats all -i /dev/vide0`\n\n   - 采集摄像头\n     `ffmpeg -hide_banner -s 1920x1080 -i /dev/vide0 output.avi`\n\n1. Linux采集设备x11grab\n\n   Linux下面采集桌面图像时，通常采用x11grab设备采集图像，输入设备的设备名规则：\n\n   [主机名]: 显示编号id.屏幕编号id+起始x轴,起始y轴\n\n   ```\n   # 桌面录制(帧率:25,图像分辨率:1366x768,采集的设备:0.0)\n   ffmpeg -f x11grab -framerate 25 -video_size 1366x768 -i :0.0 out.mp4\n   # 桌面录制指定起始位置(:0.0+300,200 指定了x坐标300,y坐标200)\n   # 注意:video_size不要超过实际采集区域的大小\n   ffmpeg -f x11grab -framerate 25 -video_size 352x288 -i :0.0+300,200 out.mp4\n   # 桌面录制带鼠标记录的视频\n   ffmpeg -f x11grab -video_size 1366x768 -follow_mouse 1 -i :0.0 out.mp4\n   ```\n\n# 其他\n\n## x265安装\n\n1. 下载\n   网站1：http://www.videolan.org/developers/x265.html\n   `hg clone http://hg.videolan.org/x265`\n   网站2：https://bitbucket.org/multicoreware/x265\n   `hg clone https://bitbucket.org/multicoreware/x265`\n\n2. 编译\n\n   ```\n   sudo apt-get install mercurial cmake cmake-curses-gui build-essential yasm\n   cd x265/build/linux\n   ./make-Makefiles.bash\n   make\n   sudo make install\n   ```\n\n## DTS、PTS 的概念\n\nDTS、PTS 的概念如下所述：\n\n- DTS（Decoding Time Stamp）：即解码时间戳，这个时间戳的意义在于告诉播放器该在什么时候解码这一帧的数据。\n- PTS（Presentation Time Stamp）：即显示时间戳，这个时间戳用来告诉播放器该在什么时候显示这一帧的数据。\n\n需要注意的是：虽然 DTS、PTS 是用于指导播放端的行为，但它们是在编码的时候由编码器生成的。\n\n当视频流中没有 B 帧时，通常 DTS 和 PTS 的顺序是一致的。但如果有 B 帧时，就回到了我们前面说的问题：解码顺序和播放顺序不一致了。\n\n比如一个视频中，帧的显示顺序是：I B B P，现在我们需要在解码 B 帧时知道 P 帧中信息，因此这几帧在视频流中的顺序可能是：I P B B，这时候就体现出每帧都有 DTS 和 PTS 的作用了。DTS 告诉我们该按什么顺序解码这几帧图像，PTS 告诉我们该按什么顺序显示这几帧图像。顺序大概如下：\n\n```\n   PTS: 1 4 2 3\n   DTS: 1 2 3 4\nStream: I P B B\n```\n\n## 其他常用命令\n\n1、将文件当作源推送到RTMP服务器\n\n```\nffmpeg -re -i localFile.mp4 -c copy -f flv rtmp://server/live/streamName\n```\n\n参数解释\n-r 以本地帧频读数据，主要用于模拟捕获设备。表示ffmpeg将按照帧率发送数据，不会按照最高的效率发送\n\n2、将直播文件保存至本地\n\n```\nffmpeg -i rtmp://server/live/streamName -c copy dump.flv\n```\n\n3、将其中一个直播流中的视频改用H.264压缩，音频不变，推送到另外一个直播服务器\n\n```\nffmpeg -i rtmp://server/live/originalStream -c:a copy -c:v libx264 -vpre slow -f flv rtmp://server/live/h264Stream\n```\n\n4、将其中一个直播流中的视频改用H.264压缩，音频改用aac压缩，推送到另外一个直播服务器\n\n```\nffmpeg -i rtmp://server/live/originalStream -c:a libfaac -ar 44100 -ab 48k -c:v libx264 -vpre slow -vpre baseline -f flv rtmp://server/live/h264Stream\n```\n\n5、将其中一个直播流中的视频不变，音频改用aac压缩，推送到另外一个直播服务器\n\n```\nffmpeg -i rtmp://server/live/originalStream -acodec libfaac -ar 44100 -ab 48k -vcodec copy -f flv rtmp://server/live/h264_AAC_Stream\n```\n\n6、将一个高清流复制为几个不同清晰度的流重新发布，其中音频不变\n\n```\nffmpeg -re -i rtmp://server/live/high_FMLE_stream -acodec copy -vcodec x264lib -s 640×360 -b 500k -vpre medium -vpre baseline rtmp://server/live/baseline_500k -acodec copy -vcodec x264lib -s 480×272 -b 300k -vpre medium -vpre baseline rtmp://server/live/baseline_300k -acodec copy -vcodec x264lib -s 320×200 -b 150k -vpre medium -vpre baseline rtmp://server/live/baseline_150k -acodec libfaac -vn -ab 48k rtmp://server/live/audio_only_AAC_48k\n```\n\n7、将当前摄像头以及扬声器通过DSHOW采集，使用H.264/AAC压缩后推送到RTMP服务器\n\n```\nffmpeg -r 25 -f dshow -s 640×480 -i video=”video source name”:audio=”audio source name” -vcodec libx264 -b 600k -vpre slow -acodec libfaac -ab 128k -f flv rtmp://server/application/stream_name\n```\n\n8、将一个JPG图片经过H.264压缩后输出为MP4文件\n\n```\nffmpeg -i INPUT.jpg -an -vcodec libx264 -coder 1 -flags +loop -cmp +chroma -subq 10 -qcomp 0.6 -qmin 10 -qmax 51 -qdiff 4 -flags2 +dct8x8 -trellis 2 -partitions +parti8x8+parti4x4 -crf 24 -threads 0 -r 25 -g 25 -y OUTPUT.mp4\n```\n\n9、将MP3转化为AAC\n\n```\nffmpeg -i 20120814164324_205.wav -acodec  libfaac -ab 64k -ar 44100  output.aac\n```\n\n10、将AAC文件转化为flv文件，编码格式采用AAC\n\n```\nffmpeg -i output.aac -acodec libfaac -y -ab 32 -ar 44100 -qscale 10 -s 640*480 -r 15 outp\n```\n\n"
        },
        {
          "name": "ffmpeg源码example解析之decode_audio.md",
          "type": "blob",
          "size": 1.33984375,
          "content": "# 音频播放\n\n## 源码文件\n\n```\n<ffmpeg>/doc/examples/decode_audio.c\n<ffmpeg>/doc/examples/muxing.c\n<ffmpeg>/doc/examples/resampling_audio.c\n<ffmpeg>/doc/examples/transcode_aac.c\n```\n\n## 代码调用流程\n\n代码流程跟《ffmpeg源码example解析之decode-video》基本类似，主要的区别是在播放上。对于播放器来说需要设置一套播放参数，比如：采样率，通道数，大小端，采样大小以及数据类型。理论上可以通过ffmpeg解析出来的的这些参数设置给播放器，但是ffmpeg的format的跟播放器的format不是同一个枚举，需要建立一个映射关系，所以觉得一般的播放器会统一设置成固定值，比如：\n采样率：44100，通道数：2，大小端：LittleEndian，采样大小：16bit，数据类型：有符号\n也就是说无论输入是什么配置参数，统一重采样成这套参数跟播放匹配。\n\n# 项目\n\n该项目用于学习ffmpeg编解码\n使用qt主要是比较方便，后续采用glfw和sdl显示\n\n## 功能描述\n\n### 播放视频\n\n1. 使用FFMpeg解码\n2. 使用sws_scale将FFMpeg解码后的yuv数据转换成rgb\n3. 使用QT的QLabel组件，通过QImage显示rgb数据\n\n### 播放音频\n\n1. 使用FFMpeg解码\n2. 使用swr_convert将FFMpeg解码后的一帧数据转换成播放器指定的播放参数\n3. 使用QT的QAudioOutput播放pcm数据\n"
        },
        {
          "name": "ffmpeg源码example解析之decode_video.md",
          "type": "blob",
          "size": 2.568359375,
          "content": "# 视频解码流程\n\n## 源码文件\n\n```\n<ffmpeg>/doc/examples/decode_video.c\n```\n\n## 代码调用流程\n\n[![image](https://xuleilx.github.io/images/decode_video.png)](https://xuleilx.github.io/images/decode_video.png)\n\n该流程并不是一个正常的流程，它假设了该文件是mpeg1video的编码格式，并且没有封装容器。\n通常情况下是需要解封装的，比如说拿到一个视频文件，并不知道是什么编码，这时候就需要解封装来了解容器里面数据流了。\n\n首先我们先要了解ffmpeg的几个大类：\n\n- AVFormat：封装、解封装、包含协议封装\n- AVCodec：编解码\n- AVFilter：音视频滤镜\n- swscale：视频图像转换\n- swresample：音频转换计算\n- AVUtil :工具类\n\n# 视频解封装，解码，图像转换流程\n\n根据ffmpeg的几个大类，介绍解码视频并显示的一般流程和操作，序号为程序调用顺序\n\n## AVFormat：封装、解封装、包含协议封装\n\n### 解封装\n\n```c\navformat_alloc_context \t#封装结构体分配内存 // 可以不调用，avformat_open_input会判断入参是否为NULL，自行分配\navformat_open_input     \t#打开输入文件用于读取数据\navformat_find_stream_info#获取流信息\n针对每个stream处理\n    - pFormatContext->nb_streams\n    - avcodec_find_decoder \t#根据流中的编码参数AVCodecParameters，查找是否支持该编码\n    - 判断流的类型 pLocalCodecParameters->codec_type\n    - 保存AVCodecParameters和AVCodec，用于后续处理\n    \nav_read_frame\t\t\t#读取一包AVPacket数据包\n```\n\n## AVCodec：编解码\n\n### 解码\n\n```c\navcodec_alloc_context3 \t\t#编解码结构体分配内存\navcodec_parameters_to_context#将解封装得到的编码参数AVCodecParameters赋值给编解码结构体\navcodec_open2 \t\t\t\t#打开编码器\navcodec_send_packet \t\t#将解封装中得到的AVPacket数据包送给解码器\navcodec_receive_frame \t\t#读回一帧解码后的数据AVFrame\n```\n\n### AVPacket：压缩的数据包\n\n```\nav_packet_alloc \t\t#压缩的数据包分配内存\n```\n\n## swscale：视频图像转换\n\n```\nsws_getContext \t\t#给SwsContext结构体分配内存\n\nsws_scale \t\t\t#视频图像转换\n```\n\n## AVUtil :工具类\n\n### AVFrame：解码后的数据帧\n\n```\nav_frame_alloc \t\t#解码后的数据帧分配内存\n```\n\n### image\n\n```\nav_image_alloc \t\t#分配内存用于存放一张图片\n```\n\n# 项目\n\n该项目用于学习ffmpeg编解码\n使用qt主要是比较方便，后续采用glfw和sdl显示\n\n## 功能描述\n\n1. 使用FFMpeg解码\n2. 将FFMpeg解码后的yuv数据转换成rgb\n3. 使用QT的QLabel组件，通过QImage显示rgb数据\n"
        },
        {
          "name": "ffplay源码和书籍",
          "type": "tree",
          "content": null
        },
        {
          "name": "iOS资料",
          "type": "tree",
          "content": null
        },
        {
          "name": "paper",
          "type": "tree",
          "content": null
        },
        {
          "name": "teaching video",
          "type": "tree",
          "content": null
        },
        {
          "name": "使用FFMpeg进行H264编码.c",
          "type": "blob",
          "size": 3.9814453125,
          "content": "/**\n使用FFMpeg可以很方便的对音视频进行编码，并且写文件。\n\n下面的代码是将5幅1280*720大小的图片进行编码，并且写到文件中。\n\n代码有些乱，但希望能抛砖引玉，对学习这方面的朋友有帮助。\n*/\n\n\nCFile file[5];\nBYTE *szTxt[5];\n\nint nWidth = 0;\nint nHeight= 0;\n\nint nDataLen=0;\n\nint nLen;\n\nCString csFileName;\nfor (int fileI = 1; fileI <= 5; fileI ++)\n{\n  csFileName.Format(\"e:\\\\pics\\\\%d.bmp\", fileI);\n  file[fileI - 1].Open(csFileName,CFile::modeRead | CFile::typeBinary);\n  nLen = file[fileI - 1].GetLength();\n\n  szTxt[fileI -1] = new BYTE[nLen];\n  file[fileI - 1].Read(szTxt[fileI - 1], nLen);\n  file[fileI - 1].Close();\n\n  //BMP bmi;//BITMAPINFO bmi;\n  //int nHeadLen = sizeof(BMP);\n  BITMAPFILEHEADER bmpFHeader;\n  BITMAPINFOHEADER bmpIHeader;\n  memcpy(&bmpFHeader,szTxt[fileI -1],sizeof(BITMAPFILEHEADER));\n\n  int nHeadLen = bmpFHeader.bfOffBits - sizeof(BITMAPFILEHEADER);\n  memcpy(&bmpIHeader,szTxt[fileI - 1]+sizeof(BITMAPFILEHEADER),nHeadLen);\n\nnWidth = bmpIHeader.biWidth;// 464;// bmi.bmpInfo.bmiHeader.biWidth;// ;\n  nHeight = bmpIHeader.biHeight;//362;// bmi.bmpInfo.bmiHeader.biHeight;// ;\n\n  szTxt[fileI - 1] += bmpFHeader.bfOffBits;\n  nDataLen = nLen-bmpFHeader.bfOffBits;\n}\n\nav_register_all();\navcodec_register_all();\nAVFrame *m_pRGBFrame =  new AVFrame[1];  //RGB帧数据\nAVFrame *m_pYUVFrame = new AVFrame[1];;  //YUV帧数据\nAVCodecContext *c= NULL;\nAVCodecContext *in_c= NULL;\nAVCodec *pCodecH264; //编码器\nuint8_t * yuv_buff;//\n\n//查找h264编码器\npCodecH264 = avcodec_find_encoder(CODEC_ID_H264);\nif(!pCodecH264)\n{\n  fprintf(stderr, \"h264 codec not found\\n\");\n  exit(1);\n}\n\nc= avcodec_alloc_context3(pCodecH264);\nc->bit_rate = 3000000;// put sample parameters\nc->width =nWidth;//\nc->height = nHeight;//\n\n// frames per second\nAVRational rate;\nrate.num = 1;\nrate.den = 25;\nc->time_base= rate;//(AVRational){1,25};\nc->gop_size = 10; // emit one intra frame every ten frames\nc->max_b_frames=1;\nc->thread_count = 1;\nc->pix_fmt = PIX_FMT_YUV420P;//PIX_FMT_RGB24;\n\n//av_opt_set(c->priv_data, /*\"preset\"*/\"libvpx-1080p.ffpreset\", /*\"slow\"*/NULL, 0);\n//打开编码器\nif(avcodec_open2(c,pCodecH264,NULL)<0)\n  TRACE(\"不能打开编码库\");\n\nint size = c->width * c->height;\n\nyuv_buff = (uint8_t *) malloc((size * 3) / 2); // size for YUV 420\n\n//将rgb图像数据填充rgb帧\nuint8_t * rgb_buff = new uint8_t[nDataLen];\n\n//图象编码\nint outbuf_size=100000;\nuint8_t * outbuf= (uint8_t*)malloc(outbuf_size);\nint u_size = 0;\nFILE *f=NULL;\nchar * filename = \"e:\\\\pics\\\\myData.h264\";\nf = fopen(filename, \"wb\");\nif (!f)\n{\n  TRACE( \"could not open %s\\n\", filename);\n  exit(1);\n}\n\n//初始化SwsContext\nSwsContext * scxt = sws_getContext(c->width,c->height,PIX_FMT_BGR24,c->width,c->height,PIX_FMT_YUV420P,SWS_POINT,NULL,NULL,NULL);\n\nAVPacket avpkt;\n\n//AVFrame *pTFrame=new AVFrame\nfor (int i=0;i<250;++i)\n{\n\n  //AVFrame *m_pYUVFrame = new AVFrame[1];\n\n  int index = (i / 25) % 5;\n  memcpy(rgb_buff,szTxt[index],nDataLen);\n\n  avpicture_fill((AVPicture*)m_pRGBFrame, (uint8_t*)rgb_buff, PIX_FMT_RGB24, nWidth, nHeight);\n\n  //将YUV buffer 填充YUV Frame\n  avpicture_fill((AVPicture*)m_pYUVFrame, (uint8_t*)yuv_buff, PIX_FMT_YUV420P, nWidth, nHeight);\n\n  // 翻转RGB图像\n  m_pRGBFrame->data[0]  += m_pRGBFrame->linesize[0] * (nHeight - 1);\n  m_pRGBFrame->linesize[0] *= -1;\n  m_pRGBFrame->data[1]  += m_pRGBFrame->linesize[1] * (nHeight / 2 - 1);\n  m_pRGBFrame->linesize[1] *= -1;\n  m_pRGBFrame->data[2]  += m_pRGBFrame->linesize[2] * (nHeight / 2 - 1);\n  m_pRGBFrame->linesize[2] *= -1;\n\n\n  //将RGB转化为YUV\n  sws_scale(scxt,m_pRGBFrame->data,m_pRGBFrame->linesize,0,c->height,m_pYUVFrame->data,m_pYUVFrame->linesize);\n\n  int got_packet_ptr = 0;\n  av_init_packet(&avpkt);\n  avpkt.data = outbuf;\n  avpkt.size = outbuf_size;\n  u_size = avcodec_encode_video2(c, &avpkt, m_pYUVFrame, &got_packet_ptr);\n  if (u_size == 0)\n  {\n   fwrite(avpkt.data, 1, avpkt.size, f);\n  }\n}\n\nfclose(f);\ndelete []m_pRGBFrame;\ndelete []m_pYUVFrame;\ndelete []rgb_buff;\nfree(outbuf);\navcodec_close(c);\nav_free(c);\n"
        },
        {
          "name": "分布式视频处理系统设计与实现.pdf",
          "type": "blob",
          "size": 8376.703125,
          "content": ""
        },
        {
          "name": "基于Android的H.264_AVC解码器的设计与实现.pdf",
          "type": "blob",
          "size": 2390.203125,
          "content": null
        },
        {
          "name": "基于FFMPEG的视频转换系统.pdf",
          "type": "blob",
          "size": 17325.6875,
          "content": ""
        },
        {
          "name": "基于FFMPEG的跨平台视频编解码研究.pdf",
          "type": "blob",
          "size": 304.90625,
          "content": null
        },
        {
          "name": "基于FFMPEG解码的音视频同步实现.pdf",
          "type": "blob",
          "size": 1042.3125,
          "content": null
        },
        {
          "name": "基于FFMpeg的稳定应用层组播流媒体直播系统研究.pdf",
          "type": "blob",
          "size": 6373.65625,
          "content": ""
        },
        {
          "name": "基于FFmpeg和SDL的智能录屏及播放系统.pdf",
          "type": "blob",
          "size": 6368.953125,
          "content": ""
        },
        {
          "name": "基于FFmpeg和SDL的视频流播放存储研究综述.pdf",
          "type": "blob",
          "size": 1284.078125,
          "content": null
        },
        {
          "name": "基于FFmpeg的H.264解码器实现.pdf",
          "type": "blob",
          "size": 3137.921875,
          "content": null
        },
        {
          "name": "基于FFmpeg的网络视频监控系统的设计与实现.pdf",
          "type": "blob",
          "size": 2403.421875,
          "content": null
        },
        {
          "name": "基于FFmpeg的视频转码与保护系统的设计与实现.pdf",
          "type": "blob",
          "size": 7211.78125,
          "content": ""
        },
        {
          "name": "基于FFmpeg的高清实时直播系统设计与实现.pdf",
          "type": "blob",
          "size": 3877.4375,
          "content": null
        },
        {
          "name": "基于H.264与H.265的低延时视频监控系统的设计与实现.pdf",
          "type": "blob",
          "size": 5234.796875,
          "content": ""
        },
        {
          "name": "基于H.265的无线视频监控系统设计与实现.pdf",
          "type": "blob",
          "size": 7990.578125,
          "content": ""
        },
        {
          "name": "基于H.265的视频教育系统的设计与实现.pdf",
          "type": "blob",
          "size": 1975.625,
          "content": null
        },
        {
          "name": "基于Hadoop的视频转码优化的研究.pdf",
          "type": "blob",
          "size": 2583.78125,
          "content": null
        },
        {
          "name": "基于RTMP协议的流媒体系统的设计实现.pdf",
          "type": "blob",
          "size": 4054.328125,
          "content": null
        },
        {
          "name": "基于RTMP的高清流媒体直播点播封装技术的研究与实现.caj",
          "type": "blob",
          "size": 1081.546875,
          "content": null
        },
        {
          "name": "基于RTSP协议的iOS视频播放器的设计与实现.pdf",
          "type": "blob",
          "size": 2501.96875,
          "content": null
        },
        {
          "name": "基于RTSP协议的多源视音频实时直播系统的设计与实现.pdf",
          "type": "blob",
          "size": 9257.953125,
          "content": ""
        },
        {
          "name": "基于RTSP的H.264实时流媒体传输方案的研究与实现.pdf",
          "type": "blob",
          "size": 2683.921875,
          "content": null
        },
        {
          "name": "基于RTSP的音视频传输系统研究与实现.pdf",
          "type": "blob",
          "size": 9114.5,
          "content": ""
        },
        {
          "name": "基于TCP传输的嵌入式流媒体播放系统.pdf",
          "type": "blob",
          "size": 14260.453125,
          "content": ""
        },
        {
          "name": "基于ffmpeg的高性能高清流媒体播放器软件设计.pdf",
          "type": "blob",
          "size": 6788.71875,
          "content": ""
        },
        {
          "name": "基于流媒体技术的移动视频直播系统的设计与实现.pdf",
          "type": "blob",
          "size": 2163.1875,
          "content": null
        },
        {
          "name": "文档库",
          "type": "tree",
          "content": null
        },
        {
          "name": "直播聚合平台的设计与实现.pdf",
          "type": "blob",
          "size": 11308.484375,
          "content": ""
        },
        {
          "name": "音视频信号采集压缩及传输系统的设计与实现.pdf",
          "type": "blob",
          "size": 2708.453125,
          "content": null
        }
      ]
    }
  ]
}