{
  "metadata": {
    "timestamp": 1736709982936,
    "page": 551,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjU2MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "antirez/neural-redis",
      "stars": 2224,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.009765625,
          "content": "*.so\n*.xo\n"
        },
        {
          "name": "COPYING",
          "type": "blob",
          "size": 1.4482421875,
          "content": "Copyright (c) 2016, Salvatore Sanfilippo\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\n    * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\n    * Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\n    * Neither the name of Disque nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n"
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 1.111328125,
          "content": "# find the OS\nuname_S := $(shell sh -c 'uname -s 2>/dev/null || echo not')\n\n# Compile flags for linux / osx\nifeq ($(uname_S),Linux)\n\tSHOBJ_CFLAGS ?= -Wall -W -O3 -fno-common -g -ggdb -std=c99\n\tSHOBJ_LDFLAGS ?= -shared\nelse\n\tSHOBJ_CFLAGS ?= -Wall -W -O3 -dynamic -fno-common -g -ggdb -std=c99\n\tSHOBJ_LDFLAGS ?= -bundle -undefined dynamic_lookup\nendif\n\n.SUFFIXES: .c .so .xo .o\n\nall:\n\t@echo \"\"\n\t@echo \"Make neon     -- Faster if you have a modern ARM CPU.\"\n\t@echo \"Make sse     -- Faster if you have a modern CPU.\"\n\t@echo \"Make avx     -- Even faster if you have a modern CPU.\"\n\t@echo \"Make generic -- Works everywhere.\"\n\t@echo \"\"\n\t@echo \"The avx code uses AVX2, it requires Haswell (Q2 2013) or better.\"\n\t@echo \"\"\n\ngeneric: neuralredis.so\nneon:\n\tmake neuralredis.so CFLAGS=-DUSE_NEON\n\nsse:\n\tmake neuralredis.so CFLAGS=-DUSE_SSE SSE=\"-msse3\"\n\navx:\n\tmake neuralredis.so CFLAGS=-DUSE_AVX AVX=\"-mavx2 -mfma\"\n\n.c.xo:\n\t$(CC) -I. $(CFLAGS) $(SHOBJ_CFLAGS) $(AVX) $(SSE) -fPIC -c $< -o $@\n\nnn.c: nn.h\n\nneuralredis.xo: redismodule.h\n\nneuralredis.so: neuralredis.xo nn.xo\n\t$(LD) -o $@ $< nn.xo $(SHOBJ_LDFLAGS) $(LIBS) -lc\n\nclean:\n\trm -rf *.xo *.so\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 41.8046875,
          "content": "Neural Redis\n===\n\n*Machine learning is like highschool sex. Everyone says they do it, nobody really does, and no one knows what it actually is.* -- [@Mikettownsend](https://twitter.com/Mikettownsend/status/780453119238955008).\n\nNeural Redis is a Redis loadable module that implements feed forward neural\nnetworks as a native data type for Redis. The project goal is to provide\nRedis users with an extremely simple to use machine learning experience.\n\nNormally machine learning is operated by collecting data, training some\nsystem, and finally executing the resulting program in order to solve actual\nproblems. In Neural Redis all this phases are compressed into a single API:\nthe data collection and training all happen inside the Redis server.\nNeural networks can be executed while there is an ongoing training, and can\nbe re-trained multiple times as new data from the outside is collected\n(for instance user events).\n\nThe project starts from the observation that, while complex problems like\ncomputer vision need slow to train and complex neural networks setups, many\nregression and classification problems that are able to enhance the user\nexperience in many applications, are approachable by feed forward fully\nconnected small networks, that are very fast to train, very generic, and\nrobust against non optimal parameters configurations.\n\nNeural Redis implements:\n\n* A very simple to use API.\n* Automatic data normalization.\n* Online training of neural networks in different threads.\n* Ability to use the neural network while the system is training it (we train a copy and only later merge the weights).\n* Fully connected neural networks using the RPROP (Resilient back propagation) learning algorithm.\n* Automatic training with simple overtraining detection.\n\nThe goal is to help developers, especially of mobile and web applications, to\nhave a simple access to machine learning, in order to answer questions like:\n\n* What promotion could work most likely with this user?\n* What AD should I display to obtain the best conversion?\n* What template is the user likely to appreciate?\n* What is a likely future trend for this data points?\n\nOf course you can do more, since neural networks are pretty flexible. You\ncan even have fun with computer visions datasets like\n[MINST](http://yann.lecun.com/exdb/mnist/), however keep in mind that\nthe neural networks implemented in Neural Redis are not optimized for\ncomplex computer visions tasks like convolutional networks (it will\nscore 2.3%, very far from the state of art!), nor Neural Redis implements\nthe wonders of recurrent neural networks.\n\nHowever you'll be surpirsed by the number of tasks in which a simple\nneural network that can be trained in minutes, will be able to discover\nlinear ad non linear correlations.\n\nLoading the extension\n===\n\nTo run this extension you need Redis `unstable`, grab it from Github, it\nis the default branch. Then compile the extension, and load it starting\nRedis with:\n\n    redis-server --loadmodule /path/to/neuralredis.so\n\nAlternatively add the following in your `redis.conf` file:\n\n    loadmodule /path/to/neuralredis.so\n\nWARNING: alpha code\n===\n\n**WARNING:** this is alpha code. It is likely to contain bugs and may\neasily crash the Redis server. Also note that currently only\nRDB persistence is implemented in the module, while AOF rewrite\nis not implemented at all. Use at your own risk.\n\nIf you are not still scared enough, please consider that I wrote the\nmore than 1000 lines of C code composing this extension, and this\nREADME file, in roughly two days.\n\nNote that this implementation may be hugely improved. For instance\ncurrently only the sigmoid activaction function and the root mean\nsquare loss functions are supported: while for the problems this\nmodule is willing to address this limited neural network implementation\nis showing to be quite flexible, it is possible to do much better\ndepending on the problem at hand.\n\nHello World\n===\n\nIn order to understand how the API works, here is an hello world example\nwhere we'll teach our neural network to do... additions :-)\n\nTo create a new neural network we use the following command:\n\n    > NR.CREATE net REGRESSOR 2 3 -> 1 NORMALIZE DATASET 50 TEST 10\n    (integer) 13\n\nThe command creates a neural network, configured for regression tasks\n(as opposed to classification: well'll explain what this means\nin the course of this tutorial).\n\nNote that the command replied with \"13\". It means that the network\nhas a total of 13 tunable parameters, considering all the weights\nthat go from units or biases to other units. Larger networks\nwill have a lot more parameters.\n\nThe neural network has 2 inputs, 3 hidden layers, and a single output.\nRegression means that given certain inputs and desired outputs, we want the\nneural network to be able to *understand* the function that given the\ninputs computes the outputs, and compute this function when new inputs\nare presented to it.\n\nThe `NORMALIZE` option means that it is up to Redis to normalize the\ndata it receives, so there is no need to provide data in the -/+ 1 range.\nThe options `DATASET 50` and `TEST 10` means that we want an internal\nmemory for the dataset of 50 and 10 items respectively for the training\ndataset, and the testing dataset.\n\nThe learning happens using the training dataset, while the testing dataset\nis used in order to detect if the network is able to generalize, that is,\nis really able to understand how to approximate a given function.\nAt the same time, the testing dataset is useful to avoid to train the network\ntoo much, a problem known as *overfitting*. Overfitting means that the\nnetwork becomes too much specific, at the point to be only capable of replying\ncorrectly to the inputs and outputs it was presented with.\n\nNow it is time to provide the network with some data, so that it can learn\nthe function we want to approximate:\n\n    > NR.OBSERVE net 1 2 -> 3\n    1) (integer) 1\n    2) (integer) 0\n\nWe are saying: given the inputs 1 and 2, the output is 3.\nThe reply to the `NR.OBSERVE` command is the number of data items\nstored in the neural network memory, respectively in the training\nand testing data sets.\n\nWe continue like that with other examples:\n\n    > NR.OBSERVE net 4 5 -> 9\n    > NR.OBSERVE net 3 4 -> 7\n    > NR.OBSERVE net 1 1 -> 2\n    > NR.OBSERVE net 2 2 -> 4\n    > NR.OBSERVE net 0 9 -> 9\n    > NR.OBSERVE net 7 5 -> 12\n    > NR.OBSERVE net 3 1 -> 4\n    > NR.OBSERVE net 5 6 -> 11\n\nAt this point we need to train the neural network, so that it\ncan learn:\n\n    > NR.TRAIN net AUTOSTOP\n\nThe `NR.TRAIN` command starts a training thread. the `AUTOSTOP` option\nmeans that we want the training to stop before overfitting starts\nto happen.\n\nUsing the `NR.INFO` command you can see if the network is still training.\nHowever in this specific case, the network will take a few milliseconds to\ntrain, so we can immediately try if it actually learned how to add two\nnumbers:\n\n    > NR.RUN net 1 1\n    1) \"2.0776522297040843\"\n\n    > NR.RUN net 3 2\n    1) \"5.1765427204933099\"\n\nWell, more or less it works. Let's look at some internal info now:\n\n    > NR.INFO net\n     1) id\n     2) (integer) 1\n     3) type\n     4) regressor\n     5) auto-normalization\n     6) (integer) 1\n     7) training\n     8) (integer) 0\n     9) layout\n    10) 1) (integer) 2\n        2) (integer) 3\n        3) (integer) 1\n    11) training-dataset-maxlen\n    12) (integer) 50\n    13) training-dataset-len\n    14) (integer) 6\n    15) test-dataset-maxlen\n    16) (integer) 10\n    17) test-dataset-len\n    18) (integer) 2\n    19) training-total-steps\n    20) (integer) 1344\n    21) training-total-seconds\n    22) 0.00\n    23) dataset-error\n    24) \"7.5369825612397299e-05\"\n    25) test-error\n    26) \"0.00042670663615723583\"\n    27) classification-errors-perc\n    28) 0.00\n\nAs you can see we have 6 dataset items and 2 test items. We configured\nthe network at creation time to have space for 50 and 10 items. As you add\nitems with `NR.OBSERVE` the network will put items evenly on both datasets,\nproportionally to their respective size. Finally when the datasets are full,\nold random entries are replaced with new\nones.\n\nWe can also see that the network was trained with 1344 steps for 0 seconds\n(just a few milliseconds). Each step is the training performed with a single\ndata item, so the same 6 items were presented to the network for 244 cycles\nin total.\n\nA few words about normalization\n===\n\nIf we try to use our network with values outside the range it learned with,\nwe'll see it failing:\n\n    > NR.RUN net 10 10\n    1) \"12.855978185382257\"\n\nThis happens because the automatic normalization will consider the maximum\nvalues seen in the training dataset. So if you plan to use auto normalization,\nmake sure to show the network samples with different values, including inputs\nat the maximum of the data you'll want to use the network with in the future.\n\nClassification tasks\n===\n\nRegression approximates a function having certain inputs and outputs in the\ntraining data set. Classification instead is the task of, given a set of\ninputs representing *something*, to label it with one of a fixed set of\nlabels.\n\nFor example the inputs may be features of Greek jars, and the classification\noutput could be one of the following three jar types:\n\n* Type 0: Kylix type A\n* Type 1: Kylix type B\n* Type 2: Kassel cup\n\nAs a programmer you may think that, the output class, is just a single output\nnumber. However neural networks don't work well this way, for example\nclassifying type 0 with an output between 0 and 0.33, type 1 with an output\nbetween 0.33 and 0.66, and finally type 2 with an output between 0.66 and 1,\nwill not work well at all.\n\nThe way to go instead is to use three distinct outputs, where we set two\nalways to 0, and a single one to 1, corresponding to the type the output\nrepresents, so:\n\n* Type 0: [1, 0, 0]\n* Type 1: [0, 1, 0]\n* Type 2: [0, 0, 1]\n\nWhen you create a neural network with the `NR.CREATE` command, and use as\nsecond argument `CLASSIFIER` instead of `REGRESSOR`, Neural Redis will do\nthe above transformation for you, so when you train your network with\n`NR.OBSERVE` you'll just use, as output, as single number: 0, 1 or 2.\n\nOf course, you need to create the network with three outputs like that:\n\n    > NR.CREATE mynet CLASSIFIER 5 10 -> 3\n    (integer) 93\n\nOur network is currently untrained, but it can already be run, even if the\nreplies it will provide are totally random:\n\n    > NR.RUN mynet 0 1 1 0 1\n    1) \"0.50930603602918945\"\n    2) \"0.48879876200255651\"\n    3) \"0.49534453421381375\"\n\nAs you can see, the network *voted* for type 0, since the first output is\ngreater than the others. There is a Neural Redis command that saves you the\nwork of finding the greatest output client side in order to interpret the\nresult as a number between 0 and 2. It is identical to `NR.RUN` but just\noutputs directly the class ID, and is called `NR.CLASS`:\n\n    > NR.CLASS mynet 0 1 1 0 1\n    (integer) 0\n\nHowever note that ofter `NR.RUN` is useful for classification problems.\nFor example a blogging platform may want to train a neural network to\npredict the template that will appeal more to the user, based on the\nregistration data we just obtained, that include the country, sex, age\nand category of the blog.\n\nWhile the prediction of the network will be the output with the highest\nvalue, if we want to present different templates, it makes sense to\npresent, in the listing, as the second one the one with the second\nhighest output value and so forth.\n\nBefore diving into a practical classification example, there is a last\nthing to say. Networks of type CLASSIFIER are also trained in a different\nway: instead of giving as output a list of zeros and ones you directly\nprovide to `NR.OBSERVE` the class ID as a number, so in the example\nof the jars, we don't need to write `NR.OBSERVE 1 0.4 .2 0 1 -> 0 0 1` to\nspecify as output of the provided data sample the third class, but\nwe should just write:\n\n    > NR.OBSERVE mynet 1 0.4 .2 0 1 -> 2\n\nThe \"2\" will be translated into \"0 0 1\" automatically, as \"1\"\nwould be translated to \"0 1 0\" and so forth.\n\nA practical example: the Titanic dataset\n===\n\n[Kaggle.com](https://www.kaggle.com/) is hosting a machine learning\ncompetition. One of the datasets they use, is the list of the Titanic\npassengers, their ticket class, fair, number of relatives, age,\nsex, and other information, and... If they survived or not during the\nTitanic incident.\n\nYou can find both the code and a CSV with a reduced dataset of 891\nentries in the `examples` directory of this Github repository.\n\nIn this example we are going to try to predict, given a few input\nvariables, if a specific person is going to survive or not, so this\nis a classification task, where we label persons with two different\nlabels: survived or died.\n\nThis problem is pretty similar, even if a bit more scaring, than the\nproblem of labeling users or predicting their response in some web\napplication according to their behavior and the other data we collected\nin the past (hint: machine learning is all about collecting data...).\n\nIn the CSV there are a number of information about each passenger,\nbut here in order to make the example simpler we'll use just the\nfollowing fields:\n\n* Ticket class (1st, 2nd, 3rd).\n* Sex.\n* Age.\n* Sibsp (Number of siblings, spouses aboard).\n* Parch (Number of parents and children aboard).\n* Ticket fare.\n\nIf there is a correlation between this input variables and the\nability to survive, our neural network should find it.\n\nNote that while we have six inputs, we'll need a total network\nwith 9 total inputs, since sex and ticket class, are actually\n*input classes*, so like we did in the output, we'll need to\ndo in the input. Each input will signal if the passenger is in\none of the possible classes. This are our nine inputs:\n\n* Is male? (0 or 1).\n* Is Female? (0 or 1).\n* Traveled in first class? (0 or 1).\n* Traveled in second class? (0 or 1).\n* Traveled in third class? (0 or 1).\n* Age.\n* Number of siblings / spouses.\n* Number of parents / children.\n* Ticket fare.\n\nWe have a bit less than 900 passengers (I'm using a reduced\ndataset here), however we want to take about 200 for verification\nat application side, without sending them to Redis at all.\n\nThe neural network will also use part of the dataset for\nverification, since here I'm planning to use the automatic training\nstop feature, in order to detect overfitting.\n\nSuch a network can be created with:\n\n    > NR.CREATE mynet CLASSIFIER 9 15 -> 2 DATASET 1000 TEST 500 NORMALIZE\n\nAlso note that we are using a neural network with a single hidden\nlayer (the layers between inputs and outputs are called hidden, in\ncase you are new to neural networks). The hidden layer has 15 units.\nThis is still a pretty small network, but we expect that for the\namount of data and the kind of correlations that there could be in\nthis data, this could be enough. It's possible to test with\ndifferent parameters, and I plan to implement a `NR.CONFIGURE`\ncommand so that it will be possible to change this things on the fly.\n\nAlso note that since we defined a testing dataset maximum size to be half\nthe one of the training dataset (1000 vs 500), `NR.OBSERVE` will automatically\nput one third of the entires in the testing dataset.\n\nIf you check the Ruby program that implements this example inside the\nsource distribution, you'll see how data is fed directly as it is\nto the network, since we asked for auto normalization:\n\n```\ndef feed_data(r,dataset,mode)\n    errors = 0\n    dataset.each{|d|\n        pclass = [0,0,0]\n        pclass[d[:pclass]-1] = 1\n        inputs = pclass +\n                 [d[:male],d[:female]] +\n                 [d[:age],\n                  d[:sibsp],\n                  d[:parch],\n                  d[:fare]]\n        outputs = d[:survived]\n        if mode == :observe\n            r.send('nr.observe',:mynet,*inputs,'->',outputs)\n        elsif mode == :test\n            res = r.send('nr.class',:mynet,*inputs)\n            if res != outputs\n                errors += 1\n            end\n        end\n    }\n    if mode == :test\n        puts \"#{errors} prediction errors on #{dataset.length} items\"\n    end\nend\n```\n\nThe function is able to both send data or evaluate the error rate.\n\nAfter we load 601 entries from the dataset, before any training, the output\nof `NR.INFO` will look like this:\n\n    > NR.INFO mynet\n     1) id\n     2) (integer) 5\n     3) type\n     4) classifier\n     5) auto-normalization\n     6) (integer) 1\n     7) training\n     8) (integer) 0\n     9) layout\n    10) 1) (integer) 9\n        2) (integer) 15\n        3) (integer) 2\n    11) training-dataset-maxlen\n    12) (integer) 1000\n    13) training-dataset-len\n    14) (integer) 401\n    15) test-dataset-maxlen\n    16) (integer) 500\n    17) test-dataset-len\n    18) (integer) 200\n    19) training-total-steps\n    20) (integer) 0\n    21) training-total-seconds\n    22) 0.00\n    23) dataset-error\n    24) \"0\"\n    25) test-error\n    26) \"0\"\n    27) classification-errors-perc\n    28) 0.00\n    29) overfitting-detected\n    30) no\n\nAs expected, we have 401 training items and 200 testing dataset.\nNote that for networks declared as classifiers, we have an additional\nfield in the info output, which is `classification-errors-perc`. Once\nwe train the network this field will be populated with the percentage (from\n0% to 100%) of items in the testing dataset which were misclassified by\nthe neural network. It's time to train our network:\n\n    > NR.TRAIN mynet AUTOSTOP\n    Training has started\n\nIf we check the `NR.INFO` output after the training, we'll discover a few\ninteresting things (only quoting the relevant part of the output):\n\n    19) training-total-steps\n    20) (integer) 64160\n    21) training-total-seconds\n    22) 0.29\n    23) dataset-error\n    24) \"0.1264141065389438\"\n    25) test-error\n    26) \"0.13803731074639586\"\n    27) classification-errors-perc\n    28) 19.00\n    29) overfitting-detected\n    30) yes\n\nThe network was trained for 0.29 seconds. At the end of the training,\nthat was stopped for overfitting, the error rate in the testing dataset\nwas 19%.\n\nYou can also specify to train for a given amonut of seconds or cycles.\nFor now we just use the `AUTOSTOP` feature since it is simpler. However we'll\ndig into more details in the next section.\n\nWe can now show the output of the Ruby program after its full execution:\n\n    47 prediction errors on 290 items\n\nDoes not look too bad, considering how simple is our model and the fact\nwe trained with just 401 data points. Modeling just on the percentage of\npeople that survived VS the ones that died, we could miss-predict more\nthan 100 passengers.\n\nWe can also play with a few variables interactively in order to check\nwhat are the inputs that make a difference according to our trained\nneural network.\n\nLet's start asking the probable outcome for a woman, 30 years old,\nfirst class, without siblings and parents:\n\n    > NR.RUN mynet 1 0 0 0 1 30 0 0 200\n    1) \"0.093071778873849084\"\n    2) \"0.90242156736283008\"\n\nThe network is positive she survived, with 90% of probabilities.\nWhat if she is a lot older than 30 years old, let's say 70?\n\n    > NR.RUN mynet 1 0 0 0 1 70 0 0 200\n    1) \"0.11650946245068818\"\n    2) \"0.88784839170875851\"\n\nThis lowers her probability to 88.7%.\nAnd if she traveled in third class with a very cheap ticket?\n\n    > NR.RUN mynet 0 0 1 0 1 70 0 0 20\n    1) \"0.53693405013043127\"\n    2) \"0.51547605838387811\"\n\nThis time is 50% and 50%... Throw your coin.\n\nThe gist of this example is that, many problems you face as a developer\nin order to optimize your application and do better choices in the\ninteraction with your users, are Titanic problems, but not in their\nsize, just in the fact that a simple model can \"solve\" them.\n\nOverfitting detection and training tricks\n===\n\nOne thing that makes neural networks hard to use in an interactive\nway like the one they are proposed in this Redis module, is for sure\noverfitting. If you train too much, the neural network ends to be\nlike that one student that can exactly tell you all the words in the\nlesson, but if you ask a more generic question about the argument she\nor he just wonders and can't reply.\n\nSo the `NR.TRAIN` command `AUTOSTOP` option attempts to detect\noverfitting to stop the training before it's too late. How is this\nperformed? Well the current solution is pretty trivial: as the training\nhappens, we check the current error of the neural network between\nthe training dataset and the testing dataset.\n\nWhen overfitting kicks in, usually what we see is that the network error\nrate starts to be lower and lower in the training dataset, but instead\nof also reducing in the testing dataset it inverts the tendency and\nstarts to grow. To detect this turning point is not simple for two\nreasons:\n\n1. The error may fluctuates as the network learns.\n2. The network error may just go higher in the testing dataset since the learning is trapped into a *local minima*, but then a better solution may be found.\n\nSo while `AUTOSTOP` kinda does what it advertises (but I'll work on\nimproving it in the future, and there are neural network experts that\nknow much better than me and can submit a kind Pull Request :-), there\nare also means to manually train the network, and see how the error\nchanges with training.\n\nFor instance, this is the error rate in the Titanic dataset after\nthe automatic stop:\n\n    21) training-total-seconds\n    22) 0.17\n    23) dataset-error\n    24) \"0.13170509045457734\"\n    25) test-error\n    26) \"0.13433443241900492\"\n    27) classification-errors-perc\n    28) 18.50\n\nWe can use the `MAXTIME` and `MAXCYCLES` options in order to train for\na specific amount of time (note that these options are also applicable\nwhen `AUTOSTOP` is specified). Normally `MAXTIME` is set to 10000, which\nare milliseconds, so to 10 seconds of total training before killing the\ntraining thread. Let's train our network for 30 seconds, without auto stop.\n\n    > NR.TRAIN mynet MAXTIME 30000\n    Training has started\n\nAs a side note, while one or more trainings are in progress, we can\nlist them:\n\n    > NR.THREADS\n    1) nn_id=9 key=mynet db=0 maxtime=30000 maxcycles=0\n\nAfter the training stops, let's show info again:\n\n    21) training-total-seconds\n    22) 30.17\n    23) dataset-error\n    24) \"0.0674554189303056\"\n    25) test-error\n    26) \"0.20468644603795394\"\n    27) classification-errors-perc\n    28) 21.50\n\nYou can see that our network overtrained: the error rate of the training\ndataset is now lower: 0.06. But actually the performances in data it\nnever saw before, that is the testing dataset, is greater at 0.20!\n\nAnd indeed, it classifies in a wrong way 21% of entries instead of 18.50%.\n\nHowever it's not always like that, so to test things manually is a good\nidea when working at machine learning experiments, especially with this\nmodule that is experimental.\n\nAn interesting example is the `iris.rb` program inside the `examples`\ndirectory: it will load the `Iris.csv` dataset into Redis, which is\na very popular dataset with three variants of Iris flowers with their\nsepal and petal features. If you run the program, the percentage of\nentries classified in a wrong way will be 4%, however if you train the\nnetwork a few more cycles with:\n\n    NR.TRAIN iris MAXCYCLES 100\n\nYou'll see that often the error will drop to 2%.\n\nBetter overfitting detection with the BACKTRACK option\n===\n\nWhen using `AUTOSTOP`, there is an additional option that can be specified\n(it has no effects alone), that is: `BACKTRACK`. When backtracking is\nenabled, while the network is trained, every time there is some hint\nthat the network may start to overfit, the current version of the network\nis saved. At the end of the training, if the saved network is better\n(has a smaller error) compared to the current one, it is used instead\nof the final version of the trained network.\n\nThis avoids certain pathological runs when `AUTOSTOP` is used but\noverfitting is not detected. However, it adds running time since we\nneed to clone the NN from time to time during the training.\n\nFor example using `BACKTRACK` with the Iris dataset (see the `iris.rb`\nfile inside the examples directory) it never overtrains, while without\nabout 2% of the runs may overtrain.\n\nA more complex non linear classification example\n===\n\nThe Titanic example is surely more interesting, however it is possible\nthat most relations between inputs and outputs are linear, so we'll now\ntry a non linear classification task, just for the sake of showing the\ncapabilities of a small neural network.\n\nIn the examples directory of this source distribution there is an example\ncalled `circles.rb`, we'll use it as a reference.\n\nWe'll just setup a classification problem where the neural network\nwill be asked to classify two inputs, which are from our point of\nview two coordinates in a 2D space, into three different classes:\n0, 1 and 2.\n\nWhile the neural network does not know this, we'll generate the data\nso that different classes actually map to three different circles\nin the 2D space: the circles also contain intersections. The function\nthat generates the dataset is the following:\n\n```\n    point_class = rand(3) # Class will be 0, 1 or 2\n    if point_class == 0\n        x = Math.sin(k)/2+rand()/10;\n        y = Math.cos(k)/2+rand()/10;\n    elsif point_class == 1\n        x = Math.sin(k)/3+0.4+rand()/8;\n        y = Math.cos(k)/4+0.4+rand()/6;\n    else\n        x = Math.sin(k)/3-0.5+rand()/30;\n        y = Math.cos(k)/3+rand()/40;\n    end\n```\n\nThe basic trigonometric function:\n\n    x = Math.sin(k)\n    y = Math.cos(k)\n\nWith `k` going from 0 to 2*PI, is just a circle, so the above functions\nare just circles, plus the `rand()` calls in order to introduce noise.\nBasically if I trace the above three classes of points in a graphical\nway with [load81](https://github.com/antirez/load81), I obtain the\nfollowing image:\n\n![Circles plotted with LOAD81](http://antirez.com/misc/nn-circles.png)\n\nThe program `circles.rb`, it will generate the same set of points and\nwill feed them into the neural network configured to accept 2 inputs\nand output one of three possible classes.\n\nAfter about 2 seconds of training, we try to visualize what the neural\nnetwork has learned (also part of the `circles.rb` command) in this way:\nfor each point in an `80x80` grid, we ask the network to classify the\npoint. This is the ASCII-artist result:\n\n```\n................................................................................\n................................................................................\n................................................................................\n................................................................................\n................................................................................\n................................................................................\n................................................................................\n................................................................................\n/...............................................................................\n///.............................................................................\n////............................................................................\n//////..........................................................................\n///////.........................................................................\n/////////.......................................................................\n//////////......................................................................\n////////////....................................................................\n/////////////...................................................................\n//////////////..................................................................\n///////////////.................................................................\n/////////////////...............................................................\n//////////////////..............................................................\n///////////////////.............................................................\n////////////////////............................................................\n/////////////////////...........................................................\n//////////////////////..........................................................\n////////////////////////........................................................\n/////////////////////////.......................................................\n//////////////////////////......................................................\n///////////////////////////.....................................................\n////////////////////....///////.................................................\n///////////////////........//////...............................................\n///////////////////.........///////.............................................\n///////////////////..........////////...........................................\n///////////////////...........///////...........................................\n///////////////////...........///////...........................................\n///////////////////............///////..........................................\n///////////////////............///////..........................................\n//////////////////.............///////..........................................\n//////////////////.............///////..........................................\n//////////////////............////////..........................................\n//////////////////............////////..........................................\n//////////////////............////////..........................................\n///////////////////.........../////////.........................................\n///////////////////..........//////////..OOOOOOOOO..............................\n///////////////////..........//////////.OOOOOOOOOOOOOO..........................\n///////////////////..........//////////OOOOOOOOOOOOOOOOOOOO.....................\n///////////////////........../////////OOOOOOOOOOOOOOOOOOOOOOOOO.................\n////////////////////......../////////OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO..\n////////////////////........///////.OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO\n/////////////////////......///////.OOOOOOOOOOOOOOOOOOOOOOOOO..OOOOOOOOOOOOOOOOOO\n/////////////////////......//////..OOOOOOOOOOOOOOOOOOOOOOO......OOOOOOOOOOOOOOOO\n//////////////////////....//////...OOOOOOOOOOOOOOOOOOOOOO........OOOOOOOOOOOOOOO\n//////////////////////////////......OOOOOOOOOOOOOOOOOOOO.........OOOOOOOOOOOOOOO\n////////////////////////////........OOOOOOOOOOOOOOOOOOO.........OOOOOOOOOOOOOOOO\n//////////////////////////...........OOOOOOOOOOOOOOOOO.........OOOOOOOOOOOOOOOOO\n////////////////////////..............OOOOOOOOOOOOOO..........OOOOOOOOOOOOOOOOOO\n///////////////////////....................OOOOOOOO..........OOOOOOOOOOOOOOOOOOO\n//////////////////////.....................OOOOOOOO.........OOOOOOOOOOOOOOOOOOOO\n/////////////////////......................OOOOOOO........OOOOOOOOOOOOOOOOOOOOOO\n////////////////////........................OOOOO........OOOOOOOOOOOOOOOOOOOOOOO\n///////////////////.........................OOOO.......OOOOOOOOOOOOOOOOOOOOOOOOO\n//////////////////..........................OOOO.....OOOOOOOOOOOOOOOOOOOOOOOOOOO\n/////////////////............................OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO\n////////////////.............................OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO\n///////////////........................../...OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO\n//////////////..........................//...OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO\n/////////////..........................////..OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO\n////////////.........................//////..OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO\n//////////..........................///////..OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO\n/////////..........................////////..OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO\n////////..........................//////////.OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO\n///////..........................///////////.OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO\n/////........................../////////////.OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO\n////..........................//////////////.OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO\n///..........................///////////////.OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO\n//........................../////////////////OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO\n...........................//////////////////OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO\n..........................///////////////////OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO\n.........................////////////////////OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO\n......................../////////////////////OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO\n```\n\nAs you can see, while the problem had no linear solution, the neural network\nwas able to split the 2D space into areas, with the *holes* where there\nis the intersection between the circles areas, and thiner surfaces where the\ncircles actually cross each other (in the intersection between the two\ncircumferences there are points of two different classes).\n\nThis example was not practical perhaps but shows well the power of the\nneural network in non linear tasks.\n\nCase study: Sentiment Analysis\n===\n\nNeural Redis is not the right tool for advanced NLP tasks, and for sentiment\nanalysis, which is a very hard problem, there are RNNs and other, more complex\ntools, that can provide state of art results.\n\nHowever exactly for the same reason, SA is a very good example to show how\nto model problems, and that even the simplest of the intuitions can allow\nNeural Redis to handle problems in a decent way (even if far from the top\nspecialized systems) after a training of 5 minutes or so.\n\nThis case study is based on the source code inside the examples directory\ncalled `sentiment.rb`. It uses a very popular dataset used for sentiment\nanalysis benchmarking, composed of 2000 movies reviews, 1000 which are\npositive, and 1000 negative.\n\nThe reviews are like the following:\n\n    It must be some sort of warped critical nightmare: the best movie of\n    the year would be a summer vehicle, a jim carrey vehicle at that.\n    And so it is. The truman show is the most perplexing, crazed, paranoid\n    and rib-tickling morality play i've seen since i-don't-know-when.\n\nNormally we should try to do the best we can do in order to pre-process\nthe data, but we are lazy dogs, so we don't do anything at all.\nHowever we still need to map our inputs and outputs to meaningful\nparameters. For the outputs, it's trivial, is a categorization task:\n*negative or positive*. But how do we map words to inputs?\n\nNormally you assign different words to different IDs, and then use such\nIDs as indexes. This creates two problems in our case:\n\n1. We need to select a vocabulary. Usually this is done in a pre-processing stage where we potentially examine a non-annotated large corpus of text. But remember that we are lazy?\n2. \"very good\" and \"not good\" have very different meanings, we can't stop to single words, otherwise our result is likely be disappointing.\n\nSo I did the following. Let's say our network is composed of 3000 inputs,\n100 hidden units, and the 2 outputs for the classification.\n\nWe split the initial inputs into two sides: 1500 of inputs just take the\nsingle words. The other 1500 inputs, we use for combinations of two words.\nWhat I did was to just use hashing to map the words in the text to the input\nunits:\n\n    INDEX_1 = HASH(word) % 1500\n    INDEX_2 = 1500 + (HASH(word + next_word) % 1500)\n\nThis is a bit crazy, I'm curious to know if it's something that people tried\nin the past, since different words and different combinations of words\nwill hash to the same, so we'll get a bit less precise results, however it\nis unlikely that words *highly polarized* in the opposite direction (positive\nVS negative) will hash to the same bucket, if we use enough inputs.\n\nSo each single word and combination of words is a \"vote\" in the input\nunit. As we scan the sentences to give the votes, we also sum all the\nsingle votes we gave, so that we finally normalize to make sure all\nour inputs summed will give \"1\". This way the sentiment analysis does not\ndepend by the length of the sentence.\n\nWhile this approach is very simple, it works and produces a NN in a matter\nof seconds that can score 80% in the 2000 movies dataset. I just spent\na couple of hours on it, probably it's possible to do much better with\na more advanced scheme. However the gist of this use case is: be creative\nwhen trying to map your data to the neural network.\n\nIf you run `sentiment.rb` you'll see the network quickly converging\nand at the end, you'll be able to type sentences that the NN will\nclassify as positive or negative:\n\n    nn_id=7 cycle=61 key=sentiment ... classerr=21.500000\n    nn_id=7 cycle=62 key=sentiment ... classerr=20.333334\n\n    Best net so far can predict sentiment polarity 78.17 of times\n\n    Imagine and type a film review sentence:\n\n    > WTF this movie was terrible\n    Negativity: 0.99966669082641602\n    Positivity: 0.00037576013710349798\n\n    > Good one\n    Negativity: 0.28475716710090637\n    Positivity: 0.73368257284164429\n\n    > This is a masterpiece\n    Negativity: 2.219095662781001e-08\n    Positivity: 0.99999994039535522\n\nOf course you'll find a number of sentences that the net will classify\nin the wrong way... However the longer sentence you type and more similar\nto an actual movie review, the more likely it can predict it correctly.\n\nAPI reference\n===\n\nIn the above tutorial not all the options of all the commands may be\ncovered, so here there is a small reference with all the commands\nsupported by this extension and associated options.\n\n### NR.CREATE key [CLASSIFIER|REGRESSOR] inputs [hidden-layer-units ...] -> outputs [NORMALIZE] [DATASET maxlen] [TEST maxlen]\n\nCreate a new neural network if the target key is empty, or returns an error.\n\n* key - The key name holding the neural network.\n* CLASSIFIER or REGRESSOR is the network type, read this tutorial for more info.\n* inputs - Number of input units\n* hidden-layer-units zero or more arguments indicating the number of hidden units, one number for each layer.\n* outputs - Number of outputs units\n* NORMALIZE - Specify if you want the network to normalize your inputs. Use this if you don't know what we are talking about.\n* DATASET maxlen - Max number of data samples in the training dataset.\n* TEST maxlen - Max number of data samples in the testing dataset.\n\nExample:\n\n    NR.CREATE mynet CLASSIFIER 64 100 -> 10 NORMALIZE DATASET 1000 TEST 500\n\n### NR.OBSERVE key i0 i1 i2 i3 i4 ... iN -> o0 o1 o3 ... oN [TRAIN|TEST]\n\nAdd a data sample into the training or testing dataset (if specified as last argument) or evenly into one or the other, according to their respective sizes, if no target is specified.\n\nFor neural networks of type CLASSIFIER the output must be just one, in the range from 0 to `number-of-outputs - 1`. It's up to the network to translate the class ID into a set of zeros and ones.\n\nThe command returns the number of data samples inside the training and testing dataset. If the target datasets are already full, a random entry is evicted and substituted with the new data.\n\n## NR.RUN key i0 i1 i2 i3 i4 ... iN\n\nRun the network stored at key, returning an array of outputs.\n\n## NR.CLASS key i0 i1 i2 i3 i4 ... iN\n\nLike `NR.RUN` but can be used only with NNs of type CLASSIFIER. Instead of outputting the raw neural network outputs, the command returns the output class directly, which is, the index of the output with the greatest value.\n\n## NR.TRAIN key [MAXCYCLES count] [MAXTIME milliseconds] [AUTOSTOP] [BACKTRACK]\n\nTrain a network in a background thread. When the training finishes\nautomatically updates the weights of the trained networks with the\nnew ones and updates the training statistics.\n\nThe command works with a copy of the network, so it is possible to\nuse the network while it is undergoing a training.\n\nIf no AUTOSTOP is specified, trains the network till the maximum number of\ncycles or milliseconds are reached. If no maximum number of cycles is specified\nthere are no cycles limits. If no milliseconds are specified, the limit is\nset to 10000 milliseconds (10 seconds).\n\nIf AUTOSTOP is specified, the training will still stop when the maximum \number of cycles or milliseconds is specified, but will also try to stop\nthe training if overfitting is detected. Check the previous sections for\na description of the (still naive) algorithm the implementation uses in\norder to stop.\n\nIf BACKTRACK is specified, and AUTOSTOP is also specified, while the network\nis trained, the trainer thread saves a copy of the neural network every\ntime it has a better score compared to the previously saved one and there are\nhints suggesting that overfitting may happen soon. This network is used later\nif it is found to have a smaller error.\n\n## NR.INFO key\n\nShow many internal information about the neural network. Just try it :-)\n\n## NR.THREADS\n\nShow all the active training threads.\n\n## NR.RESET key\n\nSet the neural network weights to random ones (that is, the network will\ncompletely unlearn what it learned so far), and reset training statistics.\nHowever the datasets are not touched at all. This is useful when you\nwant to retrain a network from scratch.\n\nContributing\n===\n\nThe main aim of Neural Redis, which is currently just a 48h personal\nhackatlon, is to show the potential that there is in an accessible API\nthat provides a simple to use machine learning tool, that can be used\nand trained interactively.\n\nHowever the neural network implementation can be surely improved in different\nways, so if you are an expert in this field, feel free to submit changes\nor ideas. One thing that I want to retain is the simplicity of the outer\nlayer: the API. However the techniques used in the internals can be more\ncomplex in order to improve the results.\n\nThere is to note that, given the API exported, the implementation of\nthe neural network should be, more than state of art in solving a specific\nproblem, more designed in order to work well enough in a large set of\nconditions. While the current fully connected network has its limits,\nit together with BPROP learning shows to be quite resistant to misuses.\nSo an improved version should be able to retain, and extend this quality.\nThe simplest way to guarantee this is to have a set of benchmarks of different\ntypes using open datasets, and to score different implementations against\nit.\n\nPlans\n===\n\n* Better overfitting detection.\n* Implement RNNs with a simpler to use API.\n* Use a different loss function for classification NNs.\n* Get some ML expert which is sensible to simple APIs involved.\n\nHave fun with machine learning,\n\nSalvatore\n"
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "neuralredis.c",
          "type": "blob",
          "size": 52.9609375,
          "content": "/* This file implements a neural network datatype with training capabilities\n * as a Redis module.\n *\n * Check https://github.com/antirez/neural-redis/ for more information\n *\n * -----------------------------------------------------------------------------\n *\n * Copyright (C) 2016, Salvatore Sanfilippo <antirez at gmail dot com>\n * All rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n *\n *   * Redistributions of source code must retain the above copyright notice,\n *     this list of conditions and the following disclaimer.\n *   * Redistributions in binary form must reproduce the above copyright\n *     notice, this list of conditions and the following disclaimer in the\n *     documentation and/or other materials provided with the distribution.\n *   * Neither the name of Redis nor the names of its contributors may be used\n *     to endorse or promote products derived from this software without\n *     specific prior written permission.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE\n * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n * POSSIBILITY OF SUCH DAMAGE.\n */\n\n#define _DEFAULT_SOURCE /* for strcasecmp() */\n\n#include \"redismodule.h\"\n#include <stdio.h>\n#include <stdlib.h>\n#include <ctype.h>\n#include <string.h>\n#include <stdint.h>\n#include <pthread.h>\n#include <sys/time.h>\n#include <math.h>\n\n#include \"nn.h\"\n\n#define UNUSED(V) ((void) V)\n\nstatic RedisModuleType *NRType;\nuint64_t NRNextId = 1; /* Next neural network unique ID. */\n\n/* ========================== Internal data structure  ====================== */\n\n#define NR_FLAG_NONE 0\n#define NR_FLAG_TRAINING (1<<0)         /* NN is training in a thread. */\n#define NR_FLAG_REGRESSOR (1<<1)        /* NN will be used for regression. */\n#define NR_FLAG_CLASSIFIER (1<<2)       /* NN will be used for classification.*/\n#define NR_FLAG_NORMALIZE (1<<3)        /* Perform input/output normalization.*/\n#define NR_FLAG_AUTO_STOP (1<<4)        /* Auto stop on training. */\n#define NR_FLAG_OF_DETECTED (1<<5)      /* Auto stopped on overfitting. */\n#define NR_FLAG_BACKTRACK (1<<6)        /* Auto stop with backtracking. */\n\n/* Flags to persist when saving the NN. */\n#define NR_FLAG_TO_PRESIST (NR_FLAG_REGRESSOR| \\\n                            NR_FLAG_CLASSIFIER| \\\n                            NR_FLAG_NORMALIZE| \\\n                            NR_FLAG_OF_DETECTED)\n\n/* Flags to transfer after training. */\n#define NR_FLAG_TO_TRANSFER (NR_FLAG_OF_DETECTED)\n\n#define NR_MAX_LAYERS 32\n#define NR_RDB_ENC_VER 2\n\ntypedef struct NRDataset {\n    uint32_t len, maxlen;\n    float *inputs, *outputs;\n} NRDataset;\n\ntypedef struct {\n    uint64_t id;        /* Neural network unique ID. */\n    uint64_t training_total_steps; /* How many steps of trainig the network\n                                      received. A step is a single input/output\n                                      pattern presented to the net (counting\n                                      the same pattern multiple times) */\n    uint64_t training_total_ms;   /* Total milliseconds time of training. */\n    uint64_t training_max_cycles; /* Max cycles of a single training. */\n    uint64_t training_max_ms; /* Max time of a single training. */\n    uint32_t flags;     /* NR_FLAG_... */\n    uint32_t epochs;    /* Number of training epochs so far. */\n    struct Ann *nn;     /* Neural network structure. */\n    NRDataset dataset;  /* Training dataset. */\n    NRDataset test;     /* Testing dataset. */\n    float dataset_error;   /* Average error in the training dataset. */\n    float test_error;      /* Average error in the test dataset. */\n    float test_class_error;    /* Percentage of wrong classifications in test\n                                   dataset. Only applicable to nets flagged with\n                                   NR_FLAG_CLASSIFIER. */\n    /* For normalized (NR_FLAG_NORMALIZE) networks. */\n    float *inorm;          /* Inputs normalization factors. */\n    float *onorm;          /* Outputs normalization factors. */\n} NRTypeObject;\n\nstruct {\n    RedisModuleString *key; /* Key name of the NN we are training.\n                               Set to NULL for unused slots. */\n    int db_id;              /* DB ID where the key is. */\n    pthread_t tid;          /* Thread ID of the trainer. */\n    int in_progress;        /* 0 if training terminated. */\n    NRTypeObject *nr;       /* A copy of the NN we are training. */\n    float dataset_error;    /* Dataset error in the last cycle. */\n    float test_error;       /* Test error in the last cycle. */\n    float class_error;      /* Percentage of wrong classifications. */\n    int curcycle;           /* Current cycle. */\n} typedef NRPendingTraining;\n\n/* We take an array with NNs currently training in other threads.\n * Every time an NN command is called, we try to see if there are\n * finished trainings, in order to udpate weights of the original\n * NN stored into the key (we work on a copy on the other thread).*/\n#define NR_PENDING_TRAINING_MAX_LEN 32\n\nstatic pthread_mutex_t NRPendingTrainingMutex = PTHREAD_MUTEX_INITIALIZER;\n/* All the followings must be accessed after acquiring the mutex. */\nstatic NRPendingTraining NRTrainings[NR_PENDING_TRAINING_MAX_LEN];\nstatic int NRPendingTrainingCount = 0; /* Number of pending trainings. */\n\n/* ========================== Low level object API ========================== */\n\nlong long NRMilliseconds(void) {\n    struct timeval tv;\n    long long ust;\n\n    gettimeofday(&tv, NULL);\n    ust = ((long long)tv.tv_sec)*1000000;\n    ust += tv.tv_usec;\n    return ust/1000;\n}\n\n/* Create a network with the specified parameters. Note that the layers\n * must be specified from the output layer[0] to the input\n * layer[N]. Each element in the integer array 'layer' specify how many\n * units there are in the corresponding layer. */\nNRTypeObject *createNRTypeObject(int flags, int *layers, int numlayers, int dset_len, int test_len) {\n    NRTypeObject *o;\n    o = RedisModule_Calloc(1,sizeof(*o));\n    o->id = NRNextId++;\n    o->flags = flags;\n    o->nn = AnnCreateNet(numlayers,layers);\n    o->dataset.maxlen = dset_len;\n    o->test.maxlen = test_len;\n    int ilen = INPUT_UNITS(o->nn);\n    int olen = OUTPUT_UNITS(o->nn);\n    o->inorm = RedisModule_Calloc(1,sizeof(float)*ilen);\n    o->onorm = RedisModule_Calloc(1,sizeof(float)*olen);\n    for (int j = 0; j < ilen; j++) o->inorm[j] = 1;\n    for (int j = 0; j < olen; j++) o->onorm[j] = 1;\n    return o;\n}\n\n/* Insert data (observations needed to train and test the NN) into the\n * NN object. While the learning and testing datasets are yet not full\n * the observed pattern is inserted evenly in one or the other side in\n * order to make sure the two datasets are populated evenly. When both\n * are already full, a random elmenet from one or the other (doing\n * a random weighted choice depending on the length) is substituted with\n * the new item. */\n#define NR_INSERT_NO_TARGET 0   /* Auto select where to insert. */\n#define NR_INSERT_TRAIN 1       /* Insert in training dataset. */\n#define NR_INSERT_TEST 2        /* Insert in testing dataset. */\nvoid NRTypeInsertData(NRTypeObject *o, float *inputs, float *outputs,\n                      int target_ds) {\n    NRDataset *target = NULL;\n\n    /* Check if there is no dataset at all. This may be a valid setup\n     * with online learning, sample by sample. */\n    if (o->dataset.maxlen == 0 && o->test.maxlen == 0) return;\n\n    /* If the user specified a target, select it. */\n    if (target_ds == NR_INSERT_TRAIN) target = &o->dataset;\n    else if (target_ds == NR_INSERT_TEST) target = &o->test;\n\n    /* If no target is specified, but there is only one possible\n     * target, select it ASAP. */\n    if (o->dataset.maxlen == 0) {\n        target = &o->test;\n    } else if (o->test.maxlen == 0) {\n        target = &o->dataset;\n    }\n\n    /* Otherwise choose as the target to populate the one with less data\n     * relatively to its size. */\n    if (target == NULL) {\n        /* If one of the two datasets are still not full, pick\n         * based on fill percentage. Otherwise pick a random\n         * target relatively to their size. */\n        if (o->dataset.len != o->dataset.maxlen ||\n            o->test.len != o->dataset.len)\n        {\n            float fill_a = (float)o->dataset.len / o->dataset.maxlen;\n            float fill_b = (float)o->test.len / o->test.maxlen;\n            target = (fill_a <= fill_b) ? &o->dataset : &o->test;\n        } else {\n            double r = rand()/RAND_MAX;\n            double sumlen = o->dataset.maxlen + o->test.maxlen;\n            if (r < (double)o->dataset.maxlen/sumlen) {\n                target = &o->dataset;\n            } else {\n                target = &o->test;\n            }\n        }\n    }\n\n    /* Append if there is room or substitute with a random entry. */\n    size_t idx;\n    int j, numin = INPUT_UNITS(o->nn),\n           numout = OUTPUT_UNITS(o->nn);\n\n    if (target->maxlen == target->len) {\n        idx = rand() % target->maxlen;\n    } else {\n        idx = target->len;\n        target->len++;\n        target->inputs = RedisModule_Realloc(target->inputs,\n            sizeof(float)*numin*target->len);\n        target->outputs = RedisModule_Realloc(target->outputs,\n            sizeof(float)*numout*target->len);\n    }\n\n    /* Finally store the values at position. */\n    for (j = 0; j < numin; j++)\n        target->inputs[idx*numin+j] = inputs[j];\n    for (j = 0; j < numout; j++)\n        target->outputs[idx*numout+j] = outputs[j];\n}\n\n/* Free the specified dataset. */\nvoid NRDatasetFree(NRDataset *dset) {\n    RedisModule_Free(dset->inputs);\n    RedisModule_Free(dset->outputs);\n}\n\n/* Free a whole NN object. */\nvoid NRTypeReleaseObject(NRTypeObject *o) {\n    AnnFree(o->nn);\n    NRDatasetFree(&o->dataset);\n    NRDatasetFree(&o->test);\n    RedisModule_Free(o->inorm);\n    RedisModule_Free(o->onorm);\n    RedisModule_Free(o);\n}\n\n/* ================================ Training =============================== */\n\n/* Clone a neural network object, including the training and test dataset.\n * We use cloning in order to train in a different thread, and later\n * copy the weights back into the original NN.\n *\n * Note when 'newid' is 0, the copied object NN unique ID is the same as the\n * original as normally this is what we want, in order to later match the\n * trained network with the object stored at the specified key\n * in the pending traning structure.\n *\n * However if the copy is performed with other goals, 'newid' should\n * be set to non-zero in order to create a net with a different ID. */\nNRTypeObject *NRClone(NRTypeObject *o, int newid) {\n    NRTypeObject *copy;\n    copy = RedisModule_Calloc(1,sizeof(*o));\n    *copy = *o;\n    if (newid) copy->id = NRNextId++;\n    copy->nn = AnnClone(o->nn);\n    copy->dataset = o->dataset;\n    copy->test = o->test;\n\n    int ilen = INPUT_UNITS(o->nn);\n    int olen = OUTPUT_UNITS(o->nn);\n    copy->dataset.inputs = RedisModule_Alloc(sizeof(float)*ilen*o->dataset.len);\n    copy->dataset.outputs = RedisModule_Alloc(sizeof(float)*olen*o->dataset.len);\n    copy->test.inputs = RedisModule_Alloc(sizeof(float)*ilen*o->test.len);\n    copy->test.outputs = RedisModule_Alloc(sizeof(float)*olen*o->test.len);\n    memcpy(copy->dataset.inputs,o->dataset.inputs,sizeof(float)*ilen*o->dataset.len);\n    memcpy(copy->dataset.outputs,o->dataset.outputs,sizeof(float)*olen*o->dataset.len);\n    memcpy(copy->test.inputs,o->test.inputs,sizeof(float)*ilen*o->test.len);\n    memcpy(copy->test.outputs,o->test.outputs,sizeof(float)*olen*o->test.len);\n\n    copy->inorm = RedisModule_Alloc(sizeof(float)*ilen);\n    copy->onorm = RedisModule_Alloc(sizeof(float)*olen);\n    memcpy(copy->inorm,o->inorm,sizeof(float)*ilen);\n    memcpy(copy->onorm,o->onorm,sizeof(float)*olen);\n    return copy;\n}\n\n/* Transfer the weights from the source to the destination NN.\n * This is used after the learning process finished in a different\n * thread in order to transfer the learning back to the orignal\n * NN. */\nvoid NRTransferWeights(RedisModuleCtx *ctx, NRTypeObject *dst, NRTypeObject *src) {\n    if (dst->id != src->id) {\n        RedisModule_Log(ctx,\"warning\",\n            \"NSTransferWeight(): source and destination neural network IDs \"\n            \"don't match. This is unexpected, probably a bug inside the \"\n            \"module. Weights not transferred back to the origina NN.\");\n        return;\n    }\n\n    /* It would be faster to memcpy just the weight array for each layer,\n     * however this way we access the NN in a more abstract way, and should\n     * be fast enough in most cases. We can always optimized it later. */\n    AnnFree(dst->nn);\n    dst->nn = AnnClone(src->nn);\n    dst->training_total_steps = src->training_total_steps;\n    dst->training_total_ms = src->training_total_ms;\n    dst->dataset_error = src->dataset_error;\n    dst->test_error = src->test_error;\n    dst->test_class_error = src->test_class_error;\n    dst->flags |= src->flags & NR_FLAG_TO_TRANSFER;\n\n    int ilen = INPUT_UNITS(src->nn);\n    int olen = OUTPUT_UNITS(src->nn);\n    memcpy(dst->inorm,src->inorm,sizeof(float)*ilen);\n    memcpy(dst->onorm,src->onorm,sizeof(float)*olen);\n}\n\n/* Threaded training entry point.\n *\n * To get some clue about overfitting algorithm behavior:\n * #define NR_TRAINING_DEBUG 1\n */\nvoid *NRTrainingThreadMain(void *arg) {\n    NRPendingTraining *pt = arg;\n    NRTypeObject *nr = pt->nr;\n    int training_iterations = 1;\n    float train_error = 0;\n    float test_error = 0;\n    float class_error = 0;\n    float past_train_error = 1.0/0.0;\n    float past_test_error = 1.0/0.0;\n    int auto_stop = nr->flags & NR_FLAG_AUTO_STOP;\n    int backtrack = nr->flags & NR_FLAG_BACKTRACK;\n\n    uint64_t cycles = 0;\n    long long start = NRMilliseconds();\n    long long cycle_time;\n    int overfitting_count = 0;\n    int overfitting_limit = 5;\n    float best_test_error = 1.0/0.0;\n\n    nr->flags &= ~NR_FLAG_TO_TRANSFER;\n\n    /* If the network is auto normalized, we need to trasnform the inputs\n     * in a way that's acceptable for the NN. We just find the maximum\n     * absolute value, and divide for it, to get a -1,1 range. There\n     * are more advanced transformations that are usually performed that\n     * could be implemented in the future.\n     *\n     * Note that we compute the normalization vectors for all the inputs\n     * and outputs, however if the network is a classifier, flagged with\n     * (NR_FLAG_CLASSIFIER), no output normalization will be done since\n     * the data is already in 0/1 format. */\n    if ((nr->flags & NR_FLAG_NORMALIZE) && nr->dataset.len) {\n        int ilen = INPUT_UNITS(nr->nn);\n        int olen = OUTPUT_UNITS(nr->nn);\n        float *imax = nr->inorm;\n        float *omax = nr->onorm;\n        float *inputs = nr->dataset.inputs;\n        float *outputs = nr->dataset.outputs;\n        for (int i = 0; i < ilen; i++) imax[i] = 1;\n        for (int i = 0; i < olen; i++) omax[i] = 1;\n\n        /* Compute the max values vectors. */\n        for (uint32_t j = 0; j < nr->dataset.len; j++) {\n            for (int i = 0; i < ilen; i++)\n                if (fabs(inputs[i]) > imax[i]) imax[i] = fabs(inputs[i]);\n            for (int i = 0; i < olen; i++)\n                if (fabs(outputs[i]) > omax[i]) omax[i] = fabs(outputs[i]);\n            inputs += ilen;\n            outputs += olen;\n        }\n\n        /* Likely we are not seeing what will really be the true input/output\n         * maximum value, so we multiply the maximum values found by a constant.\n         * However if the max is exactly \"1\" we assume it's a classification\n         * input and don't alter it. */\n        for (int i = 0; i < ilen; i++) if (imax[i] != 1) imax[i] *= 1.2;\n        for (int i = 0; i < olen; i++) if (omax[i] != 1) omax[i] *= 1.2;\n\n        /* We can normalize the dataset directly: after the training it will\n         * be discarded anyway. */\n        inputs = nr->dataset.inputs;\n        outputs = nr->dataset.outputs;\n        for (uint32_t j = 0; j < nr->dataset.len; j++) {\n            for (int i = 0; i < ilen; i++) inputs[i] /= nr->inorm[i];\n            if (!(nr->flags & NR_FLAG_CLASSIFIER))\n                for (int i = 0; i < olen; i++) outputs[i] /= nr->onorm[i];\n            inputs += ilen;\n            outputs += olen;\n        }\n\n        inputs = nr->test.inputs;\n        outputs = nr->test.outputs;\n        for (uint32_t j = 0; j < nr->test.len; j++) {\n            for (int i = 0; i < ilen; i++) inputs[i] /= nr->inorm[i];\n            if (!(nr->flags & NR_FLAG_CLASSIFIER))\n                for (int i = 0; i < olen; i++) outputs[i] /= nr->onorm[i];\n            inputs += ilen;\n            outputs += olen;\n        }\n    }\n\n    struct Ann *saved = NULL;  /* Saved to recover on overfitting. */\n    float saved_error;          /* The test error of the saved NN. */\n    float saved_train_error;    /* The training dataset error of the saved NN */\n    float saved_class_error;    /* The % of classification errors of saved NN */\n\n    while(1) {\n        long long cycle_start = NRMilliseconds();\n\n        train_error = AnnTrain(nr->nn,\n                               nr->dataset.inputs,\n                               nr->dataset.outputs,\n                               0,\n                               training_iterations,\n                               nr->dataset.len,\n                               NN_ALGO_BPROP);\n        cycle_time = NRMilliseconds() - cycle_start;\n        nr->training_total_steps += nr->dataset.len*training_iterations;\n\n        /* Evaluate the error in the case of auto training, stop it\n         * once we see that the error in the traning set is decreasing\n         * while the one in the test set is not. */\n        if (auto_stop) {\n            AnnTestError(nr->nn,\n                         nr->test.inputs,\n                         nr->test.outputs,\n                         nr->test.len, &test_error, &class_error);\n\n            if (train_error < past_train_error &&\n                test_error > past_test_error)\n            {\n                overfitting_count++;\n                #ifdef NR_TRAINING_DEBUG\n                printf(\"+YCLE %lld: [%d] %f VS %f\\n\", (long long)cycles,\n                    overfitting_count, train_error, test_error);\n                #endif\n                if (overfitting_count == overfitting_limit) {\n                    nr->flags |= NR_FLAG_OF_DETECTED;\n                    break;\n                }\n            } else if (overfitting_count > 0) {\n                #ifdef NR_TRAINING_DEBUG\n                printf(\"-YCLE %lld: [%d] %f VS %f\\n\", (long long)cycles,\n                    overfitting_count, train_error, test_error);\n                #endif\n                overfitting_count--;\n            }\n\n            /* Save all the networks with a score better than the currently\n             * saved network. This can be a bit costly, but is safe: one\n             * cycle of training more and overfitting can ruin it all. */\n            if (backtrack && (saved == NULL || test_error < saved_error)) {\n                #ifdef NR_TRAINING_DEBUG\n                printf(\"SAVED! %f < %f\\n\", test_error, saved_error);\n                #endif\n                saved_error = test_error;\n                saved_train_error = train_error;\n                saved_class_error = class_error;\n                if (saved) AnnFree(saved);\n                saved = AnnClone(nr->nn);\n            }\n\n            /* Best network found? Reset the overfitting hints counter. */\n            if (test_error < best_test_error) {\n                overfitting_count = 0;\n                best_test_error = test_error;\n                #ifdef NR_TRAINING_DEBUG\n                printf(\"BEST! %lld: <%d> %f VS %f\\n\", (long long)cycles,\n                    overfitting_limit,train_error, test_error);\n                #endif\n            }\n\n           /* Also stop if the loss is zero in both datasets. */\n            if (train_error < 0.000000000000001 &&\n                test_error  < 0.000000000000001) break;\n        }\n\n        cycles++;\n        long long total_time = NRMilliseconds()-start;\n\n        /* Cycles and milliseconds stop conditions. */\n        if (nr->training_max_cycles && cycles == nr->training_max_cycles)\n            break;\n        if (nr->training_max_ms && total_time > (long long)nr->training_max_ms)\n            break;\n\n        /* If this is a long training, to do just a single training iteration\n         * for each cycle is not optimal: tune the number of iterations to\n         * at least take 100 milliseconds. */\n        if (total_time > 10000 && cycle_time < 100) training_iterations++;\n\n        past_train_error = train_error;\n        past_test_error = test_error;\n\n        /* Update stats for NR.THREADS to show progresses. */\n        pthread_mutex_lock(&NRPendingTrainingMutex);\n        pt->dataset_error = train_error;\n        pt->test_error = test_error;\n        if (nr->flags & NR_FLAG_CLASSIFIER) pt->class_error = class_error;\n        pt->curcycle = cycles;\n        pthread_mutex_unlock(&NRPendingTrainingMutex);\n    }\n\n    /* If auto stop is disabled, we still need to compute the test error\n     * in order to return this information to the main thread. */\n    if (!auto_stop) {\n        AnnTestError(nr->nn,\n                     nr->test.inputs,\n                     nr->test.outputs,\n                     nr->test.len, &test_error, &class_error);\n    }\n\n    /* If both autostop and backtracking are enabled, we may have\n     * a better network saved! */\n    if (auto_stop && backtrack) {\n        if (saved && saved_error < test_error) {\n            #ifdef NR_TRAINING_DEBUG\n            printf(\"BACKTRACK: Saved network used!\\n\");\n            #endif\n            AnnFree(nr->nn);\n            nr->nn = saved;\n            test_error = saved_error;\n            train_error = saved_train_error;\n            class_error = saved_class_error;\n        } else if (saved) {\n            AnnFree(saved);\n        }\n    }\n\n    if (nr->flags & NR_FLAG_CLASSIFIER) nr->test_class_error = class_error;\n    nr->dataset_error = train_error;\n    nr->test_error = test_error;\n    nr->training_total_ms += NRMilliseconds()-start;\n\n    /* Signal that the training process has finished, it's up to the main\n     * thread to cleanup this training slot, copying the weights to the\n     * original neural network and reclaiming memory for the copy we\n     * used to work. */\n    pthread_mutex_lock(&NRPendingTrainingMutex);\n    pt->in_progress = 0;\n    pthread_mutex_unlock(&NRPendingTrainingMutex);\n    return NULL;\n}\n\n/* Start a background training in another thread. Return REDISMODULE_ERR if\n * there is no free slot for training, as we already reached the maximum of\n * networks we can train in parallel.\n *\n * The 'flags' argument specifies the additional NN flags to pass to the\n * training ruotine:\n *\n *  NR_FLAG_AUTO_STOP -- Automatically stop training on overtraining.\n *  NR_FLAG_BACKTRACK -- Save current NN state when overfitting is likely.\n */\nint NRStartTraining(RedisModuleCtx *ctx, RedisModuleString *key, int dbid, NRTypeObject *nr) {\n    pthread_mutex_lock(&NRPendingTrainingMutex);\n    if (NRPendingTrainingCount == NR_PENDING_TRAINING_MAX_LEN) {\n        pthread_mutex_unlock(&NRPendingTrainingMutex);\n        return REDISMODULE_ERR;\n    }\n\n    /* Setup our trainig data. */\n    NRPendingTraining *pt = &NRTrainings[NRPendingTrainingCount];\n    pt->key = RedisModule_CreateStringFromString(ctx,key);\n    RedisModule_RetainString(ctx,pt->key);\n    pt->db_id = dbid;\n    pt->in_progress = 1;\n    pt->nr = NRClone(nr,0);\n    pt->dataset_error = 0;\n    pt->test_error = 0;\n    pt->class_error = 0;\n    pt->curcycle = 0;\n    if (pthread_create(&pt->tid,NULL,NRTrainingThreadMain,pt) != 0) {\n        RedisModule_Log(ctx,\"warning\",\"Unable to create a new pthread in NRStartTraining()\");\n        RedisModule_FreeString(ctx,pt->key);\n        pt->key = NULL;\n        NRTypeReleaseObject(pt->nr);\n        pthread_mutex_unlock(&NRPendingTrainingMutex);\n        return REDISMODULE_ERR;\n    }\n    NRPendingTrainingCount++;\n    nr->flags |= NR_FLAG_TRAINING;\n    nr->flags &= ~NR_FLAG_TO_TRANSFER;\n    pthread_mutex_unlock(&NRPendingTrainingMutex);\n    return REDISMODULE_OK;\n}\n\n/* Check if there are threads that terminated the NN training, and\n * collect the info they computed (that is the new NN). */\nint NRCollectThreads(RedisModuleCtx *ctx) {\n    int collected = 0;\n    pthread_mutex_lock(&NRPendingTrainingMutex);\n    for (int j = 0; j < NRPendingTrainingCount; j++) {\n        NRPendingTraining *pt = &NRTrainings[j];\n        if (pt->in_progress == 0) {\n            /* Training terminated. Let's see if the key\n             * is still there and NN ID matches. */\n            int orig_id = RedisModule_GetSelectedDb(ctx);\n            if (orig_id != pt->db_id) RedisModule_SelectDb(ctx,pt->db_id);\n            RedisModuleKey *key = RedisModule_OpenKey(ctx,pt->key,\n                REDISMODULE_READ|REDISMODULE_WRITE);\n            if (RedisModule_ModuleTypeGetType(key) == NRType) {\n                NRTypeObject *nr = RedisModule_ModuleTypeGetValue(key);\n                if (nr->id == pt->nr->id) {\n                    NRTransferWeights(ctx,nr,pt->nr);\n                    nr->flags &= ~NR_FLAG_TRAINING;\n                }\n                RedisModule_FreeString(ctx,pt->key);\n                pt->key = NULL;\n                NRTypeReleaseObject(pt->nr);\n                NRPendingTrainingCount--;\n                memcpy(&NRTrainings[j],&NRTrainings[j+1],\n                    (NRPendingTrainingCount-j)*sizeof(NRTrainings[0]));\n            }\n            if (orig_id != pt->db_id) RedisModule_SelectDb(ctx,orig_id);\n            collected++;\n        }\n    }\n    pthread_mutex_unlock(&NRPendingTrainingMutex);\n    return collected;\n}\n\n/* ================================ Commands =============================== */\n\n/* NR.CREATE <key> <type> <inputs> [<hidden> ...] -> <outputs> [DATASET <items>]\n * [TEST <items>] [NORMALIZE] */\nint NRCreate_RedisCommand(RedisModuleCtx *ctx, RedisModuleString **argv, int argc) {\n    long long dset_size = 0, test_size = 0;\n    int layers[NR_MAX_LAYERS], num_layers = 0;\n    int flags = NR_FLAG_NONE;\n    RedisModule_AutoMemory(ctx);\n    NRCollectThreads(ctx);\n\n    if (argc < 6) return RedisModule_WrongArity(ctx);\n\n    const char *nntype = RedisModule_StringPtrLen(argv[2], NULL);\n    if (!strcasecmp(nntype,\"classifier\")) {\n        flags |= NR_FLAG_CLASSIFIER;\n    } else if (!strcasecmp(nntype,\"regressor\")) {\n        flags |= NR_FLAG_REGRESSOR;\n    } else {\n        return RedisModule_ReplyWithError(ctx,\n            \"ERR invalid neural network type. Must be \"\n            \"CLASSIFIER or REGRESSOR\");\n    }\n\n    /* Parse net layers definition. */\n    int j = 3, stop = 0;\n    while (j < argc) {\n        const char *u = RedisModule_StringPtrLen(argv[j], NULL);\n        long long units;\n\n        /* When we see -> the next layer is the final layer (output) layer. */\n        if (!strcmp(u,\"->\")) {\n            stop = 1;\n            j++;\n            continue;\n        }\n        if (RedisModule_StringToLongLong(argv[j],&units) != REDISMODULE_OK ||\n            units <= 0)\n        {\n            return RedisModule_ReplyWithError(ctx, \"ERR invalid units count\");\n        }\n        layers[num_layers++] = units;\n        j++;\n        if (stop) break;\n    }\n\n    /* Our NN library takes the definition of layers in the opposite\n     * order, swap the layers array. */\n    for (int i = 0; i < num_layers/2; i++) {\n        int t = layers[i];\n        layers[i] = layers[num_layers-1-i];\n        layers[num_layers-1-i] = t;\n    }\n\n    /* Parse the remaining options. */\n    for (; j < argc; j++) {\n        const char *o = RedisModule_StringPtrLen(argv[j], NULL);\n        long long v;\n        int lastarg = (j == argc-1);\n        if ((!strcasecmp(o,\"dataset\") ||\n             !strcasecmp(o,\"test\")) && !lastarg)\n        {\n            if ((RedisModule_StringToLongLong(argv[j+1],&v) != REDISMODULE_OK) ||\n                 v < 0)\n            {\n                return RedisModule_ReplyWithError(ctx,\n                    \"ERR invalid dataset size\");\n            }\n            if (!strcasecmp(o,\"dataset\"))\n                dset_size = v;\n            else\n                test_size = v;\n            j++;\n        } else if (!strcasecmp(o,\"normalize\")) {\n            flags |= NR_FLAG_NORMALIZE;\n        } else {\n            return RedisModule_ReplyWithError(ctx,\n                \"ERR Syntax error in NR.CREATE\");\n        }\n    }\n\n    /* Open the key, and check that's available. */\n    RedisModuleKey *key = RedisModule_OpenKey(ctx,argv[1],\n        REDISMODULE_READ|REDISMODULE_WRITE);\n    int type = RedisModule_KeyType(key);\n    if (type != REDISMODULE_KEYTYPE_EMPTY) {\n        return RedisModule_ReplyWithError(ctx,\"ERR the key name is busy\");\n    }\n\n    /* We can finally create our neural network. */\n    NRTypeObject *nr = createNRTypeObject(flags,layers,num_layers,\n                              dset_size,test_size);\n    RedisModule_ModuleTypeSetValue(key,NRType,nr);\n\n    RedisModule_ReplyWithLongLong(ctx,AnnCountWeights(nr->nn));\n    RedisModule_ReplicateVerbatim(ctx);\n    return REDISMODULE_OK;\n}\n\n/* Implements NR.RUN and NR.CLASS. */\nint NRGenericRun_RedisCommand(RedisModuleCtx *ctx, RedisModuleString **argv, int argc, int output_class) {\n    RedisModule_AutoMemory(ctx); /* Use automatic memory management. */\n    NRCollectThreads(ctx);\n\n    if (argc < 3) return RedisModule_WrongArity(ctx);\n    RedisModuleKey *key = RedisModule_OpenKey(ctx,argv[1], REDISMODULE_READ);\n    if (RedisModule_ModuleTypeGetType(key) != NRType)\n        return RedisModule_ReplyWithError(ctx,REDISMODULE_ERRORMSG_WRONGTYPE);\n\n    NRTypeObject *nr = RedisModule_ModuleTypeGetValue(key);\n    if (output_class && !(nr->flags & NR_FLAG_CLASSIFIER))\n        return RedisModule_ReplyWithError(ctx,\n            \"ERR you can't call NR.CLASS with a regressor network. \"\n            \"Use this command with a classifier network\");\n\n\n    int ilen = INPUT_UNITS(nr->nn);\n    if (argc != ilen+2)\n        return RedisModule_ReplyWithError(ctx,\n            \"ERR number of arguments does not \"\n            \"match the number of inputs in the neural network\");\n\n    for(int j = 0; j < ilen; j++) {\n        double input;\n        if (RedisModule_StringToDouble(argv[j+2],&input) != REDISMODULE_OK)\n            return RedisModule_ReplyWithError(ctx,\n                \"ERR invalid neural network input: must be a valid float \"\n                \"precision floating point number\");\n        if (nr->flags & NR_FLAG_NORMALIZE) input /= nr->inorm[j];\n        INPUT_NODE(nr->nn,j) = input;\n    }\n\n    AnnSimulate(nr->nn);\n\n    /* Output the raw net output or the class ID if the network\n     * is a classifier and the command invoked was NR.CLASS. */\n    int olen = OUTPUT_UNITS(nr->nn);\n    if (output_class) {\n        float max = OUTPUT_NODE(nr->nn,0);\n        int max_class = 0;\n        for(int j = 1; j < olen; j++) {\n            float output = OUTPUT_NODE(nr->nn,j);\n            if (output > max) {\n                max = output;\n                max_class = j;\n            }\n        }\n        RedisModule_ReplyWithLongLong(ctx, max_class);\n    } else {\n        RedisModule_ReplyWithArray(ctx,olen);\n        for(int j = 0; j < olen; j++) {\n            float output = OUTPUT_NODE(nr->nn,j);\n            if (!(nr->flags & NR_FLAG_CLASSIFIER) &&\n                 (nr->flags & NR_FLAG_NORMALIZE))\n            {\n                output *= nr->onorm[j];\n            }\n            RedisModule_ReplyWithDouble(ctx, output);\n        }\n    }\n    return REDISMODULE_OK;\n}\n\n/* NR.RUN key [input1 input2 input3 ... inputN] */\nint NRRun_RedisCommand(RedisModuleCtx *ctx, RedisModuleString **argv, int argc) {\n    return NRGenericRun_RedisCommand(ctx,argv,argc,0);\n}\n\n/* NR.CLASS key [input1 input2 input3 ... inputN] */\nint NRClass_RedisCommand(RedisModuleCtx *ctx, RedisModuleString **argv, int argc) {\n    return NRGenericRun_RedisCommand(ctx,argv,argc,1);\n}\n\n/* NR.OBSERVE key input1 [input2 input3 ... inputN] -> output [TRAIN|TEST] */\nint NRObserve_RedisCommand(RedisModuleCtx *ctx, RedisModuleString **argv, int argc) {\n    RedisModule_AutoMemory(ctx); /* Use automatic memory management. */\n    NRCollectThreads(ctx);\n\n    if (argc < 3) return RedisModule_WrongArity(ctx);\n    RedisModuleKey *key = RedisModule_OpenKey(ctx,argv[1],\n        REDISMODULE_READ|REDISMODULE_WRITE);\n    if (RedisModule_ModuleTypeGetType(key) != NRType)\n        return RedisModule_ReplyWithError(ctx,REDISMODULE_ERRORMSG_WRONGTYPE);\n\n    NRTypeObject *nr = RedisModule_ModuleTypeGetValue(key);\n    int ilen = INPUT_UNITS(nr->nn);\n    int olen = OUTPUT_UNITS(nr->nn);\n    int oargs = (nr->flags & NR_FLAG_CLASSIFIER) ? 1 : olen;\n    int target = NR_INSERT_NO_TARGET;\n\n    /* The last argument may specify the training target:\n     * testing or training dataset. */\n    if (!strcasecmp(RedisModule_StringPtrLen(argv[argc-1],NULL),\"train\")) {\n        target = NR_INSERT_TRAIN;\n        argc--;\n    } else if (!strcasecmp(RedisModule_StringPtrLen(argv[argc-1],NULL),\"test\")){\n        target = NR_INSERT_TEST;\n        argc--;\n    }\n\n    if (argc != oargs+ilen+3)\n        return RedisModule_ReplyWithError(ctx,\n            \"ERR number of arguments does not \"\n            \"match the number of inputs and outputs in the neural network\");\n\n    const char *sep = RedisModule_StringPtrLen(argv[ilen+2], NULL);\n    if (strcmp(sep,\"->\")) {\n        return RedisModule_ReplyWithError(ctx,\n            \"ERR no '->' separtor in the correct position between inputs and \"\n            \"outputs: are you sure your training data is correct?\");\n    }\n\n    float *inputs = RedisModule_Alloc(sizeof(float)*ilen);\n    float *outputs = RedisModule_Alloc(sizeof(float)*olen);\n\n    for(int j = 2; j < argc; j++) {\n        double val;\n        if (j == ilen+2) continue; /* -> separator. */\n        if (RedisModule_StringToDouble(argv[j],&val) != REDISMODULE_OK) {\n            RedisModule_Free(inputs);\n            RedisModule_Free(outputs);\n            return RedisModule_ReplyWithError(ctx,\n                \"ERR invalid neural network input: must be a valid float \"\n                \"precision floating point number\");\n        }\n        if (j < ilen+2) {\n            inputs[j-2] = val;\n        } else {\n            if (nr->flags & NR_FLAG_CLASSIFIER) {\n                int classid = val;\n                if (classid != val || val >= olen || val < 0) {\n                    RedisModule_Free(inputs);\n                    RedisModule_Free(outputs);\n                    return RedisModule_ReplyWithError(ctx,\n                        \"ERR classifier network output must be an integer \"\n                        \"in the range from 0 to outputs-1.\");\n                }\n                memset(outputs,0,sizeof(float)*olen);\n                outputs[classid] = 1;\n            } else {\n                outputs[j-ilen-3] = val;\n            }\n        }\n    }\n\n    NRTypeInsertData(nr,inputs,outputs,target);\n    RedisModule_Free(inputs);\n    RedisModule_Free(outputs);\n\n    RedisModule_ReplyWithArray(ctx,2);\n    RedisModule_ReplyWithLongLong(ctx, nr->dataset.len);\n    RedisModule_ReplyWithLongLong(ctx, nr->test.len);\n    return REDISMODULE_OK;\n}\n\n/* NR.TRAIN key [MAXCYCLES <count>] [MAXTIME <count>] [AUTOSTOP] */\nint NRTrain_RedisCommand(RedisModuleCtx *ctx, RedisModuleString **argv, int argc) {\n    RedisModule_AutoMemory(ctx); /* Use automatic memory management. */\n    NRCollectThreads(ctx);\n\n    if (argc < 2) return RedisModule_WrongArity(ctx);\n    RedisModuleKey *key = RedisModule_OpenKey(ctx,argv[1],\n        REDISMODULE_READ|REDISMODULE_WRITE);\n    if (RedisModule_ModuleTypeGetType(key) != NRType)\n        return RedisModule_ReplyWithError(ctx,REDISMODULE_ERRORMSG_WRONGTYPE);\n\n    NRTypeObject *nr = RedisModule_ModuleTypeGetValue(key);\n    if (nr->flags & NR_FLAG_TRAINING)\n        return RedisModule_ReplyWithError(ctx,\n            \"ERR neural network training already in progress\");\n\n    nr->training_max_cycles = 0;\n    nr->training_max_ms = 10000;\n    nr->flags &= ~(NR_FLAG_AUTO_STOP|NR_FLAG_BACKTRACK);\n\n    for (int j = 2; j < argc; j++) {\n        const char *o = RedisModule_StringPtrLen(argv[j], NULL);\n        long long v;\n        int lastarg = (j == argc-1);\n\n        if (!strcasecmp(o,\"autostop\")) {\n            nr->flags |= NR_FLAG_AUTO_STOP;\n        } else if (!strcasecmp(o,\"backtrack\")) {\n            nr->flags |= NR_FLAG_BACKTRACK;\n        } else if (!strcasecmp(o,\"maxcycles\") && !lastarg) {\n            if (RedisModule_StringToLongLong(argv[++j],&v) != REDISMODULE_OK) {\n                return RedisModule_ReplyWithError(ctx,\n                    \"ERR invalid number of cycles\");\n            }\n            nr->training_max_cycles = v;\n        } else if (!strcasecmp(o,\"maxtime\") && !lastarg) {\n            if (RedisModule_StringToLongLong(argv[++j],&v) != REDISMODULE_OK) {\n                return RedisModule_ReplyWithError(ctx,\n                    \"ERR invalid number of milliseconds of time\");\n            }\n            nr->training_max_ms = v;\n        } else {\n            return RedisModule_ReplyWithError(ctx,\n                \"ERR Syntax error in NR.TRAIN\");\n        }\n    }\n\n    /* Overfitting detection compares error rate in testing/training data,\n     * so does not work without entries in the testing dataset. */\n    if (nr->flags & NR_FLAG_AUTO_STOP && nr->test.len == 0) {\n        return RedisModule_ReplyWithError(ctx,\n            \"ERR Can't start training with AUTOSTOP option: \"\n            \"overfitting detection requires a non zero length testing dataset\");\n    }\n\n    if (NRStartTraining(ctx,argv[1],RedisModule_GetSelectedDb(ctx),nr) ==\n        REDISMODULE_ERR)\n    {\n        return RedisModule_ReplyWithError(ctx,\n            \"ERR Can't train the neural network: \"\n            \"too many NNs already training\");\n    } else {\n        return RedisModule_ReplyWithSimpleString(ctx,\"Training has started\");\n    }\n}\n\n/* NR.RESET key -- Set random weights in the NN and clear training stats. */\nint NRReset_RedisCommand(RedisModuleCtx *ctx, RedisModuleString **argv, int argc) {\n    RedisModule_AutoMemory(ctx); /* Use automatic memory management. */\n    NRCollectThreads(ctx);\n\n    if (argc != 2) return RedisModule_WrongArity(ctx);\n    RedisModuleKey *key = RedisModule_OpenKey(ctx,argv[1],\n        REDISMODULE_READ|REDISMODULE_WRITE);\n    if (RedisModule_ModuleTypeGetType(key) != NRType)\n        return RedisModule_ReplyWithError(ctx,REDISMODULE_ERRORMSG_WRONGTYPE);\n\n    NRTypeObject *nr = RedisModule_ModuleTypeGetValue(key);\n\n    /* Change the ID so that if there is a training in progress it will\n     * not update the weights of this network. */\n    nr->id = NRNextId++;\n\n    /* Reset training stats. */\n    nr->training_total_steps = 0;\n    nr->training_total_ms = 0;\n    nr->training_max_cycles = 0;\n    nr->training_max_ms = 0;\n    nr->dataset_error = 0;\n    nr->test_error = 0;\n    nr->test_class_error = 0;\n\n    /* Set random weights in the neural network, which is\n     * \"untrain\" the network. */\n    AnnSetRandomWeights(nr->nn);\n\n    return RedisModule_ReplyWithSimpleString(ctx,\"OK\");\n}\n\n/* NR.INFO key */\nint NRInfo_RedisCommand(RedisModuleCtx *ctx, RedisModuleString **argv, int argc) {\n    char buf[128];\n\n    RedisModule_AutoMemory(ctx); /* Use automatic memory management. */\n    NRCollectThreads(ctx);\n\n    if (argc != 2) return RedisModule_WrongArity(ctx);\n    RedisModuleKey *key = RedisModule_OpenKey(ctx,argv[1], REDISMODULE_READ);\n    if (RedisModule_ModuleTypeGetType(key) != NRType)\n        return RedisModule_ReplyWithError(ctx,REDISMODULE_ERRORMSG_WRONGTYPE);\n\n    NRTypeObject *nr = RedisModule_ModuleTypeGetValue(key);\n\n    int fields = 15;\n    if (nr->flags & NR_FLAG_CLASSIFIER) fields++;\n    RedisModule_ReplyWithArray(ctx,fields*2);\n\n    RedisModule_ReplyWithSimpleString(ctx,\"id\");\n    RedisModule_ReplyWithLongLong(ctx,nr->id);\n\n    RedisModule_ReplyWithSimpleString(ctx,\"type\");\n    RedisModule_ReplyWithSimpleString(ctx,\n        (nr->flags & NR_FLAG_CLASSIFIER) ? \"classifier\" : \"regressor\");\n\n    RedisModule_ReplyWithSimpleString(ctx,\"auto-normalization\");\n    RedisModule_ReplyWithLongLong(ctx,!!(nr->flags & NR_FLAG_NORMALIZE));\n\n    RedisModule_ReplyWithSimpleString(ctx,\"training\");\n    RedisModule_ReplyWithLongLong(ctx,!!(nr->flags & NR_FLAG_TRAINING));\n\n    RedisModule_ReplyWithSimpleString(ctx,\"layout\");\n    RedisModule_ReplyWithArray(ctx,LAYERS(nr->nn));\n    for (int i = LAYERS(nr->nn)-1; i >= 0; i--) {\n        int units = UNITS(nr->nn,i);\n        if (i != 0) units--; /* Don't count the bias unit. */\n        RedisModule_ReplyWithLongLong(ctx,units);\n    }\n\n    RedisModule_ReplyWithSimpleString(ctx,\"training-dataset-maxlen\");\n    RedisModule_ReplyWithLongLong(ctx,nr->dataset.maxlen);\n\n    RedisModule_ReplyWithSimpleString(ctx,\"training-dataset-len\");\n    RedisModule_ReplyWithLongLong(ctx,nr->dataset.len);\n\n    RedisModule_ReplyWithSimpleString(ctx,\"test-dataset-maxlen\");\n    RedisModule_ReplyWithLongLong(ctx,nr->test.maxlen);\n\n    RedisModule_ReplyWithSimpleString(ctx,\"test-dataset-len\");\n    RedisModule_ReplyWithLongLong(ctx,nr->test.len);\n\n    RedisModule_ReplyWithSimpleString(ctx,\"training-total-steps\");\n    RedisModule_ReplyWithLongLong(ctx,nr->training_total_steps);\n\n    RedisModule_ReplyWithSimpleString(ctx,\"training-total-cycles\");\n    RedisModule_ReplyWithLongLong(ctx,\n            nr->dataset.len ?\n            (nr->training_total_steps / nr->dataset.len) : 0);\n\n    RedisModule_ReplyWithSimpleString(ctx,\"training-total-seconds\");\n    {\n        snprintf(buf,sizeof(buf),\"%.02f\",(float)nr->training_total_ms/1000);\n        RedisModule_ReplyWithSimpleString(ctx,buf);\n    }\n\n    RedisModule_ReplyWithSimpleString(ctx,\"dataset-error\");\n    RedisModule_ReplyWithDouble(ctx,nr->dataset_error);\n\n    RedisModule_ReplyWithSimpleString(ctx,\"test-error\");\n    RedisModule_ReplyWithDouble(ctx,nr->test_error);\n\n    if (nr->flags & NR_FLAG_CLASSIFIER) {\n        RedisModule_ReplyWithSimpleString(ctx,\"classification-errors-perc\");\n        {\n            snprintf(buf,sizeof(buf),\"%.02f\",(float)nr->test_class_error);\n            RedisModule_ReplyWithSimpleString(ctx,buf);\n        }\n    }\n\n    RedisModule_ReplyWithSimpleString(ctx,\"overfitting-detected\");\n    RedisModule_ReplyWithSimpleString(ctx, (nr->flags & NR_FLAG_OF_DETECTED) ? \"yes\" : \"no\");\n\n    return REDISMODULE_OK;\n}\n\n/* NR.THREADS */\nint NRThreads_RedisCommand(RedisModuleCtx *ctx, RedisModuleString **argv, int argc) {\n    RedisModule_AutoMemory(ctx); /* Use automatic memory management. */\n    NRCollectThreads(ctx);\n    UNUSED(argv);\n\n    if (argc != 1) return RedisModule_WrongArity(ctx);\n\n    pthread_mutex_lock(&NRPendingTrainingMutex);\n    RedisModule_ReplyWithArray(ctx,NRPendingTrainingCount);\n    for (int j = 0; j < NRPendingTrainingCount; j++) {\n        char buf[1024];\n        NRPendingTraining *pt = &NRTrainings[j];\n        const char *keyname = RedisModule_StringPtrLen(pt->key,NULL);\n        snprintf(buf,sizeof(buf),\"nn_id=%llu cycle=%d key=%s db=%d maxtime=%llu maxcycles=%llu trainerr=%f testerr=%f classerr=%f\",\n            (unsigned long long)pt->nr->id,\n            pt->curcycle,\n            keyname, pt->db_id,\n            (unsigned long long)pt->nr->training_max_ms,\n            (unsigned long long)pt->nr->training_max_cycles,\n            pt->dataset_error,\n            pt->test_error,\n            pt->class_error);\n        RedisModule_ReplyWithSimpleString(ctx,buf);\n    }\n    pthread_mutex_unlock(&NRPendingTrainingMutex);\n    return REDISMODULE_OK;\n}\n\n/* NR.GETDATA key dataset rownum */\nint NRGetdata_RedisCommand(RedisModuleCtx *ctx, RedisModuleString **argv, int argc) {\n    RedisModule_AutoMemory(ctx); /* Use automatic memory management. */\n    NRCollectThreads(ctx);\n\n    if (argc != 4) return RedisModule_WrongArity(ctx);\n    RedisModuleKey *key = RedisModule_OpenKey(ctx,argv[1], REDISMODULE_READ);\n    if (RedisModule_ModuleTypeGetType(key) != NRType)\n        return RedisModule_ReplyWithError(ctx,REDISMODULE_ERRORMSG_WRONGTYPE);\n\n    NRTypeObject *nr = RedisModule_ModuleTypeGetValue(key);\n\n    int ilen = INPUT_UNITS(nr->nn);\n    int olen = OUTPUT_UNITS(nr->nn);\n    NRDataset *target = NULL;\n    long long idx;\n\n    /* The last argument may specify the training target:\n     * testing or training dataset. */\n    if (!strcasecmp(RedisModule_StringPtrLen(argv[2],NULL),\"train\")) {\n        target = &nr->dataset;\n    } else if (!strcasecmp(RedisModule_StringPtrLen(argv[2],NULL),\"test\")){\n        target = &nr->test;\n    } else {\n        return RedisModule_ReplyWithError(ctx,\n            \"ERR please specify as source either TRAIN or TEST\");\n    }\n\n    /* Get the row index. */\n    if (RedisModule_StringToLongLong(argv[3],&idx) != REDISMODULE_OK ||\n        idx < 0)\n    {\n        return RedisModule_ReplyWithError(ctx, \"ERR invalid row specified\");\n    } else if (idx >= target->maxlen) {\n        return RedisModule_ReplyWithNull(ctx);\n    }\n\n    RedisModule_ReplyWithArray(ctx,2);\n\n    /* Send inputs */\n    RedisModule_ReplyWithArray(ctx,ilen);\n    for(int j = 0; j < ilen; j++) {\n        double input = target->inputs[ilen*idx+j];\n        RedisModule_ReplyWithDouble(ctx,input);\n    }\n\n    /* Send outputs */\n    RedisModule_ReplyWithArray(ctx,olen);\n    for(int j = 0; j < olen; j++) {\n        double output = target->outputs[olen*idx+j];\n        RedisModule_ReplyWithDouble(ctx,output);\n    }\n    return REDISMODULE_OK;\n}\n\n\n/* =============================== Type methods ============================= */\n\n/* Helper for NRTypeRdbSave(): serialize a NRDataset dataset to RDB. */\nvoid NRTypeRdbSaveDataset(RedisModuleIO *rdb, NRDataset *ds, uint32_t ilen, uint32_t olen) {\n    RedisModule_SaveUnsigned(rdb,ds->len);\n    RedisModule_SaveUnsigned(rdb,ds->maxlen);\n    for (uint32_t j = 0; j < ilen*ds->len; j++)\n        RedisModule_SaveFloat(rdb,ds->inputs[j]);\n    for (uint32_t j = 0; j < olen*ds->len; j++)\n        RedisModule_SaveFloat(rdb,ds->outputs[j]);\n}\n\n/* Serialize a neural network object with its associated dataset\n * in RDB format. */\nvoid NRTypeRdbSave(RedisModuleIO *rdb, void *value) {\n    NRTypeObject *nr = value;\n\n    /* Save the neural network layout. */\n    RedisModule_SaveUnsigned(rdb,LAYERS(nr->nn));\n    for (int j = 0; j < LAYERS(nr->nn); j++) {\n        int units = UNITS(nr->nn,j);\n        if (j != 0) units--; /* Don't count the bias unit. */\n        RedisModule_SaveUnsigned(rdb,units);\n    }\n\n    /* Save the object metadata. */\n    RedisModule_SaveUnsigned(rdb,nr->flags & NR_FLAG_TO_PRESIST);\n    RedisModule_SaveUnsigned(rdb,nr->id);\n    RedisModule_SaveUnsigned(rdb,nr->training_total_steps);\n    RedisModule_SaveUnsigned(rdb,nr->training_total_ms);\n    RedisModule_SaveUnsigned(rdb,nr->training_max_cycles);\n    RedisModule_SaveUnsigned(rdb,nr->training_max_ms);\n    RedisModule_SaveFloat(rdb,nr->dataset_error);\n    RedisModule_SaveFloat(rdb,nr->test_error);\n    RedisModule_SaveFloat(rdb,nr->test_class_error);\n\n    /* Save the neural network weights and biases. We start\n     * at layer 1 since the first layer are just outputs. */\n    for (int j = 1; j < LAYERS(nr->nn); j++) {\n        int weights = WEIGHTS(nr->nn,j);\n        for (int i = 0; i < weights; i++)\n            RedisModule_SaveFloat(rdb,nr->nn->layer[j].weight[i]);\n        for (int i = 0; i < weights; i++)\n            RedisModule_SaveFloat(rdb,nr->nn->layer[j].delta[i]);\n        for (int i = 0; i < weights; i++)\n            RedisModule_SaveFloat(rdb,nr->nn->layer[j].pgradient[i]);\n    }\n\n    /* Save the normalization vectors. */\n    uint32_t ilen = INPUT_UNITS(nr->nn);\n    uint32_t olen = OUTPUT_UNITS(nr->nn);\n    for (uint32_t j = 0; j < ilen; j++) RedisModule_SaveFloat(rdb,nr->inorm[j]);\n    for (uint32_t j = 0; j < olen; j++) RedisModule_SaveFloat(rdb,nr->onorm[j]);\n\n    /* Save the dataset. */\n    NRTypeRdbSaveDataset(rdb,&nr->dataset,ilen,olen);\n    NRTypeRdbSaveDataset(rdb,&nr->test,ilen,olen);\n}\n\n/* Helper for NRTypeRdbLoad(): deserialize a NRDataset dataset from RDB. */\nvoid NRTypeRdbLoadDataset(RedisModuleIO *rdb, NRDataset *ds, uint32_t ilen, uint32_t olen) {\n    ds->len = RedisModule_LoadUnsigned(rdb);\n    ds->maxlen = RedisModule_LoadUnsigned(rdb);\n\n    if (ds->len == 0) return;\n\n    ds->inputs = RedisModule_Alloc(ilen*ds->len*sizeof(float));\n    ds->outputs = RedisModule_Alloc(olen*ds->len*sizeof(float));\n\n    for (uint32_t j = 0; j < ilen*ds->len; j++)\n        ds->inputs[j] = RedisModule_LoadFloat(rdb);\n    for (uint32_t j = 0; j < olen*ds->len; j++)\n        ds->outputs[j] = RedisModule_LoadFloat(rdb);\n}\n\n/* Load a neural network and its associated dataset from RDB. */\nvoid *NRTypeRdbLoad(RedisModuleIO *rdb, int encver) {\n    /* As long as the module is not stable, we don't care about\n     * loading old versions of the encoding. */\n    if (encver != 2) {\n        RedisModule_LogIOError(rdb,\"warning\",\"Sorry the Neural Redis module only supports RDB files written with the encoding version %d. This file has encoding version %d, and was likely written by a previous version of this module that is now deprecated. Once the module will be stable we'll start supporting older versions of the encodings, in case we switch to newer encodings.\", NR_RDB_ENC_VER, encver);\n        return NULL;\n    }\n\n    /* Load the network layout. */\n    uint64_t numlayers = RedisModule_LoadUnsigned(rdb);\n    int *layers = RedisModule_Alloc(sizeof(int)*numlayers);\n    for (uint32_t j = 0; j < numlayers; j++)\n        layers[j] = RedisModule_LoadUnsigned(rdb);\n\n    /* Load flags and create the object. */\n    uint32_t flags = RedisModule_LoadUnsigned(rdb);\n    NRTypeObject *nr = createNRTypeObject(flags,layers,numlayers,0,0);\n    RedisModule_Free(layers);\n\n    /* Load and set the object metadata. */\n    nr->id = RedisModule_LoadUnsigned(rdb);\n    nr->training_total_steps = RedisModule_LoadUnsigned(rdb);\n    nr->training_total_ms = RedisModule_LoadUnsigned(rdb);\n    nr->training_max_cycles = RedisModule_LoadUnsigned(rdb);\n    nr->training_max_ms = RedisModule_LoadUnsigned(rdb);\n    nr->dataset_error = RedisModule_LoadFloat(rdb);\n    nr->test_error = RedisModule_LoadFloat(rdb);\n    nr->test_class_error = RedisModule_LoadFloat(rdb);\n\n    /* Load the neural network weights. */\n    for (int j = 1; j < LAYERS(nr->nn); j++) {\n        int weights = WEIGHTS(nr->nn,j);\n        for (int i = 0; i < weights; i++)\n            nr->nn->layer[j].weight[i] = RedisModule_LoadFloat(rdb);\n        for (int i = 0; i < weights; i++)\n            nr->nn->layer[j].delta[i] = RedisModule_LoadFloat(rdb);\n        for (int i = 0; i < weights; i++)\n            nr->nn->layer[j].pgradient[i] = RedisModule_LoadFloat(rdb);\n    }\n\n    /* Load the normalization vector. */\n    uint32_t ilen = INPUT_UNITS(nr->nn);\n    uint32_t olen = OUTPUT_UNITS(nr->nn);\n    for (uint32_t j = 0; j < ilen; j++)\n        nr->inorm[j] = RedisModule_LoadFloat(rdb);\n    for (uint32_t j = 0; j < olen; j++)\n        nr->onorm[j] = RedisModule_LoadFloat(rdb);\n\n    /* Load the dataset. */\n    NRTypeRdbLoadDataset(rdb,&nr->dataset,ilen,olen);\n    NRTypeRdbLoadDataset(rdb,&nr->test,ilen,olen);\n\n    return nr;\n}\n\nvoid NRTypeAofRewrite(RedisModuleIO *aof, RedisModuleString *key, void *value) {\n    UNUSED(aof);\n    UNUSED(key);\n    UNUSED(value);\n#if 0\n    NRTypeObject *hto = value;\n    struct NRTypeNode *node = hto->head;\n    while(node) {\n        RedisModule_EmitAOF(aof,\"HELLOTYPE.INSERT\",\"sl\",key,node->value);\n        node = node->next;\n    }\n#endif\n}\n\nvoid NRTypeFree(void *value) {\n    NRTypeReleaseObject(value);\n}\n\n/* This function must be present on each Redis module. It is used in order to\n * register the commands into the Redis server. */\nint RedisModule_OnLoad(RedisModuleCtx *ctx, RedisModuleString **argv, int argc) {\n    UNUSED(argv);\n    UNUSED(argc);\n\n    if (RedisModule_Init(ctx,\"neuralredis\",1,REDISMODULE_APIVER_1)\n        == REDISMODULE_ERR) return REDISMODULE_ERR;\n\n    RedisModuleTypeMethods tm = {\n        .version = REDISMODULE_TYPE_METHOD_VERSION,\n        .rdb_load = NRTypeRdbLoad,\n        .rdb_save = NRTypeRdbSave,\n        .aof_rewrite = NRTypeAofRewrite,\n        .free = NRTypeFree\n    };\n\n    NRType = RedisModule_CreateDataType(ctx,\"neural-NN\",NR_RDB_ENC_VER,&tm);\n    if (NRType == NULL) return REDISMODULE_ERR;\n\n    if (RedisModule_CreateCommand(ctx,\"nr.create\",\n        NRCreate_RedisCommand,\"write deny-oom\",1,1,1) == REDISMODULE_ERR)\n        return REDISMODULE_ERR;\n\n    if (RedisModule_CreateCommand(ctx,\"nr.run\",\n        NRRun_RedisCommand,\"readonly\",1,1,1) == REDISMODULE_ERR)\n        return REDISMODULE_ERR;\n\n    if (RedisModule_CreateCommand(ctx,\"nr.class\",\n        NRClass_RedisCommand,\"readonly\",1,1,1) == REDISMODULE_ERR)\n        return REDISMODULE_ERR;\n\n    if (RedisModule_CreateCommand(ctx,\"nr.observe\",\n        NRObserve_RedisCommand,\"write deny-oom\",1,1,1) == REDISMODULE_ERR)\n        return REDISMODULE_ERR;\n\n    if (RedisModule_CreateCommand(ctx,\"nr.info\",\n        NRInfo_RedisCommand,\"readonly\",1,1,1) == REDISMODULE_ERR)\n        return REDISMODULE_ERR;\n\n    if (RedisModule_CreateCommand(ctx,\"nr.train\",\n        NRTrain_RedisCommand,\"write\",1,1,1) == REDISMODULE_ERR)\n        return REDISMODULE_ERR;\n\n    if (RedisModule_CreateCommand(ctx,\"nr.reset\",\n        NRReset_RedisCommand,\"write\",1,1,1) == REDISMODULE_ERR)\n        return REDISMODULE_ERR;\n\n    if (RedisModule_CreateCommand(ctx,\"nr.threads\",\n        NRThreads_RedisCommand,\"\",1,1,1) == REDISMODULE_ERR)\n        return REDISMODULE_ERR;\n\n    if (RedisModule_CreateCommand(ctx,\"nr.getdata\",\n        NRGetdata_RedisCommand,\"readonly\",1,1,1) == REDISMODULE_ERR)\n        return REDISMODULE_ERR;\n\n    return REDISMODULE_OK;\n}\n"
        },
        {
          "name": "nn.c",
          "type": "blob",
          "size": 29.4775390625,
          "content": "/* RPROP Neural Networks implementation\n * See: http://deeplearning.cs.cmu.edu/pdfs/Rprop.pdf\n *\n * Copyright (c) 2003-2016, Salvatore Sanfilippo <antirez at gmail dot com>\n * All rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n *\n *   * Redistributions of source code must retain the above copyright notice,\n *     this list of conditions and the following disclaimer.\n *   * Redistributions in binary form must reproduce the above copyright\n *     notice, this list of conditions and the following disclaimer in the\n *     documentation and/or other materials provided with the distribution.\n *   * Neither the name of Disque nor the names of its contributors may be used\n *     to endorse or promote products derived from this software without\n *     specific prior written permission.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE\n * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n * POSSIBILITY OF SUCH DAMAGE.\n */\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <time.h>\n#include <string.h>\n\n#if defined(USE_AVX512)\n#define USING_SIMD\n#include <immintrin.h>\n\ntypedef __m512 simdf_t;\n#define  SIMDF_SIZE 16\n\n#define simdf_zero() _mm512_setzero_ps()\n#define simdf_set1f(x) _mm512_set1_ps(x)\n#define simdf_loadu(x) _mm512_loadu_ps(x)\n#define simdf_mul(a,b) _mm512_mul_ps(a,b)\n#define simdf_add(a,b) _mm512_add_ps(a,b)\n#define simdf_storeu(a,b) _mm512_storeu_ps(a,b)\n\n//let the compiler optmize this\n#define simdf_sum(x) (x[0] + x[1] + x[2] + x[3] + x[4] + x[5] + x[6] + x[7] + \\\n\t\t\t\tx[8] + x[9] + x[10] + x[11] + x[12] + x[13] + x[14] + x[15])\n\n#define simdf_show(x) printf(\"%d : %f, %f, %f, %f, %f, %f, %f, %f, %f, %f, %f, %f, %f, %f, %f, %f\\n\", \\\n\t\t\t\t__LINE__, x[0], x[1], x[2], x[3], x[4], x[5], x[6], x[7], \\\n\t\t\t\tx[8], x[9], x[10], x[11], x[12], x[13], x[14], x[15]);\n#endif\n\n#if defined(USE_AVX)\n#define USING_SIMD\n#include <immintrin.h>\n\ntypedef __m256 simdf_t;\n#define  SIMDF_SIZE 8\n\n#define simdf_zero() _mm256_setzero_ps()\n#define simdf_set1f(x) _mm256_set1_ps(x)\n#define simdf_loadu(x) _mm256_loadu_ps(x)\n#define simdf_mul(a,b) _mm256_mul_ps(a,b)\n#define simdf_add(a,b) _mm256_add_ps(a,b)\n#define simdf_storeu(a,b) _mm256_storeu_ps(a,b)\n\n//let the compiler optmize this\n#define simdf_sum(x) (x[0] + x[1] + x[2] + x[3] + x[4] + x[5] + x[6] + x[7])\n\n#define simdf_show(x) printf(\"%d : %f, %f, %f, %f, %f, %f, %f, %f\\n\", \\\n\t\t\t\t__LINE__, x[0], x[1], x[2], x[3], x[4], x[5], x[6], x[7]);\n#endif\n\n#if defined(USE_SSE)\n#define USING_SIMD\n#include <xmmintrin.h>\n#include <pmmintrin.h>\ntypedef __m128 simdf_t;\n#define  SIMDF_SIZE 4\n\n#define simdf_zero() _mm_setzero_ps()\n#define simdf_set1f(x) _mm_set1_ps(x)\n#define simdf_loadu(x) _mm_loadu_ps(x)\n#define simdf_mul(a,b) _mm_mul_ps(a,b)\n#define simdf_add(a,b) _mm_add_ps(a,b)\n#define simdf_storeu(a,b) _mm_storeu_ps(a,b)\n\n//let the compiler optmize this\n#define simdf_sum(x) (x[0] + x[1] + x[2] + x[3])\n\n#define simdf_show(x) printf(\"%d : %f, %f, %f, %f\\n\", __LINE__, x[0], x[1], x[2], x[3]);\n#endif\n\n#if defined(USE_NEON)\n#define USING_SIMD\n#include <arm_neon.h>\n\ntypedef float32x4_t simdf_t;\n#define  SIMDF_SIZE 4\n\n#define simdf_zero() vdupq_n_f32(0.0f)\n#define simdf_set1f(x) vdupq_n_f32(x);\n#define simdf_loadu(x) vld1q_f32(x)\n#define simdf_mul(a,b) vmulq_f32(a,b)\n#define simdf_add(a,b) vaddq_f32(a,b)\n#define simdf_storeu(a,b) vst1q_f32((float32_t*)a,b)\n\n//let the compiler optmize this\n#define simdf_sum(x) (x[0] + x[1] + x[2] + x[3])\n\n#define simdf_show(x) printf(\"%d : %f, %f, %f, %f\\n\", __LINE__, x[0], x[1], x[2], x[3]);\n#endif\n\n#include \"nn.h\"\n\n/* Node Transfer Function */\nfloat sigmoid(float x) {\n    return (float)1/(1+exp(-x));\n}\n\nfloat relu(float x) {\n    return (x > 0) ? x : 0;\n}\n\n/* Reset layer data to zero-units */\nvoid AnnResetLayer(struct AnnLayer *layer) {\n    layer->units = 0;\n    layer->output = NULL;\n    layer->error = NULL;\n    layer->weight = NULL;\n    layer->gradient = NULL;\n    layer->pgradient = NULL;\n    layer->delta = NULL;\n    layer->sgradient = NULL;\n}\n\n/* Allocate and return an initialized N-layers network */\nstruct Ann *AnnAlloc(int layers) {\n    struct Ann *net;\n    int i;\n\n    /* Alloc the net structure */\n    if ((net = malloc(sizeof(*net))) == NULL)\n        return NULL;\n    /* Alloc layers */\n    if ((net->layer = malloc(sizeof(struct AnnLayer)*layers)) == NULL) {\n        free(net);\n        return NULL;\n    }\n    net->layers = layers;\n    net->flags = 0;\n    net->rprop_nminus = DEFAULT_RPROP_NMINUS;\n    net->rprop_nplus = DEFAULT_RPROP_NPLUS;\n    net->rprop_maxupdate = DEFAULT_RPROP_MAXUPDATE;\n    net->rprop_minupdate = DEFAULT_RPROP_MINUPDATE;\n    /* Init layers */\n    for (i = 0; i < layers; i++)\n        AnnResetLayer(&net->layer[i]);\n    return net;\n}\n\n/* Free a single layer */\nvoid AnnFreeLayer(struct AnnLayer *layer)\n{\n    free(layer->output);\n    free(layer->error);\n    free(layer->weight);\n    free(layer->gradient);\n    free(layer->pgradient);\n    free(layer->delta);\n    free(layer->sgradient);\n    AnnResetLayer(layer);\n}\n\n/* Free the target net */\nvoid AnnFree(struct Ann *net)\n{\n    int i;\n\n    /* Free layer data */\n    for (i = 0; i < net->layers; i++) AnnFreeLayer(&net->layer[i]);\n    /* Free allocated layers structures */\n    free(net->layer);\n    /* And the main structure itself */\n    free(net);\n}\n\n/* Init a layer of the net with the specified number of units.\n * Return non-zero on out of memory. */\nint AnnInitLayer(struct Ann *net, int i, int units, int bias) {\n    if (bias) units++; /* Take count of the bias unit */\n    net->layer[i].output = malloc(sizeof(float)*units);\n    net->layer[i].error = malloc(sizeof(float)*units);\n    if (i) { /* not for output layer */\n        net->layer[i].weight =\n            malloc(sizeof(float)*units*net->layer[i-1].units);\n        net->layer[i].gradient =\n            malloc(sizeof(float)*units*net->layer[i-1].units);\n        net->layer[i].pgradient =\n            malloc(sizeof(float)*units*net->layer[i-1].units);\n        net->layer[i].delta =\n            malloc(sizeof(float)*units*net->layer[i-1].units);\n        net->layer[i].sgradient =\n            malloc(sizeof(float)*units*net->layer[i-1].units);\n    }\n    net->layer[i].units = units;\n    /* Check for out of memory conditions */\n    if (net->layer[i].output == NULL ||\n        net->layer[i].error == NULL ||\n        (i && net->layer[i].weight == NULL) ||\n        (i && net->layer[i].gradient == NULL) ||\n        (i && net->layer[i].pgradient == NULL) ||\n        (i && net->layer[i].sgradient == NULL) ||\n        (i && net->layer[i].delta == NULL))\n    {\n        AnnFreeLayer(&net->layer[i]);\n        AnnResetLayer(&net->layer[i]);\n        return 1;\n    }\n    /* Set all the values to zero */\n    memset(net->layer[i].output, 0, sizeof(float)*units);\n    memset(net->layer[i].error, 0, sizeof(float)*units);\n    if (i) {\n        memset(net->layer[i].weight, 0,\n            sizeof(float)*units*net->layer[i-1].units);\n        memset(net->layer[i].gradient, 0,\n            sizeof(float)*units*net->layer[i-1].units);\n        memset(net->layer[i].pgradient, 0,\n            sizeof(float)*units*net->layer[i-1].units);\n        memset(net->layer[i].delta, 0,\n            sizeof(float)*units*net->layer[i-1].units);\n        memset(net->layer[i].sgradient, 0,\n            sizeof(float)*units*net->layer[i-1].units);\n    }\n    /* Set the bias unit output to 1 */\n    if (bias) net->layer[i].output[units-1] = 1;\n    return 0;\n}\n\n/* Clone a network. On out of memory NULL is returned. */\nstruct Ann *AnnClone(struct Ann* net) {\n    struct Ann* copy;\n    int j;\n\n    if ((copy = AnnAlloc(LAYERS(net))) == NULL) return NULL;\n    for (j = 0; j < LAYERS(net); j++) {\n        struct AnnLayer *ldst, *lsrc;\n        int units = UNITS(net,j);\n        int bias = j > 0;\n        if (AnnInitLayer(copy, j, units-bias, bias)) {\n            AnnFree(copy);\n            return NULL;\n        }\n        lsrc = &net->layer[j];\n        ldst = &copy->layer[j];\n        if (lsrc->output)\n            memcpy(ldst->output, lsrc->output, sizeof(float)*units);\n        if (lsrc->error)\n            memcpy(ldst->error, lsrc->error, sizeof(float)*units);\n        if (j) {\n            int weights = WEIGHTS(net,j);\n            if (lsrc->weight)\n                memcpy(ldst->weight, lsrc->weight, sizeof(float)*weights);\n            if (lsrc->gradient)\n                memcpy(ldst->gradient, lsrc->gradient, sizeof(float)*weights);\n            if (lsrc->pgradient)\n                memcpy(ldst->pgradient, lsrc->pgradient, sizeof(float)*weights);\n            if (lsrc->delta)\n                memcpy(ldst->delta, lsrc->delta, sizeof(float)*weights);\n            if (lsrc->sgradient)\n                memcpy(ldst->sgradient, lsrc->sgradient, sizeof(float)*weights);\n        }\n    }\n    copy->rprop_nminus = net->rprop_nminus;\n    copy->rprop_nplus = net->rprop_nplus;\n    copy->rprop_maxupdate = net->rprop_maxupdate;\n    copy->rprop_minupdate = net->rprop_minupdate;\n    copy->flags = net->flags;\n    return copy;\n}\n\n/* Create a N-layer input/hidden/output net.\n * The units array should specify the number of\n * units in every layer from the output to the input layer. */\nstruct Ann *AnnCreateNet(int layers, int *units) {\n    struct Ann *net;\n    int i;\n\n    if ((net = AnnAlloc(layers)) == NULL) return NULL;\n    for (i = 0; i < layers; i++) {\n        if (AnnInitLayer(net, i, units[i], i > 0)) {\n            AnnFree(net);\n            return NULL;\n        }\n    }\n    AnnSetRandomWeights(net);\n    AnnSetDeltas(net, RPROP_INITIAL_DELTA);\n    LEARN_RATE(net) = DEFAULT_LEARN_RATE;\n    return net;\n}\n\n/* Return the total number of weights this NN has. */\nsize_t AnnCountWeights(struct Ann *net) {\n    size_t weights = 0;\n    for (int i = net->layers-1; i > 0; i--) {\n        int nextunits = net->layer[i-1].units;\n        int units = net->layer[i].units;\n        if (i > 1) nextunits--; /* we don't output on bias units */\n        weights += units*nextunits;\n    }\n    return weights;\n}\n\n/* Create a 4-layer input/hidden/output net */\nstruct Ann *AnnCreateNet4(int iunits, int hunits, int hunits2, int ounits) {\n    int units[4];\n\n    units[0] = ounits;\n    units[1] = hunits2;\n    units[2] = hunits;\n    units[3] = iunits;\n    return AnnCreateNet(4, units);\n}\n\n/* Create a 3-layer input/hidden/output net */\nstruct Ann *AnnCreateNet3(int iunits, int hunits, int ounits) {\n    int units[3];\n\n    units[0] = ounits;\n    units[1] = hunits;\n    units[2] = iunits;\n    return AnnCreateNet(3, units);\n}\n\n\n/* Create a 2-layer \"linear\" network. */\nstruct Ann *AnnCreateNet2(int iunits, int ounits) {\n    int units[2];\n\n    units[0] = ounits;\n    units[1] = iunits;\n    return AnnCreateNet(2, units);\n}\n\n\nvoid AnnSimulate(struct Ann *net) {\n    int i, j, k;\n\n    for (i = net->layers-1; i > 0; i--) {\n        int nextunits = net->layer[i-1].units;\n        int units = net->layer[i].units;\n        if (i > 1) nextunits--; /* dont output on bias units */\n        for (j = 0; j < nextunits; j++) {\n            float A = 0; /* Activation final value. */\n            float *w = net->layer[i].weight + j*units;\n            float *o = net->layer[i].output;\n\n            k = 0;\n\n#ifdef USING_SIMD\n            int psteps = units/SIMDF_SIZE;\n            simdf_t sumA = simdf_zero();\n            for (int x = 0; x < psteps; x++) {\n                simdf_t weights = simdf_loadu(w);\n                simdf_t outputs = simdf_loadu(o);\n                simdf_t prod = simdf_mul(weights,outputs);\n                sumA = simdf_add(sumA, prod);\n                w += SIMDF_SIZE;\n                o += SIMDF_SIZE;\n            }\n            A += simdf_sum(sumA);\n            k += SIMDF_SIZE*psteps;\n#endif\n\n            /* Handle final piece shorter than SIMDF_SIZE . */\n            for (; k < units; k++) {\n                A += (*w++) * (*o++);\n            }\n            OUTPUT(net, i-1, j) = sigmoid(A);\n        }\n    }\n}\n\n/* Create a Tcl procedure that simulates the neural network */\nvoid Ann2Tcl(struct Ann *net) {\n    int i, j, k;\n\n    printf(\"proc ann input {\\n\");\n    printf(\"    set output {\");\n    for (i = 0; i < OUTPUT_UNITS(net); i++) {\n        printf(\"0 \");\n    }\n    printf(\"}\\n\");\n    for (i = net->layers-1; i > 0; i--) {\n        int nextunits = net->layer[i-1].units;\n        int units = net->layer[i].units;\n        if (i > 1) nextunits--; /* dont output on bias units */\n        for (j = 0; j < nextunits; j++) {\n            float W;\n            if (i == 1) {\n                printf(\"    lset output %d \", j);\n            } else {\n                printf(\"    set O_%d_%d\", i-1, j);\n            }\n            printf(\" [expr { \\\\\\n\");\n            for (k = 0; k < units; k++) {\n                W = WEIGHT(net, i, k, j);\n                if (i > 1 && k == units-1) {\n                    printf(\"        (%.9f)\", W);\n                } else if (i == net->layers-1) {\n                    printf(\"        (%.9f*[lindex $input %d])\", W, k);\n                } else {\n                    printf(\"        (%.9f*$O_%d_%d)\", W, i, k);\n                }\n                if ((k+1) < units) printf(\"+ \\\\\\n\");\n            }\n            printf(\"}]\\n\");\n            if (i == 1) {\n                printf(\"    lset output %d [expr {1/(1+exp(-[lindex $output %d]))}]\\n\", j, j);\n            } else {\n                printf(\"    lset O_%d_%d [expr {1/(1+exp(-$O_%d_%d))}]\\n\", i-1, j, i-1, j);\n            }\n        }\n    }\n    printf(\"    return $output\\n\");\n    printf(\"}\\n\");\n}\n\n/* Print a network representation */\nvoid AnnPrint(struct Ann *net) {\n    int i, j, k;\n\n    for (i = 0; i < LAYERS(net); i++) {\n        char *layertype = \"Hidden\";\n        if (i == 0) layertype = \"Output\";\n        if (i == LAYERS(net)-1) layertype = \"Input\";\n        printf(\"%s layer %d, units %d\\n\", layertype, i, UNITS(net,i));\n        if (i) {\n            /* Don't compute the bias unit as a target. */\n            int targets = UNITS(net,i-1) - (i-1>0);\n            /* Weights */\n            printf(\"\\tW\");\n            for (j = 0; j < UNITS(net, i); j++) {\n                printf(\"(\");\n                for (k = 0; k < targets; k++) {\n                    printf(\"%f\", WEIGHT(net,i,j,k));\n                    if (k != targets-1) printf(\" \");\n                }\n                printf(\") \");\n            }\n            printf(\"\\n\");\n            /* Gradients */\n            printf(\"\\tg\");\n            for (j = 0; j < UNITS(net, i); j++) {\n                printf(\"[\");\n                for (k = 0; k < targets; k++) {\n                    printf(\"%f\", GRADIENT(net,i,j,k));\n                    if (k != targets-1) printf(\" \");\n                }\n                printf(\"] \");\n            }\n            printf(\"\\n\");\n            /* SGradients */\n            printf(\"\\tG\");\n            for (j = 0; j < UNITS(net, i); j++) {\n                printf(\"[\");\n                for (k = 0; k < targets; k++) {\n                    printf(\"%f\", SGRADIENT(net,i,j,k));\n                    if (k != targets-1) printf(\" \");\n                }\n                printf(\"] \");\n            }\n            printf(\"\\n\");\n            /* Gradients at t-1 */\n            printf(\"\\tP\");\n            for (j = 0; j < UNITS(net, i); j++) {\n                printf(\"[\");\n                for (k = 0; k < targets; k++) {\n                    printf(\"%f\", PGRADIENT(net,i,j,k));\n                    if (k != targets-1) printf(\" \");\n                }\n                printf(\"] \");\n            }\n            printf(\"\\n\");\n            /* Delta */\n            printf(\"\\tD\");\n            for (j = 0; j < UNITS(net, i); j++) {\n                printf(\"|\");\n                for (k = 0; k < targets; k++) {\n                    printf(\"%f\", DELTA(net,i,j,k));\n                    if (k != targets-1) printf(\" \");\n                }\n                printf(\"| \");\n            }\n            printf(\"\\n\");\n        }\n        for (j = 0; j < UNITS(net,i); j++) {\n            printf(\"\\tO: %f \", OUTPUT(net,i,j));\n        }\n        printf(\"\\n\");\n        printf(\"\\tE /\");\n        for (j = 0; j < UNITS(net,i); j++) {\n            printf(\"%f \", ERROR(net,i,j));\n        }\n        printf(\"/\\n\");\n    }\n}\n\n/* Calcuate the global error of the net. This is just the\n * Root Mean Square (RMS) error, which is half the sum of the squared\n * errors. */\nfloat AnnGlobalError(struct Ann *net, float *desired) {\n    float e, t;\n    int i, outputs = OUTPUT_UNITS(net);\n\n    e = 0;\n    for (i = 0; i < outputs; i++) {\n        t = desired[i] - OUTPUT_NODE(net,i);\n        e += t*t; /* No need for fabs(t), t*t will always be positive. */\n    }\n    return .5*e;\n}\n\n/* Set the network input */\nvoid AnnSetInput(struct Ann *net, float *input)\n{\n    int i, inputs = INPUT_UNITS(net);\n\n    for (i = 0; i < inputs; i++) INPUT_NODE(net,i) = input[i];\n}\n\n/* Simulate the net, and return the global error */\nfloat AnnSimulateError(struct Ann *net, float *input, float *desired) {\n    AnnSetInput(net, input);\n    AnnSimulate(net);\n    return AnnGlobalError(net, desired);\n}\n\n/* Compute the error vector y-t in the output unit. This error depends\n * on the loss function we use. */\nvoid AnnCalculateOutputError(struct Ann *net, float *desired) {\n    int units = OUTPUT_UNITS(net);\n    float factor = (float)2/units;\n    for (int j = 0; j < units; j++) {\n        net->layer[0].error[j] =\n            factor * (net->layer[0].output[j] - desired[j]);\n    }\n}\n\n/* Calculate gradients with a trivial and slow algorithm, this\n * is useful to check that the real implementation is working\n * well, comparing the results.\n *\n * The algorithm used is: to compute the error function in two\n * points (E1, with the real weight, and E2 with the weight W = W + 0.1),\n * than the approximation of the gradient is G = (E2-E1)/0.1. */\n#define GTRIVIAL_DELTA 0.001\nvoid AnnCalculateGradientsTrivial(struct Ann *net, float *desired) {\n    int j, i, layers = LAYERS(net);\n\n    for (j = 1; j < layers; j++) {\n        int units = UNITS(net, j);\n        int weights = units * UNITS(net,j-1);\n        for (i = 0; i < weights; i++) {\n            float t, e1, e2;\n\n            /* Calculate the value of the error function\n             * in this point. */\n            AnnSimulate(net);\n            e1 = AnnGlobalError(net, desired);\n            t = net->layer[j].weight[i];\n            /* Calculate the error a bit on the right */\n            net->layer[j].weight[i] += GTRIVIAL_DELTA;\n            AnnSimulate(net);\n            e2 = AnnGlobalError(net, desired);\n            /* Restore the original weight */\n            net->layer[j].weight[i] = t;\n            /* Calculate the gradient */\n            net->layer[j].gradient[i] = (e2-e1)/GTRIVIAL_DELTA;\n        }\n    }\n}\n\n/* Calculate gradients using the back propagation algorithm */\nvoid AnnCalculateGradients(struct Ann *net, float *desired) {\n    int j, layers = LAYERS(net)-1;\n\n    /* Populate the error vector net->layer[0]->error according\n     * to the loss function. */\n    AnnCalculateOutputError(net,desired);\n\n    /* Back-propagate the error and compute the gradient\n     * for every weight in the net. */\n    for (j = 0; j < layers; j++) {\n        struct AnnLayer *layer = &net->layer[j];\n        struct AnnLayer *prev_layer = &net->layer[j+1];\n        int i, units = layer->units;\n        int prevunits = prev_layer->units;\n\n        /* Skip bias units, they have no connections with the previous\n         * layers. */\n        if (j > 1) units--;\n        /* Reset the next layer errors array */\n        for (i = 0; i < prevunits; i++) prev_layer->error[i] = 0;\n        /* For every node in this layer ... */\n        for (i = 0; i < units; i++) {\n            float error_signal, ei, oi, derivative;\n            int k;\n\n            /* Compute gradient. */\n            ei = layer->error[i];\n            oi = layer->output[i];\n\n            /* Common derivatives:\n             *\n             * identity: 1\n             * sigmoid: oi*(1-oi)\n             * softmax: oi*(1-oi)\n             * tanh:    (1-oi)*(1+oi), that's 1-(oi*oi)\n             * relu:    (oi > 0) ? 1 : 0\n             */\n            derivative = oi*(1-oi);\n            error_signal = ei*derivative;\n\n            /* For every weight between this node and\n             * the previous layer's nodes: */\n            float *g = prev_layer->gradient + i*prevunits;\n            float *w = prev_layer->weight + i*prevunits;\n            float *o = prev_layer->output;\n            float *e = prev_layer->error;\n\n            /* 1. Calculate the gradient */\n            k = 0;\n\n#ifdef USING_SIMD\n            simdf_t es = simdf_set1f(error_signal);\n\n            int psteps = prevunits/SIMDF_SIZE;\n            for (int x = 0; x < psteps; x++) {\n                simdf_t outputs = simdf_loadu(o);\n                //simdf_t gradients = simdf_mul(es,outputs);\n                simdf_storeu(g,simdf_mul(es,outputs));\n                o += SIMDF_SIZE;\n                g += SIMDF_SIZE;\n            }\n            k += SIMDF_SIZE*psteps;\n#endif\n            /* Handle final piece shorter than SIMDF_SIZE . */\n            for (; k < prevunits; k++) *g++ = error_signal*(*o++);\n\n            /* 2. And back-propagate the error to the previous layer */\n            k = 0;\n#ifdef USING_SIMD\n            for (int x = 0; x < psteps; x++) {\n                simdf_t weights = simdf_loadu(w);\n                simdf_t errors = simdf_loadu(e);\n                //simdf_t prod = simdf_mul(es, weights);\n                simdf_storeu(e, simdf_add( simdf_mul(es, weights), errors));\n                e += SIMDF_SIZE;\n                w += SIMDF_SIZE;\n            }\n            k += SIMDF_SIZE*psteps;\n#endif\n            /* Handle final piece shorter than SIMDF_SIZE . */\n            for (; k < prevunits; k++) {\n                (*e++) += error_signal * (*w++);\n            }\n        }\n    }\n}\n\n/* Set the delta values of the net to a given value */\nvoid AnnSetDeltas(struct Ann *net, float val) {\n    int j, layers = LAYERS(net);\n\n    for (j = 1; j < layers; j++) {\n        int units = UNITS(net, j);\n        int weights = units * UNITS(net,j-1);\n        int i;\n\n        for (i = 0; i < weights; i++) net->layer[j].delta[i] = val;\n    }\n}\n\n/* Set the sgradient values to zero */\nvoid AnnResetSgradient(struct Ann *net) {\n    int j, layers = LAYERS(net);\n\n    for (j = 1; j < layers; j++) {\n        int units = UNITS(net, j);\n        int weights = units * UNITS(net,j-1);\n        memset(net->layer[j].sgradient, 0, sizeof(float)*weights);\n    }\n}\n\n/* Set random weights in the range -0.05,+0.05 */\nvoid AnnSetRandomWeights(struct Ann *net) {\n    int i, j, k;\n\n    for (i = 1; i < LAYERS(net); i++) {\n        for (k = 0; k < UNITS(net, i-1); k++) {\n            for (j = 0; j < UNITS(net, i); j++) {\n                WEIGHT(net,i,j,k) = -0.05+.1*(rand()/(RAND_MAX+1.0));\n            }\n        }\n    }\n}\n\n/* Scale the net weights of the given factor */\nvoid AnnScaleWeights(struct Ann *net, float factor) {\n    int j, layers = LAYERS(net);\n\n    for (j = 1; j < layers; j++) {\n        int units = UNITS(net, j);\n        int weights = units * UNITS(net,j-1);\n        int i;\n\n        for (i = 0; i < weights; i++)\n            net->layer[j].weight[i] *= factor;\n    }\n}\n\n/* Update the sgradient, that's the sum of the weight's gradient for every\n * element of the training set. This is used for the RPROP algorithm\n * that works with the sign of the derivative for the whole set. */\nvoid AnnUpdateSgradient(struct Ann *net) {\n    int j, i, layers = LAYERS(net);\n\n    for (j = 1; j < layers; j++) {\n        int units = UNITS(net, j);\n        int weights = units * UNITS(net,j-1);\n        /* In theory this is a good target for SSE \"ADDPS\" instructions,\n         * however modern compilers figure out this automatically. */\n        for (i = 0; i < weights; i++)\n            net->layer[j].sgradient[i] += net->layer[j].gradient[i];\n    }\n}\n\n/* Helper function for RPROP, returns -1 if n < 0, +1 if n > 0, 0 if n == 0 */\nfloat sign(float n) {\n    if (n > 0) return +1;\n    if (n < 0) return -1;\n    return 0;\n}\n\n/* The core of the RPROP algorithm.\n *\n * Note that:\n * sgradient is the set-wise gradient.\n * delta is the per-weight update value. */\nvoid AnnAdjustWeightsResilientBP(struct Ann *net) {\n    int j, i, layers = LAYERS(net);\n\n    for (j = 1; j < layers; j++) {\n        int units = UNITS(net, j);\n        int weights = units * UNITS(net,j-1) - (j-1>0);\n        for (i = 0; i < weights; i++) {\n            float t = net->layer[j].pgradient[i] *\n                       net->layer[j].sgradient[i];\n            float delta = net->layer[j].delta[i];\n\n            if (t > 0) {\n                delta = MIN(delta*RPROP_NPLUS(net),RPROP_MAXUPDATE(net));\n                float wdelta = -sign(net->layer[j].sgradient[i]) * delta;\n                net->layer[j].weight[i] += wdelta;\n                net->layer[j].delta[i] = delta;\n                net->layer[j].pgradient[i] = net->layer[j].sgradient[i];\n            } else if (t < 0) {\n                float past_wdelta = -sign(net->layer[j].pgradient[i]) * delta;\n                delta = MAX(delta*RPROP_NMINUS(net),RPROP_MINUPDATE(net));\n                net->layer[j].weight[i] -= past_wdelta;\n                net->layer[j].delta[i] = delta;\n                net->layer[j].pgradient[i] = 0;\n            } else { /* t == 0 */\n                float wdelta = -sign(net->layer[j].sgradient[i]) * delta;\n                net->layer[j].weight[i] += wdelta;\n                net->layer[j].pgradient[i] = net->layer[j].sgradient[i];\n            }\n        }\n    }\n}\n\n/* Resilient Backpropagation Epoch */\nfloat AnnResilientBPEpoch(struct Ann *net, float *input, float *desired, int setlen) {\n    float error = 0;\n    int j, inputs = INPUT_UNITS(net), outputs = OUTPUT_UNITS(net);\n\n    AnnResetSgradient(net);\n    for (j = 0; j < setlen; j++) {\n        error += AnnSimulateError(net, input, desired);\n        AnnCalculateGradients(net, desired);\n        AnnUpdateSgradient(net);\n        input += inputs;\n        desired += outputs;\n    }\n    AnnAdjustWeightsResilientBP(net);\n    return error / setlen;\n}\n\n/* Update the deltas using the gradient descend algorithm.\n * Gradients should be already computed with AnnCalculateGraidents(). */\nvoid AnnUpdateDeltasGD(struct Ann *net) {\n    int j, i, layers = LAYERS(net);\n\n    for (j = 1; j < layers; j++) {\n        int units = UNITS(net, j);\n        int weights = units * UNITS(net,j-1);\n        for (i = 0; i < weights; i++)\n            net->layer[j].delta[i] += net->layer[j].gradient[i];\n    }\n}\n\n/* Adjust net weights using the (already) calculated deltas. */\nvoid AnnAdjustWeights(struct Ann *net, int setlen) {\n    int j, i, layers = LAYERS(net);\n\n    for (j = 1; j < layers; j++) {\n        int units = UNITS(net, j);\n        int weights = units * UNITS(net,j-1);\n        for (i = 0; i < weights; i++) {\n            net->layer[j].weight[i] -= LEARN_RATE(net)/setlen*net->layer[j].delta[i];\n        }\n    }\n}\n\n/* Gradient Descend training */\nfloat AnnGDEpoch(struct Ann *net, float *input, float *desidered, int setlen) {\n    float error = 0;\n    int j, inputs = INPUT_UNITS(net), outputs = OUTPUT_UNITS(net);\n\n    for (j = 0; j < setlen; j++) {\n        AnnSetDeltas(net, 0);\n        error += AnnSimulateError(net, input, desidered);\n        AnnCalculateGradients(net, desidered);\n        AnnUpdateDeltasGD(net);\n        input += inputs;\n        desidered += outputs;\n        AnnAdjustWeights(net,setlen);\n    }\n    return error / setlen;\n}\n\n/* This function, called after AnnSimulate(), will return 1 if there is\n * an error in the detected class (compared to the desired output),\n * othewise 0 is returned. */\nint AnnTestClassError(struct Ann *net, float *desired) {\n    int i, outputs = OUTPUT_UNITS(net);\n    int classid, outid;\n    float max = 0;\n\n    /* Get the class ID from the test dataset output. */\n    classid = 0;\n    for (i = 0; i < outputs; i++)\n        if (desired[i] == 1) break;\n    classid = i;\n\n    /* Get the network classification. */\n    max = OUTPUT_NODE(net,0);\n    outid = 0;\n    for (i = 1; i < outputs; i++) {\n        float o = OUTPUT_NODE(net,i);\n        if (o > max) {\n            outid = i;\n            max = o;\n        }\n    }\n    return outid != classid;\n}\n\n/* Simulate the entire test dataset with the neural network and returns the\n * average error of all the entries tested. */\nvoid AnnTestError(struct Ann *net, float *input, float *desired, int setlen, float *avgerr, float *classerr) {\n    float error = 0;\n    int j, inputs = INPUT_UNITS(net), outputs = OUTPUT_UNITS(net);\n    int class_errors = 0;\n\n    for (j = 0; j < setlen; j++) {\n        error += AnnSimulateError(net, input, desired);\n        if (classerr)\n            class_errors += AnnTestClassError(net, desired);\n        input += inputs;\n        desired += outputs;\n    }\n    if (avgerr) *avgerr = error/setlen;\n    if (classerr) *classerr = (float)class_errors*100/setlen;\n}\n\n/* Train the net */\nfloat AnnTrainWithAlgoFunc(struct Ann *net, float *input, float *desired, float maxerr,\n\t\t\t\t\tint maxepochs, int setlen, AnnTrainAlgoFunc algo_func) {\n    int i = 0;\n    float e = maxerr+1;\n\n    while (i++ < maxepochs && e >= maxerr) {\n\te = (*algo_func)(net, input, desired, setlen);\n    }\n    return e;\n}\n\t\n\nfloat AnnTrain(struct Ann *net, float *input, float *desired, float maxerr, int maxepochs,\n\t\t\t\t\t\t\t\t\t\tint setlen, int algo) {\n    AnnTrainAlgoFunc algo_func;\n    if(algo == NN_ALGO_BPROP) algo_func = AnnResilientBPEpoch;\n    else if(algo == NN_ALGO_GD) algo_func = AnnGDEpoch;\n    else return -1;\n\n    return AnnTrainWithAlgoFunc(net, input, desired, maxerr, maxepochs, setlen, algo_func);\n}\n"
        },
        {
          "name": "nn.h",
          "type": "blob",
          "size": 6.34765625,
          "content": "/* RPROP Neural Networks implementation\n * See: http://deeplearning.cs.cmu.edu/pdfs/Rprop.pdf\n *\n * Copyright (c) 2003-2016, Salvatore Sanfilippo <antirez at gmail dot com>\n * All rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n *\n *   * Redistributions of source code must retain the above copyright notice,\n *     this list of conditions and the following disclaimer.\n *   * Redistributions in binary form must reproduce the above copyright\n *     notice, this list of conditions and the following disclaimer in the\n *     documentation and/or other materials provided with the distribution.\n *   * Neither the name of Disque nor the names of its contributors may be used\n *     to endorse or promote products derived from this software without\n *     specific prior written permission.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE\n * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n * POSSIBILITY OF SUCH DAMAGE.\n */\n\n#ifndef __NN_H\n#define __NN_H\n\n/* Data structures.\n * Nets are not so 'dynamic', but enough to support\n * an arbitrary number of layers, with arbitrary units for layer.\n * Only fully connected feed-forward networks are supported. */\nstruct AnnLayer {\n\tfloat *output;\t\t/* output[i], output of i-th unit */\n\tfloat *error;\t\t/* error[i], output error of i-th unit*/\n\tfloat *weight;\t\t/* weight[(i*units)+j] */\n\t\t\t\t/* weight between unit i-th and next j-th */\n\tfloat *gradient;\t/* gradient[(i*units)+j] gradient */\n\tfloat *sgradient;\t/* gradient for the full training set */\n\t\t\t\t/* only used for RPROP */\n\tfloat *pgradient;\t/* pastgradient[(i*units)+j] t-1 gradient */\n\t\t\t\t/* (t-1 sgradient for resilient BP) */\n\tfloat *delta;\t\t/* delta[(i*units)+j] cumulative update */\n\t\t\t\t/* (per-weight delta for RPROP) */\n\tint units;\t/*moved to last position for alignment purposes*/\n};\n\n/* Feed forward network structure */\nstruct Ann {\n\tint flags;\n\tint layers;\n\tfloat rprop_nminus;\n\tfloat rprop_nplus;\n\tfloat rprop_maxupdate;\n\tfloat rprop_minupdate;\n        float learn_rate; /* Used for GD training. */\n\tfloat _filler_; /*filler for alignment*/\n\tstruct AnnLayer *layer;\n};\n\ntypedef float (*AnnTrainAlgoFunc)(struct Ann *net, float *input, float *desired, int setlen);\n\n/* Raw interface to data structures */\n#define OUTPUT(net,l,i) (net)->layer[l].output[i]\n#define ERROR(net,l,i) (net)->layer[l].error[i]\n#define WEIGHT(net,l,i,j) (net)->layer[l].weight[((j)*(net)->layer[l].units)+(i)]\n#define GRADIENT(net,l,i,j) (net)->layer[l].gradient[((j)*(net)->layer[l].units)+(i)]\n#define SGRADIENT(net,l,i,j) (net)->layer[l].sgradient[((j)*(net)->layer[l].units)+(i)]\n#define PGRADIENT(net,l,i,j) (net)->layer[l].pgradient[((j)*(net)->layer[l].units)+(i)]\n#define DELTA(net,l,i,j) (net)->layer[l].delta[((j)*(net)->layer[l].units)+(i)]\n#define LAYERS(net) (net)->layers\n#define UNITS(net,l) (net)->layer[l].units\n#define WEIGHTS(net,l) (UNITS(net,l)*UNITS(net,l-1))\n#define OUTPUT_NODE(net,i) OUTPUT(net,0,i)\n#define INPUT_NODE(net,i) OUTPUT(net,((net)->layers)-1,i)\n#define OUTPUT_UNITS(net) UNITS(net,0)\n#define INPUT_UNITS(net) (UNITS(net,((net)->layers)-1)-1)\n#define RPROP_NMINUS(net) (net)->rprop_nminus\n#define RPROP_NPLUS(net) (net)->rprop_nplus\n#define RPROP_MAXUPDATE(net) (net)->rprop_maxupdate\n#define RPROP_MINUPDATE(net) (net)->rprop_minupdate\n#define LEARN_RATE(net) (net)->learn_rate\n\n/* Constants */\n#define DEFAULT_RPROP_NMINUS 0.5\n#define DEFAULT_RPROP_NPLUS 1.2\n#define DEFAULT_RPROP_MAXUPDATE 50\n#define DEFAULT_RPROP_MINUPDATE 0.000001\n#define RPROP_INITIAL_DELTA 0.1\n#define DEFAULT_LEARN_RATE 0.1\n#define NN_ALGO_BPROP 0\n#define NN_ALGO_GD 1\n\n/* Misc */\n#define MAX(a,b) (((a)>(b))?(a):(b))\n#define MIN(a,b) (((a)<(b))?(a):(b))\n\n/* Prototypes */\nvoid AnnResetLayer(struct AnnLayer *layer);\nstruct Ann *AnnAlloc(int layers);\nvoid AnnFreeLayer(struct AnnLayer *layer);\nvoid AnnFree(struct Ann *net);\nint AnnInitLayer(struct Ann *net, int i, int units, int bias);\nstruct Ann *AnnCreateNet(int layers, int *units);\nstruct Ann *AnnCreateNet2(int iunits, int ounits);\nstruct Ann *AnnCreateNet3(int iunits, int hunits, int ounits);\nstruct Ann *AnnCreateNet4(int iunits, int hunits, int hunits2, int ounits);\nstruct Ann *AnnClone(struct Ann* net);\nsize_t AnnCountWeights(struct Ann *net);\nvoid AnnSimulate(struct Ann *net);\nvoid Ann2Tcl(struct Ann *net);\nvoid AnnPrint(struct Ann *net);\nfloat AnnGlobalError(struct Ann *net, float *desidered);\nvoid AnnSetInput(struct Ann *net, float *input);\nfloat AnnSimulateError(struct Ann *net, float *input, float *desidered);\nvoid AnnCalculateGradientsTrivial(struct Ann *net, float *desidered);\nvoid AnnCalculateGradients(struct Ann *net, float *desidered);\nvoid AnnSetDeltas(struct Ann *net, float val);\nvoid AnnResetDeltas(struct Ann *net);\nvoid AnnResetSgradient(struct Ann *net);\nvoid AnnSetRandomWeights(struct Ann *net);\nvoid AnnScaleWeights(struct Ann *net, float factor);\nvoid AnnUpdateDeltasGD(struct Ann *net);\nvoid AnnUpdateDeltasGDM(struct Ann *net);\nvoid AnnUpdateSgradient(struct Ann *net);\nvoid AnnAdjustWeights(struct Ann *net, int setlen);\nfloat AnnBatchGDEpoch(struct Ann *net, float *input, float *desidered, int setlen);\nfloat AnnBatchGDMEpoch(struct Ann *net, float *input, float *desidered, int setlen);\nvoid AnnAdjustWeightsResilientBP(struct Ann *net);\nfloat AnnResilientBPEpoch(struct Ann *net, float *input, float *desidered, int setlen);\nfloat AnnTrainWithAlgoFunc(struct Ann *net, float *input, float *desidered, float maxerr, int maxepochs, int setlen, AnnTrainAlgoFunc algo_func);\nfloat AnnTrain(struct Ann *net, float *input, float *desidered, float maxerr, int maxepochs, int setlen, int algo);\nvoid AnnTestError(struct Ann *net, float *input, float *desired, int setlen, float *avgerr, float *classerr);\n\n#endif /* __NN_H */\n"
        },
        {
          "name": "redismodule.h",
          "type": "blob",
          "size": 17.7978515625,
          "content": "#ifndef REDISMODULE_H\n#define REDISMODULE_H\n\n#include <sys/types.h>\n#include <stdint.h>\n#include <stdio.h>\n\n/* ---------------- Defines common between core and modules --------------- */\n\n/* Error status return values. */\n#define REDISMODULE_OK 0\n#define REDISMODULE_ERR 1\n\n/* API versions. */\n#define REDISMODULE_APIVER_1 1\n\n/* API flags and constants */\n#define REDISMODULE_READ (1<<0)\n#define REDISMODULE_WRITE (1<<1)\n\n#define REDISMODULE_LIST_HEAD 0\n#define REDISMODULE_LIST_TAIL 1\n\n/* Key types. */\n#define REDISMODULE_KEYTYPE_EMPTY 0\n#define REDISMODULE_KEYTYPE_STRING 1\n#define REDISMODULE_KEYTYPE_LIST 2\n#define REDISMODULE_KEYTYPE_HASH 3\n#define REDISMODULE_KEYTYPE_SET 4\n#define REDISMODULE_KEYTYPE_ZSET 5\n#define REDISMODULE_KEYTYPE_MODULE 6\n\n/* Reply types. */\n#define REDISMODULE_REPLY_UNKNOWN -1\n#define REDISMODULE_REPLY_STRING 0\n#define REDISMODULE_REPLY_ERROR 1\n#define REDISMODULE_REPLY_INTEGER 2\n#define REDISMODULE_REPLY_ARRAY 3\n#define REDISMODULE_REPLY_NULL 4\n\n/* Postponed array length. */\n#define REDISMODULE_POSTPONED_ARRAY_LEN -1\n\n/* Expire */\n#define REDISMODULE_NO_EXPIRE -1\n\n/* Sorted set API flags. */\n#define REDISMODULE_ZADD_XX      (1<<0)\n#define REDISMODULE_ZADD_NX      (1<<1)\n#define REDISMODULE_ZADD_ADDED   (1<<2)\n#define REDISMODULE_ZADD_UPDATED (1<<3)\n#define REDISMODULE_ZADD_NOP     (1<<4)\n\n/* Hash API flags. */\n#define REDISMODULE_HASH_NONE       0\n#define REDISMODULE_HASH_NX         (1<<0)\n#define REDISMODULE_HASH_XX         (1<<1)\n#define REDISMODULE_HASH_CFIELDS    (1<<2)\n#define REDISMODULE_HASH_EXISTS     (1<<3)\n\n/* A special pointer that we can use between the core and the module to signal\n * field deletion, and that is impossible to be a valid pointer. */\n#define REDISMODULE_HASH_DELETE ((RedisModuleString*)(long)1)\n\n/* Error messages. */\n#define REDISMODULE_ERRORMSG_WRONGTYPE \"WRONGTYPE Operation against a key holding the wrong kind of value\"\n\n#define REDISMODULE_POSITIVE_INFINITE (1.0/0.0)\n#define REDISMODULE_NEGATIVE_INFINITE (-1.0/0.0)\n\n#define REDISMODULE_NOT_USED(V) ((void) V)\n\n/* ------------------------- End of common defines ------------------------ */\n\n#ifndef REDISMODULE_CORE\n\ntypedef long long mstime_t;\n\n/* Incomplete structures for compiler checks but opaque access. */\ntypedef struct RedisModuleCtx RedisModuleCtx;\ntypedef struct RedisModuleKey RedisModuleKey;\ntypedef struct RedisModuleString RedisModuleString;\ntypedef struct RedisModuleCallReply RedisModuleCallReply;\ntypedef struct RedisModuleIO RedisModuleIO;\ntypedef struct RedisModuleType RedisModuleType;\ntypedef struct RedisModuleDigest RedisModuleDigest;\ntypedef struct RedisModuleBlockedClient RedisModuleBlockedClient;\n\ntypedef int (*RedisModuleCmdFunc) (RedisModuleCtx *ctx, RedisModuleString **argv, int argc);\n\ntypedef void *(*RedisModuleTypeLoadFunc)(RedisModuleIO *rdb, int encver);\ntypedef void (*RedisModuleTypeSaveFunc)(RedisModuleIO *rdb, void *value);\ntypedef void (*RedisModuleTypeRewriteFunc)(RedisModuleIO *aof, RedisModuleString *key, void *value);\ntypedef size_t (*RedisModuleTypeMemUsageFunc)(void *value);\ntypedef void (*RedisModuleTypeDigestFunc)(RedisModuleDigest *digest, void *value);\ntypedef void (*RedisModuleTypeFreeFunc)(void *value);\n\n#define REDISMODULE_TYPE_METHOD_VERSION 1\ntypedef struct RedisModuleTypeMethods {\n    uint64_t version;\n    RedisModuleTypeLoadFunc rdb_load;\n    RedisModuleTypeSaveFunc rdb_save;\n    RedisModuleTypeRewriteFunc aof_rewrite;\n    RedisModuleTypeMemUsageFunc mem_usage;\n    RedisModuleTypeDigestFunc digest;\n    RedisModuleTypeFreeFunc free;\n} RedisModuleTypeMethods;\n\n#define REDISMODULE_GET_API(name) \\\n    RedisModule_GetApi(\"RedisModule_\" #name, ((void **)&RedisModule_ ## name))\n\n#define REDISMODULE_API_FUNC(x) (*x)\n\n\nvoid *REDISMODULE_API_FUNC(RedisModule_Alloc)(size_t bytes);\nvoid *REDISMODULE_API_FUNC(RedisModule_Realloc)(void *ptr, size_t bytes);\nvoid REDISMODULE_API_FUNC(RedisModule_Free)(void *ptr);\nvoid *REDISMODULE_API_FUNC(RedisModule_Calloc)(size_t nmemb, size_t size);\nchar *REDISMODULE_API_FUNC(RedisModule_Strdup)(const char *str);\nint REDISMODULE_API_FUNC(RedisModule_GetApi)(const char *, void *);\nint REDISMODULE_API_FUNC(RedisModule_CreateCommand)(RedisModuleCtx *ctx, const char *name, RedisModuleCmdFunc cmdfunc, const char *strflags, int firstkey, int lastkey, int keystep);\nint REDISMODULE_API_FUNC(RedisModule_SetModuleAttribs)(RedisModuleCtx *ctx, const char *name, int ver, int apiver);\nint REDISMODULE_API_FUNC(RedisModule_WrongArity)(RedisModuleCtx *ctx);\nint REDISMODULE_API_FUNC(RedisModule_ReplyWithLongLong)(RedisModuleCtx *ctx, long long ll);\nint REDISMODULE_API_FUNC(RedisModule_GetSelectedDb)(RedisModuleCtx *ctx);\nint REDISMODULE_API_FUNC(RedisModule_SelectDb)(RedisModuleCtx *ctx, int newid);\nvoid *REDISMODULE_API_FUNC(RedisModule_OpenKey)(RedisModuleCtx *ctx, RedisModuleString *keyname, int mode);\nvoid REDISMODULE_API_FUNC(RedisModule_CloseKey)(RedisModuleKey *kp);\nint REDISMODULE_API_FUNC(RedisModule_KeyType)(RedisModuleKey *kp);\nsize_t REDISMODULE_API_FUNC(RedisModule_ValueLength)(RedisModuleKey *kp);\nint REDISMODULE_API_FUNC(RedisModule_ListPush)(RedisModuleKey *kp, int where, RedisModuleString *ele);\nRedisModuleString *REDISMODULE_API_FUNC(RedisModule_ListPop)(RedisModuleKey *key, int where);\nRedisModuleCallReply *REDISMODULE_API_FUNC(RedisModule_Call)(RedisModuleCtx *ctx, const char *cmdname, const char *fmt, ...);\nconst char *REDISMODULE_API_FUNC(RedisModule_CallReplyProto)(RedisModuleCallReply *reply, size_t *len);\nvoid REDISMODULE_API_FUNC(RedisModule_FreeCallReply)(RedisModuleCallReply *reply);\nint REDISMODULE_API_FUNC(RedisModule_CallReplyType)(RedisModuleCallReply *reply);\nlong long REDISMODULE_API_FUNC(RedisModule_CallReplyInteger)(RedisModuleCallReply *reply);\nsize_t REDISMODULE_API_FUNC(RedisModule_CallReplyLength)(RedisModuleCallReply *reply);\nRedisModuleCallReply *REDISMODULE_API_FUNC(RedisModule_CallReplyArrayElement)(RedisModuleCallReply *reply, size_t idx);\nRedisModuleString *REDISMODULE_API_FUNC(RedisModule_CreateString)(RedisModuleCtx *ctx, const char *ptr, size_t len);\nRedisModuleString *REDISMODULE_API_FUNC(RedisModule_CreateStringFromLongLong)(RedisModuleCtx *ctx, long long ll);\nRedisModuleString *REDISMODULE_API_FUNC(RedisModule_CreateStringFromString)(RedisModuleCtx *ctx, const RedisModuleString *str);\nRedisModuleString *REDISMODULE_API_FUNC(RedisModule_CreateStringPrintf)(RedisModuleCtx *ctx, const char *fmt, ...);\nvoid REDISMODULE_API_FUNC(RedisModule_FreeString)(RedisModuleCtx *ctx, RedisModuleString *str);\nconst char *REDISMODULE_API_FUNC(RedisModule_StringPtrLen)(const RedisModuleString *str, size_t *len);\nint REDISMODULE_API_FUNC(RedisModule_ReplyWithError)(RedisModuleCtx *ctx, const char *err);\nint REDISMODULE_API_FUNC(RedisModule_ReplyWithSimpleString)(RedisModuleCtx *ctx, const char *msg);\nint REDISMODULE_API_FUNC(RedisModule_ReplyWithArray)(RedisModuleCtx *ctx, long len);\nvoid REDISMODULE_API_FUNC(RedisModule_ReplySetArrayLength)(RedisModuleCtx *ctx, long len);\nint REDISMODULE_API_FUNC(RedisModule_ReplyWithStringBuffer)(RedisModuleCtx *ctx, const char *buf, size_t len);\nint REDISMODULE_API_FUNC(RedisModule_ReplyWithString)(RedisModuleCtx *ctx, RedisModuleString *str);\nint REDISMODULE_API_FUNC(RedisModule_ReplyWithNull)(RedisModuleCtx *ctx);\nint REDISMODULE_API_FUNC(RedisModule_ReplyWithDouble)(RedisModuleCtx *ctx, double d);\nint REDISMODULE_API_FUNC(RedisModule_ReplyWithCallReply)(RedisModuleCtx *ctx, RedisModuleCallReply *reply);\nint REDISMODULE_API_FUNC(RedisModule_StringToLongLong)(const RedisModuleString *str, long long *ll);\nint REDISMODULE_API_FUNC(RedisModule_StringToDouble)(const RedisModuleString *str, double *d);\nvoid REDISMODULE_API_FUNC(RedisModule_AutoMemory)(RedisModuleCtx *ctx);\nint REDISMODULE_API_FUNC(RedisModule_Replicate)(RedisModuleCtx *ctx, const char *cmdname, const char *fmt, ...);\nint REDISMODULE_API_FUNC(RedisModule_ReplicateVerbatim)(RedisModuleCtx *ctx);\nconst char *REDISMODULE_API_FUNC(RedisModule_CallReplyStringPtr)(RedisModuleCallReply *reply, size_t *len);\nRedisModuleString *REDISMODULE_API_FUNC(RedisModule_CreateStringFromCallReply)(RedisModuleCallReply *reply);\nint REDISMODULE_API_FUNC(RedisModule_DeleteKey)(RedisModuleKey *key);\nint REDISMODULE_API_FUNC(RedisModule_StringSet)(RedisModuleKey *key, RedisModuleString *str);\nchar *REDISMODULE_API_FUNC(RedisModule_StringDMA)(RedisModuleKey *key, size_t *len, int mode);\nint REDISMODULE_API_FUNC(RedisModule_StringTruncate)(RedisModuleKey *key, size_t newlen);\nmstime_t REDISMODULE_API_FUNC(RedisModule_GetExpire)(RedisModuleKey *key);\nint REDISMODULE_API_FUNC(RedisModule_SetExpire)(RedisModuleKey *key, mstime_t expire);\nint REDISMODULE_API_FUNC(RedisModule_ZsetAdd)(RedisModuleKey *key, double score, RedisModuleString *ele, int *flagsptr);\nint REDISMODULE_API_FUNC(RedisModule_ZsetIncrby)(RedisModuleKey *key, double score, RedisModuleString *ele, int *flagsptr, double *newscore);\nint REDISMODULE_API_FUNC(RedisModule_ZsetScore)(RedisModuleKey *key, RedisModuleString *ele, double *score);\nint REDISMODULE_API_FUNC(RedisModule_ZsetRem)(RedisModuleKey *key, RedisModuleString *ele, int *deleted);\nvoid REDISMODULE_API_FUNC(RedisModule_ZsetRangeStop)(RedisModuleKey *key);\nint REDISMODULE_API_FUNC(RedisModule_ZsetFirstInScoreRange)(RedisModuleKey *key, double min, double max, int minex, int maxex);\nint REDISMODULE_API_FUNC(RedisModule_ZsetLastInScoreRange)(RedisModuleKey *key, double min, double max, int minex, int maxex);\nint REDISMODULE_API_FUNC(RedisModule_ZsetFirstInLexRange)(RedisModuleKey *key, RedisModuleString *min, RedisModuleString *max);\nint REDISMODULE_API_FUNC(RedisModule_ZsetLastInLexRange)(RedisModuleKey *key, RedisModuleString *min, RedisModuleString *max);\nRedisModuleString *REDISMODULE_API_FUNC(RedisModule_ZsetRangeCurrentElement)(RedisModuleKey *key, double *score);\nint REDISMODULE_API_FUNC(RedisModule_ZsetRangeNext)(RedisModuleKey *key);\nint REDISMODULE_API_FUNC(RedisModule_ZsetRangePrev)(RedisModuleKey *key);\nint REDISMODULE_API_FUNC(RedisModule_ZsetRangeEndReached)(RedisModuleKey *key);\nint REDISMODULE_API_FUNC(RedisModule_HashSet)(RedisModuleKey *key, int flags, ...);\nint REDISMODULE_API_FUNC(RedisModule_HashGet)(RedisModuleKey *key, int flags, ...);\nint REDISMODULE_API_FUNC(RedisModule_IsKeysPositionRequest)(RedisModuleCtx *ctx);\nvoid REDISMODULE_API_FUNC(RedisModule_KeyAtPos)(RedisModuleCtx *ctx, int pos);\nunsigned long long REDISMODULE_API_FUNC(RedisModule_GetClientId)(RedisModuleCtx *ctx);\nvoid *REDISMODULE_API_FUNC(RedisModule_PoolAlloc)(RedisModuleCtx *ctx, size_t bytes);\nRedisModuleType *REDISMODULE_API_FUNC(RedisModule_CreateDataType)(RedisModuleCtx *ctx, const char *name, int encver, RedisModuleTypeMethods *typemethods);\nint REDISMODULE_API_FUNC(RedisModule_ModuleTypeSetValue)(RedisModuleKey *key, RedisModuleType *mt, void *value);\nRedisModuleType *REDISMODULE_API_FUNC(RedisModule_ModuleTypeGetType)(RedisModuleKey *key);\nvoid *REDISMODULE_API_FUNC(RedisModule_ModuleTypeGetValue)(RedisModuleKey *key);\nvoid REDISMODULE_API_FUNC(RedisModule_SaveUnsigned)(RedisModuleIO *io, uint64_t value);\nuint64_t REDISMODULE_API_FUNC(RedisModule_LoadUnsigned)(RedisModuleIO *io);\nvoid REDISMODULE_API_FUNC(RedisModule_SaveSigned)(RedisModuleIO *io, int64_t value);\nint64_t REDISMODULE_API_FUNC(RedisModule_LoadSigned)(RedisModuleIO *io);\nvoid REDISMODULE_API_FUNC(RedisModule_EmitAOF)(RedisModuleIO *io, const char *cmdname, const char *fmt, ...);\nvoid REDISMODULE_API_FUNC(RedisModule_SaveString)(RedisModuleIO *io, RedisModuleString *s);\nvoid REDISMODULE_API_FUNC(RedisModule_SaveStringBuffer)(RedisModuleIO *io, const char *str, size_t len);\nRedisModuleString *REDISMODULE_API_FUNC(RedisModule_LoadString)(RedisModuleIO *io);\nchar *REDISMODULE_API_FUNC(RedisModule_LoadStringBuffer)(RedisModuleIO *io, size_t *lenptr);\nvoid REDISMODULE_API_FUNC(RedisModule_SaveDouble)(RedisModuleIO *io, double value);\ndouble REDISMODULE_API_FUNC(RedisModule_LoadDouble)(RedisModuleIO *io);\nvoid REDISMODULE_API_FUNC(RedisModule_SaveFloat)(RedisModuleIO *io, float value);\nfloat REDISMODULE_API_FUNC(RedisModule_LoadFloat)(RedisModuleIO *io);\nvoid REDISMODULE_API_FUNC(RedisModule_Log)(RedisModuleCtx *ctx, const char *level, const char *fmt, ...);\nvoid REDISMODULE_API_FUNC(RedisModule_LogIOError)(RedisModuleIO *io, const char *levelstr, const char *fmt, ...);\nint REDISMODULE_API_FUNC(RedisModule_StringAppendBuffer)(RedisModuleCtx *ctx, RedisModuleString *str, const char *buf, size_t len);\nvoid REDISMODULE_API_FUNC(RedisModule_RetainString)(RedisModuleCtx *ctx, RedisModuleString *str);\nint REDISMODULE_API_FUNC(RedisModule_StringCompare)(RedisModuleString *a, RedisModuleString *b);\nRedisModuleCtx *REDISMODULE_API_FUNC(RedisModule_GetContextFromIO)(RedisModuleIO *io);\nRedisModuleBlockedClient *REDISMODULE_API_FUNC(RedisModule_BlockClient)(RedisModuleCtx *ctx, RedisModuleCmdFunc reply_callback, RedisModuleCmdFunc timeout_callback, void (*free_privdata)(void*), long long timeout_ms);\nint REDISMODULE_API_FUNC(RedisModule_UnblockClient)(RedisModuleBlockedClient *bc, void *privdata);\nint REDISMODULE_API_FUNC(RedisModule_IsBlockedReplyRequest)(RedisModuleCtx *ctx);\nint REDISMODULE_API_FUNC(RedisModule_IsBlockedTimeoutRequest)(RedisModuleCtx *ctx);\nvoid *REDISMODULE_API_FUNC(RedisModule_GetBlockedClientPrivateData)(RedisModuleCtx *ctx);\nint REDISMODULE_API_FUNC(RedisModule_AbortBlock)(RedisModuleBlockedClient *bc);\nlong long REDISMODULE_API_FUNC(RedisModule_Milliseconds)(void);\n\n/* This is included inline inside each Redis module. */\nstatic int RedisModule_Init(RedisModuleCtx *ctx, const char *name, int ver, int apiver) __attribute__((unused));\nstatic int RedisModule_Init(RedisModuleCtx *ctx, const char *name, int ver, int apiver) {\n    void *getapifuncptr = ((void**)ctx)[0];\n    RedisModule_GetApi = (int (*)(const char *, void *)) (unsigned long)getapifuncptr;\n    REDISMODULE_GET_API(Alloc);\n    REDISMODULE_GET_API(Calloc);\n    REDISMODULE_GET_API(Free);\n    REDISMODULE_GET_API(Realloc);\n    REDISMODULE_GET_API(Strdup);\n    REDISMODULE_GET_API(CreateCommand);\n    REDISMODULE_GET_API(SetModuleAttribs);\n    REDISMODULE_GET_API(WrongArity);\n    REDISMODULE_GET_API(ReplyWithLongLong);\n    REDISMODULE_GET_API(ReplyWithError);\n    REDISMODULE_GET_API(ReplyWithSimpleString);\n    REDISMODULE_GET_API(ReplyWithArray);\n    REDISMODULE_GET_API(ReplySetArrayLength);\n    REDISMODULE_GET_API(ReplyWithStringBuffer);\n    REDISMODULE_GET_API(ReplyWithString);\n    REDISMODULE_GET_API(ReplyWithNull);\n    REDISMODULE_GET_API(ReplyWithCallReply);\n    REDISMODULE_GET_API(ReplyWithDouble);\n    REDISMODULE_GET_API(ReplySetArrayLength);\n    REDISMODULE_GET_API(GetSelectedDb);\n    REDISMODULE_GET_API(SelectDb);\n    REDISMODULE_GET_API(OpenKey);\n    REDISMODULE_GET_API(CloseKey);\n    REDISMODULE_GET_API(KeyType);\n    REDISMODULE_GET_API(ValueLength);\n    REDISMODULE_GET_API(ListPush);\n    REDISMODULE_GET_API(ListPop);\n    REDISMODULE_GET_API(StringToLongLong);\n    REDISMODULE_GET_API(StringToDouble);\n    REDISMODULE_GET_API(Call);\n    REDISMODULE_GET_API(CallReplyProto);\n    REDISMODULE_GET_API(FreeCallReply);\n    REDISMODULE_GET_API(CallReplyInteger);\n    REDISMODULE_GET_API(CallReplyType);\n    REDISMODULE_GET_API(CallReplyLength);\n    REDISMODULE_GET_API(CallReplyArrayElement);\n    REDISMODULE_GET_API(CallReplyStringPtr);\n    REDISMODULE_GET_API(CreateStringFromCallReply);\n    REDISMODULE_GET_API(CreateString);\n    REDISMODULE_GET_API(CreateStringFromLongLong);\n    REDISMODULE_GET_API(CreateStringFromString);\n    REDISMODULE_GET_API(CreateStringPrintf);\n    REDISMODULE_GET_API(FreeString);\n    REDISMODULE_GET_API(StringPtrLen);\n    REDISMODULE_GET_API(AutoMemory);\n    REDISMODULE_GET_API(Replicate);\n    REDISMODULE_GET_API(ReplicateVerbatim);\n    REDISMODULE_GET_API(DeleteKey);\n    REDISMODULE_GET_API(StringSet);\n    REDISMODULE_GET_API(StringDMA);\n    REDISMODULE_GET_API(StringTruncate);\n    REDISMODULE_GET_API(GetExpire);\n    REDISMODULE_GET_API(SetExpire);\n    REDISMODULE_GET_API(ZsetAdd);\n    REDISMODULE_GET_API(ZsetIncrby);\n    REDISMODULE_GET_API(ZsetScore);\n    REDISMODULE_GET_API(ZsetRem);\n    REDISMODULE_GET_API(ZsetRangeStop);\n    REDISMODULE_GET_API(ZsetFirstInScoreRange);\n    REDISMODULE_GET_API(ZsetLastInScoreRange);\n    REDISMODULE_GET_API(ZsetFirstInLexRange);\n    REDISMODULE_GET_API(ZsetLastInLexRange);\n    REDISMODULE_GET_API(ZsetRangeCurrentElement);\n    REDISMODULE_GET_API(ZsetRangeNext);\n    REDISMODULE_GET_API(ZsetRangePrev);\n    REDISMODULE_GET_API(ZsetRangeEndReached);\n    REDISMODULE_GET_API(HashSet);\n    REDISMODULE_GET_API(HashGet);\n    REDISMODULE_GET_API(IsKeysPositionRequest);\n    REDISMODULE_GET_API(KeyAtPos);\n    REDISMODULE_GET_API(GetClientId);\n    REDISMODULE_GET_API(PoolAlloc);\n    REDISMODULE_GET_API(CreateDataType);\n    REDISMODULE_GET_API(ModuleTypeSetValue);\n    REDISMODULE_GET_API(ModuleTypeGetType);\n    REDISMODULE_GET_API(ModuleTypeGetValue);\n    REDISMODULE_GET_API(SaveUnsigned);\n    REDISMODULE_GET_API(LoadUnsigned);\n    REDISMODULE_GET_API(SaveSigned);\n    REDISMODULE_GET_API(LoadSigned);\n    REDISMODULE_GET_API(SaveString);\n    REDISMODULE_GET_API(SaveStringBuffer);\n    REDISMODULE_GET_API(LoadString);\n    REDISMODULE_GET_API(LoadStringBuffer);\n    REDISMODULE_GET_API(SaveDouble);\n    REDISMODULE_GET_API(LoadDouble);\n    REDISMODULE_GET_API(SaveFloat);\n    REDISMODULE_GET_API(LoadFloat);\n    REDISMODULE_GET_API(EmitAOF);\n    REDISMODULE_GET_API(Log);\n    REDISMODULE_GET_API(LogIOError);\n    REDISMODULE_GET_API(StringAppendBuffer);\n    REDISMODULE_GET_API(RetainString);\n    REDISMODULE_GET_API(StringCompare);\n    REDISMODULE_GET_API(GetContextFromIO);\n    REDISMODULE_GET_API(BlockClient);\n    REDISMODULE_GET_API(UnblockClient);\n    REDISMODULE_GET_API(IsBlockedReplyRequest);\n    REDISMODULE_GET_API(IsBlockedTimeoutRequest);\n    REDISMODULE_GET_API(GetBlockedClientPrivateData);\n    REDISMODULE_GET_API(AbortBlock);\n    REDISMODULE_GET_API(Milliseconds);\n\n    RedisModule_SetModuleAttribs(ctx,name,ver,apiver);\n    return REDISMODULE_OK;\n}\n\n#else\n\n/* Things only defined for the modules core, not exported to modules\n * including this file. */\n#define RedisModuleString robj\n\n#endif /* REDISMODULE_CORE */\n#endif /* REDISMOUDLE_H */\n"
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}