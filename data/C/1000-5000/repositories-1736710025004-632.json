{
  "metadata": {
    "timestamp": 1736710025004,
    "page": 632,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjY0MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "codeplea/genann",
      "stars": 2036,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".travis.yml",
          "type": "blob",
          "size": 0.0537109375,
          "content": "language: c\n\ncompiler:\n  - clang\n  - gcc\n\nscript: make\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 0.85546875,
          "content": "zlib License\n\nCopyright (C) 2015-2018 Lewis Van Winkle\n\nThis software is provided 'as-is', without any express or implied\nwarranty. In no event will the authors be held liable for any damages\narising from the use of this software.\n\nPermission is granted to anyone to use this software for any purpose,\nincluding commercial applications, and to alter it and redistribute it\nfreely, subject to the following restrictions:\n\n1. The origin of this software must not be misrepresented; you must not\n   claim that you wrote the original software. If you use this software\n   in a product, an acknowledgement in the product documentation would be\n   appreciated but is not required.\n2. Altered source versions must be plainly marked as such, and must not be\n   misrepresented as being the original software.\n3. This notice may not be removed or altered from any source distribution.\n\n"
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 0.595703125,
          "content": "CFLAGS = -Wall -Wshadow -O3 -g -march=native\nLDLIBS = -lm\n\nall: check example1 example2 example3 example4\n\nsigmoid: CFLAGS += -Dgenann_act=genann_act_sigmoid_cached\nsigmoid: all\n\nthreshold: CFLAGS += -Dgenann_act=genann_act_threshold\nthreshold: all\n\nlinear: CFLAGS += -Dgenann_act=genann_act_linear\nlinear: all\n\ntest: test.o genann.o\n\ncheck: test\n\t./$^\n\nexample1: example1.o genann.o\n\nexample2: example2.o genann.o\n\nexample3: example3.o genann.o\n\nexample4: example4.o genann.o\n\n\nclean:\n\t$(RM) *.o\n\t$(RM) test example1 example2 example3 example4 *.exe\n\t$(RM) persist.txt\n\n.PHONY: sigmoid threshold linear clean\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 5.60546875,
          "content": "[![Build Status](https://travis-ci.org/codeplea/genann.svg?branch=master)](https://travis-ci.org/codeplea/genann)\n\n<img alt=\"Genann logo\" src=\"https://codeplea.com/public/content/genann_logo.png\" align=\"right\" />\n\n# Genann\n\nGenann is a minimal, well-tested library for training and using feedforward\nartificial neural networks (ANN) in C. Its primary focus is on being simple,\nfast, reliable, and hackable. It achieves this by providing only the necessary\nfunctions and little extra.\n\n## Features\n\n- **C99 with no dependencies**.\n- Contained in a single source code and header file.\n- Simple.\n- Fast and thread-safe.\n- Easily extendible.\n- Implements backpropagation training.\n- *Compatible with alternative training methods* (classic optimization, genetic algorithms, etc)\n- Includes examples and test suite.\n- Released under the zlib license - free for nearly any use.\n\n## Building\n\nGenann is self-contained in two files: `genann.c` and `genann.h`. To use Genann, simply add those two files to your project.\n\n## Example Code\n\nFour example programs are included with the source code.\n\n- [`example1.c`](./example1.c) - Trains an ANN on the XOR function using backpropagation.\n- [`example2.c`](./example2.c) - Trains an ANN on the XOR function using random search.\n- [`example3.c`](./example3.c) - Loads and runs an ANN from a file.\n- [`example4.c`](./example4.c) - Trains an ANN on the [IRIS data-set](https://archive.ics.uci.edu/ml/datasets/Iris) using backpropagation.\n\n## Quick Example\n\nWe create an ANN taking 2 inputs, having 1 layer of 3 hidden neurons, and\nproviding 2 outputs. It has the following structure:\n\n![NN Example Structure](./doc/e1.png)\n\nWe then train it on a set of labeled data using backpropagation and ask it to\npredict on a test data point:\n\n```C\n#include \"genann.h\"\n\n/* Not shown, loading your training and test data. */\ndouble **training_data_input, **training_data_output, **test_data_input;\n\n/* New network with 2 inputs,\n * 1 hidden layer of 3 neurons each,\n * and 2 outputs. */\ngenann *ann = genann_init(2, 1, 3, 2);\n\n/* Learn on the training set. */\nfor (i = 0; i < 300; ++i) {\n    for (j = 0; j < 100; ++j)\n        genann_train(ann, training_data_input[j], training_data_output[j], 0.1);\n}\n\n/* Run the network and see what it predicts. */\ndouble const *prediction = genann_run(ann, test_data_input[0]);\nprintf(\"Output for the first test data point is: %f, %f\\n\", prediction[0], prediction[1]);\n\ngenann_free(ann);\n```\n\nThis example is to show API usage, it is not showing good machine learning\ntechniques. In a real application you would likely want to learn on the test\ndata in a random order. You would also want to monitor the learning to prevent\nover-fitting.\n\n\n## Usage\n\n### Creating and Freeing ANNs\n```C\ngenann *genann_init(int inputs, int hidden_layers, int hidden, int outputs);\ngenann *genann_copy(genann const *ann);\nvoid genann_free(genann *ann);\n```\n\nCreating a new ANN is done with the `genann_init()` function. Its arguments\nare the number of inputs, the number of hidden layers, the number of neurons in\neach hidden layer, and the number of outputs. It returns a `genann` struct pointer.\n\nCalling `genann_copy()` will create a deep-copy of an existing `genann` struct.\n\nCall `genann_free()` when you're finished with an ANN returned by `genann_init()`.\n\n\n### Training ANNs\n```C\nvoid genann_train(genann const *ann, double const *inputs,\n        double const *desired_outputs, double learning_rate);\n```\n\n`genann_train()` will preform one update using standard backpropogation. It\nshould be called by passing in an array of inputs, an array of expected outputs,\nand a learning rate. See *example1.c* for an example of learning with\nbackpropogation.\n\nA primary design goal of Genann was to store all the network weights in one\ncontigious block of memory. This makes it easy and efficient to train the\nnetwork weights using direct-search numeric optimization algorthims,\nsuch as [Hill Climbing](https://en.wikipedia.org/wiki/Hill_climbing),\n[the Genetic Algorithm](https://en.wikipedia.org/wiki/Genetic_algorithm), [Simulated\nAnnealing](https://en.wikipedia.org/wiki/Simulated_annealing), etc.\nThese methods can be used by searching on the ANN's weights directly.\nEvery `genann` struct contains the members `int total_weights;` and\n`double *weight;`.  `*weight` points to an array of `total_weights`\nsize which contains all weights used by the ANN. See *example2.c* for\nan example of training using random hill climbing search.\n\n### Saving and Loading ANNs\n\n```C\ngenann *genann_read(FILE *in);\nvoid genann_write(genann const *ann, FILE *out);\n```\n \nGenann provides the `genann_read()` and `genann_write()` functions for loading or saving an ANN in a text-based format.\n\n### Evaluating\n\n```C\ndouble const *genann_run(genann const *ann, double const *inputs);\n```\n\nCall `genann_run()` on a trained ANN to run a feed-forward pass on a given set of inputs. `genann_run()`\nwill provide a pointer to the array of predicted outputs (of `ann->outputs` length).\n\n\n## Hints\n\n- All functions start with `genann_`.\n- The code is simple. Dig in and change things.\n\n## Extra Resources\n\nThe [comp.ai.neural-nets\nFAQ](http://www.faqs.org/faqs/ai-faq/neural-nets/part1/) is an excellent\nresource for an introduction to artificial neural networks.\n\nIf you need an even smaller neural network library, check out the excellent single-hidden-layer library [tinn](https://github.com/glouw/tinn).\n\nIf you're looking for a heavier, more opinionated neural network library in C,\nI recommend the [FANN library](http://leenissen.dk/fann/wp/). Another\ngood library is Peter van Rossum's [Lightweight Neural\nNetwork](http://lwneuralnet.sourceforge.net/), which despite its name, is\nheavier and has more features than Genann.\n"
        },
        {
          "name": "doc",
          "type": "tree",
          "content": null
        },
        {
          "name": "example",
          "type": "tree",
          "content": null
        },
        {
          "name": "example1.c",
          "type": "blob",
          "size": 1.4970703125,
          "content": "#include <stdio.h>\n#include <stdlib.h>\n#include <time.h>\n#include \"genann.h\"\n\nint main(int argc, char *argv[])\n{\n    printf(\"GENANN example 1.\\n\");\n    printf(\"Train a small ANN to the XOR function using backpropagation.\\n\");\n\n    /* This will make the neural network initialize differently each run. */\n    /* If you don't get a good result, try again for a different result. */\n    srand(time(0));\n\n    /* Input and expected out data for the XOR function. */\n    const double input[4][2] = {{0, 0}, {0, 1}, {1, 0}, {1, 1}};\n    const double output[4] = {0, 1, 1, 0};\n    int i;\n\n    /* New network with 2 inputs,\n     * 1 hidden layer of 2 neurons,\n     * and 1 output. */\n    genann *ann = genann_init(2, 1, 2, 1);\n\n    /* Train on the four labeled data points many times. */\n    for (i = 0; i < 500; ++i) {\n        genann_train(ann, input[0], output + 0, 3);\n        genann_train(ann, input[1], output + 1, 3);\n        genann_train(ann, input[2], output + 2, 3);\n        genann_train(ann, input[3], output + 3, 3);\n    }\n\n    /* Run the network and see what it predicts. */\n    printf(\"Output for [%1.f, %1.f] is %1.f.\\n\", input[0][0], input[0][1], *genann_run(ann, input[0]));\n    printf(\"Output for [%1.f, %1.f] is %1.f.\\n\", input[1][0], input[1][1], *genann_run(ann, input[1]));\n    printf(\"Output for [%1.f, %1.f] is %1.f.\\n\", input[2][0], input[2][1], *genann_run(ann, input[2]));\n    printf(\"Output for [%1.f, %1.f] is %1.f.\\n\", input[3][0], input[3][1], *genann_run(ann, input[3]));\n\n    genann_free(ann);\n    return 0;\n}\n"
        },
        {
          "name": "example2.c",
          "type": "blob",
          "size": 2.111328125,
          "content": "#include <stdio.h>\n#include <stdlib.h>\n#include <time.h>\n#include <math.h>\n#include \"genann.h\"\n\nint main(int argc, char *argv[])\n{\n    printf(\"GENANN example 2.\\n\");\n    printf(\"Train a small ANN to the XOR function using random search.\\n\");\n\n    srand(time(0));\n\n    /* Input and expected out data for the XOR function. */\n    const double input[4][2] = {{0, 0}, {0, 1}, {1, 0}, {1, 1}};\n    const double output[4] = {0, 1, 1, 0};\n    int i;\n\n    /* New network with 2 inputs,\n     * 1 hidden layer of 2 neurons,\n     * and 1 output. */\n    genann *ann = genann_init(2, 1, 2, 1);\n\n    double err;\n    double last_err = 1000;\n    int count = 0;\n\n    do {\n        ++count;\n        if (count % 1000 == 0) {\n            /* We're stuck, start over. */\n            genann_randomize(ann);\n            last_err = 1000;\n        }\n\n        genann *save = genann_copy(ann);\n\n        /* Take a random guess at the ANN weights. */\n        for (i = 0; i < ann->total_weights; ++i) {\n            ann->weight[i] += ((double)rand())/RAND_MAX-0.5;\n        }\n\n        /* See how we did. */\n        err = 0;\n        err += pow(*genann_run(ann, input[0]) - output[0], 2.0);\n        err += pow(*genann_run(ann, input[1]) - output[1], 2.0);\n        err += pow(*genann_run(ann, input[2]) - output[2], 2.0);\n        err += pow(*genann_run(ann, input[3]) - output[3], 2.0);\n\n        /* Keep these weights if they're an improvement. */\n        if (err < last_err) {\n            genann_free(save);\n            last_err = err;\n        } else {\n            genann_free(ann);\n            ann = save;\n        }\n\n    } while (err > 0.01);\n\n    printf(\"Finished in %d loops.\\n\", count);\n\n    /* Run the network and see what it predicts. */\n    printf(\"Output for [%1.f, %1.f] is %1.f.\\n\", input[0][0], input[0][1], *genann_run(ann, input[0]));\n    printf(\"Output for [%1.f, %1.f] is %1.f.\\n\", input[1][0], input[1][1], *genann_run(ann, input[1]));\n    printf(\"Output for [%1.f, %1.f] is %1.f.\\n\", input[2][0], input[2][1], *genann_run(ann, input[2]));\n    printf(\"Output for [%1.f, %1.f] is %1.f.\\n\", input[3][0], input[3][1], *genann_run(ann, input[3]));\n\n    genann_free(ann);\n    return 0;\n}\n"
        },
        {
          "name": "example3.c",
          "type": "blob",
          "size": 1.1259765625,
          "content": "#include <stdio.h>\n#include <stdlib.h>\n#include \"genann.h\"\n\nconst char *save_name = \"example/xor.ann\";\n\nint main(int argc, char *argv[])\n{\n    printf(\"GENANN example 3.\\n\");\n    printf(\"Load a saved ANN to solve the XOR function.\\n\");\n\n\n    FILE *saved = fopen(save_name, \"r\");\n    if (!saved) {\n        printf(\"Couldn't open file: %s\\n\", save_name);\n        exit(1);\n    }\n\n    genann *ann = genann_read(saved);\n    fclose(saved);\n\n    if (!ann) {\n        printf(\"Error loading ANN from file: %s.\", save_name);\n        exit(1);\n    }\n\n\n    /* Input data for the XOR function. */\n    const double input[4][2] = {{0, 0}, {0, 1}, {1, 0}, {1, 1}};\n\n    /* Run the network and see what it predicts. */\n    printf(\"Output for [%1.f, %1.f] is %1.f.\\n\", input[0][0], input[0][1], *genann_run(ann, input[0]));\n    printf(\"Output for [%1.f, %1.f] is %1.f.\\n\", input[1][0], input[1][1], *genann_run(ann, input[1]));\n    printf(\"Output for [%1.f, %1.f] is %1.f.\\n\", input[2][0], input[2][1], *genann_run(ann, input[2]));\n    printf(\"Output for [%1.f, %1.f] is %1.f.\\n\", input[3][0], input[3][1], *genann_run(ann, input[3]));\n\n    genann_free(ann);\n    return 0;\n}\n"
        },
        {
          "name": "example4.c",
          "type": "blob",
          "size": 3.1806640625,
          "content": "#include <stdio.h>\n#include <stdlib.h>\n#include <time.h>\n#include <string.h>\n#include <math.h>\n#include \"genann.h\"\n\n/* This example is to illustrate how to use GENANN.\n * It is NOT an example of good machine learning techniques.\n */\n\nconst char *iris_data = \"example/iris.data\";\n\ndouble *input, *class;\nint samples;\nconst char *class_names[] = {\"Iris-setosa\", \"Iris-versicolor\", \"Iris-virginica\"};\n\nvoid load_data() {\n    /* Load the iris data-set. */\n    FILE *in = fopen(\"example/iris.data\", \"r\");\n    if (!in) {\n        printf(\"Could not open file: %s\\n\", iris_data);\n        exit(1);\n    }\n\n    /* Loop through the data to get a count. */\n    char line[1024];\n    while (!feof(in) && fgets(line, 1024, in)) {\n        ++samples;\n    }\n    fseek(in, 0, SEEK_SET);\n\n    printf(\"Loading %d data points from %s\\n\", samples, iris_data);\n\n    /* Allocate memory for input and output data. */\n    input = malloc(sizeof(double) * samples * 4);\n    class = malloc(sizeof(double) * samples * 3);\n\n    /* Read the file into our arrays. */\n    int i, j;\n    for (i = 0; i < samples; ++i) {\n        double *p = input + i * 4;\n        double *c = class + i * 3;\n        c[0] = c[1] = c[2] = 0.0;\n\n        if (fgets(line, 1024, in) == NULL) {\n            perror(\"fgets\");\n            exit(1);\n        }\n\n        char *split = strtok(line, \",\");\n        for (j = 0; j < 4; ++j) {\n            p[j] = atof(split);\n            split = strtok(0, \",\");\n        }\n\n        split[strlen(split)-1] = 0;\n        if (strcmp(split, class_names[0]) == 0) {c[0] = 1.0;}\n        else if (strcmp(split, class_names[1]) == 0) {c[1] = 1.0;}\n        else if (strcmp(split, class_names[2]) == 0) {c[2] = 1.0;}\n        else {\n            printf(\"Unknown class %s.\\n\", split);\n            exit(1);\n        }\n\n        /* printf(\"Data point %d is %f %f %f %f  ->   %f %f %f\\n\", i, p[0], p[1], p[2], p[3], c[0], c[1], c[2]); */\n    }\n\n    fclose(in);\n}\n\n\nint main(int argc, char *argv[])\n{\n    printf(\"GENANN example 4.\\n\");\n    printf(\"Train an ANN on the IRIS dataset using backpropagation.\\n\");\n\n    srand(time(0));\n\n    /* Load the data from file. */\n    load_data();\n\n    /* 4 inputs.\n     * 1 hidden layer(s) of 4 neurons.\n     * 3 outputs (1 per class)\n     */\n    genann *ann = genann_init(4, 1, 4, 3);\n\n    int i, j;\n    int loops = 5000;\n\n    /* Train the network with backpropagation. */\n    printf(\"Training for %d loops over data.\\n\", loops);\n    for (i = 0; i < loops; ++i) {\n        for (j = 0; j < samples; ++j) {\n            genann_train(ann, input + j*4, class + j*3, .01);\n        }\n        /* printf(\"%1.2f \", xor_score(ann)); */\n    }\n\n    int correct = 0;\n    for (j = 0; j < samples; ++j) {\n        const double *guess = genann_run(ann, input + j*4);\n        if (class[j*3+0] == 1.0) {if (guess[0] > guess[1] && guess[0] > guess[2]) ++correct;}\n        else if (class[j*3+1] == 1.0) {if (guess[1] > guess[0] && guess[1] > guess[2]) ++correct;}\n        else if (class[j*3+2] == 1.0) {if (guess[2] > guess[0] && guess[2] > guess[1]) ++correct;}\n        else {printf(\"Logic error.\\n\"); exit(1);}\n    }\n\n    printf(\"%d/%d correct (%0.1f%%).\\n\", correct, samples, (double)correct / samples * 100.0);\n\n\n\n    genann_free(ann);\n    free(input);\n    free(class);\n\n    return 0;\n}\n"
        },
        {
          "name": "genann.c",
          "type": "blob",
          "size": 11.8037109375,
          "content": "/*\n * GENANN - Minimal C Artificial Neural Network\n *\n * Copyright (c) 2015-2018 Lewis Van Winkle\n *\n * http://CodePlea.com\n *\n * This software is provided 'as-is', without any express or implied\n * warranty. In no event will the authors be held liable for any damages\n * arising from the use of this software.\n *\n * Permission is granted to anyone to use this software for any purpose,\n * including commercial applications, and to alter it and redistribute it\n * freely, subject to the following restrictions:\n *\n * 1. The origin of this software must not be misrepresented; you must not\n *    claim that you wrote the original software. If you use this software\n *    in a product, an acknowledgement in the product documentation would be\n *    appreciated but is not required.\n * 2. Altered source versions must be plainly marked as such, and must not be\n *    misrepresented as being the original software.\n * 3. This notice may not be removed or altered from any source distribution.\n *\n */\n\n#include \"genann.h\"\n\n#include <assert.h>\n#include <errno.h>\n#include <math.h>\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n\n#ifndef genann_act\n#define genann_act_hidden genann_act_hidden_indirect\n#define genann_act_output genann_act_output_indirect\n#else\n#define genann_act_hidden genann_act\n#define genann_act_output genann_act\n#endif\n\n#define LOOKUP_SIZE 4096\n\ndouble genann_act_hidden_indirect(const struct genann *ann, double a) {\n    return ann->activation_hidden(ann, a);\n}\n\ndouble genann_act_output_indirect(const struct genann *ann, double a) {\n    return ann->activation_output(ann, a);\n}\n\nconst double sigmoid_dom_min = -15.0;\nconst double sigmoid_dom_max = 15.0;\ndouble interval;\ndouble lookup[LOOKUP_SIZE];\n\n#ifdef __GNUC__\n#define likely(x)       __builtin_expect(!!(x), 1)\n#define unlikely(x)     __builtin_expect(!!(x), 0)\n#define unused          __attribute__((unused))\n#else\n#define likely(x)       x\n#define unlikely(x)     x\n#define unused\n#pragma warning(disable : 4996) /* For fscanf */\n#endif\n\n\ndouble genann_act_sigmoid(const genann *ann unused, double a) {\n    if (a < -45.0) return 0;\n    if (a > 45.0) return 1;\n    return 1.0 / (1 + exp(-a));\n}\n\nvoid genann_init_sigmoid_lookup(const genann *ann) {\n        const double f = (sigmoid_dom_max - sigmoid_dom_min) / LOOKUP_SIZE;\n        int i;\n\n        interval = LOOKUP_SIZE / (sigmoid_dom_max - sigmoid_dom_min);\n        for (i = 0; i < LOOKUP_SIZE; ++i) {\n            lookup[i] = genann_act_sigmoid(ann, sigmoid_dom_min + f * i);\n        }\n}\n\ndouble genann_act_sigmoid_cached(const genann *ann unused, double a) {\n    assert(!isnan(a));\n\n    if (a < sigmoid_dom_min) return lookup[0];\n    if (a >= sigmoid_dom_max) return lookup[LOOKUP_SIZE - 1];\n\n    size_t j = (size_t)((a-sigmoid_dom_min)*interval+0.5);\n\n    /* Because floating point... */\n    if (unlikely(j >= LOOKUP_SIZE)) return lookup[LOOKUP_SIZE - 1];\n\n    return lookup[j];\n}\n\ndouble genann_act_linear(const struct genann *ann unused, double a) {\n    return a;\n}\n\ndouble genann_act_threshold(const struct genann *ann unused, double a) {\n    return a > 0;\n}\n\ngenann *genann_init(int inputs, int hidden_layers, int hidden, int outputs) {\n    if (hidden_layers < 0) return 0;\n    if (inputs < 1) return 0;\n    if (outputs < 1) return 0;\n    if (hidden_layers > 0 && hidden < 1) return 0;\n\n\n    const int hidden_weights = hidden_layers ? (inputs+1) * hidden + (hidden_layers-1) * (hidden+1) * hidden : 0;\n    const int output_weights = (hidden_layers ? (hidden+1) : (inputs+1)) * outputs;\n    const int total_weights = (hidden_weights + output_weights);\n\n    const int total_neurons = (inputs + hidden * hidden_layers + outputs);\n\n    /* Allocate extra size for weights, outputs, and deltas. */\n    const int size = sizeof(genann) + sizeof(double) * (total_weights + total_neurons + (total_neurons - inputs));\n    genann *ret = malloc(size);\n    if (!ret) return 0;\n\n    ret->inputs = inputs;\n    ret->hidden_layers = hidden_layers;\n    ret->hidden = hidden;\n    ret->outputs = outputs;\n\n    ret->total_weights = total_weights;\n    ret->total_neurons = total_neurons;\n\n    /* Set pointers. */\n    ret->weight = (double*)((char*)ret + sizeof(genann));\n    ret->output = ret->weight + ret->total_weights;\n    ret->delta = ret->output + ret->total_neurons;\n\n    genann_randomize(ret);\n\n    ret->activation_hidden = genann_act_sigmoid_cached;\n    ret->activation_output = genann_act_sigmoid_cached;\n\n    genann_init_sigmoid_lookup(ret);\n\n    return ret;\n}\n\n\ngenann *genann_read(FILE *in) {\n    int inputs, hidden_layers, hidden, outputs;\n    int rc;\n\n    errno = 0;\n    rc = fscanf(in, \"%d %d %d %d\", &inputs, &hidden_layers, &hidden, &outputs);\n    if (rc < 4 || errno != 0) {\n        perror(\"fscanf\");\n        return NULL;\n    }\n\n    genann *ann = genann_init(inputs, hidden_layers, hidden, outputs);\n\n    int i;\n    for (i = 0; i < ann->total_weights; ++i) {\n        errno = 0;\n        rc = fscanf(in, \" %le\", ann->weight + i);\n        if (rc < 1 || errno != 0) {\n            perror(\"fscanf\");\n            genann_free(ann);\n\n            return NULL;\n        }\n    }\n\n    return ann;\n}\n\n\ngenann *genann_copy(genann const *ann) {\n    const int size = sizeof(genann) + sizeof(double) * (ann->total_weights + ann->total_neurons + (ann->total_neurons - ann->inputs));\n    genann *ret = malloc(size);\n    if (!ret) return 0;\n\n    memcpy(ret, ann, size);\n\n    /* Set pointers. */\n    ret->weight = (double*)((char*)ret + sizeof(genann));\n    ret->output = ret->weight + ret->total_weights;\n    ret->delta = ret->output + ret->total_neurons;\n\n    return ret;\n}\n\n\nvoid genann_randomize(genann *ann) {\n    int i;\n    for (i = 0; i < ann->total_weights; ++i) {\n        double r = GENANN_RANDOM();\n        /* Sets weights from -0.5 to 0.5. */\n        ann->weight[i] = r - 0.5;\n    }\n}\n\n\nvoid genann_free(genann *ann) {\n    /* The weight, output, and delta pointers go to the same buffer. */\n    free(ann);\n}\n\n\ndouble const *genann_run(genann const *ann, double const *inputs) {\n    double const *w = ann->weight;\n    double *o = ann->output + ann->inputs;\n    double const *i = ann->output;\n\n    /* Copy the inputs to the scratch area, where we also store each neuron's\n     * output, for consistency. This way the first layer isn't a special case. */\n    memcpy(ann->output, inputs, sizeof(double) * ann->inputs);\n\n    int h, j, k;\n\n    if (!ann->hidden_layers) {\n        double *ret = o;\n        for (j = 0; j < ann->outputs; ++j) {\n            double sum = *w++ * -1.0;\n            for (k = 0; k < ann->inputs; ++k) {\n                sum += *w++ * i[k];\n            }\n            *o++ = genann_act_output(ann, sum);\n        }\n\n        return ret;\n    }\n\n    /* Figure input layer */\n    for (j = 0; j < ann->hidden; ++j) {\n        double sum = *w++ * -1.0;\n        for (k = 0; k < ann->inputs; ++k) {\n            sum += *w++ * i[k];\n        }\n        *o++ = genann_act_hidden(ann, sum);\n    }\n\n    i += ann->inputs;\n\n    /* Figure hidden layers, if any. */\n    for (h = 1; h < ann->hidden_layers; ++h) {\n        for (j = 0; j < ann->hidden; ++j) {\n            double sum = *w++ * -1.0;\n            for (k = 0; k < ann->hidden; ++k) {\n                sum += *w++ * i[k];\n            }\n            *o++ = genann_act_hidden(ann, sum);\n        }\n\n        i += ann->hidden;\n    }\n\n    double const *ret = o;\n\n    /* Figure output layer. */\n    for (j = 0; j < ann->outputs; ++j) {\n        double sum = *w++ * -1.0;\n        for (k = 0; k < ann->hidden; ++k) {\n            sum += *w++ * i[k];\n        }\n        *o++ = genann_act_output(ann, sum);\n    }\n\n    /* Sanity check that we used all weights and wrote all outputs. */\n    assert(w - ann->weight == ann->total_weights);\n    assert(o - ann->output == ann->total_neurons);\n\n    return ret;\n}\n\n\nvoid genann_train(genann const *ann, double const *inputs, double const *desired_outputs, double learning_rate) {\n    /* To begin with, we must run the network forward. */\n    genann_run(ann, inputs);\n\n    int h, j, k;\n\n    /* First set the output layer deltas. */\n    {\n        double const *o = ann->output + ann->inputs + ann->hidden * ann->hidden_layers; /* First output. */\n        double *d = ann->delta + ann->hidden * ann->hidden_layers; /* First delta. */\n        double const *t = desired_outputs; /* First desired output. */\n\n\n        /* Set output layer deltas. */\n        if (genann_act_output == genann_act_linear ||\n                ann->activation_output == genann_act_linear) {\n            for (j = 0; j < ann->outputs; ++j) {\n                *d++ = *t++ - *o++;\n            }\n        } else {\n            for (j = 0; j < ann->outputs; ++j) {\n                *d++ = (*t - *o) * *o * (1.0 - *o);\n                ++o; ++t;\n            }\n        }\n    }\n\n\n    /* Set hidden layer deltas, start on last layer and work backwards. */\n    /* Note that loop is skipped in the case of hidden_layers == 0. */\n    for (h = ann->hidden_layers - 1; h >= 0; --h) {\n\n        /* Find first output and delta in this layer. */\n        double const *o = ann->output + ann->inputs + (h * ann->hidden);\n        double *d = ann->delta + (h * ann->hidden);\n\n        /* Find first delta in following layer (which may be hidden or output). */\n        double const * const dd = ann->delta + ((h+1) * ann->hidden);\n\n        /* Find first weight in following layer (which may be hidden or output). */\n        double const * const ww = ann->weight + ((ann->inputs+1) * ann->hidden) + ((ann->hidden+1) * ann->hidden * (h));\n\n        for (j = 0; j < ann->hidden; ++j) {\n\n            double delta = 0;\n\n            for (k = 0; k < (h == ann->hidden_layers-1 ? ann->outputs : ann->hidden); ++k) {\n                const double forward_delta = dd[k];\n                const int windex = k * (ann->hidden + 1) + (j + 1);\n                const double forward_weight = ww[windex];\n                delta += forward_delta * forward_weight;\n            }\n\n            *d = *o * (1.0-*o) * delta;\n            ++d; ++o;\n        }\n    }\n\n\n    /* Train the outputs. */\n    {\n        /* Find first output delta. */\n        double const *d = ann->delta + ann->hidden * ann->hidden_layers; /* First output delta. */\n\n        /* Find first weight to first output delta. */\n        double *w = ann->weight + (ann->hidden_layers\n                ? ((ann->inputs+1) * ann->hidden + (ann->hidden+1) * ann->hidden * (ann->hidden_layers-1))\n                : (0));\n\n        /* Find first output in previous layer. */\n        double const * const i = ann->output + (ann->hidden_layers\n                ? (ann->inputs + (ann->hidden) * (ann->hidden_layers-1))\n                : 0);\n\n        /* Set output layer weights. */\n        for (j = 0; j < ann->outputs; ++j) {\n            *w++ += *d * learning_rate * -1.0;\n            for (k = 1; k < (ann->hidden_layers ? ann->hidden : ann->inputs) + 1; ++k) {\n                *w++ += *d * learning_rate * i[k-1];\n            }\n\n            ++d;\n        }\n\n        assert(w - ann->weight == ann->total_weights);\n    }\n\n\n    /* Train the hidden layers. */\n    for (h = ann->hidden_layers - 1; h >= 0; --h) {\n\n        /* Find first delta in this layer. */\n        double const *d = ann->delta + (h * ann->hidden);\n\n        /* Find first input to this layer. */\n        double const *i = ann->output + (h\n                ? (ann->inputs + ann->hidden * (h-1))\n                : 0);\n\n        /* Find first weight to this layer. */\n        double *w = ann->weight + (h\n                ? ((ann->inputs+1) * ann->hidden + (ann->hidden+1) * (ann->hidden) * (h-1))\n                : 0);\n\n\n        for (j = 0; j < ann->hidden; ++j) {\n            *w++ += *d * learning_rate * -1.0;\n            for (k = 1; k < (h == 0 ? ann->inputs : ann->hidden) + 1; ++k) {\n                *w++ += *d * learning_rate * i[k-1];\n            }\n            ++d;\n        }\n\n    }\n\n}\n\n\nvoid genann_write(genann const *ann, FILE *out) {\n    fprintf(out, \"%d %d %d %d\", ann->inputs, ann->hidden_layers, ann->hidden, ann->outputs);\n\n    int i;\n    for (i = 0; i < ann->total_weights; ++i) {\n        fprintf(out, \" %.20e\", ann->weight[i]);\n    }\n}\n\n\n"
        },
        {
          "name": "genann.h",
          "type": "blob",
          "size": 3.283203125,
          "content": "/*\n * GENANN - Minimal C Artificial Neural Network\n *\n * Copyright (c) 2015-2018 Lewis Van Winkle\n *\n * http://CodePlea.com\n *\n * This software is provided 'as-is', without any express or implied\n * warranty. In no event will the authors be held liable for any damages\n * arising from the use of this software.\n *\n * Permission is granted to anyone to use this software for any purpose,\n * including commercial applications, and to alter it and redistribute it\n * freely, subject to the following restrictions:\n *\n * 1. The origin of this software must not be misrepresented; you must not\n *    claim that you wrote the original software. If you use this software\n *    in a product, an acknowledgement in the product documentation would be\n *    appreciated but is not required.\n * 2. Altered source versions must be plainly marked as such, and must not be\n *    misrepresented as being the original software.\n * 3. This notice may not be removed or altered from any source distribution.\n *\n */\n\n\n#ifndef GENANN_H\n#define GENANN_H\n\n#include <stdio.h>\n\n#ifdef __cplusplus\nextern \"C\" {\n#endif\n\n#ifndef GENANN_RANDOM\n/* We use the following for uniform random numbers between 0 and 1.\n * If you have a better function, redefine this macro. */\n#define GENANN_RANDOM() (((double)rand())/RAND_MAX)\n#endif\n\nstruct genann;\n\ntypedef double (*genann_actfun)(const struct genann *ann, double a);\n\ntypedef struct genann {\n    /* How many inputs, outputs, and hidden neurons. */\n    int inputs, hidden_layers, hidden, outputs;\n\n    /* Which activation function to use for hidden neurons. Default: gennann_act_sigmoid_cached*/\n    genann_actfun activation_hidden;\n\n    /* Which activation function to use for output. Default: gennann_act_sigmoid_cached*/\n    genann_actfun activation_output;\n\n    /* Total number of weights, and size of weights buffer. */\n    int total_weights;\n\n    /* Total number of neurons + inputs and size of output buffer. */\n    int total_neurons;\n\n    /* All weights (total_weights long). */\n    double *weight;\n\n    /* Stores input array and output of each neuron (total_neurons long). */\n    double *output;\n\n    /* Stores delta of each hidden and output neuron (total_neurons - inputs long). */\n    double *delta;\n\n} genann;\n\n/* Creates and returns a new ann. */\ngenann *genann_init(int inputs, int hidden_layers, int hidden, int outputs);\n\n/* Creates ANN from file saved with genann_write. */\ngenann *genann_read(FILE *in);\n\n/* Sets weights randomly. Called by init. */\nvoid genann_randomize(genann *ann);\n\n/* Returns a new copy of ann. */\ngenann *genann_copy(genann const *ann);\n\n/* Frees the memory used by an ann. */\nvoid genann_free(genann *ann);\n\n/* Runs the feedforward algorithm to calculate the ann's output. */\ndouble const *genann_run(genann const *ann, double const *inputs);\n\n/* Does a single backprop update. */\nvoid genann_train(genann const *ann, double const *inputs, double const *desired_outputs, double learning_rate);\n\n/* Saves the ann. */\nvoid genann_write(genann const *ann, FILE *out);\n\nvoid genann_init_sigmoid_lookup(const genann *ann);\ndouble genann_act_sigmoid(const genann *ann, double a);\ndouble genann_act_sigmoid_cached(const genann *ann, double a);\ndouble genann_act_threshold(const genann *ann, double a);\ndouble genann_act_linear(const genann *ann, double a);\n\n\n#ifdef __cplusplus\n}\n#endif\n\n#endif /*GENANN_H*/\n"
        },
        {
          "name": "minctest.h",
          "type": "blob",
          "size": 3.150390625,
          "content": "/*\n *\n * MINCTEST - Minimal C Test Library - 0.1\n *\n * Copyright (c) 2014, 2015, 2016 Lewis Van Winkle\n *\n * http://CodePlea.com\n *\n * This software is provided 'as-is', without any express or implied\n * warranty. In no event will the authors be held liable for any damages\n * arising from the use of this software.\n *\n * Permission is granted to anyone to use this software for any purpose,\n * including commercial applications, and to alter it and redistribute it\n * freely, subject to the following restrictions:\n *\n * 1. The origin of this software must not be misrepresented; you must not\n *    claim that you wrote the original software. If you use this software\n *    in a product, an acknowledgement in the product documentation would be\n *    appreciated but is not required.\n * 2. Altered source versions must be plainly marked as such, and must not be\n *    misrepresented as being the original software.\n * 3. This notice may not be removed or altered from any source distribution.\n *\n */\n\n\n\n/*\n * MINCTEST - Minimal testing library for C\n *\n *\n * Example:\n *\n *      void test1() {\n *           lok('a' == 'a');\n *      }\n *\n *      void test2() {\n *           lequal(5, 6);\n *           lfequal(5.5, 5.6);\n *      }\n *\n *      int main() {\n *           lrun(\"test1\", test1);\n *           lrun(\"test2\", test2);\n *           lresults();\n *           return lfails != 0;\n *      }\n *\n *\n *\n * Hints:\n *      All functions/variables start with the letter 'l'.\n *\n */\n\n\n#ifndef __MINCTEST_H__\n#define __MINCTEST_H__\n\n#include <stdio.h>\n#include <math.h>\n#include <time.h>\n\n\n/* How far apart can floats be before we consider them unequal. */\n#define LTEST_FLOAT_TOLERANCE 0.001\n\n\n/* Track the number of passes, fails. */\n/* NB this is made for all tests to be in one file. */\nstatic int ltests = 0;\nstatic int lfails = 0;\n\n\n/* Display the test results. */\n#define lresults() do {\\\n    if (lfails == 0) {\\\n        printf(\"ALL TESTS PASSED (%d/%d)\\n\", ltests, ltests);\\\n    } else {\\\n        printf(\"SOME TESTS FAILED (%d/%d)\\n\", ltests-lfails, ltests);\\\n    }\\\n} while (0)\n\n\n/* Run a test. Name can be any string to print out, test is the function name to call. */\n#define lrun(name, test) do {\\\n    const int ts = ltests;\\\n    const int fs = lfails;\\\n    const clock_t start = clock();\\\n    printf(\"\\t%-14s\", name);\\\n    test();\\\n    printf(\"pass:%2d   fail:%2d   %4dms\\n\",\\\n            (ltests-ts)-(lfails-fs), lfails-fs,\\\n            (int)((clock() - start) * 1000 / CLOCKS_PER_SEC));\\\n} while (0)\n\n\n/* Assert a true statement. */\n#define lok(test) do {\\\n    ++ltests;\\\n    if (!(test)) {\\\n        ++lfails;\\\n        printf(\"%s:%d error \\n\", __FILE__, __LINE__);\\\n    }} while (0)\n\n\n/* Assert two integers are equal. */\n#define lequal(a, b) do {\\\n    ++ltests;\\\n    if ((a) != (b)) {\\\n        ++lfails;\\\n        printf(\"%s:%d (%d != %d)\\n\", __FILE__, __LINE__, (a), (b));\\\n    }} while (0)\n\n\n/* Assert two floats are equal (Within LTEST_FLOAT_TOLERANCE). */\n#define lfequal(a, b) do {\\\n    ++ltests;\\\n    if (fabs((double)(a)-(double)(b)) > LTEST_FLOAT_TOLERANCE) {\\\n        ++lfails;\\\n        printf(\"%s:%d (%f != %f)\\n\", __FILE__, __LINE__, (double)(a), (double)(b));\\\n    }} while (0)\n\n\n#endif /*__MINCTEST_H__*/\n"
        },
        {
          "name": "test.c",
          "type": "blob",
          "size": 6.435546875,
          "content": "/*\n * GENANN - Minimal C Artificial Neural Network\n *\n * Copyright (c) 2015-2018 Lewis Van Winkle\n *\n * http://CodePlea.com\n *\n * This software is provided 'as-is', without any express or implied\n * warranty. In no event will the authors be held liable for any damages\n * arising from the use of this software.\n *\n * Permission is granted to anyone to use this software for any purpose,\n * including commercial applications, and to alter it and redistribute it\n * freely, subject to the following restrictions:\n *\n * 1. The origin of this software must not be misrepresented; you must not\n *    claim that you wrote the original software. If you use this software\n *    in a product, an acknowledgement in the product documentation would be\n *    appreciated but is not required.\n * 2. Altered source versions must be plainly marked as such, and must not be\n *    misrepresented as being the original software.\n * 3. This notice may not be removed or altered from any source distribution.\n *\n */\n\n#include \"genann.h\"\n#include \"minctest.h\"\n#include <stdio.h>\n#include <math.h>\n#include <stdlib.h>\n\n\n\nvoid basic() {\n    genann *ann = genann_init(1, 0, 0, 1);\n\n    lequal(ann->total_weights, 2);\n    double a;\n\n\n    a = 0;\n    ann->weight[0] = 0;\n    ann->weight[1] = 0;\n    lfequal(0.5, *genann_run(ann, &a));\n\n    a = 1;\n    lfequal(0.5, *genann_run(ann, &a));\n\n    a = 11;\n    lfequal(0.5, *genann_run(ann, &a));\n\n    a = 1;\n    ann->weight[0] = 1;\n    ann->weight[1] = 1;\n    lfequal(0.5, *genann_run(ann, &a));\n\n    a = 10;\n    ann->weight[0] = 1;\n    ann->weight[1] = 1;\n    lfequal(1.0, *genann_run(ann, &a));\n\n    a = -10;\n    lfequal(0.0, *genann_run(ann, &a));\n\n    genann_free(ann);\n}\n\n\nvoid xor() {\n    genann *ann = genann_init(2, 1, 2, 1);\n    ann->activation_hidden = genann_act_threshold;\n    ann->activation_output = genann_act_threshold;\n\n    lequal(ann->total_weights, 9);\n\n    /* First hidden. */\n    ann->weight[0] = .5;\n    ann->weight[1] = 1;\n    ann->weight[2] = 1;\n\n    /* Second hidden. */\n    ann->weight[3] = 1;\n    ann->weight[4] = 1;\n    ann->weight[5] = 1;\n\n    /* Output. */\n    ann->weight[6] = .5;\n    ann->weight[7] = 1;\n    ann->weight[8] = -1;\n\n\n    double input[4][2] = {{0, 0}, {0, 1}, {1, 0}, {1, 1}};\n    double output[4] = {0, 1, 1, 0};\n\n    lfequal(output[0], *genann_run(ann, input[0]));\n    lfequal(output[1], *genann_run(ann, input[1]));\n    lfequal(output[2], *genann_run(ann, input[2]));\n    lfequal(output[3], *genann_run(ann, input[3]));\n\n    genann_free(ann);\n}\n\n\nvoid backprop() {\n    genann *ann = genann_init(1, 0, 0, 1);\n\n    double input, output;\n    input = .5;\n    output = 1;\n\n    double first_try = *genann_run(ann, &input);\n    genann_train(ann, &input, &output, .5);\n    double second_try = *genann_run(ann, &input);\n    lok(fabs(first_try - output) > fabs(second_try - output));\n\n    genann_free(ann);\n}\n\n\nvoid train_and() {\n    double input[4][2] = {{0, 0}, {0, 1}, {1, 0}, {1, 1}};\n    double output[4] = {0, 0, 0, 1};\n\n    genann *ann = genann_init(2, 0, 0, 1);\n\n    int i, j;\n\n    for (i = 0; i < 50; ++i) {\n        for (j = 0; j < 4; ++j) {\n            genann_train(ann, input[j], output + j, .8);\n        }\n    }\n\n    ann->activation_output = genann_act_threshold;\n    lfequal(output[0], *genann_run(ann, input[0]));\n    lfequal(output[1], *genann_run(ann, input[1]));\n    lfequal(output[2], *genann_run(ann, input[2]));\n    lfequal(output[3], *genann_run(ann, input[3]));\n\n    genann_free(ann);\n}\n\n\nvoid train_or() {\n    double input[4][2] = {{0, 0}, {0, 1}, {1, 0}, {1, 1}};\n    double output[4] = {0, 1, 1, 1};\n\n    genann *ann = genann_init(2, 0, 0, 1);\n    genann_randomize(ann);\n\n    int i, j;\n\n    for (i = 0; i < 50; ++i) {\n        for (j = 0; j < 4; ++j) {\n            genann_train(ann, input[j], output + j, .8);\n        }\n    }\n\n    ann->activation_output = genann_act_threshold;\n    lfequal(output[0], *genann_run(ann, input[0]));\n    lfequal(output[1], *genann_run(ann, input[1]));\n    lfequal(output[2], *genann_run(ann, input[2]));\n    lfequal(output[3], *genann_run(ann, input[3]));\n\n    genann_free(ann);\n}\n\n\n\nvoid train_xor() {\n    double input[4][2] = {{0, 0}, {0, 1}, {1, 0}, {1, 1}};\n    double output[4] = {0, 1, 1, 0};\n\n    genann *ann = genann_init(2, 1, 2, 1);\n\n    int i, j;\n\n    for (i = 0; i < 500; ++i) {\n        for (j = 0; j < 4; ++j) {\n            genann_train(ann, input[j], output + j, 3);\n        }\n        /* printf(\"%1.2f \", xor_score(ann)); */\n    }\n\n    ann->activation_output = genann_act_threshold;\n    lfequal(output[0], *genann_run(ann, input[0]));\n    lfequal(output[1], *genann_run(ann, input[1]));\n    lfequal(output[2], *genann_run(ann, input[2]));\n    lfequal(output[3], *genann_run(ann, input[3]));\n\n    genann_free(ann);\n}\n\n\n\nvoid persist() {\n    genann *first = genann_init(1000, 5, 50, 10);\n\n    FILE *out = fopen(\"persist.txt\", \"w\");\n    genann_write(first, out);\n    fclose(out);\n\n\n    FILE *in = fopen(\"persist.txt\", \"r\");\n    genann *second = genann_read(in);\n    fclose(in);\n\n    lequal(first->inputs, second->inputs);\n    lequal(first->hidden_layers, second->hidden_layers);\n    lequal(first->hidden, second->hidden);\n    lequal(first->outputs, second->outputs);\n    lequal(first->total_weights, second->total_weights);\n\n    int i;\n    for (i = 0; i < first->total_weights; ++i) {\n        lok(first->weight[i] == second->weight[i]);\n    }\n\n    genann_free(first);\n    genann_free(second);\n}\n\n\nvoid copy() {\n    genann *first = genann_init(1000, 5, 50, 10);\n\n    genann *second = genann_copy(first);\n\n    lequal(first->inputs, second->inputs);\n    lequal(first->hidden_layers, second->hidden_layers);\n    lequal(first->hidden, second->hidden);\n    lequal(first->outputs, second->outputs);\n    lequal(first->total_weights, second->total_weights);\n\n    int i;\n    for (i = 0; i < first->total_weights; ++i) {\n        lfequal(first->weight[i], second->weight[i]);\n    }\n\n    genann_free(first);\n    genann_free(second);\n}\n\n\nvoid sigmoid() {\n    double i = -20;\n    const double max = 20;\n    const double d = .0001;\n\n    while (i < max) {\n        lfequal(genann_act_sigmoid(NULL, i), genann_act_sigmoid_cached(NULL, i));\n        i += d;\n    }\n}\n\n\nint main(int argc, char *argv[])\n{\n    printf(\"GENANN TEST SUITE\\n\");\n\n    srand(100); //Repeatable test results.\n\n    lrun(\"basic\", basic);\n    lrun(\"xor\", xor);\n    lrun(\"backprop\", backprop);\n    lrun(\"train and\", train_and);\n    lrun(\"train or\", train_or);\n    lrun(\"train xor\", train_xor);\n    lrun(\"persist\", persist);\n    lrun(\"copy\", copy);\n    lrun(\"sigmoid\", sigmoid);\n\n    lresults();\n\n    return lfails != 0;\n}\n"
        }
      ]
    }
  ]
}