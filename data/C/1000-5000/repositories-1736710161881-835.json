{
  "metadata": {
    "timestamp": 1736710161881,
    "page": 835,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjg0MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "sergiomsilva/alpr-unconstrained",
      "stars": 1727,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".editorconfig",
          "type": "blob",
          "size": 0.046875,
          "content": "\n[*.{py,sh}]\nindent_style = tab\nindent_size = 4\n"
        },
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0.02734375,
          "content": "darknet/* linguist-vendored\n"
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.0263671875,
          "content": "*.pyc\n*.a\n*.so\n*.csv\ndata/\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 0.955078125,
          "content": "\nCopyright (C) 2018, Sergio Montazzolli Silva and Claudio Rosito Jung\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program.  If not, see <http://www.gnu.org/licenses/>.\n\nIf you use results produced by our code in any publication,\nplease cite our paper:\n\nSergio Montazzolli Silva and Claudio Rosito Jung,\n\"License Plate Detection and Recognition in Unconstrained Scenarios\",\nIn Proceedings of the European Conference on Computer Vision (ECCV),\npp. 580-596. 2018.  \n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 3.1708984375,
          "content": "# ALPR in Unscontrained Scenarios\n\n## Introduction\n\nThis repository contains the author's implementation of ECCV 2018 paper \"License Plate Detection and Recognition in Unconstrained Scenarios\".\n\n* Paper webpage: http://sergiomsilva.com/pubs/alpr-unconstrained/\n\nIf you use results produced by our code in any publication, please cite our paper:\n\n```\n@INPROCEEDINGS{silva2018a,\n  author={S. M. Silva and C. R. Jung}, \n  booktitle={2018 European Conference on Computer Vision (ECCV)}, \n  title={License Plate Detection and Recognition in Unconstrained Scenarios}, \n  year={2018}, \n  pages={580-596}, \n  doi={10.1007/978-3-030-01258-8_36}, \n  month={Sep},}\n```\n\n## Requirements\n\nIn order to easily run the code, you must have installed the Keras framework with TensorFlow backend. The Darknet framework is self-contained in the \"darknet\" folder and must be compiled before running the tests. To build Darknet just type \"make\" in \"darknet\" folder:\n\n```shellscript\n$ cd darknet && make\n```\n\n**The current version was tested in an Ubuntu 16.04 machine, with Keras 2.2.4, TensorFlow 1.5.0, OpenCV 2.4.9, NumPy 1.14 and Python 2.7.**\n\n## Download Models\n\nAfter building the Darknet framework, you must execute the \"get-networks.sh\" script. This will download all the trained models:\n\n```shellscript\n$ bash get-networks.sh\n```\n\n## Running a simple test\n\nUse the script \"run.sh\" to run our ALPR approach. It requires 3 arguments:\n* __Input directory (-i):__ should contain at least 1 image in JPG or PNG format;\n* __Output directory (-o):__ during the recognition process, many temporary files will be generated inside this directory and erased in the end. The remaining files will be related to the automatic annotated image;\n* __CSV file (-c):__ specify an output CSV file.\n\n```shellscript\n$ bash get-networks.sh && bash run.sh -i samples/test -o /tmp/output -c /tmp/output/results.csv\n```\n\n## Training the LP detector\n\nTo train the LP detector network from scratch, or fine-tuning it for new samples, you can use the train-detector.py script. In folder samples/train-detector there are 3 annotated samples which are used just for demonstration purposes. To correctly reproduce our experiments, this folder must be filled with all the annotations provided in the training set, and their respective images transferred from the original datasets.\n\nThe following command can be used to train the network from scratch considering the data inside the train-detector folder:\n\n```shellscript\n$ mkdir models\n$ python create-model.py eccv models/eccv-model-scracth\n$ python train-detector.py --model models/eccv-model-scracth --name my-trained-model --train-dir samples/train-detector --output-dir models/my-trained-model/ -op Adam -lr .001 -its 300000 -bs 64\n```\n\nFor fine-tunning, use your model with --model option.\n\n## A word on GPU and CPU\n\nWe know that not everyone has an NVIDIA card available, and sometimes it is cumbersome to properly configure CUDA. Thus, we opted to set the Darknet makefile to use CPU as default instead of GPU to favor an easy execution for most people instead of a fast performance. Therefore, the vehicle detection and OCR will be pretty slow. If you want to accelerate them, please edit the Darknet makefile variables to use GPU.\n"
        },
        {
          "name": "annotation-tool.py",
          "type": "blob",
          "size": 10.376953125,
          "content": "'''\t\n\tAnnotation tool\n\n\t\tTool used to annotate labels for LP detection. Main keys:\n\t\t\tC: creates a new shape\n\t\t\tA: creates a new vertex over mouse position\n\t\t\tD: delete last vertex\n\t\t\tS: change position of closest vertex (to mouse pos.)\n\t\t\tX: switch to closest shape\n\t\t\tN or enter: next picture\n\t\t\tP: previous picture\n\t\t\tFor more commands, take a look at the main part...\n\n\t\tUsage:\n\n\t\t\tpython annotation-tool.py max_width max_height file1 ... fileN\n\n\t\tObs. This tool is not fully tested and crashes under unexpetected situations.\n\t\tIf you find any and correct, feel free to send a pull request =)\n\n'''\n\nimport cv2\nimport sys\nimport numpy as np\nimport os\n\nfrom math import cos, sin\nfrom os.path import isfile, splitext, basename, isdir\nfrom os import makedirs\n\nfrom src.utils import image_files_from_folder,getWH\nfrom src.label import Label, Shape, readShapes, writeShapes\nfrom src.projection_utils import perspective_transform, find_T_matrix, getRectPts\n\n\nclass ShapeDisplay(Shape):\n\n\tdef appendSide(self,pt):\n\t\tif (self.max_sides == 0) or (self.pts.shape[1] < self.max_sides):\n\t\t\tself.pts = np.append(self.pts,pt,axis=1)\n\n\tdef removeLast(self):\n\t\tself.pts = self.pts[...,:-1]\n\n\tdef changeClosest(self,pt):\n\t\tidx = np.argmin(self.distanceTo(pt))\n\t\tself.pts[...,idx] = pt[...,0]\n\n\tdef distanceTo(self,pt):\n\t\treturn np.sqrt(np.power((self.pts - pt),2).sum(0))\n\n\tdef shiftPts(self):\n\t\tif self.pts.shape[1] > 1:\n\t\t\tidx = range(1,self.pts.shape[1]) + [0]\n\t\t\tself.pts = self.pts[...,idx]\n\n\tdef getSquare(self):\n\t\ttl,br = self.pts.min(1),self.pts.max(1)\n\t\treturn Label(-1,tl,br)\n\n\tdef draw(self,drawLineFunc,drawCircleFunc,drawTextFunc,color=(255,255,255),txtcolor=(255,255,255)):\n\t\tss = self.pts.shape[1]\n\t\tif ss:\n\t\t\ttext = self.text if len(self.text) else 'NO_LABEL'\n\t\t\tdrawTextFunc(text,self.pts.min(1),color=txtcolor)\n\t\tif ss > 1:\n\t\t\tfor i in range(ss):\n\t\t\t\tdrawLineFunc(self.pts[:,i],self.pts[:,(i+1)%ss],color=color)\n\t\t\t\tdrawCircleFunc(self.pts[:,0] ,color=(255-color[0],0,255-color[2]))\n\t\t\t\tdrawCircleFunc(self.pts[:,-1],color=(255-color[0],255-color[1],255-color[2]))\n\n\ndef rotation_transform(wh,angles=np.array([0.,0.,0.]),zcop=1000., dpp=1000.):\n\trads = np.deg2rad(angles)\n\n\ta = rads[0]; Rx = np.matrix([[1, 0, 0]\t\t\t\t, [0, cos(a), sin(a)]\t, [0, -sin(a), cos(a)]\t])\n\ta = rads[1]; Ry = np.matrix([[cos(a), 0, -sin(a)]\t, [0, 1, 0]\t\t\t\t, [sin(a), 0, cos(a)]\t])\n\ta = rads[2]; Rz = np.matrix([[cos(a), sin(a), 0]\t, [-sin(a), cos(a), 0]\t, [0, 0, 1]\t\t\t\t])\n\n\tR = Rx*Ry*Rz;\n\n\treturn R\n\nclass Display():\n\n\tdef __init__(self,I,width,height,wname='Display'):\n\n\t\tself.Iorig = I.copy()\n\t\tself.width = width\n\t\tself.height = height\n\t\tself.wname = wname\n\n\t\tself.Idisplay = self.Iorig.copy()\n\t\tself.IdisplayCopy = self.Idisplay.copy()\n\n\t\tself.reset_view()\n\t\tself._setPerspective()\n\n\t\tcv2.namedWindow(self.wname)\n\t\tcv2.moveWindow(self.wname, 0, 0)\n\t\tcv2.setMouseCallback(self.wname,self.mouse_callback)\n\n\tdef reset_view(self):\n\t\tself.cx, self.cy = .5,.5\n\t\twh = np.array([self.width,self.height],dtype=float)\n\t\tself.zoom_factor = (wh/getWH(self.Iorig.shape)).min()\n\t\tself.mouse_center = np.array([.5,.5])\n\t\tself.angles = np.array([0.,0.,0.])\n\t\tself._setPerspective()\n\n\tdef updatePerspectiveMatrix(self):\n\t\tzf  = self.zoom_factor\n\t\tw,h = getWH(self.Iorig.shape)\n\n\t\tself.dx = self.cx*w*zf - self.width/2.\n\t\tself.dy = self.cy*h*zf - self.height/2.\n\n\t\tR = np.eye(3)\n\t\tR = np.matmul(R,np.matrix([[zf,0,-self.dx],[0,zf,-self.dy],[0,0,1]],dtype=float))\n\t\tR = np.matmul(R,perspective_transform((w,h),angles=self.angles))\n\n\t\tself.R = R\n\t\tself.Rinv = np.linalg.inv(R)\n\n\tdef show(self):\n\t\tcv2.imshow(self.wname,self.Idisplay)\n\n\tdef setPerspectiveAngle(self,addx=0.,addy=0.,addz=0.):\n\t\tself.angles += np.array([addx,addy,addz])\n\t\tself._setPerspective()\n\n\tdef _setPerspective(self,update=True):\n\t\tif update:\n\t\t\tself.updatePerspectiveMatrix()\n\t\tself.IdisplayCopy = cv2.warpPerspective(self.Iorig,self.R,(self.width,self.height),borderValue=.0,flags=cv2.INTER_LINEAR)\n\n\tdef resetDisplay(self):\n\t\tself.Idisplay = self.IdisplayCopy.copy()\n\n\tdef getMouseCenterRelative(self):\n\t\treturn self.mouse_center.copy().reshape((2,1))\n\n\tdef waitKey(self,time=50):\n\t\treturn cv2.waitKey(50) & 0x0000000FF\n\n\tdef __pt2xy(self,pt):\n\t\tpt = np.squeeze(np.array(np.matmul(self.R,np.append(pt*getWH(self.Iorig.shape),1.))))\n\t\tpt = pt[:2]/pt[2]\n\t\treturn tuple(pt.astype(int).tolist())\n\n\tdef __pts2xys(self,pts):\n\t\tN = pts.shape[1]\n\t\tpts = pts*getWH(self.Iorig.shape).reshape((2,1))\n\t\tpts = np.concatenate((pts,np.ones((1,N))))\n\t\tpts = np.squeeze(np.array(np.matmul(self.R,pts)))\n\t\tpts = pts[:2]/pts[2]\n\t\treturn pts\n\n\tdef drawLine(self,pt1,pt2,color=(255,255,255),thickness=3):\n\t\tpt1 = self.__pt2xy(pt1)\n\t\tpt2 = self.__pt2xy(pt2)\n\t\tcv2.line(self.Idisplay,pt1,pt2,color=color,thickness=thickness)\n\n\tdef drawCircle(self,center,color=(255,255,255),radius=7):\n\t\tcenter = self.__pt2xy(center)\n\t\tcv2.circle(self.Idisplay,center,radius,color,thickness=-1)\n\n\tdef drawText(self,text,bottom_left_pt,color=(255,255,255),bgcolor=(0,0,0),font_size=1):\n\t\tbl_corner = self.__pt2xy(bottom_left_pt)\n\t\tfont = cv2.FONT_HERSHEY_SIMPLEX\n\n\t\twh_text,v = cv2.getTextSize(text, font, font_size, 3)\n\t\ttl_corner = (bl_corner[0],bl_corner[1]-wh_text[1])\n\t\tbr_corner = (bl_corner[0]+wh_text[0],bl_corner[1])\n\n\t\tcv2.rectangle(self.Idisplay, tl_corner, br_corner, bgcolor,-1)\t\n\t\tcv2.putText(self.Idisplay,text,bl_corner,font,font_size,color,3)\n\n\tdef zoom(self,ff):\n\t\tself.zoom_factor *= ff\n\t\tself.cx,self.cy = self.mouse_center.tolist()\n\t\tself._setPerspective()\n\n\tdef rectifyToPts(self,pts):\n\t\t\n\t\tif pts.shape[1] != 4:\n\t\t\treturn\n\n\t\tptsh = pts*getWH(self.Iorig.shape).reshape((2,1))\n\t\tptsh = np.concatenate((ptsh,np.ones((1,4))))\n\n\t\tto_pts = self.__pts2xys(pts)\n\t\twi,hi = (to_pts.min(1)[:2]).tolist()\n\t\twf,hf = (to_pts.max(1)[:2]).tolist()\n\t\tto_pts = np.matrix([[wi,wf,wf,wi],[hi,hi,hf,hf],[1,1,1,1]])\n\n\t\tself.R = find_T_matrix(ptsh,to_pts)\n\t\tself.Rinv = np.linalg.inv(self.R)\n\t\tself._setPerspective(update=False)\n\n\tdef mouse_callback(self,event,x,y,flags,param):\n\t\tmc = np.array([x,y],dtype=float)\n\t\tmc = np.matmul(self.Rinv,np.append(mc,1.))\n\t\tmc = np.squeeze(np.array(mc))\n\t\tself.mouse_center = (mc[:2]/mc[2])/getWH(self.Iorig.shape)\n\ndef selectClosest(shapes,pt):\n\tif len(shapes):\n\t\tmindist,selected = shapes[0].distanceTo(pt).min(),0\n\t\tfor i,shape in enumerate(shapes[1:]):\n\t\t\tshpdist = shape.distanceTo(pt).min()\n\t\t\tif mindist > shpdist:\n\t\t\t\tselected = i+1\n\t\t\t\tmindist = shpdist\n\t\treturn selected\n\telse:\n\t\treturn -1\n\ndef displayAllShapes(disp,shapes,selected,typing_mode):\n\tfor i,shape in enumerate(shapes):\n\t\tcolor = (255,255,255) if i != selected else (0,0,255)\n\t\ttxtcolor = (0,0,255) if (i == selected and typing_mode) else (255,255,255)\n\t\tshape.draw(disp.drawLine,disp.drawCircle,disp.drawText,color=color,txtcolor=txtcolor)\n\n\nif __name__ == '__main__':\n\n\tif len(sys.argv) < 4:\n\t\tprint __doc__\n\t\tsys.exit()\n\n\tmaxW = int(sys.argv[1])\n\tmaxH = int(sys.argv[2])\n\timg_files = sys.argv[3:]\n\n\tmaxwh = np.array([maxW,maxH],dtype=float)\n\twname = 'Display'\n\n\t# Key ids\n\tENTER \t\t\t= 10\n\tESC \t\t\t= 27\n\tBACKSPACE \t\t= 8\n\tARROW_UP\t\t= 82\n\tARROW_DOWN\t\t= 84\n\tARROW_LEFT\t\t= 81\n\tARROW_RIGHT\t\t= 83\n\tGREATER_THAN\t= 46\n\tLESS_THAN \t\t= 44\n\tHOME \t\t\t= 80\n\n\tkey_exit \t\t\t\t\t= ESC\n\tkey_next \t\t\t\t\t= [ord('n'),ENTER]\n\tkey_previous \t\t\t\t= ord('p')\n\tkey_zoom_in\t\t\t\t\t= ord('q')\n\tkey_zoom_out\t\t\t\t= ord('w')\n\tkey_append_vertex \t\t\t= ord('a')\n\tkey_remove_last_vertex  \t= ord('d')\n\tkey_change_closest_vertex\t= ord('s')\n\tkey_create_new_shape \t\t= ord('c')\n\tkey_select_closest_shape \t= ord('x')\n\tkey_shift_pts \t\t\t\t= ord('g')\n\tkey_typing_mode \t\t\t= ord(' ')\n\tkey_delete_selected_shape\t= [ord('r')]\n\n\tkey_pitch_increase \t\t\t= ARROW_DOWN\n\tkey_pitch_decrease \t\t\t= ARROW_UP\n\tkey_yaw_increase \t\t\t= ARROW_LEFT\n\tkey_yaw_decrease\t\t\t= ARROW_RIGHT\n\tkey_roll_increase \t\t\t= GREATER_THAN\n\tkey_roll_decrease \t\t\t= LESS_THAN\n\n\tkey_perspective_reset\t\t= HOME\n\tzoom_factor = 1.\n\n\taction_keys = [key_exit,key_previous] + key_next\n\n\tcurr_image = 0\n\twhile curr_image < len(img_files):\n\n\t\timg_file = img_files[curr_image]\n\n\t\tlab_file = splitext(img_file)[0] + '.txt'\n\n\t\tif isfile(lab_file):\n\t\t\tshapes = readShapes(lab_file,obj_type=ShapeDisplay)\n\t\t\tselected = len(shapes) - 1\n\t\telse:\n\t\t\tshapes,selected = [ShapeDisplay()],0\n\t\t\n\t\tzoom_factor = 1.\n\n\t\tdisp = Display(cv2.imread(img_file),maxW,maxH)\n\t\tdisp.show()\n\t\tkey = disp.waitKey()\n\t\ttyping_mode = False\n\n\t\twhile not key in action_keys:\n\t\t\tdisp.resetDisplay()\n\t\t\tdisplayAllShapes(disp,shapes,selected,typing_mode)\n\t\t\tdisp.show()\n\t\t\tkey = disp.waitKey(10)\n\n\t\t\tif typing_mode:\n\t\t\t\tif key == key_typing_mode:\n\t\t\t\t\ttyping_mode = False\n\t\t\t\telse:\n\t\t\t\t\tif key != 255:\n\t\t\t\t\t\tif key >= 176:\n\t\t\t\t\t\t\tkey = key - 176 + 48\n\t\t\t\t\t\tif key == BACKSPACE: # backspace\n\t\t\t\t\t\t\tshapes[selected].text = shapes[selected].text[:-1]\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tshapes[selected].text += str(chr(key)).upper()\n\t\t\t\tkey = 255\n\t\t\t\tcontinue\n\n\t\t\tif key == key_zoom_in:\n\t\t\t\tdisp.zoom(1.5)\n\t\t\tif key == key_zoom_out:\n\t\t\t\tdisp.zoom(.5)\n\t\t\tif key == key_yaw_increase:\n\t\t\t\tdisp.setPerspectiveAngle(addy=10.)\n\t\t\tif key == key_yaw_decrease:\n\t\t\t\tdisp.setPerspectiveAngle(addy=-10.)\n\t\t\tif key == key_pitch_increase:\n\t\t\t\tdisp.setPerspectiveAngle(addx=10.)\n\t\t\tif key == key_pitch_decrease:\n\t\t\t\tdisp.setPerspectiveAngle(addx=-10.)\n\t\t\tif key == key_roll_increase:\n\t\t\t\tdisp.setPerspectiveAngle(addz=10.)\n\t\t\tif key == key_roll_decrease:\n\t\t\t\tdisp.setPerspectiveAngle(addz=-10.)\n\t\t\tif key == key_perspective_reset:\n\t\t\t\tdisp.reset_view()\n\n\t\t\tif len(shapes):\n\n\t\t\t\tif key == ord('l'):\n\t\t\t\t\tdisp.rectifyToPts(shapes[selected].pts)\n\n\t\t\t\tif key == key_typing_mode:\n\t\t\t\t\ttyping_mode = True\n\n\t\t\t\tif key == key_append_vertex:\n\t\t\t\t\tprint 'Append vertex'\n\t\t\t\t\tshapes[selected].appendSide(disp.getMouseCenterRelative())\n\n\t\t\t\tif key == key_remove_last_vertex:\n\t\t\t\t\tprint 'Remove last vertex'\n\t\t\t\t\tshapes[selected].removeLast()\n\n\t\t\t\tif key == key_change_closest_vertex:\n\t\t\t\t\tprint 'Change closest vertex'\n\t\t\t\t\tshapes[selected].changeClosest(disp.getMouseCenterRelative())\n\n\t\t\t\tif key in key_delete_selected_shape:\n\t\t\t\t\tprint 'Delete closest vertex'\n\t\t\t\t\tdel shapes[selected]\n\t\t\t\t\tpt = disp.getMouseCenterRelative()\n\t\t\t\t\tselected = selectClosest(shapes,pt)\n\n\t\t\t\tif key == key_shift_pts:\n\t\t\t\t\tshapes[selected].shiftPts()\n\n\t\t\tif key == key_create_new_shape:\n\t\t\t\tprint 'Create new shape'\n\t\t\t\tshapes.append(ShapeDisplay())\n\t\t\t\tselected = len(shapes)-1\n\n\t\t\tif key == key_select_closest_shape:\n\t\t\t\tprint 'Select closest'\n\t\t\t\tpt = disp.getMouseCenterRelative()\n\t\t\t\tselected = selectClosest(shapes,pt)\n\t\t\t\t\t\n\t\tif key == key_exit:\n\t\t\tsys.exit()\n\n\t\tif key in ([key_previous] + key_next):\n\t\t\twriteShapes(lab_file,shapes)\n\t\t\tcurr_image += 1 if key in key_next else -1\n\t\t\tcurr_image = max(curr_image,0)\n\t\t\tcontinue\n"
        },
        {
          "name": "create-model.py",
          "type": "blob",
          "size": 2.8837890625,
          "content": "\nimport sys\nimport keras\n\nfrom keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Add, Activation, Concatenate, Input\nfrom keras.models import Model\nfrom keras.applications.mobilenet import MobileNet\n\nfrom src.keras_utils import save_model\n\n\ndef res_block(x,sz,filter_sz=3,in_conv_size=1):\n\txi  = x\n\tfor i in range(in_conv_size):\n\t\txi  = Conv2D(sz, filter_sz, activation='linear', padding='same')(xi)\n\t\txi  = BatchNormalization()(xi)\n\t\txi \t= Activation('relu')(xi)\n\txi  = Conv2D(sz, filter_sz, activation='linear', padding='same')(xi)\n\txi  = BatchNormalization()(xi)\n\txi \t= Add()([xi,x])\n\txi \t= Activation('relu')(xi)\n\treturn xi\n\ndef conv_batch(_input,fsz,csz,activation='relu',padding='same',strides=(1,1)):\n\toutput = Conv2D(fsz, csz, activation='linear', padding=padding, strides=strides)(_input)\n\toutput = BatchNormalization()(output)\n\toutput = Activation(activation)(output)\n\treturn output\n\ndef end_block(x):\n\txprobs    = Conv2D(2, 3, activation='softmax', padding='same')(x)\n\txbbox     = Conv2D(6, 3, activation='linear' , padding='same')(x)\n\treturn Concatenate(3)([xprobs,xbbox])\n\n\ndef create_model_eccv():\n\t\n\tinput_layer = Input(shape=(None,None,3),name='input')\n\n\tx = conv_batch(input_layer, 16, 3)\n\tx = conv_batch(x, 16, 3)\n\tx = MaxPooling2D(pool_size=(2,2))(x)\n\tx = conv_batch(x, 32, 3)\n\tx = res_block(x, 32)\n\tx = MaxPooling2D(pool_size=(2,2))(x)\n\tx = conv_batch(x, 64, 3)\n\tx = res_block(x,64)\n\tx = res_block(x,64)\n\tx = MaxPooling2D(pool_size=(2,2))(x)\n\tx = conv_batch(x, 64, 3)\n\tx = res_block(x,64)\n\tx = res_block(x,64)\n\tx = MaxPooling2D(pool_size=(2,2))(x)\n\tx = conv_batch(x, 128, 3)\n\tx = res_block(x,128)\n\tx = res_block(x,128)\n\tx = res_block(x,128)\n\tx = res_block(x,128)\n\n\tx = end_block(x)\n\n\treturn Model(inputs=input_layer,outputs=x)\n\n\n# Model not converging...\ndef create_model_mobnet():\n\n\tinput_layer = Input(shape=(None,None,3),name='input')\n\tx = input_layer\n\n\tmbnet = MobileNet(input_shape=(224,224,3),include_top=True)\n\t\n\tbackbone = keras.models.clone_model(mbnet)\n\tfor i,bblayer in enumerate(backbone.layers[1:74]):\n\t\tlayer = bblayer.__class__.from_config(bblayer.get_config())\n\t\tlayer.name = 'backbone_' + layer.name\n\t\tx = layer(x)\n\n\tx = end_block(x)\n\n\tmodel = Model(inputs=input_layer,outputs=x)\n\n\tbackbone_layers = {'backbone_' + layer.name: layer for layer in backbone.layers}\n\tfor layer in model.layers:\n\t\tif layer.name in backbone_layers:\n\t\t\tprint 'setting ' + layer.name\n\t\t\tlayer.set_weights(backbone_layers[layer.name].get_weights())\n\n\treturn model\n\n\nif __name__ == '__main__':\n\n\tmodules = [func.replace('create_model_','') for func in dir(sys.modules[__name__]) if 'create_model_' in func]\n\n\tassert sys.argv[1] in modules, \\\n\t\t'Model name must be on of the following: %s' % ', '.join(modules)\n\n\tmodelf = getattr(sys.modules[__name__],'create_model_' + sys.argv[1])\n\t\n\tprint 'Creating model %s' % sys.argv[1]\n\tmodel = modelf()\n\tprint 'Finished'\n\n\tprint 'Saving at %s' % sys.argv[2]\n\tsave_model(model,sys.argv[2])\n\n"
        },
        {
          "name": "darknet",
          "type": "tree",
          "content": null
        },
        {
          "name": "gen-outputs.py",
          "type": "blob",
          "size": 1.4443359375,
          "content": "import sys\nimport cv2\nimport numpy as np\n\nfrom glob\t\t\t\t\t\timport glob\nfrom os.path \t\t\t\t\timport splitext, basename, isfile\nfrom src.utils \t\t\t\t\timport crop_region, image_files_from_folder\nfrom src.drawing_utils\t\t\timport draw_label, draw_losangle, write2img\nfrom src.label \t\t\t\t\timport lread, Label, readShapes\n\nfrom pdb import set_trace as pause\n\n\nYELLOW = (  0,255,255)\nRED    = (  0,  0,255)\n\ninput_dir = sys.argv[1]\noutput_dir = sys.argv[2]\n\nimg_files = image_files_from_folder(input_dir)\n\nfor img_file in img_files:\n\n\tbname = splitext(basename(img_file))[0]\n\n\tI = cv2.imread(img_file)\n\n\tdetected_cars_labels = '%s/%s_cars.txt' % (output_dir,bname)\n\n\tLcar = lread(detected_cars_labels)\n\n\tsys.stdout.write('%s' % bname)\n\n\tif Lcar:\n\n\t\tfor i,lcar in enumerate(Lcar):\n\n\t\t\tdraw_label(I,lcar,color=YELLOW,thickness=3)\n\n\t\t\tlp_label \t\t= '%s/%s_%dcar_lp.txt'\t\t% (output_dir,bname,i)\n\t\t\tlp_label_str \t= '%s/%s_%dcar_lp_str.txt'\t% (output_dir,bname,i)\n\n\t\t\tif isfile(lp_label):\n\n\t\t\t\tLlp_shapes = readShapes(lp_label)\n\t\t\t\tpts = Llp_shapes[0].pts*lcar.wh().reshape(2,1) + lcar.tl().reshape(2,1)\n\t\t\t\tptspx = pts*np.array(I.shape[1::-1],dtype=float).reshape(2,1)\n\t\t\t\tdraw_losangle(I,ptspx,RED,3)\n\n\t\t\t\tif isfile(lp_label_str):\n\t\t\t\t\twith open(lp_label_str,'r') as f:\n\t\t\t\t\t\tlp_str = f.read().strip()\n\t\t\t\t\tllp = Label(0,tl=pts.min(1),br=pts.max(1))\n\t\t\t\t\twrite2img(I,llp,lp_str)\n\n\t\t\t\t\tsys.stdout.write(',%s' % lp_str)\n\n\tcv2.imwrite('%s/%s_output.png' % (output_dir,bname),I)\n\tsys.stdout.write('\\n')\n\n\n"
        },
        {
          "name": "get-networks.sh",
          "type": "blob",
          "size": 1.0458984375,
          "content": "#!/bin/bash\n\nset -e\n\nmkdir data/lp-detector -p\nmkdir data/ocr -p\nmkdir data/vehicle-detector -p\n\nwget -c -N http://sergiomsilva.com/data/eccv2018/lp-detector/wpod-net_update1.h5   -P data/lp-detector/\nwget -c -N http://sergiomsilva.com/data/eccv2018/lp-detector/wpod-net_update1.json -P data/lp-detector/\n\nwget -c -N http://sergiomsilva.com/data/eccv2018/ocr/ocr-net.cfg     -P data/ocr/\nwget -c -N http://sergiomsilva.com/data/eccv2018/ocr/ocr-net.names   -P data/ocr/\nwget -c -N http://sergiomsilva.com/data/eccv2018/ocr/ocr-net.weights -P data/ocr/\nwget -c -N http://sergiomsilva.com/data/eccv2018/ocr/ocr-net.data    -P data/ocr/\n\nwget -c -N http://sergiomsilva.com/data/eccv2018/vehicle-detector/yolo-voc.cfg     -P data/vehicle-detector/\nwget -c -N http://sergiomsilva.com/data/eccv2018/vehicle-detector/voc.data         -P data/vehicle-detector/\nwget -c -N http://sergiomsilva.com/data/eccv2018/vehicle-detector/yolo-voc.weights -P data/vehicle-detector/\nwget -c -N http://sergiomsilva.com/data/eccv2018/vehicle-detector/voc.names        -P data/vehicle-detector/\n"
        },
        {
          "name": "license-plate-detection.py",
          "type": "blob",
          "size": 1.45703125,
          "content": "import sys, os\nimport keras\nimport cv2\nimport traceback\n\nfrom src.keras_utils \t\t\timport load_model\nfrom glob \t\t\t\t\t\timport glob\nfrom os.path \t\t\t\t\timport splitext, basename\nfrom src.utils \t\t\t\t\timport im2single\nfrom src.keras_utils \t\t\timport load_model, detect_lp\nfrom src.label \t\t\t\t\timport Shape, writeShapes\n\n\ndef adjust_pts(pts,lroi):\n\treturn pts*lroi.wh().reshape((2,1)) + lroi.tl().reshape((2,1))\n\n\nif __name__ == '__main__':\n\n\ttry:\n\t\t\n\t\tinput_dir  = sys.argv[1]\n\t\toutput_dir = input_dir\n\n\t\tlp_threshold = .5\n\n\t\twpod_net_path = sys.argv[2]\n\t\twpod_net = load_model(wpod_net_path)\n\n\t\timgs_paths = glob('%s/*car.png' % input_dir)\n\n\t\tprint 'Searching for license plates using WPOD-NET'\n\n\t\tfor i,img_path in enumerate(imgs_paths):\n\n\t\t\tprint '\\t Processing %s' % img_path\n\n\t\t\tbname = splitext(basename(img_path))[0]\n\t\t\tIvehicle = cv2.imread(img_path)\n\n\t\t\tratio = float(max(Ivehicle.shape[:2]))/min(Ivehicle.shape[:2])\n\t\t\tside  = int(ratio*288.)\n\t\t\tbound_dim = min(side + (side%(2**4)),608)\n\t\t\tprint \"\\t\\tBound dim: %d, ratio: %f\" % (bound_dim,ratio)\n\n\t\t\tLlp,LlpImgs,_ = detect_lp(wpod_net,im2single(Ivehicle),bound_dim,2**4,(240,80),lp_threshold)\n\n\t\t\tif len(LlpImgs):\n\t\t\t\tIlp = LlpImgs[0]\n\t\t\t\tIlp = cv2.cvtColor(Ilp, cv2.COLOR_BGR2GRAY)\n\t\t\t\tIlp = cv2.cvtColor(Ilp, cv2.COLOR_GRAY2BGR)\n\n\t\t\t\ts = Shape(Llp[0].pts)\n\n\t\t\t\tcv2.imwrite('%s/%s_lp.png' % (output_dir,bname),Ilp*255.)\n\t\t\t\twriteShapes('%s/%s_lp.txt' % (output_dir,bname),[s])\n\n\texcept:\n\t\ttraceback.print_exc()\n\t\tsys.exit(1)\n\n\tsys.exit(0)\n\n\n"
        },
        {
          "name": "license-plate-ocr.py",
          "type": "blob",
          "size": 1.2783203125,
          "content": "import sys\nimport cv2\nimport numpy as np\nimport traceback\n\nimport darknet.python.darknet as dn\n\nfrom os.path \t\t\t\timport splitext, basename\nfrom glob\t\t\t\t\timport glob\nfrom darknet.python.darknet import detect\nfrom src.label\t\t\t\timport dknet_label_conversion\nfrom src.utils \t\t\t\timport nms\n\n\nif __name__ == '__main__':\n\n\ttry:\n\t\n\t\tinput_dir  = sys.argv[1]\n\t\toutput_dir = input_dir\n\n\t\tocr_threshold = .4\n\n\t\tocr_weights = 'data/ocr/ocr-net.weights'\n\t\tocr_netcfg  = 'data/ocr/ocr-net.cfg'\n\t\tocr_dataset = 'data/ocr/ocr-net.data'\n\n\t\tocr_net  = dn.load_net(ocr_netcfg, ocr_weights, 0)\n\t\tocr_meta = dn.load_meta(ocr_dataset)\n\n\t\timgs_paths = sorted(glob('%s/*lp.png' % output_dir))\n\n\t\tprint 'Performing OCR...'\n\n\t\tfor i,img_path in enumerate(imgs_paths):\n\n\t\t\tprint '\\tScanning %s' % img_path\n\n\t\t\tbname = basename(splitext(img_path)[0])\n\n\t\t\tR,(width,height) = detect(ocr_net, ocr_meta, img_path ,thresh=ocr_threshold, nms=None)\n\n\t\t\tif len(R):\n\n\t\t\t\tL = dknet_label_conversion(R,width,height)\n\t\t\t\tL = nms(L,.45)\n\n\t\t\t\tL.sort(key=lambda x: x.tl()[0])\n\t\t\t\tlp_str = ''.join([chr(l.cl()) for l in L])\n\n\t\t\t\twith open('%s/%s_str.txt' % (output_dir,bname),'w') as f:\n\t\t\t\t\tf.write(lp_str + '\\n')\n\n\t\t\t\tprint '\\t\\tLP: %s' % lp_str\n\n\t\t\telse:\n\n\t\t\t\tprint 'No characters found'\n\n\texcept:\n\t\ttraceback.print_exc()\n\t\tsys.exit(1)\n\n\tsys.exit(0)\n"
        },
        {
          "name": "run.sh",
          "type": "blob",
          "size": 1.9755859375,
          "content": "#!/bin/bash\n\ncheck_file() \n{\n\tif [ ! -f \"$1\" ]\n\tthen\n\t\treturn 0\n\telse\n\t\treturn 1\n\tfi\n}\n\ncheck_dir() \n{\n\tif [ ! -d \"$1\" ]\n\tthen\n\t\treturn 0\n\telse\n\t\treturn 1\n\tfi\n}\n\n\n# Check if Darknet is compiled\ncheck_file \"darknet/libdarknet.so\"\nretval=$?\nif [ $retval -eq 0 ]\nthen\n\techo \"Darknet is not compiled! Go to 'darknet' directory and 'make'!\"\n\texit 1\nfi\n\nlp_model=\"data/lp-detector/wpod-net_update1.h5\"\ninput_dir=''\noutput_dir=''\ncsv_file=''\n\n\n# Check # of arguments\nusage() {\n\techo \"\"\n\techo \" Usage:\"\n\techo \"\"\n\techo \"   bash $0 -i input/dir -o output/dir -c csv_file.csv [-h] [-l path/to/model]:\"\n\techo \"\"\n\techo \"   -i   Input dir path (containing JPG or PNG images)\"\n\techo \"   -o   Output dir path\"\n\techo \"   -c   Output CSV file path\"\n\techo \"   -l   Path to Keras LP detector model (default = $lp_model)\"\n\techo \"   -h   Print this help information\"\n\techo \"\"\n\texit 1\n}\n\nwhile getopts 'i:o:c:l:h' OPTION; do\n\tcase $OPTION in\n\t\ti) input_dir=$OPTARG;;\n\t\to) output_dir=$OPTARG;;\n\t\tc) csv_file=$OPTARG;;\n\t\tl) lp_model=$OPTARG;;\n\t\th) usage;;\n\tesac\ndone\n\nif [ -z \"$input_dir\"  ]; then echo \"Input dir not set.\"; usage; exit 1; fi\nif [ -z \"$output_dir\" ]; then echo \"Ouput dir not set.\"; usage; exit 1; fi\nif [ -z \"$csv_file\"   ]; then echo \"CSV file not set.\" ; usage; exit 1; fi\n\n# Check if input dir exists\ncheck_dir $input_dir\nretval=$?\nif [ $retval -eq 0 ]\nthen\n\techo \"Input directory ($input_dir) does not exist\"\n\texit 1\nfi\n\n# Check if output dir exists, if not, create it\ncheck_dir $output_dir\nretval=$?\nif [ $retval -eq 0 ]\nthen\n\tmkdir -p $output_dir\nfi\n\n# End if any error occur\nset -e\n\n# Detect vehicles\npython vehicle-detection.py $input_dir $output_dir\n\n# Detect license plates\npython license-plate-detection.py $output_dir $lp_model\n\n# OCR\npython license-plate-ocr.py $output_dir\n\n# Draw output and generate list\npython gen-outputs.py $input_dir $output_dir > $csv_file\n\n# Clean files and draw output\nrm $output_dir/*_lp.png\nrm $output_dir/*car.png\nrm $output_dir/*_cars.txt\nrm $output_dir/*_lp.txt\nrm $output_dir/*_str.txt"
        },
        {
          "name": "samples",
          "type": "tree",
          "content": null
        },
        {
          "name": "src",
          "type": "tree",
          "content": null
        },
        {
          "name": "train-detector.py",
          "type": "blob",
          "size": 3.8916015625,
          "content": "\nimport sys\nimport numpy as np\nimport cv2\nimport argparse\nimport keras\n\nfrom random import choice\nfrom os.path import isfile, isdir, basename, splitext\nfrom os import makedirs\n\nfrom src.keras_utils import save_model, load_model\nfrom src.label import readShapes\nfrom src.loss import loss\nfrom src.utils import image_files_from_folder, show\nfrom src.sampler import augment_sample, labels2output_map\nfrom src.data_generator import DataGenerator\n\nfrom pdb import set_trace as pause\n\n\ndef load_network(modelpath,input_dim):\n\n\tmodel = load_model(modelpath)\n\tinput_shape = (input_dim,input_dim,3)\n\n\t# Fixed input size for training\n\tinputs  = keras.layers.Input(shape=(input_dim,input_dim,3))\n\toutputs = model(inputs)\n\n\toutput_shape = tuple([s.value for s in outputs.shape[1:]])\n\toutput_dim   = output_shape[1]\n\tmodel_stride = input_dim / output_dim\n\n\tassert input_dim % output_dim == 0, \\\n\t\t'The output resolution must be divisible by the input resolution'\n\n\tassert model_stride == 2**4, \\\n\t\t'Make sure your model generates a feature map with resolution ' \\\n\t\t'16x smaller than the input'\n\n\treturn model, model_stride, input_shape, output_shape\n\ndef process_data_item(data_item,dim,model_stride):\n\tXX,llp,pts = augment_sample(data_item[0],data_item[1].pts,dim)\n\tYY = labels2output_map(llp,pts,dim,model_stride)\n\treturn XX,YY\n\n\nif __name__ == '__main__':\n\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument('-m' \t\t,'--model'\t\t\t,type=str   , required=True\t\t,help='Path to previous model')\n\tparser.add_argument('-n' \t\t,'--name'\t\t\t,type=str   , required=True\t\t,help='Model name')\n\tparser.add_argument('-tr'\t\t,'--train-dir'\t\t,type=str   , required=True\t\t,help='Input data directory for training')\n\tparser.add_argument('-its'\t\t,'--iterations'\t\t,type=int   , default=300000\t,help='Number of mini-batch iterations (default = 300.000)')\n\tparser.add_argument('-bs'\t\t,'--batch-size'\t\t,type=int   , default=32\t\t,help='Mini-batch size (default = 32)')\n\tparser.add_argument('-od'\t\t,'--output-dir'\t\t,type=str   , default='./'\t\t,help='Output directory (default = ./)')\n\tparser.add_argument('-op'\t\t,'--optimizer'\t\t,type=str   , default='Adam'\t,help='Optmizer (default = Adam)')\n\tparser.add_argument('-lr'\t\t,'--learning-rate'\t,type=float , default=.01\t\t,help='Optmizer (default = 0.01)')\n\targs = parser.parse_args()\n\n\tnetname \t= basename(args.name)\n\ttrain_dir \t= args.train_dir\n\toutdir \t\t= args.output_dir\n\n\titerations \t= args.iterations\n\tbatch_size \t= args.batch_size\n\tdim \t\t= 208\n\n\tif not isdir(outdir):\n\t\tmakedirs(outdir)\n\n\tmodel,model_stride,xshape,yshape = load_network(args.model,dim)\n\n\topt = getattr(keras.optimizers,args.optimizer)(lr=args.learning_rate)\n\tmodel.compile(loss=loss, optimizer=opt)\n\n\tprint 'Checking input directory...'\n\tFiles = image_files_from_folder(train_dir)\n\n\tData = []\n\tfor file in Files:\n\t\tlabfile = splitext(file)[0] + '.txt'\n\t\tif isfile(labfile):\n\t\t\tL = readShapes(labfile)\n\t\t\tI = cv2.imread(file)\n\t\t\tData.append([I,L[0]])\n\n\tprint '%d images with labels found' % len(Data)\n\n\tdg = DataGenerator(\tdata=Data, \\\n\t\t\t\t\t\tprocess_data_item_func=lambda x: process_data_item(x,dim,model_stride),\\\n\t\t\t\t\t\txshape=xshape, \\\n\t\t\t\t\t\tyshape=(yshape[0],yshape[1],yshape[2]+1), \\\n\t\t\t\t\t\tnthreads=2, \\\n\t\t\t\t\t\tpool_size=1000, \\\n\t\t\t\t\t\tmin_nsamples=100 )\n\tdg.start()\n\n\tXtrain = np.empty((batch_size,dim,dim,3),dtype='single')\n\tYtrain = np.empty((batch_size,dim/model_stride,dim/model_stride,2*4+1))\n\n\tmodel_path_backup = '%s/%s_backup' % (outdir,netname)\n\tmodel_path_final  = '%s/%s_final'  % (outdir,netname)\n\n\tfor it in range(iterations):\n\n\t\tprint 'Iter. %d (of %d)' % (it+1,iterations)\n\n\t\tXtrain,Ytrain = dg.get_batch(batch_size)\n\t\ttrain_loss = model.train_on_batch(Xtrain,Ytrain)\n\n\t\tprint '\\tLoss: %f' % train_loss\n\n\t\t# Save model every 1000 iterations\n\t\tif (it+1) % 1000 == 0:\n\t\t\tprint 'Saving model (%s)' % model_path_backup\n\t\t\tsave_model(model,model_path_backup)\n\n\tprint 'Stopping data generator'\n\tdg.stop()\n\n\tprint 'Saving model (%s)' % model_path_final\n\tsave_model(model,model_path_final)\n"
        },
        {
          "name": "vehicle-detection.py",
          "type": "blob",
          "size": 1.7216796875,
          "content": "import sys\nimport cv2\nimport numpy as np\nimport traceback\n\nimport darknet.python.darknet as dn\n\nfrom src.label \t\t\t\timport Label, lwrite\nfrom os.path \t\t\t\timport splitext, basename, isdir\nfrom os \t\t\t\t\timport makedirs\nfrom src.utils \t\t\t\timport crop_region, image_files_from_folder\nfrom darknet.python.darknet import detect\n\n\nif __name__ == '__main__':\n\n\ttry:\n\t\n\t\tinput_dir  = sys.argv[1]\n\t\toutput_dir = sys.argv[2]\n\n\t\tvehicle_threshold = .5\n\n\t\tvehicle_weights = 'data/vehicle-detector/yolo-voc.weights'\n\t\tvehicle_netcfg  = 'data/vehicle-detector/yolo-voc.cfg'\n\t\tvehicle_dataset = 'data/vehicle-detector/voc.data'\n\n\t\tvehicle_net  = dn.load_net(vehicle_netcfg, vehicle_weights, 0)\n\t\tvehicle_meta = dn.load_meta(vehicle_dataset)\n\n\t\timgs_paths = image_files_from_folder(input_dir)\n\t\timgs_paths.sort()\n\n\t\tif not isdir(output_dir):\n\t\t\tmakedirs(output_dir)\n\n\t\tprint 'Searching for vehicles using YOLO...'\n\n\t\tfor i,img_path in enumerate(imgs_paths):\n\n\t\t\tprint '\\tScanning %s' % img_path\n\n\t\t\tbname = basename(splitext(img_path)[0])\n\n\t\t\tR,_ = detect(vehicle_net, vehicle_meta, img_path ,thresh=vehicle_threshold)\n\n\t\t\tR = [r for r in R if r[0] in ['car','bus']]\n\n\t\t\tprint '\\t\\t%d cars found' % len(R)\n\n\t\t\tif len(R):\n\n\t\t\t\tIorig = cv2.imread(img_path)\n\t\t\t\tWH = np.array(Iorig.shape[1::-1],dtype=float)\n\t\t\t\tLcars = []\n\n\t\t\t\tfor i,r in enumerate(R):\n\n\t\t\t\t\tcx,cy,w,h = (np.array(r[2])/np.concatenate( (WH,WH) )).tolist()\n\t\t\t\t\ttl = np.array([cx - w/2., cy - h/2.])\n\t\t\t\t\tbr = np.array([cx + w/2., cy + h/2.])\n\t\t\t\t\tlabel = Label(0,tl,br)\n\t\t\t\t\tIcar = crop_region(Iorig,label)\n\n\t\t\t\t\tLcars.append(label)\n\n\t\t\t\t\tcv2.imwrite('%s/%s_%dcar.png' % (output_dir,bname,i),Icar)\n\n\t\t\t\tlwrite('%s/%s_cars.txt' % (output_dir,bname),Lcars)\n\n\texcept:\n\t\ttraceback.print_exc()\n\t\tsys.exit(1)\n\n\tsys.exit(0)\n\t"
        }
      ]
    }
  ]
}