{
  "metadata": {
    "timestamp": 1736710122162,
    "page": 761,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjc3MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "marella/ctransformers",
      "stars": 1826,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0.0419921875,
          "content": "# Enforce Unix newlines\n* text=auto eol=lf\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 13.189453125,
          "content": "tmp\n*.bin\n\n# Created by https://www.toptal.com/developers/gitignore/api/c++,python,cmake,linux,macos,windows,sublimetext,vim,visualstudio,visualstudiocode\n# Edit at https://www.toptal.com/developers/gitignore?templates=c++,python,cmake,linux,macos,windows,sublimetext,vim,visualstudio,visualstudiocode\n\n### C++ ###\n# Prerequisites\n*.d\n\n# Compiled Object files\n*.slo\n*.lo\n*.o\n*.obj\n\n# Precompiled Headers\n*.gch\n*.pch\n\n# Fortran module files\n*.mod\n*.smod\n\n# Compiled Static libraries\n*.lai\n*.la\n*.a\n*.lib\n\n# Executables\n*.exe\n*.out\n*.app\n\n### CMake ###\nCMakeLists.txt.user\nCMakeCache.txt\nCMakeFiles\nCMakeScripts\nTesting\nMakefile\ncmake_install.cmake\ninstall_manifest.txt\ncompile_commands.json\nCTestTestfile.cmake\n_deps\n\n### CMake Patch ###\n# External projects\n*-prefix/\n\n### Linux ###\n*~\n\n# temporary files which can be created if a process still has a handle open of a deleted file\n.fuse_hidden*\n\n# KDE directory preferences\n.directory\n\n# Linux trash folder which might appear on any partition or disk\n.Trash-*\n\n# .nfs files are created when an open file is removed but is still being accessed\n.nfs*\n\n### macOS ###\n# General\n.DS_Store\n.AppleDouble\n.LSOverride\n\n# Icon must end with two \\r\nIcon\n\n\n# Thumbnails\n._*\n\n# Files that might appear in the root of a volume\n.DocumentRevisions-V100\n.fseventsd\n.Spotlight-V100\n.TemporaryItems\n.Trashes\n.VolumeIcon.icns\n.com.apple.timemachine.donotpresent\n\n# Directories potentially created on remote AFP share\n.AppleDB\n.AppleDesktop\nNetwork Trash Folder\nTemporary Items\n.apdisk\n\n### macOS Patch ###\n# iCloud generated files\n*.icloud\n\n### Python ###\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\ncover/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\n.pybuilder/\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n#   For a library or package, you might want to ignore these files since the code is\n#   intended to run in multiple environments; otherwise, check them in:\n# .python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# poetry\n#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.\n#   This is especially recommended for binary packages to ensure reproducibility, and is more\n#   commonly ignored for libraries.\n#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control\n#poetry.lock\n\n# pdm\n#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.\n#pdm.lock\n#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it\n#   in version control.\n#   https://pdm.fming.dev/#use-with-ide\n.pdm.toml\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n\n# pytype static type analyzer\n.pytype/\n\n# Cython debug symbols\ncython_debug/\n\n# PyCharm\n#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can\n#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore\n#  and can be added to the global gitignore or merged into this file.  For a more nuclear\n#  option (not recommended) you can uncomment the following to ignore the entire idea folder.\n#.idea/\n\n### Python Patch ###\n# Poetry local configuration file - https://python-poetry.org/docs/configuration/#local-configuration\npoetry.toml\n\n# ruff\n.ruff_cache/\n\n# LSP config files\npyrightconfig.json\n\n### SublimeText ###\n# Cache files for Sublime Text\n*.tmlanguage.cache\n*.tmPreferences.cache\n*.stTheme.cache\n\n# Workspace files are user-specific\n*.sublime-workspace\n\n# Project files should be checked into the repository, unless a significant\n# proportion of contributors will probably not be using Sublime Text\n# *.sublime-project\n\n# SFTP configuration file\nsftp-config.json\nsftp-config-alt*.json\n\n# Package control specific files\nPackage Control.last-run\nPackage Control.ca-list\nPackage Control.ca-bundle\nPackage Control.system-ca-bundle\nPackage Control.cache/\nPackage Control.ca-certs/\nPackage Control.merged-ca-bundle\nPackage Control.user-ca-bundle\noscrypto-ca-bundle.crt\nbh_unicode_properties.cache\n\n# Sublime-github package stores a github token in this file\n# https://packagecontrol.io/packages/sublime-github\nGitHub.sublime-settings\n\n### Vim ###\n# Swap\n[._]*.s[a-v][a-z]\n!*.svg  # comment out if you don't need vector files\n[._]*.sw[a-p]\n[._]s[a-rt-v][a-z]\n[._]ss[a-gi-z]\n[._]sw[a-p]\n\n# Session\nSession.vim\nSessionx.vim\n\n# Temporary\n.netrwhist\n# Auto-generated tag files\ntags\n# Persistent undo\n[._]*.un~\n\n### VisualStudioCode ###\n.vscode/*\n!.vscode/settings.json\n!.vscode/tasks.json\n!.vscode/launch.json\n!.vscode/extensions.json\n!.vscode/*.code-snippets\n\n# Local History for Visual Studio Code\n.history/\n\n# Built Visual Studio Code Extensions\n*.vsix\n\n### VisualStudioCode Patch ###\n# Ignore all local history of files\n.history\n.ionide\n\n### Windows ###\n# Windows thumbnail cache files\nThumbs.db\nThumbs.db:encryptable\nehthumbs.db\nehthumbs_vista.db\n\n# Dump file\n*.stackdump\n\n# Folder config file\n[Dd]esktop.ini\n\n# Recycle Bin used on file shares\n$RECYCLE.BIN/\n\n# Windows Installer files\n*.cab\n*.msi\n*.msix\n*.msm\n*.msp\n\n# Windows shortcuts\n*.lnk\n\n### VisualStudio ###\n## Ignore Visual Studio temporary files, build results, and\n## files generated by popular Visual Studio add-ons.\n##\n## Get latest from https://github.com/github/gitignore/blob/main/VisualStudio.gitignore\n\n# User-specific files\n*.rsuser\n*.suo\n*.user\n*.userosscache\n*.sln.docstates\n\n# User-specific files (MonoDevelop/Xamarin Studio)\n*.userprefs\n\n# Mono auto generated files\nmono_crash.*\n\n# Build results\n[Dd]ebug/\n[Dd]ebugPublic/\n[Rr]elease/\n[Rr]eleases/\nx64/\nx86/\n[Ww][Ii][Nn]32/\n[Aa][Rr][Mm]/\n[Aa][Rr][Mm]64/\nbld/\n[Bb]in/\n[Oo]bj/\n[Ll]og/\n[Ll]ogs/\n\n# Visual Studio 2015/2017 cache/options directory\n.vs/\n# Uncomment if you have tasks that create the project's static files in wwwroot\n#wwwroot/\n\n# Visual Studio 2017 auto generated files\nGenerated\\ Files/\n\n# MSTest test Results\n[Tt]est[Rr]esult*/\n[Bb]uild[Ll]og.*\n\n# NUnit\n*.VisualState.xml\nTestResult.xml\nnunit-*.xml\n\n# Build Results of an ATL Project\n[Dd]ebugPS/\n[Rr]eleasePS/\ndlldata.c\n\n# Benchmark Results\nBenchmarkDotNet.Artifacts/\n\n# .NET Core\nproject.lock.json\nproject.fragment.lock.json\nartifacts/\n\n# ASP.NET Scaffolding\nScaffoldingReadMe.txt\n\n# StyleCop\nStyleCopReport.xml\n\n# Files built by Visual Studio\n*_i.c\n*_p.c\n*_h.h\n*.ilk\n*.meta\n*.iobj\n*.pdb\n*.ipdb\n*.pgc\n*.pgd\n*.rsp\n*.sbr\n*.tlb\n*.tli\n*.tlh\n*.tmp\n*.tmp_proj\n*_wpftmp.csproj\n*.tlog\n*.vspscc\n*.vssscc\n.builds\n*.pidb\n*.svclog\n*.scc\n\n# Chutzpah Test files\n_Chutzpah*\n\n# Visual C++ cache files\nipch/\n*.aps\n*.ncb\n*.opendb\n*.opensdf\n*.sdf\n*.cachefile\n*.VC.db\n*.VC.VC.opendb\n\n# Visual Studio profiler\n*.psess\n*.vsp\n*.vspx\n*.sap\n\n# Visual Studio Trace Files\n*.e2e\n\n# TFS 2012 Local Workspace\n$tf/\n\n# Guidance Automation Toolkit\n*.gpState\n\n# ReSharper is a .NET coding add-in\n_ReSharper*/\n*.[Rr]e[Ss]harper\n*.DotSettings.user\n\n# TeamCity is a build add-in\n_TeamCity*\n\n# DotCover is a Code Coverage Tool\n*.dotCover\n\n# AxoCover is a Code Coverage Tool\n.axoCover/*\n!.axoCover/settings.json\n\n# Coverlet is a free, cross platform Code Coverage Tool\ncoverage*.json\ncoverage*.xml\ncoverage*.info\n\n# Visual Studio code coverage results\n*.coverage\n*.coveragexml\n\n# NCrunch\n_NCrunch_*\n.*crunch*.local.xml\nnCrunchTemp_*\n\n# MightyMoose\n*.mm.*\nAutoTest.Net/\n\n# Web workbench (sass)\n.sass-cache/\n\n# Installshield output folder\n[Ee]xpress/\n\n# DocProject is a documentation generator add-in\nDocProject/buildhelp/\nDocProject/Help/*.HxT\nDocProject/Help/*.HxC\nDocProject/Help/*.hhc\nDocProject/Help/*.hhk\nDocProject/Help/*.hhp\nDocProject/Help/Html2\nDocProject/Help/html\n\n# Click-Once directory\npublish/\n\n# Publish Web Output\n*.[Pp]ublish.xml\n*.azurePubxml\n# Note: Comment the next line if you want to checkin your web deploy settings,\n# but database connection strings (with potential passwords) will be unencrypted\n*.pubxml\n*.publishproj\n\n# Microsoft Azure Web App publish settings. Comment the next line if you want to\n# checkin your Azure Web App publish settings, but sensitive information contained\n# in these scripts will be unencrypted\nPublishScripts/\n\n# NuGet Packages\n*.nupkg\n# NuGet Symbol Packages\n*.snupkg\n# The packages folder can be ignored because of Package Restore\n**/[Pp]ackages/*\n# except build/, which is used as an MSBuild target.\n!**/[Pp]ackages/build/\n# Uncomment if necessary however generally it will be regenerated when needed\n#!**/[Pp]ackages/repositories.config\n# NuGet v3's project.json files produces more ignorable files\n*.nuget.props\n*.nuget.targets\n\n# Microsoft Azure Build Output\ncsx/\n*.build.csdef\n\n# Microsoft Azure Emulator\necf/\nrcf/\n\n# Windows Store app package directories and files\nAppPackages/\nBundleArtifacts/\nPackage.StoreAssociation.xml\n_pkginfo.txt\n*.appx\n*.appxbundle\n*.appxupload\n\n# Visual Studio cache files\n# files ending in .cache can be ignored\n*.[Cc]ache\n# but keep track of directories ending in .cache\n!?*.[Cc]ache/\n\n# Others\nClientBin/\n~$*\n*.dbmdl\n*.dbproj.schemaview\n*.jfm\n*.pfx\n*.publishsettings\norleans.codegen.cs\n\n# Including strong name files can present a security risk\n# (https://github.com/github/gitignore/pull/2483#issue-259490424)\n#*.snk\n\n# Since there are multiple workflows, uncomment next line to ignore bower_components\n# (https://github.com/github/gitignore/pull/1529#issuecomment-104372622)\n#bower_components/\n\n# RIA/Silverlight projects\nGenerated_Code/\n\n# Backup & report files from converting an old project file\n# to a newer Visual Studio version. Backup files are not needed,\n# because we have git ;-)\n_UpgradeReport_Files/\nBackup*/\nUpgradeLog*.XML\nUpgradeLog*.htm\nServiceFabricBackup/\n*.rptproj.bak\n\n# SQL Server files\n*.mdf\n*.ldf\n*.ndf\n\n# Business Intelligence projects\n*.rdl.data\n*.bim.layout\n*.bim_*.settings\n*.rptproj.rsuser\n*- [Bb]ackup.rdl\n*- [Bb]ackup ([0-9]).rdl\n*- [Bb]ackup ([0-9][0-9]).rdl\n\n# Microsoft Fakes\nFakesAssemblies/\n\n# GhostDoc plugin setting file\n*.GhostDoc.xml\n\n# Node.js Tools for Visual Studio\n.ntvs_analysis.dat\nnode_modules/\n\n# Visual Studio 6 build log\n*.plg\n\n# Visual Studio 6 workspace options file\n*.opt\n\n# Visual Studio 6 auto-generated workspace file (contains which files were open etc.)\n*.vbw\n\n# Visual Studio 6 auto-generated project file (contains which files were open etc.)\n*.vbp\n\n# Visual Studio 6 workspace and project file (working project files containing files to include in project)\n*.dsw\n*.dsp\n\n# Visual Studio 6 technical files\n\n# Visual Studio LightSwitch build output\n**/*.HTMLClient/GeneratedArtifacts\n**/*.DesktopClient/GeneratedArtifacts\n**/*.DesktopClient/ModelManifest.xml\n**/*.Server/GeneratedArtifacts\n**/*.Server/ModelManifest.xml\n_Pvt_Extensions\n\n# Paket dependency manager\n.paket/paket.exe\npaket-files/\n\n# FAKE - F# Make\n.fake/\n\n# CodeRush personal settings\n.cr/personal\n\n# Python Tools for Visual Studio (PTVS)\n*.pyc\n\n# Cake - Uncomment if you are using it\n# tools/**\n# !tools/packages.config\n\n# Tabs Studio\n*.tss\n\n# Telerik's JustMock configuration file\n*.jmconfig\n\n# BizTalk build output\n*.btp.cs\n*.btm.cs\n*.odx.cs\n*.xsd.cs\n\n# OpenCover UI analysis results\nOpenCover/\n\n# Azure Stream Analytics local run output\nASALocalRun/\n\n# MSBuild Binary and Structured Log\n*.binlog\n\n# NVidia Nsight GPU debugger configuration file\n*.nvuser\n\n# MFractors (Xamarin productivity tool) working folder\n.mfractor/\n\n# Local History for Visual Studio\n.localhistory/\n\n# Visual Studio History (VSHistory) files\n.vshistory/\n\n# BeatPulse healthcheck temp database\nhealthchecksdb\n\n# Backup folder for Package Reference Convert tool in Visual Studio 2017\nMigrationBackup/\n\n# Ionide (cross platform F# VS Code tools) working folder\n.ionide/\n\n# Fody - auto-generated XML schema\nFodyWeavers.xsd\n\n# VS Code files for those working on multiple tools\n*.code-workspace\n\n# Local History for Visual Studio Code\n\n# Windows Installer files from build outputs\n\n# JetBrains Rider\n*.sln.iml\n\n### VisualStudio Patch ###\n# Additional files built by Visual Studio\n\n# End of https://www.toptal.com/developers/gitignore/api/c++,python,cmake,linux,macos,windows,sublimetext,vim,visualstudio,visualstudiocode\n"
        },
        {
          "name": ".gitmodules",
          "type": "blob",
          "size": 0.3251953125,
          "content": "[submodule \"models/ggml\"]\n\tpath = models/submodules/ggml\n\turl = https://github.com/ggerganov/ggml\n[submodule \"models/llama.cpp\"]\n\tpath = models/submodules/llama.cpp\n\turl = https://github.com/ggerganov/llama.cpp\n[submodule \"models/submodules/ggllm.cpp\"]\n\tpath = models/submodules/ggllm.cpp\n\turl = https://github.com/cmp-nct/ggllm.cpp\n"
        },
        {
          "name": "CMakeLists.txt",
          "type": "blob",
          "size": 8.2587890625,
          "content": "cmake_minimum_required(VERSION 3.18)\nproject(ctransformers C CXX)\n\nset(CT_INSTRUCTIONS \"avx2\" CACHE STRING \"avx2 | avx | basic\")\n\noption(CT_CUBLAS \"Use cuBLAS\" OFF)\noption(CT_CUDA_FORCE_DMMV \"use dmmv instead of mmvq CUDA kernels\" OFF)\noption(CT_CUDA_F16 \"use 16 bit floats for some calculations\" OFF)\nset(CT_CUDA_MMQ_Y \"64\" CACHE STRING \"y tile size for mmq CUDA kernels\")\nset(CT_CUDA_DMMV_X \"32\" CACHE STRING \"x stride for dmmv CUDA kernels\")\nset(CT_CUDA_MMV_Y \"1\" CACHE STRING \"y block size for mmv CUDA kernels\")\nset(CT_CUDA_KQUANTS_ITER \"2\" CACHE STRING \"iters/thread per block for Q2_K/Q6_K\")\n\noption(CT_HIPBLAS \"Use hipBLAS\" OFF)\noption(CT_METAL \"Use Metal\" OFF)\n\nmessage(STATUS \"CT_INSTRUCTIONS: ${CT_INSTRUCTIONS}\")\nmessage(STATUS \"CT_CUBLAS: ${CT_CUBLAS}\")\nmessage(STATUS \"CT_HIPBLAS: ${CT_HIPBLAS}\")\nmessage(STATUS \"CT_METAL: ${CT_METAL}\")\n\nset(BUILD_SHARED_LIBS ON)\nset(CMAKE_WINDOWS_EXPORT_ALL_SYMBOLS ON)\nset(CMAKE_LIBRARY_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}/lib/$<0:>)\nset(CMAKE_RUNTIME_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}/lib/$<0:>)\n\n# Compile Flags\n\nset(CMAKE_C_STANDARD 11)\nset(CMAKE_C_STANDARD_REQUIRED ON)\nset(CMAKE_CXX_STANDARD 11)\nset(CMAKE_CXX_STANDARD_REQUIRED ON)\nset(THREADS_PREFER_PTHREAD_FLAG ON)\nfind_package(Threads REQUIRED)\n\nif (NOT CMAKE_BUILD_TYPE AND NOT CMAKE_CONFIGURATION_TYPES)\n    set(CMAKE_BUILD_TYPE Release CACHE STRING \"Build type\" FORCE)\n    set_property(CACHE CMAKE_BUILD_TYPE PROPERTY STRINGS \"Debug\" \"Release\" \"RelWithDebInfo\")\nendif()\n\nif (NOT MSVC)\n    set(c_flags\n        -Wall\n        -Wextra\n        -Wpedantic\n        -Wcast-qual\n        -Wdouble-promotion\n        -Wshadow\n        -Wstrict-prototypes\n        -Wpointer-arith\n    )\n    set(cxx_flags\n        # TODO(marella): Add other warnings.\n        # -Wall\n        -Wextra\n        -Wpedantic\n        -Wcast-qual\n        -Wno-unused-function\n        -Wno-multichar\n    )\nendif()\n\nadd_compile_options(\n    \"$<$<COMPILE_LANGUAGE:C>:${c_flags}>\"\n    \"$<$<COMPILE_LANGUAGE:CXX>:${cxx_flags}>\"\n)\n\n# Architecture Flags\n\nif (${CMAKE_SYSTEM_PROCESSOR} MATCHES \"arm\" OR ${CMAKE_SYSTEM_PROCESSOR} MATCHES \"aarch64\")\n    message(STATUS \"ARM detected\")\n    if (NOT MSVC)\n        add_compile_options(-mcpu=native)\n    endif()\nelse()\n    message(STATUS \"x86 detected\")\n    if (APPLE AND CT_INSTRUCTIONS STREQUAL \"basic\")\n        # Universal binary.\n        set(CMAKE_OSX_ARCHITECTURES \"arm64;x86_64\" CACHE STRING \"\" FORCE)\n    endif()\n\n    if (MSVC)\n        if (CT_INSTRUCTIONS STREQUAL \"avx2\")\n            add_compile_options($<$<COMPILE_LANGUAGE:C>:/arch:AVX2>)\n            add_compile_options($<$<COMPILE_LANGUAGE:CXX>:/arch:AVX2>)\n        elseif (CT_INSTRUCTIONS STREQUAL \"avx\")\n            add_compile_options($<$<COMPILE_LANGUAGE:C>:/arch:AVX>)\n            add_compile_options($<$<COMPILE_LANGUAGE:CXX>:/arch:AVX>)\n        endif()\n    else()\n        if (CT_INSTRUCTIONS STREQUAL \"avx2\")\n            add_compile_options(-mfma -mavx2)\n            add_compile_options(-mf16c -mavx)\n        elseif (CT_INSTRUCTIONS STREQUAL \"avx\")\n            add_compile_options(-mf16c -mavx)\n        endif()\n    endif()\nendif()\n\nif (CT_CUBLAS)\n    if (NOT DEFINED CMAKE_CUDA_ARCHITECTURES)\n        # 52 == lowest CUDA 12 standard\n        # 60 == f16 CUDA intrinsics\n        # 61 == integer CUDA intrinsics\n        # 70 == compute capability at which unrolling a loop in mul_mat_q kernels is faster\n        if (CT_CUDA_F16)\n            set(CMAKE_CUDA_ARCHITECTURES \"60;61;70\") # needed for f16 CUDA intrinsics\n        else()\n            set(CMAKE_CUDA_ARCHITECTURES \"52;61;70\") # lowest CUDA 12 standard + lowest for integer intrinsics\n        endif()\n    endif()\n    message(STATUS \"Using CUDA architectures: ${CMAKE_CUDA_ARCHITECTURES}\")\nendif()\n\n# Library\n\nadd_library(\n    ctransformers SHARED\n    models/llm.cc\n    models/ggml/ggml.c\n    models/ggml/ggml-alloc.c\n    models/ggml/k_quants.c\n    models/ggml/cmpnct_unicode.cpp\n)\n\ntarget_include_directories(ctransformers PRIVATE models)\ntarget_link_libraries(ctransformers PRIVATE Threads::Threads)\nset_target_properties(ctransformers PROPERTIES POSITION_INDEPENDENT_CODE ON)\ntarget_compile_definitions(ctransformers PRIVATE GGML_USE_K_QUANTS)\n\nif (APPLE)\n    find_library(ACCELERATE_FRAMEWORK Accelerate)\n    if (ACCELERATE_FRAMEWORK)\n        message(STATUS \"Accelerate framework found\")\n        target_link_libraries(ctransformers PRIVATE ${ACCELERATE_FRAMEWORK})\n        target_compile_definitions(ctransformers PRIVATE GGML_USE_ACCELERATE)\n    else()\n        message(WARNING \"Accelerate framework not found\")\n    endif()\nendif()\n\nif (CT_CUBLAS)\n    find_package(CUDAToolkit)\n    if (CUDAToolkit_FOUND)\n        message(STATUS \"cuBLAS found\")\n        enable_language(CUDA)\n\n        target_sources(ctransformers PRIVATE models/ggml/ggml-cuda.cu)\n        target_link_libraries(ctransformers PRIVATE CUDA::cudart CUDA::cublas CUDA::cublasLt)\n\n        target_compile_definitions(ctransformers PRIVATE GGML_USE_CUBLAS)\n        target_compile_definitions(ctransformers PRIVATE GGML_CUDA_MMQ_Y=${CT_CUDA_MMQ_Y})\n        target_compile_definitions(ctransformers PRIVATE GGML_CUDA_DMMV_X=${CT_CUDA_DMMV_X})\n        target_compile_definitions(ctransformers PRIVATE GGML_CUDA_MMV_Y=${CT_CUDA_MMV_Y})\n        target_compile_definitions(ctransformers PRIVATE K_QUANTS_PER_ITERATION=${CT_CUDA_KQUANTS_ITER})\n        if (CT_CUDA_FORCE_DMMV)\n            target_compile_definitions(ctransformers PRIVATE GGML_CUDA_FORCE_DMMV)\n        endif()\n        if (CT_CUDA_F16)\n            target_compile_definitions(ctransformers PRIVATE GGML_CUDA_F16)\n        endif()\n    else()\n        message(WARNING \"cuBLAS not found\")\n    endif()\nendif()\n\nif (CT_HIPBLAS)\n    list(APPEND CMAKE_PREFIX_PATH /opt/rocm)\n\n    if (NOT ${CMAKE_C_COMPILER_ID} MATCHES \"Clang\")\n        message(WARNING \"Only LLVM is supported for HIP, hint: CC=/opt/rocm/llvm/bin/clang\")\n    endif()\n    if (NOT ${CMAKE_CXX_COMPILER_ID} MATCHES \"Clang\")\n        message(WARNING \"Only LLVM is supported for HIP, hint: CXX=/opt/rocm/llvm/bin/clang++\")\n    endif()\n\n    find_package(hip)\n    find_package(hipblas)\n    find_package(rocblas)\n\n    if (${hipblas_FOUND} AND ${hip_FOUND})\n        message(STATUS \"HIP and hipBLAS found\")\n        set_source_files_properties(models/ggml/ggml-cuda.cu PROPERTIES LANGUAGE CXX)\n\n        target_sources(ctransformers PRIVATE models/ggml/ggml-cuda.cu)\n        target_link_libraries(ctransformers PRIVATE hip::device hip::host roc::rocblas roc::hipblas)\n\n        target_compile_definitions(ctransformers PRIVATE GGML_USE_HIPBLAS)\n        target_compile_definitions(ctransformers PRIVATE GGML_USE_CUBLAS)\n        target_compile_definitions(ctransformers PRIVATE GGML_CUDA_MMQ_Y=${CT_CUDA_MMQ_Y})\n        target_compile_definitions(ctransformers PRIVATE GGML_CUDA_DMMV_X=${CT_CUDA_DMMV_X})\n        target_compile_definitions(ctransformers PRIVATE GGML_CUDA_MMV_Y=${CT_CUDA_MMV_Y})\n        target_compile_definitions(ctransformers PRIVATE K_QUANTS_PER_ITERATION=${CT_CUDA_KQUANTS_ITER})\n        target_compile_definitions(ctransformers PRIVATE CC_TURING=1000000000)\n        if (CT_CUDA_FORCE_DMMV)\n            target_compile_definitions(ctransformers PRIVATE GGML_CUDA_FORCE_DMMV)\n        endif()\n    else()\n        message(WARNING \"hipBLAS or HIP not found. Try setting CMAKE_PREFIX_PATH=/opt/rocm\")\n    endif()\nendif()\n\nif (CT_METAL)\n    find_library(FOUNDATION_LIBRARY         Foundation              REQUIRED)\n    find_library(METAL_FRAMEWORK            Metal                   REQUIRED)\n    find_library(METALKIT_FRAMEWORK         MetalKit                REQUIRED)\n    find_library(METALPERFORMANCE_FRAMEWORK MetalPerformanceShaders REQUIRED)\n\n    target_sources(ctransformers PRIVATE models/ggml/ggml-metal.m)\n    target_link_libraries(\n        ctransformers PRIVATE\n        ${FOUNDATION_LIBRARY}\n        ${METAL_FRAMEWORK}\n        ${METALKIT_FRAMEWORK}\n        ${METALPERFORMANCE_FRAMEWORK}\n    )\n\n    target_compile_definitions(ctransformers PRIVATE GGML_USE_METAL)\n    target_compile_definitions(ctransformers PRIVATE GGML_METAL_NDEBUG)\n\n    set_target_properties(ctransformers PROPERTIES RESOURCE \"${CMAKE_CURRENT_SOURCE_DIR}/models/ggml/ggml-metal.metal\")\n    configure_file(models/ggml/ggml-metal.metal lib/ggml-metal.metal COPYONLY)\nendif()\n\n# scikit-build\n\ninstall(\n    TARGETS ctransformers\n    LIBRARY DESTINATION ctransformers/lib/local\n    RUNTIME DESTINATION ctransformers/lib/local\n    RESOURCE DESTINATION ctransformers/lib/local\n)\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.052734375,
          "content": "The MIT License (MIT)\n\nCopyright (c) Ravindra Marella\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 15.8466796875,
          "content": "# [CTransformers](https://github.com/marella/ctransformers) [![PyPI](https://img.shields.io/pypi/v/ctransformers)](https://pypi.org/project/ctransformers/) [![tests](https://github.com/marella/ctransformers/actions/workflows/tests.yml/badge.svg)](https://github.com/marella/ctransformers/actions/workflows/tests.yml) [![build](https://github.com/marella/ctransformers/actions/workflows/build.yml/badge.svg)](https://github.com/marella/ctransformers/actions/workflows/build.yml)\n\nPython bindings for the Transformer models implemented in C/C++ using [GGML](https://github.com/ggerganov/ggml) library.\n\n> Also see [ChatDocs](https://github.com/marella/chatdocs)\n\n- [Supported Models](#supported-models)\n- [Installation](#installation)\n- [Usage](#usage)\n  - [🤗 Transformers](#transformers)\n  - [LangChain](#langchain)\n  - [GPU](#gpu)\n  - [GPTQ](#gptq)\n- [Documentation](#documentation)\n- [License](#license)\n\n## Supported Models\n\n| Models              | Model Type    | CUDA | Metal |\n| :------------------ | ------------- | :--: | :---: |\n| GPT-2               | `gpt2`        |      |       |\n| GPT-J, GPT4All-J    | `gptj`        |      |       |\n| GPT-NeoX, StableLM  | `gpt_neox`    |      |       |\n| Falcon              | `falcon`      |  ✅  |       |\n| LLaMA, LLaMA 2      | `llama`       |  ✅  |  ✅   |\n| MPT                 | `mpt`         |  ✅  |       |\n| StarCoder, StarChat | `gpt_bigcode` |  ✅  |       |\n| Dolly V2            | `dolly-v2`    |      |       |\n| Replit              | `replit`      |      |       |\n\n## Installation\n\n```sh\npip install ctransformers\n```\n\n## Usage\n\nIt provides a unified interface for all models:\n\n```py\nfrom ctransformers import AutoModelForCausalLM\n\nllm = AutoModelForCausalLM.from_pretrained(\"/path/to/ggml-model.bin\", model_type=\"gpt2\")\n\nprint(llm(\"AI is going to\"))\n```\n\n[Run in Google Colab](https://colab.research.google.com/drive/1GMhYMUAv_TyZkpfvUI1NirM8-9mCXQyL)\n\nTo stream the output, set `stream=True`:\n\n```py\nfor text in llm(\"AI is going to\", stream=True):\n    print(text, end=\"\", flush=True)\n```\n\nYou can load models from Hugging Face Hub directly:\n\n```py\nllm = AutoModelForCausalLM.from_pretrained(\"marella/gpt-2-ggml\")\n```\n\nIf a model repo has multiple model files (`.bin` or `.gguf` files), specify a model file using:\n\n```py\nllm = AutoModelForCausalLM.from_pretrained(\"marella/gpt-2-ggml\", model_file=\"ggml-model.bin\")\n```\n\n<a id=\"transformers\"></a>\n\n### 🤗 Transformers\n\n> **Note:** This is an experimental feature and may change in the future.\n\nTo use it with 🤗 Transformers, create model and tokenizer using:\n\n```py\nfrom ctransformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"marella/gpt-2-ggml\", hf=True)\ntokenizer = AutoTokenizer.from_pretrained(model)\n```\n\n[Run in Google Colab](https://colab.research.google.com/drive/1FVSLfTJ2iBbQ1oU2Rqz0MkpJbaB_5Got)\n\nYou can use 🤗 Transformers text generation pipeline:\n\n```py\nfrom transformers import pipeline\n\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\nprint(pipe(\"AI is going to\", max_new_tokens=256))\n```\n\nYou can use 🤗 Transformers generation [parameters](https://huggingface.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationConfig):\n\n```py\npipe(\"AI is going to\", max_new_tokens=256, do_sample=True, temperature=0.8, repetition_penalty=1.1)\n```\n\nYou can use 🤗 Transformers tokenizers:\n\n```py\nfrom ctransformers import AutoModelForCausalLM\nfrom transformers import AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"marella/gpt-2-ggml\", hf=True)  # Load model from GGML model repo.\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")  # Load tokenizer from original model repo.\n```\n\n### LangChain\n\nIt is integrated into LangChain. See [LangChain docs](https://python.langchain.com/docs/ecosystem/integrations/ctransformers).\n\n### GPU\n\nTo run some of the model layers on GPU, set the `gpu_layers` parameter:\n\n```py\nllm = AutoModelForCausalLM.from_pretrained(\"TheBloke/Llama-2-7B-GGML\", gpu_layers=50)\n```\n\n[Run in Google Colab](https://colab.research.google.com/drive/1Ihn7iPCYiqlTotpkqa1tOhUIpJBrJ1Tp)\n\n#### CUDA\n\nInstall CUDA libraries using:\n\n```sh\npip install ctransformers[cuda]\n```\n\n#### ROCm\n\nTo enable ROCm support, install the `ctransformers` package using:\n\n```sh\nCT_HIPBLAS=1 pip install ctransformers --no-binary ctransformers\n```\n\n#### Metal\n\nTo enable Metal support, install the `ctransformers` package using:\n\n```sh\nCT_METAL=1 pip install ctransformers --no-binary ctransformers\n```\n\n### GPTQ\n\n> **Note:** This is an experimental feature and only LLaMA models are supported using [ExLlama](https://github.com/turboderp/exllama).\n\nInstall additional dependencies using:\n\n```sh\npip install ctransformers[gptq]\n```\n\nLoad a GPTQ model using:\n\n```py\nllm = AutoModelForCausalLM.from_pretrained(\"TheBloke/Llama-2-7B-GPTQ\")\n```\n\n[Run in Google Colab](https://colab.research.google.com/drive/1SzHslJ4CiycMOgrppqecj4VYCWFnyrN0)\n\n> If model name or path doesn't contain the word `gptq` then specify `model_type=\"gptq\"`.\n\nIt can also be used with LangChain. Low-level APIs are not fully supported.\n\n## Documentation\n\n<!-- API_DOCS -->\n\n### Config\n\n| Parameter            | Type        | Description                                                     | Default |\n| :------------------- | :---------- | :-------------------------------------------------------------- | :------ |\n| `top_k`              | `int`       | The top-k value to use for sampling.                            | `40`    |\n| `top_p`              | `float`     | The top-p value to use for sampling.                            | `0.95`  |\n| `temperature`        | `float`     | The temperature to use for sampling.                            | `0.8`   |\n| `repetition_penalty` | `float`     | The repetition penalty to use for sampling.                     | `1.1`   |\n| `last_n_tokens`      | `int`       | The number of last tokens to use for repetition penalty.        | `64`    |\n| `seed`               | `int`       | The seed value to use for sampling tokens.                      | `-1`    |\n| `max_new_tokens`     | `int`       | The maximum number of new tokens to generate.                   | `256`   |\n| `stop`               | `List[str]` | A list of sequences to stop generation when encountered.        | `None`  |\n| `stream`             | `bool`      | Whether to stream the generated text.                           | `False` |\n| `reset`              | `bool`      | Whether to reset the model state before generating text.        | `True`  |\n| `batch_size`         | `int`       | The batch size to use for evaluating tokens in a single prompt. | `8`     |\n| `threads`            | `int`       | The number of threads to use for evaluating tokens.             | `-1`    |\n| `context_length`     | `int`       | The maximum context length to use.                              | `-1`    |\n| `gpu_layers`         | `int`       | The number of layers to run on GPU.                             | `0`     |\n\n> **Note:** Currently only LLaMA, MPT and Falcon models support the `context_length` parameter.\n\n### <kbd>class</kbd> `AutoModelForCausalLM`\n\n---\n\n#### <kbd>classmethod</kbd> `AutoModelForCausalLM.from_pretrained`\n\n```python\nfrom_pretrained(\n    model_path_or_repo_id: str,\n    model_type: Optional[str] = None,\n    model_file: Optional[str] = None,\n    config: Optional[ctransformers.hub.AutoConfig] = None,\n    lib: Optional[str] = None,\n    local_files_only: bool = False,\n    revision: Optional[str] = None,\n    hf: bool = False,\n    **kwargs\n) → LLM\n```\n\nLoads the language model from a local file or remote repo.\n\n**Args:**\n\n- <b>`model_path_or_repo_id`</b>: The path to a model file or directory or the name of a Hugging Face Hub model repo.\n- <b>`model_type`</b>: The model type.\n- <b>`model_file`</b>: The name of the model file in repo or directory.\n- <b>`config`</b>: `AutoConfig` object.\n- <b>`lib`</b>: The path to a shared library or one of `avx2`, `avx`, `basic`.\n- <b>`local_files_only`</b>: Whether or not to only look at local files (i.e., do not try to download the model).\n- <b>`revision`</b>: The specific model version to use. It can be a branch name, a tag name, or a commit id.\n- <b>`hf`</b>: Whether to create a Hugging Face Transformers model.\n\n**Returns:**\n`LLM` object.\n\n### <kbd>class</kbd> `LLM`\n\n### <kbd>method</kbd> `LLM.__init__`\n\n```python\n__init__(\n    model_path: str,\n    model_type: Optional[str] = None,\n    config: Optional[ctransformers.llm.Config] = None,\n    lib: Optional[str] = None\n)\n```\n\nLoads the language model from a local file.\n\n**Args:**\n\n- <b>`model_path`</b>: The path to a model file.\n- <b>`model_type`</b>: The model type.\n- <b>`config`</b>: `Config` object.\n- <b>`lib`</b>: The path to a shared library or one of `avx2`, `avx`, `basic`.\n\n---\n\n##### <kbd>property</kbd> LLM.bos_token_id\n\nThe beginning-of-sequence token.\n\n---\n\n##### <kbd>property</kbd> LLM.config\n\nThe config object.\n\n---\n\n##### <kbd>property</kbd> LLM.context_length\n\nThe context length of model.\n\n---\n\n##### <kbd>property</kbd> LLM.embeddings\n\nThe input embeddings.\n\n---\n\n##### <kbd>property</kbd> LLM.eos_token_id\n\nThe end-of-sequence token.\n\n---\n\n##### <kbd>property</kbd> LLM.logits\n\nThe unnormalized log probabilities.\n\n---\n\n##### <kbd>property</kbd> LLM.model_path\n\nThe path to the model file.\n\n---\n\n##### <kbd>property</kbd> LLM.model_type\n\nThe model type.\n\n---\n\n##### <kbd>property</kbd> LLM.pad_token_id\n\nThe padding token.\n\n---\n\n##### <kbd>property</kbd> LLM.vocab_size\n\nThe number of tokens in vocabulary.\n\n---\n\n#### <kbd>method</kbd> `LLM.detokenize`\n\n```python\ndetokenize(tokens: Sequence[int], decode: bool = True) → Union[str, bytes]\n```\n\nConverts a list of tokens to text.\n\n**Args:**\n\n- <b>`tokens`</b>: The list of tokens.\n- <b>`decode`</b>: Whether to decode the text as UTF-8 string.\n\n**Returns:**\nThe combined text of all tokens.\n\n---\n\n#### <kbd>method</kbd> `LLM.embed`\n\n```python\nembed(\n    input: Union[str, Sequence[int]],\n    batch_size: Optional[int] = None,\n    threads: Optional[int] = None\n) → List[float]\n```\n\nComputes embeddings for a text or list of tokens.\n\n> **Note:** Currently only LLaMA and Falcon models support embeddings.\n\n**Args:**\n\n- <b>`input`</b>: The input text or list of tokens to get embeddings for.\n- <b>`batch_size`</b>: The batch size to use for evaluating tokens in a single prompt. Default: `8`\n- <b>`threads`</b>: The number of threads to use for evaluating tokens. Default: `-1`\n\n**Returns:**\nThe input embeddings.\n\n---\n\n#### <kbd>method</kbd> `LLM.eval`\n\n```python\neval(\n    tokens: Sequence[int],\n    batch_size: Optional[int] = None,\n    threads: Optional[int] = None\n) → None\n```\n\nEvaluates a list of tokens.\n\n**Args:**\n\n- <b>`tokens`</b>: The list of tokens to evaluate.\n- <b>`batch_size`</b>: The batch size to use for evaluating tokens in a single prompt. Default: `8`\n- <b>`threads`</b>: The number of threads to use for evaluating tokens. Default: `-1`\n\n---\n\n#### <kbd>method</kbd> `LLM.generate`\n\n```python\ngenerate(\n    tokens: Sequence[int],\n    top_k: Optional[int] = None,\n    top_p: Optional[float] = None,\n    temperature: Optional[float] = None,\n    repetition_penalty: Optional[float] = None,\n    last_n_tokens: Optional[int] = None,\n    seed: Optional[int] = None,\n    batch_size: Optional[int] = None,\n    threads: Optional[int] = None,\n    reset: Optional[bool] = None\n) → Generator[int, NoneType, NoneType]\n```\n\nGenerates new tokens from a list of tokens.\n\n**Args:**\n\n- <b>`tokens`</b>: The list of tokens to generate tokens from.\n- <b>`top_k`</b>: The top-k value to use for sampling. Default: `40`\n- <b>`top_p`</b>: The top-p value to use for sampling. Default: `0.95`\n- <b>`temperature`</b>: The temperature to use for sampling. Default: `0.8`\n- <b>`repetition_penalty`</b>: The repetition penalty to use for sampling. Default: `1.1`\n- <b>`last_n_tokens`</b>: The number of last tokens to use for repetition penalty. Default: `64`\n- <b>`seed`</b>: The seed value to use for sampling tokens. Default: `-1`\n- <b>`batch_size`</b>: The batch size to use for evaluating tokens in a single prompt. Default: `8`\n- <b>`threads`</b>: The number of threads to use for evaluating tokens. Default: `-1`\n- <b>`reset`</b>: Whether to reset the model state before generating text. Default: `True`\n\n**Returns:**\nThe generated tokens.\n\n---\n\n#### <kbd>method</kbd> `LLM.is_eos_token`\n\n```python\nis_eos_token(token: int) → bool\n```\n\nChecks if a token is an end-of-sequence token.\n\n**Args:**\n\n- <b>`token`</b>: The token to check.\n\n**Returns:**\n`True` if the token is an end-of-sequence token else `False`.\n\n---\n\n#### <kbd>method</kbd> `LLM.prepare_inputs_for_generation`\n\n```python\nprepare_inputs_for_generation(\n    tokens: Sequence[int],\n    reset: Optional[bool] = None\n) → Sequence[int]\n```\n\nRemoves input tokens that are evaluated in the past and updates the LLM context.\n\n**Args:**\n\n- <b>`tokens`</b>: The list of input tokens.\n- <b>`reset`</b>: Whether to reset the model state before generating text. Default: `True`\n\n**Returns:**\nThe list of tokens to evaluate.\n\n---\n\n#### <kbd>method</kbd> `LLM.reset`\n\n```python\nreset() → None\n```\n\nDeprecated since 0.2.27.\n\n---\n\n#### <kbd>method</kbd> `LLM.sample`\n\n```python\nsample(\n    top_k: Optional[int] = None,\n    top_p: Optional[float] = None,\n    temperature: Optional[float] = None,\n    repetition_penalty: Optional[float] = None,\n    last_n_tokens: Optional[int] = None,\n    seed: Optional[int] = None\n) → int\n```\n\nSamples a token from the model.\n\n**Args:**\n\n- <b>`top_k`</b>: The top-k value to use for sampling. Default: `40`\n- <b>`top_p`</b>: The top-p value to use for sampling. Default: `0.95`\n- <b>`temperature`</b>: The temperature to use for sampling. Default: `0.8`\n- <b>`repetition_penalty`</b>: The repetition penalty to use for sampling. Default: `1.1`\n- <b>`last_n_tokens`</b>: The number of last tokens to use for repetition penalty. Default: `64`\n- <b>`seed`</b>: The seed value to use for sampling tokens. Default: `-1`\n\n**Returns:**\nThe sampled token.\n\n---\n\n#### <kbd>method</kbd> `LLM.tokenize`\n\n```python\ntokenize(text: str, add_bos_token: Optional[bool] = None) → List[int]\n```\n\nConverts a text into list of tokens.\n\n**Args:**\n\n- <b>`text`</b>: The text to tokenize.\n- <b>`add_bos_token`</b>: Whether to add the beginning-of-sequence token.\n\n**Returns:**\nThe list of tokens.\n\n---\n\n#### <kbd>method</kbd> `LLM.__call__`\n\n```python\n__call__(\n    prompt: str,\n    max_new_tokens: Optional[int] = None,\n    top_k: Optional[int] = None,\n    top_p: Optional[float] = None,\n    temperature: Optional[float] = None,\n    repetition_penalty: Optional[float] = None,\n    last_n_tokens: Optional[int] = None,\n    seed: Optional[int] = None,\n    batch_size: Optional[int] = None,\n    threads: Optional[int] = None,\n    stop: Optional[Sequence[str]] = None,\n    stream: Optional[bool] = None,\n    reset: Optional[bool] = None\n) → Union[str, Generator[str, NoneType, NoneType]]\n```\n\nGenerates text from a prompt.\n\n**Args:**\n\n- <b>`prompt`</b>: The prompt to generate text from.\n- <b>`max_new_tokens`</b>: The maximum number of new tokens to generate. Default: `256`\n- <b>`top_k`</b>: The top-k value to use for sampling. Default: `40`\n- <b>`top_p`</b>: The top-p value to use for sampling. Default: `0.95`\n- <b>`temperature`</b>: The temperature to use for sampling. Default: `0.8`\n- <b>`repetition_penalty`</b>: The repetition penalty to use for sampling. Default: `1.1`\n- <b>`last_n_tokens`</b>: The number of last tokens to use for repetition penalty. Default: `64`\n- <b>`seed`</b>: The seed value to use for sampling tokens. Default: `-1`\n- <b>`batch_size`</b>: The batch size to use for evaluating tokens in a single prompt. Default: `8`\n- <b>`threads`</b>: The number of threads to use for evaluating tokens. Default: `-1`\n- <b>`stop`</b>: A list of sequences to stop generation when encountered. Default: `None`\n- <b>`stream`</b>: Whether to stream the generated text. Default: `False`\n- <b>`reset`</b>: Whether to reset the model state before generating text. Default: `True`\n\n**Returns:**\nThe generated text.\n\n<!-- API_DOCS -->\n\n## License\n\n[MIT](https://github.com/marella/ctransformers/blob/main/LICENSE)\n"
        },
        {
          "name": "ctransformers",
          "type": "tree",
          "content": null
        },
        {
          "name": "models",
          "type": "tree",
          "content": null
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 0.146484375,
          "content": "[build-system]\nrequires = [\n    \"setuptools>=42\",\n    \"scikit-build>=0.13\",\n    \"cmake>=3.18\",\n    \"ninja\",\n]\nbuild-backend = \"setuptools.build_meta\"\n"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 1.99609375,
          "content": "import os\n\nkwargs = {}\nif os.environ.get(\"CT_WHEEL\") == \"1\":\n    from setuptools import setup\nelse:\n    from skbuild import setup\n\n    cmake_args = []\n    for key in [\"CT_INSTRUCTIONS\", \"CT_CUBLAS\", \"CT_HIPBLAS\", \"CT_METAL\"]:\n        value = os.environ.get(key)\n        if value:\n            cmake_args.append(f\"-D{key}={value}\")\n    if cmake_args:\n        kwargs[\"cmake_args\"] = cmake_args\n\nwith open(\"README.md\") as f:\n    long_description = f.read()\n\nname = \"ctransformers\"\n\nsetup(\n    name=name,\n    version=\"0.2.27\",\n    description=\"Python bindings for the Transformer models implemented in C/C++ using GGML library.\",\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    author=\"Ravindra Marella\",\n    author_email=\"mv.ravindra007@gmail.com\",\n    url=\"https://github.com/marella/{}\".format(name),\n    license=\"MIT\",\n    packages=[name, \"ctransformers.gptq\"],\n    package_data={name: [\"lib/*/*.so\", \"lib/*/*.dll\", \"lib/*/*.dylib\"]},\n    install_requires=[\n        \"huggingface-hub\",\n        \"py-cpuinfo>=9.0.0,<10.0.0\",\n    ],\n    extras_require={\n        \"cuda\": [\n            \"nvidia-cuda-runtime-cu12\",\n            \"nvidia-cublas-cu12\",\n        ],\n        \"gptq\": [\n            \"exllama==0.1.0\",\n        ],\n        \"tests\": [\n            \"pytest\",\n        ],\n    },\n    zip_safe=False,\n    classifiers=[\n        \"Development Status :: 1 - Planning\",\n        \"Intended Audience :: Developers\",\n        \"Intended Audience :: Education\",\n        \"Intended Audience :: Science/Research\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Programming Language :: Python :: 3\",\n        \"Topic :: Scientific/Engineering\",\n        \"Topic :: Scientific/Engineering :: Mathematics\",\n        \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\n        \"Topic :: Software Development\",\n        \"Topic :: Software Development :: Libraries\",\n        \"Topic :: Software Development :: Libraries :: Python Modules\",\n    ],\n    keywords=\"{} transformers ai llm\".format(name),\n    **kwargs,\n)\n"
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}