{
  "metadata": {
    "timestamp": 1736709809347,
    "page": 263,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjI3MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "orioledb/orioledb",
      "stars": 3107,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".dockerignore",
          "type": "blob",
          "size": 1.8310546875,
          "content": "# Exclude files and directories to minimize\n# the Docker build context size.\n# This practice limits the scope of COPY commands in the Dockerfile.\n# When not using multi-layer builds, it can effectively reduce the final image size.\n\n# The first part of this file is should be the same as the .gitignore file\n# The second part is the extra not needed content for .dockerignore\n\n########################################################\n# first part: .gitignore contents\n########################################################\n\n# Global excludes across all subdirectories\n**/*.o\n**/*.obj\n**/*.bc\n**/*.so\n**/*.so.[0-9]\n**/*.so.[0-9].[0-9]\n**/*.so.[0-9].[0-9][0-9]\n**/*.sl\n**/*.sl.[0-9]\n**/*.sl.[0-9].[0-9]\n**/*.sl.[0-9].[0-9][0-9]\n**/*.dylib\n**/*.dll\n**/*.exp\n**/*.a\n**/*.mo\n**/*.pot\n**/objfiles.txt\n**/.deps/\n**/*.gcno\n**/*.gcda\n**/*.gcov\n**/*.gcov.out\n**/lcov*.info\n**/coverage/\n**/coverage-html-stamp\n**/*.vcproj\n**/*.vcxproj\n**/win32ver.rc\n**/*.exe\n**/lib*dll.def\n**/lib*.pc\n\n# Local excludes in root directory\ntest/t/__pycache__/\ntest/__pycache__/\ntest/log/\nlog_docker_build/\ntest/results/\ntest/tmp_check/\ntest/tmp_check_iso/\ntest/output_iso/\ninclude/utils/stopevents_defs.h\ninclude/utils/stopevents_data.h\norioledb.typedefs\nci/antithesis\n\n# Ignore generated scripts\nsql/orioledb--1.0.sql\n\n\n#######################################################\n# second part: extra .dockerignore contents\n#######################################################\n\n# Exclude version control and continuous integration (CI) directories\n.git\n.github\n\n# Exclude Dockerfiles\nDockerfile\nDockerfile.ubuntu\n\n# Exclude OrioleDB Docker test definitions and code\n# as they are not needed inside the Docker image.\ntest/\nci/local_docker_matrix.sh\nci/docker_matrix.sh\n\n# Documentation files, which are not needed inside the Docker image.\ndoc/\n\n# Exclude the PostGIS Docker build directory\ndocker-postgis\n"
        },
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0.01171875,
          "content": "*.svg -diff\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.8505859375,
          "content": "# When adding new definitions here,\n#     please also include them in the first part of the .dockerignore file.\n\n# Global excludes across all subdirectories\n*.o\n*.obj\n*.bc\n*.so\n*.so.[0-9]\n*.so.[0-9].[0-9]\n*.so.[0-9].[0-9][0-9]\n*.sl\n*.sl.[0-9]\n*.sl.[0-9].[0-9]\n*.sl.[0-9].[0-9][0-9]\n*.dylib\n*.dll\n*.exp\n*.a\n*.mo\n*.pot\nobjfiles.txt\n.deps/\n*.gcno\n*.gcda\n*.gcov\n*.gcov.out\nlcov*.info\ncoverage/\ncoverage-html-stamp\n*.vcproj\n*.vcxproj\nwin32ver.rc\n*.exe\nlib*dll.def\nlib*.pc\n\n# Local excludes in root directory\n/test/t/__pycache__/\n/test/__pycache__/\n/test/log/\n/log_docker_build/\n/test/results/\n/test/tmp_check/\n/test/tmp_check_iso/\n/test/output_iso/\n/include/utils/stopevents_defs.h\n/include/utils/stopevents_data.h\n/orioledb.typedefs\n!ci/antithesis/libvoidstar.so\n\n# Ignore generated scripts\nsql/orioledb--1.0.sql\n\n# Exclude the PostGIS Docker build directory\n/docker-postgis\n"
        },
        {
          "name": ".pgtags",
          "type": "blob",
          "size": 0.0322265625,
          "content": "16: patches16_33\n17: patches17_5\n"
        },
        {
          "name": ".style.yapf",
          "type": "blob",
          "size": 0.0439453125,
          "content": "[style]\nbased_on_style = pep8\nuse_tabs = True"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.1259765625,
          "content": "OrioleDB is released under the PostgreSQL License,\na liberal Open Source license, similar to the BSD or MIT licenses.\n\nOrioleDB extension for PostgresSQL\n\nCopyright Â© 2021-2025, Oriole DB Inc.\n\nPermission to use, copy, modify, and distribute this software and its\ndocumentation for any purpose, without fee, and without a written agreement\nis hereby granted, provided that the above copyright notice and this paragraph\nand the following two paragraphs appear in all copies.\n\nIN NO EVENT SHALL THE UNIVERSITY OF CALIFORNIA BE LIABLE TO ANY PARTY FOR\nDIRECT, INDIRECT, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES, INCLUDING\nLOST PROFITS, ARISING OUT OF THE USE OF THIS SOFTWARE AND ITS DOCUMENTATION,\nEVEN IF THE UNIVERSITY OF CALIFORNIA HAS BEEN ADVISED OF THE POSSIBILITY\nOF SUCH DAMAGE.\n\nTHE UNIVERSITY OF CALIFORNIA SPECIFICALLY DISCLAIMS ANY WARRANTIES, INCLUDING,\nBUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\nA PARTICULAR PURPOSE. THE SOFTWARE PROVIDED HEREUNDER IS ON AN \"AS IS\" BASIS,\nAND THE UNIVERSITY OF CALIFORNIA HAS NO OBLIGATIONS TO PROVIDE MAINTENANCE,\nSUPPORT, UPDATES, ENHANCEMENTS, OR MODIFICATIONS.\n"
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 9.2431640625,
          "content": "# contrib/orioledb/Makefile\n\nMODULE_big = orioledb\nEXTENSION = orioledb\nPGFILEDESC = \"orioledb - orioledb transactional storage engine via TableAm\"\nSHLIB_LINK += -lzstd -lcurl -lssl -lcrypto\n\nDATA_built = $(patsubst %_prod.sql,%.sql,$(wildcard sql/*_prod.sql))\nDATA = $(filter-out $(wildcard sql/*_*.sql) $(DATA_built), $(wildcard sql/*sql))\n\nEXTRA_CLEAN = include/utils/stopevents_defs.h \\\n\t\t\t  include/utils/stopevents_data.h\nOBJS = src/btree/btree.o \\\n\t   src/btree/build.o \\\n\t   src/btree/check.o \\\n\t   src/btree/find.o \\\n\t   src/btree/insert.o \\\n\t   src/btree/io.o \\\n\t   src/btree/iterator.o \\\n\t   src/btree/merge.o \\\n\t   src/btree/modify.o \\\n\t   src/btree/page_chunks.o \\\n\t   src/btree/page_contents.o \\\n\t   src/btree/page_state.o \\\n\t   src/btree/print.o \\\n\t   src/btree/scan.o \\\n\t   src/btree/split.o \\\n\t   src/btree/undo.o \\\n\t   src/catalog/ddl.o \\\n\t   src/catalog/free_extents.o \\\n\t   src/catalog/indices.o \\\n\t   src/catalog/o_aggregate_cache.o \\\n\t   src/catalog/o_amop_cache.o \\\n\t   src/catalog/o_amproc_cache.o \\\n\t   src/catalog/o_class_cache.o \\\n\t   src/catalog/o_enum_cache.o \\\n\t   src/catalog/o_collation_cache.o \\\n\t   src/catalog/o_database_cache.o \\\n\t   src/catalog/o_indices.o \\\n\t   src/catalog/o_operator_cache.o \\\n\t   src/catalog/o_opclass_cache.o \\\n\t   src/catalog/o_proc_cache.o \\\n\t   src/catalog/o_range_cache.o \\\n\t   src/catalog/o_sys_cache.o \\\n\t   src/catalog/o_tables.o \\\n\t   src/catalog/o_type_cache.o \\\n\t   src/catalog/sys_trees.o \\\n\t   src/checkpoint/checkpoint.o \\\n\t   src/checkpoint/control.o \\\n\t   src/indexam/handler.o \\\n\t   src/orioledb.o \\\n\t   src/recovery/logical.o \\\n\t   src/recovery/recovery.o \\\n\t   src/recovery/wal.o \\\n\t   src/recovery/worker.o \\\n\t   src/s3/archive.o \\\n\t   src/s3/checkpoint.o \\\n\t   src/s3/control.o \\\n\t   src/s3/checksum.o \\\n\t   src/s3/headers.o \\\n\t   src/s3/queue.o \\\n\t   src/s3/requests.o \\\n\t   src/s3/worker.o \\\n\t   src/tableam/bitmap_scan.o \\\n\t   src/tableam/descr.o \\\n\t   src/tableam/func.o \\\n\t   src/tableam/handler.o \\\n\t   src/tableam/index_scan.o \\\n\t   src/tableam/key_range.o \\\n\t   src/tableam/key_bitmap.o \\\n\t   src/tableam/operations.o \\\n\t   src/tableam/scan.o \\\n\t   src/tableam/tree.o \\\n\t   src/transam/undo.o \\\n\t   src/transam/oxid.o \\\n\t   src/tuple/format.o \\\n\t   src/tuple/toast.o \\\n\t   src/tuple/slot.o \\\n\t   src/tuple/sort.o \\\n\t   src/workers/bgwriter.o \\\n\t   src/utils/compress.o \\\n\t   src/utils/o_buffers.o \\\n\t   src/utils/page_pool.o \\\n\t   src/utils/planner.o \\\n\t   src/utils/seq_buf.o \\\n\t   src/utils/stopevent.o \\\n\t   src/utils/ucm.o \\\n\t   $(WIN32RES)\n\nREGRESSCHECKS = btree_sys_check \\\n\t\t\t\talter_type \\\n\t\t\t\tbitmap_scan \\\n\t\t\t\tbtree_compression \\\n\t\t\t\tbtree_print \\\n\t\t\t\tcreateas \\\n\t\t\t\tddl \\\n\t\t\t\texplain \\\n\t\t\t\tfillfactor \\\n\t\t\t\tforeign_keys \\\n\t\t\t\tgenerated \\\n\t\t\t\tgetsomeattrs \\\n\t\t\t\tindices \\\n\t\t\t\tindices_build \\\n\t\t\t\tinherits \\\n\t\t\t\tioc \\\n\t\t\t\tjoins \\\n\t\t\t\tnulls \\\n\t\t\t\topclass \\\n\t\t\t\tparallel_scan \\\n\t\t\t\tpartial \\\n\t\t\t\tpartition \\\n\t\t\t\tprimary_key \\\n\t\t\t\trow_level_locks \\\n\t\t\t\trow_security \\\n\t\t\t\tsubquery \\\n\t\t\t\tsubtransactions \\\n\t\t\t\ttableam \\\n\t\t\t\ttemp \\\n\t\t\t\ttoast \\\n\t\t\t\ttrigger \\\n\t\t\t\ttypes\nISOLATIONCHECKS = bitmap_hist_scan \\\n\t\t\t\t  btree_iterate \\\n\t\t\t\t  btree_print_backend_id \\\n\t\t\t\t  concurrent_update_delete \\\n\t\t\t\t  fkeys \\\n\t\t\t\t  included \\\n\t\t\t\t  insert_fails \\\n\t\t\t\t  ioc_deadlock \\\n\t\t\t\t  ioc_lost_update \\\n\t\t\t\t  isol_ddl \\\n\t\t\t\t  isol_rc \\\n\t\t\t\t  isol_rr \\\n\t\t\t\t  isol_rr_bscan \\\n\t\t\t\t  isol_rr_seqscan \\\n\t\t\t\t  load_refind_page \\\n\t\t\t\t  merge \\\n\t\t\t\t  partition_move \\\n\t\t\t\t  rightlink \\\n\t\t\t\t  rll \\\n\t\t\t\t  rll_deadlock \\\n\t\t\t\t  rll_mix \\\n\t\t\t\t  rll_subtrans \\\n\t\t\t\t  table_lock_test \\\n\t\t\t\t  uniq\nTESTGRESCHECKS_PART_1 = test/t/checkpointer_test.py \\\n\t\t\t\t\t\ttest/t/eviction_bgwriter_test.py \\\n\t\t\t\t\t\ttest/t/eviction_compression_test.py \\\n\t\t\t\t\t\ttest/t/eviction_test.py \\\n\t\t\t\t\t\ttest/t/file_operations_test.py \\\n\t\t\t\t\t\ttest/t/files_test.py \\\n\t\t\t\t\t\ttest/t/incomplete_split_test.py \\\n\t\t\t\t\t\ttest/t/merge_test.py \\\n\t\t\t\t\t\ttest/t/o_tables_test.py \\\n\t\t\t\t\t\ttest/t/o_tables_2_test.py \\\n\t\t\t\t\t\ttest/t/recovery_test.py \\\n\t\t\t\t\t\ttest/t/recovery_opclass_test.py \\\n\t\t\t\t\t\ttest/t/recovery_worker_test.py \\\n\t\t\t\t\t\ttest/t/replication_test.py \\\n\t\t\t\t\t\ttest/t/types_test.py \\\n\t\t\t\t\t\ttest/t/undo_eviction_test.py\nTESTGRESCHECKS_PART_2 = test/t/checkpoint_concurrent_test.py \\\n\t\t\t\t\t\ttest/t/checkpoint_eviction_test.py \\\n\t\t\t\t\t\ttest/t/checkpoint_same_trx_test.py \\\n\t\t\t\t\t\ttest/t/checkpoint_split1_test.py \\\n\t\t\t\t\t\ttest/t/checkpoint_split2_test.py \\\n\t\t\t\t\t\ttest/t/checkpoint_split3_test.py \\\n\t\t\t\t\t\ttest/t/checkpoint_update_compress_test.py \\\n\t\t\t\t\t\ttest/t/checkpoint_update_test.py \\\n\t\t\t\t\t\ttest/t/ddl_test.py \\\n\t\t\t\t\t\ttest/t/eviction_full_memory_test.py \\\n\t\t\t\t\t\ttest/t/include_indices_test.py \\\n\t\t\t\t\t\ttest/t/indices_build_test.py \\\n\t\t\t\t\t\ttest/t/logical_test.py \\\n\t\t\t\t\t\ttest/t/not_supported_yet_test.py \\\n\t\t\t\t\t\ttest/t/parallel_test.py \\\n\t\t\t\t\t\ttest/t/reindex_test.py \\\n\t\t\t\t\t\ttest/t/s3_test.py \\\n\t\t\t\t\t\ttest/t/schema_test.py \\\n\t\t\t\t\t\ttest/t/toast_index_test.py \\\n\t\t\t\t\t\ttest/t/trigger_test.py \\\n\t\t\t\t\t\ttest/t/unlogged_test.py \\\n\t\t\t\t\t\ttest/t/vacuum_test.py\n\nPG_REGRESS_ARGS=--no-locale --inputdir=test --outputdir=test --temp-instance=./test/tmp_check\nPG_ISOLATION_REGRESS_ARGS=--no-locale --inputdir=test --outputdir=test/output_iso --temp-instance=./test/tmp_check_iso\n\nifdef IS_DEV\nsql/%.sql:\n\t@cat sql/$*_prod.sql sql/$*_dev.sql > $@\nelse\nsql/%.sql:\n\t@cat sql/$*_prod.sql > $@\nendif\n\nifdef USE_PGXS\nPG_CONFIG = pg_config\nPGXS := $(shell $(PG_CONFIG) --pgxs)\noverride PG_CPPFLAGS += -I$(CURDIR)/include\ninclude $(PGXS)\n\nifeq ($(shell expr $(MAJORVERSION) \\>= 14), 1)\n  REGRESSCHECKS += toast_column_compress\nendif\n\nifeq ($(shell expr $(MAJORVERSION) \\>= 15), 1)\n  TESTGRESCHECKS_PART_2 += test/t/merge_into_test.py\n  ISOLATIONCHECKS += isol_merge\nendif\n\nregresscheck: | install\n\t$(pg_regress_check) \\\n\t\t--temp-config test/orioledb_regression.conf \\\n\t\t$(PG_REGRESS_ARGS) \\\n\t\t$(REGRESSCHECKS)\n\nisolationcheck: | install\n\t$(pg_isolation_regress_check) \\\n\t\t--temp-config test/orioledb_isolation.conf \\\n\t\t$(PG_ISOLATION_REGRESS_ARGS) \\\n\t\t$(ISOLATIONCHECKS)\n\n$(TESTGRESCHECKS_PART_1) $(TESTGRESCHECKS_PART_2): | install\n\t$(with_temp_install) \\\n\tpython3 -W ignore::DeprecationWarning -m unittest -v $@\n\ninstallcheck: regresscheck isolationcheck testgrescheck\n\techo \"All checks are successful!\"\n\nelse\nsubdir = contrib/orioledb\ntop_builddir = ../..\noverride PG_CPPFLAGS += -I$(top_srcdir)/$(subdir)/include\ninclude $(top_builddir)/src/Makefile.global\ninclude $(top_srcdir)/contrib/contrib-global.mk\n\nregresscheck: | submake-regress submake-orioledb temp-install\n\t$(pg_regress_check) \\\n\t\t--temp-config $(top_srcdir)/contrib/orioledb/test/orioledb_regression.conf \\\n\t\t$(PG_REGRESS_ARGS) \\\n\t\t$(REGRESSCHECKS)\n\nisolationcheck: | submake-isolation submake-orioledb temp-install\n\t$(pg_isolation_regress_check) \\\n\t\t--temp-config $(top_srcdir)/contrib/orioledb/test/orioledb_isolation.conf \\\n\t\t$(PG_ISOLATION_REGRESS_ARGS) \\\n\t\t$(ISOLATIONCHECKS)\n\n$(TESTGRESCHECKS_PART_1) $(TESTGRESCHECKS_PART_2): | submake-orioledb temp-install\n\tPG_CONFIG=\"$(abs_top_builddir)/tmp_install$(bindir)/pg_config\" \\\n\t\t$(with_temp_install) \\\n\t\tpython3 -m unittest -v $@\n\ncheck: regresscheck isolationcheck testgrescheck\n\techo \"All checks are successful!\"\nendif\n\n# Retrieve the current commit hash from the Git repository.\n# If the .git environment does not exist (e.g., in a Docker environment or a non-Git setup),\n# fallback to a default \"fake\" commit hash (all zeros) to avoid errors.\nCOMMIT_HASH := $(shell git rev-parse HEAD 2>/dev/null)\nifeq ($(strip $(COMMIT_HASH)),)\n\tCOMMIT_HASH := 0000000000000000000000000000000000000000\nendif\noverride CFLAGS_SL += -DCOMMIT_HASH=$(COMMIT_HASH) -Wno-error=deprecated-declarations\n\nifdef VALGRIND\noverride with_temp_install += PGCTLTIMEOUT=3000 \\\n\tvalgrind --vgdb=yes --leak-check=no --gen-suppressions=all \\\n\t--suppressions=valgrind.supp --time-stamp=yes \\\n\t--log-file=pid-%p.log --trace-children=yes \\\n\t--trace-children-skip=*/initdb\nelse\noverride with_temp_install += PGCTLTIMEOUT=900\nendif\n\ninclude/utils/stopevents_data.h: include/utils/stopevents_defs.h\n\ninclude/utils/stopevents_defs.h: stopevents.txt stopevents_gen.py\n\tpython3 stopevents_gen.py\n\n\nifndef ORIOLEDB_PATCHSET_VERSION\nORIOLEDB_PATCHSET_VERSION=1\nendif\nCUR_ORIOLEDB_PATCHSET_VERSION := $(shell grep '^$(MAJORVERSION):' .pgtags | cut -d'_' -f2)\n\ncheck_patchset_version:\n\t@if [ $(CUR_ORIOLEDB_PATCHSET_VERSION) != $(ORIOLEDB_PATCHSET_VERSION) ]; then \\\n\t\techo \"Wrong orioledb patchset version:\"\\\n\t\t\t\t\"expected $(CUR_ORIOLEDB_PATCHSET_VERSION),\"\\\n\t\t\t\t\"got $(ORIOLEDB_PATCHSET_VERSION)\"; \\\n\t\techo \"Rebuild and install patched orioledb/postgres using tag\"\\\n\t\t\t\t\"'patches$(MAJORVERSION)_$(CUR_ORIOLEDB_PATCHSET_VERSION)'\"; \\\n\t\tfalse; \\\n\tfi\n\n$(OBJS): include/utils/stopevents_defs.h check_patchset_version\n\nsubmake-regress:\n\t$(MAKE) -C $(top_builddir)/src/test/regress all\n\nsubmake-isolation:\n\t$(MAKE) -C $(top_builddir)/src/test/isolation all\n\nsubmake-orioledb:\n\t$(MAKE) -C $(top_builddir)/contrib/orioledb\n\ntestgrescheck: $(TESTGRESCHECKS_PART_1) $(TESTGRESCHECKS_PART_2)\n\ntestgrescheck_part_1: $(TESTGRESCHECKS_PART_1)\n\ntestgrescheck_part_2: $(TESTGRESCHECKS_PART_2)\n\ntemp-install: EXTRA_INSTALL=contrib/orioledb\n\norioledb.typedefs: $(OBJS)\n\t./typedefs_gen.py\n\npgindent: orioledb.typedefs\n\tpgindent --typedefs=orioledb.typedefs \\\n\tsrc/*.c \\\n\tsrc/*/*.c \\\n\tinclude/*.h \\\n\tinclude/*/*.h\n\nyapf:\n\tyapf -i test/t/*.py\n\tyapf -i *.py\n\n.PHONY: submake-orioledb submake-regress check \\\n\tregresscheck isolationcheck testgrescheck pgindent \\\n\t$(TESTGRESCHECKS_PART_1) $(TESTGRESCHECKS_PART_2)\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 6.5185546875,
          "content": "# OrioleDB â building a modern cloud-native storage engine\n(... and solving some PostgreSQL wicked problems)\n\n[![build status](https://github.com/orioledb/orioledb/actions/workflows/build.yml/badge.svg)](https://github.com/orioledb/orioledb/actions)\n[![codecov](https://codecov.io/gh/orioledb/orioledb/branch/main/graph/badge.svg?token=shh4jn0DUK)](https://codecov.io/gh/orioledb/orioledb) [![dockerhub](https://github.com/orioledb/orioledb/actions/workflows/docker.yml/badge.svg)](https://hub.docker.com/r/orioledb/orioledb/tags)\n\n\nOrioleDB is a new storage engine for PostgreSQL, bringing a modern approach to\ndatabase capacity, capabilities and performance to the world's most-loved\ndatabase platform.\n\nOrioleDB consists of an extension, building on the innovative table access\nmethod framework and other standard Postgres extension interfaces. By extending\nand enhancing the current table access methods, OrioleDB opens the door to\na future of more powerful storage models that are optimized for cloud and\nmodern hardware architectures.\n\nOrioleDB is currently distributed under the standard PostgreSQL license.\n\n1. Designed for modern hardware.  OrioleDB design avoids legacy CPU bottlenecks\n   on modern servers containing dozens and hundreds CPU cores, providing\n   optimized usage of modern storage technologies such as SSD and NVRAM.\n\n2. Reduced maintenance needs.  OrioleDB implements the concepts of undo log\n   and page-mergins, eliminating the need for dedicated garbage collection\n   processes.  Additionally, OrioleDB implements default 64-bit transaction\n   identifiers, thus eliminating the well-known and painful wraparound problem.\n\n3. Designed to be distributed.  OrioleDB implements a row-level write-ahead\n   log with support for parallel apply.  This log architecture is optimized\n   for raft consensus-based replication allowing the implementation of\n   active-active multimaster.\n\nThe key technical differentiations of OrioleDB are as follows:\n\n1. No buffer mapping and lock-less page reading.  In-memory pages in OrioleDB\n   are connected with direct links to the storage pages.  This eliminates the\n   need for in-buffer mapping along with its related bottlenecks. Additionally,\n   in OrioleDB in-memory page reading doesn't involve atomic operations.\n   Together, these design decisions bring vertical scalability for Postgres\n   to the whole new level.\n\n2. MVCC is based on the UNDO log concept.  In OrioleDB, old versions of tuples\n   do not cause bloat in the main storage system, but eviction into the undo\n   log comprising undo chains.  Page-level undo records allow the system\n   to easily reclaim space occupied by deleted tuples as soon as possible.\n   Together with page-mergins, these mechanisms eliminate bloat in the majority\n   of cases.  Dedicated VACUUMing of tables is not needed as well, removing\n   a significant and common cause of system performance deterioration and\n   database outages.\n\n3. Copy-on-write checkpoints and row-level WAL.  OrioleDB utilizes\n   copy-on-write checkpoints, which provides a structurally consistent snapshot\n   of data every moment of time.  This is friendly for modern SSDs and allows\n   row-level WAL logging.  In turn, row-level WAL logging is easy to\n   parallelize (done), compact and suitable for active-active\n   multimaster (planned).\n\nSee [introduction](doc/intro.mdx), [getting started](doc/usage/getting-started.mdx), and [architecture](doc/architecture/overview.mdx)\n documentation as well as\n[PostgresBuild 2021 slides](https://www.slideshare.net/AlexanderKorotkov/solving-postgresql-wicked-problems).  To start the development see [OrioleDB development quickstart](doc/contributing/local-builds.mdx), and [project structure](doc/contributing/structure.mdx).\n\n## Status\n\nOrioleDB now has public beta status.  It is recommended for experiments,\ntesting, benchmarking, etc., but is not recommended for production usage.\nIf you are interested in OrioleDB's benefits in production, please\n[contact us](mailto:sales@orioledb.com).\n\n## Installation\n\n### Use docker container\n\nWe provide docker images for `amd64` and `arm64v8` architectures under Alpine Linux.\n\n```\ndocker pull orioledb/orioledb:latest-pg16\n```\nFor example it can be started same as postgres server:\n```bash\n# !Don't forget to set default locale to C, POSIX or use icu-locale\ndocker run --name some-postgres -e POSTGRES_PASSWORD=... -e POSTGRES_INITDB_ARGS=\"--locale=C\" -d -p5432:5432 orioledb/orioledb:latest-pg16\n```\n\nSee [our dockerhub](https://hub.docker.com/r/orioledb/orioledb) for details on our docker container usage.  See [the docker build guide](doc/contributing/docker-builds.mdx) for information on how to build the docker images locally.\n\n### Build from source\n\nBefore building and installing OrioleDB, one should ensure to have the following:\n\n * [PostgreSQL with extensibility patches](https://github.com/orioledb/postgres): [16 (tag: patches16_33)](https://github.com/orioledb/postgres/tree/patches16_33) or [17 (tag: patches17_5)](https://github.com/orioledb/postgres/tree/patches17_5);\n * Development package of libzstd;\n * python 3.5+ with testgres package.\n\nTypical installation procedure may look like this:\n\n```bash\n $ git clone https://github.com/orioledb/orioledb\n $ cd orioledb\n # Make sure that postgres bin directory is in PATH before running\n $ make USE_PGXS=1\n # IS_DEV=1 needed for tests to success\n $ make USE_PGXS=1 install IS_DEV=1\n $ make USE_PGXS=1 installcheck\n```\n\nBefore starting working with OrioleDB, adding the following line to\n`postgresql.conf` is required.  This change requires a restart of\nthe PostgreSQL database server.\n\n```\nshared_preload_libraries = 'orioledb.so'\n```\n\n## Collations\nOrioleDB tables support only ICU, C, and POSIX collations.\n\nSo that you don't have to write COLLATE for every \"text\" field of tables you have options:\n### Create whole cluster with one of these collations:\n```bash\ninitdb --locale=C -D..\n# OR\ninitdb --locale=POSIX -D..\n# OR\ninitdb --locale-provider=icu --icu-locale=en -D...\n```\n\n### Create new database with default collation from template0\n```bash\ncreatedb --locale=C --template template0 ...\n# OR\ncreatedb --locale=POSIX --template template0 ...\n# OR\ncreatedb --locale-provider=icu --icu-locale=en --template template0 ...\n```\nOr using `CREATE DATABASE` with `LOCALE` or `ICU_LOCALE` parameters.\n\n## Setup\n\nRun the following SQL query on the database to enable the OrioleDB engine.\n\n\n```sql\nCREATE EXTENSION orioledb;\n```\n\nOnce the above steps are complete, you can start using OrioleDB's tables.\nSee [getting started](doc/usage/getting-started.mdx) documentation for details.\n\n```sql\nCREATE TABLE table_name (...) USING orioledb;\n```\n"
        },
        {
          "name": "ci",
          "type": "tree",
          "content": null
        },
        {
          "name": "doc",
          "type": "tree",
          "content": null
        },
        {
          "name": "docker",
          "type": "tree",
          "content": null
        },
        {
          "name": "include",
          "type": "tree",
          "content": null
        },
        {
          "name": "orioledb.control",
          "type": "blob",
          "size": 0.162109375,
          "content": "# orioledb extension\ncomment = 'OrioleDB -- the next generation transactional engine'\ndefault_version = '1.3'\nmodule_pathname = '$libdir/orioledb'\nrelocatable = true\n"
        },
        {
          "name": "orioledb_s3_loader.py",
          "type": "blob",
          "size": 18.0576171875,
          "content": "#!/usr/bin/env python3\n\nimport argparse\nimport boto3\nimport os\nimport re\nimport struct\nimport testgres\n\nfrom botocore.config import Config\nfrom botocore.exceptions import ClientError, ParamValidationError\nfrom concurrent.futures import ThreadPoolExecutor\nfrom boto3.s3.transfer import TransferConfig\nfrom threading import Event\nfrom typing import Callable\nfrom urllib.parse import urlparse\n\n\nclass OrioledbS3ObjectLoader:\n\n\tdef parse_args(self):\n\t\tepilog = \"\"\"\n\t\t\tThis util uses boto3 under the hood.\n\t\t\tYou can set credentials using AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY and AWS_DEFAULT_REGION variables\n\t\t\tOr by using ~/.aws/config file.\n\t\t\tRead for more details:\n\t\t\thttps://boto3.amazonaws.com/v1/documentation/api/latest/guide/configuration.html#using-environment-variables\n\t\t\"\"\"\n\t\tparser = argparse.ArgumentParser(usage=argparse.SUPPRESS,\n\t\t                                 epilog=epilog)\n\t\tparser.add_argument(\n\t\t    '--endpoint',\n\t\t    dest='endpoint',\n\t\t    required=True,\n\t\t    help=\"AWS url (must contain bucket name if no prefix set)\")\n\t\tparser.add_argument('-d',\n\t\t                    '--data-dir',\n\t\t                    dest='data_dir',\n\t\t                    required=True,\n\t\t                    help=\"Destination data directory\")\n\t\tparser.add_argument(\n\t\t    '--prefix',\n\t\t    dest='prefix',\n\t\t    required=False,\n\t\t    default=\"\",\n\t\t    help=\"Prefix to prepend to S3 object name (may contain bucket name)\"\n\t\t)\n\t\tparser.add_argument('--cert-file',\n\t\t                    dest='cert_file',\n\t\t                    help=\"Path to crt file\")\n\t\tparser.add_argument(\n\t\t    '--verbose',\n\t\t    dest='verbose',\n\t\t    action='store_true',\n\t\t    help=\"More verbose output. Downloaded files displayed.\")\n\n\t\ttry:\n\t\t\targs = parser.parse_args()\n\t\texcept SystemExit as e:\n\t\t\tif e.code != 0:\n\t\t\t\tparser.print_help()\n\t\t\traise\n\n\t\tif 'cert_file' in args:\n\t\t\tverify = args.cert_file\n\t\telse:\n\t\t\tverify = None\n\n\t\tparsed_url = urlparse(args.endpoint)\n\t\tbucket = parsed_url.netloc.split('.')[0]\n\t\traw_endpoint = f\"{parsed_url.scheme}://{'.'.join(parsed_url.netloc.split('.')[1:])}\"\n\n\t\tsplitted_prefix = args.prefix.strip('/').split('/')\n\t\tsplitted_path = parsed_url.path.strip('/').split('/')\n\t\tprefix = os.path.join(*splitted_path, *splitted_prefix)\n\t\tsplitted_prefix = prefix.split('/')\n\n\t\tbucket_in_endpoint = True\n\t\tbucket_in_prefix = False\n\t\ttry:\n\t\t\tconfig = Config(s3={'addressing_style': 'virtual'})\n\t\t\ts3_client = boto3.client(\"s3\",\n\t\t\t                         endpoint_url=raw_endpoint,\n\t\t\t                         verify=verify,\n\t\t\t                         config=config)\n\t\t\ts3_client.head_bucket(Bucket=bucket)\n\t\t\tbucket_name = bucket\n\t\texcept ValueError:\n\t\t\tbucket_in_endpoint = False\n\t\t\tbucket_in_prefix = True\n\t\tif bucket_in_prefix:\n\t\t\tconfig = None\n\t\t\tbucket = splitted_prefix[0]\n\t\t\tprefix = '/'.join(splitted_prefix[1:])\n\t\t\ts3_client = boto3.client(\n\t\t\t    \"s3\",\n\t\t\t    endpoint_url=f\"{parsed_url.scheme}://{parsed_url.netloc}\",\n\t\t\t    verify=verify)\n\t\t\ttry:\n\t\t\t\ts3_client.head_bucket(Bucket=bucket)\n\t\t\texcept ParamValidationError:\n\t\t\t\tbucket_in_prefix = False\n\t\t\texcept ClientError:\n\t\t\t\tbucket_in_prefix = False\n\t\t\tbucket_name = bucket\n\n\t\tif not bucket_in_endpoint and not bucket_in_prefix:\n\t\t\traise Exception(\"No valid bucket name in endpoint or prefix\")\n\n\t\tself._error_occurred = Event()\n\t\tself.data_dir = args.data_dir\n\t\tself.bucket_name = bucket_name\n\t\tself.prefix = prefix\n\t\tself.verbose = args.verbose\n\t\tself.s3 = s3_client\n\n\tdef run(self):\n\t\tchkp_num = self.last_checkpoint_number(self.bucket_name)\n\t\tself.download_files_in_directory(self.bucket_name,\n\t\t                                 'data/',\n\t\t                                 chkp_num,\n\t\t                                 self.data_dir,\n\t\t                                 transform=self.transform_pg)\n\t\tself.download_files_in_directory(self.bucket_name,\n\t\t                                 'orioledb_data/',\n\t\t                                 chkp_num,\n\t\t                                 f\"{self.data_dir}/orioledb_data\",\n\t\t                                 transform=self.transform_orioledb,\n\t\t                                 filter=self.filter_orioledb)\n\n\t\tself.download_unchanged_files(\n\t\t    self.bucket_name, os.path.join(\"orioledb_data\", \"file_checksums\"),\n\t\t    chkp_num, None)\n\n\t\tself.download_unchanged_small_files(\n\t\t    self.bucket_name,\n\t\t    os.path.join(\"orioledb_data\", \"small_file_checksums\"), chkp_num,\n\t\t    None)\n\n\t\tcontrol = get_control_data(self.data_dir)\n\t\torioledb_control = get_orioledb_control_data(self.data_dir)\n\t\tself.download_undo(orioledb_control['undoRegularStartLocation'],\n\t\t                   orioledb_control['undoRegularEndLocation'],\n\t\t                   \"orioledb_data/%02X%08Xdata\")\n\t\tself.download_undo(orioledb_control['undoSystemStartLocation'],\n\t\t                   orioledb_control['undoSystemEndLocation'],\n\t\t                   \"orioledb_data/%02X%08Xsystem\")\n\t\twal_file = control[\"Latest checkpoint's REDO WAL file\"]\n\t\tlocal_path = os.path.join(self.data_dir, f\"pg_wal/{wal_file}\")\n\t\twal_file = os.path.join(self.prefix, f\"wal/{wal_file}\")\n\t\tself.download_file(self.bucket_name, wal_file, local_path)\n\n\tdef download_undo(self, startLocation, endLocation, template):\n\t\tUNDO_FILE_SIZE = 0x4000000\n\t\tif startLocation >= endLocation:\n\t\t\treturn\n\t\tfor fileNum in range(startLocation // UNDO_FILE_SIZE,\n\t\t                     (endLocation - 1) // UNDO_FILE_SIZE):\n\t\t\tfileName = template % (fileNum >> 32, fileNum & 0xFFFFFFFF)\n\t\t\tfileName = os.path.join(self.prefix, fileName)\n\t\t\tloader.download_file(self.bucket_name, fileName, fileName)\n\n\tdef last_checkpoint_number(self, bucket_name):\n\t\tpaginator = self.s3.get_paginator('list_objects_v2')\n\n\t\tnumbers = []\n\t\tprefix = os.path.join(self.prefix, 'data/')\n\t\tfor page in paginator.paginate(Bucket=bucket_name,\n\t\t                               Prefix=prefix,\n\t\t                               Delimiter='/'):\n\t\t\tif 'CommonPrefixes' in page:\n\t\t\t\tfor prefix in page['CommonPrefixes']:\n\t\t\t\t\tprefix_key = prefix['Prefix'].rstrip('/')\n\t\t\t\t\tsubdirectory = prefix_key.split('/')[-1]\n\t\t\t\t\ttry:\n\t\t\t\t\t\tnumber = int(subdirectory)\n\t\t\t\t\t\tnumbers += [number]\n\t\t\t\t\texcept ValueError:\n\t\t\t\t\t\tpass\n\n\t\tnumbers = sorted(numbers)\n\n\t\tfound = False\n\t\tchkp_list_index = len(numbers) - 1\n\n\t\tlast_chkp_data_dir = os.path.join(self.prefix, 'data',\n\t\t                                  str(numbers[chkp_list_index]))\n\n\t\twhile not found and chkp_list_index >= 0:\n\t\t\ttry:\n\t\t\t\tself.s3.head_object(\n\t\t\t\t    Bucket=bucket_name,\n\t\t\t\t    Key=f'{last_chkp_data_dir}/global/pg_control')\n\t\t\t\tself.s3.head_object(\n\t\t\t\t    Bucket=bucket_name,\n\t\t\t\t    Key=f'{last_chkp_data_dir}/orioledb_data/control')\n\t\t\t\tfound = True\n\t\t\texcept ClientError as e:\n\t\t\t\tif e.response['Error']['Code'] == \"404\":\n\t\t\t\t\tchkp_list_index -= 1\n\t\t\t\t\tif chkp_list_index >= 0:\n\t\t\t\t\t\tlast_chkp_data_dir = os.path.join(\n\t\t\t\t\t\t    self.prefix, 'data', str(numbers[chkp_list_index]))\n\t\t\t\telse:\n\t\t\t\t\traise\n\n\t\tif chkp_list_index < 0:\n\t\t\traise Exception(\"Failed to find valid checkpoint in s3 bucket\")\n\n\t\treturn numbers[chkp_list_index]\n\n\tdef list_objects(self, bucket_name, directory):\n\t\tobjects = []\n\t\tpaginator = self.s3.get_paginator('list_objects_v2')\n\n\t\tprefix = os.path.join(self.prefix, directory)\n\t\tfor page in paginator.paginate(Bucket=bucket_name, Prefix=prefix):\n\t\t\tif 'Contents' in page:\n\t\t\t\tpage_objs = [x[\"Key\"] for x in page['Contents']]\n\t\t\t\tobjects.extend(page_objs)\n\n\t\treturn objects\n\n\t# Reimplement os.dirs so it sets mode for intermediate dirs also\n\tdef makedirs(self, name, mode=0o777, exist_ok=False):\n\t\t\"\"\"makedirs(name [, mode=0o777][, exist_ok=False])\n\n\t\tSuper-mkdir; create a leaf directory and all intermediate ones.  Works like\n\t\tmkdir, except that any intermediate path segment (not just the rightmost)\n\t\twill be created if it does not exist. If the target directory already\n\t\texists, raise an OSError if exist_ok is False. Otherwise no exception is\n\t\traised.  This is recursive.\n\n\t\t\"\"\"\n\t\thead, tail = os.path.split(name)\n\t\tif not tail:\n\t\t\thead, tail = os.path.split(head)\n\t\tif head and tail and not os.path.exists(head):\n\t\t\ttry:\n\t\t\t\tself.makedirs(head, mode, exist_ok=exist_ok)\n\t\t\texcept FileExistsError:\n\t\t\t\t# Defeats race condition when another thread created the path\n\t\t\t\tpass\n\t\t\tcdir = os.curdir\n\t\t\tif isinstance(tail, bytes):\n\t\t\t\tcdir = bytes(os.curdir, 'ASCII')\n\t\t\tif tail == cdir:  # xxx/newdir/. exists if xxx/newdir exists\n\t\t\t\treturn\n\t\ttry:\n\t\t\tos.mkdir(name, mode)\n\t\texcept OSError:\n\t\t\t# Cannot rely on checking for EEXIST, since the operating system\n\t\t\t# could give priority to other errors like EACCES or EROFS\n\t\t\tif not exist_ok or not os.path.isdir(name):\n\t\t\t\traise\n\n\tdef download_file(self, bucket_name, file_key, local_path) -> bool:\n\t\ttry:\n\t\t\ttransfer_config = TransferConfig(use_threads=False,\n\t\t\t                                 max_concurrency=1)\n\t\t\tif file_key[-1] == '/':\n\t\t\t\tdirs = local_path\n\t\t\telse:\n\t\t\t\tdirs = '/'.join(local_path.split('/')[:-1])\n\t\t\tself.makedirs(dirs, exist_ok=True, mode=0o700)\n\t\t\tif file_key[-1] != '/':\n\t\t\t\tself.s3.download_file(bucket_name,\n\t\t\t\t                      file_key,\n\t\t\t\t                      local_path,\n\t\t\t\t                      Config=transfer_config)\n\t\t\tif self.verbose:\n\t\t\t\tprint(f\"{file_key} -> {local_path}\", flush=True)\n\t\t\tif re.match(r'.*/orioledb_data/small_files_\\d+$', local_path):\n\t\t\t\tbase_dir = '/'.join(local_path.split('/')[:-2])\n\t\t\t\twith open(local_path, 'rb') as file:\n\t\t\t\t\tdata = file.read()\n\t\t\t\tnumFiles = struct.unpack('i', data[0:4])[0]\n\t\t\t\tfor i in range(0, numFiles):\n\t\t\t\t\t(nameOffset, dataOffset,\n\t\t\t\t\t dataLength) = struct.unpack('iii',\n\t\t\t\t\t                             data[4 + i * 12:16 + i * 12])\n\t\t\t\t\tname = data[nameOffset:data.find(b'\\0', nameOffset\n\t\t\t\t\t                                 )].decode('ascii')\n\t\t\t\t\tfullname = f\"{base_dir}/{name}\"\n\t\t\t\t\tif self.verbose:\n\t\t\t\t\t\tprint(f\"{file_key} -> {fullname}\", flush=True)\n\t\t\t\t\tself.makedirs(os.path.dirname(fullname),\n\t\t\t\t\t              exist_ok=True,\n\t\t\t\t\t              mode=0o700)\n\t\t\t\t\twith open(fullname, 'wb') as file:\n\t\t\t\t\t\tfile.write(data[dataOffset:dataOffset + dataLength])\n\t\t\t\t\tos.chmod(fullname, 0o600)\n\t\t\t\tos.unlink(local_path)\n\n\t\texcept ClientError as e:\n\t\t\tif e.response['Error']['Code'] == \"404\":\n\t\t\t\tprint(f\"File not found: {file_key}\")\n\t\t\telse:\n\t\t\t\tprint(f\"An error occurred: {e}\")\n\t\t\tself._error_occurred.set()\n\t\t\treturn False\n\n\t\treturn True\n\n\tdef transform_orioledb(self, val: str) -> str:\n\t\toffset = 0\n\t\tprefix = self.prefix.strip('/')\n\t\tif prefix != \"\":\n\t\t\toffset = len(prefix.split('/'))\n\t\tparts = val.split('/')\n\t\tfile_parts = parts[offset + 3].split('.')\n\t\tresult = f\"{parts[offset + 2]}/{file_parts[0]}-{parts[offset + 1]}\"\n\t\tif file_parts[-1] == 'map':\n\t\t\tresult += '.map'\n\t\treturn result\n\n\tdef filter_orioledb(self, val: str) -> bool:\n\t\toffset = 0\n\t\tprefix = self.prefix.strip('/')\n\t\tif prefix != \"\":\n\t\t\toffset = len(prefix.split('/'))\n\t\tparts = val.split('/')\n\t\tfile_parts = parts[offset + 3].split('.')\n\t\tis_map = file_parts[-1] == 'map'\n\t\treturn is_map\n\n\tdef transform_pg(self, val: str) -> str:\n\t\toffset = 0\n\t\tprefix = self.prefix.strip('/')\n\t\tif prefix != \"\":\n\t\t\toffset = len(prefix.split('/'))\n\t\tparts = val.split('/')\n\t\tresult = '/'.join(parts[offset + 2:])\n\t\treturn result\n\n\tdef download_files_in_directory(self,\n\t                                bucket_name,\n\t                                directory,\n\t                                chkp_num,\n\t                                local_directory,\n\t                                transform: Callable[[str], str],\n\t                                filter: Callable[[str], bool] = None):\n\t\tlast_chkp_dir = os.path.join(directory, str(chkp_num))\n\t\tobjects = self.list_objects(bucket_name, last_chkp_dir)\n\t\tmax_threads = os.cpu_count()\n\n\t\twith ThreadPoolExecutor(max_threads) as executor:\n\t\t\tfutures = []\n\n\t\t\tfor file_key in objects:\n\t\t\t\tlocal_file = transform(file_key)\n\t\t\t\tif filter and not filter(file_key):\n\t\t\t\t\tcontinue\n\t\t\t\tlocal_path = f\"{local_directory}/{local_file}\"\n\t\t\t\tfuture = executor.submit(self.download_file, bucket_name,\n\t\t\t\t                         file_key, local_path)\n\t\t\t\tfutures.append(future)\n\n\t\t\tfor future in futures:\n\t\t\t\tfuture.result()\n\n\t\t\t\tif self._error_occurred.is_set():\n\t\t\t\t\tprint(\"An error occurred. Stopping all downloads.\")\n\t\t\t\t\texecutor.shutdown(wait=False, cancel_futures=True)\n\t\t\t\t\tbreak\n\n\tdef download_unchanged_files(self, bucket_name: str,\n\t                             file_checksums_name: str, chkp_num: int,\n\t                             file_checksums: dict[str, str] | None):\n\t\t# We won't be able to download unchanged previous files if this is\n\t\t# the first checkpoint\n\t\tif chkp_num <= 1:\n\t\t\treturn\n\n\t\tprev_chkp_num = chkp_num - 1\n\t\tprev_chkp_dir = os.path.join(self.prefix, \"data\", str(prev_chkp_num))\n\n\t\tif file_checksums is None:\n\t\t\tfile_checksums_path = os.path.join(self.data_dir,\n\t\t\t                                   file_checksums_name)\n\t\t\tfile_checksums = self.get_unchanged_file_checksums(\n\t\t\t    file_checksums_path, chkp_num)\n\n\t\tprev_file_checksums = {}\n\t\tfor filename, checkpoint in file_checksums.items():\n\t\t\t# Ignore changed files\n\t\t\tif int(checkpoint) == chkp_num:\n\t\t\t\tcontinue\n\n\t\t\t# This file needs to be downloaded from pre-previous checkpoint\n\t\t\tif int(checkpoint) < prev_chkp_num:\n\t\t\t\tprev_file_checksums[filename] = checkpoint\n\t\t\t\tcontinue\n\n\t\t\tremote_file = os.path.join(prev_chkp_dir, filename)\n\t\t\tlocal_file = os.path.join(self.data_dir, filename)\n\n\t\t\tself.download_file(bucket_name, remote_file, local_file)\n\n\t\t# Some files are still missing\n\t\tif len(prev_file_checksums) > 0:\n\t\t\t# Recursively download unchanged files\n\t\t\tself.download_unchanged_files(bucket_name, file_checksums_name,\n\t\t\t                              prev_chkp_num, prev_file_checksums)\n\n\tdef download_unchanged_small_files(self, bucket_name: str,\n\t                                   file_checksums_name: str, chkp_num: int,\n\t                                   file_checksums: dict[str, str] | None):\n\t\t# We won't be able to download unchanged previous files if this is\n\t\t# the first checkpoint\n\t\tif chkp_num <= 1:\n\t\t\treturn\n\n\t\tprev_chkp_num = chkp_num - 1\n\t\tprev_chkp_dir = os.path.join(self.prefix, \"data\", str(prev_chkp_num))\n\n\t\tif file_checksums is None:\n\t\t\tfile_checksums_path = os.path.join(self.data_dir,\n\t\t\t                                   file_checksums_name)\n\t\t\tfile_checksums = self.get_unchanged_file_checksums(\n\t\t\t    file_checksums_path, chkp_num)\n\n\t\tif len(file_checksums) == 0:\n\t\t\treturn\n\n\t\tsmall_files_num = 0\n\t\tfiles_restored = 0\n\t\twhile True:\n\t\t\tsmall_filename = os.path.join(\"orioledb_data\",\n\t\t\t                              f\"small_files_{small_files_num}\")\n\t\t\tremote_path = os.path.join(prev_chkp_dir, small_filename)\n\t\t\ttemp_path = os.path.join(self.data_dir,\n\t\t\t                         f\"{small_filename}.{prev_chkp_num}\")\n\n\t\t\t# Looks like the file doesn't exist, break\n\t\t\tif not self.download_file(bucket_name, remote_path, temp_path):\n\t\t\t\tbreak\n\n\t\t\twith open(temp_path, 'rb') as file:\n\t\t\t\tdata = file.read()\n\t\t\t\tnumFiles = struct.unpack('i', data[0:4])[0]\n\n\t\t\t\tfor i in range(0, numFiles):\n\t\t\t\t\t(nameOffset, dataOffset,\n\t\t\t\t\t dataLength) = struct.unpack('iii',\n\t\t\t\t\t                             data[4 + i * 12:16 + i * 12])\n\n\t\t\t\t\tname = data[nameOffset:data.find(b'\\0', nameOffset\n\t\t\t\t\t                                 )].decode('ascii')\n\t\t\t\t\tfullname = os.path.join(self.data_dir, name)\n\n\t\t\t\t\tif not name in file_checksums:\n\t\t\t\t\t\tcontinue\n\n\t\t\t\t\tif self.verbose:\n\t\t\t\t\t\tprint(f\"{remote_path} -> {fullname}\", flush=True)\n\n\t\t\t\t\tself.makedirs(os.path.dirname(fullname),\n\t\t\t\t\t              exist_ok=True,\n\t\t\t\t\t              mode=0o700)\n\n\t\t\t\t\twith open(fullname, 'wb') as file:\n\t\t\t\t\t\tfile.write(data[dataOffset:dataOffset + dataLength])\n\t\t\t\t\tos.chmod(fullname, 0o600)\n\n\t\t\t\t\tfiles_restored += 1\n\t\t\t\t\t# Looks like we restored all unchanged files, break\n\t\t\t\t\tif files_restored == len(file_checksums):\n\t\t\t\t\t\tbreak\n\n\t\t\tos.unlink(temp_path)\n\t\t\tsmall_files_num += 1\n\n\t\t\t# Looks like we restored all unchanged files, break\n\t\t\tif files_restored == len(file_checksums):\n\t\t\t\tbreak\n\n\t\t# Not all files were restored, check the previous checkpoint recursively\n\t\tif files_restored < len(file_checksums):\n\t\t\tprev_file_checksums = {}\n\t\t\tfor filename, checkpoint in file_checksums:\n\t\t\t\tif int(checkpoint) < prev_chkp_num:\n\t\t\t\t\tprev_file_checksums[filename] = checkpoint\n\n\t\t\tassert len(prev_file_checksums) > 0\n\n\t\t\t# Recursively download unchanged small files\n\t\t\tself.download_unchanged_small_files(bucket_name,\n\t\t\t                                    file_checksums_name,\n\t\t\t                                    prev_chkp_num,\n\t\t\t                                    prev_file_checksums)\n\n\tdef get_unchanged_file_checksums(self, file_checksums_name: str,\n\t                                 chkp_num: int) -> dict[str, str]:\n\t\tres = {}\n\n\t\tpattern_str = r\"^FILE: (?P<filename>.+), CHECKSUM: (?P<checksum>.+), CHECKPOINT: (?P<checkpoint>\\d+)$\"\n\t\tpattern = re.compile(pattern_str)\n\t\twith open(file_checksums_name) as file:\n\t\t\tfor line in file:\n\t\t\t\tm = pattern.search(line)\n\n\t\t\t\tif m is None or len(m.groups()) != 3:\n\t\t\t\t\traise Exception(\n\t\t\t\t\t    f\"Invalid line format of the checksum file {file_checksums_name}: {line}\"\n\t\t\t\t\t)\n\n\t\t\t\tline_dict = m.groupdict()\n\n\t\t\t\tif int(line_dict[\"checkpoint\"]) > chkp_num:\n\t\t\t\t\traise Exception(f'Unexpected checkpoint number \"{line}\"')\n\n\t\t\t\tif int(line_dict[\"checkpoint\"]) < chkp_num:\n\t\t\t\t\tres[line_dict[\"filename\"]] = line_dict[\"checkpoint\"]\n\n\t\treturn res\n\n\ndef get_control_data(data_dir: str):\n\t\"\"\"\n\tReturn contents of pg_control file.\n\t\"\"\"\n\n\t# this one is tricky (blame PG 9.4)\n\t_params = [testgres.get_bin_path(\"pg_controldata\")]\n\t_params += [\"-D\"]\n\t_params += [data_dir]\n\n\tdata = testgres.utils.execute_utility(_params)\n\n\tout_dict = {}\n\n\tfor line in data.splitlines():\n\t\tkey, _, value = line.partition(':')\n\t\tout_dict[key.strip()] = value.strip()\n\n\treturn out_dict\n\n\ndef get_orioledb_control_data(data_dir: str):\n\t\"\"\"\n\tReturn contents of OrioleDB control file.\n\t\"\"\"\n\n\tf = open(f\"{data_dir}/orioledb_data/control\", 'rb')\n\tdata = f.read(8 * 13)\n\t(undoRegularStartLocation,\n\t undoRegularEndLocation) = struct.unpack('QQ', data[8 * 8:8 * 10])\n\t(undoSystemStartLocation,\n\t undoSystemEndLocation) = struct.unpack('QQ', data[8 * 11:8 * 13])\n\tf.close()\n\n\tdict = {\n\t    'undoRegularStartLocation': undoRegularStartLocation,\n\t    'undoRegularEndLocation': undoRegularEndLocation,\n\t    'undoSystemStartLocation': undoSystemStartLocation,\n\t    'undoSystemEndLocation': undoSystemEndLocation\n\t}\n\n\treturn dict\n\n\nif __name__ == '__main__':\n\tloader = OrioledbS3ObjectLoader()\n\tloader.parse_args()\n\tloader.run()\n"
        },
        {
          "name": "sql",
          "type": "tree",
          "content": null
        },
        {
          "name": "src",
          "type": "tree",
          "content": null
        },
        {
          "name": "stopevents.txt",
          "type": "blob",
          "size": 0.3798828125,
          "content": "after_write_page\napply_undo\nbefore_blkno_lock\nbefore_write_page\ncheckpoint_index_start\ncheckpoint_step\ncheckpoint_writeback\ncheckpoint_table_start\nindex_insert\nioc_before_update\nmodify_start\npage_read\npage_split\nrecovery_start\nscan_end\nsplit_fail\nstep_left\nstep_right\nstep_down\nrelock_page\nbefore_apply_undo\nscan_disk_page\nload_page_refind\nafter_ionum_set\nbuild_index_placeholder_inserted\n"
        },
        {
          "name": "stopevents_gen.py",
          "type": "blob",
          "size": 0.6201171875,
          "content": "#!/usr/bin/env python3\n# coding: utf-8\n\nevent_names = []\n\nf = open('stopevents.txt', 'r')\nline = f.readline()\nwhile line:\n\tevent_names.append(line.strip())\n\tline = f.readline()\nf.close()\n\nf = open('include/utils/stopevents_defs.h', 'w')\nf.write('/* Generated file, see stopevents_gen.py */\\n\\n')\n\nfor i, e in enumerate(event_names):\n\tf.write(f'#define STOPEVENT_{e.upper()} ({str(i)})\\n')\n\nf.write('#define STOPEVENTS_COUNT (' + str(len(event_names)) + ')\\n')\nf.close()\n\nf = open('include/utils/stopevents_data.h', 'w')\nf.write('/* Generated file, see stopevents_gen.py */\\n\\n')\n\nfor e in event_names:\n\tf.write(f'\"{e}\",\\n')\n\nf.close()\n"
        },
        {
          "name": "test",
          "type": "tree",
          "content": null
        },
        {
          "name": "toast-logrep-debug-2.sh",
          "type": "blob",
          "size": 0.703125,
          "content": "initdb pgdata --locale=C\necho -e \"wal_level logical\\nmax_replication_slots 1\\nshared_preload_libraries = 'orioledb.so'\" > ./pgdata/postgresql.auto.conf\n\npg-ctl -D pgdata -l logfile start\n\npsql -dpostgres -c \"CREATE EXTENSION IF NOT EXISTS orioledb;\"\npsql -dpostgres -c \"CREATE TABLE IF NOT EXISTS t(id integer PRIMARY KEY, v1 text, v2 text, v3 text) using orioledb;\"\npsql -dpostgres -c \"SELECT * FROM pg_create_logical_replication_slot('regression_slot', 'test_decoding', false, true);\"\n\nfor i in {1..5}\ndo\n\tpsql -dpostgres -c \"INSERT INTO t VALUES ('$i', repeat('Pg', 2500), repeat('ab', 2500), repeat('xy', 2500));\"\n\tpsql -dpostgres -c \"SELECT * FROM pg_logical_slot_get_changes('regression_slot', NULL, NULL);\"\ndone\n\n"
        },
        {
          "name": "typedefs_gen.py",
          "type": "blob",
          "size": 1.2392578125,
          "content": "#!/usr/bin/env python3\n# coding: utf-8\n\nimport glob\nimport os\nimport re\nimport subprocess\n\n\ndef is_objdump(execname):\n\ttry:\n\t\toutput = subprocess.run([execname, '-v'],\n\t\t                        stdout=subprocess.PIPE,\n\t\t                        stderr=subprocess.PIPE).stdout.decode('utf-8')\n\texcept:\n\t\treturn False\n\treturn output.startswith('GNU objdump')\n\n\ndef find_objdump():\n\texecname = os.getenv('OBJDUMP')\n\tif execname and is_objdump(execname):\n\t\treturn execname\n\tif is_objdump('objdump'):\n\t\treturn 'objdump'\n\tif is_objdump('gobjdump'):\n\t\treturn 'gobjdump'\n\traise Exception('objdump not found')\n\n\noutput = subprocess.run([find_objdump(), '-W'] + glob.glob('src/*/*.o') +\n                        glob.glob('src/*.o'),\n                        stdout=subprocess.PIPE,\n                        stderr=subprocess.PIPE).stdout.decode('utf-8')\n\ntypenames = []\ni = 3\nfor line in output.splitlines():\n\tif line.find('DW_TAG_typedef') >= 0:\n\t\ti = 0\n\t\tcontinue\n\ti = i + 1\n\tif i > 3:\n\t\tcontinue\n\tfields = line.split()\n\tif fields[0] != 'DW_AT_name' and fields[1] != 'DW_AT_name':\n\t\tcontinue\n\tif fields[-1].startswith('DW_FORM_str'):\n\t\tcontinue\n\ttypenames.append(fields[-1])\n\nfile = open('orioledb.typedefs', 'w')\nfile.write(\"\\n\".join(sorted(set(typenames))) + \"\\n\")\nfile.close()\n"
        },
        {
          "name": "valgrind.supp",
          "type": "blob",
          "size": 4.68359375,
          "content": "# This is a suppression file for use with Valgrind tools.  File format\n# documentation:\n#  http://valgrind.org/docs/manual/mc-manual.html#mc-manual.suppfiles\n\n# The libc symbol that implements a particular standard interface is\n# implementation-dependent.  For example, strncpy() shows up as \"__GI_strncpy\"\n# on some platforms.  Use wildcards to avoid mentioning such specific names.\n# Avoid mentioning functions that are good candidates for inlining,\n# particularly single-caller static functions.  Suppressions mentioning them\n# would be ineffective at higher optimization levels.\n\n\n# We have occasion to write raw binary structures to disk or to the network.\n# These may contain uninitialized padding bytes.  Since recipients also ignore\n# those bytes as padding, this is harmless.\n\n{\n   padding_pgstat_send\n   Memcheck:Param\n   socketcall.send(msg)\n\n   fun:*send*\n   fun:pgstat_send\n}\n\n{\n   padding_pgstat_sendto\n   Memcheck:Param\n   socketcall.sendto(msg)\n\n   fun:*send*\n   fun:pgstat_send\n}\n\n{\n   padding_pgstat_write\n   Memcheck:Param\n   write(buf)\n\n   ...\n   fun:pgstat_write_statsfiles\n}\n\n{\n   padding_XLogRecData_CRC\n   Memcheck:Value8\n\n   fun:pg_comp_crc32c*\n   fun:XLogRecordAssemble\n}\n\n{\n   padding_XLogRecData_write\n   Memcheck:Param\n   pwrite64(buf)\n\n   ...\n   fun:XLogWrite\n}\n\n{\n   padding_relcache\n   Memcheck:Param\n   write(buf)\n\n   ...\n   fun:write_relcache_init_file\n}\n\n{\n   padding_reorderbuffer_serialize\n   Memcheck:Param\n   write(buf)\n\n   ...\n   fun:ReorderBufferSerializeTXN\n}\n\n{\n   padding_twophase_prepare\n   Memcheck:Param\n   write(buf)\n\n   ...\n   fun:EndPrepare\n}\n\n\n{\n   padding_twophase_CRC\n   Memcheck:Value8\n   fun:pg_comp_crc32c*\n   fun:EndPrepare\n}\n\n{\n   padding_bootstrap_initial_xlog_write\n   Memcheck:Param\n   write(buf)\n\n   ...\n   fun:BootStrapXLOG\n}\n\n{\n   padding_bootstrap_control_file_write\n   Memcheck:Param\n   write(buf)\n\n   ...\n   fun:WriteControlFile\n   fun:BootStrapXLOG\n}\n\n{\n   bootstrap_write_relmap_overlap\n   Memcheck:Overlap\n   fun:memcpy*\n   fun:write_relmap_file\n   fun:RelationMapFinishBootstrap\n}\n\n\n# gcc on ppc64 can generate a four-byte read to fetch the final \"char\" fields\n# of a FormData_pg_cast.  This is valid compiler behavior, because a proper\n# FormData_pg_cast has trailing padding.  Tuples we treat as structures omit\n# that padding, so Valgrind reports an invalid read.  Practical trouble would\n# entail the missing pad bytes falling in a different memory page.  So long as\n# the structure is aligned, that will not happen.\n{\n   overread_tuplestruct_pg_cast\n   Memcheck:Addr4\n\n   fun:IsBinaryCoercible\n}\n\n# Python's allocator does some low-level tricks for efficiency. Those\n# can be disabled for better instrumentation; but few people testing\n# postgres will have such a build of python. So add broad\n# suppressions of the resulting errors.\n# See also https://svn.python.org/projects/python/trunk/Misc/README.valgrind\n{\n   python_clever_allocator\n   Memcheck:Addr4\n   fun:PyObject_Free\n}\n\n{\n   python_clever_allocator\n   Memcheck:Addr8\n   fun:PyObject_Free\n}\n\n{\n   python_clever_allocator\n   Memcheck:Value4\n   fun:PyObject_Free\n}\n\n{\n   python_clever_allocator\n   Memcheck:Value8\n   fun:PyObject_Free\n}\n\n{\n   python_clever_allocator\n   Memcheck:Cond\n   fun:PyObject_Free\n}\n\n{\n   python_clever_allocator\n   Memcheck:Addr4\n   fun:PyObject_Realloc\n}\n\n{\n   python_clever_allocator\n   Memcheck:Addr8\n   fun:PyObject_Realloc\n}\n\n{\n   python_clever_allocator\n   Memcheck:Value4\n   fun:PyObject_Realloc\n}\n\n{\n   python_clever_allocator\n   Memcheck:Value8\n   fun:PyObject_Realloc\n}\n\n{\n   python_clever_allocator\n   Memcheck:Cond\n   fun:PyObject_Realloc\n}\n\n# Ubuntu Focal\n\n{\n   readdir_results\n   Memcheck:Cond\n\n   fun:RemoveTempXlogFiles\n   fun:StartupXLOG\n   fun:StartupProcessMain\n}\n\n{\n   backupFromStandby\n   Memcheck:Cond\n\n   fun:StartupXLOG\n   fun:StartupProcessMain\n}\n\n{\n   scanDir\n   Memcheck:Cond\n\n   fun:sendDir\n}\n\n{\n   do_pg_stop_backup\n   Memcheck:Cond\n   fun:do_pg_stop_backup\n}\n\n{\n   epoll_ctl(event)\n   Memcheck:Param\n   epoll_ctl(event)\n   fun:epoll_ctl\n}\n\n{\n   backend_read_statsfile\n   Memcheck:Param\n   socketcall.sendto(msg)\n   fun:send\n   fun:backend_read_statsfile\n}\n\n\n{\n   pgstat_send_tabstat\n   Memcheck:Param\n   socketcall.sendto(msg)\n   fun:send\n   fun:pgstat_send_tabstat\n\n}\n\n{\n   pgstat_report_analyze\n   Memcheck:Param\n   socketcall.sendto(msg)\n   fun:send\n   fun:pgstat_report_analyze\n}\n\n# Shared memory\n\n{\n   shared_memory_create\n   Memcheck:Value8\n   fun:PGSharedMemoryCreate\n}\n\n{\n   shared_memory_create\n   Memcheck:Cond\n   fun:PGSharedMemoryCreate\n}\n\n# pg_popcount64()\n\n{\n   pg_popcount64\n   Memcheck:Cond\n   ...\n   fun:pg_popcount64_choose\n}\n\n# pg_check_dir()\n\n{\n   pg_check_dir\n   Memcheck:Cond\n   fun:pg_check_dir\n}\n\n# icu_language_tag\n\n{\n   icu_language_tag\n   Memcheck:Cond\n   fun:icu_language_tag\n}\n"
        }
      ]
    }
  ]
}