{
  "metadata": {
    "timestamp": 1736709862273,
    "page": 340,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjM0MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "huggingface/neuralcoref",
      "stars": 2865,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.595703125,
          "content": "# NeuralCoref\n/neuralcoref/train/runs/*\n/models/\n/data/\n\n# Cython / C extensions\ncythonize.json\nneuralcoref/*.html\n*.cpp\n*.so\n\n# Vim / VSCode / editors\n*.swp\n*.sw*\nProfile.prof\n.vscode\n.sass-cache\n.idea/*\n\n# Python\n/.Python\n.python-version\n__pycache__/\n*.py[cod]\n.env/\n.env*\n.~env/\n.venv\nvenv/\n.dev\n.denv\n.pypyenv\n\n# Temporary files\n*.~*\ntmp/\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Distribution / packaging\nenv/\nbuild/\ndevelop-eggs/\ndist/\neggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\n*.egg-info/\n.installed.cfg\n*.egg\n.eggs\nMANIFEST\n\n# Windows\n*.bat\nThumbs.db\nDesktop.ini\n\n# Mac OS X\n*.DS_Store\n"
        },
        {
          "name": ".travis.yml",
          "type": "blob",
          "size": 0.85546875,
          "content": "group: travis_latest\nlanguage: python\ncache: pip\npython:\n    - 3.6\n    - 3.8\n    #- nightly\n    #- pypy\n    #- pypy3\nmatrix:\n    allow_failures:\n        - python: nightly\n        - python: pypy\n        - python: pypy3\ninstall:\n    #- pip install -r requirements.txt\n    - pip install flake8  # pytest  # add another testing frameworks later\nbefore_script:\n    # stop the build if there are Python syntax errors or undefined names\n    - flake8 . --count --select=E901,E999,F821,F822,F823 --show-source --statistics\n    # exit-zero treats all errors as warnings.  The GitHub editor is 127 chars wide\n    - flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\nscript:\n    - true  # pytest --capture=sys  # add other tests here\nnotifications:\n    on_success: change\n    on_failure: change  # `always` will be the setting once code changes slow down\n"
        },
        {
          "name": "LICENSE.txt",
          "type": "blob",
          "size": 1.046875,
          "content": "MIT License\n\nCopyright (c) 2018 Huggingface Inc.\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE."
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.0654296875,
          "content": "recursive-include include *.h\ninclude LICENSE.txt\ninclude README.md"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 15.4658203125,
          "content": "# ✨NeuralCoref 4.0: Coreference Resolution in spaCy with Neural Networks.\n\nNeuralCoref is a pipeline extension for spaCy 2.1+ which annotates and resolves coreference clusters using a neural network. NeuralCoref is production-ready, integrated in spaCy's NLP pipeline and extensible to new training datasets.\n\nFor a brief introduction to coreference resolution and NeuralCoref, please refer to our [blog post](https://medium.com/huggingface/state-of-the-art-neural-coreference-resolution-for-chatbots-3302365dcf30).\nNeuralCoref is written in Python/Cython and comes with a pre-trained statistical model for **English only**.\n\nNeuralCoref is accompanied by a visualization client [NeuralCoref-Viz](https://github.com/huggingface/neuralcoref-viz), a web interface  powered by a REST server that can be [tried online](https://huggingface.co/coref/). NeuralCoref is released under the MIT license.\n\n✨ Version 4.0 out now! Available on pip and compatible with SpaCy 2.1+.\n\n[![Current Release Version](https://img.shields.io/github/release/huggingface/neuralcoref.svg?style=flat-square)](https://github.com/huggingface/neuralcoref/releases)\n[![spaCy](https://img.shields.io/badge/made%20with%20❤%20and-spaCy-09a3d5.svg)](https://spacy.io)\n[![Travis-CI](https://travis-ci.org/huggingface/neuralcoref.svg?branch=master)](https://travis-ci.org/huggingface/neuralcoref)\n[![NeuralCoref online Demo](https://huggingface.co/coref/assets/thumbnail-large.png)](https://huggingface.co/coref/)\n\n- **Operating system**: macOS / OS X · Linux · Windows (Cygwin, MinGW, Visual Studio)\n- **Python version**: Python 3.6+ (only 64 bit)\n- **Package managers**: [pip]\n\n## Install NeuralCoref\n\n### Install NeuralCoref with pip\n\nThis is the easiest way to install NeuralCoref.\n\n```bash\npip install neuralcoref\n```\n\n#### `spacy.strings.StringStore size changed` error\n\nIf you have an error mentioning `spacy.strings.StringStore size changed, may indicate binary incompatibility` when loading NeuralCoref with `import neuralcoref`, it means you'll have to install NeuralCoref from the distribution's sources instead of the wheels to get NeuralCoref to build against the most recent version of SpaCy for your system.\n\nIn this case, simply re-install neuralcoref as follows:\n\n```bash\npip uninstall neuralcoref\npip install neuralcoref --no-binary neuralcoref\n```\n\n#### Installing SpaCy's model\n\nTo be able to use NeuralCoref you will also need to have an English model for SpaCy.\n\nYou can use whatever english model works fine for your application but note that the performances of NeuralCoref are strongly dependent on the performances of the SpaCy model and in particular on the performances of SpaCy model's tagger, parser and NER components. A larger SpaCy English model will thus improve the quality of the coreference resolution as well (see some details in the [Internals and Model](#-Internals-and-Model) section below).\n\nHere is an example of how you can install SpaCy and a (small) English model for SpaCy, more information can be found on [spacy's website](https://spacy.io/usage/models):\n\n```bash\npip install -U spacy\npython -m spacy download en\n```\n\n### Install NeuralCoref from source\n\nYou can also install NeuralCoref from sources. You will need to install the dependencies first which includes Cython and SpaCy.\n\nHere is the process:\n\n```bash\nvenv .env\nsource .env/bin/activate\ngit clone https://github.com/huggingface/neuralcoref.git\ncd neuralcoref\npip install -r requirements.txt\npip install -e .\n```\n\n## Internals and Model\n\nNeuralCoref is made of two sub-modules:\n\n- a rule-based mentions-detection module which uses SpaCy's tagger, parser and NER annotations to identify a set of potential coreference mentions, and\n- a feed-forward neural-network which compute a coreference score for each pair of potential mentions.\n\nThe first time you import NeuralCoref in python, it will download the weights of the neural network model in a cache folder.\n\nThe cache folder is set by defaults to `~/.neuralcoref_cache` (see [file_utils.py](./neuralcoref/file_utils.py)) but this behavior can be overided by setting the environment variable `NEURALCOREF_CACHE` to point to another location.\n\nThe cache folder can be safely deleted at any time and the module will download again the model the next time it is loaded.\n\nYou can have more information on the location, downloading and caching process of the internal model by activating python's `logging` module before loading NeuralCoref as follows:\n\n```python\nimport logging;\nlogging.basicConfig(level=logging.INFO)\nimport neuralcoref\n>>> INFO:neuralcoref:Getting model from https://s3.amazonaws.com/models.huggingface.co/neuralcoref/neuralcoref.tar.gz or cache\n>>> INFO:neuralcoref.file_utils:https://s3.amazonaws.com/models.huggingface.co/neuralcoref/neuralcoref.tar.gz not found in cache, downloading to /var/folders/yx/cw8n_njx3js5jksyw_qlp8p00000gn/T/tmp_8y5_52m\n100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40155833/40155833 [00:06<00:00, 6679263.76B/s]\n>>> INFO:neuralcoref.file_utils:copying /var/folders/yx/cw8n_njx3js5jksyw_qlp8p00000gn/T/tmp_8y5_52m to cache at /Users/thomaswolf/.neuralcoref_cache/f46bc05a4bfba2ae0d11ffd41c4777683fa78ed357dc04a23c67137abf675e14.7d6f9a6fecf5cf09e74b65f85c7d6896b21decadb2554d486474f63b95ec4633\n>>> INFO:neuralcoref.file_utils:creating metadata file for /Users/thomaswolf/.neuralcoref_cache/f46bc05a4bfba2ae0d11ffd41c4777683fa78ed357dc04a23c67137abf675e14.7d6f9a6fecf5cf09e74b65f85c7d6896b21decadb2554d486474f63b95ec4633\n>>> INFO:neuralcoref.file_utils:removing temp file /var/folders/yx/cw8n_njx3js5jksyw_qlp8p00000gn/T/tmp_8y5_52m\n>>> INFO:neuralcoref:extracting archive file /Users/thomaswolf/.neuralcoref_cache/f46bc05a4bfba2ae0d11ffd41c4777683fa78ed357dc04a23c67137abf675e14.7d6f9a6fecf5cf09e74b65f85c7d6896b21decadb2554d486474f63b95ec4633 to dir /Users/thomaswolf/.neuralcoref_cache/neuralcoref\n```\n\n## Loading NeuralCoref\n\n### Adding NeuralCoref to the pipe of an English SpaCy Language\n\nHere is the recommended way to instantiate NeuralCoref and add it to SpaCY's pipeline of annotations:\n\n```python\n\n# Load your usual SpaCy model (one of SpaCy English models)\nimport spacy\nnlp = spacy.load('en')\n\n# Add neural coref to SpaCy's pipe\nimport neuralcoref\nneuralcoref.add_to_pipe(nlp)\n\n# You're done. You can now use NeuralCoref as you usually manipulate a SpaCy document annotations.\ndoc = nlp(u'My sister has a dog. She loves him.')\n\ndoc._.has_coref\ndoc._.coref_clusters\n```\n\n### Loading NeuralCoref and adding it manually to the pipe of an English SpaCy Language\n\nAn equivalent way of adding NeuralCoref to a SpaCy model pipe is to instantiate the NeuralCoref class first and then add it manually to the pipe of the SpaCy Language model.\n\n```python\n\n# Load your usual SpaCy model (one of SpaCy English models)\nimport spacy\nnlp = spacy.load('en')\n\n# load NeuralCoref and add it to the pipe of SpaCy's model\nimport neuralcoref\ncoref = neuralcoref.NeuralCoref(nlp.vocab)\nnlp.add_pipe(coref, name='neuralcoref')\n\n# You're done. You can now use NeuralCoref the same way you usually manipulate a SpaCy document and it's annotations.\ndoc = nlp(u'My sister has a dog. She loves him.')\n\ndoc._.has_coref\ndoc._.coref_clusters\n```\n\n## Using NeuralCoref\n\nNeuralCoref will resolve the coreferences and annotate them as [extension attributes](https://spacy.io/usage/processing-pipelines#custom-components-extensions) in the spaCy `Doc`,  `Span` and `Token` objects under the `._.` dictionary.\n\nHere is the list of the annotations:\n\n|  Attribute                |  Type              |  Description\n|---------------------------|--------------------|-----------------------------------------------------\n|`doc._.has_coref`          |boolean             |Has any coreference has been resolved in the Doc\n|`doc._.coref_clusters`     |list of `Cluster`   |All the clusters of corefering mentions in the doc\n|`doc._.coref_resolved`     |unicode             |Unicode representation of the doc where each corefering mention is replaced by the main mention in the associated cluster.\n|`doc._.coref_scores`       |Dict of Dict        |Scores of the coreference resolution between mentions.\n|`span._.is_coref`          |boolean             |Whether the span has at least one corefering mention\n|`span._.coref_cluster`     |`Cluster`           |Cluster of mentions that corefer with the span\n|`span._.coref_scores`      |Dict                |Scores of the coreference resolution of & span with other mentions (if applicable).\n|`token._.in_coref`         |boolean             |Whether the token is inside at least one corefering mention\n|`token._.coref_clusters`   |list of `Cluster`   |All the clusters of corefering mentions that contains the token\n\nA `Cluster` is a cluster of coreferring mentions which has 3 attributes and a few methods to simplify the navigation inside a cluster:\n\n|  Attribute or method   |  Type / Return type |  Description\n|------------------------|---------------------|-----------------------------------------------------\n|`i`                     |int                  |Index of the cluster in the Doc\n|`main`                  |`Span`               |Span of the most representative mention in the cluster\n|`mentions`              |list of `Span`       |List of all the mentions in the cluster\n|`__getitem__`           |return `Span`        |Access a mention in the cluster\n|`__iter__`              |yields `Span`        |Iterate over mentions in the cluster\n|`__len__`               |return int           |Number of mentions in the cluster\n\n### Navigating the coreference cluster chains\n\nYou can also easily navigate the coreference cluster chains and display clusters and mentions.\n\nHere are some examples, try them out to test it for yourself.\n\n```python\nimport spacy\nimport neuralcoref\nnlp = spacy.load('en')\nneuralcoref.add_to_pipe(nlp)\n\ndoc = nlp(u'My sister has a dog. She loves him')\n\ndoc._.coref_clusters\ndoc._.coref_clusters[1].mentions\ndoc._.coref_clusters[1].mentions[-1]\ndoc._.coref_clusters[1].mentions[-1]._.coref_cluster.main\n\ntoken = doc[-1]\ntoken._.in_coref\ntoken._.coref_clusters\n\nspan = doc[-1:]\nspan._.is_coref\nspan._.coref_cluster.main\nspan._.coref_cluster.main._.coref_cluster\n```\n\n**Important**: NeuralCoref mentions are spaCy [Span objects](https://spacy.io/api/span) which means you can access all the usual [Span attributes](https://spacy.io/api/span#attributes) like `span.start` (index of the first token of the span in the document), `span.end` (index of the first token after the span in the document), etc...\n\nEx: `doc._.coref_clusters[1].mentions[-1].start` will give you the index of the first token of the last mention of the second coreference cluster in the document.\n\n## Parameters\n\nYou can pass several additional parameters to `neuralcoref.add_to_pipe` or `NeuralCoref()` to control the behavior of NeuralCoref.\n\nHere is the full list of these parameters and their descriptions:\n\n| Parameter       | Type                    | Description\n|-----------------|-------------------------|----------------\n|`greedyness`     |float                    |A number between 0 and 1 determining how greedy the model is about making coreference decisions (more greedy means more coreference links). The default value is 0.5.\n|`max_dist`       |int                      |How many mentions back to look when considering possible antecedents of the current mention. Decreasing the value will cause the system to run faster but less accurately. The default value is 50.\n|`max_dist_match` |int                      |The system will consider linking the current mention to a preceding one further than `max_dist` away if they share a noun or proper noun. In this case, it looks `max_dist_match` away instead. The default value is 500.\n|`blacklist`      |boolean                  |Should the system resolve coreferences for pronouns in the following list: `[\"i\", \"me\", \"my\", \"you\", \"your\"]`. The default value is True (coreference resolved).\n|`store_scores`   |boolean                  |Should the system store the scores for the coreferences in annotations. The default value is True.\n|`conv_dict`      |dict(str, list(str))     |A conversion dictionary that you can use to replace the embeddings of *rare words* (keys) by an average of the embeddings of a list of *common words* (values). Ex: `conv_dict={\"Angela\": [\"woman\", \"girl\"]}` will help resolving coreferences for `Angela` by using the embeddings for the more common `woman` and `girl` instead of the embedding of `Angela`. This currently only works for single words (not for words groups).\n\n### How to change a parameter\n\n```python\nimport spacy\nimport neuralcoref\n\n# Let's load a SpaCy model\nnlp = spacy.load('en')\n\n# First way we can control a parameter\nneuralcoref.add_to_pipe(nlp, greedyness=0.75)\n\n# Another way we can control a parameter\nnlp.remove_pipe(\"neuralcoref\")  # This remove the current neuralcoref instance from SpaCy pipe\ncoref = neuralcoref.NeuralCoref(nlp.vocab, greedyness=0.75)\nnlp.add_pipe(coref, name='neuralcoref')\n```\n\n### Using the conversion dictionary parameter to help resolve rare words\n\nHere is an example on how we can use the parameter `conv_dict` to help resolving coreferences of a rare word like a name:\n\n```python\nimport spacy\nimport neuralcoref\n\nnlp = spacy.load('en')\n\n# Let's try before using the conversion dictionary:\nneuralcoref.add_to_pipe(nlp)\ndoc = nlp(u'Deepika has a dog. She loves him. The movie star has always been fond of animals')\ndoc._.coref_clusters\ndoc._.coref_resolved\n# >>> [Deepika: [Deepika, She, him, The movie star]]\n# >>> 'Deepika has a dog. Deepika loves Deepika. Deepika has always been fond of animals'\n# >>> Not very good...\n\n# Here are three ways we can add the conversion dictionary\nnlp.remove_pipe(\"neuralcoref\")\nneuralcoref.add_to_pipe(nlp, conv_dict={'Deepika': ['woman', 'actress']})\n# or\nnlp.remove_pipe(\"neuralcoref\")\ncoref = neuralcoref.NeuralCoref(nlp.vocab, conv_dict={'Deepika': ['woman', 'actress']})\nnlp.add_pipe(coref, name='neuralcoref')\n# or after NeuralCoref is already in SpaCy's pipe, by modifying NeuralCoref in the pipeline\nnlp.get_pipe('neuralcoref').set_conv_dict({'Deepika': ['woman', 'actress']})\n\n# Let's try agin with the conversion dictionary:\ndoc = nlp(u'Deepika has a dog. She loves him. The movie star has always been fond of animals')\ndoc._.coref_clusters\n# >>> [Deepika: [Deepika, She, The movie star], a dog: [a dog, him]]\n# >>> 'Deepika has a dog. Deepika loves a dog. Deepika has always been fond of animals'\n# >>> A lot better!\n```\n\n## Using NeuralCoref as a server\n\nA simple example of server script for integrating NeuralCoref in a REST API is provided as an example in [`examples/server.py`](examples/server.py).\n\nTo use it you need to install falcon first:\n\n```bash\npip install falcon\n```\n\nYou can then start the server as follows:\n\n```bash\ncd examples\npython ./server.py\n```\n\nAnd query the server like this:\n\n```bash\ncurl --data-urlencode \"text=My sister has a dog. She loves him.\" -G localhost:8000\n```\n\nThere are many other ways you can manage and deploy NeuralCoref. Some examples can be found in [spaCy Universe](https://spacy.io/universe/).\n\n## Re-train the model / Extend to another language\n\nIf you want to retrain the model or train it on another language, see our [training instructions](./neuralcoref/train/training.md) as well as our [blog post](https://medium.com/huggingface/how-to-train-a-neural-coreference-model-neuralcoref-2-7bb30c1abdfe)\n"
        },
        {
          "name": "bin",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "include",
          "type": "tree",
          "content": null
        },
        {
          "name": "neuralcoref",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.0390625,
          "content": "spacy>=2.1.0,<3.0.0\ncython>=0.25\npytest\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 8.1142578125,
          "content": "#!/usr/bin/env python\n\"\"\"\nSimple check list from AllenNLP repo: https://github.com/allenai/allennlp/blob/master/setup.py\n\nTo create the package for pypi.\n\n1. Change the version in __init__.py and setup.py.\n\n2. Commit these changes with the message: \"Release: VERSION\"\n\n3. Add a tag in git to mark the release: \"git tag VERSION -m'Adds tag VERSION for pypi' \"\n   Push the tag to git: git push --tags origin master\n\n4. Build both the sources and the wheel. Do not change anything in setup.py between\n   creating the wheel and the source distribution (obviously).\n\n   For the wheel, run: \"python setup.py bdist_wheel\" in the top level allennlp directory.\n   (this will build a wheel for the python version you use to build it - make sure you use python 3.x).\n\n   For the sources, run: \"python setup.py sdist\"\n   You should now have a /dist directory with both .whl and .tar.gz source versions of allennlp.\n\n5. Check that everything looks correct by uploading the package to the pypi test server:\n\n   twine upload dist/* -r pypitest\n   (pypi suggest using twine as other methods upload files via plaintext.)\n\n   Check that you can install it in a virtualenv by running:\n   pip install -i https://testpypi.python.org/pypi neuralcoref\n\n6. Upload the final version to actual pypi:\n   twine upload dist/* -r pypi\n\n7. Copy the release notes from RELEASE.md to the tag in github once everything is looking hunky-dory.\n\n\"\"\"\nimport os\nimport subprocess\nimport sys\nimport contextlib\nfrom distutils.command.build_ext import build_ext\nfrom distutils.sysconfig import get_python_inc\nimport distutils.util\nfrom distutils import ccompiler, msvccompiler\nfrom setuptools import Extension, setup, find_packages\n\ndef is_new_osx():\n    \"\"\"Check whether we're on OSX >= 10.10\"\"\"\n    name = distutils.util.get_platform()\n    if sys.platform != \"darwin\":\n        return False\n    elif name.startswith(\"macosx-10\"):\n        minor_version = int(name.split(\"-\")[1].split(\".\")[1])\n        if minor_version >= 7:\n            return True\n        else:\n            return False\n    else:\n        return False\n\n\nPACKAGE_DATA = {'': ['*.pyx', '*.pxd'],\n                '': ['*.h'],}\n\n\nPACKAGES = find_packages()\n\n\nMOD_NAMES = ['neuralcoref.neuralcoref']\n\n\n\nCOMPILE_OPTIONS = {\n    \"msvc\": [\"/Ox\", \"/EHsc\"],\n    \"mingw32\": [\"-O2\", \"-Wno-strict-prototypes\", \"-Wno-unused-function\"],\n    \"other\": [\"-O2\", \"-Wno-strict-prototypes\", \"-Wno-unused-function\"],\n}\n\n\nLINK_OPTIONS = {\"msvc\": [], \"mingw32\": [], \"other\": []}\n\n\nif is_new_osx():\n    # On Mac, use libc++ because Apple deprecated use of\n    # libstdc\n    COMPILE_OPTIONS[\"other\"].append(\"-stdlib=libc++\")\n    LINK_OPTIONS[\"other\"].append(\"-lc++\")\n    # g++ (used by unix compiler on mac) links to libstdc++ as a default lib.\n    # See: https://stackoverflow.com/questions/1653047/avoid-linking-to-libstdc\n    LINK_OPTIONS[\"other\"].append(\"-nodefaultlibs\")\n\n\nUSE_OPENMP_DEFAULT = \"0\" if sys.platform != \"darwin\" else None\nif os.environ.get(\"USE_OPENMP\", USE_OPENMP_DEFAULT) == \"1\":\n    if sys.platform == \"darwin\":\n        COMPILE_OPTIONS[\"other\"].append(\"-fopenmp\")\n        LINK_OPTIONS[\"other\"].append(\"-fopenmp\")\n        PACKAGE_DATA[\"spacy.platform.darwin.lib\"] = [\"*.dylib\"]\n        PACKAGES.append(\"spacy.platform.darwin.lib\")\n\n    elif sys.platform == \"win32\":\n        COMPILE_OPTIONS[\"msvc\"].append(\"/openmp\")\n\n    else:\n        COMPILE_OPTIONS[\"other\"].append(\"-fopenmp\")\n        LINK_OPTIONS[\"other\"].append(\"-fopenmp\")\n\n\n# By subclassing build_extensions we have the actual compiler that will be used which is really known only after finalize_options\n# http://stackoverflow.com/questions/724664/python-distutils-how-to-get-a-compiler-that-is-going-to-be-used\nclass build_ext_options:\n    def build_options(self):\n        for e in self.extensions:\n            e.extra_compile_args += COMPILE_OPTIONS.get(\n                self.compiler.compiler_type, COMPILE_OPTIONS[\"other\"]\n            )\n        for e in self.extensions:\n            e.extra_link_args += LINK_OPTIONS.get(\n                self.compiler.compiler_type, LINK_OPTIONS[\"other\"]\n            )\n\n\nclass build_ext_subclass(build_ext, build_ext_options):\n    def build_extensions(self):\n        build_ext_options.build_options(self)\n        build_ext.build_extensions(self)\n\n\n# def is_installed(requirement):\n#     try:\n#         pkg_resources.require(requirement)\n#     except pkg_resources.ResolutionError:\n#         return False\n#     else:\n#         return True\n\n# if not is_installed('numpy>=1.11.0') or not is_installed('spacy>=2.1.0'):\n#     print(textwrap.dedent(\"\"\"\n#             Error: requirements needs to be installed first.\n#             You can install them via:\n#             $ pip install -r requirements.txt\n#             \"\"\"), file=sys.stderr)\n#     exit(1)\n\n@contextlib.contextmanager\ndef chdir(new_dir):\n    old_dir = os.getcwd()\n    try:\n        os.chdir(new_dir)\n        sys.path.insert(0, new_dir)\n        yield\n    finally:\n        del sys.path[0]\n        os.chdir(old_dir)\n\n\ndef generate_cython(root, source):\n    print('Cythonizing sources')\n    p = subprocess.call([sys.executable,\n                         os.path.join(root, 'bin', 'cythonize.py'),\n                         source], env=os.environ)\n    if p != 0:\n        raise RuntimeError('Running cythonize failed')\n\n\ndef is_source_release(path):\n    return os.path.exists(os.path.join(path, 'PKG-INFO'))\n\n\ndef setup_package():\n    root = os.path.abspath(os.path.dirname(__file__))\n    with chdir(root):\n        if not is_source_release(root):\n            generate_cython(root, 'neuralcoref')\n\n        include_dirs = [\n            get_python_inc(plat_specific=True),\n            os.path.join(root, 'include')]\n\n        if (ccompiler.new_compiler().compiler_type == 'msvc'\n            and msvccompiler.get_build_version() == 9):\n            include_dirs.append(os.path.join(root, 'include', 'msvc9'))\n\n        ext_modules = []\n        for mod_name in MOD_NAMES:\n            mod_path = mod_name.replace('.', '/') + '.cpp'\n            extra_link_args = []\n            # ???\n            # Imported from patch from @mikepb\n            # See Issue #267. Running blind here...\n            if sys.platform == 'darwin':\n                dylib_path = ['..' for _ in range(mod_name.count('.'))]\n                dylib_path = '/'.join(dylib_path)\n                dylib_path = '@loader_path/%s/neuralcoref/platform/darwin/lib' % dylib_path\n                extra_link_args.append('-Wl,-rpath,%s' % dylib_path)\n            ext_modules.append(\n                Extension(mod_name, [mod_path],\n                    language='c++', include_dirs=include_dirs,\n                    extra_link_args=extra_link_args))\n\n        setup(name='neuralcoref',\n            version='4.0',\n            description=\"Coreference Resolution in spaCy with Neural Networks\",\n            url='https://github.com/huggingface/neuralcoref',\n            author='Thomas Wolf',\n            author_email='thomwolf@gmail.com',\n            ext_modules=ext_modules,\n            classifiers=[\n                'Development Status :: 3 - Alpha',\n                'Environment :: Console',\n                'Intended Audience :: Developers',\n                \"Intended Audience :: Science/Research\",\n                \"License :: OSI Approved :: MIT License\",\n                \"Operating System :: POSIX :: Linux\",\n                \"Operating System :: MacOS :: MacOS X\",\n                \"Operating System :: Microsoft :: Windows\",\n                \"Programming Language :: Cython\",\n                \"Programming Language :: Python :: 3.6\",\n                \"Programming Language :: Python :: 3.7\",\n                \"Programming Language :: Python :: 3.8\",\n                \"Topic :: Scientific/Engineering\",\n            ],\n            install_requires=[\n                \"numpy>=1.15.0\",\n                \"boto3\",\n                \"requests>=2.13.0,<3.0.0\",\n                \"spacy>=2.1.0,<3.0.0\"],\n            setup_requires=['wheel', 'spacy>=2.1.0,<3.0.0'],\n            python_requires=\">=3.6\",\n            packages=PACKAGES,\n            package_data=PACKAGE_DATA,\n            keywords='NLP chatbots coreference resolution',\n            license='MIT',\n            zip_safe=False,\n            platforms='any',\n            cmdclass={\"build_ext\": build_ext_subclass})\n\nif __name__ == '__main__':\n    setup_package()\n"
        }
      ]
    }
  ]
}