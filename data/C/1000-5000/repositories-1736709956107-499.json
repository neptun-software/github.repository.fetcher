{
  "metadata": {
    "timestamp": 1736709956107,
    "page": 499,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjUwMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "vnmakarov/mir",
      "stars": 2350,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".clang-format",
          "type": "blob",
          "size": 0.6064453125,
          "content": "BasedOnStyle: google\nSpaceBeforeParens: Always\nIndentCaseLabels: false\nAllowShortIfStatementsOnASingleLine: true\nAllowShortLoopsOnASingleLine: true\nSpaceAfterCStyleCast: true\nPointerAlignment: Right\nBreakBeforeBinaryOperators: All\nConstructorInitializerIndentWidth: 2\nContinuationIndentWidth: 2\nPenaltyBreakBeforeFirstCallParameter: 10000\nSortIncludes: false\nBreakStringLiterals: true\nBreakBeforeTernaryOperators: true\nAllowShortCaseLabelsOnASingleLine: true\nColumnLimit: 100\nMaxEmptyLinesToKeep: 1\n#StatementMacros: [ 'REP2', 'REP3', 'REP4', 'REP5', 'REP6', 'REP7', 'REP8' ]\n#TypenameMacros: [ 'VARR', 'DLIST', 'HTAB' ]\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.0693359375,
          "content": "*.o\n*.d\n/b2ctab\n/b2m\n/c2m\n/m2b\n*.so\n*.so.*\n*.a\n/mir-run\n/build\n/.cache\n"
        },
        {
          "name": ".travis.yml",
          "type": "blob",
          "size": 0.2490234375,
          "content": "language: c\nos:\n - linux\n# - osx\n#jobs:\n#  allow_failures:\n#    - os: osx\ndist: bionic\narch:\n# - amd64\n - ppc64le\n - arm64\n - s390x\n#jobs:\n#  allow_failures:\n#    - arch: s390x\ngit:\n  quiet: true\nbranches:\n  only:\n    - master\nscript:\n  - make c2mir-test\n"
        },
        {
          "name": "CMakeLists.txt",
          "type": "blob",
          "size": 8.88671875,
          "content": "\r\ncmake_minimum_required (VERSION 3.8)\r\n\r\n#set(CMAKE_C_COMPILER \"clang-cl\")\r\n\r\nproject (\"MIR\")\r\n\r\nif(NOT DEFINED CMAKE_BUILD_TYPE)\r\n  set(CMAKE_BUILD_TYPE \"RelWithDebInfo\")\r\nendif()\r\n\r\n#set(CMAKE_VERBOSE_MAKEFILE ON)\r\n\r\nif(NOT WIN32)\r\n  add_compile_options(\r\n    $<$<CONFIG:RELEASE>:-O3>\r\n    $<$<CONFIG:RelWithDebInfo>:-O3>\r\n  )\r\n  add_compile_options(-std=gnu11 -Wno-abi -fsigned-char -fPIC)\r\nendif()\r\n\r\ninclude(CheckCCompilerFlag)\r\nif(CMAKE_COMPILER_IS_GNUCC)\r\n  check_c_compiler_flag(-fno-tree-sra NO_TREE_SRA)\r\n  if (NO_TREE_SRA)\r\n    add_compile_options(-fno-tree-sra)\r\n  endif()\r\n  check_c_compiler_flag(-fno-ipa-cp-clone NO_IPA_CP_CLONE)\r\n  if (NO_IPA_CP_CLONE)\r\n    add_compile_options(-fno-ipa-cp-clone)\r\n  endif()\r\nendif()\r\n\r\ninclude(FindThreads)\r\nif(Threads_FOUND)\r\n  link_libraries(${CMAKE_THREAD_LIBS_INIT})\r\nendif()\r\n\r\nmessage(\"C compiler flags: ${CMAKE_C_FLAGS}\")\r\n\r\noption(BUILD_TESTING \"\" ON)\r\ninclude(CTest)\r\n\r\nadd_library(mir OBJECT mir.c mir-gen.c c2mir/c2mir.c mir.h mir-gen.h c2mir/c2mir.h)\r\ntarget_include_directories(mir PRIVATE ${PROJECT_SOURCE_DIR})\r\nif(Threads_FOUND)\r\n  target_compile_definitions(mir PUBLIC \"MIR_PARALLEL_GEN\")\r\nendif()\r\nif(UNIX)\r\n\ttarget_link_libraries(mir m)\r\nendif()\r\n\r\n# ------------------ LIBMIR -----------------------\r\nadd_library(mir_static STATIC)\r\ntarget_link_libraries(mir_static PRIVATE mir)\r\n\r\n# ------------------ LIBMIR SO --------------------\r\nadd_library(mir_shared SHARED)\r\nif(WIN32)\r\n  set_target_properties(mir_shared PROPERTIES WINDOWS_EXPORT_ALL_SYMBOLS TRUE)\r\nendif()\r\ntarget_link_libraries(mir_shared PRIVATE mir)\r\n\r\n# ------------------ C2M --------------------------\r\nadd_executable (c2m \"c2mir/c2mir-driver.c\")\r\ntarget_include_directories(c2m PRIVATE ${PROJECT_SOURCE_DIR})\r\ntarget_link_libraries(c2m mir ${CMAKE_DL_LIBS} )\r\n\r\n# ------------------ MIR RUN ----------------------\r\n\r\n#add_executable (mir-run \"mir-run.c\")\r\n#target_include_directories (mir-run PRIVATE ${PROJECT_SOURCE_DIR})\r\n#target_link_libraries(mir-run mir ${CMAKE_DL_LIBS} )\r\n\r\n# ------------------ MIR utils --------------------\r\nadd_executable (m2b \"mir-utils/m2b.c\")\r\ntarget_include_directories(m2b PRIVATE ${PROJECT_SOURCE_DIR})\r\ntarget_link_libraries(m2b mir)\r\n\r\nadd_executable (b2m \"mir-utils/b2m.c\")\r\ntarget_include_directories(b2m PRIVATE ${PROJECT_SOURCE_DIR})\r\ntarget_link_libraries(b2m mir)\r\n\r\nadd_executable (b2ctab \"mir-utils/b2ctab.c\")\r\ntarget_include_directories(b2ctab PRIVATE ${PROJECT_SOURCE_DIR})\r\ntarget_link_libraries(b2ctab mir)\r\n\r\n# ------------------ L2M --------------------------\r\nfind_package(LLVM)\r\nif(LLVM_FOUND)\r\n  add_executable (l2m \"llvm2mir/llvm2mir.c\" \"llvm2mir/llvm2mir-driver.c\" \"llvm2mir/llvm2mir.h\")\r\n  target_include_directories(l2m PRIVATE ${PROJECT_SOURCE_DIR} ${LLVM_INCLUDE_DIRS})\r\n  target_link_libraries(l2m mir m ${CMAKE_DL_LIBS} LLVM)\r\nendif()\r\n\r\n# ------------------ ADT tests --------------------------\r\nadd_executable (varr_test \"adt-tests/mir-varr-test.c\")\r\ntarget_include_directories(varr_test PRIVATE ${PROJECT_SOURCE_DIR})\r\n\r\nadd_executable (dlist_test \"adt-tests/mir-dlist-test.c\")\r\ntarget_include_directories(dlist_test PRIVATE ${PROJECT_SOURCE_DIR})\r\n\r\nadd_executable (bitmap_test \"adt-tests/mir-bitmap-test.c\")\r\ntarget_include_directories(bitmap_test PRIVATE ${PROJECT_SOURCE_DIR})\r\n\r\nadd_executable (htab_test \"adt-tests/mir-htab-test.c\")\r\ntarget_include_directories(htab_test PRIVATE ${PROJECT_SOURCE_DIR})\r\n\r\nadd_executable (reduce_test \"adt-tests/mir-reduce-test.c\")\r\ntarget_include_directories(reduce_test PRIVATE ${PROJECT_SOURCE_DIR})\r\n\r\nadd_test(varr-test varr_test)\r\nadd_test(dlist-test dlist_test)\r\nadd_test(bitmap-test bitmap_test)\r\nadd_test(htab-test htab_test)\r\nadd_test(reduce-test reduce_test ${PROJECT_SOURCE_DIR}/c2mir/c2mir.c)\r\n\r\n# ------------------ MIR utility tests ------------------------\r\nadd_executable (simplify_test \"mir-tests/simplify.c\")\r\ntarget_link_libraries(simplify_test mir)\r\n\r\nadd_executable (scan_test \"mir-tests/scan-test.c\")\r\ntarget_link_libraries(scan_test mir)\r\n\r\nadd_executable (io_test \"mir-tests/io.c\")\r\ntarget_link_libraries(io_test mir)\r\n\r\nadd_test(simplify-test simplify_test)\r\nadd_test(scan-test scan_test)\r\nadd_test(io-test io_test)\r\n\r\n# ------------------ Common for MIR tests ----------------------\r\nadd_executable (run_test \"mir-tests/run-test.c\")\r\ntarget_include_directories(run_test PRIVATE ${PROJECT_SOURCE_DIR})\r\ntarget_link_libraries(run_test mir)\r\n\r\n# ------------------ MIR interp tests --------------------------\r\nadd_executable (interp_loop \"mir-tests/loop-interp.c\")\r\ntarget_include_directories(interp_loop PRIVATE ${PROJECT_SOURCE_DIR})\r\ntarget_compile_definitions(interp_loop PRIVATE MIR_INTERP_DEBUG=1)\r\ntarget_link_libraries(interp_loop mir)\r\n\r\nadd_executable (interp_loop_c \"mir-tests/loop-interp.c\")\r\ntarget_include_directories(interp_loop_c PRIVATE ${PROJECT_SOURCE_DIR})\r\ntarget_compile_definitions(interp_loop_c PRIVATE MIR_INTERP_DEBUG=1 MIR_C_INTERFACE=1)\r\ntarget_link_libraries(interp_loop_c mir)\r\n\r\nadd_executable (interp_sieve \"mir-tests/sieve-interp.c\")\r\ntarget_include_directories(interp_sieve PRIVATE ${PROJECT_SOURCE_DIR})\r\ntarget_compile_definitions(interp_sieve PRIVATE MIR_INTERP_DEBUG=1)\r\ntarget_link_libraries(interp_sieve mir)\r\n\r\nadd_executable (interp_sieve_c \"mir-tests/sieve-interp.c\")\r\ntarget_include_directories(interp_sieve_c PRIVATE ${PROJECT_SOURCE_DIR})\r\ntarget_compile_definitions(interp_sieve_c PRIVATE MIR_INTERP_DEBUG=1 MIR_C_INTERFACE=1)\r\ntarget_link_libraries(interp_sieve_c mir)\r\n\r\nadd_executable (interp_hi \"mir-tests/hi-interp.c\")\r\ntarget_include_directories(interp_hi PRIVATE ${PROJECT_SOURCE_DIR})\r\ntarget_compile_definitions(interp_hi PRIVATE MIR_INTERP_DEBUG=1)\r\ntarget_link_libraries(interp_hi mir)\r\n\r\nadd_executable (interp_args \"mir-tests/args-interp.c\")\r\ntarget_include_directories(interp_args PRIVATE ${PROJECT_SOURCE_DIR})\r\ntarget_link_libraries(interp_args mir)\r\n\r\nadd_executable (interp_args_c \"mir-tests/args-interp.c\")\r\ntarget_include_directories(interp_args_c PRIVATE ${PROJECT_SOURCE_DIR})\r\ntarget_compile_definitions(interp_args_c PRIVATE MIR_C_INTERFACE=1)\r\ntarget_link_libraries(interp_args_c mir)\r\n\r\nadd_executable (run_test_d \"mir-tests/run-test.c\")\r\ntarget_include_directories(run_test_d PRIVATE ${PROJECT_SOURCE_DIR})\r\ntarget_compile_definitions(run_test_d PRIVATE TEST_GEN_DEBUG=1)\r\ntarget_link_libraries(run_test_d mir)\r\n\r\nadd_test(interp-test1 interp_loop)\r\nadd_test(interp-test2 interp_loop_c)\r\nadd_test(interp-test3 interp_sieve)\r\nadd_test(interp-test4 interp_sieve_c)\r\nadd_test(interp-test5 interp_hi)\r\nadd_test(interp-test6 interp_args)\r\nadd_test(interp-test7 interp_args_c)\r\n\r\nforeach (num 8 9 10)\r\n  add_test(interp-test${num} run_test -i ${PROJECT_SOURCE_DIR}/mir-tests/test${num}.mir)\r\nendforeach()\r\n\r\nif(NOT WIN32)\r\n  add_test(interp-test11 run_test -i ${PROJECT_SOURCE_DIR}/mir-tests/test11.mir) # for long double >= 10 bytes\r\n  add_test(interp-test12 run_test -i ${PROJECT_SOURCE_DIR}/mir-tests/test12.mir) # multiple return values\r\nendif()\r\n\r\nforeach (num 13 14 15 16)\r\n  add_test(interp-test${num} run_test -i ${PROJECT_SOURCE_DIR}/mir-tests/test${num}.mir)\r\nendforeach()\r\n\r\n# ------------------ MIR gen tests --------------------------\r\n\r\nforeach (num 1 2 3 4 5 6 7)\r\n  add_test(gen-test${num} run_test -d ${PROJECT_SOURCE_DIR}/mir-tests/test${num}.mir)\r\nendforeach()\r\n\r\nforeach (num 8 9 10)\r\n  add_test(gen-test${num} run_test -g ${PROJECT_SOURCE_DIR}/mir-tests/test${num}.mir)\r\nendforeach()\r\n\r\nif(NOT WIN32)\r\n  add_test(gen-test11 run_test -g ${PROJECT_SOURCE_DIR}/mir-tests/test11.mir) # for long double >= 10 bytes\r\n  add_test(gen-test12 run_test -g ${PROJECT_SOURCE_DIR}/mir-tests/test12.mir) # multiple return values\r\nendif()\r\n\r\nforeach (num 13 14 15 16)\r\n  add_test(gen-test${num} run_test -g ${PROJECT_SOURCE_DIR}/mir-tests/test${num}.mir)\r\nendforeach()\r\n\r\nadd_executable (gen_loop \"mir-tests/loop-sieve-gen.c\")\r\ntarget_include_directories(gen_loop PRIVATE ${PROJECT_SOURCE_DIR})\r\ntarget_compile_definitions(gen_loop PRIVATE TEST_GEN_DEBUG=1 TEST_GEN_LOOP)\r\ntarget_link_libraries(gen_loop mir)\r\n\r\nadd_executable (gen_sieve \"mir-tests/loop-sieve-gen.c\")\r\ntarget_include_directories(gen_sieve PRIVATE ${PROJECT_SOURCE_DIR})\r\ntarget_compile_definitions(gen_sieve PRIVATE TEST_GEN_DEBUG=1 TEST_GEN_SIEVE)\r\ntarget_link_libraries(gen_sieve mir)\r\n\r\nadd_test(gen-test-loop gen_loop)\r\nadd_test(gen-test-sieve gen_sieve)\r\n\r\n# ------------------ readme example test ----------------\r\n\r\nadd_executable (readme_example \"mir-tests/readme-example.c\")\r\ntarget_include_directories(readme_example PRIVATE ${PROJECT_SOURCE_DIR})\r\ntarget_link_libraries(readme_example mir)\r\n\r\nadd_test(readme-example-test readme_example)\r\n\r\n# ------------------ mir2c test -------------------------\r\n\r\nadd_executable (mir2c_test \"mir2c/mir2c.c\")\r\ntarget_include_directories(mir2c_test PRIVATE ${PROJECT_SOURCE_DIR})\r\ntarget_compile_definitions(mir2c_test PRIVATE TEST_MIR2C)\r\ntarget_link_libraries(mir2c_test mir)\r\n\r\nadd_test(mir2c-test mir2c_test)\r\n\r\n# ------------------ c2m tests --------------------------\r\n\r\nadd_test(c2mir-simple-test c2m -v ${PROJECT_SOURCE_DIR}/sieve.c -ei)\r\n\r\n"
        },
        {
          "name": "CUSTOM-ALLOCATORS.md",
          "type": "blob",
          "size": 10.1455078125,
          "content": "# Custom Allocators\n\nIn some environments, memory cannot / should not directly be managed by calls to `malloc`, `free` etc. for various reasons. To support this use case, MIR lets you provide user defined allocators. These can be supplied during context creation by calling `MIR_context_t MIR_init2 (MIR_alloc_t alloc, MIR_code_alloc_t code_alloc)`.\n\nCalling `MIR_context_t MIR_init (void)` instead without passing custom allocators will default to using the standard functions `malloc`, `free`, ..., as well as the operating systems default routines for memory mapping and protection.\n\n## User Guide\n\nThe following sections are intended for users of MIR as a library. If you want to contribute to MIR directly, take a look at [the developer guide](#developer-guide).\n\n### General Purpose Allocators\n\n`MIR_alloc` is the general purpose allocator type defined by MIR, used for most allocations. Users wishing to provide a general prupose allocator need to define the following functions:\n\n- `void *malloc (size_t size, void *user_data)`\n- `void *calloc (size_t num, size_t size, void *user_data)`\n- `void *realloc (void *ptr, size_t old_size, size_t new_size, void *user_data)`\n- `void free (void *ptr, void *user_data)`\n\nThese functions should follow the same semantics as the standard C functions of the same name. This includes the platform's alignment guarantees.\n\n> [!IMPORTANT]\n> The `realloc` function required by `MIR_alloc` slightly differs from its standard C counterpart in that it takes an additional parameter `old_size`, which denotes the size of the allocation `realloc` is invoked on.\n> This was introduced to support allocators that do not provide `realloc` natively, as shown in [the example below](#example).\n> Allocators that do support `realloc` out of the box can ignore this parameter or use it for validation purposes.\n\n> [!IMPORTANT]\n> Some allocator implementations (such as `std::pmr::(un)synchronized_pool_resource` in libstd++ / libc++) require users to provide the exact size of the allocation to calls of their deallocation function.\n> This approach turns out to be largely infeasible for MIR as there are countless allocations whose size is dynamically determined, which would (in contrast to the `realloc` compromise outlined above) require a lot of additional bookkeeping on MIR's part.\n> Users wishing to use such an allocator with MIR may need to implement this additional bookkeeping themselves.\n\nApart from the pointers and sizes one would expected, all functions additionally accept a `user_data` parameter. This can be used to pass additional context as outlined in [the example below](#example).\n\n> [!WARNING]\n> The `MIR_alloc` instance passed to `MIR_init2` must have a lifetime greater or equal to the resulting `MIR_context`, i.e. live at least as long as the subsequent call to `MIR_finish`.\n> The `MIR_alloc` instance being destroyed or going out of scope beforehand may result in undefined behavior.\n\n### Executable Code Allocators\n\n`MIR_code_alloc` is the executable code related allocator type defined by MIR. It is used to map and unmap pages of memory, as well as manipulate their protection. Users wishing to provide an executable code allocator need to define the following functions:\n\n- `void *(*mem_map) (size_t len, void *user_data)`: allocate and zero `len` bytes of memory (see `mmap` / `VirtualAlloc`)\n- `int (*mem_unmap) (void *ptr, size_t len, void *user_data)`: free `len` bytes of memory at `ptr`, previously allocated by a call to `mem_map` (see `munmap` / `VirtualFree`)\n- `int (*mem_protect) (void *ptr, size_t len, MIR_mem_protect_t prot, void *user_data)`: change the protection of memory identified by `ptr` and `len` according to the flags specified in `prot` (see `mprotect` / `VirtualProtect`)\n\nPossible values for `prot` are contained in enum `MIR_mem_protect_t` (`PROT_READ_EXEC` and `PROT_WRITE_EXEC`).\n\nSimilar to `MIR_alloc`, `MIR_code_alloc` lets users pass `user_data` to the different functions.\n\nMIR will not try to directly write to or execute memory returned by `mem_map`, but will instead call `mem_protect` with appropriate flags beforehand.\n\n> [!WARNING]\n> The `MIR_code_alloc` instance passed to `MIR_init2` must have a lifetime greater or equal to the resulting `MIR_context`, i.e. live at least as long as the subsequent call to `MIR_finish`.\n> The `MIR_code_alloc` instance being destroyed or going out of scope beforehand may result in undefined behavior.\n\n### Thread Safety\n\nUsers intending to use custom allocators while calling MIR functions from different threads need to ensure that their provided functions are thread safe.\n\n### Example\n\nThis example showcases an approach to wrap a given stateful allocator interface, `my_allocator`, for use with MIR.\n\nIt uses some C++11/14 features, but can be easily adapted to work with older C++ standards.\n\n```cpp\n#include <cstddef>\n#include <cstdint>\n#include <cstring>\n\n#include \"mir.h\"\n\ntemplate<typename T>\ninline constexpr T align(T value, uint64_t alignment)\n{\n    // sadly `std::align` is only useful for very specific use cases,\n    // hence we roll our own alignment routine:\n    return (T) ((((uint64_t) value) + alignment - 1) & ~(alignment - 1));\n}\n\nclass my_allocator\n{\npublic:\n    virtual ~my_allocator() = default;\n    void *allocate(size_t size) = 0;\n    void deallocate(void *ptr) = 0;\n};\n\nclass context\n{\npublic:\n    context(my_allocator &allocator)\n        : _allocator{allocator}\n        , _mir_alloc{&context::do_malloc,\n                     &context::do_calloc,\n                     &context::do_realloc,\n                     &context::do_free,\n                     this} // user_data\n        , _mir_context{MIR_init2(&_mir_alloc, nullptr)}\n    {\n    }\n\n    ~context()\n    {\n        if (_mir_context != nullptr)\n        {\n            MIR_finish(_mir_context);\n        }\n    }\n\n    // ...\n\nprivate:\n    static context &context_from_user_data(void *user_data)\n    {\n        return *static_cast<context *>(user_data);\n    }\n\n    static void *do_malloc(size_t size, void *user_data)\n    {\n        auto &self = context_from_user_data(user_data);\n        return self._allocator.allocate(size);\n    }\n\n    static void *do_calloc(size_t num, size_t size, void *user_data)\n    {\n        auto &self = context_from_user_data(user_data);\n        const size_t aligned_size = align(size, alignof(std::max_align_t));\n        const size_t total_size = aligned_size * num;\n        void *const ptr = self._allocator.allocate(total_size);\n        std::memset(ptr, 0, total_size);\n        return ptr;\n    }\n\n    static void *do_realloc(void *ptr, size_t old_size, size_t new_size, void *user_data)\n    {\n        auto &self = context_from_user_data(user_data);\n        void *const new_ptr = self._allocator.allocate(size);\n        // if the `my_alloctor` interface supports a `realloc` method natively,\n        // we could simply call it here;\n        // instead, for the purpose of this example, we have to rely on the size\n        // of the previous allocation to be able to translate `realloc` into\n        // `allocate` - `memcpy` - `deallocate`:\n        std::memcpy(new_ptr, ptr, old_size);\n        self._allocator.deallocate(ptr);\n        return new_ptr;\n    }\n\n    static void do_free (void *ptr, void *user_data)\n    {\n        if (ptr == nullptr)\n        {\n            return;\n        }\n        auto &self = context_from_user_data(user_data);\n        self._allocator.deallocate(ptr);\n    }\n\nprivate:\n    my_allocator &_allocator;\n    MIR_alloc _mir_alloc;\n    MIR_context_t _mir_context;\n};\n```\n\n## Developer Guide\n\nThe following sections are intended for contributors to MIR.\n\n### Overview\n\nPointers to allocators are stored in fields `alloc` and `code_alloc` of struct `MIR_context`. These pointers are always valid, even if the user did not provide any or only some allocators explicitly (in this case, default allocators are used where needed).\n\nPassing the executable code allocator only to `MIR_gen_init` may seem conceptually more sound, but does not seem to work in practice as the interpreter relies on some of the code generation infrastructure as well.\n\nThe vector implementation in [`mir-varr.h`](mir-varr.h) keeps an additional pointer to the allocator it was created with. While this slightly increases its memory footprint, the alternative (passing a `MIR_alloc_t` to each and every of its operations) made for a very verbose API.\n\n### Executables shipped with MIR\n\nCustom allocators are mostly relevant for uses of MIR as a library in some other project. In case you are working on some executable specific part of MIR, e.g. tests, you can mostly ignore custom allocators and simply call `MIR_init` instead of `MIR_init2` as before.\n\nIn case you are testing / using some of the lower level APIs that require you to explicitly pass an allocator, such as the [`VARR`](mir-varr.h) or [`HTAB`](mir-htab.h) implementations, you can include [`mir-alloc-default.c`](mir-alloc-default.c) into your translation unit and simply pass `&default_alloc` where required. The same goes for code allocators and [`mir-code-alloc-default.c`](mir-code-alloc-default.c) / `&default_code_alloc` respectively.\n\n### MIR as a Library\n\nCode shipped as part of the main MIR library should avoid calling standard memory management routines such as `malloc`, `free`, `mmap`, ... directly and instead use the following allocator aware replacements (located in [`mir-alloc.h`](mir-alloc.h) and [`mir-code-alloc.h`](mir-code-alloc.h) respectively):\n\n- `void *MIR_malloc (MIR_alloc_t alloc, size_t size)`\n- `void *MIR_calloc (MIR_alloc_t alloc, size_t num, size_t size)`\n- `void *MIR_realloc (MIR_alloc_t alloc,  void *ptr, size_t old_size, size_t new_size)`\n- `void MIR_free (MIR_alloc_t alloc, void *ptr)`\n- `void *MIR_mem_map (MIR_code_alloc_t code_alloc, size_t len)`\n- `int MIR_mem_unmap (MIR_code_alloc_t code_alloc, void *ptr, size_t len)`\n- `int MIR_mem_protect (MIR_code_alloc_t code_alloc, void *ptr, size_t len, MIR_mem_protect_t prot)`\n\nSuitable allocators can usually be obtained directly from the `MIR_context` (fields `alloc` and `code_alloc`), or by calling `MIR_alloc_t MIR_get_alloc (MIR_context_t ctx)`.\n\nIn case no `MIR_context` is available in a function where you require an allocator (neither directly nor indirectly through other sub-contexts such as `gen_ctx_t`), consider taking a `MIR_alloc_t` (or `MIR_code_alloc_t`) as a parameter.\n"
        },
        {
          "name": "GNUmakefile",
          "type": "blob",
          "size": 35.9482421875,
          "content": "PREFIX=/usr/local\nSRC_DIR=.\nBUILD_DIR=.\n\nADDITIONAL_INCLUDE_PATH:=\nUNAME_S := $(shell uname -s)\nUNAME_M := $(shell uname -m)\nifeq ($(UNAME_S),Darwin)\n    XCRUN := $(shell xcrun --show-sdk-path >/dev/null 2>&1 && echo yes || echo no)\n    ifeq ($(XCRUN),yes)\n      ADDITIONAL_INCLUDE_PATH := $(shell xcrun --show-sdk-path)/usr/include\n    endif\nendif\n\nLDFLAGS =\nOBJO=-o #trailing space is important\nEXEO=-o #trailing space is important\nifeq ($(OS),Windows_NT)\n  EXE=.exe\n  ifeq ($(CC),cc)\n    ifeq ($(findstring MINGW,$(UNAME_S)),MINGW)\n      CC=gcc\n    else\n      CC=cl\n    endif\n  endif\n  ifeq ($(CC),gcc)\n    CFLAGS += -fPIC -g -std=gnu11 -Wno-abi -fsigned-char\n    CFLAGS += -fno-tree-sra\n    COPTFLAGS = -O3 -DNDEBUG\n    CDEBFLAGS =\n    CDEB2FLAGS = -Wall -Wextra -g3 -dwarf4 -fsanitize=address -fsanitize=undefined -fno-sanitize=alignment\n    CFLAGS += $(COPTFLAGS)\n    LDFLAGS=-Wl,--stack,8388608\n    LD2FLAGS= $(LDFLAGS)\n    MIR_LIBS=-lm -lkernel32 -lpsapi\n  else ifeq ($(CC),cl)\n    COPTFLAGS = -O2 -DNDEBUG\n    CDEBFLAGS = -Od -Z7\n    CDEB2FLAGS = $(CDEBFLAGS)\n    CFLAGS += -nologo $(COPTFLAGS)\n    LDFLAGS= -nologo -F 8388608\n    LD2FLAGS= $(LDFLAGS)\n    MIR_LIBS=\n    OBJO=-Fo:\n    EXEO=-Fe:\n  endif\n\n  CPPFLAGS = -I$(SRC_DIR)\n  LDLIBS   = $(MIR_LIBS)\n  COMPILE = $(CC) $(CPPFLAGS) $(CFLAGS)\n  ifeq ($(CC),gcc)\n    COMPILE += -MMD -MP\n  endif\n  LINK = $(CC) $(LDFLAGS)\n  COMPILE_AND_LINK = $(CC) $(CPPFLAGS) $(CFLAGS) $(LDFLAGS)\n\nelse\n  EXE=\n  CC=gcc\n  CFLAGS += -fPIC -g -std=gnu11 -Wno-abi -fsigned-char\n  ifneq ($(ADDITIONAL_INCLUDE_PATH),)\n    CFLAGS += -DADDITIONAL_INCLUDE_PATH=\\\"$(ADDITIONAL_INCLUDE_PATH)\\\"\n  endif\n\n  ifeq ($(shell $(CC) -v 2>&1 | grep -c \"clang version\"), 0)\n    ifeq ($(shell $(CC) -fno-tree-sra 2>&1 | grep -c 'fno-tree-sra'), 0)\n      CFLAGS += -fno-tree-sra\n    endif\n    ifeq ($(shell $(CC) -fno-ipa-cp-clone 2>&1 | grep -c 'fno-ipa-cp-clone'), 0)\n      CFLAGS += -fno-ipa-cp-clone\n    endif\n  endif\n\n  MIR_LIBS=-lm -ldl\n  COPTFLAGS = -O3 -DNDEBUG\n  CDEBFLAGS =\n  CDEB2FLAGS = -Wall -Wextra -Wshadow -g3 -fsanitize=address -fsanitize=undefined -fno-sanitize=alignment\n  LD2FLAGS =  -fsanitize=address -fsanitize=undefined  -fno-sanitize=alignment\n  CFLAGS += $(COPTFLAGS)\n  CPPFLAGS = -I$(SRC_DIR)\n  LDLIBS   = $(MIR_LIBS)\n  COMPILE = $(CC) $(CPPFLAGS) -MMD -MP $(CFLAGS)\n  LINK = $(CC) $(LDFLAGS)\n  COMPILE_AND_LINK = $(CC) $(CPPFLAGS) $(CFLAGS) $(LDFLAGS)\nendif\n\nAPI_VERSION=1\nMAJOR_VERSION=0\nMINOR_VERSION=1\nGITCOMMIT:= $(shell cd $(SRC_DIR) && git log -1 --pretty='%H')\nCFLAGS += -DGITCOMMIT=$(GITCOMMIT)\n\nifeq ($(CC),cl)\n  OBJSUFF=obj\n  LIBSUFF=lib\n  SOLIB=libmir.dll\nelse\n  OBJSUFF=o\n  LIBSUFF=a\n  ifeq ($(OS),Windows_NT)\n    ifeq ($(findstring MINGW,$(UNAME_S)),MINGW)\n      SONAME=libmir.so.$(API_VERSION)\n      SOLIBFLAGS=-shared -Wl,-soname,$(SONAME)\n      SOLIB=$(SONAME).$(MAJOR_VERSION).$(MINOR_VERSION)\n    else\n      SOLIB=libmir.dll\n    endif\n  else\n    ifeq ($(UNAME_S),Darwin)\n      SOLIBVERSION=$(API_VERSION).$(MAJOR_VERSION)\n      SOLIB=libmir.$(API_VERSION).dylib\n      SOLIBFLAGS=-dynamiclib -install_name \"$(SOLIB)\" -current_version $(SOLIBVERSION).$(MINOR_VERSION) -compatibility_version $(SOLIBVERSION)\n    else\n      SONAME=libmir.so.$(API_VERSION)\n      SOLIBFLAGS=-shared -Wl,-soname,$(SONAME)\n      SOLIB=$(SONAME).$(MAJOR_VERSION).$(MINOR_VERSION)\n    endif\n  endif\nendif\n\nC2M_BOOTSTRAP_FLAGS = -DMIR_BOOTSTRAP\nC2M_BOOTSTRAP_FLAGS0 := $(C2M_BOOTSTRAP_FLAGS)\nifeq ($(shell sh $(SRC_DIR)/check-threads.sh), ok)\n  ifneq ($(CC),cl)\n    MIR_LIBS += -lpthread\n    CFLAGS += -DC2MIR_PARALLEL\n    C2M_BOOTSTRAP_FLAGS += -DC2MIR_PARALLEL\n  endif\nendif\n\nL2M_EXE=\nL2M_TEST=\nifneq ($(shell test -f /usr/include/llvm-c/Core.h|echo 1), 1)\nL2M_EXE += $(BUILD_DIR)/l2m$(EXE)\nL2M_TEST += l2m-test$(EXE)\nendif\n\nEXECUTABLES=$(BUILD_DIR)/c2m$(EXE) $(BUILD_DIR)/m2b$(EXE) $(BUILD_DIR)/b2m$(EXE) $(BUILD_DIR)/b2ctab$(EXE) $(L2M_EXE) $(BUILD_DIR)/mir-bin-run$(EXE)\n\nQ=@\n\n# Entries should be used for building and installation\n.PHONY: all debug install uninstall clean test bench\n\nall: $(BUILD_DIR)/libmir.$(LIBSUFF) $(BUILD_DIR)/$(SOLIB) $(EXECUTABLES)\n\ndebug: CFLAGS:=$(subst $(COPTFLAGS),$(CDEBFLAGS),$(CFLAGS))\ndebug: $(BUILD_DIR)/libmir.$(LIBSUFF) $(BUILD_DIR)/$(SOLIB) $(EXECUTABLES)\n\ndebug2: CFLAGS:=$(subst $(COPTFLAGS),$(CDEB2FLAGS),$(CFLAGS))\ndebug2: LDFLAGS:=$(LD2FLAGS)\ndebug2: $(BUILD_DIR)/libmir.$(LIBSUFF) $(BUILD_DIR)/$(SOLIB) $(EXECUTABLES)\n\ninstall: $(BUILD_DIR)/libmir.$(LIBSUFF) $(BUILD_DIR)/$(SOLIB) $(EXECUTABLES) | $(PREFIX)/include $(PREFIX)/lib $(PREFIX)/bin\n\tinstall -m a+r $(SRC_DIR)/mir.h $(SRC_DIR)/mir-dlist.h $(SRC_DIR)/mir-varr.h $(SRC_DIR)/mir-htab.h\\\n\t\t       $(SRC_DIR)/mir-gen.h $(SRC_DIR)/c2mir/c2mir.h $(PREFIX)/include\n\tinstall -m a+r $(BUILD_DIR)/libmir.$(LIBSUFF) $(BUILD_DIR)/$(SOLIB) $(PREFIX)/lib\nifeq ($(OS),Windows_NT)\nelse\n    ifeq ($(UNAME_S),Darwin)\n\trm -f $(PREFIX)/lib/libmir.dylib\n\tln -s $(PREFIX)/lib/$(SOLIB) $(PREFIX)/lib/libmir.dylib\n\tinstall_name_tool -change \"$(SOLIB)\" \"$(PREFIX)/lib/$(SOLIB)\" $(PREFIX)/lib/$(SOLIB)\n    else\n\trm -f $(PREFIX)/lib/$(SONAME)\n\tln -s $(PREFIX)/lib/$(SOLIB) $(PREFIX)/lib/$(SONAME)\n    endif\nendif\n\tinstall -m a+rx $(EXECUTABLES) $(PREFIX)/bin\n\n$(PREFIX)/include $(PREFIX)/lib $(PREFIX)/bin:\n\t   mkdir -p $@\n\nuninstall: $(BUILD_DIR)/libmir.$(LIBSUFF) $(BUILD_DIR)/$(SOLIB) $(EXECUTABLES) | $(PREFIX)/include $(PREFIX)/lib $(PREFIX)/bin\n\t$(RM) $(PREFIX)/include/mir.h $(PREFIX)/include/mir-dlist.h $(PREFIX)/include/mir-varr.h $(PREFIX)/include/mir-htab.h\\\n\t\t       $(PREFIX)/include/mir-gen.h $(PREFIX)/include/c2mir.h\n\t$(RM) $(PREFIX)/lib/libmir.$(LIBSUFF) $(PREFIX)/lib/$(SOLIB)\nifeq ($(OS),Windows_NT)\nelse\n    ifeq ($(UNAME_S),Darwin)\n\trm -f $(PREFIX)/lib/libmir.dylib\n    else\n\trm -f $(PREFIX)/lib/$(SONAME)\n    endif\nendif\n\t$(RM) $(EXECUTABLES:$(BUILD_DIR)/%=$(PREFIX)/bin/%)\n\t-rmdir $(PREFIX)/include $(PREFIX)/lib $(PREFIX)/bin\n\t-rmdir $(PREFIX)\n\nclean: clean-mir clean-c2m clean-utils clean-l2m clean-adt-tests clean-mir-tests clean-mir2c-test clean-bench\n\t$(RM) $(EXECUTABLES) $(BUILD_DIR)/libmir.$(LIBSUFF) $(BUILD_DIR)/$(SOLIB)\n\ntest: readme-example-test mir-bin-run-test c2mir-test\n\ntest-all: adt-test simplify-test io-test scan-test mir2c-test $(L2M-TEST) test\n\nbench: interp-bench gen-bench gen-bench2 io-bench mir2c-bench c2mir-sieve-bench gen-speed c2mir-bench\n\t@echo ==============================Bench is done\n\n# ------------------ MIR --------------------------\nMIR_SRC:=$(SRC_DIR)/mir.c $(SRC_DIR)/mir-gen.c\nMIR_BUILD:=$(MIR_SRC:$(SRC_DIR)/%.c=$(BUILD_DIR)/%.$(OBJSUFF))\n\n$(BUILD_DIR)/%.$(OBJSUFF): $(SRC_DIR)/%.c | $(BUILD_DIR)\n\t$(COMPILE) -c $< $(OBJO)$@\n\n.PHONY: clean-mir\nclean-mir:\n\t$(RM) $(MIR_BUILD) $(MIR_BUILD:.$(OBJSUFF)=.d)\n\n-include $(MIR_BUILD:.$(OBJSUFF)=.d)\n\n# ------------------ LIBMIR -----------------------\n$(BUILD_DIR)/libmir.$(LIBSUFF): $(BUILD_DIR)/mir.$(OBJSUFF) $(BUILD_DIR)/mir-gen.$(OBJSUFF) $(BUILD_DIR)/c2mir/c2mir.$(OBJSUFF)\nifeq ($(CC),cl)\n\tlib -nologo $^ -OUT:$@\nelse\n\t$(AR) rcs $@ $^\nendif\n\n# ------------------ LIBMIR SO --------------------\n$(BUILD_DIR)/$(SOLIB): $(BUILD_DIR)/mir.$(OBJSUFF) $(BUILD_DIR)/mir-gen.$(OBJSUFF) $(BUILD_DIR)/c2mir/c2mir.$(OBJSUFF)\nifeq ($(CC),cl)\n\t$(CC) -nologo -D_USRDLL -D_WINDLL $^ -link -DLL -OUT:$@\nelse\n\t$(CC) $(SOLIBFLAGS) -o $@ $^\nendif\n\n# ------------------ C2M --------------------------\nC2M_SRC:=$(SRC_DIR)/c2mir/c2mir.c $(SRC_DIR)/c2mir/c2mir-driver.c\nC2M_BUILD:=$(C2M_SRC:$(SRC_DIR)/%.c=$(BUILD_DIR)/%.$(OBJSUFF))\n\n$(BUILD_DIR)/c2mir/%.$(OBJSUFF): $(SRC_DIR)/c2mir/%.c | $(BUILD_DIR)/c2mir\n\t$(COMPILE) -c $< $(OBJO)$@\n\n$(BUILD_DIR)/c2m$(EXE): $(C2M_BUILD) $(BUILD_DIR)/libmir.$(LIBSUFF) | $(BUILD_DIR)\n\t$(LINK) $^ $(LDLIBS) $(EXEO)$@\n\n$(BUILD_DIR)/c2mir:\n\t   mkdir -p $@\n\n.PHONY: clean-c2m\nclean-c2m:\n\t$(RM) $(C2M_BUILD) $(C2M_BUILD:.$(OBJSUFF)=.d)\n\n-include $(C2M_BUILD:.$(OBJSUFF)=.d)\n\n# ------------------ MIR RUN ----------------------\n\nMIR_RUN_SRC:=$(SRC_DIR)/mir-bin-run.c\nMIR_RUN_BUILD:=$(MIR_RUN_SRC:$(SRC_DIR)/%.c=$(BUILD_DIR)/%.$(OBJSUFF))\n\n$(BUILD_DIR)/mir-bin-run$(EXE): $(MIR_RUN_BUILD) $(BUILD_DIR)/libmir.$(LIBSUFF) | $(BUILD_DIR)\n\t$(LINK) $^ $(LDLIBS) $(EXEO)$@ $(BUILD_DIR)/libmir.$(LIBSUFF)\n\n.PHONY: clean-mir-bin-run\nclean-mir-bin-run:\n\t$(RM) $(MIR_RUN_BUILD) $(MIR_RUN_BUILD:.$(OBJSUFF)=.d)\n\n-include $(MIR_RUN_BUILD:.$(OBJSUFF)=.d)\n\n# ------------------ L2M --------------------------\nL2M_SRC:=$(SRC_DIR)/llvm2mir/llvm2mir.c $(SRC_DIR)/llvm2mir/llvm2mir-driver.c\nL2M_BUILD:=$(L2M_SRC:$(SRC_DIR)/%.c=$(BUILD_DIR)/%.$(OBJSUFF))\n\n$(BUILD_DIR)/llvm2mir/%.$(OBJSUFF): $(SRC_DIR)/llvm2mir/%.c | $(BUILD_DIR)/llvm2mir\n\t$(COMPILE) -c $< $(OBJO)$@\n\n$(BUILD_DIR)/l2m$(EXE): $(L2M_BUILD) $(BUILD_DIR)/libmir.$(LIBSUFF) | $(BUILD_DIR)\n\t$(LINK) $^ $(LDLIBS) -lLLVM $(OBJO)$@ $(BUILD_DIR)/libmir.$(LIBSUFF)\n\n$(BUILD_DIR)/llvm2mir:\n\t   mkdir -p $@\n\n.PHONY: clean-l2m\nclean-l2m:\n\t$(RM) $(L2M_BUILD) $(L2M_BUILD:.$(OBJSUFF)=.d)\n\n-include $(L2M_BUILD:.$(OBJSUFF)=.d)\n\n# ------------------ Common for utils -------------\n\n$(BUILD_DIR)/mir-utils:\n\t   mkdir -p $@\n\n$(BUILD_DIR)/mir-utils/%.$(OBJSUFF): $(SRC_DIR)/mir-utils/%.c | $(BUILD_DIR)/mir-utils\n\t$(COMPILE) -c $< $(OBJO)$@\n\n.PHONY: clean-utils\nclean-utils: clean-m2b clean-b2m clean-b2ctab\n\n# ------------------ M2B --------------------------\nM2B_SRC:=$(SRC_DIR)/mir-utils/m2b.c\nM2B_BUILD:=$(M2B_SRC:$(SRC_DIR)/%.c=$(BUILD_DIR)/%.$(OBJSUFF))\n\n$(BUILD_DIR)/m2b$(EXE): $(M2B_BUILD) $(BUILD_DIR)/libmir.$(LIBSUFF) | $(BUILD_DIR)\n\t$(LINK) $^ $(LDLIBS) $(EXEO)$@ $(BUILD_DIR)/libmir.$(LIBSUFF)\n\n.PHONY: clean-m2b\nclean-m2b:\n\t$(RM) $(M2B_BUILD) $(M2B_BUILD:.$(OBJSUFF)=.d)\n\n-include $(M2B_BUILD:.$(OBJSUFF)=.d)\n\n# ------------------ B2M --------------------------\nB2M_SRC:=$(SRC_DIR)/mir-utils/b2m.c\nB2M_BUILD:=$(B2M_SRC:$(SRC_DIR)/%.c=$(BUILD_DIR)/%.$(OBJSUFF))\n\n$(BUILD_DIR)/b2m$(EXE): $(B2M_BUILD) $(BUILD_DIR)/libmir.$(LIBSUFF) | $(BUILD_DIR)\n\t$(LINK) $^ $(LDLIBS) $(EXEO)$@ $(BUILD_DIR)/libmir.$(LIBSUFF)\n\n.PHONY: clean-b2m\nclean-b2m:\n\t$(RM) $(B2M_BUILD) $(B2M_BUILD:.$(OBJSUFF)=.d)\n\n-include $(B2M_BUILD:.$(OBJSUFF)=.d)\n\n# ------------------ B2CTAB --------------------------\nB2CTAB_SRC:=$(SRC_DIR)/mir-utils/b2ctab.c\nB2CTAB_BUILD:=$(B2CTAB_SRC:$(SRC_DIR)/%.c=$(BUILD_DIR)/%.$(OBJSUFF))\n\n$(BUILD_DIR)/b2ctab$(EXE): $(B2CTAB_BUILD) $(BUILD_DIR)/libmir.$(LIBSUFF) | $(BUILD_DIR)\n\t$(LINK) $^ $(LDLIBS) $(EXEO)$@  $(BUILD_DIR)/libmir.$(LIBSUFF)\n\n.PHONY: clean-b2ctab\nclean-b2ctab:\n\t$(RM) $(B2CTAB_BUILD) $(B2CTAB_BUILD:.$(OBJSUFF)=.d)\n\n-include $(B2CTAB_BUILD:.$(OBJSUFF)=.d)\n\n# ------------------ ADT tests --------------------------\n\n.PHONY: clean-adt-tests\n.PHONY: adt-test varr-test dlist-test bitmap-test htab-test reduce-test\n\nadt-test: varr-test dlist-test bitmap-test htab-test reduce-test\n\nvarr-test: $(BUILD_DIR)/adt-tests\n\t$(COMPILE_AND_LINK) $(SRC_DIR)/adt-tests/mir-varr-test.c $(EXEO)$(BUILD_DIR)/adt-tests/varr-test$(EXE)\n\t$(BUILD_DIR)/adt-tests/varr-test$(EXE)\n\ndlist-test: $(BUILD_DIR)/adt-tests\n\t$(COMPILE_AND_LINK) $(SRC_DIR)/adt-tests/mir-dlist-test.c $(EXEO)$(BUILD_DIR)/adt-tests/dlist-test$(EXE)\n\t$(BUILD_DIR)/adt-tests/dlist-test$(EXE)\n\nbitmap-test: $(BUILD_DIR)/adt-tests\n\t$(COMPILE_AND_LINK) $(SRC_DIR)/adt-tests/mir-bitmap-test.c $(EXEO)$(BUILD_DIR)/adt-tests/bitmap-test$(EXE)\n\t$(BUILD_DIR)/adt-tests/bitmap-test$(EXE)\n\nhtab-test: $(BUILD_DIR)/adt-tests\n\t$(COMPILE_AND_LINK) $(SRC_DIR)/adt-tests/mir-htab-test.c $(EXEO)$(BUILD_DIR)/adt-tests/htab-test$(EXE)\n\t$(BUILD_DIR)/adt-tests/htab-test$(EXE)\n\nreduce-test: $(BUILD_DIR)/adt-tests\n\t$(COMPILE_AND_LINK) $(SRC_DIR)/adt-tests/mir-reduce-test.c $(EXEO)$(BUILD_DIR)/adt-tests/reduce-test$(EXE)\n\t$(BUILD_DIR)/adt-tests/reduce-test$(EXE) $(SRC_DIR)/c2mir/c2mir.c\n\n$(BUILD_DIR)/adt-tests:\n\tmkdir -p $@\n\nclean-adt-tests:\n\t$(RM) $(BUILD_DIR)/adt-tests/varr-test$(EXE) $(BUILD_DIR)/adt-tests/dlist-test$(EXE)\n\t$(RM) $(BUILD_DIR)/adt-tests/bitmap-test$(EXE)\n\t$(RM) $(BUILD_DIR)/adt-tests/htab-test$(EXE) $(BUILD_DIR)/adt-tests/reduce-test$(EXE)\n\n# ------------------ Common for MIR tests ---------------------\n\n$(BUILD_DIR)/mir-tests:\n\tmkdir -p $@\n\n$(BUILD_DIR)/run-test$(EXE): $(BUILD_DIR)/mir.$(OBJSUFF) $(BUILD_DIR)/mir-gen.$(OBJSUFF) $(SRC_DIR)/mir-tests/run-test.c | $(BUILD_DIR)/mir-tests\n\t$(COMPILE_AND_LINK) $^ $(LDLIBS) $(EXEO)$@\n\n.PHONY: clean-mir-tests\nclean-mir-tests: clean-mir-utility-tests clean-mir-interp-tests clean-mir-gen-tests clean-readme-example-test\n\t$(RM) $(BUILD_DIR)/run-test$(EXE)\n\n# ------------------ MIR utility tests ------------------------\n\n.PHONY: simplify-test scan-test io-test clean-mir-utility-tests\n\nsimplify-test: $(BUILD_DIR)/mir.$(OBJSUFF) $(SRC_DIR)/mir-tests/simplify.c\n\t$(COMPILE_AND_LINK) $^ $(EXEO)$(BUILD_DIR)/simplify-test $(LDLIBS) && $(BUILD_DIR)/simplify-test$(EXE)\n\nhello-test: $(BUILD_DIR)/mir.$(OBJSUFF) $(BUILD_DIR)/mir-gen.$(OBJSUFF) $(SRC_DIR)/mir-tests/hello.c\n\t$(COMPILE_AND_LINK) $^ $(EXEO)$(BUILD_DIR)/hello-test $(LDLIBS) && $(BUILD_DIR)/hello-test$(EXE)\n\nscan-test: $(BUILD_DIR)/mir.$(OBJSUFF) $(SRC_DIR)/mir-tests/scan-test.c\n\t$(COMPILE_AND_LINK) $^ $(EXEO)$(BUILD_DIR)/scan-test $(LDLIBS) && $(BUILD_DIR)/scan-test$(EXE)\n\nio-test: $(BUILD_DIR)/mir.$(OBJSUFF) $(SRC_DIR)/mir-tests/io.c\n\t$(COMPILE_AND_LINK) $^ $(EXEO)$(BUILD_DIR)/io-test $(LDLIBS) && $(BUILD_DIR)/io-test$(EXE)\n\nclean-mir-utility-tests:\n\t$(RM) $(BUILD_DIR)/run-test$(EXE) $(BUILD_DIR)/simplify-test$(EXE)\n\t$(RM) $(BUILD_DIR)/hello-test$(EXE)\n\t$(RM) $(BUILD_DIR)/scan-test$(EXE) $(BUILD_DIR)/io-test$(EXE)\n\n# ------------------ MIR interp tests --------------------------\n\n.PHONY: clean-mir-interp-tests\n.PHONY: interp-test interp-test1 interp-test2 interp-test3 interp-test4 interp-test5 interp-test6 interp-test7\n.PHONY: interp-test8 interp-test9 interp-test10 interp-test11 interp-test12 interp-test13 interp-test14\n.PHONY: interp-test15 interp-test16\n\ninterp-test: interp-test1 interp-test2 interp-test3 interp-test4 interp-test5 interp-test6 interp-test7\\\n\t     interp-test8 interp-test9 interp-test10 interp-test11 interp-test12 interp-test13 interp-test14\\\n\t     interp-test15 interp-test16\n\ninterp-test1: $(BUILD_DIR)/mir.$(OBJSUFF) $(SRC_DIR)/mir-tests/loop-interp.c | $(BUILD_DIR)/mir-tests\n\t$(COMPILE_AND_LINK) -DMIR_INTERP_DEBUG=1 $^ $(EXEO)$(BUILD_DIR)/mir-tests/interp-test1$(EXE)\n\t$(BUILD_DIR)/mir-tests/interp-test1$(EXE)\n\ninterp-test2: $(BUILD_DIR)/mir.$(OBJSUFF) $(SRC_DIR)/mir-tests/loop-interp.c | $(BUILD_DIR)/mir-tests\n\t$(COMPILE_AND_LINK) -DMIR_INTERP_DEBUG=1 -DMIR_C_INTERFACE=1 $^ $(EXEO)$(BUILD_DIR)/mir-tests/interp-test2$(EXE)\n\t$(BUILD_DIR)/mir-tests/interp-test2$(EXE)\n\ninterp-test3: $(BUILD_DIR)/mir.$(OBJSUFF) $(SRC_DIR)/mir-tests/sieve-interp.c | $(BUILD_DIR)/mir-tests\n\t$(COMPILE_AND_LINK) -DMIR_INTERP_DEBUG=1 $^ $(EXEO)$(BUILD_DIR)/mir-tests/interp-test3$(EXE)\n\t$(BUILD_DIR)/mir-tests/interp-test3$(EXE)\n\ninterp-test4:  $(BUILD_DIR)/mir.$(OBJSUFF) $(SRC_DIR)/mir-tests/sieve-interp.c | $(BUILD_DIR)/mir-tests\n\t$(COMPILE_AND_LINK) -DMIR_INTERP_DEBUG=1 -DMIR_C_INTERFACE=1 $^ $(EXEO)$(BUILD_DIR)/mir-tests/interp-test4$(EXE)\n\t$(BUILD_DIR)/mir-tests/interp-test4$(EXE)\n\ninterp-test5: $(BUILD_DIR)/mir.$(OBJSUFF) $(SRC_DIR)/mir-tests/hi-interp.c | $(BUILD_DIR)/mir-tests\n\t$(COMPILE_AND_LINK) -DMIR_INTERP_DEBUG=1 $^ $(EXEO)$(BUILD_DIR)/mir-tests/interp-test5$(EXE)\n\t$(BUILD_DIR)/mir-tests/interp-test5$(EXE)\n\ninterp-test6: $(BUILD_DIR)/mir.$(OBJSUFF) $(SRC_DIR)/mir-tests/args-interp.c | $(BUILD_DIR)/mir-tests\n\t$(COMPILE_AND_LINK) $^ $(EXEO)$(BUILD_DIR)/mir-tests/interp-test6$(EXE)\n\t$(BUILD_DIR)/mir-tests/interp-test6$(EXE)\n\ninterp-test7: $(BUILD_DIR)/mir.$(OBJSUFF) $(SRC_DIR)/mir-tests/args-interp.c | $(BUILD_DIR)/mir-tests\n\t$(COMPILE_AND_LINK) -DMIR_C_INTERFACE=1 $^ $(EXEO)$(BUILD_DIR)/mir-tests/interp-test7$(EXE)\n\t$(BUILD_DIR)/mir-tests/interp-test7$(EXE)\n\ninterp-test8: $(BUILD_DIR)/run-test$(EXE)\n\t$(BUILD_DIR)/run-test$(EXE) -i $(SRC_DIR)/mir-tests/test8.mir\n\ninterp-test9: $(BUILD_DIR)/run-test$(EXE)\n\t$(BUILD_DIR)/run-test$(EXE) -i $(SRC_DIR)/mir-tests/test9.mir\n\ninterp-test10: $(BUILD_DIR)/run-test$(EXE)\n\t$(BUILD_DIR)/run-test$(EXE) -i $(SRC_DIR)/mir-tests/test10.mir\n\ninterp-test11: $(BUILD_DIR)/run-test$(EXE)\n\t$(BUILD_DIR)/run-test$(EXE) -i $(SRC_DIR)/mir-tests/test11.mir\n\ninterp-test12: $(BUILD_DIR)/run-test$(EXE)\nifeq ($(OS),Windows_NT)\n\techo Skipping test with multiple returns\nelse\n\t$(BUILD_DIR)/run-test$(EXE) -i $(SRC_DIR)/mir-tests/test12.mir\nendif\n\ninterp-test13: $(BUILD_DIR)/run-test$(EXE)\n\t$(BUILD_DIR)/run-test$(EXE) -i $(SRC_DIR)/mir-tests/test13.mir\n\ninterp-test14: $(BUILD_DIR)/run-test$(EXE)\n\t$(BUILD_DIR)/run-test$(EXE) -i $(SRC_DIR)/mir-tests/test14.mir\n\ninterp-test15: $(BUILD_DIR)/run-test$(EXE)\n\t$(BUILD_DIR)/run-test$(EXE) -i $(SRC_DIR)/mir-tests/test15.mir\n\ninterp-test16: $(BUILD_DIR)/run-test$(EXE)\n\t$(BUILD_DIR)/run-test$(EXE) -i $(SRC_DIR)/mir-tests/test16.mir\n\nclean-mir-interp-tests:\n\t$(RM) $(BUILD_DIR)/mir-tests/interp-test1$(EXE) $(BUILD_DIR)/mir-tests/interp-test2$(EXE)\n\t$(RM) $(BUILD_DIR)/mir-tests/interp-test3$(EXE) $(BUILD_DIR)/mir-tests/interp-test4$(EXE)\n\t$(RM) $(BUILD_DIR)/mir-tests/interp-test5$(EXE) $(BUILD_DIR)/mir-tests/interp-test6$(EXE)\n\t$(RM) $(BUILD_DIR)/mir-tests/interp-test7$(EXE)\n\n# ------------------ MIR gen tests --------------------------\n\n.PHONY: clean-mir-gen-tests\n.PHONY: gen-test gen-test-loop gen-test-sieve gen-issue219-test gen-test-get-thunk-addr\n.PHONY: gen-test1 gen-test2 gen-test3 gen-test4 gen-test5 gen-test6 gen-test7\n.PHONY: gen-test8 gen-test9 gen-test10 gen-test11 gen-test12 gen-test13 gen-test14 gen-test15 gen-test16\n\ngen-test: gen-test-loop gen-test-sieve gen-test-get-thunk-addr gen-issue219-test gen-test1 gen-test2 gen-test3 gen-test4 gen-test5 gen-test6 gen-test7\\\n          gen-test8 gen-test9 gen-test10 gen-test11 gen-test12 gen-test13 gen-test14 gen-test15 gen-test16\n\ngen-test-loop: $(BUILD_DIR)/mir.$(OBJSUFF) $(BUILD_DIR)/mir-gen.$(OBJSUFF) $(SRC_DIR)/mir-tests/loop-sieve-gen.c | $(BUILD_DIR)/mir-tests\n\t$(COMPILE_AND_LINK) -DTEST_GEN_LOOP -DTEST_GEN_DEBUG=1 $^ $(LDLIBS) $(EXEO)$(BUILD_DIR)/mir-tests/gen-loop-test$(EXE)\n\t$(BUILD_DIR)/mir-tests/gen-loop-test\n\ngen-test-sieve: $(BUILD_DIR)/mir.$(OBJSUFF) $(BUILD_DIR)/mir-gen.$(OBJSUFF) $(SRC_DIR)/mir-tests/loop-sieve-gen.c | $(BUILD_DIR)/mir-tests\n\t$(COMPILE_AND_LINK) -DTEST_GEN_SIEVE -DTEST_GEN_DEBUG=1 $^ $(LDLIBS) $(EXEO)$(BUILD_DIR)/mir-tests/gen-sieve-test$(EXE)\n\t$(BUILD_DIR)/mir-tests/gen-sieve-test\n\ngen-issue219-test: $(BUILD_DIR)/mir.$(OBJSUFF) $(BUILD_DIR)/mir-gen.$(OBJSUFF) $(SRC_DIR)/mir-tests/issue219.c | $(BUILD_DIR)/mir-tests\n\t$(COMPILE_AND_LINK) -DTEST_GEN_SIEVE -DTEST_GEN_DEBUG=1 $^ $(LDLIBS) $(EXEO)$(BUILD_DIR)/mir-tests/issue219$(EXE)\n\t$(BUILD_DIR)/mir-tests/issue219\n\ngen-test-get-thunk-addr: $(BUILD_DIR)/mir.$(OBJSUFF) $(BUILD_DIR)/mir-gen.$(OBJSUFF) $(SRC_DIR)/mir-tests/get-thunk-addr.c | $(BUILD_DIR)/mir-tests\n\t$(COMPILE_AND_LINK) $^ $(LDLIBS) $(EXEO)$(BUILD_DIR)/mir-tests/gen-get-thunk-addr-test$(EXE)\n\t$(BUILD_DIR)/mir-tests/gen-get-thunk-addr-test\n\ngen-test1: $(BUILD_DIR)/run-test$(EXE)\n\t$(BUILD_DIR)/run-test$(EXE) -d $(SRC_DIR)/mir-tests/test1.mir\n\ngen-test2: $(BUILD_DIR)/run-test$(EXE)\n\t$(BUILD_DIR)/run-test$(EXE) -d $(SRC_DIR)/mir-tests/test2.mir\n\ngen-test3: $(BUILD_DIR)/run-test$(EXE)\n\t$(BUILD_DIR)/run-test$(EXE) -d $(SRC_DIR)/mir-tests/test3.mir\n\ngen-test4: $(BUILD_DIR)/run-test$(EXE)\n\t$(BUILD_DIR)/run-test$(EXE) -d $(SRC_DIR)/mir-tests/test4.mir\n\ngen-test5: $(BUILD_DIR)/run-test$(EXE)\n\t$(BUILD_DIR)/run-test$(EXE) -d $(SRC_DIR)/mir-tests/test5.mir\n\ngen-test6: $(BUILD_DIR)/run-test$(EXE)\n\t$(BUILD_DIR)/run-test$(EXE) -d $(SRC_DIR)/mir-tests/test6.mir\n\ngen-test7: $(BUILD_DIR)/run-test$(EXE)\n\t$(BUILD_DIR)/run-test$(EXE) -d $(SRC_DIR)/mir-tests/test7.mir\n\ngen-test8: $(BUILD_DIR)/run-test$(EXE)\n\t$(BUILD_DIR)/run-test$(EXE) -g $(SRC_DIR)/mir-tests/test8.mir\n\ngen-test9: $(BUILD_DIR)/run-test$(EXE)\n\t$(BUILD_DIR)/run-test$(EXE) -gd $(SRC_DIR)/mir-tests/test9.mir\n\ngen-test10: $(BUILD_DIR)/run-test$(EXE)\n\t$(BUILD_DIR)/run-test$(EXE) -g $(SRC_DIR)/mir-tests/test10.mir\n\ngen-test11: $(BUILD_DIR)/run-test$(EXE)\n\t$(BUILD_DIR)/run-test$(EXE) -gd $(SRC_DIR)/mir-tests/test11.mir\n\ngen-test12: $(BUILD_DIR)/run-test$(EXE)\nifeq ($(OS),Windows_NT)\n\techo Skipping test with multiple returns\nelse\n\t$(BUILD_DIR)/run-test$(EXE) -gd $(SRC_DIR)/mir-tests/test12.mir\nendif\n\ngen-test13: $(BUILD_DIR)/run-test$(EXE)\n\t$(BUILD_DIR)/run-test$(EXE) -g $(SRC_DIR)/mir-tests/test13.mir\n\ngen-test14: $(BUILD_DIR)/run-test$(EXE)\n\t$(BUILD_DIR)/run-test$(EXE) -g $(SRC_DIR)/mir-tests/test14.mir\n\ngen-test15: $(BUILD_DIR)/run-test$(EXE)\n\t$(BUILD_DIR)/run-test$(EXE) -g $(SRC_DIR)/mir-tests/test15.mir\n\ngen-test16: $(BUILD_DIR)/run-test$(EXE)\n\t$(BUILD_DIR)/run-test$(EXE) -g $(SRC_DIR)/mir-tests/test16.mir\n\nclean-mir-gen-tests:\n\t$(RM) $(BUILD_DIR)/mir-tests/gen-loop-test$(EXE) $(BUILD_DIR)/mir-tests/gen-sieve-test$(EXE)\n\t$(RM) $(BUILD_DIR)/mir-tests/issue219$(EXE) $(BUILD_DIR)/mir-tests/gen-get-thunk-addr-test\n\n# ------------------ MIR run tests --------------------------\n\nmir-bin-run-test: $(BUILD_DIR)/mir-bin-run$(EXE) $(BUILD_DIR)/c2m$(EXE)\n\t      $(BUILD_DIR)/c2m$(EXE) -c $(SRC_DIR)/sieve.c\n\t      $(BUILD_DIR)/mir-bin-run$(EXE) `pwd`/sieve.bmir sieve.bmir\n\t      MIR_TYPE=interp $(BUILD_DIR)/mir-bin-run$(EXE) `pwd`/sieve.bmir sieve.bmir\n\t      MIR_TYPE=gen $(BUILD_DIR)/mir-bin-run$(EXE) `pwd`/sieve.bmir sieve.bmir\n\t      MIR_TYPE=lazy $(BUILD_DIR)/mir-bin-run$(EXE) `pwd`/sieve.bmir sieve.bmir\n\t      rm sieve.bmir\n\t\n\n# ------------------ readme example test ----------------\n\n.PHONY: readme-example-test clean-readme-example-test\n\nreadme-example-test: $(BUILD_DIR)/mir.$(OBJSUFF) $(BUILD_DIR)/mir-gen.$(OBJSUFF)\\\n                           $(SRC_DIR)/mir-tests/readme-example.c | $(BUILD_DIR)/mir-tests\n\t$(COMPILE_AND_LINK) $^ $(LDLIBS) $(EXEO)$(BUILD_DIR)/mir-tests/readme-example-test$(EXE)\n\t$(BUILD_DIR)/mir-tests/readme-example-test$(EXE)\n\nclean-readme-example-test:\n\t$(RM) $(BUILD_DIR)/mir-tests/readme-example-test$(EXE)\n\n# ------------------ mir2c test -------------------------\n\n.PHONY: mir2c-test clean-mir2c-test\n\nmir2c-test: $(BUILD_DIR)/mir.$(OBJSUFF) $(SRC_DIR)/mir2c/mir2c.c\n\t$(COMPILE_AND_LINK) -DTEST_MIR2C $^ $(EXEO)$(BUILD_DIR)/mir2c-test $(LDLIBS) && $(BUILD_DIR)/mir2c-test$(EXE)\n\nclean-mir2c-test:\n\t$(RM) $(BUILD_DIR)/mir2c-test$(EXE)\n\n# ------------------ c2m tests --------------------------\n\n.PHONY: c2mir-test c2mir-simple-test c2mir-full-test c2mir-interp-test\n.PHONY: c2mir-gen-test c2mir-parallel-gen-test c2mir-gen-test0 c2mir-gen-test1 c2mir-gen-test3\n\nc2mir-test: c2mir-simple-test c2mir-full-test\n\nc2mir-simple-test: $(BUILD_DIR)/c2m$(EXE)\n\t$(BUILD_DIR)/c2m$(EXE) -v $(SRC_DIR)/sieve.c -ei\n\nc2mir-full-test: c2mir-interp-test c2mir-gen-test c2mir-bb-gen-test c2mir-gen-test0 c2mir-gen-test1 c2mir-gen-test3 c2mir-bootstrap\n\nc2mir-interp-test: $(BUILD_DIR)/c2m$(EXE)\n\t$(SHELL) $(SRC_DIR)/c-tests/runtests.sh $(SRC_DIR)/c-tests/use-c2m-interp $(BUILD_DIR)/c2m$(EXE)\nc2mir-gen-test: $(BUILD_DIR)/c2m$(EXE)\n\t$(SHELL) $(SRC_DIR)/c-tests/runtests.sh $(SRC_DIR)/c-tests/use-c2m-gen $(BUILD_DIR)/c2m$(EXE)\nc2mir-bb-gen-test: $(BUILD_DIR)/c2m$(EXE)\n\t$(SHELL) $(SRC_DIR)/c-tests/runtests.sh $(SRC_DIR)/c-tests/use-c2m-gen-bb $(BUILD_DIR)/c2m$(EXE)\nc2mir-parallel-gen-test: $(BUILD_DIR)/c2m$(EXE)\n\t$(SHELL) $(SRC_DIR)/c-tests/runtests.sh $(SRC_DIR)/c-tests/use-c2m-parallel-gen $(BUILD_DIR)/c2m$(EXE)\nc2mir-gen-test0: $(BUILD_DIR)/c2m$(EXE)\n\t$(SHELL) $(SRC_DIR)/c-tests/runtests.sh $(SRC_DIR)/c-tests/use-c2m-gen-O0 $(BUILD_DIR)/c2m$(EXE)\nc2mir-gen-test1: $(BUILD_DIR)/c2m$(EXE)\n\t$(SHELL) $(SRC_DIR)/c-tests/runtests.sh $(SRC_DIR)/c-tests/use-c2m-gen-O1 $(BUILD_DIR)/c2m$(EXE)\nc2mir-gen-test3: $(BUILD_DIR)/c2m$(EXE)\n\t$(SHELL) $(SRC_DIR)/c-tests/runtests.sh $(SRC_DIR)/c-tests/use-c2m-gen-O3 $(BUILD_DIR)/c2m$(EXE)\n\n# ------------------ c2m bootstrap tests ----------------\n\n.PHONY: c2mir-bootstrap c2mir-bootstrap-test0 c2mir-bootstrap-test1 c2mir-bootstrap-test c2mir-bootstrap-test3\n.PHONY: c2mir-parallel-bootstrap-test c2mir-bootstrap-test4 c2mir-bootstrap-test5\n\nc2mir-bootstrap: c2mir-bootstrap-test c2mir-bootstrap-test0 c2mir-bootstrap-test1 c2mir-bootstrap-test3 c2mir-parallel-bootstrap-test c2mir-bb-bootstrap-test\n\nc2mir-bootstrap-test0: $(BUILD_DIR)/c2m$(EXE)\n\t$(Q) echo -n +++++++ C2MIR Bootstrap lazy func test with -O0 '... '\n\t$(Q) $(BUILD_DIR)/c2m$(EXE) -w $(C2M_BOOTSTRAP_FLAGS) -O0 -I$(SRC_DIR) $(SRC_DIR)/mir-gen.c $(SRC_DIR)/c2mir/c2mir.c\\\n\t                    $(SRC_DIR)/c2mir/c2mir-driver.c $(SRC_DIR)/mir.c -o $(BUILD_DIR)/1o0.bmir\n\t$(Q) $(BUILD_DIR)/c2m$(EXE) $(C2M_BOOTSTRAP_FLAGS) -O0 $(BUILD_DIR)/1o0.bmir -el -w $(C2M_BOOTSTRAP_FLAGS) -O0\\\n\t                    -I$(SRC_DIR) $(SRC_DIR)/mir-gen.c $(SRC_DIR)/c2mir/c2mir.c\\\n\t\t\t    $(SRC_DIR)/c2mir/c2mir-driver.c $(SRC_DIR)/mir.c -o $(BUILD_DIR)/2o0.bmir\n\t$(Q) cmp  $(BUILD_DIR)/1o0.bmir $(BUILD_DIR)/2o0.bmir && echo Passed || echo FAIL\n\t$(Q) rm -rf $(BUILD_DIR)/1o0.bmir $(BUILD_DIR)/2o0.bmir\n\nc2mir-bootstrap-test1: $(BUILD_DIR)/c2m$(EXE)\n\t$(Q) echo -n +++++++ C2MIR Bootstrap lazy func test with -O1 '... '\n\t$(Q) $(BUILD_DIR)/c2m$(EXE) -w $(C2M_BOOTSTRAP_FLAGS) -O1 -I$(SRC_DIR) $(SRC_DIR)/mir-gen.c $(SRC_DIR)/c2mir/c2mir.c\\\n\t                    $(SRC_DIR)/c2mir/c2mir-driver.c $(SRC_DIR)/mir.c -o $(BUILD_DIR)/1o1.bmir\n\t$(Q) $(BUILD_DIR)/c2m$(EXE) $(C2M_BOOTSTRAP_FLAGS) -O1 $(BUILD_DIR)/1o1.bmir -el -w $(C2M_BOOTSTRAP_FLAGS) -O1\\\n\t                    -I$(SRC_DIR) $(SRC_DIR)/mir-gen.c $(SRC_DIR)/c2mir/c2mir.c\\\n\t\t\t    $(SRC_DIR)/c2mir/c2mir-driver.c $(SRC_DIR)/mir.c -o $(BUILD_DIR)/2o1.bmir\n\t$(Q) cmp $(BUILD_DIR)/1o1.bmir $(BUILD_DIR)/2o1.bmir && echo Passed || echo FAIL\n\t$(Q) rm -rf $(BUILD_DIR)/1o1.bmir $(BUILD_DIR)/2o1.bmir\n\nc2mir-bootstrap-test: $(BUILD_DIR)/c2m$(EXE)\n\t$(Q) echo -n +++++++ C2MIR Bootstrap lazy func test with default optimize level '... '\n\t$(Q) $(BUILD_DIR)/c2m$(EXE) -w $(C2M_BOOTSTRAP_FLAGS) -I$(SRC_DIR) $(SRC_DIR)/mir-gen.c $(SRC_DIR)/c2mir/c2mir.c\\\n\t                    $(SRC_DIR)/c2mir/c2mir-driver.c $(SRC_DIR)/mir.c -o $(BUILD_DIR)/1o2.bmir\n\t$(Q) $(BUILD_DIR)/c2m$(EXE) $(C2M_BOOTSTRAP_FLAGS) $(BUILD_DIR)/1o2.bmir -el -w $(C2M_BOOTSTRAP_FLAGS)\\\n\t                    -I$(SRC_DIR) $(SRC_DIR)/mir-gen.c $(SRC_DIR)/c2mir/c2mir.c\\\n\t\t\t     $(SRC_DIR)/c2mir/c2mir-driver.c $(SRC_DIR)/mir.c -o $(BUILD_DIR)/2o2.bmir\n\t$(Q) cmp $(BUILD_DIR)/1o2.bmir $(BUILD_DIR)/2o2.bmir && echo Passed || echo FAIL\n\t$(Q) rm -rf $(BUILD_DIR)/1o2.bmir $(BUILD_DIR)/2o2.bmir\n\nc2mir-bb-bootstrap-test: $(BUILD_DIR)/c2m$(EXE)\n\t$(Q) echo -n +++++++ C2MIR Bootstrap lazy bb test with default optimize level '... '\n\t$(Q) $(BUILD_DIR)/c2m$(EXE) -w $(C2M_BOOTSTRAP_FLAGS) -I$(SRC_DIR) $(SRC_DIR)/mir-gen.c $(SRC_DIR)/c2mir/c2mir.c\\\n\t                    $(SRC_DIR)/c2mir/c2mir-driver.c $(SRC_DIR)/mir.c -o $(BUILD_DIR)/1o2.bmir\n\t$(Q) $(BUILD_DIR)/c2m$(EXE) $(C2M_BOOTSTRAP_FLAGS) $(BUILD_DIR)/1o2.bmir -p4 -eb -w $(C2M_BOOTSTRAP_FLAGS)\\\n\t                    -I$(SRC_DIR) $(SRC_DIR)/mir-gen.c $(SRC_DIR)/c2mir/c2mir.c\\\n\t\t\t     $(SRC_DIR)/c2mir/c2mir-driver.c $(SRC_DIR)/mir.c -o $(BUILD_DIR)/2o2.bmir\n\t$(Q) cmp $(BUILD_DIR)/1o2.bmir $(BUILD_DIR)/2o2.bmir && echo Passed || echo FAIL\n\t$(Q) rm -rf $(BUILD_DIR)/1o2.bmir $(BUILD_DIR)/2o2.bmir\n\nc2mir-bootstrap-test3: $(BUILD_DIR)/c2m$(EXE)\n\t$(Q) echo -n +++++++ C2MIR Bootstrap lazy func test with -O3 '... '\n\t$(Q) $(BUILD_DIR)/c2m$(EXE) -w $(C2M_BOOTSTRAP_FLAGS) -O3 -I$(SRC_DIR) $(SRC_DIR)/mir-gen.c $(SRC_DIR)/c2mir/c2mir.c\\\n\t                    $(SRC_DIR)/c2mir/c2mir-driver.c $(SRC_DIR)/mir.c -o $(BUILD_DIR)/1o3.bmir\n\t$(Q) $(BUILD_DIR)/c2m$(EXE) $(C2M_BOOTSTRAP_FLAGS) -O3 $(BUILD_DIR)/1o3.bmir -el -w $(C2M_BOOTSTRAP_FLAGS) -O3\\\n\t                    -I$(SRC_DIR) $(SRC_DIR)/mir-gen.c $(SRC_DIR)/c2mir/c2mir.c\\\n\t\t\t    $(SRC_DIR)/c2mir/c2mir-driver.c $(SRC_DIR)/mir.c -o $(BUILD_DIR)/2o3.bmir\n\t$(Q) cmp $(BUILD_DIR)/1o3.bmir $(BUILD_DIR)/2o3.bmir && echo Passed || echo FAIL\n\t$(Q) rm -rf $(BUILD_DIR)/1o3.bmir $(BUILD_DIR)/2o3.bmir\n\nc2mir-parallel-bootstrap-test: $(BUILD_DIR)/c2m$(EXE)\n\t$(Q) echo -n +++++++ C2MIR Parallel Bootstrap Test with default optimize level '... '\n\t$(Q) $(BUILD_DIR)/c2m$(EXE) -w -p4 $(C2M_BOOTSTRAP_FLAGS) -I$(SRC_DIR) $(SRC_DIR)/mir-gen.c $(SRC_DIR)/c2mir/c2mir.c\\\n\t                    $(SRC_DIR)/c2mir/c2mir-driver.c $(SRC_DIR)/mir.c -o $(BUILD_DIR)/1p2.bmir\n\t$(Q) $(BUILD_DIR)/c2m$(EXE) -p4 $(C2M_BOOTSTRAP_FLAGS) $(BUILD_DIR)/1p2.bmir -eg -w -p4 $(C2M_BOOTSTRAP_FLAGS)\\\n\t                    -I$(SRC_DIR) $(SRC_DIR)/mir-gen.c $(SRC_DIR)/c2mir/c2mir.c\\\n\t\t\t    $(SRC_DIR)/c2mir/c2mir-driver.c $(SRC_DIR)/mir.c -o $(BUILD_DIR)/2p2.bmir\n\t$(Q) cmp $(BUILD_DIR)/1p2.bmir $(BUILD_DIR)/2p2.bmir && echo Passed || echo FAIL\n\t$(Q) rm -rf $(BUILD_DIR)/1p2.bmir $(BUILD_DIR)/2p2.bmir\n\nc2mir-bootstrap-test4: $(BUILD_DIR)/c2m$(EXE) $(BUILD_DIR)/b2ctab$(EXE)\n\t$(Q) echo -n +++++++ C2MIR Bootstrap Test 2 '(usually it takes about 10-20 sec) ... '\n\t$(Q) $(BUILD_DIR)/c2m$(EXE) -w $(C2M_BOOTSTRAP_FLAGS) -I$(SRC_DIR) $(SRC_DIR)/mir-gen.c $(SRC_DIR)/c2mir/c2mir.c\\\n\t                    $(SRC_DIR)/c2mir/c2mir-driver.c $(SRC_DIR)/mir.c -o $(BUILD_DIR)/t1.bmir\n\t$(Q) $(BUILD_DIR)/b2ctab$(EXE) < $(BUILD_DIR)/t1.bmir > $(BUILD_DIR)/mir-ctab\n\t$(Q) $(COMPILE_AND_LINK) -w $(SRC_DIR)/mir.c $(SRC_DIR)/mir-gen.c $(SRC_DIR)/mir-bin-driver.c\\\n\t                         $(LDLIBS) -o $(BUILD_DIR)/c2m-test$(EXE)\n\t$(Q) $(BUILD_DIR)/c2m-test$(EXE) $(C2M_BOOTSTRAP_FLAGS) -w -I$(SRC_DIR) $(SRC_DIR)/mir-gen.c\\\n\t                         $(SRC_DIR)/c2mir/c2mir.c $(SRC_DIR)/c2mir/c2mir-driver.c\\\n\t\t\t\t $(SRC_DIR)/mir.c -o $(BUILD_DIR)/t2.bmir\n\t$(Q) cmp $(BUILD_DIR)/t1.bmir $(BUILD_DIR)/t2.bmir && echo Passed || echo FAIL\n\t$(Q) rm -rf $(BUILD_DIR)/t1.bmir $(BUILD_DIR)/t2.bmir $(BUILD_DIR)/mir-ctab $(BUILD_DIR)/c2m-test$(EXE)\n\nc2mir-bootstrap-test5: $(BUILD_DIR)/c2m$(EXE)\n\t$(Q) echo -n +++++++ C2MIR Bootstrap Interpreter Test '... '\n\t$(Q) $(BUILD_DIR)/c2m$(EXE) -w $(C2M_BOOTSTRAP_FLAGS0) -I$(SRC_DIR) $(SRC_DIR)/mir-gen.c\\\n\t                    $(SRC_DIR)/c2mir/c2mir.c $(SRC_DIR)/c2mir/c2mir-driver.c\\\n\t\t\t    $(SRC_DIR)/mir.c -o $(BUILD_DIR)/i1.bmir\n\t$(Q) $(BUILD_DIR)/c2m$(EXE) $(C2M_BOOTSTRAP_FLAGS0) $(BUILD_DIR)/i1.bmir -ei -w $(C2M_BOOTSTRAP_FLAGS0)\\\n\t                    -I$(SRC_DIR) $(SRC_DIR)/mir-gen.c $(SRC_DIR)/c2mir/c2mir.c\\\n\t\t\t    $(SRC_DIR)/c2mir/c2mir-driver.c $(SRC_DIR)/mir.c -o $(BUILD_DIR)/i2.bmir\n\t$(Q) cmp $(BUILD_DIR)/i1.bmir $(BUILD_DIR)/i2.bmir && echo Passed || echo FAIL\n\t$(Q) rm -rf $(BUILD_DIR)/i1.bmir $(BUILD_DIR)/i2.bmir\n\n# ------------------ l2m tests --------------------------\n\n.PHONY:  l2m-test l2m-simple-test l2m-full-test l2m-interp-test l2m-gen-test l2m-test1 l2m-test2\n\nl2m-test: l2m-simple-test # l2m-full-test\n\nl2m-simple-test: l2m-test1 l2m-test2\n\nl2m-full-test: l2m-interp-test l2m-gen-test\n\nl2m-interp-test: $(BUILD_DIR)/l2m\n\t$(SHELL) c-tests/runtests.sh c-tests/use-l2m-interp $(BUILD_DIR)/c2m\nl2m-gen-test: $(BUILD_DIR)/l2m\n\t$(SHELL) c-tests/runtests.sh c-tests/use-l2m-gen $(BUILD_DIR)/c2m\n\nl2m-test1: $(BUILD_DIR)/l2m\n\t@echo +++++ LLVM to MIR translator test '(-O0)' +++++++\n\tclang -O0 -fno-vectorize -w -c -emit-llvm $(SRC_DIR)/sieve.c -o $(BUILD_DIR)/sieve.bc\n\t@echo +++++ Interpreter +++++++ && $(BUILD_DIR)/l2m -i $(BUILD_DIR)/sieve.bc\n\t@echo +++++ Generator +++++++ && $(BUILD_DIR)/l2m -g $(BUILD_DIR)/sieve.bc\n\nl2m-test2: $(BUILD_DIR)/l2m\n\t@echo +++++ LLVM to MIR translator test '(-O2)' +++++++\n\tclang -O2 -fno-vectorize -w -c -emit-llvm $(SRC_DIR)/sieve.c -o $(BUILD_DIR)/sieve.bc\n\t@echo +++++ Interpreter +++++++ && $(BUILD_DIR)/l2m -i $(BUILD_DIR)/sieve.bc\n\t@echo +++++ Generator +++++++ && $(BUILD_DIR)/l2m -g $(BUILD_DIR)/sieve.bc\n\n# ------------------ benchmarks -------------------------\n\n.PHONY: clean-bench\n.PHONY: io-bench interp-bench gen-bench gen-bench2 gen-speed c2mir-sieve-bench c2mir-bench mir2c-bench\n\nio-bench: $(BUILD_DIR)/mir.$(OBJSUFF) $(SRC_DIR)/mir-tests/io-bench.c\n\t@echo ========io-bench can take upto 2 min===============\n\t$(COMPILE_AND_LINK) $^ $(EXEO)$(BUILD_DIR)/io-bench $(LDLIBS) && $(BUILD_DIR)/io-bench$(EXE)\n\ninterp-bench: $(BUILD_DIR)/mir.$(OBJSUFF) $(SRC_DIR)/mir-tests/loop-interp.c\n\t$(COMPILE_AND_LINK) -DMIR_C_INTERFACE=1 $^ $(EXEO)$(BUILD_DIR)/interp-bench$(EXE) $(LDLIBS)\n\t$(BUILD_DIR)/interp-bench && size $(BUILD_DIR)/interp-bench\n\t$(COMPILE_AND_LINK) $^ $(EXEO)$(BUILD_DIR)/interp-bench$(EXE) $(LDLIBS)\n\t$(BUILD_DIR)/interp-bench && size $(BUILD_DIR)/interp-bench\n\ngen-bench: $(BUILD_DIR)/mir.$(OBJSUFF) $(BUILD_DIR)/mir-gen.$(OBJSUFF) $(SRC_DIR)/mir-tests/loop-sieve-gen.c\n\t$(COMPILE_AND_LINK) -DTEST_GEN_LOOP $^ $(EXEO)$(BUILD_DIR)/gen-bench$(EXE) $(LDLIBS)\n\t$(BUILD_DIR)/gen-bench && size $(BUILD_DIR)/gen-bench\n\t$(COMPILE_AND_LINK) -DTEST_GEN_SIEVE $^ $(EXEO)$(BUILD_DIR)/gen-bench$(EXE) $(LDLIBS)\n\t$(BUILD_DIR)/gen-bench && size $(BUILD_DIR)/gen-bench\n\ngen-bench2: $(BUILD_DIR)/c2m # Ignore M1 MacOs as it needs another procedure to make code executable\n\t@if test $(UNAME_S) != Darwin || test $(UNAME_M) != arm64; then\\\n\t  echo +++++ Compiling and generating all code for c2m: +++++;\\\n\t  for i in 0 1 2 3;do \\\n\t    echo === Optimization level $$i:;\\\n        echo 'int main () {return 0;}' > __a.c;\\\n\t    time $(BUILD_DIR)/c2m -O$$i -Dx86_64 -I$(SRC_DIR) $(SRC_DIR)/mir-gen.c $(SRC_DIR)/c2mir/c2mir.c\\\n\t                       $(SRC_DIR)/c2mir/c2mir-driver.c $(SRC_DIR)/mir.c -el -i -o __a.bmir < __a.c;\\\n\t    rm -f __a.c __a.bmir;\\\n\t  done;\\\n\tfi\n\ngen-speed: $(BUILD_DIR)/mir.$(OBJSUFF) $(BUILD_DIR)/mir-gen.$(OBJSUFF) $(SRC_DIR)/mir-tests/loop-sieve-gen.c\n\tif type valgrind  > /dev/null 2>&1; then \\\n\t  $(COMPILE_AND_LINK) -DTEST_GEN_SIEVE -DTEST_GENERATION_ONLY $^ $(LDLIBS) $(EXEO)$(BUILD_DIR)/gen-speed$(EXE)\\\n\t  && valgrind --tool=lackey $(BUILD_DIR)/gen-speed; \\\n\tfi\n\nc2mir-sieve-bench: $(BUILD_DIR)/c2m\n\t$(BUILD_DIR)/c2m -DSIEVE_BENCH -v $(SRC_DIR)/sieve.c -eg && size $(BUILD_DIR)/c2m\n\nc2mir-bench: $(BUILD_DIR)/c2m\n\t$(SRC_DIR)/c-benchmarks/run-benchmarks.sh\n\nc2mir-bench-short: $(BUILD_DIR)/c2m\n\t$(SRC_DIR)/c-benchmarks/run-benchmarks.sh short\n\nmir2c-bench: $(BUILD_DIR)/mir.$(OBJSUFF) $(SRC_DIR)/mir2c/mir2c.c\n\t$(COMPILE_AND_LINK) -DTEST_MIR2C $^ $(EXEO)$(BUILD_DIR)/mir2c-bench$(EXE) $(LDLIBS)\n\t$(BUILD_DIR)/mir2c-bench -v && size $(BUILD_DIR)/mir2c-bench\n\nclean-bench:\n\t$(RM) $(BUILD_DIR)/io-bench$(EXE) $(BUILD_DIR)/interp-bench$(EXE) $(BUILD_DIR)/gen-bench$(EXE)\n\t$(RM) $(BUILD_DIR)/gen-speed$(EXE) $(BUILD_DIR)/mir2c-bench$(EXE)\n\n# ------------------ miscellaneous ----------------------\n\n.PHONY: sloc gcc-test clang-test\n\nsloc:\n\t@echo -n 'C2MIR: ' && wc -l $(SRC_DIR)/c2mir/c2mir.c | awk '{last=$$1} END {print last}'\n\t@echo -n 'ADT: ' && wc -l $(SRC_DIR)/mir-dlist.h $(SRC_DIR)/mir-hash.h $(SRC_DIR)/mir-htab.h\\\n\t                          $(SRC_DIR)/mir-varr.h $(SRC_DIR)/mir-reduce.h $(SRC_DIR)/mir-bitmap.h\\\n\t\t\t\t  | awk '{last=$$1} END {print last}'\n\t@echo -n 'MIR API: ' && wc -l $(SRC_DIR)/mir.[ch] | awk '{last=$$1} END {print last}'\n\t@echo -n 'MIR Interpreter: ' && wc -l $(SRC_DIR)/mir-interp.c | awk '{last=$$1} END {print last}'\n\t@echo -n 'MIR Generator: ' && wc -l $(SRC_DIR)/mir-gen.[ch] | awk '{last=$$1} END {print last}'\n\t@echo -n 'x86-64 machine dependent code: ' && wc -l $(SRC_DIR)/mir-x86_64.c $(SRC_DIR)/mir-gen-x86_64.c\\\n\t| awk '{last=$$1} END {print last}'\n\t@echo -n 'aarch64 machine dependent code: ' && wc -l $(SRC_DIR)/mir-aarch64.c $(SRC_DIR)/mir-gen-aarch64.c\\\n\t| awk '{last=$$1} END {print last}'\n\t@echo -n 'ppc64 machine dependent code: ' && wc -l $(SRC_DIR)/mir-ppc64.c $(SRC_DIR)/mir-gen-ppc64.c\\\n\t| awk '{last=$$1} END {print last}'\n\t@echo -n 's390x machine dependent code: ' && wc -l $(SRC_DIR)/mir-s390x.c $(SRC_DIR)/mir-gen-s390x.c\\\n\t| awk '{last=$$1} END {print last}'\n\t@echo -n 'riscv64 machine dependent code: ' && wc -l $(SRC_DIR)/mir-riscv64.c $(SRC_DIR)/mir-gen-riscv64.c\\\n\t| awk '{last=$$1} END {print last}'\n\t@echo -n 'Overall: ' && wc -l $(SRC_DIR)/c2mir/c2mir.c $(SRC_DIR)/mir-dlist.h $(SRC_DIR)/mir-hash.h\\\n\t                              $(SRC_DIR)/mir-htab.h $(SRC_DIR)/mir-varr.h $(SRC_DIR)/mir-reduce.h\\\n\t\t\t\t      $(SRC_DIR)/mir-bitmap.h $(SRC_DIR)/mir.[ch]\\\n\t\t\t\t      $(SRC_DIR)/mir-interp.c $(SRC_DIR)/mir-gen.[ch] $(SRC_DIR)/mir-x86_64.c\\\n\t\t\t\t      $(SRC_DIR)/mir-gen-x86_64.c $(SRC_DIR)/mir-aarch64.c\\\n\t\t\t\t      $(SRC_DIR)/mir-gen-aarch64.c $(SRC_DIR)/mir-ppc64.c $(SRC_DIR)/mir-gen-ppc64.c\\\n\t\t\t\t      $(SRC_DIR)/mir-riscv64.c $(SRC_DIR)/mir-gen-riscv64.c\\\n\t\t\t\t      $(SRC_DIR)/mir-riscv64.c $(SRC_DIR)/mir-gen-riscv64.c\\\n\t| awk '{last=$$1} END {print last}'\n\ngcc-test:\n\t$(SHELL) $(SRC_DIR)/c-tests/runtests.sh $(SRC_DIR)/c-tests/use-gcc\n\nclang-test:\n\t$(SHELL) $(SRC_DIR)/c-tests/runtests.sh $(SRC_DIR)/c-tests/use-clang\n\n# Comparison of gcc, c2m, wasmer, wasmtime on ogg encoding.  You need wasi-clang script calling clang from wasi-sdk.\noggenc-bench:\n\t$(SHELL) $(SRC_DIR)/c-benchmarks/run-oggenc.sh\n\n# Very long testing (hours and hours):\ncsmith: csmith-c2m-gcc csmith-c2m\n\ncsmith-c2m-gcc:\n\t$(SHELL) csmith-c2m-gcc.sh\n\ncsmith-c2m:\n\t$(SHELL) csmith-c2m.sh\n"
        },
        {
          "name": "HOW-TO-PORT-MIR.md",
          "type": "blob",
          "size": 15.6513671875,
          "content": "# Introduction\n\n  Porting MIR to another target requires implementing a lot of\nmachine-dependent code. On my estimation this code is about 3K C\nlines for target and writing it can take 1 month of work for\nexperienced person.\n  \n  This document outlines what should be done for porting MIR.  First I\nrecommend to port MIR-interpreter and `c2mir` using it.  You need to\nwrite file `mir-<target>.c` for this. Then you can port MIR-generator\nby writing file `mir-gen-<target>.c` file.  For more details you can\nexamine existing files `mir-x86_64.c`, `mir-aarhc64.c`,\n`mir-gen-x86_64.c`, and `mir-gen-aarhc64.c`\n  \n## Common machine dependent functions (file `mir-<target.c>`)\n\n  These function can be used to work with MIR-interpreter and\n  MIR-generator.  These functions should be placed in file\n  `mir-<target>.c` like `mir-x86_64.c` or `mir-aarch64.c`.  You can\n  use MIR internal functions `_MIR_publish_code`,\n  `_MIR_get_new_code_addr`, `_MIR_publish_code_by_addr`,\n  `_MIR_change_code`, `_MIR_update_code`, and `_MIR_update_code_arr` to\n  work with executable code.\n\n  Here is the list of the common function you should provide:\n  \n  * `void *_MIR_get_thunk (MIR_context_t ctx)` generates small code (thunk) and\n    returns address to the code.  The code redirects control flow to\n    address stored in it.  The address is not defined by the function\n    but can be set by the following function\n\n  * `void _MIR_redirect_thunk (MIR_context_t ctx, void *thunk, void\n    *to)` sets up address in the thunk given as the 2nd argument to\n    value given by the 3rd argument\n\n  * `void *_MIR_get_wrapper (MIR_context_t ctx, MIR_item_t\n    called_func, void *hook_address)` generates a function (`fun1`) code and\n    returns its address.  The code calls a function given by\n    `hook_address` passing it `ctx` and `called_func` and getting an\n    address of another function (`fun2`) which is called at end.  The\n    function `fun2` gets arguments which are passed to the generated\n    function `fun1`\n\n    * Function `_MIR_get_wrapper` is used for lazy code generation but can be used\n      for other purposes in the future.  For lazy code generation,\n      `hook_address` is an address of function which generates machine\n      code for MIR function `called_func`, redirects MIR function\n      thunk to the generated machine code, and returns this code address\n    \n## Interpreter machine dependent functions (file `mir-<target>.c`)\n\n  These function are used to implement MIR interpreter.  These\n  functions should be also placed in file `mir-<target>.c` and you can\n  also use the MIR internal function mentioned above for their\n  implementation\n\n  * `void *_MIR_get_bstart_builtin (MIR_context_t ctx)` generates a\n    function and returns address to the generated function. The generated\n    function has no arguments and returns SP right before the function\n    call.  The generated function is used to implement `MIR_BSTART`\n    insn in the interpreter\n    \n  * `void *_MIR_get_bend_builtin (MIR_context_t ctx)` generates a\n    function and returns address to the generated function. The\n    generated function has one argument and sets up SP from the\n    function argument.  The generated function is used to implement\n    `MIR_BEND` insn in the interpreter\n\n  * `void *va_arg_builtin (void *p, uint64_t t)` returns address of\n    the next argument of MIR type `t`, where `p` is `va_list`\n  \n  * `void va_block_arg_builtin (void *res, void *p, size_t s, uint64_t\n    ncase)` set up memory block with address `res` by the next block\n    argument of size `s` and `ncase` (e.g. `MIR_T_BLK` has case 0,\n    `MIR_T_BLK + 1` has case 1, and `MIR_T_BLK + MIR_BLK_NUM -1` has\n    case `MIR_BLK_NUM - 1`), where `p` is `va_list`\n  \n  * `void *_MIR_get_interp_shim (MIR_context_t ctx, MIR_item_t\n    func_item, void *handler)` generates and returns a function behaving as usual\n    C function which calls function `void handler (MIR_context_t ctx,\n    MIR_item_t func_item, va_list va, MIR_val_t *results)` where `va` can be used to access to\n    arguments of the generated function call and `results` will contain\n    what the generated function returns.  MIR design assumes that call of external\n    C function and MIR function can not be recognized and also that we\n    should easily switch from generated code from MIR to the MIR code\n    interpretation. It means that we need to have the same interface\n    for MIR function interpretation as usual C function.  The\n    generated function provides such interface\n\n  * `void *_MIR_get_ff_call (MIR_context_t ctx, size_t nres,\n    MIR_type_t *res_types, size_t nargs, MIR_type_t *arg_types)`\n    generates function `void fun1 (void *func_addr, MIR_val_t\n    *res_arg_addresses)` and returns its address.  The generated\n    function `fun1` calls `func_addr` as usual C function which is returning\n    `nres` results given as the first elements of `res_arg_addresses`\n    and is taking `nargs` arguments as the subsequent elements of\n    `res_arg_addresses`.  Result and argument types are described\n    correspondingly by vectors `res_types` and `arg_types`\n    \n## C2MIR machine dependent code\n\n  After writing `mir-<target>.c` file and making MIR interpreter work\n(use make `interp-test` to check this), you can make `c2m` working\nwith the interpreter.  You need to create a lot files but they are\nmostly copies of already existing ones.\n\n  * First create directory `c2mir/<target>` and files `c<target>.h`,\n    `c<target>-code.c`, `c<target>-ABI-code.c`, and\n    `mirc_<target>_linux.h` in this directory.  The simplest way to do\n    this is to copy an existing directory `x86_64` or `aarch64`,\n    rename files in the new directory, and modify them\n\n    * file `c<target>.h` defines types and macros used by C2MIR compiler.\n      In most cases of 64-bit target, you don't need to change\n      anything\n\n    * file `c<target>-code.c` defines platform depended constants\n      (e.g. standard include directories) and functions (concerning\n      data alignments) used by C2MIR compiler. You just need to rename\n      definitions containing `<target>` in their names\n\n    * file `c<target>-ABI-code.c` defines platform depended data and\n      functions to generate ABI-compliant calls.  You can use some\n      functions of simple ABI in file `c2mir.c` or write code to\n      generate ABI compliant calls.  See comments for functions\n      `target_init_arg_vars`, `target_return_by_addr_p`,\n      `target_add_res_proto`, `target_add_call_res_op`,\n      `target_gen_post_call_res_code`, `target_add_ret_ops`,\n      `target_get_blk_type`, `target_add_arg_proto`,\n      `target_add_call_arg_op`, and `target_gen_gather_arg`\n\n    * file `mirc_<target>_linux.h` contains predefined macros of C2MIR\n      compiler.  You should rename some of them.  To find what macros\n      to rename, you can use `cpp -dM < /dev/null` on platform to\n      which you port MIR\n\n  * Second include files `c<target>.h` and `c<target>-code.c` into\n    file `c2mir.c` and add target standard include and library\n    directories in files `c2mir.c` and `c2mir-driver.c`\n\nFor debugging the MIR interpreter, you can switch on MIR insn execution\ntracing by adding `-DMIR_INTERP_TRACE` for compilation `mir.c` file.\n\nC programs compiled by C2MIR compiler need some compiler specific\nstandard files.  They are stored in `c2m` program itself to make `c2m`\nmovable.  The stringified versions of the files are placed in\ndirectory `c2mir/<target>` and have names `mirc_<target>_float.h`,\n`mirc_<target>_limits.h`, `mirc_<target>_stdarg.h`,\n`mirc_<target>_stddef.h`, and `mirc_<target>_stdint.h`.\n\nThe easiest way to port the stringified standard include files is to\ncopy them from existing target dependent directory (e.g. aarch64)\nmodify files into the new directory.  Again in most cases of 64-bit\ntarget, you don't need to change anything probably except macros\nrelated long double, char signedness, wchar, and `va_list`\ndefinitions.\n\nTo run C tests for C2MIR with the MIR interpreter you can use `make\nc2mir-interp-test`.\n  \n## Machine-dependent code for MIR-generator (file `mir-gen-<target>.c`)\n\n  The last step for porting MIR is to make the MIR generator generating\ncode for your target. You need to provide the following\nmachine-dependent functions and definitions used by the MIR-generator:\n \n  * `void target_init (MIR_context_t ctx)` and `void target_finish (MIR_context_t ctx)`\n    can be used to initialize/finalize data\n    internally for this file which can be used during all MIR generator\n    work in given context\n\n  * Register Description\n    * it is a good practice to describe all hard regs you would like\n      to use in MIR-code during MIR-generator as C enumerator.\n      The enumerator should contains hard regs used for register allocations, stack\n      access, parameter passing etc.  All hard registers added in\n      machinize should be here.  The MIR generator does not use the enumerator constants but you\n      can use them in `mir-gen-target.c` file for you convenience.  The MIR generator only refers for\n      the following target hard registers descriptions:\n\n      * `MAX_HARD_REG` is maximal number of the described hard registers\n      * `SP_HARD_REG` and `HARD_REG_FRAME_POINTER` are hard register numbers used\n        as stack and frame pointer according target ABI\n      * `TEMP_INT_HARD_REG1`, `TEMP_INT_HARD_REG2`, `TEMP_FLOAT_HARD_REG1`, `TEMP_FLOAT_HARD_REG2`,\n        `TEMP_DOUBLE_HARD_REG1`, `TEMP_DOUBLE_HARD_REG2`, `TEMP_LDOUBLE_HARD_REG1`, and `TEMP_DOUBLE_HARD_REG2`\n\tare numbers of hard regs not used in machinized code and for register allocation.\n\tIt is better use callee-cloberred hard regs.  These regs occurs in MIR code only after RA.\n\tThe corresponding `_REG1` and `_REG2` with the same prefix should be distinctive but hard regs\n\twith different prefixes can be the same\n    * `int target_hard_reg_type_ok_p (MIR_reg_t hard_reg, MIR_type_t type)` should return true\n      only if HARD_REG can contain value of `type`\n    * `int target_fixed_hard_reg_p (MIR_reg_t hard_reg)` should return true you want that `hard_reg` \n      is not used for register allocation.  For stack and frame pointers, the temporary hard registers\n      (ones with prefix `TEMP_` above) the function should always return true\n    * `int target_call_used_hard_reg_p (MIR_reg_t hard_reg)` returns true if `hard_reg` can be cloberred\n      by callee according target ABI\n    \n  * `int target_locs_num (MIR_reg_t loc, MIR_type_t type)` returns number of standard 64-bit stack\n    slots should be used for value of `type`.  Usually it is just `1` but it can be `2` for\n    long double type\n\n  * Array `const MIR_insn_code_t target_io_dup_op_insn_codes[]` contains codes of insns\n    requiring to have output and one input operand to be\n    the same on this target.  `MIR_INSN_BOUND` is used as the end\n    marker\n\n  * `MIR_disp_t target_get_stack_slot_offset (MIR_context_t ctx, MIR_type_t type, MIR_reg_t slot)`\n    returns offset of the slot relative the stack frame\n    pointer. MIR registers which did not get a hard register will keep their\n    values in 64-bit stack slots.  The slot numbers start with 0.  The transformation of slot number\n    to the offset depends on your own or ABI stack frame layout.  You should be not aware of stack\n    slot alignment.  It is MIR generator responsibility\n\n  * `void target_machinize (MIR_context_t ctx)` transforms MIR code\n    for the target.  Usually it includes call transformations\n    according to ABI by adding MIR insns which put call arguments\n    into hard registers or on the stack and setting the called\n    function result from the return register, setting up return\n    register(s) for the current functions.  The function can also\n\n    * change some MIR insns into another sequence of MIR insns which are better for the target.\n      It may also include peephole optimization\n    * change some MIR insns into built-in function calls if a MIR insn requires a lot of\n      target insns to implement it\n    * change MIR insn operands on some hard register if the corresponding target insn works\n      only with this hard register.  In this case you need generate move insn to copy the original\n      operand into the hard register\n    * implements block data argument passing in a way to implement target call ABI for C aggregates\n    \n    Adding and deleting MIR insns should be done with MIR-generator\n    functions `gen_add_insn_before`, `gen_add_insn_after`, and\n    `gen_delete_insn`.\n\n  * `void target_make_prolog_epilog (MIR_context_t ctx, bitmap_t\n    used_hard_regs, size_t stack_slots_num)` adds MIR insns for prologue and epilogue of the\n    generated function.  This usually includes\n    saving/restoring used callee-saved hard registers, stack space\n    allocation, setting up set frame pointer register\n\n  * `int target_insn_ok_p (MIR_context_t ctx, MIR_insn_t insn)`. MIR\n    generator works mostly on simplified code.  For any simplified (or\n    machinized) insn the function should return TRUE.  In brief simplified insns are\n\n    * all insns with registers\n    * register moves with immediate\n    * moves which are loads or stores with indirect reg addressing\n\n    MIR generator tries to combine several data dependent insns into one.  Return\n    also TRUE if you provide translation of the combined insn.  The\n    more combined insns you can translate, the better generated code\n    will be\n    \n  * `uint8_t *target_translate (MIR_context_t ctx, size_t *len)`\n    generates and returns address of machine code for the current MIR\n    function. The function should generate machine insn(s) for any MIR\n    insn for which `target_insn_ok_p` return TRUE.  The function\n    returns code length in bytes through parameter `len`.\n\n  * `void target_rebase (MIR_context_t ctx, uint8_t *base)` is called\n    by MIR generator after `target_translate` call.  Sometimes after\n    `target_translate` call we want to modify the output machine code\n    for start code address `base` known only at this point.  This function\n    can be used for this.  Usually it works on data collected by\n    during `target_translate` execution\n\n  * `struct target_bb_version` is a structure used to keep\n    machine-dependent data for the subsequent functions used for lazy\n    BB code generation\n  \n  * `void target_init_bb_version_data (target_bb_version_t data)` is\n    called to initialize the above machine dependent data\n  \n  * `void target_bb_translate_start (gen_ctx_t gen_ctx)` is called by\n    MIR generator before new BB machine code generation\n  \n  * `void target_bb_insn_translate (gen_ctx_t gen_ctx, MIR_insn_t\n    insn, void **jump_addrs)` is called to generate machine code\n    during lazy BB machine code generation.  `jump_addrs` should be\n    used for label in in the insn\n  \n  * `void target_output_jump (gen_ctx_t gen_ctx, void **jump_addrs)`\n    is called to generate jump to the address given as zero element of\n    `jump_addrs`\n  \n  * `uint8_t *target_bb_translate_finish (gen_ctx_t gen_ctx, size_t\n    *len)` is called to finish BB code generation.  The function\n    returns BB machine code address and its length through arg `len`\n  \n  * `void target_bb_rebase (gen_ctx_t gen_ctx, uint8_t *base)` is\n    called to rebase code placed on address `base`\n  \n  * `void target_setup_succ_bb_version_data (gen_ctx_t gen_ctx,\n    uint8_t *base)` is called to store information about branches in\n    the current BB to its successors in machine dependent way\n  \n  * `void target_redirect_bb_origin_branch (gen_ctx_t gen_ctx,\n    target_bb_version_t data, void *addr)` is called to redirect the\n    first original branch to address `addr` of the current BB machine\n    code\n\nIt is better to start with small generator tests by using `make\ngen-test`.  After the successful test code generation you can\ncontinue with bigger test set generated by `c2m`.  Please use `make\nc2mir-gen-test` to run this test set.  And finally you can run `make\nc2mir-bootstrap-test` which uses MIR-generator on a pretty big test\n(C-to-MIR compiler).\n"
        },
        {
          "name": "INSTALL.md",
          "type": "blob",
          "size": 2.1962890625,
          "content": "# Building MIR\n\nYou can build MIR in source directory simply with `make` or `make\nall`\n\nYou can also build MIR in a separate directory.  In this case you\nneed to use `make SRC_DIR=<path to MIR sources> -f <path to MIR sources>/GNUmakefile`.\n**All other calls of make** should have the\nsame `SRC_DIR` value and `-f` argument on the make command line.\n\n By default MIR is built in release mode (with optimizations).  If you\nwant to build debugging version (without optimizations and additional\nchecks), you can use `make debug` instead of `make`.  Please don't\nforget to remove debug version up before installing (you can do this\nwith `make clean all`).\n\n# Testing and Benchmarking MIR\n\n Please use `make test` or `make bench` for testing and benchmarking\nwith the right `SRC_DIR` if you build MIR in a different directory.\n\n# Installing MIR\n \n `make install` installs the following:\n\n  * `libmir.a` - a static library containing MIR API functions,\n    MIR generator and interpreter, and C-to-MIR compiler\n  * `libmir.so.x.x.x` - a dynamic library containing MIR API functions,\n    MIR generator and interpreter, and C-to-MIR compiler\n  * `mir.h`, `mir-gen.h`, `c2mir.h` - include files to use MIR API functions\n    and interpreter, MIR-generator, and C-to-MIR compiler\n  * `c2m` - a standalone C compiler based on MIR\n  * `b2m` - an utility to transform binary MIR file into textual one\n  * `m2b` - an utility to transform textual MIR file into binary one\n  * `b2ctab` - an utility to transform binary MIR file into C file\n    containing this file as an byte array with name `mir_code`.  This utility\n    can be useful to generate a standalone executable based on using\n    MIR interpreter or generator\n\n  The default destination is `/usr/local/include` for include files,\n`/usr/local/lib` for the library, and `/usr/local/bin` for the\nexecutables.\n\n  You can change the default destination by using `PREFIX` variable on the make command line.  For example,\n\n```\nmake PREFIX=/usr install\n```\n  will use `/usr/include`, `/usr/lib`, and `/usr/bin` for the destinations.\n\n  `make uninstall` undoes `make install` if the both use the same `PREFIX` value.\n\n\n# Cleaning\n\n  `make clean` removes all files created during building, testing, and\nbenchmarking.\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.052734375,
          "content": "MIT License\n\nCopyright (c) 2018-2024 Vladimir Makarov\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "MIR.md",
          "type": "blob",
          "size": 51.8623046875,
          "content": "# Medium Intermediate Representation (file mir.h)\n  * This document describes MIR itself, API for its creation, and MIR textual representation\n  * MIR textual representation is assembler like.  Each directive or insn should be put on a separate line\n  * In MIR textual syntax we use\n    * `[]` for optional construction\n    * `{}` for repeating zero or more times\n    * `<>` for some informal construction description or construction already described or will be described\n\n## MIR context\n  * MIR API code has an implicit state called by MIR context\n  * MIR context is represented by data of `MIR_context_t`\n  * MIR context is created by function `MIR_context_t MIR_init (void)`\n  * In case you want to use custom allocators, use `MIR_context_t MIR_init2 (MIR_alloc_t, MIR_code_alloc_t)` instead (see [here](CUSTOM-ALLOCATORS.md) for more details)\n  * Every MIR API function (except for `MIR_init` / `MIR_init2`) requires MIR context passed through the first argument of type `MIR_context_t`\n  * You can use MIR functions in different threads without any synchronization\n    if they work with different contexts in each thread\n\n## MIR program\n   * MIR program consists of MIR **modules**\n   * To start work with MIR program, you should first call API function `MIR_init` / `MIR_init2`\n   * API function `MIR_finish (MIR_context_t ctx)` should be called last.  It frees all internal data used to work with MIR program and all IR (insns, functions, items, and modules) created in this context\n   * API function `MIR_output (MIR_context_t ctx, FILE *f)` outputs MIR textual representation of the program into given file\n   * API function `MIR_scan_string (MIR_context_t ctx, const char *str)` reads textual MIR representation given by a string\n   * API functions `MIR_write (MIR_context_t ctx, FILE *f)` and\n     `MIR_read (MIR_context_t ctx, FILE *f)` outputs and reads\n     **binary MIR representation** to/from given file.  There are also\n     functions `MIR_write_with_func (MIR_context_t ctx, const int\n     (*writer_func) (MIR_context_t, uint8_t))` and `MIR_read_with_func\n     (MIR_context_t ctx, const int (*reader_func) (MIR_context_t))` to\n     output and read **binary MIR representation** through a function\n     given as an argument.  The reader function should return EOF as\n     the end of the binary MIR representation, the writer function\n     should be return the number of successfully output bytes\n     * Binary MIR representation much more compact and faster to read than textual one\n\n## MIR data type\n   * MIR program works with the following **data types**:\n     * `MIR_T_I8` and `MIR_T_U8` -- signed and unsigned 8-bit integer values\n     * `MIR_T_I16` and `MIR_T_U16` -- signed and unsigned 16-bit integer values\n     * `MIR_T_I32` and `MIR_T_U32` -- signed and unsigned 32-bit integer values\n     * `MIR_T_I64` and `MIR_T_U64` -- signed and unsigned 64-bit integer values\n       * signed and unsigned 64-bit integer types in most cases\n         are interchangeable as insns themselves decide how to treat\n         their value\n     * `MIR_T_F` and `MIR_T_D` -- IEEE single and double precision floating point values\n     * `MIR_T_LD` - long double values.  It is machine-dependent and can be IEEE double, x86 80-bit FP,\n       or IEEE quad precision FP values.  If it is the same as double, the double type will be used instead.\n       So please don't expect machine-independence of MIR code working with long double values\n     * `MIR_T_P` -- pointer values.  Depending on the target pointer value is actually 32-bit or 64-bit integer value\n     * `MIR_T_BLK` .. `MIR_T_BLK + MIR_BLK_NUM - 1` -- block data with given case.  This type can be used only\n       for argument of function.  Different case numbers can denote different ways to pass the block data\n       on a particular target to implement the target call ABI.  Currently there are 6 block\n       types (`MIR_BLK_NUM = 5`)\n     * `MIR_T_RBLK` -- return block data.  This type can be used only for argument of function\n   * MIR textual representation of the types are correspondingly `i8`,\n     `u8`, `i16`, `u16`, `i32`, `u32`, `i64`, `u64`, `f`, `d`, `ld`, `p`,\n     and `blk`\n   * Function `int MIR_int_type_p (MIR_type_t t)` returns TRUE if given type is an integer one (it includes pointer type too)\n   * Function `int MIR_fp_type_p (MIR_type_t t)` returns TRUE if given type is a floating point type\n\n## MIR module\n  * Module is a high level entity of MIR program\n\n  * Module is created through API function `MIR_module_t MIR_new_module (const char *name)`\n\n  * Module creation is finished by calling API function `MIR_finish_module`\n\n  * You can create only one module at any given time\n\n  * List of all created modules can be gotten by function `DLIST (MIR_module_t) *MIR_get_module_list (MIR_context_t ctx)`\n\n  * MIR module consists of **items**.  There are following **item types** (and function for their creation):\n    * **Function**: `MIR_func_item`\n      * textual representation of function is described below\n\n    * **Import**: `MIR_import_item` (`MIR_item_t MIR_new_import (MIR_context_t ctx, const char *name)`)\n      * textual representation of import items is `import <name>{, <name>}`\n\n    * **Export**: `MIR_export_item` (`MIR_item_t MIR_new_export (MIR_context_t ctx, const char *name)`)\n      * textual representation of export items is `export <name>{, <name>}`\n\n    * **Forward declaration**: `MIR_forward_item` (`MIR_item_t MIR_new_forward (MIR_context_t ctx, const char *name)`)\n      * textual representation of forward items is `forward <name>{, <name>}`\n\n    * **Prototype**: `MIR_proto_item` (`MIR_new_proto_arr`, `MIR_new_proto`, `MIR_new_vararg_proto_arr`,\n      `MIR_new_vararg_proto` analogous to `MIR_new_func_arr`, `MIR_new_func`, `MIR_new_vararg_func_arr` and\n      `MIR_new_vararg_func` -- see below).  The only difference is that\n      two or more prototype argument names can be the same.  The textual representation is analogous to\n      function title (see below) but `proto` is used instead of `func`\n\n    * **Data**: `MIR_data_item` with optional name\n      (`MIR_item_t MIR_new_data (MIR_context_t ctx, const char *name, MIR_type_t el_type, size_t nel, const void *els)`\n       or `MIR_item_t MIR_new_string_data (MIR_context_t ctx, const char *name, MIR_str_t str)`)\n      * textual representation of data items `[<name>:] <type> <value>{, <value>}`\n\n    * **Reference data**: `MIR_ref_data_item` with optional name\n      (`MIR_item_t MIR_new_ref_data (MIR_context_t ctx, const char *name, MIR_item_t item, int64_t disp)`\n      * The address of the item after linking plus `disp` is used to initialize the data\n      * textual representation of reference data items `[<name>:] ref <name>[, <disp>]`\n\n    * **Expression Data**: `MIR_expr_data_item` with optional name\n      (`MIR_item_t MIR_new_expr_data (MIR_context_t ctx, const char *name, MIR_item_t expr_item)`)\n      * Not all MIR functions can be used for expression data.  The expression function should have\n        only one result, have no arguments, not use any call or any instruction with memory\n      * The expression function is called during linking and its result is used to initialize the data\n      * textual representation of expression data items `[<name>:] expr <func name>`\n\n    * **Memory segment**: `MIR_bss_item` with optional name (`MIR_item_t MIR_new_bss (MIR_context_t ctx, const char *name, size_t len)`)\n      * textual representation of memory items `[<name>:] bss <length>`\n\n    * **Label reference**: `MIR_lref_item` with optional name\n      (`MIR_item_t MIR_new_lref_data (MIR_context_t ctx, const char *name, MIR_label_t label,\n                                      MIR_label_t label2, int64_t disp)`) which keeps values derived from label\n      addresses defined as `label[-label2]+disp` where `label2` can be NULL.\n      * Please remember that label scope is whole module\n      * `lref` can refers for labels the same function (this is checked during module load) and\n         there is a warranty label addresses to be defined only at the beginning of the function execution\n      * textual representation of label reference items `[<name>:] lref <label>[, <label2>][, <disp>]`\n\n  * Long double data item is changed to double one, if long double coincides with double for given target or ABI\n\n  * Names of MIR functions, imports, and prototypes should be unique in a module\n\n  * API functions `MIR_output_item (MIR_context_t ctx, FILE *f, MIR_item_t item)`\n    and `MIR_output_module (MIR_context_t ctx, FILE *f, MIR_module_t module)` output item or module\n    textual representation into given file\n\n  * MIR text module syntax looks the following:\n```\n    <module name>: module\n                   {<module item>}\n                   endmodule\n```\n\n## MIR function\n  * Function is an module item\n  * Function has a **frame**, a stack memory reserved for each function invocation\n  * Function has **local variables** (sometimes called **registers**), a part of which are **arguments**\n    * A variable should have an unique name in the function\n    * A variable is represented by a structure of type `MIR_var_t`\n      * The structure contains variable name and its type\n      * The structure contains also type size for variable of block types (`MIR_T_BLK`..`MIR_T_BLK + MIR_BLK_NUM - 1`)\n        or `MIR_T_RBLK` type\n  * MIR function with its arguments is created through API function `MIR_item_t MIR_new_func (MIR_context_t ctx, const\n    char *name, size_t nres, MIR_type_t *res_types, size_t nargs, ...)`\n    or function `MIR_item_t MIR_new_func_arr (MIR_context_t ctx, const char *name, size_t nres, MIR_type_t *res_types, size_t nargs, MIR_var_t *arg_vars)`\n    * Argument variables can be any type\n      * This type only denotes how the argument value is passed\n      * Any integer type argument variable has actually type `MIR_T_I64`\n  * MIR functions with variable number of arguments are created through API functions\n    `MIR_item_t MIR_new_vararg_func (MIR_context_t ctx, const char *name, size_t nres, MIR_type_t *res_types, size_t nargs, ...)`\n    or function `MIR_item_t MIR_new_vararg_func_arr (MIR_context_t ctx, const char *name, size_t nres, MIR_type_t *res_types, size_t nargs, MIR_var_t *arg_vars)`\n    * `nargs` and `arg_vars` define only fixed arguments\n    * MIR functions can have more one result but possible number of results\n      and combination of their types are machine-defined.  For example, for x86-64\n      the function can have up to six results and return two integer\n      values, two float or double values, and two long double values\n      in any combination\n  * MIR function creation is finished by calling API function `MIR_finish_func (MIR_context_t ctx)`\n  * You can create only one MIR function at any given time\n  * MIR text function syntax looks the following (arg-var always has a name besides type):\n```\n    <function name>: func {<result type>, } [ arg-var {, <arg-var> } [, ...]]\n                     {<insn>}\n                     endfunc\n```\n    * Textual presentation of block type argument in `func` has form `blk:<size>(<var_name>)`.\n      The corresponding argument in `call` insn should have analogous form\n      `blk:<the same size>(<local var name containing address of passed block data>)`\n    * Block data are passed by value.  How they are exactly passed is machine-defined (please read files mir-<target>.c):\n      * they can be passed on stack, or (partially) in registers, or by address\n  * Non-argument function variables are created through API function\n    `MIR_reg_t MIR_new_func_reg (MIR_context_t ctx, MIR_func_t func, MIR_type_t type, const char *name)`\n    or `MIR_new_global_func_reg (MIR_context_t ctx, MIR_func_t func, MIR_type_t type,\n    const char *name, const char *hard_reg_name)`\n    * The only permitted integer type for the variable is `MIR_T_I64` (or MIR_T_U64???)\n    * Names in form `t<number>` can not be used as they are fixed for internal purposes\n    * You can create function variables even after finishing the\n      function creation.  This can be used to modify function insns,\n      e.g. for optimizations\n    * Global variables are variables which always bound to specific hard register.  They are used to implement\n      extension \"GNU C global reg variables\".  Here are the permitted hard register names:\n      * x86_64: `rax`,   `rcx`,   `rdx`,   `rbx`,   `rsp`,   `rbp`,  `rsi`,  `rdi`,  `r8`,\n        `r9`,    `r10`,   `r11`,   `r12`,   `r13`,   `r14`,  `r15`,  `xmm0`, `xmm1`,\n        `xmm2`,  `xmm3`,  `xmm4`,  `xmm5`,  `xmm6`,  `xmm7`, `xmm8`, `xmm9`, `xmm10`,\n        `xmm11`, `xmm12`, `xmm13`, `xmm14`, `xmm15`, `st0`,  `st1`\n      * aarch64: `r0`,  `r1`,  `r2`,  `r3`,  `r4`,  `r5`,  `r6`,  `r7`,  `r8`,  `r9`,  `r10`, `r11`, `r12`,\n        `r13`, `r14`, `r15`, `r16`, `r17`, `r18`, `r19`, `r20`, `r21`, `r22`, `r23`, `r24`, `r25`,\n        `r26`, `r27`, `r28`, `r29`, `r30`, `sp`,  `v0`,  `v1`,  `v2`,  `v3`,  `v4`,  `v5`,  `v6`,\n        `v7`,  `v8`,  `v9`,  `v10`, `v11`, `v12`, `v13`, `v14`, `v15`, `v16`, `v17`, `v18`, `v19`,\n        `v20`, `v21`, `v22`, `v23`, `v24`, `v25`, `v26`, `v27`, `v28`, `v29`, `v30`, `v31`\n      * ppc64: `r0`,  `r1`,  `r2`,  `r3`,  `r4`,  `r5`,  `r6`,  `r7`,  `r8`,  `r9`,  `r10`, `r11`, `r12`,\n        `r13`, `r14`, `r15`, `r16`, `r17`, `r18`, `r19`, `r20`, `r21`, `r22`, `r23`, `r24`, `r25`,\n        `r26`, `r27`, `r28`, `r29`, `r30`, `r31`, `f0`,  `f1`,  `f2`,  `f3`,  `f4`,  `f5`,  `f6`,\n        `f7`,  `f8`,  `f9`,  `f10`, `f11`, `f12`, `f13`, `f14`, `f15`, `f16`, `f17`, `f18`, `f19`,\n        `f20`, `f21`, `f22`, `f23`, `f24`, `f25`, `f26`, `f27`, `f28`, `f29`, `f30`, `f31`, `lr`\n      * riscv64: `r0`,  `r1`,  `r2`,  `r3`,  `r4`,  `r5`,  `r6`,  `r7`,  `r8`,  `r9`,  `r10`, `r11`, `r12`,\n        `r13`, `r14`, `r15`, `r16`, `r17`, `r18`, `r19`, `r20`, `r21`, `r22`, `r23`, `r24`, `r25`,\n        `r26`, `r27`, `r28`, `r29`, `r30`, `r31`, `f0`,  `f1`,  `f2`,  `f3`,  `f4`,  `f5`,  `f6`,\n        `f7`,  `f8`,  `f9`,  `f10`, `f11`, `f12`, `f13`, `f14`, `f15`, `f16`, `f17`, `f18`, `f19`,\n        `f20`, `f21`, `f22`, `f23`, `f24`, `f25`, `f26`, `f27`, `f28`, `f29`, `f30`, `f31`\n      * s390x: `r0`,  `r1`,  `r2`,  `r3`,  `r4`,  `r5`,  `r6`,  `r7`,  `r8`,  `r9`,  `r10`,\n        `r11`, `r12`, `r13`, `r14`, `r15`, `f0`,  `f1`,  `f2`,  `f3`,  `f4`,  `f5`,\n        `f6`,  `f7`,  `f8`,  `f9`,  `f10`, `f11`, `f12`, `f13`, `f14`, `f15`\n\n  * Non-argument variable declaration syntax in MIR textual representation looks the following:\n```\n    local [ <var type>:<var name>[:<hard reg name>] {, <var type>:<var name>[:<hard reg name>]} ]\n```\n  * In MIR textual representation variable should be defined through `local` before its use\n\n## MIR insn operands\n  * MIR insns work with operands\n  * There are following operands:\n    * Signed or unsigned **64-bit integer value operands** created through API functions\n      `MIR_op_t MIR_new_int_op (MIR_context_t ctx, int64_t v)` and `MIR_op_t MIR_new_uint_op (MIR_context_t ctx, uint64_t v)`\n      * In MIR text they are represented the same way as C integer numbers (e.g. octal, decimal, hexadecimal ones)\n    * **Float, double or long double value operands** created through API functions `MIR_op_t MIR_new_float_op (MIR_context_t ctx, float v)`,\n      `MIR_op_t MIR_new_double_op (MIR_context_t ctx, double v)`,\n      and `MIR_op_t MIR_new_ldouble_op (MIR_context_t ctx, long double v)`.\n      Long double operand is changed to double one when long double coincides with double for given target or ABI\n      * In MIR text, they are represented the same way as C floating point numbers\n    * **String operands** created through API functions `MIR_op_t MIR_new_str_op (MIR_context_t ctx, MIR_str_t str)`\n      * In MIR text, they are represented by `typedef struct MIR_str {size_t len; const char *s;} MIR_str_t`\n      * Strings for each operand are put into memory (which can be modified) and the memory address actually presents the string\n    * **Label operand** created through API function `MIR_op_t MIR_new_label_op (MIR_context_t ctx, MIR_label_t label)`\n      * Here `label` is a special insn created by API function `MIR_insn_t MIR_new_label (MIR_context_t ctx)`\n      * In MIR text, they are represented by unique label name\n    * **Reference operands** created through API function `MIR_op_t MIR_new_ref_op (MIR_context_t ctx, MIR_item_t item)`\n      * In MIR text, they are represented by the corresponding item name\n    * **Register (variable) operands** created through API function `MIR_op_t MIR_new_reg_op (MIR_context_t ctx, MIR_reg_t reg)`\n      * In MIR text, they are represented by the corresponding variable name\n      * Value of type `MIR_reg_t` is returned by function `MIR_new_func_reg`\n        or can be gotten by function `MIR_reg_t MIR_reg (MIR_context_t ctx, const char *reg_name, MIR_func_t func)`, e.g. for argument-variables\n    * **Memory operands** consists of type, displacement, base\n      register, index register and index scale.  Memory operand is\n      created through API function `MIR_op_t MIR_new_mem_op (MIR_context_t ctx, MIR_type_t type,\n      MIR_disp_t disp, MIR_reg_t base, MIR_reg_t index, MIR_scale_t\n      scale)`\n      * The arguments define address of memory as `disp + base + index * scale`\n      * Integer type input memory is transformed to 64-bit integer value with sign or zero extension\n        depending on signedness of the type\n      * result 64-bit integer value is truncated to integer memory type\n      * Memory operand has the following syntax in MIR text (absent displacement means zero one,\n        absent scale means one, scale should be 1, 2, 4, or 8):\n\n```\n\t  <type>: <disp>\n\t  <type>: [<disp>] (<base reg> [, <index reg> [, <scale> ]])\n```\n  * API function `MIR_output_str (MIR_context_t ctx, FILE *f, MIR_str_t str)` outputs the MIR string\n    textual representation into given file\n  * API function `MIR_output_op (MIR_context_t ctx, FILE *f, MIR_op_t op, MIR_func_t func)` outputs the operand\n    textual representation into given file\n\n\n## MIR insns\n  * All MIR insns (except `call` or `ret`) expects fixed number of operands\n  * Most MIR insns are 3-operand insns: two inputs and one output\n  * In majority cases **the first insn operand** describes where the insn result (if any) will be placed\n  * Only register or memory operand can be insn output (result) operand\n  * MIR insn can be created through API functions `MIR_insn_t MIR_new_insn (MIR_context_t ctx, MIR_insn_code_t code, ...)`\n    and `MIR_insn_t MIR_new_insn_arr (MIR_context_t ctx, MIR_insn_code_t code, size_t nops, MIR_op_t *ops)`\n    * Number of operands and their types should be what is expected by the insn being created\n    * You can not use `MIR_new_insn` for the creation of call and ret insns as these insns have a variable number of operands.\n      To create such insns you should use `MIR_new_insn_arr` or special functions\n      `MIR_insn_t MIR_new_call_insn (MIR_context_t ctx, size_t nops, ...)` and `MIR_insn_t MIR_new_ret_insn (MIR_context_t ctx, size_t nops, ...)`\n    * Long double insns are changed by double ones if long double coincides with double for given target or ABI\n  * You can get insn name and number of insn operands through API functions\n    `const char *MIR_insn_name (MIR_context_t ctx, MIR_insn_code_t code)` and `size_t MIR_insn_nops (MIR_context_t ctx, MIR_insn_t insn)`\n  * You can add a created insn at the beginning or end of function insn list through API functions\n    `MIR_prepend_insn (MIR_context_t ctx, MIR_item_t func, MIR_insn_t insn)` and `MIR_append_insn (MIR_context_t ctx, MIR_item_t func, MIR_insn_t insn)`\n  * You can insert a created insn in the middle of function insn list through API functions\n    `MIR_insert_insn_after (MIR_context_t ctx, MIR_item_t func, MIR_insn_t after, MIR_insn_t insn)` and\n    `MIR_insert_insn_before (MIR_context_t ctx, MIR_item_t func, MIR_insn_t before, MIR_insn_t insn)`\n    * The insn `after` and `before` should be already in the list\n  * You can remove insn from the function list through API function `MIR_remove_insn (MIR_context_t ctx, MIR_item_t func, MIR_insn_t insn)`\n  * The insn should be not inserted in the list if it is already there\n  * The insn should be not removed form the list if it is not there\n  * API function `MIR_output_insn (MIR_context_t ctx, FILE *f, MIR_insn_t insn, MIR_func_t func, int newline_p)` outputs the insn\n    textual representation into given file with a newline at the end depending on value of `newline_p`\n  * Insn has the following syntax in MIR text:\n```\n\t  {<label name>:} [<insn name> <operand> {, <operand>}]\n```\n  * More one insn can be put on the same line by separating the insns by `;`\n\n### MIR move insns\n  * There are following MIR move insns:\n\n    | Insn Code               | Nops |   Description                                          |\n    |-------------------------|-----:|--------------------------------------------------------|\n    | `MIR_MOV`               | 2    | move 64-bit integer values                             |\n    | `MIR_FMOV`              | 2    | move **single precision** floating point values        |\n    | `MIR_DMOV`              | 2    | move **double precision** floating point values        |\n    | `MIR_LDMOV`             | 2    | move **long double** floating point values             |\n\n### MIR integer insns\n  * If insn has suffix `S` in insn name, the insn works with lower 32-bit part of 64-bit integer value\n  * The higher part of 32-bit insn result is undefined\n  * If insn has prefix `U` in insn name, the insn treats integer as unsigned integers\n  * Some insns has no unsigned variant as MIR is oriented to CPUs with two complement integer arithmetic\n    (the huge majority of all CPUs)\n\n    | Insn Code               | Nops |   Description                                          |\n    |-------------------------|-----:|--------------------------------------------------------|\n    | `MIR_EXT8`              | 2    | **sign** extension of lower **8 bit** input part       |\n    | `MIR_UEXT8`             | 2    | **zero** extension of lower **8 bit** input part       |\n    | `MIR_EXT16`             | 2    | **sign** extension of lower **16 bit** input part      |\n    | `MIR_UEXT16`            | 2    | **zero** extension of lower **16 bit** input part      |\n    | `MIR_EXT32`             | 2    | **sign** extension of lower **32 bit** input part      |\n    | `MIR_UEXT32`            | 2    | **zero** extension of lower **32 bit** input part      |\n    |                         |      |                                                        |\n    | `MIR_NEG`               | 2    | changing sign of **64-bit* integer value               |\n    | `MIR_NEGS`              | 2    | changing sign of **32-bit* integer value               |\n    |                         |      |                                                        |\n    | `MIR_ADD`, `MIR_SUB`    | 3    | **64-bit** integer addition and subtraction            |\n    | `MIR_ADDS`, `MIR_SUBS`  | 3    | **32-bit** integer addition and subtraction            |\n    | `MIR_MUL`, `MIR_DIV`    | 3    | **64-bit signed**  multiplication and divison          |\n    | `MIR_UMUL`, `MIR_UDIV`  | 3    | **64-bit unsigned** integer multiplication and divison |\n    | `MIR_MULS`, `MIR_DIVS`  | 3    | **32-bit signed**  multiplication and divison          |\n    | `MIR_UMULS`, `MIR_UDIVS`| 3    | **32-bit unsigned** integer multiplication and divison |\n    | `MIR_MOD`               | 3    | **64-bit signed**  modulo operation                    |\n    | `MIR_UMOD`              | 3    | **64-bit unsigned** integer modulo operation           |\n    | `MIR_MODS`              | 3    | **32-bit signed**  modulo operation                    |\n    | `MIR_UMODS`             | 3    | **32-bit unsigned** integer modulo operation           |\n    |                         |      |                                                        |\n    | `MIR_AND`, `MIR_OR`     | 3    | **64-bit** integer bitwise AND and OR                  |\n    | `MIR_ANDS`, `MIR_ORS`   | 3    | **32-bit** integer bitwise AND and OR                  |\n    | `MIR_XOR`               | 3    | **64-bit** integer bitwise XOR                         |\n    | `MIR_XORS`              | 3    | **32-bit** integer bitwise XOR                         |\n    |                         |      |                                                        |\n    | `MIR_LSH`               | 3    | **64-bit** integer left shift                          |\n    | `MIR_LSHS`              | 3    | **32-bit** integer left shift                          |\n    | `MIR_RSH`               | 3    | **64-bit** integer right shift with **sign** extension |\n    | `MIR_RSHS`              | 3    | **32-bit** integer right shift with **sign** extension |\n    | `MIR_URSH`              | 3    | **64-bit** integer right shift with **zero** extension |\n    | `MIR_URSHS`             | 3    | **32-bit** integer right shift with **zero** extension |\n    |                         |      |                                                        |\n    | `MIR_EQ`, `MIR_NE`      | 3    | equality/inequality of **64-bit** integers             |\n    | `MIR_EQS`, `MIR_NES`    | 3    | equality/inequality of **32-bit** integers             |\n    | `MIR_LT`, `MIR_LE`      | 3    | **64-bit signed** less than/less than or equal         |\n    | `MIR_ULT`, `MIR_ULE`    | 3    | **64-bit unsigned** less than/less than or equal       |\n    | `MIR_LTS`, `MIR_LES`    | 3    | **32-bit signed** less than/less than or equal         |\n    | `MIR_ULTS`, `MIR_ULES`  | 3    | **32-bit unsigned** less than/less than or equal       |\n    | `MIR_GT`, `MIR_GE`      | 3    | **64-bit signed** greater than/greater than or equal   |\n    | `MIR_UGT`, `MIR_UGE`    | 3    | **64-bit unsigned** greater than/greater than or equal |\n    | `MIR_GTS`, `MIR_GES`    | 3    | **32-bit signed** greater than/greater than or equal   |\n    | `MIR_UGTS`, `MIR_UGES`  | 3    | **32-bit unsigned** greater than/greater than or equal |\n\n### MIR integer overflow insns\n  * All the insns set up an overflow flag which can be checked by branches on overflow `MIR_BO`, `MIR_BNO`, `MIR_UBO`, and `MIR_UBNO`\n  * If insn has suffix `S` in insn name, the insn works with lower 32-bit part of 64-bit integer value\n  * The higher part of 32-bit insn result is undefined\n  * If insn has prefix `U` in insn name, the insn treats integer as unsigned integers\n\n    | Insn Code               | Nops |   Description                                          |\n    |-------------------------|-----:|--------------------------------------------------------|\n    | `MIR_ADDO`, `MIR_SUBO`  | 3    | **64-bit** integer addition and subtraction            |\n    | `MIR_ADDOS`, `MIR_SUBOS`| 3    | **32-bit** integer addition and subtraction            |\n    | `MIR_MULO`, `MIR_MULOS` | 3    | **64-bit signed**  multiplication and divison          |\n    | `MIR_UMUL`, `MIR_UMULOS`| 3    | **64-bit unsigned** integer multiplication and divison |\n\n### MIR floating point insns\n  * If insn has prefix `F` in insn name, the insn is single precision float point insn.  Its operands should have `MIR_T_F` type\n  * If insn has prefix `D` in insn name, the insn is double precision float point insn.  Its operands should have `MIR_T_D` type\n  * Otherwise, insn has prefix `LD` in insn name and the insn is a long double insn.\n    Its operands should have `MIR_T_LD` type.\n  * The result of comparison insn is a 64-bit integer value, so the result operand should be of integer type\n\n    | Insn Code                            | Nops |   Description                                                   |\n    |--------------------------------------|-----:|-----------------------------------------------------------------|\n    | `MIR_F2I`, `MIR_D2I`, `MIR_LD2I`     | 2    | transforming floating point value into 64-bit integer           |\n    | `MIR_F2D`                            | 2    | transforming single to double precision FP value                |\n    | `MIR_F2LD`                           | 2    | transforming single precision to long double FP value           |\n    | `MIR_D2F`                            | 2    | transforming double to single precision FP value                |\n    | `MIR_D2LD`                           | 2    | transforming double precision to long double FP value           |\n    | `MIR_LD2F`                           | 2    | transforming long double to single precision FP value           |\n    | `MIR_LD2D`                           | 2    | transforming long double to double precision FP value           |\n    | `MIR_I2F`, `MIR_I2D`, `MIR_I2LD`     | 2    | transforming 64-bit integer into a floating point value         |\n    | `MIR_UI2F`, `MIR_UI2D`, `MIR_UI2LD`  | 2    | transforming unsigned 64-bit integer into a floating point value|\n    | `MIR_FNEG`, `MIR_DNEG`, `MIR_LDNEG`  | 2    | changing sign of floating point value                           |\n    | `MIR_FADD`, `MIR_FSUB`               | 3    | **single** precision addition and subtraction                   |\n    | `MIR_DADD`, `MIR_DSUB`               | 3    | **double** precision addition and subtraction                   |\n    | `MIR_LDADD`, `MIR_LDSUB`             | 3    | **long double** addition and subtraction                        |\n    | `MIR_FMUL`, `MIR_FDIV`               | 3    | **single** precision multiplication and divison                 |\n    | `MIR_DMUL`, `MIR_DDIV`               | 3    | **double** precision multiplication and divison                 |\n    | `MIR_LDMUL`, `MIR_LDDIV`             | 3    | **long double** multiplication and divison                      |\n    | `MIR_FEQ`, `MIR_FNE`                 | 3    | equality/inequality of **single** precision values              |\n    | `MIR_DEQ`, `MIR_DNE`                 | 3    | equality/inequality of **double** precision values              |\n    | `MIR_LDEQ`, `MIR_LDNE`               | 3    | equality/inequality of **long double** values                   |\n    | `MIR_FLT`, `MIR_FLE`                 | 3    | **single** precision less than/less than or equal               |\n    | `MIR_DLT`, `MIR_DLE`                 | 3    | **double** precision less than/less than or equal               |\n    | `MIR_LDLT`, `MIR_LDLE`               | 3    | **long double** less than/less than or equal                    |\n    | `MIR_FGT`, `MIR_FGE`                 | 3    | **single** precision greater than/greater than or equal         |\n    | `MIR_DGT`, `MIR_DGE`                 | 3    | **double** precision greater than/greater than or equal         |\n    | `MIR_LDGT`, `MIR_LDGE`               | 3    | **long double** greater than/greater than or equal              |\n\n### MIR address insns\n  * The insns take address of variable as the 2nd operand and put it into the 1st operand\n    * They are used to implement C expression `&local_variable`\n\n    | Insn Code               | Nops |   Description                                                         |\n    |-------------------------|-----:|-----------------------------------------------------------------------|\n    | `MIR_ADDR`              | 2    | Take address of variable in its natural type                          |\n    | `MIR_ADDR8`             | 2    | Take address of variable treated as 8-bit value                       |\n    | `MIR_ADDR16`            | 2    | Take address of variable treated as 16-bit value                      |\n    | `MIR_ADDR32`            | 2    | Take address of variable treated as 32-bit value                      |\n\n  * `MIR_ADDR` is used to address variables keeping values of 64-bit integer, float, double, or long double types\n  * Other address insns are used to address variables keeping integer values of smaller types\n  * Usage of the right address insn is important for big-endian targets\n\n### MIR branch insns\n  * The first operand of the insn should be label\n\n    | Insn Code               | Nops |   Description                                                         |\n    |-------------------------|-----:|-----------------------------------------------------------------------|\n    | `MIR_JMP`               | 1    | unconditional jump to the label                                       |\n    | `MIR_BT`                | 2    | jump to the label when 2nd **64-bit** operand is **nonzero**          |\n    | `MIR_BTS`               | 2    | jump to the label when 2nd **32-bit** operand is **nonzero**          |\n    | `MIR_BF`                | 2    | jump to the label when 2nd **64-bit** operand is **zero**             |\n    | `MIR_BFS`               | 2    | jump to the label when 2nd **32-bit** operand is **zero**             |\n    | `MIR_JMPI`              | 1    | unconditional jump to address in **64-bit** operand (see `MIR_LADRR`) |\n\n### MIR branch on overflow insns\n  * The first operand of the insn should be label\n  * The previous insn must be a MIR integer overflow insn\n\n    | Insn Code               | Nops |   Description                                                         |\n    |-------------------------|-----:|-----------------------------------------------------------------------|\n    | `MIR_BO`                | 1    | jump to the label when signed overflow is set up                      |\n    | `MIR_BNO`               | 1    | jump to the label when signed overflow is not set up                  |\n    | `MIR_UBO`               | 1    | jump to the label when unsigned overflow is set up                    |\n    | `MIR_UBNO`              | 1    | jump to the label when unsigned overflow is set up                    |\n\n### `MIR_LADDR` insn\n  * The insn takes address of a function label given as the 2nd operand and put it into 64-bit integer register\n    or memory given as the the first operand\n  * The insn with `MIR_JMPI` insn is used to implement GNU C extension \"labels as values\"\n\n### MIR switch insn\n  * The first operand of `MIR_SWITCH` insn should have an integer value from 0 to `N - 1` inclusive\n  * The rest operands should be `N` labels, where `N > 0`\n  * Execution of the insn will be an jump on the label corresponding to the first operand value\n  * If the first operand value is out of the range of permitted values, the execution result is undefined\n\n### MIR integer comparison and branch insn\n  * The first operand of the insn should be label.  Label will be the next executed insn if the result of comparison is non-zero\n\n    | Insn Code               | Nops |   Description                                                 |\n    |-------------------------|-----:|---------------------------------------------------------------|\n    | `MIR_BEQ`, `MIR_BNE`    | 3    | jump on **64-bit** equality/inequality                        |\n    | `MIR_BEQS`, `MIR_BNES`  | 3    | jump on **32-bit** equality/inequality                        |\n    | `MIR_BLT`, `MIR_BLE`    | 3    | jump on **signed 64-bit** less than/less than or equal        |\n    | `MIR_UBLT`, `MIR_UBLE`  | 3    | jump on **unsigned 64-bit** less than/less than or equal      |\n    | `MIR_BLTS`, `MIR_BLES`  | 3    | jump on **signed 32-bit** less than/less than or equal        |\n    | `MIR_UBLTS`, `MIR_UBLES`| 3    | jump on **unsigned 32-bit** less than/less than or equal      |\n    | `MIR_BGT`, `MIR_BGE`    | 3    | jump on **signed 64-bit** greater than/greater than or equal  |\n    | `MIR_UBGT`, `MIR_UBGE`  | 3    | jump on **unsigned 64-bit** greater than/greater than or equal|\n    | `MIR_BGTS`, `MIR_BGES`  | 3    | jump on **signed 32-bit** greater than/greater than or equal  |\n    | `MIR_UBGTS`, `MIR_UBLES`| 3    | jump on **unsigned 32-bit** greater than/greater than or equal|\n\n### MIR floating point comparison and branch insn\n  * The first operand of the insn should be label.  Label will be the next executed insn if the result of comparison is non-zero\n  * See comparison semantics in the corresponding comparison insns\n\n    | Insn Code                 | Nops |   Description                                                  |\n    |---------------------------|-----:|----------------------------------------------------------------|\n    | `MIR_FBEQ`, `MIR_FBNE`    | 3    | jump on **single** precision equality/inequality               |\n    | `MIR_DBEQ`, `MIR_DBNE`    | 3    | jump on **double** precision equality/inequality               |\n    | `MIR_LDBEQ`, `MIR_LDBNE`  | 3    | jump on **long double** equality/inequality                    |\n    | `MIR_FBLT`, `MIR_FBLE`    | 3    | jump on **single** precision less than/less than or equal      |\n    | `MIR_DBLT`, `MIR_DBLE`    | 3    | jump on **double** precision less than/less than or equal      |\n    | `MIR_LDBLT`, `MIR_LDBLE`  | 3    | jump on **long double** less than/less than or equal           |\n    | `MIR_FBGT`, `MIR_FBGE`    | 3    | jump on **single** precision greater than/greater than or equal|\n    | `MIR_DBGT`, `MIR_DBGE`    | 3    | jump on **double** precision greater than/less/ than or equal  |\n    | `MIR_LDBGT`, `MIR_LDBGE`  | 3    | jump on **long double** greater than/less/ than or equal       |\n\n### MIR_RET insn\n  * Return insn has zero or more operands\n  * Return insn operands should correspond to return types of the function\n  * 64-bit integer value is truncated to the corresponding function return type first\n  * The return values will be the function call values\n\n### MIR_CALL insn\n  * The insn has variable number of operands\n  * The first operand is a prototype reference operand\n  * The second operand is a called function address\n    * The prototype should correspond MIR function definition if function address represents a MIR function\n    * The prototype should correspond C function definition if the address is C function address\n  * If the prototype has *N* return types, the next *N* operands are\n    output operands which will contain the result values of the function\n    call\n  * The subsequent operands are arguments.  Their types and number and should be the same as in the prototype\n    * Integer arguments are truncated according to integer prototype argument type\n\n### MIR_INLINE insn\n  * This insn is analogous to `MIR_CALL` but after linking this insn\n    will be changed by inlined function body if it is possible\n  * Calls of vararg functions are never inlined\n\n### MIR_JCALL and MIR_JRET insns\n  * `MIR_JCALL` calls a function without setting up the return address\n    (usually return address is put put on the stack or in register depending on ABI)\n    * The first operand is a prototype reference operand\n    * The second operand is a called function address\n  * `MIR_JRET` returns from the function called by `MIR_JCALL`\n    * The single operand contains and the return address as 64-bit integer value\n  * To call a function you can only use pairs `MIR_CALL` (or `MIR_INLINE`) and `MIR_RET` or `MIR_JCALL` and `MIR_JRET`\n  * `MIR_JCALL` and `MIR_JRET` implement non-standard ABI for functions without args and return values\n    * The ABI is useful for high performance switching from direct threading interpreters to JITted code\n    * The argument and return values can be passed through global vars which are tied to specific hard regs\n\n### MIR_ALLOCA insn\n  * Reserve memory on the stack whose size is given as the 2nd operand and assign the memory address to the 1st operand\n  * The reserved memory will be aligned according target ABI\n\n### MIR_BSTART and MIR_BEND insns\n  * MIR users can use them implement blocks with automatic\n    deallocation of memory allocated by `MIR_ALLOCA` inside the\n    blocks.  But mostly these insns are used to implement call\n    inlining of functions using alloca\n  * The both insns use one operand\n  * The first insn saves the stack pointer in the operand\n  * The second insn restores stack pointer from the operand\n\n### MIR_VA_START, MIR_VA_ARG, MIR_VA_BLOCK_ARG, and MIR_VA_END insns\n  * These insns are only for variable number arguments functions\n  * `MIR_VA_START` and `MIR_VA_END` have one input operand, an address\n    of va_list structure (see C stdarg.h for more details).  Unlike C\n    va_start, MIR_VA_START just takes one parameter\n  * `MIR_VA_ARG` takes va_list and any memory operand and returns\n    address of the next argument in the 1st insn operand.  The memory\n    operand type defines the type of the argument\n  * `MIR_VA_BLOCK_ARG` takes result address, va_list address, integer operand (size),\n    and block type (case) number and moves the next argument passed as block of given\n    size and type to the result address\n  * va_list operand can be memory with undefined type.  In this case\n    address of the va_list is not in the memory but is the\n    memory address\n\n### MIR property insns\n  * These insns never generate any corresponding machine insns but can be used to a generate specialized code\n    when lazy basic block versioning is used\n  * `MIR_PRSET` sets property of the variable given as the 1st operand to integer constant given as the 2nd operand\n  * `MIR_PRBEQ` and `MIR_PRBNE` jumps to the label given as the 1st operand if property of the variable given as the\n    2nd operand equal or not equal to integer constant given as the 2rd operand\n\n## MIR API example\n  * The following code on C creates MIR analog of C code\n    `int64_t loop (int64_t arg1) {int64_t count = 0; while (count < arg1) count++; return count;}`\n```c\n  MIR_module_t m = MIR_new_module (ctx, \"m\");\n  MIR_item_t func = MIR_new_func (ctx, \"loop\", MIR_T_I64, 1, MIR_T_I64, \"arg1\");\n  MIR_reg_t COUNT = MIR_new_func_reg (ctx, func->u.func, MIR_T_I64, \"count\");\n  MIR_reg_t ARG1 = MIR_reg (ctx, \"arg1\", func->u.func);\n  MIR_label_t fin = MIR_new_label (ctx), cont = MIR_new_label (ctx);\n\n  MIR_append_insn (ctx, func, MIR_new_insn (ctx, MIR_MOV, MIR_new_reg_op (ctx, COUNT),\n                                            MIR_new_int_op (ctx, 0)));\n  MIR_append_insn (ctx, func, MIR_new_insn (ctx, MIR_BGE, MIR_new_label_op (ctx, fin),\n                                            MIR_new_reg_op (ctx, COUNT), MIR_new_reg_op (ctx, ARG1)));\n  MIR_append_insn (ctx, func, cont);\n  MIR_append_insn (ctx, func, MIR_new_insn (ctx, MIR_ADD, MIR_new_reg_op (ctx, COUNT),\n                                            MIR_new_reg_op (ctx, COUNT), MIR_new_int_op (ctx, 1)));\n  MIR_append_insn (ctx, func, MIR_new_insn (ctx, MIR_BLT, MIR_new_label_op (ctx, cont),\n                                            MIR_new_reg_op (ctx, COUNT), MIR_new_reg_op (ctx, ARG1)));\n  MIR_append_insn (ctx, func, fin);\n  MIR_append_insn (ctx, func, MIR_new_ret_insn (ctx, 1, MIR_new_reg_op (ctx, COUNT)));\n  MIR_finish_func (ctx);\n  MIR_finish_module (ctx);\n```\n\n## MIR text examples\n\n  * Sieve of Eratosthenes:\n\n```mir\nm_sieve:  module\n          export sieve\nsieve:    func i32, i32:N\n          local i64:iter, i64:count, i64:i, i64:k, i64:prime, i64:temp, i64:flags\n          alloca flags, 819000\n          mov iter, 0\nloop:     bge fin, iter, N\n          mov count, 0;  mov i, 0\nloop2:    bge fin2, i, 819000\n          mov u8:(flags, i), 1;  add i, i, 1\n          jmp loop2\nfin2:     mov i, 0\nloop3:    bge fin3, i, 819000\n          beq cont3, u8:(flags,i), 0\n          add temp, i, i;  add prime, temp, 3;  add k, i, prime\nloop4:    bge fin4, k, 819000\n          mov u8:(flags, k), 0;  add k, k, prime\n          jmp loop4\nfin4:     add count, count, 1\ncont3:    add i, i, 1\n          jmp loop3\nfin3:     add iter, iter, 1\n          jmp loop\nfin:      ret count\n          endfunc\n          endmodule\n\nm_ex100:  module\nformat:   string \"sieve (10) = %d\\n\"\np_printf: proto p:fmt, i32:v\np_sieve:  proto i32, i32:iter\n          export ex100\n          import sieve, printf\nmain:    func\n          local i64:r\n          call p_sieve, sieve, r, 100\n          call p_printf, printf, format, r\n          endfunc\n          endmodule\n\n```\n\n  * Example of block arguments and `va_stack_arg`\n\n```mir\nm0:       module\nf_p:\t  proto i64, 16:blk(a), ...\nf:\t  func i64, 16:blk(a), ...\n          local i64:r, i64:va, i64:a2\n\t  alloca va, 32  # allocate enough space va_list\n\t  va_start va\n\t  va_stack_arg a2, va, 16 # get address of the 2nd blk arg\n\t  add r, i64:0(a), i64:8(a2)\n\t  ret r\nmain:\t  func\n\t  local i64:a, i64:r\n\t  alloca a, 16\n          mov i64:0(a), 42\n          mov i64:8(a), 24\n\t  call f_p, f, r, blk:16(a), blk:16(a)\n\t  ret r\n\t  endfunc\n          endmodule\n```\n\n## Other MIR API functions\n  * MIR API can find a lot of errors.  They are reported through a\n    error function of type `void (*MIR_error_func_t) (MIR_context ctx, MIR_error_type_t\n    error_type, const char *message)`.  The function is considered to\n    never return.  To see all error types, please look at the\n    definition of error type `MIR_error_type_t` in file mir.h\n  * You can get and set up the current error function through API\n    functions `MIR_error_func_t MIR_get_error_func (MIR_context ctx)` and `MIR_set_error_func\n    (MIR_context ctx, MIR_error_func_t func)`.\n    * The default error function prints the message into stderr and call `exit (1)`\n  * MIR is pretty flexible and can describe complex insns, e.g. insns\n    whose all operands are memory.  Sometimes you need a very simple\n    form of MIR representation.  During load of module all its functions are simplified as much\n    as possible by adding new insns and registers resulting in a form in which:\n    * immediate, memory, reference operands can be used only in move insns\n    * memory have only base register (no displacement and index register)\n    * string and float immediate operands (if `mem_float_p`) are changed onto\n      references for new string and data items\n  * Before execution of MIR code (through interpreter or machine code generated by JIT),\n    you need to load and link it\n    * You can load MIR module through API function `MIR_load_module\n      (MIR_context ctx, MIR_module_t m)`.  The function simplifies module code.\n      It also allocates the module data/bss/refs/lrefs\n      and makes visible the exported module items to other module\n      during subsequent linking.  There is a guarantee that the\n      different data/bss/lref items will be in adjacent memory if the\n      data/bss/lref items go one after another and all the data/bss/lref items\n      except the first one are anonymous (it means they have no name).\n      Such adjacent data/bss/lref items are called a **section**.\n      Alignment of the section is malloc alignment.  There are no any\n      memory space between data/bss/refs/lrefs in the section.  If you need to\n      provide necessary alignment of a data/bss/refs/lrefs in the section you\n      should do it yourself by putting additional anonymous data/bss/refs/lrefs\n      before given data/bss/refs/lrefs if it is necessary.  BSS memory is\n      initialized by zero and data memory is initialized by the\n      corresponding data.  If there is already an exported item with\n      the same name, it will be not visible for linking anymore.  Such\n      visibility mechanism permits usage of different versions of the\n      same function\n    * Reference data are initialized not during loading but during linking after\n      the referenced item address is known.  The address is used for the data\n      initialization\n    * Label references are initialized during the corresponding function generation or interpretation\n    * Expression data are also initialized not during loading but during linking after\n      all addresses are known.  The expression function is evaluated by the interpreter\n      and its evaluation result is used for the data initialization.  For example, if\n      you need to initialize data by item address plus offset you should use\n      an expression data\n    * MIR permits to use imported items not implemented in MIR, for\n      example to use C standard function `strcmp`.  You need to inform\n      MIR about it.  API function `MIR_load_external (MIR_context ctx, const char\n      *name, void *addr)` informs that imported items with given name\n      have given address (e.g. C function address or data)\n    * Imports/exports of modules loaded since the last link can be\n      linked through API function `MIR_link (MIR_context ctx, void (*set_interface) (MIR_item_t item),\n      void * (*import_resolver) (const char *))`\n    * `MIR_link` function inlines most `MIR_INLINE` calls\n    * `MIR_link` function also sets up call interface\n      * If you pass `MIR_set_interp_interface` to `MIR_link`, then\n        called functions from MIR code will be interpreted\n      * If you pass `MIR_set_gen_interface` to `MIR_link`, then\n        MIR-generator will generate machine code for all loaded MIR\n        functions and called functions from MIR code will execute the\n        machine code\n      * If you pass `MIR_set_lazy_gen_interface` to `MIR_link`, then\n        MIR-generator will generate machine code only on the first\n        function call and called functions from MIR code will execute\n        the machine code\n      * If you pass `MIR_set_lazy_bb_gen_interface` to `MIR_link`, then\n        MIR-generator will generate machine code of function basic blocks\n\tonly on their first execution\n      * If you pass non-null `import_resolver` function, it will be\n        called for defining address for import without definition.\n        The function get the import name and return the address which\n        will be used for the import item.  This function can be useful\n        for searching `dlopen` library symbols when use of\n        MIR_load_external is not convenient\n\n# MIR code execution\n  * Linked MIR code can be executed by an **interpreter** or machine code generated by **MIR generator**\n\n# MIR code interpretation\n  * The interpreter is an obligatory part of MIR API because it can be used during linking\n  * The interpreter is automatically initialized and finished with MIR API initialization and finishing\n  * The interpreter works with values represented by type `MIR_val_t` which is union\n    `union {..., int64_t i; uint64_t u; float f; double d; long double d;}`\n  * You can execute a MIR function code by API functions `void\n    MIR_interp (MIR_context ctx, MIR_item_t func_item, MIR_val_t *results, size_t nargs, ...)` and\n    `void MIR_interp_arr (MIR_context ctx, MIR_item_t func_item, MIR_val_t *results, size_t nargs,\n    MIR_val_t *vals)`\n    * The function results are returned through parameter `results`.  You should pass\n      a container of enough size to return all function results.\n  * You can execute a MIR function code also through C function call\n    mechanism.  First you need to setup the C function interface\n    through API function `MIR_set_interp_interface (MIR_context ctx, MIR_item_t\n    func_item)`.  After that you can `func_item->addr` to call the\n    MIR function as usual C function\n    * C function interface is implemented by generation of machine\n      code specialized for MIR function.  Therefore the interface\n      works only on the same targets as MIR generator\n\n# MIR generator (file mir-gen.h)\n  * Before use of MIR generator for given context you should initialize it by API function\n    `MIR_gen_init (MIR_context ctx)`\n  * API function `MIR_gen_finish (MIR_context ctx)` frees all internal generator data (and its instances) for the context.\n    If you want to generate code for the context again after the `MIR_gen_finish` call, you should call\n    `MIR_gen_init` again first\n  * API function `void *MIR_gen (MIR_context ctx, MIR_item_t func_item)` generates machine code\n    of given MIR function in generator and returns an address to call it.  You can call\n    the code as usual C function by using this address as the called function address\n  * API function `void MIR_gen_set_debug_file (MIR_context_t ctx, FILE *f)` sets up MIR generator\n    debug file to `f` for the generator.\n    If it is not NULL a debugging and optimization information will be output to the file according to the\n    current generator debug level.  It is useful mostly for MIR developers\n  * API function `void MIR_gen_set_debug_level (MIR_context_t ctx, level)` sets up MIR generator\n    debug level to `level` for the generator.  The default level value is maximum possible level\n    for printing information as much as possible.  Negative level results in no output.  The function is useful\n    mostly for MIR developers\n  * API function `void MIR_gen_set_optimize_level (MIR_context_t ctx, unsigned int level)` sets up optimization\n    level for MIR generator:\n    * `0` means only register allocator and machine code generator work\n    * `1` means additional code selection task.  On this level MIR generator creates more compact and faster\n      code than on zero level with practically on the same speed\n    * `2` means additionally common sub-expression elimination and sparse conditional constant propagation.\n       This is a default level.  This level is valuable if you generate bad input MIR code with a lot redundancy\n       and constants.  The generation speed on level `1` is about 50% faster than on level `2`\n    * `3` means additionally register renaming and loop invariant code motion.  The generation speed\n      on level `2` is about 50% faster than on level `3`\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 25.64453125,
          "content": "<p>\n<a href=\"https://github.com/vnmakarov/mir/actions?query=workflow%3AAMD64%2DLinux%2DOSX%2DWindows%2Dtest\"><img alt=\"GitHub MIR test status\" src=\"https://github.com/vnmakarov/mir/workflows/AMD64%2DLinux%2DOSX%2DWindows%2Dtest/badge.svg\"></a>\n<a href=\"https://github.com/vnmakarov/mir/actions?query=workflow%3Aapple%2Daarch64%2Dtest\"><img alt=\"GitHub MIR test status on Apple Silicon\" src=\"https://github.com/vnmakarov/mir/workflows/apple%2Daarch64%2Dtest/badge.svg\"></a>\n<a href=\"https://github.com/vnmakarov/mir/actions?query=workflow%3Aaarch64%2Dtest\"><img alt=\"GitHub MIR test status on aarch64\" src=\"https://github.com/vnmakarov/mir/workflows/aarch64%2Dtest/badge.svg\"></a>\n<a href=\"https://github.com/vnmakarov/mir/actions?query=workflow%3Appc64le%2Dtest\"><img alt=\"GitHub MIR test status on ppc64le\" src=\"https://github.com/vnmakarov/mir/workflows/ppc64le%2Dtest/badge.svg\"></a>\n<a href=\"https://github.com/vnmakarov/mir/actions?query=workflow%3As390x%2Dtest\"><img alt=\"GitHub MIR test status on s390x\" src=\"https://github.com/vnmakarov/mir/workflows/s390x%2Dtest/badge.svg\"></a>\n<a href=\"https://github.com/vnmakarov/mir/actions?query=workflow%3Ariscv64%2Dtest\"><img alt=\"GitHub MIR test status on riscv64\" src=\"https://github.com/vnmakarov/mir/workflows/riscv64%2Dtest/badge.svg\"></a>\n<a href=\"https://github.com/vnmakarov/mir/actions?query=workflow%3AAMD64%2DLinux%2Dbench\"><img alt=\"GitHub MIR benchmark status\" src=\"https://github.com/vnmakarov/mir/workflows/AMD64%2DLinux%2Dbench/badge.svg\"></a>\n</p>\n\n# MIR Project\n  * MIR means **M**edium **I**nternal **R**epresentation\n  * MIR project goal is to provide a basis to implement fast and lightweight JITs\n  * Plans to try MIR light-weight JIT first for CRuby or/and MRuby implementation\n  * Motivations for the project can be found in [this blog post](https://developers.redhat.com/blog/2020/01/20/mir-a-lightweight-jit-compiler-project)\n  * C2MIR compiler description can be found in [this blog post](https://developers.redhat.com/blog/2021/04/27/the-mir-c-interpreter-and-just-in-time-jit-compiler)\n  * Future of code specialization in MIR for dynamic language JITs can be found in [this blog post](https://developers.redhat.com/articles/2022/02/16/code-specialization-mir-lightweight-jit-compiler)\n\n## Disclaimer\n   * **There is absolutely no warranty that the code will work for any tests except ones given here and on platforms\n     other than x86_64 Linux/OSX, aarch64 Linux/OSX(Apple M1), and ppc64le/s390x/riscv64 Linux**\n\n## MIR\n  * MIR is strongly typed IR\n  * MIR can represent machine 32-bit and 64-bit insns of different architectures\n  * [MIR.md](https://github.com/vnmakarov/mir/blob/master/MIR.md) contains detail description of MIR and its API.\n    Here is a brief MIR description:\n  * MIR consists of **modules**\n    * Each module can contain **functions** and some declarations and data\n    * Each function has **signature** (parameters and return types), **local variables**\n      (including function arguments) and **instructions**\n      * Each local variable has **type** which can be only 64-bit integer, float, double, or long double\n        and can be bound to a particular target machine register\n      * Each instruction has **opcode** and **operands**\n        * Operand can be a local variable\n\t  (or a function argument), **immediate**, **memory**, **label**, or **reference**\n          * Immediate operand can be 64-bit integer, float, double, or long double value\n\t  * Memory operand has a **type**, **displacement**, **base** and **index** integer local variable,\n\t    and integer constant as a **scale** for the index\n\t    * Memory type can be 8-, 16-, 32- and 64-bit signed or unsigned integer type,\n\t      float type, double, or long double type\n\t      * When integer memory value is used it is expanded with sign or zero promoting\n\t        to 64-bit integer value first\n\t  * Label operand has name and used for control flow instructions\n\t  * Reference operand is used to refer to functions and declarations in the current module,\n\t    in other MIR modules, or for C external functions or declarations\n\t* opcode describes what the instruction does\n\t* There are **conversion instructions** for conversion between different\n\t  32- and 64-bit signed and unsigned values, float, double, and long double values\n\t* There are **arithmetic instructions** (addition, subtraction, multiplication, division,\n\t  modulo) working on 32- and 64-bit signed and unsigned values, float, double, and long double values\n\t* There are **logical instructions** (and, or, xor, different shifts) working on\n\t  32- and 64-bit signed and unsigned values\n\t* There are **comparison instructions**  working on 32- and 64-bit\n\t  signed and unsigned values, float, double, and long double values\n\t* There are **local variable address instructions** to get address of local variable\n\t* There are **branch insns** (unconditional jump, and jump on zero or non-zero value)\n\t  which take a label as one their operand\n\t* There are **combined comparison and branch instructions** taking a label as one operand\n\t  and two 32- and 64-bit signed and unsigned values, float, double, and long double values\n\t* There is **switch** instruction to jump to a label from labels given as operands\n\t  depending on index given as the first operand\n\t* There is **label address instruction** to get a label address\n\t  and **unconditional indirect jump instruction** whose operand contains previously taken label address\n\t* There are **function and procedural call instructions**\n\t* There are **return instructions** optionally returning 32- and 64-bit\n\t  integer values, float, double, and long double values\n\t* There are **specialized light-weight call and return instructions** can be used for\n\t  fast switching from threaded interpreter to JITted code and vice verse\n\t* There are **property** instructions to generated specialized machine code when lazy basic block versioning is used\n\n## MIR Example\n  * You can create MIR through **API** consisting of functions for creation of modules,\n    functions, instructions, operands etc\n  * You can also create MIR from MIR **binary** or **text** file\n  * The best way to get a feel about MIR is to use textual MIR representation\n  * Example of Eratosthenes sieve on C\n```c\n#define Size 819000\nint sieve (int N) {\n  int64_t i, k, prime, count, n; char flags[Size];\n\n  for (n = 0; n < N; n++) {\n    count = 0;\n    for (i = 0; i < Size; i++)\n      flags[i] = 1;\n    for (i = 0; i < Size; i++)\n      if (flags[i]) {\n        prime = i + i + 3;\n        for (k = i + prime; k < Size; k += prime)\n          flags[k] = 0;\n        count++;\n      }\n  }\n  return count;\n}\nvoid ex100 (void) {\n  printf (\"sieve (100) = %d\\\", sieve (100));\n}\n```\n  * Example of MIR textual file for the same function:\n```mir\nm_sieve:  module\n          export sieve\nsieve:    func i32, i32:N\n          local i64:iter, i64:count, i64:i, i64:k, i64:prime, i64:temp, i64:flags\n          alloca flags, 819000\n          mov iter, 0\nloop:     bge fin, iter, N\n          mov count, 0;  mov i, 0\nloop2:    bge fin2, i, 819000\n          mov u8:(flags, i), 1;  add i, i, 1\n          jmp loop2\nfin2:     mov i, 0\nloop3:    bge fin3, i, 819000\n          beq cont3, u8:(flags,i), 0\n          add temp, i, i;  add prime, temp, 3;  add k, i, prime\nloop4:    bge fin4, k, 819000\n          mov u8:(flags, k), 0;  add k, k, prime\n          jmp loop4\nfin4:     add count, count, 1\ncont3:    add i, i, 1\n          jmp loop3\nfin3:     add iter, iter, 1\n          jmp loop\nfin:      ret count\n          endfunc\n          endmodule\nm_ex100:  module\nformat:   string \"sieve (10) = %d\\n\"\np_printf: proto p:fmt, i32:result\np_sieve:  proto i32, i32:iter\n          export ex100\n          import sieve, printf\nex100:    func v, 0\n          local i64:r\n          call p_sieve, sieve, r, 100\n          call p_printf, printf, format, r\n          endfunc\n          endmodule\n```\n\n  * `func` describes signature of the function (taking 32-bit signed\n    integer argument and returning 32-bit signed integer value)\n    and function argument `N` which will be local\n    variable of 64-bit signed integer type\n    * Function results are described first by their types and have no names.\n      Parameters always have names and go after the result description\n    * Function may have more than one result but possible number and combination\n      of result types are currently machine defined\n  * You can write several instructions on one line if you separate them by `;`\n  * The instruction result, if any, is always the first operand\n  * We use 64-bit instructions in calculations\n  * We could use 32-bit instructions in calculations which would have sense if we use 32-bit CPU\n    * When we use 32-bit instructions we take only 32-bit significant part of 64-bit operand\n      and high 32-bit part of the result is machine defined (so if you write a portable MIR code\n      consider the high 32-bit part value is undefined)\n  * `string` describes data in form of C string\n     * C string can be used directly as an insn operand.  In this case the data will be added\n       to the module and the data address will be used as an operand\n  * `export` describes the module functions or data which are visible outside the current module\n  * `import` describes the module functions or data which should be defined in other MIR modules\n  * `proto` describes function prototypes.  Its syntax is the same as `func` syntax\n  * `call` are MIR instruction to call functions\n\n## Running MIR code\n  * After creating MIR modules (through MIR API or reading MIR binary or textual files),\n    you should load the modules\n    * Loading modules makes visible exported module functions and data\n    * You can load external C function with `MIR_load_external`\n  * After loading modules, you should link the loaded modules\n    * Linking modules resolves imported module references, initializes data,\n      and set up call interfaces\n  * After linking, you can interpret functions from the modules or call machine code\n    for the functions generated with MIR JIT compiler (generator).  What way the function can be executed\n    is usually defined by set up interface.  How the generated code is produced (lazily on the first call or ahead of time)\n    can be also dependent on the interface\n  * Running code from the above example could look like the following (here `m1` and `m2` are modules\n    `m_sieve` and `m_e100`, `func` is function `ex100`, `sieve` is function `sieve`):\n```c\n    /* ctx is a context created by MIR_init / MIR_init2 */\n    MIR_load_module (ctx, m1); MIR_load_module (ctx, m2);\n    MIR_load_external (ctx, \"printf\", printf);\n    MIR_link (ctx, MIR_set_interp_interface, import_resolver);\n    /* or use MIR_set_gen_interface to generate and use the machine code */\n    /* or use MIR_set_lazy_gen_interface to generate function code on its 1st call */\n    /* use MIR_gen (ctx, func) to explicitly generate the function machine code */\n    MIR_interp (ctx, func, &result, 0); /* zero here is arguments number  */\n    /* or ((void (*) (void)) func->addr) (); to call interpr. or gen. code through the interface */\n```\n\n### Running binary MIR files on Linux through `binfmt_misc`\n\nThe `mir-bin-run` binary is prepared to be used from `binfmt_misc` with the\nfollowing line (example):\n\n```bash\nline=:mir:M::MIR::/usr/local/bin/mir-bin-run:P\necho $line > /proc/sys/fs/binfmt_misc/register\n```\n\n> Do adapt the mir-bin-run binary path to your system, that is the default one\n\nAnd run with\n```bash\nc2m your-file.c -o your-file\nchmod +x your-file\n./your-file your args\n```\n\nThe executable is \"configurable\" with environment variables:\n\n* `MIR_TYPE` sets the interface for code execution: `interp` (for interpretation),\n  `jit` (for generation) and `lazy` (for lazy generation, default);\n* `MIR_LIBS` (colon separated list) defines a list of extra libraries to load;\n* `MIR_LIB_DIRS` or `LD_LIBRARY_PATH` (colon separated list) defines an extra list\n  of directories to search the libraries on.\n\n\n> Due to the tied nature of `mir-bin-run` with `binfmt_misc`, it may be a bit weird\n> to call `mir-bin-run` directly.\n> The `P` flag on the binfmt_misc passes an extra argument with the full path\n> to the MIR binary.\n\n## The current state of MIR project\n\n  ![Current MIR](mir3.svg)\n\n  * You can use C **setjmp/longjmp** functions to implement **longjump** in MIR\n  * Binary MIR code is usually upto **10 times more compact** and upto **10 times faster to read**\n    than analogous MIR textual code\n  * MIR interpreter is about 6-10 times slower than code generated by MIR JIT compiler\n  * LLVM IR to MIR translator has not been finished and probably will be never fully implemented\n    as LLVM IR is much richer than MIR but translation of LLVM IR generated from standard C/C++ to MIR\n    is a doable task\n\n## The possible future state of MIR project\n  ![Future MIR](mirall.svg)\n\n  * WASM to MIR translation should be pretty straightforward\n    * Only small WASM runtime for WASM floating point round insns needed to be provided for MIR\n  * Porting GCC to MIR is possible too.  An experienced GCC developer can implement this\n    for 6 to 12 months\n  * On my estimation porting MIR JIT compiler to mips64 or sparc64 will take\n    1-2 months of work for each target\n  * Performance minded porting MIR JIT compiler to 32-bit targets will need an implementation of\n    additional small analysis pass to get info what 64-bit variables are used only\n    in 32-bit instructions\n\n## MIR JIT compiler\n  * Very short optimization pipeline for speed and light-weight\n  * Only the **most valuable** optimization usage:\n    * **function inlining**\n    * **global common sub-expression elimination**\n    * **variable renaming**\n    * **register pressure sensitive loop invariant code motion**\n    * **conditional constant propagation**\n    * **dead code elimination**\n    * **code selection**\n    * fast **register allocator** with\n      * aggressive coalescing registers and stack slots for copy elimination\n      * live range splitting\n  * Different optimization levels to tune compilation speed vs generated code performance\n  * **SSA** form of MIR is used before register allocation\n    * We use a form of Braun's algorithm to build SSA (M. Braun et al. \"Simple and Efficient\n      Construction of Static Single Assignment Form\")\n  * Simplicity of optimizations implementation over extreme generated code performance\n\n  * More details about **full JIT compiler pipeline**:\n![MIR generator](mir-gen.svg)\n  * **Simplify**: lowering MIR\n  * **Inline**: inlining MIR calls\n  * **Build CFG**: building Control Flow Graph (basic blocks and CFG edges)\n  * **Build SSA**: Building Single Static Assignment Form by adding phi nodes and SSA edges to operands\n  * **Address Transformation**: remove or change MIR ADDR instructions\n  * **Global Value Numbering**: removing redundant insns through GVN.  This includes constant\n    propagation and redundant load eliminations\n  * **Copy Propagation**: SSA copy propagation and removing redundant extension instructions\n  * **Dead store elimination**: removing redundant stores\n  * **Dead Code Elimination**: removing insns with unused outputs\n  * **Pressure relief**: moving insns to decrease register pressure\n  * **SSA combine**: combining addresses and compare and branch instruction pairs\n  * **Out of SSA**: Removing phi nodes and SSA edges\n  * **Jump opts**: Different jump optimizations\n  * **Machinize**: run machine-dependent code transforming MIR for calls ABI, 2-op insns, etc\n  * **Find Loops**: finding natural loops and building loop tree\n  * **Build Live Info**: calculating live in and live out for the basic blocks\n  * **Build Register Conflicts**: building conflict matrix for registers involved in moves.\n    It is used for register coalescing\n  * **Coalesce**: aggressive register coalescing\n  * **Register Allocator (RA)**: priority-based linear scan RA with live range splitting\n  * **Build Live Ranges**: calculating program point ranges for registers\n  * **Assign**: fast RA for `-O0` or priority-based linear scan RA for `-O1` and above\n  * **Rewrite**: transform MIR according to the assign using reserved hard regs\n  * **Combine** (code selection): merging data-depended insns into one\n  * **Dead Code Elimination**: removing insns with unused outputs\n  * **Generate Machine Insns**: run machine-dependent code creating machine insns\n\n## C to MIR translation\n  * We implemented a small C11 (2011 ANSI C standard with some GCC extensions) to MIR compiler `c2m`.\n    See [README.md](https://github.com/vnmakarov/mir/tree/master/c2mir)\n  * C code can be used as an input of JIT compiler besides MIR\n    * Usage of C as an input to JIT compiler can slow down compilation speed up to 2 times\n\n## Structure of the project code\n * Files `mir.h` and `mir.c` contain major API code including input/output of MIR binary\n   and MIR text representation\n * Files `mir-dlist.h`, `mir-mp.h`, `mir-varr.h`, `mir-bitmap.h`, `mir-hash.h`, `mir-htab.h`, `mir-reduce.h`\n   contain generic code  correspondingly for double-linked lists, memory pools, variable length arrays, bitmaps,\n   hash calculations, hash tables, and compressing/decompressing data.  File `mir-hash.h` is a general, simple,\n   high quality hash function used by hashtables\n * File `mir-interp.c` contains code for interpretation of MIR code.  It is included in `mir.c`\n   and never compiled separately\n * Files `mir-gen.h`, `mir-gen.c`, `mir-gen-x86_64.c`, `mir-gen-aarch64.c`, `mir-gen-ppc64.c`, `mir-gen-s390x.c`,\n   and `mir-gen-riscv64.c` contain code for MIR JIT compiler\n   * Files `mir-gen-x86_64.c`, `mir-gen-aarch64.c`, `mir-gen-ppc64.c`, `mir-gen-s390x.c`,\n   and `mir-gen-riscv64.c` is machine dependent code of JIT compiler\n * Files `mir-<target>.c` contain simple machine dependent code common for interpreter and\n   JIT compiler\n * Files `mir-<target>.h` contain declarations common for interpreter and JIT compiler\n * Files `mir2c/mir2c.h` and `mir2c/mir2c.c` contain code for MIR to C compiler.  The generated code might be not portable\n * Files `c2mir/c2mir.h`, `c2mir/c2mir.c`, `c2mir/c2mir-driver.c`, and `c2mir/mirc.h` contain code for\n   C to MIR compiler.  Files in directories `c2mir/x86_64` and `c2mir/aarch64`, `c2mir/ppc64`, `c2mir/s390x`,\n   and `c2mir/riscv64` contain correspondingly x86_64, aarch64, ppc64le, s390x, and riscv machine-dependent\n   code for C to MIR compiler\n * File `mir-bin-run.c` contains code for `mir-bin-run` described above\n * File `mir-bin-driver.c` with `b2ctab` utility can be used for portable way to generate binary from MIR binary files\n * Directory `mir-utils` contains different utilities to work with MIR,\n   e.g. transforming binary MIR to textual MIR and vice verse\n * Directory `adt-tests`, `mir-tests`, `c-tests`, and `c-benchmarks` contains code for testing and benchmarking MIR and `c2m`\n\n## Playing with current MIR project code\n  * You can run some benchmarks and tests by `make bench` and `make test`\n\n## Current MIR Performance Data\n\n  * Intel i5-13600K with 64GB memory under FC37 with GCC-12.3.1\n\n    |                | MIR-generator   | MIR-interpreter |     gcc -O2      |     gcc -O0     |\n    |----------------|-----------------|-----------------|------------------|-----------------|\n    | compilation [1]| **1.0** (249us) | 0.09 (22us)     | **109** (27.1ms) |  105 (26.1ms)   |\n    | execution [2]  | **1.0** (1.74s) | 13.7 (23.8s)    | **0.92** (1.6s)  |  2.28 (3.97s)   |\n    | code size [3]  | **1.0** (557KB) | 0.43 (240KB)    | **58** (32.2MB)  |  58 (32.2MB)    |\n    | LOC [4]        | **1.0** (23.4K) | 0.48 (11.3K)    | **103** (2420K)  | 103  (2402K)    |\n\n   [1] is based on wall time of compilation of C sieve code (w/o any include file and with\n   using memory file system for GCC) and the corresponding MIR sieve code by MIR-interpreter\n   and MIR-generator with optimization level 2\n\n   [2] is based on the best wall time of 10 runs with used MIR-generator optimization level 2\n\n   [3] is based on stripped sizes of cc1 for GCC and MIR core and interpreter or generator for MIR\n\n   [4] my estimation based only on files required for x86-64 GNU C compiler and MIR files for minimal program to create\n   and run MIR code\n\n## Current C2MIR Performance Data\n\n  * Intel i5-13600K with 64GB memory under FC37 with GCC-12.3.1\n\n    |                | c2m -O2 -eg (generator) | c2m -ei (interpreter) |     gcc -O2      |     gcc -O0     |\n    |----------------|-------------------------|-----------------------|------------------|-----------------|\n    | compilation [1]| **1.0** (336us)         | 1.0 (337us)           | **80** (27.1ms)  |  77 (26.1ms)    |\n    | execution [2]  | **1.0** (1.74s)         | 13.7 (23.8s)          | **0.92** (1.6s)  |  2.28 (3.97s)   |\n    | code size [3]  | **1.0** (961KB)         | 1.0 (961KB)           | **34** (32.2MB)  |  34 (32.2MB)    |\n    | LOC [4]        | **1.0** (54.8K)         | 1.0 (54.8K)           | **44** (2420K)   |  44  (2420K)    |\n\n   [1] is based on wall time of compilation of C sieve code (w/o any include file and with\n   using memory file system for GCC)\n\n   [2] is based on the best wall time of 10 runs with used MIR-generator optimization level 2\n\n   [3] is based on stripped sizes of cc1 for GCC and C2MIR, MIR core, interpreter, and generator for MIR\n\n   [4] is based on all source files excluding tests\n\n  * Here is generated code performance related to GCC -O2 for different C compilers on 15 small C benchmarks (from directory `c-benchmarks`) on the same machine where\n    * gcc version is 12.3.1\n    * clang version is 15.0.7\n    * [chibicc](https://github.com/rui314/chibicc) is Rui Ueyama's latest C11 implementation\n    * [cparser](https://github.com/libfirm/cparser) is a C99 implementation based on a pretty sophisticated backend, libFirm version 1.22\n    * [cproc](https://github.com/michaelforney/cproc) is Michael Forney's C11 implementation based on the **QBE** compiler backend\n    * [lacc](https://github.com/larmel/lacc) is a C89 implementation\n    * [pcc](http://pcc.ludd.ltu.se) (1.2.0.DEVEL) is a modern version of the Portable C compiler\n    * [tcc](https://bellard.org/tcc/) (0.9.27) is the tiny C11 compiler\n    * emcc (2.0.20) is emscripten compiler to Webassembly with wasmer (1.0.2) runtime\n    * wasi cranelift is a C to webassember clang compiler (11.0.0) with wasmer (1.0.2) based on cranelift backend\n    * wasi LLVM is a C to webassember clang compiler (11.0.0) with wasmer (1.0.2) based on LLVM backend\n    * wasi singlepass is a C to webassember clang compiler (11.0.0) with wasmer (1.0.2) based on singlepass backend\n    * wasi wasmtime is a C to webassember clang compiler (11.0.0) with wasmtime (0.26.0) runtime based on cranelift backend\n\n    |                                                  |  Average  |   Geomean |\n    |--------------------------------------------------|-----------|-----------|\n    | gcc -O2                                          |    1.00   |   1.00    |\n    | gcc -O0                                          |    0.63   |   0.57    |\n    | **c2m -eg**                                      |  **0.96** | **0.91**  |\n    | c2m -eb                                          |    0.92   |   0.85    |\n    | chibicc                                          |    0.38   |   0.30    |\n    | clang -O2                                        |    1.12   |   1.09    |\n    | cparser -O3                                      |    1.02   |   0.98    |\n    | cproc                                            |    0.68   |   0.65    |\n    | lacc -O3                                         |    0.47   |   0.39    |\n    | pcc -O                                           |    0.80   |   0.78    |\n    | tcc                                              |    0.54   |   0.50    |\n    | emcc -O2/wasmer                                  |    0.60   |   0.55    |\n    | wasi -O2/wasmer cranelift                        |    0.60   |   0.54    |\n    | wasi -O2/wasmer LLVM                             |    0.78   |   0.72    |\n    | wasi -O2/wasmer singlepass                       |    0.45   |   0.36    |\n    | wasi -O2/wasmtime                                |    0.92   |   0.87    |\n\n## MIR project competitors\n  * I only see three projects which could be considered or adapted as real universal light-weight JIT competitors\n  * [**QBE**](https://c9x.me/compile/):\n    * It is small (10K C lines)\n    * It uses SSA based IR (kind of simplified LLVM IR)\n    * It has the same optimizations as MIR-generator plus aliasing but QBE has no inlining\n    * It generates assembler code which makes QBE 30 slower in machine code generation than MIR-generator\n    * On my benchmarks it generates code whose geomean performance is only 65% of GCC with -O2\n      (performance of MIR generated code is 91% of GCC with -O2) while having the same compilation speed as MIR\n  * [**LIBJIT**](https://www.gnu.org/software/libjit/) started as a part of DotGNU Project:\n    * LIBJIT is bigger:\n      * 80K C lines (for LIBJIT w/o dynamic Pascal compiler) vs 20K C lines for MIR\n        (excluding C to MIR compiler)\n    * LIBJIT has fewer optimizations: only copy propagation and register allocation\n  * [**RyuJIT**](https://github.com/dotnet/runtime/blob/main/docs/design/coreclr/jit/ryujit-overview.md)\n    is a part of runtime for .NET Core:\n    * RyuJIT is even bigger: 360K SLOC\n    * RyuJIT optimizations is basically MIR-generator optimizations\n    * RyuJIT uses SSA\n  * Other candidates:\n    * [**LIBFirm**](https://github.com/libfirm/libfirm): less standalone-, big- (140K LOC), SSA,\n      ASM generation-, LGPL2\n    * [**CraneLift**](https://github.com/CraneStation/cranelift): less standalone-,\n      big- (70K LOC of Rust-), SSA, Apache License\n    * [**NanoJIT**](https://github.com/dibyendumajumdar/nanojit), standalone+, medium (40K C++ LOC), only simple RA-,\n      Mozilla Public License\n\n## Porting MIR\n  * Currently MIR works on x86_64, aarch64, ppc64le, s390x, riscv64 Linux and x86_64/aarch64 (Apple M1) MacOS\n  * [HOW-TO-PORT-MIR.md](https://github.com/vnmakarov/mir/blob/master/HOW-TO-PORT-MIR.md) outlines process of porting MIR\n    * On my estimation an experienced developer can port MIR (including `c2m`) to another target for 1-2 months\n"
        },
        {
          "name": "adt-tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "c-benchmarks",
          "type": "tree",
          "content": null
        },
        {
          "name": "c-tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "c2mir",
          "type": "tree",
          "content": null
        },
        {
          "name": "check-threads.sh",
          "type": "blob",
          "size": 0.3193359375,
          "content": "#!/bin/sh\n# Check pthread library presence:\n#\n\necho \"#include <pthread.h>\" >__tmp.c && \\\n  echo \"void *f (void *a) {} void main (void) {pthread_t t1;pthread_create (&t1, NULL, &f, NULL);}\" >>__tmp.c && \\\n  cc -w __tmp.c -lpthread -o __tmp.out\nok=$?\nrm -f __tmp.c __tmp.out\nif test $ok; then echo ok; else echo fail; fi\nexit 0\n\n"
        },
        {
          "name": "csmith-c2m-gcc.sh",
          "type": "blob",
          "size": 1.498046875,
          "content": "#!/bin/sh\n# Run csmith: c2m interpreter vs gcc:\n#\n\ntrap_ctrlc() {\n  echo finished\n  rm -f c2m.out gcc.out gcc-a.out tttt.c tttt.bmir\n  exit 0\n}\n\ntrap trap_ctrlc INT\n\nif type timeout >/dev/null 2>&1;then\n    TIMEOUT=\"timeout 10s\"\nelif type gtimeout >/dev/null 2>&1;then\n    TIMEOUT=\"gtimeout 10s\"\nelse\n    TIMEOUT=\nfi\ni=0;\nwhile test $i -lt 10000;do\n  ${CSMITH_HOME}/bin/csmith --no-packed-struct > tttt.c #  --no-bitfields\n  if ./c2m -I${CSMITH_HOME}/include -w tttt.c -o tttt.bmir;then echo -n .;else echo c2m failed; exit 1;fi\n  if gcc -I${CSMITH_HOME}/include -w tttt.c -o gcc-a.out;then echo -n +;else echo gcc failed; exit 1;fi\n  $TIMEOUT ./gcc-a.out >gcc.out; res=$?\n  $TIMEOUT sh -c './c2m tttt.bmir -ei >c2m.out'; c2mres=$?\n  if test $res -gt 127 || test $c2mres -gt 127; then\n# a signal (seg fault, bus error etc)\n      if test $res -ne $c2mres; then exit 1; fi\n  fi\n# exit code 124 is timeout\n  if test $res -eq 0 && test $c2mres -ne 0 && test $c2mres -ne 124; then echo only c2m JIT failed; exit 1; fi\n  if test $c2mres -eq 0 && test $res -ne 0 && test $res -ne 124; then echo only gcc code failed; exit 1; fi\n  if test $res -eq 0 && test $c2mres -ne 0; then echo c2m code only timeout; fi\n  if test $c2mres -eq 0 && test $res -ne 0; then echo gcc code only timeout; fi\n  if test $c2mres -eq 0 && test $res -eq 0; then\n    if cmp c2m.out gcc.out;then echo -n =;else diff -up c2m.out gcc.out; wc tttt.c; exit 1;fi\n  else\n    echo -n \"?\"\n  fi\n  i=`expr $i + 1`\n  if expr $i % 25 = 0 >/dev/null; then echo;fi\ndone\n\ntrap_ctrlc\n"
        },
        {
          "name": "csmith-c2m.sh",
          "type": "blob",
          "size": 1.28515625,
          "content": "#!/bin/bash\n# Run csmith: c2m interpreter vs JIT:\n#\n\ntrap_ctrlc() {\n  echo finished\n  rm -f c2m-interp.out c2m-jit.out ttt.c ttt.bmir\n  exit 0\n}\n\ntrap trap_ctrlc INT\n\nif type timeout >/dev/null 2>&1;then\n    TIMEOUT=\"timeout 15s\"\nelif type gtimeout >/dev/null 2>&1;then\n    TIMEOUT=\"gtimeout 15s\"\nelse\n    TIMEOUT=\nfi\ni=0;\nwhile test $i -lt 10000;do\n  ${CSMITH_HOME}/bin/csmith --no-packed-struct > ttt.c #  --no-bitfields\n  if ./c2m -I${CSMITH_HOME}/include -w ttt.c -o ttt.bmir;then echo -n .;else echo c2m failed; exit 1;fi\n  $TIMEOUT sh -c './c2m ttt.bmir -ei >c2m-interp.out'; c2mires=$?\n  $TIMEOUT sh -c './c2m ttt.bmir -el >c2m-jit.out'; c2mjres=$?\n  if test $c2mires -gt 127 || test $c2mjres -gt 127; then\n     if test $c2mires -ne $c2mjres; then exit 1; fi\n  fi\n# exit code 124 is timeout\n  if test $c2mires -eq 0 && test $c2mjres -ne 0 && test $c2mjres -ne 124; then echo only c2m JIT failed; exit 1; fi\n  if test $c2mjres -eq 0 && test $c2mires -ne 0 && test $c2mires -ne 124; then echo only c2m interpeter failed; exit 1; fi\n  if test $c2mjres -eq 0 && test $c2mires -eq 0; then\n    if cmp c2m-interp.out c2m-jit.out;then echo -n =;else diff -up c2m-interp.out c2m-jit.out; wc ttt.c; exit 1;fi\n  else\n    echo -n \"?\"\n  fi\n  i=`expr $i + 1`\n  if expr $i % 25 = 0 >/dev/null; then echo;fi\ndone\n\ntrap_ctrlc\n"
        },
        {
          "name": "llvm2mir",
          "type": "tree",
          "content": null
        },
        {
          "name": "mir-aarch64.c",
          "type": "blob",
          "size": 33.8134765625,
          "content": "/* This file is a part of MIR project.\n   Copyright (C) 2018-2024 Vladimir Makarov <vmakarov.gcc@gmail.com>.\n*/\n\n#include \"mir-aarch64.h\"\n\n/* x31 - sp; x30 - link reg; x29 - fp; x0-x7, v0-v7 - arg/result regs;\n   x19-x29, v8-v15 - callee-saved (only bottom 64-bits are saved for v8-v15);\n   x9-x15, v0-v7, v16-v31 - temp regs\n   x8 - indirect result location address\n   stack is 16-byte aligned\n\n   Apple M1 ABI specific:\n   o long double is double (64-bit)\n   o va_list is a pointer\n   o all varargs are passed only on stack\n   o reg x18 is reserved\n   o empty struct args are ignored\n*/\n\n/* Any small BLK type (less or equal to two quadwords) args are passed in\n   *fully* regs or on stack (w/o address), otherwise it is put\n   somewhere on stack and its address passed instead. First RBLK arg\n   is passed in r8. Other RBLK independently of size is always passed\n   by address as an usual argument.  */\n\nvoid *_MIR_get_bstart_builtin (MIR_context_t ctx) {\n  static const uint32_t bstart_code[] = {\n    0x910003e0, /* r0 = rsp */\n    0xd65f03c0, /* ret r30 */\n  };\n  return _MIR_publish_code (ctx, (uint8_t *) bstart_code, sizeof (bstart_code));\n}\n\nvoid *_MIR_get_bend_builtin (MIR_context_t ctx) {\n  static const uint32_t bend_code[] = {\n    0x9100001f, /* rsp = r0 */\n    0xd65f03c0, /* ret r30 */\n  };\n  return _MIR_publish_code (ctx, (uint8_t *) bend_code, sizeof (bend_code));\n}\n\n#define VA_LIST_IS_ARRAY_P 0\n#if defined(__APPLE__)\nstruct aarch64_va_list {\n  uint64_t *arg_area;\n};\n#else\nstruct aarch64_va_list {\n  /* address following the last (highest addressed) named incoming\n     argument on the stack, rounded upwards to a multiple of 8 bytes,\n     or if there are no named arguments on the stack, then the value\n     of the stack pointer when the function was entered. */\n  void *__stack;\n  /* the address of the byte immediately following the general\n     register argument save area, the end of the save area being\n     aligned to a 16 byte boundary. */\n  void *__gr_top;\n  /* the address of the byte immediately following the FP/SIMD\n     register argument save area, the end of the save area being\n     aligned to a 16 byte boundary. */\n  void *__vr_top;\n  int __gr_offs; /* set to 0 – ((8 – named_gr) * 8) */\n  int __vr_offs; /* set to 0 – ((8 – named_vr) * 16) */\n};\n#endif\n\nvoid *va_arg_builtin (void *p, uint64_t t) {\n  struct aarch64_va_list *va = p;\n  MIR_type_t type = t;\n#if defined(__APPLE__)\n  void *a = va->arg_area;\n\n  if (type == MIR_T_LD && __SIZEOF_LONG_DOUBLE__ == 16) {\n    va->arg_area += 2;\n  } else {\n    va->arg_area++;\n  }\n#if __BYTE_ORDER__ == __ORDER_BIG_ENDIAN__\n  if (type == MIR_T_F || type == MIR_T_I32) a = (char *) a + 4; /* 2nd word of doubleword */\n#endif\n#else\n  int fp_p = type == MIR_T_F || type == MIR_T_D || type == MIR_T_LD;\n  void *a;\n\n  if (fp_p && va->__vr_offs < 0) {\n    a = (char *) va->__vr_top + va->__vr_offs;\n    va->__vr_offs += 16;\n  } else if (!fp_p && va->__gr_offs < 0) {\n    a = (char *) va->__gr_top + va->__gr_offs;\n    va->__gr_offs += 8;\n  } else {\n    if (type == MIR_T_LD && __SIZEOF_LONG_DOUBLE__ == 16)\n      va->__stack = (void *) (((uint64_t) va->__stack + 15) % 16);\n    a = va->__stack;\n    va->__stack\n      = (char *) va->__stack + (type == MIR_T_LD && __SIZEOF_LONG_DOUBLE__ == 16 ? 16 : 8);\n  }\n#endif\n  return a;\n}\n\nvoid va_block_arg_builtin (void *res, void *p, size_t s, uint64_t ncase MIR_UNUSED) {\n  struct aarch64_va_list *va = p;\n#if defined(__APPLE__)\n  void *a = (void *) va->arg_area;\n  if (s <= 2 * 8) {\n    va->arg_area += (s + sizeof (uint64_t) - 1) / sizeof (uint64_t);\n  } else {\n    a = *(void **) a;\n    va->arg_area++;\n  }\n  if (res != NULL) memcpy (res, a, s);\n#else\n  void *a;\n  long size = (s + 7) / 8 * 8;\n\n  if (size <= 2 * 8 && va->__gr_offs + size > 0) { /* not enough regs to pass: */\n    a = va->__stack;\n    va->__stack = (char *) va->__stack + size;\n    va->__gr_offs += size;\n    if (res != NULL) memcpy (res, a, s);\n    return;\n  }\n  if (size > 2 * 8) size = 8;\n  if (va->__gr_offs < 0) {\n    a = (char *) va->__gr_top + va->__gr_offs;\n    va->__gr_offs += size;\n  } else {\n    a = va->__stack;\n    va->__stack = (char *) va->__stack + size;\n  }\n  if (s > 2 * 8) a = *(void **) a; /* address */\n  if (res != NULL) memcpy (res, a, s);\n#endif\n}\n\nvoid va_start_interp_builtin (MIR_context_t ctx MIR_UNUSED, void *p, void *a) {\n  struct aarch64_va_list *va = p;\n  va_list *vap = a;\n\n  assert (sizeof (struct aarch64_va_list) == sizeof (va_list));\n  *va = *(struct aarch64_va_list *) vap;\n}\n\nvoid va_end_interp_builtin (MIR_context_t ctx MIR_UNUSED, void *p MIR_UNUSED) {}\n\nstatic int setup_imm64_insns (uint32_t *to, int reg, uint64_t imm64) {\n  /* xd=imm64 */\n  static const uint32_t imm64_pat[] = {\n    0xd2800000, /*  0: mov xd, xxxx(0-15) */\n    0xf2a00000, /*  4: movk xd, xxxx(16-31) */\n    0xf2c00000, /*  8: movk xd, xxxx(32-47) */\n    0xf2e00000, /* 12: movk xd, xxxx(48-63) */\n  };\n  uint32_t mask = ~(0xffff << 5);\n\n  mir_assert (0 <= reg && reg <= 31);\n  to[0] = (imm64_pat[0] & mask) | ((uint32_t) (imm64 & 0xffff) << 5) | reg;\n  to[1] = (imm64_pat[1] & mask) | (((uint32_t) (imm64 >> 16) & 0xffff) << 5) | reg;\n  to[2] = (imm64_pat[2] & mask) | (((uint32_t) (imm64 >> 32) & 0xffff) << 5) | reg;\n  to[3] = (imm64_pat[3] & mask) | (((uint32_t) (imm64 >> 48) & 0xffff) << 5) | reg;\n  return sizeof (imm64_pat) / sizeof (uint32_t);\n}\n\nstatic uint8_t *push_insns (VARR (uint8_t) * insn_varr, const uint32_t *pat, size_t pat_len) {\n  uint8_t *p = (uint8_t *) pat;\n\n  for (size_t i = 0; i < pat_len; i++) VARR_PUSH (uint8_t, insn_varr, p[i]);\n  return VARR_ADDR (uint8_t, insn_varr) + VARR_LENGTH (uint8_t, insn_varr) - pat_len;\n}\n\nstatic size_t gen_mov_addr (VARR (uint8_t) * insn_varr, int reg, void *addr) {\n  uint32_t insns[4];\n  int insns_num = setup_imm64_insns (insns, reg, (uint64_t) addr);\n\n  mir_assert (insns_num == 4 && sizeof (insns) == insns_num * sizeof (uint32_t));\n  push_insns (insn_varr, insns, insns_num * sizeof (uint32_t));\n  return insns_num * sizeof (uint32_t);\n}\n\n#define BR_OFFSET_BITS 26\n#define MAX_BR_OFFSET (1 << (BR_OFFSET_BITS - 1)) /* 1 for sign */\n#define BR_OFFSET_MASK (~(-1u << BR_OFFSET_BITS))\n\nstatic void gen_call_addr (VARR (uint8_t) * insn_varr, void *base_addr, int temp_reg,\n                           void *call_addr) {\n  static const uint32_t call_pat1 = 0x94000000; /* bl x */\n  static const uint32_t call_pat2 = 0xd63f0000; /* blr x */\n  uint32_t insn;\n  int64_t offset = (uint32_t *) call_addr - (uint32_t *) base_addr;\n\n  mir_assert (0 <= temp_reg && temp_reg <= 31);\n  if (base_addr != NULL && -(int64_t) MAX_BR_OFFSET <= offset && offset < (int64_t) MAX_BR_OFFSET) {\n    insn = call_pat1 | ((uint32_t) offset & BR_OFFSET_MASK);\n  } else {\n    gen_mov_addr (insn_varr, temp_reg, call_addr);\n    insn = call_pat2 | (temp_reg << 5);\n  }\n  push_insns (insn_varr, &insn, sizeof (insn));\n}\n\nvoid *_MIR_get_thunk (MIR_context_t ctx) {\n  /* maximal size thunk -- see _MIR_redirect_thunk */\n  int pat[4] = {TARGET_NOP, TARGET_NOP, TARGET_NOP, TARGET_NOP};\n\n  return _MIR_publish_code (ctx, (uint8_t *) pat, sizeof (pat));\n}\n\nvoid _MIR_redirect_thunk (MIR_context_t ctx, void *thunk, void *to) {\n  static const uint32_t branch_pat1 = 0xd61f0120; /* br x9 */\n  static const uint32_t branch_pat2 = 0x14000000; /* b x */\n  int64_t offset = (uint32_t *) to - (uint32_t *) thunk;\n  uint32_t code[4];\n\n  mir_assert (((uint64_t) thunk & 0x3) == 0 && ((uint64_t) to & 0x3) == 0); /* alignment */\n  if (-(int64_t) MAX_BR_OFFSET <= offset && offset < (int64_t) MAX_BR_OFFSET) {\n    code[0] = branch_pat2 | ((uint32_t) offset & BR_OFFSET_MASK);\n    _MIR_change_code (ctx, thunk, (uint8_t *) &code[0], sizeof (code[0]));\n  } else {\n    code[0] = 0x58000049; /* ldr x9,8 (pc-relative) */\n    code[1] = branch_pat1;\n    *(void **) &code[2] = to;\n    _MIR_change_code (ctx, thunk, (uint8_t *) code, sizeof (code));\n  }\n}\n\nvoid *_MIR_get_thunk_addr (MIR_context_t ctx MIR_UNUSED, void *thunk) {\n  void *addr;\n  int short_p = (*(uint32_t *) thunk >> BR_OFFSET_BITS) == 0x5;\n  if (short_p) {\n    int32_t offset = *(uint32_t *) thunk & BR_OFFSET_MASK;\n    addr = (uint8_t *) thunk + ((offset << (32 - BR_OFFSET_BITS)) >> (30 - BR_OFFSET_BITS));\n  } else {\n    addr = *(void **) ((char *) thunk + 8);\n  }\n  return addr;\n}\n\nstatic void gen_blk_mov (VARR (uint8_t) * insn_varr, uint32_t offset, uint32_t addr_offset,\n                         uint32_t qwords, uint32_t addr_reg) {\n  static const uint32_t blk_mov_pat[] = {\n    /* 0:*/ 0xf940026c,  /* ldr x12, [x19,<addr_offset>]*/\n    /* 4:*/ 0x910003e0,  /* add <addr_reg>, sp, <offset>*/\n    /* 8:*/ 0xd280000b,  /* mov x11, 0*/\n    /* c:*/ 0xd280000e,  /* mov x14, <qwords>*/\n    /* 10:*/ 0xf86c696a, /* ldr x10, [x11,x12]*/\n    /* 14:*/ 0xd10005ce, /* sub x14, x14, #0x1*/\n    /* 18:*/ 0xf820696a, /* str x10, [x11,<addr_reg>x13]*/\n    /* 1c:*/ 0xf10001df, /* cmp x14, 0*/\n    /* 20:*/ 0x9100216b, /* add x11, x11, 8*/\n    /* 24:*/ 0x54ffff61, /* b.ne 10 */\n  };\n  if (qwords == 0) {\n    uint32_t pat = 0x910003e0 | addr_reg | (offset << 10); /* add <add_reg>, sp, <offset>*/\n    push_insns (insn_varr, &pat, sizeof (pat));\n  } else {\n    uint32_t *addr = (uint32_t *) push_insns (insn_varr, blk_mov_pat, sizeof (blk_mov_pat));\n    mir_assert (offset < (1 << 12) && addr_offset % 8 == 0 && (addr_offset >> 3) < (1 << 12));\n    mir_assert (addr_reg < 32 && qwords < (1 << 16));\n    addr[0] |= (addr_offset >> 3) << 10;\n    addr[1] |= addr_reg | (offset << 10);\n    addr[3] |= qwords << 5;\n    addr[6] |= addr_reg << 16;\n  }\n}\n\nstatic const uint32_t save_insns[] = {\n  /* save r0-r8,v0-v7 */\n  0xa9bf1fe6, /* stp R6, R7, [SP, #-16]! */\n  0xa9bf17e4, /* stp R4, R5, [SP, #-16]! */\n  0xa9bf0fe2, /* stp R2, R3, [SP, #-16]! */\n  0xa9bf07e0, /* stp R0, R1, [SP, #-16]! */\n  0xd10043ff, /* sub SP, SP, #16 */\n  0xf90007e8, /* str x8, [SP, #8] */\n  0xadbf1fe6, /* stp Q6, Q7, [SP, #-32]! */\n  0xadbf17e4, /* stp Q4, Q5, [SP, #-32]! */\n  0xadbf0fe2, /* stp Q2, Q3, [SP, #-32]! */\n  0xadbf07e0, /* stp Q0, Q1, [SP, #-32]! */\n};\nstatic const uint32_t restore_insns[] = {\n  /* restore r0-r8,v0-v7 */\n  0xacc107e0, /* ldp Q0, Q1, SP, #32 */\n  0xacc10fe2, /* ldp Q2, Q3, SP, #32 */\n  0xacc117e4, /* ldp Q4, Q5, SP, #32 */\n  0xacc11fe6, /* ldp Q6, Q7, SP, #32 */\n  0xf94007e8, /* ldr x8, [SP, #8] */\n  0x910043ff, /* add SP, SP, #16 */\n  0xa8c107e0, /* ldp R0, R1, SP, #16 */\n  0xa8c10fe2, /* ldp R2, R3, SP, #16 */\n  0xa8c117e4, /* ldp R4, R5, SP, #16 */\n  0xa8c11fe6, /* ldp R6, R7, SP, #16 */\n};\n\nstatic const uint32_t ld_pat = 0xf9400260;     /* ldr x, [x19], offset */\nstatic const uint32_t lds_pat = 0xbd400260;    /* ldr s, [x19], offset */\nstatic const uint32_t ldd_pat = 0xfd400260;    /* ldr d, [x19], offset */\nstatic const uint32_t ldld_pat = 0x3dc00260;   /* ldr q, [x19], offset */\nstatic const uint32_t gen_ld_pat = 0xf9400000; /* ldr x, [xn|sp], offset */\n\nstatic const uint32_t st_pat = 0xf9000000;   /* str x, [xn|sp], offset */\nstatic const uint32_t sts_pat = 0xbd000000;  /* str s, [xn|sp], offset */\nstatic const uint32_t std_pat = 0xfd000000;  /* str d, [xn|sp], offset */\nstatic const uint32_t stld_pat = 0x3d800000; /* str q, [xn|sp], offset */\n\n/* Generation: fun (fun_addr, res_arg_addresses):\n   push x19, x30; sp-=sp_offset; x9=fun_addr; x19=res/arg_addrs\n   x10=mem[x19,<offset>]; (arg_reg=mem[x10](or addr of blk copy on the stack)\n                          or x10=mem[x10] or x13=addr of blk copy on the stack;\n                             mem[sp,sp_offset]=x10|x13) ...\n   call fun_addr; sp+=offset\n   x10=mem[x19,<offset>]; res_reg=mem[x10]; ...\n   pop x19, x30; ret x30. */\nvoid *_MIR_get_ff_call (MIR_context_t ctx, size_t nres, MIR_type_t *res_types, size_t nargs,\n                        _MIR_arg_desc_t *arg_descs, size_t arg_vars_num MIR_UNUSED) {\n  static const uint32_t prolog[] = {\n    0xa9bf7bf3, /* stp x19,x30,[sp, -16]! */\n    0xd10003ff, /* sub sp,sp,<sp_offset> */\n    0xaa0003e9, /* mov x9,x0   # fun addr */\n    0xaa0103f3, /* mov x19, x1 # result/arg addresses */\n  };\n  static const uint32_t call_end[] = {\n    0xd63f0120, /* blr  x9\t   */\n    0x910003ff, /* add sp,sp,<sp_offset> */\n  };\n  static const uint32_t epilog[] = {\n    0xa8c17bf3, /* ldp x19,x30,[sp],16 */\n    0xd65f03c0, /* ret x30 */\n  };\n  MIR_type_t type;\n  uint32_t n_xregs = 0, n_vregs = 0, sp_offset = 0, blk_offset = 0, pat, offset_imm, scale;\n  uint32_t sp = 31, addr_reg, qwords;\n  const uint32_t temp_reg = 10; /* x10 */\n  VARR (uint8_t) * code;\n  void *res;\n\n  VARR_CREATE (uint8_t, code, ctx->alloc, 128);\n  mir_assert (__SIZEOF_LONG_DOUBLE__ == 8 || __SIZEOF_LONG_DOUBLE__ == 16);\n  for (size_t i = 0; i < nargs; i++) { /* calculate offset for blk params */\n#if defined(__APPLE__)                 /* all varargs are passed on stack */\n    if (i == arg_vars_num) n_xregs = n_vregs = 8;\n#endif\n    type = arg_descs[i].type;\n    if ((MIR_T_I8 <= type && type <= MIR_T_U64) || type == MIR_T_P || MIR_all_blk_type_p (type)) {\n      if (MIR_blk_type_p (type) && (qwords = (arg_descs[i].size + 7) / 8) <= 2) {\n        if (n_xregs + qwords > 8) blk_offset += qwords * 8;\n        n_xregs += qwords;\n      } else {\n        if (n_xregs++ >= 8) blk_offset += 8;\n      }\n    } else if (type == MIR_T_F || type == MIR_T_D || type == MIR_T_LD) {\n      if (n_vregs++ >= 8) blk_offset += type == MIR_T_LD && __SIZEOF_LONG_DOUBLE__ == 16 ? 16 : 8;\n    } else {\n      MIR_get_error_func (ctx) (MIR_call_op_error, \"wrong type of arg value\");\n    }\n  }\n  blk_offset = (blk_offset + 15) / 16 * 16;\n  push_insns (code, prolog, sizeof (prolog));\n  n_xregs = n_vregs = 0;\n  for (size_t i = 0; i < nargs; i++) { /* args */\n#if defined(__APPLE__)                 /* all varargs are passed on stack */\n    if (i == arg_vars_num) n_xregs = n_vregs = 8;\n#endif\n    type = arg_descs[i].type;\n    scale = type == MIR_T_F ? 2 : type == MIR_T_LD && __SIZEOF_LONG_DOUBLE__ == 16 ? 4 : 3;\n    offset_imm = (((i + nres) * sizeof (long double) << 10)) >> scale;\n    if (MIR_blk_type_p (type)) {\n      qwords = (arg_descs[i].size + 7) / 8;\n      if (qwords <= 2) {\n        addr_reg = 13;\n        pat = ld_pat | offset_imm | addr_reg;\n        push_insns (code, &pat, sizeof (pat));\n        if (n_xregs + qwords <= 8) {\n          for (uint32_t n = 0; n < qwords; n++) {\n            pat = gen_ld_pat | (((n * 8) >> scale) << 10) | (n_xregs + n) | (addr_reg << 5);\n            push_insns (code, &pat, sizeof (pat));\n          }\n        } else {\n          for (uint32_t n = 0; n < qwords; n++) {\n            pat = gen_ld_pat | (((n * 8) >> scale) << 10) | temp_reg | (addr_reg << 5);\n            push_insns (code, &pat, sizeof (pat));\n            pat = st_pat | ((sp_offset >> scale) << 10) | temp_reg | (sp << 5);\n            push_insns (code, &pat, sizeof (pat));\n            sp_offset += 8;\n          }\n        }\n        n_xregs += qwords;\n      } else {\n        addr_reg = n_xregs < 8 ? n_xregs : 13;\n        gen_blk_mov (code, blk_offset, (i + nres) * sizeof (long double), qwords, addr_reg);\n        blk_offset += qwords * 8;\n        if (n_xregs++ >= 8) {\n          pat = st_pat | ((sp_offset >> scale) << 10) | addr_reg | (sp << 5);\n          push_insns (code, &pat, sizeof (pat));\n          sp_offset += 8;\n        }\n      }\n    } else if ((MIR_T_I8 <= type && type <= MIR_T_U64) || type == MIR_T_P || type == MIR_T_RBLK) {\n      if (type == MIR_T_RBLK && i == 0) {\n        pat = ld_pat | offset_imm | 8; /* x8 - hidden result address */\n      } else if (n_xregs < 8) {\n        pat = ld_pat | offset_imm | n_xregs++;\n      } else {\n        pat = ld_pat | offset_imm | temp_reg;\n        push_insns (code, &pat, sizeof (pat));\n        pat = st_pat | ((sp_offset >> scale) << 10) | temp_reg | (sp << 5);\n        sp_offset += 8;\n      }\n      push_insns (code, &pat, sizeof (pat));\n    } else if (type == MIR_T_F || type == MIR_T_D || type == MIR_T_LD) {\n      pat = type == MIR_T_F                                  ? lds_pat\n            : type == MIR_T_D || __SIZEOF_LONG_DOUBLE__ == 8 ? ldd_pat\n                                                             : ldld_pat;\n      if (n_vregs < 8) {\n        pat |= offset_imm | n_vregs++;\n      } else {\n        if (type == MIR_T_LD && __SIZEOF_LONG_DOUBLE__ == 16) sp_offset = (sp_offset + 15) % 16;\n        pat |= offset_imm | temp_reg;\n        push_insns (code, &pat, sizeof (pat));\n        pat = type == MIR_T_F                                  ? sts_pat\n              : type == MIR_T_D || __SIZEOF_LONG_DOUBLE__ == 8 ? std_pat\n                                                               : stld_pat;\n        pat |= ((sp_offset >> scale) << 10) | temp_reg | (sp << 5);\n        sp_offset += type == MIR_T_LD && __SIZEOF_LONG_DOUBLE__ == 16 ? 16 : 8;\n      }\n      push_insns (code, &pat, sizeof (pat));\n    } else {\n      MIR_get_error_func (ctx) (MIR_call_op_error, \"wrong type of arg value\");\n    }\n  }\n  sp_offset = (sp_offset + 15) / 16 * 16;\n  blk_offset = (blk_offset + 15) / 16 * 16;\n  if (blk_offset != 0) sp_offset = blk_offset;\n  mir_assert (sp_offset < (1 << 12));\n  ((uint32_t *) VARR_ADDR (uint8_t, code))[1] |= sp_offset << 10; /* sub sp,sp,<offset> */\n  push_insns (code, call_end, sizeof (call_end));\n  ((uint32_t *) (VARR_ADDR (uint8_t, code) + VARR_LENGTH (uint8_t, code)))[-1] |= sp_offset << 10;\n  n_xregs = n_vregs = 0;\n  for (size_t i = 0; i < nres; i++) { /* results */\n    offset_imm = i * sizeof (long double) << 10;\n    if (((MIR_T_I8 <= res_types[i] && res_types[i] <= MIR_T_U64) || res_types[i] == MIR_T_P)\n        && n_xregs < 8) {\n      offset_imm >>= 3;\n      pat = st_pat | offset_imm | n_xregs++ | (19 << 5);\n      push_insns (code, &pat, sizeof (pat));\n    } else if ((res_types[i] == MIR_T_F || res_types[i] == MIR_T_D || res_types[i] == MIR_T_LD)\n               && n_vregs < 8) {\n      offset_imm >>= res_types[i] == MIR_T_F                                  ? 2\n                     : res_types[i] == MIR_T_D || __SIZEOF_LONG_DOUBLE__ == 8 ? 3\n                                                                              : 4;\n      pat = res_types[i] == MIR_T_F                                  ? sts_pat\n            : res_types[i] == MIR_T_D || __SIZEOF_LONG_DOUBLE__ == 8 ? std_pat\n                                                                     : stld_pat;\n      pat |= offset_imm | n_vregs++ | (19 << 5);\n      push_insns (code, &pat, sizeof (pat));\n    } else {\n      MIR_get_error_func (ctx) (MIR_ret_error,\n                                \"aarch64 can not handle this combination of return values\");\n    }\n  }\n  push_insns (code, epilog, sizeof (epilog));\n  res = _MIR_publish_code (ctx, VARR_ADDR (uint8_t, code), VARR_LENGTH (uint8_t, code));\n#if 0\n  if (getenv (\"MIR_code_dump\") != NULL)\n    _MIR_dump_code (\"ffi:\", VARR_ADDR (uint8_t, code), VARR_LENGTH (uint8_t, code));\n#endif\n  VARR_DESTROY (uint8_t, code);\n  return res;\n}\n\n/* Transform C call to call of void handler (MIR_context_t ctx, MIR_item_t func_item,\n                                             va_list va, MIR_val_t *results) */\nvoid *_MIR_get_interp_shim (MIR_context_t ctx, MIR_item_t func_item, void *handler) {\n  static const uint32_t save_x19_pat = 0xf81f0ff3; /* str x19, [sp,-16]! */\n  static const uint32_t prepare_pat[] = {\n#if !defined(__APPLE__)\n    0xd10083ff, /* sub sp, sp, 32 # allocate va_list */\n    0x910003ea, /* mov x10, sp # va_list addr         */\n    0xb9001949, /* str w9,[x10, 24] # va_list.gr_offs */\n    0x12800fe9, /* mov w9, #-128 # vr_offs */\n    0xb9001d49, /* str w9,[x10, 28]  #va_list.vr_offs */\n    0x9103c3e9, /* add x9, sp, #240 # gr_top */\n    0xf9000549, /* str x9,[x10, 8] # va_list.gr_top */\n    0x91004129, /* add x9, x9, #16 # stack */\n    0xf9000149, /* str x9,[x10] # valist.stack */\n    0x910283e9, /* add x9, sp, #160 # vr_top*/\n    0xf9000949, /* str x9,[x10, 16] # va_list.vr_top */\n    0xaa0a03e2, /* mov x2, x10 # va arg  */\n#endif\n    0xd2800009, /* mov x9, <(nres+1)*16> */\n    0xcb2963ff, /* sub sp, sp, x9 # reserve results and place for saved lr */\n#if defined(__APPLE__)\n    0x910023e3, /* add x3, sp, 8 # results arg */\n#else\n    0x910043e3,                                      /* add x3, sp, 16 # results arg */\n#endif\n    0xaa0303f3, /* mov x19, x3 # results */\n    0xf90003fe, /* str x30, [sp] # save lr */\n  };\n  static const uint32_t shim_end[]\n    = { 0xf94003fe, /* ldr x30, [sp] */\n        0xd2800009, /* mov x9, 240+(nres+1)*16 or APPLE: (nres * 8 + 8 + 15)/16*16 + sp_offset */\n#if defined(__APPLE__)\n        0xf94003f3, /* ldr x19, [sp, <(nres * 8 + 8 + 15)/16*16>] */\n#endif\n        0x8b2963ff, /* add sp, sp, x9 */\n#if !defined(__APPLE__)\n        0xf84107f3, /* ldr x19, sp, 16 */\n#endif\n        0xd65f03c0, /* ret x30 */\n      };\n  uint32_t pat, imm, n_xregs, n_vregs, offset, offset_imm;\n  MIR_func_t func = func_item->u.func;\n  uint32_t nres = func->nres;\n  MIR_type_t *results = func->res_types;\n  VARR (uint8_t) * code;\n  void *res;\n\n  VARR_CREATE (uint8_t, code, ctx->alloc, 128);\n#if defined(__APPLE__)\n  int stack_arg_sp_offset, sp_offset, scale;\n  uint32_t qwords, sp = 31;\n  uint32_t base_reg_mask = ~(uint32_t) (0x3f << 5);\n  static const uint32_t temp_reg = 10; /* x10 */\n  MIR_type_t type;\n  static const uint32_t add_x2_sp = 0x910003e2; /* add x2, sp, X*/\n  static const uint32_t arg_mov_start_pat[] = {\n    0x910003e9, /* mov x9,sp */\n    0xd10003ff, /* sub sp, sp, <frame size:10-21> # non-varg */\n  };\n\n  assert (__SIZEOF_LONG_DOUBLE__ == 8);\n  push_insns (code, arg_mov_start_pat, sizeof (arg_mov_start_pat));\n  sp_offset = 0;\n  for (size_t i = 0; i < func->nargs; i++) { /* args */\n    type = VARR_GET (MIR_var_t, func->vars, i).type;\n    if (MIR_blk_type_p (type)\n        && (qwords = (VARR_GET (MIR_var_t, func->vars, i).size + 7) / 8) <= 2) {\n      /* passing by one or two qwords */\n      sp_offset += 8 * qwords;\n      continue;\n    }\n    sp_offset += 8;\n  }\n  imm = sp_offset % 16;\n  sp_offset = imm == 0 ? 0 : 8;\n  stack_arg_sp_offset = 0;\n  n_xregs = n_vregs = 0;\n  for (size_t i = 0; i < func->nargs; i++) { /* args */\n    type = VARR_GET (MIR_var_t, func->vars, i).type;\n    scale = type == MIR_T_F ? 2 : 3;\n    if (MIR_blk_type_p (type)\n        && (qwords = (VARR_GET (MIR_var_t, func->vars, i).size + 7) / 8) <= 2) {\n      /* passing by one or two qwords */\n      if (n_xregs + qwords\n          <= 8) { /* passed by hard regs: str xreg, offset[sp]; str xreg, offset+8[sp] */\n        for (uint32_t n = 0; n < qwords; n++) {\n          pat = st_pat | ((sp_offset >> scale) << 10) | n_xregs++ | (sp << 5);\n          sp_offset += 8;\n          push_insns (code, &pat, sizeof (pat));\n        }\n      } else {\n        /* passed on stack: ldr t, stack_arg_offset[x9]; st t, offset[sp];\n                            ldr t, stack_arg_offset+8[x9]; st t, offset+8[sp]: */\n        for (int n = 0; n < qwords; n++) {\n          pat\n            = (ld_pat & base_reg_mask) | (stack_arg_sp_offset >> scale) << 10 | temp_reg | (9 << 5);\n          push_insns (code, &pat, sizeof (pat));\n          pat = st_pat | ((sp_offset >> scale) << 10) | temp_reg | (sp << 5);\n          push_insns (code, &pat, sizeof (pat));\n          stack_arg_sp_offset += 8;\n          sp_offset += 8;\n        }\n      }\n      continue;\n    }\n    if ((MIR_T_I8 <= type && type <= MIR_T_U64) || type == MIR_T_P || type == MIR_T_RBLK\n        || MIR_blk_type_p (type)) {       /* including address for long blocks */\n      if (type == MIR_T_RBLK && i == 0) { /* x8 - hidden result address */\n        pat = st_pat | ((sp_offset >> scale) << 10) | 8 | (sp << 5);\n      } else if (n_xregs < 8) { /* str xreg, sp_offset[sp]  */\n        pat = st_pat | ((sp_offset >> scale) << 10) | n_xregs++ | (sp << 5);\n      } else { /* ldr t, stack_arg_offset[x9]; st t, sp_offset[sp]: */\n        pat\n          = (ld_pat & base_reg_mask) | ((stack_arg_sp_offset >> scale) << 10) | temp_reg | (9 << 5);\n        push_insns (code, &pat, sizeof (pat));\n        pat = st_pat | ((sp_offset >> scale) << 10) | temp_reg | (sp << 5);\n        stack_arg_sp_offset += 8;\n      }\n      sp_offset += 8;\n      push_insns (code, &pat, sizeof (pat));\n    } else if (type == MIR_T_F || type == MIR_T_D || type == MIR_T_LD) {\n      if (n_vregs < 8) { /* st[s|d] vreg, sp_offset[sp]  */\n        pat = (type == MIR_T_F ? sts_pat : std_pat) | ((sp_offset >> scale) << 10) | n_vregs++\n              | (sp << 5);\n        sp_offset += 8;\n      } else {\n        pat = ((type == MIR_T_F ? lds_pat : ldd_pat) & base_reg_mask) | (9 << 5);\n        pat |= stack_arg_sp_offset | temp_reg;\n        push_insns (code, &pat, sizeof (pat));\n        pat = (type == MIR_T_F ? sts_pat : std_pat) | ((sp_offset >> scale) << 10) | temp_reg\n              | (sp << 5);\n        stack_arg_sp_offset += 8;\n        sp_offset += 8;\n      }\n      push_insns (code, &pat, sizeof (pat));\n    } else {\n      MIR_get_error_func (ctx) (MIR_call_op_error, \"wrong type of arg value\");\n    }\n  }\n  pat = add_x2_sp | (imm << 10);\n  push_insns (code, &pat, sizeof (pat));\n  sp_offset = (sp_offset + 15) / 16 * 16;\n  ((uint32_t *) VARR_ADDR (uint8_t, code))[1] |= sp_offset << 10;\n  push_insns (code, &save_x19_pat, sizeof (save_x19_pat));\n#else\n  static const uint32_t set_gr_offs = 0x128007e9;    /* mov w9, #-64 # gr_offs */\n  static const uint32_t set_x8_gr_offs = 0x128008e9; /* mov w9, #-72 # gr_offs */\n  int x8_res_p = func->nargs != 0 && VARR_GET (MIR_var_t, func->vars, 0).type == MIR_T_RBLK;\n  push_insns (code, &save_x19_pat, sizeof (save_x19_pat));\n  push_insns (code, save_insns, sizeof (save_insns));\n  if (x8_res_p)\n    push_insns (code, &set_x8_gr_offs, sizeof (set_x8_gr_offs));\n  else\n    push_insns (code, &set_gr_offs, sizeof (set_gr_offs));\n#endif\n  push_insns (code, prepare_pat, sizeof (prepare_pat));\n  imm = (nres * sizeof (MIR_val_t) + 8 + 15) / 16 * 16; /* results + saved x30 aligned to 16 */\n  mir_assert (imm < (1 << 16));\n  ((uint32_t *) (VARR_ADDR (uint8_t, code) + VARR_LENGTH (uint8_t, code)))[-5] |= imm << 5;\n  gen_mov_addr (code, 0, ctx);       /* mov x0, ctx */\n  gen_mov_addr (code, 1, func_item); /* mov x1, func_item */\n  gen_call_addr (code, NULL, 9, handler);\n  /* move results: */\n  n_xregs = n_vregs = offset = 0;\n  for (uint32_t i = 0; i < nres; i++) {\n    if ((results[i] == MIR_T_F || results[i] == MIR_T_D || results[i] == MIR_T_LD) && n_vregs < 8) {\n      pat = results[i] == MIR_T_F                                  ? lds_pat\n            : results[i] == MIR_T_D || __SIZEOF_LONG_DOUBLE__ == 8 ? ldd_pat\n                                                                   : ldld_pat;\n      pat |= n_vregs;\n      n_vregs++;\n    } else if (n_xregs < 8) {  // ??? ltp use\n      pat = ld_pat | n_xregs;\n      n_xregs++;\n    } else {\n      MIR_get_error_func (ctx) (MIR_ret_error,\n                                \"aarch64 can not handle this combination of return values\");\n    }\n    offset_imm = offset >> (results[i] == MIR_T_F                                    ? 2\n                            : results[i] == MIR_T_LD && __SIZEOF_LONG_DOUBLE__ == 16 ? 4\n                                                                                     : 3);\n    mir_assert (offset_imm < (1 << 12));\n    pat |= offset_imm << 10;\n    push_insns (code, &pat, sizeof (pat));\n    offset += sizeof (MIR_val_t);\n  }\n  push_insns (code, shim_end, sizeof (shim_end));\n#if defined(__APPLE__)\n  assert (imm % 8 == 0);\n  ((uint32_t *) (VARR_ADDR (uint8_t, code) + VARR_LENGTH (uint8_t, code)))[-3] |= imm << 7;\n  imm += sp_offset + 16;\n#else\n  imm = 240 + (nres + 1) * 16;\n#endif\n  mir_assert (imm < (1 << 16));\n  ((uint32_t *) (VARR_ADDR (uint8_t, code) + VARR_LENGTH (uint8_t, code)))[-4] |= imm << 5;\n  res = _MIR_publish_code (ctx, VARR_ADDR (uint8_t, code), VARR_LENGTH (uint8_t, code));\n#if 0\n  if (getenv (\"MIR_code_dump\") != NULL)\n    _MIR_dump_code (func->name, VARR_ADDR (uint8_t, code), VARR_LENGTH (uint8_t, code));\n#endif\n  VARR_DESTROY (uint8_t, code);\n  return res;\n}\n\n/* Save x0,x1; x0=ctx; x1=called_func; x10=hook_address;goto wrap_end. */\nvoid *_MIR_get_wrapper (MIR_context_t ctx, MIR_item_t called_func, void *hook_address) {\n  static const uint32_t save_insn = 0xa9bf07e0; /* stp R0, R1, [SP, #-16]! */\n  static const uint32_t jmp_pat = 0x14000000;   /* jmp */\n  uint32_t insn;\n  uint8_t *base_addr, *curr_addr, *res_code = NULL;\n  size_t len = 5 * 4; /* initial len */\n  VARR (uint8_t) * code;\n\n  VARR_CREATE (uint8_t, code, ctx->alloc, 128);\n  for (;;) { /* dealing with moving code to another page as the immediate call is pc relative */\n    curr_addr = base_addr = _MIR_get_new_code_addr (ctx, len);\n    if (curr_addr == NULL) break;\n    VARR_TRUNC (uint8_t, code, 0);\n    push_insns (code, &save_insn, sizeof (save_insn));\n    curr_addr += 4;\n    curr_addr += gen_mov_addr (code, 0, ctx);           /*mov x0,ctx  \t   */\n    curr_addr += gen_mov_addr (code, 1, called_func);   /*mov x1,called_func */\n    curr_addr += gen_mov_addr (code, 10, hook_address); /*mov x10,hook_address */\n    int64_t offset = (uint32_t *) wrapper_end_addr - (uint32_t *) curr_addr;\n    mir_assert (-(int64_t) MAX_BR_OFFSET <= offset && offset < (int64_t) MAX_BR_OFFSET);\n    insn = jmp_pat | ((uint32_t) offset & BR_OFFSET_MASK);\n    push_insns (code, &insn, sizeof (insn));\n    len = VARR_LENGTH (uint8_t, code);\n    res_code = _MIR_publish_code_by_addr (ctx, base_addr, VARR_ADDR (uint8_t, code), len);\n    if (res_code != NULL) break;\n  }\n  VARR_DESTROY (uint8_t, code);\n  return res_code;\n}\n\nvoid *_MIR_get_wrapper_end (MIR_context_t ctx) {\n  static const uint32_t wrap_end[] = {\n    0xa9bf7bfd, /* stp R29, R30, [SP, #-16]! */\n    0xa9bf1fe6, /* stp R6, R7, [SP, #-16]! */\n    0xa9bf17e4, /* stp R4, R5, [SP, #-16]! */\n    0xa9bf0fe2, /* stp R2, R3, [SP, #-16]! */\n    0xd10043ff, /* sub SP, SP, #16 */\n    0xf90007e8, /* str x8, [SP, #8] */\n    0xadbf1fe6, /* stp Q6, Q7, [SP, #-32]! */\n    0xadbf17e4, /* stp Q4, Q5, [SP, #-32]! */\n    0xadbf0fe2, /* stp Q2, Q3, [SP, #-32]! */\n    0xadbf07e0, /* stp Q0, Q1, [SP, #-32]! */\n    0xd63f0140, /* call *x10 */\n    0xaa0003e9, /* mov x9, x0 */\n    0xacc107e0, /* ldp Q0, Q1, SP, #32 */\n    0xacc10fe2, /* ldp Q2, Q3, SP, #32 */\n    0xacc117e4, /* ldp Q4, Q5, SP, #32 */\n    0xacc11fe6, /* ldp Q6, Q7, SP, #32 */\n    0xf94007e8, /* ldr x8, [SP, #8] */\n    0x910043ff, /* add SP, SP, #16 */\n    0xa8c10fe2, /* ldp R2, R3, SP, #16 */\n    0xa8c117e4, /* ldp R4, R5, SP, #16 */\n    0xa8c11fe6, /* ldp R6, R7, SP, #16 */\n    0xa8c17bfd, /* ldp R29, R30, SP, #16 */\n    0xa8c107e0, /* ldp R0, R1, SP, #16 */\n    0xd61f0120, /* br x9 */\n  };\n  uint8_t *res_code = NULL;\n  VARR (uint8_t) * code;\n  size_t len;\n\n  VARR_CREATE (uint8_t, code, ctx->alloc, 128);\n  push_insns (code, wrap_end, sizeof (wrap_end));\n  len = VARR_LENGTH (uint8_t, code);\n  res_code = _MIR_publish_code (ctx, VARR_ADDR (uint8_t, code), len);\n  VARR_DESTROY (uint8_t, code);\n  return res_code;\n}\n\n/* r9=<bb_version>; (b|br) handler  ??? mutex free */\nvoid *_MIR_get_bb_thunk (MIR_context_t ctx, void *bb_version, void *handler) {\n  /* maximal size thunk -- see _MIR_redirect_thunk */\n  uint32_t pat[5] = {TARGET_NOP, TARGET_NOP, TARGET_NOP, TARGET_NOP, TARGET_NOP};\n  void *res;\n  size_t offset;\n  VARR (uint8_t) * code;\n\n  VARR_CREATE (uint8_t, code, ctx->alloc, 64);\n  offset = gen_mov_addr (code, 9, bb_version); /* x9 = bb_version */\n  push_insns (code, pat, sizeof (pat));\n  res = _MIR_publish_code (ctx, VARR_ADDR (uint8_t, code), VARR_LENGTH (uint8_t, code));\n  _MIR_redirect_thunk (ctx, (uint8_t *) res + offset, handler);\n#if 0\n  if (getenv (\"MIR_code_dump\") != NULL)\n    _MIR_dump_code (\"bb thunk:\", res, VARR_LENGTH (uint8_t, code));\n#endif\n  VARR_DESTROY (uint8_t, code);\n  return res;\n}\n\n/* change to (b|br) to */\nvoid _MIR_replace_bb_thunk (MIR_context_t ctx, void *thunk, void *to) {\n  _MIR_redirect_thunk (ctx, thunk, to);\n}\n\nstatic const uint32_t save_fplr = 0xa9bf7bfd;    /* stp R29, R30, [SP, #-16]! */\nstatic const uint32_t restore_fplr = 0xa8c17bfd; /* ldp R29, R30, SP, #16 */\n\nstatic const uint32_t save_insns2[] = {\n  /* save r10-r18,v16-v31: should be used only right after save_insn */\n  0xf90043ea, /* str R10, [SP, #128]  */\n  0xa9bf4bf1, /* stp R17, R18, [SP, #-16]! */\n  0xa9bf43ef, /* stp R15, R16, [SP, #-16]! */\n  0xa9bf3bed, /* stp R13, R14, [SP, #-16]! */\n  0xa9bf33eb, /* stp R11, R12, [SP, #-16]! */\n  0xadbf7ffe, /* stp Q30, Q31, [SP, #-32]! */\n  0xadbf77fc, /* stp Q28, Q29, [SP, #-32]! */\n  0xadbf6ffa, /* stp Q26, Q27, [SP, #-32]! */\n  0xadbf67f8, /* stp Q24, Q25, [SP, #-32]! */\n  0xadbf5ff6, /* stp Q22, Q23, [SP, #-32]! */\n  0xadbf57f4, /* stp Q20, Q21, [SP, #-32]! */\n  0xadbf4ff2, /* stp Q18, Q19, [SP, #-32]! */\n  0xadbf47f0, /* stp Q16, Q17, [SP, #-32]! */\n};\nstatic const uint32_t restore_insns2[] = {\n  /* restore r10-r18,v16-v32: should be used only right before restore_insns */\n  0xacc147f0, /* ldp Q16, Q17, SP, #32 */\n  0xacc14ff2, /* ldp Q18, Q19, SP, #32 */\n  0xacc157f4, /* ldp Q20, Q21, SP, #32 */\n  0xacc15ff6, /* ldp Q22, Q23, SP, #32 */\n  0xacc167f8, /* ldp Q24, Q25, SP, #32 */\n  0xacc16ffa, /* ldp Q26, Q27, SP, #32 */\n  0xacc177fc, /* ldp Q28, Q29, SP, #32 */\n  0xacc17ffe, /* ldp Q30, Q31, SP, #32 */\n  0xa8c133eb, /* ldp R11, R12, SP, #16 */\n  0xa8c13bed, /* ldp R13, R14, SP, #16 */\n  0xa8c143ef, /* ldp R15, R16, SP, #16 */\n  0xa8c14bf1, /* ldp R17, R18, SP, #16 */\n  0xf94043ea, /* ldr R10, [SP, #128]  */\n};\n\n/* save all clobbered regs but 9; r9 = call hook_address (data, r9); restore regs; br r9\n   r9 is a generator temp reg which is not used across bb borders. */\nvoid *_MIR_get_bb_wrapper (MIR_context_t ctx, void *data, void *hook_address) {\n  static const uint32_t wrap_end = 0xd61f0120; /* br   x9\t\t\t   */\n  static const uint32_t call_pat[] = {\n    0xaa0903e1, /* mov x1,x9 */\n    0xd63f0140, /* blr  x10 */\n    0xaa0003e9, /* mov x9,x0 */\n  };\n  void *res;\n  VARR (uint8_t) * code;\n\n  VARR_CREATE (uint8_t, code, ctx->alloc, 128);\n  push_insns (code, &save_fplr, sizeof (save_fplr));\n  push_insns (code, save_insns, sizeof (save_insns));\n  push_insns (code, save_insns2, sizeof (save_insns2));\n  gen_mov_addr (code, 10, hook_address); /* x10 = hook_address */\n  gen_mov_addr (code, 0, data);          /* x0 = data */\n  push_insns (code, call_pat, sizeof (call_pat));\n  push_insns (code, restore_insns2, sizeof (restore_insns2));\n  push_insns (code, restore_insns, sizeof (restore_insns));\n  push_insns (code, &restore_fplr, sizeof (restore_fplr));\n  push_insns (code, &wrap_end, sizeof (wrap_end));\n  res = _MIR_publish_code (ctx, VARR_ADDR (uint8_t, code), VARR_LENGTH (uint8_t, code));\n#if 0\n  if (getenv (\"MIR_code_dump\") != NULL)\n    _MIR_dump_code (\"bb wrapper:\", VARR_ADDR (uint8_t, code), VARR_LENGTH (uint8_t, code));\n#endif\n  VARR_DESTROY (uint8_t, code);\n  return res;\n}\n"
        },
        {
          "name": "mir-aarch64.h",
          "type": "blob",
          "size": 2.6533203125,
          "content": "/* This file is a part of MIR project.\n   Copyright (C) 2018-2024 Vladimir Makarov <vmakarov.gcc@gmail.com>.\n   A common include file for mir-aarch64.c and mir-gen-aarch64.c\n*/\n\n#include \"mir.h\"\n\n#define TARGET_NOP 0xd503201f\n\n#define HREG_EL(h) h##_HARD_REG\n#define REP_SEP ,\nenum {\n  REP8 (HREG_EL, R0, R1, R2, R3, R4, R5, R6, R7),\n  REP8 (HREG_EL, R8, R9, R10, R11, R12, R13, R14, R15),\n  REP8 (HREG_EL, R16, R17, R18, R19, R20, R21, R22, R23),\n  REP8 (HREG_EL, R24, R25, R26, R27, R28, R29, R30, SP),\n  ZR_HARD_REG = SP_HARD_REG,\n  REP8 (HREG_EL, V0, V1, V2, V3, V4, V5, V6, V7),\n  REP8 (HREG_EL, V8, V9, V10, V11, V12, V13, V14, V15),\n  REP8 (HREG_EL, V16, V17, V18, V19, V20, V21, V22, V23),\n  REP8 (HREG_EL, V24, V25, V26, V27, V28, V29, V30, V31),\n};\n#undef REP_SEP\n\nstatic const char *const target_hard_reg_names[] = {\n  \"r0\",  \"r1\",  \"r2\",  \"r3\",  \"r4\",  \"r5\",  \"r6\",  \"r7\",  \"r8\",  \"r9\",  \"r10\", \"r11\", \"r12\",\n  \"r13\", \"r14\", \"r15\", \"r16\", \"r17\", \"r18\", \"r19\", \"r20\", \"r21\", \"r22\", \"r23\", \"r24\", \"r25\",\n  \"r26\", \"r27\", \"r28\", \"r29\", \"r30\", \"sp\",  \"v0\",  \"v1\",  \"v2\",  \"v3\",  \"v4\",  \"v5\",  \"v6\",\n  \"v7\",  \"v8\",  \"v9\",  \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\", \"v18\", \"v19\",\n  \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\", \"v26\", \"v27\", \"v28\", \"v29\", \"v30\", \"v31\",\n};\n\n#define MAX_HARD_REG V31_HARD_REG\n\n/* Hard regs not used in machinized code, preferably call used ones. */\nstatic const MIR_reg_t TEMP_INT_HARD_REG1 = R9_HARD_REG, TEMP_INT_HARD_REG2 = R10_HARD_REG;\nstatic const MIR_reg_t TEMP_FLOAT_HARD_REG1 = V16_HARD_REG, TEMP_FLOAT_HARD_REG2 = V17_HARD_REG;\nstatic const MIR_reg_t TEMP_DOUBLE_HARD_REG1 = V16_HARD_REG, TEMP_DOUBLE_HARD_REG2 = V17_HARD_REG;\nstatic const MIR_reg_t TEMP_LDOUBLE_HARD_REG1 = V16_HARD_REG;\nstatic const MIR_reg_t TEMP_LDOUBLE_HARD_REG2 = V17_HARD_REG;\n\nstatic inline int target_hard_reg_type_ok_p (MIR_reg_t hard_reg, MIR_type_t type) {\n  assert (hard_reg <= MAX_HARD_REG);\n  return MIR_fp_type_p (type) ? hard_reg >= V0_HARD_REG : hard_reg < V0_HARD_REG;\n}\n\nstatic inline int target_fixed_hard_reg_p (MIR_reg_t hard_reg) {\n  assert (hard_reg <= MAX_HARD_REG);\n#if defined(__APPLE__)\n  if (hard_reg == R18_HARD_REG) return TRUE;\n#endif\n  return (hard_reg == R29_HARD_REG /*FP*/ || hard_reg == SP_HARD_REG\n          || hard_reg == TEMP_INT_HARD_REG1 || hard_reg == TEMP_INT_HARD_REG2\n          || hard_reg == TEMP_FLOAT_HARD_REG1 || hard_reg == TEMP_FLOAT_HARD_REG2\n          || hard_reg == TEMP_DOUBLE_HARD_REG1 || hard_reg == TEMP_DOUBLE_HARD_REG2\n          || hard_reg == TEMP_LDOUBLE_HARD_REG1 || hard_reg == TEMP_LDOUBLE_HARD_REG2);\n}\n\nstatic int target_locs_num (MIR_reg_t loc, MIR_type_t type) {\n  return loc > MAX_HARD_REG && type == MIR_T_LD ? 2 : 1;\n}\n"
        },
        {
          "name": "mir-alloc-default.c",
          "type": "blob",
          "size": 0.8701171875,
          "content": "/* This file is a part of MIR project.\n   Copyright (C) 2018-2024 Vladimir Makarov <vmakarov.gcc@gmail.com>.\n*/\n\n#include <stdlib.h>\n#include \"mir-alloc.h\"\n\n#ifdef __GNUC__\n#define ALLOC_UNUSED __attribute__ ((unused))\n#else\n#define ALLOC_UNUSED\n#endif\n\nstatic void *default_malloc (size_t size, void *user_data ALLOC_UNUSED) {\n  return malloc (size);\n}\n\nstatic void *default_calloc (size_t num, size_t size, void *user_data ALLOC_UNUSED) {\n  return calloc (num, size);\n}\n\nstatic void *default_realloc (void *ptr, size_t old_size ALLOC_UNUSED, size_t new_size, void *user_data ALLOC_UNUSED) {\n  return realloc (ptr, new_size);\n}\n\nstatic void default_free (void *ptr, void *user_data ALLOC_UNUSED) {\n  free (ptr);\n}\n\nstatic struct MIR_alloc default_alloc = {\n  .malloc = default_malloc,\n  .calloc = default_calloc,\n  .realloc = default_realloc,\n  .free = default_free,\n  .user_data = NULL\n};\n"
        },
        {
          "name": "mir-alloc.h",
          "type": "blob",
          "size": 1.1298828125,
          "content": "/* This file is a part of MIR project.\n   Copyright (C) 2018-2024 Vladimir Makarov <vmakarov.gcc@gmail.com>.\n*/\n\n#ifndef MIR_ALLOC_H\n#define MIR_ALLOC_H\n\n#include <assert.h>\n#include <stddef.h>\n\n#ifdef __cplusplus\nextern \"C\" {\n#endif\n\ntypedef struct MIR_alloc {\n  void *(*malloc) (size_t, void *);\n  void *(*calloc) (size_t, size_t, void *);\n  void *(*realloc) (void *, size_t, size_t, void *);\n  void (*free) (void *, void *);\n  void *user_data;\n} *MIR_alloc_t;\n\nstatic inline void *MIR_malloc (MIR_alloc_t alloc, size_t size) {\n  assert (alloc != NULL);\n  return alloc->malloc (size, alloc->user_data);\n}\n\nstatic inline void *MIR_calloc (MIR_alloc_t alloc, size_t num, size_t size) {\n  assert (alloc != NULL);\n  return alloc->calloc (num, size, alloc->user_data);\n}\n\nstatic inline void *MIR_realloc (MIR_alloc_t alloc, void *ptr, size_t old_size, size_t new_size) {\n  assert (alloc != NULL);\n  return alloc->realloc (ptr, old_size, new_size, alloc->user_data);\n}\n\nstatic inline void MIR_free (MIR_alloc_t alloc, void *ptr) {\n  assert (alloc != NULL);\n  alloc->free (ptr, alloc->user_data);\n}\n\n#ifdef __cplusplus\n}\n#endif\n\n#endif /* #ifndef MIR_ALLOC_H */\n"
        },
        {
          "name": "mir-bin-driver.c",
          "type": "blob",
          "size": 4.8818359375,
          "content": "#include <string.h>\n#include \"mir.h\"\n#include \"mir-gen.h\"\n\n#ifdef CTAB_INCLUDE_STRING\n#include CTAB_INCLUDE_STRING\n#else\n#include \"mir-ctab\"\n#endif\n\nstatic size_t curr_input_byte_num;\n\nstatic int read_byte (MIR_context_t ctx) {\n  if (curr_input_byte_num >= sizeof (mir_code)) return EOF;\n  return mir_code[curr_input_byte_num++];\n}\n\n#include <dlfcn.h>\n#if defined(__unix__) || defined(__APPLE__)\n#include <sys/stat.h>\n#endif\n\nstatic struct lib {\n  char *name;\n  void *handler;\n} libs[] = {\n#if !defined(__APPLE__)\n  {\"/lib64/libc.so.6\", NULL}, {\"/lib64/libm.so.6\", NULL}, {\"/lib64/libpthread.so.0\", NULL}\n#else\n  {\"/usr/lib/libc.dylib\", NULL}, {\"/usr/lib/libm.dylib\", NULL}\n#endif\n};\n\nstatic void close_libs (void) {\n  for (int i = 0; i < sizeof (libs) / sizeof (struct lib); i++)\n    if (libs[i].handler != NULL) dlclose (libs[i].handler);\n}\n\nstatic void open_libs (void) {\n  for (int i = 0; i < sizeof (libs) / sizeof (struct lib); i++)\n    if ((libs[i].handler = dlopen (libs[i].name, RTLD_LAZY)) == NULL) {\n      fprintf (stderr, \"can not open lib %s\\n\", libs[i].name);\n      close_libs ();\n      exit (1);\n    }\n}\n\nstatic void *import_resolver (const char *name) {\n  void *sym = NULL;\n\n  if (strcmp (name, \"dlopen\") == 0) return dlopen;\n  if (strcmp (name, \"dlsym\") == 0) return dlsym;\n  if (strcmp (name, \"dlclose\") == 0) return dlclose;\n#if defined(__unix__) || defined(__APPLE__)\n  if (strcmp (name, \"stat\") == 0) return stat;\n#if defined(__APPLE__) && defined(__aarch64__)\n  if (strcmp (name, \"_MIR_set_code\") == 0) return _MIR_set_code;\n#endif\n#endif\n  for (int i = 0; i < sizeof (libs) / sizeof (struct lib); i++) {\n    if ((sym = dlsym (libs[i].handler, name)) != NULL) break;\n  }\n  if (sym == NULL) {\n    fprintf (stderr, \"can not load symbol %s\\n\", name);\n    close_libs ();\n    exit (1);\n  }\n  return sym;\n}\n\n#ifndef MIR_BIN_DEBUG\n#define MIR_BIN_DEBUG 0\n#endif\n\n#if MIR_BIN_DEBUG\n#include \"real-time.h\"\n#endif\n\n#ifndef MIR_USE_INTERP\n#define MIR_USE_INTERP 0\n#endif\n\n#ifndef MIR_USE_GEN\n#define MIR_USE_GEN 0\n#endif\n\n#ifndef MIR_USE_LAZY_GEN\n#define MIR_USE_LAZY_GEN 1\n#endif\n\nint main (int argc, char *argv[], char *env[]) {\n  int exit_code;\n  MIR_val_t val;\n  MIR_module_t module;\n  MIR_item_t func, main_func = NULL;\n  uint64_t (*fun_addr) (int, char *argv[], char *env[]);\n  MIR_context_t ctx = MIR_init ();\n  unsigned funcs_num = 0;\n#if MIR_BIN_DEBUG\n  double start_time = real_usec_time ();\n#endif\n\n  assert (MIR_USE_INTERP || MIR_USE_GEN || MIR_USE_LAZY_GEN);\n  curr_input_byte_num = 0;\n  MIR_read_with_func (ctx, read_byte);\n#if MIR_BIN_DEBUG\n  fprintf (stderr, \"Finish of MIR reading from memory -- curr_time %.0f usec\\n\",\n           real_usec_time () - start_time);\n#endif\n  for (module = DLIST_HEAD (MIR_module_t, *MIR_get_module_list (ctx)); module != NULL;\n       module = DLIST_NEXT (MIR_module_t, module)) {\n    for (func = DLIST_HEAD (MIR_item_t, module->items); func != NULL;\n         func = DLIST_NEXT (MIR_item_t, func)) {\n      if (func->item_type != MIR_func_item) continue;\n      funcs_num++;\n      if (strcmp (func->u.func->name, \"main\") == 0) main_func = func;\n    }\n    MIR_load_module (ctx, module);\n  }\n  if (main_func == NULL) {\n    fprintf (stderr, \"cannot execute program w/o main function\\n\");\n    return 1;\n  }\n  open_libs ();\n  if (MIR_USE_INTERP) {\n    MIR_link (ctx, MIR_set_interp_interface, import_resolver);\n#if MIR_BIN_DEBUG\n    fprintf (stderr, \"Finish of loading/linking (%d funcs) -- curr_time %.0f usec\\n\", funcs_num,\n             real_usec_time () - start_time);\n    start_time = real_usec_time ();\n#endif\n    MIR_interp (ctx, main_func, &val, 3, (MIR_val_t){.i = argc}, (MIR_val_t){.a = (void *) argv},\n                (MIR_val_t){.a = (void *) env});\n#if MIR_BIN_DEBUG\n    fprintf (stderr, \"Finish of execution -- overall execution time %.0f usec\\n\",\n             real_usec_time () - start_time);\n#endif\n    exit_code = val.i;\n  } else {\n    MIR_gen_init (ctx);\n#if MIR_BIN_DEBUG\n    MIR_gen_set_debug_file (ctx, stderr);\n    MIR_gen_set_debug_level (ctx, MIR_BIN_DEBUG);\n#endif\n    MIR_link (ctx, MIR_USE_GEN ? MIR_set_gen_interface : MIR_set_lazy_gen_interface,\n              import_resolver);\n#if MIR_BIN_DEBUG\n    fprintf (stderr,\n             (!MIR_USE_GEN\n                ? \"Finish of MIR loading/linking (%d funcs) -- curr_time %.0f usec\\n\"\n                : \"Finish of MIR loading/linking/generation (%d funcs) -- curr_time %.0f usec\\n\"),\n             funcs_num, real_usec_time () - start_time);\n#endif\n    fun_addr = MIR_gen (ctx, main_func);\n#if MIR_BIN_DEBUG\n    start_time = real_usec_time ();\n#endif\n    exit_code = fun_addr (argc, argv, env);\n#if MIR_BIN_DEBUG\n    fprintf (stderr,\n             (MIR_USE_GEN\n                ? \"Finish of execution -- overall execution time %.0f usec\\n\"\n                : \"Finish of generation and execution -- overall execution time %.0f usec\\n\"),\n             real_usec_time () - start_time);\n#endif\n    MIR_gen_finish (ctx);\n  }\n  MIR_finish (ctx);\n  close_libs ();\n  return exit_code;\n}\n"
        },
        {
          "name": "mir-bin-run.c",
          "type": "blob",
          "size": 11.4013671875,
          "content": "#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <alloca.h>\n#include <dlfcn.h>\n#include <sys/stat.h>\n#include \"mir-alloc-default.c\"\n#include \"mir-gen.h\"  // mir.h gets included as well\n\n#define MIR_TYPE_INTERP 1\n#define MIR_TYPE_INTERP_NAME \"interp\"\n#define MIR_TYPE_GEN 2\n#define MIR_TYPE_GEN_NAME \"gen\"\n#define MIR_TYPE_LAZY 3\n#define MIR_TYPE_LAZY_NAME \"lazy\"\n\n#define MIR_TYPE_DEFAULT MIR_TYPE_LAZY\n\n#define MIR_ENV_VAR_LIB_DIRS \"MIR_LIB_DIRS\"\n#define MIR_ENV_VAR_EXTRA_LIBS \"MIR_LIBS\"\n#define MIR_ENV_VAR_TYPE \"MIR_TYPE\"\n\nstruct lib {\n  char *name;\n  void *handler;\n};\ntypedef struct lib lib_t;\n\n/* stdlibs according to c2mir */\n#if defined(__unix__)\n#if UINTPTR_MAX == 0xffffffff\nstatic lib_t std_libs[]\n  = {{\"/lib/libc.so.6\", NULL},   {\"/lib32/libc.so.6\", NULL},     {\"/lib/libm.so.6\", NULL},\n     {\"/lib32/libm.so.6\", NULL}, {\"/lib/libpthread.so.0\", NULL}, {\"/lib32/libpthread.so.0\", NULL}};\nstatic const char *std_lib_dirs[] = {\"/lib\", \"/lib32\"};\n#elif UINTPTR_MAX == 0xffffffffffffffff\n#if defined(__x86_64__)\nstatic lib_t std_libs[] = {{\"/lib64/libc.so.6\", NULL},\n                           {\"/lib/x86_64-linux-gnu/libc.so.6\", NULL},\n                           {\"/lib64/libm.so.6\", NULL},\n                           {\"/lib/x86_64-linux-gnu/libm.so.6\", NULL},\n                           {\"/usr/lib64/libpthread.so.0\", NULL},\n                           {\"/lib/x86_64-linux-gnu/libpthread.so.0\", NULL},\n                           {\"/usr/lib/libc.so\", NULL}};\nstatic const char *std_lib_dirs[] = {\"/lib64\", \"/lib/x86_64-linux-gnu\"};\n#elif (__aarch64__)\nstatic lib_t std_libs[]\n  = {{\"/lib64/libc.so.6\", NULL},       {\"/lib/aarch64-linux-gnu/libc.so.6\", NULL},\n     {\"/lib64/libm.so.6\", NULL},       {\"/lib/aarch64-linux-gnu/libm.so.6\", NULL},\n     {\"/lib64/libpthread.so.0\", NULL}, {\"/lib/aarch64-linux-gnu/libpthread.so.0\", NULL}};\nstatic const char *std_lib_dirs[] = {\"/lib64\", \"/lib/aarch64-linux-gnu\"};\n#elif (__PPC64__)\nstatic lib_t std_libs[] = {\n  {\"/lib64/libc.so.6\", NULL},\n  {\"/lib64/libm.so.6\", NULL},\n  {\"/lib64/libpthread.so.0\", NULL},\n#if __BYTE_ORDER__ == __ORDER_LITTLE_ENDIAN__\n  {\"/lib/powerpc64le-linux-gnu/libc.so.6\", NULL},\n  {\"/lib/powerpc64le-linux-gnu/libm.so.6\", NULL},\n  {\"/lib/powerpc64le-linux-gnu/libpthread.so.0\", NULL},\n#else\n  {\"/lib/powerpc64-linux-gnu/libc.so.6\", NULL},\n  {\"/lib/powerpc64-linux-gnu/libm.so.6\", NULL},\n  {\"/lib/powerpc64-linux-gnu/libpthread.so.0\", NULL},\n#endif\n};\nstatic const char *std_lib_dirs[] = {\n  \"/lib64\",\n#if __BYTE_ORDER__ == __ORDER_LITTLE_ENDIAN__\n  \"/lib/powerpc64le-linux-gnu\",\n#else\n  \"/lib/powerpc64-linux-gnu\",\n#endif\n};\n#elif (__s390x__)\nstatic lib_t std_libs[]\n  = {{\"/lib64/libc.so.6\", NULL},       {\"/lib/s390x-linux-gnu/libc.so.6\", NULL},\n     {\"/lib64/libm.so.6\", NULL},       {\"/lib/s390x-linux-gnu/libm.so.6\", NULL},\n     {\"/lib64/libpthread.so.0\", NULL}, {\"/lib/s390x-linux-gnu/libpthread.so.0\", NULL}};\nstatic const char *std_lib_dirs[] = {\"/lib64\", \"/lib/s390x-linux-gnu\"};\n#elif (__riscv)\nstatic lib_t std_libs[]\n  = {{\"/lib64/libc.so.6\", NULL},       {\"/lib/riscv64-linux-gnu/libc.so.6\", NULL},\n     {\"/lib64/libm.so.6\", NULL},       {\"/lib/riscv64-linux-gnu/libm.so.6\", NULL},\n     {\"/lib64/libpthread.so.0\", NULL}, {\"/lib/riscv64-linux-gnu/libpthread.so.0\", NULL}};\nstatic const char *std_lib_dirs[] = {\"/lib64\", \"/lib/riscv64-linux-gnu\"};\n#else\n#error cannot recognize 32- or 64-bit target\n#endif\n#endif\nstatic const char *lib_suffix = \".so\";\n#endif\n\n#ifdef _WIN32\nstatic const int slash = '\\\\';\n#else\nstatic const int slash = '/';\n#endif\n\n#if defined(__APPLE__)\nstatic lib_t std_libs[] = {{\"/usr/lib/libc.dylib\", NULL}, {\"/usr/lib/libm.dylib\", NULL}};\nstatic const char *std_lib_dirs[] = {\"/usr/lib\"};\nstatic const char *lib_suffix = \".dylib\";\n#endif\n\n#ifdef _WIN32\nstatic lib_t std_libs[] = {{\"C:\\\\Windows\\\\System32\\\\msvcrt.dll\", NULL},\n                           {\"C:\\\\Windows\\\\System32\\\\kernel32.dll\", NULL},\n                           {\"C:\\\\Windows\\\\System32\\\\ucrtbase.dll\", NULL}};\nstatic const char *std_lib_dirs[] = {\"C:\\\\Windows\\\\System32\"};\nstatic const char *lib_suffix = \".dll\";\n#define dlopen(n, f) LoadLibrary (n)\n#define dlclose(h) FreeLibrary (h)\n#define dlsym(h, s) GetProcAddress (h, s)\n#endif\n\nstatic void close_std_libs (void) {\n  for (int i = 0; i < sizeof (std_libs) / sizeof (lib_t); i++)\n    if (std_libs[i].handler != NULL) dlclose (std_libs[i].handler);\n}\n\nstatic void open_std_libs (void) {\n  for (int i = 0; i < sizeof (std_libs) / sizeof (struct lib); i++)\n    std_libs[i].handler = dlopen (std_libs[i].name, RTLD_LAZY);\n}\n\nDEF_VARR (lib_t);\nstatic VARR (lib_t) * extra_libs;\n\ntypedef const char *char_ptr_t;\nDEF_VARR (char_ptr_t);\nstatic VARR (char_ptr_t) * lib_dirs;\n\nDEF_VARR (char);\nstatic VARR (char) * temp_string;\n\nstatic void *open_lib (const char *dir, const char *name) {\n  const char *last_slash = strrchr (dir, slash);\n  void *res;\n  FILE *f;\n\n  VARR_TRUNC (char, temp_string, 0);\n  VARR_PUSH_ARR (char, temp_string, dir, strlen (dir));\n  if (last_slash == NULL || last_slash[1] != '\\0') VARR_PUSH (char, temp_string, slash);\n#ifndef _WIN32\n  VARR_PUSH_ARR (char, temp_string, \"lib\", 3);\n#endif\n  VARR_PUSH_ARR (char, temp_string, name, strlen (name));\n  VARR_PUSH_ARR (char, temp_string, lib_suffix, strlen (lib_suffix));\n  VARR_PUSH (char, temp_string, 0);\n  if ((res = dlopen (VARR_ADDR (char, temp_string), RTLD_LAZY)) == NULL) {\n#ifndef _WIN32\n    if ((f = fopen (VARR_ADDR (char, temp_string), \"rb\")) != NULL) {\n      fclose (f);\n      fprintf (stderr, \"loading %s:%s\\n\", VARR_ADDR (char, temp_string), dlerror ());\n    }\n#endif\n  }\n  return res;\n}\n\nstatic void process_extra_lib (char *lib_name) {\n  lib_t lib;\n\n  lib.name = lib_name;\n  for (size_t i = 0; i < VARR_LENGTH (char_ptr_t, lib_dirs); i++)\n    if ((lib.handler = open_lib (VARR_GET (char_ptr_t, lib_dirs, i), lib_name)) != NULL) break;\n  if (lib.handler == NULL) {\n    fprintf (stderr, \"cannot find library lib%s -- good bye\\n\", lib_name);\n    exit (1);\n  }\n  VARR_PUSH (lib_t, extra_libs, lib);\n}\n\nstatic void close_extra_libs (void) {\n  void *handler;\n\n  for (size_t i = 0; i < VARR_LENGTH (lib_t, extra_libs); i++)\n    if ((handler = VARR_GET (lib_t, extra_libs, i).handler) != NULL) dlclose (handler);\n}\n\n#if defined(__APPLE__) && defined(__aarch64__)\nfloat __nan (void) {\n  union {\n    uint32_t i;\n    float f;\n  } u = {0x7fc00000};\n  return u.f;\n}\n#endif\n\nstatic void *import_resolver (const char *name) {\n  void *handler, *sym = NULL;\n\n  for (int i = 0; i < sizeof (std_libs) / sizeof (struct lib); i++)\n    if ((handler = std_libs[i].handler) != NULL && (sym = dlsym (handler, name)) != NULL) break;\n  if (sym == NULL)\n    for (int i = 0; i < VARR_LENGTH (lib_t, extra_libs); i++)\n      if ((handler = VARR_GET (lib_t, extra_libs, i).handler) != NULL\n          && (sym = dlsym (handler, name)) != NULL)\n        break;\n  if (sym == NULL) {\n#ifdef _WIN32\n    if (strcmp (name, \"LoadLibrary\") == 0) return LoadLibrary;\n    if (strcmp (name, \"FreeLibrary\") == 0) return FreeLibrary;\n    if (strcmp (name, \"GetProcAddress\") == 0) return GetProcAddress;\n#else\n    if (strcmp (name, \"dlopen\") == 0) return dlopen;\n    if (strcmp (name, \"dlerror\") == 0) return dlerror;\n    if (strcmp (name, \"dlclose\") == 0) return dlclose;\n    if (strcmp (name, \"dlsym\") == 0) return dlsym;\n    if (strcmp (name, \"stat\") == 0) return stat;\n    if (strcmp (name, \"lstat\") == 0) return lstat;\n    if (strcmp (name, \"fstat\") == 0) return fstat;\n#if defined(__APPLE__) && defined(__aarch64__)\n    if (strcmp (name, \"__nan\") == 0) return __nan;\n    if (strcmp (name, \"_MIR_set_code\") == 0) return _MIR_set_code;\n#endif\n#endif\n    fprintf (stderr, \"can not load symbol %s\\n\", name);\n    close_std_libs ();\n    exit (1);\n  }\n  return sym;\n}\n\nvoid lib_dirs_from_env_var (const char *env_var) {\n  const char *var_value = getenv (env_var);\n  if (var_value == NULL || var_value[0] == '\\0') return;\n\n  // copy to an allocated buffer\n  int value_len = strlen (var_value);\n  char *value = (char *) malloc (value_len + 1);\n  strcpy (value, var_value);\n\n  // colon separated list\n  char *value_ptr = value;\n  char *colon = NULL;\n  while ((colon = strchr (value_ptr, ':')) != NULL) {\n    colon[0] = '\\0';\n    VARR_PUSH (char_ptr_t, lib_dirs, value_ptr);\n    // goto next\n    value_ptr = colon + 1;\n  }\n  // final part of string\n  // colon == NULL\n  VARR_PUSH (char_ptr_t, lib_dirs, value_ptr);\n}\n\nint get_mir_type (void) {\n  const char *type_value = getenv (MIR_ENV_VAR_TYPE);\n  if (type_value == NULL || type_value[0] == '\\0') return MIR_TYPE_DEFAULT;\n\n  if (strcmp (type_value, MIR_TYPE_INTERP_NAME) == 0) return MIR_TYPE_INTERP;\n\n  if (strcmp (type_value, MIR_TYPE_GEN_NAME) == 0) return MIR_TYPE_GEN;\n\n  if (strcmp (type_value, MIR_TYPE_LAZY_NAME) == 0) return MIR_TYPE_LAZY;\n\n  fprintf (stderr, \"warning: unknown MIR_TYPE '%s', using default one\\n\", type_value);\n  return MIR_TYPE_DEFAULT;\n}\n\nvoid open_extra_libs (void) {\n  const char *var_value = getenv (MIR_ENV_VAR_EXTRA_LIBS);\n  if (var_value == NULL || var_value[0] == '\\0') return;\n\n  int value_len = strlen (var_value);\n  char *value = (char *) malloc (value_len + 1);\n  strcpy (value, var_value);\n\n  char *value_ptr = value;\n  char *colon = NULL;\n  while ((colon = strchr (value_ptr, ':')) != NULL) {\n    colon[0] = '\\0';\n    process_extra_lib (value_ptr);\n\n    value_ptr = colon + 1;\n  }\n  process_extra_lib (value_ptr);\n}\n\nint main (int argc, char **argv, char **envp) {\n  MIR_alloc_t alloc = &default_alloc;\n  // from binfmt_misc we expect the arguments to be:\n  // `mir-run /full/path/to/mir-binary mir-binary <args...>`\n  if (argc < 3) {\n    fprintf (stderr, \"usage: %s <full-path> <name> [<args>...]\\n\", argv[0]);\n    return 1;\n  }\n\n  int mir_type = get_mir_type ();\n\n  MIR_val_t val;\n  int exit_code;\n\n  VARR_CREATE (char, temp_string, alloc, 0);\n  VARR_CREATE (lib_t, extra_libs, alloc, 16);\n  VARR_CREATE (char_ptr_t, lib_dirs, alloc, 16);\n  for (int i = 0; i < sizeof (std_lib_dirs) / sizeof (char_ptr_t); i++)\n    VARR_PUSH (char_ptr_t, lib_dirs, std_lib_dirs[i]);\n  lib_dirs_from_env_var (\"LD_LIBRARY_PATH\");\n  lib_dirs_from_env_var (MIR_ENV_VAR_LIB_DIRS);\n\n  MIR_item_t main_func = NULL;\n\n  MIR_context_t mctx = MIR_init ();\n  FILE *mir_file = fopen (argv[1], \"rb\");\n  if (!mir_file) {\n    fprintf (stderr, \"failed to open file '%s'\\n\", argv[1]);\n    return 1;\n  }\n  MIR_read (mctx, mir_file);\n\n  for (MIR_module_t module = DLIST_HEAD (MIR_module_t, *MIR_get_module_list (mctx)); module != NULL;\n       module = DLIST_NEXT (MIR_module_t, module)) {\n    for (MIR_item_t func = DLIST_HEAD (MIR_item_t, module->items); func != NULL;\n         func = DLIST_NEXT (MIR_item_t, func)) {\n      if (func->item_type != MIR_func_item) continue;\n      if (strcmp (func->u.func->name, \"main\") == 0) main_func = func;\n    }\n    MIR_load_module (mctx, module);\n  }\n  if (main_func == NULL) {\n    fprintf (stderr, \"cannot execute program w/o main function\\n\");\n    return 1;\n  }\n\n  open_std_libs ();\n  open_extra_libs ();\n\n  if (mir_type == MIR_TYPE_INTERP) {\n    MIR_link (mctx, MIR_set_interp_interface, import_resolver);\n    MIR_interp (mctx, main_func, &val, 3, (MIR_val_t){.i = (argc - 2)},\n                (MIR_val_t){.a = (void *) (argv + 2)}, (MIR_val_t){.a = (void *) envp});\n    exit_code = val.i;\n  } else {\n    MIR_gen_init (mctx);\n    MIR_link (mctx, mir_type == MIR_TYPE_GEN ? MIR_set_gen_interface : MIR_set_lazy_gen_interface,\n              import_resolver);\n    uint64_t (*fun_addr) (int, char **, char **) = MIR_gen (mctx, main_func);\n    exit_code = fun_addr (argc - 2, argv + 2, envp);\n    MIR_gen_finish (mctx);\n  }\n  MIR_finish (mctx);\n  close_extra_libs ();\n  close_std_libs ();\n\n  return exit_code;\n}\n"
        },
        {
          "name": "mir-bitmap.h",
          "type": "blob",
          "size": 11.470703125,
          "content": "/* This file is a part of MIR project.\n   Copyright (C) 2018-2024 Vladimir Makarov <vmakarov.gcc@gmail.com>.\n*/\n\n#ifndef MIR_BITMAP_H\n\n#define MIR_BITMAP_H\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <assert.h>\n#include <stdint.h>\n#include <limits.h>\n#include \"mir-alloc.h\"\n#include \"mir-varr.h\"\n\n#define FALSE 0\n#define TRUE 1\n\n#if !defined(BITMAP_ENABLE_CHECKING) && !defined(NDEBUG)\n#define BITMAP_ENABLE_CHECKING\n#endif\n\n#ifndef BITMAP_ENABLE_CHECKING\n#define BITMAP_ASSERT(EXPR, OP) ((void) (EXPR))\n\n#else\nstatic inline void mir_bitmap_assert_fail (const char *op) {\n  fprintf (stderr, \"wrong %s for a bitmap\", op);\n  assert (0);\n}\n\n#define BITMAP_ASSERT(EXPR, OP) (void) ((EXPR) ? 0 : (mir_bitmap_assert_fail (#OP), 0))\n\n#endif\n\n#define BITMAP_WORD_BITS 64\n\ntypedef uint64_t bitmap_el_t;\n\nDEF_VARR (bitmap_el_t);\n\ntypedef VARR (bitmap_el_t) * bitmap_t;\ntypedef const VARR (bitmap_el_t) * const_bitmap_t;\n\nstatic inline bitmap_t bitmap_create2 (MIR_alloc_t alloc, size_t init_bits_num) {\n  bitmap_t bm;\n\n  VARR_CREATE (bitmap_el_t, bm, alloc, (init_bits_num + BITMAP_WORD_BITS - 1) / BITMAP_WORD_BITS);\n  return bm;\n}\n\nstatic inline bitmap_t bitmap_create (MIR_alloc_t alloc) { return bitmap_create2 (alloc, 0); }\n\nstatic inline void bitmap_destroy (bitmap_t bm) { VARR_DESTROY (bitmap_el_t, bm); }\n\nstatic inline size_t bitmap_size (bitmap_t bm) {\n  return VARR_CAPACITY (bitmap_el_t, bm) * sizeof (bitmap_el_t);\n}\n\nstatic inline void bitmap_clear (bitmap_t bm) { VARR_TRUNC (bitmap_el_t, bm, 0); }\n\nstatic inline void bitmap_expand (bitmap_t bm, size_t nb) {\n  size_t i, len = VARR_LENGTH (bitmap_el_t, bm);\n  size_t new_len = (nb + BITMAP_WORD_BITS - 1) / BITMAP_WORD_BITS;\n\n  for (i = len; i < new_len; i++) VARR_PUSH (bitmap_el_t, bm, (bitmap_el_t) 0);\n}\n\nstatic inline int bitmap_bit_p (const_bitmap_t bm, size_t nb) {\n  size_t nw, sh, len = VARR_LENGTH (bitmap_el_t, bm);\n  bitmap_el_t *addr = VARR_ADDR (bitmap_el_t, bm);\n\n  if (nb >= BITMAP_WORD_BITS * len) return 0;\n  nw = nb / BITMAP_WORD_BITS;\n  sh = nb % BITMAP_WORD_BITS;\n  return (addr[nw] >> sh) & 1;\n}\n\nstatic inline int bitmap_set_bit_p (bitmap_t bm, size_t nb) {\n  size_t nw, sh;\n  bitmap_el_t *addr;\n  int res;\n\n  bitmap_expand (bm, nb + 1);\n  addr = VARR_ADDR (bitmap_el_t, bm);\n  nw = nb / BITMAP_WORD_BITS;\n  sh = nb % BITMAP_WORD_BITS;\n  res = ((addr[nw] >> sh) & 1) == 0;\n  addr[nw] |= (bitmap_el_t) 1 << sh;\n  return res;\n}\n\nstatic inline int bitmap_clear_bit_p (bitmap_t bm, size_t nb) {\n  size_t nw, sh, len = VARR_LENGTH (bitmap_el_t, bm);\n  bitmap_el_t *addr = VARR_ADDR (bitmap_el_t, bm);\n  int res;\n\n  if (nb >= BITMAP_WORD_BITS * len) return 0;\n  nw = nb / BITMAP_WORD_BITS;\n  sh = nb % BITMAP_WORD_BITS;\n  res = (addr[nw] >> sh) & 1;\n  addr[nw] &= ~((bitmap_el_t) 1 << sh);\n  return res;\n}\n\nstatic inline int bitmap_set_or_clear_bit_range_p (bitmap_t bm, size_t nb, size_t len, int set_p) {\n  size_t nw, lsh, rsh, range_len;\n  bitmap_el_t mask, *addr;\n  int res = 0;\n\n  bitmap_expand (bm, nb + len);\n  addr = VARR_ADDR (bitmap_el_t, bm);\n  while (len > 0) {\n    nw = nb / BITMAP_WORD_BITS;\n    lsh = nb % BITMAP_WORD_BITS;\n    rsh = len >= BITMAP_WORD_BITS - lsh ? 0 : BITMAP_WORD_BITS - (nb + len) % BITMAP_WORD_BITS;\n    mask = ((~(bitmap_el_t) 0) >> (rsh + lsh)) << lsh;\n    if (set_p) {\n      res |= (~addr[nw] & mask) != 0;\n      addr[nw] |= mask;\n    } else {\n      res |= (addr[nw] & mask) != 0;\n      addr[nw] &= ~mask;\n    }\n    range_len = BITMAP_WORD_BITS - rsh - lsh;\n    len -= range_len;\n    nb += range_len;\n  }\n  return res;\n}\n\nstatic inline int bitmap_set_bit_range_p (bitmap_t bm, size_t nb, size_t len) {\n  return bitmap_set_or_clear_bit_range_p (bm, nb, len, TRUE);\n}\n\nstatic inline int bitmap_clear_bit_range_p (bitmap_t bm, size_t nb, size_t len) {\n  return bitmap_set_or_clear_bit_range_p (bm, nb, len, FALSE);\n}\n\nstatic inline void bitmap_copy (bitmap_t dst, const_bitmap_t src) {\n  size_t dst_len = VARR_LENGTH (bitmap_el_t, dst);\n  size_t src_len = VARR_LENGTH (bitmap_el_t, src);\n\n  if (dst_len >= src_len)\n    VARR_TRUNC (bitmap_el_t, dst, src_len);\n  else\n    bitmap_expand (dst, src_len * BITMAP_WORD_BITS);\n  memcpy (VARR_ADDR (bitmap_el_t, dst), VARR_ADDR (bitmap_el_t, src),\n          src_len * sizeof (bitmap_el_t));\n}\n\nstatic inline int bitmap_equal_p (const_bitmap_t bm1, const_bitmap_t bm2) {\n  const_bitmap_t temp_bm;\n  size_t i, temp_len, bm1_len = VARR_LENGTH (bitmap_el_t, bm1);\n  size_t bm2_len = VARR_LENGTH (bitmap_el_t, bm2);\n  bitmap_el_t *addr1, *addr2;\n\n  if (bm1_len > bm2_len) {\n    temp_bm = bm1;\n    bm1 = bm2;\n    bm2 = temp_bm;\n    temp_len = bm1_len;\n    bm1_len = bm2_len;\n    bm2_len = temp_len;\n  }\n  addr1 = VARR_ADDR (bitmap_el_t, bm1);\n  addr2 = VARR_ADDR (bitmap_el_t, bm2);\n  if (memcmp (addr1, addr2, bm1_len * sizeof (bitmap_el_t)) != 0) return FALSE;\n  for (i = bm1_len; i < bm2_len; i++)\n    if (addr2[i] != 0) return FALSE;\n  return TRUE;\n}\n\nstatic inline int bitmap_intersect_p (const_bitmap_t bm1, const_bitmap_t bm2) {\n  size_t i, min_len, bm1_len = VARR_LENGTH (bitmap_el_t, bm1);\n  size_t bm2_len = VARR_LENGTH (bitmap_el_t, bm2);\n  bitmap_el_t *addr1 = VARR_ADDR (bitmap_el_t, bm1);\n  bitmap_el_t *addr2 = VARR_ADDR (bitmap_el_t, bm2);\n\n  min_len = bm1_len <= bm2_len ? bm1_len : bm2_len;\n  for (i = 0; i < min_len; i++)\n    if ((addr1[i] & addr2[i]) != 0) return TRUE;\n  return FALSE;\n}\n\nstatic inline int bitmap_empty_p (const_bitmap_t bm) {\n  size_t i, len = VARR_LENGTH (bitmap_el_t, bm);\n  bitmap_el_t *addr = VARR_ADDR (bitmap_el_t, bm);\n\n  for (i = 0; i < len; i++)\n    if (addr[i] != 0) return FALSE;\n  return TRUE;\n}\n\nstatic inline bitmap_el_t bitmap_el_max2 (bitmap_el_t el1, bitmap_el_t el2) {\n  return el1 < el2 ? el2 : el1;\n}\n\nstatic inline bitmap_el_t bitmap_el_max3 (bitmap_el_t el1, bitmap_el_t el2, bitmap_el_t el3) {\n  if (el1 <= el2) return el2 < el3 ? el3 : el2;\n  return el1 < el3 ? el3 : el1;\n}\n\n/* Return the number of bits set in BM.  */\nstatic inline size_t bitmap_bit_count (const_bitmap_t bm) {\n  size_t i, len = VARR_LENGTH (bitmap_el_t, bm);\n  bitmap_el_t el, *addr = VARR_ADDR (bitmap_el_t, bm);\n  size_t count = 0;\n\n  for (i = 0; i < len; i++) {\n    if ((el = addr[i]) != 0) {\n      for (; el != 0; el >>= 1)\n        if (el & 1) count++;\n    }\n  }\n  return count;\n}\n\n/* Return min bit number in BM.  Return 0 for empty bitmap.  */\nstatic inline size_t bitmap_bit_min (const_bitmap_t bm) {\n  size_t i, len = VARR_LENGTH (bitmap_el_t, bm);\n  bitmap_el_t el, *addr = VARR_ADDR (bitmap_el_t, bm);\n  int count;\n\n  for (i = 0; i < len; i++) {\n    if ((el = addr[i]) != 0) {\n      for (count = 0; el != 0; el >>= 1, count++)\n        if (el & 1) return i * BITMAP_WORD_BITS + count;\n    }\n  }\n  return 0;\n}\n\n/* Return max bit number in BM.  Return 0 for empty bitmap.  */\nstatic inline size_t bitmap_bit_max (const_bitmap_t bm) {\n  size_t i, len = VARR_LENGTH (bitmap_el_t, bm);\n  bitmap_el_t el, *addr = VARR_ADDR (bitmap_el_t, bm);\n  int count;\n\n  if (len == 0) return 0;\n  for (i = len - 1;; i--) {\n    if ((el = addr[i]) != 0) {\n      for (count = BITMAP_WORD_BITS - 1; count >= 0; count--)\n        if ((el >> count) & 1) return i * BITMAP_WORD_BITS + count;\n    }\n    if (i == 0) break;\n  }\n  return 0;\n}\n\nstatic inline int bitmap_op2 (bitmap_t dst, const_bitmap_t src1, const_bitmap_t src2,\n                              bitmap_el_t (*op) (bitmap_el_t, bitmap_el_t)) {\n  size_t i, len, bound, src1_len, src2_len;\n  bitmap_el_t old, *dst_addr, *src1_addr, *src2_addr;\n  int change_p = FALSE;\n\n  src1_len = VARR_LENGTH (bitmap_el_t, src1);\n  src2_len = VARR_LENGTH (bitmap_el_t, src2);\n  len = bitmap_el_max2 (src1_len, src2_len);\n  bitmap_expand (dst, len * BITMAP_WORD_BITS);\n  dst_addr = VARR_ADDR (bitmap_el_t, dst);\n  src1_addr = VARR_ADDR (bitmap_el_t, src1);\n  src2_addr = VARR_ADDR (bitmap_el_t, src2);\n  for (bound = i = 0; i < len; i++) {\n    old = dst_addr[i];\n    if ((dst_addr[i] = op (i >= src1_len ? 0 : src1_addr[i], i >= src2_len ? 0 : src2_addr[i]))\n        != 0)\n      bound = i + 1;\n    if (old != dst_addr[i]) change_p = TRUE;\n  }\n  VARR_TRUNC (bitmap_el_t, dst, bound);\n  return change_p;\n}\n\nstatic inline bitmap_el_t bitmap_el_and (bitmap_el_t el1, bitmap_el_t el2) { return el1 & el2; }\n\nstatic inline int bitmap_and (bitmap_t dst, bitmap_t src1, bitmap_t src2) {\n  return bitmap_op2 (dst, src1, src2, bitmap_el_and);\n}\n\nstatic inline bitmap_el_t bitmap_el_and_compl (bitmap_el_t el1, bitmap_el_t el2) {\n  return el1 & ~el2;\n}\n\nstatic inline int bitmap_and_compl (bitmap_t dst, bitmap_t src1, bitmap_t src2) {\n  return bitmap_op2 (dst, src1, src2, bitmap_el_and_compl);\n}\n\nstatic inline bitmap_el_t bitmap_el_ior (bitmap_el_t el1, bitmap_el_t el2) { return el1 | el2; }\n\nstatic inline int bitmap_ior (bitmap_t dst, bitmap_t src1, bitmap_t src2) {\n  return bitmap_op2 (dst, src1, src2, bitmap_el_ior);\n}\n\nstatic inline int bitmap_op3 (bitmap_t dst, const_bitmap_t src1, const_bitmap_t src2,\n                              const_bitmap_t src3,\n                              bitmap_el_t (*op) (bitmap_el_t, bitmap_el_t, bitmap_el_t)) {\n  size_t i, len, bound, src1_len, src2_len, src3_len;\n  bitmap_el_t old, *dst_addr, *src1_addr, *src2_addr, *src3_addr;\n  int change_p = FALSE;\n\n  src1_len = VARR_LENGTH (bitmap_el_t, src1);\n  src2_len = VARR_LENGTH (bitmap_el_t, src2);\n  src3_len = VARR_LENGTH (bitmap_el_t, src3);\n  len = bitmap_el_max3 (src1_len, src2_len, src3_len);\n  bitmap_expand (dst, len * BITMAP_WORD_BITS);\n  dst_addr = VARR_ADDR (bitmap_el_t, dst);\n  src1_addr = VARR_ADDR (bitmap_el_t, src1);\n  src2_addr = VARR_ADDR (bitmap_el_t, src2);\n  src3_addr = VARR_ADDR (bitmap_el_t, src3);\n  for (bound = i = 0; i < len; i++) {\n    old = dst_addr[i];\n    if ((dst_addr[i] = op (i >= src1_len ? 0 : src1_addr[i], i >= src2_len ? 0 : src2_addr[i],\n                           i >= src3_len ? 0 : src3_addr[i]))\n        != 0)\n      bound = i + 1;\n    if (old != dst_addr[i]) change_p = TRUE;\n  }\n  VARR_TRUNC (bitmap_el_t, dst, bound);\n  return change_p;\n}\n\nstatic inline bitmap_el_t bitmap_el_ior_and (bitmap_el_t el1, bitmap_el_t el2, bitmap_el_t el3) {\n  return el1 | (el2 & el3);\n}\n\n/* DST = SRC1 | (SRC2 & SRC3).  Return true if DST changed.  */\nstatic inline int bitmap_ior_and (bitmap_t dst, bitmap_t src1, bitmap_t src2, bitmap_t src3) {\n  return bitmap_op3 (dst, src1, src2, src3, bitmap_el_ior_and);\n}\n\nstatic inline bitmap_el_t bitmap_el_ior_and_compl (bitmap_el_t el1, bitmap_el_t el2,\n                                                   bitmap_el_t el3) {\n  return el1 | (el2 & ~el3);\n}\n\n/* DST = SRC1 | (SRC2 & ~SRC3).  Return true if DST changed.  */\nstatic inline int bitmap_ior_and_compl (bitmap_t dst, bitmap_t src1, bitmap_t src2, bitmap_t src3) {\n  return bitmap_op3 (dst, src1, src2, src3, bitmap_el_ior_and_compl);\n}\n\ntypedef struct {\n  bitmap_t bitmap;\n  size_t nbit;\n} bitmap_iterator_t;\n\nstatic inline void bitmap_iterator_init (bitmap_iterator_t *iter, bitmap_t bitmap) {\n  iter->bitmap = bitmap;\n  iter->nbit = 0;\n}\n\nstatic inline int bitmap_iterator_next (bitmap_iterator_t *iter, size_t *nbit) {\n  const size_t el_bits_num = sizeof (bitmap_el_t) * CHAR_BIT;\n  size_t curr_nel = iter->nbit / el_bits_num, len = VARR_LENGTH (bitmap_el_t, iter->bitmap);\n  bitmap_el_t el, *addr = VARR_ADDR (bitmap_el_t, iter->bitmap);\n\n  for (; curr_nel < len; curr_nel++, iter->nbit = curr_nel * el_bits_num)\n    if ((el = addr[curr_nel]) != 0)\n      for (el >>= iter->nbit % el_bits_num; el != 0; el >>= 1, iter->nbit++)\n        if (el & 1) {\n          *nbit = iter->nbit++;\n          return TRUE;\n        }\n  return FALSE;\n}\n\n#define FOREACH_BITMAP_BIT(iter, bitmap, nbit) \\\n  for (bitmap_iterator_init (&iter, bitmap); bitmap_iterator_next (&iter, &nbit);)\n\n#endif /* #ifndef MIR_BITMAP_H */\n"
        },
        {
          "name": "mir-code-alloc-default.c",
          "type": "blob",
          "size": 2.658203125,
          "content": "/* This file is a part of MIR project.\n   Copyright (C) 2018-2024 Vladimir Makarov <vmakarov.gcc@gmail.com>.\n*/\n\n#include <stdlib.h>\n#include \"mir-code-alloc.h\"\n\n#ifdef __GNUC__\n#define CODE_ALLOC_UNUSED __attribute__ ((unused))\n#else\n#define CODE_ALLOC_UNUSED\n#endif\n\n#ifndef _WIN32\n#include <sys/mman.h>\n#include <unistd.h>\n\nstatic inline int get_native_mem_protect_flags (MIR_mem_protect_t prot) {\n  return prot == PROT_WRITE_EXEC ?\n#if defined(__riscv)\n    (PROT_WRITE | PROT_READ | PROT_EXEC)\n#else\n    (PROT_WRITE | PROT_EXEC)\n#endif\n    : (PROT_READ | PROT_EXEC);\n}\n\n#if defined(__APPLE__) && defined(__aarch64__)\n#include <libkern/OSCacheControl.h>\n#include <pthread.h>\n#endif\n\nstatic int default_mem_protect (void *addr, size_t len, MIR_mem_protect_t prot, void *user_data CODE_ALLOC_UNUSED) {\n  int native_prot = get_native_mem_protect_flags (prot);\n#if !defined(__APPLE__) || !defined(__aarch64__)\n  return mprotect (addr, len, native_prot);\n#else\n  if ((native_prot & PROT_WRITE) && pthread_jit_write_protect_supported_np ())\n    pthread_jit_write_protect_np (FALSE);\n  if (native_prot & PROT_READ) {\n    if (pthread_jit_write_protect_supported_np ()) pthread_jit_write_protect_np (TRUE);\n    sys_icache_invalidate (addr, len);\n  } else if (0) {\n    if (mprotect (addr, len, native_prot) != 0) {\n      perror (\"mem_protect\");\n      fprintf (stderr, \"good bye!\\n\");\n      exit (1);\n    }\n  }\n  return 0;\n#endif\n}\n\nstatic int default_mem_unmap (void *addr, size_t len, void *user_data CODE_ALLOC_UNUSED) {\n  return munmap (addr, len);\n}\n\nstatic void *default_mem_map (size_t len, void *user_data CODE_ALLOC_UNUSED) {\n#if defined(__APPLE__) && defined(__aarch64__)\n  return mmap (NULL, len, PROT_EXEC | PROT_WRITE | PROT_READ, MAP_PRIVATE | MAP_ANONYMOUS | MAP_JIT,\n               -1, 0);\n#else\n  return mmap (NULL, len, PROT_EXEC, MAP_PRIVATE | MAP_ANONYMOUS, -1, 0);\n#endif\n}\n#else\n#define WIN32_LEAN_AND_MEAN\n#include <windows.h>\n\nstatic int default_mem_protect (void *addr, size_t len, MIR_mem_protect_t prot, void *user_data CODE_ALLOC_UNUSED) {\n  int native_prod = prot == PROT_WRITE_EXEC ? PAGE_EXECUTE_READWRITE : PAGE_EXECUTE_READ;\n  DWORD old_prot = 0;\n  return VirtualProtect (addr, len, native_prod, &old_prot) ? 0 : -1;\n}\n\nstatic int default_mem_unmap (void *addr, size_t len, void *user_data CODE_ALLOC_UNUSED) {\n  return VirtualFree (addr, len, MEM_RELEASE) ? 0 : -1;\n}\n\nstatic void *default_mem_map (size_t len, void *user_data CODE_ALLOC_UNUSED) {\n  return VirtualAlloc (NULL, len, MEM_COMMIT, PAGE_EXECUTE);\n}\n#endif\n\nstatic struct MIR_code_alloc default_code_alloc = {\n  .mem_map = default_mem_map,\n  .mem_unmap = default_mem_unmap,\n  .mem_protect = default_mem_protect,\n  .user_data = NULL\n};\n"
        },
        {
          "name": "mir-code-alloc.h",
          "type": "blob",
          "size": 1.0888671875,
          "content": "/* This file is a part of MIR project.\n   Copyright (C) 2018-2024 Vladimir Makarov <vmakarov.gcc@gmail.com>.\n*/\n\n#ifndef MIR_CODE_ALLOC_H\n#define MIR_CODE_ALLOC_H\n\n#include <assert.h>\n\n#ifdef __cplusplus\nextern \"C\" {\n#endif\n\n#define MAP_FAILED NULL\n\ntypedef enum MIR_mem_protect {\n  PROT_WRITE_EXEC,\n  PROT_READ_EXEC\n} MIR_mem_protect_t;\n\ntypedef struct MIR_code_alloc {\n  void *(*mem_map) (size_t, void *);\n  int (*mem_unmap) (void *, size_t, void *);\n  int (*mem_protect) (void *, size_t, MIR_mem_protect_t, void *);\n  void *user_data;\n} *MIR_code_alloc_t;\n\nstatic inline void *MIR_mem_map (MIR_code_alloc_t code_alloc, size_t len) {\n  return code_alloc->mem_map (len, code_alloc->user_data);\n}\n\nstatic inline int MIR_mem_unmap (MIR_code_alloc_t code_alloc, void *addr, size_t len) {\n  return code_alloc->mem_unmap (addr, len, code_alloc->user_data);\n}\n\nstatic inline int MIR_mem_protect (MIR_code_alloc_t code_alloc, void *addr, size_t len, MIR_mem_protect_t prot) {\n  return code_alloc->mem_protect (addr, len, prot, code_alloc->user_data);\n}\n\n#ifdef __cplusplus\n}\n#endif\n\n#endif /* #ifndef MIR_CODE_ALLOC_H */\n"
        },
        {
          "name": "mir-dlist.h",
          "type": "blob",
          "size": 12.765625,
          "content": "/* This file is part of MIR project.\n   Copyright (C) 2018-2024 Vladimir Makarov <vmakarov.gcc@gmail.com>.\n*/\n\n/* Typed doubly linked lists.  */\n\n#ifndef MIR_DLIST_H\n\n#define MIR_DLIST_H\n\n#include <stdio.h>\n#include <assert.h>\n\n#if !defined(DLIST_ENABLE_CHECKING) && !defined(NDEBUG)\n#define DLIST_ENABLE_CHECKING\n#endif\n\n#ifndef DLIST_ENABLE_CHECKING\n#define DLIST_ASSERT(EXPR, OP, T) ((void) (EXPR))\n\n#else\nstatic inline void dlist_assert_fail (const char* op, const char* var) {\n  fprintf (stderr, \"wrong %s for %s\", op, var);\n  assert (0);\n}\n\n#define DLIST_ASSERT(EXPR, OP, T) (void) ((EXPR) ? 0 : (dlist_assert_fail (OP, #T), 0))\n\n#endif\n\n#ifdef __GNUC__\n#define MIR_DLIST_UNUSED __attribute__ ((unused))\n#else\n#define MIR_DLIST_UNUSED\n#endif\n\n#define DLIST(T) DLIST_##T\n#define DLIST_OP(T, OP) DLIST_##T##_##OP\n#define DLIST_OP_DEF(T, OP) MIR_DLIST_UNUSED DLIST_OP (T, OP)\n#define DLIST_LINK(T) DLIST_LINK_##T\n\n#define DLIST_LINK_T(T)           \\\n  typedef struct DLIST_LINK (T) { \\\n    T prev, next;                 \\\n  } DLIST_LINK (T)\n\n#define DEF_DLIST_LINK(T) DLIST_LINK_T (T);\n\n#define DEF_DLIST_TYPE(T)    \\\n  typedef struct DLIST (T) { \\\n    T head, tail;            \\\n  } DLIST (T)\n\n#define DEF_DLIST_CODE(T, LINK)                                                                    \\\n                                                                                                   \\\n  static inline void DLIST_OP_DEF (T, init) (DLIST (T) * list) { list->head = list->tail = NULL; } \\\n                                                                                                   \\\n  static inline T DLIST_OP_DEF (T, head) (DLIST (T) * list) { return list->head; }                 \\\n                                                                                                   \\\n  static inline T DLIST_OP_DEF (T, tail) (DLIST (T) * list) { return list->tail; }                 \\\n                                                                                                   \\\n  static inline T DLIST_OP_DEF (T, prev) (T elem) { return elem->LINK.prev; }                      \\\n  static inline T DLIST_OP_DEF (T, next) (T elem) { return elem->LINK.next; }                      \\\n                                                                                                   \\\n  static inline T DLIST_OP_DEF (T, el) (DLIST (T) * list, int n) {                                 \\\n    T e;                                                                                           \\\n                                                                                                   \\\n    if (n >= 0) {                                                                                  \\\n      for (e = list->head; e != NULL && n != 0; e = e->LINK.next, n--)                             \\\n        ;                                                                                          \\\n    } else {                                                                                       \\\n      for (e = list->tail; e != NULL && n != -1; e = e->LINK.prev, n++)                            \\\n        ;                                                                                          \\\n    }                                                                                              \\\n    return e;                                                                                      \\\n  }                                                                                                \\\n                                                                                                   \\\n  static inline void DLIST_OP_DEF (T, prepend) (DLIST (T) * list, T elem) {                        \\\n    DLIST_ASSERT (list&& elem, \"prepend\", T);                                                      \\\n    if (list->head == NULL) {                                                                      \\\n      DLIST_ASSERT (list->tail == NULL, \"prepend\", T);                                             \\\n      list->tail = elem;                                                                           \\\n    } else {                                                                                       \\\n      DLIST_ASSERT (list->head->LINK.prev == NULL, \"prepend\", T);                                  \\\n      list->head->LINK.prev = elem;                                                                \\\n    }                                                                                              \\\n    elem->LINK.prev = NULL;                                                                        \\\n    elem->LINK.next = list->head;                                                                  \\\n    list->head = elem;                                                                             \\\n  }                                                                                                \\\n                                                                                                   \\\n  static inline void DLIST_OP_DEF (T, append) (DLIST (T) * list, T elem) {                         \\\n    DLIST_ASSERT (list&& elem, \"append\", T);                                                       \\\n    if (list->tail == NULL) {                                                                      \\\n      DLIST_ASSERT (list->head == NULL, \"append\", T);                                              \\\n      list->head = elem;                                                                           \\\n    } else {                                                                                       \\\n      DLIST_ASSERT (list->tail->LINK.next == NULL, \"append\", T);                                   \\\n      list->tail->LINK.next = elem;                                                                \\\n    }                                                                                              \\\n    elem->LINK.next = NULL;                                                                        \\\n    elem->LINK.prev = list->tail;                                                                  \\\n    list->tail = elem;                                                                             \\\n  }                                                                                                \\\n                                                                                                   \\\n  static inline void DLIST_OP_DEF (T, insert_before) (DLIST (T) * list, T before, T elem) {        \\\n    DLIST_ASSERT (list&& before&& elem && list->tail, \"insert_before\", T);                         \\\n    if (before->LINK.prev == NULL) {                                                               \\\n      DLIST_ASSERT (list->head == before, \"insert_before\", T);                                     \\\n      before->LINK.prev = elem;                                                                    \\\n      elem->LINK.next = before;                                                                    \\\n      elem->LINK.prev = NULL;                                                                      \\\n      list->head = elem;                                                                           \\\n    } else {                                                                                       \\\n      DLIST_ASSERT (list->head, \"insert_before\", T);                                               \\\n      before->LINK.prev->LINK.next = elem;                                                         \\\n      elem->LINK.prev = before->LINK.prev;                                                         \\\n      before->LINK.prev = elem;                                                                    \\\n      elem->LINK.next = before;                                                                    \\\n    }                                                                                              \\\n  }                                                                                                \\\n                                                                                                   \\\n  static inline void DLIST_OP_DEF (T, insert_after) (DLIST (T) * list, T after, T elem) {          \\\n    DLIST_ASSERT (list&& after&& elem && list->head, \"insert_after\", T);                           \\\n    if (after->LINK.next == NULL) {                                                                \\\n      DLIST_ASSERT (list->tail == after, \"insert_after\", T);                                       \\\n      after->LINK.next = elem;                                                                     \\\n      elem->LINK.prev = after;                                                                     \\\n      elem->LINK.next = NULL;                                                                      \\\n      list->tail = elem;                                                                           \\\n    } else {                                                                                       \\\n      DLIST_ASSERT (list->tail, \"insert_after\", T);                                                \\\n      after->LINK.next->LINK.prev = elem;                                                          \\\n      elem->LINK.next = after->LINK.next;                                                          \\\n      after->LINK.next = elem;                                                                     \\\n      elem->LINK.prev = after;                                                                     \\\n    }                                                                                              \\\n  }                                                                                                \\\n                                                                                                   \\\n  static inline void DLIST_OP_DEF (T, remove) (DLIST (T) * list, T elem) {                         \\\n    DLIST_ASSERT (list&& elem, \"remove\", T);                                                       \\\n    if (elem->LINK.prev != NULL) {                                                                 \\\n      elem->LINK.prev->LINK.next = elem->LINK.next;                                                \\\n    } else {                                                                                       \\\n      DLIST_ASSERT (list->head == elem, \"remove\", T);                                              \\\n      list->head = elem->LINK.next;                                                                \\\n    }                                                                                              \\\n    if (elem->LINK.next != NULL) {                                                                 \\\n      elem->LINK.next->LINK.prev = elem->LINK.prev;                                                \\\n    } else {                                                                                       \\\n      DLIST_ASSERT (list->tail == elem, \"remove\", T);                                              \\\n      list->tail = elem->LINK.prev;                                                                \\\n    }                                                                                              \\\n    elem->LINK.prev = elem->LINK.next = NULL;                                                      \\\n  }                                                                                                \\\n                                                                                                   \\\n  static inline size_t DLIST_OP_DEF (T, length) (DLIST (T) * list) {                               \\\n    size_t len = 0;                                                                                \\\n    T curr;                                                                                        \\\n                                                                                                   \\\n    for (curr = list->head; curr != NULL; curr = curr->LINK.next) len++;                           \\\n    return len;                                                                                    \\\n  }\n\n#define DEF_DLIST(T, LINK) \\\n  DEF_DLIST_TYPE (T);      \\\n  DEF_DLIST_CODE (T, LINK)\n\n#define DLIST_INIT(T, L) (DLIST_OP (T, init) (&(L)))\n#define DLIST_HEAD(T, L) (DLIST_OP (T, head) (&(L)))\n#define DLIST_TAIL(T, L) (DLIST_OP (T, tail) (&(L)))\n#define DLIST_PREV(T, E) (DLIST_OP (T, prev) (E))\n#define DLIST_NEXT(T, E) (DLIST_OP (T, next) (E))\n#define DLIST_EL(T, L, N) (DLIST_OP (T, el) (&(L), N))\n#define DLIST_PREPEND(T, L, E) (DLIST_OP (T, prepend) (&(L), (E)))\n#define DLIST_APPEND(T, L, E) (DLIST_OP (T, append) (&(L), (E)))\n#define DLIST_INSERT_BEFORE(T, L, B, E) (DLIST_OP (T, insert_before) (&(L), (B), (E)))\n#define DLIST_INSERT_AFTER(T, L, A, E) (DLIST_OP (T, insert_after) (&(L), (A), (E)))\n#define DLIST_REMOVE(T, L, E) (DLIST_OP (T, remove) (&(L), (E)))\n#define DLIST_LENGTH(T, L) (DLIST_OP (T, length) (&(L)))\n\n#endif /* #ifndef MIR_DLIST_H */\n"
        },
        {
          "name": "mir-gen-aarch64.c",
          "type": "blob",
          "size": 108.458984375,
          "content": "/* This file is a part of MIR project.\n   Copyright (C) 2020-2024 Vladimir Makarov <vmakarov.gcc@gmail.com>.\n*/\n\nstatic void fancy_abort (int code) {\n  if (!code) abort ();\n}\n#undef gen_assert\n#define gen_assert(c) fancy_abort (c)\n\n#include <limits.h>\n\n#include \"mir-aarch64.h\"\n\nstatic const MIR_reg_t FP_HARD_REG = R29_HARD_REG;\nstatic const MIR_reg_t LINK_HARD_REG = R30_HARD_REG;\n\nstatic inline MIR_reg_t target_nth_loc (MIR_reg_t loc, MIR_type_t type MIR_UNUSED, int n) {\n  return loc + n;\n}\n\nstatic inline int target_call_used_hard_reg_p (MIR_reg_t hard_reg, MIR_type_t type) {\n  assert (hard_reg <= MAX_HARD_REG);\n  if (hard_reg <= SP_HARD_REG) return !(hard_reg >= R19_HARD_REG && hard_reg <= R28_HARD_REG);\n  return type == MIR_T_LD || !(hard_reg >= V8_HARD_REG && hard_reg <= V15_HARD_REG);\n}\n\n/* Stack layout (sp refers to the last reserved stack slot address)\n   from higher address to lower address memory:\n\n   | ...           |  prev func stack (start aligned to 16 bytes)\n   |---------------|\n   | gr save area  |  64 bytes optional area for vararg func integer reg save area (absent for\n   APPLE)\n   |---------------|\n   | vr save area  |  128 bytes optional area for vararg func fp reg save area (absent for APPLE)\n   |---------------|\n   | saved regs    |  callee saved regs used in the func (known only after RA), rounded 16 bytes\n   |---------------|\n   | slots assigned|  can be absent for small functions (known only after RA), rounded 16 bytes\n   |   to pseudos  |\n   |---------------|\n   |   previous    |  16-bytes setup in prolog, used only for varag func or args passed on stack\n   | stack start   |  to move args and to setup va_start on machinize pass\n   |---------------|\n   | LR            |  sp before prologue and after saving LR = start sp\n   |---------------|\n   | old FP        |  frame pointer for previous func stack frame; new FP refers for here\n   |               |  it has lowest address as 12-bit offsets are only positive\n   |---------------|\n   |  small aggr   |\n   |  save area    |  optional\n   |---------------|\n   | alloca areas  |  optional\n   |---------------|\n   | slots for     |  dynamically allocated/deallocated by caller\n   |  passing args |\n\n   size of slots and saved regs is multiple of 16 bytes\n\n */\n\n#if !defined(__APPLE__)\nstatic const int int_reg_save_area_size = 8 * 8;\nstatic const int reg_save_area_size = 8 * 8 + 8 * 16;\n#endif\n\nstatic const MIR_insn_code_t target_io_dup_op_insn_codes[] = {MIR_INSN_BOUND};\n\nstatic MIR_insn_code_t get_ext_code (MIR_type_t type) {\n  switch (type) {\n  case MIR_T_I8: return MIR_EXT8;\n  case MIR_T_U8: return MIR_UEXT8;\n  case MIR_T_I16: return MIR_EXT16;\n  case MIR_T_U16: return MIR_UEXT16;\n  case MIR_T_I32: return MIR_EXT32;\n  case MIR_T_U32: return MIR_UEXT32;\n  default: return MIR_INVALID_INSN;\n  }\n}\n\nstatic MIR_reg_t get_arg_reg (MIR_type_t arg_type, size_t *int_arg_num, size_t *fp_arg_num,\n                              MIR_insn_code_t *mov_code) {\n  MIR_reg_t arg_reg;\n\n  if (arg_type == MIR_T_F || arg_type == MIR_T_D || arg_type == MIR_T_LD) {\n    switch (*fp_arg_num) {\n    case 0:\n    case 1:\n    case 2:\n    case 3:\n    case 4:\n    case 5:\n    case 6:\n    case 7: arg_reg = V0_HARD_REG + *fp_arg_num; break;\n    default: arg_reg = MIR_NON_VAR; break;\n    }\n    (*fp_arg_num)++;\n    *mov_code = arg_type == MIR_T_F ? MIR_FMOV : arg_type == MIR_T_D ? MIR_DMOV : MIR_LDMOV;\n  } else { /* including BLK, RBLK: */\n    switch (*int_arg_num) {\n    case 0:\n    case 1:\n    case 2:\n    case 3:\n    case 4:\n    case 5:\n    case 6:\n    case 7: arg_reg = R0_HARD_REG + *int_arg_num; break;\n    default: arg_reg = MIR_NON_VAR; break;\n    }\n    (*int_arg_num)++;\n    *mov_code = MIR_MOV;\n  }\n  return arg_reg;\n}\n\nstatic void mir_blk_mov (uint64_t *to, uint64_t *from, uint64_t nwords) {\n  for (; nwords > 0; nwords--) *to++ = *from++;\n}\n\nstatic MIR_insn_t gen_mov (gen_ctx_t gen_ctx, MIR_insn_t anchor, MIR_insn_code_t code,\n                           MIR_op_t dst_op, MIR_op_t src_op) {\n  MIR_insn_t insn = MIR_new_insn (gen_ctx->ctx, code, dst_op, src_op);\n  gen_add_insn_before (gen_ctx, anchor, insn);\n  return insn;\n}\n\nstatic MIR_reg_t target_get_stack_slot_base_reg (gen_ctx_t gen_ctx MIR_UNUSED) {\n  return FP_HARD_REG;\n}\n\nstatic int target_valid_mem_offset_p (gen_ctx_t gen_ctx, MIR_type_t type, MIR_disp_t offset);\n\nstatic MIR_op_t new_mem_op (gen_ctx_t gen_ctx, MIR_insn_t anchor, MIR_type_t type, MIR_disp_t disp,\n                            MIR_reg_t base) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  if (target_valid_mem_offset_p (gen_ctx, type, disp))\n    return _MIR_new_var_mem_op (ctx, type, disp, base, MIR_NON_VAR, 1);\n  MIR_reg_t temp_reg = gen_new_temp_reg (gen_ctx, MIR_T_I64, curr_func_item->u.func);\n  MIR_op_t temp_reg_op = _MIR_new_var_op (ctx, temp_reg);\n  gen_mov (gen_ctx, anchor, MIR_MOV, temp_reg_op, MIR_new_int_op (ctx, disp));\n  gen_add_insn_before (gen_ctx, anchor,\n                       MIR_new_insn (ctx, MIR_ADD, temp_reg_op, temp_reg_op,\n                                     _MIR_new_var_op (ctx, base)));\n  return _MIR_new_var_mem_op (ctx, type, 0, temp_reg, MIR_NON_VAR, 1);\n}\n\nstatic MIR_op_t get_new_hard_reg_mem_op (gen_ctx_t gen_ctx, MIR_type_t type, MIR_disp_t disp,\n                                         MIR_reg_t base, MIR_insn_t *insn1, MIR_insn_t *insn2) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  *insn1 = *insn2 = NULL;\n  if (target_valid_mem_offset_p (gen_ctx, type, disp))\n    return _MIR_new_var_mem_op (ctx, type, disp, base, MIR_NON_VAR, 1);\n  MIR_op_t temp_reg_op = _MIR_new_var_op (ctx, TEMP_INT_HARD_REG2);\n  *insn1 = MIR_new_insn (ctx, MIR_MOV, temp_reg_op, MIR_new_int_op (ctx, disp));\n  *insn2 = MIR_new_insn (ctx, MIR_ADD, temp_reg_op, temp_reg_op, _MIR_new_var_op (ctx, base));\n  return _MIR_new_var_mem_op (ctx, type, 0, TEMP_INT_HARD_REG2, MIR_NON_VAR, 1);\n}\n\nstatic MIR_op_t new_hard_reg_mem_op (gen_ctx_t gen_ctx, MIR_insn_t anchor, MIR_type_t type,\n                                     MIR_disp_t disp, MIR_reg_t base) {\n  MIR_insn_t insn1, insn2;\n  MIR_op_t op = get_new_hard_reg_mem_op (gen_ctx, type, disp, base, &insn1, &insn2);\n  if (insn1 != NULL) gen_add_insn_before (gen_ctx, anchor, insn1);\n  if (insn2 != NULL) gen_add_insn_before (gen_ctx, anchor, insn2);\n  return op;\n}\n\nstatic const char *BLK_MOV = \"mir.blk_mov\";\nstatic const char *BLK_MOV_P = \"mir.blk_mov.p\";\n\nstatic void gen_blk_mov (gen_ctx_t gen_ctx, MIR_insn_t anchor, size_t to_disp,\n                         MIR_reg_t to_base_hard_reg, size_t from_disp, MIR_reg_t from_base_reg,\n                         size_t qwords, int save_regs) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  MIR_func_t func = curr_func_item->u.func;\n  MIR_item_t proto_item, func_import_item;\n  MIR_insn_t new_insn;\n  MIR_op_t ops[5], freg_op, treg_op, treg_op2, treg_op3;\n\n  treg_op = _MIR_new_var_op (ctx, gen_new_temp_reg (gen_ctx, MIR_T_I64, func));\n  if (qwords <= 16) {\n    for (; qwords > 0; qwords--, to_disp += 8, from_disp += 8) {\n      gen_mov (gen_ctx, anchor, MIR_MOV, treg_op,\n               new_mem_op (gen_ctx, anchor, MIR_T_I64, from_disp, from_base_reg));\n      gen_mov (gen_ctx, anchor, MIR_MOV,\n               new_hard_reg_mem_op (gen_ctx, anchor, MIR_T_I64, to_disp, to_base_hard_reg),\n               treg_op);\n    }\n    return;\n  }\n  treg_op2 = _MIR_new_var_op (ctx, gen_new_temp_reg (gen_ctx, MIR_T_I64, func));\n  treg_op3 = _MIR_new_var_op (ctx, gen_new_temp_reg (gen_ctx, MIR_T_I64, func));\n  /* Save arg regs: */\n  if (save_regs > 0)\n    gen_mov (gen_ctx, anchor, MIR_MOV, treg_op, _MIR_new_var_op (ctx, R0_HARD_REG));\n  if (save_regs > 1)\n    gen_mov (gen_ctx, anchor, MIR_MOV, treg_op2, _MIR_new_var_op (ctx, R1_HARD_REG));\n  if (save_regs > 2)\n    gen_mov (gen_ctx, anchor, MIR_MOV, treg_op3, _MIR_new_var_op (ctx, R2_HARD_REG));\n  /* call blk move: */\n  proto_item = _MIR_builtin_proto (ctx, curr_func_item->module, BLK_MOV_P, 0, NULL, 3, MIR_T_I64,\n                                   \"to\", MIR_T_I64, \"from\", MIR_T_I64, \"nwords\");\n  func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, BLK_MOV, mir_blk_mov);\n  freg_op = _MIR_new_var_op (ctx, gen_new_temp_reg (gen_ctx, MIR_T_I64, func));\n  new_insn = MIR_new_insn (ctx, MIR_MOV, freg_op, MIR_new_ref_op (ctx, func_import_item));\n  gen_add_insn_before (gen_ctx, anchor, new_insn);\n  gen_add_insn_before (gen_ctx, anchor,\n                       MIR_new_insn (gen_ctx->ctx, MIR_ADD, _MIR_new_var_op (ctx, R0_HARD_REG),\n                                     _MIR_new_var_op (ctx, to_base_hard_reg),\n                                     MIR_new_int_op (ctx, to_disp)));\n  gen_add_insn_before (gen_ctx, anchor,\n                       MIR_new_insn (gen_ctx->ctx, MIR_ADD, _MIR_new_var_op (ctx, R1_HARD_REG),\n                                     _MIR_new_var_op (ctx, from_base_reg),\n                                     MIR_new_int_op (ctx, from_disp)));\n  gen_mov (gen_ctx, anchor, MIR_MOV, _MIR_new_var_op (ctx, R2_HARD_REG),\n           MIR_new_int_op (ctx, qwords));\n  ops[0] = MIR_new_ref_op (ctx, proto_item);\n  ops[1] = freg_op;\n  ops[2] = _MIR_new_var_op (ctx, R0_HARD_REG);\n  ops[3] = _MIR_new_var_op (ctx, R1_HARD_REG);\n  ops[4] = _MIR_new_var_op (ctx, R2_HARD_REG);\n  new_insn = MIR_new_insn_arr (ctx, MIR_CALL, 5, ops);\n  gen_add_insn_before (gen_ctx, anchor, new_insn);\n  /* Restore arg regs: */\n  if (save_regs > 0)\n    gen_mov (gen_ctx, anchor, MIR_MOV, _MIR_new_var_op (ctx, R0_HARD_REG), treg_op);\n  if (save_regs > 1)\n    gen_mov (gen_ctx, anchor, MIR_MOV, _MIR_new_var_op (ctx, R1_HARD_REG), treg_op2);\n  if (save_regs > 2)\n    gen_mov (gen_ctx, anchor, MIR_MOV, _MIR_new_var_op (ctx, R2_HARD_REG), treg_op3);\n}\n\nstatic void machinize_call (gen_ctx_t gen_ctx, MIR_insn_t call_insn) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  MIR_func_t func = curr_func_item->u.func;\n  MIR_proto_t proto = call_insn->ops[0].u.ref->u.proto;\n  size_t nargs, nops = MIR_insn_nops (ctx, call_insn), start = proto->nres + 2;\n  size_t int_arg_num = 0, fp_arg_num = 0, mem_size = 0, blk_offset = 0, qwords;\n  MIR_type_t type, mem_type;\n  MIR_op_mode_t mode;\n  MIR_var_t *arg_vars = NULL;\n  MIR_reg_t arg_reg;\n  MIR_op_t arg_op, temp_op, arg_reg_op, ret_reg_op, mem_op;\n  MIR_insn_code_t new_insn_code, ext_code;\n  MIR_insn_t new_insn, prev_insn, next_insn, ext_insn, insn1, insn2;\n  MIR_insn_t prev_call_insn = DLIST_PREV (MIR_insn_t, call_insn);\n  MIR_insn_t curr_prev_call_insn = prev_call_insn;\n  uint32_t n_iregs, n_vregs;\n\n  if (call_insn->code == MIR_INLINE) call_insn->code = MIR_CALL;\n  if (proto->args == NULL) {\n    nargs = 0;\n  } else {\n    gen_assert (nops >= VARR_LENGTH (MIR_var_t, proto->args)\n                && (proto->vararg_p || nops - start == VARR_LENGTH (MIR_var_t, proto->args)));\n    nargs = VARR_LENGTH (MIR_var_t, proto->args);\n    arg_vars = VARR_ADDR (MIR_var_t, proto->args);\n  }\n  if (call_insn->ops[1].mode != MIR_OP_VAR) {\n    // ??? to optimize (can be immediate operand for func call)\n    temp_op = _MIR_new_var_op (ctx, gen_new_temp_reg (gen_ctx, MIR_T_I64, func));\n    new_insn = MIR_new_insn (ctx, MIR_MOV, temp_op, call_insn->ops[1]);\n    call_insn->ops[1] = temp_op;\n    gen_add_insn_before (gen_ctx, call_insn, new_insn);\n  }\n  for (size_t i = start; i < nops; i++) { /* calculate offset for blk params */\n    if (i - start < nargs) {\n      type = arg_vars[i - start].type;\n    } else if (call_insn->ops[i].mode == MIR_OP_VAR_MEM) {\n      type = call_insn->ops[i].u.var_mem.type;\n      gen_assert (MIR_all_blk_type_p (type));\n    } else {\n      mode = call_insn->ops[i].value_mode;  // ??? smaller ints\n      gen_assert (mode == MIR_OP_INT || mode == MIR_OP_UINT || mode == MIR_OP_FLOAT\n                  || mode == MIR_OP_DOUBLE || mode == MIR_OP_LDOUBLE);\n      if (mode == MIR_OP_FLOAT)\n        (*MIR_get_error_func (ctx)) (MIR_call_op_error,\n                                     \"passing float variadic arg (should be passed as double)\");\n      if (mode == MIR_OP_LDOUBLE && __SIZEOF_LONG_DOUBLE__ == 8) mode = MIR_OP_DOUBLE;\n      type = mode == MIR_OP_DOUBLE ? MIR_T_D : mode == MIR_OP_LDOUBLE ? MIR_T_LD : MIR_T_I64;\n    }\n    gen_assert (!MIR_all_blk_type_p (type) || call_insn->ops[i].mode == MIR_OP_VAR_MEM);\n    if (type == MIR_T_RBLK && i == start) continue; /* hidden arg */\n#if defined(__APPLE__)                              /* all varargs are passed on stack */\n    if (i - start == nargs) int_arg_num = fp_arg_num = 8;\n#endif\n    if (MIR_blk_type_p (type) && (qwords = (call_insn->ops[i].u.var_mem.disp + 7) / 8) <= 2) {\n      if (int_arg_num + qwords > 8) blk_offset += qwords * 8;\n      int_arg_num += qwords;\n    } else if (get_arg_reg (type, &int_arg_num, &fp_arg_num, &new_insn_code) == MIR_NON_VAR) {\n      if (type == MIR_T_LD && __SIZEOF_LONG_DOUBLE__ == 16 && blk_offset % 16 != 0)\n        blk_offset = (blk_offset + 15) / 16 * 16;\n      blk_offset += type == MIR_T_LD && __SIZEOF_LONG_DOUBLE__ == 16 ? 16 : 8;\n    }\n  }\n  blk_offset = (blk_offset + 15) / 16 * 16;\n  int_arg_num = fp_arg_num = 0;\n  for (size_t i = start; i < nops; i++) {\n#if defined(__APPLE__) /* all varargs are passed on stack */\n    if (i - start == nargs) int_arg_num = fp_arg_num = 8;\n#endif\n    arg_op = call_insn->ops[i];\n    gen_assert (arg_op.mode == MIR_OP_VAR\n                || (arg_op.mode == MIR_OP_VAR_MEM && MIR_all_blk_type_p (arg_op.u.var_mem.type)));\n    if (i - start < nargs) {\n      type = arg_vars[i - start].type;\n    } else if (call_insn->ops[i].mode == MIR_OP_VAR_MEM) {\n      type = call_insn->ops[i].u.var_mem.type;\n      gen_assert (MIR_all_blk_type_p (type));\n    } else {\n      mode = call_insn->ops[i].value_mode;  // ??? smaller ints\n      if (mode == MIR_OP_LDOUBLE && __SIZEOF_LONG_DOUBLE__ == 8) mode = MIR_OP_DOUBLE;\n      type = mode == MIR_OP_DOUBLE ? MIR_T_D : mode == MIR_OP_LDOUBLE ? MIR_T_LD : MIR_T_I64;\n    }\n    ext_insn = NULL;\n    if ((ext_code = get_ext_code (type)) != MIR_INVALID_INSN) { /* extend arg if necessary */\n      temp_op = _MIR_new_var_op (ctx, gen_new_temp_reg (gen_ctx, MIR_T_I64, func));\n      ext_insn = MIR_new_insn (ctx, ext_code, temp_op, arg_op);\n      call_insn->ops[i] = arg_op = temp_op;\n    }\n    gen_assert (!MIR_all_blk_type_p (type)\n                || (arg_op.mode == MIR_OP_VAR_MEM && arg_op.u.var_mem.disp >= 0\n                    && arg_op.u.var_mem.index == MIR_NON_VAR));\n    if (type == MIR_T_RBLK && i == start) { /* hidden arg */\n      arg_reg_op = _MIR_new_var_op (ctx, R8_HARD_REG);\n      gen_mov (gen_ctx, call_insn, MIR_MOV, arg_reg_op,\n               _MIR_new_var_op (ctx, arg_op.u.var_mem.base));\n      call_insn->ops[i] = arg_reg_op;\n      continue;\n    } else if (MIR_blk_type_p (type)) {\n      qwords = (arg_op.u.var_mem.disp + 7) / 8;\n      if (qwords <= 2) {\n        arg_reg = R0_HARD_REG + int_arg_num;\n        if (int_arg_num + qwords <= 8) {\n          /* A trick to keep arg regs live: */\n          call_insn->ops[i] = _MIR_new_var_mem_op (ctx, MIR_T_UNDEF, 0, int_arg_num,\n                                                   qwords < 2 ? MIR_NON_VAR : int_arg_num + 1, 1);\n          if (qwords == 0) continue;\n          new_insn = MIR_new_insn (ctx, MIR_MOV, _MIR_new_var_op (ctx, R0_HARD_REG + int_arg_num++),\n                                   _MIR_new_var_mem_op (ctx, MIR_T_I64, 0, arg_op.u.var_mem.base,\n                                                        MIR_NON_VAR, 1));\n          gen_add_insn_before (gen_ctx, call_insn, new_insn);\n          if (qwords == 2) {\n            new_insn\n              = MIR_new_insn (ctx, MIR_MOV, _MIR_new_var_op (ctx, R0_HARD_REG + int_arg_num++),\n                              _MIR_new_var_mem_op (ctx, MIR_T_I64, 8, arg_op.u.var_mem.base,\n                                                   MIR_NON_VAR, 1));\n            gen_add_insn_before (gen_ctx, call_insn, new_insn);\n          }\n        } else { /* pass on stack w/o address: */\n          gen_blk_mov (gen_ctx, call_insn, mem_size, SP_HARD_REG, 0, arg_op.u.var_mem.base, qwords,\n                       int_arg_num);\n          call_insn->ops[i]\n            = _MIR_new_var_mem_op (ctx, MIR_T_UNDEF,\n                                   mem_size, /* we don't care about valid mem disp here */\n                                   SP_HARD_REG, MIR_NON_VAR, 1);\n          mem_size += qwords * 8;\n          blk_offset += qwords * 8;\n          int_arg_num += qwords;\n        }\n        continue;\n      }\n      gen_blk_mov (gen_ctx, call_insn, blk_offset, SP_HARD_REG, 0, arg_op.u.var_mem.base, qwords,\n                   int_arg_num);\n      arg_op = _MIR_new_var_op (ctx, gen_new_temp_reg (gen_ctx, MIR_T_I64, func));\n      gen_assert (curr_prev_call_insn\n                  != NULL); /* call_insn should not be 1st after simplification */\n      new_insn = MIR_new_insn (gen_ctx->ctx, MIR_ADD, arg_op, _MIR_new_var_op (ctx, SP_HARD_REG),\n                               MIR_new_int_op (ctx, blk_offset));\n      gen_add_insn_after (gen_ctx, curr_prev_call_insn, new_insn);\n      curr_prev_call_insn = DLIST_NEXT (MIR_insn_t, new_insn);\n      blk_offset += qwords * 8;\n    }\n    if ((arg_reg = get_arg_reg (type, &int_arg_num, &fp_arg_num, &new_insn_code)) != MIR_NON_VAR) {\n      /* put arguments to argument hard regs */\n      if (ext_insn != NULL) gen_add_insn_before (gen_ctx, call_insn, ext_insn);\n      arg_reg_op = _MIR_new_var_op (ctx, arg_reg);\n      if (type != MIR_T_RBLK) {\n        new_insn = MIR_new_insn (ctx, new_insn_code, arg_reg_op, arg_op);\n      } else {\n        assert (arg_op.mode == MIR_OP_VAR_MEM);\n        new_insn\n          = MIR_new_insn (ctx, MIR_MOV, arg_reg_op, _MIR_new_var_op (ctx, arg_op.u.var_mem.base));\n        arg_reg_op\n          = _MIR_new_var_mem_op (ctx, MIR_T_RBLK,\n                                 arg_op.u.var_mem.disp, /* we don't care about valid disp here */\n                                 arg_reg, MIR_NON_VAR, 1);\n      }\n      gen_add_insn_before (gen_ctx, call_insn, new_insn);\n      call_insn->ops[i] = arg_reg_op;\n    } else { /* put arguments on the stack */\n      if (type == MIR_T_LD && __SIZEOF_LONG_DOUBLE__ == 16 && mem_size % 16 != 0)\n        mem_size = (mem_size + 15) / 16 * 16;\n      mem_type = type == MIR_T_F || type == MIR_T_D || type == MIR_T_LD ? type : MIR_T_I64;\n      new_insn_code = (type == MIR_T_F    ? MIR_FMOV\n                       : type == MIR_T_D  ? MIR_DMOV\n                       : type == MIR_T_LD ? MIR_LDMOV\n                                          : MIR_MOV);\n      mem_op = get_new_hard_reg_mem_op (gen_ctx, mem_type, mem_size, SP_HARD_REG, &insn1, &insn2);\n      if (type != MIR_T_RBLK) {\n        new_insn = MIR_new_insn (ctx, new_insn_code, mem_op, arg_op);\n      } else {\n        assert (arg_op.mode == MIR_OP_VAR_MEM);\n        new_insn\n          = MIR_new_insn (ctx, new_insn_code, mem_op, _MIR_new_var_op (ctx, arg_op.u.var_mem.base));\n      }\n      gen_assert (curr_prev_call_insn\n                  != NULL); /* call_insn should not be 1st after simplification */\n      MIR_insert_insn_after (ctx, curr_func_item, curr_prev_call_insn, new_insn);\n      if (insn2 != NULL) MIR_insert_insn_after (ctx, curr_func_item, curr_prev_call_insn, insn2);\n      if (insn1 != NULL) MIR_insert_insn_after (ctx, curr_func_item, curr_prev_call_insn, insn1);\n      prev_insn = curr_prev_call_insn;\n      next_insn = DLIST_NEXT (MIR_insn_t, new_insn);\n      create_new_bb_insns (gen_ctx, prev_insn, next_insn, call_insn);\n      call_insn->ops[i] = mem_op;\n      mem_size += type == MIR_T_LD && __SIZEOF_LONG_DOUBLE__ == 16 ? 16 : 8;\n      if (ext_insn != NULL) gen_add_insn_after (gen_ctx, curr_prev_call_insn, ext_insn);\n      curr_prev_call_insn = new_insn;\n    }\n  }\n  blk_offset = (blk_offset + 15) / 16 * 16;\n  if (blk_offset != 0) mem_size = blk_offset;\n  n_iregs = n_vregs = 0;\n  for (size_t i = 0; i < proto->nres; i++) {\n    int float_p;\n\n    ret_reg_op = call_insn->ops[i + 2];\n    gen_assert (ret_reg_op.mode == MIR_OP_VAR);\n    type = proto->res_types[i];\n    float_p = type == MIR_T_F || type == MIR_T_D || type == MIR_T_LD;\n    if (float_p && n_vregs < 8) {\n      new_insn = MIR_new_insn (ctx,\n                               type == MIR_T_F   ? MIR_FMOV\n                               : type == MIR_T_D ? MIR_DMOV\n                                                 : MIR_LDMOV,\n                               ret_reg_op, _MIR_new_var_op (ctx, V0_HARD_REG + n_vregs));\n      n_vregs++;\n    } else if (!float_p && n_iregs < 8) {\n      new_insn\n        = MIR_new_insn (ctx, MIR_MOV, ret_reg_op, _MIR_new_var_op (ctx, R0_HARD_REG + n_iregs));\n      n_iregs++;\n    } else {\n      (*MIR_get_error_func (ctx)) (MIR_ret_error,\n                                   \"aarch64 can not handle this combination of return values\");\n    }\n    MIR_insert_insn_after (ctx, curr_func_item, call_insn, new_insn);\n    call_insn->ops[i + 2] = new_insn->ops[1];\n    if ((ext_code = get_ext_code (type)) != MIR_INVALID_INSN) {\n      MIR_insert_insn_after (ctx, curr_func_item, new_insn,\n                             MIR_new_insn (ctx, ext_code, ret_reg_op, ret_reg_op));\n      new_insn = DLIST_NEXT (MIR_insn_t, new_insn);\n    }\n    create_new_bb_insns (gen_ctx, call_insn, DLIST_NEXT (MIR_insn_t, new_insn), call_insn);\n  }\n  if (mem_size != 0) {                    /* allocate/deallocate stack for args passed on stack */\n    mem_size = (mem_size + 15) / 16 * 16; /* make it of several 16 bytes */\n    new_insn = MIR_new_insn (ctx, MIR_SUB, _MIR_new_var_op (ctx, SP_HARD_REG),\n                             _MIR_new_var_op (ctx, SP_HARD_REG), MIR_new_int_op (ctx, mem_size));\n    MIR_insert_insn_after (ctx, curr_func_item, prev_call_insn, new_insn);\n    next_insn = DLIST_NEXT (MIR_insn_t, new_insn);\n    create_new_bb_insns (gen_ctx, prev_call_insn, next_insn, call_insn);\n    new_insn = MIR_new_insn (ctx, MIR_ADD, _MIR_new_var_op (ctx, SP_HARD_REG),\n                             _MIR_new_var_op (ctx, SP_HARD_REG), MIR_new_int_op (ctx, mem_size));\n    MIR_insert_insn_after (ctx, curr_func_item, call_insn, new_insn);\n    next_insn = DLIST_NEXT (MIR_insn_t, new_insn);\n    create_new_bb_insns (gen_ctx, call_insn, next_insn, call_insn);\n  }\n}\n\nstatic long double mir_i2ld (int64_t i) { return i; }\nstatic const char *I2LD = \"mir.i2ld\";\nstatic const char *I2LD_P = \"mir.i2ld.p\";\n\nstatic long double mir_ui2ld (uint64_t i) { return i; }\nstatic const char *UI2LD = \"mir.ui2ld\";\nstatic const char *UI2LD_P = \"mir.ui2ld.p\";\n\nstatic long double mir_f2ld (float f) { return f; }\nstatic const char *F2LD = \"mir.f2ld\";\nstatic const char *F2LD_P = \"mir.f2ld.p\";\n\nstatic long double mir_d2ld (double d) { return d; }\nstatic const char *D2LD = \"mir.d2ld\";\nstatic const char *D2LD_P = \"mir.d2ld.p\";\n\nstatic int64_t mir_ld2i (long double ld) { return ld; }\nstatic const char *LD2I = \"mir.ld2i\";\nstatic const char *LD2I_P = \"mir.ld2i.p\";\n\nstatic float mir_ld2f (long double ld) { return ld; }\nstatic const char *LD2F = \"mir.ld2f\";\nstatic const char *LD2F_P = \"mir.ld2f.p\";\n\nstatic double mir_ld2d (long double ld) { return ld; }\nstatic const char *LD2D = \"mir.ld2d\";\nstatic const char *LD2D_P = \"mir.ld2d.p\";\n\nstatic long double mir_ldadd (long double d1, long double d2) { return d1 + d2; }\nstatic const char *LDADD = \"mir.ldadd\";\nstatic const char *LDADD_P = \"mir.ldadd.p\";\n\nstatic long double mir_ldsub (long double d1, long double d2) { return d1 - d2; }\nstatic const char *LDSUB = \"mir.ldsub\";\nstatic const char *LDSUB_P = \"mir.ldsub.p\";\n\nstatic long double mir_ldmul (long double d1, long double d2) { return d1 * d2; }\nstatic const char *LDMUL = \"mir.ldmul\";\nstatic const char *LDMUL_P = \"mir.ldmul.p\";\n\nstatic long double mir_lddiv (long double d1, long double d2) { return d1 / d2; }\nstatic const char *LDDIV = \"mir.lddiv\";\nstatic const char *LDDIV_P = \"mir.lddiv.p\";\n\nstatic long double mir_ldneg (long double d) { return -d; }\nstatic const char *LDNEG = \"mir.ldneg\";\nstatic const char *LDNEG_P = \"mir.ldneg.p\";\n\nstatic const char *VA_ARG_P = \"mir.va_arg.p\";\nstatic const char *VA_ARG = \"mir.va_arg\";\nstatic const char *VA_BLOCK_ARG_P = \"mir.va_block_arg.p\";\nstatic const char *VA_BLOCK_ARG = \"mir.va_block_arg\";\n\nstatic int64_t mir_ldeq (long double d1, long double d2) { return d1 == d2; }\nstatic const char *LDEQ = \"mir.ldeq\";\nstatic const char *LDEQ_P = \"mir.ldeq.p\";\n\nstatic int64_t mir_ldne (long double d1, long double d2) { return d1 != d2; }\nstatic const char *LDNE = \"mir.ldne\";\nstatic const char *LDNE_P = \"mir.ldne.p\";\n\nstatic int64_t mir_ldlt (long double d1, long double d2) { return d1 < d2; }\nstatic const char *LDLT = \"mir.ldlt\";\nstatic const char *LDLT_P = \"mir.ldlt.p\";\n\nstatic int64_t mir_ldge (long double d1, long double d2) { return d1 >= d2; }\nstatic const char *LDGE = \"mir.ldge\";\nstatic const char *LDGE_P = \"mir.ldge.p\";\n\nstatic int64_t mir_ldgt (long double d1, long double d2) { return d1 > d2; }\nstatic const char *LDGT = \"mir.ldgt\";\nstatic const char *LDGT_P = \"mir.ldgt.p\";\n\nstatic int64_t mir_ldle (long double d1, long double d2) { return d1 <= d2; }\nstatic const char *LDLE = \"mir.ldle\";\nstatic const char *LDLE_P = \"mir.ldle.p\";\n\nstatic int get_builtin (gen_ctx_t gen_ctx, MIR_insn_code_t code, MIR_item_t *proto_item,\n                        MIR_item_t *func_import_item) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  MIR_type_t res_type;\n\n  *func_import_item = *proto_item = NULL; /* to remove uninitialized warning */\n  switch (code) {\n  case MIR_I2LD:\n    res_type = MIR_T_LD;\n    *proto_item\n      = _MIR_builtin_proto (ctx, curr_func_item->module, I2LD_P, 1, &res_type, 1, MIR_T_I64, \"v\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, I2LD, mir_i2ld);\n    return 1;\n  case MIR_UI2LD:\n    res_type = MIR_T_LD;\n    *proto_item\n      = _MIR_builtin_proto (ctx, curr_func_item->module, UI2LD_P, 1, &res_type, 1, MIR_T_I64, \"v\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, UI2LD, mir_ui2ld);\n    return 1;\n  case MIR_F2LD:\n    res_type = MIR_T_LD;\n    *proto_item\n      = _MIR_builtin_proto (ctx, curr_func_item->module, F2LD_P, 1, &res_type, 1, MIR_T_F, \"v\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, F2LD, mir_f2ld);\n    return 1;\n  case MIR_D2LD:\n    res_type = MIR_T_LD;\n    *proto_item\n      = _MIR_builtin_proto (ctx, curr_func_item->module, D2LD_P, 1, &res_type, 1, MIR_T_D, \"v\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, D2LD, mir_d2ld);\n    return 1;\n  case MIR_LD2I:\n    res_type = MIR_T_I64;\n    *proto_item\n      = _MIR_builtin_proto (ctx, curr_func_item->module, LD2I_P, 1, &res_type, 1, MIR_T_LD, \"v\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, LD2I, mir_ld2i);\n    return 1;\n  case MIR_LD2F:\n    res_type = MIR_T_F;\n    *proto_item\n      = _MIR_builtin_proto (ctx, curr_func_item->module, LD2F_P, 1, &res_type, 1, MIR_T_LD, \"v\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, LD2F, mir_ld2f);\n    return 1;\n  case MIR_LD2D:\n    res_type = MIR_T_D;\n    *proto_item\n      = _MIR_builtin_proto (ctx, curr_func_item->module, LD2D_P, 1, &res_type, 1, MIR_T_LD, \"v\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, LD2D, mir_ld2d);\n    return 1;\n  case MIR_LDADD:\n    res_type = MIR_T_LD;\n    *proto_item = _MIR_builtin_proto (ctx, curr_func_item->module, LDADD_P, 1, &res_type, 2,\n                                      MIR_T_LD, \"d1\", MIR_T_LD, \"d2\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, LDADD, mir_ldadd);\n    return 2;\n  case MIR_LDSUB:\n    res_type = MIR_T_LD;\n    *proto_item = _MIR_builtin_proto (ctx, curr_func_item->module, LDSUB_P, 1, &res_type, 2,\n                                      MIR_T_LD, \"d1\", MIR_T_LD, \"d2\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, LDSUB, mir_ldsub);\n    return 2;\n  case MIR_LDMUL:\n    res_type = MIR_T_LD;\n    *proto_item = _MIR_builtin_proto (ctx, curr_func_item->module, LDMUL_P, 1, &res_type, 2,\n                                      MIR_T_LD, \"d1\", MIR_T_LD, \"d2\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, LDMUL, mir_ldmul);\n    return 2;\n  case MIR_LDDIV:\n    res_type = MIR_T_LD;\n    *proto_item = _MIR_builtin_proto (ctx, curr_func_item->module, LDDIV_P, 1, &res_type, 2,\n                                      MIR_T_LD, \"d1\", MIR_T_LD, \"d2\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, LDDIV, mir_lddiv);\n    return 2;\n  case MIR_LDNEG:\n    res_type = MIR_T_LD;\n    *proto_item\n      = _MIR_builtin_proto (ctx, curr_func_item->module, LDNEG_P, 1, &res_type, 1, MIR_T_LD, \"d\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, LDNEG, mir_ldneg);\n    return 1;\n  case MIR_LDEQ:\n    res_type = MIR_T_I64;\n    *proto_item = _MIR_builtin_proto (ctx, curr_func_item->module, LDEQ_P, 1, &res_type, 2,\n                                      MIR_T_LD, \"d1\", MIR_T_LD, \"d2\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, LDEQ, mir_ldeq);\n    return 2;\n  case MIR_LDNE:\n    res_type = MIR_T_I64;\n    *proto_item = _MIR_builtin_proto (ctx, curr_func_item->module, LDNE_P, 1, &res_type, 2,\n                                      MIR_T_LD, \"d1\", MIR_T_LD, \"d2\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, LDNE, mir_ldne);\n    return 2;\n  case MIR_LDLT:\n    res_type = MIR_T_I64;\n    *proto_item = _MIR_builtin_proto (ctx, curr_func_item->module, LDLT_P, 1, &res_type, 2,\n                                      MIR_T_LD, \"d1\", MIR_T_LD, \"d2\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, LDLT, mir_ldlt);\n    return 2;\n  case MIR_LDGE:\n    res_type = MIR_T_I64;\n    *proto_item = _MIR_builtin_proto (ctx, curr_func_item->module, LDGE_P, 1, &res_type, 2,\n                                      MIR_T_LD, \"d1\", MIR_T_LD, \"d2\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, LDGE, mir_ldge);\n    return 2;\n  case MIR_LDGT:\n    res_type = MIR_T_I64;\n    *proto_item = _MIR_builtin_proto (ctx, curr_func_item->module, LDGT_P, 1, &res_type, 2,\n                                      MIR_T_LD, \"d1\", MIR_T_LD, \"d2\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, LDGT, mir_ldgt);\n    return 2;\n  case MIR_LDLE:\n    res_type = MIR_T_I64;\n    *proto_item = _MIR_builtin_proto (ctx, curr_func_item->module, LDLE_P, 1, &res_type, 2,\n                                      MIR_T_LD, \"d1\", MIR_T_LD, \"d2\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, LDLE, mir_ldle);\n    return 2;\n  case MIR_VA_ARG:\n    res_type = MIR_T_I64;\n    *proto_item = _MIR_builtin_proto (ctx, curr_func_item->module, VA_ARG_P, 1, &res_type, 2,\n                                      MIR_T_I64, \"va\", MIR_T_I64, \"type\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, VA_ARG, va_arg_builtin);\n    return 2;\n  case MIR_VA_BLOCK_ARG:\n    *proto_item\n      = _MIR_builtin_proto (ctx, curr_func_item->module, VA_BLOCK_ARG_P, 0, NULL, 4, MIR_T_I64,\n                            \"res\", MIR_T_I64, \"va\", MIR_T_I64, \"size\", MIR_T_I64, \"ncase\");\n    *func_import_item\n      = _MIR_builtin_func (ctx, curr_func_item->module, VA_BLOCK_ARG, va_block_arg_builtin);\n    return 4;\n  default: return 0;\n  }\n}\n\nstruct insn_pattern_info {\n  int start, num;\n};\n\ntypedef struct insn_pattern_info insn_pattern_info_t;\nDEF_VARR (insn_pattern_info_t);\n\nstruct label_ref {\n  int abs_addr_p, short_p;\n  size_t label_val_disp;\n  union {\n    MIR_label_t label;\n    void *jump_addr; /* absolute addr for BBV */\n  } u;\n};\n\ntypedef struct label_ref label_ref_t;\nDEF_VARR (label_ref_t);\n\nstruct target_ctx {\n  unsigned char alloca_p, block_arg_func_p, leaf_p, short_bb_branch_p;\n  size_t small_aggregate_save_area;\n  MIR_insn_t temp_jump;\n  const char *temp_jump_replacement;\n  VARR (int) * pattern_indexes;\n  VARR (insn_pattern_info_t) * insn_pattern_info;\n  VARR (uint8_t) * result_code;\n  VARR (label_ref_t) * label_refs;\n  VARR (uint64_t) * abs_address_locs;\n  VARR (MIR_code_reloc_t) * relocs;\n};\n\n#define alloca_p gen_ctx->target_ctx->alloca_p\n#define block_arg_func_p gen_ctx->target_ctx->block_arg_func_p\n#define leaf_p gen_ctx->target_ctx->leaf_p\n#define short_bb_branch_p gen_ctx->target_ctx->short_bb_branch_p\n#define small_aggregate_save_area gen_ctx->target_ctx->small_aggregate_save_area\n#define temp_jump gen_ctx->target_ctx->temp_jump\n#define temp_jump_replacement gen_ctx->target_ctx->temp_jump_replacement\n#define pattern_indexes gen_ctx->target_ctx->pattern_indexes\n#define insn_pattern_info gen_ctx->target_ctx->insn_pattern_info\n#define result_code gen_ctx->target_ctx->result_code\n#define label_refs gen_ctx->target_ctx->label_refs\n#define abs_address_locs gen_ctx->target_ctx->abs_address_locs\n#define relocs gen_ctx->target_ctx->relocs\n\nstatic MIR_disp_t target_get_stack_slot_offset (gen_ctx_t gen_ctx, MIR_type_t type MIR_UNUSED,\n                                                MIR_reg_t slot) {\n  /* slot is 0, 1, ... */\n  size_t offset = curr_func_item->u.func->vararg_p || block_arg_func_p ? 32 : 16;\n\n  return ((MIR_disp_t) slot * 8 + offset);\n}\n\nstatic int target_valid_mem_offset_p (gen_ctx_t gen_ctx MIR_UNUSED, MIR_type_t type,\n                                      MIR_disp_t offset) {\n  int scale;\n  switch (type) {\n  case MIR_T_I8:\n  case MIR_T_U8: scale = 1; break;\n  case MIR_T_I16:\n  case MIR_T_U16: scale = 2; break;\n  case MIR_T_I32:\n  case MIR_T_U32:\n#if MIR_PTR32\n  case MIR_T_P:\n#endif\n  case MIR_T_F: scale = 4; break;\n  case MIR_T_LD: scale = 16; break;\n  default: scale = 8; break;\n  }\n  return offset >= 0 && offset % scale == 0 && offset / scale < (1 << 12);\n}\n\nstatic void target_machinize (gen_ctx_t gen_ctx) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  MIR_func_t func;\n  MIR_type_t type, mem_type, res_type;\n  MIR_insn_code_t code, new_insn_code;\n  MIR_insn_t insn, next_insn, new_insn, anchor;\n  MIR_var_t var;\n  MIR_reg_t ret_reg, arg_reg;\n  MIR_op_t ret_reg_op, arg_reg_op, mem_op, temp_op;\n  size_t i, int_arg_num, fp_arg_num, mem_size, qwords;\n\n  assert (curr_func_item->item_type == MIR_func_item);\n  func = curr_func_item->u.func;\n  block_arg_func_p = FALSE;\n  anchor = DLIST_HEAD (MIR_insn_t, func->insns);\n  small_aggregate_save_area = 0;\n  for (i = int_arg_num = fp_arg_num = mem_size = 0; i < func->nargs; i++) {\n    /* Argument extensions is already done in simplify */\n    /* Prologue: generate arg_var = hard_reg|stack mem|stack addr ... */\n    var = VARR_GET (MIR_var_t, func->vars, i);\n    type = var.type;\n    if (type == MIR_T_RBLK && i == 0) { /* hidden arg */\n      arg_reg_op = _MIR_new_var_op (ctx, R8_HARD_REG);\n      gen_mov (gen_ctx, anchor, MIR_MOV, _MIR_new_var_op (ctx, i + MAX_HARD_REG + 1), arg_reg_op);\n      continue;\n    } else if (MIR_blk_type_p (type) && (qwords = (var.size + 7) / 8) <= 2) {\n      if (int_arg_num + qwords <= 8) {\n        small_aggregate_save_area += qwords * 8;\n        new_insn = MIR_new_insn (ctx, MIR_SUB, _MIR_new_var_op (ctx, i + MAX_HARD_REG + 1),\n                                 _MIR_new_var_op (ctx, FP_HARD_REG),\n                                 MIR_new_int_op (ctx, small_aggregate_save_area));\n        gen_add_insn_before (gen_ctx, anchor, new_insn);\n        if (qwords == 0) continue;\n        gen_mov (gen_ctx, anchor, MIR_MOV,\n                 _MIR_new_var_mem_op (ctx, MIR_T_I64, 0, i + MAX_HARD_REG + 1, MIR_NON_VAR, 1),\n                 _MIR_new_var_op (ctx, int_arg_num));\n        if (qwords == 2)\n          gen_mov (gen_ctx, anchor, MIR_MOV,\n                   _MIR_new_var_mem_op (ctx, MIR_T_I64, 8, i + MAX_HARD_REG + 1, MIR_NON_VAR, 1),\n                   _MIR_new_var_op (ctx, int_arg_num + 1));\n      } else { /* pass on stack w/o address: */\n        if (!block_arg_func_p) {\n          block_arg_func_p = TRUE;\n          gen_mov (gen_ctx, anchor, MIR_MOV, _MIR_new_var_op (ctx, R8_HARD_REG),\n                   _MIR_new_var_mem_op (ctx, MIR_T_I64, 16, FP_HARD_REG, MIR_NON_VAR, 1));\n        }\n        gen_add_insn_before (gen_ctx, anchor,\n                             MIR_new_insn (ctx, MIR_ADD,\n                                           _MIR_new_var_op (ctx, i + MAX_HARD_REG + 1),\n                                           _MIR_new_var_op (ctx, R8_HARD_REG),\n                                           MIR_new_int_op (ctx, mem_size)));\n        mem_size += qwords * 8;\n      }\n      int_arg_num += qwords;\n      continue;\n    }\n    arg_reg = get_arg_reg (type, &int_arg_num, &fp_arg_num, &new_insn_code);\n    if (arg_reg != MIR_NON_VAR) {\n      arg_reg_op = _MIR_new_var_op (ctx, arg_reg);\n      gen_mov (gen_ctx, anchor, new_insn_code, _MIR_new_var_op (ctx, i + MAX_HARD_REG + 1),\n               arg_reg_op);\n    } else {\n      /* arg is on the stack */\n      if (!block_arg_func_p) {\n        block_arg_func_p = TRUE;\n        gen_mov (gen_ctx, anchor, MIR_MOV, _MIR_new_var_op (ctx, R8_HARD_REG),\n                 _MIR_new_var_mem_op (ctx, MIR_T_I64, 16, FP_HARD_REG, MIR_NON_VAR, 1));\n      }\n      mem_type = type == MIR_T_F || type == MIR_T_D || type == MIR_T_LD ? type : MIR_T_I64;\n      if (type == MIR_T_LD) mem_size = (mem_size + 15) / 16 * 16;\n      new_insn_code = (type == MIR_T_F    ? MIR_FMOV\n                       : type == MIR_T_D  ? MIR_DMOV\n                       : type == MIR_T_LD ? MIR_LDMOV\n                                          : MIR_MOV);\n      mem_op = new_hard_reg_mem_op (gen_ctx, anchor, mem_type, mem_size, R8_HARD_REG);\n      gen_mov (gen_ctx, anchor, new_insn_code, _MIR_new_var_op (ctx, i + MAX_HARD_REG + 1), mem_op);\n      mem_size += type == MIR_T_LD ? 16 : 8;\n    }\n  }\n  alloca_p = FALSE;\n  leaf_p = TRUE;\n  for (insn = DLIST_HEAD (MIR_insn_t, func->insns); insn != NULL; insn = next_insn) {\n    MIR_item_t proto_item, func_import_item;\n    int nargs;\n\n    next_insn = DLIST_NEXT (MIR_insn_t, insn);\n    code = insn->code;\n    if (code == MIR_LDBEQ || code == MIR_LDBNE || code == MIR_LDBLT || code == MIR_LDBGE\n        || code == MIR_LDBGT || code == MIR_LDBLE) {\n      temp_op = _MIR_new_var_op (ctx, gen_new_temp_reg (gen_ctx, MIR_T_I64, func));\n      code = (code == MIR_LDBEQ   ? MIR_LDEQ\n              : code == MIR_LDBNE ? MIR_LDNE\n              : code == MIR_LDBLT ? MIR_LDLT\n              : code == MIR_LDBGE ? MIR_LDGE\n              : code == MIR_LDBGT ? MIR_LDGT\n                                  : MIR_LDLE);\n      new_insn = MIR_new_insn (ctx, code, temp_op, insn->ops[1], insn->ops[2]);\n      gen_add_insn_before (gen_ctx, insn, new_insn);\n      next_insn = MIR_new_insn (ctx, MIR_BT, insn->ops[0], temp_op);\n      gen_add_insn_after (gen_ctx, new_insn, next_insn);\n      gen_delete_insn (gen_ctx, insn);\n      insn = new_insn;\n    }\n    if ((nargs = get_builtin (gen_ctx, code, &proto_item, &func_import_item)) > 0) {\n      if (code == MIR_VA_ARG || code == MIR_VA_BLOCK_ARG) {\n        /* Use a builtin func call:\n           mov func_reg, func ref; [mov reg3, type;] call proto, func_reg, res_reg, va_reg,\n           reg3 */\n        MIR_op_t ops[6], func_reg_op, reg_op3;\n        MIR_op_t res_reg_op = insn->ops[0], va_reg_op = insn->ops[1], op3 = insn->ops[2];\n\n        assert (res_reg_op.mode == MIR_OP_VAR && va_reg_op.mode == MIR_OP_VAR\n                && op3.mode == (code == MIR_VA_ARG ? MIR_OP_VAR_MEM : MIR_OP_VAR));\n        func_reg_op = _MIR_new_var_op (ctx, gen_new_temp_reg (gen_ctx, MIR_T_I64, func));\n        reg_op3 = _MIR_new_var_op (ctx, gen_new_temp_reg (gen_ctx, MIR_T_I64, func));\n        next_insn = new_insn\n          = MIR_new_insn (ctx, MIR_MOV, func_reg_op, MIR_new_ref_op (ctx, func_import_item));\n        gen_add_insn_before (gen_ctx, insn, new_insn);\n        if (code == MIR_VA_ARG) {\n          new_insn = MIR_new_insn (ctx, MIR_MOV, reg_op3,\n                                   MIR_new_int_op (ctx, (int64_t) op3.u.var_mem.type));\n          op3 = reg_op3;\n          gen_add_insn_before (gen_ctx, insn, new_insn);\n        }\n        ops[0] = MIR_new_ref_op (ctx, proto_item);\n        ops[1] = func_reg_op;\n        ops[2] = res_reg_op;\n        ops[3] = va_reg_op;\n        ops[4] = op3;\n        if (code == MIR_VA_BLOCK_ARG) ops[5] = insn->ops[3];\n        new_insn = MIR_new_insn_arr (ctx, MIR_CALL, code == MIR_VA_ARG ? 5 : 6, ops);\n        gen_add_insn_before (gen_ctx, insn, new_insn);\n        gen_delete_insn (gen_ctx, insn);\n      } else { /* Use builtin: mov freg, func ref; call proto, freg, res_reg, op_reg[, op_reg2] */\n        MIR_op_t freg_op, res_reg_op = insn->ops[0], op_reg_op = insn->ops[1], ops[5];\n\n        assert (res_reg_op.mode == MIR_OP_VAR && op_reg_op.mode == MIR_OP_VAR);\n        freg_op = _MIR_new_var_op (ctx, gen_new_temp_reg (gen_ctx, MIR_T_I64, func));\n        next_insn = new_insn\n          = MIR_new_insn (ctx, MIR_MOV, freg_op, MIR_new_ref_op (ctx, func_import_item));\n        gen_add_insn_before (gen_ctx, insn, new_insn);\n        ops[0] = MIR_new_ref_op (ctx, proto_item);\n        ops[1] = freg_op;\n        ops[2] = res_reg_op;\n        ops[3] = op_reg_op;\n        if (nargs == 2) ops[4] = insn->ops[2];\n        new_insn = MIR_new_insn_arr (ctx, MIR_CALL, nargs + 3, ops);\n        gen_add_insn_before (gen_ctx, insn, new_insn);\n        gen_delete_insn (gen_ctx, insn);\n      }\n    } else if (code == MIR_VA_START) {\n#if !defined(__APPLE__)\n      MIR_op_t treg_op = _MIR_new_var_op (ctx, gen_new_temp_reg (gen_ctx, MIR_T_I64, func));\n#endif\n      MIR_op_t prev_sp_op = _MIR_new_var_op (ctx, gen_new_temp_reg (gen_ctx, MIR_T_I64, func));\n      MIR_op_t va_op = insn->ops[0];\n      MIR_reg_t va_reg;\n      int gp_offset, fp_offset;\n\n      assert (func->vararg_p && va_op.mode == MIR_OP_VAR);\n      gp_offset = (int_arg_num >= 8 ? 0 : 8 * int_arg_num - 64);\n      fp_offset = (fp_arg_num >= 8 ? 0 : 16 * fp_arg_num - 128);\n      va_reg = va_op.u.var;\n      /* Insns can be not simplified as soon as they match a machine insn.  */\n#if !defined(__APPLE__)\n      /* mem32[va_reg].__gr_offset = gp_offset; mem32[va_reg].__vr_offset = fp_offset */\n      gen_mov (gen_ctx, insn, MIR_MOV, treg_op, MIR_new_int_op (ctx, gp_offset));\n      gen_mov (gen_ctx, insn, MIR_MOV,\n               _MIR_new_var_mem_op (ctx, MIR_T_U32, 24, va_reg, MIR_NON_VAR, 1), treg_op);\n      gen_mov (gen_ctx, insn, MIR_MOV, treg_op, MIR_new_int_op (ctx, fp_offset));\n      gen_mov (gen_ctx, insn, MIR_MOV,\n               _MIR_new_var_mem_op (ctx, MIR_T_U32, 28, va_reg, MIR_NON_VAR, 1), treg_op);\n#endif\n      /* __stack: prev_sp = mem64[fp + 16] */\n      gen_mov (gen_ctx, insn, MIR_MOV, prev_sp_op,\n               _MIR_new_var_mem_op (ctx, MIR_T_I64, 16, FP_HARD_REG, MIR_NON_VAR, 1));\n#if defined(__APPLE__)\n      gen_mov (gen_ctx, insn, MIR_MOV,\n               _MIR_new_var_mem_op (ctx, MIR_T_I64, 0, va_reg, MIR_NON_VAR, 1), prev_sp_op);\n#else\n      /* mem64[va_reg].__stack = prev_sp + mem_size */\n      new_insn = MIR_new_insn (ctx, MIR_ADD, treg_op, prev_sp_op, MIR_new_int_op (ctx, mem_size));\n      gen_add_insn_before (gen_ctx, insn, new_insn);\n      gen_mov (gen_ctx, insn, MIR_MOV,\n               _MIR_new_var_mem_op (ctx, MIR_T_I64, 0, va_reg, MIR_NON_VAR, 1), treg_op);\n      /* __gr_top: mem64[va_reg].__gr_top = prev_sp */\n      gen_mov (gen_ctx, insn, MIR_MOV,\n               _MIR_new_var_mem_op (ctx, MIR_T_I64, 8, va_reg, MIR_NON_VAR, 1), prev_sp_op);\n      /* __vr_top: treg = prev_sp - int_reg_save_area; mem64[va_reg].__vr_top = treg */\n      new_insn = MIR_new_insn (ctx, MIR_SUB, treg_op, prev_sp_op,\n                               MIR_new_int_op (ctx, int_reg_save_area_size));\n      gen_add_insn_before (gen_ctx, insn, new_insn);\n      gen_mov (gen_ctx, insn, MIR_MOV,\n               _MIR_new_var_mem_op (ctx, MIR_T_I64, 16, va_reg, MIR_NON_VAR, 1), treg_op);\n#endif\n      gen_delete_insn (gen_ctx, insn);\n    } else if (code == MIR_VA_END) { /* do nothing */\n      gen_delete_insn (gen_ctx, insn);\n    } else if (MIR_call_code_p (code)) {\n      machinize_call (gen_ctx, insn);\n      leaf_p = FALSE;\n    } else if (code == MIR_ALLOCA) {\n      alloca_p = TRUE;\n    } else if (code == MIR_FBLT) { /* don't use blt/ble for correct nan processing: */\n      SWAP (insn->ops[1], insn->ops[2], temp_op);\n      insn->code = MIR_FBGT;\n    } else if (code == MIR_FBLE) {\n      SWAP (insn->ops[1], insn->ops[2], temp_op);\n      insn->code = MIR_FBGE;\n    } else if (code == MIR_DBLT) {\n      SWAP (insn->ops[1], insn->ops[2], temp_op);\n      insn->code = MIR_DBGT;\n    } else if (code == MIR_DBLE) {\n      SWAP (insn->ops[1], insn->ops[2], temp_op);\n      insn->code = MIR_DBGE;\n    } else if (code == MIR_RET) {\n      /* In simplify we already transformed code for one return insn\n         and added extension insn (if any).  */\n      uint32_t n_iregs = 0, n_vregs = 0;\n\n      assert (func->nres == MIR_insn_nops (ctx, insn));\n      for (i = 0; i < func->nres; i++) {\n        assert (insn->ops[i].mode == MIR_OP_VAR);\n        res_type = func->res_types[i];\n        if ((res_type == MIR_T_F || res_type == MIR_T_D || res_type == MIR_T_LD) && n_vregs < 8) {\n          new_insn_code = res_type == MIR_T_F   ? MIR_FMOV\n                          : res_type == MIR_T_D ? MIR_DMOV\n                                                : MIR_LDMOV;\n          ret_reg = V0_HARD_REG + n_vregs++;\n        } else if (n_iregs < 8) {\n          new_insn_code = MIR_MOV;\n          ret_reg = R0_HARD_REG + n_iregs++;\n        } else {\n          (*MIR_get_error_func (ctx)) (MIR_ret_error,\n                                       \"aarch64 can not handle this combination of return values\");\n        }\n        ret_reg_op = _MIR_new_var_op (ctx, ret_reg);\n        gen_mov (gen_ctx, insn, new_insn_code, ret_reg_op, insn->ops[i]);\n        insn->ops[i] = ret_reg_op;\n      }\n    }\n  }\n}\n\n#if !defined(__APPLE__)\nstatic void isave (gen_ctx_t gen_ctx, MIR_insn_t anchor, int disp, MIR_reg_t base,\n                   MIR_reg_t hard_reg) {\n  gen_mov (gen_ctx, anchor, MIR_MOV, new_hard_reg_mem_op (gen_ctx, anchor, MIR_T_I64, disp, base),\n           _MIR_new_var_op (gen_ctx->ctx, hard_reg));\n}\n\nstatic void fsave (gen_ctx_t gen_ctx, MIR_insn_t anchor, int disp, MIR_reg_t base,\n                   MIR_reg_t hard_reg) {\n  gen_mov (gen_ctx, anchor, MIR_LDMOV, new_hard_reg_mem_op (gen_ctx, anchor, MIR_T_LD, disp, base),\n           _MIR_new_var_op (gen_ctx->ctx, hard_reg));\n}\n#endif\n\nstatic void target_make_prolog_epilog (gen_ctx_t gen_ctx, bitmap_t used_hard_regs,\n                                       size_t stack_slots_num) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  MIR_func_t func;\n  MIR_insn_t anchor, new_insn;\n  MIR_op_t sp_reg_op, fp_reg_op, treg_op, treg_op2;\n  int save_prev_stack_p;\n  size_t i, offset, frame_size, frame_size_after_saved_regs, saved_iregs_num, saved_fregs_num;\n\n  assert (curr_func_item->item_type == MIR_func_item);\n  func = curr_func_item->u.func;\n  for (i = saved_iregs_num = saved_fregs_num = 0; i <= MAX_HARD_REG; i++)\n    if (!target_call_used_hard_reg_p (i, MIR_T_UNDEF) && bitmap_bit_p (used_hard_regs, i)) {\n      if (i < V0_HARD_REG)\n        saved_iregs_num++;\n      else\n        saved_fregs_num++;\n    }\n  if (leaf_p && !alloca_p && saved_iregs_num == 0 && saved_fregs_num == 0 && !func->vararg_p\n      && stack_slots_num == 0 && !block_arg_func_p && small_aggregate_save_area == 0)\n    return;\n  sp_reg_op = _MIR_new_var_op (ctx, SP_HARD_REG);\n  fp_reg_op = _MIR_new_var_op (ctx, FP_HARD_REG);\n  /* Prologue: */\n  anchor = DLIST_HEAD (MIR_insn_t, func->insns);\n#if defined(__APPLE__)\n  frame_size = 0;\n#else\n  frame_size = func->vararg_p ? reg_save_area_size : 0;\n#endif\n  for (i = 0; i <= MAX_HARD_REG; i++)\n    if (!target_call_used_hard_reg_p (i, MIR_T_UNDEF) && bitmap_bit_p (used_hard_regs, i)) {\n      if (i < V0_HARD_REG) {\n        frame_size += 8;\n      } else {\n        if (frame_size % 16 != 0) frame_size = (frame_size + 15) / 16 * 16;\n        frame_size += 16;\n      }\n    }\n  if (frame_size % 16 != 0) frame_size = (frame_size + 15) / 16 * 16;\n  frame_size_after_saved_regs = frame_size;\n  frame_size += stack_slots_num * 8;\n  if (frame_size % 16 != 0) frame_size = (frame_size + 15) / 16 * 16;\n  save_prev_stack_p = func->vararg_p || block_arg_func_p;\n  treg_op = _MIR_new_var_op (ctx, R9_HARD_REG);\n  if (save_prev_stack_p) { /* prev stack pointer */\n    gen_mov (gen_ctx, anchor, MIR_MOV, treg_op, sp_reg_op);\n    frame_size += 16;\n  }\n  frame_size += 16; /* lr/fp */\n  treg_op2 = _MIR_new_var_op (ctx, R10_HARD_REG);\n  if (frame_size < (1 << 12)) {\n    new_insn = MIR_new_insn (ctx, MIR_SUB, sp_reg_op, sp_reg_op, MIR_new_int_op (ctx, frame_size));\n  } else {\n    new_insn = MIR_new_insn (ctx, MIR_MOV, treg_op2, MIR_new_int_op (ctx, frame_size));\n    gen_add_insn_before (gen_ctx, anchor, new_insn); /* t = frame_size */\n    new_insn = MIR_new_insn (ctx, MIR_SUB, sp_reg_op, sp_reg_op, treg_op2);\n  }\n  gen_add_insn_before (gen_ctx, anchor, new_insn); /* sp = sp - (frame_size|t) */\n  if (save_prev_stack_p)\n    gen_mov (gen_ctx, anchor, MIR_MOV,\n             _MIR_new_var_mem_op (ctx, MIR_T_I64, 16, SP_HARD_REG, MIR_NON_VAR, 1),\n             treg_op); /* mem[sp + 16] = treg */\n  if (!func->jret_p)\n    gen_mov (gen_ctx, anchor, MIR_MOV,\n             _MIR_new_var_mem_op (ctx, MIR_T_I64, 8, SP_HARD_REG, MIR_NON_VAR, 1),\n             _MIR_new_var_op (ctx, LINK_HARD_REG)); /* mem[sp + 8] = lr */\n  gen_mov (gen_ctx, anchor, MIR_MOV,\n           _MIR_new_var_mem_op (ctx, MIR_T_I64, 0, SP_HARD_REG, MIR_NON_VAR, 1),\n           _MIR_new_var_op (ctx, FP_HARD_REG));             /* mem[sp] = fp */\n  gen_mov (gen_ctx, anchor, MIR_MOV, fp_reg_op, sp_reg_op); /* fp = sp */\n#if !defined(__APPLE__)\n  if (func->vararg_p) {  // ??? saving only regs corresponding to ...\n    MIR_reg_t base = SP_HARD_REG;\n    int64_t start;\n\n    start = (int64_t) frame_size - reg_save_area_size;\n    if ((start + 184) >= (1 << 12)) {\n      new_insn = MIR_new_insn (ctx, MIR_MOV, treg_op, MIR_new_int_op (ctx, start));\n      gen_add_insn_before (gen_ctx, anchor, new_insn); /* t = frame_size - reg_save_area_size */\n      start = 0;\n      base = R9_HARD_REG;\n    }\n    fsave (gen_ctx, anchor, start, base, V0_HARD_REG);\n    fsave (gen_ctx, anchor, start + 16, base, V1_HARD_REG);\n    fsave (gen_ctx, anchor, start + 32, base, V2_HARD_REG);\n    fsave (gen_ctx, anchor, start + 48, base, V3_HARD_REG);\n    fsave (gen_ctx, anchor, start + 64, base, V4_HARD_REG);\n    fsave (gen_ctx, anchor, start + 80, base, V5_HARD_REG);\n    fsave (gen_ctx, anchor, start + 96, base, V6_HARD_REG);\n    fsave (gen_ctx, anchor, start + 112, base, V7_HARD_REG);\n    isave (gen_ctx, anchor, start + 128, base, R0_HARD_REG);\n    isave (gen_ctx, anchor, start + 136, base, R1_HARD_REG);\n    isave (gen_ctx, anchor, start + 144, base, R2_HARD_REG);\n    isave (gen_ctx, anchor, start + 152, base, R3_HARD_REG);\n    isave (gen_ctx, anchor, start + 160, base, R4_HARD_REG);\n    isave (gen_ctx, anchor, start + 168, base, R5_HARD_REG);\n    isave (gen_ctx, anchor, start + 176, base, R6_HARD_REG);\n    isave (gen_ctx, anchor, start + 184, base, R7_HARD_REG);\n  }\n#endif\n  /* Saving callee saved hard registers: */\n  offset = frame_size - frame_size_after_saved_regs;\n  for (i = 0; i <= MAX_HARD_REG; i++)\n    if (!target_call_used_hard_reg_p (i, MIR_T_UNDEF) && bitmap_bit_p (used_hard_regs, i)) {\n      if (i < V0_HARD_REG) {\n        gen_mov (gen_ctx, anchor, MIR_MOV,\n                 new_hard_reg_mem_op (gen_ctx, anchor, MIR_T_I64, offset, FP_HARD_REG),\n                 _MIR_new_var_op (ctx, i));\n        offset += 8;\n      } else {\n        if (offset % 16 != 0) offset = (offset + 15) / 16 * 16;\n        new_insn = gen_mov (gen_ctx, anchor, MIR_LDMOV,\n                            new_hard_reg_mem_op (gen_ctx, anchor, MIR_T_LD, offset, FP_HARD_REG),\n                            _MIR_new_var_op (ctx, i));\n#if defined(__APPLE__)\n        /* MIR API can change insn code - change it back as we need to generate code to save all\n         * vreg. */\n        if (new_insn->code == MIR_DMOV) new_insn->code = MIR_LDMOV;\n#endif\n        offset += 16;\n      }\n    }\n  if (small_aggregate_save_area != 0) {  // ??? duplication with vararg saved regs\n    if (small_aggregate_save_area % 16 != 0)\n      small_aggregate_save_area = (small_aggregate_save_area + 15) / 16 * 16;\n    new_insn = MIR_new_insn (ctx, MIR_SUB, sp_reg_op, sp_reg_op,\n                             MIR_new_int_op (ctx, small_aggregate_save_area));\n    gen_add_insn_before (gen_ctx, anchor, new_insn); /* sp -= <small aggr save area size> */\n  }\n  /* Epilogue: */\n  for (anchor = DLIST_TAIL (MIR_insn_t, func->insns); anchor != NULL;\n       anchor = DLIST_PREV (MIR_insn_t, anchor))\n    if (anchor->code == MIR_RET || anchor->code == MIR_JRET) break;\n  if (anchor == NULL) return;\n  /* Restoring hard registers: */\n  offset = frame_size - frame_size_after_saved_regs;\n  for (i = 0; i <= MAX_HARD_REG; i++)\n    if (!target_call_used_hard_reg_p (i, MIR_T_UNDEF) && bitmap_bit_p (used_hard_regs, i)) {\n      if (i < V0_HARD_REG) {\n        gen_mov (gen_ctx, anchor, MIR_MOV, _MIR_new_var_op (ctx, i),\n                 new_hard_reg_mem_op (gen_ctx, anchor, MIR_T_I64, offset, FP_HARD_REG));\n        offset += 8;\n      } else {\n        if (offset % 16 != 0) offset = (offset + 15) / 16 * 16;\n        new_insn = gen_mov (gen_ctx, anchor, MIR_LDMOV, _MIR_new_var_op (ctx, i),\n                            new_hard_reg_mem_op (gen_ctx, anchor, MIR_T_LD, offset, FP_HARD_REG));\n#if defined(__APPLE__)\n        if (new_insn->code == MIR_DMOV) new_insn->code = MIR_LDMOV;\n#endif\n        offset += 16;\n      }\n    }\n  /* Restore lr, sp, fp */\n  if (!func->jret_p)\n    gen_mov (gen_ctx, anchor, MIR_MOV, _MIR_new_var_op (ctx, LINK_HARD_REG),\n             _MIR_new_var_mem_op (ctx, MIR_T_I64, 8, FP_HARD_REG, MIR_NON_VAR, 1));\n  gen_mov (gen_ctx, anchor, MIR_MOV, treg_op2, fp_reg_op); /* r10 = fp */\n  gen_mov (gen_ctx, anchor, MIR_MOV, fp_reg_op,\n           _MIR_new_var_mem_op (ctx, MIR_T_I64, 0, FP_HARD_REG, MIR_NON_VAR, 1));\n  if (frame_size < (1 << 12)) {\n    new_insn = MIR_new_insn (ctx, MIR_ADD, sp_reg_op, treg_op2, MIR_new_int_op (ctx, frame_size));\n  } else {\n    new_insn = MIR_new_insn (ctx, MIR_MOV, treg_op, MIR_new_int_op (ctx, frame_size));\n    gen_add_insn_before (gen_ctx, anchor, new_insn); /* t(r9) = frame_size */\n    new_insn = MIR_new_insn (ctx, MIR_ADD, sp_reg_op, treg_op2, treg_op);\n  }\n  gen_add_insn_before (gen_ctx, anchor, new_insn); /* sp = r10 + (frame_size|t) */\n}\n\nstruct pattern {\n  MIR_insn_code_t code;\n  /* Pattern elements:\n     blank - ignore\n     X - match everything\n     $ - finish successfully matching\n     r - register\n     h[0-63] - hard register with given number\n\n        memory with indexed reg offset:\n     m[0-3] - int (signed or unsigned) type memory of size 8,16,32,64-bits\n     ms[0-3] - signed int type memory of size 8,16,32,64-bits\n     mu[0-3] - unsigned int type memory of size 8,16,32,64-bits\n       option(field[13..15]) == 011 -- shifted reg (Rm=R31 means SP)\n       option == 010 (UXTW), 110 (SXTW), 111 (SXTX) -- extended reg (RM=R31 means ZR)\n       we use option == 111 only for non-index mem and 011 for indexed memory\n\n       memory with immediate offset:\n     M[0-3] - int (signed or unsigned) type memory of size 8,16,32,64-bits\n     Ms[0-3] - signed int type memory of size 8,16,32,64-bits\n     Mu[0-3] - unsigned int type memory of size 8,16,32,64-bits\n       zero extended scaled 12-bit offset (field[10..21])\n\n     N[0-2] - 2nd immediate (or reference) operand can be created by movn and n movk insns\n     Z[0-2] - 2nd immediate (or reference) operand can be created by movz and n movk insns\n     Z3 - any 2nd 64-bit immediate (or reference) operand\n\n     Zf - floating point 0.0\n     Zd - double floating point 0.0\n\n       memory with indexed reg offset:\n     mf - memory of float\n     md - memory of double\n     mld - memory of long double\n\n       memory with immediate offset:\n     Mf - memory of float\n     Md - memory of double\n     Mld - memory of long double\n     I -- immediate as 3th op for arithmetic insn (12-bit unsigned with possible 12-bit LSL)\n     Iu -- immediate for arithmetic insn roundup to 16\n     SR -- any immediate for right 64-bit shift (0-63)\n     Sr -- any immediate for right 32-bit shift (0-31)\n     SL -- any immediate for left 64-bit shift (0-63)\n     Sl -- any immediate for left 32-bit shift (0-31)\n     L - reference or label as the 1st or 2nd op which can be present by signed 26-bit pc offset\n     (in 4 insn bytes) l - label as the 1st op which can be present by signed 19-bit pc offset (in\n     4 insn bytes)\n\n     Remember we have no float or (long) double immediate at this stage. They are represented\n     by a reference to data item.  */\n\n  const char *pattern;\n  /* Replacement elements:\n     blank - ignore\n     ; - insn separation\n     hex:hex - opcode and its mask (the mask should include opcode, the mask defines bits\n                                    which can not be changed by other fields)\n     rd[0-2] - put n-th operand register into rd field [0..4]\n     rn[0-2] - put n-th operand register into rn field [5..9]\n     rm[0-2] - put n-th operand register into rm field [16..20]\n     ra[0-2] - put n-th operand register into ra field [10..14]\n     h(d,n,m)<one or two hex digits> - hardware register with given number in rd,rn,rm field\n     m = 1st or 2nd operand is (8-,16-,32-,64-bit) mem with base, scaled index\n     M = 1st or 2nd operand is (8-,16-,32-,64-bit) mem with base, scaled imm12 disp [10..21]\n     S - immr[16..21]  for right shift SR/Sr\n     SL, Sl - immr[16..21] and imms[10..15] for left shift SL/Sl\n\n     Z[0-3] -- n-th 16-bit immediate[5..20] from Z[0-3] and its shift [21..22]\n     N[0-3] -- n-th 16-bit immediate[5..20] from N[0-3] and its shift [21..22]\n     I -- arithmetic op 12-bit immediate [10..21] and its shift [22..23]\n     Iu -- arithmetic op immediate [10..21] got from roundup value to 16 and its shift [22..23]\n     L -- operand-label as 26-bit offset\n     l -- operand-label as 19-bit offset\n     T -- pc-relative address [5..23]\n     i<one or two hex digits> -- shift value in [10..15]\n     I<one or two hex digits> -- shift value in [16..21]\n  */\n  const char *replacement;\n};\n\n#define SUB_UBO MIR_INSN_BOUND\n#define SUB_UBNO (SUB_UBO + 1)\n#define MUL_BO (SUB_UBNO + 1)\n#define MUL_BNO (MUL_BO + 1)\n#define ARM_INSN_BOUND (MUL_BNO + 1)\n\nstatic const struct pattern patterns[] = {\n  {MIR_MOV, \"r h31\", \"91000000:fffffc00 rd0 hn1f\"}, /* mov Rd,SP */\n  {MIR_MOV, \"h31 r\", \"91000000:fffffc00 hd1f rn1\"}, /* mov SP,Rn */\n  {MIR_MOV, \"r r\", \"aa0003e0:ffe0ffe0 rd0 rm1\"},    /* mov Rd,Rm */\n\n  // ??? patterns to extend 32-bit index register\n  {MIR_MOV, \"r m3\", \"f8600800:ffe00c00 rd0 m\"}, /* ldr Rd,[Rn,Rm{,#3}] */\n  {MIR_MOV, \"m3 r\", \"f8200800:ffe00c00 rd1 m\"}, /* str Rd,[Rn,Rm{,#3}] */\n  {MIR_MOV, \"r M3\", \"f9400000:ffc00000 rd0 M\"}, /* ldr Rd,[Rn,{,#imm12}] */\n  {MIR_MOV, \"M3 r\", \"f9000000:ffc00000 rd1 M\"}, /* str Rd,[Rn,Rm{,#imm12}] */\n\n  {MIR_MOV, \"r mu2\", \"b8600800:ffe00c00 rd0 m\"}, /* ldr Wd,[Rn,Rm{,#2}] */\n  {MIR_MOV, \"m2 r\", \"b8200800:ffe00c00 rd1 m\"},  /* str Wd,[Rn,Rm{,#2}] */\n  {MIR_MOV, \"r Mu2\", \"b9400000:ffc00000 rd0 M\"}, /* ldr Wd,[Rn{,#imm12}] */\n  {MIR_MOV, \"M2 r\", \"b9000000:ffc00000 rd1 M\"},  /* str Wd,[Rn,Rm{,#imm12}] */\n\n  {MIR_MOV, \"r ms2\", \"b8a00800:ffe00c00 rd0 m\"}, /* ldrsw Rd,[Rn,Rm{,#2}] */\n  {MIR_MOV, \"r Ms2\", \"b9800000:ffc00000 rd0 M\"}, /* ldrsw Rd,[Rn{,#imm12}] */\n\n  {MIR_MOV, \"r mu1\", \"78600800:ffe00c00 rd0 m\"}, /* ldrh Wd,[Rn,Rm{,#1}] */\n  {MIR_MOV, \"m1 r\", \"78200800:ffe00c00 rd1 m\"},  /* strh Wd,[Rn,Rm{,#1}] */\n  {MIR_MOV, \"r Mu1\", \"79400000:ffc00000 rd0 M\"}, /* ldrh Wd,[Rn{,#imm12}] */\n  {MIR_MOV, \"M1 r\", \"79000000:ffc00000 rd1 M\"},  /* strh Wd,[Rn,Rm{,#imm12}] */\n\n  {MIR_MOV, \"r ms1\", \"78a00800:ffe00c00 rd0 m\"}, /* ldrsh Wd,[Rn,Rm{,#2}] */\n  {MIR_MOV, \"r Ms1\", \"79800000:ffc00000 rd0 M\"}, /* ldrsh Wd,[Rn{,#imm12}] */\n\n  {MIR_MOV, \"r mu0\", \"38600800:ffe00c00 rd0 m\"}, /* ldrb Wd,[Rn,Rm{,#1}] */\n  {MIR_MOV, \"m0 r\", \"38200800:ffe00c00 rd1 m\"},  /* strb Wd,[Rn,Rm{,#1}] */\n  {MIR_MOV, \"r Mu0\", \"39400000:ffc00000 rd0 M\"}, /* ldrb Wd,[Rn{,#imm12}] */\n  {MIR_MOV, \"M0 r\", \"39000000:ffc00000 rd1 M\"},  /* strb Wd,[Rn,Rm{,#imm12}] */\n\n  {MIR_MOV, \"r ms0\", \"38a00800:ffa00c00 rd0 m\"}, /* ldrsb Rd,[Rn,Rm{,#1}] */\n  {MIR_MOV, \"r Ms0\", \"39800000:ffc00000 rd0 M\"}, /* ldrsb Rd,[Rn{,#imm12}] */\n\n  {MIR_MOV, \"r Z0\", \"d2800000:ff800000 rd0 Z0\"}, /* movz Rd, imm */\n  {MIR_MOV, \"r N0\", \"92800000:ff800000 rd0 N0\"}, /* movn Rd, imm */\n  /* movn Rd, imm0, sh0; movk Rd, imm1, sh1: */\n  {MIR_MOV, \"r Z1\", \"d2800000:ff800000 rd0 Z0; f2800000:ff800000 rd0 Z1\"},\n  /* movn imm0, sh0; movk Rd, imm1, sh1:  */\n  {MIR_MOV, \"r N1\", \"92800000:ff800000 rd0 N0; f2800000:ff800000 rd0 N1\"},\n  /* movz Rd, imm0, sh0; movk Rd, imm1, sh1; movk Rd, imm3, sh3: */\n  {MIR_MOV, \"r Z2\", \"d2800000:ff800000 rd0 Z0; f2800000:ff800000 rd0 Z1; f2800000:ff800000 rd0 Z2\"},\n  /* movn Rd, imm0, sh0; movk Rd, imm1, sh1; movk Rd, imm3, sh3: */\n  {MIR_MOV, \"r N2\", \"92800000:ff800000 rd0 N0; f2800000:ff800000 rd0 N1; f2800000:ff800000 rd0 N2\"},\n  /* movz Rd, imm0, sh0; movk Rd, imm1, sh1; movk Rd, imm2, sh2; movk Rd, imm3, sh3: */\n  {MIR_MOV, \"r Z3\",\n   \"d2800000:ff800000 rd0 Z0; f2800000:ff800000 rd0 Z1; f2800000:ff800000 rd0 Z2;\"\n   \"f2800000:ff800000 rd0 Z3\"},\n\n  {MIR_FMOV, \"r r\", \"1e204000:fffffc00 vd0 vn1\"}, /* fmov Sd,Sn */\n  {MIR_FMOV, \"r mf\", \"bc600800:ff600c00 vd0 m\"},  /* ldr Sd,[Rn,Rm{,#2}] */\n  {MIR_FMOV, \"mf r\", \"bc200800:ff600c00 vd1 m\"},  /* str Sd,[Rn,Rm{,#2}] */\n  {MIR_FMOV, \"r Mf\", \"bd400000:ffc00000 vd0 M\"},  /* ldr Sd,[Rn,{,#imm12}] */\n  {MIR_FMOV, \"Mf r\", \"bd000000:ffc00000 vd1 M\"},  /* str Sd,[Rn,Rm{,#imm12}] */\n\n  {MIR_DMOV, \"r r\", \"1e604000:fffffc00 vd0 vn1\"}, /* fmov Dd,Dn */\n  {MIR_DMOV, \"r md\", \"fc600800:ff600c00 vd0 m\"},  /* ldr Dd,[Rn,Rm{,#3}] */\n  {MIR_DMOV, \"md r\", \"fc200800:ff600c00 vd1 m\"},  /* str Dd,[Rn,Rm{,#3}] */\n  {MIR_DMOV, \"r Md\", \"fd400000:ffc00000 vd0 M\"},  /* ldr Dd,[Rn,{,#imm12}] */\n  {MIR_DMOV, \"Md r\", \"fd000000:ffc00000 vd1 M\"},  /* str Dd,[Rn,Rm{,#imm12}] */\n\n  {MIR_LDMOV, \"r r\", \"4ea01c00:ffe0fc00 vd0 vm1 vn1\"}, /* orr Qd.16b,Qm.16b,Qn.16b */\n  {MIR_LDMOV, \"r mld\", \"3ce00800:ffe00c00 vd0 m\"},     /* ldr Qd,[Rn,Rm{,#4}] */\n  {MIR_LDMOV, \"mld r\", \"3ca00800:ffe00c00 vd1 m\"},     /* str Qd,[Rn,Rm{,#4}] */\n  {MIR_LDMOV, \"r Mld\", \"3dc00000:ffc00000 vd0 M\"},     /* ldr Qd,[Rn,{,#imm12}] */\n  {MIR_LDMOV, \"Mld r\", \"3d800000:ffc00000 vd1 M\"},     /* str Qd,[Rn,Rm{,#imm12}] */\n\n  {MIR_EXT8, \"r r\", \"93401c00:fffffc00 rd0 rn1\"},  /* sxtb rd, wn */\n  {MIR_EXT16, \"r r\", \"93403c00:fffffc00 rd0 rn1\"}, /* sxth rd, wn */\n  {MIR_EXT32, \"r r\", \"93407c00:fffffc00 rd0 rn1\"}, /* sxtw rd, wn */\n\n  {MIR_UEXT8, \"r r\", \"53001c00:fffffc00 rd0 rn1\"},  /* uxtb wd, wn */\n  {MIR_UEXT16, \"r r\", \"53003c00:fffffc00 rd0 rn1\"}, /* uxth wd, wn */\n  {MIR_UEXT32, \"r r\", \"2a0003e0:7fe0ffe0 rd0 rm1\"}, /* mov wd, wm */\n\n#define IOP(icode, rop, iop, rops, iops)                                         \\\n  {icode, \"r r r\", rop \":ffe0fc00 rd0 rn1 rm2\"},       /* extended op Rd,Rn,Rm*/ \\\n    {icode, \"r r I\", iop \":ff000000 rd0 rn1 I\"},       /* op Rd,Rn,I,shift */    \\\n    {icode##S, \"r r r\", rops \":ff200000 rd0 rn1 rm2\"}, /* op Wd,Wn,Wm*/          \\\n    {icode##S, \"r r I\", iops \":ff000000 rd0 rn1 I\"},   /* op Wd,Wn,I,shift */\n\n  IOP (MIR_ADD, \"8b206000\", \"91000000\", \"0b000000\", \"11000000\")\n    IOP (MIR_ADDO, \"ab206000\", \"b1000000\", \"2b000000\", \"31000000\")\n\n      {MIR_FADD, \"r r r\", \"1e202800:ffe0fc00 vd0 vn1 vm2\"}, /* fadd Sd,Sn,Sm*/\n  {MIR_DADD, \"r r r\", \"1e602800:ffe0fc00 vd0 vn1 vm2\"},     /* fadd Dd,Dn,Dm*/\n  // ldadd is implemented through builtin\n\n  IOP (MIR_SUB, \"cb206000\", \"d1000000\", \"4b000000\", \"51000000\")\n    IOP (MIR_SUBO, \"eb206000\", \"f1000000\", \"6b000000\", \"71000000\")\n\n      {MIR_FSUB, \"r r r\", \"1e203800:ffe0fc00 vd0 vn1 vm2\"}, /* fsub Sd,Sn,Sm*/\n  {MIR_DSUB, \"r r r\", \"1e603800:ffe0fc00 vd0 vn1 vm2\"},     /* fsub Dd,Dn,Dm*/\n  // ldsub is implemented through builtin\n\n  {MIR_MUL, \"r r r\", \"9b007c00:ffe0fc00 rd0 rn1 rm2\"},  /* mul Rd,Rn,Rm*/\n  {MIR_MULS, \"r r r\", \"1b007c00:ffe0fc00 rd0 rn1 rm2\"}, /* mul Wd,Wn,Wm*/\n  {MIR_FMUL, \"r r r\", \"1e200800:ffe0fc00 vd0 vn1 vm2\"}, /* fmul Sd,Sn,Sm*/\n  {MIR_DMUL, \"r r r\", \"1e600800:ffe0fc00 vd0 vn1 vm2\"}, /* fmul Dd,Dn,Dm*/\n  // ldmul is implemented through builtin\n\n  {MIR_DIV, \"r r r\", \"9ac00c00:ffe0fc00 rd0 rn1 rm2\"},   /* sdiv Rd,Rn,Rm*/\n  {MIR_DIVS, \"r r r\", \"1ac00c00:ffe0fc00 rd0 rn1 rm2\"},  /* sdiv Wd,Wn,Wm*/\n  {MIR_UDIV, \"r r r\", \"9ac00800:ffe0fc00 rd0 rn1 rm2\"},  /* udiv Rd,Rn,Rm*/\n  {MIR_UDIVS, \"r r r\", \"1ac00800:ffe0fc00 rd0 rn1 rm2\"}, /* udiv Wd,Wn,Wm*/\n  {MIR_FDIV, \"r r r\", \"1e201800:ffe0fc00 vd0 vn1 vm2\"},  /* fdiv Sd,Sn,Sm*/\n  {MIR_DDIV, \"r r r\", \"1e601800:ffe0fc00 vd0 vn1 vm2\"},  /* fmul Dd,Dn,Dm*/\n  // lddiv is implemented through builtin\n\n  /* sdiv r8,Rn,Rm;msub Rd,r8,Rm,Rn: */\n  {MIR_MOD, \"r r r\", \"9ac00c00:ffe0fc00 hd8 rn1 rm2;9b008000:ffe08000 rd0 hm8 rn2 ra1\"},\n  /* sdiv r8,Wn,Wm;msub Wd,r8,Wm,Wn: */\n  {MIR_MODS, \"r r r\", \"1ac00c00:ffe0fc00 hd8 rn1 rm2;1b008000:ffe08000 rd0 hm8 rn2 ra1\"},\n  /* udiv r8,Rn,Rm;msub Rd,r8,Rm,Rn: */\n  {MIR_UMOD, \"r r r\", \"9ac00800:ffe0fc00 hd8 rn1 rm2;9b008000:ffe08000 rd0 hm8 rn2 ra1\"},\n  /* udiv r8,Wn,Wm;msub Wd,r8,Wm,Wn: */\n  {MIR_UMODS, \"r r r\", \"1ac00800:ffe0fc00 hd8 rn1 rm2;1b008000:ffe08000 rd0 hm8 rn2 ra1\"},\n\n#define CMPR \"eb00001f:ff20001f rn1 rm2\"\n#define CMPI \"f100001f:ff00001f rn1 I\"\n#define SCMPR \"6b00001f:ff20001f rn1 rm2\"\n#define SCMPI \"7100001f:ff00001f rn1 I\"\n#define FCMP \"1e202010:ffe0fc1f vn1 vm2\"\n#define DCMP \"1e602010:ffe0fc1f vn1 vm2\"\n\n#define REQ \"9a9f17e0:ffffffe0 rd0\"\n#define REQS \"1a9f17e0:ffffffe0 rd0\"\n  // ??? add extended reg cmp insns:\n  // all ld insn are changed to builtins\n  /* cmp Rn,Rm; cset Rd,eq */\n  {MIR_EQ, \"r r r\", CMPR \"; \" REQ},\n  /* cmp Rn,I,shift ; cset Wd,eq */\n  {MIR_EQ, \"r r I\", CMPI \"; \" REQ},\n  /* cmp Wn,Wm;cset Rd,eq */\n  {MIR_EQS, \"r r r\", SCMPR \"; \" REQS},\n  /* cmp Wn,I,shift; cset Wd,eq */\n  {MIR_EQS, \"r r I\", SCMPI \"; \" REQS},\n  /* fcmpe Sn,Sm; cset Rd,mi */\n  {MIR_FEQ, \"r r r\", FCMP \"; \" REQ},\n  /* fcmpe Dn,Dm; cset Rd,mi */\n  {MIR_DEQ, \"r r r\", DCMP \"; \" REQ},\n  /* fcmpe Sn,0.0; cset Rd,mi */\n  {MIR_FEQ, \"r r Zf\", \"1e202018:fffffc1f vn1 vm2; \" REQ},\n  /* fcmpe Dn,0.0; cset Rd,mi */\n  {MIR_DEQ, \"r r Zd\", \"1e602018:fffffc1f vn1 vm2; \" REQ},\n\n#define RNE \"9a9f07e0:ffffffe0 rd0\"\n#define RNES \"1a9f07e0:ffffffe0 rd0\"\n  /* cmp Rn,Rm; cset Rd,ne */\n  {MIR_NE, \"r r r\", CMPR \"; \" RNE},\n  /* cmp Rn,I,shift ; cset Wd,ne */\n  {MIR_NE, \"r r I\", CMPI \"; \" RNE},\n  /* cmp Wn,Wm;cset Rd,ne */\n  {MIR_NES, \"r r r\", SCMPR \"; \" RNES},\n  /* cmp Wn,I,shift; cset Wd,ne */\n  {MIR_NES, \"r r I\", SCMPI \"; \" RNES},\n  /* fcmpe Sn,Sm; cset Rd,ne */\n  {MIR_FNE, \"r r r\", FCMP \"; \" RNE},\n  /* fcmpe Dn,Dm; cset Rd,ne */\n  {MIR_DNE, \"r r r\", DCMP \"; \" RNE},\n  /* fcmpe Sn,0.0; cset Rd,ne */\n  {MIR_FNE, \"r r Zf\", \"1e202018:fffffc1f vn1 vm2; \" RNE},\n  /* fcmpe Dn,0.0; cset Rd,ne */\n  {MIR_DNE, \"r r Zd\", \"1e602018:fffffc1f vn1 vm2; \" RNE},\n\n#define RLT \"9a9fa7e0:ffffffe0 rd0\"\n#define RLTS \"1a9fa7e0:ffffffe0 rd0\"\n#define RULT \"9a9f27e0:ffffffe0 rd0\"\n#define RULTS \"1a9f27e0:ffffffe0 rd0\"\n#define FLTC \"9a9f57e0:ffffffe0 rd0\"\n  /* cmp Rn,Rm; cset Rd,lt */\n  {MIR_LT, \"r r r\", CMPR \"; \" RLT},\n  /* cmp Rn,I,shift ; cset Wd,lt */\n  {MIR_LT, \"r r I\", CMPI \"; \" RLT},\n  /* cmp Wn,Wm;cset Rd,lt */\n  {MIR_LTS, \"r r r\", SCMPR \"; \" RLTS},\n  /* cmp Wn,I,shift; cset Wd,lt */\n  {MIR_LTS, \"r r I\", SCMPI \"; \" RLTS},\n  /* cmp Rn,Rm; cset Rd,ult */\n  {MIR_ULT, \"r r r\", CMPR \"; \" RULT},\n  /* cmp Rn,I,shift ; cset Wd,cc */\n  {MIR_ULT, \"r r I\", CMPI \"; \" RULT},\n  /* cmp Wn,Wm;cset Rd,cc */\n  {MIR_ULTS, \"r r r\", SCMPR \"; \" RULTS},\n  /* cmp Wn,I,shift; cset Wd,cc */\n  {MIR_ULTS, \"r r I\", SCMPI \"; \" RULTS},\n  /* fcmpe Sn,Sm; cset Rd,mi */\n  {MIR_FLT, \"r r r\", FCMP \"; \" FLTC},\n  /* fcmpe Dn,Dm; cset Rd,mi */\n  {MIR_DLT, \"r r r\", DCMP \"; \" FLTC},\n  /* fcmpe Sn,0.0; cset Rd,mi */\n  {MIR_FLT, \"r r Zf\", \"1e202018:fffffc1f vn1 vm2; \" FLTC},\n  /* fcmpe Dn,0.0; cset Rd,mi */\n  {MIR_DLT, \"r r Zd\", \"1e602018:fffffc1f vn1 vm2; \" FLTC},\n\n#define RGE \"9a9fb7e0:ffffffe0 rd0\"\n#define RGES \"1a9fb7e0:ffffffe0 rd0\"\n#define RUGE \"9a9f37e0:ffffffe0 rd0\"\n#define RUGES \"1a9f37e0:ffffffe0 rd0\"\n  /* cmp Rn,Rm; cset Rd,ge */\n  {MIR_GE, \"r r r\", CMPR \"; \" RGE},\n  /* cmp Rn,I,shift ; cset Wd,ge */\n  {MIR_GE, \"r r I\", CMPI \"; \" RGE},\n  /* cmp Wn,Wm;cset Rd,ge */\n  {MIR_GES, \"r r r\", SCMPR \"; \" RGES},\n  /* cmp Wn,I,shift; cset Wd,ge */\n  {MIR_GES, \"r r I\", SCMPI \"; \" RGES},\n  /* cmp Rn,Rm; cset Rd,cs */\n  {MIR_UGE, \"r r r\", CMPR \"; \" RUGE},\n  /* cmp Rn,I,shift ; cset Wd,cs */\n  {MIR_UGE, \"r r I\", CMPI \"; \" RUGE},\n  /* cmp Wn,Wm;cset Rd,cs */\n  {MIR_UGES, \"r r r\", SCMPR \"; \" RUGES},\n  /* cmp Wn,I,shift; cset Wd,cs */\n  {MIR_UGES, \"r r I\", SCMPI \"; \" RUGES},\n  /* fcmpe Sn,Sm; cset Rd,ge */\n  {MIR_FGE, \"r r r\", FCMP \"; \" RGE},\n  /* fcmpe Dn,Dm; cset Rd,ge */\n  {MIR_DGE, \"r r r\", DCMP \"; \" RGE},\n  /* fcmpe Sn,0.0; cset Rd,ge */\n  {MIR_FGE, \"r r Zf\", \"1e202018:fffffc1f vn1 vm2; \" RGE},\n  /* fcmpe Dn,0.0; cset Rd,ge */\n  {MIR_DGE, \"r r Zd\", \"1e602018:fffffc1f vn1 vm2; \" RGE},\n\n#define RGT \"9a9fd7e0:ffffffe0 rd0\"\n#define RGTS \"1a9fd7e0:ffffffe0 rd0\"\n#define RUGT \"9a9f97e0:ffffffe0 rd0\"\n#define RUGTS \"1a9f97e0:ffffffe0 rd0\"\n  /* cmp Rn,Rm; cset Rd,gt */\n  {MIR_GT, \"r r r\", CMPR \"; \" RGT},\n  /* cmp Rn,I,shift ; cset Wd,gt */\n  {MIR_GT, \"r r I\", CMPI \"; \" RGT},\n  /* cmp Wn,Wm;cset Rd,gt */\n  {MIR_GTS, \"r r r\", SCMPR \"; \" RGTS},\n  /* cmp Wn,I,shift; cset Wd,gt */\n  {MIR_GTS, \"r r I\", SCMPI \"; \" RGTS},\n  /* cmp Rn,Rm; cset Rd,hi */\n  {MIR_UGT, \"r r r\", CMPR \"; \" RUGT},\n  /* cmp Rn,I,shift ; cset Wd,hi */\n  {MIR_UGT, \"r r I\", CMPI \"; \" RUGT},\n  /* cmp Wn,Wm;cset Rd,hi */\n  {MIR_UGTS, \"r r r\", SCMPR \"; \" RUGTS},\n  /* cmp Wn,I,shift; cset Wd,hi */\n  {MIR_UGTS, \"r r I\", SCMPI \"; \" RUGTS},\n  /* fcmpe Sn,Sm; cset Rd,gt */\n  {MIR_FGT, \"r r r\", FCMP \"; \" RGT},\n  /* fcmpe Dn,Dm; cset Rd,gt */\n  {MIR_DGT, \"r r r\", DCMP \"; \" RGT},\n  /* fcmpe Sn,0.0; cset Rd,gt */\n  {MIR_FGT, \"r r Zf\", \"1e202018:fffffc1f vn1 vm2; \" RGT},\n  /* fcmpe Dn,0.0; cset Rd,gt */\n  {MIR_DGT, \"r r Zd\", \"1e602018:fffffc1f vn1 vm2; \" RGT},\n\n#define RLE \"9a9fc7e0:ffffffe0 rd0\"\n#define RLES \"1a9fc7e0:ffffffe0 rd0\"\n#define RULE \"9a9f87e0:ffffffe0 rd0\"\n#define RULES \"1a9f87e0:ffffffe0 rd0\"\n#define FLEC \"9a9f87e0:ffffffe0 rd0\"\n  /* cmp Rn,Rm; cset Rd,le */\n  {MIR_LE, \"r r r\", CMPR \"; \" RLE},\n  /* cmp Rn,I,shift ; cset Wd,le */\n  {MIR_LE, \"r r I\", CMPI \"; \" RLE},\n  /* cmp Wn,Wm;cset Rd,le */\n  {MIR_LES, \"r r r\", SCMPR \"; \" RLES},\n  /* cmp Wn,I,shift; cset Wd,le */\n  {MIR_LES, \"r r I\", SCMPI \"; \" RLES},\n  /* cmp Rn,Rm; cset Rd,ls */\n  {MIR_ULE, \"r r r\", CMPR \"; \" RULE},\n  /* cmp Rn,I,shift ; cset Wd,ls */\n  {MIR_ULE, \"r r I\", CMPI \"; \" RULE},\n  /* cmp Wn,Wm;cset Rd,ls */\n  {MIR_ULES, \"r r r\", SCMPR \"; \" RULES},\n  /* cmp Wn,I,shift; cset Wd,ls */\n  {MIR_ULES, \"r r I\", SCMPI \"; \" RULES},\n  /* fcmpe Sn,Sm; cset Rd,ls */\n  {MIR_FLE, \"r r r\", FCMP \"; \" FLEC},\n  /* fcmpe Dn,Dm; cset Rd,ls */\n  {MIR_DLE, \"r r r\", DCMP \"; \" FLEC},\n  /* fcmpe Sn,0.0; cset Rd,ls */\n  {MIR_FLE, \"r r Zf\", \"1e202018:fffffc1f vn1 vm2; \" FLEC},\n  /* fcmpe Dn,0.0; cset Rd,ls */\n  {MIR_DLE, \"r r Zd\", \"1e602018:fffffc1f vn1 vm2; \" FLEC},\n\n  {MIR_JMP, \"L\", \"14000000:fc000000 L\"}, /* 26-bit offset jmp */\n\n  {MIR_LADDR, \"r l\", \"10000000:ff000000 rd0 l\"}, /* adr r, L ip-relative address */\n  {MIR_JMPI, \"r\", \"d61f0000:fffffc00 rn0\"},      /* jmp *r */\n\n  {MIR_BT, \"l r\", \"b5000000:ff000000 rd1 l\"},  /* cbnz rd,l */\n  {MIR_BTS, \"l r\", \"35000000:ff000000 rd1 l\"}, /* cbnz wd,l */\n  {MIR_BF, \"l r\", \"b4000000:ff000000 rd1 l\"},  /* cbz rd,l */\n  {MIR_BFS, \"l r\", \"34000000:ff000000 rd1 l\"}, /* cbz wd,l */\n\n  {MIR_BO, \"l\", \"54000006:ff00001f l\"},  /* b.vs */\n  {MIR_UBO, \"l\", \"54000002:ff00001f l\"}, /* b.cs */\n\n  {MIR_BNO, \"l\", \"54000007:ff00001f l\"},  /* b.vc */\n  {MIR_UBNO, \"l\", \"54000003:ff00001f l\"}, /* b.cc */\n\n#define BEQ \"54000000:ff00001f l\"\n  // ??? add extended reg cmp insns:\n  // all ld insn are changed to builtins and bt/bts\n  /* cmp Rn,Rm; beq l */\n  {MIR_BEQ, \"l r r\", CMPR \"; \" BEQ},\n  /* cmp Rn,I,shift ; beq l */\n  {MIR_BEQ, \"l r I\", CMPI \"; \" BEQ},\n  /* cmp Wn,Wm;beq l */\n  {MIR_BEQS, \"l r r\", SCMPR \"; \" BEQ},\n  /* cmp Wn,I,shift; beq l */\n  {MIR_BEQS, \"l r I\", SCMPI \"; \" BEQ},\n  /* fcmpe Sn,Sm; beq l */\n  {MIR_FBEQ, \"l r r\", FCMP \"; \" BEQ},\n  /* fcmpe Dn,Dm; beq l */\n  {MIR_DBEQ, \"l r r\", DCMP \"; \" BEQ},\n  /* fcmpe Sn,0.0; beq l */\n  {MIR_FBEQ, \"l r Zf\", \"1e202018:fffffc1f vn1 vm2; \" BEQ},\n  /* fcmpe Dn,0.0; beq l */\n  {MIR_DBEQ, \"l r Zd\", \"1e602018:fffffc1f vn1 vm2; \" BEQ},\n\n#define BNE \"54000001:ff00001f l\"\n  /* cmp Rn,Rm; bne l */\n  {MIR_BNE, \"l r r\", CMPR \"; \" BNE},\n  /* cmp Rn,I,shift ; bne l */\n  {MIR_BNE, \"l r I\", CMPI \"; \" BNE},\n  /* cmp Wn,Wm;bne l */\n  {MIR_BNES, \"l r r\", SCMPR \"; \" BNE},\n  /* cmp Wn,I,shift; bne l */\n  {MIR_BNES, \"l r I\", SCMPI \"; \" BNE},\n  /* fcmpe Sn,Sm; bne l */\n  {MIR_FBNE, \"l r r\", FCMP \"; \" BNE},\n  /* fcmpe Dn,Dm; bne l */\n  {MIR_DBNE, \"l r r\", DCMP \"; \" BNE},\n  /* fcmpe Sn,0.0; bne l */\n  {MIR_FBNE, \"l r Zf\", \"1e202018:fffffc1f vn1 vm2; \" BNE},\n  /* fcmpe Dn,0.0; bne l */\n  {MIR_DBNE, \"l r Zd\", \"1e602018:fffffc1f vn1 vm2; \" BNE},\n\n#define BLT \"5400000b:ff00001f l\"\n#define UBLT \"54000003:ff00001f l\"\n  /* cmp Rn,Rm; blt l */\n  {MIR_BLT, \"l r r\", CMPR \"; \" BLT},\n  /* cmp Rn,I,shift ; blt l */\n  {MIR_BLT, \"l r I\", CMPI \"; \" BLT},\n  /* cmp Wn,Wm;blt l */\n  {MIR_BLTS, \"l r r\", SCMPR \"; \" BLT},\n  /* cmp Wn,I,shift; blt l */\n  {MIR_BLTS, \"l r I\", SCMPI \"; \" BLT},\n  /* cmp Rn,Rm; bcc l */\n  {MIR_UBLT, \"l r r\", CMPR \"; \" UBLT},\n  /* cmp Rn,I,shift ; bcc l */\n  {MIR_UBLT, \"l r I\", CMPI \"; \" UBLT},\n  /* cmp Wn,Wm;bcc l */\n  {MIR_UBLTS, \"l r r\", SCMPR \"; \" UBLT},\n  /* cmp Wn,I,shift; bcc l */\n  {MIR_UBLTS, \"l r I\", SCMPI \"; \" UBLT},\n\n#define BGE \"5400000a:ff00001f l\"\n#define UBGE \"54000002:ff00001f l\"\n  /* cmp Rn,Rm; bge l */\n  {MIR_BGE, \"l r r\", CMPR \"; \" BGE},\n  /* cmp Rn,I,shift ; bge l */\n  {MIR_BGE, \"l r I\", CMPI \"; \" BGE},\n  /* cmp Wn,Wm;bge l */\n  {MIR_BGES, \"l r r\", SCMPR \"; \" BGE},\n  /* cmp Wn,I,shift; bge l */\n  {MIR_BGES, \"l r I\", SCMPI \"; \" BGE},\n  /* cmp Rn,Rm; bcs l */\n  {MIR_UBGE, \"l r r\", CMPR \"; \" UBGE},\n  /* cmp Rn,I,shift ; bcs l */\n  {MIR_UBGE, \"l r I\", CMPI \"; \" UBGE},\n  /* cmp Wn,Wm;bcs l */\n  {MIR_UBGES, \"l r r\", SCMPR \"; \" UBGE},\n  /* cmp Wn,I,shift; bcs l */\n  {MIR_UBGES, \"l r I\", SCMPI \"; \" UBGE},\n  /* fcmpe Sn,Sm; bpl l */\n  {MIR_FBGE, \"l r r\", FCMP \"; \" BGE},\n  /* fcmpe Dn,Dm; bpl l */\n  {MIR_DBGE, \"l r r\", DCMP \"; \" BGE},\n  /* fcmpe Sn,0.0; bpl l */\n  {MIR_FBGE, \"l r Zf\", \"1e202018:fffffc1f vn1 vm2; \" BGE},\n  /* fcmpe Dn,0.0; bpl l */\n  {MIR_DBGE, \"l r Zd\", \"1e602018:fffffc1f vn1 vm2; \" BGE},\n\n#define BGT \"5400000c:ff00001f l\"\n#define UBGT \"54000008:ff00001f l\"\n  /* cmp Rn,Rm; bgt l */\n  {MIR_BGT, \"l r r\", CMPR \"; \" BGT},\n  /* cmp Rn,I,shift ; bgt l */\n  {MIR_BGT, \"l r I\", CMPI \"; \" BGT},\n  /* cmp Wn,Wm;bgt l */\n  {MIR_BGTS, \"l r r\", SCMPR \"; \" BGT},\n  /* cmp Wn,I,shift; bgt l */\n  {MIR_BGTS, \"l r I\", SCMPI \"; \" BGT},\n  /* cmp Rn,Rm; bhi l */\n  {MIR_UBGT, \"l r r\", CMPR \"; \" UBGT},\n  /* cmp Rn,I,shift ; bhi l */\n  {MIR_UBGT, \"l r I\", CMPI \"; \" UBGT},\n  /* cmp Wn,Wm;bhi l */\n  {MIR_UBGTS, \"l r r\", SCMPR \"; \" UBGT},\n  /* cmp Wn,I,shift; bhi l */\n  {MIR_UBGTS, \"l r I\", SCMPI \"; \" UBGT},\n  /* fcmpe Sn,Sm; bhi l */\n  {MIR_FBGT, \"l r r\", FCMP \"; \" BGT},\n  /* fcmpe Dn,Dm; bhi l */\n  {MIR_DBGT, \"l r r\", DCMP \"; \" BGT},\n  /* fcmpe Sn,0.0; bhi l */\n  {MIR_FBGT, \"l r Zf\", \"1e202018:fffffc1f vn1 vm2; \" BGT},\n  /* fcmpe Dn,0.0; bhi l */\n  {MIR_DBGT, \"l r Zd\", \"1e602018:fffffc1f vn1 vm2; \" BGT},\n\n#define BLE \"5400000d:ff00001f l\"\n#define UBLE \"54000009:ff00001f l\"\n  /* cmp Rn,Rm; ble l */\n  {MIR_BLE, \"l r r\", CMPR \"; \" BLE},\n  /* cmp Rn,I,shift ; ble l */\n  {MIR_BLE, \"l r I\", CMPI \"; \" BLE},\n  /* cmp Wn,Wm;ble l */\n  {MIR_BLES, \"l r r\", SCMPR \"; \" BLE},\n  /* cmp Wn,I,shift; ble l */\n  {MIR_BLES, \"l r I\", SCMPI \"; \" BLE},\n  /* cmp Rn,Rm; bls l */\n  {MIR_UBLE, \"l r r\", CMPR \"; \" UBLE},\n  /* cmp Rn,I,shift ; bls l */\n  {MIR_UBLE, \"l r I\", CMPI \"; \" UBLE},\n  /* cmp Wn,Wm;bls l */\n  {MIR_UBLES, \"l r r\", SCMPR \"; \" UBLE},\n  /* cmp Wn,I,shift; bls l */\n  {MIR_UBLES, \"l r I\", SCMPI \"; \" UBLE},\n\n  // ??? with shift\n  {MIR_NEG, \"r r\", \"cb0003e0:ff2003e0 rd0 rm1\"},  /* neg Rd,Rm */\n  {MIR_NEGS, \"r r\", \"4b0003e0:ff2003e0 rd0 rm1\"}, /* neg Wd,Wm */\n  {MIR_FNEG, \"r r\", \"1e214000:fffffc00 vd0 vn1\"}, /* fneg Sd,Sn */\n  {MIR_DNEG, \"r r\", \"1e614000:fffffc00 vd0 vn1\"}, /* fneg Dd,Dn */\n  // ldneg is a builtin\n\n  {MIR_LSH, \"r r r\", \"9ac02000:ffe0fc00 rd0 rn1 rm2\"},  /* lsl Rd,Rn,Rm */\n  {MIR_LSHS, \"r r r\", \"1ac02000:ffe0fc00 rd0 rn1 rm2\"}, /* lsl Wd,Wn,Wm */\n  {MIR_LSH, \"r r SL\", \"d3400000:ffc00000 rd0 rn1 SL\"},  /* ubfm Rd,Rn,immr,imms */\n  {MIR_LSHS, \"r r Sl\", \"53000000:ffc00000 rd0 rn1 Sl\"}, /* ubfm Wd,Wn,immr,imms */\n\n  {MIR_RSH, \"r r r\", \"9ac02800:ffe0fc00 rd0 rn1 rm2\"},  /* asr Rd,Rn,Rm */\n  {MIR_RSHS, \"r r r\", \"1ac02800:ffe0fc00 rd0 rn1 rm2\"}, /* asr Wd,Wn,Wm */\n  {MIR_RSH, \"r r SR\", \"9340fc00:ffc0fc00 rd0 rn1 S\"},   /* asr Rd,Rn,S */\n  {MIR_RSHS, \"r r Sr\", \"13007c00:ffc0fc00 rd0 rn1 S\"},  /* asr Wd,Wn,S */\n\n  {MIR_URSH, \"r r r\", \"9ac02400:ffe0fc00 rd0 rn1 rm2\"},  /* lsr Rd,Rn,Rm */\n  {MIR_URSHS, \"r r r\", \"1ac02400:ffe0fc00 rd0 rn1 rm2\"}, /* lsr Wd,Wn,Wm */\n  {MIR_URSH, \"r r SR\", \"d340fc00:ffc0fc00 rd0 rn1 S\"},   /* lsr Rd,Rn,S */\n  {MIR_URSHS, \"r r Sr\", \"53007c00:ffc0fc00 rd0 rn1 S\"},  /* lsr Wd,Wn,S */\n\n  // ??? adding shift, negate, immediate\n  {MIR_AND, \"r r r\", \"8a000000:ffe0fc00 rd0 rn1 rm2\"},  /* and Rd,Rn,Rm */\n  {MIR_ANDS, \"r r r\", \"0a000000:ffe0fc00 rd0 rn1 rm2\"}, /* and Wd,Wn,Wm */\n\n  {MIR_OR, \"r r r\", \"aa000000:ffe0fc00 rd0 rn1 rm2\"},  /* orr Rd,Rn,Rm */\n  {MIR_ORS, \"r r r\", \"2a000000:ffe0fc00 rd0 rn1 rm2\"}, /* orr Wd,Wn,Wm */\n\n  {MIR_XOR, \"r r r\", \"ca000000:ffe0fc00 rd0 rn1 rm2\"},  /* eor Rd,Rn,Rm */\n  {MIR_XORS, \"r r r\", \"4a000000:ffe0fc00 rd0 rn1 rm2\"}, /* eor Wd,Wn,Wm */\n\n  // ??? can we add scale\n  {MIR_I2F, \"r r\", \"9e220000:ffff0000 vd0 rn1\"},  /* scvtf Sd,Rn */\n  {MIR_I2D, \"r r\", \"9e620000:ffff0000 vd0 rn1\"},  /* scvtf Dd,Rn */\n  {MIR_UI2F, \"r r\", \"9e230000:ffff0000 vd0 rn1\"}, /* ucvtf Sd,Rn */\n  {MIR_UI2D, \"r r\", \"9e630000:ffff0000 vd0 rn1\"}, /* ucvtf Dd,Rn */\n  {MIR_F2I, \"r r\", \"9e380000:ffff0000 rd0 vn1\"},  /* fcvtzs Rd,Sn */\n  {MIR_D2I, \"r r\", \"9e780000:ffff0000 rd0 vn1\"},  /* fcvtzs Rd,Dn */\n  {MIR_F2D, \"r r\", \"1e22c000:fffffc00 vd0 vn1\"},  /* fcvt Dd,Sn */\n  {MIR_D2F, \"r r\", \"1e624000:fffffc00 vd0 vn1\"},  /* fcvt Sd,Dn */\n  // i2ld, ui2ld, ld2i, f2ld, d2ld, ld2f, ld2d are builtins\n\n  {MIR_CALL, \"X r $\", \"d63f0000:fffffc1f rn1\"},   /* blr *Rn */\n  {MIR_CALL, \"X L $\", \"94000000:fc000000 rn1\"},   /* bl address */\n  {MIR_INLINE, \"X r $\", \"d63f0000:fffffc1f rn1\"}, /* blr *Rn */\n  {MIR_INLINE, \"X L $\", \"94000000:fc000000 rn1\"}, /* bl address */\n  {MIR_RET, \"$\", \"d65f0000:fffffc1f hn1e\"},       /* ret R30  */\n\n  {MIR_JCALL, \"X r $\", \"d61f0000:fffffc00 rn1\"}, /* br r1 */\n  {MIR_JRET, \"r $\", \"d61f0000:fffffc00 rn0\"},    /* br r0  */\n\n  /* add r0, r1, 15; and r0, r0, -16; sub sp, sp, r0; mov r0, sp: */\n  {MIR_ALLOCA, \"r r\",\n   \"91003c00:fffffc00 rd0 rn1; 927cec00:fffffc00 rd0 rn0;\"         /* add r0,r1,15;and r0,r0,-16\n                                                                    */\n   \"cb206000:ffe0fc00 hn1f hd1f rm0; 91000000:fffffc00 rd0 hn1f\"}, /* sub sp,sp,r0; mov r0,sp */\n  /* sub sp, sp, roundup (imm, 16); mov r0, sp: */\n  {MIR_ALLOCA, \"r Iu\", \"d1000000:ff000000 hd1f hn1f Iu; 91000000:fffffc00 rd0 hn1f\"},\n\n  {MIR_BSTART, \"r\", \"91000000:fffffc00 rd0 hn1f\"}, /* Rd = sp */\n  {MIR_BEND, \"r\", \"91000000:fffffc00 hd1f rn0\"},   /* sp = Rn */\n\n  /* adr r10,PC-relative TableAddress; ldr r10,(r10,r,8);br r10; TableContent\n     We use r10 as r9 can be used if switch operand is memory. */\n  {MIR_SWITCH, \"r $\",\n   \"10000000:ff000000 hda T; f8607800:ffe0fc00 hda hna rm0; d61f0000:fffffc00 hna;\"},\n\n  /* Used only during machine code generation.  Should have the same format as branch on overflow\n     insns */\n  /* unsigned sub sets up carry flag when there is no overflow: */\n  {SUB_UBO, \"l\", \"54000003:ff00001f l\"},  /* b.cc */\n  {SUB_UBNO, \"l\", \"54000002:ff00001f l\"}, /* b.cs */\n\n  /* MULOS:smull Rd,Wn,Wm; asr r10,Rd,32; cmp W10,Wd,asr 31 */\n  {MIR_MULOS, \"r r r\",\n   \"9b207c00:ffe0fc00 rd0 rn1 rm2; 9340fc00:ffc0fc00 hda rn0 I20; \"\n   \"6b80001f:ffe0001f hna rm0 i1f\"},\n  /* UMULOS:umull Rd,Wn,Wm; cmp xzr,Rd,lsr 32 */\n  {MIR_UMULOS, \"r r r\", \"9ba07c00:ffe0fc00 rd0 rn1 rm2; eb40001f:ffe0001f hn1f rm0 i20\"},\n  /* MULO:smulh h11,Rn,Rm; mul Rd,Rn,Rm; cmp h11,Rd,asr 63 (r11 is a scratch reg) */\n  {MIR_MULO, \"r r r\",\n   \"9b407c00:ffe0fc00 hdb rn1 rm2; 9b007c00:ffe0fc00 rd0 rn1 rm2; \"\n   \"eb80001f:ffe0001f hnb rm0 i3f\"},\n  /* UMULO:umulh h11,Rn,Rm; mul Rd,Rn,Rm; cmp xzr,h11 (r11 is a scratch reg) */\n  {MIR_UMULO, \"r r r\",\n   \"9bc07c00:ffe0fc00 hdb rn1 rm2; 9b007c00:ffe0fc00 rd0 rn1 rm2; \"\n   \"eb00001f:ff20001f hn1f hmb\"},\n\n  /* [u]mulo[s] insns uses zero flag to check overflow: */\n  {MUL_BO, \"l\", BNE},  /* b.ne */\n  {MUL_BNO, \"l\", BEQ}, /* b.eq */\n};\n\nstatic void target_get_early_clobbered_hard_regs (MIR_insn_t insn, MIR_reg_t *hr1, MIR_reg_t *hr2) {\n  *hr1 = *hr2 = MIR_NON_VAR;\n  if (insn->code == MIR_MOD || insn->code == MIR_MODS || insn->code == MIR_UMOD\n      || insn->code == MIR_UMODS)\n    *hr1 = R8_HARD_REG;\n  else if (insn->code == MIR_MULO || insn->code == MIR_UMULO)\n    *hr1 = R11_HARD_REG;\n}\n\nstatic int pattern_index_cmp (const void *a1, const void *a2) {\n  int i1 = *(const int *) a1, i2 = *(const int *) a2;\n  int c1 = (int) patterns[i1].code, c2 = (int) patterns[i2].code;\n\n  return c1 != c2 ? c1 - c2 : (long) i1 - (long) i2;\n}\n\nstatic void patterns_init (gen_ctx_t gen_ctx) {\n  MIR_alloc_t alloc = gen_alloc (gen_ctx);\n  int i, ind, n = sizeof (patterns) / sizeof (struct pattern);\n  MIR_insn_code_t prev_code, code;\n  insn_pattern_info_t *info_addr;\n  insn_pattern_info_t pinfo = {0, 0};\n\n  VARR_CREATE (int, pattern_indexes, alloc, 0);\n  for (i = 0; i < n; i++) VARR_PUSH (int, pattern_indexes, i);\n  qsort (VARR_ADDR (int, pattern_indexes), n, sizeof (int), pattern_index_cmp);\n  VARR_CREATE (insn_pattern_info_t, insn_pattern_info, alloc, 0);\n  for (i = 0; i < ARM_INSN_BOUND; i++) VARR_PUSH (insn_pattern_info_t, insn_pattern_info, pinfo);\n  info_addr = VARR_ADDR (insn_pattern_info_t, insn_pattern_info);\n  for (prev_code = ARM_INSN_BOUND, i = 0; i < n; i++) {\n    ind = VARR_GET (int, pattern_indexes, i);\n    if ((code = patterns[ind].code) != prev_code) {\n      if (i != 0) info_addr[prev_code].num = i - info_addr[prev_code].start;\n      info_addr[code].start = i;\n      prev_code = code;\n    }\n  }\n  assert (prev_code != ARM_INSN_BOUND);\n  info_addr[prev_code].num = n - info_addr[prev_code].start;\n}\n\nstruct imm {\n  int v, shift;\n};\n\n/* Return number of insn mov{n|z} movk* to express constant V. Return immediates with their shifts\n   for mov{n|z}, movk in IMMS. */\nstatic int movnzk_const (uint64_t v, int n_p, struct imm *imms) {\n  int i16, shift, n = 0;\n\n  if (n_p) v = ~v;\n  if (v == 0) {\n    imms[0].v = 0;\n    imms[0].shift = 0;\n    return 1;\n  }\n  for (shift = 0; v != 0; v >>= 16, shift += 16) {\n    for (; (i16 = v & 0xffff) == 0; shift += 16) v >>= 16;\n    gen_assert (n < 4);\n    imms[n].v = n_p && n != 0 ? (~i16 & 0xffff) : i16;\n    imms[n++].shift = shift;\n  }\n  return n;\n}\n\n/* Return shift flag (0 or 1) for arithm insn 12-bit immediate. If V cannot be represented, return\n   -1. */\nstatic int arithm_const (uint64_t v, int *imm) {\n  if (v < (1 << 12)) {\n    *imm = v;\n    return 0;\n  }\n  if ((v & 0xfff) == 0 && (v >> 12) < (1 << 12)) {\n    *imm = v >> 12;\n    return 1;\n  }\n  return -1;\n}\n\n/* Return shift flag (0 or 1) for arithm insn 12-bit immediate rounded\n   up to 16. If the rounded V cannot be represented, return -1. */\nstatic int arithm_roundup_const (uint64_t v, int *imm) {\n  return arithm_const ((v + 15) / 16 * 16, imm);\n}\n\n/* Return immr for right 64-bit or 32-bit (if SHORT_P) shift by V.  If the\n   shift can not be represented, return FALSE. */\nstatic int rshift_const (int64_t v, int short_p) {\n  return v < 0 || v > 63 || (short_p && v > 31) ? -1 : v;\n}\n\n/* Return immr and imms for left 64-bit or 32-bit (if SHORT_P) shift\n   by V.  If the shift can not be represented, return FALSE. */\nstatic int lshift_const_p (int64_t v, int short_p, int *immr, int *imms) {\n  if (short_p) {\n    if (v < 0 || v > 31) return FALSE;\n    *immr = (-v) & 0x1f;\n    *imms = 31 - v;\n  } else {\n    if (v < 0 || v > 63) return FALSE;\n    *immr = (-v) & 0x3f;\n    *imms = 63 - v;\n  }\n  return TRUE;\n}\n\nstatic int pattern_match_p (gen_ctx_t gen_ctx, const struct pattern *pat, MIR_insn_t insn) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  size_t nop, nops = MIR_insn_nops (ctx, insn);\n  const char *p;\n  char ch, start_ch;\n  MIR_op_t op;\n  MIR_reg_t hr;\n\n  for (nop = 0, p = pat->pattern; *p != 0; p++, nop++) {\n    while (*p == ' ' || *p == '\\t') p++;\n    if (*p == '$') return TRUE;\n    if (MIR_call_code_p (insn->code) && nop >= nops) return FALSE;\n    gen_assert (nop < nops);\n    op = insn->ops[nop];\n    switch (start_ch = *p) {\n    case 'X': break;\n    case 'r':\n      if (op.mode != MIR_OP_VAR) return FALSE;\n      break;\n    case 'h':\n      if (op.mode != MIR_OP_VAR) return FALSE;\n      ch = *++p;\n      gen_assert ('0' <= ch && ch <= '9');\n      hr = ch - '0';\n      ch = *++p;\n      if ('0' <= ch && ch <= '9')\n        hr = hr * 10 + ch - '0';\n      else\n        --p;\n      gen_assert (hr <= MAX_HARD_REG);\n      if (op.u.var != hr) return FALSE;\n      break;\n    case 'm':\n    case 'M': {\n      MIR_type_t type, type2, type3 = MIR_T_BOUND;\n      int scale, u_p, s_p;\n\n      if (op.mode != MIR_OP_VAR_MEM) return FALSE;\n      u_p = s_p = TRUE;\n      ch = *++p;\n      switch (ch) {\n      case 'f':\n        type = MIR_T_F;\n        type2 = MIR_T_BOUND;\n        scale = 4;\n        break;\n      case 'd':\n        type = MIR_T_D;\n        type2 = MIR_T_BOUND;\n        scale = 8;\n        break;\n      case 'l':\n        ch = *++p;\n        gen_assert (ch == 'd');\n        type = MIR_T_LD;\n        type2 = MIR_T_BOUND;\n        scale = 16;\n        break;\n      case 'u':\n      case 's':\n        u_p = ch == 'u';\n        s_p = ch == 's';\n        ch = *++p;\n        /* fall through */\n      default:\n        gen_assert ('0' <= ch && ch <= '3');\n        scale = 1 << (ch - '0');\n        if (ch == '0') {\n          type = u_p ? MIR_T_U8 : MIR_T_I8;\n          type2 = u_p && s_p ? MIR_T_I8 : MIR_T_BOUND;\n        } else if (ch == '1') {\n          type = u_p ? MIR_T_U16 : MIR_T_I16;\n          type2 = u_p && s_p ? MIR_T_I16 : MIR_T_BOUND;\n        } else if (ch == '2') {\n          type = u_p ? MIR_T_U32 : MIR_T_I32;\n          type2 = u_p && s_p ? MIR_T_I32 : MIR_T_BOUND;\n#if MIR_PTR32\n          if (u_p) type3 = MIR_T_P;\n#endif\n        } else {\n          type = u_p ? MIR_T_U64 : MIR_T_I64;\n          type2 = u_p && s_p ? MIR_T_I64 : MIR_T_BOUND;\n#if MIR_PTR64\n          type3 = MIR_T_P;\n#endif\n        }\n      }\n      if (op.u.var_mem.type != type && op.u.var_mem.type != type2 && op.u.var_mem.type != type3)\n        return FALSE;\n      if (start_ch == 'm'\n          && (op.u.var_mem.disp != 0\n              || (op.u.var_mem.index != MIR_NON_VAR && op.u.var_mem.scale != 1\n                  && op.u.var_mem.scale != scale)))\n        return FALSE;\n      if (start_ch == 'M'\n          && (op.u.var_mem.index != MIR_NON_VAR || op.u.var_mem.disp < 0\n              || op.u.var_mem.disp % scale != 0 || op.u.var_mem.disp / scale >= (1 << 12)))\n        return FALSE;\n      break;\n    }\n    case 'Z':\n    case 'N': {\n      int n;\n      uint64_t v;\n      struct imm imms[4];\n\n      ch = *++p;\n      if (ch == 'f' && op.mode == MIR_OP_FLOAT) {\n        if (op.u.f != 0.0f) return FALSE;\n      } else if (ch == 'd' && op.mode == MIR_OP_DOUBLE) {\n        if (op.u.d != 0.0) return FALSE;\n      } else {\n        if (op.mode != MIR_OP_INT && op.mode != MIR_OP_UINT && op.mode != MIR_OP_REF) return FALSE;\n        gen_assert (('0' <= ch && ch <= '2') || (start_ch == 'Z' && ch == '3'));\n        n = ch - '0';\n        if (op.mode != MIR_OP_REF) {\n          v = op.u.u;\n        } else if (op.u.ref->item_type == MIR_data_item && op.u.ref->u.data->name != NULL\n                   && _MIR_reserved_ref_name_p (ctx, op.u.ref->u.data->name)) {\n          v = (uint64_t) op.u.ref->u.data->u.els;\n        } else {\n          v = (uint64_t) op.u.ref->addr;\n        }\n        if (movnzk_const (v, start_ch == 'N', imms) > n + 1) return FALSE;\n        gen_assert (nop == 1); /* only 2nd move operand */\n      }\n      break;\n    }\n    case 'I': {\n      int imm;\n\n      if (op.mode != MIR_OP_INT && op.mode != MIR_OP_UINT) return FALSE;\n      ch = *++p;\n      if (ch == 'u') {\n        if (arithm_roundup_const (op.u.u, &imm) < 0) return FALSE;\n      } else {\n        p--;\n        if (arithm_const (op.u.u, &imm) < 0) return FALSE;\n      }\n      break;\n    }\n    case 'S': {\n      int immr, imms;\n\n      if (op.mode != MIR_OP_INT && op.mode != MIR_OP_UINT) return FALSE;\n      gen_assert (op.mode != MIR_OP_INT || op.u.i >= 0);\n      ch = *++p;\n      if (ch == 'r' || ch == 'R') {\n        if ((op.mode == MIR_OP_UINT && op.u.i < 0) || rshift_const (op.u.i, ch == 'r') < 0)\n          return FALSE;\n      } else {\n        gen_assert (ch == 'l' || ch == 'L');\n        if ((op.mode == MIR_OP_UINT && op.u.i < 0)\n            || !lshift_const_p (op.u.i, ch == 'l', &immr, &imms))\n          return FALSE;\n      }\n      break;\n    }\n    case 'l':\n      if (op.mode != MIR_OP_LABEL) return FALSE;\n      break;\n    case 'L':\n      if (op.mode != MIR_OP_LABEL && op.mode != MIR_OP_REF) return FALSE;\n      break;\n    default: gen_assert (FALSE);\n    }\n  }\n  gen_assert (nop == nops);\n  return TRUE;\n}\n\nstatic const char *find_insn_pattern_replacement (gen_ctx_t gen_ctx, MIR_insn_t insn) {\n  int i;\n  const struct pattern *pat;\n  insn_pattern_info_t info;\n  int code = insn->code;\n\n  if (code == MIR_BO || code == MIR_BNO || code == MIR_UBO || code == MIR_UBNO) {\n    for (MIR_insn_t prev_insn = DLIST_PREV (MIR_insn_t, insn); prev_insn != NULL;\n         prev_insn = DLIST_PREV (MIR_insn_t, prev_insn)) {\n      if (prev_insn->code == MIR_SUBOS || prev_insn->code == MIR_SUBO) {\n        /* unsigned sub sets up carry flag when there is no overflow: */\n        if (code == MIR_UBO || code == MIR_UBNO) code = code == MIR_UBO ? SUB_UBO : SUB_UBNO;\n        break;\n      } else if (prev_insn->code == MIR_MULOS || prev_insn->code == MIR_MULO\n                 || prev_insn->code == MIR_UMULOS || prev_insn->code == MIR_UMULO) {\n        /* [u]mulo[s] insns uses zero flag to check overflow: */\n        code = code == MIR_BO || code == MIR_UBO ? MUL_BO : MUL_BNO;\n        break;\n      } else if (prev_insn->code == MIR_ADDOS || prev_insn->code == MIR_ADDO\n                 || prev_insn->code == MIR_LABEL || MIR_branch_code_p (prev_insn->code)) {\n        break;\n      }\n    }\n  }\n  info = VARR_GET (insn_pattern_info_t, insn_pattern_info, code);\n  for (i = 0; i < info.num; i++) {\n    pat = &patterns[VARR_GET (int, pattern_indexes, info.start + i)];\n    if (pattern_match_p (gen_ctx, pat, insn)) return pat->replacement;\n  }\n  return NULL;\n}\n\nstatic void patterns_finish (gen_ctx_t gen_ctx) {\n  VARR_DESTROY (int, pattern_indexes);\n  VARR_DESTROY (insn_pattern_info_t, insn_pattern_info);\n}\n\nstatic int hex_value (int ch) {\n  return ('0' <= ch && ch <= '9'   ? ch - '0'\n          : 'A' <= ch && ch <= 'F' ? ch - 'A' + 10\n          : 'a' <= ch && ch <= 'f' ? ch - 'a' + 10\n                                   : -1);\n}\n\nstatic uint64_t read_hex (const char **ptr) {\n  int v;\n  const char *p;\n  uint64_t res = 0;\n\n  for (p = *ptr; (v = hex_value (*p)) >= 0; p++) {\n    gen_assert ((res >> 60) == 0);\n    res = res * 16 + v;\n  }\n  gen_assert (p != *ptr);\n  *ptr = p - 1;\n  return res;\n}\n\nstatic void put_byte (struct gen_ctx *gen_ctx, int byte) { VARR_PUSH (uint8_t, result_code, byte); }\n\nstatic void put_uint64 (struct gen_ctx *gen_ctx, uint64_t v, int nb) {\n  for (; nb > 0; nb--) {\n    put_byte (gen_ctx, v & 0xff);\n    v >>= 8;\n  }\n}\n\nstatic void set_int64 (uint8_t *addr, int64_t v, int nb) { /* Little endian */\n  for (; nb > 0; nb--) {\n    *addr++ = v & 0xff;\n    v >>= 8;\n  }\n}\n\nstatic int64_t get_int64 (uint8_t *addr, int nb) { /* Little endian */\n  int64_t v = 0;\n  int i, sh = (8 - nb) * 8;\n\n  for (i = nb - 1; i >= 0; i--) v = (v << 8) | addr[i];\n  if (sh > 0) v = (v << sh) >> sh; /* make it signed */\n  return v;\n}\n\nstatic uint32_t check_and_set_mask (uint32_t opcode_mask, uint32_t mask) {\n  gen_assert ((opcode_mask & mask) == 0);\n  return opcode_mask | mask;\n}\n\nstatic void out_insn (gen_ctx_t gen_ctx, MIR_insn_t insn, const char *replacement,\n                      void **jump_addrs) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  size_t offset;\n  const char *p, *insn_str;\n  label_ref_t lr;\n  int switch_table_adr_insn_start = -1;\n\n  if (insn->code == MIR_ALLOCA\n      && (insn->ops[1].mode == MIR_OP_INT || insn->ops[1].mode == MIR_OP_UINT))\n    insn->ops[1].u.u = (insn->ops[1].u.u + 15) & -16;\n  for (insn_str = replacement;; insn_str = p + 1) {\n    char ch, ch2, start_ch, d;\n    uint32_t opcode = 0, opcode_mask = 0xffffffff;\n    int rd = -1, rn = -1, rm = -1, ra = -1, disp = -1, scale = -1;\n    int immr = -1, imms = -1, imm16 = -1, imm16_shift = -1, imm12 = -1, imm12_shift = -1;\n    MIR_op_t op;\n    int label_ref_num = -1, switch_table_addr_p = FALSE;\n\n    for (p = insn_str; (ch = *p) != '\\0' && ch != ';'; p++) {\n      if ((d = hex_value (ch = *p)) >= 0) { /* opcode and mask */\n        gen_assert (opcode == 0 && opcode_mask == 0xffffffff);\n        do {\n          opcode = opcode * 16 + d;\n          p++;\n        } while ((d = hex_value (*p)) >= 0);\n        if ((ch = *p) == ':') {\n          p++;\n          opcode_mask = 0;\n          while ((d = hex_value (ch = *p)) >= 0) {\n            opcode_mask = opcode_mask * 16 + d;\n            p++;\n          }\n        }\n        gen_assert ((opcode & ~opcode_mask) == 0);\n      }\n      if ((ch = *p) == 0 || ch == ';') break;\n      switch ((start_ch = ch = *p)) {\n      case ' ':\n      case '\\t': break;\n      case 'r':\n      case 'v':\n      case 'h': {\n        int reg;\n\n        ch2 = *++p;\n        gen_assert (ch2 == 'd' || ch2 == 'n' || ch2 == 'm'\n                    || (ch2 == 'a'\n                        && (insn->code == MIR_MOD || insn->code == MIR_MODS\n                            || insn->code == MIR_UMOD || insn->code == MIR_UMODS)));\n        ch = *++p;\n        if (start_ch == 'h') {\n          reg = read_hex (&p);\n        } else {\n          gen_assert ('0' <= ch && ch <= '2' && ch - '0' < (int) insn->nops);\n          op = insn->ops[ch - '0'];\n          gen_assert (op.mode == MIR_OP_VAR);\n          reg = op.u.var;\n          if (start_ch != 'v') {\n            gen_assert (reg < V0_HARD_REG);\n          } else {\n            gen_assert (reg >= V0_HARD_REG);\n            reg -= V0_HARD_REG;\n          }\n        }\n        gen_assert (reg <= 31);\n        if (ch2 == 'd')\n          rd = reg;\n        else if (ch2 == 'n')\n          rn = reg;\n        else if (ch2 == 'm')\n          rm = reg;\n        else\n          ra = reg;\n        break;\n      }\n      case 'm':\n        op = insn->ops[0].mode == MIR_OP_VAR_MEM ? insn->ops[0] : insn->ops[1];\n        rn = op.u.var_mem.base;\n        rm = op.u.var_mem.index == MIR_NON_VAR ? ZR_HARD_REG : op.u.var_mem.index;\n        scale = op.u.var_mem.scale;\n        break;\n      case 'M': {\n        int dsize = 1;\n\n        op = insn->ops[0].mode == MIR_OP_VAR_MEM ? insn->ops[0] : insn->ops[1];\n        switch (op.u.var_mem.type) {\n        case MIR_T_I8:\n        case MIR_T_U8: dsize = 1; break;\n        case MIR_T_I16:\n        case MIR_T_U16: dsize = 2; break;\n#if MIR_PTR32\n        case MIR_T_P:\n#endif\n        case MIR_T_I32:\n        case MIR_T_U32:\n        case MIR_T_F: dsize = 4; break;\n#if MIR_PTR64\n        case MIR_T_P:\n#endif\n        case MIR_T_I64:\n        case MIR_T_U64:\n        case MIR_T_D: dsize = 8; break;\n        case MIR_T_LD: dsize = 16; break;\n        default: assert (FALSE);\n        }\n        gen_assert (op.u.var_mem.disp % dsize == 0);\n        rn = op.u.var_mem.base;\n        disp = op.u.var_mem.disp / dsize;\n        gen_assert (disp < (1 << 12));\n        break;\n      }\n      case 'S': { /* S, SL, Sl */\n        int flag;\n\n        op = insn->ops[2];\n        gen_assert (op.mode == MIR_OP_INT || op.mode == MIR_OP_UINT);\n        ch = *++p;\n        if (ch == 'L' || ch == 'l') {\n          flag = lshift_const_p (op.u.i, ch == 'l', &immr, &imms);\n          gen_assert (flag);\n        } else {\n          p--;\n          immr = rshift_const (op.u.i, FALSE);\n          gen_assert (immr >= 0);\n        }\n        break;\n      }\n      case 'N':\n      case 'Z': {\n        int n, n2;\n        uint64_t v;\n        struct imm immediates[4];\n\n        ch = *++p;\n        gen_assert ('0' <= ch && ch <= '3');\n        op = insn->ops[1];\n        n = ch - '0';\n        if (op.mode != MIR_OP_REF) {\n          v = op.u.u;\n        } else if (op.u.ref->item_type == MIR_data_item && op.u.ref->u.data->name != NULL\n                   && _MIR_reserved_ref_name_p (ctx, op.u.ref->u.data->name)) {\n          v = (uint64_t) op.u.ref->u.data->u.els;\n        } else {\n          v = (uint64_t) op.u.ref->addr;\n        }\n        n2 = movnzk_const (v, start_ch == 'N', immediates);\n        gen_assert (n < n2);\n        imm16 = immediates[n].v;\n        imm16_shift = immediates[n].shift >> 4;\n        break;\n      }\n      case 'I': {\n        ch = *++p;\n        if (ch == 'u') { /* Iu */\n          op = insn->ops[1];\n          gen_assert (op.mode == MIR_OP_INT || op.mode == MIR_OP_UINT);\n          imm12_shift = arithm_roundup_const (op.u.u, &imm12);\n        } else if (hex_value (ch) >= 0) {\n          immr = read_hex (&p);\n        } else { /* I */\n          op = insn->ops[2];\n          gen_assert (op.mode == MIR_OP_INT || op.mode == MIR_OP_UINT);\n          imm12_shift = arithm_const (op.u.u, &imm12);\n          p--;\n        }\n        break;\n      }\n      case 'i': {\n        p++;\n        gen_assert (hex_value (*p) >= 0);\n        imms = read_hex (&p);\n        break;\n      }\n      case 'T': {\n        gen_assert (!switch_table_addr_p && switch_table_adr_insn_start < 0);\n        switch_table_addr_p = TRUE;\n        break;\n      }\n      case 'l':\n      case 'L': {\n        int nop = 0;\n        if (insn->code == MIR_LADDR || insn->code == MIR_CALL || insn->code == MIR_INLINE) nop = 1;\n        op = insn->ops[nop];\n        gen_assert (op.mode == MIR_OP_LABEL || (start_ch == 'L' && op.mode == MIR_OP_REF));\n        lr.abs_addr_p = FALSE;\n        lr.short_p = start_ch == 'l';\n        lr.label_val_disp = 0;\n        if (jump_addrs == NULL)\n          lr.u.label = op.u.label;\n        else\n          lr.u.jump_addr = jump_addrs[0];\n        label_ref_num = VARR_LENGTH (label_ref_t, label_refs);\n        VARR_PUSH (label_ref_t, label_refs, lr);\n        break;\n      }\n      default: gen_assert (FALSE);\n      }\n    }\n\n    if (rd >= 0) {\n      gen_assert (rd <= 31);\n      opcode |= rd;\n      opcode_mask = check_and_set_mask (opcode_mask, 0x1f);\n    }\n    if (rn >= 0) {\n      gen_assert (rn <= 31);\n      opcode |= rn << 5;\n      opcode_mask = check_and_set_mask (opcode_mask, 0x1f << 5);\n    }\n    if (rm >= 0) {\n      gen_assert (rm <= 31);\n      opcode |= rm << 16;\n      opcode_mask = check_and_set_mask (opcode_mask, 0x1f << 16);\n    }\n    if (ra >= 0) {\n      gen_assert (rm <= 31);\n      opcode |= ra << 10;\n      opcode_mask = check_and_set_mask (opcode_mask, 0x1f << 10);\n    }\n    if (scale >= 0) {\n      opcode |= (scale == 1 ? 0x6 : 0x7) << 12;\n      opcode_mask = check_and_set_mask (opcode_mask, 0xf << 12);\n    }\n    if (disp >= 0) {\n      gen_assert (disp < (1 << 12));\n      opcode |= disp << 10;\n      opcode_mask = check_and_set_mask (opcode_mask, 0xfff << 10);\n    }\n    if (immr >= 0) {\n      gen_assert (immr < (1 << 6));\n      opcode |= immr << 16;\n      opcode_mask = check_and_set_mask (opcode_mask, 0x3f << 16);\n    }\n    if (imms >= 0) {\n      gen_assert (imms < (1 << 6));\n      opcode |= imms << 10;\n      opcode_mask = check_and_set_mask (opcode_mask, 0x3f << 10);\n    }\n    if (imm16 >= 0) {\n      gen_assert (imm16 < (1 << 16));\n      opcode |= imm16 << 5;\n      opcode_mask = check_and_set_mask (opcode_mask, 0xffff << 5);\n    }\n    if (imm16_shift >= 0) {\n      gen_assert (imm16_shift < (1 << 2));\n      opcode |= imm16_shift << 21;\n      opcode_mask = check_and_set_mask (opcode_mask, 0x3 << 21);\n    }\n    if (imm12 >= 0) {\n      gen_assert (imm12 < (1 << 12));\n      opcode |= imm12 << 10;\n      opcode_mask = check_and_set_mask (opcode_mask, 0xfff << 10);\n    }\n    if (imm12_shift >= 0) {\n      gen_assert (imm12_shift < (1 << 2));\n      opcode |= imm12_shift << 22;\n      opcode_mask = check_and_set_mask (opcode_mask, 0x3 << 22);\n    }\n    if (label_ref_num >= 0) VARR_ADDR (label_ref_t, label_refs)\n    [label_ref_num].label_val_disp = VARR_LENGTH (uint8_t, result_code);\n\n    if (switch_table_addr_p) switch_table_adr_insn_start = VARR_LENGTH (uint8_t, result_code);\n    put_uint64 (gen_ctx, opcode, 4); /* output the machine insn */\n\n    if (*p == 0) break;\n  }\n  if (switch_table_adr_insn_start < 0) return;\n  if (VARR_LENGTH (uint8_t, result_code) % 8 == 4) put_uint64 (gen_ctx, 0, 4);\n  offset = (VARR_LENGTH (uint8_t, result_code) - switch_table_adr_insn_start) / 4; /* pc offset */\n  *(uint32_t *) (VARR_ADDR (uint8_t, result_code) + switch_table_adr_insn_start) |= (offset << 5);\n  gen_assert (insn->code == MIR_SWITCH);\n  for (size_t i = 1; i < insn->nops; i++) {\n    gen_assert (insn->ops[i].mode == MIR_OP_LABEL);\n    lr.abs_addr_p = TRUE;\n    lr.short_p = FALSE;\n    lr.label_val_disp = VARR_LENGTH (uint8_t, result_code);\n    if (jump_addrs == NULL)\n      lr.u.label = insn->ops[i].u.label;\n    else\n      lr.u.jump_addr = jump_addrs[i - 1];\n    VARR_PUSH (label_ref_t, label_refs, lr);\n    put_uint64 (gen_ctx, 0, 8);\n  }\n}\n\nstatic int target_memory_ok_p (gen_ctx_t gen_ctx, MIR_op_t *op_ref) {\n  gen_assert (op_ref->mode == MIR_OP_VAR_MEM);\n  MIR_context_t ctx = gen_ctx->ctx;\n  size_t size = _MIR_type_size (ctx, op_ref->u.var_mem.type);\n  int scale = gen_int_log2 ((int64_t) size);\n\n  if (op_ref->u.var_mem.disp == 0\n      && ((op_ref->u.var_mem.index == MIR_NON_VAR || op_ref->u.var_mem.scale == 1\n           || op_ref->u.var_mem.scale == scale)))\n    return TRUE;\n  if (op_ref->u.var_mem.index == MIR_NON_VAR && op_ref->u.var_mem.disp >= 0\n      && op_ref->u.var_mem.disp % scale == 0 && op_ref->u.var_mem.disp / scale < (1 << 12))\n    return TRUE;\n  return FALSE;\n}\n\nstatic int target_insn_ok_p (gen_ctx_t gen_ctx, MIR_insn_t insn) {\n  return find_insn_pattern_replacement (gen_ctx, insn) != NULL;\n}\n\nstatic void target_split_insns (gen_ctx_t gen_ctx MIR_UNUSED) {}\n\nstatic uint8_t *target_translate (gen_ctx_t gen_ctx, size_t *len) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  size_t i;\n  MIR_insn_t insn;\n  const char *replacement;\n\n  gen_assert (curr_func_item->item_type == MIR_func_item);\n  VARR_TRUNC (uint8_t, result_code, 0);\n  VARR_TRUNC (label_ref_t, label_refs, 0);\n  VARR_TRUNC (uint64_t, abs_address_locs, 0);\n  for (insn = DLIST_HEAD (MIR_insn_t, curr_func_item->u.func->insns); insn != NULL;\n       insn = DLIST_NEXT (MIR_insn_t, insn)) {\n    if (insn->code == MIR_LABEL) {\n      set_label_disp (gen_ctx, insn, VARR_LENGTH (uint8_t, result_code));\n    } else if (insn->code != MIR_USE) {\n      replacement = find_insn_pattern_replacement (gen_ctx, insn);\n      if (replacement == NULL) {\n        fprintf (stderr, \"fatal failure in matching insn:\");\n        MIR_output_insn (ctx, stderr, insn, curr_func_item->u.func, TRUE);\n        exit (1);\n      } else {\n        gen_assert (replacement != NULL);\n        out_insn (gen_ctx, insn, replacement, NULL);\n      }\n    }\n  }\n  /* Setting up labels */\n  for (i = 0; i < VARR_LENGTH (label_ref_t, label_refs); i++) {\n    label_ref_t lr = VARR_GET (label_ref_t, label_refs, i);\n\n    if (!lr.abs_addr_p) {\n      int64_t offset = (int64_t) get_label_disp (gen_ctx, lr.u.label)\n                       - (int64_t) lr.label_val_disp; /* pc offset */\n      gen_assert ((offset & 0x3) == 0);\n      if (lr.short_p)\n        *(uint32_t *) (VARR_ADDR (uint8_t, result_code) + lr.label_val_disp)\n          |= ((offset / 4) & 0x7ffff) << 5; /* 19-bit */\n      else\n        *(uint32_t *) (VARR_ADDR (uint8_t, result_code) + lr.label_val_disp)\n          |= (offset / 4) & 0x3ffffff; /* 26-bit */\n    } else {\n      set_int64 (&VARR_ADDR (uint8_t, result_code)[lr.label_val_disp],\n                 (int64_t) get_label_disp (gen_ctx, lr.u.label), 8);\n      VARR_PUSH (uint64_t, abs_address_locs, lr.label_val_disp);\n    }\n  }\n  while (VARR_LENGTH (uint8_t, result_code) % 16 != 0) /* Align the pool */\n    VARR_PUSH (uint8_t, result_code, 0);\n  *len = VARR_LENGTH (uint8_t, result_code);\n  return VARR_ADDR (uint8_t, result_code);\n}\n\nstatic void target_rebase (gen_ctx_t gen_ctx, uint8_t *base) {\n  MIR_code_reloc_t reloc;\n\n  VARR_TRUNC (MIR_code_reloc_t, relocs, 0);\n  for (size_t i = 0; i < VARR_LENGTH (uint64_t, abs_address_locs); i++) {\n    reloc.offset = VARR_GET (uint64_t, abs_address_locs, i);\n    reloc.value = base + get_int64 (base + reloc.offset, 8);\n    VARR_PUSH (MIR_code_reloc_t, relocs, reloc);\n  }\n  _MIR_update_code_arr (gen_ctx->ctx, base, VARR_LENGTH (MIR_code_reloc_t, relocs),\n                        VARR_ADDR (MIR_code_reloc_t, relocs));\n  gen_setup_lrefs (gen_ctx, base);\n}\n\nstatic void target_change_to_direct_calls (MIR_context_t ctx MIR_UNUSED) {}\n\nstruct target_bb_version {\n  uint8_t *base;\n  label_ref_t branch_ref; /* label cand used for jump to this bb version */\n};\n\nstatic void target_init_bb_version_data (target_bb_version_t data) {\n  data->base = NULL; /* we don't know origin branch */\n}\n\nstatic void target_bb_translate_start (gen_ctx_t gen_ctx) {\n  short_bb_branch_p = FALSE;\n  VARR_TRUNC (uint8_t, result_code, 0);\n  VARR_TRUNC (label_ref_t, label_refs, 0);\n  VARR_TRUNC (uint64_t, abs_address_locs, 0);\n}\n\nstatic void target_bb_insn_translate (gen_ctx_t gen_ctx, MIR_insn_t insn, void **jump_addrs) {\n  const char *replacement;\n  if (insn->code == MIR_LABEL) return;\n  replacement = find_insn_pattern_replacement (gen_ctx, insn);\n  gen_assert (replacement != NULL);\n  out_insn (gen_ctx, insn, replacement, jump_addrs);\n  if (MIR_branch_code_p (insn->code) && insn->code != MIR_JMP) short_bb_branch_p = TRUE;\n}\n\nstatic void target_output_jump (gen_ctx_t gen_ctx, void **jump_addrs) {\n  out_insn (gen_ctx, temp_jump, temp_jump_replacement, jump_addrs);\n}\n\nstatic uint8_t *target_bb_translate_finish (gen_ctx_t gen_ctx, size_t *len) {\n  /* add nop for possible conversion short branch to branch and jump */\n  if (short_bb_branch_p) put_uint64 (gen_ctx, TARGET_NOP, 4);\n  while (VARR_LENGTH (uint8_t, result_code) % 16 != 0) /* Align the pool */\n    VARR_PUSH (uint8_t, result_code, 0);\n  *len = VARR_LENGTH (uint8_t, result_code);\n  return VARR_ADDR (uint8_t, result_code);\n}\n\nstatic void setup_rel (gen_ctx_t gen_ctx, label_ref_t *lr, uint8_t *base, void *addr) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  int64_t offset = (int64_t) addr - (int64_t) (base + lr->label_val_disp);\n\n  gen_assert ((offset & 0x3) == 0);\n  offset >>= 2;\n  /* check max 26-bit offset with possible branch conversion (see offset - 2): */\n  if (lr->abs_addr_p || !(-(1 << 25) <= (offset - 2) && offset < (1 << 25))) {\n    fprintf (stderr, \"too big offset (%lld) in setup_rel\", (long long) offset);\n    exit (1);\n  }\n  /* ??? thread safe: */\n  uint32_t *insn_ptr = (uint32_t *) (base + lr->label_val_disp), insn = *insn_ptr;\n  if (!lr->short_p) {\n    insn = (insn & ~0x3ffffff) | (offset & 0x3ffffff);\n  } else if (-(1 << 18) <= offset && offset < (1 << 18)) { /* 19 bit offset */\n    insn = (insn & ~(0x7ffff << 5)) | ((offset & 0x7ffff) << 5);\n  } else {\n    insn = (insn & ~(0x7ffff << 5)) | (2 << 5); /* skip jump */\n    uint32_t *nop_ptr = (uint32_t *) (base + lr->label_val_disp + 8);\n    gen_assert (TARGET_NOP == *nop_ptr || (*nop_ptr & ~0x3ffffff) == 0x14000000); /* nop or jump */\n    uint32_t jump_insn = 0x14000000 | ((offset - 2) & 0x3ffffff);\n    _MIR_change_code (ctx, (uint8_t *) nop_ptr, (uint8_t *) &jump_insn, 4);\n    lr->short_p = FALSE;\n    lr->label_val_disp += 8;\n  }\n  _MIR_change_code (ctx, (uint8_t *) insn_ptr, (uint8_t *) &insn, 4);\n}\n\nstatic void target_bb_rebase (gen_ctx_t gen_ctx, uint8_t *base) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  MIR_code_reloc_t reloc;\n\n  /* Setting up relative labels */\n  for (size_t i = 0; i < VARR_LENGTH (label_ref_t, label_refs); i++) {\n    label_ref_t lr = VARR_GET (label_ref_t, label_refs, i);\n    if (lr.abs_addr_p) {\n      _MIR_change_code (ctx, (uint8_t *) base + lr.label_val_disp, (uint8_t *) &lr.u.jump_addr, 8);\n    } else {\n      setup_rel (gen_ctx, &lr, base, lr.u.jump_addr);\n    }\n  }\n  VARR_TRUNC (MIR_code_reloc_t, relocs, 0);\n  for (size_t i = 0; i < VARR_LENGTH (uint64_t, abs_address_locs); i++) {\n    reloc.offset = VARR_GET (uint64_t, abs_address_locs, i);\n    reloc.value = base + get_int64 (base + reloc.offset, 8);\n    VARR_PUSH (MIR_code_reloc_t, relocs, reloc);\n  }\n  _MIR_update_code_arr (gen_ctx->ctx, base, VARR_LENGTH (MIR_code_reloc_t, relocs),\n                        VARR_ADDR (MIR_code_reloc_t, relocs));\n}\n\nstatic void target_setup_succ_bb_version_data (gen_ctx_t gen_ctx, uint8_t *base) {\n  if (VARR_LENGTH (label_ref_t, label_refs)\n      != VARR_LENGTH (target_bb_version_t, target_succ_bb_versions))\n    /* We can have more one possible branch from original insn\n       (e.g. SWITCH, FBNE).  If it is so, we will make jumps only\n       through BB thunk. */\n    return;\n  for (size_t i = 0; i < VARR_LENGTH (target_bb_version_t, target_succ_bb_versions); i++) {\n    target_bb_version_t data = VARR_GET (target_bb_version_t, target_succ_bb_versions, i);\n    if (data == NULL) continue;\n    data->branch_ref = VARR_GET (label_ref_t, label_refs, i);\n    data->base = base;\n  }\n}\n\nstatic void target_redirect_bb_origin_branch (gen_ctx_t gen_ctx, target_bb_version_t data,\n                                              void *addr) {\n  MIR_context_t ctx = gen_ctx->ctx;\n\n  if (data->base == NULL) return;\n  if (data->branch_ref.abs_addr_p) {\n    _MIR_change_code (ctx, (uint8_t *) data->base + data->branch_ref.label_val_disp,\n                      (uint8_t *) &addr, 8);\n  } else {\n    setup_rel (gen_ctx, &data->branch_ref, data->base, addr);\n  }\n  data->base = NULL;\n}\n\nstatic void target_init (gen_ctx_t gen_ctx) {\n  MIR_alloc_t alloc = gen_alloc (gen_ctx);\n  MIR_context_t ctx = gen_ctx->ctx;\n\n  gen_ctx->target_ctx = gen_malloc (gen_ctx, sizeof (struct target_ctx));\n  VARR_CREATE (uint8_t, result_code, alloc, 0);\n  VARR_CREATE (label_ref_t, label_refs, alloc, 0);\n  VARR_CREATE (uint64_t, abs_address_locs, alloc, 0);\n  VARR_CREATE (MIR_code_reloc_t, relocs, alloc, 0);\n  patterns_init (gen_ctx);\n  temp_jump = MIR_new_insn (ctx, MIR_JMP, MIR_new_label_op (ctx, NULL));\n  temp_jump_replacement = find_insn_pattern_replacement (gen_ctx, temp_jump);\n}\n\nstatic void target_finish (gen_ctx_t gen_ctx) {\n  MIR_alloc_t alloc = gen_alloc (gen_ctx);\n  patterns_finish (gen_ctx);\n  _MIR_free_insn (gen_ctx->ctx, temp_jump);\n  VARR_DESTROY (uint8_t, result_code);\n  VARR_DESTROY (label_ref_t, label_refs);\n  VARR_DESTROY (uint64_t, abs_address_locs);\n  VARR_DESTROY (MIR_code_reloc_t, relocs);\n  MIR_free (alloc, gen_ctx->target_ctx);\n  gen_ctx->target_ctx = NULL;\n}\n"
        },
        {
          "name": "mir-gen-ppc64.c",
          "type": "blob",
          "size": 109.7490234375,
          "content": "/* This file is a part of MIR project.\n   Copyright (C) 2020-2024 Vladimir Makarov <vmakarov.gcc@gmail.com>.\n*/\n\n#include \"mir-ppc64.h\"\n\n#include <limits.h>\n\n/* We don't use TOC.  So r2 is not necessary for the generated code.  */\nstatic void fancy_abort (int code) {\n  if (!code) abort ();\n}\n#undef gen_assert\n#define gen_assert(c) fancy_abort (c)\n\n#define TARGET_EXPAND_ADDO\n#define TARGET_EXPAND_ADDOS\n#define TARGET_EXPAND_UADDO\n#define TARGET_EXPAND_UADDOS\n#define TARGET_EXPAND_MULO\n#define TARGET_EXPAND_MULOS\n#define TARGET_EXPAND_UMULO\n#define TARGET_EXPAND_UMULOS\n\nstatic const MIR_reg_t LINK_HARD_REG = LR_HARD_REG;\n\nstatic inline MIR_reg_t target_nth_loc (MIR_reg_t loc, MIR_type_t type MIR_UNUSED, int n) {\n  return loc + n;\n}\n\nstatic inline int target_call_used_hard_reg_p (MIR_reg_t hard_reg, MIR_type_t type MIR_UNUSED) {\n  assert (hard_reg <= MAX_HARD_REG);\n  return ((R0_HARD_REG <= hard_reg && hard_reg <= R13_HARD_REG)\n          || (F0_HARD_REG <= hard_reg && hard_reg <= F13_HARD_REG));\n}\n\nstatic MIR_reg_t target_get_stack_slot_base_reg (gen_ctx_t gen_ctx MIR_UNUSED) {\n  return FP_HARD_REG;\n}\n\n/* Stack layout (r1(sp) refers to the last reserved stack slot\n   address) from higher address to lower address memory:\n\n        +-> Back chain                                    BE                 LE\n        |   Floating point register save area             optional        optional\n        |   General register save area                    optional        optional\n        |   VRSAVE save word (32-bits)                      0              NA\n        |   Alignment padding (4 or 12 bytes)\n        |   Vector register save area (quadword aligned)    we don't have it\n        |   Local variable space                          optional        optional\n        |   Parameter save area  (for callees)            (SP + 48)       (SP + 32) optional\n        |   TOC save area                                 (SP + 40)       (SP + 24)\n        |   link editor doubleword (we don't use it)      (SP + 32)          NA\n        |   compiler doubleword    (we don't use it)      (SP + 24)          NA\n        |   LR save area (used by callee)                 (SP + 16)       (SP + 16)\n        |   CR save area                                  (SP + 8)        (SP + 8)\nSP,R31->+-- Back chain                                    (SP + 0)        (SP + 0)\n            Alloca area (after that new 48 or 32 bytes header should be created with new values)\n\nOriginally SP(r1) and FP (r31) are the same but r1 can be changed by alloca */\n\n/* ppc64 has 3-ops insns */\nstatic const MIR_insn_code_t target_io_dup_op_insn_codes[] = {MIR_INSN_BOUND};\n\nstatic MIR_insn_code_t get_ext_code (MIR_type_t type) {\n  switch (type) {\n  case MIR_T_I8: return MIR_EXT8;\n  case MIR_T_U8: return MIR_UEXT8;\n  case MIR_T_I16: return MIR_EXT16;\n  case MIR_T_U16: return MIR_UEXT16;\n  case MIR_T_I32: return MIR_EXT32;\n  case MIR_T_U32: return MIR_UEXT32;\n  default: return MIR_INVALID_INSN;\n  }\n}\n\nstruct insn_pattern_info {\n  int start, num;\n};\n\ntypedef struct insn_pattern_info insn_pattern_info_t;\nDEF_VARR (insn_pattern_info_t);\n\nenum branch_type { BRCOND, JUMP, LADDR, BCTR };\nstruct label_ref {\n  int abs_addr_p;\n  enum branch_type branch_type;\n  size_t label_val_disp;\n  union {\n    MIR_label_t label;\n    void *jump_addr; /* absolute addr for BBV */\n  } u;\n};\n\ntypedef struct label_ref label_ref_t;\nDEF_VARR (label_ref_t);\n\nstruct target_ctx {\n  unsigned char alloca_p, block_arg_func_p, leaf_p, switch_p, laddr_p, short_bb_branch_p;\n  size_t param_save_area_size;\n  MIR_insn_t temp_jump;\n  const char *temp_jump_replacement;\n  VARR (int) * pattern_indexes;\n  VARR (insn_pattern_info_t) * insn_pattern_info;\n  VARR (uint8_t) * result_code;\n  VARR (label_ref_t) * label_refs;\n  VARR (uint64_t) * abs_address_locs;\n  VARR (MIR_code_reloc_t) * relocs;\n};\n\n#define alloca_p gen_ctx->target_ctx->alloca_p\n#define block_arg_func_p gen_ctx->target_ctx->block_arg_func_p\n#define leaf_p gen_ctx->target_ctx->leaf_p\n#define switch_p gen_ctx->target_ctx->switch_p\n#define laddr_p gen_ctx->target_ctx->laddr_p\n#define short_bb_branch_p gen_ctx->target_ctx->short_bb_branch_p\n#define param_save_area_size gen_ctx->target_ctx->param_save_area_size\n#define temp_jump gen_ctx->target_ctx->temp_jump\n#define temp_jump_replacement gen_ctx->target_ctx->temp_jump_replacement\n#define pattern_indexes gen_ctx->target_ctx->pattern_indexes\n#define insn_pattern_info gen_ctx->target_ctx->insn_pattern_info\n#define result_code gen_ctx->target_ctx->result_code\n#define label_refs gen_ctx->target_ctx->label_refs\n#define abs_address_locs gen_ctx->target_ctx->abs_address_locs\n#define relocs gen_ctx->target_ctx->relocs\n\nstatic void gen_mov (gen_ctx_t gen_ctx, MIR_insn_t anchor, MIR_insn_code_t code, MIR_op_t dst_op,\n                     MIR_op_t src_op) {\n  gen_add_insn_before (gen_ctx, anchor, MIR_new_insn (gen_ctx->ctx, code, dst_op, src_op));\n}\n\nstatic void mir_blk_mov (uint64_t *to, uint64_t *from, uint64_t nwords) {\n  for (; nwords > 0; nwords--) *to++ = *from++;\n}\n\nstatic const char *BLK_MOV = \"mir.blk_mov\";\nstatic const char *BLK_MOV_P = \"mir.blk_mov.p\";\n\nstatic void gen_blk_mov (gen_ctx_t gen_ctx, MIR_insn_t anchor, size_t to_disp,\n                         MIR_reg_t to_base_hard_reg, size_t from_disp, MIR_reg_t from_base_reg,\n                         size_t qwords, int save_regs) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  MIR_func_t func = curr_func_item->u.func;\n  MIR_item_t proto_item, func_import_item;\n  MIR_insn_t new_insn;\n  MIR_op_t ops[5], freg_op, treg_op, treg_op2, treg_op3;\n\n  treg_op = _MIR_new_var_op (ctx, gen_new_temp_reg (gen_ctx, MIR_T_I64, func));\n  if (qwords <= 16) {\n    for (; qwords > 0; qwords--, to_disp += 8, from_disp += 8) {\n      gen_mov (gen_ctx, anchor, MIR_MOV, treg_op,\n               _MIR_new_var_mem_op (ctx, MIR_T_I64, from_disp, from_base_reg, MIR_NON_VAR, 1));\n      gen_mov (gen_ctx, anchor, MIR_MOV,\n               _MIR_new_var_mem_op (ctx, MIR_T_I64, to_disp, to_base_hard_reg, MIR_NON_VAR, 1),\n               treg_op);\n    }\n    return;\n  }\n  treg_op2 = _MIR_new_var_op (ctx, gen_new_temp_reg (gen_ctx, MIR_T_I64, func));\n  treg_op3 = _MIR_new_var_op (ctx, gen_new_temp_reg (gen_ctx, MIR_T_I64, func));\n  /* Save arg regs: */\n  if (save_regs > 0) gen_mov (gen_ctx, anchor, MIR_MOV, treg_op, _MIR_new_var_op (ctx, 3));\n  if (save_regs > 1) gen_mov (gen_ctx, anchor, MIR_MOV, treg_op2, _MIR_new_var_op (ctx, 4));\n  if (save_regs > 2) gen_mov (gen_ctx, anchor, MIR_MOV, treg_op3, _MIR_new_var_op (ctx, 5));\n  /* call blk move: */\n  proto_item = _MIR_builtin_proto (ctx, curr_func_item->module, BLK_MOV_P, 0, NULL, 3, MIR_T_I64,\n                                   \"to\", MIR_T_I64, \"from\", MIR_T_I64, \"nwords\");\n  func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, BLK_MOV, mir_blk_mov);\n  freg_op = _MIR_new_var_op (ctx, gen_new_temp_reg (gen_ctx, MIR_T_I64, func));\n  new_insn = MIR_new_insn (ctx, MIR_MOV, freg_op, MIR_new_ref_op (ctx, func_import_item));\n  gen_add_insn_before (gen_ctx, anchor, new_insn);\n  gen_add_insn_before (gen_ctx, anchor,\n                       MIR_new_insn (gen_ctx->ctx, MIR_ADD, _MIR_new_var_op (ctx, 3),\n                                     _MIR_new_var_op (ctx, to_base_hard_reg),\n                                     MIR_new_int_op (ctx, to_disp)));\n  gen_add_insn_before (gen_ctx, anchor,\n                       MIR_new_insn (gen_ctx->ctx, MIR_ADD, _MIR_new_var_op (ctx, 4),\n                                     _MIR_new_var_op (ctx, from_base_reg),\n                                     MIR_new_int_op (ctx, from_disp)));\n  gen_mov (gen_ctx, anchor, MIR_MOV, _MIR_new_var_op (ctx, 5), MIR_new_int_op (ctx, qwords));\n  ops[0] = MIR_new_ref_op (ctx, proto_item);\n  ops[1] = freg_op;\n  ops[2] = _MIR_new_var_op (ctx, 3);\n  ops[3] = _MIR_new_var_op (ctx, 4);\n  ops[4] = _MIR_new_var_op (ctx, 5);\n  new_insn = MIR_new_insn_arr (ctx, MIR_CALL, 5, ops);\n  gen_add_insn_before (gen_ctx, anchor, new_insn);\n  /* Restore arg regs: */\n  if (save_regs > 0) gen_mov (gen_ctx, anchor, MIR_MOV, _MIR_new_var_op (ctx, 3), treg_op);\n  if (save_regs > 1) gen_mov (gen_ctx, anchor, MIR_MOV, _MIR_new_var_op (ctx, 4), treg_op2);\n  if (save_regs > 2) gen_mov (gen_ctx, anchor, MIR_MOV, _MIR_new_var_op (ctx, 5), treg_op3);\n}\n\nstatic void machinize_call (gen_ctx_t gen_ctx, MIR_insn_t call_insn) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  MIR_func_t func = curr_func_item->u.func;\n  MIR_proto_t proto = call_insn->ops[0].u.ref->u.proto;\n  int vararg_p = proto->vararg_p;\n  size_t qwords, disp, nargs, nops = MIR_insn_nops (ctx, call_insn), start = proto->nres + 2;\n  size_t mem_size = 0, n_iregs = 0, n_fregs = 0;\n  MIR_type_t type, mem_type;\n  MIR_op_mode_t mode;\n  MIR_var_t *arg_vars = NULL;\n  MIR_reg_t ret_reg;\n  MIR_op_t arg_op, temp_op, arg_reg_op, ret_reg_op, mem_op;\n  MIR_insn_code_t new_insn_code, ext_code;\n  MIR_insn_t new_insn, ext_insn;\n\n  if (call_insn->code == MIR_INLINE) call_insn->code = MIR_CALL;\n  if (proto->args == NULL) {\n    nargs = 0;\n  } else {\n    gen_assert (nops >= VARR_LENGTH (MIR_var_t, proto->args)\n                && (vararg_p || nops - start == VARR_LENGTH (MIR_var_t, proto->args)));\n    nargs = VARR_LENGTH (MIR_var_t, proto->args);\n    arg_vars = VARR_ADDR (MIR_var_t, proto->args);\n  }\n  if (call_insn->ops[1].mode != MIR_OP_VAR) {\n    // ??? to optimize (can be immediate operand for func call)\n    temp_op = _MIR_new_var_op (ctx, gen_new_temp_reg (gen_ctx, MIR_T_I64, func));\n    new_insn = MIR_new_insn (ctx, MIR_MOV, temp_op, call_insn->ops[1]);\n    call_insn->ops[1] = temp_op;\n    gen_add_insn_before (gen_ctx, call_insn, new_insn);\n  }\n  for (size_t i = start; i < nops; i++) {\n    arg_op = call_insn->ops[i];\n    gen_assert (arg_op.mode == MIR_OP_VAR\n                || (arg_op.mode == MIR_OP_VAR_MEM && MIR_all_blk_type_p (arg_op.u.mem.type)));\n    if (i - start < nargs) {\n      type = arg_vars[i - start].type;\n    } else if (call_insn->ops[i].mode == MIR_OP_VAR_MEM) {\n      type = arg_op.u.mem.type;\n      gen_assert (MIR_all_blk_type_p (type));\n    } else {\n      mode = call_insn->ops[i].value_mode;  // ??? smaller ints\n      gen_assert (mode == MIR_OP_INT || mode == MIR_OP_UINT || mode == MIR_OP_FLOAT\n                  || mode == MIR_OP_DOUBLE || mode == MIR_OP_LDOUBLE);\n      if (mode == MIR_OP_FLOAT)\n        (*MIR_get_error_func (ctx)) (MIR_call_op_error,\n                                     \"passing float variadic arg (should be passed as double)\");\n      type = mode == MIR_OP_DOUBLE ? MIR_T_D : mode == MIR_OP_LDOUBLE ? MIR_T_LD : MIR_T_I64;\n    }\n    ext_insn = NULL;\n    if ((ext_code = get_ext_code (type)) != MIR_INVALID_INSN) { /* extend arg if necessary */\n      temp_op = _MIR_new_var_op (ctx, gen_new_temp_reg (gen_ctx, MIR_T_I64, func));\n      ext_insn = MIR_new_insn (ctx, ext_code, temp_op, arg_op);\n      call_insn->ops[i] = arg_op = temp_op;\n    }\n    mem_type = type == MIR_T_F || type == MIR_T_D || type == MIR_T_LD ? type : MIR_T_I64;  // ???\n    if ((type == MIR_T_F || type == MIR_T_D || type == MIR_T_LD) && n_fregs < 13) {\n      /* put arguments to argument hard regs */\n      if (ext_insn != NULL) gen_add_insn_before (gen_ctx, call_insn, ext_insn);\n      arg_reg_op = _MIR_new_var_op (ctx, F1_HARD_REG + n_fregs);\n      gen_mov (gen_ctx, call_insn,\n               type == MIR_T_F   ? MIR_FMOV\n               : type == MIR_T_D ? MIR_DMOV\n                                 : MIR_LDMOV,  // ???\n               arg_reg_op, arg_op);\n      call_insn->ops[i] = arg_reg_op;\n      if (vararg_p) {                                             // ??? dead insns\n        if (n_iregs >= 8 || (type == MIR_T_LD && n_iregs == 7)) { /* store in memory too */\n          mem_op = _MIR_new_var_mem_op (ctx, mem_type, mem_size + PPC64_STACK_HEADER_SIZE,\n                                        SP_HARD_REG, MIR_NON_VAR, 1);\n          gen_assert (n_fregs < 12);\n          gen_mov (gen_ctx, call_insn, type == MIR_T_LD ? MIR_LDMOV : MIR_DMOV, mem_op, arg_reg_op);\n        }\n        if (n_iregs < 8) { /* load into gp reg too */\n          mem_op = _MIR_new_var_mem_op (ctx, mem_type, -16, SP_HARD_REG, MIR_NON_VAR, 1);\n          gen_mov (gen_ctx, call_insn, type == MIR_T_LD ? MIR_LDMOV : MIR_DMOV, mem_op, arg_reg_op);\n          mem_type = mem_type == MIR_T_F ? MIR_T_I32 : MIR_T_I64;  // ???\n          mem_op = _MIR_new_var_mem_op (ctx, mem_type, -16, SP_HARD_REG, MIR_NON_VAR, 1);\n          arg_reg_op = _MIR_new_var_op (ctx, R3_HARD_REG + n_iregs);\n          gen_mov (gen_ctx, call_insn, MIR_MOV, arg_reg_op, mem_op);\n          call_insn->ops[i] = arg_reg_op; /* keep it alive */\n          if (type == MIR_T_LD && n_iregs + 1 < 8) {\n            mem_op = _MIR_new_var_mem_op (ctx, mem_type, -8, SP_HARD_REG, MIR_NON_VAR, 1);\n            gen_mov (gen_ctx, call_insn, MIR_MOV, _MIR_new_var_op (ctx, R3_HARD_REG + n_iregs + 1),\n                     mem_op);\n          }\n        }\n      }\n      n_fregs += type == MIR_T_LD ? 2 : 1;\n    } else if (MIR_blk_type_p (type)) {\n      gen_assert (arg_op.mode == MIR_OP_VAR_MEM && arg_op.u.mem.disp >= 0\n                  && arg_op.u.mem.index == MIR_NON_VAR);\n      qwords = (arg_op.u.mem.disp + 7) / 8;\n      for (disp = 0; qwords > 0 && n_iregs < 8; qwords--, n_iregs++, mem_size += 8, disp += 8) {\n        arg_reg_op = _MIR_new_var_op (ctx, R3_HARD_REG + n_iregs);\n        gen_mov (gen_ctx, call_insn, MIR_MOV, arg_reg_op,\n                 _MIR_new_var_mem_op (ctx, MIR_T_I64, disp, arg_op.u.mem.base, MIR_NON_VAR, 1));\n        setup_call_hard_reg_args (gen_ctx, call_insn, R3_HARD_REG + n_iregs);\n      }\n      if (qwords > 0)\n        gen_blk_mov (gen_ctx, call_insn, mem_size + PPC64_STACK_HEADER_SIZE, SP_HARD_REG, disp,\n                     arg_op.u.mem.base, qwords, n_iregs);\n      mem_size += qwords * 8;\n      n_iregs += qwords;\n      continue;\n    } else if (type != MIR_T_F && type != MIR_T_D && type != MIR_T_LD && n_iregs < 8) {\n      if (ext_insn != NULL) gen_add_insn_before (gen_ctx, call_insn, ext_insn);\n      arg_reg_op = _MIR_new_var_op (ctx, R3_HARD_REG + n_iregs);\n      if (type != MIR_T_RBLK) {\n        gen_mov (gen_ctx, call_insn, MIR_MOV, arg_reg_op, arg_op);\n      } else {\n        assert (arg_op.mode == MIR_OP_VAR_MEM);\n        gen_mov (gen_ctx, call_insn, MIR_MOV, arg_reg_op, _MIR_new_var_op (ctx, arg_op.u.mem.base));\n        arg_reg_op = _MIR_new_var_mem_op (ctx, MIR_T_RBLK, arg_op.u.mem.disp, R3_HARD_REG + n_iregs,\n                                          MIR_NON_VAR, 1);\n      }\n      call_insn->ops[i] = arg_reg_op;\n    } else { /* put arguments on the stack */\n      if (ext_insn != NULL) gen_add_insn_before (gen_ctx, call_insn, ext_insn);\n      new_insn_code = (type == MIR_T_F    ? MIR_FMOV\n                       : type == MIR_T_D  ? MIR_DMOV\n                       : type == MIR_T_LD ? MIR_LDMOV\n                                          : MIR_MOV);\n      mem_op = _MIR_new_var_mem_op (ctx, mem_type, mem_size + PPC64_STACK_HEADER_SIZE, SP_HARD_REG,\n                                    MIR_NON_VAR, 1);\n      if (type != MIR_T_RBLK) {\n        gen_mov (gen_ctx, call_insn, new_insn_code, mem_op, arg_op);\n      } else {\n        assert (arg_op.mode == MIR_OP_VAR_MEM);\n        gen_mov (gen_ctx, call_insn, new_insn_code, mem_op,\n                 _MIR_new_var_op (ctx, arg_op.u.mem.base));\n      }\n      call_insn->ops[i] = mem_op;\n    }\n    mem_size += type == MIR_T_LD ? 16 : 8;\n    n_iregs += type == MIR_T_LD ? 2 : 1;\n  }\n  if (vararg_p && mem_size < 64) mem_size = 64; /* to save all arg gprs  */\n  if (param_save_area_size < mem_size) param_save_area_size = mem_size;\n  n_iregs = n_fregs = 0;\n  for (size_t i = 0; i < proto->nres; i++) {\n    ret_reg_op = call_insn->ops[i + 2];\n    gen_assert (ret_reg_op.mode == MIR_OP_VAR);\n    type = proto->res_types[i];\n    if (((type == MIR_T_F || type == MIR_T_D) && n_fregs < 4)\n        || (type == MIR_T_LD && n_fregs < 3)) {\n      new_insn_code = type == MIR_T_F ? MIR_FMOV : type == MIR_T_D ? MIR_DMOV : MIR_LDMOV;\n      ret_reg = F1_HARD_REG + n_fregs++;\n    } else if (n_iregs < 8) {\n      new_insn_code = MIR_MOV;\n      ret_reg = R3_HARD_REG + n_iregs++;\n    } else {\n      (*MIR_get_error_func (ctx)) (MIR_ret_error,\n                                   \"ppc64 can not handle this combination of return values\");\n    }\n    new_insn = MIR_new_insn (ctx, new_insn_code, ret_reg_op, _MIR_new_var_op (ctx, ret_reg));\n    MIR_insert_insn_after (ctx, curr_func_item, call_insn, new_insn);\n    call_insn->ops[i + 2] = new_insn->ops[1];\n    if ((ext_code = get_ext_code (type)) != MIR_INVALID_INSN) {\n      MIR_insert_insn_after (ctx, curr_func_item, new_insn,\n                             MIR_new_insn (ctx, ext_code, ret_reg_op, ret_reg_op));\n      new_insn = DLIST_NEXT (MIR_insn_t, new_insn);\n    }\n    create_new_bb_insns (gen_ctx, call_insn, DLIST_NEXT (MIR_insn_t, new_insn), call_insn);\n  }\n}\n\nstatic long double mir_i2ld (int64_t i) { return i; }\nstatic const char *I2LD = \"mir.i2ld\";\nstatic const char *I2LD_P = \"mir.i2ld.p\";\n\nstatic long double mir_ui2ld (uint64_t i) { return i; }\nstatic const char *UI2LD = \"mir.ui2ld\";\nstatic const char *UI2LD_P = \"mir.ui2ld.p\";\n\nstatic long double mir_f2ld (float f) { return f; }\nstatic const char *F2LD = \"mir.f2ld\";\nstatic const char *F2LD_P = \"mir.f2ld.p\";\n\nstatic long double mir_d2ld (double d) { return d; }\nstatic const char *D2LD = \"mir.d2ld\";\nstatic const char *D2LD_P = \"mir.d2ld.p\";\n\nstatic int64_t mir_ld2i (long double ld) { return ld; }\nstatic const char *LD2I = \"mir.ld2i\";\nstatic const char *LD2I_P = \"mir.ld2i.p\";\n\nstatic float mir_ld2f (long double ld) { return ld; }\nstatic const char *LD2F = \"mir.ld2f\";\nstatic const char *LD2F_P = \"mir.ld2f.p\";\n\nstatic double mir_ld2d (long double ld) { return ld; }\nstatic const char *LD2D = \"mir.ld2d\";\nstatic const char *LD2D_P = \"mir.ld2d.p\";\n\nstatic long double mir_ldadd (long double d1, long double d2) { return d1 + d2; }\nstatic const char *LDADD = \"mir.ldadd\";\nstatic const char *LDADD_P = \"mir.ldadd.p\";\n\nstatic long double mir_ldsub (long double d1, long double d2) { return d1 - d2; }\nstatic const char *LDSUB = \"mir.ldsub\";\nstatic const char *LDSUB_P = \"mir.ldsub.p\";\n\nstatic long double mir_ldmul (long double d1, long double d2) { return d1 * d2; }\nstatic const char *LDMUL = \"mir.ldmul\";\nstatic const char *LDMUL_P = \"mir.ldmul.p\";\n\nstatic long double mir_lddiv (long double d1, long double d2) { return d1 / d2; }\nstatic const char *LDDIV = \"mir.lddiv\";\nstatic const char *LDDIV_P = \"mir.lddiv.p\";\n\nstatic long double mir_ldneg (long double d) { return -d; }\nstatic const char *LDNEG = \"mir.ldneg\";\nstatic const char *LDNEG_P = \"mir.ldneg.p\";\n\nstatic const char *VA_ARG_P = \"mir.va_arg.p\";\nstatic const char *VA_ARG = \"mir.va_arg\";\nstatic const char *VA_BLOCK_ARG_P = \"mir.va_block_arg.p\";\nstatic const char *VA_BLOCK_ARG = \"mir.va_block_arg\";\n\nstatic int64_t mir_ldeq (long double d1, long double d2) { return d1 == d2; }\nstatic const char *LDEQ = \"mir.ldeq\";\nstatic const char *LDEQ_P = \"mir.ldeq.p\";\n\nstatic int64_t mir_ldne (long double d1, long double d2) { return d1 != d2; }\nstatic const char *LDNE = \"mir.ldne\";\nstatic const char *LDNE_P = \"mir.ldne.p\";\n\nstatic int64_t mir_ldlt (long double d1, long double d2) { return d1 < d2; }\nstatic const char *LDLT = \"mir.ldlt\";\nstatic const char *LDLT_P = \"mir.ldlt.p\";\n\nstatic int64_t mir_ldge (long double d1, long double d2) { return d1 >= d2; }\nstatic const char *LDGE = \"mir.ldge\";\nstatic const char *LDGE_P = \"mir.ldge.p\";\n\nstatic int64_t mir_ldgt (long double d1, long double d2) { return d1 > d2; }\nstatic const char *LDGT = \"mir.ldgt\";\nstatic const char *LDGT_P = \"mir.ldgt.p\";\n\nstatic int64_t mir_ldle (long double d1, long double d2) { return d1 <= d2; }\nstatic const char *LDLE = \"mir.ldle\";\nstatic const char *LDLE_P = \"mir.ldle.p\";\n\nstatic int get_builtin (gen_ctx_t gen_ctx, MIR_insn_code_t code, MIR_item_t *proto_item,\n                        MIR_item_t *func_import_item) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  MIR_type_t res_type;\n\n  *func_import_item = *proto_item = NULL; /* to remove uninitialized warning */\n  switch (code) {\n  case MIR_I2LD:\n    res_type = MIR_T_LD;\n    *proto_item\n      = _MIR_builtin_proto (ctx, curr_func_item->module, I2LD_P, 1, &res_type, 1, MIR_T_I64, \"v\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, I2LD, mir_i2ld);\n    return 1;\n  case MIR_UI2LD:\n    res_type = MIR_T_LD;\n    *proto_item\n      = _MIR_builtin_proto (ctx, curr_func_item->module, UI2LD_P, 1, &res_type, 1, MIR_T_I64, \"v\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, UI2LD, mir_ui2ld);\n    return 1;\n  case MIR_F2LD:\n    res_type = MIR_T_LD;\n    *proto_item\n      = _MIR_builtin_proto (ctx, curr_func_item->module, F2LD_P, 1, &res_type, 1, MIR_T_F, \"v\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, F2LD, mir_f2ld);\n    return 1;\n  case MIR_D2LD:\n    res_type = MIR_T_LD;\n    *proto_item\n      = _MIR_builtin_proto (ctx, curr_func_item->module, D2LD_P, 1, &res_type, 1, MIR_T_D, \"v\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, D2LD, mir_d2ld);\n    return 1;\n  case MIR_LD2I:\n    res_type = MIR_T_I64;\n    *proto_item\n      = _MIR_builtin_proto (ctx, curr_func_item->module, LD2I_P, 1, &res_type, 1, MIR_T_LD, \"v\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, LD2I, mir_ld2i);\n    return 1;\n  case MIR_LD2F:\n    res_type = MIR_T_F;\n    *proto_item\n      = _MIR_builtin_proto (ctx, curr_func_item->module, LD2F_P, 1, &res_type, 1, MIR_T_LD, \"v\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, LD2F, mir_ld2f);\n    return 1;\n  case MIR_LD2D:\n    res_type = MIR_T_D;\n    *proto_item\n      = _MIR_builtin_proto (ctx, curr_func_item->module, LD2D_P, 1, &res_type, 1, MIR_T_LD, \"v\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, LD2D, mir_ld2d);\n    return 1;\n  case MIR_LDADD:\n    res_type = MIR_T_LD;\n    *proto_item = _MIR_builtin_proto (ctx, curr_func_item->module, LDADD_P, 1, &res_type, 2,\n                                      MIR_T_LD, \"d1\", MIR_T_LD, \"d2\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, LDADD, mir_ldadd);\n    return 2;\n  case MIR_LDSUB:\n    res_type = MIR_T_LD;\n    *proto_item = _MIR_builtin_proto (ctx, curr_func_item->module, LDSUB_P, 1, &res_type, 2,\n                                      MIR_T_LD, \"d1\", MIR_T_LD, \"d2\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, LDSUB, mir_ldsub);\n    return 2;\n  case MIR_LDMUL:\n    res_type = MIR_T_LD;\n    *proto_item = _MIR_builtin_proto (ctx, curr_func_item->module, LDMUL_P, 1, &res_type, 2,\n                                      MIR_T_LD, \"d1\", MIR_T_LD, \"d2\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, LDMUL, mir_ldmul);\n    return 2;\n  case MIR_LDDIV:\n    res_type = MIR_T_LD;\n    *proto_item = _MIR_builtin_proto (ctx, curr_func_item->module, LDDIV_P, 1, &res_type, 2,\n                                      MIR_T_LD, \"d1\", MIR_T_LD, \"d2\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, LDDIV, mir_lddiv);\n    return 2;\n  case MIR_LDNEG:\n    res_type = MIR_T_LD;\n    *proto_item\n      = _MIR_builtin_proto (ctx, curr_func_item->module, LDNEG_P, 1, &res_type, 1, MIR_T_LD, \"d\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, LDNEG, mir_ldneg);\n    return 1;\n  case MIR_LDEQ:\n    res_type = MIR_T_I64;\n    *proto_item = _MIR_builtin_proto (ctx, curr_func_item->module, LDEQ_P, 1, &res_type, 2,\n                                      MIR_T_LD, \"d1\", MIR_T_LD, \"d2\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, LDEQ, mir_ldeq);\n    return 2;\n  case MIR_LDNE:\n    res_type = MIR_T_I64;\n    *proto_item = _MIR_builtin_proto (ctx, curr_func_item->module, LDNE_P, 1, &res_type, 2,\n                                      MIR_T_LD, \"d1\", MIR_T_LD, \"d2\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, LDNE, mir_ldne);\n    return 2;\n  case MIR_LDLT:\n    res_type = MIR_T_I64;\n    *proto_item = _MIR_builtin_proto (ctx, curr_func_item->module, LDLT_P, 1, &res_type, 2,\n                                      MIR_T_LD, \"d1\", MIR_T_LD, \"d2\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, LDLT, mir_ldlt);\n    return 2;\n  case MIR_LDGE:\n    res_type = MIR_T_I64;\n    *proto_item = _MIR_builtin_proto (ctx, curr_func_item->module, LDGE_P, 1, &res_type, 2,\n                                      MIR_T_LD, \"d1\", MIR_T_LD, \"d2\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, LDGE, mir_ldge);\n    return 2;\n  case MIR_LDGT:\n    res_type = MIR_T_I64;\n    *proto_item = _MIR_builtin_proto (ctx, curr_func_item->module, LDGT_P, 1, &res_type, 2,\n                                      MIR_T_LD, \"d1\", MIR_T_LD, \"d2\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, LDGT, mir_ldgt);\n    return 2;\n  case MIR_LDLE:\n    res_type = MIR_T_I64;\n    *proto_item = _MIR_builtin_proto (ctx, curr_func_item->module, LDLE_P, 1, &res_type, 2,\n                                      MIR_T_LD, \"d1\", MIR_T_LD, \"d2\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, LDLE, mir_ldle);\n    return 2;\n  case MIR_VA_ARG:\n    res_type = MIR_T_I64;\n    *proto_item = _MIR_builtin_proto (ctx, curr_func_item->module, VA_ARG_P, 1, &res_type, 2,\n                                      MIR_T_I64, \"va\", MIR_T_I64, \"type\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, VA_ARG, va_arg_builtin);\n    return 2;\n  case MIR_VA_BLOCK_ARG:\n    *proto_item\n      = _MIR_builtin_proto (ctx, curr_func_item->module, VA_BLOCK_ARG_P, 0, NULL, 4, MIR_T_I64,\n                            \"res\", MIR_T_I64, \"va\", MIR_T_I64, \"size\", MIR_T_I64, \"ncase\");\n    *func_import_item\n      = _MIR_builtin_func (ctx, curr_func_item->module, VA_BLOCK_ARG, va_block_arg_builtin);\n    return 4;\n  default: return 0;\n  }\n}\n\nstatic MIR_disp_t target_get_stack_slot_offset (gen_ctx_t gen_ctx, MIR_type_t type MIR_UNUSED,\n                                                MIR_reg_t slot) {\n  /* slot is 0, 1, ... */\n  return ((MIR_disp_t) slot * 8 + PPC64_STACK_HEADER_SIZE + param_save_area_size);\n}\n\nstatic void set_prev_sp_op (gen_ctx_t gen_ctx, MIR_insn_t anchor, MIR_op_t *prev_sp_op) {\n  if (!block_arg_func_p) {\n    /* don't use r11 as we can have spilled param<-mem in param set up which needs r11 as a temp */\n    block_arg_func_p = TRUE;\n    *prev_sp_op = _MIR_new_var_op (gen_ctx->ctx, R12_HARD_REG);\n    gen_mov (gen_ctx, anchor, MIR_MOV, *prev_sp_op,\n             _MIR_new_var_mem_op (gen_ctx->ctx, MIR_T_I64, 0, SP_HARD_REG, MIR_NON_VAR, 1));\n  }\n}\n\nstatic int target_valid_mem_offset_p (gen_ctx_t gen_ctx MIR_UNUSED, MIR_type_t type MIR_UNUSED,\n                                      MIR_disp_t offset MIR_UNUSED) {\n  return TRUE;\n}\n\nstatic void target_machinize (gen_ctx_t gen_ctx) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  MIR_func_t func;\n  MIR_type_t type, res_type;\n  MIR_insn_code_t code, new_insn_code;\n  MIR_insn_t insn, next_insn, new_insn, anchor;\n  MIR_reg_t ret_reg;\n  MIR_op_t ret_reg_op, arg_reg_op, prev_sp_op, temp_op, arg_var_op;\n  size_t i, int_arg_num, fp_arg_num, disp, var_args_start, qwords, offset;\n\n  assert (curr_func_item->item_type == MIR_func_item);\n  func = curr_func_item->u.func;\n  block_arg_func_p = FALSE;\n  param_save_area_size = 0;\n  anchor = DLIST_HEAD (MIR_insn_t, func->insns);\n  if (func->vararg_p)\n    set_prev_sp_op (gen_ctx, anchor, &prev_sp_op); /* arg can be taken from memory */\n  disp = PPC64_STACK_HEADER_SIZE;                  /* param area start in the caller frame */\n  for (i = int_arg_num = fp_arg_num = 0; i < func->nargs; i++) {\n    /* Argument extensions is already done in simplify */\n    /* Prologue: generate arg_var = hard_reg|stack mem ... */\n    type = VARR_GET (MIR_var_t, func->vars, i).type;\n    arg_var_op = _MIR_new_var_op (ctx, i + MAX_HARD_REG + 1);\n    if ((type == MIR_T_F || type == MIR_T_D || type == MIR_T_LD) && fp_arg_num < 13) {\n      if (type == MIR_T_LD && fp_arg_num == 12) { /* dmov f14,disp(r1) -> lfd f14,disp(r1): */\n        set_prev_sp_op (gen_ctx, anchor, &prev_sp_op);\n        arg_reg_op = _MIR_new_var_op (ctx, F14_HARD_REG);\n        gen_mov (gen_ctx, anchor, MIR_DMOV, arg_reg_op,\n                 _MIR_new_var_mem_op (ctx, MIR_T_D, disp + 8, R12_HARD_REG, MIR_NON_VAR, 1));\n      }\n      arg_reg_op = _MIR_new_var_op (ctx, F1_HARD_REG + fp_arg_num);\n      gen_mov (gen_ctx, anchor,\n               type == MIR_T_F   ? MIR_FMOV\n               : type == MIR_T_D ? MIR_DMOV\n                                 : MIR_LDMOV,\n               arg_var_op, arg_reg_op); /* (f|d|ld|)mov arg, arg_hard_reg */\n      fp_arg_num += type == MIR_T_LD ? 2 : 1;\n    } else if (type == MIR_T_F || type == MIR_T_D\n               || type == MIR_T_LD) { /* (f|d|ld|)mov arg, arg_memory */\n      set_prev_sp_op (gen_ctx, anchor, &prev_sp_op);\n      gen_mov (gen_ctx, anchor,\n               type == MIR_T_F   ? MIR_FMOV\n               : type == MIR_T_D ? MIR_DMOV\n                                 : MIR_LDMOV,\n               arg_var_op, _MIR_new_var_mem_op (ctx, type, disp, R12_HARD_REG, MIR_NON_VAR, 1));\n    } else if (MIR_blk_type_p (type)) {\n      qwords = (VARR_GET (MIR_var_t, func->vars, i).size + 7) / 8;\n      offset = int_arg_num < 8 ? PPC64_STACK_HEADER_SIZE + int_arg_num * 8 : disp;\n      set_prev_sp_op (gen_ctx, anchor, &prev_sp_op);\n      for (; qwords > 0 && int_arg_num < 8; qwords--, int_arg_num++, disp += 8) {\n        if (!func->vararg_p)\n          gen_mov (gen_ctx, anchor, MIR_MOV,\n                   _MIR_new_var_mem_op (ctx, MIR_T_I64, PPC64_STACK_HEADER_SIZE + int_arg_num * 8,\n                                        R12_HARD_REG, MIR_NON_VAR, 1),\n                   _MIR_new_var_op (ctx, R3_HARD_REG + int_arg_num));\n      }\n      gen_add_insn_before (gen_ctx, anchor,\n                           MIR_new_insn (ctx, MIR_ADD, arg_var_op,\n                                         _MIR_new_var_op (ctx, R12_HARD_REG),\n                                         MIR_new_int_op (ctx, offset)));\n      disp += qwords * 8;\n      int_arg_num += qwords;\n      continue;\n    } else if (int_arg_num < 8) { /* mov arg, arg_hard_reg */\n      arg_reg_op = _MIR_new_var_op (ctx, R3_HARD_REG + int_arg_num);\n      gen_mov (gen_ctx, anchor, MIR_MOV, arg_var_op, arg_reg_op);\n    } else { /* mov arg, arg_memory */\n      set_prev_sp_op (gen_ctx, anchor, &prev_sp_op);\n      gen_mov (gen_ctx, anchor, MIR_MOV, arg_var_op,\n               _MIR_new_var_mem_op (ctx, MIR_T_I64, disp, R12_HARD_REG, MIR_NON_VAR, 1));\n    }\n    disp += type == MIR_T_LD ? 16 : 8;\n    int_arg_num += type == MIR_T_LD ? 2 : 1;\n  }\n  var_args_start = disp;\n  switch_p = laddr_p = alloca_p = FALSE;\n  leaf_p = TRUE;\n  for (insn = DLIST_HEAD (MIR_insn_t, func->insns); insn != NULL; insn = next_insn) {\n    MIR_item_t proto_item, func_import_item;\n    int nargs;\n\n    next_insn = DLIST_NEXT (MIR_insn_t, insn);\n    code = insn->code;\n    if (code == MIR_LDBEQ || code == MIR_LDBNE || code == MIR_LDBLT || code == MIR_LDBGE\n        || code == MIR_LDBGT || code == MIR_LDBLE) { /* split to cmp and branch */\n      temp_op = _MIR_new_var_op (ctx, gen_new_temp_reg (gen_ctx, MIR_T_I64, func));\n      code = (code == MIR_LDBEQ   ? MIR_LDEQ\n              : code == MIR_LDBNE ? MIR_LDNE\n              : code == MIR_LDBLT ? MIR_LDLT\n              : code == MIR_LDBGE ? MIR_LDGE\n              : code == MIR_LDBGT ? MIR_LDGT\n                                  : MIR_LDLE);\n      new_insn = MIR_new_insn (ctx, code, temp_op, insn->ops[1], insn->ops[2]);\n      gen_add_insn_before (gen_ctx, insn, new_insn);\n      next_insn = MIR_new_insn (ctx, MIR_BT, insn->ops[0], temp_op);\n      gen_add_insn_after (gen_ctx, new_insn, next_insn);\n      gen_delete_insn (gen_ctx, insn);\n      insn = new_insn;\n    }\n    if ((nargs = get_builtin (gen_ctx, code, &proto_item, &func_import_item)) > 0) {\n      if (code == MIR_VA_ARG || code == MIR_VA_BLOCK_ARG) {\n        /* Use a builtin func call:\n           mov func_reg, func ref; [mov reg3, type;] call proto, func_reg, res_reg, va_reg,\n           reg3 */\n        MIR_op_t ops[6], func_reg_op, reg_op3;\n        MIR_op_t res_reg_op = insn->ops[0], va_reg_op = insn->ops[1], op3 = insn->ops[2];\n\n        assert (res_reg_op.mode == MIR_OP_VAR && va_reg_op.mode == MIR_OP_VAR\n                && op3.mode == (code == MIR_VA_ARG ? MIR_OP_VAR_MEM : MIR_OP_VAR));\n        func_reg_op = _MIR_new_var_op (ctx, gen_new_temp_reg (gen_ctx, MIR_T_I64, func));\n        reg_op3 = _MIR_new_var_op (ctx, gen_new_temp_reg (gen_ctx, MIR_T_I64, func));\n        next_insn = new_insn\n          = MIR_new_insn (ctx, MIR_MOV, func_reg_op, MIR_new_ref_op (ctx, func_import_item));\n        gen_add_insn_before (gen_ctx, insn, new_insn);\n        if (code == MIR_VA_ARG) {\n          new_insn\n            = MIR_new_insn (ctx, MIR_MOV, reg_op3, MIR_new_int_op (ctx, (int64_t) op3.u.mem.type));\n          op3 = reg_op3;\n          gen_add_insn_before (gen_ctx, insn, new_insn);\n        }\n        ops[0] = MIR_new_ref_op (ctx, proto_item);\n        ops[1] = func_reg_op;\n        ops[2] = res_reg_op;\n        ops[3] = va_reg_op;\n        ops[4] = op3;\n        if (code == MIR_VA_BLOCK_ARG) ops[5] = insn->ops[3];\n        new_insn = MIR_new_insn_arr (ctx, MIR_CALL, code == MIR_VA_ARG ? 5 : 6, ops);\n        gen_add_insn_before (gen_ctx, insn, new_insn);\n        gen_delete_insn (gen_ctx, insn);\n      } else { /* Use builtin: mov freg, func ref; call proto, freg, res_reg, op_reg[, op_reg2] */\n        MIR_op_t freg_op, res_reg_op = insn->ops[0], op_reg_op = insn->ops[1], ops[5];\n\n        assert (res_reg_op.mode == MIR_OP_VAR && op_reg_op.mode == MIR_OP_VAR);\n        freg_op = _MIR_new_var_op (ctx, gen_new_temp_reg (gen_ctx, MIR_T_I64, func));\n        next_insn = new_insn\n          = MIR_new_insn (ctx, MIR_MOV, freg_op, MIR_new_ref_op (ctx, func_import_item));\n        gen_add_insn_before (gen_ctx, insn, new_insn);\n        ops[0] = MIR_new_ref_op (ctx, proto_item);\n        ops[1] = freg_op;\n        ops[2] = res_reg_op;\n        ops[3] = op_reg_op;\n        if (nargs == 2) ops[4] = insn->ops[2];\n        new_insn = MIR_new_insn_arr (ctx, MIR_CALL, nargs + 3, ops);\n        gen_add_insn_before (gen_ctx, insn, new_insn);\n        gen_delete_insn (gen_ctx, insn);\n      }\n    } else if (code == MIR_VA_START) {\n      MIR_op_t treg_op = _MIR_new_var_op (ctx, gen_new_temp_reg (gen_ctx, MIR_T_I64, func));\n      MIR_op_t treg_op2 = _MIR_new_var_op (ctx, gen_new_temp_reg (gen_ctx, MIR_T_I64, func));\n      MIR_op_t va_op = insn->ops[0];\n      MIR_reg_t va_reg;\n\n      assert (func->vararg_p && va_op.mode == MIR_OP_VAR);\n      va_reg = va_op.u.reg;\n      /* Insns can be non-simplified as soon as they match a machine insn.  */\n      /* treg = mem64[r1]; treg = treg + var_args_start; mem64[va_reg] = treg */\n      gen_mov (gen_ctx, insn, MIR_MOV, treg_op,\n               _MIR_new_var_mem_op (ctx, MIR_T_I64, 0, R1_HARD_REG, MIR_NON_VAR, 1));\n      gen_mov (gen_ctx, insn, MIR_MOV, treg_op2, MIR_new_int_op (ctx, var_args_start));\n      /* don't use immediate in ADD as treg_op can become r0: */\n      new_insn = MIR_new_insn (ctx, MIR_ADD, treg_op, treg_op, treg_op2);\n      gen_add_insn_before (gen_ctx, insn, new_insn);\n      gen_mov (gen_ctx, insn, MIR_MOV,\n               _MIR_new_var_mem_op (ctx, MIR_T_I64, 0, va_reg, MIR_NON_VAR, 1), treg_op);\n      gen_delete_insn (gen_ctx, insn);\n    } else if (code == MIR_VA_END) { /* do nothing */\n      gen_delete_insn (gen_ctx, insn);\n    } else if (MIR_call_code_p (code)) {\n      machinize_call (gen_ctx, insn);\n      leaf_p = FALSE;\n    } else if (code == MIR_ALLOCA) {\n      alloca_p = TRUE;\n    } else if (code == MIR_SWITCH) {\n      switch_p = TRUE;\n    } else if (code == MIR_LADDR) {\n      laddr_p = TRUE;\n    } else if (code == MIR_RET) {\n      /* In simplify we already transformed code for one return insn\n         and added extension insn (if any).  */\n      uint32_t n_gpregs = 0, n_fregs = 0;\n\n      assert (func->nres == MIR_insn_nops (ctx, insn));\n      for (i = 0; i < func->nres; i++) {\n        assert (insn->ops[i].mode == MIR_OP_VAR);\n        res_type = func->res_types[i];\n        if (((res_type == MIR_T_F || res_type == MIR_T_D) && n_fregs < 4)\n            || (res_type == MIR_T_LD && n_fregs < 3)) {\n          new_insn_code = res_type == MIR_T_F   ? MIR_FMOV\n                          : res_type == MIR_T_D ? MIR_DMOV\n                                                : MIR_LDMOV;\n          ret_reg = F1_HARD_REG + n_fregs++;\n        } else if (n_gpregs < 8) {\n          new_insn_code = MIR_MOV;\n          ret_reg = R3_HARD_REG + n_gpregs++;\n        } else {\n          (*MIR_get_error_func (ctx)) (MIR_ret_error,\n                                       \"ppc64 can not handle this combination of return values\");\n        }\n        ret_reg_op = _MIR_new_var_op (ctx, ret_reg);\n        gen_mov (gen_ctx, insn, new_insn_code, ret_reg_op, insn->ops[i]);\n        insn->ops[i] = ret_reg_op;\n      }\n    }\n  }\n}\n\nstatic void isave (gen_ctx_t gen_ctx, MIR_insn_t anchor, int disp, MIR_reg_t hard_reg) {\n  gen_mov (gen_ctx, anchor, MIR_MOV,\n           _MIR_new_var_mem_op (gen_ctx->ctx, MIR_T_I64, disp, R1_HARD_REG, MIR_NON_VAR, 1),\n           _MIR_new_var_op (gen_ctx->ctx, hard_reg));\n}\n\nstatic void fsave (gen_ctx_t gen_ctx, MIR_insn_t anchor, int disp, MIR_reg_t hard_reg) {\n  gen_mov (gen_ctx, anchor, MIR_DMOV,\n           _MIR_new_var_mem_op (gen_ctx->ctx, MIR_T_D, disp, R1_HARD_REG, MIR_NON_VAR, 1),\n           _MIR_new_var_op (gen_ctx->ctx, hard_reg));\n}\n\nstatic void target_make_prolog_epilog (gen_ctx_t gen_ctx, bitmap_t used_hard_regs,\n                                       size_t stack_slots_num) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  MIR_func_t func;\n  MIR_insn_t anchor, new_insn;\n  MIR_op_t sp_reg_op, fp_reg_op, r0_reg_op, lr_reg_op;\n  int64_t start_save_regs_offset;\n  size_t i, n, frame_size, saved_iregs_num, saved_fregs_num;\n\n  assert (curr_func_item->item_type == MIR_func_item);\n  func = curr_func_item->u.func;\n  anchor = DLIST_HEAD (MIR_insn_t, func->insns);\n  if (func->vararg_p) {\n    for (i = 0; i < 8; i++)\n      isave (gen_ctx, anchor, PPC64_STACK_HEADER_SIZE + i * 8, i + R3_HARD_REG);\n  }\n  for (i = saved_iregs_num = saved_fregs_num = 0; i <= MAX_HARD_REG; i++)\n    if (!target_call_used_hard_reg_p (i, MIR_T_UNDEF) && bitmap_bit_p (used_hard_regs, i)) {\n      if (i < F0_HARD_REG)\n        saved_iregs_num++;\n      else\n        saved_fregs_num++;\n    }\n  if (leaf_p && !alloca_p && !switch_p && !laddr_p /* switch and laddr changes LR */\n      && saved_iregs_num == 0 && saved_fregs_num == 0 && stack_slots_num == 0)\n    return;\n  saved_iregs_num++; /* for fp (R31) ??? only alloca_p */\n  r0_reg_op = _MIR_new_var_op (ctx, R0_HARD_REG);\n  lr_reg_op = _MIR_new_var_op (ctx, LR_HARD_REG);\n  sp_reg_op = _MIR_new_var_op (ctx, R1_HARD_REG);\n  fp_reg_op = _MIR_new_var_op (ctx, R31_HARD_REG);\n  /* Prologue: */\n  frame_size = param_save_area_size + PPC64_STACK_HEADER_SIZE + stack_slots_num * 8;\n  start_save_regs_offset = frame_size;\n  frame_size += (saved_iregs_num + saved_fregs_num) * 8;\n  if (frame_size % 16 != 0) frame_size = (frame_size + 15) / 16 * 16;\n  if (!func->jret_p) {\n    gen_mov (gen_ctx, anchor, MIR_MOV, r0_reg_op, lr_reg_op); /* r0 = lr */\n    gen_mov (gen_ctx, anchor, MIR_MOV,\n             _MIR_new_var_mem_op (ctx, MIR_T_I64, 16, R1_HARD_REG, MIR_NON_VAR, 1),\n             r0_reg_op); /* mem[r1] = r0 */\n  }\n  gen_mov (gen_ctx, anchor, MIR_MOV, r0_reg_op, sp_reg_op);\n  new_insn = MIR_new_insn (ctx, MIR_ADD, sp_reg_op, sp_reg_op, MIR_new_int_op (ctx, -frame_size));\n  gen_add_insn_before (gen_ctx, anchor, new_insn); /* r1 -= frame_size */\n  gen_mov (gen_ctx, anchor, MIR_MOV,\n           _MIR_new_var_mem_op (ctx, MIR_T_I64, 0, R1_HARD_REG, MIR_NON_VAR, 1),\n           r0_reg_op); /* mem[r1] = r0 */\n  gen_mov (gen_ctx, anchor, MIR_MOV,\n           _MIR_new_var_mem_op (ctx, MIR_T_I64, PPC64_TOC_OFFSET, R1_HARD_REG, MIR_NON_VAR, 1),\n           _MIR_new_var_op (ctx, R2_HARD_REG)); /* mem[r1+toc_off] = r2 */\n  for (n = i = 0; i <= MAX_HARD_REG; i++)\n    if (!target_call_used_hard_reg_p (i, MIR_T_UNDEF) && bitmap_bit_p (used_hard_regs, i)) {\n      if (i < F0_HARD_REG)\n        isave (gen_ctx, anchor, start_save_regs_offset + (n++) * 8, i);\n      else\n        fsave (gen_ctx, anchor, start_save_regs_offset + (n++) * 8, i);\n    }\n  isave (gen_ctx, anchor, start_save_regs_offset + n * 8, R31_HARD_REG); /* save R31 */\n  gen_mov (gen_ctx, anchor, MIR_MOV, fp_reg_op, sp_reg_op);              /* r31 = r1 */\n  /* Epilogue: */\n  for (anchor = DLIST_TAIL (MIR_insn_t, func->insns); anchor != NULL;\n       anchor = DLIST_PREV (MIR_insn_t, anchor))\n    if (anchor->code == MIR_RET || anchor->code == MIR_JRET) break;\n  if (anchor == NULL) return;\n  /* Restoring hard registers: */\n  for (i = n = 0; i <= MAX_HARD_REG; i++)\n    if (!target_call_used_hard_reg_p (i, MIR_T_UNDEF) && bitmap_bit_p (used_hard_regs, i)) {\n      if (i < F0_HARD_REG) {\n        gen_mov (gen_ctx, anchor, MIR_MOV, _MIR_new_var_op (ctx, i),\n                 _MIR_new_var_mem_op (ctx, MIR_T_I64, start_save_regs_offset + (n++) * 8,\n                                      FP_HARD_REG, MIR_NON_VAR, 1));\n      } else {\n        gen_mov (gen_ctx, anchor, MIR_DMOV, _MIR_new_var_op (ctx, i),\n                 _MIR_new_var_mem_op (ctx, MIR_T_D, start_save_regs_offset + (n++) * 8, FP_HARD_REG,\n                                      MIR_NON_VAR, 1));\n      }\n    }\n  /* Restore sp, fp, lr */\n  new_insn = MIR_new_insn (ctx, MIR_ADD, sp_reg_op, fp_reg_op, MIR_new_int_op (ctx, frame_size));\n  gen_add_insn_before (gen_ctx, anchor, new_insn); /* sp = fp + frame_size */\n  gen_mov (gen_ctx, anchor, MIR_MOV, fp_reg_op,\n           _MIR_new_var_mem_op (ctx, MIR_T_I64, start_save_regs_offset + n * 8, FP_HARD_REG,\n                                MIR_NON_VAR, 1)); /* restore fp */\n  if (!func->jret_p) {\n    gen_mov (gen_ctx, anchor, MIR_MOV, r0_reg_op,\n             _MIR_new_var_mem_op (ctx, MIR_T_I64, 16, R1_HARD_REG, MIR_NON_VAR,\n                                  1));                        /* r0 = 16(sp) */\n    gen_mov (gen_ctx, anchor, MIR_MOV, lr_reg_op, r0_reg_op); /* lr = r0 */\n  }\n}\n\nstruct pattern {\n  MIR_insn_code_t code;\n  /* Pattern elements:\n     blank - ignore\n     X - match everything\n     $ - finish successfully matching\n     r - register but LR\n     R - r but R0\n\n     h<one or two decimal digits> - hard register with given number\n\n        memory with signed 16-bit disp and optional non-R0 base:\n     m[0-2] - int (signed or unsigned) memory of size 8,16,32,64-bits\n     ms[0-2] - signed int type memory of size 8,16,32,64-bits\n     mu[0-2] - unsigned int type memory of size 8,16,32,64-bits\n\n       memory with non-R0 base and index:\n     M[0-3] - int (signed or unsigned) type memory of size 8,16,32,64-bits\n     Ms[0-2] - signed int type memory of size 8,16,32,64-bits\n     Mu[0-2] - unsigned int type memory of size 8,16,32,64-bits\n\n     mds - signed 32-bit memory with scaled by 4 signed 16-bit disp and optional non-R0 base:\n     Mds - 64-bit memory with scaled by 4 signed 16-bit disp and optional non-R0 base:\n\n     i - 16 bit signed immediate\n     I - 16 bit signed immediate shift left by 16\n     u - 16 bit unsigned immediate\n     U - 16 bit unsigned immediate shift left by 16\n     x - 64 bit unsigned immediate whose high 32-bit part is described by pattern 0*1*\n     z - 32-bit unsigned immediate\n     zs - 32-bit unsigned immediate with zero 0-th bit\n     Z - any integer immediate\n     Zs - 48-bit unsigned immediate with zero 0-th bit\n     Sh - 6-bit unsigned shift\n     sh - 5-bit unsigned shift\n     ia - roundup (i, 16) as 16 bit signed integer\n\n       memory with signed 16-bit disp and optional non-R0 base:\n     mf - memory of float\n     md - memory of double\n     mld - memory of long double where disp + 8 is also in 16-bit range and non-R0 base reg\n     mld0 - as previous bit with R0 base reg\n     mds - signed 32-bit memory with scaled by 4 signed 16-bit disp and option non-R0 base:\n\n       memory with non-R0 base and index:\n     Mf - memory of float\n     Md - memory of double\n\n     L - reference or label as the 1st or 2nd op which can be present by signed 24-bit pc word\n     offset\n\n     Remember we have no float or (long) double immediate at this stage.  They were removed during\n     simplification.  */\n\n  const char *pattern;\n  /* Bit addressing: 0..31\n     Replacement elements:\n     blank - ignore\n     ; - insn separation\n     o<number> - opcode [0..5], <number> is decimal\n     O<number> - opcode [21..30], <number> is decimal\n     P<number> - opcode [26..30], <number> is decimal\n     (r|n)t[0-2] - put n-th operand register into rd field [6..10]\n     (r|n)s[0-2] - put n-th operand register into rs field [6..10] -- source\n     (r|n)a[0-2] - put n-th operand register into rn field [11..15]\n     (r|n)b[0-2] - put n-th operand register into rm field [16..20]\n     rc[0-2] - put n-th operand register into rc field [21..25]\n                   n above means operand reg+1\n     Ra[0-2] - put n-th operand register (which is not R0) into rn field [11..15]\n     h(t,s,a,b)<dec digits> - hardware register with given number in rt,ra,rb field\n     sr<number> - special reg with given number [11..15]\n     m = operand is (8-,16-,32-,64-bit) mem with base (0 reg means 0) and signed 16-bit disp\n     M = operand is (8-,16-,32-,64-bit) mem with base (0 reg means 0) and index\n     mds = 32-bit scaled mem with base (0 reg means 0) and signed 16-bit disp scaled by 4\n     Mds = 64-bit scaled mem with base (0 reg means 0) and signed 16-bit disp scaled by 4\n     d<number> = field [30..31]\n     mt = 8-byte memory -16(r1)\n     i - 16 bit signed immediate - field [16..31]\n     I - 16 bit signed immediate shift left by 16 - field [16..31]\n     u - 16 bit unsigned immediate - field [16..31]\n     U - 16 bit unsigned immediate shift left by 16 - field [16..31]\n     z[0-3] - n-th 16 bytes of 64-bit immediate\n     x - mb for x immediate\n     sh<number> - field [16..20]\n     Sh<number> - field [16..20,30]\n     Mb<number> - field [21..26], and zero bits [27..28]\n     Me<number> - field [21..26], and one in bit [27..29]\n     Sh - Sh value: field [16..20,30]\n     sh - sh value: field [16..20]\n     Shr - 64 - Sh value: field [16..20,30]\n     shr - 32 - sh value: field [16..20]\n     mbSh - mb with value 64-Sh: field [21..26] and zero in bits [27..29]\n     mbsh - mb with value 32-sh: field [21..25]\n     meSh - mb with value 63-Sh: field [21..26] and 1 in bits [27..29]\n     mesh - mb with value 31-sh: field [26..30]\n     mb<number> - number in filed [21..25]\n     me<number> - number in filed [26..30]\n     ia - roundup (i, 16)\n     ih - PPC64_STACK_HEADER_SIZE + param_area_size\n\n     bf<number> - field [6..8]\n     L - operand-label as 24-bit word pc offset scaled by 4: field [6..29]\n     l - operand-label as 14-bit word pc offset scaled by 4: field [16..29]\n     l<number> - operand-label as 14-bit word pc offset scaled by 4: field [16..29]\n     L<number> - long data bitfield [10..10]\n     BO<number> - field [6..10] for bcond\n     BI<number> - field [11..15] for bcond\n     LK<number> - field [31..31]\n\n     W - LADDR label which is a 32-bit signed offset from previous insn\n     at - address disp PPC64_TOC_OFFSET\n     T - switch table displacement\n  */\n  const char *replacement;\n};\n\nstatic const struct pattern patterns[] = {\n  {MIR_MOV, \"r r\", \"o31 O444 ra0 rs1 rb1\"}, /* or ra,rs,rs */\n  {MIR_MOV, \"r h64\", \"o31 O339 rt0 sr8\"},   /* mflr rt */\n  {MIR_MOV, \"h64 r\", \"o31 O467 rs1 sr8\"},   /* mtlr rs */\n\n  // ??? loads/stores with update\n  {MIR_MOV, \"r Mds\", \"o58 rt0 Mds\"},   /* ld rt,ds-mem */\n  {MIR_MOV, \"Mds r\", \"o62 rs1 Mds\"},   /* std rt,ds-mem */\n  {MIR_MOV, \"r M3\", \"o31 O21 rt0 M\"},  /* ldx rt,index-mem */\n  {MIR_MOV, \"M3 r\", \"o31 O149 rs1 M\"}, /* stdx rs,index-mem */\n\n  {MIR_MOV, \"r mu2\", \"o32 rt0 m\"},     /* lwz rt,disp-mem */\n  {MIR_MOV, \"m2 r\", \"o36 rs1 m\"},      /* stw rs,disp-mem */\n  {MIR_MOV, \"r Mu2\", \"o31 O23 rt0 M\"}, /* lwzx rt,index-mem */\n  {MIR_MOV, \"M2 r\", \"o31 O151 rs1 M\"}, /* stwx rs,index-mem */\n\n  {MIR_MOV, \"r mds\", \"o58 rt0 mds d2\"}, /* lwa rt,ds-mem */\n  {MIR_MOV, \"r Ms2\", \"o31 O341 rt0 M\"}, /* lwax rt,index-mem */\n\n  {MIR_MOV, \"r mu1\", \"o40 rt0 m\"},      /* lhz rt,disp-mem */\n  {MIR_MOV, \"m1 r\", \"o44 rs1 m\"},       /* sth rs,disp-mem */\n  {MIR_MOV, \"r Mu1\", \"o31 O279 rt0 M\"}, /* lhzx rt,index-mem */\n  {MIR_MOV, \"M1 r\", \"o31 O407 rs1 M\"},  /* sthx rs,index-mem */\n\n  {MIR_MOV, \"r ms1\", \"o42 rt0 m\"},      /* lha rt,disp-mem */\n  {MIR_MOV, \"r Ms1\", \"o31 O343 rt0 M\"}, /* lhax rt,index-mem */\n\n  {MIR_MOV, \"r mu0\", \"o34 rt0 m\"},     /* lbz rt,disp-mem */\n  {MIR_MOV, \"m0 r\", \"o38 rs1 m\"},      /* stb rs,disp-mem */\n  {MIR_MOV, \"r Mu0\", \"o31 O87 rt0 M\"}, /* lbzx rt,index-mem */\n  {MIR_MOV, \"M0 r\", \"o31 O215 rs1 M\"}, /* stbx rs,index-mem */\n\n  {MIR_MOV, \"r ms0\", \"o34 rt0 m; o31 O954 rs0 ra0\"},     /* lbz rt,disp-mem; extsb rt,rt */\n  {MIR_MOV, \"r Ms0\", \"o31 O87 rt0 M; o31 O954 rs0 ra0\"}, /* lbzx rt,index-mem; extsb rt,rt */\n\n  {MIR_MOV, \"r i\", \"o14 rt0 ha0 i\"},                   /* li rt,i == addi rt,0,i */\n  {MIR_MOV, \"r I\", \"o15 rt0 ha0 I\"},                   /* lis rt,i == addis rt,0,i */\n  {MIR_MOV, \"r zs\", \"o15 rt0 ha0 z2; o24 rt0 ra0 z3\"}, /* lis rt,z2; ori rt,rt,z3 */\n  /* lis rt,rt,z2; ori rt,rt,z3; clrdi rt,rt,X */\n  {MIR_MOV, \"r x\", \"o15 rt0 ha0 z2; o24 ra0 rs0 z3; o30 ra0 rs0 Sh0 x\"},\n  /* xor rt,rt,rt; oris rt,rt,z2; ori rt,rt,z3: */\n  {MIR_MOV, \"r z\", \"o31 O316 rs0 ra0 rb0; o25 ra0 rs0 z2; o24 ra0 rs0 z3\"},\n  /* li rt,r0,z1; rldicr rt,rt,32,31; oris rt,rt,z2; ori rt,rt,z3 */\n  {MIR_MOV, \"r Zs\", \"o14 rt0 ha0 z1; o30 rt0 ra0 Sh32 Me31; o25 ra0 rs0 z2; o24 ra0 rs0 z3\"},\n  /* lis rt,r0,z0; ori rt,rt,z1; rldicr rt,rt,32,31; oris rt,rt,z2; ori rt,rt,z3: */\n  {MIR_MOV, \"r Z\",\n   \"o15 rt0 ha0 z0; o24 ra0 rs0 z1; o30 rt0 ra0 Sh32 Me31; o25 ra0 rs0 z2; o24 ra0 rs0 z3\"},\n\n  {MIR_FMOV, \"r r\", \"o63 O72 rt0 rb1\"}, /*  fmr rt,rb */\n  {MIR_FMOV, \"r mf\", \"o48 rt0 m\"},      /* lfs rt, disp-mem */\n  {MIR_FMOV, \"r Mf\", \"o31 O535 rt0 M\"}, /* lfsx rt, index-mem */\n  {MIR_FMOV, \"mf r\", \"o52 rt1 m\"},      /* stfs rt, disp-mem */\n  {MIR_FMOV, \"Mf r\", \"o31 O663 rt1 M\"}, /* stfsx rt, index-mem */\n\n  {MIR_DMOV, \"r r\", \"o63 O72 rt0 rb1\"}, /*  fmr rt,rb */\n  {MIR_DMOV, \"r md\", \"o50 rt0 m\"},      /* lds rt, disp-mem */\n  {MIR_DMOV, \"r Md\", \"o31 O599 rt0 M\"}, /* lfdx rt, index-mem */\n  {MIR_DMOV, \"md r\", \"o54 rt1 m\"},      /* stfd rt, disp-mem */\n  {MIR_DMOV, \"Md r\", \"o31 O727 rt1 M\"}, /* stfdx rt, index-mem */\n\n  {MIR_LDMOV, \"r r\", \"o63 O72 rt0 rb1;o63 O72 nt0 nb1\"}, /* fmr rt,rb; fmr rt+1,rb+1 */\n  {MIR_LDMOV, \"r mld\", \"o50 rt0 m; o50 nt0 mn\"},         /* lfd rt,disp-mem; lfd rt+1,disp+8-mem */\n  {MIR_LDMOV, \"mld r\", \"o54 rt1 m; o54 nt1 mn\"}, /* stfd rt,disp-mem; stfdx rt+1,disp+8-mem */\n  {MIR_LDMOV, \"r mld0\",\n   \"o31 O444 ha11 hs0 hb0; o50 rt0 ha11; o50 nt0 ha11 i8\"}, /* mr r11,r0; lfd rt,(r11); lfd\n                                                               rt+1,8(r11) */\n  {MIR_LDMOV, \"mld0 r\",\n   \"o31 O444 ha11 hs0 hb0; o54 rt1 ha11; o54 nt1 ha11 i8\"}, /* mr r11,r0; stfd rt,(r11); stfdx\n                                                               rt+1,8(r11) */\n\n  {MIR_EXT8, \"r r\", \"o31 O954 ra0 rs1\"},  /* extsb ra,rs */\n  {MIR_EXT16, \"r r\", \"o31 O922 ra0 rs1\"}, /* extsh ra,rs */\n  {MIR_EXT32, \"r r\", \"o31 O986 ra0 rs1\"}, /* extsw ra,rs */\n\n  {MIR_UEXT8, \"r r\", \"o30 ra0 rs1 Sh0 Mb56\"},  /* rldicl ra,rs,0,56 */\n  {MIR_UEXT16, \"r r\", \"o30 ra0 rs1 Sh0 Mb48\"}, /* rldicl ra,rs,0,48 */\n  {MIR_UEXT32, \"r r\", \"o30 ra0 rs1 Sh0 Mb32\"}, /* rldicl ra,rs,0,32 */\n\n  {MIR_ADD, \"r r r\", \"o31 O266 rt0 ra1 rb2\"},  /* add rt,ra,rb */\n  {MIR_ADD, \"r R i\", \"o14 rt0 ra1 i\"},         /* addi rt,ra,i */\n  {MIR_ADD, \"r R I\", \"o15 rt0 ra1 I\"},         /* addis rt,ra,I */\n  {MIR_ADDS, \"r r r\", \"o31 O266 rt0 ra1 rb2\"}, /* add rt,ra,rb */\n  {MIR_ADDS, \"r R i\", \"o14 rt0 ra1 i\"},        /* addi rt,ra,i */\n  {MIR_ADDS, \"r R I\", \"o15 rt0 ra1 I\"},        /* addis rt,ra,I */\n  {MIR_FADD, \"r r r\", \"o59 O21 rt0 ra1 rb2\"},  /* fadds rt,ra,rb*/\n  {MIR_DADD, \"r r r\", \"o63 O21 rt0 ra1 rb2\"},  /* fadd rt,ra,rb*/\n  // ldadd is implemented through builtin\n\n  // ??? transform sub immediate to add immediate\n  {MIR_SUB, \"r r r\", \"o31 O40 rt0 rb1 ra2\"},  /* subf rt,ra,rb */\n  {MIR_SUBS, \"r r r\", \"o31 O40 rt0 rb1 ra2\"}, /* subf rt,ra,rb */\n  {MIR_FSUB, \"r r r\", \"o59 O20 rt0 ra1 rb2\"}, /* fsubs rt,ra,rb*/\n  {MIR_DSUB, \"r r r\", \"o63 O20 rt0 ra1 rb2\"}, /* fsub rt,ra,rb*/\n  // ldsub is implemented through builtin\n\n  {MIR_MUL, \"r r r\", \"o31 O233 rt0 ra1 rb2\"},  /* mulld rt,ra,rb*/\n  {MIR_MUL, \"r r i\", \"o7 rt0 ra1 i\"},          /* mulli rt,ra,i*/\n  {MIR_MULS, \"r r r\", \"o31 O235 rt0 ra1 rb2\"}, /* mullw rt,ra,rb*/\n  {MIR_FMUL, \"r r r\", \"o59 P25 rt0 ra1 rc2\"},  /* fmuls rt,ra,rc*/\n  {MIR_DMUL, \"r r r\", \"o63 P25 rt0 ra1 rc2\"},  /* fmul rt,ra,rc*/\n  // ldmul is implemented through builtin\n\n  {MIR_DIV, \"r r r\", \"o31 O489 rt0 ra1 rb2\"},   /* divd rt,ra,rb*/\n  {MIR_DIVS, \"r r r\", \"o31 O491 rt0 ra1 rb2\"},  /* divw rt,ra,rb*/\n  {MIR_UDIV, \"r r r\", \"o31 O457 rt0 ra1 rb2\"},  /* divdu rt,ra,rb*/\n  {MIR_UDIVS, \"r r r\", \"o31 O459 rt0 ra1 rb2\"}, /* divwu rt,ra,rb*/\n  {MIR_FDIV, \"r r r\", \"o59 O18 rt0 ra1 rb2\"},   /* fdivs rt,ra,rb*/\n  {MIR_DDIV, \"r r r\", \"o63 O18 rt0 ra1 rb2\"},   /* fdiv rt,ra,rb*/\n  // lddiv is implemented through builtin\n\n  /* divd r10,ra,rb;muld r10,r10,rb;subf r,r10,ra: */\n  {MIR_MOD, \"r r r\", \"o31 O489 ht10 ra1 rb2; o31 O233 ht10 ha10 rb2; o31 O40 rt0 ha10 rb1\"},\n  /* divw r10,ra,rb;mulw r10,r10,rb;subf r,r10,ra: */\n  {MIR_MODS, \"r r r\", \"o31 O491 ht10 ra1 rb2; o31 O235 ht10 ha10 rb2; o31 O40 rt0 ha10 rb1\"},\n  /* divdu r10,ra,rb;muld r10,r10,rb;subf r,r10,ra: */\n  {MIR_UMOD, \"r r r\", \"o31 O457 ht10 ra1 rb2; o31 O233 ht10 ha10 rb2; o31 O40 rt0 ha10 rb1\"},\n  /* divwu r10,ra,rb;mulw r10,r10,rb;subf r,r10,ra: */\n  {MIR_UMODS, \"r r r\", \"o31 O459 ht10 ra1 rb2; o31 O235 ht10 ha10 rb2; o31 O40 rt0 ha10 rb1\"},\n\n#define MFCR \"o31 O19 rt0\"\n#define EQEND MFCR \"; o21 rs0 ra0 sh31 mb31 me31\"\n#define CMPD \"o31 O0 bf7 L1 ra1 rb2\"\n#define CMPDI(i) \"o11 bf7 L1 ra1 \" #i\n#define CMPW \"o31 O0 bf7 L0 ra1 rb2\"\n#define CMPWI(i) \"o11 bf7 L0 ra1 \" #i\n#define FCMPU \"o63 O0 bf7 ra1 rb2\"\n#define CRNOT(s, f) \"o19 O33 ht\" #s \" ha\" #f \" hb\" #f \";\"\n#define CROR(t, a, b) \"o19 O449 ht\" #t \" ha\" #a \" hb\" #b \";\"\n#define CRORC(t, a, b) \"o19 O417 ht\" #t \" ha\" #a \" hb\" #b \";\"\n#define CRNOR(t, a, b) \"o19 O33 ht\" #t \" ha\" #a \" hb\" #b \";\"\n#define CRANDC(t, a, b) \"o19 O129 ht\" #t \" ha\" #a \" hb\" #b \";\"\n  // all ld insn are changed to builtins\n  /* cmpd 7,ra,rb; mfcr rt; rlwinm rt,rt,31,31,31;*/\n  {MIR_EQ, \"r r r\", CMPD \"; \" EQEND},\n  /* cmpdi 7,ra,i; mfcr rt; rlwinm rt,rt,31,31,31*/\n  {MIR_EQ, \"r r i\", CMPDI (i) \"; \" EQEND},\n  /* cmpw 7,ra,rb; mfcr rt; rlwinm rt,rt,31,31,31;*/\n  {MIR_EQS, \"r r r\", CMPW \"; \" EQEND},\n  /* cmpwi 7,ra,i; mfcr rt; rlwinm rt,rt,31,31,31*/\n  {MIR_EQS, \"r r i\", CMPWI (i) \"; \" EQEND},\n  /* fcmpu 7,ra,rb; mfcr rt; crandc 30,30,31; rlwinm rt,rt,31,31,31;*/\n  {MIR_FEQ, \"r r r\", FCMPU \";\" CRANDC (30, 30, 31) EQEND},\n  /* fcmpu 7,ra,rb; mfcr rt; crandc 30,30,31; rlwinm rt,rt,31,31,31;*/\n  {MIR_DEQ, \"r r r\", FCMPU \";\" CRANDC (30, 30, 31) EQEND},\n\n#define NEEND EQEND \"; o26 rs0 ra0 i1\"\n  /* cmpd 7,ra,rb; mfcr rt; rlwinm rt,rt,31,31,31; xori rt,rt,1*/\n  {MIR_NE, \"r r r\", CMPD \"; \" NEEND},\n  /* cmpdi 7,ra,i; mfcr rt; rlwinm rt,rt,31,31,31; xori rt,rt,1*/\n  {MIR_NE, \"r r i\", CMPDI (i) \"; \" NEEND},\n  /* cmpw 7,ra,rb; mfcr rt; rlwinm rt,rt,31,31,31; xori rt,rt,1*/\n  {MIR_NES, \"r r r\", CMPW \"; \" NEEND},\n  /* cmpwi 7,ra,i; mfcr rt; rlwinm rt,rt,31,31,31; xori rt,rt,1*/\n  {MIR_NES, \"r r i\", CMPWI (i) \"; \" NEEND},\n  /* fcmpu 7,ra,rb; crorc 30,31,30; mfcr rt; rlwinm rt,rt,31,31,31;*/\n  {MIR_FNE, \"r r r\", FCMPU \"; \" CRORC (30, 31, 30) EQEND},\n  /* fcmpu 7,ra,rb; crorc 30,31,30; mfcr rt; rlwinm rt,rt,31,31,31;*/\n  {MIR_DNE, \"r r r\", FCMPU \"; \" CRORC (30, 31, 30) EQEND},\n\n#define RLWINM(n) \"o21 rs0 ra0 sh\" #n \" mb31 me31\"\n  /* cmpd 7,ra,rb; mfcr rt; rlwinm rt,rt,29,31,31;*/\n  {MIR_LT, \"r r r\", CMPD \"; \" MFCR \";  \" RLWINM (29)},\n  /* cmpdi 7,ra,i; mfcr rt; rlwinm rt,rt,29,31,31*/\n  {MIR_LT, \"r r i\", CMPDI (i) \"; \" MFCR \";  \" RLWINM (29)},\n  /* cmpw 7,ra,rb; mfcr rt; rlwinm rt,rt,29,31,31;*/\n  {MIR_LTS, \"r r r\", CMPW \"; \" MFCR \";  \" RLWINM (29)},\n  /* cmpwi 7,ra,i; mfcr rt; rlwinm rt,rt,29,31,31*/\n  {MIR_LTS, \"r r i\", CMPWI (i) \"; \" MFCR \";  \" RLWINM (29)},\n  /* fcmpu 7,ra,rb; crandc 28,28,31; mfcr rt; rlwinm rt,rt,29,31,31;*/\n  {MIR_FLT, \"r r r\", FCMPU \"; \" CRANDC (28, 28, 31) MFCR \"; \" RLWINM (29)},\n  /* fcmpu 7,ra,rb; crandc 28,28,31; mfcr rt; rlwinm rt,rt,29,31,31;*/\n  {MIR_DLT, \"r r r\", FCMPU \"; \" CRANDC (28, 28, 31) MFCR \"; \" RLWINM (29)},\n\n#define CMPLD \"o31 O32 bf7 L1 ra1 rb2\"\n#define CMPLDI \"o10 bf7 L1 ra1 u\"\n#define CMPLW \"o31 O32 bf7 L0 ra1 rb2\"\n#define CMPLWI \"o10 bf7 L0 ra1 u\"\n  /* cmpld 7,ra,rb; mfcr rt; rlwinm rt,rt,29,31,31;*/\n  {MIR_ULT, \"r r r\", CMPLD \"; \" MFCR \";  \" RLWINM (29)},\n  /* cmpldi 7,ra,u; mfcr rt; rlwinm rt,rt,29,31,31;*/\n  {MIR_ULT, \"r r u\", CMPLDI \"; \" MFCR \";  \" RLWINM (29)},\n  /* cmplw 7,ra,rb; mfcr rt; rlwinm rt,rt,29,31,31;*/\n  {MIR_ULTS, \"r r r\", CMPLW \"; \" MFCR \";  \" RLWINM (29)},\n  /* cmplwi 7,ra,u; mfcr rt; rlwinm rt,rt,29,31,31;*/\n  {MIR_ULTS, \"r r u\", CMPLWI \"; \" MFCR \";  \" RLWINM (29)},\n\n  /* cmpd 7,ra,rb; crnot 28,28; mfcr rt; rlwinm rt,rt,29,31,31;*/\n  {MIR_GE, \"r r r\", CMPD \"; \" CRNOT (28, 28) MFCR \";  \" RLWINM (29)},\n  /* cmpdi 7,ra,i; crnot 28,28; mfcr rt; rlwinm rt,rt,29,31,31*/\n  {MIR_GE, \"r r i\", CMPDI (i) \"; \" CRNOT (28, 28) MFCR \";  \" RLWINM (29)},\n  /* cmpw 7,ra,rb; crnot 28,28; mfcr rt; rlwinm rt,rt,29,31,31;*/\n  {MIR_GES, \"r r r\", CMPW \"; \" CRNOT (28, 28) MFCR \";  \" RLWINM (29)},\n  /* cmpwi 7,ra,i; crnot 28,28; mfcr rt; rlwinm rt,rt,29,31,31*/\n  {MIR_GES, \"r r i\", CMPWI (i) \"; \" CRNOT (28, 28) MFCR \";  \" RLWINM (29)},\n  /* fcmpu 7,ra,rb; crnot 28,28,31; mfcr rt; rlwinm rt,rt,29,31,31;*/\n  {MIR_FGE, \"r r r\", FCMPU \"; \" CRNOR (28, 28, 31) MFCR \";  \" RLWINM (29)},\n  /* fcmpu 7,ra,rb; crnor 28,28,31; mfcr rt; rlwinm rt,rt,29,31,31;*/\n  {MIR_DGE, \"r r r\", FCMPU \"; \" CRNOR (28, 28, 31) MFCR \";  \" RLWINM (29)},\n  /* cmpld 7,ra,rb; crnot 28,28; mfcr rt; rlwinm rt,rt,29,31,31;*/\n  {MIR_UGE, \"r r r\", CMPLD \"; \" CRNOT (28, 28) MFCR \";  \" RLWINM (29)},\n  /* cmpldi 7,ra,u; crnot 28,28; mfcr rt; rlwinm rt,rt,29,31,31;*/\n  {MIR_UGE, \"r r u\", CMPLDI \"; \" CRNOT (28, 28) MFCR \";  \" RLWINM (29)},\n  /* cmplw 7,ra,rb; crnot 28,28; mfcr rt; rlwinm rt,rt,29,31,31;*/\n  {MIR_UGES, \"r r r\", CMPLW \"; \" CRNOT (28, 28) MFCR \";  \" RLWINM (29)},\n  /* cmplwi 7,ra,u; crnot 28,28; mfcr rt; rlwinm rt,rt,29,31,31;*/\n  {MIR_UGES, \"r r u\", CMPLWI \"; \" CRNOT (28, 28) MFCR \";  \" RLWINM (29)},\n\n  /* cmpd 7,ra,rb; mfcr rt; rlwinm rt,rt,30,31,31;*/\n  {MIR_GT, \"r r r\", CMPD \"; \" MFCR \";  \" RLWINM (30)},\n  /* cmpdi 7,ra,i; mfcr rt; rlwinm rt,rt,30,31,31*/\n  {MIR_GT, \"r r i\", CMPDI (i) \"; \" MFCR \";  \" RLWINM (30)},\n  /* cmpw 7,ra,rb; mfcr rt; rlwinm rt,rt,30,31,31;*/\n  {MIR_GTS, \"r r r\", CMPW \"; \" MFCR \";  \" RLWINM (30)},\n  /* cmpwi 7,ra,i; mfcr rt; rlwinm rt,rt,30,31,31*/\n  {MIR_GTS, \"r r i\", CMPWI (i) \"; \" MFCR \";  \" RLWINM (30)},\n  /* fcmpu 7,ra,rb; crandc 29,29,30; mfcr rt; rlwinm rt,rt,30,31,31;*/\n  {MIR_FGT, \"r r r\", FCMPU \"; \" CRANDC (29, 29, 31) MFCR \"; \" RLWINM (30)},\n  /* fcmpu 7,ra,rb; crandc 29,29,30; mfcr rt; rlwinm rt,rt,30,31,31;*/\n  {MIR_DGT, \"r r r\", FCMPU \"; \" CRANDC (29, 29, 31) MFCR \";  \" RLWINM (30)},\n  /* cmpld 7,ra,rb; mfcr rt; rlwinm rt,rt,30,31,31;*/\n  {MIR_UGT, \"r r r\", CMPLD \"; \" MFCR \";  \" RLWINM (30)},\n  /* cmpldi 7,ra,u; mfcr rt; rlwinm rt,rt,30,31,31;*/\n  {MIR_UGT, \"r r u\", CMPLDI \"; \" MFCR \";  \" RLWINM (30)},\n  /* cmplw 7,ra,rb; mfcr rt; rlwinm rt,rt,30,31,31;*/\n  {MIR_UGTS, \"r r r\", CMPLW \"; \" MFCR \";  \" RLWINM (30)},\n  /* cmplwi 7,ra,u; mfcr rt; rlwinm rt,rt,30,31,31;*/\n  {MIR_UGTS, \"r r u\", CMPLWI \"; \" MFCR \";  \" RLWINM (30)},\n\n  /* cmpd 7,ra,rb; crnot 29,29; mfcr rt; rlwinm rt,rt,30,31,31;*/\n  {MIR_LE, \"r r r\", CMPD \"; \" CRNOT (29, 29) MFCR \";  \" RLWINM (30)},\n  /* cmpdi 7,ra,i; crnot 29,29; mfcr rt; rlwinm rt,rt,30,31,31*/\n  {MIR_LE, \"r r i\", CMPDI (i) \"; \" CRNOT (29, 29) MFCR \";  \" RLWINM (30)},\n  /* cmpw 7,ra,rb; crnot 29,29; mfcr rt; rlwinm rt,rt,30,31,31;*/\n  {MIR_LES, \"r r r\", CMPW \"; \" CRNOT (29, 29) MFCR \";  \" RLWINM (30)},\n  /* cmpwi 7,ra,i; crnot 29,29; mfcr rt; rlwinm rt,rt,30,31,31*/\n  {MIR_LES, \"r r i\", CMPWI (i) \"; \" CRNOT (29, 29) MFCR \";  \" RLWINM (30)},\n  /* fcmpu 7,ra,rb; crnor 29,29,31; mfcr rt; rlwinm rt,rt,30,31,31;*/\n  {MIR_FLE, \"r r r\", FCMPU \"; \" CRNOR (29, 29, 31) MFCR \";  \" RLWINM (30)},\n  /* fcmpu 7,ra,rb; crnor 29,29,31; mfcr rt; rlwinm rt,rt,30,31,31;*/\n  {MIR_DLE, \"r r r\", FCMPU \"; \" CRNOR (29, 29, 31) MFCR \";  \" RLWINM (30)},\n  /* cmpld 7,ra,rb; crnot 29,29; mfcr rt; rlwinm rt,rt,30,31,31;*/\n  {MIR_ULE, \"r r r\", CMPLD \"; \" CRNOT (29, 29) MFCR \";  \" RLWINM (30)},\n  /* cmpldi 7,ra,u; crnot 29,29; mfcr rt; rlwinm rt,rt,30,31,31;*/\n  {MIR_ULE, \"r r u\", CMPLDI \"; \" CRNOT (29, 29) MFCR \";  \" RLWINM (30)},\n  /* cmplw 7,ra,rb; crnot 29,29; mfcr rt; rlwinm rt,rt,30,31,31;*/\n  {MIR_ULES, \"r r r\", CMPLW \"; \" CRNOT (29, 29) MFCR \";  \" RLWINM (30)},\n  /* cmplwi 7,ra,u; crnot 29,29; mfcr rt; rlwinm rt,rt,30,31,31;*/\n  {MIR_ULES, \"r r u\", CMPLWI \"; \" CRNOT (29, 29) MFCR \";  \" RLWINM (30)},\n\n  {MIR_JMP, \"L\", \"o18 L\"}, /* 24-bit offset word jmp */\n\n  /* bl l4; mflr r0; addis r0,r0,I; addi r0,r0,i: */\n  {MIR_LADDR, \"r W\", \"o18 l4 LK1; o31 O339 rt0 sr8; o15 rt0 ra0 W; o14 rt0 ra0\"},\n  {MIR_JMPI, \"r\", \"o31 O467 rs0 sr9; o19 O528 BO20 BI0\"}, /* mtctr r; bcctr */\n\n#define BRC(o, i) \"o16 BO\" #o \" BI\" #i \" l\"\n#define BRCL(o, i) \"o16 BO\" #o \" BI\" #i \" l8; o18 L\"\n#define BRLOG(CODE, CMP, BI, COND, NEG_COND)                                \\\n  {CODE, \"l r\", CMP (i0) \"; \" BRC (COND, BI)}, /* cmp 7,ra1,0;bcond 7,l; */ \\\n  {                                                                         \\\n    CODE, \"L r\", CMP (i0) \"; \" BRCL (NEG_COND, BI)                          \\\n  } /* cmp 7,ra1,0;bneg_cond 7,o8;b L;l8:*/\n\n  BRLOG (MIR_BT, CMPDI, 30, 4, 12),\n  BRLOG (MIR_BTS, CMPWI, 30, 4, 12),\n  BRLOG (MIR_BF, CMPDI, 30, 12, 4),\n  BRLOG (MIR_BFS, CMPWI, 30, 12, 4),\n\n#define BRCMP(CODE, CMP, CMPI, BI, COND, NEG_COND)                                           \\\n  {CODE, \"l r r\", CMP \"; \" BRC (COND, BI)},        /* cmp 7,ra1,rb2;bcond 7,l; */            \\\n    {CODE, \"l r i\", CMPI \"; \" BRC (COND, BI)},     /* cmpi 7,ra1,i;bcond 7,l;:*/             \\\n    {CODE, \"L r r\", CMP \"; \" BRCL (NEG_COND, BI)}, /* cmp 7,ra1,rb2;bneg_cond 7,o8;b L;l8:*/ \\\n  {                                                                                          \\\n    CODE, \"L r i\", CMPI \"; \" BRCL (NEG_COND, BI)                                             \\\n  } /* cmp 7,ra1,i;bneg_cond 7,o8;b L;l8:*/\n\n#define BRFCMP(CODE, BI, COND, COND_NAND, NEG_COND, NEG_COND_NAND)                     \\\n  {CODE, \"l r r\", FCMPU \"; \" COND_NAND BRC (COND, BI)}, /* cmp 7,ra1,rb2;bcond 7,l; */ \\\n  {                                                                                    \\\n    CODE, \"L r r\", FCMPU \"; \" NEG_COND_NAND BRCL (NEG_COND, BI)                        \\\n  } /* cmp 7,ra1,rb2;bneg_cond 7,o8;b L;l8:*/\n\n#define LT_OR CROR (28, 28, 31)\n#define GT_OR CROR (29, 29, 31)\n#define EQ_OR CROR (30, 30, 31)\n\n#define LT_ANDC CRANDC (28, 28, 31)\n#define GT_ANDC CRANDC (29, 29, 31)\n#define EQ_ANDC CRANDC (30, 30, 31)\n\n  // all ld insn are changed to builtins and bt/bts\n  BRCMP (MIR_BEQ, CMPD, CMPDI (i), 30, 12, 4),\n  BRCMP (MIR_BEQS, CMPW, CMPWI (i), 30, 12, 4),\n  BRFCMP (MIR_FBEQ, 30, 12, EQ_ANDC, 4, EQ_OR),\n  BRFCMP (MIR_DBEQ, 30, 12, EQ_ANDC, 4, EQ_OR),\n\n  BRCMP (MIR_BNE, CMPD, CMPDI (i), 30, 4, 12),\n  BRCMP (MIR_BNES, CMPW, CMPWI (i), 30, 4, 12),\n  BRFCMP (MIR_FBNE, 30, 4, EQ_ANDC, 12, EQ_ANDC),\n  BRFCMP (MIR_DBNE, 30, 4, EQ_ANDC, 12, EQ_ANDC),\n\n#define BRUCMP(CODE, CMP, CMPI, BI, COND, NEG_COND)                                          \\\n  {CODE, \"l r r\", CMP \"; \" BRC (COND, BI)},        /* cmp 7,ra1,rb2;bcond 7,l; */            \\\n    {CODE, \"l r u\", CMPI \"; \" BRC (COND, BI)},     /* cmpi 7,ra1,u;bcond 7,l;:*/             \\\n    {CODE, \"L r r\", CMP \"; \" BRCL (NEG_COND, BI)}, /* cmp 7,ra1,rb2;bneg_cond 7,o8;b L;l8:*/ \\\n  {                                                                                          \\\n    CODE, \"L r u\", CMPI \"; \" BRCL (NEG_COND, BI)                                             \\\n  } /* cmp 7,ra1,u;bneg_cond 7,o8;b L;l8:*/\n\n  /* LT: */\n  BRCMP (MIR_BLT, CMPD, CMPDI (i), 28, 12, 4),\n  BRCMP (MIR_BLTS, CMPW, CMPWI (i), 28, 12, 4),\n  BRFCMP (MIR_FBLT, 28, 12, LT_ANDC, 4, LT_OR),\n  BRFCMP (MIR_DBLT, 28, 12, LT_ANDC, 4, LT_OR),\n  BRUCMP (MIR_UBLT, CMPLD, CMPLDI, 28, 12, 4),\n  BRUCMP (MIR_UBLTS, CMPLW, CMPLWI, 28, 12, 4),\n\n  /* GE: */\n  BRCMP (MIR_BGE, CMPD, CMPDI (i), 28, 4, 12),\n  BRCMP (MIR_BGES, CMPW, CMPWI (i), 28, 4, 12),\n  BRFCMP (MIR_FBGE, 28, 4, LT_OR, 12, LT_ANDC),\n  BRFCMP (MIR_DBGE, 28, 4, LT_OR, 12, LT_ANDC),\n  BRUCMP (MIR_UBGE, CMPLD, CMPLDI, 28, 4, 12),\n  BRUCMP (MIR_UBGES, CMPLW, CMPLWI, 28, 4, 12),\n\n  /* GT: */\n  BRCMP (MIR_BGT, CMPD, CMPDI (i), 29, 12, 4),\n  BRCMP (MIR_BGTS, CMPW, CMPWI (i), 29, 12, 4),\n  BRFCMP (MIR_FBGT, 29, 12, GT_ANDC, 4, GT_OR),\n  BRFCMP (MIR_DBGT, 29, 12, GT_ANDC, 4, GT_OR),\n  BRUCMP (MIR_UBGT, CMPLD, CMPLDI, 29, 12, 4),\n  BRUCMP (MIR_UBGTS, CMPLW, CMPLWI, 29, 12, 4),\n\n  /* LE: */\n  BRCMP (MIR_BLE, CMPD, CMPDI (i), 29, 4, 12),\n  BRCMP (MIR_BLES, CMPW, CMPWI (i), 29, 4, 12),\n  BRFCMP (MIR_FBLE, 29, 4, GT_OR, 12, GT_ANDC),\n  BRFCMP (MIR_DBLE, 29, 4, GT_OR, 12, GT_ANDC),\n  BRUCMP (MIR_UBLE, CMPLD, CMPLDI, 29, 4, 12),\n  BRUCMP (MIR_UBLES, CMPLW, CMPLWI, 29, 4, 12),\n\n#define NEG \"o31 O104 rt0 ra1\"\n#define FNEG \"o63 O40 rt0 rb1\"\n\n  {MIR_NEG, \"r r\", NEG},   /* neg Rt,Ra */\n  {MIR_NEGS, \"r r\", NEG},  /* neg Rt,Ra */\n  {MIR_FNEG, \"r r\", FNEG}, /* fneg rt,rb */\n  {MIR_DNEG, \"r r\", FNEG}, /* fneg rt,rb */\n// ldneg is a builtin\n\n#define SHR(s) \"o31 O\" #s \" ra0 rs1 rb2\"\n  {MIR_LSH, \"r r r\", SHR (27)},                    /* sld ra,rs,rb */\n  {MIR_LSHS, \"r r r\", SHR (24)},                   /* slw ra,rs,rb */\n  {MIR_LSH, \"r r Sh\", \"o30 ra0 rs1 Sh meSh\"},      /* rldicr ra,rs,sh,63-sh */\n  {MIR_LSHS, \"r r sh\", \"o21 ra0 rs1 sh mb0 mesh\"}, /* rlwinm ra,rs,sh,0,31-sh */\n\n  {MIR_RSH, \"r r r\", SHR (794)},               /* srad ra,rs,rb */\n  {MIR_RSHS, \"r r r\", SHR (792)},              /* sraw ra,rs,rb */\n  {MIR_RSH, \"r r Sh\", \"o31 p413 rs1 ra0 Sh\"},  /* sradi ra,rs */\n  {MIR_RSHS, \"r r sh\", \"o31 O824 rs1 ra0 sh\"}, /* srawi ra,rs */\n\n  {MIR_URSH, \"r r r\", SHR (539)},                     /* srd ra,rs,rb */\n  {MIR_URSHS, \"r r r\", SHR (536)},                    /* srw ra,rs,rb */\n  {MIR_URSH, \"r r Sh\", \"o30 ra0 rs1 Shr mbSh\"},       /* rldicl ra,rs,64-sh,sh */\n  {MIR_URSHS, \"r r sh\", \"o21 ra0 rs1 shr mbsh me31\"}, /* rlwinm ra,rs,32-sh,sh,31 */\n\n// ??? nand nor\n#define LOGR(s) \"o31 O\" #s \"  ra0 rs1 rb2\"\n#define LOGU(s) \"o\" #s \"  ra0 rs1 u\"\n#define LOGUS(s) \"o\" #s \"  ra0 rs1 U\"\n  {MIR_AND, \"r r r\", LOGR (28)},   /* and ra,rs,rb */\n  {MIR_AND, \"r r u\", LOGU (28)},   /* andi. ra,rs,u */\n  {MIR_AND, \"r r U\", LOGUS (29)},  /* andis. ra,rs,U */\n  {MIR_ANDS, \"r r r\", LOGR (28)},  /* and ra,rs,rb */\n  {MIR_ANDS, \"r r u\", LOGU (28)},  /* andi. ra,rs,u */\n  {MIR_ANDS, \"r r U\", LOGUS (29)}, /* andis. ra,rs,U */\n\n  {MIR_OR, \"r r r\", LOGR (444)},  /* or ra,rs,rb */\n  {MIR_OR, \"r r u\", LOGU (24)},   /* ori ra,rs,u */\n  {MIR_OR, \"r r U\", LOGUS (25)},  /* oris ra,rs,U */\n  {MIR_ORS, \"r r r\", LOGR (444)}, /* or ra,rs,rb */\n  {MIR_ORS, \"r r u\", LOGU (24)},  /* ori ra,rs,u */\n  {MIR_ORS, \"r r U\", LOGUS (25)}, /* oris ra,rs,U */\n\n  {MIR_XOR, \"r r r\", LOGR (316)},  /* xor ra,rs,rb */\n  {MIR_XOR, \"r r u\", LOGU (26)},   /* xori ra,rs,u */\n  {MIR_XOR, \"r r U\", LOGUS (27)},  /* xoris ra,rs,U */\n  {MIR_XORS, \"r r r\", LOGR (316)}, /* xor ra,rs,rb */\n  {MIR_XORS, \"r r u\", LOGU (26)},  /* xori ra,rs,u */\n  {MIR_XORS, \"r r U\", LOGUS (27)}, /* xoris ra,rs,U */\n\n  /* std rt1,-16(r1); lfd f0,-16(r1); fcfids rt0,f0: */\n  {MIR_I2F, \"r r\", \"o62 rs1 mt; o50 ht32 mt; o59 O846 rt0 hb32\"},\n  /* std rt1,-16(r1); lfd f0,-16(r1); fcfid rt0,f0: */\n  {MIR_I2D, \"r r\", \"o62 rs1 mt; o50 ht32 mt; o63 O846 rt0 hb32\"},\n  /* std rt1,-16(r1); lfd f0,-16(r1); fcfidus rt0,f0: */\n  {MIR_UI2F, \"r r\", \"o62 rs1 mt; o50 ht32 mt; o59 O974 rt0 hb32\"},\n  /* std rt1,-16(r1); lfd f0,-16(r1); fcfid rt0,f0: */\n  {MIR_UI2D, \"r r\", \"o62 rs1 mt; o50 ht32 mt; o63 O974 rt0 hb32\"},\n  /* fctidz f0,rb; stfd f0,-16(r1);ld rt,-16(r1): */\n  {MIR_F2I, \"r r\", \"o63 O815 ht32 rb1; o54 hs32 mt; o58 rt0 mt\"},\n  /* fctidz f0,rb; stfd f0,-16(r1);ld rt,-16(r1): */\n  {MIR_D2I, \"r r\", \"o63 O815 ht32 rb1; o54 hs32 mt; o58 rt0 mt\"},\n  {MIR_F2D, \"r r\", \"o63 O72 rt0 rb1\"}, /* fmr rt,rb */\n  {MIR_D2F, \"r r\", \"o63 O12 rt0 rb1\"}, /* frsp rt,rb */\n                                       // i2ld, ui2ld, ld2i, f2ld, d2ld, ld2f, ld2d are builtins\n\n  {MIR_CALL, \"X h12 $\", \"o31 O467 rs1 sr9; o19 O528 BO20 BI0 LK1\"}, /* mtctr r12; bcctrl */\n  {MIR_CALL, \"X r $\",\n   \"o31 O444 ha12 rs1 rb1; o31 O467 rs1 sr9; o19 O528 BO20 BI0 LK1\"}, /* mr r12,r; mtctr r; bcctrl\n                                                                       */\n\n  {MIR_RET, \"$\", \"o19 O16 BO20 BI0\"}, /* bclr */\n\n  {MIR_JCALL, \"X r $\", \"o31 O467 rs1 sr9; o19 O528 BO20 BI0\"}, /* mtctr r; bcctr */\n  {MIR_JRET, \"r $\", \"o31 O467 rs0 sr9; o19 O528 BO20 BI0\"},    /* mtctr r; bcctr */\n\n/* subf r1,rt,r1; ldx r0,(r1,rt); std r0,0(r1);\n   add rt,r1,PPC64_STACK_HEADER_SIZE+PARAM_AREA_SIZE: */\n#define ALLOCA_END                             \\\n  \"o31 O40 ht1 ra0 hb1; o31 O21 ht0 ha1 rb0; \" \\\n  \"o62 hs0 ha1; o14 rt0 ha1 ih\"\n  /* addi rt,ra,15;rldicr rt,rt,0,59; ... : */\n  {MIR_ALLOCA, \"r r\", \"o14 rt0 ra1 i15; o30 ra0 rs0 Sh0 Me59; \" ALLOCA_END},\n  /* mov rt,ia; ...: */\n  {MIR_ALLOCA, \"r ia\", \"o14 rt0 ha0 ia; \" ALLOCA_END},\n\n  {MIR_BSTART, \"r\", \"o31 O444 ra0 hs1 hb1\"}, /* or ra,r1,r1 */\n  /* ld r0,0(r1);or r1,rs,rs; std r0,0(r1): */\n  {MIR_BEND, \"r\", \"o58 hs0 ha1;o31 O444 ha1 rs0 rb0; o62 hs0 ha1\"},\n\n  /* bl l4; mflr r0; rldicr r10,rt,3,60; add r10,r0,r10; ld r0,table-disp(r10); mtctr r0; bcctr;\n     TableContent: */\n  {MIR_SWITCH, \"r $\",\n   \"o18 l4 LK1; o31 O339 ht0 sr8; o30 ha10 rs0 Sh3 Me60; o31 O266 ht10 ha0 hb10; o58 ht0 ha10 T; \"\n   \"o31 O467 hs0 sr9; o19 O528 BO20 BI0\"},\n};\n\nstatic void target_get_early_clobbered_hard_regs (MIR_insn_t insn, MIR_reg_t *hr1, MIR_reg_t *hr2) {\n  MIR_insn_code_t code = insn->code;\n\n  *hr1 = *hr2 = MIR_NON_VAR;\n  if (code == MIR_MOD || code == MIR_MODS || code == MIR_UMOD || code == MIR_UMODS) {\n    *hr1 = R10_HARD_REG;\n  } else if (code == MIR_I2F || code == MIR_I2D || code == MIR_UI2F || code == MIR_UI2D\n             || code == MIR_F2I || code == MIR_D2I) {\n    *hr1 = F0_HARD_REG;\n  } else if (code == MIR_LDMOV) { /* if mem base reg is R0 */\n    *hr1 = R11_HARD_REG; /* don't use arg regs as ldmov can be used in param passing part */\n  } else if (code == MIR_CALL || code == MIR_INLINE) {\n    *hr1 = R12_HARD_REG;\n  } else if (code == MIR_SWITCH) {\n    *hr1 = R10_HARD_REG;\n  }\n}\n\nstatic int pattern_index_cmp (const void *a1, const void *a2) {\n  int i1 = *(const int *) a1, i2 = *(const int *) a2;\n  int c1 = (int) patterns[i1].code, c2 = (int) patterns[i2].code;\n\n  return c1 != c2 ? c1 - c2 : (long) i1 - (long) i2;\n}\n\nstatic void patterns_init (gen_ctx_t gen_ctx) {\n  MIR_alloc_t alloc = gen_alloc (gen_ctx);\n  int i, ind, n = sizeof (patterns) / sizeof (struct pattern);\n  MIR_insn_code_t prev_code, code;\n  insn_pattern_info_t *info_addr;\n  insn_pattern_info_t pinfo = {0, 0};\n\n  VARR_CREATE (int, pattern_indexes, alloc, 0);\n  for (i = 0; i < n; i++) VARR_PUSH (int, pattern_indexes, i);\n  qsort (VARR_ADDR (int, pattern_indexes), n, sizeof (int), pattern_index_cmp);\n  VARR_CREATE (insn_pattern_info_t, insn_pattern_info, alloc, 0);\n  for (i = 0; i < MIR_INSN_BOUND; i++) VARR_PUSH (insn_pattern_info_t, insn_pattern_info, pinfo);\n  info_addr = VARR_ADDR (insn_pattern_info_t, insn_pattern_info);\n  for (prev_code = MIR_INSN_BOUND, i = 0; i < n; i++) {\n    ind = VARR_GET (int, pattern_indexes, i);\n    if ((code = patterns[ind].code) != prev_code) {\n      if (i != 0) info_addr[prev_code].num = i - info_addr[prev_code].start;\n      info_addr[code].start = i;\n      prev_code = code;\n    }\n  }\n  assert (prev_code != MIR_INSN_BOUND);\n  info_addr[prev_code].num = n - info_addr[prev_code].start;\n}\n\nstatic int int16_p (int64_t i) { return -(1 << 15) <= i && i < (1 << 15); }\nstatic int uint16_p (uint64_t u) { return !(u >> 16); }\nstatic int int16_shifted_p (int64_t i) { return (i & 0xffff) == 0 && int16_p (i >> 16); }\nstatic int uint16_shifted_p (uint64_t u) { return (u & 0xffff) == 0 && uint16_p (u >> 16); }\nstatic int uint31_p (uint64_t u) { return !(u >> 31); }\nstatic int uint47_p (uint64_t u) { return !(u >> 47); }\nstatic int uint32_p (uint64_t u) { return !(u >> 32); }\nstatic int uint6_p (uint64_t u) { return !(u >> 6); }\nstatic int uint5_p (uint64_t u) { return !(u >> 5); }\nstatic int negative32_p (uint64_t u, uint64_t *n) {\n  if (((u >> 31) & 1) == 0) return FALSE;\n  /* high 32-bit part pattern: 0*1*, n contains number of ones. */\n  for (u >>= 32, *n = 0; u & 1; u >>= 1, (*n)++)\n    ;\n  return u == 0;\n}\n\nstatic int pattern_match_p (gen_ctx_t gen_ctx, const struct pattern *pat, MIR_insn_t insn,\n                            int use_short_label_p) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  size_t nop, nops = MIR_insn_nops (ctx, insn);\n  const char *p;\n  char ch, start_ch;\n  MIR_op_t op;\n  MIR_reg_t hr;\n\n  for (nop = 0, p = pat->pattern; *p != 0; p++, nop++) {\n    while (*p == ' ' || *p == '\\t') p++;\n    if (*p == '$') return TRUE;\n    if (MIR_call_code_p (insn->code) && nop >= nops) return FALSE;\n    gen_assert (nop < nops);\n    op = insn->ops[nop];\n    switch (start_ch = *p) {\n    case 'X': break;\n    case 'r':\n      if (op.mode != MIR_OP_VAR || op.u.var == LR_HARD_REG) return FALSE;\n      break;\n    case 'R':\n      if (op.mode != MIR_OP_VAR || op.u.var == R0_HARD_REG || op.u.var == LR_HARD_REG) return FALSE;\n      break;\n    case 'h':\n      if (op.mode != MIR_OP_VAR) return FALSE;\n      ch = *++p;\n      gen_assert ('0' <= ch && ch <= '9');\n      hr = ch - '0';\n      ch = *++p;\n      if ('0' <= ch && ch <= '9')\n        hr = hr * 10 + ch - '0';\n      else\n        --p;\n      gen_assert (hr <= MAX_HARD_REG);\n      if (op.u.var != hr) return FALSE;\n      break;\n    case 'm':\n    case 'M': {\n      MIR_type_t type, type2, type3 = MIR_T_BOUND;\n      int ds_p = FALSE, l_p = FALSE, br0_p = FALSE, u_p = TRUE, s_p = TRUE;\n\n      if (op.mode != MIR_OP_VAR_MEM) return FALSE;\n      ch = *++p;\n      switch (ch) {\n      case 'f':\n        type = MIR_T_F;\n        type2 = MIR_T_BOUND;\n        break;\n      case 'd':\n        ch = *++p;\n        if (ch == 's') {\n          ds_p = s_p = TRUE;\n          type = start_ch == 'M' ? MIR_T_I64 : MIR_T_I32;\n          type2 = start_ch == 'M' ? MIR_T_U64 : MIR_T_BOUND;\n#if MIR_PTR32\n          if (start_ch == 'm') type3 = MIR_T_P;\n#else\n          if (start_ch == 'M') type3 = MIR_T_P;\n#endif\n        } else {\n          p--;\n          type = MIR_T_D;\n          type2 = MIR_T_BOUND;\n        }\n        break;\n      case 'l':\n        ch = *++p;\n        gen_assert (ch == 'd' && start_ch != 'M');\n        ch = *++p;\n        if (ch != '0')\n          p--;\n        else\n          br0_p = TRUE;\n        l_p = TRUE;\n        type = MIR_T_LD;\n        type2 = MIR_T_BOUND;\n        break;\n      case 'u':\n      case 's':\n        u_p = ch == 'u';\n        s_p = ch == 's';\n        ch = *++p;\n        /* fall through */\n      default:\n        gen_assert ('0' <= ch && ch <= '3');\n        if (ch == '0') {\n          type = u_p ? MIR_T_U8 : MIR_T_I8;\n          type2 = u_p && s_p ? MIR_T_I8 : MIR_T_BOUND;\n        } else if (ch == '1') {\n          type = u_p ? MIR_T_U16 : MIR_T_I16;\n          type2 = u_p && s_p ? MIR_T_I16 : MIR_T_BOUND;\n        } else if (ch == '2') {\n          type = u_p ? MIR_T_U32 : MIR_T_I32;\n          type2 = u_p && s_p ? MIR_T_I32 : MIR_T_BOUND;\n#if MIR_PTR32\n          if (u_p) type3 = MIR_T_P;\n#endif\n        } else {\n          type = u_p ? MIR_T_U64 : MIR_T_I64;\n          type2 = u_p && s_p ? MIR_T_I64 : MIR_T_BOUND;\n#if MIR_PTR64\n          type3 = MIR_T_P;\n#endif\n        }\n      }\n      if (op.u.var_mem.type != type && op.u.var_mem.type != type2 && op.u.var_mem.type != type3)\n        return FALSE;\n      if (ds_p\n          && (op.u.var_mem.index != MIR_NON_VAR || op.u.var_mem.base == R0_HARD_REG\n              || op.u.var_mem.disp % 4 != 0 || !int16_p (op.u.var_mem.disp)))\n        return FALSE;\n      if (!ds_p && start_ch == 'm'\n          && (op.u.var_mem.index != MIR_NON_VAR || (!br0_p && op.u.var_mem.base == R0_HARD_REG)\n              || (br0_p && op.u.var_mem.base != R0_HARD_REG) || !int16_p (op.u.var_mem.disp)\n              || (l_p && !int16_p (op.u.var_mem.disp + 8))))\n        return FALSE;\n      if (!ds_p && start_ch == 'M'\n          && (op.u.var_mem.disp != 0\n              || (op.u.var_mem.index != MIR_NON_VAR && op.u.var_mem.scale != 1)\n              || (op.u.var_mem.base == R0_HARD_REG && op.u.var_mem.index != MIR_NON_VAR)))\n        return FALSE;\n      break;\n    }\n    case 'i':\n      if (op.mode != MIR_OP_INT && op.mode != MIR_OP_UINT) return FALSE;\n      ch = *++p;\n      if (ch == 'a') {\n        if (!int16_p ((op.u.i + 15) / 16 * 16)) return FALSE;\n      } else {\n        p--;\n        if (!int16_p (op.u.i)) return FALSE;\n      }\n      break;\n    case 'u':\n      if ((op.mode != MIR_OP_INT && op.mode != MIR_OP_UINT) || !uint16_p (op.u.u)) return FALSE;\n      break;\n    case 'I':\n      if ((op.mode != MIR_OP_INT && op.mode != MIR_OP_UINT) || !int16_shifted_p (op.u.i))\n        return FALSE;\n      break;\n    case 'U':\n      if ((op.mode != MIR_OP_INT && op.mode != MIR_OP_UINT) || !uint16_shifted_p (op.u.u))\n        return FALSE;\n      break;\n    case 'x':\n    case 'z':\n    case 'Z': {\n      uint64_t v, n;\n\n      if (op.mode != MIR_OP_INT && op.mode != MIR_OP_UINT && op.mode != MIR_OP_REF) return FALSE;\n      if (op.mode != MIR_OP_REF) {\n        v = op.u.u;\n      } else if (op.u.ref->item_type == MIR_data_item && op.u.ref->u.data->name != NULL\n                 && _MIR_reserved_ref_name_p (ctx, op.u.ref->u.data->name)) {\n        v = (uint64_t) op.u.ref->u.data->u.els;\n      } else {\n        v = (uint64_t) op.u.ref->addr;\n      }\n      if (start_ch == 'x') {\n        if (!negative32_p (v, &n)) return FALSE;\n      } else {\n        ch = *++p;\n        if (ch == 's') {\n          if (start_ch == 'z' ? !uint31_p (v) : !uint47_p (op.u.u)) return FALSE;\n        } else {\n          p--;\n          if (start_ch == 'z' && !uint32_p (v)) return FALSE;\n        }\n      }\n      break;\n    }\n    case 's':\n    case 'S':\n      ch = *++p;\n      gen_assert (ch == 'h');\n      if (op.mode != MIR_OP_INT && op.mode != MIR_OP_UINT) return FALSE;\n      if ((start_ch == 's' && !uint5_p (op.u.u)) || (start_ch == 'S' && !uint6_p (op.u.u)))\n        return FALSE;\n      break;\n    case 'l':\n      if (op.mode != MIR_OP_LABEL || !use_short_label_p) return FALSE;\n      break;\n    case 'L':\n      if (op.mode != MIR_OP_LABEL && op.mode != MIR_OP_REF) return FALSE;\n      break;\n    case 'W':\n      if (op.mode != MIR_OP_LABEL) return FALSE;\n      break;\n    default: gen_assert (FALSE);\n    }\n  }\n  gen_assert (nop == nops);\n  return TRUE;\n}\n\nstatic const char *find_insn_pattern_replacement (gen_ctx_t gen_ctx, MIR_insn_t insn,\n                                                  int use_short_label_p) {\n  int i;\n  const struct pattern *pat;\n  insn_pattern_info_t info = VARR_GET (insn_pattern_info_t, insn_pattern_info, insn->code);\n\n  for (i = 0; i < info.num; i++) {\n    pat = &patterns[VARR_GET (int, pattern_indexes, info.start + i)];\n    if (pattern_match_p (gen_ctx, pat, insn, use_short_label_p)) return pat->replacement;\n  }\n  return NULL;\n}\n\nstatic void patterns_finish (gen_ctx_t gen_ctx) {\n  VARR_DESTROY (int, pattern_indexes);\n  VARR_DESTROY (insn_pattern_info_t, insn_pattern_info);\n}\n\nstatic int dec_value (int ch) { return '0' <= ch && ch <= '9' ? ch - '0' : -1; }\n\nstatic uint64_t read_dec (const char **ptr) {\n  int v;\n  const char *p;\n  uint64_t res = 0;\n\n  for (p = *ptr; (v = dec_value (*p)) >= 0; p++) {\n    gen_assert ((res >> 60) == 0);\n    res = res * 10 + v;\n  }\n  gen_assert (p != *ptr);\n  *ptr = p - 1;\n  return res;\n}\n\nstatic uint32_t check_and_set_mask (uint32_t result_mask, uint32_t mask) {\n  gen_assert ((result_mask & mask) == 0);\n  return result_mask | mask;\n}\n\nstatic void put_uint32 (struct gen_ctx *gen_ctx, uint32_t v) {\n  VARR_PUSH_ARR (uint8_t, result_code, (uint8_t *) &v, sizeof (v)); /* reserve */\n  /* write with the right endianess: */\n  ((uint32_t *) (VARR_ADDR (uint8_t, result_code) + VARR_LENGTH (uint8_t, result_code)))[-1] = v;\n}\n\nstatic void put_uint64 (struct gen_ctx *gen_ctx, uint64_t v) {\n  VARR_PUSH_ARR (uint8_t, result_code, (uint8_t *) &v, sizeof (v)); /* reserve */\n  ((uint64_t *) (VARR_ADDR (uint8_t, result_code) + VARR_LENGTH (uint8_t, result_code)))[-1] = v;\n}\n\nstatic void set_int64 (uint8_t *addr, int64_t v) { *(int64_t *) addr = v; }\n\nstatic int64_t get_int64 (uint8_t *addr) { return *(int64_t *) addr; }\n\nstatic void out_insn (gen_ctx_t gen_ctx, MIR_insn_t insn, const char *replacement,\n                      void **jump_addrs) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  size_t nops = MIR_insn_nops (ctx, insn);\n  size_t offset;\n  const char *p, *insn_str;\n  label_ref_t lr;\n  int switch_table_addr_insn_start = -1;\n  uint32_t nop_binsn = 24 << (32 - 6); /* ori 0,0,0 */\n\n  if (insn->code == MIR_ALLOCA\n      && (insn->ops[1].mode == MIR_OP_INT || insn->ops[1].mode == MIR_OP_UINT))\n    insn->ops[1].u.u = (insn->ops[1].u.u + 15) & -16;\n  for (insn_str = replacement;; insn_str = p + 1) {\n    MIR_op_t op;\n    char ch, ch2, start_ch;\n    uint32_t binsn = 0;\n    int opcode = -1, opcode2 = -1, opcode3 = -1, opcode4 = -1, rt = -1, rs = -1, ra = -1, rb = -1,\n        rc = -1, spreg = -1, sh = -1, Sh = -1;\n    int disp = -1, disp4 = -1, mb = -1, me = -1, Mb = -1, Me = -1, bf = -1, BO = -1, BI = -1,\n        imm = -1, LK = -1;\n    int d = -1, lab_off = -1, lb = -1, label_ref_num = -1, n;\n    uint32_t binsn_mask = 0;\n    int switch_table_addr_p = FALSE;\n\n    for (p = insn_str; (ch = *p) != '\\0' && ch != ';'; p++) {\n      if ((ch = *p) == 0 || ch == ';') break;\n      switch ((start_ch = ch = *p)) {\n      case ' ':\n      case '\\t': break;\n      case 'o':\n        ch2 = *++p;\n        gen_assert (dec_value (ch2) >= 0 && opcode < 0);\n        opcode = read_dec (&p);\n        break;\n      case 'O':\n        ch2 = *++p;\n        gen_assert (dec_value (ch2) >= 0 && opcode2 < 0);\n        opcode2 = read_dec (&p);\n        break;\n      case 'p':\n        ch2 = *++p;\n        gen_assert (dec_value (ch2) >= 0 && opcode3 < 0);\n        opcode3 = read_dec (&p);\n        break;\n      case 'P':\n        ch2 = *++p;\n        gen_assert (dec_value (ch2) >= 0 && opcode4 < 0);\n        opcode4 = read_dec (&p);\n        break;\n      case 'r':\n      case 'n':\n      case 'R':\n      case 'h': {\n        int reg;\n\n        ch2 = *++p;\n        gen_assert (ch2 == 't' || ch2 == 's' || ch2 == 'a' || ch2 == 'b' || ch2 == 'c');\n        gen_assert (start_ch != 'R' || ch2 == 'a');\n        ch = *++p;\n        if (start_ch == 'h') {\n          reg = read_dec (&p);\n        } else {\n          gen_assert ('0' <= ch && ch <= '2' && ch - '0' < (int) insn->nops);\n          op = insn->ops[ch - '0'];\n          gen_assert (op.mode == MIR_OP_VAR);\n          reg = op.u.var + (start_ch == 'n' ? 1 : 0);\n        }\n        if (reg > R31_HARD_REG) reg -= F0_HARD_REG;\n        gen_assert (reg <= 31);\n        if (ch2 == 't') {\n          gen_assert (rt < 0);\n          rt = reg;\n        } else if (ch2 == 's') {\n          gen_assert (rs < 0);\n          rs = reg;\n        } else if (ch2 == 'a') {\n          gen_assert (ra < 0);\n          ra = reg;\n        } else if (ch2 == 'b') {\n          gen_assert (rb < 0);\n          rb = reg;\n        } else {\n          gen_assert (rc < 0);\n          rc = reg;\n        }\n        break;\n      }\n      case 's':\n        ch2 = *++p;\n        if (ch2 == 'r') {\n          ch2 = *++p;\n          gen_assert (dec_value (ch2) >= 0 && spreg < 0);\n          spreg = read_dec (&p);\n        } else if (ch2 == 'h') {\n          op = insn->ops[2];\n          ch2 = *++p;\n          gen_assert (sh < 0);\n          if (dec_value (ch2) >= 0) {\n            sh = read_dec (&p);\n          } else if (ch2 == 'r') {\n            gen_assert (op.mode == MIR_OP_INT || op.mode == MIR_OP_UINT);\n            sh = 32 - op.u.u;\n          } else {\n            --p;\n            gen_assert (op.mode == MIR_OP_INT || op.mode == MIR_OP_UINT);\n            sh = op.u.u;\n          }\n        }\n        break;\n      case 'S':\n        ch2 = *++p;\n        gen_assert (ch2 == 'h' && Sh < 0);\n        ch2 = *++p;\n        if (dec_value (ch2) >= 0) {\n          Sh = read_dec (&p);\n        } else if (ch2 == 'r') {\n          op = insn->ops[2];\n          gen_assert (op.mode == MIR_OP_INT || op.mode == MIR_OP_UINT);\n          Sh = 64 - op.u.u;\n        } else {\n          --p;\n          op = insn->ops[2];\n          gen_assert (op.mode == MIR_OP_INT || op.mode == MIR_OP_UINT);\n          Sh = op.u.u;\n        }\n        break;\n      case 'M': {\n        int b_p;\n\n        ch2 = *++p;\n        if (ch2 == '9') {\n          ch2 = *++p;\n          gen_assert (ch2 == '1');\n          ch2 = *++p;\n          gen_assert (ch2 == '0' && ra < 0 && rb < 0);\n          ra = R9_HARD_REG;\n          rb = R10_HARD_REG;\n        } else if ((b_p = ch2 == 'b') || ch2 == 'e') {\n          ch2 = *++p;\n          gen_assert (dec_value (ch2) >= 0 && Mb < 0 && Me < 0);\n          if (b_p) {\n            Mb = read_dec (&p);\n            Mb = ((Mb & 0x1f) << 1) | ((Mb >> 5) & 1);\n          } else {\n            Me = read_dec (&p);\n            Me = ((Me & 0x1f) << 1) | ((Me >> 5) & 1);\n          }\n        } else {\n          op = insn->ops[0].mode == MIR_OP_VAR_MEM ? insn->ops[0] : insn->ops[1];\n          gen_assert (op.mode == MIR_OP_VAR_MEM);\n          if (ch2 == 'd') {\n            ch2 = *++p;\n            gen_assert (ch2 == 's' && ra < 0 && disp4 < 0);\n            ra = (int) op.u.var_mem.base;\n            if (op.u.var_mem.base == MIR_NON_VAR) ra = R0_HARD_REG;\n            disp4 = op.u.var_mem.disp & 0xffff;\n            gen_assert ((disp4 & 0x3) == 0);\n          } else {\n            --p;\n            gen_assert (ra < 0 && rb < 0);\n            ra = (int) op.u.var_mem.base;\n            rb = (int) op.u.var_mem.index;\n            if (op.u.var_mem.index == MIR_NON_VAR) {\n              rb = ra;\n              ra = R0_HARD_REG;\n            } else if (op.u.var_mem.base == MIR_NON_VAR) {\n              ra = R0_HARD_REG;\n            } else if (ra == R0_HARD_REG) {\n              ra = rb;\n              ra = R0_HARD_REG;\n            }\n          }\n        }\n        break;\n      }\n      case 'm': {\n        int b_p, single_p;\n\n        ch2 = *++p;\n        if (ch2 == 't') {\n          gen_assert (ra < 0 && disp < 0);\n          disp = (-16) & 0xffff;\n          ra = R1_HARD_REG;\n        } else if ((b_p = ch2 == 'b') || ch2 == 'e') {\n          ch2 = *++p;\n          if (dec_value (ch2) >= 0) {\n            if (b_p) {\n              gen_assert (mb < 0);\n              mb = read_dec (&p);\n            } else {\n              gen_assert (me < 0);\n              me = read_dec (&p);\n            }\n          } else {\n            single_p = ch2 == 'S';\n            gen_assert (ch2 == 's' || ch2 == 'S');\n            ch2 = *++p;\n            gen_assert (ch2 == 'h');\n            op = insn->ops[2];\n            gen_assert (op.mode == MIR_OP_INT || op.mode == MIR_OP_UINT);\n            if (single_p) {\n              gen_assert (Mb < 0 && Me < 0);\n              if (b_p) {\n                Mb = op.u.i;\n                Mb = ((Mb & 0x1f) << 1) | ((Mb >> 5) & 1);\n              } else {\n                Me = 63 - op.u.i;\n                Me = ((Me & 0x1f) << 1) | ((Me >> 5) & 1);\n              }\n            } else if (b_p) {\n              gen_assert (mb < 0);\n              mb = op.u.i;\n            } else {\n              gen_assert (me < 0);\n              me = 31 - op.u.i;\n            }\n          }\n        } else {\n          op = insn->ops[0].mode == MIR_OP_VAR_MEM ? insn->ops[0] : insn->ops[1];\n          gen_assert (op.mode == MIR_OP_VAR_MEM);\n          if (ch2 == 'd') {\n            ch2 = *++p;\n            gen_assert (ch2 == 's' && ra < 0 && disp4 < 0);\n            ra = (int) op.u.var_mem.base;\n            if (op.u.var_mem.base == MIR_NON_VAR) ra = R0_HARD_REG;\n            disp4 = op.u.var_mem.disp & 0xffff;\n            gen_assert ((disp4 & 0x3) == 0);\n          } else {\n            if (ch2 != 'n') --p;\n            gen_assert (ra < 0 && disp < 0);\n            ra = (int) op.u.var_mem.base;\n            if (op.u.var_mem.base == MIR_NON_VAR) ra = R0_HARD_REG;\n            disp = (op.u.var_mem.disp + (ch2 != 'n' ? 0 : 8)) & 0xffff;\n          }\n        }\n        break;\n      }\n      case 'd':\n        ch2 = *++p;\n        gen_assert (d < 0 && dec_value (ch2) >= 0);\n        d = read_dec (&p);\n        break;\n      case 'i':\n        ch2 = *++p;\n        if (ch2 == 'a') {\n          op = insn->ops[nops - 1];\n          gen_assert (op.mode == MIR_OP_INT || op.mode == MIR_OP_UINT);\n          gen_assert (imm < 0);\n          imm = (op.u.i + 15) / 16 * 16;\n          break;\n        } else if (ch2 == 'h') {\n          gen_assert (imm < 0);\n          imm = PPC64_STACK_HEADER_SIZE + param_save_area_size;\n          break;\n        } else if (dec_value (ch2) >= 0) {\n          gen_assert (imm < 0);\n          imm = read_dec (&p);\n          break;\n        }\n        p--;\n        /* fall through */\n      case 'u':\n      case 'I':\n      case 'U':\n        op = insn->ops[nops - 1];\n        gen_assert (op.mode == MIR_OP_INT || op.mode == MIR_OP_UINT);\n        gen_assert (imm < 0);\n        imm = (start_ch == 'i' || start_ch == 'u' ? op.u.u : op.u.u >> 16) & 0xffff;\n        break;\n      case 'x':\n      case 'z': {\n        int ok_p;\n        uint64_t v;\n\n        op = insn->ops[nops - 1];\n        gen_assert (op.mode == MIR_OP_INT || op.mode == MIR_OP_UINT || op.mode == MIR_OP_REF);\n        if (op.mode != MIR_OP_REF) {\n          v = op.u.u;\n        } else if (op.u.ref->item_type == MIR_data_item && op.u.ref->u.data->name != NULL\n                   && _MIR_reserved_ref_name_p (ctx, op.u.ref->u.data->name)) {\n          v = (uint64_t) op.u.ref->u.data->u.els;\n        } else {\n          v = (uint64_t) op.u.ref->addr;\n        }\n        if (start_ch == 'x') {\n          uint64_t num;\n          ok_p = negative32_p (v, &num);\n          num = 32 - num;\n          gen_assert (Mb < 0 && ok_p);\n          Mb = ((num & 0x1f) << 1) | ((num >> 5) & 1);\n        } else {\n          gen_assert (imm < 0);\n          n = dec_value (*++p);\n          gen_assert (n >= 0 && n <= 3);\n          imm = (v >> (3 - n) * 16) & 0xffff;\n        }\n        break;\n      }\n      case 'b':\n        ch2 = *++p;\n        gen_assert (ch2 == 'f');\n        ch2 = *++p;\n        gen_assert (dec_value (ch2) >= 0);\n        gen_assert (bf < 0);\n        bf = read_dec (&p);\n        break;\n      case 'B': {\n        int o_p;\n\n        ch2 = *++p;\n        gen_assert (ch2 == 'O' || ch2 == 'I');\n        o_p = ch2 == 'O';\n        ch2 = *++p;\n        gen_assert (dec_value (ch2) >= 0);\n        if (o_p) {\n          gen_assert (BO < 0);\n          BO = read_dec (&p);\n        } else {\n          gen_assert (BI < 0);\n          BI = read_dec (&p);\n        }\n        break;\n      }\n      case 'l': {\n        ch2 = *++p;\n        if ((n = dec_value (ch2)) >= 0) {\n          gen_assert (lab_off < 0 && (n & 0x3) == 0);\n          lab_off = n;\n        } else {\n          --p;\n          gen_assert (insn->code != MIR_CALL);\n          op = insn->ops[0];\n          gen_assert (op.mode == MIR_OP_LABEL);\n          lr.abs_addr_p = FALSE;\n          lr.branch_type = BRCOND;\n          lr.label_val_disp = 0;\n          if (jump_addrs == NULL)\n            lr.u.label = op.u.label;\n          else\n            lr.u.jump_addr = jump_addrs[0];\n          label_ref_num = VARR_LENGTH (label_ref_t, label_refs);\n          VARR_PUSH (label_ref_t, label_refs, lr);\n        }\n        break;\n      }\n      case 'L': {\n        ch2 = *++p;\n        if (ch2 == 'K') {\n          ch2 = *++p;\n          gen_assert (LK < 0 && dec_value (ch2) >= 0);\n          LK = read_dec (&p);\n          gen_assert (LK <= 1);\n        } else if ((n = dec_value (ch2)) >= 0) {\n          gen_assert (lb < 0);\n          lb = n;\n        } else {\n          --p;\n          op = insn->ops[insn->code != MIR_CALL ? 0 : 1];\n          gen_assert (op.mode == MIR_OP_LABEL);\n          lr.abs_addr_p = FALSE;\n          lr.branch_type = JUMP;\n          lr.label_val_disp = 0;\n          if (jump_addrs == NULL)\n            lr.u.label = op.u.label;\n          else\n            lr.u.jump_addr = jump_addrs[0];\n          label_ref_num = VARR_LENGTH (label_ref_t, label_refs);\n          VARR_PUSH (label_ref_t, label_refs, lr);\n        }\n        break;\n      }\n      case 'W': {\n        op = insn->ops[1];\n        gen_assert (insn->code == MIR_LADDR && op.mode == MIR_OP_LABEL);\n        lr.abs_addr_p = FALSE;\n        lr.branch_type = LADDR;\n        lr.label_val_disp = 0;\n        if (jump_addrs == NULL)\n          lr.u.label = op.u.label;\n        else\n          lr.u.jump_addr = jump_addrs[0];\n        label_ref_num = VARR_LENGTH (label_ref_t, label_refs);\n        VARR_PUSH (label_ref_t, label_refs, lr);\n        break;\n      }\n      case 'a':\n        gen_assert (imm < 0);\n        ch2 = *++p;\n        gen_assert (ch2 == 't' || ch2 == 'a');\n        imm = ch2 == 't' ? PPC64_TOC_OFFSET : 15 + PPC64_STACK_HEADER_SIZE + param_save_area_size;\n        break;\n      case 'T':\n        gen_assert (!switch_table_addr_p && switch_table_addr_insn_start < 0);\n        switch_table_addr_p = TRUE;\n        break;\n      default: gen_assert (FALSE);\n      }\n    }\n\n    if (opcode >= 0) {\n      gen_assert (opcode < 64);\n      binsn |= opcode << (32 - 6);\n      binsn_mask = check_and_set_mask (binsn_mask, 0x3f << (32 - 6));\n    }\n    if (opcode2 >= 0) {\n      gen_assert (opcode2 < (1 << 10));\n      binsn |= opcode2 << 1;\n      binsn_mask = check_and_set_mask (binsn_mask, 0x3ff << 1);\n    }\n    if (opcode3 >= 0) {\n      gen_assert (opcode3 < (1 << 9));\n      binsn |= opcode3 << 2;\n      binsn_mask = check_and_set_mask (binsn_mask, 0x1ff << 2);\n    }\n    if (opcode4 >= 0) {\n      gen_assert (opcode4 < (1 << 5));\n      binsn |= opcode4 << 1;\n      binsn_mask = check_and_set_mask (binsn_mask, 0x1f << 1);\n    }\n    if (rt >= 0) {\n      gen_assert (rt < 32);\n      binsn |= rt << (32 - 11);\n      binsn_mask = check_and_set_mask (binsn_mask, 0x1f << (32 - 11));\n    }\n    if (rs >= 0) {\n      gen_assert (rs < 32);\n      binsn |= rs << (32 - 11);\n      binsn_mask = check_and_set_mask (binsn_mask, 0x1f << (32 - 11));\n    }\n    if (ra >= 0) {\n      gen_assert (ra < 32);\n      binsn |= ra << (32 - 16);\n      binsn_mask = check_and_set_mask (binsn_mask, 0x1f << (32 - 16));\n    }\n    if (rb >= 0) {\n      gen_assert (rb < 32);\n      binsn |= rb << (32 - 21);\n      binsn_mask = check_and_set_mask (binsn_mask, 0x1f << (32 - 21));\n    }\n    if (rc >= 0) {\n      gen_assert (rc < 32);\n      binsn |= rc << (32 - 26);\n      binsn_mask = check_and_set_mask (binsn_mask, 0x1f << (32 - 26));\n    }\n    if (spreg >= 0) {\n      gen_assert (spreg < (1 << 5));\n      binsn |= spreg << 16;\n      binsn_mask = check_and_set_mask (binsn_mask, 0x3ff << (32 - 21));\n    }\n    if (disp >= 0) {\n      gen_assert (disp < (1 << 16));\n      binsn |= disp;\n      binsn_mask = check_and_set_mask (binsn_mask, 0xffff);\n    }\n    if (disp4 >= 0) {\n      gen_assert (disp4 < (1 << 16) && (disp4 & 0x3) == 0);\n      binsn |= disp4;\n      binsn_mask = check_and_set_mask (binsn_mask, 0xfffc);\n    }\n    if (d >= 0) {\n      gen_assert (d < (1 << 2));\n      binsn |= d;\n      binsn_mask = check_and_set_mask (binsn_mask, 0x3);\n    }\n    if (Sh >= 0) {\n      gen_assert (Sh < (1 << 6));\n      binsn |= ((Sh & 0x1f) << (32 - 21));\n      binsn |= (Sh >> 4) & 0x2;\n      binsn_mask = check_and_set_mask (binsn_mask, (0x1f << (32 - 21)) | 0x2);\n    }\n    if (sh >= 0) {\n      gen_assert (sh < (1 << 5));\n      binsn |= sh << (32 - 21);\n      binsn_mask = check_and_set_mask (binsn_mask, 0x1f << (32 - 21));\n    }\n    if (mb >= 0) {\n      gen_assert (mb < (1 << 5));\n      binsn |= (mb & 0x1f) << 6;\n      binsn_mask = check_and_set_mask (binsn_mask, (0x1f << 6));\n    }\n    if (me >= 0) {\n      gen_assert (me < (1 << 5));\n      binsn |= (me & 0x1f) << 1;\n      binsn_mask = check_and_set_mask (binsn_mask, (0x1f << 1));\n    }\n    if (Mb >= 0) {\n      gen_assert (Mb < (1 << 6));\n      binsn |= (Mb & 0x3f) << (32 - 27);\n      binsn_mask = check_and_set_mask (binsn_mask, (0x3f << (32 - 27)));\n    }\n    if (Me >= 0) {\n      gen_assert (Me < (1 << 6));\n      binsn |= (Me & 0x3f) << (32 - 27);\n      binsn |= 1 << 2;\n      binsn_mask = check_and_set_mask (binsn_mask, (0x3f << (32 - 27)) | (1 << 2));\n    }\n    if (imm >= 0) {\n      gen_assert (imm < (1 << 16));\n      binsn |= imm;\n      binsn_mask = check_and_set_mask (binsn_mask, 0xffff);\n    }\n    if (lab_off >= 0) {\n      gen_assert (lab_off < (1 << 16) && (lab_off & 0x3) == 0);\n      binsn |= lab_off;\n      binsn_mask = check_and_set_mask (binsn_mask, 0xfffc);\n    }\n    if (bf >= 0) {\n      gen_assert (bf < 8);\n      binsn |= bf << (32 - 9);\n      binsn_mask = check_and_set_mask (binsn_mask, 0x7 << (32 - 9));\n    }\n    if (BO >= 0) {\n      gen_assert (BO < 32);\n      binsn |= BO << 21;\n      binsn_mask = check_and_set_mask (binsn_mask, 0x1f << 21);\n    }\n    if (BI >= 0) {\n      gen_assert (BI < 32);\n      binsn |= BI << 16;\n      binsn_mask = check_and_set_mask (binsn_mask, 0x1f << 16);\n    }\n    if (LK >= 0) {\n      gen_assert (LK < 2);\n      binsn |= LK;\n      binsn_mask = check_and_set_mask (binsn_mask, 0x1);\n    }\n    if (lb >= 0) {\n      gen_assert (lb < 2);\n      binsn |= lb << (32 - 11);\n      binsn_mask = check_and_set_mask (binsn_mask, 1 << (32 - 11));\n    }\n    if (label_ref_num >= 0) VARR_ADDR (label_ref_t, label_refs)\n    [label_ref_num].label_val_disp = VARR_LENGTH (uint8_t, result_code);\n\n    if (switch_table_addr_p) switch_table_addr_insn_start = VARR_LENGTH (uint8_t, result_code);\n    put_uint32 (gen_ctx, binsn); /* output the machine insn */\n\n    if (*p == 0) break;\n  }\n\n  if (switch_table_addr_insn_start < 0) return;\n  if (VARR_LENGTH (uint8_t, result_code) % 8 == 4) put_uint32 (gen_ctx, nop_binsn);\n  /* pc offset of T plus 3 insns after T: see switch */\n  offset = (VARR_LENGTH (uint8_t, result_code) - switch_table_addr_insn_start) + 12;\n  gen_assert ((offset & 0x3) == 0);\n  *(uint32_t *) (VARR_ADDR (uint8_t, result_code) + switch_table_addr_insn_start) |= offset;\n  gen_assert (insn->code == MIR_SWITCH);\n  for (size_t i = 1; i < insn->nops; i++) {\n    gen_assert (insn->ops[i].mode == MIR_OP_LABEL);\n    lr.abs_addr_p = TRUE;\n    lr.branch_type = BCTR; /* the value does not matter */\n    lr.label_val_disp = VARR_LENGTH (uint8_t, result_code);\n    if (jump_addrs == NULL)\n      lr.u.label = insn->ops[i].u.label;\n    else\n      lr.u.jump_addr = jump_addrs[i - 1];\n    VARR_PUSH (label_ref_t, label_refs, lr);\n    put_uint64 (gen_ctx, 0); /* reserve mem for label address */\n  }\n}\n\nstatic int target_memory_ok_p (gen_ctx_t gen_ctx, MIR_op_t *op_ref) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  if (op_ref->mode != MIR_OP_VAR_MEM) return FALSE;\n  if (op_ref->u.var_mem.index == MIR_NON_VAR && int16_p (op_ref->u.var_mem.disp)) return TRUE;\n  size_t size = _MIR_type_size (ctx, op_ref->u.var_mem.type);\n  if (op_ref->u.var_mem.index != MIR_NON_VAR && op_ref->u.var_mem.disp == 0\n      && op_ref->u.var_mem.scale == size)\n    return TRUE;\n  if (op_ref->u.var_mem.index == MIR_NON_VAR && op_ref->u.var_mem.disp % 4 == 0\n      && (size == 4 || size == 8) && int16_p (op_ref->u.var_mem.disp))\n    return TRUE;\n  return FALSE;\n}\n\nstatic int target_insn_ok_p (gen_ctx_t gen_ctx, MIR_insn_t insn) {\n  return find_insn_pattern_replacement (gen_ctx, insn, TRUE) != NULL;\n}\n\nstatic void target_split_insns (gen_ctx_t gen_ctx) {\n  for (MIR_insn_t insn = DLIST_HEAD (MIR_insn_t, curr_func_item->u.func->insns); insn != NULL;\n       insn = DLIST_NEXT (MIR_insn_t, insn)) {\n    MIR_insn_code_t code = insn->code;\n\n    if ((code != MIR_RSH && code != MIR_LSH && code != MIR_URSH && code != MIR_RSHS\n         && code != MIR_LSHS && code != MIR_URSHS)\n        || (insn->ops[2].mode != MIR_OP_INT && insn->ops[2].mode != MIR_OP_UINT))\n      continue;\n    if (insn->ops[2].u.i == 0) {\n      gen_mov (gen_ctx, insn, MIR_MOV, insn->ops[0], insn->ops[1]);\n      MIR_insn_t old_insn = insn;\n      insn = DLIST_PREV (MIR_insn_t, insn);\n      gen_delete_insn (gen_ctx, old_insn);\n    } else {\n      if (insn->ops[2].mode == MIR_OP_INT && insn->ops[2].u.i < 0) {\n        switch (code) {\n        case MIR_RSH: insn->code = MIR_LSH; break;\n        case MIR_URSH: insn->code = MIR_LSH; break;\n        case MIR_LSH: insn->code = MIR_RSH; break;\n        case MIR_RSHS: insn->code = MIR_LSHS; break;\n        case MIR_URSHS: insn->code = MIR_LSHS; break;\n        case MIR_LSHS: insn->code = MIR_RSHS; break;\n        default: gen_assert (FALSE); break;\n        }\n        insn->ops[2].u.i = -insn->ops[2].u.i;\n      }\n      if (code == MIR_RSH || code == MIR_LSH || code == MIR_URSH) {\n        if (insn->ops[2].u.i > 64) insn->ops[2].u.i = 64;\n      } else if (insn->ops[2].u.i > 32) {\n        insn->ops[2].u.i = 32;\n      }\n    }\n  }\n}\n\nstatic uint8_t *target_translate (gen_ctx_t gen_ctx, size_t *len) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  size_t i;\n  int short_label_disp_fail_p, n_iter = 0;\n  MIR_insn_t insn;\n  const char *replacement;\n\n  gen_assert (curr_func_item->item_type == MIR_func_item);\n  do {\n    VARR_TRUNC (uint8_t, result_code, 0);\n    VARR_TRUNC (label_ref_t, label_refs, 0);\n    VARR_TRUNC (uint64_t, abs_address_locs, 0);\n    short_label_disp_fail_p = FALSE;\n    for (insn = DLIST_HEAD (MIR_insn_t, curr_func_item->u.func->insns); insn != NULL;\n         insn = DLIST_NEXT (MIR_insn_t, insn)) {\n      if (insn->code == MIR_LABEL) {\n        set_label_disp (gen_ctx, insn, VARR_LENGTH (uint8_t, result_code));\n      } else if (insn->code != MIR_USE) {\n        int use_short_label_p = TRUE;\n\n        if (n_iter > 0 && MIR_branch_code_p (insn->code)) {\n          MIR_label_t label = insn->ops[0].u.label;\n          int64_t offset = (int64_t) get_label_disp (gen_ctx, label)\n                           - (int64_t) VARR_LENGTH (uint8_t, result_code);\n\n          use_short_label_p = ((offset < 0 ? -offset : offset) & ~(int64_t) 0x7fff) == 0;\n        }\n        replacement = find_insn_pattern_replacement (gen_ctx, insn, use_short_label_p);\n        if (replacement == NULL) {\n          fprintf (stderr, \"fatal failure in matching insn:\");\n          MIR_output_insn (ctx, stderr, insn, curr_func_item->u.func, TRUE);\n          exit (1);\n        } else {\n          gen_assert (replacement != NULL);\n          out_insn (gen_ctx, insn, replacement, NULL);\n        }\n      }\n    }\n    /* Setting up labels */\n    for (i = 0; i < VARR_LENGTH (label_ref_t, label_refs); i++) {\n      label_ref_t lr = VARR_GET (label_ref_t, label_refs, i);\n\n      if (lr.abs_addr_p) {\n        set_int64 (&VARR_ADDR (uint8_t, result_code)[lr.label_val_disp],\n                   (int64_t) get_label_disp (gen_ctx, lr.u.label));\n        VARR_PUSH (uint64_t, abs_address_locs, lr.label_val_disp);\n      } else if (lr.branch_type == LADDR) {\n        int64_t offset\n          = (int64_t) get_label_disp (gen_ctx, lr.u.label) - (int64_t) lr.label_val_disp + 4;\n        int hi = offset >> 16, low = offset & 0xffff;\n        if ((low & 0x8000) != 0) hi++;\n        *(uint32_t *) (VARR_ADDR (uint8_t, result_code) + lr.label_val_disp) |= hi & 0xffff;\n        *(uint32_t *) (VARR_ADDR (uint8_t, result_code) + lr.label_val_disp + 4) |= low;\n      } else if (lr.branch_type == BRCOND) { /* 14-bit relative addressing */\n        int64_t offset\n          = (int64_t) get_label_disp (gen_ctx, lr.u.label) - (int64_t) lr.label_val_disp;\n\n        gen_assert ((offset & 0x3) == 0);\n        if (((offset < 0 ? -offset : offset) & ~(int64_t) 0x7fff) != 0) {\n          short_label_disp_fail_p = TRUE;\n        } else {\n          *(uint32_t *) (VARR_ADDR (uint8_t, result_code) + lr.label_val_disp)\n            |= ((offset / 4) & 0x3fff) << 2;\n        }\n      } else { /* 24-bit relative address */\n        int64_t offset\n          = (int64_t) get_label_disp (gen_ctx, lr.u.label) - (int64_t) lr.label_val_disp;\n        gen_assert ((offset & 0x3) == 0\n                    && ((offset < 0 ? -offset : offset) & ~(int64_t) 0x1ffffff) == 0);\n        *(uint32_t *) (VARR_ADDR (uint8_t, result_code) + lr.label_val_disp)\n          |= ((offset / 4) & 0xffffff) << 2;\n      }\n    }\n    n_iter++;\n  } while (short_label_disp_fail_p);\n  while (VARR_LENGTH (uint8_t, result_code) % 16 != 0) /* Align the pool */\n    VARR_PUSH (uint8_t, result_code, 0);\n  *len = VARR_LENGTH (uint8_t, result_code);\n  return VARR_ADDR (uint8_t, result_code);\n}\n\nstatic void target_rebase (gen_ctx_t gen_ctx, uint8_t *base) {\n  MIR_code_reloc_t reloc;\n\n  VARR_TRUNC (MIR_code_reloc_t, relocs, 0);\n  for (size_t i = 0; i < VARR_LENGTH (uint64_t, abs_address_locs); i++) {\n    reloc.offset = VARR_GET (uint64_t, abs_address_locs, i);\n    reloc.value = base + get_int64 (base + reloc.offset);\n    VARR_PUSH (MIR_code_reloc_t, relocs, reloc);\n  }\n  _MIR_update_code_arr (gen_ctx->ctx, base, VARR_LENGTH (MIR_code_reloc_t, relocs),\n                        VARR_ADDR (MIR_code_reloc_t, relocs));\n  gen_setup_lrefs (gen_ctx, base);\n}\n\nstatic void target_change_to_direct_calls (MIR_context_t ctx MIR_UNUSED) {}\n\nstruct target_bb_version {\n  uint8_t *base;\n  label_ref_t branch_ref; /* label cand used for jump to this bb version */\n};\n\nstatic void target_init_bb_version_data (target_bb_version_t data) {\n  data->base = NULL; /* we don't know origin branch */\n}\n\nstatic void target_bb_translate_start (gen_ctx_t gen_ctx) {\n  short_bb_branch_p = FALSE;\n  VARR_TRUNC (uint8_t, result_code, 0);\n  VARR_TRUNC (label_ref_t, label_refs, 0);\n  VARR_TRUNC (uint64_t, abs_address_locs, 0);\n}\n\nstatic void target_bb_insn_translate (gen_ctx_t gen_ctx, MIR_insn_t insn, void **jump_addrs) {\n  const char *replacement;\n  if (insn->code == MIR_LABEL) return;\n  replacement = find_insn_pattern_replacement (gen_ctx, insn, TRUE);\n  gen_assert (replacement != NULL);\n  out_insn (gen_ctx, insn, replacement, jump_addrs);\n  if (MIR_branch_code_p (insn->code) && insn->code != MIR_JMP) short_bb_branch_p = TRUE;\n}\n\nstatic void target_output_jump (gen_ctx_t gen_ctx, void **jump_addrs) {\n  out_insn (gen_ctx, temp_jump, temp_jump_replacement, jump_addrs);\n}\n\nstatic uint8_t *target_bb_translate_finish (gen_ctx_t gen_ctx, size_t *len) {\n  /* add nops for possible conversion short branch or jump to branch and bctr */\n  for (int i = 0; i < (short_bb_branch_p ? 13 : 6); i++) put_uint32 (gen_ctx, TARGET_NOP);\n  while (VARR_LENGTH (uint8_t, result_code) % 16 != 0) /* Align the pool */\n    VARR_PUSH (uint8_t, result_code, 0);\n  *len = VARR_LENGTH (uint8_t, result_code);\n  return VARR_ADDR (uint8_t, result_code);\n}\n\nstatic void setup_rel (gen_ctx_t gen_ctx, label_ref_t *lr, uint8_t *base, void *addr) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  int64_t offset = (int64_t) addr - (int64_t) (base + lr->label_val_disp);\n\n  gen_assert ((offset & 0x3) == 0 && !lr->abs_addr_p);\n  /* ??? thread safe: */\n  uint32_t *insn_ptr = (uint32_t *) (base + lr->label_val_disp), insn = *insn_ptr;\n  if (lr->branch_type == BRCOND) {\n    if (((offset < 0 ? -offset : offset) & ~(int64_t) 0x7fff) == 0) { /* a valid branch offset */\n      insn = (insn & ~0xffff) | (((offset / 4) & 0x3fff) << 2);\n      _MIR_change_code (ctx, (uint8_t *) insn_ptr, (uint8_t *) &insn, 4);\n      return;\n    }\n    insn = (insn & ~0xffff) | (4 * 8); /* skip next jump and 6 nops for it */\n    _MIR_change_code (ctx, (uint8_t *) insn_ptr, (uint8_t *) &insn, 4);\n    insn = PPC_JUMP_OPCODE << (32 - 6);\n    insn_ptr += 8;\n    lr->branch_type = JUMP;\n    lr->label_val_disp += 4 * 8;\n    offset -= 4 * 8;\n  }\n  if (lr->branch_type == LADDR) {\n    offset += 4;\n    int hi = offset >> 16, low = offset & 0xffff;\n    if ((low & 0x8000) != 0) hi++;\n    insn |= hi & 0xffff;\n    _MIR_change_code (ctx, (uint8_t *) insn_ptr, (uint8_t *) &insn, 4);\n    insn_ptr++;\n    insn = *insn_ptr | low;\n    _MIR_change_code (ctx, (uint8_t *) insn_ptr, (uint8_t *) &insn, 4);\n    return;\n  } else if (lr->branch_type == JUMP) {\n    if (((offset < 0 ? -offset : offset) & ~(int64_t) 0x1ffffff) == 0) { /* a valid jump offset */\n      insn = (insn & ~0x3ffffff) | (((offset / 4) & 0xffffff) << 2);\n      _MIR_change_code (ctx, (uint8_t *) insn_ptr, (uint8_t *) &insn, 4);\n      return;\n    }\n    lr->branch_type = BCTR;\n  }\n  gen_assert (lr->branch_type == BCTR);\n  VARR_TRUNC (uint8_t, result_code, 0);\n  ppc64_gen_address (result_code, 12, addr); /* r12 = addr */\n  insn = 0x7d8903a6;                         /* mtctr r12 */\n  put_uint32 (gen_ctx, insn);\n  insn = 0x4e800420; /* bctr */\n  put_uint32 (gen_ctx, insn);\n  _MIR_change_code (ctx, (uint8_t *) insn_ptr, VARR_ADDR (uint8_t, result_code),\n                    VARR_LENGTH (uint8_t, result_code));\n}\n\nstatic void target_bb_rebase (gen_ctx_t gen_ctx, uint8_t *base) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  MIR_code_reloc_t reloc;\n\n  /* Setting up relative labels */\n  for (size_t i = 0; i < VARR_LENGTH (label_ref_t, label_refs); i++) {\n    label_ref_t lr = VARR_GET (label_ref_t, label_refs, i);\n    if (lr.abs_addr_p) {\n      _MIR_change_code (ctx, (uint8_t *) base + lr.label_val_disp, (uint8_t *) &lr.u.jump_addr, 8);\n    } else {\n      setup_rel (gen_ctx, &lr, base, lr.u.jump_addr);\n    }\n  }\n  VARR_TRUNC (MIR_code_reloc_t, relocs, 0);\n  for (size_t i = 0; i < VARR_LENGTH (uint64_t, abs_address_locs); i++) {\n    reloc.offset = VARR_GET (uint64_t, abs_address_locs, i);\n    reloc.value = base + get_int64 (base + reloc.offset);\n    VARR_PUSH (MIR_code_reloc_t, relocs, reloc);\n  }\n  _MIR_update_code_arr (gen_ctx->ctx, base, VARR_LENGTH (MIR_code_reloc_t, relocs),\n                        VARR_ADDR (MIR_code_reloc_t, relocs));\n}\n\nstatic void target_setup_succ_bb_version_data (gen_ctx_t gen_ctx, uint8_t *base) {\n  if (VARR_LENGTH (label_ref_t, label_refs)\n      != VARR_LENGTH (target_bb_version_t, target_succ_bb_versions))\n    /* We can have more one possible branch from original insn\n       (e.g. SWITCH, FBNE).  If it is so, we will make jumps only\n       through BB thunk. */\n    return;\n  for (size_t i = 0; i < VARR_LENGTH (target_bb_version_t, target_succ_bb_versions); i++) {\n    target_bb_version_t data = VARR_GET (target_bb_version_t, target_succ_bb_versions, i);\n    if (data == NULL) continue;\n    data->branch_ref = VARR_GET (label_ref_t, label_refs, i);\n    data->base = base;\n  }\n}\n\nstatic void target_redirect_bb_origin_branch (gen_ctx_t gen_ctx, target_bb_version_t data,\n                                              void *addr) {\n  MIR_context_t ctx = gen_ctx->ctx;\n\n  if (data->base == NULL) return;\n  if (data->branch_ref.abs_addr_p) {\n    _MIR_change_code (ctx, (uint8_t *) data->base + data->branch_ref.label_val_disp,\n                      (uint8_t *) &addr, 8);\n  } else {\n    setup_rel (gen_ctx, &data->branch_ref, data->base, addr);\n  }\n  data->base = NULL;\n}\n\nstatic void target_init (gen_ctx_t gen_ctx) {\n  MIR_alloc_t alloc = gen_alloc (gen_ctx);\n  MIR_context_t ctx = gen_ctx->ctx;\n\n  gen_ctx->target_ctx = gen_malloc (gen_ctx, sizeof (struct target_ctx));\n  VARR_CREATE (uint8_t, result_code, alloc, 0);\n  VARR_CREATE (label_ref_t, label_refs, alloc, 0);\n  VARR_CREATE (uint64_t, abs_address_locs, alloc, 0);\n  VARR_CREATE (MIR_code_reloc_t, relocs, alloc, 0);\n  patterns_init (gen_ctx);\n  temp_jump = MIR_new_insn (ctx, MIR_JMP, MIR_new_label_op (ctx, NULL));\n  temp_jump_replacement = find_insn_pattern_replacement (gen_ctx, temp_jump, FALSE);\n}\n\nstatic void target_finish (gen_ctx_t gen_ctx) {\n  MIR_alloc_t alloc = gen_alloc (gen_ctx);\n  patterns_finish (gen_ctx);\n  _MIR_free_insn (gen_ctx->ctx, temp_jump);\n  VARR_DESTROY (uint8_t, result_code);\n  VARR_DESTROY (label_ref_t, label_refs);\n  VARR_DESTROY (uint64_t, abs_address_locs);\n  VARR_DESTROY (MIR_code_reloc_t, relocs);\n  MIR_free (alloc, gen_ctx->target_ctx);\n  gen_ctx->target_ctx = NULL;\n}\n"
        },
        {
          "name": "mir-gen-riscv64.c",
          "type": "blob",
          "size": 122.775390625,
          "content": "/* This file is a part of MIR project.\n   Copyright (C) 2020-2024 Vladimir Makarov <vmakarov.gcc@gmail.com>.\n*/\n\n/* In MIR generated code unsigned 32-bit values are zero extended but\n   passing/returning such value is signed extended (which is their\n   riscv ABI representation).\n\n   In theory we should pass vararg unsigned as sign extended according\n   to ABI but gcc/clang va_arg code do the correct sign extension\n   anyway.  So we can ignore sign extension for integer vararg\n   arguments.  */\n\n/* ??? TODO: o compressed c.j, c.beqz, c.bnez (is it worth because such jumps are\n               usually forward and it requires complicated algorithm for relaxing)\n             o rd = rs - const -> rd = rs + (-const) for using addi\n             o implement generation for FLEN=128\n             o save/set/restore sp/fp only we use it\n             o optimization: remove ext32 for branches generated in machinize if operands\n                             are generated by short insns\n*/\n\nstatic void fancy_abort (int code) {\n  if (!code) abort ();\n}\n#undef gen_assert\n#define gen_assert(c) fancy_abort (c)\n\n#define TARGET_EXPAND_ADDO\n#define TARGET_EXPAND_ADDOS\n#define TARGET_EXPAND_UADDO\n#define TARGET_EXPAND_UADDOS\n#define TARGET_EXPAND_MULO\n#define TARGET_EXPAND_MULOS\n#define TARGET_EXPAND_UMULO\n#define TARGET_EXPAND_UMULOS\n\n#include <limits.h>\n\n#include \"mir-riscv64.h\"\n\n#define REP_SEP ,\nstatic const MIR_reg_t hard_reg_alloc_order[] = {\n  REP8 (HREG_EL, R8, R9, R10, R11, R12, R13, R14, R15),\n  REP8 (HREG_EL, F8, F9, F10, F11, F12, F13, F14, F15),\n\n  REP8 (HREG_EL, R0, R1, R2, R3, R4, R5, R6, R7),\n  REP8 (HREG_EL, R16, R17, R18, R19, R20, R21, R22, R23),\n  REP8 (HREG_EL, R24, R25, R26, R27, R28, R29, R30, R31),\n\n  REP8 (HREG_EL, F0, F1, F2, F3, F4, F5, F6, F7),\n  REP8 (HREG_EL, F16, F17, F18, F19, F20, F21, F22, F23),\n  REP8 (HREG_EL, F24, F25, F26, F27, F28, F29, F30, F31),\n};\n#undef REP_SEP\n\nstatic const MIR_reg_t LINK_HARD_REG = RA_HARD_REG;\n\n#define TARGET_HARD_REG_ALLOC_ORDER(n) hard_reg_alloc_order[n]\n\nstatic void check_hard_reg_alloc_order (void) {\n  int i;\n  char check_p[F31_HARD_REG + 1];\n\n  gen_assert (MAX_HARD_REG == F31_HARD_REG\n              && sizeof (hard_reg_alloc_order) / sizeof (MIR_reg_t) == MAX_HARD_REG + 1);\n  for (i = 0; i <= MAX_HARD_REG; i++) check_p[i] = FALSE;\n  for (i = 0; i <= MAX_HARD_REG; i++) {\n    gen_assert (!check_p[hard_reg_alloc_order[i]]);\n    check_p[hard_reg_alloc_order[i]] = TRUE;\n  }\n  for (i = 0; i <= MAX_HARD_REG; i++) gen_assert (check_p[i]);\n}\n\nstatic inline MIR_reg_t target_nth_loc (MIR_reg_t loc, MIR_type_t type MIR_UNUSED, int n) {\n  return loc + n;\n}\n\nstatic inline int target_call_used_hard_reg_p (MIR_reg_t hard_reg, MIR_type_t type) {\n  assert (hard_reg <= MAX_HARD_REG);\n  if (hard_reg <= R31_HARD_REG)\n    return !(hard_reg == R8_HARD_REG || hard_reg == R9_HARD_REG\n             || (hard_reg >= R18_HARD_REG && hard_reg <= R27_HARD_REG));\n  return type == MIR_T_LD\n         || !(hard_reg == F8_HARD_REG || hard_reg == F9_HARD_REG\n              || (hard_reg >= F18_HARD_REG && hard_reg <= F27_HARD_REG));\n}\n\n/* Stack layout (sp refers to the last reserved stack slot address)\n   from higher address to lower address memory:\n\n   | ...           |  prev func stack (start aligned to 16 bytes)\n   |---------------|\n   | gr save area  |  optional area for vararg func reg save area\n   |               |  (int arg regs corresponding to varargs)\n   |---------------|\n   | saved regs    |  callee saved regs used in the func (known only after RA), rounded 16 bytes\n   |---------------|\n   | slots assigned|  can be absent for small functions (known only after RA), rounded 16 bytes\n   |   to pseudos  |\n   |---------------|\n   |   previous    |  (sp right after call) 16-bytes setup in prolog, used only for varag func or\n   | stack start   |   args passed on stack to move args and to setup va_start on machinize pass\n   |---------------|\n   | RA            |  sp before prologue and after saving RA = start sp\n   |---------------|\n   | old FP        |  frame pointer for previous func stack frame; new FP refers for here\n   |---------------|\n   |  small aggreg |\n   |  save area    |  optional\n   |---------------|\n   | alloca areas  |  optional\n   |---------------|\n   | slots for     |  dynamically allocated/deallocated by caller\n   |  passing args |\n\n   size of slots and saved regs is multiple of 16 bytes\n\n */\n\nstatic const MIR_insn_code_t target_io_dup_op_insn_codes[] = {MIR_INSN_BOUND};\n\n/* Return extension insn for passing args and returns.  */\nstatic MIR_insn_code_t get_ext_code (MIR_type_t type, int arg_pass_p) {\n  switch (type) {\n  case MIR_T_I8: return MIR_EXT8;\n  case MIR_T_U8: return MIR_UEXT8;\n  case MIR_T_I16: return MIR_EXT16;\n  case MIR_T_U16: return MIR_UEXT16;\n  case MIR_T_I32: return MIR_EXT32;\n  case MIR_T_U32:\n    /* even unsigned 32-bit is extended by sign according to ABI -- pass it the right way: */\n    return (arg_pass_p ? MIR_EXT32 : MIR_UEXT32);\n  default: return MIR_INVALID_INSN;\n  }\n}\n\nstatic MIR_reg_t get_arg_reg (MIR_type_t arg_type, int vararg_p, size_t *int_arg_num,\n                              size_t *fp_arg_num, MIR_insn_code_t *mov_code) {\n  MIR_reg_t arg_reg;\n\n  if (!vararg_p && (arg_type == MIR_T_F || arg_type == MIR_T_D)) {\n    switch (*fp_arg_num) {\n    case 0:\n    case 1:\n    case 2:\n    case 3:\n    case 4:\n    case 5:\n    case 6:\n    case 7: arg_reg = FA0_HARD_REG + *fp_arg_num; break;\n    default: arg_reg = MIR_NON_VAR; break;\n    }\n    (*fp_arg_num)++;\n    *mov_code = arg_type == MIR_T_F ? MIR_FMOV : MIR_DMOV;\n  } else { /* including LD, BLK, RBLK: */\n    if (arg_type == MIR_T_LD && *int_arg_num % 2 != 0) (*int_arg_num)++;\n    switch (*int_arg_num) {\n    case 0:\n    case 1:\n    case 2:\n    case 3:\n    case 4:\n    case 5:\n    case 6:\n    case 7: arg_reg = A0_HARD_REG + *int_arg_num; break;\n    default: arg_reg = MIR_NON_VAR; break;\n    }\n    (*int_arg_num)++;\n    if (arg_type != MIR_T_LD) {\n      *mov_code = MIR_MOV;\n    } else {\n      (*int_arg_num)++;\n      *mov_code = MIR_LDMOV;\n    }\n  }\n  return arg_reg;\n}\n\nstatic void mir_blk_mov (uint64_t *to, uint64_t *from, uint64_t nwords) {\n  for (; nwords > 0; nwords--) *to++ = *from++;\n}\n\nstatic MIR_insn_t gen_mov (gen_ctx_t gen_ctx, MIR_insn_t anchor, MIR_insn_code_t code,\n                           MIR_op_t dst_op, MIR_op_t src_op) {\n  MIR_insn_t insn = MIR_new_insn (gen_ctx->ctx, code, dst_op, src_op);\n  gen_add_insn_before (gen_ctx, anchor, insn);\n  return insn;\n}\n\nstatic const char *BLK_MOV = \"mir.blk_mov\";\nstatic const char *BLK_MOV_P = \"mir.blk_mov.p\";\n\nstatic void gen_blk_mov (gen_ctx_t gen_ctx, MIR_insn_t anchor, size_t to_disp,\n                         MIR_reg_t to_base_hard_reg, MIR_reg_t from_base_reg, size_t qwords,\n                         int save_regs) {\n  size_t from_disp = 0;\n  MIR_context_t ctx = gen_ctx->ctx;\n  MIR_func_t func = curr_func_item->u.func;\n  MIR_item_t proto_item, func_import_item;\n  MIR_insn_t new_insn;\n  MIR_op_t ops[5], freg_op, treg_op, treg_op2, treg_op3, treg_op4;\n\n  treg_op = _MIR_new_var_op (ctx, gen_new_temp_reg (gen_ctx, MIR_T_I64, func));\n  treg_op2 = _MIR_new_var_op (ctx, gen_new_temp_reg (gen_ctx, MIR_T_I64, func));\n  if (qwords <= 16) {\n    gen_mov (gen_ctx, anchor, MIR_MOV, treg_op2, MIR_new_int_op (ctx, to_disp));\n    gen_add_insn_before (gen_ctx, anchor,\n                         MIR_new_insn (gen_ctx->ctx, MIR_ADD, treg_op2, treg_op2,\n                                       _MIR_new_var_op (ctx, to_base_hard_reg)));\n    for (; qwords > 0; qwords--, to_disp += 8, from_disp += 8) {\n      gen_mov (gen_ctx, anchor, MIR_MOV, treg_op,\n               _MIR_new_var_mem_op (ctx, MIR_T_I64, from_disp, from_base_reg, MIR_NON_VAR, 1));\n      gen_mov (gen_ctx, anchor, MIR_MOV,\n               _MIR_new_var_mem_op (ctx, MIR_T_I64, to_disp, to_base_hard_reg, MIR_NON_VAR, 1),\n               treg_op);\n    }\n    return;\n  }\n  treg_op3 = _MIR_new_var_op (ctx, gen_new_temp_reg (gen_ctx, MIR_T_I64, func));\n  /* Save arg regs: */\n  if (save_regs > 0)\n    gen_mov (gen_ctx, anchor, MIR_MOV, treg_op, _MIR_new_var_op (ctx, A0_HARD_REG));\n  if (save_regs > 1)\n    gen_mov (gen_ctx, anchor, MIR_MOV, treg_op2, _MIR_new_var_op (ctx, A1_HARD_REG));\n  if (save_regs > 2)\n    gen_mov (gen_ctx, anchor, MIR_MOV, treg_op3, _MIR_new_var_op (ctx, A2_HARD_REG));\n  /* call blk move: */\n  proto_item = _MIR_builtin_proto (ctx, curr_func_item->module, BLK_MOV_P, 0, NULL, 3, MIR_T_I64,\n                                   \"to\", MIR_T_I64, \"from\", MIR_T_I64, \"nwords\");\n  func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, BLK_MOV, mir_blk_mov);\n  freg_op = _MIR_new_var_op (ctx, gen_new_temp_reg (gen_ctx, MIR_T_I64, func));\n  new_insn = MIR_new_insn (ctx, MIR_MOV, freg_op, MIR_new_ref_op (ctx, func_import_item));\n  gen_add_insn_before (gen_ctx, anchor, new_insn);\n  treg_op4 = _MIR_new_var_op (ctx, gen_new_temp_reg (gen_ctx, MIR_T_I64, func));\n  gen_mov (gen_ctx, anchor, MIR_MOV, treg_op4, MIR_new_int_op (ctx, to_disp));\n  gen_add_insn_before (gen_ctx, anchor,\n                       MIR_new_insn (gen_ctx->ctx, MIR_ADD, _MIR_new_var_op (ctx, A0_HARD_REG),\n                                     _MIR_new_var_op (ctx, to_base_hard_reg), treg_op4));\n  gen_add_insn_before (gen_ctx, anchor,\n                       MIR_new_insn (gen_ctx->ctx, MIR_ADD, _MIR_new_var_op (ctx, A1_HARD_REG),\n                                     _MIR_new_var_op (ctx, from_base_reg),\n                                     MIR_new_int_op (ctx, from_disp)));\n  gen_mov (gen_ctx, anchor, MIR_MOV, _MIR_new_var_op (ctx, A2_HARD_REG),\n           MIR_new_int_op (ctx, qwords));\n  ops[0] = MIR_new_ref_op (ctx, proto_item);\n  ops[1] = freg_op;\n  ops[2] = _MIR_new_var_op (ctx, A0_HARD_REG);\n  ops[3] = _MIR_new_var_op (ctx, A1_HARD_REG);\n  ops[4] = _MIR_new_var_op (ctx, A2_HARD_REG);\n  new_insn = MIR_new_insn_arr (ctx, MIR_CALL, 5, ops);\n  gen_add_insn_before (gen_ctx, anchor, new_insn);\n  /* Restore arg regs: */\n  if (save_regs > 0)\n    gen_mov (gen_ctx, anchor, MIR_MOV, _MIR_new_var_op (ctx, A0_HARD_REG), treg_op);\n  if (save_regs > 1)\n    gen_mov (gen_ctx, anchor, MIR_MOV, _MIR_new_var_op (ctx, A1_HARD_REG), treg_op2);\n  if (save_regs > 2)\n    gen_mov (gen_ctx, anchor, MIR_MOV, _MIR_new_var_op (ctx, R2_HARD_REG), treg_op3);\n}\n\n#define FMVXW_CODE 0\n#define FMVXD_CODE 1\n\nstatic void machinize_call (gen_ctx_t gen_ctx, MIR_insn_t call_insn) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  MIR_func_t func = curr_func_item->u.func;\n  MIR_proto_t proto = call_insn->ops[0].u.ref->u.proto;\n  int float_p;\n  size_t nargs, nops = MIR_insn_nops (ctx, call_insn), start = proto->nres + 2;\n  size_t int_arg_num = 0, fp_arg_num = 0, mem_size = 0, blk_offset = 0, qwords;\n  MIR_type_t type, mem_type;\n  MIR_op_mode_t mode;\n  MIR_var_t *arg_vars = NULL;\n  MIR_reg_t arg_reg;\n  MIR_op_t arg_op, temp_op, arg_reg_op, ret_reg_op, mem_op, treg_op;\n  MIR_insn_code_t new_insn_code, ext_code;\n  MIR_insn_t new_insn, prev_insn, next_insn, ext_insn;\n  MIR_insn_t prev_call_insn = DLIST_PREV (MIR_insn_t, call_insn);\n  MIR_insn_t curr_prev_call_insn = prev_call_insn;\n\n  assert (__SIZEOF_LONG_DOUBLE__ == 16);\n  if (call_insn->code == MIR_INLINE) call_insn->code = MIR_CALL;\n  if (proto->args == NULL) {\n    nargs = 0;\n  } else {\n    gen_assert (nops >= VARR_LENGTH (MIR_var_t, proto->args)\n                && (proto->vararg_p || nops - start == VARR_LENGTH (MIR_var_t, proto->args)));\n    nargs = VARR_LENGTH (MIR_var_t, proto->args);\n    arg_vars = VARR_ADDR (MIR_var_t, proto->args);\n  }\n  if (call_insn->ops[1].mode != MIR_OP_VAR) {\n    // ??? to optimize (can be immediate operand for func call)\n    temp_op = _MIR_new_var_op (ctx, gen_new_temp_reg (gen_ctx, MIR_T_I64, func));\n    new_insn = MIR_new_insn (ctx, MIR_MOV, temp_op, call_insn->ops[1]);\n    call_insn->ops[1] = temp_op;\n    gen_add_insn_before (gen_ctx, call_insn, new_insn);\n  }\n  for (size_t i = start; i < nops; i++) { /* calculate offset for blk params */\n    if (i - start < nargs) {\n      type = arg_vars[i - start].type;\n    } else if (call_insn->ops[i].mode == MIR_OP_VAR_MEM) {\n      type = call_insn->ops[i].u.mem.type;\n      gen_assert (MIR_all_blk_type_p (type));\n    } else {\n      mode = call_insn->ops[i].value_mode;  // ??? smaller ints\n      gen_assert (mode == MIR_OP_INT || mode == MIR_OP_UINT || mode == MIR_OP_FLOAT\n                  || mode == MIR_OP_DOUBLE || mode == MIR_OP_LDOUBLE);\n      if (mode == MIR_OP_FLOAT)\n        (*MIR_get_error_func (ctx)) (MIR_call_op_error,\n                                     \"passing float variadic arg (should be passed as double)\");\n      type = mode == MIR_OP_DOUBLE ? MIR_T_D : mode == MIR_OP_LDOUBLE ? MIR_T_LD : MIR_T_I64;\n    }\n    gen_assert (!MIR_all_blk_type_p (type) || call_insn->ops[i].mode == MIR_OP_VAR_MEM);\n    if ((MIR_T_I8 <= type && type <= MIR_T_U64) || type == MIR_T_P || type == MIR_T_LD\n        || MIR_all_blk_type_p (type)) {\n      if (MIR_blk_type_p (type) && (qwords = (call_insn->ops[i].u.mem.disp + 7) / 8) <= 2) {\n        if (type == MIR_T_BLK + 1) int_arg_num = (int_arg_num + 1) / 2 * 2; /* Make even */\n        if (int_arg_num + qwords > 8)\n          blk_offset += (qwords - (int_arg_num + qwords == 9 ? 1 : 0)) * 8;\n        int_arg_num += qwords;\n      } else { /* blocks here are passed by address */\n        if (type == MIR_T_LD) int_arg_num = (int_arg_num + 1) / 2 * 2; /* Make even */\n        if (int_arg_num >= 8) blk_offset += 8 + (type == MIR_T_LD ? 8 : 0);\n        int_arg_num++;\n        if (type == MIR_T_LD) int_arg_num++;\n      }\n    } else if (type == MIR_T_F || type == MIR_T_D) {\n      if (i - start >= nargs) { /* varargs are passed by int regs */\n        if (int_arg_num >= 8) blk_offset += 8;\n        int_arg_num++;\n      } else {\n        if (fp_arg_num >= 8) blk_offset += 8;\n        fp_arg_num++;\n      }\n    } else {\n      MIR_get_error_func (ctx) (MIR_call_op_error, \"wrong type of arg value\");\n    }\n  }\n  blk_offset = (blk_offset + 15) / 16 * 16; /* align stack */\n  int_arg_num = fp_arg_num = 0;\n  for (size_t i = start; i < nops; i++) {\n    arg_op = call_insn->ops[i];\n    gen_assert (arg_op.mode == MIR_OP_VAR\n                || (arg_op.mode == MIR_OP_VAR_MEM && MIR_all_blk_type_p (arg_op.u.mem.type)));\n    if (i - start < nargs) {\n      type = arg_vars[i - start].type;\n    } else if (call_insn->ops[i].mode == MIR_OP_VAR_MEM) {\n      type = call_insn->ops[i].u.mem.type;\n      gen_assert (MIR_all_blk_type_p (type));\n    } else {\n      mode = call_insn->ops[i].value_mode;  // ??? smaller ints\n      type = mode == MIR_OP_DOUBLE ? MIR_T_D : mode == MIR_OP_LDOUBLE ? MIR_T_LD : MIR_T_I64;\n    }\n    ext_insn = NULL;\n    if ((ext_code = get_ext_code (type, TRUE)) != MIR_INVALID_INSN) { /* extend arg if necessary */\n      temp_op = _MIR_new_var_op (ctx, gen_new_temp_reg (gen_ctx, MIR_T_I64, func));\n      ext_insn = MIR_new_insn (ctx, ext_code, temp_op, arg_op);\n      call_insn->ops[i] = arg_op = temp_op;\n    }\n    gen_assert (!MIR_all_blk_type_p (type)\n                || (arg_op.mode == MIR_OP_VAR_MEM && arg_op.u.mem.disp >= 0\n                    && arg_op.u.mem.index == MIR_NON_VAR));\n    if (MIR_blk_type_p (type)) {\n      qwords = (arg_op.u.mem.disp + 7) / 8;\n      if (qwords <= 2) {\n        arg_reg = A0_HARD_REG + int_arg_num;\n        if (type == MIR_T_BLK + 1) int_arg_num = (int_arg_num + 1) / 2 * 2; /* Make even */\n        for (size_t n = 0; n < qwords; n++) {\n          if (int_arg_num < 8) {\n            new_insn = MIR_new_insn (ctx, MIR_MOV, _MIR_new_var_op (ctx, A0_HARD_REG + int_arg_num),\n                                     _MIR_new_var_mem_op (ctx, MIR_T_I64, n * 8, arg_op.u.mem.base,\n                                                          MIR_NON_VAR, 1));\n            gen_add_insn_before (gen_ctx, call_insn, new_insn);\n            setup_call_hard_reg_args (gen_ctx, call_insn, A0_HARD_REG + int_arg_num);\n            int_arg_num++;\n          } else { /* put word on stack */\n            treg_op = _MIR_new_var_op (ctx, gen_new_temp_reg (gen_ctx, MIR_T_I64, func));\n            new_insn = MIR_new_insn (ctx, MIR_MOV, treg_op,\n                                     _MIR_new_var_mem_op (ctx, MIR_T_I64, n * 8, arg_op.u.mem.base,\n                                                          MIR_NON_VAR, 1));\n            gen_add_insn_before (gen_ctx, call_insn, new_insn);\n            mem_op = _MIR_new_var_mem_op (ctx, MIR_T_I64, mem_size, SP_HARD_REG, MIR_NON_VAR, 1);\n            new_insn = MIR_new_insn (ctx, MIR_MOV, mem_op, treg_op);\n            gen_add_insn_before (gen_ctx, call_insn, new_insn);\n            mem_size += 8;\n          }\n        }\n        continue;\n      }\n      /* Put on stack and pass the address: */\n      gen_blk_mov (gen_ctx, call_insn, blk_offset, SP_HARD_REG, arg_op.u.mem.base, qwords,\n                   int_arg_num);\n      arg_op = _MIR_new_var_op (ctx, gen_new_temp_reg (gen_ctx, MIR_T_I64, func));\n      gen_assert (curr_prev_call_insn\n                  != NULL); /* call_insn should not be 1st after simplification */\n      new_insn = MIR_new_insn (gen_ctx->ctx, MIR_ADD, arg_op, _MIR_new_var_op (ctx, SP_HARD_REG),\n                               MIR_new_int_op (ctx, blk_offset));\n      gen_add_insn_after (gen_ctx, curr_prev_call_insn, new_insn);\n      curr_prev_call_insn = DLIST_NEXT (MIR_insn_t, new_insn);\n      blk_offset += qwords * 8;\n    }\n    if ((arg_reg\n         = get_arg_reg (type, i - start >= nargs, &int_arg_num, &fp_arg_num, &new_insn_code))\n        != MIR_NON_VAR) {\n      /* put arguments to argument hard regs */\n      if (ext_insn != NULL) gen_add_insn_before (gen_ctx, call_insn, ext_insn);\n      arg_reg_op = _MIR_new_var_op (ctx, arg_reg);\n      if (type != MIR_T_RBLK) {\n        if (new_insn_code == MIR_MOV && (type == MIR_T_F || type == MIR_T_D)) {\n          new_insn\n            = _MIR_new_unspec_insn (ctx, 3,\n                                    MIR_new_int_op (ctx, type == MIR_T_F ? FMVXW_CODE : FMVXD_CODE),\n                                    arg_reg_op, arg_op);\n        } else {\n          new_insn = MIR_new_insn (ctx, new_insn_code, arg_reg_op, arg_op);\n        }\n      } else {\n        assert (arg_op.mode == MIR_OP_VAR_MEM);\n        new_insn\n          = MIR_new_insn (ctx, MIR_MOV, arg_reg_op, _MIR_new_var_op (ctx, arg_op.u.mem.base));\n        arg_reg_op\n          = _MIR_new_var_mem_op (ctx, MIR_T_RBLK, arg_op.u.mem.disp, arg_reg, MIR_NON_VAR, 1);\n      }\n      gen_add_insn_before (gen_ctx, call_insn, new_insn);\n      call_insn->ops[i] = arg_reg_op;\n      if (type == MIR_T_LD) /* long double is passed in 2 int hard regs: */\n        setup_call_hard_reg_args (gen_ctx, call_insn, arg_reg + 1);\n    } else { /* put arguments on the stack */\n      mem_type = type == MIR_T_F || type == MIR_T_D || type == MIR_T_LD ? type : MIR_T_I64;\n      new_insn_code = (type == MIR_T_F    ? MIR_FMOV\n                       : type == MIR_T_D  ? MIR_DMOV\n                       : type == MIR_T_LD ? MIR_LDMOV\n                                          : MIR_MOV);\n      mem_op = _MIR_new_var_mem_op (ctx, mem_type, mem_size, SP_HARD_REG, MIR_NON_VAR, 1);\n      if (type != MIR_T_RBLK) {\n        new_insn = MIR_new_insn (ctx, new_insn_code, mem_op, arg_op);\n      } else {\n        assert (arg_op.mode == MIR_OP_VAR_MEM);\n        new_insn\n          = MIR_new_insn (ctx, new_insn_code, mem_op, _MIR_new_var_op (ctx, arg_op.u.mem.base));\n      }\n      gen_assert (curr_prev_call_insn != NULL); /* call should not be 1st after simplification */\n      MIR_insert_insn_after (ctx, curr_func_item, curr_prev_call_insn, new_insn);\n      prev_insn = DLIST_PREV (MIR_insn_t, new_insn);\n      next_insn = DLIST_NEXT (MIR_insn_t, new_insn);\n      create_new_bb_insns (gen_ctx, prev_insn, next_insn, call_insn);\n      call_insn->ops[i] = mem_op;\n      mem_size += type == MIR_T_LD ? 16 : 8;\n      if (ext_insn != NULL) gen_add_insn_after (gen_ctx, curr_prev_call_insn, ext_insn);\n    }\n  }\n  blk_offset = (blk_offset + 15) / 16 * 16;\n  if (blk_offset != 0) mem_size = blk_offset;\n  int_arg_num = fp_arg_num = 0;\n  for (size_t i = 0; i < proto->nres; i++) {\n    ret_reg_op = call_insn->ops[i + 2];\n    gen_assert (ret_reg_op.mode == MIR_OP_VAR);\n    type = proto->res_types[i];\n    float_p = type == MIR_T_F || type == MIR_T_D;\n    if (float_p && fp_arg_num < 2) {\n      new_insn = MIR_new_insn (ctx, type == MIR_T_F ? MIR_FMOV : MIR_DMOV, ret_reg_op,\n                               _MIR_new_var_op (ctx, FA0_HARD_REG + fp_arg_num));\n      fp_arg_num++;\n    } else if (type == MIR_T_LD && int_arg_num < 2) {\n      new_insn = MIR_new_insn (ctx, MIR_LDMOV, ret_reg_op,\n                               _MIR_new_var_op (ctx, A0_HARD_REG + int_arg_num));\n      int_arg_num += 2;\n    } else if (!float_p && int_arg_num < 2) {\n      new_insn\n        = MIR_new_insn (ctx, MIR_MOV, ret_reg_op, _MIR_new_var_op (ctx, A0_HARD_REG + int_arg_num));\n      int_arg_num++;\n    } else {\n      (*MIR_get_error_func (ctx)) (MIR_ret_error,\n                                   \"riscv can not handle this combination of return values\");\n    }\n    MIR_insert_insn_after (ctx, curr_func_item, call_insn, new_insn);\n    call_insn->ops[i + 2] = new_insn->ops[1];\n    if ((ext_code = get_ext_code (type, FALSE)) != MIR_INVALID_INSN) {\n      MIR_insert_insn_after (ctx, curr_func_item, new_insn,\n                             MIR_new_insn (ctx, ext_code, ret_reg_op, ret_reg_op));\n      new_insn = DLIST_NEXT (MIR_insn_t, new_insn);\n    }\n    create_new_bb_insns (gen_ctx, call_insn, DLIST_NEXT (MIR_insn_t, new_insn), call_insn);\n  }\n  if (mem_size != 0) { /* allocate/deallocate stack for args passed on stack */\n    temp_op = _MIR_new_var_op (ctx, gen_new_temp_reg (gen_ctx, MIR_T_I64, func));\n    mem_size = (mem_size + 15) / 16 * 16; /* make it of several 16 bytes */\n    new_insn = MIR_new_insn (ctx, MIR_ADD, _MIR_new_var_op (ctx, SP_HARD_REG),\n                             _MIR_new_var_op (ctx, SP_HARD_REG), temp_op);\n    MIR_insert_insn_after (ctx, curr_func_item, prev_call_insn, new_insn);\n    next_insn = DLIST_NEXT (MIR_insn_t, new_insn);\n    new_insn = MIR_new_insn (ctx, MIR_MOV, temp_op, MIR_new_int_op (ctx, -(int64_t) mem_size));\n    MIR_insert_insn_after (ctx, curr_func_item, prev_call_insn, new_insn);\n    create_new_bb_insns (gen_ctx, prev_call_insn, next_insn, call_insn);\n    temp_op = _MIR_new_var_op (ctx, gen_new_temp_reg (gen_ctx, MIR_T_I64, func));\n    new_insn = MIR_new_insn (ctx, MIR_MOV, temp_op, MIR_new_int_op (ctx, mem_size));\n    MIR_insert_insn_after (ctx, curr_func_item, call_insn, new_insn);\n    next_insn = DLIST_NEXT (MIR_insn_t, new_insn);\n    new_insn = MIR_new_insn (ctx, MIR_ADD, _MIR_new_var_op (ctx, SP_HARD_REG),\n                             _MIR_new_var_op (ctx, SP_HARD_REG), temp_op);\n    MIR_insert_insn_before (ctx, curr_func_item, next_insn, new_insn);\n    create_new_bb_insns (gen_ctx, call_insn, next_insn, call_insn);\n  }\n}\n\nstatic long double mir_i2ld (int64_t i) { return i; }\nstatic const char *I2LD = \"mir.i2ld\";\nstatic const char *I2LD_P = \"mir.i2ld.p\";\n\nstatic long double mir_ui2ld (uint64_t i) { return i; }\nstatic const char *UI2LD = \"mir.ui2ld\";\nstatic const char *UI2LD_P = \"mir.ui2ld.p\";\n\nstatic long double mir_f2ld (float f) { return f; }\nstatic const char *F2LD = \"mir.f2ld\";\nstatic const char *F2LD_P = \"mir.f2ld.p\";\n\nstatic long double mir_d2ld (double d) { return d; }\nstatic const char *D2LD = \"mir.d2ld\";\nstatic const char *D2LD_P = \"mir.d2ld.p\";\n\nstatic int64_t mir_ld2i (long double ld) { return ld; }\nstatic const char *LD2I = \"mir.ld2i\";\nstatic const char *LD2I_P = \"mir.ld2i.p\";\n\nstatic float mir_ld2f (long double ld) { return ld; }\nstatic const char *LD2F = \"mir.ld2f\";\nstatic const char *LD2F_P = \"mir.ld2f.p\";\n\nstatic double mir_ld2d (long double ld) { return ld; }\nstatic const char *LD2D = \"mir.ld2d\";\nstatic const char *LD2D_P = \"mir.ld2d.p\";\n\nstatic long double mir_ldadd (long double d1, long double d2) { return d1 + d2; }\nstatic const char *LDADD = \"mir.ldadd\";\nstatic const char *LDADD_P = \"mir.ldadd.p\";\n\nstatic long double mir_ldsub (long double d1, long double d2) { return d1 - d2; }\nstatic const char *LDSUB = \"mir.ldsub\";\nstatic const char *LDSUB_P = \"mir.ldsub.p\";\n\nstatic long double mir_ldmul (long double d1, long double d2) { return d1 * d2; }\nstatic const char *LDMUL = \"mir.ldmul\";\nstatic const char *LDMUL_P = \"mir.ldmul.p\";\n\nstatic long double mir_lddiv (long double d1, long double d2) { return d1 / d2; }\nstatic const char *LDDIV = \"mir.lddiv\";\nstatic const char *LDDIV_P = \"mir.lddiv.p\";\n\nstatic long double mir_ldneg (long double d) { return -d; }\nstatic const char *LDNEG = \"mir.ldneg\";\nstatic const char *LDNEG_P = \"mir.ldneg.p\";\n\nstatic const char *VA_ARG_P = \"mir.va_arg.p\";\nstatic const char *VA_ARG = \"mir.va_arg\";\nstatic const char *VA_BLOCK_ARG_P = \"mir.va_block_arg.p\";\nstatic const char *VA_BLOCK_ARG = \"mir.va_block_arg\";\n\nstatic int64_t mir_ldeq (long double d1, long double d2) { return d1 == d2; }\nstatic const char *LDEQ = \"mir.ldeq\";\nstatic const char *LDEQ_P = \"mir.ldeq.p\";\n\nstatic int64_t mir_ldne (long double d1, long double d2) { return d1 != d2; }\nstatic const char *LDNE = \"mir.ldne\";\nstatic const char *LDNE_P = \"mir.ldne.p\";\n\nstatic int64_t mir_ldlt (long double d1, long double d2) { return d1 < d2; }\nstatic const char *LDLT = \"mir.ldlt\";\nstatic const char *LDLT_P = \"mir.ldlt.p\";\n\nstatic int64_t mir_ldge (long double d1, long double d2) { return d1 >= d2; }\nstatic const char *LDGE = \"mir.ldge\";\nstatic const char *LDGE_P = \"mir.ldge.p\";\n\nstatic int64_t mir_ldgt (long double d1, long double d2) { return d1 > d2; }\nstatic const char *LDGT = \"mir.ldgt\";\nstatic const char *LDGT_P = \"mir.ldgt.p\";\n\nstatic int64_t mir_ldle (long double d1, long double d2) { return d1 <= d2; }\nstatic const char *LDLE = \"mir.ldle\";\nstatic const char *LDLE_P = \"mir.ldle.p\";\n\nstatic int get_builtin (gen_ctx_t gen_ctx, MIR_insn_code_t code, MIR_item_t *proto_item,\n                        MIR_item_t *func_import_item) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  MIR_type_t res_type;\n\n  *func_import_item = *proto_item = NULL; /* to remove uninitialized warning */\n  switch (code) {\n  case MIR_I2LD:\n    res_type = MIR_T_LD;\n    *proto_item\n      = _MIR_builtin_proto (ctx, curr_func_item->module, I2LD_P, 1, &res_type, 1, MIR_T_I64, \"v\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, I2LD, mir_i2ld);\n    return 1;\n  case MIR_UI2LD:\n    res_type = MIR_T_LD;\n    *proto_item\n      = _MIR_builtin_proto (ctx, curr_func_item->module, UI2LD_P, 1, &res_type, 1, MIR_T_I64, \"v\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, UI2LD, mir_ui2ld);\n    return 1;\n  case MIR_F2LD:\n    res_type = MIR_T_LD;\n    *proto_item\n      = _MIR_builtin_proto (ctx, curr_func_item->module, F2LD_P, 1, &res_type, 1, MIR_T_F, \"v\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, F2LD, mir_f2ld);\n    return 1;\n  case MIR_D2LD:\n    res_type = MIR_T_LD;\n    *proto_item\n      = _MIR_builtin_proto (ctx, curr_func_item->module, D2LD_P, 1, &res_type, 1, MIR_T_D, \"v\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, D2LD, mir_d2ld);\n    return 1;\n  case MIR_LD2I:\n    res_type = MIR_T_I64;\n    *proto_item\n      = _MIR_builtin_proto (ctx, curr_func_item->module, LD2I_P, 1, &res_type, 1, MIR_T_LD, \"v\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, LD2I, mir_ld2i);\n    return 1;\n  case MIR_LD2F:\n    res_type = MIR_T_F;\n    *proto_item\n      = _MIR_builtin_proto (ctx, curr_func_item->module, LD2F_P, 1, &res_type, 1, MIR_T_LD, \"v\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, LD2F, mir_ld2f);\n    return 1;\n  case MIR_LD2D:\n    res_type = MIR_T_D;\n    *proto_item\n      = _MIR_builtin_proto (ctx, curr_func_item->module, LD2D_P, 1, &res_type, 1, MIR_T_LD, \"v\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, LD2D, mir_ld2d);\n    return 1;\n  case MIR_LDADD:\n    res_type = MIR_T_LD;\n    *proto_item = _MIR_builtin_proto (ctx, curr_func_item->module, LDADD_P, 1, &res_type, 2,\n                                      MIR_T_LD, \"d1\", MIR_T_LD, \"d2\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, LDADD, mir_ldadd);\n    return 2;\n  case MIR_LDSUB:\n    res_type = MIR_T_LD;\n    *proto_item = _MIR_builtin_proto (ctx, curr_func_item->module, LDSUB_P, 1, &res_type, 2,\n                                      MIR_T_LD, \"d1\", MIR_T_LD, \"d2\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, LDSUB, mir_ldsub);\n    return 2;\n  case MIR_LDMUL:\n    res_type = MIR_T_LD;\n    *proto_item = _MIR_builtin_proto (ctx, curr_func_item->module, LDMUL_P, 1, &res_type, 2,\n                                      MIR_T_LD, \"d1\", MIR_T_LD, \"d2\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, LDMUL, mir_ldmul);\n    return 2;\n  case MIR_LDDIV:\n    res_type = MIR_T_LD;\n    *proto_item = _MIR_builtin_proto (ctx, curr_func_item->module, LDDIV_P, 1, &res_type, 2,\n                                      MIR_T_LD, \"d1\", MIR_T_LD, \"d2\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, LDDIV, mir_lddiv);\n    return 2;\n  case MIR_LDNEG:\n    res_type = MIR_T_LD;\n    *proto_item\n      = _MIR_builtin_proto (ctx, curr_func_item->module, LDNEG_P, 1, &res_type, 1, MIR_T_LD, \"d\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, LDNEG, mir_ldneg);\n    return 1;\n  case MIR_LDEQ:\n    res_type = MIR_T_I64;\n    *proto_item = _MIR_builtin_proto (ctx, curr_func_item->module, LDEQ_P, 1, &res_type, 2,\n                                      MIR_T_LD, \"d1\", MIR_T_LD, \"d2\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, LDEQ, mir_ldeq);\n    return 2;\n  case MIR_LDNE:\n    res_type = MIR_T_I64;\n    *proto_item = _MIR_builtin_proto (ctx, curr_func_item->module, LDNE_P, 1, &res_type, 2,\n                                      MIR_T_LD, \"d1\", MIR_T_LD, \"d2\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, LDNE, mir_ldne);\n    return 2;\n  case MIR_LDLT:\n    res_type = MIR_T_I64;\n    *proto_item = _MIR_builtin_proto (ctx, curr_func_item->module, LDLT_P, 1, &res_type, 2,\n                                      MIR_T_LD, \"d1\", MIR_T_LD, \"d2\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, LDLT, mir_ldlt);\n    return 2;\n  case MIR_LDGE:\n    res_type = MIR_T_I64;\n    *proto_item = _MIR_builtin_proto (ctx, curr_func_item->module, LDGE_P, 1, &res_type, 2,\n                                      MIR_T_LD, \"d1\", MIR_T_LD, \"d2\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, LDGE, mir_ldge);\n    return 2;\n  case MIR_LDGT:\n    res_type = MIR_T_I64;\n    *proto_item = _MIR_builtin_proto (ctx, curr_func_item->module, LDGT_P, 1, &res_type, 2,\n                                      MIR_T_LD, \"d1\", MIR_T_LD, \"d2\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, LDGT, mir_ldgt);\n    return 2;\n  case MIR_LDLE:\n    res_type = MIR_T_I64;\n    *proto_item = _MIR_builtin_proto (ctx, curr_func_item->module, LDLE_P, 1, &res_type, 2,\n                                      MIR_T_LD, \"d1\", MIR_T_LD, \"d2\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, LDLE, mir_ldle);\n    return 2;\n  case MIR_VA_ARG:\n    res_type = MIR_T_I64;\n    *proto_item = _MIR_builtin_proto (ctx, curr_func_item->module, VA_ARG_P, 1, &res_type, 2,\n                                      MIR_T_I64, \"va\", MIR_T_I64, \"type\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, VA_ARG, va_arg_builtin);\n    return 2;\n  case MIR_VA_BLOCK_ARG:\n    *proto_item\n      = _MIR_builtin_proto (ctx, curr_func_item->module, VA_BLOCK_ARG_P, 0, NULL, 4, MIR_T_I64,\n                            \"res\", MIR_T_I64, \"va\", MIR_T_I64, \"size\", MIR_T_I64, \"ncase\");\n    *func_import_item\n      = _MIR_builtin_func (ctx, curr_func_item->module, VA_BLOCK_ARG, va_block_arg_builtin);\n    return 4;\n  default: return 0;\n  }\n}\n\nstruct insn_pattern_info {\n  int start, num;\n};\n\ntypedef struct insn_pattern_info insn_pattern_info_t;\nDEF_VARR (insn_pattern_info_t);\n\nenum branch_type { BRANCH, JAL, AUIPC, AUIPC_JALR };\nstruct label_ref {\n  int abs_addr_p;\n  enum branch_type branch_type;\n  size_t label_val_disp;\n  union {\n    MIR_label_t label;\n    void *jump_addr; /* absolute addr for BBV */\n  } u;\n};\n\ntypedef struct label_ref label_ref_t;\nDEF_VARR (label_ref_t);\n\nstruct const_ref {\n  uint64_t val;\n  size_t const_addr_disp;\n};\n\ntypedef struct const_ref const_ref_t;\nDEF_VARR (const_ref_t);\n\nstruct target_ctx {\n  unsigned char alloca_p, block_arg_func_p, leaf_p, add_nops;\n  uint32_t non_vararg_int_args_num;\n  size_t small_aggregate_save_area;\n  MIR_insn_t temp_jump;\n  const char *temp_jump_replacement;\n  VARR (int) * pattern_indexes;\n  VARR (insn_pattern_info_t) * insn_pattern_info;\n  VARR (uint8_t) * result_code;\n  VARR (label_ref_t) * label_refs;\n  VARR (const_ref_t) * const_refs;\n  VARR (uint64_t) * abs_address_locs;\n  VARR (MIR_code_reloc_t) * relocs;\n};\n\n#define alloca_p gen_ctx->target_ctx->alloca_p\n#define block_arg_func_p gen_ctx->target_ctx->block_arg_func_p\n#define leaf_p gen_ctx->target_ctx->leaf_p\n#define add_nops gen_ctx->target_ctx->add_nops\n#define non_vararg_int_args_num gen_ctx->target_ctx->non_vararg_int_args_num\n#define small_aggregate_save_area gen_ctx->target_ctx->small_aggregate_save_area\n#define temp_jump gen_ctx->target_ctx->temp_jump\n#define temp_jump_replacement gen_ctx->target_ctx->temp_jump_replacement\n#define pattern_indexes gen_ctx->target_ctx->pattern_indexes\n#define insn_pattern_info gen_ctx->target_ctx->insn_pattern_info\n#define result_code gen_ctx->target_ctx->result_code\n#define label_refs gen_ctx->target_ctx->label_refs\n#define const_refs gen_ctx->target_ctx->const_refs\n#define abs_address_locs gen_ctx->target_ctx->abs_address_locs\n#define relocs gen_ctx->target_ctx->relocs\n\nstatic MIR_disp_t target_get_stack_slot_offset (gen_ctx_t gen_ctx, MIR_type_t type MIR_UNUSED,\n                                                MIR_reg_t slot) {\n  /* slot is 0, 1, ... */\n  size_t offset = curr_func_item->u.func->vararg_p || block_arg_func_p ? 32 : 16;\n\n  return ((MIR_disp_t) slot * 8 + offset);\n}\n\nstatic MIR_reg_t target_get_stack_slot_base_reg (gen_ctx_t gen_ctx MIR_UNUSED) {\n  return FP_HARD_REG;\n}\n\nstatic int target_valid_mem_offset_p (gen_ctx_t gen_ctx MIR_UNUSED, MIR_type_t type,\n                                      MIR_disp_t offset) {\n  MIR_disp_t offset2 = type == MIR_T_LD ? offset + 8 : offset;\n  return -(1 << 11) <= offset && offset2 < (1 << 11);\n}\n\nstatic void target_machinize (gen_ctx_t gen_ctx) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  MIR_func_t func;\n  MIR_type_t type, mem_type, res_type;\n  MIR_insn_code_t code, ext_code, new_insn_code;\n  MIR_insn_t insn, next_insn, new_insn, anchor;\n  MIR_var_t var;\n  MIR_reg_t ret_reg, arg_reg;\n  MIR_op_t ret_reg_op, arg_reg_op, mem_op, temp_op, treg_op;\n  size_t i, int_arg_num, fp_arg_num, mem_size, qwords;\n\n  assert (curr_func_item->item_type == MIR_func_item);\n  func = curr_func_item->u.func;\n  block_arg_func_p = FALSE;\n  anchor = DLIST_HEAD (MIR_insn_t, func->insns);\n  small_aggregate_save_area = 0;\n  for (i = int_arg_num = fp_arg_num = mem_size = 0; i < func->nargs; i++) {\n    /* Argument extensions is already done in simplify */\n    /* Prologue: generate arg_var = hard_reg|stack mem|stack addr ... */\n    var = VARR_GET (MIR_var_t, func->vars, i);\n    type = var.type;\n    if (MIR_blk_type_p (type) && (qwords = (var.size + 7) / 8) <= 2) {\n      if (type == MIR_T_BLK + 1) int_arg_num = (int_arg_num + 1) / 2 * 2; /* Make even */\n      if (int_arg_num < 8) {\n        MIR_insn_code_t mov_code1 = MIR_MOV;\n        MIR_type_t mem_type1 = MIR_T_I64;\n        MIR_reg_t base_arg_reg = A0_HARD_REG;\n        size_t arg_reg_num = int_arg_num;\n\n        small_aggregate_save_area += qwords * 8;\n        gen_assert (small_aggregate_save_area < (1 << 11));\n        new_insn = MIR_new_insn (ctx, MIR_ADD, _MIR_new_var_op (ctx, i + MAX_HARD_REG + 1),\n                                 _MIR_new_var_op (ctx, FP_HARD_REG),\n                                 MIR_new_int_op (ctx, -(int64_t) small_aggregate_save_area));\n        gen_add_insn_before (gen_ctx, anchor, new_insn);\n        if (qwords == 0) continue;\n        gen_mov (gen_ctx, anchor, mov_code1,\n                 _MIR_new_var_mem_op (ctx, mem_type1, 0, i + MAX_HARD_REG + 1, MIR_NON_VAR, 1),\n                 _MIR_new_var_op (ctx, base_arg_reg + arg_reg_num));\n        if (qwords == 2) {\n          MIR_insn_code_t mov_code2 = MIR_MOV;\n          MIR_type_t mem_type2 = MIR_T_I64;\n          MIR_disp_t disp = 8;\n          if (arg_reg_num < 7) {\n            gen_mov (gen_ctx, anchor, mov_code2,\n                     _MIR_new_var_mem_op (ctx, mem_type2, disp, i + MAX_HARD_REG + 1, MIR_NON_VAR,\n                                          1),\n                     _MIR_new_var_op (ctx, base_arg_reg + arg_reg_num + 1));\n          } else {\n            if (!block_arg_func_p) { /* t0 = prev sp */\n              block_arg_func_p = TRUE;\n              gen_mov (gen_ctx, anchor, MIR_MOV, _MIR_new_var_op (ctx, T0_HARD_REG),\n                       _MIR_new_var_mem_op (ctx, MIR_T_I64, 16, FP_HARD_REG, MIR_NON_VAR, 1));\n            }\n            treg_op = _MIR_new_var_op (ctx, T1_HARD_REG);\n            gen_mov (gen_ctx, anchor, mov_code2, treg_op,\n                     _MIR_new_var_mem_op (ctx, mem_type2, mem_size, T0_HARD_REG, MIR_NON_VAR, 1));\n            gen_mov (gen_ctx, anchor, mov_code2,\n                     _MIR_new_var_mem_op (ctx, mem_type2, disp, i + MAX_HARD_REG + 1, MIR_NON_VAR,\n                                          1),\n                     treg_op);\n            mem_size += 8;\n          }\n        }\n        int_arg_num += qwords;\n      } else {                   /* fully on stack -- use the address: */\n        if (!block_arg_func_p) { /* t0 = prev sp */\n          block_arg_func_p = TRUE;\n          gen_mov (gen_ctx, anchor, MIR_MOV, _MIR_new_var_op (ctx, T0_HARD_REG),\n                   _MIR_new_var_mem_op (ctx, MIR_T_I64, 16, FP_HARD_REG, MIR_NON_VAR, 1));\n        }\n        new_insn\n          = MIR_new_insn (ctx, MIR_ADD, _MIR_new_var_op (ctx, i + MAX_HARD_REG + 1),\n                          _MIR_new_var_op (ctx, T0_HARD_REG), MIR_new_int_op (ctx, mem_size));\n        gen_add_insn_before (gen_ctx, anchor, new_insn);\n        mem_size += qwords * 8;\n      }\n      continue;\n    }\n    arg_reg = get_arg_reg (type, FALSE, &int_arg_num, &fp_arg_num, &new_insn_code);\n    if (arg_reg != MIR_NON_VAR) {\n      arg_reg_op = _MIR_new_var_op (ctx, arg_reg);\n      gen_mov (gen_ctx, anchor, new_insn_code, _MIR_new_var_op (ctx, i + MAX_HARD_REG + 1),\n               arg_reg_op);\n    } else { /* arg is on the stack or blk address is on the stack: */\n      if (!block_arg_func_p) {\n        block_arg_func_p = TRUE;\n        gen_mov (gen_ctx, anchor, MIR_MOV, _MIR_new_var_op (ctx, T0_HARD_REG),\n                 _MIR_new_var_mem_op (ctx, MIR_T_I64, 16, FP_HARD_REG, MIR_NON_VAR, 1));\n      }\n      mem_type = type == MIR_T_F || type == MIR_T_D || type == MIR_T_LD ? type : MIR_T_I64;\n      new_insn_code = (type == MIR_T_F    ? MIR_FMOV\n                       : type == MIR_T_D  ? MIR_DMOV\n                       : type == MIR_T_LD ? MIR_LDMOV\n                                          : MIR_MOV);\n      mem_op = _MIR_new_var_mem_op (ctx, mem_type, mem_size, T0_HARD_REG, MIR_NON_VAR, 1);\n      gen_mov (gen_ctx, anchor, new_insn_code, _MIR_new_var_op (ctx, i + MAX_HARD_REG + 1), mem_op);\n      mem_size += type == MIR_T_LD ? 16 : 8;\n    }\n  }\n  non_vararg_int_args_num = int_arg_num;\n  alloca_p = FALSE;\n  leaf_p = TRUE;\n  for (insn = DLIST_HEAD (MIR_insn_t, func->insns); insn != NULL; insn = next_insn) {\n    MIR_item_t proto_item, func_import_item;\n    int nargs;\n\n    next_insn = DLIST_NEXT (MIR_insn_t, insn);\n    code = insn->code;\n    switch (code) {\n    case MIR_FBEQ: code = MIR_FEQ; break;\n    case MIR_FBNE: code = MIR_FNE; break;\n    case MIR_FBLT: code = MIR_FLT; break;\n    case MIR_FBGE: code = MIR_FGE; break;\n    case MIR_FBGT: code = MIR_FGT; break;\n    case MIR_FBLE: code = MIR_FLE; break;\n    case MIR_DBEQ: code = MIR_DEQ; break;\n    case MIR_DBNE: code = MIR_DNE; break;\n    case MIR_DBLT: code = MIR_DLT; break;\n    case MIR_DBGE: code = MIR_DGE; break;\n    case MIR_DBGT: code = MIR_DGT; break;\n    case MIR_DBLE: code = MIR_DLE; break;\n    case MIR_LDBEQ: code = MIR_LDEQ; break;\n    case MIR_LDBNE: code = MIR_LDNE; break;\n    case MIR_LDBLT: code = MIR_LDLT; break;\n    case MIR_LDBGE: code = MIR_LDGE; break;\n    case MIR_LDBGT: code = MIR_LDGT; break;\n    case MIR_LDBLE: code = MIR_LDLE; break;\n    case MIR_EQS:\n    case MIR_NES:\n    case MIR_BEQS:\n    case MIR_BNES:\n    case MIR_LTS:\n    case MIR_LES:\n    case MIR_GTS:\n    case MIR_GES:\n    case MIR_BLTS:\n    case MIR_BLES:\n    case MIR_BGTS:\n    case MIR_BGES: ext_code = MIR_EXT32; goto short_cmp;\n    case MIR_ULTS:\n    case MIR_ULES:\n    case MIR_UGTS:\n    case MIR_UGES:\n    case MIR_UBLTS:\n    case MIR_UBLES:\n    case MIR_UBGTS:\n    case MIR_UBGES:\n      ext_code = MIR_UEXT32;\n    short_cmp:\n      temp_op = _MIR_new_var_op (ctx, gen_new_temp_reg (gen_ctx, MIR_T_I64, func));\n      new_insn = MIR_new_insn (ctx, ext_code, temp_op, insn->ops[1]);\n      gen_add_insn_before (gen_ctx, insn, new_insn);\n      treg_op = _MIR_new_var_op (ctx, gen_new_temp_reg (gen_ctx, MIR_T_I64, func));\n      new_insn = MIR_new_insn (ctx, ext_code, treg_op, insn->ops[2]);\n      gen_add_insn_before (gen_ctx, insn, new_insn);\n      insn->ops[1] = temp_op;\n      insn->ops[2] = treg_op;\n      break;\n    default: break;\n    }\n    if (code != insn->code) {\n      temp_op = _MIR_new_var_op (ctx, gen_new_temp_reg (gen_ctx, MIR_T_I64, func));\n      new_insn = MIR_new_insn (ctx, code, temp_op, insn->ops[1], insn->ops[2]);\n      gen_add_insn_before (gen_ctx, insn, new_insn);\n      next_insn = MIR_new_insn (ctx, MIR_BT, insn->ops[0], temp_op);\n      gen_add_insn_after (gen_ctx, new_insn, next_insn);\n      gen_delete_insn (gen_ctx, insn);\n      insn = new_insn;\n    }\n    if ((nargs = get_builtin (gen_ctx, code, &proto_item, &func_import_item)) > 0) {\n      if (code == MIR_VA_ARG || code == MIR_VA_BLOCK_ARG) {\n        /* Use a builtin func call:\n           mov func_reg, func ref; [mov reg3, type;] call proto, func_reg, res_reg, va_reg,\n           reg3 */\n        MIR_op_t ops[6], func_reg_op, reg_op3;\n        MIR_op_t res_reg_op = insn->ops[0], va_reg_op = insn->ops[1], op3 = insn->ops[2];\n\n        assert (res_reg_op.mode == MIR_OP_VAR && va_reg_op.mode == MIR_OP_VAR\n                && op3.mode == (code == MIR_VA_ARG ? MIR_OP_VAR_MEM : MIR_OP_VAR));\n        func_reg_op = _MIR_new_var_op (ctx, gen_new_temp_reg (gen_ctx, MIR_T_I64, func));\n        reg_op3 = _MIR_new_var_op (ctx, gen_new_temp_reg (gen_ctx, MIR_T_I64, func));\n        next_insn = new_insn\n          = MIR_new_insn (ctx, MIR_MOV, func_reg_op, MIR_new_ref_op (ctx, func_import_item));\n        gen_add_insn_before (gen_ctx, insn, new_insn);\n        if (code == MIR_VA_ARG) {\n          new_insn\n            = MIR_new_insn (ctx, MIR_MOV, reg_op3, MIR_new_int_op (ctx, (int64_t) op3.u.mem.type));\n          op3 = reg_op3;\n          gen_add_insn_before (gen_ctx, insn, new_insn);\n        }\n        ops[0] = MIR_new_ref_op (ctx, proto_item);\n        ops[1] = func_reg_op;\n        ops[2] = res_reg_op;\n        ops[3] = va_reg_op;\n        ops[4] = op3;\n        if (code == MIR_VA_BLOCK_ARG) ops[5] = insn->ops[3];\n        new_insn = MIR_new_insn_arr (ctx, MIR_CALL, code == MIR_VA_ARG ? 5 : 6, ops);\n        gen_add_insn_before (gen_ctx, insn, new_insn);\n        gen_delete_insn (gen_ctx, insn);\n      } else { /* Use builtin: mov freg, func ref; call proto, freg, res_reg, op_reg[, op_reg2] */\n        MIR_op_t freg_op, res_reg_op = insn->ops[0], op_reg_op = insn->ops[1], ops[5];\n\n        assert (res_reg_op.mode == MIR_OP_VAR && op_reg_op.mode == MIR_OP_VAR);\n        freg_op = _MIR_new_var_op (ctx, gen_new_temp_reg (gen_ctx, MIR_T_I64, func));\n        next_insn = new_insn\n          = MIR_new_insn (ctx, MIR_MOV, freg_op, MIR_new_ref_op (ctx, func_import_item));\n        gen_add_insn_before (gen_ctx, insn, new_insn);\n        ops[0] = MIR_new_ref_op (ctx, proto_item);\n        ops[1] = freg_op;\n        ops[2] = res_reg_op;\n        ops[3] = op_reg_op;\n        if (nargs == 2) ops[4] = insn->ops[2];\n        new_insn = MIR_new_insn_arr (ctx, MIR_CALL, nargs + 3, ops);\n        gen_add_insn_before (gen_ctx, insn, new_insn);\n        gen_delete_insn (gen_ctx, insn);\n      }\n    } else if (code == MIR_VA_START) {\n      MIR_op_t prev_sp_op = _MIR_new_var_op (ctx, gen_new_temp_reg (gen_ctx, MIR_T_I64, func));\n      MIR_op_t va_op = insn->ops[0];\n      MIR_reg_t va_reg;\n\n      assert (func->vararg_p && va_op.mode == MIR_OP_VAR);\n      va_reg = va_op.u.reg;\n      /* Insns can be not simplified as soon as they match a machine insn.  */\n      /* __stack: prev_sp = mem64[fp + 16] */\n      gen_mov (gen_ctx, insn, MIR_MOV, prev_sp_op,\n               _MIR_new_var_mem_op (ctx, MIR_T_I64, 16, FP_HARD_REG, MIR_NON_VAR, 1));\n      if (non_vararg_int_args_num != 8)\n        gen_add_insn_before (gen_ctx, insn,\n                             MIR_new_insn (ctx, MIR_ADD, prev_sp_op, prev_sp_op,\n                                           MIR_new_int_op (ctx,\n                                                           ((uint64_t) non_vararg_int_args_num - 8)\n                                                             * 8)));\n      gen_mov (gen_ctx, insn, MIR_MOV,\n               _MIR_new_var_mem_op (ctx, MIR_T_I64, 0, va_reg, MIR_NON_VAR, 1), prev_sp_op);\n      gen_delete_insn (gen_ctx, insn);\n    } else if (code == MIR_VA_END) { /* do nothing */\n      gen_delete_insn (gen_ctx, insn);\n    } else if (MIR_call_code_p (code)) {\n      machinize_call (gen_ctx, insn);\n      leaf_p = FALSE;\n    } else if (code == MIR_ALLOCA) {\n      alloca_p = TRUE;\n    } else if (code == MIR_RET) {\n      /* In simplify we already transformed code for one return insn\n         and added extension insn (if any).  */\n      uint32_t n_xregs = 0, n_fpregs = 0;\n\n      assert (func->nres == MIR_insn_nops (ctx, insn));\n      for (i = 0; i < func->nres; i++) {\n        assert (insn->ops[i].mode == MIR_OP_VAR);\n        res_type = func->res_types[i];\n        if ((res_type == MIR_T_F || res_type == MIR_T_D) && n_fpregs < 2) {\n          new_insn_code = res_type == MIR_T_F ? MIR_FMOV : MIR_DMOV;\n          ret_reg = FA0_HARD_REG + n_fpregs++;\n        } else if (n_xregs < 2) {\n          new_insn_code = res_type == MIR_T_LD ? MIR_LDMOV : MIR_MOV;\n          ret_reg = A0_HARD_REG + n_xregs++;\n          if (res_type == MIR_T_LD) n_xregs++;\n        } else {\n          (*MIR_get_error_func (ctx)) (MIR_ret_error,\n                                       \"riscv can not handle this combination of return values\");\n        }\n        ret_reg_op = _MIR_new_var_op (ctx, ret_reg);\n        /* We should return unsigned 32-bit integer with sign extension according to ABI: */\n        gen_mov (gen_ctx, insn, res_type == MIR_T_U32 ? MIR_EXT32 : new_insn_code, ret_reg_op,\n                 insn->ops[i]);\n        insn->ops[i] = ret_reg_op;\n      }\n    }\n  }\n}\n\nstatic void isave (gen_ctx_t gen_ctx, MIR_insn_t anchor, int disp, MIR_reg_t base,\n                   MIR_reg_t hard_reg) {\n  gen_mov (gen_ctx, anchor, MIR_MOV,\n           _MIR_new_var_mem_op (gen_ctx->ctx, MIR_T_I64, disp, base, MIR_NON_VAR, 1),\n           _MIR_new_var_op (gen_ctx->ctx, hard_reg));\n}\n\nstatic MIR_reg_t get_base_reg_offset_for_saved_regs (gen_ctx_t gen_ctx, MIR_insn_t anchor,\n                                                     size_t *offset) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  MIR_insn_t new_insn;\n  MIR_reg_t base_reg;\n\n  if (*offset + MAX_HARD_REG * 8 < (1 << 11)) return FP_HARD_REG;\n  base_reg = T2_HARD_REG;\n  gen_mov (gen_ctx, anchor, MIR_MOV, _MIR_new_var_op (ctx, base_reg),\n           MIR_new_int_op (ctx, *offset));\n  new_insn = MIR_new_insn (ctx, MIR_ADD, _MIR_new_var_op (ctx, base_reg),\n                           _MIR_new_var_op (ctx, base_reg), _MIR_new_var_op (ctx, FP_HARD_REG));\n  gen_add_insn_before (gen_ctx, anchor, new_insn);\n  *offset = 0;\n  return base_reg;\n}\n\nstatic void target_make_prolog_epilog (gen_ctx_t gen_ctx, bitmap_t used_hard_regs,\n                                       size_t stack_slots_num) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  MIR_func_t func;\n  MIR_insn_t anchor, new_insn;\n  MIR_op_t sp_reg_op, fp_reg_op, treg_op, treg_op2;\n  MIR_reg_t base_reg;\n  int64_t start;\n  int save_prev_stack_p;\n  size_t i, offset, frame_size, frame_size_after_saved_regs, saved_iregs_num, saved_fregs_num;\n\n  assert (curr_func_item->item_type == MIR_func_item);\n  func = curr_func_item->u.func;\n  for (i = saved_iregs_num = saved_fregs_num = 0; i <= MAX_HARD_REG; i++)\n    if (!target_call_used_hard_reg_p (i, MIR_T_UNDEF) && bitmap_bit_p (used_hard_regs, i)\n        && i != FP_HARD_REG) {\n      if (i < F0_HARD_REG)\n        saved_iregs_num++;\n      else\n        saved_fregs_num++;\n    }\n  if (leaf_p && !alloca_p && saved_iregs_num == 0 && saved_fregs_num == 0 && !func->vararg_p\n      && stack_slots_num == 0 && !block_arg_func_p && small_aggregate_save_area == 0\n      && !bitmap_bit_p (used_hard_regs, RA_HARD_REG))\n    return;\n  sp_reg_op = _MIR_new_var_op (ctx, SP_HARD_REG);\n  fp_reg_op = _MIR_new_var_op (ctx, FP_HARD_REG);\n  /* Prologue: */\n  anchor = DLIST_HEAD (MIR_insn_t, func->insns);\n  frame_size = 0;\n  if (func->vararg_p && non_vararg_int_args_num < 8) /* space for vararg int regs (a<n>..a7): */\n    frame_size = (8 - non_vararg_int_args_num) * 8;\n  for (i = 0; i <= MAX_HARD_REG; i++)\n    if (!target_call_used_hard_reg_p (i, MIR_T_UNDEF) && bitmap_bit_p (used_hard_regs, i))\n      frame_size += 8;\n  if (frame_size % 16 != 0) frame_size = (frame_size + 15) / 16 * 16;\n  frame_size_after_saved_regs = frame_size;\n  frame_size += stack_slots_num * 8;\n  if (frame_size % 16 != 0) frame_size = (frame_size + 15) / 16 * 16;\n  save_prev_stack_p = func->vararg_p || block_arg_func_p;\n  treg_op = _MIR_new_var_op (ctx, T1_HARD_REG);\n  if (save_prev_stack_p) { /* the 1st insn: putting stack pointer into T1: */\n    gen_mov (gen_ctx, anchor, MIR_MOV, treg_op, sp_reg_op);\n    frame_size += 16;\n  }\n  frame_size += 16; /* ra/fp */\n  if (frame_size < (1 << 11)) {\n    new_insn = MIR_new_insn (ctx, MIR_ADD, sp_reg_op, sp_reg_op,\n                             MIR_new_int_op (ctx, -(int64_t) frame_size));\n  } else {\n    treg_op2 = _MIR_new_var_op (ctx, T2_HARD_REG);\n    new_insn = MIR_new_insn (ctx, MIR_MOV, treg_op2, MIR_new_int_op (ctx, -(int64_t) frame_size));\n    gen_add_insn_before (gen_ctx, anchor, new_insn); /* t = -frame_size */\n    new_insn = MIR_new_insn (ctx, MIR_ADD, sp_reg_op, sp_reg_op, treg_op2);\n  }\n  gen_add_insn_before (gen_ctx, anchor, new_insn); /* sp = sp - (frame_size|t) */\n  if (save_prev_stack_p)                           /* save prev sp value which is in T1: */\n    gen_mov (gen_ctx, anchor, MIR_MOV,\n             _MIR_new_var_mem_op (ctx, MIR_T_I64, 16, SP_HARD_REG, MIR_NON_VAR, 1),\n             treg_op); /* mem[sp + 16] = t1 */\n  if (!func->jret_p)\n    gen_mov (gen_ctx, anchor, MIR_MOV,\n             _MIR_new_var_mem_op (ctx, MIR_T_I64, 8, SP_HARD_REG, MIR_NON_VAR, 1),\n             _MIR_new_var_op (ctx, LINK_HARD_REG)); /* mem[sp + 8] = ra */\n  gen_mov (gen_ctx, anchor, MIR_MOV,\n           _MIR_new_var_mem_op (ctx, MIR_T_I64, 0, SP_HARD_REG, MIR_NON_VAR, 1),\n           _MIR_new_var_op (ctx, FP_HARD_REG));             /* mem[sp] = fp */\n  gen_mov (gen_ctx, anchor, MIR_MOV, fp_reg_op, sp_reg_op); /* fp = sp */\n  if (func->vararg_p && non_vararg_int_args_num < 8) {      /* save vararg int regs: */\n    MIR_reg_t base = SP_HARD_REG;\n    int reg_save_area_size = 8 * (8 - non_vararg_int_args_num);\n\n    start = (int64_t) frame_size - reg_save_area_size;\n    if (start + reg_save_area_size >= (1 << 11)) {\n      new_insn = MIR_new_insn (ctx, MIR_MOV, treg_op, MIR_new_int_op (ctx, start));\n      gen_add_insn_before (gen_ctx, anchor, new_insn); /* t = frame_size - reg_save_area_size */\n      start = 0;\n      base = T1_HARD_REG;\n    }\n    for (MIR_reg_t r = non_vararg_int_args_num + A0_HARD_REG; r <= A7_HARD_REG; r++, start += 8)\n      isave (gen_ctx, anchor, start, base, r);\n  }\n  /* Saving callee saved hard registers: */\n  offset = frame_size - frame_size_after_saved_regs;\n  base_reg = get_base_reg_offset_for_saved_regs (gen_ctx, anchor, &offset);\n  for (i = 0; i <= MAX_HARD_REG; i++)\n    if (!target_call_used_hard_reg_p (i, MIR_T_UNDEF) && bitmap_bit_p (used_hard_regs, i)\n        && i != FP_HARD_REG) {\n      if (i < F0_HARD_REG) {\n        gen_assert (offset < (1 << 11));\n        gen_mov (gen_ctx, anchor, MIR_MOV,\n                 _MIR_new_var_mem_op (ctx, MIR_T_I64, offset, base_reg, MIR_NON_VAR, 1),\n                 _MIR_new_var_op (ctx, i));\n        offset += 8;\n      } else {\n        // if (offset % 16 != 0) offset = (offset + 15) / 16 * 16;\n        gen_assert (offset < (1 << 11));\n        new_insn = gen_mov (gen_ctx, anchor, MIR_DMOV,\n                            _MIR_new_var_mem_op (ctx, MIR_T_D, offset, base_reg, MIR_NON_VAR, 1),\n                            _MIR_new_var_op (ctx, i));\n        offset += 8;\n      }\n    }\n  if (small_aggregate_save_area != 0) {\n    if (small_aggregate_save_area % 16 != 0)\n      small_aggregate_save_area = (small_aggregate_save_area + 15) / 16 * 16;\n    new_insn = MIR_new_insn (ctx, MIR_ADD, sp_reg_op, sp_reg_op,\n                             MIR_new_int_op (ctx, -(int64_t) small_aggregate_save_area));\n    gen_add_insn_before (gen_ctx, anchor, new_insn); /* sp -= <small aggr save area size> */\n  }\n  /* Epilogue: */\n  for (anchor = DLIST_TAIL (MIR_insn_t, func->insns); anchor != NULL;\n       anchor = DLIST_PREV (MIR_insn_t, anchor))\n    if (anchor->code == MIR_RET || anchor->code == MIR_JRET) break;\n  if (anchor == NULL) return;\n  /* Restoring hard registers: */\n  offset = frame_size - frame_size_after_saved_regs;\n  base_reg = get_base_reg_offset_for_saved_regs (gen_ctx, anchor, &offset);\n  for (i = 0; i <= MAX_HARD_REG; i++)\n    if (!target_call_used_hard_reg_p (i, MIR_T_UNDEF) && bitmap_bit_p (used_hard_regs, i)\n        && i != FP_HARD_REG) {\n      if (i < F0_HARD_REG) {\n        gen_assert (offset < (1 << 11));\n        gen_mov (gen_ctx, anchor, MIR_MOV, _MIR_new_var_op (ctx, i),\n                 _MIR_new_var_mem_op (ctx, MIR_T_I64, offset, base_reg, MIR_NON_VAR, 1));\n        offset += 8;\n      } else {\n        gen_assert (offset < (1 << 11));\n        new_insn = gen_mov (gen_ctx, anchor, MIR_DMOV, _MIR_new_var_op (ctx, i),\n                            _MIR_new_var_mem_op (ctx, MIR_T_D, offset, base_reg, MIR_NON_VAR, 1));\n        offset += 8;\n      }\n    }\n  /* Restore ra, sp, fp */\n  if (!func->jret_p)\n    gen_mov (gen_ctx, anchor, MIR_MOV, _MIR_new_var_op (ctx, LINK_HARD_REG),\n             _MIR_new_var_mem_op (ctx, MIR_T_I64, 8, FP_HARD_REG, MIR_NON_VAR, 1));\n  if (frame_size < (1 << 11)) {\n    new_insn = MIR_new_insn (ctx, MIR_ADD, sp_reg_op, fp_reg_op, MIR_new_int_op (ctx, frame_size));\n  } else {\n    new_insn = MIR_new_insn (ctx, MIR_MOV, treg_op, MIR_new_int_op (ctx, frame_size));\n    gen_add_insn_before (gen_ctx, anchor, new_insn); /* t = frame_size */\n    new_insn = MIR_new_insn (ctx, MIR_ADD, sp_reg_op, fp_reg_op, treg_op);\n  }\n  gen_add_insn_before (gen_ctx, anchor, new_insn); /* sp = fp + (frame_size|t) */\n  gen_mov (gen_ctx, anchor, MIR_MOV, fp_reg_op,\n           _MIR_new_var_mem_op (ctx, MIR_T_I64, 0, FP_HARD_REG, MIR_NON_VAR, 1));\n}\n\n/* 32-bit insn formats:\n|31             25|24 20|19 15|14        12|11             7|6     0|\n|funct7           | rs2 | rs1 |funct3      | rd             | opcode|  :R-type\n|imm[11:0]              | rs1 |funct3      | rd             | opcode|  :I-type\n|imm[11:5]        | rs2 | rs1 |funct3      |imm[4:0]        | opcode|  :S-type\n|imm[31:12]                                | rd             | opcode|  :U-type\n|imm[12]imm[10-5] | rs2 | rs1 |funct3      |imm[4-1]imm[11] | opcode|  :B-type\n|imm[20]imm[10:1]imm[11]imm[19-12]         | rd             | opcode|  :J-type\n\n16-bits insns:\nFormat Meaning\t\t      |15 14 13|12|11 10|9 8 7|6 5|4 3 2|1 0|\nCR     Register\t              | funct4    | rd/rs1    |    rs2  | op|\nCI     Immediate\t      | funct3 |im| rd/rs1    |    imm  | op|\nCSS    Stack-relative Store   | funct3 |    imm       |    rs2  | op|\nCIW    Wide Immediate\t      | funct3 |    imm           | rd' | op|\nCL     Load\t\t      | funct3 |   imm  | rs1'|imm| rd' | op|\nCS     Store\t\t      | funct3 |   imm  | rs1'|imm| rs2'| op|\nCS     Reg insn\t\t      | funct6          | rs1'|fn2| rs2'| op|\nCB     Branch\t\t      | funct3 | offset | rs1'| offset  | op|\nCJ     Jump                   | funct3 |    jump target         | op|\n\nRVC Register Number (rs1',rs2',rd')    000  001  010  011  100  101  110  111\nInteger Register Number\t\t       x8   x9   x10  x11  x12  x13  x14  x15\nInteger Register ABI Name\t       s0   s1   a0   a1   a2   a3   a4   a5\nFloating-Point Register Number\t       f8   f9   f10  f11  f12  f13  f14  f15\nFloating-Point Register ABI Name       fs0  fs1  fa0  fa1  fa2  fa3  fa4  fa5\n\n*/\n\nstruct pattern {\n  MIR_insn_code_t code;\n  /* Pattern elements:\n     blank - ignore\n     X - match everything\n     $ - finish successfully matching\n     r - register\n     rp - register but sp\n     C - compressed register\n     h[0-63] - hard register with given number\n     c<number> - immediate integer <number>\n\n       memory with immediate offset:\n     m[0-3] - int (signed or unsigned) type memory of size 8,16,32,64-bits\n     ms[0-3] - signed int type memory of size 8,16,32,64-bits\n     mu[0-3] - unsigned int type memory of size 8,16,32,64-bits\n       sign extended 12-bit offset\n\n     mc[s]2[s],mc3[s] - (signed) int memory of size 32 or 64-bits with compressed\n         based register and 5-bit unsigned displacement scaled by 4 or 8\n         or stack reg as base and 6-bit unsigned displacement scaled by 4 or 8\n\n       memory with immediate offset:\n     mf - memory of float\n     md - memory of double\n     mld - memory of long double (whose disp can be increased by 8)\n       sign extended 12-bit offset\n\n     mcd[s] - double memory with compressed based register (or stack register)\n              and 6-bit unsigned displacement scaled by 8\n\n     i -- 2nd or 3rd immediate op for arithmetic insn (12-bit signed)\n     j -- as i but -j should be also i (it means excluding minimal 12-bit signed) and only 3rd op\n     ju -- as j but but rounded to 16 first and only 2nd op\n     iu -- 32-bit signed immediate for arithmetic insn with zero 12 bits as 2nd op\n     ia -- any 32-bit signed immediate as 2nd op\n     I --  any 64-bit immediate\n     s --  immediate shift (5 bits) as 3th op\n     S --  immediate shift (6 bits) as 3th op\n     Sp --  nonzero immediate shift (6 bits) as 3th op\n     l --  label as the 1st or 2nd op which can be present by signed 13-bit pc offset\n     L --  label as the 1st or 2nd op which can be present by unsigned 20-bit pc offset\n     U --  label used in LADDR as 2nd op which can be present by signed 32-bit pc offset\n\n     k -- 2nd or 3rd immediate op for arithmetic insn (6-bit signed)\n     kp -- nonzero 2nd or 3rd immediate op for arithmetic insn (6-bit signed)\n     ks -- nonzero 2nd or 3rd immediate op for arithmetic insn (9-bit signed) multiple of 16\n     ku -- 18-bit signed immediate for arithmetic insn with zero low 12-bits as 2nd op\n     kw -- nonzero scaled by 4 8-bit unsigned immediate\n     jus -- imm rounded to 16 first and considered as ks\n\n     Remember we have no float or (long) double immediate at this stage. They are represented\n     by a reference to data item.  */\n\n  const char *pattern;\n  /* Replacement elements (if insn size is not mentioned it is a 32-bit insn):\n     blank - ignore\n     ; - insn separation\n\n     Ohex - opcode [6..0]\n     Fhex - funct3 (or round mode rm) [14..12]\n     fhex - funct7 [31..25]\n     ghex - funct7 w/o 1 bit [31..26]\n\n     ohex - 16-bit insn opcode [1..0] (opcodec)\n     ahex - 16-bit insn funct3 [15..13] (funct3c)\n     bhex - 16-bit insn funct4 [15..12] (funct4c)\n     chex - 16-bit insn funct6 [15..10] (funct6c)\n     dhex - 16-bit insn funct2 [6..5] (funct2c)\n     ehex - 16-bit insn functb2 [11..10] (funct2bc)\n\n     rd[0-2] - put n-th operand register into rd field [11..7] (16- and 32-bit insns)\n     rs[0-2] - put n-th operand register into rs1 field [19..15]\n     rS[0-2] - put n-th operand register into rs2 field [24..20]\n\n     rt[0-2] - put n-th operand register into rs2 field [6..2] 16-bit insns\n     ru[0-2] - put n-th operand register into rd'/rs1' field [9..7] 16-bit insns\n     rv[0-2] - put n-th operand register into rs2' field [4..2] 16-bit insns\n\n     h(d,s,S,t,u,v)<one or two hex digits> - hardware register with given number in\n     rd,rs1,rs2,rd',rs1',rs2' field m = 1st or 2nd operand is (8-,16-,32-,64-bit) mem with base and\n     signed disp\n\n     ml = 1st or 2nd operand for load is mem with base (rs1), signed imm12 disp [31..20]\n     ms = 1st or 2nd operand for store is mem with base (rs1), signed imm12 disp [31..25,11..7]\n\n     mc[2-3],mcd = 1st or 2nd operand is mem of given type with base (rs1[9..7])\n                   and scaled unsigned imm5 disp [12..10,6..5]\n     mc[2-3]s[s],mcds[s] = 1st or 2nd operand is mem with stack reg as base and\n                   scaled unsigned imm6 disp [12,6..2], last `s` means store disp [12..7]\n\n     i -- 2nd or 3rd arithmetic op 12-bit immediate [31..20]\n     j -- 3rd arithmetic op 12-bit immediate [31..20] with opposite sign\n     ju -- j but j round up to 16 first and used only as 2nd operand\n     iu -- 2nd arithmetic op immediate [31..12]\n     ih -- 20-bit upper part [31..12] of 32-bit signed 2nd op\n     il -- 12-bit lower part [31..20] of 32-bit signed 2nd op\n     I -- 20-bit upper part [31..12] of 32-bit signed pc-relative address of 64 bit\n          constant (2nd op) in the 1st word and 12-bit lower part [31..20] in the 2nd word\n     s --  immediate shift [24-20]\n     S --  immediate shift [25-20]\n     Sp --  immediate shift [12,6:2], 16-bit insn\n     shex --  immediate shift value [24-20]\n     Shex --  immediate shift value [25-20]\n     i[-]hex -- i with given value\n     iuhex -- 20-bit immediate [31..12]\n     T - 12-bit immediate which is 16 + alignment of the insn addr + 8 to 8 == (0,2,4,6)\n\n     k - immediate in field [12, 6-2], 16-bit insn\n     k[-]hex -- k with given value\n     ku - [16:12] of immediate value in [12, 6-2], 16-bit insn\n     ks - [9,4,6,8-7,5] of immediate value in [12, 6-2], 16-bit insn\n\n     l -- operand-label as signed 13-bit offset ([12|10:5] as [31:25] and [4:1|11] as [11:7]),\n          remember address of any insn is even\n     L -- operand-label as signed 21-bit offset ([20|10:1|11|19:12] as [31:12])\n     U -- operand-label as signed 32-bit offset in auipc,addi\n\n     lc -- operand-label as signed 9-bit offset ([12..10,6-2]), 16-bit insn\n     Lc -- operand-label as signed 12-bit offset ([12-2]), 16-bit insn\n     [0-3] - an operand matching n-th operand (n should be less than given operand number)\n  */\n  const char *replacement;\n};\n\n#define COMPRESS_INSNS __riscv_compressed\n\nstatic const struct pattern patterns[] = {\n#if COMPRESS_INSNS\n  {MIR_MOV, \"r r\", \"o2 b8 rd0 rt1\"}, /* c.mv rd,rs2 */\n#endif\n  {MIR_MOV, \"r r\", \"O13 F0 rd0 rs1 i0\"}, /* addi rd,rs1,0 */\n#if COMPRESS_INSNS\n  {MIR_MOV, \"C mc3\", \"o0 a3 rv0 mc3\"},   /* c.ld rd',mc3 */\n  {MIR_MOV, \"r mc3s\", \"o2 a3 rd0 mc3s\"}, /* c.ldsp rd,mc3s */\n#endif\n  {MIR_MOV, \"r m3\", \"O3 F3 rd0 ml\"}, /* ld rd,m */\n#if COMPRESS_INSNS\n  {MIR_MOV, \"mc3 C\", \"o0 a7 rv1 mc3\"},    /* c.sd rd',mc3 */\n  {MIR_MOV, \"mc3s r\", \"o2 a7 rt1 mc3ss\"}, /* c.sdsp rd,mc3s */\n#endif\n  {MIR_MOV, \"m3 r\", \"O23 F3 rS1 ms\"}, /* sd rs2,m */\n\n#if COMPRESS_INSNS\n  {MIR_MOV, \"C mcs2\", \"o0 a2 rv0 mc2\"},   /* c.lw rd',mc2 */\n  {MIR_MOV, \"r mcs2s\", \"o2 a2 rd0 mc2s\"}, /* c.lwsp rd,mc2s */\n#endif\n  {MIR_MOV, \"r ms2\", \"O3 F2 rd0 ml\"}, /* lw rd,m */\n  {MIR_MOV, \"r mu2\", \"O3 F6 rd0 ml\"}, /* lwu rd,m */\n#if COMPRESS_INSNS\n  {MIR_MOV, \"mc2 C\", \"o0 a6 rv1 mc2\"},    /* c.sw mc2,rd' */\n  {MIR_MOV, \"mc2s r\", \"o2 a6 rt1 mc2ss\"}, /* c.swsp rd,mc2s */\n#endif\n  {MIR_MOV, \"m2 r\", \"O23 F2 rS1 ms\"}, /* sw rs2,m */\n\n  {MIR_MOV, \"r ms1\", \"O3 F1 rd0 ml\"}, /* lh rd,m */\n  {MIR_MOV, \"r mu1\", \"O3 F5 rd0 ml\"}, /* lhu rd,m */\n  {MIR_MOV, \"m1 r\", \"O23 F1 rS1 ms\"}, /* sh rs2,m */\n\n  {MIR_MOV, \"r ms0\", \"O3 F0 rd0 ml\"}, /* lb rd,m */\n  {MIR_MOV, \"r mu0\", \"O3 F4 rd0 ml\"}, /* lbu rd,m */\n  {MIR_MOV, \"m0 r\", \"O23 F0 rS1 ms\"}, /* sb rs2,m */\n\n#if COMPRESS_INSNS\n  {MIR_MOV, \"r k\", \"o1 a2 rd0 k\"}, /* c.li rd,k */\n#endif\n  {MIR_MOV, \"r i\", \"O13 F0 rd0 hs0 i\"}, /* addi r,zero,i */\n#if COMPRESS_INSNS\n  {MIR_MOV, \"rp ku\", \"o1 a3 rd0 ku\"}, /* c.lui rd,k */\n#endif\n  {MIR_MOV, \"r iu\", \"O37 rd0 iu\"}, /* lui r,i */\n  //  {MIR_MOV, \"r ia\", \"O37 rd0 ih; O13 F0 rd0 rs0 il\"}, /* lui r,i; addi r,r,i */\n  {MIR_MOV, \"r I\", \"O17 rd0 I; O3 F3 rd0 rs0\"}, /* auipc r,rel-caddr; ld r,rel-caddr(r) */\n\n  {MIR_FMOV, \"r r\", \"O53 F0 f10 rd0 rs1 rS1\"}, /* fsgnj.s rd,rs1,rs2 */\n  {MIR_FMOV, \"r mf\", \"O7 F2 rd0 ml\"},          /* flw rd,m */\n  {MIR_FMOV, \"mf r\", \"O27 F2 rS1 ms\"},         /* fsw rd,m */\n\n  {MIR_DMOV, \"r r\", \"O53 F0 f11 rd0 rs1 rS1\"}, /* fsgnj.d rd,rs1,rs2 */\n#if COMPRESS_INSNS\n  {MIR_DMOV, \"C mcd\", \"o0 a1 rv0 mcd\"},   /* c.fld rd',mcd */\n  {MIR_DMOV, \"r mcds\", \"o2 a1 rd0 mcds\"}, /* c.fldsp rd,mcds */\n#endif\n  {MIR_DMOV, \"r md\", \"O7 F3 rd0 ml\"}, /* fld rd,m */\n#if COMPRESS_INSNS\n  {MIR_DMOV, \"mcd C\", \"o0 a5 rv1 mcd\"},    /* c.fsd rd',mcd */\n  {MIR_DMOV, \"mcds r\", \"o2 a5 rt1 mcdss\"}, /* c.fsdsp rd,mcdss */\n#endif\n  {MIR_DMOV, \"md r\", \"O27 F3 rS1 ms\"}, /* fsd rd,m */\n\n  /* LD values are always kept in memory.  We place them into int hard regs for passing\n     args/returning values (see machinize).  We don't need insn replacement as we split\n     load moves in target_translate: */\n  {MIR_LDMOV, \"r mld\", \"\"}, /* int_reg <- mem */\n  {MIR_LDMOV, \"mld r\", \"\"}, /* mem <- int_reg */\n  /* mem <- mem by using temp fp regs: */\n  {MIR_LDMOV, \"mld mld\", \"\"},\n\n#define STR(c) #c\n#define STR_VAL(c) STR (c)\n\n  {MIR_UNSPEC, \"c\" STR_VAL (FMVXW_CODE) \" r r\", \"O53 F0 f70 rd1 rs2\"}, /* fmv.x.w r0,r1 */\n  {MIR_UNSPEC, \"c\" STR_VAL (FMVXD_CODE) \" r r\", \"O53 F0 f71 rd1 rs2\"}, /* fmv.x.d r0,r1 */\n\n  {MIR_EXT8, \"r r\",\n   \"O13 F1 rd0 rs1 S38; O13 F5 f20 rd0 rs0 S38\"}, /* slli rd,rs1,56;srai rd,rs1,56 */\n  {MIR_EXT16, \"r r\",\n   \"O13 F1 rd0 rs1 S30; O13 F5 f20 rd0 rs0 S30\"}, /* slli rd,rs1,48;srai rd,rs1,48 */\n  {MIR_EXT32, \"r r\", \"O1b F0 rd0 rs1 i0\"},        /* addiw rd,rs1,0 */\n\n  {MIR_UEXT8, \"r r\",\n   \"O13 F1 rd0 rs1 S38; O13 F5 f0 rd0 rs0 S38\"}, /* slli rd,rs1,56;srli rd,rs1,56 */\n  {MIR_UEXT16, \"r r\",\n   \"O13 F1 rd0 rs1 S30; O13 F5 f0 rd0 rs0 S30\"}, /* slli rd,rs1,48;srli rd,rs1,48 */\n  {MIR_UEXT32, \"r r\",\n   \"O13 F1 rd0 rs1 S20; O13 F5 f0 rd0 rs0 S20\"}, /* slli rd,rs1,32;srli rd,rs1,32 */\n\n#if COMPRESS_INSNS\n  {MIR_ADD, \"r 0 r\", \"o2 b9 rd0 rt2\"}, /* c.add rd,rd,rs2 */\n#endif\n  {MIR_ADD, \"r r r\", \"O33 F0 rd0 rs1 rS2\"}, /* add rd,rs1,rs2 */\n#if COMPRESS_INSNS\n  {MIR_ADD, \"h2 0 ks\", \"o1 a3 rd0 ks\"}, /* c.addi16sp sp,sp,i */\n  {MIR_ADD, \"r 0 kp\", \"o1 a0 rd0 k\"},   /* c.addi rd,rd,i */\n  {MIR_ADD, \"C h2 kw\", \"o0 a0 rv0 kw\"}, /* c.addi4spn rd',i */\n#endif\n  {MIR_ADD, \"r r i\", \"O13 F0 rd0 rs1 i\"}, /* addi rd,rs1,i */\n#if COMPRESS_INSNS\n  {MIR_ADDS, \"C 0 C\", \"o1 c27 d1 ru0 rv2\"}, /* c.addw rd',rd',rs2' */\n#endif\n  {MIR_ADDS, \"r r r\", \"O3b F0 rd0 rs1 rS2\"}, /* addw rd,rs1,rs2 */\n#if COMPRESS_INSNS\n  {MIR_ADDS, \"r 0 k\", \"o1 a1 rd0 k\"}, /* c.addiw rd,rd,i */\n#endif\n  {MIR_ADDS, \"r r i\", \"O1b F0 rd0 rs1 i\"},      /* addiw rd,rs1,i */\n  {MIR_FADD, \"r r r\", \"O53 F7 f0 rd0 rs1 rS2\"}, /* fadd.s rd,rs1,rs2 */\n  {MIR_DADD, \"r r r\", \"O53 F7 f1 rd0 rs1 rS2\"}, /* fadd.d rd,rs1,rs2 */\n// ldadd is implemented through builtin\n\n#if COMPRESS_INSNS\n  {MIR_SUB, \"C 0 C\", \"o1 c23 d0 ru0 rv2\"}, /* c.sub rd',rd',rs2' */\n#endif\n  {MIR_SUB, \"r r r\", \"O33 F0 f20 rd0 rs1 rS2\"}, /* sub rd,rs1,rs2 */\n  {MIR_SUB, \"r r j\", \"O13 F0 rd0 rs1 j\"},       /* addi rd,rs1,-j */\n#if COMPRESS_INSNS\n  {MIR_SUBS, \"C 0 C\", \"o1 c27 d0 ru0 rv2\"}, /* c.subw rd',rd',rs2' */\n#endif\n  {MIR_SUBS, \"r r r\", \"O3b F0 f20 rd0 rs1 rS2\"}, /* subw rd,rs1,rs2 */\n  {MIR_SUBS, \"r r j\", \"O1b F0 rd0 rs1 j\"},       /* addiw rd,rs1,-j */\n  {MIR_FSUB, \"r r r\", \"O53 F7 f4 rd0 rs1 rS2\"},  /* fsub.s rd,rs1,rs2 */\n  {MIR_DSUB, \"r r r\", \"O53 F7 f5 rd0 rs1 rS2\"},  /* fsub.d rd,rs1,rs2 */\n  // ldsub is implemented through builtin\n\n  {MIR_MUL, \"r r r\", \"O33 F0 f1 rd0 rs1 rS2\"},  /* mul rd,rs1,rs2 */\n  {MIR_MULS, \"r r r\", \"O3b F0 f1 rd0 rs1 rS2\"}, /* mulw rd,rs1,rs2 */\n  {MIR_FMUL, \"r r r\", \"O53 F7 f8 rd0 rs1 rS2\"}, /* fmul.s rd,rs1,rs2*/\n  {MIR_DMUL, \"r r r\", \"O53 F7 f9 rd0 rs1 rS2\"}, /* fmul.d rd,rs1,rs2*/\n  // ldmul is implemented through builtin\n\n  {MIR_DIV, \"r r r\", \"O33 F4 f1 rd0 rs1 rS2\"},   /* div rd,rs1,rs2 */\n  {MIR_DIVS, \"r r r\", \"O3b F4 f1 rd0 rs1 rS2\"},  /* divw rd,rs1,rs2 */\n  {MIR_UDIV, \"r r r\", \"O33 F5 f1 rd0 rs1 rS2\"},  /* divu rd,rs1,rs2 */\n  {MIR_UDIVS, \"r r r\", \"O3b F5 f1 rd0 rs1 rS2\"}, /* divuw rd,rs1,rs2 */\n  {MIR_FDIV, \"r r r\", \"O53 F7 fc rd0 rs1 rS2\"},  /* fdiv.s rd,rs1,rs2*/\n  {MIR_DDIV, \"r r r\", \"O53 F7 fd rd0 rs1 rS2\"},  /* fdiv.d rd,rs1,rs2*/\n  // lddiv is implemented through builtin\n\n  {MIR_MOD, \"r r r\", \"O33 F6 f1 rd0 rs1 rS2\"},   /* rem rd,rs1,rs2 */\n  {MIR_MODS, \"r r r\", \"O3b F6 f1 rd0 rs1 rS2\"},  /* remw rd,rs1,rs2 */\n  {MIR_UMOD, \"r r r\", \"O33 F7 f1 rd0 rs1 rS2\"},  /* remu rd,rs1,rs2 */\n  {MIR_UMODS, \"r r r\", \"O3b F7 f1 rd0 rs1 rS2\"}, /* remuw rd,rs1,rs2 */\n\n  {MIR_EQ, \"r r r\",\n   \"O33 F0 f20 rd0 rs1 rS2; O13 F3 rd0 rs0 i1\"},            /* sub rd,rs1,rs2; sltiu rd,rs1,1 */\n  {MIR_EQ, \"r r j\", \"O13 F0 rd0 rs1 j; O13 F3 rd0 rs0 i1\"}, /* addi rd,rs1,-j; sltiu rd,rs1,1 */\n  {MIR_EQS, \"r r r\",\n   \"O3b F0 f20 rd0 rs1 rS2; O13 F3 rd0 rs0 i1\"},             /* subw rd,rs1,rs2; sltiu rd,rs1,1 */\n  {MIR_EQS, \"r r j\", \"O1b F0 rd0 rs1 j; O13 F3 rd0 rs0 i1\"}, /* addiw rd,rs1,-j; sltiu rd,rs1,1 */\n\n  {MIR_NE, \"r r r\",\n   \"O33 F0 f20 rd0 rs1 rS2; O33 F3 rd0 hs0 rS0\"},            /* sub rd,rs1,rs2; sltu rd,z,rs2 */\n  {MIR_NE, \"r r j\", \"O13 F0 rd0 rs1 j; O33 F3 rd0 hs0 rS0\"}, /* addi rd,rs1,-j; sltu rd,z,rs2 */\n  {MIR_NES, \"r r r\",\n   \"O33 F0 f20 rd0 rs1 rS2; O33 F3 rd0 hs0 rS0\"},             /* sub rd,rs1,rs2; sltu rd,z,rs2 */\n  {MIR_NES, \"r r j\", \"O13 F0 rd0 rs1 j; O33 F3 rd0 hs0 rS0\"}, /* addi rd,rs1,-j; sltu rd,z,rs2 */\n\n  {MIR_LT, \"r r r\", \"O33 F2 f0 rd0 rs1 rS2\"},   /* slt rd,rs1,rs2 */\n  {MIR_LT, \"r r i\", \"O13 F2 f0 rd0 rs1 i\"},     /* slti rd,rs1,i */\n  {MIR_LTS, \"r r r\", \"O33 F2 f0 rd0 rs1 rS2\"},  /* slt rd,rs1,rs2 */\n  {MIR_LTS, \"r r i\", \"O13 F2 f0 rd0 rs1 i\"},    /* slti rd,rs1,i */\n  {MIR_ULT, \"r r r\", \"O33 F3 f0 rd0 rs1 rS2\"},  /* sltu rd,rs1,rs2 */\n  {MIR_ULT, \"r r i\", \"O13 F3 f0 rd0 rs1 i\"},    /* sltiu rd,rs1,i */\n  {MIR_ULTS, \"r r r\", \"O33 F3 f0 rd0 rs1 rS2\"}, /* sltu rd,rs1,rs2 */\n  {MIR_ULTS, \"r r i\", \"O13 F3 f0 rd0 rs1 i\"},   /* sltiu rd,rs1,i */\n\n  // ??? le r,imm -> lt r,imm+1\n  /* sgt rd,rs1,rs2;xori rd,rs1,1 */\n  {MIR_LE, \"r r r\", \"O33 F2 f0 rd0 rs2 rS1; O13 F4 f0 rd0 rs0 i1\"},\n  /* sgti rd,rs1,i;xori rd,rs1,1 */\n  {MIR_LE, \"r i r\", \"O13 F2 f0 rd0 rs2 i; O13 F4 f0 rd0 rs0 i1\"},\n  /* sgt rd,rs1,rs2;xori rd,rs1,1 */\n  {MIR_LES, \"r r r\", \"O33 F2 f0 rd0 rs2 rS1; O13 F4 f0 rd0 rs0 i1\"},\n  /* sgti rd,rs1,i;xori rd,rs1,1 */\n  {MIR_LES, \"r i r\", \"O13 F2 f0 rd0 rs2 i; O13 F4 f0 rd0 rs0 i1\"},\n  /* sgtu rd,rs1,rs2;xori rd,rs1,1 */\n  {MIR_ULE, \"r r r\", \"O33 F3 f0 rd0 rs2 rS1; O13 F4 f0 rd0 rs0 i1\"},\n  /* sgtui rd,rs1,i;xori rd,rs1,1 */\n  {MIR_ULE, \"r i r\", \"O13 F3 f0 rd0 rs2 i; O13 F4 f0 rd0 rs0 i1\"},\n  /* sgtu rd,rs1,rs2;xori rd,rs1,1 */\n  {MIR_ULES, \"r r r\", \"O33 F3 f0 rd0 rs2 rS1; O13 F4 f0 rd0 rs0 i1\"},\n  /* sgtui rd,rs1,i;xori rd,rs1,1 */\n  {MIR_ULES, \"r i r\", \"O13 F3 f0 rd0 rs2 i; O13 F4 f0 rd0 rs0 i1\"},\n\n  {MIR_GT, \"r r r\", \"O33 F2 f0 rd0 rs2 rS1\"},   /* slt rd,rs1,rs2 */\n  {MIR_GT, \"r i r\", \"O13 F2 f0 rd0 rs2 i\"},     /* slti rd,rs1,i */\n  {MIR_GTS, \"r r r\", \"O33 F2 f0 rd0 rs2 rS1\"},  /* slt rd,rs1,rs2 */\n  {MIR_GTS, \"r i r\", \"O13 F2 f0 rd0 rs2 i\"},    /* slti rd,rs1,i */\n  {MIR_UGT, \"r r r\", \"O33 F3 f0 rd0 rs2 rS1\"},  /* sltu rd,rs1,rs2 */\n  {MIR_UGT, \"r i r\", \"O13 F3 f0 rd0 rs2 i\"},    /* sltiu rd,rs1,i */\n  {MIR_UGTS, \"r r r\", \"O33 F3 f0 rd0 rs2 rS1\"}, /* sltu rd,rs1,rs2 */\n  {MIR_UGTS, \"r i r\", \"O13 F3 f0 rd0 rs2 i\"},   /* sltiu rd,rs1,i */\n\n  /* slt rd,rs1,rs2;xori rd,rs1,1 */\n  {MIR_GE, \"r r r\", \"O33 F2 f0 rd0 rs1 rS2; O13 F4 f0 rd0 rs0 i1\"},\n  /* slti rd,rs1,i;xori rd,rs1,1 */\n  {MIR_GE, \"r r i\", \"O13 F2 f0 rd0 rs1 i; O13 F4 f0 rd0 rs0 i1\"},\n  /* slt rd,rs1,rs2;xori rd,rs1,1 */\n  {MIR_GES, \"r r r\", \"O33 F2 f0 rd0 rs1 rS2; O13 F4 f0 rd0 rs0 i1\"},\n  /* slti rd,rs1,i;xori rd,rs1,1 */\n  {MIR_GES, \"r r i\", \"O13 F2 f0 rd0 rs1 i; O13 F4 f0 rd0 rs0 i1\"},\n  /* sltu rd,rs1,rs2;xori rd,rs1,1 */\n  {MIR_UGE, \"r r r\", \"O33 F3 f0 rd0 rs1 rS2; O13 F4 f0 rd0 rs0 i1\"},\n  /* sltui rd,rs1,i;xori rd,rs1,1 */\n  {MIR_UGE, \"r r i\", \"O13 F3 f0 rd0 rs1 i; O13 F4 f0 rd0 rs0 i1\"},\n  /* sltu rd,rs1,rs2;xori rd,rs1,1 */\n  {MIR_UGES, \"r r r\", \"O33 F3 f0 rd0 rs1 rS2; O13 F4 f0 rd0 rs0 i1\"},\n  /* sltui rd,rs1,i;xori rd,rs1,1 */\n  {MIR_UGES, \"r r i\", \"O13 F3 f0 rd0 rs1 i; O13 F4 f0 rd0 rs0 i1\"},\n\n  {MIR_FEQ, \"r r r\", \"O53 F2 f50 rd0 rs1 rS2\"}, /* feq.s rd,rs1,rs2 */\n  {MIR_DEQ, \"r r r\", \"O53 F2 f51 rd0 rs1 rS2\"}, /* feq.d rd,rs1,rs2 */\n  {MIR_FNE, \"r r r\",\n   \"O53 F2 f50 rd0 rs1 rS2; O13 F4 rd0 rs0 i1\"}, /* feq.s rd,rs1,rs2; xori rd,r1,1 */\n  {MIR_DNE, \"r r r\",\n   \"O53 F2 f51 rd0 rs1 rS2; O13 F4 rd0 rs0 i1\"}, /* feq.d rd,rs1,rs2;xori rd,rs1,1 */\n  {MIR_FLT, \"r r r\", \"O53 F1 f50 rd0 rs1 rS2\"},  /* flt.s rd,rs1,rs2 */\n  {MIR_DLT, \"r r r\", \"O53 F1 f51 rd0 rs1 rS2\"},  /* flt.d rd,rs1,rs2 */\n  {MIR_FLE, \"r r r\", \"O53 F0 f50 rd0 rs1 rS2\"},  /* fle.s rd,rs1,rs2 */\n  {MIR_DLE, \"r r r\", \"O53 F0 f51 rd0 rs1 rS2\"},  /* fle.d rd,rs1,rs2 */\n  {MIR_FGT, \"r r r\", \"O53 F1 f50 rd0 rs2 rS1\"},  /* flt.s rd,rs1,rs2 */\n  {MIR_DGT, \"r r r\", \"O53 F1 f51 rd0 rs2 rS1\"},  /* flt.d rd,rs1,rs2 */\n  {MIR_FGE, \"r r r\", \"O53 F0 f50 rd0 rs2 rS1\"},  /* fle.s rd,rs1,rs2 */\n  {MIR_DGE, \"r r r\", \"O53 F0 f51 rd0 rs2 rS1\"},  /* fle.d rd,rs1,rs2 */\n\n  {MIR_JMP, \"L\", \"O6f hd0 L\"}, /* jal: 20-bit offset (w/o 1 bit) jmp */\n\n  {MIR_LADDR, \"r U\", \"O17 rd0 U; O13 F0 rd0 rs0\"}, /* auipc r,hi(l);addi r,r,low(L) */\n  {MIR_JMPI, \"r\", \"O67 F0 hd0 rs0 i0\"},            /* jmp *r: jalr zero,r,0 */\n\n  {MIR_BT, \"l r\", \"O63 F1 rs1 hS0 l\"},  /* bne rs1,zero,l */\n  {MIR_BTS, \"l r\", \"O63 F1 rs1 hS0 l\"}, /* bne rs1,zero,l */\n  {MIR_BF, \"l r\", \"O63 F0 rs1 hS0 l\"},  /* beq rs1,zero,l */\n  {MIR_BFS, \"l r\", \"O63 F0 rs1 hS0 l\"}, /* beq rs1,zero,l */\n\n  {MIR_BEQ, \"l r r\", \"O63 F0 rs1 rS2 l\"},  /* beq rs1,rs2,l */\n  {MIR_BEQS, \"l r r\", \"O63 F0 rs1 rS2 l\"}, /* beq rs1,rs2,l */\n\n  {MIR_BNE, \"l r r\", \"O63 F1 rs1 rS2 l\"},  /* bne rs1,rs2,l */\n  {MIR_BNES, \"l r r\", \"O63 F1 rs1 rS2 l\"}, /* bne rs1,rs2,l */\n\n  {MIR_BLT, \"l r r\", \"O63 F4 rs1 rS2 l\"},   /* blt rs1,rs2,l */\n  {MIR_BLTS, \"l r r\", \"O63 F4 rs1 rS2 l\"},  /* blt rs1,rs2,l */\n  {MIR_UBLT, \"l r r\", \"O63 F6 rs1 rS2 l\"},  /* bltu rs1,rs2,l */\n  {MIR_UBLTS, \"l r r\", \"O63 F6 rs1 rS2 l\"}, /* bltu rs1,rs2,l */\n\n  {MIR_BGE, \"l r r\", \"O63 F5 rs1 rS2 l\"},   /* bge rs1,rs2,l */\n  {MIR_BGES, \"l r r\", \"O63 F5 rs1 rS2 l\"},  /* bge rs1,rs2,l */\n  {MIR_UBGE, \"l r r\", \"O63 F7 rs1 rS2 l\"},  /* bgeu rs1,rs2,l */\n  {MIR_UBGES, \"l r r\", \"O63 F7 rs1 rS2 l\"}, /* bgeu rs1,rs2,l */\n\n  {MIR_BGT, \"l r r\", \"O63 F4 rs2 rS1 l\"},   /* blt rs1,rs2,l */\n  {MIR_BGTS, \"l r r\", \"O63 F4 rs2 rS1 l\"},  /* blt rs1,rs2,l */\n  {MIR_UBGT, \"l r r\", \"O63 F6 rs2 rS1 l\"},  /* bltu rs1,rs2,l */\n  {MIR_UBGTS, \"l r r\", \"O63 F6 rs2 rS1 l\"}, /* bltu rs1,rs2,l */\n\n  {MIR_BLE, \"l r r\", \"O63 F5 rs2 rS1 l\"},   /* bge rs1,rs2,l */\n  {MIR_BLES, \"l r r\", \"O63 F5 rs2 rS1 l\"},  /* bge rs1,rs2,l */\n  {MIR_UBLE, \"l r r\", \"O63 F7 rs2 rS1 l\"},  /* bgeu rs1,rs2,l */\n  {MIR_UBLES, \"l r r\", \"O63 F7 rs2 rS1 l\"}, /* bgeu rs1,rs2,l */\n  // there are no FBx,DBx,LDBx as they are machinized into compare and BT\n\n  {MIR_NEG, \"r r\", \"O33 F0 f20 rd0 hs0 rS1\"},  /* sub rd,z,rs2 */\n  {MIR_NEGS, \"r r\", \"O3b F0 f20 rd0 hs0 rS1\"}, /* subw rd,z,rs2 */\n  {MIR_FNEG, \"r r\", \"O53 F1 f10 rd0 rs1 rS1\"}, /* fsgnjn.s rd,rs1,rs2 */\n  {MIR_DNEG, \"r r\", \"O53 F1 f11 rd0 rs1 rS1\"}, /* fsgnjn.d rd,rs1,rs2 */\n  // ldneg is a builtin\n\n  {MIR_LSH, \"r r r\", \"O33 F1 f0 rd0 rs1 rS2\"},  /* sll rd,rs1,rs2 */\n  {MIR_LSHS, \"r r r\", \"O3b F1 f0 rd0 rs1 rS2\"}, /* sllw rd,rs1,rs2 */\n#if COMPRESS_INSNS\n  {MIR_LSH, \"r 0 Sp\", \"o2 a0 rd0 Sp\"}, /* c.slli rd,rd,sh */\n#endif\n  {MIR_LSH, \"r r S\", \"O13 F1 f0 rd0 rs1 S\"},  /* slli rd,rs1,sh */\n  {MIR_LSHS, \"r r s\", \"O1b F1 f0 rd0 rs1 s\"}, /* slliw rd,rs1,sh */\n\n  {MIR_RSH, \"r r r\", \"O33 F5 f20 rd0 rs1 rS2\"},  /* sra rd,rs1,rs2 */\n  {MIR_RSHS, \"r r r\", \"O3b F5 f20 rd0 rs1 rS2\"}, /* sraw rd,rs1,rs2 */\n#if COMPRESS_INSNS\n  {MIR_RSH, \"C 0 Sp\", \"o1 a4 e1 ru0 Sp\"}, /* c.srai rd',rd',sh */\n#endif\n  {MIR_RSH, \"r r S\", \"O13 F5 f20 rd0 rs1 S\"},  /* srai rd,rs1,sh */\n  {MIR_RSHS, \"r r s\", \"O1b F5 f20 rd0 rs1 s\"}, /* sraiw rd,rs1,sh */\n\n  {MIR_URSH, \"r r r\", \"O33 F5 f0 rd0 rs1 rS2\"},  /* srl rd,rs1,rs2 */\n  {MIR_URSHS, \"r r r\", \"O3b F5 f0 rd0 rs1 rS2\"}, /* srlw rd,rs1,rs2 */\n#if COMPRESS_INSNS\n  {MIR_URSH, \"C 0 Sp\", \"o1 a4 e0 ru0 Sp\"}, /* c.srli rd',rd',sh */\n#endif\n  {MIR_URSH, \"r r S\", \"O13 F5 f0 rd0 rs1 S\"},  /* srli rd,rs1,rs2 */\n  {MIR_URSHS, \"r r s\", \"O1b F5 f0 rd0 rs1 s\"}, /* srliw rd,rs1,sh */\n\n#if COMPRESS_INSNS\n  {MIR_AND, \"C 0 C\", \"o1 c23 d3 ru0 rv2\"}, /* c.and rd',rd',rs2' */\n#endif\n  {MIR_AND, \"r r r\", \"O33 F7 f0 rd0 rs1 rS2\"}, /* and rd,rs1,rs2 */\n#if COMPRESS_INSNS\n  {MIR_AND, \"C 0 k\", \"o1 a4 e2 ru0 k\"}, /* c.andi rd',rd',i */\n#endif\n  {MIR_AND, \"r r i\", \"O13 F7 f0 rd0 rs1 i\"}, /* andi rd,rs1,i */\n#if COMPRESS_INSNS\n  {MIR_ANDS, \"C 0 C\", \"o1 c23 d3 ru0 rv2\"}, /* c.and rd',rd',rs2' */\n#endif\n  {MIR_ANDS, \"r r r\", \"O33 F7 f0 rd0 rs1 rS2\"}, /* and rd,rs1,rs2 */\n#if COMPRESS_INSNS\n  {MIR_ANDS, \"C 0 k\", \"o1 a4 e2 ru0 k\"}, /* c.andi rd',rd',i */\n#endif\n  {MIR_ANDS, \"r r i\", \"O13 F7 f0 rd0 rs1 i\"}, /* andi rd,rs1,i */\n\n#if COMPRESS_INSNS\n  {MIR_OR, \"C 0 C\", \"o1 c23 d2 ru0 rv2\"}, /* c.or rd',rd',rs2' */\n#endif\n  {MIR_OR, \"r r r\", \"O33 F6 f0 rd0 rs1 rS2\"}, /* or rd,rs1,rs2 */\n  {MIR_OR, \"r r i\", \"O13 F6 f0 rd0 rs1 i\"},   /* ori rd,rs1,i */\n#if COMPRESS_INSNS\n  {MIR_ORS, \"C 0 C\", \"o1 c23 d2 ru0 rv2\"}, /* c.or rd',rd',rs2' */\n#endif\n  {MIR_ORS, \"r r r\", \"O33 F6 f0 rd0 rs1 rS2\"}, /* or rd,rs1,rs2 */\n  {MIR_ORS, \"r r i\", \"O13 F6 f0 rd0 rs1 i\"},   /* ori rd,rs1,i */\n\n#if COMPRESS_INSNS\n  {MIR_XOR, \"C 0 C\", \"o1 c23 d1 ru0 rv2\"}, /* c.xor rd',rd',rs2' */\n#endif\n  {MIR_XOR, \"r r r\", \"O33 F4 f0 rd0 rs1 rS2\"}, /* xor rd,rs1,rs2 */\n  {MIR_XOR, \"r r i\", \"O13 F4 f0 rd0 rs1 i\"},   /* xori rd,rs1,i */\n#if COMPRESS_INSNS\n  {MIR_XORS, \"C 0 C\", \"o1 c23 d1 ru0 rv2\"}, /* c.xor rd',rd',rs2' */\n#endif\n  {MIR_XORS, \"r r r\", \"O33 F4 f0 rd0 rs1 rS2\"}, /* xor rd,rs1,rs2 */\n  {MIR_XORS, \"r r i\", \"O13 F4 f0 rd0 rs1 i\"},   /* xori rd,rs1,i */\n\n  {MIR_I2F, \"r r\", \"O53 F7 f68 hS2 rd0 rs1\"},  /* fcvt.s.l rd,rs1 */\n  {MIR_I2D, \"r r\", \"O53 F7 f69 hS2 rd0 rs1\"},  /* fcvt.d.l rd,rs1 */\n  {MIR_UI2F, \"r r\", \"O53 F7 f68 hS3 rd0 rs1\"}, /* fcvt.s.lu rd,rs1 */\n  {MIR_UI2D, \"r r\", \"O53 F7 f69 hS3 rd0 rs1\"}, /* fcvt.d.lu rd,rs1 */\n\n  {MIR_F2I, \"r r\", \"O53 F1 f60 hS2 rd0 rs1\"}, /* fcvt.l.s rd,rs1,rtz */\n  {MIR_D2I, \"r r\", \"O53 F1 f61 hS2 rd0 rs1\"}, /* fcvt.l.d rd,rs1,rtz */\n  {MIR_F2D, \"r r\", \"O53 F0 f21 hS0 rd0 rs1\"}, /* fcvt.d.s rd,rs1 -- never round */\n  {MIR_D2F, \"r r\", \"O53 F7 f20 hS1 rd0 rs1\"}, /* fcvt.s.d rd,rs1 */\n// i2ld, ui2ld, ld2i, f2ld, d2ld, ld2f, ld2d are builtins\n\n#if COMPRESS_INSNS\n  {MIR_CALL, \"X r $\", \"o2 b9 rd1\"},   /* c.jalr rd */\n  {MIR_INLINE, \"X r $\", \"o2 b9 rd1\"}, /* c.jalr rd */\n  {MIR_RET, \"$\", \"o2 b8 hd1\"},        /* c.jr ra  */\n#endif\n  {MIR_CALL, \"X r $\", \"O67 F0 hd1 rs1 i0\"},   /* jalr rd,rs1 */\n  {MIR_INLINE, \"X r $\", \"O67 F0 hd1 rs1 i0\"}, /* jalr rd,rs1 */\n  {MIR_RET, \"$\", \"O67 F0 hd0 hs1 i0\"},        /* jalr hr0,hr1,0  */\n\n  {MIR_JCALL, \"X r $\", \"O67 F0 hd0 rs1 i0\"}, /* jmp *r: jalr zero,r,0 */\n  {MIR_JRET, \"r $\", \"O67 F0 hd0 rs0 i0\"},    /* jmp *r: jalr zero,r,0 */\n\n#if COMPRESS_INSNS\n  /* addi r0,r0,15; andi r0,r0,-16; c.sub sp,sp,r0; c.mov r0,sp: */\n  {MIR_ALLOCA, \"C 0\",\n   \"o1 a0 rd0 kf; o1 a4 e2 ru0 k-10;\"        /* c.addi r0,r0,15; c.andi r0,r0,-16 */\n   \"O33 F0 f20 hd2 hs2 rS0; o2 b8 rd0 ht2\"}, /* sub sp,sp,r0; c.mv r0,sp */\n  /* addi r0,r1,15; c.andi r0,r0,-16; c.sub sp,sp,r0; c.mov r0,sp: */\n  {MIR_ALLOCA, \"C r\",\n   \"O13 F0 rd0 rs1 if; o1 a4 e2 ru0 k-10;\"   /* addi r0,r1,15; c.andi r0,r0,-16 */\n   \"O33 F0 f20 hd2 hs2 rS0; o2 b8 rd0 ht2\"}, /* sub sp,sp,r0; c.mv r0,sp */\n  /* addi r0,r1,15; andi r0,r0,-16; sub sp,sp,r0; c.mov r0,sp: */\n  {MIR_ALLOCA, \"r r\",\n   \"O13 F0 rd0 rs1 if; O13 F7 f0 rd0 rs0 i-10;\" /* addi r0,r1,15; andi r0,r0,-16 */\n   \"O33 F0 f20 hd2 hs2 rS0; o2 b8 rd0 ht2\"},    /* sub sp,sp,r0; c.mv r0,sp */\n  /* c.addi16sp sp,sp,-roundup(imm,16); c.mv r0,sp: */\n  {MIR_ALLOCA, \"r jus\", \"o1 a3 hd2 jus; o2 b8 rd0 ht2\"},\n  /* addi sp,sp,-roundup(imm,16); c.mv r0,sp: */\n  {MIR_ALLOCA, \"r ju\", \"O13 F0 hd2 hs2 ju; o2 b8 rd0 ht2\"},\n#else\n  /* addi r0,r1,15; andi r0,r0,-16; sub sp,sp,r0; mov r0,sp: */\n  {MIR_ALLOCA, \"r r\",\n   \"O13 F0 rd0 rs1 if; O13 F7 f0 rd0 rs0 i-10;\"  /* addi r0,r1,15; andi r0,r0,-16 */\n   \"O33 F0 f20 hd2 hs2 rS0; O13 F0 rd0 hs2 i0\"}, /* sub sp,sp,r0; addi r0,sp,0 */\n  /* addi sp,sp,-roundup(imm,16); c.mv r0,sp: */\n  {MIR_ALLOCA, \"r ju\", \"O13 F0 hd2 hs2 ju; O13 F0 rd0 hs2 i0\"},\n#endif\n\n#if COMPRESS_INSNS\n  {MIR_BSTART, \"r\", \"o2 b8 rd0 ht2\"}, /* r = sp: c.mv rd,rs2 */\n  {MIR_BEND, \"r\", \"o2 b8 hd2 rt0\"},   /* sp = r: c.mv rd,rs2 */\n#else\n  {MIR_BSTART, \"r\", \"O13 F0 rd0 hs2 i0\"}, /* r = sp: addi rd,rs1,0 */\n  {MIR_BEND, \"r\", \"O13 F0 hd2 rs0 i0\"},   /* sp = r: addi rd,rs1,0 */\n#endif\n  /* slli t5,r,3; auipc t6,0; add t6,t6,t5;ld t6,T(t6);jalr zero,t6,0;\n     8-byte aligned TableContent.  Remember r can be t5 can be if switch operand is memory. */\n  {MIR_SWITCH, \"r $\",\n   \"O13 F1 hd1e rs0 S3; O17 hd1f iu0; O33 F0 hd1f hs1f hS1e; O3 F3 hd1f hs1f T; O67 F0 hd0 hs1f \"\n   \"i0\"},\n\n};\n\nstatic void target_get_early_clobbered_hard_regs (MIR_insn_t insn, MIR_reg_t *hr1, MIR_reg_t *hr2) {\n  *hr1 = *hr2 = MIR_NON_VAR;\n  if (insn->code == MIR_MOD || insn->code == MIR_MODS || insn->code == MIR_UMOD\n      || insn->code == MIR_UMODS)\n    *hr1 = R8_HARD_REG;\n}\n\nstatic int pattern_index_cmp (const void *a1, const void *a2) {\n  int i1 = *(const int *) a1, i2 = *(const int *) a2;\n  int c1 = (int) patterns[i1].code, c2 = (int) patterns[i2].code;\n\n  return c1 != c2 ? c1 - c2 : (long) i1 - (long) i2;\n}\n\nstatic void patterns_init (gen_ctx_t gen_ctx) {\n  MIR_alloc_t alloc = gen_alloc (gen_ctx);\n  int i, ind, n = sizeof (patterns) / sizeof (struct pattern);\n  MIR_insn_code_t prev_code, code;\n  insn_pattern_info_t *info_addr;\n  insn_pattern_info_t pinfo = {0, 0};\n\n  VARR_CREATE (int, pattern_indexes, alloc, 0);\n  for (i = 0; i < n; i++) VARR_PUSH (int, pattern_indexes, i);\n  qsort (VARR_ADDR (int, pattern_indexes), n, sizeof (int), pattern_index_cmp);\n  VARR_CREATE (insn_pattern_info_t, insn_pattern_info, alloc, 0);\n  for (i = 0; i < MIR_INSN_BOUND; i++) VARR_PUSH (insn_pattern_info_t, insn_pattern_info, pinfo);\n  info_addr = VARR_ADDR (insn_pattern_info_t, insn_pattern_info);\n  for (prev_code = MIR_INSN_BOUND, i = 0; i < n; i++) {\n    ind = VARR_GET (int, pattern_indexes, i);\n    if ((code = patterns[ind].code) != prev_code) {\n      if (i != 0) info_addr[prev_code].num = i - info_addr[prev_code].start;\n      info_addr[code].start = i;\n      prev_code = code;\n    }\n  }\n  assert (prev_code != MIR_INSN_BOUND);\n  info_addr[prev_code].num = n - info_addr[prev_code].start;\n}\n\nstatic int dec_value (int ch) { return '0' <= ch && ch <= '9' ? ch - '0' : -1; }\n\nstatic uint64_t read_dec (const char **ptr) {\n  int v;\n  const char *p;\n  uint64_t res = 0;\n\n  for (p = *ptr; (v = dec_value (*p)) >= 0; p++) {\n    gen_assert ((res >> 60) == 0);\n    res = res * 10 + v;\n  }\n  gen_assert (p != *ptr);\n  *ptr = p - 1;\n  return res;\n}\n\nstatic int compressed_reg_p (MIR_reg_t reg, int int_only_p) {\n  if (R8_HARD_REG <= reg && reg <= R15_HARD_REG) return TRUE;\n  if (!int_only_p && F8_HARD_REG <= reg && reg <= F15_HARD_REG) return TRUE;\n  return FALSE;\n}\n\nstatic int pattern_match_p (gen_ctx_t gen_ctx, const struct pattern *pat, MIR_insn_t insn) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  int n;\n  size_t nop, nops = MIR_insn_nops (ctx, insn);\n  const char *p;\n  char ch, start_ch;\n  MIR_op_t op, original;\n  MIR_op_mode_t mode;\n\n  for (nop = 0, p = pat->pattern; *p != 0; p++, nop++) {\n    while (*p == ' ' || *p == '\\t') p++;\n    if (*p == '$') return TRUE;\n    if (MIR_call_code_p (insn->code) && nop >= nops) return FALSE;\n    gen_assert (nop < nops);\n    op = insn->ops[nop];\n    switch (start_ch = *p) {\n    case 'X': break;\n    case 'r':\n      ch = *++p;\n      if (ch != 'p') {\n        p--;\n        if (op.mode != MIR_OP_VAR || op.u.var == R0_HARD_REG) return FALSE;\n      } else {\n        if (op.mode != MIR_OP_VAR || op.u.var == R0_HARD_REG || op.u.var == SP_HARD_REG)\n          return FALSE;\n      }\n      break;\n    case 'h': {\n      uint64_t num;\n      p++;\n      num = read_dec (&p);\n      if (op.mode != MIR_OP_VAR || op.u.var != num) return FALSE;\n      break;\n    }\n    case 'C':\n      if (op.mode != MIR_OP_VAR || !compressed_reg_p (op.u.var, FALSE)) return FALSE;\n      break;\n    case 'c': {\n      uint64_t num;\n      p++;\n      num = read_dec (&p);\n      if ((op.mode != MIR_OP_INT && op.mode != MIR_OP_UINT) || op.u.u != num) return FALSE;\n      break;\n    }\n    case 'm': {\n      MIR_type_t type, type2, type3 = MIR_T_BOUND;\n      int scale, u_p, s_p, compressed_p = FALSE;\n\n      if (op.mode != MIR_OP_VAR_MEM) return FALSE;\n      u_p = s_p = TRUE;\n      ch = *++p;\n      if (ch == 'c') {\n        compressed_p = TRUE;\n        ch = *++p;\n      }\n      switch (ch) {\n      case 'f':\n        gen_assert (!compressed_p);\n        type = MIR_T_F;\n        type2 = MIR_T_BOUND;\n        scale = 4;\n        break;\n      case 'd':\n        type = MIR_T_D;\n        type2 = MIR_T_BOUND;\n        scale = 8;\n        break;\n      case 'l':\n        ch = *++p;\n        gen_assert (ch == 'd' && !compressed_p);\n        type = MIR_T_LD;\n        type2 = MIR_T_BOUND;\n        scale = 16;\n        break;\n      case 'u':\n        gen_assert (!compressed_p);\n        /* fall through */\n      case 's':\n        u_p = ch == 'u';\n        s_p = ch == 's';\n        ch = *++p;\n        /* fall through */\n      default:\n        gen_assert ('0' <= ch && ch <= '3');\n        gen_assert (!compressed_p || '2' <= ch);\n        scale = 1 << (ch - '0');\n        if (ch == '0') {\n          type = u_p ? MIR_T_U8 : MIR_T_I8;\n          type2 = u_p && s_p ? MIR_T_I8 : MIR_T_BOUND;\n        } else if (ch == '1') {\n          type = u_p ? MIR_T_U16 : MIR_T_I16;\n          type2 = u_p && s_p ? MIR_T_I16 : MIR_T_BOUND;\n        } else if (ch == '2') {\n          type = u_p ? MIR_T_U32 : MIR_T_I32;\n          type2 = u_p && s_p ? MIR_T_I32 : MIR_T_BOUND;\n#if MIR_PTR32\n          if (u_p) type3 = MIR_T_P;\n#endif\n        } else {\n          type = u_p ? MIR_T_U64 : MIR_T_I64;\n          type2 = u_p && s_p ? MIR_T_I64 : MIR_T_BOUND;\n#if MIR_PTR64\n          type3 = MIR_T_P;\n#endif\n        }\n      }\n      if (op.u.var_mem.type != type && op.u.var_mem.type != type2 && op.u.var_mem.type != type3)\n        return FALSE;\n      if (op.u.var_mem.index != MIR_NON_VAR || op.u.var_mem.disp < -(1 << 11)\n          || op.u.var_mem.disp >= (1 << 11)\n          || (type == MIR_T_LD && op.u.var_mem.disp + 8 >= (1 << 11)))\n        return FALSE;\n      if (compressed_p) {\n        if (op.u.var_mem.disp < 0 || op.u.var_mem.disp % scale != 0) return FALSE;\n        ch = *++p;\n        if (ch == 's') {\n          if (op.u.var_mem.base != SP_HARD_REG) return FALSE;\n          if (op.u.var_mem.disp / scale >= (1 << 6)) return FALSE;\n        } else {\n          p--;\n          if (!compressed_reg_p (op.u.var_mem.base, TRUE)) return FALSE;\n          if (op.u.var_mem.disp / scale >= (1 << 5)) return FALSE;\n        }\n      }\n      break;\n    }\n    case 'i': {\n      ch = *++p;\n      if (op.mode != MIR_OP_INT && op.mode != MIR_OP_UINT && (ch != 'a' || op.mode != MIR_OP_REF))\n        return FALSE;\n      if ((ch == 'u' || ch == 'a') && (op.mode == MIR_OP_INT || op.mode == MIR_OP_UINT)) {\n        assert (nop == 1);\n        if (op.u.i < -(1l << 31) || op.u.i >= (1l << 31)) return FALSE;\n        if (ch == 'u' && (op.u.i & 0xfff) != 0) return FALSE;\n      } else if (ch == 'a' && op.mode == MIR_OP_REF) {\n        int64_t v;\n\n        if (op.u.ref->item_type == MIR_data_item && op.u.ref->u.data->name != NULL\n            && _MIR_reserved_ref_name_p (ctx, op.u.ref->u.data->name)) {\n          v = (int64_t) op.u.ref->u.data->u.els;\n        } else {\n          v = (int64_t) op.u.ref->addr;\n        }\n        if (v < -(1l << 31) || v >= (1l << 31)) return FALSE;\n      } else {\n        assert (nop == 1 || nop == 2);\n        p--;\n        if (op.u.i < -(1 << 11) || op.u.i >= (1 << 11)) return FALSE;\n      }\n      break;\n    }\n    case 'j':\n      if (op.mode != MIR_OP_INT && op.mode != MIR_OP_UINT) return FALSE;\n      int64_t i = op.u.i;\n      ch = *++p;\n      if (ch == 'u') {\n        assert (nop == 1);\n        i = (i + 15) / 16 * 16;\n        ch = *++p;\n        if (ch != 's') {\n          p--;\n          if (i <= -(1 << 11) || i >= (1 << 11)) return FALSE;\n        } else {\n          if (i == 0 || i <= -(1 << 9) || i >= (1 << 9)) return FALSE;\n        }\n      } else {\n        p--;\n        assert (nop == 2);\n        if (i <= -(1 << 11) || i >= (1 << 11)) return FALSE;\n      }\n      break;\n    case 'I': {\n      if (op.mode != MIR_OP_INT && op.mode != MIR_OP_UINT && op.mode != MIR_OP_REF) return FALSE;\n      break;\n    }\n    case 's':\n    case 'S': {\n      assert (nop == 2);\n      if (op.mode != MIR_OP_INT && op.mode != MIR_OP_UINT) return FALSE;\n      if (op.u.i < 0 || (start_ch == 's' && op.u.i > 31) || (start_ch == 'S' && op.u.i > 63))\n        return FALSE;\n      if (start_ch == 'S') {\n        ch = *++p;\n        if (ch != 'p') {\n          p--;\n        } else {\n          if (op.u.i == 0) return FALSE;\n        }\n      }\n      break;\n    }\n    case 'k':\n      if (op.mode != MIR_OP_INT && op.mode != MIR_OP_UINT) return FALSE;\n      assert (nop == 1 || nop == 2);\n      ch = *++p;\n      if (ch == 'p') {\n        if (op.u.i == 0 || op.u.i < -(1 << 5) || op.u.i >= (1 << 5)) return FALSE;\n      } else if (ch == 's') {\n        if (op.u.i == 0 || op.u.i % 16 != 0 || op.u.i < -(1 << 9) || op.u.i >= (1 << 9))\n          return FALSE;\n      } else if (ch == 'w') {\n        if (op.u.i <= 0 || op.u.i % 4 != 0 || op.u.i / 4 >= (1 << 8)) return FALSE;\n      } else if (ch != 'u') {\n        p--;\n        if (op.u.i < -(1 << 5) || op.u.i >= (1 << 5)) return FALSE;\n      } else {\n        if (op.u.i == 0 || (op.u.i & 0xfff) != 0 || (((int64_t) op.u.i << 46) >> 46) != op.u.i)\n          return FALSE;\n      }\n      break;\n    case 'l':\n    case 'L':\n    case 'U':\n      if (op.mode != MIR_OP_LABEL) return FALSE;\n      break;\n    case '0':\n    case '1':\n    case '2':\n      n = start_ch - '0';\n      gen_assert (n < (int) nop);\n      original = insn->ops[n];\n      mode = op.mode;\n      if (mode == MIR_OP_UINT) mode = MIR_OP_INT;\n      if (original.mode != mode && (original.mode != MIR_OP_UINT || mode != MIR_OP_INT))\n        return FALSE;\n      gen_assert (mode == MIR_OP_VAR || mode == MIR_OP_INT || mode == MIR_OP_FLOAT\n                  || mode == MIR_OP_DOUBLE || mode == MIR_OP_LDOUBLE || mode == MIR_OP_VAR_MEM\n                  || mode == MIR_OP_LABEL);\n      if (mode == MIR_OP_VAR && op.u.var != original.u.var)\n        return FALSE;\n      else if (mode == MIR_OP_INT && op.u.i != original.u.i)\n        return FALSE;\n      else if (mode == MIR_OP_FLOAT && op.u.f != original.u.f)\n        return FALSE;\n      else if (mode == MIR_OP_DOUBLE && op.u.d != original.u.d)\n        return FALSE;\n      else if (mode == MIR_OP_LDOUBLE && op.u.ld != original.u.ld)\n        return FALSE;\n      else if (mode == MIR_OP_LABEL && op.u.label != original.u.label)\n        return FALSE;\n      else if (mode == MIR_OP_VAR_MEM\n               && (op.u.var_mem.type != original.u.var_mem.type\n                   || op.u.var_mem.scale != original.u.var_mem.scale\n                   || op.u.var_mem.base != original.u.var_mem.base\n                   || op.u.var_mem.index != original.u.var_mem.index\n                   || op.u.var_mem.disp != original.u.var_mem.disp))\n        return FALSE;\n      break;\n    default: gen_assert (FALSE);\n    }\n  }\n  gen_assert (nop == nops);\n  return TRUE;\n}\n\nstatic const char *find_insn_pattern_replacement (gen_ctx_t gen_ctx, MIR_insn_t insn) {\n  int i;\n  const struct pattern *pat;\n  insn_pattern_info_t info = VARR_GET (insn_pattern_info_t, insn_pattern_info, insn->code);\n\n  for (i = 0; i < info.num; i++) {\n    pat = &patterns[VARR_GET (int, pattern_indexes, info.start + i)];\n    if (pattern_match_p (gen_ctx, pat, insn)) return pat->replacement;\n  }\n  return NULL;\n}\n\nstatic void patterns_finish (gen_ctx_t gen_ctx) {\n  VARR_DESTROY (int, pattern_indexes);\n  VARR_DESTROY (insn_pattern_info_t, insn_pattern_info);\n}\n\nstatic int hex_value (int ch) {\n  return ('0' <= ch && ch <= '9'   ? ch - '0'\n          : 'A' <= ch && ch <= 'F' ? ch - 'A' + 10\n          : 'a' <= ch && ch <= 'f' ? ch - 'a' + 10\n                                   : -1);\n}\n\nstatic uint64_t read_hex (const char **ptr) {\n  int v;\n  const char *p;\n  uint64_t res = 0;\n\n  for (p = *ptr; (v = hex_value (*p)) >= 0; p++) {\n    gen_assert ((res >> 60) == 0);\n    res = res * 16 + v;\n  }\n  gen_assert (p != *ptr);\n  *ptr = p - 1;\n  return res;\n}\n\nstatic void put_byte (struct gen_ctx *gen_ctx, int byte) { VARR_PUSH (uint8_t, result_code, byte); }\n\nstatic void put_uint64 (struct gen_ctx *gen_ctx, uint64_t v, int nb) { /* Little endian */\n  for (; nb > 0; nb--) {\n    put_byte (gen_ctx, v & 0xff);\n    v >>= 8;\n  }\n}\n\nstatic void set_int64 (uint8_t *addr, int64_t v, int nb) { /* Little endian */\n  for (; nb > 0; nb--) {\n    *addr++ = v & 0xff;\n    v >>= 8;\n  }\n}\n\nstatic int64_t get_int64 (uint8_t *addr, int nb) { /* Little endian */\n  int64_t v = 0;\n  int i, sh = (8 - nb) * 8;\n\n  for (i = nb - 1; i >= 0; i--) v = (v << 8) | addr[i];\n  if (sh > 0) v = (v << sh) >> sh; /* make it signed */\n  return v;\n}\n\nstatic uint32_t check_and_set_mask (uint32_t opcode_mask, uint32_t mask) {\n  gen_assert ((opcode_mask & mask) == 0);\n  return opcode_mask | mask;\n}\n\nstatic void out_insn (gen_ctx_t gen_ctx, MIR_insn_t insn, const char *replacement,\n                      void **jump_addrs) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  const char *p, *insn_str;\n  label_ref_t lr;\n  const_ref_t cr;\n  int switch_table_addr_p = FALSE;\n  size_t nops = MIR_insn_nops (ctx, insn);\n\n  if (insn->code == MIR_ALLOCA\n      && (insn->ops[1].mode == MIR_OP_INT || insn->ops[1].mode == MIR_OP_UINT))\n    insn->ops[1].u.u = (insn->ops[1].u.u + 15) & -16;\n  for (insn_str = replacement;; insn_str = p + 1) {\n    char ch, ch2, start_ch, d;\n    uint32_t insn32 = 0, insn_mask = 0, el_mask = 0;\n    int n, opcode = -1, funct3 = -1, funct7 = -1, rd = -1, rs1 = -1, rs2 = -1;\n    int opcodec = -1, funct3c = -1, funct4c = -1, funct6c = -1, funct2c = -1, funct2bc = -1;\n    int rs2m = -1, rdc = -1, rs2c = -1, uimm8c = -1;\n    int shamt = -1, shamtc = -1, imm12, imm20, imm6c = 0, st_disp = 0;\n    int unsign_disp4 = -1, unsign_disp8 = -1;\n    int unsign_sp_disp4 = -1, unsign_sp_disp8 = -1;\n    int unsign_sp_store_disp4 = -1, unsign_sp_store_disp8 = -1;\n    int imm12_p = FALSE, imm20_p = FALSE, imm6c_p = FALSE, st_disp_p = FALSE;\n    MIR_op_t op;\n    int label_ref_num = -1;\n\n    for (p = insn_str; (ch = *p) != '\\0' && ch != ';'; p++) {\n      if ((ch = *p) == 0 || ch == ';') break;\n      el_mask = 0;\n      switch ((start_ch = ch = *p)) {\n      case ' ':\n      case '\\t': break;\n      case 'O':\n        p++;\n        gen_assert (hex_value (*p) >= 0 && opcode < 0 && opcodec < 0);\n        opcode = read_hex (&p);\n        assert (opcode < (1 << 7));\n        el_mask = 0x3f;\n        break;\n      case 'F':\n        p++;\n        gen_assert (hex_value (*p) >= 0 && funct3 < 0);\n        funct3 = read_hex (&p);\n        assert (funct3 < (1 << 3));\n        el_mask = 0xf000;\n        break;\n      case 'f':\n        p++;\n        gen_assert (hex_value (*p) >= 0 && funct7 < 0);\n        funct7 = read_hex (&p);\n        assert (funct7 < (1 << 7));\n        el_mask = 0xfe000000;\n        break;\n      case 'g':\n        p++;\n        gen_assert (hex_value (*p) >= 0 && funct7 < 0);\n        funct7 = read_hex (&p);\n        assert (funct7 < (1 << 6));\n        el_mask = 0xfc000000;\n        break;\n      case 'o':\n        p++;\n        gen_assert (hex_value (*p) >= 0 && opcode < 0 && opcodec < 0);\n        opcodec = read_hex (&p);\n        assert (opcode < 4);\n        el_mask = 0x3;\n        break;\n      case 'a':\n        gen_assert (opcodec >= 0);\n        p++;\n        gen_assert (hex_value (*p) >= 0 && opcode < 0);\n        funct3c = read_hex (&p);\n        assert (funct3c < 8);\n        el_mask = 0xe000;\n        break;\n      case 'b':\n        gen_assert (opcodec >= 0);\n        p++;\n        gen_assert (hex_value (*p) >= 0 && opcode < 0);\n        funct4c = read_hex (&p);\n        assert (funct4c < 16);\n        el_mask = 0xf000;\n        break;\n      case 'c':\n        gen_assert (opcodec >= 0);\n        p++;\n        gen_assert (hex_value (*p) >= 0 && opcode < 0);\n        funct6c = read_hex (&p);\n        assert (funct6c < 64);\n        el_mask = 0xfc00;\n        break;\n      case 'd':\n      case 'e':\n        gen_assert (opcodec >= 0);\n        p++;\n        gen_assert (hex_value (*p) >= 0 && opcode < 0);\n        n = read_hex (&p);\n        assert (n < 4);\n        if (start_ch == 'd') {\n          funct2c = n;\n          el_mask = 0x60;\n        } else {\n          funct2bc = n;\n          el_mask = 0xc00;\n        }\n        break;\n      case 'r':\n      case 'h': {\n        int reg;\n        ch2 = *++p;\n        gen_assert (ch2 == 'd' || ch2 == 's' || ch2 == 'S' || ch2 == 't' || ch2 == 'u'\n                    || ch2 == 'v');\n        ch = *++p;\n        if (start_ch == 'h') {\n          reg = read_hex (&p);\n        } else {\n          gen_assert ('0' <= ch && ch <= '2' && ch - '0' < (int) nops);\n          op = insn->ops[ch - '0'];\n          gen_assert (op.mode == MIR_OP_VAR);\n          reg = op.u.var;\n        }\n        if (reg >= F0_HARD_REG) reg -= F0_HARD_REG;\n        gen_assert (reg <= 31);\n        if (ch2 == 'd') {\n          rd = reg;\n          el_mask = 0xf80;\n        } else if (ch2 == 's') {\n          rs1 = reg;\n          el_mask = 0xf8000;\n        } else if (ch2 == 'S') {\n          rs2 = reg;\n          el_mask = 0x1f00000;\n        } else if (ch2 == 't') {\n          rs2m = reg;\n          el_mask = 0x7c;\n        } else if (ch2 == 'u') {\n          gen_assert (compressed_reg_p (reg, FALSE));\n          rdc = reg - (reg <= R15_HARD_REG ? R8_HARD_REG : F8_HARD_REG);\n          el_mask = 0x380;\n        } else if (ch2 == 'v') {\n          gen_assert (compressed_reg_p (reg, FALSE));\n          rs2c = reg - (reg <= R15_HARD_REG ? R8_HARD_REG : F8_HARD_REG);\n          el_mask = 0x1c;\n        } else {\n          gen_assert (FALSE);\n        }\n        break;\n      }\n      case 'm':\n        ch = *++p;\n        if (ch == 'c') {\n          op = insn->ops[0];\n          if (op.mode == MIR_OP_VAR_MEM) { /* store */\n            gen_assert (insn->ops[1].mode == MIR_OP_VAR);\n          } else {\n            op = insn->ops[1];\n            gen_assert (op.mode == MIR_OP_VAR_MEM && insn->ops[0].mode == MIR_OP_VAR);\n          }\n          ch = *++p;\n          gen_assert (ch == '2' || ch == '3' || ch == 'd');\n          d = op.u.var_mem.disp >> (ch == '2' ? 2 : 3);\n          if (*++p == 's') {\n            gen_assert (d < (1 << 6) && op.u.var_mem.base == SP_HARD_REG);\n            if (*++p == 's') {\n              if (ch == '2') {\n                unsign_sp_store_disp4 = d;\n              } else {\n                unsign_sp_store_disp8 = d;\n              }\n              el_mask = 0x1f80;\n            } else {\n              p--;\n              if (ch == '2') {\n                unsign_sp_disp4 = d;\n              } else {\n                unsign_sp_disp8 = d;\n              }\n              el_mask = 0x107c;\n            }\n          } else {\n            gen_assert (compressed_reg_p (op.u.var_mem.base, TRUE));\n            rdc = op.u.var_mem.base - R8_HARD_REG;\n            p--;\n            gen_assert (d < (1 << 5));\n            if (ch == '2') {\n              unsign_disp4 = d;\n            } else {\n              unsign_disp8 = d;\n            }\n            el_mask = 0x1fe0;\n          }\n        } else {\n          if (ch == 's') { /* store */\n            gen_assert (insn->ops[0].mode == MIR_OP_VAR_MEM);\n            op = insn->ops[0];\n            st_disp = ((op.u.var_mem.disp << 13) & 0x01fc0000) | (op.u.var_mem.disp & 0x1f);\n            el_mask = 0xfe000f80;\n            st_disp_p = TRUE;\n          } else { /* load */\n            gen_assert (ch == 'l' && insn->ops[1].mode == MIR_OP_VAR_MEM);\n            op = insn->ops[1];\n            imm12 = op.u.var_mem.disp;\n            imm12_p = TRUE;\n            el_mask = 0xfff00000;\n          }\n          el_mask |= 0xf8000;\n          rs1 = op.u.var_mem.base;\n        }\n        break;\n      case 's':\n      case 'S':\n        el_mask = (start_ch == 's' ? 0x1f00000 : 0x3f00000);\n        ch = *++p;\n        if (hex_value (ch) >= 0) {\n          shamt = read_hex (&p);\n        } else if (start_ch == 'S' && ch == 'p') {\n          op = insn->ops[2];\n          gen_assert (op.mode == MIR_OP_INT || op.mode == MIR_OP_UINT);\n          shamtc = op.u.i;\n          el_mask = 0x107c;\n          gen_assert (shamtc > 0);\n        } else {\n          p--;\n          op = insn->ops[2];\n          gen_assert (op.mode == MIR_OP_INT || op.mode == MIR_OP_UINT);\n          shamt = op.u.i;\n          gen_assert (shamt >= 0);\n        }\n        break;\n      case 'i':\n        ch = *++p;\n        if (ch == '-' || hex_value (ch) >= 0) { /* i[-]<hex> */\n          int neg_p = FALSE;\n          if (ch == '-') {\n            ch = *++p;\n            neg_p = TRUE;\n          }\n          gen_assert (hex_value (ch) >= 0);\n          imm12 = read_hex (&p);\n          if (neg_p) imm12 = -imm12;\n          el_mask = 0xfff00000;\n          imm12_p = TRUE;\n        } else if (ch == 'h' || ch == 'l') {\n          int32_t v;\n          op = insn->ops[1];\n          if (op.mode != MIR_OP_REF) {\n            v = (int32_t) op.u.i;\n          } else if (op.u.ref->item_type == MIR_data_item && op.u.ref->u.data->name != NULL\n                     && _MIR_reserved_ref_name_p (ctx, op.u.ref->u.data->name)) {\n            v = (int32_t) (int64_t) op.u.ref->u.data->u.els;\n          } else {\n            v = (int32_t) (int64_t) op.u.ref->addr;\n          }\n          imm12 = (v << 20) >> 20;\n          el_mask = 0xfff00000;\n          imm12_p = TRUE;\n          if (ch == 'h') {\n            imm20 = (v - imm12) >> 12;\n            el_mask = 0xfffff000;\n            imm12_p = FALSE;\n            imm20_p = TRUE;\n          }\n        } else if (ch == 'u') {\n          ch = *++p;\n          if (hex_value (ch) >= 0) { /* iu<hex> */\n            imm20 = read_hex (&p);\n            el_mask = 0xfffff000;\n            imm20_p = TRUE;\n          } else { /* iu */\n            p--;\n            op = insn->ops[1];\n            gen_assert (op.mode == MIR_OP_INT || op.mode == MIR_OP_UINT);\n            gen_assert ((op.u.i & 0xfff) == 0);\n            imm20 = op.u.i >> 12;\n            el_mask = 0xfffff000;\n            imm20_p = TRUE;\n          }\n        } else { /* i */\n          p--;\n          imm12 = (nops > 2 && (insn->ops[2].mode == MIR_OP_INT || insn->ops[2].mode == MIR_OP_UINT)\n                     ? insn->ops[2].u.i\n                     : insn->ops[1].u.i);\n          imm12_p = TRUE;\n          el_mask = 0xfff00000;\n        }\n        break;\n      case 'j':\n        ch = *++p;\n        if (ch == 'u') { /* ju */\n          op = insn->ops[1];\n          gen_assert (op.mode == MIR_OP_INT || op.mode == MIR_OP_UINT);\n          ch = *++p;\n          if (ch != 's') {\n            p--;\n            imm12 = -(op.u.i + 15) / 16 * 16;\n            el_mask = 0xfff00000;\n            imm12_p = TRUE;\n          } else {\n            imm6c = -(op.u.i + 15) / 16;\n            imm6c = (imm6c & 0x20) | ((imm6c & 0x1) << 4) | ((imm6c & 0x4) << 1)\n                    | ((imm6c & 0x18) >> 2) | ((imm6c & 0x2) >> 1);\n            imm6c_p = TRUE;\n            el_mask = 0x107c;\n          }\n        } else { /* j */\n          p--;\n          op = insn->ops[2];\n          gen_assert (op.mode == MIR_OP_INT || op.mode == MIR_OP_UINT);\n          imm12 = -op.u.i;\n          el_mask = 0xfff00000;\n          imm12_p = TRUE;\n        }\n        break;\n      case 'I': {\n        op = insn->ops[1];\n        gen_assert (op.mode == MIR_OP_INT || op.mode == MIR_OP_UINT || op.mode == MIR_OP_REF);\n        if (op.mode != MIR_OP_REF) {\n          cr.val = op.u.u;\n        } else if (op.u.ref->item_type == MIR_data_item && op.u.ref->u.data->name != NULL\n                   && _MIR_reserved_ref_name_p (ctx, op.u.ref->u.data->name)) {\n          cr.val = (uint64_t) op.u.ref->u.data->u.els;\n        } else {\n          cr.val = (uint64_t) op.u.ref->addr;\n        }\n        cr.const_addr_disp = VARR_LENGTH (uint8_t, result_code);\n        VARR_PUSH (const_ref_t, const_refs, cr);\n        break;\n      }\n      case 'T': {\n        gen_assert (!switch_table_addr_p);\n        imm12 = VARR_LENGTH (uint8_t, result_code) % 8;\n        if (imm12 != 0) imm12 = 8 - imm12;\n        imm12 += 16;\n        el_mask = 0xfff00000;\n        imm12_p = TRUE;\n        switch_table_addr_p = TRUE;\n        break;\n      }\n      case 'k':\n        ch = *++p;\n        imm6c_p = TRUE;\n        el_mask = 0x107c;\n        if (ch == '-' || hex_value (ch) >= 0) { /* i[-]<hex> */\n          int neg_p = FALSE;\n          if (ch == '-') {\n            ch = *++p;\n            neg_p = TRUE;\n          }\n          gen_assert (hex_value (ch) >= 0);\n          imm6c = read_hex (&p);\n          gen_assert (imm6c != 0 && -32 < imm6c && imm6c < 32);\n          if (neg_p) imm6c = -imm6c;\n        } else {\n          imm6c = (nops > 2 && (insn->ops[2].mode == MIR_OP_INT || insn->ops[2].mode == MIR_OP_UINT)\n                     ? insn->ops[2].u.i\n                     : insn->ops[1].u.i);\n          if (ch == 'u') { /* ku */\n            imm6c >>= 12;\n          } else if (ch == 's') { /* ks */\n            imm6c >>= 4;\n            imm6c = (imm6c & 0x20) | ((imm6c & 0x1) << 4) | ((imm6c & 0x4) << 1)\n                    | ((imm6c & 0x18) >> 2) | ((imm6c & 0x2) >> 1);\n          } else if (ch == 'w') { /* kw */\n            uimm8c = imm6c >> 2;\n            uimm8c = ((uimm8c & 0xc) << 4) | ((uimm8c & 0xf0) >> 2) | ((uimm8c & 0x1) << 1)\n                     | ((uimm8c & 0x2) >> 1);\n            el_mask = 0x1fe0;\n            imm6c_p = FALSE;\n          } else {\n            p--;\n          }\n        }\n        break;\n      case 'l':\n      case 'L':\n      case 'U':\n        n = 0;\n        if (insn->code == MIR_CALL || insn->code == MIR_INLINE || insn->code == MIR_LADDR) n = 1;\n        op = insn->ops[n];\n        gen_assert (op.mode == MIR_OP_LABEL || op.mode == MIR_OP_REF);\n        lr.abs_addr_p = FALSE;\n        lr.branch_type = start_ch == 'l' ? BRANCH : start_ch == 'L' ? JAL : AUIPC;\n        lr.label_val_disp = 0;\n        if (jump_addrs == NULL)\n          lr.u.label = op.u.label;\n        else\n          lr.u.jump_addr = jump_addrs[0];\n        label_ref_num = VARR_LENGTH (label_ref_t, label_refs);\n        VARR_PUSH (label_ref_t, label_refs, lr);\n        el_mask = start_ch == 'l' ? 0xfe000f80 : 0xfffff000;\n        break;\n      default: gen_assert (FALSE);\n      }\n    }\n    if (opcode >= 0) insn32 |= opcode;\n    if (funct3 >= 0) insn32 |= (funct3 << 12);\n    if (funct7 >= 0) insn32 |= (funct7 << 25);\n    if (opcodec >= 0) insn32 |= opcodec;\n    if (funct3c >= 0) insn32 |= (funct3c << 13);\n    if (funct4c >= 0) insn32 |= (funct4c << 12);\n    if (funct6c >= 0) insn32 |= (funct6c << 10);\n    if (funct2c >= 0) insn32 |= (funct2c << 5);\n    if (funct2bc >= 0) insn32 |= (funct2bc << 10);\n    if (rd >= 0) {\n      gen_assert (rd <= 31);\n      insn32 |= rd << 7;\n    }\n    if (rs1 >= 0) {\n      gen_assert (rs1 <= 31);\n      insn32 |= rs1 << 15;\n    }\n    if (rs2 >= 0) {\n      gen_assert (rs2 <= 31);\n      insn32 |= rs2 << 20;\n    }\n    if (rs2m >= 0) {\n      gen_assert (rs2m <= 31);\n      insn32 |= rs2m << 2;\n    }\n    if (rdc >= 0) {\n      gen_assert (rdc <= 15);\n      insn32 |= rdc << 7;\n    }\n    if (rs2c >= 0) {\n      gen_assert (rs2c <= 15);\n      insn32 |= rs2c << 2;\n    }\n    if (shamt >= 0) insn32 |= shamt << 20;\n    if (shamtc >= 0) insn32 |= ((shamtc & 0x20) << 7) | ((shamtc & 0x1f) << 2);\n    if (imm12_p) insn32 |= imm12 << 20;\n    if (imm20_p) insn32 |= imm20 << 12;\n    if (imm6c_p) insn32 |= ((imm6c & 0x20) << 7) | ((imm6c & 0x1f) << 2);\n    if (uimm8c >= 0) insn32 |= uimm8c << 5;\n    if (st_disp_p) insn32 |= st_disp << 7;\n    if (unsign_disp4 >= 0)\n      insn32\n        |= ((unsign_disp4 & 0xe) << 9) | ((unsign_disp4 & 0x1) << 6) | ((unsign_disp4 & 0x10) << 1);\n    if (unsign_disp8 >= 0) insn32 |= ((unsign_disp8 & 0x7) << 10) | ((unsign_disp8 & 0x18) << 2);\n    if (unsign_sp_disp4 >= 0)\n      insn32 |= ((unsign_sp_disp4 & 0x8) << 9) | ((unsign_sp_disp4 & 0x7) << 4)\n                | ((unsign_sp_disp4 & 0x30) >> 2);\n    if (unsign_sp_disp8 >= 0)\n      insn32 |= ((unsign_sp_disp8 & 0x4) << 10) | ((unsign_sp_disp8 & 0x3) << 5)\n                | ((unsign_sp_disp8 & 0x38) >> 1);\n    if (unsign_sp_store_disp4 >= 0)\n      insn32 |= ((unsign_sp_store_disp4 & 0xf) << 9) | ((unsign_sp_store_disp4 & 0x30) << 3);\n    if (unsign_sp_store_disp8 >= 0)\n      insn32 |= ((unsign_sp_store_disp8 & 0x7) << 10) | ((unsign_sp_store_disp8 & 0x38) << 4);\n    insn_mask = check_and_set_mask (insn_mask, el_mask);\n    if (label_ref_num >= 0) VARR_ADDR (label_ref_t, label_refs)\n    [label_ref_num].label_val_disp = VARR_LENGTH (uint8_t, result_code);\n\n    if (opcode >= 0) {\n      put_uint64 (gen_ctx, insn32, 4); /* output the machine insn */\n    } else {\n      gen_assert ((insn32 & 0xffff0000) == 0 && (insn_mask & 0xffff0000) == 0);\n      put_uint64 (gen_ctx, insn32, 2); /* output the machine insn */\n    }\n    if (*p == 0) break;\n  }\n  if (!switch_table_addr_p) return;\n  gen_assert (insn->code == MIR_SWITCH);\n  if (VARR_LENGTH (uint8_t, result_code) % 8 != 0)\n    put_uint64 (gen_ctx, 0, 8 - VARR_LENGTH (uint8_t, result_code) % 8);\n  for (size_t i = 1; i < insn->nops; i++) {\n    gen_assert (insn->ops[i].mode == MIR_OP_LABEL);\n    lr.abs_addr_p = TRUE;\n    lr.label_val_disp = VARR_LENGTH (uint8_t, result_code);\n    if (jump_addrs == NULL)\n      lr.u.label = insn->ops[i].u.label;\n    else\n      lr.u.jump_addr = jump_addrs[i - 1];\n    VARR_PUSH (label_ref_t, label_refs, lr);\n    put_uint64 (gen_ctx, 0, 8);\n  }\n}\n\nstatic int target_memory_ok_p (gen_ctx_t gen_ctx MIR_UNUSED, MIR_op_t *op_ref) {\n  if (op_ref->mode != MIR_OP_VAR_MEM) return FALSE;\n  if (op_ref->u.var_mem.index == MIR_NON_VAR && op_ref->u.var_mem.disp >= -(1 << 11)\n      && op_ref->u.var_mem.disp < (1 << 11)\n      && (op_ref->u.var_mem.type != MIR_T_LD || op_ref->u.var_mem.disp + 8 < (1 << 11)))\n    return TRUE;\n  return FALSE;\n}\n\nstatic int target_insn_ok_p (gen_ctx_t gen_ctx, MIR_insn_t insn) {\n  return find_insn_pattern_replacement (gen_ctx, insn) != NULL;\n}\n\nstatic const uint32_t b_imm_mask = ((0x7f << 25) | (0x1f << 7));\nstatic uint32_t get_b_format_imm (int32_t offset) {\n  int d = offset >> 1; /* scale */\n  gen_assert (-(1 << 11) <= d && d < (1 << 11));\n  return ((((d >> 5) & 0x40) | ((d >> 4) & 0x3f)) << 25)\n         | ((((d & 0xf) << 1) | ((d >> 10) & 0x1)) << 7);\n}\n\nstatic void add_consts (gen_ctx_t gen_ctx) {\n  /* Setting up 64-bit const addresses */\n  for (size_t i = 0; i < VARR_LENGTH (const_ref_t, const_refs); i++) {\n    const_ref_t cr = VARR_GET (const_ref_t, const_refs, i);\n    uint32_t disp, carry;\n    gen_assert (VARR_LENGTH (uint8_t, result_code) > cr.const_addr_disp\n                && VARR_LENGTH (uint8_t, result_code) - cr.const_addr_disp < (1l << 31));\n    disp = (uint32_t) (VARR_LENGTH (uint8_t, result_code) - cr.const_addr_disp);\n    carry = (disp & 0x800) << 1;\n    *(uint32_t *) (&VARR_ADDR (uint8_t, result_code)[cr.const_addr_disp])\n      |= (disp + carry) & 0xfffff000;\n    *(uint32_t *) (&VARR_ADDR (uint8_t, result_code)[cr.const_addr_disp + 4]) |= disp << 20;\n    put_uint64 (gen_ctx, cr.val, 8);\n  }\n}\n\nstatic void target_split_insns (gen_ctx_t gen_ctx) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  MIR_insn_t insn, next_insn;\n  MIR_op_t op;\n\n  for (insn = DLIST_HEAD (MIR_insn_t, curr_func_item->u.func->insns); insn != NULL;\n       insn = next_insn) {\n    next_insn = DLIST_NEXT (MIR_insn_t, insn);\n    if (insn->code != MIR_LDMOV) continue;\n\n    if (insn->ops[0].mode == MIR_OP_VAR) {\n      gen_assert (insn->ops[0].u.var + 1 < F0_HARD_REG && insn->ops[1].mode == MIR_OP_VAR_MEM);\n      op = insn->ops[1];\n      op.u.var_mem.type = MIR_T_I64;\n      next_insn = gen_mov (gen_ctx, insn, MIR_MOV, insn->ops[0], op);\n      op.u.var_mem.disp += 8;\n      gen_mov (gen_ctx, insn, MIR_MOV, _MIR_new_var_op (ctx, insn->ops[0].u.var + 1), op);\n      gen_delete_insn (gen_ctx, insn);\n    } else if (insn->ops[1].mode == MIR_OP_VAR) {\n      gen_assert (insn->ops[1].u.var + 1 < F0_HARD_REG && insn->ops[0].mode == MIR_OP_VAR_MEM);\n      op = insn->ops[0];\n      op.u.var_mem.type = MIR_T_I64;\n      next_insn = gen_mov (gen_ctx, insn, MIR_MOV, op, insn->ops[1]);\n      op.u.var_mem.disp += 8;\n      gen_mov (gen_ctx, insn, MIR_MOV, op, _MIR_new_var_op (ctx, insn->ops[1].u.var + 1));\n      gen_delete_insn (gen_ctx, insn);\n    } else {\n      gen_assert (insn->ops[0].mode == MIR_OP_VAR_MEM && insn->ops[1].mode == MIR_OP_VAR_MEM);\n      op = insn->ops[1];\n      op.u.var_mem.type = MIR_T_D;\n      next_insn\n        = gen_mov (gen_ctx, insn, MIR_DMOV, _MIR_new_var_op (ctx, TEMP_DOUBLE_HARD_REG1), op);\n      op.u.var_mem.disp += 8;\n      gen_mov (gen_ctx, insn, MIR_DMOV, _MIR_new_var_op (ctx, TEMP_DOUBLE_HARD_REG2), op);\n      op = insn->ops[0];\n      op.u.var_mem.type = MIR_T_D;\n      gen_mov (gen_ctx, insn, MIR_DMOV, op, _MIR_new_var_op (ctx, TEMP_DOUBLE_HARD_REG1));\n      op.u.var_mem.disp += 8;\n      gen_mov (gen_ctx, insn, MIR_DMOV, op, _MIR_new_var_op (ctx, TEMP_DOUBLE_HARD_REG2));\n      gen_delete_insn (gen_ctx, insn);\n    }\n  }\n}\n\nstatic uint8_t *target_translate (gen_ctx_t gen_ctx, size_t *len) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  size_t i;\n  MIR_insn_t insn;\n  const char *replacement;\n\n  gen_assert (curr_func_item->item_type == MIR_func_item);\n  VARR_TRUNC (uint8_t, result_code, 0);\n  VARR_TRUNC (label_ref_t, label_refs, 0);\n  VARR_TRUNC (const_ref_t, const_refs, 0);\n  VARR_TRUNC (uint64_t, abs_address_locs, 0);\n  for (insn = DLIST_HEAD (MIR_insn_t, curr_func_item->u.func->insns); insn != NULL;\n       insn = DLIST_NEXT (MIR_insn_t, insn)) {\n    if (insn->code == MIR_LABEL) {\n      set_label_disp (gen_ctx, insn, VARR_LENGTH (uint8_t, result_code));\n    } else if (insn->code != MIR_USE) {\n      replacement = find_insn_pattern_replacement (gen_ctx, insn);\n      if (replacement == NULL) {\n        fprintf (stderr, \"fatal failure in matching insn:\");\n        MIR_output_insn (ctx, stderr, insn, curr_func_item->u.func, TRUE);\n        exit (1);\n      } else {\n        gen_assert (replacement != NULL);\n        out_insn (gen_ctx, insn, replacement, NULL);\n      }\n    }\n  }\n  /* Setting up labels */\n  for (i = 0; i < VARR_LENGTH (label_ref_t, label_refs); i++) {\n    label_ref_t lr = VARR_GET (label_ref_t, label_refs, i);\n\n    if (!lr.abs_addr_p) {\n      int64_t offset = (int64_t) get_label_disp (gen_ctx, lr.u.label) - (int64_t) lr.label_val_disp;\n      uint32_t bin_insn;\n      gen_assert ((offset & 0x1) == 0);\n      if (lr.branch_type == BRANCH && (offset < -(1 << 12) || offset >= (1 << 12))) {\n        /* BL:br L => BL:jmp NBL; ... NBL: br TL;jmp BL+4;TL:jmp L: */\n        bin_insn = *(uint32_t *) (VARR_ADDR (uint8_t, result_code) + lr.label_val_disp);\n        offset = (int64_t) VARR_LENGTH (uint8_t, result_code) - (int64_t) lr.label_val_disp;\n        *(uint32_t *) (VARR_ADDR (uint8_t, result_code) + lr.label_val_disp)\n          = 0x6f | get_j_format_imm (offset);\n        bin_insn |= get_b_format_imm (8);\n        put_uint64 (gen_ctx, bin_insn, 4);\n        offset = (int64_t) lr.label_val_disp - (int64_t) VARR_LENGTH (uint8_t, result_code) + 4;\n        bin_insn = 0x6f | get_j_format_imm (offset);\n        put_uint64 (gen_ctx, bin_insn, 4);\n        offset = (int64_t) get_label_disp (gen_ctx, lr.u.label)\n                 - (int64_t) VARR_LENGTH (uint8_t, result_code);\n        bin_insn = 0x6f | get_j_format_imm (offset);\n        put_uint64 (gen_ctx, bin_insn, 4);\n      } else if (lr.branch_type == AUIPC) {\n        int hi = offset >> 12, low = offset & 0xfff;\n        if ((low & 0x800) != 0) hi++;\n        *(uint32_t *) (VARR_ADDR (uint8_t, result_code) + lr.label_val_disp) |= hi << 12;\n        *(uint32_t *) (VARR_ADDR (uint8_t, result_code) + lr.label_val_disp + 4) |= low << 20;\n      } else {\n        gen_assert (lr.branch_type != AUIPC_JALR);\n        *(uint32_t *) (VARR_ADDR (uint8_t, result_code) + lr.label_val_disp)\n          |= (lr.branch_type == BRANCH ? get_b_format_imm (offset) : get_j_format_imm (offset));\n      }\n    } else {\n      set_int64 (&VARR_ADDR (uint8_t, result_code)[lr.label_val_disp],\n                 (int64_t) get_label_disp (gen_ctx, lr.u.label), 8);\n      VARR_PUSH (uint64_t, abs_address_locs, lr.label_val_disp);\n    }\n  }\n  while (VARR_LENGTH (uint8_t, result_code) % 8 != 0) /* Align the pool */\n    VARR_PUSH (uint8_t, result_code, 0);\n  add_consts (gen_ctx);\n  while (VARR_LENGTH (uint8_t, result_code) % 16 != 0) /* Align the pool */\n    VARR_PUSH (uint8_t, result_code, 0);\n  *len = VARR_LENGTH (uint8_t, result_code);\n  return VARR_ADDR (uint8_t, result_code);\n}\n\nstatic void target_rebase (gen_ctx_t gen_ctx, uint8_t *base) {\n  MIR_code_reloc_t reloc;\n\n  VARR_TRUNC (MIR_code_reloc_t, relocs, 0);\n  for (size_t i = 0; i < VARR_LENGTH (uint64_t, abs_address_locs); i++) {\n    reloc.offset = VARR_GET (uint64_t, abs_address_locs, i);\n    reloc.value = base + get_int64 (base + reloc.offset, 8);\n    VARR_PUSH (MIR_code_reloc_t, relocs, reloc);\n  }\n  _MIR_update_code_arr (gen_ctx->ctx, base, VARR_LENGTH (MIR_code_reloc_t, relocs),\n                        VARR_ADDR (MIR_code_reloc_t, relocs));\n  gen_setup_lrefs (gen_ctx, base);\n}\n\nstatic void target_change_to_direct_calls (MIR_context_t ctx MIR_UNUSED) {}\n\nstruct target_bb_version {\n  uint8_t *base;\n  label_ref_t branch_ref; /* label cand used for jump to this bb version */\n};\n\nstatic void target_init_bb_version_data (target_bb_version_t data) {\n  data->base = NULL; /* we don't know origin branch */\n}\n\nstatic void target_bb_translate_start (gen_ctx_t gen_ctx) {\n  add_nops = 0;\n  VARR_TRUNC (uint8_t, result_code, 0);\n  VARR_TRUNC (label_ref_t, label_refs, 0);\n  VARR_TRUNC (const_ref_t, const_refs, 0);\n  VARR_TRUNC (uint64_t, abs_address_locs, 0);\n}\n\nstatic void target_bb_insn_translate (gen_ctx_t gen_ctx, MIR_insn_t insn, void **jump_addrs) {\n  const char *replacement;\n\n  if (insn->code == MIR_LABEL) return;\n  replacement = find_insn_pattern_replacement (gen_ctx, insn);\n  gen_assert (replacement != NULL);\n  out_insn (gen_ctx, insn, replacement, jump_addrs);\n  if (MIR_branch_code_p (insn->code)) add_nops = insn->code == MIR_JMP ? 1 : 3;\n}\n\nstatic void target_output_jump (gen_ctx_t gen_ctx, void **jump_addrs) {\n  out_insn (gen_ctx, temp_jump, temp_jump_replacement, jump_addrs);\n  put_uint64 (gen_ctx, TARGET_NOP, 4); /* add space for transformation to auipc;jar */\n}\n\nstatic uint8_t *target_bb_translate_finish (gen_ctx_t gen_ctx, size_t *len) {\n  /* add nops for conversion jmp->lui+jalr and br->jmp|lui+jalr */\n  for (int i = 0; i < add_nops; i++) put_uint64 (gen_ctx, TARGET_NOP, 4);\n  while (VARR_LENGTH (uint8_t, result_code) % 8 != 0) /* Align the pool */\n    VARR_PUSH (uint8_t, result_code, 0);\n  add_consts (gen_ctx);\n  *len = VARR_LENGTH (uint8_t, result_code);\n  return VARR_ADDR (uint8_t, result_code);\n}\n\nstatic void setup_rel (gen_ctx_t gen_ctx, label_ref_t *lr, uint8_t *base, void *addr) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  int64_t offset = (int64_t) addr - (int64_t) (base + lr->label_val_disp);\n\n  gen_assert ((offset & 0x1) == 0);\n  /* check max 32-bit offset with possible branch conversion (see offset - 3): */\n  if (lr->abs_addr_p || !(-(1l << 31) <= (offset / 2 - 3) && offset / 2 < (1l << 31))) {\n    fprintf (stderr, \"too big offset (%lld) in setup_rel\", (long long) offset);\n    exit (1);\n  }\n  /* ??? thread safe: */\n  uint32_t *insn_ptr = (uint32_t *) (base + lr->label_val_disp), insn = *insn_ptr;\n  if (lr->branch_type == BRANCH) {\n    if (-(1 << 12) <= offset && offset < (1 << 12)) { /* a valid branch offset*/\n      insn = (insn & ~b_imm_mask) | get_b_format_imm (offset);\n    } else {\n      insn = (insn & ~b_imm_mask) | get_b_format_imm (12); /* skip next jump and nop */\n      _MIR_change_code (ctx, (uint8_t *) insn_ptr, (uint8_t *) &insn, 4);\n      insn_ptr += 3;\n      lr->branch_type = JAL;\n      lr->label_val_disp += 12;\n      offset -= 12;\n    }\n  }\n  if (lr->branch_type == JAL) {\n    if (-(1 << 20) <= offset && offset < (1 << 20)) { /* a valid jal offset*/\n      insn = 0x6f | get_j_format_imm (offset);\n    } else {\n      lr->branch_type = AUIPC_JALR;\n    }\n  }\n  if (lr->branch_type == AUIPC) {\n    int hi = offset >> 12, low = offset & 0xfff;\n    if ((low & 0x800) != 0) hi++;\n    insn |= hi << 12;\n    _MIR_change_code (ctx, (uint8_t *) insn_ptr, (uint8_t *) &insn, 4);\n    insn_ptr += 1;\n    insn = *insn_ptr | (low << 20);\n  } else if (lr->branch_type == AUIPC_JALR) {\n    uint32_t carry = (offset & 0x800) << 1;\n    insn = 0x17 | (TEMP_INT_HARD_REG1 << 7)\n           | (((uint32_t) offset + carry) & 0xfffff000); /* auipc t5 */\n    _MIR_change_code (ctx, (uint8_t *) insn_ptr, (uint8_t *) &insn, 4);\n    insn = 0x67 | (TEMP_INT_HARD_REG1 << 15) | ((offset & 0xfff) << 20); /* jr t5 */\n    insn_ptr += 1;\n  }\n  _MIR_change_code (ctx, (uint8_t *) insn_ptr, (uint8_t *) &insn, 4);\n}\n\nstatic void target_bb_rebase (gen_ctx_t gen_ctx, uint8_t *base) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  MIR_code_reloc_t reloc;\n\n  /* Setting up relative labels */\n  for (size_t i = 0; i < VARR_LENGTH (label_ref_t, label_refs); i++) {\n    label_ref_t lr = VARR_GET (label_ref_t, label_refs, i);\n    if (lr.abs_addr_p) {\n      _MIR_change_code (ctx, (uint8_t *) base + lr.label_val_disp, (uint8_t *) &lr.u.jump_addr, 8);\n    } else {\n      setup_rel (gen_ctx, &lr, base, lr.u.jump_addr);\n    }\n  }\n  VARR_TRUNC (MIR_code_reloc_t, relocs, 0);\n  for (size_t i = 0; i < VARR_LENGTH (uint64_t, abs_address_locs); i++) {\n    reloc.offset = VARR_GET (uint64_t, abs_address_locs, i);\n    reloc.value = base + get_int64 (base + reloc.offset, 8);\n    VARR_PUSH (MIR_code_reloc_t, relocs, reloc);\n  }\n  _MIR_update_code_arr (gen_ctx->ctx, base, VARR_LENGTH (MIR_code_reloc_t, relocs),\n                        VARR_ADDR (MIR_code_reloc_t, relocs));\n}\n\nstatic void target_setup_succ_bb_version_data (gen_ctx_t gen_ctx, uint8_t *base) {\n  if (VARR_LENGTH (label_ref_t, label_refs)\n      != VARR_LENGTH (target_bb_version_t, target_succ_bb_versions))\n    /* We can have more one possible branch from original insn\n       (e.g. SWITCH, FBNE).  If it is so, we will make jumps only\n       through BB thunk. */\n    return;\n  for (size_t i = 0; i < VARR_LENGTH (target_bb_version_t, target_succ_bb_versions); i++) {\n    target_bb_version_t data = VARR_GET (target_bb_version_t, target_succ_bb_versions, i);\n    if (data == NULL) continue;\n    data->branch_ref = VARR_GET (label_ref_t, label_refs, i);\n    data->base = base;\n  }\n}\n\nstatic void target_redirect_bb_origin_branch (gen_ctx_t gen_ctx, target_bb_version_t data,\n                                              void *addr) {\n  MIR_context_t ctx = gen_ctx->ctx;\n\n  if (data->base == NULL) return;\n  if (data->branch_ref.abs_addr_p) {\n    _MIR_change_code (ctx, (uint8_t *) data->base + data->branch_ref.label_val_disp,\n                      (uint8_t *) &addr, 8);\n  } else {\n    setup_rel (gen_ctx, &data->branch_ref, data->base, addr);\n  }\n  data->base = NULL;\n}\n\nstatic void target_init (gen_ctx_t gen_ctx) {\n  MIR_alloc_t alloc = gen_alloc (gen_ctx);\n  MIR_context_t ctx = gen_ctx->ctx;\n  check_hard_reg_alloc_order ();\n  gen_ctx->target_ctx = gen_malloc (gen_ctx, sizeof (struct target_ctx));\n  VARR_CREATE (uint8_t, result_code, alloc, 0);\n  VARR_CREATE (label_ref_t, label_refs, alloc, 0);\n  VARR_CREATE (const_ref_t, const_refs, alloc, 0);\n  VARR_CREATE (uint64_t, abs_address_locs, alloc, 0);\n  VARR_CREATE (MIR_code_reloc_t, relocs, alloc, 0);\n  MIR_type_t res = MIR_T_I64;\n  MIR_var_t args1[] = {{MIR_T_F, \"src\", 0}};\n  MIR_var_t args2[] = {{MIR_T_D, \"src\", 0}};\n  _MIR_register_unspec_insn (ctx, FMVXW_CODE, \"fmv.x.w\", 1, &res, 1, FALSE, args1);\n  _MIR_register_unspec_insn (ctx, FMVXD_CODE, \"fmv.x.d\", 1, &res, 1, FALSE, args2);\n  patterns_init (gen_ctx);\n  temp_jump = MIR_new_insn (ctx, MIR_JMP, MIR_new_label_op (ctx, NULL));\n  temp_jump_replacement = find_insn_pattern_replacement (gen_ctx, temp_jump);\n}\n\nstatic void target_finish (gen_ctx_t gen_ctx) {\n  MIR_alloc_t alloc = gen_alloc (gen_ctx);\n  patterns_finish (gen_ctx);\n  _MIR_free_insn (gen_ctx->ctx, temp_jump);\n  VARR_DESTROY (uint8_t, result_code);\n  VARR_DESTROY (label_ref_t, label_refs);\n  VARR_DESTROY (const_ref_t, const_refs);\n  VARR_DESTROY (uint64_t, abs_address_locs);\n  VARR_DESTROY (MIR_code_reloc_t, relocs);\n  MIR_free (alloc, gen_ctx->target_ctx);\n  gen_ctx->target_ctx = NULL;\n}\n"
        },
        {
          "name": "mir-gen-s390x.c",
          "type": "blob",
          "size": 100.2705078125,
          "content": "/* This file is a part of MIR project.\n   Copyright (C) 2020-2024 Vladimir Makarov <vmakarov.gcc@gmail.com>.\n*/\n\n// ??? More patterns (ult, ugt, ule, uge w/o branches, multi-insn combining).\n\nstatic void fancy_abort (int code) {\n  if (!code) abort ();\n}\n\n#undef gen_assert\n#define gen_assert(c) fancy_abort (c)\n\n#define TARGET_EXPAND_UADDO\n#define TARGET_EXPAND_UADDOS\n#define TARGET_EXPAND_MULO\n#define TARGET_EXPAND_MULOS\n#define TARGET_EXPAND_UMULO\n#define TARGET_EXPAND_UMULOS\n\n#include <limits.h>\n\n#include \"mir-s390x.h\"\n\nstatic inline MIR_reg_t target_nth_loc (MIR_reg_t loc, MIR_type_t type, int n) {\n  gen_assert (n == 0 || (type == MIR_T_LD && loc >= F0_HARD_REG && n == 1));\n  if (n == 0) return loc;\n  return loc >= F15_HARD_REG ? loc + 1 : loc + 2; /* coupled fp reg */\n}\n\nstatic inline int target_call_used_hard_reg_p (MIR_reg_t hard_reg, MIR_type_t type MIR_UNUSED) {\n  gen_assert (hard_reg <= MAX_HARD_REG);\n  return ((R0_HARD_REG <= hard_reg && hard_reg <= R5_HARD_REG) || hard_reg == R14_HARD_REG\n          || (F0_HARD_REG <= hard_reg && hard_reg <= F7_HARD_REG));\n}\n\n/* Stack layout (r15(sp) refers to the last reserved stack slot\n   address) from higher address to lower address memory:\n\n        +-> Back chain\n        |   area for saved f8-f15\n        |   Local and spill variable area of calling function\n        |   ld value area for passing args and returns\n        |   Parameter area passed to called function by memory (SP + 160)\n        |   Register save area for called function use:\n        |      f0, f2, f4, f6 (fp argument save area)          (SP + 128)\n        |      r6-r15 (other register save area)               (SP + 48)\n        |      r2-r5  (argument register save area)            (SP + 16)\n        |   Reserved for compiler                              (SP + 8)\nSP,R11->+-- Back chain (optional)                              (SP + 0)\n            Alloca area (after that new 160 bytes header should be created with new values)\n\nSP alignment is always 8.\nOriginally SP(r15) and FP (r11) are the same but r15 can be changed by alloca */\n\n#define S390X_STACK_HEADER_SIZE 160\n#define S390X_GP_REG_RSAVE_AREA_START 16\n#define S390X_FP_REG_ARG_SAVE_AREA_START 128\n\n/* s390x has 3-ops insns */\nstatic const MIR_insn_code_t target_io_dup_op_insn_codes[]\n  = {MIR_ADD,   MIR_ADDS,  MIR_FADD, MIR_DADD,  MIR_SUB,  MIR_SUBS, MIR_SUBO, MIR_SUBOS,\n     MIR_ADDO,  MIR_ADDOS, MIR_FSUB, MIR_DSUB,  MIR_MUL,  MIR_MULS, MIR_FMUL, MIR_DMUL,\n     MIR_DIV,   MIR_DIVS,  MIR_UDIV, MIR_UDIVS, MIR_FDIV, MIR_DDIV, MIR_MOD,  MIR_MODS,\n     MIR_UMOD,  MIR_UMODS, MIR_EQ,   MIR_EQS,   MIR_NE,   MIR_NES,  MIR_LSHS, MIR_RSHS,\n     MIR_URSHS, MIR_AND,   MIR_ANDS, MIR_OR,    MIR_ORS,  MIR_XOR,  MIR_XORS, MIR_INSN_BOUND};\n\nstatic MIR_insn_code_t get_ext_code (MIR_type_t type) {\n  switch (type) {\n  case MIR_T_I8: return MIR_EXT8;\n  case MIR_T_U8: return MIR_UEXT8;\n  case MIR_T_I16: return MIR_EXT16;\n  case MIR_T_U16: return MIR_UEXT16;\n  case MIR_T_I32: return MIR_EXT32;\n  case MIR_T_U32: return MIR_UEXT32;\n  default: return MIR_INVALID_INSN;\n  }\n}\n\nstruct insn_pattern_info {\n  int start, num;\n};\n\ntypedef struct insn_pattern_info insn_pattern_info_t;\nDEF_VARR (insn_pattern_info_t);\n\nstruct const_ref {\n  size_t insn_pc;      /* where rel32 address should be in code */\n  size_t next_insn_pc; /* displacement of the next insn */\n  size_t const_num;\n};\n\ntypedef struct const_ref const_ref_t;\nDEF_VARR (const_ref_t);\nstruct label_ref {\n  int abs_addr_p;\n  size_t label_val_disp;\n  union {\n    MIR_label_t label;\n    void *jump_addr; /* absolute addr for BBV */\n  } u;\n};\n\ntypedef struct label_ref label_ref_t;\nDEF_VARR (label_ref_t);\n\nstruct target_ctx {\n  unsigned char alloca_p, leaf_p, stack_param_p, switch_p;\n  size_t param_save_area_size, blk_ld_value_save_area_size;\n  MIR_insn_t temp_jump;\n  const char *temp_jump_replacement;\n  VARR (int) * pattern_indexes;\n  VARR (insn_pattern_info_t) * insn_pattern_info;\n  VARR (uint8_t) * result_code;\n  VARR (uint64_t) * const_pool;\n  VARR (const_ref_t) * const_refs;\n  VARR (label_ref_t) * label_refs;\n  VARR (uint64_t) * abs_address_locs;\n  VARR (MIR_code_reloc_t) * relocs;\n  VARR (uint64_t) * ld_addr_regs;\n};\n\n#define alloca_p gen_ctx->target_ctx->alloca_p\n#define leaf_p gen_ctx->target_ctx->leaf_p\n#define stack_param_p gen_ctx->target_ctx->stack_param_p\n#define switch_p gen_ctx->target_ctx->switch_p\n#define param_save_area_size gen_ctx->target_ctx->param_save_area_size\n#define blk_ld_value_save_area_size gen_ctx->target_ctx->blk_ld_value_save_area_size\n#define temp_jump gen_ctx->target_ctx->temp_jump\n#define temp_jump_replacement gen_ctx->target_ctx->temp_jump_replacement\n#define pattern_indexes gen_ctx->target_ctx->pattern_indexes\n#define insn_pattern_info gen_ctx->target_ctx->insn_pattern_info\n#define result_code gen_ctx->target_ctx->result_code\n#define const_pool gen_ctx->target_ctx->const_pool\n#define const_refs gen_ctx->target_ctx->const_refs\n#define label_refs gen_ctx->target_ctx->label_refs\n#define abs_address_locs gen_ctx->target_ctx->abs_address_locs\n#define relocs gen_ctx->target_ctx->relocs\n#define ld_addr_regs gen_ctx->target_ctx->ld_addr_regs\n\nstatic void gen_mov (gen_ctx_t gen_ctx, MIR_insn_t anchor, MIR_insn_code_t code, MIR_op_t dst_op,\n                     MIR_op_t src_op) {\n  gen_add_insn_before (gen_ctx, anchor, MIR_new_insn (gen_ctx->ctx, code, dst_op, src_op));\n}\n\nstatic void mir_blk_mov (uint64_t *to, uint64_t *from, uint64_t nwords) {\n  for (; nwords > 0; nwords--) *to++ = *from++;\n}\n\nstatic const char *BLK_MOV = \"mir.blk_mov\";\nstatic const char *BLK_MOV_P = \"mir.blk_mov.p\";\n\nstatic void gen_blk_mov (gen_ctx_t gen_ctx, MIR_insn_t anchor, size_t to_disp,\n                         MIR_reg_t to_base_hard_reg, size_t from_disp, MIR_reg_t from_base_reg,\n                         size_t qwords, int save_regs) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  MIR_func_t func = curr_func_item->u.func;\n  MIR_item_t proto_item, func_import_item;\n  MIR_insn_t new_insn;\n  MIR_op_t ops[5], freg_op, treg_op, treg_op2, treg_op3;\n\n  treg_op = _MIR_new_var_op (ctx, gen_new_temp_reg (gen_ctx, MIR_T_I64, func));\n  if (qwords <= 16) {\n    for (; qwords > 0; qwords--, to_disp += 8, from_disp += 8) {\n      gen_mov (gen_ctx, anchor, MIR_MOV, treg_op,\n               _MIR_new_var_mem_op (ctx, MIR_T_I64, from_disp, from_base_reg, MIR_NON_VAR, 1));\n      gen_mov (gen_ctx, anchor, MIR_MOV,\n               _MIR_new_var_mem_op (ctx, MIR_T_I64, to_disp, to_base_hard_reg, MIR_NON_VAR, 1),\n               treg_op);\n    }\n    return;\n  }\n  treg_op2 = _MIR_new_var_op (ctx, gen_new_temp_reg (gen_ctx, MIR_T_I64, func));\n  treg_op3 = _MIR_new_var_op (ctx, gen_new_temp_reg (gen_ctx, MIR_T_I64, func));\n  /* Save arg regs: */\n  if (save_regs > 0)\n    gen_mov (gen_ctx, anchor, MIR_MOV, treg_op, _MIR_new_var_op (ctx, R2_HARD_REG));\n  if (save_regs > 1)\n    gen_mov (gen_ctx, anchor, MIR_MOV, treg_op2, _MIR_new_var_op (ctx, R3_HARD_REG));\n  if (save_regs > 2)\n    gen_mov (gen_ctx, anchor, MIR_MOV, treg_op3, _MIR_new_var_op (ctx, R4_HARD_REG));\n  /* call blk move: */\n  proto_item = _MIR_builtin_proto (ctx, curr_func_item->module, BLK_MOV_P, 0, NULL, 3, MIR_T_I64,\n                                   \"to\", MIR_T_I64, \"from\", MIR_T_I64, \"nwords\");\n  func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, BLK_MOV, mir_blk_mov);\n  freg_op = _MIR_new_var_op (ctx, gen_new_temp_reg (gen_ctx, MIR_T_I64, func));\n  new_insn = MIR_new_insn (ctx, MIR_MOV, freg_op, MIR_new_ref_op (ctx, func_import_item));\n  gen_add_insn_before (gen_ctx, anchor, new_insn);\n  gen_add_insn_before (gen_ctx, anchor,\n                       MIR_new_insn (gen_ctx->ctx, MIR_ADD, _MIR_new_var_op (ctx, R2_HARD_REG),\n                                     _MIR_new_var_op (ctx, to_base_hard_reg),\n                                     MIR_new_int_op (ctx, to_disp)));\n  gen_add_insn_before (gen_ctx, anchor,\n                       MIR_new_insn (gen_ctx->ctx, MIR_ADD, _MIR_new_var_op (ctx, R3_HARD_REG),\n                                     _MIR_new_var_op (ctx, from_base_reg),\n                                     MIR_new_int_op (ctx, from_disp)));\n  gen_mov (gen_ctx, anchor, MIR_MOV, _MIR_new_var_op (ctx, R4_HARD_REG),\n           MIR_new_int_op (ctx, qwords));\n  ops[0] = MIR_new_ref_op (ctx, proto_item);\n  ops[1] = freg_op;\n  ops[2] = _MIR_new_var_op (ctx, R2_HARD_REG);\n  ops[3] = _MIR_new_var_op (ctx, R3_HARD_REG);\n  ops[4] = _MIR_new_var_op (ctx, R4_HARD_REG);\n  new_insn = MIR_new_insn_arr (ctx, MIR_CALL, 5, ops);\n  gen_add_insn_before (gen_ctx, anchor, new_insn);\n  /* Restore arg regs: */\n  if (save_regs > 0)\n    gen_mov (gen_ctx, anchor, MIR_MOV, _MIR_new_var_op (ctx, R2_HARD_REG), treg_op);\n  if (save_regs > 1)\n    gen_mov (gen_ctx, anchor, MIR_MOV, _MIR_new_var_op (ctx, R3_HARD_REG), treg_op2);\n  if (save_regs > 2)\n    gen_mov (gen_ctx, anchor, MIR_MOV, _MIR_new_var_op (ctx, R4_HARD_REG), treg_op3);\n}\n\nstatic void machinize_call (gen_ctx_t gen_ctx, MIR_insn_t call_insn) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  MIR_func_t func = curr_func_item->u.func;\n  MIR_proto_t proto = call_insn->ops[0].u.ref->u.proto;\n  int vararg_p = proto->vararg_p;\n  size_t nargs, nops = MIR_insn_nops (ctx, call_insn), start = proto->nres + 2;\n  size_t param_mem_size, call_blk_ld_value_area_size, ld_n_iregs, n_iregs, n_fregs;\n  size_t qwords, blk_ld_value_disp;\n  MIR_type_t type, mem_type;\n  MIR_op_mode_t mode;\n  MIR_var_t *arg_vars = NULL;\n  MIR_op_t arg_op, temp_op, arg_reg_op, ret_op, mem_op, ret_val_op, call_res_op;\n  MIR_insn_code_t new_insn_code, ext_code;\n  MIR_insn_t new_insn, ext_insn;\n\n  if (call_insn->code == MIR_INLINE) call_insn->code = MIR_CALL;\n  if (proto->args == NULL) {\n    nargs = 0;\n  } else {\n    gen_assert (nops >= VARR_LENGTH (MIR_var_t, proto->args)\n                && (vararg_p || nops - start == VARR_LENGTH (MIR_var_t, proto->args)));\n    nargs = VARR_LENGTH (MIR_var_t, proto->args);\n    arg_vars = VARR_ADDR (MIR_var_t, proto->args);\n  }\n  if (call_insn->ops[1].mode != MIR_OP_VAR) {\n    // ??? to optimize (can be immediate operand for func call)\n    temp_op = _MIR_new_var_op (ctx, gen_new_temp_reg (gen_ctx, MIR_T_I64, func));\n    new_insn = MIR_new_insn (ctx, MIR_MOV, temp_op, call_insn->ops[1]);\n    call_insn->ops[1] = temp_op;\n    gen_add_insn_before (gen_ctx, call_insn, new_insn);\n  }\n  n_iregs = n_fregs = param_mem_size = call_blk_ld_value_area_size = 0;\n  for (size_t i = 2; i < nops; i++) {\n    arg_op = call_insn->ops[i];\n    /* process long double results and ld and block args to calculate memory for them: */\n    if (i < start) {\n      type = proto->res_types[i - 2];\n    } else if (i - start < nargs) {\n      type = arg_vars[i - start].type;\n    } else if (arg_op.mode == MIR_OP_VAR_MEM) {\n      type = arg_op.u.mem.type;\n      gen_assert (MIR_all_blk_type_p (type));\n    } else {\n      mode = arg_op.value_mode;  // ??? smaller ints\n      gen_assert (mode == MIR_OP_INT || mode == MIR_OP_UINT || mode == MIR_OP_FLOAT\n                  || mode == MIR_OP_DOUBLE || mode == MIR_OP_LDOUBLE);\n      if (mode == MIR_OP_FLOAT)\n        (*MIR_get_error_func (ctx)) (MIR_call_op_error,\n                                     \"passing float variadic arg (should be passed as double)\");\n      type = mode == MIR_OP_DOUBLE ? MIR_T_D : mode == MIR_OP_LDOUBLE ? MIR_T_LD : MIR_T_I64;\n    }\n    if (type != MIR_T_LD && i < start) continue;\n    if (type == MIR_T_LD)\n      call_blk_ld_value_area_size += 16;\n    else if (MIR_blk_type_p (type)) {\n      gen_assert (arg_op.mode == MIR_OP_VAR_MEM && arg_op.u.mem.disp >= 0\n                  && arg_op.u.mem.index == MIR_NON_VAR);\n      call_blk_ld_value_area_size += (arg_op.u.mem.disp + 7) / 8 * 8;\n    }\n    if ((type == MIR_T_F || type == MIR_T_D) && n_fregs < 4) {\n      /* put arguments to argument hard regs: */\n      n_fregs++;\n    } else if (type != MIR_T_F && type != MIR_T_D && n_iregs < 5) { /* RBLK too */\n      n_iregs++;\n    } else { /* put arguments on the stack */\n      param_mem_size += 8;\n    }\n  }\n  if (param_save_area_size < param_mem_size) param_save_area_size = param_mem_size;\n  if (blk_ld_value_save_area_size < call_blk_ld_value_area_size)\n    blk_ld_value_save_area_size = call_blk_ld_value_area_size;\n  blk_ld_value_disp = param_save_area_size;\n  param_mem_size = n_fregs = n_iregs = 0;\n  for (size_t i = 2; i < nops; i++) { /* process args and ???long double results: */\n    arg_op = call_insn->ops[i];\n    gen_assert (arg_op.mode == MIR_OP_VAR\n                || (arg_op.mode == MIR_OP_VAR_MEM && MIR_all_blk_type_p (arg_op.u.mem.type)));\n    if (i < start) {\n      type = proto->res_types[i - 2];\n    } else if (i - start < nargs) {\n      type = arg_vars[i - start].type;\n    } else if (call_insn->ops[i].mode == MIR_OP_VAR_MEM) {\n      type = call_insn->ops[i].u.mem.type;\n      gen_assert (MIR_all_blk_type_p (type));\n    } else {\n      mode = call_insn->ops[i].value_mode;  // ??? smaller ints\n      gen_assert (mode == MIR_OP_INT || mode == MIR_OP_UINT || mode == MIR_OP_DOUBLE\n                  || mode == MIR_OP_LDOUBLE);\n      type = mode == MIR_OP_DOUBLE ? MIR_T_D : mode == MIR_OP_LDOUBLE ? MIR_T_LD : MIR_T_I64;\n    }\n    if (type != MIR_T_LD && i < start) continue;\n    ext_insn = NULL;\n    if ((ext_code = get_ext_code (type)) != MIR_INVALID_INSN) { /* extend arg if necessary */\n      temp_op = _MIR_new_var_op (ctx, gen_new_temp_reg (gen_ctx, MIR_T_I64, func));\n      ext_insn = MIR_new_insn (ctx, ext_code, temp_op, arg_op);\n      call_insn->ops[i] = arg_op = temp_op;\n    }\n    if (type == MIR_T_LD || MIR_blk_type_p (type)) {\n      if (i >= start) { /* put arg value in saved blk/ld value area: */\n        if (type == MIR_T_LD) {\n          mem_op = _MIR_new_var_mem_op (ctx, MIR_T_LD, blk_ld_value_disp + S390X_STACK_HEADER_SIZE,\n                                        SP_HARD_REG, MIR_NON_VAR, 1);\n          gen_mov (gen_ctx, call_insn, MIR_LDMOV, mem_op, arg_op);\n        } else {\n          qwords = (arg_op.u.mem.disp + 7) / 8;\n          gen_blk_mov (gen_ctx, call_insn, S390X_STACK_HEADER_SIZE + blk_ld_value_disp, SP_HARD_REG,\n                       0, arg_op.u.mem.base, qwords, n_iregs);\n        }\n      }\n      arg_op = _MIR_new_var_op (ctx, gen_new_temp_reg (gen_ctx, MIR_T_I64, func));\n      new_insn = MIR_new_insn (ctx, MIR_ADD, arg_op, _MIR_new_var_op (ctx, SP_HARD_REG),\n                               MIR_new_int_op (ctx, S390X_STACK_HEADER_SIZE + blk_ld_value_disp));\n      gen_add_insn_before (gen_ctx, call_insn, new_insn);\n      blk_ld_value_disp += type == MIR_T_LD ? 16 : qwords * 8;\n    }\n    mem_type = type == MIR_T_F || type == MIR_T_D ? type : MIR_T_I64;\n    if ((type == MIR_T_F || type == MIR_T_D) && n_fregs < 4) {\n      /* put arguments to argument hard regs: */\n      if (ext_insn != NULL) gen_add_insn_before (gen_ctx, call_insn, ext_insn);\n      arg_reg_op = _MIR_new_var_op (ctx, F0_HARD_REG + n_fregs * 2);\n      gen_mov (gen_ctx, call_insn, type == MIR_T_F ? MIR_FMOV : MIR_DMOV, arg_reg_op, arg_op);\n      call_insn->ops[i] = arg_reg_op;\n      n_fregs++;\n    } else if (type != MIR_T_F && type != MIR_T_D && n_iregs < 5) {\n      if (ext_insn != NULL) gen_add_insn_before (gen_ctx, call_insn, ext_insn);\n      arg_reg_op = _MIR_new_var_op (ctx, R2_HARD_REG + n_iregs);\n      if (type != MIR_T_RBLK) {\n        gen_mov (gen_ctx, call_insn, MIR_MOV, arg_reg_op, arg_op);\n      } else {\n        assert (arg_op.mode == MIR_OP_VAR_MEM);\n        gen_mov (gen_ctx, call_insn, MIR_MOV, arg_reg_op, _MIR_new_var_op (ctx, arg_op.u.mem.base));\n        arg_reg_op = _MIR_new_var_mem_op (ctx, MIR_T_RBLK, arg_op.u.mem.disp, R2_HARD_REG + n_iregs,\n                                          MIR_NON_VAR, 1);\n      }\n      if (i >= start) call_insn->ops[i] = arg_reg_op; /* don't change LD return yet */\n      n_iregs++;\n    } else { /* put arguments on the stack: */\n      if (ext_insn != NULL) gen_add_insn_before (gen_ctx, call_insn, ext_insn);\n      new_insn_code = (type == MIR_T_F ? MIR_FMOV : type == MIR_T_D ? MIR_DMOV : MIR_MOV);\n      mem_op = _MIR_new_var_mem_op (ctx, mem_type, param_mem_size + S390X_STACK_HEADER_SIZE,\n                                    SP_HARD_REG, MIR_NON_VAR, 1);\n      if (type != MIR_T_RBLK) {\n        gen_mov (gen_ctx, call_insn, new_insn_code, mem_op, arg_op);\n      } else {\n        assert (arg_op.mode == MIR_OP_VAR_MEM);\n        gen_mov (gen_ctx, call_insn, new_insn_code, mem_op,\n                 _MIR_new_var_op (ctx, arg_op.u.mem.base));\n      }\n      if (i >= start) call_insn->ops[i] = mem_op;\n      param_mem_size += 8;\n    }\n  }\n  ld_n_iregs = n_iregs = n_fregs = 0;\n  blk_ld_value_disp = param_mem_size;\n  for (size_t i = 0; i < proto->nres; i++) {\n    ret_op = call_insn->ops[i + 2];\n    gen_assert (ret_op.mode == MIR_OP_VAR);\n    type = proto->res_types[i];\n    if (type == MIR_T_LD) { /* returned by address */\n      new_insn_code = MIR_LDMOV;\n      call_res_op = ret_val_op\n        = _MIR_new_var_mem_op (ctx, MIR_T_LD, S390X_STACK_HEADER_SIZE + blk_ld_value_disp,\n                               SP_HARD_REG, MIR_NON_VAR, 1);\n      if (n_iregs < 5) { /* use it as a call result to keep assignment to ld_n_iregs: */\n        call_res_op\n          = _MIR_new_var_mem_op (ctx, MIR_T_LD, 0, R2_HARD_REG + ld_n_iregs, MIR_NON_VAR, 1);\n        ld_n_iregs++;\n      }\n      blk_ld_value_disp += 16;\n    } else if ((type == MIR_T_F || type == MIR_T_D) && n_fregs < 4) {\n      new_insn_code = type == MIR_T_F ? MIR_FMOV : MIR_DMOV;\n      call_res_op = ret_val_op = _MIR_new_var_op (ctx, F0_HARD_REG + n_fregs * 2);\n      n_fregs++;\n    } else if (type != MIR_T_F && type != MIR_T_D && n_iregs < 1) {\n      new_insn_code = MIR_MOV;\n      call_res_op = ret_val_op = _MIR_new_var_op (ctx, R2_HARD_REG + n_iregs);\n      n_iregs++;\n    } else {\n      (*MIR_get_error_func (ctx)) (MIR_ret_error,\n                                   \"s390x can not handle this combination of return values\");\n    }\n    new_insn = MIR_new_insn (ctx, new_insn_code, ret_op, ret_val_op);\n    MIR_insert_insn_after (ctx, curr_func_item, call_insn, new_insn);\n    call_insn->ops[i + 2] = call_res_op;\n    if ((ext_code = get_ext_code (type)) != MIR_INVALID_INSN) {\n      MIR_insert_insn_after (ctx, curr_func_item, new_insn,\n                             MIR_new_insn (ctx, ext_code, ret_op, ret_op));\n      new_insn = DLIST_NEXT (MIR_insn_t, new_insn);\n    }\n    create_new_bb_insns (gen_ctx, call_insn, DLIST_NEXT (MIR_insn_t, new_insn), call_insn);\n  }\n}\n\n/* Long double insns are implemented through the following builtins: */\nstatic long double mir_i2ld (int64_t i) { return i; }\nstatic const char *I2LD = \"mir.i2ld\";\nstatic const char *I2LD_P = \"mir.i2ld.p\";\n\nstatic long double mir_ui2ld (uint64_t i) { return i; }\nstatic const char *UI2LD = \"mir.ui2ld\";\nstatic const char *UI2LD_P = \"mir.ui2ld.p\";\n\nstatic long double mir_f2ld (float f) { return f; }\nstatic const char *F2LD = \"mir.f2ld\";\nstatic const char *F2LD_P = \"mir.f2ld.p\";\n\nstatic long double mir_d2ld (double d) { return d; }\nstatic const char *D2LD = \"mir.d2ld\";\nstatic const char *D2LD_P = \"mir.d2ld.p\";\n\nstatic int64_t mir_ld2i (long double ld) { return ld; }\nstatic const char *LD2I = \"mir.ld2i\";\nstatic const char *LD2I_P = \"mir.ld2i.p\";\n\nstatic float mir_ld2f (long double ld) { return ld; }\nstatic const char *LD2F = \"mir.ld2f\";\nstatic const char *LD2F_P = \"mir.ld2f.p\";\n\nstatic double mir_ld2d (long double ld) { return ld; }\nstatic const char *LD2D = \"mir.ld2d\";\nstatic const char *LD2D_P = \"mir.ld2d.p\";\n\nstatic long double mir_ldadd (long double d1, long double d2) { return d1 + d2; }\nstatic const char *LDADD = \"mir.ldadd\";\nstatic const char *LDADD_P = \"mir.ldadd.p\";\n\nstatic long double mir_ldsub (long double d1, long double d2) { return d1 - d2; }\nstatic const char *LDSUB = \"mir.ldsub\";\nstatic const char *LDSUB_P = \"mir.ldsub.p\";\n\nstatic long double mir_ldmul (long double d1, long double d2) { return d1 * d2; }\nstatic const char *LDMUL = \"mir.ldmul\";\nstatic const char *LDMUL_P = \"mir.ldmul.p\";\n\nstatic long double mir_lddiv (long double d1, long double d2) { return d1 / d2; }\nstatic const char *LDDIV = \"mir.lddiv\";\nstatic const char *LDDIV_P = \"mir.lddiv.p\";\n\nstatic long double mir_ldneg (long double d) { return -d; }\nstatic const char *LDNEG = \"mir.ldneg\";\nstatic const char *LDNEG_P = \"mir.ldneg.p\";\n\nstatic const char *VA_ARG_P = \"mir.va_arg.p\";\nstatic const char *VA_ARG = \"mir.va_arg\";\nstatic const char *VA_BLOCK_ARG_P = \"mir.va_block_arg.p\";\nstatic const char *VA_BLOCK_ARG = \"mir.va_block_arg\";\n\nstatic int64_t mir_ldeq (long double d1, long double d2) { return d1 == d2; }\nstatic const char *LDEQ = \"mir.ldeq\";\nstatic const char *LDEQ_P = \"mir.ldeq.p\";\n\nstatic int64_t mir_ldne (long double d1, long double d2) { return d1 != d2; }\nstatic const char *LDNE = \"mir.ldne\";\nstatic const char *LDNE_P = \"mir.ldne.p\";\n\nstatic int64_t mir_ldlt (long double d1, long double d2) { return d1 < d2; }\nstatic const char *LDLT = \"mir.ldlt\";\nstatic const char *LDLT_P = \"mir.ldlt.p\";\n\nstatic int64_t mir_ldge (long double d1, long double d2) { return d1 >= d2; }\nstatic const char *LDGE = \"mir.ldge\";\nstatic const char *LDGE_P = \"mir.ldge.p\";\n\nstatic int64_t mir_ldgt (long double d1, long double d2) { return d1 > d2; }\nstatic const char *LDGT = \"mir.ldgt\";\nstatic const char *LDGT_P = \"mir.ldgt.p\";\n\nstatic int64_t mir_ldle (long double d1, long double d2) { return d1 <= d2; }\nstatic const char *LDLE = \"mir.ldle\";\nstatic const char *LDLE_P = \"mir.ldle.p\";\n\nstatic int get_builtin (gen_ctx_t gen_ctx, MIR_insn_code_t code, MIR_item_t *proto_item,\n                        MIR_item_t *func_import_item) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  MIR_type_t res_type;\n\n  *func_import_item = *proto_item = NULL; /* to remove uninitialized warning */\n  switch (code) {\n  case MIR_I2LD:\n    res_type = MIR_T_LD;\n    *proto_item\n      = _MIR_builtin_proto (ctx, curr_func_item->module, I2LD_P, 1, &res_type, 1, MIR_T_I64, \"v\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, I2LD, mir_i2ld);\n    return 1;\n  case MIR_UI2LD:\n    res_type = MIR_T_LD;\n    *proto_item\n      = _MIR_builtin_proto (ctx, curr_func_item->module, UI2LD_P, 1, &res_type, 1, MIR_T_I64, \"v\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, UI2LD, mir_ui2ld);\n    return 1;\n  case MIR_F2LD:\n    res_type = MIR_T_LD;\n    *proto_item\n      = _MIR_builtin_proto (ctx, curr_func_item->module, F2LD_P, 1, &res_type, 1, MIR_T_F, \"v\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, F2LD, mir_f2ld);\n    return 1;\n  case MIR_D2LD:\n    res_type = MIR_T_LD;\n    *proto_item\n      = _MIR_builtin_proto (ctx, curr_func_item->module, D2LD_P, 1, &res_type, 1, MIR_T_D, \"v\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, D2LD, mir_d2ld);\n    return 1;\n  case MIR_LD2I:\n    res_type = MIR_T_I64;\n    *proto_item\n      = _MIR_builtin_proto (ctx, curr_func_item->module, LD2I_P, 1, &res_type, 1, MIR_T_LD, \"v\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, LD2I, mir_ld2i);\n    return 1;\n  case MIR_LD2F:\n    res_type = MIR_T_F;\n    *proto_item\n      = _MIR_builtin_proto (ctx, curr_func_item->module, LD2F_P, 1, &res_type, 1, MIR_T_LD, \"v\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, LD2F, mir_ld2f);\n    return 1;\n  case MIR_LD2D:\n    res_type = MIR_T_D;\n    *proto_item\n      = _MIR_builtin_proto (ctx, curr_func_item->module, LD2D_P, 1, &res_type, 1, MIR_T_LD, \"v\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, LD2D, mir_ld2d);\n    return 1;\n  case MIR_LDADD:\n    res_type = MIR_T_LD;\n    *proto_item = _MIR_builtin_proto (ctx, curr_func_item->module, LDADD_P, 1, &res_type, 2,\n                                      MIR_T_LD, \"d1\", MIR_T_LD, \"d2\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, LDADD, mir_ldadd);\n    return 2;\n  case MIR_LDSUB:\n    res_type = MIR_T_LD;\n    *proto_item = _MIR_builtin_proto (ctx, curr_func_item->module, LDSUB_P, 1, &res_type, 2,\n                                      MIR_T_LD, \"d1\", MIR_T_LD, \"d2\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, LDSUB, mir_ldsub);\n    return 2;\n  case MIR_LDMUL:\n    res_type = MIR_T_LD;\n    *proto_item = _MIR_builtin_proto (ctx, curr_func_item->module, LDMUL_P, 1, &res_type, 2,\n                                      MIR_T_LD, \"d1\", MIR_T_LD, \"d2\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, LDMUL, mir_ldmul);\n    return 2;\n  case MIR_LDDIV:\n    res_type = MIR_T_LD;\n    *proto_item = _MIR_builtin_proto (ctx, curr_func_item->module, LDDIV_P, 1, &res_type, 2,\n                                      MIR_T_LD, \"d1\", MIR_T_LD, \"d2\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, LDDIV, mir_lddiv);\n    return 2;\n  case MIR_LDNEG:\n    res_type = MIR_T_LD;\n    *proto_item\n      = _MIR_builtin_proto (ctx, curr_func_item->module, LDNEG_P, 1, &res_type, 1, MIR_T_LD, \"d\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, LDNEG, mir_ldneg);\n    return 1;\n  case MIR_LDEQ:\n    res_type = MIR_T_I64;\n    *proto_item = _MIR_builtin_proto (ctx, curr_func_item->module, LDEQ_P, 1, &res_type, 2,\n                                      MIR_T_LD, \"d1\", MIR_T_LD, \"d2\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, LDEQ, mir_ldeq);\n    return 2;\n  case MIR_LDNE:\n    res_type = MIR_T_I64;\n    *proto_item = _MIR_builtin_proto (ctx, curr_func_item->module, LDNE_P, 1, &res_type, 2,\n                                      MIR_T_LD, \"d1\", MIR_T_LD, \"d2\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, LDNE, mir_ldne);\n    return 2;\n  case MIR_LDLT:\n    res_type = MIR_T_I64;\n    *proto_item = _MIR_builtin_proto (ctx, curr_func_item->module, LDLT_P, 1, &res_type, 2,\n                                      MIR_T_LD, \"d1\", MIR_T_LD, \"d2\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, LDLT, mir_ldlt);\n    return 2;\n  case MIR_LDGE:\n    res_type = MIR_T_I64;\n    *proto_item = _MIR_builtin_proto (ctx, curr_func_item->module, LDGE_P, 1, &res_type, 2,\n                                      MIR_T_LD, \"d1\", MIR_T_LD, \"d2\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, LDGE, mir_ldge);\n    return 2;\n  case MIR_LDGT:\n    res_type = MIR_T_I64;\n    *proto_item = _MIR_builtin_proto (ctx, curr_func_item->module, LDGT_P, 1, &res_type, 2,\n                                      MIR_T_LD, \"d1\", MIR_T_LD, \"d2\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, LDGT, mir_ldgt);\n    return 2;\n  case MIR_LDLE:\n    res_type = MIR_T_I64;\n    *proto_item = _MIR_builtin_proto (ctx, curr_func_item->module, LDLE_P, 1, &res_type, 2,\n                                      MIR_T_LD, \"d1\", MIR_T_LD, \"d2\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, LDLE, mir_ldle);\n    return 2;\n  case MIR_VA_ARG:\n    res_type = MIR_T_I64;\n    *proto_item = _MIR_builtin_proto (ctx, curr_func_item->module, VA_ARG_P, 1, &res_type, 2,\n                                      MIR_T_I64, \"va\", MIR_T_I64, \"type\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, VA_ARG, va_arg_builtin);\n    return 2;\n  case MIR_VA_BLOCK_ARG:\n    *proto_item\n      = _MIR_builtin_proto (ctx, curr_func_item->module, VA_BLOCK_ARG_P, 0, NULL, 4, MIR_T_I64,\n                            \"res\", MIR_T_I64, \"va\", MIR_T_I64, \"size\", MIR_T_I64, \"ncase\");\n    *func_import_item\n      = _MIR_builtin_func (ctx, curr_func_item->module, VA_BLOCK_ARG, va_block_arg_builtin);\n    return 4;\n  default: return 0;\n  }\n}\n\nstatic MIR_disp_t target_get_stack_slot_offset (gen_ctx_t gen_ctx, MIR_type_t type MIR_UNUSED,\n                                                MIR_reg_t slot) {\n  /* slot is 0, 1, ... */\n  return ((MIR_disp_t) slot * 8 + S390X_STACK_HEADER_SIZE + param_save_area_size\n          + blk_ld_value_save_area_size);\n}\n\nstatic void set_prev_sp_reg (gen_ctx_t gen_ctx, MIR_insn_t anchor, int *prev_sp_set_p,\n                             MIR_reg_t *prev_sp_reg) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  MIR_func_t func = curr_func_item->u.func;\n\n  if (!*prev_sp_set_p) {\n    *prev_sp_set_p = TRUE;\n    *prev_sp_reg = gen_new_temp_reg (gen_ctx, MIR_T_I64, func);\n    gen_mov (gen_ctx, anchor, MIR_MOV, _MIR_new_var_op (ctx, *prev_sp_reg),\n             _MIR_new_var_mem_op (ctx, MIR_T_I64, 0, SP_HARD_REG, MIR_NON_VAR, 1));\n  }\n}\n\nstatic MIR_reg_t target_get_stack_slot_base_reg (gen_ctx_t gen_ctx MIR_UNUSED) {\n  return FP_HARD_REG;\n}\n\nstatic int target_valid_mem_offset_p (gen_ctx_t gen_ctx MIR_UNUSED, MIR_type_t type MIR_UNUSED,\n                                      MIR_disp_t offset MIR_UNUSED) {\n  return TRUE;\n}\n\nstatic void target_machinize (gen_ctx_t gen_ctx) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  MIR_func_t func;\n  MIR_type_t type, res_type;\n  MIR_insn_code_t code, new_insn_code;\n  MIR_insn_t insn, next_insn, new_insn, anchor;\n  MIR_reg_t ret_reg, ld_addr_reg, prev_sp_reg;\n  MIR_op_t ret_reg_op, arg_reg_op, temp_op, arg_var_op;\n  int prev_sp_set_p = FALSE;\n  size_t i, int_arg_num = 0, fp_arg_num = 0, disp;\n\n  gen_assert (curr_func_item->item_type == MIR_func_item);\n  func = curr_func_item->u.func;\n  anchor = DLIST_HEAD (MIR_insn_t, func->insns);\n  disp = S390X_STACK_HEADER_SIZE; /* param area start in the caller frame */\n  VARR_TRUNC (uint64_t, ld_addr_regs, 0);\n  for (i = 0; i < func->nres; i++) { /* reserve regs/space for LD result addresses */\n    if (func->res_types[i] != MIR_T_LD) continue;\n    ld_addr_reg = gen_new_temp_reg (gen_ctx, MIR_T_I64, func);\n    VARR_PUSH (uint64_t, ld_addr_regs, ld_addr_reg);\n    if (int_arg_num < 5) {\n      gen_mov (gen_ctx, anchor, MIR_MOV, _MIR_new_var_op (ctx, ld_addr_reg),\n               _MIR_new_var_op (ctx, R2_HARD_REG + int_arg_num));\n      int_arg_num++;\n    } else {\n      set_prev_sp_reg (gen_ctx, anchor, &prev_sp_set_p, &prev_sp_reg);\n      gen_mov (gen_ctx, anchor, MIR_MOV, _MIR_new_var_op (ctx, ld_addr_reg),\n               _MIR_new_var_mem_op (ctx, MIR_T_I64, disp, prev_sp_reg, MIR_NON_VAR, 1));\n      disp += 8;\n    }\n  }\n  for (i = 0; i < func->nargs; i++) { /* Prologue: generate arg_var = hard_reg|stack mem ... */\n    /* Argument extensions is already done in simplify */\n    type = VARR_GET (MIR_var_t, func->vars, i).type;\n    arg_var_op = _MIR_new_var_op (ctx, i + MAX_HARD_REG + 1);\n    if ((type == MIR_T_F || type == MIR_T_D) && fp_arg_num < 4) {\n      arg_reg_op = _MIR_new_var_op (ctx, F0_HARD_REG + fp_arg_num * 2);\n      /* (f|d)mov arg, arg_hard_reg: */\n      gen_mov (gen_ctx, anchor, type == MIR_T_F ? MIR_FMOV : MIR_DMOV, arg_var_op, arg_reg_op);\n      fp_arg_num++;\n    } else if (type == MIR_T_F || type == MIR_T_D) { /* (f|d)mov arg, arg_memory */\n      set_prev_sp_reg (gen_ctx, anchor, &prev_sp_set_p, &prev_sp_reg);\n      gen_mov (gen_ctx, anchor, type == MIR_T_F ? MIR_FMOV : MIR_DMOV, arg_var_op,\n               _MIR_new_var_mem_op (ctx, type, disp + (type == MIR_T_F ? 4 : 0), prev_sp_reg,\n                                    MIR_NON_VAR, 1));\n      disp += 8;\n    } else if (int_arg_num < 5) { /* (ld)mov arg, arg_hard_reg */\n      if (type != MIR_T_LD)\n        gen_mov (gen_ctx, anchor, MIR_MOV, arg_var_op,\n                 _MIR_new_var_op (ctx, R2_HARD_REG + int_arg_num));\n      else\n        gen_mov (gen_ctx, anchor, MIR_LDMOV, arg_var_op,\n                 _MIR_new_var_mem_op (ctx, type, 0, R2_HARD_REG + int_arg_num, MIR_NON_VAR, 1));\n      int_arg_num++;\n    } else { /* (ld)mov arg, arg_memory */\n      set_prev_sp_reg (gen_ctx, anchor, &prev_sp_set_p, &prev_sp_reg);\n      if (type != MIR_T_LD) {\n        gen_mov (gen_ctx, anchor, MIR_MOV, arg_var_op,\n                 _MIR_new_var_mem_op (ctx, MIR_T_I64, disp, prev_sp_reg, MIR_NON_VAR, 1));\n      } else {\n        gen_mov (gen_ctx, anchor, MIR_MOV, _MIR_new_var_op (ctx, R1_HARD_REG),\n                 _MIR_new_var_mem_op (ctx, MIR_T_I64, disp, prev_sp_reg, MIR_NON_VAR, 1));\n        gen_mov (gen_ctx, anchor, MIR_MOV, arg_var_op,\n                 _MIR_new_var_mem_op (ctx, MIR_T_LD, 0, R1_HARD_REG, MIR_NON_VAR, 1));\n      }\n      disp += 8;\n    }\n  }\n  stack_param_p = disp != 0;\n  switch_p = alloca_p = FALSE;\n  leaf_p = TRUE;\n  param_save_area_size = blk_ld_value_save_area_size = 0;\n  for (insn = DLIST_HEAD (MIR_insn_t, func->insns); insn != NULL; insn = next_insn) {\n    MIR_item_t proto_item, func_import_item;\n    int nargs;\n\n    next_insn = DLIST_NEXT (MIR_insn_t, insn);\n    code = insn->code;\n    if (code == MIR_LDBEQ || code == MIR_LDBNE || code == MIR_LDBLT || code == MIR_LDBGE\n        || code == MIR_LDBGT || code == MIR_LDBLE) { /* split to cmp and branch */\n      temp_op = _MIR_new_var_op (ctx, gen_new_temp_reg (gen_ctx, MIR_T_I64, func));\n      code = (code == MIR_LDBEQ   ? MIR_LDEQ\n              : code == MIR_LDBNE ? MIR_LDNE\n              : code == MIR_LDBLT ? MIR_LDLT\n              : code == MIR_LDBGE ? MIR_LDGE\n              : code == MIR_LDBGT ? MIR_LDGT\n                                  : MIR_LDLE);\n      new_insn = MIR_new_insn (ctx, code, temp_op, insn->ops[1], insn->ops[2]);\n      gen_add_insn_before (gen_ctx, insn, new_insn);\n      next_insn = MIR_new_insn (ctx, MIR_BT, insn->ops[0], temp_op);\n      gen_add_insn_after (gen_ctx, new_insn, next_insn);\n      gen_delete_insn (gen_ctx, insn);\n      insn = new_insn;\n    }\n    if ((nargs = get_builtin (gen_ctx, code, &proto_item, &func_import_item)) > 0) {\n      if (code == MIR_VA_ARG || code == MIR_VA_BLOCK_ARG) {\n        /* Use a builtin func call:\n           mov func_reg, func ref; [mov reg3, type;] call proto, func_reg, res_reg, va_reg,\n           reg3 */\n        MIR_op_t ops[6], func_reg_op, reg_op3;\n        MIR_op_t res_reg_op = insn->ops[0], va_reg_op = insn->ops[1], op3 = insn->ops[2];\n\n        assert (res_reg_op.mode == MIR_OP_VAR && va_reg_op.mode == MIR_OP_VAR\n                && op3.mode == (code == MIR_VA_ARG ? MIR_OP_VAR_MEM : MIR_OP_VAR));\n        func_reg_op = _MIR_new_var_op (ctx, gen_new_temp_reg (gen_ctx, MIR_T_I64, func));\n        reg_op3 = _MIR_new_var_op (ctx, gen_new_temp_reg (gen_ctx, MIR_T_I64, func));\n        next_insn = new_insn\n          = MIR_new_insn (ctx, MIR_MOV, func_reg_op, MIR_new_ref_op (ctx, func_import_item));\n        gen_add_insn_before (gen_ctx, insn, new_insn);\n        if (code == MIR_VA_ARG) {\n          new_insn\n            = MIR_new_insn (ctx, MIR_MOV, reg_op3, MIR_new_int_op (ctx, (int64_t) op3.u.mem.type));\n          op3 = reg_op3;\n          gen_add_insn_before (gen_ctx, insn, new_insn);\n        }\n        ops[0] = MIR_new_ref_op (ctx, proto_item);\n        ops[1] = func_reg_op;\n        ops[2] = res_reg_op;\n        ops[3] = va_reg_op;\n        ops[4] = op3;\n        if (code == MIR_VA_BLOCK_ARG) ops[5] = insn->ops[3];\n        new_insn = MIR_new_insn_arr (ctx, MIR_CALL, code == MIR_VA_ARG ? 5 : 6, ops);\n        gen_add_insn_before (gen_ctx, insn, new_insn);\n        gen_delete_insn (gen_ctx, insn);\n      } else { /* Use builtin: mov freg, func ref; call proto, freg, res_reg, op_reg[, op_reg2] */\n        MIR_op_t freg_op, res_reg_op = insn->ops[0], op_reg_op = insn->ops[1], ops[5];\n\n        gen_assert (res_reg_op.mode == MIR_OP_VAR && op_reg_op.mode == MIR_OP_VAR);\n        freg_op = _MIR_new_var_op (ctx, gen_new_temp_reg (gen_ctx, MIR_T_I64, func));\n        next_insn = new_insn\n          = MIR_new_insn (ctx, MIR_MOV, freg_op, MIR_new_ref_op (ctx, func_import_item));\n        gen_add_insn_before (gen_ctx, insn, new_insn);\n        ops[0] = MIR_new_ref_op (ctx, proto_item);\n        ops[1] = freg_op;\n        ops[2] = res_reg_op;\n        ops[3] = op_reg_op;\n        if (nargs == 2) ops[4] = insn->ops[2];\n        new_insn = MIR_new_insn_arr (ctx, MIR_CALL, nargs + 3, ops);\n        gen_add_insn_before (gen_ctx, insn, new_insn);\n        gen_delete_insn (gen_ctx, insn);\n      }\n    } else if (code == MIR_VA_START) {\n      MIR_op_t treg_op\n        = _MIR_new_var_op (ctx, gen_new_temp_reg (gen_ctx, MIR_T_I64, curr_func_item->u.func));\n      MIR_op_t va_op = insn->ops[0];\n      MIR_reg_t va_reg;\n      int gpr_val = 0, fpr_val = 0;\n      MIR_var_t var;\n\n      disp = 0;\n      assert (func->vararg_p && (va_op.mode == MIR_OP_VAR));\n      for (i = 0; i < func->nargs; i++)\n        if (func->res_types[i] == MIR_T_LD) {\n          if (gpr_val > 5) disp += 8;\n          gpr_val++;\n        }\n      for (i = 0; i < func->nargs; i++) {\n        var = VARR_GET (MIR_var_t, func->vars, i);\n        if (var.type == MIR_T_F || var.type == MIR_T_D) {\n          if (fpr_val > 4) disp += 8;\n          fpr_val++;\n        } else {\n          if (gpr_val > 5) disp += 8;\n          gpr_val++;\n        }\n      }\n      va_reg = va_op.u.var;\n      /* Insns can be not simplified as soon as they match a machine insn.  */\n      /* mem64[va_reg] = gpr_val; mem64[va_reg + 8] = fpr_val */\n      gen_mov (gen_ctx, insn, MIR_MOV,\n               _MIR_new_var_mem_op (ctx, MIR_T_I64, 0, va_reg, MIR_NON_VAR, 1),\n               MIR_new_int_op (ctx, gpr_val));\n      next_insn = DLIST_PREV (MIR_insn_t, insn);\n      gen_mov (gen_ctx, insn, MIR_MOV,\n               _MIR_new_var_mem_op (ctx, MIR_T_I64, 8, va_reg, MIR_NON_VAR, 1),\n               MIR_new_int_op (ctx, fpr_val));\n      /* reg_save_area: treg = mem64[sp]; mem64[va_reg+24] = treg: */\n      gen_mov (gen_ctx, insn, MIR_MOV, treg_op,\n               _MIR_new_var_mem_op (ctx, MIR_T_I64, 0, SP_HARD_REG, MIR_NON_VAR, 1));\n      gen_mov (gen_ctx, insn, MIR_MOV,\n               _MIR_new_var_mem_op (ctx, MIR_T_I64, 24, va_reg, MIR_NON_VAR, 1), treg_op);\n      /* overflow_arg_area_reg: treg = treg_op+S390X_STACK_HEADER_SIZE + disp;\n         mem64[va_reg+16] = treg: */\n      new_insn = MIR_new_insn (ctx, MIR_ADD, treg_op, treg_op,\n                               MIR_new_int_op (ctx, S390X_STACK_HEADER_SIZE + disp));\n      gen_add_insn_before (gen_ctx, insn, new_insn);\n      gen_mov (gen_ctx, insn, MIR_MOV,\n               _MIR_new_var_mem_op (ctx, MIR_T_I64, 16, va_reg, MIR_NON_VAR, 1), treg_op);\n      gen_delete_insn (gen_ctx, insn);\n    } else if (code == MIR_VA_END) { /* do nothing */\n      gen_delete_insn (gen_ctx, insn);\n    } else if (MIR_call_code_p (code)) {\n      machinize_call (gen_ctx, insn);\n      leaf_p = FALSE;\n    } else if (code == MIR_ALLOCA) {\n      alloca_p = TRUE;\n    } else if (code == MIR_SWITCH) {\n      switch_p = TRUE;\n    } else if (code == MIR_RET) {\n      /* In simplify we already transformed code for one return insn and added extension insns.  */\n      uint32_t n_gpregs = 0, n_fregs = 0, ld_addr_n = 0;\n\n      gen_assert (func->nres == MIR_insn_nops (ctx, insn));\n      for (i = 0; i < func->nres; i++) {\n        gen_assert (insn->ops[i].mode == MIR_OP_VAR);\n        res_type = func->res_types[i];\n        if (res_type == MIR_T_LD) { /* ldmov f1,0(addr_reg);std f1,0(r2);std f3,8(r2): */\n          ld_addr_reg = VARR_GET (uint64_t, ld_addr_regs, ld_addr_n);\n          gen_mov (gen_ctx, insn, MIR_LDMOV, _MIR_new_var_op (ctx, F1_HARD_REG), insn->ops[i]);\n          insn->ops[i] = _MIR_new_var_mem_op (ctx, MIR_T_LD, 0, ld_addr_reg, MIR_NON_VAR, 1);\n          gen_mov (gen_ctx, insn, MIR_LDMOV, insn->ops[i], _MIR_new_var_op (ctx, F1_HARD_REG));\n          ld_addr_n++;\n          continue;\n        }\n        if ((res_type == MIR_T_F || res_type == MIR_T_D) && n_fregs < 4) {\n          new_insn_code = res_type == MIR_T_F ? MIR_FMOV : MIR_DMOV;\n          ret_reg = F0_HARD_REG + 2 * n_fregs;\n          n_fregs++;\n        } else if (n_gpregs < 1) {\n          ret_reg = R2_HARD_REG + n_gpregs++;\n          new_insn_code = MIR_MOV;\n        } else {\n          (*MIR_get_error_func (ctx)) (MIR_ret_error,\n                                       \"s390x can not handle this combination of return values\");\n        }\n        ret_reg_op = _MIR_new_var_op (ctx, ret_reg);\n        gen_mov (gen_ctx, insn, new_insn_code, ret_reg_op, insn->ops[i]);\n        insn->ops[i] = ret_reg_op;\n      }\n    }\n  }\n}\n\nstatic void isave (gen_ctx_t gen_ctx, MIR_insn_t anchor, int disp, MIR_reg_t hard_reg) {\n  gen_mov (gen_ctx, anchor, MIR_MOV,\n           _MIR_new_var_mem_op (gen_ctx->ctx, MIR_T_I64, disp, SP_HARD_REG, MIR_NON_VAR, 1),\n           _MIR_new_var_op (gen_ctx->ctx, hard_reg));\n}\n\nstatic void fsave (gen_ctx_t gen_ctx, MIR_insn_t anchor, int disp, MIR_reg_t hard_reg) {\n  gen_mov (gen_ctx, anchor, MIR_DMOV,\n           _MIR_new_var_mem_op (gen_ctx->ctx, MIR_T_D, disp, SP_HARD_REG, MIR_NON_VAR, 1),\n           _MIR_new_var_op (gen_ctx->ctx, hard_reg));\n}\n\nstatic void target_make_prolog_epilog (gen_ctx_t gen_ctx, bitmap_t used_hard_regs,\n                                       size_t stack_slots_num) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  MIR_func_t func;\n  MIR_insn_t anchor, new_insn;\n  MIR_op_t r15_reg_op, r14_reg_op, r11_reg_op, r0_reg_op;\n  int saved_regs_p = FALSE;\n  int64_t start_saved_fregs_offset;\n  size_t i, n, frame_size, saved_fregs_num;\n\n  gen_assert (curr_func_item->item_type == MIR_func_item);\n  func = curr_func_item->u.func;\n  anchor = DLIST_HEAD (MIR_insn_t, func->insns);\n  if (func->vararg_p) { /* save r2-r6,f0,f2,f4,f6: */\n    for (i = 0; i < 5; i++)\n      isave (gen_ctx, anchor, S390X_GP_REG_RSAVE_AREA_START + i * 8, i + R2_HARD_REG);\n    for (i = 0; i < 4; i++)\n      fsave (gen_ctx, anchor, S390X_FP_REG_ARG_SAVE_AREA_START + i * 8, i * 2 + F0_HARD_REG);\n  }\n  for (i = saved_fregs_num = 0; i <= MAX_HARD_REG; i++)\n    if (!target_call_used_hard_reg_p (i, MIR_T_UNDEF) && bitmap_bit_p (used_hard_regs, i)) {\n      saved_regs_p = TRUE;\n      if (i >= F0_HARD_REG) saved_fregs_num++;\n    }\n  if (leaf_p && !stack_param_p && !alloca_p && saved_regs_p == 0 && stack_slots_num == 0) return;\n  r0_reg_op = _MIR_new_var_op (ctx, R0_HARD_REG);\n  r11_reg_op = _MIR_new_var_op (ctx, R11_HARD_REG);\n  r14_reg_op = _MIR_new_var_op (ctx, R14_HARD_REG);\n  r15_reg_op = _MIR_new_var_op (ctx, R15_HARD_REG);\n  /* Prologue: */\n  frame_size = (param_save_area_size + S390X_STACK_HEADER_SIZE + blk_ld_value_save_area_size\n                + stack_slots_num * 8);\n  start_saved_fregs_offset = frame_size;\n  frame_size += saved_fregs_num * 8;\n  gen_assert (frame_size % 8 == 0);\n  if (!func->jret_p)\n    gen_mov (gen_ctx, anchor, MIR_MOV,\n             _MIR_new_var_mem_op (ctx, MIR_T_I64, S390X_GP_REG_RSAVE_AREA_START + (14 - 2) * 8,\n                                  R15_HARD_REG, MIR_NON_VAR, 1),\n             r14_reg_op); /* mem[r15+112] = r14 */\n  gen_mov (gen_ctx, anchor, MIR_MOV,\n           _MIR_new_var_mem_op (ctx, MIR_T_I64, S390X_GP_REG_RSAVE_AREA_START + (11 - 2) * 8,\n                                R15_HARD_REG, MIR_NON_VAR, 1),\n           r11_reg_op);                        /* mem[r15+76] = r11 */\n  for (i = R2_HARD_REG; i < R15_HARD_REG; i++) /* exclude r15 */\n    if (!target_call_used_hard_reg_p (i, MIR_T_UNDEF) && bitmap_bit_p (used_hard_regs, i)\n        && (i != 6 || !func->vararg_p))\n      isave (gen_ctx, anchor, S390X_GP_REG_RSAVE_AREA_START + (i - R2_HARD_REG) * 8, i);\n  gen_mov (gen_ctx, anchor, MIR_MOV, r0_reg_op, r15_reg_op); /* r0 = r15 */\n  new_insn = MIR_new_insn (ctx, MIR_ADD, r15_reg_op, r15_reg_op, MIR_new_int_op (ctx, -frame_size));\n  gen_add_insn_before (gen_ctx, anchor, new_insn); /* r15 -= frame_size */\n  gen_mov (gen_ctx, anchor, MIR_MOV,\n           _MIR_new_var_mem_op (ctx, MIR_T_I64, 0, R15_HARD_REG, MIR_NON_VAR, 1),\n           r0_reg_op); /* mem[r15] = r0 */\n  for (n = 0, i = F0_HARD_REG; i <= MAX_HARD_REG; i++)\n    if (!target_call_used_hard_reg_p (i, MIR_T_UNDEF) && bitmap_bit_p (used_hard_regs, i))\n      fsave (gen_ctx, anchor, start_saved_fregs_offset + (n++) * 8, i);\n  gen_mov (gen_ctx, anchor, MIR_MOV, r11_reg_op, r15_reg_op); /* r11 = r15 */\n  /* Epilogue: */\n  for (anchor = DLIST_TAIL (MIR_insn_t, func->insns); anchor != NULL;\n       anchor = DLIST_PREV (MIR_insn_t, anchor))\n    if (anchor->code == MIR_RET || anchor->code == MIR_JRET) break;\n  if (anchor == NULL) return;\n  /* Restoring fp hard registers: */\n  for (n = 0, i = F0_HARD_REG; i <= MAX_HARD_REG; i++)\n    if (!target_call_used_hard_reg_p (i, MIR_T_UNDEF) && bitmap_bit_p (used_hard_regs, i))\n      gen_mov (gen_ctx, anchor, MIR_DMOV, _MIR_new_var_op (ctx, i),\n               _MIR_new_var_mem_op (ctx, MIR_T_D, start_saved_fregs_offset + (n++) * 8,\n                                    R11_HARD_REG, MIR_NON_VAR, 1));\n  new_insn = MIR_new_insn (ctx, MIR_ADD, r15_reg_op, r11_reg_op, MIR_new_int_op (ctx, frame_size));\n  gen_add_insn_before (gen_ctx, anchor, new_insn); /* r15 = r11 + frame_size */\n  /* Restore saved gp regs (including r11 and excluding r15) and r14 */\n  for (i = R2_HARD_REG; i < R15_HARD_REG; i++)\n    if (!target_call_used_hard_reg_p (i, MIR_T_UNDEF) && bitmap_bit_p (used_hard_regs, i))\n      gen_mov (gen_ctx, anchor, MIR_MOV, _MIR_new_var_op (ctx, i),\n               _MIR_new_var_mem_op (ctx, MIR_T_I64,\n                                    S390X_GP_REG_RSAVE_AREA_START + (i - R2_HARD_REG) * 8,\n                                    SP_HARD_REG, MIR_NON_VAR, 1));\n  gen_mov (gen_ctx, anchor, MIR_MOV, r11_reg_op,\n           _MIR_new_var_mem_op (ctx, MIR_T_I64, S390X_GP_REG_RSAVE_AREA_START + (11 - 2) * 8,\n                                R15_HARD_REG, MIR_NON_VAR, 1)); /* restore r11 */\n  if (!func->jret_p)\n    gen_mov (gen_ctx, anchor, MIR_MOV, r14_reg_op,\n             _MIR_new_var_mem_op (ctx, MIR_T_I64, S390X_GP_REG_RSAVE_AREA_START + (14 - 2) * 8,\n                                  R15_HARD_REG, MIR_NON_VAR, 1)); /* restore r14 */\n}\n\nstruct pattern {\n  MIR_insn_code_t code;\n  /* Pattern elements:\n     blank - ignore\n     X - match everything\n     $ - finish successfully matching\n     r - register\n\n     h<one or two decimal digits> - hard register with given number\n\n        memory with unsigned 12-bit disp:\n     m[0-2] - int (signed or unsigned) memory of size 8,16,32,64-bits\n     m3 - 64-bit memory w/o index\n     ms[0-2] - signed int type memory of size 8,16,32,64-bits\n     mu[0-2] - unsigned int type memory of size 8,16,32,64-bits\n\n       memory with signed 20-bit disp:\n     M[0-3] - int (signed or unsigned) type memory of size 8,16,32,64-bits\n     Ms[0-2] - signed int type memory of size 8,16,32,64-bits\n     Mu[0-2] - unsigned int type memory of size 8,16,32,64-bits\n\n       memory with unsigned 12-bit disp:\n     mf - memory of float\n     md - memory of double\n     mld - memory of long double where disp + 8 is also in 12-bit range\n\n       memory with signed 20-bit disp:\n     Mf - memory of float\n     Md - memory of double\n     Mld - memory of long double where disp + 8 is also in 20-bit range\n\n     i - signed 16-bit immediate\n     I - any 64-bit immediate\n     ua - roundup unsigned 16-bit immediate\n     u[0-3] - 16-bit unsigned at pos 48,32,16,0 in 64-bit value\n     un[0-3] - 16-bit unsigned at pos 48,32,16,0 in 64-bit value and all ones in others\n\n     d - unsigned 12-bit immediate\n     D - signed 20-bit immediate\n\n     z - 0.0f immediate\n     Z - 0.0 immediate\n\n     L - reference or label which can be present by signed 32-bit pc word offset\n     [0-9] - an operand matching n-th operand (n should be less than given operand number)\n\n     Remember we have no float or (long) double immediate at this stage.  They were removed during\n     simplification.  */\n\n  const char *pattern;\n  /* Bit addressing: 0..63\n     Replacement elements:\n     blank - ignore\n     ; - insn separation\n\n     2hex* - opcode1 [0..7] (insn of format rr)\n     2hex - opcode1 [0..7] (insn of formats rx and rs)\n     4hex - opcode2 [0..15] (insn of formats rre and rrfe)\n     4hex* - opcode2 [0..15] (insn of sil)\n     2hex:2hex - opcode1 [0..7] and opcode12 [40..47] (insn of formats rxe, rxy, and rsy)\n     2hex:1hex - opcode1 [0..7] and opcode11 [12..15] (insn format ri)\n     2hex:1hex* - opcode1 [0..7] and opcode11 [12..15] (insn format ril)\n\n     s[0-2] - n-the operand reg as base reg [16..19]\n     x[0-2] - n-the operand reg as index reg [12..15]\n     hs<number>, hx<number> - base and index regs with given numbers\n     h<number> - hardware register with given number in r1\n     H<number> - hardware register with given number in r2\n     r[0-2] - r1 [8..11] or R1 [24..27] for 4hex opcode\n     R[0-2] - r2 [12..15] or R2 [28..31] for 4hex opcode\n     n[0-2] - r1/R1 with n-th reg + 2 from MIR insn\n\n     m = operand is (8-,16-,32-,64-bit) mem with base and index (0 reg means 0) and disp\n     mn = operand is (8-,16-,32-,64-bit) mem with base and index (0 reg means 0) and disp + 8\n     ma<number> - mask [8..11] (or [16..19] for 4hex opcode) with given number\n     md - 12-bit unsigned [20..31]\n     mD - 20-bit signed [20..39]: low part [20..31], high part [32..39]\n     md<number> - md with given number\n     L - label offset [16..47]\n     l<number> - label with given number [16..31]\n\n     i - 16 bit signed immediate [16..31]\n     u[0-3] - 16 bit unsigned immediate starting with position 48,32,16,0 in field [16..31]\n     j - 16 bit signed immediate [32..47]\n     i<number> - 16 bit signed immediate with given number\n     ua - roundup (i, 8)\n     Ia - pc-relative address of 64-bit immediate\n     sD<number> - displacement ([20..31]) used as shift\n     SD<number> - displacement (low part [20..31], high part [32..39]) used as shift\n     T - switch table displacement\n     Q - stack header + param_area + block param area\n  */\n  const char *replacement;\n};\n\n/* ??? movghi */\n/* Byte length: rr - 2, ri, rx, rs, rre, rrfe - 4, ril, rxe, rxy, rsy - 6 bytes */\n/* The longest insn is 48-bit */\nstatic const struct pattern patterns[] = {\n  {MIR_MOV, \"r r\", \"b904 r0 R1\"}, /* lgr r0,r1 */\n\n  {MIR_MOV, \"r M3\", \"e3:04 r0 m\"},  /* lg r0,m */\n  {MIR_MOV, \"r Ms2\", \"e3:14 r0 m\"}, /* lgf r0,m */\n  {MIR_MOV, \"r Mu2\", \"e3:16 r0 m\"}, /* llgf r0,m */\n\n  {MIR_MOV, \"r Ms0\", \"e3:77 r0 m\"}, /* lgb r0,m */\n  {MIR_MOV, \"r Mu0\", \"e3:90 r0 m\"}, /* llgc r0,m */\n\n  {MIR_MOV, \"r Ms1\", \"e3:15 r0 m\"}, /* lgh r0,m */\n  {MIR_MOV, \"r Mu1\", \"e3:91 r0 m\"}, /* llgh r0,m */\n\n  {MIR_MOV, \"M3 r\", \"e3:24 r1 m\"}, /* stg r0,m */\n  {MIR_MOV, \"m2 r\", \"50 r1 m\"},    /* st r0,m */\n  {MIR_MOV, \"M2 r\", \"e3:50 r1 m\"}, /* sty r0,m */\n\n  {MIR_MOV, \"m1 r\", \"40 r1 m\"},    /* sth r0,m */\n  {MIR_MOV, \"M1 r\", \"e3:70 r1 m\"}, /* sthy r0,m */\n\n  {MIR_MOV, \"m0 r\", \"42 r1 m\"},    /* stc r0,m */\n  {MIR_MOV, \"M0 r\", \"e3:72 r1 m\"}, /* stcy r0,m */\n\n  {MIR_MOV, \"r i\", \"a7:9 r0 i\"}, /* lghi r,i */\n\n  {MIR_MOV, \"m3 i\", \"e548* m j\"}, /* mvghi m,i */\n\n  {MIR_MOV, \"r u0\", \"a5:f r0 u0\"}, /* llill r,u */\n  {MIR_MOV, \"r u1\", \"a5:e r0 u1\"}, /* llilh r,u */\n  {MIR_MOV, \"r u2\", \"a5:d r0 u2\"}, /* llihl r,u */\n  {MIR_MOV, \"r u3\", \"a5:c r0 u3\"}, /* llihh r,u */\n\n  {MIR_MOV, \"r D\", \"e3:71 r0 mD\"},              /* lay r0,D */\n  {MIR_MOV, \"r I\", \"c0:0* r0 Ia; e3:04 r0 s0\"}, /* larl r,pc-relative addr; lg r,0(r) */\n\n  {MIR_FMOV, \"r r\", \"38* r0 R1\"}, /*  ler r,r */\n  {MIR_DMOV, \"r r\", \"28* r0 R1\"}, /*  ldr r,r */\n\n  {MIR_FMOV, \"r z\", \"b374 r0\"}, /*  lzer r,r */\n  {MIR_DMOV, \"r Z\", \"b375 r0\"}, /*  lzdr r,r */\n\n  {MIR_FMOV, \"r mf\", \"78 r0 m\"},    /*  le r,m */\n  {MIR_DMOV, \"r md\", \"68 r0 m\"},    /*  ld r,m */\n  {MIR_FMOV, \"r Mf\", \"ed:64 r0 m\"}, /*  ley r,m */\n  {MIR_DMOV, \"r Md\", \"ed:65 r0 m\"}, /*  ldy r,m */\n\n  {MIR_FMOV, \"mf r\", \"70 r1 m\"},    /* ste r,m */\n  {MIR_DMOV, \"md r\", \"60 r1 m\"},    /* std r,m */\n  {MIR_FMOV, \"Mf r\", \"ed:66 r1 m\"}, /* stey r,m */\n  {MIR_DMOV, \"Md r\", \"ed:67 r1 m\"}, /* stdy r,m */\n\n  {MIR_LDMOV, \"r r\", \"b365 r0 R1\"},                /* lxr r0,r1 */\n  {MIR_LDMOV, \"r mld\", \"68 r0 m; 68 n0 mn\"},       /* ld r0,m;ld r0+2,disp+8-m */\n  {MIR_LDMOV, \"r Mld\", \"ed:65 r0 m; ed:65 n0 mn\"}, /* ldy r0,m;ldy r0+2,disp+8-m */\n  {MIR_LDMOV, \"mld r\", \"60 r1 m; 60 n1 mn\"},       /* std r1,m;std r1+2,disp+8-m */\n  {MIR_LDMOV, \"Mld r\", \"ed:67 r1 m; ed:67 n1 mn\"}, /* stdy r1,m;stdy r1+2,disp+8-m */\n\n  /* sllg r0,r1,56; srag r0,r0,56: */\n  {MIR_EXT8, \"r r\", \"eb:0d r0 R1 SD56; eb:0a r0 R0 SD56\"},\n  /* sllg r0,r1,56; srlg r0,r0,56: */\n  {MIR_UEXT8, \"r r\", \"eb:0d r0 R1 SD56; eb:0c r0 R0 SD56\"},\n  {MIR_EXT8, \"r Ms0\", \"e3:77 r0 m\"},  /* lgb r0,m */\n  {MIR_UEXT8, \"r Mu0\", \"e3:90 r0 m\"}, /* llgc r0,m */\n\n  /* sllg r0,r1,48; srag r0,r0,48: */\n  {MIR_EXT16, \"r r\", \"eb:0d r0 R1 SD48; eb:0a r0 R0 SD48\"},\n  /* sllg r0,r1,48; srlg r0,r0,48: */\n  {MIR_UEXT16, \"r r\", \"eb:0d r0 R1 SD48; eb:0c r0 R0 SD48\"},\n  {MIR_EXT16, \"r Ms1\", \"e3:78 r0 m\"},  /* lhy r0,m */\n  {MIR_UEXT16, \"r Mu1\", \"e3:91 r0 m\"}, /* llgh r0,m */\n\n  {MIR_EXT32, \"r r\", \"b914 r0 R1\"},    /* lgfr r0,r1 */\n  {MIR_EXT32, \"r Ms2\", \"e3:14 r0 m\"},  /* lgf r0,m */\n  {MIR_UEXT32, \"r r\", \"b916 r0 R1\"},   /* llgfr r0,r1 */\n  {MIR_UEXT32, \"r Mu2\", \"e3:16 r0 m\"}, /* llgf r0,m */\n\n  {MIR_ADDS, \"r 0 r\", \"1a* r0 R2\"},   /* ar r0,r1 */\n  {MIR_ADDS, \"r 0 m2\", \"5a r0 m\"},    /* a r0,m */\n  {MIR_ADD, \"r 0 r\", \"b908 r0 R2\"},   /* agr r0,r1 */\n  {MIR_ADD, \"r 0 M2\", \"e3:5a r0 m\"},  /* ay r0,m */\n  {MIR_ADD, \"r 0 M3\", \"e3:08 r0 m\"},  /* ag r0,m */\n  {MIR_ADD, \"r 0 Ms2\", \"e3:18 r0 m\"}, /* agf r0,m */\n\n  {MIR_ADD, \"r r r\", \"41 r0 s1 x2\"},    /* la r0,(r1,r2) */\n  {MIR_ADD, \"r r d\", \"41 r0 s1 md\"},    /* la r0,d(r1) */\n  {MIR_ADD, \"r r D\", \"e3:71 r0 s1 mD\"}, /* lay r0,D(r1) */\n\n  {MIR_FADD, \"r 0 r\", \"b30a r0 R2\"},  /* aebr r0,r1*/\n  {MIR_DADD, \"r 0 r\", \"b31a r0 R2\"},  /* adbr r0,r1*/\n  {MIR_FADD, \"r 0 mf\", \"ed:0a r0 m\"}, /* aeb r,m*/\n  {MIR_DADD, \"r 0 md\", \"ed:1a r0 m\"}, /* adb r,m*/\n  // ldadd is implemented through builtin\n\n  {MIR_SUBS, \"r 0 r\", \"1b* r0 R2\"},   /* sr r0,r1 */\n  {MIR_SUBS, \"r 0 m2\", \"5b r0 m\"},    /* s r0,m */\n  {MIR_SUB, \"r 0 r\", \"b909 r0 R2\"},   /* sgr r0,r1 */\n  {MIR_SUB, \"r 0 M2\", \"e3:5b r0 m\"},  /* sy r0,m */\n  {MIR_SUB, \"r 0 M3\", \"e3:09 r0 m\"},  /* sg r0,m */\n  {MIR_SUB, \"r 0 Ms2\", \"e3:19 r0 m\"}, /* sgf r0,m */\n  // ??? changing sub imm to add imm\n\n  {MIR_ADDOS, \"r 0 r\", \"1a* r0 R2\"},   /* ar r0,r1 */\n  {MIR_ADDOS, \"r 0 m2\", \"5a r0 m\"},    /* a r0,m */\n  {MIR_ADDO, \"r 0 r\", \"b908 r0 R2\"},   /* agr r0,r1 */\n  {MIR_ADDO, \"r 0 M2\", \"e3:5a r0 m\"},  /* ay r0,m */\n  {MIR_ADDO, \"r 0 M3\", \"e3:08 r0 m\"},  /* ag r0,m */\n  {MIR_ADDO, \"r 0 Ms2\", \"e3:18 r0 m\"}, /* agf r0,m */\n\n  {MIR_SUBOS, \"r 0 r\", \"1b* r0 R2\"},   /* sr r0,r1 */\n  {MIR_SUBOS, \"r 0 m2\", \"5b r0 m\"},    /* s r0,m */\n  {MIR_SUBO, \"r 0 r\", \"b909 r0 R2\"},   /* sgr r0,r1 */\n  {MIR_SUBO, \"r 0 M2\", \"e3:5b r0 m\"},  /* sy r0,m */\n  {MIR_SUBO, \"r 0 M3\", \"e3:09 r0 m\"},  /* sg r0,m */\n  {MIR_SUBO, \"r 0 Ms2\", \"e3:19 r0 m\"}, /* sgf r0,m */\n\n  {MIR_FSUB, \"r 0 r\", \"b30b r0 R2\"},  /* sebr r0,r1*/\n  {MIR_DSUB, \"r 0 r\", \"b31b r0 R2\"},  /* sdbr r0,r1*/\n  {MIR_FSUB, \"r 0 mf\", \"ed:0b r0 m\"}, /* seb r,m*/\n  {MIR_DSUB, \"r 0 md\", \"ed:1b r0 m\"}, /* sdb r,m*/\n  // ldsub is implemented through builtin\n\n  {MIR_MULS, \"r 0 r\", \"b252 r0 R2\"},  /* msr r0,r1 */\n  {MIR_MULS, \"r 0 m2\", \"71 r0 m\"},    /* ms r0,m */\n  {MIR_MULS, \"r 0 M2\", \"e3:51 r0 m\"}, /* msy r0,m */\n  {MIR_MULS, \"r 0 i\", \"a7:c r0 i\"},   /* mhi r0,i */\n  {MIR_MUL, \"r 0 r\", \"b90c r0 R2\"},   /* msgr r0,r1 */\n  {MIR_MUL, \"r 0 M2\", \"71 r0 m\"},     /* msg r0,m */\n  {MIR_MUL, \"r 0 Ms2\", \"e3:1c r0 m\"}, /* msgf r0,m */\n  {MIR_MUL, \"r 0 i\", \"a7:d r0 i\"},    /* mghi r0,i */\n\n  {MIR_FMUL, \"r 0 r\", \"b317 r0 R2\"},  /* meebr r0,r1 */\n  {MIR_DMUL, \"r 0 r\", \"b31c r0 R2\"},  /* mdbr r0,r1 */\n  {MIR_FMUL, \"r 0 mf\", \"ed:17 r0 m\"}, /* meeb r,m*/\n  {MIR_DMUL, \"r 0 md\", \"ed:1c r0 m\"}, /* mdb r,m*/\n  // ldmul is implemented through builtin\n\n  {MIR_DIV, \"h1 0 r\", \"b90d h0 R2\"}, /* dsgr h0, r2 */\n  /* lgr h1,r0; dsgr h0,r2; lgr r0,h1: */\n  {MIR_DIV, \"r 0 r\", \"b904 h1 R0; b90d h0 R2; b904 r0 H1\"},\n  {MIR_DIV, \"h1 0 M3\", \"e3:0d h0 m\"}, /* dsg h0, m */\n  /* lgr h1,r0; dsg h0,m; lgr r0,h1: */\n  {MIR_DIV, \"r 0 M3\", \"b904 h1 R0; e3:0d h0 m; b904 r0 H1\"},\n  /* lgfr h1,r0; dsgfr h0,r2; lgfr r0,h1: */\n  {MIR_DIVS, \"r 0 r\", \"b914 h1 R0; b91d h0 R2; b914 r0 H1\"},\n  /* lgfr h1,r0; dsgf h0,m; lgfr r0,h1: */\n  {MIR_DIVS, \"r 0 M2\", \"b914 h1 R0; e3:1d h0 m; b914 r0 H1\"},\n\n  {MIR_UDIV, \"h1 0 r\", \"a5:f h0 i0; b987 h0 R2\"}, /* llill h,0; dlgr h0, r2 */\n  /* llill h,0; lgr h1,r0; dlgr h0,r2; lgr r0,h1: */\n  {MIR_UDIV, \"r 0 r\", \"a5:f h0 i0; b904 h1 R0; b987 h0 R2; b904 r0 H1\"},\n  {MIR_UDIV, \"h1 0 M3\", \"a5:f h0 i0; e3:87 h0 m\"}, /* llill h,0; dlg h0, m */\n  /* llill h,0; lgr h1,r0; dlg h0,m; lgr r0,h1: */\n  {MIR_UDIV, \"r 0 M3\", \"a5:f h0 i0; b904 h1 R0; e3:87 h0 m; b904 r0 H1\"},\n  /* llill h,0; llgfr h1,r0; dlr h0,r2; llgfr r0,h1: */\n  {MIR_UDIVS, \"r 0 r\", \"a5:f h0 i0; b916 h1 R0; b997 h0 R2; b916 r0 H1\"},\n  /* llill h,0; llgfr h1,r0; dl h0,m; llgfr r0,h1: */\n  {MIR_UDIVS, \"r 0 M2\", \"a5:f h0 i0; b916 h1 R0; e3:97 h0 m; b916 r0 H1\"},\n\n  {MIR_FDIV, \"r 0 r\", \"b30d r0 R2\"},  /* debr r0,r1 */\n  {MIR_DDIV, \"r 0 r\", \"b31d r0 R2\"},  /* ddbr r0,r1 */\n  {MIR_FDIV, \"r 0 mf\", \"ed:0d r0 m\"}, /* deb r,m*/\n  {MIR_DDIV, \"r 0 md\", \"ed:1d r0 m\"}, /* ddb r,m*/\n  // lddiv is implemented through builtin\n\n  {MIR_MOD, \"h1 0 r\", \"b90d h0 R2; b904 r0 H0\"}, /* dsgr h0, r2; lgr r0, h0 */\n  /* lgr h1,r0; dsgr h0,r2; lgr r0,h0: */\n  {MIR_MOD, \"r 0 r\", \"b904 h1 R0; b90d h0 R2; b904 r0 H0\"},\n  {MIR_MOD, \"h1 0 M3\", \"e3:0d h0 m; b904 r0 H0\"}, /* dsg h0, m; lgr, h0 */\n  /* lgr h1,r0; dsg h0,m; lgr r0,h0: */\n  {MIR_MOD, \"r 0 M3\", \"b904 h1 R0; e3:0d h0 m; b904 r0 H0\"},\n  /* lgfr h1,r0; dsgfr h0,r2; lgfr r0,h0: */\n  {MIR_MODS, \"r 0 r\", \"b914 h1 R0; b91d h0 R2; b914 r0 H0\"},\n  /* lgfr h1,r0; dsgf h0,m; lgfr r0,h0: */\n  {MIR_MODS, \"r 0 M2\", \"b914 h1 R0; e3:1d h0 m; b914 r0 H0\"},\n\n  /* llill h,0; dlgr h0, r2; lgr r0, h0 */\n  {MIR_UMOD, \"h1 0 r\", \"a5:f h0 i0; b987 h0 R2; b904 r0 H0\"},\n  /* llill h,0; lgr h1,r0; dlgr h0,r2; lgr r0,h0: */\n  {MIR_UMOD, \"r 0 r\", \"a5:f h0 i0; b904 h1 R0; b987 h0 R2; b904 r0 H0\"},\n  /* llill h,0; dlg h0, m; lgr r0, h0: */\n  {MIR_UMOD, \"h1 0 M3\", \"a5:f h0 i0; e3:87 h0 m; b904 r0 H0\"},\n  /* llill h,0; lgr h1,r0; dlg h0,m; lgr r0,h0: */\n  {MIR_UMOD, \"r 0 M3\", \"a5:f h0 i0; b904 h1 R0; e3:87 h0 m; b904 r0 H0\"},\n  /* llill h,0; llgfr h1,r0; dlr h0,r2; llgfr r0,h0: */\n  {MIR_UMODS, \"r 0 r\", \"a5:f h0 i0; b916 h1 R0; b997 h0 R2; b916 r0 H0\"},\n  /* llill h,0; llgfr h1,r0; dl h0,m; llgfr r0,h0: */\n  {MIR_UMODS, \"r 0 M2\", \"a5:f h0 i0; b916 h1 R0; e3:97 h0 m; b916 r0 H0\"},\n// all ld insn are changed to builtins\n\n/* lghi r0,1; jmask<m> L; lghi r0,0 */\n#define CMPEND(m) \"; a7:9 r0 i1; a7:4 ma\" #m \" l8; a7:9 r0 i0\"\n\n  /* (xgr r0,r2 | xg r0,m); lpgr r0,r0; aghi r0,-1; srlg r0,r0,63: */\n  {MIR_EQ, \"r 0 r\", \"b982 r0 R2; b900 r0 R0; a7:b r0 i65535; eb:0c r0 R0 SD63\"},\n  {MIR_EQ, \"r 0 M3\", \"e3:82 r0 m; b900 r0 R0; a7:b r0 i65535; eb:0c r0 R0 SD63\"},\n  /* (xr r0,r2 | x r0,m | xy r0, m); lpr r0,r0; ahi r0,-1; srl r0,r0,31: */\n  {MIR_EQS, \"r 0 r\", \"17* r0 R2; 10* r0 R0; a7:a r0 i65535; 88 r0 R0 Sd31\"},\n  {MIR_EQS, \"r 0 m2\", \"57 r0 m; 10* r0 R0; a7:a r0 i65535; 88 r0 R0 Sd31\"},\n  {MIR_EQS, \"r 0 M2\", \"e3:57 r0 m; 10* r0 R0; a7:a r0 i65535; 88 r0 R0 Sd31\"},\n  /* (cer r1,r2 | ce r1, mf); lghi r0,1; je L; lghi r0,0: */\n  {MIR_FEQ, \"r r r\", \"b309 r1 R2\" CMPEND (8)},\n  {MIR_FEQ, \"r r mf\", \"ed:09 r1 m\" CMPEND (8)},\n  /* (cdbr r1,r2 | cdb r1, mf); lghi r0,1; je L; lghi r0,0: */\n  {MIR_DEQ, \"r r r\", \"b319 r1 R2\" CMPEND (8)},\n  {MIR_DEQ, \"r r md\", \"ed:19 r1 m\" CMPEND (8)},\n\n  /* (xgr r0,r2 | xg r0,m); lngr r0,r0; srlg r0,r0,63: */\n  {MIR_NE, \"r 0 r\", \"b982 r0 R2; b901 r0 R0; eb:0c r0 R0 SD63\"},\n  {MIR_NE, \"r 0 M3\", \"e3:82 r0 m; b901 r0 R0; eb:0c r0 R0 SD63\"},\n  /* (xr r0,r2 | x r0,m | xy r0, m); lnr r0,r0; srl r0,r0,31: */\n  {MIR_NES, \"r 0 r\", \"17* r0 R2; 11* r0 R0; 88 r0 R0 Sd31\"},\n  {MIR_NES, \"r 0 m2\", \"57 r0 m; 11* r0 R0; 88 r0 R0 Sd31\"},\n  {MIR_NES, \"r 0 M2\", \"e3:57 r0 m; 11* r0 R0; 88 r0 R0 Sd31\"},\n\n  /* (cer r1,r2 | ce r1, mf); lghi r0,1; j<lt, gt, un> L; lghi r0,0: */\n  {MIR_FNE, \"r r r\", \"b309 r1 R2\" CMPEND (7)},\n  {MIR_FNE, \"r r mf\", \"ed:09 r1 m\" CMPEND (7)},\n  /* (cdbr r1,r2 | cdb r1, mf); lghi r0,1; j<lt, gt, un> L; lghi r0,0: */\n  {MIR_DNE, \"r r r\", \"b319 r1 R2\" CMPEND (7)},\n  {MIR_DNE, \"r r md\", \"ed:19 r1 m\" CMPEND (7)},\n\n#define CMP(LC, SC, ULC, USC, FC, DC, m)                                                 \\\n  {LC, \"r r r\", \"b920 r1 R2\" CMPEND (m)},      /* cgr r1,r2;lghi r0,1;jm L;lghi r0,0 */  \\\n    {LC, \"r r M3\", \"e3:20 r1 m\" CMPEND (m)},   /* cg r1,m;lghi r0,1;jm L;lghi r0,0 */    \\\n    {LC, \"r r Ms2\", \"e3:30 r1 m\" CMPEND (m)},  /* cgf r1,m;lghi r0,1;jm L;lghi r0,0 */   \\\n    {SC, \"r r r\", \"19* r1 R2\" CMPEND (m)},     /* cr r1,r2;lghi r0,1;jm L;lghi r0,0 */   \\\n    {SC, \"r r m2\", \"59 r1 m\" CMPEND (m)},      /* c r1,m;lghi r0,1;jm L;lghi r0,0 */     \\\n    {SC, \"r r M2\", \"e3:59 r1 m\" CMPEND (m)},   /* cy r1,m;lghi r0,1;jm L;lghi r0,0 */    \\\n    {ULC, \"r r r\", \"b921 r1 R2\" CMPEND (m)},   /* clgr r1,r2;lghi r0,1;jm L;lghi r0,0 */ \\\n    {ULC, \"r r M3\", \"e3:21 r1 m\" CMPEND (m)},  /* clg r1,m;lghi r0,1;jm L;lghi r0,0 */   \\\n    {ULC, \"r r Mu2\", \"e3:31 r1 m\" CMPEND (m)}, /* clgf r1,m;lghi r0,1;jm L;lghi r0,0 */  \\\n    {USC, \"r r r\", \"15* r1 R2\" CMPEND (m)},    /* clr r1,r2;lghi r0,1;jm L;lghi r0,0 */  \\\n    {USC, \"r r m2\", \"55 r1 m\" CMPEND (m)},     /* cl r1,m;lghi r0,1;jm L;lghi r0,0 */    \\\n    {USC, \"r r M2\", \"e3:55 r1 m\" CMPEND (m)},  /* cly r1,m;lghi r0,1;jm L;lghi r0,0 */   \\\n    {FC, \"r r r\", \"b309 r1 R2\" CMPEND (m)},    /* cer r1,r2;lghi r0,1;jm L;lghi r0,0 */  \\\n    {FC, \"r r mf\", \"ed:09 r1 m\" CMPEND (m)},   /* ce r1,mf;lghi r0,1;jm L;lghi r0,0 */   \\\n    {DC, \"r r r\", \"b319 r1 R2\" CMPEND (m)},    /* cdbr r1,r2;lghi r0,1;jm L;lghi r0,0 */ \\\n    {DC, \"r r md\", \"ed:19 r1 m\" CMPEND (m)},   /* cdb r1,mf;lghi r0,1;jm L;lghi r0,0 */\n\n  CMP (MIR_LT, MIR_LTS, MIR_ULT, MIR_ULTS, MIR_FLT, MIR_DLT, 4)\n    CMP (MIR_GT, MIR_GTS, MIR_UGT, MIR_UGTS, MIR_FGT, MIR_DGT, 2)\n      CMP (MIR_LE, MIR_LES, MIR_ULE, MIR_ULES, MIR_FLE, MIR_DLE, 12)\n        CMP (MIR_GE, MIR_GES, MIR_UGE, MIR_UGES, MIR_FGE, MIR_DGE, 10)\n\n#define SBRCL(mask) \"c0:4* ma\" #mask \" L\"\n#define BRCL(mask) \"; \" SBRCL (mask)\n\n          {MIR_JMP, \"L\", SBRCL (15)}, /* bcril m15, l */\n\n  {MIR_LADDR, \"r L\", \"c0:0* r0 L\"}, /* lalr r,offset */\n  {MIR_JMPI, \"r\", \"07* ma15 R0\"},   /* br r */\n\n  {MIR_BT, \"L r\", \"b902 r1 R1\" BRCL (6)}, /* ltgr r0,r0; bcril m8,l */\n  {MIR_BF, \"L r\", \"b902 r1 R1\" BRCL (8)}, /* ltgr r1,r1; bcril m6,l */\n  {MIR_BTS, \"L r\", \"12* r1 R1\" BRCL (6)}, /* ltr r0,r0; bcril m8,l */\n  {MIR_BFS, \"L r\", \"12* r1 R1\" BRCL (8)}, /* ltr r1,r1; bcril m6,l */\n\n#define BCMP(LC, SC, FC, DC, m, fm)                                    \\\n  {LC, \"L r r\", \"b920 r1 R2\" BRCL (m)},     /* cgr r1,r2; bcril m,l */ \\\n    {LC, \"L r M3\", \"e3:20 r1 m\" BRCL (m)},  /* cg r1,m; bcril m,l */   \\\n    {LC, \"L r Ms2\", \"e3:30 r1 m\" BRCL (m)}, /* cgf r1,m; bcril m,l */  \\\n    {SC, \"L r r\", \"19* r1 R2\" BRCL (m)},    /* cr r1,r2; bcril m,l */  \\\n    {SC, \"L r m2\", \"59 r1 m\" BRCL (m)},     /* c r1,m; bcril m,l */    \\\n    {SC, \"L r M2\", \"e3:59 r1 m\" BRCL (m)},  /* cy r1,m; bcril m,l */   \\\n    {FC, \"L r r\", \"b309 r1 R2\" BRCL (fm)},  /* cer r1,r2; bcril L */   \\\n    {FC, \"L r mf\", \"ed:09 r1 m\" BRCL (fm)}, /* ce r1, mf; bcril L */   \\\n    {DC, \"L r r\", \"b319 r1 R2\" BRCL (fm)},  /* cdbr r1,r2; bcril L */  \\\n    {DC, \"L r md\", \"ed:19 r1 m\" BRCL (fm)}, /* cdb r1, md; bcril L: */\n\n  BCMP (MIR_BEQ, MIR_BEQS, MIR_FBEQ, MIR_DBEQ, 8, 8)\n    BCMP (MIR_BNE, MIR_BNES, MIR_FBNE, MIR_DBNE, 6, 7) /* only fp ne has unordered mask bit */\n  BCMP (MIR_BLT, MIR_BLTS, MIR_FBLT, MIR_DBLT, 4, 4)\n    BCMP (MIR_BGT, MIR_BGTS, MIR_FBGT, MIR_DBGT, 2, 2)\n      BCMP (MIR_BLE, MIR_BLES, MIR_FBLE, MIR_DBLE, 12, 12)\n        BCMP (MIR_BGE, MIR_BGES, MIR_FBGE, MIR_DBGE, 10, 10)\n\n#define BCMPU(LC, SC, m)                                                \\\n  {LC, \"L r r\", \"b921 r1 R2\" BRCL (m)},     /* clgr r1,r2; bcril m,l */ \\\n    {LC, \"L r M3\", \"e3:21 r1 m\" BRCL (m)},  /* clg r1,m; bcril m,l */   \\\n    {LC, \"L r Ms2\", \"e3:31 r1 m\" BRCL (m)}, /* clgf r1,m; bcril m,l */  \\\n    {SC, \"L r r\", \"15* r1 R2\" BRCL (m)},    /* clr r1,r2; bcril m,l */  \\\n    {SC, \"L r m2\", \"55 r1 m\" BRCL (m)},     /* cl r1,m; bcril m,l */    \\\n    {SC, \"L r M2\", \"e3:55 r1 m\" BRCL (m)},  /* cly r1,m; bcril m,l */\n\n          BCMPU (MIR_UBLT, MIR_UBLTS, 4) BCMPU (MIR_UBGT, MIR_UBGTS, 2)\n            BCMPU (MIR_UBLE, MIR_UBLES, 12) BCMPU (MIR_UBGE, MIR_UBGES, 10)\n\n              {MIR_BO, \"L\", SBRCL (1)}, /* jo l */\n  {MIR_BNO, \"L\", SBRCL (14)},           /* jno l */\n\n  {MIR_NEG, \"r r\", \"b903 r0 R1\"},  /* lcgr r0,r1 */\n  {MIR_NEGS, \"r r\", \"13* r0 R1\"},  /* lcr r0,r1 */\n  {MIR_FNEG, \"r r\", \"b303 r0 R1\"}, /* lcebr r0,r1 */\n  {MIR_DNEG, \"r r\", \"b313 r0 R1\"}, /* lcdbr r0,r1 */\n                                   // ldneg is a builtin\n\n  {MIR_LSH, \"r r r\", \"eb:0d r0 R1 s2\"}, /* sllg r0,r2,b3 */\n  {MIR_LSH, \"r r D\", \"eb:0d r0 R1 mD\"}, /* sllg r0,r2,d */\n  {MIR_LSHS, \"r 0 r\", \"89 r0 s2\"},      /* sll r0,b2 */\n  {MIR_LSHS, \"r 0 d\", \"89 r0 md\"},      /* sll r0,r2,d */\n\n  {MIR_RSH, \"r r r\", \"eb:0a r0 R1 s2\"}, /* srag r0,r2,b3 */\n  {MIR_RSH, \"r r D\", \"eb:0a r0 R1 mD\"}, /* srag r0,r2,d */\n  {MIR_RSHS, \"r 0 r\", \"8a r0 s2\"},      /* sra r0,b2 */\n  {MIR_RSHS, \"r 0 d\", \"8a r0 md\"},      /* sra r0,r2,d */\n\n  {MIR_URSH, \"r r r\", \"eb:0c r0 R1 s2\"}, /* srlg r0,r2,b3 */\n  {MIR_URSH, \"r r D\", \"eb:0c r0 R1 mD\"}, /* srlg r0,r2,d */\n  {MIR_URSHS, \"r 0 r\", \"88 r0 s2\"},      /* srl r0,b2 */\n  {MIR_URSHS, \"r 0 d\", \"88 r0 md\"},      /* srl r0,r2,d */\n\n  {MIR_AND, \"r 0 r\", \"b980 r0 R2\"},    /* ngr r0,r1 */\n  {MIR_AND, \"r 0 M3\", \"e3:80 r0 m\"},   /* ng r0,m */\n  {MIR_AND, \"r 0 un0\", \"a5:7 r0 u0\"},  /* nill r0,u */\n  {MIR_AND, \"r 0 un1\", \"a5:6 r0 u1\"},  /* nilh r0,u */\n  {MIR_AND, \"r 0 un2\", \"a5:5 r0 u2\"},  /* nihl r0,u */\n  {MIR_AND, \"r 0 un3\", \"a5:4 r0 u3\"},  /* nihh r0,u */\n  {MIR_ANDS, \"r 0 r\", \"14* r0 R2\"},    /* nr r0,r1 */\n  {MIR_ANDS, \"r 0 m2\", \"54 r0 m\"},     /* n r0,m */\n  {MIR_ANDS, \"r 0 M2\", \"e3:54 r0 m\"},  /* ny r0,m */\n  {MIR_ANDS, \"r 0 un0\", \"a5:7 r0 u0\"}, /* nill r0,u */\n  {MIR_ANDS, \"r 0 un1\", \"a5:6 r0 u1\"}, /* nilh r0,u */\n\n  {MIR_OR, \"r 0 r\", \"b981 r0 R2\"},   /* ogr r0,r1 */\n  {MIR_OR, \"r 0 M3\", \"e3:81 r0 m\"},  /* og r0,m */\n  {MIR_OR, \"r 0 u0\", \"a5:b r0 u0\"},  /* oill r0,u */\n  {MIR_OR, \"r 0 u1\", \"a5:a r0 u1\"},  /* oilh r0,u */\n  {MIR_OR, \"r 0 u2\", \"a5:9 r0 u2\"},  /* oihl r0,u */\n  {MIR_OR, \"r 0 u3\", \"a5:8 r0 u3\"},  /* oihh r0,u */\n  {MIR_ORS, \"r 0 r\", \"16* r0 R2\"},   /* or r0,r1 */\n  {MIR_ORS, \"r 0 m2\", \"56 r0 m\"},    /* o r0,m */\n  {MIR_ORS, \"r 0 M2\", \"e3:56 r0 m\"}, /* oy r0,m */\n  {MIR_ORS, \"r 0 u0\", \"a5:b r0 u0\"}, /* oill r0,u */\n  {MIR_ORS, \"r 0 u1\", \"a5:a r0 u1\"}, /* oilh r0,u */\n\n  {MIR_XOR, \"r 0 r\", \"b982 r0 R2\"},   /* xgr r0,r1 */\n  {MIR_XOR, \"r 0 M3\", \"e3:82 r0 m\"},  /* xg r0,m */\n  {MIR_XORS, \"r 0 r\", \"17* r0 R2\"},   /* xr r0,r1 */\n  {MIR_XORS, \"r 0 m2\", \"57 r0 m\"},    /* x r0,m */\n  {MIR_XORS, \"r 0 M2\", \"e3:57 r0 m\"}, /* xy r0,m */\n\n  {MIR_I2F, \"r r\", \"b3a4 r0 R1\"},  /* cegbr r0,r1 */\n  {MIR_I2D, \"r r\", \"b3a5 r0 R1\"},  /* cdgbr r0,r1 */\n  {MIR_UI2F, \"r r\", \"b3a0 r0 R1\"}, /* celgbr r0,r1 */\n  {MIR_UI2D, \"r r\", \"b3a1 r0 R1\"}, /* cdlgbr r0,r1 */\n\n  {MIR_F2I, \"r r\", \"b3a8 ma5 r0 R1\"}, /* cgebr r0,5,r1 */\n  {MIR_D2I, \"r r\", \"b3a9 ma5 r0 R1\"}, /* cgdbr r0,5,r1 */\n  {MIR_F2D, \"r r\", \"b304 r0 R1\"},     /* ldebr r0,r1 */\n  {MIR_D2F, \"r r\", \"b344 r0 R1\"},     /* ledbr r0,r1 */\n  // i2ld, ui2ld, ld2i, f2ld, d2ld, ld2f, ld2d are builtins\n\n  {MIR_CALL, \"X r $\", \"0d* h14 R1\"}, /* basr h14,r0 */\n  {MIR_RET, \"$\", \"07* ma15 H14\"},    /* bcr m15,14 */\n\n  {MIR_JCALL, \"X r $\", \"07* ma15 R1\"}, /* br r */\n  {MIR_JRET, \"r $\", \"07* ma15 R0\"},    /* br r */\n\n/* sgr h15,r0; lg h0,(h15,r0); stg h0,0(h15); lay r0,160+param_area_size+blkparamsize(h15): */\n#define ALLOCA_END \"; b909 h15 R0; e3:04 h0 hs15 x0; e3:24 h0 hs15; e3:71 r0 Q hs15\"\n\n  /* la r0,7(r1);nill r0,0xfff8; ... : */\n  {MIR_ALLOCA, \"r r\", \"e3:71 r0 s1 md7; a5:7 r0 i65528\" ALLOCA_END},\n  /* lllill r0,ua; ...: */\n  {MIR_ALLOCA, \"r ua\", \"a5:f r0 ua\" ALLOCA_END},\n\n  {MIR_BSTART, \"r\", \"b904 r0 H15\"}, /* lgr r0,h15 */\n  /* lg h0,0(h15);lgr h15,r0; stg h0,0(r15): */\n  {MIR_BEND, \"r\", \"e3:04 h0 hs15; b904 h15 R0; e3:24 h0 hs15\"},\n\n  /* sllg h4,r0,3; lalr h5,T; lg h4,0(h4,h5); br h4; TableContent: */\n  {MIR_SWITCH, \"r $\", \"eb:0d h4 R0 SD3; c0:0* h5 T; e3:04 h4 hs4 hx5; 07* ma15 H4\"},\n};\n\nstatic void target_get_early_clobbered_hard_regs (MIR_insn_t insn, MIR_reg_t *hr1, MIR_reg_t *hr2) {\n  MIR_insn_code_t code = insn->code;\n\n  *hr1 = *hr2 = MIR_NON_VAR;\n  if (code == MIR_DIV || code == MIR_DIVS || code == MIR_UDIV || code == MIR_UDIVS\n      || code == MIR_MOD || code == MIR_MODS || code == MIR_UMOD || code == MIR_UMODS) {\n    *hr1 = R0_HARD_REG;\n    *hr2 = R1_HARD_REG;\n  } else if (code == MIR_ULE || code == MIR_ULES || code == MIR_UGE || code == MIR_UGES\n             || code == MIR_ALLOCA) {\n    *hr1 = R0_HARD_REG;\n  } else if (code == MIR_CALL) {  // ??? to strict: additional output, not early clobber\n    *hr1 = R14_HARD_REG;\n  } else if (code == MIR_SWITCH) {\n    *hr1 = R4_HARD_REG;\n    *hr2 = R5_HARD_REG;\n  }\n}\n\nstatic int pattern_index_cmp (const void *a1, const void *a2) {\n  int i1 = *(const int *) a1, i2 = *(const int *) a2;\n  int c1 = (int) patterns[i1].code, c2 = (int) patterns[i2].code;\n\n  return c1 != c2 ? c1 - c2 : (long) i1 - (long) i2;\n}\n\nstatic void patterns_init (gen_ctx_t gen_ctx) {\n  MIR_alloc_t alloc = gen_alloc (gen_ctx);\n  int i, ind, n = sizeof (patterns) / sizeof (struct pattern);\n  MIR_insn_code_t prev_code, code;\n  insn_pattern_info_t *info_addr;\n  insn_pattern_info_t pinfo = {0, 0};\n\n  VARR_CREATE (int, pattern_indexes, alloc, 0);\n  for (i = 0; i < n; i++) VARR_PUSH (int, pattern_indexes, i);\n  qsort (VARR_ADDR (int, pattern_indexes), n, sizeof (int), pattern_index_cmp);\n  VARR_CREATE (insn_pattern_info_t, insn_pattern_info, alloc, 0);\n  for (i = 0; i < MIR_INSN_BOUND; i++) VARR_PUSH (insn_pattern_info_t, insn_pattern_info, pinfo);\n  info_addr = VARR_ADDR (insn_pattern_info_t, insn_pattern_info);\n  for (prev_code = MIR_INSN_BOUND, i = 0; i < n; i++) {\n    ind = VARR_GET (int, pattern_indexes, i);\n    if ((code = patterns[ind].code) != prev_code) {\n      if (i != 0) info_addr[prev_code].num = i - info_addr[prev_code].start;\n      info_addr[code].start = i;\n      prev_code = code;\n    }\n  }\n  gen_assert (prev_code != MIR_INSN_BOUND);\n  info_addr[prev_code].num = n - info_addr[prev_code].start;\n}\n\nstatic int int20_p (int64_t i) { return -(1 << 19) <= i && i < (1 << 19); }\nstatic int uint12_p (uint64_t u) { return !(u >> 12); }\nstatic int int16_p (int64_t i) { return -(1 << 15) <= i && i < (1 << 15); }\nstatic int uint16_p (uint64_t u) { return !(u >> 16); }\nstatic int nth_uint16_p (uint64_t u, int n) { return !(u & ~(((uint64_t) 0xffff) << n * 16)); }\n\nstatic int pattern_match_p (gen_ctx_t gen_ctx, const struct pattern *pat, MIR_insn_t insn) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  int n;\n  size_t nop, nops = MIR_insn_nops (ctx, insn);\n  const char *p;\n  char ch, start_ch;\n  MIR_op_mode_t mode;\n  MIR_op_t op, original;\n  MIR_reg_t hr;\n\n  for (nop = 0, p = pat->pattern; *p != 0; p++, nop++) {\n    while (*p == ' ' || *p == '\\t') p++;\n    if (*p == '$') return TRUE;\n    if (MIR_call_code_p (insn->code) && nop >= nops) return FALSE;\n    gen_assert (nop < nops);\n    op = insn->ops[nop];\n    switch (start_ch = *p) {\n    case 'X': break;\n    case 'r':\n      if (op.mode != MIR_OP_VAR) return FALSE;\n      break;\n    case 'h':\n      if (op.mode != MIR_OP_VAR) return FALSE;\n      ch = *++p;\n      gen_assert ('0' <= ch && ch <= '9');\n      hr = ch - '0';\n      ch = *++p;\n      if ('0' <= ch && ch <= '9')\n        hr = hr * 10 + ch - '0';\n      else\n        --p;\n      gen_assert (hr <= MAX_HARD_REG);\n      if (op.u.var != hr) return FALSE;\n      break;\n    case 'm':\n    case 'M': {\n      MIR_type_t type, type2, type3 = MIR_T_BOUND;\n      int l_p = FALSE, u_p = TRUE, s_p = TRUE, index_p = TRUE;\n\n      if (op.mode != MIR_OP_VAR_MEM) return FALSE;\n      ch = *++p;\n      switch (ch) {\n      case 'f':\n        type = MIR_T_F;\n        type2 = MIR_T_BOUND;\n        break;\n      case 'd':\n        type = MIR_T_D;\n        type2 = MIR_T_BOUND;\n        break;\n      case 'l':\n        ch = *++p;\n        gen_assert (ch == 'd');\n        l_p = TRUE;\n        type = MIR_T_LD;\n        type2 = MIR_T_BOUND;\n        break;\n      case 'u':\n      case 's':\n        u_p = ch == 'u';\n        s_p = ch == 's';\n        ch = *++p;\n        /* fall through */\n      default:\n        gen_assert ('0' <= ch && ch <= '3');\n        if (ch == '0') {\n          type = u_p ? MIR_T_U8 : MIR_T_I8;\n          type2 = u_p && s_p ? MIR_T_I8 : MIR_T_BOUND;\n        } else if (ch == '1') {\n          type = u_p ? MIR_T_U16 : MIR_T_I16;\n          type2 = u_p && s_p ? MIR_T_I16 : MIR_T_BOUND;\n        } else if (ch == '2') {\n          type = u_p ? MIR_T_U32 : MIR_T_I32;\n          type2 = u_p && s_p ? MIR_T_I32 : MIR_T_BOUND;\n#if MIR_PTR32\n          if (u_p) type3 = MIR_T_P;\n#endif\n        } else {\n          index_p = start_ch != 'm'; /* m3 special treatment */\n          type = u_p ? MIR_T_U64 : MIR_T_I64;\n          type2 = u_p && s_p ? MIR_T_I64 : MIR_T_BOUND;\n#if MIR_PTR64\n          type3 = MIR_T_P;\n#endif\n        }\n      }\n      if (op.u.var_mem.type != type && op.u.var_mem.type != type2 && op.u.var_mem.type != type3)\n        return FALSE;\n      if ((!index_p && op.u.var_mem.base != MIR_NON_VAR && op.u.var_mem.index != MIR_NON_VAR)\n          || (op.u.var_mem.index != MIR_NON_VAR && op.u.var_mem.scale != 1)\n          || op.u.var_mem.base == R0_HARD_REG || op.u.var_mem.index == R0_HARD_REG\n          || !((start_ch == 'm' && uint12_p (op.u.var_mem.disp))\n               || (start_ch != 'm' && int20_p (op.u.var_mem.disp)))\n          || (l_p\n              && !((start_ch == 'm' && uint12_p (op.u.var_mem.disp + 8))\n                   || (start_ch != 'm' && int20_p (op.u.var_mem.disp + 8)))))\n        return FALSE;\n      break;\n    }\n    case 'i':\n      if ((op.mode != MIR_OP_INT && op.mode != MIR_OP_UINT) || !int16_p (op.u.i)) return FALSE;\n      break;\n    case 'I':\n      if (op.mode != MIR_OP_INT && op.mode != MIR_OP_UINT && op.mode != MIR_OP_REF) return FALSE;\n      break;\n    case 'u':\n      if (op.mode != MIR_OP_INT && op.mode != MIR_OP_UINT) return FALSE;\n      ch = *++p;\n      if (ch == 'a') {\n        if (!uint16_p ((op.u.u + 7) / 8 * 8)) return FALSE;\n      } else if ('0' <= ch && ch <= '3') {\n        if (!nth_uint16_p (op.u.u, ch - '0')) return FALSE;\n      } else if (ch == 'n') {\n        ch = *++p;\n        gen_assert ('0' <= ch && ch <= '3');\n        if (!nth_uint16_p (~op.u.u, ch - '0')) return FALSE;\n      } else {\n        p--;\n        if (!uint16_p (op.u.u)) return FALSE;\n      }\n      break;\n    case 'd':\n      if ((op.mode != MIR_OP_INT && op.mode != MIR_OP_UINT) || !uint12_p (op.u.u)) return FALSE;\n      break;\n    case 'D':\n      if ((op.mode != MIR_OP_INT && op.mode != MIR_OP_UINT) || !int20_p (op.u.i)) return FALSE;\n      break;\n    case 'z':\n      if (op.mode != MIR_OP_FLOAT || op.u.f == 0.0f) return FALSE;\n      break;\n    case 'Z':\n      if (op.mode != MIR_OP_DOUBLE || op.u.d == 0.0) return FALSE;\n      break;\n    case 'L':\n      if (op.mode != MIR_OP_LABEL && op.mode != MIR_OP_REF) return FALSE;\n      break;\n    case '0':\n    case '1':\n    case '2':\n    case '3':\n    case '4':\n    case '5':\n    case '6':\n    case '7':\n    case '8':\n    case '9':\n      n = start_ch - '0';\n      gen_assert (n < (int) nop);\n      original = insn->ops[n];\n      mode = op.mode;\n      if (mode == MIR_OP_UINT) mode = MIR_OP_INT;\n      if (original.mode != mode && (original.mode != MIR_OP_UINT || mode != MIR_OP_INT))\n        return FALSE;\n      gen_assert (mode == MIR_OP_VAR || mode == MIR_OP_INT || mode == MIR_OP_FLOAT\n                  || mode == MIR_OP_DOUBLE || mode == MIR_OP_LDOUBLE || mode == MIR_OP_VAR_MEM\n                  || mode == MIR_OP_LABEL);\n      if (mode == MIR_OP_VAR && op.u.var != original.u.var)\n        return FALSE;\n      else if (mode == MIR_OP_INT && op.u.i != original.u.i)\n        return FALSE;\n      else if (mode == MIR_OP_FLOAT && op.u.f != original.u.f)\n        return FALSE;\n      else if (mode == MIR_OP_DOUBLE && op.u.d != original.u.d)\n        return FALSE;\n      else if (mode == MIR_OP_LDOUBLE && op.u.ld != original.u.ld)\n        return FALSE;\n      else if (mode == MIR_OP_LABEL && op.u.label != original.u.label)\n        return FALSE;\n      else if (mode == MIR_OP_VAR_MEM\n               && (op.u.var_mem.type != original.u.var_mem.type\n                   || op.u.var_mem.scale != original.u.var_mem.scale\n                   || op.u.var_mem.base != original.u.var_mem.base\n                   || op.u.var_mem.index != original.u.var_mem.index\n                   || op.u.var_mem.disp != original.u.var_mem.disp))\n        return FALSE;\n      break;\n    default: gen_assert (FALSE);\n    }\n  }\n  gen_assert (nop == nops);\n  return TRUE;\n}\n\nstatic const char *find_insn_pattern_replacement (gen_ctx_t gen_ctx, MIR_insn_t insn) {\n  int i;\n  const struct pattern *pat;\n  insn_pattern_info_t info = VARR_GET (insn_pattern_info_t, insn_pattern_info, insn->code);\n\n  for (i = 0; i < info.num; i++) {\n    pat = &patterns[VARR_GET (int, pattern_indexes, info.start + i)];\n    if (pattern_match_p (gen_ctx, pat, insn)) return pat->replacement;\n  }\n  return NULL;\n}\n\nstatic void patterns_finish (gen_ctx_t gen_ctx) {\n  VARR_DESTROY (int, pattern_indexes);\n  VARR_DESTROY (insn_pattern_info_t, insn_pattern_info);\n}\n\nstatic int dec_value (int ch) { return '0' <= ch && ch <= '9' ? ch - '0' : -1; }\n\nstatic int hex_value (int ch) {\n  if ('0' <= ch && ch <= '9') return ch - '0';\n  if ('a' <= ch && ch <= 'f') return ch - 'a' + 10;\n  if ('A' <= ch && ch <= 'F') return ch - 'A' + 10;\n  return -1;\n}\n\nstatic uint64_t read_dec (const char **ptr) {\n  int v;\n  const char *p = *ptr + 1;\n  uint64_t res = 0;\n\n  if (dec_value (*p) < 0) return -1;\n  for (p = *ptr + 1; (v = dec_value (*p)) >= 0; p++) {\n    gen_assert ((res >> 60) == 0);\n    res = res * 10 + v;\n  }\n  gen_assert (p != *ptr);\n  *ptr = p - 1;\n  return res;\n}\n\nstatic int read_curr_hex (const char **ptr, int *v) {\n  const char *p;\n  int n = 0, d;\n\n  for (*v = 0, p = *ptr; (d = hex_value (*p)) >= 0; p++, n++) {\n    gen_assert (n < 4);\n    *v = *v * 16 + d;\n  }\n  if (n != 0) *ptr = p - 1;\n  return n; /* number of consumed hex digits */\n}\n\nstatic void put_arr (struct gen_ctx *gen_ctx, void *v, size_t len) { /* BE only */\n  for (size_t i = 0; i < len; i++) VARR_PUSH (uint8_t, result_code, ((uint8_t *) v)[i]);\n}\n\nstatic void set_int32 (uint8_t *addr, int32_t v) { *(int32_t *) addr = v; }\n\nstatic void set_int64 (uint8_t *addr, int64_t v) { *(int64_t *) addr = v; }\n\nstatic int64_t get_int64 (uint8_t *addr) { return *(int64_t *) addr; }\n\nstatic size_t add_to_const_pool (struct gen_ctx *gen_ctx, uint64_t v) {\n  uint64_t *addr = VARR_ADDR (uint64_t, const_pool);\n  size_t n, len = VARR_LENGTH (uint64_t, const_pool);\n\n  for (n = 0; n < len; n++)\n    if (addr[n] == v) return n;\n  VARR_PUSH (uint64_t, const_pool, v);\n  return len;\n}\n\nstatic int setup_imm_addr (struct gen_ctx *gen_ctx, uint64_t v) {\n  const_ref_t cr;\n  size_t n;\n\n  n = add_to_const_pool (gen_ctx, v);\n  cr.insn_pc = 0;\n  cr.next_insn_pc = 0;\n  cr.const_num = n;\n  VARR_PUSH (const_ref_t, const_refs, cr);\n  return VARR_LENGTH (const_ref_t, const_refs) - 1;\n}\n\nstatic uint64_t get_op_imm (gen_ctx_t gen_ctx, MIR_op_t op) {\n  if (op.mode == MIR_OP_INT || op.mode == MIR_OP_UINT) return op.u.u;\n  gen_assert (op.mode == MIR_OP_REF);\n  if (op.u.ref->item_type == MIR_data_item && op.u.ref->u.data->name != NULL\n      && _MIR_reserved_ref_name_p (gen_ctx->ctx, op.u.ref->u.data->name))\n    return (uint64_t) op.u.ref->u.data->u.els;\n  return (uint64_t) op.u.ref->addr;\n}\n\nstatic uint64_t get_imm (gen_ctx_t gen_ctx, MIR_insn_t insn) {\n  MIR_op_t *ops = insn->ops;\n  if (insn->nops >= 2\n      && (ops[1].mode == MIR_OP_INT || ops[1].mode == MIR_OP_UINT || ops[1].mode == MIR_OP_REF))\n    return get_op_imm (gen_ctx, ops[1]);\n  if (insn->nops >= 3\n      && (ops[2].mode == MIR_OP_INT || ops[2].mode == MIR_OP_UINT || ops[2].mode == MIR_OP_REF))\n    return get_op_imm (gen_ctx, ops[2]);\n  gen_assert (FALSE);\n  return 0;\n}\n\nstatic uint64_t place_field (uint64_t v, int start_bit, int len) {\n  gen_assert (start_bit >= 0 && len > 0 && start_bit + len <= 64);\n  return (v & (-(uint64_t) 1 >> (64 - len))) << (64 - start_bit - len);\n}\n\nstatic void set_insn_field (uint64_t *binsn, uint64_t v, int start_bit, int len) {\n  *binsn |= place_field (v, start_bit, len);\n}\n\nstatic void check_and_set_mask (uint64_t *binsn_mask, uint64_t mask, int start_bit, int len) {\n  gen_assert ((*binsn_mask & place_field (mask, start_bit, len)) == 0);\n  *binsn_mask |= place_field (mask, start_bit, len);\n}\n\nstatic void out_insn (gen_ctx_t gen_ctx, MIR_insn_t insn, const char *replacement,\n                      void **jump_addrs) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  size_t nops = MIR_insn_nops (ctx, insn);\n  size_t offset;\n  const char *p, *insn_str;\n  label_ref_t lr;\n  int switch_table_addr_insn_start = -1;\n  uint64_t zero64 = 0;\n  uint16_t nop_binsn = 0x18 << 8; /* lr 0,0 */\n\n  if (insn->code == MIR_ALLOCA\n      && (insn->ops[1].mode == MIR_OP_INT || insn->ops[1].mode == MIR_OP_UINT))\n    insn->ops[1].u.u = (insn->ops[1].u.u + 15) & -16;\n  for (insn_str = replacement;; insn_str = p + 1) {\n    MIR_op_t op;\n    char ch, ch2, start_ch;\n    uint64_t u, binsn = 0, binsn_mask = 0;\n    int opcode1 = -1, opcode2 = -1, opcode11 = -1, opcode12 = -1, mask = -1, MASK = -1;\n    int r1 = -1, r2 = -1, R1 = -1, R2 = -1, rs = -1, rx = -1;\n    int imm = -1, IMM = -1, d = -1, dh = -1, label_off = -1, const_ref_num = -1, label_ref_num = -1;\n    int n, len = 0, v, reg;\n    int switch_table_addr_p = FALSE;\n\n    for (p = insn_str; (ch = *p) != '\\0' && ch != ';'; p++) {\n      if ((ch = *p) == 0 || ch == ';') break;\n      if ((n = read_curr_hex (&p, &v)) > 0) {\n        gen_assert (n == 4 || n == 2);\n        len = 4;\n        if (n == 4) {\n          opcode2 = v;\n          ch2 = *++p;\n          if (ch2 == '*') { /* sil */\n            len = 6;\n          } else {\n            p--;\n          }\n        } else {\n          opcode1 = v;\n          ch2 = *++p;\n          if (ch2 != ':') { /* rr, rx, rs */\n            if (ch2 == '*') {\n              len = 2;\n            } else {\n              p--;\n            }\n          } else {\n            p++;\n            n = read_curr_hex (&p, &v);\n            gen_assert (n == 1 || n == 2);\n            if (n == 1) {\n              ch2 = *++p;\n              if (ch2 == '*') { /* ril */\n                len = 6;\n              } else {\n                p--;\n              }\n              opcode11 = v;\n            } else {\n              len = 6;\n              opcode12 = v;\n            }\n          }\n        }\n        continue;\n      }\n      switch ((start_ch = ch = *p)) {\n      case ' ':\n      case '\\t': break;\n      case 'h':\n      case 'H':\n        ch2 = *++p;\n        if (ch2 == 's' || ch2 == 'x') {\n          ch = ch2;\n        } else {\n          p--;\n        }\n        reg = read_dec (&p);\n        gen_assert (reg >= 0 && reg <= F15_HARD_REG);\n        if (reg >= F0_HARD_REG) reg -= F0_HARD_REG;\n        goto set_reg;\n      case 'r':\n      case 'R':\n      case 'x':\n      case 's':\n      case 'n':\n        ch2 = *++p;\n        gen_assert ('0' <= ch2 && ch2 <= '2' && ch2 - '0' < (int) nops);\n        op = insn->ops[ch2 - '0'];\n        gen_assert (op.mode == MIR_OP_VAR);\n        reg = op.u.var;\n        gen_assert (ch != 'n' || reg >= F0_HARD_REG);\n        if (start_ch == 'n') reg += 2;\n        gen_assert (reg <= F15_HARD_REG);\n        if (reg >= F0_HARD_REG) reg -= F0_HARD_REG;\n      set_reg:\n        if (ch == 'r' || ch == 'h' || ch == 'n') {\n          if (opcode2 < 0) {\n            gen_assert (r1 < 0);\n            r1 = reg;\n          } else {\n            gen_assert (R1 < 0);\n            R1 = reg;\n          }\n        } else if (ch == 'R' || ch == 'H') {\n          if (opcode2 < 0) {\n            gen_assert (r2 < 0);\n            r2 = reg;\n          } else {\n            gen_assert (R2 < 0);\n            R2 = reg;\n          }\n        } else if (ch == 'x') {\n          gen_assert (rx < 0 && reg != 0);\n          rx = reg;\n        } else {\n          gen_assert (ch == 's' && rs < 0 && reg != 0);\n          rs = reg;\n        }\n        break;\n      case 'm':\n        ch2 = *++p;\n        if (ch2 == 'a') { /* mask */\n          if (opcode2 < 0) {\n            gen_assert (mask < 0);\n            mask = read_dec (&p);\n          } else {\n            gen_assert (MASK < 0);\n            MASK = read_dec (&p);\n          }\n        } else if (ch2 == 'd' || ch2 == 'D') { /* displacement from immediate */\n          gen_assert (d < 0 && dh < 0);\n          if (ch2 != 'd' || (d = read_dec (&p)) < 0) {\n            u = get_imm (gen_ctx, insn);\n            d = u & 0xfff;\n            dh = ((int64_t) u >> 12) & 0xff;\n            if (dh == 0) dh = -1;\n          }\n        } else {\n          if (ch2 != 'n') p--;\n          if (insn->ops[0].mode == MIR_OP_VAR_MEM) {\n            op = insn->ops[0];\n          } else if (nops >= 2 && insn->ops[1].mode == MIR_OP_VAR_MEM) {\n            op = insn->ops[1];\n          } else if (nops >= 3 && insn->ops[2].mode == MIR_OP_VAR_MEM) {\n            op = insn->ops[2];\n          } else {\n            gen_assert (FALSE);\n          }\n          gen_assert (rs < 0 && rx < 0);\n          if (ch2 == 'n') op.u.var_mem.disp += 8;\n          gen_assert (op.u.var_mem.index == MIR_NON_VAR || op.u.var_mem.scale == 1);\n          if (op.u.var_mem.base == MIR_NON_VAR) {\n            if (op.u.var_mem.index != MIR_NON_VAR) rs = op.u.var_mem.index;\n          } else {\n            rs = op.u.var_mem.base;\n            if (op.u.var_mem.index != MIR_NON_VAR) rx = op.u.var_mem.index;\n          }\n          gen_assert (d < 0 && dh < 0);\n          d = op.u.var_mem.disp & 0xfff;\n          dh = (op.u.var_mem.disp >> 12) & 0xff;\n          if (dh == 0) dh = -1;\n        }\n        break;\n      case 'i':\n        gen_assert (imm < 0);\n        if ((imm = read_dec (&p)) >= 0) {\n        } else {\n          u = get_imm (gen_ctx, insn);\n          imm = u & 0xffff;\n        }\n        break;\n      case 'u':\n        gen_assert (imm < 0);\n        u = get_imm (gen_ctx, insn);\n        ch2 = *++p;\n        if (ch2 == 'a') {\n          imm = (u + 7) / 8 * 8;\n        } else {\n          gen_assert ('0' <= ch2 && ch2 <= '3');\n          imm = (u >> (ch2 - '0') * 16) & 0xffff;\n        }\n        break;\n      case 'j':\n        gen_assert (IMM < 0);\n        u = get_imm (gen_ctx, insn);\n        IMM = u & 0xffff;\n        break;\n      case 'I': {\n        uint64_t imm_val;\n\n        ch2 = *++p;\n        gen_assert (ch2 == 'a');\n        gen_assert (const_ref_num < 0);\n        imm_val = get_imm (gen_ctx, insn);\n        const_ref_num = setup_imm_addr (gen_ctx, imm_val);\n        break;\n      }\n      case 'S':\n        ch2 = *++p;\n        gen_assert (ch2 == 'd' || ch2 == 'D');\n        gen_assert (d < 0 && dh < 0);\n        u = read_dec (&p);\n        d = u & 0xfff;\n        dh = ((int64_t) u >> 12) & 0xff;\n        gen_assert (ch2 == 'D' || dh == 0);\n        if (dh == 0) dh = -1;\n        break;\n      case 'l':\n        label_off = read_dec (&p);\n        gen_assert (label_off % 2 == 0 && label_off >= 0);\n        label_off /= 2;\n        break;\n      case 'L':\n        op = insn->ops[insn->code != MIR_CALL && insn->code != MIR_LADDR ? 0 : 1];\n        gen_assert (op.mode == MIR_OP_LABEL);\n        lr.abs_addr_p = FALSE;\n        lr.label_val_disp = 0;\n        if (jump_addrs == NULL)\n          lr.u.label = op.u.label;\n        else\n          lr.u.jump_addr = jump_addrs[0];\n        label_ref_num = VARR_LENGTH (label_ref_t, label_refs);\n        VARR_PUSH (label_ref_t, label_refs, lr);\n        break;\n      case 'T':\n        gen_assert (!switch_table_addr_p && switch_table_addr_insn_start < 0);\n        switch_table_addr_p = TRUE;\n        break;\n      case 'Q': {\n        int64_t size = S390X_STACK_HEADER_SIZE + param_save_area_size + blk_ld_value_save_area_size;\n        gen_assert (d < 0 && dh < 0 && int20_p (size));\n        d = (size) &0xfff;\n        dh = ((size) >> 12) & 0xff;\n        if (dh == 0) dh = -1;\n        break;\n      }\n      default: gen_assert (FALSE);\n      }\n    }\n\n    if (opcode1 >= 0) {\n      gen_assert (opcode1 < 256);\n      set_insn_field (&binsn, opcode1, 0, 8);\n      check_and_set_mask (&binsn_mask, 0xff, 0, 8);\n    }\n    if (opcode2 >= 0) {\n      gen_assert (opcode2 < (1 << 16));\n      set_insn_field (&binsn, opcode2, 0, 16);\n      check_and_set_mask (&binsn_mask, 0xffff, 0, 16);\n    }\n    if (opcode11 >= 0) {\n      gen_assert (opcode11 < 16);\n      set_insn_field (&binsn, opcode11, 12, 4);\n      check_and_set_mask (&binsn_mask, 0xf, 12, 4);\n    }\n    if (opcode12 >= 0) {\n      gen_assert (opcode12 < 256);\n      set_insn_field (&binsn, opcode12, 40, 8);\n      check_and_set_mask (&binsn_mask, 0xff, 40, 8);\n    }\n    if (r1 >= 0) {\n      gen_assert (r1 < 16);\n      set_insn_field (&binsn, r1, 8, 4);\n      check_and_set_mask (&binsn_mask, 0xf, 8, 4);\n    }\n    if (R1 >= 0) {\n      gen_assert (R1 < 16);\n      set_insn_field (&binsn, R1, 24, 4);\n      check_and_set_mask (&binsn_mask, 0xf, 24, 4);\n    }\n    if (r2 >= 0) {\n      gen_assert (r2 < 16);\n      set_insn_field (&binsn, r2, 12, 4);\n      check_and_set_mask (&binsn_mask, 0xf, 12, 4);\n    }\n    if (R2 >= 0) {\n      gen_assert (R2 < 16);\n      set_insn_field (&binsn, R2, 28, 4);\n      check_and_set_mask (&binsn_mask, 0xf, 28, 4);\n    }\n    if (rs >= 0) {\n      gen_assert (rs < 16);\n      set_insn_field (&binsn, rs, 16, 4);\n      check_and_set_mask (&binsn_mask, 0xf, 16, 4);\n    }\n    if (rx >= 0) {\n      gen_assert (rx < 16);\n      set_insn_field (&binsn, rx, 12, 4);\n      check_and_set_mask (&binsn_mask, 0xf, 12, 4);\n    }\n    if (d >= 0) {\n      gen_assert (d < (1 << 12));\n      set_insn_field (&binsn, d, 20, 12);\n      check_and_set_mask (&binsn_mask, 0xfff, 20, 12);\n    }\n    if (dh >= 0) {\n      gen_assert (dh < (1 << 8));\n      set_insn_field (&binsn, dh, 32, 8);\n      check_and_set_mask (&binsn_mask, 0xff, 32, 8);\n    }\n    if (imm >= 0) {\n      gen_assert (imm < (1 << 16));\n      set_insn_field (&binsn, imm, 16, 16);\n      check_and_set_mask (&binsn_mask, 0xffff, 16, 16);\n    }\n    if (IMM >= 0) {\n      gen_assert (IMM < (1 << 16));\n      set_insn_field (&binsn, IMM, 32, 16);\n      check_and_set_mask (&binsn_mask, 0xffff, 32, 16);\n    }\n    if (mask >= 0) {\n      gen_assert (mask < 16);\n      set_insn_field (&binsn, mask, 8, 4);\n      check_and_set_mask (&binsn_mask, 0xf, 8, 4);\n    }\n    if (MASK >= 0) {\n      gen_assert (MASK < 16);\n      set_insn_field (&binsn, MASK, 16, 4);\n      check_and_set_mask (&binsn_mask, 0xf, 16, 4);\n    }\n    if (label_off >= 0) {\n      gen_assert (label_off < (1 << 16));\n      set_insn_field (&binsn, label_off, 16, 16);\n      check_and_set_mask (&binsn_mask, 0xffff, 16, 16);\n    }\n    if (const_ref_num >= 0) VARR_ADDR (const_ref_t, const_refs)\n    [const_ref_num].insn_pc = VARR_LENGTH (uint8_t, result_code);\n    if (label_ref_num >= 0) VARR_ADDR (label_ref_t, label_refs)\n    [label_ref_num].label_val_disp = VARR_LENGTH (uint8_t, result_code);\n\n    if (switch_table_addr_p) switch_table_addr_insn_start = VARR_LENGTH (uint8_t, result_code);\n    VARR_PUSH_ARR (uint8_t, result_code, (uint8_t *) &binsn, len); /* output the machine insn */\n    if (const_ref_num >= 0) VARR_ADDR (const_ref_t, const_refs)\n    [const_ref_num].next_insn_pc = VARR_LENGTH (uint8_t, result_code);\n\n    if (*p == 0) break;\n  }\n\n  if (switch_table_addr_insn_start < 0) return;\n  while (VARR_LENGTH (uint8_t, result_code) % 8 != 0) put_arr (gen_ctx, &nop_binsn, 2);\n  /* pc offset of insn with T plus 8 bytes of insns after T: see switch */\n  offset = (VARR_LENGTH (uint8_t, result_code) - switch_table_addr_insn_start);\n  *(uint32_t *) (VARR_ADDR (uint8_t, result_code) + switch_table_addr_insn_start + 2) |= offset / 2;\n  gen_assert (insn->code == MIR_SWITCH);\n  for (size_t i = 1; i < insn->nops; i++) {\n    gen_assert (insn->ops[i].mode == MIR_OP_LABEL);\n    lr.abs_addr_p = TRUE;\n    lr.label_val_disp = VARR_LENGTH (uint8_t, result_code);\n    if (jump_addrs == NULL)\n      lr.u.label = insn->ops[i].u.label;\n    else\n      lr.u.jump_addr = jump_addrs[i - 1];\n    VARR_PUSH (label_ref_t, label_refs, lr);\n    /* reserve space for absolute label address: */\n    VARR_PUSH_ARR (uint8_t, result_code, (uint8_t *) &zero64, sizeof (zero64));\n  }\n}\n\nstatic int target_memory_ok_p (gen_ctx_t gen_ctx MIR_UNUSED, MIR_op_t *op_ref) {\n  if (op_ref->mode != MIR_OP_VAR_MEM) return FALSE;\n  if (((op_ref->u.var_mem.type != MIR_T_U64 && op_ref->u.var_mem.type != MIR_T_U64)\n       || op_ref->u.var_mem.base == MIR_NON_VAR || op_ref->u.var_mem.index == MIR_NON_VAR)\n      && (op_ref->u.var_mem.index == MIR_NON_VAR || op_ref->u.var_mem.scale == 1)\n      && int20_p (op_ref->u.var_mem.disp)\n      && (op_ref->u.var_mem.type != MIR_T_LD || int20_p (op_ref->u.var_mem.disp + 8)))\n    return TRUE;\n  return FALSE;\n}\n\nstatic int target_insn_ok_p (gen_ctx_t gen_ctx, MIR_insn_t insn) {\n  return find_insn_pattern_replacement (gen_ctx, insn) != NULL;\n}\n\nstatic void add_consts (gen_ctx_t gen_ctx) {\n  while (VARR_LENGTH (uint8_t, result_code) % 16 != 0) /* Align the pool */\n    VARR_PUSH (uint8_t, result_code, 0);\n  for (size_t i = 0; i < VARR_LENGTH (const_ref_t, const_refs); i++) { /* Add pool constants */\n    const_ref_t cr = VARR_GET (const_ref_t, const_refs, i);\n    int64_t offset = VARR_LENGTH (uint8_t, result_code) - cr.insn_pc;\n    uint64_t zero64 = 0;\n\n    gen_assert (offset > 0 && offset % 2 == 0);\n    offset /= 2;\n    gen_assert ((offset >> 31) == 0);\n    set_int32 (VARR_ADDR (uint8_t, result_code) + cr.insn_pc + 2 /* start disp in LALR */, offset);\n    put_arr (gen_ctx, &VARR_ADDR (uint64_t, const_pool)[cr.const_num], 8);\n    put_arr (gen_ctx, &zero64, sizeof (zero64)); /* keep 16 bytes align */\n  }\n}\n\nstatic void target_split_insns (gen_ctx_t gen_ctx MIR_UNUSED) {}\n\nstatic uint8_t *target_translate (gen_ctx_t gen_ctx, size_t *len) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  size_t i;\n  MIR_insn_t insn, old_insn;\n  const char *replacement;\n\n  gen_assert (curr_func_item->item_type == MIR_func_item);\n  VARR_TRUNC (uint8_t, result_code, 0);\n  VARR_TRUNC (uint64_t, const_pool, 0);\n  VARR_TRUNC (const_ref_t, const_refs, 0);\n  VARR_TRUNC (label_ref_t, label_refs, 0);\n  VARR_TRUNC (uint64_t, abs_address_locs, 0);\n  for (insn = DLIST_HEAD (MIR_insn_t, curr_func_item->u.func->insns); insn != NULL;\n       insn = DLIST_NEXT (MIR_insn_t, insn)) {\n    MIR_insn_code_t code = insn->code;\n\n    /* ???split/change */\n    if ((code == MIR_RSH || code == MIR_LSH || code == MIR_URSH || code == MIR_RSHS\n         || code == MIR_LSHS || code == MIR_URSHS)\n        && (insn->ops[2].mode == MIR_OP_INT || insn->ops[2].mode == MIR_OP_UINT)) {\n      if (insn->ops[2].u.i == 0) {\n        gen_mov (gen_ctx, insn, MIR_MOV, insn->ops[0], insn->ops[1]);\n        old_insn = insn;\n        insn = DLIST_NEXT (MIR_insn_t, insn);\n        gen_delete_insn (gen_ctx, old_insn);\n      } else {\n        if (insn->ops[2].mode == MIR_OP_INT && insn->ops[2].u.i < 0) {\n          switch (code) {\n          case MIR_RSH: insn->code = MIR_LSH; break;\n          case MIR_URSH: insn->code = MIR_LSH; break;\n          case MIR_LSH: insn->code = MIR_RSH; break;\n          case MIR_RSHS: insn->code = MIR_LSHS; break;\n          case MIR_URSHS: insn->code = MIR_LSHS; break;\n          case MIR_LSHS: insn->code = MIR_RSHS; break;\n          default: gen_assert (FALSE); break;\n          }\n          insn->ops[2].u.i = -insn->ops[2].u.i;\n        }\n        if (code == MIR_RSH || code == MIR_LSH || code == MIR_URSH) {\n          if (insn->ops[2].u.i > 64) insn->ops[2].u.i = 64;\n        } else if (insn->ops[2].u.i > 32) {\n          insn->ops[2].u.i = 32;\n        }\n      }\n    }\n    if (insn->code == MIR_LABEL) {\n      set_label_disp (gen_ctx, insn, VARR_LENGTH (uint8_t, result_code));\n    } else if (insn->code != MIR_USE) {\n      replacement = find_insn_pattern_replacement (gen_ctx, insn);\n      if (replacement == NULL) {\n        fprintf (stderr, \"fatal failure in matching insn:\");\n        MIR_output_insn (ctx, stderr, insn, curr_func_item->u.func, TRUE);\n        exit (1);\n      } else {\n        gen_assert (replacement != NULL);\n        out_insn (gen_ctx, insn, replacement, NULL);\n      }\n    }\n  }\n  /* Setting up labels */\n  for (i = 0; i < VARR_LENGTH (label_ref_t, label_refs); i++) {\n    label_ref_t lr = VARR_GET (label_ref_t, label_refs, i);\n\n    if (lr.abs_addr_p) {\n      set_int64 (&VARR_ADDR (uint8_t, result_code)[lr.label_val_disp],\n                 (int64_t) get_label_disp (gen_ctx, lr.u.label));\n      VARR_PUSH (uint64_t, abs_address_locs, lr.label_val_disp);\n    } else { /* 32-bit relative address */\n      int64_t offset = (int64_t) get_label_disp (gen_ctx, lr.u.label) - (int64_t) lr.label_val_disp;\n      gen_assert (offset % 2 == 0);\n      offset /= 2;\n      gen_assert (((offset < 0 ? -offset : offset) & ~(int64_t) 0x7fffffff) == 0);\n      *(uint32_t *) (VARR_ADDR (uint8_t, result_code) + lr.label_val_disp + 2)\n        |= (offset & 0xffffffff);\n    }\n  }\n  add_consts (gen_ctx);\n  *len = VARR_LENGTH (uint8_t, result_code);\n  return VARR_ADDR (uint8_t, result_code);\n}\n\nstatic void target_rebase (gen_ctx_t gen_ctx, uint8_t *base) {\n  MIR_code_reloc_t reloc;\n\n  VARR_TRUNC (MIR_code_reloc_t, relocs, 0);\n  for (size_t i = 0; i < VARR_LENGTH (uint64_t, abs_address_locs); i++) {\n    reloc.offset = VARR_GET (uint64_t, abs_address_locs, i);\n    reloc.value = base + get_int64 (base + reloc.offset);\n    VARR_PUSH (MIR_code_reloc_t, relocs, reloc);\n  }\n  _MIR_update_code_arr (gen_ctx->ctx, base, VARR_LENGTH (MIR_code_reloc_t, relocs),\n                        VARR_ADDR (MIR_code_reloc_t, relocs));\n  gen_setup_lrefs (gen_ctx, base);\n}\n\nstatic void target_change_to_direct_calls (MIR_context_t ctx MIR_UNUSED) {}\n\nstruct target_bb_version {\n  uint8_t *base;\n  label_ref_t branch_ref; /* label cand used for jump to this bb version */\n};\n\nstatic void target_init_bb_version_data (target_bb_version_t data) {\n  data->base = NULL; /* we don't know origin branch */\n}\n\nstatic void target_bb_translate_start (gen_ctx_t gen_ctx) {\n  VARR_TRUNC (uint8_t, result_code, 0);\n  VARR_TRUNC (const_ref_t, const_refs, 0);\n  VARR_TRUNC (label_ref_t, label_refs, 0);\n  VARR_TRUNC (uint64_t, abs_address_locs, 0);\n}\n\nstatic void target_bb_insn_translate (gen_ctx_t gen_ctx, MIR_insn_t insn, void **jump_addrs) {\n  const char *replacement;\n  if (insn->code == MIR_LABEL) return;\n  replacement = find_insn_pattern_replacement (gen_ctx, insn);\n  gen_assert (replacement != NULL);\n  out_insn (gen_ctx, insn, replacement, jump_addrs);\n}\n\nstatic void target_output_jump (gen_ctx_t gen_ctx, void **jump_addrs) {\n  out_insn (gen_ctx, temp_jump, temp_jump_replacement, jump_addrs);\n}\n\nstatic uint8_t *target_bb_translate_finish (gen_ctx_t gen_ctx, size_t *len) {\n  add_consts (gen_ctx);\n  *len = VARR_LENGTH (uint8_t, result_code);\n  return VARR_ADDR (uint8_t, result_code);\n}\n\nstatic void setup_rel (gen_ctx_t gen_ctx, label_ref_t *lr, uint8_t *base, void *addr) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  int64_t offset = (int64_t) addr - (int64_t) (base + lr->label_val_disp);\n  int32_t rel32 = offset;\n\n  gen_assert ((offset & 0x1) == 0);\n  offset >>= 1;\n  gen_assert (((offset < 0 ? -offset : offset) & ~(int64_t) 0x7fffffff) == 0);\n  /* check max 32-bit offset with possible branch conversion (see offset): */\n  if (lr->abs_addr_p || ((offset < 0 ? -offset : offset) & ~(int64_t) 0x7fffffff) != 0) {\n    fprintf (stderr, \"too big offset (%lld) in setup_rel\", (long long) offset);\n    exit (1);\n  }\n  /* ??? thread safe: */\n  uint32_t *insn_ptr = (uint32_t *) (base + lr->label_val_disp);\n  rel32 = offset & 0xffffffff;\n  _MIR_change_code (ctx, (uint8_t *) insn_ptr + 2, (uint8_t *) &rel32, 4);\n}\n\nstatic void target_bb_rebase (gen_ctx_t gen_ctx, uint8_t *base) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  MIR_code_reloc_t reloc;\n\n  /* Setting up relative labels */\n  for (size_t i = 0; i < VARR_LENGTH (label_ref_t, label_refs); i++) {\n    label_ref_t lr = VARR_GET (label_ref_t, label_refs, i);\n    if (lr.abs_addr_p) {\n      _MIR_change_code (ctx, (uint8_t *) base + lr.label_val_disp, (uint8_t *) &lr.u.jump_addr, 8);\n    } else {\n      setup_rel (gen_ctx, &lr, base, lr.u.jump_addr);\n    }\n  }\n  VARR_TRUNC (MIR_code_reloc_t, relocs, 0);\n  for (size_t i = 0; i < VARR_LENGTH (uint64_t, abs_address_locs); i++) {\n    reloc.offset = VARR_GET (uint64_t, abs_address_locs, i);\n    reloc.value = base + get_int64 (base + reloc.offset);\n    VARR_PUSH (MIR_code_reloc_t, relocs, reloc);\n  }\n  _MIR_update_code_arr (gen_ctx->ctx, base, VARR_LENGTH (MIR_code_reloc_t, relocs),\n                        VARR_ADDR (MIR_code_reloc_t, relocs));\n}\n\nstatic void target_setup_succ_bb_version_data (gen_ctx_t gen_ctx, uint8_t *base) {\n  if (VARR_LENGTH (label_ref_t, label_refs)\n      != VARR_LENGTH (target_bb_version_t, target_succ_bb_versions))\n    /* We can have more one possible branch from original insn\n       (e.g. SWITCH, FBNE).  If it is so, we will make jumps only\n       through BB thunk. */\n    return;\n  for (size_t i = 0; i < VARR_LENGTH (target_bb_version_t, target_succ_bb_versions); i++) {\n    target_bb_version_t data = VARR_GET (target_bb_version_t, target_succ_bb_versions, i);\n    if (data == NULL) continue;\n    data->branch_ref = VARR_GET (label_ref_t, label_refs, i);\n    data->base = base;\n  }\n}\n\nstatic void target_redirect_bb_origin_branch (gen_ctx_t gen_ctx, target_bb_version_t data,\n                                              void *addr) {\n  MIR_context_t ctx = gen_ctx->ctx;\n\n  if (data->base == NULL) return;\n  if (data->branch_ref.abs_addr_p) {\n    _MIR_change_code (ctx, (uint8_t *) data->base + data->branch_ref.label_val_disp,\n                      (uint8_t *) &addr, 8);\n  } else {\n    setup_rel (gen_ctx, &data->branch_ref, data->base, addr);\n  }\n  data->base = NULL;\n}\n\nstatic void target_init (gen_ctx_t gen_ctx) {\n  MIR_alloc_t alloc = gen_alloc (gen_ctx);\n  MIR_context_t ctx = gen_ctx->ctx;\n\n  gen_ctx->target_ctx = gen_malloc (gen_ctx, sizeof (struct target_ctx));\n  VARR_CREATE (uint8_t, result_code, alloc, 0);\n  VARR_CREATE (uint64_t, const_pool, alloc, 0);\n  VARR_CREATE (const_ref_t, const_refs, alloc, 0);\n  VARR_CREATE (label_ref_t, label_refs, alloc, 0);\n  VARR_CREATE (uint64_t, abs_address_locs, alloc, 0);\n  VARR_CREATE (MIR_code_reloc_t, relocs, alloc, 0);\n  VARR_CREATE (uint64_t, ld_addr_regs, alloc, 0);\n  patterns_init (gen_ctx);\n  temp_jump = MIR_new_insn (ctx, MIR_JMP, MIR_new_label_op (ctx, NULL));\n  temp_jump_replacement = find_insn_pattern_replacement (gen_ctx, temp_jump);\n}\n\nstatic void target_finish (gen_ctx_t gen_ctx) {\n  MIR_alloc_t alloc = gen_alloc (gen_ctx);\n  patterns_finish (gen_ctx);\n  _MIR_free_insn (gen_ctx->ctx, temp_jump);\n  VARR_DESTROY (uint8_t, result_code);\n  VARR_DESTROY (uint64_t, const_pool);\n  VARR_DESTROY (const_ref_t, const_refs);\n  VARR_DESTROY (label_ref_t, label_refs);\n  VARR_DESTROY (uint64_t, abs_address_locs);\n  VARR_DESTROY (MIR_code_reloc_t, relocs);\n  VARR_DESTROY (uint64_t, ld_addr_regs);\n  MIR_free (alloc, gen_ctx->target_ctx);\n  gen_ctx->target_ctx = NULL;\n}\n"
        },
        {
          "name": "mir-gen-stub.c",
          "type": "blob",
          "size": 3.8017578125,
          "content": "/* This file is a part of MIR project.\n   Copyright (C) 2018-2024 Vladimir Makarov <vmakarov.gcc@gmail.com>.\n\n   Stub for MIR generator machine dependent file.  It contains\n   definitions used by MIR generator.  You can use this file for\n   successful compilation of mir-gen.c.\n\n   See HOW-TO-PORT-MIR.md document for the definitions description.\n*/\n\nenum {\n  R0_HARD_REG,\n  R1_HARD_REG,\n  R2_HARD_REG,\n  R3_HARD_REG,\n  R4_HARD_REG,\n  R5_HARD_REG,\n  R6_HARD_REG,\n  R7_HARD_REG,\n  F0_HARD_REG,\n  F1_HARD_REG,\n  F2_HARD_REG,\n  F3_HARD_REG,\n  F4_HARD_REG,\n  F5_HARD_REG,\n  F6_HARD_REG,\n  F7_HARD_REG\n};\n\nstatic const MIR_reg_t MAX_HARD_REG = F7_HARD_REG; /* max value for the previous regs */\nstatic const MIR_reg_t FP_HARD_REG = R6_HARD_REG;  /* stack frame pointer according ABI */\nstatic const MIR_reg_t SP_HARD_REG = R7_HARD_REG;  /* stack pointer according ABI */\n\nconst MIR_reg_t TEMP_INT_HARD_REG1 = R2_HARD_REG, TEMP_INT_HARD_REG2 = R3_HARD_REG;\nconst MIR_reg_t TEMP_FLOAT_HARD_REG1 = F2_HARD_REG, TEMP_FLOAT_HARD_REG2 = F3_HARD_REG;\nconst MIR_reg_t TEMP_DOUBLE_HARD_REG1 = F2_HARD_REG, TEMP_DOUBLE_HARD_REG2 = F3_HARD_REG;\nconst MIR_reg_t TEMP_LDOUBLE_HARD_REG1 = F2_HARD_REG;\nconst MIR_reg_t TEMP_LDOUBLE_HARD_REG2 = F3_HARD_REG;\n\nstatic int target_locs_num (MIR_reg_t loc, MIR_type_t type) {\n  return loc > MAX_HARD_REG && type == MIR_T_LD ? 2 : 1;\n}\n\nstatic inline int target_hard_reg_type_ok_p (MIR_reg_t hard_reg, MIR_type_t type) {\n  assert (hard_reg <= MAX_HARD_REG);\n  return (type == MIR_T_F || type == MIR_T_D || type == MIR_T_LD ? hard_reg >= F0_HARD_REG\n                                                                 : hard_reg < F0_HARD_REG);\n}\n\nstatic inline int target_fixed_hard_reg_p (MIR_reg_t hard_reg) {\n  assert (hard_reg <= MAX_HARD_REG);\n  return (hard_reg == FP_HARD_REG || hard_reg == SP_HARD_REG || hard_reg == TEMP_INT_HARD_REG1\n          || hard_reg == TEMP_INT_HARD_REG2 || hard_reg == TEMP_FLOAT_HARD_REG1\n          || hard_reg == TEMP_FLOAT_HARD_REG2 || hard_reg == TEMP_DOUBLE_HARD_REG1\n          || hard_reg == TEMP_DOUBLE_HARD_REG2 || hard_reg == TEMP_LDOUBLE_HARD_REG1\n          || hard_reg == TEMP_LDOUBLE_HARD_REG2);\n}\n\nstatic inline int target_call_used_hard_reg_p (MIR_reg_t hard_reg) {\n  assert (hard_reg <= MAX_HARD_REG);\n  return !((hard_reg >= R4_HARD_REG && hard_reg <= R5_HARD_REG)\n           || (hard_reg >= F2_HARD_REG && hard_reg <= F7_HARD_REG));\n}\n\nstatic const int slots_offset = 176; /* It is used in this file but not in MIR generator */\n\nstatic MIR_disp_t target_get_stack_slot_offset (MIR_context_t ctx, MIR_type_t type,\n                                                MIR_reg_t slot) {\n  /* slot is 0, 1, ... */\n  struct gen_ctx *gen_ctx = *gen_ctx_loc (ctx);\n\n  return -((MIR_disp_t) (slot + (type == MIR_T_LD ? 2 : 1)) * 8 + slots_offset);\n}\n\nstatic const MIR_insn_code_t target_io_dup_op_insn_codes[] = {MIR_INSN_BOUND};\n\nstatic int target_valid_mem_offset_p (gen_ctx_t gen_ctx, MIR_type_t type, MIR_disp_t offset) {\n  return TRUE;\n}\n\nstatic void target_machinize (MIR_context_t ctx) {}\n\nstatic void target_make_prolog_epilog (MIR_context_t ctx, bitmap_t used_hard_regs,\n                                       size_t stack_slots_num) {}\n\nstatic void target_get_early_clobbered_hard_regs (MIR_insn_t insn, MIR_reg_t *hr1, MIR_reg_t *hr2) {\n  *hr1 = *hr2 = MIR_NON_HARD_REG;\n}\n\nstatic int target_insn_ok_p (MIR_context_t ctx, MIR_insn_t insn) { return FALSE; }\nstatic uint8_t *target_translate (MIR_context_t ctx, size_t *len) { return NULL; }\nstatic void target_rebase (MIR_context_t ctx, uint8_t *base) {}\nstatic void target_change_to_direct_calls (MIR_context_t ctx) {}\n\nstatic void target_init (MIR_context_t ctx) {\n  fprintf (stderr, \"Your generator target dependent file is just a stub!\\n\");\n  fprintf (stderr, \"MIR generator can not use it -- good bye.\\n\");\n  exit (1);\n}\nstatic void target_finish (MIR_context_t ctx) {}\n"
        },
        {
          "name": "mir-gen-x86_64.c",
          "type": "blob",
          "size": 131.8603515625,
          "content": "/* This file is a part of MIR project.\n   Copyright (C) 2018-2024 Vladimir Makarov <vmakarov.gcc@gmail.com>.\n*/\n\n#include <limits.h>\n\n#include \"mir-x86_64.h\"\n\nstatic const MIR_reg_t FP_HARD_REG = BP_HARD_REG;\n\nstatic inline MIR_reg_t target_nth_loc (MIR_reg_t loc, MIR_type_t type MIR_UNUSED, int n) {\n  return loc + n;\n}\n\nstatic inline int target_call_used_hard_reg_p (MIR_reg_t hard_reg, MIR_type_t type MIR_UNUSED) {\n  assert (hard_reg <= MAX_HARD_REG);\n#ifndef _WIN32\n  return !(hard_reg == BX_HARD_REG || (hard_reg >= R12_HARD_REG && hard_reg <= R15_HARD_REG));\n#else\n  return !(hard_reg == BX_HARD_REG || hard_reg == SI_HARD_REG || hard_reg == DI_HARD_REG\n           || (hard_reg >= R12_HARD_REG && hard_reg <= R15_HARD_REG)\n           || (hard_reg >= XMM6_HARD_REG && hard_reg <= XMM15_HARD_REG));\n#endif\n}\n\n/* Stack layout (sp refers to the last reserved stack slot address)\n   from higher address to lower address memory:\n\n      FP is required:                                      FP omitted:\n\n|               |  prev func stack frame               |               |\n| ...           |  (start addr aligned to 16 bytes)    | ...           |\n|---------------|  \t\t\t\t       |---------------|\n| return pc     |  sp before call = start sp hard reg  |               |\n|               |  absent for jcall/jret func          | return pc     |\n|---------------|\t\t\t\t       |               |\n| old bp        |  new bp refers here\t\t       |               |\n|---------------|     \t\t\t\t       |---------------|\n|   reg save    |  176 bytes optional area for\t       |   reg save    |\n|     area      |  vararg func reg save area\t       |     area      |\n|---------------|\t\t\t\t       |---------------|\n| slots assigned|  can be absent for small functions   | saved regs    |\n|   to pseudos  |     (known only after RA)\t       |               |\n|---------------|\t\t\t\t       |               |\n| saved regs    |  callee saved regs used in the func  |               |\n|---------------|     (known only after RA)\t       |---------------|\n| alloca areas  |  optional\t\t\t       |               |\n|---------------|\t\t\t\t       | slot assigned |\n| slots for     |  dynamically reserved/freed\t       |   to pseudos  |\n|  passing args |      by caller\t\t       |               |\n|---------------|\t\t\t\t       |---------------|\n|  spill space  |  WIN32 only: 32 bytes spill space    |  spill space  |\n|---------------|     for reg args (allocated at call) |---------------|\n\n   size of slots and saved regs is multiple of 16 bytes\n   will be fp ommited is defined after machinize\n\n */\n\n#ifndef _WIN32\nstatic const int reg_save_area_size = 176;\nstatic const int spill_space_size = 0;\n#else\nstatic const int reg_save_area_size = 0;\nstatic const int spill_space_size = 32;\n#endif\n\nstatic const MIR_insn_code_t target_io_dup_op_insn_codes[] = {\n  /* see possible patterns */\n  MIR_ADD,   MIR_ADDS,  MIR_FADD,   MIR_DADD,       MIR_LDADD, MIR_SUB,   MIR_SUBS,  MIR_FSUB,\n  MIR_DSUB,  MIR_LDSUB, MIR_MUL,    MIR_MULS,       MIR_FMUL,  MIR_DMUL,  MIR_LDMUL, MIR_FDIV,\n  MIR_DDIV,  MIR_LDDIV, MIR_AND,    MIR_ANDS,       MIR_OR,    MIR_ORS,   MIR_XOR,   MIR_XORS,\n  MIR_LSH,   MIR_LSHS,  MIR_RSH,    MIR_RSHS,       MIR_URSH,  MIR_URSHS, MIR_NEG,   MIR_NEGS,\n  MIR_FNEG,  MIR_DNEG,  MIR_LDNEG,  MIR_ADDO,       MIR_ADDOS, MIR_SUBO,  MIR_SUBOS, MIR_MULO,\n  MIR_MULOS, MIR_UMULO, MIR_UMULOS, MIR_INSN_BOUND,\n};\n\nstatic MIR_insn_code_t get_ext_code (MIR_type_t type) {\n  switch (type) {\n  case MIR_T_I8: return MIR_EXT8;\n  case MIR_T_U8: return MIR_UEXT8;\n  case MIR_T_I16: return MIR_EXT16;\n  case MIR_T_U16: return MIR_UEXT16;\n  case MIR_T_I32: return MIR_EXT32;\n  case MIR_T_U32: return MIR_UEXT32;\n  default: return MIR_INVALID_INSN;\n  }\n}\n\nstatic MIR_reg_t get_fp_arg_reg (size_t fp_arg_num) {\n  switch (fp_arg_num) {\n  case 0:\n  case 1:\n  case 2:\n  case 3:\n#ifndef _WIN32\n  case 4:\n  case 5:\n  case 6:\n  case 7:\n#endif\n    return (MIR_reg_t) (XMM0_HARD_REG + fp_arg_num);\n  default: return MIR_NON_VAR;\n  }\n}\n\nstatic MIR_reg_t get_int_arg_reg (size_t int_arg_num) {\n  switch (int_arg_num\n#ifdef _WIN32\n          + 2\n#endif\n  ) {\n  case 0: return DI_HARD_REG;\n  case 1: return SI_HARD_REG;\n#ifdef _WIN32\n  case 2: return CX_HARD_REG;\n  case 3: return DX_HARD_REG;\n#else\n  case 2: return DX_HARD_REG;\n  case 3: return CX_HARD_REG;\n#endif\n  case 4: return R8_HARD_REG;\n  case 5: return R9_HARD_REG;\n  default: return MIR_NON_VAR;\n  }\n}\n\n#ifdef _WIN32\nstatic int get_int_arg_reg_num (MIR_reg_t arg_reg) {\n  switch (arg_reg) {\n  case CX_HARD_REG: return 0;\n  case DX_HARD_REG: return 1;\n  case R8_HARD_REG: return 2;\n  case R9_HARD_REG: return 3;\n  default: assert (FALSE); return 0;\n  }\n}\n#endif\n\nstatic MIR_reg_t get_arg_reg (MIR_type_t arg_type, size_t *int_arg_num, size_t *fp_arg_num,\n                              MIR_insn_code_t *mov_code) {\n  MIR_reg_t arg_reg;\n\n  if (arg_type == MIR_T_LD) {\n    arg_reg = MIR_NON_VAR;\n    *mov_code = MIR_LDMOV;\n  } else if (arg_type == MIR_T_F || arg_type == MIR_T_D) {\n    arg_reg = get_fp_arg_reg (*fp_arg_num);\n    (*fp_arg_num)++;\n#ifdef _WIN32\n    (*int_arg_num)++; /* arg slot used by fp, skip int register */\n#endif\n    *mov_code = arg_type == MIR_T_F ? MIR_FMOV : MIR_DMOV;\n  } else { /* including RBLK */\n    arg_reg = get_int_arg_reg (*int_arg_num);\n#ifdef _WIN32\n    (*fp_arg_num)++; /* arg slot used by int, skip fp register */\n#endif\n    (*int_arg_num)++;\n    *mov_code = MIR_MOV;\n  }\n  return arg_reg;\n}\n\nstatic void gen_mov (gen_ctx_t gen_ctx, MIR_insn_t anchor, MIR_insn_code_t code, MIR_op_t dst_op,\n                     MIR_op_t src_op) {\n  gen_add_insn_before (gen_ctx, anchor, MIR_new_insn (gen_ctx->ctx, code, dst_op, src_op));\n}\n\nstatic void prohibit_omitting_fp (gen_ctx_t gen_ctx);\n\nstatic void machinize_call (gen_ctx_t gen_ctx, MIR_insn_t call_insn) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  MIR_func_t func = curr_func_item->u.func;\n  MIR_proto_t proto = call_insn->ops[0].u.ref->u.proto;\n  size_t size, nargs, nops = MIR_insn_nops (ctx, call_insn), start = proto->nres + 2;\n  size_t int_arg_num = 0, fp_arg_num = 0, xmm_args = 0;\n  size_t init_arg_stack_size = spill_space_size, arg_stack_size = init_arg_stack_size;\n#ifdef _WIN32\n  size_t block_offset = spill_space_size;\n#endif\n  MIR_type_t type, mem_type;\n  MIR_op_mode_t mode;\n  MIR_var_t *arg_vars = NULL;\n  MIR_reg_t arg_reg;\n  MIR_op_t arg_op, new_arg_op, temp_op, ret_reg_op, mem_op;\n  MIR_insn_code_t new_insn_code, ext_code;\n  MIR_insn_t new_insn, prev_insn, next_insn, ext_insn;\n  MIR_insn_t prev_call_insn = DLIST_PREV (MIR_insn_t, call_insn);\n  uint32_t n_iregs, n_xregs, n_fregs;\n\n  assert (prev_call_insn != NULL);\n  if (call_insn->code == MIR_INLINE) call_insn->code = MIR_CALL;\n  if (proto->args == NULL) {\n    nargs = 0;\n  } else {\n    gen_assert (nops >= VARR_LENGTH (MIR_var_t, proto->args)\n                && (proto->vararg_p || nops - start == VARR_LENGTH (MIR_var_t, proto->args)));\n    nargs = VARR_LENGTH (MIR_var_t, proto->args);\n    arg_vars = VARR_ADDR (MIR_var_t, proto->args);\n  }\n  if (call_insn->ops[1].mode != MIR_OP_VAR && call_insn->ops[1].mode != MIR_OP_REF) {\n    temp_op = _MIR_new_var_op (ctx, gen_new_temp_reg (gen_ctx, MIR_T_I64, func));\n    new_insn = MIR_new_insn (ctx, MIR_MOV, temp_op, call_insn->ops[1]);\n    call_insn->ops[1] = temp_op;\n    gen_add_insn_before (gen_ctx, call_insn, new_insn);\n  }\n#ifdef _WIN32\n  if ((nops - start) > 4) block_offset = (nops - start) * 8;\n#endif\n  for (size_t i = start; i < nops; i++) {\n    arg_op = call_insn->ops[i];\n    gen_assert (arg_op.mode == MIR_OP_VAR\n                || (arg_op.mode == MIR_OP_VAR_MEM && MIR_all_blk_type_p (arg_op.u.var_mem.type)));\n    if (i - start < nargs) {\n      type = arg_vars[i - start].type;\n    } else if (arg_op.mode == MIR_OP_VAR_MEM) {\n      type = arg_op.u.var_mem.type;\n      assert (MIR_all_blk_type_p (type));\n    } else {\n      mode = call_insn->ops[i].value_mode;  // ??? smaller ints\n      gen_assert (mode == MIR_OP_INT || mode == MIR_OP_UINT || mode == MIR_OP_FLOAT\n                  || mode == MIR_OP_DOUBLE || mode == MIR_OP_LDOUBLE);\n      if (mode == MIR_OP_FLOAT)\n        (*MIR_get_error_func (ctx)) (MIR_call_op_error,\n                                     \"passing float variadic arg (should be passed as double)\");\n      type = mode == MIR_OP_DOUBLE ? MIR_T_D : mode == MIR_OP_LDOUBLE ? MIR_T_LD : MIR_T_I64;\n    }\n    if (xmm_args < 8 && (type == MIR_T_F || type == MIR_T_D)) xmm_args++;\n    ext_insn = NULL;\n    if ((ext_code = get_ext_code (type)) != MIR_INVALID_INSN) { /* extend arg if necessary */\n      temp_op = _MIR_new_var_op (ctx, gen_new_temp_reg (gen_ctx, MIR_T_I64, func));\n      ext_insn = MIR_new_insn (ctx, ext_code, temp_op, arg_op);\n      call_insn->ops[i] = arg_op = temp_op;\n    }\n    size = 0;\n    if (MIR_blk_type_p (type)) {\n      gen_assert (arg_op.mode == MIR_OP_VAR_MEM);\n      size = (arg_op.u.var_mem.disp + 7) / 8 * 8;\n      gen_assert (prev_call_insn != NULL); /* call_insn should not be 1st after simplification */\n    }\n#ifndef _WIN32\n    if ((type == MIR_T_BLK + 1 && get_int_arg_reg (int_arg_num) != MIR_NON_VAR\n         && (size <= 8 || get_int_arg_reg (int_arg_num + 1) != MIR_NON_VAR))\n        || (type == MIR_T_BLK + 2 && get_fp_arg_reg (fp_arg_num) != MIR_NON_VAR\n            && (size <= 8 || get_fp_arg_reg (fp_arg_num + 1) != MIR_NON_VAR))) {\n      /* all is passed in gprs or fprs */\n      MIR_type_t mov_type = type == MIR_T_BLK + 1 ? MIR_T_I64 : MIR_T_D;\n      MIR_insn_code_t mov_code;\n      MIR_reg_t reg2, reg1 = get_arg_reg (mov_type, &int_arg_num, &fp_arg_num, &mov_code);\n\n      assert (size <= 16);\n      new_insn = MIR_new_insn (ctx, mov_code, _MIR_new_var_op (ctx, reg1),\n                               _MIR_new_var_mem_op (ctx, mov_type, 0, arg_op.u.var_mem.base,\n                                                    MIR_NON_VAR, 1));\n      gen_add_insn_before (gen_ctx, call_insn, new_insn);\n      setup_call_hard_reg_args (gen_ctx, call_insn, reg1);\n      call_insn->ops[i].u.var_mem.base = MIR_NON_VAR; /* not used anymore */\n      if (size > 8) {\n        reg2 = get_arg_reg (mov_type, &int_arg_num, &fp_arg_num, &mov_code);\n        new_insn = MIR_new_insn (ctx, mov_code, _MIR_new_var_op (ctx, reg2),\n                                 _MIR_new_var_mem_op (ctx, mov_type, 8, arg_op.u.var_mem.base,\n                                                      MIR_NON_VAR, 1));\n        gen_add_insn_before (gen_ctx, call_insn, new_insn);\n        setup_call_hard_reg_args (gen_ctx, call_insn, reg2);\n      }\n      continue;\n    } else if ((type == MIR_T_BLK + 3 || type == MIR_T_BLK + 4)\n               && get_int_arg_reg (int_arg_num) != MIR_NON_VAR\n               && get_fp_arg_reg (fp_arg_num) != MIR_NON_VAR) {\n      /* gpr and then fpr or fpr and then gpr */\n      MIR_type_t mov_type1 = type == MIR_T_BLK + 3 ? MIR_T_I64 : MIR_T_D;\n      MIR_type_t mov_type2 = type == MIR_T_BLK + 3 ? MIR_T_D : MIR_T_I64;\n      MIR_insn_code_t mov_code1, mov_code2;\n      MIR_reg_t reg1 = get_arg_reg (mov_type1, &int_arg_num, &fp_arg_num, &mov_code1);\n      MIR_reg_t reg2 = get_arg_reg (mov_type2, &int_arg_num, &fp_arg_num, &mov_code2);\n\n      assert (size > 8 && size <= 16);\n      new_insn = MIR_new_insn (ctx, mov_code1, _MIR_new_var_op (ctx, reg1),\n                               _MIR_new_var_mem_op (ctx, mov_type1, 0, arg_op.u.var_mem.base,\n                                                    MIR_NON_VAR, 1));\n      setup_call_hard_reg_args (gen_ctx, call_insn, reg1);\n      call_insn->ops[i].u.var_mem.base = MIR_NON_VAR; /* not used anymore */\n      gen_add_insn_before (gen_ctx, call_insn, new_insn);\n      new_insn = MIR_new_insn (ctx, mov_code2, _MIR_new_var_op (ctx, reg2),\n                               _MIR_new_var_mem_op (ctx, mov_type2, 8, arg_op.u.var_mem.base,\n                                                    MIR_NON_VAR, 1));\n      gen_add_insn_before (gen_ctx, call_insn, new_insn);\n      setup_call_hard_reg_args (gen_ctx, call_insn, reg2);\n      continue;\n    }\n#endif\n    if (MIR_blk_type_p (type)) { /* put block arg on the stack */\n      MIR_insn_t load_insn;\n      size_t disp, dest_disp, start_dest_disp;\n      int first_p, by_val_p = FALSE;\n\n#ifdef _WIN32\n      by_val_p = size <= 8;\n#endif\n      if (by_val_p) {\n        temp_op = _MIR_new_var_op (ctx, gen_new_temp_reg (gen_ctx, MIR_T_I64, func));\n        mem_op = _MIR_new_var_mem_op (ctx, MIR_T_I64, 0, arg_op.u.var_mem.base, MIR_NON_VAR, 1);\n        load_insn = MIR_new_insn (ctx, MIR_MOV, temp_op, mem_op);\n        gen_add_insn_after (gen_ctx, prev_call_insn, load_insn);\n        arg_op = temp_op;\n      } else if (size > 0 && size <= 2 * 8) { /* upto 2 moves */\n        disp = 0;\n        first_p = TRUE;\n        temp_op = _MIR_new_var_op (ctx, gen_new_temp_reg (gen_ctx, MIR_T_I64, func));\n        while (size != 0) {\n          mem_op\n            = _MIR_new_var_mem_op (ctx, MIR_T_I64, disp, arg_op.u.var_mem.base, MIR_NON_VAR, 1);\n          load_insn = MIR_new_insn (ctx, MIR_MOV, temp_op, mem_op);\n          gen_add_insn_after (gen_ctx, prev_call_insn, load_insn);\n          disp += 8;\n#ifdef _WIN32\n          dest_disp = block_offset;\n          if (first_p) start_dest_disp = dest_disp;\n          block_offset += 8;\n#else\n          dest_disp = arg_stack_size;\n          arg_stack_size += 8;\n#endif\n          mem_op = _MIR_new_var_mem_op (ctx, MIR_T_I64, dest_disp, SP_HARD_REG, MIR_NON_VAR, 1);\n          new_insn = MIR_new_insn (ctx, MIR_MOV, mem_op, temp_op);\n          size -= 8;\n          gen_add_insn_after (gen_ctx, load_insn, new_insn);\n          if (first_p) {\n            call_insn->ops[i]\n              = _MIR_new_var_mem_op (ctx, type, dest_disp, SP_HARD_REG, MIR_NON_VAR, 1);\n            first_p = FALSE;\n          }\n        }\n#ifdef _WIN32\n        arg_op\n          = _MIR_new_var_op (ctx, gen_new_temp_reg (gen_ctx, MIR_T_I64, curr_func_item->u.func));\n        new_insn = MIR_new_insn (ctx, MIR_ADD, arg_op, _MIR_new_var_op (ctx, SP_HARD_REG),\n                                 MIR_new_int_op (ctx, start_dest_disp));\n        gen_add_insn_before (gen_ctx, call_insn, new_insn);\n#endif\n      } else { /* generate memcpy call before call arg moves */\n        MIR_reg_t dest_reg;\n        MIR_op_t freg_op, dest_reg_op, ops[5];\n        MIR_item_t memcpy_proto_item\n          = _MIR_builtin_proto (ctx, curr_func_item->module, \"mir.arg_memcpy.p\", 0, NULL, 3,\n                                MIR_T_I64, \"dest\", MIR_T_I64, \"src\", MIR_T_I64, \"n\");\n        MIR_item_t memcpy_import_item\n          = _MIR_builtin_func (ctx, curr_func_item->module, \"mir.arg_memcpy\", memcpy);\n        freg_op\n          = _MIR_new_var_op (ctx, gen_new_temp_reg (gen_ctx, MIR_T_I64, curr_func_item->u.func));\n        dest_reg = gen_new_temp_reg (gen_ctx, MIR_T_I64, curr_func_item->u.func);\n        dest_reg_op = _MIR_new_var_op (ctx, dest_reg);\n        ops[0] = MIR_new_ref_op (ctx, memcpy_proto_item);\n        ops[1] = freg_op;\n        ops[2] = _MIR_new_var_op (ctx, get_int_arg_reg (0));\n        ops[3] = _MIR_new_var_op (ctx, get_int_arg_reg (1));\n        ops[4] = _MIR_new_var_op (ctx, get_int_arg_reg (2));\n        new_insn = MIR_new_insn_arr (ctx, MIR_CALL, 5, ops);\n        gen_add_insn_after (gen_ctx, prev_call_insn, new_insn);\n        new_insn = MIR_new_insn (ctx, MIR_MOV, ops[4], MIR_new_int_op (ctx, size));\n        gen_add_insn_after (gen_ctx, prev_call_insn, new_insn);\n        new_insn\n          = MIR_new_insn (ctx, MIR_MOV, ops[3], _MIR_new_var_op (ctx, arg_op.u.var_mem.base));\n        gen_add_insn_after (gen_ctx, prev_call_insn, new_insn);\n        new_insn = MIR_new_insn (ctx, MIR_MOV, ops[2], dest_reg_op);\n        gen_add_insn_after (gen_ctx, prev_call_insn, new_insn);\n#ifdef _WIN32\n        start_dest_disp = block_offset;\n        block_offset += size;\n#else\n        start_dest_disp = arg_stack_size;\n        arg_stack_size += size;\n#endif\n        new_insn = MIR_new_insn (ctx, MIR_ADD, dest_reg_op, _MIR_new_var_op (ctx, SP_HARD_REG),\n                                 MIR_new_int_op (ctx, start_dest_disp));\n        gen_add_insn_after (gen_ctx, prev_call_insn, new_insn);\n        new_insn = MIR_new_insn (ctx, MIR_MOV, ops[1], MIR_new_ref_op (ctx, memcpy_import_item));\n        gen_add_insn_after (gen_ctx, prev_call_insn, new_insn);\n        call_insn->ops[i]\n          = _MIR_new_var_mem_op (ctx, MIR_T_BLK, arg_op.u.var_mem.disp, dest_reg, MIR_NON_VAR, 1);\n#ifdef _WIN32\n        arg_op = dest_reg_op;\n#endif\n      }\n#ifdef _WIN32\n      if ((arg_reg = get_arg_reg (MIR_T_P, &int_arg_num, &fp_arg_num, &new_insn_code))\n          != MIR_NON_VAR) {\n        new_arg_op = _MIR_new_var_op (ctx, arg_reg);\n        new_insn = MIR_new_insn (ctx, MIR_MOV, new_arg_op, arg_op);\n        call_insn->ops[i] = new_arg_op;\n      } else {\n        mem_op = _MIR_new_var_mem_op (ctx, MIR_T_I64, arg_stack_size, SP_HARD_REG, MIR_NON_VAR, 1);\n        new_insn = MIR_new_insn (ctx, MIR_MOV, mem_op, arg_op);\n        call_insn->ops[i] = mem_op;\n        arg_stack_size += 8;\n      }\n      gen_add_insn_before (gen_ctx, call_insn, new_insn);\n#endif\n    } else if ((arg_reg = get_arg_reg (type, &int_arg_num, &fp_arg_num, &new_insn_code))\n               != MIR_NON_VAR) {\n      /* put arguments to argument hard regs */\n      if (ext_insn != NULL) gen_add_insn_before (gen_ctx, call_insn, ext_insn);\n      if (type != MIR_T_RBLK) {\n        new_arg_op = _MIR_new_var_op (ctx, arg_reg);\n        new_insn = MIR_new_insn (ctx, new_insn_code, new_arg_op, arg_op);\n      } else {\n        assert (arg_op.mode == MIR_OP_VAR_MEM);\n        new_insn = MIR_new_insn (ctx, new_insn_code, _MIR_new_var_op (ctx, arg_reg),\n                                 _MIR_new_var_op (ctx, arg_op.u.var_mem.base));\n        new_arg_op\n          = _MIR_new_var_mem_op (ctx, MIR_T_RBLK, arg_op.u.var_mem.disp, arg_reg, MIR_NON_VAR, 1);\n      }\n      gen_add_insn_before (gen_ctx, call_insn, new_insn);\n      call_insn->ops[i] = new_arg_op;\n#ifdef _WIN32\n      /* copy fp reg varargs into corresponding int regs */\n      if (proto->vararg_p && type == MIR_T_D) {\n        gen_assert (int_arg_num > 0 && int_arg_num <= 4);\n        arg_reg = get_int_arg_reg (int_arg_num - 1);\n        setup_call_hard_reg_args (gen_ctx, call_insn, arg_reg);\n        /* mir does not support moving fp to int regs directly, spill and load them instead */\n        mem_op = _MIR_new_var_mem_op (ctx, MIR_T_D, 8, SP_HARD_REG, MIR_NON_VAR, 1);\n        new_insn = MIR_new_insn (ctx, MIR_DMOV, mem_op, arg_op);\n        gen_add_insn_before (gen_ctx, call_insn, new_insn);\n        mem_op = _MIR_new_var_mem_op (ctx, MIR_T_I64, 8, SP_HARD_REG, MIR_NON_VAR, 1);\n        new_insn = MIR_new_insn (ctx, MIR_MOV, _MIR_new_var_op (ctx, arg_reg), mem_op);\n        gen_add_insn_before (gen_ctx, call_insn, new_insn);\n      }\n#endif\n    } else { /* put arguments on the stack */\n      if (type == MIR_T_RBLK) {\n        assert (arg_op.mode == MIR_OP_VAR_MEM);\n        arg_op = _MIR_new_var_op (ctx, arg_op.u.var_mem.base);\n      }\n      mem_type = type == MIR_T_F || type == MIR_T_D || type == MIR_T_LD ? type : MIR_T_I64;\n      new_insn_code = (type == MIR_T_F    ? MIR_FMOV\n                       : type == MIR_T_D  ? MIR_DMOV\n                       : type == MIR_T_LD ? MIR_LDMOV\n                                          : MIR_MOV);\n      mem_op = _MIR_new_var_mem_op (ctx, mem_type, arg_stack_size, SP_HARD_REG, MIR_NON_VAR, 1);\n      new_insn = MIR_new_insn (ctx, new_insn_code, mem_op, arg_op);\n      gen_assert (prev_call_insn != NULL); /* call_insn should not be 1st after simplification */\n      MIR_insert_insn_after (ctx, curr_func_item, prev_call_insn, new_insn);\n      prev_insn = DLIST_PREV (MIR_insn_t, new_insn);\n      next_insn = DLIST_NEXT (MIR_insn_t, new_insn);\n      create_new_bb_insns (gen_ctx, prev_insn, next_insn, call_insn);\n      call_insn->ops[i] = mem_op;\n#ifdef _WIN32\n      arg_stack_size += 8;\n#else\n      arg_stack_size += type == MIR_T_LD ? 16 : 8;\n#endif\n      if (ext_insn != NULL) gen_add_insn_after (gen_ctx, prev_call_insn, ext_insn);\n    }\n  }\n#ifndef _WIN32\n  if (proto->vararg_p) {\n    setup_call_hard_reg_args (gen_ctx, call_insn, AX_HARD_REG);\n    new_insn = MIR_new_insn (ctx, MIR_MOV, _MIR_new_var_op (ctx, AX_HARD_REG),\n                             MIR_new_int_op (ctx, xmm_args));\n    gen_add_insn_before (gen_ctx, call_insn, new_insn);\n  }\n#else\n  if (proto->nres > 1)\n    (*MIR_get_error_func (ctx)) (MIR_ret_error,\n                                 \"Windows x86-64 doesn't support multiple return values\");\n#endif\n  n_iregs = n_xregs = n_fregs = 0;\n  for (size_t i = 0; i < proto->nres; i++) {\n    ret_reg_op = call_insn->ops[i + 2];\n    gen_assert (ret_reg_op.mode == MIR_OP_VAR);\n    if (proto->res_types[i] == MIR_T_F && n_xregs < 2) {\n      new_insn = MIR_new_insn (ctx, MIR_FMOV, ret_reg_op,\n                               _MIR_new_var_op (ctx, n_xregs == 0 ? XMM0_HARD_REG : XMM1_HARD_REG));\n      n_xregs++;\n    } else if (proto->res_types[i] == MIR_T_D && n_xregs < 2) {\n      new_insn = MIR_new_insn (ctx, MIR_DMOV, ret_reg_op,\n                               _MIR_new_var_op (ctx, n_xregs == 0 ? XMM0_HARD_REG : XMM1_HARD_REG));\n      n_xregs++;\n    } else if (proto->res_types[i] == MIR_T_LD && n_fregs < 2) {\n      new_insn = MIR_new_insn (ctx, MIR_LDMOV, ret_reg_op,\n                               _MIR_new_var_op (ctx, n_fregs == 0 ? ST0_HARD_REG : ST1_HARD_REG));\n      n_fregs++;\n    } else if (n_iregs < 2) {\n      new_insn = MIR_new_insn (ctx, MIR_MOV, ret_reg_op,\n                               _MIR_new_var_op (ctx, n_iregs == 0 ? AX_HARD_REG : DX_HARD_REG));\n      n_iregs++;\n    } else {\n      (*MIR_get_error_func (ctx)) (MIR_ret_error,\n                                   \"x86-64 can not handle this combination of return values\");\n    }\n    MIR_insert_insn_after (ctx, curr_func_item, call_insn, new_insn);\n    call_insn->ops[i + 2] = new_insn->ops[1];\n    if ((ext_code = get_ext_code (proto->res_types[i])) != MIR_INVALID_INSN) {\n      MIR_insert_insn_after (ctx, curr_func_item, new_insn,\n                             MIR_new_insn (ctx, ext_code, ret_reg_op, ret_reg_op));\n      new_insn = DLIST_NEXT (MIR_insn_t, new_insn);\n    }\n    create_new_bb_insns (gen_ctx, call_insn, DLIST_NEXT (MIR_insn_t, new_insn), call_insn);\n  }\n#ifdef _WIN32\n  if (block_offset > arg_stack_size) arg_stack_size = block_offset;\n#endif\n  if (arg_stack_size != 0) { /* allocate/deallocate stack for args passed on stack */\n    arg_stack_size = (arg_stack_size + 15) / 16 * 16; /* make it of several 16 bytes */\n    new_insn\n      = MIR_new_insn (ctx, MIR_SUB, _MIR_new_var_op (ctx, SP_HARD_REG),\n                      _MIR_new_var_op (ctx, SP_HARD_REG), MIR_new_int_op (ctx, arg_stack_size));\n    MIR_insert_insn_after (ctx, curr_func_item, prev_call_insn, new_insn);\n    next_insn = DLIST_NEXT (MIR_insn_t, new_insn);\n    create_new_bb_insns (gen_ctx, prev_call_insn, next_insn, call_insn);\n    new_insn\n      = MIR_new_insn (ctx, MIR_ADD, _MIR_new_var_op (ctx, SP_HARD_REG),\n                      _MIR_new_var_op (ctx, SP_HARD_REG), MIR_new_int_op (ctx, arg_stack_size));\n    MIR_insert_insn_after (ctx, curr_func_item, call_insn, new_insn);\n    next_insn = DLIST_NEXT (MIR_insn_t, new_insn);\n    create_new_bb_insns (gen_ctx, call_insn, next_insn, call_insn);\n  }\n  if (arg_stack_size != 0) prohibit_omitting_fp (gen_ctx);\n}\n\nstatic float mir_ui2f (uint64_t i) { return (float) i; }\nstatic double mir_ui2d (uint64_t i) { return (double) i; }\nstatic long double mir_ui2ld (uint64_t i) { return (long double) i; }\nstatic int64_t mir_ld2i (long double ld) { return (int64_t) ld; }\nstatic const char *UI2F = \"mir.ui2f\";\nstatic const char *UI2D = \"mir.ui2d\";\nstatic const char *UI2LD = \"mir.ui2ld\";\nstatic const char *LD2I = \"mir.ld2i\";\nstatic const char *UI2F_P = \"mir.ui2f.p\";\nstatic const char *UI2D_P = \"mir.ui2d.p\";\nstatic const char *UI2LD_P = \"mir.ui2ld.p\";\nstatic const char *LD2I_P = \"mir.ld2i.p\";\n\nstatic const char *VA_ARG_P = \"mir.va_arg.p\";\nstatic const char *VA_ARG = \"mir.va_arg\";\nstatic const char *VA_BLOCK_ARG_P = \"mir.va_block_arg.p\";\nstatic const char *VA_BLOCK_ARG = \"mir.va_block_arg\";\n\nstatic void get_builtin (gen_ctx_t gen_ctx, MIR_insn_code_t code, MIR_item_t *proto_item,\n                         MIR_item_t *func_import_item) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  MIR_type_t res_type;\n\n  *func_import_item = *proto_item = NULL; /* to remove uninitialized warning */\n  switch (code) {\n  case MIR_UI2F:\n    res_type = MIR_T_F;\n    *proto_item\n      = _MIR_builtin_proto (ctx, curr_func_item->module, UI2F_P, 1, &res_type, 1, MIR_T_I64, \"v\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, UI2F, mir_ui2f);\n    break;\n  case MIR_UI2D:\n    res_type = MIR_T_D;\n    *proto_item\n      = _MIR_builtin_proto (ctx, curr_func_item->module, UI2D_P, 1, &res_type, 1, MIR_T_I64, \"v\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, UI2D, mir_ui2d);\n    break;\n  case MIR_UI2LD:\n    res_type = MIR_T_LD;\n    *proto_item\n      = _MIR_builtin_proto (ctx, curr_func_item->module, UI2LD_P, 1, &res_type, 1, MIR_T_I64, \"v\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, UI2LD, mir_ui2ld);\n    break;\n  case MIR_LD2I:\n    res_type = MIR_T_I64;\n    *proto_item\n      = _MIR_builtin_proto (ctx, curr_func_item->module, LD2I_P, 1, &res_type, 1, MIR_T_LD, \"v\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, LD2I, mir_ld2i);\n    break;\n  case MIR_VA_ARG:\n    res_type = MIR_T_I64;\n    *proto_item = _MIR_builtin_proto (ctx, curr_func_item->module, VA_ARG_P, 1, &res_type, 2,\n                                      MIR_T_I64, \"va\", MIR_T_I64, \"type\");\n    *func_import_item = _MIR_builtin_func (ctx, curr_func_item->module, VA_ARG, va_arg_builtin);\n    break;\n  case MIR_VA_BLOCK_ARG:\n    *proto_item\n      = _MIR_builtin_proto (ctx, curr_func_item->module, VA_BLOCK_ARG_P, 0, NULL, 4, MIR_T_I64,\n                            \"res\", MIR_T_I64, \"va\", MIR_T_I64, \"size\", MIR_T_I64, \"ncase\");\n    *func_import_item\n      = _MIR_builtin_func (ctx, curr_func_item->module, VA_BLOCK_ARG, va_block_arg_builtin);\n    break;\n  default: assert (FALSE);\n  }\n}\n\nstruct insn_pattern_info {\n  int start, num;\n};\n\ntypedef struct insn_pattern_info insn_pattern_info_t;\n\nDEF_VARR (insn_pattern_info_t);\n\nstruct const_ref {\n  int call_p;            /* flag that constant from call insn */\n  MIR_item_t func_item;  /* non-null for constant representing reference to func item */\n  size_t pc;             /* where rel32 address should be in code */\n  size_t next_insn_disp; /* displacement of the next insn */\n  size_t const_num;\n};\n\ntypedef struct const_ref const_ref_t;\nDEF_VARR (const_ref_t);\n\nstruct label_ref {\n  char abs_addr_p, short_p; /* 8 or 32-bit target */\n  size_t label_val_disp, next_insn_disp;\n  union {\n    MIR_label_t label;\n    void *jump_addr; /* absolute addr for BBV */\n  } u;\n};\n\ntypedef struct label_ref label_ref_t;\nDEF_VARR (label_ref_t);\n\n#define MOVDQA_CODE 0\n\nstruct call_ref {\n  MIR_item_t ref_func_item; /* func where the ref is located and referenced func */\n  uint8_t *call_addr;       /* addr of rex call disp32(rip) or call *disp32(rip) */\n};\n\ntypedef struct call_ref call_ref_t;\nDEF_VARR (call_ref_t);\n\nstruct target_ctx {\n  unsigned char alloca_p, block_arg_func_p, leaf_p, keep_fp_p;\n  int start_sp_from_bp_offset;\n  MIR_insn_t temp_jump;\n  int temp_jump_pat_ind;\n  VARR (int) * pattern_indexes, *insn_pattern_indexes;\n  VARR (insn_pattern_info_t) * insn_pattern_info;\n  VARR (uint8_t) * result_code;\n  VARR (uint64_t) * const_pool;\n  VARR (const_ref_t) * const_refs;\n  VARR (label_ref_t) * label_refs;\n  VARR (uint64_t) * abs_address_locs;\n  VARR (MIR_code_reloc_t) * relocs;\n  VARR (call_ref_t) * call_refs;\n};\n\n#define alloca_p gen_ctx->target_ctx->alloca_p\n#define block_arg_func_p gen_ctx->target_ctx->block_arg_func_p\n#define leaf_p gen_ctx->target_ctx->leaf_p\n#define keep_fp_p gen_ctx->target_ctx->keep_fp_p\n#define start_sp_from_bp_offset gen_ctx->target_ctx->start_sp_from_bp_offset\n#define temp_jump gen_ctx->target_ctx->temp_jump\n#define temp_jump_pat_ind gen_ctx->target_ctx->temp_jump_pat_ind\n#define pattern_indexes gen_ctx->target_ctx->pattern_indexes\n#define insn_pattern_indexes gen_ctx->target_ctx->insn_pattern_indexes\n#define insn_pattern_info gen_ctx->target_ctx->insn_pattern_info\n#define result_code gen_ctx->target_ctx->result_code\n#define const_pool gen_ctx->target_ctx->const_pool\n#define const_refs gen_ctx->target_ctx->const_refs\n#define label_refs gen_ctx->target_ctx->label_refs\n#define abs_address_locs gen_ctx->target_ctx->abs_address_locs\n#define relocs gen_ctx->target_ctx->relocs\n\nstatic void prohibit_omitting_fp (gen_ctx_t gen_ctx) { keep_fp_p = TRUE; }\n\nstatic MIR_disp_t target_get_stack_slot_offset (gen_ctx_t gen_ctx, MIR_type_t type,\n                                                MIR_reg_t slot) {\n  /* slot is 0, 1, ... */\n  if (keep_fp_p)\n    return -((MIR_disp_t) (slot + (type == MIR_T_LD ? 2 : 1)) * 8\n             + (curr_func_item->u.func->vararg_p ? reg_save_area_size : 0));\n  return (MIR_disp_t) slot * 8;\n}\n\nstatic MIR_reg_t target_get_stack_slot_base_reg (gen_ctx_t gen_ctx) {\n  return keep_fp_p ? FP_HARD_REG : SP_HARD_REG;\n}\n\nstatic int target_valid_mem_offset_p (gen_ctx_t gen_ctx MIR_UNUSED, MIR_type_t type MIR_UNUSED,\n                                      MIR_disp_t offset MIR_UNUSED) {\n  return TRUE;\n}\n\nstatic void prepend_insn (gen_ctx_t gen_ctx, MIR_insn_t new_insn) {\n  MIR_prepend_insn (gen_ctx->ctx, curr_func_item, new_insn);\n  create_new_bb_insns (gen_ctx, NULL, DLIST_NEXT (MIR_insn_t, new_insn), NULL);\n}\n\nstatic void target_machinize (gen_ctx_t gen_ctx) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  MIR_func_t func;\n  MIR_type_t type, mem_type, res_type;\n  MIR_insn_code_t code, new_insn_code;\n  MIR_insn_t insn, next_insn, new_insn;\n  MIR_reg_t ret_reg, arg_reg;\n  MIR_op_t ret_reg_op, arg_reg_op, mem_op, temp_op;\n  size_t i, blk_size, int_arg_num = 0, fp_arg_num = 0, mem_size = spill_space_size;\n\n  assert (curr_func_item->item_type == MIR_func_item);\n  func = curr_func_item->u.func;\n  block_arg_func_p = FALSE;\n  start_sp_from_bp_offset = 8;\n  keep_fp_p = func->vararg_p;\n  for (i = 0; i < func->nargs; i++) {\n    /* Argument extensions is already done in simplify */\n    /* Prologue: generate arg_var = hard_reg|stack mem|stack addr ... */\n    type = VARR_GET (MIR_var_t, func->vars, i).type;\n    blk_size = MIR_blk_type_p (type) ? (VARR_GET (MIR_var_t, func->vars, i).size + 7) / 8 * 8 : 0;\n#ifndef _WIN32\n    if ((type == MIR_T_BLK + 1 && get_int_arg_reg (int_arg_num) != MIR_NON_VAR\n         && (blk_size <= 8 || get_int_arg_reg (int_arg_num + 1) != MIR_NON_VAR))\n        || (type == MIR_T_BLK + 2 && get_fp_arg_reg (fp_arg_num) != MIR_NON_VAR\n            && (blk_size <= 8 || get_fp_arg_reg (fp_arg_num + 1) != MIR_NON_VAR))) {\n      /* all is passed in gprs or fprs */\n      MIR_type_t mov_type = type == MIR_T_BLK + 1 ? MIR_T_I64 : MIR_T_D;\n      MIR_insn_code_t mov_code1, mov_code2;\n      MIR_reg_t reg2, reg1 = get_arg_reg (mov_type, &int_arg_num, &fp_arg_num, &mov_code1);\n\n      assert (blk_size <= 16);\n      if (blk_size > 8) {\n        reg2 = get_arg_reg (mov_type, &int_arg_num, &fp_arg_num, &mov_code2);\n        new_insn = MIR_new_insn (ctx, mov_code1,\n                                 _MIR_new_var_mem_op (ctx, mov_type, 8, i + MAX_HARD_REG + 1,\n                                                      MIR_NON_VAR, 1),\n                                 _MIR_new_var_op (ctx, reg2));\n        prepend_insn (gen_ctx, new_insn);\n      }\n      new_insn = MIR_new_insn (ctx, mov_code1,\n                               _MIR_new_var_mem_op (ctx, mov_type, 0, i + MAX_HARD_REG + 1,\n                                                    MIR_NON_VAR, 1),\n                               _MIR_new_var_op (ctx, reg1));\n      prepend_insn (gen_ctx, new_insn);\n      new_insn = MIR_new_insn (ctx, MIR_ALLOCA, _MIR_new_var_op (ctx, i + MAX_HARD_REG + 1),\n                               MIR_new_int_op (ctx, blk_size));\n      prepend_insn (gen_ctx, new_insn);\n      continue;\n    } else if ((type == MIR_T_BLK + 3 || type == MIR_T_BLK + 4)\n               && get_int_arg_reg (int_arg_num) != MIR_NON_VAR\n               && get_fp_arg_reg (fp_arg_num) != MIR_NON_VAR) {\n      /* gpr and then fpr or fpr and then gpr */\n      MIR_type_t mov_type1 = type == MIR_T_BLK + 3 ? MIR_T_I64 : MIR_T_D;\n      MIR_type_t mov_type2 = type == MIR_T_BLK + 3 ? MIR_T_D : MIR_T_I64;\n      MIR_insn_code_t mov_code1, mov_code2;\n      MIR_reg_t reg1 = get_arg_reg (mov_type1, &int_arg_num, &fp_arg_num, &mov_code1);\n      MIR_reg_t reg2 = get_arg_reg (mov_type2, &int_arg_num, &fp_arg_num, &mov_code2);\n\n      assert (blk_size > 8 && blk_size <= 16);\n      new_insn = MIR_new_insn (ctx, mov_code2,\n                               _MIR_new_var_mem_op (ctx, mov_type2, 8, i + MAX_HARD_REG + 1,\n                                                    MIR_NON_VAR, 1),\n                               _MIR_new_var_op (ctx, reg2));\n      prepend_insn (gen_ctx, new_insn);\n      new_insn = MIR_new_insn (ctx, mov_code1,\n                               _MIR_new_var_mem_op (ctx, mov_type1, 0, i + MAX_HARD_REG + 1,\n                                                    MIR_NON_VAR, 1),\n                               _MIR_new_var_op (ctx, reg1));\n      prepend_insn (gen_ctx, new_insn);\n      new_insn = MIR_new_insn (ctx, MIR_ALLOCA, _MIR_new_var_op (ctx, i + MAX_HARD_REG + 1),\n                               MIR_new_int_op (ctx, blk_size));\n      prepend_insn (gen_ctx, new_insn);\n      continue;\n    }\n#endif\n    int blk_p = MIR_blk_type_p (type);\n#ifdef _WIN32\n    if (blk_p && blk_size > 8) { /* just address */\n      blk_p = FALSE;\n      type = MIR_T_I64;\n    }\n#endif\n    if (blk_p) {\n      keep_fp_p = block_arg_func_p = TRUE;\n#ifdef _WIN32\n      assert (blk_size <= 8);\n      if ((arg_reg = get_arg_reg (MIR_T_I64, &int_arg_num, &fp_arg_num, &new_insn_code))\n          == MIR_NON_VAR) {\n        new_insn\n          = MIR_new_insn (ctx, MIR_ADD, _MIR_new_var_op (ctx, (MIR_reg_t) (i + MAX_HARD_REG + 1)),\n                          _MIR_new_var_op (ctx, FP_HARD_REG),\n                          MIR_new_int_op (ctx, mem_size + 8 /* ret */\n                                                 + start_sp_from_bp_offset));\n        mem_size += 8;\n      } else { /* put reg into spill space and use its address: prepend in reverse order:  */\n        int disp = (int) (mem_size + 8 /* ret */ + start_sp_from_bp_offset - spill_space_size\n                          + 8 * get_int_arg_reg_num (arg_reg));\n        new_insn\n          = MIR_new_insn (ctx, MIR_ADD, _MIR_new_var_op (ctx, (MIR_reg_t) (i + MAX_HARD_REG + 1)),\n                          _MIR_new_var_op (ctx, FP_HARD_REG), MIR_new_int_op (ctx, disp));\n        prepend_insn (gen_ctx, new_insn);\n        arg_reg_op = _MIR_new_var_op (ctx, arg_reg);\n        mem_op = _MIR_new_var_mem_op (ctx, MIR_T_I64, disp, FP_HARD_REG, MIR_NON_VAR, 1);\n        new_insn = MIR_new_insn (ctx, MIR_MOV, mem_op, arg_reg_op);\n      }\n#else\n      new_insn = MIR_new_insn (ctx, MIR_ADD, _MIR_new_var_op (ctx, i + MAX_HARD_REG + 1),\n                               _MIR_new_var_op (ctx, FP_HARD_REG),\n                               MIR_new_int_op (ctx, mem_size + 8 /* ret addr */\n                                                      + start_sp_from_bp_offset));\n      mem_size += blk_size;\n#endif\n      prepend_insn (gen_ctx, new_insn);\n    } else if ((arg_reg = get_arg_reg (type, &int_arg_num, &fp_arg_num, &new_insn_code))\n               != MIR_NON_VAR) {\n      arg_reg_op = _MIR_new_var_op (ctx, arg_reg);\n      new_insn\n        = MIR_new_insn (ctx, new_insn_code,\n                        _MIR_new_var_op (ctx, (MIR_reg_t) (i + MAX_HARD_REG + 1)), arg_reg_op);\n      prepend_insn (gen_ctx, new_insn);\n    } else {\n      /* arg is on the stack */\n      keep_fp_p = block_arg_func_p = TRUE;\n      mem_type = type == MIR_T_F || type == MIR_T_D || type == MIR_T_LD ? type : MIR_T_I64;\n      new_insn_code = (type == MIR_T_F    ? MIR_FMOV\n                       : type == MIR_T_D  ? MIR_DMOV\n                       : type == MIR_T_LD ? MIR_LDMOV\n                                          : MIR_MOV);\n      mem_op = _MIR_new_var_mem_op (ctx, mem_type,\n                                    mem_size + 8 /* ret */\n                                      + start_sp_from_bp_offset,\n                                    FP_HARD_REG, MIR_NON_VAR, 1);\n      new_insn = MIR_new_insn (ctx, new_insn_code,\n                               _MIR_new_var_op (ctx, (MIR_reg_t) (i + MAX_HARD_REG + 1)), mem_op);\n      prepend_insn (gen_ctx, new_insn);\n      mem_size += type == MIR_T_LD ? 16 : 8;\n    }\n  }\n  alloca_p = FALSE;\n  leaf_p = TRUE;\n  for (insn = DLIST_HEAD (MIR_insn_t, func->insns); insn != NULL; insn = next_insn) {\n    next_insn = DLIST_NEXT (MIR_insn_t, insn);\n    code = insn->code;\n    switch (code) {\n    case MIR_UI2F:\n    case MIR_UI2D:\n    case MIR_UI2LD:\n    case MIR_LD2I: {\n      /* Use a builtin func call: mov freg, func ref; call proto, freg, res_reg, op_reg */\n      MIR_item_t proto_item, func_import_item;\n      MIR_op_t freg_op, res_reg_op = insn->ops[0], op_reg_op = insn->ops[1], ops[4];\n\n      get_builtin (gen_ctx, code, &proto_item, &func_import_item);\n      assert (res_reg_op.mode == MIR_OP_VAR && op_reg_op.mode == MIR_OP_VAR);\n      freg_op\n        = _MIR_new_var_op (ctx, gen_new_temp_reg (gen_ctx, MIR_T_I64, curr_func_item->u.func));\n      next_insn = new_insn\n        = MIR_new_insn (ctx, MIR_MOV, freg_op, MIR_new_ref_op (ctx, func_import_item));\n      gen_add_insn_before (gen_ctx, insn, new_insn);\n      ops[0] = MIR_new_ref_op (ctx, proto_item);\n      ops[1] = freg_op;\n      ops[2] = res_reg_op;\n      ops[3] = op_reg_op;\n      new_insn = MIR_new_insn_arr (ctx, MIR_CALL, 4, ops);\n      gen_add_insn_before (gen_ctx, insn, new_insn);\n      gen_delete_insn (gen_ctx, insn);\n      break;\n    }\n    case MIR_VA_START: {\n      MIR_op_t treg_op\n        = _MIR_new_var_op (ctx, gen_new_temp_reg (gen_ctx, MIR_T_I64, curr_func_item->u.func));\n      MIR_op_t va_op = insn->ops[0];\n      MIR_reg_t va_reg;\n#ifndef _WIN32\n      int gp_offset = 0, fp_offset = 48, mem_offset = 0;\n      MIR_var_t var;\n\n      assert (func->vararg_p && va_op.mode == MIR_OP_VAR);\n      for (uint32_t narg = 0; narg < func->nargs; narg++) {\n        var = VARR_GET (MIR_var_t, func->vars, narg);\n        if (var.type == MIR_T_F || var.type == MIR_T_D) {\n          fp_offset += 16;\n          if (gp_offset >= 176) mem_offset += 8;\n        } else if (var.type == MIR_T_LD) {\n          mem_offset += 16;\n        } else if (MIR_blk_type_p (var.type)) {\n          mem_offset += var.size;\n        } else { /* including RBLK */\n          gp_offset += 8;\n          if (gp_offset >= 48) mem_offset += 8;\n        }\n      }\n      va_reg = va_op.u.var;\n      /* Insns can be not simplified as soon as they match a machine insn.  */\n      /* mem32[va_reg] = gp_offset; mem32[va_reg] = fp_offset */\n      gen_mov (gen_ctx, insn, MIR_MOV,\n               _MIR_new_var_mem_op (ctx, MIR_T_U32, 0, va_reg, MIR_NON_VAR, 1),\n               MIR_new_int_op (ctx, gp_offset));\n      next_insn = DLIST_PREV (MIR_insn_t, insn);\n      gen_mov (gen_ctx, insn, MIR_MOV,\n               _MIR_new_var_mem_op (ctx, MIR_T_U32, 4, va_reg, MIR_NON_VAR, 1),\n               MIR_new_int_op (ctx, fp_offset));\n      /* overflow_arg_area_reg: treg = start sp + 8 + mem_offset; mem64[va_reg + 8] = treg */\n      new_insn\n        = MIR_new_insn (ctx, MIR_ADD, treg_op, _MIR_new_var_op (ctx, FP_HARD_REG),\n                        MIR_new_int_op (ctx, 8 /*ret*/ + mem_offset + start_sp_from_bp_offset));\n      gen_add_insn_before (gen_ctx, insn, new_insn);\n      gen_mov (gen_ctx, insn, MIR_MOV,\n               _MIR_new_var_mem_op (ctx, MIR_T_I64, 8, va_reg, MIR_NON_VAR, 1), treg_op);\n      /* reg_save_area: treg = start sp - reg_save_area_size; mem64[va_reg + 16] = treg */\n      new_insn = MIR_new_insn (ctx, MIR_ADD, treg_op, _MIR_new_var_op (ctx, FP_HARD_REG),\n                               MIR_new_int_op (ctx, -reg_save_area_size));\n      gen_add_insn_before (gen_ctx, insn, new_insn);\n      gen_mov (gen_ctx, insn, MIR_MOV,\n               _MIR_new_var_mem_op (ctx, MIR_T_I64, 16, va_reg, MIR_NON_VAR, 1), treg_op);\n#else\n      /* init va_list */\n      mem_size = 8 /*ret*/ + start_sp_from_bp_offset + func->nargs * 8;\n      new_insn = MIR_new_insn (ctx, MIR_ADD, treg_op, _MIR_new_var_op (ctx, FP_HARD_REG),\n                               MIR_new_int_op (ctx, mem_size));\n      gen_add_insn_before (gen_ctx, insn, new_insn);\n      va_reg = va_op.u.var;\n      gen_mov (gen_ctx, insn, MIR_MOV,\n               _MIR_new_var_mem_op (ctx, MIR_T_I64, 0, va_reg, MIR_NON_VAR, 1), treg_op);\n#endif\n      gen_delete_insn (gen_ctx, insn);\n      break;\n    }\n    case MIR_VA_END: /* do nothing */ gen_delete_insn (gen_ctx, insn); break;\n    case MIR_VA_ARG:\n    case MIR_VA_BLOCK_ARG: {\n      /* Use a builtin func call:\n         mov func_reg, func ref; [mov reg3, type;] call proto, func_reg, res_reg, va_reg,\n         reg3 */\n      MIR_item_t proto_item, func_import_item;\n      MIR_op_t ops[6], func_reg_op, reg_op3;\n      MIR_op_t res_reg_op = insn->ops[0], va_reg_op = insn->ops[1], op3 = insn->ops[2];\n\n      get_builtin (gen_ctx, code, &proto_item, &func_import_item);\n      assert (res_reg_op.mode == MIR_OP_VAR && va_reg_op.mode == MIR_OP_VAR\n              && op3.mode == (code == MIR_VA_ARG ? MIR_OP_VAR_MEM : MIR_OP_VAR));\n      func_reg_op = _MIR_new_var_op (ctx, gen_new_temp_reg (gen_ctx, MIR_T_I64, func));\n      reg_op3 = _MIR_new_var_op (ctx, gen_new_temp_reg (gen_ctx, MIR_T_I64, func));\n      next_insn = new_insn\n        = MIR_new_insn (ctx, MIR_MOV, func_reg_op, MIR_new_ref_op (ctx, func_import_item));\n      gen_add_insn_before (gen_ctx, insn, new_insn);\n      if (code == MIR_VA_ARG) {\n        new_insn = MIR_new_insn (ctx, MIR_MOV, reg_op3,\n                                 MIR_new_int_op (ctx, (int64_t) op3.u.var_mem.type));\n        op3 = reg_op3;\n        gen_add_insn_before (gen_ctx, insn, new_insn);\n      }\n      ops[0] = MIR_new_ref_op (ctx, proto_item);\n      ops[1] = func_reg_op;\n      ops[2] = res_reg_op;\n      ops[3] = va_reg_op;\n      ops[4] = op3;\n      if (code == MIR_VA_BLOCK_ARG) ops[5] = insn->ops[3];\n      new_insn = MIR_new_insn_arr (ctx, MIR_CALL, code == MIR_VA_ARG ? 5 : 6, ops);\n      gen_add_insn_before (gen_ctx, insn, new_insn);\n      gen_delete_insn (gen_ctx, insn);\n      break;\n    }\n    case MIR_ALLOCA: keep_fp_p = alloca_p = TRUE; break;\n    case MIR_RET: {\n      /* In simplify we already transformed code for one return insn\n         and added extension in return (if any).  */\n      uint32_t n_iregs = 0, n_xregs = 0, n_fregs = 0;\n\n#ifdef _WIN32\n      if (curr_func_item->u.func->nres > 1)\n        (*MIR_get_error_func (ctx)) (MIR_ret_error,\n                                     \"Windows x86-64 doesn't support multiple return values\");\n#endif\n      assert (curr_func_item->u.func->nres == MIR_insn_nops (ctx, insn));\n      for (size_t nres = 0; nres < curr_func_item->u.func->nres; nres++) {\n        res_type = curr_func_item->u.func->res_types[nres];\n        if ((res_type == MIR_T_F || res_type == MIR_T_D) && n_xregs < 2) {\n          new_insn_code = res_type == MIR_T_F ? MIR_FMOV : MIR_DMOV;\n          ret_reg = n_xregs++ == 0 ? XMM0_HARD_REG : XMM1_HARD_REG;\n        } else if (res_type == MIR_T_LD && n_fregs < 2) {  // ???\n          new_insn_code = MIR_LDMOV;\n          ret_reg = n_fregs == 0 ? ST0_HARD_REG : ST1_HARD_REG;\n          n_fregs++;\n        } else if (n_iregs < 2) {\n          new_insn_code = MIR_MOV;\n          ret_reg = n_iregs++ == 0 ? AX_HARD_REG : DX_HARD_REG;\n        } else {\n          (*MIR_get_error_func (ctx)) (MIR_ret_error,\n                                       \"x86-64 can not handle this combination of return values\");\n        }\n        ret_reg_op = _MIR_new_var_op (ctx, ret_reg);\n        new_insn = MIR_new_insn (ctx, new_insn_code, ret_reg_op, insn->ops[nres]);\n        gen_add_insn_before (gen_ctx, insn, new_insn);\n        insn->ops[nres] = ret_reg_op;\n      }\n      break;\n    }\n    case MIR_LSH:\n    case MIR_RSH:\n    case MIR_URSH:\n    case MIR_LSHS:\n    case MIR_RSHS:\n    case MIR_URSHS: {\n      /* We can access only cl as shift register: */\n      MIR_op_t creg_op = _MIR_new_var_op (ctx, CX_HARD_REG);\n\n      new_insn = MIR_new_insn (ctx, MIR_MOV, creg_op, insn->ops[2]);\n      gen_add_insn_before (gen_ctx, insn, new_insn);\n      insn->ops[2] = creg_op;\n      break;\n    }\n    case MIR_UMULO:\n    case MIR_UMULOS: {\n      /* We can use only ax as zero and the 1st operand: */\n      MIR_op_t areg_op = _MIR_new_var_op (ctx, AX_HARD_REG);\n\n      new_insn = MIR_new_insn (ctx, MIR_MOV, areg_op, insn->ops[1]);\n      gen_add_insn_before (gen_ctx, insn, new_insn);\n      new_insn = MIR_new_insn (ctx, MIR_MOV, insn->ops[0], areg_op);\n      gen_add_insn_after (gen_ctx, insn, new_insn);\n      insn->ops[0] = insn->ops[1] = areg_op;\n      break;\n    }\n    case MIR_DIV:\n    case MIR_UDIV:\n    case MIR_DIVS:\n    case MIR_UDIVS: {\n      /* Divide uses ax/dx as operands: */\n      MIR_op_t areg_op = _MIR_new_var_op (ctx, AX_HARD_REG);\n\n      new_insn = MIR_new_insn (ctx, MIR_MOV, areg_op, insn->ops[1]);\n      gen_add_insn_before (gen_ctx, insn, new_insn);\n      new_insn = MIR_new_insn (ctx, MIR_MOV, insn->ops[0], areg_op);\n      gen_add_insn_after (gen_ctx, insn, new_insn);\n      insn->ops[0] = insn->ops[1] = areg_op;\n      break;\n    }\n    case MIR_MOD:\n    case MIR_UMOD:\n    case MIR_MODS:\n    case MIR_UMODS: {\n      /* Divide uses ax/dx as operands: */\n      MIR_op_t areg_op = _MIR_new_var_op (ctx, AX_HARD_REG);\n      MIR_op_t dreg_op = _MIR_new_var_op (ctx, DX_HARD_REG);\n\n      new_insn = MIR_new_insn (ctx, MIR_MOV, areg_op, insn->ops[1]);\n      gen_add_insn_before (gen_ctx, insn, new_insn);\n      insn->ops[1] = areg_op;\n      new_insn = MIR_new_insn (ctx, MIR_MOV, insn->ops[0], dreg_op);\n      gen_add_insn_after (gen_ctx, insn, new_insn);\n      insn->ops[0] = dreg_op;\n      break;\n    }\n    case MIR_EQ:\n    case MIR_NE:\n    case MIR_LT:\n    case MIR_ULT:\n    case MIR_LE:\n    case MIR_ULE:\n    case MIR_GT:\n    case MIR_UGT:\n    case MIR_GE:\n    case MIR_UGE:\n    case MIR_EQS:\n    case MIR_NES:\n    case MIR_LTS:\n    case MIR_ULTS:\n    case MIR_LES:\n    case MIR_ULES:\n    case MIR_GTS:\n    case MIR_UGTS:\n    case MIR_GES:\n    case MIR_UGES:\n    case MIR_FEQ:\n    case MIR_FNE:\n    case MIR_FLT:\n    case MIR_FLE:\n    case MIR_FGT:\n    case MIR_FGE:\n    case MIR_DEQ:\n    case MIR_DNE:\n    case MIR_DLT:\n    case MIR_DLE:\n    case MIR_DGT:\n    case MIR_DGE: {\n      new_insn = MIR_new_insn (ctx, MIR_UEXT8, insn->ops[0], insn->ops[0]);\n      gen_add_insn_after (gen_ctx, insn, new_insn);\n      /* Following conditional branches are changed to correctly process unordered numbers: */\n      switch (code) {\n      case MIR_FLT:\n        SWAP (insn->ops[1], insn->ops[2], temp_op);\n        insn->code = MIR_FGT;\n        break;\n      case MIR_FLE:\n        SWAP (insn->ops[1], insn->ops[2], temp_op);\n        insn->code = MIR_FGE;\n        break;\n      case MIR_DLT:\n        SWAP (insn->ops[1], insn->ops[2], temp_op);\n        insn->code = MIR_DGT;\n        break;\n      case MIR_DLE:\n        SWAP (insn->ops[1], insn->ops[2], temp_op);\n        insn->code = MIR_DGE;\n        break;\n      default: break; /* do nothing */\n      }\n      break;\n    }\n    /* Following conditional branches are changed to correctly process unordered numbers: */\n    case MIR_LDLT:\n      SWAP (insn->ops[1], insn->ops[2], temp_op);\n      insn->code = MIR_LDGT;\n      break;\n    case MIR_LDLE:\n      SWAP (insn->ops[1], insn->ops[2], temp_op);\n      insn->code = MIR_LDGE;\n      break;\n    case MIR_FBLT:\n      SWAP (insn->ops[1], insn->ops[2], temp_op);\n      insn->code = MIR_FBGT;\n      break;\n    case MIR_FBLE:\n      SWAP (insn->ops[1], insn->ops[2], temp_op);\n      insn->code = MIR_FBGE;\n      break;\n    case MIR_DBLT:\n      SWAP (insn->ops[1], insn->ops[2], temp_op);\n      insn->code = MIR_DBGT;\n      break;\n    case MIR_DBLE:\n      SWAP (insn->ops[1], insn->ops[2], temp_op);\n      insn->code = MIR_DBGE;\n      break;\n    case MIR_LDBLT:\n      SWAP (insn->ops[1], insn->ops[2], temp_op);\n      insn->code = MIR_LDBGT;\n      break;\n    case MIR_LDBLE:\n      SWAP (insn->ops[1], insn->ops[2], temp_op);\n      insn->code = MIR_LDBGE;\n      break;\n    default:\n      if (MIR_call_code_p (code)) {\n        machinize_call (gen_ctx, insn);\n        leaf_p = FALSE;\n      }\n      break;\n    }\n  }\n}\n\nstatic void isave (gen_ctx_t gen_ctx, MIR_insn_t anchor, int disp, MIR_reg_t hard_reg) {\n  MIR_context_t ctx = gen_ctx->ctx;\n\n  gen_mov (gen_ctx, anchor, MIR_MOV,\n           _MIR_new_var_mem_op (ctx, MIR_T_I64, disp, SP_HARD_REG, MIR_NON_VAR, 1),\n           _MIR_new_var_op (ctx, hard_reg));\n}\n\nstatic void dsave (gen_ctx_t gen_ctx, MIR_insn_t anchor, int disp, MIR_reg_t hard_reg) {\n  MIR_context_t ctx = gen_ctx->ctx;\n\n  gen_mov (gen_ctx, anchor, MIR_DMOV,\n           _MIR_new_var_mem_op (ctx, MIR_T_D, disp, SP_HARD_REG, MIR_NON_VAR, 1),\n           _MIR_new_var_op (ctx, hard_reg));\n}\n\nstatic void target_make_prolog_epilog (gen_ctx_t gen_ctx, bitmap_t used_hard_regs,\n                                       size_t stack_slots_num) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  MIR_func_t func;\n  MIR_insn_t anchor, new_insn;\n  MIR_op_t sp_reg_op, fp_reg_op;\n#ifdef MIR_NO_RED_ZONE_ABI\n  MIR_op_t temp_reg_op;\n#endif\n  MIR_reg_t base_reg;\n  int64_t bp_saved_reg_offset, offset;\n  size_t i, service_area_size, saved_hard_regs_size, stack_slots_size, block_size;\n\n  assert (curr_func_item->item_type == MIR_func_item);\n  func = curr_func_item->u.func;\n  for (i = saved_hard_regs_size = 0; i <= R15_HARD_REG; i++)\n    if (!target_call_used_hard_reg_p ((MIR_reg_t) i, MIR_T_UNDEF)\n        && bitmap_bit_p (used_hard_regs, i))\n      saved_hard_regs_size += 8;\n#ifdef _WIN32\n  for (; i <= XMM15_HARD_REG; i++)\n    if (!target_call_used_hard_reg_p ((MIR_reg_t) i, MIR_T_UNDEF)\n        && bitmap_bit_p (used_hard_regs, i))\n      saved_hard_regs_size += 16;\n#endif\n  if (leaf_p && !alloca_p && !block_arg_func_p && saved_hard_regs_size == 0 && !func->vararg_p\n      && stack_slots_num == 0)\n    return;\n  anchor = DLIST_HEAD (MIR_insn_t, func->insns);\n  sp_reg_op = _MIR_new_var_op (ctx, SP_HARD_REG);\n  if (keep_fp_p) {\n    fp_reg_op = _MIR_new_var_op (ctx, FP_HARD_REG);\n#ifdef MIR_NO_RED_ZONE_ABI\n    temp_reg_op = _MIR_new_var_op (ctx, TEMP_INT_HARD_REG1);\n#endif\n    /* Prologue: */\n#ifdef MIR_NO_RED_ZONE_ABI\n    new_insn = MIR_new_insn (ctx, MIR_ADD, temp_reg_op, sp_reg_op, MIR_new_int_op (ctx, -8));\n    gen_add_insn_before (gen_ctx, anchor, new_insn); /* temp = sp - 8 */\n#else\n    new_insn = MIR_new_insn (ctx, MIR_MOV,\n                             _MIR_new_var_mem_op (ctx, MIR_T_I64, -8, SP_HARD_REG, MIR_NON_VAR, 1),\n                             fp_reg_op);\n    gen_add_insn_before (gen_ctx, anchor, new_insn); /* -8(sp) = bp */\n    /* Use add for matching LEA: */\n    new_insn = MIR_new_insn (ctx, MIR_ADD, fp_reg_op, sp_reg_op, MIR_new_int_op (ctx, -8));\n    gen_add_insn_before (gen_ctx, anchor, new_insn); /* bp = sp - 8 */\n#endif\n  }\n#ifdef _WIN32\n  if (func->vararg_p) { /* filling spill space */\n    assert (keep_fp_p);\n    for (i = 0, offset = 16 /* ret & bp */; i < 4; i++, offset += 8)\n      gen_mov (gen_ctx, anchor, MIR_MOV,\n               _MIR_new_var_mem_op (ctx, MIR_T_I64, offset, FP_HARD_REG, MIR_NON_VAR, 1),\n               _MIR_new_var_op (ctx, get_int_arg_reg (i)));\n  }\n#endif\n  service_area_size = func->vararg_p ? reg_save_area_size : 0;\n  if (!func->jret_p) service_area_size += 8; /* return address */\n  stack_slots_size = stack_slots_num * 8;\n  if (!keep_fp_p) stack_slots_size = (stack_slots_size + 15) / 16 * 16;\n  /* stack slots, and saved regs as multiple of 16 bytes: */\n  block_size = (stack_slots_size + saved_hard_regs_size + 15) / 16 * 16;\n  new_insn = MIR_new_insn (ctx, MIR_SUB, sp_reg_op, sp_reg_op,\n                           MIR_new_int_op (ctx, block_size + service_area_size));\n  gen_add_insn_before (gen_ctx, anchor, new_insn); /* sp -= block size + service_area_size */\n  bp_saved_reg_offset = block_size;\n#ifdef MIR_NO_RED_ZONE_ABI\n  if (keep_fp_p) {\n    new_insn\n      = MIR_new_insn (ctx, MIR_MOV,\n                      _MIR_new_var_mem_op (ctx, MIR_T_I64, block_size + service_area_size - 8,\n                                           SP_HARD_REG, MIR_NON_VAR, 1),\n                      fp_reg_op);\n    gen_add_insn_before (gen_ctx, anchor, new_insn); /* -8(old sp) = bp */\n    new_insn = MIR_new_insn (ctx, MIR_MOV, fp_reg_op, temp_reg_op);\n    gen_add_insn_before (gen_ctx, anchor, new_insn); /* bp = temp */\n  }\n#endif\n#ifndef _WIN32\n  if (func->vararg_p) {\n    offset = block_size;\n    isave (gen_ctx, anchor, offset, DI_HARD_REG);\n    isave (gen_ctx, anchor, offset + 8, SI_HARD_REG);\n    isave (gen_ctx, anchor, offset + 16, DX_HARD_REG);\n    isave (gen_ctx, anchor, offset + 24, CX_HARD_REG);\n    isave (gen_ctx, anchor, offset + 32, R8_HARD_REG);\n    isave (gen_ctx, anchor, offset + 40, R9_HARD_REG);\n    dsave (gen_ctx, anchor, offset + 48, XMM0_HARD_REG);\n    dsave (gen_ctx, anchor, offset + 64, XMM1_HARD_REG);\n    dsave (gen_ctx, anchor, offset + 80, XMM2_HARD_REG);\n    dsave (gen_ctx, anchor, offset + 96, XMM3_HARD_REG);\n    dsave (gen_ctx, anchor, offset + 112, XMM4_HARD_REG);\n    dsave (gen_ctx, anchor, offset + 128, XMM5_HARD_REG);\n    dsave (gen_ctx, anchor, offset + 144, XMM6_HARD_REG);\n    dsave (gen_ctx, anchor, offset + 160, XMM7_HARD_REG);\n    bp_saved_reg_offset += reg_save_area_size;\n  }\n#endif\n  /* Saving callee saved hard registers: */\n  offset = keep_fp_p ? -bp_saved_reg_offset : (int64_t) stack_slots_size;\n  base_reg = keep_fp_p ? FP_HARD_REG : SP_HARD_REG;\n#ifdef _WIN32\n  for (i = XMM0_HARD_REG; i <= XMM15_HARD_REG; i++)\n    if (!target_call_used_hard_reg_p ((MIR_reg_t) i, MIR_T_UNDEF)\n        && bitmap_bit_p (used_hard_regs, i)) {\n      new_insn = _MIR_new_unspec_insn (ctx, 3, MIR_new_int_op (ctx, MOVDQA_CODE),\n                                       _MIR_new_var_mem_op (ctx, MIR_T_D, offset, base_reg,\n                                                            MIR_NON_VAR, 1),\n                                       _MIR_new_var_op (ctx, (MIR_reg_t) i));\n      gen_add_insn_before (gen_ctx, anchor, new_insn); /* disp(bp|sp) = saved hard reg */\n      offset += 16;\n    }\n#endif\n  for (i = 0; i <= R15_HARD_REG; i++)\n    if (!target_call_used_hard_reg_p ((MIR_reg_t) i, MIR_T_UNDEF)\n        && bitmap_bit_p (used_hard_regs, i)) {\n      new_insn\n        = MIR_new_insn (ctx, MIR_MOV,\n                        _MIR_new_var_mem_op (ctx, MIR_T_I64, offset, base_reg, MIR_NON_VAR, 1),\n                        _MIR_new_var_op (ctx, (MIR_reg_t) i));\n      gen_add_insn_before (gen_ctx, anchor, new_insn); /* disp(bp|sp) = saved hard reg */\n      offset += 8;\n    }\n  /* Epilogue: */\n  for (anchor = DLIST_TAIL (MIR_insn_t, func->insns); anchor != NULL;\n       anchor = DLIST_PREV (MIR_insn_t, anchor))\n    if (anchor->code == MIR_RET || anchor->code == MIR_JRET) break;\n  if (anchor == NULL) return;\n  /* Restoring hard registers: */\n  offset = keep_fp_p ? -bp_saved_reg_offset : (int64_t) stack_slots_size;\n#ifdef _WIN32\n  for (i = XMM0_HARD_REG; i <= XMM15_HARD_REG; i++)\n    if (!target_call_used_hard_reg_p ((MIR_reg_t) i, MIR_T_UNDEF)\n        && bitmap_bit_p (used_hard_regs, i)) {\n      new_insn = _MIR_new_unspec_insn (ctx, 3, MIR_new_int_op (ctx, MOVDQA_CODE),\n                                       _MIR_new_var_op (ctx, (MIR_reg_t) i),\n                                       _MIR_new_var_mem_op (ctx, MIR_T_D, offset, base_reg,\n                                                            MIR_NON_VAR, 1));\n      gen_add_insn_before (gen_ctx, anchor, new_insn); /* hard reg = disp(bp|sp) */\n      offset += 16;\n    }\n#endif\n  for (i = 0; i <= R15_HARD_REG; i++)\n    if (!target_call_used_hard_reg_p ((MIR_reg_t) i, MIR_T_UNDEF)\n        && bitmap_bit_p (used_hard_regs, i)) {\n      new_insn\n        = MIR_new_insn (ctx, MIR_MOV, _MIR_new_var_op (ctx, (MIR_reg_t) i),\n                        _MIR_new_var_mem_op (ctx, MIR_T_I64, offset, base_reg, MIR_NON_VAR, 1));\n      gen_add_insn_before (gen_ctx, anchor, new_insn); /* hard reg = disp(bp|sp) */\n      offset += 8;\n    }\n  if (!keep_fp_p) {\n    new_insn = MIR_new_insn (ctx, MIR_ADD, sp_reg_op, sp_reg_op,\n                             MIR_new_int_op (ctx, block_size + service_area_size));\n    gen_add_insn_before (gen_ctx, anchor, new_insn); /* sp += block size + service_area_size */\n  } else {\n#ifdef MIR_NO_RED_ZONE_ABI\n    new_insn = MIR_new_insn (ctx, MIR_MOV, temp_reg_op, fp_reg_op);\n    gen_add_insn_before (gen_ctx, anchor, new_insn); /* temp = bp */\n    new_insn\n      = MIR_new_insn (ctx, MIR_MOV, fp_reg_op,\n                      _MIR_new_var_mem_op (ctx, MIR_T_I64, 0, TEMP_INT_HARD_REG1, MIR_NON_VAR, 1));\n    gen_add_insn_before (gen_ctx, anchor, new_insn); /* bp = 0(bp) */\n    new_insn = MIR_new_insn (ctx, MIR_ADD, sp_reg_op, temp_reg_op, MIR_new_int_op (ctx, 8));\n    gen_add_insn_before (gen_ctx, anchor, new_insn); /* sp = temp + 8 */\n#else\n    new_insn = MIR_new_insn (ctx, MIR_ADD, sp_reg_op, fp_reg_op, MIR_new_int_op (ctx, 8));\n    gen_add_insn_before (gen_ctx, anchor, new_insn); /* sp = bp + 8 */\n    new_insn = MIR_new_insn (ctx, MIR_MOV, fp_reg_op,\n                             _MIR_new_var_mem_op (ctx, MIR_T_I64, -8, SP_HARD_REG, MIR_NON_VAR, 1));\n    gen_add_insn_before (gen_ctx, anchor, new_insn); /* bp = -8(sp) */\n#endif\n  }\n}\n\nstruct pattern {\n  const MIR_insn_code_t code;\n  /* Pattern elements:\n     blank - ignore\n     X - match everything\n     $ - finish successfully matching\n     r - register (we don't care about bp and sp because they are fixed and used correctly)\n     t - ax, cx, dx, or bx register\n     h[0-31] - hard register with given number\n     z - operand is zero\n     i[0-3] - immediate (including refs) of size 8,16,32,64-bits\n     s - immediate 1, 2, 4, or 8 (scale)\n     c<number> - immediate integer <number>\n     m[0-3] - int (signed or unsigned) type memory of size 8,16,32,64-bits\n     ms[0-3] - signed int type memory of size 8,16,32,64-bits\n     mu[0-3] - unsigned int type memory of size 8,16,32,64-bits\n     mf - memory of float\n     md - memory of double\n     mld - memory of long double\n     L - label which can be present by 32-bit\n     l - label which can be present by 8-bit\n     [0-9] - an operand matching n-th operand (n should be less than given operand number)\n\n     Remember we have no float or (long) double immediate at this stage. They are represented by\n     a reference to data item.  */\n  const char *pattern;\n  /* Replacement elements:\n     blank - ignore\n     ; - insn separation\n     X - REX byte with W=1\n     Y - Optional REX byte with W=0\n     Z - Obligatory REX byte with W=0\n     [0-9A-F]+ pairs of hexidecimal digits opcode\n     r[0-2] = n-th operand in ModRM:reg\n     R[0-2] = n-th operand in ModRM:rm with mod == 3\n     S[0-2] = n-th operand in ModRM:rm with mod == 3, 8-bit registers\n     m[0-2] = n-th operand is mem\n     mt = temp memory in red zone (-16(sp))\n     mT = switch table memory (h11,r,8)\n     ap = 2 and 3 operand forms address by plus (1st reg to base, 2nd reg to index, disp to disp)\n     am = 2 and 3 operand forms address by mult (1st reg to index and mult const to scale)\n     ad<value> - forms address: 1th operand is base reg and <value> is displacement\n     i[0-2] - n-th operand in byte immediate (should be imm of type i8)\n     I[0-2] - n-th operand in 4 byte immediate (should be imm of type i32)\n     J[0-2] - n-th operand in 8 byte immediate\n     P[0-2] - n-th operand is 64-bit call address in memory pool\n     T     - relative switch table address\n     q     - mod==0, rm==5 (ip-relative addressing)\n     L[0-2] - n-th operand-label in 32-bit\n     l[0-2] - n-th operand-label in 8-bit\n     /[0-7] - opmod with given value (reg of MOD-RM)\n     +[0-2] - lower 3-bit part of opcode used for n-th reg operand\n     +h<one hex digit> - lower 3-bit part of opcode used for 0-15 hard reg operand\n     c<value> - address of 32-bit or 64-bit constant in memory pool (we keep always 64-bit\n                in memory pool. x86_64 is LE)\n     h<one or two hex digits> - hardware register with given number in reg of ModRM:reg;\n                                 one bit of 8-15 in REX.R\n     H<one or two hex digits> - hardware register with given number in rm of MOD-RM with and mod=3\n     (register); one bit of 8-15 in REX.B\n     v<value> - 8-bit immediate with given hex value\n     V<value> - 32-bit immediate with given hex value\n  */\n  const char *replacement;\n  int max_insn_size;\n};\n\n// make imm always second operand (simplify for cmp and commutative op)\n// make result of cmp op always a register and memory only the 2nd operand if first is reg,\n// but not for FP (NAN) (simplify)\n// for FP cmp first operand should be always reg (machinize)\n\n#define IOP0(ICODE, SUFF, RRM_CODE, MR_CODE, RMI8_CODE, RMI32_CODE)      \\\n  {ICODE##SUFF, \"r 0 r\", \"X \" RRM_CODE \" r0 R2\", 0},       /* op r0,r2*/ \\\n    {ICODE##SUFF, \"r 0 m3\", \"X \" RRM_CODE \" r0 m2\", 0},    /* op r0,m2*/ \\\n    {ICODE##SUFF, \"m3 0 r\", \"X \" MR_CODE \" r2 m0\", 0},     /* op m0,r2*/ \\\n    {ICODE##SUFF, \"r 0 i0\", \"X \" RMI8_CODE \" R0 i2\", 0},   /* op r0,i2*/ \\\n    {ICODE##SUFF, \"m3 0 i0\", \"X \" RMI8_CODE \" m0 i2\", 0},  /* op m0,i2*/ \\\n    {ICODE##SUFF, \"r 0 i2\", \"X \" RMI32_CODE \" R0 I2\", 0},  /* op r0,i2*/ \\\n    {ICODE##SUFF, \"m3 0 i2\", \"X \" RMI32_CODE \" m0 I2\", 0}, /* op m0,i2*/\n\n#define IOP0S(ICODE, SUFF, RRM_CODE, MR_CODE, RMI8_CODE, RMI32_CODE)     \\\n  {ICODE##SUFF, \"r 0 r\", \"Y \" RRM_CODE \" r0 R2\", 0},       /* op r0,r2*/ \\\n    {ICODE##SUFF, \"r 0 m2\", \"Y \" RRM_CODE \" r0 m2\", 0},    /* op r0,m2*/ \\\n    {ICODE##SUFF, \"m2 0 r\", \"Y \" MR_CODE \" r2 m0\", 0},     /* op m0,r2*/ \\\n    {ICODE##SUFF, \"r 0 i0\", \"Y \" RMI8_CODE \" R0 i2\", 0},   /* op r0,i2*/ \\\n    {ICODE##SUFF, \"m2 0 i0\", \"Y \" RMI8_CODE \" m0 i2\", 0},  /* op m0,i2*/ \\\n    {ICODE##SUFF, \"r 0 i2\", \"Y \" RMI32_CODE \" R0 I2\", 0},  /* op r0,i2*/ \\\n    {ICODE##SUFF, \"m2 0 i2\", \"Y \" RMI32_CODE \" m0 I2\", 0}, /* op m0,i2*/\n\n#define IOP(ICODE, RRM_CODE, MR_CODE, RMI8_CODE, RMI32_CODE) \\\n  IOP0 (ICODE, , RRM_CODE, MR_CODE, RMI8_CODE, RMI32_CODE)   \\\n  IOP0S (ICODE, S, RRM_CODE, MR_CODE, RMI8_CODE, RMI32_CODE)\n\n#define FOP(ICODE, OP_CODE) \\\n  {ICODE, \"r 0 r\", OP_CODE \" r0 R2\", 0}, {ICODE, \"r 0 mf\", OP_CODE \" r0 m2\", 0},\n\n#define DOP(ICODE, OP_CODE) \\\n  {ICODE, \"r 0 r\", OP_CODE \" r0 R2\", 0}, {ICODE, \"r 0 md\", OP_CODE \" r0 m2\", 0},\n\n#define LDOP(ICODE, OP_CODE)      \\\n  /* fld m1;fld m2;op;fstp m0: */ \\\n  {ICODE, \"mld mld mld\", \"DB /5 m1; DB /5 m2; \" OP_CODE \"; DB /7 m0\", 0},\n\n#define SHOP(ICODE, CL_OP_CODE, I8_OP_CODE)                      \\\n  {ICODE, \"r 0 h1\", \"X \" CL_OP_CODE \" R0\"},       /* sh r0,cl */ \\\n    {ICODE, \"m3 0 h1\", \"X \" CL_OP_CODE \" m0\"},    /* sh m0,cl */ \\\n    {ICODE, \"r 0 i0\", \"X \" I8_OP_CODE \" R0 i2\"},  /* sh r0,i2 */ \\\n    {ICODE, \"m3 0 i0\", \"X \" I8_OP_CODE \" m0 i2\"}, /* sh m0,i2 */\n\n#define SHOPS(ICODE, CL_OP_CODE, I8_OP_CODE)                     \\\n  {ICODE, \"r 0 h1\", \"Y \" CL_OP_CODE \" R0\"},       /* sh r0,cl */ \\\n    {ICODE, \"m2 0 h1\", \"Y \" CL_OP_CODE \" m0\"},    /* sh m0,cl */ \\\n    {ICODE, \"r 0 i0\", \"Y \" I8_OP_CODE \" R0 i2\"},  /* sh r0,i2 */ \\\n    {ICODE, \"m2 0 i0\", \"Y \" I8_OP_CODE \" m0 i2\"}, /* sh m0,i2 */\n\n/* cmp ...; setx r0: */\n#define CMP(ICODE, SETX)                                                      \\\n  {ICODE, \"r r r\", \"X 3B r1 R2; Y \" SETX \" S0\", 0},        /* cmp r1,r2;...*/ \\\n    {ICODE, \"r r m3\", \"X 3B r1 m2; Y \" SETX \" S0\", 0},     /* cmp r1,m2;...*/ \\\n    {ICODE, \"r r i0\", \"X 83 /7 R1 i2; Y \" SETX \" S0\", 0},  /* cmp r1,i2;...*/ \\\n    {ICODE, \"r r i2\", \"X 81 /7 R1 I2; Y \" SETX \" S0\", 0},  /* cmp r1,i2;...*/ \\\n    {ICODE, \"r m3 i0\", \"X 83 /7 m1 i2; Y \" SETX \" S0\", 0}, /* cmp m1,i2;...*/ \\\n    {ICODE, \"r m3 i2\", \"X 81 /7 m1 I2; Y \" SETX \" S0\", 0}, /* cmp m1,i2;...*/\n\n#define CMPS(ICODE, SETX)                                                     \\\n  {ICODE, \"r r r\", \"Y 3B r1 R2; Y \" SETX \" S0\", 0},        /* cmp r1,r2;...*/ \\\n    {ICODE, \"r r m2\", \"Y 3B r1 m2; Y \" SETX \" S0\", 0},     /* cmp r1,m2;...*/ \\\n    {ICODE, \"r r i0\", \"Y 83 /7 R1 i2; Y \" SETX \" S0\", 0},  /* cmp r1,i2;...*/ \\\n    {ICODE, \"r r i2\", \"Y 81 /7 R1 I2; Y \" SETX \" S0\", 0},  /* cmp r1,i2;...*/ \\\n    {ICODE, \"r m2 i0\", \"Y 83 /7 m1 i2; Y \" SETX \" S0\", 0}, /* cmp m1,i2;...*/ \\\n    {ICODE, \"r m2 i2\", \"Y 81 /7 m1 I2; Y \" SETX \" S0\", 0}, /* cmp m1,i2;...*/\n\n#define FEQ(ICODE, V, SET_OPCODE)                                                            \\\n  /*xor %eax,%eax;ucomiss r1,{r,m2};mov V,%edx;set[n]p r0;cmovne %rdx,%rax; mov %rax,r0:  */ \\\n  {ICODE, \"r r r\",                                                                           \\\n   \"33 h0 H0; 0F 2E r1 R2; BA \" V \"; \" SET_OPCODE \" H0; X 0F 45 h0 H2; X 8B r0 H0\", 0},      \\\n    {ICODE, \"r r md\",                                                                        \\\n     \"33 h0 H0; 0F 2E r1 m2; BA \" V \"; \" SET_OPCODE \" H0; X 0F 45 h0 H2; X 8B r0 H0\", 0},\n\n#define DEQ(ICODE, V, SET_OPCODE)                                                            \\\n  /*xor %eax,%eax;ucomisd r1,{r,m2};mov V,%edx;set[n]p r0;cmovne %rdx,%rax; mov %rax,r0:  */ \\\n  {ICODE, \"r r r\",                                                                           \\\n   \"33 h0 H0; 66 Y 0F 2E r1 R2; BA \" V \"; \" SET_OPCODE \" H0; X 0F 45 h0 H2; X 8B r0 H0\", 0}, \\\n    {ICODE, \"r r md\",                                                                        \\\n     \"33 h0 H0; 66 Y 0F 2E r1 m2; BA \" V \"; \" SET_OPCODE \" H0; X 0F 45 h0 H2; X 8B r0 H0\", 0},\n\n#define LDEQ(ICODE, V, SET_OPCODE)                                     \\\n  /*fld m2;fld m1;xor %eax,%eax;fucomip st,st(1);fstp %st;mov V,%edx;  \\\n    set[n]p r0;cmovne %rdx,%rax;mov %rax,r0: */                        \\\n  {ICODE, \"r mld mld\",                                                 \\\n   \"DB /5 m2; DB /5 m1; 33 h0 H0; DF E9; DD D8; BA \" V \"; \" SET_OPCODE \\\n   \" H0; X 0F 45 h0 H2; X 8B r0 H0\",                                   \\\n   0},\n\n#define FCMP(ICODE, SET_OPCODE)                                                 \\\n  /*xor %eax,%eax;ucomiss r1,r2;setx az; mov %rax,r0:  */                       \\\n  {ICODE, \"r r r\", \"33 h0 H0; Y 0F 2E r1 R2; \" SET_OPCODE \" H0;X 8B r0 H0\", 0}, \\\n    {ICODE, \"r r mf\", \"33 h0 H0; Y 0F 2E r1 m2; \" SET_OPCODE \" H0;X 8B r0 H0\", 0},\n\n#define DCMP(ICODE, SET_OPCODE)                                                    \\\n  /*xor %eax,%eax;ucomisd r1,r2;setx az; mov %rax,r0:  */                          \\\n  {ICODE, \"r r r\", \"33 h0 H0; 66 Y 0F 2E r1 R2; \" SET_OPCODE \" H0;X 8B r0 H0\", 0}, \\\n    {ICODE, \"r r md\", \"33 h0 H0; 66 Y 0F 2E r1 m2; \" SET_OPCODE \" H0;X 8B r0 H0\", 0},\n\n#define LDCMP(ICODE, SET_OPCODE)                                                                   \\\n  /*fld m2;fld m1;xor %eax,%eax;fcomip st,st(1);fstp %st;setx az; mov %rax,r0:  */                 \\\n  {ICODE, \"r mld mld\", \"DB /5 m2; DB /5 m1; 33 h0 H0; DF F1; DD D8; \" SET_OPCODE \" H0;X 8B r0 H0\", \\\n   0},\n\n#define BRS0(ICODE, SUFF, PREF, SHORT_JMP_OPCODE)                                             \\\n  {ICODE##SUFF, \"l r\", #PREF \" 85 r1 R1;\" SHORT_JMP_OPCODE \" l0\", 0}, /*test r0,r0;jxx rel8*/ \\\n    {ICODE##SUFF, \"l m3\", #PREF \" 83 /7 m1 v0;\" SHORT_JMP_OPCODE \" l0\", 0}, /*cmp m0,$0;jxx rel8*/\n\n#define BRS1(ICODE, SUFF, PREF, SHORT_JMP_OPCODE)                                                 \\\n  {ICODE##SUFF, \"l m0\", #PREF \" 80 /7 m1 v0;\" SHORT_JMP_OPCODE \" l0\", 0}, /*cmpb m0,$0;jxx rel8*/ \\\n    {ICODE##SUFF, \"l m1\", \"66 \" #PREF \" 83 /7 m1 v0;\" SHORT_JMP_OPCODE \" l0\", 0}, /*cmpw ...*/    \\\n    {ICODE##SUFF, \"l m2\", #PREF \" 83 /7 m1 v0;\" SHORT_JMP_OPCODE \" l0\", 0},       /*cmpl ...*/\n\n#define BRS(ICODE, SHORT_JMP_OPCODE)   \\\n  BRS0 (ICODE, , X, SHORT_JMP_OPCODE)  \\\n  BRS0 (ICODE, S, Y, SHORT_JMP_OPCODE) \\\n  BRS1 (ICODE, , Y, SHORT_JMP_OPCODE)  \\\n  BRS1 (ICODE, S, Y, SHORT_JMP_OPCODE)\n\n#define BR0(ICODE, SUFF, PREF, LONG_JMP_OPCODE)                                               \\\n  {ICODE##SUFF, \"L r\", #PREF \" 85 r1 R1;\" LONG_JMP_OPCODE \" L0\", 0}, /*test r0,r0;jxx rel32*/ \\\n    {ICODE##SUFF, \"L m3\", #PREF \" 83 /7 m1 v0;\" LONG_JMP_OPCODE \" L0\", 0}, /*cmp m0,$0;jxx rel32*/\n\n#define BR1(ICODE, SUFF, PREF, LONG_JMP_OPCODE)                                                   \\\n  {ICODE##SUFF, \"L m0\", #PREF \" 80 /7 m1 v0;\" LONG_JMP_OPCODE \" L0\", 0}, /*cmpb m0,$0;jxx rel32*/ \\\n    {ICODE##SUFF, \"L m1\", \"66 \" #PREF \" 83 /7 m1 v0;\" LONG_JMP_OPCODE \" L0\", 0}, /*cmpw ...*/     \\\n    {ICODE##SUFF, \"L m2\", #PREF \" 83 /7 m1 v0;\" LONG_JMP_OPCODE \" L0\", 0},       /*cmpl ...*/\n\n#define BR(ICODE, LONG_JMP_OPCODE)   \\\n  BR0 (ICODE, , X, LONG_JMP_OPCODE)  \\\n  BR0 (ICODE, S, Y, LONG_JMP_OPCODE) \\\n  BR1 (ICODE, , Y, LONG_JMP_OPCODE)  \\\n  BR1 (ICODE, S, Y, LONG_JMP_OPCODE)\n\n#define BCMPS(ICODE, SHORT_JMP_OPCODE)                                                     \\\n  {ICODE, \"l r r\", \"X 3B r1 R2;\" SHORT_JMP_OPCODE \" l0\", 0},        /*cmp r0,r1;jxx rel8*/ \\\n    {ICODE, \"l r m3\", \"X 3B r1 m2;\" SHORT_JMP_OPCODE \" l0\", 0},     /*cmp r0,m1;...*/      \\\n    {ICODE, \"l r i0\", \"X 83 /7 R1 i2;\" SHORT_JMP_OPCODE \" l0\", 0},  /*cmp r0,i1;...*/      \\\n    {ICODE, \"l r i2\", \"X 81 /7 R1 I2;\" SHORT_JMP_OPCODE \" l0\", 0},  /*cmp r0,i1;...*/      \\\n    {ICODE, \"l m3 i0\", \"X 83 /7 m1 i2;\" SHORT_JMP_OPCODE \" l0\", 0}, /*cmp m0,i1;...*/      \\\n    {ICODE, \"l m3 i2\", \"X 81 /7 m1 I2;\" SHORT_JMP_OPCODE \" l0\", 0}, /*cmp m0,i1;...*/\n#define SBCMPS(ICODE, SHORT_JMP_OPCODE)                                                    \\\n  {ICODE, \"l r r\", \"Y 3B r1 R2;\" SHORT_JMP_OPCODE \" l0\", 0},        /*cmp r0,r1;jxx rel8*/ \\\n    {ICODE, \"l r m2\", \"Y 3B r1 m2;\" SHORT_JMP_OPCODE \" l0\", 0},     /*cmp r0,m1;...*/      \\\n    {ICODE, \"l r i0\", \"Y 83 /7 R1 i2;\" SHORT_JMP_OPCODE \" l0\", 0},  /*cmp r0,i1;...*/      \\\n    {ICODE, \"l r i2\", \"Y 81 /7 R1 I2;\" SHORT_JMP_OPCODE \" l0\", 0},  /*cmp r0,i1;...*/      \\\n    {ICODE, \"l m2 i0\", \"Y 83 /7 m1 i2;\" SHORT_JMP_OPCODE \" l0\", 0}, /*cmp m0,i1;...*/      \\\n    {ICODE, \"l m2 i2\", \"Y 81 /7 m1 I2;\" SHORT_JMP_OPCODE \" l0\", 0}, /*cmp m0,i1;...*/\n\n#define BCMP(ICODE, LONG_JMP_OPCODE)                                                       \\\n  {ICODE, \"L r r\", \"X 3B r1 R2;\" LONG_JMP_OPCODE \" L0\", 0},        /*cmp r0,r1;jxx rel32*/ \\\n    {ICODE, \"L r m3\", \"X 3B r1 m2;\" LONG_JMP_OPCODE \" L0\", 0},     /*cmp r0,m1;...*/       \\\n    {ICODE, \"L r i0\", \"X 83 /7 R1 i2;\" LONG_JMP_OPCODE \" L0\", 0},  /*cmp r0,i1;...*/       \\\n    {ICODE, \"L r i2\", \"X 81 /7 R1 I2;\" LONG_JMP_OPCODE \" L0\", 0},  /*cmp r0,i1;...*/       \\\n    {ICODE, \"L m3 i0\", \"X 83 /7 m1 i2;\" LONG_JMP_OPCODE \" L0\", 0}, /*cmp m0,i1;...*/       \\\n    {ICODE, \"L m3 i2\", \"X 81 /7 m1 I2;\" LONG_JMP_OPCODE \" L0\", 0}, /*cmp m0,i1;...*/\n#define SBCMP(ICODE, LONG_JMP_OPCODE)                                                      \\\n  {ICODE, \"L r r\", \"Y 3B r1 R2;\" LONG_JMP_OPCODE \" L0\", 0},        /*cmp r0,r1;jxx rel32*/ \\\n    {ICODE, \"L r m2\", \"Y 3B r1 m2;\" LONG_JMP_OPCODE \" L0\", 0},     /*cmp r0,m1;...*/       \\\n    {ICODE, \"L r i0\", \"Y 83 /7 R1 i2;\" LONG_JMP_OPCODE \" L0\", 0},  /*cmp r0,i1;...*/       \\\n    {ICODE, \"L r i2\", \"Y 81 /7 R1 I2;\" LONG_JMP_OPCODE \" L0\", 0},  /*cmp r0,i1;...*/       \\\n    {ICODE, \"L m2 i0\", \"Y 83 /7 m1 i2;\" LONG_JMP_OPCODE \" L0\", 0}, /*cmp m0,i1;...*/       \\\n    {ICODE, \"L m2 i2\", \"Y 81 /7 m1 I2;\" LONG_JMP_OPCODE \" L0\", 0}, /*cmp m0,i1;...*/\n\n#define FBCMPS(ICODE, SHORT_JMP_OPCODE) \\\n  {ICODE, \"l r r\", \"Y 0F 2E r1 R2;\" SHORT_JMP_OPCODE \" l0\", 0}, /* ucomiss r0,r1;jxx rel8*/\n#define DBCMPS(ICODE, SHORT_JMP_OPCODE) \\\n  {ICODE, \"l r r\", \"66 Y 0F 2E r1 R2;\" SHORT_JMP_OPCODE \" l0\", 0}, /* ucomisd r0,r1;jxx rel8*/\n#define LDBCMPS(ICODE, SHORT_JMP_OPCODE)                 \\\n  /* fld m2;fld m1; fcomip st,st(1); fstp st; jxx rel8*/ \\\n  {ICODE, \"l mld mld\", \"DB /5 m2; DB /5 m1; DF F1; DD D8; \" SHORT_JMP_OPCODE \" l0\", 0},\n\n#define FBCMP(ICODE, LONG_JMP_OPCODE) \\\n  {ICODE, \"L r r\", \"Y 0F 2E r1 R2;\" LONG_JMP_OPCODE \" L0\", 0}, /* ucomiss r0,r1;jxx rel32*/\n#define DBCMP(ICODE, LONG_JMP_OPCODE) \\\n  {ICODE, \"L r r\", \"66 Y 0F 2E r1 R2;\" LONG_JMP_OPCODE \" L0\", 0}, /* ucomisd r0,r1;jxx rel32*/\n#define LDBCMP(ICODE, LONG_JMP_OPCODE)                    \\\n  /* fld m2;fld m1; fcomip st,st(1); fstp st; jxx rel32*/ \\\n  {ICODE, \"L mld mld\", \"DB /5 m2; DB /5 m1; DF F1; DD D8; \" LONG_JMP_OPCODE \" L0\", 0},\n\nstatic struct pattern patterns[] = {\n  {MIR_MOV, \"r z\", \"Y 33 r0 R0\", 0},      /* xor r0,r0 -- 32 bit xor */\n  {MIR_MOV, \"r r\", \"X 8B r0 R1\", 0},      /* mov r0,r1 */\n  {MIR_MOV, \"r m3\", \"X 8B r0 m1\", 0},     /* mov r0,m1 */\n  {MIR_MOV, \"m3 r\", \"X 89 r1 m0\", 0},     /* mov m0,r1 */\n  {MIR_MOV, \"r i2\", \"X C7 /0 R0 I1\", 0},  /* mov r0,i32 */\n  {MIR_MOV, \"m3 i2\", \"X C7 /0 m0 I1\", 0}, /* mov m0,i32 */\n  {MIR_MOV, \"r i3\", \"X B8 +0 J1\", 0},     /* mov r0,i64 */\n\n  {MIR_MOV, \"m0 r\", \"Z 88 r1 m0\", 0},    /* mov m0, r1 */\n  {MIR_MOV, \"m1 r\", \"66 Y 89 r1 m0\", 0}, /* mov m0, r1 */\n  {MIR_MOV, \"m2 r\", \"Y 89 r1 m0\", 0},    /* mov m0, r1 */\n\n  {MIR_MOV, \"r ms0\", \"X 0F BE r0 m1\", 0}, /* movsx r0, m1 */\n  {MIR_MOV, \"r ms1\", \"X 0F BF r0 m1\", 0}, /* movsx r0, m1 */\n  {MIR_MOV, \"r ms2\", \"X 63 r0 m1\", 0},    /* movsx r0, m1 */\n\n  {MIR_MOV, \"r mu0\", \"X 0F B6 r0 m1\", 0}, /* movzx r0, m1 */\n  {MIR_MOV, \"r mu1\", \"X 0F B7 r0 m1\", 0}, /* movzx r0, m1 */\n  {MIR_MOV, \"r mu2\", \"Y 8B r0 m1\", 0},    /* mov r0, m1 */\n\n  {MIR_MOV, \"m0 i0\", \"Y C6 /0 m0 i1\", 0}, /* mov m0,i8 */\n  {MIR_MOV, \"m2 i2\", \"Y C7 /0 m0 I1\", 0}, /* mov m0,i32 */\n\n  {MIR_FMOV, \"r r\", \"Y 0F 28 r0 R1\", 0},     /* movaps r0,r1 */\n  {MIR_FMOV, \"r mf\", \"F3 Y 0F 10 r0 m1\", 0}, /* movss r0,m32 */\n  {MIR_FMOV, \"mf r\", \"F3 Y 0F 11 r1 m0\", 0}, /* movss r0,m32 */\n\n  {MIR_DMOV, \"r r\", \"66 Y 0F 28 r0 R1\", 0},  /* movapd r0,r1 */\n  {MIR_DMOV, \"r md\", \"F2 Y 0F 10 r0 m1\", 0}, /* movsd r0,m64 */\n  {MIR_DMOV, \"md r\", \"F2 Y 0F 11 r1 m0\", 0}, /* movsd m64,r0 */\n\n  {MIR_LDMOV, \"mld h32\", \"DB /7 m0\", 0},        /*only for ret and calls in given order: fstp m0 */\n  {MIR_LDMOV, \"h32 mld\", \"DB /5 m1\", 0},        /*only for ret and calls in given order: fld m1 */\n  {MIR_LDMOV, \"mld h33\", \"D9 C9; DB /7 m0\", 0}, /*only for ret and calls: fxch;fstp m0 */\n  {MIR_LDMOV, \"h33 mld\", \"DB /5 m1; D9 C9\", 0}, /*only for ret and calls: fld m1; fxch */\n  {MIR_LDMOV, \"mld mld\", \"DB /5 m1; DB /7 m0\", 0}, /* fld m1; fstp m0 */\n\n#define STR(c) #c\n#define STR_VAL(c) STR (c)\n\n  {MIR_UNSPEC, \"c\" STR_VAL (MOVDQA_CODE) \" r r\", \"66 Y 0F 6F r1 R2\", 0},  /* movdqa r0,r1 */\n  {MIR_UNSPEC, \"c\" STR_VAL (MOVDQA_CODE) \" r md\", \"66 Y 0F 6F r1 m2\", 0}, /* movdqa r0,m128 */\n  {MIR_UNSPEC, \"c\" STR_VAL (MOVDQA_CODE) \" md r\", \"66 Y 0F 7F r2 m1\", 0}, /* movdqa m128,r0 */\n\n  {MIR_EXT8, \"r r\", \"X 0F BE r0 R1\", 0},    /* movsx r0,r1 */\n  {MIR_EXT8, \"r m0\", \"X 0F BE r0 m1\", 0},   /* movsx r0,m1 */\n  {MIR_EXT16, \"r r\", \"X 0F BF r0 R1\", 0},   /* movsx r0,r1 */\n  {MIR_EXT16, \"r m1\", \"X 0F BF r0 m1\", 0},  /* movsx r0,m1 */\n  {MIR_EXT32, \"r r\", \"X 63 r0 R1\", 0},      /* movsx r0,r1 */\n  {MIR_EXT32, \"r m2\", \"X 63 r0 m1\", 0},     /* movsx r0,m1 */\n  {MIR_UEXT8, \"r r\", \"Y 0F B6 r0 S1\", 0},   /* movzx r0,r1 */\n  {MIR_UEXT8, \"r m0\", \"X 0F B6 r0 m1\", 0},  /* movzx r0,m1 */\n  {MIR_UEXT16, \"r r\", \"X 0F B7 r0 R1\", 0},  /* movzx r0,r1 */\n  {MIR_UEXT16, \"r m1\", \"X 0F B7 r0 m1\", 0}, /* movzx r0,m1 */\n  {MIR_UEXT32, \"r r\", \"Y 8B r0 R1\", 0},     /* mov r0,r1 */\n  {MIR_UEXT32, \"r m2\", \"Y 8B r0 m1\", 0},    /* mov r0,m1 */\n\n  {MIR_I2F, \"r r\", \"F3 X 0F 2A r0 R1\", 0},                  /* cvtsi2ss r0,r1 */\n  {MIR_I2F, \"r m3\", \"F3 X 0F 2A r0 m1\", 0},                 /* cvtsi2ss r0,m1 */\n  {MIR_I2D, \"r r\", \"F2 X 0F 2A r0 R1\", 0},                  /* cvtsi2sd r0,r1 */\n  {MIR_I2D, \"r m3\", \"F2 X 0F 2A r0 m1\", 0},                 /* cvtsi2sd r0,m1 */\n  {MIR_I2LD, \"mld r\", \"X 89 r1 mt; DF /5 mt; DB /7 m0\", 0}, /*mov -16(sp),r1;fild -16(sp);fstp m0 */\n\n  {MIR_F2I, \"r r\", \"F3 X 0F 2C r0 R1\", 0},  /* cvttss2si r0,r1 */\n  {MIR_F2I, \"r mf\", \"F3 X 0F 2C r0 m1\", 0}, /* cvttss2si r0,m1 */\n  {MIR_D2I, \"r r\", \"F2 X 0F 2C r0 R1\", 0},  /* cvttsd2si r0,r1 */\n  {MIR_D2I, \"r md\", \"F2 X 0F 2C r0 m1\", 0}, /* cvttsd2si r0,m1 */\n\n  {MIR_F2D, \"r r\", \"F3 Y 0F 5A r0 R1\", 0},  /* cvtss2sd r0,r1 */\n  {MIR_F2D, \"r mf\", \"F3 Y 0F 5A r0 m1\", 0}, /* cvtss2sd r0,m1 */\n                                            /* fld m1;fstpl -16(sp);movsd r0,-16(sp): */\n  {MIR_LD2D, \"r mld\", \"DB /5 m1; DD /3 mt; F2 Y 0F 10 r0 mt\", 0},\n\n  {MIR_D2F, \"r r\", \"F2 Y 0F 5A r0 R1\", 0},  /* cvtsd2ss r0,r1 */\n  {MIR_D2F, \"r md\", \"F2 Y 0F 5A r0 m1\", 0}, /* cvtsd2ss r0,m1 */\n  /* fld m1;fstps -16(sp);movss r0, -16(sp): */\n  {MIR_LD2F, \"r mld\", \"DB /5 m1; D9 /3 mt; F3 Y 0F 10 r0 mt\", 0},\n\n  /* movss -16(sp), r1; flds -16(sp); fstp m0: */\n  {MIR_F2LD, \"mld r\", \"F3 Y 0F 11 r1 mt; D9 /0 mt; DB /7 m0\", 0},\n  {MIR_F2LD, \"mld mf\", \"D9 /0 m1; DB /7 m0\", 0}, /* flds m1; fstp m0 */\n  /* movsd -16(sp), r1; fldl -16(sp); fstp m0: */\n  {MIR_D2LD, \"mld r\", \"F2 Y 0F 11 r1 mt; DD /0 mt; DB /7 m0\", 0},\n  {MIR_D2LD, \"mld md\", \"DD /0 m1; DB /7 m0\", 0}, /* fldl m1; fstp m0 */\n\n  /* lea r0, 15(r1); and r0, r0, -16; sub sp, r0; mov r0, sp: */\n  {MIR_ALLOCA, \"r r\", \"Y 8D r0 adF; X 81 /4 R0 VFFFFFFF0; X 2B h04 R0; X 8B r0 H04\", 0},\n  {MIR_ALLOCA, \"r i2\", \"X 81 /5 H04 I1; X 8B r0 H04\", 0}, /* sub sp, i2; mov r0, sp */\n\n  {MIR_BSTART, \"r\", \"X 8B r0 H4\", 0}, /* r0 = sp */\n  {MIR_BEND, \"r\", \"X 8B h4 R0\", 0},   /* sp = r0 */\n\n  {MIR_NEG, \"r 0\", \"X F7 /3 R1\", 0},   /* neg r0 */\n  {MIR_NEG, \"m3 0\", \"X F7 /3 m1\", 0},  /* neg m0 */\n  {MIR_NEGS, \"r 0\", \"Y F7 /3 R1\", 0},  /* neg r0 */\n  {MIR_NEGS, \"m2 0\", \"Y F7 /3 m1\", 0}, /* neg m0 */\n\n  {MIR_FNEG, \"r 0\", \"Y 0F 57 r0 c0000000080000000\", 0},    /* xorps r0,80000000 */\n  {MIR_DNEG, \"r 0\", \"66 Y 0F 57 r0 c8000000000000000\", 0}, /* xorpd r0,0x8000000000000000 */\n  {MIR_LDNEG, \"mld mld\", \"DB /5 m1; D9 E0; DB /7 m0\", 0},  /* fld m1; fchs; fstp m0 */\n\n  IOP (MIR_ADD, \"03\", \"01\", \"83 /0\", \"81 /0\") /* x86_64 int additions */\n\n  {MIR_ADD, \"r r r\", \"X 8D r0 ap\", 0},   /* lea r0,(r1,r2)*/\n  {MIR_ADD, \"r r i2\", \"X 8D r0 ap\", 0},  /* lea r0,i2(r1)*/\n  {MIR_ADDS, \"r r r\", \"Y 8D r0 ap\", 0},  /* lea r0,(r1,r2)*/\n  {MIR_ADDS, \"r r i2\", \"Y 8D r0 ap\", 0}, /* lea r0,i2(r1)*/\n\n  IOP (MIR_SUB, \"2B\", \"29\", \"83 /5\", \"81 /5\") /* x86_64 int subtractions */\n\n  IOP (MIR_ADDO, \"03\", \"01\", \"83 /0\", \"81 /0\") /* x86_64 int additions with ovfl flag */\n  IOP (MIR_SUBO, \"2B\", \"29\", \"83 /5\", \"81 /5\") /* x86_64 int subtractions with ovfl flag */\n\n#define IMULL(ICODE, ICODES)                                     \\\n  {ICODE, \"r 0 r\", \"X 0F AF r0 R2\", 0},      /* imul r0,r1*/     \\\n    {ICODE, \"r 0 m3\", \"X 0F AF r0 m2\", 0},   /* imul r0,m1*/     \\\n    {ICODE, \"r r i2\", \"X 69 r0 R1 I2\", 0},   /* imul r0,r1,i32*/ \\\n    {ICODE, \"r m3 i2\", \"X 69 r0 m1 I2\", 0},  /* imul r0,m1,i32*/ \\\n    {ICODES, \"r 0 r\", \"Y 0F AF r0 R2\", 0},   /* imul r0,r1*/     \\\n    {ICODES, \"r 0 m2\", \"Y 0F AF r0 m2\", 0},  /* imul r0,m1*/     \\\n    {ICODES, \"r r i2\", \"Y 69 r0 R1 I2\", 0},  /* imul r0,r1,i32*/ \\\n    {ICODES, \"r m2 i2\", \"Y 69 r0 m1 I2\", 0}, /* imul r0,m1,i32*/\n\n  IMULL (MIR_MUL, MIR_MULS)\n\n    {MIR_MUL, \"r r s\", \"X 8D r0 ap\", 0}, /* lea r0,(,r1,s2)*/\n  {MIR_MULS, \"r r s\", \"Y 8D r0 ap\", 0},  /* lea r0,(,r1,s2)*/\n\n  IMULL (MIR_MULO, MIR_MULOS)\n\n    {MIR_UMULO, \"h0 0 r\", \"X F7 /4 R2\", 0}, /* mul rax,r1*/\n  {MIR_UMULO, \"h0 0 m3\", \"X F7 /4 m2\", 0},  /* mul rax,m1*/\n  {MIR_UMULOS, \"h0 0 r\", \"Y F7 /4 R2\", 0},  /* mul rax,r1*/\n  {MIR_UMULOS, \"h0 0 m2\", \"Y F7 /4 m2\", 0}, /* mul rax,m1*/\n\n  {MIR_DIV, \"h0 h0 r\", \"X 99; X F7 /7 R2\", 0},  /* cqo; idiv r2*/\n  {MIR_DIV, \"h0 h0 m3\", \"X 99; X F7 /7 m2\", 0}, /* cqo; idiv m2*/\n  {MIR_DIVS, \"h0 h0 r\", \"99; Y F7 /7 R2\", 0},   /* cdq; idiv r2*/\n  {MIR_DIVS, \"h0 h0 m2\", \"99; Y F7 /7 m2\", 0},  /* cdq; idiv m2*/\n\n  {MIR_UDIV, \"h0 h0 r\", \"31 D2; X F7 /6 R2\", 0},   /* xorl edx,edx; div r2*/\n  {MIR_UDIV, \"h0 h0 m3\", \"31 D2; X F7 /6 m2\", 0},  /* xorl edx,edx; div m2*/\n  {MIR_UDIVS, \"h0 h0 r\", \"31 D2; Y F7 /6 R2\", 0},  /* xorl edx,edx; div r2*/\n  {MIR_UDIVS, \"h0 h0 m2\", \"31 D2; Y F7 /6 m2\", 0}, /* xorl edx,edx; div m2*/\n\n  {MIR_MOD, \"h2 h0 r\", \"X 99; X F7 /7 R2\", 0},  /* cqo; idiv r2*/\n  {MIR_MOD, \"h2 h0 m3\", \"X 99; X F7 /7 m2\", 0}, /* cqo; idiv m2*/\n  {MIR_MODS, \"h2 h0 r\", \"99; Y F7 /7 R2\", 0},   /* cdq; idiv r2*/\n  {MIR_MODS, \"h2 h0 m2\", \"99; Y F7 /7 m2\", 0},  /* cdq; idiv m2*/\n\n  {MIR_UMOD, \"h2 h0 r\", \"31 D2; X F7 /6 R2\", 0},   /* xorl edx,edx; div r2*/\n  {MIR_UMOD, \"h2 h0 m3\", \"31 D2; X F7 /6 m2\", 0},  /* xorl edx,edx; div m2*/\n  {MIR_UMODS, \"h2 h0 r\", \"31 D2; Y F7 /6 R2\", 0},  /* xorl edx,edx; div r2*/\n  {MIR_UMODS, \"h2 h0 m2\", \"31 D2; Y F7 /6 m2\", 0}, /* xorl edx,edx; div m2*/\n\n  IOP (MIR_AND, \"23\", \"21\", \"83 /4\", \"81 /4\")                                            /*ands*/\n  IOP (MIR_OR, \"0B\", \"09\", \"83 /1\", \"81 /1\") IOP (MIR_XOR, \"33\", \"31\", \"83 /6\", \"81 /6\") /*(x)ors*/\n\n  FOP (MIR_FADD, \"F3 Y 0F 58\") DOP (MIR_DADD, \"F2 Y 0F 58\") FOP (MIR_FSUB, \"F3 Y 0F 5C\") /**/\n  DOP (MIR_DSUB, \"F2 Y 0F 5C\") FOP (MIR_FMUL, \"F3 Y 0F 59\") DOP (MIR_DMUL, \"F2 Y 0F 59\") /**/\n  FOP (MIR_FDIV, \"F3 Y 0F 5E\") DOP (MIR_DDIV, \"F2 Y 0F 5E\")                              /**/\n\n  LDOP (MIR_LDADD, \"DE C1\") LDOP (MIR_LDSUB, \"DE E9\") /* long double adds/subs */\n  LDOP (MIR_LDMUL, \"DE C9\") LDOP (MIR_LDDIV, \"DE F9\") /* long double muls/divs */\n\n  SHOP (MIR_LSH, \"D3 /4\", \"C1 /4\") SHOP (MIR_RSH, \"D3 /7\", \"C1 /7\") /* arithm shifts */\n  SHOP (MIR_URSH, \"D3 /5\", \"C1 /5\")                                 /* logical shifts */\n\n  SHOPS (MIR_LSHS, \"D3 /4\", \"C1 /4\") SHOPS (MIR_RSHS, \"D3 /7\", \"C1 /7\") /* arithm shifts */\n  SHOPS (MIR_URSHS, \"D3 /5\", \"C1 /5\")                                   /* logical shifts */\n\n  CMP (MIR_EQ, \"0F 94\") CMP (MIR_NE, \"0F 95\") CMP (MIR_LT, \"0F 9C\")   /* 1.int cmps */\n  CMP (MIR_ULT, \"0F 92\") CMP (MIR_LE, \"0F 9E\") CMP (MIR_ULE, \"0F 96\") /* 2.int cmps */\n  CMP (MIR_GT, \"0F 9F\") CMP (MIR_UGT, \"0F 97\") CMP (MIR_GE, \"0F 9D\")  /* 3.int cmps */\n  CMP (MIR_UGE, \"0F 93\")                                              /* 4.int cmps */\n\n  CMPS (MIR_EQS, \"0F 94\") CMPS (MIR_NES, \"0F 95\") CMPS (MIR_LTS, \"0F 9C\")   /* 1.short cmps */\n  CMPS (MIR_ULTS, \"0F 92\") CMPS (MIR_LES, \"0F 9E\") CMPS (MIR_ULES, \"0F 96\") /* 2.short cmps */\n  CMPS (MIR_GTS, \"0F 9F\") CMPS (MIR_UGTS, \"0F 97\") CMPS (MIR_GES, \"0F 9D\")  /* 3.short cmps */\n  CMPS (MIR_UGES, \"0F 93\")                                                  /* 4.short cmps */\n\n  FEQ (MIR_FEQ, \"V0\", \"0F 9B\") DEQ (MIR_DEQ, \"V0\", \"0F 9B\")   /* 1. fp cmps */\n  LDEQ (MIR_LDEQ, \"V0\", \"0F 9B\") FEQ (MIR_FNE, \"V1\", \"0F 9A\") /* 2. fp cmps */\n  DEQ (MIR_DNE, \"V1\", \"0F 9A\") LDEQ (MIR_LDNE, \"V1\", \"0F 9A\") /* 3. fp cmps */\n\n  FCMP (MIR_FLT, \"0F 92\") DCMP (MIR_DLT, \"0F 92\") LDCMP (MIR_LDLT, \"0F 92\") /*4*/\n  FCMP (MIR_FLE, \"0F 96\") DCMP (MIR_DLE, \"0F 96\") LDCMP (MIR_LDLE, \"0F 96\") /*5*/\n  FCMP (MIR_FGT, \"0F 97\") DCMP (MIR_DGT, \"0F 97\") LDCMP (MIR_LDGT, \"0F 97\") /*6*/\n  FCMP (MIR_FGE, \"0F 93\") DCMP (MIR_DGE, \"0F 93\") LDCMP (MIR_LDGE, \"0F 93\") /*7*/\n\n  {MIR_JMP, \"L\", \"E9 L0\", 0}, /* 32-bit offset jmp */\n  {MIR_JMP, \"l\", \"EB l0\", 0}, /* 8-bit offset jmp */\n\n  {MIR_LADDR, \"r L\", \"X 8D r0 q L1\", 0}, /* ip-relative addressing */\n  {MIR_JMPI, \"r\", \"Y FF /4 R0\", 0},      /* jmp *r */\n  {MIR_JMPI, \"m3\", \"Y FF /4 m0\", 0},     /* jmp *m0 */\n\n  /* lea table_offset(rip),r11; jmp *(r11,r,8); TableContent */\n  {MIR_SWITCH, \"r $\", \"X 8D hB T; Y FF /4 mT\", 0},\n\n  BRS (MIR_BT, \"75\") BRS (MIR_BF, \"74\")     /* short branches */\n  BR (MIR_BT, \"0F 85\") BR (MIR_BF, \"0F 84\") /* branches */\n\n  {MIR_BO, \"l\", \"70 l0\", 0},   /* 8-bit offset jmp on signed overflow */\n  {MIR_UBO, \"l\", \"72 l0\", 0},  /* 8-bit offset jmp on unsigned overflow */\n  {MIR_BNO, \"l\", \"71 l0\", 0},  /* 8-bit offset jmp on signed non-overflow */\n  {MIR_UBNO, \"l\", \"73 l0\", 0}, /* 8-bit offset jmp on unsigned non-overflow */\n\n  {MIR_BO, \"L\", \"0F 80 L0\", 0},   /* 32-bit offset jmp on signed overflow */\n  {MIR_UBO, \"L\", \"0F 82 L0\", 0},  /* 32-bit offset jmp on unsigned overflow */\n  {MIR_BNO, \"L\", \"0F 81 L0\", 0},  /* 32-bit offset jmp on signed non-overflow */\n  {MIR_UBNO, \"L\", \"0F 83 L0\", 0}, /* 32-bit offset jmp on unsigned non-overflow */\n\n  BCMPS (MIR_BEQ, \"74\") BCMPS (MIR_BNE, \"75\")  /* 1. int compare and branch */\n  BCMPS (MIR_BLT, \"7C\") BCMPS (MIR_UBLT, \"72\") /* 2. int compare and branch */\n  BCMPS (MIR_BLE, \"7E\") BCMPS (MIR_UBLE, \"76\") /* 3. int compare and branch */\n  BCMPS (MIR_BGT, \"7F\") BCMPS (MIR_UBGT, \"77\") /* 4. int compare and branch */\n  BCMPS (MIR_BGE, \"7D\") BCMPS (MIR_UBGE, \"73\") /* 5. int compare and branch */\n\n  SBCMPS (MIR_BEQS, \"74\") SBCMPS (MIR_BNES, \"75\")  /* 1. short compare and branch */\n  SBCMPS (MIR_BLTS, \"7C\") SBCMPS (MIR_UBLTS, \"72\") /* 2. short compare and branch */\n  SBCMPS (MIR_BLES, \"7E\") SBCMPS (MIR_UBLES, \"76\") /* 3. short compare and branch */\n  SBCMPS (MIR_BGTS, \"7F\") SBCMPS (MIR_UBGTS, \"77\") /* 4. short compare and branch */\n  SBCMPS (MIR_BGES, \"7D\") SBCMPS (MIR_UBGES, \"73\") /* 5. short compare and branch */\n\n  BCMP (MIR_BEQ, \"0F 84\") BCMP (MIR_BNE, \"0F 85\")  /* 1. int compare and branch */\n  BCMP (MIR_BLT, \"0F 8C\") BCMP (MIR_UBLT, \"0F 82\") /* 2. int compare and branch */\n  BCMP (MIR_BLE, \"0F 8E\") BCMP (MIR_UBLE, \"0F 86\") /* 3. int compare and branch */\n  BCMP (MIR_BGT, \"0F 8F\") BCMP (MIR_UBGT, \"0F 87\") /* 4. int compare and branch */\n  BCMP (MIR_BGE, \"0F 8D\") BCMP (MIR_UBGE, \"0F 83\") /* 5. int compare and branch */\n\n  SBCMP (MIR_BEQS, \"0F 84\") SBCMP (MIR_BNES, \"0F 85\")  /* 1. short compare and branch */\n  SBCMP (MIR_BLTS, \"0F 8C\") SBCMP (MIR_UBLTS, \"0F 82\") /* 2. short compare and branch */\n  SBCMP (MIR_BLES, \"0F 8E\") SBCMP (MIR_UBLES, \"0F 86\") /* 3. short compare and branch */\n  SBCMP (MIR_BGTS, \"0F 8F\") SBCMP (MIR_UBGTS, \"0F 87\") /* 4. short compare and branch */\n  SBCMP (MIR_BGES, \"0F 8D\") SBCMP (MIR_UBGES, \"0F 83\") /* 5. short compare and branch */\n\n#if 0 /* it is switched off because we change the following insn in machinize pass: */\n  FBCMP (MIR_FBLT, \"0F 82\") DBCMP (MIR_DBLT, \"0F 82\")   /* 1. fp cmp and branch */\n  LDBCMP (MIR_LDBLT, \"0F 82\") FBCMP (MIR_FBLE, \"0F 86\") /* 2. fp cmp and branch */\n  DBCMP (MIR_DBLE, \"0F 86\") LDBCMP (MIR_LDBLE, \"0F 86\") /* 3. fp cmp and branch */\n#endif\n\n  FBCMPS (MIR_FBGT, \"77\") DBCMPS (MIR_DBGT, \"77\")   /* fp cmp and short branch */\n  LDBCMPS (MIR_LDBGT, \"77\") FBCMPS (MIR_FBGE, \"73\") /* fp cmp and short branch */\n  DBCMPS (MIR_DBGE, \"73\") LDBCMPS (MIR_LDBGE, \"73\") /* fp cmp and short branch */\n\n  FBCMP (MIR_FBGT, \"0F 87\") DBCMP (MIR_DBGT, \"0F 87\")   /* fp cmp and branch */\n  LDBCMP (MIR_LDBGT, \"0F 87\") FBCMP (MIR_FBGE, \"0F 83\") /* fp cmp and branch */\n  DBCMP (MIR_DBGE, \"0F 83\") LDBCMP (MIR_LDBGE, \"0F 83\") /* fp cmp and branch */\n\n  /* we don't have short branch patterns for NE as the label will be in two branches: */\n  {MIR_FBEQ, \"l r r\", \"Y 0F 2E r1 R2; 7A v2; 74 l0\", 0},    /* ucomiss r0,r1;jp l;je rel32 l: */\n  {MIR_DBEQ, \"l r r\", \"66 Y 0F 2E r1 R2; 7A v2; 74 l0\", 0}, /* ucomisd r0,r1;jp l;je rel32 l: */\n  /* fld m2;fld m1;fucomip st,st1;fstp st;jp l;je rel32 l: */\n  {MIR_LDBEQ, \"l mld mld\", \"DB /5 m2; DB /5 m1; DF E9; DD D8; 7A v2; 74 l0\", 0},\n\n  {MIR_FBEQ, \"L r r\", \"Y 0F 2E r1 R2; 7A v6; 0F 84 L0\", 0},    /* ucomiss r0,r1;jp L;je rel32 L: */\n  {MIR_DBEQ, \"L r r\", \"66 Y 0F 2E r1 R2; 7A v6; 0F 84 L0\", 0}, /* ucomisd r0,r1;jp L;je rel32 L: */\n  /* fld m2;fld m1;fucomip st,st1;fstp st;jp L;je rel32 L: */\n  {MIR_LDBEQ, \"L mld mld\", \"DB /5 m2; DB /5 m1; DF E9; DD D8; 7A v6; 0F 84 L0\", 0},\n  {MIR_FBNE, \"L r r\", \"Y 0F 2E r1 R2; 0F 8A L0; 0F 85 L0\", 0}, /* ucomiss r0,r1;jp rel32;jne rel32*/\n  {MIR_DBNE, \"L r r\", \"66 Y 0F 2E r1 R2; 0F 8A L0; 0F 85 L0\",\n   0}, /* ucomisd r0,r1;jp rel32;jne rel32*/\n  /* fld m2;fld m1;fucomip st,st1;fstp st;jp rel32;jne rel32 */\n  {MIR_LDBNE, \"L mld mld\", \"DB /5 m2; DB /5 m1; DF E9; DD D8; 0F 8A L0; 0F 85 L0\", 0},\n\n  {MIR_CALL, \"X i3 $\", \"FF /2 P1\", 0},  /* call *rel32(rip)  */\n  {MIR_CALL, \"X r $\", \"Y FF /2 R1\", 0}, /* call *r1 */\n  {MIR_RET, \"$\", \"C3\", 0},              /* ret ax, dx, xmm0, xmm1, st0, st1  */\n\n  {MIR_JCALL, \"X i3 $\", \"FF /4 P1\", 0},  /* jmp *rel32(rip)  */\n  {MIR_JCALL, \"X r $\", \"Y FF /4 R1\", 0}, /* jmp *r */\n  {MIR_JRET, \"r $\", \"Y FF /4 R0\", 0},    /* jmp *r  */\n};\n\nstatic void target_get_early_clobbered_hard_regs (MIR_insn_t insn, MIR_reg_t *hr1, MIR_reg_t *hr2) {\n  MIR_insn_code_t code = insn->code;\n\n  *hr1 = *hr2 = MIR_NON_VAR;\n  if (code == MIR_DIV || code == MIR_UDIV || code == MIR_DIVS || code == MIR_UDIVS\n      || code == MIR_MOD || code == MIR_UMOD || code == MIR_MODS || code == MIR_UMODS\n      || code == MIR_UMULO || code == MIR_UMULOS) {\n    *hr1 = DX_HARD_REG;\n  } else if (code == MIR_FEQ || code == MIR_FNE || code == MIR_DEQ || code == MIR_DNE\n             || code == MIR_LDEQ || code == MIR_LDNE) {\n    *hr1 = AX_HARD_REG;\n    *hr2 = DX_HARD_REG;\n  } else if (code == MIR_FLT || code == MIR_FLE || code == MIR_FGT || code == MIR_FGE\n             || code == MIR_DLT || code == MIR_DLE || code == MIR_DGT || code == MIR_DGE\n             || code == MIR_LDLT || code == MIR_LDLE || code == MIR_LDGT || code == MIR_LDGE) {\n    *hr1 = AX_HARD_REG;\n  }\n}\n\n// constraint: esp can not be index\n\nstatic int int8_p (int64_t v) { return INT8_MIN <= v && v <= INT8_MAX; }\nstatic int MIR_UNUSED uint8_p (int64_t v) { return 0 <= v && v <= UINT8_MAX; }\nstatic int int16_p (int64_t v) { return INT16_MIN <= v && v <= INT16_MAX; }\nstatic int MIR_UNUSED uint16_p (int64_t v) { return 0 <= v && v <= UINT16_MAX; }\nstatic int int32_p (int64_t v) { return INT32_MIN <= v && v <= INT32_MAX; }\nstatic int MIR_UNUSED uint32_p (int64_t v) { return 0 <= v && v <= UINT32_MAX; }\n\nstatic int dec_value (int ch) { return '0' <= ch && ch <= '9' ? ch - '0' : -1; }\n\nstatic uint64_t read_dec (const char **ptr) {\n  int v;\n  const char *p;\n  uint64_t res = 0;\n\n  for (p = *ptr; (v = dec_value (*p)) >= 0; p++) {\n    gen_assert ((res >> 60) == 0);\n    res = res * 10 + v;\n  }\n  gen_assert (p != *ptr);\n  *ptr = p - 1;\n  return res;\n}\n\nstatic int pattern_index_cmp (const void *a1, const void *a2) {\n  int i1 = *(const int *) a1, i2 = *(const int *) a2;\n  int c1 = (int) patterns[i1].code, c2 = (int) patterns[i2].code;\n\n  return c1 != c2 ? c1 - c2 : (long) i1 - (long) i2;\n}\n\nstatic int get_max_insn_size (gen_ctx_t gen_ctx, const char *replacement);\n\nstatic void patterns_init (gen_ctx_t gen_ctx) {\n  MIR_alloc_t alloc = gen_alloc (gen_ctx);\n  int i, ind, n = sizeof (patterns) / sizeof (struct pattern);\n  MIR_insn_code_t prev_code, code;\n  insn_pattern_info_t *info_addr;\n  insn_pattern_info_t pinfo = {0, 0};\n\n  VARR_CREATE (int, pattern_indexes, alloc, 0);\n  for (i = 0; i < n; i++) {\n    patterns[i].max_insn_size = get_max_insn_size (gen_ctx, patterns[i].replacement);\n#if 0\n    fprintf (stderr, \"size of \\\"%s\\\" = %d\\n\", patterns[i].replacement,\n\t     patterns[i].max_insn_size);\n#endif\n    VARR_PUSH (int, pattern_indexes, i);\n  }\n  qsort (VARR_ADDR (int, pattern_indexes), n, sizeof (int), pattern_index_cmp);\n  VARR_CREATE (insn_pattern_info_t, insn_pattern_info, alloc, 0);\n  for (i = 0; i < MIR_INSN_BOUND; i++) VARR_PUSH (insn_pattern_info_t, insn_pattern_info, pinfo);\n  info_addr = VARR_ADDR (insn_pattern_info_t, insn_pattern_info);\n  for (prev_code = MIR_INSN_BOUND, i = 0; i < n; i++) {\n    ind = VARR_GET (int, pattern_indexes, i);\n    if ((code = patterns[ind].code) != prev_code) {\n      if (i != 0) info_addr[prev_code].num = i - info_addr[prev_code].start;\n      info_addr[code].start = i;\n      prev_code = code;\n    }\n  }\n  assert (prev_code != MIR_INSN_BOUND);\n  info_addr[prev_code].num = n - info_addr[prev_code].start;\n}\n\nstatic int64_t int_value (gen_ctx_t gen_ctx, const MIR_op_t *op) {\n  gen_assert (op->mode == MIR_OP_REF || op->mode == MIR_OP_INT || op->mode == MIR_OP_UINT);\n  return (op->mode != MIR_OP_REF ? op->u.i : (int64_t) get_ref_value (gen_ctx, op));\n}\n\nstatic int pattern_match_p (gen_ctx_t gen_ctx, const struct pattern *pat, MIR_insn_t insn,\n                            int try_short_jump_p) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  size_t nop, nops = MIR_insn_nops (ctx, insn);\n  const char *p;\n  char ch, start_ch;\n  MIR_op_mode_t mode;\n  MIR_op_t original;\n  const MIR_op_t *op_ref;\n  MIR_reg_t hr;\n\n  for (nop = 0, p = pat->pattern; *p != 0; p++, nop++) {\n    while (*p == ' ' || *p == '\\t') p++;\n    if (*p == '$') return TRUE;\n    if (MIR_call_code_p (insn->code) && nop >= nops) return FALSE;\n    gen_assert (nop < nops);\n    op_ref = &insn->ops[nop];\n    switch (start_ch = *p) {\n    case 'X': break;\n    case 'r':\n      if (op_ref->mode != MIR_OP_VAR) return FALSE;\n      break;\n    case 't':\n      if (op_ref->mode != MIR_OP_VAR\n          || !(AX_HARD_REG == op_ref->u.var || op_ref->u.var == CX_HARD_REG\n               || op_ref->u.var == DX_HARD_REG || op_ref->u.var == BX_HARD_REG))\n        return FALSE;\n      break;\n    case 'h':\n      if (op_ref->mode != MIR_OP_VAR) return FALSE;\n      ch = *++p;\n      gen_assert ('0' <= ch && ch <= '9');\n      hr = ch - '0';\n      ch = *++p;\n      if ('0' <= ch && ch <= '9')\n        hr = hr * 10 + ch - '0';\n      else\n        --p;\n      if (op_ref->u.var != hr) return FALSE;\n      break;\n    case 'z':\n      if ((op_ref->mode != MIR_OP_INT && op_ref->mode != MIR_OP_UINT) || op_ref->u.i != 0)\n        return FALSE;\n      break;\n    case 'i': {\n      if (op_ref->mode != MIR_OP_INT && op_ref->mode != MIR_OP_UINT && op_ref->mode != MIR_OP_REF)\n        return FALSE;\n      ch = *++p;\n      gen_assert ('0' <= ch && ch <= '3');\n      int64_t n = int_value (gen_ctx, op_ref);\n      if ((ch == '0' && !int8_p (n)) || (ch == '1' && !int16_p (n)) || (ch == '2' && !int32_p (n)))\n        return FALSE;\n      break;\n    }\n    case 's':\n      if ((op_ref->mode != MIR_OP_INT && op_ref->mode != MIR_OP_UINT)\n          || (op_ref->u.i != 1 && op_ref->u.i != 2 && op_ref->u.i != 4 && op_ref->u.i != 8))\n        return FALSE;\n      break;\n    case 'c': {\n      uint64_t dec_val;\n      p++;\n      dec_val = read_dec (&p);\n      if ((op_ref->mode != MIR_OP_INT && op_ref->mode != MIR_OP_UINT) || op_ref->u.u != dec_val)\n        return FALSE;\n      break;\n    }\n    case 'm': {\n      MIR_type_t type, type2, type3 = MIR_T_BOUND;\n      int u_p, s_p;\n\n      u_p = s_p = TRUE;\n      ch = *++p;\n      switch (ch) {\n      case 'f':\n        type = MIR_T_F;\n        type2 = MIR_T_BOUND;\n        break;\n      case 'd':\n        type = MIR_T_D;\n        type2 = MIR_T_BOUND;\n        break;\n      case 'l':\n        ch = *++p;\n        gen_assert (ch == 'd');\n        type = MIR_T_LD;\n        type2 = MIR_T_BOUND;\n        break;\n      case 'u':\n      case 's':\n        u_p = ch == 'u';\n        s_p = ch == 's';\n        ch = *++p;\n        /* falls through */\n      default:\n        gen_assert ('0' <= ch && ch <= '3');\n        if (ch == '0') {\n          type = u_p ? MIR_T_U8 : MIR_T_I8;\n          type2 = u_p && s_p ? MIR_T_I8 : MIR_T_BOUND;\n        } else if (ch == '1') {\n          type = u_p ? MIR_T_U16 : MIR_T_I16;\n          type2 = u_p && s_p ? MIR_T_I16 : MIR_T_BOUND;\n        } else if (ch == '2') {\n          type = u_p ? MIR_T_U32 : MIR_T_I32;\n          type2 = u_p && s_p ? MIR_T_I32 : MIR_T_BOUND;\n#if MIR_PTR32\n          if (u_p) type3 = MIR_T_P;\n#endif\n        } else {\n          type = u_p ? MIR_T_U64 : MIR_T_I64;\n          type2 = u_p && s_p ? MIR_T_I64 : MIR_T_BOUND;\n#if MIR_PTR64\n          type3 = MIR_T_P;\n#endif\n        }\n      }\n      /* LD pseudos always get memory: */\n      if (type == MIR_T_LD && op_ref->mode == MIR_OP_VAR && op_ref->u.var > MAX_HARD_REG) break;\n      if (op_ref->mode != MIR_OP_VAR_MEM) return FALSE;\n      if (op_ref->u.var_mem.type != type && op_ref->u.var_mem.type != type2\n          && op_ref->u.var_mem.type != type3)\n        return FALSE;\n      if (op_ref->u.var_mem.index != MIR_NON_VAR && op_ref->u.var_mem.scale != 1\n          && op_ref->u.var_mem.scale != 2 && op_ref->u.var_mem.scale != 4\n          && op_ref->u.var_mem.scale != 8)\n        return FALSE;\n      if (!int32_p (op_ref->u.var_mem.disp)) return FALSE;\n      break;\n    }\n    case 'L': break;\n    case 'l':\n      if (op_ref->mode != MIR_OP_LABEL) return FALSE;\n      if (!try_short_jump_p) return FALSE; /* we are in size estimation mode */\n      int64_t disp = ((int64_t) get_label_disp (gen_ctx, op_ref->u.label)\n                      - (int64_t) VARR_LENGTH (uint8_t, result_code));\n      /* short->long (+1 for long jump prefix +3 for offset), minimal jump is 2 bytes: */\n      disp = disp < 0 ? disp - (pat->max_insn_size + 4) : disp - 2;\n      if (-128 <= disp && disp < 128) break;\n      return FALSE;\n    case '0':\n    case '1':\n    case '2':\n    case '3':\n    case '4':\n    case '5':\n    case '6':\n    case '7':\n    case '8':\n    case '9': {\n      size_t n = start_ch - '0';\n      gen_assert (n < nop);\n      original = insn->ops[n];\n      mode = op_ref->mode;\n      if (mode == MIR_OP_UINT) mode = MIR_OP_INT;\n      if (original.mode != mode && (original.mode != MIR_OP_UINT || mode != MIR_OP_INT))\n        return FALSE;\n      gen_assert (mode == MIR_OP_VAR || mode == MIR_OP_INT || mode == MIR_OP_FLOAT\n                  || mode == MIR_OP_DOUBLE || mode == MIR_OP_LDOUBLE || mode == MIR_OP_VAR_MEM\n                  || mode == MIR_OP_LABEL);\n      if (mode == MIR_OP_VAR && op_ref->u.var != original.u.var)\n        return FALSE;\n      else if (mode == MIR_OP_INT && op_ref->u.i != original.u.i)\n        return FALSE;\n      else if (mode == MIR_OP_FLOAT && op_ref->u.f != original.u.f)\n        return FALSE;\n      else if (mode == MIR_OP_DOUBLE && op_ref->u.d != original.u.d)\n        return FALSE;\n      else if (mode == MIR_OP_LDOUBLE && op_ref->u.ld != original.u.ld)\n        return FALSE;\n      else if (mode == MIR_OP_LABEL && op_ref->u.label != original.u.label)\n        return FALSE;\n      else if (mode == MIR_OP_VAR_MEM\n               && (op_ref->u.var_mem.type != original.u.var_mem.type\n                   || op_ref->u.var_mem.disp != original.u.var_mem.disp\n                   || op_ref->u.var_mem.base != original.u.var_mem.base\n                   || op_ref->u.var_mem.index != original.u.var_mem.index\n                   || (op_ref->u.var_mem.index != MIR_NON_VAR\n                       && op_ref->u.var_mem.scale != original.u.var_mem.scale)))\n        return FALSE;\n      break;\n    }\n    default: gen_assert (FALSE);\n    }\n  }\n  gen_assert (nop == nops);\n  return TRUE;\n}\n\nstatic int find_insn_pattern (gen_ctx_t gen_ctx, MIR_insn_t insn, int *size) {\n  int i, ind;\n  const struct pattern *pat;\n  insn_pattern_info_t info = VARR_GET (insn_pattern_info_t, insn_pattern_info, insn->code);\n\n  for (i = 0; i < info.num; i++) {\n    ind = VARR_GET (int, pattern_indexes, info.start + i);\n    pat = &patterns[ind];\n    if (pattern_match_p (gen_ctx, pat, insn, size == NULL)) {\n      if (size != NULL) *size = patterns[ind].max_insn_size;\n      return ind;\n    }\n  }\n  return -1;\n}\n\nstatic void patterns_finish (gen_ctx_t gen_ctx) {\n  VARR_DESTROY (int, pattern_indexes);\n  VARR_DESTROY (insn_pattern_info_t, insn_pattern_info);\n}\n\nstatic int hex_value (int ch) {\n  return '0' <= ch && ch <= '9' ? ch - '0' : 'A' <= ch && ch <= 'F' ? ch - 'A' + 10 : -1;\n}\n\nstatic uint64_t read_hex (const char **ptr) {\n  int v;\n  const char *p;\n  uint64_t res = 0;\n\n  for (p = *ptr; (v = hex_value (*p)) >= 0; p++) {\n    gen_assert ((res >> 60) == 0);\n    res = res * 16 + v;\n  }\n  gen_assert (p != *ptr);\n  *ptr = p - 1;\n  return res;\n}\n\nstatic void setup_r (int *rex, int *r, int v) {\n  gen_assert ((rex == NULL || *rex < 0) && *r < 0 && v >= 0 && v <= MAX_HARD_REG);\n  if (v >= 16) v -= 16;\n  if (v >= 8) {\n    if (rex != NULL) *rex = 1;\n    v -= 8;\n  }\n  *r = v;\n}\n\nstatic void setup_rm_byte (int *rex, int *high, int *r, int v) {\n  gen_assert ((rex == NULL || *rex < 0) && *r < 0 && v >= 0 && v <= MAX_HARD_REG);\n  if (v >= 16) v -= 16;\n  if (v >= 4) {\n    if (rex != NULL) *rex = 1;\n  }\n  if (v >= 8) {\n    if (high != NULL) *high = 1;\n    v -= 8;\n  }\n  *r = v;\n}\n\nstatic void setup_reg (int *rex_reg, int *reg, int v) { setup_r (rex_reg, reg, v); }\n\nstatic void setup_rm (int *rex_b, int *rm, int v) { setup_r (rex_b, rm, v); }\n\nstatic void setup_mod (int *mod, int v) {\n  gen_assert (*mod < 0 && v >= 0 && v <= 3);\n  *mod = v;\n}\n\nstatic void setup_scale (int *scale, int v) {\n  gen_assert (*scale < 0 && v >= 0 && v <= 3);\n  *scale = v;\n}\n\nstatic void setup_base (int *rex_b, int *base, int v) { setup_r (rex_b, base, v); }\n\nstatic void setup_index (int *rex_i, int *index, int v) { setup_r (rex_i, index, v); }\n\nstatic void setup_rip_rel_addr (MIR_disp_t rip_disp, int *mod, int *rm, int64_t *disp32) {\n  gen_assert (*mod < 0 && *rm < 0 && *disp32 < 0);\n  setup_rm (NULL, rm, 5);\n  gen_assert (int32_p (rip_disp));\n  setup_mod (mod, 0);\n  *disp32 = (uint32_t) rip_disp;\n}\n\nstatic void setup_mem (MIR_mem_t mem, int *mod, int *rm, int *scale, int *base, int *rex_b,\n                       int *index, int *rex_x, int *disp8, int64_t *disp32) {\n  MIR_disp_t disp = mem.disp;\n\n  gen_assert (*disp8 < 0 && *disp32 < 0 && mem.index != SP_HARD_REG);\n  if (mem.index == MIR_NON_VAR && mem.base == MIR_NON_VAR) { /* SIB: disp only */\n    setup_rm (NULL, rm, 4);\n    *disp32 = (uint32_t) disp;\n    setup_base (NULL, base, BP_HARD_REG);\n    setup_index (NULL, index, SP_HARD_REG);\n  } else if (mem.index == MIR_NON_VAR && mem.base != SP_HARD_REG && mem.base != R12_HARD_REG) {\n    setup_rm (rex_b, rm, mem.base);\n    if (disp == 0 && mem.base != BP_HARD_REG && mem.base != R13_HARD_REG) {\n      setup_mod (mod, 0);\n    } else if (int8_p (disp)) {\n      setup_mod (mod, 1);\n      *disp8 = (uint8_t) disp;\n    } else {\n      setup_mod (mod, 2);\n      *disp32 = (uint32_t) disp;\n    }\n  } else if (mem.index == MIR_NON_VAR) { /* SIB: only base = sp or r12 */\n    setup_rm (NULL, rm, 4);\n    setup_index (NULL, index, SP_HARD_REG);\n    setup_base (rex_b, base, mem.base);\n    if (disp == 0) {\n      setup_mod (mod, 0);\n    } else if (int8_p (disp)) {\n      setup_mod (mod, 1);\n      *disp8 = (uint8_t) disp;\n    } else {\n      setup_mod (mod, 2);\n      *disp32 = (uint32_t) disp;\n    }\n  } else if (mem.base == MIR_NON_VAR) { /* SIB: index with scale only */\n    setup_rm (NULL, rm, 4);\n    setup_index (rex_x, index, mem.index);\n    setup_base (NULL, base, BP_HARD_REG);\n    setup_mod (mod, 0);\n    *disp32 = (uint32_t) disp;\n    setup_scale (scale, mem.scale == 1 ? 0 : mem.scale == 2 ? 1 : mem.scale == 4 ? 2 : 3);\n  } else { /* SIB: base and index */\n    setup_rm (NULL, rm, 4);\n    setup_base (rex_b, base, mem.base);\n    setup_index (rex_x, index, mem.index);\n    setup_scale (scale, mem.scale == 1 ? 0 : mem.scale == 2 ? 1 : mem.scale == 4 ? 2 : 3);\n    if (disp == 0 && mem.base != BP_HARD_REG && mem.base != R13_HARD_REG) {\n      setup_mod (mod, 0);\n    } else if (int8_p (disp)) {\n      setup_mod (mod, 1);\n      *disp8 = (uint8_t) disp;\n    } else {\n      setup_mod (mod, 2);\n      *disp32 = (uint32_t) disp;\n    }\n  }\n}\n\nstatic void put_byte (struct gen_ctx *gen_ctx, int byte) { VARR_PUSH (uint8_t, result_code, byte); }\n\nstatic void put_uint64 (struct gen_ctx *gen_ctx, uint64_t v, int nb) {\n  for (; nb > 0; nb--) {\n    put_byte (gen_ctx, v & 0xff);\n    v >>= 8;\n  }\n}\n\nstatic void set_int64 (uint8_t *addr, int64_t v, int nb) {\n  for (; nb > 0; nb--) {\n    *addr++ = v & 0xff;\n    v >>= 8;\n  }\n}\n\nstatic int64_t get_int64 (uint8_t *addr, int nb) {\n  int64_t v = 0;\n  int i, sh = (8 - nb) * 8;\n\n  for (i = nb - 1; i >= 0; i--) v = (v << 8) | addr[i];\n  if (sh > 0) v = (v << sh) >> sh; /* make it signed */\n  return v;\n}\n\nstatic size_t add_to_const_pool (struct gen_ctx *gen_ctx, uint64_t v) {\n  uint64_t *addr = VARR_ADDR (uint64_t, const_pool);\n  size_t n, len = VARR_LENGTH (uint64_t, const_pool);\n\n  for (n = 0; n < len; n++)\n    if (addr[n] == v) return n;\n  VARR_PUSH (uint64_t, const_pool, v);\n  return len;\n}\n\nstatic int setup_imm_addr (struct gen_ctx *gen_ctx, uint64_t v, int *mod, int *rm, int64_t *disp32,\n                           int call_p, MIR_item_t func_item) {\n  const_ref_t cr;\n  size_t n;\n\n  n = add_to_const_pool (gen_ctx, v);\n  setup_rip_rel_addr (0, mod, rm, disp32);\n  cr.call_p = call_p;\n  cr.func_item = func_item;\n  cr.pc = 0;\n  cr.next_insn_disp = 0;\n  cr.const_num = n;\n  VARR_PUSH (const_ref_t, const_refs, cr);\n  return (int) VARR_LENGTH (const_ref_t, const_refs) - 1;\n}\n\nstatic int get_max_insn_size (gen_ctx_t gen_ctx MIR_UNUSED, const char *replacement) {\n  const char *p, *insn_str;\n  int size = 0;\n\n  for (insn_str = replacement;; insn_str = p + 1) {\n    char ch, start_ch;\n    int opcode0_p = FALSE, opcode1_p = FALSE, opcode2_p = FALSE;\n    int rex_p = FALSE, modrm_p = FALSE, addr_p = FALSE, prefix_p = FALSE;\n    int disp8_p = FALSE, imm8_p = FALSE, disp32_p = FALSE, imm32_p = FALSE, imm64_p = FALSE;\n    int switch_table_addr_p = FALSE;\n\n    for (p = insn_str; (ch = *p) != '\\0' && ch != ';'; p++) {\n      if (hex_value (ch = *p) >= 0) {\n        hex_value (ch = *++p);\n        if (!opcode0_p)\n          opcode0_p = TRUE;\n        else if (!opcode1_p)\n          opcode1_p = TRUE;\n        else {\n          gen_assert (!opcode2_p);\n          opcode2_p = TRUE;\n        }\n        p++;\n      }\n      if ((ch = *p) == 0 || ch == ';') break;\n      switch ((start_ch = ch = *p)) {\n      case ' ':\n      case '\\t': break;\n      case 'X':\n      case 'Y':\n      case 'Z':\n        if (opcode0_p) {\n          gen_assert (!opcode1_p);\n          prefix_p = opcode0_p;\n          opcode0_p = FALSE;\n        }\n        rex_p = TRUE;\n        break;\n      case 'r':\n      case 'R':\n      case 'S':\n        ch = *++p;\n        gen_assert ('0' <= ch && ch <= '2');\n        modrm_p = TRUE;\n        break;\n      case 'm':\n        ch = *++p;\n        modrm_p = TRUE;\n        addr_p = TRUE;\n        if (ch == 't') { /* -16(%rsp) */\n          disp8_p = TRUE;\n        } else if (ch == 'T') {\n          disp8_p = TRUE;\n        } else {\n          gen_assert ('0' <= ch && ch <= '2');\n          disp32_p = TRUE;\n        }\n        break;\n      case 'a':\n        ch = *++p;\n        addr_p = TRUE;\n        if (ch == 'p') {\n          disp32_p = TRUE;\n        } else if (ch == 'd') {\n          ++p;\n          uint64_t disp = read_hex (&p);\n          if (int8_p (disp))\n            disp8_p = TRUE;\n          else\n            disp32_p = TRUE;\n        } else {\n          gen_assert (ch == 'm');\n        }\n        break;\n      case 'i':\n      case 'I':\n      case 'J':\n        ch = *++p;\n        gen_assert ('0' <= ch && ch <= '7');\n        if (start_ch == 'i') {\n          imm8_p = TRUE;\n        } else if (start_ch == 'I') {\n          imm32_p = TRUE;\n        } else {\n          imm64_p = TRUE;\n        }\n        break;\n      case 'T': switch_table_addr_p = modrm_p = TRUE; break;\n      case 'q': modrm_p = TRUE; break;\n      case 'l': disp8_p = TRUE; goto label_rest;\n      case 'L':\n        disp32_p = TRUE;\n      label_rest:\n        ch = *++p;\n        gen_assert ('0' <= ch && ch <= '2');\n        break;\n      case 'P':\n        ch = *++p;\n        gen_assert ('0' <= ch && ch <= '7');\n        modrm_p = TRUE;\n        disp32_p = TRUE;\n        break;\n      case '/':\n        ch = *++p;\n        gen_assert ('0' <= ch && ch <= '7');\n        modrm_p = TRUE;\n        break;\n      case '+':\n        ch = *++p;\n        if (ch == 'h') {\n          ch = *++p;\n        } else {\n          gen_assert ('0' <= ch && ch <= '2');\n        }\n        opcode0_p = TRUE;\n        break;\n      case 'c':  // ???\n        ++p;\n        read_hex (&p);\n        gen_assert (!disp32_p);\n        disp32_p = TRUE;\n        break;\n      case 'h':\n        ++p;\n        read_hex (&p);\n        modrm_p = TRUE;\n        break;\n      case 'H':\n        ++p;\n        read_hex (&p);\n        modrm_p = TRUE;\n        break;\n      case 'v':\n      case 'V':\n        ++p;\n        read_hex (&p);\n        if (start_ch == 'v') {\n          imm8_p = TRUE;\n        } else {\n          imm32_p = TRUE;\n        }\n        break;\n      default: gen_assert (FALSE);\n      }\n    }\n    if (prefix_p) size++;\n    if (rex_p) size++;\n    if (opcode0_p) size++;\n    if (opcode1_p) size++;\n    if (opcode2_p) size++;\n    if (modrm_p) size++;\n    if (addr_p) size++;\n    if (disp8_p) size++;\n    if (disp32_p) size += 4;\n    if (imm8_p) size++;\n    if (imm32_p) size += 4;\n    if (imm64_p) size += 8;\n    if (switch_table_addr_p) size += 4;\n    if (ch == '\\0') break;\n  }\n  return size;\n}\n\nstatic void out_insn (gen_ctx_t gen_ctx, MIR_insn_t insn, const char *replacement,\n                      void **jump_addrs) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  const char *p, *insn_str;\n  label_ref_t lr;\n  int switch_table_addr_start_offset = -1;\n\n  if (insn->code == MIR_ALLOCA\n      && (insn->ops[1].mode == MIR_OP_INT || insn->ops[1].mode == MIR_OP_UINT))\n    insn->ops[1].u.u = (insn->ops[1].u.u + 15) & -16;\n  for (insn_str = replacement;; insn_str = p + 1) {\n    char ch, start_ch;\n    int d1, d2;\n    int opcode0 = -1, opcode1 = -1, opcode2 = -1;\n    int rex_w = -1, rex_r = -1, rex_x = -1, rex_b = -1, rex_0 = -1;\n    int mod = -1, reg = -1, rm = -1;\n    int scale = -1, index = -1, base = -1;\n    int prefix = -1, disp8 = -1, imm8 = -1, lb = -1;\n    int64_t disp32 = -1, imm32 = -1;\n    int imm64_p = FALSE;\n    uint64_t imm64 = 0, v;\n    const MIR_op_t *op_ref;\n    int const_ref_num = -1, label_ref_num = -1, switch_table_addr_p = FALSE;\n\n    for (p = insn_str; (ch = *p) != '\\0' && ch != ';'; p++) {\n      if ((d1 = hex_value (ch = *p)) >= 0) {\n        d2 = hex_value (ch = *++p);\n        gen_assert (d2 >= 0);\n        if (opcode0 == -1)\n          opcode0 = d1 * 16 + d2;\n        else if (opcode1 == -1)\n          opcode1 = d1 * 16 + d2;\n        else {\n          gen_assert (opcode2 == -1);\n          opcode2 = d1 * 16 + d2;\n        }\n        p++;\n      }\n      if ((ch = *p) == 0 || ch == ';') break;\n      switch ((start_ch = ch = *p)) {\n      case ' ':\n      case '\\t': break;\n      case 'X':\n        if (opcode0 >= 0) {\n          gen_assert (opcode1 < 0);\n          prefix = opcode0;\n          opcode0 = -1;\n        }\n        rex_w = 1;\n        break;\n      case 'Y':\n        if (opcode0 >= 0) {\n          gen_assert (opcode1 < 0);\n          prefix = opcode0;\n          opcode0 = -1;\n        }\n        rex_w = 0;\n        break;\n      case 'Z':\n        if (opcode0 >= 0) {\n          gen_assert (opcode1 < 0);\n          prefix = opcode0;\n          opcode0 = -1;\n        }\n        rex_w = 0;\n        rex_0 = 0;\n        break;\n      case 'r':\n      case 'R':\n      case 'S':\n        ch = *++p;\n        gen_assert ('0' <= ch && ch <= '2');\n        op_ref = &insn->ops[ch - '0'];\n        gen_assert (op_ref->mode == MIR_OP_VAR);\n        if (start_ch == 'r')\n          setup_reg (&rex_r, &reg, op_ref->u.var);\n        else if (start_ch == 'R') {\n          setup_rm (&rex_b, &rm, op_ref->u.var);\n          setup_mod (&mod, 3);\n        } else if (start_ch == 'S') {\n          setup_rm_byte (&rex_0, &rex_b, &rm, op_ref->u.var);\n          setup_mod (&mod, 3);\n        }\n        break;\n      case 'm':\n        ch = *++p;\n        if (ch == 't') { /* -16(%rsp) */\n          setup_rm (NULL, &rm, 4);\n          setup_index (NULL, &index, SP_HARD_REG);\n          setup_base (&rex_b, &base, SP_HARD_REG);\n          setup_mod (&mod, 1);\n          disp8 = (uint8_t) -16;\n        } else if (ch == 'T') {\n          MIR_op_t mem;\n\n          op_ref = &insn->ops[0];\n          gen_assert (op_ref->mode == MIR_OP_VAR);\n          mem = _MIR_new_var_mem_op (ctx, MIR_T_I64, 0, R11_HARD_REG, op_ref->u.var, 8);\n          setup_mem (mem.u.var_mem, &mod, &rm, &scale, &base, &rex_b, &index, &rex_x, &disp8,\n                     &disp32);\n        } else {\n          gen_assert ('0' <= ch && ch <= '2');\n          op_ref = &insn->ops[ch - '0'];\n          gen_assert (op_ref->mode == MIR_OP_VAR_MEM);\n          setup_mem (op_ref->u.var_mem, &mod, &rm, &scale, &base, &rex_b, &index, &rex_x, &disp8,\n                     &disp32);\n        }\n        break;\n      case 'a': {\n        MIR_mem_t mem;\n        MIR_op_t op2;\n\n        ch = *++p;\n        op_ref = &insn->ops[1];\n        gen_assert (op_ref->mode == MIR_OP_VAR);\n        mem.type = MIR_T_I8;\n        if (ch == 'p') {\n          op2 = insn->ops[2];\n          mem.base = op_ref->u.var;\n          mem.scale = 1;\n          if (op2.mode == MIR_OP_VAR) {\n            mem.index = op2.u.var;\n            mem.disp = 0;\n          } else {\n            gen_assert (op2.mode == MIR_OP_INT || op2.mode == MIR_OP_UINT\n                        || op2.mode == MIR_OP_REF);\n            mem.index = MIR_NON_VAR;\n            mem.disp = int_value (gen_ctx, &op2);\n          }\n        } else if (ch == 'd') {\n          mem.base = op_ref->u.var;\n          mem.index = MIR_NON_VAR;\n          mem.scale = 1;\n          ++p;\n          mem.disp = read_hex (&p);\n        } else {\n          gen_assert (ch == 'm');\n          op2 = insn->ops[2];\n          mem.index = op_ref->u.var;\n          mem.base = MIR_NON_VAR;\n          mem.disp = 0;\n          gen_assert ((op2.mode == MIR_OP_INT || op2.mode == MIR_OP_UINT)\n                      && (op2.u.i == 1 || op2.u.i == 2 || op2.u.i == 4 || op2.u.i == 8));\n          mem.scale = (MIR_scale_t) op2.u.i;\n        }\n        setup_mem (mem, &mod, &rm, &scale, &base, &rex_b, &index, &rex_x, &disp8, &disp32);\n        break;\n      }\n      case 'i':\n      case 'I':\n      case 'J':\n        ch = *++p;\n        gen_assert ('0' <= ch && ch <= '7');\n        op_ref = &insn->ops[ch - '0'];\n        gen_assert (op_ref->mode == MIR_OP_INT || op_ref->mode == MIR_OP_UINT\n                    || op_ref->mode == MIR_OP_REF);\n        int64_t n = int_value (gen_ctx, op_ref);\n        if (start_ch == 'i') {\n          gen_assert (int8_p (n));\n          imm8 = (uint8_t) n;\n        } else if (start_ch == 'I') {\n          gen_assert (int32_p (n));\n          imm32 = (uint32_t) n;\n        } else {\n          imm64_p = TRUE;\n          imm64 = (uint64_t) n;\n        }\n        break;\n      case 'T': {\n        gen_assert (!switch_table_addr_p && switch_table_addr_start_offset < 0);\n        switch_table_addr_p = TRUE;\n        mod = 0;\n        rm = 5;\n        break;\n      }\n      case 'q':\n        mod = 0;\n        rm = 5;\n        break;\n      case 'l':\n        gen_assert (disp32 < 0 && disp8 < 0);\n        lr.short_p = TRUE;\n        disp8 = 0;\n        goto label_rest;\n      case 'L':\n        gen_assert (disp32 < 0 && disp8 < 0);\n        lr.short_p = FALSE;\n        disp32 = 0; /* To reserve the space */\n      label_rest:\n        ch = *++p;\n        gen_assert ('0' <= ch && ch <= '2');\n        op_ref = &insn->ops[ch - '0'];\n        gen_assert (op_ref->mode == MIR_OP_LABEL);\n        lr.abs_addr_p = FALSE;\n        lr.label_val_disp = lr.next_insn_disp = 0;\n        if (jump_addrs == NULL)\n          lr.u.label = op_ref->u.label;\n        else\n          lr.u.jump_addr = jump_addrs[0];\n        gen_assert (label_ref_num < 0);\n        label_ref_num = (int) VARR_LENGTH (label_ref_t, label_refs);\n        VARR_PUSH (label_ref_t, label_refs, lr);\n        break;\n      case 'P':\n        ch = *++p;\n        gen_assert ('0' <= ch && ch <= '7');\n        op_ref = &insn->ops[ch - '0'];\n        gen_assert (op_ref->mode == MIR_OP_INT || op_ref->mode == MIR_OP_UINT\n                    || op_ref->mode == MIR_OP_REF);\n        v = (uint64_t) int_value (gen_ctx, op_ref);\n        gen_assert (const_ref_num < 0 && disp32 < 0);\n        MIR_item_t func_item\n          = op_ref->mode != MIR_OP_REF || op_ref->u.ref->item_type != MIR_func_item ? NULL\n                                                                                    : op_ref->u.ref;\n        const_ref_num = setup_imm_addr (gen_ctx, v, &mod, &rm, &disp32, TRUE, func_item);\n        break;\n      case '/':\n        ch = *++p;\n        gen_assert ('0' <= ch && ch <= '7');\n        setup_reg (NULL, &reg, ch - '0');\n        break;\n      case '+': {\n        int hreg;\n        ch = *++p;\n        if (ch == 'h') {\n          ch = *++p;\n          hreg = hex_value (*p++);\n          gen_assert (hreg >= 0);\n        } else {\n          gen_assert ('0' <= ch && ch <= '2');\n          op_ref = &insn->ops[ch - '0'];\n          gen_assert (op_ref->mode == MIR_OP_VAR);\n          hreg = op_ref->u.var;\n        }\n        setup_reg (&rex_b, &lb, hreg);\n        break;\n      }\n      case 'c':\n        ++p;\n        v = read_hex (&p);\n        gen_assert (const_ref_num < 0 && disp32 < 0);\n        const_ref_num = setup_imm_addr (gen_ctx, v, &mod, &rm, &disp32, FALSE, NULL);\n        break;\n      case 'h':\n        ++p;\n        v = read_hex (&p);\n        gen_assert (v <= 31);\n        setup_reg (&rex_r, &reg, (int) v);\n        break;\n      case 'H':\n        ++p;\n        v = read_hex (&p);\n        gen_assert (v <= 31);\n        setup_rm (&rex_b, &rm, (int) v);\n        setup_mod (&mod, 3);\n        break;\n      case 'v':\n      case 'V':\n        ++p;\n        v = read_hex (&p);\n        if (start_ch == 'v') {\n          gen_assert (uint8_p (v));\n          imm8 = (int) v;\n        } else {\n          gen_assert (uint32_p (v));\n          imm32 = v;\n        }\n        break;\n      default: gen_assert (FALSE);\n      }\n    }\n    if (prefix >= 0) put_byte (gen_ctx, prefix);\n\n    if (rex_w > 0 || rex_r >= 0 || rex_x >= 0 || rex_b >= 0 || rex_0 >= 0) {\n      if (rex_w < 0) rex_w = 0;\n      if (rex_r < 0) rex_r = 0;\n      if (rex_x < 0) rex_x = 0;\n      if (rex_b < 0) rex_b = 0;\n      gen_assert (rex_w <= 1 && rex_r <= 1 && rex_x <= 1 && rex_b <= 1);\n      put_byte (gen_ctx, 0x40 | (rex_w << 3) | (rex_r << 2) | (rex_x << 1) | rex_b);\n    }\n\n    gen_assert (opcode0 >= 0 && lb <= 7);\n    if (lb >= 0) opcode0 |= lb;\n    put_byte (gen_ctx, opcode0);\n\n    if (opcode1 >= 0) put_byte (gen_ctx, opcode1);\n    if (opcode2 >= 0) put_byte (gen_ctx, opcode2);\n\n    if (mod >= 0 || reg >= 0 || rm >= 0) {\n      if (mod < 0) mod = 0;\n      if (reg < 0) reg = 0;\n      if (rm < 0) rm = 0;\n      gen_assert (mod <= 3 && reg <= 7 && rm <= 7);\n      put_byte (gen_ctx, (mod << 6) | (reg << 3) | rm);\n    }\n    if (scale >= 0 || base >= 0 || index >= 0) {\n      if (scale < 0) scale = 0;\n      if (base < 0) base = 0;\n      if (index < 0) index = 0;\n      gen_assert (scale <= 3 && base <= 7 && index <= 7);\n      put_byte (gen_ctx, (scale << 6) | (index << 3) | base);\n    }\n    if (const_ref_num >= 0)\n      VARR_ADDR (const_ref_t, const_refs)[const_ref_num].pc = VARR_LENGTH (uint8_t, result_code);\n    if (label_ref_num >= 0) VARR_ADDR (label_ref_t, label_refs)\n    [label_ref_num].label_val_disp = VARR_LENGTH (uint8_t, result_code);\n    if (disp8 >= 0) put_byte (gen_ctx, disp8);\n    if (disp32 >= 0) put_uint64 (gen_ctx, disp32, 4);\n    if (imm8 >= 0) put_byte (gen_ctx, imm8);\n    if (imm32 >= 0) put_uint64 (gen_ctx, imm32, 4);\n    if (imm64_p) put_uint64 (gen_ctx, imm64, 8);\n\n    if (switch_table_addr_p) {\n      switch_table_addr_start_offset = (int) VARR_LENGTH (uint8_t, result_code);\n      put_uint64 (gen_ctx, 0, 4);\n    }\n\n    if (label_ref_num >= 0) VARR_ADDR (label_ref_t, label_refs)\n    [label_ref_num].next_insn_disp = VARR_LENGTH (uint8_t, result_code);\n\n    if (const_ref_num >= 0) VARR_ADDR (const_ref_t, const_refs)\n    [const_ref_num].next_insn_disp = VARR_LENGTH (uint8_t, result_code);\n    if (ch == '\\0') break;\n  }\n  if (switch_table_addr_start_offset < 0) return;\n  while (VARR_LENGTH (uint8_t, result_code) % 8 != 0) put_byte (gen_ctx, 0); /* align the table */\n  gen_assert (insn->code == MIR_SWITCH\n              && (int) VARR_LENGTH (uint8_t, result_code) > switch_table_addr_start_offset);\n  set_int64 (&VARR_ADDR (uint8_t, result_code)[switch_table_addr_start_offset],\n             (int64_t) VARR_LENGTH (uint8_t, result_code) - switch_table_addr_start_offset - 4, 4);\n  for (size_t i = 1; i < insn->nops; i++) {\n    gen_assert (insn->ops[i].mode == MIR_OP_LABEL);\n    lr.abs_addr_p = TRUE;\n    lr.label_val_disp = VARR_LENGTH (uint8_t, result_code);\n    if (jump_addrs == NULL)\n      lr.u.label = insn->ops[i].u.label;\n    else\n      lr.u.jump_addr = jump_addrs[i - 1];\n    VARR_PUSH (label_ref_t, label_refs, lr);\n    put_uint64 (gen_ctx, 0, 8);\n  }\n}\n\nstatic uint8_t MIR_UNUSED get_short_jump_opcode (uint8_t *long_jump_opcode) {\n  gen_assert (long_jump_opcode[0] == 0x0F && long_jump_opcode[1] > 0x10);\n  return long_jump_opcode[1] - 0x10;\n}\n\nstatic int target_memory_ok_p (gen_ctx_t gen_ctx MIR_UNUSED, MIR_op_t *op_ref) {\n  if (op_ref->mode != MIR_OP_VAR_MEM) return FALSE;\n  if (op_ref->u.var_mem.index != MIR_NON_VAR && op_ref->u.var_mem.scale != 1\n      && op_ref->u.var_mem.scale != 2 && op_ref->u.var_mem.scale != 4\n      && op_ref->u.var_mem.scale != 8)\n    return FALSE;\n  return int32_p (op_ref->u.var_mem.disp);\n}\n\nstatic int target_insn_ok_p (gen_ctx_t gen_ctx, MIR_insn_t insn) {\n  return find_insn_pattern (gen_ctx, insn, NULL) >= 0;\n}\n\nstatic void translate_init (gen_ctx_t gen_ctx) {\n  VARR_TRUNC (uint8_t, result_code, 0);\n  VARR_TRUNC (uint64_t, const_pool, 0);\n  VARR_TRUNC (const_ref_t, const_refs, 0);\n  VARR_TRUNC (label_ref_t, label_refs, 0);\n  VARR_TRUNC (uint64_t, abs_address_locs, 0);\n}\n\nstatic uint8_t *translate_finish (gen_ctx_t gen_ctx, size_t *len) {\n  /* Setting up labels */\n  for (size_t i = 0; i < VARR_LENGTH (label_ref_t, label_refs); i++) {\n    label_ref_t lr = VARR_GET (label_ref_t, label_refs, i);\n\n    if (lr.abs_addr_p) {\n      set_int64 (&VARR_ADDR (uint8_t, result_code)[lr.label_val_disp],\n                 (int64_t) get_label_disp (gen_ctx, lr.u.label), 8);\n      VARR_PUSH (uint64_t, abs_address_locs, lr.label_val_disp);\n    } else if (lr.short_p) {\n      int64_t disp = (int64_t) get_label_disp (gen_ctx, lr.u.label) - (int64_t) lr.next_insn_disp;\n      gen_assert (-128 <= disp && disp < 128);\n      set_int64 (&VARR_ADDR (uint8_t, result_code)[lr.label_val_disp], disp, 1);\n    } else {\n      set_int64 (&VARR_ADDR (uint8_t, result_code)[lr.label_val_disp],\n                 (int64_t) get_label_disp (gen_ctx, lr.u.label) - (int64_t) lr.next_insn_disp, 4);\n    }\n  }\n  while (VARR_LENGTH (uint8_t, result_code) % 16 != 0) /* Align the pool */\n    VARR_PUSH (uint8_t, result_code, 0);\n  for (size_t i = 0; i < VARR_LENGTH (const_ref_t, const_refs); i++) { /* Add pool constants */\n    const_ref_t cr = VARR_GET (const_ref_t, const_refs, i);\n\n    set_int64 (VARR_ADDR (uint8_t, result_code) + cr.pc,\n               VARR_LENGTH (uint8_t, result_code) - cr.next_insn_disp, 4);\n    put_uint64 (gen_ctx, VARR_GET (uint64_t, const_pool, cr.const_num), 8);\n    put_uint64 (gen_ctx, 0, 8); /* keep 16 bytes align */\n  }\n  *len = VARR_LENGTH (uint8_t, result_code);\n  return VARR_ADDR (uint8_t, result_code);\n}\n\nstatic void target_split_insns (gen_ctx_t gen_ctx MIR_UNUSED) {}\n\n#define LOOP_ALIGN 8\n\nstatic const char *nop_pats[] = {\n  \"\",\n  \"\\x90\" /* 1:nop */,\n  \"\\x66\\x90\", /* 2: xchg ax,ax */\n  \"\\x0f\\x1f\\xc0\" /* 3:nopl eax */,\n  \"\\x48\\x0f\\x1f\\xc0\" /* 4:nop rax */,\n  \"\\x0f\\x1f\\x44\\x00\\x00\" /* 5: nopl 0x0(%rax,%rax,1) */,\n  \"\\x66\\x0f\\x1f\\x44\\x00\\x00\" /* 6: nopw   0x0(%rax,%rax,1) */,\n  \"\\x0f\\x1f\\x80\\x00\\x00\\x00\\x00\" /* 7: nopl 0x0(%rax) */,\n};\n\nstatic uint8_t *target_translate (gen_ctx_t gen_ctx, size_t *len) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  MIR_insn_t insn;\n  int ind, max_insn_size;\n  size_t curr_size, n;\n\n  gen_assert (curr_func_item->item_type == MIR_func_item);\n  translate_init (gen_ctx);\n  curr_size = 0;\n  VARR_TRUNC (int, insn_pattern_indexes, 0);\n  for (insn = DLIST_HEAD (MIR_insn_t, curr_func_item->u.func->insns); insn != NULL;\n       insn = DLIST_NEXT (MIR_insn_t, insn)) {\n    if (insn->code == MIR_LABEL) {\n      if (gen_nested_loop_label_p (gen_ctx, insn)) curr_size += LOOP_ALIGN;\n      set_label_disp (gen_ctx, insn, curr_size); /* estimation */\n    } else if (insn->code != MIR_USE) {\n      ind = find_insn_pattern (gen_ctx, insn, &max_insn_size);\n      if (ind < 0) {\n        fprintf (stderr, \"Fatal failure in matching insn:\");\n        MIR_output_insn (ctx, stderr, insn, curr_func_item->u.func, TRUE);\n        exit (1);\n      }\n      curr_size += max_insn_size;\n      if (insn->code == MIR_SWITCH) curr_size += (insn->nops - 1) * 8; /* label addresses */\n      VARR_PUSH (int, insn_pattern_indexes, ind);\n    }\n  }\n  for (n = 0, insn = DLIST_HEAD (MIR_insn_t, curr_func_item->u.func->insns); insn != NULL;\n       insn = DLIST_NEXT (MIR_insn_t, insn)) {\n    if (insn->code == MIR_LABEL) {\n      if (gen_nested_loop_label_p (gen_ctx, insn)) {\n        int padn = LOOP_ALIGN - (int) (VARR_LENGTH (uint8_t, result_code) % LOOP_ALIGN);\n        if (padn == LOOP_ALIGN) padn = 0;\n        gen_assert ((size_t) padn < sizeof (nop_pats) / sizeof (char *));\n        if (padn != 0) VARR_PUSH_ARR (uint8_t, result_code, (uint8_t *) nop_pats[padn], padn);\n      }\n      set_label_disp (gen_ctx, insn, VARR_LENGTH (uint8_t, result_code));\n    } else if (insn->code != MIR_USE) {\n      ind = VARR_GET (int, insn_pattern_indexes, n++);\n      if (MIR_branch_code_p (insn->code)) /* possible replacement change */\n        ind = find_insn_pattern (gen_ctx, insn, NULL);\n      gen_assert (ind >= 0);\n#ifndef NDEBUG\n      size_t len_before = VARR_LENGTH (uint8_t, result_code);\n#endif\n      out_insn (gen_ctx, insn, patterns[ind].replacement, NULL);\n#ifndef NDEBUG\n      size_t insn_len = VARR_LENGTH (uint8_t, result_code) - len_before;\n      if (insn_len > (size_t) patterns[ind].max_insn_size && insn->code != MIR_SWITCH) {\n        fprintf (stderr, \"\\\"%s\\\" max size(%d) < real size(%d)\\n\", patterns[ind].replacement,\n                 patterns[ind].max_insn_size, (int) insn_len);\n        gen_assert (FALSE);\n      }\n#endif\n    }\n  }\n  return translate_finish (gen_ctx, len);\n}\n\nstatic void store_call_ref (gen_ctx_t gen_ctx, MIR_item_t ref_func_item, uint8_t *call_addr) {\n  call_ref_t call_ref;\n\n  if (MIR_get_func_redef_permission_p (gen_ctx->ctx)) return;\n  call_ref.ref_func_item = ref_func_item;\n  call_ref.call_addr = call_addr;\n  VARR_PUSH (call_ref_t, gen_ctx->target_ctx->call_refs, call_ref);\n}\n\nstatic void change_calls (gen_ctx_t gen_ctx, uint8_t *base) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  /* changing calls to rel32 calls: */\n  for (size_t i = 0; i < VARR_LENGTH (const_ref_t, const_refs); i++) {\n    const_ref_t cr = VARR_GET (const_ref_t, const_refs, i);\n    if (!cr.call_p) continue;\n    gen_assert (base[cr.pc - 2] == 0xff);\n    gen_assert (base[cr.pc - 1] == 0x15 || base[cr.pc - 1] == 0x25);\n    if (cr.func_item != NULL) store_call_ref (gen_ctx, cr.func_item, (uint8_t *) base + cr.pc - 2);\n    uint64_t v = VARR_GET (uint64_t, const_pool, cr.const_num);\n    int64_t off = (int64_t) v - (int64_t) (base + cr.next_insn_disp);\n    if (!int32_p (off)) continue;\n    uint8_t rel_insn[] = {0x40, 0xe8, 0, 0, 0, 0};   /* rex call rel32 */\n    if (base[cr.pc - 1] == 0x25) rel_insn[1] = 0xe9; /* rex jmp rel32 */\n    set_int64 (rel_insn + 2, off, 4);\n    _MIR_change_code (ctx, (uint8_t *) base + cr.pc - 2, (uint8_t *) rel_insn, 6);\n  }\n}\n\nstatic void target_rebase (gen_ctx_t gen_ctx, uint8_t *base) {\n  MIR_code_reloc_t reloc;\n\n  VARR_TRUNC (MIR_code_reloc_t, relocs, 0);\n  for (size_t i = 0; i < VARR_LENGTH (uint64_t, abs_address_locs); i++) {\n    reloc.offset = VARR_GET (uint64_t, abs_address_locs, i);\n    reloc.value = base + get_int64 (base + reloc.offset, 8);\n    VARR_PUSH (MIR_code_reloc_t, relocs, reloc);\n  }\n  _MIR_update_code_arr (gen_ctx->ctx, base, VARR_LENGTH (MIR_code_reloc_t, relocs),\n                        VARR_ADDR (MIR_code_reloc_t, relocs));\n  change_calls (gen_ctx, base);\n  gen_setup_lrefs (gen_ctx, base);\n}\n\nstatic void target_change_to_direct_calls (MIR_context_t ctx) {\n  gen_ctx_t gen_ctx = *gen_ctx_loc (ctx);\n  size_t len = VARR_LENGTH (call_ref_t, gen_ctx->target_ctx->call_refs);\n  if (len == 0) return;\n  call_ref_t *call_refs_addr = VARR_ADDR (call_ref_t, gen_ctx->target_ctx->call_refs);\n  for (size_t i = 0; i < len; i++) {\n    MIR_item_t ref_func_item = call_refs_addr[i].ref_func_item;\n    MIR_func_t ref_func = ref_func_item->u.func;\n    uint8_t *addr_loc, *addr_before, *addr = ref_func->machine_code;\n    uint8_t *call_addr = call_refs_addr[i].call_addr;\n    int32_t off = *(int32_t *) (call_addr + 2);\n    int call32_p = FALSE;\n    if (call_addr[0] == 0xff) { /* call *rel32(rip) */\n      addr_loc = call_addr + 6 + off;\n      addr_before = (uint8_t *) *(uint64_t *) addr_loc;\n      if (addr_before == addr) continue;\n      _MIR_change_code (ctx, addr_loc, (uint8_t *) &addr, sizeof (uint64_t));\n    } else { /* rex call rel32(rip) */\n      gen_assert (call_addr[0] == 0x40);\n      addr_loc = call_addr;\n      addr_before = call_addr + 6 + off;\n      int64_t new_off = addr - (call_addr + 6);\n      if (addr_before == addr || !int32_p (new_off)) {\n        DEBUG (2, {\n          fprintf (stderr,\n                   \"Failing to make direct 32-bit call of func %s at 0x%llx (addr: before=0x%llx, \"\n                   \"after=0x%llx)\\n\",\n                   ref_func->name, (unsigned long long) addr_loc, (unsigned long long) addr_before,\n                   (unsigned long long) addr);\n        });\n        continue;\n      }\n      off = (int32_t) new_off;\n      _MIR_change_code (ctx, addr_loc + 2, (uint8_t *) &off, sizeof (uint32_t));\n      call32_p = TRUE;\n    }\n    DEBUG (2, {\n      fprintf (stderr,\n               \"Making direct %s-bit call of func %s at 0x%llx (addr: before=0x%llx, \"\n               \"after=0x%llx)\\n\",\n               (call32_p ? \"32\" : \"64\"), ref_func->name, (unsigned long long) addr_loc,\n               (unsigned long long) addr_before, (unsigned long long) addr);\n    });\n  }\n  VARR_TRUNC (call_ref_t, gen_ctx->target_ctx->call_refs, 0);\n}\n\nstruct target_bb_version {\n  uint8_t *base;\n  label_ref_t branch_ref; /* label cand used for jump to this bb version */\n};\n\nstatic void target_init_bb_version_data (target_bb_version_t data) {\n  data->base = NULL; /* we don't know origin branch */\n}\n\nstatic void target_bb_translate_start (gen_ctx_t gen_ctx) {\n  VARR_TRUNC (uint8_t, result_code, 0);\n  VARR_TRUNC (uint64_t, const_pool, 0);\n  VARR_TRUNC (const_ref_t, const_refs, 0);\n  VARR_TRUNC (label_ref_t, label_refs, 0);\n  VARR_TRUNC (uint64_t, abs_address_locs, 0);\n}\n\nstatic void target_bb_insn_translate (gen_ctx_t gen_ctx, MIR_insn_t insn, void **jump_addrs) {\n  if (insn->code == MIR_LABEL) return;\n  int ind = find_insn_pattern (gen_ctx, insn, &ind); /* &ind for no short jumps */\n  gen_assert (ind >= 0);\n  out_insn (gen_ctx, insn, patterns[ind].replacement, jump_addrs);\n}\n\nstatic void target_output_jump (gen_ctx_t gen_ctx, void **jump_addrs) {\n  out_insn (gen_ctx, temp_jump, patterns[temp_jump_pat_ind].replacement, jump_addrs);\n}\n\nstatic uint8_t *target_bb_translate_finish (gen_ctx_t gen_ctx, size_t *len) {\n  while (VARR_LENGTH (uint8_t, result_code) % 16 != 0) /* Align the pool */\n    VARR_PUSH (uint8_t, result_code, 0);\n  for (size_t i = 0; i < VARR_LENGTH (const_ref_t, const_refs); i++) { /* Add pool constants */\n    const_ref_t cr = VARR_GET (const_ref_t, const_refs, i);\n\n    set_int64 (VARR_ADDR (uint8_t, result_code) + cr.pc,\n               VARR_LENGTH (uint8_t, result_code) - cr.next_insn_disp, 4);\n    put_uint64 (gen_ctx, VARR_GET (uint64_t, const_pool, cr.const_num), 8);\n    put_uint64 (gen_ctx, 0, 8); /* keep 16 bytes align */\n  }\n  *len = VARR_LENGTH (uint8_t, result_code);\n  return VARR_ADDR (uint8_t, result_code);\n}\n\nstatic void setup_rel32 (gen_ctx_t gen_ctx, label_ref_t *lr, uint8_t *base, void *addr) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  int64_t offset = (int64_t) addr - (int64_t) (base + lr->next_insn_disp);\n  int32_t rel32 = (int32_t) offset;\n\n  if (lr->abs_addr_p || !(INT32_MIN <= offset && offset <= INT32_MAX)) {\n    fprintf (stderr, \"too big offset (%lld) in setup_rel32\", (long long) offset);\n    exit (1);\n  }\n  _MIR_change_code (ctx, (uint8_t *) base + lr->label_val_disp, (uint8_t *) &rel32, 4);\n}\n\nstatic void target_bb_rebase (gen_ctx_t gen_ctx, uint8_t *base) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  MIR_code_reloc_t reloc;\n\n  /* Setting up relative labels */\n  for (size_t i = 0; i < VARR_LENGTH (label_ref_t, label_refs); i++) {\n    label_ref_t lr = VARR_GET (label_ref_t, label_refs, i);\n    if (lr.abs_addr_p) {\n      _MIR_change_code (ctx, (uint8_t *) base + lr.label_val_disp, (uint8_t *) &lr.u.jump_addr, 8);\n    } else {\n      setup_rel32 (gen_ctx, &lr, base, lr.u.jump_addr);\n    }\n  }\n  change_calls (gen_ctx, base);\n  VARR_TRUNC (MIR_code_reloc_t, relocs, 0);\n  for (size_t i = 0; i < VARR_LENGTH (uint64_t, abs_address_locs); i++) {\n    reloc.offset = VARR_GET (uint64_t, abs_address_locs, i);\n    reloc.value = base + get_int64 (base + reloc.offset, 8);\n    VARR_PUSH (MIR_code_reloc_t, relocs, reloc);\n  }\n  _MIR_update_code_arr (gen_ctx->ctx, base, VARR_LENGTH (MIR_code_reloc_t, relocs),\n                        VARR_ADDR (MIR_code_reloc_t, relocs));\n}\n\nstatic void target_setup_succ_bb_version_data (gen_ctx_t gen_ctx, uint8_t *base) {\n  if (VARR_LENGTH (label_ref_t, label_refs)\n      != VARR_LENGTH (target_bb_version_t, target_succ_bb_versions))\n    /* We can have more one possible branch from original insn\n       (e.g. SWITCH, FBNE).  If it is so, we will make jumps only\n       through BB thunk. */\n    return;\n  for (size_t i = 0; i < VARR_LENGTH (target_bb_version_t, target_succ_bb_versions); i++) {\n    target_bb_version_t data = VARR_GET (target_bb_version_t, target_succ_bb_versions, i);\n    if (data == NULL) continue;\n    data->branch_ref = VARR_GET (label_ref_t, label_refs, i);\n    data->base = base;\n  }\n}\n\nstatic void target_redirect_bb_origin_branch (gen_ctx_t gen_ctx, target_bb_version_t data,\n                                              void *addr) {\n  MIR_context_t ctx = gen_ctx->ctx;\n\n  if (data->base == NULL) return;\n  if (data->branch_ref.abs_addr_p) {\n    _MIR_change_code (ctx, (uint8_t *) data->base + data->branch_ref.label_val_disp,\n                      (uint8_t *) &addr, 8);\n  } else {\n    setup_rel32 (gen_ctx, &data->branch_ref, data->base, addr);\n  }\n  data->base = NULL;\n}\n\nstatic void target_init (gen_ctx_t gen_ctx) {\n  MIR_alloc_t alloc = gen_alloc (gen_ctx);\n  MIR_context_t ctx = gen_ctx->ctx;\n\n  gen_ctx->target_ctx = gen_malloc (gen_ctx, sizeof (struct target_ctx));\n  VARR_CREATE (uint8_t, result_code, alloc, 0);\n  VARR_CREATE (int, insn_pattern_indexes, alloc, 0);\n  VARR_CREATE (uint64_t, const_pool, alloc, 0);\n  VARR_CREATE (const_ref_t, const_refs, alloc, 0);\n  VARR_CREATE (label_ref_t, label_refs, alloc, 0);\n  VARR_CREATE (uint64_t, abs_address_locs, alloc, 0);\n  VARR_CREATE (MIR_code_reloc_t, relocs, alloc, 0);\n  VARR_CREATE (call_ref_t, gen_ctx->target_ctx->call_refs, alloc, 0);\n  MIR_type_t res = MIR_T_D;\n  MIR_var_t args[] = {{MIR_T_D, \"src\", 0}};\n  _MIR_register_unspec_insn (gen_ctx->ctx, MOVDQA_CODE, \"movdqa\", 1, &res, 1, FALSE, args);\n  patterns_init (gen_ctx);\n  temp_jump = MIR_new_insn (ctx, MIR_JMP, MIR_new_label_op (ctx, NULL));\n  temp_jump_pat_ind = find_insn_pattern (gen_ctx, temp_jump, NULL);\n}\n\nstatic void target_finish (gen_ctx_t gen_ctx) {\n  MIR_alloc_t alloc = gen_alloc (gen_ctx);\n  patterns_finish (gen_ctx);\n  _MIR_free_insn (gen_ctx->ctx, temp_jump);\n  VARR_DESTROY (uint8_t, result_code);\n  VARR_DESTROY (int, insn_pattern_indexes);\n  VARR_DESTROY (uint64_t, const_pool);\n  VARR_DESTROY (const_ref_t, const_refs);\n  VARR_DESTROY (label_ref_t, label_refs);\n  VARR_DESTROY (uint64_t, abs_address_locs);\n  VARR_DESTROY (MIR_code_reloc_t, relocs);\n  VARR_DESTROY (call_ref_t, gen_ctx->target_ctx->call_refs);\n  MIR_free (alloc, gen_ctx->target_ctx);\n  gen_ctx->target_ctx = NULL;\n}\n"
        },
        {
          "name": "mir-gen.c",
          "type": "blob",
          "size": 400.7314453125,
          "content": "/* This file is a part of MIR project.\n   Copyright (C) 2018-2024 Vladimir Makarov <vmakarov.gcc@gmail.com>.\n*/\n\n/* Optimization pipeline:\n                                                          ---------------     ------------\n           ----------     -----------     -----------    | Address       |   | Block      |\n   MIR -->| Simplify |-->| Build CFG |-->| Build SSA |-->| Transformation|-->| Cloning    |\n           ----------     -----------     -----------     ---------------     ------------\n                                                                                   |\n                                                                                   V\n      ------------       ------------      ------------      ---------------------------\n     |Dead Code   |     |Dead Store  |    | Copy       |    | Global Value Numbering,   |\n     |Elimination |<--- |Elimination |<---| Propagation|<---| Constant Propagation,     |\n      ------------       ------------      ------------     | Redundat Load Elimination |\n           |                                                 ---------------------------\n           V\n      -----------     --------     -------     ------     ----                   -------\n     | Loop      |   |Register|   | SSA   |   |Out of|   |Jump|    ---------    | Build |\n     | Invariant |-->|Pressure|-->|Combine|-->|  SSA |-->|Opts|-->|Machinize|-->| Live  |\n     | Motion    |   | Relief |    -------     ------     ----     ---------    | Info  |\n      -----------     --------                                                   -------\n                                                                                    |\n                                                                                    V\n                  --------                           ----------                  ---------\n                 |Generate|    -----     -------    |Register  |    --------    |Build    |\n     Machine <---|Machine |<--| DCE |<--|Combine|<--|Allocator |<--|Coalesce|<--|Register |\n      Insns      | Insns  |    -----     -------     ----------     --------    |Conflicts|\n                  --------\t                                                 ---------\n\n   Simplify: Lowering MIR (in mir.c).  Always.\n   Build CGF: Building Control Flow Graph (basic blocks and CFG edges).  Always.\n   Build SSA: Building Single Static Assignment Form by adding phi nodes and SSA edges\n              (for -O2 and above).\n   Address Transformation: Optional pass to remove or change ADDR insns (for -O2 and above).\n   Block Cloning: Cloning insns and BBs to improve hot path optimization opportunities\n                  (for -O2 and above).\n   Global Value Numbering: Removing redundant insns through GVN.  This includes constant\n                           propagation and redundant load eliminations (for -O2 and above).\n   Copy Propagation: SSA copy propagation and removing redundant extension insns\n                     (for -O2 and above).\n   Dead store elimination: Removing redundant stores (for -O2 and above).\n   Dead code elimination: Removing insns with unused outputs (for -O2 and above).\n   Loop invariant motion (LICM): Moving invarinat insns out of loop (for -O2 and above).\n   Pressure relief: Moving insns to decrease register pressure (for -O2 and above).\n   SSA combine: Combining addresses and cmp and branch pairs (for -O2 and above).\n   Out of SSA: Making conventional SSA and removing phi nodes and SSA edges (for -O2 and above).\n   Jump optimizations: Different optimizations on jumps and branches (for -O2 and above).\n   Machinize: Machine-dependent code (e.g. in mir-gen-x86_64.c)\n              transforming MIR for calls ABI, 2-op insns, etc.  Always.\n   Building Live Info: Calculating live in and live out for the basic blocks.  Always.\n   Build Register Conflicts: Build conflict matrix for registers involved in moves.\n                             It is used for register coalescing\n   Coalesce: Aggressive register coalescing\n   Register Allocator (RA): Priority-based linear scan RA (always) with live range splitting\n                            (for -O2 and above).\n   Combine: Code selection by merging data-depended insns into one (for -O1 and above).\n   Dead code elimination (DCE): Removing insns with unused outputs (for -O1 and above).\n   Generate machine insns: Machine-dependent code (e.g. in mir-gen-x86_64.c) creating\n                           machine insns. Always.\n\n   -O0 and -O1 are 2-3 times faster than -O2 but generate considerably slower code.\n\n   Terminology:\n   reg - MIR (pseudo-)register (their numbers are in MIR_OP_VAR and MIR_OP_VAR_MEM > MAX_HARD_REG)\n   hard reg - MIR hard register (their numbers are in MIR_OP_VAR and MIR_OP_VAR_MEM\n                                 and less or equal MAX_HARD_REG)\n   var - pseudo and hard register (MIR_NON_VAR means no var)\n   loc - hard register and stack locations (stack slot numbers start with MAX_HARD_REG + 1).\n\n   Memory aliasing rules:\n\n   * Memory has aliases and they are used for recognizing aliased memory\n\n   * Memory has nloc attribute.  Memory with the same nloc always refer for the same memory\n     although memory with different nloc still may refer for the same memory.  Memory with\n     the same nloc has the same alias attributes\n\n   * Memory found aliased with alias attributes can be recognized as non-aliased one by\n     using alloca flags described below\n\n   * Memory can have flags 'must alloca' and 'may alloca'.  'Must alloca' always goes\n     with 'may alloca'.  'Must alloca' means that we guarantee memory can be allocated\n     only alloca in the func. 'May alloca' means that it is not excluded that memory is\n     allocated by alloca\n\n   * Memory with 'must alloca' flag can have disp attribute.  We can define that\n     'must alloca' memory refers the same memory using disp attribute\n\n*/\n\n#include <stdlib.h>\n#include <string.h>\n#include <inttypes.h>\n\n#include <assert.h>\n#include \"mir-code-alloc.h\"\n#include \"mir-alloc.h\"\n\n#define gen_assert(cond) assert (cond)\n\ntypedef struct gen_ctx *gen_ctx_t;\n\nstatic void util_error (gen_ctx_t gen_ctx, const char *message);\nstatic void varr_error (const char *message) { util_error (NULL, message); }\n\n#define MIR_VARR_ERROR varr_error\n\n#include \"mir.h\"\n#include \"mir-dlist.h\"\n#include \"mir-bitmap.h\"\n#include \"mir-htab.h\"\n#include \"mir-hash.h\"\n#include \"mir-gen.h\"\n\n/* Functions used by target dependent code: */\nstatic MIR_alloc_t gen_alloc (gen_ctx_t gen_ctx);\nstatic void *gen_malloc (gen_ctx_t gen_ctx, size_t size);\nstatic void gen_free (gen_ctx_t gen_ctx, void *ptr);\nstatic MIR_reg_t gen_new_temp_reg (gen_ctx_t gen_ctx, MIR_type_t type, MIR_func_t func);\nstatic int gen_nested_loop_label_p (gen_ctx_t gen_ctx, MIR_insn_t insn);\nstatic void set_label_disp (gen_ctx_t gen_ctx, MIR_insn_t insn, size_t disp);\nstatic size_t get_label_disp (gen_ctx_t gen_ctx, MIR_insn_t insn);\nstatic void create_new_bb_insns (gen_ctx_t gen_ctx, MIR_insn_t before, MIR_insn_t after,\n                                 MIR_insn_t insn_for_bb);\nstatic void gen_delete_insn (gen_ctx_t gen_ctx, MIR_insn_t insn);\nstatic void gen_add_insn_before (gen_ctx_t gen_ctx, MIR_insn_t before, MIR_insn_t insn);\nstatic void gen_add_insn_after (gen_ctx_t gen_ctx, MIR_insn_t after, MIR_insn_t insn);\nstatic void setup_call_hard_reg_args (gen_ctx_t gen_ctx, MIR_insn_t call_insn, MIR_reg_t hard_reg);\nstatic uint64_t get_ref_value (gen_ctx_t gen_ctx, const MIR_op_t *ref_op);\nstatic void gen_setup_lrefs (gen_ctx_t gen_ctx, uint8_t *func_code);\nstatic int64_t gen_int_log2 (int64_t i);\n\n#define SWAP(v1, v2, temp) \\\n  do {                     \\\n    temp = v1;             \\\n    v1 = v2;               \\\n    v2 = temp;             \\\n  } while (0)\n\n#ifndef MIR_GEN_CALL_TRACE\n#define MIR_GEN_CALL_TRACE 0\n#endif\n\n#if MIR_NO_GEN_DEBUG\n#define DEBUG(level, code)\n#else\n#define DEBUG(level, code)                                \\\n  {                                                       \\\n    if (debug_file != NULL && debug_level >= level) code; \\\n  }\n#endif\n\ntypedef struct func_cfg *func_cfg_t;\n\nstruct target_ctx;\nstruct data_flow_ctx;\nstruct ssa_ctx;\nstruct gvn_ctx;\nstruct lr_ctx;\nstruct coalesce_ctx;\nstruct ra_ctx;\nstruct combine_ctx;\n\ntypedef struct loop_node *loop_node_t;\nDEF_VARR (loop_node_t);\n\ntypedef struct dead_var *dead_var_t;\nDEF_DLIST_LINK (dead_var_t);\n\nstruct dead_var {\n  MIR_reg_t var;\n  DLIST_LINK (dead_var_t) dead_var_link;\n};\nDEF_DLIST (dead_var_t, dead_var_link);\n\ntypedef struct bb_insn *bb_insn_t;\nDEF_VARR (bb_insn_t);\n\ntypedef struct target_bb_version *target_bb_version_t;\nDEF_VARR (target_bb_version_t);\n\ntypedef void *void_ptr_t;\nDEF_VARR (void_ptr_t);\n\ntypedef struct {\n  unsigned char alloca_flag;\n  unsigned char disp_def_p;    /* can be true only for MUST_ALLOCA */\n  MIR_type_t type;             /* memory type */\n  MIR_alias_t alias, nonalias; /* memory aliases */\n  MIR_insn_t def_insn;         /* base def insn: its value + disp form address */\n  int64_t disp;                /* defined only when disp_def_p, otherwise disp is unknown */\n} mem_attr_t;\n\nDEF_VARR (mem_attr_t);\n\ntypedef struct spot_attr {\n  uint32_t spot, prop;\n  MIR_op_t *mem_ref; /* ref for memory if the spot is memory, NULL otherwise */\n} spot_attr_t;\n\nDEF_VARR (spot_attr_t);\n\nDEF_VARR (MIR_op_t);\nDEF_VARR (MIR_insn_t);\n\nstruct gen_ctx {\n  MIR_context_t ctx;\n  unsigned optimize_level; /* 0:fast gen; 1:RA+combiner; 2: +GVN/CCP (default); >=3: everything  */\n  MIR_item_t curr_func_item;\n#if !MIR_NO_GEN_DEBUG\n  FILE *debug_file;\n  int debug_level;\n#endif\n  VARR (void_ptr_t) * to_free;\n  int addr_insn_p;    /* true if we have address insns in the input func */\n  bitmap_t tied_regs; /* regs tied to hard reg */\n  bitmap_t addr_regs; /* regs in addr insns as 2nd op */\n  bitmap_t insn_to_consider, temp_bitmap, temp_bitmap2, temp_bitmap3;\n  bitmap_t call_used_hard_regs[MIR_T_BOUND];\n  bitmap_t func_used_hard_regs; /* before prolog: used hard regs except global var hard regs */\n  func_cfg_t curr_cfg;\n  uint32_t curr_bb_index, curr_loop_node_index;\n  DLIST (dead_var_t) free_dead_vars;\n  unsigned long long overall_bbs_num, overall_gen_bbs_num;\n  struct target_ctx *target_ctx;\n  struct data_flow_ctx *data_flow_ctx;\n  struct ssa_ctx *ssa_ctx;\n  struct gvn_ctx *gvn_ctx;\n  struct lr_ctx *lr_ctx;\n  struct coalesce_ctx *coalesce_ctx;\n  struct ra_ctx *ra_ctx;\n  struct combine_ctx *combine_ctx;\n  VARR (MIR_op_t) * temp_ops;\n  VARR (MIR_insn_t) * temp_insns, *temp_insns2;\n  VARR (bb_insn_t) * temp_bb_insns, *temp_bb_insns2;\n  VARR (loop_node_t) * loop_nodes, *queue_nodes, *loop_entries; /* used in building loop tree */\n  /* true when alloca memory escapes by assigning alloca address to memory: */\n  unsigned char full_escape_p;\n  VARR (mem_attr_t) * mem_attrs; /* nloc (> 0) => mem attributes */\n  int max_int_hard_regs, max_fp_hard_regs;\n  /* Slots num for variables.  Some variable can take several slots and can be aligned. */\n  size_t func_stack_slots_num;\n  VARR (target_bb_version_t) * target_succ_bb_versions;\n  VARR (void_ptr_t) * succ_bb_addrs;\n  void *bb_wrapper;                /* to jump to lazy basic block generation */\n  VARR (spot_attr_t) * spot2attr;  /* map: spot number -> spot_attr */\n  VARR (spot_attr_t) * spot_attrs; /* spot attrs wit only non-zero properies */\n};\n\n#define optimize_level gen_ctx->optimize_level\n#define curr_func_item gen_ctx->curr_func_item\n#define debug_file gen_ctx->debug_file\n#define debug_level gen_ctx->debug_level\n#define to_free gen_ctx->to_free\n#define addr_insn_p gen_ctx->addr_insn_p\n#define tied_regs gen_ctx->tied_regs\n#define addr_regs gen_ctx->addr_regs\n#define insn_to_consider gen_ctx->insn_to_consider\n#define temp_bitmap gen_ctx->temp_bitmap\n#define temp_bitmap2 gen_ctx->temp_bitmap2\n#define temp_bitmap3 gen_ctx->temp_bitmap3\n#define call_used_hard_regs gen_ctx->call_used_hard_regs\n#define func_used_hard_regs gen_ctx->func_used_hard_regs\n#define curr_cfg gen_ctx->curr_cfg\n#define curr_bb_index gen_ctx->curr_bb_index\n#define curr_loop_node_index gen_ctx->curr_loop_node_index\n#define full_escape_p gen_ctx->full_escape_p\n#define mem_attrs gen_ctx->mem_attrs\n#define free_dead_vars gen_ctx->free_dead_vars\n#define overall_bbs_num gen_ctx->overall_bbs_num\n#define overall_gen_bbs_num gen_ctx->overall_gen_bbs_num\n#define temp_ops gen_ctx->temp_ops\n#define temp_insns gen_ctx->temp_insns\n#define temp_insns2 gen_ctx->temp_insns2\n#define temp_bb_insns gen_ctx->temp_bb_insns\n#define temp_bb_insns2 gen_ctx->temp_bb_insns2\n#define loop_nodes gen_ctx->loop_nodes\n#define queue_nodes gen_ctx->queue_nodes\n#define loop_entries gen_ctx->loop_entries\n#define max_int_hard_regs gen_ctx->max_int_hard_regs\n#define max_fp_hard_regs gen_ctx->max_fp_hard_regs\n#define func_stack_slots_num gen_ctx->func_stack_slots_num\n#define target_succ_bb_versions gen_ctx->target_succ_bb_versions\n#define succ_bb_addrs gen_ctx->succ_bb_addrs\n#define bb_wrapper gen_ctx->bb_wrapper\n#define spot_attrs gen_ctx->spot_attrs\n#define spot2attr gen_ctx->spot2attr\n\n#define LOOP_COST_FACTOR 5\n\ntypedef struct bb_version *bb_version_t;\n\nstruct func_or_bb {\n  /* full_p is used only when func_p and means generation machine code for full func */\n  char func_p, full_p;\n  union {\n    MIR_item_t func_item;\n    bb_version_t bb_version;\n  } u;\n};\n\ntypedef struct func_or_bb func_or_bb_t;\nDEF_VARR (func_or_bb_t);\n\nstatic inline gen_ctx_t *gen_ctx_loc (MIR_context_t ctx) { return (gen_ctx_t *) ctx; }\n\nDEF_VARR (int);\nDEF_VARR (uint8_t);\nDEF_VARR (uint64_t);\nDEF_VARR (MIR_code_reloc_t);\n\n#if defined(__x86_64__) || defined(_M_AMD64)\n#include \"mir-gen-x86_64.c\"\n#elif defined(__aarch64__)\n#include \"mir-gen-aarch64.c\"\n#elif defined(__PPC64__)\n#include \"mir-gen-ppc64.c\"\n#elif defined(__s390x__)\n#include \"mir-gen-s390x.c\"\n#elif defined(__riscv)\n#if __riscv_xlen != 64 || __riscv_flen < 64 || !__riscv_float_abi_double || !__riscv_mul \\\n  || !__riscv_div || !__riscv_compressed\n#error \"only 64-bit RISCV supported (at least rv64imafd)\"\n#endif\n#if __riscv_flen == 128\n#error \"RISCV 128-bit floats (Q set) is not supported\"\n#endif\n#include \"mir-gen-riscv64.c\"\n#else\n#error \"undefined or unsupported generation target\"\n#endif\n\ntypedef struct bb_stub *bb_stub_t;\nDEF_DLIST_LINK (bb_version_t);\n\nstruct bb_version {\n  bb_stub_t bb_stub;\n  DLIST_LINK (bb_version_t) bb_version_link;\n  int call_p;\n  void *addr; /* bb code address or generator creating and returning address */\n  void *machine_code;\n  struct target_bb_version target_data; /* data container for the target code */\n  uint32_t n_attrs;\n  spot_attr_t attrs[1];\n};\n\n/* Definition of double list of bb_version_t type elements */\nDEF_DLIST (bb_version_t, bb_version_link);\n\nstruct bb_stub {\n  DLIST (bb_version_t) bb_versions;\n  MIR_item_t func_item;\n  MIR_insn_t first_insn, last_insn;\n};\n\nstatic void MIR_NO_RETURN util_error (gen_ctx_t gen_ctx, const char *message) {\n  (*MIR_get_error_func (gen_ctx->ctx)) (MIR_alloc_error, message);\n}\n\nstatic MIR_alloc_t gen_alloc (gen_ctx_t gen_ctx) {\n  return MIR_get_alloc (gen_ctx->ctx);\n}\n\nstatic void *gen_malloc (gen_ctx_t gen_ctx, size_t size) {\n  MIR_alloc_t alloc = MIR_get_alloc (gen_ctx->ctx);\n  void *res = MIR_malloc (alloc, size);\n  if (res == NULL) util_error (gen_ctx, \"no memory\");\n  return res;\n}\n\nstatic void gen_free (gen_ctx_t gen_ctx, void *ptr) {\n  MIR_alloc_t alloc = gen_alloc (gen_ctx);\n  MIR_free (alloc, ptr);\n}\n\nstatic void *gen_malloc_and_mark_to_free (gen_ctx_t gen_ctx, size_t size) {\n  void *res = gen_malloc (gen_ctx, size);\n  VARR_PUSH (void_ptr_t, to_free, res);\n  return res;\n}\n\n#define DEFAULT_INIT_BITMAP_BITS_NUM 256\n\ntypedef struct bb *bb_t;\n\nDEF_DLIST_LINK (bb_t);\n\ntypedef struct insn_data *insn_data_t;\n\nDEF_DLIST_LINK (bb_insn_t);\n\ntypedef struct edge *edge_t;\n\ntypedef edge_t in_edge_t;\n\ntypedef edge_t out_edge_t;\n\nDEF_DLIST_LINK (in_edge_t);\nDEF_DLIST_LINK (out_edge_t);\n\nstruct edge {\n  bb_t src, dst;\n  DLIST_LINK (in_edge_t) in_link;\n  DLIST_LINK (out_edge_t) out_link;\n  unsigned char fall_through_p, back_edge_p, flag1, flag2;\n};\n\nDEF_DLIST (in_edge_t, in_link);\nDEF_DLIST (out_edge_t, out_link);\n\nstruct insn_data { /* used only for calls/labels in -O0 mode */\n  bb_t bb;\n  union {\n    bitmap_t call_hard_reg_args; /* non-null for calls */\n    size_t label_disp;           /* used for labels */\n  } u;\n};\n\n#define MAY_ALLOCA 0x1\n#define MUST_ALLOCA 0x2\nstruct bb_insn {\n  MIR_insn_t insn;\n  unsigned char gvn_val_const_p; /* true for int value, false otherwise */\n  unsigned char alloca_flag;     /* true for value may and/or must be from alloca */\n  uint32_t index, mem_index;\n  int64_t gvn_val; /* used for GVN, it is negative index for non GVN expr insns */\n  DLIST_LINK (bb_insn_t) bb_insn_link;\n  bb_t bb;\n  DLIST (dead_var_t) insn_dead_vars;\n  bitmap_t call_hard_reg_args; /* non-null for calls */\n  size_t label_disp;           /* for label */\n};\n\nDEF_DLIST (bb_insn_t, bb_insn_link);\n\nstruct bb {\n  size_t index, pre, rpost, bfs; /* preorder, reverse post order, breadth first order */\n  DLIST_LINK (bb_t) bb_link;\n  DLIST (in_edge_t) in_edges;\n  /* The out edges order: optional fall through bb, optional label bb,\n     optional exit bb.  There is always at least one edge.  */\n  DLIST (out_edge_t) out_edges;\n  DLIST (bb_insn_t) bb_insns;\n  unsigned char call_p;        /* used in mem avail calculation, true if there is a call in BB */\n  unsigned char flag;          /* used in different calculation */\n  unsigned char reachable_p;   /* reachable if its label is used as value */\n  bitmap_t in, out, gen, kill; /* var bitmaps for different data flow problems */\n  bitmap_t dom_in, dom_out;    /* additional var bitmaps */\n  loop_node_t loop_node;\n  int max_int_pressure, max_fp_pressure;\n};\n\nDEF_DLIST (bb_t, bb_link);\n\nDEF_DLIST_LINK (loop_node_t);\nDEF_DLIST_TYPE (loop_node_t);\n\nstruct loop_node {\n  uint32_t index; /* if BB != NULL, it is index of BB */\n  bb_t bb;        /* NULL for internal tree node  */\n  loop_node_t entry;\n  loop_node_t parent;\n  union {                       /* used in LICM */\n    loop_node_t preheader;      /* used for non-bb loop it is loop node of preheader bb */\n    loop_node_t preheader_loop; /* used for preheader bb it is the loop node */\n  } u;\n  DLIST (loop_node_t) children;\n  DLIST_LINK (loop_node_t) children_link;\n  int max_int_pressure, max_fp_pressure;\n};\n\nDEF_DLIST_CODE (loop_node_t, children_link);\n\nDEF_DLIST_LINK (func_cfg_t);\n\nstruct reg_info {\n  long freq;\n  /* The following members are defined and used only in RA */\n  size_t live_length; /* # of program points where reg lives */\n};\n\ntypedef struct reg_info reg_info_t;\n\nDEF_VARR (reg_info_t);\n\ntypedef struct {\n  int uns_p;\n  union {\n    int64_t i;\n    uint64_t u;\n  } u;\n} const_t;\n\nstruct func_cfg {\n  MIR_reg_t max_var;\n  uint32_t curr_bb_insn_index;\n  VARR (reg_info_t) * reg_info; /* regs */\n  bitmap_t call_crossed_regs;\n  DLIST (bb_t) bbs;\n  loop_node_t root_loop_node;\n};\n\nstatic void init_dead_vars (gen_ctx_t gen_ctx) { DLIST_INIT (dead_var_t, free_dead_vars); }\n\nstatic void free_dead_var (gen_ctx_t gen_ctx, dead_var_t dv) {\n  DLIST_APPEND (dead_var_t, free_dead_vars, dv);\n}\n\nstatic dead_var_t get_dead_var (gen_ctx_t gen_ctx) {\n  dead_var_t dv;\n\n  if ((dv = DLIST_HEAD (dead_var_t, free_dead_vars)) == NULL)\n    return gen_malloc (gen_ctx, sizeof (struct dead_var));\n  DLIST_REMOVE (dead_var_t, free_dead_vars, dv);\n  return dv;\n}\n\nstatic void finish_dead_vars (gen_ctx_t gen_ctx) {\n  dead_var_t dv;\n\n  while ((dv = DLIST_HEAD (dead_var_t, free_dead_vars)) != NULL) {\n    DLIST_REMOVE (dead_var_t, free_dead_vars, dv);\n    gen_free (gen_ctx, dv);\n  }\n}\n\nstatic void add_bb_insn_dead_var (gen_ctx_t gen_ctx, bb_insn_t bb_insn, MIR_reg_t var) {\n  dead_var_t dv;\n\n  for (dv = DLIST_HEAD (dead_var_t, bb_insn->insn_dead_vars); dv != NULL;\n       dv = DLIST_NEXT (dead_var_t, dv))\n    if (dv->var == var) return;\n  dv = get_dead_var (gen_ctx);\n  dv->var = var;\n  DLIST_APPEND (dead_var_t, bb_insn->insn_dead_vars, dv);\n}\n\nstatic dead_var_t find_bb_insn_dead_var (bb_insn_t bb_insn, MIR_reg_t var) {\n  dead_var_t dv;\n\n  for (dv = DLIST_HEAD (dead_var_t, bb_insn->insn_dead_vars); dv != NULL;\n       dv = DLIST_NEXT (dead_var_t, dv))\n    if (dv->var == var) return dv;\n  return NULL;\n}\n\nstatic void clear_bb_insn_dead_vars (gen_ctx_t gen_ctx, bb_insn_t bb_insn) {\n  dead_var_t dv;\n\n  while ((dv = DLIST_HEAD (dead_var_t, bb_insn->insn_dead_vars)) != NULL) {\n    DLIST_REMOVE (dead_var_t, bb_insn->insn_dead_vars, dv);\n    free_dead_var (gen_ctx, dv);\n  }\n}\n\nstatic void remove_bb_insn_dead_var (gen_ctx_t gen_ctx, bb_insn_t bb_insn, MIR_reg_t var) {\n  dead_var_t dv, next_dv;\n\n  gen_assert (var != MIR_NON_VAR);\n  for (dv = DLIST_HEAD (dead_var_t, bb_insn->insn_dead_vars); dv != NULL; dv = next_dv) {\n    next_dv = DLIST_NEXT (dead_var_t, dv);\n    if (dv->var != var) continue;\n    DLIST_REMOVE (dead_var_t, bb_insn->insn_dead_vars, dv);\n    free_dead_var (gen_ctx, dv);\n  }\n}\n\nstatic void move_bb_insn_dead_vars (gen_ctx_t gen_ctx, bb_insn_t bb_insn, bb_insn_t from_bb_insn,\n                                    int (*filter_p) (gen_ctx_t, bb_insn_t, MIR_reg_t)) {\n  dead_var_t dv;\n\n  while ((dv = DLIST_HEAD (dead_var_t, from_bb_insn->insn_dead_vars)) != NULL) {\n    DLIST_REMOVE (dead_var_t, from_bb_insn->insn_dead_vars, dv);\n    if (filter_p (gen_ctx, bb_insn, dv->var)) {\n      DLIST_APPEND (dead_var_t, bb_insn->insn_dead_vars, dv);\n    } else {\n      free_dead_var (gen_ctx, dv);\n    }\n  }\n}\n\nstatic int insn_data_p (MIR_insn_t insn) {\n  return insn->code == MIR_LABEL || MIR_call_code_p (insn->code);\n}\n\nstatic void setup_insn_data (gen_ctx_t gen_ctx, MIR_insn_t insn, bb_t bb) {\n  insn_data_t insn_data;\n\n  if (!insn_data_p (insn)) {\n    insn->data = bb;\n    return;\n  }\n  insn_data = insn->data = gen_malloc (gen_ctx, sizeof (struct insn_data));\n  insn_data->bb = bb;\n  insn_data->u.call_hard_reg_args = NULL;\n}\n\nstatic bb_t get_insn_data_bb (MIR_insn_t insn) {\n  return insn_data_p (insn) ? ((insn_data_t) insn->data)->bb : (bb_t) insn->data;\n}\n\nstatic void delete_insn_data (gen_ctx_t gen_ctx, MIR_insn_t insn) {\n  insn_data_t insn_data = insn->data;\n\n  if (insn_data == NULL || !insn_data_p (insn)) return;\n  if (MIR_call_code_p (insn->code) && insn_data->u.call_hard_reg_args != NULL)\n    bitmap_destroy (insn_data->u.call_hard_reg_args);\n  gen_free (gen_ctx, insn_data);\n}\n\nstatic bb_insn_t create_bb_insn (gen_ctx_t gen_ctx, MIR_insn_t insn, bb_t bb) {\n  bb_insn_t bb_insn = gen_malloc (gen_ctx, sizeof (struct bb_insn));\n  MIR_alloc_t alloc = gen_alloc (gen_ctx);\n\n  insn->data = bb_insn;\n  bb_insn->bb = bb;\n  bb_insn->insn = insn;\n  bb_insn->gvn_val_const_p = FALSE;\n  bb_insn->alloca_flag = insn->code == MIR_ALLOCA ? MAY_ALLOCA | MUST_ALLOCA : 0;\n  bb_insn->call_hard_reg_args = NULL;\n  gen_assert (curr_cfg->curr_bb_insn_index != (uint32_t) ~0ull);\n  bb_insn->index = curr_cfg->curr_bb_insn_index++;\n  bb_insn->mem_index = 0;\n  bb_insn->gvn_val = bb_insn->index;\n  DLIST_INIT (dead_var_t, bb_insn->insn_dead_vars);\n  if (MIR_call_code_p (insn->code))\n    bb_insn->call_hard_reg_args = bitmap_create2 (alloc, MAX_HARD_REG + 1);\n  bb_insn->label_disp = 0;\n  return bb_insn;\n}\n\nstatic bb_insn_t add_new_bb_insn (gen_ctx_t gen_ctx, MIR_insn_t insn, bb_t bb, int append_p) {\n  bb_insn_t bb_insn = create_bb_insn (gen_ctx, insn, bb);\n\n  if (append_p)\n    DLIST_APPEND (bb_insn_t, bb->bb_insns, bb_insn);\n  else\n    DLIST_PREPEND (bb_insn_t, bb->bb_insns, bb_insn);\n  return bb_insn;\n}\n\nstatic void delete_bb_insn (gen_ctx_t gen_ctx, bb_insn_t bb_insn) {\n  DLIST_REMOVE (bb_insn_t, bb_insn->bb->bb_insns, bb_insn);\n  bb_insn->insn->data = NULL;\n  clear_bb_insn_dead_vars (gen_ctx, bb_insn);\n  if (bb_insn->call_hard_reg_args != NULL) bitmap_destroy (bb_insn->call_hard_reg_args);\n  gen_free (gen_ctx, bb_insn);\n}\n\nstatic bb_t get_insn_bb (gen_ctx_t gen_ctx, MIR_insn_t insn) {\n  return optimize_level == 0 ? get_insn_data_bb (insn) : ((bb_insn_t) insn->data)->bb;\n}\n\nstatic void create_new_bb_insns (gen_ctx_t gen_ctx, MIR_insn_t before, MIR_insn_t after,\n                                 MIR_insn_t insn_for_bb) {\n  MIR_insn_t insn;\n  bb_insn_t bb_insn, new_bb_insn;\n  bb_t bb;\n\n  /* Null insn_for_bb means it should be in the 1st block: skip entry and exit blocks: */\n  bb = insn_for_bb == NULL ? DLIST_EL (bb_t, curr_cfg->bbs, 2) : get_insn_bb (gen_ctx, insn_for_bb);\n  if (optimize_level == 0) {\n    for (insn = (before == NULL ? DLIST_HEAD (MIR_insn_t, curr_func_item->u.func->insns)\n                                : DLIST_NEXT (MIR_insn_t, before));\n         insn != after; insn = DLIST_NEXT (MIR_insn_t, insn))\n      setup_insn_data (gen_ctx, insn, bb);\n    return;\n  }\n  if (before != NULL && (bb_insn = before->data)->bb == bb) {\n    for (insn = DLIST_NEXT (MIR_insn_t, before); insn != after;\n         insn = DLIST_NEXT (MIR_insn_t, insn), bb_insn = new_bb_insn) {\n      new_bb_insn = create_bb_insn (gen_ctx, insn, bb);\n      DLIST_INSERT_AFTER (bb_insn_t, bb->bb_insns, bb_insn, new_bb_insn);\n    }\n  } else {\n    gen_assert (after != NULL);\n    bb_insn = after->data;\n    insn = (before == NULL ? DLIST_HEAD (MIR_insn_t, curr_func_item->u.func->insns)\n                           : DLIST_NEXT (MIR_insn_t, before));\n    for (; insn != after; insn = DLIST_NEXT (MIR_insn_t, insn)) {\n      new_bb_insn = create_bb_insn (gen_ctx, insn, bb);\n      if (bb == bb_insn->bb)\n        DLIST_INSERT_BEFORE (bb_insn_t, bb->bb_insns, bb_insn, new_bb_insn);\n      else\n        DLIST_APPEND (bb_insn_t, bb->bb_insns, new_bb_insn);\n    }\n  }\n}\n\nstatic void gen_delete_insn (gen_ctx_t gen_ctx, MIR_insn_t insn) {\n  if (optimize_level == 0)\n    delete_insn_data (gen_ctx, insn);\n  else\n    delete_bb_insn (gen_ctx, insn->data);\n  MIR_remove_insn (gen_ctx->ctx, curr_func_item, insn);\n}\n\nstatic void gen_add_insn_before (gen_ctx_t gen_ctx, MIR_insn_t before, MIR_insn_t insn) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  MIR_insn_t insn_for_bb = before;\n\n  gen_assert (!MIR_any_branch_code_p (insn->code) && insn->code != MIR_LABEL);\n  if (before->code == MIR_LABEL) {\n    insn_for_bb = DLIST_PREV (MIR_insn_t, before);\n    gen_assert (insn_for_bb == NULL || !MIR_any_branch_code_p (insn_for_bb->code));\n  }\n  MIR_insert_insn_before (ctx, curr_func_item, before, insn);\n  create_new_bb_insns (gen_ctx, DLIST_PREV (MIR_insn_t, insn), before, insn_for_bb);\n}\n\nstatic void gen_add_insn_after (gen_ctx_t gen_ctx, MIR_insn_t after, MIR_insn_t insn) {\n  MIR_insn_t insn_for_bb = after;\n\n  gen_assert (insn->code != MIR_LABEL);\n  if (MIR_any_branch_code_p (insn_for_bb->code)) insn_for_bb = DLIST_NEXT (MIR_insn_t, insn_for_bb);\n  gen_assert (!MIR_any_branch_code_p (insn_for_bb->code));\n  MIR_insert_insn_after (gen_ctx->ctx, curr_func_item, after, insn);\n  create_new_bb_insns (gen_ctx, after, DLIST_NEXT (MIR_insn_t, insn), insn_for_bb);\n}\n\nstatic void gen_move_insn_before (gen_ctx_t gen_ctx, MIR_insn_t before, MIR_insn_t insn) {\n  bb_insn_t bb_insn = insn->data, before_bb_insn = before->data;\n\n  DLIST_REMOVE (MIR_insn_t, curr_func_item->u.func->insns, insn);\n  MIR_insert_insn_before (gen_ctx->ctx, curr_func_item, before, insn);\n  DLIST_REMOVE (bb_insn_t, bb_insn->bb->bb_insns, bb_insn);\n  DLIST_INSERT_BEFORE (bb_insn_t, before_bb_insn->bb->bb_insns, before_bb_insn, bb_insn);\n  bb_insn->bb = before_bb_insn->bb;\n}\n\nstatic void MIR_UNUSED setup_call_hard_reg_args (gen_ctx_t gen_ctx, MIR_insn_t call_insn,\n                                                 MIR_reg_t hard_reg) {\n  MIR_alloc_t alloc = gen_alloc (gen_ctx);\n  insn_data_t insn_data;\n\n  gen_assert (MIR_call_code_p (call_insn->code) && hard_reg <= MAX_HARD_REG);\n  if (optimize_level != 0) {\n    bitmap_set_bit_p (((bb_insn_t) call_insn->data)->call_hard_reg_args, hard_reg);\n    return;\n  }\n  if ((insn_data = call_insn->data)->u.call_hard_reg_args == NULL)\n    insn_data->u.call_hard_reg_args = (void *) bitmap_create2 (alloc, MAX_HARD_REG + 1);\n  bitmap_set_bit_p (insn_data->u.call_hard_reg_args, hard_reg);\n}\n\nstatic int MIR_UNUSED gen_nested_loop_label_p (gen_ctx_t gen_ctx, MIR_insn_t insn) {\n  gen_assert (insn->code == MIR_LABEL);\n  if (optimize_level <= 1) return FALSE;\n  bb_t bb = get_insn_bb (gen_ctx, insn);\n  if (bb->loop_node == NULL) return FALSE;\n  loop_node_t node, parent = bb->loop_node->parent;\n  if (parent->entry == NULL || parent->entry->bb != bb) return FALSE;\n  for (node = DLIST_HEAD (loop_node_t, parent->children); node != NULL;\n       node = DLIST_NEXT (loop_node_t, node))\n    if (node->bb == NULL) return FALSE; /* subloop */\n  return TRUE;\n}\n\nstatic void set_label_disp (gen_ctx_t gen_ctx, MIR_insn_t insn, size_t disp) {\n  gen_assert (insn->code == MIR_LABEL);\n  if (optimize_level == 0)\n    ((insn_data_t) insn->data)->u.label_disp = disp;\n  else\n    ((bb_insn_t) insn->data)->label_disp = disp;\n}\nstatic size_t get_label_disp (gen_ctx_t gen_ctx, MIR_insn_t insn) {\n  gen_assert (insn->code == MIR_LABEL);\n  return (optimize_level == 0 ? ((insn_data_t) insn->data)->u.label_disp\n                              : ((bb_insn_t) insn->data)->label_disp);\n}\n\nstatic uint64_t get_ref_value (gen_ctx_t gen_ctx, const MIR_op_t *ref_op) {\n  gen_assert (ref_op->mode == MIR_OP_REF);\n  if (ref_op->u.ref->item_type == MIR_data_item && ref_op->u.ref->u.data->name != NULL\n      && _MIR_reserved_ref_name_p (gen_ctx->ctx, ref_op->u.ref->u.data->name))\n    return (uint64_t) ref_op->u.ref->u.data->u.els;\n  return (uint64_t) ref_op->u.ref->addr;\n}\n\nstatic void gen_setup_lrefs (gen_ctx_t gen_ctx, uint8_t *func_code) {\n  for (MIR_lref_data_t lref = curr_func_item->u.func->first_lref; lref != NULL;\n       lref = lref->next) { /* set up lrefs */\n    int64_t disp = (int64_t) get_label_disp (gen_ctx, lref->label) + lref->disp;\n    *(void **) lref->load_addr\n      = lref->label2 == NULL ? (void *) (func_code + disp)\n                             : (void *) (disp - (int64_t) get_label_disp (gen_ctx, lref->label2));\n  }\n}\n\nstatic void setup_used_hard_regs (gen_ctx_t gen_ctx, MIR_type_t type, MIR_reg_t hard_reg) {\n  MIR_reg_t curr_hard_reg;\n  int i, slots_num = target_locs_num (hard_reg, type);\n\n  for (i = 0; i < slots_num; i++)\n    if ((curr_hard_reg = target_nth_loc (hard_reg, type, i)) <= MAX_HARD_REG)\n      bitmap_set_bit_p (func_used_hard_regs, curr_hard_reg);\n}\n\nstatic MIR_reg_t get_temp_hard_reg (MIR_type_t type, int first_p) {\n  if (type == MIR_T_F) return first_p ? TEMP_FLOAT_HARD_REG1 : TEMP_FLOAT_HARD_REG2;\n  if (type == MIR_T_D) return first_p ? TEMP_DOUBLE_HARD_REG1 : TEMP_DOUBLE_HARD_REG2;\n  if (type == MIR_T_LD) return first_p ? TEMP_LDOUBLE_HARD_REG1 : TEMP_LDOUBLE_HARD_REG2;\n  return first_p ? TEMP_INT_HARD_REG1 : TEMP_INT_HARD_REG2;\n}\n\nstatic bb_t create_bb (gen_ctx_t gen_ctx, MIR_insn_t insn) {\n  bb_t bb = gen_malloc (gen_ctx, sizeof (struct bb));\n  MIR_alloc_t alloc = gen_alloc (gen_ctx);\n\n  bb->pre = bb->rpost = bb->bfs = 0;\n  bb->loop_node = NULL;\n  DLIST_INIT (bb_insn_t, bb->bb_insns);\n  DLIST_INIT (in_edge_t, bb->in_edges);\n  DLIST_INIT (out_edge_t, bb->out_edges);\n  bb->call_p = bb->flag = bb->reachable_p = FALSE;\n  bb->in = bitmap_create2 (alloc, DEFAULT_INIT_BITMAP_BITS_NUM);\n  bb->out = bitmap_create2 (alloc, DEFAULT_INIT_BITMAP_BITS_NUM);\n  bb->gen = bitmap_create2 (alloc, DEFAULT_INIT_BITMAP_BITS_NUM);\n  bb->kill = bitmap_create2 (alloc, DEFAULT_INIT_BITMAP_BITS_NUM);\n  bb->dom_in = bitmap_create2 (alloc, DEFAULT_INIT_BITMAP_BITS_NUM);\n  bb->dom_out = bitmap_create2 (alloc, DEFAULT_INIT_BITMAP_BITS_NUM);\n  bb->max_int_pressure = bb->max_fp_pressure = 0;\n  if (insn != NULL) {\n    if (optimize_level == 0)\n      setup_insn_data (gen_ctx, insn, bb);\n    else\n      add_new_bb_insn (gen_ctx, insn, bb, TRUE);\n  }\n  return bb;\n}\n\nstatic void add_new_bb (gen_ctx_t gen_ctx, bb_t bb) {\n  DLIST_APPEND (bb_t, curr_cfg->bbs, bb);\n  bb->index = curr_bb_index++;\n}\n\nstatic void insert_new_bb_after (gen_ctx_t gen_ctx, bb_t after, bb_t bb) {\n  DLIST_INSERT_AFTER (bb_t, curr_cfg->bbs, after, bb);\n  bb->index = curr_bb_index++;\n}\n\nstatic void insert_new_bb_before (gen_ctx_t gen_ctx, bb_t before, bb_t bb) {\n  DLIST_INSERT_BEFORE (bb_t, curr_cfg->bbs, before, bb);\n  bb->index = curr_bb_index++;\n}\n\nstatic edge_t create_edge (gen_ctx_t gen_ctx, bb_t src, bb_t dst, int fall_through_p,\n                           int append_p) {\n  edge_t e = gen_malloc (gen_ctx, sizeof (struct edge));\n\n  e->src = src;\n  e->dst = dst;\n  if (append_p) {\n    DLIST_APPEND (in_edge_t, dst->in_edges, e);\n    DLIST_APPEND (out_edge_t, src->out_edges, e);\n  } else {\n    DLIST_PREPEND (in_edge_t, dst->in_edges, e);\n    DLIST_PREPEND (out_edge_t, src->out_edges, e);\n  }\n  e->fall_through_p = fall_through_p;\n  e->back_edge_p = e->flag1 = e->flag2 = FALSE;\n  return e;\n}\n\nstatic void delete_edge (gen_ctx_t gen_ctx, edge_t e) {\n  DLIST_REMOVE (out_edge_t, e->src->out_edges, e);\n  DLIST_REMOVE (in_edge_t, e->dst->in_edges, e);\n  gen_free (gen_ctx, e);\n}\n\nstatic edge_t find_edge (bb_t src, bb_t dst) {\n  for (edge_t e = DLIST_HEAD (out_edge_t, src->out_edges); e != NULL;\n       e = DLIST_NEXT (out_edge_t, e))\n    if (e->dst == dst) return e;\n  return NULL;\n}\n\nstatic void delete_bb (gen_ctx_t gen_ctx, bb_t bb) {\n  MIR_alloc_t alloc = gen_alloc (gen_ctx);\n  edge_t e, next_e;\n\n  for (e = DLIST_HEAD (out_edge_t, bb->out_edges); e != NULL; e = next_e) {\n    next_e = DLIST_NEXT (out_edge_t, e);\n    delete_edge (gen_ctx, e);\n  }\n  for (e = DLIST_HEAD (in_edge_t, bb->in_edges); e != NULL; e = next_e) {\n    next_e = DLIST_NEXT (in_edge_t, e);\n    delete_edge (gen_ctx, e);\n  }\n  if (bb->loop_node != NULL) {\n    if (bb->loop_node->parent->entry == bb->loop_node) bb->loop_node->parent->entry = NULL;\n    DLIST_REMOVE (loop_node_t, bb->loop_node->parent->children, bb->loop_node);\n    if (bb->loop_node->u.preheader_loop != NULL)\n      bb->loop_node->u.preheader_loop->u.preheader = NULL;\n    gen_free (gen_ctx, bb->loop_node);\n  }\n  DLIST_REMOVE (bb_t, curr_cfg->bbs, bb);\n  bitmap_destroy (bb->in);\n  bitmap_destroy (bb->out);\n  bitmap_destroy (bb->gen);\n  bitmap_destroy (bb->kill);\n  bitmap_destroy (bb->dom_in);\n  bitmap_destroy (bb->dom_out);\n  gen_free (gen_ctx, bb);\n}\n\nstatic void print_bb_insn (gen_ctx_t gen_ctx, bb_insn_t bb_insn, int with_notes_p);\n\n/* Return BB to put insns from edge E.  If necessary, split edge by creating new bb, bb enumeration\n   and new bb bitmaps can be invalid after that.  Loop info is undefined for the new bb. */\nstatic bb_t split_edge_if_necessary (gen_ctx_t gen_ctx, edge_t e) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  size_t i;\n  bb_t new_bb, src = e->src, dst = e->dst;\n  edge_t e2;\n  bb_insn_t last_bb_insn = DLIST_TAIL (bb_insn_t, src->bb_insns);\n  bb_insn_t first_bb_insn = DLIST_HEAD (bb_insn_t, dst->bb_insns);\n  MIR_insn_t insn, tail_insn, last_insn = last_bb_insn->insn, first_insn = first_bb_insn->insn;\n  DEBUG (4, {\n    fprintf (debug_file, \"    Splitting bb%lu->bb%lu:\\n\", (unsigned long) src->index,\n             (unsigned long) dst->index);\n  });\n  if (DLIST_HEAD (out_edge_t, src->out_edges) == DLIST_TAIL (out_edge_t, src->out_edges)\n      || e->fall_through_p) { /* fall through or src with one dest */\n    if (e->fall_through_p) {\n      insn = MIR_new_insn_arr (ctx, MIR_USE, 0, NULL); /* just nop */\n      MIR_insert_insn_after (ctx, curr_func_item, last_insn, insn);\n    } else if (DLIST_HEAD (in_edge_t, src->in_edges) == DLIST_TAIL (in_edge_t, src->in_edges)) {\n      return src;\n    } else { /* jump with one dest only: move jmp to new fall-though block */\n      gen_assert (last_insn->code == MIR_JMP || last_insn->code == MIR_RET\n                  || last_insn->code == MIR_JRET);\n      delete_bb_insn (gen_ctx, last_bb_insn);\n      insn = last_insn;\n    }\n    new_bb = create_bb (gen_ctx, insn);\n    insert_new_bb_after (gen_ctx, src, new_bb);\n    DLIST_REMOVE (in_edge_t, dst->in_edges, e);\n    e->dst = new_bb;\n    DLIST_APPEND (in_edge_t, new_bb->in_edges, e);\n    create_edge (gen_ctx, new_bb, dst, e->fall_through_p, TRUE);\n    e->fall_through_p = TRUE;\n    DEBUG (4, {\n      fprintf (debug_file, \"     creating fall through bb%lu after bb%lu, redirect the edge to it\",\n               (unsigned long) new_bb->index, (unsigned long) src->index);\n      fprintf (debug_file, \", and create edge bb%lu->bb%lu:\\n\", (unsigned long) new_bb->index,\n               (unsigned long) dst->index);\n      fprintf (debug_file, \"       new bb insn is \");\n      print_bb_insn (gen_ctx, (bb_insn_t) insn->data, FALSE);\n    });\n  } else if (DLIST_HEAD (in_edge_t, dst->in_edges) == DLIST_TAIL (in_edge_t, dst->in_edges)) {\n    gen_assert (first_insn->code == MIR_LABEL);\n    if (first_bb_insn == DLIST_TAIL (bb_insn_t, dst->bb_insns)) return dst;\n    /* non-fall through dest with one source only: move dest label to new block */\n    delete_bb_insn (gen_ctx, first_bb_insn);\n    new_bb = create_bb (gen_ctx, first_insn);\n    insert_new_bb_before (gen_ctx, dst, new_bb);\n    DLIST_REMOVE (in_edge_t, dst->in_edges, e);\n    e->dst = new_bb;\n    DLIST_APPEND (in_edge_t, new_bb->in_edges, e);\n    create_edge (gen_ctx, new_bb, dst, TRUE, TRUE);\n    DEBUG (4, {\n      fprintf (debug_file, \"     creating bb%lu before bb%lu, redirect the edge to it\",\n               (unsigned long) new_bb->index, (unsigned long) dst->index);\n      fprintf (debug_file, \", and create fall-through edge bb%lu->bb%lu:\\n\",\n               (unsigned long) new_bb->index, (unsigned long) dst->index);\n      fprintf (debug_file, \"       new bb insn is \");\n      print_bb_insn (gen_ctx, (bb_insn_t) first_insn->data, FALSE);\n    });\n  } else { /* critical non-fall through edge: */\n    gen_assert (first_insn->code == MIR_LABEL);\n    for (e2 = DLIST_HEAD (in_edge_t, dst->in_edges); e2 != NULL; e2 = DLIST_NEXT (in_edge_t, e2)) {\n      if (e2->fall_through_p) break;\n      gen_assert (DLIST_TAIL (bb_insn_t, e2->src->bb_insns) != NULL\n                  && MIR_any_branch_code_p (DLIST_TAIL (bb_insn_t, e2->src->bb_insns)->insn->code));\n    }\n    if (e2 != NULL) { /* make fall through edge to dst a jump edge */\n      gen_assert (e2->dst == dst);\n      insn = MIR_new_insn (ctx, MIR_JMP, MIR_new_label_op (ctx, first_insn));\n      tail_insn = DLIST_TAIL (bb_insn_t, e2->src->bb_insns)->insn;\n      if (DLIST_NEXT (out_edge_t, e2) == NULL && DLIST_PREV (out_edge_t, e2) == NULL) {\n        /* e2->src with the only output edge: just put jump at the end of e2->src */\n        gen_assert (!MIR_any_branch_code_p (tail_insn->code));\n        gen_add_insn_after (gen_ctx, tail_insn, insn);\n        e2->fall_through_p = FALSE;\n        DEBUG (4, {\n          fprintf (debug_file,\n                   \"     Make edge bb%lu->bb%lu a non-fall through, add new insn at the of bb%lu \",\n                   (unsigned long) e2->src->index, (unsigned long) e2->dst->index,\n                   (unsigned long) e2->src->index);\n          print_bb_insn (gen_ctx, (bb_insn_t) insn->data, FALSE);\n        });\n      } else {\n        MIR_insert_insn_after (ctx, curr_func_item, tail_insn, insn);\n        new_bb = create_bb (gen_ctx, insn);\n        insert_new_bb_after (gen_ctx, e2->src, new_bb);\n        DLIST_REMOVE (in_edge_t, e2->dst->in_edges, e2);\n        e2->dst = new_bb;\n        DLIST_APPEND (in_edge_t, new_bb->in_edges, e2);\n        create_edge (gen_ctx, new_bb, dst, FALSE, TRUE);\n        DEBUG (4, {\n          fprintf (debug_file,\n                   \"     creating bb%lu after bb%lu, redirect edge bb%lu->bb%lu to bb%lu\",\n                   (unsigned long) new_bb->index, (unsigned long) e2->src->index,\n                   (unsigned long) e2->src->index, (unsigned long) dst->index,\n                   (unsigned long) new_bb->index);\n          fprintf (debug_file, \", and create jump edge bb%lu->bb%lu:\\n\",\n                   (unsigned long) new_bb->index, (unsigned long) dst->index);\n          fprintf (debug_file, \"       new bb insn is \");\n          print_bb_insn (gen_ctx, (bb_insn_t) insn->data, FALSE);\n        });\n      }\n    }\n    /* add fall through new block before dst */\n    insn = MIR_new_label (ctx);\n    MIR_insert_insn_before (ctx, curr_func_item, first_insn, insn);\n    new_bb = create_bb (gen_ctx, insn);\n    insert_new_bb_before (gen_ctx, dst, new_bb);\n    DLIST_REMOVE (in_edge_t, dst->in_edges, e);\n    e->dst = new_bb;\n    DLIST_APPEND (in_edge_t, new_bb->in_edges, e);\n    create_edge (gen_ctx, new_bb, dst, TRUE, TRUE);\n    DEBUG (4, {\n      fprintf (debug_file, \"     creating bb%lu before bb%lu, redirect the edge to it\",\n               (unsigned long) new_bb->index, (unsigned long) dst->index);\n      fprintf (debug_file, \", and create fall-through edge bb%lu->bb%lu:\\n\",\n               (unsigned long) new_bb->index, (unsigned long) dst->index);\n      fprintf (debug_file, \"       new bb insn is \");\n      print_bb_insn (gen_ctx, (bb_insn_t) insn->data, FALSE);\n      fprintf (debug_file, \"       change src bb insn \");\n      print_bb_insn (gen_ctx, last_bb_insn, FALSE);\n    });\n    /* change label first_insn to label insn in src */\n    if (last_insn->code != MIR_SWITCH) {\n      gen_assert (last_insn->ops[0].mode == MIR_OP_LABEL\n                  && last_insn->ops[0].u.label == first_insn);\n      last_insn->ops[0] = MIR_new_label_op (ctx, insn);\n    } else {\n      for (i = 1; i < last_insn->nops; i++)\n        if (last_insn->ops[i].u.label == first_insn) break;\n      gen_assert (i < last_insn->nops);\n      last_insn->ops[i] = MIR_new_label_op (ctx, insn);\n    }\n    DEBUG (4, {\n      fprintf (debug_file, \"         to insn \");\n      print_bb_insn (gen_ctx, last_bb_insn, FALSE);\n    });\n  }\n  return new_bb;\n}\n\nstatic void DFS (bb_t bb, size_t *pre, size_t *rpost) {\n  edge_t e;\n\n  bb->pre = (*pre)++;\n  for (e = DLIST_HEAD (out_edge_t, bb->out_edges); e != NULL; e = DLIST_NEXT (out_edge_t, e))\n    if (e->dst->pre == 0)\n      DFS (e->dst, pre, rpost);\n    else if (e->dst->rpost == 0)\n      e->back_edge_p = TRUE;\n  bb->rpost = (*rpost)--;\n}\n\nstatic void enumerate_bbs (gen_ctx_t gen_ctx) {\n  size_t pre, rpost;\n\n  for (bb_t bb = DLIST_HEAD (bb_t, curr_cfg->bbs); bb != NULL; bb = DLIST_NEXT (bb_t, bb))\n    bb->pre = bb->rpost = 0;\n  pre = 1;\n  rpost = DLIST_LENGTH (bb_t, curr_cfg->bbs);\n  DFS (DLIST_HEAD (bb_t, curr_cfg->bbs), &pre, &rpost);\n}\n\nstatic loop_node_t top_loop_node (bb_t bb) {\n  for (loop_node_t loop_node = bb->loop_node;; loop_node = loop_node->parent)\n    if (loop_node->parent == NULL) return loop_node;\n}\n\nstatic loop_node_t create_loop_node (gen_ctx_t gen_ctx, bb_t bb) {\n  loop_node_t loop_node = gen_malloc (gen_ctx, sizeof (struct loop_node));\n\n  loop_node->index = curr_loop_node_index++;\n  loop_node->bb = bb;\n  if (bb != NULL) bb->loop_node = loop_node;\n  loop_node->parent = NULL;\n  loop_node->entry = NULL;\n  loop_node->u.preheader = NULL;\n  loop_node->max_int_pressure = loop_node->max_fp_pressure = 0;\n  DLIST_INIT (loop_node_t, loop_node->children);\n  return loop_node;\n}\n\nstatic int process_loop (gen_ctx_t gen_ctx, bb_t entry_bb) {\n  edge_t e;\n  loop_node_t loop_node, new_loop_node, queue_node;\n  bb_t queue_bb;\n\n  VARR_TRUNC (loop_node_t, loop_nodes, 0);\n  VARR_TRUNC (loop_node_t, queue_nodes, 0);\n  bitmap_clear (temp_bitmap);\n  for (e = DLIST_HEAD (in_edge_t, entry_bb->in_edges); e != NULL; e = DLIST_NEXT (in_edge_t, e))\n    if (e->back_edge_p && e->src != entry_bb) {\n      loop_node = top_loop_node (e->src);\n      if (!bitmap_set_bit_p (temp_bitmap, loop_node->index)\n          || (e->src->pre == 0 && e->src->rpost == 0))\n        continue; /* processed or unreachable */\n      VARR_PUSH (loop_node_t, loop_nodes, loop_node);\n      VARR_PUSH (loop_node_t, queue_nodes, loop_node);\n    }\n  while (VARR_LENGTH (loop_node_t, queue_nodes) != 0) {\n    queue_node = VARR_POP (loop_node_t, queue_nodes);\n    if ((queue_bb = queue_node->bb) == NULL) queue_bb = queue_node->entry->bb; /* subloop */\n    /* entry block is achieved which means multiple entry loop -- just ignore */\n    if (queue_bb == DLIST_HEAD (bb_t, curr_cfg->bbs)) return FALSE;\n    for (e = DLIST_HEAD (in_edge_t, queue_bb->in_edges); e != NULL; e = DLIST_NEXT (in_edge_t, e))\n      if (e->src != entry_bb) {\n        loop_node = top_loop_node (e->src);\n        if (!bitmap_set_bit_p (temp_bitmap, loop_node->index)\n            || (e->src->pre == 0 && e->src->rpost == 0))\n          continue; /* processed or unreachable */\n        VARR_PUSH (loop_node_t, loop_nodes, loop_node);\n        VARR_PUSH (loop_node_t, queue_nodes, loop_node);\n      }\n  }\n  loop_node = entry_bb->loop_node;\n  VARR_PUSH (loop_node_t, loop_nodes, loop_node);\n  new_loop_node = create_loop_node (gen_ctx, NULL);\n  new_loop_node->entry = loop_node;\n  while (VARR_LENGTH (loop_node_t, loop_nodes) != 0) {\n    loop_node = VARR_POP (loop_node_t, loop_nodes);\n    DLIST_APPEND (loop_node_t, new_loop_node->children, loop_node);\n    loop_node->parent = new_loop_node;\n  }\n  return TRUE;\n}\n\nstatic void setup_loop_pressure (gen_ctx_t gen_ctx, loop_node_t loop_node) {\n  for (loop_node_t curr = DLIST_HEAD (loop_node_t, loop_node->children); curr != NULL;\n       curr = DLIST_NEXT (loop_node_t, curr)) {\n    if (curr->bb == NULL) {\n      setup_loop_pressure (gen_ctx, curr);\n    } else {\n      curr->max_int_pressure = curr->bb->max_int_pressure;\n      curr->max_fp_pressure = curr->bb->max_fp_pressure;\n    }\n    if (loop_node->max_int_pressure < curr->max_int_pressure)\n      loop_node->max_int_pressure = curr->max_int_pressure;\n    if (loop_node->max_fp_pressure < curr->max_fp_pressure)\n      loop_node->max_fp_pressure = curr->max_fp_pressure;\n  }\n}\n\nstatic int compare_bb_loop_nodes (const void *p1, const void *p2) {\n  bb_t bb1 = (*(const loop_node_t *) p1)->bb, bb2 = (*(const loop_node_t *) p2)->bb;\n\n  return bb1->rpost > bb2->rpost ? -1 : bb1->rpost < bb2->rpost ? 1 : 0;\n}\n\nstatic int build_loop_tree (gen_ctx_t gen_ctx) {\n  loop_node_t loop_node;\n  edge_t e;\n  int loops_p = FALSE;\n\n  curr_loop_node_index = 0;\n  enumerate_bbs (gen_ctx);\n  VARR_TRUNC (loop_node_t, loop_entries, 0);\n  for (bb_t bb = DLIST_HEAD (bb_t, curr_cfg->bbs); bb != NULL; bb = DLIST_NEXT (bb_t, bb)) {\n    loop_node = create_loop_node (gen_ctx, bb);\n    loop_node->entry = loop_node;\n    for (e = DLIST_HEAD (in_edge_t, bb->in_edges); e != NULL; e = DLIST_NEXT (in_edge_t, e))\n      if (e->back_edge_p) {\n        VARR_PUSH (loop_node_t, loop_entries, loop_node);\n        break;\n      }\n  }\n  qsort (VARR_ADDR (loop_node_t, loop_entries), VARR_LENGTH (loop_node_t, loop_entries),\n         sizeof (loop_node_t), compare_bb_loop_nodes);\n  for (size_t i = 0; i < VARR_LENGTH (loop_node_t, loop_entries); i++)\n    if (process_loop (gen_ctx, VARR_GET (loop_node_t, loop_entries, i)->bb)) loops_p = TRUE;\n  curr_cfg->root_loop_node = create_loop_node (gen_ctx, NULL);\n  for (bb_t bb = DLIST_HEAD (bb_t, curr_cfg->bbs); bb != NULL; bb = DLIST_NEXT (bb_t, bb))\n    if ((loop_node = top_loop_node (bb)) != curr_cfg->root_loop_node) {\n      DLIST_APPEND (loop_node_t, curr_cfg->root_loop_node->children, loop_node);\n      loop_node->parent = curr_cfg->root_loop_node;\n    }\n  setup_loop_pressure (gen_ctx, curr_cfg->root_loop_node);\n  return loops_p;\n}\n\nstatic void destroy_loop_tree (gen_ctx_t gen_ctx, loop_node_t root) {\n  loop_node_t node, next;\n\n  if (root->bb != NULL) {\n    root->u.preheader_loop = NULL;\n    root->bb->loop_node = NULL;\n  } else {\n    for (node = DLIST_HEAD (loop_node_t, root->children); node != NULL; node = next) {\n      next = DLIST_NEXT (loop_node_t, node);\n      destroy_loop_tree (gen_ctx, node);\n    }\n  }\n  gen_free (gen_ctx, root);\n}\n\nstatic void update_max_var (gen_ctx_t gen_ctx, MIR_reg_t reg) {\n  if (reg == MIR_NON_VAR) return;\n  if (curr_cfg->max_var < reg) curr_cfg->max_var = reg;\n}\n\nstatic MIR_reg_t gen_new_temp_reg (gen_ctx_t gen_ctx, MIR_type_t type, MIR_func_t func) {\n  MIR_reg_t reg = _MIR_new_temp_reg (gen_ctx->ctx, type, func) + MAX_HARD_REG;\n\n  update_max_var (gen_ctx, reg);\n  return reg;\n}\n\nstatic MIR_reg_t get_max_var (gen_ctx_t gen_ctx) { return curr_cfg->max_var; }\n\nstatic int move_code_p (MIR_insn_code_t code) {\n  return code == MIR_MOV || code == MIR_FMOV || code == MIR_DMOV || code == MIR_LDMOV;\n}\n\nstatic int move_p (MIR_insn_t insn) {\n  return (move_code_p (insn->code) && insn->ops[0].mode == MIR_OP_VAR\n          && insn->ops[1].mode == MIR_OP_VAR && insn->ops[0].u.var > MAX_HARD_REG\n          && insn->ops[1].u.var > MAX_HARD_REG);\n}\n\nstatic int imm_move_p (MIR_insn_t insn) {\n  return (move_code_p (insn->code) && insn->ops[0].mode == MIR_OP_VAR\n          && insn->ops[0].u.var > MAX_HARD_REG\n          && (insn->ops[1].mode == MIR_OP_INT || insn->ops[1].mode == MIR_OP_UINT\n              || insn->ops[1].mode == MIR_OP_FLOAT || insn->ops[1].mode == MIR_OP_DOUBLE\n              || insn->ops[1].mode == MIR_OP_LDOUBLE || insn->ops[1].mode == MIR_OP_REF));\n}\n\ntypedef struct {\n  MIR_insn_t insn;\n  size_t nops, op_num, op_part_num;\n} insn_var_iterator_t;\n\nstatic inline void insn_var_iterator_init (insn_var_iterator_t *iter, MIR_insn_t insn) {\n  iter->insn = insn;\n  iter->nops = insn->nops;\n  iter->op_num = 0;\n  iter->op_part_num = 0;\n}\n\nstatic inline int input_insn_var_iterator_next (gen_ctx_t gen_ctx, insn_var_iterator_t *iter,\n                                                MIR_reg_t *var, int *op_num) {\n  MIR_op_t *op_ref;\n  int out_p;\n\n  while (iter->op_num < iter->nops) {\n    *op_num = (int) iter->op_num;\n    MIR_insn_op_mode (gen_ctx->ctx, iter->insn, iter->op_num, &out_p);\n    op_ref = &iter->insn->ops[iter->op_num];\n    if (out_p && op_ref->mode != MIR_OP_VAR_MEM) {\n      iter->op_num++;\n      continue;\n    }\n    while (iter->op_part_num < 2) {\n      if (op_ref->mode == MIR_OP_VAR_MEM) {\n        *var = iter->op_part_num == 0 ? op_ref->u.var_mem.base : op_ref->u.var_mem.index;\n        if (*var == MIR_NON_VAR) {\n          iter->op_part_num++;\n          continue;\n        }\n      } else if (iter->op_part_num > 0) {\n        break;\n      } else if (op_ref->mode == MIR_OP_VAR) {\n        *var = op_ref->u.var;\n      } else\n        break;\n      iter->op_part_num++;\n      return TRUE;\n    }\n    iter->op_num++;\n    iter->op_part_num = 0;\n  }\n  return FALSE;\n}\n\nstatic inline int output_insn_var_iterator_next (gen_ctx_t gen_ctx, insn_var_iterator_t *iter,\n                                                 MIR_reg_t *var, int *op_num) {\n  MIR_op_t *op_ref;\n  int out_p;\n\n  while (iter->op_num < iter->nops) {\n    *op_num = (int) iter->op_num;\n    MIR_insn_op_mode (gen_ctx->ctx, iter->insn, iter->op_num, &out_p);\n    op_ref = &iter->insn->ops[iter->op_num];\n    if (!out_p || op_ref->mode == MIR_OP_VAR_MEM) {\n      iter->op_num++;\n      continue;\n    }\n    gen_assert (op_ref->mode == MIR_OP_VAR);\n    *var = op_ref->u.var;\n    iter->op_num++;\n    return TRUE;\n  }\n  return FALSE;\n}\n\n#define FOREACH_IN_INSN_VAR(gen_ctx, iterator, insn, var, op_num) \\\n  for (insn_var_iterator_init (&iterator, insn);                  \\\n       input_insn_var_iterator_next (gen_ctx, &iterator, &var, &op_num);)\n\n#define FOREACH_OUT_INSN_VAR(gen_ctx, iterator, insn, var, op_num) \\\n  for (insn_var_iterator_init (&iterator, insn);                   \\\n       output_insn_var_iterator_next (gen_ctx, &iterator, &var, &op_num);)\n\n#if !MIR_NO_GEN_DEBUG\nstatic void output_in_edges (gen_ctx_t gen_ctx, bb_t bb) {\n  edge_t e;\n\n  fprintf (debug_file, \"  in edges:\");\n  for (e = DLIST_HEAD (in_edge_t, bb->in_edges); e != NULL; e = DLIST_NEXT (in_edge_t, e)) {\n    fprintf (debug_file, \" %3lu%s%s\", (unsigned long) e->src->index, e->fall_through_p ? \"f\" : \"\",\n             e->back_edge_p ? \"*\" : \"\");\n  }\n  fprintf (debug_file, \"\\n\");\n}\n\nstatic void output_out_edges (gen_ctx_t gen_ctx, bb_t bb) {\n  edge_t e;\n\n  fprintf (debug_file, \"  out edges:\");\n  for (e = DLIST_HEAD (out_edge_t, bb->out_edges); e != NULL; e = DLIST_NEXT (out_edge_t, e)) {\n    fprintf (debug_file, \" %3lu%s%s\", (unsigned long) e->dst->index, e->fall_through_p ? \"f\" : \"\",\n             e->back_edge_p ? \"*\" : \"\");\n  }\n  fprintf (debug_file, \"\\n\");\n}\n\n/* When print_name_p, treat as reg bitmap. */\nstatic void output_bitmap (gen_ctx_t gen_ctx, const char *head, bitmap_t bm, int print_name_p,\n                           MIR_reg_t *reg_map) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  MIR_reg_t reg;\n  size_t nel;\n  bitmap_iterator_t bi;\n\n  if (bm == NULL || bitmap_empty_p (bm)) return;\n  fprintf (debug_file, \"%s\", head);\n  FOREACH_BITMAP_BIT (bi, bm, nel) {\n    fprintf (debug_file, \" %3lu\", (unsigned long) nel);\n    if (print_name_p && (reg_map != NULL || nel > MAX_HARD_REG)) {\n      reg = (MIR_reg_t) nel;\n      if (reg_map != NULL) reg = reg_map[nel];\n      gen_assert (reg >= MAX_HARD_REG);\n      fprintf (debug_file, \"(%s:%s)\",\n               MIR_type_str (ctx, MIR_reg_type (ctx, reg - MAX_HARD_REG, curr_func_item->u.func)),\n               MIR_reg_name (ctx, reg - MAX_HARD_REG, curr_func_item->u.func));\n    }\n  }\n  fprintf (debug_file, \"\\n\");\n}\n\nstatic void print_insn (gen_ctx_t gen_ctx, MIR_insn_t insn, int newln_p) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  int flag;\n  MIR_op_t op;\n\n  MIR_output_insn (ctx, debug_file, insn, curr_func_item->u.func, FALSE);\n  for (size_t i = 0; i < insn->nops; i++) {\n    op = insn->ops[i];\n    if (op.mode == MIR_OP_VAR_MEM && op.u.var_mem.nloc != 0) {\n      flag = VARR_GET (mem_attr_t, mem_attrs, op.u.var_mem.nloc).alloca_flag;\n      fprintf (debug_file, \" # m%lu%s\", (unsigned long) op.u.var_mem.nloc,\n               !flag                               ? \"\"\n               : flag & (MAY_ALLOCA | MUST_ALLOCA) ? \"au\"\n               : flag & MAY_ALLOCA                 ? \"a\"\n                                                   : \"u\");\n    }\n  }\n  if (newln_p) fprintf (debug_file, \"\\n\");\n}\n\nstatic void print_op_data (gen_ctx_t gen_ctx, void *op_data, bb_insn_t from);\nstatic void print_bb_insn (gen_ctx_t gen_ctx, bb_insn_t bb_insn, int with_notes_p) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  MIR_op_t op;\n  int first_p;\n  size_t nel;\n  bitmap_iterator_t bi;\n\n  print_insn (gen_ctx, bb_insn->insn, FALSE);\n  fprintf (debug_file, \" # indexes: \");\n  for (size_t i = 0; i < bb_insn->insn->nops; i++) {\n    if (i != 0) fprintf (debug_file, \",\");\n    print_op_data (gen_ctx, bb_insn->insn->ops[i].data, bb_insn);\n  }\n  if (with_notes_p) {\n    for (dead_var_t dv = DLIST_HEAD (dead_var_t, bb_insn->insn_dead_vars); dv != NULL;\n         dv = DLIST_NEXT (dead_var_t, dv)) {\n      op.mode = MIR_OP_VAR;\n      op.u.var = dv->var;\n      fprintf (debug_file,\n               dv == DLIST_HEAD (dead_var_t, bb_insn->insn_dead_vars) ? \" # dead: \" : \" \");\n      MIR_output_op (ctx, debug_file, op, curr_func_item->u.func);\n    }\n    if (MIR_call_code_p (bb_insn->insn->code)) {\n      first_p = TRUE;\n      FOREACH_BITMAP_BIT (bi, bb_insn->call_hard_reg_args, nel) {\n        fprintf (debug_file, first_p ? \" # call used: hr%ld\" : \" hr%ld\", (unsigned long) nel);\n        first_p = FALSE;\n      }\n    }\n  }\n  fprintf (debug_file, \"\\n\");\n}\n\nstatic void print_CFG (gen_ctx_t gen_ctx, int bb_p, int pressure_p, int insns_p, int insn_index_p,\n                       void (*bb_info_print_func) (gen_ctx_t, bb_t)) {\n  bb_t bb, insn_bb;\n\n  if (optimize_level == 0) {\n    bb = NULL;\n    for (MIR_insn_t insn = DLIST_HEAD (MIR_insn_t, curr_func_item->u.func->insns); insn != NULL;\n         insn = DLIST_NEXT (MIR_insn_t, insn)) {\n      if (bb_p && (insn_bb = get_insn_data_bb (insn)) != bb) {\n        bb = insn_bb;\n        fprintf (debug_file, \"BB %3lu:\\n\", (unsigned long) bb->index);\n        output_in_edges (gen_ctx, bb);\n        output_out_edges (gen_ctx, bb);\n        if (bb_info_print_func != NULL) {\n          bb_info_print_func (gen_ctx, bb);\n          fprintf (debug_file, \"\\n\");\n        }\n      }\n      if (insns_p) MIR_output_insn (gen_ctx->ctx, debug_file, insn, curr_func_item->u.func, TRUE);\n    }\n    return;\n  }\n  for (bb = DLIST_HEAD (bb_t, curr_cfg->bbs); bb != NULL; bb = DLIST_NEXT (bb_t, bb)) {\n    if (bb_p) {\n      fprintf (debug_file, \"BB %3lu\", (unsigned long) bb->index);\n      if (pressure_p)\n        fprintf (debug_file, \" (pressure: int=%d, fp=%d)\", bb->max_int_pressure,\n                 bb->max_fp_pressure);\n      if (bb->loop_node == NULL)\n        fprintf (debug_file, \"\\n\");\n      else\n        fprintf (debug_file, \" (loop%3lu):\\n\", (unsigned long) bb->loop_node->parent->index);\n      output_in_edges (gen_ctx, bb);\n      output_out_edges (gen_ctx, bb);\n      if (bb_info_print_func != NULL) {\n        bb_info_print_func (gen_ctx, bb);\n        fprintf (debug_file, \"\\n\");\n      }\n    }\n    if (insns_p) {\n      for (bb_insn_t bb_insn = DLIST_HEAD (bb_insn_t, bb->bb_insns); bb_insn != NULL;\n           bb_insn = DLIST_NEXT (bb_insn_t, bb_insn)) {\n        if (insn_index_p) fprintf (debug_file, \"  %-5lu\", (unsigned long) bb_insn->index);\n        print_bb_insn (gen_ctx, bb_insn, TRUE);\n      }\n      fprintf (debug_file, \"\\n\");\n    }\n  }\n}\n\nstatic void print_varr_insns (gen_ctx_t gen_ctx, const char *title, VARR (bb_insn_t) * bb_insns) {\n  fprintf (debug_file, \"%s insns:\\n\", title);\n  for (size_t i = 0; i < VARR_LENGTH (bb_insn_t, bb_insns); i++) {\n    bb_insn_t bb_insn = VARR_GET (bb_insn_t, bb_insns, i);\n    if (bb_insn == NULL) continue;\n    fprintf (debug_file, \"  %-5lu\", (unsigned long) bb_insn->index);\n    print_bb_insn (gen_ctx, bb_insn, TRUE);\n  }\n}\n\nstatic void print_loop_subtree (gen_ctx_t gen_ctx, loop_node_t root, int level, int bb_p) {\n  if (root->bb != NULL && !bb_p) return;\n  for (int i = 0; i < 2 * level + 2; i++) fprintf (debug_file, \" \");\n  if (root->bb != NULL) {\n    gen_assert (DLIST_HEAD (loop_node_t, root->children) == NULL);\n    fprintf (debug_file, \"BB%-3lu (pressure: int=%d, fp=%d)\", (unsigned long) root->bb->index,\n             root->max_int_pressure, root->max_fp_pressure);\n    if (root->bb != NULL && root->u.preheader_loop != NULL)\n      fprintf (debug_file, \" (loop of the preheader - loop%lu)\",\n               (unsigned long) root->u.preheader_loop->index);\n    fprintf (debug_file, \"\\n\");\n    return;\n  }\n  fprintf (debug_file, \"Loop%3lu (pressure: int=%d, fp=%d)\", (unsigned long) root->index,\n           root->max_int_pressure, root->max_fp_pressure);\n  if (curr_cfg->root_loop_node == root || root->entry == NULL)\n    fprintf (debug_file, \":\\n\");\n  else {\n    if (root->bb == NULL && root->u.preheader != NULL)\n      fprintf (debug_file, \" (preheader - bb%lu)\", (unsigned long) root->u.preheader->bb->index);\n    fprintf (debug_file, \" (entry - bb%lu):\\n\", (unsigned long) root->entry->bb->index);\n  }\n  for (loop_node_t node = DLIST_HEAD (loop_node_t, root->children); node != NULL;\n       node = DLIST_NEXT (loop_node_t, node))\n    print_loop_subtree (gen_ctx, node, level + 1, bb_p);\n}\n\nstatic void print_loop_tree (gen_ctx_t gen_ctx, int bb_p) {\n  if (curr_cfg->root_loop_node == NULL) return;\n  fprintf (debug_file, \"Loop Tree\\n\");\n  print_loop_subtree (gen_ctx, curr_cfg->root_loop_node, 0, bb_p);\n}\n\n#endif\n\nstatic void rename_op_reg (gen_ctx_t gen_ctx, MIR_op_t *op_ref, MIR_reg_t reg, MIR_reg_t new_reg,\n                           MIR_insn_t insn, int print_p) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  int change_p = FALSE;\n\n  gen_assert (reg > MAX_HARD_REG);\n  if (op_ref->mode == MIR_OP_VAR && op_ref->u.var == reg) {\n    op_ref->u.var = new_reg;\n    change_p = TRUE;\n  } else if (op_ref->mode == MIR_OP_VAR_MEM) {\n    if (op_ref->u.var_mem.base == reg) {\n      op_ref->u.var_mem.base = new_reg;\n      change_p = TRUE;\n    }\n    if (op_ref->u.var_mem.index == reg) {\n      op_ref->u.var_mem.index = new_reg;\n      change_p = TRUE;\n    }\n  }\n  if (!change_p || !print_p) return; /* definition was already changed from another use */\n  DEBUG (2, {\n    MIR_func_t func = curr_func_item->u.func;\n\n    fprintf (debug_file, \"    Change %s to %s in insn %-5lu\",\n             MIR_reg_name (ctx, reg - MAX_HARD_REG, func),\n             MIR_reg_name (ctx, new_reg - MAX_HARD_REG, func),\n             (long unsigned) ((bb_insn_t) insn->data)->index);\n    print_bb_insn (gen_ctx, insn->data, FALSE);\n  });\n}\n\nstatic void update_tied_regs (gen_ctx_t gen_ctx, MIR_reg_t reg) {\n  gen_assert (reg > MAX_HARD_REG);\n  if (reg == MIR_NON_VAR\n      || MIR_reg_hard_reg_name (gen_ctx->ctx, reg - MAX_HARD_REG, curr_func_item->u.func) == NULL)\n    return;\n  bitmap_set_bit_p (tied_regs, reg);\n}\n\nstatic long remove_unreachable_bbs (gen_ctx_t gen_ctx);\n\nstatic void MIR_UNUSED new_temp_op (gen_ctx_t gen_ctx, MIR_op_t *temp_op) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  *temp_op = MIR_new_reg_op (ctx, _MIR_new_temp_reg (ctx, MIR_T_I64, curr_func_item->u.func));\n}\n\nstatic MIR_insn_t MIR_UNUSED find_bo (gen_ctx_t gen_ctx MIR_UNUSED, MIR_insn_t insn) {\n  for (; insn != NULL; insn = DLIST_NEXT (MIR_insn_t, insn))\n    if (insn->code == MIR_BO || insn->code == MIR_BNO || insn->code == MIR_UBO\n        || insn->code == MIR_UBNO)\n      return insn;\n  gen_assert (FALSE);\n  return NULL;\n}\n\nstatic int label_cmp (const void *l1, const void *l2) {\n  const MIR_insn_t lab1 = *(const MIR_insn_t *) l1, lab2 = *(const MIR_insn_t *) l2;\n  gen_assert (lab1->code == MIR_LABEL && lab2->code == MIR_LABEL);\n  return lab1 < lab2 ? -1 : lab1 == lab2 ? 0 : 1;\n}\n\nstatic void build_func_cfg (gen_ctx_t gen_ctx) {\n  MIR_alloc_t alloc = gen_alloc (gen_ctx);\n  MIR_context_t ctx = gen_ctx->ctx;\n  MIR_func_t func = curr_func_item->u.func;\n  MIR_insn_t insn, insn2, next_insn, ret_insn, use_insn;\n  MIR_insn_t new_insn MIR_UNUSED, new_label MIR_UNUSED, bo_insn MIR_UNUSED;\n  size_t i, j, nops;\n  MIR_op_t *op, temp_op1 MIR_UNUSED, temp_op2 MIR_UNUSED, temp_op3 MIR_UNUSED;\n  MIR_reg_t reg;\n  MIR_var_t mir_var;\n  bb_t bb, bb2, prev_bb, entry_bb, exit_bb, label_bb;\n\n  DLIST_INIT (bb_t, curr_cfg->bbs);\n  curr_cfg->curr_bb_insn_index = 0;\n  curr_cfg->max_var = MAX_HARD_REG;\n  curr_cfg->root_loop_node = NULL;\n  curr_bb_index = 0;\n  for (i = 0; i < VARR_LENGTH (MIR_var_t, func->vars); i++) {\n    mir_var = VARR_GET (MIR_var_t, func->vars, i);\n    update_max_var (gen_ctx, MIR_reg (ctx, mir_var.name, func) + MAX_HARD_REG);\n  }\n  entry_bb = create_bb (gen_ctx, NULL);\n  add_new_bb (gen_ctx, entry_bb);\n  exit_bb = create_bb (gen_ctx, NULL);\n  add_new_bb (gen_ctx, exit_bb);\n  /* To deal with special cases like adding insns before call in\n     machinize or moving invariant out of loop: */\n  MIR_prepend_insn (ctx, curr_func_item, MIR_new_label (ctx));\n  bb = create_bb (gen_ctx, NULL);\n  add_new_bb (gen_ctx, bb);\n  bitmap_clear (tied_regs);\n  bitmap_clear (addr_regs);\n  addr_insn_p = FALSE;\n  VARR_TRUNC (MIR_insn_t, temp_insns, 0);\n  VARR_TRUNC (MIR_insn_t, temp_insns2, 0);\n  for (ret_insn = NULL, insn = DLIST_HEAD (MIR_insn_t, func->insns); insn != NULL;\n       insn = next_insn) {\n    next_insn = DLIST_NEXT (MIR_insn_t, insn);\n    if (MIR_addr_code_p (insn->code)) {\n      addr_insn_p = TRUE;\n      bitmap_set_bit_p (addr_regs, insn->ops[1].u.reg + MAX_HARD_REG);\n    } else if (insn->code == MIR_RET) {\n      if (ret_insn != NULL) { /* we should have only one ret insn before generator */\n        gen_assert (ret_insn == insn);\n      } else if (func->global_vars != NULL) {\n        VARR_TRUNC (MIR_op_t, temp_ops, 0);\n        for (i = 0; i < VARR_LENGTH (MIR_var_t, func->global_vars); i++) {\n          reg = MIR_reg (ctx, VARR_GET (MIR_var_t, func->global_vars, i).name, func) + MAX_HARD_REG;\n          VARR_PUSH (MIR_op_t, temp_ops, _MIR_new_var_op (ctx, reg));\n        }\n        use_insn = MIR_new_insn_arr (ctx, MIR_USE, VARR_LENGTH (MIR_op_t, temp_ops),\n                                     VARR_ADDR (MIR_op_t, temp_ops));\n        MIR_insert_insn_before (ctx, curr_func_item, insn, use_insn);\n        next_insn = use_insn;\n        ret_insn = insn;\n        continue;\n      }\n    } else if (MIR_call_code_p (insn->code)) {\n      bb->call_p = TRUE;\n    } else {\n      switch (insn->code) { /* ??? should we copy result change before insn and bo */\n#if defined(TARGET_EXPAND_ADDOS) || defined(TARGET_EXPAND_UADDOS)\n      case MIR_ADDOS:\n      case MIR_SUBOS: bo_insn = find_bo (gen_ctx, insn);\n#ifndef TARGET_EXPAND_UADDOS\n        if (bo_insn->code == MIR_UBO || bo_insn->code == MIR_UBNO) break;\n#endif\n#ifndef TARGET_EXPAND_ADDOS\n        if (bo_insn->code == MIR_BO || bo_insn->code == MIR_BNO) break;\n#endif\n        insn->code = insn->code == MIR_ADDO ? MIR_ADDS : MIR_SUBS;\n        new_temp_op (gen_ctx, &temp_op1);\n        if (bo_insn->code == MIR_UBO || bo_insn->code == MIR_UBNO) {\n          /* t1=a1;adds r,t1,a2; ublts r,t1,ov_label or t1=a1;subs r,t1,a2; ublts t1,res,ov_label */\n          next_insn = new_insn = MIR_new_insn (ctx, MIR_MOV, temp_op1, insn->ops[1]);\n          MIR_insert_insn_before (ctx, curr_func_item, insn, new_insn);\n          insn->ops[1] = temp_op1;\n          new_insn\n            = MIR_new_insn (ctx, bo_insn->code == MIR_UBO ? MIR_UBLTS : MIR_UBGES, bo_insn->ops[0],\n                            insn->code == MIR_ADDS ? insn->ops[0] : temp_op1,\n                            insn->code == MIR_ADDS ? temp_op1 : insn->ops[0]);\n          MIR_insert_insn_before (ctx, curr_func_item, bo_insn, new_insn);\n        } else {\n          /* ext32 t1,a1; ext32 t2,a2; (adds|subs) r,a1,a2; (add|sub) t1,t1,t2; ext32 t2,r;\n             bne t1,t2,ov_lab */\n          new_temp_op (gen_ctx, &temp_op2);\n          next_insn = new_insn = MIR_new_insn (ctx, MIR_EXT32, temp_op1, insn->ops[1]);\n          MIR_insert_insn_before (ctx, curr_func_item, insn, new_insn);\n          new_insn = MIR_new_insn (ctx, MIR_EXT32, temp_op2, insn->ops[2]);\n          MIR_insert_insn_before (ctx, curr_func_item, insn, new_insn);\n          new_insn = MIR_new_insn (ctx, insn->code == MIR_ADDS ? MIR_ADD : MIR_SUB, temp_op1,\n                                   temp_op1, temp_op2);\n          MIR_insert_insn_before (ctx, curr_func_item, bo_insn, new_insn);\n          new_insn = MIR_new_insn (ctx, MIR_EXT32, temp_op2, insn->ops[0]);\n          MIR_insert_insn_before (ctx, curr_func_item, bo_insn, new_insn);\n          new_insn = MIR_new_insn (ctx, bo_insn->code == MIR_BO ? MIR_BNE : MIR_BEQ,\n                                   bo_insn->ops[0], temp_op1, temp_op2);\n          MIR_insert_insn_before (ctx, curr_func_item, bo_insn, new_insn);\n        }\n        MIR_remove_insn (gen_ctx->ctx, curr_func_item, bo_insn);\n        continue;\n#endif\n#if defined(TARGET_EXPAND_ADDO) || defined(TARGET_EXPAND_UADDO)\n      case MIR_ADDO:\n      case MIR_SUBO: bo_insn = find_bo (gen_ctx, insn);\n#ifndef TARGET_EXPAND_UADDO\n        if (bo_insn->code == MIR_UBO || bo_insn->code == MIR_UBNO) break;\n#endif\n#ifndef TARGET_EXPAND_ADDO\n        if (bo_insn->code == MIR_BO || bo_insn->code == MIR_BNO) break;\n#endif\n        insn->code = insn->code == MIR_ADDO ? MIR_ADD : MIR_SUB;\n        if (bo_insn->code == MIR_UBO || bo_insn->code == MIR_UBNO) {\n          /* t1=a1;add r,t1,a2; ublt r,t1,ov_label or t1=a1;sub r,t1,a2; ublt t1,r,ov_lab */\n          next_insn = new_insn = MIR_new_insn (ctx, MIR_MOV, temp_op1, insn->ops[1]);\n          MIR_insert_insn_before (ctx, curr_func_item, insn, new_insn);\n          insn->ops[1] = temp_op1;\n          new_insn = MIR_new_insn (ctx, bo_insn->code == MIR_UBO ? MIR_UBLT : MIR_UBGE,\n                                   bo_insn->ops[0], insn->code == MIR_ADD ? insn->ops[0] : temp_op1,\n                                   insn->code == MIR_ADD ? temp_op1 : insn->ops[0]);\n          MIR_insert_insn_before (ctx, curr_func_item, bo_insn, new_insn);\n        } else {\n          /* t1=a1;t2=a2;(add|sub) r,t1,t2;(lt t1,r,t1|lt t1,t1,r1);lt t2,t2,0;bne t2,t1,ov_lab */\n          new_temp_op (gen_ctx, &temp_op1);\n          new_temp_op (gen_ctx, &temp_op2);\n          new_temp_op (gen_ctx, &temp_op3);\n          next_insn = new_insn = MIR_new_insn (ctx, MIR_MOV, temp_op1, insn->ops[1]);\n          MIR_insert_insn_before (ctx, curr_func_item, insn, new_insn);\n          new_insn = MIR_new_insn (ctx, MIR_MOV, temp_op2, insn->ops[2]);\n          MIR_insert_insn_before (ctx, curr_func_item, insn, new_insn);\n          if (insn->code == MIR_ADDO)\n            new_insn = MIR_new_insn (ctx, MIR_LT, temp_op1, insn->ops[0], temp_op1);\n          else\n            new_insn = MIR_new_insn (ctx, MIR_LT, temp_op1, temp_op1, insn->ops[0]);\n          MIR_insert_insn_before (ctx, curr_func_item, bo_insn, new_insn);\n          new_insn = MIR_new_insn (ctx, MIR_MOV, temp_op3, MIR_new_int_op (ctx, 0));\n          MIR_insert_insn_before (ctx, curr_func_item, bo_insn, new_insn);\n          new_insn = MIR_new_insn (ctx, MIR_LT, temp_op2, temp_op2, temp_op3);\n          MIR_insert_insn_before (ctx, curr_func_item, bo_insn, new_insn);\n          new_insn = MIR_new_insn (ctx, bo_insn->code == MIR_BO ? MIR_BNE : MIR_BEQ,\n                                   bo_insn->ops[0], temp_op1, temp_op2);\n          MIR_insert_insn_before (ctx, curr_func_item, bo_insn, new_insn);\n        }\n        MIR_remove_insn (gen_ctx->ctx, curr_func_item, bo_insn);\n        continue;\n#endif\n#if defined(TARGET_EXPAND_MULOS) || defined(TARGET_EXPAND_UMULOS)\n      case MIR_MULOS:\n      case MIR_UMULOS:\n        /* [u]ext32 t1,a1; [u]ext32 t2,a2;[u]mul t1,t1,t2; [u]ext32 r,t1;..; b(ne|eq) lab,t1,r */\n        bo_insn = find_bo (gen_ctx, insn);\n#ifndef TARGET_EXPAND_UMULOS\n        if (bo_insn->code == MIR_UBO || bo_insn->code == MIR_UBNO) break;\n#endif\n#ifndef TARGET_EXPAND_MULOS\n        if (bo_insn->code == MIR_BO || bo_insn->code == MIR_BNO) break;\n#endif\n        new_temp_op (gen_ctx, &temp_op1);\n        new_temp_op (gen_ctx, &temp_op2);\n        MIR_insn_code_t ext_code = insn->code == MIR_MULOS ? MIR_EXT32 : MIR_UEXT32;\n        next_insn = new_insn = MIR_new_insn (ctx, ext_code, temp_op1, insn->ops[1]);\n        MIR_insert_insn_before (ctx, curr_func_item, insn, new_insn);\n        new_insn = MIR_new_insn (ctx, ext_code, temp_op2, insn->ops[2]);\n        MIR_insert_insn_before (ctx, curr_func_item, insn, new_insn);\n        new_insn = MIR_new_insn (ctx, ext_code, insn->ops[0], temp_op1);\n        MIR_insert_insn_after (ctx, curr_func_item, insn, new_insn);\n        insn->code = MIR_MUL;\n        insn->ops[0] = temp_op1;\n        insn->ops[1] = temp_op1;\n        insn->ops[2] = temp_op2;\n        new_insn\n          = MIR_new_insn (ctx,\n                          bo_insn->code == MIR_BO || bo_insn->code == MIR_UBO ? MIR_BNE : MIR_BEQ,\n                          bo_insn->ops[0], new_insn->ops[0], new_insn->ops[1]);\n        MIR_insert_insn_after (ctx, curr_func_item, bo_insn, new_insn);\n        MIR_remove_insn (gen_ctx->ctx, curr_func_item, bo_insn);\n        continue;\n#endif\n#if defined(TARGET_EXPAND_MULO) || defined(TARGET_EXPAND_UMULO)\n      case MIR_MULO:\n      case MIR_UMULO:\n        /* t1=a1;t2=t2;mul r,t1,t2;...; [u]bno: bf lab,t1;[u]div t1,r,t1;bne lab,t,t2\n           [u]bo: bf new_lab,t1;[u]div t1,r,t1;bne lab,t,t2;new_lab: */\n        bo_insn = find_bo (gen_ctx, insn);\n#ifndef TARGET_EXPAND_UMULO\n        if (bo_insn->code == MIR_UBO || bo_insn->code == MIR_UBNO) break;\n#endif\n#ifndef TARGET_EXPAND_MULO\n        if (bo_insn->code == MIR_BO || bo_insn->code == MIR_BNO) break;\n#endif\n        new_temp_op (gen_ctx, &temp_op1);\n        new_temp_op (gen_ctx, &temp_op2);\n        next_insn = new_insn = MIR_new_insn (ctx, MIR_MOV, temp_op1, insn->ops[1]);\n        MIR_insert_insn_before (ctx, curr_func_item, insn, new_insn);\n        new_insn = MIR_new_insn (ctx, MIR_MOV, temp_op2, insn->ops[2]);\n        MIR_insert_insn_before (ctx, curr_func_item, insn, new_insn);\n        insn->code = MIR_MUL;\n        insn->ops[1] = temp_op1;\n        insn->ops[2] = temp_op2;\n        if (bo_insn->code == MIR_BNO || bo_insn->code == MIR_UBNO) {\n          new_insn = MIR_new_insn (ctx, MIR_BF, bo_insn->ops[0], temp_op1);\n        } else {\n          new_label = MIR_new_label (ctx);\n          new_insn = MIR_new_insn (ctx, MIR_BF, MIR_new_label_op (ctx, new_label), temp_op1);\n        }\n        MIR_insert_insn_before (ctx, curr_func_item, bo_insn, new_insn);\n        new_insn\n          = MIR_new_insn (ctx,\n                          bo_insn->code == MIR_BO || bo_insn->code == MIR_BNO ? MIR_DIV : MIR_UDIV,\n                          temp_op1, insn->ops[0], temp_op1);\n        MIR_insert_insn_before (ctx, curr_func_item, bo_insn, new_insn);\n        new_insn\n          = MIR_new_insn (ctx,\n                          bo_insn->code == MIR_BO || bo_insn->code == MIR_UBO ? MIR_BNE : MIR_BEQ,\n                          bo_insn->ops[0], temp_op1, temp_op2);\n        MIR_insert_insn_before (ctx, curr_func_item, bo_insn, new_insn);\n        if (bo_insn->code == MIR_BO || bo_insn->code == MIR_UBO)\n          MIR_insert_insn_before (ctx, curr_func_item, bo_insn, new_label);\n        MIR_remove_insn (gen_ctx->ctx, curr_func_item, bo_insn);\n        continue;\n#endif\n      default: break;\n      }\n    }\n    if (insn->data == NULL) {\n      if (optimize_level != 0)\n        add_new_bb_insn (gen_ctx, insn, bb, TRUE);\n      else\n        setup_insn_data (gen_ctx, insn, bb);\n    }\n    if (insn->code == MIR_LADDR) {\n      gen_assert (insn->ops[1].mode == MIR_OP_LABEL);\n      VARR_PUSH (MIR_insn_t, temp_insns2, insn->ops[1].u.label);\n    } else if (insn->code == MIR_JMPI) {\n      VARR_PUSH (MIR_insn_t, temp_insns, insn);\n    }\n    nops = MIR_insn_nops (ctx, insn);\n    if (next_insn != NULL\n        && (MIR_any_branch_code_p (insn->code) || insn->code == MIR_RET || insn->code == MIR_JRET\n            || insn->code == MIR_PRBEQ || insn->code == MIR_PRBNE\n            || next_insn->code == MIR_LABEL)) {\n      prev_bb = bb;\n      if (next_insn->code == MIR_LABEL && next_insn->data != NULL)\n        bb = get_insn_bb (gen_ctx, next_insn);\n      else\n        bb = create_bb (gen_ctx, next_insn);\n      add_new_bb (gen_ctx, bb);\n      if (insn->code != MIR_JMP && insn->code != MIR_JMPI && insn->code != MIR_RET\n          && insn->code != MIR_JRET && insn->code != MIR_SWITCH)\n        create_edge (gen_ctx, prev_bb, bb, TRUE, TRUE); /* fall through */\n    }\n    for (i = 0; i < nops; i++) { /* Transform all ops to var ops */\n      op = &insn->ops[i];\n      if (op->mode == MIR_OP_REG) {\n        op->mode = MIR_OP_VAR;\n        op->u.var = op->u.reg == 0 ? MIR_NON_VAR : op->u.reg + MAX_HARD_REG;\n      } else if (op->mode == MIR_OP_MEM) {\n        op->mode = MIR_OP_VAR_MEM;\n        op->u.var_mem.base = op->u.mem.base == 0 ? MIR_NON_VAR : op->u.mem.base + MAX_HARD_REG;\n        op->u.var_mem.index = op->u.mem.index == 0 ? MIR_NON_VAR : op->u.mem.index + MAX_HARD_REG;\n      }\n      if (op->mode == MIR_OP_LABEL) {\n        if (op->u.label->data == NULL) create_bb (gen_ctx, op->u.label);\n        if (insn->code != MIR_LADDR) {\n          label_bb = get_insn_bb (gen_ctx, op->u.label);\n          create_edge (gen_ctx, get_insn_bb (gen_ctx, insn), label_bb, FALSE, TRUE);\n        }\n      } else if (op->mode == MIR_OP_VAR) {\n        update_max_var (gen_ctx, op->u.var);\n        update_tied_regs (gen_ctx, op->u.var);\n      } else if (op->mode == MIR_OP_VAR_MEM) {\n        update_max_var (gen_ctx, op->u.var_mem.base);\n        update_tied_regs (gen_ctx, op->u.var_mem.base);\n        update_max_var (gen_ctx, op->u.var_mem.index);\n        update_tied_regs (gen_ctx, op->u.var_mem.index);\n      }\n    }\n  }\n  for (MIR_lref_data_t lref = func->first_lref; lref != NULL; lref = lref->next) {\n    VARR_PUSH (MIR_insn_t, temp_insns2, lref->label);\n    if (lref->label2 != NULL)\n      VARR_PUSH (MIR_insn_t, temp_insns2, lref->label2);\n  }\n  qsort (VARR_ADDR (MIR_insn_t, temp_insns2), VARR_LENGTH (MIR_insn_t, temp_insns2),\n         sizeof (MIR_insn_t), label_cmp);\n  for (i = 0; i < VARR_LENGTH (MIR_insn_t, temp_insns); i++) {\n    insn = VARR_GET (MIR_insn_t, temp_insns, i);\n    gen_assert (insn->code == MIR_JMPI);\n    bb = get_insn_bb (gen_ctx, insn);\n    MIR_insn_t prev_label = NULL;\n    for (j = 0; j < VARR_LENGTH (MIR_insn_t, temp_insns2); j++) {\n      insn2 = VARR_GET (MIR_insn_t, temp_insns2, j);\n      if (insn2 == prev_label) continue;\n      gen_assert (insn2->code == MIR_LABEL);\n      prev_label = insn2;\n      bb2 = get_insn_bb (gen_ctx, insn2);\n      create_edge (gen_ctx, bb, bb2, FALSE, TRUE);\n    }\n  }\n  for (i = 0; i < VARR_LENGTH (MIR_insn_t, temp_insns2); i++) {\n    insn = VARR_GET (MIR_insn_t, temp_insns2, i);\n    gen_assert (insn->code == MIR_LABEL);\n    bb = get_insn_bb (gen_ctx, insn);\n    bb->reachable_p = TRUE;\n  }\n  if (optimize_level > 0) remove_unreachable_bbs (gen_ctx);\n  /* Add additional edges with entry and exit */\n  for (bb = DLIST_HEAD (bb_t, curr_cfg->bbs); bb != NULL; bb = DLIST_NEXT (bb_t, bb)) {\n    if (bb != entry_bb && DLIST_HEAD (in_edge_t, bb->in_edges) == NULL)\n      create_edge (gen_ctx, entry_bb, bb, FALSE, TRUE);\n    if (bb != exit_bb && DLIST_HEAD (out_edge_t, bb->out_edges) == NULL)\n      create_edge (gen_ctx, bb, exit_bb, FALSE, TRUE);\n  }\n  enumerate_bbs (gen_ctx);\n  VARR_CREATE (reg_info_t, curr_cfg->reg_info, alloc, 128);\n  curr_cfg->call_crossed_regs = bitmap_create2 (alloc, curr_cfg->max_var);\n}\n\nstatic void destroy_func_cfg (gen_ctx_t gen_ctx) {\n  MIR_insn_t insn;\n  bb_insn_t bb_insn;\n  bb_t bb, next_bb;\n\n  gen_assert (curr_func_item->item_type == MIR_func_item && curr_func_item->data != NULL);\n  for (insn = DLIST_HEAD (MIR_insn_t, curr_func_item->u.func->insns); insn != NULL;\n       insn = DLIST_NEXT (MIR_insn_t, insn))\n    if (optimize_level == 0) {\n      gen_assert (insn->data != NULL);\n      delete_insn_data (gen_ctx, insn);\n    } else {\n      bb_insn = insn->data;\n      gen_assert (bb_insn != NULL);\n      delete_bb_insn (gen_ctx, bb_insn);\n    }\n  for (bb = DLIST_HEAD (bb_t, curr_cfg->bbs); bb != NULL; bb = next_bb) {\n    next_bb = DLIST_NEXT (bb_t, bb);\n    delete_bb (gen_ctx, bb);\n  }\n  VARR_DESTROY (reg_info_t, curr_cfg->reg_info);\n  bitmap_destroy (curr_cfg->call_crossed_regs);\n  gen_free (gen_ctx, curr_func_item->data);\n  curr_func_item->data = NULL;\n}\n\nstatic int rpost_cmp (const void *a1, const void *a2) {\n  return (int) ((*(const struct bb **) a1)->rpost - (*(const struct bb **) a2)->rpost);\n}\n\nstatic int post_cmp (const void *a1, const void *a2) { return -rpost_cmp (a1, a2); }\n\nDEF_VARR (bb_t);\n\nstruct data_flow_ctx {\n  VARR (bb_t) * worklist, *pending;\n  bitmap_t bb_to_consider;\n};\n\n#define worklist gen_ctx->data_flow_ctx->worklist\n#define pending gen_ctx->data_flow_ctx->pending\n#define bb_to_consider gen_ctx->data_flow_ctx->bb_to_consider\n\nstatic void solve_dataflow (gen_ctx_t gen_ctx, int forward_p, void (*con_func_0) (bb_t),\n                            int (*con_func_n) (gen_ctx_t, bb_t),\n                            int (*trans_func) (gen_ctx_t, bb_t)) {\n  size_t i, iter;\n  bb_t bb, *addr;\n  VARR (bb_t) * t;\n\n  VARR_TRUNC (bb_t, worklist, 0);\n  for (bb = DLIST_HEAD (bb_t, curr_cfg->bbs); bb != NULL; bb = DLIST_NEXT (bb_t, bb))\n    VARR_PUSH (bb_t, worklist, bb);\n  VARR_TRUNC (bb_t, pending, 0);\n  iter = 0;\n  while (VARR_LENGTH (bb_t, worklist) != 0) {\n    VARR_TRUNC (bb_t, pending, 0);\n    addr = VARR_ADDR (bb_t, worklist);\n    qsort (addr, VARR_LENGTH (bb_t, worklist), sizeof (bb), forward_p ? rpost_cmp : post_cmp);\n    bitmap_clear (bb_to_consider);\n    for (i = 0; i < VARR_LENGTH (bb_t, worklist); i++) {\n      int changed_p = iter == 0;\n      edge_t e;\n\n      bb = addr[i];\n      if (forward_p) {\n        if (DLIST_HEAD (in_edge_t, bb->in_edges) == NULL)\n          con_func_0 (bb);\n        else\n          changed_p |= con_func_n (gen_ctx, bb);\n      } else {\n        if (DLIST_HEAD (out_edge_t, bb->out_edges) == NULL)\n          con_func_0 (bb);\n        else\n          changed_p |= con_func_n (gen_ctx, bb);\n      }\n      if (changed_p && trans_func (gen_ctx, bb)) {\n        if (forward_p) {\n          for (e = DLIST_HEAD (out_edge_t, bb->out_edges); e != NULL;\n               e = DLIST_NEXT (out_edge_t, e))\n            if (bitmap_set_bit_p (bb_to_consider, e->dst->index)) VARR_PUSH (bb_t, pending, e->dst);\n        } else {\n          for (e = DLIST_HEAD (in_edge_t, bb->in_edges); e != NULL; e = DLIST_NEXT (in_edge_t, e))\n            if (bitmap_set_bit_p (bb_to_consider, e->src->index)) VARR_PUSH (bb_t, pending, e->src);\n        }\n      }\n    }\n    iter++;\n    t = worklist;\n    worklist = pending;\n    pending = t;\n  }\n}\n\nstatic void init_data_flow (gen_ctx_t gen_ctx) {\n  MIR_alloc_t alloc = gen_alloc (gen_ctx);\n  gen_ctx->data_flow_ctx = gen_malloc (gen_ctx, sizeof (struct data_flow_ctx));\n  VARR_CREATE (bb_t, worklist, alloc, 0);\n  VARR_CREATE (bb_t, pending, alloc, 0);\n  bb_to_consider = bitmap_create2 (alloc, 512);\n}\n\nstatic void finish_data_flow (gen_ctx_t gen_ctx) {\n  VARR_DESTROY (bb_t, worklist);\n  VARR_DESTROY (bb_t, pending);\n  bitmap_destroy (bb_to_consider);\n  gen_free (gen_ctx, gen_ctx->data_flow_ctx);\n  gen_ctx->data_flow_ctx = NULL;\n}\n\n/* New Page */\n\nstatic MIR_insn_t get_insn_label (gen_ctx_t gen_ctx, MIR_insn_t insn) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  MIR_insn_t label;\n\n  if (insn->code == MIR_LABEL) return insn;\n  label = MIR_new_label (ctx);\n  MIR_insert_insn_before (ctx, curr_func_item, insn, label);\n  add_new_bb_insn (gen_ctx, label, ((bb_insn_t) insn->data)->bb, FALSE);\n  return label;\n}\n\n/* Clone hot BBs to cold ones (which are after ret insn) to improve\n   optimization opportunities in hot part.  */\nstatic int clone_bbs (gen_ctx_t gen_ctx) {\n  const int max_bb_growth_factor = 3;\n  MIR_context_t ctx = gen_ctx->ctx;\n  MIR_insn_t dst_insn, last_dst_insn, new_insn, label, next_insn, after;\n  bb_t bb, dst, new_bb;\n  edge_t e;\n  bb_insn_t bb_insn, dst_bb_insn, next_bb_insn;\n  MIR_func_t func = curr_func_item->u.func;\n  size_t size, orig_size, len, last_orig_bound;\n  int res;\n\n  gen_assert (optimize_level != 0);\n  bitmap_clear (temp_bitmap);\n  for (bb = DLIST_HEAD (bb_t, curr_cfg->bbs); bb != NULL; bb = DLIST_NEXT (bb_t, bb)) {\n    bitmap_set_bit_p (temp_bitmap, bb->index);\n    if ((bb_insn = DLIST_TAIL (bb_insn_t, bb->bb_insns)) == NULL) continue;\n    if (bb_insn->insn->code == MIR_RET || bb_insn->insn->code == MIR_JRET) break;\n  }\n  if (bb == NULL) return FALSE;\n  VARR_TRUNC (bb_t, worklist, 0);\n  for (bb = DLIST_NEXT (bb_t, bb); bb != NULL; bb = DLIST_NEXT (bb_t, bb)) {\n    bb_insn = DLIST_TAIL (bb_insn_t, bb->bb_insns);\n    gen_assert (bb_insn != NULL);\n    if (bb_insn->insn->code == MIR_JMP && (e = DLIST_HEAD (out_edge_t, bb->out_edges)) != NULL\n        && bitmap_bit_p (temp_bitmap, e->dst->index))\n      VARR_PUSH (bb_t, worklist, bb);\n  }\n  res = FALSE;\n  last_orig_bound = VARR_LENGTH (bb_t, worklist);\n  orig_size = size = 0;\n  while ((len = VARR_LENGTH (bb_t, worklist)) != 0) {\n    if (last_orig_bound > len) {\n      last_orig_bound = len;\n      orig_size = size = DLIST_LENGTH (bb_insn_t, VARR_LAST (bb_t, worklist)->bb_insns);\n    }\n    bb = VARR_POP (bb_t, worklist);\n    e = DLIST_HEAD (out_edge_t, bb->out_edges);\n    gen_assert (DLIST_NEXT (out_edge_t, e) == NULL);\n    if (e->back_edge_p) continue;\n    bb_insn = DLIST_TAIL (bb_insn_t, bb->bb_insns);\n    gen_assert (bb_insn != NULL && bb_insn->insn->code == MIR_JMP);\n    dst = e->dst;\n    dst_bb_insn = DLIST_TAIL (bb_insn_t, dst->bb_insns);\n    if (dst_bb_insn->insn->code == MIR_RET || dst_bb_insn->insn->code == MIR_JRET\n        || dst_bb_insn->insn->code == MIR_SWITCH || size > max_bb_growth_factor * orig_size)\n      continue;\n    res = TRUE;\n    DEBUG (2, {\n      fprintf (debug_file, \"  Cloning from BB%lu into BB%lu:\\n\", (unsigned long) dst->index,\n               (unsigned long) bb->index);\n    });\n    last_dst_insn = DLIST_TAIL (bb_insn_t, dst->bb_insns)->insn;\n    after = DLIST_PREV (MIR_insn_t, bb_insn->insn);\n    gen_delete_insn (gen_ctx, bb_insn->insn);\n    bb_insn = NULL;\n    for (dst_bb_insn = DLIST_HEAD (bb_insn_t, dst->bb_insns); dst_bb_insn != NULL;\n         dst_bb_insn = DLIST_NEXT (bb_insn_t, dst_bb_insn)) {\n      dst_insn = dst_bb_insn->insn;\n      if (dst_insn->code == MIR_LABEL) continue;\n      new_insn = MIR_copy_insn (ctx, dst_insn);\n      /* we can not use gen_add_insn_xxx becuase of some cases (e.g. bb_insn is the last insn): */\n      MIR_insert_insn_after (ctx, curr_func_item, after, new_insn);\n      add_new_bb_insn (gen_ctx, new_insn, bb, TRUE);\n      after = new_insn;\n      DEBUG (2, {\n        fprintf (debug_file, \"  Adding insn %-5lu\",\n                 (unsigned long) ((bb_insn_t) new_insn->data)->index);\n        MIR_output_insn (ctx, debug_file, new_insn, func, TRUE);\n      });\n      size++;\n    }\n    delete_edge (gen_ctx, e);\n    gen_assert (last_dst_insn != NULL);\n    if (last_dst_insn->code == MIR_JMP) {\n      label = last_dst_insn->ops[0].u.label;\n      create_edge (gen_ctx, bb, ((bb_insn_t) label->data)->bb, FALSE, TRUE);\n      if (bitmap_bit_p (temp_bitmap, ((bb_insn_t) label->data)->index))\n        VARR_PUSH (bb_t, worklist, bb);\n    } else if (!MIR_branch_code_p (last_dst_insn->code)) {\n      next_insn = DLIST_NEXT (MIR_insn_t, last_dst_insn);\n      next_bb_insn = next_insn->data;\n      gen_assert (next_insn->code == MIR_LABEL);\n      new_insn = MIR_new_insn (ctx, MIR_JMP, MIR_new_label_op (ctx, next_insn));\n      MIR_insert_insn_after (ctx, curr_func_item, after, new_insn);\n      add_new_bb_insn (gen_ctx, new_insn, bb, TRUE);\n      if (bitmap_bit_p (temp_bitmap, next_bb_insn->index)) VARR_PUSH (bb_t, worklist, bb);\n      create_edge (gen_ctx, bb, ((bb_insn_t) next_insn->data)->bb, FALSE, TRUE);\n    } else {\n      label = get_insn_label (gen_ctx, DLIST_NEXT (MIR_insn_t, last_dst_insn)); /* fallthrough */\n      new_insn = MIR_new_insn (ctx, MIR_JMP, MIR_new_label_op (ctx, label));\n      MIR_insert_insn_after (ctx, curr_func_item, after, new_insn);\n      new_bb = create_bb (gen_ctx, new_insn);\n      insert_new_bb_after (gen_ctx, bb, new_bb);\n      if (bitmap_bit_p (temp_bitmap, ((bb_insn_t) label->data)->bb->index))\n        VARR_PUSH (bb_t, worklist, new_bb);\n      create_edge (gen_ctx, bb, new_bb, TRUE, TRUE); /* fall through */\n      create_edge (gen_ctx, bb, ((bb_insn_t) last_dst_insn->ops[0].u.label->data)->bb, FALSE,\n                   TRUE); /* branch */\n      create_edge (gen_ctx, new_bb, ((bb_insn_t) label->data)->bb, FALSE, TRUE);\n    }\n    DEBUG (2, {\n      fprintf (debug_file, \"  Result BB%lu:\\n\", (unsigned long) bb->index);\n      output_in_edges (gen_ctx, bb);\n      output_out_edges (gen_ctx, bb);\n      for (bb_insn = DLIST_HEAD (bb_insn_t, bb->bb_insns); bb_insn != NULL;\n           bb_insn = DLIST_NEXT (bb_insn_t, bb_insn)) {\n        fprintf (debug_file, \"  %-5lu\", (unsigned long) bb_insn->index);\n        MIR_output_insn (ctx, debug_file, bb_insn->insn, func, TRUE);\n      }\n    });\n  }\n  if (res) {\n    remove_unreachable_bbs (gen_ctx);\n    enumerate_bbs (gen_ctx);\n  }\n  return res;\n}\n\n/* New Page */\n\n/* Building SSA.  First we build optimized maximal SSA, then we minimize it\n   getting minimal SSA for reducible CFGs. There are two SSA representations:\n\n   1. Def pointers only:\n\n      phi|insn: out:v1, in, in\n                       ^\n                       |\n      phi|insn: out, in:v1, ...\n\n   2. Def-use chains (we don't use mir-lists to use less memory):\n\n      phi|insn: out:v1, in, in\n                    | (op.data)\n                    V\n                  ssa_edge (next_use)---------------> ssa_edge\n                       ^                                ^\n                       | (op.data)                      | (op.data)\n      phi|insn: out, in:v1, ...        phi|insn: out, in:v1, ...\n\n*/\n\ntypedef struct ssa_edge *ssa_edge_t;\n\nstruct ssa_edge {\n  bb_insn_t use, def;\n  char flag;\n  uint16_t def_op_num;\n  uint32_t use_op_num;\n  ssa_edge_t prev_use, next_use; /* of the same def: we have only head in op.data */\n};\n\ntypedef struct def_tab_el {\n  bb_t bb;       /* table key */\n  MIR_reg_t reg; /* another key */\n  bb_insn_t def;\n} def_tab_el_t;\nDEF_HTAB (def_tab_el_t);\n\nDEF_VARR (ssa_edge_t);\nDEF_VARR (size_t);\nDEF_VARR (char);\n\nstruct ssa_ctx {\n  /* Different fake insns: defining undef, initial arg values. They are not in insn lists. */\n  VARR (bb_insn_t) * arg_bb_insns, *undef_insns;\n  VARR (bb_insn_t) * phis, *deleted_phis;\n  HTAB (def_tab_el_t) * def_tab; /* reg,bb -> insn defining reg  */\n  /* used for renaming: */\n  VARR (ssa_edge_t) * ssa_edges_to_process;\n  VARR (size_t) * curr_reg_indexes;\n  VARR (char) * reg_name;\n};\n\n#define arg_bb_insns gen_ctx->ssa_ctx->arg_bb_insns\n#define undef_insns gen_ctx->ssa_ctx->undef_insns\n#define phis gen_ctx->ssa_ctx->phis\n#define deleted_phis gen_ctx->ssa_ctx->deleted_phis\n#define def_tab gen_ctx->ssa_ctx->def_tab\n#define ssa_edges_to_process gen_ctx->ssa_ctx->ssa_edges_to_process\n#define curr_reg_indexes gen_ctx->ssa_ctx->curr_reg_indexes\n#define reg_name gen_ctx->ssa_ctx->reg_name\n\nstatic htab_hash_t def_tab_el_hash (def_tab_el_t el, void *arg MIR_UNUSED) {\n  return (htab_hash_t) mir_hash_finish (\n    mir_hash_step (mir_hash_step (mir_hash_init (0x33), (uint64_t) el.bb), (uint64_t) el.reg));\n}\n\nstatic int def_tab_el_eq (def_tab_el_t el1, def_tab_el_t el2, void *arg MIR_UNUSED) {\n  return el1.reg == el2.reg && el1.bb == el2.bb;\n}\n\nstatic MIR_insn_code_t get_move_code (MIR_type_t type) {\n  return (type == MIR_T_F    ? MIR_FMOV\n          : type == MIR_T_D  ? MIR_DMOV\n          : type == MIR_T_LD ? MIR_LDMOV\n                             : MIR_MOV);\n}\n\nstatic bb_insn_t get_fake_insn (gen_ctx_t gen_ctx, VARR (bb_insn_t) * fake_insns, MIR_reg_t reg) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  MIR_type_t type;\n  MIR_op_t op;\n  MIR_insn_t insn;\n  bb_insn_t bb_insn;\n  bb_t bb = DLIST_HEAD (bb_t, curr_cfg->bbs); /* enter bb */\n\n  gen_assert (bb->index == 0); /* enter bb */\n  op = _MIR_new_var_op (ctx, reg);\n  while (VARR_LENGTH (bb_insn_t, fake_insns) <= reg) VARR_PUSH (bb_insn_t, fake_insns, NULL);\n  if ((bb_insn = VARR_GET (bb_insn_t, fake_insns, reg)) == NULL) {\n    gen_assert (reg > MAX_HARD_REG);\n    type = MIR_reg_type (ctx, reg - MAX_HARD_REG, curr_func_item->u.func);\n    insn = MIR_new_insn (ctx, get_move_code (type), op, op);\n    bb_insn = create_bb_insn (gen_ctx, insn, bb);\n    VARR_SET (bb_insn_t, fake_insns, reg, bb_insn);\n  }\n  return bb_insn;\n}\n\nstatic int fake_insn_p (bb_insn_t bb_insn) { return bb_insn->bb->index == 0; /* enter bb */ }\n\nstatic bb_insn_t redundant_phi_def (gen_ctx_t gen_ctx, bb_insn_t phi, int *def_op_num_ref) {\n  bb_insn_t def, same = NULL;\n\n  *def_op_num_ref = 0;\n  for (size_t op_num = 1; op_num < phi->insn->nops; op_num++) { /* check input defs: */\n    def = phi->insn->ops[op_num].data;\n    if (def == same || def == phi) continue;\n    if (same != NULL) return NULL;\n    same = def;\n  }\n  gen_assert (phi->insn->ops[0].mode == MIR_OP_VAR);\n  if (same == NULL) same = get_fake_insn (gen_ctx, undef_insns, phi->insn->ops[0].u.var);\n  return same;\n}\n\nstatic bb_insn_t create_phi (gen_ctx_t gen_ctx, bb_t bb, MIR_op_t op) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  MIR_insn_t phi_insn;\n  bb_insn_t bb_insn, phi;\n  size_t len = DLIST_LENGTH (in_edge_t, bb->in_edges) + 1; /* output and inputs */\n\n  VARR_TRUNC (MIR_op_t, temp_ops, 0);\n  while (VARR_LENGTH (MIR_op_t, temp_ops) < len) VARR_PUSH (MIR_op_t, temp_ops, op);\n  phi_insn = MIR_new_insn_arr (ctx, MIR_PHI, len, VARR_ADDR (MIR_op_t, temp_ops));\n  bb_insn = DLIST_HEAD (bb_insn_t, bb->bb_insns);\n  if (bb_insn->insn->code == MIR_LABEL) {\n    gen_add_insn_after (gen_ctx, bb_insn->insn, phi_insn);\n  } else {\n    gen_add_insn_before (gen_ctx, bb_insn->insn, phi_insn);\n  }\n  phi_insn->ops[0].data = phi = phi_insn->data;\n  VARR_PUSH (bb_insn_t, phis, phi);\n  return phi;\n}\n\nstatic MIR_insn_t get_last_bb_phi_insn (MIR_insn_t phi_insn) {\n  MIR_insn_t curr_insn, next_insn;\n  bb_t bb = ((bb_insn_t) phi_insn->data)->bb;\n\n  gen_assert (phi_insn->code == MIR_PHI);\n  for (curr_insn = phi_insn;\n       (next_insn = DLIST_NEXT (MIR_insn_t, curr_insn)) != NULL\n       && ((bb_insn_t) next_insn->data)->bb == bb && next_insn->code == MIR_PHI;\n       curr_insn = next_insn)\n    ;\n  return curr_insn;\n}\n\nstatic bb_insn_t get_def (gen_ctx_t gen_ctx, MIR_reg_t reg, bb_t bb) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  bb_t src;\n  bb_insn_t def;\n  def_tab_el_t el, tab_el;\n  MIR_op_t op;\n\n  el.bb = bb;\n  el.reg = reg;\n  if (HTAB_DO (def_tab_el_t, def_tab, el, HTAB_FIND, tab_el)) return tab_el.def;\n  if (DLIST_LENGTH (in_edge_t, bb->in_edges) == 1) {\n    if ((src = DLIST_HEAD (in_edge_t, bb->in_edges)->src)->index == 0) { /* start bb: args */\n      return get_fake_insn (gen_ctx, arg_bb_insns, reg);\n    }\n    return get_def (gen_ctx, reg, DLIST_HEAD (in_edge_t, bb->in_edges)->src);\n  }\n  op = _MIR_new_var_op (ctx, reg);\n  el.def = def = create_phi (gen_ctx, bb, op);\n  HTAB_DO (def_tab_el_t, def_tab, el, HTAB_INSERT, tab_el);\n  return el.def;\n}\n\nstatic void add_phi_operands (gen_ctx_t gen_ctx, MIR_reg_t reg, bb_insn_t phi) {\n  size_t nop = 1;\n  bb_insn_t def;\n  edge_t in_edge;\n\n  for (in_edge = DLIST_HEAD (in_edge_t, phi->bb->in_edges); in_edge != NULL;\n       in_edge = DLIST_NEXT (in_edge_t, in_edge)) {\n    def = get_def (gen_ctx, reg, in_edge->src);\n    phi->insn->ops[nop++].data = def;\n  }\n}\n\nstatic bb_insn_t skip_redundant_phis (bb_insn_t def) {\n  while (def->insn->code == MIR_PHI && def != def->insn->ops[0].data) def = def->insn->ops[0].data;\n  return def;\n}\n\nstatic void minimize_ssa (gen_ctx_t gen_ctx, size_t insns_num) {\n  MIR_insn_t insn;\n  bb_insn_t phi, def;\n  size_t i, j, saved_bound;\n  int op_num, change_p;\n  MIR_reg_t var;\n  insn_var_iterator_t iter;\n\n  VARR_TRUNC (bb_insn_t, deleted_phis, 0);\n  do {\n    change_p = FALSE;\n    saved_bound = 0;\n    for (i = 0; i < VARR_LENGTH (bb_insn_t, phis); i++) {\n      phi = VARR_GET (bb_insn_t, phis, i);\n      for (j = 1; j < phi->insn->nops; j++)\n        phi->insn->ops[j].data = skip_redundant_phis (phi->insn->ops[j].data);\n      if ((def = redundant_phi_def (gen_ctx, phi, &op_num)) == NULL) {\n        VARR_SET (bb_insn_t, phis, saved_bound++, phi);\n        continue;\n      }\n      phi->insn->ops[0].data = def;\n      gen_assert (phi != def);\n      VARR_PUSH (bb_insn_t, deleted_phis, phi);\n      change_p = TRUE;\n    }\n    VARR_TRUNC (bb_insn_t, phis, saved_bound);\n  } while (change_p);\n  DEBUG (2, {\n    fprintf (debug_file, \"Minimizing SSA phis: from %ld to %ld phis (non-phi insns %ld)\\n\",\n             (long) VARR_LENGTH (bb_insn_t, deleted_phis) + (long) VARR_LENGTH (bb_insn_t, phis),\n             (long) VARR_LENGTH (bb_insn_t, phis), (long) insns_num);\n  });\n  for (bb_t bb = DLIST_HEAD (bb_t, curr_cfg->bbs); bb != NULL; bb = DLIST_NEXT (bb_t, bb))\n    for (bb_insn_t bb_insn = DLIST_HEAD (bb_insn_t, bb->bb_insns); bb_insn != NULL;\n         bb_insn = DLIST_NEXT (bb_insn_t, bb_insn)) {\n      insn = bb_insn->insn;\n      FOREACH_IN_INSN_VAR (gen_ctx, iter, insn, var, op_num) {\n        if (insn->ops[op_num].data == NULL) continue;\n        insn->ops[op_num].data = skip_redundant_phis (insn->ops[op_num].data);\n      }\n    }\n  for (i = 0; i < VARR_LENGTH (bb_insn_t, deleted_phis); i++) {\n    phi = VARR_GET (bb_insn_t, deleted_phis, i);\n    gen_delete_insn (gen_ctx, phi->insn);\n  }\n  for (i = 0; i < VARR_LENGTH (bb_insn_t, phis); i++) {\n    phi = VARR_GET (bb_insn_t, phis, i);\n    phi->insn->ops[0].data = NULL;\n  }\n}\n\nstatic void print_op_data (gen_ctx_t gen_ctx, void *op_data, bb_insn_t from) {\n  ssa_edge_t se;\n\n  if (op_data == NULL) {\n    fprintf (debug_file, \"_\");\n  } else if ((se = op_data)->def != from) {\n    fprintf (debug_file, \"%d\", se->def->index);\n  } else {\n    for (; se != NULL; se = se->next_use)\n      fprintf (debug_file, \"%s%d\", se == op_data ? \"(\" : \", \", se->use->index);\n    fprintf (debug_file, \")\");\n  }\n}\n\nstatic ssa_edge_t add_ssa_edge_1 (gen_ctx_t gen_ctx, bb_insn_t def, int def_op_num, bb_insn_t use,\n                                  int use_op_num, int dup_p MIR_UNUSED) {\n  MIR_op_t *op_ref;\n  ssa_edge_t ssa_edge = gen_malloc (gen_ctx, sizeof (struct ssa_edge));\n\n  gen_assert (use_op_num >= 0 && def_op_num >= 0 && def_op_num < (1 << 16));\n  gen_assert (def->insn->code != MIR_CALL || def_op_num != 0);\n  ssa_edge->flag = FALSE;\n  ssa_edge->def = def;\n  ssa_edge->def_op_num = def_op_num;\n  ssa_edge->use = use;\n  ssa_edge->use_op_num = use_op_num;\n  gen_assert (dup_p || use->insn->ops[use_op_num].data == NULL);\n  use->insn->ops[use_op_num].data = ssa_edge;\n  op_ref = &def->insn->ops[def_op_num];\n  ssa_edge->next_use = op_ref->data;\n  if (ssa_edge->next_use != NULL) ssa_edge->next_use->prev_use = ssa_edge;\n  ssa_edge->prev_use = NULL;\n  op_ref->data = ssa_edge;\n  return ssa_edge;\n}\n\nstatic ssa_edge_t add_ssa_edge (gen_ctx_t gen_ctx, bb_insn_t def, int def_op_num, bb_insn_t use,\n                                int use_op_num) {\n  return add_ssa_edge_1 (gen_ctx, def, def_op_num, use, use_op_num, FALSE);\n}\n\nstatic ssa_edge_t add_ssa_edge_dup (gen_ctx_t gen_ctx, bb_insn_t def, int def_op_num, bb_insn_t use,\n                                    int use_op_num) {\n  return add_ssa_edge_1 (gen_ctx, def, def_op_num, use, use_op_num, TRUE);\n}\n\nstatic void free_ssa_edge (gen_ctx_t gen_ctx, ssa_edge_t ssa_edge) { gen_free (gen_ctx, ssa_edge); }\n\nstatic void remove_ssa_edge (gen_ctx_t gen_ctx, ssa_edge_t ssa_edge) {\n  if (ssa_edge->prev_use != NULL) {\n    ssa_edge->prev_use->next_use = ssa_edge->next_use;\n  } else {\n    MIR_op_t *op_ref = &ssa_edge->def->insn->ops[ssa_edge->def_op_num];\n    gen_assert (op_ref->data == ssa_edge);\n    op_ref->data = ssa_edge->next_use;\n  }\n  if (ssa_edge->next_use != NULL) ssa_edge->next_use->prev_use = ssa_edge->prev_use;\n  gen_assert (ssa_edge->use->insn->ops[ssa_edge->use_op_num].data == ssa_edge);\n  ssa_edge->use->insn->ops[ssa_edge->use_op_num].data = NULL;\n  free_ssa_edge (gen_ctx, ssa_edge);\n}\n\nstatic void remove_insn_ssa_edges (gen_ctx_t gen_ctx, MIR_insn_t insn) {\n  ssa_edge_t ssa_edge;\n  for (size_t i = 0; i < insn->nops; i++) {\n    /* output operand refers to chain of ssa edges -- remove them all: */\n    while ((ssa_edge = insn->ops[i].data) != NULL)\n      remove_ssa_edge (gen_ctx, ssa_edge);\n  }\n}\n\nstatic void change_ssa_edge_list_def (ssa_edge_t list, bb_insn_t new_bb_insn,\n                                      unsigned new_def_op_num, MIR_reg_t reg, MIR_reg_t new_reg) {\n  gen_assert (new_reg > MAX_HARD_REG);\n  for (ssa_edge_t se = list; se != NULL; se = se->next_use) {\n    se->def = new_bb_insn;\n    se->def_op_num = new_def_op_num;\n    if (new_reg != MIR_NON_VAR) {\n      MIR_op_t *op_ref = &se->use->insn->ops[se->use_op_num];\n      if (op_ref->mode == MIR_OP_VAR) {\n        if (op_ref->u.var == reg) op_ref->u.var = new_reg;\n      } else {\n        gen_assert (op_ref->mode == MIR_OP_VAR_MEM);\n        if (op_ref->u.var_mem.base == reg) op_ref->u.var_mem.base = new_reg;\n        if (op_ref->u.var_mem.index == reg) op_ref->u.var_mem.index = new_reg;\n      }\n    }\n  }\n}\n\nstatic void redirect_def (gen_ctx_t gen_ctx, MIR_insn_t insn, MIR_insn_t by, int def_use_ssa_p) {\n#ifndef NDEBUG\n  int out_p, by_out_p;\n  MIR_insn_op_mode (gen_ctx->ctx, insn, 0, &out_p);\n  MIR_insn_op_mode (gen_ctx->ctx, by, 0, &by_out_p);\n  gen_assert (insn->ops[0].mode == MIR_OP_VAR && by->ops[0].mode == MIR_OP_VAR\n              && (def_use_ssa_p || insn->ops[0].u.var == by->ops[0].u.var)\n              && !MIR_call_code_p (insn->code) && out_p && by_out_p);\n#endif\n  by->ops[0].data = insn->ops[0].data;\n  insn->ops[0].data = NULL; /* make redundant insn having no uses */\n  change_ssa_edge_list_def (by->ops[0].data, by->data, 0, insn->ops[0].u.var, by->ops[0].u.var);\n  if (def_use_ssa_p) {\n    gen_assert (move_p (by) && insn->ops[0].mode == MIR_OP_VAR && by->ops[1].mode == MIR_OP_VAR\n                && insn->ops[0].u.var == by->ops[1].u.var);\n    add_ssa_edge (gen_ctx, insn->data, 0, by->data, 1);\n  }\n}\n\nstatic int get_var_def_op_num (gen_ctx_t gen_ctx, MIR_reg_t var, MIR_insn_t insn) {\n  int op_num;\n  MIR_reg_t insn_var;\n  insn_var_iterator_t iter;\n\n  FOREACH_OUT_INSN_VAR (gen_ctx, iter, insn, insn_var, op_num) {\n    if (var == insn_var) return op_num;\n  }\n  gen_assert (FALSE);\n  return -1;\n}\n\nstatic void process_insn_inputs_for_ssa_def_use_repr (gen_ctx_t gen_ctx, bb_insn_t bb_insn) {\n  MIR_insn_t insn = bb_insn->insn;\n  bb_insn_t def;\n  int op_num;\n  MIR_reg_t var;\n  insn_var_iterator_t iter;\n\n  FOREACH_IN_INSN_VAR (gen_ctx, iter, insn, var, op_num) {\n    if (var <= MAX_HARD_REG) continue;\n    def = insn->ops[op_num].data;\n    gen_assert (def != NULL);\n    insn->ops[op_num].data = NULL;\n    add_ssa_edge (gen_ctx, def, get_var_def_op_num (gen_ctx, var, def->insn), bb_insn, op_num);\n  }\n}\n\nstatic void make_ssa_def_use_repr (gen_ctx_t gen_ctx) {\n  bb_insn_t bb_insn;\n\n  for (bb_t bb = DLIST_HEAD (bb_t, curr_cfg->bbs); bb != NULL; bb = DLIST_NEXT (bb_t, bb))\n    for (bb_insn = DLIST_HEAD (bb_insn_t, bb->bb_insns); bb_insn != NULL;\n         bb_insn = DLIST_NEXT (bb_insn_t, bb_insn))\n      process_insn_inputs_for_ssa_def_use_repr (gen_ctx, bb_insn);\n}\n\nstatic void ssa_delete_insn (gen_ctx_t gen_ctx, MIR_insn_t insn) {\n  remove_insn_ssa_edges (gen_ctx, insn);\n  gen_delete_insn (gen_ctx, insn);\n}\n\nstatic MIR_reg_t get_new_reg (gen_ctx_t gen_ctx, MIR_reg_t old_reg, int sep, size_t index) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  MIR_func_t func = curr_func_item->u.func;\n  MIR_type_t type = MIR_reg_type (ctx, old_reg - MAX_HARD_REG, func);\n  const char *name = MIR_reg_name (ctx, old_reg - MAX_HARD_REG, func);\n  const char *hard_reg_name = MIR_reg_hard_reg_name (ctx, old_reg - MAX_HARD_REG, func);\n  char ind_str[30];\n  MIR_reg_t new_reg;\n\n  VARR_TRUNC (char, reg_name, 0);\n  VARR_PUSH_ARR (char, reg_name, name, strlen (name));\n  VARR_PUSH (char, reg_name, sep);\n  sprintf (ind_str, \"%lu\", (unsigned long) index); /* ??? should be enough to unique */\n  VARR_PUSH_ARR (char, reg_name, ind_str, strlen (ind_str) + 1);\n  if (hard_reg_name == NULL) {\n    new_reg = MIR_new_func_reg (ctx, func, type, VARR_ADDR (char, reg_name)) + MAX_HARD_REG;\n  } else {\n    new_reg = (MIR_new_global_func_reg (ctx, func, type, VARR_ADDR (char, reg_name), hard_reg_name)\n               + MAX_HARD_REG);\n    bitmap_set_bit_p (tied_regs, new_reg);\n  }\n  update_max_var (gen_ctx, new_reg);\n  return new_reg;\n}\n\nstatic int push_to_rename (gen_ctx_t gen_ctx, ssa_edge_t ssa_edge) {\n  if (ssa_edge->flag) return FALSE;\n  VARR_PUSH (ssa_edge_t, ssa_edges_to_process, ssa_edge);\n  ssa_edge->flag = TRUE;\n  DEBUG (2, {\n    fprintf (debug_file, \"     Adding ssa edge: def %lu:%d -> use %lu:%d:\\n      \",\n             (unsigned long) ssa_edge->def->index, ssa_edge->def_op_num,\n             (unsigned long) ssa_edge->use->index, ssa_edge->use_op_num);\n    print_bb_insn (gen_ctx, ssa_edge->def, FALSE);\n    fprintf (debug_file, \"     \");\n    print_bb_insn (gen_ctx, ssa_edge->use, FALSE);\n  });\n  return TRUE;\n}\n\nstatic int pop_to_rename (gen_ctx_t gen_ctx, ssa_edge_t *ssa_edge) {\n  if (VARR_LENGTH (ssa_edge_t, ssa_edges_to_process) == 0) return FALSE;\n  *ssa_edge = VARR_POP (ssa_edge_t, ssa_edges_to_process);\n  return TRUE;\n}\n\nstatic void process_insn_to_rename (gen_ctx_t gen_ctx, MIR_insn_t insn, int op_num) {\n  for (ssa_edge_t curr_edge = insn->ops[op_num].data; curr_edge != NULL;\n       curr_edge = curr_edge->next_use)\n    push_to_rename (gen_ctx, curr_edge);\n}\n\nstatic MIR_reg_t get_new_ssa_reg (gen_ctx_t gen_ctx, MIR_reg_t reg, int sep, int new_p) {\n  size_t reg_index;\n\n  while (VARR_LENGTH (size_t, curr_reg_indexes) <= reg) VARR_PUSH (size_t, curr_reg_indexes, 0);\n  reg_index = VARR_GET (size_t, curr_reg_indexes, reg);\n  VARR_SET (size_t, curr_reg_indexes, reg, reg_index + 1);\n  return reg_index == 0 && !new_p ? MIR_NON_VAR : get_new_reg (gen_ctx, reg, sep, reg_index);\n}\n\nstatic void rename_bb_insn (gen_ctx_t gen_ctx, bb_insn_t bb_insn) {\n  int op_num;\n  MIR_reg_t var, reg, new_reg;\n  MIR_insn_t insn, def_insn, use_insn;\n  ssa_edge_t ssa_edge;\n  insn_var_iterator_t iter;\n\n  insn = bb_insn->insn;\n  FOREACH_OUT_INSN_VAR (gen_ctx, iter, insn, var, op_num) {\n    if (var <= MAX_HARD_REG) continue;\n    ssa_edge = insn->ops[op_num].data;\n    if (ssa_edge != NULL && ssa_edge->flag) continue; /* already processed */\n    DEBUG (2, {\n      fprintf (debug_file, \"  Start def insn %-5lu\", (long unsigned) bb_insn->index);\n      print_bb_insn (gen_ctx, bb_insn, FALSE);\n    });\n    reg = var;\n    new_reg = get_new_ssa_reg (gen_ctx, reg, '@', FALSE);\n    if (ssa_edge == NULL) { /* special case: unused output */\n      if (new_reg != MIR_NON_VAR)\n        rename_op_reg (gen_ctx, &insn->ops[op_num], reg, new_reg, insn, TRUE);\n      continue;\n    }\n    VARR_TRUNC (ssa_edge_t, ssa_edges_to_process, 0);\n    process_insn_to_rename (gen_ctx, insn, op_num);\n    if (new_reg != MIR_NON_VAR) {\n      while (pop_to_rename (gen_ctx, &ssa_edge)) {\n        def_insn = ssa_edge->def->insn;\n        use_insn = ssa_edge->use->insn;\n        rename_op_reg (gen_ctx, &def_insn->ops[ssa_edge->def_op_num], reg, new_reg, def_insn, TRUE);\n        rename_op_reg (gen_ctx, &use_insn->ops[ssa_edge->use_op_num], reg, new_reg, use_insn, TRUE);\n      }\n    }\n  }\n}\n\nstatic void rename_regs (gen_ctx_t gen_ctx) {\n  bb_insn_t bb_insn;\n  int op_num;\n  MIR_reg_t var;\n  MIR_insn_t insn;\n  ssa_edge_t ssa_edge;\n  insn_var_iterator_t iter;\n\n  for (bb_t bb = DLIST_HEAD (bb_t, curr_cfg->bbs); bb != NULL; bb = DLIST_NEXT (bb_t, bb))\n    for (bb_insn = DLIST_HEAD (bb_insn_t, bb->bb_insns); bb_insn != NULL;\n         bb_insn = DLIST_NEXT (bb_insn_t, bb_insn)) { /* clear all ssa edge flags */\n      insn = bb_insn->insn;\n      FOREACH_IN_INSN_VAR (gen_ctx, iter, insn, var, op_num) {\n        if (var <= MAX_HARD_REG || MIR_addr_code_p (insn->code)) continue;\n        ssa_edge = insn->ops[op_num].data;\n        ssa_edge->flag = FALSE;\n      }\n    }\n  /* Process arg insns first to have first use of reg in the program with zero index.\n     We need this because machinize for args will use reg with zero index: */\n  for (size_t i = 0; i < VARR_LENGTH (bb_insn_t, arg_bb_insns); i++)\n    if ((bb_insn = VARR_GET (bb_insn_t, arg_bb_insns, i)) != NULL)\n      rename_bb_insn (gen_ctx, bb_insn);\n  for (bb_t bb = DLIST_HEAD (bb_t, curr_cfg->bbs); bb != NULL; bb = DLIST_NEXT (bb_t, bb))\n    for (bb_insn = DLIST_HEAD (bb_insn_t, bb->bb_insns); bb_insn != NULL;\n         bb_insn = DLIST_NEXT (bb_insn_t, bb_insn))\n      rename_bb_insn (gen_ctx, bb_insn);\n}\n\nstatic void process_bb_insn_for_ssa (gen_ctx_t gen_ctx, bb_insn_t bb_insn) {\n  bb_t bb = bb_insn->bb;\n  bb_insn_t def;\n  int op_num;\n  MIR_reg_t var, reg;\n  def_tab_el_t el;\n  insn_var_iterator_t iter;\n\n  FOREACH_IN_INSN_VAR (gen_ctx, iter, bb_insn->insn, var, op_num) {\n    gen_assert (var > MAX_HARD_REG);\n    reg = var;\n    def = get_def (gen_ctx, reg, bb);\n    bb_insn->insn->ops[op_num].data = def;\n  }\n  FOREACH_OUT_INSN_VAR (gen_ctx, iter, bb_insn->insn, var, op_num) {\n    reg = var;\n    el.bb = bb;\n    el.reg = reg;\n    el.def = bb_insn;\n    HTAB_DO (def_tab_el_t, def_tab, el, HTAB_REPLACE, el);\n  }\n}\n\nstatic void build_ssa (gen_ctx_t gen_ctx, int rename_p) {\n  bb_t bb;\n  bb_insn_t bb_insn, phi;\n  size_t i, insns_num;\n\n  gen_assert (VARR_LENGTH (bb_insn_t, arg_bb_insns) == 0\n              && VARR_LENGTH (bb_insn_t, undef_insns) == 0);\n  HTAB_CLEAR (def_tab_el_t, def_tab);\n  VARR_TRUNC (bb_t, worklist, 0);\n  for (bb = DLIST_HEAD (bb_t, curr_cfg->bbs); bb != NULL; bb = DLIST_NEXT (bb_t, bb))\n    VARR_PUSH (bb_t, worklist, bb);\n  qsort (VARR_ADDR (bb_t, worklist), VARR_LENGTH (bb_t, worklist), sizeof (bb_t), rpost_cmp);\n  VARR_TRUNC (bb_insn_t, phis, 0);\n  insns_num = 0;\n  for (i = 0; i < VARR_LENGTH (bb_t, worklist); i++) {\n    bb = VARR_GET (bb_t, worklist, i);\n    for (bb_insn = DLIST_HEAD (bb_insn_t, bb->bb_insns); bb_insn != NULL;\n         bb_insn = DLIST_NEXT (bb_insn_t, bb_insn))\n      if (bb_insn->insn->code != MIR_PHI) {\n        insns_num++;\n        process_bb_insn_for_ssa (gen_ctx, bb_insn);\n      }\n  }\n  for (i = 0; i < VARR_LENGTH (bb_insn_t, phis); i++) {\n    phi = VARR_GET (bb_insn_t, phis, i);\n    add_phi_operands (gen_ctx, phi->insn->ops[0].u.var, phi);\n  }\n  /* minimization can not be switched off for def_use representation\n     building as it clears ops[0].data: */\n  minimize_ssa (gen_ctx, insns_num);\n  make_ssa_def_use_repr (gen_ctx);\n  if (rename_p) {\n    VARR_TRUNC (size_t, curr_reg_indexes, 0);\n    rename_regs (gen_ctx);\n  }\n}\n\nstatic void make_conventional_ssa (gen_ctx_t gen_ctx) { /* requires life info */\n  MIR_context_t ctx = gen_ctx->ctx;\n  MIR_type_t type;\n  MIR_reg_t var, dest_var;\n  MIR_insn_code_t move_code;\n  MIR_insn_t insn, new_insn;\n  bb_t bb, prev_bb;\n  bb_insn_t bb_insn, next_bb_insn, tail, new_bb_insn, after;\n  edge_t e;\n  ssa_edge_t se;\n\n  for (bb = DLIST_HEAD (bb_t, curr_cfg->bbs); bb != NULL; bb = DLIST_NEXT (bb_t, bb))\n    for (bb_insn = DLIST_HEAD (bb_insn_t, bb->bb_insns); bb_insn != NULL; bb_insn = next_bb_insn) {\n      next_bb_insn = DLIST_NEXT (bb_insn_t, bb_insn);\n      if ((insn = bb_insn->insn)->code == MIR_LABEL) continue;\n      if (insn->code != MIR_PHI) break;\n      gen_assert (insn->ops[0].mode == MIR_OP_VAR && insn->ops[0].u.var > MAX_HARD_REG);\n      dest_var = var = insn->ops[0].u.var;\n      type = MIR_reg_type (gen_ctx->ctx, var - MAX_HARD_REG, curr_func_item->u.func);\n      move_code = get_move_code (type);\n      dest_var = get_new_ssa_reg (gen_ctx, var, '%', TRUE);\n      gen_assert (dest_var != MIR_NON_VAR);\n      e = DLIST_HEAD (in_edge_t, bb->in_edges);\n      for (size_t i = 1; i < insn->nops; i++) {\n        se = insn->ops[i].data;\n        insn->ops[i].data = NULL;\n        new_insn = MIR_new_insn (ctx, move_code, _MIR_new_var_op (ctx, dest_var), insn->ops[i]);\n        if ((tail = DLIST_TAIL (bb_insn_t, e->src->bb_insns)) == NULL) {\n          for (prev_bb = DLIST_PREV (bb_t, e->src), after = NULL;\n               prev_bb != NULL && (after = DLIST_TAIL (bb_insn_t, prev_bb->bb_insns)) == NULL;\n               prev_bb = DLIST_PREV (bb_t, prev_bb))\n            ;\n          if (after != NULL)\n            MIR_insert_insn_after (ctx, curr_func_item, after->insn, new_insn);\n          else\n            MIR_prepend_insn (ctx, curr_func_item, new_insn);\n          new_bb_insn = create_bb_insn (gen_ctx, new_insn, e->src);\n          DLIST_APPEND (bb_insn_t, e->src->bb_insns, new_bb_insn);\n        } else if (MIR_any_branch_code_p (tail->insn->code)) {\n          gen_add_insn_before (gen_ctx, tail->insn, new_insn);\n        } else {\n          gen_add_insn_after (gen_ctx, tail->insn, new_insn);\n        }\n        new_insn->ops[1].data = se;\n        se->use = new_insn->data;\n        se->use_op_num = 1;\n        add_ssa_edge (gen_ctx, new_insn->data, 0, bb_insn, (int) i);\n        insn->ops[i].mode = MIR_OP_VAR;\n        insn->ops[i].u.var = dest_var;\n        e = DLIST_NEXT (in_edge_t, e);\n      }\n      for (se = insn->ops[0].data; se != NULL; se = se->next_use)\n        if (se->use->bb != bb) break;\n      if (se == NULL) { /* we should do this only after adding moves at the end of bbs */\n        /* r=phi(...), all r uses in the same bb: change new_r = phi(...) and all uses by new_r */\n        insn->ops[0].u.var = dest_var;\n        change_ssa_edge_list_def (insn->ops[0].data, bb_insn, 0, var, dest_var);\n      } else {\n        new_insn = MIR_new_insn (ctx, move_code, _MIR_new_var_op (ctx, var),\n                                 _MIR_new_var_op (ctx, dest_var));\n        gen_add_insn_after (gen_ctx, insn, new_insn);\n        new_insn->ops[0].data = insn->ops[0].data;\n        insn->ops[0] = new_insn->ops[1];\n        change_ssa_edge_list_def (new_insn->ops[0].data, new_insn->data, 0, MIR_NON_VAR,\n                                  MIR_NON_VAR);\n        add_ssa_edge (gen_ctx, bb_insn, 0, new_insn->data, 1);\n      }\n    }\n}\n\nstatic void free_fake_bb_insns (gen_ctx_t gen_ctx, VARR (bb_insn_t) * bb_insns) {\n  bb_insn_t bb_insn;\n\n  while (VARR_LENGTH (bb_insn_t, bb_insns) != 0)\n    if ((bb_insn = VARR_POP (bb_insn_t, bb_insns)) != NULL) {  // ??? specialized free funcs\n      remove_insn_ssa_edges (gen_ctx, bb_insn->insn);\n      gen_free (gen_ctx, bb_insn->insn); /* we can not use gen_delete as the insn not in the list */\n      gen_free (gen_ctx, bb_insn);\n    }\n}\n\nstatic void undo_build_ssa (gen_ctx_t gen_ctx) {\n  bb_t bb;\n  bb_insn_t bb_insn, next_bb_insn;\n  ssa_edge_t se, next_se;\n  int op_num;\n  MIR_reg_t var;\n  MIR_insn_t insn;\n  insn_var_iterator_t iter;\n\n  free_fake_bb_insns (gen_ctx, arg_bb_insns);\n  free_fake_bb_insns (gen_ctx, undef_insns);\n  for (bb = DLIST_HEAD (bb_t, curr_cfg->bbs); bb != NULL; bb = DLIST_NEXT (bb_t, bb))\n    for (bb_insn = DLIST_HEAD (bb_insn_t, bb->bb_insns); bb_insn != NULL;\n         bb_insn = DLIST_NEXT (bb_insn_t, bb_insn)) {\n      insn = bb_insn->insn;\n      FOREACH_OUT_INSN_VAR (gen_ctx, iter, insn, var, op_num) {\n        /* all sse after ssa combine available only from defs */\n        for (se = insn->ops[op_num].data; se != NULL; se = next_se) {\n          next_se = se->next_use;\n          free_ssa_edge (gen_ctx, se);\n        }\n      }\n      for (size_t i = 0; i < insn->nops; i++) insn->ops[i].data = NULL;\n    }\n  for (bb = DLIST_HEAD (bb_t, curr_cfg->bbs); bb != NULL; bb = DLIST_NEXT (bb_t, bb))\n    for (bb_insn = DLIST_HEAD (bb_insn_t, bb->bb_insns); bb_insn != NULL; bb_insn = next_bb_insn) {\n      next_bb_insn = DLIST_NEXT (bb_insn_t, bb_insn);\n      if (bb_insn->insn->code == MIR_PHI) gen_delete_insn (gen_ctx, bb_insn->insn);\n    }\n}\n\nstatic void init_ssa (gen_ctx_t gen_ctx) {\n  MIR_alloc_t alloc = gen_alloc (gen_ctx);\n  gen_ctx->ssa_ctx = gen_malloc (gen_ctx, sizeof (struct ssa_ctx));\n  VARR_CREATE (bb_insn_t, arg_bb_insns, alloc, 0);\n  VARR_CREATE (bb_insn_t, undef_insns, alloc, 0);\n  VARR_CREATE (bb_insn_t, phis, alloc, 0);\n  VARR_CREATE (bb_insn_t, deleted_phis, alloc, 0);\n  HTAB_CREATE (def_tab_el_t, def_tab, alloc, 1024, def_tab_el_hash, def_tab_el_eq, gen_ctx);\n  VARR_CREATE (ssa_edge_t, ssa_edges_to_process, alloc, 512);\n  VARR_CREATE (size_t, curr_reg_indexes, alloc, 4096);\n  VARR_CREATE (char, reg_name, alloc, 20);\n}\n\nstatic void finish_ssa (gen_ctx_t gen_ctx) {\n  VARR_DESTROY (bb_insn_t, arg_bb_insns);\n  VARR_DESTROY (bb_insn_t, undef_insns);\n  VARR_DESTROY (bb_insn_t, phis);\n  VARR_DESTROY (bb_insn_t, deleted_phis);\n  HTAB_DESTROY (def_tab_el_t, def_tab);\n  VARR_DESTROY (ssa_edge_t, ssa_edges_to_process);\n  VARR_DESTROY (size_t, curr_reg_indexes);\n  VARR_DESTROY (char, reg_name);\n  gen_free (gen_ctx, gen_ctx->ssa_ctx);\n  gen_ctx->ssa_ctx = NULL;\n}\n\n/* New Page */\n\n/* If we have addr insns we transforming addressable pseudos to memory if the addr insn can not be\n   elimnated and memory of addressable pseudos to pseudos otherwise.  */\n\n/* Add all copies which are uses of bb_insn to temp_bb_insns2.  Return TRUE if all bb_insn uses\n   (skipping moves) are memory address.  Collect insns which bb_insn uses are memory in\n   bb_mem_insns. */\nstatic int collect_addr_uses (gen_ctx_t gen_ctx, bb_insn_t bb_insn,\n                              VARR (bb_insn_t) * bb_mem_insns) {\n  int res = TRUE;\n\n  gen_assert (MIR_addr_code_p (bb_insn->insn->code) || move_p (bb_insn->insn));\n  for (ssa_edge_t se = bb_insn->insn->ops[0].data; se != NULL; se = se->next_use) {\n    if (se->use->insn->ops[se->use_op_num].mode == MIR_OP_VAR_MEM) {\n      gen_assert (move_code_p (se->use->insn->code) && se->use_op_num <= 1);\n      if (bb_mem_insns != NULL) VARR_PUSH (bb_insn_t, bb_mem_insns, se->use);\n      continue;\n    }\n    if (!move_p (se->use->insn)) {\n      res = FALSE;\n    } else if (bitmap_set_bit_p (temp_bitmap2, se->use->index)) {\n      VARR_PUSH (bb_insn_t, temp_bb_insns2, se->use);\n    }\n  }\n  return res;\n}\n\n/* Return TRUE if all addr insn (bb_insn) uses (skipping moves) are memory address.\n   Collect insns which addr uses are memory in bb_mem_insns. */\nstatic int addr_eliminable_p (gen_ctx_t gen_ctx, bb_insn_t bb_insn,\n                              VARR (bb_insn_t) * bb_mem_insns) {\n  int res = TRUE;\n\n  bitmap_clear (temp_bitmap2);\n  VARR_TRUNC (bb_insn_t, temp_bb_insns2, 0);\n  if (bb_mem_insns != NULL) VARR_TRUNC (bb_insn_t, bb_mem_insns, 0);\n  if (!collect_addr_uses (gen_ctx, bb_insn, bb_mem_insns)) res = FALSE;\n  while (VARR_LENGTH (bb_insn_t, temp_bb_insns2) != 0) {\n    bb_insn_t copy_bb_insn = VARR_POP (bb_insn_t, temp_bb_insns2);\n    if (!collect_addr_uses (gen_ctx, copy_bb_insn, bb_mem_insns)) res = FALSE;\n  }\n  return res;\n}\n\n// aliasing, loc ???\nstatic void transform_addrs (gen_ctx_t gen_ctx) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  int op_num, out_p, ssa_rebuild_p = FALSE;\n  MIR_type_t type;\n  MIR_insn_code_t move_code;\n  MIR_reg_t var, reg, addr_reg, new_reg;\n  MIR_insn_t insn, addr_insn, new_insn;\n  bb_insn_t bb_insn, next_bb_insn;\n  ssa_edge_t se;\n  MIR_func_t func = curr_func_item->u.func;\n\n  gen_assert (addr_insn_p);\n  bitmap_clear (addr_regs);\n  VARR_TRUNC (bb_insn_t, temp_bb_insns, 0);\n  for (bb_t bb = DLIST_HEAD (bb_t, curr_cfg->bbs); bb != NULL; bb = DLIST_NEXT (bb_t, bb))\n    for (bb_insn = DLIST_HEAD (bb_insn_t, bb->bb_insns); bb_insn != NULL;\n         bb_insn = DLIST_NEXT (bb_insn_t, bb_insn))\n      if (MIR_addr_code_p (bb_insn->insn->code)) {\n        VARR_PUSH (bb_insn_t, temp_bb_insns, bb_insn);\n      } else if (move_p (bb_insn->insn)) {\n        gen_assert (bb_insn->insn->ops[1].data != NULL);\n      }\n  if (VARR_LENGTH (bb_insn_t, temp_bb_insns) == 0)\n    return; /* all addr insns can be unreachable and removed */\n  for (size_t i = 0; i < VARR_LENGTH (bb_insn_t, temp_bb_insns); i++) {\n    bb_insn = VARR_GET (bb_insn_t, temp_bb_insns, i);\n    insn = bb_insn->insn;\n    gen_assert (MIR_addr_code_p (insn->code) && insn->ops[0].mode == MIR_OP_VAR\n                && insn->ops[1].mode == MIR_OP_VAR);\n    if (!addr_eliminable_p (gen_ctx, bb_insn, NULL))\n      bitmap_set_bit_p (addr_regs, insn->ops[1].u.var);\n  }\n  addr_insn = NULL;       /* to remove warning */\n  addr_reg = MIR_NON_VAR; /* to remove warning */\n  for (bb_t bb = DLIST_HEAD (bb_t, curr_cfg->bbs); bb != NULL; bb = DLIST_NEXT (bb_t, bb))\n    for (bb_insn = DLIST_HEAD (bb_insn_t, bb->bb_insns); bb_insn != NULL; bb_insn = next_bb_insn) {\n      insn = bb_insn->insn;\n      next_bb_insn = DLIST_NEXT (bb_insn_t, bb_insn);\n      if (insn->code == MIR_PHI) {\n        /* we keep conventional SSA -- do nothing when we keep pseudo */\n        if (!bitmap_bit_p (addr_regs, insn->ops[0].u.var)) continue;\n        DEBUG (2, {\n          fprintf (debug_file, \"  deleting phi for pseudo transformed into memory \");\n          print_bb_insn (gen_ctx, insn->data, TRUE);\n        });\n        ssa_delete_insn (gen_ctx, insn);\n      } else if (insn->code == MIR_USE) {\n        int change_p = FALSE;\n        /* we keep conventional SSA -- do nothing */\n        for (size_t i = 0; i < insn->nops; i++) {\n          gen_assert (insn->ops[i].mode == MIR_OP_VAR);\n          if (!bitmap_bit_p (addr_regs, insn->ops[i].u.var)) continue;\n          remove_ssa_edge (gen_ctx, insn->ops[i].data);\n          for (size_t j = i; j + 1 < insn->nops; j++) insn->ops[j] = insn->ops[j + 1];\n          change_p = TRUE;\n          i--;\n          insn->nops--;\n        }\n        if (change_p) {\n          DEBUG (2, {\n            fprintf (debug_file, \"  modifying use to \");\n            print_bb_insn (gen_ctx, insn->data, TRUE);\n          });\n        }\n      } else if (!MIR_addr_code_p (insn->code)) { /* change reg to memory */\n        MIR_reg_t prev_reg = 0;\n        for (op_num = 0; op_num < (int) insn->nops; op_num++) {\n          if (insn->ops[op_num].mode == MIR_OP_VAR) {\n            var = insn->ops[op_num].u.var;\n            MIR_insn_op_mode (gen_ctx->ctx, insn, op_num, &out_p);\n          } else if (insn->ops[op_num].mode == MIR_OP_VAR_MEM) {\n            var = insn->ops[op_num].u.var_mem.base;\n            if (var == MIR_NON_VAR) continue;\n            out_p = FALSE;\n          } else {\n            continue;\n          }\n          if (var <= MAX_HARD_REG) continue;\n          reg = var;\n          if (!bitmap_bit_p (addr_regs, reg)) continue;\n          DEBUG (2, {\n            fprintf (debug_file, \"  \");\n            print_bb_insn (gen_ctx, bb_insn, TRUE);\n          });\n          if (reg != prev_reg) {\n            addr_reg = gen_new_temp_reg (gen_ctx, MIR_T_I64, func);\n            addr_insn = MIR_new_insn (ctx, MIR_ADDR, _MIR_new_var_op (ctx, addr_reg),\n                                      _MIR_new_var_op (ctx, reg));\n            gen_add_insn_before (gen_ctx, insn, addr_insn);\n            prev_reg = reg;\n            DEBUG (2, {\n              fprintf (debug_file, \"    adding before: \");\n              print_bb_insn (gen_ctx, addr_insn->data, TRUE);\n            });\n          }\n          type = MIR_reg_type (ctx, reg - MAX_HARD_REG, func);\n          move_code = get_move_code (type);\n          new_reg = gen_new_temp_reg (gen_ctx, type, func);\n          if (out_p) { /* p = ... => addr t2, p (no edge for p); t = ...; mem[t2] = t */\n            new_insn = MIR_new_insn (ctx, move_code,\n                                     _MIR_new_var_mem_op (ctx, type, 0, addr_reg, MIR_NON_VAR, 0),\n                                     _MIR_new_var_op (ctx, new_reg));\n            gen_add_insn_after (gen_ctx, insn, new_insn);\n            gen_assert (insn->ops[op_num].mode == MIR_OP_VAR);\n            insn->ops[op_num].u.var = new_reg;\n            while ((se = insn->ops[op_num].data) != NULL)\n              remove_ssa_edge (gen_ctx, se);\n            if (!ssa_rebuild_p) {\n              add_ssa_edge (gen_ctx, addr_insn->data, 0, new_insn->data, 0);\n              add_ssa_edge (gen_ctx, bb_insn, op_num, new_insn->data, 1);\n            }\n          } else { /* ... = p => addr t2, p (no edge for p); t = mem[t2]; ... = t */\n            new_insn = MIR_new_insn (ctx, move_code, _MIR_new_var_op (ctx, new_reg),\n                                     _MIR_new_var_mem_op (ctx, type, 0, addr_reg, MIR_NON_VAR, 0));\n            gen_add_insn_before (gen_ctx, insn, new_insn);\n            if (insn->ops[op_num].mode == MIR_OP_VAR) {\n              insn->ops[op_num].u.var = new_reg;\n            } else {\n              gen_assert (insn->ops[op_num].mode == MIR_OP_VAR_MEM\n                          && insn->ops[op_num].u.var_mem.base == reg);\n              insn->ops[op_num].u.var_mem.base = new_reg;\n            }\n            if (insn->ops[op_num].data != NULL)\n              remove_ssa_edge (gen_ctx,insn->ops[op_num].data);\n            if (!ssa_rebuild_p) {\n              add_ssa_edge (gen_ctx, addr_insn->data, 0, new_insn->data, 1);\n              add_ssa_edge (gen_ctx, new_insn->data, 0, bb_insn, op_num);\n            }\n          }\n          DEBUG (2, {\n            fprintf (debug_file, \"    adding %s: \", out_p ? \"after\" : \"before\");\n            print_bb_insn (gen_ctx, new_insn->data, TRUE);\n            fprintf (debug_file, \"    changing to \");\n            print_bb_insn (gen_ctx, bb_insn, TRUE);\n          });\n        }\n      } else if (!bitmap_bit_p (addr_regs, insn->ops[1].u.var)) {\n        /* addr a, p: change reg mem to reg */\n        MIR_UNUSED int res = addr_eliminable_p (gen_ctx, bb_insn, temp_bb_insns);\n        se = insn->ops[1].data;\n        gen_assert (res);\n        while (VARR_LENGTH (bb_insn_t, temp_bb_insns) != 0) {\n          /* ... = m[a] => ... = p; m[a] = ... => p = ... */\n          bb_insn_t use_bb_insn = VARR_POP (bb_insn_t, temp_bb_insns);\n          MIR_insn_t use_insn = use_bb_insn->insn;\n          gen_assert (move_code_p (use_insn->code));\n          op_num = use_insn->ops[0].mode == MIR_OP_VAR_MEM ? 0 : 1;\n          ssa_rebuild_p = TRUE;\n          switch (use_insn->ops[op_num].u.var_mem.type) {\n          case MIR_T_I8: use_insn->code = MIR_EXT8; break;\n          case MIR_T_U8: use_insn->code = MIR_UEXT8; break;\n          case MIR_T_I16: use_insn->code = MIR_EXT16; break;\n          case MIR_T_U16: use_insn->code = MIR_UEXT16; break;\n          case MIR_T_I32: use_insn->code = MIR_EXT32; break;\n          case MIR_T_U32: use_insn->code = MIR_UEXT32; break;\n          default: break;\n          }\n          if (use_insn->ops[op_num].data != NULL)\n            remove_ssa_edge (gen_ctx, use_insn->ops[op_num].data);\n          use_insn->ops[op_num].mode = MIR_OP_VAR;\n          use_insn->ops[op_num].u.var = insn->ops[1].u.var;\n          if (!ssa_rebuild_p) add_ssa_edge (gen_ctx, se->def, se->def_op_num, use_bb_insn, op_num);\n        }\n        DEBUG (2, {\n          fprintf (debug_file, \"  deleting \");\n          print_bb_insn (gen_ctx, insn->data, TRUE);\n        });\n        ssa_delete_insn (gen_ctx, insn);\n      }\n    }\n}\n\n/* New Page */\n\n/* Copy propagation */\n\nstatic int64_t gen_int_log2 (int64_t i) {\n  int64_t n;\n\n  if (i <= 0) return -1;\n  for (n = 0; (i & 1) == 0; n++, i >>= 1)\n    ;\n  return i == 1 ? n : -1;\n}\n\nstatic int power2_int_op (ssa_edge_t se, MIR_op_t **op_ref) {\n  MIR_op_t *op;\n\n  *op_ref = NULL;\n  if (se->def->insn->code != MIR_MOV) return -1;\n  *op_ref = op = &se->def->insn->ops[1];\n  if (op->mode != MIR_OP_INT && op->mode != MIR_OP_UINT) return -1;\n  return (int) gen_int_log2 (op->u.i);\n}\n\nstatic MIR_insn_t transform_mul_div (gen_ctx_t gen_ctx, MIR_insn_t insn) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  MIR_insn_t new_insns[7];\n  MIR_op_t temp[6], *op_ref;\n  MIR_func_t func = curr_func_item->u.func;\n  MIR_insn_code_t new_code;\n  int sh;\n  ssa_edge_t se;\n  int n;\n\n  switch (insn->code) {\n  case MIR_MUL: new_code = MIR_LSH; break;\n  case MIR_MULS: new_code = MIR_LSHS; break;\n  case MIR_UDIV: new_code = MIR_URSH; break;\n  case MIR_UDIVS: new_code = MIR_URSHS; break;\n  case MIR_DIV: new_code = MIR_RSH; break;\n  case MIR_DIVS: new_code = MIR_RSHS; break;\n  default: return insn;\n  }\n  sh = power2_int_op (insn->ops[2].data, &op_ref);\n  if (sh < 0 && (insn->code == MIR_MUL || insn->code == MIR_MULS)\n      && (sh = power2_int_op (insn->ops[1].data, &op_ref)) >= 0) {\n    temp[0] = insn->ops[1];\n    insn->ops[1] = insn->ops[2];\n    insn->ops[2] = temp[0];\n    ((ssa_edge_t) insn->ops[1].data)->use_op_num = 1;\n    ((ssa_edge_t) insn->ops[2].data)->use_op_num = 2;\n  }\n  if (sh < 0) return insn;\n  if (sh == 0) {\n    new_insns[0] = MIR_new_insn (ctx, MIR_MOV, insn->ops[0], insn->ops[1]);\n    new_insns[0]->ops[1].data = NULL;\n    gen_add_insn_before (gen_ctx, insn, new_insns[0]);\n    redirect_def (gen_ctx, insn, new_insns[0], FALSE);\n    se = insn->ops[1].data;\n    add_ssa_edge (gen_ctx, se->def, se->def_op_num, new_insns[0]->data, 1);\n    n = 1;\n  } else if (insn->code != MIR_DIV && insn->code != MIR_DIVS) {\n    temp[0] = _MIR_new_var_op (ctx, gen_new_temp_reg (gen_ctx, MIR_T_I64, func));\n    new_insns[0] = MIR_new_insn (ctx, MIR_MOV, temp[0], MIR_new_int_op (ctx, sh));\n    gen_add_insn_before (gen_ctx, insn, new_insns[0]);\n    new_insns[1] = MIR_new_insn (ctx, new_code, insn->ops[0], insn->ops[1], temp[0]);\n    new_insns[1]->ops[1].data = NULL;\n    gen_add_insn_before (gen_ctx, insn, new_insns[1]);\n    redirect_def (gen_ctx, insn, new_insns[1], FALSE);\n    se = insn->ops[1].data;\n    add_ssa_edge (gen_ctx, se->def, se->def_op_num, new_insns[1]->data, 1);\n    add_ssa_edge (gen_ctx, new_insns[0]->data, 0, new_insns[1]->data, 2);\n    n = 2;\n  } else {\n    for (int i = 0; i < 6; i++)\n      temp[i] = _MIR_new_var_op (ctx, gen_new_temp_reg (gen_ctx, MIR_T_I64, func));\n    if (insn->code == MIR_DIV) {\n      new_insns[0] = MIR_new_insn (ctx, MIR_MOV, temp[0], MIR_new_int_op (ctx, 63));\n      new_insns[1] = MIR_new_insn (ctx, MIR_RSH, temp[1], insn->ops[1], temp[0]);\n      new_insns[2] = MIR_new_insn (ctx, MIR_MOV, temp[2], MIR_new_int_op (ctx, op_ref->u.i - 1));\n      new_insns[3] = MIR_new_insn (ctx, MIR_AND, temp[3], temp[1], temp[2]);\n      new_insns[4] = MIR_new_insn (ctx, MIR_ADD, temp[4], temp[3], insn->ops[1]);\n    } else {\n      new_insns[0] = MIR_new_insn (ctx, MIR_MOV, temp[0], MIR_new_int_op (ctx, 31));\n      new_insns[1] = MIR_new_insn (ctx, MIR_RSHS, temp[1], insn->ops[1], temp[0]);\n      new_insns[2] = MIR_new_insn (ctx, MIR_MOV, temp[2], MIR_new_int_op (ctx, op_ref->u.i - 1));\n      new_insns[3] = MIR_new_insn (ctx, MIR_ANDS, temp[3], temp[1], temp[2]);\n      new_insns[4] = MIR_new_insn (ctx, MIR_ADDS, temp[4], temp[3], insn->ops[1]);\n    }\n    new_insns[1]->ops[1].data = NULL;\n    new_insns[4]->ops[2].data = NULL;\n    new_insns[5] = MIR_new_insn (ctx, MIR_MOV, temp[5], MIR_new_int_op (ctx, sh));\n    new_insns[6] = MIR_new_insn (ctx, new_code, insn->ops[0], temp[4], temp[5]);\n    for (int i = 0; i < 7; i++) gen_add_insn_before (gen_ctx, insn, new_insns[i]);\n    add_ssa_edge (gen_ctx, new_insns[0]->data, 0, new_insns[1]->data, 2);\n    add_ssa_edge (gen_ctx, new_insns[1]->data, 0, new_insns[3]->data, 1);\n    add_ssa_edge (gen_ctx, new_insns[2]->data, 0, new_insns[3]->data, 2);\n    add_ssa_edge (gen_ctx, new_insns[3]->data, 0, new_insns[4]->data, 1);\n    add_ssa_edge (gen_ctx, new_insns[4]->data, 0, new_insns[6]->data, 1);\n    add_ssa_edge (gen_ctx, new_insns[5]->data, 0, new_insns[6]->data, 2);\n    se = insn->ops[1].data;\n    add_ssa_edge (gen_ctx, se->def, se->def_op_num, new_insns[1]->data, 1);\n    add_ssa_edge (gen_ctx, se->def, se->def_op_num, new_insns[4]->data, 2);\n    redirect_def (gen_ctx, insn, new_insns[6], FALSE);\n    n = 7;\n  }\n  DEBUG (2, {\n    for (int i = 0; i < n; i++) {\n      fprintf (debug_file, i == 0 ? \"      adding \" : \"        and \");\n      print_bb_insn (gen_ctx, new_insns[i]->data, TRUE);\n    }\n    fprintf (debug_file, \"        and deleting \");\n    print_bb_insn (gen_ctx, insn->data, TRUE);\n  });\n  ssa_delete_insn (gen_ctx, insn);\n  return new_insns[n - 1];\n}\n\nstatic int get_ext_params (MIR_insn_code_t code, int *sign_p) {\n  *sign_p = code == MIR_EXT8 || code == MIR_EXT16 || code == MIR_EXT32;\n  switch (code) {\n  case MIR_EXT8:\n  case MIR_UEXT8: return 8;\n  case MIR_EXT16:\n  case MIR_UEXT16: return 16;\n  case MIR_EXT32:\n  case MIR_UEXT32: return 32;\n  default: return 0;\n  }\n}\n\nstatic int cmp_res64_p (MIR_insn_code_t cmp_code) {\n  switch (cmp_code) {\n#define REP_SEP :\n#define CASE_EL(e) case MIR_##e\n    REP4 (CASE_EL, EQ, FEQ, DEQ, LDEQ)\n      : REP4 (CASE_EL, NE, FNE, DNE, LDNE)\n      : REP5 (CASE_EL, LT, ULT, FLT, DLT, LDLT)\n      : REP5 (CASE_EL, LE, ULE, FLE, DLE, LDLE)\n      : REP5 (CASE_EL, GT, UGT, FGT, DGT, LDGT)\n      : REP5 (CASE_EL, GE, UGE, FGE, DGE, LDGE) : return TRUE;\n#undef REP_SEP\n  default: return FALSE;\n  }\n}\n\nstatic void copy_prop (gen_ctx_t gen_ctx) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  MIR_func_t func = curr_func_item->u.func;\n  MIR_insn_t insn, def_insn, new_insn, mov_insn;\n  MIR_op_t temp_op;\n  bb_insn_t bb_insn, next_bb_insn, def;\n  ssa_edge_t se, se2;\n  int op_num, w, w2, sign_p, sign2_p;\n  MIR_reg_t var, reg, new_reg, src_reg;\n  insn_var_iterator_t iter;\n\n  for (bb_t bb = DLIST_HEAD (bb_t, curr_cfg->bbs); bb != NULL; bb = DLIST_NEXT (bb_t, bb)) {\n    for (bb_insn = DLIST_HEAD (bb_insn_t, bb->bb_insns); bb_insn != NULL; bb_insn = next_bb_insn) {\n      next_bb_insn = DLIST_NEXT (bb_insn_t, bb_insn);\n      insn = bb_insn->insn;\n      if (MIR_addr_code_p (insn->code)) {\n        continue; /* no input reg propagation */\n      }\n      FOREACH_IN_INSN_VAR (gen_ctx, iter, insn, var, op_num) {\n        if (var <= MAX_HARD_REG) continue;\n        reg = var;\n        for (int n = 0; n < 30; n++) { /* unreachable code can create loops in copies */\n          se = insn->ops[op_num].data;\n          def = se->def;\n          if (def->bb->index == 0) break; /* arg init or undef insn */\n          def_insn = def->insn;\n          if (!move_p (def_insn) || def_insn->ops[0].u.var == def_insn->ops[1].u.var) break;\n          src_reg = def_insn->ops[1].u.var;\n          gen_assert (src_reg > MAX_HARD_REG);\n          if (MIR_reg_hard_reg_name (ctx, def_insn->ops[0].u.var - MAX_HARD_REG, func)\n              != MIR_reg_hard_reg_name (ctx, src_reg - MAX_HARD_REG, func))\n            break;\n          DEBUG (2, {\n            fprintf (debug_file, \"  Propagate from copy insn \");\n            print_bb_insn (gen_ctx, def, FALSE);\n          });\n          new_reg = def_insn->ops[1].u.var;\n          gen_assert (reg > MAX_HARD_REG && new_reg > MAX_HARD_REG);\n          remove_ssa_edge (gen_ctx, se);\n          se = def_insn->ops[1].data;\n          add_ssa_edge (gen_ctx, se->def, se->def_op_num, bb_insn, op_num);\n          rename_op_reg (gen_ctx, &insn->ops[op_num], reg, new_reg, insn, TRUE);\n          reg = new_reg;\n        }\n      }\n      if (move_p (insn) && insn->ops[0].data != NULL && (se = insn->ops[1].data) != NULL\n          && se->def == DLIST_PREV (bb_insn_t, bb_insn)\n          && (se = se->def->insn->ops[se->def_op_num].data) != NULL && se->next_use != NULL\n          && se->next_use->next_use == NULL\n          && (se->use == DLIST_NEXT (bb_insn_t, bb_insn)\n              || se->next_use->use == DLIST_NEXT (bb_insn_t, bb_insn))) {\n        /* a = ...; non-dead insn: b = a; ... = a & only two uses of a =>  b = ...; ... = b */\n        MIR_op_t *def_op_ref = &se->def->insn->ops[se->def_op_num];\n        remove_ssa_edge (gen_ctx, insn->ops[1].data);\n        se = def_op_ref->data;\n        gen_assert (se != NULL && se->next_use == NULL\n                    && se->use == DLIST_NEXT (bb_insn_t, bb_insn));\n        def_op_ref->u.var = insn->ops[0].u.var;\n        MIR_op_t *use_op_ref = &se->use->insn->ops[se->use_op_num];\n        gen_assert (use_op_ref->mode == MIR_OP_VAR || use_op_ref->mode == MIR_OP_VAR_MEM);\n        if (use_op_ref->mode == MIR_OP_VAR)\n          use_op_ref->u.var = def_op_ref->u.var;\n        else\n          use_op_ref->u.var_mem.base = def_op_ref->u.var;\n        change_ssa_edge_list_def (insn->ops[0].data, se->def, se->def_op_num, MIR_NON_VAR,\n                                  MIR_NON_VAR);\n        se->next_use = insn->ops[0].data;\n        se->next_use->prev_use = se;\n        insn->ops[0].data = insn->ops[1].data = NULL;\n        DEBUG (2, {\n          fprintf (debug_file, \"    Remove move %-5lu\", (unsigned long) bb_insn->index);\n          print_bb_insn (gen_ctx, bb_insn, FALSE);\n        });\n        gen_delete_insn (gen_ctx, insn);\n        continue;\n      }\n      insn = transform_mul_div (gen_ctx, insn);\n      bb_insn = insn->data;\n      w = get_ext_params (insn->code, &sign_p);\n      if (w == 0 || insn->ops[1].mode != MIR_OP_VAR) continue;\n      se = insn->ops[1].data;\n      def_insn = se->def->insn;\n      if (cmp_res64_p (def_insn->code)) {\n        DEBUG (2, {\n          fprintf (debug_file, \"    Change code of insn %lu \", (unsigned long) bb_insn->index);\n          MIR_output_insn (ctx, debug_file, insn, func, FALSE);\n          fprintf (debug_file, \"    to move\\n\");\n        });\n        insn->code = MIR_MOV;\n        next_bb_insn = bb_insn; /* process the new move */\n        continue;\n      }\n      w2 = get_ext_params (def_insn->code, &sign2_p);\n      if (w2 != 0 && w <= w2) {\n        /* [u]ext2<w2> b,a; ...[u]ext1<w> c,b -> [u]ext1<w> c,a when <w> <= <w2>: */\n        DEBUG (2, {\n          fprintf (debug_file, \"    Change code of insn %lu: before\",\n                   (unsigned long) bb_insn->index);\n          MIR_output_insn (ctx, debug_file, insn, func, FALSE);\n        });\n        insn->ops[1].u.var = def_insn->ops[1].u.var;\n        remove_ssa_edge (gen_ctx, se);\n        se = def_insn->ops[1].data;\n        add_ssa_edge (gen_ctx, se->def, se->def_op_num, bb_insn, 1);\n        DEBUG (2, {\n          fprintf (debug_file, \"    after \");\n          print_bb_insn (gen_ctx, bb_insn, FALSE);\n        });\n        next_bb_insn = bb_insn; /* process ext again */\n        continue;\n      } else if (w2 != 0 && w2 < w && (sign_p || !sign2_p)) { /* exclude ext<w2>, uext<w> pair */\n        /* [u]ext1<w2> b,a; .. [u]ext<w> c,b -> .. [u]ext1<w2> c,a */\n        DEBUG (2, {\n          fprintf (debug_file, \"    Change code of insn %lu: before\",\n                   (unsigned long) bb_insn->index);\n          MIR_output_insn (ctx, debug_file, insn, func, FALSE);\n        });\n        insn->code = def_insn->code;\n        insn->ops[1].u.var = def_insn->ops[1].u.var;\n        remove_ssa_edge (gen_ctx, se);\n        se = def_insn->ops[1].data;\n        add_ssa_edge (gen_ctx, se->def, se->def_op_num, bb_insn, 1);\n        DEBUG (2, {\n          fprintf (debug_file, \"    after \");\n          print_bb_insn (gen_ctx, bb_insn, FALSE);\n        });\n        next_bb_insn = bb_insn; /* process ext again */\n        continue;\n      }\n      if (!sign_p && (def_insn->code == MIR_AND || def_insn->code == MIR_ANDS)) {\n        if ((se2 = def_insn->ops[1].data) != NULL && (mov_insn = se2->def->insn)->code == MIR_MOV\n            && (mov_insn->ops[1].mode == MIR_OP_INT || mov_insn->ops[1].mode == MIR_OP_UINT))\n          SWAP (def_insn->ops[1], def_insn->ops[2], temp_op);\n        if ((se2 = def_insn->ops[2].data) == NULL || (mov_insn = se2->def->insn)->code != MIR_MOV\n            || (mov_insn->ops[1].mode != MIR_OP_INT && mov_insn->ops[1].mode != MIR_OP_UINT))\n          continue;\n        uint64_t c1 = mov_insn->ops[1].u.u;\n        uint64_t c2 = w == 8 ? 0xff : w == 16 ? 0xffff : 0xffffffff;\n        /* and r1,r2,c1; ... uext r, r1 => and r1,r2,c1; ... mov t, c1 & c2; and r, r2, t */\n        DEBUG (2, {\n          fprintf (debug_file, \"    Change code of insn %lu \", (unsigned long) bb_insn->index);\n          MIR_output_insn (ctx, debug_file, insn, func, FALSE);\n        });\n        new_reg = gen_new_temp_reg (gen_ctx, MIR_T_I64, func);\n        mov_insn = MIR_new_insn (ctx, MIR_MOV, _MIR_new_var_op (ctx, new_reg),\n                                 MIR_new_int_op (ctx, c1 & c2));\n        gen_add_insn_before (gen_ctx, insn, mov_insn);\n        new_insn = MIR_new_insn (ctx, MIR_AND, insn->ops[0], /* include ssa def list */\n                                 _MIR_new_var_op (ctx, def_insn->ops[1].u.var),\n                                 _MIR_new_var_op (ctx, new_reg));\n        gen_add_insn_before (gen_ctx, insn, new_insn);\n        remove_ssa_edge (gen_ctx, se);                                         /* r1 */\n        add_ssa_edge (gen_ctx, mov_insn->data, 0, new_insn->data, 2); /* t */\n        se = def_insn->ops[1].data;\n        add_ssa_edge (gen_ctx, se->def, se->def_op_num, new_insn->data, 1); /* r2 */\n        insn->ops[0].data = NULL;\n        change_ssa_edge_list_def (new_insn->ops[0].data, new_insn->data, 0, MIR_NON_VAR,\n                                  MIR_NON_VAR); /* r */\n        ssa_delete_insn (gen_ctx, insn);\n        DEBUG (2, {\n          fprintf (debug_file, \" on \");\n          MIR_output_insn (ctx, debug_file, mov_insn, func, FALSE);\n          fprintf (debug_file, \" and \");\n          MIR_output_insn (ctx, debug_file, new_insn, func, TRUE);\n        });\n      }\n    }\n  }\n}\n\n/* New Page */\n\n/* Removing redundant insns through GVN.  */\n\ntypedef struct expr {\n  MIR_insn_t insn;\n  uint32_t num;       /* the expression number (0, 1 ...) */\n  MIR_reg_t temp_reg; /* 0 initially and reg used to remove redundant expr */\n} *expr_t;\n\nDEF_VARR (expr_t);\nDEF_HTAB (expr_t);\n\ntypedef struct mem_expr {\n  MIR_insn_t insn;    /* load or store */\n  uint32_t mem_num;   /* the memory expression number (0, 1 ...) */\n  MIR_reg_t temp_reg; /* 0 initially and reg used to remove redundant load/store */\n  struct mem_expr *next;\n} *mem_expr_t;\n\nDEF_VARR (mem_expr_t);\nDEF_HTAB (mem_expr_t);\n\nstruct insn_nop_pair {\n  bb_insn_t bb_insn;\n  size_t nop;\n};\ntypedef struct insn_nop_pair insn_nop_pair_t;\n\nDEF_VARR (insn_nop_pair_t);\n\nstruct gvn_ctx {\n  MIR_insn_t temp_mem_insn;\n  VARR (expr_t) * exprs; /* the expr number -> expression */\n  VARR (mem_expr_t) * mem_exprs;\n  HTAB (expr_t) * expr_tab; /* keys: insn code and input operands */\n  /* keys: gvn val of memory address -> list of mem exprs: last added is the first */\n  HTAB (mem_expr_t) * mem_expr_tab;\n  VARR (insn_nop_pair_t) * insn_nop_pairs;\n};\n\n#define temp_mem_insn gen_ctx->gvn_ctx->temp_mem_insn\n#define exprs gen_ctx->gvn_ctx->exprs\n#define mem_exprs gen_ctx->gvn_ctx->mem_exprs\n#define expr_tab gen_ctx->gvn_ctx->expr_tab\n#define mem_expr_tab gen_ctx->gvn_ctx->mem_expr_tab\n#define insn_nop_pairs gen_ctx->gvn_ctx->insn_nop_pairs\n\nstatic void dom_con_func_0 (bb_t bb) { bitmap_clear (bb->dom_in); }\n\nstatic int dom_con_func_n (gen_ctx_t gen_ctx, bb_t bb) {\n  edge_t e, head;\n  bitmap_t prev_dom_in = temp_bitmap;\n\n  bitmap_copy (prev_dom_in, bb->dom_in);\n  head = DLIST_HEAD (in_edge_t, bb->in_edges);\n  bitmap_copy (bb->dom_in, head->src->dom_out);\n  for (e = DLIST_NEXT (in_edge_t, head); e != NULL; e = DLIST_NEXT (in_edge_t, e))\n    bitmap_and (bb->dom_in, bb->dom_in, e->src->dom_out); /* dom_in &= dom_out */\n  return !bitmap_equal_p (bb->dom_in, prev_dom_in);\n}\n\nstatic int dom_trans_func (gen_ctx_t gen_ctx, bb_t bb) {\n  bitmap_clear (temp_bitmap);\n  bitmap_set_bit_p (temp_bitmap, bb->index);\n  return bitmap_ior (bb->dom_out, bb->dom_in, temp_bitmap);\n}\n\nstatic void calculate_dominators (gen_ctx_t gen_ctx) {\n  bb_t entry_bb = DLIST_HEAD (bb_t, curr_cfg->bbs);\n\n  bitmap_clear (entry_bb->dom_out);\n  for (bb_t bb = DLIST_NEXT (bb_t, entry_bb); bb != NULL; bb = DLIST_NEXT (bb_t, bb))\n    bitmap_set_bit_range_p (bb->dom_out, 0, curr_bb_index);\n  solve_dataflow (gen_ctx, TRUE, dom_con_func_0, dom_con_func_n, dom_trans_func);\n}\n\n#define mem_av_in in\n#define mem_av_out out\n\nstatic int may_alias_p (MIR_alias_t alias1, MIR_alias_t alias2, MIR_alias_t nonalias1,\n                        MIR_alias_t nonalias2) {\n  return (alias1 == 0 || alias2 == 0 || alias1 == alias2)\n         && (nonalias1 == 0 || nonalias2 == 0 || nonalias1 != nonalias2);\n}\n\nstatic int may_mem_alias_p (const MIR_op_t *mem1, const MIR_op_t *mem2) {\n  gen_assert (mem1->mode == MIR_OP_VAR_MEM && mem2->mode == MIR_OP_VAR_MEM);\n  return may_alias_p (mem1->u.var_mem.alias, mem2->u.var_mem.alias, mem1->u.var_mem.nonalias,\n                      mem2->u.var_mem.nonalias);\n}\n\nstatic void mem_av_con_func_0 (bb_t bb) { bitmap_clear (bb->mem_av_in); }\n\nstatic int mem_av_con_func_n (gen_ctx_t gen_ctx, bb_t bb) {\n  edge_t e, head;\n  bitmap_t prev_mem_av_in = temp_bitmap;\n\n  bitmap_copy (prev_mem_av_in, bb->mem_av_in);\n  head = DLIST_HEAD (in_edge_t, bb->in_edges);\n  bitmap_copy (bb->mem_av_in, head->src->mem_av_out);\n  for (e = DLIST_NEXT (in_edge_t, head); e != NULL; e = DLIST_NEXT (in_edge_t, e))\n    bitmap_and (bb->mem_av_in, bb->mem_av_in, e->src->mem_av_out); /* mem_av_in &= mem_av_out */\n  return !bitmap_equal_p (bb->mem_av_in, prev_mem_av_in);\n}\n\nstatic int mem_av_trans_func (gen_ctx_t gen_ctx, bb_t bb) {\n  int alias_p;\n  size_t nel, nel2;\n  MIR_insn_t insn, mem_insn;\n  MIR_op_t *mem_ref;\n  bitmap_iterator_t bi, bi2;\n  bitmap_t prev_mem_av_out = temp_bitmap;\n\n  bitmap_copy (prev_mem_av_out, bb->mem_av_out);\n  bitmap_copy (bb->mem_av_out, bb->gen);\n  if (!bb->call_p) {\n    FOREACH_BITMAP_BIT (bi, bb->mem_av_in, nel) {\n      alias_p = FALSE;\n      insn = VARR_GET (mem_expr_t, mem_exprs, nel)->insn;\n      mem_ref = insn->ops[0].mode == MIR_OP_VAR_MEM ? &insn->ops[0] : &insn->ops[1];\n      FOREACH_BITMAP_BIT (bi2, bb->gen, nel2) { /* consider only stores */\n        mem_insn = VARR_GET (mem_expr_t, mem_exprs, nel2)->insn;\n        if (mem_insn->ops[0].mode == MIR_OP_VAR_MEM\n            && may_mem_alias_p (mem_ref, &mem_insn->ops[0])) {\n          alias_p = TRUE;\n          break;\n        }\n      }\n      if (!alias_p) bitmap_set_bit_p (bb->mem_av_out, nel);\n    }\n  }\n  return !bitmap_equal_p (bb->mem_av_out, prev_mem_av_out);\n}\n\nstatic void update_mem_availability (gen_ctx_t gen_ctx, bitmap_t mem_av, bb_insn_t mem_bb_insn) {\n  size_t nel;\n  bitmap_iterator_t bi;\n  MIR_insn_t mem_insn;\n  MIR_op_t *mem_ref = &mem_bb_insn->insn->ops[0];\n  int ld_p;\n\n  gen_assert (move_code_p (mem_bb_insn->insn->code));\n  if ((ld_p = mem_ref->mode != MIR_OP_VAR_MEM)) mem_ref = &mem_bb_insn->insn->ops[1];\n  gen_assert (mem_ref->mode == MIR_OP_VAR_MEM);\n  FOREACH_BITMAP_BIT (bi, mem_av, nel) {\n    mem_insn = VARR_GET (mem_expr_t, mem_exprs, nel)->insn;\n    if (!ld_p\n        && may_mem_alias_p (&mem_insn->ops[mem_insn->ops[0].mode == MIR_OP_VAR_MEM ? 0 : 1],\n                            mem_ref))\n      bitmap_clear_bit_p (mem_av, nel);\n  }\n  bitmap_set_bit_p (mem_av, mem_bb_insn->mem_index);\n}\n\nstatic void calculate_memory_availability (gen_ctx_t gen_ctx) {\n  MIR_context_t ctx = gen_ctx->ctx;\n\n  DEBUG (2, { fprintf (debug_file, \"Calculate memory availability:\\n\"); });\n  gen_assert (VARR_LENGTH (mem_expr_t, mem_exprs) == 0);\n  for (bb_t bb = DLIST_HEAD (bb_t, curr_cfg->bbs); bb != NULL; bb = DLIST_NEXT (bb_t, bb)) {\n    DEBUG (2, { fprintf (debug_file, \"  BB%lu:\\n\", (unsigned long) bb->index); });\n    bitmap_clear (bb->gen);\n    for (bb_insn_t bb_insn = DLIST_HEAD (bb_insn_t, bb->bb_insns); bb_insn != NULL;\n         bb_insn = DLIST_NEXT (bb_insn_t, bb_insn)) {\n      MIR_insn_t insn = bb_insn->insn;\n      mem_expr_t e;\n      size_t mem_num;\n\n      if (MIR_call_code_p (insn->code)) { /* ??? improving */\n        bitmap_clear (bb->gen);\n        continue;\n      }\n      if (!move_code_p (insn->code)) continue;\n      if (insn->ops[0].mode != MIR_OP_VAR_MEM && insn->ops[1].mode != MIR_OP_VAR_MEM) continue;\n      mem_num = VARR_LENGTH (mem_expr_t, mem_exprs);\n      bb_insn->mem_index = (uint32_t) mem_num;\n      DEBUG (2, {\n        fprintf (debug_file, \"     Adding mem insn %-5llu:\", (unsigned long long) mem_num);\n        MIR_output_insn (ctx, debug_file, insn, curr_func_item->u.func, TRUE);\n      });\n      e = gen_malloc (gen_ctx, sizeof (struct mem_expr));\n      e->insn = bb_insn->insn;\n      e->temp_reg = MIR_NON_VAR;\n      e->mem_num = (uint32_t) mem_num;\n      e->next = NULL;\n      VARR_PUSH (mem_expr_t, mem_exprs, e);\n      if (insn->ops[0].mode == MIR_OP_VAR_MEM || insn->ops[1].mode == MIR_OP_VAR_MEM)\n        update_mem_availability (gen_ctx, bb->gen, bb_insn);\n    }\n    DEBUG (2, { output_bitmap (gen_ctx, \"   Mem availabilty gen:\", bb->gen, FALSE, NULL); });\n  }\n  for (bb_t bb = DLIST_HEAD (bb_t, curr_cfg->bbs); bb != NULL; bb = DLIST_NEXT (bb_t, bb))\n    bitmap_set_bit_range_p (bb->mem_av_out, 0, VARR_LENGTH (mem_expr_t, mem_exprs));\n  solve_dataflow (gen_ctx, TRUE, mem_av_con_func_0, mem_av_con_func_n, mem_av_trans_func);\n  DEBUG (2, {\n    fprintf (debug_file, \"BB mem availability in/out:\\n\");\n    for (bb_t bb = DLIST_HEAD (bb_t, curr_cfg->bbs); bb != NULL; bb = DLIST_NEXT (bb_t, bb)) {\n      fprintf (debug_file, \"  BB%lu:\\n\", (unsigned long) bb->index);\n      output_bitmap (gen_ctx, \"    mem av in:\", bb->mem_av_in, FALSE, NULL);\n      output_bitmap (gen_ctx, \"    mem av out:\", bb->mem_av_out, FALSE, NULL);\n    }\n  });\n}\n\n#undef mem_av_in\n#undef mem_av_out\n\nstatic int op_eq (gen_ctx_t gen_ctx, MIR_op_t op1, MIR_op_t op2) {\n  return MIR_op_eq_p (gen_ctx->ctx, op1, op2);\n}\n\nstatic int multi_out_insn_p (MIR_insn_t insn) {\n  if (!MIR_call_code_p (insn->code)) return FALSE;\n  gen_assert (insn->ops[0].u.ref->item_type == MIR_proto_item);\n  return insn->ops[0].u.ref->u.proto->nres > 1;\n}\n\nstatic MIR_type_t canonic_mem_type (MIR_type_t type) {\n  switch (type) {\n  case MIR_T_U64: return MIR_T_I64;\n#ifdef MIR_PTR32\n  case MIR_T_P: return MIR_T_I32;\n#else\n  case MIR_T_P: return MIR_T_I64;\n#endif\n  default: return type;\n  }\n}\n\nstatic int expr_eq (expr_t e1, expr_t e2, void *arg) {\n  gen_ctx_t gen_ctx = arg;\n  MIR_context_t ctx = gen_ctx->ctx;\n  MIR_insn_t insn1 = e1->insn, insn2 = e2->insn;\n  size_t i, nops;\n  int out_p;\n  ssa_edge_t ssa_edge1, ssa_edge2;\n\n  if (insn1->code != insn2->code) return FALSE;\n  nops = MIR_insn_nops (gen_ctx->ctx, insn1);\n  for (i = 0; i < nops; i++) {\n    MIR_insn_op_mode (ctx, insn1, i, &out_p);\n    if (out_p && insn1->ops[i].mode != MIR_OP_VAR_MEM) continue;\n    if ((insn1->ops[i].mode != MIR_OP_VAR || insn2->ops[i].mode != MIR_OP_VAR)\n        && (insn1->ops[i].mode != MIR_OP_VAR_MEM || insn2->ops[i].mode != MIR_OP_VAR_MEM)\n        && !op_eq (gen_ctx, insn1->ops[i], insn2->ops[i]))\n      return FALSE;\n    ssa_edge1 = insn1->ops[i].data;\n    ssa_edge2 = insn2->ops[i].data;\n    if (ssa_edge1 != NULL && ssa_edge2 != NULL\n        && (ssa_edge1->def->gvn_val_const_p != ssa_edge2->def->gvn_val_const_p\n            || ssa_edge1->def->gvn_val != ssa_edge2->def->gvn_val\n            /* we can not be sure what definition we use in multi-output insn: */\n            || multi_out_insn_p (ssa_edge1->def->insn) || multi_out_insn_p (ssa_edge2->def->insn)))\n      return FALSE;\n    if (insn1->ops[i].mode == MIR_OP_VAR_MEM && insn2->ops[i].mode == MIR_OP_VAR_MEM\n        && canonic_mem_type (insn1->ops[i].u.var_mem.type)\n             != canonic_mem_type (insn2->ops[i].u.var_mem.type))\n      return FALSE;\n  }\n  return TRUE;\n}\n\nstatic htab_hash_t expr_hash (expr_t e, void *arg) {\n  gen_ctx_t gen_ctx = arg;\n  MIR_context_t ctx = gen_ctx->ctx;\n  size_t i, nops;\n  int out_p;\n  ssa_edge_t ssa_edge;\n  htab_hash_t h = (htab_hash_t) mir_hash_init (0x42);\n\n  h = (htab_hash_t) mir_hash_step (h, (uint64_t) e->insn->code);\n  nops = MIR_insn_nops (ctx, e->insn);\n  for (i = 0; i < nops; i++) {\n    MIR_insn_op_mode (ctx, e->insn, i, &out_p);\n    if (out_p && e->insn->ops[i].mode != MIR_OP_VAR_MEM) continue;\n    if (e->insn->ops[i].mode != MIR_OP_VAR && e->insn->ops[i].mode != MIR_OP_VAR_MEM)\n      h = MIR_op_hash_step (ctx, h, e->insn->ops[i]);\n    if ((ssa_edge = e->insn->ops[i].data) != NULL) {\n      h = (htab_hash_t) mir_hash_step (h, (uint64_t) ssa_edge->def->gvn_val_const_p);\n      h = (htab_hash_t) mir_hash_step (h, (uint64_t) ssa_edge->def->gvn_val);\n      if (e->insn->ops[i].mode == MIR_OP_VAR_MEM) {\n        gen_assert (e->insn->ops[i].u.var_mem.disp == 0);\n        h = (htab_hash_t) mir_hash_step (h, (uint64_t) canonic_mem_type (\n                                              e->insn->ops[i].u.var_mem.type));\n      }\n    }\n  }\n  return (htab_hash_t) mir_hash_finish (h);\n}\n\nstatic int find_expr (gen_ctx_t gen_ctx, MIR_insn_t insn, expr_t *e) {\n  struct expr es;\n\n  es.insn = insn;\n  return HTAB_DO (expr_t, expr_tab, &es, HTAB_FIND, *e);\n}\n\nstatic void insert_expr (gen_ctx_t gen_ctx, expr_t e) {\n  expr_t MIR_UNUSED e2;\n\n  gen_assert (!find_expr (gen_ctx, e->insn, &e2));\n  HTAB_DO (expr_t, expr_tab, e, HTAB_INSERT, e);\n}\n\nstatic void replace_expr (gen_ctx_t gen_ctx, expr_t e) {\n  expr_t MIR_UNUSED e2;\n\n  gen_assert (find_expr (gen_ctx, e->insn, &e2));\n  HTAB_DO (expr_t, expr_tab, e, HTAB_REPLACE, e);\n}\n\nstatic expr_t add_expr (gen_ctx_t gen_ctx, MIR_insn_t insn, int replace_p) {\n  expr_t e = gen_malloc (gen_ctx, sizeof (struct expr));\n\n  /* can not be calls, rets, stores */\n  gen_assert (!MIR_call_code_p (insn->code) && insn->code != MIR_RET && insn->code != MIR_JRET\n              && (!move_code_p (insn->code) || insn->ops[0].mode != MIR_OP_VAR_MEM));\n  e->insn = insn;\n  e->num = ((bb_insn_t) insn->data)->index;\n  e->temp_reg = MIR_NON_VAR;\n  VARR_PUSH (expr_t, exprs, e);\n  if (replace_p)\n    replace_expr (gen_ctx, e);\n  else\n    insert_expr (gen_ctx, e);\n  return e;\n}\n\nstatic int mem_expr_eq (mem_expr_t e1, mem_expr_t e2, void *arg MIR_UNUSED) {\n  MIR_insn_t st1 = e1->insn, st2 = e2->insn;\n  MIR_op_t *op_ref1 = &st1->ops[0], *op_ref2 = &st2->ops[0];\n  ssa_edge_t ssa_edge1, ssa_edge2;\n\n  gen_assert (move_code_p (st1->code) && move_code_p (st2->code));\n  if (op_ref1->mode != MIR_OP_VAR_MEM) op_ref1 = &st1->ops[1];\n  if (op_ref2->mode != MIR_OP_VAR_MEM) op_ref2 = &st2->ops[1];\n  gen_assert (op_ref1->mode == MIR_OP_VAR_MEM && op_ref2->mode == MIR_OP_VAR_MEM);\n  ssa_edge1 = op_ref1->data;\n  ssa_edge2 = op_ref2->data;\n  return (ssa_edge1 != NULL && ssa_edge2 != NULL\n          && ssa_edge1->def->gvn_val_const_p == ssa_edge2->def->gvn_val_const_p\n          && ssa_edge1->def->gvn_val == ssa_edge2->def->gvn_val\n          && canonic_mem_type (op_ref1->u.var_mem.type)\n               == canonic_mem_type (op_ref2->u.var_mem.type)\n          && op_ref1->u.var_mem.alias == op_ref2->u.var_mem.alias\n          && op_ref1->u.var_mem.nonalias == op_ref2->u.var_mem.nonalias);\n}\n\nstatic htab_hash_t mem_expr_hash (mem_expr_t e, void *arg MIR_UNUSED) {\n  MIR_insn_t st = e->insn;\n  MIR_op_t *op_ref;\n  ssa_edge_t ssa_edge;\n  htab_hash_t h = (htab_hash_t) mir_hash_init (0x23);\n\n  gen_assert (move_code_p (st->code));\n  op_ref = st->ops[0].mode == MIR_OP_VAR_MEM ? &st->ops[0] : &st->ops[1];\n  gen_assert (op_ref->mode == MIR_OP_VAR_MEM);\n  if ((ssa_edge = op_ref->data) != NULL) {\n    h = (htab_hash_t) mir_hash_step (h, (uint64_t) ssa_edge->def->gvn_val_const_p);\n    h = (htab_hash_t) mir_hash_step (h, (uint64_t) ssa_edge->def->gvn_val);\n  }\n  h = (htab_hash_t) mir_hash_step (h, (uint64_t) canonic_mem_type (op_ref->u.var_mem.type));\n  h = (htab_hash_t) mir_hash_step (h, (uint64_t) op_ref->u.var_mem.alias);\n  h = (htab_hash_t) mir_hash_step (h, (uint64_t) op_ref->u.var_mem.nonalias);\n  return (htab_hash_t) mir_hash_finish (h);\n}\n\nstatic mem_expr_t find_mem_expr (gen_ctx_t gen_ctx, MIR_insn_t mem_insn) {\n  mem_expr_t tab_e, e;\n\n  gen_assert (\n    move_code_p (mem_insn->code)\n    && (mem_insn->ops[0].mode == MIR_OP_VAR_MEM || mem_insn->ops[1].mode == MIR_OP_VAR_MEM));\n  e = VARR_GET (mem_expr_t, mem_exprs, ((bb_insn_t) mem_insn->data)->mem_index);\n  if (HTAB_DO (mem_expr_t, mem_expr_tab, e, HTAB_FIND, tab_e)) return tab_e;\n  return NULL;\n}\n\nstatic mem_expr_t add_mem_insn (gen_ctx_t gen_ctx, MIR_insn_t mem_insn) {\n  bb_insn_t bb_insn = mem_insn->data;\n  mem_expr_t tab_e, e;\n\n  gen_assert (\n    move_code_p (mem_insn->code)\n    && (mem_insn->ops[0].mode == MIR_OP_VAR_MEM || mem_insn->ops[1].mode == MIR_OP_VAR_MEM));\n  e = VARR_GET (mem_expr_t, mem_exprs, bb_insn->mem_index);\n  e->next = NULL;\n  if (HTAB_DO (mem_expr_t, mem_expr_tab, e, HTAB_FIND, tab_e)) e->next = tab_e;\n  HTAB_DO (mem_expr_t, mem_expr_tab, e, HTAB_REPLACE, tab_e);\n  return e;\n}\n\nstatic MIR_type_t mode2type (MIR_op_mode_t mode) {\n  return (mode == MIR_OP_FLOAT     ? MIR_T_F\n          : mode == MIR_OP_DOUBLE  ? MIR_T_D\n          : mode == MIR_OP_LDOUBLE ? MIR_T_LD\n                                   : MIR_T_I64);\n}\n\nstatic MIR_op_mode_t type2mode (MIR_type_t type) {\n  return (type == MIR_T_F    ? MIR_OP_FLOAT\n          : type == MIR_T_D  ? MIR_OP_DOUBLE\n          : type == MIR_T_LD ? MIR_OP_LDOUBLE\n                             : MIR_OP_INT);\n}\n\nstatic MIR_reg_t get_expr_temp_reg (gen_ctx_t gen_ctx, MIR_insn_t insn, MIR_reg_t *temp_reg) {\n  int out_p;\n  MIR_op_mode_t mode;\n\n  if (*temp_reg != MIR_NON_VAR) return *temp_reg;\n  mode = MIR_insn_op_mode (gen_ctx->ctx, insn, 0, &out_p);\n  *temp_reg = gen_new_temp_reg (gen_ctx, mode2type (mode), curr_func_item->u.func);\n  return *temp_reg;\n}\n\nstatic int fixed_place_insn_p (MIR_insn_t insn) {\n  return (insn->code == MIR_RET || insn->code == MIR_JRET || insn->code == MIR_SWITCH\n          || insn->code == MIR_LABEL || MIR_call_code_p (insn->code) || insn->code == MIR_ALLOCA\n          || insn->code == MIR_BSTART || insn->code == MIR_BEND || insn->code == MIR_VA_START\n          || insn->code == MIR_VA_ARG || insn->code == MIR_VA_END);\n}\n\nstatic int gvn_insn_p (MIR_insn_t insn) { return !fixed_place_insn_p (insn); }\n\n#if !MIR_NO_GEN_DEBUG\nstatic void print_expr (gen_ctx_t gen_ctx, expr_t e, const char *title) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  size_t nops;\n\n  fprintf (debug_file, \"  %s %3lu: \", title, (unsigned long) e->num);\n  fprintf (debug_file, \"%s _\", MIR_insn_name (ctx, e->insn->code));\n  nops = MIR_insn_nops (ctx, e->insn);\n  for (size_t j = 1; j < nops; j++) {\n    fprintf (debug_file, \", \");\n    MIR_output_op (ctx, debug_file, e->insn->ops[j], curr_func_item->u.func);\n  }\n  fprintf (debug_file, \"\\n\");\n}\n#endif\n\nstatic int add_sub_const_insn_p (gen_ctx_t gen_ctx, MIR_insn_t insn, int64_t *val) {\n  ssa_edge_t ssa_edge;\n  bb_insn_t def_bb_insn;\n  // ??? , minimal gvn->val\n  if (insn->code != MIR_ADD && insn->code != MIR_SUB && insn->code != MIR_ADDS\n      && insn->code != MIR_SUBS)\n    return FALSE;\n  if ((ssa_edge = insn->ops[2].data) == NULL || !(def_bb_insn = ssa_edge->def)->gvn_val_const_p)\n    return FALSE;\n  MIR_func_t func = curr_func_item->u.func;\n  if (insn->ops[1].mode == MIR_OP_VAR\n      && MIR_reg_hard_reg_name (gen_ctx->ctx, insn->ops[1].u.var - MAX_HARD_REG, func) != NULL)\n    return FALSE;\n  *val = insn->code == MIR_SUB || insn->code == MIR_SUBS ? -def_bb_insn->gvn_val\n                                                         : def_bb_insn->gvn_val;\n  return TRUE;\n}\n\nstatic MIR_insn_t skip_moves (gen_ctx_t gen_ctx, MIR_insn_t insn) {\n  ssa_edge_t se;\n  MIR_func_t func = curr_func_item->u.func;\n\n  while (insn->code == MIR_MOV && insn->ops[1].mode == MIR_OP_VAR) {\n    if ((se = insn->ops[1].data) == NULL\n        || MIR_reg_hard_reg_name (gen_ctx->ctx, insn->ops[1].u.var - MAX_HARD_REG, func) != NULL)\n      return insn;\n    insn = se->def->insn;\n  }\n  return insn;\n}\n\nstatic void print_bb_insn_value (gen_ctx_t gen_ctx, bb_insn_t bb_insn) {\n  MIR_context_t ctx = gen_ctx->ctx;\n\n  DEBUG (2, {\n    fprintf (debug_file, \"%s%s=%lld for insn %lu:\",\n             !bb_insn->alloca_flag                               ? \"\"\n             : bb_insn->alloca_flag & (MAY_ALLOCA | MUST_ALLOCA) ? \"may/must alloca \"\n             : bb_insn->alloca_flag & MAY_ALLOCA                 ? \"may alloca\"\n                                                                 : \"must alloca\",\n             bb_insn->gvn_val_const_p ? \"const val\" : \"val\", (long long) bb_insn->gvn_val,\n             (unsigned long) bb_insn->index);\n    MIR_output_insn (ctx, debug_file, bb_insn->insn, curr_func_item->u.func, TRUE);\n  });\n}\n\nstatic int get_gvn_op (MIR_insn_t insn, size_t nop, int64_t *val) {\n  MIR_op_t *op_ref = &insn->ops[nop];\n  ssa_edge_t ssa_edge;\n  bb_insn_t def_bb_insn;\n\n  if ((ssa_edge = op_ref->data) != NULL && (def_bb_insn = ssa_edge->def)->gvn_val_const_p) {\n    *val = def_bb_insn->gvn_val;\n    return TRUE;\n  }\n  return FALSE;\n}\n\nstatic int get_gvn_2ops (MIR_insn_t insn, int64_t *val1) { return get_gvn_op (insn, 1, val1); }\n\nstatic int get_gvn_3ops (MIR_insn_t insn, int64_t *val1, int64_t *val2) {\n  if (get_gvn_op (insn, 1, val1) && get_gvn_op (insn, 2, val2)) return TRUE;\n  return FALSE;\n}\n\nstatic int get_gvn_2iops (MIR_insn_t insn, int64_t *p) {\n  int64_t val;\n\n  if (!get_gvn_2ops (insn, &val)) return FALSE;\n  *p = val;\n  return TRUE;\n}\n\nstatic int get_gvn_2isops (MIR_insn_t insn, int32_t *p) {\n  int64_t val;\n\n  if (!get_gvn_2ops (insn, &val)) return FALSE;\n  *p = (int32_t) val;\n  return TRUE;\n}\n\nstatic int MIR_UNUSED get_gvn_2usops (MIR_insn_t insn, uint32_t *p) {\n  int64_t val;\n\n  if (!get_gvn_2ops (insn, &val)) return FALSE;\n  *p = (uint32_t) val;\n  return TRUE;\n}\n\nstatic int get_gvn_3iops (MIR_insn_t insn, int64_t *p1, int64_t *p2) {\n  int64_t val1, val2;\n\n  if (!get_gvn_3ops (insn, &val1, &val2)) return FALSE;\n  *p1 = val1;\n  *p2 = val2;\n  return TRUE;\n}\n\nstatic int get_gvn_3isops (MIR_insn_t insn, int32_t *p1, int32_t *p2) {\n  int64_t val1, val2;\n\n  if (!get_gvn_3ops (insn, &val1, &val2)) return FALSE;\n  *p1 = (int32_t) val1;\n  *p2 = (int32_t) val2;\n  return TRUE;\n}\n\nstatic int get_gvn_3uops (MIR_insn_t insn, uint64_t *p1, uint64_t *p2) {\n  int64_t val1, val2;\n\n  if (!get_gvn_3ops (insn, &val1, &val2)) return FALSE;\n  *p1 = val1;\n  *p2 = val2;\n  return TRUE;\n}\n\nstatic int get_gvn_3usops (MIR_insn_t insn, uint32_t *p1, uint32_t *p2) {\n  int64_t val1, val2;\n\n  if (!get_gvn_3ops (insn, &val1, &val2)) return FALSE;\n  *p1 = (uint32_t) val1;\n  *p2 = (uint32_t) val2;\n  return TRUE;\n}\n\n#define GVN_EXT(tp)                                         \\\n  do {                                                      \\\n    int64_t p;                                              \\\n    if ((const_p = get_gvn_2iops (insn, &p))) val = (tp) p; \\\n  } while (0)\n\n#define GVN_IOP2(op)                                      \\\n  do {                                                    \\\n    int64_t p;                                            \\\n    if ((const_p = get_gvn_2iops (insn, &p))) val = op p; \\\n  } while (0)\n\n#define GVN_IOP2S(op)                                      \\\n  do {                                                     \\\n    int32_t p;                                             \\\n    if ((const_p = get_gvn_2isops (insn, &p))) val = op p; \\\n  } while (0)\n\n#define GVN_IOP3(op)                                                \\\n  do {                                                              \\\n    int64_t p1, p2;                                                 \\\n    if ((const_p = get_gvn_3iops (insn, &p1, &p2))) val = p1 op p2; \\\n  } while (0)\n\n#define GVN_IOP3S(op)                                                \\\n  do {                                                               \\\n    int32_t p1, p2;                                                  \\\n    if ((const_p = get_gvn_3isops (insn, &p1, &p2))) val = p1 op p2; \\\n  } while (0)\n\n#define GVN_UOP3(op)                                                \\\n  do {                                                              \\\n    uint64_t p1, p2;                                                \\\n    if ((const_p = get_gvn_3uops (insn, &p1, &p2))) val = p1 op p2; \\\n  } while (0)\n\n#define GVN_UOP3S(op)                                                \\\n  do {                                                               \\\n    uint32_t p1, p2;                                                 \\\n    if ((const_p = get_gvn_3usops (insn, &p1, &p2))) val = p1 op p2; \\\n  } while (0)\n\n#define GVN_IOP30(op)                                          \\\n  do {                                                         \\\n    if (get_gvn_op (insn, 2, &val) && val != 0) GVN_IOP3 (op); \\\n  } while (0)\n\n#define GVN_IOP3S0(op)                                          \\\n  do {                                                          \\\n    if (get_gvn_op (insn, 2, &val) && val != 0) GVN_IOP3S (op); \\\n  } while (0)\n\n#define GVN_UOP30(op)                                          \\\n  do {                                                         \\\n    if (get_gvn_op (insn, 2, &val) && val != 0) GVN_UOP3 (op); \\\n  } while (0)\n\n#define GVN_UOP3S0(op)                                          \\\n  do {                                                          \\\n    if (get_gvn_op (insn, 2, &val) && val != 0) GVN_UOP3S (op); \\\n  } while (0)\n\n#define GVN_ICMP(op)                                                \\\n  do {                                                              \\\n    int64_t p1, p2;                                                 \\\n    if ((const_p = get_gvn_3iops (insn, &p1, &p2))) val = p1 op p2; \\\n  } while (0)\n\n#define GVN_ICMPS(op)                                                \\\n  do {                                                               \\\n    int32_t p1, p2;                                                  \\\n    if ((const_p = get_gvn_3isops (insn, &p1, &p2))) val = p1 op p2; \\\n  } while (0)\n\n#define GVN_UCMP(op)                                                \\\n  do {                                                              \\\n    uint64_t p1, p2;                                                \\\n    if ((const_p = get_gvn_3uops (insn, &p1, &p2))) val = p1 op p2; \\\n  } while (0)\n\n#define GVN_UCMPS(op)                                                \\\n  do {                                                               \\\n    uint32_t p1, p2;                                                 \\\n    if ((const_p = get_gvn_3usops (insn, &p1, &p2))) val = p1 op p2; \\\n  } while (0)\n\nstatic int gvn_phi_val (bb_insn_t phi, int64_t *val) {\n  MIR_insn_t phi_insn = phi->insn;\n  bb_t bb = phi->bb;\n  bb_insn_t def_bb_insn;\n  edge_t e;\n  size_t nop;\n  ssa_edge_t se;\n  int const_p = TRUE, same_p = TRUE;\n\n  nop = 1;\n  for (e = DLIST_HEAD (in_edge_t, bb->in_edges); e != NULL; e = DLIST_NEXT (in_edge_t, e), nop++) {\n    /* Update phi value: */\n    gen_assert (nop < phi_insn->nops);\n    if (same_p) {\n      if ((se = phi_insn->ops[nop].data) == NULL || (def_bb_insn = se->def) == NULL) {\n        same_p = FALSE;\n      } else if (nop == 1) {\n        const_p = def_bb_insn->gvn_val_const_p;\n        *val = def_bb_insn->gvn_val;\n      } else if (const_p != def_bb_insn->gvn_val_const_p || *val != def_bb_insn->gvn_val) {\n        same_p = FALSE;\n      }\n    }\n    if ((se = phi_insn->ops[nop].data) != NULL) {\n      phi->alloca_flag = nop == 1 ? se->def->alloca_flag\n                                  : ((phi->alloca_flag | se->def->alloca_flag) & MAY_ALLOCA)\n                                      | (phi->alloca_flag & se->def->alloca_flag & MUST_ALLOCA);\n    }\n  }\n  if (!same_p) *val = phi->index;\n  return same_p && const_p;\n}\n\nstatic void remove_edge_phi_ops (gen_ctx_t gen_ctx, edge_t e) {\n  size_t i, nop;\n  edge_t e2;\n  MIR_insn_t insn;\n  ssa_edge_t se;\n\n  for (nop = 1, e2 = DLIST_HEAD (in_edge_t, e->dst->in_edges); e2 != NULL && e2 != e;\n       e2 = DLIST_NEXT (in_edge_t, e2), nop++)\n    ;\n  gen_assert (e2 != NULL);\n  for (bb_insn_t bb_insn = DLIST_HEAD (bb_insn_t, e->dst->bb_insns); bb_insn != NULL;\n       bb_insn = DLIST_NEXT (bb_insn_t, bb_insn)) {\n    if ((insn = bb_insn->insn)->code == MIR_LABEL) continue;\n    if (insn->code != MIR_PHI) break;\n    if ((se = insn->ops[nop].data) != NULL)\n      remove_ssa_edge (gen_ctx, se);\n    for (i = nop; i + 1 < insn->nops; i++) {\n      insn->ops[i] = insn->ops[i + 1];\n      /* se can be null from some previously removed BB insn: */\n      if ((se = insn->ops[i].data) != NULL) {\n        gen_assert (se->use_op_num == i + 1);\n        se->use_op_num = (uint32_t) i;\n      }\n    }\n    insn->nops--;\n  }\n}\n\nstatic void MIR_UNUSED remove_dest_phi_ops (gen_ctx_t gen_ctx, bb_t bb) {\n  for (edge_t e = DLIST_HEAD (out_edge_t, bb->out_edges); e != NULL; e = DLIST_NEXT (out_edge_t, e))\n    remove_edge_phi_ops (gen_ctx, e);\n}\n\nstatic void set_alloca_based_flag (bb_insn_t bb_insn, int must_p) {\n  MIR_insn_t insn = bb_insn->insn;\n  ssa_edge_t se;\n\n  gen_assert (insn->nops >= 2);\n  if (must_p) {\n    if (((se = insn->ops[1].data) != NULL && (se->def->alloca_flag & MUST_ALLOCA))\n        || (insn->nops == 3 && (se = insn->ops[2].data) != NULL\n            && (se->def->alloca_flag & MUST_ALLOCA)))\n      bb_insn->alloca_flag |= MUST_ALLOCA;\n  }\n  if (((se = insn->ops[1].data) != NULL && (se->def->alloca_flag & MAY_ALLOCA))\n      || (insn->nops == 3 && (se = insn->ops[2].data) != NULL\n          && (se->def->alloca_flag & MAY_ALLOCA)))\n    bb_insn->alloca_flag |= MAY_ALLOCA;\n}\n\nstatic ssa_edge_t skip_move_ssa_edges (ssa_edge_t se, MIR_insn_t *def_insn) {\n  for (;;) {\n    gen_assert (se != NULL);\n    *def_insn = se->def->insn;\n    if (!move_p (*def_insn)) return se;\n    se = (*def_insn)->ops[1].data;\n  }\n}\n\nstatic MIR_insn_t get_def_disp (ssa_edge_t se, int64_t *disp) {\n  MIR_insn_t def_insn;\n\n  *disp = 0;\n  for (;;) {\n    se = skip_move_ssa_edges (se, &def_insn);\n    if ((def_insn->code != MIR_ADD && def_insn->code != MIR_ADDS && def_insn->code != MIR_SUB\n         && def_insn->code != MIR_SUBS)\n        || (se = def_insn->ops[2].data) == NULL || !se->def->gvn_val_const_p)\n      return def_insn;\n    int add_p = def_insn->code == MIR_ADD || def_insn->code == MIR_ADDS;\n    *disp += add_p ? se->def->gvn_val : -se->def->gvn_val;\n    se = def_insn->ops[1].data; /* new base */\n  }\n}\n\nstatic void new_mem_loc (gen_ctx_t gen_ctx, MIR_op_t *mem_op_ref, int flag) {\n  /* zero loc is fixed: */\n  int64_t disp;\n  mem_attr_t mem_attr;\n\n  if ((mem_op_ref->u.var_mem.nloc = (uint32_t) VARR_LENGTH (mem_attr_t, mem_attrs)) == 0)\n    mem_op_ref->u.var_mem.nloc = 1;\n  mem_attr.alloca_flag = flag;\n  mem_attr.type = mem_op_ref->u.var_mem.type;\n  mem_attr.alias = mem_op_ref->u.var_mem.alias;\n  mem_attr.nonalias = mem_op_ref->u.var_mem.nonalias;\n  mem_attr.disp_def_p = FALSE;\n  mem_attr.disp = 0;\n  mem_attr.def_insn = NULL;\n  if ((flag & MUST_ALLOCA) != 0) {\n    mem_attr.def_insn = get_def_disp (mem_op_ref->data, &disp);\n    mem_attr.disp_def_p = TRUE;\n    mem_attr.disp = disp;\n  }\n  if (VARR_LENGTH (mem_attr_t, mem_attrs) == 0) VARR_PUSH (mem_attr_t, mem_attrs, mem_attr);\n  DEBUG (2, {\n    fprintf (debug_file, \"    new m%lu\", (unsigned long) mem_op_ref->u.var_mem.nloc);\n    if (mem_attr.def_insn != NULL)\n      fprintf (debug_file, \"(def_insn=%u)\", ((bb_insn_t) mem_attr.def_insn->data)->index);\n    if (mem_attr.disp_def_p) fprintf (debug_file, \"(disp=%lld)\", (long long) mem_attr.disp);\n    if (flag)\n      fprintf (debug_file, \" is %s alloca based\",\n               flag & (MAY_ALLOCA | MUST_ALLOCA) ? \"may/must\"\n               : flag & MAY_ALLOCA               ? \"may\"\n                                                 : \"must\");\n    fprintf (debug_file, \"\\n\");\n  });\n  VARR_PUSH (mem_attr_t, mem_attrs, mem_attr);\n}\n\nstatic void update_mem_loc_alloca_flag (gen_ctx_t gen_ctx, size_t nloc, int flag) {\n  int old_flag;\n  mem_attr_t *mem_attr_ref;\n\n  gen_assert (VARR_LENGTH (mem_attr_t, mem_attrs) > nloc);\n  mem_attr_ref = &VARR_ADDR (mem_attr_t, mem_attrs)[nloc];\n  old_flag = mem_attr_ref->alloca_flag;\n  mem_attr_ref->alloca_flag = ((old_flag | flag) & MAY_ALLOCA) | (old_flag & flag & MUST_ALLOCA);\n  DEBUG (2, {\n    if (flag != old_flag) {\n      fprintf (debug_file, \"    m%lu \", (unsigned long) nloc);\n      if (!flag)\n        fprintf (debug_file, \"is no more alloca based\\n\");\n      else\n        fprintf (debug_file, \"becomes %s alloca based\\n\",\n                 flag & (MAY_ALLOCA | MUST_ALLOCA) ? \"may/must\"\n                 : flag & MAY_ALLOCA               ? \"may\"\n                                                   : \"must\");\n    }\n  });\n}\n\nstatic long remove_bb (gen_ctx_t gen_ctx, bb_t bb) {\n  MIR_insn_t insn;\n  bb_insn_t bb_insn, next_bb_insn;\n  long deleted_insns_num = 0;\n\n  gen_assert (bb->index != 2);\n  DEBUG (2, {\n    fprintf (debug_file, \"  BB%lu is unreachable and removed\\n\", (unsigned long) bb->index);\n  });\n  for (bb_insn = DLIST_HEAD (bb_insn_t, bb->bb_insns); bb_insn != NULL; bb_insn = next_bb_insn) {\n    next_bb_insn = DLIST_NEXT (bb_insn_t, bb_insn);\n    insn = bb_insn->insn;\n    gen_delete_insn (gen_ctx, insn);\n    deleted_insns_num++;\n  }\n  delete_bb (gen_ctx, bb);\n  return deleted_insns_num;\n}\n\nstatic void mark_unreachable_bbs (gen_ctx_t gen_ctx) {\n  bb_t bb = DLIST_EL (bb_t, curr_cfg->bbs, 2);\n\n  if (bb == NULL) return;\n  gen_assert (bb->index == 2);\n  bitmap_clear (temp_bitmap);\n  VARR_TRUNC (bb_t, worklist, 0);\n  VARR_PUSH (bb_t, worklist, bb);\n  bitmap_set_bit_p (temp_bitmap, bb->index);\n  for (bb = DLIST_EL (bb_t, curr_cfg->bbs, 2); bb != NULL; bb = DLIST_NEXT (bb_t, bb))\n    if (bb->reachable_p) {\n      VARR_PUSH (bb_t, worklist, bb);\n      bitmap_set_bit_p (temp_bitmap, bb->index);\n    }\n  while (VARR_LENGTH (bb_t, worklist) != 0) {\n    bb = VARR_POP (bb_t, worklist);\n    for (edge_t e = DLIST_HEAD (out_edge_t, bb->out_edges); e != NULL;\n         e = DLIST_NEXT (out_edge_t, e))\n      if (bitmap_set_bit_p (temp_bitmap, e->dst->index)) VARR_PUSH (bb_t, worklist, e->dst);\n  }\n}\n\nstatic long remove_unreachable_bbs (gen_ctx_t gen_ctx) {\n  long deleted_insns_num = 0;\n  bb_t next_bb;\n\n  mark_unreachable_bbs (gen_ctx);\n  for (bb_t bb = DLIST_EL (bb_t, curr_cfg->bbs, 2); bb != NULL; bb = next_bb) {\n    next_bb = DLIST_NEXT (bb_t, bb);\n    if (!bitmap_bit_p (temp_bitmap, bb->index)) deleted_insns_num += remove_bb (gen_ctx, bb);\n  }\n  return deleted_insns_num;\n}\n\nstatic void copy_gvn_info (bb_insn_t to, bb_insn_t from) {\n  to->gvn_val_const_p = from->gvn_val_const_p;\n  to->gvn_val = from->gvn_val;\n  to->alloca_flag = from->alloca_flag;\n}\n\nstatic void remove_copy (gen_ctx_t gen_ctx, MIR_insn_t insn) {\n  ssa_edge_t se, last_se;\n  bb_insn_t def;\n  int def_op_num;\n\n  gen_assert (move_p (insn) || (insn->code == MIR_PHI && insn->nops == 2));\n  se = insn->ops[1].data;\n  def = se->def;\n  def_op_num = se->def_op_num;\n  remove_ssa_edge (gen_ctx, se);\n  if ((last_se = def->insn->ops[def_op_num].data) != NULL)\n    while (last_se->next_use != NULL) last_se = last_se->next_use;\n  change_ssa_edge_list_def (insn->ops[0].data, def, def_op_num, insn->ops[0].u.var,\n                            insn->ops[1].u.var);\n  if (last_se != NULL)\n    last_se->next_use = insn->ops[0].data;\n  else\n    def->insn->ops[def_op_num].data = insn->ops[0].data;\n  if (insn->ops[0].data != NULL) ((ssa_edge_t) insn->ops[0].data)->prev_use = last_se;\n  insn->ops[0].data = NULL;\n  DEBUG (2, {\n    fprintf (debug_file, \"    Remove copy %-5lu\", (unsigned long) ((bb_insn_t) insn->data)->index);\n    print_bb_insn (gen_ctx, insn->data, FALSE);\n  });\n  gen_delete_insn (gen_ctx, insn);\n}\n\n/* we are at curr bb from start, return true if can go to start avoiding dst */\nstatic int reachable_without_visiting_bb_p (gen_ctx_t gen_ctx, bb_t curr, bb_t start,\n                                            bb_t exclude) {\n  if (curr == exclude || !bitmap_set_bit_p (temp_bitmap2, curr->index)) return FALSE;\n  for (edge_t e = DLIST_HEAD (out_edge_t, curr->out_edges); e != NULL;\n       e = DLIST_NEXT (out_edge_t, e))\n    if (e->dst == start || reachable_without_visiting_bb_p (gen_ctx, e->dst, start, exclude))\n      return TRUE;\n  return FALSE;\n}\n\nstatic int cycle_without_bb_visit_p (gen_ctx_t gen_ctx, bb_t start, bb_t exclude) {\n  bitmap_clear (temp_bitmap2);\n  return reachable_without_visiting_bb_p (gen_ctx, start, start, exclude);\n}\n\nstatic mem_expr_t find_first_available_mem_expr (mem_expr_t list, bitmap_t available_mem,\n                                                 int any_p) {\n  for (mem_expr_t curr = list; curr != NULL; curr = curr->next)\n    if (bitmap_bit_p (available_mem, ((bb_insn_t) curr->insn->data)->mem_index)\n        && (any_p || curr->insn->ops[0].mode == MIR_OP_VAR_MEM))\n      return curr;\n  return NULL;\n}\n\n/* Memory displacement to prefer for memory address recalculation instead.  */\n#ifndef TARGET_MAX_MEM_DISP\n#define TARGET_MAX_MEM_DISP 127\n#endif\n\n#ifndef TARGET_MIN_MEM_DISP\n#define TARGET_MIN_MEM_DISP -128\n#endif\n\nstatic void remove_unreachable_bb_edges (gen_ctx_t gen_ctx, bb_t bb, VARR (bb_t) * bbs) {\n  bb_t dst;\n  edge_t e, next_e;\n\n  VARR_TRUNC (bb_t, bbs, 0);\n  VARR_PUSH (bb_t, bbs, bb);\n  while (VARR_LENGTH (bb_t, bbs) != 0) {\n    bb = VARR_POP (bb_t, bbs);\n    DEBUG (2, {\n      fprintf (debug_file, \"  Deleting output edges of unreachable bb%lu\\n\",\n               (unsigned long) bb->index);\n    });\n    for (e = DLIST_HEAD (out_edge_t, bb->out_edges); e != NULL; e = next_e) {\n      next_e = DLIST_NEXT (out_edge_t, e);\n      remove_edge_phi_ops (gen_ctx, e);\n      dst = e->dst;\n      dst->flag = TRUE; /* to recalculate dst mem_av_in */\n      delete_edge (gen_ctx, e);\n      if (dst->index > 2 && DLIST_HEAD (in_edge_t, dst->in_edges) == NULL)\n        VARR_PUSH (bb_t, bbs, dst);\n    }\n  }\n}\n\nstatic void gvn_modify (gen_ctx_t gen_ctx) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  bb_t bb;\n  bb_insn_t bb_insn, mem_bb_insn, new_bb_insn, new_bb_insn2, next_bb_insn, expr_bb_insn;\n  MIR_reg_t temp_reg;\n  long gvn_insns_num = 0, ccp_insns_num = 0, deleted_branches_num = 0;\n  bitmap_t curr_available_mem = temp_bitmap, removed_mem = temp_bitmap3;\n  MIR_func_t func = curr_func_item->u.func;\n\n  full_escape_p = FALSE;\n  VARR_TRUNC (mem_attr_t, mem_attrs, 0);\n  bitmap_clear (removed_mem);\n  for (size_t i = 0; i < VARR_LENGTH (bb_t, worklist); i++)\n    VARR_GET (bb_t, worklist, i)->flag = FALSE;\n  while (VARR_LENGTH (bb_t, worklist) != 0) {\n    bb = VARR_POP (bb_t, worklist);\n    DEBUG (2, { fprintf (debug_file, \"  BB%lu:\\n\", (unsigned long) bb->index); });\n    if (bb->index > 2 && DLIST_HEAD (in_edge_t, bb->in_edges) == NULL) {\n      /* Unreachable bb because of branch transformation: remove output edges\n         recursively as it shorten phis in successors and this creates more opportunity for\n         optimizations. But don't remove insns as their output can be used in unreachable loops\n         (unreachable loops will be removed in jump optimization pass). */\n      remove_unreachable_bb_edges (gen_ctx, bb, pending);\n      continue;\n    }\n    /* Recalculate mem_avin and mem_av_out: */\n    if (DLIST_HEAD (in_edge_t, bb->in_edges) != NULL && bb->flag\n        && mem_av_con_func_n (gen_ctx, bb)) {\n      DEBUG (2, { fprintf (debug_file, \"   changed mem_avin\\n\"); });\n      bitmap_and_compl (bb->in, bb->in, removed_mem);\n      if (mem_av_trans_func (gen_ctx, bb)) {\n        DEBUG (2, { fprintf (debug_file, \"   changed mem_avout\\n\"); });\n        for (edge_t e = DLIST_HEAD (out_edge_t, bb->out_edges); e != NULL;\n             e = DLIST_NEXT (out_edge_t, e))\n          e->dst->flag = TRUE;\n      }\n    }\n    bitmap_and_compl (curr_available_mem, bb->in, removed_mem);\n    for (bb_insn = DLIST_HEAD (bb_insn_t, bb->bb_insns); bb_insn != NULL; bb_insn = next_bb_insn) {\n      expr_t e, new_e;\n      mem_expr_t prev_mem_expr, mem_expr;\n      MIR_op_t op;\n      int add_def_p, const_p, cont_p;\n      MIR_type_t type;\n      MIR_insn_code_t move_code;\n      MIR_insn_t mem_insn, new_insn, new_insn2, def_insn, after, insn = bb_insn->insn;\n      ssa_edge_t se, se2;\n      bb_insn_t def_bb_insn, new_bb_copy_insn;\n      int64_t val = 0, val2;\n\n      next_bb_insn = DLIST_NEXT (bb_insn_t, bb_insn);\n      if (insn->code == MIR_MOV\n          && (insn->ops[1].mode == MIR_OP_INT || insn->ops[1].mode == MIR_OP_UINT)) {\n        bb_insn->gvn_val_const_p = TRUE;\n        bb_insn->gvn_val = insn->ops[1].u.i;\n        print_bb_insn_value (gen_ctx, bb_insn);\n        continue;\n      }\n      if (MIR_call_code_p (insn->code)) bitmap_clear (curr_available_mem);\n      if (!gvn_insn_p (insn)) continue;\n      const_p = FALSE;\n      switch (insn->code) {\n      case MIR_PHI:\n        const_p = gvn_phi_val (bb_insn, &val);\n        if (const_p) break;\n        if (insn->nops == 2 && insn->ops[0].mode == MIR_OP_VAR && insn->ops[1].mode == MIR_OP_VAR\n            && MIR_reg_hard_reg_name (ctx, insn->ops[0].u.var - MAX_HARD_REG, func) == NULL\n            && MIR_reg_hard_reg_name (ctx, insn->ops[1].u.var - MAX_HARD_REG, func) == NULL) {\n          remove_copy (gen_ctx, insn);\n          continue;\n        }\n        bb_insn->gvn_val_const_p = FALSE;\n        bb_insn->gvn_val = val;\n        print_bb_insn_value (gen_ctx, bb_insn);\n        continue;\n      case MIR_EXT8: GVN_EXT (int8_t); break;\n      case MIR_EXT16: GVN_EXT (int16_t); break;\n      case MIR_EXT32: GVN_EXT (int32_t); break;\n      case MIR_UEXT8: GVN_EXT (uint8_t); break;\n      case MIR_UEXT16: GVN_EXT (uint16_t); break;\n      case MIR_UEXT32: GVN_EXT (uint32_t); break;\n\n      case MIR_NEG: GVN_IOP2 (-); break;\n      case MIR_NEGS: GVN_IOP2S (-); break;\n\n      case MIR_MUL: GVN_IOP3 (*); break;\n      case MIR_MULS: GVN_IOP3S (*); break;\n\n      case MIR_MULO: GVN_IOP3 (*); break;\n      case MIR_MULOS: GVN_IOP3S (*); break;\n      case MIR_UMULO: GVN_UOP3 (*); break;\n      case MIR_UMULOS: GVN_UOP3S (*); break;\n\n      case MIR_DIV: GVN_IOP30 (/); break;\n      case MIR_DIVS: GVN_IOP3S0 (/); break;\n      case MIR_UDIV: GVN_UOP30 (/); break;\n      case MIR_UDIVS: GVN_UOP3S0 (/); break;\n\n      case MIR_MOD: GVN_IOP30 (%); break;\n      case MIR_MODS: GVN_IOP3S0 (%); break;\n      case MIR_UMOD: GVN_UOP30 (%); break;\n      case MIR_UMODS:\n        GVN_UOP3S0 (%);\n        break;\n\n        /* The following insn can be involved in addres calculation too: */\n      case MIR_AND: GVN_IOP3 (&); goto set_alloca_flag;\n      case MIR_ANDS: GVN_IOP3S (&); goto set_alloca_flag;\n      case MIR_OR: GVN_IOP3 (|); goto set_alloca_flag;\n      case MIR_ORS: GVN_IOP3S (|); goto set_alloca_flag;\n      case MIR_XOR: GVN_IOP3 (^); goto set_alloca_flag;\n      case MIR_XORS:\n        GVN_IOP3S (^);\n      set_alloca_flag:\n        set_alloca_based_flag (bb_insn, FALSE);\n        break;\n\n      case MIR_LSH: GVN_IOP3 (<<); break;\n      case MIR_LSHS: GVN_IOP3S (<<); break;\n      case MIR_RSH: GVN_IOP3 (>>); break;\n      case MIR_RSHS: GVN_IOP3S (>>); break;\n      case MIR_URSH: GVN_UOP3 (>>); break;\n      case MIR_URSHS: GVN_UOP3S (>>); break;\n\n      case MIR_EQ: GVN_ICMP (==); break;\n      case MIR_EQS: GVN_ICMPS (==); break;\n      case MIR_NE: GVN_ICMP (!=); break;\n      case MIR_NES: GVN_ICMPS (!=); break;\n\n      case MIR_LT: GVN_ICMP (<); break;\n      case MIR_LTS: GVN_ICMPS (<); break;\n      case MIR_ULT: GVN_UCMP (<); break;\n      case MIR_ULTS: GVN_UCMPS (<); break;\n      case MIR_LE: GVN_ICMP (<=); break;\n      case MIR_LES: GVN_ICMPS (<=); break;\n      case MIR_ULE: GVN_UCMP (<=); break;\n      case MIR_ULES: GVN_UCMPS (<=); break;\n      case MIR_GT: GVN_ICMP (>); break;\n      case MIR_GTS: GVN_ICMPS (>); break;\n      case MIR_UGT: GVN_UCMP (>); break;\n      case MIR_UGTS: GVN_UCMPS (>); break;\n      case MIR_GE: GVN_ICMP (>=); break;\n      case MIR_GES: GVN_ICMPS (>=); break;\n      case MIR_UGE: GVN_UCMP (>=); break;\n      case MIR_UGES:\n        GVN_UCMPS (>=);\n        break;\n        /* special treatement for address canonization: */\n      case MIR_ADD:\n      case MIR_ADDO:\n        set_alloca_based_flag (bb_insn, TRUE);\n        GVN_IOP3 (+);\n        if (!const_p) goto canon_expr;\n        break;\n      case MIR_ADDS:\n      case MIR_ADDOS:\n        set_alloca_based_flag (bb_insn, TRUE);\n        GVN_IOP3S (+);\n        if (!const_p) goto canon_expr;\n        break;\n      case MIR_SUB:\n      case MIR_SUBO:\n        set_alloca_based_flag (bb_insn, TRUE);\n        GVN_IOP3 (-);\n        if (!const_p) goto canon_expr;\n        break;\n      case MIR_SUBS:\n      case MIR_SUBOS:\n        set_alloca_based_flag (bb_insn, TRUE);\n        GVN_IOP3S (-);\n        if (!const_p) goto canon_expr;\n        break;\n      canon_expr:\n        cont_p = TRUE;\n        if ((insn->code == MIR_ADD || insn->code == MIR_ADDS) && (se = insn->ops[1].data) != NULL\n            && se->def->gvn_val_const_p\n            && ((se2 = insn->ops[2].data) == NULL || !se2->def->gvn_val_const_p)) {\n          MIR_op_t temp = insn->ops[2];\n          insn->ops[2] = insn->ops[1];\n          insn->ops[1] = temp;\n          se->use_op_num = 2;\n          se2->use_op_num = 1;\n          DEBUG (2, {\n            fprintf (debug_file, \"  exchange ops of insn\");\n            MIR_output_insn (ctx, debug_file, insn, func, TRUE);\n          });\n        }\n        if (add_sub_const_insn_p (gen_ctx, insn, &val2) && (se = insn->ops[1].data) != NULL\n            && (def_insn = skip_moves (gen_ctx, se->def->insn)) != NULL\n            && add_sub_const_insn_p (gen_ctx, def_insn, &val)) {\n          /* r1=r0+const; ... r2=r1+const2 =>\n             temp = r0; r1=r0+const; ... r2=r1+const2;r2=temp+(const+const2): */\n          temp_reg = gen_new_temp_reg (gen_ctx, MIR_T_I64, func);\n          op = _MIR_new_var_op (ctx, temp_reg);\n          new_insn = MIR_new_insn (ctx, MIR_MOV, op, def_insn->ops[1]);\n          new_insn->ops[1].data = NULL;\n          gen_add_insn_before (gen_ctx, def_insn, new_insn);\n          new_bb_copy_insn = new_insn->data;\n          se = def_insn->ops[1].data;\n          def_bb_insn = se->def; /* ops[1] def */\n          add_ssa_edge (gen_ctx, def_bb_insn, se->def_op_num, new_bb_copy_insn, 1);\n          copy_gvn_info (new_bb_copy_insn, def_bb_insn);\n          DEBUG (2, {\n            fprintf (debug_file, \"  adding insn \");\n            MIR_output_insn (ctx, debug_file, new_insn, func, FALSE);\n            fprintf (debug_file, \"  before def insn \");\n            MIR_output_insn (ctx, debug_file, def_insn, func, TRUE);\n          });\n          print_bb_insn_value (gen_ctx, new_bb_copy_insn);\n          new_insn2 = NULL;\n          if (insn->code == MIR_ADDS || insn->code == MIR_SUBS) {\n            if ((uint32_t) val + (uint32_t) val2 == 0) {\n              new_insn = MIR_new_insn (ctx, MIR_MOV, insn->ops[0], op);\n            } else {\n              temp_reg = gen_new_temp_reg (gen_ctx, MIR_T_I64, func);\n              new_insn\n                = MIR_new_insn (ctx, MIR_MOV, _MIR_new_var_op (ctx, temp_reg),\n                                MIR_new_int_op (ctx, (int32_t) ((uint32_t) val + (uint32_t) val2)));\n              new_insn2\n                = MIR_new_insn (ctx, MIR_ADDS, insn->ops[0], op, _MIR_new_var_op (ctx, temp_reg));\n            }\n          } else {\n            if ((uint64_t) val + (uint64_t) val2 == 0) {\n              new_insn = MIR_new_insn (ctx, MIR_MOV, insn->ops[0], op);\n            } else {\n              temp_reg = gen_new_temp_reg (gen_ctx, MIR_T_I64, func);\n              new_insn\n                = MIR_new_insn (ctx, MIR_MOV, _MIR_new_var_op (ctx, temp_reg),\n                                MIR_new_int_op (ctx, (int64_t) ((uint64_t) val + (uint64_t) val2)));\n              new_insn2\n                = MIR_new_insn (ctx, MIR_ADD, insn->ops[0], op, _MIR_new_var_op (ctx, temp_reg));\n            }\n          }\n          new_bb_insn2 = NULL;\n          if (new_insn2 != NULL) {\n            gen_add_insn_after (gen_ctx, insn, new_insn2);\n            new_bb_insn2 = new_insn2->data;\n          }\n          gen_add_insn_after (gen_ctx, insn, new_insn);\n          new_bb_insn = new_insn->data;\n          if (new_insn2 != NULL) {\n            new_bb_insn->gvn_val_const_p = TRUE;\n            new_bb_insn->gvn_val = new_insn->ops[1].u.i;\n            add_ssa_edge (gen_ctx, new_bb_insn, 0, new_bb_insn2, 2);\n          }\n          redirect_def (gen_ctx, insn, (new_insn2 != NULL ? new_insn2 : new_insn), FALSE);\n          add_ssa_edge (gen_ctx, new_bb_copy_insn, 0,\n                        (new_insn2 != NULL ? new_bb_insn2 : new_bb_insn), 1);\n          DEBUG (2, {\n            fprintf (debug_file, \"  adding insn after:\");\n            MIR_output_insn (ctx, debug_file, new_insn, func, TRUE);\n            if (new_insn2 != NULL) {\n              fprintf (debug_file, \"  adding 2nd insn after:\");\n              MIR_output_insn (ctx, debug_file, new_insn2, func, TRUE);\n            }\n          });\n          if (new_insn2 != NULL) { /* start with modified add */\n            next_bb_insn = new_bb_insn;\n            continue;\n          }\n          set_alloca_based_flag (new_bb_copy_insn, TRUE);\n          cont_p = new_insn->code != MIR_MOV || new_insn->ops[1].mode != MIR_OP_VAR;\n          if (!cont_p) set_alloca_based_flag (new_bb_insn, TRUE);\n          insn = new_insn; /* to consider new insn next */\n          bb_insn = new_bb_insn;\n          next_bb_insn = DLIST_NEXT (bb_insn_t, bb_insn);\n        }\n        if (cont_p) break;\n        /* fall through */\n      case MIR_MOV:\n      case MIR_FMOV:\n      case MIR_DMOV:\n      case MIR_LDMOV:\n        if (insn->ops[0].mode == MIR_OP_VAR_MEM) { /* store */\n          if ((se = insn->ops[1].data) != NULL && se->def->alloca_flag) full_escape_p = TRUE;\n          se = insn->ops[0].data; /* address def actually */\n          mem_expr = find_mem_expr (gen_ctx, insn);\n          prev_mem_expr = find_first_available_mem_expr (mem_expr, curr_available_mem, FALSE);\n          /* If we can reach prev available store bb from itself w/o going through given bb then\n             it means it can be stores with different addresses and we just have the same\n             memory only for the last store and can not make dead store in prev expr bb.  It\n             is also not worth to reuse stored value as it will create a move from some loop\n             containing prev expr bb and not containing given bb.  Make new memory for such\n             case.  */\n          int new_mem_p\n            = (prev_mem_expr != NULL\n               && cycle_without_bb_visit_p (gen_ctx, ((bb_insn_t) prev_mem_expr->insn->data)->bb,\n                                            bb));\n          prev_mem_expr = find_first_available_mem_expr (mem_expr, curr_available_mem, TRUE);\n          def_bb_insn = ((ssa_edge_t) insn->ops[1].data)->def;\n          if (new_mem_p || prev_mem_expr == NULL) {\n            new_mem_loc (gen_ctx, &insn->ops[0], se->def->alloca_flag);\n          } else if (prev_mem_expr->insn->ops[0].mode\n                     == MIR_OP_VAR_MEM) { /* mem = x; ... ; mem=y */\n            insn->ops[0].u.var_mem.nloc = prev_mem_expr->insn->ops[0].u.var_mem.nloc;\n            update_mem_loc_alloca_flag (gen_ctx, insn->ops[0].u.var_mem.nloc, se->def->alloca_flag);\n          } else { /* x = mem; ...; mem = y */\n            gen_assert (prev_mem_expr->insn->ops[1].mode == MIR_OP_VAR_MEM);\n            insn->ops[0].u.var_mem.nloc = prev_mem_expr->insn->ops[1].u.var_mem.nloc;\n            update_mem_loc_alloca_flag (gen_ctx, insn->ops[0].u.var_mem.nloc, se->def->alloca_flag);\n            bb_insn_t prev_bb_insn = prev_mem_expr->insn->data;\n            if (def_bb_insn->gvn_val_const_p == prev_bb_insn->gvn_val_const_p\n                && def_bb_insn->gvn_val == prev_bb_insn->gvn_val) { /* x == y: remove insn */\n              gen_assert (def_bb_insn->alloca_flag == prev_bb_insn->alloca_flag);\n              DEBUG (2, {\n                fprintf (debug_file, \"  deleting \");\n                print_bb_insn (gen_ctx, insn->data, TRUE);\n              });\n              bitmap_clear_bit_p (curr_available_mem, bb_insn->mem_index);\n              bitmap_set_bit_p (removed_mem, bb_insn->mem_index);\n              ssa_delete_insn (gen_ctx, insn);\n              continue;\n            }\n          }\n          add_mem_insn (gen_ctx, insn);\n          update_mem_availability (gen_ctx, curr_available_mem, bb_insn);\n          copy_gvn_info (bb_insn, def_bb_insn);\n          print_bb_insn_value (gen_ctx, bb_insn);\n          continue;\n        } else if (insn->ops[1].mode == MIR_OP_VAR_MEM) { /* load */\n          if (insn->ops[0].data == NULL) continue;        /* dead load */\n          se = insn->ops[1].data;                         /* address def actually */\n          mem_expr = find_mem_expr (gen_ctx, insn);\n          mem_expr = find_first_available_mem_expr (mem_expr, curr_available_mem, TRUE);\n          if (mem_expr == NULL) {\n            new_mem_loc (gen_ctx, &insn->ops[1], se->def->alloca_flag);\n            add_mem_insn (gen_ctx, insn);\n          } else {\n            MIR_op_t *op_ref;\n            mem_insn = mem_expr->insn;\n            op_ref\n              = mem_insn->ops[0].mode == MIR_OP_VAR_MEM ? &mem_insn->ops[0] : &mem_insn->ops[1];\n            insn->ops[1].u.var_mem.nloc = op_ref->u.var_mem.nloc;\n            update_mem_loc_alloca_flag (gen_ctx, op_ref->u.var_mem.nloc, se->def->alloca_flag);\n            if (!bitmap_bit_p (curr_available_mem, (mem_bb_insn = mem_insn->data)->mem_index)\n                /* last available load can become dead: */\n                || (mem_insn->ops[1].mode == MIR_OP_VAR_MEM && mem_insn->ops[0].data == NULL)) {\n              add_mem_insn (gen_ctx, insn);\n            } else { /* (mem=x|x=mem); ...; r=mem => (mem=x|x=mem); t=x; ...; r=t */\n              copy_gvn_info (bb_insn, mem_bb_insn);\n              print_bb_insn_value (gen_ctx, bb_insn);\n              temp_reg = mem_expr->temp_reg;\n              add_def_p = temp_reg == MIR_NON_VAR;\n              if (add_def_p) {\n                mem_expr->temp_reg = temp_reg\n                  = get_expr_temp_reg (gen_ctx, mem_expr->insn, &mem_expr->temp_reg);\n                new_insn = MIR_new_insn (ctx, insn->code, _MIR_new_var_op (ctx, temp_reg),\n                                         op_ref == &mem_insn->ops[0] ? mem_insn->ops[1]\n                                                                     : mem_insn->ops[0]);\n                new_insn->ops[1].data = NULL; /* remove ssa edge taken from load/store op */\n                gen_add_insn_after (gen_ctx, mem_insn, new_insn);\n                new_bb_insn = new_insn->data;\n                copy_gvn_info (new_bb_insn, mem_bb_insn);\n                se = op_ref == &mem_insn->ops[0] ? mem_insn->ops[1].data : mem_insn->ops[0].data;\n                add_ssa_edge (gen_ctx, se->def, se->def_op_num, new_bb_insn, 1);\n                DEBUG (2, {\n                  fprintf (debug_file, \"  adding insn \");\n                  MIR_output_insn (ctx, debug_file, new_insn, func, FALSE);\n                  fprintf (debug_file, \"  after def insn \");\n                  MIR_output_insn (ctx, debug_file, mem_insn, func, TRUE);\n                });\n              }\n              bitmap_clear_bit_p (curr_available_mem, bb_insn->mem_index);\n              bitmap_set_bit_p (removed_mem, bb_insn->mem_index);\n              remove_ssa_edge (gen_ctx, (ssa_edge_t) insn->ops[1].data);\n              insn->ops[1] = _MIR_new_var_op (ctx, temp_reg); /* changing mem */\n              def_insn = DLIST_NEXT (MIR_insn_t, mem_insn);\n              add_ssa_edge (gen_ctx, def_insn->data, 0, bb_insn, 1);\n              gvn_insns_num++;\n              DEBUG (2, {\n                fprintf (debug_file, \"  changing curr insn to \");\n                MIR_output_insn (ctx, debug_file, insn, func, TRUE);\n              });\n              continue;\n            }\n          }\n          update_mem_availability (gen_ctx, curr_available_mem, bb_insn);\n        } else if (move_p (insn) && (se = insn->ops[1].data) != NULL && !fake_insn_p (se->def)\n                   && (se = se->def->insn->ops[se->def_op_num].data) != NULL && se->next_use == NULL\n                   && MIR_reg_hard_reg_name (ctx, insn->ops[0].u.var - MAX_HARD_REG, func) == NULL\n                   && MIR_reg_hard_reg_name (ctx, insn->ops[1].u.var - MAX_HARD_REG, func)\n                        == NULL) {\n          /* one source for definition: remove copy */\n          gen_assert (se->use == bb_insn && se->use_op_num == 1);\n          remove_copy (gen_ctx, insn);\n          continue;\n        }\n        break;\n      case MIR_BT:\n      case MIR_BTS:\n        const_p = get_gvn_op (insn, 1, &val);\n        if (const_p && insn->code == MIR_BTS) val = (int32_t) val;\n        break;\n      case MIR_BF:\n      case MIR_BFS:\n        const_p = get_gvn_op (insn, 1, &val);\n        if (const_p) {\n          if (insn->code == MIR_BF)\n            val = !val;\n          else\n            val = !(int32_t) val;\n        }\n        break;\n      case MIR_BEQ: GVN_ICMP (==); break;\n      case MIR_BEQS: GVN_ICMPS (==); break;\n      case MIR_BNE: GVN_ICMP (!=); break;\n      case MIR_BNES: GVN_ICMPS (!=); break;\n\n      case MIR_BLT: GVN_ICMP (<); break;\n      case MIR_BLTS: GVN_ICMPS (<); break;\n      case MIR_UBLT: GVN_UCMP (<); break;\n      case MIR_UBLTS: GVN_UCMPS (<); break;\n      case MIR_BLE: GVN_ICMP (<=); break;\n      case MIR_BLES: GVN_ICMPS (<=); break;\n      case MIR_UBLE: GVN_UCMP (<=); break;\n      case MIR_UBLES: GVN_UCMPS (<=); break;\n      case MIR_BGT: GVN_ICMP (>); break;\n      case MIR_BGTS: GVN_ICMPS (>); break;\n      case MIR_UBGT: GVN_UCMP (>); break;\n      case MIR_UBGTS: GVN_UCMPS (>); break;\n      case MIR_BGE: GVN_ICMP (>=); break;\n      case MIR_BGES: GVN_ICMPS (>=); break;\n      case MIR_UBGE: GVN_UCMP (>=); break;\n      case MIR_UBGES: GVN_UCMPS (>=); break;\n      default: break;\n      }\n      if (const_p) {\n        ccp_insns_num++;\n        print_bb_insn_value (gen_ctx, bb_insn);\n        if (MIR_any_branch_code_p (insn->code)) {\n          gen_assert (insn->code != MIR_SWITCH);\n          if (val == 0) {\n            DEBUG (2, {\n              fprintf (debug_file, \"  removing branch insn \");\n              MIR_output_insn (ctx, debug_file, insn, curr_func_item->u.func, TRUE);\n              fprintf (debug_file, \"\\n\");\n            });\n            ssa_delete_insn (gen_ctx, insn);\n            edge_t edge = DLIST_EL (out_edge_t, bb->out_edges, 1);\n            remove_edge_phi_ops (gen_ctx, edge);\n            edge->dst->flag = TRUE; /* to recalculate dst mem_av_in */\n            delete_edge (gen_ctx, edge);\n            deleted_branches_num++;\n          } else {\n            new_insn = MIR_new_insn (ctx, MIR_JMP, insn->ops[0]); /* label is always 0-th op */\n            DEBUG (2, {\n              fprintf (debug_file, \"  changing branch insn \");\n              MIR_output_insn (ctx, debug_file, insn, curr_func_item->u.func, FALSE);\n              fprintf (debug_file, \" onto jump insn \");\n              MIR_output_insn (ctx, debug_file, new_insn, curr_func_item->u.func, TRUE);\n              fprintf (debug_file, \"\\n\");\n            });\n            MIR_insert_insn_before (ctx, curr_func_item, insn, new_insn);\n            remove_insn_ssa_edges (gen_ctx, insn);\n            MIR_remove_insn (ctx, curr_func_item, insn);\n            new_insn->data = bb_insn;\n            bb_insn->insn = new_insn;\n            edge_t edge = DLIST_EL (out_edge_t, bb->out_edges, 0);\n            remove_edge_phi_ops (gen_ctx, edge);\n            edge->dst->flag = TRUE; /* to recalculate dst mem_av_in */\n            delete_edge (gen_ctx, edge);\n          }\n        } else { /* x=... and x is const => x=...; x=const */\n          new_insn = MIR_new_insn (ctx, MIR_MOV, insn->ops[0], MIR_new_int_op (ctx, val));\n          after = insn->code == MIR_PHI ? get_last_bb_phi_insn (insn) : insn;\n          gen_add_insn_after (gen_ctx, after, new_insn);\n          new_bb_insn = new_insn->data;\n          redirect_def (gen_ctx, insn, new_insn, FALSE);\n          new_bb_insn->gvn_val_const_p = TRUE;\n          new_bb_insn->gvn_val = val;\n          gvn_insns_num++;\n          DEBUG (2, {\n            fprintf (debug_file, \"  Adding insn after:\");\n            MIR_output_insn (ctx, debug_file, new_insn, func, TRUE);\n          });\n          print_bb_insn_value (gen_ctx, new_bb_insn);\n        }\n        continue;\n      }\n      if (MIR_any_branch_code_p (insn->code) || insn->code == MIR_PHI) continue;\n      e = NULL;\n      if (move_p (insn)) {\n        def_bb_insn = ((ssa_edge_t) insn->ops[1].data)->def;\n        copy_gvn_info (bb_insn, def_bb_insn);\n      } else if (!MIR_overflow_insn_code_p (insn->code)) {\n        /* r=e; ...; x=e => r=e; t=r; ...; x=e; x=t */\n        if (!find_expr (gen_ctx, insn, &e)) {\n          e = add_expr (gen_ctx, insn, FALSE);\n          DEBUG (2, { print_expr (gen_ctx, e, \"Adding\"); });\n        } else if (move_code_p (insn->code) && insn->ops[1].mode == MIR_OP_VAR_MEM\n                   && !bitmap_bit_p (curr_available_mem, ((bb_insn_t) e->insn->data)->mem_index)) {\n          e = add_expr (gen_ctx, insn, TRUE);\n          DEBUG (2, { print_expr (gen_ctx, e, \"Replacing\"); });\n        }\n        bb_insn->gvn_val_const_p = FALSE;\n        bb_insn->gvn_val = e->num;\n        bb_insn->alloca_flag = ((bb_insn_t) e->insn->data)->alloca_flag;\n      }\n      print_bb_insn_value (gen_ctx, bb_insn);\n      if (e == NULL || e->insn == insn || (imm_move_p (insn) && insn->ops[1].mode != MIR_OP_REF))\n        continue;\n      if (MIR_addr_code_p (insn->code)) {\n        continue;\n      } else if ((insn->code == MIR_ADD || insn->code == MIR_SUB)\n                 && (se = insn->ops[0].data) != NULL && se->next_use == NULL\n                 && se->use->insn->ops[se->use_op_num].mode == MIR_OP_VAR_MEM\n                 && (((se2 = insn->ops[2].data) != NULL && imm_move_p (se2->def->insn))\n                     || (insn->code == MIR_ADD && (se2 = insn->ops[1].data) != NULL\n                         && imm_move_p (se2->def->insn)))) {\n        /* Do not recalculate reg + const if it is only used in address: */\n        int64_t disp = se2->def->insn->ops[1].u.i;\n        if (insn->code == MIR_SUB) disp = -disp;\n        if (TARGET_MIN_MEM_DISP <= disp && disp <= TARGET_MAX_MEM_DISP) continue;\n      }\n      expr_bb_insn = e->insn->data;\n      if (bb->index != expr_bb_insn->bb->index\n          && !bitmap_bit_p (bb->dom_in, expr_bb_insn->bb->index))\n        continue;\n      add_def_p = e->temp_reg == MIR_NON_VAR;\n      temp_reg = get_expr_temp_reg (gen_ctx, e->insn, &e->temp_reg);\n      op = _MIR_new_var_op (ctx, temp_reg);\n      type = MIR_reg_type (ctx, temp_reg - MAX_HARD_REG, func);\n#ifndef NDEBUG\n      int out_p;\n      MIR_insn_op_mode (ctx, insn, 0, &out_p); /* result here is always 0-th op */\n      gen_assert (out_p);\n#endif\n      move_code = get_move_code (type);\n      if (add_def_p) {\n        gen_assert (e->insn->ops[0].mode == MIR_OP_VAR);\n        new_insn = MIR_new_insn (ctx, move_code, op, _MIR_new_var_op (ctx, e->insn->ops[0].u.var));\n        gen_add_insn_after (gen_ctx, e->insn, new_insn);\n        new_bb_insn = new_insn->data;\n        redirect_def (gen_ctx, e->insn, new_insn, TRUE);\n        if (!find_expr (gen_ctx, new_insn, &new_e)) new_e = add_expr (gen_ctx, new_insn, FALSE);\n        new_bb_insn->gvn_val_const_p = FALSE;\n        new_bb_insn->gvn_val = e->num;\n        new_bb_insn->alloca_flag = ((bb_insn_t) e->insn->data)->alloca_flag;\n        DEBUG (2, {\n          fprintf (debug_file, \"  adding insn \");\n          MIR_output_insn (ctx, debug_file, new_insn, func, FALSE);\n          fprintf (debug_file, \"  after def insn \");\n          MIR_output_insn (ctx, debug_file, e->insn, func, TRUE);\n        });\n      }\n      new_insn = MIR_new_insn (ctx, move_code, insn->ops[0], op);\n      gen_add_insn_after (gen_ctx, insn, new_insn);\n      new_bb_insn = new_insn->data;\n      redirect_def (gen_ctx, insn, new_insn, FALSE);\n      def_insn = DLIST_NEXT (MIR_insn_t, e->insn);\n      add_ssa_edge (gen_ctx, def_insn->data, 0, new_insn->data, 1);\n      if (!find_expr (gen_ctx, new_insn, &new_e)) new_e = add_expr (gen_ctx, new_insn, FALSE);\n      new_bb_insn->gvn_val_const_p = FALSE;\n      new_bb_insn->gvn_val = e->num;\n      new_bb_insn->alloca_flag = ((bb_insn_t) e->insn->data)->alloca_flag;\n      gvn_insns_num++;\n      DEBUG (2, {\n        fprintf (debug_file, \"  adding insn \");\n        MIR_output_insn (ctx, debug_file, new_insn, func, FALSE);\n        fprintf (debug_file, \"  after use insn \");\n        MIR_output_insn (ctx, debug_file, insn, func, TRUE);\n      });\n    }\n  }\n  DEBUG (1, {\n    fprintf (debug_file,\n             \"%5ld found GVN redundant insns, %ld ccp insns, %ld deleted \"\n             \"branches\\n\",\n             gvn_insns_num, ccp_insns_num, deleted_branches_num);\n  });\n}\n\nstatic void gvn (gen_ctx_t gen_ctx) {\n  calculate_memory_availability (gen_ctx);\n  calculate_dominators (gen_ctx);\n  VARR_TRUNC (bb_t, worklist, 0);\n  for (bb_t bb = DLIST_HEAD (bb_t, curr_cfg->bbs); bb != NULL; bb = DLIST_NEXT (bb_t, bb))\n    VARR_PUSH (bb_t, worklist, bb);\n  qsort (VARR_ADDR (bb_t, worklist), VARR_LENGTH (bb_t, worklist), sizeof (bb_t), post_cmp);\n  gvn_modify (gen_ctx);\n}\n\nstatic void gvn_clear (gen_ctx_t gen_ctx) {\n  HTAB_CLEAR (expr_t, expr_tab);\n  while (VARR_LENGTH (expr_t, exprs) != 0) gen_free (gen_ctx, VARR_POP (expr_t, exprs));\n  HTAB_CLEAR (mem_expr_t, mem_expr_tab);\n  while (VARR_LENGTH (mem_expr_t, mem_exprs) != 0) gen_free (gen_ctx, VARR_POP (mem_expr_t, mem_exprs));\n}\n\nstatic void init_gvn (gen_ctx_t gen_ctx) {\n  MIR_alloc_t alloc = gen_alloc (gen_ctx);\n  MIR_context_t ctx = gen_ctx->ctx;\n\n  gen_ctx->gvn_ctx = gen_malloc (gen_ctx, sizeof (struct gvn_ctx));\n  VARR_CREATE (expr_t, exprs, alloc, 512);\n  HTAB_CREATE (expr_t, expr_tab, alloc, 1024, expr_hash, expr_eq, gen_ctx);\n  temp_mem_insn\n    = MIR_new_insn (ctx, MIR_MOV,\n                    _MIR_new_var_mem_op (ctx, MIR_T_I64, 0, MIR_NON_VAR, MIR_NON_VAR, 0),\n                    _MIR_new_var_op (ctx, 0));\n  VARR_CREATE (mem_expr_t, mem_exprs, alloc, 256);\n  HTAB_CREATE (mem_expr_t, mem_expr_tab, alloc, 512, mem_expr_hash, mem_expr_eq, gen_ctx);\n  VARR_CREATE (insn_nop_pair_t, insn_nop_pairs, alloc, 16);\n}\n\nstatic void finish_gvn (gen_ctx_t gen_ctx) {\n  VARR_DESTROY (expr_t, exprs);\n  HTAB_DESTROY (expr_t, expr_tab);\n  gen_free (gen_ctx, temp_mem_insn); /* ??? */\n  VARR_DESTROY (mem_expr_t, mem_exprs);\n  HTAB_DESTROY (mem_expr_t, mem_expr_tab);\n  VARR_DESTROY (insn_nop_pair_t, insn_nop_pairs);\n  gen_free (gen_ctx, gen_ctx->gvn_ctx);\n  gen_ctx->gvn_ctx = NULL;\n}\n\n/* New page */\n\n/* Dead store elimination */\n\n#define mem_live_in in\n#define mem_live_out out\n#define mem_live_gen gen\n#define mem_live_kill kill\n\nstatic void mem_live_con_func_0 (bb_t bb) {\n  if (bb->index != 1) bitmap_clear (bb->mem_live_in);\n}\n\nstatic int mem_live_con_func_n (gen_ctx_t gen_ctx MIR_UNUSED, bb_t bb) {\n  edge_t e;\n  int change_p = FALSE;\n\n  for (e = DLIST_HEAD (out_edge_t, bb->out_edges); e != NULL; e = DLIST_NEXT (out_edge_t, e))\n    change_p |= bitmap_ior (bb->mem_live_out, bb->mem_live_out, e->dst->mem_live_in);\n  return change_p;\n}\n\nstatic int mem_live_trans_func (gen_ctx_t gen_ctx MIR_UNUSED, bb_t bb) {\n  return bitmap_ior_and_compl (bb->mem_live_in, bb->mem_live_gen, bb->mem_live_out,\n                               bb->mem_live_kill);\n}\n\nstatic int alloca_arg_p (gen_ctx_t gen_ctx MIR_UNUSED, MIR_insn_t call_insn) {\n  MIR_proto_t proto;\n  ssa_edge_t se;\n\n  gen_assert (MIR_call_code_p (call_insn->code) && call_insn->ops[0].mode == MIR_OP_REF\n              && call_insn->ops[0].u.ref->item_type == MIR_proto_item);\n  proto = call_insn->ops[0].u.ref->u.proto;\n  for (size_t i = proto->nres + 1; i < call_insn->nops; i++) {\n    if (call_insn->ops[i].mode != MIR_OP_VAR && call_insn->ops[i].mode != MIR_OP_VAR_MEM) continue;\n    if ((se = call_insn->ops[i].data) == NULL) continue;\n    if ((se->def->alloca_flag & MUST_ALLOCA) || (se->def->alloca_flag & MAY_ALLOCA)) return TRUE;\n  }\n  return FALSE;\n}\n\nstatic void update_call_mem_live (gen_ctx_t gen_ctx, bitmap_t mem_live, MIR_insn_t call_insn) {\n  gen_assert (MIR_call_code_p (call_insn->code));\n  gen_assert (call_insn->ops[0].mode == MIR_OP_REF\n              && call_insn->ops[0].u.ref->item_type == MIR_proto_item);\n  if (full_escape_p || alloca_arg_p (gen_ctx, call_insn)) {\n    bitmap_set_bit_range_p (mem_live, 1, VARR_LENGTH (mem_attr_t, mem_attrs));\n  } else {\n    mem_attr_t *mem_attr_addr = VARR_ADDR (mem_attr_t, mem_attrs);\n\n    for (size_t i = 1; i < VARR_LENGTH (mem_attr_t, mem_attrs); i++)\n      if (!mem_attr_addr[i].alloca_flag) bitmap_set_bit_p (mem_live, i);\n  }\n}\n\nstatic int alloca_mem_intersect_p (gen_ctx_t gen_ctx, uint32_t nloc1, MIR_type_t type1,\n                                   uint32_t nloc2, MIR_type_t type2) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  mem_attr_t *mem_attr_ref1 = &VARR_ADDR (mem_attr_t, mem_attrs)[nloc1];\n  mem_attr_t *mem_attr_ref2 = &VARR_ADDR (mem_attr_t, mem_attrs)[nloc2];\n  int64_t disp1, disp2, size1, size2;\n\n  gen_assert (nloc1 != 0 && nloc2 != 0);\n  if (!mem_attr_ref1->disp_def_p || !mem_attr_ref2->disp_def_p) return TRUE;\n  if (mem_attr_ref1->def_insn == NULL || mem_attr_ref1->def_insn != mem_attr_ref2->def_insn)\n    return TRUE;\n  disp1 = mem_attr_ref1->disp;\n  disp2 = mem_attr_ref2->disp;\n  size1 = _MIR_type_size (ctx, type1);\n  size2 = _MIR_type_size (ctx, type2);\n  if (disp2 <= disp1 && disp1 < disp2 + size2) return TRUE;\n  return disp1 <= disp2 && disp2 < disp1 + size1;\n}\n\nstatic void make_live_from_mem (gen_ctx_t gen_ctx, MIR_op_t *mem_ref, bitmap_t gen, bitmap_t kill,\n                                int must_alloca_p) {\n  mem_attr_t *mem_attr_addr = VARR_ADDR (mem_attr_t, mem_attrs);\n\n  gen_assert (mem_ref->mode == MIR_OP_VAR_MEM);\n  for (size_t i = 1; i < VARR_LENGTH (mem_attr_t, mem_attrs); i++) {\n    if (!may_alias_p (mem_ref->u.var_mem.alias, mem_attr_addr[i].alias, mem_ref->u.var_mem.nonalias,\n                      mem_attr_addr[i].nonalias))\n      continue;\n    if (must_alloca_p && (mem_attr_addr[i].alloca_flag & MUST_ALLOCA)\n        && !alloca_mem_intersect_p (gen_ctx, mem_ref->u.var_mem.nloc, mem_ref->u.var_mem.type,\n                                    (uint32_t) i, mem_attr_addr[i].type))\n      continue;\n    /* all aliased but unintersected must alloca: */\n    bitmap_set_bit_p (gen, i);\n    if (kill != NULL) bitmap_clear_bit_p (kill, i);\n  }\n}\n\nstatic MIR_insn_t initiate_bb_mem_live_info (gen_ctx_t gen_ctx, MIR_insn_t bb_tail_insn) {\n  bb_t bb = get_insn_bb (gen_ctx, bb_tail_insn);\n  MIR_insn_t insn;\n  ssa_edge_t se;\n  uint32_t nloc;\n\n  for (insn = bb_tail_insn; insn != NULL && get_insn_bb (gen_ctx, insn) == bb;\n       insn = DLIST_PREV (MIR_insn_t, insn)) {\n    if (MIR_call_code_p (insn->code)) update_call_mem_live (gen_ctx, bb->mem_live_gen, insn);\n    if (!move_code_p (insn->code)) continue;\n    if (insn->ops[0].mode == MIR_OP_VAR_MEM) { /* store */\n      if ((nloc = insn->ops[0].u.var_mem.nloc) != 0) {\n        bitmap_clear_bit_p (bb->mem_live_gen, nloc);\n        bitmap_set_bit_p (bb->mem_live_kill, nloc);\n      }\n    } else if (insn->ops[1].mode == MIR_OP_VAR_MEM) { /* load */\n      if ((nloc = insn->ops[1].u.var_mem.nloc) != 0) {\n        bitmap_set_bit_p (bb->mem_live_gen, nloc);\n        bitmap_clear_bit_p (bb->mem_live_kill, nloc);\n        se = insn->ops[1].data;\n        make_live_from_mem (gen_ctx, &insn->ops[1], bb->mem_live_gen, bb->mem_live_kill,\n                            se != NULL && (se->def->alloca_flag & MUST_ALLOCA));\n      } else {\n        bitmap_set_bit_range_p (bb->mem_live_gen, 1, VARR_LENGTH (mem_attr_t, mem_attrs));\n      }\n    }\n  }\n  return insn;\n}\n\nstatic void initiate_mem_live_info (gen_ctx_t gen_ctx) {\n  bb_t exit_bb = DLIST_EL (bb_t, curr_cfg->bbs, 1);\n  mem_attr_t *mem_attr_addr;\n\n  for (bb_t bb = DLIST_HEAD (bb_t, curr_cfg->bbs); bb != NULL; bb = DLIST_NEXT (bb_t, bb)) {\n    gen_assert (bb->mem_live_in != NULL && bb->mem_live_out != NULL && bb->mem_live_gen != NULL\n                && bb->mem_live_kill != NULL);\n    bitmap_clear (bb->mem_live_in);\n    bitmap_clear (bb->mem_live_out);\n    bitmap_clear (bb->mem_live_gen);\n    bitmap_clear (bb->mem_live_kill);\n  }\n  for (MIR_insn_t tail = DLIST_TAIL (MIR_insn_t, curr_func_item->u.func->insns); tail != NULL;)\n    tail = initiate_bb_mem_live_info (gen_ctx, tail);\n  mem_attr_addr = VARR_ADDR (mem_attr_t, mem_attrs);\n  for (size_t i = 1; i < VARR_LENGTH (mem_attr_t, mem_attrs); i++) {\n    if (mem_attr_addr[i].alloca_flag & MUST_ALLOCA) continue; /* skip alloca memory */\n    bitmap_set_bit_p (exit_bb->mem_live_in, i);\n    bitmap_set_bit_p (exit_bb->mem_live_out, i);\n  }\n}\n\nstatic void print_mem_bb_live_info (gen_ctx_t gen_ctx, bb_t bb) {\n  fprintf (debug_file, \"BB %3lu:\\n\", (unsigned long) bb->index);\n  output_bitmap (gen_ctx, \"   Mem live in:\", bb->mem_live_in, FALSE, NULL);\n  output_bitmap (gen_ctx, \"   Mem live out:\", bb->mem_live_out, FALSE, NULL);\n  output_bitmap (gen_ctx, \"   Mem live gen:\", bb->mem_live_gen, FALSE, NULL);\n  output_bitmap (gen_ctx, \"   Mem live kill:\", bb->mem_live_kill, FALSE, NULL);\n}\n\nstatic void calculate_mem_live_info (gen_ctx_t gen_ctx) {\n  initiate_mem_live_info (gen_ctx);\n  solve_dataflow (gen_ctx, FALSE, mem_live_con_func_0, mem_live_con_func_n, mem_live_trans_func);\n  DEBUG (2, {\n    for (bb_t bb = DLIST_HEAD (bb_t, curr_cfg->bbs); bb != NULL; bb = DLIST_NEXT (bb_t, bb))\n      print_mem_bb_live_info (gen_ctx, bb);\n  });\n}\n\nstatic void dse (gen_ctx_t gen_ctx) {\n  MIR_insn_t insn;\n  uint32_t nloc;\n  long dead_stores_num = 0;\n  ssa_edge_t se;\n  bb_insn_t bb_insn, prev_bb_insn;\n  bitmap_t live = temp_bitmap;\n\n  calculate_mem_live_info (gen_ctx);\n  for (bb_t bb = DLIST_HEAD (bb_t, curr_cfg->bbs); bb != NULL; bb = DLIST_NEXT (bb_t, bb)) {\n    bitmap_copy (live, bb->mem_live_out);\n    for (bb_insn = DLIST_TAIL (bb_insn_t, bb->bb_insns); bb_insn != NULL; bb_insn = prev_bb_insn) {\n      prev_bb_insn = DLIST_PREV (bb_insn_t, bb_insn);\n      insn = bb_insn->insn;\n      if (MIR_call_code_p (insn->code)) update_call_mem_live (gen_ctx, live, insn);\n      if (!move_code_p (insn->code)) continue;\n      if (insn->ops[0].mode == MIR_OP_VAR_MEM) { /* store */\n        if ((nloc = insn->ops[0].u.var_mem.nloc) != 0) {\n          if (!bitmap_clear_bit_p (live, nloc)) {\n            DEBUG (2, {\n              fprintf (debug_file, \"Removing dead store \");\n              print_bb_insn (gen_ctx, bb_insn, FALSE);\n            });\n            ssa_delete_insn (gen_ctx, insn);\n            dead_stores_num++;\n          }\n        }\n      } else if (insn->ops[1].mode == MIR_OP_VAR_MEM) { /* load */\n        if ((nloc = insn->ops[1].u.var_mem.nloc) != 0) {\n          bitmap_set_bit_p (live, nloc);\n          se = insn->ops[1].data;\n          make_live_from_mem (gen_ctx, &insn->ops[1], live, NULL,\n                              se != NULL && (se->def->alloca_flag & MUST_ALLOCA));\n        } else {\n          bitmap_set_bit_range_p (live, 1, VARR_LENGTH (mem_attr_t, mem_attrs));\n        }\n      }\n    }\n  }\n  DEBUG (1, { fprintf (debug_file, \"%5ld removed dead stores\\n\", dead_stores_num); });\n}\n\n#undef mem_live_in\n#undef mem_live_out\n#undef mem_live_gen\n#undef mem_live_kill\n\n/* New Page */\n\n/* SSA dead code elimination */\n\nstatic int reachable_bo_exists_p (bb_insn_t bb_insn) {\n  for (; bb_insn != NULL; bb_insn = DLIST_NEXT (bb_insn_t, bb_insn))\n    if (bb_insn->insn->code == MIR_BO || bb_insn->insn->code == MIR_UBO\n        || bb_insn->insn->code == MIR_BNO || bb_insn->insn->code == MIR_UBNO)\n      return TRUE;\n    else if (bb_insn->insn->code != MIR_MOV && bb_insn->insn->code != MIR_EXT32\n             && bb_insn->insn->code != MIR_UEXT32)\n      break;\n  return FALSE;\n}\n\nstatic int ssa_dead_insn_p (gen_ctx_t gen_ctx, bb_insn_t bb_insn) {\n  MIR_insn_t insn = bb_insn->insn;\n  int op_num, output_exists_p = FALSE;\n  MIR_reg_t var;\n  insn_var_iterator_t iter;\n  ssa_edge_t ssa_edge;\n\n  /* check control insns with possible output: */\n  if (MIR_call_code_p (insn->code) || insn->code == MIR_ALLOCA || insn->code == MIR_BSTART\n      || insn->code == MIR_VA_START || insn->code == MIR_VA_ARG\n      || (insn->nops > 0 && insn->ops[0].mode == MIR_OP_VAR\n          && (insn->ops[0].u.var == FP_HARD_REG || insn->ops[0].u.var == SP_HARD_REG)))\n    return FALSE;\n  if (fake_insn_p (bb_insn)) return FALSE;\n  FOREACH_OUT_INSN_VAR (gen_ctx, iter, insn, var, op_num) {\n    output_exists_p = TRUE;\n    if (insn->ops[op_num].mode == MIR_OP_VAR_MEM || (ssa_edge = insn->ops[op_num].data) != NULL)\n      return FALSE;\n  }\n  if (!MIR_overflow_insn_code_p (insn->code)\n      || !reachable_bo_exists_p (DLIST_NEXT (bb_insn_t, bb_insn)))\n    return output_exists_p;\n  return FALSE;\n}\n\nstatic int ssa_delete_insn_if_dead_p (gen_ctx_t gen_ctx, bb_insn_t bb_insn) {\n  if (!ssa_dead_insn_p (gen_ctx, bb_insn)) return FALSE;\n  DEBUG (2, {\n    fprintf (debug_file, \"  deleting now dead insn \");\n    print_bb_insn (gen_ctx, bb_insn, FALSE);\n  });\n  ssa_delete_insn (gen_ctx, bb_insn->insn);\n  return TRUE;\n}\n\nstatic void ssa_dead_code_elimination (gen_ctx_t gen_ctx) {\n  MIR_insn_t insn;\n  bb_insn_t bb_insn, def;\n  int op_num;\n  MIR_reg_t var;\n  insn_var_iterator_t iter;\n  ssa_edge_t ssa_edge;\n  long dead_insns_num = 0;\n\n  DEBUG (2, { fprintf (debug_file, \"+++++++++++++Dead code elimination:\\n\"); });\n  VARR_TRUNC (bb_insn_t, temp_bb_insns, 0);\n  for (bb_t bb = DLIST_HEAD (bb_t, curr_cfg->bbs); bb != NULL; bb = DLIST_NEXT (bb_t, bb))\n    for (bb_insn = DLIST_HEAD (bb_insn_t, bb->bb_insns); bb_insn != NULL;\n         bb_insn = DLIST_NEXT (bb_insn_t, bb_insn))\n      if (ssa_dead_insn_p (gen_ctx, bb_insn)) VARR_PUSH (bb_insn_t, temp_bb_insns, bb_insn);\n  while (VARR_LENGTH (bb_insn_t, temp_bb_insns) != 0) {\n    bb_insn = VARR_POP (bb_insn_t, temp_bb_insns);\n    insn = bb_insn->insn;\n    DEBUG (2, {\n      fprintf (debug_file, \"  Removing dead insn %-5lu\", (unsigned long) bb_insn->index);\n      print_bb_insn (gen_ctx, bb_insn, FALSE);\n    });\n    FOREACH_IN_INSN_VAR (gen_ctx, iter, insn, var, op_num) {\n      if ((ssa_edge = insn->ops[op_num].data) == NULL) continue;\n      def = ssa_edge->def;\n      remove_ssa_edge (gen_ctx, ssa_edge);\n      if (ssa_dead_insn_p (gen_ctx, def)) VARR_PUSH (bb_insn_t, temp_bb_insns, def);\n    }\n    gen_delete_insn (gen_ctx, insn);\n    dead_insns_num++;\n  }\n  DEBUG (1, { fprintf (debug_file, \"%5ld removed SSA dead insns\\n\", dead_insns_num); });\n}\n\n/* New Page */\n\n/* Loop invariant motion */\n\nstatic edge_t find_loop_entry_edge (bb_t loop_entry) {\n  edge_t e, entry_e = NULL;\n  bb_insn_t head, tail;\n\n  for (e = DLIST_HEAD (in_edge_t, loop_entry->in_edges); e != NULL; e = DLIST_NEXT (in_edge_t, e)) {\n    if (e->back_edge_p) continue;\n    if (entry_e != NULL) return NULL;\n    entry_e = e;\n  }\n  if (entry_e == NULL) return NULL; /* unreachable loop */\n  tail = DLIST_TAIL (bb_insn_t, entry_e->src->bb_insns);\n  head = DLIST_HEAD (bb_insn_t, entry_e->dst->bb_insns);\n  if (tail != NULL && head != NULL && DLIST_NEXT (MIR_insn_t, tail->insn) != head->insn)\n    return NULL; /* not fall through */\n  return entry_e;\n}\n\nstatic void create_preheader_from_edge (gen_ctx_t gen_ctx, edge_t e, loop_node_t loop) {\n  bb_t new_bb = create_bb (gen_ctx, NULL), prev_bb;\n  loop_node_t bb_loop_node = create_loop_node (gen_ctx, new_bb), parent = loop->parent;\n\n  add_new_bb (gen_ctx, new_bb);\n  DLIST_REMOVE (bb_t, curr_cfg->bbs, new_bb);\n  DLIST_INSERT_BEFORE (bb_t, curr_cfg->bbs, e->dst, new_bb); /* insert before loop entry */\n  gen_assert (parent != NULL);\n  if ((prev_bb = DLIST_PREV (bb_t, e->dst)) != NULL && prev_bb->loop_node->parent == parent)\n    DLIST_INSERT_AFTER (loop_node_t, parent->children, prev_bb->loop_node, bb_loop_node);\n  else if (e->src->loop_node->parent == parent)\n    DLIST_INSERT_AFTER (loop_node_t, parent->children, e->src->loop_node, bb_loop_node);\n  else\n    DLIST_APPEND (loop_node_t, parent->children, bb_loop_node);\n  bb_loop_node->parent = parent;\n  bb_loop_node->u.preheader_loop = loop;\n  loop->u.preheader = bb_loop_node;\n  create_edge (gen_ctx, e->src, new_bb, TRUE, FALSE); /* fall through should be the 1st edge */\n  create_edge (gen_ctx, new_bb, e->dst, TRUE, FALSE);\n  delete_edge (gen_ctx, e);\n}\n\nstatic void licm_add_loop_preheaders (gen_ctx_t gen_ctx, loop_node_t loop) {\n  int subloop_p = FALSE;\n  bb_insn_t bb_insn;\n  edge_t e;\n\n  for (loop_node_t node = DLIST_HEAD (loop_node_t, loop->children); node != NULL;\n       node = DLIST_NEXT (loop_node_t, node))\n    if (node->bb == NULL) {\n      subloop_p = TRUE;\n      licm_add_loop_preheaders (gen_ctx, node); /* process sub-loops */\n    }\n  /* See loop_licm where we process only the nested loops: */\n  if (subloop_p || loop == curr_cfg->root_loop_node) return;\n  loop->u.preheader = NULL;\n  if ((e = find_loop_entry_edge (loop->entry->bb)) == NULL) return;\n  if ((bb_insn = DLIST_TAIL (bb_insn_t, e->src->bb_insns)) == NULL || bb_insn->insn->code == MIR_JMP\n      || !MIR_any_branch_code_p (bb_insn->insn->code)) {\n    loop->u.preheader = e->src->loop_node;      /* The preheader already exists */\n    e->src->loop_node->u.preheader_loop = loop; /* The preheader already exists */\n  } else {\n    create_preheader_from_edge (gen_ctx, e, loop);\n  }\n}\n\nstatic int loop_invariant_p (gen_ctx_t gen_ctx, loop_node_t loop, bb_insn_t bb_insn,\n                             bitmap_t loop_invariant_insn_bitmap) {\n  MIR_insn_t insn = bb_insn->insn;\n  bb_t bb = bb_insn->bb;\n  loop_node_t curr_loop;\n  int op_num;\n  MIR_reg_t var;\n  ssa_edge_t se;\n  insn_var_iterator_t iter;\n\n  if (MIR_any_branch_code_p (insn->code) || insn->code == MIR_PHI || insn->code == MIR_RET\n      || insn->code == MIR_JRET || insn->code == MIR_LABEL || MIR_call_code_p (insn->code)\n      || insn->code == MIR_ALLOCA || insn->code == MIR_BSTART || insn->code == MIR_BEND\n      || insn->code == MIR_VA_START || insn->code == MIR_VA_ARG || insn->code == MIR_VA_BLOCK_ARG\n      || insn->code == MIR_VA_END\n      /* possible exception insns: */\n      || insn->code == MIR_DIV || insn->code == MIR_DIVS || insn->code == MIR_UDIV\n      || insn->code == MIR_UDIVS || insn->code == MIR_MOD || insn->code == MIR_MODS\n      || insn->code == MIR_UMOD || insn->code == MIR_UMODS)\n    return FALSE;\n  for (size_t i = 0; i < insn->nops; i++) {\n    if (insn->ops[i].mode == MIR_OP_VAR_MEM) return FALSE;\n    if (insn->ops[i].mode == MIR_OP_VAR && bitmap_bit_p (tied_regs, insn->ops[i].u.var))\n      return FALSE;\n  }\n  FOREACH_IN_INSN_VAR (gen_ctx, iter, insn, var, op_num) {\n    se = insn->ops[op_num].data;\n    gen_assert (se != NULL);\n    bb_insn = se->def;\n    if (loop_invariant_insn_bitmap != NULL\n        && bitmap_bit_p (loop_invariant_insn_bitmap, bb_insn->index))\n      continue;\n    bb = bb_insn->bb;\n    for (curr_loop = loop->parent; curr_loop != NULL; curr_loop = curr_loop->parent)\n      if (curr_loop == bb->loop_node->parent) break;\n    if (curr_loop == NULL) return FALSE;\n  }\n  return TRUE;\n}\n\nstatic void licm_move_insn (gen_ctx_t gen_ctx, bb_insn_t bb_insn, bb_t to, bb_insn_t before) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  bb_t bb = bb_insn->bb;\n  MIR_insn_t insn = bb_insn->insn;\n  bb_insn_t last = DLIST_TAIL (bb_insn_t, to->bb_insns);\n\n  gen_assert (before != NULL);\n  DLIST_REMOVE (bb_insn_t, bb->bb_insns, bb_insn);\n  DLIST_REMOVE (MIR_insn_t, curr_func_item->u.func->insns, insn);\n  if (last != NULL && last->insn->code == MIR_JMP) {\n    DLIST_INSERT_BEFORE (bb_insn_t, to->bb_insns, last, bb_insn);\n    MIR_insert_insn_before (ctx, curr_func_item, last->insn, insn);\n  } else {\n    DLIST_APPEND (bb_insn_t, to->bb_insns, bb_insn);\n    MIR_insert_insn_before (ctx, curr_func_item, before->insn, insn);\n  }\n  bb_insn->bb = to;\n}\n\nstatic void mark_as_moved (gen_ctx_t gen_ctx, bb_insn_t bb_insn,\n                           bitmap_t loop_invariant_bb_insn_bitmap,\n                           bitmap_t bb_insns_to_move_bitmap) {\n  int op_num;\n  MIR_reg_t var;\n  insn_var_iterator_t iter;\n  ssa_edge_t se;\n\n  VARR_TRUNC (bb_insn_t, temp_bb_insns2, 0);\n  VARR_PUSH (bb_insn_t, temp_bb_insns2, bb_insn);\n  gen_assert (bitmap_bit_p (loop_invariant_bb_insn_bitmap, bb_insn->index));\n  while (VARR_LENGTH (bb_insn_t, temp_bb_insns2) != 0) {\n    bb_insn = VARR_POP (bb_insn_t, temp_bb_insns2);\n    bitmap_set_bit_p (bb_insns_to_move_bitmap, bb_insn->index);\n    FOREACH_IN_INSN_VAR (gen_ctx, iter, bb_insn->insn, var, op_num)\n    if ((se = bb_insn->insn->ops[op_num].data) != NULL\n        && bitmap_bit_p (loop_invariant_bb_insn_bitmap, bb_insn->index))\n      VARR_PUSH (bb_insn_t, temp_bb_insns2, se->def);\n  }\n}\n\nstatic int non_invariant_use_p (gen_ctx_t gen_ctx, bb_insn_t bb_insn,\n                                bitmap_t loop_invariant_bb_insn_bitmap) {\n  int op_num;\n  MIR_reg_t var;\n  insn_var_iterator_t iter;\n  ssa_edge_t se;\n\n  FOREACH_OUT_INSN_VAR (gen_ctx, iter, bb_insn->insn, var, op_num) {\n    for (se = bb_insn->insn->ops[op_num].data; se != NULL; se = se->next_use)\n      if (!bitmap_bit_p (loop_invariant_bb_insn_bitmap, se->use->index)) return TRUE;\n  }\n  return FALSE;\n}\n\nstatic int expensive_insn_p (MIR_insn_t insn) {\n  return insn->code == MIR_MUL || insn->code == MIR_MULS;\n}\n\nstatic int loop_licm (gen_ctx_t gen_ctx, loop_node_t loop) {\n  MIR_insn_t insn;\n  bb_insn_t bb_insn;\n  loop_node_t node;\n  int subloop_p = FALSE, move_p = FALSE, op_num;\n  MIR_reg_t var;\n  ssa_edge_t se;\n  insn_var_iterator_t iter;\n  bitmap_t loop_invariant_bb_insn_bitmap = temp_bitmap;\n  bitmap_t bb_insns_to_move_bitmap = temp_bitmap2;\n  VARR (bb_insn_t) *loop_invariant_bb_insns = temp_bb_insns;\n\n  for (node = DLIST_HEAD (loop_node_t, loop->children); node != NULL;\n       node = DLIST_NEXT (loop_node_t, node))\n    if (node->bb == NULL) {\n      subloop_p = TRUE;\n      if (loop_licm (gen_ctx, node)) move_p = TRUE; /* process sub-loops first */\n    }\n  if (subloop_p || curr_cfg->root_loop_node == loop || loop->u.preheader == NULL)\n    return move_p; /* e.g. root or unreachable root */\n  DEBUG (2, {\n    fprintf (debug_file, \"Processing Loop%3lu for loop invariant motion:\\n\",\n             (unsigned long) loop->index);\n  });\n  VARR_TRUNC (bb_insn_t, loop_invariant_bb_insns, 0);\n  bitmap_clear (loop_invariant_bb_insn_bitmap);\n  for (node = DLIST_HEAD (loop_node_t, loop->children); node != NULL;\n       node = DLIST_NEXT (loop_node_t, node)) {\n    if (node->bb == NULL) continue; /* skip subloops */\n    for (bb_insn = DLIST_HEAD (bb_insn_t, node->bb->bb_insns); bb_insn != NULL;\n         bb_insn = DLIST_NEXT (bb_insn_t, bb_insn))\n      if (loop_invariant_p (gen_ctx, loop, bb_insn, NULL)) { /* Push start invariants */\n        VARR_PUSH (bb_insn_t, loop_invariant_bb_insns, bb_insn);\n        bitmap_set_bit_p (loop_invariant_bb_insn_bitmap, bb_insn->index);\n      }\n  }\n  for (size_t i = 0; i < VARR_LENGTH (bb_insn_t, loop_invariant_bb_insns); i++) {\n    /* Add insns becoming invariant if we move its inputs: */\n    bb_insn = VARR_GET (bb_insn_t, loop_invariant_bb_insns, i);\n    insn = bb_insn->insn;\n    FOREACH_OUT_INSN_VAR (gen_ctx, iter, insn, var, op_num) {\n      for (se = insn->ops[op_num].data; se != NULL; se = se->next_use) {\n        if (loop_invariant_p (gen_ctx, loop, se->use, loop_invariant_bb_insn_bitmap)\n            && bitmap_set_bit_p (loop_invariant_bb_insn_bitmap, se->use->index))\n          VARR_PUSH (bb_insn_t, loop_invariant_bb_insns, se->use);\n      }\n    }\n  }\n  bitmap_clear (bb_insns_to_move_bitmap);\n  for (int i = (int) VARR_LENGTH (bb_insn_t, loop_invariant_bb_insns) - 1; i >= 0; i--) {\n    bb_insn = VARR_GET (bb_insn_t, loop_invariant_bb_insns, i);\n    insn = bb_insn->insn;\n    DEBUG (2, {\n      fprintf (debug_file, \"  Considering invariant \");\n      print_bb_insn (gen_ctx, bb_insn, FALSE);\n    });\n    if (bitmap_bit_p (bb_insns_to_move_bitmap, bb_insn->index)) {\n      DEBUG (2, { fprintf (debug_file, \"     -- already marked as moved\\n\"); });\n      continue;\n    }\n    if (expensive_insn_p (insn)) {\n      DEBUG (2, { fprintf (debug_file, \"     -- marked as moved becuase it is costly\\n\"); });\n      mark_as_moved (gen_ctx, bb_insn, loop_invariant_bb_insn_bitmap, bb_insns_to_move_bitmap);\n      continue;\n    }\n    int can_be_moved = TRUE, input_var_p = FALSE;\n    FOREACH_IN_INSN_VAR (gen_ctx, iter, insn, var, op_num) {\n      input_var_p = TRUE;\n      if ((se = insn->ops[op_num].data) != NULL\n          && bitmap_bit_p (loop_invariant_bb_insn_bitmap, se->def->index)\n          && !bitmap_bit_p (bb_insns_to_move_bitmap, se->def->index)\n          && non_invariant_use_p (gen_ctx, se->def, loop_invariant_bb_insn_bitmap)) {\n        can_be_moved = FALSE;\n        break;\n      }\n    }\n    DEBUG (2, {\n      if (input_var_p)\n        fprintf (debug_file, \"     -- %s be moved because reg presure consideration\\n\",\n                 can_be_moved ? \"can\" : \"can't\");\n      else\n        fprintf (debug_file, \"     -- can't be moved because single insn\\n\");\n    });\n    if (can_be_moved && input_var_p)\n      mark_as_moved (gen_ctx, bb_insn, loop_invariant_bb_insn_bitmap, bb_insns_to_move_bitmap);\n  }\n  for (size_t i = 0; i < VARR_LENGTH (bb_insn_t, loop_invariant_bb_insns); i++) {\n    bb_insn = VARR_GET (bb_insn_t, loop_invariant_bb_insns, i);\n    if (!bitmap_bit_p (bb_insns_to_move_bitmap, bb_insn->index)) continue;\n    insn = bb_insn->insn;\n    DEBUG (2, {\n      fprintf (debug_file, \"  Move invariant (target bb%lu) %-5lu\",\n               (unsigned long) loop->u.preheader->bb->index, (unsigned long) bb_insn->index);\n      print_bb_insn (gen_ctx, bb_insn, FALSE);\n    });\n    licm_move_insn (gen_ctx, bb_insn, loop->u.preheader->bb,\n                    DLIST_HEAD (bb_insn_t, loop->entry->bb->bb_insns));\n    move_p = TRUE;\n  }\n  return move_p;\n}\n\nstatic int licm (gen_ctx_t gen_ctx) {\n  loop_node_t node;\n  for (node = DLIST_HEAD (loop_node_t, curr_cfg->root_loop_node->children); node != NULL;\n       node = DLIST_NEXT (loop_node_t, node))\n    if (node->bb == NULL) break;\n  if (node == NULL) return FALSE; /* no loops */\n  licm_add_loop_preheaders (gen_ctx, curr_cfg->root_loop_node);\n  return loop_licm (gen_ctx, curr_cfg->root_loop_node);\n}\n\n/* New Page */\n\n/* Pressure relief */\n\nstatic int pressure_relief (gen_ctx_t gen_ctx) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  MIR_func_t func = curr_func_item->u.func;\n  MIR_insn_t insn;\n  bb_insn_t bb_insn, next_bb_insn, use;\n  loop_node_t loop;\n  ssa_edge_t se;\n  int moved_p = FALSE;\n\n  DEBUG (2, { fprintf (debug_file, \"+++++++++++++Pressure Relief:\\n\"); });\n  for (bb_t bb = DLIST_HEAD (bb_t, curr_cfg->bbs); bb != NULL; bb = DLIST_NEXT (bb_t, bb))\n    for (bb_insn = DLIST_HEAD (bb_insn_t, bb->bb_insns); bb_insn != NULL; bb_insn = next_bb_insn) {\n      next_bb_insn = DLIST_NEXT (bb_insn_t, bb_insn);\n      insn = bb_insn->insn;\n      if (!move_code_p (insn->code) || insn->ops[0].mode != MIR_OP_VAR\n          || insn->ops[1].mode == MIR_OP_VAR || insn->ops[1].mode == MIR_OP_VAR_MEM\n          || (se = insn->ops[0].data) == NULL || se->next_use != NULL || (use = se->use)->bb == bb\n          || use->insn->code == MIR_PHI)\n        continue;\n      if ((loop = use->bb->loop_node) != NULL) {\n        for (loop = loop->parent; loop != NULL; loop = loop->parent)\n          if (loop == bb->loop_node->parent) break;\n        if (loop != NULL) continue; /* avoid move into a loop */\n      }\n      /* One use in another BB: move closer */\n      DEBUG (2, {\n        fprintf (debug_file, \"  Move insn %-5lu\", (unsigned long) bb_insn->index);\n        MIR_output_insn (ctx, debug_file, insn, func, FALSE);\n        fprintf (debug_file, \"  before insn %-5lu\", (unsigned long) use->index);\n        MIR_output_insn (ctx, debug_file, use->insn, func, TRUE);\n      });\n      gen_move_insn_before (gen_ctx, use->insn, insn);\n      moved_p = TRUE;\n    }\n  return moved_p;\n}\n\n/* New Page */\n\n/* SSA insn combining is done on conventional SSA (to work with move insns) in reverse insn order.\n   We combine addresses and cmp and branch case.  Copy prop before permits to ignore moves for\n   combining. It is the last SSA pass as it makes ssa edges unreachable from uses (in\n   mem[base,index] case). Advantages in comparison with combining after RA:\n     o no artificial dependencies on a hard reg assigned to different regs\n     o no missed dependencies on spilled regs */\n\ntypedef struct addr_info {\n  MIR_type_t type;\n  MIR_disp_t disp;\n  MIR_op_t *base, *index; /* var operands can be used for memory base and index */\n  MIR_scale_t scale;\n} addr_info_t;\n\nstatic int get_int_const (gen_ctx_t gen_ctx, MIR_op_t *op_ref, int64_t *c) {\n  if (op_ref->mode == MIR_OP_VAR) {\n    ssa_edge_t se = op_ref->data;\n    if (se == NULL || se->def->insn->code != MIR_MOV) return FALSE;\n    op_ref = &se->def->insn->ops[1];\n  }\n  if (op_ref->mode == MIR_OP_INT || op_ref->mode == MIR_OP_UINT) {\n    *c = op_ref->u.i;\n  } else if (op_ref->mode == MIR_OP_REF && op_ref->u.ref->item_type != MIR_func_item) {\n    *c = get_ref_value (gen_ctx, op_ref);\n  } else {\n    return FALSE;\n  }\n  return TRUE;\n}\n\nstatic int cycle_phi_p (bb_insn_t bb_insn) { /* we are not in pure SSA at this stage */\n  ssa_edge_t se;\n  if (bb_insn->insn->code != MIR_PHI) return FALSE;\n  for (size_t i = 1; i < bb_insn->insn->nops; i++)\n    if ((se = bb_insn->insn->ops[i].data) != NULL && se->def->bb == bb_insn->bb) return TRUE;\n  return FALSE;\n}\n\nstatic int var_plus_const (gen_ctx_t gen_ctx, ssa_edge_t se, bb_t from_bb, MIR_op_t **var_op_ref,\n                           int64_t *c) {\n  if (se == NULL) return FALSE; /* e.g. for arg */\n  gen_assert (*var_op_ref != NULL && (*var_op_ref)->mode == MIR_OP_VAR);\n  MIR_reg_t reg = (*var_op_ref)->u.var - MAX_HARD_REG;\n  if (MIR_reg_hard_reg_name (gen_ctx->ctx, reg, curr_func_item->u.func) != NULL) return FALSE;\n  MIR_insn_t insn = se->def->insn;\n  MIR_op_t *res_ref = NULL;\n  *c = 0;\n  if (insn->code == MIR_MOV && insn->ops[1].mode == MIR_OP_VAR) {\n    res_ref = &insn->ops[1];\n  } else if ((insn->code == MIR_ADD || insn->code == MIR_SUB) && insn->ops[1].mode == MIR_OP_VAR\n             && get_int_const (gen_ctx, &insn->ops[2], c)) {\n    res_ref = &insn->ops[1];\n    if (insn->code == MIR_SUB) *c = -*c;\n  } else if (insn->code == MIR_ADD && insn->ops[2].mode == MIR_OP_VAR\n             && get_int_const (gen_ctx, &insn->ops[1], c)) {\n    res_ref = &insn->ops[2];\n  } else {\n    return FALSE;\n  }\n  if ((se = res_ref->data) != NULL && se->def->bb != from_bb && cycle_phi_p (se->def)) return FALSE;\n  *var_op_ref = res_ref;\n  return TRUE;\n}\n\nstatic int var_mult_const (gen_ctx_t gen_ctx, ssa_edge_t se, bb_t from_bb, MIR_op_t **var_op_ref,\n                           int64_t *c) {\n  if (se == NULL) return FALSE; /* e.g. for arg */\n  gen_assert (*var_op_ref != NULL && (*var_op_ref)->mode == MIR_OP_VAR);\n  MIR_reg_t reg = (*var_op_ref)->u.var - MAX_HARD_REG;\n  if (MIR_reg_hard_reg_name (gen_ctx->ctx, reg, curr_func_item->u.func) != NULL) return FALSE;\n  MIR_insn_t insn = se->def->insn;\n  MIR_op_t *res_ref = NULL;\n  *c = 0;\n  if ((insn->code == MIR_MUL || insn->code == MIR_LSH) && insn->ops[1].mode == MIR_OP_VAR\n      && get_int_const (gen_ctx, &insn->ops[2], c)) {\n    res_ref = &insn->ops[1];\n    if (insn->code == MIR_LSH) {\n      if (*c < 0 || *c >= (int) sizeof (int64_t) * 8)\n        res_ref = NULL;\n      else\n        *c = (int64_t) 1 << *c;\n    }\n  } else if (insn->code == MIR_MUL && insn->ops[2].mode == MIR_OP_VAR\n             && get_int_const (gen_ctx, &insn->ops[1], c)) {\n    res_ref = &insn->ops[2];\n  }\n  if (res_ref == NULL) return FALSE;\n  if (*c < 0 || *c > MIR_MAX_SCALE) return FALSE;\n  if ((se = res_ref->data) != NULL && se->def->bb != from_bb && cycle_phi_p (se->def)) return FALSE;\n  *var_op_ref = res_ref;\n  return TRUE;\n}\n\nstatic int var_plus_var (gen_ctx_t gen_ctx, ssa_edge_t se, bb_t from_bb, MIR_op_t **var_op_ref1,\n                         MIR_op_t **var_op_ref2) {\n  if (se == NULL) return FALSE; /* e.g. for arg */\n  gen_assert (*var_op_ref1 != NULL && (*var_op_ref1)->mode == MIR_OP_VAR && *var_op_ref2 == NULL);\n  MIR_reg_t reg = (*var_op_ref1)->u.var - MAX_HARD_REG;\n  if (MIR_reg_hard_reg_name (gen_ctx->ctx, reg, curr_func_item->u.func) != NULL) return FALSE;\n  MIR_insn_t insn = se->def->insn;\n  if (insn->code != MIR_ADD || insn->ops[1].mode != MIR_OP_VAR || insn->ops[2].mode != MIR_OP_VAR)\n    return FALSE;\n  if ((se = insn->ops[1].data) != NULL && se->def->bb != from_bb && cycle_phi_p (se->def))\n    return FALSE;\n  if ((se = insn->ops[2].data) != NULL && se->def->bb != from_bb && cycle_phi_p (se->def))\n    return FALSE;\n  *var_op_ref1 = &insn->ops[1];\n  *var_op_ref2 = &insn->ops[2];\n  return TRUE;\n}\n\nstatic int addr_info_eq_p (addr_info_t *addr1, addr_info_t *addr2) {\n  return (addr1->type == addr2->type && addr1->disp == addr2->disp && addr1->base == addr2->base\n          && addr1->index == addr2->index && addr1->scale == addr2->scale);\n}\n\nstatic int addr_info_ok_p (gen_ctx_t gen_ctx, addr_info_t *addr) {\n  MIR_op_t mem_op\n    = _MIR_new_var_mem_op (gen_ctx->ctx, addr->type, addr->disp,\n                           addr->base == NULL ? MIR_NON_VAR : addr->base->u.var,\n                           addr->index == NULL ? MIR_NON_VAR : addr->index->u.var, addr->scale);\n  return target_memory_ok_p (gen_ctx, &mem_op);\n}\n\nstatic int update_addr_p (gen_ctx_t gen_ctx, bb_t from_bb, MIR_op_t *mem_op_ref,\n                          MIR_op_t *temp_op_ref, addr_info_t *addr_info) {\n  int temp_int, stop_base_p = FALSE, stop_index_p = TRUE, temp_stop_index_p;\n  int64_t c;\n  addr_info_t temp_addr_info;\n\n  gen_assert (mem_op_ref->mode == MIR_OP_VAR_MEM && mem_op_ref->u.var_mem.index == MIR_NON_VAR);\n  if (mem_op_ref->u.var_mem.base == MIR_NON_VAR) return FALSE;\n  *temp_op_ref = _MIR_new_var_op (gen_ctx->ctx, mem_op_ref->u.var_mem.base);\n  temp_op_ref->data = mem_op_ref->data;\n  addr_info->type = mem_op_ref->u.var_mem.type;\n  addr_info->disp = mem_op_ref->u.var_mem.disp;\n  addr_info->scale = 1;\n  addr_info->base = temp_op_ref;\n  addr_info->index = NULL;\n  for (int change_p = FALSE;;) {\n    temp_addr_info = *addr_info;\n    temp_stop_index_p = stop_index_p;\n    if (!stop_base_p) {\n      if (var_plus_const (gen_ctx, addr_info->base->data, from_bb, &addr_info->base, &c)) {\n        addr_info->disp += c;\n      } else if (addr_info->scale == 1\n                 && var_mult_const (gen_ctx, addr_info->base->data, from_bb, &addr_info->base,\n                                    &c)) {\n        if (c != 1) {\n          SWAP (addr_info->base, addr_info->index, temp_op_ref);\n          SWAP (stop_base_p, stop_index_p, temp_int);\n          addr_info->scale = (MIR_scale_t) c;\n        }\n      } else if (addr_info->index == NULL\n                 && var_plus_var (gen_ctx, addr_info->base->data, from_bb, &addr_info->base,\n                                  &addr_info->index)) {\n        stop_index_p = FALSE;\n      }\n    }\n    if (!addr_info_eq_p (addr_info, &temp_addr_info) && addr_info_ok_p (gen_ctx, addr_info)) {\n      change_p = TRUE;\n      continue;\n    }\n    *addr_info = temp_addr_info;\n    stop_index_p = temp_stop_index_p;\n    stop_base_p = TRUE;\n    if (stop_index_p) return change_p;\n    if (var_plus_const (gen_ctx, addr_info->index->data, from_bb, &addr_info->index, &c)) {\n      addr_info->disp += c * addr_info->scale;\n    } else if (var_mult_const (gen_ctx, addr_info->index->data, from_bb, &addr_info->index, &c)) {\n      addr_info->scale *= (MIR_scale_t) c;\n    } else {\n      gen_assert (addr_info->base != NULL || addr_info->scale != 1);\n    }\n    if (!addr_info_eq_p (addr_info, &temp_addr_info) && addr_info_ok_p (gen_ctx, addr_info)) {\n      change_p = TRUE;\n      continue;\n    }\n    *addr_info = temp_addr_info;\n    return change_p;\n  }\n}\n\nstatic MIR_insn_code_t get_combined_br_code (int true_p, MIR_insn_code_t cmp_code) {\n  switch (cmp_code) {\n  case MIR_EQ: return true_p ? MIR_BEQ : MIR_BNE;\n  case MIR_EQS: return true_p ? MIR_BEQS : MIR_BNES;\n  case MIR_NE: return true_p ? MIR_BNE : MIR_BEQ;\n  case MIR_NES: return true_p ? MIR_BNES : MIR_BEQS;\n  case MIR_LT: return true_p ? MIR_BLT : MIR_BGE;\n  case MIR_LTS: return true_p ? MIR_BLTS : MIR_BGES;\n  case MIR_ULT: return true_p ? MIR_UBLT : MIR_UBGE;\n  case MIR_ULTS: return true_p ? MIR_UBLTS : MIR_UBGES;\n  case MIR_LE: return true_p ? MIR_BLE : MIR_BGT;\n  case MIR_LES: return true_p ? MIR_BLES : MIR_BGTS;\n  case MIR_ULE: return true_p ? MIR_UBLE : MIR_UBGT;\n  case MIR_ULES: return true_p ? MIR_UBLES : MIR_UBGTS;\n  case MIR_GT: return true_p ? MIR_BGT : MIR_BLE;\n  case MIR_GTS: return true_p ? MIR_BGTS : MIR_BLES;\n  case MIR_UGT: return true_p ? MIR_UBGT : MIR_UBLE;\n  case MIR_UGTS: return true_p ? MIR_UBGTS : MIR_UBLES;\n  case MIR_GE: return true_p ? MIR_BGE : MIR_BLT;\n  case MIR_GES: return true_p ? MIR_BGES : MIR_BLTS;\n  case MIR_UGE: return true_p ? MIR_UBGE : MIR_UBLT;\n  case MIR_UGES:\n    return true_p ? MIR_UBGES : MIR_UBLTS;\n    /* Cannot revert in the false case for IEEE754: */\n  case MIR_FEQ: return true_p ? MIR_FBEQ : MIR_INSN_BOUND;\n  case MIR_DEQ: return true_p ? MIR_DBEQ : MIR_INSN_BOUND;\n  case MIR_LDEQ: return true_p ? MIR_LDBEQ : MIR_INSN_BOUND;\n  case MIR_FNE: return true_p ? MIR_FBNE : MIR_INSN_BOUND;\n  case MIR_DNE: return true_p ? MIR_DBNE : MIR_INSN_BOUND;\n  case MIR_LDNE: return true_p ? MIR_LDBNE : MIR_INSN_BOUND;\n  case MIR_FLT: return true_p ? MIR_FBLT : MIR_INSN_BOUND;\n  case MIR_DLT: return true_p ? MIR_DBLT : MIR_INSN_BOUND;\n  case MIR_LDLT: return true_p ? MIR_LDBLT : MIR_INSN_BOUND;\n  case MIR_FLE: return true_p ? MIR_FBLE : MIR_INSN_BOUND;\n  case MIR_DLE: return true_p ? MIR_DBLE : MIR_INSN_BOUND;\n  case MIR_LDLE: return true_p ? MIR_LDBLE : MIR_INSN_BOUND;\n  case MIR_FGT: return true_p ? MIR_FBGT : MIR_INSN_BOUND;\n  case MIR_DGT: return true_p ? MIR_DBGT : MIR_INSN_BOUND;\n  case MIR_LDGT: return true_p ? MIR_LDBGT : MIR_INSN_BOUND;\n  case MIR_FGE: return true_p ? MIR_FBGE : MIR_INSN_BOUND;\n  case MIR_DGE: return true_p ? MIR_DBGE : MIR_INSN_BOUND;\n  case MIR_LDGE: return true_p ? MIR_LDBGE : MIR_INSN_BOUND;\n  default: return MIR_INSN_BOUND;\n  }\n}\n\nstatic bb_insn_t combine_branch_and_cmp (gen_ctx_t gen_ctx, bb_insn_t bb_insn) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  MIR_insn_t def_insn, new_insn, insn = bb_insn->insn;\n  bb_t bb = bb_insn->bb;\n  bb_insn_t def_bb_insn;\n  ssa_edge_t se;\n  MIR_insn_code_t code = insn->code;\n  MIR_op_t *op_ref;\n\n  if (code != MIR_BT && code != MIR_BF && code != MIR_BTS && code != MIR_BFS) return NULL;\n  op_ref = &insn->ops[1];\n  if (op_ref->mode != MIR_OP_VAR || (se = op_ref->data) == NULL) return NULL;\n  def_bb_insn = se->def;\n  def_insn = def_bb_insn->insn;\n  if ((code = get_combined_br_code (code == MIR_BT || code == MIR_BTS, def_insn->code))\n      == MIR_INSN_BOUND)\n    return NULL;\n  new_insn = MIR_new_insn (ctx, code, insn->ops[0], def_insn->ops[1], def_insn->ops[2]);\n  new_insn->ops[1].data = new_insn->ops[2].data = NULL;\n  /* don't use gen_add_insn_before as it checks adding branch after branch: */\n  MIR_insert_insn_before (ctx, curr_func_item, insn, new_insn);\n  ssa_delete_insn (gen_ctx, insn);\n  bb_insn = add_new_bb_insn (gen_ctx, new_insn, bb, TRUE);\n  se = def_insn->ops[1].data;\n  if (se != NULL) add_ssa_edge (gen_ctx, se->def, se->def_op_num, bb_insn, 1);\n  se = def_insn->ops[2].data;\n  if (se != NULL) add_ssa_edge (gen_ctx, se->def, se->def_op_num, bb_insn, 2);\n  DEBUG (2, {\n    fprintf (debug_file, \"      changing to \");\n    print_bb_insn (gen_ctx, bb_insn, TRUE);\n  });\n  ssa_delete_insn_if_dead_p (gen_ctx, def_bb_insn);\n  return bb_insn;\n}\n\nstatic void ssa_combine (gen_ctx_t gen_ctx) {  // tied reg, alias ???\n  MIR_op_t temp_op;\n  MIR_insn_t insn;\n  bb_insn_t bb_insn, prev_bb_insn, new_bb_insn;\n  ssa_edge_t se;\n  addr_info_t addr_info;\n\n  for (bb_t bb = DLIST_HEAD (bb_t, curr_cfg->bbs); bb != NULL; bb = DLIST_NEXT (bb_t, bb)) {\n    DEBUG (2, { fprintf (debug_file, \"Processing bb%lu\\n\", (unsigned long) bb->index); });\n    for (bb_insn = DLIST_TAIL (bb_insn_t, bb->bb_insns); bb_insn != NULL; bb_insn = prev_bb_insn) {\n      prev_bb_insn = DLIST_PREV (bb_insn_t, bb_insn);\n      insn = bb_insn->insn;\n      /* not all insn is deleted if we use addr defs from other bbs */\n      if (ssa_delete_insn_if_dead_p (gen_ctx, bb_insn)) continue;\n      if (insn->code == MIR_LABEL || MIR_call_code_p (insn->code)) continue;\n      DEBUG (2, {\n        fprintf (debug_file, \"  combining insn \");\n        print_bb_insn (gen_ctx, bb_insn, FALSE);\n      });\n      if ((new_bb_insn = combine_branch_and_cmp (gen_ctx, bb_insn)) != NULL) {\n        bb_insn = new_bb_insn;\n        prev_bb_insn = DLIST_PREV (bb_insn_t, bb_insn);\n        insn = bb_insn->insn;\n      }\n      for (size_t i = 0; i < insn->nops; i++) {\n        if (insn->ops[i].mode != MIR_OP_VAR_MEM) continue;\n        if (!update_addr_p (gen_ctx, bb, &insn->ops[i], &temp_op, &addr_info)) continue;\n        remove_ssa_edge (gen_ctx, insn->ops[i].data);\n        insn->ops[i].u.var_mem.disp = addr_info.disp;\n        insn->ops[i].u.var_mem.base = insn->ops[i].u.var_mem.index = MIR_NON_VAR;\n        if (addr_info.base != NULL) {\n          insn->ops[i].u.var_mem.base = addr_info.base->u.var;\n          if ((se = addr_info.base->data) != NULL)\n            add_ssa_edge (gen_ctx, se->def, se->def_op_num, bb_insn, (int) i);\n        }\n        if (addr_info.index != NULL) {\n          insn->ops[i].u.var_mem.index = addr_info.index->u.var;\n          if ((se = addr_info.index->data) != NULL)\n            add_ssa_edge_dup (gen_ctx, se->def, se->def_op_num, bb_insn, (int) i);\n        }\n        insn->ops[i].u.var_mem.scale = addr_info.scale;\n        DEBUG (2, {\n          fprintf (debug_file, \"    changing mem op %lu to \", (unsigned long) i);\n          print_insn (gen_ctx, insn, TRUE);\n        });\n      }\n    }\n  }\n}\n\n/* New Page */\n\n/* Live and live range analysis: */\n\n#define live_in in\n#define live_out out\n#define live_kill kill\n#define live_gen gen\n\ntypedef struct lr_bb *lr_bb_t;\nstruct lr_bb {\n  bb_t bb;\n  lr_bb_t next;\n};\n\ntypedef struct live_range *live_range_t; /* vars */\nstruct live_range {\n  lr_bb_t lr_bb; /* first BB which is entirely in this range, NULL otherwise */\n  int start, finish;\n  int ref_cost;\n  /* to smaller start and finish, but still this start can be equal to the next finish: */\n  live_range_t next;\n};\n\nDEF_VARR (live_range_t);\nDEF_VARR (MIR_reg_t);\n\nstruct lr_ctx {\n  int ssa_live_info_p;                    /* TRUE if found PHIs */\n  int scan_vars_num;                      /* vars considered for live analysis: 0 means all vars */\n  VARR (int) * var_to_scan_var_map;       /* if var is less than the map size: its live_var or -1 */\n  VARR (MIR_reg_t) * scan_var_to_var_map; /* of size scan_vars_num */\n  live_range_t free_lr_list;\n  lr_bb_t free_lr_bb_list;\n  int curr_point;\n  bitmap_t live_vars, referenced_vars;\n  bitmap_t points_with_born_vars, points_with_dead_vars, points_with_born_or_dead_vars;\n  VARR (live_range_t) * var_live_ranges;\n  VARR (int) * point_map;\n};\n\n#define ssa_live_info_p gen_ctx->lr_ctx->ssa_live_info_p\n#define scan_vars_num gen_ctx->lr_ctx->scan_vars_num\n#define var_to_scan_var_map gen_ctx->lr_ctx->var_to_scan_var_map\n#define scan_var_to_var_map gen_ctx->lr_ctx->scan_var_to_var_map\n#define free_lr_list gen_ctx->lr_ctx->free_lr_list\n#define free_lr_bb_list gen_ctx->lr_ctx->free_lr_bb_list\n#define curr_point gen_ctx->lr_ctx->curr_point\n#define live_vars gen_ctx->lr_ctx->live_vars\n#define referenced_vars gen_ctx->lr_ctx->referenced_vars\n#define points_with_born_vars gen_ctx->lr_ctx->points_with_born_vars\n#define points_with_dead_vars gen_ctx->lr_ctx->points_with_dead_vars\n#define points_with_born_or_dead_vars gen_ctx->lr_ctx->points_with_born_or_dead_vars\n#define var_live_ranges gen_ctx->lr_ctx->var_live_ranges\n#define point_map gen_ctx->lr_ctx->point_map\n\nstatic int var_to_scan_var (gen_ctx_t gen_ctx, MIR_reg_t var) {\n  if (scan_vars_num == 0) return (int) var;\n  if (VARR_LENGTH (int, var_to_scan_var_map) <= var) return -1;\n  return VARR_GET (int, var_to_scan_var_map, var);\n}\n\nstatic MIR_reg_t scan_var_to_var (gen_ctx_t gen_ctx, int scan_var) {\n  if (scan_vars_num == 0) return (MIR_reg_t) scan_var;\n  gen_assert (scan_var >= 0 && (int) VARR_LENGTH (MIR_reg_t, scan_var_to_var_map) > scan_var);\n  return VARR_GET (MIR_reg_t, scan_var_to_var_map, scan_var);\n}\n\n/* Life analysis */\nstatic void live_con_func_0 (bb_t bb MIR_UNUSED) {}\n\nstatic int live_con_func_n (gen_ctx_t gen_ctx, bb_t bb) {\n  MIR_op_t *op_ref;\n  bb_insn_t bb_insn;\n  edge_t e, e2;\n  int n, change_p = FALSE;\n\n  for (e = DLIST_HEAD (out_edge_t, bb->out_edges); e != NULL; e = DLIST_NEXT (out_edge_t, e)) {\n    change_p |= bitmap_ior (bb->live_out, bb->live_out, e->dst->live_in);\n    if (ssa_live_info_p) {\n      for (bb_insn = DLIST_HEAD (bb_insn_t, e->dst->bb_insns); bb_insn != NULL;\n           bb_insn = DLIST_NEXT (bb_insn_t, bb_insn))\n        if (bb_insn->insn->code != MIR_LABEL) break;\n      if (bb_insn == NULL || bb_insn->insn->code != MIR_PHI) continue; /* no phis in dst */\n      for (n = 1, e2 = DLIST_HEAD (in_edge_t, e->dst->in_edges); e2 != NULL;\n           e2 = DLIST_NEXT (in_edge_t, e2), n++)\n        if (e2 == e) break;\n      gen_assert (e2 == e);\n      for (;;) {\n        op_ref = &bb_insn->insn->ops[n];\n        if (op_ref->mode == MIR_OP_VAR)\n          change_p |= bitmap_set_bit_p (bb->live_out, var_to_scan_var (gen_ctx, op_ref->u.var));\n        if ((bb_insn = DLIST_NEXT (bb_insn_t, bb_insn)) == NULL || bb_insn->insn->code != MIR_PHI)\n          break;\n      }\n    }\n  }\n  return change_p;\n}\n\nstatic int live_trans_func (gen_ctx_t gen_ctx MIR_UNUSED, bb_t bb) {\n  return bitmap_ior_and_compl (bb->live_in, bb->live_gen, bb->live_out, bb->live_kill);\n}\n\nstatic int bb_loop_level (bb_t bb) {\n  loop_node_t loop_node;\n  int level = -1;\n\n  for (loop_node = bb->loop_node; loop_node->parent != NULL; loop_node = loop_node->parent) level++;\n  gen_assert (level >= 0);\n  return level;\n}\n\nstatic void increase_pressure (int int_p, bb_t bb, int *int_pressure, int *fp_pressure) {\n  if (int_p) {\n    if (bb->max_int_pressure < ++(*int_pressure)) bb->max_int_pressure = *int_pressure;\n  } else {\n    if (bb->max_fp_pressure < ++(*fp_pressure)) bb->max_fp_pressure = *fp_pressure;\n  }\n}\n\nstatic int int_var_type_p (gen_ctx_t gen_ctx, MIR_reg_t var) {\n  if (var <= MAX_HARD_REG) return target_hard_reg_type_ok_p (var, MIR_T_I32);\n  return MIR_int_type_p (MIR_reg_type (gen_ctx->ctx, var - MAX_HARD_REG, curr_func_item->u.func));\n}\n\nstatic MIR_insn_t initiate_bb_live_info (gen_ctx_t gen_ctx, MIR_insn_t bb_tail_insn, int freq_p) {\n  bb_t bb = get_insn_bb (gen_ctx, bb_tail_insn);\n  MIR_insn_t insn;\n  long bb_freq;\n  MIR_reg_t var, early_clobbered_hard_reg1, early_clobbered_hard_reg2;\n  int scan_var, op_num, int_p = FALSE;\n  int bb_int_pressure, bb_fp_pressure;\n  reg_info_t *reg_infos;\n  insn_var_iterator_t insn_var_iter;\n  bitmap_t global_hard_regs\n    = _MIR_get_module_global_var_hard_regs (gen_ctx->ctx, curr_func_item->module);\n\n  reg_infos = VARR_ADDR (reg_info_t, curr_cfg->reg_info);\n  bb_freq = 1;\n  if (optimize_level != 0 && freq_p)\n    for (int i = bb_loop_level (bb); i > 0; i--)\n      if (bb_freq < LONG_MAX / 8) bb_freq *= LOOP_COST_FACTOR;\n  bb->max_int_pressure = bb_int_pressure = bb->max_fp_pressure = bb_fp_pressure = 0;\n  for (insn = bb_tail_insn; insn != NULL && get_insn_bb (gen_ctx, insn) == bb;\n       insn = DLIST_PREV (MIR_insn_t, insn)) {\n    if (insn->code == MIR_PHI) {\n      ssa_live_info_p = TRUE;\n      var = insn->ops[0].u.var;\n      if ((scan_var = var_to_scan_var (gen_ctx, var)) < 0) continue;\n      if (bitmap_clear_bit_p (bb->live_gen, scan_var) && optimize_level != 0)\n        (int_var_type_p (gen_ctx, var) ? bb_int_pressure-- : bb_fp_pressure--);\n      bitmap_set_bit_p (bb->live_kill, scan_var);\n      continue;\n    }\n    if (MIR_call_code_p (insn->code) && scan_vars_num == 0) {\n      bitmap_ior (bb->live_kill, bb->live_kill, call_used_hard_regs[MIR_T_UNDEF]);\n      if (global_hard_regs != NULL)\n        bitmap_ior_and_compl (bb->live_gen, global_hard_regs, bb->live_gen,\n                              call_used_hard_regs[MIR_T_UNDEF]);\n      else\n        bitmap_and_compl (bb->live_gen, bb->live_gen, call_used_hard_regs[MIR_T_UNDEF]);\n    }\n    FOREACH_OUT_INSN_VAR (gen_ctx, insn_var_iter, insn, var, op_num) { /* output vars */\n      if ((scan_var = var_to_scan_var (gen_ctx, var)) < 0) continue;\n      if (bitmap_clear_bit_p (bb->live_gen, scan_var) && optimize_level != 0)\n        (int_var_type_p (gen_ctx, var) ? bb_int_pressure-- : bb_fp_pressure--);\n      bitmap_set_bit_p (bb->live_kill, scan_var);\n      if (freq_p && var > MAX_HARD_REG)\n        reg_infos[var].freq\n          = reg_infos[var].freq < LONG_MAX - bb_freq ? reg_infos[var].freq + bb_freq : LONG_MAX;\n    }\n    FOREACH_IN_INSN_VAR (gen_ctx, insn_var_iter, insn, var, op_num) { /* input vars */\n      if ((scan_var = var_to_scan_var (gen_ctx, var)) < 0) continue;\n      if (bitmap_set_bit_p (bb->live_gen, scan_var) && optimize_level != 0)\n        increase_pressure (int_var_type_p (gen_ctx, var), bb, &bb_int_pressure, &bb_fp_pressure);\n      if (freq_p && var > MAX_HARD_REG)\n        reg_infos[var].freq\n          = reg_infos[var].freq < LONG_MAX - bb_freq ? reg_infos[var].freq + bb_freq : LONG_MAX;\n    }\n    if (scan_vars_num != 0) continue;\n    target_get_early_clobbered_hard_regs (insn, &early_clobbered_hard_reg1,\n                                          &early_clobbered_hard_reg2);\n    if (early_clobbered_hard_reg1 != MIR_NON_VAR) {\n      if (optimize_level != 0) {\n        int_p = int_var_type_p (gen_ctx, early_clobbered_hard_reg1);\n        increase_pressure (int_p, bb, &bb_int_pressure, &bb_fp_pressure);\n      }\n      bitmap_clear_bit_p (bb->live_gen, early_clobbered_hard_reg1);\n      bitmap_set_bit_p (bb->live_kill, early_clobbered_hard_reg1);\n      if (optimize_level != 0) (int_p ? bb_int_pressure-- : bb_fp_pressure--);\n    }\n    if (early_clobbered_hard_reg2 != MIR_NON_VAR) {\n      if (optimize_level != 0) {\n        int_p = int_var_type_p (gen_ctx, early_clobbered_hard_reg2);\n        increase_pressure (int_p, bb, &bb_int_pressure, &bb_fp_pressure);\n      }\n      bitmap_clear_bit_p (bb->live_gen, early_clobbered_hard_reg2);\n      bitmap_set_bit_p (bb->live_kill, early_clobbered_hard_reg2);\n      if (optimize_level != 0) (int_p ? bb_int_pressure-- : bb_fp_pressure--);\n    }\n    if (MIR_call_code_p (insn->code)) {\n      bitmap_t reg_args;\n\n      if (optimize_level != 0)\n        bitmap_ior (bb->live_gen, bb->live_gen, ((bb_insn_t) insn->data)->call_hard_reg_args);\n      else if ((reg_args = ((insn_data_t) insn->data)->u.call_hard_reg_args) != NULL)\n        bitmap_ior (bb->live_gen, bb->live_gen, reg_args);\n    }\n  }\n  return insn;\n}\n\nstatic void initiate_live_info (gen_ctx_t gen_ctx, int freq_p) {\n  MIR_reg_t max_var, n;\n  reg_info_t ri;\n  bitmap_t global_hard_regs\n    = _MIR_get_module_global_var_hard_regs (gen_ctx->ctx, curr_func_item->module);\n\n  VARR_TRUNC (reg_info_t, curr_cfg->reg_info, 0);\n  max_var = get_max_var (gen_ctx);\n  for (n = 0; n <= max_var; n++) {\n    ri.freq = 0;\n    ri.live_length = 0;\n    VARR_PUSH (reg_info_t, curr_cfg->reg_info, ri);\n  }\n  for (bb_t bb = DLIST_HEAD (bb_t, curr_cfg->bbs); bb != NULL; bb = DLIST_NEXT (bb_t, bb)) {\n    gen_assert (bb != NULL && bb->live_in != NULL && bb->live_out != NULL && bb->live_gen != NULL\n                && bb->live_kill != NULL);\n    bitmap_clear (bb->live_in);\n    bitmap_clear (bb->live_out);\n    bitmap_clear (bb->live_gen);\n    bitmap_clear (bb->live_kill);\n  }\n  if (global_hard_regs != NULL && scan_vars_num == 0) /* exit bb */\n    bitmap_copy (DLIST_EL (bb_t, curr_cfg->bbs, 1)->live_out, global_hard_regs);\n  for (MIR_insn_t tail = DLIST_TAIL (MIR_insn_t, curr_func_item->u.func->insns); tail != NULL;)\n    tail = initiate_bb_live_info (gen_ctx, tail, freq_p);\n}\n\nstatic void update_bb_pressure (gen_ctx_t gen_ctx) {\n  size_t nel;\n  bitmap_iterator_t bi;\n\n  for (bb_t bb = DLIST_HEAD (bb_t, curr_cfg->bbs); bb != NULL; bb = DLIST_NEXT (bb_t, bb)) {\n    int int_pressure = bb->max_int_pressure, fp_pressure = bb->max_fp_pressure;\n\n    FOREACH_BITMAP_BIT (bi, bb->live_out, nel) {\n      increase_pressure (int_var_type_p (gen_ctx, (MIR_reg_t) nel), bb, &int_pressure,\n                         &fp_pressure);\n    }\n  }\n}\n\nstatic void calculate_func_cfg_live_info (gen_ctx_t gen_ctx, int freq_p) {\n  ssa_live_info_p = FALSE;\n  initiate_live_info (gen_ctx, freq_p);\n  solve_dataflow (gen_ctx, FALSE, live_con_func_0, live_con_func_n, live_trans_func);\n  if (optimize_level != 0) update_bb_pressure (gen_ctx);\n}\n\nstatic void consider_all_live_vars (gen_ctx_t gen_ctx) { scan_vars_num = 0; }\n\n#ifndef MIR_MAX_COALESCE_VARS\n#define MIR_MAX_COALESCE_VARS 10000 /* 10K means about 8MB for conflict matrix */\n#endif\n\nstatic void collect_scan_var (gen_ctx_t gen_ctx, MIR_reg_t var) {\n  if (!bitmap_set_bit_p (temp_bitmap, var)) return;\n  if (scan_vars_num >= MIR_MAX_COALESCE_VARS) return;\n  while (VARR_LENGTH (int, var_to_scan_var_map) <= var) VARR_PUSH (int, var_to_scan_var_map, -1);\n  VARR_PUSH (MIR_reg_t, scan_var_to_var_map, var);\n  VARR_SET (int, var_to_scan_var_map, var, scan_vars_num++);\n}\n\nstatic void scan_collected_moves (gen_ctx_t gen_ctx);\n\nstatic int consider_move_vars_only (gen_ctx_t gen_ctx) {\n  VARR_TRUNC (int, var_to_scan_var_map, 0);\n  VARR_TRUNC (MIR_reg_t, scan_var_to_var_map, 0);\n  bitmap_clear (temp_bitmap);\n  scan_vars_num = 0;\n  scan_collected_moves (gen_ctx);\n  return scan_vars_num > 0 && scan_vars_num < MIR_MAX_COALESCE_VARS;\n}\n\nstatic void add_bb_insn_dead_vars (gen_ctx_t gen_ctx) {\n  MIR_alloc_t alloc = gen_alloc (gen_ctx);\n  MIR_insn_t insn;\n  bb_insn_t bb_insn, prev_bb_insn;\n  MIR_reg_t var, early_clobbered_hard_reg1, early_clobbered_hard_reg2;\n  int scan_var, op_num;\n  bitmap_t live;\n  insn_var_iterator_t insn_var_iter;\n\n  /* we need all var analysis and bb insns to keep dead var info */\n  gen_assert (optimize_level > 0);\n  live = bitmap_create2 (alloc, DEFAULT_INIT_BITMAP_BITS_NUM);\n  for (bb_t bb = DLIST_HEAD (bb_t, curr_cfg->bbs); bb != NULL; bb = DLIST_NEXT (bb_t, bb)) {\n    bitmap_copy (live, bb->live_out);\n    for (bb_insn = DLIST_TAIL (bb_insn_t, bb->bb_insns); bb_insn != NULL; bb_insn = prev_bb_insn) {\n      prev_bb_insn = DLIST_PREV (bb_insn_t, bb_insn);\n      clear_bb_insn_dead_vars (gen_ctx, bb_insn);\n      insn = bb_insn->insn;\n      FOREACH_OUT_INSN_VAR (gen_ctx, insn_var_iter, insn, var, op_num) {\n        if ((scan_var = var_to_scan_var (gen_ctx, var)) < 0) continue;\n        bitmap_clear_bit_p (live, scan_var);\n      }\n      if (scan_vars_num == 0 && MIR_call_code_p (insn->code))\n        bitmap_and_compl (live, live, call_used_hard_regs[MIR_T_UNDEF]);\n      FOREACH_IN_INSN_VAR (gen_ctx, insn_var_iter, insn, var, op_num) {\n        if ((scan_var = var_to_scan_var (gen_ctx, var)) < 0) continue;\n        if (bitmap_set_bit_p (live, scan_var)) add_bb_insn_dead_var (gen_ctx, bb_insn, var);\n      }\n      if (scan_vars_num != 0) continue;\n      target_get_early_clobbered_hard_regs (insn, &early_clobbered_hard_reg1,\n                                            &early_clobbered_hard_reg2);\n      if (early_clobbered_hard_reg1 != MIR_NON_VAR)\n        bitmap_clear_bit_p (live, early_clobbered_hard_reg1);\n      if (early_clobbered_hard_reg2 != MIR_NON_VAR)\n        bitmap_clear_bit_p (live, early_clobbered_hard_reg2);\n      if (MIR_call_code_p (insn->code)) bitmap_ior (live, live, bb_insn->call_hard_reg_args);\n    }\n  }\n  bitmap_destroy (live);\n}\n\n#if !MIR_NO_GEN_DEBUG\nstatic void output_bb_border_live_info (gen_ctx_t gen_ctx, bb_t bb) {\n  MIR_reg_t *map = scan_vars_num == 0 ? NULL : VARR_ADDR (MIR_reg_t, scan_var_to_var_map);\n  output_bitmap (gen_ctx, \"  live_in:\", bb->live_in, TRUE, map);\n  output_bitmap (gen_ctx, \"  live_out:\", bb->live_out, TRUE, map);\n}\n\nstatic void output_bb_live_info (gen_ctx_t gen_ctx, bb_t bb) {\n  MIR_reg_t *map = scan_vars_num == 0 ? NULL : VARR_ADDR (MIR_reg_t, scan_var_to_var_map);\n  output_bb_border_live_info (gen_ctx, bb);\n  output_bitmap (gen_ctx, \"  live_gen:\", bb->live_gen, TRUE, map);\n  output_bitmap (gen_ctx, \"  live_kill:\", bb->live_kill, TRUE, map);\n}\n#endif\n\nstatic void print_live_info (gen_ctx_t gen_ctx, const char *title, int dead_var_p, int pressure_p) {\n  DEBUG (2, {\n    if (dead_var_p) add_bb_insn_dead_vars (gen_ctx);\n    fprintf (debug_file, \"+++++++++++++%s:\\n\", title);\n    print_loop_tree (gen_ctx, TRUE);\n    print_CFG (gen_ctx, TRUE, pressure_p, TRUE, TRUE, output_bb_live_info);\n  });\n}\n\n#undef live_kill\n#undef live_gen\n\nstatic lr_bb_t create_lr_bb (gen_ctx_t gen_ctx, bb_t bb, lr_bb_t next) {\n  lr_bb_t lr_bb;\n  if ((lr_bb = free_lr_bb_list) != NULL) {\n    free_lr_bb_list = free_lr_bb_list->next;\n  } else {\n    lr_bb = gen_malloc (gen_ctx, sizeof (struct lr_bb));\n  }\n  lr_bb->bb = bb;\n  lr_bb->next = next;\n  return lr_bb;\n}\n\nstatic void free_lr_bbs (gen_ctx_t gen_ctx, lr_bb_t list) {\n  for (lr_bb_t lr_bb = list; lr_bb != NULL; lr_bb = list) {\n    list = lr_bb->next;\n    lr_bb->next = free_lr_bb_list;\n    free_lr_bb_list = lr_bb;\n  }\n}\n\nstatic void init_lr_bbs (gen_ctx_t gen_ctx) { free_lr_bb_list = NULL; }\nstatic void finish_lr_bbs (gen_ctx_t gen_ctx) {\n  for (lr_bb_t lr_bb = free_lr_bb_list; lr_bb != NULL; lr_bb = free_lr_bb_list) {\n    free_lr_bb_list = lr_bb->next;\n    gen_free (gen_ctx, lr_bb);\n  }\n}\n\nstatic void free_one_live_range (gen_ctx_t gen_ctx, live_range_t lr) {\n  free_lr_bbs (gen_ctx, lr->lr_bb);\n  lr->next = free_lr_list;\n  free_lr_list = lr;\n}\n\nstatic void free_live_ranges (gen_ctx_t gen_ctx, live_range_t list) {\n  for (live_range_t lr = list; lr != NULL; lr = list) {\n    list = lr->next;\n    free_one_live_range (gen_ctx, lr);\n  }\n}\n\nstatic live_range_t create_live_range (gen_ctx_t gen_ctx, int start, int finish,\n                                       live_range_t next) {\n  live_range_t lr;\n  if ((lr = free_lr_list) != NULL) {\n    free_lr_list = free_lr_list->next;\n  } else {\n    lr = gen_malloc (gen_ctx, sizeof (struct live_range));\n  }\n  gen_assert (start >= 0);\n  gen_assert (finish < 0 || start <= finish);\n  lr->start = start;\n  lr->finish = finish;\n  lr->ref_cost = 1;\n  lr->next = next;\n  lr->lr_bb = NULL;\n  return lr;\n}\n\nstatic void move_lr_bbs (live_range_t from, live_range_t to) {\n  lr_bb_t lr_bb, next_lr_bb;\n  for (lr_bb = from->lr_bb; lr_bb != NULL; lr_bb = from->lr_bb) {\n    next_lr_bb = lr_bb->next;\n    lr_bb->next = to->lr_bb;\n    to->lr_bb = lr_bb;\n    from->lr_bb = next_lr_bb;\n  }\n}\n\nstatic void init_lrs (gen_ctx_t gen_ctx) { free_lr_list = NULL; }\nstatic void finish_lrs (gen_ctx_t gen_ctx) {\n  for (live_range_t lr = free_lr_list; lr != NULL; lr = free_lr_list) {\n    free_lr_list = lr->next;\n    gen_free (gen_ctx, lr);\n  }\n}\n\nstatic inline int make_var_dead (gen_ctx_t gen_ctx, MIR_reg_t var, int scan_var, int point,\n                                 int insn_p) {\n  live_range_t lr;\n  if (insn_p && scan_vars_num == 0) bitmap_set_bit_p (referenced_vars, var);\n  lr = VARR_GET (live_range_t, var_live_ranges, var);\n  if (bitmap_clear_bit_p (live_vars, scan_var)) {\n    lr->finish = point;\n  } else {\n    /* insn with unused result: result still needs a hard register */\n    VARR_SET (live_range_t, var_live_ranges, var, create_live_range (gen_ctx, point, point, lr));\n  }\n  return TRUE;\n}\n\nstatic inline int make_var_live (gen_ctx_t gen_ctx, MIR_reg_t var, int scan_var, int point,\n                                 int insn_p) {\n  live_range_t lr;\n  lr = VARR_GET (live_range_t, var_live_ranges, var);\n  if (insn_p && scan_vars_num == 0) bitmap_set_bit_p (referenced_vars, var);\n  if (!bitmap_set_bit_p (live_vars, scan_var)) return FALSE;\n  /* Always start new live range for starting living at bb end or if\n     the last live range is covering a whole bb: */\n  if (!insn_p || lr == NULL || lr->lr_bb != NULL\n      || (lr->finish != point && lr->finish + 1 != point)) {\n    VARR_SET (live_range_t, var_live_ranges, var, create_live_range (gen_ctx, point, -1, lr));\n  }\n  return TRUE;\n}\n\nstatic void add_lr_bb (gen_ctx_t gen_ctx, MIR_reg_t var, bb_t bb) {\n  live_range_t lr = VARR_GET (live_range_t, var_live_ranges, var);\n  gen_assert (lr != NULL && lr->lr_bb == NULL);\n  lr->lr_bb = create_lr_bb (gen_ctx, bb, NULL);\n}\n\n#if !MIR_NO_GEN_DEBUG\nstatic void print_live_range (gen_ctx_t gen_ctx, live_range_t lr) {\n  fprintf (debug_file, \" [%d..%d]\", lr->start, lr->finish);\n  if (lr->lr_bb == NULL) return;\n  for (lr_bb_t lr_bb = lr->lr_bb; lr_bb != NULL; lr_bb = lr_bb->next)\n    fprintf (debug_file, \"%cbb%lu\", lr_bb == lr->lr_bb ? '(' : ' ',\n             (long unsigned) lr_bb->bb->index);\n  fprintf (debug_file, \")\");\n}\n\nstatic void print_live_ranges (gen_ctx_t gen_ctx, live_range_t lr) {\n  for (; lr != NULL; lr = lr->next) print_live_range (gen_ctx, lr);\n  fprintf (debug_file, \"\\n\");\n}\n\nstatic void print_all_live_ranges (gen_ctx_t gen_ctx) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  live_range_t lr;\n\n  fprintf (debug_file, \"+++++++++++++Live ranges:\\n\");\n  for (size_t i = 0; i < VARR_LENGTH (live_range_t, var_live_ranges); i++) {\n    if ((lr = VARR_GET (live_range_t, var_live_ranges, i)) == NULL) continue;\n    fprintf (debug_file, \"%lu\", (unsigned long) i);\n    if (scan_vars_num != 0)\n      fprintf (debug_file, \" (%lu)\", (unsigned long) var_to_scan_var (gen_ctx, (MIR_reg_t) i));\n    if (i > MAX_HARD_REG)\n      fprintf (debug_file, \" (%s:%s)\",\n               MIR_type_str (ctx, MIR_reg_type (ctx, (MIR_reg_t) (i - MAX_HARD_REG),\n                                                curr_func_item->u.func)),\n               MIR_reg_name (ctx, (MIR_reg_t) (i - MAX_HARD_REG), curr_func_item->u.func));\n    fprintf (debug_file, \":\");\n    print_live_ranges (gen_ctx, lr);\n  }\n}\n#endif\n\nstatic void shrink_live_ranges (gen_ctx_t gen_ctx) {\n  size_t p;\n  long int pn, rn, old_rn;\n  live_range_t lr, prev_lr, next_lr;\n  int born_p, dead_p, prev_dead_p;\n  bitmap_iterator_t bi;\n\n  bitmap_clear (points_with_born_vars);\n  bitmap_clear (points_with_dead_vars);\n  for (size_t i = 0; i < VARR_LENGTH (live_range_t, var_live_ranges); i++) {\n    for (lr = VARR_GET (live_range_t, var_live_ranges, i); lr != NULL; lr = lr->next) {\n      gen_assert (lr->start <= lr->finish);\n      bitmap_set_bit_p (points_with_born_vars, lr->start);\n      bitmap_set_bit_p (points_with_dead_vars, lr->finish);\n    }\n  }\n\n  VARR_TRUNC (int, point_map, 0);\n  for (int i = 0; i <= curr_point; i++) VARR_PUSH (int, point_map, 0);\n  bitmap_ior (points_with_born_or_dead_vars, points_with_born_vars, points_with_dead_vars);\n  pn = -1;\n  prev_dead_p = TRUE;\n  FOREACH_BITMAP_BIT (bi, points_with_born_or_dead_vars, p) {\n    born_p = bitmap_bit_p (points_with_born_vars, p);\n    dead_p = bitmap_bit_p (points_with_dead_vars, p);\n    assert (born_p || dead_p);\n    if (!prev_dead_p || !born_p) /* 1st point is always a born */\n      VARR_SET (int, point_map, p, pn);\n    else\n      VARR_SET (int, point_map, p, ++pn);\n    prev_dead_p = dead_p;\n  }\n  pn++;\n\n  old_rn = rn = 0;\n  for (size_t i = 0; i < VARR_LENGTH (live_range_t, var_live_ranges); i++) {\n    for (lr = VARR_GET (live_range_t, var_live_ranges, i), prev_lr = NULL; lr != NULL;\n         lr = next_lr) {\n      old_rn++;\n      next_lr = lr->next;\n      lr->start = VARR_GET (int, point_map, lr->start);\n      lr->finish = VARR_GET (int, point_map, lr->finish);\n      if (prev_lr == NULL || (prev_lr->start != lr->finish && prev_lr->start != lr->finish + 1)\n          || (prev_lr->lr_bb != NULL && lr->lr_bb == NULL)\n          || (prev_lr->lr_bb == NULL && lr->lr_bb != NULL)) {\n        rn++;\n        prev_lr = lr;\n        continue;\n      }\n      prev_lr->start = lr->start;\n      prev_lr->next = next_lr;\n      move_lr_bbs (lr, prev_lr);\n      free_one_live_range (gen_ctx, lr);\n    }\n  }\n  DEBUG (2, {\n    fprintf (debug_file, \"Compressing live range points: from %d to %ld - %ld%%\\n\", curr_point, pn,\n             curr_point == 0 ? 100 : 100 * pn / curr_point);\n    if (rn != old_rn)\n      fprintf (debug_file, \"Compressing live ranges: from %ld to %ld - %ld%%\\n\", old_rn, rn,\n               rn == 0 ? 100 : 100 * rn / old_rn);\n  });\n  curr_point = pn;\n  DEBUG (2, {\n    fprintf (debug_file, \"Ranges after the compression:\\n\");\n    print_all_live_ranges (gen_ctx);\n  });\n}\n\n#define spill_gen gen   /* pseudo regs fully spilled in BB */\n#define spill_kill kill /* pseudo regs referenced in the BB */\n\nstatic void process_bb_ranges (gen_ctx_t gen_ctx, bb_t bb, MIR_insn_t start_insn,\n                               MIR_insn_t tail_insn) {\n  MIR_reg_t var, reg, early_clobbered_hard_reg1, early_clobbered_hard_reg2;\n  size_t nel;\n  int scan_var, op_num, incr_p;\n  bitmap_iterator_t bi;\n  insn_var_iterator_t insn_var_iter;\n\n  DEBUG (2, {\n    fprintf (debug_file, \"  ------BB%u end: point=%d\\n\", (unsigned) bb->index, curr_point);\n  });\n  bitmap_clear (referenced_vars);\n  bitmap_clear (live_vars);\n  if (bb->live_out != NULL) FOREACH_BITMAP_BIT (bi, bb->live_out, nel) {\n      make_var_live (gen_ctx, scan_var_to_var (gen_ctx, (int) nel), (int) nel, curr_point, FALSE);\n    }\n  for (MIR_insn_t insn = tail_insn;; insn = DLIST_PREV (MIR_insn_t, insn)) {\n    DEBUG (2, {\n      fprintf (debug_file, \"  p%-5d\", curr_point);\n      MIR_output_insn (gen_ctx->ctx, debug_file, insn, curr_func_item->u.func, TRUE);\n    });\n    if (insn->code == MIR_PHI) {\n      if ((scan_var = var_to_scan_var (gen_ctx, insn->ops[0].u.var)) >= 0) {\n        make_var_dead (gen_ctx, insn->ops[0].u.var, scan_var, curr_point, TRUE);\n        curr_point++;\n      }\n      if (insn == start_insn) break;\n      continue;\n    }\n    incr_p = FALSE;\n    FOREACH_OUT_INSN_VAR (gen_ctx, insn_var_iter, insn, var, op_num) {\n      if ((scan_var = var_to_scan_var (gen_ctx, var)) < 0) continue;\n      incr_p |= make_var_dead (gen_ctx, var, scan_var, curr_point, TRUE);\n    }\n    if (scan_vars_num == 0 && MIR_call_code_p (insn->code)) {\n      if (incr_p) curr_point++;\n      incr_p = FALSE;\n      FOREACH_BITMAP_BIT (bi, call_used_hard_regs[MIR_T_UNDEF], nel) {\n        make_var_dead (gen_ctx, (MIR_reg_t) nel, (int) nel, curr_point, TRUE);\n        incr_p = TRUE;\n      }\n      bitmap_t args = (optimize_level > 0 ? ((bb_insn_t) insn->data)->call_hard_reg_args\n                                          : ((insn_data_t) insn->data)->u.call_hard_reg_args);\n      if (args != NULL) {\n        FOREACH_BITMAP_BIT (bi, args, nel) {\n          make_var_live (gen_ctx, (MIR_reg_t) nel, (int) nel, curr_point, TRUE);\n        }\n      }\n      FOREACH_BITMAP_BIT (bi, live_vars, nel) {\n        MIR_reg_t live_reg;\n\n        if (nel <= MAX_HARD_REG) continue;\n        live_reg = (MIR_reg_t) nel;\n        bitmap_set_bit_p (curr_cfg->call_crossed_regs, live_reg);\n      }\n    }\n    if (incr_p) curr_point++;\n    incr_p = FALSE;\n    FOREACH_IN_INSN_VAR (gen_ctx, insn_var_iter, insn, var, op_num) {\n      if ((scan_var = var_to_scan_var (gen_ctx, var)) < 0) continue;\n      incr_p |= make_var_live (gen_ctx, var, scan_var, curr_point, TRUE);\n    }\n    if (scan_vars_num == 0) {\n      target_get_early_clobbered_hard_regs (insn, &early_clobbered_hard_reg1,\n                                            &early_clobbered_hard_reg2);\n      if ((reg = early_clobbered_hard_reg1) != MIR_NON_VAR) {\n        incr_p |= make_var_live (gen_ctx, reg, (int) reg, curr_point, TRUE);\n        incr_p |= make_var_dead (gen_ctx, reg, (int) reg, curr_point, TRUE);\n      }\n      if ((reg = early_clobbered_hard_reg2) != MIR_NON_VAR) {\n        incr_p |= make_var_live (gen_ctx, reg, (int) reg, curr_point, TRUE);\n        incr_p |= make_var_dead (gen_ctx, reg, (int) reg, curr_point, TRUE);\n      }\n    }\n    if (incr_p) curr_point++;\n    if (insn == start_insn) break;\n  }\n  gen_assert (bitmap_equal_p (bb->live_in, live_vars));\n  FOREACH_BITMAP_BIT (bi, bb->live_in, nel) {\n    make_var_dead (gen_ctx, scan_var_to_var (gen_ctx, (int) nel), (int) nel, curr_point, FALSE);\n    if (scan_vars_num == 0 && !bitmap_bit_p (referenced_vars, nel))\n      add_lr_bb (gen_ctx, (MIR_reg_t) nel, bb);\n  }\n  if (scan_vars_num == 0) { /* setup spill info for RA */\n    bitmap_clear (bb->spill_gen);\n    bitmap_clear (bb->spill_kill);\n    FOREACH_BITMAP_BIT (bi, referenced_vars, nel) {\n      if (nel > MAX_HARD_REG) bitmap_set_bit_p (bb->spill_kill, nel);\n    }\n  }\n  if (!bitmap_empty_p (bb->live_in)) curr_point++;\n}\n\n#undef spill_gen\n#undef spill_kill\n\nstatic void build_live_ranges (gen_ctx_t gen_ctx) {\n  size_t i;\n  MIR_reg_t max_var;\n  MIR_insn_t insn, next_insn, head_insn;\n  bb_t bb;\n\n  curr_point = 0;\n  max_var = get_max_var (gen_ctx);\n  gen_assert (VARR_LENGTH (live_range_t, var_live_ranges) == 0);\n  for (i = 0; i <= max_var; i++) VARR_PUSH (live_range_t, var_live_ranges, NULL);\n  if (optimize_level == 0) {\n    for (head_insn = insn = DLIST_HEAD (MIR_insn_t, curr_func_item->u.func->insns); insn != NULL;\n         insn = next_insn) {\n      next_insn = DLIST_NEXT (MIR_insn_t, insn);\n      bb = get_insn_bb (gen_ctx, head_insn);\n      if (next_insn == NULL || bb != get_insn_bb (gen_ctx, next_insn)) {\n        process_bb_ranges (gen_ctx, bb, head_insn, insn);\n        head_insn = next_insn;\n      }\n    }\n  } else {\n    VARR_TRUNC (bb_t, worklist, 0);\n    for (bb = DLIST_HEAD (bb_t, curr_cfg->bbs); bb != NULL; bb = DLIST_NEXT (bb_t, bb))\n      VARR_PUSH (bb_t, worklist, bb);\n    if (optimize_level <= 1) /* arrange BBs in PO (post order) for more compact ranges: */\n      qsort (VARR_ADDR (bb_t, worklist), VARR_LENGTH (bb_t, worklist), sizeof (bb), post_cmp);\n    for (i = 0; i < VARR_LENGTH (bb_t, worklist); i++) {\n      bb = VARR_GET (bb_t, worklist, i);\n      if (DLIST_HEAD (bb_insn_t, bb->bb_insns) == NULL) continue;\n      process_bb_ranges (gen_ctx, bb, DLIST_HEAD (bb_insn_t, bb->bb_insns)->insn,\n                         DLIST_TAIL (bb_insn_t, bb->bb_insns)->insn);\n    }\n  }\n  DEBUG (2, { print_all_live_ranges (gen_ctx); });\n  shrink_live_ranges (gen_ctx);\n}\n\nstatic void free_func_live_ranges (gen_ctx_t gen_ctx) {\n  size_t i;\n\n  for (i = 0; i < VARR_LENGTH (live_range_t, var_live_ranges); i++)\n    free_live_ranges (gen_ctx, VARR_GET (live_range_t, var_live_ranges, i));\n  VARR_TRUNC (live_range_t, var_live_ranges, 0);\n}\n\nstatic void init_live_ranges (gen_ctx_t gen_ctx) {\n  MIR_alloc_t alloc = gen_alloc (gen_ctx);\n  gen_ctx->lr_ctx = gen_malloc (gen_ctx, sizeof (struct lr_ctx));\n  VARR_CREATE (int, var_to_scan_var_map, alloc, 0);\n  VARR_CREATE (MIR_reg_t, scan_var_to_var_map, alloc, 0);\n  init_lr_bbs (gen_ctx);\n  init_lrs (gen_ctx);\n  VARR_CREATE (live_range_t, var_live_ranges, alloc, 0);\n  VARR_CREATE (int, point_map, alloc, 1024);\n  live_vars = bitmap_create2 (alloc, DEFAULT_INIT_BITMAP_BITS_NUM);\n  referenced_vars = bitmap_create2 (alloc, DEFAULT_INIT_BITMAP_BITS_NUM);\n  points_with_born_vars = bitmap_create2 (alloc, DEFAULT_INIT_BITMAP_BITS_NUM);\n  points_with_dead_vars = bitmap_create2 (alloc, DEFAULT_INIT_BITMAP_BITS_NUM);\n  points_with_born_or_dead_vars = bitmap_create2 (alloc, DEFAULT_INIT_BITMAP_BITS_NUM);\n}\n\nstatic void finish_live_ranges (gen_ctx_t gen_ctx) {\n  VARR_DESTROY (live_range_t, var_live_ranges);\n  VARR_DESTROY (int, point_map);\n  bitmap_destroy (live_vars);\n  bitmap_destroy (referenced_vars);\n  bitmap_destroy (points_with_born_vars);\n  bitmap_destroy (points_with_dead_vars);\n  bitmap_destroy (points_with_born_or_dead_vars);\n  finish_lrs (gen_ctx);\n  finish_lr_bbs (gen_ctx);\n  VARR_DESTROY (int, var_to_scan_var_map);\n  VARR_DESTROY (MIR_reg_t, scan_var_to_var_map);\n  gen_free (gen_ctx, gen_ctx->lr_ctx);\n  gen_ctx->lr_ctx = NULL;\n}\n\n/* New page */\n\n/* Jump optimizations */\n\n/* Remove empty blocks, branches to next insn, change branches to\n   jumps.  ??? consider switch as a branch. */\nstatic void jump_opt (gen_ctx_t gen_ctx) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  bb_t bb, next_bb;\n  int maybe_unreachable_bb_p = FALSE;\n  long bb_deleted_insns_num;\n\n  if ((bb_deleted_insns_num = remove_unreachable_bbs (gen_ctx)) != 0) {\n    DEBUG (1, { fprintf (debug_file, \"%ld deleted unrechable bb insns\\n\", bb_deleted_insns_num); });\n  }\n  bitmap_clear (temp_bitmap);\n  for (bb = DLIST_EL (bb_t, curr_cfg->bbs, 2); bb != NULL; bb = DLIST_NEXT (bb_t, bb)) {\n    bb_insn_t bb_insn;\n    int i, start_nop, bound_nop;\n\n    if ((bb_insn = DLIST_TAIL (bb_insn_t, bb->bb_insns)) == NULL) continue;\n    if (bb_insn->insn->code == MIR_SWITCH) {\n      start_nop = 1;\n      bound_nop = bb_insn->insn->nops;\n    } else if (MIR_branch_code_p (bb_insn->insn->code)) {\n      start_nop = 0;\n      bound_nop = 1;\n    } else {\n      continue;\n    }\n    for (i = start_nop; i < bound_nop; i++)\n      bitmap_set_bit_p (temp_bitmap, bb_insn->insn->ops[i].u.label->ops[0].u.u);\n  }\n  for (bb = DLIST_EL (bb_t, curr_cfg->bbs, 2); bb != NULL; bb = next_bb) {\n    edge_t e, out_e;\n    bb_insn_t label_bb_insn, last_label_bb_insn, bb_insn = DLIST_TAIL (bb_insn_t, bb->bb_insns);\n    MIR_insn_t insn, next_insn, last_label;\n\n    next_bb = DLIST_NEXT (bb_t, bb);\n    if (bb->index != 2 /* BB2 will be used for machinize */\n        && (e = DLIST_HEAD (in_edge_t, bb->in_edges)) != NULL && DLIST_NEXT (in_edge_t, e) == NULL\n        && (bb_insn == NULL\n            || ((insn = bb_insn->insn)->code == MIR_LABEL && DLIST_NEXT (bb_insn_t, bb_insn) == NULL\n                && DLIST_PREV (bb_insn_t, bb_insn) == NULL\n                && !bitmap_bit_p (temp_bitmap, insn->ops[0].u.u)))) {\n      /* empty bb or bb with the only label which can be removed. we can have more one the same\n         dest edge (e.g. when removed cond branch to the next insn). */\n      out_e = DLIST_HEAD (out_edge_t, bb->out_edges);\n      gen_assert (out_e != NULL);\n      e->dst = out_e->dst;\n      DLIST_REMOVE (in_edge_t, bb->in_edges, e);\n      DLIST_INSERT_BEFORE (in_edge_t, out_e->dst->in_edges, out_e, e);\n      gen_assert (DLIST_HEAD (in_edge_t, bb->in_edges) == NULL);\n      /* Don't shorten phis in dest bbs. We don't care about SSA in this kind of bb. */\n      remove_bb (gen_ctx, bb);\n      continue;\n    }\n    if (bb_insn == NULL) continue;\n    insn = bb_insn->insn;\n    if (!MIR_branch_code_p (insn->code)) continue;\n    DEBUG (2, { fprintf (debug_file, \"  BB%lu:\\n\", (unsigned long) bb->index); });\n    gen_assert (insn->ops[0].mode == MIR_OP_LABEL);\n    if ((next_insn = DLIST_NEXT (MIR_insn_t, insn)) != NULL && next_insn->code == MIR_LABEL\n        && next_insn == insn->ops[0].u.label) {\n      DEBUG (2, {\n        fprintf (debug_file, \"  Removing trivial branch insn \");\n        MIR_output_insn (ctx, debug_file, insn, curr_func_item->u.func, TRUE);\n      });\n      out_e = DLIST_HEAD (out_edge_t, bb->out_edges);\n      out_e->fall_through_p = TRUE;\n      e = DLIST_NEXT (out_edge_t, out_e);\n      gen_assert (e == NULL || DLIST_NEXT (out_edge_t, e) == NULL);\n      if (e != NULL) delete_edge (gen_ctx, e);\n      gen_delete_insn (gen_ctx, insn);\n      next_bb = bb; /* bb can became empty after removing jump.  */\n    } else {\n      for (;;) {\n        for (last_label = insn->ops[0].u.label;\n             (next_insn = DLIST_NEXT (MIR_insn_t, last_label)) != NULL\n             && next_insn->code == MIR_LABEL;)\n          last_label = next_insn;\n        if ((next_insn = DLIST_NEXT (MIR_insn_t, last_label)) != NULL\n            && next_insn->code == MIR_JMP) {\n          last_label = next_insn->ops[0].u.label;\n        }\n        if (insn->ops[0].u.label == last_label) break;\n        DEBUG (2, {\n          fprintf (debug_file, \"  Changing label in branch insn \");\n          MIR_output_insn (ctx, debug_file, insn, curr_func_item->u.func, FALSE);\n        });\n        label_bb_insn = insn->ops[0].u.label->data;\n        insn->ops[0].u.label = last_label;\n        last_label_bb_insn = last_label->data;\n        gen_assert (label_bb_insn->bb != last_label_bb_insn->bb);\n        e = find_edge (bb, label_bb_insn->bb);\n        e->dst = last_label_bb_insn->bb;\n        DLIST_REMOVE (in_edge_t, label_bb_insn->bb->in_edges, e);\n        /* We don't need to keep the edge order as we are already off SSA: */\n        DLIST_APPEND (in_edge_t, e->dst->in_edges, e);\n        DEBUG (2, {\n          fprintf (debug_file, \"  , result insn \");\n          MIR_output_insn (ctx, debug_file, insn, curr_func_item->u.func, TRUE);\n        });\n        maybe_unreachable_bb_p = TRUE;\n      }\n    }\n  }\n  /* Don't shorten phis in dest bbs. We don't care about SSA for new trivial unreachable bbs. */\n  if (maybe_unreachable_bb_p) remove_unreachable_bbs (gen_ctx);\n  enumerate_bbs (gen_ctx);\n}\n\n/* New Page */\n/* Aggressive register coalescing */\n\ntypedef struct mv {\n  bb_insn_t bb_insn;\n  size_t freq;\n} mv_t;\n\nDEF_VARR (mv_t);\n\nstruct coalesce_ctx {\n  VARR (mv_t) * moves;\n  /* the first and the next res in the coalesced regs group */\n  VARR (MIR_reg_t) * first_coalesced_reg, *next_coalesced_reg;\n  bitmap_t conflict_matrix;\n};\n\n#define moves gen_ctx->coalesce_ctx->moves\n#define first_coalesced_reg gen_ctx->coalesce_ctx->first_coalesced_reg\n#define next_coalesced_reg gen_ctx->coalesce_ctx->next_coalesced_reg\n#define conflict_matrix gen_ctx->coalesce_ctx->conflict_matrix\n\nstatic void set_scan_var_conflict (gen_ctx_t gen_ctx, int scan_var1, int scan_var2) {\n  int temp_scan_var;\n  if (scan_var1 > scan_var2) SWAP (scan_var1, scan_var2, temp_scan_var);\n  bitmap_set_bit_p (conflict_matrix, scan_var1 * scan_vars_num + scan_var2);\n}\n\nstatic int scan_var_conflict_p (gen_ctx_t gen_ctx, int scan_var1, int scan_var2) {\n  int temp_scan_var;\n  if (scan_var1 > scan_var2) SWAP (scan_var1, scan_var2, temp_scan_var);\n  return bitmap_bit_p (conflict_matrix, scan_var1 * scan_vars_num + scan_var2);\n}\n\nstatic void process_bb_conflicts (gen_ctx_t gen_ctx, bb_t bb, MIR_insn_t start_insn,\n                                  MIR_insn_t tail_insn) {\n  MIR_reg_t var;\n  size_t nel;\n  int scan_var, ignore_scan_var, live_scan_var, op_num;\n  bitmap_iterator_t bi;\n  insn_var_iterator_t insn_var_iter;\n\n  bitmap_clear (live_vars);\n  if (bb->live_out != NULL) FOREACH_BITMAP_BIT (bi, bb->live_out, nel) {\n      bitmap_set_bit_p (live_vars, nel);\n    }\n  for (MIR_insn_t insn = tail_insn;; insn = DLIST_PREV (MIR_insn_t, insn)) {\n    ignore_scan_var = -1;\n    if (move_p (insn)) ignore_scan_var = var_to_scan_var (gen_ctx, insn->ops[1].u.var);\n    FOREACH_OUT_INSN_VAR (gen_ctx, insn_var_iter, insn, var, op_num) {\n      if ((scan_var = var_to_scan_var (gen_ctx, var)) < 0) continue;\n      FOREACH_BITMAP_BIT (bi, live_vars, nel) {\n        live_scan_var = (MIR_reg_t) nel;\n        if (live_scan_var != scan_var && live_scan_var != ignore_scan_var)\n          set_scan_var_conflict (gen_ctx, scan_var, live_scan_var);\n      }\n      bitmap_clear_bit_p (live_vars, scan_var);\n    }\n    FOREACH_IN_INSN_VAR (gen_ctx, insn_var_iter, insn, var, op_num) {\n      if ((scan_var = var_to_scan_var (gen_ctx, var)) >= 0) bitmap_set_bit_p (live_vars, scan_var);\n    }\n    if (insn == start_insn) break;\n  }\n  gen_assert (bitmap_equal_p (bb->live_in, live_vars));\n}\n\nstatic void build_conflict_matrix (gen_ctx_t gen_ctx) {\n  bitmap_clear (conflict_matrix);\n  for (bb_t bb = DLIST_HEAD (bb_t, curr_cfg->bbs); bb != NULL; bb = DLIST_NEXT (bb_t, bb)) {\n    if (DLIST_HEAD (bb_insn_t, bb->bb_insns) == NULL) continue;\n    process_bb_conflicts (gen_ctx, bb, DLIST_HEAD (bb_insn_t, bb->bb_insns)->insn,\n                          DLIST_TAIL (bb_insn_t, bb->bb_insns)->insn);\n  }\n  DEBUG (2, {\n    fprintf (debug_file, \"  Conflict matrix size=%lu, scan vars = %d\\n\",\n             (unsigned long) bitmap_size (conflict_matrix), scan_vars_num);\n  });\n}\n\nstatic int substitute_reg (gen_ctx_t gen_ctx, MIR_reg_t *reg) {\n  if (*reg == MIR_NON_VAR || VARR_GET (MIR_reg_t, first_coalesced_reg, *reg) == *reg) return FALSE;\n  *reg = VARR_GET (MIR_reg_t, first_coalesced_reg, *reg);\n  return TRUE;\n}\n\nstatic int mv_freq_cmp (const void *v1p, const void *v2p) {\n  const mv_t *mv1 = (const mv_t *) v1p;\n  const mv_t *mv2 = (const mv_t *) v2p;\n\n  if (mv1->freq > mv2->freq) return -1;\n  if (mv1->freq < mv2->freq) return 1;\n  return (long) mv1->bb_insn->index - (long) mv2->bb_insn->index;\n}\n\nstatic int var_conflict_p (gen_ctx_t gen_ctx, MIR_reg_t var1, MIR_reg_t var2) {\n  gen_assert (var1 == VARR_GET (MIR_reg_t, first_coalesced_reg, var1));\n  gen_assert (var2 == VARR_GET (MIR_reg_t, first_coalesced_reg, var2));\n  for (MIR_reg_t last_reg1 = var1, reg1 = VARR_GET (MIR_reg_t, next_coalesced_reg, var1);;\n       reg1 = VARR_GET (MIR_reg_t, next_coalesced_reg, reg1)) {\n    int scan_var1 = var_to_scan_var (gen_ctx, reg1);\n    for (MIR_reg_t last_reg2 = var2, reg2 = VARR_GET (MIR_reg_t, next_coalesced_reg, var2);;\n         reg2 = VARR_GET (MIR_reg_t, next_coalesced_reg, reg2)) {\n      int scan_var2 = var_to_scan_var (gen_ctx, reg2);\n      if (scan_var_conflict_p (gen_ctx, scan_var1, scan_var2)) return TRUE;\n      if (reg2 == last_reg2) break;\n    }\n    if (reg1 == last_reg1) break;\n  }\n  return FALSE;\n}\n\n/* Merge two sets of coalesced regs given correspondingly by regs REG1 and REG2.  */\nstatic void merge_regs (gen_ctx_t gen_ctx, MIR_reg_t reg1, MIR_reg_t reg2) {\n  MIR_reg_t reg, first, first2, last, next, temp;\n\n  first = VARR_GET (MIR_reg_t, first_coalesced_reg, reg1);\n  if ((first2 = VARR_GET (MIR_reg_t, first_coalesced_reg, reg2)) == first) return;\n  if (MIR_reg_hard_reg_name (gen_ctx->ctx, first2 - MAX_HARD_REG, curr_func_item->u.func) != NULL\n      || (MIR_reg_hard_reg_name (gen_ctx->ctx, first - MAX_HARD_REG, curr_func_item->u.func) == NULL\n          && first2 < first)) {\n    SWAP (first, first2, temp);\n    SWAP (reg1, reg2, temp);\n  }\n  for (last = reg2, reg = VARR_GET (MIR_reg_t, next_coalesced_reg, reg2);;\n       reg = VARR_GET (MIR_reg_t, next_coalesced_reg, reg)) {\n    VARR_SET (MIR_reg_t, first_coalesced_reg, reg, first);\n    if (reg == reg2) break;\n    last = reg;\n  }\n  next = VARR_GET (MIR_reg_t, next_coalesced_reg, first);\n  VARR_SET (MIR_reg_t, next_coalesced_reg, first, reg2);\n  VARR_SET (MIR_reg_t, next_coalesced_reg, last, next);\n}\n\nstatic void update_bitmap_after_coalescing (gen_ctx_t gen_ctx, bitmap_t bm) {\n  MIR_reg_t reg, first_reg;\n  size_t nel;\n  bitmap_iterator_t bi;\n\n  FOREACH_BITMAP_BIT (bi, bm, nel) {\n    reg = (MIR_reg_t) nel;\n    if (reg <= MAX_HARD_REG) continue;\n    if ((first_reg = VARR_GET (MIR_reg_t, first_coalesced_reg, reg)) == reg) continue;\n    bitmap_clear_bit_p (bm, reg);\n    bitmap_set_bit_p (bm, first_reg);\n  }\n}\n\nstatic void collect_moves (gen_ctx_t gen_ctx) {\n  gen_assert (optimize_level > 0);\n  VARR_TRUNC (mv_t, moves, 0);\n  for (bb_t bb = DLIST_HEAD (bb_t, curr_cfg->bbs); bb != NULL; bb = DLIST_NEXT (bb_t, bb)) {\n    for (bb_insn_t bb_insn = DLIST_HEAD (bb_insn_t, bb->bb_insns); bb_insn != NULL;\n         bb_insn = DLIST_NEXT (bb_insn_t, bb_insn)) {\n      MIR_insn_t insn = bb_insn->insn;\n      mv_t mv;\n\n      if (move_p (insn)) {\n        mv.bb_insn = bb_insn;\n        mv.freq = 1;\n        for (int i = bb_loop_level (bb); i > 0; i--)\n          if (mv.freq < SIZE_MAX / 8) mv.freq *= LOOP_COST_FACTOR;\n        VARR_PUSH (mv_t, moves, mv);\n      }\n    }\n  }\n  qsort (VARR_ADDR (mv_t, moves), VARR_LENGTH (mv_t, moves), sizeof (mv_t), mv_freq_cmp);\n}\n\nstatic void scan_collected_moves (gen_ctx_t gen_ctx) {\n  for (size_t i = 0; i < VARR_LENGTH (mv_t, moves); i++) {\n    mv_t mv = VARR_GET (mv_t, moves, i);\n    MIR_insn_t insn = mv.bb_insn->insn;\n    collect_scan_var (gen_ctx, insn->ops[0].u.var);\n    collect_scan_var (gen_ctx, insn->ops[1].u.var);\n  }\n}\n\nstatic void coalesce (gen_ctx_t gen_ctx) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  MIR_reg_t reg, sreg, dreg, first_reg, first_sreg, first_dreg, sreg_var, dreg_var;\n  MIR_insn_t insn, new_insn, next_insn;\n  bb_t bb;\n  bb_insn_t bb_insn;\n  mv_t mv;\n  size_t nops;\n  int coalesced_moves = 0, change_p;\n\n  gen_assert (optimize_level > 0);\n  VARR_TRUNC (MIR_reg_t, first_coalesced_reg, 0);\n  VARR_TRUNC (MIR_reg_t, next_coalesced_reg, 0);\n  for (MIR_reg_t i = 0; i <= curr_cfg->max_var; i++) {\n    VARR_PUSH (MIR_reg_t, first_coalesced_reg, i);\n    VARR_PUSH (MIR_reg_t, next_coalesced_reg, i);\n  }\n  build_conflict_matrix (gen_ctx);\n  /* Coalesced moves, most frequently executed first. */\n  for (size_t i = 0; i < VARR_LENGTH (mv_t, moves); i++) {\n    mv = VARR_GET (mv_t, moves, i);\n    bb_insn = mv.bb_insn;\n    insn = bb_insn->insn;\n    dreg = insn->ops[0].u.var;\n    sreg = insn->ops[1].u.var;\n    gen_assert (sreg > MAX_HARD_REG && dreg > MAX_HARD_REG);\n    first_sreg = VARR_GET (MIR_reg_t, first_coalesced_reg, sreg);\n    first_dreg = VARR_GET (MIR_reg_t, first_coalesced_reg, dreg);\n    if (first_sreg == first_dreg) {\n      coalesced_moves++;\n      DEBUG (2, {\n        fprintf (debug_file, \"Coalescing move r%d-r%d (freq=%llud):\", sreg, dreg,\n                 (unsigned long long) mv.freq);\n        print_bb_insn (gen_ctx, bb_insn, TRUE);\n      });\n    } else {\n      sreg_var = first_sreg;\n      dreg_var = first_dreg;\n      if (!var_conflict_p (gen_ctx, sreg_var, dreg_var)\n          && (MIR_reg_hard_reg_name (ctx, first_sreg - MAX_HARD_REG, curr_func_item->u.func) == NULL\n              || MIR_reg_hard_reg_name (ctx, first_dreg - MAX_HARD_REG, curr_func_item->u.func)\n                   == NULL)) {\n        coalesced_moves++;\n        DEBUG (2, {\n          fprintf (debug_file, \"Coalescing move r%d-r%d (freq=%llu):\", sreg, dreg,\n                   (unsigned long long) mv.freq);\n          print_bb_insn (gen_ctx, bb_insn, TRUE);\n        });\n        merge_regs (gen_ctx, sreg, dreg);\n      }\n    }\n  }\n  reg_info_t *reg_infos = VARR_ADDR (reg_info_t, curr_cfg->reg_info);\n  for (reg = MAX_HARD_REG + 1; reg <= curr_cfg->max_var; reg++) {\n    if ((first_reg = VARR_GET (MIR_reg_t, first_coalesced_reg, reg)) == reg) continue;\n    reg_infos[first_reg].freq += reg_infos[reg].freq;\n    reg_infos[reg].freq = 0;\n  }\n  for (size_t i = 0; i < VARR_LENGTH (mv_t, moves); i++) {\n    mv = VARR_GET (mv_t, moves, i);\n    bb_insn = mv.bb_insn;\n    bb = bb_insn->bb;\n    insn = bb_insn->insn;\n    dreg = insn->ops[0].u.var;\n    sreg = insn->ops[1].u.var;\n    gen_assert (sreg > MAX_HARD_REG && dreg > MAX_HARD_REG);\n    first_reg = VARR_GET (MIR_reg_t, first_coalesced_reg, sreg);\n    if (first_reg != VARR_GET (MIR_reg_t, first_coalesced_reg, dreg)) continue;\n    if (DLIST_TAIL (bb_insn_t, bb->bb_insns) == bb_insn\n        && DLIST_HEAD (bb_insn_t, bb->bb_insns) == bb_insn) { /* bb is becoming empty */\n      new_insn = MIR_new_label (ctx);\n      MIR_insert_insn_before (ctx, curr_func_item, insn, new_insn);\n      add_new_bb_insn (gen_ctx, new_insn, bb, FALSE);\n      DEBUG (2, {\n        fprintf (debug_file, \"Adding label for becoming empty BB \");\n        MIR_output_insn (ctx, debug_file, new_insn, curr_func_item->u.func, TRUE);\n      });\n    }\n    DEBUG (2, {\n      fprintf (debug_file, \"Deleting coalesced move \");\n      MIR_output_insn (ctx, debug_file, insn, curr_func_item->u.func, TRUE);\n    });\n    gen_delete_insn (gen_ctx, insn);\n  }\n  for (insn = DLIST_HEAD (MIR_insn_t, curr_func_item->u.func->insns); insn != NULL;\n       insn = next_insn) {\n    next_insn = DLIST_NEXT (MIR_insn_t, insn);\n    nops = MIR_insn_nops (ctx, insn);\n    change_p = FALSE;\n    for (size_t i = 0; i < nops; i++) {\n      MIR_op_t *op = &insn->ops[i];\n      switch (op->mode) {\n      case MIR_OP_VAR: change_p = substitute_reg (gen_ctx, &op->u.var) || change_p; break;\n      case MIR_OP_VAR_MEM:\n        change_p = substitute_reg (gen_ctx, &op->u.var_mem.base) || change_p;\n        change_p = substitute_reg (gen_ctx, &op->u.var_mem.index) || change_p;\n        break;\n      default: /* do nothing */ break;\n      }\n    }\n    if (change_p)\n      for (dead_var_t dv = DLIST_HEAD (dead_var_t, ((bb_insn_t) insn->data)->insn_dead_vars);\n           dv != NULL; dv = DLIST_NEXT (dead_var_t, dv)) {\n        if (dv->var <= MAX_HARD_REG) continue;\n        reg = dv->var;\n        if ((first_reg = VARR_GET (MIR_reg_t, first_coalesced_reg, reg)) != reg)\n          dv->var = first_reg;\n      }\n  }\n  /* Update live_in & live_out */\n  for (bb = DLIST_HEAD (bb_t, curr_cfg->bbs); bb != NULL; bb = DLIST_NEXT (bb_t, bb)) {\n    update_bitmap_after_coalescing (gen_ctx, bb->live_in);\n    update_bitmap_after_coalescing (gen_ctx, bb->live_out);\n  }\n  DEBUG (1, {\n    int moves_num = (int) VARR_LENGTH (mv_t, moves);\n    if (coalesced_moves != 0) {\n      fprintf (debug_file, \"Coalesced Moves = %d out of %d moves (%.1f%%)\\n\", coalesced_moves,\n               moves_num, 100.0 * coalesced_moves / moves_num);\n    }\n  });\n}\n#undef live_in\n#undef live_out\n\nstatic void init_coalesce (gen_ctx_t gen_ctx) {\n  MIR_alloc_t alloc = gen_alloc (gen_ctx);\n  gen_ctx->coalesce_ctx = gen_malloc (gen_ctx, sizeof (struct coalesce_ctx));\n  VARR_CREATE (mv_t, moves, alloc, 0);\n  VARR_CREATE (MIR_reg_t, first_coalesced_reg, alloc, 0);\n  VARR_CREATE (MIR_reg_t, next_coalesced_reg, alloc, 0);\n  conflict_matrix = bitmap_create (alloc);\n}\n\nstatic void finish_coalesce (gen_ctx_t gen_ctx) {\n  VARR_DESTROY (mv_t, moves);\n  VARR_DESTROY (MIR_reg_t, first_coalesced_reg);\n  VARR_DESTROY (MIR_reg_t, next_coalesced_reg);\n  bitmap_destroy (conflict_matrix);\n  gen_free (gen_ctx, gen_ctx->coalesce_ctx);\n  gen_ctx->coalesce_ctx = NULL;\n}\n\n/* New page */\n\nstatic void add_reload (gen_ctx_t gen_ctx, MIR_insn_t anchor, const MIR_op_t *op1,\n                        const MIR_op_t *op2, MIR_type_t type, int to_p) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  MIR_insn_t new_insn;\n  MIR_insn_code_t move_code = get_move_code (type);\n  if (to_p) {\n    new_insn = MIR_new_insn (ctx, move_code, *op1, *op2);\n    gen_add_insn_after (gen_ctx, anchor, new_insn);\n  } else {\n    new_insn = MIR_new_insn (ctx, move_code, *op2, *op1);\n    gen_add_insn_before (gen_ctx, anchor, new_insn);\n  }\n  DEBUG (2, {\n    fprintf (debug_file, \"    Add %s insn\", (to_p ? \"after\" : \"before\"));\n    MIR_output_insn (ctx, debug_file, anchor, curr_func_item->u.func, FALSE);\n    fprintf (debug_file, \": \");\n    MIR_output_insn (ctx, debug_file, new_insn, curr_func_item->u.func, TRUE);\n  });\n}\n\nstatic void add_inout_reloads (gen_ctx_t gen_ctx, MIR_insn_t insn) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  int out_p;\n  MIR_op_mode_t mode;\n  MIR_type_t type;\n  MIR_reg_t temp_reg;\n  MIR_op_t temp_op;\n\n  gen_assert (MIR_insn_nops (ctx, insn) >= 2 && !MIR_call_code_p (insn->code)\n              && insn->code != MIR_RET);\n  MIR_insn_op_mode (ctx, insn, 1, &out_p);\n  gen_assert (!out_p);\n  mode = MIR_insn_op_mode (ctx, insn, 0, &out_p);\n  gen_assert (out_p && mode == MIR_insn_op_mode (ctx, insn, 1, &out_p) && !out_p);\n  type = mode2type (mode);\n  temp_reg = gen_new_temp_reg (gen_ctx, type, curr_func_item->u.func);\n  temp_op = _MIR_new_var_op (ctx, temp_reg);\n  add_reload (gen_ctx, insn, &insn->ops[1], &temp_op, type, FALSE);\n  add_reload (gen_ctx, insn, &insn->ops[0], &temp_op, type, TRUE);\n  insn->ops[0] = insn->ops[1] = temp_op;\n}\n\nstatic void make_io_dup_op_insns (gen_ctx_t gen_ctx) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  MIR_func_t func;\n  MIR_insn_t insn, next_insn;\n  MIR_insn_code_t code;\n  MIR_op_t mem_op, *mem_op_ref;\n  MIR_op_mode_t mode;\n  MIR_type_t type;\n  MIR_reg_t temp_reg, type_regs[MIR_T_BOUND];\n  size_t i;\n  int out_p;\n\n  gen_assert (curr_func_item->item_type == MIR_func_item);\n  func = curr_func_item->u.func;\n  for (i = 0; target_io_dup_op_insn_codes[i] != MIR_INSN_BOUND; i++)\n    bitmap_set_bit_p (insn_to_consider, target_io_dup_op_insn_codes[i]);\n  for (type = 0; type < MIR_T_BOUND; type++) type_regs[type] = MIR_NON_VAR;\n  for (insn = DLIST_HEAD (MIR_insn_t, func->insns); insn != NULL; insn = next_insn) {\n    next_insn = DLIST_NEXT (MIR_insn_t, insn);\n    code = insn->code;\n    if (code == MIR_LABEL || MIR_addr_code_p (code) || code == MIR_USE) continue;\n    if (bitmap_bit_p (insn_to_consider, code) && !MIR_op_eq_p (ctx, insn->ops[0], insn->ops[1]))\n      add_inout_reloads (gen_ctx, insn);\n    if (target_insn_ok_p (gen_ctx, insn)) continue;\n    for (i = 0; i < insn->nops; i++) { /* try to change one non-dup mem op to reg */\n      if (insn->ops[i].mode != MIR_OP_VAR_MEM) continue;\n      if (bitmap_bit_p (insn_to_consider, code) && (i == 0 || i == 1)) continue;\n      mode = MIR_insn_op_mode (ctx, insn, i, &out_p);\n      type = mode2type (mode);\n      /* we don't use hard regs for this type: */\n      if (get_temp_hard_reg (type, TRUE) == MIR_NON_VAR) continue;\n      if (type_regs[type] == MIR_NON_VAR)\n        type_regs[type] = gen_new_temp_reg (gen_ctx, type, curr_func_item->u.func);\n      mem_op = insn->ops[i];\n      insn->ops[i] = _MIR_new_var_op (ctx, type_regs[type]);\n      if (target_insn_ok_p (gen_ctx, insn)) {\n        add_reload (gen_ctx, insn, &mem_op, &insn->ops[i], type, out_p);\n        type_regs[type] = MIR_NON_VAR;\n        break;\n      }\n      insn->ops[i] = mem_op;\n    }\n    if (i < insn->nops) continue;\n    if (bitmap_bit_p (insn_to_consider, code) && insn->ops[0].mode == MIR_OP_VAR_MEM) {\n      add_inout_reloads (gen_ctx, insn);\n      if (target_insn_ok_p (gen_ctx, insn)) continue;\n    }\n    for (i = 0; i < insn->nops; i++) { /* change all non-dup mem ops to pseudo */\n      if (insn->ops[i].mode != MIR_OP_VAR_MEM) continue;\n      if (bitmap_bit_p (insn_to_consider, code) && (i == 0 || i == 1)) continue;\n      mode = MIR_insn_op_mode (ctx, insn, i, &out_p);\n      type = mode2type (mode);\n      /* we don't use hard regs for this type: */\n      if (get_temp_hard_reg (type, TRUE) == MIR_NON_VAR) continue;\n      temp_reg = gen_new_temp_reg (gen_ctx, type, curr_func_item->u.func);\n      mem_op_ref = &insn->ops[i];\n      insn->ops[i] = _MIR_new_var_op (ctx, temp_reg);\n      add_reload (gen_ctx, insn, mem_op_ref, &insn->ops[i], type, out_p);\n    }\n    /* target_insn_ok_p still can return FALSE here for insn which will be converted to builtin */\n  }\n}\n\n/* New Page */\n\n/* Register allocation */\n\ntypedef struct allocno_info {\n  MIR_reg_t reg;\n  int tied_reg_p;\n  reg_info_t *reg_infos;\n} allocno_info_t;\n\nDEF_VARR (allocno_info_t);\n\ntypedef struct spill_cache_el {\n  uint32_t age;\n  MIR_reg_t slot;\n} spill_cache_el_t;\n\nDEF_VARR (spill_cache_el_t);\n\nDEF_VARR (bitmap_t);\n\ntypedef struct lr_gap {\n  int16_t hreg;    /* key, hard reg assigned to reg */\n  int16_t type;    /* type of reg */\n  MIR_reg_t reg;   /* reg of the gap lr */\n  live_range_t lr; /* the gap lr, lr->start is another key */\n} lr_gap_t;\n\nDEF_VARR (lr_gap_t);\nDEF_HTAB (lr_gap_t);\n\ntypedef struct spill_el {\n  MIR_reg_t reg;\n  signed char spill_p, edge_p, bb_end_p /* used only for !edge_p */;\n  union {\n    edge_t e;\n    bb_t bb;\n  } u;\n} spill_el_t;\n\nDEF_VARR (spill_el_t);\n\ntypedef struct insn_reload {\n  MIR_type_t type;\n  MIR_reg_t var, hreg;\n} insn_reload_t;\n\n#define MAX_INSN_RELOADS (2 * 4) /* 2 temp regs * 4 types */\nstruct ra_ctx {\n  MIR_reg_t start_mem_loc;\n  VARR (MIR_reg_t) * reg_renumber;\n  VARR (allocno_info_t) * sorted_regs;\n  VARR (bitmap_t) * used_locs, *busy_used_locs; /* indexed by bb or point */\n  VARR (bitmap_t) * var_bbs;\n  VARR (lr_gap_t) * spill_gaps, *curr_gaps; /* used to find live ranges to spill */\n  bitmap_t lr_gap_bitmaps[MAX_HARD_REG + 1];\n  HTAB (lr_gap_t) * lr_gap_tab;\n  VARR (spill_el_t) * spill_els;\n  VARR (spill_cache_el_t) * spill_cache;\n  uint32_t spill_cache_age;\n  bitmap_t conflict_locs1;\n  reg_info_t *curr_reg_infos;\n  int in_reloads_num, out_reloads_num;\n  insn_reload_t in_reloads[MAX_INSN_RELOADS], out_reloads[MAX_INSN_RELOADS];\n};\n\n#define start_mem_loc gen_ctx->ra_ctx->start_mem_loc\n#define reg_renumber gen_ctx->ra_ctx->reg_renumber\n#define sorted_regs gen_ctx->ra_ctx->sorted_regs\n#define used_locs gen_ctx->ra_ctx->used_locs\n#define busy_used_locs gen_ctx->ra_ctx->busy_used_locs\n#define var_bbs gen_ctx->ra_ctx->var_bbs\n#define spill_gaps gen_ctx->ra_ctx->spill_gaps\n#define curr_gaps gen_ctx->ra_ctx->curr_gaps\n#define lr_gap_bitmaps gen_ctx->ra_ctx->lr_gap_bitmaps\n#define lr_gap_tab gen_ctx->ra_ctx->lr_gap_tab\n#define spill_els gen_ctx->ra_ctx->spill_els\n#define spill_cache gen_ctx->ra_ctx->spill_cache\n#define spill_cache_age gen_ctx->ra_ctx->spill_cache_age\n#define conflict_locs1 gen_ctx->ra_ctx->conflict_locs1\n#define curr_reg_infos gen_ctx->ra_ctx->curr_reg_infos\n#define in_reloads_num gen_ctx->ra_ctx->in_reloads_num\n#define out_reloads_num gen_ctx->ra_ctx->out_reloads_num\n#define in_reloads gen_ctx->ra_ctx->in_reloads\n#define out_reloads gen_ctx->ra_ctx->out_reloads\n\n/* Priority RA */\n\n#define live_in in\n#define live_out out\n#define spill_gen gen   /* pseudo regs fully spilled in BB, for them spill_kill is false */\n#define spill_kill kill /* pseudo regs referenced in the BB and should use assigned hreg */\n\nstatic htab_hash_t lr_gap_hash (lr_gap_t el, void *arg MIR_UNUSED) {\n  return (htab_hash_t) mir_hash_finish (\n    mir_hash_step (mir_hash_step (mir_hash_init (0x88), (uint64_t) el.hreg),\n                   (uint64_t) el.lr->start));\n}\n\nstatic int lr_gap_eq (lr_gap_t el1, lr_gap_t el2, void *arg MIR_UNUSED) {\n  return el1.hreg == el2.hreg && el1.lr->start == el2.lr->start;\n}\n\nstatic void insert_lr_gap (gen_ctx_t gen_ctx, int hreg, MIR_type_t type, MIR_reg_t reg,\n                           live_range_t lr) {\n  MIR_alloc_t alloc = gen_alloc (gen_ctx);\n  lr_gap_t el = {hreg, type, reg, lr}, tab_el;\n  gen_assert (lr->lr_bb != NULL);\n  if (lr_gap_bitmaps[hreg] == NULL)\n    lr_gap_bitmaps[hreg] = bitmap_create2 (alloc, 3 * lr->start / 2);\n  bitmap_set_bit_p (lr_gap_bitmaps[hreg], lr->start);\n  HTAB_DO (lr_gap_t, lr_gap_tab, el, HTAB_INSERT, tab_el);\n}\n\nstatic void delete_lr_gap (gen_ctx_t gen_ctx, int hreg, live_range_t lr) {\n  lr_gap_t el, tab_el;\n  gen_assert (lr->lr_bb != NULL && lr_gap_bitmaps[hreg] != NULL);\n  bitmap_clear_bit_p (lr_gap_bitmaps[hreg], lr->start);\n  el.hreg = hreg;\n  el.lr = lr;\n  HTAB_DO (lr_gap_t, lr_gap_tab, el, HTAB_DELETE, tab_el);\n}\n\nstatic inline int find_lr_gap (gen_ctx_t gen_ctx, int hreg, int point, lr_gap_t *tab_el) {\n  struct live_range lr;\n  lr_gap_t el;\n  if (lr_gap_bitmaps[hreg] == NULL || !bitmap_bit_p (lr_gap_bitmaps[hreg], point)) return FALSE;\n  el.hreg = hreg;\n  lr.start = point;\n  el.lr = &lr;\n  return HTAB_DO (lr_gap_t, lr_gap_tab, el, HTAB_FIND, *tab_el);\n}\n\nstatic void init_lr_gap_tab (gen_ctx_t gen_ctx) {\n  MIR_alloc_t alloc = gen_alloc (gen_ctx);\n  for (int i = 0; i <= MAX_HARD_REG; i++) lr_gap_bitmaps[i] = NULL;\n  HTAB_CREATE (lr_gap_t, lr_gap_tab, alloc, 1024, lr_gap_hash, lr_gap_eq, NULL);\n}\n\nstatic void finish_lr_gap_tab (gen_ctx_t gen_ctx) {\n  for (int i = 0; i <= MAX_HARD_REG; i++)\n    if (lr_gap_bitmaps[i] != NULL) bitmap_destroy (lr_gap_bitmaps[i]);\n  HTAB_DESTROY (lr_gap_t, lr_gap_tab);\n}\n\nstatic int allocno_info_compare_func (const void *a1, const void *a2) {\n  const allocno_info_t *allocno_info1 = (const allocno_info_t *) a1,\n                       *allocno_info2 = (const allocno_info_t *) a2;\n  MIR_reg_t reg1 = allocno_info1->reg, reg2 = allocno_info2->reg;\n  reg_info_t *reg_infos = allocno_info1->reg_infos;\n  long diff;\n\n  gen_assert (reg_infos == allocno_info2->reg_infos);\n  if (allocno_info1->tied_reg_p) {\n    if (allocno_info2->tied_reg_p) return -1;\n  } else if (allocno_info2->tied_reg_p) {\n    return 1;\n  }\n  if ((diff = reg_infos[reg2].freq - reg_infos[reg1].freq) != 0) return diff;\n  if (reg_infos[reg2].live_length < reg_infos[reg1].live_length) return -1;\n  if (reg_infos[reg1].live_length < reg_infos[reg2].live_length) return 1;\n  return reg1 < reg2 ? -1 : 1; /* make sort stable */\n}\n\nstatic int hreg_in_bitmap_p (int hreg, MIR_type_t type, int nregs, bitmap_t bm) {\n  for (int i = 0; i < nregs; i++)\n    if (bitmap_bit_p (bm, target_nth_loc (hreg, type, i))) return TRUE;\n  return FALSE;\n}\n\nstatic MIR_reg_t get_hard_reg (gen_ctx_t gen_ctx, MIR_reg_t type, bitmap_t conflict_locs) {\n  MIR_reg_t hreg, curr_hreg, best_hreg = MIR_NON_VAR;\n  int n, k, nregs, best_saved_p = FALSE;\n  for (n = 0; n <= MAX_HARD_REG; n++) {\n#ifdef TARGET_HARD_REG_ALLOC_ORDER\n    hreg = TARGET_HARD_REG_ALLOC_ORDER (n);\n#else\n    hreg = n;\n#endif\n    if (bitmap_bit_p (conflict_locs, hreg)) continue;\n    if (!target_hard_reg_type_ok_p (hreg, type) || target_fixed_hard_reg_p (hreg)) continue;\n    if ((nregs = target_locs_num (hreg, type)) > 1) {\n      if (target_nth_loc (hreg, type, nregs - 1) > MAX_HARD_REG) break;\n      for (k = nregs - 1; k > 0; k--) {\n        curr_hreg = target_nth_loc (hreg, type, k);\n        if (target_fixed_hard_reg_p (curr_hreg) || bitmap_bit_p (conflict_locs, curr_hreg)) break;\n      }\n      if (k > 0) continue;\n    }\n    if (best_hreg == MIR_NON_VAR\n        || (best_saved_p && bitmap_bit_p (call_used_hard_regs[MIR_T_UNDEF], hreg))) {\n      best_hreg = hreg;\n      best_saved_p = !bitmap_bit_p (call_used_hard_regs[MIR_T_UNDEF], hreg);\n    }\n  }\n  return best_hreg;\n}\n\nstatic int available_hreg_p (int hreg, MIR_reg_t type, int nregs, bitmap_t *conflict_locs,\n                             live_range_t lr) {\n  for (int j = lr->start; j <= lr->finish; j++) {\n    if (bitmap_bit_p (conflict_locs[j], hreg)) return FALSE;\n    if (nregs > 1) {\n      if (target_nth_loc (hreg, type, nregs - 1) > MAX_HARD_REG) return FALSE;\n      for (int k = nregs - 1; k > 0; k--) {\n        MIR_reg_t curr_hreg = target_nth_loc (hreg, type, k);\n        if (bitmap_bit_p (conflict_locs[j], curr_hreg)) return FALSE;\n      }\n    }\n  }\n  return TRUE;\n}\n\n/* Return cost spill of given lr */\nstatic int gap_lr_spill_cost (gen_ctx_t gen_ctx, live_range_t lr) {\n  int cost_factor = LOOP_COST_FACTOR / 2;\n  bitmap_clear (temp_bitmap);\n  for (lr_bb_t lr_bb = lr->lr_bb; lr_bb != NULL; lr_bb = lr_bb->next)\n    bitmap_set_bit_p (temp_bitmap, lr_bb->bb->index);\n  int cost = 0;\n  for (lr_bb_t lr_bb = lr->lr_bb; lr_bb != NULL; lr_bb = lr_bb->next) {\n    bb_t bb = lr_bb->bb;\n    int level, max_level, bb_level = bb_loop_level (bb) + 1;\n    edge_t e;\n    max_level = -1;\n    for (e = DLIST_HEAD (out_edge_t, bb->out_edges); e != NULL; e = DLIST_NEXT (out_edge_t, e))\n      if (!bitmap_bit_p (temp_bitmap, e->dst->index)\n          && (level = bb_loop_level (e->dst) + 1) > max_level)\n        max_level = level;\n    if (max_level >= 0) cost += (max_level < bb_level ? max_level : bb_level) * cost_factor;\n    max_level = -1;\n    for (e = DLIST_HEAD (in_edge_t, bb->in_edges); e != NULL; e = DLIST_NEXT (in_edge_t, e))\n      if (!bitmap_bit_p (temp_bitmap, e->src->index)\n          && (level = bb_loop_level (e->src) + 1) > max_level)\n        max_level = level;\n    if (max_level >= 0) cost += (max_level < bb_level ? max_level : bb_level) * cost_factor;\n  }\n  return cost;\n}\n\nstatic void find_lr_gaps (gen_ctx_t gen_ctx, live_range_t for_lr, MIR_reg_t hreg, MIR_type_t type,\n                          int nregs MIR_UNUSED, int *spill_cost, VARR (lr_gap_t) * lr_gaps) {\n  MIR_reg_t curr_hreg;\n  int i, j, cont, slots_num = target_locs_num (hreg, type);\n  int found_p MIR_UNUSED;\n  lr_gap_t lr_gap, last_lr_gap;\n  for (i = 0; i < slots_num; i++) {\n    curr_hreg = target_nth_loc (hreg, type, i);\n    gen_assert (curr_hreg <= MAX_HARD_REG);\n    if (VARR_LENGTH (lr_gap_t, lr_gaps) == 0) {\n      last_lr_gap.lr = NULL;\n    } else {\n      last_lr_gap = VARR_LAST (lr_gap_t, lr_gaps);\n    }\n    *spill_cost = 0;\n    for (j = for_lr->start; j >= 0; j--)\n      if (find_lr_gap (gen_ctx, curr_hreg, j, &lr_gap)) break;\n    cont = for_lr->start + 1;\n    if (j >= 0 && lr_gap.lr->finish >= for_lr->start) { /* found the leftmost interesecting */\n      cont = lr_gap.lr->finish + 1;\n      if (last_lr_gap.lr != lr_gap.lr) {\n        VARR_PUSH (lr_gap_t, lr_gaps, lr_gap);\n        *spill_cost = gap_lr_spill_cost (gen_ctx, lr_gap.lr);\n        last_lr_gap = lr_gap;\n      }\n    }\n    for (j = cont; j <= for_lr->finish; j++) {\n      if (!find_lr_gap (gen_ctx, curr_hreg, j, &lr_gap)) continue;\n      if (last_lr_gap.lr != lr_gap.lr) {\n        VARR_PUSH (lr_gap_t, lr_gaps, lr_gap);\n        *spill_cost += gap_lr_spill_cost (gen_ctx, lr_gap.lr);\n        last_lr_gap = lr_gap;\n      }\n      j = lr_gap.lr->finish;\n    }\n  }\n}\n\n/* If we find a hard reg then info about spilled lrs will in spill_gaps */\nstatic MIR_reg_t get_hard_reg_with_split (gen_ctx_t gen_ctx, MIR_reg_t reg, MIR_reg_t type,\n                                          live_range_t start_lr) {\n  MIR_reg_t hreg, curr_hreg, best_hreg = MIR_NON_VAR;\n  int n, k, nregs, profit, cost, best_profit = 0, gap_size, best_gap_size = 0;\n  int clobbered_p, best_saved_p = FALSE;\n  live_range_t lr;\n  bitmap_t *all_locs = VARR_ADDR (bitmap_t, used_locs);\n  bitmap_t *busy_locs = VARR_ADDR (bitmap_t, busy_used_locs);\n  VARR (lr_gap_t) * temp_gaps;\n  for (n = 0; n <= MAX_HARD_REG; n++) {\n#ifdef TARGET_HARD_REG_ALLOC_ORDER\n    hreg = TARGET_HARD_REG_ALLOC_ORDER (n);\n#else\n    hreg = n;\n#endif\n    if (!target_hard_reg_type_ok_p (hreg, type) || target_fixed_hard_reg_p (hreg)) continue;\n    if ((nregs = target_locs_num (hreg, type)) > 1) {\n      if (target_nth_loc (hreg, type, nregs - 1) > MAX_HARD_REG) break;\n      for (k = nregs - 1; k > 0; k--) {\n        curr_hreg = target_nth_loc (hreg, type, k);\n        if (target_fixed_hard_reg_p (curr_hreg)) break;\n      }\n      if (k > 0) continue;\n    }\n    VARR_TRUNC (lr_gap_t, curr_gaps, 0);\n    profit = curr_reg_infos[reg].freq;\n    gap_size = 0;\n    for (lr = start_lr; lr != NULL; lr = lr->next) {\n      if (available_hreg_p (hreg, type, nregs, all_locs, lr)) {\n      } else if (available_hreg_p (hreg, type, nregs, busy_locs, lr)) {\n        /* spill other pseudo regs in their gap */\n        find_lr_gaps (gen_ctx, lr, hreg, type, nregs, &cost, curr_gaps);\n        profit -= cost;\n        gap_size += (lr->finish - lr->start + 1);\n      } else if (lr->lr_bb == NULL) { /* not a gap */\n        break;\n      } else { /* spill this pseudo reg gap */\n        lr_gap_t lr_gap = {hreg, type, reg, lr};\n        cost = gap_lr_spill_cost (gen_ctx, lr_gap.lr);\n        profit -= cost;\n        VARR_PUSH (lr_gap_t, curr_gaps, lr_gap);\n      }\n    }\n    if (lr != NULL || profit < 0) continue;\n    clobbered_p = bitmap_bit_p (call_used_hard_regs[MIR_T_UNDEF], hreg);\n    if (best_hreg == MIR_NON_VAR || profit > best_profit\n        || (profit == best_profit && best_saved_p && clobbered_p)\n        || (profit == best_profit && best_saved_p == !clobbered_p && gap_size > best_gap_size)) {\n      best_hreg = hreg;\n      best_profit = profit;\n      best_saved_p = !clobbered_p;\n      best_gap_size = gap_size;\n      SWAP (spill_gaps, curr_gaps, temp_gaps);\n    }\n  }\n  return best_hreg;\n}\n\nstatic MIR_reg_t get_new_stack_slot (gen_ctx_t gen_ctx, MIR_reg_t type, int *slots_num_ref) {\n  MIR_reg_t best_loc;\n  int k, slots_num = 1;\n\n  for (k = 0; k < slots_num; k++) {\n    if (k == 0) {\n      best_loc = (MIR_reg_t) func_stack_slots_num + MAX_HARD_REG + 1;\n      slots_num = target_locs_num (best_loc, type);\n    }\n    func_stack_slots_num++;\n    if (k == 0 && (best_loc - MAX_HARD_REG - 1) % slots_num != 0) k--; /* align */\n  }\n  *slots_num_ref = slots_num;\n  return best_loc;\n}\n\nstatic MIR_reg_t get_stack_loc (gen_ctx_t gen_ctx, MIR_reg_t start_loc, MIR_type_t type,\n                                bitmap_t conflict_locs, int *slots_num_ref) {\n  MIR_reg_t loc, curr_loc, best_loc = MIR_NON_VAR;\n  int k, slots_num = 1;\n  for (loc = start_loc; loc <= func_stack_slots_num + MAX_HARD_REG; loc++) {\n    slots_num = target_locs_num (loc, type);\n    if (target_nth_loc (loc, type, slots_num - 1) > func_stack_slots_num + MAX_HARD_REG) break;\n    for (k = 0; k < slots_num; k++) {\n      curr_loc = target_nth_loc (loc, type, k);\n      if (bitmap_bit_p (conflict_locs, curr_loc)) break;\n    }\n    if (k < slots_num) continue;\n    if ((loc - MAX_HARD_REG - 1) % slots_num != 0)\n      continue; /* we align stack slots according to the type size */\n    if (best_loc == MIR_NON_VAR) best_loc = loc;\n  }\n  if (best_loc == MIR_NON_VAR) best_loc = get_new_stack_slot (gen_ctx, type, &slots_num);\n  *slots_num_ref = slots_num;\n  return best_loc;\n}\n\n#define ONLY_SIMPLIFIED_RA FALSE\n\nstatic void assign (gen_ctx_t gen_ctx) {\n  MIR_alloc_t alloc = gen_alloc (gen_ctx);\n  MIR_context_t ctx = gen_ctx->ctx;\n  MIR_reg_t best_loc, i, reg, var, max_var = get_max_var (gen_ctx);\n  MIR_type_t type;\n  int slots_num;\n  int j, k, simple_loc_update_p, reserve_p, nregs;\n  live_range_t start_lr, lr;\n  bitmap_t bm;\n  size_t length;\n  bitmap_t *used_locs_addr, *busy_used_locs_addr;\n  allocno_info_t allocno_info;\n  MIR_func_t func = curr_func_item->u.func;\n  bitmap_t global_hard_regs = _MIR_get_module_global_var_hard_regs (ctx, curr_func_item->module);\n  const char *msg;\n  const int simplified_p = ONLY_SIMPLIFIED_RA || optimize_level < 2;\n  bitmap_t conflict_locs = conflict_locs1, spill_lr_starts = temp_bitmap2;\n\n  func_stack_slots_num = 0;\n  curr_reg_infos = VARR_ADDR (reg_info_t, curr_cfg->reg_info);\n  VARR_TRUNC (MIR_reg_t, reg_renumber, 0);\n  for (i = 0; i <= max_var; i++) {\n    VARR_PUSH (MIR_reg_t, reg_renumber, MIR_NON_VAR);\n  }\n  /* max_var for func */\n  VARR_TRUNC (allocno_info_t, sorted_regs, 0);\n  allocno_info.reg_infos = curr_reg_infos;\n  start_mem_loc = MAX_HARD_REG + 1;\n  for (reg = MAX_HARD_REG + 1; reg <= max_var; reg++) {\n    allocno_info.reg = reg;\n    allocno_info.tied_reg_p = bitmap_bit_p (tied_regs, reg);\n    if (bitmap_bit_p (addr_regs, reg)) {\n      type = MIR_reg_type (gen_ctx->ctx, reg - MAX_HARD_REG, func);\n      best_loc = get_new_stack_slot (gen_ctx, type, &slots_num);\n      VARR_SET (MIR_reg_t, reg_renumber, reg, best_loc);\n      start_mem_loc = best_loc + slots_num;\n      DEBUG (2, {\n        fprintf (debug_file, \" Assigning to addressable %s:reg=%3u (freq %-3ld) -- %lu\\n\",\n                 MIR_reg_name (gen_ctx->ctx, reg - MAX_HARD_REG, func), reg,\n                 curr_reg_infos[reg].freq, (unsigned long) best_loc);\n      });\n      continue;\n    }\n    VARR_PUSH (allocno_info_t, sorted_regs, allocno_info);\n    for (length = 0, lr = VARR_GET (live_range_t, var_live_ranges, reg); lr != NULL; lr = lr->next)\n      length += lr->finish - lr->start + 1;\n    curr_reg_infos[reg].live_length = length;\n  }\n  for (int n = 0; n <= curr_point && n < (int) VARR_LENGTH (bitmap_t, used_locs); n++)\n    if (global_hard_regs == NULL) {\n      bitmap_clear (VARR_GET (bitmap_t, used_locs, n));\n      if (!simplified_p) bitmap_clear (VARR_GET (bitmap_t, busy_used_locs, n));\n    } else {\n      bitmap_copy (VARR_GET (bitmap_t, used_locs, n), global_hard_regs);\n      if (!simplified_p) bitmap_copy (VARR_GET (bitmap_t, busy_used_locs, n), global_hard_regs);\n    }\n  while ((int) VARR_LENGTH (bitmap_t, used_locs) <= curr_point) {\n    bm = bitmap_create2 (alloc, MAX_HARD_REG + 1);\n    if (global_hard_regs != NULL) bitmap_copy (bm, global_hard_regs);\n    VARR_PUSH (bitmap_t, used_locs, bm);\n    if (!simplified_p) {\n      bm = bitmap_create2 (alloc, MAX_HARD_REG + 1);\n      if (global_hard_regs != NULL) bitmap_copy (bm, global_hard_regs);\n      VARR_PUSH (bitmap_t, busy_used_locs, bm);\n    }\n  }\n  nregs = (int) VARR_LENGTH (allocno_info_t, sorted_regs);\n  qsort (VARR_ADDR (allocno_info_t, sorted_regs), nregs, sizeof (allocno_info_t),\n         allocno_info_compare_func);\n  used_locs_addr = VARR_ADDR (bitmap_t, used_locs);\n  busy_used_locs_addr = VARR_ADDR (bitmap_t, busy_used_locs);\n  /* Mark ranges used by hard regs for pseudo reg splitting: */\n  for (i = 0; i <= MAX_HARD_REG; i++) {\n    for (lr = VARR_GET (live_range_t, var_live_ranges, i); lr != NULL; lr = lr->next)\n      for (j = lr->start; j <= lr->finish; j++) {\n        bitmap_set_bit_p (used_locs_addr[j], i);\n        if (!simplified_p) bitmap_set_bit_p (busy_used_locs_addr[j], i);\n      }\n  }\n  bitmap_clear (func_used_hard_regs);\n  if (!simplified_p) HTAB_CLEAR (lr_gap_t, lr_gap_tab);\n  for (int n = 0; n < nregs; n++) { /* hard reg and stack slot assignment */\n    reg = VARR_GET (allocno_info_t, sorted_regs, n).reg;\n    if (VARR_GET (MIR_reg_t, reg_renumber, reg) != MIR_NON_VAR) continue;\n    type = MIR_reg_type (gen_ctx->ctx, reg - MAX_HARD_REG, func);\n    if (VARR_GET (allocno_info_t, sorted_regs, n).tied_reg_p) {\n      const char *hard_reg_name = MIR_reg_hard_reg_name (ctx, reg - MAX_HARD_REG, func);\n      int hard_reg = _MIR_get_hard_reg (ctx, hard_reg_name);\n      gen_assert (hard_reg >= 0 && hard_reg <= MAX_HARD_REG\n                  && target_locs_num (hard_reg, type) == 1);\n      VARR_SET (MIR_reg_t, reg_renumber, reg, hard_reg);\n#ifndef NDEBUG\n      for (lr = VARR_GET (live_range_t, var_live_ranges, reg); lr != NULL; lr = lr->next)\n        for (j = lr->start; j <= lr->finish; j++)\n          gen_assert (bitmap_bit_p (used_locs_addr[j], hard_reg));\n#endif\n      if (hard_reg_name == NULL) setup_used_hard_regs (gen_ctx, type, hard_reg);\n      DEBUG (2, {\n        fprintf (debug_file, \" Assigning to global %s:reg=%3u (freq %-3ld) -- %lu\\n\",\n                 MIR_reg_name (gen_ctx->ctx, reg - MAX_HARD_REG, func), reg,\n                 curr_reg_infos[reg].freq, (unsigned long) hard_reg);\n      });\n      continue;\n    }\n    var = reg;\n    if ((start_lr = VARR_GET (live_range_t, var_live_ranges, var)) == NULL) continue;\n    bitmap_clear (conflict_locs);\n    for (lr = start_lr; lr != NULL; lr = lr->next) {\n      for (j = lr->start; j <= lr->finish; j++)\n        bitmap_ior (conflict_locs, conflict_locs, used_locs_addr[j]);\n    }\n    msg = \"\";\n    VARR_TRUNC (lr_gap_t, spill_gaps, 0);\n    if ((best_loc = get_hard_reg (gen_ctx, type, conflict_locs)) != MIR_NON_VAR) {\n      setup_used_hard_regs (gen_ctx, type, best_loc);\n    } else if (!simplified_p\n               && ((best_loc = get_hard_reg_with_split (gen_ctx, reg, type, start_lr))\n                   != MIR_NON_VAR)) {\n      /* try to use gaps in already allocated pseudos or given pseudo: */\n      setup_used_hard_regs (gen_ctx, type, best_loc);\n      msg = \"(with splitting live ranges)\";\n    } else {\n      best_loc = get_stack_loc (gen_ctx, start_mem_loc, type, conflict_locs, &slots_num);\n    }\n    DEBUG (2, {\n      fprintf (debug_file, \" Assigning %s to %s:reg=%3u (freq %-3ld) -- %lu\\n\", msg,\n               MIR_reg_name (gen_ctx->ctx, reg - MAX_HARD_REG, func), reg, curr_reg_infos[reg].freq,\n               (unsigned long) best_loc);\n      fprintf (debug_file, \"  live range: \");\n      print_live_ranges (gen_ctx, start_lr);\n    });\n    bitmap_clear (spill_lr_starts);\n    while (VARR_LENGTH (lr_gap_t, spill_gaps) != 0) {\n      lr_gap_t lr_gap = VARR_POP (lr_gap_t, spill_gaps);\n      DEBUG (4, {\n        fprintf (debug_file, \"   Splitting lr gap: r%d%s, h%d [%d..%d]\\n\", lr_gap.reg,\n                 lr_gap.reg == reg ? \"*\" : \"\", lr_gap.hreg, lr_gap.lr->start, lr_gap.lr->finish);\n      });\n      for (lr_bb_t lr_bb = lr_gap.lr->lr_bb; lr_bb != NULL; lr_bb = lr_bb->next)\n        bitmap_set_bit_p (lr_bb->bb->spill_gen, lr_gap.reg);\n      if (lr_gap.reg == reg) {\n        bitmap_set_bit_p (spill_lr_starts, lr_gap.lr->start);\n        continue; /* spilled gap of the current reg */\n      }\n      slots_num = target_locs_num (lr_gap.hreg, lr_gap.type);\n      for (k = 0; k < slots_num; k++) {\n        MIR_reg_t curr_hr = target_nth_loc (lr_gap.hreg, lr_gap.type, k);\n        delete_lr_gap (gen_ctx, curr_hr, lr_gap.lr);\n        for (j = lr_gap.lr->start; j <= lr_gap.lr->finish; j++)\n          bitmap_clear_bit_p (used_locs_addr[j], curr_hr);\n      }\n    }\n    VARR_SET (MIR_reg_t, reg_renumber, reg, best_loc);\n    slots_num = target_locs_num (best_loc, type);\n    simple_loc_update_p = simplified_p || best_loc > MAX_HARD_REG;\n    for (lr = VARR_GET (live_range_t, var_live_ranges, var); lr != NULL; lr = lr->next) {\n      if ((reserve_p = simple_loc_update_p || !bitmap_bit_p (spill_lr_starts, lr->start))) {\n        for (j = lr->start; j <= lr->finish; j++)\n          for (k = 0; k < slots_num; k++)\n            bitmap_set_bit_p (used_locs_addr[j], target_nth_loc (best_loc, type, k));\n      }\n      if (simple_loc_update_p) continue;\n      if (lr->lr_bb == NULL) {\n        for (j = lr->start; j <= lr->finish; j++)\n          for (k = 0; k < slots_num; k++)\n            bitmap_set_bit_p (busy_used_locs_addr[j], target_nth_loc (best_loc, type, k));\n      } else if (reserve_p) {\n        for (k = 0; k < slots_num; k++) {\n          MIR_reg_t hr = target_nth_loc (best_loc, type, k);\n          DEBUG (4, {\n            fprintf (debug_file, \"    Adding lr gap: r%d, h%d [%d..%d]\\n\", reg, hr, lr->start,\n                     lr->finish);\n          });\n          insert_lr_gap (gen_ctx, hr, type, reg, lr);\n        }\n      }\n    }\n  }\n}\n\n/* Add store (st_p) or load of hard_reg with data mode to/from memory\n   loc using temp_hard_reg for addressing an put it before after anchor.  */\nstatic MIR_reg_t add_ld_st (gen_ctx_t gen_ctx, MIR_op_t *mem_op, MIR_reg_t loc, MIR_reg_t base_reg,\n                            MIR_op_mode_t data_mode, MIR_reg_t hard_reg, int st_p,\n                            MIR_reg_t temp_hard_reg, MIR_insn_t anchor, int after_p) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  MIR_disp_t offset;\n  MIR_insn_code_t code;\n  MIR_insn_t new_insn, new_insns[3];\n  MIR_type_t type;\n  bb_insn_t bb_insn, new_bb_insn;\n  MIR_op_t hard_reg_op;\n  size_t n;\n\n  gen_assert (temp_hard_reg != MIR_NON_VAR);\n  type = mode2type (data_mode);\n  code = (type == MIR_T_I64 ? MIR_MOV\n          : type == MIR_T_F ? MIR_FMOV\n          : type == MIR_T_D ? MIR_DMOV\n                            : MIR_LDMOV);\n  if (hard_reg != MIR_NON_VAR) setup_used_hard_regs (gen_ctx, type, hard_reg);\n  offset = target_get_stack_slot_offset (gen_ctx, type, loc - MAX_HARD_REG - 1);\n  n = 0;\n  if (target_valid_mem_offset_p (gen_ctx, type, offset)) {\n    *mem_op = _MIR_new_var_mem_op (ctx, type, offset, base_reg, MIR_NON_VAR, 0);\n  } else {\n    new_insns[0] = MIR_new_insn (ctx, MIR_MOV, _MIR_new_var_op (ctx, temp_hard_reg),\n                                 MIR_new_int_op (ctx, offset));\n    new_insns[1]\n      = MIR_new_insn (ctx, MIR_ADD, _MIR_new_var_op (ctx, temp_hard_reg),\n                      _MIR_new_var_op (ctx, temp_hard_reg), _MIR_new_var_op (ctx, base_reg));\n    n = 2;\n    *mem_op = _MIR_new_var_mem_op (ctx, type, 0, temp_hard_reg, MIR_NON_VAR, 0);\n  }\n  if (hard_reg == MIR_NON_VAR) return hard_reg; /* LD vars can be always kept in memory */\n  hard_reg_op = _MIR_new_var_op (ctx, hard_reg);\n  if (!st_p) {\n    new_insns[n++] = MIR_new_insn (ctx, code, hard_reg_op, *mem_op);\n  } else {\n    new_insns[n++] = MIR_new_insn (ctx, code, *mem_op, hard_reg_op);\n  }\n  DEBUG (2, {\n    bb_t bb = get_insn_bb (gen_ctx, anchor);\n    fprintf (debug_file, \"    Adding %s insn \", after_p ? \"after\" : \"before\");\n    fprintf (debug_file, \" (in BB %lu\", (unsigned long) bb->index);\n    if (optimize_level == 0 || bb->loop_node == NULL)\n      fprintf (debug_file, \") \");\n    else\n      fprintf (debug_file, \", level %d) \", bb_loop_level (bb));\n    MIR_output_insn (ctx, debug_file, anchor, curr_func_item->u.func, FALSE);\n    fprintf (debug_file, \":\\n\");\n    for (size_t i = 0; i < n; i++) {\n      fprintf (debug_file, \"      \");\n      MIR_output_insn (ctx, debug_file, new_insns[i], curr_func_item->u.func, TRUE);\n    }\n  });\n  if (after_p) {\n    for (size_t i = 0, j = n - 1; i < j; i++, j--) { /* reverse for subsequent correct insertion: */\n      new_insn = new_insns[i];\n      new_insns[i] = new_insns[j];\n      new_insns[j] = new_insn;\n    }\n  }\n  for (size_t i = 0; i < n; i++) {\n    new_insn = new_insns[i];\n    if (after_p)\n      MIR_insert_insn_after (ctx, curr_func_item, anchor, new_insn);\n    else\n      MIR_insert_insn_before (ctx, curr_func_item, anchor, new_insn);\n    if (optimize_level == 0) {\n      new_insn->data = get_insn_data_bb (anchor);\n    } else {\n      bb_insn = anchor->data;\n      new_bb_insn = create_bb_insn (gen_ctx, new_insn, bb_insn->bb);\n      if (after_p)\n        DLIST_INSERT_AFTER (bb_insn_t, bb_insn->bb->bb_insns, bb_insn, new_bb_insn);\n      else\n        DLIST_INSERT_BEFORE (bb_insn_t, bb_insn->bb->bb_insns, bb_insn, new_bb_insn);\n    }\n  }\n  return hard_reg;\n}\n\nstatic MIR_reg_t get_reload_hreg (gen_ctx_t gen_ctx, MIR_reg_t var, MIR_reg_t type, int out_p) {\n  MIR_reg_t hr;\n  insn_reload_t *reloads = out_p ? out_reloads : in_reloads;\n  int rld_num = 0, *reloads_num = out_p ? &out_reloads_num : &in_reloads_num;\n\n  for (int i = 0; i < *reloads_num; i++) {\n    if (var != MIR_NON_VAR && reloads[i].var == var) return reloads[i].hreg;\n    if (rld_num == 0 && reloads[i].hreg == get_temp_hard_reg (type, TRUE))\n      rld_num = 1;\n    else if (reloads[i].hreg == get_temp_hard_reg (type, FALSE))\n      rld_num = 2;\n  }\n  gen_assert (rld_num <= 1);\n  hr = get_temp_hard_reg (type, rld_num == 0);\n  rld_num = (*reloads_num)++;\n  gen_assert (rld_num < MAX_INSN_RELOADS);\n  reloads[rld_num].var = var;\n  reloads[rld_num].type = type;\n  reloads[rld_num].hreg = hr;\n  return hr;\n}\n\n/* Return hard reg to use in insn instead of pseudo (reg) with given data_mode.\n   If reg got a stack slot (will be in *mem_op after the call), add load or store insn\n   from this slot depending on out_p using base_reg and possibly a temp hard reg\n   depending on out_p.  */\nstatic MIR_reg_t change_reg (gen_ctx_t gen_ctx, MIR_op_t *mem_op, MIR_reg_t reg, MIR_reg_t base_reg,\n                             MIR_op_mode_t data_mode, MIR_insn_t insn, int out_p) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  MIR_reg_t loc = VARR_GET (MIR_reg_t, reg_renumber, reg);\n  if (loc <= MAX_HARD_REG) return loc;\n  MIR_type_t type = MIR_reg_type (ctx, reg - MAX_HARD_REG, curr_func_item->u.func);\n  MIR_reg_t reload_hreg = get_reload_hreg (gen_ctx, reg, type, out_p);\n  MIR_reg_t temp_addr_hreg\n    = (out_p || type != MIR_T_I64 ? get_reload_hreg (gen_ctx, MIR_NON_VAR, MIR_T_I64, out_p)\n                                  : reload_hreg);\n  gen_assert (!MIR_addr_code_p (insn->code));\n  return add_ld_st (gen_ctx, mem_op, loc, base_reg, data_mode, reload_hreg, out_p, temp_addr_hreg,\n                    insn, out_p);\n}\n\nstatic void update_live (MIR_reg_t var, int out_p, bitmap_t live) {\n  if (out_p) {\n    bitmap_clear_bit_p (live, var);\n  } else {\n    bitmap_set_bit_p (live, var);\n  }\n}\n\nstatic int get_spill_mem_loc (gen_ctx_t gen_ctx, MIR_reg_t reg) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  int slots_num;\n  MIR_type_t type;\n  MIR_reg_t var, slot;\n  live_range_t lr;\n  bitmap_t conflict_locs = conflict_locs1;\n  bitmap_t *used_locs_addr = VARR_ADDR (bitmap_t, used_locs);\n  spill_cache_el_t *spill_cache_addr = VARR_ADDR (spill_cache_el_t, spill_cache);\n  gen_assert (reg != MIR_NON_VAR && reg < VARR_LENGTH (spill_cache_el_t, spill_cache));\n  if (spill_cache_addr[reg].age == spill_cache_age) return spill_cache_addr[reg].slot;\n  bitmap_clear (conflict_locs);\n  var = reg;\n  for (lr = VARR_GET (live_range_t, var_live_ranges, var); lr != NULL; lr = lr->next)\n    for (int j = lr->start; j <= lr->finish; j++)\n      bitmap_ior (conflict_locs, conflict_locs, used_locs_addr[j]);\n  type = MIR_reg_type (ctx, reg - MAX_HARD_REG, curr_func_item->u.func);\n  spill_cache_addr[reg].slot = slot\n    = get_stack_loc (gen_ctx, start_mem_loc, type, conflict_locs, &slots_num);\n  spill_cache_addr[reg].age = spill_cache_age;\n  for (lr = VARR_GET (live_range_t, var_live_ranges, var); lr != NULL; lr = lr->next)\n    for (int j = lr->start; j <= lr->finish; j++)\n      for (int k = 0; k < slots_num; k++)\n        bitmap_set_bit_p (used_locs_addr[j], target_nth_loc (slot, type, k));\n  return slot;\n}\n\n/* Add spill or restore (restore_p) of pseudo (reg) assigned to hard reg and\n   put the insns after anchor.  Use base_reg to address the stack lot.  */\nstatic void spill_restore_reg (gen_ctx_t gen_ctx, MIR_reg_t reg, MIR_reg_t base_reg,\n                               MIR_insn_t anchor, int after_p, int restore_p) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  MIR_reg_t mem_loc;\n  MIR_op_t mem_op;\n  MIR_type_t type = MIR_reg_type (ctx, reg - MAX_HARD_REG, curr_func_item->u.func);\n  MIR_op_mode_t data_mode = type2mode (type);\n  MIR_reg_t hard_reg = VARR_GET (MIR_reg_t, reg_renumber, reg);\n  gen_assert (hard_reg <= MAX_HARD_REG);\n  mem_loc = get_spill_mem_loc (gen_ctx, reg);\n  DEBUG (2, { fprintf (debug_file, \"    %s r%d:\", restore_p ? \"Restore\" : \"Spill\", reg); });\n  add_ld_st (gen_ctx, &mem_op, mem_loc, base_reg, data_mode, hard_reg, !restore_p,\n             TEMP_INT_HARD_REG1, anchor, after_p);\n}\n\nstatic void reload_addr (gen_ctx_t gen_ctx, MIR_insn_t insn, int in_mem_op_num, int out_mem_op_num,\n                         MIR_reg_t base_reg) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  MIR_op_t mem_op, temp_op1, temp_op2, *ops = insn->ops;\n  gen_assert (in_mem_op_num >= 0 || out_mem_op_num >= 0);\n  int op_num = out_mem_op_num >= 0 ? out_mem_op_num : in_mem_op_num;\n  MIR_reg_t base = ops[op_num].u.var_mem.base, index = ops[op_num].u.var_mem.index;\n  MIR_insn_t new_insn;\n  gen_assert (in_mem_op_num < 0 || out_mem_op_num < 0\n              || MIR_op_eq_p (ctx, ops[in_mem_op_num], ops[out_mem_op_num]));\n  add_ld_st (gen_ctx, &mem_op, VARR_GET (MIR_reg_t, reg_renumber, base), base_reg, MIR_OP_INT,\n             TEMP_INT_HARD_REG1, FALSE, TEMP_INT_HARD_REG1, insn, FALSE);\n  add_ld_st (gen_ctx, &mem_op, VARR_GET (MIR_reg_t, reg_renumber, index), base_reg, MIR_OP_INT,\n             TEMP_INT_HARD_REG2, FALSE, TEMP_INT_HARD_REG2, insn, FALSE);\n  temp_op1 = _MIR_new_var_op (ctx, TEMP_INT_HARD_REG1);\n  temp_op2 = _MIR_new_var_op (ctx, TEMP_INT_HARD_REG2);\n  if (ops[op_num].u.var_mem.scale != 1) {\n    new_insn = MIR_new_insn (ctx, MIR_LSH, temp_op2, temp_op2,\n                             MIR_new_int_op (ctx, gen_int_log2 (ops[op_num].u.var_mem.scale)));\n    gen_add_insn_before (gen_ctx, insn, new_insn);\n    /* continuation of debug output in add_ld_st: */\n    DEBUG (2, {\n      fprintf (debug_file, \"      \");\n      MIR_output_insn (ctx, debug_file, new_insn, curr_func_item->u.func, TRUE);\n    });\n  }\n  new_insn = MIR_new_insn (ctx, MIR_ADD, temp_op1, temp_op1, temp_op2);\n  gen_add_insn_before (gen_ctx, insn, new_insn);\n  DEBUG (2, {\n    fprintf (debug_file, \"      \");\n    MIR_output_insn (ctx, debug_file, new_insn, curr_func_item->u.func, TRUE);\n  });\n  if (out_mem_op_num >= 0) {\n    ops[out_mem_op_num].u.var_mem.base = TEMP_INT_HARD_REG1;\n    ops[out_mem_op_num].u.var_mem.index = MIR_NON_VAR;\n  }\n  if (in_mem_op_num >= 0) {\n    ops[in_mem_op_num].u.var_mem.base = TEMP_INT_HARD_REG1;\n    ops[in_mem_op_num].u.var_mem.index = MIR_NON_VAR;\n  }\n}\n\nstruct rewrite_data {\n  bb_t bb;\n  bitmap_t live, regs_to_save;\n};\n\n#define MAX_INSN_RELOAD_MEM_OPS 2\nstatic int try_spilled_reg_mem (gen_ctx_t gen_ctx, MIR_insn_t insn, int nop, MIR_reg_t loc,\n                                MIR_reg_t base_reg) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  MIR_op_t *op = &insn->ops[nop];\n  MIR_type_t type = MIR_reg_type (ctx, op->u.var - MAX_HARD_REG, curr_func_item->u.func);\n  MIR_disp_t offset = target_get_stack_slot_offset (gen_ctx, type, loc - MAX_HARD_REG - 1);\n  if (!target_valid_mem_offset_p (gen_ctx, type, offset)) return FALSE;\n  MIR_reg_t reg = op->u.var;\n  MIR_op_t saved_op = *op;\n  MIR_op_t mem_op = _MIR_new_var_mem_op (ctx, type, offset, base_reg, MIR_NON_VAR, 0);\n  int n = 0, op_nums[MAX_INSN_RELOAD_MEM_OPS];\n  for (int i = nop; i < (int) insn->nops; i++)\n    if (insn->ops[i].mode == MIR_OP_VAR && insn->ops[i].u.var == reg) {\n      insn->ops[i] = mem_op;\n      gen_assert (n < MAX_INSN_RELOAD_MEM_OPS);\n      op_nums[n++] = i;\n    }\n  if (target_insn_ok_p (gen_ctx, insn)) return TRUE;\n  for (int i = 0; i < n; i++) insn->ops[op_nums[i]] = saved_op;\n  return FALSE;\n}\n\nstatic void transform_addr (gen_ctx_t gen_ctx, MIR_insn_t insn, MIR_reg_t base_reg) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  MIR_reg_t reg, temp_hard_reg, loc;\n  MIR_type_t type;\n  MIR_disp_t offset;\n  MIR_insn_t new_insn1, new_insn2;\n  gen_assert (MIR_addr_code_p (insn->code));\n  gen_assert (insn->ops[1].mode == MIR_OP_VAR);\n  reg = insn->ops[1].u.reg;\n  gen_assert (reg > MAX_HARD_REG && reg != MIR_NON_VAR);\n  loc = VARR_GET (MIR_reg_t, reg_renumber, reg);\n  type = MIR_reg_type (ctx, reg - MAX_HARD_REG, curr_func_item->u.func);\n  gen_assert (loc > MAX_HARD_REG);\n  offset = target_get_stack_slot_offset (gen_ctx, type, loc - MAX_HARD_REG - 1);\n  temp_hard_reg = get_reload_hreg (gen_ctx, MIR_NON_VAR, MIR_T_I64, FALSE);\n  new_insn1 = MIR_new_insn (ctx, MIR_MOV, _MIR_new_var_op (ctx, temp_hard_reg),\n                            MIR_new_int_op (ctx, offset + _MIR_addr_offset (ctx, insn->code)));\n  new_insn2 = MIR_new_insn (ctx, MIR_ADD, _MIR_new_var_op (ctx, temp_hard_reg),\n                            _MIR_new_var_op (ctx, temp_hard_reg), _MIR_new_var_op (ctx, base_reg));\n  DEBUG (2, {\n    bb_t bb = get_insn_bb (gen_ctx, insn);\n    fprintf (debug_file, \"    Adding before insn (in BB %lu) \", (unsigned long) bb->index);\n    MIR_output_insn (ctx, debug_file, insn, curr_func_item->u.func, FALSE);\n    fprintf (debug_file, \":\\n      \");\n    MIR_output_insn (ctx, debug_file, new_insn1, curr_func_item->u.func, TRUE);\n    fprintf (debug_file, \"      \");\n    MIR_output_insn (ctx, debug_file, new_insn2, curr_func_item->u.func, TRUE);\n  });\n  gen_add_insn_before (gen_ctx, insn, new_insn1);\n  gen_add_insn_before (gen_ctx, insn, new_insn2);\n  DEBUG (2, {\n    fprintf (debug_file, \"Changing \");\n    MIR_output_insn (ctx, debug_file, insn, curr_func_item->u.func, FALSE);\n  });\n  insn->code = MIR_MOV;\n  insn->ops[1] = _MIR_new_var_op (ctx, temp_hard_reg);\n  DEBUG (2, {\n    fprintf (debug_file, \" to \");\n    MIR_output_insn (ctx, debug_file, insn, curr_func_item->u.func, TRUE);\n  });\n}\n\nstatic int rewrite_insn (gen_ctx_t gen_ctx, MIR_insn_t insn, MIR_reg_t base_reg,\n                         struct rewrite_data *data) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  MIR_type_t type;\n  size_t nops, i;\n  MIR_op_t *op, mem_op;\n#if !MIR_NO_GEN_DEBUG\n  MIR_op_t in_op = MIR_new_int_op (ctx, 0),\n           out_op = MIR_new_int_op (ctx, 0); /* To remove unitilized warning */\n#endif\n  MIR_op_mode_t data_mode;\n  MIR_reg_t hard_reg, reg;\n  int out_p, call_p, addr_reload_p, rld_in_num;\n  int out_mem_op_num, in_mem_op_num, other_mem_op_num;\n  nops = MIR_insn_nops (ctx, insn);\n  out_mem_op_num = in_mem_op_num = -1;\n  rld_in_num = 0;\n  /* Update live info, save/restore regs living across calls, and check do we need addr reload:\n   */\n  for (int niter = 0; niter <= 1; niter++) {\n    for (i = 0; i < nops; i++) {\n      op = &insn->ops[i];\n      data_mode = MIR_insn_op_mode (ctx, insn, i, &out_p);\n      if (niter == 0 && (!out_p || op->mode == MIR_OP_VAR_MEM)) continue;\n      if (niter == 1 && (out_p && op->mode != MIR_OP_VAR_MEM)) continue;\n      switch (op->mode) {\n      case MIR_OP_VAR:\n        if (op->u.var <= MAX_HARD_REG) {\n          bitmap_set_bit_p (func_used_hard_regs, op->u.var);\n          if (data != NULL) update_live (op->u.var, out_p, data->live);\n        } else {\n          if (!out_p && VARR_GET (MIR_reg_t, reg_renumber, op->u.var) > MAX_HARD_REG\n              && mode2type (data_mode) == MIR_T_I64)\n            rld_in_num++;\n          if (data != NULL) {\n            update_live (op->u.var, out_p, data->live);\n            if (bitmap_clear_bit_p (data->regs_to_save, op->u.var)) /* put (slot<-hr) after insn */\n              spill_restore_reg (gen_ctx, op->u.var, base_reg, insn, TRUE, FALSE);\n          }\n        }\n        break;\n      case MIR_OP_VAR_MEM:\n        if (op->u.var_mem.base <= MAX_HARD_REG)\n          bitmap_set_bit_p (func_used_hard_regs, op->u.var_mem.base);\n        if (op->u.var_mem.index <= MAX_HARD_REG)\n          bitmap_set_bit_p (func_used_hard_regs, op->u.var_mem.index);\n        if (op->u.var_mem.base != MIR_NON_VAR && op->u.var_mem.index != MIR_NON_VAR\n            && op->u.var_mem.base > MAX_HARD_REG && op->u.var_mem.index > MAX_HARD_REG\n            && VARR_GET (MIR_reg_t, reg_renumber, op->u.var_mem.base) > MAX_HARD_REG\n            && VARR_GET (MIR_reg_t, reg_renumber, op->u.var_mem.index) > MAX_HARD_REG) {\n          other_mem_op_num = -1;\n          if (out_p) {\n            gen_assert (out_mem_op_num < 0);\n            out_mem_op_num = (int) i;\n            if (in_mem_op_num >= 0) other_mem_op_num = in_mem_op_num;\n          } else {\n            gen_assert (in_mem_op_num < 0);\n            in_mem_op_num = (int) i;\n            if (out_mem_op_num >= 0) other_mem_op_num = out_mem_op_num;\n          }\n          if (other_mem_op_num < 0\n              || op->u.var_mem.base != insn->ops[other_mem_op_num].u.var_mem.base\n              || op->u.var_mem.index != insn->ops[other_mem_op_num].u.var_mem.index)\n            rld_in_num += 2;\n        }\n        if (data != NULL) {\n          if (op->u.var_mem.base != MIR_NON_VAR) {\n            update_live (op->u.var_mem.base, FALSE, data->live);\n            if (op->u.var_mem.base > MAX_HARD_REG) {\n              if (bitmap_clear_bit_p (data->regs_to_save,\n                                      op->u.var_mem.base)) /* put slot<-hr after */\n                spill_restore_reg (gen_ctx, op->u.var_mem.base, base_reg, insn, TRUE, FALSE);\n            }\n          }\n          if (op->u.var_mem.index != MIR_NON_VAR) {\n            update_live (op->u.var_mem.index, FALSE, data->live);\n            if (op->u.var_mem.index > MAX_HARD_REG) {\n              if (bitmap_clear_bit_p (data->regs_to_save,\n                                      op->u.var_mem.index)) /* put slot<-hr after */\n                spill_restore_reg (gen_ctx, op->u.var_mem.index, base_reg, insn, TRUE, FALSE);\n            }\n          }\n        }\n        break;\n      default: /* do nothing */ break;\n      }\n    }\n    if (data != NULL && niter == 0) { /* right after processing outputs */\n      MIR_reg_t early_clobbered_hard_reg1, early_clobbered_hard_reg2;\n      target_get_early_clobbered_hard_regs (insn, &early_clobbered_hard_reg1,\n                                            &early_clobbered_hard_reg2);\n      if (early_clobbered_hard_reg1 != MIR_NON_VAR)\n        update_live (early_clobbered_hard_reg1, TRUE, data->live);\n      if (early_clobbered_hard_reg2 != MIR_NON_VAR)\n        update_live (early_clobbered_hard_reg2, TRUE, data->live);\n      if (MIR_call_code_p (insn->code)) {\n        size_t nel;\n        bb_insn_t bb_insn = insn->data;\n        bitmap_iterator_t bi;\n        FOREACH_BITMAP_BIT (bi, call_used_hard_regs[MIR_T_UNDEF], nel) {\n          update_live ((MIR_reg_t) nel, TRUE, data->live);\n        }\n        FOREACH_BITMAP_BIT (bi, bb_insn->call_hard_reg_args, nel) {\n          update_live ((MIR_reg_t) nel, FALSE, data->live);\n        }\n        FOREACH_BITMAP_BIT (bi, data->live, nel) {\n          if (nel <= MAX_HARD_REG) continue;\n          reg = (MIR_reg_t) nel;\n          if (bitmap_bit_p (data->bb->spill_gen, reg)) continue; /* will be spilled in split */\n          MIR_reg_t loc = VARR_GET (MIR_reg_t, reg_renumber, reg);\n          if (loc > MAX_HARD_REG) continue;\n          type = MIR_reg_type (gen_ctx->ctx, reg - MAX_HARD_REG, curr_func_item->u.func);\n          int nregs = target_locs_num (loc, type);\n          if (hreg_in_bitmap_p (loc, type, nregs, call_used_hard_regs[MIR_T_UNDEF])\n              && bitmap_set_bit_p (data->regs_to_save, reg)) /* put (hr<-slot) after call */\n            spill_restore_reg (gen_ctx, reg, base_reg, insn, TRUE, TRUE);\n        }\n      }\n    }\n  }\n  addr_reload_p = rld_in_num > 2;\n  out_reloads_num = in_reloads_num = 0;\n  if (addr_reload_p) { /* not enough 2 temp int hard regs: address reload: */\n    reload_addr (gen_ctx, insn, in_mem_op_num, out_mem_op_num, base_reg);\n    get_reload_hreg (gen_ctx, MIR_NON_VAR, MIR_T_I64,\n                     FALSE); /* reserve the 1st int temp hard reg */\n  }\n  if (MIR_addr_code_p (insn->code)) transform_addr (gen_ctx, insn, base_reg);\n  call_p = MIR_call_code_p (insn->code);\n  for (i = 0; i < nops; i++) {\n    op = &insn->ops[i];\n    data_mode = MIR_insn_op_mode (ctx, insn, i, &out_p);\n    DEBUG (2, {\n      if (out_p)\n        out_op = *op; /* we don't care about multiple call outputs here */\n      else\n        in_op = *op;\n    });\n    switch (op->mode) {\n    case MIR_OP_VAR:\n      if (op->u.var <= MAX_HARD_REG) break;\n      if (data_mode == MIR_OP_VAR) {\n        gen_assert (insn->code == MIR_USE || (MIR_addr_code_p (insn->code) && i == 1));\n        type = MIR_reg_type (ctx, op->u.var - MAX_HARD_REG, curr_func_item->u.func);\n        data_mode = type == MIR_T_F    ? MIR_OP_FLOAT\n                    : type == MIR_T_D  ? MIR_OP_DOUBLE\n                    : type == MIR_T_LD ? MIR_OP_LDOUBLE\n                                       : MIR_OP_INT;\n      }\n      MIR_reg_t loc = VARR_GET (MIR_reg_t, reg_renumber, op->u.var);\n      if (!MIR_addr_code_p (insn->code) && i == 0 && loc > MAX_HARD_REG\n          && try_spilled_reg_mem (gen_ctx, insn, (int) i, loc, base_reg))\n        break;\n      hard_reg = change_reg (gen_ctx, &mem_op, op->u.var, base_reg, data_mode, insn, out_p);\n      if (hard_reg == MIR_NON_VAR) { /* we don't use hard regs for this type reg (e.g. ldouble) */\n        *op = mem_op;\n      } else {\n        op->u.var = hard_reg;\n      }\n      break;\n    case MIR_OP_VAR_MEM:\n      if (call_p && MIR_blk_type_p (op->u.var_mem.type)) break;\n      if (op->u.var_mem.base > MAX_HARD_REG && op->u.var_mem.base != MIR_NON_VAR) {\n        op->u.var_mem.base\n          = change_reg (gen_ctx, &mem_op, op->u.var_mem.base, base_reg, MIR_OP_INT, insn, FALSE);\n        gen_assert (op->u.var_mem.base != MIR_NON_VAR); /* we can always use GP regs */\n      }\n      if (op->u.var_mem.index > MAX_HARD_REG && op->u.var_mem.index != MIR_NON_VAR) {\n        op->u.var_mem.index\n          = change_reg (gen_ctx, &mem_op, op->u.var_mem.index, base_reg, MIR_OP_INT, insn, FALSE);\n        gen_assert (op->u.var_mem.index != MIR_NON_VAR); /* we can always use GP regs */\n      }\n      break;\n    default: /* do nothing */ break;\n    }\n  }\n  if (move_code_p (insn->code)) {\n    if (MIR_op_eq_p (ctx, insn->ops[0], insn->ops[1])) {\n      DEBUG (2, {\n        fprintf (debug_file, \"Deleting noop move \");\n        MIR_output_insn (ctx, debug_file, insn, curr_func_item->u.func, FALSE);\n        fprintf (debug_file, \" which was \");\n        insn->ops[0] = out_op;\n        insn->ops[1] = in_op;\n        MIR_output_insn (ctx, debug_file, insn, curr_func_item->u.func, TRUE);\n      });\n      bb_insn_t bb_insn;\n      if (optimize_level > 0 && (bb_insn = insn->data) != NULL\n          && DLIST_HEAD (bb_insn_t, bb_insn->bb->bb_insns) == bb_insn\n          && DLIST_TAIL (bb_insn_t, bb_insn->bb->bb_insns) == bb_insn) { /* avoid empty bb */\n        MIR_insn_t nop = MIR_new_insn_arr (gen_ctx->ctx, MIR_USE, 0, NULL);\n        MIR_insert_insn_before (gen_ctx->ctx, curr_func_item, bb_insn->insn, nop);\n        add_new_bb_insn (gen_ctx, nop, bb_insn->bb, FALSE);\n      }\n      gen_delete_insn (gen_ctx, insn);\n      return 1;\n    }\n  }\n  return 0;\n}\n\nstatic void rewrite (gen_ctx_t gen_ctx) {\n  MIR_insn_t insn, next_insn, head_insn;\n  MIR_reg_t base_reg = target_get_stack_slot_base_reg (gen_ctx);\n  size_t insns_num = 0, movs_num = 0, deleted_movs_num = 0;\n  bitmap_t global_hard_regs\n    = _MIR_get_module_global_var_hard_regs (gen_ctx->ctx, curr_func_item->module);\n  const int simplified_p = ONLY_SIMPLIFIED_RA || optimize_level < 2;\n\n  if (simplified_p) {\n    for (insn = DLIST_HEAD (MIR_insn_t, curr_func_item->u.func->insns); insn != NULL;\n         insn = next_insn) {\n      next_insn = DLIST_NEXT (MIR_insn_t, insn);\n      if (move_code_p (insn->code)) movs_num++;\n      deleted_movs_num += rewrite_insn (gen_ctx, insn, base_reg, NULL);\n      insns_num++;\n    }\n  } else {\n    MIR_reg_t reg;\n    bb_insn_t prev_bb_insn;\n    bitmap_t live = temp_bitmap, regs_to_save = temp_bitmap2;\n    bitmap_iterator_t bi;\n    size_t nel;\n    spill_cache_el_t spill_cache_el = {0, 0};\n    spill_cache_age++;\n    VARR_TRUNC (spill_cache_el_t, spill_cache, 0);\n    while (VARR_LENGTH (spill_cache_el_t, spill_cache) <= curr_cfg->max_var)\n      VARR_PUSH (spill_cache_el_t, spill_cache, spill_cache_el);\n    for (bb_t bb = DLIST_HEAD (bb_t, curr_cfg->bbs); bb != NULL; bb = DLIST_NEXT (bb_t, bb)) {\n      struct rewrite_data data = {bb, live, regs_to_save};\n      bitmap_copy (live, bb->live_out);\n      bitmap_clear (regs_to_save);\n      for (bb_insn_t bb_insn = DLIST_TAIL (bb_insn_t, bb->bb_insns); bb_insn != NULL;\n           bb_insn = prev_bb_insn) {\n        prev_bb_insn = DLIST_PREV (bb_insn_t, bb_insn);\n        insn = bb_insn->insn;\n        if (move_code_p (insn->code)) movs_num++;\n        deleted_movs_num += rewrite_insn (gen_ctx, insn, base_reg, &data);\n        insns_num++;\n      }\n      gen_assert (bitmap_equal_p (live, bb->live_in));\n      FOREACH_BITMAP_BIT (bi, regs_to_save, nel) {\n        gen_assert (nel > MAX_HARD_REG);\n        reg = (MIR_reg_t) nel;\n        gen_assert (bitmap_bit_p (bb->spill_kill, reg));\n        head_insn = DLIST_HEAD (bb_insn_t, bb->bb_insns)->insn;\n        spill_restore_reg (gen_ctx, reg, base_reg, head_insn, head_insn->code == MIR_LABEL, FALSE);\n      }\n    }\n  }\n  DEBUG (1, {\n    fprintf (debug_file,\n             \"%5lu deleted RA noop moves out of %lu all moves (%.1f%%), out of %lu all insns \"\n             \"(%.1f%%)\\n\",\n             (unsigned long) deleted_movs_num, (unsigned long) movs_num,\n             deleted_movs_num * 100.0 / movs_num, (unsigned long) insns_num,\n             deleted_movs_num * 100.0 / insns_num);\n  });\n  if (global_hard_regs != NULL) /* we should not save/restore hard regs used by globals */\n    bitmap_and_compl (func_used_hard_regs, func_used_hard_regs, global_hard_regs);\n}\n\n#if !MIR_NO_GEN_DEBUG\nstatic void output_bb_spill_info (gen_ctx_t gen_ctx, bb_t bb) {\n  output_bitmap (gen_ctx, \"  live_in:\", bb->live_in, TRUE, NULL);\n  output_bitmap (gen_ctx, \"  live_out:\", bb->live_out, TRUE, NULL);\n  output_bitmap (gen_ctx, \"  spill_gen:\", bb->spill_gen, TRUE, NULL);\n  output_bitmap (gen_ctx, \"  spill_kill:\", bb->spill_kill, TRUE, NULL);\n}\n#endif\n\nstatic void collect_spill_els (gen_ctx_t gen_ctx) {\n  bb_t bb;\n  edge_t e;\n  size_t nel;\n  bitmap_iterator_t bi;\n  spill_el_t spill_el;\n\n  VARR_TRUNC (spill_el_t, spill_els, 0); /* collect spill elements */\n  for (bb = DLIST_EL (bb_t, curr_cfg->bbs, 2); bb != NULL; bb = DLIST_NEXT (bb_t, bb)) {\n    /* we need non-empty BBs for splitting. we can not remove empty BB as a reg can be splitted\n       around the BB and we need to generate spills/restores in this BB. */\n    gen_assert (DLIST_HEAD (bb_insn_t, bb->bb_insns) != NULL);\n    /* skip entry/exit bbs and split bbs */\n    DEBUG (2, {\n      fprintf (debug_file, \" Process BB%lu(level %d) for splitting\\n\", (unsigned long) bb->index,\n               bb_loop_level (bb));\n    });\n    /* Process out edges for spills: */\n    spill_el.edge_p = spill_el.bb_end_p = TRUE;\n    for (e = DLIST_TAIL (out_edge_t, bb->out_edges); e != NULL; e = DLIST_PREV (out_edge_t, e)) {\n      bitmap_and_compl (temp_bitmap, e->dst->spill_gen, bb->spill_gen);\n      if (bitmap_empty_p (temp_bitmap)) continue;\n      FOREACH_BITMAP_BIT (bi, temp_bitmap, nel) {\n        gen_assert (nel > MAX_HARD_REG);\n        spill_el.reg = (MIR_reg_t) nel;\n        spill_el.spill_p = TRUE;\n        spill_el.u.e = e;\n        VARR_PUSH (spill_el_t, spill_els, spill_el);\n      }\n    }\n    /* Process input edges for restores: */\n    for (e = DLIST_TAIL (in_edge_t, bb->in_edges); e != NULL; e = DLIST_PREV (in_edge_t, e)) {\n      bitmap_clear (temp_bitmap);\n      FOREACH_BITMAP_BIT (bi, e->src->spill_gen, nel) {\n        if (bitmap_bit_p (bb->spill_gen, nel) || !bitmap_bit_p (bb->live_in, nel)) continue;\n        bitmap_set_bit_p (temp_bitmap, nel);\n      }\n      if (bitmap_empty_p (temp_bitmap)) continue;\n      FOREACH_BITMAP_BIT (bi, temp_bitmap, nel) {\n        gen_assert (nel > MAX_HARD_REG);\n        spill_el.reg = (MIR_reg_t) nel;\n        spill_el.spill_p = FALSE;\n        spill_el.u.e = e;\n        VARR_PUSH (spill_el_t, spill_els, spill_el);\n      }\n    }\n  }\n}\n\n#undef live_in\n#undef live_out\n#undef spill_gen\n#undef spill_kill\n\nstatic int spill_el_cmp (const spill_el_t *e1, const spill_el_t *e2) {\n  if (e1->edge_p != e2->edge_p) return e1->edge_p - e2->edge_p; /* put bb first */\n  if (e1->edge_p && e1->u.e != e2->u.e) return e1->u.e < e2->u.e ? -1 : 1;\n  if (!e1->edge_p && e1->u.bb != e2->u.bb) return e1->u.bb->index < e2->u.bb->index ? -1 : 1;\n  if (!e1->edge_p && e1->bb_end_p != e2->bb_end_p)\n    return e1->bb_end_p - e2->bb_end_p; /* start first */\n  if (e1->spill_p != e2->spill_p)       /* load first for bb start, store first otherwise: */\n    return !e1->edge_p && !e1->bb_end_p ? e1->spill_p - e2->spill_p : e2->spill_p - e1->spill_p;\n  return e1->reg == e2->reg ? 0 : e1->reg < e2->reg ? -1 : 1; /* smaller reg first */\n}\n\nstatic int spill_el_sort_cmp (const void *e1, const void *e2) { return spill_el_cmp (e1, e2); }\n\nstatic void make_uniq_spill_els (gen_ctx_t gen_ctx) {\n  size_t i, last, len = VARR_LENGTH (spill_el_t, spill_els);\n  spill_el_t *els = VARR_ADDR (spill_el_t, spill_els);\n  if (len == 0) return;\n  for (last = 0, i = 1; i < len; i++) {\n    if (spill_el_cmp (&els[last], &els[i]) == 0) continue;\n    els[++last] = els[i];\n  }\n  VARR_TRUNC (spill_el_t, spill_els, last + 1);\n}\n\n#define at_start gen\n#define at_end kill\n#define at_src_p flag1\n#define at_dst_p flag2\nstatic void transform_edge_to_bb_placement (gen_ctx_t gen_ctx) {\n  size_t i, j;\n  MIR_insn_t insn;\n  bb_t bb;\n  edge_t e, e2, first_e;\n  bitmap_t edge_regs = temp_bitmap;\n  spill_el_t *spill_els_addr = VARR_ADDR (spill_el_t, spill_els);\n\n  if (VARR_LENGTH (spill_el_t, spill_els) == 0) return;\n  /* Initialize: */\n  for (bb = DLIST_EL (bb_t, curr_cfg->bbs, 2); bb != NULL; bb = DLIST_NEXT (bb_t, bb)) {\n    bitmap_clear (bb->at_end);\n    bitmap_clear (bb->at_start);\n    for (e = DLIST_HEAD (out_edge_t, bb->out_edges); e != NULL; e = DLIST_NEXT (out_edge_t, e))\n      e->at_src_p = e->at_dst_p = FALSE;\n  }\n  /* Setup common at_{start,end} and initial at_{src,dst}_p: */\n  for (i = 0; i < VARR_LENGTH (spill_el_t, spill_els); i++) {\n    gen_assert (spill_els_addr[i].edge_p);\n    e = spill_els_addr[i].u.e;\n    insn = DLIST_TAIL (bb_insn_t, e->src->bb_insns)->insn;\n    /* remember restores sorted after spills: */\n    e->at_src_p\n      = spill_els_addr[i].spill_p || !MIR_any_branch_code_p (insn->code) || insn->code == MIR_JMP;\n    e->at_dst_p = TRUE;\n    bitmap_set_bit_p (e->src->at_end, spill_els_addr[i].reg);\n    bitmap_set_bit_p (e->dst->at_start, spill_els_addr[i].reg);\n  }\n  /* Check edge spills/restores and with common one at src end and dst start and refine\n     at_{src,dst}_p: */\n  for (i = 0; i < VARR_LENGTH (spill_el_t, spill_els); i = j) {\n    e = spill_els_addr[i].u.e;\n    bitmap_clear (edge_regs);\n    for (j = i; j < VARR_LENGTH (spill_el_t, spill_els) && e == spill_els_addr[j].u.e; j++)\n      bitmap_set_bit_p (edge_regs, spill_els_addr[j].reg);\n    if (e->at_src_p) {\n      first_e = e2 = DLIST_HEAD (out_edge_t, e->src->out_edges);\n      while (e2 != NULL && e2->at_src_p) e2 = DLIST_NEXT (out_edge_t, e2);\n      if (e2 != NULL || !bitmap_equal_p (edge_regs, e->src->at_end))\n        for (e2 = first_e; e2 != NULL; e2 = DLIST_NEXT (out_edge_t, e2)) e2->at_src_p = FALSE;\n    }\n    if (e->at_dst_p) {\n      first_e = e2 = DLIST_HEAD (in_edge_t, e->dst->in_edges);\n      while (e2 != NULL && e2->at_dst_p) e2 = DLIST_NEXT (in_edge_t, e2);\n      if (e2 != NULL || !bitmap_equal_p (edge_regs, e->dst->at_start))\n        for (e2 = first_e; e2 != NULL; e2 = DLIST_NEXT (in_edge_t, e2)) e2->at_dst_p = FALSE;\n    }\n  }\n  for (size_t n = 0; n < VARR_LENGTH (spill_el_t, spill_els); n++) {\n    e = spill_els_addr[n].u.e;\n    if (!e->at_src_p || !e->at_dst_p) continue;\n    if (DLIST_HEAD (out_edge_t, e->src->out_edges) == DLIST_TAIL (out_edge_t, e->src->out_edges))\n      e->at_src_p = FALSE;\n    else if (DLIST_HEAD (in_edge_t, e->dst->in_edges) == DLIST_TAIL (in_edge_t, e->dst->in_edges))\n      e->at_dst_p = FALSE;\n  }\n  uint32_t start_split_bb_index = curr_bb_index;\n  /* Changing to BB placement with splitting edges if necessary */\n  for (size_t n = 0; n < VARR_LENGTH (spill_el_t, spill_els); n++) {\n    gen_assert (spill_els_addr[n].edge_p);\n    e = spill_els_addr[n].u.e;\n    spill_els_addr[n].edge_p = FALSE;\n    spill_els_addr[n].bb_end_p = FALSE;\n    if (e->at_src_p) {\n      spill_els_addr[n].u.bb = e->src;\n      spill_els_addr[n].bb_end_p = TRUE;\n    } else if (e->at_dst_p) {\n      spill_els_addr[n].u.bb = e->dst;\n    } else if (e->src->index >= start_split_bb_index) {  // ??? split_bb\n      gen_assert (DLIST_LENGTH (out_edge_t, e->src->out_edges) == 1\n                  && DLIST_LENGTH (in_edge_t, e->src->in_edges) == 1);\n      spill_els_addr[n].u.bb = e->src;\n    } else if (e->dst->index >= start_split_bb_index) {  // ?? split_bb\n      gen_assert (DLIST_LENGTH (out_edge_t, e->dst->out_edges) == 1\n                  && DLIST_LENGTH (in_edge_t, e->dst->in_edges) == 1);\n      spill_els_addr[n].u.bb = e->dst;\n    } else {\n      /* put at split bb start, as we reuse existing edge to connect split bb, we will put\n         next spill at the same split bb -- see processing order above */\n      // ??? check reuse existing edge (start,end) in split_edge_if_necessary\n      bb = split_edge_if_necessary (gen_ctx, e);\n      spill_els_addr[n].u.bb = bb;\n    }\n  }\n}\n\n#undef at_start\n#undef at_end\n#undef at_src_p\n#undef at_dst_p\n\nstatic void split (gen_ctx_t gen_ctx) { /* split by putting spill/restore insns */\n  int restore_p, after_p;\n  bb_t bb;\n  bb_insn_t bb_insn = NULL;\n  MIR_reg_t reg, base_hreg = target_get_stack_slot_base_reg (gen_ctx);\n  spill_el_t *spill_els_addr;\n\n  collect_spill_els (gen_ctx);\n  spill_els_addr = VARR_ADDR (spill_el_t, spill_els);\n  qsort (spill_els_addr, VARR_LENGTH (spill_el_t, spill_els), sizeof (spill_el_t),\n         spill_el_sort_cmp);\n  DEBUG (2, {\n    fprintf (debug_file, \" Spills on edges:\\n\");\n    for (size_t i = 0; i < VARR_LENGTH (spill_el_t, spill_els); i++) {\n      gen_assert (spill_els_addr[i].edge_p);\n      fprintf (debug_file, \"  %s r%d on %s of edge bb%lu->bb%lu\\n\",\n               spill_els_addr[i].spill_p ? \"spill\" : \"restore\", spill_els_addr[i].reg,\n               spill_els_addr[i].bb_end_p ? \"end\" : \"start\",\n               (unsigned long) spill_els_addr[i].u.e->src->index,\n               (unsigned long) spill_els_addr[i].u.e->dst->index);\n    }\n  });\n  transform_edge_to_bb_placement (gen_ctx);\n  qsort (spill_els_addr, VARR_LENGTH (spill_el_t, spill_els), sizeof (spill_el_t),\n         spill_el_sort_cmp);\n  make_uniq_spill_els (gen_ctx);\n  spill_els_addr = VARR_ADDR (spill_el_t, spill_els);\n  DEBUG (2, {\n    fprintf (debug_file, \"+++++++++++++MIR after splitting edges:\\n\");\n    print_CFG (gen_ctx, TRUE, FALSE, TRUE, FALSE, NULL);\n    fprintf (debug_file, \"  Spills on BBs:\\n\");\n    for (size_t i = 0; i < VARR_LENGTH (spill_el_t, spill_els); i++) {\n      gen_assert (!spill_els_addr[i].edge_p);\n      fprintf (debug_file, \"    %s r%d on %s of bb%lu\\n\",\n               spill_els_addr[i].spill_p ? \"spill\" : \"restore\", spill_els_addr[i].reg,\n               spill_els_addr[i].bb_end_p ? \"end\" : \"start\",\n               (unsigned long) spill_els_addr[i].u.bb->index);\n    }\n  });\n  /* place spills and restores: */\n  for (size_t i = 0; i < VARR_LENGTH (spill_el_t, spill_els); i++) { /* ??? debug info */\n    bb = spill_els_addr[i].u.bb;\n    reg = spill_els_addr[i].reg;\n    gen_assert (reg > MAX_HARD_REG);\n    restore_p = !spill_els_addr[i].spill_p;\n    after_p = FALSE;\n    if (spill_els_addr[i].bb_end_p) {\n      bb_insn = DLIST_TAIL (bb_insn_t, bb->bb_insns);\n      if (!MIR_any_branch_code_p (bb_insn->insn->code)) after_p = TRUE;\n    } else {\n      bb_insn = DLIST_HEAD (bb_insn_t, bb->bb_insns);\n      if (bb_insn->insn->code == MIR_LABEL) after_p = TRUE;\n    }\n    spill_restore_reg (gen_ctx, reg, base_hreg, bb_insn->insn, after_p, restore_p);\n  }\n}\n\nstatic void reg_alloc (gen_ctx_t gen_ctx) {\n  MIR_reg_t reg, max_var = get_max_var (gen_ctx);\n  const int simplified_p = ONLY_SIMPLIFIED_RA || optimize_level < 2;\n\n  build_live_ranges (gen_ctx);\n  assign (gen_ctx);\n  DEBUG (2, {\n    fprintf (debug_file, \"+++++++++++++Disposition after assignment:\");\n    for (reg = MAX_HARD_REG + 1; reg <= max_var; reg++) {\n      if ((reg - MAX_HARD_REG + 1) % 8 == 0) fprintf (debug_file, \"\\n\");\n      fprintf (debug_file, \" %3u=>\", reg);\n      if (VARR_LENGTH (live_range_t, var_live_ranges) <= reg\n          || VARR_GET (live_range_t, var_live_ranges, reg) == NULL)\n        fprintf (debug_file, \"UA\");\n      else\n        fprintf (debug_file, \"%-2u\", VARR_GET (MIR_reg_t, reg_renumber, reg));\n    }\n    fprintf (debug_file, \"\\n\");\n  });\n  rewrite (gen_ctx); /* After rewrite the BB live info is invalid as it is used for spill info */\n  if (!simplified_p) {\n    DEBUG (2, {\n      fprintf (debug_file, \"+++++++++++++Spill info:\\n\");\n      print_CFG (gen_ctx, TRUE, FALSE, FALSE, FALSE, output_bb_spill_info);\n    });\n    split (gen_ctx);\n  }\n  free_func_live_ranges (gen_ctx);\n}\n\nstatic void init_ra (gen_ctx_t gen_ctx) {\n  MIR_alloc_t alloc = gen_alloc (gen_ctx);\n  gen_ctx->ra_ctx = gen_malloc (gen_ctx, sizeof (struct ra_ctx));\n  VARR_CREATE (MIR_reg_t, reg_renumber, alloc, 0);\n  VARR_CREATE (allocno_info_t, sorted_regs, alloc, 0);\n  VARR_CREATE (bitmap_t, used_locs, alloc, 0);\n  VARR_CREATE (bitmap_t, busy_used_locs, alloc, 0);\n  VARR_CREATE (bitmap_t, var_bbs, alloc, 0);\n  VARR_CREATE (lr_gap_t, spill_gaps, alloc, 0);\n  VARR_CREATE (lr_gap_t, curr_gaps, alloc, 0);\n  VARR_CREATE (spill_el_t, spill_els, alloc, 0);\n  init_lr_gap_tab (gen_ctx);\n  VARR_CREATE (spill_cache_el_t, spill_cache, alloc, 0);\n  spill_cache_age = 0;\n  conflict_locs1 = bitmap_create2 (alloc, 3 * MAX_HARD_REG / 2);\n}\n\nstatic void finish_ra (gen_ctx_t gen_ctx) {\n  VARR_DESTROY (MIR_reg_t, reg_renumber);\n  VARR_DESTROY (allocno_info_t, sorted_regs);\n  while (VARR_LENGTH (bitmap_t, used_locs) != 0) bitmap_destroy (VARR_POP (bitmap_t, used_locs));\n  VARR_DESTROY (bitmap_t, used_locs);\n  while (VARR_LENGTH (bitmap_t, busy_used_locs) != 0)\n    bitmap_destroy (VARR_POP (bitmap_t, busy_used_locs));\n  VARR_DESTROY (bitmap_t, busy_used_locs);\n  while (VARR_LENGTH (bitmap_t, var_bbs) != 0) bitmap_destroy (VARR_POP (bitmap_t, var_bbs));\n  VARR_DESTROY (bitmap_t, var_bbs);\n  VARR_DESTROY (lr_gap_t, spill_gaps);\n  VARR_DESTROY (lr_gap_t, curr_gaps);\n  VARR_DESTROY (spill_el_t, spill_els);\n  finish_lr_gap_tab (gen_ctx);\n  VARR_DESTROY (spill_cache_el_t, spill_cache);\n  bitmap_destroy (conflict_locs1);\n  gen_free (gen_ctx, gen_ctx->ra_ctx);\n  gen_ctx->ra_ctx = NULL;\n}\n\n/* New Page */\n\n/* Insn combining after RA requires dead notes and is done in forward insn processing.\n   It is done for the following cases:\n     o splitting insns: lr restore (r = mem) ; bcmp r => bcmp mem\n     o meeting 2-op constraints after RA (when p2 and p0 got hr0):\n       p1=mem; add p2,p0,p1(dead p1) => hr1=mem; add hr0,hr0,hr1 => add hr0,mem */\n\nstruct var_ref { /* We keep track of the last reg ref in this struct. */\n  MIR_insn_t insn;\n  size_t insn_num;\n  size_t nop;\n  char def_p, del_p; /* def/use and deleted */\n};\n\ntypedef struct var_ref var_ref_t;\n\nDEF_VARR (var_ref_t);\n\nstruct combine_ctx {\n  VARR (size_t) * var_ref_ages;\n  VARR (var_ref_t) * var_refs;\n  var_ref_t *var_refs_addr;\n  size_t *var_ref_ages_addr;\n  size_t curr_bb_var_ref_age;\n  size_t last_mem_ref_insn_num;\n  VARR (MIR_reg_t) * insn_vars; /* registers considered for substitution */\n  VARR (size_t) * changed_op_numbers;\n  VARR (MIR_op_t) * last_right_ops;\n  bitmap_t vars_bitmap;\n};\n\n#define var_ref_ages gen_ctx->combine_ctx->var_ref_ages\n#define var_refs gen_ctx->combine_ctx->var_refs\n#define var_refs_addr gen_ctx->combine_ctx->var_refs_addr\n#define var_ref_ages_addr gen_ctx->combine_ctx->var_ref_ages_addr\n#define curr_bb_var_ref_age gen_ctx->combine_ctx->curr_bb_var_ref_age\n#define last_mem_ref_insn_num gen_ctx->combine_ctx->last_mem_ref_insn_num\n#define insn_vars gen_ctx->combine_ctx->insn_vars\n#define changed_op_numbers gen_ctx->combine_ctx->changed_op_numbers\n#define last_right_ops gen_ctx->combine_ctx->last_right_ops\n#define vars_bitmap gen_ctx->combine_ctx->vars_bitmap\n\nstatic MIR_insn_code_t commutative_insn_code (MIR_insn_code_t insn_code) {\n  switch (insn_code) {\n    /* we can not change fp comparison and branches because NaNs */\n  case MIR_ADD:\n  case MIR_ADDS:\n  case MIR_FADD:\n  case MIR_DADD:\n  case MIR_LDADD:\n  case MIR_MUL:\n  case MIR_MULS:\n  case MIR_MULO:\n  case MIR_MULOS:\n  case MIR_UMULO:\n  case MIR_UMULOS:\n  case MIR_FMUL:\n  case MIR_DMUL:\n  case MIR_LDMUL:\n  case MIR_AND:\n  case MIR_OR:\n  case MIR_XOR:\n  case MIR_ANDS:\n  case MIR_ORS:\n  case MIR_XORS:\n  case MIR_EQ:\n  case MIR_EQS:\n  case MIR_FEQ:\n  case MIR_DEQ:\n  case MIR_LDEQ:\n  case MIR_NE:\n  case MIR_NES:\n  case MIR_FNE:\n  case MIR_DNE:\n  case MIR_LDNE:\n  case MIR_BEQ:\n  case MIR_BEQS:\n  case MIR_FBEQ:\n  case MIR_DBEQ:\n  case MIR_LDBEQ:\n  case MIR_BNE:\n  case MIR_BNES:\n  case MIR_FBNE:\n  case MIR_DBNE:\n  case MIR_LDBNE: return insn_code; break;\n  case MIR_LT: return MIR_GT;\n  case MIR_LTS: return MIR_GTS;\n  case MIR_ULT: return MIR_UGT;\n  case MIR_ULTS: return MIR_UGTS;\n  case MIR_LE: return MIR_GE;\n  case MIR_LES: return MIR_GES;\n  case MIR_ULE: return MIR_UGE;\n  case MIR_ULES: return MIR_UGES;\n  case MIR_GT: return MIR_LT;\n  case MIR_GTS: return MIR_LTS;\n  case MIR_UGT: return MIR_ULT;\n  case MIR_UGTS: return MIR_ULTS;\n  case MIR_GE: return MIR_LE;\n  case MIR_GES: return MIR_LES;\n  case MIR_UGE: return MIR_ULE;\n  case MIR_UGES: return MIR_ULES;\n  case MIR_BLT: return MIR_BGT;\n  case MIR_BLTS: return MIR_BGTS;\n  case MIR_UBLT: return MIR_UBGT;\n  case MIR_UBLTS: return MIR_UBGTS;\n  case MIR_BLE: return MIR_BGE;\n  case MIR_BLES: return MIR_BGES;\n  case MIR_UBLE: return MIR_UBGE;\n  case MIR_UBLES: return MIR_UBGES;\n  case MIR_BGT: return MIR_BLT;\n  case MIR_BGTS: return MIR_BLTS;\n  case MIR_UBGT: return MIR_UBLT;\n  case MIR_UBGTS: return MIR_UBLTS;\n  case MIR_BGE: return MIR_BLE;\n  case MIR_BGES: return MIR_BLES;\n  case MIR_UBGE: return MIR_UBLE;\n  case MIR_UBGES: return MIR_UBLES;\n  default: return MIR_INSN_BOUND;\n  }\n}\n\nstatic int obsolete_var_p (gen_ctx_t gen_ctx, MIR_reg_t var, size_t def_insn_num) {\n  return (var < VARR_LENGTH (size_t, var_ref_ages) && var_ref_ages_addr[var] == curr_bb_var_ref_age\n          && var_refs_addr[var].insn_num > def_insn_num);\n}\n\nstatic int obsolete_var_op_p (gen_ctx_t gen_ctx, MIR_op_t op, size_t def_insn_num) {\n  return op.mode == MIR_OP_VAR && obsolete_var_p (gen_ctx, op.u.var, def_insn_num);\n}\n\nstatic int obsolete_op_p (gen_ctx_t gen_ctx, MIR_op_t op, size_t def_insn_num) {\n  if (obsolete_var_op_p (gen_ctx, op, def_insn_num)) return TRUE;\n  if (op.mode != MIR_OP_VAR_MEM) return FALSE;\n  if (op.u.var_mem.base != MIR_NON_VAR && obsolete_var_p (gen_ctx, op.u.var_mem.base, def_insn_num))\n    return TRUE;\n  if (op.u.var_mem.index != MIR_NON_VAR\n      && obsolete_var_p (gen_ctx, op.u.var_mem.index, def_insn_num))\n    return TRUE;\n  return last_mem_ref_insn_num > def_insn_num;\n}\n\nstatic int safe_var_substitution_p (gen_ctx_t gen_ctx, MIR_reg_t var, bb_insn_t bb_insn) {\n  return (var != MIR_NON_VAR && var < VARR_LENGTH (size_t, var_ref_ages)\n          && var_ref_ages_addr[var] == curr_bb_var_ref_age\n          && var_refs_addr[var].def_p\n          /* It is not safe to substitute if there is another use after def insn before\n             the current insn as we delete def insn after the substitution. */\n          && find_bb_insn_dead_var (bb_insn, var) != NULL);\n}\n\nstatic void combine_process_var (gen_ctx_t gen_ctx, MIR_reg_t var, bb_insn_t bb_insn) {\n  if (!safe_var_substitution_p (gen_ctx, var, bb_insn) || !bitmap_set_bit_p (vars_bitmap, var))\n    return;\n  VARR_PUSH (MIR_reg_t, insn_vars, var);\n}\n\nstatic void combine_process_op (gen_ctx_t gen_ctx, const MIR_op_t *op_ref, bb_insn_t bb_insn) {\n  if (op_ref->mode == MIR_OP_VAR) {\n    combine_process_var (gen_ctx, op_ref->u.var, bb_insn);\n  } else if (op_ref->mode == MIR_OP_VAR_MEM) {\n    if (op_ref->u.var_mem.base != MIR_NON_VAR)\n      combine_process_var (gen_ctx, op_ref->u.var_mem.base, bb_insn);\n    if (op_ref->u.var_mem.index != MIR_NON_VAR)\n      combine_process_var (gen_ctx, op_ref->u.var_mem.index, bb_insn);\n  }\n}\n\nstatic int hard_reg_used_in_bb_insn_p (gen_ctx_t gen_ctx, bb_insn_t bb_insn, MIR_reg_t var) {\n  int op_num;\n  MIR_reg_t v;\n  insn_var_iterator_t iter;\n\n  FOREACH_IN_INSN_VAR (gen_ctx, iter, bb_insn->insn, v, op_num) {\n    if (v == var) return TRUE;\n  }\n  return FALSE;\n}\n\nstatic int combine_delete_insn (gen_ctx_t gen_ctx, MIR_insn_t def_insn, bb_insn_t bb_insn) {\n  MIR_reg_t var;\n\n  gen_assert (def_insn->ops[0].mode == MIR_OP_VAR);\n  var = def_insn->ops[0].u.var;\n  if (var_ref_ages_addr[var] != curr_bb_var_ref_age || var_refs_addr[var].del_p) return FALSE;\n  DEBUG (2, {\n    fprintf (debug_file, \"      deleting now dead insn \");\n    print_bb_insn (gen_ctx, def_insn->data, TRUE);\n  });\n  remove_bb_insn_dead_var (gen_ctx, bb_insn, var);\n  move_bb_insn_dead_vars (gen_ctx, bb_insn, def_insn->data, hard_reg_used_in_bb_insn_p);\n  /* We should delete the def insn here because of possible substitution of the def\n     insn 'r0 = ... r0 ...'.  We still need valid entry for def here to find obsolete\n     definiton, e.g. \"r1 = r0; r0 = ...; r0 = ... (deleted); ...= ...r1...\" */\n  gen_delete_insn (gen_ctx, def_insn);\n  var_refs_addr[var].del_p = TRUE; /* to exclude repetitive deletion */\n  return TRUE;\n}\n\nstatic MIR_UNUSED int64_t power2 (int64_t p) {\n  int64_t n = 1;\n\n  if (p < 0) return 0;\n  while (p-- > 0) n *= 2;\n  return n;\n}\n\nstatic MIR_insn_t get_uptodate_def_insn (gen_ctx_t gen_ctx, int var) {\n  MIR_insn_t def_insn;\n\n  if (!var_refs_addr[var].def_p) return NULL;\n  gen_assert (!var_refs_addr[var].del_p);\n  def_insn = var_refs_addr[var].insn;\n  /* Checking r0 = ... r1 ...; ...; r1 = ...; ...; insn */\n  if ((def_insn->nops > 1 && obsolete_op_p (gen_ctx, def_insn->ops[1], var_refs_addr[var].insn_num))\n      || (def_insn->nops > 2\n          && obsolete_op_p (gen_ctx, def_insn->ops[2], var_refs_addr[var].insn_num)))\n    return NULL;\n  return def_insn;\n}\n\nstatic int combine_substitute (gen_ctx_t gen_ctx, bb_insn_t *bb_insn_ref, long *deleted_insns_num) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  bb_insn_t bb_insn = *bb_insn_ref;\n  MIR_insn_t insn = bb_insn->insn, def_insn;\n  size_t i, nops = insn->nops;\n  int out_p, insn_change_p, insn_var_change_p, op_change_p, success_p;\n  MIR_op_t *op_ref, saved_op;\n  MIR_reg_t var, early_clobbered_hard_reg1, early_clobbered_hard_reg2;\n\n  if (nops == 0) return FALSE;\n  VARR_TRUNC (MIR_op_t, last_right_ops, 0);\n  for (i = 0; i < nops; i++) VARR_PUSH (MIR_op_t, last_right_ops, insn->ops[i]);\n  VARR_TRUNC (MIR_reg_t, insn_vars, 0);\n  bitmap_clear (vars_bitmap);\n  for (i = 0; i < nops; i++) {\n    MIR_insn_op_mode (ctx, insn, i, &out_p);\n    if (out_p || insn->ops[i].mode == MIR_OP_VAR_MEM) continue;\n    combine_process_op (gen_ctx, &insn->ops[i], bb_insn);\n  }\n\n  if (move_code_p (insn->code) && insn->ops[1].mode == MIR_OP_VAR\n      && VARR_LENGTH (MIR_reg_t, insn_vars) != 0\n      && VARR_LAST (MIR_reg_t, insn_vars) == insn->ops[1].u.var) {\n    /* We can change move src.  Try to change insn the following way:\n       r0 = r2 op r3; ...; ... = r0  =>  ...; ... = r2 op r3 */\n    var = insn->ops[1].u.var;\n    if ((def_insn = get_uptodate_def_insn (gen_ctx, var)) == NULL\n        || MIR_call_code_p (def_insn->code))\n      return FALSE;\n    target_get_early_clobbered_hard_regs (def_insn, &early_clobbered_hard_reg1,\n                                          &early_clobbered_hard_reg2);\n    if (!move_code_p (def_insn->code) && early_clobbered_hard_reg1 == MIR_NON_VAR\n        && early_clobbered_hard_reg2 == MIR_NON_VAR && insn->ops[1].mode == MIR_OP_VAR\n        && insn->ops[1].u.var == var\n        /* Check that insn->ops[0] is not mem[...hr0...]: */\n        && (insn->ops[0].mode != MIR_OP_VAR_MEM\n            || (insn->ops[0].u.var_mem.base != var && insn->ops[0].u.var_mem.index != var))) {\n      saved_op = def_insn->ops[0];\n      def_insn->ops[0] = insn->ops[0];\n      success_p = target_insn_ok_p (gen_ctx, def_insn);\n      def_insn->ops[0] = saved_op;\n      if (!success_p) return FALSE;\n      gen_move_insn_before (gen_ctx, insn, def_insn);\n      DEBUG (2, {\n        fprintf (debug_file, \"      moving insn \");\n        print_bb_insn (gen_ctx, def_insn->data, FALSE);\n        fprintf (debug_file, \"      before insn \");\n        print_bb_insn (gen_ctx, bb_insn, TRUE);\n      });\n      def_insn->ops[0] = insn->ops[0];\n      DEBUG (2, {\n        fprintf (debug_file, \"      changing it to \");\n        print_bb_insn (gen_ctx, def_insn->data, TRUE);\n        fprintf (debug_file, \"      deleting insn \");\n        print_bb_insn (gen_ctx, bb_insn, TRUE);\n      });\n      gen_delete_insn (gen_ctx, insn);\n      (*deleted_insns_num)++;\n      *bb_insn_ref = def_insn->data;\n      return TRUE;\n    }\n  }\n  insn_change_p = FALSE;\n  success_p = TRUE;\n  while (VARR_LENGTH (MIR_reg_t, insn_vars) != 0) {\n    var = VARR_POP (MIR_reg_t, insn_vars);\n    if ((def_insn = get_uptodate_def_insn (gen_ctx, var)) == NULL) continue;\n    if (!move_code_p (def_insn->code)) continue;\n    insn_var_change_p = FALSE;\n    for (i = 0; i < nops; i++) { /* Change all var occurences: */\n      op_ref = &insn->ops[i];\n      op_change_p = FALSE;\n      MIR_insn_op_mode (ctx, insn, i, &out_p);\n      if (!out_p && op_ref->mode == MIR_OP_VAR && op_ref->u.var == var) {\n        /* It is not safe to substitute if there is another use after def insn before\n           the current as we delete def insn after substitution. */\n        *op_ref = def_insn->ops[1];\n        insn_var_change_p = op_change_p = TRUE;\n      } else if (op_ref->mode == MIR_OP_VAR_MEM\n                 && (op_ref->u.var_mem.base == var || op_ref->u.var_mem.index == var)) {\n        if (def_insn->ops[1].mode != MIR_OP_VAR) {\n          success_p = FALSE;\n        } else {\n          if (op_ref->u.var_mem.base == var) op_ref->u.var_mem.base = def_insn->ops[1].u.var;\n          if (op_ref->u.var_mem.index == var) op_ref->u.var_mem.index = def_insn->ops[1].u.var;\n          insn_var_change_p = op_change_p = TRUE;\n        }\n      }\n      if (op_change_p) VARR_PUSH (size_t, changed_op_numbers, i);\n    }\n    if (insn_var_change_p) {\n      if (success_p) success_p = i >= nops && target_insn_ok_p (gen_ctx, insn);\n      if (success_p) insn_change_p = TRUE;\n      while (VARR_LENGTH (size_t, changed_op_numbers)) {\n        i = VARR_POP (size_t, changed_op_numbers);\n        if (success_p)\n          VARR_SET (MIR_op_t, last_right_ops, i, insn->ops[i]);\n        else\n          insn->ops[i] = VARR_GET (MIR_op_t, last_right_ops, i); /* restore changed operands */\n      }\n      if (success_p) {\n        gen_assert (def_insn != NULL);\n        if (combine_delete_insn (gen_ctx, def_insn, bb_insn)) (*deleted_insns_num)++;\n        DEBUG (2, {\n          fprintf (debug_file, \"      changing to \");\n          print_bb_insn (gen_ctx, bb_insn, TRUE);\n        });\n      }\n    }\n  }\n  return insn_change_p;\n}\n\nstatic MIR_insn_t combine_exts (gen_ctx_t gen_ctx, bb_insn_t bb_insn, long *deleted_insns_num) {\n  MIR_insn_t def_insn, insn = bb_insn->insn;\n  MIR_insn_code_t code = insn->code;\n  MIR_op_t *op, saved_op;\n  int w, w2, sign_p = FALSE, sign2_p = FALSE, ok_p;\n\n  if ((w = get_ext_params (code, &sign_p)) == 0) return NULL;\n  op = &insn->ops[1];\n  if (op->mode != MIR_OP_VAR || !safe_var_substitution_p (gen_ctx, op->u.var, bb_insn)) return NULL;\n  def_insn = var_refs_addr[op->u.var].insn;\n  if ((w2 = get_ext_params (def_insn->code, &sign2_p)) == 0) return NULL;\n  if (obsolete_op_p (gen_ctx, def_insn->ops[1], var_refs_addr[op->u.var].insn_num)) return NULL;\n  if (w <= w2) {\n    /* [u]ext<w2> b,a; ... [u]ext<w> c,b -> [u]ext<w> c,a when <w> <= <w2>: */\n    saved_op = insn->ops[1];\n    insn->ops[1] = def_insn->ops[1];\n    if (!target_insn_ok_p (gen_ctx, insn)) {\n      insn->ops[1] = saved_op;\n      return NULL;\n    }\n    DEBUG (2, {\n      fprintf (debug_file, \"      changing to \");\n      print_bb_insn (gen_ctx, bb_insn, TRUE);\n    });\n    if (combine_delete_insn (gen_ctx, def_insn, bb_insn)) (*deleted_insns_num)++;\n    return insn;\n  } else if (w2 < w && (sign_p || !sign2_p)) { /* exclude ext<w2>, uext<w> pair */\n    /* [u]ext<w2> b,a; .. [u]ext<w> c,b -> [[u]ext<w2> b,a;] .. [u]ext<w2> c,a */\n    saved_op = insn->ops[1];\n    insn->ops[1] = def_insn->ops[1];\n    insn->code = def_insn->code;\n    ok_p = target_insn_ok_p (gen_ctx, insn);\n    insn->ops[1] = saved_op;\n    insn->code = code;\n    if (!ok_p) return NULL;\n    DEBUG (2, {\n      fprintf (debug_file, \"      changing \");\n      print_bb_insn (gen_ctx, bb_insn, FALSE);\n    });\n    insn->ops[1] = def_insn->ops[1];\n    insn->code = def_insn->code;\n    DEBUG (2, {\n      fprintf (debug_file, \" to \");\n      print_bb_insn (gen_ctx, bb_insn, TRUE);\n    });\n    if (combine_delete_insn (gen_ctx, def_insn, bb_insn)) (*deleted_insns_num)++;\n    return insn;\n  }\n  return NULL;\n}\n\nstatic void setup_var_ref (gen_ctx_t gen_ctx, MIR_reg_t var, MIR_insn_t insn, size_t nop,\n                           size_t insn_num, int def_p) {\n  static const var_ref_t var_ref = {NULL, 0, 0, FALSE, FALSE};\n\n  if (var == MIR_NON_VAR) return;\n  gen_assert (VARR_LENGTH (var_ref_t, var_refs) == VARR_LENGTH (size_t, var_ref_ages));\n  if (VARR_LENGTH (var_ref_t, var_refs) <= var) {\n    do {\n      VARR_PUSH (size_t, var_ref_ages, 0);\n      VARR_PUSH (var_ref_t, var_refs, var_ref);\n    } while (VARR_LENGTH (var_ref_t, var_refs) <= var);\n    var_refs_addr = VARR_ADDR (var_ref_t, var_refs);\n    var_ref_ages_addr = VARR_ADDR (size_t, var_ref_ages);\n  }\n  var_ref_ages_addr[var] = curr_bb_var_ref_age;\n  var_refs_addr[var].insn = insn;\n  var_refs_addr[var].nop = nop;\n  var_refs_addr[var].insn_num = insn_num;\n  var_refs_addr[var].def_p = def_p;\n  var_refs_addr[var].del_p = FALSE;\n}\n\nstatic void remove_property_insn (gen_ctx_t gen_ctx, MIR_insn_t insn) {\n  gen_assert (insn->code == MIR_PRSET || insn->code == MIR_PRBEQ || insn->code == MIR_PRBNE);\n  if (insn->code == MIR_PRSET\n      || (insn->code == MIR_PRBEQ && (insn->ops[2].mode != MIR_OP_INT || insn->ops[2].u.i != 0))\n      || (insn->code == MIR_PRBNE && (insn->ops[2].mode != MIR_OP_INT || insn->ops[2].u.i == 0))) {\n    DEBUG (2, {\n      fprintf (debug_file, \"      removing \");\n      print_insn (gen_ctx, insn, TRUE);\n    });\n    gen_delete_insn (gen_ctx, insn);\n  } else { /* make unconditional jump */\n    MIR_context_t ctx = gen_ctx->ctx;\n    MIR_insn_t new_insn = MIR_new_insn (ctx, MIR_JMP, insn->ops[0]);\n    MIR_insert_insn_before (ctx, curr_func_item, insn, new_insn);\n    DEBUG (2, {\n      fprintf (debug_file, \"      changing \");\n      print_insn (gen_ctx, insn, FALSE);\n    });\n    new_insn->data = insn->data;\n    if (optimize_level > 0) {\n      bb_insn_t bb_insn = insn->data;\n      bb_insn->insn = new_insn;\n    }\n    MIR_remove_insn (ctx, curr_func_item, insn);\n    DEBUG (2, {\n      fprintf (debug_file, \" to \");\n      print_insn (gen_ctx, new_insn, TRUE);\n    });\n  }\n}\n\nstatic void combine (gen_ctx_t gen_ctx, int no_property_p) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  MIR_insn_code_t code, new_code;\n  MIR_insn_t insn, new_insn;\n  bb_insn_t bb_insn, next_bb_insn;\n  size_t iter, nops, i, curr_insn_num;\n  MIR_op_t temp_op, *op_ref;\n  MIR_reg_t early_clobbered_hard_reg1, early_clobbered_hard_reg2;\n  int out_p, change_p, block_change_p, label_only_p;\n  long insns_num = 0, deleted_insns_num = 0;\n\n  gen_assert (optimize_level > 0);\n  for (bb_t bb = DLIST_HEAD (bb_t, curr_cfg->bbs); bb != NULL; bb = DLIST_NEXT (bb_t, bb)) {\n    do {\n      DEBUG (2, { fprintf (debug_file, \"Processing bb%lu\\n\", (unsigned long) bb->index); });\n      block_change_p = FALSE;\n      curr_bb_var_ref_age++;\n      last_mem_ref_insn_num = 0; /* means undef */\n      label_only_p = TRUE;\n      for (bb_insn = DLIST_HEAD (bb_insn_t, bb->bb_insns), curr_insn_num = 1; bb_insn != NULL;\n           bb_insn = next_bb_insn, curr_insn_num++) {\n        next_bb_insn = DLIST_NEXT (bb_insn_t, bb_insn);\n        insn = bb_insn->insn;\n        nops = MIR_insn_nops (ctx, insn);\n        if (insn->code == MIR_LABEL) {\n          if (!label_only_p) {\n            /* We can move insns with temp hard regs inside BB. It\n               is important to remove labels inside BB as we use labels to find BBs for lazy BB\n               generation and temp regs can be used between BBs in this generation mode. */\n            DEBUG (2, {\n              fprintf (debug_file, \"  Remove label inside BB \");\n              print_bb_insn (gen_ctx, bb_insn, TRUE);\n            });\n            gen_delete_insn (gen_ctx, insn);\n          }\n          continue;\n        }\n        label_only_p = FALSE;\n        insns_num++;\n        DEBUG (2, {\n          fprintf (debug_file, \"  Processing \");\n          print_bb_insn (gen_ctx, bb_insn, TRUE);\n        });\n        if (insn->code == MIR_PRSET || insn->code == MIR_PRBEQ || insn->code == MIR_PRBNE) {\n          if (no_property_p) remove_property_insn (gen_ctx, insn);\n          continue;\n        }\n        target_get_early_clobbered_hard_regs (insn, &early_clobbered_hard_reg1,\n                                              &early_clobbered_hard_reg2);\n        if (early_clobbered_hard_reg1 != MIR_NON_VAR)\n          setup_var_ref (gen_ctx, early_clobbered_hard_reg1, insn, 0 /* whatever */, curr_insn_num,\n                         TRUE);\n        if (early_clobbered_hard_reg2 != MIR_NON_VAR)\n          setup_var_ref (gen_ctx, early_clobbered_hard_reg2, insn, 0 /* whatever */, curr_insn_num,\n                         TRUE);\n        if (MIR_call_code_p (code = insn->code)) {\n          for (size_t hr = 0; hr <= MAX_HARD_REG; hr++)\n            if (bitmap_bit_p (call_used_hard_regs[MIR_T_UNDEF], hr)) {\n              setup_var_ref (gen_ctx, (MIR_reg_t) hr, insn, 0 /* whatever */, curr_insn_num, TRUE);\n            }\n          last_mem_ref_insn_num = curr_insn_num; /* Potentially call can change memory */\n        } else if (code == MIR_VA_BLOCK_ARG) {\n          last_mem_ref_insn_num = curr_insn_num; /* Change memory */\n        } else if (code == MIR_RET) {\n          /* ret is transformed in machinize and should be not modified after that */\n        } else if ((new_insn = combine_exts (gen_ctx, bb_insn, &deleted_insns_num)) != NULL) {\n          /* ssa ext removal is not enough as we can add ext insn in machinize for args and rets\n           */\n          bb_insn = new_insn->data;\n          insn = new_insn;\n          nops = MIR_insn_nops (ctx, insn);\n          block_change_p = TRUE;\n        } else {\n          if ((change_p = combine_substitute (gen_ctx, &bb_insn, &deleted_insns_num))) {\n            insn = bb_insn->insn;\n            nops = MIR_insn_nops (ctx, insn);\n          } else if (!change_p\n                     && (new_code = commutative_insn_code (insn->code)) != MIR_INSN_BOUND) {\n            insn->code = new_code;\n            temp_op = insn->ops[1];\n            insn->ops[1] = insn->ops[2];\n            insn->ops[2] = temp_op;\n            if (combine_substitute (gen_ctx, &bb_insn, &deleted_insns_num)) {\n              insn = bb_insn->insn;\n              nops = MIR_insn_nops (ctx, insn);\n              change_p = TRUE;\n            } else {\n              insn->code = code;\n              temp_op = insn->ops[1];\n              insn->ops[1] = insn->ops[2];\n              insn->ops[2] = temp_op;\n            }\n          }\n          if (change_p) block_change_p = TRUE;\n          if (code == MIR_BSTART || code == MIR_BEND) last_mem_ref_insn_num = curr_insn_num;\n        }\n\n        for (iter = 0; iter < 2; iter++) { /* update var ref info: */\n          for (i = 0; i < nops; i++) {\n            op_ref = &insn->ops[i];\n            MIR_insn_op_mode (ctx, insn, i, &out_p);\n            if (op_ref->mode == MIR_OP_VAR && !iter == !out_p) {\n              /* process in ops on 0th iteration and out ops on 1th iteration */\n              setup_var_ref (gen_ctx, op_ref->u.var, insn, i, curr_insn_num, iter == 1);\n            } else if (op_ref->mode == MIR_OP_VAR_MEM) {\n              if (out_p && iter == 1)\n                last_mem_ref_insn_num = curr_insn_num;\n              else if (iter == 0) {\n                setup_var_ref (gen_ctx, op_ref->u.var_mem.base, insn, i, curr_insn_num, FALSE);\n                setup_var_ref (gen_ctx, op_ref->u.var_mem.index, insn, i, curr_insn_num, FALSE);\n              }\n            }\n          }\n        }\n      }\n    } while (block_change_p);\n  }\n  DEBUG (1, {\n    fprintf (debug_file, \"%5ld deleted combine insns out of %ld (%.1f%%)\\n\", deleted_insns_num,\n             insns_num, 100.0 * deleted_insns_num / insns_num);\n  });\n}\n\nstatic void init_combine (gen_ctx_t gen_ctx) {\n  MIR_alloc_t alloc = gen_alloc (gen_ctx);\n  gen_ctx->combine_ctx = gen_malloc (gen_ctx, sizeof (struct combine_ctx));\n  curr_bb_var_ref_age = 0;\n  VARR_CREATE (size_t, var_ref_ages, alloc, 0);\n  VARR_CREATE (var_ref_t, var_refs, alloc, 0);\n  VARR_CREATE (MIR_reg_t, insn_vars, alloc, 0);\n  VARR_CREATE (size_t, changed_op_numbers, alloc, 16);\n  VARR_CREATE (MIR_op_t, last_right_ops, alloc, 16);\n  vars_bitmap = bitmap_create (alloc);\n}\n\nstatic void finish_combine (gen_ctx_t gen_ctx) {\n  VARR_DESTROY (size_t, var_ref_ages);\n  VARR_DESTROY (var_ref_t, var_refs);\n  VARR_DESTROY (MIR_reg_t, insn_vars);\n  VARR_DESTROY (size_t, changed_op_numbers);\n  VARR_DESTROY (MIR_op_t, last_right_ops);\n  bitmap_destroy (vars_bitmap);\n  gen_free (gen_ctx, gen_ctx->combine_ctx);\n  gen_ctx->combine_ctx = NULL;\n}\n\nstatic void remove_property_insns (gen_ctx_t gen_ctx) {\n  MIR_insn_t insn, next_insn;\n  for (insn = DLIST_HEAD (MIR_insn_t, curr_func_item->u.func->insns); insn != NULL;\n       insn = next_insn) {\n    next_insn = DLIST_NEXT (MIR_insn_t, insn);\n    if (insn->code == MIR_PRSET || insn->code == MIR_PRBEQ || insn->code == MIR_PRBNE)\n      remove_property_insn (gen_ctx, insn);\n  }\n}\n\n/* New Page */\n\n/* Dead code elimnination */\n\n#define live_out out\n\nstatic void dead_code_elimination (gen_ctx_t gen_ctx) {\n  MIR_alloc_t alloc = gen_alloc (gen_ctx);\n  MIR_insn_t insn, nop_insn;\n  bb_insn_t bb_insn, prev_bb_insn;\n  MIR_reg_t var, early_clobbered_hard_reg1, early_clobbered_hard_reg2;\n  int op_num, reg_def_p, dead_p;\n  bitmap_t live;\n  insn_var_iterator_t insn_var_iter;\n  long dead_insns_num = 0;\n  bitmap_t global_hard_regs\n    = _MIR_get_module_global_var_hard_regs (gen_ctx->ctx, curr_func_item->module);\n\n  gen_assert (optimize_level > 0);\n  DEBUG (2, { fprintf (debug_file, \"+++++++++++++Dead code elimination:\\n\"); });\n  live = bitmap_create2 (alloc, DEFAULT_INIT_BITMAP_BITS_NUM);\n  for (bb_t bb = DLIST_HEAD (bb_t, curr_cfg->bbs); bb != NULL; bb = DLIST_NEXT (bb_t, bb)) {\n    bitmap_copy (live, bb->live_out);\n    for (bb_insn = DLIST_TAIL (bb_insn_t, bb->bb_insns); bb_insn != NULL; bb_insn = prev_bb_insn) {\n      prev_bb_insn = DLIST_PREV (bb_insn_t, bb_insn);\n      insn = bb_insn->insn;\n      reg_def_p = FALSE;\n      dead_p = TRUE;\n      FOREACH_OUT_INSN_VAR (gen_ctx, insn_var_iter, insn, var, op_num) {\n        reg_def_p = TRUE;\n        if (bitmap_clear_bit_p (live, var) || bitmap_bit_p (addr_regs, var)) dead_p = FALSE;\n      }\n      if (!reg_def_p) dead_p = FALSE;\n      if (dead_p && !MIR_call_code_p (insn->code) && insn->code != MIR_RET && insn->code != MIR_JRET\n          && insn->code != MIR_ALLOCA && insn->code != MIR_BSTART && insn->code != MIR_BEND\n          && insn->code != MIR_VA_START && insn->code != MIR_VA_ARG && insn->code != MIR_VA_END\n          && !(MIR_overflow_insn_code_p (insn->code)\n               && reachable_bo_exists_p (DLIST_NEXT (bb_insn_t, bb_insn)))\n          && !(insn->ops[0].mode == MIR_OP_VAR\n               && (insn->ops[0].u.var == FP_HARD_REG || insn->ops[0].u.var == SP_HARD_REG))) {\n        DEBUG (2, {\n          fprintf (debug_file, \"  Removing dead insn %-5lu\", (unsigned long) bb_insn->index);\n          MIR_output_insn (gen_ctx->ctx, debug_file, insn, curr_func_item->u.func, TRUE);\n        });\n        if (DLIST_HEAD (bb_insn_t, bb->bb_insns) == DLIST_TAIL (bb_insn_t, bb->bb_insns)) {\n          gen_assert (bb_insn == DLIST_HEAD (bb_insn_t, bb->bb_insns));\n          nop_insn = MIR_new_insn_arr (gen_ctx->ctx, MIR_USE, 0, NULL);\n          DEBUG (2, {\n            fprintf (debug_file,\n                     \"  Adding nop to keep bb%lu non-empty: \", (unsigned long) bb->index);\n            MIR_output_insn (gen_ctx->ctx, debug_file, nop_insn, curr_func_item->u.func, TRUE);\n          });\n          gen_add_insn_after (gen_ctx, insn, nop_insn);\n        }\n        gen_delete_insn (gen_ctx, insn);\n        dead_insns_num++;\n        continue;\n      }\n      if (MIR_call_code_p (insn->code)) {\n        bitmap_and_compl (live, live, call_used_hard_regs[MIR_T_UNDEF]);\n        if (global_hard_regs != NULL) bitmap_ior (live, live, global_hard_regs);\n      }\n      FOREACH_IN_INSN_VAR (gen_ctx, insn_var_iter, insn, var, op_num) {\n        bitmap_set_bit_p (live, var);\n      }\n      target_get_early_clobbered_hard_regs (insn, &early_clobbered_hard_reg1,\n                                            &early_clobbered_hard_reg2);\n      if (early_clobbered_hard_reg1 != MIR_NON_VAR)\n        bitmap_clear_bit_p (live, early_clobbered_hard_reg1);\n      if (early_clobbered_hard_reg2 != MIR_NON_VAR)\n        bitmap_clear_bit_p (live, early_clobbered_hard_reg2);\n      if (MIR_call_code_p (insn->code)) bitmap_ior (live, live, bb_insn->call_hard_reg_args);\n    }\n  }\n  bitmap_destroy (live);\n  DEBUG (1, { fprintf (debug_file, \"%5ld removed dead insns\\n\", dead_insns_num); });\n}\n\n#undef live_out\n\n/* New Page */\n\n#if !MIR_NO_GEN_DEBUG\n#include \"real-time.h\"\n#endif\n\n#if MIR_GEN_CALL_TRACE\nstatic void *print_and_execute_wrapper (gen_ctx_t gen_ctx, MIR_item_t called_func) {\n  gen_assert (called_func->item_type == MIR_func_item);\n  fprintf (stderr, \"Calling %s\\n\", called_func->u.func->name);\n  return called_func->u.func->machine_code;\n}\n#endif\n\nstatic const int collect_bb_stat_p = FALSE;\n\nstatic void *generate_func_code (MIR_context_t ctx, MIR_item_t func_item, int machine_code_p) {\n  gen_ctx_t gen_ctx = *gen_ctx_loc (ctx);\n  uint8_t *code;\n  void *machine_code = NULL;\n  size_t code_len = 0;\n  double start_time = real_usec_time ();\n  uint32_t bbs_num;\n\n  gen_assert (func_item->item_type == MIR_func_item && func_item->data == NULL);\n  if (func_item->u.func->machine_code != NULL) {\n    gen_assert (func_item->u.func->call_addr != NULL);\n    _MIR_redirect_thunk (ctx, func_item->addr, func_item->u.func->call_addr);\n    DEBUG (2, {\n      fprintf (debug_file, \"+++++++++++++The code for %s has been already generated\\n\",\n               MIR_item_name (ctx, func_item));\n    });\n    return func_item->addr;\n  }\n  DEBUG (0, {\n    fprintf (debug_file, \"Code generation of function %s:\\n\", MIR_item_name (ctx, func_item));\n  });\n  DEBUG (2, {\n    fprintf (debug_file, \"+++++++++++++MIR before generator:\\n\");\n    MIR_output_item (ctx, debug_file, func_item);\n  });\n  curr_func_item = func_item;\n  _MIR_duplicate_func_insns (ctx, func_item);\n  curr_cfg = func_item->data = gen_malloc (gen_ctx, sizeof (struct func_cfg));\n  build_func_cfg (gen_ctx);\n  bbs_num = curr_bb_index;\n  DEBUG (2, {\n    fprintf (debug_file, \"+++++++++++++MIR after building CFG:\\n\");\n    print_CFG (gen_ctx, TRUE, FALSE, TRUE, FALSE, NULL);\n  });\n  if (optimize_level >= 2 && !addr_insn_p && clone_bbs (gen_ctx)) {\n    /* do not clone bbs before addr transformation: it can prevent addr transformations */\n    DEBUG (2, {\n      fprintf (debug_file, \"+++++++++++++MIR after cloning BBs:\\n\");\n      print_CFG (gen_ctx, TRUE, FALSE, TRUE, FALSE, NULL);\n    });\n  }\n  if (optimize_level >= 2) {\n    build_ssa (gen_ctx, !addr_insn_p);\n    DEBUG (2, {\n      fprintf (debug_file, \"+++++++++++++MIR after building SSA%s:\\n\",\n               addr_insn_p ? \" for address transformation\" : \"\");\n      print_varr_insns (gen_ctx, \"undef init\", undef_insns);\n      print_varr_insns (gen_ctx, \"arg init\", arg_bb_insns);\n      fprintf (debug_file, \"\\n\");\n      print_CFG (gen_ctx, TRUE, FALSE, TRUE, TRUE, NULL);\n    });\n  }\n  if (optimize_level >= 2 && addr_insn_p) {\n    DEBUG (2, { fprintf (debug_file, \"+++++++++++++Transform Addr Insns and cloning BBs:\\n\"); });\n    transform_addrs (gen_ctx);\n    undo_build_ssa (gen_ctx);\n    clone_bbs (gen_ctx);\n    build_ssa (gen_ctx, TRUE);\n    DEBUG (2, {\n      fprintf (debug_file, \"+++++++++++++MIR after Addr Insns Transformation and cloning BBs:\\n\");\n      print_varr_insns (gen_ctx, \"undef init\", undef_insns);\n      print_varr_insns (gen_ctx, \"arg init\", arg_bb_insns);\n      fprintf (debug_file, \"\\n\");\n      print_CFG (gen_ctx, TRUE, FALSE, TRUE, TRUE, NULL);\n    });\n  }\n  if (optimize_level >= 2) {\n    DEBUG (2, { fprintf (debug_file, \"+++++++++++++GVN:\\n\"); });\n    gvn (gen_ctx);\n    DEBUG (2, {\n      fprintf (debug_file, \"+++++++++++++MIR after GVN:\\n\");\n      print_CFG (gen_ctx, TRUE, FALSE, TRUE, TRUE, NULL);\n    });\n    gvn_clear (gen_ctx);\n  }\n  if (optimize_level >= 2) {\n    DEBUG (2, { fprintf (debug_file, \"+++++++++++++Copy Propagation:\\n\"); });\n    copy_prop (gen_ctx);\n    DEBUG (2, {\n      fprintf (debug_file, \"+++++++++++++MIR after Copy Propagation:\\n\");\n      print_CFG (gen_ctx, TRUE, FALSE, TRUE, TRUE, NULL);\n    });\n  }\n  if (optimize_level >= 2) {\n    DEBUG (2, { fprintf (debug_file, \"+++++++++++++DSE:\\n\"); });\n    dse (gen_ctx);\n    DEBUG (2, {\n      fprintf (debug_file, \"+++++++++++++MIR after DSE:\\n\");\n      print_CFG (gen_ctx, TRUE, FALSE, TRUE, TRUE, NULL);\n    });\n  }\n  if (optimize_level >= 2) {\n    ssa_dead_code_elimination (gen_ctx);\n    DEBUG (2, {\n      fprintf (debug_file, \"+++++++++++++MIR after dead code elimination:\\n\");\n      print_CFG (gen_ctx, TRUE, TRUE, TRUE, TRUE, NULL);\n    });\n  }\n  if (optimize_level >= 2) {\n    build_loop_tree (gen_ctx);\n    DEBUG (2, { print_loop_tree (gen_ctx, TRUE); });\n    if (licm (gen_ctx)) {\n      DEBUG (2, {\n        fprintf (debug_file, \"+++++++++++++MIR after loop invariant motion:\\n\");\n        print_CFG (gen_ctx, TRUE, TRUE, TRUE, TRUE, NULL);\n      });\n    }\n    destroy_loop_tree (gen_ctx, curr_cfg->root_loop_node);\n    curr_cfg->root_loop_node = NULL;\n  }\n  if (optimize_level >= 2 && pressure_relief (gen_ctx)) {\n    DEBUG (2, {\n      fprintf (debug_file, \"+++++++++++++MIR after pressure relief:\\n\");\n      print_CFG (gen_ctx, TRUE, TRUE, TRUE, TRUE, NULL);\n    });\n  }\n  if (optimize_level >= 2) {\n    make_conventional_ssa (gen_ctx);\n    DEBUG (2, {\n      fprintf (debug_file, \"+++++++++++++MIR after transformation to conventional SSA:\\n\");\n      print_CFG (gen_ctx, TRUE, TRUE, TRUE, TRUE, NULL);\n    });\n    ssa_combine (gen_ctx);\n    DEBUG (2, {\n      fprintf (debug_file, \"+++++++++++++MIR after ssa combine:\\n\");\n      print_CFG (gen_ctx, TRUE, TRUE, TRUE, TRUE, NULL);\n    });\n    undo_build_ssa (gen_ctx);\n    DEBUG (2, {\n      fprintf (debug_file, \"+++++++++++++MIR after destroying ssa:\\n\");\n      print_varr_insns (gen_ctx, \"undef init\", undef_insns);\n      print_varr_insns (gen_ctx, \"arg init\", arg_bb_insns);\n      fprintf (debug_file, \"\\n\");\n      print_CFG (gen_ctx, TRUE, FALSE, TRUE, TRUE, NULL);\n    });\n  }\n  if (optimize_level >= 2) {\n    DEBUG (2, { fprintf (debug_file, \"+++++++++++++Jump optimization:\\n\"); });\n    jump_opt (gen_ctx);\n    DEBUG (2, {\n      fprintf (debug_file, \"+++++++++++++MIR after Jump optimization:\\n\");\n      print_CFG (gen_ctx, TRUE, FALSE, TRUE, TRUE, NULL);\n    });\n  }\n  target_machinize (gen_ctx);\n  make_io_dup_op_insns (gen_ctx);\n  DEBUG (2, {\n    fprintf (debug_file, \"+++++++++++++MIR after machinize:\\n\");\n    print_CFG (gen_ctx, FALSE, FALSE, TRUE, TRUE, NULL);\n  });\n  if (optimize_level >= 1) {\n    build_loop_tree (gen_ctx);\n    DEBUG (2, { print_loop_tree (gen_ctx, TRUE); });\n  }\n  if (optimize_level >= 2) {\n    collect_moves (gen_ctx);\n    if (consider_move_vars_only (gen_ctx)) {\n      calculate_func_cfg_live_info (gen_ctx, FALSE);\n      print_live_info (gen_ctx, \"Live info before coalesce\", TRUE, FALSE);\n      coalesce (gen_ctx);\n      DEBUG (2, {\n        fprintf (debug_file, \"+++++++++++++MIR after coalescing:\\n\");\n        print_CFG (gen_ctx, TRUE, TRUE, TRUE, TRUE, output_bb_border_live_info);\n      });\n    }\n  }\n  consider_all_live_vars (gen_ctx);\n  calculate_func_cfg_live_info (gen_ctx, TRUE);\n  print_live_info (gen_ctx, \"Live info before RA\", optimize_level > 0, TRUE);\n  reg_alloc (gen_ctx);\n  DEBUG (2, {\n    fprintf (debug_file, \"+++++++++++++MIR after RA:\\n\");\n    print_CFG (gen_ctx, TRUE, FALSE, TRUE, FALSE, NULL);\n  });\n  if (optimize_level < 2 && machine_code_p) remove_property_insns (gen_ctx);\n  if (optimize_level >= 1) {\n    consider_all_live_vars (gen_ctx);\n    calculate_func_cfg_live_info (gen_ctx, FALSE);\n    add_bb_insn_dead_vars (gen_ctx);\n    print_live_info (gen_ctx, \"Live info before combine\", FALSE, FALSE);\n    combine (gen_ctx, machine_code_p); /* After combine the BB live info is still valid */\n    DEBUG (2, {\n      fprintf (debug_file, \"+++++++++++++MIR after combine:\\n\");\n      print_CFG (gen_ctx, FALSE, FALSE, TRUE, FALSE, NULL);\n    });\n    dead_code_elimination (gen_ctx);\n    DEBUG (2, {\n      fprintf (debug_file, \"+++++++++++++MIR after dead code elimination after 2nd combine:\\n\");\n      print_CFG (gen_ctx, TRUE, TRUE, TRUE, FALSE, output_bb_live_info);\n    });\n  }\n  target_make_prolog_epilog (gen_ctx, func_used_hard_regs, func_stack_slots_num);\n  target_split_insns (gen_ctx);\n  DEBUG (2, {\n    fprintf (debug_file, \"+++++++++++++MIR after forming prolog/epilog and insn splitting:\\n\");\n    print_CFG (gen_ctx, FALSE, FALSE, TRUE, TRUE, NULL);\n  });\n  if (machine_code_p) {\n    code = target_translate (gen_ctx, &code_len);\n    machine_code = func_item->u.func->call_addr = _MIR_publish_code (ctx, code, code_len);\n    target_rebase (gen_ctx, func_item->u.func->call_addr);\n#if MIR_GEN_CALL_TRACE\n    func_item->u.func->call_addr = _MIR_get_wrapper (ctx, func_item, print_and_execute_wrapper);\n#endif\n    DEBUG (2, {\n      _MIR_dump_code (NULL, machine_code, code_len);\n      fprintf (debug_file, \"code size = %lu:\\n\", (unsigned long) code_len);\n    });\n    _MIR_redirect_thunk (ctx, func_item->addr, func_item->u.func->call_addr);\n  }\n  if (optimize_level != 0) destroy_loop_tree (gen_ctx, curr_cfg->root_loop_node);\n  destroy_func_cfg (gen_ctx);\n  if (collect_bb_stat_p) overall_bbs_num += bbs_num;\n  if (!machine_code_p) return NULL;\n  DEBUG (0, {\n    fprintf (debug_file,\n             \"  Code generation for %s: %lu MIR insns (addr=%llx, len=%lu) -- time %.2f ms\\n\",\n             MIR_item_name (ctx, func_item),\n             (long unsigned) DLIST_LENGTH (MIR_insn_t, func_item->u.func->insns),\n             (unsigned long long) machine_code, (unsigned long) code_len,\n             (real_usec_time () - start_time) / 1000.0);\n  });\n  _MIR_restore_func_insns (ctx, func_item);\n  /* ??? We should use atomic here but c2mir does not implement them yet.  */\n  func_item->u.func->machine_code = machine_code;\n  return func_item->addr;\n}\n\nvoid *MIR_gen (MIR_context_t ctx, MIR_item_t func_item) {\n  return generate_func_code (ctx, func_item, TRUE);\n}\n\nvoid MIR_gen_set_debug_file (MIR_context_t ctx, FILE *f) {\n#if !MIR_NO_GEN_DEBUG\n  gen_ctx_t gen_ctx = *gen_ctx_loc (ctx);\n  if (gen_ctx == NULL) {\n    fprintf (stderr, \"Calling MIR_gen_set_debug_file before MIR_gen_init -- good bye\\n\");\n    exit (1);\n  }\n  debug_file = f;\n#endif\n}\n\nvoid MIR_gen_set_debug_level (MIR_context_t ctx, int level) {\n#if !MIR_NO_GEN_DEBUG\n  gen_ctx_t gen_ctx = *gen_ctx_loc (ctx);\n  if (gen_ctx == NULL) {\n    fprintf (stderr, \"Calling MIR_gen_set_debug_level before MIR_gen_init -- good bye\\n\");\n    exit (1);\n  }\n  debug_level = level;\n#endif\n}\n\nvoid MIR_gen_set_optimize_level (MIR_context_t ctx, unsigned int level) {\n  gen_ctx_t gen_ctx = *gen_ctx_loc (ctx);\n  if (gen_ctx == NULL) {\n    fprintf (stderr, \"Calling MIR_gen_set_optimize_level before MIR_gen_init -- good bye\\n\");\n    exit (1);\n  }\n  optimize_level = level;\n}\n\nstatic void generate_bb_version_machine_code (gen_ctx_t gen_ctx, bb_version_t bb_version);\nstatic void *bb_version_generator (gen_ctx_t gen_ctx, bb_version_t bb_version);\n\nstatic bb_version_t get_bb_version (gen_ctx_t gen_ctx, bb_stub_t bb_stub, uint32_t n_attrs,\n                                    spot_attr_t *attrs, int call_p, void **addr) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  bb_version_t bb_version;\n\n  if ((bb_version = DLIST_HEAD (bb_version_t, bb_stub->bb_versions)) != NULL) {\n    VARR_PUSH (target_bb_version_t, target_succ_bb_versions, NULL);\n    *addr = bb_version->addr;\n    return bb_version;\n  }\n  bb_version = gen_malloc_and_mark_to_free (gen_ctx, sizeof (struct bb_version)\n                                                       + (n_attrs <= 1 ? 0 : n_attrs)\n                                                           * sizeof (spot_attr_t));\n  target_init_bb_version_data (&bb_version->target_data);\n  VARR_PUSH (target_bb_version_t, target_succ_bb_versions,\n             call_p ? NULL : &bb_version->target_data);\n  bb_version->bb_stub = bb_stub;\n  bb_version->n_attrs = n_attrs;\n  if (n_attrs != 0) memcpy (bb_version->attrs, attrs, n_attrs * sizeof (spot_attr_t));\n  bb_version->call_p = call_p;\n  DLIST_APPEND (bb_version_t, bb_stub->bb_versions, bb_version);\n  bb_version->machine_code = NULL;\n  *addr = bb_version->addr = _MIR_get_bb_thunk (ctx, bb_version, bb_wrapper);\n  return bb_version;\n}\n\n/* create bb stubs and set up label data to the corresponding bb stub */\n/* todo finish bb on calls ??? */\nstatic void create_bb_stubs (gen_ctx_t gen_ctx) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  MIR_insn_t insn, last_lab_insn;\n  size_t n_bbs;\n  int new_bb_p = TRUE;\n  bb_stub_t bb_stubs;\n\n  n_bbs = 0;\n  for (insn = DLIST_HEAD (MIR_insn_t, curr_func_item->u.func->insns); insn != NULL;\n       insn = DLIST_NEXT (MIR_insn_t, insn)) {\n    if (insn->code == MIR_LABEL || new_bb_p) {\n      last_lab_insn = insn;\n      if (insn->code == MIR_LABEL)\n        for (insn = DLIST_NEXT (MIR_insn_t, insn); insn != NULL && insn->code == MIR_LABEL;\n             last_lab_insn = insn, insn = DLIST_NEXT (MIR_insn_t, insn))\n          ;\n      insn = last_lab_insn;\n      n_bbs++;\n    }\n    new_bb_p = MIR_any_branch_code_p (insn->code) || insn->code == MIR_RET || insn->code == MIR_JRET\n               || insn->code == MIR_PRBEQ || insn->code == MIR_PRBNE;\n  }\n  curr_func_item->data = bb_stubs = gen_malloc (gen_ctx, sizeof (struct bb_stub) * n_bbs);\n  n_bbs = 0;\n  new_bb_p = TRUE;\n  for (insn = DLIST_HEAD (MIR_insn_t, curr_func_item->u.func->insns); insn != NULL;\n       insn = DLIST_NEXT (MIR_insn_t, insn)) {\n    if (insn->code == MIR_LABEL || new_bb_p) {\n      if (n_bbs != 0) bb_stubs[n_bbs - 1].last_insn = DLIST_PREV (MIR_insn_t, insn);\n      bb_stubs[n_bbs].func_item = curr_func_item;\n      bb_stubs[n_bbs].first_insn = insn;\n      DLIST_INIT (bb_version_t, bb_stubs[n_bbs].bb_versions);\n      last_lab_insn = insn;\n      if (insn->code == MIR_LABEL) {\n        insn->data = &bb_stubs[n_bbs];\n        for (insn = DLIST_NEXT (MIR_insn_t, insn); insn != NULL && insn->code == MIR_LABEL;\n             last_lab_insn = insn, insn = DLIST_NEXT (MIR_insn_t, insn))\n          insn->data = &bb_stubs[n_bbs];\n      }\n      insn = last_lab_insn;\n      n_bbs++;\n    }\n    new_bb_p = MIR_any_branch_code_p (insn->code) || insn->code == MIR_RET || insn->code == MIR_JRET\n               || insn->code == MIR_PRBEQ || insn->code == MIR_PRBNE;\n  }\n  bb_stubs[n_bbs - 1].last_insn = DLIST_TAIL (MIR_insn_t, curr_func_item->u.func->insns);\n  if (debug_file != NULL) {\n    fprintf (debug_file, \"BBs for lazy code generation:\\n\");\n    for (size_t i = 0; i < n_bbs; i++) {\n      fprintf (debug_file, \"  BBStub%llx:\\n\", (long long unsigned) &bb_stubs[i]);\n      for (insn = bb_stubs[i].first_insn;; insn = DLIST_NEXT (MIR_insn_t, insn)) {\n        MIR_output_insn (ctx, debug_file, insn, curr_func_item->u.func, TRUE);\n        if (insn == bb_stubs[i].last_insn) break;\n      }\n    }\n  }\n  for (MIR_lref_data_t lref = curr_func_item->u.func->first_lref; lref != NULL; lref = lref->next) {\n    bb_stub_t lab_bb_stub = lref->label->data;\n    void *addr, *addr2;\n    (void) get_bb_version (gen_ctx, lab_bb_stub, 0, NULL, FALSE, &addr);\n    if (lref->label2 != NULL) {\n      lab_bb_stub = lref->label2->data;\n      (void) get_bb_version (gen_ctx, lab_bb_stub, 0, NULL, FALSE, &addr2);\n      addr = (void *) ((char *) addr - (char *) addr2);\n    }\n    addr = (void *) ((char *) addr + lref->disp);\n    *(void **) lref->load_addr = addr;\n  }\n}\n\nvoid MIR_gen_init (MIR_context_t ctx) {\n  MIR_alloc_t alloc = MIR_get_alloc (ctx);\n  gen_ctx_t gen_ctx, *gen_ctx_ptr = gen_ctx_loc (ctx);\n\n  *gen_ctx_ptr = gen_ctx = MIR_malloc (alloc, sizeof (struct gen_ctx));\n  if (gen_ctx == NULL) util_error (gen_ctx, \"no memory\");\n\n  gen_ctx->ctx = ctx;\n  optimize_level = 2;\n  gen_ctx->target_ctx = NULL;\n  gen_ctx->data_flow_ctx = NULL;\n  gen_ctx->gvn_ctx = NULL;\n  gen_ctx->lr_ctx = NULL;\n  gen_ctx->ra_ctx = NULL;\n  gen_ctx->combine_ctx = NULL;\n#if !MIR_NO_GEN_DEBUG\n  debug_file = NULL;\n  debug_level = 100;\n#endif\n  VARR_CREATE (void_ptr_t, to_free, alloc, 0);\n  addr_insn_p = FALSE;\n  VARR_CREATE (MIR_op_t, temp_ops, alloc, 16);\n  VARR_CREATE (MIR_insn_t, temp_insns, alloc, 16);\n  VARR_CREATE (MIR_insn_t, temp_insns2, alloc, 16);\n  VARR_CREATE (bb_insn_t, temp_bb_insns, alloc, 16);\n  VARR_CREATE (bb_insn_t, temp_bb_insns2, alloc, 16);\n  VARR_CREATE (loop_node_t, loop_nodes, alloc, 32);\n  VARR_CREATE (loop_node_t, queue_nodes, alloc, 32);\n  VARR_CREATE (loop_node_t, loop_entries, alloc, 16);\n  VARR_CREATE (mem_attr_t, mem_attrs, alloc, 32);\n  VARR_CREATE (target_bb_version_t, target_succ_bb_versions, alloc, 16);\n  VARR_CREATE (void_ptr_t, succ_bb_addrs, alloc, 16);\n  VARR_CREATE (spot_attr_t, spot_attrs, alloc, 32);\n  VARR_CREATE (spot_attr_t, spot2attr, alloc, 32);\n  temp_bitmap = bitmap_create2 (alloc, DEFAULT_INIT_BITMAP_BITS_NUM);\n  temp_bitmap2 = bitmap_create2 (alloc, DEFAULT_INIT_BITMAP_BITS_NUM);\n  temp_bitmap3 = bitmap_create2 (alloc, DEFAULT_INIT_BITMAP_BITS_NUM);\n  init_dead_vars (gen_ctx);\n  init_data_flow (gen_ctx);\n  init_ssa (gen_ctx);\n  init_gvn (gen_ctx);\n  init_live_ranges (gen_ctx);\n  init_coalesce (gen_ctx);\n  init_ra (gen_ctx);\n  init_combine (gen_ctx);\n  target_init (gen_ctx);\n  max_int_hard_regs = max_fp_hard_regs = 0;\n  for (int i = 0; i <= MAX_HARD_REG; i++) {\n    if (target_fixed_hard_reg_p (i)) continue;\n    target_hard_reg_type_ok_p (i, MIR_T_I32) ? max_int_hard_regs++ : max_fp_hard_regs++;\n  }\n  for (MIR_type_t type = MIR_T_I8; type < MIR_T_BOUND; type++) {\n    call_used_hard_regs[type] = bitmap_create2 (alloc, MAX_HARD_REG + 1);\n    for (int i = 0; i <= MAX_HARD_REG; i++) {\n      /* We need call_used_hard_regs even for fixed regs in combine. */\n      if (target_call_used_hard_reg_p (i, type)) bitmap_set_bit_p (call_used_hard_regs[type], i);\n    }\n  }\n  tied_regs = bitmap_create2 (alloc, 256);\n  addr_regs = bitmap_create2 (alloc, 256);\n  insn_to_consider = bitmap_create2 (alloc, 1024);\n  func_used_hard_regs = bitmap_create2 (alloc, MAX_HARD_REG + 1);\n  bb_wrapper = _MIR_get_bb_wrapper (ctx, gen_ctx, bb_version_generator);\n  overall_bbs_num = overall_gen_bbs_num = 0;\n}\n\nvoid MIR_gen_finish (MIR_context_t ctx) {\n  gen_ctx_t *gen_ctx_ptr = gen_ctx_loc (ctx), gen_ctx = *gen_ctx_ptr;\n\n  if (gen_ctx == NULL) {\n    fprintf (stderr, \"Calling MIR_gen_finish before MIR_gen_init -- good bye\\n\");\n    exit (1);\n  }\n  finish_data_flow (gen_ctx);\n  finish_ssa (gen_ctx);\n  finish_gvn (gen_ctx);\n  finish_live_ranges (gen_ctx);\n  finish_coalesce (gen_ctx);\n  finish_ra (gen_ctx);\n  finish_combine (gen_ctx);\n  for (MIR_type_t type = MIR_T_I8; type < MIR_T_BOUND; type++)\n    bitmap_destroy (call_used_hard_regs[type]);\n  bitmap_destroy (tied_regs);\n  bitmap_destroy (addr_regs);\n  bitmap_destroy (insn_to_consider);\n  bitmap_destroy (func_used_hard_regs);\n  target_finish (gen_ctx);\n  finish_dead_vars (gen_ctx);\n  gen_free (gen_ctx, gen_ctx->data_flow_ctx);\n  bitmap_destroy (temp_bitmap);\n  bitmap_destroy (temp_bitmap2);\n  bitmap_destroy (temp_bitmap3);\n  VARR_DESTROY (MIR_op_t, temp_ops);\n  VARR_DESTROY (MIR_insn_t, temp_insns);\n  VARR_DESTROY (MIR_insn_t, temp_insns2);\n  VARR_DESTROY (bb_insn_t, temp_bb_insns);\n  VARR_DESTROY (bb_insn_t, temp_bb_insns2);\n  VARR_DESTROY (loop_node_t, loop_nodes);\n  VARR_DESTROY (loop_node_t, queue_nodes);\n  VARR_DESTROY (loop_node_t, loop_entries);\n  VARR_DESTROY (mem_attr_t, mem_attrs);\n  VARR_DESTROY (target_bb_version_t, target_succ_bb_versions);\n  VARR_DESTROY (void_ptr_t, succ_bb_addrs);\n  VARR_DESTROY (spot_attr_t, spot_attrs);\n  VARR_DESTROY (spot_attr_t, spot2attr);\n  while (VARR_LENGTH (void_ptr_t, to_free) != 0) gen_free (gen_ctx, VARR_POP (void_ptr_t, to_free));\n  VARR_DESTROY (void_ptr_t, to_free);\n  if (collect_bb_stat_p)\n    fprintf (stderr, \"Overall bbs num = %llu, generated bbs num = %llu\\n\", overall_bbs_num,\n             overall_gen_bbs_num);\n  gen_free (gen_ctx, gen_ctx);\n  *gen_ctx_ptr = NULL;\n}\n\nvoid MIR_set_gen_interface (MIR_context_t ctx, MIR_item_t func_item) {\n  if (func_item == NULL) { /* finish setting interfaces */\n    target_change_to_direct_calls (ctx);\n  } else {\n    MIR_gen (ctx, func_item);\n  }\n}\n\n/* Lazy func generation is done right away. */\nstatic void generate_func_and_redirect (MIR_context_t ctx, MIR_item_t func_item, int full_p) {\n  generate_func_code (ctx, func_item, full_p);\n  if (full_p) return;\n  gen_ctx_t gen_ctx = *gen_ctx_loc (ctx);\n  void *addr;\n  create_bb_stubs (gen_ctx);\n  (void) get_bb_version (gen_ctx, &((struct bb_stub *) func_item->data)[0], 0, NULL, TRUE, &addr);\n  _MIR_redirect_thunk (ctx, func_item->addr, addr);\n}\n\nstatic void *generate_func_and_redirect_to_func_code (MIR_context_t ctx, MIR_item_t func_item) {\n  generate_func_and_redirect (ctx, func_item, TRUE);\n  return func_item->u.func->machine_code;\n}\n\nvoid MIR_set_lazy_gen_interface (MIR_context_t ctx, MIR_item_t func_item) {\n  void *addr;\n\n  if (func_item == NULL) return;\n  addr = _MIR_get_wrapper (ctx, func_item, generate_func_and_redirect_to_func_code);\n  _MIR_redirect_thunk (ctx, func_item->addr, addr);\n}\n\nstatic void set_spot2attr (gen_ctx_t gen_ctx, const spot_attr_t *attr) {\n  gen_assert (attr->spot != 0 && attr->prop != 0);\n  while (VARR_LENGTH (spot_attr_t, spot2attr) <= attr->spot)\n    VARR_PUSH (spot_attr_t, spot2attr, *attr);\n  VARR_SET (spot_attr_t, spot2attr, attr->spot, *attr);\n}\n\n#define FIRST_MEM_SPOT (MAX_HARD_REG + 2)\nstatic int mem_spot_p (uint32_t spot) { return spot >= FIRST_MEM_SPOT; }\n\nstatic uint32_t mem_nloc2spot (uint32_t nloc) {\n  return nloc == 0 ? 0 : nloc + 1 + MAX_HARD_REG + 1;\n}\n\nstatic uint32_t op2spot (MIR_op_t *op_ref) {\n  if (op_ref->mode == MIR_OP_VAR) return op_ref->u.var + 1;\n  if (op_ref->mode == MIR_OP_VAR_MEM) return mem_nloc2spot (op_ref->u.var_mem.nloc);\n  return 0;\n}\n\nstatic void generate_bb_version_machine_code (gen_ctx_t gen_ctx, bb_version_t bb_version) {\n  MIR_context_t ctx = gen_ctx->ctx;\n  int skip_p;\n  bb_stub_t branch_bb_stub, bb_stub = bb_version->bb_stub;\n  MIR_insn_t curr_insn, new_insn, next_insn;\n  void *addr;\n  uint8_t *code;\n  size_t code_len, nel;\n  uint32_t prop, spot, dest_spot, src_spot, max_spot = 0;\n  bitmap_t nonzero_property_spots = temp_bitmap;\n  bitmap_iterator_t bi;\n  spot_attr_t spot_attr;\n\n  bitmap_clear (nonzero_property_spots);\n  DEBUG (2, {\n    fprintf (debug_file, \"  IN BBStub%llx nonzero properties: \", (long long unsigned) bb_stub);\n  });\n  for (size_t i = 0; i < bb_version->n_attrs; i++) {\n    bitmap_set_bit_p (nonzero_property_spots, bb_version->attrs[i].spot);\n    set_spot2attr (gen_ctx, &bb_version->attrs[i]);\n    DEBUG (2, {\n      if (i != 0) fprintf (debug_file, \", \");\n      fprintf (debug_file, \"(spot=%u,prop=%u)\", (unsigned) bb_version->attrs[i].spot,\n               (unsigned) bb_version->attrs[i].prop);\n    });\n  }\n  DEBUG (2, { fprintf (debug_file, \"\\n\"); });\n  if (bb_version->n_attrs != 0) max_spot = bb_version->attrs[bb_version->n_attrs - 1].spot;\n  VARR_TRUNC (target_bb_version_t, target_succ_bb_versions, 0);\n  target_bb_translate_start (gen_ctx);\n  for (curr_insn = bb_stub->first_insn;; curr_insn = next_insn) {\n    next_insn = DLIST_NEXT (MIR_insn_t, curr_insn);\n    if (MIR_any_branch_code_p (curr_insn->code)) break;\n    skip_p = FALSE;\n    switch (curr_insn->code) {\n    case MIR_USE: skip_p = TRUE; break;\n    case MIR_PRSET:\n      gen_assert (curr_insn->ops[1].mode == MIR_OP_INT || curr_insn->ops[1].mode == MIR_OP_UINT);\n      dest_spot = op2spot (&curr_insn->ops[0]);\n      if (dest_spot == 0) {\n      } else if (curr_insn->ops[1].u.i == 0) { /* ??? aliased */\n        bitmap_clear_bit_p (nonzero_property_spots, dest_spot);\n      } else {\n        bitmap_set_bit_p (nonzero_property_spots, dest_spot);\n        spot_attr.spot = dest_spot;\n        spot_attr.prop = (uint32_t) curr_insn->ops[1].u.i;\n        spot_attr.mem_ref = mem_spot_p (dest_spot) ? &curr_insn->ops[0] : NULL;\n        set_spot2attr (gen_ctx, &spot_attr);\n      }\n      skip_p = TRUE;\n      break;\n    case MIR_PRBEQ:\n    case MIR_PRBNE:\n      gen_assert (curr_insn->ops[2].mode == MIR_OP_INT || curr_insn->ops[2].mode == MIR_OP_UINT);\n      spot = op2spot (&curr_insn->ops[1]);\n      prop = 0;\n      if (bitmap_bit_p (nonzero_property_spots, spot)) {\n        spot_attr = VARR_GET (spot_attr_t, spot2attr, spot);\n        prop = spot_attr.prop;\n      }\n      if ((curr_insn->code == MIR_PRBEQ && curr_insn->ops[2].u.i != prop)\n          || (curr_insn->code == MIR_PRBNE && curr_insn->ops[2].u.i == prop)) {\n        DEBUG (2, {\n          fprintf (debug_file, \"  Remove property insn \");\n          MIR_output_insn (ctx, debug_file, curr_insn, curr_func_item->u.func, TRUE);\n        });\n        MIR_remove_insn (ctx, curr_func_item, curr_insn);\n        skip_p = TRUE;\n        break;\n      } else { /* make unconditional jump */\n        new_insn = MIR_new_insn (ctx, MIR_JMP, curr_insn->ops[0]);\n        MIR_insert_insn_before (ctx, curr_func_item, curr_insn, new_insn);\n        DEBUG (2, {\n          fprintf (debug_file, \"  Change \");\n          MIR_output_insn (ctx, debug_file, curr_insn, curr_func_item->u.func, FALSE);\n          fprintf (debug_file, \" to \");\n          MIR_output_insn (ctx, debug_file, new_insn, curr_func_item->u.func, TRUE);\n        });\n        MIR_remove_insn (ctx, curr_func_item, curr_insn);\n        next_insn = new_insn;\n        continue;\n      }\n    case MIR_MOV:\n    case MIR_FMOV:\n    case MIR_DMOV:\n    case MIR_LDMOV:\n      dest_spot = op2spot (&curr_insn->ops[0]);\n      src_spot = op2spot (&curr_insn->ops[1]);\n      if (src_spot == 0) {\n        bitmap_clear_bit_p (nonzero_property_spots, dest_spot);\n      } else if (dest_spot == 0) { /* clear attrs of all memory locations */\n        if (max_spot >= FIRST_MEM_SPOT)\n          bitmap_clear_bit_range_p (nonzero_property_spots, FIRST_MEM_SPOT,\n                                    max_spot - FIRST_MEM_SPOT + 1);\n      } else if (bitmap_bit_p (nonzero_property_spots, src_spot)) {\n        spot_attr = VARR_GET (spot_attr_t, spot2attr, src_spot);\n        spot_attr.mem_ref = NULL;\n        if (mem_spot_p (dest_spot)) {\n          spot_attr_t *spot_attr_addr = VARR_ADDR (spot_attr_t, spot_attrs);\n          for (spot = FIRST_MEM_SPOT; spot <= max_spot; spot++)\n            if (may_mem_alias_p (spot_attr_addr[dest_spot].mem_ref, spot_attr_addr[spot].mem_ref))\n              bitmap_clear_bit_p (nonzero_property_spots, spot);\n          spot_attr.mem_ref = &curr_insn->ops[0];\n        }\n        bitmap_set_bit_p (nonzero_property_spots, dest_spot);\n        spot_attr.spot = dest_spot;\n        set_spot2attr (gen_ctx, &spot_attr);\n      }\n      break;\n    default: break;\n    }\n    if (!skip_p) {\n      if (curr_insn->code != MIR_LADDR) {\n        target_bb_insn_translate (gen_ctx, curr_insn, NULL);\n      } else {\n        VARR_TRUNC (spot_attr_t, spot_attrs, 0);\n        FOREACH_BITMAP_BIT (bi, nonzero_property_spots, nel) {\n          VARR_PUSH (spot_attr_t, spot_attrs, VARR_GET (spot_attr_t, spot2attr, nel));\n        }\n        VARR_TRUNC (void_ptr_t, succ_bb_addrs, 0);\n        branch_bb_stub = curr_insn->ops[1].u.label->data;\n        (void) get_bb_version (gen_ctx, branch_bb_stub,\n                               (uint32_t) VARR_LENGTH (spot_attr_t, spot_attrs),\n                               VARR_ADDR (spot_attr_t, spot_attrs), FALSE, &addr);\n        VARR_PUSH (void_ptr_t, succ_bb_addrs, addr);\n        target_bb_insn_translate (gen_ctx, curr_insn, VARR_ADDR (void_ptr_t, succ_bb_addrs));\n      }\n    }\n    if (curr_insn == bb_stub->last_insn) break;\n  }\n  VARR_TRUNC (spot_attr_t, spot_attrs, 0);\n  DEBUG (2, {\n    fprintf (debug_file, \"  OUT BBStub%llx nonzero properties: \", (long long unsigned) bb_stub);\n  });\n  FOREACH_BITMAP_BIT (bi, nonzero_property_spots, nel) {\n    if (MIR_call_code_p (curr_insn->code) && mem_spot_p ((uint32_t) nel)) break;\n    spot_attr = VARR_GET (spot_attr_t, spot2attr, nel);\n    DEBUG (2, {\n      if (VARR_LENGTH (spot_attr_t, spot_attrs) != 0) fprintf (debug_file, \", \");\n      fprintf (debug_file, \"(spot=%u,prop=%u)\", (unsigned) spot_attr.spot,\n               (unsigned) spot_attr.prop);\n    });\n    VARR_PUSH (spot_attr_t, spot_attrs, spot_attr);\n  }\n  DEBUG (2, { fprintf (debug_file, \"\\n\"); });\n  VARR_TRUNC (void_ptr_t, succ_bb_addrs, 0);\n  if (curr_insn->code == MIR_JMPI) {\n    target_bb_insn_translate (gen_ctx, curr_insn, NULL);\n  } else if (curr_insn->code == MIR_SWITCH) {\n    for (size_t i = 1; i < curr_insn->nops; i++) {\n      branch_bb_stub = curr_insn->ops[i].u.label->data;\n      (void) get_bb_version (gen_ctx, branch_bb_stub,\n                             (uint32_t) VARR_LENGTH (spot_attr_t, spot_attrs),\n                             VARR_ADDR (spot_attr_t, spot_attrs), FALSE, &addr);\n      VARR_PUSH (void_ptr_t, succ_bb_addrs, addr);\n    }\n    target_bb_insn_translate (gen_ctx, curr_insn, VARR_ADDR (void_ptr_t, succ_bb_addrs));\n  } else if (MIR_branch_code_p (curr_insn->code)) {  // ??? generate branch\n    branch_bb_stub = curr_insn->ops[0].u.label->data;\n    (void) get_bb_version (gen_ctx, branch_bb_stub,\n                           (uint32_t) VARR_LENGTH (spot_attr_t, spot_attrs),\n                           VARR_ADDR (spot_attr_t, spot_attrs), FALSE, &addr);\n    VARR_PUSH (void_ptr_t, succ_bb_addrs, addr);\n    target_bb_insn_translate (gen_ctx, curr_insn, VARR_ADDR (void_ptr_t, succ_bb_addrs));\n  }\n  if (curr_insn->code != MIR_JMP && curr_insn->code != MIR_JMPI && curr_insn->code != MIR_SWITCH\n      && curr_insn->code != MIR_RET && curr_insn->code != MIR_JRET) {\n    VARR_TRUNC (void_ptr_t, succ_bb_addrs, 0);\n    (void) get_bb_version (gen_ctx, bb_stub + 1, (uint32_t) VARR_LENGTH (spot_attr_t, spot_attrs),\n                           VARR_ADDR (spot_attr_t, spot_attrs), FALSE, &addr);\n    VARR_PUSH (void_ptr_t, succ_bb_addrs, addr);\n    target_output_jump (gen_ctx, VARR_ADDR (void_ptr_t, succ_bb_addrs));\n  }\n  code = target_bb_translate_finish (gen_ctx, &code_len);\n  addr = _MIR_publish_code (ctx, code, code_len);\n  target_bb_rebase (gen_ctx, addr);\n  target_setup_succ_bb_version_data (gen_ctx, addr);\n  DEBUG (1, {\n    _MIR_dump_code (NULL, addr, code_len);\n    fprintf (debug_file, \"BBStub%llx code size = %lu:\\n\", (unsigned long long) bb_stub,\n             (unsigned long) code_len);\n  });\n  target_redirect_bb_origin_branch (gen_ctx, &bb_version->target_data, addr);\n  _MIR_replace_bb_thunk (ctx, bb_version->addr, addr);\n  bb_version->addr = addr;\n  overall_gen_bbs_num++;\n  bb_version->machine_code = addr;\n}\n\nstatic void *bb_version_generator (gen_ctx_t gen_ctx, bb_version_t bb_version) {\n  generate_bb_version_machine_code (gen_ctx, bb_version);\n  return bb_version->machine_code;\n}\n\n/* attrs ignored ??? implement versions */\nstatic void *generate_func_and_redirect_to_bb_gen (MIR_context_t ctx, MIR_item_t func_item) {\n  generate_func_and_redirect (ctx, func_item, FALSE);\n  return func_item->addr;\n}\n\nvoid MIR_set_lazy_bb_gen_interface (MIR_context_t ctx, MIR_item_t func_item) {\n  void *addr;\n\n  if (func_item == NULL) return; /* finish setting interfaces */\n  addr = _MIR_get_wrapper (ctx, func_item, generate_func_and_redirect_to_bb_gen);\n  _MIR_redirect_thunk (ctx, func_item->addr, addr);\n}\n\n/* Local Variables:                */\n/* mode: c                         */\n/* page-delimiter: \"/\\\\* New Page\" */\n/* End:                            */\n"
        },
        {
          "name": "mir-gen.h",
          "type": "blob",
          "size": 0.9287109375,
          "content": "/* This file is a part of MIR project.\n   Copyright (C) 2018-2024 Vladimir Makarov <vmakarov.gcc@gmail.com>.\n*/\n\n#ifndef MIR_GEN_H\n\n#define MIR_GEN_H\n\n#include \"mir.h\"\n\n#ifndef MIR_NO_GEN_DEBUG\n#define MIR_NO_GEN_DEBUG 0\n#endif\n\n#ifdef __cplusplus\nextern \"C\" {\n#endif\n\nextern void MIR_gen_init (MIR_context_t ctx);\nextern void MIR_gen_set_debug_file (MIR_context_t ctx, FILE *f);\nextern void MIR_gen_set_debug_level (MIR_context_t ctx, int debug_level);\nextern void MIR_gen_set_optimize_level (MIR_context_t ctx, unsigned int level);\nextern void *MIR_gen (MIR_context_t ctx, MIR_item_t func_item);\nextern void MIR_set_gen_interface (MIR_context_t ctx, MIR_item_t func_item);\nextern void MIR_set_lazy_gen_interface (MIR_context_t ctx, MIR_item_t func_item);\nextern void MIR_set_lazy_bb_gen_interface (MIR_context_t ctx, MIR_item_t func_item);\nextern void MIR_gen_finish (MIR_context_t ctx);\n\n#ifdef __cplusplus\n}\n#endif\n\n#endif /* #ifndef MIR_GEN_H */\n"
        },
        {
          "name": "mir-gen.svg",
          "type": "blob",
          "size": 22.3583984375,
          "content": "<?xml version=\"1.0\" encoding=\"UTF-8\"?><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"980.0\" height=\"520.0\" viewBox=\"-31.0 50.0 980.0 520.0\" version=\"1.1\">\n<rect x=\"-31.0\" y=\"50.0\" width=\"980.0\" height=\"520.0\" fill=\"rgb(255,255,255)\"/>\n<g id=\"s262578132\">\n<polygon id=\"s262578132\" points=\"180.00002,80.0 260.00006,80.0 260.00006,159.99992 180.00002,160.0 \" fill=\"rgb(220,237,200)\" stroke-width=\"2.0\" stroke=\"rgb(0,0,0)\"/>\n<g id=\"t262578132\">\n<text xml:space=\"preserve\" x=\"202.00003\" y=\"115.66406\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"16\">Build</text>\n<text xml:space=\"preserve\" x=\"205.00003\" y=\"136.89844\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"16\">CFG</text>\n</g>\n</g>\n<g id=\"s58925830\">\n<polygon id=\"s58925830\" points=\"1.5258789E-4,200.0 99.99992,200.0 99.99986,280.0 -1.296997E-4,279.99966 \" fill=\"rgb(144,202,249)\" stroke-width=\"2.0\" stroke=\"rgb(0,0,0)\"/>\n<g id=\"t58925830\">\n<text xml:space=\"preserve\" x=\"34.999897\" y=\"235.66406\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"16\">SSA</text>\n<text xml:space=\"preserve\" x=\"15.999897\" y=\"256.89844\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"16\">Combiner</text>\n</g>\n</g>\n<g id=\"s10278269\">\n<polygon id=\"s10278269\" points=\"300.0,80.0 379.99994,80.0 380.0,159.99982 300.0,160.0 \" fill=\"rgb(144,202,249)\" stroke-width=\"2.0\" stroke=\"rgb(0,0,0)\"/>\n<g id=\"t10278269\">\n<text xml:space=\"preserve\" x=\"322.0\" y=\"115.66406\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"16\">Build</text>\n<text xml:space=\"preserve\" x=\"325.0\" y=\"136.89844\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"16\">SSA</text>\n</g>\n</g>\n<g id=\"s125906034\">\n<polygon id=\"s125906034\" points=\"139.99998,320.0 280.0,320.0 280.0,380.0 139.99998,379.99985 \" fill=\"rgb(144,202,249)\" stroke-width=\"2.0\" stroke=\"rgb(0,0,0)\"/>\n<g id=\"t125906034\">\n<text xml:space=\"preserve\" x=\"189.5\" y=\"345.66406\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"16\">Jump</text>\n<text xml:space=\"preserve\" x=\"160.5\" y=\"366.89844\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"16\">Optimizations</text>\n</g>\n</g>\n<g id=\"s263641283\">\n<polygon id=\"s263641283\" points=\"320.0,320.0 420.0,320.0 420.0,380.0 320.0,379.9996 \" fill=\"rgb(220,237,200)\" stroke-width=\"2.0\" stroke=\"rgb(0,0,0)\"/>\n<g id=\"t263641283\">\n<text xml:space=\"preserve\" x=\"331.5\" y=\"356.28125\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"16\">Machinize </text>\n</g>\n</g>\n<g id=\"s12051776\">\n<polygon id=\"s12051776\" points=\"460.0,320.0 559.99994,320.0 560.0,379.99994 460.0,380.0 \" fill=\"rgb(220,237,200)\" stroke-width=\"2.0\" stroke=\"rgb(0,0,0)\"/>\n<g id=\"t12051776\">\n<text xml:space=\"preserve\" x=\"492.0\" y=\"345.66406\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"16\">Build</text>\n<text xml:space=\"preserve\" x=\"479.5\" y=\"366.89844\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"16\">Live Info</text>\n</g>\n</g>\n<g id=\"s94784633\">\n<polygon id=\"s94784633\" points=\"600.0,320.0 740.0,320.0 739.9991,379.9997 600.0,380.0 \" fill=\"rgb(144,202,249)\" stroke-width=\"2.0\" stroke=\"rgb(0,0,0)\"/>\n<g id=\"t94784633\">\n<text xml:space=\"preserve\" x=\"621.5\" y=\"345.66406\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"16\">Build Register</text>\n<text xml:space=\"preserve\" x=\"638.5\" y=\"366.89844\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"16\">Conflicts</text>\n</g>\n</g>\n<g id=\"s28417470\">\n<polygon id=\"s28417470\" points=\"800.0,420.0 900.0,420.0 899.9998,499.99854 800.0,500.0 \" fill=\"rgb(220,237,200)\" stroke-width=\"2.0\" stroke=\"rgb(0,0,0)\"/>\n<g id=\"t28417470\">\n<text xml:space=\"preserve\" x=\"821.5\" y=\"455.66406\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"16\">Register</text>\n<text xml:space=\"preserve\" x=\"818.5\" y=\"476.89844\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"16\">Allocator</text>\n</g>\n</g>\n<g id=\"s94649887\">\n<polygon id=\"s94649887\" points=\"380.0,420.0 480.0,420.0 479.9995,499.99988 380.0,500.0 \" fill=\"rgb(220,237,200)\" stroke-width=\"2.0\" stroke=\"rgb(0,0,0)\"/>\n<g id=\"t94649887\">\n<text xml:space=\"preserve\" x=\"398.5\" y=\"445.04688\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"16\">Generate</text>\n<text xml:space=\"preserve\" x=\"399.5\" y=\"466.28125\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"16\">Machine</text>\n<text xml:space=\"preserve\" x=\"412.0\" y=\"487.51562\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"16\">Code</text>\n</g>\n</g>\n<g id=\"s215356780\">\n<polygon id=\"s215356780\" points=\"-3.2123336E-13,100.0 39.999928,100.0 40.0,140.0 1.7794384E-5,139.99991 \" fill=\"none\" stroke-width=\"0.0\" stroke=\"rgb(0,0,0)\"/>\n<g id=\"t215356780\">\n<text xml:space=\"preserve\" x=\"6.0\" y=\"126.28125\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"16\">MIR</text>\n</g>\n</g>\n<g id=\"s255211317\">\n<polygon id=\"s255211317\" points=\"280.0,420.0 360.00012,420.0 360.00006,500.0 280.0,499.9998 \" fill=\"none\" stroke-width=\"0.0\" stroke=\"rgb(0,0,0)\"/>\n<g id=\"t255211317\">\n<text xml:space=\"preserve\" x=\"289.50006\" y=\"455.66406\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"16\">Machine</text>\n<text xml:space=\"preserve\" x=\"302.00006\" y=\"476.89844\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"16\">Code</text>\n</g>\n</g>\n<g id=\"s110515995\">\n<polygon id=\"s110515995\" points=\"420.0,80.0 519.99945,80.0 520.0,159.99997 420.0,160.0 \" fill=\"rgb(144,202,249)\" stroke-width=\"2.0\" stroke=\"rgb(0,0,0)\"/>\n<g id=\"t110515995\">\n<text xml:space=\"preserve\" x=\"441.5\" y=\"105.046875\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"16\">Address</text>\n<text xml:space=\"preserve\" x=\"437.5\" y=\"126.28125\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"16\">Transfor-</text>\n<text xml:space=\"preserve\" x=\"445.0\" y=\"147.51562\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"16\">mation</text>\n</g>\n</g>\n<g id=\"s71355690\">\n<polygon id=\"s71355690\" points=\"560.0,80.0 660.0,80.0 659.99976,160.0 560.0,159.99988 \" fill=\"rgb(144,202,249)\" stroke-width=\"2.0\" stroke=\"rgb(0,0,0)\"/>\n<g id=\"t71355690\">\n<text xml:space=\"preserve\" x=\"590.5\" y=\"115.66406\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"16\">Block</text>\n<text xml:space=\"preserve\" x=\"583.0\" y=\"136.89844\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"16\">Cloning</text>\n</g>\n</g>\n<g id=\"s138725192\">\n<polygon id=\"s138725192\" points=\"140.0,200.0 259.9997,200.0 260.0,279.99973 140.0,280.0 \" fill=\"rgb(144,202,249)\" stroke-width=\"2.0\" stroke=\"rgb(0,0,0)\"/>\n<g id=\"t138725192\">\n<text xml:space=\"preserve\" x=\"171.5\" y=\"225.04688\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"16\">Register</text>\n<text xml:space=\"preserve\" x=\"169.5\" y=\"246.28125\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"16\">Pressure</text>\n<text xml:space=\"preserve\" x=\"180.0\" y=\"267.51562\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"16\">Relief</text>\n</g>\n</g>\n<g id=\"s115726282\">\n<polygon id=\"s115726282\" points=\"60.0,100.0 139.99998,100.0 139.99998,140.0 60.0,139.99992 \" fill=\"rgb(220,237,200)\" stroke-width=\"2.0\" stroke=\"rgb(0,0,0)\"/>\n<g id=\"t115726282\">\n<text xml:space=\"preserve\" x=\"70.49999\" y=\"126.28125\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"16\">Simplify</text>\n</g>\n</g>\n<g id=\"s131636539\">\n<polygon id=\"s131636539\" points=\"780.0,320.0 920.0,320.0 919.99994,380.0 780.0,379.99997 \" fill=\"rgb(144,202,249)\" stroke-width=\"2.0\" stroke=\"rgb(0,0,0)\"/>\n<g id=\"t131636539\">\n<text xml:space=\"preserve\" x=\"818.0\" y=\"356.28125\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"16\">Coalesce</text>\n</g>\n</g>\n<g id=\"s169231814\">\n<polygon id=\"s169231814\" points=\"300.0,200.0 419.99915,200.0 420.0,279.99994 300.0,280.0 \" fill=\"rgb(144,202,249)\" stroke-width=\"2.0\" stroke=\"rgb(0,0,0)\"/>\n<g id=\"t169231814\">\n<text xml:space=\"preserve\" x=\"342.0\" y=\"225.04688\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"16\">Loop</text>\n<text xml:space=\"preserve\" x=\"329.0\" y=\"246.28125\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"16\">Invariant</text>\n<text xml:space=\"preserve\" x=\"335.0\" y=\"267.51562\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"16\">Motion</text>\n</g>\n</g>\n<g id=\"s99543830\">\n<polygon id=\"s99543830\" points=\"700.0,80.0 919.9999,80.0 920.0,159.99998 700.0,160.0 \" fill=\"rgb(144,202,249)\" stroke-width=\"2.0\" stroke=\"rgb(0,0,0)\"/>\n<g id=\"t99543830\">\n<text xml:space=\"preserve\" x=\"722.5\" y=\"105.046875\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"16\">Global Value Numbering,</text>\n<text xml:space=\"preserve\" x=\"729.0\" y=\"126.28125\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"16\">Constant  Propagation,</text>\n<text xml:space=\"preserve\" x=\"709.5\" y=\"147.51562\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"16\">Redundant Load Elimination</text>\n</g>\n</g>\n<g id=\"s84974694\">\n<polygon id=\"s84974694\" points=\"780.0,200.0 919.9994,200.0 920.0,280.0 780.0,279.9999 \" fill=\"rgb(144,202,249)\" stroke-width=\"2.0\" stroke=\"rgb(0,0,0)\"/>\n<g id=\"t84974694\">\n<text xml:space=\"preserve\" x=\"832.0\" y=\"235.66406\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"16\">Copy</text>\n<text xml:space=\"preserve\" x=\"806.5\" y=\"256.89844\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"16\">Propagation</text>\n</g>\n</g>\n<g id=\"s165697974\">\n<polygon id=\"s165697974\" points=\"620.0,200.0 740.0,200.0 739.9996,280.0 620.0,279.99976 \" fill=\"rgb(144,202,249)\" stroke-width=\"2.0\" stroke=\"rgb(0,0,0)\"/>\n<g id=\"t165697974\">\n<text xml:space=\"preserve\" x=\"661.5\" y=\"225.04688\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"16\">Dead</text>\n<text xml:space=\"preserve\" x=\"661.5\" y=\"246.28125\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"16\">Store</text>\n<text xml:space=\"preserve\" x=\"638.0\" y=\"267.51562\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"16\">Elimination </text>\n</g>\n</g>\n<g id=\"s61929953\">\n<polygon id=\"s61929953\" points=\"460.0,200.0 579.9998,200.0 580.0,280.0 460.0,279.99994 \" fill=\"rgb(144,202,249)\" stroke-width=\"2.0\" stroke=\"rgb(0,0,0)\"/>\n<g id=\"t61929953\">\n<text xml:space=\"preserve\" x=\"501.5\" y=\"225.04688\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"16\">Dead</text>\n<text xml:space=\"preserve\" x=\"502.0\" y=\"246.28125\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"16\">Code</text>\n<text xml:space=\"preserve\" x=\"478.0\" y=\"267.51562\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"16\">Elimination </text>\n</g>\n</g>\n<g id=\"s231021656\">\n<polygon id=\"s231021656\" points=\"-2.0872946E-8,320.0 99.99983,320.0 99.99995,379.99994 5.66234E-6,380.0 \" fill=\"rgb(144,202,249)\" stroke-width=\"2.0\" stroke=\"rgb(0,0,0)\"/>\n<g id=\"t231021656\">\n<text xml:space=\"preserve\" x=\"27.999973\" y=\"345.66406\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"16\">Out of</text>\n<text xml:space=\"preserve\" x=\"34.999973\" y=\"366.89844\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"16\">SSA</text>\n</g>\n</g>\n<g id=\"s262148529\">\n<polygon id=\"s262148529\" points=\"119.99999,420.0 219.99863,420.0 219.99997,539.99347 119.99999,540.0 \" fill=\"none\" stroke-width=\"0.0\" stroke=\"rgb(0,0,0)\"/>\n<g id=\"t262148529\">\n<text xml:space=\"preserve\" x=\"125.0\" y=\"443.8125\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"16\">-O0</text>\n<text xml:space=\"preserve\" x=\"125.0\" y=\"465.04688\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"16\">-O1</text>\n<text xml:space=\"preserve\" x=\"125.0\" y=\"486.28125\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"16\">-O2 default</text>\n<text xml:space=\"preserve\" x=\"125.0\" y=\"507.51562\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"16\">-O3</text>\n<text xml:space=\"preserve\" x=\"125.0\" y=\"528.75\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"16\"/>\n</g>\n</g>\n<g id=\"s208774806\">\n<polygon id=\"s208774806\" points=\"0.0,420.0 99.998085,420.0 99.999794,540.0 0.0,539.9997 \" fill=\"none\" stroke-width=\"0.0\" stroke=\"rgb(0,0,0)\"/>\n<g id=\"t208774806\">\n<text xml:space=\"preserve\" x=\"0.499897\" y=\"454.4297\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"16\">Optimizations</text>\n<text xml:space=\"preserve\" x=\"27.999897\" y=\"475.66406\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"16\">added</text>\n<text xml:space=\"preserve\" x=\"21.999897\" y=\"496.89844\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"16\">on each</text>\n<text xml:space=\"preserve\" x=\"31.999897\" y=\"518.1328\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"16\">level:</text>\n</g>\n</g>\n<g id=\"s767015\">\n<polygon id=\"s767015\" points=\"660.0,420.0 759.99976,420.0 760.0,499.9999 660.0,500.0 \" fill=\"rgb(178,235,242)\" stroke-width=\"2.0\" stroke=\"rgb(0,0,0)\"/>\n<g id=\"t767015\">\n<text xml:space=\"preserve\" x=\"678.5\" y=\"466.28125\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"16\">Combine</text>\n</g>\n</g>\n<g id=\"s219239269\">\n<polygon id=\"s219239269\" points=\"520.0,420.0 620.0,420.0 620.0,500.0 520.0,499.99963 \" fill=\"rgb(178,235,242)\" stroke-width=\"2.0\" stroke=\"rgb(0,0,0)\"/>\n<g id=\"t219239269\">\n<text xml:space=\"preserve\" x=\"551.5\" y=\"445.04688\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"16\">Dead</text>\n<text xml:space=\"preserve\" x=\"552.0\" y=\"466.28125\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"16\">Code</text>\n<text xml:space=\"preserve\" x=\"530.0\" y=\"487.51562\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"16\">Elimination</text>\n</g>\n</g>\n<g id=\"la187290135\">\n<path id=\"s187290135\" d=\"M 139.99998,120.0 180.00002,120.0 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(0,0,0)\"/>\n<path id=\"s92761995\" d=\"M 170.82191,127.73061 180.00002,120.0 170.82191,112.26939 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(0,0,0)\"/>\n<g id=\"t187290135\">\n<text xml:space=\"preserve\" x=\"160.0\" y=\"112.206055\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"14\"/>\n</g>\n</g>\n<g id=\"la71365580\">\n<path id=\"s71365580\" d=\"M 519.99976,119.999985 560.0,119.99994 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(0,0,0)\"/>\n<path id=\"s225997953\" d=\"M 550.8219,127.73056 560.0,119.99994 550.8219,112.26933 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(0,0,0)\"/>\n<g id=\"t71365580\">\n<text xml:space=\"preserve\" x=\"539.9999\" y=\"112.20602\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"14\"/>\n</g>\n</g>\n<g id=\"la60532228\">\n<path id=\"s60532228\" d=\"M 379.99997,119.99991 420.0,120.0 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(0,0,0)\"/>\n<path id=\"s210704678\" d=\"M 410.82187,127.73059 420.0,120.0 410.8219,112.26936 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(0,0,0)\"/>\n<g id=\"t60532228\">\n<text xml:space=\"preserve\" x=\"400.0\" y=\"112.20601\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"14\"/>\n</g>\n</g>\n<g id=\"la15931373\">\n<path id=\"s15931373\" d=\"M 260.00006,119.99996 300.0,120.0 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(0,0,0)\"/>\n<path id=\"s201029991\" d=\"M 290.8219,127.730606 300.0,120.0 290.8219,112.26938 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(0,0,0)\"/>\n<g id=\"t15931373\">\n<text xml:space=\"preserve\" x=\"280.00003\" y=\"111.66405\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"16\"/>\n</g>\n</g>\n<g id=\"la88931521\">\n<path id=\"s88931521\" d=\"M 739.99976,240.0 780.0,239.99995 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(0,0,0)\"/>\n<path id=\"s96092948\" d=\"M 749.17786,232.26938 739.99976,240.0 749.17786,247.7306 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(0,0,0)\"/>\n<g id=\"t88931521\">\n<text xml:space=\"preserve\" x=\"759.9999\" y=\"231.66403\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"16\"/>\n</g>\n</g>\n<g id=\"la215686481\">\n<path id=\"s215686481\" d=\"M 620.0,239.99988 579.9999,240.0 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(0,0,0)\"/>\n<path id=\"s104257469\" d=\"M 589.178,232.26936 579.9999,240.0 589.178,247.73059 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(0,0,0)\"/>\n<g id=\"t215686481\">\n<text xml:space=\"preserve\" x=\"599.99994\" y=\"231.664\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"16\"/>\n</g>\n</g>\n<g id=\"la119117346\">\n<path id=\"s119117346\" d=\"M 280.0,350.0 320.0,349.99982 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(0,0,0)\"/>\n<path id=\"s71541170\" d=\"M 310.82193,357.73047 320.0,349.99982 310.82187,342.26926 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(0,0,0)\"/>\n<g id=\"t119117346\">\n<text xml:space=\"preserve\" x=\"300.0\" y=\"341.66397\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"16\"/>\n</g>\n</g>\n<g id=\"la32471219\">\n<path id=\"s32471219\" d=\"M 420.0,350.0 460.0,350.0 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(0,0,0)\"/>\n<path id=\"s230442243\" d=\"M 450.8219,357.73062 460.0,350.0 450.8219,342.26938 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(0,0,0)\"/>\n<g id=\"t32471219\">\n<text xml:space=\"preserve\" x=\"440.0\" y=\"341.66406\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"16\"/>\n</g>\n</g>\n<g id=\"la120528496\">\n<path id=\"s120528496\" d=\"M 600.0,350.0 560.0,349.99997 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(0,0,0)\"/>\n<path id=\"s159546496\" d=\"M 590.8219,357.7306 600.0,350.0 590.8219,342.26938 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(0,0,0)\"/>\n<g id=\"t120528496\">\n<text xml:space=\"preserve\" x=\"580.0\" y=\"341.66406\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"16\"/>\n</g>\n</g>\n<g id=\"la83248617\">\n<path id=\"s83248617\" d=\"M 850.0,420.0 850.0,380.0 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(0,0,0)\"/>\n<path id=\"s159265465\" d=\"M 842.2694,410.8219 850.0,420.0 857.7306,410.8219 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(0,0,0)\"/>\n<g id=\"t83248617\">\n<text xml:space=\"preserve\" x=\"854.0\" y=\"406.28125\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"16\"/>\n</g>\n</g>\n<g id=\"la22999662\">\n<path id=\"s22999662\" d=\"M 780.0,350.00003 739.9995,349.99985 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(0,0,0)\"/>\n<path id=\"s38151934\" d=\"M 770.82184,357.7306 780.0,350.00003 770.8219,342.26938 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(0,0,0)\"/>\n<g id=\"t22999662\">\n<text xml:space=\"preserve\" x=\"759.99976\" y=\"341.664\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"16\"/>\n</g>\n</g>\n<g id=\"la121614748\">\n<path id=\"s121614748\" d=\"M 360.0001,460.0 380.0,460.0 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(0,0,0)\"/>\n<path id=\"s238300767\" d=\"M 369.1782,452.26938 360.0001,460.0 369.1782,467.73062 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(0,0,0)\"/>\n<g id=\"t121614748\">\n<text xml:space=\"preserve\" x=\"370.00006\" y=\"451.66406\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"16\"/>\n</g>\n</g>\n<g id=\"la176932730\">\n<path id=\"s176932730\" d=\"M 99.99989,349.99997 139.99998,349.99994 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(0,0,0)\"/>\n<path id=\"s248701100\" d=\"M 130.82188,357.73056 139.99998,349.99994 130.82187,342.26935 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(0,0,0)\"/>\n<g id=\"t176932730\">\n<text xml:space=\"preserve\" x=\"119.99994\" y=\"341.66403\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"16\"/>\n</g>\n</g>\n<g id=\"la93412843\">\n<path id=\"s93412843\" d=\"M 140.0,240.0 99.99989,240.0 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(0,0,0)\"/>\n<path id=\"s21381493\" d=\"M 109.178,232.2694 99.99989,240.0 109.178,247.7306 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(0,0,0)\"/>\n<g id=\"t93412843\">\n<text xml:space=\"preserve\" x=\"119.99995\" y=\"231.66406\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"16\"/>\n</g>\n</g>\n<g id=\"la111627051\">\n<path id=\"s111627051\" d=\"M 39.99996,120.0 60.0,119.99996 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(0,0,0)\"/>\n<path id=\"s140096778\" d=\"M 50.821907,127.73059 60.0,119.99996 50.82188,112.26936 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(0,0,0)\"/>\n<g id=\"t111627051\">\n<text xml:space=\"preserve\" x=\"49.99998\" y=\"111.66404\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"16\"/>\n</g>\n</g>\n<g id=\"la207899528\">\n<path id=\"s207899528\" d=\"M 99.99894,480.0 119.99999,480.0 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(144,202,249)\"/>\n<g id=\"t207899528\">\n<text xml:space=\"preserve\" x=\"109.999466\" y=\"471.66406\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"16\"/>\n</g>\n</g>\n<g id=\"la149408033\">\n<path id=\"s149408033\" d=\"M 99.99866,460.0 119.99999,460.0 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(178,235,242)\"/>\n<g id=\"t149408033\">\n<text xml:space=\"preserve\" x=\"109.99933\" y=\"451.66406\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"16\"/>\n</g>\n</g>\n<g id=\"la176087366\">\n<path id=\"s176087366\" d=\"M 99.99837,440.0 119.99999,440.0 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(220,237,200)\"/>\n<g id=\"t176087366\">\n<text xml:space=\"preserve\" x=\"109.999176\" y=\"431.66406\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"16\"/>\n</g>\n</g>\n<g id=\"la47017735\">\n<path id=\"s47017735\" d=\"M 99.99922,500.0 119.99999,500.0 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(209,196,233)\"/>\n<g id=\"t47017735\">\n<text xml:space=\"preserve\" x=\"109.9996\" y=\"491.66406\" font-family=\"Sans,Helvetica\" fill=\"rgb(0,0,0)\" font-size=\"16\"/>\n</g>\n</g>\n<g id=\"la39107872\">\n<path id=\"s39107872\" d=\"M 49.999866,279.99982 49.999916,320.0 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(0,0,0)\"/>\n<path id=\"s51859835\" d=\"M 42.269295,310.8219 49.999916,320.0 57.73052,310.8219 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(0,0,0)\"/>\n</g>\n<g id=\"la108295897\">\n<path id=\"s108295897\" d=\"M 759.9999,459.99994 800.0,460.0 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(0,0,0)\"/>\n<path id=\"s199449496\" d=\"M 769.178,452.26935 759.9999,459.99994 769.178,467.73056 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(0,0,0)\"/>\n</g>\n<g id=\"la10676382\">\n<path id=\"s10676382\" d=\"M 620.0,460.0 660.0,460.0 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(0,0,0)\"/>\n<path id=\"s88506353\" d=\"M 629.1781,452.26938 620.0,460.0 629.1781,467.73062 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(0,0,0)\"/>\n</g>\n<g id=\"la7430015\">\n<path id=\"s7430015\" d=\"M 479.99976,459.99994 520.0,459.99982 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(0,0,0)\"/>\n<path id=\"s258349014\" d=\"M 489.17783,452.2693 479.99976,459.99994 489.1779,467.73053 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(0,0,0)\"/>\n</g>\n<g id=\"la172587755\">\n<path id=\"s172587755\" d=\"M 850.0,160.0 849.9997,200.0 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(0,0,0)\"/>\n<path id=\"s255822423\" d=\"M 842.26917,190.82184 849.9997,200.0 857.73035,190.82195 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(0,0,0)\"/>\n</g>\n<g id=\"la257854866\">\n<path id=\"s257854866\" d=\"M 659.9999,120.0 700.0,120.0 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(0,0,0)\"/>\n<path id=\"s8296772\" d=\"M 690.8219,127.73061 700.0,120.0 690.8219,112.26939 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(0,0,0)\"/>\n</g>\n<g id=\"la189854563\">\n<path id=\"s189854563\" d=\"M 419.99957,239.99997 460.0,239.99997 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(0,0,0)\"/>\n<path id=\"s120728109\" d=\"M 429.17767,232.26936 419.99957,239.99997 429.17767,247.73058 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(0,0,0)\"/>\n</g>\n<g id=\"la221379424\">\n<path id=\"s221379424\" d=\"M 259.99985,239.99986 300.0,240.0 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(0,0,0)\"/>\n<path id=\"s84138850\" d=\"M 269.17798,232.26929 259.99985,239.99986 269.17792,247.7305 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(0,0,0)\"/>\n</g>\n</svg>\n"
        },
        {
          "name": "mir-hash.h",
          "type": "blob",
          "size": 3.3984375,
          "content": "/* This file is a part of MIR project.\n\n   Copyright (C) 2018-2024 Vladimir Makarov <vmakarov.gcc@gmail.com>.\n*/\n\n/* Simple high-quality multiplicative hash passing demerphq-smhasher,\n   faster than spooky, city, or xxhash for strings less 100 bytes.\n   Hash for the same key can be different on different architectures.\n   To get machine-independent hash, use mir_hash_strict which is about\n   1.5 times slower than mir_hash.  */\n#ifndef __MIR_HASH__\n#define __MIR_HASH__\n\n#include <stddef.h>\n#include <stdint.h>\n\n#if defined(__x86_64__) || defined(__i386__) || defined(__PPC64__) || defined(__s390__) \\\n  || defined(__m32c__) || defined(cris) || defined(__CR16__) || defined(__vax__)        \\\n  || defined(__m68k__) || defined(__aarch64__) || defined(_M_AMD64) || defined(_M_IX86)\n#define MIR_HASH_UNALIGNED_ACCESS 1\n#else\n#define MIR_HASH_UNALIGNED_ACCESS 0\n#endif\n\n#if (defined(__BYTE_ORDER__) && (__BYTE_ORDER__ == __ORDER_LITTLE_ENDIAN__)) || defined(_MSC_VER)\n#define MIR_LITTLE_ENDIAN 1\n#else\n#define MIR_LITTLE_ENDIAN 0\n#endif\n\nstatic inline uint64_t mir_get_key_part (const uint8_t *v, size_t len, int relax_p) {\n  size_t i, start = 0;\n  uint64_t tail = 0;\n\n  if (relax_p || MIR_LITTLE_ENDIAN) {\n#if MIR_HASH_UNALIGNED_ACCESS\n    if (len == sizeof (uint64_t)) return *(uint64_t *) v;\n    if (len >= sizeof (uint32_t)) {\n      tail = (uint64_t) * (uint32_t *) v << 32;\n      start = 4;\n    }\n#endif\n  }\n  for (i = start; i < len; i++) tail = (tail >> 8) | ((uint64_t) v[i] << 56);\n  return tail;\n}\n\nstatic const uint64_t mir_hash_p1 = 0X65862b62bdf5ef4d, mir_hash_p2 = 0X288eea216831e6a7;\nstatic inline uint64_t mir_mum (uint64_t v, uint64_t c, int relax_p) {\n  if (relax_p) {\n#if defined(__SIZEOF_INT128__)\n    __uint128_t r = (__uint128_t) v * (__uint128_t) c;\n    return (uint64_t) (r >> 64) + (uint64_t) r;\n#endif\n  }\n  uint64_t v1 = v >> 32, v2 = (uint32_t) v, c1 = c >> 32, c2 = (uint32_t) c, rm = v2 * c1 + v1 * c2;\n  return v1 * c1 + (rm >> 32) + v2 * c2 + (rm << 32);\n}\n\nstatic inline uint64_t mir_round (uint64_t state, uint64_t v, int relax_p) {\n  state ^= mir_mum (v, mir_hash_p1, relax_p);\n  return state ^ mir_mum (state, mir_hash_p2, relax_p);\n}\n\nstatic inline uint64_t mir_hash_1 (const void *key, size_t len, uint64_t seed, int relax_p) {\n  const uint8_t *v = (const uint8_t *) key;\n  uint64_t r = seed + len;\n\n  for (; len >= 16; len -= 16, v += 16) {\n    r ^= mir_mum (mir_get_key_part (v, 8, relax_p), mir_hash_p1, relax_p);\n    r ^= mir_mum (mir_get_key_part (v + 8, 8, relax_p), mir_hash_p2, relax_p);\n    r ^= mir_mum (r, mir_hash_p1, relax_p);\n  }\n  if (len >= 8) {\n    r ^= mir_mum (mir_get_key_part (v, 8, relax_p), mir_hash_p1, relax_p);\n    len -= 8, v += 8;\n  }\n  if (len != 0) r ^= mir_mum (mir_get_key_part (v, len, relax_p), mir_hash_p2, relax_p);\n  return mir_round (r, r, relax_p);\n}\n\nstatic inline uint64_t mir_hash (const void *key, size_t len, uint64_t seed) {\n  return mir_hash_1 (key, len, seed, 1);\n}\n\nstatic inline uint64_t mir_hash_strict (const void *key, size_t len, uint64_t seed) {\n  return mir_hash_1 (key, len, seed, 0);\n}\n\nstatic inline uint64_t mir_hash_init (uint64_t seed) { return seed; }\nstatic inline uint64_t mir_hash_step (uint64_t h, uint64_t key) { return mir_round (h, key, 1); }\nstatic inline uint64_t mir_hash_finish (uint64_t h) { return mir_round (h, h, 1); }\n\nstatic inline uint64_t mir_hash64 (uint64_t key, uint64_t seed) {\n  return mir_hash_finish (mir_hash_step (mir_hash_init (seed), key));\n}\n\n#endif\n"
        },
        {
          "name": "mir-htab.h",
          "type": "blob",
          "size": 17.7080078125,
          "content": "/* This file is a part of MIR project.\n   Copyright (C) 2018-2024 Vladimir Makarov <vmakarov.gcc@gmail.com>.\n*/\n\n#ifndef MIR_HTAB_H\n#define MIR_HTAB_H\n\n#include \"mir-alloc.h\"\n#include \"mir-varr.h\"\n\n#ifdef __cplusplus\nextern \"C\" {\n#endif\n\n#define FALSE 0\n#define TRUE 1\n\n#if !defined(VARR_ENABLE_CHECKING) && !defined(NDEBUG)\n#define VARR_ENABLE_CHECKING\n#endif\n\n#ifndef HTAB_ENABLE_CHECKING\n#define HTAB_ASSERT(EXPR, OP, T) ((void) (EXPR))\n\n#else\nstatic inline void mir_htab_assert_fail (const char *op, const char *var) {\n  fprintf (stderr, \"wrong %s for %s\\n\", op, var);\n  assert (0);\n}\n\n#define HTAB_ASSERT(EXPR, OP, T) (void) ((EXPR) ? 0 : (mir_htab_assert_fail (OP, #T), 0))\n\n#endif\n\n#ifdef __GNUC__\n#define MIR_HTAB_UNUSED __attribute__ ((unused))\n#define MIR_HTAB_NO_RETURN __attribute__ ((noreturn))\n#else\n#define MIR_HTAB_UNUSED\n#define MIR_HTAB_NO_RETURN\n#endif\n\nstatic inline void MIR_HTAB_NO_RETURN mir_htab_error (const char *message) {\n#ifdef MIR_HTAB_ERROR\n  MIR_HTAB_ERROR (message);\n  assert (FALSE);\n#else\n  fprintf (stderr, \"%s\\n\", message);\n#endif\n  exit (1);\n}\n\n/*---------------- Typed hash table -----------------------------*/\ntypedef unsigned htab_ind_t;\ntypedef unsigned htab_size_t;\ntypedef unsigned htab_hash_t;\n\n#define HTAB_EMPTY_IND (~(htab_ind_t) 0)\n#define HTAB_DELETED_IND (HTAB_EMPTY_IND - 1)\n#define HTAB_DELETED_HASH 0\n\nenum htab_action { HTAB_FIND, HTAB_INSERT, HTAB_REPLACE, HTAB_DELETE };\n\n#define HTAB(T) HTAB_##T\n#define HTAB_OP(T, OP) HTAB_##T##_##OP\n#define HTAB_OP_DEF(T, OP) MIR_HTAB_UNUSED HTAB_OP (T, OP)\n\nDEF_VARR (htab_ind_t)\n\n#define HTAB_EL(T) HTAB_EL_##T\n\n#define HTAB_T(T)                                          \\\n  typedef struct HTAB_EL (T) {                             \\\n    htab_hash_t hash;                                      \\\n    T el;                                                  \\\n  } HTAB_EL (T);                                           \\\n  DEF_VARR (HTAB_EL (T))                                   \\\n  typedef struct {                                         \\\n    htab_size_t els_num, els_start, els_bound, collisions; \\\n    void *arg;                                             \\\n    htab_hash_t (*hash_func) (T el, void *arg);            \\\n    int (*eq_func) (T el1, T el2, void *arg);              \\\n    void (*free_func) (T el, void *arg);                   \\\n    VARR (HTAB_EL (T)) * els;                              \\\n    VARR (htab_ind_t) * entries;                           \\\n  } HTAB (T);\n\n#define DEF_HTAB(T)                                                                              \\\n  HTAB_T (T)                                                                                     \\\n                                                                                                 \\\n  static inline void HTAB_OP_DEF (T, create) (HTAB (T) * *htab, MIR_alloc_t alloc,               \\\n                                              htab_size_t min_size,                              \\\n                                              htab_hash_t (*hash_func) (T el, void *arg),        \\\n                                              int (*eq_func) (T el1, T el2, void *arg),          \\\n                                              void (*free_func) (T el, void *arg), void *arg) {  \\\n    HTAB (T) * ht;                                                                               \\\n    htab_size_t i, size;                                                                         \\\n                                                                                                 \\\n    for (size = 2; min_size > size; size *= 2)                                                   \\\n      ;                                                                                          \\\n    ht = MIR_malloc (alloc, sizeof (*ht));                                                       \\\n    if (ht == NULL) mir_htab_error (\"htab: no memory\");                                          \\\n    VARR_CREATE (HTAB_EL (T), ht->els, alloc, size);                                             \\\n    VARR_TAILOR (HTAB_EL (T), ht->els, size);                                                    \\\n    VARR_CREATE (htab_ind_t, ht->entries, alloc, 2 * size);                                      \\\n    ht->arg = arg;                                                                               \\\n    ht->hash_func = hash_func;                                                                   \\\n    ht->eq_func = eq_func;                                                                       \\\n    ht->free_func = free_func;                                                                   \\\n    ht->els_num = ht->els_start = ht->els_bound = ht->collisions = 0;                            \\\n    for (i = 0; i < 2 * size; i++) VARR_PUSH (htab_ind_t, ht->entries, HTAB_EMPTY_IND);          \\\n    *htab = ht;                                                                                  \\\n  }                                                                                              \\\n                                                                                                 \\\n  static inline void HTAB_OP_DEF (T, clear) (HTAB (T) * htab) {                                  \\\n    htab_ind_t *addr;                                                                            \\\n    htab_size_t i, size;                                                                         \\\n    HTAB_EL (T) * els_addr;                                                                      \\\n    void *arg;                                                                                   \\\n                                                                                                 \\\n    HTAB_ASSERT (htab != NULL, \"clear\", T);                                                      \\\n    arg = htab->arg;                                                                             \\\n    if (htab->free_func != NULL) {                                                               \\\n      els_addr = VARR_ADDR (HTAB_EL (T), htab->els);                                             \\\n      size = (htab_size_t) VARR_LENGTH (HTAB_EL (T), htab->els);                                 \\\n      for (i = 0; i < htab->els_bound; i++)                                                      \\\n        if (els_addr[i].hash != HTAB_DELETED_HASH) htab->free_func (els_addr[i].el, arg);        \\\n    }                                                                                            \\\n    htab->els_num = htab->els_start = htab->els_bound = 0;                                       \\\n    addr = VARR_ADDR (htab_ind_t, htab->entries);                                                \\\n    size = (htab_size_t) VARR_LENGTH (htab_ind_t, htab->entries);                                \\\n    for (i = 0; i < size; i++) addr[i] = HTAB_EMPTY_IND;                                         \\\n  }                                                                                              \\\n                                                                                                 \\\n  static inline void HTAB_OP_DEF (T, destroy) (HTAB (T) * *htab) {                               \\\n    HTAB_ASSERT (*htab != NULL, \"destroy\", T);                                                   \\\n    if ((*htab)->free_func != NULL) HTAB_OP (T, clear) (*htab);                                  \\\n    MIR_alloc_t alloc = (*htab)->els->alloc;                                                     \\\n    VARR_DESTROY (HTAB_EL (T), (*htab)->els);                                                    \\\n    VARR_DESTROY (htab_ind_t, (*htab)->entries);                                                 \\\n    MIR_free (alloc, *htab);                                                                     \\\n    *htab = NULL;                                                                                \\\n  }                                                                                              \\\n                                                                                                 \\\n  static inline int HTAB_OP_DEF (T, do) (HTAB (T) * htab, T el, enum htab_action action,         \\\n                                         T * res) {                                              \\\n    htab_ind_t ind, el_ind, *entry, *first_deleted_entry = NULL;                                 \\\n    htab_hash_t hash, peterb;                                                                    \\\n    htab_size_t els_size, size, mask, start, bound, i;                                           \\\n    htab_ind_t *addr;                                                                            \\\n    HTAB_EL (T) * els_addr;                                                                      \\\n    void *arg;                                                                                   \\\n                                                                                                 \\\n    HTAB_ASSERT (htab != NULL, \"do htab\", T);                                                    \\\n    size = (htab_size_t) VARR_LENGTH (htab_ind_t, htab->entries);                                \\\n    els_size = (htab_size_t) VARR_LENGTH (HTAB_EL (T), htab->els);                               \\\n    arg = htab->arg;                                                                             \\\n    HTAB_ASSERT (els_size * 2 == size, \"do size\", T);                                            \\\n    if ((action == HTAB_INSERT || action == HTAB_REPLACE) && htab->els_bound == els_size) {      \\\n      size *= 2;                                                                                 \\\n      VARR_TAILOR (htab_ind_t, htab->entries, size);                                             \\\n      addr = VARR_ADDR (htab_ind_t, htab->entries);                                              \\\n      for (i = 0; i < size; i++) addr[i] = HTAB_EMPTY_IND;                                       \\\n      VARR_TAILOR (HTAB_EL (T), htab->els, els_size * 2);                                        \\\n      els_addr = VARR_ADDR (HTAB_EL (T), htab->els);                                             \\\n      start = htab->els_start;                                                                   \\\n      bound = htab->els_bound;                                                                   \\\n      htab->els_start = htab->els_bound = htab->els_num = 0;                                     \\\n      for (i = start; i < bound; i++)                                                            \\\n        if (els_addr[i].hash != HTAB_DELETED_HASH) {                                             \\\n          HTAB_OP (T, do) (htab, els_addr[i].el, HTAB_INSERT, res);                              \\\n          HTAB_ASSERT ((*htab->eq_func) (*res, els_addr[i].el, arg), \"do expand\", T);            \\\n        }                                                                                        \\\n      HTAB_ASSERT (bound - start >= htab->els_bound, \"do bound\", T);                             \\\n    }                                                                                            \\\n    mask = size - 1;                                                                             \\\n    hash = (*htab->hash_func) (el, arg);                                                         \\\n    if (hash == HTAB_DELETED_HASH) hash += 1;                                                    \\\n    peterb = hash;                                                                               \\\n    ind = hash & mask;                                                                           \\\n    addr = VARR_ADDR (htab_ind_t, htab->entries);                                                \\\n    els_addr = VARR_ADDR (HTAB_EL (T), htab->els);                                               \\\n    for (;; htab->collisions++) {                                                                \\\n      entry = addr + ind;                                                                        \\\n      el_ind = *entry;                                                                           \\\n      if (el_ind != HTAB_EMPTY_IND) {                                                            \\\n        if (el_ind == HTAB_DELETED_IND) {                                                        \\\n          first_deleted_entry = entry;                                                           \\\n        } else if (els_addr[el_ind].hash == hash                                                 \\\n                   && (*htab->eq_func) (els_addr[el_ind].el, el, arg)) {                         \\\n          if (action == HTAB_REPLACE) {                                                          \\\n            if (htab->free_func != NULL) htab->free_func (els_addr[el_ind].el, arg);             \\\n            els_addr[el_ind].el = el;                                                            \\\n          }                                                                                      \\\n          if (action != HTAB_DELETE) {                                                           \\\n            *res = els_addr[el_ind].el;                                                          \\\n          } else {                                                                               \\\n            htab->els_num--;                                                                     \\\n            *entry = HTAB_DELETED_IND;                                                           \\\n            if (htab->free_func != NULL) htab->free_func (els_addr[el_ind].el, arg);             \\\n            els_addr[el_ind].hash = HTAB_DELETED_HASH;                                           \\\n          }                                                                                      \\\n          return TRUE;                                                                           \\\n        }                                                                                        \\\n      } else {                                                                                   \\\n        if (action == HTAB_INSERT || action == HTAB_REPLACE) {                                   \\\n          htab->els_num++;                                                                       \\\n          if (first_deleted_entry != NULL) entry = first_deleted_entry;                          \\\n          els_addr[htab->els_bound].hash = hash;                                                 \\\n          els_addr[htab->els_bound].el = el;                                                     \\\n          *entry = htab->els_bound++;                                                            \\\n          *res = el;                                                                             \\\n        }                                                                                        \\\n        return FALSE;                                                                            \\\n      }                                                                                          \\\n      peterb >>= 11;                                                                             \\\n      ind = (5 * ind + peterb + 1) & mask;                                                       \\\n    }                                                                                            \\\n  }                                                                                              \\\n                                                                                                 \\\n  static inline htab_size_t HTAB_OP_DEF (T, els_num) (HTAB (T) * htab) {                         \\\n    HTAB_ASSERT (htab != NULL, \"els_num\", T);                                                    \\\n    return htab->els_num;                                                                        \\\n  }                                                                                              \\\n  static inline htab_size_t HTAB_OP_DEF (T, collisions) (HTAB (T) * htab) {                      \\\n    HTAB_ASSERT (htab != NULL, \"collisions\", T);                                                 \\\n    return htab->collisions;                                                                     \\\n  }                                                                                              \\\n                                                                                                 \\\n  static inline void HTAB_OP_DEF (T, foreach_elem) (HTAB (T) * htab,                             \\\n                                                    void (*func) (T el, void *arg), void *arg) { \\\n    htab_size_t i;                                                                               \\\n    HTAB_EL (T) * els_addr;                                                                      \\\n                                                                                                 \\\n    HTAB_ASSERT (htab != NULL, \"foreach_elem\", T);                                               \\\n    els_addr = VARR_ADDR (HTAB_EL (T), htab->els);                                               \\\n    for (i = 0; i < htab->els_bound; i++)                                                        \\\n      if (els_addr[i].hash != HTAB_DELETED_HASH) func (els_addr[i].el, arg);                     \\\n  }\n\n#define HTAB_CREATE(T, V, M, S, H, EQ, A) (HTAB_OP (T, create) (&(V), M, S, H, EQ, NULL, A))\n#define HTAB_CREATE_WITH_FREE_FUNC(T, V, M, S, H, EQ, F, A) \\\n  (HTAB_OP (T, create) (&(V), M, S, H, EQ, F, A))\n#define HTAB_CLEAR(T, V) (HTAB_OP (T, clear) (V))\n#define HTAB_DESTROY(T, V) (HTAB_OP (T, destroy) (&(V)))\n/* It returns TRUE if the element existed in the table.  */\n#define HTAB_DO(T, V, EL, A, TAB_EL) (HTAB_OP (T, do) (V, EL, A, &(TAB_EL)))\n#define HTAB_ELS_NUM(T, V) (HTAB_OP (T, els_num) (V))\n#define HTAB_COLLISIONS(T, V) (HTAB_OP (T, collisions) (V))\n#define HTAB_FOREACH_ELEM(T, V, F, A) (HTAB_OP (T, foreach_elem) (V, F, A))\n\n#ifdef __cplusplus\n}\n#endif\n\n#endif /* #ifndef MIR_HTAB_H */\n"
        },
        {
          "name": "mir-interp.c",
          "type": "blob",
          "size": 69.3427734375,
          "content": "/* This file is a part of MIR project.\n   Copyright (C) 2018-2024 Vladimir Makarov <vmakarov.gcc@gmail.com>.\n\n   File contains MIR interpreter which is an obligatory part of MIR API.\n*/\n\n#include \"mir-alloc.h\"\n#include \"mir.h\"\n#ifdef MIR_NO_INTERP\nstatic void interp_init (MIR_context_t ctx) {}\nstatic void finish_func_interpretation (MIR_item_t func_item, MIR_alloc_t alloc) {}\nstatic void interp_finish (MIR_context_t ctx) {}\nvoid MIR_interp (MIR_context_t ctx, MIR_item_t func_item, MIR_val_t *results, size_t nargs, ...) {}\nvoid MIR_interp_arr_varg (MIR_context_t ctx, MIR_item_t func_item, MIR_val_t *results, size_t nargs,\n                          MIR_val_t *vals, va_list va) {}\nvoid MIR_interp_arr (MIR_context_t ctx, MIR_item_t func_item, MIR_val_t *results, size_t nargs,\n                     MIR_val_t *vals) {}\nvoid MIR_set_interp_interface (MIR_context_t ctx, MIR_item_t func_item) {}\n#else\n\n#ifndef MIR_INTERP_TRACE\n#define MIR_INTERP_TRACE 0\n#endif\n\n#if !defined(MIR_DIRECT_DISPATCH) && defined(__GNUC__)\n#define DIRECT_THREADED_DISPATCH 1\n#else\n#define DIRECT_THREADED_DISPATCH 0\n#endif\n\n#if defined(__GNUC__)\n#define ALWAYS_INLINE inline __attribute ((always_inline))\n#else\n#define ALWAYS_INLINE inline\n#endif\n\n#if defined(_MSC_VER)\n#define alloca _alloca\n#endif\n\ntypedef MIR_val_t *code_t;\n\ntypedef struct func_desc {\n  MIR_reg_t nregs;\n  MIR_item_t func_item;\n  MIR_val_t code[1];\n} *func_desc_t;\n\nstatic void update_max_nreg (MIR_reg_t reg, MIR_reg_t *max_nreg) {\n  if (*max_nreg < reg) *max_nreg = reg;\n}\n\nstatic MIR_reg_t get_reg (MIR_op_t op, MIR_reg_t *max_nreg) {\n  /* We do not interpret code with hard regs */\n  mir_assert (op.mode == MIR_OP_REG);\n  update_max_nreg (op.u.reg, max_nreg);\n  return op.u.reg;\n}\n\n#define IC_EL(i) IC_##i\n#define REP_SEP ,\ntypedef enum {\n  IC_LDI8 = MIR_INSN_BOUND,\n  REP6 (IC_EL, LDU8, LDI16, LDU16, LDI32, LDU32, LDI64),\n  REP3 (IC_EL, LDF, LDD, LDLD),\n  REP7 (IC_EL, STI8, STU8, STI16, STU16, STI32, STU32, STI64),\n  REP8 (IC_EL, STF, STD, STLD, MOVI, MOVP, MOVF, MOVD, MOVLD),\n  REP6 (IC_EL, IMM_CALL, IMM_JCALL, MOVFG, FMOVFG, DMOVFG, LDMOVFG),\n  REP5 (IC_EL, MOVTG, FMOVTG, DMOVTG, LDMOVTG, INSN_BOUND),\n} MIR_full_insn_code_t;\n#undef REP_SEP\n\nDEF_VARR (MIR_val_t);\n\nstruct ff_interface {\n  size_t arg_vars_num, nres, nargs;\n  MIR_type_t *res_types;\n  _MIR_arg_desc_t *arg_descs;\n  void *interface_addr;\n};\n\ntypedef struct ff_interface *ff_interface_t;\nDEF_HTAB (ff_interface_t);\n\nDEF_VARR (_MIR_arg_desc_t);\n\nstruct interp_ctx {\n#if DIRECT_THREADED_DISPATCH\n  void *dispatch_label_tab[IC_INSN_BOUND];\n#endif\n  MIR_val_t global_regs[MAX_HARD_REG + 1];\n  VARR (MIR_val_t) * code_varr;\n  VARR (MIR_insn_t) * branches;\n  VARR (MIR_val_t) * arg_vals_varr;\n  MIR_val_t *arg_vals;\n#if MIR_INTERP_TRACE\n  int trace_insn_ident;\n#endif\n  void *(*bstart_builtin) (void);\n  void (*bend_builtin) (void *);\n  void *jret_addr;\n  VARR (MIR_val_t) * call_res_args_varr;\n  MIR_val_t *call_res_args;\n  VARR (_MIR_arg_desc_t) * call_arg_descs_varr;\n  _MIR_arg_desc_t *call_arg_descs;\n  HTAB (ff_interface_t) * ff_interface_tab;\n};\n\n#define dispatch_label_tab interp_ctx->dispatch_label_tab\n#define global_regs interp_ctx->global_regs\n#define code_varr interp_ctx->code_varr\n#define branches interp_ctx->branches\n#define jret_addr interp_ctx->jret_addr\n#define arg_vals_varr interp_ctx->arg_vals_varr\n#define arg_vals interp_ctx->arg_vals\n#define trace_insn_ident interp_ctx->trace_insn_ident\n#define trace_ident interp_ctx->trace_ident\n#define bstart_builtin interp_ctx->bstart_builtin\n#define bend_builtin interp_ctx->bend_builtin\n#define call_res_args_varr interp_ctx->call_res_args_varr\n#define call_res_args interp_ctx->call_res_args\n#define call_arg_descs_varr interp_ctx->call_arg_descs_varr\n#define call_arg_descs interp_ctx->call_arg_descs\n#define ff_interface_tab interp_ctx->ff_interface_tab\n\nstatic void get_icode (struct interp_ctx *interp_ctx, MIR_val_t *v, int code) {\n#if DIRECT_THREADED_DISPATCH\n  v->a = dispatch_label_tab[code];\n#else\n  v->ic = code;\n#endif\n}\n\nstatic void push_insn_start (struct interp_ctx *interp_ctx, int code,\n                             MIR_insn_t original_insn MIR_UNUSED) {\n  MIR_val_t v;\n\n  get_icode (interp_ctx, &v, code);\n  VARR_PUSH (MIR_val_t, code_varr, v);\n#if MIR_INTERP_TRACE\n  v.a = original_insn;\n  VARR_PUSH (MIR_val_t, code_varr, v);\n#endif\n}\n\nstatic MIR_full_insn_code_t get_int_mem_insn_code (int load_p, MIR_type_t t) {\n  switch (t) {\n  case MIR_T_I8: return load_p ? IC_LDI8 : IC_STI8;\n  case MIR_T_U8: return load_p ? IC_LDU8 : IC_STU8;\n  case MIR_T_I16: return load_p ? IC_LDI16 : IC_STI16;\n  case MIR_T_U16: return load_p ? IC_LDU16 : IC_STU16;\n  case MIR_T_I32: return load_p ? IC_LDI32 : IC_STI32;\n#if MIR_PTR32\n  case MIR_T_P:\n#endif\n  case MIR_T_U32: return load_p ? IC_LDU32 : IC_STU32;\n#if MIR_PTR64\n  case MIR_T_P:\n#endif\n  case MIR_T_I64:\n  case MIR_T_U64: return load_p ? IC_LDI64 : IC_STI64;\n  default: mir_assert (FALSE); return load_p ? IC_LDI64 : IC_STI64; /* to remove a warning */\n  }\n}\n\nstatic void push_mem (struct interp_ctx *interp_ctx, MIR_op_t op) {\n  MIR_val_t v;\n\n  mir_assert (op.mode == MIR_OP_MEM && op.u.mem.disp == 0 && op.u.mem.index == 0);\n  v.i = op.u.mem.base;\n  VARR_PUSH (MIR_val_t, code_varr, v);\n}\n\nstatic void redirect_interface_to_interp (MIR_context_t ctx, MIR_item_t func_item);\n\nstatic void generate_icode (MIR_context_t ctx, MIR_item_t func_item) {\n  struct interp_ctx *interp_ctx = ctx->interp_ctx;\n  int imm_call_p;\n  MIR_func_t func = func_item->u.func;\n  MIR_insn_t insn, label;\n  MIR_type_t type;\n  MIR_val_t v;\n  size_t i;\n  MIR_reg_t max_nreg = 0;\n  func_desc_t func_desc;\n\n  VARR_TRUNC (MIR_insn_t, branches, 0);\n  VARR_TRUNC (MIR_val_t, code_varr, 0);\n  for (insn = DLIST_HEAD (MIR_insn_t, func->insns); insn != NULL;\n       insn = DLIST_NEXT (MIR_insn_t, insn)) {\n    MIR_insn_code_t code = insn->code;\n    size_t nops = MIR_insn_nops (ctx, insn);\n    MIR_op_t *ops = insn->ops;\n\n    insn->data = (void *) VARR_LENGTH (MIR_val_t, code_varr);\n    switch (code) {\n    case MIR_MOV: /* loads, imm moves */\n      if (ops[0].mode == MIR_OP_MEM) {\n        push_insn_start (interp_ctx, get_int_mem_insn_code (FALSE, ops[0].u.mem.type), insn);\n        v.i = get_reg (ops[1], &max_nreg);\n        VARR_PUSH (MIR_val_t, code_varr, v);\n        push_mem (interp_ctx, ops[0]);\n      } else if (ops[1].mode == MIR_OP_MEM) {\n        push_insn_start (interp_ctx, get_int_mem_insn_code (TRUE, ops[1].u.mem.type), insn);\n        v.i = get_reg (ops[0], &max_nreg);\n        VARR_PUSH (MIR_val_t, code_varr, v);\n        push_mem (interp_ctx, ops[1]);\n      } else if (ops[1].mode == MIR_OP_INT || ops[1].mode == MIR_OP_UINT) {\n        push_insn_start (interp_ctx, IC_MOVI, insn);\n        v.i = get_reg (ops[0], &max_nreg);\n        VARR_PUSH (MIR_val_t, code_varr, v);\n        if (ops[1].mode == MIR_OP_INT)\n          v.i = ops[1].u.i;\n        else\n          v.u = ops[1].u.u;\n        VARR_PUSH (MIR_val_t, code_varr, v);\n      } else if (ops[1].mode == MIR_OP_REF) {\n        MIR_item_t item = ops[1].u.ref;\n\n        if (item->item_type == MIR_import_item && item->ref_def != NULL)\n          item->addr = item->ref_def->addr;\n        push_insn_start (interp_ctx, IC_MOVP, insn);\n        v.i = get_reg (ops[0], &max_nreg);\n        VARR_PUSH (MIR_val_t, code_varr, v);\n        v.a = item->addr;\n        VARR_PUSH (MIR_val_t, code_varr, v);\n      } else {\n        const char *hard_reg_name;\n      regreg:\n        mir_assert (ops[0].mode == MIR_OP_REG && ops[1].mode == MIR_OP_REG);\n        type = MIR_reg_type (ctx, ops[0].u.reg, func);\n        mir_assert (type == MIR_reg_type (ctx, ops[1].u.reg, func));\n        if ((hard_reg_name = MIR_reg_hard_reg_name (ctx, ops[0].u.reg, func)) != NULL) {\n          mir_assert (MIR_reg_hard_reg_name (ctx, ops[1].u.reg, func) == NULL);\n          push_insn_start (interp_ctx,\n                           type == MIR_T_F    ? IC_FMOVTG\n                           : type == MIR_T_D  ? IC_DMOVTG\n                           : type == MIR_T_LD ? IC_LDMOVTG\n                                              : IC_MOVTG,\n                           insn);\n          v.i = _MIR_get_hard_reg (ctx, hard_reg_name);\n          mir_assert (v.i <= MAX_HARD_REG);\n          VARR_PUSH (MIR_val_t, code_varr, v);\n          v.i = get_reg (ops[1], &max_nreg);\n          VARR_PUSH (MIR_val_t, code_varr, v);\n        } else if ((hard_reg_name = MIR_reg_hard_reg_name (ctx, ops[1].u.reg, func)) != NULL) {\n          mir_assert (MIR_reg_hard_reg_name (ctx, ops[0].u.reg, func) == NULL);\n          push_insn_start (interp_ctx,\n                           type == MIR_T_F    ? IC_FMOVFG\n                           : type == MIR_T_D  ? IC_DMOVFG\n                           : type == MIR_T_LD ? IC_LDMOVFG\n                                              : IC_MOVFG,\n                           insn);\n          v.i = get_reg (ops[0], &max_nreg);\n          VARR_PUSH (MIR_val_t, code_varr, v);\n          v.i = _MIR_get_hard_reg (ctx, hard_reg_name);\n          mir_assert (v.i <= MAX_HARD_REG);\n          VARR_PUSH (MIR_val_t, code_varr, v);\n        } else {\n          push_insn_start (interp_ctx, code, insn);\n          v.i = get_reg (ops[0], &max_nreg);\n          VARR_PUSH (MIR_val_t, code_varr, v);\n          v.i = get_reg (ops[1], &max_nreg);\n          VARR_PUSH (MIR_val_t, code_varr, v);\n        }\n      }\n      break;\n    case MIR_FMOV:\n      if (ops[0].mode == MIR_OP_MEM) {\n        push_insn_start (interp_ctx, IC_STF, insn);\n        v.i = get_reg (ops[1], &max_nreg);\n        VARR_PUSH (MIR_val_t, code_varr, v);\n        push_mem (interp_ctx, ops[0]);\n      } else if (ops[1].mode == MIR_OP_MEM) {\n        push_insn_start (interp_ctx, IC_LDF, insn);\n        v.i = get_reg (ops[0], &max_nreg);\n        VARR_PUSH (MIR_val_t, code_varr, v);\n        push_mem (interp_ctx, ops[1]);\n      } else if (ops[1].mode == MIR_OP_FLOAT) {\n        push_insn_start (interp_ctx, IC_MOVF, insn);\n        v.i = get_reg (ops[0], &max_nreg);\n        VARR_PUSH (MIR_val_t, code_varr, v);\n        v.f = ops[1].u.f;\n        VARR_PUSH (MIR_val_t, code_varr, v);\n      } else {\n        goto regreg;\n      }\n      break;\n    case MIR_DMOV:\n      if (ops[0].mode == MIR_OP_MEM) {\n        push_insn_start (interp_ctx, IC_STD, insn);\n        v.i = get_reg (ops[1], &max_nreg);\n        VARR_PUSH (MIR_val_t, code_varr, v);\n        push_mem (interp_ctx, ops[0]);\n      } else if (ops[1].mode == MIR_OP_MEM) {\n        push_insn_start (interp_ctx, IC_LDD, insn);\n        v.i = get_reg (ops[0], &max_nreg);\n        VARR_PUSH (MIR_val_t, code_varr, v);\n        push_mem (interp_ctx, ops[1]);\n      } else if (ops[1].mode == MIR_OP_DOUBLE) {\n        push_insn_start (interp_ctx, IC_MOVD, insn);\n        v.i = get_reg (ops[0], &max_nreg);\n        VARR_PUSH (MIR_val_t, code_varr, v);\n        v.d = ops[1].u.d;\n        VARR_PUSH (MIR_val_t, code_varr, v);\n      } else {\n        goto regreg;\n      }\n      break;\n    case MIR_LDMOV:\n      if (ops[0].mode == MIR_OP_MEM) {\n        push_insn_start (interp_ctx, IC_STLD, insn);\n        v.i = get_reg (ops[1], &max_nreg);\n        VARR_PUSH (MIR_val_t, code_varr, v);\n        push_mem (interp_ctx, ops[0]);\n      } else if (ops[1].mode == MIR_OP_MEM) {\n        push_insn_start (interp_ctx, IC_LDLD, insn);\n        v.i = get_reg (ops[0], &max_nreg);\n        VARR_PUSH (MIR_val_t, code_varr, v);\n        push_mem (interp_ctx, ops[1]);\n      } else if (ops[1].mode == MIR_OP_LDOUBLE) {\n        push_insn_start (interp_ctx, IC_MOVLD, insn);\n        v.i = get_reg (ops[0], &max_nreg);\n        VARR_PUSH (MIR_val_t, code_varr, v);\n        v.ld = ops[1].u.ld;\n        VARR_PUSH (MIR_val_t, code_varr, v);\n      } else {\n        goto regreg;\n      }\n      break;\n    case MIR_LABEL: break;\n    case MIR_INVALID_INSN:\n      (*MIR_get_error_func (ctx)) (MIR_invalid_insn_error, \"invalid insn for interpreter\");\n      break;\n    case MIR_JMP:\n      VARR_PUSH (MIR_insn_t, branches, insn);\n      push_insn_start (interp_ctx, code, insn);\n      v.i = 0;\n      VARR_PUSH (MIR_val_t, code_varr, v);\n      break;\n    case MIR_LADDR:\n      VARR_PUSH (MIR_insn_t, branches, insn);\n      push_insn_start (interp_ctx, code, insn);\n      v.i = get_reg (ops[0], &max_nreg);\n      VARR_PUSH (MIR_val_t, code_varr, v);\n      v.i = 0;\n      VARR_PUSH (MIR_val_t, code_varr, v);\n      break;\n    case MIR_BT:\n    case MIR_BTS:\n    case MIR_BF:\n    case MIR_BFS:\n      VARR_PUSH (MIR_insn_t, branches, insn);\n      push_insn_start (interp_ctx, code, insn);\n      v.i = 0;\n      VARR_PUSH (MIR_val_t, code_varr, v);\n      v.i = get_reg (ops[1], &max_nreg);\n      VARR_PUSH (MIR_val_t, code_varr, v);\n      break;\n    case MIR_BEQ:\n    case MIR_BEQS:\n    case MIR_FBEQ:\n    case MIR_DBEQ:\n    case MIR_BNE:\n    case MIR_BNES:\n    case MIR_FBNE:\n    case MIR_DBNE:\n    case MIR_BLT:\n    case MIR_BLTS:\n    case MIR_UBLT:\n    case MIR_UBLTS:\n    case MIR_FBLT:\n    case MIR_DBLT:\n    case MIR_BLE:\n    case MIR_BLES:\n    case MIR_UBLE:\n    case MIR_UBLES:\n    case MIR_FBLE:\n    case MIR_DBLE:\n    case MIR_BGT:\n    case MIR_BGTS:\n    case MIR_UBGT:\n    case MIR_UBGTS:\n    case MIR_FBGT:\n    case MIR_DBGT:\n    case MIR_BGE:\n    case MIR_BGES:\n    case MIR_UBGE:\n    case MIR_UBGES:\n    case MIR_FBGE:\n    case MIR_DBGE:\n    case MIR_LDBEQ:\n    case MIR_LDBNE:\n    case MIR_LDBLT:\n    case MIR_LDBLE:\n    case MIR_LDBGT:\n    case MIR_LDBGE:\n      VARR_PUSH (MIR_insn_t, branches, insn);\n      push_insn_start (interp_ctx, code, insn);\n      v.i = 0;\n      VARR_PUSH (MIR_val_t, code_varr, v);\n      v.i = get_reg (ops[1], &max_nreg);\n      VARR_PUSH (MIR_val_t, code_varr, v);\n      v.i = get_reg (ops[2], &max_nreg);\n      VARR_PUSH (MIR_val_t, code_varr, v);\n      break;\n    case MIR_BO:\n    case MIR_UBO:\n    case MIR_BNO:\n    case MIR_UBNO:\n      VARR_PUSH (MIR_insn_t, branches, insn);\n      push_insn_start (interp_ctx, code, insn);\n      v.i = 0;\n      VARR_PUSH (MIR_val_t, code_varr, v);\n      break;\n    case MIR_PRSET: break; /* just ignore */\n    case MIR_PRBEQ:        /* make jump if property is zero or ignore otherwise */\n      if (ops[2].mode == MIR_OP_INT && ops[2].u.i == 0) goto jump;\n      break;\n    case MIR_PRBNE: /* make jump if property is nonzero or ignore otherwise */\n      if (ops[2].mode != MIR_OP_INT || ops[2].u.i == 0) break;\n    jump:\n      VARR_PUSH (MIR_insn_t, branches, insn);\n      push_insn_start (interp_ctx, MIR_JMP, insn);\n      v.i = 0;\n      VARR_PUSH (MIR_val_t, code_varr, v); /* place for label */\n      break;\n    default:\n      imm_call_p = FALSE;\n      if (MIR_call_code_p (code))\n        imm_call_p = (ops[1].mode == MIR_OP_REF\n                      && (ops[1].u.ref->item_type == MIR_import_item\n                          || ops[1].u.ref->item_type == MIR_export_item\n                          || ops[1].u.ref->item_type == MIR_forward_item\n                          || ops[1].u.ref->item_type == MIR_func_item));\n      push_insn_start (interp_ctx,\n                       imm_call_p           ? (code == MIR_JCALL ? IC_IMM_JCALL : IC_IMM_CALL)\n                       : code == MIR_INLINE ? MIR_CALL\n                                            : code,\n                       insn);\n      if (code == MIR_SWITCH) {\n        VARR_PUSH (MIR_insn_t, branches, insn);\n        v.i = nops;\n        VARR_PUSH (MIR_val_t, code_varr, v);\n      } else if (code == MIR_RET) {\n        v.i = nops;\n        VARR_PUSH (MIR_val_t, code_varr, v);\n      } else if (MIR_call_code_p (code)) {\n        v.i = nops;\n        VARR_PUSH (MIR_val_t, code_varr, v);\n        v.a = insn;\n        VARR_PUSH (MIR_val_t, code_varr, v);\n        v.a = NULL;\n        VARR_PUSH (MIR_val_t, code_varr, v); /* for ffi interface */\n      }\n      for (i = 0; i < nops; i++) {\n        if (i == 0 && MIR_call_code_p (code)) { /* prototype ??? */\n          mir_assert (ops[i].mode == MIR_OP_REF && ops[i].u.ref->item_type == MIR_proto_item);\n          v.a = ops[i].u.ref;\n        } else if (i == 1 && imm_call_p) {\n          MIR_item_t item = ops[i].u.ref;\n\n          mir_assert (item->item_type == MIR_import_item || item->item_type == MIR_export_item\n                      || item->item_type == MIR_forward_item || item->item_type == MIR_func_item);\n          v.a = item->addr;\n        } else if (code == MIR_VA_ARG && i == 2) { /* type */\n          mir_assert (ops[i].mode == MIR_OP_MEM);\n          v.i = ops[i].u.mem.type;\n        } else if (code == MIR_SWITCH && i > 0) {\n          mir_assert (ops[i].mode == MIR_OP_LABEL);\n          v.i = 0;\n        } else if (MIR_call_code_p (code) && ops[i].mode == MIR_OP_MEM) {\n          mir_assert (MIR_all_blk_type_p (ops[i].u.mem.type));\n          v.i = ops[i].u.mem.base;\n          update_max_nreg ((MIR_reg_t) v.i, &max_nreg);\n        } else {\n          mir_assert (ops[i].mode == MIR_OP_REG);\n          v.i = get_reg (ops[i], &max_nreg);\n        }\n        VARR_PUSH (MIR_val_t, code_varr, v);\n      }\n    }\n  }\n  for (i = 0; i < VARR_LENGTH (MIR_insn_t, branches); i++) {\n    size_t start_label_nop = 0, bound_label_nop = 1, start_label_loc = 1, n;\n\n    insn = VARR_GET (MIR_insn_t, branches, i);\n    if (insn->code == MIR_LADDR) {\n      start_label_nop = 1;\n      bound_label_nop = 2;\n    } else if (insn->code == MIR_SWITCH) {\n      start_label_nop = 1;\n      bound_label_nop = start_label_nop + insn->nops - 1;\n      start_label_loc++; /* we put nops for MIR_SWITCH */\n    }\n    for (n = start_label_nop; n < bound_label_nop; n++) {\n      label = insn->ops[n].u.label;\n      v.i = (size_t) label->data;\n#if MIR_INTERP_TRACE\n      VARR_SET (MIR_val_t, code_varr, (size_t) insn->data + n + start_label_loc + 1, v);\n#else\n      VARR_SET (MIR_val_t, code_varr, (size_t) insn->data + n + start_label_loc, v);\n#endif\n    }\n  }\n  func_item->data = func_desc\n    = MIR_malloc (ctx->alloc, sizeof (struct func_desc) + VARR_LENGTH (MIR_val_t, code_varr) * sizeof (MIR_val_t));\n  if (func_desc == NULL)\n    (*MIR_get_error_func (ctx)) (MIR_alloc_error, \"no memory for interpreter code\");\n  memmove (func_desc->code, VARR_ADDR (MIR_val_t, code_varr),\n           VARR_LENGTH (MIR_val_t, code_varr) * sizeof (MIR_val_t));\n  for (MIR_lref_data_t lref = func->first_lref; lref != NULL; lref = lref->next) {\n    if (lref->label2 == NULL)\n      *(void **) lref->load_addr\n        = (char *) (func_desc->code + (int64_t) lref->label->data) + lref->disp;\n    else\n      *(int64_t *) lref->load_addr\n        = (int64_t) lref->label->data - (int64_t) lref->label2->data + lref->disp;\n  }\n  mir_assert (max_nreg < MIR_MAX_REG_NUM);\n  func_desc->nregs = max_nreg + 1;\n  func_desc->func_item = func_item;\n}\n\nstatic void finish_func_interpretation (MIR_item_t func_item, MIR_alloc_t alloc) {\n  mir_assert (func_item->item_type == MIR_func_item);\n  if (func_item->data == NULL) return;\n  for (MIR_insn_t insn = DLIST_HEAD (MIR_insn_t, func_item->u.func->insns); insn != NULL;\n       insn = DLIST_NEXT (MIR_insn_t, insn))\n    insn->data = NULL; /* it was used for interpretation preparation */\n  MIR_free (alloc, func_item->data);\n  func_item->data = NULL;\n}\n\nstatic ALWAYS_INLINE void *get_a (MIR_val_t *v) { return v->a; }\nstatic ALWAYS_INLINE int64_t get_i (MIR_val_t *v) { return v->i; }\nstatic ALWAYS_INLINE float get_f (MIR_val_t *v) { return v->f; }\nstatic ALWAYS_INLINE double get_d (MIR_val_t *v) { return v->d; }\nstatic ALWAYS_INLINE long double get_ld (MIR_val_t *v) { return v->ld; }\n\nstatic ALWAYS_INLINE void **get_aop (MIR_val_t *bp, code_t c) { return &bp[get_i (c)].a; }\nstatic ALWAYS_INLINE int64_t *get_iop (MIR_val_t *bp, code_t c) { return &bp[get_i (c)].i; }\nstatic ALWAYS_INLINE uint64_t *get_uop (MIR_val_t *bp, code_t c) { return &bp[get_i (c)].u; }\nstatic ALWAYS_INLINE float *get_fop (MIR_val_t *bp, code_t c) { return &bp[get_i (c)].f; }\nstatic ALWAYS_INLINE double *get_dop (MIR_val_t *bp, code_t c) { return &bp[get_i (c)].d; }\nstatic ALWAYS_INLINE long double *get_ldop (MIR_val_t *bp, code_t c) { return &bp[get_i (c)].ld; }\n\nstatic ALWAYS_INLINE int64_t *get_2iops (MIR_val_t *bp, code_t c, int64_t *p) {\n  *p = *get_iop (bp, c + 1);\n  return get_iop (bp, c);\n}\n\nstatic ALWAYS_INLINE int64_t *get_2isops (MIR_val_t *bp, code_t c, int32_t *p) {\n  *p = (int32_t) *get_iop (bp, c + 1);\n  return get_iop (bp, c);\n}\n\nstatic ALWAYS_INLINE int64_t *get_3iops (MIR_val_t *bp, code_t c, int64_t *p1, int64_t *p2) {\n  *p1 = *get_iop (bp, c + 1);\n  *p2 = *get_iop (bp, c + 2);\n  return get_iop (bp, c);\n}\n\nstatic ALWAYS_INLINE int64_t *get_3isops (MIR_val_t *bp, code_t c, int32_t *p1, int32_t *p2) {\n  *p1 = (int32_t) *get_iop (bp, c + 1);\n  *p2 = (int32_t) *get_iop (bp, c + 2);\n  return get_iop (bp, c);\n}\n\nstatic ALWAYS_INLINE uint64_t *get_3uops (MIR_val_t *bp, code_t c, uint64_t *p1, uint64_t *p2) {\n  *p1 = *get_uop (bp, c + 1);\n  *p2 = *get_uop (bp, c + 2);\n  return get_uop (bp, c);\n}\n\nstatic ALWAYS_INLINE uint64_t *get_3usops (MIR_val_t *bp, code_t c, uint32_t *p1, uint32_t *p2) {\n  *p1 = (uint32_t) *get_uop (bp, c + 1);\n  *p2 = (uint32_t) *get_uop (bp, c + 2);\n  return get_uop (bp, c);\n}\n\nstatic ALWAYS_INLINE float *get_2fops (MIR_val_t *bp, code_t c, float *p) {\n  *p = *get_fop (bp, c + 1);\n  return get_fop (bp, c);\n}\n\nstatic ALWAYS_INLINE float *get_3fops (MIR_val_t *bp, code_t c, float *p1, float *p2) {\n  *p1 = *get_fop (bp, c + 1);\n  *p2 = *get_fop (bp, c + 2);\n  return get_fop (bp, c);\n}\n\nstatic ALWAYS_INLINE int64_t *get_fcmp_ops (MIR_val_t *bp, code_t c, float *p1, float *p2) {\n  *p1 = *get_fop (bp, c + 1);\n  *p2 = *get_fop (bp, c + 2);\n  return get_iop (bp, c);\n}\n\nstatic ALWAYS_INLINE double *get_2dops (MIR_val_t *bp, code_t c, double *p) {\n  *p = *get_dop (bp, c + 1);\n  return get_dop (bp, c);\n}\n\nstatic ALWAYS_INLINE double *get_3dops (MIR_val_t *bp, code_t c, double *p1, double *p2) {\n  *p1 = *get_dop (bp, c + 1);\n  *p2 = *get_dop (bp, c + 2);\n  return get_dop (bp, c);\n}\n\nstatic ALWAYS_INLINE int64_t *get_dcmp_ops (MIR_val_t *bp, code_t c, double *p1, double *p2) {\n  *p1 = *get_dop (bp, c + 1);\n  *p2 = *get_dop (bp, c + 2);\n  return get_iop (bp, c);\n}\n\nstatic ALWAYS_INLINE long double *get_2ldops (MIR_val_t *bp, code_t c, long double *p) {\n  *p = *get_ldop (bp, c + 1);\n  return get_ldop (bp, c);\n}\n\nstatic ALWAYS_INLINE long double *get_3ldops (MIR_val_t *bp, code_t c, long double *p1,\n                                              long double *p2) {\n  *p1 = *get_ldop (bp, c + 1);\n  *p2 = *get_ldop (bp, c + 2);\n  return get_ldop (bp, c);\n}\n\nstatic ALWAYS_INLINE int64_t *get_ldcmp_ops (MIR_val_t *bp, code_t c, long double *p1,\n                                             long double *p2) {\n  *p1 = *get_ldop (bp, c + 1);\n  *p2 = *get_ldop (bp, c + 2);\n  return get_iop (bp, c);\n}\n\nstatic ALWAYS_INLINE int64_t get_mem_addr (MIR_val_t *bp, code_t c) { return bp[get_i (c)].i; }\n\n#define EXT(tp)                          \\\n  do {                                   \\\n    int64_t *r = get_iop (bp, ops);      \\\n    tp s = (tp) * get_iop (bp, ops + 1); \\\n    *r = (int64_t) s;                    \\\n  } while (0)\n#define IOP2(op)                 \\\n  do {                           \\\n    int64_t *r, p;               \\\n    r = get_2iops (bp, ops, &p); \\\n    *r = op p;                   \\\n  } while (0)\n#define IOP2S(op)                 \\\n  do {                            \\\n    int64_t *r;                   \\\n    int32_t p;                    \\\n    r = get_2isops (bp, ops, &p); \\\n    *r = op p;                    \\\n  } while (0)\n#define IOP3(op)                       \\\n  do {                                 \\\n    int64_t *r, p1, p2;                \\\n    r = get_3iops (bp, ops, &p1, &p2); \\\n    *r = p1 op p2;                     \\\n  } while (0)\n#define IOP3S(op)                       \\\n  do {                                  \\\n    int64_t *r;                         \\\n    int32_t p1, p2;                     \\\n    r = get_3isops (bp, ops, &p1, &p2); \\\n    *r = p1 op p2;                      \\\n  } while (0)\n#define ICMP(op)                       \\\n  do {                                 \\\n    int64_t *r, p1, p2;                \\\n    r = get_3iops (bp, ops, &p1, &p2); \\\n    *r = p1 op p2;                     \\\n  } while (0)\n#define ICMPS(op)                       \\\n  do {                                  \\\n    int64_t *r;                         \\\n    int32_t p1, p2;                     \\\n    r = get_3isops (bp, ops, &p1, &p2); \\\n    *r = p1 op p2;                      \\\n  } while (0)\n#define BICMP(op)                                                       \\\n  do {                                                                  \\\n    int64_t op1 = *get_iop (bp, ops + 1), op2 = *get_iop (bp, ops + 2); \\\n    if (op1 op op2) pc = code + get_i (ops);                            \\\n  } while (0)\n#define BICMPS(op)                                                                            \\\n  do {                                                                                        \\\n    int32_t op1 = (int32_t) * get_iop (bp, ops + 1), op2 = (int32_t) * get_iop (bp, ops + 2); \\\n    if (op1 op op2) pc = code + get_i (ops);                                                  \\\n  } while (0)\n#define UOP3(op)                       \\\n  do {                                 \\\n    uint64_t *r, p1, p2;               \\\n    r = get_3uops (bp, ops, &p1, &p2); \\\n    *r = p1 op p2;                     \\\n  } while (0)\n#define UOP3S(op)                       \\\n  do {                                  \\\n    uint64_t *r;                        \\\n    uint32_t p1, p2;                    \\\n    r = get_3usops (bp, ops, &p1, &p2); \\\n    *r = p1 op p2;                      \\\n  } while (0)\n#define UIOP3(op)                      \\\n  do {                                 \\\n    uint64_t *r, p1, p2;               \\\n    r = get_3uops (bp, ops, &p1, &p2); \\\n    *r = p1 op p2;                     \\\n  } while (0)\n#define UIOP3S(op)                      \\\n  do {                                  \\\n    uint64_t *r;                        \\\n    uint32_t p1, p2;                    \\\n    r = get_3usops (bp, ops, &p1, &p2); \\\n    *r = p1 op p2;                      \\\n  } while (0)\n#define UCMP(op)                       \\\n  do {                                 \\\n    uint64_t *r, p1, p2;               \\\n    r = get_3uops (bp, ops, &p1, &p2); \\\n    *r = p1 op p2;                     \\\n  } while (0)\n#define UCMPS(op)                       \\\n  do {                                  \\\n    uint64_t *r;                        \\\n    uint32_t p1, p2;                    \\\n    r = get_3usops (bp, ops, &p1, &p2); \\\n    *r = p1 op p2;                      \\\n  } while (0)\n#define BUCMP(op)                                                        \\\n  do {                                                                   \\\n    uint64_t op1 = *get_uop (bp, ops + 1), op2 = *get_uop (bp, ops + 2); \\\n    if (op1 op op2) pc = code + get_i (ops);                             \\\n  } while (0)\n#define BUCMPS(op)                                                                               \\\n  do {                                                                                           \\\n    uint32_t op1 = (uint32_t) * get_uop (bp, ops + 1), op2 = (uint32_t) * get_uop (bp, ops + 2); \\\n    if (op1 op op2) pc = code + get_i (ops);                                                     \\\n  } while (0)\n\n#define FOP2(op)                 \\\n  do {                           \\\n    float *r, p;                 \\\n    r = get_2fops (bp, ops, &p); \\\n    *r = op p;                   \\\n  } while (0)\n#define FOP3(op)                       \\\n  do {                                 \\\n    float *r, p1, p2;                  \\\n    r = get_3fops (bp, ops, &p1, &p2); \\\n    *r = p1 op p2;                     \\\n  } while (0)\n#define FCMP(op)                          \\\n  do {                                    \\\n    int64_t *r;                           \\\n    float p1, p2;                         \\\n    r = get_fcmp_ops (bp, ops, &p1, &p2); \\\n    *r = p1 op p2;                        \\\n  } while (0)\n#define BFCMP(op)                                                     \\\n  do {                                                                \\\n    float op1 = *get_fop (bp, ops + 1), op2 = *get_fop (bp, ops + 2); \\\n    if (op1 op op2) pc = code + get_i (ops);                          \\\n  } while (0)\n\n#define DOP2(op)                 \\\n  do {                           \\\n    double *r, p;                \\\n    r = get_2dops (bp, ops, &p); \\\n    *r = op p;                   \\\n  } while (0)\n#define DOP3(op)                       \\\n  do {                                 \\\n    double *r, p1, p2;                 \\\n    r = get_3dops (bp, ops, &p1, &p2); \\\n    *r = p1 op p2;                     \\\n  } while (0)\n#define DCMP(op)                          \\\n  do {                                    \\\n    int64_t *r;                           \\\n    double p1, p2;                        \\\n    r = get_dcmp_ops (bp, ops, &p1, &p2); \\\n    *r = p1 op p2;                        \\\n  } while (0)\n#define BDCMP(op)                                                      \\\n  do {                                                                 \\\n    double op1 = *get_dop (bp, ops + 1), op2 = *get_dop (bp, ops + 2); \\\n    if (op1 op op2) pc = code + get_i (ops);                           \\\n  } while (0)\n\n#define LDOP2(op)                 \\\n  do {                            \\\n    long double *r, p;            \\\n    r = get_2ldops (bp, ops, &p); \\\n    *r = op p;                    \\\n  } while (0)\n#define LDOP3(op)                       \\\n  do {                                  \\\n    long double *r, p1, p2;             \\\n    r = get_3ldops (bp, ops, &p1, &p2); \\\n    *r = p1 op p2;                      \\\n  } while (0)\n#define LDCMP(op)                          \\\n  do {                                     \\\n    int64_t *r;                            \\\n    long double p1, p2;                    \\\n    r = get_ldcmp_ops (bp, ops, &p1, &p2); \\\n    *r = p1 op p2;                         \\\n  } while (0)\n#define BLDCMP(op)                                                            \\\n  do {                                                                        \\\n    long double op1 = *get_ldop (bp, ops + 1), op2 = *get_ldop (bp, ops + 2); \\\n    if (op1 op op2) pc = code + get_i (ops);                                  \\\n  } while (0)\n\n#define LD(op, val_type, mem_type)          \\\n  do {                                      \\\n    val_type *r = get_##op (bp, ops);       \\\n    int64_t a = get_mem_addr (bp, ops + 1); \\\n    *r = *((mem_type *) a);                 \\\n  } while (0)\n#define ST(op, val_type, mem_type)                \\\n  do {                                            \\\n    val_type v = (val_type) * get_##op (bp, ops); \\\n    int64_t a = get_mem_addr (bp, ops + 1);       \\\n    *((mem_type *) a) = (mem_type) v;             \\\n  } while (0)\n\n#if !MIR_INTERP_TRACE && defined(__GNUC__) && !defined(__clang__)\n#define OPTIMIZE \\\n  __attribute__ ((__optimize__ (\"O2\"))) __attribute__ ((__optimize__ (\"-fno-ipa-cp-clone\")))\n#else\n#define OPTIMIZE\n#endif\n\nstatic void call (MIR_context_t ctx, MIR_val_t *bp, MIR_op_t *insn_arg_ops, code_t ffi_address_ptr,\n                  MIR_item_t proto_item, void *addr, code_t res_ops, size_t nargs);\n\n#if MIR_INTERP_TRACE\nstatic void start_insn_trace (MIR_context_t ctx, const char *name, func_desc_t func_desc, code_t pc,\n                              size_t nops) {\n  struct interp_ctx *interp_ctx = ctx->interp_ctx;\n  MIR_insn_t insn = pc[1].a;\n  code_t ops = pc + 2;\n\n  for (int i = 0; i < trace_insn_ident; i++) fprintf (stderr, \" \");\n  fprintf (stderr, \"%s\", name);\n  for (size_t i = 0; i < nops; i++) {\n    fprintf (stderr, i == 0 ? \"\\t\" : \", \");\n    fprintf (stderr, \"%\" PRId64, ops[i].i);\n  }\n  fprintf (stderr, \"\\t#\");\n  MIR_output_insn (ctx, stderr, insn, func_desc->func_item->u.func, FALSE);\n}\n\nstatic void finish_insn_trace (MIR_context_t ctx, MIR_full_insn_code_t code, code_t ops,\n                               MIR_val_t *bp) {\n  struct interp_ctx *interp_ctx = ctx->interp_ctx;\n  int out_p;\n  MIR_op_mode_t op_mode = MIR_OP_UNDEF;\n  MIR_val_t *res = bp;\n\n  switch (code) {\n  case IC_LDI8:\n  case IC_LDU8:\n  case IC_LDI16:\n  case IC_LDU16:\n  case IC_LDI32:\n  case IC_LDU32:\n  case IC_LDI64:\n  case IC_MOVI:\n  case IC_MOVTG:\n    res = global_regs;\n    /* falls through */\n  case IC_MOVFG:\n  case IC_MOVP: op_mode = MIR_OP_INT; break;\n  case IC_LDF:\n  case IC_FMOVTG:\n    res = global_regs;\n    /* falls through */\n  case IC_FMOVFG:\n  case IC_MOVF: op_mode = MIR_OP_FLOAT; break;\n  case IC_LDD:\n  case IC_DMOVTG:\n    res = global_regs;\n    /* falls through */\n  case IC_DMOVFG:\n  case IC_MOVD: op_mode = MIR_OP_DOUBLE; break;\n  case IC_LDLD:\n  case IC_LDMOVTG:\n    res = global_regs;\n    /* falls through */\n  case IC_LDMOVFG:\n  case IC_MOVLD: op_mode = MIR_OP_LDOUBLE; break;\n  case IC_STI8:\n  case IC_STU8:\n  case IC_STI16:\n  case IC_STU16:\n  case IC_STI32:\n  case IC_STU32:\n  case IC_STI64:\n  case IC_STF:\n  case IC_STD:;\n  case IC_STLD: break;\n  case IC_IMM_CALL: break;\n  case IC_IMM_JCALL: break;\n  default:\n    op_mode = _MIR_insn_code_op_mode (ctx, (MIR_insn_code_t) code, 0, &out_p);\n    if (op_mode == MIR_OP_BOUND || !out_p) op_mode = MIR_OP_UNDEF;\n    break;\n  }\n  switch (op_mode) {\n  case MIR_OP_INT:\n  case MIR_OP_UINT:\n    fprintf (stderr, \"\\t# res = %\" PRId64 \" (%\" PRIu64 \"u, 0x%\" PRIx64 \")\", res[ops[0].i].i,\n             res[ops[0].i].u, res[ops[0].i].u);\n    break;\n  case MIR_OP_FLOAT: fprintf (stderr, \"\\t# res = %.*ef\", FLT_DECIMAL_DIG, res[ops[0].i].f); break;\n  case MIR_OP_LDOUBLE:\n#ifndef _WIN32\n    fprintf (stderr, \"\\t# res = %.*Le\", LDBL_DECIMAL_DIG, res[ops[0].i].ld);\n    break;\n#endif\n  case MIR_OP_DOUBLE: fprintf (stderr, \"\\t# res = %.*e\", DBL_DECIMAL_DIG, res[ops[0].i].d); break;\n  default: assert (op_mode == MIR_OP_UNDEF);\n  }\n  fprintf (stderr, \"\\n\");\n}\n#endif\n\nstatic code_t call_insn_execute (MIR_context_t ctx, code_t pc, MIR_val_t *bp, code_t ops,\n                                 int imm_p) {\n  struct interp_ctx *interp_ctx = ctx->interp_ctx;\n  int64_t nops = get_i (ops); /* #args w/o nop, insn, and ff interface address */\n  MIR_insn_t insn = get_a (ops + 1);\n  MIR_item_t proto_item = get_a (ops + 3);\n  void *func_addr = imm_p ? get_a (ops + 4) : *get_aop (bp, ops + 4);\n  size_t start = proto_item->u.proto->nres + 5;\n\n  if (VARR_EXPAND (MIR_val_t, arg_vals_varr, nops)) arg_vals = VARR_ADDR (MIR_val_t, arg_vals_varr);\n\n  for (size_t i = start; i < (size_t) nops + 3; i++) arg_vals[i - start] = bp[get_i (ops + i)];\n\n#if MIR_INTERP_TRACE\n  trace_insn_ident += 2;\n#endif\n  call (ctx, bp, &insn->ops[proto_item->u.proto->nres + 2] /* arg ops */,\n        ops + 2 /* ffi address holder */, proto_item, func_addr, ops + 5 /* results start */,\n        nops - start + 3 /* arg # */);\n#if MIR_INTERP_TRACE\n  trace_insn_ident -= 2;\n#endif\n  pc += nops + 3; /* nops itself, the call insn, add ff interface address */\n  return pc;\n}\n\nstatic int64_t addr_offset8, addr_offset16, addr_offset32;\n\nstatic void OPTIMIZE eval (MIR_context_t ctx, func_desc_t func_desc, MIR_val_t *bp,\n                           MIR_val_t *results) {\n  struct interp_ctx *interp_ctx = ctx->interp_ctx;\n  MIR_val_t *globals = global_regs;\n  code_t pc, ops, code;\n  void *jmpi_val; /* where label thunk execution result will be: */\n  int64_t offset;\n  int signed_overflow_p = FALSE, unsigned_overflow_p = FALSE; /* to avoid uninitialized warnings */\n\n#if MIR_INTERP_TRACE\n  MIR_full_insn_code_t trace_insn_code;\n#define START_INSN(v, nops)                          \\\n  do {                                               \\\n    trace_insn_code = (MIR_full_insn_code_t) v;      \\\n    start_insn_trace (ctx, #v, func_desc, pc, nops); \\\n    ops = pc + 2; /* skip original insn too */       \\\n    pc += nops + 2;                                  \\\n  } while (0)\n#else\n#define START_INSN(v, nops) \\\n  do {                      \\\n    ops = pc + 1;           \\\n    pc += nops + 1;         \\\n  } while (0)\n#endif\n\n#if DIRECT_THREADED_DISPATCH\n  void **ltab = dispatch_label_tab;\n\n#define LAB_EL(i) ltab[i] = &&L_##i\n#define REP_SEP ;\n  if (bp == NULL) {\n    REP4 (LAB_EL, MIR_MOV, MIR_FMOV, MIR_DMOV, MIR_LDMOV);\n    REP6 (LAB_EL, MIR_EXT8, MIR_EXT16, MIR_EXT32, MIR_UEXT8, MIR_UEXT16, MIR_UEXT32);\n    REP6 (LAB_EL, MIR_I2F, MIR_I2D, MIR_I2LD, MIR_UI2F, MIR_UI2D, MIR_UI2LD);\n    REP8 (LAB_EL, MIR_F2I, MIR_D2I, MIR_LD2I, MIR_F2D, MIR_F2LD, MIR_D2F, MIR_D2LD, MIR_LD2F);\n    REP6 (LAB_EL, MIR_LD2D, MIR_NEG, MIR_NEGS, MIR_FNEG, MIR_DNEG, MIR_LDNEG);\n    REP6 (LAB_EL, MIR_ADDR, MIR_ADDR8, MIR_ADDR16, MIR_ADDR32, MIR_ADD, MIR_ADDS);\n    REP8 (LAB_EL, MIR_FADD, MIR_DADD, MIR_LDADD, MIR_SUB, MIR_SUBS, MIR_FSUB, MIR_DSUB, MIR_LDSUB);\n    REP8 (LAB_EL, MIR_MUL, MIR_MULS, MIR_FMUL, MIR_DMUL, MIR_LDMUL, MIR_DIV, MIR_DIVS, MIR_UDIV);\n    REP8 (LAB_EL, MIR_UDIVS, MIR_FDIV, MIR_DDIV, MIR_LDDIV, MIR_MOD, MIR_MODS, MIR_UMOD, MIR_UMODS);\n    REP8 (LAB_EL, MIR_AND, MIR_ANDS, MIR_OR, MIR_ORS, MIR_XOR, MIR_XORS, MIR_LSH, MIR_LSHS);\n    REP8 (LAB_EL, MIR_RSH, MIR_RSHS, MIR_URSH, MIR_URSHS, MIR_EQ, MIR_EQS, MIR_FEQ, MIR_DEQ);\n    REP8 (LAB_EL, MIR_LDEQ, MIR_NE, MIR_NES, MIR_FNE, MIR_DNE, MIR_LDNE, MIR_LT, MIR_LTS);\n    REP8 (LAB_EL, MIR_ULT, MIR_ULTS, MIR_FLT, MIR_DLT, MIR_LDLT, MIR_LE, MIR_LES, MIR_ULE);\n    REP8 (LAB_EL, MIR_ULES, MIR_FLE, MIR_DLE, MIR_LDLE, MIR_GT, MIR_GTS, MIR_UGT, MIR_UGTS);\n    REP8 (LAB_EL, MIR_FGT, MIR_DGT, MIR_LDGT, MIR_GE, MIR_GES, MIR_UGE, MIR_UGES, MIR_FGE);\n    REP6 (LAB_EL, MIR_DGE, MIR_LDGE, MIR_ADDO, MIR_ADDOS, MIR_SUBO, MIR_SUBOS);\n    REP4 (LAB_EL, MIR_MULO, MIR_MULOS, MIR_UMULO, MIR_UMULOS);\n    REP6 (LAB_EL, MIR_JMP, MIR_BT, MIR_BTS, MIR_BF, MIR_BFS, MIR_BEQ);\n    REP8 (LAB_EL, MIR_BEQS, MIR_FBEQ, MIR_DBEQ, MIR_LDBEQ, MIR_BNE, MIR_BNES, MIR_FBNE, MIR_DBNE);\n    REP8 (LAB_EL, MIR_LDBNE, MIR_BLT, MIR_BLTS, MIR_UBLT, MIR_UBLTS, MIR_FBLT, MIR_DBLT, MIR_LDBLT);\n    REP8 (LAB_EL, MIR_BLE, MIR_BLES, MIR_UBLE, MIR_UBLES, MIR_FBLE, MIR_DBLE, MIR_LDBLE, MIR_BGT);\n    REP8 (LAB_EL, MIR_BGTS, MIR_UBGT, MIR_UBGTS, MIR_FBGT, MIR_DBGT, MIR_LDBGT, MIR_BGE, MIR_BGES);\n    REP5 (LAB_EL, MIR_UBGE, MIR_UBGES, MIR_FBGE, MIR_DBGE, MIR_LDBGE);\n    REP6 (LAB_EL, MIR_BO, MIR_UBO, MIR_BNO, MIR_UBNO, MIR_LADDR, MIR_JMPI);\n    REP6 (LAB_EL, MIR_CALL, MIR_INLINE, MIR_JCALL, MIR_SWITCH, MIR_RET, MIR_JRET);\n    REP3 (LAB_EL, MIR_ALLOCA, MIR_BSTART, MIR_BEND);\n    REP4 (LAB_EL, MIR_VA_ARG, MIR_VA_BLOCK_ARG, MIR_VA_START, MIR_VA_END);\n    REP8 (LAB_EL, IC_LDI8, IC_LDU8, IC_LDI16, IC_LDU16, IC_LDI32, IC_LDU32, IC_LDI64, IC_LDF);\n    REP8 (LAB_EL, IC_LDD, IC_LDLD, IC_STI8, IC_STU8, IC_STI16, IC_STU16, IC_STI32, IC_STU32);\n    REP8 (LAB_EL, IC_STI64, IC_STF, IC_STD, IC_STLD, IC_MOVI, IC_MOVP, IC_MOVF, IC_MOVD);\n    REP3 (LAB_EL, IC_MOVLD, IC_IMM_CALL, IC_IMM_JCALL);\n    REP4 (LAB_EL, IC_MOVFG, IC_FMOVFG, IC_DMOVFG, IC_LDMOVFG);\n    REP4 (LAB_EL, IC_MOVTG, IC_FMOVTG, IC_DMOVTG, IC_LDMOVTG);\n    return;\n  }\n#undef REP_SEP\n#define CASE0(value) L_##value:\n\n#if MIR_INTERP_TRACE\n#define END_INSN                                     \\\n  finish_insn_trace (ctx, trace_insn_code, ops, bp); \\\n  goto * pc->a\n#else\n#define END_INSN goto * pc->a\n#endif\n\n#else\n\n#define CASE0(value) case value:\n\n#if MIR_INTERP_TRACE\n#define END_INSN                                     \\\n  finish_insn_trace (ctx, trace_insn_code, ops, bp); \\\n  break\n#else\n#define END_INSN break\n#endif\n\n#endif\n\n#define CASE(value, nops) CASE0 (value) START_INSN (value, nops);\n\n#define SCASE(insn, nop, stmt) \\\n  CASE (insn, nop) {           \\\n    stmt;                      \\\n    END_INSN;                  \\\n  }\n\n  code = func_desc->code;\n  pc = code;\n\n#if DIRECT_THREADED_DISPATCH\n  goto * pc->a;\n#else\n  for (;;) {\n    int insn_code = pc->ic;\n    switch (insn_code) {\n#endif\n\n#if 0\n    L_jmpi_finish :\n#endif\n  { /* jmpi thunk return */\n    pc = jmpi_val;\n    END_INSN;\n  }\n\n  CASE (MIR_MOV, 2) {\n    int64_t p, *r = get_2iops (bp, ops, &p);\n    *r = p;\n    END_INSN;\n  }\n  CASE (MIR_FMOV, 2) {\n    float p, *r = get_2fops (bp, ops, &p);\n    *r = p;\n    END_INSN;\n  }\n  CASE (MIR_DMOV, 2) {\n    double p, *r = get_2dops (bp, ops, &p);\n    *r = p;\n    END_INSN;\n  }\n  CASE (MIR_LDMOV, 2) {\n    long double p, *r = get_2ldops (bp, ops, &p);\n    *r = p;\n    END_INSN;\n  }\n\n  CASE (IC_MOVFG, 2) {\n    int64_t l = get_i (ops), r = get_i (ops + 1);\n    bp[l].i = globals[r].i;\n    END_INSN;\n  }\n  CASE (IC_FMOVFG, 2) {\n    int64_t l = get_i (ops), r = get_i (ops + 1);\n    bp[l].f = globals[r].f;\n    END_INSN;\n  }\n  CASE (IC_DMOVFG, 2) {\n    int64_t l = get_i (ops), r = get_i (ops + 1);\n    bp[l].d = globals[r].d;\n    END_INSN;\n  }\n  CASE (IC_LDMOVFG, 2) {\n    int64_t l = get_i (ops), r = get_i (ops + 1);\n    bp[l].ld = globals[r].ld;\n    END_INSN;\n  }\n\n  CASE (IC_MOVTG, 2) {\n    int64_t l = get_i (ops), r = get_i (ops + 1);\n    globals[l].i = bp[r].i;\n    END_INSN;\n  }\n  CASE (IC_FMOVTG, 2) {\n    int64_t l = get_i (ops), r = get_i (ops + 1);\n    globals[l].f = bp[r].f;\n    END_INSN;\n  }\n  CASE (IC_DMOVTG, 2) {\n    int64_t l = get_i (ops), r = get_i (ops + 1);\n    globals[l].d = bp[r].d;\n    END_INSN;\n  }\n  CASE (IC_LDMOVTG, 2) {\n    int64_t l = get_i (ops), r = get_i (ops + 1);\n    globals[l].ld = bp[r].ld;\n    END_INSN;\n  }\n\n  SCASE (MIR_EXT8, 2, EXT (int8_t));\n  SCASE (MIR_EXT16, 2, EXT (int16_t));\n  SCASE (MIR_EXT32, 2, EXT (int32_t));\n  SCASE (MIR_UEXT8, 2, EXT (uint8_t));\n  SCASE (MIR_UEXT16, 2, EXT (uint16_t));\n  SCASE (MIR_UEXT32, 2, EXT (uint32_t));\n  CASE (MIR_I2F, 2) {\n    float *r = get_fop (bp, ops);\n    int64_t i = *get_iop (bp, ops + 1);\n\n    *r = (float) i;\n    END_INSN;\n  }\n  CASE (MIR_I2D, 2) {\n    double *r = get_dop (bp, ops);\n    int64_t i = *get_iop (bp, ops + 1);\n\n    *r = (double) i;\n    END_INSN;\n  }\n  CASE (MIR_I2LD, 2) {\n    long double *r = get_ldop (bp, ops);\n    int64_t i = *get_iop (bp, ops + 1);\n\n    *r = (long double) i;\n    END_INSN;\n  }\n\n  CASE (MIR_UI2F, 2) {\n    float *r = get_fop (bp, ops);\n    uint64_t i = *get_iop (bp, ops + 1);\n\n    *r = (float) i;\n    END_INSN;\n  }\n  CASE (MIR_UI2D, 2) {\n    double *r = get_dop (bp, ops);\n    uint64_t i = *get_iop (bp, ops + 1);\n\n    *r = (double) i;\n    END_INSN;\n  }\n  CASE (MIR_UI2LD, 2) {\n    long double *r = get_ldop (bp, ops);\n    uint64_t i = *get_iop (bp, ops + 1);\n\n    *r = (long double) i;\n    END_INSN;\n  }\n\n  CASE (MIR_F2I, 2) {\n    int64_t *r = get_iop (bp, ops);\n    float f = *get_fop (bp, ops + 1);\n\n    *r = (int64_t) f;\n    END_INSN;\n  }\n  CASE (MIR_D2I, 2) {\n    int64_t *r = get_iop (bp, ops);\n    double d = *get_dop (bp, ops + 1);\n\n    *r = (int64_t) d;\n    END_INSN;\n  }\n  CASE (MIR_LD2I, 2) {\n    int64_t *r = get_iop (bp, ops);\n    long double ld = *get_ldop (bp, ops + 1);\n\n    *r = (int64_t) ld;\n    END_INSN;\n  }\n\n  CASE (MIR_F2D, 2) {\n    double *r = get_dop (bp, ops);\n    float f = *get_fop (bp, ops + 1);\n    *r = f;\n    END_INSN;\n  }\n  CASE (MIR_F2LD, 2) {\n    long double *r = get_ldop (bp, ops);\n    float f = *get_fop (bp, ops + 1);\n\n    *r = f;\n    END_INSN;\n  }\n  CASE (MIR_D2F, 2) {\n    float *r = get_fop (bp, ops);\n    double d = *get_dop (bp, ops + 1);\n\n    *r = (float) d;\n    END_INSN;\n  }\n  CASE (MIR_D2LD, 2) {\n    long double *r = get_ldop (bp, ops);\n    double d = *get_dop (bp, ops + 1);\n\n    *r = d;\n    END_INSN;\n  }\n  CASE (MIR_LD2F, 2) {\n    float *r = get_fop (bp, ops);\n    long double ld = *get_ldop (bp, ops + 1);\n\n    *r = (float) ld;\n    END_INSN;\n  }\n\n  CASE (MIR_LD2D, 2) {\n    double *r = get_dop (bp, ops);\n    long double ld = *get_ldop (bp, ops + 1);\n\n    *r = ld;\n    END_INSN;\n  }\n\n  SCASE (MIR_NEG, 2, IOP2 (-));\n  SCASE (MIR_NEGS, 2, IOP2S (-));\n  SCASE (MIR_FNEG, 2, FOP2 (-));\n  SCASE (MIR_DNEG, 2, DOP2 (-));\n  SCASE (MIR_LDNEG, 2, LDOP2 (-));\n\n  CASE (MIR_ADDR8, 2) {\n    offset = addr_offset8;\n    goto common_addr;\n  }\n  CASE (MIR_ADDR16, 2) {\n    offset = addr_offset16;\n    goto common_addr;\n  }\n  CASE (MIR_ADDR32, 2) {\n    offset = addr_offset32;\n    goto common_addr;\n  }\n  CASE (MIR_ADDR, 2)\n  offset = 0;\ncommon_addr:;\n  {\n    int64_t *r = get_iop (bp, ops);\n    void **p = get_aop (bp, ops + 1);\n\n    *r = (int64_t) p + offset;\n    END_INSN;\n  }\n\n  SCASE (MIR_ADD, 3, IOP3 (+));\n  SCASE (MIR_ADDS, 3, IOP3S (+));\n  SCASE (MIR_FADD, 3, FOP3 (+));\n  SCASE (MIR_DADD, 3, DOP3 (+));\n  SCASE (MIR_LDADD, 3, LDOP3 (+));\n\n  SCASE (MIR_SUB, 3, IOP3 (-));\n  SCASE (MIR_SUBS, 3, IOP3S (-));\n  SCASE (MIR_FSUB, 3, FOP3 (-));\n  SCASE (MIR_DSUB, 3, DOP3 (-));\n  SCASE (MIR_LDSUB, 3, LDOP3 (-));\n\n  SCASE (MIR_MUL, 3, IOP3 (*));\n  SCASE (MIR_MULS, 3, IOP3S (*));\n  SCASE (MIR_FMUL, 3, FOP3 (*));\n  SCASE (MIR_DMUL, 3, DOP3 (*));\n  SCASE (MIR_LDMUL, 3, LDOP3 (*));\n\n  SCASE (MIR_DIV, 3, IOP3 (/));\n  SCASE (MIR_DIVS, 3, IOP3S (/));\n  SCASE (MIR_UDIV, 3, UOP3 (/));\n  SCASE (MIR_UDIVS, 3, UOP3S (/));\n  SCASE (MIR_FDIV, 3, FOP3 (/));\n  SCASE (MIR_DDIV, 3, DOP3 (/));\n  SCASE (MIR_LDDIV, 3, LDOP3 (/));\n\n  SCASE (MIR_MOD, 3, IOP3 (%));\n  SCASE (MIR_MODS, 3, IOP3S (%));\n  SCASE (MIR_UMOD, 3, UOP3 (%));\n  SCASE (MIR_UMODS, 3, UOP3S (%));\n\n  SCASE (MIR_AND, 3, IOP3 (&));\n  SCASE (MIR_ANDS, 3, IOP3S (&));\n  SCASE (MIR_OR, 3, IOP3 (|));\n  SCASE (MIR_ORS, 3, IOP3S (|));\n  SCASE (MIR_XOR, 3, IOP3 (^));\n  SCASE (MIR_XORS, 3, IOP3S (^));\n  SCASE (MIR_LSH, 3, IOP3 (<<));\n  SCASE (MIR_LSHS, 3, IOP3S (<<));\n\n  SCASE (MIR_RSH, 3, IOP3 (>>));\n  SCASE (MIR_RSHS, 3, IOP3S (>>));\n  SCASE (MIR_URSH, 3, UIOP3 (>>));\n  SCASE (MIR_URSHS, 3, UIOP3S (>>));\n\n  SCASE (MIR_EQ, 3, ICMP (==));\n  SCASE (MIR_EQS, 3, ICMPS (==));\n  SCASE (MIR_FEQ, 3, FCMP (==));\n  SCASE (MIR_DEQ, 3, DCMP (==));\n  SCASE (MIR_LDEQ, 3, LDCMP (==));\n\n  SCASE (MIR_NE, 3, ICMP (!=));\n  SCASE (MIR_NES, 3, ICMPS (!=));\n  SCASE (MIR_FNE, 3, FCMP (!=));\n  SCASE (MIR_DNE, 3, DCMP (!=));\n  SCASE (MIR_LDNE, 3, LDCMP (!=));\n\n  SCASE (MIR_LT, 3, ICMP (<));\n  SCASE (MIR_LTS, 3, ICMPS (<));\n  SCASE (MIR_ULT, 3, UCMP (<));\n  SCASE (MIR_ULTS, 3, UCMPS (<));\n  SCASE (MIR_FLT, 3, FCMP (<));\n  SCASE (MIR_DLT, 3, DCMP (<));\n  SCASE (MIR_LDLT, 3, LDCMP (<));\n\n  SCASE (MIR_LE, 3, ICMP (<=));\n  SCASE (MIR_LES, 3, ICMPS (<=));\n  SCASE (MIR_ULE, 3, UCMP (<=));\n  SCASE (MIR_ULES, 3, UCMPS (<=));\n  SCASE (MIR_FLE, 3, FCMP (<=));\n  SCASE (MIR_DLE, 3, DCMP (<=));\n  SCASE (MIR_LDLE, 3, LDCMP (<=));\n\n  SCASE (MIR_GT, 3, ICMP (>));\n  SCASE (MIR_GTS, 3, ICMPS (>));\n  SCASE (MIR_UGT, 3, UCMP (>));\n  SCASE (MIR_UGTS, 3, UCMPS (>));\n  SCASE (MIR_FGT, 3, FCMP (>));\n  SCASE (MIR_DGT, 3, DCMP (>));\n  SCASE (MIR_LDGT, 3, LDCMP (>));\n\n  SCASE (MIR_GE, 3, ICMP (>=));\n  SCASE (MIR_GES, 3, ICMPS (>=));\n  SCASE (MIR_UGE, 3, UCMP (>=));\n  SCASE (MIR_UGES, 3, UCMPS (>=));\n  SCASE (MIR_FGE, 3, FCMP (>=));\n  SCASE (MIR_DGE, 3, DCMP (>=));\n  SCASE (MIR_LDGE, 3, LDCMP (>=));\n\n  CASE (MIR_ADDO, 3) {\n    int64_t *r = get_iop (bp, ops);\n    int64_t op1 = *get_iop (bp, ops + 1), op2 = *get_iop (bp, ops + 2);\n    unsigned_overflow_p = (uint64_t) op1 > UINT64_MAX - (uint64_t) op2;\n    signed_overflow_p = op2 >= 0 ? op1 > INT64_MAX - op2 : op1 < INT64_MIN - op2;\n    *r = op1 + op2;\n    END_INSN;\n  }\n\n  CASE (MIR_ADDOS, 3) {\n    int64_t *r = get_iop (bp, ops);\n    int32_t op1 = (int32_t) *get_iop (bp, ops + 1), op2 = (int32_t) *get_iop (bp, ops + 2);\n    unsigned_overflow_p = (uint32_t) op1 > UINT32_MAX - (uint32_t) op2;\n    signed_overflow_p = op2 >= 0 ? op1 > INT32_MAX - op2 : op1 < INT32_MIN - op2;\n    *r = op1 + op2;\n    END_INSN;\n  }\n\n  CASE (MIR_SUBO, 3) {\n    int64_t *r = get_iop (bp, ops);\n    int64_t op1 = *get_iop (bp, ops + 1), op2 = *get_iop (bp, ops + 2);\n    unsigned_overflow_p = (uint64_t) op1 < (uint64_t) op2;\n    signed_overflow_p = op2 < 0 ? op1 > INT64_MAX + op2 : op1 < INT64_MIN + op2;\n    *r = op1 - op2;\n    END_INSN;\n  }\n\n  CASE (MIR_SUBOS, 3) {\n    int64_t *r = get_iop (bp, ops);\n    int32_t op1 = (int32_t) *get_iop (bp, ops + 1), op2 = (int32_t) *get_iop (bp, ops + 2);\n    unsigned_overflow_p = (uint32_t) op1 < (uint32_t) op2;\n    signed_overflow_p = op2 < 0 ? op1 > INT32_MAX + op2 : op1 < INT32_MIN + op2;\n    *r = op1 - op2;\n    END_INSN;\n  }\n\n  CASE (MIR_MULO, 3) {\n    int64_t *r = get_iop (bp, ops);\n    int64_t op1 = *get_iop (bp, ops + 1), op2 = *get_iop (bp, ops + 2);\n    signed_overflow_p = (op1 == 0    ? FALSE\n                         : op1 == -1 ? op2 < -INT64_MAX\n                         : op1 > 0   ? (op2 > 0 ? INT64_MAX / op1 < op2 : INT64_MIN / op1 > op2)\n                                     : (op2 > 0 ? INT64_MIN / op1 < op2 : INT64_MAX / op1 > op2));\n    *r = op1 * op2;\n    END_INSN;\n  }\n\n  CASE (MIR_MULOS, 3) {\n    int64_t *r = get_iop (bp, ops);\n    int32_t op1 = (int32_t) *get_iop (bp, ops + 1), op2 = (int32_t) *get_iop (bp, ops + 2);\n    signed_overflow_p = (op1 == 0    ? FALSE\n                         : op1 == -1 ? op2 < -INT32_MAX\n                         : op1 > 0   ? (op2 > 0 ? INT32_MAX / op1 < op2 : INT32_MIN / op1 > op2)\n                                     : (op2 > 0 ? INT32_MIN / op1 < op2 : INT32_MAX / op1 > op2));\n    *r = op1 * op2;\n    END_INSN;\n  }\n\n  CASE (MIR_UMULO, 3) {\n    uint64_t *r = get_uop (bp, ops);\n    uint64_t op1 = *get_uop (bp, ops + 1), op2 = *get_uop (bp, ops + 2);\n    unsigned_overflow_p = op1 == 0 ? FALSE : UINT64_MAX / op1 < op2;\n    *r = op1 * op2;\n    END_INSN;\n  }\n\n  CASE (MIR_UMULOS, 3) {\n    uint64_t *r = get_uop (bp, ops);\n    uint32_t op1 = (uint32_t) *get_uop (bp, ops + 1), op2 = (uint32_t) *get_uop (bp, ops + 2);\n    unsigned_overflow_p = op1 == 0 ? FALSE : UINT32_MAX / op1 < op2;\n    *r = op1 * op2;\n    END_INSN;\n  }\n\n  SCASE (MIR_JMP, 1, pc = code + get_i (ops));\n  CASE (MIR_BT, 2) {\n    int64_t cond = *get_iop (bp, ops + 1);\n\n    if (cond) pc = code + get_i (ops);\n    END_INSN;\n  }\n  CASE (MIR_BF, 2) {\n    int64_t cond = *get_iop (bp, ops + 1);\n\n    if (!cond) pc = code + get_i (ops);\n    END_INSN;\n  }\n  CASE (MIR_BTS, 2) {\n    int32_t cond = (int32_t) *get_iop (bp, ops + 1);\n\n    if (cond) pc = code + get_i (ops);\n    END_INSN;\n  }\n  CASE (MIR_BFS, 2) {\n    int32_t cond = (int32_t) *get_iop (bp, ops + 1);\n\n    if (!cond) pc = code + get_i (ops);\n    END_INSN;\n  }\n  SCASE (MIR_BEQ, 3, BICMP (==));\n  SCASE (MIR_BEQS, 3, BICMPS (==));\n  SCASE (MIR_FBEQ, 3, BFCMP (==));\n  SCASE (MIR_DBEQ, 3, BDCMP (==));\n  SCASE (MIR_LDBEQ, 3, BLDCMP (==));\n  SCASE (MIR_BNE, 3, BICMP (!=));\n  SCASE (MIR_BNES, 3, BICMPS (!=));\n  SCASE (MIR_FBNE, 3, BFCMP (!=));\n  SCASE (MIR_DBNE, 3, BDCMP (!=));\n  SCASE (MIR_LDBNE, 3, BLDCMP (!=));\n  SCASE (MIR_BLT, 3, BICMP (<));\n  SCASE (MIR_BLTS, 3, BICMPS (<));\n  SCASE (MIR_UBLT, 3, BUCMP (<));\n  SCASE (MIR_UBLTS, 3, BUCMPS (<));\n  SCASE (MIR_FBLT, 3, BFCMP (<));\n  SCASE (MIR_DBLT, 3, BDCMP (<));\n  SCASE (MIR_LDBLT, 3, BLDCMP (<));\n  SCASE (MIR_BLE, 3, BICMP (<=));\n  SCASE (MIR_BLES, 3, BICMPS (<=));\n  SCASE (MIR_UBLE, 3, BUCMP (<=));\n  SCASE (MIR_UBLES, 3, BUCMPS (<=));\n  SCASE (MIR_FBLE, 3, BFCMP (<=));\n  SCASE (MIR_DBLE, 3, BDCMP (<=));\n  SCASE (MIR_LDBLE, 3, BLDCMP (<=));\n  SCASE (MIR_BGT, 3, BICMP (>));\n  SCASE (MIR_BGTS, 3, BICMPS (>));\n  SCASE (MIR_UBGT, 3, BUCMP (>));\n  SCASE (MIR_UBGTS, 3, BUCMPS (>));\n  SCASE (MIR_FBGT, 3, BFCMP (>));\n  SCASE (MIR_DBGT, 3, BDCMP (>));\n  SCASE (MIR_LDBGT, 3, BLDCMP (>));\n  SCASE (MIR_BGE, 3, BICMP (>=));\n  SCASE (MIR_BGES, 3, BICMPS (>=));\n  SCASE (MIR_UBGE, 3, BUCMP (>=));\n  SCASE (MIR_UBGES, 3, BUCMPS (>=));\n  SCASE (MIR_FBGE, 3, BFCMP (>=));\n  SCASE (MIR_DBGE, 3, BDCMP (>=));\n  SCASE (MIR_LDBGE, 3, BLDCMP (>=));\n\n  CASE (MIR_BO, 1) {\n    if (signed_overflow_p) pc = code + get_i (ops);\n    END_INSN;\n  }\n  CASE (MIR_UBO, 1) {\n    if (unsigned_overflow_p) pc = code + get_i (ops);\n    END_INSN;\n  }\n  CASE (MIR_BNO, 1) {\n    if (!signed_overflow_p) pc = code + get_i (ops);\n    END_INSN;\n  }\n  CASE (MIR_UBNO, 1) {\n    if (!unsigned_overflow_p) pc = code + get_i (ops);\n    END_INSN;\n  }\n  CASE (MIR_LADDR, 2) {\n    void **r = get_aop (bp, ops);\n    *r = code + get_i (ops + 1);\n    END_INSN;\n  }\n\n  CASE (MIR_JMPI, 1) { /* jmpi thunk */\n    void **r = get_aop (bp, ops);\n    pc = *r;\n    END_INSN;\n  }\n\n  CASE (MIR_CALL, 0) {\n    int (*func_addr) (void *buf) = *get_aop (bp, ops + 4);\n\n    if (func_addr != setjmp_addr) {\n      pc = call_insn_execute (ctx, pc, bp, ops, FALSE);\n    } else {\n      int res;\n      int64_t nops = get_i (ops); /* #args w/o nop, insn, and ff interface address */\n      MIR_item_t proto_item = get_a (ops + 3);\n      size_t start = proto_item->u.proto->nres + 5;\n      bp[-2].a = pc;\n      res = (*func_addr) (*get_aop (bp, ops + start));\n      ops = pc = bp[-2].a;\n      nops = get_i (ops);\n      bp[get_i (ops + 5)].i = res;\n      pc += nops + 3; /* nops itself, the call insn, add ff interface address */\n    }\n    END_INSN;\n  }\n  CASE (IC_IMM_CALL, 0) {\n    int (*func_addr) (void *buf) = get_a (ops + 4);\n\n    if (func_addr != setjmp_addr) {\n      pc = call_insn_execute (ctx, pc, bp, ops, TRUE);\n    } else {\n      int res;\n      int64_t nops = get_i (ops); /* #args w/o nop, insn, and ff interface address */\n      MIR_item_t proto_item = get_a (ops + 3);\n      size_t start = proto_item->u.proto->nres + 5;\n      bp[-2].a = pc;\n      res = (*func_addr) (*get_aop (bp, ops + start));\n      ops = pc = bp[-2].a;\n      nops = get_i (ops);\n      bp[get_i (ops + 5)].i = res;\n      pc += nops + 3; /* nops itself, the call insn, add ff interface address */\n    }\n    END_INSN;\n  }\n\n  SCASE (MIR_INLINE, 0, mir_assert (FALSE));\n\n  CASE (MIR_JCALL, 0) {\n    int (*func_addr) (void *buf) = *get_aop (bp, ops + 4);\n    if (func_addr == setjmp_addr)\n      (*MIR_get_error_func (ctx)) (MIR_invalid_insn_error, \"jcall of setjmp\");\n    call_insn_execute (ctx, pc, bp, ops, FALSE);\n    pc = jret_addr;\n    END_INSN;\n  }\n  CASE (IC_IMM_JCALL, 0) {\n    int (*func_addr) (void *buf) = get_a (ops + 4);\n    if (func_addr == setjmp_addr)\n      (*MIR_get_error_func (ctx)) (MIR_invalid_insn_error, \"jcall of setjmp\");\n    call_insn_execute (ctx, pc, bp, ops, TRUE);\n    pc = jret_addr;\n    END_INSN;\n  }\n\n  CASE (MIR_SWITCH, 0) {\n    int64_t nops = get_i (ops); /* #ops */\n    int64_t index = *get_iop (bp, ops + 1);\n\n    mir_assert (index + 1 < nops);\n    pc = code + get_i (ops + index + 2);\n    END_INSN;\n  }\n\n  CASE (MIR_RET, 0) {\n    int64_t nops = get_i (ops); /* #ops */\n    for (int64_t i = 0; i < nops; i++) results[i] = bp[get_i (ops + i + 1)];\n    pc += nops + 1;\n    return;\n    END_INSN;\n  }\n\n  CASE (MIR_JRET, 0) {\n    jret_addr = bp[get_i (ops)].a; /* pc for continuation */\n    return;\n    END_INSN;\n  }\n\n  CASE (MIR_ALLOCA, 2) {\n    int64_t *r, s;\n\n    r = get_2iops (bp, ops, &s);\n    *r = (uint64_t) alloca (s);\n    END_INSN;\n  }\n  CASE (MIR_BSTART, 1) {\n    void **p = get_aop (bp, ops);\n\n    *p = bstart_builtin ();\n    END_INSN;\n  }\n  SCASE (MIR_BEND, 1, bend_builtin (*get_aop (bp, ops)));\n  CASE (MIR_VA_ARG, 3) {\n    int64_t *r, va, tp;\n\n    r = get_2iops (bp, ops, &va);\n    tp = get_i (ops + 2);\n    *r = (uint64_t) va_arg_builtin ((void *) va, tp);\n    END_INSN;\n  }\n  CASE (MIR_VA_BLOCK_ARG, 4) {\n    int64_t *r, va, size;\n\n    r = get_3iops (bp, ops, &va, &size);\n    va_block_arg_builtin ((void *) *r, (void *) va, size, *get_iop (bp, ops + 3));\n    END_INSN;\n  }\n  SCASE (MIR_VA_START, 1, va_start_interp_builtin (ctx, bp[get_i (ops)].a, bp[-1].a));\n  SCASE (MIR_VA_END, 1, va_end_interp_builtin (ctx, bp[get_i (ops)].a));\n\n  SCASE (IC_LDI8, 2, LD (iop, int64_t, int8_t));\n  SCASE (IC_LDU8, 2, LD (uop, uint64_t, uint8_t));\n  SCASE (IC_LDI16, 2, LD (iop, int64_t, int16_t));\n  SCASE (IC_LDU16, 2, LD (uop, uint64_t, uint16_t));\n  SCASE (IC_LDI32, 2, LD (iop, int64_t, int32_t));\n  SCASE (IC_LDU32, 2, LD (uop, uint64_t, uint32_t));\n  SCASE (IC_LDI64, 2, LD (iop, int64_t, int64_t));\n  SCASE (IC_LDF, 2, LD (fop, float, float));\n  SCASE (IC_LDD, 2, LD (dop, double, double));\n  SCASE (IC_LDLD, 2, LD (ldop, long double, long double));\n  CASE (IC_MOVP, 2) {\n    void **r = get_aop (bp, ops), *a = get_a (ops + 1);\n    *r = a;\n    END_INSN;\n  }\n  SCASE (IC_STI8, 2, ST (iop, int64_t, int8_t));\n  SCASE (IC_STU8, 2, ST (iop, uint64_t, uint8_t));\n  SCASE (IC_STI16, 2, ST (iop, int64_t, int16_t));\n  SCASE (IC_STU16, 2, ST (iop, uint64_t, uint16_t));\n  SCASE (IC_STI32, 2, ST (iop, int64_t, int32_t));\n  SCASE (IC_STU32, 2, ST (iop, uint64_t, uint32_t));\n  SCASE (IC_STI64, 2, ST (iop, int64_t, int64_t));\n  SCASE (IC_STF, 2, ST (fop, float, float));\n  SCASE (IC_STD, 2, ST (dop, double, double));\n  SCASE (IC_STLD, 2, ST (ldop, long double, long double));\n  CASE (IC_MOVI, 2) {\n    int64_t *r = get_iop (bp, ops), imm = get_i (ops + 1);\n    *r = imm;\n    END_INSN;\n  }\n  CASE (IC_MOVF, 2) {\n    float *r = get_fop (bp, ops), imm = get_f (ops + 1);\n    *r = imm;\n    END_INSN;\n  }\n  CASE (IC_MOVD, 2) {\n    double *r = get_dop (bp, ops), imm = get_d (ops + 1);\n    *r = imm;\n    END_INSN;\n  }\n  CASE (IC_MOVLD, 2) {\n    long double *r = get_ldop (bp, ops), imm = get_ld (ops + 1);\n    *r = imm;\n    END_INSN;\n  }\n#if !DIRECT_THREADED_DISPATCH\ndefault: mir_assert (FALSE);\n}\n}\n#endif\n}\n\nstatic inline func_desc_t get_func_desc (MIR_item_t func_item) {\n  mir_assert (func_item->item_type == MIR_func_item);\n  return func_item->data;\n}\n\nstatic htab_hash_t ff_interface_hash (ff_interface_t i, void *arg MIR_UNUSED) {\n  htab_hash_t h = (htab_hash_t) mir_hash_step (mir_hash_init (0), i->nres);\n  h = (htab_hash_t) mir_hash_step (h, i->nargs);\n  h = (htab_hash_t) mir_hash_step (h, i->arg_vars_num);\n  h = (htab_hash_t) mir_hash (i->res_types, sizeof (MIR_type_t) * i->nres, h);\n  for (size_t n = 0; n < i->nargs; n++) {\n    h = (htab_hash_t) mir_hash_step (h, i->arg_descs[n].type);\n    if (MIR_all_blk_type_p (i->arg_descs[n].type))\n      h = (htab_hash_t) mir_hash_step (h, i->arg_descs[n].size);\n  }\n  return (htab_hash_t) mir_hash_finish (h);\n}\n\nstatic int ff_interface_eq (ff_interface_t i1, ff_interface_t i2, void *arg MIR_UNUSED) {\n  if (i1->nres != i2->nres || i1->nargs != i2->nargs || i1->arg_vars_num != i2->arg_vars_num)\n    return FALSE;\n  if (memcmp (i1->res_types, i2->res_types, sizeof (MIR_type_t) * i1->nres) != 0) return FALSE;\n  for (size_t n = 0; n < i1->nargs; n++) {\n    if (i1->arg_descs[n].type != i2->arg_descs[n].type) return FALSE;\n    if (MIR_all_blk_type_p (i1->arg_descs[n].type)\n        && i1->arg_descs[n].size != i2->arg_descs[n].size)\n      return FALSE;\n  }\n  return TRUE;\n}\n\nstatic void ff_interface_clear (ff_interface_t ffi, void *arg) {\n  MIR_alloc_t alloc = (MIR_alloc_t) arg;\n  MIR_free (alloc, ffi);\n}\n\nstatic void *get_ff_interface (MIR_context_t ctx, size_t arg_vars_num, size_t nres,\n                               MIR_type_t *res_types, size_t nargs, _MIR_arg_desc_t *arg_descs,\n                               int vararg_p MIR_UNUSED) {\n  struct interp_ctx *interp_ctx = ctx->interp_ctx;\n  struct ff_interface ffi_s;\n  ff_interface_t tab_ffi, ffi;\n  int htab_res;\n\n  ffi_s.arg_vars_num = arg_vars_num;\n  ffi_s.nres = nres;\n  ffi_s.nargs = nargs;\n  ffi_s.res_types = res_types;\n  ffi_s.arg_descs = arg_descs;\n  if (HTAB_DO (ff_interface_t, ff_interface_tab, &ffi_s, HTAB_FIND, tab_ffi))\n    return tab_ffi->interface_addr;\n  ffi = MIR_malloc (ctx->alloc, sizeof (struct ff_interface) + sizeof (_MIR_arg_desc_t) * nargs\n                    + sizeof (MIR_type_t) * nres);\n  ffi->arg_vars_num = arg_vars_num;\n  ffi->nres = nres;\n  ffi->nargs = nargs;\n  ffi->arg_descs = (_MIR_arg_desc_t *) ((char *) ffi + sizeof (struct ff_interface));\n  ffi->res_types = (MIR_type_t *) ((char *) ffi->arg_descs + nargs * sizeof (_MIR_arg_desc_t));\n  memcpy (ffi->res_types, res_types, sizeof (MIR_type_t) * nres);\n  memcpy (ffi->arg_descs, arg_descs, sizeof (_MIR_arg_desc_t) * nargs);\n  ffi->interface_addr\n    = _MIR_get_ff_call (ctx, nres, res_types, nargs, call_arg_descs, arg_vars_num);\n  htab_res = HTAB_DO (ff_interface_t, ff_interface_tab, ffi, HTAB_INSERT, tab_ffi);\n  mir_assert (!htab_res && ffi == tab_ffi);\n  return ffi->interface_addr;\n}\n\nstatic void call (MIR_context_t ctx, MIR_val_t *bp, MIR_op_t *insn_arg_ops, code_t ffi_address_ptr,\n                  MIR_item_t proto_item, void *addr, code_t res_ops, size_t nargs) {\n  struct interp_ctx *interp_ctx = ctx->interp_ctx;\n  size_t i, arg_vars_num, nres;\n  MIR_val_t *res;\n  MIR_type_t type;\n  MIR_var_t *arg_vars = NULL;\n  MIR_proto_t proto = proto_item->u.proto;\n  MIR_op_mode_t mode;\n  void *ff_interface_addr;\n\n  if (proto->args == NULL) {\n    mir_assert (nargs == 0 && !proto->vararg_p);\n    arg_vars_num = 0;\n  } else {\n    mir_assert (nargs >= VARR_LENGTH (MIR_var_t, proto->args)\n                && (proto->vararg_p || nargs == VARR_LENGTH (MIR_var_t, proto->args)));\n    arg_vars = VARR_ADDR (MIR_var_t, proto->args);\n    arg_vars_num = VARR_LENGTH (MIR_var_t, proto->args);\n  }\n  nres = proto->nres;\n  if (VARR_EXPAND (MIR_val_t, call_res_args_varr, nargs + nres)\n      || VARR_EXPAND (_MIR_arg_desc_t, call_arg_descs_varr, nargs)) {\n    call_res_args = VARR_ADDR (MIR_val_t, call_res_args_varr);\n    call_arg_descs = VARR_ADDR (_MIR_arg_desc_t, call_arg_descs_varr);\n  }\n  if ((ff_interface_addr = ffi_address_ptr->a) == NULL) {\n    for (i = 0; i < nargs; i++) {\n      if (i < arg_vars_num) {\n        call_arg_descs[i].type = arg_vars[i].type;\n        if (MIR_all_blk_type_p (arg_vars[i].type)) call_arg_descs[i].size = arg_vars[i].size;\n        continue;\n      }\n      if (insn_arg_ops[i].mode == MIR_OP_MEM) { /* (r)block arg */\n        mir_assert (MIR_all_blk_type_p (insn_arg_ops[i].u.mem.type));\n        call_arg_descs[i].type = insn_arg_ops[i].u.mem.type;\n        call_arg_descs[i].size = insn_arg_ops[i].u.mem.disp;\n      } else {\n        mode = insn_arg_ops[i].value_mode;\n        mir_assert (mode == MIR_OP_INT || mode == MIR_OP_UINT || mode == MIR_OP_FLOAT\n                    || mode == MIR_OP_DOUBLE || mode == MIR_OP_LDOUBLE);\n        if (mode == MIR_OP_FLOAT)\n          (*MIR_get_error_func (ctx)) (MIR_call_op_error,\n                                       \"passing float variadic arg (should be passed as double)\");\n        call_arg_descs[i].type = (mode == MIR_OP_DOUBLE    ? MIR_T_D\n                                  : mode == MIR_OP_LDOUBLE ? MIR_T_LD\n                                                           : MIR_T_I64);\n      }\n    }\n    ff_interface_addr = ffi_address_ptr->a\n      = get_ff_interface (ctx, arg_vars_num, nres, proto->res_types, nargs, call_arg_descs,\n                          proto->vararg_p);\n  }\n\n  for (i = 0; i < nargs; i++) {\n    if (i >= arg_vars_num) {\n      call_res_args[i + nres] = arg_vals[i];\n      continue;\n    }\n    type = arg_vars[i].type;\n    switch (type) {\n    case MIR_T_I8: call_res_args[i + nres].i = (int8_t) (arg_vals[i].i); break;\n    case MIR_T_U8: call_res_args[i + nres].u = (uint8_t) (arg_vals[i].i); break;\n    case MIR_T_I16: call_res_args[i + nres].i = (int16_t) (arg_vals[i].i); break;\n    case MIR_T_U16: call_res_args[i + nres].u = (uint16_t) (arg_vals[i].i); break;\n    case MIR_T_I32: call_res_args[i + nres].i = (int32_t) (arg_vals[i].i); break;\n    case MIR_T_U32: call_res_args[i + nres].u = (uint32_t) (arg_vals[i].i); break;\n    case MIR_T_I64: call_res_args[i + nres].i = (int64_t) (arg_vals[i].i); break;\n    case MIR_T_U64: call_res_args[i + nres].u = (uint64_t) (arg_vals[i].i); break;\n    case MIR_T_F: call_res_args[i + nres].f = arg_vals[i].f; break;\n    case MIR_T_D: call_res_args[i + nres].d = arg_vals[i].d; break;\n    case MIR_T_LD: call_res_args[i + nres].ld = arg_vals[i].ld; break;\n    case MIR_T_P: call_res_args[i + nres].u = (uint64_t) arg_vals[i].a; break;\n    default:\n      mir_assert (MIR_all_blk_type_p (type));\n      call_res_args[i + nres].u = (uint64_t) arg_vals[i].a;\n      break;\n    }\n  }\n  ((void (*) (void *, void *)) ff_interface_addr) (addr, call_res_args); /* call */\n  for (i = 0; i < nres; i++) {\n    res = &bp[get_i (res_ops + i)];\n    switch (proto->res_types[i]) {\n    case MIR_T_I8: res->i = (int8_t) (call_res_args[i].i); break;\n    case MIR_T_U8: res->u = (uint8_t) (call_res_args[i].u); break;\n    case MIR_T_I16: res->i = (int16_t) (call_res_args[i].i); break;\n    case MIR_T_U16: res->u = (uint16_t) (call_res_args[i].u); break;\n    case MIR_T_I32: res->i = (int32_t) (call_res_args[i].i); break;\n    case MIR_T_U32: res->u = (uint32_t) (call_res_args[i].u); break;\n    case MIR_T_I64: res->i = (int64_t) (call_res_args[i].i); break;\n    case MIR_T_U64: res->u = (uint64_t) (call_res_args[i].u); break;\n    case MIR_T_F: res->f = call_res_args[i].f; break;\n    case MIR_T_D: res->d = call_res_args[i].d; break;\n    case MIR_T_LD: res->ld = call_res_args[i].ld; break;\n    case MIR_T_P: res->a = call_res_args[i].a; break;\n    default: mir_assert (FALSE);\n    }\n  }\n}\n\nstatic void interp_init (MIR_context_t ctx) {\n  MIR_alloc_t alloc = ctx->alloc;\n  struct interp_ctx *interp_ctx;\n\n  addr_offset8 = _MIR_addr_offset (ctx, MIR_ADDR8);\n  addr_offset16 = _MIR_addr_offset (ctx, MIR_ADDR16);\n  addr_offset32 = _MIR_addr_offset (ctx, MIR_ADDR32);\n  if ((interp_ctx = ctx->interp_ctx = MIR_malloc (alloc, sizeof (struct interp_ctx))) == NULL)\n    MIR_get_error_func (ctx) (MIR_alloc_error, \"Not enough memory for ctx\");\n#if DIRECT_THREADED_DISPATCH\n  eval (ctx, NULL, NULL, NULL);\n#endif\n  VARR_CREATE (MIR_insn_t, branches, alloc, 0);\n  VARR_CREATE (MIR_val_t, code_varr, alloc, 0);\n  VARR_CREATE (MIR_val_t, arg_vals_varr, alloc, 0);\n  arg_vals = VARR_ADDR (MIR_val_t, arg_vals_varr);\n  VARR_CREATE (MIR_val_t, call_res_args_varr, alloc, 0);\n  VARR_CREATE (_MIR_arg_desc_t, call_arg_descs_varr, alloc, 0);\n  call_res_args = VARR_ADDR (MIR_val_t, call_res_args_varr);\n  call_arg_descs = VARR_ADDR (_MIR_arg_desc_t, call_arg_descs_varr);\n  HTAB_CREATE_WITH_FREE_FUNC (ff_interface_t, ff_interface_tab, alloc, 1000, ff_interface_hash,\n                              ff_interface_eq, ff_interface_clear, alloc);\n#if MIR_INTERP_TRACE\n  trace_insn_ident = 0;\n#endif\n  bstart_builtin = _MIR_get_bstart_builtin (ctx);\n  bend_builtin = _MIR_get_bend_builtin (ctx);\n}\n\nstatic void interp_finish (MIR_context_t ctx) {\n  struct interp_ctx *interp_ctx = ctx->interp_ctx;\n\n  VARR_DESTROY (MIR_insn_t, branches);\n  VARR_DESTROY (MIR_val_t, code_varr);\n  VARR_DESTROY (MIR_val_t, arg_vals_varr);\n  VARR_DESTROY (MIR_val_t, call_res_args_varr);\n  VARR_DESTROY (_MIR_arg_desc_t, call_arg_descs_varr);\n  HTAB_DESTROY (ff_interface_t, ff_interface_tab);\n  /* Clear func descs???  */\n  MIR_free (ctx->alloc, ctx->interp_ctx);\n  ctx->interp_ctx = NULL;\n}\n\n#if VA_LIST_IS_ARRAY_P\ntypedef va_list va_t;\n#else\n    typedef va_list *va_t;\n#endif\n\nstatic void interp_arr_varg (MIR_context_t ctx, MIR_item_t func_item, MIR_val_t *results,\n                             size_t nargs, MIR_val_t *vals, va_t va) {\n  func_desc_t func_desc;\n  MIR_val_t *bp;\n\n  mir_assert (func_item->item_type == MIR_func_item);\n  if (func_item->data == NULL) generate_icode (ctx, func_item);\n  func_desc = get_func_desc (func_item);\n  bp = alloca ((func_desc->nregs + 2) * sizeof (MIR_val_t));\n  bp++; /* reserved for setjmp/longjmp */\n  bp[0].a = va;\n  bp++;\n  if (func_desc->nregs < nargs + 1) nargs = func_desc->nregs - 1;\n  bp[0].i = 0;\n  memcpy (&bp[1], vals, sizeof (MIR_val_t) * nargs);\n  eval (ctx, func_desc, bp, results);\n  if (va != NULL)\n#if VA_LIST_IS_ARRAY_P\n    va_end (va);\n#else\n        va_end (*va);\n#endif\n}\n\nvoid MIR_interp (MIR_context_t ctx, MIR_item_t func_item, MIR_val_t *results, size_t nargs, ...) {\n  struct interp_ctx *interp_ctx = ctx->interp_ctx;\n  va_list argp;\n  size_t i;\n\n  if (VARR_EXPAND (MIR_val_t, arg_vals_varr, nargs))\n    arg_vals = VARR_ADDR (MIR_val_t, arg_vals_varr);\n  va_start (argp, nargs);\n  for (i = 0; i < nargs; i++) arg_vals[i] = va_arg (argp, MIR_val_t);\n#if VA_LIST_IS_ARRAY_P\n  interp_arr_varg (ctx, func_item, results, nargs, arg_vals, argp);\n#else\n      interp_arr_varg (ctx, func_item, results, nargs, arg_vals, (va_t) &argp);\n#endif\n}\n\nvoid MIR_interp_arr_varg (MIR_context_t ctx, MIR_item_t func_item, MIR_val_t *results, size_t nargs,\n                          MIR_val_t *vals, va_list va) {\n  func_desc_t func_desc;\n  MIR_val_t *bp;\n\n  mir_assert (func_item->item_type == MIR_func_item);\n  if (func_item->data == NULL) generate_icode (ctx, func_item);\n  func_desc = get_func_desc (func_item);\n  bp = alloca ((func_desc->nregs + 2) * sizeof (MIR_val_t));\n  bp++; /* reserved for setjmp/longjmp */\n#if VA_LIST_IS_ARRAY_P\n  bp[0].a = va;\n#else\n      bp[0].a = &va;\n#endif\n  bp++;\n  if (func_desc->nregs < nargs + 1) nargs = func_desc->nregs - 1;\n  bp[0].i = 0;\n  memcpy (&bp[1], vals, sizeof (MIR_val_t) * nargs);\n  eval (ctx, func_desc, bp, results);\n}\n\nvoid MIR_interp_arr (MIR_context_t ctx, MIR_item_t func_item, MIR_val_t *results, size_t nargs,\n                     MIR_val_t *vals) {\n  interp_arr_varg (ctx, func_item, results, nargs, vals, NULL);\n}\n\n/* C call interface to interpreter.  It is based on knowledge of\n   common vararg implementation.  For some targets it might not\n   work.  */\nstatic void interp (MIR_context_t ctx, MIR_item_t func_item, va_list va, MIR_val_t *results) {\n  struct interp_ctx *interp_ctx = ctx->interp_ctx;\n  size_t nargs;\n  MIR_var_t *arg_vars;\n  MIR_func_t func = func_item->u.func;\n\n  nargs = func->nargs;\n  arg_vars = VARR_ADDR (MIR_var_t, func->vars);\n  if (VARR_EXPAND (MIR_val_t, arg_vals_varr, nargs))\n    arg_vals = VARR_ADDR (MIR_val_t, arg_vals_varr);\n  for (size_t i = 0; i < nargs; i++) {\n    MIR_type_t type = arg_vars[i].type;\n    switch (type) {\n    case MIR_T_I8: arg_vals[i].i = (int8_t) va_arg (va, int32_t); break;\n    case MIR_T_I16: arg_vals[i].i = (int16_t) va_arg (va, int32_t); break;\n    case MIR_T_I32: arg_vals[i].i = va_arg (va, int32_t); break;\n    case MIR_T_I64: arg_vals[i].i = va_arg (va, int64_t); break;\n    case MIR_T_U8: arg_vals[i].i = (uint8_t) va_arg (va, uint32_t); break;\n    case MIR_T_U16: arg_vals[i].i = (uint16_t) va_arg (va, uint32_t); break;\n    case MIR_T_U32: arg_vals[i].i = va_arg (va, uint32_t); break;\n    case MIR_T_U64: arg_vals[i].i = va_arg (va, uint64_t); break;\n    case MIR_T_F: {\n      union {\n        double d;\n        float f;\n      } u;\n      u.d = va_arg (va, double);\n#if defined(__PPC64__)\n      arg_vals[i].f = u.d;\n#else\n          arg_vals[i].f = u.f;\n#endif\n      break;\n    }\n    case MIR_T_D: arg_vals[i].d = va_arg (va, double); break;\n    case MIR_T_LD: arg_vals[i].ld = va_arg (va, long double); break;\n    case MIR_T_P:\n    case MIR_T_RBLK: arg_vals[i].a = va_arg (va, void *); break;\n    default: mir_assert (MIR_blk_type_p (type)); arg_vals[i].a = alloca (arg_vars[i].size);\n#if defined(__PPC64__) || defined(__aarch64__) || defined(__riscv) || defined(_WIN32)\n      va_block_arg_builtin (arg_vals[i].a, &va, arg_vars[i].size, type - MIR_T_BLK);\n#else\n          va_block_arg_builtin (arg_vals[i].a, va, arg_vars[i].size, type - MIR_T_BLK);\n#endif\n      break;\n    }\n  }\n#if VA_LIST_IS_ARRAY_P\n  interp_arr_varg (ctx, func_item, results, nargs, arg_vals, va);\n#else\n      interp_arr_varg (ctx, func_item, results, nargs, arg_vals, (va_t) &va);\n#endif\n}\n\nstatic void redirect_interface_to_interp (MIR_context_t ctx, MIR_item_t func_item) {\n  _MIR_redirect_thunk (ctx, func_item->addr, _MIR_get_interp_shim (ctx, func_item, interp));\n}\n\nvoid MIR_set_interp_interface (MIR_context_t ctx, MIR_item_t func_item) {\n  if (func_item != NULL) redirect_interface_to_interp (ctx, func_item);\n}\n\n#endif /* #ifdef MIR_NO_INTERP */\n"
        },
        {
          "name": "mir-ppc64.c",
          "type": "blob",
          "size": 27.994140625,
          "content": "/* This file is a part of MIR project.\n   Copyright (C) 2018-2024 Vladimir Makarov <vmakarov.gcc@gmail.com>.\n*/\n\n#include \"mir-ppc64.h\"\n#include \"mir-alloc.h\"\n#include \"mir.h\"\n\n/* All BLK type values is passed in int regs, and if the regs are not enough, the rest is passed on\n   the stack. RBLK is always passed by address.  */\n\n#define VA_LIST_IS_ARRAY_P 1 /* one element which is a pointer to args */\n\nstatic void ppc64_push_func_desc (MIR_alloc_t alloc, VARR (uint8_t) * *insn_varr);\nvoid (*ppc64_func_desc) (MIR_alloc_t alloc, VARR (uint8_t) * *insn_varr) = ppc64_push_func_desc;\n\nstatic void ppc64_push_func_desc (MIR_alloc_t alloc, VARR (uint8_t) * *insn_varr) {\n  VARR_CREATE (uint8_t, *insn_varr, alloc, 128);\n  for (int i = 0; i < PPC64_FUNC_DESC_LEN; i++)\n    VARR_PUSH (uint8_t, *insn_varr, ((uint8_t *) ppc64_func_desc)[i]);\n}\n\nstatic void *ppc64_publish_func_and_redirect (MIR_context_t ctx, VARR (uint8_t) * insn_varr) {\n  void *res\n    = _MIR_publish_code (ctx, VARR_ADDR (uint8_t, insn_varr), VARR_LENGTH (uint8_t, insn_varr));\n  VARR_DESTROY (uint8_t, insn_varr);\n  return res;\n}\n\nstatic void push_insns (VARR (uint8_t) * insn_varr, const uint32_t *pat, size_t pat_len) {\n  uint8_t *p = (uint8_t *) pat;\n  for (size_t i = 0; i < pat_len; i++) VARR_PUSH (uint8_t, insn_varr, p[i]);\n}\n\nstatic void ppc64_gen_mov (VARR (uint8_t) * insn_varr, unsigned to, unsigned from) {\n  /* or to,from,from: */\n  push_insn (insn_varr, (31 << 26) | (444 << 1) | (from << 21) | (to << 16) | (from << 11));\n}\n\nstatic void ppc64_gen_addi (VARR (uint8_t) * insn_varr, unsigned rt_reg, unsigned ra_reg,\n                            int disp) {\n  push_insn (insn_varr, (14 << 26) | (rt_reg << 21) | (ra_reg << 16) | (disp & 0xffff));\n}\n\nstatic void ppc64_gen_add (VARR (uint8_t) * insn_varr, unsigned rt_reg, unsigned ra_reg,\n                           unsigned rb_reg) {\n  push_insn (insn_varr, (31 << 26) | (266 << 1) | (rt_reg << 21) | (ra_reg << 16) | (rb_reg << 11));\n}\n\nstatic void ppc64_gen_ld (VARR (uint8_t) * insn_varr, unsigned to, unsigned base, int disp,\n                          MIR_type_t type) {\n  int single_p = type == MIR_T_F;\n  int double_p = type == MIR_T_D || type == MIR_T_LD;\n  /* (ld | lf[sd]) to, disp(base): */\n  assert (base != 0 && base < 32 && to < 32 && (single_p || double_p || (disp & 0x3) == 0));\n  push_insn (insn_varr, ((single_p   ? 48\n                          : double_p ? 50\n                                     : 58)\n                         << 26)\n                          | (to << 21) | (base << 16) | (disp & 0xffff));\n}\n\nstatic void ppc64_gen_st (VARR (uint8_t) * insn_varr, unsigned from, unsigned base, int disp,\n                          MIR_type_t type) {\n  int single_p = type == MIR_T_F;\n  int double_p = type == MIR_T_D || type == MIR_T_LD;\n  /* std|stf[sd] from, disp(base): */\n  assert (base != 0 && base < 32 && from < 32 && (single_p || double_p || (disp & 0x3) == 0));\n  push_insn (insn_varr, ((single_p   ? 52\n                          : double_p ? 54\n                                     : 62)\n                         << 26)\n                          | (from << 21) | (base << 16) | (disp & 0xffff));\n}\n\nstatic void ppc64_gen_stdu (VARR (uint8_t) * insn_varr, int disp) {\n  assert ((disp & 0x3) == 0);\n  push_insn (insn_varr, 0xf8210001 | (disp & 0xfffc)); /* stdu 1, disp (1) */\n}\n\nstatic void ppc64_gen_jump (VARR (uint8_t) * insn_varr, unsigned int reg) {\n  push_insn (insn_varr, (31 << 26) | (467 << 1) | (reg << 21) | (9 << 16)); /* mctr reg */\n  push_insn (insn_varr, (19 << 26) | (528 << 1) | (20 << 21));              /* bcctr */\n}\n\nstatic void ppc64_gen_call (VARR (uint8_t) * insn_varr, unsigned int reg) {\n  if (reg != 12) ppc64_gen_mov (insn_varr, 12, reg);                       /* 12 = func addr */\n  push_insn (insn_varr, (31 << 26) | (467 << 1) | (12 << 21) | (9 << 16)); /* mctr 12 */\n  push_insn (insn_varr, (19 << 26) | (528 << 1) | (20 << 21) | 1);         /* bcctrl */\n}\n\n/* r11=addr_reg+addr_disp; r15=r1(sp)+sp_offset; r0=qwords-1;\n   ctr=r0; L: r0=mem[r11]; r11+=8; mem[r15]=r0; r15+=8; bdnz L; */\nstatic void gen_blk_mov (VARR (uint8_t) * insn_varr, size_t sp_offset, unsigned int addr_reg,\n                         int addr_disp, size_t qwords) {\n  static const uint32_t blk_mov_loop[] = {\n    /*0:*/ 0x7c0903a6,  /*mctr r0*/\n    /*4:*/ 0xe80b0000,  /*ld r0,0(r11)*/\n    /*8:*/ 0x396b0008,  /*addi r11,r11,8*/\n    /*12:*/ 0xf80f0000, /*std r0,0(r15)*/\n    /*16:*/ 0x39ef0008, /*addi r15,r15,8*/\n    /*20:*/ 0x4200fff0, /*bdnz 4*/\n  };\n  /* r11=addr_reg+addr_disp: */\n  if (addr_reg != 11 || addr_disp != 0) ppc64_gen_addi (insn_varr, 11, addr_reg, addr_disp);\n  if (sp_offset < 0x10000) {\n    ppc64_gen_addi (insn_varr, 15, 1, sp_offset);\n  } else {\n    ppc64_gen_address (insn_varr, 15, (void *) sp_offset);\n    ppc64_gen_add (insn_varr, 15, 15, 1);\n  }\n  ppc64_gen_address (insn_varr, 0, (void *) qwords); /*r0 = qwords*/\n  push_insns (insn_varr, blk_mov_loop, sizeof (blk_mov_loop));\n}\n\nvoid *_MIR_get_bstart_builtin (MIR_context_t ctx) {\n  static const uint32_t bstart_code[] = {\n    0x7c230b78, /* mr 3,1 */\n    0x4e800020, /* blr */\n  };\n  VARR (uint8_t) * code;\n\n  ppc64_push_func_desc (ctx->alloc, &code);\n  push_insns (code, bstart_code, sizeof (bstart_code));\n  return ppc64_publish_func_and_redirect (ctx, code);\n}\n\nvoid *_MIR_get_bend_builtin (MIR_context_t ctx) {\n  static const uint32_t bend_finish_code[] = {\n    0x7c611b78, /* mr      r1,r3 */\n    0x4e800020, /* blr */\n  };\n  VARR (uint8_t) * code;\n\n  ppc64_push_func_desc (ctx->alloc, &code);\n  ppc64_gen_ld (code, 0, 1, 0, MIR_T_I64);                /* r0 = 0(r1) */\n  ppc64_gen_st (code, 0, 3, 0, MIR_T_I64);                /* 0(r3) = r0 */\n  ppc64_gen_ld (code, 0, 1, PPC64_TOC_OFFSET, MIR_T_I64); /* r0 = toc_offset(r1) */\n  ppc64_gen_st (code, 0, 3, PPC64_TOC_OFFSET, MIR_T_I64); /* toc_offset(r3) = r0 */\n  push_insns (code, bend_finish_code, sizeof (bend_finish_code));\n  return ppc64_publish_func_and_redirect (ctx, code);\n}\n\nstatic const int max_thunk_len = (7 * 4 + 8); /* 5 for r=addr, 2 for goto r, addr itself */\n\nvoid *_MIR_get_thunk (MIR_context_t ctx) { /* emit 3 doublewords for func descriptor: */\n  VARR (uint8_t) * code;\n  void *res;\n\n  VARR_CREATE (uint8_t, code, ctx->alloc, 128);\n  for (int i = 0; i < max_thunk_len / 4; i++) push_insn (code, TARGET_NOP);\n  res = _MIR_publish_code (ctx, VARR_ADDR (uint8_t, code), VARR_LENGTH (uint8_t, code));\n  VARR_DESTROY (uint8_t, code);\n  return res;\n}\n\nstatic const uint32_t thunk_code_end[] = {\n  0x7d8903a6, /* mtctr r12 */\n  0x4e800420, /* bctr */\n};\n\nvoid _MIR_redirect_thunk (MIR_context_t ctx, void *thunk, void *to) {\n  VARR (uint8_t) * code;\n  VARR_CREATE (uint8_t, code, ctx->alloc, 256);\n  ppc64_gen_address (code, 12, to);\n  push_insns (code, thunk_code_end, sizeof (thunk_code_end));\n  mir_assert ((VARR_LENGTH (uint8_t, code) & 0x3) == 0\n              && VARR_LENGTH (uint8_t, code) <= (size_t) max_thunk_len);\n  push_insns (code, (uint32_t *) &to, sizeof (to));\n  _MIR_change_code (ctx, thunk, VARR_ADDR (uint8_t, code), VARR_LENGTH (uint8_t, code));\n  VARR_DESTROY (uint8_t, code);\n}\n\nstatic void *get_jump_addr (uint32_t *insns) {\n  int i;\n  for (i = 0; i < 8; i++)\n    if (insns[i] == 0x4e800420) break; /* bctr */\n  mir_assert (i < 8);\n  return (void *) (insns[i + 1] | ((uint64_t) insns[i + 2] << 32));\n}\n\nvoid *_MIR_get_thunk_addr (MIR_context_t ctx MIR_UNUSED, void *thunk) {\n  return get_jump_addr (thunk);\n}\n\nstruct ppc64_va_list {\n  uint64_t *arg_area;\n};\n\nvoid *va_arg_builtin (void *p, uint64_t t) {\n  struct ppc64_va_list *va = p;\n  MIR_type_t type = t;\n  void *a = va->arg_area;\n\n  if (type == MIR_T_LD) {\n    va->arg_area += 2;\n  } else {\n    va->arg_area++;\n  }\n  return a;\n}\n\nvoid va_block_arg_builtin (void *res, void *p, size_t s, uint64_t ncase MIR_UNUSED) {\n  struct ppc64_va_list *va = p;\n  void *a = va->arg_area;\n  if (res != NULL) memcpy (res, a, s);\n  va->arg_area += (s + sizeof (uint64_t) - 1) / sizeof (uint64_t);\n}\n\nvoid va_start_interp_builtin (MIR_context_t ctx MIR_UNUSED, void *p, void *a) {\n  struct ppc64_va_list **va = p;\n  va_list *vap = a;\n\n  assert (sizeof (struct ppc64_va_list) == sizeof (va_list));\n  *va = (struct ppc64_va_list *) vap;\n}\n\nvoid va_end_interp_builtin (MIR_context_t ctx MIR_UNUSED, void *p MIR_UNUSED) {}\n\n/* Generation: fun (fun_addr, res_arg_addresses):\n   save lr (r1 + 16); allocate and form minimal stack frame (with necessary param area); save\n   r14,r15; r12=fun_addr (r3); r14 = res_arg_addresses (r4); r0=mem[r14,<args_offset>];\n   (arg_reg=mem[r0] or r0=mem[r0];mem[r1,r1_offset]=r0) ... if func is vararg: put fp args also in\n   gp regs call *r12; r0=mem[r14,<offset>]; res_reg=mem[r0]; ... restore r15, r14, r1, lr; return.\n */\nvoid *_MIR_get_ff_call (MIR_context_t ctx, size_t nres, MIR_type_t *res_types, size_t nargs,\n                        _MIR_arg_desc_t *arg_descs, size_t arg_vars_num) {\n  static uint32_t start_pattern[] = {\n    0x7c0802a6, /* mflr r0 */\n    0xf8010010, /* std  r0,16(r1) */\n  };\n  static uint32_t finish_pattern[] = {\n    0xe8010010, /* ld   r0,16(r1) */\n    0x7c0803a6, /* mtlr r0 */\n    0x4e800020, /* blr */\n  };\n  int vararg_p = nargs > arg_vars_num;\n  MIR_type_t type;\n  int n_gpregs = 0, n_fpregs = 0, res_reg = 14, qwords, frame_size;\n  int disp, blk_disp, param_offset, param_size = 0;\n  VARR (uint8_t) * code;\n\n  ppc64_push_func_desc (ctx->alloc, &code);\n  for (uint32_t i = 0; i < nargs; i++) {\n    type = arg_descs[i].type;\n    if (MIR_blk_type_p (type))\n      param_size += (arg_descs[i].size + 7) / 8 * 8;\n    else\n      param_size += type == MIR_T_LD ? 16 : 8;\n  }\n  if (param_size < 64) param_size = 64;\n  frame_size = PPC64_STACK_HEADER_SIZE + param_size + 16; /* +local var to save res_reg and 15 */\n  if (frame_size % 16 != 0) frame_size += 8;              /* align */\n  ppc64_gen_st (code, 2, 1, PPC64_TOC_OFFSET, MIR_T_I64);\n  push_insns (code, start_pattern, sizeof (start_pattern));\n  ppc64_gen_stdu (code, -frame_size);\n  ppc64_gen_st (code, res_reg, 1, PPC64_STACK_HEADER_SIZE + param_size,\n                MIR_T_I64); /* save res_reg */\n  ppc64_gen_st (code, 15, 1, PPC64_STACK_HEADER_SIZE + param_size + 8, MIR_T_I64); /* save 15 */\n  mir_assert (sizeof (long double) == 16);\n  ppc64_gen_mov (code, res_reg, 4); /* results & args */\n  ppc64_gen_mov (code, 12, 3);      /* func addr */\n  n_gpregs = n_fpregs = 0;\n  param_offset = nres * 16;              /* args start */\n  disp = PPC64_STACK_HEADER_SIZE;        /* param area start */\n  for (uint32_t i = 0; i < nargs; i++) { /* load args: */\n    type = arg_descs[i].type;\n    if ((type == MIR_T_F || type == MIR_T_D || type == MIR_T_LD) && n_fpregs < 13) {\n      ppc64_gen_ld (code, 1 + n_fpregs, res_reg, param_offset, type);\n      if (vararg_p) {\n        if (n_gpregs >= 8) {\n          ppc64_gen_st (code, 1 + n_fpregs, 1, disp, MIR_T_D);\n        } else { /* load into gp reg too */\n          ppc64_gen_st (code, 1 + n_fpregs, 1, -8, MIR_T_D);\n          ppc64_gen_ld (code, 3 + n_gpregs, 1, -8, MIR_T_I64);\n        }\n      }\n      n_fpregs++;\n      if (type == MIR_T_LD) {\n        if (n_fpregs < 13) {\n          ppc64_gen_ld (code, 1 + n_fpregs, res_reg, param_offset + 8, type);\n          if (vararg_p) {\n            if (n_gpregs + 1 >= 8) {\n              ppc64_gen_st (code, 1 + n_fpregs, 1, disp + 8, MIR_T_D);\n            } else { /* load gp reg to */\n              ppc64_gen_st (code, 1 + n_fpregs, 1, -8, MIR_T_D);\n              ppc64_gen_ld (code, 4 + n_gpregs, 1, -8, MIR_T_I64);\n            }\n          }\n          n_fpregs++;\n        } else {\n          ppc64_gen_ld (code, 0, res_reg, param_offset + 8, type);\n          ppc64_gen_st (code, 0, 1, disp + 8, MIR_T_D);\n        }\n      }\n    } else if (type == MIR_T_F || type == MIR_T_D || type == MIR_T_LD) {\n      ppc64_gen_ld (code, 0, res_reg, param_offset, type);\n      ppc64_gen_st (code, 0, 1, disp, MIR_T_D);\n      if (type == MIR_T_LD) {\n        ppc64_gen_ld (code, 0, res_reg, param_offset + 8, type);\n        ppc64_gen_st (code, 0, 1, disp + 8, MIR_T_D);\n      }\n    } else if (MIR_blk_type_p (type)) {\n      qwords = (arg_descs[i].size + 7) / 8;\n      if (qwords > 0) ppc64_gen_ld (code, 11, res_reg, param_offset, MIR_T_I64);\n      for (blk_disp = 0; qwords > 0 && n_gpregs < 8; qwords--, n_gpregs++, blk_disp += 8, disp += 8)\n        ppc64_gen_ld (code, n_gpregs + 3, 11, blk_disp, MIR_T_I64);\n      if (qwords > 0) gen_blk_mov (code, disp, 11, blk_disp, qwords);\n      disp += qwords * 8;\n      param_offset += 16;\n      continue;\n    } else if (n_gpregs < 8) { /* including RBLK */\n      ppc64_gen_ld (code, n_gpregs + 3, res_reg, param_offset, MIR_T_I64);\n    } else {\n      ppc64_gen_ld (code, 0, res_reg, param_offset, MIR_T_I64);\n      ppc64_gen_st (code, 0, 1, disp, MIR_T_I64);\n    }\n    disp += type == MIR_T_LD ? 16 : 8;\n    param_offset += 16;\n    n_gpregs += type == MIR_T_LD ? 2 : 1;\n  }\n  ppc64_gen_call (code, 12); /* call func_addr */\n  n_gpregs = n_fpregs = 0;\n  disp = 0;\n  for (uint32_t i = 0; i < nres; i++) {\n    type = res_types[i];\n    if ((type == MIR_T_F || type == MIR_T_D || type == MIR_T_LD) && n_fpregs < 8) {\n      ppc64_gen_st (code, n_fpregs + 1, res_reg, disp, type);\n      n_fpregs++;\n      if (type == MIR_T_LD) {\n        if (n_fpregs >= 8)\n          MIR_get_error_func (ctx) (MIR_ret_error,\n                                    \"ppc64 can not handle this combination of return values\");\n        ppc64_gen_st (code, n_fpregs + 1, res_reg, disp + 8, type);\n        n_fpregs++;\n      }\n    } else if (n_gpregs < 2) {  // just one-two gp reg\n      ppc64_gen_st (code, n_gpregs + 3, res_reg, disp, MIR_T_I64);\n      n_gpregs++;\n    } else {\n      MIR_get_error_func (ctx) (MIR_ret_error,\n                                \"ppc64 can not handle this combination of return values\");\n    }\n    disp += 16;\n  }\n  ppc64_gen_ld (code, res_reg, 1, PPC64_STACK_HEADER_SIZE + param_size,\n                MIR_T_I64); /* restore res_reg */\n  ppc64_gen_ld (code, 15, 1, PPC64_STACK_HEADER_SIZE + param_size + 8, MIR_T_I64); /* restore r15 */\n  ppc64_gen_addi (code, 1, 1, frame_size);\n  push_insns (code, finish_pattern, sizeof (finish_pattern));\n  return ppc64_publish_func_and_redirect (ctx, code);\n}\n\n/* Transform C call to call of void handler (MIR_context_t ctx, MIR_item_t func_item,\n                                             va_list va, MIR_val_t *results):\n   Brief: put all C call args to local vars (or if va_arg do nothing); save lr (r1+16), r14;\n          allocate and form minimal shim stack frame (param area = 8 * 8);\n          call handler with args; move results(r14) to return regs; restore lr,r14,r1; return */\nvoid *_MIR_get_interp_shim (MIR_context_t ctx, MIR_item_t func_item, void *handler) {\n  MIR_func_t func = func_item->u.func;\n  uint32_t nres = func->nres, nargs = func->nargs;\n  int vararg_p = func->vararg_p;\n  MIR_type_t type, *res_types = func->res_types;\n  MIR_var_t *arg_vars = VARR_ADDR (MIR_var_t, func->vars);\n  int disp, start_disp, qwords, size, frame_size, local_var_size, param_offset;\n  int va_reg = 11, caller_r1 = 12, res_reg = 14;\n  int n_gpregs, n_fpregs;\n  static uint32_t start_pattern[] = {\n    0x7c0802a6, /* mflr r0 */\n    0xf8010010, /* std  r0,16(r1) */\n  };\n  static uint32_t finish_pattern[] = {\n    0xe8010010, /* ld   r0,16(r1) */\n    0x7c0803a6, /* mtlr r0 */\n    0x4e800020, /* blr */\n  };\n  VARR (uint8_t) * code;\n  void *res;\n\n  VARR_CREATE (uint8_t, code, ctx->alloc, 256);\n  frame_size = PPC64_STACK_HEADER_SIZE + 64; /* header + 8(param area) */\n  local_var_size = nres * 16 + 16;           /* saved r14, r15, results */\n  if (vararg_p) {\n    for (unsigned reg = 3; reg <= 10; reg++) /* std rn,dispn(r1) : */\n      ppc64_gen_st (code, reg, 1, PPC64_STACK_HEADER_SIZE + (reg - 3) * 8, MIR_T_I64);\n    ppc64_gen_addi (code, va_reg, 1, PPC64_STACK_HEADER_SIZE);\n  } else {\n    ppc64_gen_mov (code, caller_r1, 1); /* caller frame r1 */\n    for (uint32_t i = 0; i < nargs; i++) {\n      type = arg_vars[i].type;\n      if (MIR_blk_type_p (type))\n        local_var_size += (arg_vars[i].size + 7) / 8 * 8;\n      else\n        local_var_size += type == MIR_T_LD ? 16 : 8;\n    }\n  }\n  frame_size += local_var_size;\n  if (frame_size % 16 != 0) frame_size += 8; /* align */\n  push_insns (code, start_pattern, sizeof (start_pattern));\n  ppc64_gen_stdu (code, -frame_size);\n  ppc64_gen_st (code, res_reg, 1, PPC64_STACK_HEADER_SIZE + 64, MIR_T_I64); /* save res_reg */\n  ppc64_gen_st (code, 15, 1, PPC64_STACK_HEADER_SIZE + 72, MIR_T_I64);      /* save r15 */\n  if (!vararg_p) { /* save args in local vars: */\n    /* header_size + 64 + nres * 16 + 16 -- start of stack memory to keep args: */\n    start_disp = disp = PPC64_STACK_HEADER_SIZE + 64 + nres * 16 + 16;\n    param_offset = PPC64_STACK_HEADER_SIZE;\n    n_gpregs = n_fpregs = 0;\n    for (uint32_t i = 0; i < nargs; i++) {\n      type = arg_vars[i].type;\n      if ((type == MIR_T_F || type == MIR_T_D || type == MIR_T_LD) && n_fpregs < 13) {\n        ppc64_gen_st (code, n_fpregs + 1, 1, disp, MIR_T_D);\n        n_fpregs++;\n        if (type == MIR_T_LD) {\n          if (n_fpregs < 13) {\n            ppc64_gen_st (code, n_fpregs + 1, 1, disp + 8, MIR_T_D);\n            n_fpregs++;\n          } else {\n            ppc64_gen_ld (code, 0, caller_r1, param_offset + 8, MIR_T_D);\n            ppc64_gen_st (code, 0, 1, disp + 8, MIR_T_D);\n          }\n        }\n      } else if (MIR_blk_type_p (type)) {\n        qwords = (arg_vars[i].size + 7) / 8;\n        for (; qwords > 0 && n_gpregs < 8; qwords--, n_gpregs++, disp += 8, param_offset += 8)\n          ppc64_gen_st (code, n_gpregs + 3, 1, disp, MIR_T_I64);\n        if (qwords > 0) {\n          gen_blk_mov (code, disp, caller_r1, param_offset, qwords);\n          disp += qwords * 8;\n          param_offset += qwords * 8;\n        }\n        continue;\n      } else if (n_gpregs < 8) {\n        ppc64_gen_st (code, n_gpregs + 3, 1, disp, MIR_T_I64);\n      } else if (type == MIR_T_F || type == MIR_T_D || type == MIR_T_LD) {\n        ppc64_gen_ld (code, 0, caller_r1, param_offset + (type == MIR_T_F ? 4 : 0), type);\n        ppc64_gen_st (code, 0, 1, disp, MIR_T_D);\n        if (type == MIR_T_LD) {\n          ppc64_gen_ld (code, 0, caller_r1, param_offset + 8, MIR_T_D);\n          ppc64_gen_st (code, 0, 1, disp + 8, MIR_T_D);\n        }\n      } else {\n        ppc64_gen_ld (code, 0, caller_r1, param_offset, MIR_T_I64);\n        ppc64_gen_st (code, 0, 1, disp, MIR_T_I64);\n      }\n      size = type == MIR_T_LD ? 16 : 8;\n      disp += size;\n      param_offset += size;\n      n_gpregs += type == MIR_T_LD ? 2 : 1;\n    }\n    ppc64_gen_addi (code, va_reg, 1, start_disp);\n  }\n  ppc64_gen_addi (code, res_reg, 1, 64 + PPC64_STACK_HEADER_SIZE + 16);\n  ppc64_gen_address (code, 3, ctx);\n  ppc64_gen_address (code, 4, func_item);\n  ppc64_gen_mov (code, 5, va_reg);\n  ppc64_gen_mov (code, 6, res_reg);\n  ppc64_gen_address (code, 12, handler);\n  ppc64_gen_call (code, 12);\n  disp = n_gpregs = n_fpregs = 0;\n  for (uint32_t i = 0; i < nres; i++) {\n    type = res_types[i];\n    if ((type == MIR_T_F || type == MIR_T_D || type == MIR_T_LD) && n_fpregs < 8) {\n      ppc64_gen_ld (code, n_fpregs + 1, res_reg, disp, type);\n      n_fpregs++;\n      if (type == MIR_T_LD) {\n        if (n_fpregs >= 8)\n          MIR_get_error_func (ctx) (MIR_ret_error,\n                                    \"ppc64 can not handle this combination of return values\");\n        ppc64_gen_ld (code, n_fpregs + 1, res_reg, disp + 8, type);\n        n_fpregs++;\n      }\n    } else if (n_gpregs < 2) {  // just one-two gp reg\n      ppc64_gen_ld (code, n_gpregs + 3, res_reg, disp, MIR_T_I64);\n      n_gpregs++;\n    } else {\n      MIR_get_error_func (ctx) (MIR_ret_error,\n                                \"ppc64 can not handle this combination of return values\");\n    }\n    disp += 16;\n  }\n  ppc64_gen_ld (code, res_reg, 1, PPC64_STACK_HEADER_SIZE + 64, MIR_T_I64); /* restore res_reg */\n  ppc64_gen_ld (code, 15, 1, PPC64_STACK_HEADER_SIZE + 72, MIR_T_I64);      /* restore r15 */\n  ppc64_gen_addi (code, 1, 1, frame_size);\n  push_insns (code, finish_pattern, sizeof (finish_pattern));\n  res = _MIR_publish_code (ctx, VARR_ADDR (uint8_t, code), VARR_LENGTH (uint8_t, code));\n  VARR_DESTROY (uint8_t, code);\n  return res;\n}\n\nstatic void redirect_bb_thunk (MIR_context_t ctx, VARR (uint8_t) * code, void *start, void *to) {\n  int64_t offset = (uint8_t *) to - (uint8_t *) start;\n  mir_assert ((offset & 0x3) == 0);\n  VARR_TRUNC (uint8_t, code, 0);\n  if (((offset < 0 ? -offset : offset) & ~(int64_t) 0x1ffffff) == 0) { /* just jump */\n    uint32_t insn\n      = (PPC_JUMP_OPCODE << (32 - 6)) /* jump opcode */ | (((offset / 4) & 0xffffff) << 2);\n    push_insn (code, insn);\n  } else {\n    ppc64_gen_address (code, 12, to); /* r12 = to */\n    push_insns (code, thunk_code_end, sizeof (thunk_code_end));\n    mir_assert ((VARR_LENGTH (uint8_t, code) & 0x3) == 0\n                && VARR_LENGTH (uint8_t, code) <= (size_t) max_thunk_len);\n  }\n  _MIR_change_code (ctx, start, VARR_ADDR (uint8_t, code), VARR_LENGTH (uint8_t, code));\n}\n\n/* r11=<bb_version>; jump handler  ??? mutex free */\nvoid *_MIR_get_bb_thunk (MIR_context_t ctx, void *bb_version, void *handler) {\n  void *res;\n  size_t offset;\n  VARR (uint8_t) * code;\n\n  VARR_CREATE (uint8_t, code, ctx->alloc, 64);\n  ppc64_gen_address (code, 11, bb_version); /* x11 = bb_version */\n  offset = VARR_LENGTH (uint8_t, code);\n  for (int i = 0; i < max_thunk_len / 4; i++) push_insn (code, TARGET_NOP);\n  res = _MIR_publish_code (ctx, VARR_ADDR (uint8_t, code), VARR_LENGTH (uint8_t, code));\n  redirect_bb_thunk (ctx, code, (uint8_t *) res + offset, handler);\n#if 0\n  if (getenv (\"MIR_code_dump\") != NULL)\n    _MIR_dump_code (\"bb thunk:\", res, offset + VARR_LENGTH (uint8_t, code));\n#endif\n  VARR_DESTROY (uint8_t, code);\n  return res;\n}\n\n/* change to jump to */\nvoid _MIR_replace_bb_thunk (MIR_context_t ctx, void *thunk, void *to) {\n  size_t i, offset;\n  VARR (uint8_t) * code;\n  uint32_t opcode, *insns = (uint32_t *) thunk;\n\n  /* find jump code offset (see ppc64_gen_address): */\n  for (i = 0; i <= 5; i++) {\n    if ((opcode = insns[i] >> 26) == (uint32_t) PPC_JUMP_OPCODE) break; /* uncond branch */\n    if ((opcode == LI_OPCODE || opcode == LIS_OPCODE\n         || opcode == XOR_OPCODE) /* (li|lis|xor) r12, ... */\n        && ((insns[i] >> 21) & 0x1f) == 12)\n      break;\n  }\n  mir_assert (i <= 5);\n  offset = i * 4;\n  VARR_CREATE (uint8_t, code, ctx->alloc, 64);\n  redirect_bb_thunk (ctx, code, (char *) thunk + offset, to);\n  VARR_DESTROY (uint8_t, code);\n}\n\nstatic const int wrapper_frame_size = PPC64_STACK_HEADER_SIZE + 8 * 8 + 13 * 8 + 8 * 8;\n\n/* save lr(r1+16);update r1,save r3,r4 regs;r3=ctx;r4=called_func;r12=hook_address;jmp wrap_end */\nvoid *_MIR_get_wrapper (MIR_context_t ctx, MIR_item_t called_func, void *hook_address) {\n  static const uint32_t prologue[] = {\n    0x7c0802a6, /* mflr r0 */\n    0xf8010010, /* std  r0,16(r1) */\n  };\n  VARR (uint8_t) * code;\n  void *res;\n  int frame_size = wrapper_frame_size;\n\n  VARR_CREATE (uint8_t, code, ctx->alloc, 256);\n  push_insns (code, prologue, sizeof (prologue));\n  /* stdu r1,n(r1): header + 8(gp args) + 13(fp args) + 8(param area): */\n  if (frame_size % 16 != 0) frame_size += 8;\n  ppc64_gen_stdu (code, -frame_size);\n  for (unsigned reg = 3; reg <= 4; reg++) /* std rn,dispn(r1) : */\n    ppc64_gen_st (code, reg, 1, PPC64_STACK_HEADER_SIZE + (reg - 3) * 8 + 64, MIR_T_I64);\n  ppc64_gen_address (code, 3, ctx);\n  ppc64_gen_address (code, 4, called_func);\n  ppc64_gen_address (code, 12, hook_address);\n  ppc64_gen_address (code, 11, wrapper_end_addr);\n  ppc64_gen_jump (code, 11);\n  res = _MIR_publish_code (ctx, VARR_ADDR (uint8_t, code), VARR_LENGTH (uint8_t, code));\n#if 0\n  if (getenv (\"MIR_code_dump\") != NULL)\n    _MIR_dump_code (\"wapper:\", res, VARR_LENGTH (uint8_t, code));\n#endif\n  VARR_DESTROY (uint8_t, code);\n  return res;\n}\n\n/* save all param regs but r3, r4; allocate and form minimal wrapper stack frame (param area = 8*8);\n   r3 = call r12 (r3, r4); r12=r3; restore all params regs,r1,lr (r1+16);ctr=r12; b *ctr */\nvoid *_MIR_get_wrapper_end (MIR_context_t ctx) {\n  static const uint32_t epilogue[] = {\n    0xe8010010, /* ld   r0,16(r1) */\n    0x7c0803a6, /* mtlr r0 */\n  };\n  VARR (uint8_t) * code;\n  void *res;\n  int frame_size = wrapper_frame_size;\n\n  if (frame_size % 16 != 0) frame_size += 8;\n  VARR_CREATE (uint8_t, code, ctx->alloc, 256);\n  for (unsigned reg = 5; reg <= 10; reg++) /* std rn,dispn(r1) : */\n    ppc64_gen_st (code, reg, 1, PPC64_STACK_HEADER_SIZE + (reg - 3) * 8 + 64, MIR_T_I64);\n  for (unsigned reg = 1; reg <= 13; reg++) /* stfd fn,dispn(r1) : */\n    ppc64_gen_st (code, reg, 1, PPC64_STACK_HEADER_SIZE + (reg - 1 + 8) * 8 + 64, MIR_T_D);\n  ppc64_gen_call (code, 12);\n  ppc64_gen_mov (code, 12, 3);\n  for (unsigned reg = 3; reg <= 10; reg++) /* ld rn,dispn(r1) : */\n    ppc64_gen_ld (code, reg, 1, PPC64_STACK_HEADER_SIZE + (reg - 3) * 8 + 64, MIR_T_I64);\n  for (unsigned reg = 1; reg <= 13; reg++) /* lfd fn,dispn(r1) : */\n    ppc64_gen_ld (code, reg, 1, PPC64_STACK_HEADER_SIZE + (reg - 1 + 8) * 8 + 64, MIR_T_D);\n  ppc64_gen_addi (code, 1, 1, frame_size);\n  push_insns (code, epilogue, sizeof (epilogue));\n  push_insn (code, (31 << 26) | (467 << 1) | (12 << 21) | (9 << 16)); /* mctr 12 */\n  push_insn (code, (19 << 26) | (528 << 1) | (20 << 21));             /* bcctr */\n  res = _MIR_publish_code (ctx, VARR_ADDR (uint8_t, code), VARR_LENGTH (uint8_t, code));\n#if 0\n  if (getenv (\"MIR_code_dump\") != NULL)\n    _MIR_dump_code (\"wapper end:\", res, VARR_LENGTH (uint8_t, code));\n#endif\n  VARR_DESTROY (uint8_t, code);\n  return res;\n}\n\n/* save all clobbered regs but r11 and r12; r11 = call hook_address (data, r11); restore regs; br\n   r11 r11 is a generator temp reg which is not used across bb borders. */\nvoid *_MIR_get_bb_wrapper (MIR_context_t ctx, void *data, void *hook_address) {\n  static const uint32_t prologue[] = {\n    0x7d800026, /* mfcr r12 */\n    0xf9810008, /* std r12,8(r1) */\n    0x7d8802a6, /* mflr r12 */\n    0xf9810010, /* std  r12,16(r1) */\n  };\n  static const uint32_t epilogue[] = {\n    0xe9810010, /* ld r12,16(r1) */\n    0x7d8803a6, /* mtlr r12 */\n    0xe9810008, /* ld r12,8(r1) */\n    0x7d8ff120, /* mtcr r12 */\n  };\n  int frame_size = PPC64_STACK_HEADER_SIZE + 14 * 8 + 14 * 8 + 8 * 8;\n  void *res;\n  VARR (uint8_t) * code;\n\n  VARR_CREATE (uint8_t, code, ctx->alloc, 256);\n  push_insns (code, prologue, sizeof (prologue));\n  /* stdu r1,n(r1): header + 14(gp regs, r{1,2,11} space alloc is not used) + 14(fp args) + 8(param\n   * area): */\n  if (frame_size % 16 != 0) frame_size += 8;\n  ppc64_gen_stdu (code, -frame_size);\n  ppc64_gen_st (code, R0_HARD_REG, SP_HARD_REG, PPC64_STACK_HEADER_SIZE + R0_HARD_REG * 8 + 64,\n                MIR_T_I64);\n  for (unsigned reg = R2_HARD_REG; reg <= R10_HARD_REG; reg++) /* ld rn,dispn(r1) : */\n    ppc64_gen_st (code, reg, SP_HARD_REG, PPC64_STACK_HEADER_SIZE + reg * 8 + 64, MIR_T_I64);\n  ppc64_gen_st (code, R13_HARD_REG, SP_HARD_REG, PPC64_STACK_HEADER_SIZE + R13_HARD_REG * 8 + 64,\n                MIR_T_I64);\n  for (unsigned reg = 0; reg <= F13_HARD_REG - F0_HARD_REG; reg++) /* lfd fn,dispn(r1) : */\n    ppc64_gen_st (code, reg, SP_HARD_REG, PPC64_STACK_HEADER_SIZE + (reg + 14) * 8 + 64, MIR_T_D);\n  ppc64_gen_address (code, 3, data);          /* r3 = data */\n  ppc64_gen_mov (code, 4, 11);                /* r4 = r11 */\n  ppc64_gen_address (code, 12, hook_address); /* r12 = hook addres */\n  ppc64_gen_call (code, 12);                  /* call r12 */\n  ppc64_gen_mov (code, 11, 3);                /* r11 = r3 */\n  ppc64_gen_ld (code, R0_HARD_REG, SP_HARD_REG, PPC64_STACK_HEADER_SIZE + R0_HARD_REG * 8 + 64,\n                MIR_T_I64);\n  for (unsigned reg = R2_HARD_REG; reg <= R10_HARD_REG; reg++) /* ld rn,dispn(r1) : */\n    ppc64_gen_ld (code, reg, SP_HARD_REG, PPC64_STACK_HEADER_SIZE + reg * 8 + 64, MIR_T_I64);\n  ppc64_gen_ld (code, R13_HARD_REG, SP_HARD_REG, PPC64_STACK_HEADER_SIZE + R13_HARD_REG * 8 + 64,\n                MIR_T_I64);\n  for (unsigned reg = 0; reg <= F13_HARD_REG - F0_HARD_REG; reg++) /* lfd fn,dispn(r1) : */\n    ppc64_gen_ld (code, reg, SP_HARD_REG, PPC64_STACK_HEADER_SIZE + (reg + 14) * 8 + 64, MIR_T_D);\n  ppc64_gen_addi (code, 1, 1, frame_size);\n  push_insns (code, epilogue, sizeof (epilogue));\n  push_insn (code, (31 << 26) | (467 << 1) | (11 << 21) | (9 << 16)); /* mctr 11 */\n  push_insn (code, (19 << 26) | (528 << 1) | (20 << 21));             /* bcctr */\n  res = _MIR_publish_code (ctx, VARR_ADDR (uint8_t, code), VARR_LENGTH (uint8_t, code));\n#if 0\n  if (getenv (\"MIR_code_dump\") != NULL)\n    _MIR_dump_code (\"bb wrapper:\", res, VARR_LENGTH (uint8_t, code));\n#endif\n  VARR_DESTROY (uint8_t, code);\n  return res;\n}\n"
        },
        {
          "name": "mir-ppc64.h",
          "type": "blob",
          "size": 4.9521484375,
          "content": "/* This file is a part of MIR project.\n   Copyright (C) 2018-2024 Vladimir Makarov <vmakarov.gcc@gmail.com>.\n   A common include file for mir-ppc64.c and mir-gen-ppc64.c\n*/\n\n#include \"mir.h\"\n\n#if __BYTE_ORDER__ == __ORDER_BIG_ENDIAN__\n#error \"pp64 big endian is not supported anymore\"\n#endif\n\n#define PPC64_STACK_HEADER_SIZE 32\n#define PPC64_TOC_OFFSET 24\n#define PPC64_FUNC_DESC_LEN 0\n\n#define TARGET_NOP (24 << (32 - 6)) /* ori 0,0,0 */\n\n#define HREG_EL(h) h##_HARD_REG\n#define REP_SEP ,\nenum {\n  REP8 (HREG_EL, R0, R1, R2, R3, R4, R5, R6, R7),\n  REP8 (HREG_EL, R8, R9, R10, R11, R12, R13, R14, R15),\n  REP8 (HREG_EL, R16, R17, R18, R19, R20, R21, R22, R23),\n  REP8 (HREG_EL, R24, R25, R26, R27, R28, R29, R30, R31),\n  REP8 (HREG_EL, F0, F1, F2, F3, F4, F5, F6, F7),\n  REP8 (HREG_EL, F8, F9, F10, F11, F12, F13, F14, F15),\n  REP8 (HREG_EL, F16, F17, F18, F19, F20, F21, F22, F23),\n  REP8 (HREG_EL, F24, F25, F26, F27, F28, F29, F30, F31),\n  HREG_EL (LR),\n};\n#undef REP_SEP\n\nstatic const char *const target_hard_reg_names[] = {\n  \"r0\",  \"r1\",  \"r2\",  \"r3\",  \"r4\",  \"r5\",  \"r6\",  \"r7\",  \"r8\",  \"r9\",  \"r10\", \"r11\", \"r12\",\n  \"r13\", \"r14\", \"r15\", \"r16\", \"r17\", \"r18\", \"r19\", \"r20\", \"r21\", \"r22\", \"r23\", \"r24\", \"r25\",\n  \"r26\", \"r27\", \"r28\", \"r29\", \"r30\", \"r31\", \"f0\",  \"f1\",  \"f2\",  \"f3\",  \"f4\",  \"f5\",  \"f6\",\n  \"f7\",  \"f8\",  \"f9\",  \"f10\", \"f11\", \"f12\", \"f13\", \"f14\", \"f15\", \"f16\", \"f17\", \"f18\", \"f19\",\n  \"f20\", \"f21\", \"f22\", \"f23\", \"f24\", \"f25\", \"f26\", \"f27\", \"f28\", \"f29\", \"f30\", \"f31\", \"lr\",\n};\n\n#define MAX_HARD_REG LR_HARD_REG\nstatic const MIR_reg_t SP_HARD_REG = R1_HARD_REG;\nstatic const MIR_reg_t FP_HARD_REG = R31_HARD_REG;\n\n/* Hard regs not used in machinized code, preferably call used ones. */\nstatic const MIR_reg_t TEMP_INT_HARD_REG1 = R11_HARD_REG, TEMP_INT_HARD_REG2 = R12_HARD_REG;\nstatic const MIR_reg_t TEMP_FLOAT_HARD_REG1 = F11_HARD_REG, TEMP_FLOAT_HARD_REG2 = F12_HARD_REG;\nstatic const MIR_reg_t TEMP_DOUBLE_HARD_REG1 = F11_HARD_REG, TEMP_DOUBLE_HARD_REG2 = F12_HARD_REG;\nstatic const MIR_reg_t TEMP_LDOUBLE_HARD_REG1 = F11_HARD_REG;  //???\nstatic const MIR_reg_t TEMP_LDOUBLE_HARD_REG2 = F12_HARD_REG;\n\nstatic inline int target_hard_reg_type_ok_p (MIR_reg_t hard_reg, MIR_type_t type) {\n  assert (hard_reg <= MAX_HARD_REG);\n  if (type == MIR_T_LD) return FALSE;\n  return MIR_fp_type_p (type) ? F0_HARD_REG <= hard_reg && hard_reg <= F31_HARD_REG\n                              : hard_reg < F0_HARD_REG;\n}\n\nstatic inline int target_fixed_hard_reg_p (MIR_reg_t hard_reg) {\n  assert (hard_reg <= MAX_HARD_REG);\n  return (hard_reg == FP_HARD_REG || hard_reg == SP_HARD_REG\n          || hard_reg == LR_HARD_REG\n          /* don't bother to allocate R0 as it has special meaning for base reg and of addi: */\n          || hard_reg == R0_HARD_REG || hard_reg == R2_HARD_REG || hard_reg == R13_HARD_REG\n          || hard_reg == TEMP_INT_HARD_REG1 || hard_reg == TEMP_INT_HARD_REG2\n          || hard_reg == TEMP_FLOAT_HARD_REG1 || hard_reg == TEMP_FLOAT_HARD_REG2\n          || hard_reg == TEMP_DOUBLE_HARD_REG1 || hard_reg == TEMP_DOUBLE_HARD_REG2\n          || hard_reg == TEMP_LDOUBLE_HARD_REG1 || hard_reg == TEMP_LDOUBLE_HARD_REG2);\n}\n\nstatic int target_locs_num (MIR_reg_t loc MIR_UNUSED, MIR_type_t type) {\n  return /*loc > MAX_HARD_REG && */ type == MIR_T_LD ? 2 : 1;\n}\n\nstatic inline void push_insn (VARR (uint8_t) * insn_varr, uint32_t insn) {\n  uint8_t *p = (uint8_t *) &insn;\n  for (size_t i = 0; i < 4; i++) VARR_PUSH (uint8_t, insn_varr, p[i]);\n}\n\nstatic const int PPC_JUMP_OPCODE = 18;\n\n#define LI_OPCODE 14\n#define LIS_OPCODE 15\n#define ORI_OPCODE 24\n#define ORIS_OPCODE 25\n#define XOR_OPCODE 31\nstatic inline void ppc64_gen_address (VARR (uint8_t) * insn_varr, unsigned int reg, void *p) {\n  uint64_t a = (uint64_t) p;\n  if ((a >> 32) == 0) {\n    if (((a >> 31) & 1) == 0) { /* lis r,0,Z2 */\n      push_insn (insn_varr, (LIS_OPCODE << 26) | (reg << 21) | (0 << 16) | ((a >> 16) & 0xffff));\n    } else { /* xor r,r,r; oris r,r,Z2 */\n      push_insn (insn_varr,\n                 (XOR_OPCODE << 26) | (316 << 1) | (reg << 21) | (reg << 16) | (reg << 11));\n      push_insn (insn_varr, (ORIS_OPCODE << 26) | (reg << 21) | (reg << 16) | ((a >> 16) & 0xffff));\n    }\n  } else {\n    if ((a >> 47) != 0) {\n      /* lis r,0,Z0; [ori r,r,Z1]; rldicr r,r,32,31; [oris r,r,Z2]; [ori r,r,Z3]: */\n      push_insn (insn_varr, (LIS_OPCODE << 26) | (reg << 21) | (0 << 16) | (a >> 48));\n      if (((a >> 32) & 0xffff) != 0)\n        push_insn (insn_varr,\n                   (ORI_OPCODE << 26) | (reg << 21) | (reg << 16) | ((a >> 32) & 0xffff));\n    } else {\n      /* li r,0,Z1; rldicr r,r,32,31; [oris r,r,Z2]; [ori r,r,Z3]: */\n      push_insn (insn_varr, (LI_OPCODE << 26) | (reg << 21) | (0 << 16) | ((a >> 32) & 0xffff));\n    }\n    push_insn (insn_varr, (30 << 26) | (reg << 21) | (reg << 16) | 0x07c6);\n    if (((a >> 16) & 0xffff) != 0)\n      push_insn (insn_varr, (ORIS_OPCODE << 26) | (reg << 21) | (reg << 16) | ((a >> 16) & 0xffff));\n  }\n  if ((a & 0xffff) != 0)\n    push_insn (insn_varr, (ORI_OPCODE << 26) | (reg << 21) | (reg << 16) | (a & 0xffff));\n}\n"
        },
        {
          "name": "mir-reduce.h",
          "type": "blob",
          "size": 16.12890625,
          "content": "/* This file is a part of MIR project.\n   Copyright (C) 2018-2024 Vladimir Makarov <vmakarov.gcc@gmail.com>.\n*/\n\n#ifndef MIR_REDUCE_H\n#define MIR_REDUCE_H\n\n/* Data compression.  Major goals are simplicity, fast decompression\n   speed, moderate compression speed.  The algorithm is tuned for\n   binary MIR compression and close to LZ4.  Only we use a bit\n   different format and offsets in symbol numbers instead of just\n   offsets.\n\n   A better compression (on par with LZ4) could be achieved by adding\n   elements for all positions (now positions inside referenced symbols\n   are excluded) or/and increasing the buffer or/and increasing the\n   table.  But it would slow down the compression or/and increase the\n   used memory.\n\n   Functions reduce_encode, reduce_decode, reduce_encode_start,\n   reduce_encode_put, reduce_encode_finish, reduce_decode_start,\n   reduce_decode_get, reduce_decode_finish are the only interface\n   functions.\n\n   Format of compressed data: \"MIR\", elements*, zero byte, 8-byte check hash in little endian form\n   Format of element:\n    o 8 bits tag\n      (N bits for symbol length; 0 means no sym, 2^N -1 means symbol length as uint present;\n      (8-N) bits for reference length; 0 means no ref, 2^(8-N) - 1 means length as uint present)\n    o [uint for symbol lenght]*, symbol string,\n    o [uint for ref len]*, symbol number as uint */\n\n#include <stdlib.h>\n#include <string.h>\n#include <assert.h>\n#include \"mir-hash.h\"\n#include \"mir-alloc.h\"\n\n#define FALSE 0\n#define TRUE 1\n#define _REDUCE_DATA_PREFIX \"MIR\" /* first chars of compressed data */\n#define _REDUCE_SYMB_TAG_LEN 3    /* for some application could be 4 */\n#define _REDUCE_SYMB_TAG_LONG ((1 << _REDUCE_SYMB_TAG_LEN) - 1) /* should be not changed */\n#define _REDUCE_REF_TAG_LEN (8 - _REDUCE_SYMB_TAG_LEN)\n#define _REDUCE_REF_TAG_LONG ((1 << _REDUCE_REF_TAG_LEN) - 1) /* should be not changed */\n#define _REDUCE_START_LEN 4                                   /* Should be at least 4 */\n#define _REDUCE_BUF_LEN (1 << 18)\n/* The following should be power of two. There will be no space saving if it is less than 1/4 of buf\n   length. */\n#define _REDUCE_TABLE_SIZE (_REDUCE_BUF_LEN / 4)\n#define _REDUCE_MAX_SYMB_LEN (2047)\n\ntypedef size_t (*reduce_reader_t) (void *start, size_t len, void *aux_data);\ntypedef size_t (*reduce_writer_t) (const void *start, size_t len, void *aux_data);\n\nstruct _reduce_el {\n  uint32_t pos, num, next, head;\n};\n\nstruct _reduce_encode_data {\n  reduce_writer_t writer;\n  uint32_t el_free;\n  uint32_t curr_symb_len;\n  uint8_t curr_symb[_REDUCE_MAX_SYMB_LEN];\n  struct _reduce_el table[_REDUCE_TABLE_SIZE]; /* hash -> el */\n};\n\nstruct _reduce_decode_data {\n  uint8_t eof_p;\n  uint32_t buf_get_pos;\n  reduce_reader_t reader;\n  uint32_t ind2pos[_REDUCE_BUF_LEN];\n};\n\nstruct reduce_data {\n  union {\n    struct _reduce_encode_data encode;\n    struct _reduce_decode_data decode;\n  } u;\n  void *aux_data;\n  uint8_t ok_p;\n  uint64_t check_hash;\n  uint32_t curr_num, buf_bound;\n  uint8_t buf[_REDUCE_BUF_LEN];\n};\n\nstatic inline uint32_t _reduce_min (uint32_t a, uint32_t b) { return a < b ? a : b; }\n\nstatic inline uint32_t _reduce_get_new_el (struct reduce_data *data) {\n  struct _reduce_encode_data *encode_data = &data->u.encode;\n  uint32_t res = encode_data->el_free;\n\n  if (res != UINT32_MAX) encode_data->el_free = encode_data->table[res].next;\n  return res;\n}\n\nstatic inline void _reduce_put (struct reduce_data *data, int byte) {\n  uint8_t u = byte;\n\n  if (data->u.encode.writer (&u, 1, data->aux_data) != 1) data->ok_p = FALSE;\n}\n\nstatic inline int _reduce_get (reduce_reader_t reader, void *aux_data) {\n  uint8_t u;\n\n  if (reader (&u, 1, aux_data) != 1) return -1;\n  return u;\n}\n\nstatic inline uint32_t _reduce_ref_offset_size (uint32_t offset) {\n  return offset < (1 << 7) ? 1 : offset < (1 << 14) ? 2 : offset < (1 << 21) ? 3 : 4;\n}\n\nstatic inline uint32_t _reduce_ref_size (uint32_t len, uint32_t offset) {\n  assert (len >= _REDUCE_START_LEN);\n  len -= _REDUCE_START_LEN - 1;\n  return ((len < _REDUCE_REF_TAG_LONG ? 0 : _reduce_ref_offset_size (len))\n          + _reduce_ref_offset_size (offset));\n}\n\nstatic inline void _reduce_uint_write (struct reduce_data *data, uint32_t u) {\n  int n;\n\n  assert (u < (1 << 7 * 4));\n  for (n = 1; n <= 4 && u >= (1u << 7 * n); n++)\n    ;\n  _reduce_put (data, (1 << (8 - n)) | ((u >> (n - 1) * 8) & 0xff)); /* tag */\n  for (int i = 2; i <= n; i++) _reduce_put (data, (u >> (n - i) * 8) & 0xff);\n}\n\nstatic inline int64_t _reduce_uint_read (reduce_reader_t reader, void *aux_data) {\n  int i, n, r = _reduce_get (reader, aux_data);\n  uint32_t u, v;\n\n  if (r < 0) return -1;\n  for (u = (uint32_t) r, n = 1; n <= 4 && (u >> (8 - n)) != 1; n++)\n    ;\n  assert ((u >> (8 - n)) == 1);\n  v = u & (0xff >> n);\n  for (i = 1; i < n; i++) {\n    if ((r = _reduce_get (reader, aux_data)) < 0) return -1;\n    v = v * 256 + (uint32_t) r;\n  }\n  return v;\n}\n\nstatic inline void _reduce_hash_write (struct reduce_data *data, uint64_t h) {\n  _reduce_put (data, 0); /* 0 tag */\n  for (size_t i = 0; i < sizeof (uint64_t); i++) _reduce_put (data, (h >> i * 8) & 0xff);\n}\n\nstatic inline uint64_t _reduce_str2hash (const uint8_t *s) {\n  uint64_t h = 0;\n\n  for (size_t i = 0; i < sizeof (uint64_t); i++) h |= (uint64_t) s[i] << i * 8;\n  return h;\n}\n\nstatic inline int _reduce_symb_flush (struct reduce_data *data, int ref_tag) {\n  uint8_t u;\n  struct _reduce_encode_data *encode_data = &data->u.encode;\n  uint32_t len = encode_data->curr_symb_len;\n\n  if (len == 0 && ref_tag == 0) return FALSE;\n  u = ((len < _REDUCE_SYMB_TAG_LONG ? len : _REDUCE_SYMB_TAG_LONG) << _REDUCE_REF_TAG_LEN)\n      | ref_tag;\n  encode_data->writer (&u, 1, data->aux_data);\n  if (len >= _REDUCE_SYMB_TAG_LONG) _reduce_uint_write (data, len);\n  encode_data->writer (encode_data->curr_symb, len, data->aux_data);\n  encode_data->curr_symb_len = 0;\n  return TRUE;\n}\n\nstatic inline void _reduce_output_byte (struct reduce_data *data, uint32_t pos) {\n  struct _reduce_encode_data *encode_data = &data->u.encode;\n\n  if (encode_data->curr_symb_len + 1 > _REDUCE_MAX_SYMB_LEN) {\n    _reduce_symb_flush (data, 0);\n    encode_data->curr_symb_len = 0;\n  }\n  encode_data->curr_symb[encode_data->curr_symb_len++] = data->buf[pos];\n}\n\nstatic inline void _reduce_output_ref (struct reduce_data *data, uint32_t offset, uint32_t len) {\n  uint32_t ref_tag;\n\n  assert (len >= _REDUCE_START_LEN);\n  len -= _REDUCE_START_LEN - 1;\n  ref_tag = len < _REDUCE_REF_TAG_LONG ? len : _REDUCE_REF_TAG_LONG;\n  _reduce_symb_flush (data, ref_tag);\n  if (len >= _REDUCE_REF_TAG_LONG) _reduce_uint_write (data, len);\n  _reduce_uint_write (data, offset);\n}\n\n#define _REDUCE_HASH_SEED 24\n\nstatic inline uint32_t _reduce_dict_find_longest (struct reduce_data *data, uint32_t pos,\n                                                  uint32_t *dict_pos) {\n  uint32_t len, best_len, len_bound;\n  uint64_t hash;\n  uint32_t off, ref_size, best_ref_size = 0;\n  uint32_t curr, next;\n  const uint8_t *s1, *s2;\n  struct _reduce_el *el, *best_el = NULL;\n  struct _reduce_encode_data *encode_data = &data->u.encode;\n\n  if (pos + _REDUCE_START_LEN > data->buf_bound) return 0;\n  /* To have the same compressed output independently of the target\n     and the used compiler, use strict hash even if it decreases\n     compression speed by 10%.  */\n  hash\n    = mir_hash_strict (&data->buf[pos], _REDUCE_START_LEN, _REDUCE_HASH_SEED) % _REDUCE_TABLE_SIZE;\n  best_len = 0; /* to remove a warning */\n  for (curr = encode_data->table[hash].head; curr != UINT32_MAX; curr = next) {\n    next = encode_data->table[curr].next;\n    el = &encode_data->table[curr];\n    len_bound = _reduce_min (data->buf_bound - pos, pos - el->pos);\n    if (len_bound < _REDUCE_START_LEN) continue;\n    s1 = &data->buf[el->pos];\n    s2 = &data->buf[pos];\n#if MIR_HASH_UNALIGNED_ACCESS\n    assert (_REDUCE_START_LEN >= 4);\n    if (*(uint32_t *) &s1[0] != *(uint32_t *) &s2[0]) continue;\n    len = 4;\n#else\n    len = 0;\n#endif\n    for (; len < len_bound; len++)\n      if (s1[len] != s2[len]) break;\n#if !MIR_HASH_UNALIGNED_ACCESS\n    if (len < _REDUCE_START_LEN) continue;\n#endif\n    off = data->curr_num - el->num;\n    if (best_el == NULL) {\n      best_len = len;\n      best_el = el;\n      best_ref_size = _reduce_ref_size (len, off);\n      continue;\n    }\n    ref_size = _reduce_ref_size (len, off);\n    if (best_len + ref_size < len + best_ref_size) {\n      best_len = len;\n      best_el = el;\n      best_ref_size = ref_size;\n    }\n  }\n  if (best_el == NULL) return 0;\n  *dict_pos = best_el->num;\n  return best_len;\n}\n\nstatic inline void _reduce_dict_add (struct reduce_data *data, uint32_t pos) {\n  uint64_t hash;\n  uint32_t prev, curr, num = data->curr_num++;\n  struct _reduce_encode_data *encode_data = &data->u.encode;\n\n  if (pos + _REDUCE_START_LEN > data->buf_bound) return;\n  hash\n    = mir_hash_strict (&data->buf[pos], _REDUCE_START_LEN, _REDUCE_HASH_SEED) % _REDUCE_TABLE_SIZE;\n  if ((curr = _reduce_get_new_el (data)) == UINT32_MAX) { /* rare case: use last if any */\n    for (prev = UINT32_MAX, curr = encode_data->table[hash].head;\n         curr != UINT32_MAX && encode_data->table[curr].next != UINT32_MAX;\n         prev = curr, curr = encode_data->table[curr].next)\n      ;\n    if (curr == UINT32_MAX) return; /* no more free els */\n    if (prev != UINT32_MAX)\n      encode_data->table[prev].next = encode_data->table[curr].next;\n    else\n      encode_data->table[hash].head = encode_data->table[curr].next;\n  }\n  encode_data->table[curr].pos = pos;\n  encode_data->table[curr].num = num;\n  encode_data->table[curr].next = encode_data->table[hash].head;\n  encode_data->table[hash].head = curr;\n}\n\nstatic void _reduce_reset_next (struct reduce_data *data) {\n  struct _reduce_encode_data *encode_data = &data->u.encode;\n\n  for (uint32_t i = 0; i < _REDUCE_TABLE_SIZE; i++) {\n    encode_data->table[i].next = i + 1;\n    encode_data->table[i].head = UINT32_MAX;\n  }\n  encode_data->table[_REDUCE_TABLE_SIZE - 1].next = UINT32_MAX;\n  encode_data->el_free = 0;\n}\n\n#define _REDUCE_CHECK_HASH_SEED 42\n\nstatic inline struct reduce_data *reduce_encode_start (MIR_alloc_t alloc, reduce_writer_t writer,\n                                                       void *aux_data) {\n  struct reduce_data *data = MIR_malloc (alloc, sizeof (struct reduce_data));\n  char prefix[] = _REDUCE_DATA_PREFIX;\n  size_t prefix_size = strlen (prefix);\n\n  if (data == NULL) return data;\n  data->u.encode.writer = writer;\n  data->aux_data = aux_data;\n  data->check_hash = _REDUCE_CHECK_HASH_SEED;\n  data->buf_bound = 0;\n  data->ok_p = writer (prefix, prefix_size, aux_data) == prefix_size;\n  return data;\n}\n\nstatic inline void _reduce_encode_buf (struct reduce_data *data) {\n  uint32_t dict_len, dict_pos, base;\n\n  if (data->buf_bound == 0) return;\n  data->check_hash = mir_hash_strict (data->buf, data->buf_bound, data->check_hash);\n  data->curr_num = data->u.encode.curr_symb_len = 0;\n  _reduce_reset_next (data);\n  for (uint32_t pos = 0; pos < data->buf_bound;) {\n    dict_len = _reduce_dict_find_longest (data, pos, &dict_pos);\n    base = data->curr_num;\n    if (dict_len == 0) {\n      _reduce_output_byte (data, pos);\n      _reduce_dict_add (data, pos);\n      pos++;\n      continue;\n    }\n    _reduce_output_ref (data, base - dict_pos, dict_len);\n    _reduce_dict_add (data, pos); /* replace */\n    pos += dict_len;\n  }\n  _reduce_symb_flush (data, 0);\n}\n\nstatic inline void reduce_encode_put (struct reduce_data *data, int c) {\n  if (data->buf_bound < _REDUCE_BUF_LEN) {\n    data->buf[data->buf_bound++] = c;\n    return;\n  }\n  _reduce_encode_buf (data);\n  data->buf_bound = 0;\n  data->buf[data->buf_bound++] = c;\n}\n\nstatic inline int reduce_encode_finish (MIR_alloc_t alloc, struct reduce_data *data) {\n  int ok_p;\n\n  _reduce_encode_buf (data);\n  _reduce_hash_write (data, data->check_hash);\n  ok_p = data->ok_p;\n  MIR_free (alloc, data);\n  return ok_p;\n}\n\nstatic inline struct reduce_data *reduce_decode_start (MIR_alloc_t alloc, reduce_reader_t reader,\n                                                       void *aux_data) {\n  struct reduce_data *data = MIR_malloc (alloc, sizeof (struct reduce_data));\n  struct _reduce_decode_data *decode_data = &data->u.decode;\n  char prefix[] = _REDUCE_DATA_PREFIX, str[sizeof (prefix)];\n  size_t prefix_size = strlen (prefix);\n\n  if (data == NULL) return data;\n  decode_data->reader = reader;\n  data->aux_data = aux_data;\n  data->check_hash = _REDUCE_CHECK_HASH_SEED;\n  decode_data->buf_get_pos = data->buf_bound = 0;\n  data->ok_p\n    = reader (str, prefix_size, aux_data) == prefix_size && memcmp (prefix, str, prefix_size) == 0;\n  decode_data->eof_p = FALSE;\n  return data;\n}\n\nstatic inline int reduce_decode_get (struct reduce_data *data) {\n  uint8_t tag, hash_str[sizeof (uint64_t)];\n  uint32_t sym_len, ref_len, ref_ind, sym_pos, pos = 0, curr_ind = 0;\n  int64_t r;\n  struct _reduce_decode_data *decode_data = &data->u.decode;\n  reduce_reader_t reader = decode_data->reader;\n\n  if (decode_data->buf_get_pos < data->buf_bound) return data->buf[decode_data->buf_get_pos++];\n  if (decode_data->eof_p) return -1;\n  for (;;) {\n    if (reader (&tag, 1, data->aux_data) == 0) break;\n    if (tag == 0) { /* check hash */\n      if (reader (hash_str, sizeof (hash_str), data->aux_data) != sizeof (hash_str)\n          || reader (&tag, 1, data->aux_data) != 0)\n        break;\n      if (pos != 0) data->check_hash = mir_hash_strict (data->buf, pos, data->check_hash);\n      if (_reduce_str2hash (hash_str) != data->check_hash) break;\n      decode_data->eof_p = TRUE;\n      decode_data->buf_get_pos = 0;\n      data->buf_bound = pos;\n      return pos == 0 ? -1 : data->buf[decode_data->buf_get_pos++];\n    }\n    sym_len = tag >> _REDUCE_REF_TAG_LEN;\n    if (sym_len != 0) {\n      if (sym_len == _REDUCE_SYMB_TAG_LONG) {\n        if ((r = _reduce_uint_read (reader, data->aux_data)) < 0) break;\n        sym_len = (uint32_t) r;\n      }\n      if (sym_len > _REDUCE_MAX_SYMB_LEN || pos + sym_len > _REDUCE_BUF_LEN) break;\n      if (reader (&data->buf[pos], sym_len, data->aux_data) != sym_len) break;\n      for (uint32_t i = 0; i < sym_len; i++, pos++, curr_ind++)\n        decode_data->ind2pos[curr_ind] = pos;\n    }\n    ref_len = tag & _REDUCE_REF_TAG_LONG;\n    if (ref_len != 0) {\n      if (ref_len == _REDUCE_REF_TAG_LONG) {\n        if ((r = _reduce_uint_read (reader, data->aux_data)) < 0) break;\n        ref_len = (uint32_t) r;\n      }\n      ref_len += _REDUCE_START_LEN - 1;\n      if ((r = _reduce_uint_read (reader, data->aux_data)) < 0) break;\n      ref_ind = (uint32_t) r;\n      if (curr_ind < ref_ind) break;\n      sym_pos = decode_data->ind2pos[curr_ind - ref_ind];\n      if (sym_pos + ref_len > _REDUCE_BUF_LEN) break;\n      memcpy (&data->buf[pos], &data->buf[sym_pos], ref_len);\n      decode_data->ind2pos[curr_ind++] = pos;\n      pos += ref_len;\n    }\n    if (pos >= _REDUCE_BUF_LEN) {\n      assert (pos == _REDUCE_BUF_LEN);\n      data->check_hash = mir_hash_strict (data->buf, pos, data->check_hash);\n      data->buf_bound = _REDUCE_BUF_LEN;\n      decode_data->buf_get_pos = 0;\n      return data->buf[decode_data->buf_get_pos++];\n    }\n  }\n  data->ok_p = FALSE;\n  return -1;\n}\n\nstatic inline int reduce_decode_finish (MIR_alloc_t alloc, struct reduce_data *data) {\n  uint8_t tag;\n  int ok_p\n    = data->ok_p && data->u.decode.eof_p && data->u.decode.reader (&tag, 1, data->aux_data) == 0;\n\n  MIR_free (alloc, data);\n  return ok_p;\n}\n\n#define _REDUCE_WRITE_IO_LEN 256\nstatic inline int reduce_encode (MIR_alloc_t alloc, reduce_reader_t reader, reduce_writer_t writer,\n                                 void *aux_data) {\n  size_t i, size;\n  uint8_t buf[_REDUCE_WRITE_IO_LEN];\n  struct reduce_data *data = reduce_encode_start (alloc, writer, aux_data);\n\n  if (data == NULL) return FALSE;\n  for (;;) {\n    if ((size = reader (buf, _REDUCE_WRITE_IO_LEN, data->aux_data)) == 0) break;\n    for (i = 0; i < size; i++) reduce_encode_put (data, buf[i]);\n  }\n  return reduce_encode_finish (alloc, data);\n}\n\nstatic inline int reduce_decode (MIR_alloc_t alloc, reduce_reader_t reader, reduce_writer_t writer,\n                                 void *aux_data) {\n  int c, i;\n  uint8_t buf[_REDUCE_WRITE_IO_LEN];\n  struct reduce_data *data = reduce_decode_start (alloc, reader, aux_data);\n\n  if (data == NULL) return FALSE;\n  for (;;) {\n    for (i = 0; i < _REDUCE_WRITE_IO_LEN && (c = reduce_decode_get (data)) >= 0; i++) buf[i] = c;\n    if (i != 0) writer (buf, i, aux_data);\n    if (c < 0) break;\n  }\n  return reduce_decode_finish (alloc, data);\n}\n\n#endif /* #ifndef MIR_REDUCE_H */\n"
        },
        {
          "name": "mir-riscv64.c",
          "type": "blob",
          "size": 45.48046875,
          "content": "/* This file is a part of MIR project.\n   Copyright (C) 2018-2024 Vladimir Makarov <vmakarov.gcc@gmail.com>.\n*/\n\n#include \"mir-riscv64.h\"\n\n/* x0 (zero) - always zero; x1 (ra) - link reg; x2 (sp) - sp, x3 (gp) - global pointer, x4 (tp) -\n   thread pointer; x8 (s0/fp) - fp; x10-x11 (a0-a1), f10-f11 (fa0-fa1) - ret values, x10-x17\n   (a0-a7), f10-f17 (fa0-fa7) - arg regs; x8-x9 (s0-s1), x18-x27 (s2-s11) - callee-saved; x1 (ra),\n   x5-x7 (t0-t2), x10-x17 (a0-a7), x28-x31 (t3-t6) - temp regs f0-f7 (ft0-ft7), f10-f17 (fa0-fa7),\n   f28-f31 (ft8-ft11) - temp regs f8-f9 (fs0-fs1), f18-f27 (fs2-fs11) - callee-saved\n\n   o pc holds address of the current insn\n   o stack is 16-byte aligned\n   o long double is 128-bit\n   o va_list is a pointer\n   o 1-16-bytes structs/unions are passed through int regs (or stack) unless they contains only\n     float/double values\n   o 9 to 16-bytes values are passed in even (least-significant bits) odd\n     register pair (one arg reg can be skipped for this)\n   o 9 to 16-bytes values can be passed partially in reg and stack\n   o 17 or more bytes values are passed by reference (caller allocates memory for this)\n   o all fp varargs are passed only through int regs (or stack)\n   o 1 to 16-bytes values are returned in int regs (x10-x11) unless they are float or\n     struct/union containing only float/double (in this case they are returned through f10-f11)\n   o 17 or more bytes values are returned on stack (allocated by caller)\n     whose address is passed by x10 (a0)\n   o long doubles for passing and returning purposes are integer\n   o empty struct args are ignored\n*/\n\nstatic const int a0_num = 10;\nstatic const int fa0_num = 10;\n\n/* Small block types (less or equal to two quadwords) args are passed in\n   BLK: int regs and/or on stack (w/o address)\n   BLK1: int regs (even-odd for 9-16 bytes) and/or on stack (w/o address)\n\n   Otherwise any BLK is put somewhere on the stack and its address passed instead.\n   All RBLK independently of size is always passed by address as an usual argument.  */\n\nvoid *_MIR_get_bstart_builtin (MIR_context_t ctx) {\n#if __riscv_compressed\n  static const uint16_t bstart_code[] = {\n    0x850a, /* c.mv a0,sp */\n    0x8082, /* c.jr ra */\n  };\n#else\n  static const uint32_t bstart_code[] = {\n    0x00010513, /* addi a0,sp,0 */\n    0x00008067, /* jalr zero,0(ra) */\n  };\n#endif\n  return _MIR_publish_code (ctx, (uint8_t *) bstart_code, sizeof (bstart_code));\n}\n\nvoid *_MIR_get_bend_builtin (MIR_context_t ctx) {\n#if __riscv_compressed\n  static const uint16_t bend_code[] = {\n    0x812a, /* c.mv sp,a0 */\n    0x8082, /* c.jr ra */\n  };\n#else\n  static const uint32_t bend_code[] = {\n    0x00050113, /* addi sp,a0,0 */\n    0x00008067, /* jalr zero,0(ra) */\n  };\n#endif\n  return _MIR_publish_code (ctx, (uint8_t *) bend_code, sizeof (bend_code));\n}\n\n#define VA_LIST_IS_ARRAY_P 0\nstruct riscv64_va_list {\n  uint64_t *arg_area;\n};\n\nvoid *va_arg_builtin (void *p, uint64_t t) {\n  struct riscv64_va_list *va = p;\n  MIR_type_t type = t;\n  void *a = va->arg_area;\n\n  if (type == MIR_T_LD && __SIZEOF_LONG_DOUBLE__ == 16) {\n    va->arg_area = a = (char *) (((size_t) a + 15) / 16 * 16); /* align */\n    va->arg_area += 2;\n  } else {\n    va->arg_area++;\n  }\n#if __BYTE_ORDER__ == __ORDER_BIG_ENDIAN__\n  if (type == MIR_T_F || type == MIR_T_I32) a = (char *) a + 4; /* 2nd word of doubleword */\n#endif\n  return a;\n}\n\nvoid va_block_arg_builtin (void *res, void *p, size_t s, uint64_t ncase) {\n  struct riscv64_va_list *va = p;\n  void *a = (void *) va->arg_area;\n  if (s <= 2 * 8) {\n    if (s > 8 && ncase == 1) {                                   /* BLK1: */\n      va->arg_area = a = (char *) (((size_t) a + 15) / 16 * 16); /* align */\n    }\n    va->arg_area += (s + sizeof (uint64_t) - 1) / sizeof (uint64_t);\n  } else {\n    a = *(void **) a;\n    va->arg_area++;\n  }\n  if (res != NULL) memcpy (res, a, s);\n}\n\nvoid va_start_interp_builtin (MIR_context_t ctx MIR_UNUSED, void *p, void *a) {\n  struct riscv64_va_list *va = p;\n  va_list *vap = a;\n\n  assert (sizeof (struct riscv64_va_list) == sizeof (va_list));\n  *va = *(struct riscv64_va_list *) vap;\n}\n\nvoid va_end_interp_builtin (MIR_context_t ctx MIR_UNUSED, void *p MIR_UNUSED) {}\n\nstatic uint8_t *push_insns (VARR (uint8_t) * insn_varr, const void *pat, size_t pat_len) {\n  const uint8_t *p = pat;\n  for (size_t i = 0; i < pat_len; i++) VARR_PUSH (uint8_t, insn_varr, p[i]);\n  return VARR_ADDR (uint8_t, insn_varr) + VARR_LENGTH (uint8_t, insn_varr) - pat_len;\n}\n\n#define MAX_JUMP_CODE 6 /* in insns */\n/* Possible combinations: jal (20-bits); auipc+jalr (32-bits); auipc+ld+jalr+64-bit abs address: */\nvoid *_MIR_get_thunk (MIR_context_t ctx) {\n  static const uint32_t call_pat[MAX_JUMP_CODE] = {\n    /* max 3-insns and aligned abs addr */\n    TARGET_NOP, TARGET_NOP, TARGET_NOP, TARGET_NOP, TARGET_NOP, TARGET_NOP,\n  };\n  return _MIR_publish_code (ctx, (uint8_t *) call_pat, sizeof (call_pat));\n}\n\nstatic int get_jump_code (uint32_t *insns, void *to, int64_t offset, int temp_hard_reg) {\n  assert ((offset & 1) == 0 && 0 <= temp_hard_reg && temp_hard_reg < 32);\n  if (-(1 << 20) <= offset && offset < (1 << 20)) {\n    insns[0] = 0x6f | get_j_format_imm (offset); /* jal */\n    insns[1] = TARGET_NOP;                       /* size should be aligned to 8 */\n    return 8;\n  } else if (-(1l << 31) <= offset && offset < (1 << 31)) {\n    int hi = offset >> 12, low = offset & 0xfff;\n    if ((low & 0x800) != 0) hi++;\n    insns[0] = 0x17 | (temp_hard_reg << 7) | (hi << 12);   /* auipc t */\n    insns[1] = 0x67 | (temp_hard_reg << 15) | (low << 20); /* jalr t */\n    return 8;\n  } else {\n    insns[0] = 0x17 | (temp_hard_reg << 7); /* auipc t,0x0 */\n    insns[1]\n      = 0x0003003 | (16 << 20) | (temp_hard_reg << 7) | (temp_hard_reg << 15); /* ld t,16(t) */\n    insns[2] = 0x67 | (temp_hard_reg << 15);                                   /* jalr t */\n    *(void **) &insns[4] = to;\n    return 24;\n  }\n}\n\nstatic void *get_jump_addr (uint32_t *insns) { /* see get_jump_code */\n  int32_t offset;\n  if ((insns[0] & 0x7f) == 0x6f) { /* jal */\n    offset = (((int32_t) insns[0] >> 30) << 20) | (insns[0] & 0xff000) | ((insns[0] >> 9) & 0x800)\n             | ((insns[0] >> 20) & 0x7fe);\n    return (int8_t *) insns + offset;\n  } else if ((insns[0] & 0x7f) == 0x17 && (insns[1] & 0x7f) == 0x67) {\n    int32_t hi = (int32_t) insns[0] & 0xfffff000, low = (int32_t) insns[1] >> 20;\n    return (int8_t *) insns + hi + low;\n  } else {\n    assert ((insns[0] & ~0xf80) == 0x17 && (insns[1] & ~0xf8f80) == (0x0003003 | (16 << 20))\n            && (insns[2] & 0x7f) == 0x67);\n    return *(void **) &insns[4];\n  }\n}\n\nvoid *_MIR_get_thunk_addr (MIR_context_t ctx MIR_UNUSED, void *thunk) {\n  return get_jump_addr (thunk);\n}\n\nstatic void redirect_thunk (MIR_context_t ctx, void *thunk, void *to, int temp_hard_reg) {\n  uint32_t insns[MAX_JUMP_CODE];\n  uint64_t offset = (uint8_t *) to - (uint8_t *) thunk;\n  int len = get_jump_code (insns, to, offset, temp_hard_reg);\n\n  assert (len <= MAX_JUMP_CODE * 4);\n  _MIR_change_code (ctx, (uint8_t *) thunk, (uint8_t *) insns, len);\n#if 0\n  if (getenv (\"MIR_code_dump\") != NULL)\n    _MIR_dump_code (\"thunk\", thunk, len);\n#endif\n}\n\nvoid _MIR_redirect_thunk (MIR_context_t ctx, void *thunk, void *to) {\n  redirect_thunk (ctx, thunk, to, T0_HARD_REG);\n}\n\nstatic const uint32_t add_sp_pat = 0x00010113;  /* addi sp,sp,0 */\nstatic const uint32_t ld_arg_pat = 0x0004b003;  /* ld zero,0(s1) */\nstatic const uint32_t flw_arg_pat = 0x0004a007; /* flw f0,0(s1) */\nstatic const uint32_t fld_arg_pat = 0x0004b007; /* fld f0,0(s1) */\n\nstatic uint32_t get_i_format_imm (int offset) {\n  assert (-(1 << 11) <= offset && offset < (1 << 11));\n  return offset << 20;\n}\n\nstatic uint32_t get_i_format_rd (int reg) {\n  assert (0 <= reg && reg < 32);\n  return reg << 7;\n}\n\nstatic uint32_t get_s_format_imm (int offset) {\n  assert (-(1 << 11) <= offset && offset < (1 << 11));\n  return (offset >> 5) << 25 | (offset & 0x1f) << 7;\n}\n\nstatic uint32_t get_s_format_rs2 (int reg) {\n  assert (0 <= reg && reg < 32);\n  return reg << 20;\n}\n\nstatic uint32_t get_u_format_imm (int offset) {\n  assert (-(1 << 19) <= offset && offset < (1 << 19));\n  return offset << 12;\n}\n\nstatic uint32_t get_opfp_format_rd (int reg) {\n  assert (0 <= reg && reg < 32);\n  return reg << 7;\n}\n\nstatic uint32_t get_opfp_format_rs1 (int reg) {\n  assert (0 <= reg && reg < 32);\n  return reg << 15;\n}\n\n/* Move qwords from addr_offset(s1) to offset(sp). offset(sp) will be in t1.  */\nstatic void gen_blk_mov (VARR (uint8_t) * insn_varr, size_t offset, size_t addr_offset,\n                         size_t qwords) {\n  static const uint32_t blk_mov_pat[] = {\n    /*  0: */ 0x00010313, /* addi t1,sp,0 (<offset>) */\n    /*  4: */ 0x0004b383, /* ld t2,0(s1) (<addr_offset>(s1)) */\n    /*  8: */ 0x00000e13, /* addi t3,zero,0 */\n    /*  c: */ 0x00000e93, /* addi t4,zero,0 (qwords) */\n    /* 10: */ 0x01c38fb3, /* L:add t6,t2,t3 */\n    /* 14: */ 0x000fbf03, /* ld t5,0(t6) */\n    /* 18: */ 0xfffe8e93, /* addi t4,t4,-1 */\n    /* 1c: */ 0x01c30fb3, /* add t6,t1,t3 */\n    /* 20: */ 0x01efb023, /* sd t5,0(t6) */\n    /* 24: */ 0x008e0e13, /* addi t3,t3,8 */\n    /* 28: */ 0xfe0e94e3, /* bne t4,zero,-28(L) */\n  };\n  static const uint32_t blk_mov_pat2[] = {\n    /*  0: */ 0x00000e17, /* auipc\tt3,0x0 */\n    /*  4: */ 0x000e3303, /* ld\tt1,0(t3) (disp for <offset>(t3)) */\n    /*  8: */ 0x00610333, /* add\tt1,sp,t1 */\n    /*  c: */ 0x000e3383, /* ld\tt2,0(t3) (disp for <addr_offset>(t3)) */\n    /* 10: */ 0x009383b3, /* add\tt2,t2,s1 */\n    /* 14: */ 0x0003b383, /* ld\tt2,0(t2) */\n    /* 18: */ 0x000e3e83, /* ld\tt4,0(t3) (disp for qwords(t3)) */\n    /* 1c: */ 0x00000e13, /* addi\tt3,zero,0 */\n    /* 20: */ 0x01c38fb3, /* add\tt6,t2,t3 */\n    /* 24: */ 0x000fbf03, /* ld\tt5,0(t6) */\n    /* 28: */ 0xfffe8e93, /* addi\tt4,t4,-1 */\n    /* 2c: */ 0x01c30fb3, /* add\tt6,t1,t3 */\n    /* 30: */ 0x01efb023, /* sd\tt5,0(t6) */\n    /* 34: */ 0x008e0e13, /* addi\tt3,t3,8 */\n    /* 38: */ 0xfe0e94e3, /* bne\tt4,zero,20 <L> */\n    /* 3c: */ 0x0000006f, /* jal\tzero,0 */\n  };\n  if (offset < (1 << 11) && addr_offset < (1 << 11) && qwords < (1 < 11)) {\n    uint32_t *addr = (uint32_t *) push_insns (insn_varr, blk_mov_pat, sizeof (blk_mov_pat));\n    addr[0] |= get_i_format_imm (offset);\n    addr[1] |= get_i_format_imm (addr_offset);\n    addr[3] |= get_i_format_imm (qwords);\n  } else {\n    size_t start = VARR_LENGTH (uint8_t, insn_varr), data_start, data_bound;\n    push_insns (insn_varr, blk_mov_pat2, sizeof (blk_mov_pat2));\n    while (VARR_LENGTH (uint8_t, insn_varr) % 8 != 0) VARR_PUSH (uint8_t, insn_varr, 0); /* align */\n    data_start = VARR_LENGTH (uint8_t, insn_varr);\n    push_insns (insn_varr, &offset, sizeof (offset));\n    push_insns (insn_varr, &addr_offset, sizeof (addr_offset));\n    push_insns (insn_varr, &qwords, sizeof (qwords));\n    data_bound = VARR_LENGTH (uint8_t, insn_varr);\n    uint32_t *addr = (uint32_t *) (VARR_ADDR (uint8_t, insn_varr) + start);\n    addr[1] |= get_i_format_imm (data_start - start /* - 4*/);\n    addr[3] |= get_i_format_imm (data_start - start + 8 /* - 12 + 8*/);\n    addr[6] |= get_i_format_imm (data_start - start + 16 /*- 24 + 16*/);\n    addr[15] |= get_j_format_imm (data_bound - start - 15 /* #insns before jal */ * 4);\n  }\n}\n\n/* Generation: fun (fun_addr, res_arg_addresses):\n   push ra, s1; t0=fun_addr; s1=res/arg_addrs; sp-=sp_offset;\n   (arg_reg=mem[s1,offset](or addr of blk copy on the stack)\n    or t1=mem[s1,offset] (or addr of blk copy on the stack); mem[sp,sp_offset]=t1) ...\n   call t0; sp+=offset\n   x10=mem[s1,<offset>]; res_reg=mem[x10]; ...\n   pop s1, s0; ret ra. */\nvoid *_MIR_get_ff_call (MIR_context_t ctx, size_t nres, MIR_type_t *res_types, size_t nargs,\n                        _MIR_arg_desc_t *arg_descs, size_t arg_vars_num) {\n#if __riscv_compressed\n  static const uint16_t prolog[] = {\n    0x1141, /* c.addi sp,-16 */\n    0xe406, /* c.sdsp ra,8(sp) */\n    0xe026, /* c.sdsp s1,0(sp) */\n    0x82aa, /* c.mv t0,a0 */\n    0x84ae, /* c.mv s1,a1 */\n  };\n#else\n  static const uint32_t prolog[] = {\n    0xff010113, /* addi sp,sp,-16 */\n    0x00113423, /* sd ra,8(sp) */\n    0x00913023, /* sd s1,0(sp) */\n    0x00050293, /* addi t0,a0,0 */\n    0x00058493, /* addi s1,a1,0 */\n  };\n#endif\n  static const uint32_t ld_word_pat = 0x0003b003;      /* ld zero,0(t2) */\n  static const uint32_t ld_word_temp_pat = 0x0003b303; /* ld t1,0(t2) */\n  static const uint32_t ld_temp_pat = 0x0004b303;      /* ld t1,0(s1) */\n  static const uint32_t st_temp_pat = 0x00613023;      /* sd t1,0(sp) */\n  static const uint32_t st_arg_pat = 0x0004b023;       /* sd x0,0(s1) */\n  static const uint32_t fsw_arg_pat = 0x0004a027;      /* fsw f0,0(s1) */\n  static const uint32_t fsd_arg_pat = 0x0004b027;      /* fsd f0,0(s1) */\n  static const uint32_t flw_temp_pat = 0x0004a087;     /* flw ft1,0(s1) */\n  static const uint32_t fld_temp_pat = 0x0004b087;     /* fld ft1,0(s1) */\n  static const uint32_t fsw_temp_pat = 0x00112027;     /* fsw ft1,0(sp) */\n  static const uint32_t fsd_temp_pat = 0x00113027;     /* fsd ft1,0(sp) */\n  static const uint32_t fmvs_arg_pat = 0xe0000053;     /* fmv.x.w x0,f0 */\n  static const uint32_t fmvd_arg_pat = 0xe2000053;     /* fmv.x.d x0,f0 */\n  static const uint32_t fmvs_temp_pat = 0xe0008353;    /* fmv.x.w t1,ft1 */\n  static const uint32_t fmvd_temp_pat = 0xe2008353;    /* fmv.x.d t1,ft1 */\n  static const uint32_t mv_t1_pat = 0x00030013;        /* addi zero,t1,0 */\n  static const uint32_t long_sp_add_pat[] = {\n    0x00000337, /* lui t1,0 */\n    0x00030313, /* addi t1,t1,0 */\n    0x00610133, /* add sp,sp,t1 */\n  };\n  static const uint32_t call = 0x000280e7; /* jalr ra,0(t0) */\n#if __riscv_compressed\n  static const uint16_t epilog[] = {\n    0x60a2, /* ld ra,8(sp) */\n    0x6482, /* ld s1,0(sp) */\n    0x0141, /* addi sp,sp,16 */\n    0x8082, /* c.jr ra */\n  };\n#else\n  static const uint32_t epilog[] = {\n    0x00813083, /* ld ra,8(sp) */\n    0x00013483, /* ld s1,0(sp) */\n    0x01010113, /* addi sp,sp,16 */\n    0x00008067, /* jalr zero,0(ra) */\n  };\n#endif\n  size_t len, offset;\n  MIR_type_t type;\n  uint32_t n_xregs = 0, n_fregs = 0, sp_offset = 0, blk_offset = 0, pat;\n  uint32_t parts;\n  VARR (uint8_t) * code;\n  void *res;\n\n  VARR_CREATE (uint8_t, code, ctx->alloc, 128);\n  mir_assert (__SIZEOF_LONG_DOUBLE__ == 16);\n  for (size_t i = 0; i < nargs; i++) { /* calculate offset for blk params */\n    type = arg_descs[i].type;\n    if ((MIR_T_I8 <= type && type <= MIR_T_U64) || type == MIR_T_P || type == MIR_T_LD\n        || MIR_all_blk_type_p (type)) {\n      if ((parts = (arg_descs[i].size + 7) / 8) <= 2 && MIR_blk_type_p (type)) {\n        if (type == MIR_T_BLK + 1) n_xregs = (n_xregs + 1) / 2 * 2; /* Make even */\n        if (n_xregs + parts > 8) blk_offset += (parts - (n_xregs + parts == 9 ? 1 : 0)) * 8;\n        n_xregs += parts;\n      } else { /* blocks here are passed by address */\n        if (type == MIR_T_LD) n_xregs = (n_xregs + 1) / 2 * 2; /* Make even */\n        if (n_xregs >= 8) blk_offset += 8 + (type == MIR_T_LD ? 8 : 0);\n        n_xregs++;\n        if (type == MIR_T_LD) n_xregs++;\n      }\n    } else if (type == MIR_T_F || type == MIR_T_D) {\n      if (i >= arg_vars_num) { /* vararg */\n        if (n_xregs >= 8) blk_offset += 8;\n        n_xregs++;\n      } else {\n        if (n_fregs >= 8) blk_offset += 8;\n        n_fregs++;\n      }\n    } else {\n      MIR_get_error_func (ctx) (MIR_call_op_error, \"wrong type of arg value\");\n    }\n  }\n  blk_offset = (blk_offset + 15) / 16 * 16; /* align stack */\n  push_insns (code, prolog, sizeof (prolog));\n  len = VARR_LENGTH (uint8_t, code);\n  push_insns (code, long_sp_add_pat, sizeof (long_sp_add_pat));\n  n_xregs = n_fregs = 0;\n  for (size_t i = 0; i < nargs; i++) { /* args */\n    type = arg_descs[i].type;\n    offset = (i + nres) * sizeof (MIR_val_t);\n    if (MIR_blk_type_p (type)) {\n      parts = (arg_descs[i].size + 7) / 8;\n      if (parts <= 2) {\n        pat = ld_arg_pat | get_i_format_imm (offset) | get_i_format_rd (7); /* ld t2,offset(s1) */\n        push_insns (code, &pat, sizeof (pat));\n        if (type == MIR_T_BLK + 1) n_xregs = (n_xregs + 1) / 2 * 2; /* Make even */\n        for (uint32_t n = 0; n < parts; n++) {\n          if (n_xregs < 8) {\n            pat = ld_word_pat | get_i_format_imm (n * 8) | get_i_format_rd (n_xregs + a0_num);\n            push_insns (code, &pat, sizeof (pat));\n            n_xregs++;\n          } else {\n            pat = ld_word_temp_pat | get_i_format_imm (n * 8);\n            push_insns (code, &pat, sizeof (pat));\n            pat = st_temp_pat | get_s_format_imm (sp_offset);\n            push_insns (code, &pat, sizeof (pat));\n            sp_offset += 8;\n            n_xregs++;\n          }\n          offset += sizeof (MIR_val_t);\n        }\n      } else {\n        gen_blk_mov (code, blk_offset, (i + nres) * sizeof (MIR_val_t), parts);\n        blk_offset += parts * 8;\n        if (n_xregs < 8) {\n          pat = mv_t1_pat | get_i_format_rd (n_xregs + a0_num);\n        } else {\n          pat = st_temp_pat | get_s_format_imm (sp_offset);\n          sp_offset += 8;\n        }\n        push_insns (code, &pat, sizeof (pat));\n        n_xregs++;\n      }\n    } else if ((MIR_T_I8 <= type && type <= MIR_T_U64) || type == MIR_T_P || type == MIR_T_LD\n               || type == MIR_T_RBLK) {\n      if (type == MIR_T_LD) n_xregs = (n_xregs + 1) / 2 * 2; /* Make even */\n      if (n_xregs < 8) {\n        pat = ld_arg_pat | get_i_format_imm (offset) | get_i_format_rd (n_xregs + a0_num);\n        n_xregs++;\n        if (type == MIR_T_LD) {\n          push_insns (code, &pat, sizeof (pat));\n          pat = ld_arg_pat | get_i_format_imm (offset + 8) | get_i_format_rd (n_xregs + a0_num);\n          n_xregs++;\n        }\n      } else {\n        pat = ld_temp_pat | get_i_format_imm (offset);\n        push_insns (code, &pat, sizeof (pat));\n        pat = st_temp_pat | get_s_format_imm (sp_offset);\n        sp_offset += 8;\n        if (type == MIR_T_LD) {\n          push_insns (code, &pat, sizeof (pat));\n          pat = ld_temp_pat | get_i_format_imm (offset + 8);\n          push_insns (code, &pat, sizeof (pat));\n          pat = st_temp_pat | get_s_format_imm (sp_offset);\n          sp_offset += 8;\n        }\n      }\n      push_insns (code, &pat, sizeof (pat));\n    } else if (type == MIR_T_F || type == MIR_T_D) {\n      if (i >= arg_vars_num) { /* vararg */\n        pat = type == MIR_T_F ? flw_arg_pat : fld_arg_pat;\n        pat |= get_i_format_imm (offset) | get_i_format_rd (1); /* fl(w|d) ft1, <offset>(s1) */\n        push_insns (code, &pat, sizeof (pat));\n        if (n_xregs < 8) {\n          pat = type == MIR_T_F ? fmvs_arg_pat : fmvd_arg_pat;\n          pat |= get_opfp_format_rs1 (1)\n                 | get_opfp_format_rd (n_xregs + a0_num); /* fmv.x.(w|d) arg, ft1 */\n          n_xregs++;\n        } else {\n          pat = type == MIR_T_F ? fmvs_temp_pat : fmvd_temp_pat;\n          pat |= get_opfp_format_rd (6); /* fmv.x.(w|d) t1,ft1 */\n          push_insns (code, &pat, sizeof (pat));\n          pat = st_temp_pat | get_s_format_imm (sp_offset); /* sd t1,<sp_offset>(sp) */\n          sp_offset += 8;\n        }\n      } else if (n_fregs < 8) {\n        pat = type == MIR_T_F ? flw_arg_pat : fld_arg_pat;\n        pat |= get_i_format_imm (offset) | get_i_format_rd (n_fregs + fa0_num);\n        n_fregs++;\n      } else {\n        pat = type == MIR_T_F ? flw_temp_pat : fld_temp_pat;\n        pat |= get_i_format_imm (offset);\n        push_insns (code, &pat, sizeof (pat));\n        pat = type == MIR_T_F ? fsw_temp_pat : fsd_temp_pat;\n        pat |= get_s_format_imm (sp_offset);\n        sp_offset += 8;\n      }\n      push_insns (code, &pat, sizeof (pat));\n    } else {\n      MIR_get_error_func (ctx) (MIR_call_op_error, \"wrong type of arg value\");\n    }\n  }\n  sp_offset = (sp_offset + 15) / 16 * 16;\n  blk_offset = (blk_offset + 15) / 16 * 16;\n  if (blk_offset != 0) sp_offset = blk_offset;\n  int imm = -(int) sp_offset, imm12 = (imm << 20) >> 20, imm20 = ((imm - imm12) >> 12) << 12;\n  ((uint32_t *) (&VARR_ADDR (uint8_t, code)[len]))[0] |= get_u_format_imm (imm20); /* lui */\n  ((uint32_t *) (&VARR_ADDR (uint8_t, code)[len]))[1] |= get_i_format_imm (imm12); /* addi */\n  push_insns (code, &call, sizeof (call));\n  if (sp_offset < (1 << 11)) {\n    pat = add_sp_pat | get_i_format_imm (sp_offset);\n    push_insns (code, &pat, sizeof (pat));\n  } else {\n    len = VARR_LENGTH (uint8_t, code);\n    push_insns (code, long_sp_add_pat, sizeof (long_sp_add_pat));\n    ((uint32_t *) (&VARR_ADDR (uint8_t, code)[len]))[0] |= get_u_format_imm ((int) sp_offset >> 12);\n    ((uint32_t *) (&VARR_ADDR (uint8_t, code)[len]))[1]\n      |= get_i_format_imm ((int) sp_offset & 0xfff);\n  }\n  n_xregs = n_fregs = 0;\n  for (size_t i = 0; i < nres; i++) { /* results */\n    offset = i * sizeof (long double);\n    if (((MIR_T_I8 <= res_types[i] && res_types[i] <= MIR_T_U64) || res_types[i] == MIR_T_P)\n        && n_xregs < 2) {\n      pat = st_arg_pat | get_s_format_imm (offset) | get_s_format_rs2 (n_xregs + a0_num);\n      n_xregs++;\n      push_insns (code, &pat, sizeof (pat));\n    } else if (res_types[i] == MIR_T_LD && n_fregs + 1 < 2) {\n      pat = st_arg_pat | get_s_format_imm (offset) | get_s_format_rs2 (n_xregs + a0_num);\n      n_xregs++;\n      push_insns (code, &pat, sizeof (pat));\n      pat = st_arg_pat | get_s_format_imm (offset + 8) | get_s_format_rs2 (n_xregs + a0_num);\n      n_xregs++;\n      push_insns (code, &pat, sizeof (pat));\n    } else if ((res_types[i] == MIR_T_F || res_types[i] == MIR_T_D) && n_fregs < 2) {\n      pat = res_types[i] == MIR_T_F ? fsw_arg_pat : fsd_arg_pat;\n      pat |= get_s_format_imm (offset) | get_s_format_rs2 (n_fregs + fa0_num);\n      n_fregs++;\n      push_insns (code, &pat, sizeof (pat));\n    } else {\n      MIR_get_error_func (ctx) (MIR_ret_error,\n                                \"riscv64 can not handle this combination of return values\");\n    }\n  }\n  push_insns (code, epilog, sizeof (epilog));\n  res = _MIR_publish_code (ctx, VARR_ADDR (uint8_t, code), VARR_LENGTH (uint8_t, code));\n#if 0\n  if (getenv (\"MIR_code_dump\") != NULL)\n    _MIR_dump_code (\"ffi:\", VARR_ADDR (uint8_t, code), VARR_LENGTH (uint8_t, code));\n#endif\n  VARR_DESTROY (uint8_t, code);\n  return res;\n}\n\n/* Transform C call to call of void handler (MIR_context_t ctx, MIR_item_t func_item,\n                                             va_list va, MIR_val_t *results) */\nvoid *_MIR_get_interp_shim (MIR_context_t ctx, MIR_item_t func_item, void *handler) {\n  static const uint32_t t0_sp = 0x00010293;      /* addi t0,sp,0 */\n  static const uint32_t sub_arg_sp = 0x00010113; /* addi sp,sp,0 */\n  static const uint32_t set_a2_pat = 0x00010613; /* addi a2,sp,0 */\n#if __riscv_compressed\n  static const uint16_t prepare_pat[] = {\n    0xe026, /* c.sdsp\ts1,0(sp) */\n    0xe406, /* c.sdsp\tra,8(sp) */\n    0x0804, /* c.addi4spn s1,sp,16 */\n    0x86a6, /* c.mv a3,s1 */\n  };\n  static const uint16_t ra_s1_restore[] = {\n    0x6482, /* c.ldsp s1,0(sp) */\n    0x60a2, /* c.ldsp ra,8(sp) */\n  };\n  static const uint16_t ret = 0x8082; /* c.jr ra */\n#else\n  static const uint32_t prepare_pat[] = {\n    0x00913023, /* sd\ts1,0(sp) */\n    0x00113423, /* sd\tra,8(sp) */\n    0x01010493, /* addi\ts1,sp,16 */\n    0x00048693, /* addi a3,s1,0 */\n  };\n  static const uint32_t ra_s1_restore[] = {\n    0x00013483, /* ld s1,0(sp) */\n    0x00813083, /* ld ra,8(sp) */\n  };\n  static const uint32_t ret = 0x00008067; /* jalr zero,0(ra) */\n#endif\n  static const uint32_t sd_arg_pat = 0x00013023;      /* sd zero,0(sp) */\n  static const uint32_t ld_arg_temp_pat = 0x0002b303; /* ld t1,0(t0) */\n  static const uint32_t st_arg_temp_pat = 0x00613023; /* sd t1,0(sp) */\n  static const uint32_t fsd_arg_pat = 0x00013027;     /* fsd f0,0(sp) */\n  static const uint32_t fsw_arg_pat = 0x00012027;     /* fsw f0,0(sp) */\n  static const uint32_t call_pat[] = {\n    0x00000297, /* auipc t0,0x0 */\n    0x0002b503, /* ld a0,0(t0) */\n    0x0002b583, /* ld a1,0(t0) */\n    0x0002b283, /* ld t0,0(t0) */\n    0x000280e7, /* jalr\tra,0(t0) */\n  };\n  size_t start, args_start, offset, sp_offset, arg_offset, align_pad;\n  MIR_func_t func = func_item->u.func;\n  uint32_t nargs = func->nargs, nres = func->nres;\n  MIR_var_t *args = VARR_ADDR (MIR_var_t, func->vars);\n  MIR_type_t type, *results = func->res_types;\n  VARR (uint8_t) * code, *code2;\n  void *res;\n  uint32_t pat, n_xregs, n_fregs, parts;\n\n  assert (__SIZEOF_LONG_DOUBLE__ == 16);\n  VARR_CREATE (uint8_t, code, ctx->alloc, 128);\n  VARR_CREATE (uint8_t, code2, ctx->alloc, 128);\n  push_insns (code, &t0_sp, sizeof (t0_sp));           /* t0 = sp */\n  push_insns (code, &sub_arg_sp, sizeof (sub_arg_sp)); /* sp -= <sp_offset> */\n  sp_offset = 0;\n  n_xregs = n_fregs = 0;\n  for (size_t i = 0; i < nargs; i++) { /* args */\n    type = args[i].type;\n    if (MIR_blk_type_p (type) && (parts = (args[i].size + 7) / 8) <= 2) {\n      if (type == MIR_T_BLK + 1 && n_xregs % 2 != 0) { /* Make even */\n        sp_offset += 8;\n        n_xregs = (n_xregs + 1) / 2 * 2;\n      }\n      for (uint32_t n = 0; n < parts; n++) {\n        if (n_xregs < 8) {\n          n_xregs++;\n        }\n        sp_offset += 8;\n      }\n    } else if ((MIR_T_I8 <= type && type <= MIR_T_U64) || type == MIR_T_P || type == MIR_T_F\n               || type == MIR_T_D || type == MIR_T_LD || type == MIR_T_RBLK\n               || MIR_blk_type_p (type)) {\n      if (type == MIR_T_LD && n_xregs % 2 != 0) { /* Make even */\n        sp_offset += 8;\n        n_xregs = (n_xregs + 1) / 2 * 2;\n      }\n      if (type != MIR_T_F && type != MIR_T_D && n_xregs < 8) {\n        n_xregs++;\n        sp_offset += 8;\n        if (type == MIR_T_LD) {\n          sp_offset += 8;\n          n_xregs++;\n        }\n      } else if ((type == MIR_T_F || type == MIR_T_D) && n_fregs < 8) {\n        sp_offset += 8;\n        n_fregs++;\n      } else {\n        sp_offset += 8;\n        if (type == MIR_T_LD) sp_offset += 8;\n      }\n    } else {\n      MIR_get_error_func (ctx) (MIR_call_op_error, \"wrong type of arg value\");\n    }\n  }\n  if (n_xregs < 8) sp_offset += 8 * (8 - n_xregs); /* saving rest of arg regs */\n  align_pad = sp_offset % 16 != 0 ? 8 : 0;\n  sp_offset += align_pad; /* align */\n  ((uint32_t *) VARR_ADDR (uint8_t, code))[1] |= get_i_format_imm (-sp_offset);\n  arg_offset = 0;\n  sp_offset = align_pad;\n  n_xregs = n_fregs = 0;\n  for (size_t i = 0; i < nargs; i++) { /* args */\n    type = args[i].type;\n    if (MIR_blk_type_p (type) && (parts = (args[i].size + 7) / 8) <= 2) {\n      if (type == MIR_T_BLK + 1 && n_xregs % 2 != 0) { /* Make even */\n        sp_offset += 8;\n        n_xregs = (n_xregs + 1) / 2 * 2;\n      }\n      for (uint32_t n = 0; n < parts; n++) {\n        if (n_xregs < 8) {\n          pat = sd_arg_pat | get_s_format_imm (sp_offset) | get_s_format_rs2 (n_xregs + a0_num);\n          push_insns (code2, &pat, sizeof (pat));\n          n_xregs++;\n        } else {\n          pat = ld_arg_temp_pat | get_i_format_imm (arg_offset);\n          push_insns (code, &pat, sizeof (pat));\n          arg_offset += 8;\n          pat = st_arg_temp_pat | get_s_format_imm (sp_offset);\n          push_insns (code, &pat, sizeof (pat));\n        }\n        sp_offset += 8;\n      }\n    } else if ((MIR_T_I8 <= type && type <= MIR_T_U64) || type == MIR_T_P || type == MIR_T_F\n               || type == MIR_T_D || type == MIR_T_LD || type == MIR_T_RBLK\n               || MIR_blk_type_p (type)) {\n      if (type == MIR_T_LD && n_xregs % 2 != 0) { /* Make even */\n        sp_offset += 8;\n        n_xregs = (n_xregs + 1) / 2 * 2;\n      }\n      if (type != MIR_T_F && type != MIR_T_D && n_xregs < 8) {\n        pat = sd_arg_pat | get_s_format_imm (sp_offset) | get_s_format_rs2 (n_xregs + a0_num);\n        push_insns (code2, &pat, sizeof (pat));\n        n_xregs++;\n        sp_offset += 8;\n        if (type == MIR_T_LD) {\n          pat = sd_arg_pat | get_s_format_imm (sp_offset) | get_s_format_rs2 (n_xregs + a0_num);\n          push_insns (code2, &pat, sizeof (pat));\n          sp_offset += 8;\n          n_xregs++;\n        }\n      } else if ((type == MIR_T_F || type == MIR_T_D) && n_fregs < 8) {\n        pat = type == MIR_T_F ? fsw_arg_pat : fsd_arg_pat;\n        pat |= get_s_format_imm (sp_offset) | get_s_format_rs2 (n_fregs + fa0_num);\n        push_insns (code2, &pat, sizeof (pat));\n        sp_offset += 8;\n        n_fregs++;\n      } else {\n        pat = ld_arg_temp_pat | get_i_format_imm (arg_offset);\n        push_insns (code, &pat, sizeof (pat));\n        arg_offset += 8;\n        pat = st_arg_temp_pat | get_s_format_imm (sp_offset);\n        push_insns (code, &pat, sizeof (pat));\n        sp_offset += 8;\n        if (type == MIR_T_LD) {\n          pat = ld_arg_temp_pat | get_i_format_imm (arg_offset);\n          push_insns (code, &pat, sizeof (pat));\n          arg_offset += 8;\n          pat = st_arg_temp_pat | get_s_format_imm (sp_offset);\n          push_insns (code, &pat, sizeof (pat));\n          sp_offset += 8;\n        }\n      }\n    }\n  }\n  while (n_xregs < 8) { /* save rest of arg registers (a<n>..a7) */\n    pat = sd_arg_pat | get_s_format_imm (sp_offset) | get_s_format_rs2 (n_xregs + a0_num);\n    push_insns (code2, &pat, sizeof (pat));\n    sp_offset += 8;\n    n_xregs++;\n  }\n  for (size_t i = 0; i < VARR_LENGTH (uint8_t, code2) / 4; i++) {\n    push_insns (code, (uint32_t *) VARR_ADDR (uint8_t, code2) + i, sizeof (uint32_t));\n  }\n  assert (sp_offset % 16 == 0);\n  pat = set_a2_pat | get_i_format_imm (align_pad);\n  push_insns (code, &pat, sizeof (pat)); /* a2 = sp + align_pad */\n  start = VARR_LENGTH (uint8_t, code);\n  push_insns (code, &add_sp_pat, sizeof (add_sp_pat)); /* sp -= <nres*16+16(ra&s1)> */\n  ((uint32_t *) (VARR_ADDR (uint8_t, code) + start))[0] |= get_i_format_imm (-nres * 16 - 16);\n  push_insns (code, prepare_pat, sizeof (prepare_pat));\n  args_start = VARR_LENGTH (uint8_t, code);\n  push_insns (code, call_pat, sizeof (call_pat));\n  /* move results: */\n  n_xregs = n_fregs = offset = 0;\n  for (uint32_t i = 0; i < nres; i++) {\n    if ((results[i] == MIR_T_F || results[i] == MIR_T_D) && n_fregs < 2) {\n      pat = results[i] == MIR_T_F ? flw_arg_pat : fld_arg_pat;\n      pat |= get_i_format_imm (offset) | get_i_format_rd (n_fregs + fa0_num);\n      n_fregs++;\n    } else if (results[i] == MIR_T_LD && n_xregs + 1 < 2) {\n      pat = ld_arg_pat | get_i_format_imm (offset) | get_i_format_rd (n_xregs + a0_num);\n      push_insns (code, &pat, sizeof (pat));\n      pat = ld_arg_pat | get_i_format_imm (offset + 8) | get_i_format_rd (n_xregs + 1 + a0_num);\n      n_xregs += 2;\n    } else if (n_xregs < 2) {\n      pat = ld_arg_pat | get_i_format_imm (offset) | get_i_format_rd (n_xregs + a0_num);\n      n_xregs++;\n    } else {\n      MIR_get_error_func (ctx) (MIR_ret_error,\n                                \"riscv64 can not handle this combination of return values\");\n    }\n    push_insns (code, &pat, sizeof (pat));\n    offset += sizeof (MIR_val_t);\n  }\n  push_insns (code, ra_s1_restore, sizeof (ra_s1_restore)); /* ld ra,8(sp); ld s1,0(sp) */\n  start = VARR_LENGTH (uint8_t, code);\n  push_insns (code, &add_sp_pat, sizeof (add_sp_pat)); /* sp += <nres * 16 + 16 + sp_offset> */\n  ((uint32_t *) (VARR_ADDR (uint8_t, code) + start))[0]\n    |= get_i_format_imm ((nres + 1) * 16 + sp_offset);\n  push_insns (code, &ret, sizeof (ret));                                     /* jalr ra */\n  while (VARR_LENGTH (uint8_t, code) % 8 != 0) VARR_PUSH (uint8_t, code, 0); /* align */\n  offset = VARR_LENGTH (uint8_t, code) - args_start;\n  push_insns (code, &ctx, sizeof (ctx));\n  push_insns (code, &func_item, sizeof (func_item));\n  push_insns (code, &handler, sizeof (handler));\n  ((uint32_t *) (VARR_ADDR (uint8_t, code) + args_start))[1] |= get_i_format_imm (offset);\n  ((uint32_t *) (VARR_ADDR (uint8_t, code) + args_start))[2] |= get_i_format_imm (offset + 8);\n  ((uint32_t *) (VARR_ADDR (uint8_t, code) + args_start))[3] |= get_i_format_imm (offset + 16);\n  res = _MIR_publish_code (ctx, VARR_ADDR (uint8_t, code), VARR_LENGTH (uint8_t, code));\n#if 0\n  if (getenv (\"MIR_code_dump\") != NULL)\n    _MIR_dump_code (func->name, VARR_ADDR (uint8_t, code), VARR_LENGTH (uint8_t, code));\n#endif\n  VARR_DESTROY (uint8_t, code);\n  VARR_DESTROY (uint8_t, code2);\n  return res;\n}\n\n/* save a0-a7,fa0-fa7: */\n#if __riscv_compressed\nstatic const uint16_t save_insns[] = {\n  0xe42a, /* sd a0,8(sp) */\n  0xe82e, /* sd a1,16(sp) */\n  0xec32, /* sd a2,24(sp) */\n  0xf036, /* sd a3,32(sp) */\n  0xf43a, /* sd a4,40(sp) */\n  0xf83e, /* sd a5,48(sp) */\n  0xfc42, /* sd a6,56(sp) */\n  0xe0c6, /* sd a7,64(sp) */\n  0xa4aa, /* fsd fa0,72(sp) */\n  0xa8ae, /* fsd fa1,80(sp) */\n  0xacb2, /* fsd fa2,88(sp) */\n  0xb0b6, /* fsd fa3,96(sp) */\n  0xb4ba, /* fsd fa4,104(sp) */\n  0xb8be, /* fsd fa5,112(sp) */\n  0xbcc2, /* fsd fa6,120(sp) */\n  0xa146, /* fsd fa7,128(sp) */\n};\n#else\nstatic const uint32_t save_insns[] = {\n  0x00a13423, /* sd a0,8(sp) */\n  0x00b13823, /* sd a1,16(sp) */\n  0x00c13c23, /* sd a2,24(sp) */\n  0x02d13023, /* sd a3,32(sp) */\n  0x02e13423, /* sd a4,40(sp) */\n  0x02f13823, /* sd a5,48(sp) */\n  0x03013c23, /* sd a6,56(sp) */\n  0x05113023, /* sd a7,64(sp) */\n  0x04a13427, /* fsd fa0,72(sp) */\n  0x04b13827, /* fsd fa1,80(sp) */\n  0x04c13c27, /* fsd fa2,88(sp) */\n  0x06d13027, /* fsd fa3,96(sp) */\n  0x06e13427, /* fsd fa4,104(sp) */\n  0x06f13827, /* fsd fa5,112(sp) */\n  0x07013c27, /* fsd fa6,120(sp) */\n  0x09113027, /* fsd fa7,128(sp) */\n};\n#endif\n/* restore a0-a7,fa0-fa7: */\n#if __riscv_compressed\nstatic const uint16_t restore_insns[] = {\n  0x6522, /* ld a0,8(sp) */\n  0x65c2, /* ld a1,16(sp) */\n  0x6662, /* ld a2,24(sp) */\n  0x7682, /* ld a3,32(sp) */\n  0x7722, /* ld a4,40(sp) */\n  0x77c2, /* ld a5,48(sp) */\n  0x7862, /* ld a6,56(sp) */\n  0x6886, /* ld a7,64(sp) */\n  0x2526, /* fld fa0,72(sp) */\n  0x25c6, /* fld fa1,80(sp) */\n  0x2666, /* fld fa2,88(sp) */\n  0x3686, /* fld fa3,96(sp) */\n  0x3726, /* fld fa4,104(sp) */\n  0x37c6, /* fld fa5,112(sp) */\n  0x3866, /* fld fa6,120(sp) */\n  0x288a, /* fld fa7,128(sp) */\n};\n#else\nstatic const uint32_t restore_insns[] = {\n  0x00813503, /* ld a0,8(sp) */\n  0x01013583, /* ld a1,16(sp) */\n  0x01813603, /* ld a2,24(sp) */\n  0x02013683, /* ld a3,32(sp) */\n  0x02813703, /* ld a4,40(sp) */\n  0x03013783, /* ld a5,48(sp) */\n  0x03813803, /* ld a6,56(sp) */\n  0x04013883, /* ld a7,64(sp) */\n  0x04813507, /* fld fa0,72(sp) */\n  0x05013587, /* fld fa1,80(sp) */\n  0x05813607, /* fld fa2,88(sp) */\n  0x06013687, /* fld fa3,96(sp) */\n  0x06813707, /* fld fa4,104(sp) */\n  0x07013787, /* fld fa5,112(sp) */\n  0x07813807, /* fld fa6,120(sp) */\n  0x08013887, /* fld fa7,128(sp) */\n};\n#endif\n\n/* set t0 to [ctx,called_func,hook_address]; jump wrapp_end */\nvoid *_MIR_get_wrapper (MIR_context_t ctx, MIR_item_t called_func, void *hook_address) {\n  static const uint32_t set_pat[] = {\n    0x00000297, /* auipc t0,0x0 */\n    0x00028293, /* addi\tt0,t0,0 */\n  };\n  uint8_t *base_addr, *res_code;\n  size_t offset = 0;\n  VARR (uint8_t) * code;\n  uint32_t insns[MAX_JUMP_CODE];\n  int len = 64; /* initial len */\n\n  VARR_CREATE (uint8_t, code, ctx->alloc, 128);\n  for (;;) { /* dealing with moving code to another page as the immediate call is pc relative */\n    base_addr = _MIR_get_new_code_addr (ctx, len);\n    if (base_addr == NULL) break;\n    VARR_TRUNC (uint8_t, code, 0);\n    push_insns (code, set_pat, sizeof (set_pat));\n    len = get_jump_code (insns, wrapper_end_addr,\n                         (uint8_t *) wrapper_end_addr - base_addr - sizeof (set_pat), T1_HARD_REG);\n    push_insns (code, insns, len);\n    offset = VARR_LENGTH (uint8_t, code);\n    assert (offset % 8 == 0);\n    push_insns (code, &ctx, sizeof (ctx));\n    push_insns (code, &called_func, sizeof (called_func));\n    push_insns (code, &hook_address, sizeof (hook_address));\n    ((uint32_t *) (VARR_ADDR (uint8_t, code)))[1] |= get_i_format_imm (offset);\n    len = VARR_LENGTH (uint8_t, code);\n    res_code = _MIR_publish_code_by_addr (ctx, base_addr, VARR_ADDR (uint8_t, code), len);\n    if (res_code != NULL) break;\n  }\n#if 0\n  if (getenv (\"MIR_code_dump\") != NULL)\n    _MIR_dump_code (\"wrapper:\", res_code, VARR_LENGTH (uint8_t, code));\n#endif\n  VARR_DESTROY (uint8_t, code);\n  return res_code;\n}\n\n/* Save regs ra,a0-a7,fa0-fa7;\n   a0 = call hook_address (ctx, called_func); t0=a0; restore regs; br t0 */\nvoid *_MIR_get_wrapper_end (MIR_context_t ctx) {\n  static const uint32_t jmp_insn = 0x00028067; /* jalr zero,0(t0) */\n#if __riscv_compressed\n  static const uint16_t sub_sp = 0x7175;     /* c.addi16sp sp,-144 */\n  static const uint16_t add_sp = 0x6149;     /* c.addi16sp sp,144 */\n  static const uint16_t save_ra = 0xe006;    /* sd ra,0(sp) */\n  static const uint16_t restore_ra = 0x6082; /* ld ra,0(sp) */\n#else\n  static const uint32_t sub_sp = 0xf7010113;     /* addi sp,sp,-144 */\n  static const uint32_t add_sp = 0x09010113;     /* addi sp,sp,144 */\n  static const uint32_t save_ra = 0x00113023;    /* sd ra,0(sp) */\n  static const uint32_t restore_ra = 0x00013083; /* ld ra,0(sp) */\n#endif\n  static const uint32_t call_pat[] = {\n    0x0002b503, /* ld a0,0(t0) */\n    0x0002b583, /* ld a1,0(t0) */\n    0x0002b603, /* ld a2,0(t0) */\n    0x000600e7, /* jalr ra,0(a2) */\n    0x00050293, /* mv t0,a0 */\n  };\n  size_t args_start;\n  uint8_t *res_code;\n  VARR (uint8_t) * code;\n\n  VARR_CREATE (uint8_t, code, ctx->alloc, 128);\n  push_insns (code, &sub_sp, sizeof (sub_sp));\n  push_insns (code, &save_ra, sizeof (save_ra));\n  push_insns (code, save_insns, sizeof (save_insns));\n  args_start = VARR_LENGTH (uint8_t, code);\n  push_insns (code, call_pat, sizeof (call_pat));\n  push_insns (code, &restore_ra, sizeof (restore_ra));\n  push_insns (code, restore_insns, sizeof (restore_insns));\n  push_insns (code, &add_sp, sizeof (add_sp));\n  push_insns (code, &jmp_insn, sizeof (jmp_insn));\n  ((uint32_t *) (VARR_ADDR (uint8_t, code) + args_start))[0] |= get_i_format_imm (0);\n  ((uint32_t *) (VARR_ADDR (uint8_t, code) + args_start))[1] |= get_i_format_imm (8);\n  ((uint32_t *) (VARR_ADDR (uint8_t, code) + args_start))[2] |= get_i_format_imm (16);\n  res_code = _MIR_publish_code (ctx, VARR_ADDR (uint8_t, code), VARR_LENGTH (uint8_t, code));\n#if 0\n  if (getenv (\"MIR_code_dump\") != NULL)\n    _MIR_dump_code (\"wrapper end:\", res_code, VARR_LENGTH (uint8_t, code));\n#endif\n  VARR_DESTROY (uint8_t, code);\n  return res_code;\n}\n\n/* save x5-x7,x10-x17,x28-x29,x31;f0-f7,f10-f17,f28-31: */\n#if __riscv_compressed\nstatic const uint16_t bb_save_insns[] = {\n  0xe816, /* sd t0,16(sp)*/\n  0xec1a, /* sd t1,24(sp)*/\n  0xf01e, /* sd t2,32(sp)*/\n  0xf42a, /* sd a0,40(sp)*/\n  0xf82e, /* sd a1,48(sp)*/\n  0xfc32, /* sd a2,56(sp)*/\n  0xe0b6, /* sd a3,64(sp)*/\n  0xe4ba, /* sd a4,72(sp)*/\n  0xe8be, /* sd a5,80(sp)*/\n  0xecc2, /* sd a6,88(sp)*/\n  0xf0c6, /* sd a7,96(sp)*/\n  0xf4f2, /* sd t3,104(sp)*/\n  0xf8f6, /* sd t4,112(sp)*/\n  0xfcfe, /* sd t6,120(sp)*/\n  0xa102, /* fsd ft0,128(sp)*/\n  0xa506, /* fsd ft1,136(sp)*/\n  0xa90a, /* fsd ft2,144(sp)*/\n  0xad0e, /* fsd ft3,152(sp)*/\n  0xb112, /* fsd ft4,160(sp)*/\n  0xb516, /* fsd ft5,168(sp)*/\n  0xb91a, /* fsd ft6,176(sp)*/\n  0xbd1e, /* fsd ft7,184(sp)*/\n  0xa1aa, /* fsd fa0,192(sp)*/\n  0xa5ae, /* fsd fa1,200(sp)*/\n  0xa9b2, /* fsd fa2,208(sp)*/\n  0xadb6, /* fsd fa3,216(sp)*/\n  0xb1ba, /* fsd fa4,224(sp)*/\n  0xb5be, /* fsd fa5,232(sp)*/\n  0xb9c2, /* fsd fa6,240(sp)*/\n  0xbdc6, /* fsd fa7,248(sp)*/\n  0xa272, /* fsd ft8,256(sp)*/\n  0xa676, /* fsd ft9,264(sp)*/\n  0xaa7a, /* fsd ft10,272(sp)*/\n  0xae7e, /* fsd ft11,280(sp)*/\n};\n#else\nstatic const uint32_t bb_save_insns[] = {\n  0x00513823, /* sd t0,16(sp)*/\n  0x00613c23, /* sd t1,24(sp)*/\n  0x02713023, /* sd t2,32(sp)*/\n  0x02a13423, /* sd a0,40(sp)*/\n  0x02b13823, /* sd a1,48(sp)*/\n  0x02c13c23, /* sd a2,56(sp)*/\n  0x04d13023, /* sd a3,64(sp)*/\n  0x04e13423, /* sd a4,72(sp)*/\n  0x04f13823, /* sd a5,80(sp)*/\n  0x05013c23, /* sd a6,88(sp)*/\n  0x07113023, /* sd a7,96(sp)*/\n  0x07c13423, /* sd t3,104(sp)*/\n  0x07d13823, /* sd t4,112(sp)*/\n  0x07f13c23, /* sd t6,120(sp)*/\n  0x08013027, /* fsd ft0,128(sp)*/\n  0x08113427, /* fsd ft1,136(sp)*/\n  0x08213827, /* fsd ft2,144(sp)*/\n  0x08313c27, /* fsd ft3,152(sp)*/\n  0x0a413027, /* fsd ft4,160(sp)*/\n  0x0a513427, /* fsd ft5,168(sp)*/\n  0x0a613827, /* fsd ft6,176(sp)*/\n  0x0a713c27, /* fsd ft7,184(sp)*/\n  0x0ca13027, /* fsd fa0,192(sp)*/\n  0x0cb13427, /* fsd fa1,200(sp)*/\n  0x0cc13827, /* fsd fa2,208(sp)*/\n  0x0cd13c27, /* fsd fa3,216(sp)*/\n  0x0ee13027, /* fsd fa4,224(sp)*/\n  0x0ef13427, /* fsd fa5,232(sp)*/\n  0x0f013827, /* fsd fa6,240(sp)*/\n  0x0f113c27, /* fsd fa7,248(sp)*/\n  0x11c13027, /* fsd ft8,256(sp)*/\n  0x11d13427, /* fsd ft9,264(sp)*/\n  0x11e13827, /* fsd ft10,272(sp)*/\n  0x11f13c27, /* fsd ft11,280(sp) */\n};\n#endif\n/* restore x5-x7,x10-x17,x28-x29,x31;f0-f7,f10-f17,f28-31: */\n#if __riscv_compressed\nstatic const uint16_t bb_restore_insns[] = {\n  0x62c2, /* ld\tt0,16(sp)*/\n  0x6362, /* ld\tt1,24(sp)*/\n  0x7382, /* ld\tt2,32(sp)*/\n  0x7522, /* ld\ta0,40(sp)*/\n  0x75c2, /* ld\ta1,48(sp)*/\n  0x7662, /* ld\ta2,56(sp)*/\n  0x6686, /* ld\ta3,64(sp)*/\n  0x6726, /* ld\ta4,72(sp)*/\n  0x67c6, /* ld\ta5,80(sp)*/\n  0x6866, /* ld\ta6,88(sp)*/\n  0x7886, /* ld\ta7,96(sp)*/\n  0x7e26, /* ld\tt3,104(sp)*/\n  0x7ec6, /* ld\tt4,112(sp)*/\n  0x7fe6, /* ld\tt6,120(sp)*/\n  0x200a, /* fld\tft0,128(sp)*/\n  0x20aa, /* fld\tft1,136(sp)*/\n  0x214a, /* fld\tft2,144(sp)*/\n  0x21ea, /* fld\tft3,152(sp)*/\n  0x320a, /* fld\tft4,160(sp)*/\n  0x32aa, /* fld\tft5,168(sp)*/\n  0x334a, /* fld\tft6,176(sp)*/\n  0x33ea, /* fld\tft7,184(sp)*/\n  0x250e, /* fld\tfa0,192(sp)*/\n  0x25ae, /* fld\tfa1,200(sp)*/\n  0x264e, /* fld\tfa2,208(sp)*/\n  0x26ee, /* fld\tfa3,216(sp)*/\n  0x370e, /* fld\tfa4,224(sp)*/\n  0x37ae, /* fld\tfa5,232(sp)*/\n  0x384e, /* fld\tfa6,240(sp)*/\n  0x38ee, /* fld\tfa7,248(sp)*/\n  0x2e12, /* fld\tft8,256(sp)*/\n  0x2eb2, /* fld\tft9,264(sp)*/\n  0x2f52, /* fld\tft10,272(sp)*/\n  0x2ff2, /* fld\tft11,280(sp)*/\n};\n#else\nstatic const uint32_t bb_restore_insns[] = {\n  0x01013283, /* ld t0,16(sp)*/\n  0x01813303, /* ld t1,24(sp)*/\n  0x02013383, /* ld t2,32(sp)*/\n  0x02813503, /* ld a0,40(sp)*/\n  0x03013583, /* ld a1,48(sp)*/\n  0x03813603, /* ld a2,56(sp)*/\n  0x04013683, /* ld a3,64(sp)*/\n  0x04813703, /* ld a4,72(sp)*/\n  0x05013783, /* ld a5,80(sp)*/\n  0x05813803, /* ld a6,88(sp)*/\n  0x06013883, /* ld a7,96(sp)*/\n  0x06813e03, /* ld t3,104(sp)*/\n  0x07013e83, /* ld t4,112(sp)*/\n  0x07813f83, /* ld t6,120(sp)*/\n  0x08013007, /* fld ft0,128(sp)*/\n  0x08813087, /* fld ft1,136(sp)*/\n  0x09013107, /* fld ft2,144(sp)*/\n  0x09813187, /* fld ft3,152(sp)*/\n  0x0a013207, /* fld ft4,160(sp)*/\n  0x0a813287, /* fld ft5,168(sp)*/\n  0x0b013307, /* fld ft6,176(sp)*/\n  0x0b813387, /* fld ft7,184(sp)*/\n  0x0c013507, /* fld fa0,192(sp)*/\n  0x0c813587, /* fld fa1,200(sp)*/\n  0x0d013607, /* fld fa2,208(sp)*/\n  0x0d813687, /* fld fa3,216(sp)*/\n  0x0e013707, /* fld fa4,224(sp)*/\n  0x0e813787, /* fld fa5,232(sp)*/\n  0x0f013807, /* fld fa6,240(sp)*/\n  0x0f813887, /* fld fa7,248(sp)*/\n  0x10013e07, /* fld ft8,256(sp)*/\n  0x10813e87, /* fld ft9,264(sp)*/\n  0x11013f07, /* fld ft10,272(sp)*/\n  0x11813f87, /* fld ft11,280(sp)*/\n};\n#endif\n\n/* t5(x30)=<bb_version>; (b|br) handler  ??? mutex free */\nvoid *_MIR_get_bb_thunk (MIR_context_t ctx, void *bb_version, void *handler) {\n  /* maximal size thunk -- see _MIR_redirect_thunk */\n  uint32_t pat[2] = {\n    0x00000f17, /* auipc t5,0 */\n    0x020f3f03, /* ld t5,32(t5) */\n  };\n  uint32_t nop = TARGET_NOP;\n  void *res;\n  VARR (uint8_t) * code;\n\n  VARR_CREATE (uint8_t, code, ctx->alloc, 64);\n  assert (MAX_JUMP_CODE == 6);\n  push_insns (code, pat, sizeof (pat));\n  for (int i = 0; i < MAX_JUMP_CODE + 2; i++)\n    push_insns (code, &nop, 4); /* for max branch and 2 for bb_version */\n  *(void **) (VARR_ADDR (uint8_t, code) + 32) = bb_version;\n  res = _MIR_publish_code (ctx, VARR_ADDR (uint8_t, code), VARR_LENGTH (uint8_t, code));\n  redirect_thunk (ctx, (uint8_t *) res + 8, handler, T6_HARD_REG);\n#if 0\n  if (getenv (\"MIR_code_dump\") != NULL)\n    _MIR_dump_code (\"bb thunk:\", res, VARR_LENGTH (uint8_t, code));\n#endif\n  VARR_DESTROY (uint8_t, code);\n  return res;\n}\n\n/* change to jump */\nvoid _MIR_replace_bb_thunk (MIR_context_t ctx, void *thunk, void *to) {\n  redirect_thunk (ctx, thunk, to, TEMP_INT_HARD_REG1);\n}\n\n/* save all clobbered regs but x30(t5); x30 = call hook_address (data, x30); restore regs; br x30\n   x30 is a generator temp reg which is not used across bb borders. */\nvoid *_MIR_get_bb_wrapper (MIR_context_t ctx, void *data, void *hook_address) {\n  static const uint32_t jmp_insn = 0x000f0067; /* jalr zero,0(t5) */\n#if __riscv_compressed\n  static const uint16_t sub_sp = 0x712d;     /* c.addi16sp sp,-288 */\n  static const uint16_t add_sp = 0x6115;     /* c.addi16sp sp,288 */\n  static const uint16_t save_ra = 0xe006;    /* sd ra,0(sp) */\n  static const uint16_t restore_ra = 0x6082; /* ld ra,0(sp) */\n  static const uint16_t mva1t5 = 0x85fa;     /* mv a1,t5 */\n#else\n  static const uint32_t sub_sp = 0xee010113;     /* addi sp,sp,-288 */\n  static const uint32_t add_sp = 0x12010113;     /* addi sp,sp,288 */\n  static const uint32_t save_ra = 0x00113023;    /* sd ra,0(sp) */\n  static const uint32_t restore_ra = 0x00013083; /* ld ra,0(sp) */\n  static const uint32_t mva1t5 = 0x000f0593;     /* mv a1,t5 */\n#endif\n  static const uint32_t call_pat[] = {\n    0x00000297, /* auipc t0,0x0 */\n    0x0002b503, /* ld a0,0(t0) */\n    0x0002b603, /* ld a2,0(t0) */\n    0x000600e7, /* jalr ra,0(a2) */\n    0x00050f13, /* mv t5,a0 */\n  };\n  uint8_t *res_code;\n  size_t args_start, offset;\n  VARR (uint8_t) * code;\n\n  VARR_CREATE (uint8_t, code, ctx->alloc, 128);\n  VARR_TRUNC (uint8_t, code, 0);\n  push_insns (code, &sub_sp, sizeof (sub_sp));\n  push_insns (code, &save_ra, sizeof (save_ra));\n  push_insns (code, bb_save_insns, sizeof (bb_save_insns));\n  push_insns (code, &mva1t5, sizeof (mva1t5));\n  args_start = VARR_LENGTH (uint8_t, code);\n  push_insns (code, call_pat, sizeof (call_pat));\n  push_insns (code, &restore_ra, sizeof (restore_ra));\n  push_insns (code, bb_restore_insns, sizeof (bb_restore_insns));\n  push_insns (code, &add_sp, sizeof (add_sp));\n  push_insns (code, &jmp_insn, sizeof (jmp_insn));\n  while (VARR_LENGTH (uint8_t, code) % 8 != 0) VARR_PUSH (uint8_t, code, 0); /* align */\n  offset = VARR_LENGTH (uint8_t, code) - args_start;\n  push_insns (code, &data, sizeof (data));\n  push_insns (code, &hook_address, sizeof (hook_address));\n  ((uint32_t *) (VARR_ADDR (uint8_t, code) + args_start))[1] |= get_i_format_imm (offset);\n  ((uint32_t *) (VARR_ADDR (uint8_t, code) + args_start))[2] |= get_i_format_imm (offset + 8);\n  res_code = _MIR_publish_code (ctx, VARR_ADDR (uint8_t, code), VARR_LENGTH (uint8_t, code));\n#if 0\n  if (getenv (\"MIR_code_dump\") != NULL)\n    _MIR_dump_code (\"bb wrapper:\", res_code, VARR_LENGTH (uint8_t, code));\n#endif\n  VARR_DESTROY (uint8_t, code);\n  return res_code;\n}\n"
        },
        {
          "name": "mir-riscv64.h",
          "type": "blob",
          "size": 3.6103515625,
          "content": "/* This file is a part of MIR project.\n   Copyright (C) 2020-2024 Vladimir Makarov <vmakarov.gcc@gmail.com>.\n   A common include file for mir-riscv64.c and mir-gen-riscv64.c\n*/\n\n#include \"mir.h\"\n\n#define TARGET_NOP 0x00000013 /* nop:addi zero,zero,0 */\n\n#define HREG_EL(h) h##_HARD_REG\n#define REP_SEP ,\nenum {\n  REP8 (HREG_EL, R0, R1, R2, R3, R4, R5, R6, R7),\n  REP8 (HREG_EL, R8, R9, R10, R11, R12, R13, R14, R15),\n  REP8 (HREG_EL, R16, R17, R18, R19, R20, R21, R22, R23),\n  REP8 (HREG_EL, R24, R25, R26, R27, R28, R29, R30, R31),\n  /*Aliases: */ ZERO_HARD_REG = R0_HARD_REG,\n  REP7 (HREG_EL, RA, SP, GP, TP, T0, T1, T2),\n  REP8 (HREG_EL, FP, S1, A0, A1, A2, A3, A4, A5),\n  REP8 (HREG_EL, A6, A7, S2, S3, S4, S5, S6, S7),\n  REP8 (HREG_EL, S8, S9, S10, S11, T3, T4, T5, T6),\n\n  REP8 (HREG_EL, F0, F1, F2, F3, F4, F5, F6, F7),\n  REP8 (HREG_EL, F8, F9, F10, F11, F12, F13, F14, F15),\n  REP8 (HREG_EL, F16, F17, F18, F19, F20, F21, F22, F23),\n  REP8 (HREG_EL, F24, F25, F26, F27, F28, F29, F30, F31),\n  /* Aliases: */ FT0_HARD_REG = F0_HARD_REG,\n  REP7 (HREG_EL, FT1, FT2, FT3, FT4, FT5, FT6, FT7),\n  REP8 (HREG_EL, FS0, FS1, FA0, FA1, FA2, FA3, FA4, FA5),\n  REP8 (HREG_EL, FA6, FA7, FS2, FS3, FS4, FS5, FS6, FS7),\n  REP8 (HREG_EL, FS8, FS9, FS10, FS11, FT8, FT9, FT10, FT11),\n};\n#undef REP_SEP\n\nstatic const char *const target_hard_reg_names[] = {\n  \"r0\",  \"r1\",  \"r2\",  \"r3\",  \"r4\",  \"r5\",  \"r6\",  \"r7\",  \"r8\",  \"r9\",  \"r10\", \"r11\", \"r12\",\n  \"r13\", \"r14\", \"r15\", \"r16\", \"r17\", \"r18\", \"r19\", \"r20\", \"r21\", \"r22\", \"r23\", \"r24\", \"r25\",\n  \"r26\", \"r27\", \"r28\", \"r29\", \"r30\", \"r31\", \"f0\",  \"f1\",  \"f2\",  \"f3\",  \"f4\",  \"f5\",  \"f6\",\n  \"f7\",  \"f8\",  \"f9\",  \"f10\", \"f11\", \"f12\", \"f13\", \"f14\", \"f15\", \"f16\", \"f17\", \"f18\", \"f19\",\n  \"f20\", \"f21\", \"f22\", \"f23\", \"f24\", \"f25\", \"f26\", \"f27\", \"f28\", \"f29\", \"f30\", \"f31\",\n};\n\n#define MAX_HARD_REG F31_HARD_REG\n\n/* Hard regs not used in machinized code, preferably call used ones. */\nstatic const MIR_reg_t TEMP_INT_HARD_REG1 = T5_HARD_REG, TEMP_INT_HARD_REG2 = T6_HARD_REG;\nstatic const MIR_reg_t TEMP_FLOAT_HARD_REG1 = FT10_HARD_REG, TEMP_FLOAT_HARD_REG2 = FT11_HARD_REG;\nstatic const MIR_reg_t TEMP_DOUBLE_HARD_REG1 = FT10_HARD_REG, TEMP_DOUBLE_HARD_REG2 = FT11_HARD_REG;\n/* we use only builtins for long double ops: */\nstatic const MIR_reg_t TEMP_LDOUBLE_HARD_REG1 = MIR_NON_VAR;\nstatic const MIR_reg_t TEMP_LDOUBLE_HARD_REG2 = MIR_NON_VAR;\n\nstatic inline int target_hard_reg_type_ok_p (MIR_reg_t hard_reg, MIR_type_t type) {\n  assert (hard_reg <= MAX_HARD_REG);\n  if (type == MIR_T_LD) return FALSE; /* long double can be in hard regs only for arg passing */\n  return MIR_fp_type_p (type) ? hard_reg >= F0_HARD_REG : hard_reg < F0_HARD_REG;\n}\n\nstatic inline int target_fixed_hard_reg_p (MIR_reg_t hard_reg) {\n  assert (hard_reg <= MAX_HARD_REG);\n  return (hard_reg == ZERO_HARD_REG || hard_reg == FP_HARD_REG || hard_reg == SP_HARD_REG\n          || hard_reg == GP_HARD_REG || hard_reg == TP_HARD_REG  // ???\n          || hard_reg == TEMP_INT_HARD_REG1 || hard_reg == TEMP_INT_HARD_REG2\n          || hard_reg == TEMP_FLOAT_HARD_REG1 || hard_reg == TEMP_FLOAT_HARD_REG2\n          || hard_reg == TEMP_DOUBLE_HARD_REG1 || hard_reg == TEMP_DOUBLE_HARD_REG2\n          || hard_reg == TEMP_LDOUBLE_HARD_REG1 || hard_reg == TEMP_LDOUBLE_HARD_REG2);\n}\n\nstatic inline int target_locs_num (MIR_reg_t loc, MIR_type_t type) {\n  return loc > MAX_HARD_REG && type == MIR_T_LD ? 2 : 1;\n}\n\nstatic const uint32_t j_imm_mask = 0xfffff000;\nstatic inline uint32_t get_j_format_imm (int32_t offset) {\n  int d = offset >> 1; /* scale */\n  assert (-(1 << 19) <= d && d < (1 << 19));\n  return ((d & 0x80000) | ((d & 0x3ff) << 9) | (((d >> 10) & 0x1) << 8) | ((d >> 11) & 0xff)) << 12;\n}\n"
        },
        {
          "name": "mir-s390x.c",
          "type": "blob",
          "size": 26.7412109375,
          "content": "/* This file is a part of MIR project.\n   Copyright (C) 2018-2024 Vladimir Makarov <vmakarov.gcc@gmail.com>.\n*/\n\n#include \"mir-s390x.h\"\n\n/* Long doubles (-mlong-double=128) are always passed by its address (for args and results) */\n\n/* All BLK type values and RBLK args are always passed by address.  */\n\n#if __BYTE_ORDER__ == __ORDER_LITTLE_ENDIAN__\n#error \"s390x works only in BE mode\"\n#endif\n\n#define VA_LIST_IS_ARRAY_P 1 /* one element array of struct s390x_va_list */\n\n#define S390X_STACK_HEADER_SIZE 160\n\nstatic uint8_t *push_insns (VARR (uint8_t) * insn_varr, const uint8_t *pat, size_t pat_len) {\n  for (size_t i = 0; i < pat_len; i++) VARR_PUSH (uint8_t, insn_varr, pat[i]);\n  return VARR_ADDR (uint8_t, insn_varr) + VARR_LENGTH (uint8_t, insn_varr) - pat_len;\n}\n\nstatic void s390x_gen_mov (VARR (uint8_t) * insn_varr, unsigned to, unsigned from) {\n  uint32_t lgr = (0xb904 << 16) | (to << 4) | from; /* lgr to,from: */\n  assert (to < 16 && from < 16);\n  push_insns (insn_varr, (uint8_t *) &lgr, 4);\n}\n\nstatic void s390x_gen_mvi (VARR (uint8_t) * insn_varr, int val, unsigned base, int disp) {\n  uint64_t mvghi /* mvghi disp(base), val: */\n    = ((0xe548l << 32) | ((uint64_t) base << 28) | ((disp & 0xfff) << 16) | (val & 0xffff)) << 16;\n  assert (base < 16 && 0 <= disp && disp < (1 << 12) && -(1 << 15) < val && val < (1 << 15));\n  push_insns (insn_varr, (uint8_t *) &mvghi, 6);\n}\n\nstatic void s390x_gen_ld_st (VARR (uint8_t) * insn_varr, unsigned reg, unsigned base, int disp,\n                             MIR_type_t type, int ld_p) {\n  int single_p = type == MIR_T_F;\n  int double_p = type == MIR_T_D;\n  uint64_t dl = disp & 0xfff, dh = (disp >> 12) & 0xff;\n  uint64_t common = ((uint64_t) reg << 36) | ((uint64_t) base << 28) | (dl << 16) | (dh << 8);\n  uint64_t lgopcode = (type == MIR_T_I8    ? (ld_p ? 0x77 : 0x72)\n                       : type == MIR_T_U8  ? (ld_p ? 0x90 : 0x72)\n                       : type == MIR_T_I16 ? (ld_p ? 0x78 : 0x70)\n                       : type == MIR_T_U16 ? (ld_p ? 0x91 : 0x70)\n                       : type == MIR_T_I32 ? (ld_p ? 0x14 : 0x50)\n                       : type == MIR_T_U32 ? (ld_p ? 0x16 : 0x50)\n                                           : (ld_p ? 0x04 : 0x24));\n  uint64_t g = ((0xe3l << 40) | common | lgopcode) << 16;\n  uint64_t ey = ((0xedl << 40) | common | (ld_p ? 0x64 : 0x66)) << 16;\n  uint64_t dy = ((0xedl << 40) | common | (ld_p ? 0x65 : 0x67)) << 16;\n  /* (lg|lgf|llgf|lgb|llgc|lhy|llgh|ley|ldy|stg|sty|sthy|stcy|stey|stdy) reg, disp(base): */\n  assert (type != MIR_T_LD && reg < 16 && base < 16 && -(1 << 19) < disp && disp < (1 << 19));\n  push_insns (insn_varr, (uint8_t *) (single_p ? &ey : double_p ? &dy : &g), 6);\n}\n\nstatic void s390x_gen_ld (VARR (uint8_t) * insn_varr, unsigned to, unsigned base, int disp,\n                          MIR_type_t type) {\n  s390x_gen_ld_st (insn_varr, to, base, disp, type, TRUE);\n}\n\nstatic void s390x_gen_st (VARR (uint8_t) * insn_varr, unsigned from, unsigned base, int disp,\n                          MIR_type_t type) {\n  s390x_gen_ld_st (insn_varr, from, base, disp, type, FALSE);\n}\n\nstatic void s390x_gen_ldstm (VARR (uint8_t) * insn_varr, unsigned from, unsigned to, unsigned base,\n                             int disp, int ld_p) {\n  uint64_t dl = disp & 0xfff, dh = (disp >> 12) & 0xff;\n  uint64_t common = ((uint64_t) from << 36) | ((uint64_t) to << 32) | ((uint64_t) base << 28)\n                    | (dl << 16) | (dh << 8);\n  uint64_t g = ((0xebl << 40) | common | (ld_p ? 0x4 : 0x24)) << 16;\n  /* (lmg|stmg) from,to,disp(base): */\n  assert (from < 16 && to < 16 && base < 16 && -(1 << 19) < disp && disp < (1 << 19));\n  push_insns (insn_varr, (uint8_t *) &g, 6);\n}\n\nstatic void s390x_gen_jump (VARR (uint8_t) * insn_varr, unsigned int reg, int call_p) {\n  uint16_t bcr = (0x7 << 8) | (15 << 4) | reg;  /* bcr 15,reg: */\n  uint16_t balr = (0x5 << 8) | (14 << 4) | reg; /* balr 14,reg: */\n  assert (reg != 0 && reg < 16);\n  push_insns (insn_varr, (uint8_t *) (call_p ? &balr : &bcr), 2);\n}\n\nstatic void s390x_gen_addi (VARR (uint8_t) * insn_varr, unsigned dst, unsigned src, int disp) {\n  uint64_t dl = disp & 0xfff, dh = (disp >> 12) & 0xff;\n  uint64_t ops = ((uint64_t) dst << 36) | ((uint64_t) src << 28) | (dl << 16) | (dh << 8);\n  uint64_t lay = ((0xe3l << 40) | ops | 0x71) << 16; /* lay dst,disp(src) */\n  assert (dst < 16 && src < 16 && -(1 << 19) < disp && disp < (1 << 19));\n  push_insns (insn_varr, (uint8_t *) &lay, 6);\n}\n\nstatic void s390x_gen_3addrs (VARR (uint8_t) * insn_varr, unsigned int r1, void *a1,\n                              unsigned int r2, void *a2, int r3, void *a3) {\n  /* 6b:lalr r3,22+align;6b:lg r1,0(r3);6b:lg r2,8(r3);[6b:lg r3,16(r3);]4b:bc m15,s;align;a1-a3:s\n   */\n  size_t off = (r3 < 0 ? 22 : 28);\n  size_t rem = (VARR_LENGTH (uint8_t, insn_varr) + off) % 8;\n  size_t padding = rem == 0 ? 0 : 8 - rem;\n  uint64_t lalr = ((0xc0l << 40) | ((uint64_t) r1 << 36) | (off + padding) / 2) << 16;\n  uint32_t brc\n    = (0xa7 << 24) | (15 << 20) | (4 << 16) | ((r3 < 0 ? 20 : 28) + padding) / 2; /* brc m15,28: */\n  assert (r1 != 0);\n  push_insns (insn_varr, (uint8_t *) &lalr, 6);\n  if (r3 >= 0) s390x_gen_ld (insn_varr, r3, r1, 16, MIR_T_I64); /* lg r3,16(r1) */\n  s390x_gen_ld (insn_varr, r2, r1, 8, MIR_T_I64);               /* lg r2,8(r1) */\n  s390x_gen_ld (insn_varr, r1, r1, 0, MIR_T_I64);               /* lg r1,0(r1) */\n  push_insns (insn_varr, (uint8_t *) &brc, 4);\n  for (size_t i = 0; i < padding; i++) VARR_PUSH (uint8_t, insn_varr, 0);\n  push_insns (insn_varr, (uint8_t *) &a1, 8);\n  push_insns (insn_varr, (uint8_t *) &a2, 8);\n  if (r3 >= 0) push_insns (insn_varr, (uint8_t *) &a3, 8);\n}\n\nstatic void s390x_gen_blk_mov (VARR (uint8_t) * insn_varr, uint32_t param_offset,\n                               uint32_t addr_offset, uint32_t qwords, uint32_t addr_reg) {\n  uint16_t *addr;\n  static const uint16_t blk_mov_pat[] = {\n    /*0:*/ 0xa7a9,  0x0000,         /* lghi\t%r10,<size> */\n    /*4:*/ 0xa7ab,  0xfff8,         /* aghi\t%r10,-8 */\n    /*8:*/ 0xe30a,  0x9000, 0x0004, /* lg %r0,0(%r10,%r9) */\n    /*14:*/ 0xe30a, 0x0000, 0x0024, /* stg %r0,0(%r10,<addr_reg:2-6,8>) */\n    /*20:*/ 0xb902, 0x00aa,         /* ltgr %r10,%r10 */\n    /*24:*/ 0xa724, 0xfff6,         /* jh 4 */\n  };\n  s390x_gen_addi (insn_varr, addr_reg, 15, addr_offset); /* lay <addr_reg>,addr_offset(r15) */\n  if (qwords == 0) return;\n  assert (qwords * 8 < (1 << 15) && addr_reg < 16 && addr_offset % 8 == 0);\n  s390x_gen_ld (insn_varr, 9, 7, param_offset, MIR_T_I64); /* lg* 9,param_offset(r7) */\n  addr = (uint16_t *) push_insns (insn_varr, (uint8_t *) blk_mov_pat, sizeof (blk_mov_pat));\n  addr[1] |= qwords * 8;     /* lghi */\n  addr[8] |= addr_reg << 12; /* stg */\n}\n\nvoid *_MIR_get_bstart_builtin (MIR_context_t ctx) {\n  VARR (uint8_t) * code;\n  void *res;\n\n  VARR_CREATE (uint8_t, code, ctx->alloc, 128);\n  s390x_gen_mov (code, 2, 15);      /* lgr r2,15 */\n  s390x_gen_jump (code, 14, FALSE); /* bcr m15,r14 */\n  res = _MIR_publish_code (ctx, VARR_ADDR (uint8_t, code), VARR_LENGTH (uint8_t, code));\n  VARR_DESTROY (uint8_t, code);\n  return res;\n}\n\nvoid *_MIR_get_bend_builtin (MIR_context_t ctx) {\n  VARR (uint8_t) * code;\n  void *res;\n\n  VARR_CREATE (uint8_t, code, ctx->alloc, 128);\n  s390x_gen_ld (code, 0, 15, 0, MIR_T_I64); /* r0 = 0(r15) */\n  s390x_gen_st (code, 0, 2, 0, MIR_T_I64);  /* 0(r2) = r0 */\n  s390x_gen_mov (code, 15, 2);              /* lgr r15,2 */\n  s390x_gen_jump (code, 14, FALSE);         /* bcr m15,r14 */\n  res = _MIR_publish_code (ctx, VARR_ADDR (uint8_t, code), VARR_LENGTH (uint8_t, code));\n  VARR_DESTROY (uint8_t, code);\n  return res;\n}\n\nstatic const int max_thunk_len = (4 * 8); /* see _MIR_redirect_thunk */\nvoid *_MIR_get_thunk (MIR_context_t ctx) {\n  VARR (uint8_t) * code;\n  void *res;\n\n  VARR_CREATE (uint8_t, code, ctx->alloc, 128);\n  for (int i = 0; i < max_thunk_len; i++) VARR_PUSH (uint8_t, code, 0);\n  res = _MIR_publish_code (ctx, VARR_ADDR (uint8_t, code), VARR_LENGTH (uint8_t, code));\n  VARR_DESTROY (uint8_t, code);\n  return res;\n}\n\nstatic void redirect_thunk (MIR_context_t ctx, void *thunk, void *to, long temp_reg) {\n  int64_t offset = (uint8_t *) to - (uint8_t *) thunk;\n  VARR (uint8_t) * code;\n\n  assert (temp_reg != 0);\n  VARR_CREATE (uint8_t, code, ctx->alloc, 128);\n  assert (offset % 2 == 0);\n  offset /= 2;\n  if (-(1l << 31) < offset && offset < (1l << 31)) { /* brcl m15,offset: */\n    uint64_t brcl = ((0xc0l << 40) | (15l << 36) | (4l << 32) | (offset & 0xffffffff)) << 16;\n    push_insns (code, (uint8_t *) &brcl, 6);\n  } else { /* 6b:lalr tr,14+padding; 6b:lg tr,0(tr); 2b:bcr m15,tr;padding; 64-bit address: */\n    size_t rem = (VARR_LENGTH (uint8_t, code) + 14) % 8;\n    size_t padding = rem == 0 ? 0 : 8 - rem;\n    uint64_t lalr = ((0xc0l << 40) | (temp_reg << 36) | (14 + padding) / 2) << 16;\n    uint64_t lg = ((0xe3l << 40) | (temp_reg << 36) | (temp_reg << 28) | 0x4) << 16;\n    uint16_t bcr = (0x7 << 8) | (15 << 4) | temp_reg; /* bcr 15,tr: */\n    push_insns (code, (uint8_t *) &lalr, 6);\n    push_insns (code, (uint8_t *) &lg, 6);\n    push_insns (code, (uint8_t *) &bcr, 2);\n    for (size_t i = 0; i < padding; i++) VARR_PUSH (uint8_t, code, 0);\n    push_insns (code, (uint8_t *) &to, 8);\n  }\n  _MIR_change_code (ctx, thunk, VARR_ADDR (uint8_t, code), VARR_LENGTH (uint8_t, code));\n  VARR_DESTROY (uint8_t, code);\n}\n\nvoid _MIR_redirect_thunk (MIR_context_t ctx, void *thunk, void *to) {\n  redirect_thunk (ctx, thunk, to, 1);\n}\n\nstatic void *get_jump_addr (uint8_t *insns) {\n  assert (insns[0] == 0xc0);\n  if ((insns[1] >> 4) == 15) { /* bcrl m15,offset */\n    int32_t offset = (insns[2] << 24) | (insns[3] << 16) | (insns[4] << 8) | insns[5];\n    return insns + offset * 2;\n  } else {\n    size_t addr = ((size_t) insns + 14 + 7) / 8 * 8;\n    return *(void **) addr;\n  }\n}\n\nvoid *_MIR_get_thunk_addr (MIR_context_t ctx MIR_UNUSED, void *thunk) {\n  return get_jump_addr (thunk);\n}\n\nstruct s390x_va_list {\n  long __gpr, __fpr;         /* number of args read until now */\n  void *__overflow_arg_area; /* argument on the stack to read next */\n  void *__reg_save_area;     /* curr func frame start */\n};\n\nvoid *va_arg_builtin (void *p, uint64_t t) {\n  struct s390x_va_list *va = p;\n  MIR_type_t type = t;\n  int fp_p = type == MIR_T_F || type == MIR_T_D;\n  void *a;\n\n  if (!fp_p) {\n    if (va->__gpr < 5) {\n      a = (char *) va->__reg_save_area + 16 + 8 * va->__gpr;\n    } else {\n      a = va->__overflow_arg_area;\n      va->__overflow_arg_area = (char *) va->__overflow_arg_area + 8;\n    }\n    va->__gpr++;\n    if (type == MIR_T_LD) a = *(void **) a; /* always passed by address */\n  } else {\n    if (va->__fpr < 4) {\n      a = (char *) va->__reg_save_area + 128 + 8 * va->__fpr;\n    } else {\n      a = va->__overflow_arg_area;\n      va->__overflow_arg_area = (char *) va->__overflow_arg_area + 8;\n    }\n    va->__fpr++;\n  }\n  if (type == MIR_T_F || type == MIR_T_I32) a = (char *) a + 4; /* 2nd word of doubleword */\n  return a;\n}\n\nvoid va_block_arg_builtin (void *res, void *p, size_t s, uint64_t ncase MIR_UNUSED) {\n  void *a = *(void **) va_arg_builtin (p, MIR_T_I64);\n  if (res != NULL) memcpy (res, a, s);\n}\n\nvoid va_start_interp_builtin (MIR_context_t ctx MIR_UNUSED, void *p, void *a) {\n  struct s390x_va_list *va = p;\n  va_list *vap = a;\n\n  assert (sizeof (struct s390x_va_list) == sizeof (va_list));\n  *va = *(struct s390x_va_list *) vap;\n}\n\nvoid va_end_interp_builtin (MIR_context_t ctx MIR_UNUSED, void *p MIR_UNUSED) {}\n\n/* Generation: fun (fun_addr, res_arg_addresses):\n   save r6, r7, r14 (r15 + 48,112);\n   allocate and stack frame (S390X_STACK_HEADER_SIZE + param area size + ld arg values size);\n   r1=r2 (fun_addr);\n   r7=r3 (res_arg_addresses);\n   (arg_reg=mem[r7,arg_offset] or\n   (f1,r0)=mem[r7,arg_offset];mem[r15,S390X_STACK_HEADER_SIZE+offset]=(f1,r0)) ... call *r1;\n   r0=mem[r7,<res_offset>]; res_reg=mem[r0]; ...\n   restore r15; restore r6, r7, r14; return. */\nvoid *_MIR_get_ff_call (MIR_context_t ctx, size_t nres, MIR_type_t *res_types, size_t nargs,\n                        _MIR_arg_desc_t *arg_descs, size_t arg_vars_num MIR_UNUSED) {\n  MIR_type_t type;\n  int n_gpregs = 0, n_fpregs = 0, res_reg = 7, frame_size, disp, param_offset, blk_offset;\n  uint32_t qwords, addr_reg;\n  VARR (uint8_t) * code;\n  void *res;\n\n  VARR_CREATE (uint8_t, code, ctx->alloc, 128);\n  blk_offset = frame_size = S390X_STACK_HEADER_SIZE;\n  if (nres > 0 && res_types[0] == MIR_T_LD) n_gpregs++; /* ld address */\n  for (uint32_t i = 0; i < nargs; i++) {                /* calculate param area size: */\n    type = arg_descs[i].type;\n    if (MIR_blk_type_p (type)) frame_size += (arg_descs[i].size + 7) / 8 * 8; /* blk value space */\n    if ((type == MIR_T_F || type == MIR_T_D) && n_fpregs < 4) {\n      n_fpregs++;\n    } else if (type != MIR_T_F && type != MIR_T_D && n_gpregs < 5) { /* RBLK too */\n      n_gpregs++;\n    } else {\n      frame_size += 8;\n      blk_offset += 8;\n    }\n  }\n  s390x_gen_ldstm (code, 6, 7, 15, 48, FALSE); /* stmg 6,7,48(r15) : */\n  s390x_gen_ldstm (code, 8, 9, 15, 64, FALSE); /* stmg 8,9,64(r15) : */\n  s390x_gen_st (code, 10, 15, 80, MIR_T_I64);  /* stg r10,80(r15) */\n  s390x_gen_st (code, 14, 15, 112, MIR_T_I64); /* stg r14,112(r15) */\n  s390x_gen_addi (code, 15, 15, -frame_size);  /* lay r15,-frame_size(r15) */\n  s390x_gen_mov (code, 1, 2);                  /* fun_addr */\n  s390x_gen_mov (code, res_reg, 3);            /* results & args */\n  n_gpregs = n_fpregs = 0;\n  param_offset = nres * 16;                   /* args start */\n  disp = S390X_STACK_HEADER_SIZE;             /* param area start */\n  if (nres > 0 && res_types[0] == MIR_T_LD) { /* ld address: */\n    s390x_gen_mov (code, 2, res_reg);         /* lgr r2,r7 */\n    n_gpregs++;\n  }\n  for (uint32_t i = 0; i < nargs; i++) { /* load args: */\n    type = arg_descs[i].type;\n    if ((type == MIR_T_F || type == MIR_T_D) && n_fpregs < 4) {\n      /* (le,ld) (f0,f2,f4,f6),param_ofset(r7) */\n      s390x_gen_ld (code, n_fpregs * 2, res_reg, param_offset, type);\n      n_fpregs++;\n    } else if (type == MIR_T_F || type == MIR_T_D) {\n      s390x_gen_ld (code, 1, res_reg, param_offset, type); /* (le,ld) f1,param_offset(r7) */\n      s390x_gen_st (code, 1, 15, disp, type);              /* (ste,std) f1,disp(r15) */\n      disp += 8;\n    } else if (type == MIR_T_LD && n_gpregs < 5) {                /* ld address */\n      s390x_gen_addi (code, n_gpregs + 2, res_reg, param_offset); /* lay rn,param_offset(r7) */\n      n_gpregs++;\n    } else if (type == MIR_T_LD) {                     /* pass address of location in the result: */\n      s390x_gen_addi (code, 0, res_reg, param_offset); /* lay r0,param_offset(r7) */\n      s390x_gen_st (code, 0, 15, disp, MIR_T_I64);     /* stg r0,disp(r15) */\n      disp += 8;\n    } else if (MIR_blk_type_p (type)) {\n      qwords = (arg_descs[i].size + 7) / 8;\n      addr_reg = n_gpregs < 5 ? n_gpregs + 2 : 8;\n      s390x_gen_blk_mov (code, param_offset, blk_offset, qwords, addr_reg);\n      blk_offset += qwords * 8;\n      if (n_gpregs < 5) {\n        n_gpregs++;\n      } else {\n        s390x_gen_st (code, 8, 15, disp, MIR_T_I64); /* stg r8,disp(r15) */\n        disp += 8;\n      }\n    } else if (n_gpregs < 5) { /* RBLK too */\n      s390x_gen_ld (code, n_gpregs + 2, res_reg, param_offset,\n                    MIR_T_I64); /* lg* rn,param_offset(r7) */\n      n_gpregs++;\n    } else {\n      s390x_gen_ld (code, 0, res_reg, param_offset, MIR_T_I64); /* lg* r0,param_offset(r7) */\n      s390x_gen_st (code, 0, 15, disp, MIR_T_I64);              /* stg* r0,disp(r15) */\n      disp += 8;\n    }\n    param_offset += 16;\n  }\n  s390x_gen_jump (code, 1, TRUE); /* call *r1 */\n  n_gpregs = n_fpregs = 0;\n  disp = 0;\n  for (uint32_t i = 0; i < nres; i++) {\n    type = res_types[i];\n    if (type == MIR_T_LD) continue; /* do nothing: the result value is already in results */\n    if ((type == MIR_T_F || type == MIR_T_D) && n_fpregs < 4) {\n      s390x_gen_st (code, n_fpregs * 2, res_reg, disp, type);\n      n_fpregs++;\n    } else if (type != MIR_T_F && type != MIR_T_D && n_gpregs < 1) {  // just one gp reg\n      s390x_gen_st (code, n_gpregs + 2, res_reg, disp, MIR_T_I64);\n      n_gpregs++;\n    } else {\n      MIR_get_error_func (ctx) (MIR_ret_error,\n                                \"s390x can not handle this combination of return values\");\n    }\n    disp += 16;\n  }\n  s390x_gen_addi (code, 15, 15, frame_size);   /* lay 15,frame_size(15) */\n  s390x_gen_ldstm (code, 6, 7, 15, 48, TRUE);  /* lmg 6,7,48(r15) : */\n  s390x_gen_ldstm (code, 8, 9, 15, 64, TRUE);  /* lmg 8,9,64(r15) : */\n  s390x_gen_ld (code, 10, 15, 80, MIR_T_I64);  /* lg 10,80(r15) */\n  s390x_gen_ld (code, 14, 15, 112, MIR_T_I64); /* lg 14,112(r15) */\n  s390x_gen_jump (code, 14, FALSE);            /* bcr m15,r14 */\n  res = _MIR_publish_code (ctx, VARR_ADDR (uint8_t, code), VARR_LENGTH (uint8_t, code));\n#if 0\n  if (getenv (\"MIR_code_dump\") != NULL)\n    _MIR_dump_code (\"ffi:\", VARR_ADDR (uint8_t, code), VARR_LENGTH (uint8_t, code));\n#endif\n  VARR_DESTROY (uint8_t, code);\n  return res;\n}\n\n/* Transform C call to call of void handler (MIR_context_t ctx, MIR_item_t func_item,\n                                             va_list va, MIR_val_t *results):\n   Brief: save all C call args to register save area; save r7, r14;\n          allocate shim stack frame (S390X_STACK_HEADER_SIZE + space for results and va);\n          call handler with args; move results to return regs; restore r7,r14,r15; return */\nvoid *_MIR_get_interp_shim (MIR_context_t ctx, MIR_item_t func_item, void *handler) {\n  MIR_func_t func = func_item->u.func;\n  uint32_t nres = func->nres;\n  MIR_type_t type, *res_types = func->res_types;\n  int disp, frame_size, local_var_size, n_gpregs, n_fpregs, va_list_disp, results_disp;\n  VARR (uint8_t) * code;\n  void *res;\n\n  VARR_CREATE (uint8_t, code, ctx->alloc, 128);\n  frame_size = S390X_STACK_HEADER_SIZE;        /* register save area */\n  s390x_gen_st (code, 14, 15, 112, MIR_T_I64); /* stg 14,112(r15) */\n  s390x_gen_ldstm (code, 2, 6, 15, 16, FALSE); /* stmg 2,6,16(r15) : */\n  for (unsigned reg = 0; reg <= 6; reg += 2)   /* stdy f0,f2,f4,f6,128(r15) : */\n    s390x_gen_st (code, reg, 15, reg * 4 + 128, MIR_T_D);\n  local_var_size = sizeof (struct s390x_va_list) + nres * 16; /* allocate va and results */\n  va_list_disp = frame_size;\n  results_disp = va_list_disp + sizeof (struct s390x_va_list);\n  frame_size += local_var_size;\n  assert (frame_size % 8 == 0);\n  s390x_gen_addi (code, 15, 15, -frame_size);\n  /* setup va: mvghi va(15),(0,1): __gpr */\n  s390x_gen_mvi (code, nres > 0 && res_types[0] == MIR_T_LD ? 1 : 0, 15, va_list_disp);\n  s390x_gen_mvi (code, 0, 15, va_list_disp + 8);            /* mvghi va+8(15),0: __fpr */\n  s390x_gen_addi (code, 1, 15, frame_size);                 /* lay 1,frame_size(15) */\n  s390x_gen_st (code, 1, 15, va_list_disp + 24, MIR_T_I64); /* stg 1,va+24(r15): __reg_save_area */\n  s390x_gen_addi (code, 1, 1, S390X_STACK_HEADER_SIZE);     /* lay 1,S390X_STACK_HEADER_SIZE(1) */\n  /* stg 1,va+16(r15):__overflow_arg_area: */\n  s390x_gen_st (code, 1, 15, va_list_disp + 16, MIR_T_I64);\n  /* call handler: */\n  s390x_gen_3addrs (code, 2, ctx, 3, func_item, 1, handler);\n  s390x_gen_addi (code, 4, 15, va_list_disp);\n  s390x_gen_addi (code, 5, 15, results_disp);\n  s390x_gen_jump (code, 1, TRUE);\n  /* setup result regs: */\n  disp = results_disp;\n  n_gpregs = n_fpregs = 0;\n  for (uint32_t i = 0; i < nres; i++) {\n    type = res_types[i];\n    if ((type == MIR_T_F || type == MIR_T_D) && n_fpregs < 4) {\n      s390x_gen_ld (code, n_fpregs * 2, 15, disp, type);\n      n_fpregs++;\n    } else if (type != MIR_T_F && type != MIR_T_D && n_gpregs < 1) {  // just one gp reg\n      if (type != MIR_T_LD) {\n        s390x_gen_ld (code, n_gpregs + 2, 15, disp, MIR_T_I64);\n      } else {\n        /* ld address: lg r2,16+frame_size(r15)  */\n        s390x_gen_ld (code, 2, 15, 16 + frame_size, MIR_T_I64);\n        s390x_gen_ld (code, 0, 15, disp, MIR_T_D);     /* ld f0,disp(r15) */\n        s390x_gen_ld (code, 2, 15, disp + 8, MIR_T_D); /* ld f2,disp + 8(r15) */\n        s390x_gen_st (code, 0, 2, 0, MIR_T_D);         /* st f0,0(r2) */\n        s390x_gen_st (code, 2, 2, 8, MIR_T_D);         /* st f2,8(r2) */\n      }\n      n_gpregs++;\n    } else {\n      MIR_get_error_func (ctx) (MIR_ret_error,\n                                \"s390x can not handle this combination of return values\");\n    }\n    disp += 16;\n  }\n  s390x_gen_addi (code, 15, 15, frame_size);   /* lay 15,frame_size(15) */\n  s390x_gen_ld (code, 6, 15, 48, MIR_T_I64);   /* lg 6,48(r15) : */\n  s390x_gen_ld (code, 14, 15, 112, MIR_T_I64); /* lg 14,112(r15) */\n  s390x_gen_jump (code, 14, FALSE);            /* bcr m15,r14 */\n  res = _MIR_publish_code (ctx, VARR_ADDR (uint8_t, code), VARR_LENGTH (uint8_t, code));\n#if 0\n  if (getenv (\"MIR_code_dump\") != NULL)\n    _MIR_dump_code (func->name, VARR_ADDR (uint8_t, code), VARR_LENGTH (uint8_t, code));\n#endif\n  VARR_DESTROY (uint8_t, code);\n  return res;\n}\n\n/* Brief: save r14 (r15+120); save all param regs r2-r6 (r15+16),f0,f2,f4,f6 (r15+128);\n   update r15; allocate and form minimal wrapper stack frame (S390X_STACK_HEADER_SIZE);\n   r2 = call hook_address (ctx, called_func); r1=r2; restore all params regs, r15, r14; bcr r1 */\nvoid *_MIR_get_wrapper (MIR_context_t ctx, MIR_item_t called_func, void *hook_address) {\n  VARR (uint8_t) * code;\n  void *res;\n  /* 16b offset -- 6b:lalr r1; 2b(align): lr r0,r0;  6b:lg r1,24(r1); 2b: balr r1,r1; */\n  uint64_t lalr = ((0xc0l << 40) | ((uint64_t) 1 << 36) | (16 / 2)) << 16;\n  uint16_t lr = (0x18 << 8);                 /* lr r0,r0 */\n  uint16_t balr = (0x5 << 8) | (1 << 4) | 1; /* balr r1,r1: */\n\n  VARR_CREATE (uint8_t, code, ctx->alloc, 128);\n  push_insns (code, (uint8_t *) &lalr, 6);\n  push_insns (code, (uint8_t *) &lr, 2);\n  s390x_gen_ld (code, 1, 1, 24, MIR_T_I64); /* lg r1,24(r1) */\n  push_insns (code, (uint8_t *) &balr, 2);  /* balr r1,r1 */\n  push_insns (code, (uint8_t *) &ctx, 8);\n  push_insns (code, (uint8_t *) &called_func, 8);\n  push_insns (code, (uint8_t *) &hook_address, 8);\n  push_insns (code, (uint8_t *) &wrapper_end_addr, 8);\n  res = _MIR_publish_code (ctx, VARR_ADDR (uint8_t, code), VARR_LENGTH (uint8_t, code));\n#if 0\n  if (getenv (\"MIR_code_dump\") != NULL)\n    _MIR_dump_code (\"func wrapper:\", res, VARR_LENGTH (uint8_t, code));\n#endif\n  VARR_DESTROY (uint8_t, code);\n  return res;\n}\n\n/* Brief: save r14 (r15+120); save all param regs r2-r6 (r15+16),f0,f2,f4,f6 (r15+128);\n   update r15; allocate and form minimal wrapper stack frame (S390X_STACK_HEADER_SIZE);\n   r2 = call hook_address (ctx, called_func); r1=r2; restore all params regs, r15, r14; bcr r1 */\nvoid *_MIR_get_wrapper_end (MIR_context_t ctx) {\n  VARR (uint8_t) * code;\n  void *res;\n\n  VARR_CREATE (uint8_t, code, ctx->alloc, 128);\n  s390x_gen_st (code, 14, 15, 112, MIR_T_I64); /* stg 14,112(r15) */\n  s390x_gen_ldstm (code, 2, 6, 15, 16, FALSE); /* stmg 2,6,16(r15) : */\n  for (unsigned reg = 0; reg <= 6; reg += 2)   /* stdy f0,f2,f4,f6,128(r15) : */\n    s390x_gen_st (code, reg, 15, reg * 4 + 128, MIR_T_D);\n  /* r15 -= frame_size: */\n  s390x_gen_addi (code, 15, 15, -S390X_STACK_HEADER_SIZE);\n  s390x_gen_ld (code, 2, 1, 0, MIR_T_I64);  /* lg r2,0(r1) */\n  s390x_gen_ld (code, 3, 1, 8, MIR_T_I64);  /* lg r3,8(r1) */\n  s390x_gen_ld (code, 4, 1, 16, MIR_T_I64); /* lg r4,16(r1) */\n  s390x_gen_jump (code, 4, TRUE);\n  s390x_gen_mov (code, 1, 2);\n  s390x_gen_addi (code, 15, 15, S390X_STACK_HEADER_SIZE);\n  for (unsigned reg = 0; reg <= 6; reg += 2) /* ldy fn,disp(r15) : */\n    s390x_gen_ld (code, reg, 15, reg * 4 + 128, MIR_T_D);\n  s390x_gen_ldstm (code, 2, 6, 15, 16, TRUE);  /* lmg 2,6,16(r15) : */\n  s390x_gen_ld (code, 14, 15, 112, MIR_T_I64); /* lg 14,112(r15) */\n  s390x_gen_jump (code, 1, FALSE);\n  res = _MIR_publish_code (ctx, VARR_ADDR (uint8_t, code), VARR_LENGTH (uint8_t, code));\n#if 0\n  if (getenv (\"MIR_code_dump\") != NULL)\n    _MIR_dump_code (\"func wrapper end:\", res, VARR_LENGTH (uint8_t, code));\n#endif\n  VARR_DESTROY (uint8_t, code);\n  return res;\n}\n\n/* r0=<bb_version>; jump handler  ??? mutex free */\nvoid *_MIR_get_bb_thunk (MIR_context_t ctx, void *bb_version, void *handler) {\n  void *res;\n  size_t offset;\n  VARR (uint8_t) * code;\n  uint64_t lalr = ((0xc0l << 40) | (1l << 36) | (16 + max_thunk_len) / 2) << 16;\n  uint64_t lg = ((0xe3l << 40) | (0l << 36) | (1l << 28) | 0x4) << 16;\n  uint32_t nop = (0x47 << 24);\n\n  VARR_CREATE (uint8_t, code, ctx->alloc, 64);\n  /* 6b:lalr r1,8; 6b:lg r0,0(r1); 4b: nop for padding; */\n  push_insns (code, (uint8_t *) &lalr, 6);\n  push_insns (code, (uint8_t *) &lg, 6);\n  push_insns (code, (uint8_t *) &nop, 4);\n  offset = VARR_LENGTH (uint8_t, code);\n  for (int i = 0; i < max_thunk_len; i++) VARR_PUSH (uint8_t, code, 0);\n  assert (max_thunk_len % 8 == 0 && VARR_LENGTH (uint8_t, code) % 8 == 0);\n  push_insns (code, (uint8_t *) &bb_version, 8);\n  res = _MIR_publish_code (ctx, VARR_ADDR (uint8_t, code), VARR_LENGTH (uint8_t, code));\n  _MIR_redirect_thunk (ctx, (uint8_t *) res + offset, handler);\n#if 0\n  if (getenv (\"MIR_code_dump\") != NULL)\n    _MIR_dump_code (\"bb thunk:\", res, VARR_LENGTH (uint8_t, code));\n#endif\n  VARR_DESTROY (uint8_t, code);\n  return res;\n}\n\n/* change to (b|br) to */\nvoid _MIR_replace_bb_thunk (MIR_context_t ctx, void *thunk, void *to) {\n  redirect_thunk (ctx, thunk, to, 1);\n}\n\n/* save clobbered regs (r2-r6, f0-f7); r1 = call hook_address (data, r0); restore regs; br\n   r1, r1 is a generator temp reg which is not used across bb borders. */\nvoid *_MIR_get_bb_wrapper (MIR_context_t ctx, void *data, void *hook_address) {\n  void *res;\n  VARR (uint8_t) * code;\n\n  VARR_CREATE (uint8_t, code, ctx->alloc, 128);\n  /* saving regs which can be put reg save area: */\n  s390x_gen_ldstm (code, 2, 6, 15, 16, FALSE); /* stmg 2,6,16(r15) : */\n  s390x_gen_st (code, 14, 15, 112, MIR_T_I64); /* ???do we need this: stg 14,112(r15) */\n  for (unsigned reg = 0; reg <= 6; reg += 2)   /* stdy f0,f2,f4,f6,128(r15) : */\n    s390x_gen_st (code, reg, 15, reg * 4 + 128, MIR_T_D);\n  s390x_gen_addi (code, 15, 15, -48);\n  /* saving f1,f3,f5,f5: */\n  for (unsigned reg = 1; reg <= 7; reg += 2) /* stdy f1,f3,f5,f7,16(r15) : */\n    s390x_gen_st (code, reg, 15, (reg - 1) * 4 + 16, MIR_T_D);\n  /* r15 -= 160: */\n  s390x_gen_addi (code, 15, 15, -S390X_STACK_HEADER_SIZE);\n  s390x_gen_3addrs (code, 1, hook_address, 2, data, -1, NULL);\n  s390x_gen_mov (code, 3, 0);     /* r3=r0 */\n  s390x_gen_jump (code, 1, TRUE); /* call r1 */\n  s390x_gen_mov (code, 1, 2);     /* r1=r2 */\n  s390x_gen_addi (code, 15, 15, S390X_STACK_HEADER_SIZE);\n  /* restoring f1,f3,f5,f5: */\n  for (unsigned reg = 1; reg <= 7; reg += 2) /* stdy f1,f3,f5,f7,16(r15) : */\n    s390x_gen_ld (code, reg, 15, (reg - 1) * 4 + 16, MIR_T_D);\n  s390x_gen_addi (code, 15, 15, 48);\n  /* restoring regs which can be put reg save area: */\n  for (unsigned reg = 0; reg <= 6; reg += 2) /* ldy fn,disp(r15) : */\n    s390x_gen_ld (code, reg, 15, reg * 4 + 128, MIR_T_D);\n  s390x_gen_ld (code, 14, 15, 112, MIR_T_I64); /* ??? do we need this: lg 14,112(r15) */\n  s390x_gen_ldstm (code, 2, 6, 15, 16, TRUE);  /* lmg 2,6,16(r15) : */\n  s390x_gen_jump (code, 1, FALSE);             /* bcr r1 */\n  res = _MIR_publish_code (ctx, VARR_ADDR (uint8_t, code), VARR_LENGTH (uint8_t, code));\n#if 0\n  if (getenv (\"MIR_code_dump\") != NULL)\n    _MIR_dump_code (\"bb wrapper:\", res, VARR_LENGTH (uint8_t, code));\n#endif\n  VARR_DESTROY (uint8_t, code);\n  return res;\n}\n"
        },
        {
          "name": "mir-s390x.h",
          "type": "blob",
          "size": 2.5087890625,
          "content": "/* This file is a part of MIR project.\n   Copyright (C) 2018-2024 Vladimir Makarov <vmakarov.gcc@gmail.com>.\n   A common include file for mir-s390x.c and mir-gen-s390x.c\n*/\n\n#include \"mir.h\"\n\n#define HREG_EL(h) h##_HARD_REG\n#define REP_SEP ,\nenum {\n  REP8 (HREG_EL, R0, R1, R2, R3, R4, R5, R6, R7),\n  REP8 (HREG_EL, R8, R9, R10, R11, R12, R13, R14, R15),\n  REP8 (HREG_EL, F0, F1, F2, F3, F4, F5, F6, F7),\n  REP8 (HREG_EL, F8, F9, F10, F11, F12, F13, F14, F15),\n};\n#undef REP_SEP\n\nstatic const char *const target_hard_reg_names[] = {\n  \"r0\",  \"r1\",  \"r2\",  \"r3\",  \"r4\",  \"r5\",  \"r6\",  \"r7\",  \"r8\",  \"r9\",  \"r10\",\n  \"r11\", \"r12\", \"r13\", \"r14\", \"r15\", \"f0\",  \"f1\",  \"f2\",  \"f3\",  \"f4\",  \"f5\",\n  \"f6\",  \"f7\",  \"f8\",  \"f9\",  \"f10\", \"f11\", \"f12\", \"f13\", \"f14\", \"f15\",\n};\n\n#define MAX_HARD_REG F15_HARD_REG\nstatic const MIR_reg_t SP_HARD_REG = R15_HARD_REG;\nstatic const MIR_reg_t FP_HARD_REG = R11_HARD_REG;\n\nstatic int target_locs_num (MIR_reg_t loc MIR_UNUSED, MIR_type_t type) {\n  return type == MIR_T_LD ? 2 : 1;\n}\n\n/* Hard regs not used in machinized code and for passing args, preferably call saved ones. */\nstatic const MIR_reg_t TEMP_INT_HARD_REG1 = R1_HARD_REG, TEMP_INT_HARD_REG2 = R9_HARD_REG;\nstatic const MIR_reg_t TEMP_FLOAT_HARD_REG1 = F8_HARD_REG, TEMP_FLOAT_HARD_REG2 = F10_HARD_REG;\nstatic const MIR_reg_t TEMP_DOUBLE_HARD_REG1 = F8_HARD_REG, TEMP_DOUBLE_HARD_REG2 = F10_HARD_REG;\nstatic const MIR_reg_t TEMP_LDOUBLE_HARD_REG1 = F8_HARD_REG;  //???\nstatic const MIR_reg_t TEMP_LDOUBLE_HARD_REG2 = F10_HARD_REG;\n\nstatic inline int target_hard_reg_type_ok_p (MIR_reg_t hard_reg, MIR_type_t type) {\n  assert (hard_reg <= MAX_HARD_REG);\n  if (type == MIR_T_LD) /* f0,f1,f4,f5,f8,f9,f12,f13 - pair starts */\n    return hard_reg >= F0_HARD_REG && (hard_reg - F0_HARD_REG) % 4 <= 1;\n  return MIR_fp_type_p (type) ? F0_HARD_REG <= hard_reg && hard_reg <= F15_HARD_REG\n                              : hard_reg < F0_HARD_REG;\n}\n\nstatic inline int target_fixed_hard_reg_p (MIR_reg_t hard_reg) {\n  assert (hard_reg <= MAX_HARD_REG);\n  return (hard_reg == FP_HARD_REG\n          || hard_reg == SP_HARD_REG\n          /* don't bother to allocate R0 as it has special meaning for base and index reg: */\n          || hard_reg == R0_HARD_REG || hard_reg == TEMP_INT_HARD_REG1\n          || hard_reg == TEMP_INT_HARD_REG2 || hard_reg == TEMP_FLOAT_HARD_REG1\n          || hard_reg == TEMP_FLOAT_HARD_REG2 || hard_reg == TEMP_DOUBLE_HARD_REG1\n          || hard_reg == TEMP_DOUBLE_HARD_REG2 || hard_reg == TEMP_LDOUBLE_HARD_REG1\n          || hard_reg == TEMP_LDOUBLE_HARD_REG2);\n}\n"
        },
        {
          "name": "mir-tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "mir-utils",
          "type": "tree",
          "content": null
        },
        {
          "name": "mir-varr.h",
          "type": "blob",
          "size": 12.208984375,
          "content": "/* This file is a part of MIR project.\n   Copyright (C) 2018-2024 Vladimir Makarov <vmakarov.gcc@gmail.com>.\n*/\n\n#ifndef MIR_VARR_H\n#define MIR_VARR_H\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <assert.h>\n#include \"mir-alloc.h\"\n\n#ifdef __cplusplus\nextern \"C\" {\n#endif\n\n#if !defined(VARR_ENABLE_CHECKING) && !defined(NDEBUG)\n#define VARR_ENABLE_CHECKING\n#endif\n\n#ifndef VARR_ENABLE_CHECKING\n#define VARR_ASSERT(EXPR, OP, T) ((void) (EXPR))\n\n#else\nstatic inline void mir_varr_assert_fail (const char *op, const char *var) {\n  fprintf (stderr, \"wrong %s for %s\", op, var);\n  assert (0);\n}\n\n#define VARR_ASSERT(EXPR, OP, T) (void) ((EXPR) ? 0 : (mir_varr_assert_fail (OP, #T), 0))\n\n#endif\n\n#ifdef __GNUC__\n#define MIR_VARR_UNUSED __attribute__ ((unused))\n#define MIR_VARR_NO_RETURN __attribute__ ((noreturn))\n#else\n#define MIR_VARR_UNUSED\n#define MIR_VARR_NO_RETURN\n#endif\n\nstatic inline void MIR_VARR_NO_RETURN mir_varr_error (const char *message) {\n#ifdef MIR_VARR_ERROR\n  MIR_VARR_ERROR (message);\n  assert (0);\n#else\n  fprintf (stderr, \"%s\\n\", message);\n#endif\n  exit (1);\n}\n\n/*---------------- Typed variable length arrays -----------------------------*/\n#define VARR_CONCAT2(A, B) A##B\n#define VARR_CONCAT3(A, B, C) A##B##C\n#define VARR(T) VARR_CONCAT2 (VARR_, T)\n#define VARR_OP(T, OP) VARR_CONCAT3 (VARR_, T, OP)\n#define VARR_OP_DEF(T, OP) MIR_VARR_UNUSED VARR_OP (T, OP)\n\n#define VARR_T(T)           \\\n  typedef struct VARR (T) { \\\n    size_t els_num;         \\\n    size_t size;            \\\n    T *varr;                \\\n    MIR_alloc_t alloc;      \\\n  } VARR (T)\n\n#define VARR_DEFAULT_SIZE 64\n\n/* Vector of pointer to object.  */\n#define DEF_VARR(T)                                                                           \\\n  VARR_T (T);                                                                                 \\\n                                                                                              \\\n  static inline void VARR_OP_DEF (T, create) (VARR (T) * *varr, MIR_alloc_t alloc,            \\\n                                              size_t size) {                                  \\\n    VARR (T) * va;                                                                            \\\n    if (size == 0) size = VARR_DEFAULT_SIZE;                                                  \\\n    *varr = va = (VARR (T) *) MIR_malloc (alloc, sizeof (VARR (T)));                          \\\n    if (va == NULL) mir_varr_error (\"varr: no memory\");                                       \\\n    va->els_num = 0;                                                                          \\\n    va->size = size;                                                                          \\\n    va->varr = (T *) MIR_malloc (alloc, size * sizeof (T));                                   \\\n    va->alloc = alloc;                                                                        \\\n  }                                                                                           \\\n                                                                                              \\\n  static inline void VARR_OP_DEF (T, destroy) (VARR (T) * *varr) {                            \\\n    VARR (T) *va = *varr;                                                                     \\\n    MIR_alloc_t alloc = va->alloc;                                                            \\\n    VARR_ASSERT (va && va->varr, \"destroy\", T);                                               \\\n    MIR_free (alloc, va->varr);                                                               \\\n    MIR_free (alloc, va);                                                                     \\\n    *varr = NULL;                                                                             \\\n  }                                                                                           \\\n                                                                                              \\\n  static inline size_t VARR_OP_DEF (T, length) (const VARR (T) * varr) {                      \\\n    VARR_ASSERT (varr, \"length\", T);                                                          \\\n    return varr->els_num;                                                                     \\\n  }                                                                                           \\\n                                                                                              \\\n  static inline size_t VARR_OP_DEF (T, capacity) (const VARR (T) * varr) {                    \\\n    VARR_ASSERT (varr, \"size\", T);                                                            \\\n    return varr->size;                                                                        \\\n  }                                                                                           \\\n                                                                                              \\\n  static inline T *VARR_OP_DEF (T, addr) (const VARR (T) * varr) {                            \\\n    VARR_ASSERT (varr, \"addr\", T);                                                            \\\n    return &varr->varr[0];                                                                    \\\n  }                                                                                           \\\n                                                                                              \\\n  static inline T VARR_OP_DEF (T, last) (const VARR (T) * varr) {                             \\\n    VARR_ASSERT (varr && varr->varr && varr->els_num, \"last\", T);                             \\\n    return varr->varr[varr->els_num - 1];                                                     \\\n  }                                                                                           \\\n                                                                                              \\\n  static inline T VARR_OP_DEF (T, get) (const VARR (T) * varr, size_t ix) {                   \\\n    VARR_ASSERT (varr && varr->varr && ix < varr->els_num, \"get\", T);                         \\\n    return varr->varr[ix];                                                                    \\\n  }                                                                                           \\\n                                                                                              \\\n  static inline void VARR_OP_DEF (T, set) (const VARR (T) * varr, size_t ix, T obj) {         \\\n    VARR_ASSERT (varr && varr->varr && ix < varr->els_num, \"set\", T);                         \\\n    varr->varr[ix] = obj;                                                                     \\\n  }                                                                                           \\\n                                                                                              \\\n  static inline void VARR_OP_DEF (T, trunc) (VARR (T) * varr, size_t size) {                  \\\n    VARR_ASSERT (varr && varr->varr && varr->els_num >= size, \"trunc\", T);                    \\\n    varr->els_num = size;                                                                     \\\n  }                                                                                           \\\n                                                                                              \\\n  static inline int VARR_OP_DEF (T, expand) (VARR (T) * varr, size_t size) {                  \\\n    VARR_ASSERT (varr && varr->varr, \"expand\", T);                                            \\\n    MIR_alloc_t alloc = varr->alloc;                                                          \\\n    if (varr->size < size) {                                                                  \\\n      size += size / 2;                                                                       \\\n      varr->varr = (T *) MIR_realloc (alloc, varr->varr, sizeof (T) * varr->size,             \\\n                                      sizeof (T) * size);                                     \\\n      varr->size = size;                                                                      \\\n      return 1;                                                                               \\\n    }                                                                                         \\\n    return 0;                                                                                 \\\n  }                                                                                           \\\n                                                                                              \\\n  static inline void VARR_OP_DEF (T, tailor) (VARR (T) * varr, size_t size) {                 \\\n    VARR_ASSERT (varr && varr->varr, \"tailor\", T);                                            \\\n    MIR_alloc_t alloc = varr->alloc;                                                          \\\n    if (varr->size != size)                                                                   \\\n      varr->varr = (T *) MIR_realloc (alloc, varr->varr, sizeof (T) * varr->size,             \\\n                                      sizeof (T) * size);                                     \\\n    varr->els_num = varr->size = size;                                                        \\\n  }                                                                                           \\\n                                                                                              \\\n  static inline void VARR_OP_DEF (T, push) (VARR (T) * varr, T obj) {                         \\\n    T *slot;                                                                                  \\\n    VARR_OP (T, expand) (varr, varr->els_num + 1);                                            \\\n    slot = &varr->varr[varr->els_num++];                                                      \\\n    *slot = obj;                                                                              \\\n  }                                                                                           \\\n                                                                                              \\\n  static inline void VARR_OP_DEF (T, push_arr) (VARR (T) * varr, const T *objs, size_t len) { \\\n    size_t i;                                                                                 \\\n    T *slot;                                                                                  \\\n    VARR_OP (T, expand) (varr, varr->els_num + len);                                          \\\n    for (i = 0; i < len; i++) {                                                               \\\n      slot = &varr->varr[varr->els_num++];                                                    \\\n      *slot = objs[i];                                                                        \\\n    }                                                                                         \\\n  }                                                                                           \\\n                                                                                              \\\n  static inline T VARR_OP_DEF (T, pop) (VARR (T) * varr) {                                    \\\n    T obj;                                                                                    \\\n    VARR_ASSERT (varr && varr->varr && varr->els_num, \"pop\", T);                              \\\n    obj = varr->varr[--varr->els_num];                                                        \\\n    return obj;                                                                               \\\n  }\n\n#define VARR_CREATE(T, V, A, L) (VARR_OP (T, create) (&(V), A, L))\n#define VARR_DESTROY(T, V) (VARR_OP (T, destroy) (&(V)))\n#define VARR_LENGTH(T, V) (VARR_OP (T, length) (V))\n#define VARR_CAPACITY(T, V) (VARR_OP (T, capacity) (V))\n#define VARR_ADDR(T, V) (VARR_OP (T, addr) (V))\n#define VARR_LAST(T, V) (VARR_OP (T, last) (V))\n#define VARR_GET(T, V, I) (VARR_OP (T, get) (V, I))\n#define VARR_SET(T, V, I, O) (VARR_OP (T, set) (V, I, O))\n#define VARR_TRUNC(T, V, S) (VARR_OP (T, trunc) (V, S))\n#define VARR_EXPAND(T, V, S) (VARR_OP (T, expand) (V, S))\n#define VARR_TAILOR(T, V, S) (VARR_OP (T, tailor) (V, S))\n#define VARR_PUSH(T, V, O) (VARR_OP (T, push) (V, O))\n#define VARR_PUSH_ARR(T, V, A, L) (VARR_OP (T, push_arr) (V, A, L))\n#define VARR_POP(T, V) (VARR_OP (T, pop) (V))\n#define VARR_FOREACH_ELEM(T, V, I, EL) \\\n  for ((I) = 0; (I) >= VARR_LENGTH (T, V) ? 0 : (EL = VARR_GET (T, V, I), 1); (I)++)\n\n#ifdef __cplusplus\n}\n#endif\n\n#endif /* #ifndef MIR_VARR_H */\n"
        },
        {
          "name": "mir-x86_64.c",
          "type": "blob",
          "size": 43.0322265625,
          "content": "/* This file is a part of MIR project.\n   Copyright (C) 2018-2024 Vladimir Makarov <vmakarov.gcc@gmail.com>.\n*/\n\n#include \"mir-x86_64.h\"\n\n/* RBLK args are always passed by address.\n   BLK0 first is copied on the caller stack and passed implicitly.\n   BLK1 is passed in general regs\n   BLK2 is passed in fp regs\n   BLK3 is passed in gpr and then fpr\n   BLK4 is passed in fpr and then gpr\n   If there are no enough regs, they work as BLK.\n   Windows: small BLKs (<= 8 bytes) are passed by value;\n            all other BLKs is always passed by pointer as regular int arg.  */\n\n#define VA_LIST_IS_ARRAY_P 1\n\nvoid *_MIR_get_bstart_builtin (MIR_context_t ctx) {\n  static const uint8_t bstart_code[] = {\n    0x48, 0x8d, 0x44, 0x24, 0x08, /* rax = rsp + 8 (lea) */\n    0xc3,                         /* ret */\n  };\n  return _MIR_publish_code (ctx, bstart_code, sizeof (bstart_code));\n}\nvoid *_MIR_get_bend_builtin (MIR_context_t ctx) {\n  static const uint8_t bend_code[] = {\n#ifndef _WIN32\n    0x48, 0x8b, 0x04, 0x24, /* rax = (rsp) */\n    0x48, 0x89, 0xfc,       /* rsp = rdi */\n    0xff, 0xe0,             /* jmp *rax */\n#else\n    0x48, 0x8b, 0x04, 0x24, /* rax = (rsp) */\n    0x48, 0x89, 0xcc,       /* rsp = rcx */\n    0xff, 0xe0,             /* jmp *rax */\n#endif\n  };\n  return _MIR_publish_code (ctx, bend_code, sizeof (bend_code));\n}\n\n#ifndef _WIN32\nstruct x86_64_va_list {\n  uint32_t gp_offset, fp_offset;\n  uint64_t *overflow_arg_area, *reg_save_area;\n};\n\nvoid *va_arg_builtin (void *p, uint64_t t) {\n  struct x86_64_va_list *va = p;\n  MIR_type_t type = t;\n  int fp_p = type == MIR_T_F || type == MIR_T_D;\n  void *a;\n\n  if (fp_p && va->fp_offset <= 160) {\n    a = (char *) va->reg_save_area + va->fp_offset;\n    va->fp_offset += 16;\n  } else if (!fp_p && type != MIR_T_LD && va->gp_offset <= 40) {\n    a = (char *) va->reg_save_area + va->gp_offset;\n    va->gp_offset += 8;\n  } else {\n    a = va->overflow_arg_area;\n    va->overflow_arg_area += type == MIR_T_LD ? 2 : 1;\n  }\n  return a;\n}\n\nvoid va_block_arg_builtin (void *res, void *p, size_t s, uint64_t ncase) {\n  struct x86_64_va_list *va = p;\n  size_t size = ((s + 7) / 8) * 8;\n  void *a = va->overflow_arg_area;\n  union {\n    uint64_t i;\n    double d;\n  } u[2];\n\n  switch (ncase) {\n  case 1:\n    if (va->gp_offset + size > 48) break;\n    u[0].i = *(uint64_t *) ((char *) va->reg_save_area + va->gp_offset);\n    va->gp_offset += 8;\n    if (size > 8) {\n      u[1].i = *(uint64_t *) ((char *) va->reg_save_area + va->gp_offset);\n      va->gp_offset += 8;\n    }\n    if (res != NULL) memcpy (res, &u, s);\n    return;\n  case 2:\n    u[0].d = *(double *) ((char *) va->reg_save_area + va->fp_offset);\n    va->fp_offset += 16;\n    if (size > 8) {\n      u[1].d = *(double *) ((char *) va->reg_save_area + va->fp_offset);\n      va->fp_offset += 16;\n    }\n    if (res != NULL) memcpy (res, &u, s);\n    return;\n  case 3:\n  case 4:\n    if (va->fp_offset > 160 || va->gp_offset > 40) break;\n    if (ncase == 3) {\n      u[0].i = *(uint64_t *) ((char *) va->reg_save_area + va->gp_offset);\n      u[1].d = *(double *) ((char *) va->reg_save_area + va->fp_offset);\n    } else {\n      u[0].d = *(double *) ((char *) va->reg_save_area + va->fp_offset);\n      u[1].i = *(uint64_t *) ((char *) va->reg_save_area + va->gp_offset);\n    }\n    va->fp_offset += 8;\n    va->gp_offset += 8;\n    if (res != NULL) memcpy (res, &u, s);\n    return;\n  default: break;\n  }\n  if (res != NULL) memcpy (res, a, s);\n  va->overflow_arg_area += size / 8;\n}\n\nvoid va_start_interp_builtin (MIR_context_t ctx MIR_UNUSED, void *p, void *a) {\n  struct x86_64_va_list *va = p;\n  va_list *vap = a;\n\n  assert (sizeof (struct x86_64_va_list) == sizeof (va_list));\n  *va = *(struct x86_64_va_list *) vap;\n}\n\n#else\n\nstruct x86_64_va_list {\n  uint64_t *arg_area;\n};\n\nvoid *va_arg_builtin (void *p, uint64_t t) {\n  struct x86_64_va_list *va = p;\n  void *a = va->arg_area;\n  va->arg_area++;\n  return a;\n}\n\nvoid va_block_arg_builtin (void *res, void *p, size_t s, uint64_t ncase) {\n  struct x86_64_va_list *va = p;\n  void *a = s <= 8 ? va->arg_area : *(void **) va->arg_area; /* pass by pointer */\n  if (res != NULL) memcpy (res, a, s);\n  va->arg_area++;\n}\n\nvoid va_start_interp_builtin (MIR_context_t ctx, void *p, void *a) {\n  struct x86_64_va_list **va = p;\n  va_list *vap = a;\n\n  assert (sizeof (struct x86_64_va_list) == sizeof (va_list));\n  *va = (struct x86_64_va_list *) vap;\n}\n\n#endif\n\nvoid va_end_interp_builtin (MIR_context_t ctx MIR_UNUSED, void *p MIR_UNUSED) {}\n\nstatic const uint8_t short_jmp_pattern[] = {\n  0xe9, 0, 0, 0, 0,         /* 0x0: jmp rel32 */\n  0,    0, 0, 0, 0, 0, 0, 0 /* 0x5: abs address holder */\n};\nstatic const uint8_t long_jmp_pattern[] = {\n  0x49, 0xbb, 0,    0, 0, 0, 0, 0, 0, 0, /* 0x0: movabsq 0, r11 */\n  0x41, 0xff, 0xe3,                      /* 0xa: jmpq   *%r11 */\n};\n\n/* r11=<address to go to>; jump *r11  */\nvoid *_MIR_get_thunk (MIR_context_t ctx) {\n  void *res;\n  assert (sizeof (short_jmp_pattern) == sizeof (long_jmp_pattern));\n  res = _MIR_publish_code (ctx, short_jmp_pattern, sizeof (short_jmp_pattern));\n  return res;\n}\n\nvoid *_MIR_get_thunk_addr (MIR_context_t ctx MIR_UNUSED, void *thunk) {\n  void *addr;\n  int short_p = *(unsigned char *) thunk == 0xe9;\n  memcpy ((char *) &addr, (char *) thunk + (short_p ? 5 : 2), sizeof (addr));\n  return addr;\n}\n\nvoid _MIR_redirect_thunk (MIR_context_t ctx, void *thunk, void *to) {\n  int64_t disp = (char *) to - ((char *) thunk + 5);\n  int short_p = INT32_MIN <= disp && disp <= INT32_MAX;\n  uint8_t pattern[sizeof (short_jmp_pattern)];\n  if (short_p) {\n    memcpy (pattern, short_jmp_pattern, sizeof (short_jmp_pattern));\n    memcpy (pattern + 1, &disp, 4); /* little endian */\n    memcpy (pattern + 5, &to, 8);\n  } else {\n    memcpy (pattern, long_jmp_pattern, sizeof (long_jmp_pattern));\n    memcpy (pattern + 2, &to, 8);\n  }\n  _MIR_change_code (ctx, thunk, pattern, sizeof (short_jmp_pattern));\n}\n\nstatic const uint8_t save_pat[] = {\n#ifndef _WIN32\n  0x48, 0x81, 0xec, 0x80, 0,    0,    0, /*sub    $0x88,%rsp\t\t   */\n  0xf3, 0x0f, 0x7f, 0x04, 0x24,          /*movdqu %xmm0,(%rsp)\t\t   */\n  0xf3, 0x0f, 0x7f, 0x4c, 0x24, 0x10,    /*movdqu %xmm1,0x10(%rsp)\t   */\n  0xf3, 0x0f, 0x7f, 0x54, 0x24, 0x20,    /*movdqu %xmm2,0x20(%rsp)\t   */\n  0xf3, 0x0f, 0x7f, 0x5c, 0x24, 0x30,    /*movdqu %xmm3,0x30(%rsp)\t   */\n  0xf3, 0x0f, 0x7f, 0x64, 0x24, 0x40,    /*movdqu %xmm4,0x40(%rsp)\t   */\n  0xf3, 0x0f, 0x7f, 0x6c, 0x24, 0x50,    /*movdqu %xmm5,0x50(%rsp)\t   */\n  0xf3, 0x0f, 0x7f, 0x74, 0x24, 0x60,    /*movdqu %xmm6,0x60(%rsp)\t   */\n  0xf3, 0x0f, 0x7f, 0x7c, 0x24, 0x70,    /*movdqu %xmm7,0x70(%rsp)\t   */\n  0x41, 0x51,                            /*push   %r9\t\t\t   */\n  0x41, 0x50,                            /*push   %r8\t\t\t   */\n  0x51,                                  /*push   %rcx\t\t\t   */\n  0x52,                                  /*push   %rdx\t\t\t   */\n  0x56,                                  /*push   %rsi\t\t\t   */\n  0x57,                                  /*push   %rdi\t\t\t   */\n#else\n  0x48, 0x89, 0x4c, 0x24, 0x08, /*mov  %rcx,0x08(%rsp) */\n  0x48, 0x89, 0x54, 0x24, 0x10, /*mov  %rdx,0x10(%rsp) */\n  0x4c, 0x89, 0x44, 0x24, 0x18, /*mov  %r8, 0x18(%rsp) */\n  0x4c, 0x89, 0x4c, 0x24, 0x20, /*mov  %r9, 0x20(%rsp) */\n#endif\n};\n\nstatic const uint8_t restore_pat[] = {\n#ifndef _WIN32\n  0x5f,                                  /*pop    %rdi\t\t\t   */\n  0x5e,                                  /*pop    %rsi\t\t\t   */\n  0x5a,                                  /*pop    %rdx\t\t\t   */\n  0x59,                                  /*pop    %rcx\t\t\t   */\n  0x41, 0x58,                            /*pop    %r8\t\t\t   */\n  0x41, 0x59,                            /*pop    %r9\t\t\t   */\n  0xf3, 0x0f, 0x6f, 0x04, 0x24,          /*movdqu (%rsp),%xmm0\t\t   */\n  0xf3, 0x0f, 0x6f, 0x4c, 0x24, 0x10,    /*movdqu 0x10(%rsp),%xmm1\t   */\n  0xf3, 0x0f, 0x6f, 0x54, 0x24, 0x20,    /*movdqu 0x20(%rsp),%xmm2\t   */\n  0xf3, 0x0f, 0x6f, 0x5c, 0x24, 0x30,    /*movdqu 0x30(%rsp),%xmm3\t   */\n  0xf3, 0x0f, 0x6f, 0x64, 0x24, 0x40,    /*movdqu 0x40(%rsp),%xmm4\t   */\n  0xf3, 0x0f, 0x6f, 0x6c, 0x24, 0x50,    /*movdqu 0x50(%rsp),%xmm5\t   */\n  0xf3, 0x0f, 0x6f, 0x74, 0x24, 0x60,    /*movdqu 0x60(%rsp),%xmm6\t   */\n  0xf3, 0x0f, 0x6f, 0x7c, 0x24, 0x70,    /*movdqu 0x70(%rsp),%xmm7\t   */\n  0x48, 0x81, 0xc4, 0x80, 0,    0,    0, /*add    $0x80,%rsp\t\t   */\n#else\n  0x48, 0x8b, 0x4c, 0x24, 0x08,       /*mov  0x08(%rsp),%rcx */\n  0x48, 0x8b, 0x54, 0x24, 0x10,       /*mov  0x10(%rsp),%rdx */\n  0x4c, 0x8b, 0x44, 0x24, 0x18,       /*mov  0x18(%rsp),%r8  */\n  0x4c, 0x8b, 0x4c, 0x24, 0x20,       /*mov  0x20(%rsp),%r9  */\n  0xf3, 0x0f, 0x7e, 0x44, 0x24, 0x08, /*movq 0x08(%rsp),%xmm0*/\n  0xf3, 0x0f, 0x7e, 0x4c, 0x24, 0x10, /*movq 0x10(%rsp),%xmm1*/\n  0xf3, 0x0f, 0x7e, 0x54, 0x24, 0x18, /*movq 0x18(%rsp),%xmm2*/\n  0xf3, 0x0f, 0x7e, 0x5c, 0x24, 0x20, /*movq 0x20(%rsp),%xmm3*/\n#endif\n};\n\nstatic uint8_t *push_insns (VARR (uint8_t) * insn_varr, const uint8_t *pat, size_t pat_len) {\n  for (size_t i = 0; i < pat_len; i++) VARR_PUSH (uint8_t, insn_varr, pat[i]);\n  return VARR_ADDR (uint8_t, insn_varr) + VARR_LENGTH (uint8_t, insn_varr) - pat_len;\n}\n\nstatic void gen_mov (VARR (uint8_t) * insn_varr, uint32_t offset, uint32_t reg, int ld_p) {\n  static const uint8_t ld_gp_reg[] = {0x48, 0x8b, 0x83, 0, 0, 0, 0 /* mov <offset>(%rbx),%reg */};\n  static const uint8_t st_gp_reg[] = {0x48, 0x89, 0x83, 0, 0, 0, 0 /* mov %reg,<offset>(%rbx) */};\n  uint8_t *addr = push_insns (insn_varr, ld_p ? ld_gp_reg : st_gp_reg,\n                              ld_p ? sizeof (ld_gp_reg) : sizeof (st_gp_reg));\n  memcpy (addr + 3, &offset, sizeof (uint32_t));\n  assert (reg <= 15);\n  addr[0] |= (reg >> 1) & 4;\n  addr[2] |= (reg & 7) << 3;\n}\n\nstatic void gen_mov2 (VARR (uint8_t) * insn_varr, uint32_t offset, uint32_t reg, int ld_p) {\n  static const uint8_t ld_gp_reg[] = {0x49, 0x8b, 0x44, 0x24, 0 /* mov <offset>(%r12),%reg */};\n  static const uint8_t st_gp_reg[] = {0x49, 0x89, 0x44, 0x24, 0 /* mov %reg,<offset>(%r12) */};\n  uint8_t *addr = push_insns (insn_varr, ld_p ? ld_gp_reg : st_gp_reg,\n                              ld_p ? sizeof (ld_gp_reg) : sizeof (st_gp_reg));\n  addr[4] = offset;\n  assert (reg <= 15);\n  addr[0] |= (reg >> 1) & 4;\n  addr[2] |= (reg & 7) << 3;\n}\n\nstatic void gen_blk_mov (VARR (uint8_t) * insn_varr, uint32_t offset, uint32_t addr_offset,\n                         uint32_t qwords) {\n  static const uint8_t blk_mov_pat[] = {\n    /*0:*/ 0x4c,  0x8b, 0xa3, 0,    0, 0, 0,    /*mov <addr_offset>(%rbx),%r12*/\n    /*7:*/ 0x48,  0xc7, 0xc0, 0,    0, 0, 0,    /*mov <qwords>,%rax*/\n    /*e:*/ 0x48,  0x83, 0xe8, 0x01,             /*sub $0x1,%rax*/\n    /*12:*/ 0x4d, 0x8b, 0x14, 0xc4,             /*mov (%r12,%rax,8),%r10*/\n    /*16:*/ 0x4c, 0x89, 0x94, 0xc4, 0, 0, 0, 0, /*mov %r10,<offset>(%rsp,%rax,8)*/\n    /*1e:*/ 0x48, 0x85, 0xc0,                   /*test %rax,%rax*/\n    /*21:*/ 0x7f, 0xeb,                         /*jg e <L0>*/\n  };\n  uint8_t *addr = push_insns (insn_varr, blk_mov_pat, sizeof (blk_mov_pat));\n  memcpy (addr + 3, &addr_offset, sizeof (uint32_t));\n  memcpy (addr + 10, &qwords, sizeof (uint32_t));\n  memcpy (addr + 26, &offset, sizeof (uint32_t));\n}\n\nstatic void gen_movxmm (VARR (uint8_t) * insn_varr, uint32_t offset, uint32_t reg, int b32_p,\n                        int ld_p) {\n  static const uint8_t ld_xmm_reg_pat[] = {\n    0xf2, 0x0f, 0x10, 0x83, 0, 0, 0, 0 /* movs[sd] <offset>(%rbx),%xmm */\n  };\n  static const uint8_t st_xmm_reg_pat[] = {\n    0xf2, 0x0f, 0x11, 0x83, 0, 0, 0, 0 /* movs[sd] %xmm, <offset>(%rbx) */\n  };\n  uint8_t *addr = push_insns (insn_varr, ld_p ? ld_xmm_reg_pat : st_xmm_reg_pat,\n                              ld_p ? sizeof (ld_xmm_reg_pat) : sizeof (st_xmm_reg_pat));\n  memcpy (addr + 4, &offset, sizeof (uint32_t));\n  assert (reg <= 7);\n  addr[3] |= reg << 3;\n  if (b32_p) addr[0] |= 1;\n}\n\nstatic void gen_movxmm2 (VARR (uint8_t) * insn_varr, uint32_t offset, uint32_t reg, int ld_p) {\n  static const uint8_t ld_xmm_reg_pat[] = {\n    0xf2, 0x41, 0x0f, 0x10, 0x44, 0x24, 0 /* movsd <offset>(%r12),%xmm */\n  };\n  static const uint8_t st_xmm_reg_pat[] = {\n    0xf2, 0x41, 0x0f, 0x11, 0x44, 0x24, 0 /* movsd %xmm, <offset>(%r12) */\n  };\n  uint8_t *addr = push_insns (insn_varr, ld_p ? ld_xmm_reg_pat : st_xmm_reg_pat,\n                              ld_p ? sizeof (ld_xmm_reg_pat) : sizeof (st_xmm_reg_pat));\n  addr[6] = offset;\n  assert (reg <= 7);\n  addr[4] |= reg << 3;\n}\n\n#ifdef _WIN32\nstatic void gen_add (VARR (uint8_t) * insn_varr, uint32_t sp_offset, int reg) {\n  static const uint8_t lea_pat[] = {\n    0x48, 0x8d, 0x84, 0x24, 0, 0, 0, 0, /* lea    <sp_offset>(%sp),reg */\n  };\n  uint8_t *addr = push_insns (insn_varr, lea_pat, sizeof (lea_pat));\n  memcpy (addr + 4, &sp_offset, sizeof (uint32_t));\n  addr[2] |= (reg & 7) << 3;\n  if (reg > 7) addr[0] |= 4;\n}\n#endif\n\nstatic void gen_st (VARR (uint8_t) * insn_varr, uint32_t sp_offset, int b64_p) {\n  static const uint8_t st_pat[] = {\n    0x44, 0x89, 0x94, 0x24, 0, 0, 0, 0, /* mov    %r10,<sp_offset>(%sp) */\n  };\n  uint8_t *addr = push_insns (insn_varr, st_pat, sizeof (st_pat));\n  memcpy (addr + 4, &sp_offset, sizeof (uint32_t));\n  if (b64_p) addr[0] |= 8;\n}\n\nstatic void gen_ldst (VARR (uint8_t) * insn_varr, uint32_t sp_offset, uint32_t src_offset,\n                      int b64_p) {\n  static const uint8_t ld_pat[] = {\n    0x44, 0x8b, 0x93, 0, 0, 0, 0, /* mov    <src_offset>(%rbx),%r10 */\n  };\n  uint8_t *addr = push_insns (insn_varr, ld_pat, sizeof (ld_pat));\n  memcpy (addr + 3, &src_offset, sizeof (uint32_t));\n  if (b64_p) addr[0] |= 8;\n  gen_st (insn_varr, sp_offset, b64_p);\n}\n\nstatic void gen_ldst80 (VARR (uint8_t) * insn_varr, uint32_t sp_offset, uint32_t src_offset) {\n  static uint8_t const ldst80_pat[] = {\n    0xdb, 0xab, 0,    0, 0, 0,    /* fldt   <src_offset>(%rbx) */\n    0xdb, 0xbc, 0x24, 0, 0, 0, 0, /* fstpt  <sp_offset>(%sp) */\n  };\n  uint8_t *addr = push_insns (insn_varr, ldst80_pat, sizeof (ldst80_pat));\n  memcpy (addr + 2, &src_offset, sizeof (uint32_t));\n  memcpy (addr + 9, &sp_offset, sizeof (uint32_t));\n}\n\nstatic void gen_st80 (VARR (uint8_t) * insn_varr, uint32_t src_offset) {\n  static const uint8_t st80_pat[] = {0xdb, 0xbb, 0, 0, 0, 0 /* fstpt   <src_offset>(%rbx) */};\n  memcpy (push_insns (insn_varr, st80_pat, sizeof (st80_pat)) + 2, &src_offset, sizeof (uint32_t));\n}\n\n/* Generation: fun (fun_addr, res_arg_addresses):\n   push r12, push rbx; sp-=sp_offset; r11=fun_addr; rbx=res/arg_addrs\n   r10=mem[rbx,<offset>]; (arg_reg=mem[r10] or r10=mem[r10];mem[sp,sp_offset]=r10\n                           or r12=mem[rbx,arg_offset]; arg_reg=mem[r12]\n                                                       [;(arg_reg + 1)=mem[r12 + 8]]\n                           ...\n                           or r12=mem[rbx,arg_offset];rax=qwords;\n                              L:rax-=1;r10=mem[r12,rax]; mem[sp,sp_offset,rax]=r10;\n                                goto L if rax > 0) ...\n   rax=8; call *r11; sp+=offset\n   r10=mem[rbx,<offset>]; res_reg=mem[r10]; ...\n   pop rbx; pop r12; ret. */\nvoid *_MIR_get_ff_call (MIR_context_t ctx, size_t nres, MIR_type_t *res_types, size_t nargs,\n                        _MIR_arg_desc_t *arg_descs, size_t arg_vars_num MIR_UNUSED) {\n  static const uint8_t prolog[] = {\n#ifndef _WIN32\n    0x41, 0x54,                   /* pushq %r12 */\n    0x53,                         /* pushq %rbx */\n    0x48, 0x81, 0xec, 0, 0, 0, 0, /* subq <sp_offset>, %rsp */\n    0x49, 0x89, 0xfb,             /* mov $rdi, $r11 -- fun addr */\n    0x48, 0x89, 0xf3,             /* mov $rsi, $rbx -- result/arg addresses */\n#else\n    /* 0x0: */ 0x41,  0x54,                   /* pushq %r12 */\n    /* 0x2: */ 0x53,                          /* pushq %rbx */\n    /* 0x3: */ 0x55,                          /* push %rbp */\n    /* 0x4: */ 0x48,  0x89, 0xe5,             /* mov %rsp,%rbp */\n    /* 0x7: */ 0x48,  0x81, 0xec, 0, 0, 0, 0, /* subq <sp_offset>, %rsp */\n    /* 0xe: */ 0x49,  0x89, 0xcb,             /* mov $rcx, $r11 -- fun addr */\n    /* 0x11: */ 0x48, 0x89, 0xd3,             /* mov $rdx, $rbx -- result/arg addresses */\n#endif\n  };\n  static const uint8_t call_end[] = {\n#ifndef _WIN32\n    0x48, 0xc7, 0xc0, 0x08, 0, 0, 0, /* mov $8, rax -- to save xmm varargs */\n#endif\n    0x41, 0xff, 0xd3, /* callq  *%r11\t   */\n#ifndef _WIN32\n    0x48, 0x81, 0xc4, 0,    0, 0, 0, /* addq <sp_offset>, %rsp */\n#endif\n  };\n  static const uint8_t epilog[] = {\n#ifdef _WIN32              /* Strict form of windows epilogue for unwinding: */\n    0x48, 0x8d, 0x65, 0x0, /* lea  0x0(%rbp),%rsp */\n    0x5d,                  /* pop %rbp */\n#endif\n    0x5b,       /* pop %rbx */\n    0x41, 0x5c, /* pop %r12 */\n    0xc3,       /* ret */\n  };\n#ifndef _WIN32\n  static const uint8_t iregs[] = {7, 6, 2, 1, 8, 9}; /* rdi, rsi, rdx, rcx, r8, r9 */\n  static const uint32_t max_iregs = 6, max_xregs = 8;\n  uint32_t sp_offset = 0;\n#else\n  static const uint8_t iregs[] = {1, 2, 8, 9}; /* rcx, rdx, r8, r9 */\n  static const uint32_t max_iregs = 4, max_xregs = 4;\n  uint32_t blk_offset = nargs < 4 ? 32 : (uint32_t) nargs * 8, sp_offset = 32; /* spill area */\n#endif\n  uint32_t n_iregs = 0, n_xregs = 0, n_fregs, qwords;\n  uint8_t *addr;\n  VARR (uint8_t) * code;\n  void *res;\n\n  VARR_CREATE (uint8_t, code, ctx->alloc, 128);\n  push_insns (code, prolog, sizeof (prolog));\n  for (size_t i = 0; i < nargs; i++) {\n    MIR_type_t type = arg_descs[i].type;\n\n    if ((MIR_T_I8 <= type && type <= MIR_T_U64) || type == MIR_T_P || type == MIR_T_RBLK) {\n      if (n_iregs < max_iregs) {\n        gen_mov (code, (uint32_t) ((i + nres) * sizeof (long double)), iregs[n_iregs++], TRUE);\n#ifdef _WIN32\n        n_xregs++;\n#endif\n      } else {\n        gen_ldst (code, sp_offset, (uint32_t) ((i + nres) * sizeof (long double)), TRUE);\n        sp_offset += 8;\n      }\n    } else if (type == MIR_T_F || type == MIR_T_D) {\n      if (n_xregs < max_xregs) {\n        gen_movxmm (code, (uint32_t) ((i + nres) * sizeof (long double)), n_xregs++,\n                    type == MIR_T_F, TRUE);\n#ifdef _WIN32\n        gen_mov (code, (uint32_t) ((i + nres) * sizeof (long double)), iregs[n_iregs++], TRUE);\n#endif\n      } else {\n        gen_ldst (code, sp_offset, (uint32_t) ((i + nres) * sizeof (long double)), type == MIR_T_D);\n        sp_offset += 8;\n      }\n    } else if (type == MIR_T_LD) {\n      gen_ldst80 (code, sp_offset, (uint32_t) ((i + nres) * sizeof (long double)));\n      sp_offset += 16;\n    } else if (MIR_blk_type_p (type)) {\n      qwords = (uint32_t) ((arg_descs[i].size + 7) / 8);\n#ifndef _WIN32\n      if (type == MIR_T_BLK + 1 && n_iregs + qwords <= max_iregs) {\n        assert (qwords <= 2);\n        gen_mov (code, (i + nres) * sizeof (long double), 12, TRUE);   /* r12 = block addr */\n        gen_mov2 (code, 0, iregs[n_iregs], TRUE);                      /* arg_reg = mem[r12] */\n        if (qwords == 2) gen_mov2 (code, 8, iregs[n_iregs + 1], TRUE); /* arg_reg = mem[r12 + 8] */\n        n_iregs += qwords;\n        n_xregs += qwords;\n        continue;\n      } else if (type == MIR_T_BLK + 2 && n_xregs + qwords <= max_xregs) {\n        assert (qwords <= 2);\n        gen_mov (code, (i + nres) * sizeof (long double), 12, TRUE); /* r12 = block addr */\n        gen_movxmm2 (code, 0, n_xregs, TRUE);                        /* xmm = mem[r12] */\n        if (qwords == 2) gen_movxmm2 (code, 8, n_xregs + 1, TRUE);   /* xmm = mem[r12 +  8] */\n        n_xregs += qwords;\n        continue;\n      } else if (type == MIR_T_BLK + 3 && n_iregs < max_iregs && n_xregs < max_xregs) {\n        assert (qwords == 2);\n        gen_mov (code, (i + nres) * sizeof (long double), 12, TRUE); /* r12 = block addr */\n        gen_mov2 (code, 0, iregs[n_iregs], TRUE);                    /* arg_reg = mem[r12] */\n        n_iregs++;\n        n_xregs++;\n        gen_movxmm2 (code, 8, n_xregs, TRUE); /* xmm = mem[r12 + 8] */\n        n_xregs++;\n        continue;\n      } else if (type == MIR_T_BLK + 4 && n_iregs < max_iregs && n_xregs < max_xregs) {\n        assert (qwords == 2);\n        gen_mov (code, (i + nres) * sizeof (long double), 12, TRUE); /* r12 = block addr */\n        gen_movxmm2 (code, 0, n_xregs, TRUE);                        /* xmm = mem[r12] */\n        n_xregs++;\n        gen_mov2 (code, 8, iregs[n_iregs], TRUE); /* arg_reg = mem[r12 + 8] */\n        n_iregs++;\n        n_xregs++;\n        continue;\n      }\n      gen_blk_mov (code, sp_offset, (i + nres) * sizeof (long double), qwords);\n      sp_offset += qwords * 8;\n#else\n      if (qwords <= 1) {\n        gen_mov (code, (uint32_t) ((i + nres) * sizeof (long double)), 12,\n                 TRUE); /* r12 = mem[disp + rbx] */\n        if (n_iregs < max_iregs) {\n          gen_mov2 (code, 0, iregs[n_iregs++], TRUE); /* arg_reg = mem[r12] */\n          n_xregs++;\n        } else {\n          gen_mov2 (code, 0, 10, TRUE);   /* r10 = mem[r12] */\n          gen_st (code, sp_offset, TRUE); /* mem[sp+sp_offset] = r10; */\n          sp_offset += 8;\n        }\n      } else {\n        /* r12 = mem[disp + rbx]; mem[rsp+blk_offset + nw] = r10 = mem[r12 + nw]; */\n        gen_blk_mov (code, blk_offset, (uint32_t) ((i + nres) * sizeof (long double)), qwords);\n        if (n_iregs < max_iregs) {\n          gen_add (code, blk_offset, iregs[n_iregs++]); /* arg_reg = sp + blk_offset */\n          n_xregs++;\n        } else {\n          gen_add (code, blk_offset, 10); /* r10 = sp + blk_offset */\n          gen_st (code, sp_offset, TRUE); /* mem[sp+sp_offset] = r10; */\n          sp_offset += 8;\n        }\n        blk_offset += qwords * 8;\n      }\n#endif\n    } else {\n      MIR_get_error_func (ctx) (MIR_call_op_error, \"wrong type of arg value\");\n    }\n  }\n#ifdef _WIN32\n  if (blk_offset > sp_offset) sp_offset = blk_offset;\n#endif\n  sp_offset = (sp_offset + 15) / 16 * 16;\n#ifndef _WIN32\n  sp_offset += 8; /* align */\n#endif\n  addr = VARR_ADDR (uint8_t, code);\n#ifndef _WIN32\n  memcpy (addr + 6, &sp_offset, sizeof (uint32_t));\n#else\n  memcpy (addr + 10, &sp_offset, sizeof (uint32_t));\n#endif\n  addr = push_insns (code, call_end, sizeof (call_end));\n#ifndef _WIN32\n  memcpy (addr + sizeof (call_end) - 4, &sp_offset, sizeof (uint32_t));\n#else\n  if (nres > 1)\n    MIR_get_error_func (ctx) (MIR_call_op_error,\n                              \"Windows x86-64 doesn't support multiple return values\");\n#endif\n  n_iregs = n_xregs = n_fregs = 0;\n  for (size_t i = 0; i < nres; i++) {\n    if (((MIR_T_I8 <= res_types[i] && res_types[i] <= MIR_T_U64) || res_types[i] == MIR_T_P)\n        && n_iregs < 2) {\n      gen_mov (code, (uint32_t) (i * sizeof (long double)), n_iregs++ == 0 ? 0 : 2,\n               FALSE); /* rax or rdx */\n    } else if ((res_types[i] == MIR_T_F || res_types[i] == MIR_T_D) && n_xregs < 2) {\n      gen_movxmm (code, (uint32_t) (i * sizeof (long double)), n_xregs++, res_types[i] == MIR_T_F,\n                  FALSE);\n    } else if (res_types[i] == MIR_T_LD && n_fregs < 2) {\n      gen_st80 (code, (uint32_t) (i * sizeof (long double)));\n    } else {\n      MIR_get_error_func (ctx) (MIR_ret_error,\n                                \"x86-64 can not handle this combination of return values\");\n    }\n  }\n  push_insns (code, epilog, sizeof (epilog));\n  res = _MIR_publish_code (ctx, VARR_ADDR (uint8_t, code), VARR_LENGTH (uint8_t, code));\n  VARR_DESTROY (uint8_t, code);\n  return res;\n}\n\n/* Transform C call to call of void handler (MIR_context_t ctx, MIR_item_t func_item,\n                                             va_list va, MIR_val_t *results) */\nvoid *_MIR_get_interp_shim (MIR_context_t ctx, MIR_item_t func_item, void *handler) {\n  static const uint8_t push_rbx[] = {0x53, /*push   %rbx  */};\n  static const uint8_t prepare_pat[] = {\n#ifndef _WIN32\n    /*  0: */ 0x48, 0x83, 0xec, 0x20,                      /* sub    32,%rsp\t     */\n    /*  4: */ 0x48, 0x89, 0xe2,                            /* mov    %rsp,%rdx\t     */\n    /*  7: */ 0xc7, 0x02, 0,    0,    0,    0,             /* movl   0,(%rdx)\t     */\n    /*  d: */ 0xc7, 0x42, 0x04, 0x30, 0,    0, 0,          /* movl   48, 4(%rdx)     */\n    /* 14: */ 0x48, 0x8d, 0x44, 0x24, 0x20,                /* lea    32(%rsp),%rax   */\n    /* 19: */ 0x48, 0x89, 0x42, 0x10,                      /* mov    %rax,16(%rdx)   */\n    /* 1d: */ 0x48, 0x8d, 0x84, 0x24, 0xe0, 0, 0, 0,       /* lea    224(%rsp),%rax  */\n    /* 25: */ 0x48, 0x89, 0x42, 0x08,                      /* mov    %rax,8(%rdx)    */\n    /* 29: */ 0x48, 0x81, 0xec, 0,    0,    0, 0,          /* sub    <n>,%rsp\t     */\n    /* 30: */ 0x48, 0x89, 0xe3,                            /* mov    %rsp,%rbx\t     */\n    /* 33: */ 0x48, 0x89, 0xe1,                            /* mov    %rsp,%rcx\t     */\n    /* 36: */ 0x48, 0xbf, 0,    0,    0,    0, 0, 0, 0, 0, /* movabs <ctx>,%rdi      */\n    /* 40: */ 0x48, 0xbe, 0,    0,    0,    0, 0, 0, 0, 0, /* movabs <func_item>,%rsi*/\n    /* 4a: */ 0x48, 0xb8, 0,    0,    0,    0, 0, 0, 0, 0, /* movabs <handler>,%rax  */\n    /* 54: */ 0xff, 0xd0,                                  /* callq  *%rax           */\n  };\n  static const uint32_t nres_offset = 0x2c;\n  static const uint32_t ctx_offset = 0x38;\n  static const uint32_t func_offset = 0x42;\n  static const uint32_t hndl_offset = 0x4c;\n  static const uint32_t prep_stack_size = 208;\n#else\n    /*  0: */ 0x53,                                        /* push   %rbx            */\n    /*  1: */ 0x55,                                        /* push %rbp */\n    /*  2: */ 0x48, 0x89, 0xe5,                            /* mov %rsp,%rbp */\n    /*  5: */ 0x4c, 0x8d, 0x44, 0x24, 0x18,                /* lea    24(%rsp),%r8     */\n    /*  a: */ 0x48, 0x81, 0xec, 0,    0,    0, 0,          /* sub    <n>,%rsp        */\n    /* 11: */ 0x48, 0x89, 0xe3,                            /* mov    %rsp,%rbx       */\n    /* 14: */ 0x49, 0x89, 0xe1,                            /* mov    %rsp,%r9        */\n    /* 17: */ 0x48, 0x83, 0xec, 0x20,                      /* sub    32,%rsp         */\n    /* 1b: */ 0x48, 0xb9, 0,    0,    0,    0, 0, 0, 0, 0, /* movabs <ctx>,%rcx      */\n    /* 25: */ 0x48, 0xba, 0,    0,    0,    0, 0, 0, 0, 0, /* movabs <func_item>,%rdx*/\n    /* 2f: */ 0x48, 0xb8, 0,    0,    0,    0, 0, 0, 0, 0, /* movabs <handler>,%rax  */\n    /* 39: */ 0xff, 0xd0,                                  /* callq  *%rax           */\n  };\n  static const uint32_t nres_offset = 0x0d;\n  static const uint32_t ctx_offset = 0x1d;\n  static const uint32_t func_offset = 0x27;\n  static const uint32_t hndl_offset = 0x31;\n  static const uint32_t prep_stack_size = 32;\n#endif\n  static const uint8_t shim_end[] = {\n#ifndef _WIN32\n    /* 0: */ 0x48, 0x81, 0xc4, 0, 0, 0, 0, /*add    prep_stack_size+n,%rsp*/\n#else                                      /* Strict form of windows epilogue for unwinding: */\n    /* 0 */ 0x48,  0x8d, 0x65, 0x0, /* lea  0x0(%rbp),%rsp */\n    /* 4: */ 0x5d,                  /* pop %rbp */\n#endif\n    0x5b, /*pop                      %rbx*/\n    0xc3, /*retq                         */\n  };\n  static const uint8_t ld_pat[] = {0x48, 0x8b, 0x83, 0, 0, 0, 0}; /* mov <offset>(%rbx), %reg */\n  static const uint8_t movss_pat[]\n    = {0xf3, 0x0f, 0x10, 0x83, 0, 0, 0, 0}; /* movss <offset>(%rbx), %xmm[01] */\n  static const uint8_t movsd_pat[]\n    = {0xf2, 0x0f, 0x10, 0x83, 0, 0, 0, 0};                   /* movsd <offset>(%rbx), %xmm[01] */\n  static const uint8_t fldt_pat[] = {0xdb, 0xab, 0, 0, 0, 0}; /* fldt <offset>(%rbx) */\n  static const uint8_t fxch_pat[] = {0xd9, 0xc9};             /* fxch */\n  uint8_t *addr;\n  uint32_t imm, n_iregs, n_xregs, n_fregs, offset;\n  uint32_t nres = func_item->u.func->nres;\n  MIR_type_t *results = func_item->u.func->res_types;\n  VARR (uint8_t) * code;\n  void *res;\n\n  VARR_CREATE (uint8_t, code, ctx->alloc, 128);\n#ifndef _WIN32\n  push_insns (code, push_rbx, sizeof (push_rbx));\n#endif\n  push_insns (code, save_pat, sizeof (save_pat));\n  addr = push_insns (code, prepare_pat, sizeof (prepare_pat));\n  imm = nres * 16;\n#ifdef _WIN32\n  imm += 8; /*align */\n#endif\n  memcpy (addr + nres_offset, &imm, sizeof (uint32_t));\n  memcpy (addr + ctx_offset, &ctx, sizeof (void *));\n  memcpy (addr + func_offset, &func_item, sizeof (void *));\n  memcpy (addr + hndl_offset, &handler, sizeof (void *));\n  /* move results: */\n#ifdef _WIN32\n  if (nres > 1)\n    MIR_get_error_func (ctx) (MIR_call_op_error,\n                              \"Windows x86-64 doesn't support multiple return values\");\n#endif\n  n_iregs = n_xregs = n_fregs = offset = 0;\n  for (uint32_t i = 0; i < nres; i++) {\n    if (results[i] == MIR_T_F && n_xregs < 2) {\n      addr = push_insns (code, movss_pat, sizeof (movss_pat));\n      addr[3] |= n_xregs << 3;\n      memcpy (addr + 4, &offset, sizeof (uint32_t));\n      n_xregs++;\n    } else if (results[i] == MIR_T_D && n_xregs < 2) {\n      addr = push_insns (code, movsd_pat, sizeof (movsd_pat));\n      addr[3] |= n_xregs << 3;\n      memcpy (addr + 4, &offset, sizeof (uint32_t));\n      n_xregs++;\n    } else if (results[i] == MIR_T_LD && n_fregs < 2) {\n      addr = push_insns (code, fldt_pat, sizeof (fldt_pat));\n      memcpy (addr + 2, &offset, sizeof (uint32_t));\n      if (n_fregs == 1) push_insns (code, fxch_pat, sizeof (fxch_pat));\n      n_fregs++;\n    } else if (n_iregs < 2) {\n      addr = push_insns (code, ld_pat, sizeof (ld_pat));\n      addr[2] |= n_iregs << 4;\n      memcpy (addr + 3, &offset, sizeof (uint32_t));\n      n_iregs++;\n    } else {\n      MIR_get_error_func (ctx) (MIR_ret_error,\n                                \"x86-64 can not handle this combination of return values\");\n    }\n    offset += 16;\n  }\n  addr = push_insns (code, shim_end, sizeof (shim_end));\n#ifndef _WIN32\n  imm = prep_stack_size + nres * 16;\n  memcpy (addr + 3, &imm, sizeof (uint32_t));\n#endif\n  res = _MIR_publish_code (ctx, VARR_ADDR (uint8_t, code), VARR_LENGTH (uint8_t, code));\n  VARR_DESTROY (uint8_t, code);\n  return res;\n}\n\n/* push rsi,rdi;rsi=called_func,rdi=ctx;r10=hook_address;jmp wrapper_end; */\nvoid *_MIR_get_wrapper (MIR_context_t ctx, MIR_item_t called_func, void *hook_address) {\n#ifndef _WIN32\n  static const uint8_t start_pat[] = {\n    0x56,                               /* push   %rsi\t\t\t   */\n    0x57,                               /* push   %rdi\t\t\t   */\n    0x48, 0xbe, 0, 0, 0, 0, 0, 0, 0, 0, /* movabs called_func,%rsi  \t   */\n    0x48, 0xbf, 0, 0, 0, 0, 0, 0, 0, 0, /* movabs ctx,%rdi  \t   */\n    0x49, 0xba, 0, 0, 0, 0, 0, 0, 0, 0, /* movabs <hook_address>,%r10  \t   */\n    0xe9, 0,    0, 0, 0,                /* 0x0: jmp rel32 */\n  };\n  size_t call_func_offset = 4, ctx_offset = 14, hook_offset = 24, rel32_offset = 33;\n#else\n  static const uint8_t start_pat[] = {\n    0x48, 0x89, 0x4c, 0x24, 0x08,                /* mov  %rcx,0x08(%rsp) */\n    0x48, 0x89, 0x54, 0x24, 0x10,                /* mov  %rdx,0x10(%rsp) */\n    0x48, 0xba, 0,    0,    0,    0, 0, 0, 0, 0, /* movabs called_func,%rdx   */\n    0x48, 0xb9, 0,    0,    0,    0, 0, 0, 0, 0, /* movabs ctx,%rcx           */\n    0x49, 0xba, 0,    0,    0,    0, 0, 0, 0, 0, /* movabs <hook_address>,%r10*/\n    0xe9, 0,    0,    0,    0,                   /* 0x0: jmp rel32 */\n  };\n  size_t call_func_offset = 2, ctx_offset = 12, hook_offset = 22, rel32_offset = 31;\n#endif\n  uint8_t *addr;\n  VARR (uint8_t) * code;\n  void *res;\n\n  VARR_CREATE (uint8_t, code, ctx->alloc, 128);\n  addr = push_insns (code, start_pat, sizeof (start_pat));\n  memcpy (addr + call_func_offset, &called_func, sizeof (void *));\n  memcpy (addr + ctx_offset, &ctx, sizeof (void *));\n  memcpy (addr + hook_offset, &hook_address, sizeof (void *));\n  res = _MIR_publish_code (ctx, VARR_ADDR (uint8_t, code), VARR_LENGTH (uint8_t, code));\n  VARR_DESTROY (uint8_t, code);\n  int64_t off = (uint8_t *) wrapper_end_addr - ((uint8_t *) res + rel32_offset + 4);\n  assert (INT32_MIN <= off && off <= INT32_MAX);\n  _MIR_change_code (ctx, (uint8_t *) res + rel32_offset, (uint8_t *) &off, 4); /* LE */\n  return res;\n}\n\nvoid *_MIR_get_wrapper_end (MIR_context_t ctx) {\n#ifndef _WIN32\n  static const uint8_t wrap_end[] = {\n    0x50,                               /*push   %rax */\n    0x53,                               /*push   %rbx */\n    0x48, 0x89, 0xe0,                   /*mov    %rsp,%rax */\n    0x48, 0x89, 0xc3,                   /*mov    %rax,%rbx */\n    0x48, 0x83, 0xe0, 0x0f,             /*and    $0xf,%rax */\n    0x48, 0x05, 0x80, 0,    0,    0,    /*add    $0x80,%rax */\n    0x48, 0x29, 0xc4,                   /*sub    %rax,%rsp -- aligned now */\n    0xf3, 0x0f, 0x7f, 0x04, 0x24,       /*movdqu %xmm0,(%rsp)\t\t   */\n    0xf3, 0x0f, 0x7f, 0x4c, 0x24, 0x10, /*movdqu %xmm1,0x10(%rsp)\t   */\n    0xf3, 0x0f, 0x7f, 0x54, 0x24, 0x20, /*movdqu %xmm2,0x20(%rsp)\t   */\n    0xf3, 0x0f, 0x7f, 0x5c, 0x24, 0x30, /*movdqu %xmm3,0x30(%rsp)\t   */\n    0xf3, 0x0f, 0x7f, 0x64, 0x24, 0x40, /*movdqu %xmm4,0x40(%rsp)\t   */\n    0xf3, 0x0f, 0x7f, 0x6c, 0x24, 0x50, /*movdqu %xmm5,0x50(%rsp)\t   */\n    0xf3, 0x0f, 0x7f, 0x74, 0x24, 0x60, /*movdqu %xmm6,0x60(%rsp)\t   */\n    0xf3, 0x0f, 0x7f, 0x7c, 0x24, 0x70, /*movdqu %xmm7,0x70(%rsp)\t   */\n    0x41, 0x51,                         /*push   %r9\t\t\t   */\n    0x41, 0x50,                         /*push   %r8\t\t\t   */\n    0x51,                               /*push   %rcx\t\t\t   */\n    0x52,                               /*push   %rdx\t\t\t   */\n    0x41, 0xff, 0xd2,                   /*callq  *%r10\t\t\t   */\n    0x49, 0x89, 0xc2,                   /*mov %rax,%r10\t\t   */\n    0x5a,                               /*pop    %rdx\t\t\t   */\n    0x59,                               /*pop    %rcx\t\t\t   */\n    0x41, 0x58,                         /*pop    %r8\t\t\t   */\n    0x41, 0x59,                         /*pop    %r9\t\t\t   */\n    0xf3, 0x0f, 0x6f, 0x04, 0x24,       /*movdqu (%rsp),%xmm0\t\t   */\n    0xf3, 0x0f, 0x6f, 0x4c, 0x24, 0x10, /*movdqu 0x10(%rsp),%xmm1\t   */\n    0xf3, 0x0f, 0x6f, 0x54, 0x24, 0x20, /*movdqu 0x20(%rsp),%xmm2\t   */\n    0xf3, 0x0f, 0x6f, 0x5c, 0x24, 0x30, /*movdqu 0x30(%rsp),%xmm3\t   */\n    0xf3, 0x0f, 0x6f, 0x64, 0x24, 0x40, /*movdqu 0x40(%rsp),%xmm4\t   */\n    0xf3, 0x0f, 0x6f, 0x6c, 0x24, 0x50, /*movdqu 0x50(%rsp),%xmm5\t   */\n    0xf3, 0x0f, 0x6f, 0x74, 0x24, 0x60, /*movdqu 0x60(%rsp),%xmm6\t   */\n    0xf3, 0x0f, 0x6f, 0x7c, 0x24, 0x70, /*movdqu 0x70(%rsp),%xmm7\t   */\n    0x48, 0x89, 0xdc,                   /*mov    %rbx,%rsp */\n    0x5b,                               /*pop    %rbx */\n    0x58,                               /*pop    %rax */\n    0x5f,                               /*pop    %rdi\t\t\t   */\n    0x5e,                               /*pop    %rsi\t\t\t   */\n    0x41, 0xff, 0xe2,                   /*jmpq   *%r10\t\t\t   */\n  };\n#else\n  static const uint8_t wrap_end[] = {\n    0x4c, 0x89, 0x44, 0x24, 0x18,       /*mov  %r8, 0x18(%rsp) */\n    0x4c, 0x89, 0x4c, 0x24, 0x20,       /*mov  %r9, 0x20(%rsp) */\n    0x50,                               /*push %rax               */\n    0x55,                               /*push %rbp */\n    0x48, 0x89, 0xe5,                   /*mov %rsp,%rbp */\n    0x48, 0x89, 0xe0,                   /*mov    %rsp,%rax */\n    0x48, 0x83, 0xe0, 0x0f,             /*and    $0xf,%rax */\n    0x48, 0x05, 0x28, 0,    0,    0,    /*add    $0x40,%rax */\n    0x48, 0x29, 0xc4,                   /*sub    %rax,%rsp -- aligned now */\n    0x66, 0x0f, 0xd6, 0x04, 0x24,       /*movq   %xmm0,(%rsp) */\n    0x66, 0x0f, 0xd6, 0x4c, 0x24, 0x08, /*movq   %xmm1,0x8(%rsp) */\n    0x66, 0x0f, 0xd6, 0x54, 0x24, 0x10, /*movq   %xmm2,0x10(%rsp) */\n    0x66, 0x0f, 0xd6, 0x5c, 0x24, 0x18, /*movq   %xmm3,0x18(%rsp) */\n    0x41, 0xff, 0xd2,                   /*callq  *%r10              */\n    0x49, 0x89, 0xc2,                   /*mov    %rax,%r10          */\n    0xf3, 0x0f, 0x7e, 0x04, 0x24,       /*movq (%rsp),%xmm0*/\n    0xf3, 0x0f, 0x7e, 0x4c, 0x24, 0x08, /*movq 0x8(%rsp),%xmm1*/\n    0xf3, 0x0f, 0x7e, 0x54, 0x24, 0x10, /*movq 0x10(%rsp),%xmm2*/\n    0xf3, 0x0f, 0x7e, 0x5c, 0x24, 0x18, /*movq 0x18(%rsp),%xmm3*/\n    0x48, 0x89, 0xec,                   /*mov    %rbp,%rsp */\n    0x5d,                               /*pop    %rbp */\n    0x58,                               /*pop    %rax               */\n    0x48, 0x8b, 0x4c, 0x24, 0x08,       /*mov  0x08(%rsp),%rcx */\n    0x48, 0x8b, 0x54, 0x24, 0x10,       /*mov  0x10(%rsp),%rdx */\n    0x4c, 0x8b, 0x44, 0x24, 0x18,       /*mov  0x18(%rsp),%r8  */\n    0x4c, 0x8b, 0x4c, 0x24, 0x20,       /*mov  0x20(%rsp),%r9  */\n    0x41, 0xff, 0xe2,                   /*jmpq   *%r10\t\t\t   */\n  };\n#endif\n  VARR (uint8_t) * code;\n  void *res;\n\n  VARR_CREATE (uint8_t, code, ctx->alloc, 128);\n  push_insns (code, wrap_end, sizeof (wrap_end));\n  res = _MIR_publish_code (ctx, VARR_ADDR (uint8_t, code), VARR_LENGTH (uint8_t, code));\n  VARR_DESTROY (uint8_t, code);\n  return res;\n}\n\n/* r10=<bb_version>; jump rex32  ??? mutex free */\nvoid *_MIR_get_bb_thunk (MIR_context_t ctx, void *bb_version, void *handler) {\n  void *res;\n  int32_t disp;\n  static const uint8_t pattern[] = {\n    0x49, 0xba, 0, 0, 0, 0, 0, 0, 0, 0, /* 0x0: movabsq 0, r10 */\n    0xe9, 0,    0, 0, 0,                /* 0xa: jmpq <rel32> */\n  };\n  res = _MIR_publish_code (ctx, pattern, sizeof (pattern));\n  _MIR_update_code (ctx, res, 1, 2, bb_version);\n  disp = (int32_t) ((char *) handler - ((char *) res + sizeof (pattern)));\n  _MIR_change_code (ctx, (uint8_t *) res + 11, (uint8_t *) &disp, 4);\n  return res;\n}\n\n/* change to jmp rex32(to) */\nvoid _MIR_replace_bb_thunk (MIR_context_t ctx, void *thunk, void *to) {\n  uint8_t op = 0xe9; /* jmpq */\n  int32_t disp;\n  _MIR_change_code (ctx, (uint8_t *) thunk, &op, 1); /* jmpq <disp32> */\n  disp = (int32_t) ((char *) to - ((char *) thunk + 5));\n  _MIR_change_code (ctx, (uint8_t *) thunk + 1, (uint8_t *) &disp, 4);\n}\n\nstatic const uint8_t save_pat2[] = {\n#ifndef _WIN32\n  0x48, 0x81, 0xec, 0x80, 0,    0,    0, /*sub    $0x80,%rsp\t\t   */\n  0xf3, 0x0f, 0x7f, 0x04, 0x24,          /*movdqu %xmm0,(%rsp)\t\t   */\n  0xf3, 0x0f, 0x7f, 0x4c, 0x24, 0x10,    /*movdqu %xmm1,0x10(%rsp)\t   */\n  0xf3, 0x0f, 0x7f, 0x54, 0x24, 0x20,    /*movdqu %xmm2,0x20(%rsp)\t   */\n  0xf3, 0x0f, 0x7f, 0x5c, 0x24, 0x30,    /*movdqu %xmm3,0x30(%rsp)\t   */\n  0xf3, 0x0f, 0x7f, 0x64, 0x24, 0x40,    /*movdqu %xmm4,0x40(%rsp)\t   */\n  0xf3, 0x0f, 0x7f, 0x6c, 0x24, 0x50,    /*movdqu %xmm5,0x50(%rsp)\t   */\n  0xf3, 0x0f, 0x7f, 0x74, 0x24, 0x60,    /*movdqu %xmm6,0x60(%rsp)\t   */\n  0xf3, 0x0f, 0x7f, 0x7c, 0x24, 0x70,    /*movdqu %xmm7,0x70(%rsp)\t   */\n  0x41, 0x51,                            /*push   %r9\t\t\t   */\n  0x41, 0x50,                            /*push   %r8\t\t\t   */\n  0x51,                                  /*push   %rcx\t\t\t   */\n  0x52,                                  /*push   %rdx\t\t\t   */\n  0x56,                                  /*push   %rsi\t\t\t   */\n  0x57,                                  /*push   %rdi\t\t\t   */\n#else\n  0x48, 0x89, 0x4c, 0x24, 0x08,          /*mov  %rcx,0x08(%rsp) */\n  0x48, 0x89, 0x54, 0x24, 0x10,          /*mov  %rdx,0x10(%rsp) */\n  0x4c, 0x89, 0x44, 0x24, 0x18,          /*mov  %r8, 0x18(%rsp) */\n  0x4c, 0x89, 0x4c, 0x24, 0x20,          /*mov  %r9, 0x20(%rsp) */\n  0x48, 0x81, 0xec, 0x80, 0,    0,    0, /*sub    $0x60,%rsp\t\t   */\n  0xf3, 0x0f, 0x7f, 0x04, 0x24,          /*movdqu %xmm0,(%rsp)\t\t   */\n  0xf3, 0x0f, 0x7f, 0x4c, 0x24, 0x10,    /*movdqu %xmm1,0x10(%rsp)\t   */\n  0xf3, 0x0f, 0x7f, 0x54, 0x24, 0x20,    /*movdqu %xmm2,0x20(%rsp)\t   */\n  0xf3, 0x0f, 0x7f, 0x5c, 0x24, 0x30,    /*movdqu %xmm3,0x30(%rsp)\t   */\n  0xf3, 0x0f, 0x7f, 0x64, 0x24, 0x40,    /*movdqu %xmm4,0x40(%rsp)\t   */\n  0xf3, 0x0f, 0x7f, 0x6c, 0x24, 0x50,    /*movdqu %xmm5,0x50(%rsp)\t   */\n#endif\n  0x50,       /*push   %rax\t\t\t   */\n  0x41, 0x53, /*push   %r11\t\t\t   */\n};\n\nstatic const uint8_t restore_pat2[] = {\n  0x41, 0x5b, /*pop    %r11\t\t\t   */\n  0x58,       /*pop    %rax\t\t\t   */\n#ifndef _WIN32\n  0x5f,                                  /*pop    %rdi\t\t\t   */\n  0x5e,                                  /*pop    %rsi\t\t\t   */\n  0x5a,                                  /*pop    %rdx\t\t\t   */\n  0x59,                                  /*pop    %rcx\t\t\t   */\n  0x41, 0x58,                            /*pop    %r8\t\t\t   */\n  0x41, 0x59,                            /*pop    %r9\t\t\t   */\n  0xf3, 0x0f, 0x6f, 0x04, 0x24,          /*movdqu (%rsp),%xmm0\t\t   */\n  0xf3, 0x0f, 0x6f, 0x4c, 0x24, 0x10,    /*movdqu 0x10(%rsp),%xmm1\t   */\n  0xf3, 0x0f, 0x6f, 0x54, 0x24, 0x20,    /*movdqu 0x20(%rsp),%xmm2\t   */\n  0xf3, 0x0f, 0x6f, 0x5c, 0x24, 0x30,    /*movdqu 0x30(%rsp),%xmm3\t   */\n  0xf3, 0x0f, 0x6f, 0x64, 0x24, 0x40,    /*movdqu 0x40(%rsp),%xmm4\t   */\n  0xf3, 0x0f, 0x6f, 0x6c, 0x24, 0x50,    /*movdqu 0x50(%rsp),%xmm5\t   */\n  0xf3, 0x0f, 0x6f, 0x74, 0x24, 0x60,    /*movdqu 0x60(%rsp),%xmm6\t   */\n  0xf3, 0x0f, 0x6f, 0x7c, 0x24, 0x70,    /*movdqu 0x70(%rsp),%xmm7\t   */\n  0x48, 0x81, 0xc4, 0x80, 0,    0,    0, /*add    $0x80,%rsp\t\t   */\n#else\n  0xf3, 0x0f, 0x6f, 0x04, 0x24,          /*movdqu (%rsp),%xmm0\t\t   */\n  0xf3, 0x0f, 0x6f, 0x4c, 0x24, 0x10,    /*movdqu 0x10(%rsp),%xmm1\t   */\n  0xf3, 0x0f, 0x6f, 0x54, 0x24, 0x20,    /*movdqu 0x20(%rsp),%xmm2\t   */\n  0xf3, 0x0f, 0x6f, 0x5c, 0x24, 0x30,    /*movdqu 0x30(%rsp),%xmm3\t   */\n  0xf3, 0x0f, 0x6f, 0x64, 0x24, 0x40,    /*movdqu 0x40(%rsp),%xmm4\t   */\n  0xf3, 0x0f, 0x6f, 0x6c, 0x24, 0x50,    /*movdqu 0x50(%rsp),%xmm5\t   */\n  0x48, 0x81, 0xc4, 0x80, 0,    0,    0, /*add    $0x60,%rsp\t\t   */\n  0x48, 0x8b, 0x4c, 0x24, 0x08,          /*mov  0x08(%rsp),%rcx */\n  0x48, 0x8b, 0x54, 0x24, 0x10,          /*mov  0x10(%rsp),%rdx */\n  0x4c, 0x8b, 0x44, 0x24, 0x18,          /*mov  0x18(%rsp),%r8  */\n  0x4c, 0x8b, 0x4c, 0x24, 0x20,          /*mov  0x20(%rsp),%r9  */\n#endif\n};\n\n/* save all clobbered regs but 10; r10 = call hook_address (data, r10); restore regs; jmp *r10\n   r10 is a generator temp reg which is not used across bb borders. */\nvoid *_MIR_get_bb_wrapper (MIR_context_t ctx, void *data, void *hook_address) {\n  static const uint8_t wrap_end[] = {\n    0x41, 0xff, 0xe2, /*jmpq   *%r10\t\t\t   */\n  };\n  static const uint8_t call_pat[] =\n#ifndef _WIN32\n    {\n      0x4c, 0x89, 0xd6,                         /* mov %r10,%rsi */\n      0x48, 0xbf, 0,    0,    0, 0, 0, 0, 0, 0, /* movabs data,%rdi */\n      0x49, 0xba, 0,    0,    0, 0, 0, 0, 0, 0, /* movabs <hook_address>,%r10 */\n      0x48, 0x89, 0xe2,                         /* mov    %rsp,%rdx */\n      0x48, 0x83, 0xe2, 0x0f,                   /* and    $0xf,%rdx */\n      0x74, 0x07,                               /* je     10 <l> */\n      0x52,                                     /* push   %rdx */\n      0x41, 0xff, 0xd2,                         /* callq  *%r10 */\n      0x5a,                                     /* pop    %rdx */\n      0xeb, 0x03,                               /* jmp    13 <l2> */\n      0x41, 0xff, 0xd2,                         /* l: callq  *%r10 */\n      0x49, 0x89, 0xc2,                         /* l2:mov %rax,%r10 */\n    };\n  size_t data_offset = 5, hook_offset = 15;\n#else\n    {\n      0x55,                                     /* push %rbp */\n      0x48, 0x89, 0xe5,                         /* mov %rsp,%rbp */\n      0x4c, 0x89, 0xd2,                         /* mov %r10,%rdx   */\n      0x48, 0xb9, 0,    0,    0, 0, 0, 0, 0, 0, /* movabs data,%rcx           */\n      0x49, 0xba, 0,    0,    0, 0, 0, 0, 0, 0, /* movabs <hook_address>,%r10*/\n      0x50,                                     /* push   %rax               */\n      0x48, 0x83, 0xec, 0x28,                   /* sub    40,%rsp            */\n      0x41, 0xff, 0xd2,       /* callq  *%r10       ???align for unaligned sp       */\n      0x49, 0x89, 0xc2,       /* mov    %rax,%r10          */\n      0x48, 0x83, 0xc4, 0x28, /* add    40,%rsp            */\n      0x58,                   /* pop    %rax               */\n      0x5d,                   /* pop %rbp */\n    };\n  size_t data_offset = 9, hook_offset = 19;\n#endif\n  uint8_t *addr;\n  VARR (uint8_t) * code;\n  void *res;\n\n  VARR_CREATE (uint8_t, code, ctx->alloc, 128);\n  push_insns (code, save_pat2, sizeof (save_pat2));\n  addr = push_insns (code, call_pat, sizeof (call_pat));\n  memcpy (addr + data_offset, &data, sizeof (void *));\n  memcpy (addr + hook_offset, &hook_address, sizeof (void *));\n  push_insns (code, restore_pat2, sizeof (restore_pat2));\n  push_insns (code, wrap_end, sizeof (wrap_end));\n  res = _MIR_publish_code (ctx, VARR_ADDR (uint8_t, code), VARR_LENGTH (uint8_t, code));\n  VARR_DESTROY (uint8_t, code);\n  return res;\n}\n"
        },
        {
          "name": "mir-x86_64.h",
          "type": "blob",
          "size": 2.5498046875,
          "content": "/* This file is a part of MIR project.\n   Copyright (C) 2018-2024 Vladimir Makarov <vmakarov.gcc@gmail.com>.\n   A common include file for mir-x86_64.c and mir-gen-x86_64.c\n*/\n\n#include \"mir.h\"\n\n#define HREG_EL(h) h##_HARD_REG\n#define REP_SEP ,\nenum {\n  REP8 (HREG_EL, AX, CX, DX, BX, SP, BP, SI, DI),\n  REP8 (HREG_EL, R8, R9, R10, R11, R12, R13, R14, R15),\n  REP8 (HREG_EL, XMM0, XMM1, XMM2, XMM3, XMM4, XMM5, XMM6, XMM7),\n  REP8 (HREG_EL, XMM8, XMM9, XMM10, XMM11, XMM12, XMM13, XMM14, XMM15),\n  REP2 (HREG_EL, ST0, ST1),\n};\n#undef REP_SEP\n\nstatic const char *const target_hard_reg_names[] = {\n  \"rax\",   \"rcx\",   \"rdx\",   \"rbx\",   \"rsp\",   \"rbp\",  \"rsi\",  \"rdi\",  \"r8\",\n  \"r9\",    \"r10\",   \"r11\",   \"r12\",   \"r13\",   \"r14\",  \"r15\",  \"xmm0\", \"xmm1\",\n  \"xmm2\",  \"xmm3\",  \"xmm4\",  \"xmm5\",  \"xmm6\",  \"xmm7\", \"xmm8\", \"xmm9\", \"xmm10\",\n  \"xmm11\", \"xmm12\", \"xmm13\", \"xmm14\", \"xmm15\", \"st0\",  \"st1\",\n};\n\n#define MAX_HARD_REG ST1_HARD_REG\n\n/* Hard regs not used in machinized code, preferably call used ones. */\nstatic const MIR_reg_t TEMP_INT_HARD_REG1 = R10_HARD_REG, TEMP_INT_HARD_REG2 = R11_HARD_REG;\n#ifndef _WIN32\nstatic const MIR_reg_t TEMP_FLOAT_HARD_REG1 = XMM8_HARD_REG, TEMP_FLOAT_HARD_REG2 = XMM9_HARD_REG;\nstatic const MIR_reg_t TEMP_DOUBLE_HARD_REG1 = XMM8_HARD_REG, TEMP_DOUBLE_HARD_REG2 = XMM9_HARD_REG;\n#else\nstatic const MIR_reg_t TEMP_FLOAT_HARD_REG1 = XMM4_HARD_REG, TEMP_FLOAT_HARD_REG2 = XMM5_HARD_REG;\nstatic const MIR_reg_t TEMP_DOUBLE_HARD_REG1 = XMM4_HARD_REG, TEMP_DOUBLE_HARD_REG2 = XMM5_HARD_REG;\n#endif\nstatic const MIR_reg_t TEMP_LDOUBLE_HARD_REG1 = MIR_NON_VAR;\nstatic const MIR_reg_t TEMP_LDOUBLE_HARD_REG2 = MIR_NON_VAR;\n\nstatic inline int target_hard_reg_type_ok_p (MIR_reg_t hard_reg, MIR_type_t type) {\n  assert (hard_reg <= MAX_HARD_REG);\n  /* For LD we need x87 stack regs and it is too complicated so no\n     hard register allocation for LD: */\n  if (type == MIR_T_LD) return FALSE;\n  return MIR_int_type_p (type) ? hard_reg < XMM0_HARD_REG : hard_reg >= XMM0_HARD_REG;\n}\n\nstatic inline int target_fixed_hard_reg_p (MIR_reg_t hard_reg) {\n  assert (hard_reg <= MAX_HARD_REG);\n  return (hard_reg == BP_HARD_REG || hard_reg == SP_HARD_REG || hard_reg == TEMP_INT_HARD_REG1\n          || hard_reg == TEMP_INT_HARD_REG2 || hard_reg == TEMP_FLOAT_HARD_REG1\n          || hard_reg == TEMP_FLOAT_HARD_REG2 || hard_reg == TEMP_DOUBLE_HARD_REG1\n          || hard_reg == TEMP_DOUBLE_HARD_REG2 || hard_reg == ST0_HARD_REG\n          || hard_reg == ST1_HARD_REG);\n}\n\nstatic inline int target_locs_num (MIR_reg_t loc, MIR_type_t type) {\n  return loc > MAX_HARD_REG && type == MIR_T_LD ? 2 : 1;\n}\n"
        },
        {
          "name": "mir.c",
          "type": "blob",
          "size": 278.923828125,
          "content": "/* This file is a part of MIR project.\n   Copyright (C) 2018-2024 Vladimir Makarov <vmakarov.gcc@gmail.com>.\n*/\n\n#include \"mir.h\"\n#include \"mir-code-alloc.h\"\n\nDEF_VARR (MIR_insn_t);\nDEF_VARR (MIR_reg_t);\nDEF_VARR (MIR_op_t);\nDEF_VARR (MIR_type_t);\nDEF_HTAB (MIR_item_t);\nDEF_VARR (MIR_module_t);\nDEF_VARR (size_t);\nDEF_VARR (char);\nDEF_VARR (uint8_t);\nDEF_VARR (MIR_proto_t);\n\nstruct gen_ctx;\nstruct c2mir_ctx;\nstruct string_ctx;\nstruct reg_ctx;\nstruct alias_ctx;\nstruct simplify_ctx;\nstruct machine_code_ctx;\nstruct io_ctx;\nstruct scan_ctx;\nstruct hard_reg_ctx;\nstruct interp_ctx;\n\nstruct MIR_context {\n  struct gen_ctx *gen_ctx;     /* should be the 1st member */\n  struct c2mir_ctx *c2mir_ctx; /* should be the 2nd member */\n  MIR_error_func_t error_func;\n  MIR_alloc_t alloc;\n  MIR_code_alloc_t code_alloc;\n  int func_redef_permission_p;        /* when true loaded func can be redfined lately */\n  VARR (size_t) * insn_nops;          /* constant after initialization */\n  VARR (MIR_proto_t) * unspec_protos; /* protos of unspec insns (set only during initialization) */\n  VARR (char) * temp_string;\n  VARR (uint8_t) * temp_data, *used_label_p;\n  HTAB (MIR_item_t) * module_item_tab;\n  /* Module to keep items potentially used by all modules:  */\n  struct MIR_module environment_module;\n  MIR_module_t curr_module;\n  MIR_func_t curr_func;\n  size_t curr_label_num;\n  DLIST (MIR_module_t) all_modules;\n  VARR (MIR_module_t) * modules_to_link;\n  VARR (MIR_op_t) * temp_ops;\n  struct string_ctx *string_ctx;\n  struct reg_ctx *reg_ctx;\n  struct alias_ctx *alias_ctx;\n  struct simplify_ctx *simplify_ctx;\n  struct machine_code_ctx *machine_code_ctx;\n  struct io_ctx *io_ctx;\n  struct scan_ctx *scan_ctx;\n  struct hard_reg_ctx *hard_reg_ctx;\n  struct interp_ctx *interp_ctx;\n  void *setjmp_addr;      /* used in interpreter to call setjmp directly not from a shim and FFI */\n  void *wrapper_end_addr; /* used by generator */\n};\n\n#define error_func ctx->error_func\n#define func_redef_permission_p ctx->func_redef_permission_p\n#define unspec_protos ctx->unspec_protos\n#define insn_nops ctx->insn_nops\n#define temp_string ctx->temp_string\n#define temp_data ctx->temp_data\n#define used_label_p ctx->used_label_p\n#define module_item_tab ctx->module_item_tab\n#define environment_module ctx->environment_module\n#define curr_module ctx->curr_module\n#define curr_func ctx->curr_func\n#define curr_label_num ctx->curr_label_num\n#define all_modules ctx->all_modules\n#define modules_to_link ctx->modules_to_link\n#define temp_ops ctx->temp_ops\n#define setjmp_addr ctx->setjmp_addr\n#define wrapper_end_addr ctx->wrapper_end_addr\n\nstatic void util_error (MIR_context_t ctx, const char *message);\n#define MIR_VARR_ERROR util_error\n#define MIR_HTAB_ERROR MIR_VARR_ERROR\n\n#include \"mir-hash.h\"\n#include \"mir-htab.h\"\n#include \"mir-reduce.h\"\n#include \"mir-bitmap.h\"\n#include <string.h>\n#include <stdarg.h>\n#include <inttypes.h>\n#include <float.h>\n#include <ctype.h>\n#include <limits.h>\n\nstatic void interp_init (MIR_context_t ctx);\nstatic void finish_func_interpretation (MIR_item_t func_item, MIR_alloc_t alloc);\nstatic void interp_finish (MIR_context_t ctx);\n\nstatic void MIR_NO_RETURN default_error (enum MIR_error_type error_type MIR_UNUSED,\n                                         const char *format, ...) {\n  va_list ap;\n\n  va_start (ap, format);\n  vfprintf (stderr, format, ap);\n  fprintf (stderr, \"\\n\");\n  va_end (ap);\n  exit (1);\n}\n\nstatic void MIR_NO_RETURN MIR_UNUSED util_error (MIR_context_t ctx, const char *message) {\n  MIR_get_error_func (ctx) (MIR_alloc_error, message);\n}\n\n#define HARD_REG_NAME_PREFIX \"hr\"\n#define TEMP_REG_NAME_PREFIX \"t\"\n#define TEMP_ITEM_NAME_PREFIX \".lc\"\n\nint _MIR_reserved_ref_name_p (MIR_context_t ctx MIR_UNUSED, const char *name) {\n  return strncmp (name, TEMP_ITEM_NAME_PREFIX, strlen (TEMP_ITEM_NAME_PREFIX)) == 0;\n}\n\n/* Reserved names:\n   fp - frame pointer\n   hr<number> - a hardware reg\n   lc<number> - a temp item */\nint _MIR_reserved_name_p (MIR_context_t ctx, const char *name) {\n  size_t i, start;\n\n  if (_MIR_reserved_ref_name_p (ctx, name))\n    return TRUE;\n  else if (strncmp (name, HARD_REG_NAME_PREFIX, strlen (HARD_REG_NAME_PREFIX)) == 0)\n    start = strlen (HARD_REG_NAME_PREFIX);\n  else\n    return FALSE;\n  for (i = start; name[i] != '\\0'; i++)\n    if (name[i] < '0' || name[i] > '9') return FALSE;\n  return TRUE;\n}\n\nstruct insn_desc {\n  MIR_insn_code_t code;\n  const char *name;\n  unsigned char op_modes[5];\n};\n\n#define OUT_FLAG (1 << 7)\n\nstatic const struct insn_desc insn_descs[] = {\n  {MIR_MOV, \"mov\", {MIR_OP_INT | OUT_FLAG, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_FMOV, \"fmov\", {MIR_OP_FLOAT | OUT_FLAG, MIR_OP_FLOAT, MIR_OP_BOUND}},\n  {MIR_DMOV, \"dmov\", {MIR_OP_DOUBLE | OUT_FLAG, MIR_OP_DOUBLE, MIR_OP_BOUND}},\n  {MIR_LDMOV, \"ldmov\", {MIR_OP_LDOUBLE | OUT_FLAG, MIR_OP_LDOUBLE, MIR_OP_BOUND}},\n  {MIR_EXT8, \"ext8\", {MIR_OP_INT | OUT_FLAG, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_EXT16, \"ext16\", {MIR_OP_INT | OUT_FLAG, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_EXT32, \"ext32\", {MIR_OP_INT | OUT_FLAG, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_UEXT8, \"uext8\", {MIR_OP_INT | OUT_FLAG, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_UEXT16, \"uext16\", {MIR_OP_INT | OUT_FLAG, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_UEXT32, \"uext32\", {MIR_OP_INT | OUT_FLAG, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_I2F, \"i2f\", {MIR_OP_FLOAT | OUT_FLAG, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_I2D, \"i2d\", {MIR_OP_DOUBLE | OUT_FLAG, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_I2LD, \"i2ld\", {MIR_OP_LDOUBLE | OUT_FLAG, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_UI2F, \"ui2f\", {MIR_OP_FLOAT | OUT_FLAG, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_UI2D, \"ui2d\", {MIR_OP_DOUBLE | OUT_FLAG, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_UI2LD, \"ui2ld\", {MIR_OP_LDOUBLE | OUT_FLAG, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_F2I, \"f2i\", {MIR_OP_INT | OUT_FLAG, MIR_OP_FLOAT, MIR_OP_BOUND}},\n  {MIR_D2I, \"d2i\", {MIR_OP_INT | OUT_FLAG, MIR_OP_DOUBLE, MIR_OP_BOUND}},\n  {MIR_LD2I, \"ld2i\", {MIR_OP_INT | OUT_FLAG, MIR_OP_LDOUBLE, MIR_OP_BOUND}},\n  {MIR_F2D, \"f2d\", {MIR_OP_DOUBLE | OUT_FLAG, MIR_OP_FLOAT, MIR_OP_BOUND}},\n  {MIR_F2LD, \"f2ld\", {MIR_OP_LDOUBLE | OUT_FLAG, MIR_OP_FLOAT, MIR_OP_BOUND}},\n  {MIR_D2F, \"d2f\", {MIR_OP_FLOAT | OUT_FLAG, MIR_OP_DOUBLE, MIR_OP_BOUND}},\n  {MIR_D2LD, \"d2ld\", {MIR_OP_LDOUBLE | OUT_FLAG, MIR_OP_DOUBLE, MIR_OP_BOUND}},\n  {MIR_LD2F, \"ld2f\", {MIR_OP_FLOAT | OUT_FLAG, MIR_OP_LDOUBLE, MIR_OP_BOUND}},\n  {MIR_LD2D, \"ld2d\", {MIR_OP_DOUBLE | OUT_FLAG, MIR_OP_LDOUBLE, MIR_OP_BOUND}},\n  {MIR_NEG, \"neg\", {MIR_OP_INT | OUT_FLAG, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_NEGS, \"negs\", {MIR_OP_INT | OUT_FLAG, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_FNEG, \"fneg\", {MIR_OP_FLOAT | OUT_FLAG, MIR_OP_FLOAT, MIR_OP_BOUND}},\n  {MIR_DNEG, \"dneg\", {MIR_OP_DOUBLE | OUT_FLAG, MIR_OP_DOUBLE, MIR_OP_BOUND}},\n  {MIR_LDNEG, \"ldneg\", {MIR_OP_LDOUBLE | OUT_FLAG, MIR_OP_LDOUBLE, MIR_OP_BOUND}},\n  {MIR_ADDR, \"addr\", {MIR_OP_INT | OUT_FLAG, MIR_OP_REG, MIR_OP_BOUND}},     /* MIR_OP_REG! */\n  {MIR_ADDR8, \"addr8\", {MIR_OP_INT | OUT_FLAG, MIR_OP_REG, MIR_OP_BOUND}},   /* MIR_OP_REG! */\n  {MIR_ADDR16, \"addr16\", {MIR_OP_INT | OUT_FLAG, MIR_OP_REG, MIR_OP_BOUND}}, /* MIR_OP_REG! */\n  {MIR_ADDR32, \"addr32\", {MIR_OP_INT | OUT_FLAG, MIR_OP_REG, MIR_OP_BOUND}}, /* MIR_OP_REG! */\n  {MIR_ADD, \"add\", {MIR_OP_INT | OUT_FLAG, MIR_OP_INT, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_ADDS, \"adds\", {MIR_OP_INT | OUT_FLAG, MIR_OP_INT, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_FADD, \"fadd\", {MIR_OP_FLOAT | OUT_FLAG, MIR_OP_FLOAT, MIR_OP_FLOAT, MIR_OP_BOUND}},\n  {MIR_DADD, \"dadd\", {MIR_OP_DOUBLE | OUT_FLAG, MIR_OP_DOUBLE, MIR_OP_DOUBLE, MIR_OP_BOUND}},\n  {MIR_LDADD, \"ldadd\", {MIR_OP_LDOUBLE | OUT_FLAG, MIR_OP_LDOUBLE, MIR_OP_LDOUBLE, MIR_OP_BOUND}},\n  {MIR_SUB, \"sub\", {MIR_OP_INT | OUT_FLAG, MIR_OP_INT, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_SUBS, \"subs\", {MIR_OP_INT | OUT_FLAG, MIR_OP_INT, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_FSUB, \"fsub\", {MIR_OP_FLOAT | OUT_FLAG, MIR_OP_FLOAT, MIR_OP_FLOAT, MIR_OP_BOUND}},\n  {MIR_DSUB, \"dsub\", {MIR_OP_DOUBLE | OUT_FLAG, MIR_OP_DOUBLE, MIR_OP_DOUBLE, MIR_OP_BOUND}},\n  {MIR_LDSUB, \"ldsub\", {MIR_OP_LDOUBLE | OUT_FLAG, MIR_OP_LDOUBLE, MIR_OP_LDOUBLE, MIR_OP_BOUND}},\n  {MIR_MUL, \"mul\", {MIR_OP_INT | OUT_FLAG, MIR_OP_INT, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_MULS, \"muls\", {MIR_OP_INT | OUT_FLAG, MIR_OP_INT, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_FMUL, \"fmul\", {MIR_OP_FLOAT | OUT_FLAG, MIR_OP_FLOAT, MIR_OP_FLOAT, MIR_OP_BOUND}},\n  {MIR_DMUL, \"dmul\", {MIR_OP_DOUBLE | OUT_FLAG, MIR_OP_DOUBLE, MIR_OP_DOUBLE, MIR_OP_BOUND}},\n  {MIR_LDMUL, \"ldmul\", {MIR_OP_LDOUBLE | OUT_FLAG, MIR_OP_LDOUBLE, MIR_OP_LDOUBLE, MIR_OP_BOUND}},\n  {MIR_DIV, \"div\", {MIR_OP_INT | OUT_FLAG, MIR_OP_INT, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_DIVS, \"divs\", {MIR_OP_INT | OUT_FLAG, MIR_OP_INT, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_UDIV, \"udiv\", {MIR_OP_INT | OUT_FLAG, MIR_OP_INT, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_UDIVS, \"udivs\", {MIR_OP_INT | OUT_FLAG, MIR_OP_INT, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_FDIV, \"fdiv\", {MIR_OP_FLOAT | OUT_FLAG, MIR_OP_FLOAT, MIR_OP_FLOAT, MIR_OP_BOUND}},\n  {MIR_DDIV, \"ddiv\", {MIR_OP_DOUBLE | OUT_FLAG, MIR_OP_DOUBLE, MIR_OP_DOUBLE, MIR_OP_BOUND}},\n  {MIR_LDDIV, \"lddiv\", {MIR_OP_LDOUBLE | OUT_FLAG, MIR_OP_LDOUBLE, MIR_OP_LDOUBLE, MIR_OP_BOUND}},\n  {MIR_MOD, \"mod\", {MIR_OP_INT | OUT_FLAG, MIR_OP_INT, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_MODS, \"mods\", {MIR_OP_INT | OUT_FLAG, MIR_OP_INT, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_UMOD, \"umod\", {MIR_OP_INT | OUT_FLAG, MIR_OP_INT, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_UMODS, \"umods\", {MIR_OP_INT | OUT_FLAG, MIR_OP_INT, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_AND, \"and\", {MIR_OP_INT | OUT_FLAG, MIR_OP_INT, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_ANDS, \"ands\", {MIR_OP_INT | OUT_FLAG, MIR_OP_INT, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_OR, \"or\", {MIR_OP_INT | OUT_FLAG, MIR_OP_INT, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_ORS, \"ors\", {MIR_OP_INT | OUT_FLAG, MIR_OP_INT, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_XOR, \"xor\", {MIR_OP_INT | OUT_FLAG, MIR_OP_INT, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_XORS, \"xors\", {MIR_OP_INT | OUT_FLAG, MIR_OP_INT, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_LSH, \"lsh\", {MIR_OP_INT | OUT_FLAG, MIR_OP_INT, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_LSHS, \"lshs\", {MIR_OP_INT | OUT_FLAG, MIR_OP_INT, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_RSH, \"rsh\", {MIR_OP_INT | OUT_FLAG, MIR_OP_INT, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_RSHS, \"rshs\", {MIR_OP_INT | OUT_FLAG, MIR_OP_INT, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_URSH, \"ursh\", {MIR_OP_INT | OUT_FLAG, MIR_OP_INT, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_URSHS, \"urshs\", {MIR_OP_INT | OUT_FLAG, MIR_OP_INT, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_EQ, \"eq\", {MIR_OP_INT | OUT_FLAG, MIR_OP_INT, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_EQS, \"eqs\", {MIR_OP_INT | OUT_FLAG, MIR_OP_INT, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_FEQ, \"feq\", {MIR_OP_INT | OUT_FLAG, MIR_OP_FLOAT, MIR_OP_FLOAT, MIR_OP_BOUND}},\n  {MIR_DEQ, \"deq\", {MIR_OP_INT | OUT_FLAG, MIR_OP_DOUBLE, MIR_OP_DOUBLE, MIR_OP_BOUND}},\n  {MIR_LDEQ, \"ldeq\", {MIR_OP_INT | OUT_FLAG, MIR_OP_LDOUBLE, MIR_OP_LDOUBLE, MIR_OP_BOUND}},\n  {MIR_NE, \"ne\", {MIR_OP_INT | OUT_FLAG, MIR_OP_INT, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_NES, \"nes\", {MIR_OP_INT | OUT_FLAG, MIR_OP_INT, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_FNE, \"fne\", {MIR_OP_INT | OUT_FLAG, MIR_OP_FLOAT, MIR_OP_FLOAT, MIR_OP_BOUND}},\n  {MIR_DNE, \"dne\", {MIR_OP_INT | OUT_FLAG, MIR_OP_DOUBLE, MIR_OP_DOUBLE, MIR_OP_BOUND}},\n  {MIR_LDNE, \"ldne\", {MIR_OP_INT | OUT_FLAG, MIR_OP_LDOUBLE, MIR_OP_LDOUBLE, MIR_OP_BOUND}},\n  {MIR_LT, \"lt\", {MIR_OP_INT | OUT_FLAG, MIR_OP_INT, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_LTS, \"lts\", {MIR_OP_INT | OUT_FLAG, MIR_OP_INT, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_ULT, \"ult\", {MIR_OP_INT | OUT_FLAG, MIR_OP_INT, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_ULTS, \"ults\", {MIR_OP_INT | OUT_FLAG, MIR_OP_INT, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_FLT, \"flt\", {MIR_OP_INT | OUT_FLAG, MIR_OP_FLOAT, MIR_OP_FLOAT, MIR_OP_BOUND}},\n  {MIR_DLT, \"dlt\", {MIR_OP_INT | OUT_FLAG, MIR_OP_DOUBLE, MIR_OP_DOUBLE, MIR_OP_BOUND}},\n  {MIR_LDLT, \"ldlt\", {MIR_OP_INT | OUT_FLAG, MIR_OP_LDOUBLE, MIR_OP_LDOUBLE, MIR_OP_BOUND}},\n  {MIR_LE, \"le\", {MIR_OP_INT | OUT_FLAG, MIR_OP_INT, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_LES, \"les\", {MIR_OP_INT | OUT_FLAG, MIR_OP_INT, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_ULE, \"ule\", {MIR_OP_INT | OUT_FLAG, MIR_OP_INT, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_ULES, \"ules\", {MIR_OP_INT | OUT_FLAG, MIR_OP_INT, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_FLE, \"fle\", {MIR_OP_INT | OUT_FLAG, MIR_OP_FLOAT, MIR_OP_FLOAT, MIR_OP_BOUND}},\n  {MIR_DLE, \"dle\", {MIR_OP_INT | OUT_FLAG, MIR_OP_DOUBLE, MIR_OP_DOUBLE, MIR_OP_BOUND}},\n  {MIR_LDLE, \"ldle\", {MIR_OP_INT | OUT_FLAG, MIR_OP_LDOUBLE, MIR_OP_LDOUBLE, MIR_OP_BOUND}},\n  {MIR_GT, \"gt\", {MIR_OP_INT | OUT_FLAG, MIR_OP_INT, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_GTS, \"gts\", {MIR_OP_INT | OUT_FLAG, MIR_OP_INT, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_UGT, \"ugt\", {MIR_OP_INT | OUT_FLAG, MIR_OP_INT, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_UGTS, \"ugts\", {MIR_OP_INT | OUT_FLAG, MIR_OP_INT, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_FGT, \"fgt\", {MIR_OP_INT | OUT_FLAG, MIR_OP_FLOAT, MIR_OP_FLOAT, MIR_OP_BOUND}},\n  {MIR_DGT, \"dgt\", {MIR_OP_INT | OUT_FLAG, MIR_OP_DOUBLE, MIR_OP_DOUBLE, MIR_OP_BOUND}},\n  {MIR_LDGT, \"ldgt\", {MIR_OP_INT | OUT_FLAG, MIR_OP_LDOUBLE, MIR_OP_LDOUBLE, MIR_OP_BOUND}},\n  {MIR_GE, \"ge\", {MIR_OP_INT | OUT_FLAG, MIR_OP_INT, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_GES, \"ges\", {MIR_OP_INT | OUT_FLAG, MIR_OP_INT, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_UGE, \"uge\", {MIR_OP_INT | OUT_FLAG, MIR_OP_INT, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_UGES, \"uges\", {MIR_OP_INT | OUT_FLAG, MIR_OP_INT, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_FGE, \"fge\", {MIR_OP_INT | OUT_FLAG, MIR_OP_FLOAT, MIR_OP_FLOAT, MIR_OP_BOUND}},\n  {MIR_DGE, \"dge\", {MIR_OP_INT | OUT_FLAG, MIR_OP_DOUBLE, MIR_OP_DOUBLE, MIR_OP_BOUND}},\n  {MIR_LDGE, \"ldge\", {MIR_OP_INT | OUT_FLAG, MIR_OP_LDOUBLE, MIR_OP_LDOUBLE, MIR_OP_BOUND}},\n  {MIR_ADDO, \"addo\", {MIR_OP_INT | OUT_FLAG, MIR_OP_INT, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_ADDOS, \"addos\", {MIR_OP_INT | OUT_FLAG, MIR_OP_INT, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_SUBO, \"subo\", {MIR_OP_INT | OUT_FLAG, MIR_OP_INT, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_SUBOS, \"subos\", {MIR_OP_INT | OUT_FLAG, MIR_OP_INT, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_MULO, \"mulo\", {MIR_OP_INT | OUT_FLAG, MIR_OP_INT, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_MULOS, \"mulos\", {MIR_OP_INT | OUT_FLAG, MIR_OP_INT, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_UMULO, \"umulo\", {MIR_OP_INT | OUT_FLAG, MIR_OP_INT, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_UMULOS, \"umulos\", {MIR_OP_INT | OUT_FLAG, MIR_OP_INT, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_JMP, \"jmp\", {MIR_OP_LABEL, MIR_OP_BOUND}},\n  {MIR_BT, \"bt\", {MIR_OP_LABEL, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_BTS, \"bts\", {MIR_OP_LABEL, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_BF, \"bf\", {MIR_OP_LABEL, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_BFS, \"bfs\", {MIR_OP_LABEL, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_BEQ, \"beq\", {MIR_OP_LABEL, MIR_OP_INT, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_BEQS, \"beqs\", {MIR_OP_LABEL, MIR_OP_INT, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_FBEQ, \"fbeq\", {MIR_OP_LABEL, MIR_OP_FLOAT, MIR_OP_FLOAT, MIR_OP_BOUND}},\n  {MIR_DBEQ, \"dbeq\", {MIR_OP_LABEL, MIR_OP_DOUBLE, MIR_OP_DOUBLE, MIR_OP_BOUND}},\n  {MIR_LDBEQ, \"ldbeq\", {MIR_OP_LABEL, MIR_OP_LDOUBLE, MIR_OP_LDOUBLE, MIR_OP_BOUND}},\n  {MIR_BNE, \"bne\", {MIR_OP_LABEL, MIR_OP_INT, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_BNES, \"bnes\", {MIR_OP_LABEL, MIR_OP_INT, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_FBNE, \"fbne\", {MIR_OP_LABEL, MIR_OP_FLOAT, MIR_OP_FLOAT, MIR_OP_BOUND}},\n  {MIR_DBNE, \"dbne\", {MIR_OP_LABEL, MIR_OP_DOUBLE, MIR_OP_DOUBLE, MIR_OP_BOUND}},\n  {MIR_LDBNE, \"ldbne\", {MIR_OP_LABEL, MIR_OP_LDOUBLE, MIR_OP_LDOUBLE, MIR_OP_BOUND}},\n  {MIR_BLT, \"blt\", {MIR_OP_LABEL, MIR_OP_INT, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_BLTS, \"blts\", {MIR_OP_LABEL, MIR_OP_INT, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_UBLT, \"ublt\", {MIR_OP_LABEL, MIR_OP_INT, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_UBLTS, \"ublts\", {MIR_OP_LABEL, MIR_OP_INT, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_FBLT, \"fblt\", {MIR_OP_LABEL, MIR_OP_FLOAT, MIR_OP_FLOAT, MIR_OP_BOUND}},\n  {MIR_DBLT, \"dblt\", {MIR_OP_LABEL, MIR_OP_DOUBLE, MIR_OP_DOUBLE, MIR_OP_BOUND}},\n  {MIR_LDBLT, \"ldblt\", {MIR_OP_LABEL, MIR_OP_LDOUBLE, MIR_OP_LDOUBLE, MIR_OP_BOUND}},\n  {MIR_BLE, \"ble\", {MIR_OP_LABEL, MIR_OP_INT, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_BLES, \"bles\", {MIR_OP_LABEL, MIR_OP_INT, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_UBLE, \"uble\", {MIR_OP_LABEL, MIR_OP_INT, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_UBLES, \"ubles\", {MIR_OP_LABEL, MIR_OP_INT, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_FBLE, \"fble\", {MIR_OP_LABEL, MIR_OP_FLOAT, MIR_OP_FLOAT, MIR_OP_BOUND}},\n  {MIR_DBLE, \"dble\", {MIR_OP_LABEL, MIR_OP_DOUBLE, MIR_OP_DOUBLE, MIR_OP_BOUND}},\n  {MIR_LDBLE, \"ldble\", {MIR_OP_LABEL, MIR_OP_LDOUBLE, MIR_OP_LDOUBLE, MIR_OP_BOUND}},\n  {MIR_BGT, \"bgt\", {MIR_OP_LABEL, MIR_OP_INT, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_BGTS, \"bgts\", {MIR_OP_LABEL, MIR_OP_INT, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_UBGT, \"ubgt\", {MIR_OP_LABEL, MIR_OP_INT, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_UBGTS, \"ubgts\", {MIR_OP_LABEL, MIR_OP_INT, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_FBGT, \"fbgt\", {MIR_OP_LABEL, MIR_OP_FLOAT, MIR_OP_FLOAT, MIR_OP_BOUND}},\n  {MIR_DBGT, \"dbgt\", {MIR_OP_LABEL, MIR_OP_DOUBLE, MIR_OP_DOUBLE, MIR_OP_BOUND}},\n  {MIR_LDBGT, \"ldbgt\", {MIR_OP_LABEL, MIR_OP_LDOUBLE, MIR_OP_LDOUBLE, MIR_OP_BOUND}},\n  {MIR_BGE, \"bge\", {MIR_OP_LABEL, MIR_OP_INT, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_BGES, \"bges\", {MIR_OP_LABEL, MIR_OP_INT, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_UBGE, \"ubge\", {MIR_OP_LABEL, MIR_OP_INT, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_UBGES, \"ubges\", {MIR_OP_LABEL, MIR_OP_INT, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_FBGE, \"fbge\", {MIR_OP_LABEL, MIR_OP_FLOAT, MIR_OP_FLOAT, MIR_OP_BOUND}},\n  {MIR_DBGE, \"dbge\", {MIR_OP_LABEL, MIR_OP_DOUBLE, MIR_OP_DOUBLE, MIR_OP_BOUND}},\n  {MIR_LDBGE, \"ldbge\", {MIR_OP_LABEL, MIR_OP_LDOUBLE, MIR_OP_LDOUBLE, MIR_OP_BOUND}},\n  {MIR_BO, \"bo\", {MIR_OP_LABEL, MIR_OP_BOUND}},\n  {MIR_UBO, \"ubo\", {MIR_OP_LABEL, MIR_OP_BOUND}},\n  {MIR_BNO, \"bno\", {MIR_OP_LABEL, MIR_OP_BOUND}},\n  {MIR_UBNO, \"ubno\", {MIR_OP_LABEL, MIR_OP_BOUND}},\n  {MIR_LADDR, \"laddr\", {MIR_OP_INT, MIR_OP_LABEL, MIR_OP_BOUND}},\n  {MIR_JMPI, \"jmpi\", {MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_CALL, \"call\", {MIR_OP_BOUND}},\n  {MIR_INLINE, \"inline\", {MIR_OP_BOUND}},\n  {MIR_JCALL, \"jcall\", {MIR_OP_BOUND}},\n  {MIR_SWITCH, \"switch\", {MIR_OP_BOUND}},\n  {MIR_RET, \"ret\", {MIR_OP_BOUND}},\n  {MIR_JRET, \"jret\", {MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_ALLOCA, \"alloca\", {MIR_OP_INT | OUT_FLAG, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_BSTART, \"bstart\", {MIR_OP_INT | OUT_FLAG, MIR_OP_BOUND}},\n  {MIR_BEND, \"bend\", {MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_VA_ARG, \"va_arg\", {MIR_OP_INT | OUT_FLAG, MIR_OP_INT, MIR_OP_UNDEF, MIR_OP_BOUND}},\n  {MIR_VA_BLOCK_ARG,\n   \"va_block_arg\",\n   {MIR_OP_INT, MIR_OP_INT, MIR_OP_INT, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_VA_START, \"va_start\", {MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_VA_END, \"va_end\", {MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_LABEL, \"label\", {MIR_OP_BOUND}},\n  {MIR_UNSPEC, \"unspec\", {MIR_OP_BOUND}},\n  {MIR_PRSET, \"prset\", {MIR_OP_UNDEF, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_PRBEQ, \"prbeq\", {MIR_OP_LABEL, MIR_OP_UNDEF, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_PRBNE, \"prbne\", {MIR_OP_LABEL, MIR_OP_UNDEF, MIR_OP_INT, MIR_OP_BOUND}},\n  {MIR_USE, \"use\", {MIR_OP_BOUND}},\n  {MIR_PHI, \"phi\", {MIR_OP_BOUND}},\n  {MIR_INVALID_INSN, \"invalid-insn\", {MIR_OP_BOUND}},\n};\n\nstatic void check_and_prepare_insn_descs (MIR_context_t ctx) {\n  size_t i, j;\n\n  VARR_CREATE (size_t, insn_nops, ctx->alloc, 0);\n  for (i = 0; i < MIR_INSN_BOUND; i++) {\n    mir_assert (insn_descs[i].code == i);\n    for (j = 0; insn_descs[i].op_modes[j] != MIR_OP_BOUND; j++)\n      ;\n    VARR_PUSH (size_t, insn_nops, j);\n  }\n}\n\nstatic MIR_op_mode_t type2mode (MIR_type_t type) {\n  return (type == MIR_T_UNDEF ? MIR_OP_UNDEF\n          : type == MIR_T_F   ? MIR_OP_FLOAT\n          : type == MIR_T_D   ? MIR_OP_DOUBLE\n          : type == MIR_T_LD  ? MIR_OP_LDOUBLE\n                              : MIR_OP_INT);\n}\n\nint64_t _MIR_addr_offset (MIR_context_t ctx MIR_UNUSED, MIR_insn_code_t code) {\n  int v = 1;\n  if (code == MIR_ADDR || *(char *) &v != 0) return 0;\n  return code == MIR_ADDR8 ? 7 : code == MIR_ADDR16 ? 6 : 4;\n}\n\n/* New Page */\n\ntypedef struct string {\n  size_t num; /* string number starting with 1 */\n  MIR_str_t str;\n} string_t;\n\nDEF_VARR (string_t);\nDEF_HTAB (string_t);\n\nstruct string_ctx {\n  VARR (string_t) * strings;\n  HTAB (string_t) * string_tab;\n};\n\n#define strings ctx->string_ctx->strings\n#define string_tab ctx->string_ctx->string_tab\n\nstatic htab_hash_t str_hash (string_t str, void *arg MIR_UNUSED) {\n  return (htab_hash_t) mir_hash (str.str.s, str.str.len, 0);\n}\nstatic int str_eq (string_t str1, string_t str2, void *arg MIR_UNUSED) {\n  return str1.str.len == str2.str.len && memcmp (str1.str.s, str2.str.s, str1.str.len) == 0;\n}\n\nstatic void string_init (MIR_alloc_t alloc, VARR (string_t) * *strs, HTAB (string_t) * *str_tab) {\n  string_t string = {0, {0, NULL}};\n\n  VARR_CREATE (string_t, *strs, alloc, 0);\n  VARR_PUSH (string_t, *strs, string); /* don't use 0th string */\n  HTAB_CREATE (string_t, *str_tab, alloc, 1000, str_hash, str_eq, NULL);\n}\n\nstatic int string_find (VARR (string_t) * *strs MIR_UNUSED, HTAB (string_t) * *str_tab,\n                        MIR_str_t str, string_t *s) {\n  string_t string;\n\n  string.str = str;\n  return HTAB_DO (string_t, *str_tab, string, HTAB_FIND, *s);\n}\n\nstatic string_t string_store (MIR_context_t ctx, VARR (string_t) * *strs,\n                              HTAB (string_t) * *str_tab, MIR_str_t str) {\n  char *heap_str;\n  string_t el, string;\n\n  if (string_find (strs, str_tab, str, &el)) return el;\n  if ((heap_str = MIR_malloc (ctx->alloc, str.len)) == NULL)\n    MIR_get_error_func (ctx) (MIR_alloc_error, \"Not enough memory for strings\");\n  memcpy (heap_str, str.s, str.len);\n  string.str.s = heap_str;\n  string.str.len = str.len;\n  string.num = VARR_LENGTH (string_t, *strs);\n  VARR_PUSH (string_t, *strs, string);\n  HTAB_DO (string_t, *str_tab, string, HTAB_INSERT, el);\n  return string;\n}\n\nstatic struct string get_ctx_string (MIR_context_t ctx, MIR_str_t str) {\n  return string_store (ctx, &strings, &string_tab, str);\n}\n\nstatic const char *get_ctx_str (MIR_context_t ctx, const char *string) {\n  return get_ctx_string (ctx, (MIR_str_t){strlen (string) + 1, string}).str.s;\n}\n\nstatic void string_finish (MIR_alloc_t alloc, VARR (string_t) * *strs, HTAB (string_t) * *str_tab) {\n  size_t i;\n\n  for (i = 1; i < VARR_LENGTH (string_t, *strs); i++)\n    MIR_free (alloc, (char *) VARR_ADDR (string_t, *strs)[i].str.s);\n  VARR_DESTROY (string_t, *strs);\n  HTAB_DESTROY (string_t, *str_tab);\n}\n\n/* Functions to work with aliases.  */\n\nstruct alias_ctx {\n  VARR (string_t) * aliases;\n  HTAB (string_t) * alias_tab;\n};\n\n#define aliases ctx->alias_ctx->aliases\n#define alias_tab ctx->alias_ctx->alias_tab\n\nMIR_alias_t MIR_alias (MIR_context_t ctx, const char *name) {\n  return (MIR_alias_t) string_store (ctx, &aliases, &alias_tab,\n                                     (MIR_str_t){strlen (name) + 1, name})\n    .num;\n}\n\nconst char *MIR_alias_name (MIR_context_t ctx, MIR_alias_t alias) {\n  if (alias == 0) return \"\";\n  if (alias >= VARR_LENGTH (string_t, aliases))\n    MIR_get_error_func (ctx) (MIR_alloc_error, \"Wrong alias number\");\n  return VARR_ADDR (string_t, aliases)[alias].str.s;\n}\n\n/* New Page */\n\n/* We attribute global vars to func as func can be inlined from different module.  */\ntypedef struct reg_desc {\n  MIR_type_t type;\n  MIR_reg_t reg;       /* key reg2rdn hash tab */\n  char *name;          /* key for the name2rdn hash tab */\n  char *hard_reg_name; /* NULL unless tied global, key for hrn2rdn */\n} reg_desc_t;\n\nDEF_VARR (reg_desc_t);\n\nDEF_HTAB (size_t);\n\ntypedef struct func_regs {\n  VARR (reg_desc_t) * reg_descs;\n  HTAB (size_t) * name2rdn_tab;\n  HTAB (size_t) * hrn2rdn_tab;\n  HTAB (size_t) * reg2rdn_tab;\n} *func_regs_t;\n\nstatic int name2rdn_eq (size_t rdn1, size_t rdn2, void *arg) {\n  func_regs_t func_regs = arg;\n  reg_desc_t *addr = VARR_ADDR (reg_desc_t, func_regs->reg_descs);\n\n  return strcmp (addr[rdn1].name, addr[rdn2].name) == 0;\n}\n\nstatic htab_hash_t name2rdn_hash (size_t rdn, void *arg) {\n  func_regs_t func_regs = arg;\n  reg_desc_t *addr = VARR_ADDR (reg_desc_t, func_regs->reg_descs);\n\n  return (htab_hash_t) mir_hash (addr[rdn].name, strlen (addr[rdn].name), 0);\n}\n\nstatic int hrn2rdn_eq (size_t rdn1, size_t rdn2, void *arg) {\n  func_regs_t func_regs = arg;\n  reg_desc_t *addr = VARR_ADDR (reg_desc_t, func_regs->reg_descs);\n\n  return strcmp (addr[rdn1].hard_reg_name, addr[rdn2].hard_reg_name) == 0;\n}\n\nstatic htab_hash_t hrn2rdn_hash (size_t rdn, void *arg) {\n  func_regs_t func_regs = arg;\n  reg_desc_t *addr = VARR_ADDR (reg_desc_t, func_regs->reg_descs);\n\n  return (htab_hash_t) mir_hash (addr[rdn].hard_reg_name, strlen (addr[rdn].hard_reg_name), 0);\n}\n\nstatic int reg2rdn_eq (size_t rdn1, size_t rdn2, void *arg) {\n  func_regs_t func_regs = arg;\n  reg_desc_t *addr = VARR_ADDR (reg_desc_t, func_regs->reg_descs);\n\n  return addr[rdn1].reg == addr[rdn2].reg;\n}\n\nstatic htab_hash_t reg2rdn_hash (size_t rdn, void *arg) {\n  func_regs_t func_regs = arg;\n  reg_desc_t *addr = VARR_ADDR (reg_desc_t, func_regs->reg_descs);\n\n  return (htab_hash_t) mir_hash_finish (mir_hash_step (mir_hash_init (0), addr[rdn].reg));\n}\n\nstatic void func_regs_init (MIR_context_t ctx, MIR_func_t func) {\n  func_regs_t func_regs;\n  reg_desc_t rd = {MIR_T_I64, 0, NULL, NULL};\n\n  if ((func_regs = func->internal = MIR_malloc (ctx->alloc, sizeof (struct func_regs))) == NULL)\n    MIR_get_error_func (ctx) (MIR_alloc_error, \"Not enough memory for func regs info\");\n  VARR_CREATE (reg_desc_t, func_regs->reg_descs, ctx->alloc, 50);\n  VARR_PUSH (reg_desc_t, func_regs->reg_descs, rd); /* for 0 reg */\n  HTAB_CREATE (size_t, func_regs->name2rdn_tab, ctx->alloc, 100, name2rdn_hash, name2rdn_eq, func_regs);\n  HTAB_CREATE (size_t, func_regs->hrn2rdn_tab, ctx->alloc, 10, hrn2rdn_hash, hrn2rdn_eq, func_regs);\n  HTAB_CREATE (size_t, func_regs->reg2rdn_tab, ctx->alloc, 100, reg2rdn_hash, reg2rdn_eq, func_regs);\n}\n\nstatic int target_locs_num (MIR_reg_t loc, MIR_type_t type);\nstatic int target_hard_reg_type_ok_p (MIR_reg_t hard_reg, MIR_type_t type);\nstatic int target_fixed_hard_reg_p (MIR_reg_t hard_reg);\n\nstatic MIR_reg_t create_func_reg (MIR_context_t ctx, MIR_func_t func, const char *name,\n                                  const char *hard_reg_name, MIR_reg_t reg, MIR_type_t type,\n                                  int any_p, char **name_ptr) {\n  func_regs_t func_regs = func->internal;\n  MIR_module_t func_module;\n  reg_desc_t rd, *rd_ref;\n  size_t rdn, tab_rdn;\n  int htab_res;\n  MIR_reg_t hr;\n\n  if (!any_p && _MIR_reserved_name_p (ctx, name))\n    MIR_get_error_func (ctx) (MIR_reserved_name_error, \"redefining a reserved name %s\", name);\n  rd.name = (char *) get_ctx_str (ctx, name);\n  if (hard_reg_name != NULL) hard_reg_name = get_ctx_str (ctx, hard_reg_name);\n  rd.hard_reg_name = (char *) hard_reg_name;\n  rd.type = type;\n  rd.reg = reg; /* 0 is reserved */\n  rdn = VARR_LENGTH (reg_desc_t, func_regs->reg_descs);\n  VARR_PUSH (reg_desc_t, func_regs->reg_descs, rd);\n  if (HTAB_DO (size_t, func_regs->name2rdn_tab, rdn, HTAB_FIND, tab_rdn)) {\n    VARR_POP (reg_desc_t, func_regs->reg_descs);\n    MIR_get_error_func (ctx) (MIR_repeated_decl_error, \"Repeated reg declaration %s\", name);\n  }\n  if (hard_reg_name != NULL) {\n    if ((hr = _MIR_get_hard_reg (ctx, hard_reg_name)) == MIR_NON_VAR) {\n      MIR_get_error_func (ctx) (MIR_hard_reg_error, \"unknown hard reg %s\", hard_reg_name);\n    } else if (!target_hard_reg_type_ok_p (hr, type)) {\n      MIR_get_error_func (ctx) (MIR_hard_reg_error,\n                                \"reg %s tied to hard reg %s can not be of type %s\", name,\n                                hard_reg_name, MIR_type_str (ctx, type));\n    } else if (target_fixed_hard_reg_p (hr)) {\n      MIR_get_error_func (ctx) (MIR_hard_reg_error,\n                                \"reg %s can not be tied to reserved hard reg %s\", name,\n                                hard_reg_name);\n    } else if (target_locs_num (hr, type) > 1)\n      MIR_get_error_func (ctx) (MIR_hard_reg_error, \"reg %s tied to %s requires more one hard reg\",\n                                name, hard_reg_name);\n    if (HTAB_DO (size_t, func_regs->hrn2rdn_tab, rdn, HTAB_FIND, tab_rdn)) {\n      rd_ref = &VARR_ADDR (reg_desc_t, func_regs->reg_descs)[tab_rdn];\n      if (type != rd_ref->type)\n        MIR_get_error_func (ctx) (MIR_repeated_decl_error,\n                                  \"regs %s and %s tied to hard reg %s have different types\", name,\n                                  rd_ref->name, hard_reg_name);\n      /* Use always one reg for global vars assigned to hard regs: */\n      VARR_POP (reg_desc_t, func_regs->reg_descs);\n      *name_ptr = rd_ref->name;\n      return rd_ref->reg;\n    }\n    func_module = func->func_item->module;\n    if (func_module->data == NULL)\n      func_module->data = bitmap_create2 (ctx->alloc, 128);\n    bitmap_set_bit_p (func_module->data, hr); /* hard regs used for globals */\n  }\n  *name_ptr = rd.name;\n  htab_res = HTAB_DO (size_t, func_regs->name2rdn_tab, rdn, HTAB_INSERT, tab_rdn);\n  mir_assert (!htab_res);\n  if (hard_reg_name != NULL) {\n    htab_res = HTAB_DO (size_t, func_regs->hrn2rdn_tab, rdn, HTAB_INSERT, tab_rdn);\n    mir_assert (!htab_res);\n  }\n  htab_res = HTAB_DO (size_t, func_regs->reg2rdn_tab, rdn, HTAB_INSERT, tab_rdn);\n  mir_assert (!htab_res);\n  return reg;\n}\n\nstatic void func_regs_finish (MIR_context_t ctx MIR_UNUSED, MIR_func_t func) {\n  func_regs_t func_regs = func->internal;\n\n  VARR_DESTROY (reg_desc_t, func_regs->reg_descs);\n  HTAB_DESTROY (size_t, func_regs->name2rdn_tab);\n  HTAB_DESTROY (size_t, func_regs->hrn2rdn_tab);\n  HTAB_DESTROY (size_t, func_regs->reg2rdn_tab);\n  MIR_free (ctx->alloc, func->internal);\n  func->internal = NULL;\n}\n\n/* New Page */\n\nstatic void push_data (MIR_context_t ctx, uint8_t *els, size_t size) {\n  for (size_t i = 0; i < size; i++) VARR_PUSH (uint8_t, temp_data, els[i]);\n}\n\nconst char *MIR_item_name (MIR_context_t ctx MIR_UNUSED, MIR_item_t item) {\n  mir_assert (item != NULL);\n  switch (item->item_type) {\n  case MIR_func_item: return item->u.func->name;\n  case MIR_proto_item: return item->u.proto->name;\n  case MIR_import_item: return item->u.import_id;\n  case MIR_export_item: return item->u.export_id;\n  case MIR_forward_item: return item->u.forward_id;\n  case MIR_bss_item: return item->u.bss->name;\n  case MIR_data_item: return item->u.data->name;\n  case MIR_ref_data_item: return item->u.ref_data->name;\n  case MIR_lref_data_item: return item->u.lref_data->name;\n  case MIR_expr_data_item: return item->u.expr_data->name;\n  default: mir_assert (FALSE); return NULL;\n  }\n}\n\nMIR_func_t MIR_get_item_func (MIR_context_t ctx MIR_UNUSED, MIR_item_t item) {\n  mir_assert (item != NULL);\n  if (item->item_type == MIR_func_item) {\n    return item->u.func;\n  } else {\n    return NULL;\n  }\n}\n\n#if !MIR_NO_IO\nstatic void io_init (MIR_context_t ctx);\nstatic void io_finish (MIR_context_t ctx);\n#endif\n\n#if !MIR_NO_SCAN\nstatic void scan_init (MIR_context_t ctx);\nstatic void scan_finish (MIR_context_t ctx);\n#endif\n\nstatic void simplify_init (MIR_context_t ctx);\nstatic void simplify_finish (MIR_context_t ctx);\n\nMIR_error_func_t MIR_get_error_func (MIR_context_t ctx) { return error_func; }  // ??? atomic\n\nvoid MIR_set_error_func (MIR_context_t ctx, MIR_error_func_t func) {  // ?? atomic access\n  error_func = func;\n}\n\nMIR_alloc_t MIR_get_alloc (MIR_context_t ctx) { return ctx->alloc; }\n\nint MIR_get_func_redef_permission_p (MIR_context_t ctx) { return func_redef_permission_p; }\n\nvoid MIR_set_func_redef_permission (MIR_context_t ctx, int enable_p) {  // ?? atomic access\n  func_redef_permission_p = enable_p;\n}\n\nstatic htab_hash_t item_hash (MIR_item_t it, void *arg MIR_UNUSED) {\n  return (htab_hash_t) mir_hash_finish (\n    mir_hash_step (mir_hash_step (mir_hash_init (28), (uint64_t) MIR_item_name (NULL, it)),\n                   (uint64_t) it->module));\n}\nstatic int item_eq (MIR_item_t it1, MIR_item_t it2, void *arg MIR_UNUSED) {\n  return it1->module == it2->module && MIR_item_name (NULL, it1) == MIR_item_name (NULL, it2);\n}\n\nstatic MIR_item_t item_tab_find (MIR_context_t ctx, const char *name, MIR_module_t module) {\n  MIR_item_t tab_item;\n  struct MIR_item item_s;\n  struct MIR_func func_s;\n\n  item_s.item_type = MIR_func_item;\n  func_s.name = name;\n  item_s.module = module;\n  item_s.u.func = &func_s;\n  if (HTAB_DO (MIR_item_t, module_item_tab, &item_s, HTAB_FIND, tab_item)) return tab_item;\n  return NULL;\n}\n\nstatic MIR_item_t item_tab_insert (MIR_context_t ctx, MIR_item_t item) {\n  MIR_item_t tab_item;\n\n  HTAB_DO (MIR_item_t, module_item_tab, item, HTAB_INSERT, tab_item);\n  return tab_item;\n}\n\nstatic void item_tab_remove (MIR_context_t ctx, MIR_item_t item) {\n  HTAB_DO (MIR_item_t, module_item_tab, item, HTAB_DELETE, item);\n}\n\nstatic void init_module (MIR_context_t ctx, MIR_module_t m, const char *name) {\n  m->data = NULL;\n  m->last_temp_item_num = 0;\n  m->name = get_ctx_str (ctx, name);\n  DLIST_INIT (MIR_item_t, m->items);\n}\n\nstatic void code_init (MIR_context_t ctx);\nstatic void code_finish (MIR_context_t ctx);\n\ndouble _MIR_get_api_version (void) { return MIR_API_VERSION; }\n\nstatic void hard_reg_name_init (MIR_context_t ctx);\nstatic void hard_reg_name_finish (MIR_context_t ctx);\n\n#include \"mir-alloc-default.c\"\n#include \"mir-code-alloc-default.c\"\n\nMIR_context_t _MIR_init (MIR_alloc_t alloc, MIR_code_alloc_t code_alloc) {\n  MIR_context_t ctx;\n\n  if (alloc == NULL)\n    alloc = &default_alloc;\n  if (code_alloc == NULL)\n    code_alloc = &default_code_alloc;\n\n  mir_assert (MIR_OP_BOUND < OUT_FLAG);\n  if ((ctx = MIR_malloc (alloc, sizeof (struct MIR_context))) == NULL)\n    default_error (MIR_alloc_error, \"Not enough memory for ctx\");\n  ctx->string_ctx = NULL;\n  ctx->alias_ctx = NULL;\n  ctx->reg_ctx = NULL;\n  ctx->simplify_ctx = NULL;\n  ctx->machine_code_ctx = NULL;\n  ctx->io_ctx = NULL;\n  ctx->scan_ctx = NULL;\n  ctx->hard_reg_ctx = NULL;\n  ctx->interp_ctx = NULL;\n#ifndef NDEBUG\n  for (MIR_insn_code_t c = 0; c < MIR_INVALID_INSN; c++) mir_assert (c == insn_descs[c].code);\n#endif\n  ctx->alloc = alloc;\n  ctx->code_alloc = code_alloc;\n  error_func = default_error;\n  func_redef_permission_p = FALSE;\n  curr_module = NULL;\n  curr_func = NULL;\n  curr_label_num = 0;\n  if ((ctx->string_ctx = MIR_malloc (alloc, sizeof (struct string_ctx))) == NULL\n      || (ctx->alias_ctx = MIR_malloc (alloc, sizeof (struct string_ctx))) == NULL)\n    MIR_get_error_func (ctx) (MIR_alloc_error, \"Not enough memory for ctx\");\n  string_init (alloc, &strings, &string_tab);\n  string_init (alloc, &aliases, &alias_tab);\n  VARR_CREATE (MIR_proto_t, unspec_protos, alloc, 0);\n  check_and_prepare_insn_descs (ctx);\n  DLIST_INIT (MIR_module_t, all_modules);\n  simplify_init (ctx);\n  VARR_CREATE (char, temp_string, alloc, 64);\n  VARR_CREATE (uint8_t, temp_data, alloc, 512);\n  VARR_CREATE (uint8_t, used_label_p, alloc, 512);\n#if !MIR_NO_IO\n  io_init (ctx);\n#endif\n#if !MIR_NO_SCAN\n  scan_init (ctx);\n#endif\n  VARR_CREATE (MIR_module_t, modules_to_link, alloc, 0);\n  VARR_CREATE (MIR_op_t, temp_ops, alloc, 0);\n  init_module (ctx, &environment_module, \".environment\");\n  HTAB_CREATE (MIR_item_t, module_item_tab, ctx->alloc, 512, item_hash, item_eq, NULL);\n  setjmp_addr = NULL;\n  code_init (ctx);\n  wrapper_end_addr = _MIR_get_wrapper_end (ctx); /* should be after code_init */\n  hard_reg_name_init (ctx);\n  interp_init (ctx);\n  return ctx;\n}\n\nstatic void remove_insn (MIR_context_t ctx, MIR_item_t func_item, MIR_insn_t insn,\n                         DLIST (MIR_insn_t) * insns) {\n  mir_assert (func_item != NULL);\n  if (func_item->item_type != MIR_func_item)\n    MIR_get_error_func (ctx) (MIR_wrong_param_value_error, \"MIR_remove_insn: wrong func item\");\n  DLIST_REMOVE (MIR_insn_t, *insns, insn);\n  MIR_free (ctx->alloc, insn);\n}\n\nvoid MIR_remove_insn (MIR_context_t ctx, MIR_item_t func_item, MIR_insn_t insn) {\n  remove_insn (ctx, func_item, insn, &func_item->u.func->insns);\n}\n\nstatic void remove_func_insns (MIR_context_t ctx, MIR_item_t func_item,\n                               DLIST (MIR_insn_t) * insns) {\n  MIR_insn_t insn;\n\n  mir_assert (func_item->item_type == MIR_func_item);\n  while ((insn = DLIST_HEAD (MIR_insn_t, *insns)) != NULL) {\n    remove_insn (ctx, func_item, insn, insns);\n  }\n}\n\nstatic void remove_item (MIR_context_t ctx, MIR_item_t item) {\n  switch (item->item_type) {\n  case MIR_func_item:\n    remove_func_insns (ctx, item, &item->u.func->insns);\n    remove_func_insns (ctx, item, &item->u.func->original_insns);\n    VARR_DESTROY (MIR_var_t, item->u.func->vars);\n    if (item->u.func->global_vars != NULL) VARR_DESTROY (MIR_var_t, item->u.func->global_vars);\n    func_regs_finish (ctx, item->u.func);\n    MIR_free (ctx->alloc, item->u.func);\n    break;\n  case MIR_proto_item:\n    VARR_DESTROY (MIR_var_t, item->u.proto->args);\n    MIR_free (ctx->alloc, item->u.proto);\n    break;\n  case MIR_import_item:\n  case MIR_export_item:\n  case MIR_forward_item: break;\n  case MIR_data_item:\n    if (item->addr != NULL && item->section_head_p)\n      MIR_free (ctx->alloc, item->addr);\n    MIR_free (ctx->alloc, item->u.data);\n    break;\n  case MIR_ref_data_item:\n    if (item->addr != NULL && item->section_head_p)\n      MIR_free (ctx->alloc, item->addr);\n    MIR_free (ctx->alloc, item->u.ref_data);\n    break;\n  case MIR_lref_data_item:\n    if (item->addr != NULL && item->section_head_p)\n      MIR_free (ctx->alloc, item->addr);\n    MIR_free (ctx->alloc, item->u.lref_data);\n    break;\n  case MIR_expr_data_item:\n    if (item->addr != NULL && item->section_head_p)\n      MIR_free (ctx->alloc, item->addr);\n    MIR_free (ctx->alloc, item->u.expr_data);\n    break;\n  case MIR_bss_item:\n    if (item->addr != NULL && item->section_head_p)\n      MIR_free (ctx->alloc, item->addr);\n    MIR_free (ctx->alloc, item->u.bss);\n    break;\n  default: mir_assert (FALSE);\n  }\n  if (item->data != NULL)\n    MIR_free (ctx->alloc, item->data);\n  MIR_free (ctx->alloc, item);\n}\n\nstatic void remove_module (MIR_context_t ctx, MIR_module_t module, int free_module_p) {\n  MIR_item_t item;\n\n  while ((item = DLIST_HEAD (MIR_item_t, module->items)) != NULL) {\n    DLIST_REMOVE (MIR_item_t, module->items, item);\n    remove_item (ctx, item);\n  }\n  if (module->data != NULL)\n    bitmap_destroy (module->data);\n  if (free_module_p)\n    MIR_free (ctx->alloc, module);\n}\n\nstatic void remove_all_modules (MIR_context_t ctx) {\n  MIR_module_t module;\n\n  while ((module = DLIST_HEAD (MIR_module_t, all_modules)) != NULL) {\n    DLIST_REMOVE (MIR_module_t, all_modules, module);\n    remove_module (ctx, module, TRUE);\n  }\n  remove_module (ctx, &environment_module, FALSE);\n}\n\nvoid MIR_finish (MIR_context_t ctx) {\n  interp_finish (ctx);\n  remove_all_modules (ctx);\n  HTAB_DESTROY (MIR_item_t, module_item_tab);\n  VARR_DESTROY (MIR_module_t, modules_to_link);\n  VARR_DESTROY (MIR_op_t, temp_ops);\n#if !MIR_NO_SCAN\n  scan_finish (ctx);\n#endif\n#if !MIR_NO_IO\n  io_finish (ctx);\n#endif\n  VARR_DESTROY (uint8_t, temp_data);\n  VARR_DESTROY (uint8_t, used_label_p);\n  VARR_DESTROY (char, temp_string);\n  while (VARR_LENGTH (MIR_proto_t, unspec_protos) != 0) {\n    MIR_proto_t proto = VARR_POP (MIR_proto_t, unspec_protos);\n    VARR_DESTROY (MIR_var_t, proto->args);\n    MIR_free (ctx->alloc, proto);\n  }\n  VARR_DESTROY (MIR_proto_t, unspec_protos);\n  string_finish (ctx->alloc, &strings, &string_tab);\n  string_finish (ctx->alloc, &aliases, &alias_tab);\n  simplify_finish (ctx);\n  VARR_DESTROY (size_t, insn_nops);\n  code_finish (ctx);\n  hard_reg_name_finish (ctx);\n  if (curr_func != NULL)\n    MIR_get_error_func (ctx) (MIR_finish_error, \"finish when function %s is not finished\",\n                              curr_func->name);\n  if (curr_module != NULL)\n    MIR_get_error_func (ctx) (MIR_finish_error, \"finish when module %s is not finished\",\n                              curr_module->name);\n  MIR_free (ctx->alloc, ctx->string_ctx);\n  MIR_free (ctx->alloc, ctx->alias_ctx);\n  MIR_free (ctx->alloc, ctx);\n  ctx = NULL;\n}\n\nMIR_module_t MIR_new_module (MIR_context_t ctx, const char *name) {\n  if (curr_module != NULL)\n    MIR_get_error_func (ctx) (MIR_nested_module_error,\n                              \"Creating module when previous module %s is not finished\",\n                              curr_module->name);\n  if ((curr_module = MIR_malloc (ctx->alloc, sizeof (struct MIR_module))) == NULL)\n    MIR_get_error_func (ctx) (MIR_alloc_error, \"Not enough memory for module %s creation\", name);\n  init_module (ctx, curr_module, name);\n  DLIST_APPEND (MIR_module_t, all_modules, curr_module);\n  return curr_module;\n}\n\nDLIST (MIR_module_t) * MIR_get_module_list (MIR_context_t ctx) { return &all_modules; }\n\nstatic const char *type_str (MIR_context_t ctx, MIR_type_t tp) {\n  int n;\n  char str[100];\n\n  switch (tp) {\n  case MIR_T_I8: return \"i8\";\n  case MIR_T_U8: return \"u8\";\n  case MIR_T_I16: return \"i16\";\n  case MIR_T_U16: return \"u16\";\n  case MIR_T_I32: return \"i32\";\n  case MIR_T_U32: return \"u32\";\n  case MIR_T_I64: return \"i64\";\n  case MIR_T_U64: return \"u64\";\n  case MIR_T_F: return \"f\";\n  case MIR_T_D: return \"d\";\n  case MIR_T_LD: return \"ld\";\n  case MIR_T_P: return \"p\";\n  case MIR_T_RBLK: return \"rblk\";\n  case MIR_T_UNDEF: return \"undef\";\n  default:\n    if (MIR_blk_type_p (tp) && (n = tp - MIR_T_BLK) >= 0 && n < MIR_BLK_NUM) {\n      sprintf (str, \"blk%d\", n);\n      return get_ctx_str (ctx, str);\n    }\n    return \"\";\n  }\n}\n\nconst char *MIR_type_str (MIR_context_t ctx, MIR_type_t tp) {\n  const char *str = type_str (ctx, tp);\n\n  if (strcmp (str, \"\") == 0)\n    MIR_get_error_func (ctx) (MIR_wrong_param_value_error, \"MIR_type_str: wrong type\");\n  return str;\n}\n\nstatic const char *mode_str (MIR_op_mode_t mode) {\n  switch (mode) {\n  case MIR_OP_REG: return \"reg\";\n  case MIR_OP_VAR: return \"var\";\n  case MIR_OP_INT: return \"int\";\n  case MIR_OP_UINT: return \"uint\";\n  case MIR_OP_FLOAT: return \"float\";\n  case MIR_OP_DOUBLE: return \"double\";\n  case MIR_OP_LDOUBLE: return \"ldouble\";\n  case MIR_OP_REF: return \"ref\";\n  case MIR_OP_STR: return \"str\";\n  case MIR_OP_MEM: return \"mem\";\n  case MIR_OP_VAR_MEM: return \"var_mem\";\n  case MIR_OP_LABEL: return \"label\";\n  case MIR_OP_BOUND: return \"bound\";\n  case MIR_OP_UNDEF: return \"undef\";\n  default: return \"\";\n  }\n}\n\nstatic MIR_item_t add_item (MIR_context_t ctx, MIR_item_t item) {\n  int replace_p;\n  MIR_item_t tab_item;\n\n  if ((tab_item = item_tab_find (ctx, MIR_item_name (ctx, item), item->module)) == NULL) {\n    DLIST_APPEND (MIR_item_t, curr_module->items, item);\n    HTAB_DO (MIR_item_t, module_item_tab, item, HTAB_INSERT, item);\n    return item;\n  }\n  switch (tab_item->item_type) {\n  case MIR_import_item:\n    if (item->item_type != MIR_import_item)\n      MIR_get_error_func (ctx) (MIR_import_export_error,\n                                \"existing module definition %s already defined as import\",\n                                tab_item->u.import_id);\n    item = tab_item;\n    break;\n  case MIR_export_item:\n  case MIR_forward_item:\n    replace_p = FALSE;\n    if (item->item_type == MIR_import_item) {\n      MIR_get_error_func (ctx) (MIR_import_export_error, \"export/forward of import %s\",\n                                item->u.import_id);\n    } else if (item->item_type != MIR_export_item && item->item_type != MIR_forward_item) {\n      replace_p = TRUE;\n      DLIST_APPEND (MIR_item_t, curr_module->items, item);\n    } else {\n      if (tab_item->item_type == item->item_type)\n        item = tab_item;\n      else\n        DLIST_APPEND (MIR_item_t, curr_module->items, item);\n      if (item->item_type == MIR_export_item && tab_item->item_type == MIR_forward_item)\n        replace_p = TRUE;\n    }\n    if (replace_p) { /* replace forward by export or export/forward by its definition: */\n      tab_item->ref_def = item;\n      if (tab_item->item_type == MIR_export_item) item->export_p = TRUE;\n      item_tab_remove (ctx, tab_item);\n      tab_item = item_tab_insert (ctx, item);\n      mir_assert (item == tab_item);\n    }\n    break;\n  case MIR_proto_item:\n    MIR_get_error_func (ctx) (MIR_repeated_decl_error, \"item %s was already defined as proto\",\n                              tab_item->u.proto->name);\n    break;\n  case MIR_bss_item:\n  case MIR_data_item:\n  case MIR_ref_data_item:\n  case MIR_lref_data_item:\n  case MIR_expr_data_item:\n  case MIR_func_item:\n    if (item->item_type == MIR_export_item) {\n      if (tab_item->export_p) {\n        item = tab_item;\n      } else { /* just keep one export: */\n        tab_item->export_p = TRUE;\n        DLIST_APPEND (MIR_item_t, curr_module->items, item);\n        item->ref_def = tab_item;\n      }\n    } else if (item->item_type == MIR_forward_item) {\n      DLIST_APPEND (MIR_item_t, curr_module->items, item);\n      item->ref_def = tab_item;\n    } else if (item->item_type == MIR_import_item) {\n      MIR_get_error_func (ctx) (MIR_import_export_error, \"import of local definition %s\",\n                                item->u.import_id);\n    } else {\n      MIR_get_error_func (ctx) (MIR_repeated_decl_error, \"Repeated item declaration %s\",\n                                MIR_item_name (ctx, item));\n    }\n    break;\n  default: mir_assert (FALSE);\n  }\n  return item;\n}\n\nstatic MIR_item_t create_item (MIR_context_t ctx, MIR_item_type_t item_type,\n                               const char *item_name) {\n  MIR_item_t item;\n\n  if (curr_module == NULL)\n    MIR_get_error_func (ctx) (MIR_no_module_error, \"%s outside module\", item_name);\n  if ((item = MIR_malloc (ctx->alloc, sizeof (struct MIR_item))) == NULL)\n    MIR_get_error_func (ctx) (MIR_alloc_error, \"Not enough memory for creation of item %s\",\n                              item_name);\n  item->data = NULL;\n  item->module = curr_module;\n  item->item_type = item_type;\n  item->ref_def = NULL;\n  item->export_p = FALSE;\n  item->section_head_p = FALSE;\n  item->addr = NULL;\n  return item;\n}\n\nstatic MIR_item_t new_export_import_forward (MIR_context_t ctx, const char *name,\n                                             MIR_item_type_t item_type, const char *item_name,\n                                             int create_only_p) {\n  MIR_item_t item, tab_item;\n  const char *uniq_name;\n\n  item = create_item (ctx, item_type, item_name);\n  uniq_name = get_ctx_str (ctx, name);\n  if (item_type == MIR_export_item)\n    item->u.export_id = uniq_name;\n  else if (item_type == MIR_import_item)\n    item->u.import_id = uniq_name;\n  else\n    item->u.forward_id = uniq_name;\n  if (create_only_p) return item;\n  if ((tab_item = add_item (ctx, item)) != item) {\n    MIR_free (ctx->alloc, item);\n    item = tab_item;\n  }\n  return item;\n}\n\nMIR_item_t MIR_new_export (MIR_context_t ctx, const char *name) {\n  return new_export_import_forward (ctx, name, MIR_export_item, \"export\", FALSE);\n}\n\nMIR_item_t MIR_new_import (MIR_context_t ctx, const char *name) {\n  return new_export_import_forward (ctx, name, MIR_import_item, \"import\", FALSE);\n}\n\nMIR_item_t MIR_new_forward (MIR_context_t ctx, const char *name) {\n  return new_export_import_forward (ctx, name, MIR_forward_item, \"forward\", FALSE);\n}\n\nMIR_item_t MIR_new_bss (MIR_context_t ctx, const char *name, size_t len) {\n  MIR_item_t tab_item, item = create_item (ctx, MIR_bss_item, \"bss\");\n\n  item->u.bss = MIR_malloc (ctx->alloc, sizeof (struct MIR_bss));\n  if (item->u.bss == NULL) {\n    MIR_free (ctx->alloc, item);\n    MIR_get_error_func (ctx) (MIR_alloc_error, \"Not enough memory for creation of bss %s\", name);\n  }\n  if (name != NULL) name = get_ctx_str (ctx, name);\n  item->u.bss->name = name;\n  item->u.bss->len = len;\n  if (name == NULL) {\n    DLIST_APPEND (MIR_item_t, curr_module->items, item);\n  } else if ((tab_item = add_item (ctx, item)) != item) {\n    MIR_free (ctx->alloc, item);\n    item = tab_item;\n  }\n  return item;\n}\n\nstatic MIR_type_t canon_type (MIR_type_t type) {\n#if defined(_WIN32) || __SIZEOF_LONG_DOUBLE__ == 8\n  if (type == MIR_T_LD) type = MIR_T_D;\n#endif\n  return type;\n}\n\nsize_t _MIR_type_size (MIR_context_t ctx MIR_UNUSED, MIR_type_t type) {\n  switch (type) {\n  case MIR_T_I8: return sizeof (int8_t);\n  case MIR_T_U8: return sizeof (uint8_t);\n  case MIR_T_I16: return sizeof (int16_t);\n  case MIR_T_U16: return sizeof (uint16_t);\n  case MIR_T_I32: return sizeof (int32_t);\n  case MIR_T_U32: return sizeof (uint32_t);\n  case MIR_T_I64: return sizeof (int64_t);\n  case MIR_T_U64: return sizeof (uint64_t);\n  case MIR_T_F: return sizeof (float);\n  case MIR_T_D: return sizeof (double);\n  case MIR_T_LD: return sizeof (long double);\n  case MIR_T_P: return sizeof (void *);\n  default: mir_assert (FALSE); return 1;\n  }\n}\n\nstatic int wrong_type_p (MIR_type_t type) { return type < MIR_T_I8 || type >= MIR_T_BLK; }\n\nMIR_item_t MIR_new_data (MIR_context_t ctx, const char *name, MIR_type_t el_type, size_t nel,\n                         const void *els) {\n  MIR_item_t tab_item, item = create_item (ctx, MIR_data_item, \"data\");\n  MIR_data_t data;\n  size_t el_len;\n\n  if (wrong_type_p (el_type)) {\n    MIR_free (ctx->alloc, item);\n    MIR_get_error_func (ctx) (MIR_wrong_type_error, \"wrong type in data %s\", name);\n  }\n  el_len = _MIR_type_size (ctx, el_type);\n  item->u.data = data = MIR_malloc (ctx->alloc, sizeof (struct MIR_data) + el_len * nel);\n  if (data == NULL) {\n    MIR_free (ctx->alloc, item);\n    MIR_get_error_func (ctx) (MIR_alloc_error, \"Not enough memory for creation of data %s\",\n                              name == NULL ? \"\" : name);\n  }\n  if (name != NULL) name = get_ctx_str (ctx, name);\n  data->name = name;\n  if (name == NULL) {\n    DLIST_APPEND (MIR_item_t, curr_module->items, item);\n  } else if ((tab_item = add_item (ctx, item)) != item) {\n    MIR_free (ctx->alloc, item);\n    item = tab_item;\n  }\n  data->el_type = canon_type (el_type);\n  data->nel = nel;\n  memcpy (data->u.els, els, el_len * nel);\n  return item;\n}\n\nMIR_item_t MIR_new_string_data (MIR_context_t ctx, const char *name, MIR_str_t str) {\n  return MIR_new_data (ctx, name, MIR_T_U8, str.len, str.s);\n}\n\nMIR_item_t MIR_new_ref_data (MIR_context_t ctx, const char *name, MIR_item_t ref_item,\n                             int64_t disp) {\n  MIR_item_t tab_item, item = create_item (ctx, MIR_ref_data_item, \"ref data\");\n  MIR_ref_data_t ref_data;\n\n  item->u.ref_data = ref_data = MIR_malloc (ctx->alloc, sizeof (struct MIR_ref_data));\n  if (ref_data == NULL) {\n    MIR_free (ctx->alloc, item);\n    MIR_get_error_func (ctx) (MIR_alloc_error, \"Not enough memory for creation of ref data %s\",\n                              name == NULL ? \"\" : name);\n  }\n  if (name != NULL) name = get_ctx_str (ctx, name);\n  ref_data->name = name;\n  ref_data->ref_item = ref_item;\n  ref_data->disp = disp;\n  if (name == NULL) {\n    DLIST_APPEND (MIR_item_t, curr_module->items, item);\n  } else if ((tab_item = add_item (ctx, item)) != item) {\n    MIR_free (ctx->alloc, item);\n    item = tab_item;\n  }\n  return item;\n}\n\nMIR_item_t MIR_new_lref_data (MIR_context_t ctx, const char *name, MIR_label_t label,\n                              MIR_label_t label2, int64_t disp) {\n  MIR_item_t tab_item, item = create_item (ctx, MIR_lref_data_item, \"lref data\");\n  MIR_lref_data_t lref_data;\n\n  if (label == NULL) {\n    MIR_free (ctx->alloc, item);\n    MIR_get_error_func (ctx) (MIR_alloc_error, \"null label for lref data %s\",\n                              name == NULL ? \"\" : name);\n  }\n  item->u.lref_data = lref_data = MIR_malloc (ctx->alloc, sizeof (struct MIR_lref_data));\n  if (lref_data == NULL) {\n    MIR_free (ctx->alloc, item);\n    MIR_get_error_func (ctx) (MIR_alloc_error, \"Not enough memory for creation of lref data %s\",\n                              name == NULL ? \"\" : name);\n  }\n  if (name != NULL) name = get_ctx_str (ctx, name);\n  lref_data->name = name;\n  lref_data->label = label;\n  lref_data->label2 = label2;\n  lref_data->disp = disp;\n  lref_data->orig_label = lref_data->orig_label2 = NULL;\n  lref_data->next = NULL;\n  if (name == NULL) {\n    DLIST_APPEND (MIR_item_t, curr_module->items, item);\n  } else if ((tab_item = add_item (ctx, item)) != item) {\n    MIR_free (ctx->alloc, item);\n    item = tab_item;\n  }\n  return item;\n}\n\nMIR_item_t MIR_new_expr_data (MIR_context_t ctx, const char *name, MIR_item_t expr_item) {\n  MIR_item_t tab_item, item = create_item (ctx, MIR_expr_data_item, \"expr data\");\n  MIR_expr_data_t expr_data;\n\n  item->u.expr_data = expr_data = MIR_malloc (ctx->alloc, sizeof (struct MIR_expr_data));\n  if (expr_data == NULL) {\n    MIR_free (ctx->alloc, item);\n    MIR_get_error_func (ctx) (MIR_alloc_error, \"Not enough memory for creation of expr data %s\",\n                              name == NULL ? \"\" : name);\n  }\n  mir_assert (expr_item != NULL);\n  if (expr_item->item_type != MIR_func_item || expr_item->u.func->vararg_p\n      || expr_item->u.func->nargs != 0 || expr_item->u.func->nres != 1)\n    MIR_get_error_func (\n      ctx) (MIR_binary_io_error,\n            \"%s can not be an expr which should be non-argument, one result function\",\n            MIR_item_name (ctx, expr_item));\n  if (name != NULL) name = get_ctx_str (ctx, name);\n  expr_data->name = name;\n  expr_data->expr_item = expr_item;\n  if (name == NULL) {\n    DLIST_APPEND (MIR_item_t, curr_module->items, item);\n  } else if ((tab_item = add_item (ctx, item)) != item) {\n    MIR_free (ctx->alloc, item);\n    item = tab_item;\n  }\n  return item;\n}\n\nstatic MIR_proto_t create_proto (MIR_context_t ctx, const char *name, size_t nres,\n                                 MIR_type_t *res_types, size_t nargs, int vararg_p,\n                                 MIR_var_t *args) {\n  MIR_proto_t proto = MIR_malloc (ctx->alloc, sizeof (struct MIR_proto) + nres * sizeof (MIR_type_t));\n  MIR_var_t arg;\n\n  if (proto == NULL)\n    MIR_get_error_func (ctx) (MIR_alloc_error, \"Not enough memory for creation of proto %s\", name);\n  proto->name = get_ctx_str (ctx, name);\n  proto->res_types = (MIR_type_t *) ((char *) proto + sizeof (struct MIR_proto));\n  if (nres != 0) memcpy (proto->res_types, res_types, nres * sizeof (MIR_type_t));\n  proto->nres = (uint32_t) nres;\n  proto->vararg_p = vararg_p != 0;\n  VARR_CREATE (MIR_var_t, proto->args, ctx->alloc, nargs);\n  for (size_t i = 0; i < nargs; i++) {\n    arg = args[i];\n    arg.name = get_ctx_str (ctx, arg.name);\n    VARR_PUSH (MIR_var_t, proto->args, arg);\n  }\n  return proto;\n}\n\nstatic MIR_item_t new_proto_arr (MIR_context_t ctx, const char *name, size_t nres,\n                                 MIR_type_t *res_types, size_t nargs, int vararg_p,\n                                 MIR_var_t *args) {\n  MIR_item_t proto_item, tab_item;\n\n  if (curr_module == NULL)\n    MIR_get_error_func (ctx) (MIR_no_module_error, \"Creating proto %s outside module\", name);\n  for (size_t i = 0; i < nres; i++)\n    if (wrong_type_p (res_types[i]))\n      MIR_get_error_func (ctx) (MIR_wrong_type_error, \"wrong result type in proto %s\", name);\n  proto_item = create_item (ctx, MIR_proto_item, \"proto\");\n  proto_item->u.proto = create_proto (ctx, name, nres, res_types, nargs, vararg_p, args);\n  tab_item = add_item (ctx, proto_item);\n  mir_assert (tab_item == proto_item);\n  return proto_item;\n}\n\nMIR_item_t MIR_new_proto_arr (MIR_context_t ctx, const char *name, size_t nres,\n                              MIR_type_t *res_types, size_t nargs, MIR_var_t *args) {\n  return new_proto_arr (ctx, name, nres, res_types, nargs, FALSE, args);\n}\n\nMIR_item_t MIR_new_vararg_proto_arr (MIR_context_t ctx, const char *name, size_t nres,\n                                     MIR_type_t *res_types, size_t nargs, MIR_var_t *args) {\n  return new_proto_arr (ctx, name, nres, res_types, nargs, TRUE, args);\n}\n\n#if defined(_MSC_VER)\n#define alloca _alloca\n#endif\n\nstatic MIR_item_t new_proto (MIR_context_t ctx, const char *name, size_t nres,\n                             MIR_type_t *res_types, size_t nargs, int vararg_p, va_list argp) {\n  MIR_var_t *args = alloca (nargs * sizeof (MIR_var_t));\n  size_t i;\n\n  for (i = 0; i < nargs; i++) {\n    args[i].type = va_arg (argp, MIR_type_t);\n    args[i].name = va_arg (argp, const char *);\n  }\n  return new_proto_arr (ctx, name, nres, res_types, nargs, vararg_p, args);\n}\n\nMIR_item_t MIR_new_proto (MIR_context_t ctx, const char *name, size_t nres, MIR_type_t *res_types,\n                          size_t nargs, ...) {\n  va_list argp;\n  MIR_item_t proto_item;\n\n  va_start (argp, nargs);\n  proto_item = new_proto (ctx, name, nres, res_types, nargs, FALSE, argp);\n  va_end (argp);\n  return proto_item;\n}\n\nMIR_item_t MIR_new_vararg_proto (MIR_context_t ctx, const char *name, size_t nres,\n                                 MIR_type_t *res_types, size_t nargs, ...) {\n  va_list argp;\n  MIR_item_t proto_item;\n\n  va_start (argp, nargs);\n  proto_item = new_proto (ctx, name, nres, res_types, nargs, TRUE, argp);\n  va_end (argp);\n  return proto_item;\n}\n\nstatic MIR_item_t new_func_arr (MIR_context_t ctx, const char *name, size_t nres,\n                                MIR_type_t *res_types, size_t nargs, int vararg_p,\n                                MIR_var_t *vars) {\n  MIR_item_t func_item, tab_item;\n  MIR_func_t func;\n\n  if (curr_func != NULL)\n    MIR_get_error_func (ctx) (MIR_nested_func_error,\n                              \"Creating function when previous function %s is not finished\",\n                              curr_func->name);\n  if (nargs == 0 && vararg_p)\n    MIR_get_error_func (ctx) (MIR_vararg_func_error,\n                              \"Variable arg function %s w/o any mandatory argument\", name);\n  for (size_t i = 0; i < nres; i++)\n    if (wrong_type_p (res_types[i]))\n      MIR_get_error_func (ctx) (MIR_wrong_type_error, \"wrong result type in func %s\", name);\n  func_item = create_item (ctx, MIR_func_item, \"function\");\n  curr_func = func_item->u.func = func\n    = MIR_malloc (ctx->alloc, sizeof (struct MIR_func) + nres * sizeof (MIR_type_t));\n  if (func == NULL) {\n    MIR_free (ctx->alloc, func_item);\n    MIR_get_error_func (ctx) (MIR_alloc_error, \"Not enough memory for creation of func %s\", name);\n  }\n  func->name = get_ctx_str (ctx, name);\n  func->func_item = func_item;\n  func->nres = (uint32_t) nres;\n  func->res_types = (MIR_type_t *) ((char *) func + sizeof (struct MIR_func));\n  for (size_t i = 0; i < nres; i++) func->res_types[i] = canon_type (res_types[i]);\n  tab_item = add_item (ctx, func_item);\n  mir_assert (tab_item == func_item);\n  DLIST_INIT (MIR_insn_t, func->insns);\n  DLIST_INIT (MIR_insn_t, func->original_insns);\n  VARR_CREATE (MIR_var_t, func->vars, ctx->alloc, nargs + 8);\n  func->global_vars = NULL;\n  func->nargs = (uint32_t) nargs;\n  func->last_temp_num = 0;\n  func->vararg_p = vararg_p != 0;\n  func->expr_p = func->jret_p = FALSE;\n  func->n_inlines = 0;\n  func->machine_code = func->call_addr = NULL;\n  func->first_lref = NULL;\n  func_regs_init (ctx, func);\n  for (size_t i = 0; i < nargs; i++) {\n    char *stored_name;\n    MIR_type_t type = canon_type (vars[i].type);\n    MIR_reg_t reg\n      = create_func_reg (ctx, func, vars[i].name, NULL, (MIR_reg_t) (i + 1),\n                         type == MIR_T_F || type == MIR_T_D || type == MIR_T_LD ? type : MIR_T_I64,\n                         FALSE, &stored_name);\n    mir_assert (i + 1 == reg);\n    vars[i].name = stored_name;\n    VARR_PUSH (MIR_var_t, func->vars, vars[i]);\n  }\n  return func_item;\n}\n\nMIR_item_t MIR_new_func_arr (MIR_context_t ctx, const char *name, size_t nres,\n                             MIR_type_t *res_types, size_t nargs, MIR_var_t *vars) {\n  return new_func_arr (ctx, name, nres, res_types, nargs, FALSE, vars);\n}\n\nMIR_item_t MIR_new_vararg_func_arr (MIR_context_t ctx, const char *name, size_t nres,\n                                    MIR_type_t *res_types, size_t nargs, MIR_var_t *vars) {\n  return new_func_arr (ctx, name, nres, res_types, nargs, TRUE, vars);\n}\n\nstatic MIR_item_t new_func (MIR_context_t ctx, const char *name, size_t nres, MIR_type_t *res_types,\n                            size_t nargs, int vararg_p, va_list argp) {\n  size_t i;\n  MIR_var_t *vars = alloca (sizeof (MIR_var_t) * nargs);\n\n  for (i = 0; i < nargs; i++) {\n    vars[i].type = va_arg (argp, MIR_type_t);\n    vars[i].name = va_arg (argp, const char *);\n  }\n  return new_func_arr (ctx, name, nres, res_types, nargs, vararg_p, vars);\n}\n\nMIR_item_t MIR_new_func (MIR_context_t ctx, const char *name, size_t nres, MIR_type_t *res_types,\n                         size_t nargs, ...) {\n  va_list argp;\n  MIR_item_t func_item;\n\n  va_start (argp, nargs);\n  func_item = new_func (ctx, name, nres, res_types, nargs, FALSE, argp);\n  va_end (argp);\n  return func_item;\n}\n\nMIR_item_t MIR_new_vararg_func (MIR_context_t ctx, const char *name, size_t nres,\n                                MIR_type_t *res_types, size_t nargs, ...) {\n  va_list argp;\n  MIR_item_t func_item;\n\n  va_start (argp, nargs);\n  func_item = new_func (ctx, name, nres, res_types, nargs, TRUE, argp);\n  va_end (argp);\n  return func_item;\n}\n\nstatic MIR_reg_t new_func_reg (MIR_context_t ctx, MIR_func_t func, MIR_type_t type,\n                               const char *name, const char *hard_reg_name) {\n  MIR_var_t var;\n  MIR_reg_t res, reg;\n  char *stored_name;\n\n  if (func == NULL)\n    MIR_get_error_func (ctx) (MIR_reg_type_error, \"func can not be NULL for new reg creation\");\n  if (type != MIR_T_I64 && type != MIR_T_F && type != MIR_T_D && type != MIR_T_LD)\n    MIR_get_error_func (ctx) (MIR_reg_type_error, \"wrong type for var %s: got '%s'\", name,\n                              type_str (ctx, type));\n  reg = (MIR_reg_t) VARR_LENGTH (MIR_var_t, func->vars) + 1;\n  if (func->global_vars != NULL) reg += (MIR_reg_t) VARR_LENGTH (MIR_var_t, func->global_vars);\n  res = create_func_reg (ctx, func, name, hard_reg_name, reg, type, FALSE, &stored_name);\n  if (res != reg) return res; /* already exists */\n  var.type = type;\n  var.name = stored_name;\n  if (hard_reg_name == NULL) {\n    VARR_PUSH (MIR_var_t, func->vars, var);\n  } else {\n    if (func->global_vars == NULL) VARR_CREATE (MIR_var_t, func->global_vars, ctx->alloc, 8);\n    VARR_PUSH (MIR_var_t, func->global_vars, var);\n  }\n  return res;\n}\n\nMIR_reg_t MIR_new_func_reg (MIR_context_t ctx, MIR_func_t func, MIR_type_t type, const char *name) {\n  return new_func_reg (ctx, func, type, name, NULL);\n}\n\nMIR_reg_t MIR_new_global_func_reg (MIR_context_t ctx, MIR_func_t func, MIR_type_t type,\n                                   const char *name, const char *hard_reg_name) {\n  if (hard_reg_name == NULL)\n    MIR_get_error_func (ctx) (MIR_hard_reg_error,\n                              \"global var %s should have non-null hard reg name\", name);\n  return new_func_reg (ctx, func, type, name, hard_reg_name);\n}\n\nstatic reg_desc_t *find_rd_by_name (MIR_context_t ctx MIR_UNUSED, const char *name,\n                                    MIR_func_t func) {\n  func_regs_t func_regs = func->internal;\n  size_t rdn, temp_rdn;\n  reg_desc_t rd;\n\n  rd.name = (char *) name;\n  rd.type = MIR_T_I64;\n  rd.reg = 0; /* to eliminate warnings */\n  temp_rdn = VARR_LENGTH (reg_desc_t, func_regs->reg_descs);\n  VARR_PUSH (reg_desc_t, func_regs->reg_descs, rd);\n  if (!HTAB_DO (size_t, func_regs->name2rdn_tab, temp_rdn, HTAB_FIND, rdn)) {\n    VARR_POP (reg_desc_t, func_regs->reg_descs);\n    return NULL; /* undeclared */\n  }\n  VARR_POP (reg_desc_t, func_regs->reg_descs);\n  return &VARR_ADDR (reg_desc_t, func_regs->reg_descs)[rdn];\n}\n\nstatic reg_desc_t *find_rd_by_reg (MIR_context_t ctx, MIR_reg_t reg, MIR_func_t func) {\n  func_regs_t func_regs = func->internal;\n  size_t rdn, temp_rdn;\n  reg_desc_t rd;\n\n  rd.reg = reg;\n  rd.name = NULL;\n  rd.type = MIR_T_I64; /* to eliminate warnings */\n  temp_rdn = VARR_LENGTH (reg_desc_t, func_regs->reg_descs);\n  VARR_PUSH (reg_desc_t, func_regs->reg_descs, rd);\n  if (!HTAB_DO (size_t, func_regs->reg2rdn_tab, temp_rdn, HTAB_FIND, rdn)) {\n    VARR_POP (reg_desc_t, func_regs->reg_descs);\n    MIR_get_error_func (ctx) (MIR_undeclared_func_reg_error, \"undeclared reg %u of func %s\", reg,\n                              func->name);\n  }\n  VARR_POP (reg_desc_t, func_regs->reg_descs);\n  return &VARR_ADDR (reg_desc_t, func_regs->reg_descs)[rdn];\n}\n\nvoid MIR_finish_func (MIR_context_t ctx) {\n  int expr_p = TRUE;\n  MIR_insn_t insn, prev_insn;\n  MIR_insn_code_t code;\n  const char *func_name;\n  int ret_p = FALSE, jret_p = FALSE;\n\n  if (curr_func == NULL)\n    MIR_get_error_func (ctx) (MIR_no_func_error, \"finish of non-existing function\");\n  func_name = curr_func->name;\n  if (curr_func->vararg_p || curr_func->nargs != 0 || curr_func->nres != 1) expr_p = FALSE;\n  for (insn = DLIST_HEAD (MIR_insn_t, curr_func->insns); insn != NULL;\n       insn = DLIST_NEXT (MIR_insn_t, insn)) {\n    size_t i, actual_nops = MIR_insn_nops (ctx, insn);\n    MIR_op_mode_t mode, expected_mode;\n    reg_desc_t *rd;\n    int out_p, can_be_out_p;\n\n    code = insn->code;\n    if (code == MIR_RET) ret_p = TRUE;\n    if (code == MIR_JRET) jret_p = TRUE;\n    if (code == MIR_PHI || code == MIR_USE) {\n      curr_func = NULL;\n      MIR_get_error_func (ctx) (MIR_vararg_func_error, \"use or phi can be used only internally\");\n    } else if (!curr_func->vararg_p && code == MIR_VA_START) {\n      curr_func = NULL;\n      MIR_get_error_func (ctx) (MIR_vararg_func_error, \"va_start is not in vararg function\");\n    } else if (code == MIR_JRET && curr_func->nres != 0) {\n      curr_func = NULL;\n      MIR_get_error_func (\n        ctx) (MIR_vararg_func_error,\n              \"func %s: in insn '%s': function should not have results in this case\", func_name,\n              insn_descs[code].name);\n    } else if ((code == MIR_JRET && ret_p) || (code == MIR_RET && jret_p)) {\n      curr_func = NULL;\n      MIR_get_error_func (ctx) (MIR_vararg_func_error, \"func %s: mix of RET and JRET insns\",\n                                func_name);\n    } else if (code == MIR_RET && actual_nops != curr_func->nres) {\n      curr_func = NULL;\n      MIR_get_error_func (\n        ctx) (MIR_vararg_func_error,\n              \"func %s: in instruction '%s': number of operands in return does not \"\n              \"correspond number of function returns. Expected %d, got %d\",\n              func_name, insn_descs[code].name, curr_func->nres, actual_nops);\n    } else if (MIR_call_code_p (code)) {\n      expr_p = FALSE;\n    } else if (code == MIR_BO || code == MIR_UBO || code == MIR_BNO || code == MIR_UBNO) {\n      for (prev_insn = DLIST_PREV (MIR_insn_t, insn); prev_insn != NULL;\n           prev_insn = DLIST_PREV (MIR_insn_t, prev_insn))\n        if (prev_insn->code != MIR_MOV || prev_insn->ops[1].mode != MIR_OP_REG) break;\n      if (prev_insn == NULL || !MIR_overflow_insn_code_p (prev_insn->code))\n        MIR_get_error_func (ctx) (MIR_invalid_insn_error,\n                                  \"func %s: instruction '%s' has no previous overflow insn \"\n                                  \"separated only by stores and reg moves\",\n                                  func_name, insn_descs[code].name);\n      else if ((code == MIR_UBO || code == MIR_UBNO)\n               && (prev_insn->code == MIR_MULO || prev_insn->code == MIR_MULOS))\n        MIR_get_error_func (\n          ctx) (MIR_invalid_insn_error,\n                \"func %s: unsigned overflow branch '%s' consumes flag of signed overflow insn '%s'\",\n                func_name, insn_descs[code].name, insn_descs[prev_insn->code].name);\n      else if ((code == MIR_BO || code == MIR_BNO)\n               && (prev_insn->code == MIR_UMULO || prev_insn->code == MIR_UMULOS))\n        MIR_get_error_func (\n          ctx) (MIR_invalid_insn_error,\n                \"func %s: signed overflow branch '%s' consumes flag of unsigned overflow insn '%s'\",\n                func_name, insn_descs[code].name, insn_descs[prev_insn->code].name);\n    }\n    for (i = 0; i < actual_nops; i++) {\n      if (code == MIR_UNSPEC && i == 0) {\n        mir_assert (insn->ops[i].mode == MIR_OP_INT);\n        continue;\n      } else if (MIR_call_code_p (code)) {\n        if (i == 0) {\n          mir_assert (insn->ops[i].mode == MIR_OP_REF\n                      && insn->ops[i].u.ref->item_type == MIR_proto_item);\n          continue; /* We checked the operand during insn creation -- skip the prototype */\n        } else if (i == 1 && insn->ops[i].mode == MIR_OP_REF) {\n          mir_assert (insn->ops[i].u.ref->item_type == MIR_import_item\n                      || insn->ops[i].u.ref->item_type == MIR_export_item\n                      || insn->ops[i].u.ref->item_type == MIR_forward_item\n                      || insn->ops[i].u.ref->item_type == MIR_func_item);\n          continue; /* We checked the operand during insn creation -- skip the func */\n        }\n      }\n      if (code == MIR_VA_ARG && i == 2) {\n        mir_assert (insn->ops[i].mode == MIR_OP_MEM);\n        continue; /* We checked the operand during insn creation -- skip va_arg type  */\n      }\n      if (code == MIR_SWITCH) {\n        out_p = FALSE;\n        expected_mode = i == 0 ? MIR_OP_INT : MIR_OP_LABEL;\n      } else if (code == MIR_RET) {\n        out_p = FALSE;\n        expected_mode = type2mode (curr_func->res_types[i]);\n      } else {\n        expected_mode = MIR_insn_op_mode (ctx, insn, i, &out_p);\n      }\n      can_be_out_p = TRUE;\n      switch (insn->ops[i].mode) {\n      case MIR_OP_REG:\n        rd = find_rd_by_reg (ctx, insn->ops[i].u.reg, curr_func);\n        mir_assert (rd != NULL && insn->ops[i].u.reg == rd->reg);\n        mode = type2mode (rd->type);\n        break;\n      case MIR_OP_MEM:\n        expr_p = FALSE;\n        if (wrong_type_p (insn->ops[i].u.mem.type)\n            && (!MIR_all_blk_type_p (insn->ops[i].u.mem.type) || !MIR_call_code_p (code))) {\n          curr_func = NULL;\n          MIR_get_error_func (ctx) (MIR_wrong_type_error,\n                                    \"func %s: in instruction '%s': wrong type memory\", func_name,\n                                    insn_descs[code].name);\n        }\n        if (MIR_all_blk_type_p (insn->ops[i].u.mem.type) && insn->ops[i].u.mem.disp < 0) {\n          curr_func = NULL;\n          MIR_get_error_func (ctx) (MIR_wrong_type_error,\n                                    \"func %s: in instruction '%s': block type memory with disp < 0\",\n                                    func_name, insn_descs[code].name);\n        }\n        if (insn->ops[i].u.mem.base != 0) {\n          rd = find_rd_by_reg (ctx, insn->ops[i].u.mem.base, curr_func);\n          mir_assert (rd != NULL && insn->ops[i].u.mem.base == rd->reg);\n          if (type2mode (rd->type) != MIR_OP_INT) {\n            curr_func = NULL;\n            MIR_get_error_func (\n              ctx) (MIR_reg_type_error,\n                    \"func %s: in instruction '%s': base reg of non-integer type for operand \"\n                    \"#%d\",\n                    func_name, insn_descs[code].name, i + 1);\n          }\n        }\n        if (insn->ops[i].u.mem.index != 0) {\n          rd = find_rd_by_reg (ctx, insn->ops[i].u.mem.index, curr_func);\n          mir_assert (rd != NULL && insn->ops[i].u.mem.index == rd->reg);\n          if (type2mode (rd->type) != MIR_OP_INT) {\n            curr_func = NULL;\n            MIR_get_error_func (\n              ctx) (MIR_reg_type_error,\n                    \"func %s: in instruction '%s': index reg of non-integer type for \"\n                    \"operand #%d\",\n                    func_name, insn_descs[code].name, i + 1);\n          }\n        }\n        mode = type2mode (insn->ops[i].u.mem.type);\n        break;\n      case MIR_OP_VAR:\n      case MIR_OP_VAR_MEM:\n        expr_p = FALSE;\n        mode = expected_mode;\n        mir_assert (FALSE); /* Should not be here */\n        break;\n      default:\n        can_be_out_p = FALSE;\n        mode = insn->ops[i].mode;\n        if (mode == MIR_OP_REF || mode == MIR_OP_STR) mode = MIR_OP_INT; /* just an address */\n        break;\n      }\n      insn->ops[i].value_mode = mode;\n      if (mode == MIR_OP_UNDEF && insn->ops[i].mode == MIR_OP_MEM\n          && ((code == MIR_VA_START && i == 0)\n              || ((code == MIR_VA_ARG || code == MIR_VA_BLOCK_ARG) && i == 1)\n              || (code == MIR_VA_END && i == 1))) { /* a special case: va_list as undef type mem */\n        insn->ops[i].value_mode = expected_mode;\n      } else if (expected_mode == MIR_OP_REG) {\n        if (insn->ops[i].mode != MIR_OP_REG && insn->ops[i].mode != MIR_OP_VAR)\n          MIR_get_error_func (\n            ctx) (MIR_op_mode_error,\n                  \"func %s: in instruction '%s': expected reg for operand #%d. Got '%s'\", func_name,\n                  insn_descs[code].name, i + 1, mode_str (insn->ops[i].mode));\n      } else if (expected_mode != MIR_OP_UNDEF\n                 && (mode == MIR_OP_UINT ? MIR_OP_INT : mode) != expected_mode) {\n        curr_func = NULL;\n        MIR_get_error_func (\n          ctx) (MIR_op_mode_error,\n                \"func %s: in instruction '%s': unexpected operand mode for operand #%d. Got \"\n                \"'%s', expected '%s'\",\n                func_name, insn_descs[code].name, i + 1, mode_str (mode), mode_str (expected_mode));\n      }\n      if (out_p && !can_be_out_p) {\n        curr_func = NULL;\n        MIR_get_error_func (ctx) (MIR_out_op_error,\n                                  \"func %s; in instruction '%s': wrong operand #%d for insn output\",\n                                  func_name, insn_descs[code].name, i + 1);\n      }\n    }\n  }\n  if (!ret_p && !jret_p\n      && ((insn = DLIST_TAIL (MIR_insn_t, curr_func->insns)) == NULL || insn->code != MIR_JMP)) {\n    VARR_TRUNC (MIR_op_t, temp_ops, 0);\n    for (size_t i = 0; i < curr_func->nres; i++) { /* add absent ret */\n      MIR_op_t op;\n      if (curr_func->res_types[i] == MIR_T_F)\n        op = MIR_new_float_op (ctx, 0.0f);\n      else if (curr_func->res_types[i] == MIR_T_D)\n        op = MIR_new_double_op (ctx, 0.0);\n      else if (curr_func->res_types[i] == MIR_T_LD)\n        op = MIR_new_ldouble_op (ctx, 0.0);\n      else\n        op = MIR_new_int_op (ctx, 0);\n      VARR_PUSH (MIR_op_t, temp_ops, op);\n    }\n    MIR_append_insn (ctx, curr_func->func_item,\n                     MIR_new_insn_arr (ctx, MIR_RET, curr_func->nres,\n                                       VARR_ADDR (MIR_op_t, temp_ops)));\n  }\n  curr_func->expr_p = expr_p;\n  curr_func->jret_p = jret_p;\n  curr_func = NULL;\n}\n\nvoid MIR_finish_module (MIR_context_t ctx) {\n  if (curr_module == NULL)\n    MIR_get_error_func (ctx) (MIR_no_module_error, \"finish of non-existing module\");\n  curr_module = NULL;\n}\n\nstatic int setup_global (MIR_context_t ctx, const char *name, void *addr, MIR_item_t def) {\n  MIR_item_t item, tab_item;\n  MIR_module_t saved = curr_module;\n  int redef_p = FALSE;\n\n  curr_module = &environment_module;\n  /* Use import for proto representation: */\n  item = new_export_import_forward (ctx, name, MIR_import_item, \"import\", TRUE);\n  if ((tab_item = item_tab_find (ctx, MIR_item_name (ctx, item), &environment_module)) != item\n      && tab_item != NULL) {\n    MIR_free (ctx->alloc, item);\n    redef_p = TRUE;\n  } else {\n    HTAB_DO (MIR_item_t, module_item_tab, item, HTAB_INSERT, tab_item);\n    DLIST_APPEND (MIR_item_t, environment_module.items, item);\n    tab_item = item;\n  }\n  tab_item->addr = addr;\n  tab_item->ref_def = def;\n  curr_module = saved;\n  return redef_p;\n}\n\nstatic void undefined_interface (MIR_context_t ctx) {\n  MIR_get_error_func (ctx) (MIR_call_op_error, \"undefined call interface\");\n}\n\nstatic MIR_item_t load_bss_data_section (MIR_context_t ctx, MIR_item_t item, int first_only_p) {\n  const char *name;\n  MIR_item_t curr_item, last_item, expr_item;\n  size_t len, section_size = 0;\n  uint8_t *addr;\n\n  if (item->addr == NULL) {\n    /* Calculate section size: */\n    for (curr_item = item; curr_item != NULL && curr_item->addr == NULL;\n         curr_item = first_only_p ? NULL : DLIST_NEXT (MIR_item_t, curr_item))\n      if (curr_item->item_type == MIR_bss_item\n          && (curr_item == item || curr_item->u.bss->name == NULL))\n        section_size += curr_item->u.bss->len;\n      else if (curr_item->item_type == MIR_data_item\n               && (curr_item == item || curr_item->u.data->name == NULL))\n        section_size += (curr_item->u.data->nel * _MIR_type_size (ctx, curr_item->u.data->el_type));\n      else if (curr_item->item_type == MIR_ref_data_item\n               && (curr_item == item || curr_item->u.ref_data->name == NULL))\n        section_size += _MIR_type_size (ctx, MIR_T_P);\n      else if (curr_item->item_type == MIR_lref_data_item\n               && (curr_item == item || curr_item->u.lref_data->name == NULL))\n        section_size += _MIR_type_size (ctx, MIR_T_P);\n      else if (curr_item->item_type == MIR_expr_data_item\n               && (curr_item == item || curr_item->u.expr_data->name == NULL)) {\n        expr_item = curr_item->u.expr_data->expr_item;\n        if (expr_item->item_type != MIR_func_item || !expr_item->u.func->expr_p\n            || expr_item->u.func->nres != 1)\n          MIR_get_error_func (\n            ctx) (MIR_binary_io_error,\n                  \"%s can not be an expr which should be a func w/o calls and memory ops\",\n                  MIR_item_name (ctx, expr_item));\n        section_size += _MIR_type_size (ctx, expr_item->u.func->res_types[0]);\n      } else\n        break;\n    if (section_size % 8 != 0)\n      section_size += 8 - section_size % 8; /* we might use 64-bit copying of data */\n    if ((item->addr = MIR_malloc (ctx->alloc, section_size)) == NULL) {\n      name = MIR_item_name (ctx, item);\n      MIR_get_error_func (ctx) (MIR_alloc_error, \"Not enough memory to allocate data/bss %s\",\n                                name == NULL ? \"\" : name);\n    }\n    item->section_head_p = TRUE;\n  }\n  /* Set up section memory: */\n  for (last_item = item, curr_item = item, addr = item->addr;\n       curr_item != NULL && (curr_item == item || curr_item->addr == NULL);\n       last_item = curr_item, curr_item = first_only_p ? NULL : DLIST_NEXT (MIR_item_t, curr_item))\n    if (curr_item->item_type == MIR_bss_item\n        && (curr_item == item || curr_item->u.bss->name == NULL)) {\n      memset (addr, 0, curr_item->u.bss->len);\n      curr_item->addr = addr;\n      addr += curr_item->u.bss->len;\n    } else if (curr_item->item_type == MIR_data_item\n               && (curr_item == item || curr_item->u.data->name == NULL)) {\n      len = curr_item->u.data->nel * _MIR_type_size (ctx, curr_item->u.data->el_type);\n      memmove (addr, curr_item->u.data->u.els, len);\n      curr_item->addr = addr;\n      addr += len;\n    } else if (curr_item->item_type == MIR_ref_data_item\n               && (curr_item == item || curr_item->u.ref_data->name == NULL)) {\n      curr_item->u.ref_data->load_addr = addr;\n      curr_item->addr = addr;\n      addr += _MIR_type_size (ctx, MIR_T_P);\n    } else if (curr_item->item_type == MIR_lref_data_item\n               && (curr_item == item || curr_item->u.lref_data->name == NULL)) {\n      curr_item->u.lref_data->load_addr = addr;\n      curr_item->addr = addr;\n      addr += _MIR_type_size (ctx, MIR_T_P);\n    } else if (curr_item->item_type == MIR_expr_data_item\n               && (curr_item == item || curr_item->u.expr_data->name == NULL)) {\n      expr_item = curr_item->u.expr_data->expr_item;\n      len = _MIR_type_size (ctx, expr_item->u.func->res_types[0]);\n      curr_item->u.expr_data->load_addr = addr;\n      curr_item->addr = addr;\n      addr += len;\n    } else {\n      break;\n    }\n  return last_item;\n}\n\nstatic void link_module_lrefs (MIR_context_t ctx, MIR_module_t m) {\n  for (MIR_item_t item = DLIST_HEAD (MIR_item_t, m->items); item != NULL;\n       item = DLIST_NEXT (MIR_item_t, item)) {\n    if (item->item_type != MIR_func_item) continue;\n    for (MIR_insn_t insn = DLIST_HEAD (MIR_insn_t, item->u.func->insns); insn != NULL;\n         insn = DLIST_NEXT (MIR_insn_t, insn))\n      if (insn->code == MIR_LABEL) insn->data = item->u.func;\n  }\n  for (MIR_item_t item = DLIST_HEAD (MIR_item_t, m->items); item != NULL;\n       item = DLIST_NEXT (MIR_item_t, item)) {\n    if (item->item_type == MIR_lref_data_item) {\n      MIR_lref_data_t lref_data = item->u.lref_data;\n      MIR_label_t lab = lref_data->label, lab2 = lref_data->label2;\n      MIR_func_t func = (MIR_func_t) lab->data;\n      if (lab->data == NULL)\n        MIR_get_error_func (ctx) (MIR_wrong_lref_error, \"A label not from any function in lref %s\",\n                                  lref_data->name == NULL ? \"\" : lref_data->name);\n      else if (lab2 != NULL && lab2->data != func)\n        MIR_get_error_func (ctx) (MIR_wrong_lref_error,\n                                  \"Labels from different functions in lref %s\",\n                                  lref_data->name == NULL ? \"\" : lref_data->name);\n      lref_data->next = func->first_lref;\n      func->first_lref = lref_data;\n    }\n  }\n  for (MIR_item_t item = DLIST_HEAD (MIR_item_t, m->items); item != NULL;\n       item = DLIST_NEXT (MIR_item_t, item)) {\n    if (item->item_type != MIR_func_item) continue;\n    for (MIR_insn_t insn = DLIST_HEAD (MIR_insn_t, item->u.func->insns); insn != NULL;\n         insn = DLIST_NEXT (MIR_insn_t, insn))\n      if (insn->code == MIR_LABEL) insn->data = NULL;\n  }\n}\n\nvoid MIR_load_module (MIR_context_t ctx, MIR_module_t m) {\n  int lref_p = FALSE;\n  mir_assert (m != NULL);\n  for (MIR_item_t item = DLIST_HEAD (MIR_item_t, m->items); item != NULL;\n       item = DLIST_NEXT (MIR_item_t, item)) {\n    MIR_item_t first_item = item;\n\n    if (item->item_type == MIR_bss_item || item->item_type == MIR_data_item\n        || item->item_type == MIR_ref_data_item || item->item_type == MIR_lref_data_item\n        || item->item_type == MIR_expr_data_item) {\n      if (item->item_type == MIR_lref_data_item) lref_p = TRUE;\n      item = load_bss_data_section (ctx, item, FALSE);\n    } else if (item->item_type == MIR_func_item) {\n      if (item->addr == NULL) {\n        item->addr = _MIR_get_thunk (ctx);\n#if defined(MIR_DEBUG)\n        fprintf (stderr, \"%016llx: %s\\n\", (unsigned long long) item->addr, item->u.func->name);\n#endif\n      }\n      _MIR_redirect_thunk (ctx, item->addr, undefined_interface);\n    }\n    if (first_item->export_p) { /* update global item table */\n      mir_assert (first_item->item_type != MIR_export_item\n                  && first_item->item_type != MIR_import_item\n                  && first_item->item_type != MIR_forward_item);\n      if (setup_global (ctx, MIR_item_name (ctx, first_item), first_item->addr, first_item)\n          && item->item_type == MIR_func_item\n          && !func_redef_permission_p\n#ifdef __APPLE__\n          /* macosx can have multiple equal external inline definitions of the same function: */\n          && strncmp (item->u.func->name, \"__darwin\", 8) != 0\n#endif\n      )\n        MIR_get_error_func (ctx) (MIR_repeated_decl_error, \"func %s is prohibited for redefinition\",\n                                  item->u.func->name);\n    }\n  }\n  if (lref_p) link_module_lrefs (ctx, m);\n  VARR_PUSH (MIR_module_t, modules_to_link, m);\n}\n\n#define SETJMP_NAME \"setjmp\"\n#define SETJMP_NAME2 \"_setjmp\"\n\nvoid MIR_load_external (MIR_context_t ctx, const char *name, void *addr) {\n  if (strcmp (name, SETJMP_NAME) == 0 || (SETJMP_NAME2 != NULL && strcmp (name, SETJMP_NAME2) == 0))\n    setjmp_addr = addr;\n  setup_global (ctx, name, addr, NULL);\n}\n\nstatic void simplify_module_init (MIR_context_t ctx);\nstatic int simplify_func (MIR_context_t ctx, MIR_item_t func_item, int mem_float_p);\nstatic void process_inlines (MIR_context_t ctx, MIR_item_t func_item);\n\nvoid MIR_link (MIR_context_t ctx, void (*set_interface) (MIR_context_t ctx, MIR_item_t item),\n               void *import_resolver (const char *)) {\n  MIR_item_t item, tab_item, expr_item;\n  MIR_type_t type;\n  MIR_val_t res;\n  MIR_module_t m;\n  void *addr;\n  union {\n    int8_t i8;\n    int16_t i16;\n    int32_t i32;\n    int64_t i64;\n    float f;\n    double d;\n    long double ld;\n    void *a;\n  } v;\n\n  for (size_t i = 0; i < VARR_LENGTH (MIR_module_t, modules_to_link); i++) {\n    m = VARR_GET (MIR_module_t, modules_to_link, i);\n    simplify_module_init (ctx);\n    for (item = DLIST_HEAD (MIR_item_t, m->items); item != NULL;\n         item = DLIST_NEXT (MIR_item_t, item))\n      if (item->item_type == MIR_func_item) {\n        assert (item->data == NULL);\n        if (simplify_func (ctx, item, TRUE)) item->data = (void *) 1; /* flag inlining */\n      } else if (item->item_type == MIR_import_item) {\n        if ((tab_item = item_tab_find (ctx, item->u.import_id, &environment_module)) == NULL) {\n          if (import_resolver == NULL || (addr = import_resolver (item->u.import_id)) == NULL)\n            MIR_get_error_func (ctx) (MIR_undeclared_op_ref_error, \"import of undefined item %s\",\n                                      item->u.import_id);\n          MIR_load_external (ctx, item->u.import_id, addr);\n          tab_item = item_tab_find (ctx, item->u.import_id, &environment_module);\n          mir_assert (tab_item != NULL);\n        }\n        item->addr = tab_item->addr;\n        item->ref_def = tab_item;\n      } else if (item->item_type == MIR_export_item) {\n        if ((tab_item = item_tab_find (ctx, item->u.export_id, m)) == NULL)\n          MIR_get_error_func (ctx) (MIR_undeclared_op_ref_error, \"export of undefined item %s\",\n                                    item->u.export_id);\n        item->addr = tab_item->addr;\n        item->ref_def = tab_item;\n      } else if (item->item_type == MIR_forward_item) {\n        if ((tab_item = item_tab_find (ctx, item->u.forward_id, m)) == NULL)\n          MIR_get_error_func (ctx) (MIR_undeclared_op_ref_error, \"forward of undefined item %s\",\n                                    item->u.forward_id);\n        item->addr = tab_item->addr;\n        item->ref_def = tab_item;\n      }\n  }\n  for (size_t i = 0; i < VARR_LENGTH (MIR_module_t, modules_to_link); i++) {\n    m = VARR_GET (MIR_module_t, modules_to_link, i);\n    for (item = DLIST_HEAD (MIR_item_t, m->items); item != NULL;\n         item = DLIST_NEXT (MIR_item_t, item)) {\n      if (item->item_type == MIR_func_item && item->data != NULL) {\n        process_inlines (ctx, item);\n        item->data = NULL;\n#if 0\n        fprintf (stderr, \"+++++ Function after inlining:\\n\");\n        MIR_output_item (ctx, stderr, item);\n#endif\n      } else if (item->item_type == MIR_ref_data_item) {\n        assert (item->u.ref_data->ref_item->addr != NULL);\n        addr = (char *) item->u.ref_data->ref_item->addr + item->u.ref_data->disp;\n        memcpy (item->u.ref_data->load_addr, &addr, _MIR_type_size (ctx, MIR_T_P));\n        continue;\n      }\n      /* lref data are set up in interpreter or generator */\n      if (item->item_type != MIR_expr_data_item) continue;\n      expr_item = item->u.expr_data->expr_item;\n      MIR_interp (ctx, expr_item, &res, 0);\n      type = expr_item->u.func->res_types[0];\n      switch (type) {\n      case MIR_T_I8:\n      case MIR_T_U8: v.i8 = (int8_t) res.i; break;\n      case MIR_T_I16:\n      case MIR_T_U16: v.i16 = (int16_t) res.i; break;\n      case MIR_T_I32:\n      case MIR_T_U32: v.i32 = (int32_t) res.i; break;\n      case MIR_T_I64:\n      case MIR_T_U64: v.i64 = (int64_t) res.i; break;\n      case MIR_T_F: v.f = res.f; break;\n      case MIR_T_D: v.d = res.d; break;\n      case MIR_T_LD: v.ld = res.ld; break;\n      case MIR_T_P: v.a = res.a; break;\n      default: assert (FALSE); break;\n      }\n      memcpy (item->u.expr_data->load_addr, &v,\n              _MIR_type_size (ctx, expr_item->u.func->res_types[0]));\n    }\n  }\n  if (set_interface != NULL) {\n    while (VARR_LENGTH (MIR_module_t, modules_to_link) != 0) {\n      m = VARR_POP (MIR_module_t, modules_to_link);\n      for (item = DLIST_HEAD (MIR_item_t, m->items); item != NULL;\n           item = DLIST_NEXT (MIR_item_t, item))\n        if (item->item_type == MIR_func_item) {\n          finish_func_interpretation (item, ctx->alloc); /* in case if it was used for expr data */\n          set_interface (ctx, item);\n        }\n    }\n    set_interface (ctx, NULL); /* finish interface setting */\n  }\n}\n\nstatic const char *insn_name (MIR_insn_code_t code) {\n  return (unsigned) code >= MIR_INSN_BOUND ? \"\" : insn_descs[code].name;\n}\n\nconst char *MIR_insn_name (MIR_context_t ctx, MIR_insn_code_t code) {\n  if ((unsigned) code >= MIR_INSN_BOUND)\n    MIR_get_error_func (ctx) (MIR_wrong_param_value_error, \"MIR_insn_name: wrong insn code %d\",\n                              (int) code);\n  return insn_descs[code].name;\n}\n\nstatic size_t insn_code_nops (MIR_context_t ctx, MIR_insn_code_t code) { /* 0 for calls */\n  if ((unsigned) code >= MIR_INSN_BOUND)\n    MIR_get_error_func (ctx) (MIR_wrong_param_value_error, \"insn_code_nops: wrong insn code %d\",\n                              (int) code);\n  return VARR_GET (size_t, insn_nops, code);\n}\n\nsize_t MIR_insn_nops (MIR_context_t ctx MIR_UNUSED, MIR_insn_t insn) {\n  mir_assert (insn != NULL);\n  return insn->nops;\n}\n\nMIR_op_mode_t _MIR_insn_code_op_mode (MIR_context_t ctx, MIR_insn_code_t code, size_t nop,\n                                      int *out_p) {\n  unsigned mode;\n  mir_assert (out_p != NULL);\n\n  if (nop >= insn_code_nops (ctx, code)) return MIR_OP_BOUND;\n  mode = insn_descs[code].op_modes[nop];\n  mir_assert (out_p != NULL);\n  *out_p = (mode & OUT_FLAG) != 0;\n  return *out_p ? mode ^ OUT_FLAG : mode;\n}\n\nMIR_op_mode_t MIR_insn_op_mode (MIR_context_t ctx, MIR_insn_t insn, size_t nop, int *out_p) {\n  MIR_insn_code_t code = insn->code;\n  size_t nargs, nops = MIR_insn_nops (ctx, insn);\n  unsigned mode;\n\n  *out_p = FALSE; /* to remove unitialized warning */\n  if (nop >= nops) return MIR_OP_BOUND;\n  mir_assert (out_p != NULL);\n  switch (code) {\n  case MIR_RET:\n  case MIR_SWITCH:\n    /* should be already checked in MIR_finish_func */\n    return nop == 0 && code != MIR_RET ? MIR_OP_INT : insn->ops[nop].mode;\n  case MIR_ADDR:\n  case MIR_ADDR8:\n  case MIR_ADDR16:\n  case MIR_ADDR32: *out_p = nop == 0; return nop == 0 ? MIR_OP_INT : insn->ops[nop].mode;\n  case MIR_PHI: *out_p = nop == 0; return insn->ops[nop].mode;\n  case MIR_USE: return insn->ops[nop].mode;\n  case MIR_CALL:\n  case MIR_INLINE:\n  case MIR_UNSPEC: {\n    MIR_op_t proto_op;\n    MIR_proto_t proto;\n    size_t args_start;\n\n    if (code == MIR_UNSPEC) {\n      args_start = 1;\n      mir_assert (insn->ops[0].mode == MIR_OP_INT);\n      mir_assert (insn->ops[0].u.u < VARR_LENGTH (MIR_proto_t, unspec_protos));\n      proto = VARR_GET (MIR_proto_t, unspec_protos, insn->ops[0].u.u);\n    } else {\n      args_start = 2;\n      proto_op = insn->ops[0];\n      mir_assert (proto_op.mode == MIR_OP_REF && proto_op.u.ref->item_type == MIR_proto_item);\n      proto = proto_op.u.ref->u.proto;\n    }\n    *out_p = args_start <= nop && nop < proto->nres + args_start;\n    nargs\n      = proto->nres + args_start + (proto->args == NULL ? 0 : VARR_LENGTH (MIR_var_t, proto->args));\n    if (proto->vararg_p && nop >= nargs) return MIR_OP_UNDEF; /* unknown */\n    mir_assert (nops >= nargs && (proto->vararg_p || nops == nargs));\n    if (nop == 0) return insn->ops[nop].mode;\n    if (nop == 1 && code != MIR_UNSPEC) return MIR_OP_INT; /* call func addr */\n    if (args_start <= nop && nop < proto->nres + args_start)\n      return type2mode (proto->res_types[nop - args_start]);\n    return type2mode (VARR_GET (MIR_var_t, proto->args, nop - args_start - proto->nres).type);\n  }\n  default:\n    mode = insn_descs[code].op_modes[nop];\n    if ((mode & OUT_FLAG) == 0) return mode;\n    *out_p = TRUE;\n    return mode ^ OUT_FLAG;\n  }\n}\n\nstatic MIR_insn_t create_insn (MIR_context_t ctx, size_t nops, MIR_insn_code_t code) {\n  MIR_insn_t insn;\n\n  if (nops == 0) nops = 1;\n  insn = MIR_malloc (ctx->alloc, sizeof (struct MIR_insn) + sizeof (MIR_op_t) * (nops - 1));\n  if (insn == NULL)\n    MIR_get_error_func (ctx) (MIR_alloc_error, \"Not enough memory for insn creation\");\n#if defined(_WIN32) || __SIZEOF_LONG_DOUBLE__ == 8\n  switch (code) {\n  case MIR_LDMOV: code = MIR_DMOV; break;\n  case MIR_I2LD: code = MIR_I2D; break;\n  case MIR_UI2LD: code = MIR_UI2D; break;\n  case MIR_LD2I: code = MIR_D2I; break;\n  case MIR_F2LD: code = MIR_F2D; break;\n  case MIR_D2LD: code = MIR_DMOV; break;\n  case MIR_LD2F: code = MIR_D2F; break;\n  case MIR_LD2D: code = MIR_DMOV; break;\n  case MIR_LDNEG: code = MIR_DNEG; break;\n  case MIR_LDADD: code = MIR_DADD; break;\n  case MIR_LDSUB: code = MIR_DSUB; break;\n  case MIR_LDMUL: code = MIR_DMUL; break;\n  case MIR_LDDIV: code = MIR_DDIV; break;\n  case MIR_LDEQ: code = MIR_DEQ; break;\n  case MIR_LDNE: code = MIR_DNE; break;\n  case MIR_LDLT: code = MIR_DLT; break;\n  case MIR_LDLE: code = MIR_DLE; break;\n  case MIR_LDGT: code = MIR_DGT; break;\n  case MIR_LDGE: code = MIR_DGE; break;\n  case MIR_LDBEQ: code = MIR_DBEQ; break;\n  case MIR_LDBNE: code = MIR_DBNE; break;\n  case MIR_LDBLT: code = MIR_DBLT; break;\n  case MIR_LDBLE: code = MIR_DBLE; break;\n  case MIR_LDBGT: code = MIR_DBGT; break;\n  case MIR_LDBGE: code = MIR_DBGE; break;\n  default: break;\n  }\n#endif\n  insn->code = code;\n  insn->data = NULL;\n  return insn;\n}\n\nstatic MIR_insn_t new_insn1 (MIR_context_t ctx, MIR_insn_code_t code) {\n  return create_insn (ctx, 1, code);\n}\n\nMIR_insn_t MIR_new_insn_arr (MIR_context_t ctx, MIR_insn_code_t code, size_t nops, MIR_op_t *ops) {\n  MIR_insn_t insn;\n  MIR_proto_t proto;\n  size_t args_start, narg, i = 0, expected_nops = insn_code_nops (ctx, code);\n  mir_assert (nops == 0 || ops != NULL);\n\n  if (!MIR_call_code_p (code) && code != MIR_UNSPEC && code != MIR_USE && code != MIR_PHI\n      && code != MIR_RET && code != MIR_SWITCH && nops != expected_nops) {\n    MIR_get_error_func (ctx) (MIR_ops_num_error, \"wrong number of operands for insn %s\",\n                              insn_descs[code].name);\n  } else if (code == MIR_SWITCH) {\n    if (nops < 2)\n      MIR_get_error_func (ctx) (MIR_ops_num_error, \"number of MIR_SWITCH operands is less 2\");\n  } else if (code == MIR_PHI) {\n    if (nops < 3)\n      MIR_get_error_func (ctx) (MIR_ops_num_error, \"number of MIR_PHI operands is less 3\");\n  } else if (MIR_call_code_p (code) || code == MIR_UNSPEC) {\n    args_start = code == MIR_UNSPEC ? 1 : 2;\n    if (nops < args_start)\n      MIR_get_error_func (ctx) (MIR_ops_num_error, \"wrong number of call/unspec operands\");\n    if (code == MIR_UNSPEC) {\n      if (ops[0].mode != MIR_OP_INT || ops[0].u.u >= VARR_LENGTH (MIR_proto_t, unspec_protos))\n        MIR_get_error_func (ctx) (MIR_unspec_op_error,\n                                  \"the 1st unspec operand should be valid unspec code\");\n      proto = VARR_GET (MIR_proto_t, unspec_protos, ops[0].u.u);\n    } else {\n      if (ops[0].mode != MIR_OP_REF || ops[0].u.ref->item_type != MIR_proto_item)\n        MIR_get_error_func (ctx) (MIR_call_op_error, \"the 1st call operand should be a prototype\");\n      proto = ops[0].u.ref->u.proto;\n    }\n    i = proto->nres;\n    if (proto->args != NULL) i += VARR_LENGTH (MIR_var_t, proto->args);\n    if (nops < i + args_start || (nops != i + args_start && !proto->vararg_p))\n      MIR_get_error_func (\n        ctx) (code == MIR_UNSPEC ? MIR_unspec_op_error : MIR_call_op_error,\n              \"number of %s operands or results does not correspond to prototype %s\",\n              code == MIR_UNSPEC ? \"unspec\" : \"call\", proto->name);\n    for (i = args_start; i < nops; i++) {\n      if (ops[i].mode == MIR_OP_MEM && MIR_all_blk_type_p (ops[i].u.mem.type)) {\n        if (i - args_start < proto->nres)\n          MIR_get_error_func (ctx) (MIR_wrong_type_error, \"result of %s is block type memory\",\n                                    code == MIR_UNSPEC ? \"unspec\" : \"call\");\n        else if ((narg = i - args_start - proto->nres) < VARR_LENGTH (MIR_var_t, proto->args)) {\n          if (VARR_GET (MIR_var_t, proto->args, narg).type != ops[i].u.mem.type) {\n            MIR_get_error_func (\n              ctx) (MIR_wrong_type_error,\n                    \"arg of %s is block type memory but param is not of block type\",\n                    code == MIR_UNSPEC ? \"unspec\" : \"call\");\n          } else if (VARR_GET (MIR_var_t, proto->args, narg).size != (size_t) ops[i].u.mem.disp) {\n            MIR_get_error_func (\n              ctx) (MIR_wrong_type_error,\n                    \"different sizes (%lu vs %lu) of arg and param block memory in %s insn\",\n                    (unsigned long) VARR_GET (MIR_var_t, proto->args, narg).size,\n                    (unsigned long) ops[i].u.mem.disp, code == MIR_UNSPEC ? \"unspec\" : \"call\");\n          }\n        } else if (ops[i].u.mem.type == MIR_T_RBLK) {\n          MIR_get_error_func (ctx) (MIR_wrong_type_error,\n                                    \"RBLK memory can not correspond to unnamed param in %s insn\",\n                                    code == MIR_UNSPEC ? \"unspec\" : \"call\");\n        }\n      } else if (i - args_start >= proto->nres\n                 && (narg = i - args_start - proto->nres) < VARR_LENGTH (MIR_var_t, proto->args)\n                 && MIR_all_blk_type_p (VARR_GET (MIR_var_t, proto->args, narg).type)) {\n        MIR_get_error_func (\n          ctx) (MIR_wrong_type_error,\n                \"param of %s is of block type but arg is not of block type memory\",\n                code == MIR_UNSPEC ? \"unspec\" : \"call\");\n      }\n    }\n  } else if (code == MIR_VA_ARG) {  // undef mem ???\n    if (ops[2].mode != MIR_OP_MEM)\n      MIR_get_error_func (ctx) (MIR_op_mode_error,\n                                \"3rd operand of va_arg should be any memory with given type\");\n  } else if (code == MIR_PRSET) {\n    if (ops[1].mode != MIR_OP_INT)\n      MIR_get_error_func (ctx) (MIR_op_mode_error, \"property should be a integer operand\");\n  } else if (code == MIR_PRBEQ || code == MIR_PRBNE) {\n    if (ops[2].mode != MIR_OP_INT)\n      MIR_get_error_func (ctx) (MIR_op_mode_error, \"property should be a integer operand\");\n    if (ops[1].mode != MIR_OP_REG && ops[1].mode != MIR_OP_MEM)\n      MIR_get_error_func (\n        ctx) (MIR_op_mode_error,\n              \"2nd operand of property branch should be any memory or reg with given type\");\n  }\n  insn = create_insn (ctx, nops, code);\n  insn->nops = (unsigned int) nops;\n  for (i = 0; i < nops; i++) insn->ops[i] = ops[i];\n  return insn;\n}\n\nstatic MIR_insn_t new_insn (MIR_context_t ctx, MIR_insn_code_t code, size_t nops, va_list argp) {\n  MIR_op_t *insn_ops = alloca (sizeof (MIR_op_t) * nops);\n\n  for (size_t i = 0; i < nops; i++) insn_ops[i] = va_arg (argp, MIR_op_t);\n  va_end (argp);\n  return MIR_new_insn_arr (ctx, code, nops, insn_ops);\n}\n\nMIR_insn_t MIR_new_insn (MIR_context_t ctx, MIR_insn_code_t code, ...) {\n  va_list argp;\n  size_t nops = insn_code_nops (ctx, code);\n\n  if (code == MIR_USE || code == MIR_PHI)\n    MIR_get_error_func (ctx) (MIR_call_op_error,\n                              \"Use only MIR_new_insn_arr for creating use or phi insn\");\n  else if (MIR_call_code_p (code) || code == MIR_UNSPEC || code == MIR_RET || code == MIR_SWITCH)\n    MIR_get_error_func (\n      ctx) (MIR_call_op_error,\n            \"Use only MIR_new_insn_arr or MIR_new_{call,unspec,ret}_insn for creating a \"\n            \"call/unspec/ret/jret/switch insn\");\n  va_start (argp, code);\n  return new_insn (ctx, code, nops, argp);\n}\n\nMIR_insn_t MIR_new_call_insn (MIR_context_t ctx, size_t nops, ...) {\n  va_list argp;\n\n  va_start (argp, nops);\n  return new_insn (ctx, MIR_CALL, nops, argp);\n}\n\nMIR_insn_t MIR_new_jcall_insn (MIR_context_t ctx, size_t nops, ...) {\n  va_list argp;\n\n  va_start (argp, nops);\n  return new_insn (ctx, MIR_JCALL, nops, argp);\n}\n\nMIR_insn_t MIR_new_ret_insn (MIR_context_t ctx, size_t nops, ...) {\n  va_list argp;\n\n  va_start (argp, nops);\n  return new_insn (ctx, MIR_RET, nops, argp);\n}\n\nMIR_insn_t _MIR_new_unspec_insn (MIR_context_t ctx, size_t nops, ...) {\n  va_list argp;\n\n  va_start (argp, nops);\n  return new_insn (ctx, MIR_UNSPEC, nops, argp);\n}\n\nvoid _MIR_register_unspec_insn (MIR_context_t ctx, uint64_t code, const char *name, size_t nres,\n                                MIR_type_t *res_types, size_t nargs, int vararg_p,\n                                MIR_var_t *args) {\n  MIR_proto_t proto;\n\n  while (VARR_LENGTH (MIR_proto_t, unspec_protos) <= code)\n    VARR_PUSH (MIR_proto_t, unspec_protos, NULL);\n  if ((proto = VARR_GET (MIR_proto_t, unspec_protos, code)) == NULL) {\n    VARR_SET (MIR_proto_t, unspec_protos, code,\n              create_proto (ctx, name, nres, res_types, nargs, vararg_p, args));\n  } else {\n    assert (strcmp (proto->name, name) == 0);\n  }\n}\n\nMIR_insn_t MIR_copy_insn (MIR_context_t ctx, MIR_insn_t insn) {\n  size_t size;\n  mir_assert (insn != NULL);\n  size = sizeof (struct MIR_insn) + sizeof (MIR_op_t) * (insn->nops == 0 ? 0 : insn->nops - 1);\n  MIR_insn_t new_insn = MIR_malloc (ctx->alloc, size);\n\n  if (new_insn == NULL)\n    MIR_get_error_func (ctx) (MIR_alloc_error, \"Not enough memory to copy insn %s\",\n                              insn_name (insn->code));\n  memcpy (new_insn, insn, size);\n  return new_insn;\n}\n\nstatic MIR_insn_t create_label (MIR_context_t ctx, int64_t label_num) {\n  MIR_insn_t insn = new_insn1 (ctx, MIR_LABEL);\n\n  insn->ops[0] = MIR_new_int_op (ctx, label_num);\n  insn->nops = 0;\n  return insn;\n}\n\nMIR_insn_t MIR_new_label (MIR_context_t ctx) { return create_label (ctx, ++curr_label_num); }\n\nvoid _MIR_free_insn (MIR_context_t ctx MIR_UNUSED, MIR_insn_t insn) {\n  MIR_free (ctx->alloc, insn);\n}\n\nstatic MIR_reg_t new_temp_reg (MIR_context_t ctx, MIR_type_t type, MIR_func_t func) {\n  char reg_name[30];\n\n  if (type != MIR_T_I64 && type != MIR_T_F && type != MIR_T_D && type != MIR_T_LD)\n    MIR_get_error_func (ctx) (MIR_reg_type_error, \"wrong type %s for temporary register\",\n                              type_str (ctx, type));\n  mir_assert (func != NULL);\n  for (;;) {\n    func->last_temp_num++;\n    if (func->last_temp_num == 0)\n      MIR_get_error_func (ctx) (MIR_unique_reg_error, \"out of unique regs\");\n    sprintf (reg_name, \"%s%d\", TEMP_REG_NAME_PREFIX, func->last_temp_num);\n\n    if (find_rd_by_name (ctx, reg_name, func) == NULL)\n      return MIR_new_func_reg (ctx, func, type, reg_name);\n  }\n}\n\nMIR_reg_t _MIR_new_temp_reg (MIR_context_t ctx, MIR_type_t type, MIR_func_t func) {\n  return new_temp_reg (ctx, type, func);\n}\n\nstatic reg_desc_t *get_func_rd_by_name (MIR_context_t ctx, const char *reg_name, MIR_func_t func) {\n  reg_desc_t *rd;\n\n  rd = find_rd_by_name (ctx, reg_name, func);\n  if (rd == NULL)\n    MIR_get_error_func (ctx) (MIR_undeclared_func_reg_error, \"undeclared func reg %s\", reg_name);\n  return rd;\n}\n\nstatic reg_desc_t *get_func_rd_by_reg (MIR_context_t ctx, MIR_reg_t reg, MIR_func_t func) {\n  reg_desc_t *rd;\n\n  rd = find_rd_by_reg (ctx, reg, func);\n  return rd;\n}\n\nMIR_reg_t MIR_reg (MIR_context_t ctx, const char *reg_name, MIR_func_t func) {\n  return get_func_rd_by_name (ctx, reg_name, func)->reg;\n}\n\nMIR_type_t MIR_reg_type (MIR_context_t ctx, MIR_reg_t reg, MIR_func_t func) {\n  return get_func_rd_by_reg (ctx, reg, func)->type;\n}\n\nconst char *MIR_reg_name (MIR_context_t ctx, MIR_reg_t reg, MIR_func_t func) {\n  return get_func_rd_by_reg (ctx, reg, func)->name;\n}\n\nconst char *MIR_reg_hard_reg_name (MIR_context_t ctx, MIR_reg_t reg, MIR_func_t func) {\n  const reg_desc_t *rd = get_func_rd_by_reg (ctx, reg, func);\n  return rd->hard_reg_name;\n}\n\n/* Functions to create operands.  */\n\nstatic void init_op (MIR_op_t *op, MIR_op_mode_t mode) {\n  op->mode = mode;\n  op->data = NULL;\n}\n\nMIR_op_t MIR_new_reg_op (MIR_context_t ctx MIR_UNUSED, MIR_reg_t reg) {\n  MIR_op_t op;\n\n  init_op (&op, MIR_OP_REG);\n  op.u.reg = reg;\n  return op;\n}\n\nMIR_op_t _MIR_new_var_op (MIR_context_t ctx MIR_UNUSED, MIR_reg_t var) { /* used only internally */\n  MIR_op_t op;\n\n  init_op (&op, MIR_OP_VAR);\n  op.u.var = var;\n  return op;\n}\n\nMIR_op_t MIR_new_int_op (MIR_context_t ctx MIR_UNUSED, int64_t i) {\n  MIR_op_t op;\n\n  init_op (&op, MIR_OP_INT);\n  op.u.i = i;\n  return op;\n}\n\nMIR_op_t MIR_new_uint_op (MIR_context_t ctx MIR_UNUSED, uint64_t u) {\n  MIR_op_t op;\n\n  init_op (&op, MIR_OP_UINT);\n  op.u.u = u;\n  return op;\n}\n\nMIR_op_t MIR_new_float_op (MIR_context_t ctx MIR_UNUSED, float f) {\n  MIR_op_t op;\n\n  mir_assert (sizeof (float) == 4); /* IEEE single */\n  init_op (&op, MIR_OP_FLOAT);\n  op.u.f = f;\n  return op;\n}\n\nMIR_op_t MIR_new_double_op (MIR_context_t ctx MIR_UNUSED, double d) {\n  MIR_op_t op;\n\n  mir_assert (sizeof (double) == 8); /* IEEE double */\n  init_op (&op, MIR_OP_DOUBLE);\n  op.u.d = d;\n  return op;\n}\n\nMIR_op_t MIR_new_ldouble_op (MIR_context_t ctx MIR_UNUSED, long double ld) {\n  MIR_op_t op;\n\n#if defined(_WIN32) || __SIZEOF_LONG_DOUBLE__ == 8\n  return MIR_new_double_op (ctx, ld);\n#endif\n  mir_assert (sizeof (long double) == 16); /* machine-defined 80- or 128-bit FP  */\n  init_op (&op, MIR_OP_LDOUBLE);\n  op.u.ld = ld;\n  return op;\n}\n\nMIR_op_t MIR_new_ref_op (MIR_context_t ctx MIR_UNUSED, MIR_item_t item) {\n  MIR_op_t op;\n\n  init_op (&op, MIR_OP_REF);\n  op.u.ref = item;\n  return op;\n}\n\nMIR_op_t MIR_new_str_op (MIR_context_t ctx, MIR_str_t str) {\n  MIR_op_t op;\n\n  init_op (&op, MIR_OP_STR);\n  op.u.str = get_ctx_string (ctx, str).str;\n  return op;\n}\n\nstatic MIR_op_t new_mem_op (MIR_context_t ctx MIR_UNUSED, MIR_type_t type, MIR_disp_t disp,\n                            MIR_reg_t base, MIR_reg_t index, MIR_scale_t scale, MIR_alias_t alias,\n                            MIR_alias_t nonalias) {\n  MIR_op_t op;\n\n  init_op (&op, MIR_OP_MEM);\n  op.u.mem.type = canon_type (type);\n  op.u.mem.disp = disp;\n  op.u.mem.base = base;\n  op.u.mem.index = index;\n  op.u.mem.scale = scale;\n  op.u.mem.nloc = 0;\n  op.u.mem.alias = alias;\n  op.u.mem.nonalias = nonalias;\n  return op;\n}\n\nMIR_op_t MIR_new_mem_op (MIR_context_t ctx, MIR_type_t type, MIR_disp_t disp, MIR_reg_t base,\n                         MIR_reg_t index, MIR_scale_t scale) {\n  return new_mem_op (ctx, type, disp, base, index, scale, 0, 0);\n}\n\nMIR_op_t MIR_new_alias_mem_op (MIR_context_t ctx, MIR_type_t type, MIR_disp_t disp, MIR_reg_t base,\n                               MIR_reg_t index, MIR_scale_t scale, MIR_alias_t alias,\n                               MIR_alias_t nonalias) {\n  return new_mem_op (ctx, type, disp, base, index, scale, alias, nonalias);\n}\n\nstatic MIR_op_t new_var_mem_op (MIR_context_t ctx MIR_UNUSED, MIR_type_t type, MIR_disp_t disp,\n                                MIR_reg_t base, MIR_reg_t index, MIR_scale_t scale,\n                                MIR_alias_t alias, MIR_alias_t nonalias) {\n  MIR_op_t op;\n\n  init_op (&op, MIR_OP_VAR_MEM);\n  op.u.var_mem.type = type;\n  op.u.var_mem.disp = disp;\n  op.u.var_mem.base = base;\n  op.u.var_mem.index = index;\n  op.u.var_mem.scale = scale;\n  op.u.var_mem.nloc = 0;\n  op.u.var_mem.alias = alias;\n  op.u.var_mem.nonalias = nonalias;\n  return op;\n}\n\nMIR_op_t _MIR_new_var_mem_op (MIR_context_t ctx, MIR_type_t type, MIR_disp_t disp, MIR_reg_t base,\n                              MIR_reg_t index, MIR_scale_t scale) {\n  return new_var_mem_op (ctx, type, disp, base, index, scale, 0, 0);\n}\n\nMIR_op_t _MIR_new_alias_var_mem_op (MIR_context_t ctx, MIR_type_t type, MIR_disp_t disp,\n                                    MIR_reg_t base, MIR_reg_t index, MIR_scale_t scale,\n                                    MIR_alias_t alias, MIR_alias_t nonalias) {\n  return new_var_mem_op (ctx, type, disp, base, index, scale, alias, nonalias);\n}\n\nMIR_op_t MIR_new_label_op (MIR_context_t ctx MIR_UNUSED, MIR_label_t label) {\n  MIR_op_t op;\n\n  init_op (&op, MIR_OP_LABEL);\n  op.u.label = label;\n  return op;\n}\n\nint MIR_op_eq_p (MIR_context_t ctx, MIR_op_t op1, MIR_op_t op2) {\n  if (op1.mode != op2.mode) return FALSE;\n  switch (op1.mode) {\n  case MIR_OP_REG: return op1.u.reg == op2.u.reg;\n  case MIR_OP_VAR: return op1.u.var == op2.u.var;\n  case MIR_OP_INT: return op1.u.i == op2.u.i;\n  case MIR_OP_UINT: return op1.u.u == op2.u.u;\n  case MIR_OP_FLOAT: return op1.u.f == op2.u.f;\n  case MIR_OP_DOUBLE: return op1.u.d == op2.u.d;\n  case MIR_OP_LDOUBLE: return op1.u.ld == op2.u.ld;\n  case MIR_OP_REF:\n    if (op1.u.ref->item_type == MIR_export_item || op1.u.ref->item_type == MIR_import_item)\n      return strcmp (MIR_item_name (ctx, op1.u.ref), MIR_item_name (ctx, op2.u.ref)) == 0;\n    return op1.u.ref == op2.u.ref;\n  case MIR_OP_STR:\n    return op1.u.str.len == op2.u.str.len && memcmp (op1.u.str.s, op2.u.str.s, op1.u.str.len) == 0;\n  case MIR_OP_MEM:\n    return (op1.u.mem.type == op2.u.mem.type && op1.u.mem.disp == op2.u.mem.disp\n            && op1.u.mem.base == op2.u.mem.base && op1.u.mem.index == op2.u.mem.index\n            && (op1.u.mem.index == 0 || op1.u.mem.scale == op2.u.mem.scale));\n  case MIR_OP_VAR_MEM:\n    return (op1.u.var_mem.type == op2.u.var_mem.type && op1.u.var_mem.disp == op2.u.var_mem.disp\n            && op1.u.var_mem.base == op2.u.var_mem.base\n            && op1.u.var_mem.index == op2.u.var_mem.index\n            && (op1.u.var_mem.index == MIR_NON_VAR || op1.u.var_mem.scale == op2.u.var_mem.scale));\n  case MIR_OP_LABEL: return op1.u.label == op2.u.label;\n  default: mir_assert (FALSE); /* we should not have other operands here */\n  }\n  return FALSE;\n}\n\nhtab_hash_t MIR_op_hash_step (MIR_context_t ctx, htab_hash_t h, MIR_op_t op) {\n  h = (htab_hash_t) mir_hash_step (h, (uint64_t) op.mode);\n  switch (op.mode) {\n  case MIR_OP_REG: return (htab_hash_t) mir_hash_step (h, (uint64_t) op.u.reg);\n  case MIR_OP_VAR: return (htab_hash_t) mir_hash_step (h, (uint64_t) op.u.var);\n  case MIR_OP_INT: return (htab_hash_t) mir_hash_step (h, (uint64_t) op.u.i);\n  case MIR_OP_UINT: return (htab_hash_t) mir_hash_step (h, (uint64_t) op.u.u);\n  case MIR_OP_FLOAT: {\n    union {\n      double d;\n      uint64_t u;\n    } u;\n\n    u.d = op.u.f;\n    return (htab_hash_t) mir_hash_step (h, u.u);\n  }\n  case MIR_OP_DOUBLE: return (htab_hash_t) mir_hash_step (h, op.u.u);\n  case MIR_OP_LDOUBLE: {\n    union {\n      long double ld;\n      uint64_t u[2];\n    } u;\n\n    u.ld = op.u.ld;\n    return (htab_hash_t) mir_hash_step (mir_hash_step (h, u.u[0]), u.u[1]);\n  }\n  case MIR_OP_REF:\n    if (op.u.ref->item_type == MIR_export_item || op.u.ref->item_type == MIR_import_item)\n      return (htab_hash_t) mir_hash_step (h, (uint64_t) MIR_item_name (ctx, op.u.ref));\n    return (htab_hash_t) mir_hash_step (h, (uint64_t) op.u.ref);\n  case MIR_OP_STR: return (htab_hash_t) mir_hash_step (h, (uint64_t) op.u.str.s);\n  case MIR_OP_MEM:\n    h = (htab_hash_t) mir_hash_step (h, (uint64_t) op.u.mem.type);\n    h = (htab_hash_t) mir_hash_step (h, (uint64_t) op.u.mem.disp);\n    h = (htab_hash_t) mir_hash_step (h, (uint64_t) op.u.mem.base);\n    h = (htab_hash_t) mir_hash_step (h, (uint64_t) op.u.mem.index);\n    if (op.u.mem.index != 0) h = (htab_hash_t) mir_hash_step (h, (uint64_t) op.u.mem.scale);\n    break;\n  case MIR_OP_VAR_MEM:\n    h = (htab_hash_t) mir_hash_step (h, (uint64_t) op.u.var_mem.type);\n    h = (htab_hash_t) mir_hash_step (h, (uint64_t) op.u.var_mem.disp);\n    h = (htab_hash_t) mir_hash_step (h, (uint64_t) op.u.var_mem.base);\n    h = (htab_hash_t) mir_hash_step (h, (uint64_t) op.u.var_mem.index);\n    if (op.u.var_mem.index != MIR_NON_VAR)\n      h = (htab_hash_t) mir_hash_step (h, (uint64_t) op.u.var_mem.scale);\n    break;\n  case MIR_OP_LABEL: return (htab_hash_t) mir_hash_step (h, (uint64_t) op.u.label);\n  default: mir_assert (FALSE); /* we should not have other operands here */\n  }\n  return h;\n}\n\nvoid MIR_append_insn (MIR_context_t ctx, MIR_item_t func_item, MIR_insn_t insn) {\n  mir_assert (func_item != NULL);\n  if (func_item->item_type != MIR_func_item)\n    MIR_get_error_func (ctx) (MIR_wrong_param_value_error, \"MIR_append_insn: wrong func item\");\n  DLIST_APPEND (MIR_insn_t, func_item->u.func->insns, insn);\n}\n\nvoid MIR_prepend_insn (MIR_context_t ctx, MIR_item_t func_item, MIR_insn_t insn) {\n  mir_assert (func_item != NULL);\n  if (func_item->item_type != MIR_func_item)\n    MIR_get_error_func (ctx) (MIR_wrong_param_value_error, \"MIR_prepend_insn: wrong func item\");\n  DLIST_PREPEND (MIR_insn_t, func_item->u.func->insns, insn);\n}\n\nvoid MIR_insert_insn_after (MIR_context_t ctx, MIR_item_t func_item, MIR_insn_t after,\n                            MIR_insn_t insn) {\n  mir_assert (func_item != NULL);\n  if (func_item->item_type != MIR_func_item)\n    MIR_get_error_func (ctx) (MIR_wrong_param_value_error,\n                              \"MIR_insert_insn_after: wrong func item\");\n  DLIST_INSERT_AFTER (MIR_insn_t, func_item->u.func->insns, after, insn);\n}\n\nvoid MIR_insert_insn_before (MIR_context_t ctx, MIR_item_t func_item, MIR_insn_t before,\n                             MIR_insn_t insn) {\n  mir_assert (func_item != NULL);\n  if (func_item->item_type != MIR_func_item)\n    MIR_get_error_func (ctx) (MIR_wrong_param_value_error,\n                              \"MIR_insert_insn_before: wrong func item\");\n  DLIST_INSERT_BEFORE (MIR_insn_t, func_item->u.func->insns, before, insn);\n}\n\nstatic void store_labels_for_duplication (MIR_context_t ctx MIR_UNUSED, VARR (MIR_insn_t) * labels,\n                                          VARR (MIR_insn_t) * branch_insns, MIR_insn_t insn,\n                                          MIR_insn_t new_insn) {\n  if (MIR_any_branch_code_p (insn->code) || insn->code == MIR_LADDR || insn->code == MIR_PRBEQ\n      || insn->code == MIR_PRBNE) {\n    VARR_PUSH (MIR_insn_t, branch_insns, new_insn);\n  } else if (insn->code == MIR_LABEL) {\n    mir_assert (insn->data == NULL);\n    insn->data = new_insn;\n    VARR_PUSH (MIR_insn_t, labels, insn);\n  }\n}\n\nstatic void redirect_duplicated_labels (MIR_context_t ctx MIR_UNUSED, VARR (MIR_insn_t) * labels,\n                                        VARR (MIR_insn_t) * branch_insns) {\n  MIR_insn_t insn;\n\n  while (VARR_LENGTH (MIR_insn_t, branch_insns) != 0) { /* redirect new label operands */\n    size_t start_label_nop = 0, bound_label_nop = 1, n;\n\n    insn = VARR_POP (MIR_insn_t, branch_insns);\n    if (insn->code == MIR_JMPI) continue;\n    if (insn->code == MIR_SWITCH) {\n      start_label_nop = 1;\n      bound_label_nop = start_label_nop + insn->nops - 1;\n    } else if (insn->code == MIR_LADDR) {\n      start_label_nop = 1;\n      bound_label_nop = 2;\n    }\n    for (n = start_label_nop; n < bound_label_nop; n++)\n      insn->ops[n].u.label = insn->ops[n].u.label->data;\n  }\n  while (VARR_LENGTH (MIR_insn_t, labels) != 0) { /* reset data */\n    insn = VARR_POP (MIR_insn_t, labels);\n    insn->data = NULL;\n  }\n}\n\nvoid _MIR_duplicate_func_insns (MIR_context_t ctx, MIR_item_t func_item) {\n  MIR_func_t func;\n  MIR_insn_t insn, new_insn;\n  VARR (MIR_insn_t) * labels, *branch_insns;\n\n  mir_assert (func_item != NULL && func_item->item_type == MIR_func_item);\n  func = func_item->u.func;\n  mir_assert (DLIST_HEAD (MIR_insn_t, func->original_insns) == NULL);\n  func->original_vars_num = VARR_LENGTH (MIR_var_t, func->vars);\n  func->original_insns = func->insns;\n  DLIST_INIT (MIR_insn_t, func->insns);\n  VARR_CREATE (MIR_insn_t, labels, ctx->alloc, 0);\n  VARR_CREATE (MIR_insn_t, branch_insns, ctx->alloc, 0);\n  for (insn = DLIST_HEAD (MIR_insn_t, func->original_insns); insn != NULL;\n       insn = DLIST_NEXT (MIR_insn_t, insn)) { /* copy insns and collect label info */\n    new_insn = MIR_copy_insn (ctx, insn);\n    DLIST_APPEND (MIR_insn_t, func->insns, new_insn);\n    store_labels_for_duplication (ctx, labels, branch_insns, insn, new_insn);\n  }\n  for (MIR_lref_data_t lref = func->first_lref; lref != NULL; lref = lref->next) {\n    lref->orig_label = lref->label;\n    lref->orig_label2 = lref->label2;\n    lref->label = lref->label->data;\n    if (lref->label2 != NULL) lref->label2 = lref->label2->data;\n  }\n  redirect_duplicated_labels (ctx, labels, branch_insns);\n  VARR_DESTROY (MIR_insn_t, labels);\n  VARR_DESTROY (MIR_insn_t, branch_insns);\n}\n\nvoid _MIR_restore_func_insns (MIR_context_t ctx, MIR_item_t func_item) {\n  MIR_func_t func;\n  MIR_insn_t insn;\n\n  mir_assert (func_item != NULL && func_item->item_type == MIR_func_item);\n  func = func_item->u.func;\n  while (VARR_LENGTH (MIR_var_t, func->vars) > func->original_vars_num) {\n    reg_desc_t *rd;\n    int res_p = TRUE;\n    size_t rdn, tab_rdn;\n    MIR_var_t var = VARR_POP (MIR_var_t, func->vars);\n    func_regs_t func_regs = func->internal;\n\n    rd = find_rd_by_name (ctx, var.name, func);\n    mir_assert (rd != NULL);\n    rdn = rd - VARR_ADDR (reg_desc_t, func_regs->reg_descs);\n    res_p &= HTAB_DO (size_t, func_regs->name2rdn_tab, rdn, HTAB_DELETE, tab_rdn);\n    res_p &= HTAB_DO (size_t, func_regs->reg2rdn_tab, rdn, HTAB_DELETE, tab_rdn);\n    mir_assert (res_p);\n  }\n  while ((insn = DLIST_HEAD (MIR_insn_t, func->insns)) != NULL)\n    MIR_remove_insn (ctx, func_item, insn);\n  func->insns = func->original_insns;\n  DLIST_INIT (MIR_insn_t, func->original_insns);\n  for (MIR_lref_data_t lref = func->first_lref; lref != NULL; lref = lref->next) {\n    lref->label = lref->orig_label;\n    lref->label2 = lref->orig_label2;\n    lref->orig_label = lref->orig_label2 = NULL;\n  }\n}\n\nstatic void set_item_name (MIR_item_t item, const char *name) {\n  mir_assert (item != NULL);\n  switch (item->item_type) {\n  case MIR_func_item: item->u.func->name = name; break;\n  case MIR_proto_item: item->u.proto->name = name; break;\n  case MIR_import_item: item->u.import_id = name; break;\n  case MIR_export_item: item->u.export_id = name; break;\n  case MIR_forward_item: item->u.forward_id = name; break;\n  case MIR_bss_item: item->u.bss->name = name; break;\n  case MIR_data_item: item->u.data->name = name; break;\n  case MIR_ref_data_item: item->u.ref_data->name = name; break;\n  case MIR_lref_data_item: item->u.lref_data->name = name; break;\n  case MIR_expr_data_item: item->u.expr_data->name = name; break;\n  default: mir_assert (FALSE);\n  }\n}\n\nstatic void change_var_names (MIR_context_t new_ctx, VARR (MIR_var_t) * vars) {\n  for (size_t i = 0; i < VARR_LENGTH (MIR_var_t, vars); i++) {\n    MIR_var_t *var_ptr = &VARR_ADDR (MIR_var_t, vars)[i];\n    var_ptr->name = get_ctx_str (new_ctx, var_ptr->name);\n  }\n}\n\n/* It is not thread-safe */\nvoid MIR_change_module_ctx (MIR_context_t old_ctx, MIR_module_t m, MIR_context_t new_ctx) {\n  MIR_item_t item, tab_item;\n  MIR_op_mode_t mode;\n  const char *name, *new_name;\n\n  DLIST_REMOVE (MIR_module_t, *MIR_get_module_list (old_ctx), m);\n  DLIST_APPEND (MIR_module_t, *MIR_get_module_list (new_ctx), m);\n  m->name = get_ctx_str (new_ctx, m->name);\n  for (item = DLIST_HEAD (MIR_item_t, m->items); item != NULL;\n       item = DLIST_NEXT (MIR_item_t, item)) {\n    if (item->addr != NULL)\n      MIR_get_error_func (old_ctx) (MIR_ctx_change_error, \"Change context of a loaded module\");\n    if ((name = MIR_item_name (old_ctx, item)) != NULL) {\n      new_name = get_ctx_str (new_ctx, name);\n      if (item_tab_find (old_ctx, name, m) != item) {\n        set_item_name (item, new_name);\n      } else {\n        item_tab_remove (old_ctx, item);\n        set_item_name (item, new_name);\n        tab_item = item_tab_insert (new_ctx, item);\n        mir_assert (item == tab_item);\n      }\n    }\n    if (item->item_type == MIR_proto_item) {\n      change_var_names (new_ctx, item->u.proto->args);\n    } else if (item->item_type == MIR_func_item) {\n      func_regs_t func_regs = item->u.func->internal;\n      reg_desc_t *rds = VARR_ADDR (reg_desc_t, func_regs->reg_descs);\n\n      for (size_t i = 1; i < VARR_LENGTH (reg_desc_t, func_regs->reg_descs); i++) {\n        rds[i].name = (char *) get_ctx_str (new_ctx, rds[i].name);\n        if (rds[i].hard_reg_name != NULL)\n          rds[i].hard_reg_name = (char *) get_ctx_str (new_ctx, rds[i].hard_reg_name);\n      }\n      change_var_names (new_ctx, item->u.func->vars);\n      if (item->u.func->global_vars != NULL) change_var_names (new_ctx, item->u.func->global_vars);\n\n      for (MIR_insn_t insn = DLIST_HEAD (MIR_insn_t, item->u.func->insns); insn != NULL;\n           insn = DLIST_NEXT (MIR_insn_t, insn)) {\n        for (size_t i = 0; i < insn->nops; i++) {\n          if ((mode = insn->ops[i].mode) == MIR_OP_STR) {\n            insn->ops[i].u.str = get_ctx_string (new_ctx, insn->ops[i].u.str).str;\n          } else if (mode == MIR_OP_MEM) {\n            if (insn->ops[i].u.mem.alias != 0)\n              insn->ops[i].u.mem.alias\n                = MIR_alias (new_ctx, MIR_alias_name (old_ctx, insn->ops[i].u.mem.alias));\n            if (insn->ops[i].u.mem.nonalias != 0)\n              insn->ops[i].u.mem.nonalias\n                = MIR_alias (new_ctx, MIR_alias_name (old_ctx, insn->ops[i].u.mem.nonalias));\n          }\n        }\n      }\n    }\n  }\n#undef curr_label_num\n  if (new_ctx->curr_label_num < old_ctx->curr_label_num)\n    new_ctx->curr_label_num = old_ctx->curr_label_num;\n#define curr_label_num ctx->curr_label_num\n}\n\nstatic void output_type (MIR_context_t ctx, FILE *f, MIR_type_t tp) {\n  fprintf (f, \"%s\", MIR_type_str (ctx, tp));\n}\n\nstatic void output_disp (FILE *f, MIR_disp_t disp) { fprintf (f, \"%\" PRId64, (int64_t) disp); }\n\nstatic void output_scale (FILE *f, unsigned scale) { fprintf (f, \"%u\", scale); }\n\nstatic void output_reg (MIR_context_t ctx, FILE *f, MIR_func_t func, MIR_reg_t reg) {\n  fprintf (f, \"%s\", MIR_reg_name (ctx, reg, func));\n}\n\nstatic void output_hard_reg (FILE *f, MIR_reg_t hreg) { fprintf (f, \"hr%u\", hreg); }\n\nstatic int var_is_reg_p (MIR_reg_t var);\nstatic MIR_reg_t var2reg (MIR_reg_t var);\nstatic void output_var (MIR_context_t ctx, FILE *f, MIR_func_t func, MIR_reg_t var) {\n  var_is_reg_p (var) ? output_reg (ctx, f, func, var2reg (var)) : output_hard_reg (f, var);\n}\n\nstatic void output_label (MIR_context_t ctx, FILE *f, MIR_func_t func, MIR_label_t label);\n\nvoid MIR_output_str (MIR_context_t ctx MIR_UNUSED, FILE *f, MIR_str_t str) {\n  fprintf (f, \"\\\"\");\n  for (size_t i = 0; i < str.len; i++)\n    if (str.s[i] == '\\\\')\n      fprintf (f, \"\\\\\\\\\");\n    else if (str.s[i] == '\"')\n      fprintf (f, \"\\\\\\\"\");\n    else if (isprint (str.s[i]))\n      fprintf (f, \"%c\", str.s[i]);\n    else if (str.s[i] == '\\n')\n      fprintf (f, \"\\\\n\");\n    else if (str.s[i] == '\\t')\n      fprintf (f, \"\\\\t\");\n    else if (str.s[i] == '\\v')\n      fprintf (f, \"\\\\v\");\n    else if (str.s[i] == '\\a')\n      fprintf (f, \"\\\\a\");\n    else if (str.s[i] == '\\b')\n      fprintf (f, \"\\\\b\");\n    else if (str.s[i] == '\\f')\n      fprintf (f, \"\\\\f\");\n    else\n      fprintf (f, \"\\\\%03o\", (unsigned char) str.s[i]);\n  fprintf (f, \"\\\"\");\n}\n\nvoid MIR_output_op (MIR_context_t ctx, FILE *f, MIR_op_t op, MIR_func_t func) {\n  switch (op.mode) {\n  case MIR_OP_REG: output_reg (ctx, f, func, op.u.reg); break;\n  case MIR_OP_VAR: output_var (ctx, f, func, op.u.var); break;\n  case MIR_OP_INT: fprintf (f, \"%\" PRId64, op.u.i); break;\n  case MIR_OP_UINT: fprintf (f, \"%\" PRIu64, op.u.u); break;\n  case MIR_OP_FLOAT: fprintf (f, \"%.*ef\", FLT_MANT_DIG, op.u.f); break;\n  case MIR_OP_DOUBLE: fprintf (f, \"%.*e\", DBL_MANT_DIG, op.u.d); break;\n  case MIR_OP_LDOUBLE: fprintf (f, \"%.*LeL\", LDBL_MANT_DIG, op.u.ld); break;\n  case MIR_OP_MEM:\n  case MIR_OP_VAR_MEM: {\n    MIR_reg_t no_reg = op.mode == MIR_OP_MEM ? 0 : MIR_NON_VAR;\n\n    output_type (ctx, f, op.u.mem.type);\n    fprintf (f, \":\");\n    if (op.u.mem.disp != 0 || (op.u.mem.base == no_reg && op.u.mem.index == no_reg))\n      output_disp (f, op.u.mem.disp);\n    if (op.u.mem.base != no_reg || op.u.mem.index != no_reg) {\n      fprintf (f, \"(\");\n      if (op.u.mem.base != no_reg)\n        (op.mode == MIR_OP_MEM ? output_reg : output_var) (ctx, f, func, op.u.mem.base);\n      if (op.u.mem.index != no_reg) {\n        fprintf (f, \", \");\n        (op.mode == MIR_OP_MEM ? output_reg : output_var) (ctx, f, func, op.u.mem.index);\n        if (op.u.mem.scale != 1) {\n          fprintf (f, \", \");\n          output_scale (f, op.u.mem.scale);\n        }\n      }\n      fprintf (f, \")\");\n    }\n    if (op.u.mem.alias != 0 || op.u.mem.nonalias != 0) {\n      fprintf (f, \":\");\n      if (op.u.mem.alias != 0) fprintf (f, \"%s\", MIR_alias_name (ctx, op.u.mem.alias));\n      if (op.u.mem.nonalias != 0) fprintf (f, \":%s\", MIR_alias_name (ctx, op.u.mem.nonalias));\n    }\n    break;\n  }\n  case MIR_OP_REF:\n    if (op.u.ref->module != func->func_item->module) fprintf (f, \"%s.\", op.u.ref->module->name);\n    fprintf (f, \"%s\", MIR_item_name (ctx, op.u.ref));\n    break;\n  case MIR_OP_STR: MIR_output_str (ctx, f, op.u.str); break;\n  case MIR_OP_LABEL: output_label (ctx, f, func, op.u.label); break;\n  default: mir_assert (FALSE);\n  }\n}\n\nstatic void output_label (MIR_context_t ctx, FILE *f, MIR_func_t func, MIR_label_t label) {\n  fprintf (f, \"L\");\n  MIR_output_op (ctx, f, label->ops[0], func);\n}\n\nvoid MIR_output_insn (MIR_context_t ctx, FILE *f, MIR_insn_t insn, MIR_func_t func, int newline_p) {\n  size_t i, nops;\n\n  mir_assert (insn != NULL);\n  if (insn->code == MIR_LABEL) {\n    output_label (ctx, f, func, insn);\n    if (newline_p) fprintf (f, \":\\n\");\n    return;\n  }\n  fprintf (f, \"\\t%s\", MIR_insn_name (ctx, insn->code));\n  nops = MIR_insn_nops (ctx, insn);\n  for (i = 0; i < nops; i++) {\n    fprintf (f, i == 0 ? \"\\t\" : \", \");\n    MIR_output_op (ctx, f, insn->ops[i], func);\n  }\n  if (insn->code == MIR_UNSPEC)\n    fprintf (f, \" # %s\", VARR_GET (MIR_proto_t, unspec_protos, insn->ops[0].u.u)->name);\n  if (newline_p) fprintf (f, \"\\n\");\n}\n\nstatic void output_func_proto (MIR_context_t ctx, FILE *f, size_t nres, MIR_type_t *types,\n                               size_t nargs, VARR (MIR_var_t) * args, int vararg_p) {\n  size_t i;\n  MIR_var_t var;\n\n  for (i = 0; i < nres; i++) {\n    if (i != 0) fprintf (f, \", \");\n    fprintf (f, \"%s\", MIR_type_str (ctx, types[i]));\n  }\n  for (i = 0; i < nargs; i++) {\n    var = VARR_GET (MIR_var_t, args, i);\n    if (i != 0 || nres != 0) fprintf (f, \", \");\n    mir_assert (var.name != NULL);\n    if (!MIR_all_blk_type_p (var.type))\n      fprintf (f, \"%s:%s\", MIR_type_str (ctx, var.type), var.name);\n    else\n      fprintf (f, \"%s:%lu(%s)\", MIR_type_str (ctx, var.type), (unsigned long) var.size, var.name);\n  }\n  if (vararg_p) fprintf (f, nargs == 0 && nres == 0 ? \"...\" : \", ...\");\n  fprintf (f, \"\\n\");\n}\n\nstatic void output_vars (MIR_context_t ctx, FILE *f, MIR_func_t func, VARR (MIR_var_t) * vars,\n                         size_t start, size_t vars_num, const char *prefix) {\n  if (vars == NULL || vars_num == 0) return;\n  for (size_t i = 0; i < vars_num; i++) {\n    MIR_var_t var = VARR_GET (MIR_var_t, vars, i + start);\n    if (i % 8 == 0) {\n      if (i != 0) fprintf (f, \"\\n\");\n      fprintf (f, \"\\t%s\\t\", prefix);\n    }\n    if (i % 8 != 0) fprintf (f, \", \");\n    fprintf (f, \"%s:%s\", MIR_type_str (ctx, var.type), var.name);\n    MIR_reg_t reg = MIR_reg (ctx, var.name, func);\n    const char *hard_reg_name = MIR_reg_hard_reg_name (ctx, reg, func);\n    if (hard_reg_name != NULL) fprintf (f, \":%s\", hard_reg_name);\n  }\n  fprintf (f, \"\\n\");\n}\n\nvoid _MIR_output_data_item_els (MIR_context_t ctx, FILE *f, MIR_item_t item, int c_p) {\n  mir_assert (item->item_type == MIR_data_item);\n  MIR_data_t data = item->u.data;\n  for (size_t i = 0; i < data->nel; i++) {\n    switch (data->el_type) {\n    case MIR_T_I8: fprintf (f, \"%\" PRId8, ((int8_t *) data->u.els)[i]); break;\n    case MIR_T_U8: fprintf (f, \"%\" PRIu8, ((uint8_t *) data->u.els)[i]); break;\n    case MIR_T_I16: fprintf (f, \"%\" PRId16, ((int16_t *) data->u.els)[i]); break;\n    case MIR_T_U16: fprintf (f, \"%\" PRIu16, ((uint16_t *) data->u.els)[i]); break;\n    case MIR_T_I32: fprintf (f, \"%\" PRId32, ((int32_t *) data->u.els)[i]); break;\n    case MIR_T_U32: fprintf (f, \"%\" PRIu32, ((uint32_t *) data->u.els)[i]); break;\n    case MIR_T_I64: fprintf (f, \"%\" PRId64, ((int64_t *) data->u.els)[i]); break;\n    case MIR_T_U64: fprintf (f, \"%\" PRIu64, ((uint64_t *) data->u.els)[i]); break;\n    case MIR_T_F: fprintf (f, \"%.*ef\", FLT_MANT_DIG, ((float *) data->u.els)[i]); break;\n    case MIR_T_D: fprintf (f, \"%.*e\", DBL_MANT_DIG, ((double *) data->u.els)[i]); break;\n    case MIR_T_LD:\n      fprintf (f, \"%.*LeL\", LDBL_MANT_DIG, ((long double *) data->u.els)[i]);\n      break;\n      /* only ptr as ref ??? */\n    case MIR_T_P: fprintf (f, \"0x%\" PRIxPTR, ((uintptr_t *) data->u.els)[i]); break;\n    default: mir_assert (FALSE);\n    }\n    if (i + 1 < data->nel) fprintf (f, \", \");\n  }\n  if (data->el_type == MIR_T_U8 && data->nel != 0 && data->u.els[data->nel - 1] == '\\0') {\n    fprintf (f, c_p ? \"/* \" : \" # \"); /* print possible string as a comment */\n    MIR_output_str (ctx, f, (MIR_str_t){data->nel, (char *) data->u.els});\n    if (c_p) fprintf (f, \" */\");\n  }\n}\n\nvoid MIR_output_item (MIR_context_t ctx, FILE *f, MIR_item_t item) {\n  MIR_insn_t insn;\n  MIR_func_t func;\n  MIR_proto_t proto;\n  MIR_data_t data;\n  MIR_ref_data_t ref_data;\n  MIR_expr_data_t expr_data;\n  size_t vars_num, nglobal;\n\n  mir_assert (f != NULL && item != NULL);\n  if (item->item_type == MIR_export_item) {\n    fprintf (f, \"\\texport\\t%s\\n\", item->u.export_id);\n    return;\n  }\n  if (item->item_type == MIR_import_item) {\n    fprintf (f, \"\\timport\\t%s\\n\", item->u.import_id);\n    return;\n  }\n  if (item->item_type == MIR_forward_item) {\n    fprintf (f, \"\\tforward\\t%s\\n\", item->u.forward_id);\n    return;\n  }\n  if (item->item_type == MIR_bss_item) {\n    if (item->u.bss->name != NULL) fprintf (f, \"%s:\", item->u.bss->name);\n    fprintf (f, \"\\tbss\\t%\" PRIu64 \"\\n\", item->u.bss->len);\n    return;\n  }\n  if (item->item_type == MIR_ref_data_item) {\n    ref_data = item->u.ref_data;\n    if (ref_data->name != NULL) fprintf (f, \"%s:\", ref_data->name);\n    fprintf (f, \"\\tref\\t%s, %\" PRId64 \"\\n\", MIR_item_name (ctx, ref_data->ref_item),\n             (int64_t) ref_data->disp);\n    return;\n  }\n  if (item->item_type == MIR_lref_data_item) {\n    MIR_lref_data_t lref_data = item->u.lref_data;\n    if (lref_data->name != NULL) fprintf (f, \"%s:\", lref_data->name);\n    mir_assert (lref_data->label->ops[0].mode == MIR_OP_INT);\n    fprintf (f, \"\\tlref\\tL%\" PRId64, lref_data->label->ops[0].u.i);\n    mir_assert (lref_data->label2 == NULL || lref_data->label2->ops[0].mode == MIR_OP_INT);\n    if (lref_data->label2 != NULL) fprintf (f, \", L%\" PRId64, lref_data->label2->ops[0].u.i);\n    if (lref_data->disp != 0) fprintf (f, \", %\" PRId64, (int64_t) lref_data->disp);\n    fprintf (f, \"\\n\");\n    return;\n  }\n  if (item->item_type == MIR_expr_data_item) {\n    expr_data = item->u.expr_data;\n    if (expr_data->name != NULL) fprintf (f, \"%s:\", expr_data->name);\n    fprintf (f, \"\\texpr\\t%s\", MIR_item_name (ctx, expr_data->expr_item));\n  }\n  if (item->item_type == MIR_data_item) {\n    data = item->u.data;\n    if (data->name != NULL) fprintf (f, \"%s:\", data->name);\n    fprintf (f, \"\\t%s\\t\", MIR_type_str (ctx, data->el_type));\n    _MIR_output_data_item_els (ctx, f, item, FALSE);\n    fprintf (f, \"\\n\");\n    return;\n  }\n  if (item->item_type == MIR_proto_item) {\n    proto = item->u.proto;\n    fprintf (f, \"%s:\\tproto\\t\", proto->name);\n    output_func_proto (ctx, f, proto->nres, proto->res_types, VARR_LENGTH (MIR_var_t, proto->args),\n                       proto->args, proto->vararg_p);\n    return;\n  }\n  func = item->u.func;\n  fprintf (f, \"%s:\\tfunc\\t\", func->name);\n  output_func_proto (ctx, f, func->nres, func->res_types, func->nargs, func->vars, func->vararg_p);\n  vars_num = VARR_LENGTH (MIR_var_t, func->vars) - func->nargs;\n  nglobal = func->global_vars == NULL ? 0 : VARR_LENGTH (MIR_var_t, func->global_vars);\n  output_vars (ctx, f, func, func->vars, func->nargs, vars_num, \"local\");\n  output_vars (ctx, f, func, func->global_vars, 0, nglobal, \"global\");\n  fprintf (f, \"\\n# %u arg%s, %ld local%s, %ld global%s\\n\", func->nargs, func->nargs == 1 ? \"\" : \"s\",\n           (long) vars_num, vars_num == 1 ? \"\" : \"s\", (long) nglobal, nglobal == 1 ? \"\" : \"s\");\n  for (insn = DLIST_HEAD (MIR_insn_t, func->insns); insn != NULL;\n       insn = DLIST_NEXT (MIR_insn_t, insn))\n    MIR_output_insn (ctx, f, insn, func, TRUE);\n  fprintf (f, \"\\tendfunc\\n\");\n}\n\nvoid MIR_output_module (MIR_context_t ctx, FILE *f, MIR_module_t module) {\n  mir_assert (f != NULL && module != NULL);\n  fprintf (f, \"%s:\\tmodule\\n\", module->name);\n  for (MIR_item_t item = DLIST_HEAD (MIR_item_t, module->items); item != NULL;\n       item = DLIST_NEXT (MIR_item_t, item))\n    MIR_output_item (ctx, f, item);\n  fprintf (f, \"\\tendmodule\\n\");\n}\n\nvoid MIR_output (MIR_context_t ctx, FILE *f) {\n  mir_assert (f != NULL);\n  for (MIR_module_t module = DLIST_HEAD (MIR_module_t, all_modules); module != NULL;\n       module = DLIST_NEXT (MIR_module_t, module))\n    MIR_output_module (ctx, f, module);\n}\n\n/* New Page */\n/* This page contains code for simplification and inlining */\n\nstatic MIR_insn_t insert_op_insn (MIR_context_t ctx, int out_p, MIR_item_t func_item,\n                                  MIR_insn_t anchor, MIR_insn_t insn) {\n  if (!out_p) {\n    MIR_insert_insn_before (ctx, func_item, anchor, insn);\n    return anchor;\n  }\n  MIR_insert_insn_after (ctx, func_item, anchor, insn);\n  return insn;\n}\n\ntypedef struct {\n  MIR_insn_code_t code;\n  MIR_type_t type;\n  MIR_op_t op1, op2;\n  MIR_reg_t reg;\n} val_t;\n\nDEF_HTAB (val_t);\n\nstruct simplify_ctx {\n  HTAB (val_t) * val_tab;\n  /* temp_insns is for branch or ret insns */\n  VARR (MIR_insn_t) * temp_insns, *cold_insns, *labels;\n  VARR (MIR_reg_t) * inline_reg_map;\n  VARR (MIR_insn_t) * anchors;\n  VARR (size_t) * alloca_sizes;\n  size_t new_label_num, inlined_calls, inline_insns_before, inline_insns_after;\n};\n\n#define val_tab ctx->simplify_ctx->val_tab\n#define temp_insns ctx->simplify_ctx->temp_insns\n#define cold_insns ctx->simplify_ctx->cold_insns\n#define labels ctx->simplify_ctx->labels\n#define inline_reg_map ctx->simplify_ctx->inline_reg_map\n#define anchors ctx->simplify_ctx->anchors\n#define alloca_sizes ctx->simplify_ctx->alloca_sizes\n#define new_label_num ctx->simplify_ctx->new_label_num\n#define inlined_calls ctx->simplify_ctx->inlined_calls\n#define inline_insns_before ctx->simplify_ctx->inline_insns_before\n#define inline_insns_after ctx->simplify_ctx->inline_insns_after\n\nstatic htab_hash_t val_hash (val_t v, void *arg) {\n  MIR_context_t ctx = arg;\n  htab_hash_t h;\n\n  h = (htab_hash_t) mir_hash_step (mir_hash_init (0), (uint64_t) v.code);\n  h = (htab_hash_t) mir_hash_step (h, (uint64_t) v.type);\n  h = MIR_op_hash_step (ctx, h, v.op1);\n  if (v.code != MIR_INSN_BOUND) h = MIR_op_hash_step (ctx, h, v.op2);\n  return (htab_hash_t) mir_hash_finish (h);\n}\n\nstatic int val_eq (val_t v1, val_t v2, void *arg) {\n  MIR_context_t ctx = arg;\n\n  if (v1.code != v2.code || v1.type != v2.type || !MIR_op_eq_p (ctx, v1.op1, v2.op1)) return FALSE;\n  return v1.code == MIR_INSN_BOUND || MIR_op_eq_p (ctx, v1.op2, v2.op2);\n}\n\nstatic void simplify_init (MIR_context_t ctx) {\n  if ((ctx->simplify_ctx = MIR_malloc (ctx->alloc, sizeof (struct simplify_ctx))) == NULL)\n    MIR_get_error_func (ctx) (MIR_alloc_error, \"Not enough memory for ctx\");\n  HTAB_CREATE (val_t, val_tab, ctx->alloc, 512, val_hash, val_eq, ctx);\n  VARR_CREATE (MIR_insn_t, temp_insns, ctx->alloc, 0);\n  VARR_CREATE (MIR_insn_t, cold_insns, ctx->alloc, 0);\n  VARR_CREATE (MIR_insn_t, labels, ctx->alloc, 0);\n  VARR_CREATE (MIR_reg_t, inline_reg_map, ctx->alloc, 256);\n  VARR_CREATE (MIR_insn_t, anchors, ctx->alloc, 32);\n  VARR_CREATE (size_t, alloca_sizes, ctx->alloc, 32);\n  inlined_calls = inline_insns_before = inline_insns_after = 0;\n}\n\nstatic void simplify_finish (MIR_context_t ctx) {\n  VARR_DESTROY (size_t, alloca_sizes);\n  VARR_DESTROY (MIR_insn_t, anchors);\n  VARR_DESTROY (MIR_reg_t, inline_reg_map);\n#if 0\n  if (inlined_calls != 0)\n    fprintf (stderr, \"inlined calls = %lu, insns before = %lu, insns_after = %lu, ratio = %.2f\\n\",\n             inlined_calls, inline_insns_before, inline_insns_after,\n             (double) inline_insns_after / inline_insns_before);\n#endif\n  VARR_DESTROY (MIR_insn_t, labels);\n  VARR_DESTROY (MIR_insn_t, temp_insns);\n  VARR_DESTROY (MIR_insn_t, cold_insns);\n  HTAB_DESTROY (val_t, val_tab);\n  MIR_free (ctx->alloc, ctx->simplify_ctx);\n  ctx->simplify_ctx = NULL;\n}\n\nstatic void simplify_module_init (MIR_context_t ctx) {\n  new_label_num = 0;\n  VARR_TRUNC (uint8_t, used_label_p, 0);\n}\n\nstatic void vn_empty (MIR_context_t ctx) { HTAB_CLEAR (val_t, val_tab); }\n\nstatic MIR_reg_t vn_add_val (MIR_context_t ctx, MIR_func_t func, MIR_type_t type,\n                             MIR_insn_code_t code, MIR_op_t op1, MIR_op_t op2) {\n  val_t val, tab_val;\n\n  val.type = type;\n  val.code = code;\n  val.op1 = op1;\n  val.op2 = op2;\n  if (HTAB_DO (val_t, val_tab, val, HTAB_FIND, tab_val)) return tab_val.reg;\n  val.reg = new_temp_reg (ctx, type, func);\n  HTAB_DO (val_t, val_tab, val, HTAB_INSERT, tab_val);\n  return val.reg;\n}\n\nvoid _MIR_get_temp_item_name (MIR_context_t ctx MIR_UNUSED, MIR_module_t module, char *buff,\n                              size_t buff_len) {\n  mir_assert (module != NULL);\n  module->last_temp_item_num++;\n  snprintf (buff, buff_len, \"%s%u\", TEMP_ITEM_NAME_PREFIX, (unsigned) module->last_temp_item_num);\n}\n\nstatic MIR_insn_code_t get_type_move_code (MIR_type_t type) {\n  return (type == MIR_T_F    ? MIR_FMOV\n          : type == MIR_T_D  ? MIR_DMOV\n          : type == MIR_T_LD ? MIR_LDMOV\n                             : MIR_MOV);\n}\n\nstatic void simplify_op (MIR_context_t ctx, MIR_item_t func_item, MIR_insn_t insn, int nop,\n                         int out_p, MIR_insn_code_t code, int keep_ref_p, int mem_float_p) {\n  mir_assert (insn != NULL && func_item != NULL);\n  MIR_op_t new_op, mem_op, *op = &insn->ops[nop];\n  MIR_insn_t new_insn;\n  MIR_func_t func = func_item->u.func;\n  MIR_type_t type;\n  MIR_op_mode_t value_mode = op->value_mode;\n  int move_p = code == MIR_MOV || code == MIR_FMOV || code == MIR_DMOV || code == MIR_LDMOV;\n\n  if (code == MIR_PHI || code == MIR_USE) return; /* do nothing: it is use or phi insn */\n  if (code == MIR_UNSPEC && nop == 0) return;     /* do nothing: it is an unspec code */\n  if (MIR_call_code_p (code)) {\n    if (nop == 0) return; /* do nothing: it is a prototype */\n    if (nop == 1 && op->mode == MIR_OP_REF\n        && (op->u.ref->item_type == MIR_import_item || op->u.ref->item_type == MIR_func_item))\n      return; /* do nothing: it is an immediate operand */\n  }\n  if (code == MIR_VA_ARG && nop == 2) return; /* do nothing: this operand is used as a type */\n  if ((code == MIR_PRBEQ || code == MIR_PRBNE) && nop == 2) return; /* it is a property */\n  if (code == MIR_PRSET && nop == 1) return;                        /* it is a property */\n  switch (op->mode) {\n  case MIR_OP_REF:\n    if (keep_ref_p) break;\n    /* falls through */\n  case MIR_OP_INT:\n  case MIR_OP_UINT:\n  case MIR_OP_FLOAT:\n  case MIR_OP_DOUBLE:\n  case MIR_OP_LDOUBLE:\n  case MIR_OP_STR:\n    mir_assert (!out_p);\n    if (op->mode == MIR_OP_REF) {\n      for (MIR_item_t item = op->u.ref; item != NULL; item = item->ref_def)\n        if (item->item_type != MIR_export_item && item->item_type != MIR_forward_item) {\n          op->u.ref = item;\n          break;\n        }\n    } else if (op->mode == MIR_OP_STR\n               || (mem_float_p\n                   && (op->mode == MIR_OP_FLOAT || op->mode == MIR_OP_DOUBLE\n                       || op->mode == MIR_OP_LDOUBLE))) {\n      const char *name;\n      char buff[50];\n      MIR_item_t item;\n      MIR_module_t m = curr_module;\n\n      curr_module = func_item->module;\n      _MIR_get_temp_item_name (ctx, curr_module, buff, sizeof (buff));\n      name = buff;\n      if (op->mode == MIR_OP_STR) {\n        item = MIR_new_string_data (ctx, name, op->u.str);\n        *op = MIR_new_ref_op (ctx, item);\n      } else {\n        if (op->mode == MIR_OP_FLOAT)\n          item = MIR_new_data (ctx, name, MIR_T_F, 1, (uint8_t *) &op->u.f);\n        else if (op->mode == MIR_OP_DOUBLE)\n          item = MIR_new_data (ctx, name, MIR_T_D, 1, (uint8_t *) &op->u.d);\n        else\n          item = MIR_new_data (ctx, name, MIR_T_LD, 1, (uint8_t *) &op->u.ld);\n        type = op->mode == MIR_OP_FLOAT ? MIR_T_F : op->mode == MIR_OP_DOUBLE ? MIR_T_D : MIR_T_LD;\n        *op = MIR_new_ref_op (ctx, item);\n        new_op = MIR_new_reg_op (ctx, vn_add_val (ctx, func, MIR_T_I64, MIR_INSN_BOUND, *op, *op));\n        MIR_insert_insn_before (ctx, func_item, insn, MIR_new_insn (ctx, MIR_MOV, new_op, *op));\n        *op = MIR_new_mem_op (ctx, type, 0, new_op.u.reg, 0, 1);\n      }\n      if (func_item->addr != NULL) /* The function was already loaded: we should load new data */\n        load_bss_data_section (ctx, item, TRUE);\n      curr_module = m;\n    }\n    if (move_p) return;\n    type = (op->mode == MIR_OP_FLOAT     ? MIR_T_F\n            : op->mode == MIR_OP_DOUBLE  ? MIR_T_D\n            : op->mode == MIR_OP_LDOUBLE ? MIR_T_LD\n            : op->mode == MIR_OP_MEM     ? op->u.mem.type\n                                         : MIR_T_I64);\n    new_op = MIR_new_reg_op (ctx, vn_add_val (ctx, func, type, MIR_INSN_BOUND, *op, *op));\n    MIR_insert_insn_before (ctx, func_item, insn,\n                            MIR_new_insn (ctx, get_type_move_code (type), new_op, *op));\n    *op = new_op;\n    break;\n  case MIR_OP_REG:\n    if (MIR_reg_hard_reg_name (ctx, op->u.reg, func) == NULL) break;\n    int another_nop = nop == 0 ? 1 : 0;\n    if (move_p && insn->ops[another_nop].mode == MIR_OP_REG\n        && MIR_reg_hard_reg_name (ctx, insn->ops[another_nop].u.reg, func) == NULL)\n      break;\n    type = MIR_reg_type (ctx, op->u.reg, func);\n    new_op = MIR_new_reg_op (ctx, vn_add_val (ctx, func, type, MIR_INSN_BOUND, *op, *op));\n    if (out_p) {\n      MIR_insert_insn_after (ctx, func_item, insn,\n                             MIR_new_insn (ctx, get_type_move_code (type), *op, new_op));\n    } else {\n      MIR_insert_insn_before (ctx, func_item, insn,\n                              MIR_new_insn (ctx, get_type_move_code (type), new_op, *op));\n    }\n    *op = new_op;\n    break;\n  case MIR_OP_VAR:\n  case MIR_OP_LABEL: break; /* Do nothing */\n  case MIR_OP_MEM: {\n    MIR_op_t reg_op;\n    MIR_reg_t addr_reg = 0;\n\n    if (op->u.mem.base != 0 && MIR_reg_hard_reg_name (ctx, op->u.mem.base, func) != NULL) {\n      reg_op = MIR_new_reg_op (ctx, op->u.mem.base);\n      new_op\n        = MIR_new_reg_op (ctx, vn_add_val (ctx, func, MIR_T_I64, MIR_INSN_BOUND, reg_op, reg_op));\n      MIR_insert_insn_before (ctx, func_item, insn, MIR_new_insn (ctx, MIR_MOV, new_op, reg_op));\n      op->u.mem.base = new_op.u.reg;\n    }\n    if (op->u.mem.index != 0 && MIR_reg_hard_reg_name (ctx, op->u.mem.index, func) != NULL) {\n      reg_op = MIR_new_reg_op (ctx, op->u.mem.index);\n      new_op\n        = MIR_new_reg_op (ctx, vn_add_val (ctx, func, MIR_T_I64, MIR_INSN_BOUND, reg_op, reg_op));\n      MIR_insert_insn_before (ctx, func_item, insn, MIR_new_insn (ctx, MIR_MOV, new_op, reg_op));\n      op->u.mem.index = new_op.u.reg;\n    }\n    mem_op = *op;\n    type = mem_op.u.mem.type;\n    if (op->u.mem.base != 0 && op->u.mem.disp == 0\n        && (op->u.mem.index == 0 || op->u.mem.scale == 0)) {\n      addr_reg = op->u.mem.base;\n    } else if (op->u.mem.base == 0 && op->u.mem.index != 0 && op->u.mem.scale == 1\n               && op->u.mem.disp == 0) {\n      addr_reg = op->u.mem.index;\n    } else {\n      int after_p = !move_p && out_p;\n      MIR_reg_t disp_reg = 0, scale_ind_reg = op->u.mem.index;\n      MIR_reg_t base_reg = op->u.mem.base, base_ind_reg = 0;\n\n      if (op->u.mem.disp != 0) {\n        MIR_op_t disp_op = MIR_new_int_op (ctx, op->u.mem.disp);\n\n        disp_reg = vn_add_val (ctx, func, MIR_T_I64, MIR_INSN_BOUND, disp_op, disp_op);\n        insn\n          = insert_op_insn (ctx, after_p, func_item, insn,\n                            MIR_new_insn (ctx, MIR_MOV, MIR_new_reg_op (ctx, disp_reg), disp_op));\n      }\n      if (scale_ind_reg != 0 && op->u.mem.scale > 1) {\n        MIR_op_t ind_op = MIR_new_reg_op (ctx, op->u.mem.index);\n        MIR_op_t scale_reg_op, scale_int_op = MIR_new_int_op (ctx, op->u.mem.scale);\n\n        scale_reg_op = MIR_new_reg_op (ctx, vn_add_val (ctx, func, MIR_T_I64, MIR_INSN_BOUND,\n                                                        scale_int_op, scale_int_op));\n        insn = insert_op_insn (ctx, after_p, func_item, insn,\n                               MIR_new_insn (ctx, MIR_MOV, scale_reg_op, scale_int_op));\n        scale_ind_reg = vn_add_val (ctx, func, MIR_T_I64, MIR_MUL, ind_op, scale_reg_op);\n        insn = insert_op_insn (ctx, after_p, func_item, insn,\n                               MIR_new_insn (ctx, MIR_MUL, MIR_new_reg_op (ctx, scale_ind_reg),\n                                             ind_op, scale_reg_op));\n      }\n      if (base_reg != 0 && scale_ind_reg != 0) {\n        MIR_op_t base_op = MIR_new_reg_op (ctx, base_reg),\n                 ind_op = MIR_new_reg_op (ctx, scale_ind_reg);\n\n        base_ind_reg = vn_add_val (ctx, func, MIR_T_I64, MIR_ADD, base_op, ind_op);\n        insn = insert_op_insn (ctx, after_p, func_item, insn,\n                               MIR_new_insn (ctx, MIR_ADD, MIR_new_reg_op (ctx, base_ind_reg),\n                                             base_op, ind_op));\n      } else {\n        base_ind_reg = base_reg != 0 ? base_reg : scale_ind_reg;\n      }\n      if (base_ind_reg == 0) {\n        mir_assert (disp_reg != 0);\n        addr_reg = disp_reg;\n      } else if (disp_reg == 0) {\n        mir_assert (base_ind_reg != 0);\n        addr_reg = base_ind_reg;\n      } else {\n        MIR_op_t base_ind_op = MIR_new_reg_op (ctx, base_ind_reg);\n        MIR_op_t disp_op = MIR_new_reg_op (ctx, disp_reg);\n\n        addr_reg = vn_add_val (ctx, func, MIR_T_I64, MIR_ADD, base_ind_op, disp_op);\n        insn = insert_op_insn (ctx, after_p, func_item, insn,\n                               MIR_new_insn (ctx, MIR_ADD, MIR_new_reg_op (ctx, addr_reg),\n                                             base_ind_op, disp_op));\n      }\n    }\n    mem_op.u.mem.base = addr_reg;\n    mem_op.u.mem.disp = 0;\n    mem_op.u.mem.index = 0;\n    mem_op.u.mem.scale = 0;\n    if (move_p && (nop == 1 || insn->ops[1].mode == MIR_OP_REG)) {\n      *op = mem_op;\n    } else if (((code == MIR_VA_START && nop == 0)\n                || ((code == MIR_VA_ARG || code == MIR_VA_BLOCK_ARG) && nop == 1)\n                || (code == MIR_VA_END && nop == 0))\n               && mem_op.u.mem.type == MIR_T_UNDEF) {\n      *op = MIR_new_reg_op (ctx, addr_reg);\n    } else if (!MIR_all_blk_type_p (mem_op.u.mem.type) || !MIR_call_code_p (code)) {\n      type = (mem_op.u.mem.type == MIR_T_F || mem_op.u.mem.type == MIR_T_D\n                  || mem_op.u.mem.type == MIR_T_LD\n                ? mem_op.u.mem.type\n                : MIR_T_I64);\n      code = get_type_move_code (type);\n      new_op = MIR_new_reg_op (ctx, vn_add_val (ctx, func, type, MIR_INSN_BOUND, mem_op, mem_op));\n      if (out_p)\n        new_insn = MIR_new_insn (ctx, code, mem_op, new_op);\n      else\n        new_insn = MIR_new_insn (ctx, code, new_op, mem_op);\n      insn = insert_op_insn (ctx, out_p, func_item, insn, new_insn);\n      *op = new_op;\n    }\n    break;\n  }\n  default:\n    /* We don't simplify code with hard regs.  */\n    mir_assert (FALSE);\n  }\n  op->value_mode = value_mode;\n}\n\nstatic void simplify_insn (MIR_context_t ctx, MIR_item_t func_item, MIR_insn_t insn, int keep_ref_p,\n                           int mem_float_p) {\n  int out_p;\n  mir_assert (insn != NULL);\n  MIR_insn_code_t code = insn->code;\n  size_t i, nops = MIR_insn_nops (ctx, insn);\n\n  for (i = 0; i < nops; i++) {\n    MIR_insn_op_mode (ctx, insn, i, &out_p);\n    simplify_op (ctx, func_item, insn, (int) i, out_p, code,\n                 MIR_call_code_p (insn->code) && i == 1 && keep_ref_p, mem_float_p);\n  }\n}\n\nstatic void make_one_ret (MIR_context_t ctx, MIR_item_t func_item) {\n  size_t i, j;\n  MIR_insn_code_t mov_code, ext_code;\n  MIR_reg_t ret_reg;\n  MIR_op_t reg_op, ret_reg_op;\n  MIR_func_t func = func_item->u.func;\n  MIR_type_t *res_types = func->res_types;\n  MIR_insn_t ret_label = NULL, insn, last_ret_insn;\n  VARR (MIR_insn_t) *ret_insns = temp_insns;\n  VARR (MIR_op_t) *ret_ops = temp_ops;\n\n  if (VARR_LENGTH (MIR_insn_t, ret_insns) == 0) return; /* jcall/jret func */\n  last_ret_insn = VARR_LAST (MIR_insn_t, ret_insns);\n  VARR_TRUNC (MIR_op_t, ret_ops, 0);\n  if (VARR_LENGTH (MIR_insn_t, ret_insns) > 1) {\n    ret_label = MIR_new_label (ctx);\n    MIR_insert_insn_before (ctx, func_item, last_ret_insn, ret_label);\n  }\n  for (i = 0; i < func->nres; i++) { /* generate ext insn before last ret */\n    ret_reg_op = last_ret_insn->ops[i];\n    VARR_PUSH (MIR_op_t, ret_ops, ret_reg_op);\n    switch (res_types[i]) {\n    case MIR_T_I8: ext_code = MIR_EXT8; break;\n    case MIR_T_U8: ext_code = MIR_UEXT8; break;\n    case MIR_T_I16: ext_code = MIR_EXT16; break;\n    case MIR_T_U16: ext_code = MIR_UEXT16; break;\n    case MIR_T_I32: ext_code = MIR_EXT32; break;\n    case MIR_T_U32: ext_code = MIR_UEXT32; break;\n    default: ext_code = MIR_INVALID_INSN; break;\n    }\n    if (ext_code == MIR_INVALID_INSN) continue;\n    mov_code = get_type_move_code (res_types[i]);\n    ret_reg = _MIR_new_temp_reg (ctx, mov_code == MIR_MOV ? MIR_T_I64 : res_types[i], func);\n    ret_reg_op = MIR_new_reg_op (ctx, ret_reg);\n    MIR_insert_insn_before (ctx, func_item, last_ret_insn,\n                            MIR_new_insn (ctx, ext_code, ret_reg_op, last_ret_insn->ops[i]));\n    last_ret_insn->ops[i] = ret_reg_op;\n  }\n  /* change ret insns (except last one) to moves & jumps */\n  for (i = 0; i < VARR_LENGTH (MIR_insn_t, ret_insns); i++) {\n    insn = VARR_GET (MIR_insn_t, ret_insns, i);\n    if (insn == last_ret_insn) continue;\n    mir_assert (insn->code == MIR_RET || func->nres == MIR_insn_nops (ctx, insn));\n    for (j = 0; j < func->nres; j++) {\n      mov_code = get_type_move_code (res_types[j]);\n      reg_op = insn->ops[j];\n      mir_assert (reg_op.mode == MIR_OP_REG);\n      ret_reg_op = VARR_GET (MIR_op_t, ret_ops, j);\n      MIR_insert_insn_before (ctx, func_item, insn,\n                              MIR_new_insn (ctx, mov_code, ret_reg_op, reg_op));\n    }\n    MIR_insert_insn_before (ctx, func_item, insn,\n                            MIR_new_insn (ctx, MIR_JMP, MIR_new_label_op (ctx, ret_label)));\n    MIR_remove_insn (ctx, func_item, insn);\n  }\n}\n\nstatic void mark_used_label (MIR_context_t ctx, MIR_label_t label) {\n  int64_t label_num = label->ops[0].u.i;\n  while (label_num >= (int64_t) VARR_LENGTH (uint8_t, used_label_p))\n    VARR_PUSH (uint8_t, used_label_p, FALSE);\n  VARR_SET (uint8_t, used_label_p, label_num, TRUE);\n}\n\nstatic void remove_unused_and_enumerate_labels (MIR_context_t ctx, MIR_item_t func_item) {\n  for (size_t i = 0; i < VARR_LENGTH (MIR_insn_t, labels); i++) {\n    MIR_insn_t label = VARR_GET (MIR_insn_t, labels, i);\n    int64_t label_num = label->ops[0].u.i;\n\n    if (label_num < (int64_t) VARR_LENGTH (uint8_t, used_label_p)\n        && VARR_GET (uint8_t, used_label_p, label_num)) {\n      label->ops[0] = MIR_new_int_op (ctx, new_label_num++);\n      continue;\n    }\n    MIR_remove_insn (ctx, func_item, label);\n  }\n  VARR_TRUNC (MIR_insn_t, labels, 0);\n}\n\nMIR_insn_code_t MIR_reverse_branch_code (MIR_insn_code_t code) {\n  switch (code) {\n  case MIR_BT: return MIR_BF;\n  case MIR_BTS: return MIR_BFS;\n  case MIR_BF: return MIR_BT;\n  case MIR_BFS: return MIR_BTS;\n  case MIR_BEQ: return MIR_BNE;\n  case MIR_BEQS: return MIR_BNES;\n  case MIR_BNE: return MIR_BEQ;\n  case MIR_BNES: return MIR_BEQS;\n  case MIR_BLT: return MIR_BGE;\n  case MIR_BLTS: return MIR_BGES;\n  case MIR_UBLT: return MIR_UBGE;\n  case MIR_UBLTS: return MIR_UBGES;\n  case MIR_BLE: return MIR_BGT;\n  case MIR_BLES: return MIR_BGTS;\n  case MIR_UBLE: return MIR_UBGT;\n  case MIR_UBLES: return MIR_UBGTS;\n  case MIR_BGT: return MIR_BLE;\n  case MIR_BGTS: return MIR_BLES;\n  case MIR_UBGT: return MIR_UBLE;\n  case MIR_UBGTS: return MIR_UBLES;\n  case MIR_BGE: return MIR_BLT;\n  case MIR_BGES: return MIR_BLTS;\n  case MIR_UBGE: return MIR_UBLT;\n  case MIR_UBGES: return MIR_UBLTS;\n  case MIR_BO: return MIR_BNO;\n  case MIR_UBO: return MIR_UBNO;\n  case MIR_BNO: return MIR_BO;\n  case MIR_UBNO: return MIR_UBO;\n  case MIR_PRBEQ: return MIR_PRBNE;\n  case MIR_PRBNE: return MIR_PRBEQ;\n  default: return MIR_INSN_BOUND;\n  }\n}\n\nstatic MIR_insn_t skip_labels (MIR_label_t label, MIR_label_t stop) {\n  for (MIR_insn_t insn = label;; insn = DLIST_NEXT (MIR_insn_t, insn))\n    if (insn == NULL || insn->code != MIR_LABEL || insn == stop) return insn;\n}\n\nstatic MIR_insn_t last_label (MIR_label_t label) {\n  MIR_insn_t next_insn;\n  mir_assert (label->code == MIR_LABEL);\n  while ((next_insn = DLIST_NEXT (MIR_insn_t, label)) != NULL && next_insn->code == MIR_LABEL)\n    label = next_insn;\n  return label;\n}\n\nstatic int64_t natural_alignment (int64_t s) { return s <= 2 ? s : s <= 4 ? 4 : s <= 8 ? 8 : 16; }\n\nstatic const int MAX_JUMP_CHAIN_LEN = 32;\n\nstatic int64_t get_alloca_size_align (int64_t size, int64_t *align_ptr) {\n  int64_t align;\n  size = size <= 0 ? 1 : size;\n  *align_ptr = align = natural_alignment (size);\n  return (size + align - 1) / align * align;\n}\n\nstatic int simplify_func (MIR_context_t ctx, MIR_item_t func_item, int mem_float_p) {\n  MIR_func_t func = func_item->u.func;\n  MIR_insn_t insn, next_insn, next_next_insn, jmp_insn, new_insn, label;\n  MIR_insn_code_t ext_code, rev_code;\n  int jmps_num = 0, inline_p = FALSE;\n\n  if (func_item->item_type != MIR_func_item)\n    MIR_get_error_func (ctx) (MIR_wrong_param_value_error, \"MIR_remove_simplify: wrong func item\");\n  vn_empty (ctx);\n  func = func_item->u.func;\n  for (size_t i = 0; i < func->nargs; i++) {\n    MIR_var_t var = VARR_GET (MIR_var_t, func->vars, i);\n\n    if (var.type == MIR_T_I64 || var.type == MIR_T_U64 || var.type == MIR_T_F || var.type == MIR_T_D\n        || var.type == MIR_T_LD)\n      continue;\n    switch (var.type) {\n    case MIR_T_I8: ext_code = MIR_EXT8; break;\n    case MIR_T_U8: ext_code = MIR_UEXT8; break;\n    case MIR_T_I16: ext_code = MIR_EXT16; break;\n    case MIR_T_U16: ext_code = MIR_UEXT16; break;\n    case MIR_T_I32: ext_code = MIR_EXT32; break;\n    case MIR_T_U32: ext_code = MIR_UEXT32; break;\n    default: ext_code = MIR_INVALID_INSN; break;\n    }\n    if (ext_code != MIR_INVALID_INSN) {\n      MIR_reg_t reg = MIR_reg (ctx, var.name, func);\n      new_insn = MIR_new_insn (ctx, ext_code, MIR_new_reg_op (ctx, reg), MIR_new_reg_op (ctx, reg));\n      MIR_prepend_insn (ctx, func_item, new_insn);\n    }\n  }\n  VARR_TRUNC (MIR_insn_t, temp_insns, 0);\n  VARR_TRUNC (MIR_insn_t, labels, 0);\n  for (insn = DLIST_HEAD (MIR_insn_t, func->insns); insn != NULL; insn = next_insn) {\n    MIR_insn_code_t code = insn->code;\n    MIR_op_t temp_op;\n\n    if ((code == MIR_MOV || code == MIR_FMOV || code == MIR_DMOV || code == MIR_LDMOV)\n        && insn->ops[0].mode == MIR_OP_MEM && insn->ops[1].mode == MIR_OP_MEM) {\n      temp_op = MIR_new_reg_op (ctx, new_temp_reg (ctx,\n                                                   code == MIR_MOV    ? MIR_T_I64\n                                                   : code == MIR_FMOV ? MIR_T_F\n                                                   : code == MIR_DMOV ? MIR_T_D\n                                                                      : MIR_T_LD,\n                                                   func));\n      MIR_insert_insn_after (ctx, func_item, insn, MIR_new_insn (ctx, code, insn->ops[0], temp_op));\n      insn->ops[0] = temp_op;\n    }\n    if (code == MIR_RET) VARR_PUSH (MIR_insn_t, temp_insns, insn);\n    if (code == MIR_LABEL) VARR_PUSH (MIR_insn_t, labels, insn);\n    next_insn = DLIST_NEXT (MIR_insn_t, insn);\n    if (code == MIR_ALLOCA\n        && (insn->ops[1].mode == MIR_OP_INT || insn->ops[1].mode == MIR_OP_UINT)) {\n      /* Consolidate adjacent allocas */\n      int64_t size, overall_size, align, max_align;\n\n      overall_size = get_alloca_size_align (insn->ops[1].u.i, &max_align);\n      while (next_insn != NULL && next_insn->code == MIR_ALLOCA\n             && (next_insn->ops[1].mode == MIR_OP_INT || next_insn->ops[1].mode == MIR_OP_UINT)\n             && !MIR_op_eq_p (ctx, insn->ops[0], next_insn->ops[0])) {\n        size = get_alloca_size_align (next_insn->ops[1].u.i, &align);\n        if (max_align < align) {\n          max_align = align;\n          overall_size = (overall_size + align - 1) / align * align;\n        }\n        new_insn = MIR_new_insn (ctx, MIR_PTR32 ? MIR_ADDS : MIR_ADD, next_insn->ops[0],\n                                 insn->ops[0], MIR_new_int_op (ctx, overall_size));\n        overall_size += size;\n        MIR_insert_insn_before (ctx, func_item, next_insn, new_insn);\n        MIR_remove_insn (ctx, func_item, next_insn);\n        next_insn = DLIST_NEXT (MIR_insn_t, new_insn);\n      }\n      insn->ops[1].u.i = overall_size;\n      next_insn = DLIST_NEXT (MIR_insn_t, insn); /* to process the current and new insns */\n    }\n    if (MIR_call_code_p (code)) inline_p = TRUE;\n    if ((MIR_int_branch_code_p (code) || code == MIR_JMP) && insn->ops[0].mode == MIR_OP_LABEL\n        && skip_labels (next_insn, insn->ops[0].u.label) == insn->ops[0].u.label) {\n      /* BR L|JMP L; <labels>L: => <labels>L: Also Remember signaling NAN*/\n      MIR_remove_insn (ctx, func_item, insn);\n    } else if (((code == MIR_MUL || code == MIR_MULS || code == MIR_MULO || code == MIR_MULOS\n                 || code == MIR_DIV || code == MIR_DIVS)\n                && insn->ops[2].mode == MIR_OP_INT && insn->ops[2].u.i == 1)\n               || ((code == MIR_ADD || code == MIR_ADDS || code == MIR_SUB || code == MIR_SUBS\n                    || code == MIR_OR || code == MIR_ORS || code == MIR_XOR || code == MIR_XORS\n                    || code == MIR_LSH || code == MIR_LSHS || code == MIR_RSH || code == MIR_RSHS\n                    || code == MIR_URSH || code == MIR_URSHS)\n                   && insn->ops[2].mode == MIR_OP_INT && insn->ops[2].u.i == 0)) {\n      if (!MIR_op_eq_p (ctx, insn->ops[0], insn->ops[1])) {\n        next_insn = MIR_new_insn (ctx, MIR_MOV, insn->ops[0], insn->ops[1]);\n        MIR_insert_insn_before (ctx, func_item, insn, next_insn);\n      }\n      MIR_remove_insn (ctx, func_item, insn);\n    } else if (MIR_int_branch_code_p (code) && next_insn != NULL && next_insn->code == MIR_JMP\n               && insn->ops[0].mode == MIR_OP_LABEL && next_insn->ops[0].mode == MIR_OP_LABEL\n               && (skip_labels (next_insn->ops[0].u.label, insn->ops[0].u.label)\n                     == insn->ops[0].u.label\n                   || skip_labels (insn->ops[0].u.label, next_insn->ops[0].u.label)\n                        == next_insn->ops[0].u.label)) {\n      /* BR L1;JMP L2; L2:<labels>L1: or L1:<labels>L2: =>  JMP L2*/\n      MIR_remove_insn (ctx, func_item, insn);\n    } else if ((code == MIR_BT || code == MIR_BTS || code == MIR_BF || code == MIR_BFS)\n               && insn->ops[1].mode == MIR_OP_INT\n               && (insn->ops[1].u.i == 0 || insn->ops[1].u.i == 1)) {\n      /* BT|BF L,zero|nonzero => nothing or JMP L */\n      if ((code == MIR_BT || code == MIR_BTS) == (insn->ops[1].u.i == 1)) {\n        new_insn = MIR_new_insn (ctx, MIR_JMP, insn->ops[0]);\n        MIR_insert_insn_before (ctx, func_item, insn, new_insn);\n        next_insn = new_insn;\n      }\n      MIR_remove_insn (ctx, func_item, insn);\n      // ??? make imm always second,  what is about mem?\n    } else if ((rev_code = MIR_reverse_branch_code (insn->code)) != MIR_INSN_BOUND\n               && next_insn != NULL && next_insn->code == MIR_JMP\n               && (next_next_insn = DLIST_NEXT (MIR_insn_t, next_insn)) != NULL\n               && next_next_insn->code == MIR_LABEL && insn->ops[0].mode == MIR_OP_LABEL\n               && skip_labels (next_next_insn, insn->ops[0].u.label) == insn->ops[0].u.label) {\n      /* BCond L;JMP L2;<lables>L: => BNCond L2;<labels>L: */\n      insn->ops[0] = next_insn->ops[0];\n      insn->code = rev_code;\n      MIR_remove_insn (ctx, func_item, next_insn);\n      next_insn = insn;\n    } else if (MIR_branch_code_p (code) && insn->ops[0].mode == MIR_OP_LABEL\n               && (jmp_insn = skip_labels (insn->ops[0].u.label, NULL)) != NULL\n               && jmp_insn->code == MIR_JMP && ++jmps_num < MAX_JUMP_CHAIN_LEN) {\n      /* B L;...;L<labels>:JMP L2 => B L2; ... Constrain processing to avoid infinite loops */\n      insn->ops[0] = jmp_insn->ops[0];\n      next_insn = insn;\n      continue;\n    } else {\n      if ((MIR_any_branch_code_p (code) && code != MIR_JMPI) || code == MIR_LADDR\n          || code == MIR_PRBEQ || code == MIR_PRBNE) {\n        size_t start_label_nop = 0, bound_label_nop = 1, n;\n\n        if (code == MIR_LADDR) {\n          start_label_nop = 1;\n          bound_label_nop = 2;\n        } else if (code == MIR_SWITCH) {\n          start_label_nop = 1;\n          bound_label_nop = start_label_nop + insn->nops - 1;\n        }\n        for (n = start_label_nop; n < bound_label_nop; n++) {\n          label = last_label (insn->ops[n].u.label);\n          if (label != insn->ops[n].u.label) insn->ops[n].u.label = label;\n          mark_used_label (ctx, label);\n        }\n      }\n      simplify_insn (ctx, func_item, insn, TRUE, mem_float_p);\n    }\n    jmps_num = 0;\n  }\n  make_one_ret (ctx, func_item);\n  for (MIR_lref_data_t lref = func->first_lref; lref != NULL; lref = lref->next) {\n    mark_used_label (ctx, lref->label);\n    if (lref->label2 != NULL) mark_used_label (ctx, lref->label2);\n  }\n  remove_unused_and_enumerate_labels (ctx, func_item);\n#if 0\n  fprintf (stderr, \"+++++ Function after simplification:\\n\");\n  MIR_output_item (ctx, stderr, func_item);\n#endif\n  return inline_p;\n}\n\nstatic void set_inline_reg_map (MIR_context_t ctx, MIR_reg_t old_reg, MIR_reg_t new_reg) {\n  while (VARR_LENGTH (MIR_reg_t, inline_reg_map) <= old_reg)\n    VARR_PUSH (MIR_reg_t, inline_reg_map, 0);\n  VARR_SET (MIR_reg_t, inline_reg_map, old_reg, new_reg);\n}\n\n#ifndef MIR_MAX_INSNS_FOR_INLINE\n#define MIR_MAX_INSNS_FOR_INLINE 200\n#endif\n\n#ifndef MIR_MAX_INSNS_FOR_CALL_INLINE\n#define MIR_MAX_INSNS_FOR_CALL_INLINE 50\n#endif\n\n#ifndef MIR_MAX_FUNC_INLINE_GROWTH\n#define MIR_MAX_FUNC_INLINE_GROWTH 50\n#endif\n\n#ifndef MIR_MAX_CALLER_SIZE_FOR_ANY_GROWTH_INLINE\n#define MIR_MAX_CALLER_SIZE_FOR_ANY_GROWTH_INLINE MIR_MAX_INSNS_FOR_INLINE\n#endif\n\n/* Simple alloca analysis.  Return top alloca insn with const size.\n   If there are other allocas return true through\n   non_top_alloca_p. Should we consider bstart/bend too?  */\nstatic MIR_insn_t func_alloca_features (MIR_context_t ctx, MIR_func_t func, int *top_alloca_used_p,\n                                        int *non_top_alloca_p, int64_t *alloca_size) {\n  int set_top_alloca_p = TRUE;\n  MIR_reg_t alloca_reg;\n  MIR_op_t *op_ref;\n  MIR_insn_t top_alloca = NULL, insn, prev_insn;\n\n  *top_alloca_used_p = FALSE;\n  if (non_top_alloca_p != NULL) *non_top_alloca_p = FALSE;\n  for (insn = DLIST_HEAD (MIR_insn_t, func->insns); insn != NULL;\n       insn = DLIST_NEXT (MIR_insn_t, insn)) {\n    if (insn->code == MIR_LABEL && set_top_alloca_p) set_top_alloca_p = FALSE;\n    if (insn->code != MIR_ALLOCA) {\n      if (top_alloca == NULL || *top_alloca_used_p) continue;\n      alloca_reg = top_alloca->ops[0].u.reg;\n      for (size_t i = 0; i < insn->nops; i++)\n        if ((insn->ops[i].mode == MIR_OP_REG && insn->ops[i].u.reg == alloca_reg)\n            || (insn->ops[i].mode == MIR_OP_MEM\n                && (insn->ops[i].u.mem.base == alloca_reg\n                    || insn->ops[i].u.mem.index == alloca_reg))) {\n          *top_alloca_used_p = TRUE;\n          break;\n        }\n      continue;\n    }\n    op_ref = &insn->ops[1];\n    if (insn->ops[1].mode == MIR_OP_REG && (prev_insn = DLIST_PREV (MIR_insn_t, insn)) != NULL\n        && prev_insn->code == MIR_MOV && MIR_op_eq_p (ctx, prev_insn->ops[0], insn->ops[1]))\n      op_ref = &prev_insn->ops[1];\n    if (op_ref->mode != MIR_OP_INT && op_ref->mode != MIR_OP_UINT) op_ref = NULL;\n    if (!set_top_alloca_p || op_ref == NULL) {\n      if (non_top_alloca_p != NULL) *non_top_alloca_p = TRUE;\n      if (top_alloca == NULL) return NULL;\n    } else {\n      top_alloca = insn;\n      if (insn->ops[0].mode != MIR_OP_REG) *top_alloca_used_p = TRUE;\n      set_top_alloca_p = FALSE;\n      if (alloca_size != NULL) *alloca_size = op_ref->u.i;\n    }\n  }\n  return top_alloca;\n}\n\n/* Generate block move only in simplified MIR.  ??? short move w/o loop. */\nstatic long add_blk_move (MIR_context_t ctx, MIR_item_t func_item, MIR_insn_t before, MIR_op_t dest,\n                          MIR_op_t src, size_t src_size, long label_num) {\n  MIR_func_t func = func_item->u.func;\n  size_t blk_size = (src_size + 7) / 8 * 8;\n  MIR_insn_t insn;\n  MIR_op_t size = MIR_new_reg_op (ctx, new_temp_reg (ctx, MIR_T_I64, func));\n\n  insn = MIR_new_insn (ctx, MIR_MOV, size, MIR_new_int_op (ctx, blk_size));\n  MIR_insert_insn_before (ctx, func_item, before, insn);\n  insn = MIR_new_insn (ctx, MIR_ALLOCA, dest, size);\n  MIR_insert_insn_before (ctx, func_item, before, insn);\n  if (blk_size != 0) {\n    MIR_reg_t addr_reg = new_temp_reg (ctx, MIR_T_I64, func);\n    MIR_op_t addr = MIR_new_reg_op (ctx, addr_reg);\n    MIR_op_t disp = MIR_new_reg_op (ctx, new_temp_reg (ctx, MIR_T_I64, func));\n    MIR_op_t step = MIR_new_reg_op (ctx, new_temp_reg (ctx, MIR_T_I64, func));\n    MIR_op_t temp = MIR_new_reg_op (ctx, new_temp_reg (ctx, MIR_T_I64, func));\n    MIR_label_t loop = create_label (ctx, label_num++), skip = create_label (ctx, label_num++);\n\n    insn = MIR_new_insn (ctx, MIR_MOV, disp, MIR_new_int_op (ctx, 0));\n    MIR_insert_insn_before (ctx, func_item, before, insn);\n    insn = MIR_new_insn (ctx, MIR_BLE, MIR_new_label_op (ctx, skip), size, disp);\n    MIR_insert_insn_before (ctx, func_item, before, insn);\n    MIR_insert_insn_before (ctx, func_item, before, loop);\n    insn = MIR_new_insn (ctx, MIR_ADD, addr, src, disp);\n    MIR_insert_insn_before (ctx, func_item, before, insn);\n    insn = MIR_new_insn (ctx, MIR_MOV, temp, MIR_new_mem_op (ctx, MIR_T_I64, 0, addr_reg, 0, 1));\n    MIR_insert_insn_before (ctx, func_item, before, insn);\n    insn = MIR_new_insn (ctx, MIR_ADD, addr, dest, disp);\n    MIR_insert_insn_before (ctx, func_item, before, insn);\n    insn = MIR_new_insn (ctx, MIR_MOV, MIR_new_mem_op (ctx, MIR_T_I64, 0, addr_reg, 0, 1), temp);\n    MIR_insert_insn_before (ctx, func_item, before, insn);\n    insn = MIR_new_insn (ctx, MIR_MOV, step, MIR_new_int_op (ctx, 8));\n    MIR_insert_insn_before (ctx, func_item, before, insn);\n    insn = MIR_new_insn (ctx, MIR_ADD, disp, disp, step);\n    MIR_insert_insn_before (ctx, func_item, before, insn);\n    insn = MIR_new_insn (ctx, MIR_BLT, MIR_new_label_op (ctx, loop), disp, size);\n    MIR_insert_insn_before (ctx, func_item, before, insn);\n    MIR_insert_insn_before (ctx, func_item, before, skip);\n  }\n  return label_num;\n}\n\nstatic void rename_regs (MIR_context_t ctx, MIR_func_t func, MIR_func_t called_func,\n                         VARR (MIR_var_t) * vars, size_t nvars) {\n  char buff[50];\n  const char *hard_reg_name;\n  MIR_var_t var;\n  MIR_type_t type;\n  MIR_reg_t old_reg, new_reg;\n\n  if (vars == NULL) return;\n  for (size_t i = 0; i < nvars; i++) {\n    VARR_TRUNC (char, temp_string, 0);\n    sprintf (buff, \".c%d_\", func->n_inlines);\n    VARR_PUSH_ARR (char, temp_string, buff, strlen (buff));\n    var = VARR_GET (MIR_var_t, vars, i);\n    type\n      = (var.type == MIR_T_F || var.type == MIR_T_D || var.type == MIR_T_LD ? var.type : MIR_T_I64);\n    old_reg = MIR_reg (ctx, var.name, called_func);\n    VARR_PUSH_ARR (char, temp_string, var.name, strlen (var.name) + 1);\n    if ((hard_reg_name = MIR_reg_hard_reg_name (ctx, old_reg, called_func)) != NULL) {\n      new_reg\n        = MIR_new_global_func_reg (ctx, func, type, VARR_ADDR (char, temp_string), hard_reg_name);\n    } else {\n      new_reg = MIR_new_func_reg (ctx, func, type, VARR_ADDR (char, temp_string));\n    }\n    set_inline_reg_map (ctx, old_reg, new_reg);\n  }\n}\n\nstatic void change_inline_insn_regs (MIR_context_t ctx, MIR_insn_t new_insn) {\n  size_t i, actual_nops;\n  actual_nops = MIR_insn_nops (ctx, new_insn);\n  for (i = 0; i < actual_nops; i++) {\n    switch (new_insn->ops[i].mode) {\n    case MIR_OP_REG:\n      new_insn->ops[i].u.reg = VARR_GET (MIR_reg_t, inline_reg_map, new_insn->ops[i].u.reg);\n      break;\n    case MIR_OP_MEM:\n      if (new_insn->ops[i].u.mem.base != 0)\n        new_insn->ops[i].u.mem.base\n          = VARR_GET (MIR_reg_t, inline_reg_map, new_insn->ops[i].u.mem.base);\n      if (new_insn->ops[i].u.mem.index != 0)\n        new_insn->ops[i].u.mem.index\n          = VARR_GET (MIR_reg_t, inline_reg_map, new_insn->ops[i].u.mem.index);\n      break;\n    default: /* do nothing */ break;\n    }\n  }\n}\n\n/* Only simplified code should be inlined because we need already\n   extensions and one return.  */\nstatic void process_inlines (MIR_context_t ctx, MIR_item_t func_item) {\n  int non_top_alloca_p, func_top_alloca_used_p, called_func_top_alloca_used_p;\n  int64_t alloca_size, alloca_align, max_func_top_alloca_align;\n  int64_t init_func_top_alloca_size, curr_func_top_alloca_size, max_func_top_alloca_size;\n  size_t i, nargs, arg_num;\n  MIR_type_t type, *res_types;\n  MIR_var_t var;\n  MIR_reg_t ret_reg, temp_reg;\n  MIR_insn_t func_top_alloca, called_func_top_alloca, new_called_func_top_alloca = NULL;\n  MIR_insn_t func_insn, head_func_insn, next_func_insn;\n  MIR_insn_t call, insn, prev_insn, new_insn, ret_insn, anchor, stop_insn;\n  MIR_item_t called_func_item;\n  MIR_func_t func, called_func;\n  size_t original_func_insns_num, func_insns_num, called_func_insns_num;\n\n  mir_assert (func_item->item_type == MIR_func_item);\n  vn_empty (ctx);\n  func = func_item->u.func;\n  original_func_insns_num = func_insns_num = DLIST_LENGTH (MIR_insn_t, func->insns);\n  func_top_alloca = func_alloca_features (ctx, func, &func_top_alloca_used_p, NULL, &alloca_size);\n  mir_assert (func_top_alloca != NULL || !func_top_alloca_used_p);\n  init_func_top_alloca_size = curr_func_top_alloca_size = max_func_top_alloca_size = 0;\n  max_func_top_alloca_align = 0;\n  if (func_top_alloca != NULL && func_top_alloca_used_p)\n    init_func_top_alloca_size = max_func_top_alloca_size = curr_func_top_alloca_size\n      = get_alloca_size_align (alloca_size, &max_func_top_alloca_align);\n  VARR_TRUNC (MIR_insn_t, anchors, 0);\n  VARR_TRUNC (size_t, alloca_sizes, 0);\n  VARR_TRUNC (MIR_insn_t, cold_insns, 0);\n  for (head_func_insn = func_insn = DLIST_HEAD (MIR_insn_t, func->insns); func_insn != NULL;\n       func_insn = next_func_insn) {\n    inline_insns_before++;\n    inline_insns_after++;\n    while (VARR_LENGTH (MIR_insn_t, anchors) != 0 && VARR_LAST (MIR_insn_t, anchors) == func_insn) {\n      VARR_POP (MIR_insn_t, anchors);\n      curr_func_top_alloca_size = VARR_POP (size_t, alloca_sizes);\n    }\n    next_func_insn = DLIST_NEXT (MIR_insn_t, func_insn);\n    if (func_insn->code == MIR_LABEL) func_insn->ops[0].u.i = new_label_num++;\n    if (!MIR_call_code_p (func_insn->code)) continue;\n    call = func_insn;\n    if (call->ops[1].mode != MIR_OP_REF) {\n      simplify_op (ctx, func_item, func_insn, 1, FALSE, func_insn->code, FALSE, TRUE);\n      continue;\n    }\n    called_func_item = call->ops[1].u.ref;\n    while (called_func_item != NULL\n           && (called_func_item->item_type == MIR_import_item\n               || called_func_item->item_type == MIR_export_item\n               || called_func_item->item_type == MIR_forward_item))\n      called_func_item = called_func_item->ref_def;\n    if (called_func_item == NULL || called_func_item->item_type != MIR_func_item\n        || func_item == called_func_item) { /* Simplify function operand in the inline insn */\n      simplify_op (ctx, func_item, func_insn, 1, FALSE, func_insn->code, FALSE, TRUE);\n      continue;\n    }\n    called_func = called_func_item->u.func;\n    called_func_insns_num = DLIST_LENGTH (MIR_insn_t, called_func->insns);\n    if (called_func->first_lref != NULL || called_func->vararg_p || called_func->jret_p\n        || called_func_insns_num > (func_insn->code != MIR_CALL ? MIR_MAX_INSNS_FOR_INLINE\n                                                                : MIR_MAX_INSNS_FOR_CALL_INLINE)\n        || (func_insns_num > MIR_MAX_FUNC_INLINE_GROWTH * original_func_insns_num / 100\n            && func_insns_num > MIR_MAX_CALLER_SIZE_FOR_ANY_GROWTH_INLINE)) {\n      simplify_op (ctx, func_item, func_insn, 1, FALSE, func_insn->code, FALSE, TRUE);\n      continue;\n    }\n    func_insns_num += called_func_insns_num;\n    inlined_calls++;\n    res_types = call->ops[0].u.ref->u.proto->res_types;\n    prev_insn = DLIST_PREV (MIR_insn_t, call);\n    if ((anchor = DLIST_NEXT (MIR_insn_t, call)) == NULL) {\n      anchor = MIR_new_label (ctx);\n      MIR_insert_insn_after (ctx, func_item, call, anchor);\n    }\n    func->n_inlines++;\n    rename_regs (ctx, func, called_func, called_func->vars,\n                 VARR_LENGTH (MIR_var_t, called_func->vars));\n    rename_regs (ctx, func, called_func, called_func->global_vars,\n                 called_func->global_vars == NULL\n                   ? 0\n                   : VARR_LENGTH (MIR_var_t, called_func->global_vars));\n    nargs = called_func->nargs;\n    for (i = 2 + called_func->nres, arg_num = 0; arg_num < nargs && i < call->nops;\n         i++, arg_num++) { /* Parameter passing */\n      MIR_op_t op = call->ops[i];\n      var = VARR_GET (MIR_var_t, called_func->vars, arg_num);\n      type = (var.type == MIR_T_F || var.type == MIR_T_D || var.type == MIR_T_LD ? var.type\n                                                                                 : MIR_T_I64);\n      const char *old_var_name = var.name;\n      MIR_reg_t old_reg = MIR_reg (ctx, old_var_name, called_func);\n      MIR_reg_t new_reg = VARR_GET (MIR_reg_t, inline_reg_map, old_reg);\n\n      mir_assert (!MIR_all_blk_type_p (type) || (op.mode == MIR_OP_MEM && type == MIR_T_I64));\n      if (MIR_blk_type_p (var.type)) { /* alloca and block move: */\n        new_label_num\n          = add_blk_move (ctx, func_item, anchor, MIR_new_reg_op (ctx, new_reg),\n                          MIR_new_reg_op (ctx, op.u.mem.base), var.size, (long) new_label_num);\n      } else {\n        if (var.type == MIR_T_RBLK) op = MIR_new_reg_op (ctx, op.u.mem.base);\n        new_insn = MIR_new_insn (ctx, get_type_move_code (type), MIR_new_reg_op (ctx, new_reg), op);\n        MIR_insert_insn_before (ctx, func_item, anchor, new_insn);\n      }\n    }\n    /* ??? No frame only alloca */\n    VARR_PUSH (MIR_insn_t, anchors, anchor);\n    VARR_PUSH (size_t, alloca_sizes, curr_func_top_alloca_size);\n    /* Add new insns: */\n    ret_reg = 0;\n    called_func_top_alloca = func_alloca_features (ctx, called_func, &called_func_top_alloca_used_p,\n                                                   &non_top_alloca_p, &alloca_size);\n    if (called_func_top_alloca != NULL && called_func_top_alloca_used_p) {\n      alloca_size = get_alloca_size_align (alloca_size, &alloca_align);\n      if (max_func_top_alloca_align < alloca_align) {\n        max_func_top_alloca_align = alloca_align;\n        curr_func_top_alloca_size\n          = (curr_func_top_alloca_size + alloca_align - 1) / alloca_align * alloca_align;\n      }\n      curr_func_top_alloca_size += alloca_size;\n      if (max_func_top_alloca_size < curr_func_top_alloca_size)\n        max_func_top_alloca_size = curr_func_top_alloca_size;\n    }\n    VARR_TRUNC (MIR_insn_t, temp_insns, 0);\n    VARR_TRUNC (MIR_insn_t, labels, 0);\n    VARR_TRUNC (uint8_t, temp_data, 0);\n    stop_insn = NULL;\n    if (!non_top_alloca_p) { /* store cold code when we have no BSTART/BEND */\n      for (insn = DLIST_TAIL (MIR_insn_t, called_func->insns); insn != NULL;\n           insn = DLIST_PREV (MIR_insn_t, insn)) {\n        if (insn->code == MIR_RET || insn->code == MIR_JRET) break;\n        inline_insns_after++;\n        new_insn = MIR_copy_insn (ctx, insn);\n        change_inline_insn_regs (ctx, new_insn);\n        store_labels_for_duplication (ctx, labels, temp_insns, insn, new_insn);\n        VARR_PUSH (MIR_insn_t, cold_insns, new_insn);\n      }\n      mir_assert (insn != NULL);\n      stop_insn = DLIST_NEXT (MIR_insn_t, insn);\n    }\n    for (insn = DLIST_HEAD (MIR_insn_t, called_func->insns); insn != stop_insn;\n         insn = DLIST_NEXT (MIR_insn_t, insn)) {\n      mir_assert (insn->code != MIR_JRET);\n      inline_insns_after++;\n      new_insn = MIR_copy_insn (ctx, insn);\n      /* va insns are possible here as va_list can be passed as arg */\n      if (insn == called_func_top_alloca) new_called_func_top_alloca = new_insn;\n      change_inline_insn_regs (ctx, new_insn);\n      if (new_insn->code != MIR_RET) {\n        MIR_insert_insn_before (ctx, func_item, anchor, new_insn);\n        store_labels_for_duplication (ctx, labels, temp_insns, insn, new_insn);\n      } else {\n        size_t actual_nops = MIR_insn_nops (ctx, insn);\n        /* [J]RET should be the last insn extracting cold code */\n        mir_assert (DLIST_NEXT (MIR_insn_t, insn) == stop_insn && call->ops[0].mode == MIR_OP_REF\n                    && call->ops[0].u.ref->item_type == MIR_proto_item);\n        mir_assert (called_func->nres == actual_nops);\n        ret_insn = new_insn;\n        for (i = 0; i < actual_nops; i++) {\n          mir_assert (ret_insn->ops[i].mode == MIR_OP_REG);\n          ret_reg = ret_insn->ops[i].u.reg;\n          new_insn = MIR_new_insn (ctx, get_type_move_code (res_types[i]), call->ops[i + 2],\n                                   MIR_new_reg_op (ctx, ret_reg));\n          MIR_insert_insn_before (ctx, func_item, anchor, new_insn);\n        }\n        MIR_free (ctx->alloc, ret_insn);\n      }\n    }\n    redirect_duplicated_labels (ctx, labels, temp_insns);\n    if (non_top_alloca_p) {\n      temp_reg = new_temp_reg (ctx, MIR_T_I64, func);\n      new_insn = MIR_new_insn (ctx, MIR_BSTART, MIR_new_reg_op (ctx, temp_reg));\n      MIR_insert_insn_after (ctx, func_item, call, new_insn);\n      new_insn = MIR_new_insn (ctx, MIR_BEND, MIR_new_reg_op (ctx, temp_reg));\n      MIR_insert_insn_before (ctx, func_item, anchor, new_insn);\n    }\n    if (called_func_top_alloca != NULL) {\n      if (called_func_top_alloca_used_p) {\n        func_top_alloca_used_p = TRUE;\n        if (func_top_alloca == NULL) {\n          temp_reg = new_temp_reg (ctx, MIR_T_I64, func);\n          func_top_alloca = MIR_new_insn (ctx, MIR_ALLOCA, new_called_func_top_alloca->ops[0],\n                                          MIR_new_reg_op (ctx, temp_reg));\n          if (head_func_insn->code != MIR_LABEL)\n            MIR_insert_insn_before (ctx, func_item, head_func_insn, func_top_alloca);\n          else\n            MIR_insert_insn_after (ctx, func_item, head_func_insn, func_top_alloca);\n          init_func_top_alloca_size = 0;\n          new_insn\n            = MIR_new_insn (ctx, MIR_MOV, MIR_new_reg_op (ctx, temp_reg), MIR_new_int_op (ctx, 0));\n          MIR_insert_insn_before (ctx, func_item, func_top_alloca, new_insn);\n        }\n        if (curr_func_top_alloca_size - alloca_size == 0) {\n          new_insn = MIR_new_insn (ctx, MIR_MOV, new_called_func_top_alloca->ops[0],\n                                   func_top_alloca->ops[0]);\n        } else {\n          temp_reg = new_temp_reg (ctx, MIR_T_I64, func);\n          new_insn\n            = MIR_new_insn (ctx, MIR_PTR32 ? MIR_ADDS : MIR_ADD, new_called_func_top_alloca->ops[0],\n                            func_top_alloca->ops[0], MIR_new_reg_op (ctx, temp_reg));\n          MIR_insert_insn_after (ctx, func_item, call, new_insn);\n          new_insn = MIR_new_insn (ctx, MIR_MOV, MIR_new_reg_op (ctx, temp_reg),\n                                   MIR_new_int_op (ctx, curr_func_top_alloca_size - alloca_size));\n        }\n        MIR_insert_insn_after (ctx, func_item, call, new_insn);\n      }\n      if (head_func_insn == new_called_func_top_alloca)\n        head_func_insn = DLIST_NEXT (MIR_insn_t, head_func_insn);\n      MIR_remove_insn (ctx, func_item, new_called_func_top_alloca);\n    }\n    if (head_func_insn == call) head_func_insn = DLIST_NEXT (MIR_insn_t, head_func_insn);\n    MIR_remove_insn (ctx, func_item, call);\n    if (head_func_insn == call) head_func_insn = DLIST_HEAD (MIR_insn_t, func->insns);\n    next_func_insn = (prev_insn == NULL ? DLIST_HEAD (MIR_insn_t, func->insns)\n                                        : DLIST_NEXT (MIR_insn_t, prev_insn));\n  }\n  mir_assert (VARR_LENGTH (MIR_insn_t, anchors) == 0 && VARR_LENGTH (size_t, alloca_sizes) == 0);\n  if (func_top_alloca != NULL) {\n    if (!func_top_alloca_used_p) {\n      MIR_remove_insn (ctx, func_item, func_top_alloca);\n    } else if (max_func_top_alloca_size != init_func_top_alloca_size) {\n      temp_reg = new_temp_reg (ctx, MIR_T_I64, func);\n      new_insn = MIR_new_insn (ctx, MIR_MOV, MIR_new_reg_op (ctx, temp_reg),\n                               MIR_new_int_op (ctx, max_func_top_alloca_size));\n      func_top_alloca->ops[1] = MIR_new_reg_op (ctx, temp_reg);\n      MIR_insert_insn_before (ctx, func_item, func_top_alloca, new_insn);\n    }\n  }\n  while (VARR_LENGTH (MIR_insn_t, cold_insns) != 0) {\n    insn = VARR_POP (MIR_insn_t, cold_insns);\n    if (insn->code == MIR_LABEL) insn->ops[0].u.i = new_label_num++;\n    MIR_append_insn (ctx, func_item, insn);\n  }\n  if (curr_label_num < new_label_num) curr_label_num = new_label_num;\n}\n\n/* New Page */\n\nconst char *_MIR_uniq_string (MIR_context_t ctx, const char *str) { return get_ctx_str (ctx, str); }\n\n/* The next two function can be called any time relative to\n   load/linkage.  You can also call them many times for the same name\n   but you should always use the same prototype or/and addr for the\n   same proto/func name.  */\nMIR_item_t _MIR_builtin_proto (MIR_context_t ctx, MIR_module_t module, const char *name,\n                               size_t nres, MIR_type_t *res_types, size_t nargs, ...) {\n  size_t i;\n  va_list argp;\n  MIR_var_t *args = alloca (nargs * sizeof (MIR_var_t));\n  MIR_item_t proto_item;\n  MIR_module_t saved_module;\n\n  va_start (argp, nargs);\n  saved_module = curr_module;\n  for (i = 0; i < nargs; i++) {\n    args[i].type = va_arg (argp, MIR_type_t);\n    args[i].name = va_arg (argp, const char *);\n  }\n  va_end (argp);\n  name = _MIR_uniq_string (ctx, name);\n  proto_item = item_tab_find (ctx, name, module);\n  if (proto_item != NULL) {\n    if (proto_item->item_type == MIR_proto_item && proto_item->u.proto->nres == nres\n        && VARR_LENGTH (MIR_var_t, proto_item->u.proto->args) == nargs) {\n      for (i = 0; i < nres; i++)\n        if (res_types[i] != proto_item->u.proto->res_types[i]) break;\n      if (i >= nres) {\n        for (i = 0; i < nargs; i++)\n          if (args[i].type != VARR_GET (MIR_var_t, proto_item->u.proto->args, i).type) break;\n        if (i >= nargs) {\n          return proto_item;\n        }\n      }\n    }\n    MIR_get_error_func (ctx) (MIR_repeated_decl_error,\n                              \"_MIR_builtin_proto: proto item %s was already defined differently\",\n                              name);\n  }\n  saved_module = curr_module;\n  curr_module = module;\n  proto_item = MIR_new_proto_arr (ctx, name, nres, res_types, nargs, args);\n  DLIST_REMOVE (MIR_item_t, curr_module->items, proto_item);\n  DLIST_PREPEND (MIR_item_t, curr_module->items, proto_item); /* make it first in the list */\n  curr_module = saved_module;\n  return proto_item;\n}\n\nMIR_item_t _MIR_builtin_func (MIR_context_t ctx, MIR_module_t module, const char *name,\n                              void *addr) {\n  MIR_item_t item, ref_item;\n  MIR_module_t saved_module = curr_module;\n\n  name = _MIR_uniq_string (ctx, name);\n  if ((ref_item = item_tab_find (ctx, name, &environment_module)) != NULL) {\n    if (ref_item->item_type != MIR_import_item || ref_item->addr != addr)\n      MIR_get_error_func (ctx) (MIR_repeated_decl_error,\n                                \"_MIR_builtin_func: func %s has already another address\", name);\n  } else {\n    curr_module = &environment_module;\n    /* Use import for builtin func: */\n    item = new_export_import_forward (ctx, name, MIR_import_item, \"import\", TRUE);\n    HTAB_DO (MIR_item_t, module_item_tab, item, HTAB_INSERT, ref_item);\n    mir_assert (item == ref_item);\n    DLIST_APPEND (MIR_item_t, environment_module.items, item);\n    ref_item->addr = addr;\n    curr_module = saved_module;\n  }\n  if ((item = item_tab_find (ctx, name, module)) != NULL) {\n    if (item->item_type != MIR_import_item || item->addr != addr || item->ref_def != ref_item)\n      MIR_get_error_func (\n        ctx) (MIR_repeated_decl_error,\n              \"_MIR_builtin_func: func name %s was already defined differently in the \"\n              \"module\",\n              name);\n  } else {\n    curr_module = module;\n    item = new_export_import_forward (ctx, name, MIR_import_item, \"import\", FALSE);\n    DLIST_REMOVE (MIR_item_t, curr_module->items, item);\n    DLIST_PREPEND (MIR_item_t, curr_module->items, item); /* make it first in the list */\n    item->addr = ref_item->addr;\n    item->ref_def = ref_item;\n    curr_module = saved_module;\n  }\n  return item;\n}\n\n/* New Page */\n/* This page is for dealing with generated machine code */\n\n#ifndef _WIN32\n#include <sys/mman.h>\n#include <unistd.h>\n\nstatic size_t mem_page_size () {\n  return sysconf (_SC_PAGE_SIZE);\n}\n#else\n#define WIN32_LEAN_AND_MEAN\n#include <windows.h>\n\nstatic size_t mem_page_size () {\n  SYSTEM_INFO sysInfo;\n  GetSystemInfo (&sysInfo);\n  return sysInfo.dwPageSize;\n}\n#endif\n\nstruct code_holder {\n  uint8_t *start, *free, *bound;\n};\n\ntypedef struct code_holder code_holder_t;\n\nDEF_VARR (code_holder_t);\n\nstruct machine_code_ctx {\n  VARR (code_holder_t) * code_holders;\n  size_t page_size;\n};\n\n#define code_holders ctx->machine_code_ctx->code_holders\n#define page_size ctx->machine_code_ctx->page_size\n\nstatic code_holder_t *get_last_code_holder (MIR_context_t ctx, size_t size) {\n  uint8_t *mem;\n  size_t len, npages;\n  code_holder_t ch, *ch_ptr;\n\n  if ((len = VARR_LENGTH (code_holder_t, code_holders)) > 0) {\n    ch_ptr = VARR_ADDR (code_holder_t, code_holders) + len - 1;\n    ch_ptr->free = (uint8_t *) ((uint64_t) (ch_ptr->free + 15) / 16 * 16); /* align */\n    if (ch_ptr->free + size <= ch_ptr->bound) return ch_ptr;\n  }\n  npages = (size + page_size) / page_size;\n  len = page_size * npages;\n  mem = (uint8_t *) MIR_mem_map (ctx->code_alloc, len);\n  if (mem == MAP_FAILED) return NULL;\n  ch.start = mem;\n  ch.free = mem;\n  ch.bound = mem + len;\n  VARR_PUSH (code_holder_t, code_holders, ch);\n  len = VARR_LENGTH (code_holder_t, code_holders);\n  return VARR_ADDR (code_holder_t, code_holders) + len - 1;\n}\n\nvoid _MIR_flush_code_cache (void *start, void *bound) {\n#if defined(__GNUC__) && !defined(__MIRC__)\n  __builtin___clear_cache (start, bound);\n#endif\n}\n\n#if !defined(MIR_BOOTSTRAP) || !defined(__APPLE__) || !defined(__aarch64__)\nvoid _MIR_set_code (MIR_code_alloc_t code_alloc, size_t prot_start, size_t prot_len,\n                    uint8_t *base, size_t nloc, const MIR_code_reloc_t *relocs,\n                    size_t reloc_size) {\n  MIR_mem_protect (code_alloc, (uint8_t *) prot_start, prot_len, PROT_WRITE_EXEC);\n  if (reloc_size == 0) {\n    for (size_t i = 0; i < nloc; i++)\n      memcpy (base + relocs[i].offset, &relocs[i].value, sizeof (void *));\n  } else {\n    for (size_t i = 0; i < nloc; i++) memcpy (base + relocs[i].offset, relocs[i].value, reloc_size);\n  }\n  MIR_mem_protect (code_alloc, (uint8_t *) prot_start, prot_len, PROT_READ_EXEC);\n}\n#endif\n\nstatic uint8_t *add_code (MIR_context_t ctx MIR_UNUSED, code_holder_t *ch_ptr, const uint8_t *code,\n                          size_t code_len) {\n  uint8_t *mem = ch_ptr->free;\n\n  ch_ptr->free += code_len;\n  mir_assert (ch_ptr->free <= ch_ptr->bound);\n  MIR_code_reloc_t reloc;\n  reloc.offset = 0;\n  reloc.value = code;\n  _MIR_set_code (ctx->code_alloc, (size_t) ch_ptr->start, ch_ptr->bound - ch_ptr->start, mem, 1, &reloc, code_len);\n  _MIR_flush_code_cache (mem, ch_ptr->free);\n  return mem;\n}\n\nuint8_t *_MIR_publish_code (MIR_context_t ctx, const uint8_t *code,\n                            size_t code_len) { /* thread safe */\n  code_holder_t *ch_ptr;\n  uint8_t *res = NULL;\n\n  if ((ch_ptr = get_last_code_holder (ctx, code_len)) != NULL)\n    res = add_code (ctx, ch_ptr, code, code_len);\n  return res;\n}\n\nuint8_t *_MIR_publish_code_by_addr (MIR_context_t ctx, void *addr, const uint8_t *code,\n                                    size_t code_len) {\n  code_holder_t *ch_ptr = get_last_code_holder (ctx, 0);\n  uint8_t *res = NULL;\n\n  if (ch_ptr != NULL && ch_ptr->free == addr && ch_ptr->free + code_len <= ch_ptr->bound)\n    res = add_code (ctx, ch_ptr, code, code_len);\n  return res;\n}\n\nvoid _MIR_change_code (MIR_context_t ctx, uint8_t *addr, const uint8_t *code,\n                       size_t code_len) { /* thread safe */\n  MIR_code_reloc_t reloc;\n  size_t len, start;\n\n  start = (size_t) addr / page_size * page_size;\n  len = (size_t) addr + code_len - start;\n  reloc.offset = 0;\n  reloc.value = code;\n  _MIR_set_code (ctx->code_alloc, start, len, addr, 1, &reloc, code_len);\n  _MIR_flush_code_cache (addr, addr + code_len);\n}\n\nvoid _MIR_update_code_arr (MIR_context_t ctx, uint8_t *base, size_t nloc,\n                           const MIR_code_reloc_t *relocs) { /* thread safe */\n  size_t i, len, start, max_offset = 0;\n\n  mir_assert (relocs != NULL);\n  for (i = 0; i < nloc; i++)\n    if (max_offset < relocs[i].offset) max_offset = relocs[i].offset;\n  start = (size_t) base / page_size * page_size;\n  len = (size_t) base + max_offset + sizeof (void *) - start;\n  _MIR_set_code (ctx->code_alloc, start, len, base, nloc, relocs, 0);\n  _MIR_flush_code_cache (base, base + max_offset + sizeof (void *));\n}\n\nvoid _MIR_update_code (MIR_context_t ctx, uint8_t *base, size_t nloc, ...) { /* thread safe */\n  va_list args;\n  MIR_code_reloc_t relocs[20];\n  if (nloc >= 20)\n    MIR_get_error_func (ctx) (MIR_wrong_param_value_error, \"_MIR_update_code: too many locations\");\n  va_start (args, nloc);\n  for (size_t i = 0; i < nloc; i++) {\n    relocs[i].offset = va_arg (args, size_t);\n    relocs[i].value = va_arg (args, void *);\n  }\n  va_end (args);\n  _MIR_update_code_arr (ctx, base, nloc, relocs);\n}\n\nuint8_t *_MIR_get_new_code_addr (MIR_context_t ctx, size_t size) {\n  code_holder_t *ch_ptr = get_last_code_holder (ctx, size);\n\n  return ch_ptr == NULL ? NULL : ch_ptr->free;\n}\n\nstatic void code_init (MIR_context_t ctx) {\n  if ((ctx->machine_code_ctx = MIR_malloc (ctx->alloc, sizeof (struct machine_code_ctx))) == NULL)\n    MIR_get_error_func (ctx) (MIR_alloc_error, \"Not enough memory for ctx\");\n  page_size = mem_page_size ();\n  VARR_CREATE (code_holder_t, code_holders, ctx->alloc, 128);\n}\n\nstatic void code_finish (MIR_context_t ctx) {\n  while (VARR_LENGTH (code_holder_t, code_holders) != 0) {\n    code_holder_t ch = VARR_POP (code_holder_t, code_holders);\n    MIR_mem_unmap (ctx->code_alloc, ch.start, ch.bound - ch.start);\n  }\n  VARR_DESTROY (code_holder_t, code_holders);\n  MIR_free (ctx->alloc, ctx->machine_code_ctx);\n  ctx->machine_code_ctx = NULL;\n}\n\n/* New Page */\n\n#if !MIR_NO_IO || !MIR_NO_SCAN\nstatic void process_reserved_name (const char *s, const char *prefix, uint32_t *max_num) {\n  char *end;\n  uint32_t num;\n  size_t len = strlen (prefix);\n\n  if (strncmp (s, prefix, len) != 0) return;\n  num = strtoul (s + len, &end, 10);\n  if (*end != '\\0') return;\n  if (*max_num < num) *max_num = num;\n}\n#endif\n\n#if !MIR_NO_IO\n\n/* Input/output of binary MIR.  Major goal of binary MIR is fast\n   reading, not compression ratio.  Text MIR major CPU time consumer\n   is a scanner.  Mostly in reading binary MIR we skip the scanner\n   part by using tokens.  Each token starts with a tag which describes\n   subsequent optional bytes.  */\n\n#define TAG_EL(t) TAG_##t\n#define REP_SEP ,\ntypedef enum {\n  TAG_EL (U0),\n  REP8 (TAG_EL, U1, U2, U3, U4, U5, U6, U7, U8),\n  REP8 (TAG_EL, I1, I2, I3, I4, I5, I6, I7, I8),\n  REP3 (TAG_EL, F, D, LD),                   /* 4, 8, 16 bytes for floating point numbers */\n  REP4 (TAG_EL, REG1, REG2, REG3, REG4),     /* Reg string number in 1, 2, 3, 4 bytes */\n  REP4 (TAG_EL, NAME1, NAME2, NAME3, NAME4), /* Name string number in 1, 2, 3, 4 bytes */\n  REP4 (TAG_EL, STR1, STR2, STR3, STR4),     /* String number in 1, 2, 3, 4 bytes */\n  REP4 (TAG_EL, LAB1, LAB2, LAB3, LAB4),     /* Label number in 1, 2, 3, 4 bytes */\n  /* Tags for memory operands.  The memory address parts are the subsequent tokens */\n  REP4 (TAG_EL, MEM_DISP, MEM_BASE, MEM_INDEX, MEM_DISP_BASE),\n  REP3 (TAG_EL, MEM_DISP_INDEX, MEM_BASE_INDEX, MEM_DISP_BASE_INDEX),\n  /* MIR types. The same order as MIR types: */\n  REP8 (TAG_EL, TI8, TU8, TI16, TU16, TI32, TU32, TI64, TU64),\n  REP5 (TAG_EL, TF, TD, TP, TV, TBLOCK),\n  TAG_EL (TRBLOCK) = TAG_EL (TBLOCK) + MIR_BLK_NUM,\n  TAG_EL (EOI),\n  TAG_EL (EOFILE), /* end of insn with variable number operands (e.g. a call) or end of file */\n  REP4 (TAG_EL, ALIAS_MEM_DISP, ALIAS_MEM_BASE, ALIAS_MEM_INDEX, ALIAS_MEM_DISP_BASE),\n  REP3 (TAG_EL, ALIAS_MEM_DISP_INDEX, ALIAS_MEM_BASE_INDEX, ALIAS_MEM_DISP_BASE_INDEX),\n  TAG_EL (LAST) = TAG_EL (ALIAS_MEM_DISP_BASE_INDEX),\n  /* unsigned integer 0..127 is kept in one byte.  The most significant bit of the byte is 1: */\n  U0_MASK = 0x7f,\n  U0_FLAG = 0x80,\n} bin_tag_t;\n#undef REP_SEP\n\n/* MIR binary format:\n\n   VERSION\n   NSTR\n   (string)*\n   ( ((label)* (insn code) (operand)* | STRN=(func|global|local|import|export|forward|<data>) ...)\n     EOI?\n   )* EOF\n\n   where\n   o VERSION and NSTR are unsigned tokens\n   o insn code is unsigned token\n   o string is string number tokens\n   o operand is unsigned, signed, float, double, string, label, memory tokens\n   o EOI, EOF - tokens for end of insn (optional for most insns) and end of file\n*/\n\nstatic const int CURR_BIN_VERSION = 1;\n\nDEF_VARR (MIR_str_t);\nDEF_VARR (uint64_t);\nDEF_VARR (MIR_label_t);\n\nstruct io_ctx {\n  FILE *io_file;\n  int (*io_writer) (MIR_context_t, uint8_t);\n  int (*io_reader) (MIR_context_t);\n  struct reduce_data *io_reduce_data;\n  VARR (MIR_var_t) * proto_vars;\n  VARR (MIR_type_t) * proto_types;\n  VARR (MIR_op_t) * read_insn_ops;\n  VARR (string_t) * output_strings;\n  HTAB (string_t) * output_string_tab;\n  VARR (MIR_str_t) * bin_strings;\n  VARR (uint64_t) * insn_label_string_nums;\n  VARR (MIR_label_t) * func_labels;\n  size_t output_insns_len, output_labs_len;\n  size_t output_regs_len, output_mem_len, output_int_len, output_float_len;\n};\n\n#define io_file ctx->io_ctx->io_file\n#define io_writer ctx->io_ctx->io_writer\n#define io_reader ctx->io_ctx->io_reader\n#define io_reduce_data ctx->io_ctx->io_reduce_data\n#define proto_vars ctx->io_ctx->proto_vars\n#define proto_types ctx->io_ctx->proto_types\n#define read_insn_ops ctx->io_ctx->read_insn_ops\n#define output_strings ctx->io_ctx->output_strings\n#define output_string_tab ctx->io_ctx->output_string_tab\n#define bin_strings ctx->io_ctx->bin_strings\n#define insn_label_string_nums ctx->io_ctx->insn_label_string_nums\n#define func_labels ctx->io_ctx->func_labels\n#define output_insns_len ctx->io_ctx->output_insns_len\n#define output_labs_len ctx->io_ctx->output_labs_len\n#define output_regs_len ctx->io_ctx->output_regs_len\n#define output_mem_len ctx->io_ctx->output_mem_len\n#define output_int_len ctx->io_ctx->output_int_len\n#define output_float_len ctx->io_ctx->output_float_len\n\ntypedef reduce_writer_t writer_func_t;\n\nstatic size_t put_byte (MIR_context_t ctx, writer_func_t writer, int ch) {\n  if (writer == NULL) return 0;\n#ifdef MIR_NO_BIN_COMPRESSION\n  io_writer (ctx, ch);\n#else\n  reduce_encode_put (io_reduce_data, ch);\n#endif\n  return 1;\n}\n\nstatic size_t uint_length (uint64_t u) {\n  size_t n;\n\n  if (u <= 127) return 0;\n  for (n = 0; u != 0; n++) u >>= CHAR_BIT;\n  return n;\n}\n\nstatic size_t put_uint (MIR_context_t ctx, writer_func_t writer, uint64_t u, int nb) {\n  if (writer == NULL) return 0;\n  for (int n = 0; n < nb; n++) {\n    put_byte (ctx, writer, u & 0xff);\n    u >>= CHAR_BIT;\n  }\n  return nb;\n}\n\nstatic size_t int_length (int64_t i) {\n  uint64_t u = i;\n  size_t n = 0;\n\n  for (n = 0; u != 0; n++) u >>= CHAR_BIT;\n  return n == 0 ? 1 : n;\n}\n\nstatic size_t put_int (MIR_context_t ctx, writer_func_t writer, int64_t i, int nb) {\n  return put_uint (ctx, writer, (uint64_t) i, nb);\n}\n\nstatic size_t put_float (MIR_context_t ctx, writer_func_t writer, float fl) {\n  union {\n    uint32_t u;\n    float f;\n  } u;\n\n  if (writer == NULL) return 0;\n  u.f = fl;\n  return put_uint (ctx, writer, u.u, sizeof (uint32_t));\n}\n\nstatic size_t put_double (MIR_context_t ctx, writer_func_t writer, double d) {\n  union {\n    uint64_t u;\n    double d;\n  } u;\n\n  if (writer == NULL) return 0;\n  u.d = d;\n  return put_uint (ctx, writer, u.u, sizeof (uint64_t));\n}\n\nstatic size_t put_ldouble (MIR_context_t ctx, writer_func_t writer, long double ld) {\n  union {\n    uint64_t u[2];\n    long double ld;\n  } u;\n  size_t len;\n\n  if (writer == NULL) return 0;\n  u.ld = ld;\n  len = put_uint (ctx, writer, u.u[0], sizeof (uint64_t));\n  return put_uint (ctx, writer, u.u[1], sizeof (uint64_t)) + len;\n}\n\n/* Write binary MIR */\n\nstatic size_t write_int (MIR_context_t ctx, writer_func_t writer, int64_t i) {\n  size_t nb, len;\n\n  if (writer == NULL) return 0;\n  nb = int_length (i);\n  assert (nb > 0);\n  put_byte (ctx, writer, TAG_I1 + (int) nb - 1);\n  len = put_int (ctx, writer, i, (int) nb) + 1;\n  output_int_len += len;\n  return len;\n}\n\nstatic size_t write_uint (MIR_context_t ctx, writer_func_t writer, uint64_t u) {\n  size_t nb, len;\n\n  if (writer == NULL) return 0;\n  if ((nb = uint_length (u)) == 0) {\n    put_byte (ctx, writer, (int) (0x80 | u));\n    return 1;\n  }\n  put_byte (ctx, writer, TAG_U1 + (int) nb - 1);\n  len = put_uint (ctx, writer, u, (int) nb) + 1;\n  output_int_len += len;\n  return len;\n}\n\nstatic size_t write_float (MIR_context_t ctx, writer_func_t writer, float fl) {\n  size_t len;\n\n  if (writer == NULL) return 0;\n  put_byte (ctx, writer, TAG_F);\n  len = put_float (ctx, writer, fl) + 1;\n  output_float_len += len;\n  return len;\n}\n\nstatic size_t write_double (MIR_context_t ctx, writer_func_t writer, double d) {\n  size_t len;\n\n  if (writer == NULL) return 0;\n  put_byte (ctx, writer, TAG_D);\n  len = put_double (ctx, writer, d) + 1;\n  output_float_len += len;\n  return len;\n}\n\nstatic size_t write_ldouble (MIR_context_t ctx, writer_func_t writer, long double ld) {\n  size_t len;\n\n  if (writer == NULL) return 0;\n  put_byte (ctx, writer, TAG_LD);\n  len = put_ldouble (ctx, writer, ld) + 1;\n  output_int_len += len;\n  return len;\n}\n\nstatic size_t write_str_tag (MIR_context_t ctx, writer_func_t writer, MIR_str_t str,\n                             bin_tag_t start_tag) {\n  size_t nb;\n  int ok_p;\n  string_t string;\n\n  if (writer == NULL) {\n    string_store (ctx, &output_strings, &output_string_tab, str);\n    return 0;\n  }\n  ok_p = string_find (&output_strings, &output_string_tab, str, &string);\n  mir_assert (ok_p && string.num >= 1);\n  nb = uint_length (string.num - 1);\n  mir_assert (nb <= 4);\n  if (nb == 0) nb = 1;\n  put_byte (ctx, writer, start_tag + (int) nb - 1);\n  return put_uint (ctx, writer, string.num - 1, (int) nb) + 1;\n}\n\nstatic size_t write_str (MIR_context_t ctx, writer_func_t writer, MIR_str_t str) {\n  return write_str_tag (ctx, writer, str, TAG_STR1);\n}\nstatic size_t write_name (MIR_context_t ctx, writer_func_t writer, const char *name) {\n  return write_str_tag (ctx, writer, (MIR_str_t){strlen (name) + 1, name}, TAG_NAME1);\n}\n\nstatic size_t write_reg (MIR_context_t ctx, writer_func_t writer, const char *reg_name) {\n  size_t len = write_str_tag (ctx, writer, (MIR_str_t){strlen (reg_name) + 1, reg_name}, TAG_REG1);\n\n  output_regs_len += len;\n  return len;\n}\n\nstatic size_t write_type (MIR_context_t ctx, writer_func_t writer, MIR_type_t t) {\n  return put_byte (ctx, writer, TAG_TI8 + (t - MIR_T_I8));\n}\n\nstatic size_t write_lab (MIR_context_t ctx, writer_func_t writer, MIR_label_t lab) {\n  size_t nb, len;\n  uint64_t lab_num;\n\n  if (writer == NULL) return 0;\n  lab_num = lab->ops[0].u.u;\n  nb = uint_length (lab_num);\n  mir_assert (nb <= 4);\n  if (nb == 0) nb = 1;\n  put_byte (ctx, writer, TAG_LAB1 + (int) nb - 1);\n  len = put_uint (ctx, writer, lab_num, (int) nb) + 1;\n  output_labs_len += len;\n  return len;\n}\n\nstatic size_t write_op (MIR_context_t ctx, writer_func_t writer, MIR_func_t func, MIR_op_t op) {\n  switch (op.mode) {\n  case MIR_OP_REG: return write_reg (ctx, writer, MIR_reg_name (ctx, op.u.reg, func));\n  case MIR_OP_INT: return write_int (ctx, writer, op.u.i);\n  case MIR_OP_UINT: return write_uint (ctx, writer, op.u.u);\n  case MIR_OP_FLOAT: return write_float (ctx, writer, op.u.f);\n  case MIR_OP_DOUBLE: return write_double (ctx, writer, op.u.d);\n  case MIR_OP_LDOUBLE: return write_ldouble (ctx, writer, op.u.ld);\n  case MIR_OP_MEM: {\n    bin_tag_t tag;\n    size_t len;\n    int alias_p = op.u.mem.alias != 0 || op.u.mem.nonalias != 0;\n\n    if (op.u.mem.disp != 0) {\n      if (op.u.mem.base != 0)\n        tag = op.u.mem.index != 0\n                ? (alias_p ? TAG_ALIAS_MEM_DISP_BASE_INDEX : TAG_MEM_DISP_BASE_INDEX)\n                : (alias_p ? TAG_ALIAS_MEM_DISP_BASE : TAG_MEM_DISP_BASE);\n      else\n        tag = op.u.mem.index != 0 ? (alias_p ? TAG_ALIAS_MEM_DISP_INDEX : TAG_MEM_DISP_INDEX)\n                                  : (alias_p ? TAG_ALIAS_MEM_DISP : TAG_MEM_DISP);\n    } else if (op.u.mem.base != 0) {\n      tag = op.u.mem.index != 0 ? (alias_p ? TAG_ALIAS_MEM_BASE_INDEX : TAG_MEM_BASE_INDEX)\n                                : (alias_p ? TAG_ALIAS_MEM_BASE : TAG_MEM_BASE);\n    } else if (op.u.mem.index != 0) {\n      tag = alias_p ? TAG_ALIAS_MEM_INDEX : TAG_MEM_INDEX;\n    } else {\n      tag = alias_p ? TAG_ALIAS_MEM_DISP : TAG_MEM_DISP;\n    }\n    put_byte (ctx, writer, tag);\n    len = write_type (ctx, writer, op.u.mem.type) + 1;\n    if (op.u.mem.disp != 0 || (op.u.mem.base == 0 && op.u.mem.index == 0))\n      write_int (ctx, writer, op.u.mem.disp);\n    if (op.u.mem.base != 0) write_reg (ctx, writer, MIR_reg_name (ctx, op.u.mem.base, func));\n    if (op.u.mem.index != 0) {\n      len += write_reg (ctx, writer, MIR_reg_name (ctx, op.u.mem.index, func));\n      len += write_uint (ctx, writer, op.u.mem.scale);\n    }\n    if (alias_p) {\n      len += write_name (ctx, writer, MIR_alias_name (ctx, op.u.mem.alias));\n      len += write_name (ctx, writer, MIR_alias_name (ctx, op.u.mem.nonalias));\n    }\n    output_mem_len += len;\n    return len;\n  }\n  case MIR_OP_REF: return write_name (ctx, writer, MIR_item_name (ctx, op.u.ref));\n  case MIR_OP_STR: return write_str (ctx, writer, op.u.str);\n  case MIR_OP_LABEL: return write_lab (ctx, writer, op.u.label);\n  default: mir_assert (FALSE); return 0;\n  }\n}\n\nstatic size_t write_insn (MIR_context_t ctx, writer_func_t writer, MIR_func_t func,\n                          MIR_insn_t insn) {\n  size_t i, nops;\n  MIR_insn_code_t code = insn->code;\n  size_t len;\n\n  if (code == MIR_UNSPEC || code == MIR_USE || code == MIR_PHI)\n    MIR_get_error_func (ctx) (MIR_binary_io_error,\n                              \"UNSPEC, USE, or PHI is not portable and can not be output\");\n  if (code == MIR_LABEL) return write_lab (ctx, writer, insn);\n  nops = MIR_insn_nops (ctx, insn);\n  len = write_uint (ctx, writer, code);\n  for (i = 0; i < nops; i++) len += write_op (ctx, writer, func, insn->ops[i]);\n  if (insn_descs[code].op_modes[0] == MIR_OP_BOUND) {\n    /* first operand mode is undefined if it is a variable operand insn */\n    mir_assert (MIR_call_code_p (code) || code == MIR_RET || code == MIR_SWITCH);\n    put_byte (ctx, writer, TAG_EOI);\n    len++;\n  }\n  output_insns_len += len;\n  return len;\n}\n\nstatic size_t write_vars (MIR_context_t ctx, writer_func_t writer, MIR_func_t func,\n                          VARR (MIR_var_t) * vars, size_t start, size_t vars_num,\n                          const char *prefix) {\n  if (vars_num == 0 || vars == NULL) return 0;\n  size_t len = 0;\n  int first_p = TRUE;\n  for (size_t i = 0; i < vars_num; i++) {\n    MIR_var_t var = VARR_GET (MIR_var_t, vars, i + start);\n    if (first_p) len += write_name (ctx, writer, prefix);\n    first_p = FALSE;\n    len += write_type (ctx, writer, var.type);\n    len += write_name (ctx, writer, var.name);\n    MIR_reg_t reg = MIR_reg (ctx, var.name, func);\n    const char *hard_reg_name = MIR_reg_hard_reg_name (ctx, reg, func);\n    if (hard_reg_name != NULL) len += write_name (ctx, writer, hard_reg_name);\n  }\n  len += put_byte (ctx, writer, TAG_EOI);\n  return len;\n}\n\nstatic size_t write_item (MIR_context_t ctx, writer_func_t writer, MIR_item_t item) {\n  MIR_insn_t insn;\n  MIR_func_t func;\n  MIR_proto_t proto;\n  MIR_var_t var;\n  size_t i, vars_num, len = 0;\n\n  if (item->item_type == MIR_import_item) {\n    len += write_name (ctx, writer, \"import\");\n    len += write_name (ctx, writer, item->u.import_id);\n    return len;\n  }\n  if (item->item_type == MIR_export_item) {\n    len += write_name (ctx, writer, \"export\");\n    len += write_name (ctx, writer, item->u.export_id);\n    return len;\n  }\n  if (item->item_type == MIR_forward_item) {\n    len += write_name (ctx, writer, \"forward\");\n    len += write_name (ctx, writer, item->u.forward_id);\n    return len;\n  }\n  if (item->item_type == MIR_bss_item) {\n    if (item->u.bss->name == NULL) {\n      len += write_name (ctx, writer, \"bss\");\n    } else {\n      len += write_name (ctx, writer, \"nbss\");\n      len += write_name (ctx, writer, item->u.bss->name);\n    }\n    len += write_uint (ctx, writer, item->u.bss->len);\n    return len;\n  }\n  if (item->item_type == MIR_ref_data_item) {\n    if (item->u.ref_data->name == NULL) {\n      len += write_name (ctx, writer, \"ref\");\n    } else {\n      len += write_name (ctx, writer, \"nref\");\n      len += write_name (ctx, writer, item->u.ref_data->name);\n    }\n    len += write_name (ctx, writer, MIR_item_name (ctx, item->u.ref_data->ref_item));\n    len += write_int (ctx, writer, item->u.ref_data->disp);\n    return len;\n  }\n  if (item->item_type == MIR_lref_data_item) {\n    if (item->u.lref_data->name == NULL) {\n      len += write_name (ctx, writer, \"lref\");\n    } else {\n      len += write_name (ctx, writer, \"nlref\");\n      len += write_name (ctx, writer, item->u.lref_data->name);\n    }\n    mir_assert (item->u.lref_data->label->ops[0].mode == MIR_OP_INT);\n    mir_assert (item->u.lref_data->label2 == NULL\n                || (item->u.lref_data->label2->ops[0].mode == MIR_OP_INT\n                    && item->u.lref_data->label2->ops[0].u.i >= 0));\n    len += write_int (ctx, writer, item->u.lref_data->label->ops[0].u.i);\n    if (item->u.lref_data->label2 == NULL) {\n      len += write_int (ctx, writer, -1);\n    } else {\n      mir_assert (item->u.lref_data->label2->ops[0].mode == MIR_OP_INT\n                  && item->u.lref_data->label2->ops[0].u.i >= 0);\n      len += write_int (ctx, writer, item->u.lref_data->label2->ops[0].u.i);\n    }\n    len += write_int (ctx, writer, item->u.lref_data->disp);\n    return len;\n  }\n  if (item->item_type == MIR_expr_data_item) {\n    if (item->u.expr_data->name == NULL) {\n      len += write_name (ctx, writer, \"expr\");\n    } else {\n      len += write_name (ctx, writer, \"nexpr\");\n      len += write_name (ctx, writer, item->u.expr_data->name);\n    }\n    len += write_name (ctx, writer, MIR_item_name (ctx, item->u.expr_data->expr_item));\n    return len;\n  }\n  if (item->item_type == MIR_data_item) {\n    MIR_data_t data = item->u.data;\n\n    if (data->name == NULL) {\n      len += write_name (ctx, writer, \"data\");\n    } else {\n      len += write_name (ctx, writer, \"ndata\");\n      len += write_name (ctx, writer, data->name);\n    }\n    write_type (ctx, writer, data->el_type);\n    for (i = 0; i < data->nel; i++) switch (data->el_type) {\n      case MIR_T_I8: len += write_int (ctx, writer, ((int8_t *) data->u.els)[i]); break;\n      case MIR_T_U8: len += write_uint (ctx, writer, ((uint8_t *) data->u.els)[i]); break;\n      case MIR_T_I16: len += write_int (ctx, writer, ((int16_t *) data->u.els)[i]); break;\n      case MIR_T_U16: len += write_uint (ctx, writer, ((uint16_t *) data->u.els)[i]); break;\n      case MIR_T_I32: len += write_int (ctx, writer, ((int32_t *) data->u.els)[i]); break;\n      case MIR_T_U32: len += write_uint (ctx, writer, ((uint32_t *) data->u.els)[i]); break;\n      case MIR_T_I64: len += write_int (ctx, writer, ((int64_t *) data->u.els)[i]); break;\n      case MIR_T_U64: len += write_uint (ctx, writer, ((uint64_t *) data->u.els)[i]); break;\n      case MIR_T_F: len += write_float (ctx, writer, ((float *) data->u.els)[i]); break;\n      case MIR_T_D: len += write_double (ctx, writer, ((double *) data->u.els)[i]); break;\n      case MIR_T_LD:\n        len += write_ldouble (ctx, writer, ((long double *) data->u.els)[i]);\n        break;\n        /* only ptr as ref ??? */\n      case MIR_T_P: len += write_uint (ctx, writer, ((uintptr_t *) data->u.els)[i]); break;\n      default: mir_assert (FALSE);\n      }\n    len += put_byte (ctx, writer, TAG_EOI);\n    return len;\n  }\n  if (item->item_type == MIR_proto_item) {\n    proto = item->u.proto;\n    len += write_name (ctx, writer, \"proto\");\n    len += write_name (ctx, writer, proto->name);\n    len += write_uint (ctx, writer, proto->vararg_p != 0);\n    len += write_uint (ctx, writer, proto->nres);\n    for (i = 0; i < proto->nres; i++) write_type (ctx, writer, proto->res_types[i]);\n    for (i = 0; i < VARR_LENGTH (MIR_var_t, proto->args); i++) {\n      var = VARR_GET (MIR_var_t, proto->args, i);\n      len += write_type (ctx, writer, var.type);\n      len += write_name (ctx, writer, var.name);\n      if (MIR_all_blk_type_p (var.type)) len += write_uint (ctx, writer, var.size);\n    }\n    len += put_byte (ctx, writer, TAG_EOI);\n    return len;\n  }\n  func = item->u.func;\n  len += write_name (ctx, writer, \"func\");\n  len += write_name (ctx, writer, func->name);\n  len += write_uint (ctx, writer, func->vararg_p != 0);\n  len += write_uint (ctx, writer, func->nres);\n  for (i = 0; i < func->nres; i++) len += write_type (ctx, writer, func->res_types[i]);\n  for (i = 0; i < func->nargs; i++) {\n    var = VARR_GET (MIR_var_t, func->vars, i);\n    len += write_type (ctx, writer, var.type);\n    len += write_name (ctx, writer, var.name);\n    if (MIR_all_blk_type_p (var.type)) len += write_uint (ctx, writer, var.size);\n  }\n  len += put_byte (ctx, writer, TAG_EOI);\n  vars_num = VARR_LENGTH (MIR_var_t, func->vars) - func->nargs;\n  len += write_vars (ctx, writer, func, func->vars, func->nargs, vars_num, \"local\");\n  len += write_vars (ctx, writer, func, func->global_vars, 0,\n                     func->global_vars == NULL ? 0 : VARR_LENGTH (MIR_var_t, func->global_vars),\n                     \"global\");\n  for (insn = DLIST_HEAD (MIR_insn_t, func->insns); insn != NULL;\n       insn = DLIST_NEXT (MIR_insn_t, insn))\n    len += write_insn (ctx, writer, func, insn);\n  len += write_name (ctx, writer, \"endfunc\");\n  return len;\n}\n\nstatic size_t write_module (MIR_context_t ctx, writer_func_t writer, MIR_module_t module) {\n  size_t len = write_name (ctx, writer, \"module\");\n\n  len += write_name (ctx, writer, module->name);\n  for (MIR_item_t item = DLIST_HEAD (MIR_item_t, module->items); item != NULL;\n       item = DLIST_NEXT (MIR_item_t, item))\n    len += write_item (ctx, writer, item);\n  len += write_name (ctx, writer, \"endmodule\");\n  return len;\n}\n\nstatic size_t write_modules (MIR_context_t ctx, writer_func_t writer, MIR_module_t module) {\n  size_t len = 0;\n\n  for (MIR_module_t m = DLIST_HEAD (MIR_module_t, all_modules); m != NULL;\n       m = DLIST_NEXT (MIR_module_t, m))\n    if (module == NULL || m == module) len += write_module (ctx, writer, m);\n  return len;\n}\n\nstatic size_t reduce_writer (const void *start, size_t len, void *aux_data) {\n  MIR_context_t ctx = aux_data;\n  size_t i, n = 0;\n\n  for (i = n = 0; i < len; i++, n++)\n    if (io_writer (ctx, ((uint8_t *) start)[i]) == EOF) break;\n  return n;\n}\n\nvoid MIR_write_module_with_func (MIR_context_t ctx, int (*const writer) (MIR_context_t, uint8_t),\n                                 MIR_module_t module) {\n  size_t MIR_UNUSED len;\n  size_t str_len;\n\n  io_writer = writer;\n#ifndef MIR_NO_BIN_COMPRESSION\n  if ((io_reduce_data = reduce_encode_start (ctx->alloc, reduce_writer, ctx)) == NULL)\n    MIR_get_error_func (ctx) (MIR_binary_io_error, \"can not alloc data for MIR binary compression\");\n#endif\n  output_insns_len = output_labs_len = 0;\n  output_regs_len = output_mem_len = output_int_len = output_float_len = 0;\n  string_init (ctx->alloc, &output_strings, &output_string_tab);\n  write_modules (ctx, NULL, module); /* store strings */\n  len = write_uint (ctx, reduce_writer, CURR_BIN_VERSION);\n  str_len = write_uint (ctx, reduce_writer, VARR_LENGTH (string_t, output_strings) - 1);\n  for (size_t i = 1; i < VARR_LENGTH (string_t, output_strings); i++) { /* output strings */\n    MIR_str_t str = VARR_GET (string_t, output_strings, i).str;\n\n    str_len += write_uint (ctx, reduce_writer, str.len);\n    for (size_t j = 0; j < str.len; j++) {\n      put_byte (ctx, reduce_writer, str.s[j]);\n      str_len++;\n    }\n  }\n  len += write_modules (ctx, reduce_writer, module) + str_len;\n#if 0\n  fprintf (stderr,\n           \"Overall output length = %lu.  Number of strings = %lu.\\n\"\n           \"Lengths of: strings = %lu, insns = %lu, labs = %lu,\\n\"\n           \"   reg ops = %lu, mem ops = %lu, int ops = %lu, float ops = %lu\\n\",\n           len, VARR_LENGTH (string_t, output_strings), str_len, output_insns_len, output_labs_len,\n           output_regs_len, output_mem_len, output_int_len, output_float_len);\n#endif\n  put_byte (ctx, reduce_writer, TAG_EOFILE);\n  string_finish (ctx->alloc, &output_strings, &output_string_tab);\n#ifndef MIR_NO_BIN_COMPRESSION\n  if (!reduce_encode_finish (ctx->alloc, io_reduce_data))\n    MIR_get_error_func (ctx) (MIR_binary_io_error, \"error in writing MIR binary\");\n#endif\n}\n\nvoid MIR_write_with_func (MIR_context_t ctx, int (*const writer) (MIR_context_t, uint8_t)) {\n  MIR_write_module_with_func (ctx, writer, NULL);\n}\n\nstatic int file_writer (MIR_context_t ctx, uint8_t byte) { return fputc (byte, io_file); }\n\nvoid MIR_write_module (MIR_context_t ctx, FILE *f, MIR_module_t module) {\n  io_file = f;\n  MIR_write_module_with_func (ctx, file_writer, module);\n}\n\nvoid MIR_write (MIR_context_t ctx, FILE *f) { MIR_write_module (ctx, f, NULL); }\n\n/* New Page */\n\nstatic int get_byte (MIR_context_t ctx) {\n#ifdef MIR_NO_BIN_COMPRESSION\n  int c = io_reader (ctx);\n#else\n  int c = reduce_decode_get (io_reduce_data);\n#endif\n\n  if (c == EOF) MIR_get_error_func (ctx) (MIR_binary_io_error, \"unfinished binary MIR\");\n  return c;\n}\n\ntypedef union {\n  uint64_t u;\n  int64_t i;\n  float f;\n  double d;\n  long double ld;\n  MIR_type_t t;\n  MIR_reg_t reg;\n} token_attr_t;\n\nstatic uint64_t get_uint (MIR_context_t ctx, int nb) {\n  uint64_t res = 0;\n\n  for (int i = 0; i < nb; i++) res |= (uint64_t) get_byte (ctx) << (i * CHAR_BIT);\n  return res;\n}\n\nstatic int64_t get_int (MIR_context_t ctx, int nb) { return (int64_t) get_uint (ctx, nb); }\n\nstatic float get_float (MIR_context_t ctx) {\n  union {\n    uint32_t u;\n    float f;\n  } u;\n\n  u.u = (uint32_t) get_uint (ctx, (int) sizeof (uint32_t));\n  return u.f;\n}\n\nstatic double get_double (MIR_context_t ctx) {\n  union {\n    uint64_t u;\n    double d;\n  } u;\n\n  u.u = get_uint (ctx, sizeof (uint64_t));\n  return u.d;\n}\n\nstatic long double get_ldouble (MIR_context_t ctx) {\n  union {\n    uint64_t u[2];\n    long double ld;\n  } u;\n\n  u.u[0] = get_uint (ctx, sizeof (uint64_t));\n  u.u[1] = get_uint (ctx, sizeof (uint64_t));\n  return u.ld;\n}\n\nstatic MIR_str_t to_str (MIR_context_t ctx, uint64_t str_num) {\n  if (str_num >= VARR_LENGTH (MIR_str_t, bin_strings))\n    MIR_get_error_func (ctx) (MIR_binary_io_error, \"wrong string num %lu\", str_num);\n  return VARR_GET (MIR_str_t, bin_strings, str_num);\n}\n\nstatic MIR_reg_t to_reg (MIR_context_t ctx, uint64_t reg_str_num, MIR_item_t func) {\n  const char *s = to_str (ctx, reg_str_num).s;\n\n  process_reserved_name (s, TEMP_REG_NAME_PREFIX, &func->u.func->last_temp_num);\n  return MIR_reg (ctx, s, func->u.func);\n}\n\nstatic MIR_label_t to_lab (MIR_context_t ctx, uint64_t lab_num) {\n  MIR_label_t lab;\n\n  while (lab_num >= VARR_LENGTH (MIR_label_t, func_labels))\n    VARR_PUSH (MIR_label_t, func_labels, NULL);\n  if ((lab = VARR_GET (MIR_label_t, func_labels, lab_num)) != NULL) return lab;\n  lab = create_label (ctx, lab_num);\n  VARR_SET (MIR_label_t, func_labels, lab_num, lab);\n  return lab;\n}\n\nstatic int64_t read_int (MIR_context_t ctx, const char *err_msg) {\n  int c = get_byte (ctx);\n\n  if (TAG_I1 > c || c > TAG_I8) MIR_get_error_func (ctx) (MIR_binary_io_error, err_msg);\n  return get_int (ctx, c - TAG_I1 + 1);\n}\n\nstatic uint64_t read_uint (MIR_context_t ctx, const char *err_msg) {\n  int c = get_byte (ctx);\n\n  if (c & U0_FLAG) return c & U0_MASK;\n  if (TAG_U1 > c || c > TAG_U8) MIR_get_error_func (ctx) (MIR_binary_io_error, err_msg);\n  return get_uint (ctx, c - TAG_U1 + 1);\n}\n\nstatic void read_all_strings (MIR_context_t ctx, uint64_t nstr) {\n  int c;\n  MIR_str_t str;\n  uint64_t len, l;\n\n  VARR_TRUNC (MIR_str_t, bin_strings, 0);\n  for (uint64_t i = 0; i < nstr; i++) {\n    VARR_TRUNC (char, temp_string, 0);\n    len = read_uint (ctx, \"wrong string length\");\n    for (l = 0; l < len; l++) {\n      c = get_byte (ctx);\n      VARR_PUSH (char, temp_string, c);\n    }\n    str.s = VARR_ADDR (char, temp_string);\n    str.len = len;\n    str = get_ctx_string (ctx, str).str;\n    VARR_PUSH (MIR_str_t, bin_strings, str);\n  }\n}\n\nstatic MIR_type_t tag_type (bin_tag_t tag) { return (MIR_type_t) (tag - TAG_TI8) + MIR_T_I8; }\n\nstatic MIR_type_t read_type (MIR_context_t ctx, const char *err_msg) {\n  int c = get_byte (ctx);\n\n  if (TAG_TI8 > c || c > TAG_TRBLOCK) MIR_get_error_func (ctx) (MIR_binary_io_error, err_msg);\n  return tag_type (c);\n}\n\nstatic const char *read_name (MIR_context_t ctx, MIR_module_t module, const char *err_msg) {\n  int c = get_byte (ctx);\n  const char *s;\n\n  if (TAG_NAME1 > c || c > TAG_NAME4) MIR_get_error_func (ctx) (MIR_binary_io_error, err_msg);\n  s = to_str (ctx, get_uint (ctx, c - TAG_NAME1 + 1)).s;\n  if (module != NULL) process_reserved_name (s, TEMP_ITEM_NAME_PREFIX, &module->last_temp_item_num);\n  return s;\n}\n\n#define TAG_CASE(t) case TAG_##t:\n#define REP_SEP\nstatic bin_tag_t read_token (MIR_context_t ctx, token_attr_t *attr) {\n  int c = get_byte (ctx);\n\n  if (c & U0_FLAG) {\n    attr->u = c & U0_MASK;\n    return TAG_U0;\n  }\n  switch (c) {\n    REP8 (TAG_CASE, U1, U2, U3, U4, U5, U6, U7, U8)\n    attr->u = get_uint (ctx, c - TAG_U1 + 1);\n    break;\n    REP8 (TAG_CASE, I1, I2, I3, I4, I5, I6, I7, I8)\n    attr->i = get_int (ctx, c - TAG_I1 + 1);\n    break;\n    TAG_CASE (F)\n    attr->f = get_float (ctx);\n    break;\n    TAG_CASE (D)\n    attr->d = get_double (ctx);\n    break;\n    TAG_CASE (LD)\n    attr->ld = get_ldouble (ctx);\n    break;\n    REP4 (TAG_CASE, REG1, REG2, REG3, REG4)\n    attr->u = get_uint (ctx, c - TAG_REG1 + 1);\n    break;\n    REP4 (TAG_CASE, NAME1, NAME2, NAME3, NAME4)\n    attr->u = get_uint (ctx, c - TAG_NAME1 + 1);\n    break;\n    REP4 (TAG_CASE, STR1, STR2, STR3, STR4)\n    attr->u = get_uint (ctx, c - TAG_STR1 + 1);\n    break;\n    REP4 (TAG_CASE, LAB1, LAB2, LAB3, LAB4)\n    attr->u = get_uint (ctx, c - TAG_LAB1 + 1);\n    break;\n    REP6 (TAG_CASE, MEM_DISP, MEM_BASE, MEM_INDEX, MEM_DISP_BASE, MEM_DISP_INDEX, MEM_BASE_INDEX)\n    REP3 (TAG_CASE, MEM_DISP_BASE_INDEX, EOI, EOFILE)\n    REP4 (TAG_CASE, ALIAS_MEM_DISP, ALIAS_MEM_BASE, ALIAS_MEM_INDEX, ALIAS_MEM_DISP_BASE)\n    REP3 (TAG_CASE, ALIAS_MEM_DISP_INDEX, ALIAS_MEM_BASE_INDEX, ALIAS_MEM_DISP_BASE_INDEX)\n    break;\n    REP8 (TAG_CASE, TI8, TU8, TI16, TU16, TI32, TU32, TI64, TU64)\n    REP5 (TAG_CASE, TF, TD, TP, TV, TRBLOCK)\n    attr->t = (MIR_type_t) (c - TAG_TI8) + MIR_T_I8;\n    break;\n  default:\n    if (TAG_TBLOCK <= c && c < TAG_TBLOCK + MIR_BLK_NUM) {\n      attr->t = (MIR_type_t) (c - TAG_TBLOCK) + MIR_T_BLK;\n      break;\n    }\n    MIR_get_error_func (ctx) (MIR_binary_io_error, \"wrong tag %d\", c);\n  }\n  return c;\n}\n\nstatic MIR_disp_t read_disp (MIR_context_t ctx) {\n  bin_tag_t tag;\n  token_attr_t attr;\n\n  tag = read_token (ctx, &attr);\n  if (TAG_I1 > tag || tag > TAG_I8)\n    MIR_get_error_func (ctx) (MIR_binary_io_error, \"memory disp has wrong tag %d\", tag);\n  return attr.i;\n}\n\nstatic MIR_reg_t read_reg (MIR_context_t ctx, MIR_item_t func) {\n  bin_tag_t tag;\n  token_attr_t attr;\n\n  tag = read_token (ctx, &attr);\n  if (TAG_REG1 > tag || tag > TAG_REG4)\n    MIR_get_error_func (ctx) (MIR_binary_io_error, \"register has wrong tag %d\", tag);\n  return to_reg (ctx, attr.u, func);\n}\n\nstatic int read_operand (MIR_context_t ctx, MIR_op_t *op, MIR_item_t func) {\n  bin_tag_t tag;\n  token_attr_t attr;\n  MIR_type_t t;\n  MIR_disp_t disp;\n  MIR_reg_t base, index;\n  MIR_scale_t scale;\n  int alias_p = FALSE;\n  const char *name;\n\n  tag = read_token (ctx, &attr);\n  switch (tag) {\n    TAG_CASE (U0)\n    REP8 (TAG_CASE, U1, U2, U3, U4, U5, U6, U7, U8) *op = MIR_new_uint_op (ctx, attr.u);\n    break;\n    REP8 (TAG_CASE, I1, I2, I3, I4, I5, I6, I7, I8)\n    *op = MIR_new_int_op (ctx, attr.i);\n    break;\n    TAG_CASE (F)\n    *op = MIR_new_float_op (ctx, attr.f);\n    break;\n    TAG_CASE (D)\n    *op = MIR_new_double_op (ctx, attr.d);\n    break;\n    TAG_CASE (LD)\n    *op = MIR_new_ldouble_op (ctx, attr.ld);\n    break;\n    REP4 (TAG_CASE, REG1, REG2, REG3, REG4)\n    *op = MIR_new_reg_op (ctx, to_reg (ctx, attr.u, func));\n    break;\n    REP4 (TAG_CASE, NAME1, NAME2, NAME3, NAME4) {\n      name = to_str (ctx, attr.u).s;\n      MIR_item_t item = item_tab_find (ctx, name, func->module);\n\n      if (item == NULL) MIR_get_error_func (ctx) (MIR_binary_io_error, \"not found item %s\", name);\n      *op = MIR_new_ref_op (ctx, item);\n      break;\n    }\n    REP4 (TAG_CASE, STR1, STR2, STR3, STR4)\n    *op = MIR_new_str_op (ctx, to_str (ctx, attr.u));\n    break;\n    REP4 (TAG_CASE, LAB1, LAB2, LAB3, LAB4)\n    *op = MIR_new_label_op (ctx, to_lab (ctx, attr.u));\n    break;\n    REP7 (TAG_CASE, ALIAS_MEM_DISP, ALIAS_MEM_BASE, ALIAS_MEM_INDEX, ALIAS_MEM_DISP_BASE,\n          ALIAS_MEM_DISP_INDEX, ALIAS_MEM_BASE_INDEX, ALIAS_MEM_DISP_BASE_INDEX)\n    alias_p = TRUE;\n    /* falls through */\n  case TAG_MEM_DISP:\n    REP6 (TAG_CASE, MEM_BASE, MEM_INDEX, MEM_DISP_BASE, MEM_DISP_INDEX, MEM_BASE_INDEX,\n          MEM_DISP_BASE_INDEX)\n    t = read_type (ctx, \"wrong memory type\");\n    disp = (tag == TAG_MEM_DISP || tag == TAG_MEM_DISP_BASE || tag == TAG_MEM_DISP_INDEX\n                || tag == TAG_MEM_DISP_BASE_INDEX || tag == TAG_ALIAS_MEM_DISP\n                || tag == TAG_ALIAS_MEM_DISP_BASE || tag == TAG_ALIAS_MEM_DISP_INDEX\n                || tag == TAG_ALIAS_MEM_DISP_BASE_INDEX\n              ? read_disp (ctx)\n              : 0);\n    base = (tag == TAG_MEM_BASE || tag == TAG_MEM_DISP_BASE || tag == TAG_MEM_BASE_INDEX\n                || tag == TAG_MEM_DISP_BASE_INDEX || tag == TAG_ALIAS_MEM_BASE\n                || tag == TAG_ALIAS_MEM_DISP_BASE || tag == TAG_ALIAS_MEM_BASE_INDEX\n                || tag == TAG_ALIAS_MEM_DISP_BASE_INDEX\n              ? read_reg (ctx, func)\n              : 0);\n    index = 0;\n    scale = 0;\n    if (tag == TAG_MEM_INDEX || tag == TAG_MEM_DISP_INDEX || tag == TAG_MEM_BASE_INDEX\n        || tag == TAG_MEM_DISP_BASE_INDEX || tag == TAG_ALIAS_MEM_INDEX\n        || tag == TAG_ALIAS_MEM_DISP_INDEX || tag == TAG_ALIAS_MEM_BASE_INDEX\n        || tag == TAG_ALIAS_MEM_DISP_BASE_INDEX) {\n      index = read_reg (ctx, func);\n      scale = (MIR_scale_t) read_uint (ctx, \"wrong memory index scale\");\n    }\n    *op = MIR_new_mem_op (ctx, t, disp, base, index, scale);\n    if (alias_p) {\n      name = read_name (ctx, func->module, \"wrong alias name\");\n      if (strcmp (name, \"\") != 0) op->u.mem.alias = MIR_alias (ctx, name);\n      name = read_name (ctx, func->module, \"wrong nonalias name\");\n      if (strcmp (name, \"\") != 0) op->u.mem.nonalias = MIR_alias (ctx, name);\n    }\n    break;\n  case TAG_EOI: return FALSE;\n  default: mir_assert (FALSE);\n  }\n  return TRUE;\n}\n#undef REP_SEP\n\nstatic int func_proto_read (MIR_context_t ctx, MIR_module_t module, uint64_t *nres_ptr) {\n  bin_tag_t tag;\n  token_attr_t attr;\n  MIR_var_t var;\n  int vararg_p = read_uint (ctx, \"wrong vararg flag\") != 0;\n  uint64_t i, nres = read_uint (ctx, \"wrong func nres\");\n\n  VARR_TRUNC (MIR_type_t, proto_types, 0);\n  for (i = 0; i < nres; i++) {\n    tag = read_token (ctx, &attr);\n    if (TAG_TI8 > tag || tag > TAG_TRBLOCK)\n      MIR_get_error_func (ctx) (MIR_binary_io_error, \"wrong prototype result type tag %d\", tag);\n    VARR_PUSH (MIR_type_t, proto_types, tag_type (tag));\n  }\n  VARR_TRUNC (MIR_var_t, proto_vars, 0);\n  for (;;) {\n    tag = read_token (ctx, &attr);\n    if (tag == TAG_EOI) break;\n    if (TAG_TI8 > tag || tag > TAG_TRBLOCK)\n      MIR_get_error_func (ctx) (MIR_binary_io_error, \"wrong prototype arg type tag %d\", tag);\n    var.type = tag_type (tag);\n    var.name = read_name (ctx, module, \"wrong arg name\");\n    if (MIR_all_blk_type_p (var.type)) var.size = read_uint (ctx, \"wrong block arg size\");\n    VARR_PUSH (MIR_var_t, proto_vars, var);\n  }\n  *nres_ptr = nres;\n  return vararg_p;\n}\n\n#ifndef MIR_NO_BIN_COMPRESSION\nstatic size_t reduce_reader (void *start, size_t len, void *data) {\n  MIR_context_t ctx = data;\n  size_t i;\n  int c;\n\n  for (i = 0; i < len && (c = io_reader (ctx)) != EOF; i++) ((char *) start)[i] = c;\n  return i;\n}\n#endif\n\nvoid MIR_read_with_func (MIR_context_t ctx, int (*const reader) (MIR_context_t)) {\n  int version, global_p, nlref_p;\n  bin_tag_t tag, type_tag;\n  token_attr_t attr;\n  MIR_label_t lab, lab2;\n  uint64_t nstr, nres, u;\n  int64_t i;\n  MIR_op_t op;\n  size_t n, nop;\n  const char *name, *item_name;\n  MIR_module_t module;\n  MIR_item_t func, item;\n\n  io_reader = reader;\n#ifndef MIR_NO_BIN_COMPRESSION\n  if ((io_reduce_data = reduce_decode_start (ctx->alloc, reduce_reader, ctx)) == NULL)\n    MIR_get_error_func (ctx) (MIR_binary_io_error,\n                              \"can not alloc data for MIR binary decompression\");\n#endif\n  version = (int) read_uint (ctx, \"wrong header\");\n  if (version > CURR_BIN_VERSION)\n    MIR_get_error_func (ctx) (MIR_binary_io_error,\n                              \"can not read version %d MIR binary: expected %d or less\", version,\n                              CURR_BIN_VERSION);\n  nstr = read_uint (ctx, \"wrong header\");\n  read_all_strings (ctx, nstr);\n  module = NULL;\n  func = NULL;\n  for (;;) {\n    VARR_TRUNC (uint64_t, insn_label_string_nums, 0);\n    tag = read_token (ctx, &attr);\n    while (TAG_LAB1 <= tag && tag <= TAG_LAB4) {\n      VARR_PUSH (uint64_t, insn_label_string_nums, attr.u);\n      tag = read_token (ctx, &attr);\n    }\n    VARR_TRUNC (MIR_op_t, read_insn_ops, 0);\n    if (TAG_NAME1 <= tag && tag <= TAG_NAME4) {\n      name = to_str (ctx, attr.u).s;\n      if (strcmp (name, \"module\") == 0) {\n        name = read_name (ctx, module, \"wrong module name\");\n        if (VARR_LENGTH (uint64_t, insn_label_string_nums) != 0)\n          MIR_get_error_func (ctx) (MIR_binary_io_error, \"insn label before module %s\", name);\n        if (module != NULL)\n          MIR_get_error_func (ctx) (MIR_binary_io_error, \"nested module %s\", name);\n        module = MIR_new_module (ctx, name);\n      } else if (strcmp (name, \"endmodule\") == 0) {\n        if (VARR_LENGTH (uint64_t, insn_label_string_nums) != 0)\n          MIR_get_error_func (ctx) (MIR_binary_io_error, \"endmodule should have no labels\");\n        if (module == NULL)\n          MIR_get_error_func (ctx) (MIR_binary_io_error, \"endmodule without module\");\n        MIR_finish_module (ctx);\n        module = NULL;\n      } else if (strcmp (name, \"proto\") == 0) {\n        name = read_name (ctx, module, \"wrong prototype name\");\n        if (VARR_LENGTH (uint64_t, insn_label_string_nums) != 0)\n          MIR_get_error_func (ctx) (MIR_binary_io_error, \"insn label before proto %s\", name);\n        if (module == NULL)\n          MIR_get_error_func (ctx) (MIR_binary_io_error, \"prototype %s outside module\", name);\n        if (func_proto_read (ctx, module, &nres))\n          MIR_new_vararg_proto_arr (ctx, name, nres, VARR_ADDR (MIR_type_t, proto_types),\n                                    VARR_LENGTH (MIR_var_t, proto_vars),\n                                    VARR_ADDR (MIR_var_t, proto_vars));\n        else\n          MIR_new_proto_arr (ctx, name, nres, VARR_ADDR (MIR_type_t, proto_types),\n                             VARR_LENGTH (MIR_var_t, proto_vars),\n                             VARR_ADDR (MIR_var_t, proto_vars));\n      } else if (strcmp (name, \"func\") == 0) {\n        name = read_name (ctx, module, \"wrong func name\");\n        if (VARR_LENGTH (uint64_t, insn_label_string_nums) != 0)\n          MIR_get_error_func (ctx) (MIR_binary_io_error, \"insn label before func %s\", name);\n        if (func != NULL) MIR_get_error_func (ctx) (MIR_binary_io_error, \"nested func %s\", name);\n        if (module == NULL)\n          MIR_get_error_func (ctx) (MIR_binary_io_error, \"func %s outside module\", name);\n        if (func_proto_read (ctx, module, &nres))\n          func = MIR_new_vararg_func_arr (ctx, name, nres, VARR_ADDR (MIR_type_t, proto_types),\n                                          VARR_LENGTH (MIR_var_t, proto_vars),\n                                          VARR_ADDR (MIR_var_t, proto_vars));\n        else\n          func = MIR_new_func_arr (ctx, name, nres, VARR_ADDR (MIR_type_t, proto_types),\n                                   VARR_LENGTH (MIR_var_t, proto_vars),\n                                   VARR_ADDR (MIR_var_t, proto_vars));\n        VARR_TRUNC (MIR_label_t, func_labels, 0);\n      } else if (strcmp (name, \"endfunc\") == 0) {\n        if (VARR_LENGTH (uint64_t, insn_label_string_nums) != 0)\n          MIR_get_error_func (ctx) (MIR_binary_io_error, \"endfunc should have no labels\");\n        if (func == NULL) MIR_get_error_func (ctx) (MIR_binary_io_error, \"endfunc without func\");\n        MIR_finish_func (ctx);\n        func = NULL;\n      } else if (strcmp (name, \"export\") == 0) {\n        name = read_name (ctx, module, \"wrong export name\");\n        if (VARR_LENGTH (uint64_t, insn_label_string_nums) != 0)\n          MIR_get_error_func (ctx) (MIR_binary_io_error, \"export %s should have no labels\", name);\n        MIR_new_export (ctx, name);\n      } else if (strcmp (name, \"import\") == 0) {\n        name = read_name (ctx, module, \"wrong import name\");\n        if (VARR_LENGTH (uint64_t, insn_label_string_nums) != 0)\n          MIR_get_error_func (ctx) (MIR_binary_io_error, \"import %s should have no labels\", name);\n        MIR_new_import (ctx, name);\n      } else if (strcmp (name, \"forward\") == 0) {\n        name = read_name (ctx, module, \"wrong forward name\");\n        if (VARR_LENGTH (uint64_t, insn_label_string_nums) != 0)\n          MIR_get_error_func (ctx) (MIR_binary_io_error, \"forward %s should have no labels\", name);\n        MIR_new_forward (ctx, name);\n      } else if (strcmp (name, \"nbss\") == 0 || strcmp (name, \"bss\") == 0) {\n        name = strcmp (name, \"nbss\") == 0 ? read_name (ctx, module, \"wrong bss name\") : NULL;\n        if (VARR_LENGTH (uint64_t, insn_label_string_nums) != 0)\n          MIR_get_error_func (ctx) (MIR_binary_io_error, \"bss %s should have no labels\",\n                                    name == NULL ? \"\" : name);\n        u = read_uint (ctx, \"wrong bss len\");\n        MIR_new_bss (ctx, name, u);\n      } else if (strcmp (name, \"nref\") == 0 || strcmp (name, \"ref\") == 0) {\n        name = strcmp (name, \"nref\") == 0 ? read_name (ctx, module, \"wrong ref data name\") : NULL;\n        if (VARR_LENGTH (uint64_t, insn_label_string_nums) != 0)\n          MIR_get_error_func (ctx) (MIR_binary_io_error, \"ref data %s should have no labels\",\n                                    name == NULL ? \"\" : name);\n        item_name = read_name (ctx, module, \"wrong ref data item name\");\n        if ((item = item_tab_find (ctx, item_name, module)) == NULL)\n          MIR_get_error_func (ctx) (MIR_binary_io_error, \"ref data refers to non-existing item %s\",\n                                    item_name);\n        i = read_int (ctx, \"wrong ref disp\");\n        MIR_new_ref_data (ctx, name, item, i);\n      } else if ((nlref_p = strcmp (name, \"nlref\") == 0) || strcmp (name, \"lref\") == 0) {\n        name = NULL;\n        if (nlref_p) name = read_name (ctx, module, \"wrong lref data name\");\n        if (VARR_LENGTH (uint64_t, insn_label_string_nums) != 0)\n          MIR_get_error_func (ctx) (MIR_binary_io_error, \"lref data %s should have no labels\",\n                                    name == NULL ? \"\" : name);\n        i = read_int (ctx, \"wrong lref label num\");\n        lab = create_label (ctx, i);\n        i = read_int (ctx, \"wrong 2nd lref label num\");\n        lab2 = i < 0 ? NULL : create_label (ctx, i);\n        i = read_int (ctx, \"wrong lref disp\");\n        MIR_new_lref_data (ctx, name, lab, lab2, i);\n      } else if (strcmp (name, \"nexpr\") == 0 || strcmp (name, \"expr\") == 0) {\n        name = strcmp (name, \"nexpr\") == 0 ? read_name (ctx, module, \"wrong expr name\") : NULL;\n        if (VARR_LENGTH (uint64_t, insn_label_string_nums) != 0)\n          MIR_get_error_func (ctx) (MIR_binary_io_error, \"expr %s should have no labels\",\n                                    name == NULL ? \"\" : name);\n        item_name = read_name (ctx, module, \"wrong expr func name\");\n        if ((item = item_tab_find (ctx, item_name, module)) == NULL\n            || item->item_type != MIR_func_item)\n          MIR_get_error_func (ctx) (MIR_binary_io_error, \"expr refers to non-function %s\",\n                                    item_name);\n        MIR_new_expr_data (ctx, name, item);\n      } else if (strcmp (name, \"ndata\") == 0 || strcmp (name, \"data\") == 0) {\n        MIR_type_t type;\n        union {\n          uint8_t u8;\n          uint16_t u16;\n          uint32_t u32;\n          uint64_t u64;\n          int8_t i8;\n          int16_t i16;\n          int32_t i32;\n          int64_t i64;\n        } v;\n\n        name = strcmp (name, \"ndata\") == 0 ? read_name (ctx, module, \"wrong data name\") : NULL;\n        if (VARR_LENGTH (uint64_t, insn_label_string_nums) != 0)\n          MIR_get_error_func (ctx) (MIR_binary_io_error, \"data %s should have no labels\",\n                                    name == NULL ? \"\" : name);\n        tag = read_token (ctx, &attr);\n        if (TAG_TI8 > tag || tag > TAG_TRBLOCK)\n          MIR_get_error_func (ctx) (MIR_binary_io_error, \"wrong data type tag %d\", tag);\n        type = tag_type (tag);\n        VARR_TRUNC (uint8_t, temp_data, 0);\n        for (;;) {\n          tag = read_token (ctx, &attr);\n          if (tag == TAG_EOI) break;\n          switch (tag) {\n          case TAG_U0:\n          case TAG_U1:\n          case TAG_U2:\n          case TAG_U3:\n          case TAG_U4:\n          case TAG_U5:\n          case TAG_U6:\n          case TAG_U7:\n          case TAG_U8:\n            switch (type) {\n            case MIR_T_U8:\n              v.u8 = (uint8_t) attr.u;\n              push_data (ctx, &v.u8, sizeof (uint8_t));\n              break;\n            case MIR_T_U16:\n              v.u16 = (uint16_t) attr.u;\n              push_data (ctx, (uint8_t *) &v.u16, sizeof (uint16_t));\n              break;\n            case MIR_T_U32:\n              v.u32 = (uint32_t) attr.u;\n              push_data (ctx, (uint8_t *) &v.u32, sizeof (uint32_t));\n              break;\n            case MIR_T_U64:\n              v.u64 = attr.u;\n              push_data (ctx, (uint8_t *) &v.i64, sizeof (uint64_t));\n              break;\n            default:\n              MIR_get_error_func (ctx) (MIR_binary_io_error,\n                                        \"data type %s does not correspond value type\",\n                                        type_str (ctx, type));\n            }\n            break;\n          case TAG_I1:\n          case TAG_I2:\n          case TAG_I3:\n          case TAG_I4:\n          case TAG_I5:\n          case TAG_I6:\n          case TAG_I7:\n          case TAG_I8:\n            switch (type) {\n            case MIR_T_I8:\n              v.i8 = (int8_t) attr.i;\n              push_data (ctx, (uint8_t *) &v.i8, sizeof (int8_t));\n              break;\n            case MIR_T_I16:\n              v.i16 = (int16_t) attr.i;\n              push_data (ctx, (uint8_t *) &v.i16, sizeof (int16_t));\n              break;\n            case MIR_T_I32:\n              v.i32 = (int32_t) attr.i;\n              push_data (ctx, (uint8_t *) &v.i32, sizeof (int32_t));\n              break;\n            case MIR_T_I64:\n              v.i64 = attr.i;\n              push_data (ctx, (uint8_t *) &v.i64, sizeof (int64_t));\n              break;\n            default:\n              MIR_get_error_func (ctx) (MIR_binary_io_error,\n                                        \"data type %s does not correspond value type\",\n                                        type_str (ctx, type));\n            }\n            break;\n          case TAG_F:\n            if (type != MIR_T_F)\n              MIR_get_error_func (ctx) (MIR_binary_io_error,\n                                        \"data type %s does not correspond value type\",\n                                        type_str (ctx, type));\n            push_data (ctx, (uint8_t *) &attr.f, sizeof (float));\n            break;\n          case TAG_D:\n            if (type != MIR_T_D)\n              MIR_get_error_func (ctx) (MIR_binary_io_error,\n                                        \"data type %s does not correspond value type\",\n                                        type_str (ctx, type));\n            push_data (ctx, (uint8_t *) &attr.d, sizeof (double));\n            break;\n          case TAG_LD:\n            if (type != MIR_T_LD)\n              MIR_get_error_func (ctx) (MIR_binary_io_error,\n                                        \"data type %s does not correspond value type\",\n                                        type_str (ctx, type));\n            push_data (ctx, (uint8_t *) &attr.ld, sizeof (long double));\n            break;\n            /* ??? ptr */\n          default: MIR_get_error_func (ctx) (MIR_binary_io_error, \"wrong data value tag %d\", tag);\n          }\n        }\n        MIR_new_data (ctx, name, type,\n                      VARR_LENGTH (uint8_t, temp_data) / _MIR_type_size (ctx, type),\n                      VARR_ADDR (uint8_t, temp_data));\n      } else if ((global_p = strcmp (name, \"global\") == 0) || strcmp (name, \"local\") == 0) {\n        if (func == NULL)\n          MIR_get_error_func (ctx) (MIR_binary_io_error, \"local/global outside func\");\n        if (VARR_LENGTH (uint64_t, insn_label_string_nums) != 0)\n          MIR_get_error_func (ctx) (MIR_binary_io_error, \"local/global should have no labels\");\n        tag = read_token (ctx, &attr);\n        for (;;) {\n          if (tag == TAG_EOI) break;\n          if (TAG_TI8 > tag || tag > TAG_TRBLOCK)\n            MIR_get_error_func (ctx) (MIR_binary_io_error, \"wrong local/global var type tag %d\",\n                                      tag);\n          name = read_name (ctx, module, \"wrong local/global var name\");\n          type_tag = tag;\n          tag = read_token (ctx, &attr);\n          if (!global_p) {\n            MIR_new_func_reg (ctx, func->u.func, tag_type (type_tag), name);\n          } else if (TAG_NAME1 <= tag && tag <= TAG_NAME4) {\n            const char *reg_name = to_str (ctx, get_uint (ctx, tag - TAG_NAME1 + 1)).s;\n            MIR_new_global_func_reg (ctx, func->u.func, tag_type (type_tag), name, reg_name);\n            tag = read_token (ctx, &attr);\n          } else {\n            MIR_get_error_func (ctx) (MIR_binary_io_error, \"global without hard reg name\");\n          }\n        }\n      } else {\n        MIR_get_error_func (ctx) (MIR_binary_io_error, \"unknown insn name %s\", name);\n      }\n    } else if (TAG_U0 <= tag && tag <= TAG_U8) { /* insn code */\n      MIR_insn_code_t insn_code = attr.u;\n\n      if (insn_code >= MIR_LABEL)\n        MIR_get_error_func (ctx) (MIR_binary_io_error, \"wrong insn code %d\", insn_code);\n      if (insn_code == MIR_UNSPEC || insn_code == MIR_USE || insn_code == MIR_PHI)\n        MIR_get_error_func (ctx) (MIR_binary_io_error,\n                                  \"UNSPEC, USE, or PHI is not portable and can not be read\");\n      for (size_t j = 0; j < VARR_LENGTH (uint64_t, insn_label_string_nums); j++) {\n        lab = to_lab (ctx, VARR_GET (uint64_t, insn_label_string_nums, j));\n        MIR_append_insn (ctx, func, lab);\n      }\n      nop = insn_code_nops (ctx, insn_code);\n      mir_assert (nop != 0 || MIR_call_code_p (insn_code) || insn_code == MIR_RET\n                  || insn_code == MIR_SWITCH);\n      for (n = 0; (nop == 0 || n < nop) && read_operand (ctx, &op, func); n++)\n        VARR_PUSH (MIR_op_t, read_insn_ops, op);\n      if (nop != 0 && n < nop)\n        MIR_get_error_func (ctx) (MIR_binary_io_error, \"wrong number of operands of insn %s\",\n                                  insn_name (insn_code));\n      MIR_append_insn (ctx, func,\n                       MIR_new_insn_arr (ctx, insn_code, n, VARR_ADDR (MIR_op_t, read_insn_ops)));\n    } else if (tag == TAG_EOFILE) {\n      break;\n    } else {\n      MIR_get_error_func (ctx) (MIR_binary_io_error, \"wrong token %d\", tag);\n    }\n  }\n  if (func != NULL)\n    MIR_get_error_func (ctx) (MIR_binary_io_error, \"unfinished func %s\", func->u.func->name);\n  if (module != NULL)\n    MIR_get_error_func (ctx) (MIR_binary_io_error, \"unfinished module %s\", module->name);\n  if (reader (ctx) != EOF)\n    MIR_get_error_func (ctx) (MIR_binary_io_error, \"garbage at the end of file\");\n#ifndef MIR_NO_BIN_COMPRESSION\n  reduce_decode_finish (ctx->alloc, io_reduce_data);\n#endif\n}\n\nstatic int file_reader (MIR_context_t ctx) { return fgetc (io_file); }\n\nvoid MIR_read (MIR_context_t ctx, FILE *f) {\n  io_file = f;\n  MIR_read_with_func (ctx, file_reader);\n}\n\nstatic void io_init (MIR_context_t ctx) {\n  mir_assert (TAG_LAST < 127); /* see bin_tag_t */\n  if ((ctx->io_ctx = MIR_malloc (ctx->alloc, sizeof (struct io_ctx))) == NULL)\n    MIR_get_error_func (ctx) (MIR_alloc_error, \"Not enough memory for ctx\");\n  VARR_CREATE (MIR_var_t, proto_vars, ctx->alloc, 0);\n  VARR_CREATE (MIR_type_t, proto_types, ctx->alloc, 0);\n  VARR_CREATE (MIR_op_t, read_insn_ops, ctx->alloc, 0);\n  VARR_CREATE (MIR_str_t, bin_strings, ctx->alloc, 512);\n  VARR_CREATE (uint64_t, insn_label_string_nums, ctx->alloc, 64);\n  VARR_CREATE (MIR_label_t, func_labels, ctx->alloc, 512);\n}\n\nstatic void io_finish (MIR_context_t ctx) {\n  VARR_DESTROY (MIR_label_t, func_labels);\n  VARR_DESTROY (uint64_t, insn_label_string_nums);\n  VARR_DESTROY (MIR_str_t, bin_strings);\n  VARR_DESTROY (MIR_op_t, read_insn_ops);\n  VARR_DESTROY (MIR_var_t, proto_vars);\n  VARR_DESTROY (MIR_type_t, proto_types);\n  MIR_free (ctx->alloc, ctx->io_ctx);\n  ctx->io_ctx = NULL;\n}\n\n#endif /* if !MIR_NO_IO */\n\n/* New Page */\n\n/* Reading MIR text file */\n\nint _MIR_name_char_p (MIR_context_t ctx MIR_UNUSED, int ch, int first_p) {\n  if (isalpha (ch) || ch == '_' || ch == '$' || ch == '%' || ch == '.') return TRUE;\n  return !first_p && isdigit (ch);\n}\n\n#if !MIR_NO_SCAN\n\n#include <stddef.h>\n#include <stdlib.h>\n#include <ctype.h>\n#include <errno.h>\n#include <setjmp.h>\n\ntypedef struct insn_name {\n  const char *name;\n  MIR_insn_code_t code;\n} insn_name_t;\n\nstatic int insn_name_eq (insn_name_t in1, insn_name_t in2, void *arg MIR_UNUSED) {\n  return strcmp (in1.name, in2.name) == 0;\n}\nstatic htab_hash_t insn_name_hash (insn_name_t in, void *arg MIR_UNUSED) {\n  return (htab_hash_t) mir_hash (in.name, strlen (in.name), 0);\n}\n\n#define TC_EL(t) TC_##t\n#define REP_SEP ,\nenum token_code {\n  REP8 (TC_EL, INT, FLOAT, DOUBLE, LDOUBLE, NAME, STR, NL, EOFILE),\n  REP5 (TC_EL, LEFT_PAR, RIGHT_PAR, COMMA, SEMICOL, COL),\n};\n#undef REP_SEP\n\ntypedef struct token {\n  int code; /* enum token_code and EOF */\n  union {\n    int64_t i;\n    float f;\n    double d;\n    long double ld;\n    const char *name;\n    MIR_str_t str;\n  } u;\n} token_t;\n\nDEF_HTAB (insn_name_t);\ntypedef const char *label_name_t;\nDEF_VARR (label_name_t);\n\ntypedef struct label_desc {\n  int def_p;\n  const char *name;\n  MIR_label_t label;\n} label_desc_t;\n\nDEF_HTAB (label_desc_t);\n\nstruct scan_ctx {\n  jmp_buf error_jmp_buf; /* keep it here to provide malloc alignment */\n  VARR (char) * error_msg_buf;\n  VARR (MIR_var_t) * scan_vars;\n  VARR (MIR_type_t) * scan_types;\n  VARR (MIR_op_t) * scan_insn_ops;\n  size_t curr_lno;\n  HTAB (insn_name_t) * insn_name_tab;\n  const char *input_string;\n  size_t input_string_char_num;\n  VARR (label_name_t) * label_names;\n  HTAB (label_desc_t) * label_desc_tab;\n};\n\n#define error_jmp_buf ctx->scan_ctx->error_jmp_buf\n#define error_msg_buf ctx->scan_ctx->error_msg_buf\n#define scan_vars ctx->scan_ctx->scan_vars\n#define scan_types ctx->scan_ctx->scan_types\n#define scan_insn_ops ctx->scan_ctx->scan_insn_ops\n#define curr_lno ctx->scan_ctx->curr_lno\n#define insn_name_tab ctx->scan_ctx->insn_name_tab\n#define input_string ctx->scan_ctx->input_string\n#define input_string_char_num ctx->scan_ctx->input_string_char_num\n#define label_names ctx->scan_ctx->label_names\n#define label_desc_tab ctx->scan_ctx->label_desc_tab\n\nstatic void scan_error (MIR_context_t ctx, const char *format, ...) {\n  char message[150];\n  size_t len;\n  va_list va;\n\n  va_start (va, format);\n  if (VARR_LENGTH (char, error_msg_buf) != 0) VARR_POP (char, error_msg_buf); /* remove last '\\0' */\n  sprintf (message, \"ln %lu: \", (unsigned long) curr_lno);\n  VARR_PUSH_ARR (char, error_msg_buf, message, strlen (message));\n  len = vsnprintf (message, sizeof (message), format, va);\n  VARR_PUSH_ARR (char, error_msg_buf, message, len);\n  VARR_PUSH_ARR (char, error_msg_buf, \"\\n\", 2); /* add '\\n' and '\\0' */\n  va_end (va);\n  longjmp (error_jmp_buf, TRUE);\n}\n\n/* Read number using GET_CHAR and UNGET_CHAR and already read\n   character CH.  It should be guaranted that the input has a righ\n   prefix (+|-)?[0-9].  Return base, float and double flag through\n   BASE, FLOAT_P, DOUBLE_P.  Put number representation (0x or 0X\n   prefix is removed) into TEMP_STRING.  */\nstatic void scan_number (MIR_context_t ctx, int ch, int get_char (MIR_context_t),\n                         void unget_char (MIR_context_t, int), int *base, int *float_p,\n                         int *double_p, int *ldouble_p) {\n  enum scan_number_code { NUMBER_OK, ABSENT_EXPONENT, NON_DECIMAL_FLOAT, WRONG_OCTAL_INT };\n  enum scan_number_code err_code = NUMBER_OK;\n  int dec_p, hex_p, hex_char_p;\n\n  *base = 10;\n  *ldouble_p = *double_p = *float_p = FALSE;\n  if (ch == '+' || ch == '-') {\n    VARR_PUSH (char, temp_string, ch);\n    ch = get_char (ctx);\n  }\n  mir_assert ('0' <= ch && ch <= '9');\n  if (ch == '0') {\n    ch = get_char (ctx);\n    if (ch != 'x' && ch != 'X') {\n      *base = 8;\n      unget_char (ctx, ch);\n      ch = '0';\n    } else {\n      ch = get_char (ctx);\n      *base = 16;\n    }\n  }\n  dec_p = hex_p = FALSE;\n  for (;;) {\n    if (ch != '_') VARR_PUSH (char, temp_string, ch);\n    ch = get_char (ctx);\n    if (ch == '8' || ch == '9') dec_p = TRUE;\n    hex_char_p = (('a' <= ch && ch <= 'f') || ('A' <= ch && ch <= 'F'));\n    if (ch != '_' && !isdigit (ch) && (*base != 16 || !hex_char_p)) break;\n    if (hex_char_p) hex_p = TRUE;\n  }\n  mir_assert (*base == 16 || !hex_p);\n  if (ch == '.') {\n    *double_p = TRUE;\n    do {\n      if (ch != '_') VARR_PUSH (char, temp_string, ch);\n      ch = get_char (ctx);\n    } while (isdigit (ch) || ch == '_');\n  }\n  if (ch == 'e' || ch == 'E') {\n    *double_p = TRUE;\n    ch = get_char (ctx);\n    if (ch != '+' && ch != '-' && !isdigit (ch))\n      err_code = ABSENT_EXPONENT;\n    else {\n      VARR_PUSH (char, temp_string, 'e');\n      if (ch == '+' || ch == '-') {\n        VARR_PUSH (char, temp_string, ch);\n        ch = get_char (ctx);\n        if (!isdigit (ch)) err_code = ABSENT_EXPONENT;\n      }\n      if (err_code == NUMBER_OK) do {\n          if (ch != '_') VARR_PUSH (char, temp_string, ch);\n          ch = get_char (ctx);\n        } while (isdigit (ch) || ch == '_');\n    }\n  }\n  if (*double_p) {\n    if (*base == 16)\n      err_code = NON_DECIMAL_FLOAT;\n    else if (ch == 'f' || ch == 'F') {\n      *float_p = TRUE;\n      *double_p = FALSE;\n      ch = get_char (ctx);\n    } else if (ch == 'l' || ch == 'L') {\n#if !defined(_WIN32) && __SIZEOF_LONG_DOUBLE__ != 8\n      *ldouble_p = TRUE;\n      *double_p = FALSE;\n#endif\n      ch = get_char (ctx);\n    }\n  } else if (*base == 8 && dec_p)\n    err_code = WRONG_OCTAL_INT;\n  VARR_PUSH (char, temp_string, '\\0');\n  unget_char (ctx, ch);\n}\n\nstatic void scan_string (MIR_context_t ctx, token_t *t, int c, int get_char (MIR_context_t),\n                         void unget_char (MIR_context_t, int)) {\n  int ch_code;\n\n  mir_assert (c == '\\\"');\n  VARR_TRUNC (char, temp_string, 0);\n  for (;;) {\n    if ((c = get_char (ctx)) == EOF || c == '\\n') {\n      VARR_PUSH (char, temp_string, '\\0');\n      scan_error (ctx, \"unfinished string \\\"%s\", VARR_ADDR (char, temp_string));\n    }\n    if (c == '\"') break;\n    if (c == '\\\\') {\n      if ((c = get_char (ctx)) == 'n')\n        c = '\\n';\n      else if (c == 't')\n        c = '\\t';\n      else if (c == 'v')\n        c = '\\v';\n      else if (c == 'a')\n        c = '\\a';\n      else if (c == 'b')\n        c = '\\b';\n      else if (c == 'r')\n        c = '\\r';\n      else if (c == 'f')\n        c = '\\f';\n      else if (c == '\\\\' || c == '\\'' || c == '\\\"')\n        ;\n      else if (c == '\\n') {\n        curr_lno++;\n        continue;\n      } else if (isdigit (c) && c != '8' && c != '9') {\n        ch_code = c - '0';\n        c = get_char (ctx);\n        if (!isdigit (c) || c == '8' || c == '9')\n          unget_char (ctx, c);\n        else {\n          ch_code = ch_code * 8 + c - '0';\n          c = get_char (ctx);\n          if (!isdigit (c) || c == '8' || c == '9')\n            unget_char (ctx, c);\n          else\n            ch_code = ch_code * 8 + c - '0';\n        }\n        c = ch_code;\n      } else if (c == 'x') {\n        /* Hex escape code.  */\n        ch_code = 0;\n        for (int i = 2; i > 0; i--) {\n          c = get_char (ctx);\n          if (!isxdigit (c)) {\n            VARR_PUSH (char, temp_string, '\\0');\n            scan_error (ctx, \"wrong hexadecimal escape in %s\", VARR_ADDR (char, temp_string));\n          }\n          c = '0' <= c && c <= '9' ? c - '0' : 'a' <= c && c <= 'f' ? c - 'a' + 10 : c - 'A' + 10;\n          ch_code = (ch_code << 4) | c;\n        }\n        c = ch_code;\n      }\n    }\n    VARR_PUSH (char, temp_string, c);\n  }\n  if (VARR_LENGTH (char, temp_string) > 0 && VARR_LAST (char, temp_string) != 0)\n    VARR_PUSH (char, temp_string, 0);\n  t->code = TC_STR;\n  t->u.str\n    = string_store (ctx, &strings, &string_tab,\n                    (MIR_str_t){VARR_LENGTH (char, temp_string), VARR_ADDR (char, temp_string)})\n        .str;\n}\n\nstatic int get_string_char (MIR_context_t ctx) {\n  int ch = input_string[input_string_char_num];\n\n  if (ch == '\\0') return EOF;\n  input_string_char_num++;\n  if (ch == '\\n') curr_lno++;\n  return ch;\n}\n\nstatic void unget_string_char (MIR_context_t ctx, int ch) {\n  if (input_string_char_num == 0 || ch == EOF) return;\n  input_string_char_num--;\n  mir_assert (input_string[input_string_char_num] == ch);\n  if (ch == '\\n') curr_lno--;\n}\n\nstatic void scan_token (MIR_context_t ctx, token_t *token, int (*get_char) (MIR_context_t),\n                        void (*unget_char) (MIR_context_t, int)) {\n  int ch;\n\n  for (;;) {\n    ch = get_char (ctx);\n    switch (ch) {\n    case EOF: token->code = TC_EOFILE; return;\n    case ' ':\n    case '\\t': break;\n    case '#':\n      while ((ch = get_char (ctx)) != '\\n' && ch != EOF)\n        ;\n      /* falls through */\n    case '\\n': token->code = TC_NL; return;\n    case '(': token->code = TC_LEFT_PAR; return;\n    case ')': token->code = TC_RIGHT_PAR; return;\n    case ',': token->code = TC_COMMA; return;\n    case ';': token->code = TC_SEMICOL; return;\n    case ':': token->code = TC_COL; return;\n    case '\"': scan_string (ctx, token, ch, get_char, unget_char); return;\n    default:\n      VARR_TRUNC (char, temp_string, 0);\n      if (_MIR_name_char_p (ctx, ch, TRUE)) {\n        do {\n          VARR_PUSH (char, temp_string, ch);\n          ch = get_char (ctx);\n        } while (_MIR_name_char_p (ctx, ch, FALSE));\n        VARR_PUSH (char, temp_string, '\\0');\n        unget_char (ctx, ch);\n        token->u.name = _MIR_uniq_string (ctx, VARR_ADDR (char, temp_string));\n        token->code = TC_NAME;\n        return;\n      } else if (ch == '+' || ch == '-' || isdigit (ch)) {\n        const char *repr;\n        char *end;\n        int next_ch, base, float_p, double_p, ldouble_p;\n\n        if (ch == '+' || ch == '-') {\n          next_ch = get_char (ctx);\n          if (!isdigit (next_ch)) scan_error (ctx, \"no number after a sign %c\", ch);\n          unget_char (ctx, next_ch);\n        }\n        scan_number (ctx, ch, get_char, unget_char, &base, &float_p, &double_p, &ldouble_p);\n        repr = VARR_ADDR (char, temp_string);\n        errno = 0;\n        if (float_p) {\n          token->code = TC_FLOAT;\n          token->u.f = strtof (repr, &end);\n        } else if (double_p) {\n          token->code = TC_DOUBLE;\n          token->u.d = strtod (repr, &end);\n        } else if (ldouble_p) {\n          token->code = TC_LDOUBLE;\n          token->u.ld = strtold (repr, &end);\n        } else {\n          token->code = TC_INT;\n          token->u.i = (sizeof (long) == sizeof (int64_t) ? strtoul (repr, &end, base)\n                                                          : strtoull (repr, &end, base));\n        }\n        mir_assert (*end == '\\0');\n        if (errno != 0) {\n        }\n        return;\n      } else {\n        VARR_PUSH (char, temp_string, '\\0');\n        scan_error (ctx, \"wrong char after %s\", VARR_ADDR (char, temp_string));\n      }\n    }\n  }\n}\n\nstatic int label_eq (label_desc_t l1, label_desc_t l2, void *arg MIR_UNUSED) {\n  return strcmp (l1.name, l2.name) == 0;\n}\nstatic htab_hash_t label_hash (label_desc_t l, void *arg MIR_UNUSED) {\n  return (htab_hash_t) mir_hash (l.name, strlen (l.name), 0);\n}\n\nstatic MIR_label_t create_label_desc (MIR_context_t ctx, const char *name, int def_p) {\n  MIR_label_t label;\n  label_desc_t label_desc;\n\n  label_desc.name = name;\n  if (HTAB_DO (label_desc_t, label_desc_tab, label_desc, HTAB_FIND, label_desc)) {\n    if (def_p) {\n      if (label_desc.def_p) scan_error (ctx, \"redefinition of label %s in a module\", name);\n      label_desc.def_p = TRUE;\n      HTAB_DO (label_desc_t, label_desc_tab, label_desc, HTAB_REPLACE, label_desc);\n    }\n    label = label_desc.label;\n  } else {\n    label_desc.label = label = MIR_new_label (ctx);\n    label_desc.def_p = def_p;\n    HTAB_DO (label_desc_t, label_desc_tab, label_desc, HTAB_INSERT, label_desc);\n  }\n  return label;\n}\n\nstatic int func_reg_p (MIR_context_t ctx MIR_UNUSED, MIR_func_t func, const char *name) {\n  func_regs_t func_regs = func->internal;\n  size_t rdn, tab_rdn;\n  reg_desc_t rd;\n  int res;\n\n  rd.name = (char *) name;\n  rdn = VARR_LENGTH (reg_desc_t, func_regs->reg_descs);\n  VARR_PUSH (reg_desc_t, func_regs->reg_descs, rd);\n  res = HTAB_DO (size_t, func_regs->name2rdn_tab, rdn, HTAB_FIND, tab_rdn);\n  VARR_POP (reg_desc_t, func_regs->reg_descs);\n  return res;\n}\n\nstatic void read_func_proto (MIR_context_t ctx, size_t nops, MIR_op_t *ops) {\n  MIR_var_t var;\n  size_t i;\n\n  VARR_TRUNC (MIR_type_t, scan_types, 0);\n  for (i = 0; i < nops; i++) {\n    var.name = (const char *) ops[i].u.mem.disp;\n    if ((var.name = (const char *) ops[i].u.mem.disp) != NULL) break;\n    var.type = ops[i].u.mem.type;\n    VARR_PUSH (MIR_type_t, scan_types, var.type);\n  }\n  VARR_TRUNC (MIR_var_t, scan_vars, 0);\n  for (; i < nops; i++) {\n    if (ops[i].mode != MIR_OP_MEM) scan_error (ctx, \"wrong prototype/func arg\");\n    var.type = ops[i].u.mem.type;\n    var.name = (const char *) ops[i].u.mem.disp;\n    if (var.name == NULL)\n      scan_error (ctx, \"all func/prototype args should have form type:name or (r)blk:size(name)\");\n    if (MIR_all_blk_type_p (var.type)) var.size = ops[i].u.mem.base;\n    VARR_PUSH (MIR_var_t, scan_vars, var);\n  }\n}\n\nstatic MIR_type_t str2type (const char *type_name) {\n  if (strcmp (type_name, \"i64\") == 0) return MIR_T_I64;\n  if (strcmp (type_name, \"u64\") == 0) return MIR_T_U64;\n  if (strcmp (type_name, \"f\") == 0) return MIR_T_F;\n  if (strcmp (type_name, \"d\") == 0) return MIR_T_D;\n  if (strcmp (type_name, \"ld\") == 0) return MIR_T_LD;\n  if (strcmp (type_name, \"p\") == 0) return MIR_T_P;\n  if (strcmp (type_name, \"i32\") == 0) return MIR_T_I32;\n  if (strcmp (type_name, \"u32\") == 0) return MIR_T_U32;\n  if (strcmp (type_name, \"i16\") == 0) return MIR_T_I16;\n  if (strcmp (type_name, \"u16\") == 0) return MIR_T_U16;\n  if (strcmp (type_name, \"i8\") == 0) return MIR_T_I8;\n  if (strcmp (type_name, \"u8\") == 0) return MIR_T_U8;\n  if (strncmp (type_name, \"blk\", 3) == 0) {\n    int i, n = 0;\n    for (i = 3; isdigit (type_name[i]) && n < MIR_BLK_NUM; i++) n = n * 10 + (type_name[i] - '0');\n    if (type_name[i] == 0 && n < MIR_BLK_NUM) return MIR_T_BLK + n;\n  }\n  if (strcmp (type_name, \"rblk\") == 0) return MIR_T_RBLK;\n  return MIR_T_BOUND;\n}\n\n/* Syntax:\n     program: { insn / sep }\n     sep : ';' | NL\n     insn : {label ':'}* [ code [ {op / ','} ] ]\n     label : name\n     code : name\n     op : name | int | float | double | long double | mem | str\n     mem : type ':' addr aliases\n     addr : disp\n          | [ disp ] '(' sib ')'\n     sib : name | [ name ] ',' name [ ',' scale]\n     disp : int | name\n     scale : int\n     aliases :  [':' [name] [':' name] ]\n*/\n\nvoid MIR_scan_string (MIR_context_t ctx, const char *str) {\n  token_t t;\n  const char *name;\n  MIR_module_t module = NULL;\n  MIR_item_t item, func = NULL;\n  MIR_insn_code_t insn_code = MIR_INSN_BOUND; /* for removing uninitialized warning */\n  MIR_insn_t insn;\n  MIR_type_t type, data_type = MIR_T_BOUND;\n  MIR_op_t op, *op_addr;\n  MIR_label_t label;\n  int64_t i, n;\n  int module_p, end_module_p, proto_p, func_p, end_func_p, dots_p, export_p, import_p, forward_p;\n  int bss_p, ref_p, lref_p, expr_p, string_p, global_p, local_p, push_op_p, read_p, disp_p;\n  insn_name_t in, el;\n\n  VARR_TRUNC (char, error_msg_buf, 0);\n  curr_lno = 1;\n  input_string = str;\n  input_string_char_num = 0;\n  t.code = TC_NL;\n  for (;;) {\n    if (setjmp (error_jmp_buf)) {\n      while (t.code != TC_NL && t.code != TC_EOFILE)\n        scan_token (ctx, &t, get_string_char, unget_string_char);\n      if (t.code == TC_EOFILE) break;\n    }\n    VARR_TRUNC (label_name_t, label_names, 0);\n    scan_token (ctx, &t, get_string_char, unget_string_char);\n    while (t.code == TC_NL) scan_token (ctx, &t, get_string_char, unget_string_char);\n    if (t.code == TC_EOFILE) break;\n    for (;;) { /* label_names */\n      if (t.code != TC_NAME) scan_error (ctx, \"insn should start with label or insn name\");\n      name = t.u.name;\n      scan_token (ctx, &t, get_string_char, unget_string_char);\n      if (t.code != TC_COL) break;\n      VARR_PUSH (label_name_t, label_names, name);\n      if (module != NULL)\n        process_reserved_name (name, TEMP_ITEM_NAME_PREFIX, &module->last_temp_item_num);\n      scan_token (ctx, &t, get_string_char, unget_string_char);\n      if (t.code == TC_NL)\n        scan_token (ctx, &t, get_string_char, unget_string_char); /* label_names without insn */\n    }\n    module_p = end_module_p = proto_p = func_p = end_func_p = FALSE;\n    export_p = import_p = forward_p = bss_p = ref_p = lref_p = expr_p = string_p = FALSE;\n    global_p = local_p = FALSE;\n    if (strcmp (name, \"module\") == 0) {\n      module_p = TRUE;\n      if (VARR_LENGTH (label_name_t, label_names) != 1)\n        scan_error (ctx, \"only one label should be used for module\");\n    } else if (strcmp (name, \"endmodule\") == 0) {\n      end_module_p = TRUE;\n      if (VARR_LENGTH (label_name_t, label_names) != 0)\n        scan_error (ctx, \"endmodule should have no labels\");\n    } else if (strcmp (name, \"proto\") == 0) {\n      proto_p = TRUE;\n      if (VARR_LENGTH (label_name_t, label_names) != 1)\n        scan_error (ctx, \"only one label should be used for proto\");\n    } else if (strcmp (name, \"func\") == 0) {\n      func_p = TRUE;\n      if (VARR_LENGTH (label_name_t, label_names) != 1)\n        scan_error (ctx, \"only one label should be used for func\");\n    } else if (strcmp (name, \"endfunc\") == 0) {\n      end_func_p = TRUE;\n      if (VARR_LENGTH (label_name_t, label_names) != 0)\n        scan_error (ctx, \"endfunc should have no labels\");\n    } else if (strcmp (name, \"export\") == 0) {\n      export_p = TRUE;\n      if (VARR_LENGTH (label_name_t, label_names) != 0)\n        scan_error (ctx, \"export should have no labels\");\n    } else if (strcmp (name, \"import\") == 0) {\n      import_p = TRUE;\n      if (VARR_LENGTH (label_name_t, label_names) != 0)\n        scan_error (ctx, \"import should have no labels\");\n    } else if (strcmp (name, \"forward\") == 0) {\n      forward_p = TRUE;\n      if (VARR_LENGTH (label_name_t, label_names) != 0)\n        scan_error (ctx, \"forward should have no labels\");\n    } else if (strcmp (name, \"bss\") == 0) {\n      bss_p = TRUE;\n      if (VARR_LENGTH (label_name_t, label_names) > 1)\n        scan_error (ctx, \"at most one label should be used for bss\");\n    } else if (strcmp (name, \"ref\") == 0) {\n      ref_p = TRUE;\n      if (VARR_LENGTH (label_name_t, label_names) > 1)\n        scan_error (ctx, \"at most one label should be used for ref\");\n    } else if (strcmp (name, \"lref\") == 0) {\n      lref_p = TRUE;\n      if (VARR_LENGTH (label_name_t, label_names) > 1)\n        scan_error (ctx, \"at most one label should be used for lref\");\n    } else if (strcmp (name, \"expr\") == 0) {\n      expr_p = TRUE;\n      if (VARR_LENGTH (label_name_t, label_names) > 1)\n        scan_error (ctx, \"at most one label should be used for expr\");\n    } else if (strcmp (name, \"string\") == 0) {\n      string_p = TRUE;\n      if (VARR_LENGTH (label_name_t, label_names) > 1)\n        scan_error (ctx, \"at most one label should be used for string\");\n    } else if ((local_p = strcmp (name, \"local\") == 0)\n               || (global_p = strcmp (name, \"global\") == 0)) {\n      if (func == NULL) scan_error (ctx, \"local/global outside func\");\n      if (VARR_LENGTH (label_name_t, label_names) != 0)\n        scan_error (ctx, \"local/global should have no labels\");\n    } else if ((data_type = str2type (name)) != MIR_T_BOUND) {\n      if (VARR_LENGTH (label_name_t, label_names) > 1)\n        scan_error (ctx, \"at most one label should be used for data\");\n    } else {\n      in.name = name;\n      if (!HTAB_DO (insn_name_t, insn_name_tab, in, HTAB_FIND, el))\n        scan_error (ctx, \"Unknown insn %s\", name);\n      insn_code = el.code;\n      if (insn_code == MIR_UNSPEC || insn_code == MIR_USE || insn_code == MIR_PHI)\n        scan_error (ctx, \"UNSPEC, USE, or PHI is not portable and can not be scanned\", name);\n      for (n = 0; n < (int64_t) VARR_LENGTH (label_name_t, label_names); n++) {\n        label = create_label_desc (ctx, VARR_GET (label_name_t, label_names, n), TRUE);\n        if (func != NULL) MIR_append_insn (ctx, func, label);\n      }\n    }\n    VARR_TRUNC (MIR_op_t, scan_insn_ops, 0);\n    dots_p = FALSE;\n    for (;;) { /* ops */\n      if (t.code == TC_NL || t.code == TC_SEMICOL) {\n        /* insn end */\n        break;\n      }\n      push_op_p = read_p = TRUE;\n      switch (t.code) {\n      case TC_NAME: {\n        name = t.u.name;\n        scan_token (ctx, &t, get_string_char, unget_string_char);\n        if ((func_p || proto_p) && strcmp (name, \"...\") == 0) {\n          dots_p = TRUE;\n          break;\n        }\n        read_p = FALSE;\n        if (t.code != TC_COL && !proto_p && !func_p && !local_p && !global_p) {\n          if (export_p) {\n            MIR_new_export (ctx, name);\n            push_op_p = FALSE;\n          } else if (import_p) {\n            MIR_new_import (ctx, name);\n            push_op_p = FALSE;\n          } else if (forward_p) {\n            MIR_new_forward (ctx, name);\n            push_op_p = FALSE;\n          } else if (lref_p) {\n            op = MIR_new_label_op (ctx, create_label_desc (ctx, name, FALSE));\n          } else if (!module_p && !end_module_p && !end_func_p\n                     && (((MIR_branch_code_p (insn_code) || insn_code == MIR_PRBEQ\n                           || insn_code == MIR_PRBNE)\n                          && VARR_LENGTH (MIR_op_t, scan_insn_ops) == 0)\n                         || (insn_code == MIR_LADDR && VARR_LENGTH (MIR_op_t, scan_insn_ops) == 1)\n                         || (insn_code == MIR_SWITCH\n                             && VARR_LENGTH (MIR_op_t, scan_insn_ops) > 0))) {\n            op = MIR_new_label_op (ctx, create_label_desc (ctx, name, FALSE));\n          } else if (!expr_p && !ref_p && func != NULL && func_reg_p (ctx, func->u.func, name)) {\n            op.mode = MIR_OP_REG;\n            op.u.reg = MIR_reg (ctx, name, func->u.func);\n          } else if ((item = item_tab_find (ctx, name, module)) != NULL) {\n            op = MIR_new_ref_op (ctx, item);\n          } else {\n            scan_error (ctx, \"undeclared name %s\", name);\n          }\n          break;\n        } /* Memory, type only, arg, or var */\n        type = str2type (name);\n        if (type == MIR_T_BOUND)\n          scan_error (ctx, \"Unknown type %s\", name);\n        else if ((global_p || local_p) && type != MIR_T_I64 && type != MIR_T_F && type != MIR_T_D\n                 && type != MIR_T_LD)\n          scan_error (ctx, \"wrong type %s for local/global var\", name);\n        op = MIR_new_mem_op (ctx, type, 0, 0, 0, 1);\n        if (proto_p || func_p || global_p || local_p) {\n          if (t.code == TC_COL) {\n            scan_token (ctx, &t, get_string_char, unget_string_char);\n            if (t.code == TC_NAME) {\n              op.u.mem.disp = (MIR_disp_t) t.u.name;\n              scan_token (ctx, &t, get_string_char, unget_string_char);\n              if (!global_p) {\n              } else if (t.code != TC_COL) {\n                scan_error (ctx, \"global %s without hard register\", (const char *) op.u.mem.disp);\n              } else {\n                scan_token (ctx, &t, get_string_char, unget_string_char);\n                if (t.code != TC_NAME) {\n                  scan_error (ctx, \"hard register for %s is not a name\", (char *) op.data);\n                } else {\n                  op.data = (void *) t.u.name;\n                  scan_token (ctx, &t, get_string_char, unget_string_char);\n                }\n              }\n            } else if (global_p || local_p || t.code != TC_INT || !MIR_all_blk_type_p (type)) {\n              scan_error (ctx, local_p ? \"wrong var\" : \"wrong arg\");\n              scan_token (ctx, &t, get_string_char, unget_string_char);\n            } else {\n              op.u.mem.base = (MIR_reg_t) t.u.i;\n              if (t.u.i < 0 || t.u.i >= (1ll << sizeof (MIR_reg_t) * 8))\n                scan_error (ctx, \"invalid block arg size\");\n              scan_token (ctx, &t, get_string_char, unget_string_char);\n              if (t.code != TC_LEFT_PAR) scan_error (ctx, \"wrong block arg\");\n              scan_token (ctx, &t, get_string_char, unget_string_char);\n              if (t.code != TC_NAME) scan_error (ctx, \"wrong block arg\");\n              op.u.mem.disp = (MIR_disp_t) t.u.name;\n              scan_token (ctx, &t, get_string_char, unget_string_char);\n              if (t.code != TC_RIGHT_PAR) scan_error (ctx, \"wrong block arg\");\n              scan_token (ctx, &t, get_string_char, unget_string_char);\n            }\n          }\n        } else {\n          scan_token (ctx, &t, get_string_char, unget_string_char);\n          disp_p = FALSE;\n          if (t.code == TC_INT) {\n            op.u.mem.disp = t.u.i;\n            scan_token (ctx, &t, get_string_char, unget_string_char);\n            disp_p = TRUE;\n          } else if (t.code == TC_NAME) {\n            op.u.mem.disp = (MIR_disp_t) t.u.name;\n            scan_token (ctx, &t, get_string_char, unget_string_char);\n            disp_p = TRUE;\n          }\n          if (t.code == TC_LEFT_PAR) {\n            scan_token (ctx, &t, get_string_char, unget_string_char);\n            if (t.code == TC_NAME) {\n              op.u.mem.base = MIR_reg (ctx, t.u.name, func->u.func);\n              scan_token (ctx, &t, get_string_char, unget_string_char);\n            }\n            if (t.code == TC_COMMA) {\n              scan_token (ctx, &t, get_string_char, unget_string_char);\n              if (t.code != TC_NAME) scan_error (ctx, \"wrong index\");\n              op.u.mem.index = MIR_reg (ctx, t.u.name, func->u.func);\n              scan_token (ctx, &t, get_string_char, unget_string_char);\n              if (t.code == TC_COMMA) {\n                scan_token (ctx, &t, get_string_char, unget_string_char);\n                if (t.code != TC_INT) scan_error (ctx, \"wrong scale\");\n                op.u.mem.scale = (MIR_scale_t) t.u.i;\n                scan_token (ctx, &t, get_string_char, unget_string_char);\n              }\n            }\n            if (t.code != TC_RIGHT_PAR) scan_error (ctx, \"wrong memory op\");\n            scan_token (ctx, &t, get_string_char, unget_string_char);\n          } else if (!disp_p) {\n            scan_error (ctx, \"wrong memory\");\n          }\n          if (t.code == TC_COL) {\n            scan_token (ctx, &t, get_string_char, unget_string_char);\n            if (t.code == TC_COL) {\n              op.u.mem.alias = 0;\n              scan_token (ctx, &t, get_string_char, unget_string_char);\n              if (t.code != TC_NAME) {\n                scan_error (ctx, \"empty nonalias name\");\n              } else {\n                op.u.mem.nonalias = MIR_alias (ctx, t.u.name);\n                scan_token (ctx, &t, get_string_char, unget_string_char);\n              }\n            } else if (t.code != TC_NAME) {\n              scan_error (ctx, \"wrong alias name\");\n            } else {\n              op.u.mem.alias = MIR_alias (ctx, t.u.name);\n              scan_token (ctx, &t, get_string_char, unget_string_char);\n              if (t.code == TC_COL) {\n                scan_token (ctx, &t, get_string_char, unget_string_char);\n                if (t.code != TC_NAME) {\n                  scan_error (ctx, \"empty nonalias name\");\n                } else {\n                  op.u.mem.nonalias = MIR_alias (ctx, t.u.name);\n                  scan_token (ctx, &t, get_string_char, unget_string_char);\n                }\n              }\n            }\n          }\n        }\n        break;\n      }\n      case TC_INT:\n        op.mode = MIR_OP_INT;\n        op.u.i = t.u.i;\n        break;\n      case TC_FLOAT:\n        op.mode = MIR_OP_FLOAT;\n        op.u.f = t.u.f;\n        break;\n      case TC_LDOUBLE: op.mode = MIR_OP_LDOUBLE; op.u.ld = t.u.ld;\n#if !defined(_WIN32) && __SIZEOF_LONG_DOUBLE__ != 8\n        break;\n#endif\n      case TC_DOUBLE:\n        op.mode = MIR_OP_DOUBLE;\n        op.u.d = t.u.d;\n        break;\n      case TC_STR:\n        op.mode = MIR_OP_STR;\n        op.u.str = t.u.str;\n        break;\n      default: break;\n      }\n      if (dots_p) break;\n      if (push_op_p) {\n        VARR_PUSH (MIR_op_t, scan_insn_ops, op);\n        op.data = NULL; /* reset value set up for global */\n      }\n      if (read_p) scan_token (ctx, &t, get_string_char, unget_string_char);\n      if (t.code != TC_COMMA) break;\n      scan_token (ctx, &t, get_string_char, unget_string_char);\n    }\n    if (t.code != TC_NL && t.code != TC_EOFILE && t.code != TC_SEMICOL)\n      scan_error (ctx, \"wrong insn end\");\n    if (module_p) {\n      if (module != NULL) scan_error (ctx, \"nested module\");\n      if (VARR_LENGTH (MIR_op_t, scan_insn_ops) != 0)\n        scan_error (ctx, \"module should have no params\");\n      module = MIR_new_module (ctx, VARR_GET (label_name_t, label_names, 0));\n      HTAB_CLEAR (label_desc_t, label_desc_tab);\n    } else if (end_module_p) {\n      if (module == NULL) scan_error (ctx, \"standalone endmodule\");\n      if (VARR_LENGTH (MIR_op_t, scan_insn_ops) != 0)\n        scan_error (ctx, \"endmodule should have no params\");\n      MIR_finish_module (ctx);\n      module = NULL;\n    } else if (bss_p) {\n      if (VARR_LENGTH (MIR_op_t, scan_insn_ops) != 1)\n        scan_error (ctx, \"bss should have one operand\");\n      op_addr = VARR_ADDR (MIR_op_t, scan_insn_ops);\n      if (op_addr[0].mode != MIR_OP_INT || op_addr[0].u.i < 0)\n        scan_error (ctx, \"wrong bss operand type or value\");\n      name\n        = (VARR_LENGTH (label_name_t, label_names) == 0 ? NULL\n                                                        : VARR_GET (label_name_t, label_names, 0));\n      MIR_new_bss (ctx, name, op_addr[0].u.i);\n    } else if (ref_p) {\n      if (VARR_LENGTH (MIR_op_t, scan_insn_ops) != 2)\n        scan_error (ctx, \"ref should have two operands\");\n      op_addr = VARR_ADDR (MIR_op_t, scan_insn_ops);\n      if (op_addr[0].mode != MIR_OP_REF) scan_error (ctx, \"wrong ref operand\");\n      if (op_addr[1].mode != MIR_OP_INT) scan_error (ctx, \"wrong ref disp operand\");\n      name\n        = (VARR_LENGTH (label_name_t, label_names) == 0 ? NULL\n                                                        : VARR_GET (label_name_t, label_names, 0));\n      MIR_new_ref_data (ctx, name, op_addr[0].u.ref, op_addr[1].u.i);\n    } else if (lref_p) {\n      size_t len = VARR_LENGTH (MIR_op_t, scan_insn_ops);\n      MIR_label_t lab = NULL, lab2 = NULL;\n      int64_t disp = 0;\n      if (len == 0 || len > 3)\n        scan_error (ctx, \"lref should have at least one but at most three operands\");\n      op_addr = VARR_ADDR (MIR_op_t, scan_insn_ops);\n      if (op_addr[0].mode != MIR_OP_LABEL) scan_error (ctx, \"1st lref operand is not a label\");\n      lab = op_addr[0].u.label;\n      if (len == 2) {\n        if (op_addr[1].mode != MIR_OP_LABEL && op_addr[1].mode != MIR_OP_INT)\n          scan_error (ctx, \"2nd lref operand is not a label or displacement\");\n        if (op_addr[1].mode == MIR_OP_LABEL) lab2 = op_addr[1].u.label;\n        if (op_addr[1].mode == MIR_OP_INT) disp = op_addr[1].u.i;\n      } else if (len == 3) {\n        if (op_addr[1].mode != MIR_OP_LABEL) scan_error (ctx, \"2nd lref operand is not a label\");\n        if (op_addr[2].mode != MIR_OP_INT)\n          scan_error (ctx, \"3rd lref operand is not a displacement\");\n        lab2 = op_addr[1].u.label;\n        disp = op_addr[2].u.i;\n      }\n      name\n        = (VARR_LENGTH (label_name_t, label_names) == 0 ? NULL\n                                                        : VARR_GET (label_name_t, label_names, 0));\n      MIR_new_lref_data (ctx, name, lab, lab2, disp);\n    } else if (expr_p) {\n      if (VARR_LENGTH (MIR_op_t, scan_insn_ops) != 1)\n        scan_error (ctx, \"expr should have one operand\");\n      op_addr = VARR_ADDR (MIR_op_t, scan_insn_ops);\n      if (op_addr[0].mode != MIR_OP_REF || op_addr[0].u.ref->item_type != MIR_func_item)\n        scan_error (ctx, \"wrong expr operand\");\n      name\n        = (VARR_LENGTH (label_name_t, label_names) == 0 ? NULL\n                                                        : VARR_GET (label_name_t, label_names, 0));\n      MIR_new_expr_data (ctx, name, op_addr[0].u.ref);\n    } else if (string_p) {\n      if (VARR_LENGTH (MIR_op_t, scan_insn_ops) != 1)\n        scan_error (ctx, \"string should have one operand\");\n      op_addr = VARR_ADDR (MIR_op_t, scan_insn_ops);\n      if (op_addr[0].mode != MIR_OP_STR) scan_error (ctx, \"wrong string data operand type\");\n      name\n        = (VARR_LENGTH (label_name_t, label_names) == 0 ? NULL\n                                                        : VARR_GET (label_name_t, label_names, 0));\n      MIR_new_string_data (ctx, name, op_addr[0].u.str);\n    } else if (proto_p) {\n      if (module == NULL) scan_error (ctx, \"prototype outside module\");\n      read_func_proto (ctx, VARR_LENGTH (MIR_op_t, scan_insn_ops),\n                       VARR_ADDR (MIR_op_t, scan_insn_ops));\n      if (dots_p)\n        MIR_new_vararg_proto_arr (ctx, VARR_GET (label_name_t, label_names, 0),\n                                  VARR_LENGTH (MIR_type_t, scan_types),\n                                  VARR_ADDR (MIR_type_t, scan_types),\n                                  VARR_LENGTH (MIR_var_t, scan_vars),\n                                  VARR_ADDR (MIR_var_t, scan_vars));\n      else\n        MIR_new_proto_arr (ctx, VARR_GET (label_name_t, label_names, 0),\n                           VARR_LENGTH (MIR_type_t, scan_types), VARR_ADDR (MIR_type_t, scan_types),\n                           VARR_LENGTH (MIR_var_t, scan_vars), VARR_ADDR (MIR_var_t, scan_vars));\n    } else if (func_p) {\n      if (module == NULL) scan_error (ctx, \"func outside module\");\n      if (func != NULL) scan_error (ctx, \"nested func\");\n      read_func_proto (ctx, VARR_LENGTH (MIR_op_t, scan_insn_ops),\n                       VARR_ADDR (MIR_op_t, scan_insn_ops));\n      if (dots_p)\n        func = MIR_new_vararg_func_arr (ctx, VARR_GET (label_name_t, label_names, 0),\n                                        VARR_LENGTH (MIR_type_t, scan_types),\n                                        VARR_ADDR (MIR_type_t, scan_types),\n                                        VARR_LENGTH (MIR_var_t, scan_vars),\n                                        VARR_ADDR (MIR_var_t, scan_vars));\n      else\n        func\n          = MIR_new_func_arr (ctx, VARR_GET (label_name_t, label_names, 0),\n                              VARR_LENGTH (MIR_type_t, scan_types),\n                              VARR_ADDR (MIR_type_t, scan_types),\n                              VARR_LENGTH (MIR_var_t, scan_vars), VARR_ADDR (MIR_var_t, scan_vars));\n    } else if (end_func_p) {\n      if (func == NULL) scan_error (ctx, \"standalone endfunc\");\n      if (VARR_LENGTH (MIR_op_t, scan_insn_ops) != 0)\n        scan_error (ctx, \"endfunc should have no params\");\n      func = NULL;\n      MIR_finish_func (ctx);\n    } else if (export_p || import_p || forward_p) { /* we already created items, now do nothing: */\n      mir_assert (VARR_LENGTH (MIR_op_t, scan_insn_ops) == 0);\n    } else if (global_p || local_p) {\n      op_addr = VARR_ADDR (MIR_op_t, scan_insn_ops);\n      n = (int64_t) VARR_LENGTH (MIR_op_t, scan_insn_ops);\n      for (i = 0; i < n; i++) {\n        if (op_addr[i].mode != MIR_OP_MEM || (const char *) op_addr[i].u.mem.disp == NULL)\n          scan_error (ctx, \"wrong local/global var\");\n        if (op_addr[i].data == NULL) {\n          MIR_new_func_reg (ctx, func->u.func, op_addr[i].u.mem.type,\n                            (const char *) op_addr[i].u.mem.disp);\n        } else {\n          MIR_new_global_func_reg (ctx, func->u.func, op_addr[i].u.mem.type,\n                                   (const char *) op_addr[i].u.mem.disp,\n                                   (const char *) op_addr[i].data);\n        }\n      }\n    } else if (data_type != MIR_T_BOUND) {\n      union {\n        uint8_t u8;\n        uint16_t u16;\n        uint32_t u32;\n        uint64_t u64;\n        int8_t i8;\n        int16_t i16;\n        int32_t i32;\n        int64_t i64;\n      } v;\n\n      n = (int64_t) VARR_LENGTH (MIR_op_t, scan_insn_ops);\n      op_addr = VARR_ADDR (MIR_op_t, scan_insn_ops);\n      VARR_TRUNC (uint8_t, temp_data, 0);\n      for (i = 0; i < n; i++) {\n        if (op_addr[i].mode != type2mode (data_type))\n          scan_error (ctx, \"data operand is not of data type\");\n        switch (data_type) {\n        case MIR_T_I8:\n          v.i8 = (int8_t) op_addr[i].u.i;\n          push_data (ctx, (uint8_t *) &v.i8, sizeof (int8_t));\n          break;\n        case MIR_T_U8:\n          v.u8 = (uint8_t) op_addr[i].u.u;\n          push_data (ctx, (uint8_t *) &v.u8, sizeof (uint8_t));\n          break;\n        case MIR_T_I16:\n          v.i16 = (int16_t) op_addr[i].u.i;\n          push_data (ctx, (uint8_t *) &v.i16, sizeof (int16_t));\n          break;\n        case MIR_T_U16:\n          v.u16 = (uint16_t) op_addr[i].u.u;\n          push_data (ctx, (uint8_t *) &v.u16, sizeof (uint16_t));\n          break;\n        case MIR_T_I32:\n          v.i32 = (int32_t) op_addr[i].u.i;\n          push_data (ctx, (uint8_t *) &v.i32, sizeof (int32_t));\n          break;\n        case MIR_T_U32:\n          v.u32 = (uint32_t) op_addr[i].u.u;\n          push_data (ctx, (uint8_t *) &v.u32, sizeof (uint32_t));\n          break;\n        case MIR_T_I64:\n          v.i64 = op_addr[i].u.i;\n          push_data (ctx, (uint8_t *) &v.i64, sizeof (int64_t));\n          break;\n        case MIR_T_U64:\n          v.u64 = op_addr[i].u.u;\n          push_data (ctx, (uint8_t *) &v.u64, sizeof (uint64_t));\n          break;\n        case MIR_T_F: push_data (ctx, (uint8_t *) &op_addr[i].u.f, sizeof (float)); break;\n        case MIR_T_D: push_data (ctx, (uint8_t *) &op_addr[i].u.d, sizeof (double)); break;\n        case MIR_T_LD:\n          push_data (ctx, (uint8_t *) &op_addr[i].u.ld, sizeof (long double));\n          break;\n          /* ptr ??? */\n        default: scan_error (ctx, \"wrong data clause\");\n        }\n      }\n      name\n        = (VARR_LENGTH (label_name_t, label_names) == 0 ? NULL\n                                                        : VARR_GET (label_name_t, label_names, 0));\n      MIR_new_data (ctx, name, data_type,\n                    VARR_LENGTH (uint8_t, temp_data) / _MIR_type_size (ctx, data_type),\n                    VARR_ADDR (uint8_t, temp_data));\n    } else {\n      insn = MIR_new_insn_arr (ctx, insn_code, VARR_LENGTH (MIR_op_t, scan_insn_ops),\n                               VARR_ADDR (MIR_op_t, scan_insn_ops));\n      if (func != NULL) MIR_append_insn (ctx, func, insn);\n    }\n  }\n  if (func != NULL) {\n    if (!setjmp (error_jmp_buf)) scan_error (ctx, \"absent endfunc\");\n  }\n  if (module != NULL) {\n    if (!setjmp (error_jmp_buf)) scan_error (ctx, \"absent endmodule\");\n  }\n  if (VARR_LENGTH (char, error_msg_buf) != 0)\n    MIR_get_error_func (ctx) (MIR_syntax_error, VARR_ADDR (char, error_msg_buf));\n}\n\nstatic void scan_init (MIR_context_t ctx) {\n  insn_name_t in, el;\n  size_t i;\n\n  if ((ctx->scan_ctx = MIR_malloc (ctx->alloc, sizeof (struct scan_ctx))) == NULL)\n    MIR_get_error_func (ctx) (MIR_alloc_error, \"Not enough memory for ctx\");\n  VARR_CREATE (char, error_msg_buf, ctx->alloc, 0);\n  VARR_CREATE (MIR_var_t, scan_vars, ctx->alloc, 0);\n  VARR_CREATE (MIR_type_t, scan_types, ctx->alloc, 0);\n  VARR_CREATE (MIR_op_t, scan_insn_ops, ctx->alloc, 0);\n  VARR_CREATE (label_name_t, label_names, ctx->alloc, 0);\n  HTAB_CREATE (label_desc_t, label_desc_tab, ctx->alloc, 100, label_hash, label_eq, NULL);\n  HTAB_CREATE (insn_name_t, insn_name_tab, ctx->alloc, MIR_INSN_BOUND, insn_name_hash, insn_name_eq, NULL);\n  for (i = 0; i < MIR_INSN_BOUND; i++) {\n    in.code = i;\n    in.name = MIR_insn_name (ctx, i);\n    HTAB_DO (insn_name_t, insn_name_tab, in, HTAB_INSERT, el);\n  }\n}\n\nstatic void scan_finish (MIR_context_t ctx) {\n  VARR_DESTROY (char, error_msg_buf);\n  VARR_DESTROY (MIR_var_t, scan_vars);\n  VARR_DESTROY (MIR_type_t, scan_types);\n  VARR_DESTROY (MIR_op_t, scan_insn_ops);\n  VARR_DESTROY (label_name_t, label_names);\n  HTAB_DESTROY (label_desc_t, label_desc_tab);\n  HTAB_DESTROY (insn_name_t, insn_name_tab);\n  MIR_free (ctx->alloc, ctx->scan_ctx);\n  ctx->scan_ctx = NULL;\n}\n\n#endif /* if !MIR_NO_SCAN */\n\n/* New Page */\n\n#ifndef _WIN32\n#include <sys/types.h>\n#include <unistd.h>\n#else\n#define WIN32_LEAN_AND_MEAN\n#include <windows.h>\n#define getpid GetCurrentProcessId\n#define popen _popen\n#define pclose _pclose\n#endif\n\nvoid _MIR_dump_code (const char *name, uint8_t *code, size_t code_len) {\n  size_t i;\n  int ch;\n  char cfname[50];\n  char command[500];\n  FILE *f;\n#if !defined(__APPLE__)\n  char bfname[30];\n  FILE *bf;\n#endif\n\n  if (name != NULL) fprintf (stderr, \"%s:\", name);\n  sprintf (cfname, \"_mir_%lu.c\", (unsigned long) getpid ());\n  if ((f = fopen (cfname, \"w\")) == NULL) return;\n#if defined(__APPLE__)\n  fprintf (f, \"unsigned char code[] = {\");\n  for (i = 0; i < code_len; i++) {\n    if (i != 0) fprintf (f, \", \");\n    fprintf (f, \"0x%x\", code[i]);\n  }\n  fprintf (f, \"};\\n\");\n  fclose (f);\n#if defined(__aarch64__)\n  sprintf (command, \"gcc -c -o %s.o %s 2>&1 && objdump --section=__data -D %s.o; rm -f %s.o %s\",\n           cfname, cfname, cfname, cfname, cfname);\n#else\n  sprintf (command, \"gcc -c -o %s.o %s 2>&1 && objdump --section=.data -D %s.o; rm -f %s.o %s\",\n           cfname, cfname, cfname, cfname, cfname);\n#endif\n#else\n  sprintf (bfname, \"_mir_%lu.bin\", (unsigned long) getpid ());\n  if ((bf = fopen (bfname, \"wb\")) == NULL) return;\n  fprintf (f, \"void code (void) {}\\n\");\n  for (i = 0; i < code_len; i++) fputc (code[i], bf);\n  fclose (f);\n  fclose (bf);\n  sprintf (command,\n           \"gcc -c -o %s.o %s 2>&1 && objcopy --update-section .text=%s %s.o && objdump \"\n           \"--adjust-vma=0x%llx -d %s.o; rm -f \"\n           \"%s.o %s %s\",\n           cfname, cfname, bfname, cfname, (unsigned long long) code, cfname, cfname, cfname,\n           bfname);\n#endif\n  fprintf (stderr, \"%s\\n\", command);\n  if ((f = popen (command, \"r\")) == NULL) return;\n  while ((ch = fgetc (f)) != EOF) fprintf (stderr, \"%c\", ch);\n  pclose (f);\n}\n\n/* New Page */\n\n#if defined(__x86_64__) || defined(_M_AMD64)\n#include \"mir-x86_64.c\"\n#elif defined(__aarch64__)\n#include \"mir-aarch64.c\"\n#elif defined(__PPC64__)\n#include \"mir-ppc64.c\"\n#elif defined(__s390x__)\n#include \"mir-s390x.c\"\n#elif defined(__riscv)\n#if __riscv_xlen != 64 || __riscv_flen < 64 || !__riscv_float_abi_double || !__riscv_mul \\\n  || !__riscv_div || !__riscv_compressed\n#error \"only 64-bit RISCV supported (at least rv64imafdc)\"\n#endif\n#if __riscv_flen == 128\n#error \"RISCV 128-bit floats (Q set) is not supported\"\n#endif\n#include \"mir-riscv64.c\"\n#else\n#error \"undefined or unsupported generation target\"\n#endif\n\nstatic int var_is_reg_p (MIR_reg_t var) { return var > MAX_HARD_REG; }\nstatic MIR_reg_t var2reg (MIR_reg_t var) {\n  mir_assert (var_is_reg_p (var));\n  return var == MIR_NON_VAR ? 0 : var - MAX_HARD_REG;\n}\n\nstruct hard_reg_desc {\n  const char *name;\n  int num;\n};\ntypedef struct hard_reg_desc hard_reg_desc_t;\n\nDEF_HTAB (hard_reg_desc_t);\n\nstruct hard_reg_ctx {\n  HTAB (hard_reg_desc_t) * hard_reg_desc_tab;\n};\n\n#define hard_reg_desc_tab ctx->hard_reg_ctx->hard_reg_desc_tab\n\nstatic htab_hash_t hard_reg_desc_hash (hard_reg_desc_t desc, void *arg MIR_UNUSED) {\n  return (htab_hash_t) mir_hash (desc.name, strlen (desc.name), 0);\n}\nstatic int hard_reg_desc_eq (hard_reg_desc_t desc1, hard_reg_desc_t desc2, void *arg MIR_UNUSED) {\n  return strcmp (desc1.name, desc2.name) == 0;\n}\n\nstatic void hard_reg_name_init (MIR_context_t ctx) {\n  hard_reg_desc_t desc, tab_desc;\n  int res;\n\n  if ((ctx->hard_reg_ctx = MIR_malloc (ctx->alloc, sizeof (struct hard_reg_ctx))) == NULL)\n    MIR_get_error_func (ctx) (MIR_alloc_error, \"Not enough memory for ctx\");\n  HTAB_CREATE (hard_reg_desc_t, hard_reg_desc_tab, ctx->alloc, 200, hard_reg_desc_hash, hard_reg_desc_eq, NULL);\n  for (size_t i = 0; i * sizeof (char *) < sizeof (target_hard_reg_names); i++) {\n    desc.num = (int) i;\n    desc.name = target_hard_reg_names[i];\n    res = HTAB_DO (hard_reg_desc_t, hard_reg_desc_tab, desc, HTAB_INSERT, tab_desc);\n    mir_assert (!res);\n  }\n}\n\nstatic void hard_reg_name_finish (MIR_context_t ctx) {\n  HTAB_DESTROY (hard_reg_desc_t, hard_reg_desc_tab);\n  MIR_free (ctx->alloc, ctx->hard_reg_ctx);\n  ctx->hard_reg_ctx = NULL;\n}\n\nint _MIR_get_hard_reg (MIR_context_t ctx, const char *hard_reg_name) {\n  hard_reg_desc_t desc, tab_desc;\n\n  desc.name = hard_reg_name;\n  if (!HTAB_DO (hard_reg_desc_t, hard_reg_desc_tab, desc, HTAB_FIND, tab_desc)) return -1;\n  return tab_desc.num;\n}\n\nstatic MIR_UNUSED const char *get_hard_reg_name (MIR_context_t ctx MIR_UNUSED, int hard_reg) {\n  if (hard_reg > MAX_HARD_REG || target_fixed_hard_reg_p (hard_reg)) return NULL;\n  return target_hard_reg_names[hard_reg];\n}\n\nvoid *_MIR_get_module_global_var_hard_regs (MIR_context_t ctx MIR_UNUSED, MIR_module_t module) {\n  return module->data;\n}\n\n/* New Page */\n\n#include \"mir-interp.c\"\n\n/* Local Variables:                */\n/* mode: c                         */\n/* page-delimiter: \"/\\\\* New Page\" */\n/* End:                            */\n"
        },
        {
          "name": "mir.h",
          "type": "blob",
          "size": 32.3193359375,
          "content": "/* This file is a part of MIR project.\n   Copyright (C) 2018-2024 Vladimir Makarov <vmakarov.gcc@gmail.com>.\n*/\n\n#ifndef MIR_H\n\n#define MIR_H\n\n#ifdef __cplusplus\nextern \"C\" {\n#endif\n\n#if defined(_WIN32) && !defined(_WIN64)\n#error \"MIR does not work on 32-bit Windows\"\n#endif\n\n#include <stdio.h>\n#include <stdint.h>\n#include <assert.h>\n#include \"mir-dlist.h\"\n#include \"mir-varr.h\"\n#include \"mir-htab.h\"\n#include \"mir-alloc.h\"\n#include \"mir-code-alloc.h\"\n\n#define MIR_API_VERSION 0.2\n\n#ifdef NDEBUG\nstatic inline int mir_assert (int cond) { return 0 && cond; }\n#else\n#define mir_assert(cond) assert (cond)\n#endif\n\n#define FALSE 0\n#define TRUE 1\n\n/* Redefine MIR_NO_IO or/and MIR_NO_SCAN if you don't need the functionality they provide.  */\n#ifndef MIR_NO_IO\n#define MIR_NO_IO 0\n#endif\n\n#ifndef MIR_NO_SCAN\n#define MIR_NO_SCAN 0\n#endif\n\n#ifdef __GNUC__\n#define MIR_UNUSED __attribute__ ((unused))\n#else\n#define MIR_UNUSED\n#endif\n\n#define REP2(M, a1, a2) M (a1) REP_SEP M (a2)\n#define REP3(M, a1, a2, a3) REP2 (M, a1, a2) REP_SEP M (a3)\n#define REP4(M, a1, a2, a3, a4) REP3 (M, a1, a2, a3) REP_SEP M (a4)\n#define REP5(M, a1, a2, a3, a4, a5) REP4 (M, a1, a2, a3, a4) REP_SEP M (a5)\n#define REP6(M, a1, a2, a3, a4, a5, a6) REP5 (M, a1, a2, a3, a4, a5) REP_SEP M (a6)\n#define REP7(M, a1, a2, a3, a4, a5, a6, a7) REP6 (M, a1, a2, a3, a4, a5, a6) REP_SEP M (a7)\n#define REP8(M, a1, a2, a3, a4, a5, a6, a7, a8) REP7 (M, a1, a2, a3, a4, a5, a6, a7) REP_SEP M (a8)\n\n#define REP_SEP ,\n\n#define ERR_EL(e) MIR_##e##_error\ntypedef enum MIR_error_type {\n  REP8 (ERR_EL, no, syntax, binary_io, alloc, finish, no_module, nested_module, no_func),\n  REP5 (ERR_EL, func, vararg_func, nested_func, wrong_param_value, hard_reg),\n  REP5 (ERR_EL, reserved_name, import_export, undeclared_func_reg, repeated_decl, reg_type),\n  REP6 (ERR_EL, wrong_type, unique_reg, undeclared_op_ref, ops_num, call_op, unspec_op),\n  REP6 (ERR_EL, wrong_lref, ret, op_mode, out_op, invalid_insn, ctx_change)\n} MIR_error_type_t;\n\n#ifdef __GNUC__\n#define MIR_NO_RETURN __attribute__ ((noreturn))\n#else\n#define MIR_NO_RETURN\n#endif\n\ntypedef void MIR_NO_RETURN (*MIR_error_func_t) (MIR_error_type_t error_type, const char *format,\n                                                ...);\n\n#define INSN_EL(i) MIR_##i\n\n/* The most MIR insns have destination operand and one or two source\n   operands.  The destination can be only a register or memory.\n\n   There are additional constraints on insn operands:\n\n   o A register in program can contain only one type values: integer,\n     float, double, or long double.\n   o Operand types should be what the insn expects */\ntypedef enum {\n  /* Abbreviations:\n     I - 64-bit int, S - short (32-bit), U - unsigned, F -float, D - double, LD - long double.  */\n  /* 2 operand insns: */\n  REP4 (INSN_EL, MOV, FMOV, DMOV, LDMOV), /* Moves */\n  /* Extensions.  Truncation is not necessary because we can use an extension to use a part. */\n  REP6 (INSN_EL, EXT8, EXT16, EXT32, UEXT8, UEXT16, UEXT32),\n  REP3 (INSN_EL, I2F, I2D, I2LD),    /* Integer to float or (long) double conversion */\n  REP3 (INSN_EL, UI2F, UI2D, UI2LD), /* Unsigned integer to float or (long) double conversion */\n  REP3 (INSN_EL, F2I, D2I, LD2I),    /* Float or (long) double to integer conversion */\n  REP6 (INSN_EL, F2D, F2LD, D2F, D2LD, LD2F, LD2D), /* Float, (long) double conversions */\n  REP5 (INSN_EL, NEG, NEGS, FNEG, DNEG, LDNEG),     /* Changing sign */\n  REP4 (INSN_EL, ADDR, ADDR8, ADDR16, ADDR32), /* reg addr in natural mode or given integer mode */\n  /* 3 operand insn: */\n  REP5 (INSN_EL, ADD, ADDS, FADD, DADD, LDADD),              /* Addition */\n  REP5 (INSN_EL, SUB, SUBS, FSUB, DSUB, LDSUB),              /* Subtraction */\n  REP5 (INSN_EL, MUL, MULS, FMUL, DMUL, LDMUL),              /* Multiplication */\n  REP7 (INSN_EL, DIV, DIVS, UDIV, UDIVS, FDIV, DDIV, LDDIV), /* Division */\n  REP4 (INSN_EL, MOD, MODS, UMOD, UMODS),                    /* Modulo */\n  REP6 (INSN_EL, AND, ANDS, OR, ORS, XOR, XORS),             /* Logical */\n  REP6 (INSN_EL, LSH, LSHS, RSH, RSHS, URSH, URSHS),         /* Right signed/unsigned shift */\n  REP5 (INSN_EL, EQ, EQS, FEQ, DEQ, LDEQ),                   /* Equality */\n  REP5 (INSN_EL, NE, NES, FNE, DNE, LDNE),                   /* Inequality */\n  REP7 (INSN_EL, LT, LTS, ULT, ULTS, FLT, DLT, LDLT),        /* Less then */\n  REP7 (INSN_EL, LE, LES, ULE, ULES, FLE, DLE, LDLE),        /* Less or equal */\n  REP7 (INSN_EL, GT, GTS, UGT, UGTS, FGT, DGT, LDGT),        /* Greater then */\n  REP7 (INSN_EL, GE, GES, UGE, UGES, FGE, DGE, LDGE),        /* Greater or equal */\n  REP8 (INSN_EL, ADDO, ADDOS, SUBO, SUBOS, MULO, MULOS, UMULO, UMULOS), /* setting overflow flag */\n  /* Unconditional (1 operand) and conditional (2 operands) branch\n     insns.  The first operand is a label.  */\n  REP5 (INSN_EL, JMP, BT, BTS, BF, BFS),\n  /* Compare and branch (3 operand) insns.  The first operand is the\n     label. */\n  REP5 (INSN_EL, BEQ, BEQS, FBEQ, DBEQ, LDBEQ),\n  REP5 (INSN_EL, BNE, BNES, FBNE, DBNE, LDBNE),\n  REP7 (INSN_EL, BLT, BLTS, UBLT, UBLTS, FBLT, DBLT, LDBLT),\n  REP7 (INSN_EL, BLE, BLES, UBLE, UBLES, FBLE, DBLE, LDBLE),\n  REP7 (INSN_EL, BGT, BGTS, UBGT, UBGTS, FBGT, DBGT, LDBGT),\n  REP7 (INSN_EL, BGE, BGES, UBGE, UBGES, FBGE, DBGE, LDBGE),\n  REP2 (INSN_EL, BO, UBO),   /* branch on overflow: prev insn should be overflow add/sub */\n  REP2 (INSN_EL, BNO, UBNO), /* branch on not overflow: prev insn should be overflow add/sub */\n  INSN_EL (LADDR),           /* put label address (2nd op) into the 1st op */\n  INSN_EL (JMPI),            /* indirect jump to the label whose address stored in the 1st op */\n  /* 1st operand is a prototype, 2nd one is ref or op containing func\n     address, 3rd and subsequent ops are optional result (if result in\n     the prototype is not of void type), call arguments. */\n  REP3 (INSN_EL, CALL, INLINE, JCALL),\n  /* 1st operand is an index, subsequent ops are labels to which goto\n     according the index (1st label has index zero).  The insn\n     behavior is undefined if there is no label for the index. */\n  INSN_EL (SWITCH),\n  INSN_EL (RET),\n  INSN_EL (JRET), /* return by jumping to address of the operand */\n  /* 1 operand insn: */\n  INSN_EL (ALLOCA),             /* 2 operands: result address and size  */\n  REP2 (INSN_EL, BSTART, BEND), /* block start: result addr; block end: addr from block start */\n  /* Special insns: */\n  INSN_EL (VA_ARG),       /* result is arg address, operands: va_list addr and memory */\n  INSN_EL (VA_BLOCK_ARG), /* result is arg address, operands: va_list addr, integer (size), and\n                             integer (block type) */\n  INSN_EL (VA_START),\n  INSN_EL (VA_END),                    /* operand is va_list */\n  INSN_EL (LABEL),                     /* One immediate operand is unique label number  */\n  INSN_EL (UNSPEC),                    /* First operand unspec code and the rest are args */\n  REP3 (INSN_EL, PRSET, PRBEQ, PRBNE), /* work with properties */\n  INSN_EL (USE), /* Used only internally in the generator, all operands are input */\n  INSN_EL (PHI), /* Used only internally in the generator, the first operand is output */\n  INSN_EL (INVALID_INSN),\n  INSN_EL (INSN_BOUND), /* Should be the last  */\n} MIR_insn_code_t;\n\n#define TYPE_EL(t) MIR_T_##t\n\n#define MIR_BLK_NUM 5\n/* Data types: */\ntypedef enum {\n  REP8 (TYPE_EL, I8, U8, I16, U16, I32, U32, I64, U64), /* Integer types of different size: */\n  REP3 (TYPE_EL, F, D, LD),                             /* Float or (long) double type */\n  REP2 (TYPE_EL, P, BLK),                               /* Pointer, memory blocks */\n  TYPE_EL (RBLK) = TYPE_EL (BLK) + MIR_BLK_NUM,         /* return block */\n  REP2 (TYPE_EL, UNDEF, BOUND),\n} MIR_type_t;\n\nstatic inline int MIR_int_type_p (MIR_type_t t) {\n  return (MIR_T_I8 <= t && t <= MIR_T_U64) || t == MIR_T_P;\n}\n\nstatic inline int MIR_fp_type_p (MIR_type_t t) { return MIR_T_F <= t && t <= MIR_T_LD; }\n\nstatic inline int MIR_blk_type_p (MIR_type_t t) { return MIR_T_BLK <= t && t < MIR_T_RBLK; }\nstatic inline int MIR_all_blk_type_p (MIR_type_t t) { return MIR_T_BLK <= t && t <= MIR_T_RBLK; }\n\n#if UINTPTR_MAX == 0xffffffff\n#define MIR_PTR32 1\n#define MIR_PTR64 0\n#elif UINTPTR_MAX == 0xffffffffffffffffu\n#define MIR_PTR32 0\n#define MIR_PTR64 1\n#else\n#error MIR can work only for 32- or 64-bit targets\n#endif\n\ntypedef uint8_t MIR_scale_t; /* Index reg scale in memory */\n\n#define MIR_MAX_SCALE UINT8_MAX\n\ntypedef int64_t MIR_disp_t; /* Address displacement in memory */\n\n/* Register number (> 0).  A register always contain only one type\n   value: integer, float, or (long) double.  Register numbers in insn\n   operands can be changed in MIR_finish_func.  */\ntypedef uint32_t MIR_reg_t;\n\n#define MIR_MAX_REG_NUM UINT32_MAX\n#define MIR_NON_VAR MIR_MAX_REG_NUM\n\n/* Immediate in immediate moves.  */\ntypedef union {\n  int64_t i;\n  uint64_t u;\n  float f;\n  double d;\n  long double ld;\n} MIR_imm_t;\n\ntypedef uint32_t MIR_alias_t; /* unique number of alias name */\n\n/* Memory: mem:type[base + index * scale + disp].  It also can be memory with vars\n   (regs and hard regs) but such memory used only internally.  An integer type memory\n   value expands to int64_t value when the insn is executed.  */\ntypedef struct {\n  MIR_type_t type : 8;\n  MIR_scale_t scale;\n  MIR_alias_t alias;    /* 0 may alias any memory, memory with the same alias is aliased */\n  MIR_alias_t nonalias; /* 0 for ignoring, memory with the same nonalias is not aliased */\n  /* Used internally: mem operand with the same nonzero nloc always refers to the same memory */\n  uint32_t nloc;\n  /* 0 and MIR_NON_VAR means no reg for correspondingly for memory and var memory. */\n  MIR_reg_t base, index;\n  MIR_disp_t disp;\n} MIR_mem_t;\n\ntypedef struct MIR_insn *MIR_label_t;\n\ntypedef const char *MIR_name_t;\n\n#define OP_EL(op) MIR_OP_##op\n\n/* Operand mode */\ntypedef enum {\n  REP8 (OP_EL, UNDEF, REG, VAR, INT, UINT, FLOAT, DOUBLE, LDOUBLE),\n  REP6 (OP_EL, REF, STR, MEM, VAR_MEM, LABEL, BOUND),\n} MIR_op_mode_t;\n\ntypedef struct MIR_item *MIR_item_t;\n\nstruct MIR_str {\n  size_t len;\n  const char *s;\n};\n\ntypedef struct MIR_str MIR_str_t;\n\n/* An insn operand */\ntypedef struct {\n  void *data; /* Aux data  */\n  MIR_op_mode_t mode : 8;\n  /* Defined after MIR_func_finish.  Only MIR_OP_INT, MIR_OP_UINT,\n     MIR_OP_FLOAT, MIR_OP_DOUBLE, MIR_OP_LDOUBLE: */\n  MIR_op_mode_t value_mode : 8;\n  union {\n    MIR_reg_t reg;\n    MIR_reg_t var; /* Used only internally */\n    int64_t i;\n    uint64_t u;\n    float f;\n    double d;\n    long double ld;\n    MIR_item_t ref; /* non-export/non-forward after simplification */\n    MIR_str_t str;\n    MIR_mem_t mem;\n    MIR_mem_t var_mem; /* Used only internally */\n    MIR_label_t label;\n  } u;\n} MIR_op_t;\n\ntypedef struct MIR_insn *MIR_insn_t;\n\n/* Definition of link of double list of insns */\nDEF_DLIST_LINK (MIR_insn_t);\n\nstruct MIR_insn {\n  void *data; /* Aux data */\n  DLIST_LINK (MIR_insn_t) insn_link;\n  MIR_insn_code_t code : 32;\n  unsigned int nops : 32; /* number of operands */\n  MIR_op_t ops[1];\n};\n\n/* Definition of double list of insns */\nDEF_DLIST (MIR_insn_t, insn_link);\n\ntypedef struct MIR_var {\n  MIR_type_t type; /* MIR_T_BLK .. MIR_T_RBLK can be used only args */\n  const char *name;\n  size_t size; /* ignored for type != [MIR_T_BLK .. MIR_T_RBLK] */\n} MIR_var_t;\n\nDEF_VARR (MIR_var_t);\n\n/* Function definition */\ntypedef struct MIR_func {\n  const char *name;\n  MIR_item_t func_item;\n  size_t original_vars_num;\n  DLIST (MIR_insn_t) insns, original_insns;\n  uint32_t nres, nargs, last_temp_num, n_inlines;\n  MIR_type_t *res_types;\n  char vararg_p;                  /* flag of variable number of arguments */\n  char expr_p;                    /* flag of that the func can be used as a linker expression */\n  char jret_p;                    /* flag of jcall/jret func, set up after MIR_func_finish */\n  VARR (MIR_var_t) * vars;        /* args and locals but temps */\n  VARR (MIR_var_t) * global_vars; /* can be NULL */\n  void *machine_code;             /* address of generated machine code or NULL */\n  void *call_addr; /* address to call the function, it can be the same as machine_code */\n  void *internal;  /* internal data structure */\n  struct MIR_lref_data *first_lref; /* label addr data of the func: defined by module load */\n} *MIR_func_t;\n\ntypedef struct MIR_proto {\n  const char *name;\n  uint32_t nres;\n  MIR_type_t *res_types;   /* != MIR_T_UNDEF */\n  char vararg_p;           /* flag of variable number of arguments */\n  VARR (MIR_var_t) * args; /* args name can be NULL */\n} *MIR_proto_t;\n\ntypedef struct MIR_data {\n  const char *name; /* can be NULL */\n  MIR_type_t el_type;\n  size_t nel;\n  union {\n    long double d; /* for alignment of temporary literals */\n    uint8_t els[1];\n  } u;\n} *MIR_data_t;\n\ntypedef struct MIR_ref_data {\n  const char *name;    /* can be NULL */\n  MIR_item_t ref_item; /* base */\n  int64_t disp;        /* disp relative to base */\n  void *load_addr;\n} *MIR_ref_data_t;\n\ntypedef struct MIR_lref_data { /* describing [name:]lref lab[,label2][,disp] = lab-lab2+disp */\n  const char *name;            /* can be NULL */\n  MIR_label_t label;           /* base */\n  MIR_label_t label2;          /* can be NULL */\n  MIR_label_t orig_label, orig_label2; /* used to restore original func lrefs */\n  int64_t disp;                        /* disp relative to base */\n  void *load_addr;                     /* where is the value placed */\n  struct MIR_lref_data *next;          /* next label addr related to the same func */\n} *MIR_lref_data_t;\n\ntypedef struct MIR_expr_data {\n  const char *name;     /* can be NULL */\n  MIR_item_t expr_item; /* a special function can be called during linking */\n  void *load_addr;\n} *MIR_expr_data_t;\n\ntypedef struct MIR_bss {\n  const char *name; /* can be NULL */\n  uint64_t len;\n} *MIR_bss_t;\n\ntypedef struct MIR_module *MIR_module_t;\n\n/* Definition of link of double list of MIR_item_t type elements */\nDEF_DLIST_LINK (MIR_item_t);\n\n#define ITEM_EL(i) MIR_##i##_item\n\ntypedef enum {\n  REP8 (ITEM_EL, func, proto, import, export, forward, data, ref_data, lref_data),\n  REP2 (ITEM_EL, expr_data, bss),\n} MIR_item_type_t;\n\n#undef ERR_EL\n#undef INSN_EL\n#undef TYPE_EL\n#undef OP_EL\n#undef ITEM_EL\n#undef REP_SEP\n\n/* MIR module items (function or import): */\nstruct MIR_item {\n  void *data;\n  MIR_module_t module;\n  DLIST_LINK (MIR_item_t) item_link;\n  MIR_item_type_t item_type; /* item type */\n  /* Non-null only for export/forward items and import item after\n     linking.  It forms a chain to the final definition. */\n  MIR_item_t ref_def;\n  /* address of loaded data/bss items, function to call the function\n     item, imported definition or proto object */\n  void *addr;\n  char export_p; /* true for export items (only func items) */\n  /* defined for data-bss after loading. True if it is a start of allocated section */\n  char section_head_p;\n  union {\n    MIR_func_t func;\n    MIR_proto_t proto;\n    MIR_name_t import_id;\n    MIR_name_t export_id;\n    MIR_name_t forward_id;\n    MIR_data_t data;\n    MIR_ref_data_t ref_data;\n    MIR_lref_data_t lref_data;\n    MIR_expr_data_t expr_data;\n    MIR_bss_t bss;\n  } u;\n};\n\n/* Definition of double list of MIR_item_t type elements */\nDEF_DLIST (MIR_item_t, item_link);\n\n/* Definition of link of double list of MIR_module_t type elements */\nDEF_DLIST_LINK (MIR_module_t);\n\n/* MIR module: */\nstruct MIR_module {\n  void *data;\n  const char *name;\n  DLIST (MIR_item_t) items; /* module items */\n  DLIST_LINK (MIR_module_t) module_link;\n  uint32_t last_temp_item_num; /* Used only internally */\n};\n\n/* Definition of double list of MIR_item_t type elements */\nDEF_DLIST (MIR_module_t, module_link);\n\nstruct MIR_context;\ntypedef struct MIR_context *MIR_context_t;\n\nstatic inline int MIR_FP_branch_code_p (MIR_insn_code_t code) {\n  return (code == MIR_FBEQ || code == MIR_DBEQ || code == MIR_LDBEQ || code == MIR_FBNE\n          || code == MIR_DBNE || code == MIR_LDBNE || code == MIR_FBLT || code == MIR_DBLT\n          || code == MIR_LDBLT || code == MIR_FBLE || code == MIR_DBLE || code == MIR_LDBLE\n          || code == MIR_FBGT || code == MIR_DBGT || code == MIR_LDBGT || code == MIR_FBGE\n          || code == MIR_DBGE || code == MIR_LDBGE);\n}\n\nstatic inline int MIR_call_code_p (MIR_insn_code_t code) {\n  return code == MIR_CALL || code == MIR_INLINE || code == MIR_JCALL;\n}\n\nstatic inline int MIR_int_branch_code_p (MIR_insn_code_t code) {\n  return (code == MIR_BT || code == MIR_BTS || code == MIR_BF || code == MIR_BFS || code == MIR_BEQ\n          || code == MIR_BEQS || code == MIR_BNE || code == MIR_BNES || code == MIR_BLT\n          || code == MIR_BLTS || code == MIR_UBLT || code == MIR_UBLTS || code == MIR_BLE\n          || code == MIR_BLES || code == MIR_UBLE || code == MIR_UBLES || code == MIR_BGT\n          || code == MIR_BGTS || code == MIR_UBGT || code == MIR_UBGTS || code == MIR_BGE\n          || code == MIR_BGES || code == MIR_UBGE || code == MIR_UBGES || code == MIR_BO\n          || code == MIR_UBO || code == MIR_BNO || code == MIR_UBNO);\n}\n\nstatic inline int MIR_branch_code_p (MIR_insn_code_t code) {\n  return (code == MIR_JMP || MIR_int_branch_code_p (code) || MIR_FP_branch_code_p (code));\n}\n\nstatic inline int MIR_any_branch_code_p (MIR_insn_code_t code) {\n  return (MIR_branch_code_p (code) || code == MIR_JMPI || code == MIR_SWITCH);\n}\n\nstatic inline int MIR_addr_code_p (MIR_insn_code_t code) {\n  return (code == MIR_ADDR || code == MIR_ADDR8 || code == MIR_ADDR16 || code == MIR_ADDR32);\n}\n\nstatic inline int MIR_overflow_insn_code_p (MIR_insn_code_t code) {\n  return (code == MIR_ADDO || code == MIR_ADDOS || code == MIR_SUBO || code == MIR_SUBOS\n          || code == MIR_MULO || code == MIR_MULOS || code == MIR_UMULO || code == MIR_UMULOS);\n}\n\nextern double _MIR_get_api_version (void);\nextern MIR_context_t _MIR_init (MIR_alloc_t alloc, MIR_code_alloc_t code_alloc);\n\n/* Use only either the following API to create MIR code... */\nstatic inline MIR_context_t MIR_init2 (MIR_alloc_t alloc, MIR_code_alloc_t code_alloc) {\n  if (MIR_API_VERSION != _MIR_get_api_version ()) {\n    fprintf (stderr,\n             \"mir.h header has version %g different from used mir code version %g -- good bye!\\n\",\n             MIR_API_VERSION, _MIR_get_api_version ());\n    exit (1);\n  }\n  return _MIR_init (alloc, code_alloc);\n}\n\n/* ...or this one. */\nstatic inline MIR_context_t MIR_init (void) {\n  return MIR_init2 (NULL, NULL);\n}\n\nextern void MIR_finish (MIR_context_t ctx);\n\nextern MIR_module_t MIR_new_module (MIR_context_t ctx, const char *name);\nextern DLIST (MIR_module_t) * MIR_get_module_list (MIR_context_t ctx);\nextern MIR_item_t MIR_new_import (MIR_context_t ctx, const char *name);\nextern MIR_item_t MIR_new_export (MIR_context_t ctx, const char *name);\nextern MIR_item_t MIR_new_forward (MIR_context_t ctx, const char *name);\nextern MIR_item_t MIR_new_bss (MIR_context_t ctx, const char *name,\n                               size_t len); /* name can be NULL */\nextern MIR_item_t MIR_new_data (MIR_context_t ctx, const char *name, MIR_type_t el_type, size_t nel,\n                                const void *els); /* name can be NULL */\nextern MIR_item_t MIR_new_string_data (MIR_context_t ctx, const char *name,\n                                       MIR_str_t str); /* name can be NULL */\nextern MIR_item_t MIR_new_ref_data (MIR_context_t ctx, const char *name, MIR_item_t item,\n                                    int64_t disp); /* name can be NULL */\nextern MIR_item_t MIR_new_lref_data (MIR_context_t ctx, const char *name, MIR_label_t label,\n                                     MIR_label_t label2,\n                                     int64_t disp); /* name and label2 can be NULL */\nextern MIR_item_t MIR_new_expr_data (MIR_context_t ctx, const char *name,\n                                     MIR_item_t expr_item); /* name can be NULL */\nextern MIR_item_t MIR_new_proto_arr (MIR_context_t ctx, const char *name, size_t nres,\n                                     MIR_type_t *res_types, size_t nargs, MIR_var_t *vars);\nextern MIR_item_t MIR_new_proto (MIR_context_t ctx, const char *name, size_t nres,\n                                 MIR_type_t *res_types, size_t nargs, ...);\nextern MIR_item_t MIR_new_vararg_proto_arr (MIR_context_t ctx, const char *name, size_t nres,\n                                            MIR_type_t *res_types, size_t nargs, MIR_var_t *vars);\nextern MIR_item_t MIR_new_vararg_proto (MIR_context_t ctx, const char *name, size_t nres,\n                                        MIR_type_t *res_types, size_t nargs, ...);\nextern MIR_item_t MIR_new_func_arr (MIR_context_t ctx, const char *name, size_t nres,\n                                    MIR_type_t *res_types, size_t nargs, MIR_var_t *vars);\nextern MIR_item_t MIR_new_func (MIR_context_t ctx, const char *name, size_t nres,\n                                MIR_type_t *res_types, size_t nargs, ...);\nextern MIR_item_t MIR_new_vararg_func_arr (MIR_context_t ctx, const char *name, size_t nres,\n                                           MIR_type_t *res_types, size_t nargs, MIR_var_t *vars);\nextern MIR_item_t MIR_new_vararg_func (MIR_context_t ctx, const char *name, size_t nres,\n                                       MIR_type_t *res_types, size_t nargs, ...);\nextern const char *MIR_item_name (MIR_context_t ctx, MIR_item_t item);\nextern MIR_func_t MIR_get_item_func (MIR_context_t ctx, MIR_item_t item);\nextern MIR_reg_t MIR_new_func_reg (MIR_context_t ctx, MIR_func_t func, MIR_type_t type,\n                                   const char *name);\nextern MIR_reg_t MIR_new_global_func_reg (MIR_context_t ctx, MIR_func_t func, MIR_type_t type,\n                                          const char *name, const char *hard_reg_name);\nextern void MIR_finish_func (MIR_context_t ctx);\nextern void MIR_finish_module (MIR_context_t ctx);\n\nextern MIR_error_func_t MIR_get_error_func (MIR_context_t ctx);\nextern void MIR_set_error_func (MIR_context_t ctx, MIR_error_func_t func);\n\nextern MIR_alloc_t MIR_get_alloc (MIR_context_t ctx);\n\nextern int MIR_get_func_redef_permission_p (MIR_context_t ctx);\nextern void MIR_set_func_redef_permission (MIR_context_t ctx, int flag_p);\n\nextern MIR_insn_t MIR_new_insn_arr (MIR_context_t ctx, MIR_insn_code_t code, size_t nops,\n                                    MIR_op_t *ops);\nextern MIR_insn_t MIR_new_insn (MIR_context_t ctx, MIR_insn_code_t code, ...);\nextern MIR_insn_t MIR_new_call_insn (MIR_context_t ctx, size_t nops, ...);\nextern MIR_insn_t MIR_new_jcall_insn (MIR_context_t ctx, size_t nops, ...);\nextern MIR_insn_t MIR_new_ret_insn (MIR_context_t ctx, size_t nops, ...);\nextern MIR_insn_t MIR_copy_insn (MIR_context_t ctx, MIR_insn_t insn);\n\nextern const char *MIR_insn_name (MIR_context_t ctx, MIR_insn_code_t code);\nextern size_t MIR_insn_nops (MIR_context_t ctx, MIR_insn_t insn);\nextern MIR_op_mode_t MIR_insn_op_mode (MIR_context_t ctx, MIR_insn_t insn, size_t nop, int *out_p);\n\nextern MIR_insn_t MIR_new_label (MIR_context_t ctx);\n\nextern MIR_reg_t MIR_reg (MIR_context_t ctx, const char *reg_name, MIR_func_t func);\nextern MIR_type_t MIR_reg_type (MIR_context_t ctx, MIR_reg_t reg, MIR_func_t func);\nextern const char *MIR_reg_name (MIR_context_t ctx, MIR_reg_t reg, MIR_func_t func);\nextern const char *MIR_reg_hard_reg_name (MIR_context_t ctx, MIR_reg_t reg, MIR_func_t func);\n\nextern const char *MIR_alias_name (MIR_context_t ctx, MIR_alias_t alias);\nextern MIR_alias_t MIR_alias (MIR_context_t ctx, const char *name);\n\nextern MIR_op_t MIR_new_reg_op (MIR_context_t ctx, MIR_reg_t reg);\nextern MIR_op_t MIR_new_int_op (MIR_context_t ctx, int64_t v);\nextern MIR_op_t MIR_new_uint_op (MIR_context_t ctx, uint64_t v);\nextern MIR_op_t MIR_new_float_op (MIR_context_t ctx, float v);\nextern MIR_op_t MIR_new_double_op (MIR_context_t ctx, double v);\nextern MIR_op_t MIR_new_ldouble_op (MIR_context_t ctx, long double v);\nextern MIR_op_t MIR_new_ref_op (MIR_context_t ctx, MIR_item_t item);\nextern MIR_op_t MIR_new_str_op (MIR_context_t ctx, MIR_str_t str);\nextern MIR_op_t MIR_new_mem_op (MIR_context_t ctx, MIR_type_t type, MIR_disp_t disp, MIR_reg_t base,\n                                MIR_reg_t index, MIR_scale_t scale);\nextern MIR_op_t MIR_new_alias_mem_op (MIR_context_t ctx, MIR_type_t type, MIR_disp_t disp,\n                                      MIR_reg_t base, MIR_reg_t index, MIR_scale_t scale,\n                                      MIR_alias_t alias, MIR_alias_t noalias);\nextern MIR_op_t MIR_new_label_op (MIR_context_t ctx, MIR_label_t label);\nextern int MIR_op_eq_p (MIR_context_t ctx, MIR_op_t op1, MIR_op_t op2);\nextern htab_hash_t MIR_op_hash_step (MIR_context_t ctx, htab_hash_t h, MIR_op_t op);\n\nextern void MIR_append_insn (MIR_context_t ctx, MIR_item_t func, MIR_insn_t insn);\nextern void MIR_prepend_insn (MIR_context_t ctx, MIR_item_t func, MIR_insn_t insn);\nextern void MIR_insert_insn_after (MIR_context_t ctx, MIR_item_t func, MIR_insn_t after,\n                                   MIR_insn_t insn);\nextern void MIR_insert_insn_before (MIR_context_t ctx, MIR_item_t func, MIR_insn_t before,\n                                    MIR_insn_t insn);\nextern void MIR_remove_insn (MIR_context_t ctx, MIR_item_t func, MIR_insn_t insn);\n\nextern void MIR_change_module_ctx (MIR_context_t old_ctx, MIR_module_t m, MIR_context_t new_ctx);\n\nextern MIR_insn_code_t MIR_reverse_branch_code (MIR_insn_code_t code);\n\nextern const char *MIR_type_str (MIR_context_t ctx, MIR_type_t tp);\nextern void MIR_output_str (MIR_context_t ctx, FILE *f, MIR_str_t str);\nextern void MIR_output_op (MIR_context_t ctx, FILE *f, MIR_op_t op, MIR_func_t func);\nextern void MIR_output_insn (MIR_context_t ctx, FILE *f, MIR_insn_t insn, MIR_func_t func,\n                             int newline_p);\nextern void MIR_output_item (MIR_context_t ctx, FILE *f, MIR_item_t item);\nextern void MIR_output_module (MIR_context_t ctx, FILE *f, MIR_module_t module);\nextern void MIR_output (MIR_context_t ctx, FILE *f);\n\n#if !MIR_NO_IO\nextern void MIR_write (MIR_context_t ctx, FILE *f);\nextern void MIR_write_module (MIR_context_t ctx, FILE *f, MIR_module_t module);\nextern void MIR_read (MIR_context_t ctx, FILE *f);\nextern void MIR_write_with_func (MIR_context_t ctx,\n                                 int (*const writer_func) (MIR_context_t, uint8_t));\nextern void MIR_write_module_with_func (MIR_context_t ctx,\n                                        int (*const writer_func) (MIR_context_t, uint8_t),\n                                        MIR_module_t module);\nextern void MIR_read_with_func (MIR_context_t ctx, int (*const reader_func) (MIR_context_t));\n#endif\n\n#if !MIR_NO_SCAN\nextern void MIR_scan_string (MIR_context_t ctx, const char *str);\n#endif\n\nextern MIR_item_t MIR_get_global_item (MIR_context_t ctx, const char *name);\nextern void MIR_load_module (MIR_context_t ctx, MIR_module_t m);\nextern void MIR_load_external (MIR_context_t ctx, const char *name, void *addr);\nextern void MIR_link (MIR_context_t ctx, void (*set_interface) (MIR_context_t ctx, MIR_item_t item),\n                      void *(*import_resolver) (const char *) );\n\n/* Interpreter: */\ntypedef union {\n  MIR_insn_code_t ic;\n  void *a;\n  int64_t i;\n  uint64_t u;\n  float f;\n  double d;\n  long double ld;\n} MIR_val_t;\n\nextern void MIR_interp (MIR_context_t ctx, MIR_item_t func_item, MIR_val_t *results, size_t nargs,\n                        ...);\nextern void MIR_interp_arr (MIR_context_t ctx, MIR_item_t func_item, MIR_val_t *results,\n                            size_t nargs, MIR_val_t *vals);\nextern void MIR_interp_arr_varg (MIR_context_t ctx, MIR_item_t func_item, MIR_val_t *results,\n                                 size_t nargs, MIR_val_t *vals, va_list va);\nextern void MIR_set_interp_interface (MIR_context_t ctx, MIR_item_t func_item);\n\n/* Private: */\nextern double _MIR_get_api_version (void);\nextern MIR_context_t _MIR_init (MIR_alloc_t alloc, MIR_code_alloc_t code_alloc);\nextern const char *_MIR_uniq_string (MIR_context_t ctx, const char *str);\nextern int _MIR_reserved_ref_name_p (MIR_context_t ctx, const char *name);\nextern int _MIR_reserved_name_p (MIR_context_t ctx, const char *name);\nextern int64_t _MIR_addr_offset (MIR_context_t ctx, MIR_insn_code_t code);\nextern void _MIR_free_insn (MIR_context_t ctx, MIR_insn_t insn);\nextern MIR_reg_t _MIR_new_temp_reg (MIR_context_t ctx, MIR_type_t type,\n                                    MIR_func_t func); /* for internal use only */\nextern size_t _MIR_type_size (MIR_context_t ctx, MIR_type_t type);\nextern MIR_op_mode_t _MIR_insn_code_op_mode (MIR_context_t ctx, MIR_insn_code_t code, size_t nop,\n                                             int *out_p);\nextern MIR_insn_t _MIR_new_unspec_insn (MIR_context_t ctx, size_t nops, ...);\nextern void _MIR_register_unspec_insn (MIR_context_t ctx, uint64_t code, const char *name,\n                                       size_t nres, MIR_type_t *res_types, size_t nargs,\n                                       int vararg_p, MIR_var_t *args);\nextern void _MIR_duplicate_func_insns (MIR_context_t ctx, MIR_item_t func_item);\nextern void _MIR_restore_func_insns (MIR_context_t ctx, MIR_item_t func_item);\n\nextern void _MIR_output_data_item_els (MIR_context_t ctx, FILE *f, MIR_item_t item, int c_p);\nextern void _MIR_get_temp_item_name (MIR_context_t ctx, MIR_module_t module, char *buff,\n                                     size_t buff_len);\n\nextern MIR_op_t _MIR_new_var_op (MIR_context_t ctx, MIR_reg_t var);\n\nextern MIR_op_t _MIR_new_var_mem_op (MIR_context_t ctx, MIR_type_t type, MIR_disp_t disp,\n                                     MIR_reg_t base, MIR_reg_t index, MIR_scale_t scale);\nextern MIR_op_t _MIR_new_alias_var_mem_op (MIR_context_t ctx, MIR_type_t type, MIR_disp_t disp,\n                                           MIR_reg_t base, MIR_reg_t index, MIR_scale_t scale,\n                                           MIR_alias_t alias, MIR_alias_t no_alias);\n\nextern MIR_item_t _MIR_builtin_proto (MIR_context_t ctx, MIR_module_t module, const char *name,\n                                      size_t nres, MIR_type_t *res_types, size_t nargs, ...);\nextern MIR_item_t _MIR_builtin_func (MIR_context_t ctx, MIR_module_t module, const char *name,\n                                     void *addr);\nextern void _MIR_flush_code_cache (void *start, void *bound);\nextern uint8_t *_MIR_publish_code (MIR_context_t ctx, const uint8_t *code, size_t code_len);\nextern uint8_t *_MIR_get_new_code_addr (MIR_context_t ctx, size_t size);\nextern uint8_t *_MIR_publish_code_by_addr (MIR_context_t ctx, void *addr, const uint8_t *code,\n                                           size_t code_len);\nstruct MIR_code_reloc {\n  size_t offset;\n  const void *value;\n};\n\ntypedef struct MIR_code_reloc MIR_code_reloc_t;\n\nextern void _MIR_set_code (MIR_code_alloc_t alloc, size_t prot_start, size_t prot_len,\n                           uint8_t *base, size_t nloc, const MIR_code_reloc_t *relocs,\n                           size_t reloc_size);\nextern void _MIR_change_code (MIR_context_t ctx, uint8_t *addr, const uint8_t *code,\n                              size_t code_len);\nextern void _MIR_update_code_arr (MIR_context_t ctx, uint8_t *base, size_t nloc,\n                                  const MIR_code_reloc_t *relocs);\nextern void _MIR_update_code (MIR_context_t ctx, uint8_t *base, size_t nloc, ...);\n\nextern void *va_arg_builtin (void *p, uint64_t t);\nextern void va_block_arg_builtin (void *res, void *p, size_t s, uint64_t t);\nextern void va_start_interp_builtin (MIR_context_t ctx, void *p, void *a);\nextern void va_end_interp_builtin (MIR_context_t ctx, void *p);\n\nextern void *_MIR_get_bstart_builtin (MIR_context_t ctx);\nextern void *_MIR_get_bend_builtin (MIR_context_t ctx);\n\ntypedef struct {\n  MIR_type_t type;\n  size_t size; /* used only for block arg (type == [MIR_T_BLK ..  MIR_T_RBLK]) */\n} _MIR_arg_desc_t;\n\nextern void *_MIR_get_ff_call (MIR_context_t ctx, size_t nres, MIR_type_t *res_types, size_t nargs,\n                               _MIR_arg_desc_t *arg_descs, size_t arg_vars_num);\nextern void *_MIR_get_interp_shim (MIR_context_t ctx, MIR_item_t func_item, void *handler);\nextern void *_MIR_get_thunk (MIR_context_t ctx);\nextern void *_MIR_get_thunk_addr (MIR_context_t ctx, void *thunk);\nextern void _MIR_redirect_thunk (MIR_context_t ctx, void *thunk, void *to);\nextern void *_MIR_get_jmpi_thunk (MIR_context_t ctx, void **res_loc, void *res, void *cont);\nextern void *_MIR_get_wrapper (MIR_context_t ctx, MIR_item_t called_func, void *hook_address);\nextern void *_MIR_get_wrapper_end (MIR_context_t ctx);\nextern void *_MIR_get_bb_thunk (MIR_context_t ctx, void *bb_version, void *handler);\nextern void _MIR_replace_bb_thunk (MIR_context_t ctx, void *thunk, void *to);\nextern void *_MIR_get_bb_wrapper (MIR_context_t ctx, void *data, void *hook_address);\n\nextern int _MIR_name_char_p (MIR_context_t ctx, int ch, int first_p);\nextern void _MIR_dump_code (const char *name, uint8_t *code, size_t code_len);\n\nextern int _MIR_get_hard_reg (MIR_context_t ctx, const char *hard_reg_name);\nextern void *_MIR_get_module_global_var_hard_regs (MIR_context_t ctx, MIR_module_t module);\n\n#ifdef __cplusplus\n}\n#endif\n\n#endif /* #ifndef MIR_H */\n"
        },
        {
          "name": "mir2c",
          "type": "tree",
          "content": null
        },
        {
          "name": "mir3.svg",
          "type": "blob",
          "size": 12.9140625,
          "content": "<?xml version=\"1.0\" encoding=\"UTF-8\"?><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"1000.0\" height=\"520.0\" viewBox=\"130.0 70.0 1000.0 520.0\" version=\"1.1\">\n<rect x=\"130.0\" y=\"70.0\" width=\"1000.0\" height=\"520.0\" fill=\"rgb(255,255,255)\"/>\n<g id=\"s2554299\">\n<polygon id=\"s2554299\" points=\"240.0,260.0 999.99976,260.0 1000.00006,339.99982 240.0,340.0 \" fill=\"rgb(217,204,140)\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<g id=\"t2554299\">\n<text xml:space=\"preserve\" x=\"590.0\" y=\"313.34766\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"34\">MIR</text>\n</g>\n</g>\n<g id=\"s38895320\">\n<polygon id=\"s38895320\" points=\"160.0,280.0 200.0,280.0 199.99998,320.0 160.0,319.99997 \" fill=\"none\" stroke-width=\"0.0\" stroke=\"rgb(102,51,102)\"/>\n<g id=\"t38895320\">\n<text xml:space=\"preserve\" x=\"164.5\" y=\"307.85156\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"20\">API</text>\n</g>\n</g>\n<g id=\"s73990705\">\n<polygon id=\"s73990705\" points=\"500.0,120.0 540.0,120.0 539.99976,160.0 500.0,159.99995 \" fill=\"rgb(217,204,140)\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<g id=\"t73990705\">\n<text xml:space=\"preserve\" x=\"513.5\" y=\"147.85156\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"20\">C</text>\n</g>\n</g>\n<g id=\"s38210838\">\n<polygon id=\"s38210838\" points=\"760.0,120.0 860.0,120.0 859.9984,159.99997 760.0,160.0 \" fill=\"rgb(192,202,51)\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<g id=\"t38210838\">\n<text xml:space=\"preserve\" x=\"773.0\" y=\"147.85156\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"20\">LLVM IR</text>\n</g>\n</g>\n<g id=\"s54621847\">\n<polygon id=\"s54621847\" points=\"160.0,380.0 240.00003,380.0 239.99991,440.0 160.0,439.9999 \" fill=\"rgb(217,204,140)\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<g id=\"t54621847\">\n<text xml:space=\"preserve\" x=\"183.00002\" y=\"404.58008\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"20\">MIR</text>\n<text xml:space=\"preserve\" x=\"173.00002\" y=\"431.12305\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"20\">binary</text>\n</g>\n</g>\n<g id=\"s80457860\">\n<polygon id=\"s80457860\" points=\"260.0,380.0 320.0,380.0 319.99994,439.9993 260.0,440.0 \" fill=\"rgb(217,204,140)\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<g id=\"t80457860\">\n<text xml:space=\"preserve\" x=\"273.0\" y=\"404.58008\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"20\">MIR</text>\n<text xml:space=\"preserve\" x=\"272.5\" y=\"431.12305\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"20\">text</text>\n</g>\n</g>\n<g id=\"s77238381\">\n<polygon id=\"s77238381\" points=\"560.0,500.0 640.0,500.0 639.9999,560.0 560.0,560.0 \" fill=\"rgb(217,204,140)\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<g id=\"t77238381\">\n<text xml:space=\"preserve\" x=\"573.0\" y=\"537.0664\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"18\">x86-64</text>\n</g>\n</g>\n<g id=\"s253682850\">\n<polygon id=\"s253682850\" points=\"660.0,500.0 739.9999,500.0 740.0,560.0 660.0,559.9997 \" fill=\"rgb(217,204,140)\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<g id=\"t253682850\">\n<text xml:space=\"preserve\" x=\"667.5\" y=\"537.0664\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"18\">aarch64</text>\n</g>\n</g>\n<g id=\"s148744031\">\n<polygon id=\"s148744031\" points=\"440.0,500.0 540.0,500.0 540.0,560.0 440.0,560.0 \" fill=\"rgb(217,204,140)\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<g id=\"t148744031\">\n<text xml:space=\"preserve\" x=\"453.0\" y=\"537.0664\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"18\">PPC64LE</text>\n</g>\n</g>\n<g id=\"s18670899\">\n<polygon id=\"s18670899\" points=\"860.0,500.0 940.0,500.0 939.99927,560.0 860.0,559.99994 \" fill=\"rgb(217,204,140)\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<g id=\"t18670899\">\n<text xml:space=\"preserve\" x=\"875.0\" y=\"537.0664\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"18\">RISCV</text>\n</g>\n</g>\n<g id=\"s261443824\">\n<polygon id=\"s261443824\" points=\"160.0,160.0 239.99998,160.0 240.00005,220.0 160.0,219.99998 \" fill=\"rgb(217,204,140)\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<g id=\"t261443824\">\n<text xml:space=\"preserve\" x=\"183.00003\" y=\"184.58008\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"20\">MIR</text>\n<text xml:space=\"preserve\" x=\"173.00003\" y=\"211.12305\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"20\">binary</text>\n</g>\n</g>\n<g id=\"s182240873\">\n<polygon id=\"s182240873\" points=\"280.0,160.0 340.0,160.0 340.0,219.99998 280.0,220.0 \" fill=\"rgb(217,204,140)\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<g id=\"t182240873\">\n<text xml:space=\"preserve\" x=\"293.0\" y=\"184.58008\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"20\">MIR</text>\n<text xml:space=\"preserve\" x=\"292.5\" y=\"211.12305\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"20\">text</text>\n</g>\n</g>\n<g id=\"s44738798\">\n<ellipse id=\"s44738798\" cx=\"920.0\" cy=\"390.0\" rx=\"60.0\" ry=\"30.0\" fill=\"rgb(217,204,140)\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<g id=\"t44738798\">\n<text xml:space=\"preserve\" x=\"877.5\" y=\"397.0664\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"18\">Interpreter</text>\n</g>\n</g>\n<g id=\"s31024556\">\n<ellipse id=\"s31024556\" cx=\"750.0\" cy=\"430.0\" rx=\"70.0\" ry=\"30.0\" fill=\"rgb(217,204,140)\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<g id=\"t31024556\">\n<text xml:space=\"preserve\" x=\"705.0\" y=\"437.85156\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"20\">Generator</text>\n</g>\n</g>\n<g id=\"s137798031\">\n<polygon id=\"s137798031\" points=\"1040.0,100.0 1100.0,100.0 1099.9983,119.99997 1040.0,120.0 \" fill=\"none\" stroke-width=\"0.0\" stroke=\"rgb(102,51,102)\"/>\n<g id=\"t137798031\"/>\n</g>\n<g id=\"s135695388\">\n<polygon id=\"s135695388\" points=\"760.0,500.0 840.0,500.0 839.99994,559.9999 760.0,560.0 \" fill=\"rgb(217,204,140)\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<g id=\"t135695388\">\n<text xml:space=\"preserve\" x=\"776.0\" y=\"537.0664\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"18\">s390x</text>\n</g>\n</g>\n<g id=\"s166879269\">\n<polygon id=\"s166879269\" points=\"440.0,380.0 480.0,380.0 480.0,420.0 440.0,419.99994 \" fill=\"rgb(217,204,140)\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<g id=\"t166879269\">\n<text xml:space=\"preserve\" x=\"454.0\" y=\"407.0664\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"18\">C</text>\n</g>\n</g>\n<g id=\"la119301626\">\n<path id=\"s119301626\" d=\"M 200.0,300.0 240.0,299.99957 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<path id=\"s149566955\" d=\"M 230.82198,307.7303 240.0,299.99957 230.82181,292.26904 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<g id=\"t119301626\">\n<text xml:space=\"preserve\" x=\"220.0\" y=\"292.20584\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"14\"/>\n</g>\n</g>\n<g id=\"la126207915\">\n<path id=\"s126207915\" d=\"M 200.00002,380.0 285.59943,339.99933 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<path id=\"s115916616\" d=\"M 205.04224,369.11075 200.00002,380.0 211.58784,383.11804 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<g id=\"t126207915\">\n<text xml:space=\"preserve\" x=\"246.79971\" y=\"365.49576\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"14\"/>\n</g>\n</g>\n<g id=\"la937480\">\n<path id=\"s937480\" d=\"M 346.39987,339.99936 290.0,380.0 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<path id=\"s85037537\" d=\"M 293.0142,368.3847 290.0,380.0 301.9586,380.9961 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<g id=\"t937480\">\n<text xml:space=\"preserve\" x=\"322.19995\" y=\"365.4958\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"14\"/>\n</g>\n</g>\n<g id=\"la68016545\">\n<path id=\"s68016545\" d=\"M 809.9992,159.99998 799.9999,260.0 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<path id=\"s204752327\" d=\"M 793.2208,250.09827 799.9999,260.0 808.60535,251.63661 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<g id=\"t68016545\">\n<text xml:space=\"preserve\" x=\"808.9996\" y=\"215.4961\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"14\"/>\n</g>\n</g>\n<g id=\"la96135110\">\n<path id=\"s96135110\" d=\"M 519.9999,159.99997 519.9999,260.0 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<path id=\"s191393755\" d=\"M 512.2693,250.8219 519.9999,260.0 527.73047,250.8219 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<g id=\"t96135110\">\n<text xml:space=\"preserve\" x=\"523.9999\" y=\"215.49608\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"14\"/>\n</g>\n</g>\n<g id=\"la207041415\">\n<path id=\"s207041415\" d=\"M 539.9999,140.0 760.0,140.0 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<path id=\"s200182392\" d=\"M 750.8219,147.7306 760.0,140.0 750.8219,132.26938 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<g id=\"t207041415\">\n<text xml:space=\"preserve\" x=\"649.99994\" y=\"132.20604\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"14\"/>\n</g>\n</g>\n<g id=\"la71189172\">\n<path id=\"s71189172\" d=\"M 310.0,220.0 358.74963,260.0 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<path id=\"s59880785\" d=\"M 346.75064,260.15445 358.74963,260.0 356.55798,248.20184 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<g id=\"t71189172\">\n<text xml:space=\"preserve\" x=\"338.37482\" y=\"245.4961\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"14\"/>\n</g>\n</g>\n<g id=\"la34434781\">\n<path id=\"s34434781\" d=\"M 200.00003,220.0 263.74976,260.0 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<path id=\"s133108150\" d=\"M 251.86656,261.67023 263.74976,260.0 260.08408,248.5736 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<g id=\"t34434781\">\n<text xml:space=\"preserve\" x=\"235.8749\" y=\"245.4961\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"14\"/>\n</g>\n</g>\n<g id=\"la57522770\">\n<path id=\"s57522770\" d=\"M 899.99994,339.99985 920.0,360.0 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<path id=\"s16002743\" d=\"M 908.04376,358.97644 920.0,360.0 918.9765,348.04373 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<g id=\"t57522770\">\n<text xml:space=\"preserve\" x=\"914.0\" y=\"355.49603\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"14\"/>\n</g>\n</g>\n<g id=\"la171303934\">\n<path id=\"s171303934\" d=\"M 490.0,500.0 685.3284,441.4805 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<path id=\"s113104420\" d=\"M 496.5734,489.96054 490.0,500.0 501.01065,504.77136 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<g id=\"t171303934\">\n<text xml:space=\"preserve\" x=\"587.6642\" y=\"462.94632\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"14\"/>\n</g>\n</g>\n<g id=\"la32080163\">\n<path id=\"s32080163\" d=\"M 600.0,500.0 700.5025,451.2132 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<path id=\"s166417805\" d=\"M 604.8808,489.0374 600.0,500.0 611.6326,502.94647 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<g id=\"t32080163\">\n<text xml:space=\"preserve\" x=\"654.2512\" y=\"481.1027\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"14\"/>\n</g>\n</g>\n<g id=\"la165074464\">\n<path id=\"s165074464\" d=\"M 723.21216,457.71637 699.99994,500.0 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<path id=\"s89297730\" d=\"M 697.64,488.23434 699.99994,500.0 711.1933,495.67462 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<g id=\"t165074464\">\n<text xml:space=\"preserve\" x=\"715.606\" y=\"484.35428\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"14\"/>\n</g>\n</g>\n<g id=\"la41360345\">\n<path id=\"s41360345\" d=\"M 799.4975,451.2132 900.0,500.0 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<path id=\"s68353875\" d=\"M 888.3674,502.94647 900.0,500.0 895.1192,489.0374 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<g id=\"t41360345\">\n<text xml:space=\"preserve\" x=\"853.7488\" y=\"481.1027\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"14\"/>\n</g>\n</g>\n<g id=\"la168372638\">\n<path id=\"s168372638\" d=\"M 750.0,400.0 740.0,339.99988 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<path id=\"s51654800\" d=\"M 740.8657,392.21768 750.0,400.0 756.1166,389.67587 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<g id=\"t168372638\">\n<text xml:space=\"preserve\" x=\"749.0\" y=\"375.10345\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"13\"/>\n</g>\n</g>\n<g id=\"la248056959\">\n<path id=\"s248056959\" d=\"M 776.78784,457.71637 800.0,500.0 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<path id=\"s96779657\" d=\"M 788.80664,495.67462 800.0,500.0 802.3599,488.23434 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<g id=\"t248056959\">\n<text xml:space=\"preserve\" x=\"792.3939\" y=\"483.9617\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"13\"/>\n</g>\n</g>\n<g id=\"la111359052\">\n<path id=\"s111359052\" d=\"M 480.00003,339.99994 460.0,380.0 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<path id=\"s125477262\" d=\"M 457.1901,368.33362 460.0,380.0 471.01904,375.24808 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<g id=\"t111359052\">\n<text xml:space=\"preserve\" x=\"474.0\" y=\"365.1035\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"13\"/>\n</g>\n</g>\n</svg>\n"
        },
        {
          "name": "mirall.svg",
          "type": "blob",
          "size": 16.4462890625,
          "content": "<?xml version=\"1.0\" encoding=\"UTF-8\"?><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"1000.0\" height=\"520.0\" viewBox=\"130.0 70.0 1000.0 520.0\" version=\"1.1\">\n<rect x=\"130.0\" y=\"70.0\" width=\"1000.0\" height=\"520.0\" fill=\"rgb(255,255,255)\"/>\n<g id=\"s31092904\">\n<polygon id=\"s31092904\" points=\"240.0,260.0 999.99976,260.0 1000.00006,339.99982 240.0,340.0 \" fill=\"rgb(217,204,140)\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<g id=\"t31092904\">\n<text xml:space=\"preserve\" x=\"590.0\" y=\"313.34766\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"34\">MIR</text>\n</g>\n</g>\n<g id=\"s38098881\">\n<polygon id=\"s38098881\" points=\"160.0,280.0 200.0,280.0 199.99998,320.0 160.0,319.99997 \" fill=\"none\" stroke-width=\"0.0\" stroke=\"rgb(102,51,102)\"/>\n<g id=\"t38098881\">\n<text xml:space=\"preserve\" x=\"164.5\" y=\"307.85156\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"20\">API</text>\n</g>\n</g>\n<g id=\"s143219558\">\n<polygon id=\"s143219558\" points=\"500.0,120.0 540.0,120.0 539.99976,160.0 500.0,159.99995 \" fill=\"rgb(217,204,140)\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<g id=\"t143219558\">\n<text xml:space=\"preserve\" x=\"513.5\" y=\"147.85156\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"20\">C</text>\n</g>\n</g>\n<g id=\"s54079143\">\n<polygon id=\"s54079143\" points=\"760.0,120.0 860.0,120.0 859.9984,159.99997 760.0,160.0 \" fill=\"rgb(192,202,51)\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<g id=\"t54079143\">\n<text xml:space=\"preserve\" x=\"773.0\" y=\"147.85156\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"20\">LLVM IR</text>\n</g>\n</g>\n<g id=\"s129625940\">\n<polygon id=\"s129625940\" points=\"380.0,180.0 459.99936,180.0 460.0,219.99994 380.0,220.0 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<g id=\"t129625940\">\n<text xml:space=\"preserve\" x=\"392.5\" y=\"207.0664\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"18\">WASM</text>\n</g>\n</g>\n<g id=\"s962301\">\n<polygon id=\"s962301\" points=\"160.0,380.0 240.00003,380.0 239.99991,440.0 160.0,439.9999 \" fill=\"rgb(217,204,140)\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<g id=\"t962301\">\n<text xml:space=\"preserve\" x=\"183.00002\" y=\"404.58008\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"20\">MIR</text>\n<text xml:space=\"preserve\" x=\"173.00002\" y=\"431.12305\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"20\">binary</text>\n</g>\n</g>\n<g id=\"s71288818\">\n<polygon id=\"s71288818\" points=\"260.0,380.0 320.0,380.0 319.99994,439.9993 260.0,440.0 \" fill=\"rgb(217,204,140)\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<g id=\"t71288818\">\n<text xml:space=\"preserve\" x=\"273.0\" y=\"404.58008\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"20\">MIR</text>\n<text xml:space=\"preserve\" x=\"272.5\" y=\"431.12305\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"20\">text</text>\n</g>\n</g>\n<g id=\"s88084035\">\n<polygon id=\"s88084035\" points=\"560.0,500.0 640.0,500.0 639.9999,560.0 560.0,560.0 \" fill=\"rgb(217,204,140)\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<g id=\"t88084035\">\n<text xml:space=\"preserve\" x=\"573.0\" y=\"537.0664\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"18\">x86-64</text>\n</g>\n</g>\n<g id=\"s191933632\">\n<polygon id=\"s191933632\" points=\"660.0,500.0 739.9999,500.0 740.0,560.0 660.0,559.9997 \" fill=\"rgb(217,204,140)\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<g id=\"t191933632\">\n<text xml:space=\"preserve\" x=\"667.5\" y=\"537.0664\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"18\">aarch64</text>\n</g>\n</g>\n<g id=\"s67776072\">\n<polygon id=\"s67776072\" points=\"440.0,500.0 540.0,500.0 540.0,560.0 440.0,559.9999 \" fill=\"rgb(217,204,140)\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<g id=\"t67776072\">\n<text xml:space=\"preserve\" x=\"453.0\" y=\"537.0664\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"18\">PPC64LE</text>\n</g>\n</g>\n<g id=\"s1850873\">\n<polygon id=\"s1850873\" points=\"860.0,500.0 940.0,500.0 939.99927,560.0 860.0,559.99994 \" fill=\"rgb(217,204,140)\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<g id=\"t1850873\">\n<text xml:space=\"preserve\" x=\"875.0\" y=\"537.0664\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"18\">RISCV</text>\n</g>\n</g>\n<g id=\"s243167550\">\n<polygon id=\"s243167550\" points=\"160.0,160.0 239.99998,160.0 240.00005,220.0 160.0,219.99998 \" fill=\"rgb(217,204,140)\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<g id=\"t243167550\">\n<text xml:space=\"preserve\" x=\"183.00003\" y=\"184.58008\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"20\">MIR</text>\n<text xml:space=\"preserve\" x=\"173.00003\" y=\"211.12305\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"20\">binary</text>\n</g>\n</g>\n<g id=\"s105597855\">\n<polygon id=\"s105597855\" points=\"280.0,160.0 340.0,160.0 340.0,219.99998 280.0,220.0 \" fill=\"rgb(217,204,140)\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<g id=\"t105597855\">\n<text xml:space=\"preserve\" x=\"293.0\" y=\"184.58008\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"20\">MIR</text>\n<text xml:space=\"preserve\" x=\"292.5\" y=\"211.12305\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"20\">text</text>\n</g>\n</g>\n<g id=\"s185849068\">\n<ellipse id=\"s185849068\" cx=\"920.0\" cy=\"390.0\" rx=\"60.0\" ry=\"30.0\" fill=\"rgb(217,204,140)\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<g id=\"t185849068\">\n<text xml:space=\"preserve\" x=\"877.5\" y=\"397.0664\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"18\">Interpreter</text>\n</g>\n</g>\n<g id=\"s102749409\">\n<ellipse id=\"s102749409\" cx=\"750.0\" cy=\"430.0\" rx=\"70.0\" ry=\"30.0\" fill=\"rgb(217,204,140)\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<g id=\"t102749409\">\n<text xml:space=\"preserve\" x=\"705.0\" y=\"437.85156\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"20\">Generator</text>\n</g>\n</g>\n<g id=\"s215597237\">\n<polygon id=\"s215597237\" points=\"1040.0,100.0 1100.0,100.0 1099.9983,119.99997 1040.0,120.0 \" fill=\"none\" stroke-width=\"0.0\" stroke=\"rgb(102,51,102)\"/>\n<g id=\"t215597237\"/>\n</g>\n<g id=\"s40657738\">\n<polygon id=\"s40657738\" points=\"760.0,500.0 840.0,500.0 839.99994,559.9999 760.0,560.0 \" fill=\"rgb(217,204,140)\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<g id=\"t40657738\">\n<text xml:space=\"preserve\" x=\"776.0\" y=\"537.0664\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"18\">s390x</text>\n</g>\n</g>\n<g id=\"s61014715\">\n<polygon id=\"s61014715\" points=\"959.9999,500.0 1059.9999,500.0 1059.9995,559.99976 959.9999,560.0 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<g id=\"t61014715\">\n<text xml:space=\"preserve\" x=\"974.4999\" y=\"525.1221\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"18\">MIPS64/</text>\n<text xml:space=\"preserve\" x=\"966.4999\" y=\"549.01074\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"18\">LoongArch</text>\n</g>\n</g>\n<g id=\"s249517016\">\n<polygon id=\"s249517016\" points=\"340.0,380.0 420.0,380.0 420.0,420.0 340.0,419.99997 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<g id=\"t249517016\">\n<text xml:space=\"preserve\" x=\"352.5\" y=\"407.0664\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"18\">WASM</text>\n</g>\n</g>\n<g id=\"s28488497\">\n<polygon id=\"s28488497\" points=\"440.0,380.0 480.0,380.0 480.0,420.0 440.0,419.99994 \" fill=\"rgb(217,204,140)\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<g id=\"t28488497\">\n<text xml:space=\"preserve\" x=\"454.0\" y=\"407.0664\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"18\">C</text>\n</g>\n</g>\n<g id=\"s76900886\">\n<ellipse id=\"s76900886\" cx=\"599.99994\" cy=\"200.0\" rx=\"40.0\" ry=\"20.0\" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<g id=\"t76900886\">\n<text xml:space=\"preserve\" x=\"581.99994\" y=\"207.0664\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"18\">GCC</text>\n</g>\n</g>\n<g id=\"la217386903\">\n<path id=\"s217386903\" d=\"M 200.0,300.0 240.0,299.99957 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<path id=\"s171764708\" d=\"M 230.82198,307.7303 240.0,299.99957 230.82181,292.26904 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<g id=\"t217386903\">\n<text xml:space=\"preserve\" x=\"220.0\" y=\"292.20584\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"14\"/>\n</g>\n</g>\n<g id=\"la81909124\">\n<path id=\"s81909124\" d=\"M 200.00002,380.0 285.59943,339.99933 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<path id=\"s185073410\" d=\"M 205.04224,369.11075 200.00002,380.0 211.58784,383.11804 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<g id=\"t81909124\">\n<text xml:space=\"preserve\" x=\"246.79971\" y=\"365.49576\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"14\"/>\n</g>\n</g>\n<g id=\"la189682029\">\n<path id=\"s189682029\" d=\"M 346.39987,339.99936 290.0,380.0 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<path id=\"s262561811\" d=\"M 293.0142,368.3847 290.0,380.0 301.9586,380.9961 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<g id=\"t189682029\">\n<text xml:space=\"preserve\" x=\"322.19995\" y=\"365.4958\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"14\"/>\n</g>\n</g>\n<g id=\"la74846626\">\n<path id=\"s74846626\" d=\"M 809.9992,159.99998 799.9999,260.0 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<path id=\"s214884944\" d=\"M 793.2208,250.09827 799.9999,260.0 808.60535,251.63661 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<g id=\"t74846626\">\n<text xml:space=\"preserve\" x=\"808.9996\" y=\"215.4961\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"14\"/>\n</g>\n</g>\n<g id=\"la82906675\">\n<path id=\"s82906675\" d=\"M 519.9999,159.99997 519.9999,260.0 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<path id=\"s116059209\" d=\"M 512.2693,250.8219 519.9999,260.0 527.73047,250.8219 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<g id=\"t82906675\">\n<text xml:space=\"preserve\" x=\"523.9999\" y=\"215.49608\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"14\"/>\n</g>\n</g>\n<g id=\"la191415792\">\n<path id=\"s191415792\" d=\"M 420.0,219.99997 439.99988,260.0 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<path id=\"s259721550\" d=\"M 428.98083,255.24806 439.99988,260.0 442.8098,248.33362 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<g id=\"t191415792\">\n<text xml:space=\"preserve\" x=\"433.99994\" y=\"245.49608\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"14\"/>\n</g>\n</g>\n<g id=\"la260806505\">\n<path id=\"s260806505\" d=\"M 539.9999,140.0 760.0,140.0 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<path id=\"s76343919\" d=\"M 750.8219,147.7306 760.0,140.0 750.8219,132.26938 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<g id=\"t260806505\">\n<text xml:space=\"preserve\" x=\"649.99994\" y=\"132.20604\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"14\"/>\n</g>\n</g>\n<g id=\"la180856302\">\n<path id=\"s180856302\" d=\"M 310.0,220.0 358.74963,260.0 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<path id=\"s4707196\" d=\"M 346.75064,260.15445 358.74963,260.0 356.55798,248.20184 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<g id=\"t180856302\">\n<text xml:space=\"preserve\" x=\"338.37482\" y=\"245.4961\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"14\"/>\n</g>\n</g>\n<g id=\"la262429327\">\n<path id=\"s262429327\" d=\"M 200.00003,220.0 263.74976,260.0 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<path id=\"s250414085\" d=\"M 251.86656,261.67023 263.74976,260.0 260.08408,248.5736 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<g id=\"t262429327\">\n<text xml:space=\"preserve\" x=\"235.8749\" y=\"245.4961\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"14\"/>\n</g>\n</g>\n<g id=\"la103567644\">\n<path id=\"s103567644\" d=\"M 899.99994,339.99985 920.0,360.0 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<path id=\"s50490458\" d=\"M 908.04376,358.97644 920.0,360.0 918.9765,348.04373 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<g id=\"t103567644\">\n<text xml:space=\"preserve\" x=\"914.0\" y=\"355.49603\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"14\"/>\n</g>\n</g>\n<g id=\"la251658475\">\n<path id=\"s251658475\" d=\"M 490.0,500.0 685.3284,441.4805 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<path id=\"s212319883\" d=\"M 496.5734,489.96054 490.0,500.0 501.01065,504.77136 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<g id=\"t251658475\">\n<text xml:space=\"preserve\" x=\"587.6642\" y=\"462.94632\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"14\"/>\n</g>\n</g>\n<g id=\"la155009317\">\n<path id=\"s155009317\" d=\"M 600.0,500.0 700.5025,451.2132 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<path id=\"s167553896\" d=\"M 604.8808,489.0374 600.0,500.0 611.6326,502.94647 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<g id=\"t155009317\">\n<text xml:space=\"preserve\" x=\"654.2512\" y=\"481.1027\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"14\"/>\n</g>\n</g>\n<g id=\"la9188090\">\n<path id=\"s9188090\" d=\"M 723.21216,457.71637 699.99994,500.0 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<path id=\"s121559937\" d=\"M 697.64,488.23434 699.99994,500.0 711.1933,495.67462 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<g id=\"t9188090\">\n<text xml:space=\"preserve\" x=\"715.606\" y=\"484.35428\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"14\"/>\n</g>\n</g>\n<g id=\"la177606827\">\n<path id=\"s177606827\" d=\"M 799.4975,451.2132 900.0,500.0 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<path id=\"s57340966\" d=\"M 888.3674,502.94647 900.0,500.0 895.1192,489.0374 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<g id=\"t177606827\">\n<text xml:space=\"preserve\" x=\"853.7488\" y=\"481.1027\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"14\"/>\n</g>\n</g>\n<g id=\"la171664136\">\n<path id=\"s171664136\" d=\"M 750.0,400.0 740.0,339.99988 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<path id=\"s106018919\" d=\"M 740.8657,392.21768 750.0,400.0 756.1166,389.67587 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<g id=\"t171664136\">\n<text xml:space=\"preserve\" x=\"749.0\" y=\"375.10345\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"13\"/>\n</g>\n</g>\n<g id=\"la61196961\">\n<path id=\"s61196961\" d=\"M 776.78784,457.71637 800.0,500.0 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<path id=\"s228922900\" d=\"M 788.80664,495.67462 800.0,500.0 802.3599,488.23434 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<g id=\"t61196961\">\n<text xml:space=\"preserve\" x=\"792.3939\" y=\"483.9617\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"13\"/>\n</g>\n</g>\n<g id=\"la182092998\">\n<path id=\"s182092998\" d=\"M 814.6716,441.4805 1009.9999,500.0 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<path id=\"s58502845\" d=\"M 998.98926,504.77136 1009.9999,500.0 1003.4265,489.96054 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<g id=\"t182092998\">\n<text xml:space=\"preserve\" x=\"912.3357\" y=\"463.21732\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"13\"/>\n</g>\n</g>\n<g id=\"la178457735\">\n<path id=\"s178457735\" d=\"M 429.9995,339.9995 380.0,380.0 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<path id=\"s209857714\" d=\"M 382.3375,368.22986 380.0,380.0 391.9962,380.30295 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<g id=\"t178457735\">\n<text xml:space=\"preserve\" x=\"408.99976\" y=\"365.10327\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"13\"/>\n</g>\n</g>\n<g id=\"la212314036\">\n<path id=\"s212314036\" d=\"M 480.00003,339.99994 460.0,380.0 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<path id=\"s12022787\" d=\"M 457.1901,368.33362 460.0,380.0 471.01904,375.24808 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<g id=\"t212314036\">\n<text xml:space=\"preserve\" x=\"474.0\" y=\"365.1035\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"13\"/>\n</g>\n</g>\n<g id=\"la28536797\">\n<path id=\"s28536797\" d=\"M 539.99976,160.0 584.6926,181.52242 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<path id=\"s13074304\" d=\"M 573.0693,184.50534 584.6926,181.52242 579.7775,170.5752 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<g id=\"t28536797\">\n<text xml:space=\"preserve\" x=\"566.3462\" y=\"175.86472\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"13\"/>\n</g>\n</g>\n<g id=\"la124397394\">\n<path id=\"s124397394\" d=\"M 599.99994,220.0 599.9999,260.0 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<path id=\"s181041593\" d=\"M 592.2693,250.82188 599.9999,260.0 607.7305,250.8219 \" fill=\"none\" stroke-width=\"2.0\" stroke=\"rgb(102,51,102)\"/>\n<g id=\"t124397394\">\n<text xml:space=\"preserve\" x=\"603.9999\" y=\"245.10352\" font-family=\"Sans,Helvetica\" fill=\"rgb(102,51,102)\" font-size=\"13\"/>\n</g>\n</g>\n</svg>\n"
        },
        {
          "name": "real-time.h",
          "type": "blob",
          "size": 1.0478515625,
          "content": "/* This file is a part of MIR project.\n   Copyright (C) 2018-2024 Vladimir Makarov <vmakarov.gcc@gmail.com> and logzero <core13@gmx.net>\n*/\n\n#ifndef _WIN32\n#include <sys/time.h>\n\nstatic double __attribute__ ((unused)) real_sec_time (void) {\n  struct timeval tv;\n\n  gettimeofday (&tv, NULL);\n  return tv.tv_usec / 1000000.0 + tv.tv_sec;\n}\n\nstatic double __attribute__ ((unused)) real_usec_time (void) {\n  struct timeval tv;\n\n  gettimeofday (&tv, NULL);\n  return tv.tv_usec + tv.tv_sec * 1000000.0;\n}\n#else\n#define WIN32_LEAN_AND_MEAN\n#include <windows.h>\n\n// does not return actual time, use as a stopwatch only\nstatic double real_sec_time (void) {\n  LARGE_INTEGER freq, count;\n\n  if (QueryPerformanceFrequency (&freq) && QueryPerformanceCounter (&count))\n    return (double) count.QuadPart / (double) freq.QuadPart;\n  return 0.0;\n}\n\nstatic double real_usec_time (void) {\n  LARGE_INTEGER freq, count;\n\n  if (QueryPerformanceFrequency (&freq) && QueryPerformanceCounter (&count))\n    return (double) count.QuadPart / ((double) freq.QuadPart * 1.0E-6);\n  return 0.0;\n}\n#endif\n"
        },
        {
          "name": "sieve.c",
          "type": "blob",
          "size": 0.7841796875,
          "content": "void printf (const char *fmt, ...);\nvoid abort (void);\n#if defined(_WIN32) || !defined(SIEVE_BENCH)\n/* limitation for alloca */\n#define SieveSize 8190\n#define Expected 1027\n#else\n#define SieveSize 819000\n#define Expected 65333\n#endif\n#define N_ITER 1000\nint sieve (int n) {\n  long i, k, count, iter, prime;\n  char flags[SieveSize];\n\n  for (iter = 0; iter < n; iter++) {\n    count = 0;\n    for (i = 0; i < SieveSize; i++) flags[i] = 1;\n    for (i = 2; i < SieveSize; i++)\n      if (flags[i]) {\n        prime = i + 1;\n        for (k = i + prime; k < SieveSize; k += prime) flags[k] = 0;\n        count++;\n      }\n  }\n  return count;\n}\nint main (void) {\n  int n = sieve (N_ITER);\n\n  printf (\"%d iterations of sieve for %d: result = %d\\n\", N_ITER, SieveSize, n);\n  if (n != Expected) abort ();\n  return 0;\n}\n"
        }
      ]
    }
  ]
}