{
  "metadata": {
    "timestamp": 1736710005713,
    "page": 594,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjYwMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "glouw/tinn",
      "stars": 2112,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.0322265625,
          "content": "*.dat*\n*.txt\n*.o\n*.d\ntest\n*.tinn\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.04296875,
          "content": "MIT License\n\nCopyright (c) 2018 Gustav Louw\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 0.2109375,
          "content": "BIN = test\n\nCFLAGS = -std=c99 -Wall -Wextra -pedantic -Ofast -flto -march=native\n\nLDFLAGS = -lm\n\nCC = gcc\n\nSRC = test.c Tinn.c\n\nall:\n\t$(CC) -o $(BIN) $(SRC) $(CFLAGS) $(LDFLAGS)\n\nrun:\n\t./$(BIN)\n\nclean:\n\trm -f $(BIN)\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 2.4990234375,
          "content": "![](img/logo.PNG)\n\nTinn (Tiny Neural Network) is a 200 line dependency free neural network library written in C99.\n\nFor a demo on how to learn hand written digits, get some training data:\n\n    wget http://archive.ics.uci.edu/ml/machine-learning-databases/semeion/semeion.data\n\n    make; ./test\n\nThe training data consists of hand written digits written both slowly and quickly.\nEach line in the data set corresponds to one handwritten digit. Each digit is 16x16 pixels in size\ngiving 256 inputs to the neural network.\n\nAt the end of the line 10 digits signify the hand written digit:\n\n    0: 1 0 0 0 0 0 0 0 0 0\n    1: 0 1 0 0 0 0 0 0 0 0\n    2: 0 0 1 0 0 0 0 0 0 0\n    3: 0 0 0 1 0 0 0 0 0 0\n    4: 0 0 0 0 1 0 0 0 0 0\n    ...\n    9: 0 0 0 0 0 0 0 0 0 1\n\nThis gives 10 outputs to the neural network. The test program will output the\naccuracy for each digit. Expect above 99% accuracy for the correct digit, and\nless that 0.1% accuracy for the other digits.\n\n## Features\n\n* Portable - Runs where a C99 or C++98 compiler is present.\n\n* Sigmoidal activation.\n\n* One hidden layer.\n\n## Tips\n\n* Tinn will never use more than the C standard library.\n\n* Tinn is great for embedded systems. Train a model on your powerful desktop and load\nit onto a microcontroller and use the analog to digital converter to predict real time events.\n\n* The Tinn source code will always be less than 200 lines. Functions externed in the Tinn header\nare protected with the _xt_ namespace standing for _externed tinn_.\n\n* Tinn can easily be multi-threaded with a bit of ingenuity but the master branch will remain\nsingle threaded to aid development for embedded systems.\n\n* Tinn does not seed the random number generator. Do not forget to do so yourself.\n\n* Always shuffle your input data. Shuffle again after every training iteration.\n\n* Get greater training accuracy by annealing your learning rate. For instance, multiply\nyour learning rate by 0.99 every training iteration. This will zero in on a good learning minima.\n\n## Disclaimer\n\nTinn is a practice in minimalism.\n\nTinn is not a fully featured neural network C library like Kann, or Genann:\n\n    https://github.com/attractivechaos/kann\n\n    https://github.com/codeplea/genann\n\n## Ports\n\n    Rust: https://github.com/dvdplm/rustinn\n\n## Other\n\n [A Tutorial using Tinn NN and CTypes](https://medium.com/@cknorow/creating-a-python-interface-to-a-c-library-a-tutorial-using-tinn-nn-d935707dd225)\n\n [Tiny Neural Network Library in 200 Lines of Code](https://hackaday.com/2018/04/08/tiny-neural-network-library-in-200-lines-of-code/)\n"
        },
        {
          "name": "Tinn.c",
          "type": "blob",
          "size": 4.52734375,
          "content": "#include \"Tinn.h\"\n\n#include <stdarg.h>\n#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n\n// Computes error.\nstatic float err(const float a, const float b)\n{\n    return 0.5f * (a - b) * (a - b);\n}\n\n// Returns partial derivative of error function.\nstatic float pderr(const float a, const float b)\n{\n    return a - b;\n}\n\n// Computes total error of target to output.\nstatic float toterr(const float* const tg, const float* const o, const int size)\n{\n    float sum = 0.0f;\n    for(int i = 0; i < size; i++)\n        sum += err(tg[i], o[i]);\n    return sum;\n}\n\n// Activation function.\nstatic float act(const float a)\n{\n    return 1.0f / (1.0f + expf(-a));\n}\n\n// Returns partial derivative of activation function.\nstatic float pdact(const float a)\n{\n    return a * (1.0f - a);\n}\n\n// Returns floating point random from 0.0 - 1.0.\nstatic float frand()\n{\n    return rand() / (float) RAND_MAX;\n}\n\n// Performs back propagation.\nstatic void bprop(const Tinn t, const float* const in, const float* const tg, float rate)\n{\n    for(int i = 0; i < t.nhid; i++)\n    {\n        float sum = 0.0f;\n        // Calculate total error change with respect to output.\n        for(int j = 0; j < t.nops; j++)\n        {\n            const float a = pderr(t.o[j], tg[j]);\n            const float b = pdact(t.o[j]);\n            sum += a * b * t.x[j * t.nhid + i];\n            // Correct weights in hidden to output layer.\n            t.x[j * t.nhid + i] -= rate * a * b * t.h[i];\n        }\n        // Correct weights in input to hidden layer.\n        for(int j = 0; j < t.nips; j++)\n            t.w[i * t.nips + j] -= rate * sum * pdact(t.h[i]) * in[j];\n    }\n}\n\n// Performs forward propagation.\nstatic void fprop(const Tinn t, const float* const in)\n{\n    // Calculate hidden layer neuron values.\n    for(int i = 0; i < t.nhid; i++)\n    {\n        float sum = 0.0f;\n        for(int j = 0; j < t.nips; j++)\n            sum += in[j] * t.w[i * t.nips + j];\n        t.h[i] = act(sum + t.b[0]);\n    }\n    // Calculate output layer neuron values.\n    for(int i = 0; i < t.nops; i++)\n    {\n        float sum = 0.0f;\n        for(int j = 0; j < t.nhid; j++)\n            sum += t.h[j] * t.x[i * t.nhid + j];\n        t.o[i] = act(sum + t.b[1]);\n    }\n}\n\n// Randomizes tinn weights and biases.\nstatic void wbrand(const Tinn t)\n{\n    for(int i = 0; i < t.nw; i++) t.w[i] = frand() - 0.5f;\n    for(int i = 0; i < t.nb; i++) t.b[i] = frand() - 0.5f;\n}\n\n// Returns an output prediction given an input.\nfloat* xtpredict(const Tinn t, const float* const in)\n{\n    fprop(t, in);\n    return t.o;\n}\n\n// Trains a tinn with an input and target output with a learning rate. Returns target to output error.\nfloat xttrain(const Tinn t, const float* const in, const float* const tg, float rate)\n{\n    fprop(t, in);\n    bprop(t, in, tg, rate);\n    return toterr(tg, t.o, t.nops);\n}\n\n// Constructs a tinn with number of inputs, number of hidden neurons, and number of outputs\nTinn xtbuild(const int nips, const int nhid, const int nops)\n{\n    Tinn t;\n    // Tinn only supports one hidden layer so there are two biases.\n    t.nb = 2;\n    t.nw = nhid * (nips + nops);\n    t.w = (float*) calloc(t.nw, sizeof(*t.w));\n    t.x = t.w + nhid * nips;\n    t.b = (float*) calloc(t.nb, sizeof(*t.b));\n    t.h = (float*) calloc(nhid, sizeof(*t.h));\n    t.o = (float*) calloc(nops, sizeof(*t.o));\n    t.nips = nips;\n    t.nhid = nhid;\n    t.nops = nops;\n    wbrand(t);\n    return t;\n}\n\n// Saves a tinn to disk.\nvoid xtsave(const Tinn t, const char* const path)\n{\n    FILE* const file = fopen(path, \"w\");\n    // Save header.\n    fprintf(file, \"%d %d %d\\n\", t.nips, t.nhid, t.nops);\n    // Save biases and weights.\n    for(int i = 0; i < t.nb; i++) fprintf(file, \"%f\\n\", (double) t.b[i]);\n    for(int i = 0; i < t.nw; i++) fprintf(file, \"%f\\n\", (double) t.w[i]);\n    fclose(file);\n}\n\n// Loads a tinn from disk.\nTinn xtload(const char* const path)\n{\n    FILE* const file = fopen(path, \"r\");\n    int nips = 0;\n    int nhid = 0;\n    int nops = 0;\n    // Load header.\n    fscanf(file, \"%d %d %d\\n\", &nips, &nhid, &nops);\n    // Build a new tinn.\n    const Tinn t = xtbuild(nips, nhid, nops);\n    // Load bias and weights.\n    for(int i = 0; i < t.nb; i++) fscanf(file, \"%f\\n\", &t.b[i]);\n    for(int i = 0; i < t.nw; i++) fscanf(file, \"%f\\n\", &t.w[i]);\n    fclose(file);\n    return t;\n}\n\n// Frees object from heap.\nvoid xtfree(const Tinn t)\n{\n    free(t.w);\n    free(t.b);\n    free(t.h);\n    free(t.o);\n}\n\n// Prints an array of floats. Useful for printing predictions.\nvoid xtprint(const float* arr, const int size)\n{\n    for(int i = 0; i < size; i++)\n        printf(\"%f \", (double) arr[i]);\n    printf(\"\\n\");\n}\n"
        },
        {
          "name": "Tinn.h",
          "type": "blob",
          "size": 0.76171875,
          "content": "#pragma once\n\ntypedef struct\n{\n    // All the weights.\n    float* w;\n    // Hidden to output layer weights.\n    float* x;\n    // Biases.\n    float* b;\n    // Hidden layer.\n    float* h;\n    // Output layer.\n    float* o;\n    // Number of biases - always two - Tinn only supports a single hidden layer.\n    int nb;\n    // Number of weights.\n    int nw;\n    // Number of inputs.\n    int nips;\n    // Number of hidden neurons.\n    int nhid;\n    // Number of outputs.\n    int nops;\n}\nTinn;\n\nfloat* xtpredict(Tinn, const float* in);\n\nfloat xttrain(Tinn, const float* in, const float* tg, float rate);\n\nTinn xtbuild(int nips, int nhid, int nops);\n\nvoid xtsave(Tinn, const char* path);\n\nTinn xtload(const char* path);\n\nvoid xtfree(Tinn);\n\nvoid xtprint(const float* arr, const int size);\n"
        },
        {
          "name": "img",
          "type": "tree",
          "content": null
        },
        {
          "name": "test.c",
          "type": "blob",
          "size": 5.4697265625,
          "content": "#include \"Tinn.h\"\n#include <stdio.h>\n#include <time.h>\n#include <string.h>\n#include <stdlib.h>\n\n// Data object.\ntypedef struct\n{\n    // 2D floating point array of input.\n    float** in;\n    // 2D floating point array of target.\n    float** tg;\n    // Number of inputs to neural network.\n    int nips;\n    // Number of outputs to neural network.\n    int nops;\n    // Number of rows in file (number of sets for neural network).\n    int rows;\n}\nData;\n\n// Returns the number of lines in a file.\nstatic int lns(FILE* const file)\n{\n    int ch = EOF;\n    int lines = 0;\n    int pc = '\\n';\n    while((ch = getc(file)) != EOF)\n    {\n        if(ch == '\\n')\n            lines++;\n        pc = ch;\n    }\n    if(pc != '\\n')\n        lines++;\n    rewind(file);\n    return lines;\n}\n\n// Reads a line from a file.\nstatic char* readln(FILE* const file)\n{\n    int ch = EOF;\n    int reads = 0;\n    int size = 128;\n    char* line = (char*) malloc((size) * sizeof(char));\n    while((ch = getc(file)) != '\\n' && ch != EOF)\n    {\n        line[reads++] = ch;\n        if(reads + 1 == size)\n            line = (char*) realloc((line), (size *= 2) * sizeof(char));\n    }\n    line[reads] = '\\0';\n    return line;\n}\n\n// New 2D array of floats.\nstatic float** new2d(const int rows, const int cols)\n{\n    float** row = (float**) malloc((rows) * sizeof(float*));\n    for(int r = 0; r < rows; r++)\n        row[r] = (float*) malloc((cols) * sizeof(float));\n    return row;\n}\n\n// New data object.\nstatic Data ndata(const int nips, const int nops, const int rows)\n{\n    const Data data = {\n        new2d(rows, nips), new2d(rows, nops), nips, nops, rows\n    };\n    return data;\n}\n\n// Gets one row of inputs and outputs from a string.\nstatic void parse(const Data data, char* line, const int row)\n{\n    const int cols = data.nips + data.nops;\n    for(int col = 0; col < cols; col++)\n    {\n        const float val = atof(strtok(col == 0 ? line : NULL, \" \"));\n        if(col < data.nips)\n            data.in[row][col] = val;\n        else\n            data.tg[row][col - data.nips] = val;\n    }\n}\n\n// Frees a data object from the heap.\nstatic void dfree(const Data d)\n{\n    for(int row = 0; row < d.rows; row++)\n    {\n        free(d.in[row]);\n        free(d.tg[row]);\n    }\n    free(d.in);\n    free(d.tg);\n}\n\n// Randomly shuffles a data object.\nstatic void shuffle(const Data d)\n{\n    for(int a = 0; a < d.rows; a++)\n    {\n        const int b = rand() % d.rows;\n        float* ot = d.tg[a];\n        float* it = d.in[a];\n        // Swap output.\n        d.tg[a] = d.tg[b];\n        d.tg[b] = ot;\n        // Swap input.\n        d.in[a] = d.in[b];\n        d.in[b] = it;\n    }\n}\n\n// Parses file from path getting all inputs and outputs for the neural network. Returns data object.\nstatic Data build(const char* path, const int nips, const int nops)\n{\n    FILE* file = fopen(path, \"r\");\n    if(file == NULL)\n    {\n        printf(\"Could not open %s\\n\", path);\n        printf(\"Get it from the machine learning database: \");\n        printf(\"wget http://archive.ics.uci.edu/ml/machine-learning-databases/semeion/semeion.data\\n\");\n        exit(1);\n    }\n    const int rows = lns(file);\n    Data data = ndata(nips, nops, rows);\n    for(int row = 0; row < rows; row++)\n    {\n        char* line = readln(file);\n        parse(data, line, row);\n        free(line);\n    }\n    fclose(file);\n    return data;\n}\n\n// Learns and predicts hand written digits with 98% accuracy.\nint main()\n{\n    // Tinn does not seed the random number generator.\n    srand(time(0));\n    // Input and output size is harded coded here as machine learning\n    // repositories usually don't include the input and output size in the data itself.\n    const int nips = 256;\n    const int nops = 10;\n    // Hyper Parameters.\n    // Learning rate is annealed and thus not constant.\n    // It can be fine tuned along with the number of hidden layers.\n    // Feel free to modify the anneal rate.\n    // The number of iterations can be changed for stronger training.\n    float rate = 1.0f;\n    const int nhid = 28;\n    const float anneal = 0.99f;\n    const int iterations = 128;\n    // Load the training set.\n    const Data data = build(\"semeion.data\", nips, nops);\n    // Train, baby, train.\n    const Tinn tinn = xtbuild(nips, nhid, nops);\n    for(int i = 0; i < iterations; i++)\n    {\n        shuffle(data);\n        float error = 0.0f;\n        for(int j = 0; j < data.rows; j++)\n        {\n            const float* const in = data.in[j];\n            const float* const tg = data.tg[j];\n            error += xttrain(tinn, in, tg, rate);\n        }\n        printf(\"error %.12f :: learning rate %f\\n\",\n            (double) error / data.rows,\n            (double) rate);\n        rate *= anneal;\n    }\n    // This is how you save the neural network to disk.\n    xtsave(tinn, \"saved.tinn\");\n    xtfree(tinn);\n    // This is how you load the neural network from disk.\n    const Tinn loaded = xtload(\"saved.tinn\");\n    // Now we do a prediction with the neural network we loaded from disk.\n    // Ideally, we would also load a testing set to make the prediction with,\n    // but for the sake of brevity here we just reuse the training set from earlier.\n    // One data set is picked at random (zero index of input and target arrays is enough\n    // as they were both shuffled earlier).\n    const float* const in = data.in[0];\n    const float* const tg = data.tg[0];\n    const float* const pd = xtpredict(loaded, in);\n    // Prints target.\n    xtprint(tg, data.nops);\n    // Prints prediction.\n    xtprint(pd, data.nops);\n    // All done. Let's clean up.\n    xtfree(loaded);\n    dfree(data);\n    return 0;\n}\n"
        }
      ]
    }
  ]
}