{
  "metadata": {
    "timestamp": 1736710007395,
    "page": 600,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjYwMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "wiseman/py-webrtcvad",
      "stars": 2107,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.7705078125,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nenv/\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\n*.egg-info/\n.installed.cfg\n*.egg\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*,cover\n.hypothesis/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n#Ipython Notebook\n.ipynb_checkpoints\n\n# Emacs backup files\n*~\n"
        },
        {
          "name": ".travis.yml",
          "type": "blob",
          "size": 0.251953125,
          "content": "language: python\npython:\n  - \"2.7\"\n  - \"3.3\"\n  - \"3.4\"\n  - \"3.5\"\n  - \"3.6\"\nmatrix:\n  include:\n    - python: \"3.7\"\n      dist: xenial\n      sudo: true\n# command to install dependencies\ninstall: \"pip install -e .[dev]\"\n# command to run tests\nscript: nosetests\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 2.6513671875,
          "content": "The MIT License (MIT)\n\nCopyright (c) 2016 John Wiseman\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n\nThe VAD code under `cbits/webrtc/` is from the WebRTC project, and has\nthe following license:\n\n============================\nCopyright (c) 2011, The WebRTC project authors. All rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are\nmet:\n\n  * Redistributions of source code must retain the above copyright\n    notice, this list of conditions and the following disclaimer.\n\n  * Redistributions in binary form must reproduce the above copyright\n    notice, this list of conditions and the following disclaimer in\n    the documentation and/or other materials provided with the\n    distribution.\n\n  * Neither the name of Google nor the names of its contributors may\n    be used to endorse or promote products derived from this software\n    without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n\"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\nLIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\nA PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\nHOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\nSPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\nLIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\nDATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\nTHEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n"
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.0693359375,
          "content": "include *.py\ninclude *.raw\ninclude LICENSE\nrecursive-include cbits *.h\n"
        },
        {
          "name": "README.rst",
          "type": "blob",
          "size": 2.0703125,
          "content": ".. image:: https://travis-ci.org/wiseman/py-webrtcvad.svg?branch=master\n    :target: https://travis-ci.org/wiseman/py-webrtcvad\n\npy-webrtcvad\n============\n\nThis is a python interface to the WebRTC Voice Activity Detector\n(VAD).  It is compatible with Python 2 and Python 3.\n\nA `VAD <https://en.wikipedia.org/wiki/Voice_activity_detection>`_\nclassifies a piece of audio data as being voiced or unvoiced. It can\nbe useful for telephony and speech recognition.\n\nThe VAD that Google developed for the `WebRTC <https://webrtc.org/>`_\nproject is reportedly one of the best available, being fast, modern\nand free.\n\nHow to use it\n-------------\n\n0. Install the webrtcvad module::\n\n    pip install webrtcvad\n\n1. Create a ``Vad`` object::\n\n    import webrtcvad\n    vad = webrtcvad.Vad()\n\n2. Optionally, set its aggressiveness mode, which is an integer\n   between 0 and 3. 0 is the least aggressive about filtering out\n   non-speech, 3 is the most aggressive. (You can also set the mode\n   when you create the VAD, e.g. ``vad = webrtcvad.Vad(3)``)::\n\n    vad.set_mode(1)\n\n3. Give it a short segment (\"frame\") of audio. The WebRTC VAD only\n   accepts 16-bit mono PCM audio, sampled at 8000, 16000, 32000 or 48000 Hz.\n   A frame must be either 10, 20, or 30 ms in duration::\n\n    # Run the VAD on 10 ms of silence. The result should be False.\n    sample_rate = 16000\n    frame_duration = 10  # ms\n    frame = b'\\x00\\x00' * int(sample_rate * frame_duration / 1000)\n    print 'Contains speech: %s' % (vad.is_speech(frame, sample_rate)\n\n\nSee `example.py\n<https://github.com/wiseman/py-webrtcvad/blob/master/example.py>`_ for\na more detailed example that will process a .wav file, find the voiced\nsegments, and write each one as a separate .wav.\n\n\nHow to run unit tests\n---------------------\n\nTo run unit tests::\n\n    pip install -e \".[dev]\"\n    python setup.py test\n\n\nHistory\n-------\n\n2.0.10\n\n    Fixed memory leak. Thank you, `bond005\n    <https://github.com/bond005>`_!\n\n2.0.9\n\n    Improved example code. Added WebRTC license.\n\n2.0.8\n\n    Fixed Windows compilation errors. Thank you, `xiongyihui\n    <https://github.com/xiongyihui>`_!\n"
        },
        {
          "name": "cbits",
          "type": "tree",
          "content": null
        },
        {
          "name": "example.py",
          "type": "blob",
          "size": 5.5361328125,
          "content": "import collections\nimport contextlib\nimport sys\nimport wave\n\nimport webrtcvad\n\n\ndef read_wave(path):\n    \"\"\"Reads a .wav file.\n\n    Takes the path, and returns (PCM audio data, sample rate).\n    \"\"\"\n    with contextlib.closing(wave.open(path, 'rb')) as wf:\n        num_channels = wf.getnchannels()\n        assert num_channels == 1\n        sample_width = wf.getsampwidth()\n        assert sample_width == 2\n        sample_rate = wf.getframerate()\n        assert sample_rate in (8000, 16000, 32000, 48000)\n        pcm_data = wf.readframes(wf.getnframes())\n        return pcm_data, sample_rate\n\n\ndef write_wave(path, audio, sample_rate):\n    \"\"\"Writes a .wav file.\n\n    Takes path, PCM audio data, and sample rate.\n    \"\"\"\n    with contextlib.closing(wave.open(path, 'wb')) as wf:\n        wf.setnchannels(1)\n        wf.setsampwidth(2)\n        wf.setframerate(sample_rate)\n        wf.writeframes(audio)\n\n\nclass Frame(object):\n    \"\"\"Represents a \"frame\" of audio data.\"\"\"\n    def __init__(self, bytes, timestamp, duration):\n        self.bytes = bytes\n        self.timestamp = timestamp\n        self.duration = duration\n\n\ndef frame_generator(frame_duration_ms, audio, sample_rate):\n    \"\"\"Generates audio frames from PCM audio data.\n\n    Takes the desired frame duration in milliseconds, the PCM data, and\n    the sample rate.\n\n    Yields Frames of the requested duration.\n    \"\"\"\n    n = int(sample_rate * (frame_duration_ms / 1000.0) * 2)\n    offset = 0\n    timestamp = 0.0\n    duration = (float(n) / sample_rate) / 2.0\n    while offset + n < len(audio):\n        yield Frame(audio[offset:offset + n], timestamp, duration)\n        timestamp += duration\n        offset += n\n\n\ndef vad_collector(sample_rate, frame_duration_ms,\n                  padding_duration_ms, vad, frames):\n    \"\"\"Filters out non-voiced audio frames.\n\n    Given a webrtcvad.Vad and a source of audio frames, yields only\n    the voiced audio.\n\n    Uses a padded, sliding window algorithm over the audio frames.\n    When more than 90% of the frames in the window are voiced (as\n    reported by the VAD), the collector triggers and begins yielding\n    audio frames. Then the collector waits until 90% of the frames in\n    the window are unvoiced to detrigger.\n\n    The window is padded at the front and back to provide a small\n    amount of silence or the beginnings/endings of speech around the\n    voiced frames.\n\n    Arguments:\n\n    sample_rate - The audio sample rate, in Hz.\n    frame_duration_ms - The frame duration in milliseconds.\n    padding_duration_ms - The amount to pad the window, in milliseconds.\n    vad - An instance of webrtcvad.Vad.\n    frames - a source of audio frames (sequence or generator).\n\n    Returns: A generator that yields PCM audio data.\n    \"\"\"\n    num_padding_frames = int(padding_duration_ms / frame_duration_ms)\n    # We use a deque for our sliding window/ring buffer.\n    ring_buffer = collections.deque(maxlen=num_padding_frames)\n    # We have two states: TRIGGERED and NOTTRIGGERED. We start in the\n    # NOTTRIGGERED state.\n    triggered = False\n\n    voiced_frames = []\n    for frame in frames:\n        is_speech = vad.is_speech(frame.bytes, sample_rate)\n\n        sys.stdout.write('1' if is_speech else '0')\n        if not triggered:\n            ring_buffer.append((frame, is_speech))\n            num_voiced = len([f for f, speech in ring_buffer if speech])\n            # If we're NOTTRIGGERED and more than 90% of the frames in\n            # the ring buffer are voiced frames, then enter the\n            # TRIGGERED state.\n            if num_voiced > 0.9 * ring_buffer.maxlen:\n                triggered = True\n                sys.stdout.write('+(%s)' % (ring_buffer[0][0].timestamp,))\n                # We want to yield all the audio we see from now until\n                # we are NOTTRIGGERED, but we have to start with the\n                # audio that's already in the ring buffer.\n                for f, s in ring_buffer:\n                    voiced_frames.append(f)\n                ring_buffer.clear()\n        else:\n            # We're in the TRIGGERED state, so collect the audio data\n            # and add it to the ring buffer.\n            voiced_frames.append(frame)\n            ring_buffer.append((frame, is_speech))\n            num_unvoiced = len([f for f, speech in ring_buffer if not speech])\n            # If more than 90% of the frames in the ring buffer are\n            # unvoiced, then enter NOTTRIGGERED and yield whatever\n            # audio we've collected.\n            if num_unvoiced > 0.9 * ring_buffer.maxlen:\n                sys.stdout.write('-(%s)' % (frame.timestamp + frame.duration))\n                triggered = False\n                yield b''.join([f.bytes for f in voiced_frames])\n                ring_buffer.clear()\n                voiced_frames = []\n    if triggered:\n        sys.stdout.write('-(%s)' % (frame.timestamp + frame.duration))\n    sys.stdout.write('\\n')\n    # If we have any leftover voiced audio when we run out of input,\n    # yield it.\n    if voiced_frames:\n        yield b''.join([f.bytes for f in voiced_frames])\n\n\ndef main(args):\n    if len(args) != 2:\n        sys.stderr.write(\n            'Usage: example.py <aggressiveness> <path to wav file>\\n')\n        sys.exit(1)\n    audio, sample_rate = read_wave(args[1])\n    vad = webrtcvad.Vad(int(args[0]))\n    frames = frame_generator(30, audio, sample_rate)\n    frames = list(frames)\n    segments = vad_collector(sample_rate, 30, 300, vad, frames)\n    for i, segment in enumerate(segments):\n        path = 'chunk-%002d.wav' % (i,)\n        print(' Writing %s' % (path,))\n        write_wave(path, segment, sample_rate)\n\n\nif __name__ == '__main__':\n    main(sys.argv[1:])\n"
        },
        {
          "name": "leak-test.wav",
          "type": "blob",
          "size": 146.52734375,
          "content": null
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 3.3095703125,
          "content": "from setuptools import setup, find_packages, Extension\nimport glob\nimport os.path\nimport sys\n\n\nC_SRC_PREFIX = os.path.join('cbits', 'webrtc', 'common_audio')\nC_SRC_RTC_PREFIX = os.path.join('cbits', 'webrtc', 'rtc_base')\n\nc_sources = (\n    [os.path.join(\n        'cbits', 'pywebrtcvad.c')]\n    + glob.glob(\n        os.path.join(\n            C_SRC_PREFIX,\n            'signal_processing',\n            '*.c'))\n    + glob.glob(\n        os.path.join(\n            C_SRC_PREFIX,\n            'third_party',\n            '*.c'))\n    + glob.glob(\n        os.path.join(\n            C_SRC_PREFIX,\n            'vad',\n            '*.c'))\n    + glob.glob(\n        os.path.join(\n            C_SRC_RTC_PREFIX,\n            'checks.cc')))\n\ndefine_macros = []\nextra_compile_args = []\n\nif sys.platform.startswith('win'):\n    define_macros.extend([('_WIN32', None), ('WEBRTC_WIN', None)])\nelse:\n    define_macros.extend([('WEBRTC_POSIX', None), ])\n    extra_compile_args.extend(['-std=c++11'])\n\nmodule = Extension('_webrtcvad',\n                   define_macros=define_macros,\n                   extra_compile_args=extra_compile_args,\n                   sources=c_sources,\n                   include_dirs=['cbits'])\n\nhere = os.path.abspath(os.path.dirname(__file__))\n\n# Get the long description from the README file\nwith open(os.path.join(here, 'README.rst')) as f:\n    long_description = f.read()\n\nsetup(\n    name='webrtcvad',\n    author='John Wiseman',\n    author_email='jjwiseman@gmail.com',\n    version='2.0.11.dev0',\n    description=('Python interface to the Google WebRTC Voice '\n                 'Activity Detector (VAD)'),\n    long_description=long_description,\n    url='https://github.com/wiseman/py-webrtcvad',\n    license='MIT',\n    classifiers=[\n        # How mature is this project? Common values are\n        #   3 - Alpha\n        #   4 - Beta\n        #   5 - Production/Stable\n        'Development Status :: 4 - Beta',\n\n        # Indicate who your project is intended for\n        'Intended Audience :: Developers',\n        'Topic :: Scientific/Engineering :: Artificial Intelligence',\n        'Topic :: Scientific/Engineering :: Information Analysis',\n        'Topic :: Scientific/Engineering :: Human Machine Interfaces',\n\n        # Pick your license as you wish (should match \"license\" above)\n        'License :: OSI Approved :: MIT License',\n\n        # Specify the Python versions you support here. In particular,\n        # ensure that you indicate whether you support Python 2,\n        # Python 3 or both.\n        'Programming Language :: Python :: 2',\n        'Programming Language :: Python :: 2.7',\n        'Programming Language :: Python :: 3',\n        'Programming Language :: Python :: 3.3',\n        'Programming Language :: Python :: 3.4',\n        'Programming Language :: Python :: 3.5',\n        'Programming Language :: Python :: 3.6',\n        'Programming Language :: Python :: 3.7',\n    ],\n    keywords='speechrecognition asr voiceactivitydetection vad webrtc',\n    ext_modules=[module],\n    py_modules=['webrtcvad'],\n    test_suite='nose.collector',\n    # List additional groups of dependencies here (e.g. development\n    # dependencies). You can install these using the following syntax,\n    # for example: $ pip install -e .[dev,test]\n    extras_require={\n        'dev': ['nose', 'check-manifest', 'unittest2', 'zest.releaser',\n                'psutil', 'memory_profiler']\n    })\n"
        },
        {
          "name": "test-audio.raw",
          "type": "blob",
          "size": 14.150390625,
          "content": null
        },
        {
          "name": "test_webrtcvad.py",
          "type": "blob",
          "size": 3.701171875,
          "content": "import unittest\nimport wave\nfrom memory_profiler import memory_usage\n\nimport webrtcvad\n\n\nclass WebRtcVadTests(unittest.TestCase):\n    @staticmethod\n    def _load_wave(file_name):\n        fp = wave.open(file_name, 'rb')\n        try:\n            assert fp.getnchannels() == 1, (\n                '{0}: sound format is incorrect! Sound must be mono.'.format(\n                    file_name))\n            assert fp.getsampwidth() == 2, (\n                '{0}: sound format is incorrect! '\n                'Sample width of sound must be 2 bytes.').format(file_name)\n            assert fp.getframerate() in (8000, 16000, 32000), (\n                '{0}: sound format is incorrect! '\n                'Sampling frequency must be 8000 Hz, 16000 Hz or 32000 Hz.')\n            sampling_frequency = fp.getframerate()\n            sound_data = fp.readframes(fp.getnframes())\n        finally:\n            fp.close()\n            del fp\n        return sound_data, sampling_frequency\n\n    def test_constructor(self):\n        vad = webrtcvad.Vad()\n\n    def test_set_mode(self):\n        vad = webrtcvad.Vad()\n        vad.set_mode(0)\n        vad.set_mode(1)\n        vad.set_mode(2)\n        vad.set_mode(3)\n        self.assertRaises(\n            ValueError,\n            vad.set_mode, 4)\n\n    def test_valid_rate_and_frame_length(self):\n        self.assertTrue(webrtcvad.valid_rate_and_frame_length(8000, 160))\n        self.assertTrue(webrtcvad.valid_rate_and_frame_length(16000, 160))\n        self.assertFalse(webrtcvad.valid_rate_and_frame_length(32000, 160))\n        self.assertRaises(\n            (ValueError, OverflowError),\n            webrtcvad.valid_rate_and_frame_length, 2 ** 35, 10)\n\n    def test_process_zeroes(self):\n        frame_len = 160\n        self.assertTrue(\n            webrtcvad.valid_rate_and_frame_length(8000, frame_len))\n        sample = b'\\x00' * frame_len * 2\n        vad = webrtcvad.Vad()\n        self.assertFalse(vad.is_speech(sample, 16000))\n\n    def test_process_file(self):\n        with open('test-audio.raw', 'rb') as f:\n            data = f.read()\n        frame_ms = 30\n        n = int(8000 * 2 * 30 / 1000.0)\n        frame_len = int(n / 2)\n        self.assertTrue(\n            webrtcvad.valid_rate_and_frame_length(8000, frame_len))\n        chunks = list(data[pos:pos + n] for pos in range(0, len(data), n))\n        if len(chunks[-1]) != n:\n            chunks = chunks[:-1]\n        expecteds = [\n            '011110111111111111111111111100',\n            '011110111111111111111111111100',\n            '000000111111111111111111110000',\n            '000000111111111111111100000000'\n        ]\n        for mode in (0, 1, 2, 3):\n            vad = webrtcvad.Vad(mode)\n            result = ''\n            for chunk in chunks:\n                voiced = vad.is_speech(chunk, 8000)\n                result += '1' if voiced else '0'\n            self.assertEqual(expecteds[mode], result)\n\n    def test_leak(self):\n        sound, fs = self._load_wave('leak-test.wav')\n        frame_ms = 0.010\n        frame_len = int(round(fs * frame_ms))\n        n = int(len(sound) / (2 * frame_len))\n        nrepeats = 1000\n        vad = webrtcvad.Vad(3)\n        used_memory_before = memory_usage(-1)[0]\n        for counter in range(nrepeats):\n            find_voice = False\n            for frame_ind in range(n):\n                slice_start = (frame_ind * 2 * frame_len)\n                slice_end = ((frame_ind + 1) * 2 * frame_len)\n                if vad.is_speech(sound[slice_start:slice_end], fs):\n                    find_voice = True\n            self.assertTrue(find_voice)\n        used_memory_after = memory_usage(-1)[0]\n        self.assertGreaterEqual(\n            used_memory_before / 5.0,\n            used_memory_after - used_memory_before)\n\n\nif __name__ == '__main__':\n    unittest.main(verbosity=2)\n"
        },
        {
          "name": "webrtcvad.py",
          "type": "blob",
          "size": 0.953125,
          "content": "import pkg_resources\n\nimport _webrtcvad\n\n__author__ = \"John Wiseman jjwiseman@gmail.com\"\n__copyright__ = \"Copyright (C) 2016 John Wiseman\"\n__license__ = \"MIT\"\n__version__ = pkg_resources.get_distribution('webrtcvad').version\n\n\nclass Vad(object):\n    def __init__(self, mode=None):\n        self._vad = _webrtcvad.create()\n        _webrtcvad.init(self._vad)\n        if mode is not None:\n            self.set_mode(mode)\n\n    def set_mode(self, mode):\n        _webrtcvad.set_mode(self._vad, mode)\n\n    def is_speech(self, buf, sample_rate, length=None):\n        length = length or int(len(buf) / 2)\n        if length * 2 > len(buf):\n            raise IndexError(\n                'buffer has %s frames, but length argument was %s' % (\n                    int(len(buf) / 2.0), length))\n        return _webrtcvad.process(self._vad, sample_rate, buf, length)\n\n\ndef valid_rate_and_frame_length(rate, frame_length):\n    return _webrtcvad.valid_rate_and_frame_length(rate, frame_length)\n"
        }
      ]
    }
  ]
}