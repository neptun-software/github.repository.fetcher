{
  "metadata": {
    "timestamp": 1736709664682,
    "page": 18,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjIw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "OISF/suricata",
      "stars": 4840,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".clang-format",
          "type": "blob",
          "size": 1.779296875,
          "content": "﻿# Suricata settings as per\n#   doc/userguide/devguide/codebase/code-style.rst\n#\n# This file is set up for clang 9. For the settings available, see\n#   https://releases.llvm.org/9.0.0/tools/clang/docs/ClangFormatStyleOptions.html\n#\n# For the settings available in the latest clang release, see\n#   https://clang.llvm.org/docs/ClangFormatStyleOptions.html\n---\nBasedOnStyle: LLVM\nAlignAfterOpenBracket: DontAlign\nAlignConsecutiveMacros: true\nAlignEscapedNewlines: Right\n# clang 10: AllowShortBlocksOnASingleLine: Never\n# clang 11: AllowShortEnumsOnASingleLine: false\nAllowShortFunctionsOnASingleLine: None\n# BreakBeforeBraces: Mozilla is closest, but does not split empty functions/structs\nBraceWrapping:\n  AfterClass:      true\n  AfterControlStatement: false\n  AfterEnum:       false\n  AfterFunction:   true\n  AfterStruct:     false\n  AfterUnion:      false\n  AfterExternBlock: true\n  BeforeElse:      false\n  IndentBraces:    false\n  SplitEmptyFunction: true\n  SplitEmptyRecord: true\nBreakBeforeBraces: Custom\nCpp11BracedListStyle: false\nColumnLimit: 100\nConstructorInitializerIndentWidth: 8\nContinuationIndentWidth: 8\nForEachMacros: ['json_array_foreach', 'json_object_foreach', 'SLIST_FOREACH',\n                'SLIST_FOREACH_PREVPTR', 'LIST_FOREACH', 'SIMPLEQ_FOREACH', 'TAILQ_FOREACH',\n                'TAILQ_FOREACH_SAFE', 'TAILQ_FOREACH_REVERSE', 'CIRCLEQ_FOREACH',\n                'CIRCLEQ_FOREACH_REVERSE', 'CIRCLEQ_FOREACH_SAFE', 'CIRCLEQ_FOREACH_REVERSE_SAFE',\n                'SPLAY_FOREACH, RB_FOREACH', 'RB_FOREACH_FROM', 'RB_FOREACH_SAFE',\n                'RB_FOREACH_REVERSE', 'RB_FOREACH_REVERSE_FROM', 'RB_FOREACH_REVERSE_SAFE' ]\nIndentCaseLabels: true\nIndentWidth: 4\nReflowComments: true\nSortIncludes: false\n\n# implicit by LLVM style\n#BreakBeforeTernaryOperators: true\n#UseTab: Never\n#TabWidth: 8\n\n...\n"
        },
        {
          "name": ".clusterfuzzlite",
          "type": "tree",
          "content": null
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.9072265625,
          "content": "*.o\n*.lo\n*.a\n*.la\n*.in\n*.[ch]e\n*.log\n*.rule\n*.rules\n*.pcap\n*.pcapng\n/*.yaml\nMakefile\n.deps/\n.libs/\n*~\nm4/\nTAGS\naclocal.m4\nautoconf.h\nautom4te.cache/\nbuild/*\ncompile\nconfig.guess\nconfig.h\nconfig.log\nconfig.status\nconfig.sub\nconfigure\ndepcomp\ndoc/userguide/suricata.1\netc/suricata.logrotate\netc/suricata.service\ninstall-sh\nlibhtp/TAGS\nlibhtp/aclocal.m4\nlibhtp/autom4te.cache/\nlibhtp/config.h\nlibhtp/config.log\nlibhtp/config.status\nlibhtp/configure\nlibhtp/htp.pc\nlibhtp/htp/TAGS\nlibhtp/htp/libhtp.la\nlibhtp/libtool\nlibhtp/stamp-h1\nlibhtp/test/TAGS\nlibtool\nltmain.sh\nmissing\nqa/log/\nqa/coccinelle/struct-flags.cocci\nrust/*/target/*\nscripts/suricatasc/build/*\nscripts/suricatasc/src/__pycache__/*\nsrc/*.diff\nsrc/*.patch\nsrc/*.rej\nsrc/*.orig\nsrc/*.plist/*\nsrc/TAGS\nsrc/suricata\nstamp-h1\nsrc/build-info.h\ntest.sh\n.vscode/*\n/suricata-update/*\n!/suricata-update/Makefile.am\n/libsuricata-config\n!/libsuricata-config.in\n!/.readthedocs.yaml\n"
        },
        {
          "name": ".readthedocs.yaml",
          "type": "blob",
          "size": 0.26171875,
          "content": "# Required by Read The Docs\nversion: 2\n\nbuild:\n  os: ubuntu-22.04\n  tools:\n    python: \"3.11\"\n\npython:\n  install:\n    - requirements: doc/userguide/requirements.txt\n\nsphinx:\n  builder: html\n  configuration: doc/userguide/conf.py\n  fail_on_warning: false\n\nformats: all\n"
        },
        {
          "name": "COPYING",
          "type": "blob",
          "size": 17.66796875,
          "content": "                    GNU GENERAL PUBLIC LICENSE\n                       Version 2, June 1991\n\n Copyright (C) 1989, 1991 Free Software Foundation, Inc.,\n 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA\n Everyone is permitted to copy and distribute verbatim copies\n of this license document, but changing it is not allowed.\n\n                            Preamble\n\n  The licenses for most software are designed to take away your\nfreedom to share and change it.  By contrast, the GNU General Public\nLicense is intended to guarantee your freedom to share and change free\nsoftware--to make sure the software is free for all its users.  This\nGeneral Public License applies to most of the Free Software\nFoundation's software and to any other program whose authors commit to\nusing it.  (Some other Free Software Foundation software is covered by\nthe GNU Lesser General Public License instead.)  You can apply it to\nyour programs, too.\n\n  When we speak of free software, we are referring to freedom, not\nprice.  Our General Public Licenses are designed to make sure that you\nhave the freedom to distribute copies of free software (and charge for\nthis service if you wish), that you receive source code or can get it\nif you want it, that you can change the software or use pieces of it\nin new free programs; and that you know you can do these things.\n\n  To protect your rights, we need to make restrictions that forbid\nanyone to deny you these rights or to ask you to surrender the rights.\nThese restrictions translate to certain responsibilities for you if you\ndistribute copies of the software, or if you modify it.\n\n  For example, if you distribute copies of such a program, whether\ngratis or for a fee, you must give the recipients all the rights that\nyou have.  You must make sure that they, too, receive or can get the\nsource code.  And you must show them these terms so they know their\nrights.\n\n  We protect your rights with two steps: (1) copyright the software, and\n(2) offer you this license which gives you legal permission to copy,\ndistribute and/or modify the software.\n\n  Also, for each author's protection and ours, we want to make certain\nthat everyone understands that there is no warranty for this free\nsoftware.  If the software is modified by someone else and passed on, we\nwant its recipients to know that what they have is not the original, so\nthat any problems introduced by others will not reflect on the original\nauthors' reputations.\n\n  Finally, any free program is threatened constantly by software\npatents.  We wish to avoid the danger that redistributors of a free\nprogram will individually obtain patent licenses, in effect making the\nprogram proprietary.  To prevent this, we have made it clear that any\npatent must be licensed for everyone's free use or not licensed at all.\n\n  The precise terms and conditions for copying, distribution and\nmodification follow.\n\n                    GNU GENERAL PUBLIC LICENSE\n   TERMS AND CONDITIONS FOR COPYING, DISTRIBUTION AND MODIFICATION\n\n  0. This License applies to any program or other work which contains\na notice placed by the copyright holder saying it may be distributed\nunder the terms of this General Public License.  The \"Program\", below,\nrefers to any such program or work, and a \"work based on the Program\"\nmeans either the Program or any derivative work under copyright law:\nthat is to say, a work containing the Program or a portion of it,\neither verbatim or with modifications and/or translated into another\nlanguage.  (Hereinafter, translation is included without limitation in\nthe term \"modification\".)  Each licensee is addressed as \"you\".\n\nActivities other than copying, distribution and modification are not\ncovered by this License; they are outside its scope.  The act of\nrunning the Program is not restricted, and the output from the Program\nis covered only if its contents constitute a work based on the\nProgram (independent of having been made by running the Program).\nWhether that is true depends on what the Program does.\n\n  1. You may copy and distribute verbatim copies of the Program's\nsource code as you receive it, in any medium, provided that you\nconspicuously and appropriately publish on each copy an appropriate\ncopyright notice and disclaimer of warranty; keep intact all the\nnotices that refer to this License and to the absence of any warranty;\nand give any other recipients of the Program a copy of this License\nalong with the Program.\n\nYou may charge a fee for the physical act of transferring a copy, and\nyou may at your option offer warranty protection in exchange for a fee.\n\n  2. You may modify your copy or copies of the Program or any portion\nof it, thus forming a work based on the Program, and copy and\ndistribute such modifications or work under the terms of Section 1\nabove, provided that you also meet all of these conditions:\n\n    a) You must cause the modified files to carry prominent notices\n    stating that you changed the files and the date of any change.\n\n    b) You must cause any work that you distribute or publish, that in\n    whole or in part contains or is derived from the Program or any\n    part thereof, to be licensed as a whole at no charge to all third\n    parties under the terms of this License.\n\n    c) If the modified program normally reads commands interactively\n    when run, you must cause it, when started running for such\n    interactive use in the most ordinary way, to print or display an\n    announcement including an appropriate copyright notice and a\n    notice that there is no warranty (or else, saying that you provide\n    a warranty) and that users may redistribute the program under\n    these conditions, and telling the user how to view a copy of this\n    License.  (Exception: if the Program itself is interactive but\n    does not normally print such an announcement, your work based on\n    the Program is not required to print an announcement.)\n\nThese requirements apply to the modified work as a whole.  If\nidentifiable sections of that work are not derived from the Program,\nand can be reasonably considered independent and separate works in\nthemselves, then this License, and its terms, do not apply to those\nsections when you distribute them as separate works.  But when you\ndistribute the same sections as part of a whole which is a work based\non the Program, the distribution of the whole must be on the terms of\nthis License, whose permissions for other licensees extend to the\nentire whole, and thus to each and every part regardless of who wrote it.\n\nThus, it is not the intent of this section to claim rights or contest\nyour rights to work written entirely by you; rather, the intent is to\nexercise the right to control the distribution of derivative or\ncollective works based on the Program.\n\nIn addition, mere aggregation of another work not based on the Program\nwith the Program (or with a work based on the Program) on a volume of\na storage or distribution medium does not bring the other work under\nthe scope of this License.\n\n  3. You may copy and distribute the Program (or a work based on it,\nunder Section 2) in object code or executable form under the terms of\nSections 1 and 2 above provided that you also do one of the following:\n\n    a) Accompany it with the complete corresponding machine-readable\n    source code, which must be distributed under the terms of Sections\n    1 and 2 above on a medium customarily used for software interchange; or,\n\n    b) Accompany it with a written offer, valid for at least three\n    years, to give any third party, for a charge no more than your\n    cost of physically performing source distribution, a complete\n    machine-readable copy of the corresponding source code, to be\n    distributed under the terms of Sections 1 and 2 above on a medium\n    customarily used for software interchange; or,\n\n    c) Accompany it with the information you received as to the offer\n    to distribute corresponding source code.  (This alternative is\n    allowed only for noncommercial distribution and only if you\n    received the program in object code or executable form with such\n    an offer, in accord with Subsection b above.)\n\nThe source code for a work means the preferred form of the work for\nmaking modifications to it.  For an executable work, complete source\ncode means all the source code for all modules it contains, plus any\nassociated interface definition files, plus the scripts used to\ncontrol compilation and installation of the executable.  However, as a\nspecial exception, the source code distributed need not include\nanything that is normally distributed (in either source or binary\nform) with the major components (compiler, kernel, and so on) of the\noperating system on which the executable runs, unless that component\nitself accompanies the executable.\n\nIf distribution of executable or object code is made by offering\naccess to copy from a designated place, then offering equivalent\naccess to copy the source code from the same place counts as\ndistribution of the source code, even though third parties are not\ncompelled to copy the source along with the object code.\n\n  4. You may not copy, modify, sublicense, or distribute the Program\nexcept as expressly provided under this License.  Any attempt\notherwise to copy, modify, sublicense or distribute the Program is\nvoid, and will automatically terminate your rights under this License.\nHowever, parties who have received copies, or rights, from you under\nthis License will not have their licenses terminated so long as such\nparties remain in full compliance.\n\n  5. You are not required to accept this License, since you have not\nsigned it.  However, nothing else grants you permission to modify or\ndistribute the Program or its derivative works.  These actions are\nprohibited by law if you do not accept this License.  Therefore, by\nmodifying or distributing the Program (or any work based on the\nProgram), you indicate your acceptance of this License to do so, and\nall its terms and conditions for copying, distributing or modifying\nthe Program or works based on it.\n\n  6. Each time you redistribute the Program (or any work based on the\nProgram), the recipient automatically receives a license from the\noriginal licensor to copy, distribute or modify the Program subject to\nthese terms and conditions.  You may not impose any further\nrestrictions on the recipients' exercise of the rights granted herein.\nYou are not responsible for enforcing compliance by third parties to\nthis License.\n\n  7. If, as a consequence of a court judgment or allegation of patent\ninfringement or for any other reason (not limited to patent issues),\nconditions are imposed on you (whether by court order, agreement or\notherwise) that contradict the conditions of this License, they do not\nexcuse you from the conditions of this License.  If you cannot\ndistribute so as to satisfy simultaneously your obligations under this\nLicense and any other pertinent obligations, then as a consequence you\nmay not distribute the Program at all.  For example, if a patent\nlicense would not permit royalty-free redistribution of the Program by\nall those who receive copies directly or indirectly through you, then\nthe only way you could satisfy both it and this License would be to\nrefrain entirely from distribution of the Program.\n\nIf any portion of this section is held invalid or unenforceable under\nany particular circumstance, the balance of the section is intended to\napply and the section as a whole is intended to apply in other\ncircumstances.\n\nIt is not the purpose of this section to induce you to infringe any\npatents or other property right claims or to contest validity of any\nsuch claims; this section has the sole purpose of protecting the\nintegrity of the free software distribution system, which is\nimplemented by public license practices.  Many people have made\ngenerous contributions to the wide range of software distributed\nthrough that system in reliance on consistent application of that\nsystem; it is up to the author/donor to decide if he or she is willing\nto distribute software through any other system and a licensee cannot\nimpose that choice.\n\nThis section is intended to make thoroughly clear what is believed to\nbe a consequence of the rest of this License.\n\n  8. If the distribution and/or use of the Program is restricted in\ncertain countries either by patents or by copyrighted interfaces, the\noriginal copyright holder who places the Program under this License\nmay add an explicit geographical distribution limitation excluding\nthose countries, so that distribution is permitted only in or among\ncountries not thus excluded.  In such case, this License incorporates\nthe limitation as if written in the body of this License.\n\n  9. The Free Software Foundation may publish revised and/or new versions\nof the General Public License from time to time.  Such new versions will\nbe similar in spirit to the present version, but may differ in detail to\naddress new problems or concerns.\n\nEach version is given a distinguishing version number.  If the Program\nspecifies a version number of this License which applies to it and \"any\nlater version\", you have the option of following the terms and conditions\neither of that version or of any later version published by the Free\nSoftware Foundation.  If the Program does not specify a version number of\nthis License, you may choose any version ever published by the Free Software\nFoundation.\n\n  10. If you wish to incorporate parts of the Program into other free\nprograms whose distribution conditions are different, write to the author\nto ask for permission.  For software which is copyrighted by the Free\nSoftware Foundation, write to the Free Software Foundation; we sometimes\nmake exceptions for this.  Our decision will be guided by the two goals\nof preserving the free status of all derivatives of our free software and\nof promoting the sharing and reuse of software generally.\n\n                            NO WARRANTY\n\n  11. BECAUSE THE PROGRAM IS LICENSED FREE OF CHARGE, THERE IS NO WARRANTY\nFOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW.  EXCEPT WHEN\nOTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES\nPROVIDE THE PROGRAM \"AS IS\" WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED\nOR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF\nMERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE.  THE ENTIRE RISK AS\nTO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU.  SHOULD THE\nPROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING,\nREPAIR OR CORRECTION.\n\n  12. IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING\nWILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MAY MODIFY AND/OR\nREDISTRIBUTE THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES,\nINCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING\nOUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED\nTO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY\nYOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER\nPROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE\nPOSSIBILITY OF SUCH DAMAGES.\n\n                     END OF TERMS AND CONDITIONS\n\n            How to Apply These Terms to Your New Programs\n\n  If you develop a new program, and you want it to be of the greatest\npossible use to the public, the best way to achieve this is to make it\nfree software which everyone can redistribute and change under these terms.\n\n  To do so, attach the following notices to the program.  It is safest\nto attach them to the start of each source file to most effectively\nconvey the exclusion of warranty; and each file should have at least\nthe \"copyright\" line and a pointer to where the full notice is found.\n\n    <one line to give the program's name and a brief idea of what it does.>\n    Copyright (C) <year>  <name of author>\n\n    This program is free software; you can redistribute it and/or modify\n    it under the terms of the GNU General Public License as published by\n    the Free Software Foundation; either version 2 of the License, or\n    (at your option) any later version.\n\n    This program is distributed in the hope that it will be useful,\n    but WITHOUT ANY WARRANTY; without even the implied warranty of\n    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n    GNU General Public License for more details.\n\n    You should have received a copy of the GNU General Public License along\n    with this program; if not, write to the Free Software Foundation, Inc.,\n    51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\n\nAlso add information on how to contact you by electronic and paper mail.\n\nIf the program is interactive, make it output a short notice like this\nwhen it starts in an interactive mode:\n\n    Gnomovision version 69, Copyright (C) year name of author\n    Gnomovision comes with ABSOLUTELY NO WARRANTY; for details type `show w'.\n    This is free software, and you are welcome to redistribute it\n    under certain conditions; type `show c' for details.\n\nThe hypothetical commands `show w' and `show c' should show the appropriate\nparts of the General Public License.  Of course, the commands you use may\nbe called something other than `show w' and `show c'; they could even be\nmouse-clicks or menu items--whatever suits your program.\n\nYou should also get your employer (if you work as a programmer) or your\nschool, if any, to sign a \"copyright disclaimer\" for the program, if\nnecessary.  Here is a sample; alter the names:\n\n  Yoyodyne, Inc., hereby disclaims all copyright interest in the program\n  `Gnomovision' (which makes passes at compilers) written by James Hacker.\n\n  <signature of Ty Coon>, 1 April 1989\n  Ty Coon, President of Vice\n\nThis General Public License does not permit incorporating your program into\nproprietary programs.  If your program is a subroutine library, you may\nconsider it more useful to permit linking proprietary applications with the\nlibrary.  If this is what you want to do, use the GNU Lesser General\nPublic License instead of this License.\n"
        },
        {
          "name": "ChangeLog",
          "type": "blob",
          "size": 147.150390625,
          "content": "7.0.2 -- 2023-10-18\n\nSecurity #6306: mime: quadratic complexity in MimeDecAddEntity\nBug #6402: detect: multi-level tunneling inspection fails\nBug #6397: detect: multiple legacy buffer selection leading to multi-buffer\nBug #6381: DPDK 23.11 changed function name of Bond API\nBug #6380: email: disabled fields in suricata.yaml also get logged\nBug #6303: conf: an empty child node is not checked for NULL\nBug #6300: config: includes provided as a sequence are loaded into the wrong parent configuration node\nBug #6297: configure/docs: check for a supported version of sphinx-build\nBug #6104: detect/multi-buffer: Heap-buffer-overflow in SigMatchAppendSMToList\nBug #6009: dpdk: incorrect final stats\nBug #5831: af-packet/ips: excessive mtu log messages\nBug #5211: detect/frames: crash with detect.profiling.grouping.dump-to-disk\nBug #4624: byte_jump with negative post_offset before start of buffer failure\nFeature #6367: SMTP: do not delay mime chunk processing\nFeature #5966: dpdk: Analyze hugepage allocation on startup\nFeature #4968: QUIC v2 support\nTask #6348: detect/analyzer: add more details for the ipopts keyword\nTask #6235: decode: add drop reason for stream reassembly memcap\nDocumentation #6349: userguide: add section about tcp.flags\nDocumentation #6342: userguide: cover install-full and install-conf in the install page\n\n7.0.1 -- 2023-09-13\n\nSecurity #6279: Crash in SMTP parser during parsing of email\nSecurity #6195: process exit in hyperscan error handling \nBug #6276: community-id: Fix IPv6 address sorting not respecting byte order\nBug #6256: eve: crash if output dir isn't writeable\nBug #6255: flow: possible divide by zero at start up\nBug #6247: pcre: parsing crash in multi-tenant multi-loader setup\nBug #6244: tcp: RST with data used in reassembly\nBug #6243: Parsing ip-reputation reputation config files now rejects CR and CR+LF\nBug #6240: pcap/file: negative pcap file timestamps lead to weird output\nBug #6233: dpdk: fix overall threads check for IPS mode\nBug #6232: dpdk: treat unknown socket value as a valid value\nBug #6222: Decode-events of IPv6 GRE are not triggered\nBug #6201: multi-tenancy: crash under test mode when tenant signature load fails\nBug #6191: if protocol dcerpc first packet type is Alter_context, it will not parse dcerpc\nBug #6095: windows: lua script path truncated\nBug #6094: eve/stats: memcap_pressure and memcap_pressure_max not logged\nBug #6044: detect: multi-tenancy leaks memory if more than 1 tenant registered\nBug #5870: ips/af-packet: crash when copy-iface is the same as the interface\nBug #5619: dpdk/ips: crash at shutdown with mlx\nBug #5443: ftp-data: failed assertion\nBug #4881: alert event incorrectly log stored files\nOptimization #6265: threading: set a higher default stack size for threads\nOptimization #6263: mpm/ac: reduce stack usage\nOptimization #5920: investigate: check and fix unhandled divisions by 0\nOptimization #3637: Performance impact of Cisco Fabricpath\nFeature #6267: multi-tenancy: reload-tenants command\nFeature #6230: stats: add drop reason counters\nFeature #4756: capture: support ips stats for all IPS capture methods\nFeature #4587: dhcp: vendor class indentifier support\nDocumentation #6231: userguide: add installation from Ubuntu PPA section\nDocumentation #6124: userguide: add instructions/explanation for (not) running suricata with root\n\n7.0.0 -- 2023-07-18\n\nBug #6212: file.magic: rule reload can lead to crashes\nBug #6211: file: assert failed (!((txd->files_logged > txd->files_opened))), function CloseFile, file output-file.c, line 96.\nBug #6207: util/mime: fuzz failure on base64 remainder parser\nBug #6205: flow/hash: flow by flow_id getter never reaches right flow_id\nBug #6185: smtp: use every byte to compute email.body_md5\nBug #6169: exceptions: master switch not applied to midstream\nBug #6165: http2: fileinfo events log http2 object instead of http object as alerts and http2 do\nBug #6163: http: request_heaser keyword does not support multibuffer\nBug #6149: exceptions: 'auto' policy not considered valid value in IDS mode\nBug #6135: base64: complete support for RFC2045\nBug #6130: http2: quadratic complexity in http2_range_key_get\nBug #6116: dpdk: demote log level of some DPDK messages\nBug #6115: dpdk: NUMA warning signals to non-existent negative id NUMA\nBug #6105: byte_jump does not allow variable name to be used consistently\nBug #6081: pcap: device reopen broken\nBug #6023: smtp: Attachment not being md5 matched\nBug #5964: dpdk: Evaluate input of EAL arguments\nBug #5916: NFQ calls TmqhOutputPacketpool before release packet function is set\nBug #5912: rfb: parser returns error on unimplemented record types\nBug #5868: filestore: not saving files when filestore enabled by rule matching on file_data (instead saves 0 bytes)\nBug #5832: source-xdp: build errors/warnings with libbpf 0.8+\nBug #5757: http: response content encoding value \"none\" considered invalid\nBug #5464: eve: if alert and drop rules match for a packet, \"alert.action\" is ambigious\nBug #5022: log-pcap: fix segfault on lz4 compressed pcaps\nBug #4797: pcre2 crash in multi-tenant\nBug #4750: pcap: memory leaks\nBug #2917: Unable to find the sm in any of the sm lists\nOptimization #6194: detect: modernize filename fileext filemagic\nOptimization #6151: suricatasc: Gracefully handle unsupported commands\nOptimization #4145: file keywords: unify keyword registration\nOptimization #4141: file.data: inspect File objects for HTTP\nFeature #6162: libhtp: recognize Bearer authentication\nFeature #6145: byte_math: allow variable name for nbytes\nFeature #6144: byte_test: allow variable name for nbytes\nFeature #6106: dpdk: fail startup on uninitialized thread affinity setting\nFeature #4201: http2: full protocol support\nTask #6183: flash decompression: add deprecation warning\nTask #6159: libhtp: event on chunk extension\nTask #6157: libhtp 0.5.45\nTask #6128: af-packet: remove rollover options\nTask #4163: rust: set new minimum Rust version for 7\nDocumentation #6032: detect: document new multi-instance logic\nDocumentation #5987: doc: update build instructions\nDocumentation #5930: doc: multi-tenant states that only vlan can be used live, should also include interface\n\n7.0.0-rc2 -- 2023-06-14\n\nFeature #6099: dpdk: add support for bonding interface\nFeature #6085: detect: set explicit rule types\nFeature #5975: Add support for 'inner' PF_RING clustering modes\nFeature #5937: dpdk: Improve DPDK version checking\nFeature #5876: eve: add stream tcp logging\nFeature #5849: dpdk: add virtio-pmd support\nFeature #5822: yaml: set suricata version in generated config\nFeature #5803: github-ci: Add netmap as a Github Action\nFeature #5784: detect: allow cross buffer inspection on multi-buffer matches\nFeature #5746: http.connection - allow in server response\nFeature #5717: rfb: add frame support\nSecurity #6129: dcerpc: max-tx config parameter, also for UDP\nSecurity #6118: datasets: absolute path in rules can overwrite arbitrary files\nSecurity #5945: byte_math: Division by zero possible.\nBug #6137: SNMP: version is logged from state, instead of from transaction\nBug #6132: suricata-update: dump-sample-configs: configuration files not found\nBug #6120: streaming-buffer: exceeds limit when downloading large file with file-store enabled\nBug #6117: tcp regions streaming buffer: assert failed (!((region->stream_offset == sbb->offset && region->buf_offset > sbb->len))), function StreamingBufferSBBGetData\nBug #6109: exception/policy: reject changes flow action in IDS mode\nBug #6103: http2: cpu overconsumption in rust moving/memcpy in http2_parse_headers_blocks\nBug #6093: flow: occasional sudden spike in flow.memuse\nBug #6089: suricata --list-keywords does not work with debug validation\nBug #6087: FTP bounce detection doesn't work for big-endian platforms\nBug #6086: Decode-events of IPv6 packets are not triggered\nBug #6066: Memory Corruption in util-streaming-buffer\nBug #6064: dpdk: detect reload stuck if there are no packets\nBug #6062: flow: memory leaks at shutdown\nBug #6060: IP Datasets not supported from suricata.yaml\nBug #6057: rust/jsonbuilder: better handling of memory allocation errors\nBug #6054: ftp:  long line discard logic should be separate for server and client\nBug #6053: smtp: long line discard logic should be separate for server and client\nBug #6046: runmode/unix-socket: http range memory leak\nBug #6043: detect: multi-tenancy fails to start\nBug #6041: ASSERT: !(sb->region.buf_offset != 0)\nBug #6038: TCP resets have incorrect len, nh in IPv6\nBug #6025: detect: allow bsize 0 for existing empty buffers\nBug #6021: af-packet: reload not occurring until packets are seen\nBug #6019: smtp: fuzz debug assertion trigger\nBug #6008: smb: wrong offset when parse SMB_COM_WRITE_ANDX record\nBug #6006: dpdk: query eth stats only by the first worker\nBug #5998: exception/policy: make work with simulated flow memcap\nBug #5989: smtp: any command post a long command gets skipped\nBug #5981: smtp: Long DATA line post boundary is capped at 4k Bytes\nBug #5979: rust: update sawp dependencies to avoid future compile issues\nBug #5978: stream/reassembly: memcap exception policy incorrectly applied\nBug #5971: libhtp: differential fuzzing with rust version: only trim spaces at headers names end\nBug #5969: detect: reload can stall if flow housekeeping takes too long\nBug #5968: flowworker: per packet flow housekeeping can process too many flows\nBug #5963: dpdk: handle packets splitted in multiple segments\nBug #5960: Postpone setting of master exception policy\nBug #5957: bpf: postpone IPS check after IPS runmode is determined from the configuration file\nBug #5952: http: multipart data is not filled up to request.body-limit\nBug #5940: exception/policy: flow action doesn't fall back to packet action when there's no flow\nBug #5936: dpdk: Release mempool only after the device closes\nBug #5931: http2: urilen not supported\nBug #5929: fast_pattern assignment of specific content in combination with urilen results in FN\nBug #5927: smtp: quadratic complexity for tx iterator with linked list\nBug #5925: dpdk: VMXNET3 fails to configure\nBug #5924: AF_XDP  compile error\nBug #5923: dpdk: change in NUMA-determining API \nBug #5919: flow/manager: fix unhandled division by 0 (prealloc: 0)\nBug #5917: http: libhtp errors on multiple 100 continue response\nBug #5909: http2: quadratic complexity when reducing dynamic headers table size\nBug #5907: tcp: failed assertion ASSERT: !(ssn->state != TCP_SYN_SENT)\nBug #5905: invalid bsize and distance rule being loaded by suricata\nBug #5900: UBSAN: undefined shift in DetectByteMathDoMatch\nBug #5885: base64_decode not populating up to an invalid character\nBug #5883: mime: debug assertion on fuzz input\nBug #5881: stream: overlap with different data false positive\nBug #5877: stream: connections time out too early\nBug #5875: stream/ips: dropping spurious retransmissions times out connections\nBug #5867: false-positive drop event_types possible on passed packets\nBug #5866: detect: multi-tenancy crash\nBug #5862: netmap: packet stalls\nBug #5856: stream: SYN/ACK timestamp checking blocks valid traffic\nBug #5855: af-xdp: may fail to build on Linux systems with kernel older than 5.11\nBug #5850: frames: Assertion failed: buffer initialized\nBug #5843: tcp/stream: session reuse on tcp flows w/o sessions\nBug #5836: output: abort triggered on no permission test\nBug #5835: debug: segv on enabling debugging output\nBug #5834: tcp/regions: list corruption\nBug #5833: tcp/regions: use after free error\nBug #5825: stream.midstream: if enabled breaks exception policy\nBug #5823: smtp: config and built-in defaults mismatch\nBug #5819: SMTP does not handle LF post line limit properly\nBug #5818: time: integer comparison with different signs\nBug #5808: http2: leak with range files\nBug #5802: ips: txs still logged for dropped flow\nBug #5799: detect: sigs using DETECT_SM_LIST_PMATCH can break other signatures\nBug #5786: smb: possible evasion with trailing nbss data\nBug #5783: smb: wrong endian conversion when parse NTLM Negotiate Flags\nBug #5780: HTTP/2 - FN when matching on multiple http2.header contents\nBug #5770: smb: no consistency check between NBSS length and length field for some SMB operations\nBug #5740: content: within and distance lengths should be bounded\nBug #5667: Enable rule profiling via socket\nBug #5627: windows: windivert build broken\nBug #5621: security.limit-noproc: disabled if not provided in the configuration file\nBug #5563: stream: issue with stream debug tracking of memuse\nBug #5541: Unexpected behavior of `endswith` in combination with negated content matches\nBug #5526: tcp: Assertion failed: (!((last_ack_abs < left_edge && StreamTcpInlineMode() == 0 && !f->ffr && ssn->state < TCP_CLOSED)))\nBug #5498: flowworker: Assertion in CheckWorkQueue\nBug #5437: 'unseen' http midstream packets with TCP FIN flag set\nBug #5320: Key collisions in HTTP JSON eve-logs\nBug #5270: Flow hash table collision and flow state corruption between different capture interfaces\nBug #5261: rust: reconsider bundling Cargo.lock\nBug #5017: counters: tcp.syn, tcp.synack, tcp.rst depend on flow\nBug #4952: scan-build: Access to field 'de_state' results in a dereference of a null pointer\nBug #4759: TCP DNS query not found when tls filter is active\nBug #4578: perf shows excessive time in IPOnlyMatchPacket\nBug #4529: Not keyword matches in Kerberos requests\nBug #3152: scan-build warning for detect sigordering\nBug #3151: scan-build warning for detect port handling\nBug #3150: scan-build warnings for detect address handling\nBug #3149: scan-build warnings in radix implementation\nBug #3148: scan-build warnings for ac implementations\nBug #3147: scan-build warning for mime decoder\nOptimization #6100: mqtt: quadratic complexity in get_tx_by_pkt_id\nOptimization #6036: pgsql: remove unused Kerb5 auth message\nOptimization #5959: detect using uninitialized engine mode\nOptimization #5718: time: compact alternative to struct timeval\nOptimization #5544: tls keywords: increase code coverage and update documentation (if need be)\nOptimization #4378: file.data: split mpm per app_proto\nTask #5993: rust: x509-parser 0.15\nTask #5992: rust: snmp-parser 0.9.0\nTask #5991: rust: der-parser 8.2.0\nTask #5983: libhtp 0.5.44\nTask #5965: tracking: Improving DPDK capture interface and docs \nTask #5939: config: deprecate multiple \"include\" statements at the same level\nTask #5918: libhtp 0.5.43\nTask #5741: rust/src/rfb/* add more unittests\nTask #5628: github-ci: add windows + windivert build\nTask #5474: test: review how 7 works with config from 5 and 6\nTask #4067: http2: overload existing http keywords to support http/2\nTask #4051: Convert unittests to new FAIL/PASS API: detect-lua.c\nDocumentation #5962: documentation: mention the use of http1 in rule protocol\nDocumentation #5884: docs: update CentOS names according to their new conventions\nDocumentation #5859: docs: add build instructions for DPDK capture interface\nDocumentation #5858: docs: add list of supported NICs in DPDK mode\nDocumentation #5857: docs: refactor DPDK documentation\nDocumentation #5596: doc/optimization: move 'suricata.git/doc/userguide/convert.py' to Python3\n\n7.0.0-rc1 -- 2023-01-31\n\nFeature #5761: Unknown ethertype packets are not counted\nFeature #5516: tls: client cert detection\nFeature #5384: Thread Synchronisation: wait for all threads to be in an operating state before continuing initialisation\nFeature #5383: Support for IP addresses in dataset\nFeature #5219: ips: add 'master switch' to enable dropping on traffic (handling) exceptions\nFeature #5184: Add more dataset user interaction\nFeature #4981: frames: add general <app_proto>.stream frames\nFeature #4979: frames: implement dynamic logic to disable frames of a type\nFeature #4751: dns/eve: add 'HTTPS' type logging\nFeature #4269: Additional dataset operations\nFeature #3306: Support AF_XDP capture method\nFeature #3086: app_proto for Torrent traffic\nFeature #2497: error messages usability improvement\nSecurity #5712: tcp: crafted packets lead to resource starvation\nSecurity #5703: smb: crash inside of streaming buffer Grow()\nSecurity #5701: Suricata crashes while processing FTP\nSecurity #5700: SCRealloc of large chunk crashes Suricata\nSecurity #5686: decoder/tunnel: tunnel depth not limited properly\nSecurity #5623: smtp/base64: crash / memory corruption\nBug #5817: tls: certificates with dates prior to 1970 are not logged correctly\nBug #5814: smb: duplicate interface fields logged\nBug #5813: rfb/eve: depth in pixel format logged twice\nBug #5811: smb: tx logs sometimes have duplicate `tree_id` output\nBug #5781: smb: unbounded file chunk queuing after gap\nBug #5779: dcerpc: max-tx config parameter\nBug #5769: Incomplete values for .stats.\"app_layer\".flow.proto\nBug #5765: exceptions: midstream flows are dropped if midstream=true && stream.midstream-policy=drop-flow\nBug #5753: smb: convert transaction list to vecdeque\nBug #5747: iprep/ipv6: warning issued on valid reputation input\nBug #5725: smtp: quoted-printable encoding skips empty lines in files\nBug #5707: quic: ja3 Stack-use-after-return READ 1\nBug #5706: app-layer-htp: Condition depending on enabled IPS mode never true\nBug #5693: decode: Padded packet to minimal Ethernet length marked with invalid length event\nBug #5691: HTTP/2 decompression bug\nBug #5663: tls: buffer overhead off by one in TLSDecodeHSHelloExtensionSupportedVersions\nBug #5661: security.limit-noproc: break ASAN/LSAN when non-root user\nBug #5658: SMTP: segfault on boundary data\nBug #5654: readthedocs: not showing pdf download option for recent versions\nBug #5644: Integer overflow at dcerpc.rs:846\nBug #5637: quic: convert to vecdeque\nBug #5624: quic: rule with ja3.hash keyword fails to load\nBug #5617: dpdk: avoid per thread warnings\nBug #5580: dpdk: IDS vs IPS confusion\nBug #5579: pgsql: support out of order parameter in startup message\nBug #5574: base64: skip over all invalid characters for RFC 2045 mode\nBug #5572: pcre2: allow different include/lib paths\nBug #5567: smb: failed assertion (!((f->alproto == ALPROTO_SMB && txd->files_logged != 0))), function CloseFile, file output-file.c\nBug #5564: tls: buffer overread \nBug #5558: detect: invalid hex character in content leads to bad debug message\nBug #5557: dcerpc: rust integer underflow\nBug #5553: dpdk: Packets with invalid checksums are not counted in DPDK capture mode\nBug #5530: frames: buffer overflow in signatures parsing\nBug #5529: frame: memory leak in signature parsing\nBug #5528: tcp: assertion failed in function DoInsertSegment\nBug #5456: detect: config keyword prevents tx cleanup\nBug #5444: dns: allow dns messages with invalid opcodes\nBug #5379: detect/udp: different detection from rules when UDP/TCP header is broken\nBug #5374: pcap-log: breaking change in file names\nBug #5258: smb/ntlmssp: parser incorrectly assumes fixed field order\nBug #5235: ftp: add event when command request or response is too long\nBug #5205: FTP-data unrecognized depending on multi-threading\nBug #5198: eve/stats: ASAN error when eve output file can't be opened.\nBug #5161: smb: file not tracked on smb2 async\nBug #4580: smb: large streams can cause large memory moves (memmove)\nBug #4554: Configuration test mode succeeds when classification.config file contains invalid content\nBug #3253: tls: handling of 'Not Before' date before unix epoch\nBug #2982: invalid dsize distance rule being loaded by suricata\nOptimization #5782: smb: set defaults for file chunk limits\nOptimization #5373: Prevent process creation by Suricata process\nOptimization #4977: frames: gap handling in inspection\nOptimization #4908: ftp: use AppLayerResult instead of buffering wherever possible\nOptimization #4614: Fix warning about \"field reassign with default\"\nOptimization #4612: Fix warning about \"nonminimal bool\"\nOptimization #4611: Fix warning about \"extra unused lifetimes\"\nOptimization #4610: Fix warning about \"explicit counter loop\"\nOptimization #4608: Fix warning about \"redundant pattern matching\"\nOptimization #4606: Fix warning about \"match ref pats\"\nOptimization #4603: Fix warning about \"type complexity\"\nOptimization #4602: Fix warning about \"new without default\"\nOptimization #4601: Fix warning about \"while let loop\"\nOptimization #4600: Fix warning about \"needless lifetimes\"\nOptimization #4598: Fix warning about \"needless_range_loop\"\nOptimization #4596: Fix warning about \"single match\"\nOptimization #4594: Fix warning about \"this loop never actually loops\"\nOptimization #4592: Fix warning  about \"for loop over fallibles\"\nOptimization #4591: Fix Rust clippy lints\nOptimization #3160: clean up error codes\nTask #5638: SWF decompression: Do not depend on libhtp\nTask #5632: Disable swf decompression by default\nTask #5587: ips/tap: in layer 2 ips/tap setups, warn that mixed usage of ips and tap will be removed in 8.0\nTask #5586: rust/applayertemplate: remove pub and no_mangle from extern functions that don't need it\nTask #5504: exceptions: error out when invalid configuration value is passed\nTask #5496: detect/parse: add tests for parsing signatures with reject and drop action\nTask #4939: app-layer: template and setup script\nTask #4054: Convert unittests to new FAIL/PASS API: detect-replace.c\nTask #4050: Convert unittests to new FAIL/PASS API: detect-l3proto.c\nTask #4049: Convert unittests to new FAIL/PASS API: detect-itype.c\nTask #4043: Convert unittests to new FAIL/PASS API: detect-icmp-seq.c\nTask #4042: Convert unittests to new FAIL/PASS API: detect-icmp-id.c\nTask #4039: Convert unittests to new FAIL/PASS API: detect-filesize.c\nTask #4030: Convert unittests to new FAIL/PASS API: detect-engine-tag.c\nTask #4029: Convert unittests to new FAIL/PASS API: detect-engine-sigorder.c\nTask #4020: Convert unittests to new FAIL/PASS API - detect-distance.c\nDocumentation #5616: Ubuntu PPA: Package software-properties-common\nDocumentation #5585: devguide: bring section about installation from redmine wiki into DevGuide\nDocumentation #5515: userguide: add a dedicated chapter/section for the Exception Policies\nDocumentation #5129: devguide: clarify style guide for getframe functions\nDocumentation #4929: devguide: bring Contributing process page into it\nDocumentation #4697: devguide: document app-layer frame support\n\n7.0.0-beta1 -- 2022-10-26\n\nFeature #5509: App-layer event for protocol change failure\nFeature #5506: DHCP: signature keyword for rebinding_time\nFeature #5503: ips: add \"reject\" action to exception policies\nFeature #5479: Add landlock support\nFeature #5468: ips: midstream: add \"exception policy\" for midstream\nFeature #5442: kerberos: log ticket encryption method\nFeature #5435: DHCP: signature keyword for lease_time\nFeature #5416: SNMP: signature keyword for usm\nFeature #5218: ips: allow dropping of flow if applayer reaches error state\nFeature #5216: ips: allow dropping of flow if flow.memcap is hit\nFeature #5215: ips: allow dropping of flow if stream.reassembly.memcap is hit\nFeature #5214: ips: allow dropping of flow if stream.memcap is hit\nFeature #5202: eve/drop: include drop \"reason\"\nFeature #5191: new keyword for self signed certificates\nFeature #5190: new tls.random keyword\nFeature #5036: sip: add frames support\nFeature #4984: dns: add frames support\nFeature #4983: frames: support UDP\nFeature #4967: QUIC v1 support\nFeature #4872: nfs: add stream app-layer frame support \nFeature #4556: HTTP2: support deflate decompression\nFeature #4551: eve: add direct base64 to json option to json builder\nFeature #4550: pthreads: set minimum stack size\nFeature #4541: netmap: new API version (14) supports multi-ring software mode\nFeature #4526: SIGSEGV handling -- log stack before aborting\nFeature #4515: Add DNS logging of Z flag\nFeature #4507: dpdk: initial support for IDS and IPS modes\nFeature #4498: decoder: add VN-Tag support\nFeature #4406: unix socket: Get flow information by flow_id\nFeature #4386: Support for RFC2231\nFeature #4332: Makes libhtp decompression time limit configurable from Suricata\nFeature #4241: Protocol support: PostgreSQL (pgsql)\nFeature #4144: file.data: support for request side files in HTTP\nFeature #4142: file.data: support for NFS\nFeature #4117: http2: byte-range support\nFeature #4116: http2: body compression handling\nFeature #3957: Convert protocol to Rust: Modbus\nFeature #3887: yaml: Increase maximum size for address vars\nFeature #3767: Add IKEv1 parser\nFeature #3701: eve: add tenant_id in eve-log for other types than alert\nFeature #3512: stream depth event rule\nFeature #3440: Add GQUIC Protocol Analysis and CYU Fingerprinting\nFeature #3292: support for network service header (NSH)\nFeature #3285: rules: XOR keyword\nFeature #3002: Flow and Netflow Not Logging ESP Traffic\nFeature #2697: prefilter support for stream_size\nFeature #2450: lua: scripts access to calling rule informations\nFeature #2323: Applayer support for telnet\nFeature #2096: eve: event_type for MODBUS\nFeature #2054: Extracting HTTPS URL´s from SMTP, currently only HTTP is supported\nFeature #1576: http: byte-range support\nFeature #1478: Active flow counters\nFeature #1369: eve: json schema\nFeature #1096: tls: client certificate handling\nFeature #120: Capture full session on alert\nSecurity #5408: filestore: Segfault with filestore enabled and forced\nSecurity #5399: mqtt: DOS by quadratic with too many transactions in one parse\nSecurity #5244: Infinite loop in JsonFTPLogger\nSecurity #5243: protocol detection: exploitable type confusion due to concurrent protocol changes\nSecurity #5237: nfs: arbitrary allocation from nfs4_res_secinfo_no_name\nSecurity #5187: Rust regex crate security advisory CVE-2022-24713\nSecurity #5024: ftp: GetLine function buffers data indefinitely if 0x0a was not found int the frag'd input\nSecurity #5023: smtp: GetLine function buffers data indefinitely if 0x0a was not found int the frag'd input\nSecurity #4857: ftp: SEGV at flow cleanup due to protocol confusion\nSecurity #4710: tcp: Bypass of Payload Detection on TCP RST with options of MD5header\nSecurity #4569: tcp: crafted injected packets cause desync after 3whs\nSecurity #4504: tcp: Evasion possibility on wrong/unexpected ACK value in crafted SYN packets\nBug #5595: eve/alert: SEGV in files to alert logging\nBug #5584: detect/tag: timeout handling issues on windows\nBug #5581: eve: mac address logging for packet records reverses direction\nBug #5571: ips: encapsulated packet logged as dropped, but not actually dropped\nBug #5538: Compiler Warning on Fedora 36 / gcc 12.2.1\nBug #5536: detect: flow.age keyword\nBug #5527: postgresql: limit number of live transactions\nBug #5521: detect: transform strip whitespace creates a 0-sized variable-length array\nBug #5518: dcerpc: More efficient transaction handling for UDP\nBug #5508: SMB2 async responses are not matched with its request\nBug #5507: DHCP: signature keyword for renewal_time\nBug #5458: Reject action is no longer working\nBug #5457: Counters are not initialized in all places.\nBug #5455: ike: logging state transforms instead of transaction transforms\nBug #5419: Failed assert DeStateSearchState\nBug #5409: PCRE: use match and recursion limit for pcrexform\nBug #5402: detect: will still inspect packets of a \"dropped\" flow for non-TCP\nBug #5401: tcp: assertion failed in DoInsertSegment (BUG_ON)\nBug #5392: fileinfo: inconsistent file size tracking for GAPs\nBug #5391: events: PACKET_RECYCLE does not reset event_last_logged\nBug #5390: smb: have default stream-depth of 0\nBug #5386: detect/threshold: offline time handling issue\nBug #5377: modbus: probing parser recognizes modbus with unknown function code\nBug #5368: bypass: Memory leak of some flow bypass objects. \nBug #5361: IPS: ip only rules, but with negated addresses not treated like pure ip-only rules in IPS context\nBug #5353: detect/alert: fix segvfault when incrementing discarded alerts if alert-queue-expand fails\nBug #5331: stacktrace-on-signal: Kills all processes in the same process group\nBug #5330: flow: vlan.use-for-tracking is not used for ICMPv4\nBug #5329: rust: inconsistency between rust structure RustParser and C structure AppLayerParser\nBug #5327: track by_rule|by_both incorrectly rejected for global thresholds\nBug #5321: dcerpc: More efficient transaction handling\nBug #5317: flow manager: end of flow counters not working\nBug #5316: smtp: PreProcessCommands does not handle all the edge cases\nBug #5315: decode/mime: base64 decoding for data with spaces is broken\nBug #5314: ftp: quadratic complexity for tx iterator with linked list\nBug #5313: python: distutils deprecation warning\nBug #5312: test failure on Ubuntu 22.04 with GCC 12\nBug #5310: detect: several potential infinite loops by comparing u16 to size_t\nBug #5309: CIDR prefix calculation fails on big endian archs\nBug #5308: file handling: avoid toctou race conditions\nBug #5306: dcerpc: unsigned integer overflow in parse_dcerpc_bindack\nBug #5298: template (rust): convert transaction list to vecdeque\nBug #5297: pgsql: convert transaction list to vecdeque\nBug #5296: http2: convert transaction list to vecdeque\nBug #5295: rdp: convert transaction list to vecdeque\nBug #5294: mqtt: convert to vecdeque\nBug #5291: cppcheck: various static analyzer \"warning\"s\nBug #5285: frame: assertion failed in PrefilterMpmFrame\nBug #5281: ftp: don't let first incomplete segment be over maximum length\nBug #5280: nfs: ASSERT: attempt to subtract with overflow (compound)\nBug #5278: app-layer: Allow for non slice based transaction containers in generate get iterator (rust)\nBug #5277: dns: More efficient transaction handling\nBug #5276: eve: payload field randomly missing even if the packet field is present\nBug #5271: app-layer: timeout when removing many transactions from the beginning\nBug #5268: mqtt: integer underflow with truncated\nBug #5260: rust: update regex dependency\nBug #5259: rust: update time dependency\nBug #5248: flow: double unlock in tcp reuse case\nBug #5246: smb: integer underflows and overflows\nBug #5238: frame: memory leak in signature parsing\nBug #5236: frame: buffer over read in SCACSearch\nBug #5228: pcre2: SEGV during rule loading\nBug #5226: Frames: failed assertion !((int64_t)data_len > frame->len)\nBug #5223: base64_decode does not populate base64_data buffer once hitting non-base64 chars\nBug #5208: DCERPC protocol detection when nested in SMB\nBug #5205: FTP-data unrecognized depending on multi-threading\nBug #5201: content:\"22 2 22\"; is parsed without error\nBug #5197: fast_pattern assignment of specific content results in FN\nBug #5188: SSL : over allocation for certificates\nBug #5183: TLS Handshake Fragments not Reassembled\nBug #5174: MIME URL extraction creates invalid url in JSON \nBug #5168: detect/iponly: non-cidr netmask settings can lead incorrect detection\nBug #5162: inspection of smb traffic without smb/dcerpc doesn't work correct. \nBug #5147: frames: debug assertion on SMB2 traffic\nBug #5146: libhtp: does not handle 100 continue if there is a 0 Content Length\nBug #5145: nfs: Integer underflow in NFS\nBug #5144: Failed assert DeStateSearchState\nBug #5132: segfault: master - HTPFileCloseHandleRange\nBug #5094: output: timestamp missing usecs on Arm 32bit + Musl\nBug #5093: rust/proc-macro-crate: pin to old version to support our MSRV\nBug #5086: htp: server personality radix handling issue\nBug #5085: defrag: policy config can setup radix incorrectly\nBug #5084: iprep: cidr support can set up radix incorrectly\nBug #5081: detect/iponly: rule parsing does not always apply netmask correctly\nBug #5080: eve/dnp3: coverity warnings for string handling\nBug #5079: swf: coverity warning\nBug #5077: byte_math rule options need to be in order or will fail otherwise\nBug #5073: Off-by-one in flow-manager flow_hash row allocation\nBug #5070: Stacktrace logger should propagate original signal\nBug #5066: detect/iponly: mixing netblocks can lead to FN/FP\nBug #5065: frames: coverity warning\nBug #5046: Documentation copyright years are invalid\nBug #5040: stats: add app-layer error counters\nBug #5034: dns: probing/parser can return error when it should return incomplete\nBug #5019: dataset: error with space in rule language\nBug #5018: MQTT can return AppLayerResult::incomplete forever and buffer forever\nBug #5011: frames: buffer overread in SigValidate\nBug #5009: dpdk: fails to compile on ubuntu 22.04\nBug #5007: pgsql: coverity warning\nBug #4972: Null deference in ConfigApplyTx\nBug #4969: Libhtp timeout lzma reallocing dictionary\nBug #4953: stream: too aggressive pruning in lossy streams\nBug #4948: SMTP assertion triggered\nBug #4947: suricatasc loop if recv returns no data\nBug #4945: smb: excessive CPU utilization and higher packet processing latency due to excessive calls to Vec::extend_from_slice()\nBug #4941: alerts: 5.0.8/6.0.4 count noalert sigs towards built-in alert limit\nBug #4935: DPDK: Packet counters set incorrectly\nBug #4924: dns: transaction not created when z-bit set\nBug #4920: detect/app-layer-protocol: app-layer-protocol:http broken\nBug #4882: Netmap configuration -- need a configuration option for non-standard library locations.\nBug #4877: Run stream reassembly on both directions upon receiving a FIN packet\nBug #4862: MQTT : transactions are never cleaned by AppLayerParserTransactionsCleanup\nBug #4860: eve.json remove app-layer specific fields from root object\nBug #4859: dnp3: buffer over read in logging base64 empty objects\nBug #4849: protodetect: SMB vs TLS protocol detection in midstream\nBug #4848: TFTP: memory leak due to missing detect state\nBug #4842: smb: excessive memory use during file transfer\nBug #4839: Memory leak with signature using file_data and NFS\nBug #4836: profiling: Invalid performance counter when using sampling\nBug #4828: flow: flows not evicted & freed in time\nBug #4817: smtp: smtp transaction not logged if no email is present\nBug #4812: conf: quadratic complexity\nBug #4811: Range: memory leak from HTTP2\nBug #4810: pppoe decoder fails when protocol identity field is only 1 byte\nBug #4808: flow: worker-evicted flows need to be processed quicker\nBug #4807: packetpool: packets in pool may have capture method ReleasePacket callbacks set\nBug #4804: af-packet: tpacket v3 if/down logic broken\nBug #4803: af-packet: up/down logic leaks resources in autofp (tpacket v2)\nBug #4801: af-packet: tpacket v3 socket reference handling broken\nBug #4800: af-packet: flag collision between kernel and Suricata\nBug #4785: af-packet: threads sometimes get stuck in capture\nBug #4779: flow/bypass: flow worker not performing flow timeout \"housekeeping\"\nBug #4778: flow/bypass: app-layer/stream resources not freed when bypass activated\nBug #4771: pcrexform: does not capture substring but whole match\nBug #4769: dcerpc dce_iface just match a packet\nBug #4767: Rule error in SMB dce_iface and dce_opnum keywords\nBug #4766: Flow leaked when flow->use_cnt access race happens\nBug #4765: loopback: different AF_INET6 values per OS\nBug #4764: range: no validity check with HTTP2 leads to over allocation\nBug #4757: Incomplete range with overlap, and expected new bytes, lead to incomplete reassembly\nBug #4754: Invalid range leads to OOM\nBug #4752: Memory leak in SNMP with DetectEngineState\nBug #4741: Quadratic complexity in modus due to missing tx_iterator\nBug #4739: Absent app-layer protocol is always enabled by default\nBug #4737: ubsan: bytejump warning\nBug #4731: flows: spare pool not freeing flows aggressively enough\nBug #4724: pcre2: scan-build warning\nBug #4722: flows: TCP flow timeout handling stuck if there is no traffic\nBug #4720: pcre2: ASAN heap-buffer-overflow\nBug #4719: http2: byte-range test fails intermittently\nBug #4699: coverity warnings after output changes\nBug #4692: lua: file info callback returns wrong value\nBug #4685: detect: too many prefilter engines lead to FNs\nBug #4681: Wrong list_id with transforms for http_client_body and http file_data\nBug #4680: nfs: failed assert self.tx_data.files_logged > 1\nBug #4679: IPv6 : decoder event on invalid fragment length\nBug #4670: rules: mix of drop and pass rules issues\nBug #4666: http: ipv6 address is a valid host\nBug #4664: ipv6 evasions : fragmentation\nBug #4663: rules: drop rules with noalert not fully dropping\nBug #4659: Configuration test mode succeeds when reference.config file contains invalid content\nBug #4654: tcp: insert_data_normal_fail can hit without triggering memcap\nBug #4650: Stream TCP raw reassembly is leaking\nBug #4622: File deletions over SMB are not always logged\nBug #4621: rust panic: when using smb stream-depth\nBug #4620: Protocol detection : confusion with SMB in midstream\nBug #4619: HTTP2 null dereference in upgrade\nBug #4586: segmentfault when reopen redis\nBug #4582: BUG_ON triggered from TmThreadsInjectFlowById\nBug #4581: Excessive qsort/msort time when large number of rules using tls.fingerprint\nBug #4577: coverity: minor warnings\nBug #4570: eve/flow: many flows logged with reason==unknown\nBug #4563: Rules based on SSH banner-related keywords only match on acked data\nBug #4562: Memory leak in Protocol change during protocol detection\nBug #4561: Failed assertion in SMTP SMTPTransactionComplete\nBug #4560: Quadratic complexity in HTTP2 gzip decompression\nBug #4558: DNP3: intra structure overflow in DNP3DecodeObjectG70V6\nBug #4549: TCP reassembly, failed assert app_progress > last_ack_abs, both sides need to be pruned\nBug #4540:  unused variables warnings on Windows compiles with rust\nBug #4537: alert count shows up as 0 when stats are disabled\nBug #4536: SWF decompression overread\nBug #4534: Timeout in ikev2 parsing\nBug #4533: Rust modbus parser does not handle gaps as it claims\nBug #4530: DOS Quadratic complexity when having too many transactions\nBug #4527: Fix implicit conversions in traffic facing source code modules\nBug #4525: segv with --set cmdline option if incorrect key is provided\nBug #4523: Application log cannot to be re-opened when running as non-root user\nBug #4516: Integer overflows\nBug #4509: Incorrect flags in Rust\nBug #4508: SSH bypass is not working\nBug #4505: Rust panic while parsing (new rust) modbus rule\nBug #4503: Buffer overflow in \"by_rule\" threshold context\nBug #4502: TCP reassembly memuse approaching memcap value results in TCP detection being stopped\nBug #4495: output: threaded output coverity warning\nBug #4494: Failed assertion in HTTP2 decompression\nBug #4491: rules: rules w/o sid accepted, leading to alerts with signature_id: 0\nBug #4478: freebsd: lockups due to mutex handling issues\nBug #4477: Infinite loops in when using InspectionBufferMultipleForList\nBug #4476: heap-buffer-overflow WRITE in InspectionBufferSetup with use of InspectionBufferGetMulti \nBug #4473: Timeout in ftp parsing rs_ftp_active_eprt\nBug #4472: YAML -- interpretation of \"~\" (tilde)\nBug #4448: Properly set the ICMP emergency-bypassed value\nBug #4447: ipv6 & ftp & passive mode & error\nBug #4442: build: Build failure on FreeBSD\nBug #4440: eve: log if flow had gap\nBug #4438: Null-dereference in HTTP2MimicHttp1Request in midstream\nBug #4437: dns: high resource usage on long lived dns connections\nBug #4436: Buffer overread in SMTP SMTPParseCommandBDAT\nBug #4434: Duplicate alert record in eve log when using unix-socket mode\nBug #4433: Debug assert failed in ikev1 logger\nBug #4428: Rust panic in suricata::dcerpc::detect::handle_input_data (buffer overread)\nBug #4425: threaded eve: files not closed on deinitialization\nBug #4424: ftp: Memory leak with duplicate FTP expectation\nBug #4407: threshold: slow startup on threshold.config with many addresses in suppression\nBug #4404: eve/mqtt: mqtt logging crashes when eve is multithreaded\nBug #4403: Use after free or read overflow or use of unitized memory in TransformStripWhitespace called by HttpServerBodyXformsGetDataCallback\nBug #4401: Quadratic complexity in libhtp chunk parsing\nBug #4400: Panic in Rust HTTP2 dynamic headers table eviction\nBug #4397: eve.drop: alerts option logs lowest priority alert\nBug #4395: Incorrect AppLayerResult::incomplete for RDP\nBug #4394: detect: \"drop\" on protocol detect only rule doesn't drop flow\nBug #4389: Protocol detection tls-dcerpc\nBug #4388: Protocol detection evasion enip-dns\nBug #4387: Heap-use-after-free READ 8 · JsonDNP3LoggerToClient\nBug #4379: flow manager: using too much CPU during idle\nBug #4376: TCP flow that retransmits the SYN with a newer TSval not properly tracked\nBug #4375: segv in ApplyToU8Hash\nBug #4369: Configuration test mode succeeds when threshold.config file contains invalid content\nBug #4361: detect: file.data performance regression\nBug #4348: ftp: \"g_expectation_data_id\" and \"g_expectation_id\" in AppLayerExpectationHandle function\nBug #4335: Stack-buffer-overflow READ 4 in SetupU8Hash\nBug #4331: libhtp: don't put stream in error state on compression issues\nBug #4320: Heap use after free in parsing signatures with ip_proto and prefilter\nBug #4280: Suricata is not fully reading or loading the iprep files\nBug #4277: SIGABRT: rust panic  HTTP2State\nBug #4274: Suricata crashes at exit in NFQ mode\nBug #4273: protodetect: SEGV due to NULL ptr deref\nBug #4272: Timeout in libhtp with lzma in gzip to be decompressed in many responses\nBug #4271: datasets: reference counter issue in string lookup\nBug #4267: output: don't use /etc/protocols\nBug #4262: ebpf: llc detection failure\nBug #4261: Mismatch between capture and outputs in rules leads to seg fault\nBug #4258: ftp-data: support for file.name keyword is incomplete\nBug #4254: Leak in signature parsing with urilen\nBug #4253: lua: flowint/flowvar API naming consistency\nBug #4247: detect: NOOPT flag not enforced correctly\nBug #4246: Assertion failed in AdjustToAcked delta > 10000000ULL && delta > stream->window\nBug #4245: SMTP/Email Body md5: Only logs the md5 of the first part in a multi-part mime message\nBug #4239: dataset file not written when run as user\nBug #4238: tcp/fastopen: false positive on \"invalid option\"\nBug #4233: ssl : Integer underflow in ssl parsing SSLV3_HANDSHAKE_PROTOCOL\nBug #4232: Protocol detection evasion enip-SMB\nBug #4231: ICMPv6 failed assert p->icmpv6h == NULL with icmpv6.hdr\nBug #4228: tcp/async: incorrect flagging of ACK values as invalid\nBug #4225: SC_ERROR_CONF_YAML_ERROR anomaly logger error when in socket mode\nBug #4224: modbus: Request flood leads to CPU exhaustion\nBug #4216: 5.0.5 in socket mode crashes when using file-store due to uninitialized stats_ctx\nBug #4211: Not all manpages are built by docs Makefile\nBug #4210: Alert not generated with 2 rules - http.request body (alone) and http.request_body/url_decode\nBug #4208: Suricata crashes with multi-threaded eve logger and HTTP/2 traffic\nBug #4206: dns: output flags not set correctly on 32 bit systems\nBug #4205: eve: Memory leak from jsonbuilder in @MetadataJson@\nBug #4202: Wrong stream side after direction change\nBug #4199: Transformation keyword can’t trigger an alert \nBug #4198: dcerpc: no alert triggered with dce opnum in 6.0\nBug #4187: rs_dcerpc_udp_get_tx takes out unusual amount of CPU\nBug #4171: Failed assert in TCPProtoDetectCheckBailConditions size_ts > 1000000UL\nBug #4152: fatal error: 'gnu/stubs-32.h' file not found\nBug #4106: Duplicate TLS subjects in tls metadata.\nBug #4096: flow manager: 200% CPU in KVM host with no activity with Suricata 6\nBug #4080:  DCERPCUDPState handle fragmented data functions pegging certain CPU cores/threads \nBug #3996: SIGABRT: SMTPTransactionComplete\nBug #3995: SIGABRT stream-tcp-reassemble\nBug #3846: Infinite loop if the sniffing interface temporarily goes down\nBug #3703: fileinfo \"stored: false\" even if the file is kept on disk\nBug #3685: Incorrect logging level for messages\nBug #3542: FTP: expectation created in wrong direction.\nBug #3475: SMB evasion against EICAR file detection\nBug #3419: af-packet: cluster_id is not used when trying to set fanout support\nBug #3109: dcerpc engine not generating alerts\nBug #2809: Applayer Mismatch protocol both directions for kerberos AS-REQ/KDC_ERR_PREAUTH_REQUIRED exchange\nBug #2802: iprep: use_cnt can get desynchronized (SIGABRT)\nBug #2510: Suricata doesnt decompress HTTP Post body \nBug #2190: apparent 1000 character limit in threshold.conf IP lists\nOptimization #5592: tunnel: spinlock for tunnel packet sync\nOptimization #5577: Fix warning about \"comparing with null\" in debug code\nOptimization #5481: tls: support incomplete API to replace internal buffering\nOptimization #5454: http2: slow http2_frames_get_header_value_vec because of allocation\nOptimization #5400: dpdk: allow specifying of `rss_hf` flags in config\nOptimization #5232: rules: pattern id assignment is too slow\nOptimization #5231: rules: mpm setup more costly than needed\nOptimization #5230: rules: too much time spent in DetectUnregisterThreadCtxFuncs due to pcre2\nOptimization #5229: rules: too much time spent in SigMatchListSMBelongsTo at startup\nOptimization #4991: pgsql: convert parser to nom7 functions\nOptimization #4907: smtp: use AppLayerResult instead of buffering wherever possible\nOptimization #4805: af-packet: move vlan hdr insert logic to capture/decode\nOptimization #4795: Remove PASS_IF macro from the FAIL/PASS API\nOptimization #4748: app-layer/rust: explore if tx iterator can be implemented as a trait\nOptimization #4711: Clang 14 and rust nightly new warnings\nOptimization #4653: Flow cleaning with chunked approach is memory hungry\nOptimization #4609: Fix warning about \"if same then else\"\nOptimization #4604: Fix warning about \"branches sharing code\"\nOptimization #4599: Fix warning  about \"ptr_arg\"\nOptimization #4597: Fix warning about \"enum's name\"\nOptimization #4593: Fix warning about \"mixed case hex literals\"\nOptimization #4555: HTTP2: what to do when HTTP upgrade is requested and HTTP2 is disabled ?\nOptimization #4497: rust: clean up constructors of state, transaction structs\nOptimization #4496: decode: remove NULL checks after header casts\nOptimization #4475: Rust: Make default_port in parser registration an Option\nOptimization #4427: storage api: use dedicated 'id' type\nOptimization #4366: decoder: limit number of decoding layers\nOptimization #4319: dcerpc: improve protocol detection\nOptimization #4207: Use configurable or more dynamic @ PACKET_ALERT_MAX@\nOptimization #4154: Rust Parsers: Abstract AppLayer events to a derive macro\nOptimization #4126: Threaded eve logging for output types other than regular file (socket, plugins, redis etc)\nOptimization #4112: Use generic rust DetectU32Data in every keyword needing this\nOptimization #3832: rust: Make core::* as enum to improve readability\nOptimization #3825: Defining only one basic rust Files structure\nOptimization #3658: Use WARN_UNUSED for ByteExtract* functions\nOptimization #3315: app-layer: unify registration logic\nTask #5569: transversal: update references to suricata webpage version 2\nTask #5497: github-ci: update runners using ubuntu-18.04 image\nTask #5475: doc: add exception policy documentation\nTask #5319: add `alert-queue-expand-fails` command-line option\nTask #5179: stats/alert: log out to stats alerts that have been discarded from packet queue\nTask #5175: nfs4: Improve compound record parsers\nTask #5166: quic: Support older versions like Q039 and Q043\nTask #5143: QUIC: support JA3\nTask #5002: applayertemplate: convert parser to nom7 functions\nTask #5001: x509: convert parser to nom7 functions\nTask #5000: rfb: convert parser to nom7 functions\nTask #4999: ntp: convert parser to nom7 functions\nTask #4998: krb: convert parser to nom7 functions\nTask #4997: mime: convert parser to nom7 functions\nTask #4996: rdp: convert parser to nom7 functions\nTask #4995: snmp: convert parser to nom7 functions\nTask #4994: ike: convert parser to nom7 functions\nTask #4993: asn1: convert parser to nom7 functions\nTask #4992: dcerpc: convert parser to nom7 functions\nTask #4970: libhtp 0.5.40\nTask #4915: transversal: update references to suricata webpage\nTask #4912: Update default rule path to /var/lib/suricata/rules.\nTask #4909: devguide: move into userguide as last chapter\nTask #4796: af-packet: remove non-mmap tpacket-v1 support\nTask #4784: config: add suricata version as a comment to the top of the configuration file\nTask #4721: http2: enable by default\nTask #4668: Remove Prelude output\nTask #4667: libhtp 0.5.39\nTask #4446: pcre2: document changes vs prce1 for rule writers\nTask #4444: files: store files in transactions instead of per flow state\nTask #4221: Build Suricata into a static and shared library\nTask #4182: lua: Use lua_pushinteger for pushing integer types as integers instead of floats\nTask #4157: deprecation: remove dns eve v1 logging (May 2022)\nTask #4058: Convert unittests to new FAIL/PASS API: detect-sid.c\nTask #4056: Convert unittests to new FAIL/PASS API: detect-rpc.c\nTask #4053: Convert unittests to new FAIL/PASS API: detect-msg.c\nTask #4038: Convert unittests to new FAIL/PASS API: detect-filesha256.c\nTask #4036: Convert unittests to new FAIL/PASS API: detect-filename.c\nTask #4035: Convert unittests to new FAIL/PASS API: detect-filemd5.c\nTask #4034: Convert unittests to new FAIL/PASS API: detect-filemagic.c\nTask #4033: Convert unittests to new FAIL/PASS API: detect-fileext.c\nTask #4032: Convert unittests to new FAIL/PASS API: detect-file-data.c\nTask #3905: GitHub CI: use sccache for commits build\nTask #3194: pcre2 support\nDocumentation #5511: userguide: add subsection about setting up Suri in IPS mode with DPDK\nDocumentation #5441: userguide: rules meta page updates\nDocumentation #5385: userguide: update rule's format document\nDocumentation #5364: userguide: reorganize `Application Layers Parsers` and `Application layers` subsections in the suricata.yaml page\nDocumentation #5130: doc: add flowbits ORing doc\nDocumentation #4949: userguide: add explanation on max-streams in the suricata.yaml page\nDocumentation #4671: Document changes to HTTP events with respect to http/http2 normalization\nDocumentation #4396: Devguide: Transactions and State overview\nDocumentation #3029: No documentation for \"dcerpc\" keywords\nDocumentation #3017: No documentation for \"rawbytes\" keyword\n\n6.0.1 -- 2020-12-04\n\nFeature #2689: http: Normalized HTTP client body buffer \nFeature #4121: http2: support file inspection API \nBug #1275: ET Rule 2003927 not matching in suricata \nBug #3467: Alert metadata not present in EVE output when using Socket Control Pcap Processing Mode \nBug #3616: strip_whitespace causes FN \nBug #3726: Segmentation fault on rule reload when using libmagic \nBug #3856: dcerpc: last response packet not logged \nBug #3924: asan leak htp_connp_create \nBug #3925: dcerpc: crash in eve logging \nBug #3930: Out of memory from THashInitConfig called by DetectDatasetSetup \nBug #3994: SIGABRT TCPProtoDetectCheckBailConditions \nBug #4018: Napatech: Double release of packet possible in certain error cases. \nBug #4069: dcerpc: fix UDP transaction handling, free_tx, etc \nBug #4071: Null dereference in ipv4hdr GetData \nBug #4072: ssl: Integer underflow in SSL parser \nBug #4073: Protocol detection evasion by packet splitting on enip/SMB \nBug #4074: Timeout while loading many rules with keyword ssl_version \nBug #4076: http2: Memory leak when parsing signature with filestore \nBug #4085: Assertion from AdjustToAcked \nBug #4086: dns: memory leak in v1 dns eve logging \nBug #4090: icmpv4: header handling issue(s) \nBug #4091: byte_math: Offset is a signed value \nBug #4094: AddressSanitizer: dynamic-stack-buffer-overflow (util-crypt) \nBug #4100: ftp: Quadratic complexity in FTPGetOldestTx may lead to DOS \nBug #4109: mac address logging crash \nBug #4110: http: LibHTP wrong protocol with content duplication \nBug #4111: dnp3: DOS in long loop of zero sized objects \nBug #4120: http2: null ptr deref in http2 alert metadata \nBug #4124: dcerpc: UDP request response pair match is incorrect \nBug #4155: dnp3: memory leak when parsing objects with bytearrays \nBug #4156: dnp3: signed integer overflow \nBug #4158: PacketCopyData sets packet length even on failure \nBug #4173: dnp3: SV tests fail on big endian \nBug #4177: Rustc nightly warning getting the inner pointer of a temporary `CString` \nOptimization #4114: Optimize Rust logging macros: SCLogInfo, SCLogDebug and friends \nTask #4137: deprecate: eve.dns v1 record support \nTask #4180: libhtp 0.5.36\n\n6.0.0 -- 2020-10-08\n\nBug #3099: Weird handling of IKEv2 flows when alerts happen\nBug #3691: strip_whitespace doesn't strip_whitespace\nBug #3772: DNP3 probing parser does not detect the proper direction in midstream\nBug #3774: Assert failed in TLS due to integer underflow\nBug #3775: Memory leak in libhtp in error case\nBug #3853: Multi-byte Heap buffer over-read in ssl parser\nBug #3857: Protocol detection evasion by packet splitting on enip/dnp3\nBug #3877: Transaction list grows without bound on parsers that use unidirectional transactions\nBug #3896: app-layer-parser.c:1264: AppLayerParserParse: Assertion `!(res.needed + res.consumed < input_len)' failed.\nBug #3904: Suricata ASAN issue when detect.profiling.grouping.dump-to-disk=true\nBug #3926: dcerpc: Rust panic in handle_common_stub\nBug #3927: Alert \"fileinfo\" array conflicts with \"fileinfo\" event type\nBug #3928: eve: metadata section mixup with anomaly\nBug #3929: Unexpected exit from THashInitConfig called by DetectDatasetSetup\nBug #3930: Out of memory from THashInitConfig called by DetectDatasetSetup\nBug #3931: Memory leak from signature with file.name\nBug #3956: HTTP2 support variable integer lengths for headers\nBug #3972: HTTP2: stream_id_reuse\nBug #3977: SNMP: Better handling of unidirectional transactions\nBug #3978: DHCP: Add unidirectional transaction handling\nBug #3979: IKEv2: Add unidirectional transaction handling\nBug #3980: MQTT: Add unidirectional transaction handling\nBug #3981: SIP: Add unidirectional transaction handling\nBug #3982: RDP: Add unidirectional transaction handling\nBug #3983: KRB5: Add unidirectional transaction handling\nBug #3984: NTP: Add unidirectional transaction handling\nBug #3987: Hang while processing HTTP traffic\nBug #3989: HTTP2: invalid_frame_data anomaly\nBug #3991: Libhtp timeout in data_probe_chunk_length\nBug #3992: RDP incorrect AppLayerResult::incomplete\nBug #3993: Use of uninitialized value in DetectDatarepParse\nBug #3998: HTTP2: invalid header anomaly\nBug #4009: ENIP: Unidirectional transaction handling\nFeature #3955: Protocol detection : run probing parser for protocol found in other direction\nTask #3922: libhtp 0.5.35\nTask #4017: suricata-update: bundle 1.2.0\nDocumentation #2211: doc: document issues with --set and lists in the command line parameters section of the manual\n\n6.0.0-rc1 -- 2020-09-11\n\nFeature #2970: DNS: Parse and extract SOA app layer data from DNS packets\nFeature #3063: protocol decoder: geneve\nTask #3178: json: remove individual loggers\nTask #3559: http: support GAP recovery\nTask #3759: datasets: finalize to move out of 'experimental'\nTask #3824: libhtp 0.5.34\nTask #3868: GitHub CI: Add Fedora 32 runner with ASAN and Suricata-Verify\nTask #3903: remove BUG_ON from app-layer AppLayerResult eval\nDocumentation #3497: Document the removal of unified2 and migration options\nDocumentation #3799: Deprecated configuration keyword in \"Hardware bypass with Netronome\"\nBug #2433: memleak with suppression rules defined in threshold.conf\nBug #3776: Timeout in libhtp due to multiple responses with double lzma encoding\nBug #3816: Coverity scan issue -- null pointer deref in reject dev handling\nBug #3842: eve: logging silently continues if disk is full\nBug #3850: Invalid state for JsonBuilder with metadata signature keyword\nBug #3858: pcap recursive: coverity issues\nBug #3861: flow: check flow bypass handling\nBug #3863: reject: compile warning\nBug #3864: plugin: coverity issues\nBug #3865: flow: coverity issues\nBug #3866: http2: http1 to http2 upgrade support\nBug #3871: Include acsite.m4 in distribution\nBug #3872: Fail CROSS_COMPILE check for PCRE JIT EXEC\nBug #3874: configure: fails to check for netfilter_queue headers on older header packages\nBug #3879: datasets related memleak\nBug #3880: http parsing/alerting - continue\nBug #3882: Plugin support typo\nBug #3883: Runmode Single Memory Leak\nBug #3885: 6.0.0-beta1 stream-tcp-reassemble.c:1066: AdjustToAcked: Assertion `!(adjusted > check)' failed\nBug #3888: 6.0.0-dev - heap-buffer-overflow /opt/suricata/src/flow-manager.c:472:34 in FlowTimeoutHash with AFPv3\nBug #3890: AddressSanitizer: SEGV on unknown address - failed to setup/expand stream segment pool.\nBug #3895: Assert failed in DNS incomplete parsing\nBug #3897: Integer overflow in SCSigOrderByPriorityCompare\nBug #3898: Leak from bad signature with DCERPC keyword, then another protocol keyword\nBug #3902: flow/bypass: SEGV src/flow.c:1158:9 in FlowUpdateState\nBug #3906: mqtt 'assertion failed: `(left == right)` src/mqtt/parser.rs:500:13\nBug #3907: http2 rust - 'index out of bounds: the len is 2 but the index is 63'\nBug #3908: Port prscript to Python 3\nBug #3911: datasets: path handling issues with default-rule-path vs -S <file>\nBug #3913: Memory leak from signature with pcrexform\nBug #3914: Protocol detection gets not retries on protocol change if there is not enough data\nBug #3915: Eve output in threaded mode does not rotate logs on request (eg: SIGHUP)\nBug #3916: Dataset filename not always found on load\nBug #3917: HTTP2 incorrect incomplete after banner\n\n6.0.0-beta1 -- 2020-08-07\n\nFeature #641: Flowbits group for ORing\nFeature #1807: Cisco HDLC Decoder\nFeature #1947: HTTP2 decoder\nFeature #2015: eve: add fileinfo in alert\nFeature #2196: Add flow_id to the file extracted .meta file\nFeature #2311: math on extracted values\nFeature #2312: http: parsing for async streams\nFeature #2385: deprecate: unified2\nFeature #2524: Allow user to choose the reject iface\nFeature #2553: support 'by_both' in threshold rule keyword\nFeature #2694: thresholding: feature parity between global and per-rule options\nFeature #2698: hassh and hasshServer for ssh fingerprinting\nFeature #2859: Oss-fuzz integration\nFeature #3199: transformation should be able to take options\nFeature #3200: pcre: allow operation as transform\nFeature #3293: eve: per thread output files\nFeature #3332: Dynamic Loadable Module/Plugin Support\nFeature #3422: GRE ERSPAN Type 1 Support\nFeature #3444: app-layer: signal stream engine about expected data size\nFeature #3445: Convert SSH parser to Rust\nFeature #3501: Add RFB parser\nFeature #3546: Teredo port configuration\nFeature #3549: Add MQTT parser\nFeature #3626: implement from_end byte_jump keyword\nFeature #3635: datasets: add 'dataset-remove' unix command\nFeature #3661: validate strip_whitespace content before loading a rule\nFeature #3693: DCERPC multi tx support\nFeature #3694: DCERPC logging support\nFeature #3760: datasets: distinguish between 'static' and 'dynamic' sets\nFeature #3823: conditional logging: tx log filtering\nOptimization #749: pcre 8.32 introduces JIT pcre_jit_exec(...)\nOptimization #947: dynamic allocation of thread queues\nOptimization #1038: Flow Queue should be a stack\nOptimization #2779: Convert DCE_RPC from C to Rust\nOptimization #2845: Counters for kernel_packets decreases at times without restart\nOptimization #2977: replace asn1 parser with rust based implementation\nOptimization #3234: dns app-layer c vs rust cleanup\nOptimization #3308: rust: use cbindgen to generate bindings\nOptimization #3538: dns: use app-layer incomplete support\nOptimization #3539: rdp: use app-layer incomplete support\nOptimization #3541: applayertemplate: use app-layer incomplete support\nOptimization #3655: default to c11 standard\nOptimization #3708: Convert SSH logging to JsonBuilder\nOptimization #3709: Convert DNP3 logging to JsonBuilder\nOptimization #3710: Convert SMTP logging to JsonBuilder\nOptimization #3711: Convert NFS logging to JsonBuilder\nOptimization #3712: Convert SMB logging to JsonBuilder\nOptimization #3713: Convert RFB logging to JsonBuilder\nOptimization #3714: Convert FTP logging to JsonBuilder\nOptimization #3715: Convert RDP logging to JsonBuilder\nOptimization #3716: Use uuid crate wherever possible in smb rust parser\nOptimization #3754: Convert KRB to JsonBuilder\nOptimization #3755: Convert IKEv2 to JsonBuilder\nOptimization #3756: Convert SNMP to JsonBuilder\nOptimization #3757: Convert Netflow to JsonBuilder\nOptimization #3764: Convert TFTP to JsonBuilder\nOptimization #3765: Convert Templates to JsonBuilder\nOptimization #3773: DNP3 CRC disabled when fuzzing\nOptimization #3838: Convert 'vars' (metadata logging) to JsonBuilder\nTask #2381: deprecate: 'drop' log output\nTask #2959: deprecate: filestore v1\nTask #3128: nom 5\nTask #3167: convert all _Bool use to bool\nTask #3255: rdp: enable by default\nTask #3256: sip: enable by default\nTask #3331: Rust: Move to 2018 Edition\nTask #3344: devguide: setup sphinx\nTask #3408: FTP should place constraints on filename lengths\nTask #3409: SMTP should place restraints on variable length items (e.g., filenames)\nTask #3460: autotools: check autoscan output\nTask #3515: GRE ERSPAN Type 1 Support configuration\nTask #3564: dcerpc: support GAP recovery\nDocumentation #3335: doc: add ipv4.hdr and ipv6.hdr\nBug #2506: filestore v1: with stream-depth not null, files are never truncated\nBug #2525: Add VLAN support to reject feature\nBug #2639: Alert for tcp rules with established without 3whs\nBug #2726: writing large number of json events on high speed traffic results in packet drops\nBug #2737: Invalid memory read on malformed rule with Lua script\nBug #3053: Replace atoi with StringParse* for better error handling\nBug #3078: flow-timeout: check that 'emergency' settings are < normal settings\nBug #3096: random failures on sip and http-evader suricata-verify tests\nBug #3108: Calculation of threads in autofp mode is wrong\nBug #3188: Use FatalError wherever possible\nBug #3265: Dropping privileges does not work with NFLOG\nBug #3282: --list-app-layer-protos only uses default suricata.yaml location.\nBug #3283: bitmask option of payload-keyword byte_test not working\nBug #3339: Missing community ID in smb, rdp, tftp, dhcp\nBug #3378: ftp: asan detects leaks of expectations\nBug #3435: afl: Compile/make fails on openSUSE Leap-15.1\nBug #3441: alerts: missing rdp and snmp metadata\nBug #3451: gcc10: compilation failure unless -fcommon is supplied\nBug #3463: Faulty signature with two threshold keywords does not generate an error and never match\nBug #3465: build-info and configure wrongly display libnss status\nBug #3468: BUG_ON(strcasecmp(str, \"any\") in DetectAddressParseString\nBug #3476: datasets: Dataset not working in unix socket mode\nBug #3483: SIP: Input not parsed when header values contain trailing spaces\nBug #3486: Make Rust probing parsers optional\nBug #3489: rule parsing: memory leaks\nBug #3490: Segfault when facing malformed SNMP rules\nBug #3496: defrag: asan issue\nBug #3504: http.header.raw prematurely truncates in some conditions\nBug #3509: Behavior for tcp fastopen\nBug #3517: Convert DER parser to Rust\nBug #3519: FTP: Incorrect ftp_memuse calculation.\nBug #3522: TCP Fast Open - Bypass of stateless alerts\nBug #3523: Suricata does not log alert metadata info when running in unix-socket mode\nBug #3525: Kerberos vulnerable to TCP splitting evasion\nBug #3529: rust: smb compile warnings\nBug #3532: Skip over ERF_TYPE_META records\nBug #3547: file logging: complete files sometimes marked 'TRUNCATED'\nBug #3565: ssl/tls: ASAN issue in SSLv3ParseHandshakeType\nBug #3566: rules: minor memory leak involving pcre_get_substring\nBug #3567: rules/bsize: memory issue during parsing\nBug #3568: rules: bad rule leads to memory exhaustion\nBug #3569: fuzz: memory leak in bidir rules\nBug #3570: rfb: invalid AppLayerResult use\nBug #3583: rules: missing 'consumption' of transforms before pkt_data would lead to crash\nBug #3584: rules: crash on 'internal'-only keywords\nBug #3586: rules: bad address block leads to stack exhaustion\nBug #3593: Stack overflow when parsing ERF file\nBug #3594: rules: memory leaks in pktvar keyword\nBug #3595: sslv3: asan detects leaks\nBug #3615: Protocol detection evasion by packet splitting\nBug #3628: Incorrect ASN.1 long form length parsing\nBug #3630: Recursion stack-overflow in parsing YAML configuration\nBug #3631: FTP response buffering against TCP stream\nBug #3632: rules: memory leaks on failed rules\nBug #3638: TOS IP Keyword not triggering an alert\nBug #3640: coverity: leak in fast.log setup error path\nBug #3641: coverity: data directory handling issues\nBug #3642: RFB parser wrongly handles incomplete data\nBug #3643: Libhtp request: extra whitespace interpreted as dummy new request\nBug #3654: Rules reload with Napatech can hang Suricata UNIX manager process\nBug #3657: Multiple DetectEngineReload and bad insertion into linked list lead to buffer overflow\nBug #3662: Signature with an IP range creates one IPOnlyCIDRItem by IP address\nBug #3677: Segfault on SMTP TLS\nBug #3680: Dataset reputation invalid value logging\nBug #3683: rules: memory leak on bad rule\nBug #3687: Null dereference in DetectEngineSignatureIsDuplicate\nBug #3689: Protocol detection evasion by packet splitting on enip/nfs\nBug #3690: eve.json windows timestamp field has \"Eastern Daylight Time\" appended to timestamp\nBug #3699: smb: post-GAP file handling\nBug #3700: nfs: post-GAP file handling\nBug #3720: Incorrect handling of ASN1 relative_offset keyword\nBug #3732: filemagic logging resulting in performance hit\nBug #3749: redis: Reconnect is invalid in batch mode\nBug #3750: redis: no or delayed data in low speed network\nBug #3772: DNP3 probing parser does not detect the proper direction in midstream\nBug #3779: Exit on signature with invalid transform pcrexform\nBug #3783: Stack overflow in DetectFlowbitsAnalyze\nBug #3802: Rule filename mutation when reading file hash files from a directory other than the default-rule-directory\nBug #3808: pfring: compile warnings\nBug #3814: Coverity scan issue -- null pointer deref in ftp logger\nBug #3815: Coverity scan issue -- control flow issue ftp logger\nBug #3817: Coverity scan issue -- resource leak in filestore output logger\nBug #3818: Coverity scan issue -- null pointer deref in detect engine\nBug #3820: ssh: invalid use to 'AppLayerResult::incomplete`\nBug #3821: Memory leak in signature parsing with keyword rfb.secresult\nBug #3822: Rust panic at DCERPC signature parsing\nBug #3840: Integer overflow in DetectContentPropagateLimits leading to unintended signature behavior\nBug #3841: Heap-buffer-overflow READ 8 · DetectGetLastSMByListId\nBug #3851: Invalid DNS incomplete result\nBug #3855: mqtt: coverity static analysis issues\n\n5.0.1 -- 2019-12-13\n\nBug #1871: intermittent abort()s at shutdown and in unix-socket\nBug #2810: enabling add request/response http headers in master\nBug #3047: byte_extract does not work in some situations\nBug #3073: AC_CHECK_FILE on cross compile\nBug #3103: --engine-analysis warning for flow on an icmp request rule\nBug #3120: nfq_handle_packet error -1 Resource temporarily unavailable warnings\nBug #3237: http_accept not treated as sticky buffer by --engine-analysis\nBug #3254: tcp: empty SACK option leads to decoder event\nBug #3263: nfq: invalid number of bytes reported\nBug #3264: EVE DNS Warning about defaulting to v2 as version is not set.\nBug #3266: fast-log: icmp type prints wrong value\nBug #3267: Support for tcp.hdr Behavior\nBug #3275: address parsing: memory leak in error path\nBug #3277: segfault when test a nfs pcap file\nBug #3281: Impossible to cross-compile due to AC_CHECK_FILE\nBug #3284: hash function for string in dataset is not correct\nBug #3286: TCP evasion technique by faking a closed TCP session\nBug #3324: TCP evasion technique by overlapping a TCP segment with a fake packet\nBug #3328: bad ip option evasion\nBug #3340: DNS: DNS over TCP transactions logged with wrong direction.\nBug #3341: tcp.hdr content matches don't work as expected\nBug #3345: App-Layer: Not all parsers register TX detect flags that should\nBug #3346: BPF filter on command line not honored for pcap file\nBug #3362: cross compiling not affecting rust component of suricata\nBug #3376: http: pipelining tx id handling broken\nBug #3386: Suricata is unable to get MTU from NIC after 4.1.0\nBug #3389: EXTERNAL_NET no longer working in 5.0 as expected\nBug #3390: Eve log does not generate pcap_filename when Interacting via unix socket in pcap processing mode\nBug #3397: smtp: file tracking issues when more than one attachment in a tx\nBug #3398: smtp: 'raw-message' option file tracking issues with multi-tx\nBug #3399: smb: post-GAP some transactions never close\nBug #3401: smb1: 'event only' transactions for bad requests never close\nBug #3411: detect/asn1: crashes on packets smaller than offset setting\nTask #3364: configure: Rust 1.37+ has cargo-vendor support bundled into cargo.\nDocumentation #2885: update documentation to indicate -i can be used multiple times\n\n5.0.0 -- 2019-10-15\n\nFeature #1851: add verbosity level description to the help command\nFeature #1940: Debian Jessie - better message when trying to run 2 suricata with afpacket\nFeature #3204: ja3(s): automatically enable when rules require it\nBug #1443: deprecated library calls\nBug #1778: af_packet: IPS and defrag\nBug #2386: check if default log dir is writable at start up\nBug #2465: Eve Stats will not be reported unless stats.log is enabled\nBug #2490: Filehash rule does not fire without filestore keyword\nBug #2668: make install-full fails if CARGO_TARGET_DIR has spaces in the directory path\nBug #2669: make install-full fails due to being unable to find libhtp.so.2\nBug #2955: lua issues on arm (fedora:29)\nBug #3113: python-yaml dependency is actually python3-yaml dependency\nBug #3139: enip: compile warnings on gcc-8\nBug #3143: datasets: don't use list in global config\nBug #3190: file_data inspection inhibited by additional (non-file_data) content match rule\nBug #3196: Distributed archive do not include eBPF files\nBug #3209: Copy engine provided classification.config to $datadir/suricata.\nBug #3210: Individual output log levels capped by the default log level\nBug #3216: MSN protocol detection/parser is not working\nBug #3223: --disable-geoip does not work\nBug #3226: ftp: ASAN error\nBug #3232: Static build with pcap fails\nOptimization #3039: configure: don't generate warnings on missing features\nDocumentation #2640: http-body and http-body-printable in eve-log require metadata to be enabled, yet there is no indication of this anywhere\nDocumentation #2839: Update perf and tuning user guides\nDocumentation #2876: doc: add nftables with nfqueue section\nDocumentation #3207: Update the http app-layer doc and config\nDocumentation #3230: EVE DNS logger defaults to version 2 instead of version when version not specified.\n\n5.0.0-rc1 -- 2019-09-24\n\nFeature #524: detect double encoding in URI\nFeature #713: tls.fingerprint - file usage\nFeature #997: Add libhtp event for every htp_log() that needs an event.\nFeature #1203: TCP Fast Open support\nFeature #1249: http/dns ip-reputation alike technique\nFeature #1757: URL Reputation\nFeature #2200: Dynamically add md5 to blacklist without full restart\nFeature #2283: turn content modifiers into 'sticky buffers'\nFeature #2314: protocol parser: rdp\nFeature #2315: eve: ftp logging\nFeature #2318: matching on large amounts of data with dynamic updates\nFeature #2529: doc: include quick start guide\nFeature #2539: protocol parser: vxlan\nFeature #2670: tls_cert sticky buffer\nFeature #2684: Add JA3S\nFeature #2738: SNMP parser, logging and detection\nFeature #2754: JA3 and JA3S - sets / reputation\nFeature #2758: intel / reputation matching on arbitrary data\nFeature #2789: Use clang for building eBPF programs even if Suricata is built using GCC\nFeature #2916: FTP decoder should have Rust port parsers\nFeature #2940: document anomaly log\nFeature #2941: anomaly log: add protocol detection events\nFeature #2952: modernize http_header_names\nFeature #3011: Add new 'cluster_peer' runmode to allow for load balancing by IP header (src<->dst) only\nFeature #3058: Hardware offload for XDP bypass\nFeature #3059: Use pinned maps in XDP bypass\nFeature #3060: Add way to detect TCP MSS values\nFeature #3061: Add way to inspect TCP header\nFeature #3062: Add way to inspect UDP header\nFeature #3074: DNS full domain matching within the dns_query buffer\nFeature #3080: Provide a IP pair XDP load balancing\nFeature #3081: Decapsulation of GRE in XDP filter\nFeature #3084: SIP parser, logging and detection\nFeature #3165: New rule keyword: dns.opcode; For matching on the opcode in the DNS header.\nBug #941: Support multiple stacked compression, compression that specifies the wrong compression type\nBug #1271: Creating core dump with dropped privileges\nBug #1656: several silent bypasses at the HTTP application level (chunking, compression, HTTP 0.9...)\nBug #1776: Multiple Content-Length headers causes HTP_STREAM_ERROR\nBug #2080: Rules with bad port group var do not error\nBug #2146: DNS answer not logged with eve-log\nBug #2210: logging: SC_LOG_OP_FILTER still displays some lines not matching filter\nBug #2264: file-store.stream-depth not working as expected when configured to a specific value\nBug #2395: File_data inspection depth while inspecting base64 decoded data\nBug #2619: Malformed HTTP causes FN using http_header_names;\nBug #2626: doc/err: More descriptive message on err for escaping backslash\nBug #2654: Off-by-one iteration of EBPF flow_table_vX in EBPFForEachFlowVXTable (util-ebpf.c)\nBug #2655: GET/POST HTTP-request with no Content-Length, http_client_body miss\nBug #2662: unix socket - memcap read/set showing unlimited where there are limited values configured by default\nBug #2686: Fancy Quotes in Documentation\nBug #2765: GeoIP keyword depends on now discontinued legacy GeoIP database\nBug #2769: False positive alerts firing after upgrade suricata 3.0 -> 4.1.0\nBug #2786: make install-full does not install some source events rules\nBug #2840: xdp modes - Invalid argument (-22) on certain NICs\nBug #2847: Confusing warning “Rule is inspecting both directions” when inspecting engine analysis output\nBug #2853: filestore (v1 and v2): dropping of \"unwanted\" files\nBug #2926: engine-analysis with content modifiers not always issues correct warning\nBug #2942: anomaly log: app layer events\nBug #2951: valgrind warnings in ftp\nBug #2953: bypass keyword: Suricata 4.1.x Segmentation Faults\nBug #2961: filestore: memory leaks\nBug #2965: Version 5 Beta1 - Multiple NFQUEUE failed\nBug #2986: stream bypass not making callback as expected\nBug #2992: Build failure on m68k with uclibc\nBug #2999: AddressSanitizer: heap-buffer-overflow in HTPParseContentRange\nBug #3000: tftp: missing logs because of broken tx handling\nBug #3004: SC_ERR_PCAP_DISPATCH with message \"error code -2\" upon rule reload completion\nBug #3006: improve rule keyword alproto registration\nBug #3007: rust: updated libc crate causes deprecation warnings\nBug #3009: Fixes warning about size of integers in string formats\nBug #3051: mingw/msys: compile errors\nBug #3054: Build failure with --enable-rust-debug\nBug #3070: coverity warnings in protocol detection\nBug #3072: Rust nightly warning\nBug #3076: Suricata sometimes doesn't store the vlan id when vlan.use-for-tracking is false\nBug #3089: Fedora rawhide af-packet compilation err\nBug #3098: rule-reloads Option?\nBug #3111: ftp warnings during compile\nBug #3112: engine-analysis warning on http_content_type\nBug #3133: http_accept_enc warning with engine-analysis\nBug #3136: rust: Remove the unneeded macros\nBug #3138: Don't install Suricata provided rules to /etc/suricata/rules as part of make install-rules.\nBug #3140: ftp: compile warnings on gcc-8\nBug #3158: 'wrong thread' tracking inaccurate for bridging IPS modes\nBug #3162: TLS Lua output does not work without TLS log\nBug #3169: tls: out of bounds read (5.x)\nBug #3171: defrag: out of bounds read (5.x)\nBug #3176: ipv4: ts field decoding oob read (5.x)\nBug #3177: suricata is logging tls log repeatedly if custom mode is enabled\nBug #3185: decode/der: crafted input can lead to resource starvation (5.x)\nBug #3189: NSS Shutdown triggers crashes in test mode (5.x)\nOptimization #879: update configure.ac with autoupdate\nOptimization #1218: BoyerMooreNocase could avoid tolower() call\nOptimization #1220: Boyer Moore SPM pass in ctx instead of individual bmBc and bmBg\nOptimization #2602: add keywords to --list-keywords output\nOptimization #2843: suricatact/filestore/prune: check that directory is a filestore directory before removing files\nOptimization #2848: Rule reload when run with -s or -S arguments\nOptimization #2991: app-layer-event keyword tx handling\nOptimization #3005: make sure DetectBufferSetActiveList return codes are always checked\nOptimization #3077: FTP parser command lookup\nOptimization #3085: Suggest more appropriate location to store eBPF binaries\nOptimization #3137: Make description of all keywords consistent and pretty\nTask #2629: tracking: Rust 2018 edition\nTask #2974: detect: check all keyword urls\nTask #3014: Missing documentation for \"flags\" option\nTask #3092: Date of revision should also be a part of info from suricata -v\nTask #3135: counters: new default for decoder events\nTask #3141: libhtp 0.5.31\n\n5.0.0-beta1 -- 2019-04-30\n\nFeature #884: add man pages\nFeature #984: libhtp HTP_AUTH_UNRECOGNIZED\nFeature #1970: json: make libjansson mandatory\nFeature #2081: document byte_test\nFeature #2082: document byte_jump\nFeature #2083: document byte_extract\nFeature #2282: event log aka weird.log\nFeature #2332: Support for common http response headers - Location and Server\nFeature #2421: add system mode and user mode\nFeature #2459: Support of FTP active mode\nFeature #2484: no stream events after known pkt loss in flow\nFeature #2485: http: log byte range with file extraction\nFeature #2507: Make Rust mandatory\nFeature #2561: Add possibility for smtp raw extraction\nFeature #2563: Add dump of all headers in http eve-log\nFeature #2572: extend protocol detection to specify flow direction\nFeature #2741: netmap: add support for lb and vale switches\nFeature #2766: Simplified Napatech Configuration\nFeature #2820: pcap multi dev support for Windows (5.0.x)\nFeature #2837: Add more custom HTTP Header values for HTTP JSON Logging\nFeature #2895: OpenBSD pledge support\nFeature #2897: update http_content_type and others to new style sticky buffers\nFeature #2914: modernize tls sticky buffers\nFeature #2930: http_protocol: use mpm and content inspect v2 apis\nFeature #2937: sticky buffer access from lua script\nOptimization #2530: Print matching rule SID in filestore meta file\nOptimization #2632: remove C implementations where we have Rust as well\nOptimization #2793: Python 3 support for python tools\nOptimization #2808: Prefer Python 3 in ./configure\nBug #1013: command line parsing\nBug #1324: vlan tag in eve.json\nBug #1427: configure with libnss and libnspr\nBug #1694: unix-socket reading 0 size pcap\nBug #1860: 2220005: SURICATA SMTP bdat chunk len exceeded when using SMTP connection caching\nBug #2057: eve.json flow logs do not contain in_iface\nBug #2432: engine-analysis does not print out the tls buffers\nBug #2503: rust: nom 4.2 released\nBug #2527: FTP file extraction only working in passive mode\nBug #2605: engine-analysis warning on PCRE\nBug #2733: rust/mingw: libc::IPPROTO_* not defined\nBug #2751: Engine unable to disable detect thread, Killing engine. (in libpcap mode)\nBug #2775: dns v1/2 with rust results in less app layer data available in the alert record (for dns related alerts/rules)\nBug #2797: configure.ac: broken --{enable,disable}-xxx options\nBug #2798: --engine-analysis is unaware of http_host buffer\nBug #2800: Undocumented commands for suricatasc\nBug #2812: suricatasc multiple python issues\nBug #2813: suricatasc: failure with extra commands\nBug #2817: Suricata.yaml encrypt-handling instead encryption-handling\nBug #2821: netmap/afpacket IPS: stream.inline: auto broken (5.0.x)\nBug #2822: SSLv3 - AddressSanitizer heap-buffer-overflow (5.0.x)\nBug #2833: mem leak - rules loading hunt rules\nBug #2838: 4.1.x gcc 9 compilation warnings\nBug #2844: alignment issues in dnp3\nBug #2846: IPS mode crash under load (5.0.x)\nBug #2857: nfq asan heap-use-after-free error\nBug #2877: rust: windows build fails in gen-c-headers.py\nBug #2889: configure doesn't display additional information for missing requirements\nBug #2896: smb 1 create andx request does not parse the filename correctly (master)\nBug #2899: Suricata 4.1.2 and up to 5.x Dev branch - Make compile issue when using PF_ring library on Redhat only\nBug #2901: pcap logging with lz4 coverity warning (master)\nBug #2909: segfault on logrotation when the files cannot be opened\nBug #2912: memleaks in nflog\nBug #2915: modernize ssh sticky buffers\nBug #2921: chmod file mode warning expressed in incorrect base\nBug #2929: error messages regarding byte jump and byte extract\nBug #2944: ssh: heap buffer overflow (master)\nBug #2945: mpls: heapbuffer overflow in file decode-mpls.c (master)\nBug #2946: decode-ethernet: heapbuffer overflow in file decode-ethernet.c (master)\nBug #2947: rust/dhcp: panic in dhcp parser (master)\nBug #2948: mpls: cast of misaligned data leads to undefined behaviour (master)\nBug #2949: rust/ftp: panic in ftp parser (master)\nBug #2950: rust/nfs: integer underflow (master)\nTask #2297: deprecate: dns.log\nTask #2376: deprecate: files-json.log\nTask #2379: deprecate: Tilera / Tile support\nTask #2849: Remove C SMB parser.\nTask #2850: Remove C DNS parsers.\n\n4.1.2 -- 2018-12-21\n\nFeature #1863: smtp: improve pipelining support\nFeature #2748: bundle libhtp 0.5.29\nFeature #2749: bundle suricata-update 1.0.3\nBug #2682: python-yaml Not Listed As Ubuntu Prerequisite\nBug #2736: DNS Golden Transaction ID - detection bypass\nBug #2745: Invalid detect-engine config could lead to segfault\nBug #2752: smb: logs for IOCTL and DCERPC have tree_id value of 0\n\n4.1.1 -- 2018-12-17\n\nFeature #2637: af-packet: improve error output for BPF loading failure\nFeature #2671: Add Log level to suricata.log when using JSON type\nBug #2502: suricata.c ConfigGetCaptureValue - PCAP/AFP fallthrough to strip_trailing_plus\nBug #2528: krb parser not always parsing tgs responses\nBug #2633: Improve errors handling in AF_PACKET\nBug #2653: llc detection failure in configure.ac\nBug #2677: coverity: ja3 potential memory leak\nBug #2679: build with profiling enabled on generates compile warnings\nBug #2704: DNSv1 for Rust enabled builds.\nBug #2705: configure: Test for PyYAML and disable suricata-update if not installed.\nBug #2716: Stats interval are 1 second too early each tick\nBug #2717: nfs related panic in 4.1\nBug #2719: Failed Assertion, Suricata Abort - util-mpm-hs.c line 163 (4.1.x)\nBug #2723: dns v2 json output should always set top-level rrtype in responses\nBug #2730: rust/dns/lua - The Lua calls for DNS values when using Rust don't behave the same as the C implementation.\nBug #2731: multiple instances of transaction loggers are broken\nBug #2734: unix runmode deadlock when using too many threads\n\n4.1.0 -- 2018-11-06\n\nBug #2467: 4.1beta1 - non rust builds with SMB enabled\nBug #2657: smtp segmentation fault\nBug #2663: libhtp 0.5.28\n\n4.1.0-rc2 -- 2018-10-16\n\nFeature #2279: TLS 1.3 decoding, SNI extraction and logging\nFeature #2562: Add http_port in http eve-log if specified in the hostname\nFeature #2567: multi-tenancy: add 'device' selector\nFeature #2638: community flow id\nOptimization #2579: tcp: SegmentSmack\nOptimization #2580: ip: FragmentSmack\nBug #2100: af_packet: High latency\nBug #2212: profiling: app-layer profiling shows time spent in HTTP on UDP\nBug #2419: Increase size of length of Decoder handlers from uint16 to uint32\nBug #2491: async-oneside and midstream not working as expected\nBug #2522: The cross-effects of rules on each other, without the use of flowbits.\nBug #2541: detect-parse: missing space in error message\nBug #2552: \"Drop\" action is logged as \"allowed\" in af_packet and netmap modes\nBug #2554: suricata does not detect a web-attack\nBug #2555: Ensure strings in eve-log are json-encodable\nBug #2558: negated fileext and filename do not work as expected\nBug #2559: DCE based rule false positives\nBug #2566: memleak: applayer dhcp with 4.1.0-dev (rev 9370805)\nBug #2570: Signature affecting another's ability to detect and alert\nBug #2571: coredump: liballoc/vec.rs dhcp\nBug #2573: prefilter keyword doesn't work when detect.prefilter.default=mpm\nBug #2574: prefilter keyword as alias for fast_pattern is broken\nBug #2603: memleak/coredump: Ja3BufferInit\nBug #2604: memleak: DetectEngineStateAlloc with ipsec-events.rules\nBug #2606: File descriptor leak in af-packet mode\nBug #2615: processing of nonexistent pcap \n\n4.1.0-rc1 -- 2018-07-20\n\nFeature #2292: flow: add icmpv4 and improve icmpv6 flow handling\nFeature #2298: pcap: store pcaps in compressed form\nFeature #2416: Increase XFF coverage to files and http log\nFeature #2417: Add Option to Delete Pcap Files After Processing\nFeature #2455: Add WinDivert source to Windows builds\nFeature #2456: LZ4 compression for pcap logs\nOptimization #2461: Let user to explicit disable libnss and libnspr support\nBug #1929: yaml: ConfYamlHandleInclude memleak\nBug #2090: Rule-reload in multi-tenancy is buggy\nBug #2217: event_type flow is missing icmpv4 (while it has icmpv6) info wherever available\nBug #2463: memleak: gitmaster flash decompression - 4.1.0-dev (rev efdc592)\nBug #2469: The autoconf script throws and error when af_packet is enabled and then continues\nBug #2481: integer overflow caused by casting uin32 to uint16 in detection\nBug #2492: Inverted IP params in fileinfo events\nBug #2496: gcc 8 warnings\nBug #2498: Lua file output script causes a segfault when protocol is not HTTP\nBug #2501: Suricata stops inspecting TCP stream if a TCP RST was met\nBug #2504: ntp parser update cause build failure\nBug #2505: getrandom prevents any suricata start commands on more later OS's\nBug #2511: Suricata gzip unpacker bypass\nBug #2515: memleak: when using smb rules without rust\nBug #2516: Dead lock caused by unix command register-tenant\nBug #2518: Tenant rules reload completely broken in 4.x.x\nBug #2520: Invalid application layer logging in alert for DNS\nBug #2521: rust: dns warning during compile\nBug #2536: libhtp 0.5.27\nBug #2542: ssh out of bounds read\nBug #2543: enip out of bounds read\n\n4.1.0-beta1 -- 2018-03-22\n\nFeature #550: Extract file attachments from FTP\nFeature #646: smb log feature to be introduced\nFeature #719: finish/enable smb2 app layer parser\nFeature #723: Add support for smb 3\nFeature #724: Prevent resetting in UNIX socket mode\nFeature #735: Introduce content_len keyword\nFeature #741: Introduce endswith keyword\nFeature #742: startswith keyword\nFeature #1006: transformation api\nFeature #1198: more compact dns logging\nFeature #1201: file-store metadata in JSON format\nFeature #1386: offline: add pcap file name to EVE\nFeature #1458: unix-socket - make rule load errs available\nFeature #1476: Suricata Unix socket PCAP processing stats should not need to reset after each run\nFeature #1579: Support Modbus Unit Identifier\nFeature #1585: unix-socket: improve information regarding ruleset\nFeature #1600: flash file decompression for file_data\nFeature #1678: open umask settings or make them configurable\nFeature #1948: allow filestore name configuration options\nFeature #1949: only write unique files\nFeature #2020: eve: add body of signature to eve.json alert\nFeature #2062: tls: reimplement tls.fingerprint\nFeature #2076: Strip whitespace from buffers\nFeature #2086: DNS answer for a NS containing multiple name servers should only be one line\nFeature #2142: filesize: support other units than only bytes\nFeature #2192: JA3 TLS client fingerprinting\nFeature #2199: DNS answer events compacted\nFeature #2222: Batch submission of PCAPs over the socket\nFeature #2253: Log rule metadata in alert event\nFeature #2285: modify memcaps over unix socket\nFeature #2295: decoder: support PCAP LINKTYPE_IPV4\nFeature #2299: pcap: read directory with pcaps from the command-line\nFeature #2303: file-store enhancements (aka file-store v2): deduplication; hash-based naming; json metadata and cleanup tooling\nFeature #2352: eve: add \"metadata\" field to alert (rework of vars)\nFeature #2382: deprecate: CUDA support\nFeature #2399: eBPF and XDP bypass for AF_PACKET capture method\nFeature #2464: tftp logging\nOptimization #2193: random: support getrandom(2) if available\nOptimization #2302: rule parsing: faster parsing by not using pcre\nBug #993: libhtp upgrade to handle responses first\nBug #1503: lua output setup failure does not exit engine with --init-errors-fatal\nBug #1788: af-packet coverity warning\nBug #1842: Duplicated analyzer in Prelude alert\nBug #1904: modbus: duplicate alerts / detection unaware of direction\nBug #2202: BUG_ON asserts in AppLayerIncFlowCounter\nBug #2229: mem leak AFP with 4.0.0-dev (rev 1180687)\nBug #2240: suricatasc dump-counters returns error when return message is larger than 4096\nBug #2252: Rule parses in 4.0 when flow to client is set and http_client_body is used.\nBug #2258: rate_filter inconsistency: triggered after \"count\" detections when by_rule, and after count+1 detections when by_src/by_dst.\nBug #2268: Don't printf util-enum errors\nBug #2288: Suricata segfaults on ICMP and flowint check\nBug #2294: rules: depth < content rules not rejected (master)\nBug #2307: segfault in http_start with 4.1.0-dev (rev 83f220a)\nBug #2335: conf: stack-based buffer-overflow in ParseFilename\nBug #2345: conf: Memory-leak in DetectAddressTestConfVars\nBug #2346: conf: NULL-pointer dereference in ConfUnixSocketIsEnable\nBug #2347: conf: use of NULL-pointer in DetectLoadCompleteSigPath\nBug #2349: conf: multiple NULL-pointer dereferences in FlowInitConfig\nBug #2353: Command Line Options Ignored with pcap-file-continuous setting\nBug #2354: conf: multiple NULL-pointer dereferences in StreamTcpInitConfig\nBug #2356: coverity issues in new pcap file/directory handling\nBug #2360: possible deadlock with signal handling\nBug #2364: rust/dns: logging missing string versions of rtypes and rcodes\nBug #2365: rust/dns: flooded by 'LogDnsLogger not implemented for Rust DNS'\nBug #2367: Conf: Multiple NULL-pointer dereferences in HostInitConfig\nBug #2368: Conf: Multiple NULL-pointer dereferences after ConfGetBool in StreamTcpInitConfig\nBug #2370: Conf: Multiple NULL-pointer dereferences in PostConfLoadedSetup\nBug #2390: mingw linker error with rust\nBug #2391: libhtp 0.5.26\nBug #2394: Pcap Directory May Miss Files\nBug #2397: Call to panic()! macro in Rust NFS decoder causes crash on malformed NFS traffic\nBug #2398: Lua keyword cmd help documentation pointing to old docs\nBug #2402: http_header_names doesn't operate as documented\nBug #2403: Crash for offline pcap mode when running in single mode\nBug #2407: Fix timestamp offline when pcap timestamp is zero\nBug #2408: fix print backslash in PrintRawUriFp\nBug #2414: NTP parser registration frees used memory\nBug #2418: Skip configuration \"include\" nodes when file is empty\nBug #2420: Use pthread_sigmask instead of sigprogmask for signal handling\nBug #2425: DNP3 memcpy buffer overflow\nBug #2427: Suricata 3.x.x and 4.x.x do not parse HTTP responses if tcp data was sent before 3-way-handshake completed\nBug #2430: http eve log data source/dest flip\nBug #2437: rust/dns: Core Dump with malformed traffic\nBug #2442: der parser: bad input consumes cpu and memory\nBug #2446: http bodies / file_data: thread space creation writing out of bounds (master)\nBug #2451: Missing Files Will Cause Pcap Thread to No Longer Run in Unix Socket Mode\nBug #2454: master - suricata.c:2473-2474 - SIGUSR2 not wrapped in #ifndef OS_WIN32\nBug #2466: [4.1beta1] Messages with SC_LOG_CONFIG level are logged to syslog with EMERG priority\n\n4.0.1 -- 2017-10-18\n\nBug #2050: TLS rule mixes up server and client certificates\nBug #2064: Rules with dual classtype do not error\nBug #2074: detect msg: memory leak\nBug #2102: Rules with dual sid do not error\nBug #2103: Rules with dual rev do not error\nBug #2151: The documentation does not reflect current suricata.yaml regarding cpu-affinity\nBug #2194: rust/nfs: sigabrt/rust panic - 4.0.0-dev (rev fc22943)\nBug #2197: rust build with lua enabled fails on x86\nBug #2201: af_packet: suricata leaks memory with use-mmap enabled and incorrect BPF filter\nBug #2207: DNS UDP \"Response\" parsing recording an incorrect value\nBug #2208: mis-structured JSON stats output if interface name is shortened\nBug #2226: improve error message if stream memcaps too low\nBug #2228: enforcing specific number of threads with autofp does not seem to work\nBug #2244: detect state uses broken offset logic (4.0.x)\nFeature #2114: Redis output: add RPUSH support\nFeature #2152: Packet and Drop Counters for Napatech\n\n4.0.0 -- 2017-07-27\n\nFeature #2138: Create a sample systemd service file.\nFeature #2184: rust: increase minimally supported rustc version to 1.15\nBug #2169: dns/tcp: response traffic leads to 'app_proto_tc: failed'\nBug #2170: Suricata fails on large BPFs with AF_PACKET\nBug #2185: rust: build failure if libjansson is missing\nBug #2186: smb dcerpc segfaults in StubDataParser\nBug #2187: hyperscan: mpm setup error leads to crash\n\n4.0.0-rc2 -- 2017-07-13\n\nFeature #744: Teredo configuration\nFeature #1748: lua: expose tx in alert lua scripts\nBug #1855: alert number output\nBug #1888: noalert in a pass rule disables the rule\nBug #1957: PCRE lowercase enforcement in http_host buffer does not allow for upper case in hex-encoding\nBug #1958: Possible confusion or bypass within the stream engine with retransmits.\nBug #2110: isdataat: keyword memleak\nBug #2162: rust/nfs: reachable asserting rust panic\nBug #2175: rust/nfs: panic - 4.0.0-dev (rev 7c25a2d)\nBug #2176: gcc 7.1.1 'format truncation' compiler warnings\nBug #2177: asn1/der: stack overflow\n\n4.0.0-rc1 -- 2017-06-28\n\nFeature #2095: eve: http body in alert event\nFeature #2131: nfs: implement GAP support\nFeature #2156: Add app_proto or partial flow entry to alerts\nFeature #2163: ntp parser\nFeature #2164: rust: external parser crate support\nBug #1930: Segfault when event rule is invalid\nBug #2038: validate app-layer API use\nBug #2101: unix socket: stalling due to being unable to disable detect thread\nBug #2109: asn1: keyword memleak\nBug #2117: byte_extract and byte_test collaboration doesnt work on 3.2.1\nBug #2141: 4.0.0-dev (rev 8ea9a5a) segfault\nBug #2143: Bypass cause missing alert on packets only signatures\nBug #2144: rust: panic in dns/tcp\nBug #2148: rust/dns: panic on malformed rrnames\nBug #2153: starttls 'tunnel' packet issue - nfq_handle_packet error -1\nBug #2154: Dynamic stack overflow in payload printable output\nBug #2155: AddressSanitizer double-free error\nBug #2157: Compilation Issues Beta 4.0\nBug #2158: Suricata v4.0.0-beta1 dns_query; segmentation fault\nBug #2159: http: 2221028 triggers on underscore in hostname\nBug #2160: openbsd: pcap with raw datalink not supported\nBug #2161: libhtp 0.5.25\nBug #2165: rust: releases should include crate dependencies (cargo-vendor)\n\n4.0.0-beta1 -- 2017-06-07\n\nFeature #805: Add support for applayer change\nFeature #806: Implement STARTTLS support\nFeature #1636: Signal rotation of unified2 log file without restart\nFeature #1953: lua: expose flow_id\nFeature #1969: TLS transactions with session resumption are not logged\nFeature #1978: Using date in logs name\nFeature #1998: eve.tls: custom TLS logging\nFeature #2006: tls: decode certificate serial number\nFeature #2011: eve.alert: print outside IP addresses on alerts on traffic inside tunnels\nFeature #2046: Support custom file permissions per logger\nFeature #2061: lua: get timestamps from flow\nFeature #2077: Additional HTTP Header Contents and Negation\nFeature #2123: unix-socket: additional runmodes\nFeature #2129: nfs: parser, logger and detection\nFeature #2130: dns: rust parser with stateless behaviour\nFeature #2132: eve: flowbit and other vars logging\nFeature #2133: unix socket: add/remove hostbits\nBug #1335: suricata option --pidfile overwrites any file\nBug #1470: make install-full can have race conditions on OSX.\nBug #1759: CentOS5 EOL tasks\nBug #2037: travis: move off legacy support\nBug #2039: suricata stops processing when http-log output via unix_stream backs up\nBug #2041: bad checksum 0xffff\nBug #2044: af-packet: faulty VLAN handling in tpacket-v3 mode\nBug #2045: geoip: compile warning on CentOS 7\nBug #2049: Empty rule files cause failure exit code without corresponding message\nBug #2051: ippair: xbit unset memory leak\nBug #2053: ippair: pair is direction sensitive\nBug #2070: file store: file log / file store mismatch with multiple files\nBug #2072: app-layer: fix memleak on bad traffic\nBug #2078: http body handling: failed assertion\nBug #2088: modbus: clang-4.0 compiler warnings\nBug #2093: Handle TCP stream gaps.\nBug #2097: \"Name of device should not be null\" appears in suricata.log when using pfring with configuration from suricata.yaml\nBug #2098: isdataat: fix parsing issue with leading spaces\nBug #2108: pfring: errors when compiled with asan/debug\nBug #2111: doc: links towards http_header_names\nBug #2112: doc: links towards certain http_ keywords not working\nBug #2113: Race condition starting Unix Server\nBug #2118: defrag - overlap issue in linux policy\nBug #2125: ASAN SEGV - Suricata version 4.0dev (rev 922a27e)\nOptimization #521: Introduce per stream thread segment pool\nOptimization #1873: Classtypes missing on decoder-events,files, and stream-events\n\n3.2.1 -- 2017-02-15\n\nFeature #1951: Allow building without libmagic/file\nFeature #1972: SURICATA ICMPv6 unknown type 143 for MLDv2 report\nFeature #2010: Suricata should confirm SSSE3 presence at runtime when built with Hyperscan support\nBug #467: compilation with unittests & debug validation\nBug #1780: VLAN tags not forwarded in afpacket inline mode\nBug #1827: Mpm AC fails to alloc memory\nBug #1843: Mpm Ac: int overflow during init\nBug #1887: pcap-log sets snaplen to -1\nBug #1946: can't get response info in some situation\nBug #1973: suricata fails to start because of unix socket\nBug #1975: hostbits/xbits memory leak\nBug #1982: tls: invalid record event triggers on valid traffic\nBug #1984: http: protocol detection issue if both sides are malformed\nBug #1985: pcap-log: minor memory leaks\nBug #1987: log-pcap: pcap files created with invalid snaplen\nBug #1988: tls_cert_subject bug\nBug #1989: SMTP protocol detection is case sensitive\nBug #1991: Suricata cannot parse ports: \"![1234, 1235]\"\nBug #1997: tls-store: bug that cause Suricata to crash\nBug #2001: Handling of unsolicited DNS responses.\nBug #2003: BUG_ON body sometimes contains side-effectual code\nBug #2004: Invalid file hash computation when force-hash is used\nBug #2005: Incoherent sizes between request, capture and http length\nBug #2007: smb: protocol detection just checks toserver\nBug #2008: Suricata 3.2, pcap-log no longer works due to timestamp_pattern PCRE\nBug #2009: Suricata is unable to get offloading settings when run under non-root\nBug #2012: dns.log does not log unanswered queries\nBug #2017: EVE Log Missing Fields\nBug #2019: IPv4 defrag evasion issue\nBug #2022: dns: out of bound memory read\n\n3.2 -- 2016-12-01\n\nBug #1117: PCAP file count does not persist\nBug #1577: luajit scripts load error\nBug #1924: Windows dynamic DNS updates trigger 'DNS malformed request data' alerts\nBug #1938: suricata: log handling issues\nBug #1955: luajit script init failed\nBug #1960: Error while parsing rule with PCRE keyword with semicolon\nBug #1961: No error on missing semicolon between depth and classtype\nBug #1965: dnp3/enip/cip keywords naming convention\nBug #1966: af-packet fanout detection broken on Debian Jessie (master)\n\n3.2RC1 -- 2016-11-01\n\nFeature #1906: doc: install man page and ship pdf\nFeature #1916: lua: add an SCPacketTimestamp function\nFeature #1867: rule compatibility: flow:not_established not supported.\nBug #1525: Use pkg-config for libnetfilter_queue\nBug #1690: app-layer-proto negation issue\nBug #1909: libhtp 0.5.23\nBug #1914: file log always shows stored: no even if file is stored\nBug #1917: nfq: bypass SEGV\nBug #1919: filemd5: md5-list does not allow comments any more\nBug #1923: dns - back to back requests results in loss of response\nBug #1928: flow bypass leads to memory errors\nBug #1931: multi-tenancy fails to start\nBug #1932: make install-full does not install tls-events.rules\nBug #1935: Check redis reply in non pipeline mode\nBug #1936: Can't set fast_pattern on tls_sni content\n\n3.2beta1 -- 2016-10-03\n\nFeature #509: add SHA1 and SHA256 checksum support for files\nFeature #1231: ssl_state negation support\nFeature #1345: OOBE -3- disable NIC offloading by default\nFeature #1373: Allow different reassembly depth for filestore rules\nFeature #1495: EtherNet/IP and CIP support\nFeature #1583: tls: validity fields (notBefore and notAfter)\nFeature #1657: Per application layer stats\nFeature #1896: Reimplement tls.subject and tls.issuerdn\nFeature #1903: tls: tls_cert_valid and tls_cert_expired keywords\nFeature #1907: http_request_line and http_response_line\nOptimization #1044: TLS buffers evaluated by fast_pattern matcher.\nOptimization #1277: Trigger second live rule-reload while first one is in progress\nBug #312: incorrect parsing of rules with missing semi-colon for keywords\nBug #712: wildcard matches on tls.subject\nBug #1353: unix-command socket created with last character missing\nBug #1486: invalid rule: parser err msg not descriptive enough\nBug #1525: Use pkg-config for libnetfilter_queue\nBug #1893: tls: src_ip and dest_ip reversed in TLS events for IPS vs IDS mode.\nBug #1898: Inspection does not always stop when stream depth is reached\n\n3.1.2 -- 2016-09-06\n\nFeature #1830: support 'tag' in eve log\nFeature #1870: make logged flow_id more unique\nFeature #1874: support Cisco Fabric Path / DCE\nFeature #1885: eve: add option to log all dropped packets\nBug #1849: ICMPv6 incorrect checksum alert if Ethernet FCS is present\nBug #1853: suricata is matching everything on dce_stub_data buffer\nBug #1854: unified2: logging of tagged packets not working\nBug #1856: PCAP mode device not found\nBug #1858: Lots of TCP 'duplicated option/DNS malformed request data' after upgrading from 3.0.1 to 3.1.1\nBug #1878: dns: crash while logging sshfp records\nBug #1880: icmpv4 error packets can lead to missed detection in tcp/udp\nBug #1884: libhtp 0.5.22\n\n3.1.1 -- 2016-07-13\n\nFeature #1775: Lua: SMTP-support\nBug #1419: DNS transaction handling issues\nBug #1515: Problem with Threshold.config when using more than one IP\nBug #1664: Unreplied DNS queries not logged when flow is aged out\nBug #1808: Can't set thread priority after dropping privileges.\nBug #1821: Suricata 3.1 fails to start on CentOS6\nBug #1839: suricata 3.1 configure.ac says >=libhtp-0.5.5, but >=libhtp-0.5.20 required\nBug #1840: --list-keywords and --list-app-layer-protos not working\nBug #1841: libhtp 0.5.21\nBug #1844: netmap: IPS mode doesn't set 2nd iface in promisc mode\nBug #1845: Crash on disabling a app-layer protocol when it's logger is still enabled\nOptimization #1846: af-packet: improve thread calculation logic\nOptimization #1847: rules: don't warn on empty files\n\n3.1 -- 2016-06-20\n\nBug #1589: Cannot run nfq in workers mode\nBug #1804: yaml: legacy detect-engine parsing custom values broken\n\n3.1RC1 -- 2016-06-07\n\nFeature #681: Implement TPACKET_V3 support in AF_PACKET\nFeature #1134: tls: server name rule keyword\nFeature #1343: OOBE -1- increasing the default stream.memcap and stream.reassembly.memcap values\nFeature #1344: OOBE -2- decreasing the default flow-timeouts (at least for TCP)\nFeature #1563: dns: log sshfp records\nFeature #1760: Unit tests: Don't register return value, use 1 for success, 0 for failure.\nFeature #1761: Unit tests: Provide macros for clean test failures.\nFeature #1762: default to AF_PACKET for -i if available\nFeature #1785: hyperscan spm integration\nFeature #1789: hyperscan mpm: enable by default\nFeature #1797: netmap: implement 'threads: auto'\nFeature #1798: netmap: warn about NIC offloading on FreeBSD\nFeature #1800: update bundled libhtp to 0.5.20\nFeature #1801: reduce info level verbosity\nFeature #1802: yaml: improve default layout\nFeature #1803: reimplement rule grouping\nBug #1078: 'Not\" operator (!) in Variable causes extremely slow loading of Suricata\nBug #1202: detect-engine profile medium consumes more memory than detect-engine profile high\nBug #1289: MPM b2gm matcher has questionable code\nBug #1487: Configuration parser depends on key ordering\nBug #1524: Potential Thread Name issues due to RHEL7 Interface Naming Contentions\nBug #1584: Rule keywords conflict will cause Suricata restart itself in loop\nBug #1606: [ERRCODE: SC_ERR_SYSCALL(50)] - Failure when trying to get MTU via ioctl: 6\nBug #1665: Default maximum packet size is insufficient when VLAN tags are present (and not stripped)\nBug #1714: Kernel panic on application exit with netmap Suricata 3.0 stable\nBug #1746: deadlock with autofp and --disable-detection\nBug #1764: app-layer-modbus: AddressSanitizer error (segmentation fault)\nBug #1768: packet processing threads doubled\nBug #1771: tls store memory leak\nBug #1773: smtp: not all attachments inspected in all cases\nBug #1786: spm crash on rule reload\nBug #1792: dns-json-log produces no output\nBug #1795: Remove unused CPU affinity settings from suricata.yaml\nOptimization #563: pmq optimization -- remove patter_id_array\nOptimization #1037: Optimize TCP Option storage\nOptimization #1418: lockless flow handling during capture (autofp)\nOptimization #1784: reduce storage size of IPv4 options and IPv6 ext hdrs\n\n3.0.1 -- 2016-04-04\n\nFeature #1704: hyperscan mpm integration\nFeature #1661: Improved support for xbits/hostbits (in particular ip_pair) when running with multiple threads\nBug #1697: byte_extract incompatibility with Snort.\nBug #1737: Stats not reset between PCAPs when Suricata runs in socket mode\n\n3.0.1RC1 -- 2016-03-23\n\nFeature #1535: Expose the certificate itself in TLS-lua\nFeature #1696: improve logged flow_id\nFeature #1700: enable \"relro\" and \"now\" in compile options for 3.0\nFeature #1734: gre: support transparent ethernet bridge decoding\nFeature #1740: Create counters for decode-events errors\nBug #873: suricata.yaml: .mgc is NOT actually added to value for magic file\nBug #1166: tls: CID 1197759: Resource leak (RESOURCE_LEAK)\nBug #1268: suricata and macos/darwin: [ERRCODE: SC_ERR_MAGIC_LOAD(197)] - magic_load failed: File 5.19 supports only version 12 magic files. `/usr/share/file/magic.mgc' is version 7\nBug #1359: memory leak\nBug #1411: Suricata generates huge load when nfq_create_queue failed\nBug #1570: stream.inline defaults to IDS mode if missing\nBug #1591: afpacket: unsupported datalink type 65534 on tun device\nBug #1619: Per-Thread Delta Stats Broken\nBug #1638: rule parsing issues: rev\nBug #1641: Suricata won't build with --disable-unix-socket when libjansson is enabled\nBug #1646: smtp: fix inspected tracker values\nBug #1660: segv when using --set on a list\nBug #1669: Suricate 3.0RC3 segfault after 10 hours\nBug #1670: Modbus compiler warnings on Fedora 23\nBug #1671: Cygwin Windows compilation with libjansson from source\nBug #1674: Cannot use 'tag:session' after base64_data keyword\nBug #1676: gentoo build error\nBug #1679: sensor-name configuration parameter specified in wrong place in default suricata.yaml\nBug #1680: Output sensor name in json\nBug #1684: eve: stream payload has wrong direction in IPS mode\nBug #1686: Conflicting \"no\" for \"totals\" and \"threads\" in stats output\nBug #1689: Stack overflow in case of variables misconfiguration\nBug #1693: Crash on Debian with libpcre 8.35\nBug #1695: Unix Socket missing dump-counters mode\nBug #1698: Segmentation Fault at detect-engine-content-inspection.c:438 (master)\nBug #1699: CUDA build broken\nBug #1701: memory leaks\nBug #1702: TLS SNI parsing issue\nBug #1703: extreme slow down in HTTP multipart parsing\nBug #1706: smtp memory leaks\nBug #1707: malformed json if message is too big\nBug #1708: dcerpc memory leak\nBug #1709: http memory leak\nBug #1715: nfq: broken time stamps with recent Linux kernel 4.4\nBug #1717: Memory leak on Suricata 3.0 with Netmap\nBug #1719: fileinfo output wrong in eve in http\nBug #1720: flowbit memleak\nBug #1724: alert-debuglog: non-decoder events won't trigger rotation.\nBug #1725: smtp logging memleak\nBug #1727: unix socket runmode per pcap memory leak\nBug #1728: unix manager command channel memory leaks\nBug #1729: PCRE jit is disabled/blacklisted when it should not\nBug #1731: detect-tls memory leak\nBug #1735: cppcheck: Shifting a negative value is undefined behaviour\nBug #1736: tls-sni: memory leaks on malformed traffic\nBug #1742: vlan use-for-tracking including Priority in hashing\nBug #1743: compilation with musl c library fails\nBug #1744: tls: out of bounds memory read on malformed traffic\nOptimization #1642: Add --disable-python option\n\n3.0 -- 2016-01-27\n\nBug #1673: smtp: crash during mime parsing\n\n3.0RC3 -- 2015-12-21\n\nBug #1632: Fail to download large file with browser\nBug #1634: Fix non thread safeness of Prelude analyzer\nBug #1640: drop log crashes\nBug #1645: Race condition in unix manager\nBug #1647: FlowGetKey flow-hash.c:240 segmentation fault (master)\nBug #1650: DER parsing issue (master)\n\n3.0RC2 -- 2015-12-08\n\nBug #1551: --enable-profiling-locks broken\nBug #1602: eve-log prefix field feature broken\nBug #1614: app_proto key missing from EVE file events\nBug #1615: disable modbus by default\nBug #1616: TCP reassembly bug\nBug #1617: DNS over TCP parsing issue\nBug #1618: SMTP parsing issue\nFeature #1635: unified2 output: disable by default\n\n3.0RC1 -- 2015-11-25\n\nBug #1150: TLS store disabled by TLS EVE logging\nBug #1210: global counters in stats.log\nBug #1423: Unix domain log file writer should automatically reconnect if receiving program is restarted.\nBug #1466: Rule reload - Rules won't reload if rule files are listed in an included file.\nBug #1467: Specifying an IPv6 entry before an IPv4 entry in host-os-policy causes ASAN heap-buffer-overflow.\nBug #1472: Should 'goodsigs' be 'goodtotal' when checking if signatures were loaded in detect.c?\nBug #1475: app-layer-modbus: AddressSanitizer error (heap-buffer-overflow)\nBug #1481: Leading whitespace in flowbits variable names\nBug #1482: suricata 2.1 beta4: StoreStateTxFileOnly crashes\nBug #1485: hostbits - leading and trailing spaces are treated as part of the name and direction.\nBug #1488: stream_size <= and >= modifiers function as < and > (equality is not functional)\nBug #1491: pf_ring is not able to capture packets when running under non-root account\nBug #1493: config test (-T) doesn't fail on missing files\nBug #1494: off by one on rulefile count\nBug #1500: suricata.log\nBug #1508: address var parsing issue\nBug #1517: Order dependent, ambiguous YAML in multi-detect.\nBug #1518: multitenancy - selector vlan - vlan id range\nBug #1521: multitenancy - global vlan tracking relation to selector\nBug #1523: Decoded base64 payload short by 16 characters\nBug #1530: multitenant mapping relation\nBug #1531: multitenancy - confusing tenant id and vlan id output\nBug #1556: MTU setting on NIC interface not considered by af-packet\nBug #1557: stream: retransmission not detected\nBug #1565: defrag: evasion issue\nBug #1597: dns parser issue (master)\nBug #1601: tls: server name logging\nFeature #1116: ips packet stats in stats.log\nFeature #1137: Support IP lists in threshold.config\nFeature #1228: Suricata stats.log in JSON format\nFeature #1265: Replace response on Suricata dns decoder when dns error please\nFeature #1281: long snort ruleset support for \"SC_ERR_NOT_SUPPORTED(225): content length greater than 255 unsupported\"\nFeature #1282: support for base64_decode from snort's ruleset\nFeature #1342: Support Cisco erspan traffic\nFeature #1374: Write pre-aggregated counters for all threads\nFeature #1408: multi tenancy for detection\nFeature #1440: Load rules file from a folder or with a star pattern rather then adding them manually to suricata.yaml\nFeature #1454: Proposal to add Lumberjack/CEE formatting option to EVE JSON syslog output for compatibility with rsyslog parsing\nFeature #1492: Add HUP coverage to output json-log\nFeature #1498: color output\nFeature #1499: json output for engine messages\nFeature #1502: Expose tls fields to lua\nFeature #1514: SSH softwareversion regex should allow colon\nFeature #1527: Add ability to compile as a Position-Independent Executable (PIE)\nFeature #1568: TLS lua output support\nFeature #1569: SSH lua support\nFeature #1582: Redis output support\nFeature #1586: Add flow memcap counter\nFeature #1599: rule profiling: json output\nOptimization #1269: Convert SM List from linked list to array\n\n2.1beta4 -- 2015-05-08\n\nBug #1314: http-events performance issues\nBug #1340: null ptr dereference in Suricata v2.1beta2 (output-json.c:347)\nBug #1352: file list is not cleaned up\nBug #1358: Gradual memory leak using reload (kill -USR2 $pid)\nBug #1366: Crash if default_packet_size is below 32 bytes\nBug #1378: stats api doesn't call thread deinit funcs\nBug #1384: tcp midstream window issue (master)\nBug #1388: pcap-file hangs on systems w/o atomics support (master)\nBug #1392: http uri parsing issue (master)\nBug #1393: CentOS 5.11 build failures\nBug #1398: DCERPC traffic parsing issue (master)\nBug #1401: inverted matching on incomplete session\nBug #1402: When re-opening files on HUP (rotation) always use the append flag.\nBug #1417: no rules loaded - latest git - rev e250040\nBug #1425: dead lock in de_state vs flowints/flowvars\nBug #1426: Files prematurely truncated by detection engine even though force-md5 is enabled\nBug #1429: stream: last_ack update issue leading to stream gaps\nBug #1435: EVE-Log alert payload option loses data\nBug #1441: Local timestamps in json events\nBug #1446: Unit ID check in Modbus packet error\nBug #1449: smtp parsing issue\nBug #1451: Fix list-keywords regressions\nBug #1463: modbus parsing issue\nFeature #336: Add support for NETMAP to Suricata.\nFeature #885: smtp file_data support\nFeature #1394: Improve TCP reuse support\nFeature #1410: add alerts to EVE's drop logs\nFeature #1445: Suricata does not work on pfSense/FreeBSD interfaces using PPPoE\nFeature #1447: Ability to reject ICMP traffic\nFeature #1448: xbits\nOptimization #1014: app layer reassembly fast-path\nOptimization #1377: flow manager: reduce (try)locking\nOptimization #1403: autofp packet pool performance problems\nOptimization #1409: http pipeline support for stateful detection\n\n2.1beta3 -- 2015-01-29\n\nBug #977: WARNING on empty rules file is fatal (should not be)\nBug #1184: pfring: cppcheck warnings\nBug #1321: Flow memuse bookkeeping error\nBug #1327: pcre pkt/flowvar capture broken for non-relative matches (master)\nBug #1332: cppcheck: ioctl\nBug #1336: modbus: CID 1257762: Logically dead code (DEADCODE)\nBug #1351: output-json: duplicate logging (2.1.x)\nBug #1354: coredumps on quitting on OpenBSD\nBug #1355: Bus error when reading pcap-file on OpenBSD\nBug #1363: Suricata does not compile on OS X/Clang due to redefinition of string functions (2.1.x)\nBug #1365: evasion issues (2.1.x)\nFeature #1261: Request for Additional Lua Capabilities\nFeature #1309: Lua support for Stats output\nFeature #1310: Modbus parsing and matching\nFeature #1317: Lua: Indicator for end of flow\nFeature #1333: unix-socket: allow (easier) non-root usage\nOptimization #1339: flow timeout optimization\nOptimization #1339: flow timeout optimization\nOptimization #1371: mpm optimization\n\n2.1beta2 -- 2014-11-06\n\nFeature #549: Extract file attachments from emails\nFeature #1312: Lua output support\nFeature #899: MPLS over Ethernet support\nFeature #707: ip reputation files - network range inclusion availability (cidr)\nFeature #383: Stream logging\nFeature #1263: Lua: Access to Stream Payloads\nFeature #1264: Lua: access to TCP quad / Flow Tuple\nBug #1048: PF_RING/DNA config - suricata.yaml\nBug #1230: byte_extract, within combination not working\nBug #1257: Flow switch is missing from the eve-log section in suricata.yaml\nBug #1259: AF_PACKET IPS is broken in 2.1beta1\nBug #1260: flow logging at shutdown broken\nBug #1279: BUG: NULL pointer dereference when suricata was debug mode.\nBug #1280: BUG: IPv6 address vars issue\nBug #1285: Lua - http.request_line not working (2.1)\nBug #1287: Lua Output has dependency on eve-log:http\nBug #1288: Filestore keyword in wrong place will cause entire rule not to trigger\nBug #1294: Configure doesn't use --with-libpcap-libraries when testing PF_RING library\nBug #1301: suricata yaml - PF_RING load balance per hash option\nBug #1308: http_header keyword not matching when SYN|ACK and ACK missing (master)\nBug #1311: EVE output Unix domain socket not working (2.1)\n\n2.1beta1 -- 2014-08-12\n\nFeature #1155: Log packet payloads in eve alerts\nFeature #1208: JSON Output Enhancement - Include Payload(s)\nFeature #1248: flow/connection logging\nFeature #1258: json: include HTTP info with Alert output\nOptimization #1039: Packetpool should be a stack\nOptimization #1241: pcap recording: record per thread\n\n2.0.3 -- 2014-08-08\n\nBug #1236: fix potential crash in http parsing\nBug #1244: ipv6 defrag issue\nBug #1238: Possible evasion in stream-tcp-reassemble.c\nBug #1221: lowercase conversion table missing last value\nSupport #1207: Cannot compile on CentOS 5 x64 with --enable-profiling\n\n2.0.2 -- 2014-06-25\n\nBug #1098: http_raw_uri with relative pcre parsing issue\nBug #1175: unix socket: valgrind warning\nBug #1189: abort() in 2.0dev (rev 6fbb955) with pf_ring 5.6.3\nBug #1195: nflog: cppcheck reports memleaks\nBug #1206: ZC pf_ring not working with Suricata 2.0.1 (or latest git)\nBug #1211: defrag issue\nBug #1212: core dump (after a while) when app-layer.protocols.http.enabled = yes\nBug #1214: Global Thresholds (sig_id 0, gid_id 0) not applied correctly if a signature has event vars\nBug #1217: Segfault in unix-manager.c line 529 when using --unix-socket and sending pcap files to be analyzed via socket\nFeature #781: IDS using NFLOG iptables target\nFeature #1158: Parser DNS TXT data parsing and logging\nFeature #1197: liblua support\nFeature #1200: sighup for log rotation\n\n2.0.1 -- 2014-05-21\n\nNo changes since 2.0.1rc1\n\n2.0.1rc1 -- 2014-05-12\n\nBug #978: clean up app layer parser thread local storage\nBug #1064: Lack of Thread Deinitialization For Decoder Modules\nBug #1101: Segmentation in AppLayerParserGetTxCnt\nBug #1136: negated app-layer-protocol FP on multi-TX flows\nBug #1141: dns response parsing issue\nBug #1142: dns tcp toclient protocol detection\nBug #1143: tls protocol detection in case of tls-alert\nBug #1144: icmpv6: unknown type events for MLD_* types\nBug #1145: ipv6: support PAD1 in DST/HOP extension hdr\nBug #1146: tls: event on 'new session ticket' in handshake\nBug #1159: Possible memory exhaustion when an invalid bpf-filter is used with AF_PACKET\nBug #1160: Pcaps submitted via Unix Socket do not finish processing in Suricata 2\nBug #1161: eve: src and dst mixed up in some cases\nBug #1162: proto-detect: make sure probing parsers for all registered ports are run\nBug #1163: HTP Segfault\nBug #1165: af_packet - one thread consistently not working\nBug #1170: rohash: CID 1197756: Bad bit shift operation (BAD_SHIFT)\nBug #1176: AF_PACKET IPS mode is broken in 2.0\nBug #1177: eve log do not show action 'dropped' just 'allowed'\nBug #1180: Possible problem in stream tracking\nFeature #1157: Always create pid file if --pidfile command line option is provided.\nFeature #1173: tls: OpenSSL heartbleed detection\n\n2.0 -- 2014-03-25\n\nBug #1151: tls.store not working when a TLS filter keyword is used\n\n2.0rc3 -- 2014-03-18\n\nBug #1127: logstash & suricata parsing issue\nBug #1128: Segmentation fault - live rule reload\nBug #1129: pfring cluster & ring initialization\nBug #1130: af-packet flow balancing problems\nBug #1131: eve-log: missing user agent reported inconsistently\nBug #1133: eve-log: http depends on regular http log\nBug #1135: 2.0rc2 release doesn't set optimization flag on GCC\nBug #1138: alert fastlog drop info missing\n\n2.0rc2 -- 2014-03-06\n\nBug #611: fp: rule with ports matching on portless proto\nBug #985: default config generates rule warnings and errors\nBug #1021: 1.4.6: conf_filename not checked before use\nBug #1089: SMTP: move depends on uninitialised value\nBug #1090: FTP: Memory Leak\nBug #1091: TLS-Handshake: Uninitialized value\nBug #1092: HTTP: Memory Leak\nBug #1108: suricata.yaml config parameter - segfault\nBug #1109: PF_RING vlan handling\nBug #1110: Can have the same Pattern ID (pid) for the same pattern but different case flags\nBug #1111: capture stats at exit incorrect\nBug #1112: tls-events.rules file missing\nBug #1115: nfq: exit stats not working\nBug #1120: segv with pfring/afpacket and eve-log enabled\nBug #1121: crash in eve-log\nBug #1124: ipfw build broken\nFeature #952: Add VLAN tag ID to all outputs\nFeature #953: Add QinQ tag ID to all outputs\nFeature #1012: Introduce SSH log\nFeature #1118: app-layer protocols http memcap - info in verbose mode (-v)\nFeature #1119: restore SSH protocol detection and parser\n\n2.0rc1 -- 2014-02-13\n\nBug #839: http events alert multiple times\nBug #954: VLAN decoder stats with AF Packet get written to the first thread only - stats.log\nBug #980: memory leak in http buffers at shutdown\nBug #1066: logger API's for packet based logging and tx based logging\nBug #1068: format string issues with size_t + qa not catching them\nBug #1072: Segmentation fault in 2.0beta2: Custom HTTP log segmentation fault\nBug #1073: radix tree lookups are not thread safe\nBug #1075: CUDA 5.5 doesn't compile with 2.0 beta 2\nBug #1079: Err loading rules with variables that contain negated content.\nBug #1080: segfault - 2.0dev (rev 6e389a1)\nBug #1081: 100% CPU utilization with suricata 2.0 beta2+\nBug #1082: af-packet vlan handling is broken\nBug #1103: stats.log not incrementing decoder.ipv4/6 stats when reading in QinQ packets\nBug #1104: vlan tagged fragmentation\nBug #1106: Git compile fails on Ubuntu Lucid\nBug #1107: flow timeout causes decoders to run on pseudo packets\nFeature #424: App layer registration cleanup - Support specifying same alproto names in rules for different ip protocols\nFeature #542: TLS JSON output\nFeature #597: case insensitive fileext match\nFeature #772: JSON output for alerts\nFeature #814: QinQ tag flow support\nFeature #894: clean up output\nFeature #921: Override conf parameters\nFeature #1007: united output\nFeature #1040: Suricata should compile with -Werror\nFeature #1067: memcap for http inside suricata\nFeature #1086: dns memcap\nFeature #1093: stream: configurable segment pools\nFeature #1102: Add a decoder.QinQ stats in stats.log\nFeature #1105: Detect icmpv6 on ipv4\n\n2.0beta2 -- 2013-12-18\n\nBug #463: Suricata not fire on http reply detect if request are not http\nBug #640: app-layer-event:http.host_header_ambiguous set when it shouldn't\nBug #714: some logs not created in daemon mode\nBug #810: Alerts on http traffic storing the wrong packet as the IDS event payload\nBug #815: address parsing with negation\nBug #820: several issues found by clang 3.2\nBug #837: Af-packet statistics inconsistent under very high traffic\nBug #882: MpmACCudaRegister shouldn't call PatternMatchDefaultMatcher\nBug #887: http.log printing unknown hostname most of the time\nBug #890: af-packet segv\nBug #892: detect-engine.profile - custom - does not err out in incorrect toclient/srv values - suricata.yaml\nBug #895: response: rst packet bug\nBug #896: pfring dna mode issue\nBug #897: make install-full fails if wget is missing\nBug #903: libhtp valgrind warning\nBug #907: icmp_seq and icmp_id keyword with icmpv6 traffic (master)\nBug #910: make check fails w/o sudo/root privs\nBug #911: HUP signal\nBug #912: 1.4.3: Unit test in util-debug.c: line too long.\nBug #914: Having a high number of pickup queues (216+) makes suricata crash\nBug #915: 1.4.3: log-pcap.c: crash on printing a null filename\nBug #917: 1.4.5: decode-ipv6.c: void function cannot return value\nBug #920: Suricata failed to parse address\nBug #922: trackers value in suricata.yaml\nBug #925: prealloc-sessions value bigger than allowed in suricata.yaml\nBug #926: prealloc host value in suricata.yaml\nBug #927: detect-thread-ratio given a non numeric value in suricata.yaml\nBug #928: Max number of threads\nBug #932: wrong IP version - on stacked layers\nBug #939: thread name buffers are sized inconsistently\nBug #943: pfring: see if we can report that the module is not loaded\nBug #948: apple ppc64 build broken: thread-local storage not supported for this target\nBug #958: SSL parsing issue (master)\nBug #963: XFF compile failure on OSX\nBug #964: Modify negated content handling\nBug #967: threshold rule clobbers suppress rules\nBug #968: unified2 not logging tagged packets\nBug #970: AC memory read error\nBug #973: Use different ids for content patterns which are the same, but one of them has a fast_pattern chop set on it.\nBug #976: ip_rep supplying different no of alerts for 2 different but semantically similar rules\nBug #979: clean up app layer protocol detection memory\nBug #982: http events missing\nBug #987: default config generates error(s)\nBug #988: suricata don't exit in live mode\nBug #989: Segfault in HTPStateGetTxCnt after a few minutes\nBug #991: threshold mem leak\nBug #994: valgrind warnings in unittests\nBug #995: tag keyword: tagging sessions per time is broken\nBug #998: rule reload triggers app-layer-event FP's\nBug #999: delayed detect inits thresholds before de_ctx\nBug #1003: Segmentation fault\nBug #1023: block rule reloads during delayed detect init\nBug #1026: pfring: update configure to link with -lrt\nBug #1031: Fix IPv6 stream pseudo packets\nBug #1035: http uri/query normalization normalizes 'plus' sign to space\nBug #1042: Can't match \"emailAddress\" field in tls.subject and tls.issuerdn\nBug #1061: Multiple flowbit set in one rule\nFeature #234: add option disable/enable individual app layer protocol inspection modules\nFeature #417: ip fragmentation time out feature in yaml\nFeature #478: XFF (X-Forwarded-For)\nFeature #602: availability for http.log output - identical to apache log format\nFeature #622: Specify number of pf_ring/af_packet receive threads on the command line\nFeature #727: Explore the support for negated alprotos in sigs.\nFeature #746: Decoding API modification\nFeature #751: Add invalid packet counter\nFeature #752: Improve checksum detection algorithm\nFeature #789: Clean-up start and stop code\nFeature #813: VLAN flow support\nFeature #878: add storage api\nFeature #901: VLAN defrag support\nFeature #904: store tx id when generating an alert\nFeature #940: randomize http body chunks sizes\nFeature #944: detect nic offloading\nFeature #956: Implement IPv6 reject\nFeature #957: reject: iface setup\nFeature #959: Move post config initialisation code to PostConfLoadedSetup\nFeature #981: Update all switch case fall-throughs with comments on fall-throughs\nFeature #983: Provide rule support for specifying icmpv4 and icmpv6.\nFeature #986: set htp request and response size limits\nFeature #1008: Optionally have http_uri buffer start with uri path for use in proxied environments\nFeature #1009: Yaml file inclusion support\nFeature #1032: profiling: per keyword stats\nOptimization #583: improve Packet_ structure layout\nOptimization #1018: clean up counters api\nOptimization #1041: remove mkinstalldirs from git\n\n2.0beta1 -- 2013-07-18\n\n- Luajit flow vars and flow ints support (#593)\n- DNS parser, logger and keyword support (#792), funded by Emerging Threats\n- deflate support for HTTP response bodies (#470, #775)\n- update to libhtp 0.5 (#775)\n- improved gzip support for HTTP response bodies (#470, #775)\n- redesigned transaction handling, improving both accuracy and performance (#753)\n- redesigned CUDA support (#729)\n- Be sure to always apply verdict to NFQ packet (#769)\n- stream engine: SACK allocs should adhere to memcap (#794)\n- stream: deal with multiple different SYN/ACK's better (#796)\n- stream: Randomize stream chunk size for raw stream inspection (#804)\n- Introduce per stream thread ssn pool (#519)\n- \"pass\" IP-only rules should bypass detection engine after matching (#718)\n- Generate error if bpf is used in IPS mode (#777)\n- Add support for batch verdicts in NFQ, thanks to Florian Westphal\n- Update Doxygen config, thanks to Phil Schroeder\n- Improve libnss detection, thanks to Christian Kreibich\n- Fix a FP on rules looking for port 0 and fragments (#847), thanks to Rmkml\n- OS X unix socket build fixed (#830)\n- bytetest, bytejump and byteextract negative offset failure (#827)\n- Fix fast.log formatting issues (#771), thanks to Rmkml\n- Invalidate negative depth (#774), thanks to Rmkml\n- Fixed accuracy issues with relative pcre matching (#791)\n- Fix deadlock in flowvar capture code (#802)\n- Improved accuracy of file_data keyword (#817)\n- Fix af-packet ips mode rule processing bug (#819), thanks to Laszlo Madarassy\n- stream: fix injecting pseudo packet too soon leading to FP (#883), thanks to Francis Trudeau\n\n1.4.4 -- 2013-07-18\n\n- Bug #834: Unix socket - showing as compiled when it is not desired to do so\n- Bug #835: Unix Socket not working as expected\n- Bug #841: configure --enable-unix-socket does not err out if libs/pkgs are not present\n- Bug #846: FP on IP frag and sig use udp port 0, thanks to Rmkml\n- Bug #864: backport packet action macro's\n- Bug #876: htp tunnel fix\n- Bug #877: Flowbit check with content doesn't match consistently, thanks to Francis Trudeau\n\n1.4.3 -- 2013-06-20\n\n- Fix missed detection in bytetest, bytejump and byteextract for negative offset (#828)\n- Fix IPS mode being unable to drop tunneled packets (#826)\n- Fix OS X Unix Socket build (#829)\n\n1.4.2 -- 2013-05-29\n\n- No longer force nocase to be used on http_host\n- Invalidate rule if uppercase content is used for http_host w/o nocase\n- Warn user if bpf is used in af-packet IPS mode\n- Better test for available libjansson version\n- Fixed accuracy issues with relative pcre matching (#784)\n- Improved accuracy of file_data keyword (#788)\n- Invalidate negative depth (#770)\n- Fix http host parsing for IPv6 addresses (#761)\n- Fix fast.log formatting issues (#773)\n- Fixed deadlock in flowvar set code for http buffers (#801)\n- Various signature ordering improvements\n- Minor stream engine fix\n\n1.4.1 -- 2013-03-08\n\n- GeoIP keyword, allowing matching on Maxmind's database, contributed by Ignacio Sanchez (#559)\n- Introduce http_host and http_raw_host keywords (#733, #743)\n- Add python module for interacting with unix socket (#767)\n- Add new unix socket commands: fetching config, counters, basic runtime info (#764, #765)\n- Big Napatech support update by Matt Keeler\n- Configurable sensor id in unified2 output, contributed by Jake Gionet (#667)\n- FreeBSD IPFW fixes by Nikolay Denev\n- Add \"default\" interface setting to capture configuration in yaml (#679)\n- Make sure \"snaplen\" can be set by the user (#680)\n- Improve HTTP URI query string normalization (#739)\n- Improved error reporting in MD5 loading (#693)\n- Improve reference.config parser error reporting (#737)\n- Improve build info output to include all configure options (#738)\n- Segfault in TLS parsing reported by Charles Smutz (#725)\n- Fix crash in teredo decoding, reported by Rmkml (#736)\n- fixed UDPv4 packets without checksum being detected as invalid (#760)\n- fixed DCE/SMB parsers getting confused in some fragmented cases (#764)\n- parsing ipv6 address/subnet parsing in thresholding was fixed by Jamie Strandboge (#697)\n- FN: IP-only rule ip_proto not matching for some protocols (#689)\n- Fix build failure with other libhtp installs (#688)\n- Fix malformed yaml loading leading to a crash (#694)\n- Various Mac OS X fixes (#700, #701, #703)\n- Fix for autotools on Mac OS X by Jason Ish (#704)\n- Fix AF_PACKET under high load not updating stats (#706)\n\n1.3.6 -- 2013-03-07\n\n- fix decoder event rules not checked in all cases (#671)\n- checksum detection for icmpv6 was fixed (#673)\n- crash in HTTP server body inspection code fixed (#675)\n- fixed a icmpv6 payload bug (#676)\n- IP-only rule ip_proto not matching for some protocols was addressed (#690)\n- fixed malformed yaml crashing suricata (#702)\n- parsing ipv6 address/subnet parsing in thresholding was fixed by Jamie Strandboge (#717)\n- crash in tls parser was fixed (#759)\n- fixed UDPv4 packets without checksum being detected as invalid (#762)\n- fixed DCE/SMB parsers getting confused in some fragmented cases (#763)\n\n1.4 2012-12-13\n\n- Decoder event matching fixed (#672)\n- Unified2 would overwrite files if file rotation happened within a second of file creation, leading to loss of events/alerts (#665)\n- Add more events to IPv6 extension header anomalies (#678)\n- Fix ICMPv6 payload and checksum calculation (#677, #674)\n- Clean up flow timeout handling (#656)\n- Fix a shutdown bug when using AF_PACKET under high load (#653)\n- Fix TCP sessions being cleaned up to early (#652)\n\n1.3.5 2012-12-06\n\n- Flow engine memory leak fixed by Ludovico Cavedon (#651)\n- Unified2 would overwrite files if file rotation happened within a second of file creation, leading to loss of events/alerts (#664)\n- Flow manager mutex used uninitialized, fixed by Ludovico Cavedon (#654)\n- Windows building in CYGWIN fixed (#630)\n\n1.4rc1 2012-11-29\n\n- Interactive unix socket mode (#571, #552)\n- IP Reputation: loading and matching (#647)\n- Improved --list-keywords command-line option gives detailed info for supported keyword, including doc link (#435)\n- Rule analyzer improvement wrt ipv4/ipv6, invalid rules (#494)\n- User-Agent added to file log and filestore meta files (#629)\n- Endace DAG supports live stats and at exit drop stats (#638)\n- Add support for libhtp event \"request port doesn't match tcp port\" (#650)\n- Rules with negated addresses will not be considered IP-only (#599)\n- Rule reloads complete much faster in low traffic conditions (#526)\n- Suricata -h now displays all available options (#419)\n- Luajit configure time detection was improved (#636)\n- Flow manager mutex used w/o initialization (#628)\n- Cygwin workaround for windows shell mangling interface string (#372)\n- Fix a Prelude output crash with alerts generated by rules w/o classtype or msg (#648)\n- CLANG compiler build fixes (#649)\n- Several fixes found by code analyzers\n\n1.4beta3 2012-11-14\n\n- support for Napatech cards was greatly improved by Matt Keeler from Npulse (#430, #619)\n- support for pkt_data keyword was added\n- user and group to run as can now be set in the config file\n- make HTTP request and response body inspection sizes configurable per HTTP server config (#560)\n- PCAP/AF_PACKET/PF_RING packet stats are now printed in stats.log (#561, #625)\n- add contrib directory to the dist (#567)\n- performance improvements to signatures with dsize option\n- improved rule analyzer: print fast_pattern along with the rule (#558)\n- fixes to stream engine reducing the number of events generated (#604)\n- add stream event to match on overlaps with different data in stream reassembly (#603)\n- stream.inline option new defaults to \"auto\", meaning enabled in IPS mode, disabled in IDS mode (#592)\n- HTTP handling in OOM condition was greatly improved (#557)\n- filemagic keyword performance was improved (#585)\n- fixes and improvements to daemon mode (#624)\n- fix drop rules not working correctly when thresholded (#613)\n- fixed a possible FP when a regular and \"chopped\" fast_pattern were the same (#581)\n- fix a false positive condition in http_header (#607)\n- fix inaccuracy in byte_jump keyword when using \"from_beginning\" option (#627)\n- fixes to rule profiling (#576)\n- cleanups and misc fixes (#379, #395)\n- updated bundled libhtp to 0.2.11\n- build system improvements and cleanups\n- fix to SSL record parsing\n\n1.3.4 -- 2012-11-14\n\n- fix crash in flow and host engines in cases of low memory or low memcap settings (#617)\n- improve http handling in low memory conditions (#620)\n- fix inaccuracy in byte_jump keyword when using \"from_beginning\" option (#626)\n- fix building on OpenBSD 5.2\n- update default config's defrag settings to reflect all available options\n- fixes to make check\n- fix to SSL record parsing\n\n1.3.3 -- 2012-11-01\n\n- fix drop rules not working correctly when thresholded (#615)\n- fix a false positive condition in http_header (#606)\n- fix extracted file corruption (#601)\n- fix a false positive condition with the pcre keyword and relative matching (#588)\n- fix PF_RING set cluster problem on dma interfaces (#598)\n- improve http handling in low memory conditions (#586, #587)\n- fix FreeBSD inline mode crash (#612)\n- suppress pcre jit warning (#579)\n\n1.4beta2 -- 2012-10-04\n\n- New keyword: \"luajit\" to inspect packet, payload and all HTTP buffers with a Lua script (#346)\n- Added ability to control per server HTTP parser settings in much more detail (#503)\n- Rewrite of IP Defrag engine to improve performance and fix locking logic (#512, #540)\n- Big performance improvement in inspecting decoder, stream and app layer events (#555)\n- Pool performance improvements (#541)\n- Improved performance of signatures with simple pattern setups (#577)\n- Bundled docs are installed upon make install (#527)\n- Support for a number of global vs rule thresholds [3] was added (#425)\n- Improved rule profiling performance\n- If not explicit fast_pattern is set, pick HTTP patterns over stream patterns. HTTP method, stat code and stat msg are excluded.\n- Fix compilation on architectures other than x86 and x86_64 (#572)\n- Fix FP with anchored pcre combined with relative matching (#529)\n- Fix engine hanging instead of exiting if the pcap device doesn't exist (#533)\n- Work around for potential FP, will get properly fixed in next release (#574)\n- Improve ERF handling. Thanks to Jason Ish\n- Always set cluster_id in PF_RING\n- IPFW: fix broken broadcast handling\n- AF_PACKET kernel offset issue, IPS fix and cleanup\n- Fix stream engine sometimes resending the same data to app layer\n- Fix multiple issues in HTTP multipart parsing\n- Fixed a lockup at shutdown with NFQ (#537)\n\n1.3.2 -- 2012-10-03\n\n- Fixed a possible FP when a regular and \"chopped\" fast_pattern were the same (#562)\n- Fixed a FN condition with the flow:no_stream option (#575)\n- Fix building of perf profiling code on i386 platform. By Simon Moon (#534)\n- Fix multiple issues in HTTP multipart parsing\n- Fix stream engine sometimes resending the same data to app layer\n- Always set cluster_id in PF_RING\n- Defrag: silence some potentially noisy errors/warnings\n- IPFW: fix broken broadcast handling\n- AF_PACKET kernel offset issue\n\n1.4beta1 -- 2012-09-06\n\n- Custom HTTP logging contributed by Ignacio Sanchez (#530)\n- TLS certificate logging and fingerprint computation and keyword (#443)\n- TLS certificate store to disk feature (#444)\n- Decoding of IPv4-in-IPv6, IPv6-in-IPv6 and Teredo tunnels (#462, #514, #480)\n- AF_PACKET IPS support (#516)\n- Rules can be set to inspect only IPv4 or IPv6 (#494)\n- filesize keyword for matching on sizes of files in HTTP (#489)\n- Delayed detect initialization. Starts processing packets right away and loads detection engine in the background (#522)\n- NFQ fail open support (#507)\n- Highly experimental lua scripting support for detection\n- Live reloads now supports HTTP rule updates better (#522)\n- AF_PACKET performance improvements (#197, #415)\n- Make defrag more configurable (#517, #528)\n- Improve pool performance (#518)\n- Improve file inspection keywords by adding a separate API (#531)\n- Example threshold.config file provided (#302)\n- Fix building of perf profiling code on i386 platform. By Simon Moon (#534)\n- Various spelling corrections by Simon Moon (#533)\n\n1.3.1 -- 2012-08-21\n\n- AF_PACKET performance improvements\n- Defrag engine performance improvements\n- HTTP: add per server options to enable/disable double decoding of URI (#464, #504)\n- Stream engine packet handling for packets with non-standard flag combinations (#508)\n- Improved stream engine handling of packet loss (#523)\n- Stream engine checksum alerting fixed\n- Various rule analyzer fixes (#495, #496, #497)\n- (Rule) profiling fixed and improved (#460, #466)\n- Enforce limit on max-pending-packets (#510)\n- fast_pattern on negated content improved\n- TLS rule keyword parsing issues\n- Windows build fixes (#502)\n- Host OS parsing issues fixed (#499)\n- Reject signatures where content length is bigger than \"depth\" setting (#505)\n- Removed unused \"prune-flows\" option\n- Set main thread and live reload thread names (#498)\n\n1.3 -- 2012-07-06\n\n- make live rule reloads optional and disabled by default\n- fix a shutdown bug\n- fix several memory leaks (#492)\n- warn user if global and rule thresholding conflict (#455)\n- set thread names on FreeBSD (Nikolay Denev)\n- Fix PF_RING building on Ubuntu 12.04\n- rule analyzer updates\n- file inspection improvements when dealing with limits (#493)\n\n1.3rc1 -- 2012-06-29\n\n- experimental live rule reload by sending a USR2 signal (#279)\n- AF_PACKET BPF support (#449)\n- AF_PACKET live packet loss counters (#441)\n- Rule analyzer (#349)\n- add pcap workers runmode for use with libpcap wrappers that support load balancing, such as  Napatech's or Myricom's\n- negated filemd5 matching, allowing for md5 whitelisting\n- signatures with depth and/or offset are now checked against packets in addition to the stream (#404)\n- http_cookie keyword now also inspects \"Set-Cookie\" header (#479)\n- filemd5 keyword no longer depends on log-file output module (#447)\n- http_raw_header keyword inspects original header line terminators (#475)\n- deal with double encoded URI (#464)\n- improved SMB/SMB2/DCERPC robustness\n- ICMPv6 parsing fixes\n- improve HTTP body inspection\n- stream.inline accuracy issues fixed (#339)\n- general stability fixes (#482, #486)\n- missing unittests added (#471)\n- \"threshold.conf not found\" error made more clear (#446)\n- IPS mode segment logging for Unified2 improved\n\n1.3beta2 -- 2012-06-08\n\n- experimental support for matching on large lists of known file MD5 checksums\n- Improved performance for file_data, http_server_body and http_client_body keywords\n- Improvements to HTTP handling: multipart parsing, gzip decompression\n- Byte_extract can support negative offsets now (#445)\n- Support for PF_RING 5.4 added. Many thanks to Chris Wakelin (#459)\n- HOME_NET and EXTERNAL_NET and the other vars are now checked for common errors (#454)\n- Improved error reporting when using too long address strings (#451)\n- MD5 calculation improvements for daemon mode and other cases (#449)\n- File inspection scripts: Added Syslog action for logging to local syslog. Thanks to Martin Holste.\n- Rule parser is made more strict.\n- Unified2 output overhaul, logging individual segments in more cases.\n- detection_filter keyword accuracy problem was fixed (#453)\n- Don't inspect cookie header with http header (#461)\n- Crash with a rule with two byte_extract keywords (#456)\n- SSL parser fixes. Thanks to Chris Wakelin for testing the patches! (#476)\n- Accuracy issues in HTTP inspection fixed. Thanks to Rmkml (#452)\n- Improve escaping of some characters in logs (#418)\n- Checksum calculation bugs fixed\n- IPv6 parsing issues fixed. Thanks to Michel Saborde.\n- Endace DAG issues fixed. Thanks to Jason Ish from Endace.\n- Various OpenBSD related fixes.\n- Fixes for bugs found by Coverity source code analyzer.\n\n1.3beta1 -- 2012-04-04\n\n- TLS/SSL handshake parser, tls.subjectdn and tls.issuerdn keywords (#296, contributed by Pierre Chifflier)\n- Napatech capture card support (contributed by Randy Caldejon -- nPulse)\n- Scripts for looking up files / file md5's at Virus Total and others (contributed by Martin Holste)\n- Test mode: -T option to test the config (#271)\n- Ringbuffer and zero copy support for AF_PACKET\n- CommandLine options to list supported app layer protocols and keywords (#344, #414)\n- File extraction for HTTP POST request that do not use multipart bodies\n- On the fly md5 checksum calculation of extracted files\n- Line based file log, in json format\n- Basic support for including other yaml files into the main yaml\n- New multi pattern engine: ac-bs\n- Profiling improvements, added lock profiling code\n- Improved HTTP CONNECT support in libhtp (#427, Brian Rectanus -- Qualys)\n- Unified yaml naming convention, including fallback support (by Nikolay Denev)\n- Improved Endace DAG support (#431, Jason Ish -- Endace)\n- New default runmode: \"autofp\" (#433)\n- Major rewrite of flow engine, improving scalability.\n- Improved http_stat_msg and http_stat_code keywords (#394)\n- Improved scalability for Tag and Threshold subsystems\n- Made the rule keyword parser much stricter in detecting syntax errors\n- Split \"file\" output into \"file-store\" and \"file-log\" outputs\n- Much improved file extraction\n- CUDA build fixes (#421)\n- Various FP's reported by Rmkml (#403, #405, #411)\n- IPv6 decoding and detection issues (reported by Michel Saborde)\n- PCAP logging crash (#422)\n- Fixed many (potential) issues with the help of the Coverity source code analyzer\n- Fixed several (potential) issues with the help of the cppcheck and clang/scan-build source code analyzers\n\n1.2.1 -- 2012-01-20\n\n- fix malformed unified2 records when writing alerts trigger by stream inspection (#402)\n- only force a pseudo packet inspection cycle for TCP streams in a state >= established\n\n1.2 -- 2012-01-19\n\n- improved Windows/CYGWIN path handling (#387)\n- fixed some issues with passing an interface or ip address with -i\n- make live worker runmode threads adhere to the 'detect' cpu affinity settings\n\n1.2rc1 -- 2012-01-11\n\n- app-layer-events keyword: similar to the decoder-events and stream-events, this will allow matching on HTTP and SMTP events\n- auto detection of checksum offloading per interface (#311)\n- urilen options to match on raw or normalized URI (#341)\n- flow keyword option \"only_stream\" and \"no_stream\"\n- unixsock output options for all outputs except unified2 (PoC python script in the qa/ dir) (#250)\n- in IPS mode, reject rules now also drop (#399)\n- http_header now also inspects response headers (#389)\n- \"worker\" runmodes for NFQ and IPFW\n- performance improvement for \"ac\" pattern matcher\n- allow empty/non-initialized flowints to be incremented\n- PCRE-JIT is now enabled by default if available (#356)\n- many file inspection and extraction improvements\n- flowbits and flowints are now modified in a post-match action list\n- general performance increments\n- fixed parsing really high sid numbers >2 Billion (#393)\n- fixed ICMPv6 not matching in IP-only sigs (#363)\n\n1.2beta1 -- 2011-12-19\n\n- File name, type inspection and extraction for HTTP\n- filename, fileext, filemagic and filestore keywords added\n- \"file\" output for storing extracted files to disk\n- file_data keyword support, inspecting normalized, dechunked, decompressed HTTP response body (feature #241\n- new keyword http_server_body, pcre regex /S option\n- Option to enable/disable core dumping from the suricata.yaml (enabled by default)\n- Human readable size limit settings in suricata.yaml\n- PF_RING bpf support (required PF_RING >= 5.1) (feature #334)\n- tos keyword support (feature #364)\n- IPFW IPS mode does now support multiple divert sockets\n- New IPS running modes, Linux and FreeBSD do now support \"worker\" and \"autofp\"\n- Improved alert accuracy in autofp and single runmodes\n- major performance optimizations for the ac-gfbs pattern matcher implementation\n- unified2 output fixes\n- PF_RING supports privilege dropping now (bug #367)\n- Improved detection of duplicate signatures\n\n1.1.1 -- 2011-12-07\n\n- Fix for a error in the smtp parser that could crash Suricata.\n- Fix for AF_PACKET not compiling on modern linux systems like Fedora 16.\n\n1.1 -- 2011-11-10\n\n- CUDA build fixed\n- minor pcap, AF_PACKET and PF_RING fixes (#368)\n- bpf handling fix\n- Windows CYGWIN build\n- more cleanups\n\n1.1rc1 -- 2011-11-03\n\n- extended HTTP request logging for use with (among other things) http_agent for Sguil (#38)\n- AF_PACKET report drop stats on shutdown (#325)\n- new counters in stats.log for flow and stream engines (#348)\n- SMTP parsing code support for BDAT command (#347)\n- HTTP URI normalization no longer converts to lowercase (#362)\n- AF_PACKET works with privileges dropping now (#361)\n- Prelude output for state matches (#264, #355)\n- update of the pattern matching code that should improve accuracy\n- rule parser was made more strict (#295, #312)\n- multiple event suppressions for the same SID was fixed (#366)\n- several accuracy fixes\n- removal of the unified1 output plugins (#353)\n\n1.1beta3 -- 2011-10-25\n\n- af-packet support for high speed packet capture\n- \"replace\" keyword support (#303)\n- new \"workers\" runmode for multi-dev and/or clustered PF_RING, AF_PACKET, pcap\n- added \"stream-event\" keyword to match on TCP session anomalies\n- support for suppress keyword was added (#274)\n- byte_extract keyword support was added\n- improved handling of timed out TCP sessions in the detection engine\n- unified2 payload logging if detection was in the HTTP state (#264)\n- improved accuracy of the HTTP transaction logging\n- support for larger (64 bit) Flow/Stream memcaps (#332)\n- major speed improvements for PCRE, including support for PCRE JIT\n- support setting flowbits in ip-only rules (#292)\n- performance increases on SSE3+ CPU's\n- overhaul of the packet acquisition subsystem\n- packet based performance profiling subsystem was added\n- TCP SACK support was added to the stream engine\n- updated included libhtp to 0.2.6 which fixes several issues\n\n1.1beta2 -- 2011-04-13\n\n- New keyword support: http_raw_uri (including /I for pcre), ssl_state, ssl_version (#258, #259, #260, #262).\n- Inline mode for the stream engine (#230, #248).\n- New keyword support: nfq_set_mark\n- Included an example decoder-events.rules file\n- api for adding and selecting runmodes was added\n- pcap logging / recording output was added\n- basic SCTP protocol parsing was added\n- more fine-grained CPU affinity setting support was added\n- stream engine inspects stream in larger chunks\n- fast_pattern support for http_method content modifier (#255)\n- negation support for isdataat keyword (#257)\n- configurable interval for stats.log updates (#247)\n- new pf_ring runmode was added that scales better\n- pcap live mode now handles the monitor interface going up and down\n- several QA additions to \"make check\"\n- NFQ (linux inline) mode was improved\n- Alerts classification fix (#275)\n- compiles and runs on big-endian systems (#63)\n- unified2 output works around barnyard2 issues with DLT_RAW + IPv6\n\n1.1beta1 -- 2010-12-21\n\n- New keyword support: http_raw_header, http_stat_msg, http_stat_code.\n- A new default pattern matcher, Aho-Corasick based, that uses much less memory.\n- reference.config support as supplied by ET/ETpro and VRT.\n- Much improved fast_pattern support, including for http_uri, http_client_body, http_header, http_raw_header.\n- Improved parsers, especially the DCERPC parser.\n- Much improved performance & accuracy.\n\n1.0.5 -- 2011-07-25\n\n- Fix stream reassembly bug #300. Thanks to Rmkml for the report.\n- Fix several (potential) issues fixed after a source code scan with Coverity generously contributed by RedHat.\n\n1.0.4 -- 2011-06-24\n\n- LibHTP updated to 0.2.6\n- Large number of (potential) issues fixed after a source code scan with Coverity generously contributed by RedHat.\n- Large number of (potential) issues fixed after source code scans with the Clang static analyzer.\n\n1.0.3 -- 2011-04-13\n\n- Fix broken checksum calculation for TCP/UDP in some cases\n- Fix errors in the byte_test, byte_jump, http_method and http_header keywords\n- Fix a ASN1 parsing issue\n- Improve LibHTP memory handling\n- Fix a defrag issue\n- Fix several stream engine issues\n\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 17.66796875,
          "content": "                    GNU GENERAL PUBLIC LICENSE\n                       Version 2, June 1991\n\n Copyright (C) 1989, 1991 Free Software Foundation, Inc.,\n 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA\n Everyone is permitted to copy and distribute verbatim copies\n of this license document, but changing it is not allowed.\n\n                            Preamble\n\n  The licenses for most software are designed to take away your\nfreedom to share and change it.  By contrast, the GNU General Public\nLicense is intended to guarantee your freedom to share and change free\nsoftware--to make sure the software is free for all its users.  This\nGeneral Public License applies to most of the Free Software\nFoundation's software and to any other program whose authors commit to\nusing it.  (Some other Free Software Foundation software is covered by\nthe GNU Lesser General Public License instead.)  You can apply it to\nyour programs, too.\n\n  When we speak of free software, we are referring to freedom, not\nprice.  Our General Public Licenses are designed to make sure that you\nhave the freedom to distribute copies of free software (and charge for\nthis service if you wish), that you receive source code or can get it\nif you want it, that you can change the software or use pieces of it\nin new free programs; and that you know you can do these things.\n\n  To protect your rights, we need to make restrictions that forbid\nanyone to deny you these rights or to ask you to surrender the rights.\nThese restrictions translate to certain responsibilities for you if you\ndistribute copies of the software, or if you modify it.\n\n  For example, if you distribute copies of such a program, whether\ngratis or for a fee, you must give the recipients all the rights that\nyou have.  You must make sure that they, too, receive or can get the\nsource code.  And you must show them these terms so they know their\nrights.\n\n  We protect your rights with two steps: (1) copyright the software, and\n(2) offer you this license which gives you legal permission to copy,\ndistribute and/or modify the software.\n\n  Also, for each author's protection and ours, we want to make certain\nthat everyone understands that there is no warranty for this free\nsoftware.  If the software is modified by someone else and passed on, we\nwant its recipients to know that what they have is not the original, so\nthat any problems introduced by others will not reflect on the original\nauthors' reputations.\n\n  Finally, any free program is threatened constantly by software\npatents.  We wish to avoid the danger that redistributors of a free\nprogram will individually obtain patent licenses, in effect making the\nprogram proprietary.  To prevent this, we have made it clear that any\npatent must be licensed for everyone's free use or not licensed at all.\n\n  The precise terms and conditions for copying, distribution and\nmodification follow.\n\n                    GNU GENERAL PUBLIC LICENSE\n   TERMS AND CONDITIONS FOR COPYING, DISTRIBUTION AND MODIFICATION\n\n  0. This License applies to any program or other work which contains\na notice placed by the copyright holder saying it may be distributed\nunder the terms of this General Public License.  The \"Program\", below,\nrefers to any such program or work, and a \"work based on the Program\"\nmeans either the Program or any derivative work under copyright law:\nthat is to say, a work containing the Program or a portion of it,\neither verbatim or with modifications and/or translated into another\nlanguage.  (Hereinafter, translation is included without limitation in\nthe term \"modification\".)  Each licensee is addressed as \"you\".\n\nActivities other than copying, distribution and modification are not\ncovered by this License; they are outside its scope.  The act of\nrunning the Program is not restricted, and the output from the Program\nis covered only if its contents constitute a work based on the\nProgram (independent of having been made by running the Program).\nWhether that is true depends on what the Program does.\n\n  1. You may copy and distribute verbatim copies of the Program's\nsource code as you receive it, in any medium, provided that you\nconspicuously and appropriately publish on each copy an appropriate\ncopyright notice and disclaimer of warranty; keep intact all the\nnotices that refer to this License and to the absence of any warranty;\nand give any other recipients of the Program a copy of this License\nalong with the Program.\n\nYou may charge a fee for the physical act of transferring a copy, and\nyou may at your option offer warranty protection in exchange for a fee.\n\n  2. You may modify your copy or copies of the Program or any portion\nof it, thus forming a work based on the Program, and copy and\ndistribute such modifications or work under the terms of Section 1\nabove, provided that you also meet all of these conditions:\n\n    a) You must cause the modified files to carry prominent notices\n    stating that you changed the files and the date of any change.\n\n    b) You must cause any work that you distribute or publish, that in\n    whole or in part contains or is derived from the Program or any\n    part thereof, to be licensed as a whole at no charge to all third\n    parties under the terms of this License.\n\n    c) If the modified program normally reads commands interactively\n    when run, you must cause it, when started running for such\n    interactive use in the most ordinary way, to print or display an\n    announcement including an appropriate copyright notice and a\n    notice that there is no warranty (or else, saying that you provide\n    a warranty) and that users may redistribute the program under\n    these conditions, and telling the user how to view a copy of this\n    License.  (Exception: if the Program itself is interactive but\n    does not normally print such an announcement, your work based on\n    the Program is not required to print an announcement.)\n\nThese requirements apply to the modified work as a whole.  If\nidentifiable sections of that work are not derived from the Program,\nand can be reasonably considered independent and separate works in\nthemselves, then this License, and its terms, do not apply to those\nsections when you distribute them as separate works.  But when you\ndistribute the same sections as part of a whole which is a work based\non the Program, the distribution of the whole must be on the terms of\nthis License, whose permissions for other licensees extend to the\nentire whole, and thus to each and every part regardless of who wrote it.\n\nThus, it is not the intent of this section to claim rights or contest\nyour rights to work written entirely by you; rather, the intent is to\nexercise the right to control the distribution of derivative or\ncollective works based on the Program.\n\nIn addition, mere aggregation of another work not based on the Program\nwith the Program (or with a work based on the Program) on a volume of\na storage or distribution medium does not bring the other work under\nthe scope of this License.\n\n  3. You may copy and distribute the Program (or a work based on it,\nunder Section 2) in object code or executable form under the terms of\nSections 1 and 2 above provided that you also do one of the following:\n\n    a) Accompany it with the complete corresponding machine-readable\n    source code, which must be distributed under the terms of Sections\n    1 and 2 above on a medium customarily used for software interchange; or,\n\n    b) Accompany it with a written offer, valid for at least three\n    years, to give any third party, for a charge no more than your\n    cost of physically performing source distribution, a complete\n    machine-readable copy of the corresponding source code, to be\n    distributed under the terms of Sections 1 and 2 above on a medium\n    customarily used for software interchange; or,\n\n    c) Accompany it with the information you received as to the offer\n    to distribute corresponding source code.  (This alternative is\n    allowed only for noncommercial distribution and only if you\n    received the program in object code or executable form with such\n    an offer, in accord with Subsection b above.)\n\nThe source code for a work means the preferred form of the work for\nmaking modifications to it.  For an executable work, complete source\ncode means all the source code for all modules it contains, plus any\nassociated interface definition files, plus the scripts used to\ncontrol compilation and installation of the executable.  However, as a\nspecial exception, the source code distributed need not include\nanything that is normally distributed (in either source or binary\nform) with the major components (compiler, kernel, and so on) of the\noperating system on which the executable runs, unless that component\nitself accompanies the executable.\n\nIf distribution of executable or object code is made by offering\naccess to copy from a designated place, then offering equivalent\naccess to copy the source code from the same place counts as\ndistribution of the source code, even though third parties are not\ncompelled to copy the source along with the object code.\n\n  4. You may not copy, modify, sublicense, or distribute the Program\nexcept as expressly provided under this License.  Any attempt\notherwise to copy, modify, sublicense or distribute the Program is\nvoid, and will automatically terminate your rights under this License.\nHowever, parties who have received copies, or rights, from you under\nthis License will not have their licenses terminated so long as such\nparties remain in full compliance.\n\n  5. You are not required to accept this License, since you have not\nsigned it.  However, nothing else grants you permission to modify or\ndistribute the Program or its derivative works.  These actions are\nprohibited by law if you do not accept this License.  Therefore, by\nmodifying or distributing the Program (or any work based on the\nProgram), you indicate your acceptance of this License to do so, and\nall its terms and conditions for copying, distributing or modifying\nthe Program or works based on it.\n\n  6. Each time you redistribute the Program (or any work based on the\nProgram), the recipient automatically receives a license from the\noriginal licensor to copy, distribute or modify the Program subject to\nthese terms and conditions.  You may not impose any further\nrestrictions on the recipients' exercise of the rights granted herein.\nYou are not responsible for enforcing compliance by third parties to\nthis License.\n\n  7. If, as a consequence of a court judgment or allegation of patent\ninfringement or for any other reason (not limited to patent issues),\nconditions are imposed on you (whether by court order, agreement or\notherwise) that contradict the conditions of this License, they do not\nexcuse you from the conditions of this License.  If you cannot\ndistribute so as to satisfy simultaneously your obligations under this\nLicense and any other pertinent obligations, then as a consequence you\nmay not distribute the Program at all.  For example, if a patent\nlicense would not permit royalty-free redistribution of the Program by\nall those who receive copies directly or indirectly through you, then\nthe only way you could satisfy both it and this License would be to\nrefrain entirely from distribution of the Program.\n\nIf any portion of this section is held invalid or unenforceable under\nany particular circumstance, the balance of the section is intended to\napply and the section as a whole is intended to apply in other\ncircumstances.\n\nIt is not the purpose of this section to induce you to infringe any\npatents or other property right claims or to contest validity of any\nsuch claims; this section has the sole purpose of protecting the\nintegrity of the free software distribution system, which is\nimplemented by public license practices.  Many people have made\ngenerous contributions to the wide range of software distributed\nthrough that system in reliance on consistent application of that\nsystem; it is up to the author/donor to decide if he or she is willing\nto distribute software through any other system and a licensee cannot\nimpose that choice.\n\nThis section is intended to make thoroughly clear what is believed to\nbe a consequence of the rest of this License.\n\n  8. If the distribution and/or use of the Program is restricted in\ncertain countries either by patents or by copyrighted interfaces, the\noriginal copyright holder who places the Program under this License\nmay add an explicit geographical distribution limitation excluding\nthose countries, so that distribution is permitted only in or among\ncountries not thus excluded.  In such case, this License incorporates\nthe limitation as if written in the body of this License.\n\n  9. The Free Software Foundation may publish revised and/or new versions\nof the General Public License from time to time.  Such new versions will\nbe similar in spirit to the present version, but may differ in detail to\naddress new problems or concerns.\n\nEach version is given a distinguishing version number.  If the Program\nspecifies a version number of this License which applies to it and \"any\nlater version\", you have the option of following the terms and conditions\neither of that version or of any later version published by the Free\nSoftware Foundation.  If the Program does not specify a version number of\nthis License, you may choose any version ever published by the Free Software\nFoundation.\n\n  10. If you wish to incorporate parts of the Program into other free\nprograms whose distribution conditions are different, write to the author\nto ask for permission.  For software which is copyrighted by the Free\nSoftware Foundation, write to the Free Software Foundation; we sometimes\nmake exceptions for this.  Our decision will be guided by the two goals\nof preserving the free status of all derivatives of our free software and\nof promoting the sharing and reuse of software generally.\n\n                            NO WARRANTY\n\n  11. BECAUSE THE PROGRAM IS LICENSED FREE OF CHARGE, THERE IS NO WARRANTY\nFOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW.  EXCEPT WHEN\nOTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES\nPROVIDE THE PROGRAM \"AS IS\" WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED\nOR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF\nMERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE.  THE ENTIRE RISK AS\nTO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU.  SHOULD THE\nPROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING,\nREPAIR OR CORRECTION.\n\n  12. IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING\nWILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MAY MODIFY AND/OR\nREDISTRIBUTE THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES,\nINCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING\nOUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED\nTO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY\nYOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER\nPROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE\nPOSSIBILITY OF SUCH DAMAGES.\n\n                     END OF TERMS AND CONDITIONS\n\n            How to Apply These Terms to Your New Programs\n\n  If you develop a new program, and you want it to be of the greatest\npossible use to the public, the best way to achieve this is to make it\nfree software which everyone can redistribute and change under these terms.\n\n  To do so, attach the following notices to the program.  It is safest\nto attach them to the start of each source file to most effectively\nconvey the exclusion of warranty; and each file should have at least\nthe \"copyright\" line and a pointer to where the full notice is found.\n\n    <one line to give the program's name and a brief idea of what it does.>\n    Copyright (C) <year>  <name of author>\n\n    This program is free software; you can redistribute it and/or modify\n    it under the terms of the GNU General Public License as published by\n    the Free Software Foundation; either version 2 of the License, or\n    (at your option) any later version.\n\n    This program is distributed in the hope that it will be useful,\n    but WITHOUT ANY WARRANTY; without even the implied warranty of\n    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n    GNU General Public License for more details.\n\n    You should have received a copy of the GNU General Public License along\n    with this program; if not, write to the Free Software Foundation, Inc.,\n    51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\n\nAlso add information on how to contact you by electronic and paper mail.\n\nIf the program is interactive, make it output a short notice like this\nwhen it starts in an interactive mode:\n\n    Gnomovision version 69, Copyright (C) year name of author\n    Gnomovision comes with ABSOLUTELY NO WARRANTY; for details type `show w'.\n    This is free software, and you are welcome to redistribute it\n    under certain conditions; type `show c' for details.\n\nThe hypothetical commands `show w' and `show c' should show the appropriate\nparts of the General Public License.  Of course, the commands you use may\nbe called something other than `show w' and `show c'; they could even be\nmouse-clicks or menu items--whatever suits your program.\n\nYou should also get your employer (if you work as a programmer) or your\nschool, if any, to sign a \"copyright disclaimer\" for the program, if\nnecessary.  Here is a sample; alter the names:\n\n  Yoyodyne, Inc., hereby disclaims all copyright interest in the program\n  `Gnomovision' (which makes passes at compilers) written by James Hacker.\n\n  <signature of Ty Coon>, 1 April 1989\n  Ty Coon, President of Vice\n\nThis General Public License does not permit incorporating your program into\nproprietary programs.  If your program is a subroutine library, you may\nconsider it more useful to permit linking proprietary applications with the\nlibrary.  If this is what you want to do, use the GNU Lesser General\nPublic License instead of this License.\n"
        },
        {
          "name": "Makefile.am",
          "type": "blob",
          "size": 2.919921875,
          "content": "# not a GNU package. You can remove this line, if\n# have all needed files, that a GNU package needs\nAUTOMAKE_OPTIONS = foreign 1.4\nACLOCAL_AMFLAGS = -I m4\nEXTRA_DIST = ChangeLog COPYING LICENSE suricata.yaml.in \\\n             threshold.config SECURITY.md \\\n             $(SURICATA_UPDATE_DIR) \\\n\t     lua \\\n\t     acsite.m4 \\\n\t     scripts/generate-images.sh \\\n\t     scripts/docs-almalinux9-minimal-build.sh \\\n\t     scripts/docs-ubuntu-debian-minimal-build.sh \\\n       scripts/evedoc.py \\\n\t     examples/plugins\nSUBDIRS = $(HTP_DIR) rust src plugins qa rules doc contrib etc python ebpf \\\n          $(SURICATA_UPDATE_DIR)\nDIST_SUBDIRS = $(SUBDIRS) examples/lib/simple\n\nCLEANFILES = stamp-h[0-9]*\n\ninstall-data-am:\n\t@echo \"Run 'make install-conf' if you want to install initial configuration files. Or 'make install-full' to install configuration and rules\";\n\ninstall-full:\n\t$(MAKE) install\n\t$(MAKE) install-conf\n\t$(MAKE) install-rules\n\ninstall-conf:\n\tinstall -d \"$(DESTDIR)$(e_sysconfdir)\"\n\t@test -e \"$(DESTDIR)$(e_sysconfdir)/suricata.yaml\" || install -m 600 \"$(top_srcdir)/suricata.yaml\" \"$(DESTDIR)$(e_sysconfdir)\"\n\t@test -e \"$(DESTDIR)$(e_sysconfdir)/classification.config\" || install -m 600 \"$(top_srcdir)/etc/classification.config\" \"$(DESTDIR)$(e_sysconfdir)\"\n\t@test -e \"$(DESTDIR)$(e_sysconfdir)/reference.config\" || install -m 600 \"$(top_srcdir)/etc/reference.config\" \"$(DESTDIR)$(e_sysconfdir)\"\n\t@test -e \"$(DESTDIR)$(e_sysconfdir)/threshold.config\" || install -m 600 \"$(top_srcdir)/threshold.config\" \"$(DESTDIR)$(e_sysconfdir)\"\n\tinstall -d \"$(DESTDIR)$(e_logfilesdir)\"\n\tinstall -d \"$(DESTDIR)$(e_logcertsdir)\"\n\tinstall -d \"$(DESTDIR)$(e_rundir)\"\n\tinstall -m 770 -d \"$(DESTDIR)$(e_localstatedir)\"\n\tinstall -m 770 -d \"$(DESTDIR)$(e_datadir)\"\n\ninstall-rules:\nif INSTALL_SURICATA_UPDATE\n\tLD_LIBRARY_PATH=$(libdir) $(DESTDIR)$(bindir)/suricata-update \\\n\t\t--suricata $(DESTDIR)$(bindir)/suricata \\\n\t\t--suricata-conf $(DESTDIR)$(sysconfdir)/suricata/suricata.yaml \\\n\t\t--no-test --no-reload\nelse\n\t@echo \"\"\n\t@echo \"Warning: No rules will be downloaded as suricata-update\"\n\t@echo \"   is not available: ${install_suricata_update_reason}\"\nendif\n\t@echo \"\"\n\t@echo \"You can now start suricata by running as root something like:\"\n\t@echo \"  $(DESTDIR)$(bindir)/suricata -c $(DESTDIR)$(e_sysconfdir)suricata.yaml -i eth0\"\n\t@echo \"\"\n\t@echo \"If a library like libhtp.so is not found, you can run suricata with:\"\n\t@echo \"  LD_LIBRARY_PATH=\"$(DESTDIR)$(prefix)/lib\" \"$(DESTDIR)$(bindir)/suricata\" -c \"$(DESTDIR)$(e_sysconfdir)suricata.yaml\" -i eth0\"\n\t@echo \"\"\n\t@echo \"The Emerging Threats Open rules are now installed. Rules can be\"\n\t@echo \"updated and managed with the suricata-update tool.\"\n\t@echo \"\"\n\t@echo \"For more information please see:\"\n\t@echo \"  https://docs.suricata.io/en/latest/rule-management/index.html\"\n\t@echo \"\"\n\ninstall-library:\n\tcd src && $(MAKE) $@\n\tcd rust && $(MAKE) $@\n\t$(INSTALL) libsuricata-config \"$(DESTDIR)$(bindir)/libsuricata-config\"\n\ninstall-headers:\n\tcd src && $(MAKE) $@\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 5.5576171875,
          "content": "# Suricata\n\n[![Fuzzing Status](https://oss-fuzz-build-logs.storage.googleapis.com/badges/suricata.svg)](https://bugs.chromium.org/p/oss-fuzz/issues/list?sort=-opened&can=1&q=proj:suricata)\n[![codecov](https://codecov.io/gh/OISF/suricata/branch/master/graph/badge.svg?token=QRyyn2BSo1)](https://codecov.io/gh/OISF/suricata)\n\n## Introduction\n\n[Suricata](https://suricata.io) is a network IDS, IPS and NSM engine\ndeveloped by the [OISF](https://oisf.net) and the Suricata community.\n\n## Resources\n\n- [Home Page](https://suricata.io)\n- [Bug Tracker](https://redmine.openinfosecfoundation.org/projects/suricata)\n- [User Guide](https://docs.suricata.io)\n- [Dev Guide](https://docs.suricata.io/en/latest/devguide/index.html)\n- [Installation Guide](https://docs.suricata.io/en/latest/install.html)\n- [User Support Forum](https://forum.suricata.io)\n\n## Contributing\n\nWe're happily taking patches and other contributions. Please see our\n[Contribution\nProcess](https://docs.suricata.io/en/latest/devguide/contributing/contribution-process.html)\nfor how to get started.\n\nSuricata is a complex piece of software dealing with mostly untrusted\ninput. Mishandling this input will have serious consequences:\n\n* in IPS mode a crash may knock a network offline\n* in passive mode a compromise of the IDS may lead to loss of critical\n  and confidential data\n* missed detection may lead to undetected compromise of the network\n\nIn other words, we think the stakes are pretty high, especially since\nin many common cases the IDS/IPS will be directly reachable by an\nattacker.\n\nFor this reason, we have developed a QA process that is quite\nextensive. A consequence is that contributing to Suricata can be a\nsomewhat lengthy process.\n\nOn a high level, the steps are:\n\n1. GitHub-CI based checks. This runs automatically when a pull request\n   is made.\n2. Review by devs from the team and community\n3. QA runs from private QA setups. These are private due to the nature\n   of the test traffic.\n\n### Overview of Suricata's QA steps\n\nOISF team members are able to submit builds to our private QA\nsetup. It will run a series of build tests and a regression suite to\nconfirm no existing features break.\n\nThe final QA runs takes a few hours minimally, and generally runs\novernight. It currently runs:\n\n- extensive build tests on different OS', compilers, optimization\n  levels, configure features\n- static code analysis using cppcheck, scan-build\n- runtime code analysis using valgrind, AddressSanitizer,\n  LeakSanitizer\n- regression tests for past bugs\n- output validation of logging\n- unix socket testing\n- pcap based fuzz testing using ASAN and LSAN\n- traffic replay based IDS and IPS tests\n\nNext to these tests, based on the type of code change further tests\ncan be run manually:\n\n- traffic replay testing (multi-gigabit)\n- large pcap collection processing (multi-terabytes)\n- fuzz testing (might take multiple days or even weeks)\n- pcap based performance testing\n- live performance testing\n- various other manual tests based on evaluation of the proposed\n  changes\n\nIt's important to realize that almost all of the tests above are used\nas acceptance tests. If something fails, it's up to you to address\nthis in your code.\n\nOne step of the QA is currently run post-merge. We submit builds to\nthe Coverity Scan program. Due to limitations of this (free) service,\nwe can submit once a day max.  Of course it can happen that after the\nmerge the community will find issues. For both cases we request you to\nhelp address the issues as they may come up.\n\n## FAQ\n\n__Q: Will you accept my PR?__\n\nA: That depends on a number of things, including the code\nquality. With new features it also depends on whether the team and/or\nthe community think the feature is useful, how much it affects other\ncode and features, the risk of performance regressions, etc.\n\n__Q: When will my PR be merged?__\n\nA: It depends, if it's a major feature or considered a high risk\nchange, it will probably go into the next major version.\n\n__Q: Why was my PR closed?__\n\nA: As documented in the [Suricata GitHub\nworkflow](https://docs.suricata.io/en/latest/devguide/contributing/github-pr-workflow.html),\nwe expect a new pull request for every change.\n\nNormally, the team (or community) will give feedback on a pull request\nafter which it is expected to be replaced by an improved PR. So look\nat the comments. If you disagree with the comments we can still\ndiscuss them in the closed PR.\n\nIf the PR was closed without comments it's likely due to QA\nfailure. If the GitHub-CI checks failed, the PR should be fixed right\naway. No need for a discussion about it, unless you believe the QA\nfailure is incorrect.\n\n__Q: The compiler/code analyser/tool is wrong, what now?__\n\nA: To assist in the automation of the QA, we're not accepting warnings\nor errors to stay. In some cases this could mean that we add a\nsuppression if the tool supports that (e.g. valgrind, DrMemory). Some\nwarnings can be disabled. In some exceptional cases the only\n'solution' is to refactor the code to work around a static code\nchecker limitation false positive. While frustrating, we prefer this\nover leaving warnings in the output. Warnings tend to get ignored and\nthen increase risk of hiding other warnings.\n\n__Q: I think your QA test is wrong__\n\nA: If you really think it is, we can discuss how to improve it. But\ndon't come to this conclusion too quickly, more often it's the code\nthat turns out to be wrong.\n\n__Q: Do you require signing of a contributor license agreement?__\n\nA: Yes, we do this to keep the ownership of Suricata in one hand: the\nOpen Information Security Foundation. See\nhttp://suricata.io/about/open-source/ and\nhttp://suricata.io/about/contribution-agreement/\n"
        },
        {
          "name": "SECURITY.md",
          "type": "blob",
          "size": 3.6845703125,
          "content": "# Security Policy\n\nBeing a security tool itself, the security of Suricata itself is naturally of\ngreat importance. This document describes the policy around security issues as\nwell as how to report them.\n\nIf you believe you found a vulnerability, please report it to us as described\nin this document.\n\n\n## Severity Levels\n\nWe will determine the severity of each issue, taking into account our\nexperience dealing with past issues, versions affected, common defaults,\nour estimate of exploitation complexity, part of the code affected,\nand use cases. We use the following severity categories:\n\n* **CRITICAL** Severity. This affects Tier 1 features that are enabled by default\nwhere the issue disrupts availability of the service, leading to severe\nloss of visibility and/or availability. Remotely triggerable traffic\nbased code execution, crashes, or evasions with a wide scope are considered to be\nin-scope for this severity. These issues will be kept private and will trigger a\nnew release of all supported versions. We will attempt to address these as soon\nas possible.\n\n* **HIGH** Severity. This includes issues that are of a lower risk than critical,\nperhaps due to being disabled by default Tier 1 or affecting Tier 2 and\nCommunity features, or which are less likely to be exploitable. These issues\nwill be kept private and will trigger a new release of all supported versions.\nWe will attempt to keep the time these issues are private to a minimum; our\naim would be no longer than a month where this is something under our control.\n\n* **MODERATE** Severity. This includes issues like crashes or evasion in Tier 2 and\nCommunity features that are not enabled by default. These will in general be\nkept private until the next release, and that release will be scheduled so\nthat it can roll up several such flaws at one time.\n\n* **LOW** Severity. This includes issues such as those that only affect the\nSuricata command line utilities, or unlikely configurations. These will in\ngeneral be fixed as soon as possible in latest development versions, and may be\nbackported to older versions that are still getting updates. These will be\npart of the Changelog as a security ticket, but they may not trigger new\nreleases.\n\nNote that we'll be refining the levels based on our experiences with applying them\nto actual issues.\n\n## CVE ID's and Github Security Advisories (GHSA)\n\nWe will request a CVE ID for an issue if appropriate. Note that multiple\nissues may share the same CVE ID.\n\nWe work with the Github CNA, through the Github Security Advisory (GHSA) facility.\n\nThe GHSA's will be published at least 2 weeks after the public release addressing\nthe issue, together with the redmine security tickets.\n\n## Support Status of affected code\n\n4 levels are defined: Tier 1, Tier 2, Community and Unmaintained.\n\nThese are documented in https://docs.suricata.io/en/latest/support-status.html\n\n\n## Reporting Issues\n\nFor reporting security issues, please use `security@oisf.net`.\n\nIf you report a security issue to us, please share as much detail about the issue\nas possible: pcaps, attack scripts, potential fixes, etc. If you share pcaps or\nother data, please clearly state if these can (eventually) enter our public CI/QA.\n\nWe will assign a severity and will share our assessment with you.\n\nWe will create a security ticket, which will be private until at least 2 weeks after\na public release addressing the issue.\n\nWe will acknowledge you in the release notes, release announcement and GHSA. If you\ndo not want this, please clearly state this. For the GHSA credits, please give us\nyour github handle.\n\nPlease let us know if you've requested a CVE ID. If you haven't, we can do it.\n\nOISF does not participate in bug bounty programs, or offer any other rewards\nfor reporting issues.\n"
        },
        {
          "name": "acsite.m4",
          "type": "blob",
          "size": 10.3876953125,
          "content": "#These defines are needed because CentOS5 uses a old version of autoconf\n#\nAC_DEFUN([AC_TYPE_INT8_T], [_AC_TYPE_INT(8)])\nAC_DEFUN([AC_TYPE_INT16_T], [_AC_TYPE_INT(16)])\nAC_DEFUN([AC_TYPE_INT32_T], [_AC_TYPE_INT(32)])\nAC_DEFUN([AC_TYPE_INT64_T], [_AC_TYPE_INT(64)])\nAC_DEFUN([AC_TYPE_UINT8_T], [_AC_TYPE_UNSIGNED_INT(8)])\nAC_DEFUN([AC_TYPE_UINT16_T], [_AC_TYPE_UNSIGNED_INT(16)])\nAC_DEFUN([AC_TYPE_UINT32_T], [_AC_TYPE_UNSIGNED_INT(32)])\nAC_DEFUN([AC_TYPE_UINT64_T], [_AC_TYPE_UNSIGNED_INT(64)])\n\nAC_DEFUN([AC_TYPE_UINT], [_AC_TYPE_UTYPE(u_int)])\nAC_DEFUN([AC_TYPE_ULONG], [_AC_TYPE_UTYPE(u_long)])\nAC_DEFUN([AC_TYPE_UCHAR], [_AC_TYPE_UTYPE(u_char)])\nAC_DEFUN([AC_TYPE_USHORT], [_AC_TYPE_UTYPE(u_short)])\n\n# _AC_TYPE_UTYPE(BASE)\n# -------------------\nAC_DEFUN([_AC_TYPE_UTYPE],\n[\n  AC_CACHE_CHECK([for $1], [ac_cv_c_$1],\n   [ac_cv_c_$1=no\n    case $1 in\n        \"u_long\")\n            ac_cv_c_$1_type=\"U_LONG\"\n            ;;\n        \"u_int\")\n            ac_cv_c_$1_type=\"U_INT\"\n            ;;\n        \"u_short\")\n            ac_cv_c_$1_type=\"U_SHORT\"\n            ;;\n        \"u_char\")\n            ac_cv_c_$1_type=\"U_CHAR\"\n            ;;\n        *)\n            ;;\n    esac\n\n    AC_COMPILE_IFELSE(\n\t    [AC_LANG_BOOL_COMPILE_TRY(\n\t        [AC_INCLUDES_DEFAULT],\n\t        [[($1) -1 >> ((sizeof($1) * 8) - 1) == 1]])],\n\t    [AS_CASE([$1], [$1],\n\t    [ac_cv_c_$1=yes],\n\t    [ac_cv_c_$1=no])])\n     ])\n  case $ac_cv_c_$1 in #(\n  no)\n    AC_DEFINE_UNQUOTED([HAVE_TYPE_$ac_cv_c_$1_type_NOT_DEFINED], [1], [$1 is undefined])\n        ;;\n  *)\n        ;;\n  esac\n])# _AC_TYPE_UTYPE\n\n# _AC_TYPE_INT(NBITS)\n# -------------------\nAC_DEFUN([_AC_TYPE_INT],\n[\n  AC_CACHE_CHECK([for int$1_t], [ac_cv_c_int$1_t],\n    [ac_cv_c_int$1_t=no\n     for ac_type in 'int$1_t' 'int' 'long int' \\\n\t 'long long int' 'short int' 'signed char'; do\n       AC_COMPILE_IFELSE(\n\t [AC_LANG_BOOL_COMPILE_TRY(\n\t    [AC_INCLUDES_DEFAULT],\n\t    [[0 < ($ac_type) (((($ac_type) 1 << ($1 - 2)) - 1) * 2 + 1)]])],\n\t [AC_COMPILE_IFELSE(\n\t    [AC_LANG_BOOL_COMPILE_TRY(\n\t       [AC_INCLUDES_DEFAULT],\n\t       [[($ac_type) (((($ac_type) 1 << ($1 - 2)) - 1) * 2 + 1)\n\t         < ($ac_type) (((($ac_type) 1 << ($1 - 2)) - 1) * 2 + 2)]])],\n\t    [],\n\t    [AS_CASE([$ac_type], [int$1_t],\n\t       [ac_cv_c_int$1_t=yes],\n\t       [ac_cv_c_int$1_t=$ac_type])])])\n       test \"$ac_cv_c_int$1_t\" != no && break\n     done])\n  case $ac_cv_c_int$1_t in #(\n  no|yes) ;; #(\n  *)\n    AC_DEFINE_UNQUOTED([int$1_t], [$ac_cv_c_int$1_t],\n      [Define to the type of a signed integer type of width exactly $1 bits\n       if such a type exists and the standard includes do not define it.]);;\n  esac\n])# _AC_TYPE_INT\n\n# _AC_TYPE_UNSIGNED_INT(NBITS)\n# ----------------------------\nAC_DEFUN([_AC_TYPE_UNSIGNED_INT],\n[\n  AC_CACHE_CHECK([for uint$1_t], [ac_cv_c_uint$1_t],\n    [ac_cv_c_uint$1_t=no\n     for ac_type in 'uint$1_t' 'unsigned int' 'unsigned long int' \\\n\t 'unsigned long long int' 'unsigned short int' 'unsigned char'; do\n       AC_COMPILE_IFELSE(\n\t [AC_LANG_BOOL_COMPILE_TRY(\n\t    [AC_INCLUDES_DEFAULT],\n\t    [[($ac_type) -1 >> ($1 - 1) == 1]])],\n\t [AS_CASE([$ac_type], [uint$1_t],\n\t    [ac_cv_c_uint$1_t=yes],\n\t    [ac_cv_c_uint$1_t=$ac_type])])\n       test \"$ac_cv_c_uint$1_t\" != no && break\n     done])\n  case $ac_cv_c_uint$1_t in #(\n  no|yes) ;; #(\n  *)\n    m4_bmatch([$1], [^\\(8\\|32\\|64\\)$],\n      [AC_DEFINE([_UINT$1_T], 1,\n\t [Define for Solaris 2.5.1 so the uint$1_t typedef from\n\t  <sys/synch.h>, <pthread.h>, or <semaphore.h> is not used.\n\t  If the typedef was allowed, the #define below would cause a\n\t  syntax error.])])\n    AC_DEFINE_UNQUOTED([uint$1_t], [$ac_cv_c_uint$1_t],\n      [Define to the type of an unsigned integer type of width exactly $1 bits\n       if such a type exists and the standard includes do not define it.]);;\n  esac\n])# _AC_TYPE_UNSIGNED_INT\n\n# AS_CASE(WORD, [PATTERN1], [IF-MATCHED1]...[DEFAULT])\n# ----------------------------------------------------\n# Expand into\n# | case WORD in\n# | PATTERN1) IF-MATCHED1 ;;\n# | ...\n# | *) DEFAULT ;;\n# | esac\nm4_define([_AS_CASE],\n[m4_if([$#], 0, [m4_fatal([$0: too few arguments: $#])],\n       [$#], 1, [  *) $1 ;;],\n       [$#], 2, [  $1) m4_default([$2], [:]) ;;],\n       [  $1) m4_default([$2], [:]) ;;\n$0(m4_shiftn(2, $@))])dnl\n])\nm4_defun([AS_CASE],\n[m4_ifval([$2$3],\n[case $1 in\n_AS_CASE(m4_shift($@))\nesac\n])dnl\n])# AS_CASE\n\n# _AC_PROG_CC_C99 ([ACTION-IF-AVAILABLE], [ACTION-IF-UNAVAILABLE])\n# ----------------------------------------------------------------\n# If the C compiler is not in ISO C99 mode by default, try to add an\n# option to output variable CC to make it so.  This macro tries\n# various options that select ISO C99 on some system or another.  It\n# considers the compiler to be in ISO C99 mode if it handles _Bool,\n# // comments, flexible array members, inline, long long int, mixed\n# code and declarations, named initialization of structs, restrict,\n# va_copy, varargs macros, variable declarations in for loops and\n# variable length arrays.\nAC_DEFUN([_AC_PROG_CC_C99],\n[_AC_C_STD_TRY([c99],\n[[#include <stdarg.h>\n#include <stdbool.h>\n#include <stdlib.h>\n#include <wchar.h>\n#include <stdio.h>\n\n// Check varargs macros.  These examples are taken from C99 6.10.3.5.\n#define debug(...) fprintf (stderr, __VA_ARGS__)\n#define showlist(...) puts (#__VA_ARGS__)\n#define report(test,...) ((test) ? puts (#test) : printf (__VA_ARGS__))\nstatic void\ntest_varargs_macros (void)\n{\n  int x = 1234;\n  int y = 5678;\n  debug (\"Flag\");\n  debug (\"X = %d\\n\", x);\n  showlist (The first, second, and third items.);\n  report (x>y, \"x is %d but y is %d\", x, y);\n}\n\n// Check long long types.\n#define BIG64 18446744073709551615ull\n#define BIG32 4294967295ul\n#define BIG_OK (BIG64 / BIG32 == 4294967297ull && BIG64 % BIG32 == 0)\n#if !BIG_OK\n  your preprocessor is broken;\n#endif\n#if BIG_OK\n#else\n  your preprocessor is broken;\n#endif\nstatic long long int bignum = -9223372036854775807LL;\nstatic unsigned long long int ubignum = BIG64;\n\nstruct incomplete_array\n{\n  int datasize;\n  double data[];\n};\n\nstruct named_init {\n  int number;\n  const wchar_t *name;\n  double average;\n};\n\ntypedef const char *ccp;\n\nstatic inline int\ntest_restrict (ccp restrict text)\n{\n  // See if C++-style comments work.\n  // Iterate through items via the restricted pointer.\n  // Also check for declarations in for loops.\n  for (unsigned int i = 0; *(text+i) != '\\0'; ++i)\n    continue;\n  return 0;\n}\n\n// Check varargs and va_copy.\nstatic void\ntest_varargs (const char *format, ...)\n{\n  va_list args;\n  va_start (args, format);\n  va_list args_copy;\n  va_copy (args_copy, args);\n\n  const char *str;\n  int number;\n  float fnumber;\n\n  while (*format)\n    {\n      switch (*format++)\n        {\n        case 's': // string\n          str = va_arg (args_copy, const char *);\n          break;\n        case 'd': // int\n          number = va_arg (args_copy, int);\n          break;\n        case 'f': // float\n          fnumber = va_arg (args_copy, double);\n          break;\n        default:\n          break;\n        }\n    }\n  va_end (args_copy);\n  va_end (args);\n}\n]],\n[[\n  // Check bool.\n  _Bool success = false;\n\n  // Check restrict.\n if (test_restrict (\"String literal\") == 0)\n    success = true;\n  char *restrict newvar = \"Another string\";\n\n  // Check varargs.\n  test_varargs (\"s, d' f .\", \"string\", 65, 34.234);\n  test_varargs_macros ();\n\n  // Check flexible array members.\n  struct incomplete_array *ia =\n    malloc (sizeof (struct incomplete_array) + (sizeof (double) * 10));\n  ia->datasize = 10;\n  for (int i = 0; i < ia->datasize; ++i)\n    ia->data[i] = i * 1.234;\n\n  // Check named initializers.\n  struct named_init ni = {\n    .number = 34,\n    .name = L\"Test wide string\",\n    .average = 543.34343,\n  };\n\n  ni.number = 58;\n\n  int dynamic_array[ni.number];\n  dynamic_array[ni.number - 1] = 543;\n\n  // work around unused variable warnings\n  return (!success || bignum == 0LL || ubignum == 0uLL || newvar[0] == 'x'\n          || dynamic_array[ni.number - 1] != 543);\n]],\ndnl Try\ndnl GCC         -std=gnu99 (unused restrictive modes: -std=c99 -std=iso9899:1999)\ndnl AIX         -qlanglvl=extc99 (unused restrictive mode: -qlanglvl=stdc99)\ndnl HP cc       -AC99\ndnl Intel ICC   -std=c99, -c99 (deprecated)\ndnl IRIX        -c99\ndnl Solaris     -xc99=all (Forte Developer 7 C mishandles -xc99 on Solaris 9,\ndnl             as it incorrectly assumes C99 semantics for library functions)\ndnl Tru64       -c99\ndnl with extended modes being tried first.\n[[-std=gnu99 -std=c99 -c99 -AC99 -xc99=all -qlanglvl=extc99]], [$1], [$2])[]dnl\n])# _AC_PROG_CC_C99\n\n\n# AC_PROG_CC_C89\n# --------------\nAC_DEFUN([AC_PROG_CC_C89],\n[ AC_REQUIRE([AC_PROG_CC])dnl\n  _AC_PROG_CC_C89\n])\n\n# AC_PROG_CC_C99\n# --------------\nAC_DEFUN([AC_PROG_CC_C99],\n[ AC_REQUIRE([AC_PROG_CC])dnl\n  _AC_PROG_CC_C99\n])\n\n\n# AC_PROG_CC_STDC\n# ---------------\nAC_DEFUN([AC_PROG_CC_STDC],\n[ AC_REQUIRE([AC_PROG_CC])dnl\n  AS_CASE([$ac_cv_prog_cc_stdc],\n    [no], [ac_cv_prog_cc_c99=no; ac_cv_prog_cc_c89=no],\n          [_AC_PROG_CC_C99([ac_cv_prog_cc_stdc=$ac_cv_prog_cc_c99],\n             [_AC_PROG_CC_C89([ac_cv_prog_cc_stdc=$ac_cv_prog_cc_c89],\n                              [ac_cv_prog_cc_stdc=no])])])dnl\n  AC_MSG_CHECKING([for $CC option to accept ISO Standard C])\n  AC_CACHE_VAL([ac_cv_prog_cc_stdc], [])\n  AS_CASE([$ac_cv_prog_cc_stdc],\n    [no], [AC_MSG_RESULT([unsupported])],\n    [''], [AC_MSG_RESULT([none needed])],\n          [AC_MSG_RESULT([$ac_cv_prog_cc_stdc])])\n])\n\n# _AC_C_STD_TRY(STANDARD, TEST-PROLOGUE, TEST-BODY, OPTION-LIST,\n#               ACTION-IF-AVAILABLE, ACTION-IF-UNAVAILABLE)\n# --------------------------------------------------------------\n# Check whether the C compiler accepts features of STANDARD (e.g `c89', `c99')\n# by trying to compile a program of TEST-PROLOGUE and TEST-BODY.  If this fails,\n# try again with each compiler option in the space-separated OPTION-LIST; if one\n# helps, append it to CC.  If eventually successful, run ACTION-IF-AVAILABLE,\n# else ACTION-IF-UNAVAILABLE.\nAC_DEFUN([_AC_C_STD_TRY],\n[AC_MSG_CHECKING([for $CC option to accept ISO ]m4_translit($1, [c], [C]))\nAC_CACHE_VAL(ac_cv_prog_cc_$1,\n[ac_cv_prog_cc_$1=no\nac_save_CC=$CC\nAC_LANG_CONFTEST([AC_LANG_PROGRAM([$2], [$3])])\nfor ac_arg in '' $4\ndo\n  CC=\"$ac_save_CC $ac_arg\"\n  _AC_COMPILE_IFELSE([], [ac_cv_prog_cc_$1=$ac_arg])\n  test \"x$ac_cv_prog_cc_$1\" != \"xno\" && break\ndone\nrm -f conftest.$ac_ext\nCC=$ac_save_CC\n])# AC_CACHE_VAL\ncase \"x$ac_cv_prog_cc_$1\" in\n  x)\n    AC_MSG_RESULT([none needed]) ;;\n  xno)\n    AC_MSG_RESULT([unsupported]) ;;\n  *)\n    CC=\"$CC $ac_cv_prog_cc_$1\"\n    AC_MSG_RESULT([$ac_cv_prog_cc_$1]) ;;\nesac\nAS_IF([test \"x$ac_cv_prog_cc_$1\" != xno], [$5], [$6])\n])# _AC_C_STD_TRY\n\n\n"
        },
        {
          "name": "autogen.sh",
          "type": "blob",
          "size": 0.5615234375,
          "content": "#!/bin/sh\n# Run this to generate all the initial makefiles, etc.\nif which libtoolize > /dev/null; then\n  echo \"Found libtoolize\"\n  libtoolize -c\nelif which glibtoolize > /dev/null; then\n  echo \"Found glibtoolize\"\n  glibtoolize -c\nelse\n  echo \"Failed to find libtoolize or glibtoolize, please ensure it is installed and accessible via your PATH env variable\"\n  exit 1\nfi;\nautoreconf -fv --install || exit 1\nif which cargo > /dev/null; then\n    if [ -f rust/Cargo.lock ] ; then\n        rm -f rust/Cargo.lock\n    fi\nfi;\necho \"You can now run \\\"./configure\\\" and then \\\"make\\\".\"\n"
        },
        {
          "name": "benches",
          "type": "tree",
          "content": null
        },
        {
          "name": "config.rpath",
          "type": "blob",
          "size": 0,
          "content": ""
        },
        {
          "name": "configure.ac",
          "type": "blob",
          "size": 105.0498046875,
          "content": "    AC_INIT([suricata],[8.0.0-dev])\n    m4_ifndef([AM_SILENT_RULES], [m4_define([AM_SILENT_RULES],[])])AM_SILENT_RULES([yes])\n    AC_CONFIG_HEADERS([src/autoconf.h])\n    AC_CONFIG_SRCDIR([src/suricata.c])\n    AC_CONFIG_MACRO_DIR(m4)\n    AM_INIT_AUTOMAKE([tar-ustar subdir-objects])\n\n    AC_LANG([C])\n    LT_INIT\n    PKG_PROG_PKG_CONFIG\n\n    dnl Taken from https://llvm.org/svn/llvm-project/llvm/trunk/autoconf/configure.ac\n    dnl check if we compile using clang or gcc. On some systems the gcc binary is\n    dnl is actually clang, so do a compile test.\n    AC_MSG_CHECKING([whether GCC or Clang is our compiler])\n    AC_LANG_PUSH([C])\n    compiler=unknown\n    AC_COMPILE_IFELSE([AC_LANG_SOURCE([[#if ! __clang__\n                                        #error\n                                        #endif\n                                      ]])],\n                       compiler=clang,\n                      [AC_COMPILE_IFELSE([AC_LANG_SOURCE([[#if ! __GNUC__\n                                                           #error\n                                                           #endif\n                                                         ]])],\n                       compiler=gcc, [])])\n    AC_LANG_POP([C])\n    AC_MSG_RESULT([${compiler}])\n\n    AC_ARG_WITH([clang],\n                [  --with-clang=PROGRAM    path to Clang for compiling eBPF code. Use if the main C compiler is not Clang.],\n                [CLANG=\"$withval\"],\n                [AS_IF([test \"$compiler\" = clang],\n                       [CLANG=\"$CC\"],\n                       [AC_PATH_PROG([CLANG],[clang])])])\n\n    AC_SUBST([CLANG])\n\n    case \"$compiler\" in\n        clang)\n            CLANG_CFLAGS=\"-Wextra -Werror-implicit-function-declaration -Wno-error=unused-command-line-argument\"\n            AC_SUBST(CLANG_CFLAGS)\n            ;;\n        gcc)\n            dnl get gcc version\n            AC_MSG_CHECKING([gcc version])\n                    gccver=$($CC -dumpversion)\n                    gccvermajor=$(echo $gccver | cut -d . -f1)\n                    gccverminor=$(echo $gccver | cut -d . -f2)\n                    gccvernum=$(expr $gccvermajor \"*\" 100 + $gccverminor)\n            AC_MSG_RESULT($gccver)\n\n            if test \"$gccvernum\" -ge \"400\"; then\n                dnl gcc 4.0 or later\n                GCC_CFLAGS=\"-Wextra -Werror-implicit-function-declaration\"\n            else\n                GCC_CFLAGS=\"-W\"\n            fi\n            AC_SUBST(GCC_CFLAGS)\n            ;;\n        *)\n            AC_MSG_WARN([unsupported/untested compiler, this may or may not work])\n            ;;\n    esac\n\n    # Checks for programs.\n    AC_PROG_AWK\n    AC_PROG_CC\n    AC_PROG_CPP\n    AC_PROG_RANLIB\n    AC_PROG_INSTALL\n    AC_PROG_LN_S\n    AC_PROG_MAKE_SET\n    AC_PROG_GREP\n\n    AC_PATH_PROG(HAVE_CYGPATH, cygpath, \"no\")\n    AM_CONDITIONAL([HAVE_CYGPATH], [test \"x$HAVE_CYGPATH\" != \"xno\"])\n\n    AC_PATH_PROG(HAVE_PKG_CONFIG, pkg-config, \"no\")\n    if test \"$HAVE_PKG_CONFIG\" = \"no\"; then\n        echo\n        echo \"   ERROR! pkg-config not found, go get it  \"\n        echo \"   http://pkg-config.freedesktop.org/wiki/ \"\n        echo \"   or install from your distribution       \"\n        echo\n        exit 1\n    fi\n\n    python_path=\"not set\"\n\n    AC_ARG_ENABLE(python,\n           AS_HELP_STRING([--enable-python], [Enable python]),\n       [enable_python=$enableval],[enable_python=yes])\n    if test \"x$enable_python\" != \"xyes\"; then\n        enable_python=\"no\"\n    else\n        AC_PATH_PROGS(HAVE_PYTHON, python3 python2.7 python2 python, \"no\")\n        if test \"$HAVE_PYTHON\" = \"no\"; then\n            echo\n            echo \"   Warning! Python not found.\"\n            echo\n            echo \"   Python is required for additional tools like\"\n            echo \"   suricatasc, suricatactl and suricata-update.\"\n            echo \"   It is also required when building from git.\"\n            echo\n            enable_python=\"no\"\n        else\n            python_path=\"$HAVE_PYTHON\"\n        fi\n    fi\n    AM_CONDITIONAL([HAVE_PYTHON], [test \"x$enable_python\" = \"xyes\"])\n\n    # Get the Python major version. This is only for information\n    # messages displayed during configure.\n    if test \"x$HAVE_PYTHON\" != \"xno\"; then\n        pymv=\"$($HAVE_PYTHON -c 'import sys; print(sys.version_info[[0]]);')\"\n    fi\n\n    AC_PATH_PROG(HAVE_WGET, wget, \"no\")\n    if test \"$HAVE_WGET\" = \"no\"; then\n        AC_PATH_PROG(HAVE_CURL, curl, \"no\")\n        if test \"$HAVE_CURL\" = \"no\"; then\n            echo\n            echo \"   Warning curl or wget not found, you won't be able to\"\n            echo \"   download latest ruleset with 'make install-rules'\"\n        fi\n    fi\n    AM_CONDITIONAL([HAVE_FETCH_COMMAND], [test \"x$HAVE_WGET\" != \"xno\" || test \"x$HAVE_CURL\" != \"xno\"])\n    AM_CONDITIONAL([HAVE_WGET_COMMAND], [test \"x$HAVE_WGET\" != \"xno\"])\n\n    # Checks for libraries.\n\n    # Checks for header files.\n    AC_CHECK_HEADERS([stddef.h])\n    AC_CHECK_HEADERS([arpa/inet.h assert.h ctype.h errno.h fcntl.h inttypes.h])\n    AC_CHECK_HEADERS([getopt.h])\n    AC_CHECK_HEADERS([limits.h netdb.h netinet/in.h poll.h sched.h signal.h])\n    AC_CHECK_HEADERS([stdarg.h stdint.h stdio.h stdlib.h stdbool.h string.h strings.h sys/ioctl.h])\n    AC_CHECK_HEADERS([math.h])\n    AC_CHECK_HEADERS([syslog.h sys/prctl.h sys/socket.h sys/stat.h sys/syscall.h])\n    AC_CHECK_HEADERS([sys/time.h time.h unistd.h sys/param.h])\n    AC_CHECK_HEADERS([sys/ioctl.h linux/if_ether.h linux/if_packet.h linux/filter.h])\n    AC_CHECK_HEADERS([linux/ethtool.h linux/sockios.h])\n    AC_CHECK_HEADERS([glob.h locale.h grp.h pwd.h])\n    AC_CHECK_HEADERS([dirent.h fnmatch.h])\n    AC_CHECK_HEADERS([sys/resource.h sys/types.h sys/un.h])\n    AC_CHECK_HEADERS([sys/random.h])\n    AC_CHECK_HEADERS([utime.h])\n    AC_CHECK_HEADERS([libgen.h])\n    AC_CHECK_HEADERS([mach/mach.h])\n    AC_CHECK_HEADERS([stdatomic.h])\n    AC_CHECK_HEADERS([sys/queue.h])\n\n    AC_CHECK_HEADERS([sys/socket.h net/if.h sys/mman.h linux/if_arp.h], [], [],\n    [[#ifdef HAVE_SYS_SOCKET_H\n        #include <sys/types.h>\n        #include <sys/socket.h>\n        #endif\n    ]])\n\n    AC_CHECK_HEADERS([windows.h winsock2.h ws2tcpip.h w32api/wtypes.h], [], [],\n                    [[\n                        #ifndef _X86_\n                        #define _X86_\n                        #endif\n                     ]])\n    AC_CHECK_HEADERS([w32api/winbase.h wincrypt.h], [], [],\n                    [[\n                        #ifndef _X86_\n                        #define _X86_\n                        #endif\n                        #include <windows.h>\n                     ]])\n\n    # Checks for typedefs, structures, and compiler characteristics.\n    AC_C_INLINE\n    AC_C_RESTRICT\n    AC_TYPE_PID_T\n    AC_TYPE_MODE_T\n    AC_TYPE_SIZE_T\n    AC_TYPE_SSIZE_T\n    AC_TYPE_INT8_T\n    AC_TYPE_INT16_T\n    AC_TYPE_INT32_T\n    AC_TYPE_INT64_T\n    AC_TYPE_UINT8_T\n    AC_TYPE_UINT16_T\n    AC_TYPE_UINT32_T\n    AC_TYPE_UINT64_T\n    AC_TYPE_UINT\n    AC_TYPE_USHORT\n    AC_TYPE_ULONG\n    AC_TYPE_UCHAR\n    AC_STRUCT_TIMEZONE\n    AC_CHECK_TYPES([ptrdiff_t])\n    AC_HEADER_STDBOOL\n\n    # Checks for library functions.\n    AC_FUNC_MALLOC\n    AC_FUNC_REALLOC\n    AC_FUNC_FORK\n    AC_FUNC_MKTIME\n    AC_FUNC_MMAP\n    AC_FUNC_STRTOD\n\n    AC_CHECK_FUNCS([memmem memset memchr memrchr memmove])\n    AC_CHECK_FUNCS([strcasecmp strchr strrchr strdup strndup strncasecmp strtol strtoul strstr strpbrk strtoull strtoumax])\n    AC_CHECK_FUNCS([strerror])\n    AC_CHECK_FUNCS([gethostname inet_ntoa uname])\n    AC_CHECK_FUNCS([gettimeofday clock_gettime utime strptime tzset localtime_r])\n    AC_CHECK_FUNCS([socket setenv select putenv dup2 endgrent endpwent atexit munmap])\n    AC_CHECK_FUNCS([setrlimit setvbuf])\n\n    AC_CHECK_FUNCS([fwrite_unlocked])\n\n    AC_CHECK_DECL([getrandom],\n        AC_DEFINE([HAVE_GETRANDOM], [1], [Use getrandom]),\n        [], [\n            #include <sys/random.h> \n            ])\n    AC_CHECK_DECL([_popcnt64],\n        AC_DEFINE([HAVE_POPCNT64], [1], [Use _popcnt64]),\n        [], [\n            #include <x86intrin.h> \n            ])\n\n    AC_CHECK_HEADERS([malloc.h])\n    AC_CHECK_DECL([malloc_trim],\n        AC_DEFINE([HAVE_MALLOC_TRIM], [1], [Use malloc_trim]),\n        [], [\n            #include <malloc.h> \n            ])\n\n    OCFLAGS=$CFLAGS\n    CFLAGS=\"\"\n    AC_CHECK_FUNCS([strlcpy strlcat])\n    CFLAGS=$OCFLAGS\n\n    # Add large file support\n    AC_SYS_LARGEFILE\n\n    #check for os\n    AC_MSG_CHECKING([host os])\n\n    # If no host os was detected, try with uname\n    if test -z \"$host\" ; then\n        host=\"`uname`\"\n    fi\n    echo -n \"installation for $host OS... \"\n\n    RUST_SURICATA_LIBNAME=\"libsuricata_rust.a\"\n\n    enable_systemd=\"no\"\n    e_magic_file=\"\"\n    e_magic_file_comment=\"#\"\n    case \"$host\" in\n        *-*-*freebsd*)\n            CFLAGS=\"${CFLAGS} -DOS_FREEBSD\"\n            CPPFLAGS=\"${CPPFLAGS} -I/usr/local/include -I/usr/local/include/libnet11\"\n            LDFLAGS=\"${LDFLAGS} -L/usr/local/lib -L/usr/local/lib/libnet11\"\n            RUST_LDADD=\"-lrt -lm\"\n            ;;\n        *-*-openbsd*)\n            CFLAGS=\"${CFLAGS} -D__OpenBSD__\"\n            CPPFLAGS=\"${CPPFLAGS} -I/usr/local/include -I/usr/local/include/libnet-1.1\"\n            LDFLAGS=\"${LDFLAGS} -L/usr/local/lib -I/usr/local/lib/libnet-1.1\"\n            RUST_LDADD=\"-lm -lc++ -lc++abi\"\n            ;;\n        *darwin*|*Darwin*)\n            CFLAGS=\"${CFLAGS} -DOS_DARWIN\"\n            CPPFLAGS=\"${CPPFLAGS} -I/opt/local/include\"\n            LDFLAGS=\"${LDFLAGS} -L/opt/local/lib -framework Security\"\n            ;;\n        *-*-linux*)\n            # Always compile with -fPIC on Linux for shared library support.\n            CFLAGS=\"${CFLAGS} -fPIC -DOS_LINUX\"\n            RUST_LDADD=\"-ldl -lrt -lm\"\n            can_build_shared_library=\"yes\"\n            AC_DEFINE([SYSTEMD_NOTIFY], [1], [make Suricata notify systemd on startup])\n            enable_systemd=\"yes\"\n            ;;\n        *-*-mingw32*|*-*-msys)\n            CFLAGS=\"${CFLAGS} -DOS_WIN32\"\n            WINDOWS_PATH=\"yes\"\n            AC_DEFINE([HAVE_NON_POSIX_MKDIR], [1], [mkdir is not POSIX compliant: single arg])\n            RUST_LDADD=\" -lws2_32 -liphlpapi -lwbemuuid -lOle32 -lOleAut32 -lUuid -luserenv -lshell32 -ladvapi32 -lgcc_eh -lbcrypt -lntdll\"\n            TRY_WPCAP=\"yes\"\n            ;;\n        *-*-cygwin)\n            WINDOWS_PATH=\"yes\"\n            TRY_WPCAP=\"yes\"\n            ;;\n        *-*-solaris*)\n            AC_MSG_WARN([support for Solaris/Illumos/SunOS is experimental])\n            LDFLAGS=\"${LDFLAGS} -lsocket -lnsl\"\n            ;;\n        *)\n            AC_MSG_WARN([unsupported OS this may or may not work])\n            ;;\n    esac\n    AC_MSG_RESULT(ok)\n\n    # check if our target supports c11\n    AC_MSG_CHECKING(for c11 support)\n    OCFLAGS=$CFLAGS\n    CFLAGS=\"-std=c11\"\n    AC_COMPILE_IFELSE([AC_LANG_PROGRAM([[#include <stdlib.h>]],\n                [[ static _Thread_local int i; i = 1; i++; ]])],\n            AC_MSG_RESULT([yes])\n            [AC_DEFINE([TLS_C11], [1], [C11 Thread local storage])\n             CFLAGS=\"$OCFLAGS -std=c11\"],\n            [AC_MSG_RESULT([no])\n             CFLAGS=\"$OCFLAGS\"\n             have_c11=no\n             have_c11_tls=no])\n    if [ test \"x$have_c11\" = \"xno\" ]; then\n        CFLAGS=\"$CFLAGS -std=gnu99\"\n    fi\n\n    # check if our target supports -Wl,--start-group\n    AC_MSG_CHECKING(for -Wl,--start-group support)\n    OLDFLAGS=$LDFLAGS\n    LDFLAGS=\"-Wl,--start-group,--end-group\"\n    AC_LINK_IFELSE([AC_LANG_PROGRAM([[]], [[]])],[\n            have_linker_group_support=yes\n            AC_MSG_RESULT(yes)],\n            [AC_MSG_RESULT(no)])\n    LDFLAGS=$OLDFLAGS\n    AM_CONDITIONAL([LINKER_SUPPORTS_GROUP], [test \"x$have_linker_group_support\" = \"xyes\"])\n\n    # check if our target supports thread local storage\n    AC_MSG_CHECKING(for thread local storage gnu __thread support)\n    AC_COMPILE_IFELSE([AC_LANG_PROGRAM([[#include <stdlib.h>]],\n                [[ static __thread int i; i = 1; i++; ]])],\n            [AC_DEFINE([TLS_GNU], [1], [Thread local storage])\n            AC_MSG_RESULT([yes])],\n            [AC_MSG_RESULT([no])\n             have_gnu_tls=no])\n    if [ test \"x$have_c11_tls\" = \"xno\" ] && [ test \"x$have_gnu_tls\" = \"xno\" ]; then\n        AC_MSG_ERROR(\"no thread local support available.\")\n        exit 1\n    fi\n\n    #Enable support for gcc compile time security options. There is no great way to do detection of valid cflags that I have found\n    #AX_CFLAGS_GCC_OPTION don't seem to do a better job than the code below and are a pain because of extra m4 files etc.\n    #These flags seem to be supported on CentOS 5+, Ubuntu 8.04+, and FedoraCore 11+\n    #Options are taken from https://wiki.ubuntu.com/CompilerFlags\n    AC_ARG_ENABLE(gccprotect,\n           AS_HELP_STRING([--enable-gccprotect], [Detect and use gcc hardening options]),[enable_gccprotect=$enableval],[enable_gccprotect=no])\n\n    AS_IF([test \"x$enable_gccprotect\" = \"xyes\"], [\n        #buffer overflow protection\n        AC_MSG_CHECKING(for -fstack-protector)\n        TMPCFLAGS=\"${CFLAGS}\"\n        CFLAGS=\"${CFLAGS} -fstack-protector\"\n        AC_LINK_IFELSE([AC_LANG_PROGRAM([[]], [[]])],[SECCFLAGS=\"-fstack-protector\"\n            AC_MSG_RESULT(yes)],\n            [AC_MSG_RESULT(no)])\n        CFLAGS=\"${TMPCFLAGS}\"\n\n        #compile-time best-practices errors for certain libc functions, provides checks of buffer lengths and memory regions\n        AC_MSG_CHECKING(for -D_FORTIFY_SOURCE=2)\n        TMPCFLAGS=\"${CFLAGS}\"\n        CFLAGS=\"${CFLAGS} -D_FORTIFY_SOURCE=2\"\n        AC_COMPILE_IFELSE([AC_LANG_PROGRAM([[]], [[]])],[SECCFLAGS=\"${SECCFLAGS} -D_FORTIFY_SOURCE=2\"\n            AC_MSG_RESULT(yes)],\n            [AC_MSG_RESULT(no)])\n        CFLAGS=\"${TMPCFLAGS}\"\n\n        #compile-time warnings about misuse of format strings\n        AC_MSG_CHECKING(for -Wformat -Wformat-security)\n        TMPCFLAGS=\"${CFLAGS}\"\n        CFLAGS=\"${CFLAGS} -Wformat -Wformat-security\"\n        AC_COMPILE_IFELSE([AC_LANG_PROGRAM([[]], [[]])],[SECCFLAGS=\"${SECCFLAGS} -Wformat -Wformat-security\"\n            AC_MSG_RESULT(yes)],\n            [AC_MSG_RESULT(no)])\n        CFLAGS=\"${TMPCFLAGS}\"\n\n        #provides a read-only relocation table area in the final ELF\n        AC_MSG_CHECKING(for -z relro)\n        TMPLDFLAGS=\"${LDFLAGS}\"\n        LDFLAGS=\"${LDFLAGS} -z relro\"\n        AC_LINK_IFELSE([AC_LANG_PROGRAM([[]], [[]])],[SECLDFLAGS=\"${SECLDFLAGS} -z relro\"\n            AC_MSG_RESULT(yes)],\n            [AC_MSG_RESULT(no)])\n        LDFLAGS=\"${TMPLDFLAGS}\"\n\n        #forces all relocations to be resolved at run-time\n        AC_MSG_CHECKING(for -z now)\n        TMPLDFLAGS=\"${LDFLAGS}\"\n        LDFLAGS=\"${LDFLAGS} -z now\"\n        AC_LINK_IFELSE([AC_LANG_PROGRAM([[]], [[]])],[SECLDFLAGS=\"${SECLDFLAGS} -z now\"\n            AC_MSG_RESULT(yes)],\n            [AC_MSG_RESULT(no)])\n        LDFLAGS=\"${TMPLDFLAGS}\"\n\n        AC_SUBST(SECCFLAGS)\n        AC_SUBST(SECLDFLAGS)\n    ])\n\n    #check for Landlock support\n    AC_CHECK_HEADERS([linux/landlock.h])\n    enable_landlock=\"no\"\n    if test \"$ac_cv_header_linux_landlock_h\" = \"yes\"; then\n        enable_landlock=\"yes\"\n    fi\n\n    #check for plugin support\n    AC_CHECK_HEADERS([dlfcn.h])\n    AC_MSG_CHECKING([for plugin support])\n    TMPLDFLAGS=\"${LDFLAGS}\"\n    LDFLAGS=\"${LDFLAGS} -rdynamic\"\n    AC_LINK_IFELSE([AC_LANG_PROGRAM([[#include <dlfcn.h>]], [[]])],\n        [\n            AC_MSG_RESULT(yes)\n            has_rdynamic=yes\n        ],\n        [\n            AC_MSG_RESULT(no)\n            has_rdynamic=no\n        ])\n\n    if test \"x$has_rdynamic\" = \"xyes\"; then\n        plugin_support=yes\n        AC_DEFINE([HAVE_PLUGINS], [1], [Plugin support])\n    else\n        plugin_support=no\n        LDFLAGS=\"${TMPLDFLAGS}\"\n    fi\n\n    #enable profile generation\n    AC_ARG_ENABLE(gccprofile,\n           AS_HELP_STRING([--enable-gccprofile], [Enable gcc profile info i.e -pg flag is set]),[enable_gccprofile=$enableval],[enable_gccprofile=no])\n    AS_IF([test \"x$enable_gccprofile\" = \"xyes\"], [\n        CFLAGS=\"${CFLAGS} -pg\"\n    ])\n\n    #enable gcc march=native gcc 4.2 or later\n    AC_ARG_ENABLE(gccmarch_native,\n           AS_HELP_STRING([--enable-gccmarch-native], [Enable gcc march=native gcc 4.2 and later only]),[enable_gccmarch_native=$enableval],[enable_gccmarch_native=yes])\n    AS_IF([test \"x$enable_gccmarch_native\" = \"xyes\"], [\n        case \"$host\" in\n            *powerpc*)\n                ;;\n            *)\n            OFLAGS=\"$CFLAGS\"\n            CFLAGS=\"$CFLAGS -march=native\"\n            AC_MSG_CHECKING([checking if $CC supports -march=native])\n            AC_COMPILE_IFELSE(  [AC_LANG_PROGRAM([[#include <stdlib.h>]])],\n                        [\n                          AC_MSG_RESULT([yes])\n                          OPTIMIZATION_CFLAGS=\"-march=native\"\n                          AC_SUBST(OPTIMIZATION_CFLAGS)\n                        ],\n                        [\n                          AC_MSG_RESULT([no])\n                          CFLAGS=\"$OFLAGS\"\n                          enable_gccmarch_native=no\n                        ]\n                     )\n                ;;\n        esac\n    ])\n\n# options\n\n\n  # enable the running of unit tests\n    AC_ARG_ENABLE(unittests,\n           AS_HELP_STRING([--enable-unittests], [Enable compilation of the unit tests]),[enable_unittests=$enableval],[enable_unittests=no])\n    AS_IF([test \"x$enable_unittests\" = \"xyes\"], [\n        AC_DEFINE([UNITTESTS],[1],[Enable built-in unittests])\n    ])\n    AM_CONDITIONAL([BUILD_UNITTESTS], [test \"x$enable_unittests\" = \"xyes\"])\n\n  # enable the building of ebpf files \n    AC_ARG_ENABLE(ebpf-build,\n           AS_HELP_STRING([--enable-ebpf-build], [Enable compilation of ebpf files]),[enable_ebpf_build=$enableval],[enable_ebpf_build=no])\n    AM_CONDITIONAL([BUILD_EBPF], [test \"x$enable_ebpf_build\" = \"xyes\"])\n\n    AS_IF([test \"x$enable_ebpf_build\" = \"xyes\"],\n          [\n            AS_IF([test \"$CLANG\" != no],\n                  [\n                    llc_candidates=$($CLANG --version | sed -e 's/.*clang version/clang version/' | \\\n                      awk '/^clang version/ {\n                             split($3, v, \".\");\n                             printf(\"llc-%s.%s llc-%s llc\", v[[1]], v[[2]], v[[1]])\n                           }')\n                    AC_CHECK_PROGS([LLC], [$llc_candidates], \"no\")\n                    if test \"$LLC\" = \"no\"; then\n                        AC_MSG_ERROR([unable to find any of $llc_candidates needed to build ebpf files])\n                    fi\n                    AC_SUBST(LLC)\n                  ],\n                  [AC_MSG_ERROR([clang needed to build ebpf files])])\n            AC_MSG_CHECKING([libbpf has bpf/bpf_helpers.h])\n            AC_COMPILE_IFELSE(\n                [AC_LANG_PROGRAM(\n                    [\n                        #include <stddef.h>\n                        #include <linux/bpf.h>\n                        #include <bpf/bpf_helpers.h>\n                    ],\n                    [\n                    ])],\n                [HAVE_BPF=\"yes\"],\n                [HAVE_BPF=\"no\"])\n            if test \"$HAVE_BPF\" = \"no\"; then\n                AC_MSG_ERROR([libbpf include bpf/bpf_helpers.h not found])\n            else\n                AC_MSG_RESULT([ok])\n            fi\n          ])\n\n  # enable debug output\n    AC_ARG_ENABLE(debug,\n           AS_HELP_STRING([--enable-debug], [Enable debug output]),[enable_debug=$enableval],[enable_debug=no])\n    AS_IF([test \"x$enable_debug\" = \"xyes\"], [\n        AC_DEFINE([DEBUG],[1],[Enable debug output])\n    ])\n    AM_CONDITIONAL([DEBUG], [test \"x$enable_debug\" = \"xyes\"])\n\n  # enable debug validation functions & macro's output\n    AC_ARG_ENABLE(debug-validation,\n           AS_HELP_STRING([--enable-debug-validation], [Enable (debug) validation code output]),[enable_debug_validation=$enableval],[enable_debug_validation=no])\n    AS_IF([test \"x$enable_debug_validation\" = \"xyes\"], [\n        if test \"$enable_unittests\" = \"yes\"; then\n            AC_MSG_ERROR([debug_validation can't be enabled with enabled unittests!])\n        else\n            AC_DEFINE([DEBUG_VALIDATION],[1],[Enable (debug) validation code output])\n        fi\n    ])\n    AM_CONDITIONAL([DEBUG_VALIDATION], [test \"x$enable_debug_validation\" = \"xyes\"])\n\n  # profiling support\n    AC_ARG_ENABLE(profiling,\n           AS_HELP_STRING([--enable-profiling], [Enable performance profiling]),[enable_profiling=$enableval],[enable_profiling=no])\n    AS_IF([test \"x$enable_profiling\" = \"xyes\"], [\n    case \"$host\" in\n        *-*-openbsd*)\n            AC_MSG_ERROR([profiling is not supported on OpenBSD])\n            ;;\n        *)\n            AC_DEFINE([PROFILING],[1],[Enable performance profiling])\n            AC_DEFINE([PROFILE_RULES],[1],[Enable performance profiling for rules])\n            ;;\n    esac\n    ])\n\n  # profiling support, locking\n    AC_ARG_ENABLE(profiling-locks,\n           AS_HELP_STRING([--enable-profiling-locks], [Enable performance profiling for locks]),[enable_profiling_locks=$enableval],[enable_profiling_locks=no])\n    AS_IF([test \"x$enable_profiling_locks\" = \"xyes\"], [\n        AC_DEFINE([PROFILING],[1],[Enable performance profiling])\n        AC_DEFINE([PROFILE_LOCKING],[1],[Enable performance profiling for locks])\n    ])\n\n  # profiling support, rules\n    AC_ARG_ENABLE(profiling-rules,\n           AS_HELP_STRING([--enable-profiling-rules], [Enable performance profiling for rules (enabled by global profiling too)]),[enable_profiling_rules=$enableval],[enable_profiling_rules=no])\n    AS_IF([test \"x$enable_profiling_rules\" = \"xyes\"], [\n        AC_DEFINE([PROFILE_RULES],[1],[Enable performance profiling for rules])\n    ])\n\n  # enable support for IPFW\n    AC_ARG_ENABLE(ipfw,\n            AS_HELP_STRING([--enable-ipfw], [Enable FreeBSD IPFW support for inline IDP]),[enable_ipfw=$enableval],[enable_ipfw=no])\n    AS_IF([test \"x$enable_ipfw\" = \"xyes\"], [\n        AC_DEFINE([IPFW],[1],[Enable FreeBSD IPFW support for inline IDP])\n    ])\n\n    AC_ARG_ENABLE(coccinelle,\n           AS_HELP_STRING([--disable-coccinelle], [Disable coccinelle QA steps during make check]),[enable_coccinelle=\"$enableval\"],[enable_coccinelle=yes])\n    AS_IF([test \"x$enable_coccinelle\" = \"xyes\"], [\n        AC_PATH_PROG(HAVE_COCCINELLE_CONFIG, spatch, \"no\")\n        if test \"$HAVE_COCCINELLE_CONFIG\" = \"no\"; then\n            enable_coccinelle=no\n        fi\n    ])\n    AM_CONDITIONAL([HAVE_COCCINELLE], [test \"x$enable_coccinelle\" != \"xno\"])\n\n  # disable detection\n    AC_ARG_ENABLE(detection,\n           AS_HELP_STRING([--disable-detection], [Disable Detection Modules]), [enable_detection=\"$enableval\"],[enable_detection=yes])\n    AS_IF([test \"x$enable_detection\" = \"xno\"], [\n        AC_DEFINE([HAVE_DETECT_DISABLED], [1], [Detection is disabled])\n    ])\n\n# libraries\n\n  # zlib\n    AC_ARG_WITH(zlib_includes,\n            [  --with-zlib-includes=DIR  zlib include directory],\n            [with_zlib_includes=\"$withval\"],[with_zlib_includes=no])\n    AC_ARG_WITH(zlib_libraries,\n            [  --with-zlib-libraries=DIR    zlib library directory],\n            [with_zlib_libraries=\"$withval\"],[with_zlib_libraries=\"no\"])\n\n    if test \"$with_zlib_includes\" != \"no\"; then\n        CPPFLAGS=\"${CPPFLAGS} -I${with_zlib_includes}\"\n    fi\n\n    AC_CHECK_HEADER(zlib.h, ZLIB=\"yes\",ZLIB=\"no\")\n    if test \"$ZLIB\" = \"yes\"; then\n        if test \"$with_zlib_libraries\" != \"no\"; then\n            LDFLAGS=\"${LDFLAGS}  -L${with_zlib_libraries}\"\n        fi\n\n        # To prevent duping the lib link we reset LIBS after this check. Setting action-if-found to NULL doesn't seem to work\n        # see: http://blog.flameeyes.eu/2008/04/29/i-consider-ac_check_lib-harmful\n        ZLIB=\"\"\n        TMPLIBS=\"${LIBS}\"\n        AC_CHECK_LIB(z,inflate,,ZLIB=\"no\")\n\n        if test \"$ZLIB\" = \"no\"; then\n            echo\n            echo \"   ERROR!  zlib library not found, go get it\"\n            echo \"   Debian/Ubuntu: apt install zlib1g-dev\"\n            echo \"   Fedora: dnf install zlib-devel\"\n            echo \"   CentOS/RHEL: yum install zlib-devel\"\n            echo\n            exit 1\n        fi\n        LIBS=\"${TMPLIBS} -lz\"\n    fi\n\n    AC_ARG_WITH(libpcre2_includes,\n            [  --with-libpcre2-includes=DIR  libpcre2 include directory],\n            [with_libpcre2_includes=\"$withval\"],[with_libpcre2_includes=\"no\"])\n    AC_ARG_WITH(libpcre2_libraries,\n            [  --with-libpcre2-libraries=DIR    libpcre2 library directory],\n            [with_libpcre2_libraries=\"$withval\"],[with_libpcre2_libraries=\"no\"])\n\n    if test \"$with_libpcre2_includes\" != \"no\"; then\n        CPPFLAGS=\"${CPPFLAGS} -I${with_libpcre2_includes}\"\n    fi\n    if test \"$with_libpcre2_libraries\" != \"no\"; then\n        LDFLAGS=\"${LDFLAGS} -L${with_libpcre2_libraries}\"\n    fi\n    PCRE2=\"\"\n    AC_CHECK_LIB(pcre2-8, pcre2_compile_8,,PCRE2=\"no\")\n    if test \"$PCRE2\" = \"no\"; then\n        echo\n        echo \"   ERROR!  pcre2 library not found, go get it\"\n        echo \"   from www.pcre.org. Or from packages:\"\n        echo \"   Debian/Ubuntu: apt install libpcre2-dev\"\n        echo \"   Fedora: dnf install pcre2-devel\"\n        echo \"   CentOS/RHEL: yum install pcre2-devel\"\n        echo\n        exit 1\n    fi\n\n    AC_DEFINE([PCRE2_CODE_UNIT_WIDTH], [8], [Pcre code unit width is 8 bits])\n    AC_MSG_CHECKING(for PCRE2 JIT support)\n    AC_COMPILE_IFELSE([AC_LANG_PROGRAM([[ #include <pcre2.h> ]],\n        [[\n        int jit = 0;\n        pcre2_config(PCRE2_CONFIG_JIT, &jit);\n        ]])],[ pcre2_jit_available=yes ],[ pcre2_jit_available=no ]\n        )\n    if test \"x$pcre2_jit_available\" = \"xyes\"; then\n        AC_MSG_RESULT(yes)\n        AC_DEFINE([PCRE2_HAVE_JIT], [1], [Pcre2 with JIT compiler support enabled])\n    else\n        AC_MSG_RESULT(no)\n    fi\n\n  # libhs\n    enable_hyperscan=\"no\"\n\n    # Try pkg-config first:\n    PKG_CHECK_MODULES([libhs], libhs,, [with_pkgconfig_libhs=no])\n    if test \"$with_pkgconfig_libhs\" != \"no\"; then\n        CPPFLAGS=\"${CPPFLAGS} ${libhs_CFLAGS}\"\n        LIBS=\"${LIBS} ${libhs_LIBS}\"\n    fi\n\n    AC_ARG_WITH(libhs_includes,\n            [  --with-libhs-includes=DIR  libhs include directory],\n            [with_libhs_includes=\"$withval\"],[with_libhs_includes=no])\n    AC_ARG_WITH(libhs_libraries,\n            [  --with-libhs-libraries=DIR    libhs library directory],\n            [with_libhs_libraries=\"$withval\"],[with_libhs_libraries=\"no\"])\n\n    if test \"$with_libhs_includes\" != \"no\"; then\n        CPPFLAGS=\"${CPPFLAGS} -I${with_libhs_includes}\"\n    fi\n    AS_UNSET(ac_cv_header_hs_h)\n    AC_CHECK_HEADER(hs.h,HYPERSCAN=\"yes\",HYPERSCAN=\"no\")\n    if test \"$HYPERSCAN\" = \"yes\"; then\n        if test \"$with_libhs_libraries\" != \"no\"; then\n            LDFLAGS=\"${LDFLAGS}  -L${with_libhs_libraries}\"\n        fi\n\n        AC_CHECK_LIB(hs,hs_compile,,HYPERSCAN=\"no\")\n        AC_CHECK_FUNCS(hs_valid_platform)\n        enable_hyperscan=\"yes\"\n        if test \"$HYPERSCAN\" = \"no\"; then\n            echo\n            echo \"   Hyperscan headers are present, but link test failed.\"\n            echo \"   Check that you have a shared library and C++ linkage available.\"\n            echo\n            enable_hyperscan=\"no\"\n        fi\n    fi\n    AS_IF([test \"x$enable_hyperscan\" = \"xyes\"], [AC_DEFINE([BUILD_HYPERSCAN], [1], [Intel Hyperscan support enabled])])\n\n  # libyaml\n    AC_ARG_WITH(libyaml_includes,\n            [  --with-libyaml-includes=DIR  libyaml include directory],\n            [with_libyaml_includes=\"$withval\"],[with_libyaml_includes=no])\n    AC_ARG_WITH(libyaml_libraries,\n            [  --with-libyaml-libraries=DIR    libyaml library directory],\n            [with_libyaml_libraries=\"$withval\"],[with_libyaml_libraries=\"no\"])\n\n    if test \"$with_libyaml_includes\" != \"no\"; then\n        CPPFLAGS=\"${CPPFLAGS} -I${with_libyaml_includes}\"\n    fi\n\n    AC_CHECK_HEADER(yaml.h,,LIBYAML=\"no\")\n\n    if test \"$with_libyaml_libraries\" != \"no\"; then\n        LDFLAGS=\"${LDFLAGS} -L${with_libyaml_libraries}\"\n    fi\n\n    LIBYAML=\"\"\n    AC_CHECK_LIB(yaml,yaml_parser_initialize,,LIBYAML=\"no\")\n\n    if test \"$LIBYAML\" = \"no\"; then\n        echo\n        echo \"   ERROR!  libyaml library not found, go get it\"\n        echo \"   from http://pyyaml.org/wiki/LibYAML \"\n        echo \"   or your distribution:\"\n        echo\n        echo \"   Ubuntu: apt-get install libyaml-dev\"\n        echo \"   Fedora: dnf install libyaml-devel\"\n        echo \"   CentOS/RHEL: yum install libyaml-devel\"\n        echo\n        exit 1\n    fi\n\n  # libpthread\n    AC_ARG_WITH(libpthread_includes,\n            [  --with-libpthread-includes=DIR  libpthread include directory],\n            [with_libpthread_includes=\"$withval\"],[with_libpthread_includes=no])\n    AC_ARG_WITH(libpthread_libraries,\n            [  --with-libpthread-libraries=DIR    libpthread library directory],\n            [with_libpthread_libraries=\"$withval\"],[with_libpthread_libraries=\"no\"])\n\n    if test \"$with_libpthread_includes\" != \"no\"; then\n        CPPFLAGS=\"${CPPFLAGS} -I${with_libpthread_includes}\"\n    fi\n\n    dnl AC_CHECK_HEADER(pthread.h,,[AC_MSG_ERROR(pthread.h not found ...)])\n\n    if test \"$with_libpthread_libraries\" != \"no\"; then\n        LDFLAGS=\"${LDFLAGS}  -L${with_libpthread_libraries}\"\n    fi\n\n    PTHREAD=\"\"\n    AC_CHECK_LIB(pthread, pthread_create,, PTHREAD=\"no\")\n\n    if test \"$PTHREAD\" = \"no\"; then\n        echo\n        echo \"   ERROR! libpthread library not found, glibc problem?\"\n        echo\n        exit 1\n    fi\n\n    AC_CHECK_FUNCS([pthread_spin_unlock])\n\n    AS_IF([test \"x$enable_debug\" = \"xyes\"], [\n        # Debug only function used for diagnostic display\n        AC_CHECK_FUNCS([pthread_getattr_np])\n    ])\n\n  # libjansson\n    AC_ARG_WITH(libjansson_includes,\n            [  --with-libjansson-includes=DIR  libjansson include directory],\n            [with_libjansson_includes=\"$withval\"],[with_libjansson_includes=no])\n    AC_ARG_WITH(libjansson_libraries,\n            [  --with-libjansson-libraries=DIR    libjansson library directory],\n            [with_libjansson_libraries=\"$withval\"],[with_libjansson_libraries=\"no\"])\n\n    if test \"$with_libjansson_includes\" != \"no\"; then\n        CPPFLAGS=\"${CPPFLAGS} -I${with_libjansson_includes}\"\n    fi\n\n    if test \"$with_libjansson_libraries\" != \"no\"; then\n        LDFLAGS=\"${LDFLAGS}  -L${with_libjansson_libraries}\"\n    fi\n\n    AC_CHECK_HEADER(jansson.h,JANSSON=\"yes\",JANSSON=\"no\")\n    AC_CHECK_LIB(jansson, json_dump_callback,, JANSSON=\"no\")\n\n    if test \"$JANSSON\" = \"no\"; then\n       echo \"\"\n       echo \"    ERROR: Jansson is now required.\"\n       echo \"\"\n       echo \"    Go get it from your distribution or from:\"\n       echo \"      http://www.digip.org/jansson/\"\n       echo \"\"\n       echo \"    Ubuntu/Debian: apt install libjansson-dev\"\n       echo \"    CentOS: yum install jansson-devel\"\n       echo \"    Fedora: dnf install jansson-devel\"\n       echo \"\"\n       exit 1\n    fi\n\n    enable_jansson=\"yes\"\n    enable_unixsocket=\"no\"\n\n    AC_ARG_ENABLE(unix-socket,\n           AS_HELP_STRING([--enable-unix-socket], [Enable unix socket [default=test]]),[enable_unixsocket=\"$enableval\"],[enable_unixsocket=test])\n\n    if test \"$JANSSON\" = \"yes\"; then\n        enable_jansson=\"yes\"\n        if test \"$JANSSON\" = \"no\"; then\n            echo\n            echo \"   Jansson >= 2.2 is required for features like unix socket\"\n            echo \"   Go get it from your distribution or from:\"\n            echo \"   http://www.digip.org/jansson/\"\n            echo \"   Ubuntu: apt-get install libjansson-dev\"\n            echo \"   Fedora: dnf install jansson-devel\"\n            echo \"   CentOS/RHEL: yum install jansson-devel\"\n            echo\n            if test \"x$enable_unixsocket\" = \"xyes\"; then\n                exit 1\n            fi\n            enable_unixsocket=\"no\"\n            enable_jansson=\"no\"\n        else\n            case $host in\n                *-*-mingw32*|*-*-msys*|*-*-cygwin)\n                    enable_unixsocket=\"no\"\n                    ;;\n                *)\n                    if test \"x$enable_unixsocket\" = \"xtest\"; then\n                        enable_unixsocket=\"yes\"\n                    fi\n                    ;;\n            esac\n        fi\n    else\n        if test \"x$enable_unixsocket\" = \"xyes\"; then\n            echo\n            echo \"   Jansson >= 2.2 is required for features like unix socket\"\n            echo \"   Go get it from your distribution or from:\"\n            echo \"     http://www.digip.org/jansson/\"\n            echo \"   Ubuntu: apt-get install libjansson-dev\"\n            echo \"   Fedora: dnf install jansson-devel\"\n            echo \"   CentOS/RHEL: yum install jansson-devel\"\n            echo\n            exit 1\n        fi\n        enable_unixsocket=\"no\"\n    fi\n\n    AS_IF([test \"x$enable_unixsocket\" = \"xyes\"], [AC_DEFINE([BUILD_UNIX_SOCKET], [1], [Unix socket support enabled])])\n    e_enable_evelog=$enable_jansson\n\n    AC_ARG_ENABLE(nflog,\n            AS_HELP_STRING([--enable-nflog],[Enable libnetfilter_log support]),\n                           [ enable_nflog=\"$enableval\"],\n                           [ enable_nflog=\"no\"])\n    AC_ARG_ENABLE(nfqueue,\n           AS_HELP_STRING([--enable-nfqueue], [Enable NFQUEUE support for inline IDP]),[enable_nfqueue=$enableval],[enable_nfqueue=no])\n    if test \"$enable_nfqueue\" != \"no\"; then\n        PKG_CHECK_MODULES([libnetfilter_queue], [libnetfilter_queue], [enable_nfqueue=yes], [enable_nfqueue=no])\n        CPPFLAGS=\"${CPPFLAGS} ${libnetfilter_queue_CFLAGS}\"\n    fi\n\n    if test \"x$enable_nflog\" = \"xyes\" || test  \"x$enable_nfqueue\" = \"xyes\"; then\n  # libnfnetlink\n        case $host in\n        *-*-mingw32*)\n            ;;\n        *)\n            AC_ARG_WITH(libnfnetlink_includes,\n                    [  --with-libnfnetlink-includes=DIR  libnfnetlink include directory],\n                    [with_libnfnetlink_includes=\"$withval\"],[with_libnfnetlink_includes=no])\n            AC_ARG_WITH(libnfnetlink_libraries,\n                    [  --with-libnfnetlink-libraries=DIR    libnfnetlink library directory],\n                    [with_libnfnetlink_libraries=\"$withval\"],[with_libnfnetlink_libraries=\"no\"])\n\n            if test \"$with_libnfnetlink_includes\" != \"no\"; then\n                CPPFLAGS=\"${CPPFLAGS} -I${with_libnfnetlink_includes}\"\n            fi\n\n            if test \"$with_libnfnetlink_libraries\" != \"no\"; then\n                LDFLAGS=\"${LDFLAGS}  -L${with_libnfnetlink_libraries}\"\n            fi\n\n            NFNL=\"\"\n            AC_CHECK_LIB(nfnetlink, nfnl_fd,, NFNL=\"no\")\n\n            if test \"$NFNL\" = \"no\"; then\n                echo\n                echo \"   nfnetlink library not found, go get it\"\n                echo \"   from www.netfilter.org.\"\n                echo \"   we automatically append libnetfilter_queue/ when searching\"\n                echo \"   for headers etc. when the --with-libnfnetlink-includes directive\"\n                echo \"   is used\"\n                echo \"   Ubuntu: apt-get install libnetfilter-queue-dev\"\n                echo \"   Fedora: dnf install libnetfilter_queue-devel\"\n                echo \"   CentOS/RHEL: yum install libnetfilter_queue-devel\"\n                echo\n            fi\n            ;;\n        esac\n    fi\n\n    # enable support for NFQUEUE\n    if test \"x$enable_nfqueue\" = \"xyes\"; then\n        AC_DEFINE_UNQUOTED([NFQ],[1],[Enable Linux Netfilter NFQUEUE support for inline IDP])\n\n      #libnetfilter_queue\n        AC_ARG_WITH(libnetfilter_queue_includes,\n            [  --with-libnetfilter_queue-includes=DIR  libnetfilter_queue include directory],\n            [with_libnetfilter_queue_includes=\"$withval\"],[with_libnetfilter_queue_includes=no])\n        AC_ARG_WITH(libnetfilter_queue_libraries,\n            [  --with-libnetfilter_queue-libraries=DIR    libnetfilter_queue library directory],\n            [with_libnetfilter_queue_libraries=\"$withval\"],[with_libnetfilter_queue_libraries=\"no\"])\n\n        if test \"$with_libnetfilter_queue_includes\" != \"no\"; then\n            CPPFLAGS=\"${CPPFLAGS} -I${with_libnetfilter_queue_includes}\"\n        fi\n\n        AC_CHECK_HEADER(libnetfilter_queue/libnetfilter_queue.h,,\n            [AC_MSG_ERROR(libnetfilter_queue/libnetfilter_queue.h not found ...)],\n            [\n                #define _GNU_SOURCE\n                #include <sys/types.h>\n                #include <stdint.h>\n            ])\n\n        if test \"$with_libnetfilter_queue_libraries\" != \"no\"; then\n            LDFLAGS=\"${LDFLAGS}  -L${with_libnetfilter_queue_libraries}\"\n        fi\n\n        NFQ=\"\"\n        AC_CHECK_LIB(netfilter_queue, nfq_open,, NFQ=\"no\",)\n        AC_CHECK_LIB([netfilter_queue], [nfq_set_queue_maxlen],AC_DEFINE_UNQUOTED([HAVE_NFQ_MAXLEN],[1],[Found queue max length support in netfilter_queue]) ,,[-lnfnetlink])\n        AC_CHECK_LIB([netfilter_queue], [nfq_set_verdict2],AC_DEFINE_UNQUOTED([HAVE_NFQ_SET_VERDICT2],[1],[Found nfq_set_verdict2 function in netfilter_queue]) ,,[-lnfnetlink])\n        AC_CHECK_LIB([netfilter_queue], [nfq_set_queue_flags],AC_DEFINE_UNQUOTED([HAVE_NFQ_SET_QUEUE_FLAGS],[1],[Found nfq_set_queue_flags function in netfilter_queue]) ,,[-lnfnetlink])\n        AC_CHECK_LIB([netfilter_queue], [nfq_set_verdict_batch],AC_DEFINE_UNQUOTED([HAVE_NFQ_SET_VERDICT_BATCH],[1],[Found nfq_set_verdict_batch function in netfilter_queue]) ,,[-lnfnetlink])\n\n        # check if the argument to nfq_get_payload is signed or unsigned\n        AC_MSG_CHECKING([for signed nfq_get_payload payload argument])\n        STORECFLAGS=\"${CFLAGS}\"\n        if test `basename $CC` = \"clang\"; then\n            CFLAGS=\"${CFLAGS} -Werror=incompatible-pointer-types\"\n        else\n            CFLAGS=\"${CFLAGS} -Werror\"\n        fi\n        AC_COMPILE_IFELSE(\n            [AC_LANG_PROGRAM(\n                [\n                    #define _GNU_SOURCE\n                    #include <sys/types.h>\n                    #include <stdint.h>\n                    #include <stdio.h>\n                    #include <libnetfilter_queue/libnetfilter_queue.h>\n                ],\n                [\n                    char *pktdata;\n                    nfq_get_payload(NULL, &pktdata);\n                ])],\n            [libnetfilter_queue_nfq_get_payload_signed=\"yes\"],\n            [libnetfilter_queue_nfq_get_payload_signed=\"no\"])\n        AC_MSG_RESULT($libnetfilter_queue_nfq_get_payload_signed)\n        if test \"x$libnetfilter_queue_nfq_get_payload_signed\" = \"xyes\"; then\n            AC_DEFINE([NFQ_GET_PAYLOAD_SIGNED], [1], [For signed version of nfq_get_payload])\n        fi\n        CFLAGS=\"${STORECFLAGS}\"\n\n        if test \"$NFQ\" = \"no\"; then\n            echo\n            echo \"   ERROR!  libnetfilter_queue library not found, go get it\"\n            echo \"   from www.netfilter.org.\"\n            echo \"   we automatically append libnetfilter_queue/ when searching\"\n            echo \"   for headers etc. when the --with-libnfq-includes directive\"\n            echo \"   is used\"\n            echo \"   Ubuntu: apt-get install libnetfilter-queue-dev\"\n            echo \"   Fedora: dnf install libnetfilter_queue-devel\"\n            echo \"   CentOS/RHEL: yum install libnetfilter_queue-devel\"\n            echo\n            exit 1\n        fi\n    fi\n\n  # libnetfilter_log\n    AC_ARG_WITH(libnetfilter_log_includes,\n            [  --with-libnetfilter_log-includes=DIR  libnetfilter_log include directory],\n            [with_libnetfilter_log_includes=\"$withval\"],[with_libnetfilter_log_includes=\"no\"])\n    AC_ARG_WITH(libnetfilter_log_libraries,\n            [  --with-libnetfilter_log-libraries=DIR    libnetfilter_log library directory],\n            [with_libnetfilter_log_libraries=\"$withval\"],[with_libnetfilter_log_libraries=\"no\"])\n\n    if test \"$enable_nflog\" = \"yes\"; then\n        if test \"$with_libnetfilter_log_includes\" != \"no\"; then\n            CPPFLAGS=\"${CPPFLAGS} -I${with_libnetfilter_log_includes}\"\n        fi\n\n        AC_CHECK_HEADER(libnetfilter_log/libnetfilter_log.h,,[AC_MSG_ERROR(libnetfilter_log.h not found ...)])\n\n        if test \"$with_libnetfilter_log_libraries\" != \"no\"; then\n            LDFLAGS=\"${LDFLAGS}  -L${with_libnetfilter_log_libraries}\"\n        fi\n\n        NFLOG=\"\"\n        AC_CHECK_LIB(netfilter_log, nflog_open,, NFLOG=\"no\")\n\n        if test \"$NFLOG\" = \"no\"; then\n            echo\n            echo \"   ERROR!  libnetfilter_log library not found, go get it\"\n            echo \"   from http://www.netfilter.org.\"\n            echo\n            exit 1\n        else\n            AC_DEFINE([HAVE_NFLOG],[1],[nflog available])\n            enable_nflog=\"yes\"\n        fi\n    fi\n\n  # WinDivert support\n    AC_ARG_ENABLE(windivert,\n        AS_HELP_STRING([--enable-windivert],[Enable WinDivert support [default=no]]),[enable_windivert=$enableval],\n        [enable_windivert=\"no\"])\n    \n  # WinDivert can only be enabled on Windows builds\n    AC_CHECK_DECL([OS_WIN32],,[enable_windivert=\"no\"])\n\n    if test \"x$enable_windivert\" = \"xyes\"; then\n        # WinDivert requires Vista at a minimum. If the user has selected their own NTDDI_VERSION\n        # then don't override it.\n        AC_CHECK_DECL([NTDDI_VERSION],,\n                [CFLAGS=\"${CFLAGS} -DNTDDI_VERSION=NTDDI_VISTA -D_WIN32_WINNT=_WIN32_WINNT_VISTA\"])\n\n        AC_DEFINE_UNQUOTED([WINDIVERT],[1],[Enable Windows WinDivert support for inline IDP])\n\n        AC_ARG_WITH(windivert_include,\n                [  --with-windivert-include=DIR WinDivert include path],\n                [with_windivert_include=\"$withval\"],[with_windivert_include=\"no\"])\n        AC_ARG_WITH(windivert_libraries,\n                [  --with-windivert-libraries=DIR WinDivert library path],\n                [with_windivert_libraries=\"$withval\"],[with_windivert_libraries=\"no\"])\n\n        if test \"$with_windivert_include\" != \"no\"; then\n            CPPFLAGS=\"${CPPFLAGS} -I${with_windivert_include}\"\n        fi\n\n        if test \"$with_windivert_libraries\" != \"no\"; then\n            LDFLAGS=\"${LDFLAGS} -L${with_windivert_libraries}\"\n        fi\n\n        AC_CHECK_HEADER(windivert.h,,WINDIVERT_INC=\"no\")\n        AC_CHECK_LIB(WinDivert, WinDivertOpen,, WINDIVERT_LIB=\"no\")\n\n        if test \"$WINDIVERT_LIB\" = \"no\" || test \"$WINDIVERT_INC\" = \"no\"; then\n            echo\n            echo \"    ERROR! WinDivert not found, go get it from\"\n            echo \"    https://www.reqrypt.org/windivert.html\"\n            echo\n            exit 1\n        fi\n    fi\n  # /WinDivert\n\n\n  # libnet\n    AC_ARG_WITH(libnet_includes,\n            [  --with-libnet-includes=DIR     libnet include directory],\n            [with_libnet_includes=\"$withval\"],[with_libnet_includes=\"no\"])\n\n    AC_ARG_WITH(libnet_libraries,\n            [  --with-libnet-libraries=DIR    libnet library directory],\n            [with_libnet_libraries=\"$withval\"],[with_libnet_libraries=\"no\"])\n\n    if test \"x$with_libnet_includes\" != \"xno\"; then\n        CPPFLAGS=\"${CPPFLAGS} -I${with_libnet_includes}\"\n        libnet_dir=\"${with_libnet_includes}\"\n    else\n        libnet_dir=\"/usr/include /usr/local/include /usr/local/include/libnet11 /opt/local/include /usr/local/include/libnet-1.1\"\n    fi\n\n    if test \"x$with_libnet_libraries\" != \"xno\"; then\n        LDFLAGS=\"${LDFLAGS} -L${with_libnet_libraries}\"\n    fi\n\n    LIBNET_DETECT_FAIL=\"no\"\n    LIBNET_INC_DIR=\"\"\n\n    for i in $libnet_dir; do\n    if test -r \"$i/libnet.h\"; then\n        LIBNET_INC_DIR=\"$i\"\n    fi\n    done\n\n    enable_libnet=\"no\"\n    AC_MSG_CHECKING(for libnet.h version 1.1.x)\n    if test \"$LIBNET_INC_DIR\" != \"\"; then\n        LIBNET_VER=`grep LIBNET_VERSION $LIBNET_INC_DIR/libnet.h | grep '1.[[12]]' | sed 's/[[^\"]]*\"\\([[^\"]]*\\).*/\\1/'`\n\n        if test -z \"$LIBNET_VER\" ; then\n            AC_MSG_RESULT(no)\n        else\n            AC_MSG_RESULT(yes)\n        fi\n\n        #CentOS, Fedora, Ubuntu-LTS, Ubuntu all set defines to the same values. libnet-config seems\n        #to have been depreciated but all distro's seem to include it as part of the package.\n        if test \"$LIBNET_DETECT_FAIL\" = \"no\"; then\n            LLIBNET=\"\"\n            AC_CHECK_LIB(net, libnet_write,, LLIBNET=\"no\")\n            if test \"$LLIBNET\" != \"no\"; then\n                AC_DEFINE([HAVE_LIBNET11],[1],(libnet 1.1 available))\n                AC_DEFINE([_DEFAULT_SOURCE],[1],(default source))\n                AC_DEFINE([_BSD_SOURCE],[1],(bsd source))\n                AC_DEFINE([__BSD_SOURCE],[1],(bsd source))\n                AC_DEFINE([__FAVOR_BSD],[1],(favor bsd))\n                AC_DEFINE([HAVE_NET_ETHERNET_H],[1],(ethernet.h))\n                enable_libnet=\"yes\"\n            fi\n\n            # see if we have the patched libnet 1.1\n            # https://www.inliniac.net/blog/2007/10/16/libnet-11-ipv6-fixes-and-additions.html\n            #\n            # To prevent duping the lib link we reset LIBS after this check. Setting action-if-found to NULL doesn't seem to work\n            # see: http://blog.flameeyes.eu/2008/04/29/i-consider-ac_check_lib-harmful\n            if test \"$enable_libnet\" = \"yes\"; then\n                LLIBNET=\"\"\n                TMPLIBS=\"${LIBS}\"\n                AC_CHECK_LIB(net, libnet_build_icmpv6_unreach,, LLIBNET=\"no\")\n                if test \"$LLIBNET\" != \"no\"; then\n                    AC_DEFINE([HAVE_LIBNET_ICMPV6_UNREACH],[1],(libnet_build_icmpv6_unreach available))\n                fi\n                LIBS=\"${TMPLIBS}\"\n            fi\n\n            # See if we have libnet 1.1.6 or newer - these versions handle capabilities correctly\n            # Some patched 1.1.4 versions are also good, but it's not guaranteed for all distros.\n            #\n            # Details: https://bugzilla.redhat.com/show_bug.cgi?id=589770\n            AS_VERSION_COMPARE([LIBNET_VER], [1.1.6],\n                [],\n                [AC_DEFINE([HAVE_LIBNET_CAPABILITIES],[1], (libnet_have_capabilities_patch))],\n                [AC_DEFINE([HAVE_LIBNET_CAPABILITIES],[1], (libnet_have_capabilities_patch))])\n\n\n            # check if the argument to libnet_init is char* or const char*\n            AC_MSG_CHECKING([libnet_init dev type])\n            STORECFLAGS=\"${CFLAGS}\"\n            if test `basename $CC` = \"clang\"; then\n                CFLAGS=\"${CFLAGS} -Werror=incompatible-pointer-types\"\n            else\n                CFLAGS=\"${CFLAGS} -Werror\"\n            fi\n            AC_COMPILE_IFELSE(\n                [AC_LANG_PROGRAM(\n                    [\n                    #include <stdio.h>\n                    #include <libnet.h>\n                    ],\n                    [[\n                    const char dev[32] = \"\";\n                    char ebuf[LIBNET_ERRBUF_SIZE];\n                    (void)libnet_init(LIBNET_LINK, dev, ebuf);\n                    ]])],\n                [libnet_init_const=\"yes\"],\n                [libnet_init_const=\"no\"])\n            AC_MSG_RESULT($libnet_init_const)\n            if test \"x$libnet_init_const\" = \"xyes\"; then\n                AC_DEFINE([HAVE_LIBNET_INIT_CONST], [1], [libnet_init takes const argument])\n            fi\n            CFLAGS=\"${STORECFLAGS}\"\n        fi\n    else\n        AC_MSG_RESULT(no)\n    fi\n\n  # libpcap\n    AC_ARG_WITH(libpcap_includes,\n            [  --with-libpcap-includes=DIR  libpcap include directory],\n            [with_libpcap_includes=\"$withval\"],[with_libpcap_includes=no])\n    AC_ARG_WITH(libpcap_libraries,\n            [  --with-libpcap-libraries=DIR    libpcap library directory],\n            [with_libpcap_libraries=\"$withval\"],[with_libpcap_libraries=\"no\"])\n\n    if test \"$with_libpcap_includes\" != \"no\"; then\n        CPPFLAGS=\"${CPPFLAGS} -I${with_libpcap_includes}\"\n    fi\n\n    AC_CHECK_HEADERS([pcap.h],[],[AC_MSG_ERROR(pcap.h not found ...)],\n            [[\n                #ifdef HAVE_WINSOCK2_H\n                #include <winsock2.h>\n                #endif\n                #define _DEFAULT_SOURCE 1\n            ]])\n\n    if test \"$with_libpcap_libraries\" != \"no\"; then\n        LDFLAGS=\"${LDFLAGS}  -L${with_libpcap_libraries}\"\n    fi\n    AC_CHECK_HEADERS([pcap.h pcap/pcap.h pcap/bpf.h],[],[],\n            [[\n                #ifdef HAVE_WINSOCK2_H\n                #include <winsock2.h>\n                #endif\n                #define _DEFAULT_SOURCE 1\n            ]])\n\n    have_wpcap=\"\"\n    if test \"$TRY_WPCAP\" = \"yes\"; then\n        AC_CHECK_LIB(wpcap, pcap_activate, [], have_wpcap=\"no\")\n        if test \"$have_wpcap\" = \"no\"; then\n            echo \"\"\n            echo \"    Warning: NPCap was not found. Live capture will not be available.\"\n            echo \"\"\n        else\n            PCAP_LIB_NAME=\"wpcap\"\n            have_wpcap=\"yes\"\n        fi\n    fi\n\n    if test \"$have_wpcap\" != \"yes\"; then\n        AC_CHECK_LIB(pcap, pcap_open_dead, [], [\n            echo\n            echo \"   ERROR!  libpcap library not found, go get it\"\n            echo \"   from http://www.tcpdump.org or your distribution:\"\n            echo\n            echo \"   Ubuntu: apt-get install libpcap-dev\"\n            echo \"   Fedora: dnf install libpcap-devel\"\n            echo \"   CentOS/RHEL: yum install libpcap-devel\"\n            echo\n            exit 1\n        ])\n        PCAP_LIB_NAME=\"pcap\"\n    fi\n\n    PKG_CHECK_MODULES([PCAP],libpcap,[CPPFLAGS=\"${CPPFLAGS} ${PCAP_CFLAGS}\" LIBS=\"${LIBS} ${PCAP_LIBS}\"],[:])\n\n    AC_PATH_PROG(HAVE_PCAP_CONFIG, pcap-config, \"no\")\n    if test \"$HAVE_PCAP_CONFIG\" = \"no\" -o \"$cross_compiling\" = \"yes\"; then\n        AC_MSG_RESULT(no pcap-config is use)\n    else\n        PCAP_CFLAGS=\"$(pcap-config --defines) $(pcap-config --cflags)\"\n        AC_SUBST(PCAP_CFLAGS)\n    fi\n\n    #Appears as if pcap_set_buffer_size is linux only?\n    LIBPCAPSBUFF=\"\"\n    #To prevent duping the lib link we reset LIBS after this check. Setting action-if-found to NULL doesn't seem to work\n    #see: http://blog.flameeyes.eu/2008/04/29/i-consider-ac_check_lib-harmful\n    TMPLIBS=\"${LIBS}\"\n    AC_CHECK_LIB(${PCAP_LIB_NAME}, pcap_set_buffer_size,, LPCAPSBUFF=\"no\")\n    if test \"$LPCAPSBUFF\" != \"no\"; then\n        AC_DEFINE([HAVE_PCAP_SET_BUFF],[1],(libpcap has pcap_set_buffer_size function))\n    fi\n    LIBS=\"${TMPLIBS}\"\n\n  # libpfring\n    # libpfring (currently only supported for libpcap enabled pfring)\n    # Error on the side of caution. If libpfring enabled pcap is being used and we don't link against -lpfring compilation will fail.\n    AC_ARG_ENABLE(pfring,\n           AS_HELP_STRING([--enable-pfring], [Enable Native PF_RING support]),[enable_pfring=$enableval],[enable_pfring=no])\n    AS_IF([test \"x$enable_pfring\" = \"xyes\"], [\n        if test \"x$enable_shared\" = \"xno\"; then\n            echo\n            echo \"   ERROR! pfring cannot be enabled with --disable-shared\"\n            echo\n            exit 1\n        fi\n\n        AC_DEFINE([HAVE_PFRING],[1],(PF_RING support enabled))\n\n        #We have to set CFLAGS for AC_COMPILE_IFELSE as it doesn't pay attention to CPPFLAGS\n        AC_ARG_WITH(libpfring_includes,\n                [  --with-libpfring-includes=DIR  libpfring include directory],\n                [with_libpfring_includes=\"$withval\"],[with_libpfring_includes=no])\n        AC_ARG_WITH(libpfring_libraries,\n                [  --with-libpfring-libraries=DIR    libpfring library directory],\n                [with_libpfring_libraries=\"$withval\"],[with_libpfring_libraries=\"no\"])\n\n        if test \"$with_libpfring_includes\" != \"no\"; then\n            CPPFLAGS=\"${CPPFLAGS} -I${with_libpfring_includes}\"\n        fi\n\n        if test \"$with_libpfring_libraries\" != \"no\"; then\n            LDFLAGS=\"${LDFLAGS}  -L${with_libpfring_libraries}\"\n        fi\n\n        LIBPFRING=\"\"\n        AC_CHECK_LIB(pfring, pfring_open,, LIBPFRING=\"no\", [-lpcap])\n        if test \"$LIBPFRING\" != \"no\"; then\n            STORECFLAGS=\"${CFLAGS}\"\n            CFLAGS=\"${CFLAGS} -Werror\"\n            AC_COMPILE_IFELSE(\n                [AC_LANG_PROGRAM(\n                    [\n                    #include <pfring.h>\n                    ],\n                    [\n                    pfring_recv_chunk(NULL, NULL, 0, 0);\n                    ])],\n                [pfring_recv_chunk=\"yes\"],\n                [pfring_recv_chunk=\"no\"])\n            CFLAGS=\"${STORECFLAGS}\"\n            if test \"x$pfring_recv_chunk\" != \"xyes\"; then\n                if test \"x$enable_pfring\" = \"xyes\"; then\n                    echo\n                    echo \"   ERROR! --enable-pfring was passed but the library version is < 6, go get it\"\n                    echo \"   from http://www.ntop.org/products/pf_ring/\"\n                    echo\n                    exit 1\n                fi\n            fi\n            AC_COMPILE_IFELSE(\n                [AC_LANG_SOURCE([[\n                    #include <pfring.h>\n                    #ifndef PF_RING_FLOW_OFFLOAD\n                    # error PF_RING_FLOW_OFFLOAD not defined\n                    #endif\n                ]])],\n                [\n                    AC_DEFINE([HAVE_PF_RING_FLOW_OFFLOAD], [1], [PF_RING bypass support enabled])\n                ],\n                [\n                    echo\n                    echo \"   Warning! Pfring hw bypass not supported by this library version < 7,\"\n                    echo \"   please upgrade to a newer version to use this feature.\"\n                    echo\n                    echo \"   Continuing for now with hw bypass support disabled...\"\n                    echo\n                ])\n        else\n            if test \"x$enable_pfring\" = \"xyes\"; then\n                echo\n                echo \"   ERROR! --enable-pfring was passed but the library was not found, go get it\"\n                echo \"   from http://www.ntop.org/products/pf_ring/\"\n                echo\n                exit 1\n            fi\n        fi\n    ])\n\n    if test \"x$enable_pfring\" = \"xyes\"; then\n        AM_CONDITIONAL([BUILD_PFRING], [true])\n        pfring_comment=\"\"\n    else\n        AM_CONDITIONAL([BUILD_PFRING], [false])\n        pfring_comment=\"#\"\n    fi\n    AC_SUBST([pfring_comment])\n\n  # AF_PACKET support\n    AC_ARG_ENABLE(af-packet,\n           AS_HELP_STRING([--enable-af-packet], [Enable AF_PACKET support [default=yes]]),\n                        [enable_af_packet=$enableval],[enable_af_packet=yes])\n    AS_IF([test \"x$enable_af_packet\" = \"xyes\"], [\n        AC_CHECK_DECL([TPACKET_V2],\n            AC_DEFINE([HAVE_AF_PACKET],[1],[AF_PACKET support is available]),\n            [enable_af_packet=\"no\"],\n            [[#include <sys/socket.h>\n              #include <linux/if_packet.h>]])\n        AC_CHECK_DECL([PACKET_FANOUT_QM],\n            AC_DEFINE([HAVE_PACKET_FANOUT],[1],[Recent packet fanout support is available]),\n            [],\n            [[#include <linux/if_packet.h>]])\n        AC_CHECK_DECL([TPACKET_V3],\n            AC_DEFINE([HAVE_TPACKET_V3],[1],[AF_PACKET tpcket_v3 support is available]),\n            [],\n            [[#include <sys/socket.h>\n              #include <linux/if_packet.h>]])\n        AC_CHECK_DECL([SOF_TIMESTAMPING_RAW_HARDWARE],\n            AC_DEFINE([HAVE_HW_TIMESTAMPING],[1],[Hardware timestamping support is available]),\n            [],\n            [[#include <linux/net_tstamp.h>]])\n    ])\n\n  # AF_XDP support\n    AC_ARG_ENABLE(af-xdp,\n           AS_HELP_STRING([--disable-af-xdp], [Disable AF_XDP support [default=enabled]]),\n                        [enable_af_xdp=$enableval],[enable_af_xdp=yes])\n\n    AS_IF([test \"x$enable_af_xdp\" = \"xyes\"], [\n        # Check for the availability of elf\n        AC_CHECK_LIB(elf,elf_begin,,[enable_af_xdp=no])\n\n        # Conditionally check headers, only when found will it 'continue'\n        AS_IF([test \"x$enable_af_xdp\" = \"xyes\"],\n            # Check for the availability of libxdp\n            AC_CHECK_HEADERS([xdp/xsk.h],,[enable_af_xdp=no])\n            AC_CHECK_LIB([xdp],[xsk_umem__create],,[enable_af_xdp=no]))\n\n        AS_IF([test \"x$enable_af_xdp\" = \"xyes\"],\n            # Check for the availability of libbpf\n            AC_CHECK_HEADERS([bpf/libbpf.h],,[enable_af_xdp=no])\n            AC_CHECK_LIB([bpf],[bpf_object__open],,[enable_af_xdp=no]))\n\n        # Are all required libs installed, yes=HAVE_AF_XDP\n        AS_IF([test \"x$enable_af_xdp\" = \"xyes\"],\n            AC_DEFINE([HAVE_AF_XDP],[1],[AF_XDP support is available]))\n\n        # bpf_get_link_xpd_id has been removed in the most recent\n        # versions of libbpf.\n        AC_CHECK_FUNCS([bpf_xdp_query_id])\n    ])\n\n  # DPDK support\n    enable_dpdk_bond_pmd=\"no\"\n    AC_ARG_ENABLE(dpdk,\n            AS_HELP_STRING([--enable-dpdk], [Enable DPDK support [default=no]]),\n                        [enable_dpdk=$enableval],[enable_dpdk=no])\n    AS_IF([test \"x$enable_dpdk\" = \"xyes\"], [\n        AC_CHECK_LIB(numa, numa_available,, [numa_found=\"no\"])\n        if test \"$numa_found\" = \"no\"; then\n            echo\n            echo \"  ERROR! libnuma not found by pkg-config, go get it\"\n            echo \"      from http://github.com/numactl/numactl or your distribution:\"\n            echo \"          Ubuntu: apt-get install libnuma-dev\"\n            echo \"          Fedora: dnf install numactl-devel\"\n            echo \"          CentOS/RHEL: yum install numactl-devel\"\n            echo\n            exit 1\n        fi\n\n        AC_DEFINE([HAVE_DPDK],[1],(DPDK support enabled))\n        PKG_CHECK_EXISTS(libdpdk >= 19.11, , [with_pkgconfig_libdpdk=no])\n        if test \"$with_pkgconfig_libdpdk\" = \"no\"; then\n            echo\n            echo \"   ERROR! libdpdk >= 19.11 not found by pkg-config, go get it\"\n            echo \"   from https://www.dpdk.org/ or your distribution:\"\n            echo\n            echo \"   Ubuntu: apt-get install dpdk-dev\"\n            echo \"   Fedora: dnf install dpdk-devel\"\n            echo \"   CentOS/RHEL: yum install dpdk-devel\"\n            echo\n            exit 1\n        fi\n        CFLAGS=\"${CFLAGS} `pkg-config --cflags libdpdk`\"\n        LIBS=\"${LIBS} -lnuma `pkg-config --libs libdpdk`\"\n\n        if test ! -z \"$(ldconfig -p | grep librte_net_bond)\"; then\n            AC_DEFINE([HAVE_DPDK_BOND],[1],(DPDK Bond PMD support enabled))\n            enable_dpdk_bond_pmd=\"yes\"\n            LIBS=\"${LIBS} -lrte_net_bond\" # 20.11+\n        elif test ! -z \"$(ldconfig -p | grep librte_pmd_bond)\"; then\n            AC_DEFINE([HAVE_DPDK_BOND],[1],(DPDK Bond PMD support enabled))\n            enable_dpdk_bond_pmd=\"yes\"\n            LIBS=\"${LIBS} -lrte_pmd_bond\"\n        else\n            echo\n            echo \"  WARNING: DPDK Bond PMD was not found on your system, \"\n            echo \"  you will be unable to use DPDK Bond PMD.\"\n            echo \"  You can try to \\\"sudo ldconfig\\\" and reconfigure again\"\n            echo \"  or compile and install DPDK with Bond support enabled.\"\n            echo\n        fi\n    ])\n\n  # Netmap support\n    AC_ARG_ENABLE(netmap,\n            AS_HELP_STRING([--enable-netmap], [Enable Netmap support]),[enable_netmap=$enableval],[enable_netmap=no])\n    AC_ARG_WITH(netmap_includes,\n            [  --with-netmap-includes=DIR netmap include directory],\n            [with_netmap_includes=\"$withval\"],[with_netmap_includes=no])\n    AC_ARG_WITH(netmap_libraries,\n            [  --with-netmap-libraries=DIR netmap library directory],\n            [with_netmap_libraries=\"$withval\"],[with_netmap_libraries=no])\n\n    AS_IF([test \"x$enable_netmap\" = \"xyes\"], [\n        AC_DEFINE([HAVE_NETMAP],[1],(NETMAP support enabled))\n\n        if test \"$with_netmap_includes\" != \"no\"; then\n            CPPFLAGS=\"${CPPFLAGS} -I${with_netmap_includes}\"\n        fi\n\n        if test \"$with_netmap_libraries\" != \"no\"; then\n            LDFLAGS=\"${LDFLAGS} -L${with_netmap_libraries}\"\n        fi\n\n        AC_CHECK_HEADER(net/netmap_user.h,,[AC_MSG_ERROR(net/netmap_user.h not found ...)],)\n\n        have_recent_netmap=\"no\"\n        AC_COMPILE_IFELSE([AC_LANG_PROGRAM([\n                #include <net/netmap_user.h>\n            ],[\n                #ifndef NETMAP_API\n                #error \"Outdated netmap, need one with NETMAP_API\"\n                #endif\n                #if NETMAP_API < 14\n                #error \"Outdated netmap, need at least API version 14\"\n                #endif\n            ])], [have_recent_netmap=\"yes\"])\n        if test \"x$have_recent_netmap\" != \"xyes\"; then\n            echo \"ERROR: outdated netmap; need at least v14\"\n            exit 1\n        fi\n        have_netmap_version=\"v14+\"\n        AC_CHECK_HEADER(libnetmap.h,,[AC_MSG_ERROR(libnetmap.h not found ...)],)\n        LIBNETMAP=\"\"\n        AC_SEARCH_LIBS([nmport_open],[netmap],,[LIBNETMAP=\"no\"])\n        if test \"$LIBNETMAP\" = \"no\"; then\n            echo\n            echo \"   ERROR!  libnetmap library not found!\"\n            echo \"   Go get it from https://github.com/luigirizzo/netmap\"\n            echo \"   or your distribution.\"\n            echo\n            exit 1\n        fi\n        AC_DEFINE([HAVE_NETMAP_V14],[1],(NETMAP API v14 support enabled))\n  ])\n\n  # Suricata-Update.\n    AC_ARG_ENABLE([suricata-update], AS_HELP_STRING([--disable-suricata-update],\n        [Disable suricata-update]), [enable_suricata_update=$enableval],\n      [enable_suricata_update=\"yes\"])\n\n    # Assume suricata-update will not be installed.\n    have_suricata_update=\"no\"\n\n    if test \"$enable_suricata_update\" = \"yes\"; then\n        if test -f \"$srcdir/suricata-update/setup.py\"; then\n            have_suricata_update=\"yes\"\n        fi\n    fi\n\n    if test \"$have_suricata_update\" = \"yes\"; then\n        if test \"$enable_python\" != \"yes\"; then\n            echo \"\"\n            echo \"    Warning: suricata-update will not be installed as\"\n            echo \"        Python is not installed.\"\n            echo \"\"\n            echo\n        else\n            SURICATA_UPDATE_DIR=\"suricata-update\"\n            AC_SUBST(SURICATA_UPDATE_DIR)\n            AC_CONFIG_FILES(suricata-update/Makefile)\n            AC_OUTPUT\n        fi\n    fi\n\n    # Test to see if suricatactl (and suricatasc) can be installed.\n    if test \"x$enable_python\" != \"xyes\"; then\n        install_suricatactl=\"requires python\"\n    else\n        install_suricatactl=\"yes\"\n    fi\n\n    # Test to see if suricata-update can be installed.\n    if test \"x$have_suricata_update\" != \"xyes\"; then\n        install_suricata_update=\"no, \"\n        install_suricata_update_reason=\"not bundled\"\n    elif test \"x$enable_python\" != \"xyes\"; then\n        install_suricata_update=\"no, \"\n        install_suricata_update_reason=\"requires python\"\n    else\n        install_suricata_update=\"yes\"\n    fi\n\n    AM_CONDITIONAL([INSTALL_SURICATA_UPDATE],\n        [test \"x$install_suricata_update\" = \"xyes\"])\n    AC_SUBST([install_suricata_update_reason])\n\n  # libhtp\n    AC_ARG_ENABLE(non-bundled-htp,\n           AS_HELP_STRING([--enable-non-bundled-htp], [Enable the use of an already installed version of htp]),[enable_non_bundled_htp=$enableval],[enable_non_bundled_htp=no])\n    AS_IF([test \"x$enable_non_bundled_htp\" = \"xyes\"], [\n        PKG_CHECK_MODULES([libhtp], htp,, [with_pkgconfig_htp=no])\n        if test \"$with_pkgconfig_htp\" != \"no\"; then\n            CPPFLAGS=\"${CPPFLAGS} ${libhtp_CFLAGS}\"\n            LIBS=\"${LIBS} ${libhtp_LIBS}\"\n        fi\n\n        AC_ARG_WITH(libhtp_includes,\n                [  --with-libhtp-includes=DIR  libhtp include directory],\n                [with_libhtp_includes=\"$withval\"],[with_libhtp_includes=no])\n        AC_ARG_WITH(libhtp_libraries,\n                [  --with-libhtp-libraries=DIR    libhtp library directory],\n                [with_libhtp_libraries=\"$withval\"],[with_libhtp_libraries=\"no\"])\n\n        if test \"$with_libhtp_includes\" != \"no\"; then\n            CPPFLAGS=\"-I${with_libhtp_includes} ${CPPFLAGS}\"\n        fi\n\n        if test \"$with_libhtp_libraries\" != \"no\"; then\n            LDFLAGS=\"${LDFLAGS} -L${with_libhtp_libraries}\"\n        fi\n\n        AC_CHECK_HEADER(htp/htp.h,,[AC_MSG_ERROR(htp/htp.h not found ...)])\n\n        LIBHTP=\"\"\n        AC_CHECK_LIB(htp, htp_conn_create,, LIBHTP=\"no\")\n        if test \"$LIBHTP\" = \"no\"; then\n            echo\n            echo \"   ERROR! libhtp library not found\"\n            echo\n            exit 1\n        fi\n        PKG_CHECK_MODULES(LIBHTPMINVERSION, [htp >= 0.5.45],[libhtp_minver_found=\"yes\"],[libhtp_minver_found=\"no\"])\n        if test \"$libhtp_minver_found\" = \"no\"; then\n            PKG_CHECK_MODULES(LIBHTPDEVVERSION, [htp = 0.5.X],[libhtp_devver_found=\"yes\"],[libhtp_devver_found=\"no\"])\n            if test \"$libhtp_devver_found\" = \"no\"; then\n                echo\n                echo \"   ERROR! libhtp was found but it is neither >= 0.5.45, nor the dev 0.5.X\"\n                echo\n                exit 1\n            fi\n        fi\n\n        AC_CHECK_LIB([htp], [htp_config_register_request_uri_normalize],AC_DEFINE_UNQUOTED([HAVE_HTP_URI_NORMALIZE_HOOK],[1],[Found htp_config_register_request_uri_normalize function in libhtp]) ,,[-lhtp])\n        # check for htp_tx_get_response_headers_raw\n        AC_CHECK_LIB([htp], [htp_tx_get_response_headers_raw],AC_DEFINE_UNQUOTED([HAVE_HTP_TX_GET_RESPONSE_HEADERS_RAW],[1],[Found htp_tx_get_response_headers_raw in libhtp]) ,,[-lhtp])\n        AC_CHECK_LIB([htp], [htp_decode_query_inplace],AC_DEFINE_UNQUOTED([HAVE_HTP_DECODE_QUERY_INPLACE],[1],[Found htp_decode_query_inplace function in libhtp]) ,,[-lhtp])\n        AC_CHECK_LIB([htp], [htp_config_set_response_decompression_layer_limit],AC_DEFINE_UNQUOTED([HAVE_HTP_CONFIG_SET_RESPONSE_DECOMPRESSION_LAYER_LIMIT],[1],[Found htp_config_set_response_decompression_layer_limit function in libhtp]) ,,[-lhtp])\n        AC_CHECK_LIB([htp], [htp_config_set_allow_space_uri],AC_DEFINE_UNQUOTED([HAVE_HTP_CONFIG_SET_ALLOW_SPACE_URI],[1],[Found htp_config_set_allow_space_uri function in libhtp]) ,,[-lhtp])\n        AC_EGREP_HEADER(htp_config_set_path_decode_u_encoding, htp/htp.h, AC_DEFINE_UNQUOTED([HAVE_HTP_SET_PATH_DECODE_U_ENCODING],[1],[Found usable htp_config_set_path_decode_u_encoding function in libhtp]) )\n        AC_CHECK_LIB([htp], [htp_config_set_lzma_memlimit],AC_DEFINE_UNQUOTED([HAVE_HTP_CONFIG_SET_LZMA_MEMLIMIT],[1],[Found htp_config_set_lzma_memlimit function in libhtp]) ,,[-lhtp])\n        AC_CHECK_LIB([htp], [htp_config_set_lzma_layers],AC_DEFINE_UNQUOTED([HAVE_HTP_CONFIG_SET_LZMA_LAYERS],[1],[Found htp_config_set_lzma_layers function in libhtp]) ,,[-lhtp])\n        AC_CHECK_LIB([htp], [htp_config_set_compression_bomb_limit],AC_DEFINE_UNQUOTED([HAVE_HTP_CONFIG_SET_COMPRESSION_BOMB_LIMIT],[1],[Found htp_config_set_compression_bomb_limit function in libhtp]) ,,[-lhtp])\n        AC_CHECK_LIB([htp], [htp_config_set_compression_time_limit],AC_DEFINE_UNQUOTED([HAVE_HTP_CONFIG_SET_COMPRESSION_TIME_LIMIT],[1],[Found htp_config_set_compression_time_limit function in libhtp]) ,,[-lhtp])\n        AC_CHECK_LIB([htp], [htp_config_set_max_tx],AC_DEFINE_UNQUOTED([HAVE_HTP_CONFIG_SET_MAX_TX],[1],[Found htp_config_set_max_tx function in libhtp]) ,,[-lhtp])\n        AC_CHECK_LIB([htp], [htp_config_set_number_headers_limit],AC_DEFINE_UNQUOTED([HAVE_HTP_CONFIG_SET_HEADERS_LIMIT],[1],[Found htp_config_set_number_headers_limit function in libhtp]) ,,[-lhtp])\n    ])\n\n    if test \"x$enable_non_bundled_htp\" = \"xno\"; then\n        # test if we have a bundled htp\n        if test -d \"$srcdir/libhtp\"; then\n            AC_CONFIG_SUBDIRS([libhtp])\n            HTP_DIR=\"libhtp\"\n            AC_SUBST(HTP_DIR)\n            HTP_LDADD=\"../libhtp/htp/libhtp.la\"\n            AC_SUBST(HTP_LDADD)\n            # make sure libhtp is added to the includes\n            CPPFLAGS=\"-I\\${srcdir}/../libhtp/ ${CPPFLAGS}\"\n\n            AC_CHECK_HEADER(iconv.h,,[AC_MSG_ERROR(iconv.h not found ...)])\n            AC_CHECK_LIB(iconv, libiconv_close)\n            AC_DEFINE_UNQUOTED([HAVE_HTP_URI_NORMALIZE_HOOK],[1],[Assuming htp_config_register_request_uri_normalize function in bundled libhtp])\n            AC_DEFINE_UNQUOTED([HAVE_HTP_TX_GET_RESPONSE_HEADERS_RAW],[1],[Assuming htp_tx_get_response_headers_raw function in bundled libhtp])\n            AC_DEFINE_UNQUOTED([HAVE_HTP_DECODE_QUERY_INPLACE],[1],[Assuming htp_decode_query_inplace function in bundled libhtp])\n            # enable when libhtp has been updated\n            AC_DEFINE_UNQUOTED([HAVE_HTP_CONFIG_SET_RESPONSE_DECOMPRESSION_LAYER_LIMIT],[1],[Assuming htp_config_set_response_decompression_layer_limit function in bundled libhtp])\n            AC_DEFINE_UNQUOTED([HAVE_HTP_CONFIG_SET_ALLOW_SPACE_URI],[1],[Assuming htp_config_set_allow_space_uri function in bundled libhtp])\n            AC_DEFINE_UNQUOTED([HAVE_HTP_CONFIG_SET_LZMA_MEMLIMIT],[1],[Assuming htp_config_set_lzma_memlimit function in bundled libhtp])\n            AC_DEFINE_UNQUOTED([HAVE_HTP_CONFIG_SET_LZMA_LAYERS],[1],[Assuming htp_config_set_lzma_layers function in bundled libhtp])\n            AC_DEFINE_UNQUOTED([HAVE_HTP_CONFIG_SET_COMPRESSION_BOMB_LIMIT],[1],[Assuming htp_config_set_compression_bomb_limit function in bundled libhtp])\n            AC_DEFINE_UNQUOTED([HAVE_HTP_CONFIG_SET_COMPRESSION_TIME_LIMIT],[1],[Assuming htp_config_set_compression_time_limit function in bundled libhtp])\n            AC_DEFINE_UNQUOTED([HAVE_HTP_CONFIG_SET_MAX_TX],[1],[Assuming htp_config_set_max_tx function in bundled libhtp])\n            AC_DEFINE_UNQUOTED([HAVE_HTP_CONFIG_SET_HEADERS_LIMIT],[1],[Assuming htp_config_set_number_headers_limit function in bundled libhtp])\n        else\n            echo\n            echo \"  ERROR: Libhtp is not bundled. Get libhtp by doing:\"\n            echo \"     git clone https://github.com/OISF/libhtp\"\n            echo \"  Then re-run Suricata's autogen.sh and configure script.\"\n            echo \"  Or, if libhtp is installed in a different location,\"\n            echo \"  pass --enable-non-bundled-htp to Suricata's configure script.\"\n            echo \"  Add --with-libhtp-includes=<dir> and --with-libhtp-libraries=<dir> if\"\n            echo \"  libhtp is not installed in the include and library paths.\"\n            echo\n            exit 1\n        fi\n    fi\n\n    AM_CONDITIONAL([HTP_LDADD], [test \"x${HTP_LDADD}\" != \"x\"])\n\n  # Check for libcap-ng\n    case $host in\n    *-*-linux*)\n    AC_ARG_WITH(libcap_ng_includes,\n            [  --with-libcap_ng-includes=DIR  libcap_ng include directory],\n            [with_libcap_ng_includes=\"$withval\"],[with_libcap_ng_includes=no])\n    AC_ARG_WITH(libcap_ng_libraries,\n            [  --with-libcap_ng-libraries=DIR    libcap_ng library directory],\n            [with_libcap_ng_libraries=\"$withval\"],[with_libcap_ng_libraries=\"no\"])\n\n    if test \"$with_libcap_ng_includes\" != \"no\"; then\n        CPPFLAGS=\"${CPPFLAGS} -I${with_libcap_ng_includes}\"\n    fi\n\n    if test \"$with_libcap_ng_libraries\" != \"no\"; then\n        LDFLAGS=\"${LDFLAGS}  -L${with_libcap_ng_libraries}\"\n    fi\n\n    AC_CHECK_HEADER(cap-ng.h,,LIBCAP_NG=\"no\")\n    if test \"$LIBCAP_NG\" != \"no\"; then\n        LIBCAP_NG=\"\"\n        AC_CHECK_LIB(cap-ng,capng_clear,,LIBCAP_NG=\"no\")\n    fi\n\n    if test \"$LIBCAP_NG\" != \"no\"; then\n        AC_DEFINE([HAVE_LIBCAP_NG],[1],[Libpcap-ng support])\n    fi\n\n    if test \"$LIBCAP_NG\" = \"no\"; then\n        echo\n        echo \"   WARNING!  libcap-ng library not found, go get it\"\n        echo \"   from http://people.redhat.com/sgrubb/libcap-ng/\"\n        echo \"   or your distribution:\"\n        echo\n        echo \"   Ubuntu: apt-get install libcap-ng-dev\"\n        echo \"   Fedora: dnf install libcap-ng-devel\"\n        echo \"   CentOS/RHEL: yum install libcap-ng-devel\"\n        echo\n        echo \"   Suricata will be built without support for dropping privs.\"\n        echo\n    fi\n    ;;\n    esac\n\n    AC_CHECK_LIB(unwind,unw_backtrace,,LIBUNW=\"no\")\n    if test \"$LIBUNW\" = \"no\"; then\n        echo\n        echo \"   libunwind library and development headers not found\"\n        echo \"   stacktrace on unexpected termination due to signal not possible\"\n        echo\n    fi;\n\n    AC_ARG_ENABLE(ebpf,\n            AS_HELP_STRING([--enable-ebpf],[Enable eBPF support]),\n            [ enable_ebpf=\"$enableval\"],\n            [ enable_ebpf=\"no\"])\n\n    have_xdp=\"no\"\n    if test \"$enable_ebpf\" = \"yes\"; then\n        AC_CHECK_LIB(elf,elf_begin,,LIBELF=\"no\")\n        if test \"$LIBELF\" = \"no\"; then\n            echo\n            echo \"   libelf library and development headers not found but\"\n            echo \"   but needed to use eBPF code\"\n            echo\n            exit 1\n        fi;\n\n        AC_CHECK_LIB(bpf,bpf_object__open,,LIBBPF=\"no\")\n        if test \"$LIBBPF\" = \"no\"; then\n            echo\n            echo \"   libbpf library and development headers not found but\"\n            echo \"   needed to use eBPF code. It can be found at\"\n            echo \"   https://github.com/libbpf/libbpf\"\n            echo\n            exit 1\n        fi;\n        AC_CHECK_DECL([PACKET_FANOUT_EBPF],\n            AC_DEFINE([HAVE_PACKET_EBPF],[1],[Recent ebpf fanout support is available]),\n            [],\n            [[#include <linux/if_packet.h>]])\n        # Check for XDP specific function.\n        AC_CHECK_LIB(bpf,bpf_xdp_attach,have_xdp=\"yes\")\n        if test \"$have_xdp\" = \"yes\"; then\n            AC_DEFINE([HAVE_PACKET_XDP],[1],[XDP support is available])\n        else\n            # Check for legacy XDP function.\n            AC_CHECK_LIB(bpf,bpf_set_link_xdp_fd,have_xdp=\"yes\")\n            if test \"$have_xdp\" = \"yes\"; then\n                AC_DEFINE([HAVE_PACKET_XDP],[1],[XDP support is available])\n            fi\n        fi\n        AC_CHECK_FUNCS([bpf_program__section_name bpf_xdp_attach bpf_program__set_type])\n    fi;\n\n  # Check for DAG support.\n    AC_ARG_ENABLE(dag,\n            AS_HELP_STRING([--enable-dag],[Enable DAG capture]),\n            [ enable_dag=$enableval ],\n            [ enable_dag=no])\n    AC_ARG_WITH(dag_includes,\n            [  --with-dag-includes=DIR  dagapi include directory],\n            [with_dag_includes=\"$withval\"],[with_dag_includes=\"no\"])\n    AC_ARG_WITH(dag_libraries,\n            [  --with-dag-libraries=DIR  dagapi library directory],\n            [with_dag_libraries=\"$withval\"],[with_dag_libraries=\"no\"])\n\n    if test \"$enable_dag\" = \"yes\"; then\n\n        if test \"$with_dag_includes\" != \"no\"; then\n            CPPFLAGS=\"${CPPFLAGS} -I${with_dag_includes}\"\n        fi\n\n        if test \"$with_dag_libraries\" != \"no\"; then\n            LDFLAGS=\"${LDFLAGS} -L${with_dag_libraries}\"\n        fi\n\n        AC_CHECK_HEADER(dagapi.h,DAG=\"yes\",DAG=\"no\")\n        if test \"$DAG\" != \"no\"; then\n            DAG=\"\"\n            AC_CHECK_LIB(dag,dag_open,,DAG=\"no\",)\n        fi\n\n        if test \"$DAG\" = \"no\"; then\n            echo\n            echo \"  ERROR! libdag library not found\"\n            echo\n            exit 1\n        fi\n\n        AC_DEFINE([HAVE_DAG],[1],(Endace DAG card support enabled))\n    fi\n\n  # libmagic\n    enable_magic=\"no\"\n    AC_ARG_ENABLE(libmagic,\n           AS_HELP_STRING([--enable-libmagic], [Enable libmagic support [default=yes]]),\n                        [enable_magic=$enableval],[enable_magic=yes])\n    if test \"$enable_magic\" = \"yes\"; then\n        AC_ARG_WITH(libmagic_includes,\n                [  --with-libmagic-includes=DIR  libmagic include directory],\n                [with_libmagic_includes=\"$withval\"],[with_libmagic_includes=no])\n        AC_ARG_WITH(libmagic_libraries,\n                [  --with-libmagic-libraries=DIR    libmagic library directory],\n                [with_libmagic_libraries=\"$withval\"],[with_libmagic_libraries=\"no\"])\n\n        if test \"$with_libmagic_includes\" != \"no\"; then\n            CPPFLAGS=\"${CPPFLAGS} -I${with_libmagic_includes}\"\n        fi\n\n        AC_CHECK_HEADER(magic.h,,MAGIC=\"no\")\n        if test \"$MAGIC\" != \"no\"; then\n            MAGIC=\"\"\n            AC_CHECK_LIB(magic, magic_open,, MAGIC=\"no\")\n        fi\n\n        if test \"x$MAGIC\" != \"xno\"; then\n            if test \"$with_libmagic_libraries\" != \"no\"; then\n                LDFLAGS=\"${LDFLAGS}  -L${with_libmagic_libraries}\"\n            fi\n            AC_DEFINE([HAVE_MAGIC],[1],(Libmagic for file handling))\n        else\n            echo\n            echo \"   WARNING!  magic library not found, go get it\"\n            echo \"   from http://www.darwinsys.com/file/ or your distribution:\"\n            echo\n            echo \"   Ubuntu: apt-get install libmagic-dev\"\n            echo \"   Fedora: dnf install file-devel\"\n            echo \"   CentOS/RHEL: yum install file-devel\"\n            echo\n            enable_magic=\"no\"\n        fi\n    fi\n\n    # Napatech - Using the 3GD API\n    AC_ARG_ENABLE(napatech,\n                AS_HELP_STRING([--enable-napatech],[Enabled Napatech Devices]),\n                [ enable_napatech=$enableval ],\n                [ enable_napatech=no])\n    AS_IF([test \"x$enable_napatech\" = \"xyes\"], [\n        if test \"x$enable_shared\" = \"xno\"; then\n            echo\n            echo \"   ERROR! napatech cannot be enabled with --disable-shared\"\n            echo\n            exit 1\n        fi\n        AC_ARG_ENABLE(napatech_bypass,\n                    AS_HELP_STRING([--disable-napatech-bypass],[Disable Bypass feature on Napatech cards]),\n                    [ napatech_bypass=$enableval ],\n                    [ napatech_bypass=yes])\n        AC_ARG_WITH(napatech_includes,\n                    [  --with-napatech-includes=DIR   napatech include directory],\n                    [with_napatech_includes=\"$withval\"],[with_napatech_includes=\"/opt/napatech3/include\"])\n        AC_ARG_WITH(napatech_libraries,\n                    [  --with-napatech-libraries=DIR  napatech library directory],\n                    [with_napatech_libraries=\"$withval\"],[with_napatech_libraries=\"/opt/napatech3/lib\"])\n\n        if test \"$enable_napatech\" = \"yes\"; then\n            CPPFLAGS=\"${CPPFLAGS} -I${with_napatech_includes}\"\n            LDFLAGS=\"${LDFLAGS} -L${with_napatech_libraries}\"\n            LIBS=\"${LIBS} -lntapi\"\n            AC_CHECK_HEADER(nt.h,NAPATECH=\"yes\",NAPATECH=\"no\")\n            if test \"$NAPATECH\" != \"no\"; then\n                NAPATECH=\"\"\n                AC_CHECK_LIB(ntapi, NT_Init,NAPATECH=\"yes\",NAPATECH=\"no\")\n            fi\n\n            if test \"$NAPATECH\" = \"no\"; then\n                echo\n                echo \"  ERROR! libntapi library not found\"\n                echo\n                exit 1\n            else\n                AC_CHECK_LIB(numa, numa_available,, LIBNUMA=\"no\")\n                if test \"$LIBNUMA\" = \"no\"; then\n                    echo\n                    echo \"  WARNING: libnuma is required to use Napatech auto-config\"\n                    echo \"      libnuma is not found.  Go get it\"\n                    echo \"      from http://github.com/numactl/numactl or your distribution:\"\n                    echo \"          Ubuntu: apt-get install libnuma-dev\"\n                    echo \"          Fedora: dnf install numactl-devel\"\n                    echo \"          CentOS/RHEL: yum install numactl-devel\"\n                    echo\n                    exit 1\n                fi\n            fi\n\n            AC_DEFINE([HAVE_NAPATECH],[1],(Napatech capture card support))\n            if test \"$napatech_bypass\" = \"yes\"; then\n                AC_CHECK_LIB(ntapi, NT_FlowOpenAttrInit,NTFLOW=\"yes\",NTFLOW=\"no\")\n                if test \"$NTFLOW\" = \"yes\"; then\n                    echo \"   Napatech Flow Processing is Enabled (--disable-napatech-bypass if not needed)\"\n                    AC_DEFINE([NAPATECH_ENABLE_BYPASS],[1],(Napatech flowdirector support))\n                else\n                    echo \"Napatech Flow Processing is not available\"\n                fi\n            else\n                echo \"Napatech Flow Processing is Disabled.\"\n            fi\n        fi\n        ])\n\n    if test \"x$enable_napatech\" = \"xyes\"; then\n        AM_CONDITIONAL([BUILD_NAPATECH], [true])\n        napatech_comment=\"\"\n    else\n        AM_CONDITIONAL([BUILD_NAPATECH], [false])\n        napatech_comment=\"#\"\n    fi\n    AC_SUBST([napatech_comment])\n\n  # libmaxminddb\n    AC_ARG_ENABLE(geoip,\n            AS_HELP_STRING([--enable-geoip],[Enable GeoIP2 support]),\n            [ enable_geoip=\"$enableval\"],\n            [ enable_geoip=\"no\"])\n    AC_ARG_WITH(libmaxminddb_includes,\n            [  --with-libmaxminddb-includes=DIR  libmaxminddb include directory],\n            [with_libmaxminddb_includes=\"$withval\"],[with_libmaxminddb_includes=\"no\"])\n    AC_ARG_WITH(libmaxminddb_libraries,\n            [  --with-libmaxminddb-libraries=DIR    libmaxminddb library directory],\n            [with_libmaxminddb_libraries=\"$withval\"],[with_libmaxminddb_libraries=\"no\"])\n\n    if test \"$enable_geoip\" = \"yes\"; then\n        if test \"$with_libmaxminddb_includes\" != \"no\"; then\n            CPPFLAGS=\"${CPPFLAGS} -I${with_libmaxminddb_includes}\"\n        fi\n\n        AC_CHECK_HEADER(maxminddb.h,GEOIP=\"yes\",GEOIP=\"no\")\n        if test \"$GEOIP\" = \"yes\"; then\n            if test \"$with_libmaxminddb_libraries\" != \"no\"; then\n                LDFLAGS=\"${LDFLAGS} -L${with_libmaxminddb_libraries}\"\n            fi\n            AC_CHECK_LIB(maxminddb, MMDB_open,, GEOIP=\"no\")\n        fi\n        if test \"$GEOIP\" = \"no\"; then\n            echo\n            echo \"   ERROR!  libmaxminddb GeoIP2 library not found, go get it\"\n            echo \"   from https://github.com/maxmind/libmaxminddb or your distribution:\"\n            echo\n            echo \"   Ubuntu: apt-get install libmaxminddb-dev\"\n            echo \"   Fedora: dnf install libmaxminddb-devel\"\n            echo \"   CentOS/RHEL: yum install libmaxminddb-devel\"\n            echo\n            exit 1\n        fi\n\n        AC_DEFINE([HAVE_GEOIP],[1],[libmaxminddb available])\n        enable_geoip=\"yes\"\n    fi\n\n  # Position Independent Executable\n    AC_ARG_ENABLE(pie,\n                AS_HELP_STRING([--enable-pie],[Enable compiling as a position independent executable]),\n                [ enable_pie=\"$enableval\"],\n                [ enable_pie=\"no\"])\n    if test \"$enable_pie\" = \"yes\"; then\n        CPPFLAGS=\"${CPPFLAGS} -fPIC\"\n        LDFLAGS=\"${LDFLAGS} -pie\"\n    fi\n\n#libevent includes and libraries\n    AC_ARG_WITH(libevent_includes,\n            [  --with-libevent-includes=DIR  libevent include directory],\n            [with_libevent_includes=\"$withval\"],[with_libevent_includes=\"no\"])\n    AC_ARG_WITH(libevent_libraries,\n            [  --with-libevent-libraries=DIR    libevent library directory],\n            [with_libevent_libraries=\"$withval\"],[with_libevent_libraries=\"no\"])\n\n# libhiredis\n    AC_ARG_ENABLE(hiredis,\n            AS_HELP_STRING([--enable-hiredis],[Enable Redis support]),\n            [ enable_hiredis=\"$enableval\"],\n            [ enable_hiredis=\"no\"])\n    AC_ARG_WITH(libhiredis_includes,\n            [  --with-libhiredis-includes=DIR  libhiredis include directory],\n            [with_libhiredis_includes=\"$withval\"],[with_libhiredis_includes=\"no\"])\n    AC_ARG_WITH(libhiredis_libraries,\n            [  --with-libhiredis-libraries=DIR    libhiredis library directory],\n            [with_libhiredis_libraries=\"$withval\"],[with_libhiredis_libraries=\"no\"])\n\n    enable_hiredis_async=\"no\"\n    if test \"$enable_hiredis\" = \"yes\"; then\n        if test \"$with_libhiredis_includes\" != \"no\"; then\n            CPPFLAGS=\"${CPPFLAGS} -I${with_libhiredis_includes}\"\n        fi\n\n        AC_CHECK_HEADER(\"hiredis/hiredis.h\",HIREDIS=\"yes\",HIREDIS=\"no\")\n        if test \"$HIREDIS\" = \"yes\"; then\n            if test \"$with_libhiredis_libraries\" != \"no\"; then\n                LDFLAGS=\"${LDFLAGS}  -L${with_libhiredis_libraries}\"\n            fi\n            AC_CHECK_LIB(hiredis, redisConnect,, HIREDIS=\"no\")\n        fi\n        if test \"$HIREDIS\" = \"no\"; then\n            echo\n            echo \"   ERROR!  libhiredis library not found, go get it\"\n            echo \"   from https://github.com/redis/hiredis or your distribution:\"\n            echo\n            echo \"   Ubuntu: apt-get install libhiredis-dev\"\n            echo \"   Fedora: dnf install hiredis-devel\"\n            echo \"   CentOS/RHEL: yum install hiredis-devel\"\n            echo\n            exit 1\n        fi\n        if test \"$HIREDIS\" = \"yes\"; then\n            AC_DEFINE([HAVE_LIBHIREDIS],[1],[libhiredis available])\n            enable_hiredis=\"yes\"\n            #\n            # Check if async adapters and libevent is installed\n            #\n            AC_CHECK_HEADER(\"hiredis/adapters/libevent.h\",HIREDIS_LIBEVENT_ADAPTER=\"yes\",HIREDIS_LIBEVENT_ADAPTER=\"no\")\n            if test \"$HIREDIS_LIBEVENT_ADAPTER\" = \"yes\"; then\n                #Look for libevent headers\n                if test \"$with_libevent_includes\" != \"no\"; then\n                    CPPFLAGS=\"${CPPFLAGS} -I${with_libevent_includes}\"\n                fi\n                AC_CHECK_HEADER(\"event.h\",LIBEVENT=\"yes\",LIBEVENT=\"no\")\n                if test \"$LIBEVENT\" = \"yes\"; then\n                    if test \"$with_libevent_libraries\" != \"no\"; then\n                        LDFLAGS=\"${LDFLAGS}  -L${with_libevent_libraries}\"\n                    fi\n                    AC_CHECK_LIB(event, event_base_free,, HAVE_LIBEVENT=\"no\")\n                    AC_CHECK_LIB(event_pthreads, evthread_use_pthreads,, HAVE_LIBEVENT_PTHREADS=\"no\")\n                fi\n                if [ test \"$HAVE_LIBEVENT\" = \"no\" ] && [ -o test \"$HAVE_LIBEVENT_PTHREADS\" = \"no\"]; then\n                    if test \"$HAVE_LIBEVENT\" = \"no\"; then\n                        echo\n                        echo \"  Async mode for redis output will not be available.\"\n                        echo \"  To enable it install libevent\"\n                        echo\n                        echo \"   Ubuntu: apt-get install libevent-dev\"\n                        echo \"   Fedora: dnf install libevent-devel\"\n                        echo \"   CentOS/RHEL: yum install libevent-devel\"\n                        echo\n                   fi\n                   if test \"$HAVE_LIBEVENT_PTHREADS\" = \"no\"; then\n                        echo\n                        echo \"  Async mode for redis output will not be available.\"\n                        echo \"  To enable it install libevent with pthreads support\"\n                        echo\n                        echo \"   Ubuntu: apt-get install libevent-pthreads-2.0-5\"\n                        echo\n                   fi\n                else\n                    AC_DEFINE([HAVE_LIBEVENT],[1],[libevent available])\n                    enable_hiredis_async=\"yes\"\n                fi\n            fi\n        fi\n    fi\n\n    AC_ARG_ENABLE(ja3,\n           AS_HELP_STRING([--disable-ja3], [Disable JA3 support]),\n           [enable_ja3=\"$enableval\"],\n           [enable_ja3=yes])\n    if test \"$enable_ja3\" = \"yes\"; then\n        AC_DEFINE([HAVE_JA3],[1],[JA3 enabled])\n        enable_ja3=\"yes\"\n    fi\n    AM_CONDITIONAL([HAVE_JA3], [test \"x$enable_ja3\" != \"xno\"])\n\n    AC_ARG_ENABLE(ja4,\n           AS_HELP_STRING([--disable-ja4], [Disable JA4 support]),\n           [enable_ja4=\"$enableval\"],\n           [enable_ja4=yes])\n    if test \"$enable_ja4\" = \"yes\"; then\n        AC_DEFINE([HAVE_JA4],[1],[JA4 enabled])\n        enable_ja4=\"yes\"\n    fi\n    AM_CONDITIONAL([HAVE_JA4], [test \"x$enable_ja4\" != \"xno\"])\n\n\n# Check for lz4\nenable_liblz4=\"yes\"\nAC_CHECK_LIB(lz4, LZ4F_createCompressionContext, , enable_liblz4=\"no\")\n\nif test \"$enable_liblz4\" = \"no\"; then\n    echo\n    echo \"  Compressed pcap logging is not available without liblz4.\"\n    echo \"  If you want to enable compression, you need to install it.\"\n    echo\n    echo \"  Ubuntu: apt-get install liblz4-dev\"\n    echo \"  Fedora: dnf install lz4-devel\"\n    echo \"  CentOS/RHEL: yum install epel-release\"\n    echo \"               yum install lz4-devel\"\n    echo\nfi\n\n# get cache line size\n    AC_PATH_PROG(HAVE_GETCONF_CMD, getconf, \"no\")\n    if test \"$HAVE_GETCONF_CMD\" != \"no\"; then\n        CLS=$(getconf LEVEL1_DCACHE_LINESIZE)\n        if [test \"$CLS\" != \"undefined\" && test \"$CLS\" != \"\" && test \"$CLS\" != \"0\"]; then\n            AC_DEFINE_UNQUOTED([CLS],[${CLS}],[L1 cache line size])\n        else\n            AC_DEFINE([CLS],[64],[L1 cache line size])\n        fi\n    else\n        AC_DEFINE([CLS],[64],[L1 cache line size])\n    fi\n\n# sphinx-build for documentation, and also check for a new enough version\n    AC_PATH_PROG([SPHINX_BUILD], [sphinx-build], [no])\n    if test \"$SPHINX_BUILD\" != \"no\"; then\n        MIN_SPHINX_BUILD_VERSION=\"3.4.3\"\n        sphinx_build_version=$($SPHINX_BUILD --version 2>&1 | cut -d' ' -f2-)\n        AC_MSG_CHECKING([for sphinx-build >= $MIN_SPHINX_BUILD_VERSION])\n        AS_VERSION_COMPARE([$sphinx_build_version], [$MIN_SPHINX_BUILD_VERSION],\n            [\n                AC_MSG_RESULT([no, documentation will not be built])\n                SPHINX_BUILD=\"no\"\n            ],\n            [], [])\n        if test \"$SPHINX_BUILD\" != \"no\"; then\n            AC_MSG_RESULT([yes])\n        fi\n    fi\n\n    if test \"$SPHINX_BUILD\" = \"no\"; then\n       enable_sphinxbuild=no\n       if test -e \"$srcdir/doc/userguide/suricata.1\"; then\n           have_suricata_man=yes\n       fi\n    fi\n    AM_CONDITIONAL([SPHINX_BUILD], [test \"x$enable_sphinxbuild\" != \"xno\"])\n    AM_CONDITIONAL([HAVE_SURICATA_MAN], [test \"x$have_suricata_man\" = \"xyes\"])\n\n# pdflatex for the pdf version of the user manual\n    AC_PATH_PROG(HAVE_PDFLATEX, pdflatex, \"no\")\n    if test \"$HAVE_PDFLATEX\" = \"no\"; then\n       enable_pdflatex=no\n    fi\n    AM_CONDITIONAL([HAVE_PDFLATEX], [test \"x$enable_pdflatex\" != \"xno\"])\n\n# Cargo/Rust\n    AM_CONDITIONAL([RUST_CROSS_COMPILE], [test \"x$cross_compiling\" = \"xyes\"])\n    AC_PATH_PROG(RUSTC, rustc, \"no\")\n    if test \"$RUSTC\" = \"no\"; then\n        echo \"\"\n        echo \"    ERROR: Rust compiler not found.\"\n        echo \"\"\n        echo \"    Ubuntu/Debian: apt install rustc cargo\"\n        echo \"    Fedora: dnf install rustc cargo\"\n        echo \"    CentOS: yum install rustc cargo (requires EPEL)\"\n        echo \"\"\n        echo \"    Rustup works as well: https://rustup.rs/\"\n        echo \"\"\n        exit 1\n    fi\n\n    AC_PATH_PROG(CARGO, cargo, \"no\")\n    if test \"CARGO\" = \"no\"; then\n        AC_MSG_ERROR([cargo required])\n    fi\n\n    AC_DEFINE([HAVE_RUST],[1],[Enable Rust language])\n    AM_CONDITIONAL([HAVE_RUST],true)\n    AC_SUBST([CARGO], [$CARGO])\n\n    rust_compiler_version=$($RUSTC --version)\n    rustc_version=$(echo \"$rust_compiler_version\" | sed 's/^.*[[^0-9]]\\([[0-9]]*\\.[[0-9]]*\\.[[0-9]]*\\).*$/\\1/')\n    cargo_version_output=$($CARGO --version)\n    cargo_version=$(echo \"$cargo_version_output\" | sed 's/^.*[[^0-9]]\\([[0-9]]*\\.[[0-9]]*\\.[[0-9]]*\\).*$/\\1/')\n\n    MIN_RUSTC_VERSION=\"1.67.1\" # MSRV\n    AC_MSG_CHECKING(for Rust version $MIN_RUSTC_VERSION or newer)\n    AS_VERSION_COMPARE([$rustc_version], [$MIN_RUSTC_VERSION],\n        [\n            echo \"\"\n            echo \"ERROR: Rust $MIN_RUSTC_VERSION or newer required.\"\n            echo \"\"\n            echo \"Rust version ${rustc_version} was found.\"\n            echo \"\"\n            exit 1\n    ],\n    [],\n    [])\n    AC_MSG_RESULT(yes)\n\n    RUST_FEATURES=\"\"\n\n    rust_vendor_comment=\"# \"\n    have_rust_vendor=\"no\"\n\n    if test \"x$cross_compiling\" = \"xyes\"; then\n      RUST_SURICATA_LIB_XC_DIR=\"${host_alias}/\"\n    else\n      if test \"x$CARGO_BUILD_TARGET\" = \"x\"; then\n        RUST_SURICATA_LIB_XC_DIR=\n      else\n        RUST_SURICATA_LIB_XC_DIR=\"${CARGO_BUILD_TARGET}/\"\n      fi\n    fi\n\n    if test \"x$enable_debug\" = \"xyes\"; then\n      RUST_SURICATA_LIBDIR=\"../rust/target/${RUST_SURICATA_LIB_XC_DIR}debug\"\n    else\n      RUST_SURICATA_LIBDIR=\"../rust/target/${RUST_SURICATA_LIB_XC_DIR}release\"\n    fi\n    RUST_SURICATA_LIB=\"${RUST_SURICATA_LIBDIR}/${RUST_SURICATA_LIBNAME}\"\n\n    CFLAGS=\"${CFLAGS} -I\\${srcdir}/../rust/gen -I\\${srcdir}/../rust/dist -I../rust/gen\"\n    AC_SUBST(RUST_SURICATA_LIB)\n    AC_SUBST(RUST_LDADD)\n    if test \"x$CARGO_HOME\" = \"x\"; then\n        if test \"x$HAVE_CYGPATH\" != \"xno\"; then\n          CYGPATH_CARGO_HOME=$(cygpath -a -t mixed ~/.cargo)\n          AC_SUBST([CARGO_HOME], [$CYGPATH_CARGO_HOME])\n        else\n          AC_SUBST([CARGO_HOME], [~/.cargo])\n        fi\n    else\n      AC_SUBST([CARGO_HOME], [$CARGO_HOME])\n    fi\n\n    # Check for rustup. RUSTUP_HOME needs to be set if rustup is in\n    # use, and a user uses sudo (depending on configuration), or su to\n    # perform the install\n    rustup_home_path=\"no\"\n    if test \"x$RUSTUP_HOME\" != \"x\"; then\n        rustup_home_path=\"$RUSTUP_HOME\"\n    else\n        AC_PATH_PROG(have_rustup, rustup, \"no\")\n        if test \"x$have_rustup\" != \"xno\"; then\n            rustup_home_path=$($have_rustup show home 2>/dev/null || echo \"no\")\n        fi\n    fi\n    rustup_home=\"\"\n    if test \"x$rustup_home_path\" != \"xno\"; then\n        rustup_home=\"RUSTUP_HOME=\\$(RUSTUP_HOME_PATH)\"\n    fi\n    AC_SUBST([RUSTUP_HOME_PATH], [$rustup_home_path])\n    AC_SUBST([rustup_home])\n\n    if test -e \"$srcdir/rust/vendor\"; then\n      have_rust_vendor=\"yes\"\n    fi\n\n    if test \"x$have_rust_vendor\" = \"xyes\"; then\n      rust_vendor_comment=\"\"\n    fi\n\n    AC_SUBST(rust_vendor_comment)\n    AM_CONDITIONAL([HAVE_RUST_VENDOR], [test \"x$have_rust_vendor\" = \"xyes\"])\n\n    have_rust_headers=\"no\"\n    AC_MSG_CHECKING(for $srcdir/rust/dist/rust-bindings.h)\n    if test -f \"$srcdir/rust/dist/rust-bindings.h\"; then\n       AC_MSG_RESULT(yes)\n       have_rust_headers=\"yes\"\n    else\n       AC_MSG_RESULT(no)\n       AC_MSG_CHECKING(for $srcdir/rust/gen/rust-bindings.h)\n       if test -f \"$srcdir/rust/gen/rust-bindings.h\"; then\n           AC_MSG_RESULT(yes)\n           have_rust_headers=\"yes\"\n       else\n           AC_MSG_RESULT(no)\n       fi\n    fi\n\n    AC_PATH_PROG(CBINDGEN, cbindgen, \"no\")\n    if test \"x$CBINDGEN\" != \"xno\"; then\n      cbindgen_version=$(cbindgen --version 2>&1 | cut -d' ' -f2-)\n      min_cbindgen_version=\"0.10.0\"\n      AS_VERSION_COMPARE([$cbindgen_version], [$min_cbindgen_version],\n          [cbindgen_ok=\"no\"],\n          [cbindgen_ok=\"yes\"],\n          [cbindgen_ok=\"yes\"])\n      if test \"x$cbindgen_ok\" != \"xyes\"; then\n        echo \"  Warning: cbindgen must be at least version $min_cbindgen_version,\"\n        echo \"      found $cbindgen_version.\"\n        echo \"  To update: cargo install --force cbindgen\"\n        CBINDGEN=\"no\"\n      else\n        have_rust_headers=\"no\"\n      fi\n    fi\n\n    AC_SUBST([CBINDGEN], [$CBINDGEN])\n\n    # Require cbindgen if generated headers are not bundled.\n    if test \"x$have_rust_headers\" != \"xyes\"; then\n      if test \"x$CBINDGEN\" = \"xno\"; then\n        echo \"  Warning: cbindgen too old or not found, it is required to \"\n        echo \"      generate header files.\"\n        echo \"  To install: cargo install --force cbindgen\"\n        AC_MSG_ERROR([cbindgen required])\n      fi\n    fi\n\n    AM_CONDITIONAL([HAVE_RUST_HEADERS], [test \"x$have_rust_headers\" = \"xyes\"])\n    AM_CONDITIONAL([HAVE_CBINDGEN], [test \"x$CBINDGEN\" != \"xno\"])\n\n    AC_ARG_ENABLE(rust_strict,\n           AS_HELP_STRING([--enable-rust-strict], [Rust warnings as errors]),[enable_rust_strict=$enableval],[enable_rust_strict=no])\n    AS_IF([test \"x$enable_rust_strict\" = \"xyes\"], [\n        RUST_FEATURES=\"strict\"\n    ])\n    AC_SUBST(RUST_FEATURES)\n\n    AC_ARG_ENABLE(warnings,\n           AS_HELP_STRING([--enable-warnings], [Enable supported C compiler warnings]),[enable_warnings=$enableval],[enable_warnings=no])\n    AS_IF([test \"x$enable_warnings\" = \"xyes\"], [\n        # check if our compiler supports -Wunused-macros\n        AC_MSG_CHECKING(for -Wunused-macros support)\n        OCFLAGS=$CFLAGS\n        CFLAGS=\"$CFLAGS -Wunused-macros\"\n        AC_COMPILE_IFELSE([AC_LANG_PROGRAM([[]],\n                    [[]])],\n                AC_MSG_RESULT([yes]),\n                [AC_MSG_RESULT([no])\n                CFLAGS=\"$OCFLAGS\"])\n        # check if our compiler supports -Wimplicit-int-float-conversion\n        AC_MSG_CHECKING(for -Wimplicit-int-float-conversion)\n        OCFLAGS=$CFLAGS\n        CFLAGS=\"$CFLAGS -Wimplicit-int-float-conversion\"\n        AC_COMPILE_IFELSE([AC_LANG_PROGRAM([[]],\n                    [[]])],\n                AC_MSG_RESULT([yes]),\n                [AC_MSG_RESULT([no])\n                CFLAGS=\"$OCFLAGS\"])\n    ])\n\n    AC_CHECK_LIB(fuzzpcap, FPC_IsFuzzPacketCapture, HAS_FUZZPCAP=\"yes\")\n    AM_CONDITIONAL([HAS_FUZZPCAP], [test \"x$HAS_FUZZPCAP\" = \"xyes\"])\n    AC_ARG_ENABLE(fuzztargets,\n        AS_HELP_STRING([--enable-fuzztargets], [Enable fuzz targets]),[enable_fuzztargets=$enableval],[enable_fuzztargets=no])\n    AM_CONDITIONAL([BUILD_FUZZTARGETS], [test \"x$enable_fuzztargets\" = \"xyes\"])\n    AM_CONDITIONAL([RUST_BUILD_STD], [test \"x$enable_fuzztargets\" = \"xyes\" && echo \"$rust_compiler_version\" | grep -q nightly && echo \"$RUSTFLAGS\" | grep -v -q coverage])\n    AC_PROG_CXX\n    AS_IF([test \"x$enable_fuzztargets\" = \"xyes\"], [\n        AS_IF([test \"x$CARGO_BUILD_TARGET\" = \"x\" && echo \"$rust_compiler_version\" | grep -q nightly], [\n            CARGO_BUILD_TARGET=x86_64-unknown-linux-gnu\n            AC_SUBST(CARGO_BUILD_TARGET)\n        ])\n        AC_DEFINE([FUZZ], [1], [Fuzz targets are enabled])\n        AC_DEFINE([AFLFUZZ_NO_RANDOM], [1], [Disable all use of random functions])\n        CFLAGS_ORIG=$CFLAGS\n        CFLAGS=\"-Werror\"\n        AC_COMPILE_IFELSE([AC_LANG_PROGRAM([[]], [[while (__AFL_LOOP(1000))]])],\n                [AC_DEFINE([AFLFUZZ_PERSISTENT_MODE], [1], [Enable AFL PERSISTENT_MODE])],\n                [])\n        CFLAGS=$CFLAGS_ORIG\n        AC_LANG_PUSH(C++)\n        tmp_saved_flags=$[]_AC_LANG_PREFIX[]FLAGS\n        AS_IF([test \"x$LIB_FUZZING_ENGINE\" = \"x\"], [\n            LIB_FUZZING_ENGINE=-fsanitize=fuzzer\n            AC_SUBST(LIB_FUZZING_ENGINE)\n        ])\n        _AC_LANG_PREFIX[]FLAGS=\"$[]_AC_LANG_PREFIX[]FLAGS $LIB_FUZZING_ENGINE\"\n        AC_MSG_CHECKING([whether $CXX accepts $LIB_FUZZING_ENGINE])\n        AC_LINK_IFELSE([AC_LANG_SOURCE([[\n#include <sys/types.h>\nextern \"C\" int LLVMFuzzerTestOneInput(const unsigned char *Data, size_t Size);\nextern \"C\" int LLVMFuzzerTestOneInput(const unsigned char *Data, size_t Size) {\n(void)Data;\n(void)Size;\nreturn 0;\n}\n            ]])],\n            [ AC_MSG_RESULT(yes)\n              has_sanitizefuzzer=yes],\n            [ AC_MSG_RESULT(no) ]\n        )\n        _AC_LANG_PREFIX[]FLAGS=$tmp_saved_flags\n        AC_LANG_POP()\n    ])\n\n    AM_CONDITIONAL([HAS_FUZZLDFLAGS], [test \"x$has_sanitizefuzzer\" = \"xyes\"])\n\n# get git revision and last commit date\n    AC_PATH_PROG(HAVE_GIT_CMD, git, \"no\")\n    if test \"$HAVE_GIT_CMD\" != \"no\"; then\n        if [ test -e .git ]; then\n            REVISION=`git rev-parse --short HEAD`\n            LAST_COMMIT_DATE=`git log -1 --date=short --pretty=format:%cd`\n            REVISION=\"$REVISION $LAST_COMMIT_DATE\"\n            AC_DEFINE_UNQUOTED([REVISION],[${REVISION}],[Git revision])\n        fi\n    fi\n\n# Get the release date. If LAST_COMMIT_DATE was set in the previous\n# step, use it, otherwise parse it from the ChangeLog.\n    AC_MSG_CHECKING([for release date])\n    if test \"x$LAST_COMMIT_DATE\" != \"x\"; then\n\tRELEASE_DATE=$LAST_COMMIT_DATE\n    else\n        RELEASE_DATE=`awk '/^[[0-9\\.]]+ -- [[0-9]][[0-9]][[0-9]][[0-9]]-[[0-9]][[0-9]]-[[0-9]][[0-9]]/ { print $3; exit }' $srcdir/ChangeLog`\n        if test \"x$RELEASE_DATE\" = \"x\"; then\n            AC_MSG_ERROR([Failed to determine release date])\n        fi\n    fi\n    AC_MSG_RESULT([${RELEASE_DATE}])\n    AC_SUBST(RELEASE_DATE)\n\n# get MAJOR_MINOR version for embedding in configuration file.\n    MAJOR_MINOR=`expr \"${PACKAGE_VERSION}\" : \"\\([[0-9]]\\+\\.[[0-9]]\\+\\).*\"`\n\nif test \"${enable_ebpf}\" = \"yes\" || test \"${enable_unittests}\" = \"yes\"; then\n  AC_DEFINE([CAPTURE_OFFLOAD_MANAGER], [1],[Building flow bypass manager code])\nfi\nif test \"${enable_ebpf}\" = \"yes\" || test \"${enable_nfqueue}\" = \"yes\" || test \"${enable_pfring}\" = \"yes\" || test \"${enable_napatech}\" = \"yes\"  || test \"${enable_unittests}\" = \"yes\"; then\n  AC_DEFINE([CAPTURE_OFFLOAD], [1],[Building flow capture bypass code])\nfi\n\n# Add diagnostic filename\nCPPFLAGS=\"${CPPFLAGS} -D__SCFILENAME__=\\\\\\\"\\$(*F)\\\\\\\"\"\n\nAC_SUBST(CFLAGS)\nAC_SUBST(LDFLAGS)\nAC_SUBST(CPPFLAGS)\n\ndefine([EXPAND_VARIABLE],\n[$2=[$]$1\nif test $prefix = 'NONE'; then\n    prefix=\"/usr/local\"\nfi\nwhile true; do\n  case \"[$]$2\" in\n    *\\[$]* ) eval \"$2=[$]$2\" ;;\n    *) break ;;\n  esac\ndone\neval \"$2=[$]$2$3\"\n])dnl EXPAND_VARIABLE\n\n# suricata log dir\nif test \"$WINDOWS_PATH\" = \"yes\"; then\n    case $host in\n        x86_64-w64-mingw32)\n            e_winbase=\"C:\\\\\\\\Program Files\\\\\\\\Suricata\"\n            ;;\n        *)\n            systemtype=\"`systeminfo | grep \\\"based PC\\\"`\"\n            case \"$systemtype\" in\n            *x64*)\n                e_winbase=\"C:\\\\\\\\Program Files (x86)\\\\\\\\Suricata\"\n                ;;\n            *)\n                e_winbase=\"C:\\\\\\\\Program Files\\\\\\\\Suricata\"\n                ;;\n        esac\n    esac\n\n    e_sysconfdir=\"${e_winbase}\\\\\\\\\"\n    e_defaultruledir=\"$e_winbase\\\\\\\\rules\\\\\\\\\"\n    e_magic_file=\"$e_winbase\\\\\\\\magic.mgc\"\n    e_logdir=\"$e_winbase\\\\\\\\log\"\n    e_logfilesdir=\"$e_logdir\\\\\\\\files\"\n    e_logcertsdir=\"$e_logdir\\\\\\\\certs\"\n    e_datarulesdir=\"$e_winbase\\\\\\\\rules\\\\\\\\\"\n    if test \"x$HAVE_CYGPATH\" != \"xno\"; then\n        # turn srcdir into abs path and convert to the\n        # mixed output (/c/Users/dev into  c:/Users/dev)\n        e_rustdir=\"$(cygpath -a -t mixed ${srcdir})/rust\"\n    else\n        e_abs_srcdir=$(cd $srcdir && pwd)\n        e_rustdir=\"$e_abs_srcdir/rust\"\n    fi\nelse\n    EXPAND_VARIABLE(localstatedir, e_logdir, \"/log/suricata/\")\n    EXPAND_VARIABLE(localstatedir, e_rundir, \"/run/\")\n    EXPAND_VARIABLE(localstatedir, e_logfilesdir, \"/log/suricata/files\")\n    EXPAND_VARIABLE(localstatedir, e_logcertsdir, \"/log/suricata/certs\")\n    EXPAND_VARIABLE(sysconfdir, e_sysconfdir, \"/suricata/\")\n    EXPAND_VARIABLE(localstatedir, e_localstatedir, \"/run/suricata\")\n    EXPAND_VARIABLE(datadir, e_datarulesdir, \"/suricata/rules\")\n    EXPAND_VARIABLE(localstatedir, e_datadir, \"/lib/suricata/data\")\n    EXPAND_VARIABLE(localstatedir, e_defaultruledir, \"/lib/suricata/rules\")\n\n    e_abs_srcdir=$(cd $srcdir && pwd)\n    EXPAND_VARIABLE(e_abs_srcdir, e_rustdir, \"/rust\")\nfi\nAC_SUBST(e_logdir)\nAC_SUBST(e_rundir)\nAC_SUBST(e_logfilesdir)\nAC_SUBST(e_logcertsdir)\nAC_SUBST(e_sysconfdir)\nAC_DEFINE_UNQUOTED([CONFIG_DIR],[\"$e_sysconfdir\"],[Our CONFIG_DIR])\nAC_SUBST(e_localstatedir)\nAC_SUBST(e_datadir)\nAC_DEFINE_UNQUOTED([DATA_DIR],[\"$e_datadir\"],[Our DATA_DIR])\nAC_SUBST(e_magic_file)\nAC_SUBST(e_magic_file_comment)\nAC_SUBST(e_enable_evelog)\nAC_SUBST(e_datarulesdir)\nAC_SUBST(e_defaultruledir)\nAC_SUBST(e_rustdir)\n\nEXPAND_VARIABLE(prefix, CONFIGURE_PREFIX)\nEXPAND_VARIABLE(sysconfdir, CONFIGURE_SYSCONDIR)\nEXPAND_VARIABLE(localstatedir, CONFIGURE_LOCALSTATEDIR)\nEXPAND_VARIABLE(datadir, CONFIGURE_DATAROOTDIR)\nAC_SUBST(CONFIGURE_PREFIX)\nAC_SUBST(CONFIGURE_SYSCONDIR)\nAC_SUBST(CONFIGURE_LOCALSTATEDIR)\nAC_SUBST(CONFIGURE_DATAROOTDIR)\nAC_SUBST(PACKAGE_VERSION)\nAC_SUBST(MAJOR_MINOR)\nAC_SUBST(RUST_FEATURES)\nAC_SUBST(RUST_SURICATA_LIBDIR)\nAC_SUBST(RUST_SURICATA_LIBNAME)\nAC_SUBST(enable_non_bundled_htp)\n\nAM_CONDITIONAL([BUILD_SHARED_LIBRARY], [test \"x$enable_shared\" = \"xyes\"] && [test \"x$can_build_shared_library\" = \"xyes\"])\n\nAC_CONFIG_FILES(Makefile src/Makefile rust/Makefile rust/Cargo.lock rust/Cargo.toml rust/derive/Cargo.toml rust/.cargo/config.toml)\nAC_CONFIG_FILES(qa/Makefile qa/coccinelle/Makefile)\nAC_CONFIG_FILES(rules/Makefile doc/Makefile doc/userguide/Makefile)\nAC_CONFIG_FILES(contrib/Makefile contrib/file_processor/Makefile contrib/file_processor/Action/Makefile contrib/file_processor/Processor/Makefile)\nAC_CONFIG_FILES(suricata.yaml etc/Makefile etc/suricata.logrotate etc/suricata.service)\nAC_CONFIG_FILES(python/Makefile python/suricata/config/defaults.py)\nAC_CONFIG_FILES(ebpf/Makefile)\nAC_CONFIG_FILES(libsuricata-config)\nAC_CONFIG_FILES(examples/plugins/c-json-filetype/Makefile)\nAC_CONFIG_FILES(examples/plugins/c-custom-loggers/Makefile)\nAC_CONFIG_FILES(examples/plugins/ci-capture/Makefile)\nAC_CONFIG_FILES(examples/lib/simple/Makefile examples/lib/simple/Makefile.example)\nAC_CONFIG_FILES(plugins/Makefile)\nAC_CONFIG_FILES(plugins/pfring/Makefile)\nAC_CONFIG_FILES(plugins/napatech/Makefile)\n\nAC_OUTPUT\n\nSURICATA_BUILD_CONF=\"Suricata Configuration:\n  AF_PACKET support:                       ${enable_af_packet}\n  AF_XDP support:                          ${enable_af_xdp}\n  DPDK support:                            ${enable_dpdk}\n  eBPF support:                            ${enable_ebpf}\n  XDP support:                             ${have_xdp}\n  PF_RING support:                         ${enable_pfring}\n  NFQueue support:                         ${enable_nfqueue}\n  NFLOG support:                           ${enable_nflog}\n  IPFW support:                            ${enable_ipfw}\n  Netmap support:                          ${enable_netmap} ${have_netmap_version}\n  DAG enabled:                             ${enable_dag}\n  Napatech enabled:                        ${enable_napatech}\n  WinDivert enabled:                       ${enable_windivert}\n\n  Unix socket enabled:                     ${enable_unixsocket}\n  Detection enabled:                       ${enable_detection}\n\n  Libmagic support:                        ${enable_magic}\n  libjansson support:                      ${enable_jansson}\n  hiredis support:                         ${enable_hiredis}\n  hiredis async with libevent:             ${enable_hiredis_async}\n  PCRE jit:                                ${pcre2_jit_available}\n  GeoIP2 support:                          ${enable_geoip}\n  JA3 support:                             ${enable_ja3}\n  JA4 support:                             ${enable_ja4}\n  Non-bundled htp:                         ${enable_non_bundled_htp}\n  Hyperscan support:                       ${enable_hyperscan}\n  Libnet support:                          ${enable_libnet}\n  liblz4 support:                          ${enable_liblz4}\n  Landlock support:                        ${enable_landlock}\n  Systemd support:                         ${enable_systemd}\n\n  Rust strict mode:                        ${enable_rust_strict}\n  Rust compiler path:                      ${RUSTC}\n  Rust compiler version:                   ${rust_compiler_version}\n  Cargo path:                              ${CARGO}\n  Cargo version:                           ${cargo_version_output}\n\n  Python support:                          ${enable_python}\n  Python path:                             ${python_path}\n  Install suricatactl:                     ${install_suricatactl}\n  Install suricatasc:                      ${install_suricatactl}\n  Install suricata-update:                 ${install_suricata_update}${install_suricata_update_reason}\n\n  Profiling enabled:                       ${enable_profiling}\n  Profiling locks enabled:                 ${enable_profiling_locks}\n  Profiling rules enabled:                 ${enable_profiling_rules}\n\n  Plugin support (experimental):           ${plugin_support}\n  DPDK Bond PMD:                           ${enable_dpdk_bond_pmd}\n\nDevelopment settings:\n  Coccinelle / spatch:                     ${enable_coccinelle}\n  Unit tests enabled:                      ${enable_unittests}\n  Debug output enabled:                    ${enable_debug}\n  Debug validation enabled:                ${enable_debug_validation}\n  Fuzz targets enabled:                    ${enable_fuzztargets}\n\nGeneric build parameters:\n  Installation prefix:                     ${prefix}\n  Configuration directory:                 ${e_sysconfdir}\n  Log directory:                           ${e_logdir}\n\n  --prefix                                 ${CONFIGURE_PREFIX}\n  --sysconfdir                             ${CONFIGURE_SYSCONDIR}\n  --localstatedir                          ${CONFIGURE_LOCALSTATEDIR}\n  --datarootdir                            ${CONFIGURE_DATAROOTDIR}\n\n  Host:                                    ${host}\n  Compiler:                                ${CC} (exec name) / ${compiler} (real)\n  GCC Protect enabled:                     ${enable_gccprotect}\n  GCC march native enabled:                ${enable_gccmarch_native}\n  GCC Profile enabled:                     ${enable_gccprofile}\n  Position Independent Executable enabled: ${enable_pie}\n  CFLAGS                                   ${CFLAGS}\n  PCAP_CFLAGS                              ${PCAP_CFLAGS}\n  SECCFLAGS                                ${SECCFLAGS}\"\n\necho\necho \"$SURICATA_BUILD_CONF\"\necho \"printf(\" >src/build-info.h\necho \"$SURICATA_BUILD_CONF\" | sed -e 's/^/\"/' | sed -e 's/$/\\\\n\"/' >>src/build-info.h\necho \");\" >>src/build-info.h\n\necho \"\nTo build and install run 'make' and 'make install'.\n\nYou can run 'make install-conf' if you want to install initial configuration\nfiles to ${e_sysconfdir}. Running 'make install-full' will install configuration\nand rules and provide you a ready-to-run suricata.\"\necho\necho \"To install Suricata into /usr/bin/suricata, have the config in\n/etc/suricata and use /var/log/suricata as log dir, use:\n./configure --prefix=/usr/ --sysconfdir=/etc/ --localstatedir=/var/\"\necho\n"
        },
        {
          "name": "contrib",
          "type": "tree",
          "content": null
        },
        {
          "name": "doc",
          "type": "tree",
          "content": null
        },
        {
          "name": "doxygen.cfg",
          "type": "blob",
          "size": 108.515625,
          "content": "# Doxyfile 1.8.17\n\n# This file describes the settings to be used by the documentation system\n# doxygen (www.doxygen.org) for a project.\n#\n# All text after a double hash (##) is considered a comment and is placed in\n# front of the TAG it is preceding.\n#\n# All text after a single hash (#) is considered a comment and will be ignored.\n# The format is:\n# TAG = value [value, ...]\n# For lists, items can also be appended using:\n# TAG += value [value, ...]\n# Values that contain spaces should be placed between quotes (\\\" \\\").\n\n#---------------------------------------------------------------------------\n# Project related configuration options\n#---------------------------------------------------------------------------\n\n# This tag specifies the encoding used for all characters in the configuration\n# file that follow. The default is UTF-8 which is also the encoding used for all\n# text before the first occurrence of this tag. Doxygen uses libiconv (or the\n# iconv built into libc) for the transcoding. See\n# https://www.gnu.org/software/libiconv/ for the list of possible encodings.\n# The default value is: UTF-8.\n\nDOXYFILE_ENCODING      = UTF-8\n\n# The PROJECT_NAME tag is a single word (or a sequence of words surrounded by\n# double-quotes, unless you are using Doxywizard) that should identify the\n# project for which the documentation is generated. This name is used in the\n# title of most generated pages and in a few other places.\n# The default value is: My Project.\n\nPROJECT_NAME           = \"suricata\"\n\n# The PROJECT_NUMBER tag can be used to enter a project or revision number. This\n# could be handy for archiving the generated documentation or if some version\n# control system is used.\n\nPROJECT_NUMBER         =\n\n# Using the PROJECT_BRIEF tag one can provide an optional one line description\n# for a project that appears at the top of each page and should give viewer a\n# quick idea about the purpose of the project. Keep the description short.\n\nPROJECT_BRIEF          =\n\n# With the PROJECT_LOGO tag one can specify a logo or an icon that is included\n# in the documentation. The maximum height of the logo should not exceed 55\n# pixels and the maximum width should not exceed 200 pixels. Doxygen will copy\n# the logo to the output directory.\n\nPROJECT_LOGO           =\n\n# The OUTPUT_DIRECTORY tag is used to specify the (relative or absolute) path\n# into which the generated documentation will be written. If a relative path is\n# entered, it will be relative to the location where doxygen was started. If\n# left blank the current directory will be used.\n\nOUTPUT_DIRECTORY       = \"doc/doxygen\"\n\n# If the CREATE_SUBDIRS tag is set to YES then doxygen will create 4096 sub-\n# directories (in 2 levels) under the output directory of each output format and\n# will distribute the generated files over these directories. Enabling this\n# option can be useful when feeding doxygen a huge amount of source files, where\n# putting all generated files in the same directory would otherwise causes\n# performance problems for the file system.\n# The default value is: NO.\n\nCREATE_SUBDIRS         = NO\n\n# If the ALLOW_UNICODE_NAMES tag is set to YES, doxygen will allow non-ASCII\n# characters to appear in the names of generated files. If set to NO, non-ASCII\n# characters will be escaped, for example _xE3_x81_x84 will be used for Unicode\n# U+3044.\n# The default value is: NO.\n\nALLOW_UNICODE_NAMES    = NO\n\n# The OUTPUT_LANGUAGE tag is used to specify the language in which all\n# documentation generated by doxygen is written. Doxygen will use this\n# information to generate all constant output in the proper language.\n# Possible values are: Afrikaans, Arabic, Armenian, Brazilian, Catalan, Chinese,\n# Chinese-Traditional, Croatian, Czech, Danish, Dutch, English (United States),\n# Esperanto, Farsi (Persian), Finnish, French, German, Greek, Hungarian,\n# Indonesian, Italian, Japanese, Japanese-en (Japanese with English messages),\n# Korean, Korean-en (Korean with English messages), Latvian, Lithuanian,\n# Macedonian, Norwegian, Persian (Farsi), Polish, Portuguese, Romanian, Russian,\n# Serbian, Serbian-Cyrillic, Slovak, Slovene, Spanish, Swedish, Turkish,\n# Ukrainian and Vietnamese.\n# The default value is: English.\n\nOUTPUT_LANGUAGE        = English\n\n# The OUTPUT_TEXT_DIRECTION tag is used to specify the direction in which all\n# documentation generated by doxygen is written. Doxygen will use this\n# information to generate all generated output in the proper direction.\n# Possible values are: None, LTR, RTL and Context.\n# The default value is: None.\n\nOUTPUT_TEXT_DIRECTION  = None\n\n# If the BRIEF_MEMBER_DESC tag is set to YES, doxygen will include brief member\n# descriptions after the members that are listed in the file and class\n# documentation (similar to Javadoc). Set to NO to disable this.\n# The default value is: YES.\n\nBRIEF_MEMBER_DESC      = YES\n\n# If the REPEAT_BRIEF tag is set to YES, doxygen will prepend the brief\n# description of a member or function before the detailed description\n#\n# Note: If both HIDE_UNDOC_MEMBERS and BRIEF_MEMBER_DESC are set to NO, the\n# brief descriptions will be completely suppressed.\n# The default value is: YES.\n\nREPEAT_BRIEF           = YES\n\n# This tag implements a quasi-intelligent brief description abbreviator that is\n# used to form the text in various listings. Each string in this list, if found\n# as the leading text of the brief description, will be stripped from the text\n# and the result, after processing the whole list, is used as the annotated\n# text. Otherwise, the brief description is used as-is. If left blank, the\n# following values are used ($name is automatically replaced with the name of\n# the entity):The $name class, The $name widget, The $name file, is, provides,\n# specifies, contains, represents, a, an and the.\n\nABBREVIATE_BRIEF       = \"The $name class\" \\\n                         \"The $name widget\" \\\n                         \"The $name file\" \\\n                         is \\\n                         provides \\\n                         specifies \\\n                         contains \\\n                         represents \\\n                         a \\\n                         an \\\n                         the\n\n# If the ALWAYS_DETAILED_SEC and REPEAT_BRIEF tags are both set to YES then\n# doxygen will generate a detailed section even if there is only a brief\n# description.\n# The default value is: NO.\n\nALWAYS_DETAILED_SEC    = NO\n\n# If the INLINE_INHERITED_MEMB tag is set to YES, doxygen will show all\n# inherited members of a class in the documentation of that class as if those\n# members were ordinary class members. Constructors, destructors and assignment\n# operators of the base classes will not be shown.\n# The default value is: NO.\n\nINLINE_INHERITED_MEMB  = NO\n\n# If the FULL_PATH_NAMES tag is set to YES, doxygen will prepend the full path\n# before files name in the file list and in the header files. If set to NO the\n# shortest path that makes the file name unique will be used\n# The default value is: YES.\n\nFULL_PATH_NAMES        = YES\n\n# The STRIP_FROM_PATH tag can be used to strip a user-defined part of the path.\n# Stripping is only done if one of the specified strings matches the left-hand\n# part of the path. The tag can be used to show relative paths in the file list.\n# If left blank the directory from which doxygen is run is used as the path to\n# strip.\n#\n# Note that you can specify absolute paths here, but also relative paths, which\n# will be relative from the directory where doxygen is started.\n# This tag requires that the tag FULL_PATH_NAMES is set to YES.\n\nSTRIP_FROM_PATH        =\n\n# The STRIP_FROM_INC_PATH tag can be used to strip a user-defined part of the\n# path mentioned in the documentation of a class, which tells the reader which\n# header file to include in order to use a class. If left blank only the name of\n# the header file containing the class definition is used. Otherwise one should\n# specify the list of include paths that are normally passed to the compiler\n# using the -I flag.\n\nSTRIP_FROM_INC_PATH    =\n\n# If the SHORT_NAMES tag is set to YES, doxygen will generate much shorter (but\n# less readable) file names. This can be useful is your file systems doesn't\n# support long names like on DOS, Mac, or CD-ROM.\n# The default value is: NO.\n\nSHORT_NAMES            = NO\n\n# If the JAVADOC_AUTOBRIEF tag is set to YES then doxygen will interpret the\n# first line (until the first dot) of a Javadoc-style comment as the brief\n# description. If set to NO, the Javadoc-style will behave just like regular Qt-\n# style comments (thus requiring an explicit @brief command for a brief\n# description.)\n# The default value is: NO.\n\nJAVADOC_AUTOBRIEF      = NO\n\n# If the JAVADOC_BANNER tag is set to YES then doxygen will interpret a line\n# such as\n# /***************\n# as being the beginning of a Javadoc-style comment \"banner\". If set to NO, the\n# Javadoc-style will behave just like regular comments and it will not be\n# interpreted by doxygen.\n# The default value is: NO.\n\nJAVADOC_BANNER         = NO\n\n# If the QT_AUTOBRIEF tag is set to YES then doxygen will interpret the first\n# line (until the first dot) of a Qt-style comment as the brief description. If\n# set to NO, the Qt-style will behave just like regular Qt-style comments (thus\n# requiring an explicit \\brief command for a brief description.)\n# The default value is: NO.\n\nQT_AUTOBRIEF           = NO\n\n# The MULTILINE_CPP_IS_BRIEF tag can be set to YES to make doxygen treat a\n# multi-line C++ special comment block (i.e. a block of //! or /// comments) as\n# a brief description. This used to be the default behavior. The new default is\n# to treat a multi-line C++ comment block as a detailed description. Set this\n# tag to YES if you prefer the old behavior instead.\n#\n# Note that setting this tag to YES also means that rational rose comments are\n# not recognized any more.\n# The default value is: NO.\n\nMULTILINE_CPP_IS_BRIEF = NO\n\n# If the INHERIT_DOCS tag is set to YES then an undocumented member inherits the\n# documentation from any documented member that it re-implements.\n# The default value is: YES.\n\nINHERIT_DOCS           = YES\n\n# If the SEPARATE_MEMBER_PAGES tag is set to YES then doxygen will produce a new\n# page for each member. If set to NO, the documentation of a member will be part\n# of the file/class/namespace that contains it.\n# The default value is: NO.\n\nSEPARATE_MEMBER_PAGES  = NO\n\n# The TAB_SIZE tag can be used to set the number of spaces in a tab. Doxygen\n# uses this value to replace tabs by spaces in code fragments.\n# Minimum value: 1, maximum value: 16, default value: 4.\n\nTAB_SIZE               = 4\n\n# This tag can be used to specify a number of aliases that act as commands in\n# the documentation. An alias has the form:\n# name=value\n# For example adding\n# \"sideeffect=@par Side Effects:\\n\"\n# will allow you to put the command \\sideeffect (or @sideeffect) in the\n# documentation, which will result in a user-defined paragraph with heading\n# \"Side Effects:\". You can put \\n's in the value part of an alias to insert\n# newlines (in the resulting output). You can put ^^ in the value part of an\n# alias to insert a newline as if a physical newline was in the original file.\n# When you need a literal { or } or , in the value part of an alias you have to\n# escape them by means of a backslash (\\), this can lead to conflicts with the\n# commands \\{ and \\} for these it is advised to use the version @{ and @} or use\n# a double escape (\\\\{ and \\\\})\n\nALIASES                =\n\n# This tag can be used to specify a number of word-keyword mappings (TCL only).\n# A mapping has the form \"name=value\". For example adding \"class=itcl::class\"\n# will allow you to use the command class in the itcl::class meaning.\n\nTCL_SUBST              =\n\n# Set the OPTIMIZE_OUTPUT_FOR_C tag to YES if your project consists of C sources\n# only. Doxygen will then generate output that is more tailored for C. For\n# instance, some of the names that are used will be different. The list of all\n# members will be omitted, etc.\n# The default value is: NO.\n\nOPTIMIZE_OUTPUT_FOR_C  = YES\n\n# Set the OPTIMIZE_OUTPUT_JAVA tag to YES if your project consists of Java or\n# Python sources only. Doxygen will then generate output that is more tailored\n# for that language. For instance, namespaces will be presented as packages,\n# qualified scopes will look different, etc.\n# The default value is: NO.\n\nOPTIMIZE_OUTPUT_JAVA   = NO\n\n# Set the OPTIMIZE_FOR_FORTRAN tag to YES if your project consists of Fortran\n# sources. Doxygen will then generate output that is tailored for Fortran.\n# The default value is: NO.\n\nOPTIMIZE_FOR_FORTRAN   = NO\n\n# Set the OPTIMIZE_OUTPUT_VHDL tag to YES if your project consists of VHDL\n# sources. Doxygen will then generate output that is tailored for VHDL.\n# The default value is: NO.\n\nOPTIMIZE_OUTPUT_VHDL   = NO\n\n# Set the OPTIMIZE_OUTPUT_SLICE tag to YES if your project consists of Slice\n# sources only. Doxygen will then generate output that is more tailored for that\n# language. For instance, namespaces will be presented as modules, types will be\n# separated into more groups, etc.\n# The default value is: NO.\n\nOPTIMIZE_OUTPUT_SLICE  = NO\n\n# Doxygen selects the parser to use depending on the extension of the files it\n# parses. With this tag you can assign which parser to use for a given\n# extension. Doxygen has a built-in mapping, but you can override or extend it\n# using this tag. The format is ext=language, where ext is a file extension, and\n# language is one of the parsers supported by doxygen: IDL, Java, JavaScript,\n# Csharp (C#), C, C++, D, PHP, md (Markdown), Objective-C, Python, Slice,\n# Fortran (fixed format Fortran: FortranFixed, free formatted Fortran:\n# FortranFree, unknown formatted Fortran: Fortran. In the later case the parser\n# tries to guess whether the code is fixed or free formatted code, this is the\n# default for Fortran type files), VHDL, tcl. For instance to make doxygen treat\n# .inc files as Fortran files (default is PHP), and .f files as C (default is\n# Fortran), use: inc=Fortran f=C.\n#\n# Note: For files without extension you can use no_extension as a placeholder.\n#\n# Note that for custom extensions you also need to set FILE_PATTERNS otherwise\n# the files are not read by doxygen.\n\nEXTENSION_MAPPING      =\n\n# If the MARKDOWN_SUPPORT tag is enabled then doxygen pre-processes all comments\n# according to the Markdown format, which allows for more readable\n# documentation. See https://daringfireball.net/projects/markdown/ for details.\n# The output of markdown processing is further processed by doxygen, so you can\n# mix doxygen, HTML, and XML commands with Markdown formatting. Disable only in\n# case of backward compatibilities issues.\n# The default value is: YES.\n\nMARKDOWN_SUPPORT       = YES\n\n# When the TOC_INCLUDE_HEADINGS tag is set to a non-zero value, all headings up\n# to that level are automatically included in the table of contents, even if\n# they do not have an id attribute.\n# Note: This feature currently applies only to Markdown headings.\n# Minimum value: 0, maximum value: 99, default value: 5.\n# This tag requires that the tag MARKDOWN_SUPPORT is set to YES.\n\nTOC_INCLUDE_HEADINGS   = 5\n\n# When enabled doxygen tries to link words that correspond to documented\n# classes, or namespaces to their corresponding documentation. Such a link can\n# be prevented in individual cases by putting a % sign in front of the word or\n# globally by setting AUTOLINK_SUPPORT to NO.\n# The default value is: YES.\n\nAUTOLINK_SUPPORT       = YES\n\n# If you use STL classes (i.e. std::string, std::vector, etc.) but do not want\n# to include (a tag file for) the STL sources as input, then you should set this\n# tag to YES in order to let doxygen match functions declarations and\n# definitions whose arguments contain STL classes (e.g. func(std::string);\n# versus func(std::string) {}). This also make the inheritance and collaboration\n# diagrams that involve STL classes more complete and accurate.\n# The default value is: NO.\n\nBUILTIN_STL_SUPPORT    = NO\n\n# If you use Microsoft's C++/CLI language, you should set this option to YES to\n# enable parsing support.\n# The default value is: NO.\n\nCPP_CLI_SUPPORT        = NO\n\n# Set the SIP_SUPPORT tag to YES if your project consists of sip (see:\n# https://www.riverbankcomputing.com/software/sip/intro) sources only. Doxygen\n# will parse them like normal C++ but will assume all classes use public instead\n# of private inheritance when no explicit protection keyword is present.\n# The default value is: NO.\n\nSIP_SUPPORT            = NO\n\n# For Microsoft's IDL there are propget and propput attributes to indicate\n# getter and setter methods for a property. Setting this option to YES will make\n# doxygen to replace the get and set methods by a property in the documentation.\n# This will only work if the methods are indeed getting or setting a simple\n# type. If this is not the case, or you want to show the methods anyway, you\n# should set this option to NO.\n# The default value is: YES.\n\nIDL_PROPERTY_SUPPORT   = YES\n\n# If member grouping is used in the documentation and the DISTRIBUTE_GROUP_DOC\n# tag is set to YES then doxygen will reuse the documentation of the first\n# member in the group (if any) for the other members of the group. By default\n# all members of a group must be documented explicitly.\n# The default value is: NO.\n\nDISTRIBUTE_GROUP_DOC   = NO\n\n# If one adds a struct or class to a group and this option is enabled, then also\n# any nested class or struct is added to the same group. By default this option\n# is disabled and one has to add nested compounds explicitly via \\ingroup.\n# The default value is: NO.\n\nGROUP_NESTED_COMPOUNDS = NO\n\n# Set the SUBGROUPING tag to YES to allow class member groups of the same type\n# (for instance a group of public functions) to be put as a subgroup of that\n# type (e.g. under the Public Functions section). Set it to NO to prevent\n# subgrouping. Alternatively, this can be done per class using the\n# \\nosubgrouping command.\n# The default value is: YES.\n\nSUBGROUPING            = YES\n\n# When the INLINE_GROUPED_CLASSES tag is set to YES, classes, structs and unions\n# are shown inside the group in which they are included (e.g. using \\ingroup)\n# instead of on a separate page (for HTML and Man pages) or section (for LaTeX\n# and RTF).\n#\n# Note that this feature does not work in combination with\n# SEPARATE_MEMBER_PAGES.\n# The default value is: NO.\n\nINLINE_GROUPED_CLASSES = NO\n\n# When the INLINE_SIMPLE_STRUCTS tag is set to YES, structs, classes, and unions\n# with only public data fields or simple typedef fields will be shown inline in\n# the documentation of the scope in which they are defined (i.e. file,\n# namespace, or group documentation), provided this scope is documented. If set\n# to NO, structs, classes, and unions are shown on a separate page (for HTML and\n# Man pages) or section (for LaTeX and RTF).\n# The default value is: NO.\n\nINLINE_SIMPLE_STRUCTS  = NO\n\n# When TYPEDEF_HIDES_STRUCT tag is enabled, a typedef of a struct, union, or\n# enum is documented as struct, union, or enum with the name of the typedef. So\n# typedef struct TypeS {} TypeT, will appear in the documentation as a struct\n# with name TypeT. When disabled the typedef will appear as a member of a file,\n# namespace, or class. And the struct will be named TypeS. This can typically be\n# useful for C code in case the coding convention dictates that all compound\n# types are typedef'ed and only the typedef is referenced, never the tag name.\n# The default value is: NO.\n\nTYPEDEF_HIDES_STRUCT   = NO\n\n# The size of the symbol lookup cache can be set using LOOKUP_CACHE_SIZE. This\n# cache is used to resolve symbols given their name and scope. Since this can be\n# an expensive process and often the same symbol appears multiple times in the\n# code, doxygen keeps a cache of pre-resolved symbols. If the cache is too small\n# doxygen will become slower. If the cache is too large, memory is wasted. The\n# cache size is given by this formula: 2^(16+LOOKUP_CACHE_SIZE). The valid range\n# is 0..9, the default is 0, corresponding to a cache size of 2^16=65536\n# symbols. At the end of a run doxygen will report the cache usage and suggest\n# the optimal cache size from a speed point of view.\n# Minimum value: 0, maximum value: 9, default value: 0.\n\nLOOKUP_CACHE_SIZE      = 0\n\n#---------------------------------------------------------------------------\n# Build related configuration options\n#---------------------------------------------------------------------------\n\n# If the EXTRACT_ALL tag is set to YES, doxygen will assume all entities in\n# documentation are documented, even if no documentation was available. Private\n# class members and static file members will be hidden unless the\n# EXTRACT_PRIVATE respectively EXTRACT_STATIC tags are set to YES.\n# Note: This will also disable the warnings about undocumented members that are\n# normally produced when WARNINGS is set to YES.\n# The default value is: NO.\n\nEXTRACT_ALL            = YES\n\n# If the EXTRACT_PRIVATE tag is set to YES, all private members of a class will\n# be included in the documentation.\n# The default value is: NO.\n\nEXTRACT_PRIVATE        = NO\n\n# If the EXTRACT_PRIV_VIRTUAL tag is set to YES, documented private virtual\n# methods of a class will be included in the documentation.\n# The default value is: NO.\n\nEXTRACT_PRIV_VIRTUAL   = NO\n\n# If the EXTRACT_PACKAGE tag is set to YES, all members with package or internal\n# scope will be included in the documentation.\n# The default value is: NO.\n\nEXTRACT_PACKAGE        = NO\n\n# If the EXTRACT_STATIC tag is set to YES, all static members of a file will be\n# included in the documentation.\n# The default value is: NO.\n\nEXTRACT_STATIC         = NO\n\n# If the EXTRACT_LOCAL_CLASSES tag is set to YES, classes (and structs) defined\n# locally in source files will be included in the documentation. If set to NO,\n# only classes defined in header files are included. Does not have any effect\n# for Java sources.\n# The default value is: YES.\n\nEXTRACT_LOCAL_CLASSES  = YES\n\n# This flag is only useful for Objective-C code. If set to YES, local methods,\n# which are defined in the implementation section but not in the interface are\n# included in the documentation. If set to NO, only methods in the interface are\n# included.\n# The default value is: NO.\n\nEXTRACT_LOCAL_METHODS  = NO\n\n# If this flag is set to YES, the members of anonymous namespaces will be\n# extracted and appear in the documentation as a namespace called\n# 'anonymous_namespace{file}', where file will be replaced with the base name of\n# the file that contains the anonymous namespace. By default anonymous namespace\n# are hidden.\n# The default value is: NO.\n\nEXTRACT_ANON_NSPACES   = NO\n\n# If the HIDE_UNDOC_MEMBERS tag is set to YES, doxygen will hide all\n# undocumented members inside documented classes or files. If set to NO these\n# members will be included in the various overviews, but no documentation\n# section is generated. This option has no effect if EXTRACT_ALL is enabled.\n# The default value is: NO.\n\nHIDE_UNDOC_MEMBERS     = NO\n\n# If the HIDE_UNDOC_CLASSES tag is set to YES, doxygen will hide all\n# undocumented classes that are normally visible in the class hierarchy. If set\n# to NO, these classes will be included in the various overviews. This option\n# has no effect if EXTRACT_ALL is enabled.\n# The default value is: NO.\n\nHIDE_UNDOC_CLASSES     = NO\n\n# If the HIDE_FRIEND_COMPOUNDS tag is set to YES, doxygen will hide all friend\n# declarations. If set to NO, these declarations will be included in the\n# documentation.\n# The default value is: NO.\n\nHIDE_FRIEND_COMPOUNDS  = NO\n\n# If the HIDE_IN_BODY_DOCS tag is set to YES, doxygen will hide any\n# documentation blocks found inside the body of a function. If set to NO, these\n# blocks will be appended to the function's detailed documentation block.\n# The default value is: NO.\n\nHIDE_IN_BODY_DOCS      = NO\n\n# The INTERNAL_DOCS tag determines if documentation that is typed after a\n# \\internal command is included. If the tag is set to NO then the documentation\n# will be excluded. Set it to YES to include the internal documentation.\n# The default value is: NO.\n\nINTERNAL_DOCS          = NO\n\n# If the CASE_SENSE_NAMES tag is set to NO then doxygen will only generate file\n# names in lower-case letters. If set to YES, upper-case letters are also\n# allowed. This is useful if you have classes or files whose names only differ\n# in case and if your file system supports case sensitive file names. Windows\n# (including Cygwin) ands Mac users are advised to set this option to NO.\n# The default value is: system dependent.\n\nCASE_SENSE_NAMES       = YES\n\n# If the HIDE_SCOPE_NAMES tag is set to NO then doxygen will show members with\n# their full class and namespace scopes in the documentation. If set to YES, the\n# scope will be hidden.\n# The default value is: NO.\n\nHIDE_SCOPE_NAMES       = NO\n\n# If the HIDE_COMPOUND_REFERENCE tag is set to NO (default) then doxygen will\n# append additional text to a page's title, such as Class Reference. If set to\n# YES the compound reference will be hidden.\n# The default value is: NO.\n\nHIDE_COMPOUND_REFERENCE= NO\n\n# If the SHOW_INCLUDE_FILES tag is set to YES then doxygen will put a list of\n# the files that are included by a file in the documentation of that file.\n# The default value is: YES.\n\nSHOW_INCLUDE_FILES     = YES\n\n# If the SHOW_GROUPED_MEMB_INC tag is set to YES then Doxygen will add for each\n# grouped member an include statement to the documentation, telling the reader\n# which file to include in order to use the member.\n# The default value is: NO.\n\nSHOW_GROUPED_MEMB_INC  = NO\n\n# If the FORCE_LOCAL_INCLUDES tag is set to YES then doxygen will list include\n# files with double quotes in the documentation rather than with sharp brackets.\n# The default value is: NO.\n\nFORCE_LOCAL_INCLUDES   = NO\n\n# If the INLINE_INFO tag is set to YES then a tag [inline] is inserted in the\n# documentation for inline members.\n# The default value is: YES.\n\nINLINE_INFO            = YES\n\n# If the SORT_MEMBER_DOCS tag is set to YES then doxygen will sort the\n# (detailed) documentation of file and class members alphabetically by member\n# name. If set to NO, the members will appear in declaration order.\n# The default value is: YES.\n\nSORT_MEMBER_DOCS       = YES\n\n# If the SORT_BRIEF_DOCS tag is set to YES then doxygen will sort the brief\n# descriptions of file, namespace and class members alphabetically by member\n# name. If set to NO, the members will appear in declaration order. Note that\n# this will also influence the order of the classes in the class list.\n# The default value is: NO.\n\nSORT_BRIEF_DOCS        = NO\n\n# If the SORT_MEMBERS_CTORS_1ST tag is set to YES then doxygen will sort the\n# (brief and detailed) documentation of class members so that constructors and\n# destructors are listed first. If set to NO the constructors will appear in the\n# respective orders defined by SORT_BRIEF_DOCS and SORT_MEMBER_DOCS.\n# Note: If SORT_BRIEF_DOCS is set to NO this option is ignored for sorting brief\n# member documentation.\n# Note: If SORT_MEMBER_DOCS is set to NO this option is ignored for sorting\n# detailed member documentation.\n# The default value is: NO.\n\nSORT_MEMBERS_CTORS_1ST = NO\n\n# If the SORT_GROUP_NAMES tag is set to YES then doxygen will sort the hierarchy\n# of group names into alphabetical order. If set to NO the group names will\n# appear in their defined order.\n# The default value is: NO.\n\nSORT_GROUP_NAMES       = NO\n\n# If the SORT_BY_SCOPE_NAME tag is set to YES, the class list will be sorted by\n# fully-qualified names, including namespaces. If set to NO, the class list will\n# be sorted only by class name, not including the namespace part.\n# Note: This option is not very useful if HIDE_SCOPE_NAMES is set to YES.\n# Note: This option applies only to the class list, not to the alphabetical\n# list.\n# The default value is: NO.\n\nSORT_BY_SCOPE_NAME     = NO\n\n# If the STRICT_PROTO_MATCHING option is enabled and doxygen fails to do proper\n# type resolution of all parameters of a function it will reject a match between\n# the prototype and the implementation of a member function even if there is\n# only one candidate or it is obvious which candidate to choose by doing a\n# simple string match. By disabling STRICT_PROTO_MATCHING doxygen will still\n# accept a match between prototype and implementation in such cases.\n# The default value is: NO.\n\nSTRICT_PROTO_MATCHING  = NO\n\n# The GENERATE_TODOLIST tag can be used to enable (YES) or disable (NO) the todo\n# list. This list is created by putting \\todo commands in the documentation.\n# The default value is: YES.\n\nGENERATE_TODOLIST      = YES\n\n# The GENERATE_TESTLIST tag can be used to enable (YES) or disable (NO) the test\n# list. This list is created by putting \\test commands in the documentation.\n# The default value is: YES.\n\nGENERATE_TESTLIST      = YES\n\n# The GENERATE_BUGLIST tag can be used to enable (YES) or disable (NO) the bug\n# list. This list is created by putting \\bug commands in the documentation.\n# The default value is: YES.\n\nGENERATE_BUGLIST       = YES\n\n# The GENERATE_DEPRECATEDLIST tag can be used to enable (YES) or disable (NO)\n# the deprecated list. This list is created by putting \\deprecated commands in\n# the documentation.\n# The default value is: YES.\n\nGENERATE_DEPRECATEDLIST= YES\n\n# The ENABLED_SECTIONS tag can be used to enable conditional documentation\n# sections, marked by \\if <section_label> ... \\endif and \\cond <section_label>\n# ... \\endcond blocks.\n\nENABLED_SECTIONS       =\n\n# The MAX_INITIALIZER_LINES tag determines the maximum number of lines that the\n# initial value of a variable or macro / define can have for it to appear in the\n# documentation. If the initializer consists of more lines than specified here\n# it will be hidden. Use a value of 0 to hide initializers completely. The\n# appearance of the value of individual variables and macros / defines can be\n# controlled using \\showinitializer or \\hideinitializer command in the\n# documentation regardless of this setting.\n# Minimum value: 0, maximum value: 10000, default value: 30.\n\nMAX_INITIALIZER_LINES  = 30\n\n# Set the SHOW_USED_FILES tag to NO to disable the list of files generated at\n# the bottom of the documentation of classes and structs. If set to YES, the\n# list will mention the files that were used to generate the documentation.\n# The default value is: YES.\n\nSHOW_USED_FILES        = YES\n\n# Set the SHOW_FILES tag to NO to disable the generation of the Files page. This\n# will remove the Files entry from the Quick Index and from the Folder Tree View\n# (if specified).\n# The default value is: YES.\n\nSHOW_FILES             = YES\n\n# Set the SHOW_NAMESPACES tag to NO to disable the generation of the Namespaces\n# page. This will remove the Namespaces entry from the Quick Index and from the\n# Folder Tree View (if specified).\n# The default value is: YES.\n\nSHOW_NAMESPACES        = YES\n\n# The FILE_VERSION_FILTER tag can be used to specify a program or script that\n# doxygen should invoke to get the current version for each file (typically from\n# the version control system). Doxygen will invoke the program by executing (via\n# popen()) the command input-file, where command is the value of the\n# FILE_VERSION_FILTER tag, and input-file is the name of an input file provided\n# by doxygen. Whatever the program writes to standard output is used as the file\n# version. For an example see the documentation.\n\nFILE_VERSION_FILTER    =\n\n# The LAYOUT_FILE tag can be used to specify a layout file which will be parsed\n# by doxygen. The layout file controls the global structure of the generated\n# output files in an output format independent way. To create the layout file\n# that represents doxygen's defaults, run doxygen with the -l option. You can\n# optionally specify a file name after the option, if omitted DoxygenLayout.xml\n# will be used as the name of the layout file.\n#\n# Note that if you run doxygen from a directory containing a file called\n# DoxygenLayout.xml, doxygen will parse it automatically even if the LAYOUT_FILE\n# tag is left empty.\n\nLAYOUT_FILE            =\n\n# The CITE_BIB_FILES tag can be used to specify one or more bib files containing\n# the reference definitions. This must be a list of .bib files. The .bib\n# extension is automatically appended if omitted. This requires the bibtex tool\n# to be installed. See also https://en.wikipedia.org/wiki/BibTeX for more info.\n# For LaTeX the style of the bibliography can be controlled using\n# LATEX_BIB_STYLE. To use this feature you need bibtex and perl available in the\n# search path. See also \\cite for info how to create references.\n\nCITE_BIB_FILES         =\n\n#---------------------------------------------------------------------------\n# Configuration options related to warning and progress messages\n#---------------------------------------------------------------------------\n\n# The QUIET tag can be used to turn on/off the messages that are generated to\n# standard output by doxygen. If QUIET is set to YES this implies that the\n# messages are off.\n# The default value is: NO.\n\nQUIET                  = NO\n\n# The WARNINGS tag can be used to turn on/off the warning messages that are\n# generated to standard error (stderr) by doxygen. If WARNINGS is set to YES\n# this implies that the warnings are on.\n#\n# Tip: Turn warnings on while writing the documentation.\n# The default value is: YES.\n\nWARNINGS               = YES\n\n# If the WARN_IF_UNDOCUMENTED tag is set to YES then doxygen will generate\n# warnings for undocumented members. If EXTRACT_ALL is set to YES then this flag\n# will automatically be disabled.\n# The default value is: YES.\n\nWARN_IF_UNDOCUMENTED   = NO\n\n# If the WARN_IF_DOC_ERROR tag is set to YES, doxygen will generate warnings for\n# potential errors in the documentation, such as not documenting some parameters\n# in a documented function, or documenting parameters that don't exist or using\n# markup commands wrongly.\n# The default value is: YES.\n\nWARN_IF_DOC_ERROR      = YES\n\n# This WARN_NO_PARAMDOC option can be enabled to get warnings for functions that\n# are documented, but have no documentation for their parameters or return\n# value. If set to NO, doxygen will only warn about wrong or incomplete\n# parameter documentation, but not about the absence of documentation. If\n# EXTRACT_ALL is set to YES then this flag will automatically be disabled.\n# The default value is: NO.\n\nWARN_NO_PARAMDOC       = NO\n\n# If the WARN_AS_ERROR tag is set to YES then doxygen will immediately stop when\n# a warning is encountered.\n# The default value is: NO.\n\nWARN_AS_ERROR          = NO\n\n# The WARN_FORMAT tag determines the format of the warning messages that doxygen\n# can produce. The string should contain the $file, $line, and $text tags, which\n# will be replaced by the file and line number from which the warning originated\n# and the warning text. Optionally the format may contain $version, which will\n# be replaced by the version of the file (if it could be obtained via\n# FILE_VERSION_FILTER)\n# The default value is: $file:$line: $text.\n\nWARN_FORMAT            = \"$file:$line: $text\"\n\n# The WARN_LOGFILE tag can be used to specify a file to which warning and error\n# messages should be written. If left blank the output is written to standard\n# error (stderr).\n\nWARN_LOGFILE           =\n\n#---------------------------------------------------------------------------\n# Configuration options related to the input files\n#---------------------------------------------------------------------------\n\n# The INPUT tag is used to specify the files and/or directories that contain\n# documented source files. You may enter file names like myfile.cpp or\n# directories like /usr/src/myproject. Separate the files or directories with\n# spaces. See also FILE_PATTERNS and EXTENSION_MAPPING\n# Note: If this tag is empty the current directory is searched.\n\nINPUT                  = src/ libhtp/htp/ examples/\n\n# This tag can be used to specify the character encoding of the source files\n# that doxygen parses. Internally doxygen uses the UTF-8 encoding. Doxygen uses\n# libiconv (or the iconv built into libc) for the transcoding. See the libiconv\n# documentation (see: https://www.gnu.org/software/libiconv/) for the list of\n# possible encodings.\n# The default value is: UTF-8.\n\nINPUT_ENCODING         = UTF-8\n\n# If the value of the INPUT tag contains directories, you can use the\n# FILE_PATTERNS tag to specify one or more wildcard patterns (like *.cpp and\n# *.h) to filter out the source-files in the directories.\n#\n# Note that for custom extensions or not directly supported extensions you also\n# need to set EXTENSION_MAPPING for the extension otherwise the files are not\n# read by doxygen.\n#\n# If left blank the following patterns are tested:*.c, *.cc, *.cxx, *.cpp,\n# *.c++, *.java, *.ii, *.ixx, *.ipp, *.i++, *.inl, *.idl, *.ddl, *.odl, *.h,\n# *.hh, *.hxx, *.hpp, *.h++, *.cs, *.d, *.php, *.php4, *.php5, *.phtml, *.inc,\n# *.m, *.markdown, *.md, *.mm, *.dox (to be provided as doxygen C comment),\n# *.doc (to be provided as doxygen C comment), *.txt (to be provided as doxygen\n# C comment), *.py, *.pyw, *.f90, *.f95, *.f03, *.f08, *.f, *.for, *.tcl, *.vhd,\n# *.vhdl, *.ucf, *.qsf and *.ice.\n\nFILE_PATTERNS          = *.c \\\n                         *.cc \\\n                         *.cxx \\\n                         *.cpp \\\n                         *.c++ \\\n                         *.java \\\n                         *.ii \\\n                         *.ixx \\\n                         *.ipp \\\n                         *.i++ \\\n                         *.inl \\\n                         *.idl \\\n                         *.ddl \\\n                         *.odl \\\n                         *.h \\\n                         *.hh \\\n                         *.hxx \\\n                         *.hpp \\\n                         *.h++ \\\n                         *.cs \\\n                         *.d \\\n                         *.php \\\n                         *.php4 \\\n                         *.php5 \\\n                         *.phtml \\\n                         *.inc \\\n                         *.m \\\n                         *.markdown \\\n                         *.md \\\n                         *.mm \\\n                         *.dox \\\n                         *.doc \\\n                         *.txt \\\n                         *.py \\\n                         *.pyw \\\n                         *.f90 \\\n                         *.f95 \\\n                         *.f03 \\\n                         *.f08 \\\n                         *.f \\\n                         *.for \\\n                         *.tcl \\\n                         *.vhd \\\n                         *.vhdl \\\n                         *.ucf \\\n                         *.qsf \\\n                         *.ice\n\n# The RECURSIVE tag can be used to specify whether or not subdirectories should\n# be searched for input files as well.\n# The default value is: NO.\n\nRECURSIVE              = YES\n\n# The EXCLUDE tag can be used to specify files and/or directories that should be\n# excluded from the INPUT source files. This way you can easily exclude a\n# subdirectory from a directory tree whose root is specified with the INPUT tag.\n#\n# Note that relative paths are relative to the directory from which doxygen is\n# run.\n\nEXCLUDE                =\n\n# The EXCLUDE_SYMLINKS tag can be used to select whether or not files or\n# directories that are symbolic links (a Unix file system feature) are excluded\n# from the input.\n# The default value is: NO.\n\nEXCLUDE_SYMLINKS       = NO\n\n# If the value of the INPUT tag contains directories, you can use the\n# EXCLUDE_PATTERNS tag to specify one or more wildcard patterns to exclude\n# certain files from those directories.\n#\n# Note that the wildcards are matched against the file with absolute path, so to\n# exclude all test directories for example use the pattern */test/*\n\nEXCLUDE_PATTERNS       =\n\n# The EXCLUDE_SYMBOLS tag can be used to specify one or more symbol names\n# (namespaces, classes, functions, etc.) that should be excluded from the\n# output. The symbol name can be a fully qualified name, a word, or if the\n# wildcard * is used, a substring. Examples: ANamespace, AClass,\n# AClass::ANamespace, ANamespace::*Test\n#\n# Note that the wildcards are matched against the file with absolute path, so to\n# exclude all test directories use the pattern */test/*\n\nEXCLUDE_SYMBOLS        =\n\n# The EXAMPLE_PATH tag can be used to specify one or more files or directories\n# that contain example code fragments that are included (see the \\include\n# command).\n\nEXAMPLE_PATH           =\n\n# If the value of the EXAMPLE_PATH tag contains directories, you can use the\n# EXAMPLE_PATTERNS tag to specify one or more wildcard pattern (like *.cpp and\n# *.h) to filter out the source-files in the directories. If left blank all\n# files are included.\n\nEXAMPLE_PATTERNS       = *\n\n# If the EXAMPLE_RECURSIVE tag is set to YES then subdirectories will be\n# searched for input files to be used with the \\include or \\dontinclude commands\n# irrespective of the value of the RECURSIVE tag.\n# The default value is: NO.\n\nEXAMPLE_RECURSIVE      = NO\n\n# The IMAGE_PATH tag can be used to specify one or more files or directories\n# that contain images that are to be included in the documentation (see the\n# \\image command).\n\nIMAGE_PATH             =\n\n# The INPUT_FILTER tag can be used to specify a program that doxygen should\n# invoke to filter for each input file. Doxygen will invoke the filter program\n# by executing (via popen()) the command:\n#\n# <filter> <input-file>\n#\n# where <filter> is the value of the INPUT_FILTER tag, and <input-file> is the\n# name of an input file. Doxygen will then use the output that the filter\n# program writes to standard output. If FILTER_PATTERNS is specified, this tag\n# will be ignored.\n#\n# Note that the filter must not add or remove lines; it is applied before the\n# code is scanned, but not when the output code is generated. If lines are added\n# or removed, the anchors will not be placed correctly.\n#\n# Note that for custom extensions or not directly supported extensions you also\n# need to set EXTENSION_MAPPING for the extension otherwise the files are not\n# properly processed by doxygen.\n\nINPUT_FILTER           =\n\n# The FILTER_PATTERNS tag can be used to specify filters on a per file pattern\n# basis. Doxygen will compare the file name with each pattern and apply the\n# filter if there is a match. The filters are a list of the form: pattern=filter\n# (like *.cpp=my_cpp_filter). See INPUT_FILTER for further information on how\n# filters are used. If the FILTER_PATTERNS tag is empty or if none of the\n# patterns match the file name, INPUT_FILTER is applied.\n#\n# Note that for custom extensions or not directly supported extensions you also\n# need to set EXTENSION_MAPPING for the extension otherwise the files are not\n# properly processed by doxygen.\n\nFILTER_PATTERNS        =\n\n# If the FILTER_SOURCE_FILES tag is set to YES, the input filter (if set using\n# INPUT_FILTER) will also be used to filter the input files that are used for\n# producing the source files to browse (i.e. when SOURCE_BROWSER is set to YES).\n# The default value is: NO.\n\nFILTER_SOURCE_FILES    = NO\n\n# The FILTER_SOURCE_PATTERNS tag can be used to specify source filters per file\n# pattern. A pattern will override the setting for FILTER_PATTERN (if any) and\n# it is also possible to disable source filtering for a specific pattern using\n# *.ext= (so without naming a filter).\n# This tag requires that the tag FILTER_SOURCE_FILES is set to YES.\n\nFILTER_SOURCE_PATTERNS =\n\n# If the USE_MDFILE_AS_MAINPAGE tag refers to the name of a markdown file that\n# is part of the input, its contents will be placed on the main page\n# (index.html). This can be useful if you have a project on for instance GitHub\n# and want to reuse the introduction page also for the doxygen output.\n\nUSE_MDFILE_AS_MAINPAGE =\n\n#---------------------------------------------------------------------------\n# Configuration options related to source browsing\n#---------------------------------------------------------------------------\n\n# If the SOURCE_BROWSER tag is set to YES then a list of source files will be\n# generated. Documented entities will be cross-referenced with these sources.\n#\n# Note: To get rid of all source code in the generated output, make sure that\n# also VERBATIM_HEADERS is set to NO.\n# The default value is: NO.\n\nSOURCE_BROWSER         = YES\n\n# Setting the INLINE_SOURCES tag to YES will include the body of functions,\n# classes and enums directly into the documentation.\n# The default value is: NO.\n\nINLINE_SOURCES         = NO\n\n# Setting the STRIP_CODE_COMMENTS tag to YES will instruct doxygen to hide any\n# special comment blocks from generated source code fragments. Normal C, C++ and\n# Fortran comments will always remain visible.\n# The default value is: YES.\n\nSTRIP_CODE_COMMENTS    = NO\n\n# If the REFERENCED_BY_RELATION tag is set to YES then for each documented\n# entity all documented functions referencing it will be listed.\n# The default value is: NO.\n\nREFERENCED_BY_RELATION = YES\n\n# If the REFERENCES_RELATION tag is set to YES then for each documented function\n# all documented entities called/used by that function will be listed.\n# The default value is: NO.\n\nREFERENCES_RELATION    = YES\n\n# If the REFERENCES_LINK_SOURCE tag is set to YES and SOURCE_BROWSER tag is set\n# to YES then the hyperlinks from functions in REFERENCES_RELATION and\n# REFERENCED_BY_RELATION lists will link to the source code. Otherwise they will\n# link to the documentation.\n# The default value is: YES.\n\nREFERENCES_LINK_SOURCE = YES\n\n# If SOURCE_TOOLTIPS is enabled (the default) then hovering a hyperlink in the\n# source code will show a tooltip with additional information such as prototype,\n# brief description and links to the definition and documentation. Since this\n# will make the HTML file larger and loading of large files a bit slower, you\n# can opt to disable this feature.\n# The default value is: YES.\n# This tag requires that the tag SOURCE_BROWSER is set to YES.\n\nSOURCE_TOOLTIPS        = YES\n\n# If the USE_HTAGS tag is set to YES then the references to source code will\n# point to the HTML generated by the htags(1) tool instead of doxygen built-in\n# source browser. The htags tool is part of GNU's global source tagging system\n# (see https://www.gnu.org/software/global/global.html). You will need version\n# 4.8.6 or higher.\n#\n# To use it do the following:\n# - Install the latest version of global\n# - Enable SOURCE_BROWSER and USE_HTAGS in the configuration file\n# - Make sure the INPUT points to the root of the source tree\n# - Run doxygen as normal\n#\n# Doxygen will invoke htags (and that will in turn invoke gtags), so these\n# tools must be available from the command line (i.e. in the search path).\n#\n# The result: instead of the source browser generated by doxygen, the links to\n# source code will now point to the output of htags.\n# The default value is: NO.\n# This tag requires that the tag SOURCE_BROWSER is set to YES.\n\nUSE_HTAGS              = NO\n\n# If the VERBATIM_HEADERS tag is set the YES then doxygen will generate a\n# verbatim copy of the header file for each class for which an include is\n# specified. Set to NO to disable this.\n# See also: Section \\class.\n# The default value is: YES.\n\nVERBATIM_HEADERS       = YES\n\n#---------------------------------------------------------------------------\n# Configuration options related to the alphabetical class index\n#---------------------------------------------------------------------------\n\n# If the ALPHABETICAL_INDEX tag is set to YES, an alphabetical index of all\n# compounds will be generated. Enable this if the project contains a lot of\n# classes, structs, unions or interfaces.\n# The default value is: YES.\n\nALPHABETICAL_INDEX     = YES\n\n# The COLS_IN_ALPHA_INDEX tag can be used to specify the number of columns in\n# which the alphabetical index list will be split.\n# Minimum value: 1, maximum value: 20, default value: 5.\n# This tag requires that the tag ALPHABETICAL_INDEX is set to YES.\n\nCOLS_IN_ALPHA_INDEX    = 5\n\n# In case all classes in a project start with a common prefix, all classes will\n# be put under the same header in the alphabetical index. The IGNORE_PREFIX tag\n# can be used to specify a prefix (or a list of prefixes) that should be ignored\n# while generating the index headers.\n# This tag requires that the tag ALPHABETICAL_INDEX is set to YES.\n\nIGNORE_PREFIX          =\n\n#---------------------------------------------------------------------------\n# Configuration options related to the HTML output\n#---------------------------------------------------------------------------\n\n# If the GENERATE_HTML tag is set to YES, doxygen will generate HTML output\n# The default value is: YES.\n\nGENERATE_HTML          = YES\n\n# The HTML_OUTPUT tag is used to specify where the HTML docs will be put. If a\n# relative path is entered the value of OUTPUT_DIRECTORY will be put in front of\n# it.\n# The default directory is: html.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nHTML_OUTPUT            = html\n\n# The HTML_FILE_EXTENSION tag can be used to specify the file extension for each\n# generated HTML page (for example: .htm, .php, .asp).\n# The default value is: .html.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nHTML_FILE_EXTENSION    = .html\n\n# The HTML_HEADER tag can be used to specify a user-defined HTML header file for\n# each generated HTML page. If the tag is left blank doxygen will generate a\n# standard header.\n#\n# To get valid HTML the header file that includes any scripts and style sheets\n# that doxygen needs, which is dependent on the configuration options used (e.g.\n# the setting GENERATE_TREEVIEW). It is highly recommended to start with a\n# default header using\n# doxygen -w html new_header.html new_footer.html new_stylesheet.css\n# YourConfigFile\n# and then modify the file new_header.html. See also section \"Doxygen usage\"\n# for information on how to generate the default header that doxygen normally\n# uses.\n# Note: The header is subject to change so you typically have to regenerate the\n# default header when upgrading to a newer version of doxygen. For a description\n# of the possible markers and block names see the documentation.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nHTML_HEADER            =\n\n# The HTML_FOOTER tag can be used to specify a user-defined HTML footer for each\n# generated HTML page. If the tag is left blank doxygen will generate a standard\n# footer. See HTML_HEADER for more information on how to generate a default\n# footer and what special commands can be used inside the footer. See also\n# section \"Doxygen usage\" for information on how to generate the default footer\n# that doxygen normally uses.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nHTML_FOOTER            =\n\n# The HTML_STYLESHEET tag can be used to specify a user-defined cascading style\n# sheet that is used by each HTML page. It can be used to fine-tune the look of\n# the HTML output. If left blank doxygen will generate a default style sheet.\n# See also section \"Doxygen usage\" for information on how to generate the style\n# sheet that doxygen normally uses.\n# Note: It is recommended to use HTML_EXTRA_STYLESHEET instead of this tag, as\n# it is more robust and this tag (HTML_STYLESHEET) will in the future become\n# obsolete.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nHTML_STYLESHEET        =\n\n# The HTML_EXTRA_STYLESHEET tag can be used to specify additional user-defined\n# cascading style sheets that are included after the standard style sheets\n# created by doxygen. Using this option one can overrule certain style aspects.\n# This is preferred over using HTML_STYLESHEET since it does not replace the\n# standard style sheet and is therefore more robust against future updates.\n# Doxygen will copy the style sheet files to the output directory.\n# Note: The order of the extra style sheet files is of importance (e.g. the last\n# style sheet in the list overrules the setting of the previous ones in the\n# list). For an example see the documentation.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nHTML_EXTRA_STYLESHEET  =\n\n# The HTML_EXTRA_FILES tag can be used to specify one or more extra images or\n# other source files which should be copied to the HTML output directory. Note\n# that these files will be copied to the base HTML output directory. Use the\n# $relpath^ marker in the HTML_HEADER and/or HTML_FOOTER files to load these\n# files. In the HTML_STYLESHEET file, use the file name only. Also note that the\n# files will be copied as-is; there are no commands or markers available.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nHTML_EXTRA_FILES       =\n\n# The HTML_COLORSTYLE_HUE tag controls the color of the HTML output. Doxygen\n# will adjust the colors in the style sheet and background images according to\n# this color. Hue is specified as an angle on a colorwheel, see\n# https://en.wikipedia.org/wiki/Hue for more information. For instance the value\n# 0 represents red, 60 is yellow, 120 is green, 180 is cyan, 240 is blue, 300\n# purple, and 360 is red again.\n# Minimum value: 0, maximum value: 359, default value: 220.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nHTML_COLORSTYLE_HUE    = 220\n\n# The HTML_COLORSTYLE_SAT tag controls the purity (or saturation) of the colors\n# in the HTML output. For a value of 0 the output will use grayscales only. A\n# value of 255 will produce the most vivid colors.\n# Minimum value: 0, maximum value: 255, default value: 100.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nHTML_COLORSTYLE_SAT    = 100\n\n# The HTML_COLORSTYLE_GAMMA tag controls the gamma correction applied to the\n# luminance component of the colors in the HTML output. Values below 100\n# gradually make the output lighter, whereas values above 100 make the output\n# darker. The value divided by 100 is the actual gamma applied, so 80 represents\n# a gamma of 0.8, The value 220 represents a gamma of 2.2, and 100 does not\n# change the gamma.\n# Minimum value: 40, maximum value: 240, default value: 80.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nHTML_COLORSTYLE_GAMMA  = 80\n\n# If the HTML_TIMESTAMP tag is set to YES then the footer of each generated HTML\n# page will contain the date and time when the page was generated. Setting this\n# to YES can help to show when doxygen was last run and thus if the\n# documentation is up to date.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nHTML_TIMESTAMP         = YES\n\n# If the HTML_DYNAMIC_MENUS tag is set to YES then the generated HTML\n# documentation will contain a main index with vertical navigation menus that\n# are dynamically created via JavaScript. If disabled, the navigation index will\n# consists of multiple levels of tabs that are statically embedded in every HTML\n# page. Disable this option to support browsers that do not have JavaScript,\n# like the Qt help browser.\n# The default value is: YES.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nHTML_DYNAMIC_MENUS     = YES\n\n# If the HTML_DYNAMIC_SECTIONS tag is set to YES then the generated HTML\n# documentation will contain sections that can be hidden and shown after the\n# page has loaded.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nHTML_DYNAMIC_SECTIONS  = NO\n\n# With HTML_INDEX_NUM_ENTRIES one can control the preferred number of entries\n# shown in the various tree structured indices initially; the user can expand\n# and collapse entries dynamically later on. Doxygen will expand the tree to\n# such a level that at most the specified number of entries are visible (unless\n# a fully collapsed tree already exceeds this amount). So setting the number of\n# entries 1 will produce a full collapsed tree by default. 0 is a special value\n# representing an infinite number of entries and will result in a full expanded\n# tree by default.\n# Minimum value: 0, maximum value: 9999, default value: 100.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nHTML_INDEX_NUM_ENTRIES = 100\n\n# If the GENERATE_DOCSET tag is set to YES, additional index files will be\n# generated that can be used as input for Apple's Xcode 3 integrated development\n# environment (see: https://developer.apple.com/xcode/), introduced with OSX\n# 10.5 (Leopard). To create a documentation set, doxygen will generate a\n# Makefile in the HTML output directory. Running make will produce the docset in\n# that directory and running make install will install the docset in\n# ~/Library/Developer/Shared/Documentation/DocSets so that Xcode will find it at\n# startup. See https://developer.apple.com/library/archive/featuredarticles/Doxy\n# genXcode/_index.html for more information.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nGENERATE_DOCSET        = NO\n\n# This tag determines the name of the docset feed. A documentation feed provides\n# an umbrella under which multiple documentation sets from a single provider\n# (such as a company or product suite) can be grouped.\n# The default value is: Doxygen generated docs.\n# This tag requires that the tag GENERATE_DOCSET is set to YES.\n\nDOCSET_FEEDNAME        = \"Doxygen generated docs\"\n\n# This tag specifies a string that should uniquely identify the documentation\n# set bundle. This should be a reverse domain-name style string, e.g.\n# com.mycompany.MyDocSet. Doxygen will append .docset to the name.\n# The default value is: org.doxygen.Project.\n# This tag requires that the tag GENERATE_DOCSET is set to YES.\n\nDOCSET_BUNDLE_ID       = org.doxygen.Project\n\n# The DOCSET_PUBLISHER_ID tag specifies a string that should uniquely identify\n# the documentation publisher. This should be a reverse domain-name style\n# string, e.g. com.mycompany.MyDocSet.documentation.\n# The default value is: org.doxygen.Publisher.\n# This tag requires that the tag GENERATE_DOCSET is set to YES.\n\nDOCSET_PUBLISHER_ID    = org.doxygen.Publisher\n\n# The DOCSET_PUBLISHER_NAME tag identifies the documentation publisher.\n# The default value is: Publisher.\n# This tag requires that the tag GENERATE_DOCSET is set to YES.\n\nDOCSET_PUBLISHER_NAME  = Publisher\n\n# If the GENERATE_HTMLHELP tag is set to YES then doxygen generates three\n# additional HTML index files: index.hhp, index.hhc, and index.hhk. The\n# index.hhp is a project file that can be read by Microsoft's HTML Help Workshop\n# (see: https://www.microsoft.com/en-us/download/details.aspx?id=21138) on\n# Windows.\n#\n# The HTML Help Workshop contains a compiler that can convert all HTML output\n# generated by doxygen into a single compiled HTML file (.chm). Compiled HTML\n# files are now used as the Windows 98 help format, and will replace the old\n# Windows help format (.hlp) on all Windows platforms in the future. Compressed\n# HTML files also contain an index, a table of contents, and you can search for\n# words in the documentation. The HTML workshop also contains a viewer for\n# compressed HTML files.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nGENERATE_HTMLHELP      = NO\n\n# The CHM_FILE tag can be used to specify the file name of the resulting .chm\n# file. You can add a path in front of the file if the result should not be\n# written to the html output directory.\n# This tag requires that the tag GENERATE_HTMLHELP is set to YES.\n\nCHM_FILE               =\n\n# The HHC_LOCATION tag can be used to specify the location (absolute path\n# including file name) of the HTML help compiler (hhc.exe). If non-empty,\n# doxygen will try to run the HTML help compiler on the generated index.hhp.\n# The file has to be specified with full path.\n# This tag requires that the tag GENERATE_HTMLHELP is set to YES.\n\nHHC_LOCATION           =\n\n# The GENERATE_CHI flag controls if a separate .chi index file is generated\n# (YES) or that it should be included in the master .chm file (NO).\n# The default value is: NO.\n# This tag requires that the tag GENERATE_HTMLHELP is set to YES.\n\nGENERATE_CHI           = NO\n\n# The CHM_INDEX_ENCODING is used to encode HtmlHelp index (hhk), content (hhc)\n# and project file content.\n# This tag requires that the tag GENERATE_HTMLHELP is set to YES.\n\nCHM_INDEX_ENCODING     =\n\n# The BINARY_TOC flag controls whether a binary table of contents is generated\n# (YES) or a normal table of contents (NO) in the .chm file. Furthermore it\n# enables the Previous and Next buttons.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_HTMLHELP is set to YES.\n\nBINARY_TOC             = NO\n\n# The TOC_EXPAND flag can be set to YES to add extra items for group members to\n# the table of contents of the HTML help documentation and to the tree view.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_HTMLHELP is set to YES.\n\nTOC_EXPAND             = NO\n\n# If the GENERATE_QHP tag is set to YES and both QHP_NAMESPACE and\n# QHP_VIRTUAL_FOLDER are set, an additional index file will be generated that\n# can be used as input for Qt's qhelpgenerator to generate a Qt Compressed Help\n# (.qch) of the generated HTML documentation.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nGENERATE_QHP           = NO\n\n# If the QHG_LOCATION tag is specified, the QCH_FILE tag can be used to specify\n# the file name of the resulting .qch file. The path specified is relative to\n# the HTML output folder.\n# This tag requires that the tag GENERATE_QHP is set to YES.\n\nQCH_FILE               =\n\n# The QHP_NAMESPACE tag specifies the namespace to use when generating Qt Help\n# Project output. For more information please see Qt Help Project / Namespace\n# (see: https://doc.qt.io/archives/qt-4.8/qthelpproject.html#namespace).\n# The default value is: org.doxygen.Project.\n# This tag requires that the tag GENERATE_QHP is set to YES.\n\nQHP_NAMESPACE          = org.doxygen.Project\n\n# The QHP_VIRTUAL_FOLDER tag specifies the namespace to use when generating Qt\n# Help Project output. For more information please see Qt Help Project / Virtual\n# Folders (see: https://doc.qt.io/archives/qt-4.8/qthelpproject.html#virtual-\n# folders).\n# The default value is: doc.\n# This tag requires that the tag GENERATE_QHP is set to YES.\n\nQHP_VIRTUAL_FOLDER     = doc\n\n# If the QHP_CUST_FILTER_NAME tag is set, it specifies the name of a custom\n# filter to add. For more information please see Qt Help Project / Custom\n# Filters (see: https://doc.qt.io/archives/qt-4.8/qthelpproject.html#custom-\n# filters).\n# This tag requires that the tag GENERATE_QHP is set to YES.\n\nQHP_CUST_FILTER_NAME   =\n\n# The QHP_CUST_FILTER_ATTRS tag specifies the list of the attributes of the\n# custom filter to add. For more information please see Qt Help Project / Custom\n# Filters (see: https://doc.qt.io/archives/qt-4.8/qthelpproject.html#custom-\n# filters).\n# This tag requires that the tag GENERATE_QHP is set to YES.\n\nQHP_CUST_FILTER_ATTRS  =\n\n# The QHP_SECT_FILTER_ATTRS tag specifies the list of the attributes this\n# project's filter section matches. Qt Help Project / Filter Attributes (see:\n# https://doc.qt.io/archives/qt-4.8/qthelpproject.html#filter-attributes).\n# This tag requires that the tag GENERATE_QHP is set to YES.\n\nQHP_SECT_FILTER_ATTRS  =\n\n# The QHG_LOCATION tag can be used to specify the location of Qt's\n# qhelpgenerator. If non-empty doxygen will try to run qhelpgenerator on the\n# generated .qhp file.\n# This tag requires that the tag GENERATE_QHP is set to YES.\n\nQHG_LOCATION           =\n\n# If the GENERATE_ECLIPSEHELP tag is set to YES, additional index files will be\n# generated, together with the HTML files, they form an Eclipse help plugin. To\n# install this plugin and make it available under the help contents menu in\n# Eclipse, the contents of the directory containing the HTML and XML files needs\n# to be copied into the plugins directory of eclipse. The name of the directory\n# within the plugins directory should be the same as the ECLIPSE_DOC_ID value.\n# After copying Eclipse needs to be restarted before the help appears.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nGENERATE_ECLIPSEHELP   = NO\n\n# A unique identifier for the Eclipse help plugin. When installing the plugin\n# the directory name containing the HTML and XML files should also have this\n# name. Each documentation set should have its own identifier.\n# The default value is: org.doxygen.Project.\n# This tag requires that the tag GENERATE_ECLIPSEHELP is set to YES.\n\nECLIPSE_DOC_ID         = org.doxygen.Project\n\n# If you want full control over the layout of the generated HTML pages it might\n# be necessary to disable the index and replace it with your own. The\n# DISABLE_INDEX tag can be used to turn on/off the condensed index (tabs) at top\n# of each HTML page. A value of NO enables the index and the value YES disables\n# it. Since the tabs in the index contain the same information as the navigation\n# tree, you can set this option to YES if you also set GENERATE_TREEVIEW to YES.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nDISABLE_INDEX          = NO\n\n# The GENERATE_TREEVIEW tag is used to specify whether a tree-like index\n# structure should be generated to display hierarchical information. If the tag\n# value is set to YES, a side panel will be generated containing a tree-like\n# index structure (just like the one that is generated for HTML Help). For this\n# to work a browser that supports JavaScript, DHTML, CSS and frames is required\n# (i.e. any modern browser). Windows users are probably better off using the\n# HTML help feature. Via custom style sheets (see HTML_EXTRA_STYLESHEET) one can\n# further fine-tune the look of the index. As an example, the default style\n# sheet generated by doxygen has an example that shows how to put an image at\n# the root of the tree instead of the PROJECT_NAME. Since the tree basically has\n# the same information as the tab index, you could consider setting\n# DISABLE_INDEX to YES when enabling this option.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nGENERATE_TREEVIEW      = YES\n\n# The ENUM_VALUES_PER_LINE tag can be used to set the number of enum values that\n# doxygen will group on one line in the generated HTML documentation.\n#\n# Note that a value of 0 will completely suppress the enum values from appearing\n# in the overview section.\n# Minimum value: 0, maximum value: 20, default value: 4.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nENUM_VALUES_PER_LINE   = 4\n\n# If the treeview is enabled (see GENERATE_TREEVIEW) then this tag can be used\n# to set the initial width (in pixels) of the frame in which the tree is shown.\n# Minimum value: 0, maximum value: 1500, default value: 250.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nTREEVIEW_WIDTH         = 250\n\n# If the EXT_LINKS_IN_WINDOW option is set to YES, doxygen will open links to\n# external symbols imported via tag files in a separate window.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nEXT_LINKS_IN_WINDOW    = NO\n\n# Use this tag to change the font size of LaTeX formulas included as images in\n# the HTML documentation. When you change the font size after a successful\n# doxygen run you need to manually remove any form_*.png images from the HTML\n# output directory to force them to be regenerated.\n# Minimum value: 8, maximum value: 50, default value: 10.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nFORMULA_FONTSIZE       = 10\n\n# Use the FORMULA_TRANSPARENT tag to determine whether or not the images\n# generated for formulas are transparent PNGs. Transparent PNGs are not\n# supported properly for IE 6.0, but are supported on all modern browsers.\n#\n# Note that when changing this option you need to delete any form_*.png files in\n# the HTML output directory before the changes have effect.\n# The default value is: YES.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nFORMULA_TRANSPARENT    = YES\n\n# The FORMULA_MACROFILE can contain LaTeX \\newcommand and \\renewcommand commands\n# to create new LaTeX commands to be used in formulas as building blocks. See\n# the section \"Including formulas\" for details.\n\nFORMULA_MACROFILE      =\n\n# Enable the USE_MATHJAX option to render LaTeX formulas using MathJax (see\n# https://www.mathjax.org) which uses client side JavaScript for the rendering\n# instead of using pre-rendered bitmaps. Use this if you do not have LaTeX\n# installed or if you want to formulas look prettier in the HTML output. When\n# enabled you may also need to install MathJax separately and configure the path\n# to it using the MATHJAX_RELPATH option.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nUSE_MATHJAX            = NO\n\n# When MathJax is enabled you can set the default output format to be used for\n# the MathJax output. See the MathJax site (see:\n# http://docs.mathjax.org/en/latest/output.html) for more details.\n# Possible values are: HTML-CSS (which is slower, but has the best\n# compatibility), NativeMML (i.e. MathML) and SVG.\n# The default value is: HTML-CSS.\n# This tag requires that the tag USE_MATHJAX is set to YES.\n\nMATHJAX_FORMAT         = HTML-CSS\n\n# When MathJax is enabled you need to specify the location relative to the HTML\n# output directory using the MATHJAX_RELPATH option. The destination directory\n# should contain the MathJax.js script. For instance, if the mathjax directory\n# is located at the same level as the HTML output directory, then\n# MATHJAX_RELPATH should be ../mathjax. The default value points to the MathJax\n# Content Delivery Network so you can quickly see the result without installing\n# MathJax. However, it is strongly recommended to install a local copy of\n# MathJax from https://www.mathjax.org before deployment.\n# The default value is: https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/.\n# This tag requires that the tag USE_MATHJAX is set to YES.\n\nMATHJAX_RELPATH        = https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/\n\n# The MATHJAX_EXTENSIONS tag can be used to specify one or more MathJax\n# extension names that should be enabled during MathJax rendering. For example\n# MATHJAX_EXTENSIONS = TeX/AMSmath TeX/AMSsymbols\n# This tag requires that the tag USE_MATHJAX is set to YES.\n\nMATHJAX_EXTENSIONS     =\n\n# The MATHJAX_CODEFILE tag can be used to specify a file with javascript pieces\n# of code that will be used on startup of the MathJax code. See the MathJax site\n# (see: http://docs.mathjax.org/en/latest/output.html) for more details. For an\n# example see the documentation.\n# This tag requires that the tag USE_MATHJAX is set to YES.\n\nMATHJAX_CODEFILE       =\n\n# When the SEARCHENGINE tag is enabled doxygen will generate a search box for\n# the HTML output. The underlying search engine uses javascript and DHTML and\n# should work on any modern browser. Note that when using HTML help\n# (GENERATE_HTMLHELP), Qt help (GENERATE_QHP), or docsets (GENERATE_DOCSET)\n# there is already a search function so this one should typically be disabled.\n# For large projects the javascript based search engine can be slow, then\n# enabling SERVER_BASED_SEARCH may provide a better solution. It is possible to\n# search using the keyboard; to jump to the search box use <access key> + S\n# (what the <access key> is depends on the OS and browser, but it is typically\n# <CTRL>, <ALT>/<option>, or both). Inside the search box use the <cursor down\n# key> to jump into the search results window, the results can be navigated\n# using the <cursor keys>. Press <Enter> to select an item or <escape> to cancel\n# the search. The filter options can be selected when the cursor is inside the\n# search box by pressing <Shift>+<cursor down>. Also here use the <cursor keys>\n# to select a filter and <Enter> or <escape> to activate or cancel the filter\n# option.\n# The default value is: YES.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nSEARCHENGINE           = YES\n\n# When the SERVER_BASED_SEARCH tag is enabled the search engine will be\n# implemented using a web server instead of a web client using JavaScript. There\n# are two flavors of web server based searching depending on the EXTERNAL_SEARCH\n# setting. When disabled, doxygen will generate a PHP script for searching and\n# an index file used by the script. When EXTERNAL_SEARCH is enabled the indexing\n# and searching needs to be provided by external tools. See the section\n# \"External Indexing and Searching\" for details.\n# The default value is: NO.\n# This tag requires that the tag SEARCHENGINE is set to YES.\n\nSERVER_BASED_SEARCH    = YES\n\n# When EXTERNAL_SEARCH tag is enabled doxygen will no longer generate the PHP\n# script for searching. Instead the search results are written to an XML file\n# which needs to be processed by an external indexer. Doxygen will invoke an\n# external search engine pointed to by the SEARCHENGINE_URL option to obtain the\n# search results.\n#\n# Doxygen ships with an example indexer (doxyindexer) and search engine\n# (doxysearch.cgi) which are based on the open source search engine library\n# Xapian (see: https://xapian.org/).\n#\n# See the section \"External Indexing and Searching\" for details.\n# The default value is: NO.\n# This tag requires that the tag SEARCHENGINE is set to YES.\n\nEXTERNAL_SEARCH        = NO\n\n# The SEARCHENGINE_URL should point to a search engine hosted by a web server\n# which will return the search results when EXTERNAL_SEARCH is enabled.\n#\n# Doxygen ships with an example indexer (doxyindexer) and search engine\n# (doxysearch.cgi) which are based on the open source search engine library\n# Xapian (see: https://xapian.org/). See the section \"External Indexing and\n# Searching\" for details.\n# This tag requires that the tag SEARCHENGINE is set to YES.\n\nSEARCHENGINE_URL       =\n\n# When SERVER_BASED_SEARCH and EXTERNAL_SEARCH are both enabled the unindexed\n# search data is written to a file for indexing by an external tool. With the\n# SEARCHDATA_FILE tag the name of this file can be specified.\n# The default file is: searchdata.xml.\n# This tag requires that the tag SEARCHENGINE is set to YES.\n\nSEARCHDATA_FILE        = searchdata.xml\n\n# When SERVER_BASED_SEARCH and EXTERNAL_SEARCH are both enabled the\n# EXTERNAL_SEARCH_ID tag can be used as an identifier for the project. This is\n# useful in combination with EXTRA_SEARCH_MAPPINGS to search through multiple\n# projects and redirect the results back to the right project.\n# This tag requires that the tag SEARCHENGINE is set to YES.\n\nEXTERNAL_SEARCH_ID     =\n\n# The EXTRA_SEARCH_MAPPINGS tag can be used to enable searching through doxygen\n# projects other than the one defined by this configuration file, but that are\n# all added to the same external search index. Each project needs to have a\n# unique id set via EXTERNAL_SEARCH_ID. The search mapping then maps the id of\n# to a relative location where the documentation can be found. The format is:\n# EXTRA_SEARCH_MAPPINGS = tagname1=loc1 tagname2=loc2 ...\n# This tag requires that the tag SEARCHENGINE is set to YES.\n\nEXTRA_SEARCH_MAPPINGS  =\n\n#---------------------------------------------------------------------------\n# Configuration options related to the LaTeX output\n#---------------------------------------------------------------------------\n\n# If the GENERATE_LATEX tag is set to YES, doxygen will generate LaTeX output.\n# The default value is: YES.\n\nGENERATE_LATEX         = NO\n\n# The LATEX_OUTPUT tag is used to specify where the LaTeX docs will be put. If a\n# relative path is entered the value of OUTPUT_DIRECTORY will be put in front of\n# it.\n# The default directory is: latex.\n# This tag requires that the tag GENERATE_LATEX is set to YES.\n\nLATEX_OUTPUT           = latex\n\n# The LATEX_CMD_NAME tag can be used to specify the LaTeX command name to be\n# invoked.\n#\n# Note that when not enabling USE_PDFLATEX the default is latex when enabling\n# USE_PDFLATEX the default is pdflatex and when in the later case latex is\n# chosen this is overwritten by pdflatex. For specific output languages the\n# default can have been set differently, this depends on the implementation of\n# the output language.\n# This tag requires that the tag GENERATE_LATEX is set to YES.\n\nLATEX_CMD_NAME         =\n\n# The MAKEINDEX_CMD_NAME tag can be used to specify the command name to generate\n# index for LaTeX.\n# Note: This tag is used in the Makefile / make.bat.\n# See also: LATEX_MAKEINDEX_CMD for the part in the generated output file\n# (.tex).\n# The default file is: makeindex.\n# This tag requires that the tag GENERATE_LATEX is set to YES.\n\nMAKEINDEX_CMD_NAME     = makeindex\n\n# The LATEX_MAKEINDEX_CMD tag can be used to specify the command name to\n# generate index for LaTeX. In case there is no backslash (\\) as first character\n# it will be automatically added in the LaTeX code.\n# Note: This tag is used in the generated output file (.tex).\n# See also: MAKEINDEX_CMD_NAME for the part in the Makefile / make.bat.\n# The default value is: makeindex.\n# This tag requires that the tag GENERATE_LATEX is set to YES.\n\nLATEX_MAKEINDEX_CMD    = makeindex\n\n# If the COMPACT_LATEX tag is set to YES, doxygen generates more compact LaTeX\n# documents. This may be useful for small projects and may help to save some\n# trees in general.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_LATEX is set to YES.\n\nCOMPACT_LATEX          = NO\n\n# The PAPER_TYPE tag can be used to set the paper type that is used by the\n# printer.\n# Possible values are: a4 (210 x 297 mm), letter (8.5 x 11 inches), legal (8.5 x\n# 14 inches) and executive (7.25 x 10.5 inches).\n# The default value is: a4.\n# This tag requires that the tag GENERATE_LATEX is set to YES.\n\nPAPER_TYPE             = a4\n\n# The EXTRA_PACKAGES tag can be used to specify one or more LaTeX package names\n# that should be included in the LaTeX output. The package can be specified just\n# by its name or with the correct syntax as to be used with the LaTeX\n# \\usepackage command. To get the times font for instance you can specify :\n# EXTRA_PACKAGES=times or EXTRA_PACKAGES={times}\n# To use the option intlimits with the amsmath package you can specify:\n# EXTRA_PACKAGES=[intlimits]{amsmath}\n# If left blank no extra packages will be included.\n# This tag requires that the tag GENERATE_LATEX is set to YES.\n\nEXTRA_PACKAGES         =\n\n# The LATEX_HEADER tag can be used to specify a personal LaTeX header for the\n# generated LaTeX document. The header should contain everything until the first\n# chapter. If it is left blank doxygen will generate a standard header. See\n# section \"Doxygen usage\" for information on how to let doxygen write the\n# default header to a separate file.\n#\n# Note: Only use a user-defined header if you know what you are doing! The\n# following commands have a special meaning inside the header: $title,\n# $datetime, $date, $doxygenversion, $projectname, $projectnumber,\n# $projectbrief, $projectlogo. Doxygen will replace $title with the empty\n# string, for the replacement values of the other commands the user is referred\n# to HTML_HEADER.\n# This tag requires that the tag GENERATE_LATEX is set to YES.\n\nLATEX_HEADER           =\n\n# The LATEX_FOOTER tag can be used to specify a personal LaTeX footer for the\n# generated LaTeX document. The footer should contain everything after the last\n# chapter. If it is left blank doxygen will generate a standard footer. See\n# LATEX_HEADER for more information on how to generate a default footer and what\n# special commands can be used inside the footer.\n#\n# Note: Only use a user-defined footer if you know what you are doing!\n# This tag requires that the tag GENERATE_LATEX is set to YES.\n\nLATEX_FOOTER           =\n\n# The LATEX_EXTRA_STYLESHEET tag can be used to specify additional user-defined\n# LaTeX style sheets that are included after the standard style sheets created\n# by doxygen. Using this option one can overrule certain style aspects. Doxygen\n# will copy the style sheet files to the output directory.\n# Note: The order of the extra style sheet files is of importance (e.g. the last\n# style sheet in the list overrules the setting of the previous ones in the\n# list).\n# This tag requires that the tag GENERATE_LATEX is set to YES.\n\nLATEX_EXTRA_STYLESHEET =\n\n# The LATEX_EXTRA_FILES tag can be used to specify one or more extra images or\n# other source files which should be copied to the LATEX_OUTPUT output\n# directory. Note that the files will be copied as-is; there are no commands or\n# markers available.\n# This tag requires that the tag GENERATE_LATEX is set to YES.\n\nLATEX_EXTRA_FILES      =\n\n# If the PDF_HYPERLINKS tag is set to YES, the LaTeX that is generated is\n# prepared for conversion to PDF (using ps2pdf or pdflatex). The PDF file will\n# contain links (just like the HTML output) instead of page references. This\n# makes the output suitable for online browsing using a PDF viewer.\n# The default value is: YES.\n# This tag requires that the tag GENERATE_LATEX is set to YES.\n\nPDF_HYPERLINKS         = YES\n\n# If the USE_PDFLATEX tag is set to YES, doxygen will use pdflatex to generate\n# the PDF file directly from the LaTeX files. Set this option to YES, to get a\n# higher quality PDF documentation.\n# The default value is: YES.\n# This tag requires that the tag GENERATE_LATEX is set to YES.\n\nUSE_PDFLATEX           = YES\n\n# If the LATEX_BATCHMODE tag is set to YES, doxygen will add the \\batchmode\n# command to the generated LaTeX files. This will instruct LaTeX to keep running\n# if errors occur, instead of asking the user for help. This option is also used\n# when generating formulas in HTML.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_LATEX is set to YES.\n\nLATEX_BATCHMODE        = NO\n\n# If the LATEX_HIDE_INDICES tag is set to YES then doxygen will not include the\n# index chapters (such as File Index, Compound Index, etc.) in the output.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_LATEX is set to YES.\n\nLATEX_HIDE_INDICES     = NO\n\n# If the LATEX_SOURCE_CODE tag is set to YES then doxygen will include source\n# code with syntax highlighting in the LaTeX output.\n#\n# Note that which sources are shown also depends on other settings such as\n# SOURCE_BROWSER.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_LATEX is set to YES.\n\nLATEX_SOURCE_CODE      = NO\n\n# The LATEX_BIB_STYLE tag can be used to specify the style to use for the\n# bibliography, e.g. plainnat, or ieeetr. See\n# https://en.wikipedia.org/wiki/BibTeX and \\cite for more info.\n# The default value is: plain.\n# This tag requires that the tag GENERATE_LATEX is set to YES.\n\nLATEX_BIB_STYLE        = plain\n\n# If the LATEX_TIMESTAMP tag is set to YES then the footer of each generated\n# page will contain the date and time when the page was generated. Setting this\n# to NO can help when comparing the output of multiple runs.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_LATEX is set to YES.\n\nLATEX_TIMESTAMP        = NO\n\n# The LATEX_EMOJI_DIRECTORY tag is used to specify the (relative or absolute)\n# path from which the emoji images will be read. If a relative path is entered,\n# it will be relative to the LATEX_OUTPUT directory. If left blank the\n# LATEX_OUTPUT directory will be used.\n# This tag requires that the tag GENERATE_LATEX is set to YES.\n\nLATEX_EMOJI_DIRECTORY  =\n\n#---------------------------------------------------------------------------\n# Configuration options related to the RTF output\n#---------------------------------------------------------------------------\n\n# If the GENERATE_RTF tag is set to YES, doxygen will generate RTF output. The\n# RTF output is optimized for Word 97 and may not look too pretty with other RTF\n# readers/editors.\n# The default value is: NO.\n\nGENERATE_RTF           = NO\n\n# The RTF_OUTPUT tag is used to specify where the RTF docs will be put. If a\n# relative path is entered the value of OUTPUT_DIRECTORY will be put in front of\n# it.\n# The default directory is: rtf.\n# This tag requires that the tag GENERATE_RTF is set to YES.\n\nRTF_OUTPUT             = rtf\n\n# If the COMPACT_RTF tag is set to YES, doxygen generates more compact RTF\n# documents. This may be useful for small projects and may help to save some\n# trees in general.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_RTF is set to YES.\n\nCOMPACT_RTF            = NO\n\n# If the RTF_HYPERLINKS tag is set to YES, the RTF that is generated will\n# contain hyperlink fields. The RTF file will contain links (just like the HTML\n# output) instead of page references. This makes the output suitable for online\n# browsing using Word or some other Word compatible readers that support those\n# fields.\n#\n# Note: WordPad (write) and others do not support links.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_RTF is set to YES.\n\nRTF_HYPERLINKS         = NO\n\n# Load stylesheet definitions from file. Syntax is similar to doxygen's\n# configuration file, i.e. a series of assignments. You only have to provide\n# replacements, missing definitions are set to their default value.\n#\n# See also section \"Doxygen usage\" for information on how to generate the\n# default style sheet that doxygen normally uses.\n# This tag requires that the tag GENERATE_RTF is set to YES.\n\nRTF_STYLESHEET_FILE    =\n\n# Set optional variables used in the generation of an RTF document. Syntax is\n# similar to doxygen's configuration file. A template extensions file can be\n# generated using doxygen -e rtf extensionFile.\n# This tag requires that the tag GENERATE_RTF is set to YES.\n\nRTF_EXTENSIONS_FILE    =\n\n# If the RTF_SOURCE_CODE tag is set to YES then doxygen will include source code\n# with syntax highlighting in the RTF output.\n#\n# Note that which sources are shown also depends on other settings such as\n# SOURCE_BROWSER.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_RTF is set to YES.\n\nRTF_SOURCE_CODE        = NO\n\n#---------------------------------------------------------------------------\n# Configuration options related to the man page output\n#---------------------------------------------------------------------------\n\n# If the GENERATE_MAN tag is set to YES, doxygen will generate man pages for\n# classes and files.\n# The default value is: NO.\n\nGENERATE_MAN           = NO\n\n# The MAN_OUTPUT tag is used to specify where the man pages will be put. If a\n# relative path is entered the value of OUTPUT_DIRECTORY will be put in front of\n# it. A directory man3 will be created inside the directory specified by\n# MAN_OUTPUT.\n# The default directory is: man.\n# This tag requires that the tag GENERATE_MAN is set to YES.\n\nMAN_OUTPUT             = man\n\n# The MAN_EXTENSION tag determines the extension that is added to the generated\n# man pages. In case the manual section does not start with a number, the number\n# 3 is prepended. The dot (.) at the beginning of the MAN_EXTENSION tag is\n# optional.\n# The default value is: .3.\n# This tag requires that the tag GENERATE_MAN is set to YES.\n\nMAN_EXTENSION          = .3\n\n# The MAN_SUBDIR tag determines the name of the directory created within\n# MAN_OUTPUT in which the man pages are placed. If defaults to man followed by\n# MAN_EXTENSION with the initial . removed.\n# This tag requires that the tag GENERATE_MAN is set to YES.\n\nMAN_SUBDIR             =\n\n# If the MAN_LINKS tag is set to YES and doxygen generates man output, then it\n# will generate one additional man file for each entity documented in the real\n# man page(s). These additional files only source the real man page, but without\n# them the man command would be unable to find the correct page.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_MAN is set to YES.\n\nMAN_LINKS              = NO\n\n#---------------------------------------------------------------------------\n# Configuration options related to the XML output\n#---------------------------------------------------------------------------\n\n# If the GENERATE_XML tag is set to YES, doxygen will generate an XML file that\n# captures the structure of the code including all documentation.\n# The default value is: NO.\n\nGENERATE_XML           = NO\n\n# The XML_OUTPUT tag is used to specify where the XML pages will be put. If a\n# relative path is entered the value of OUTPUT_DIRECTORY will be put in front of\n# it.\n# The default directory is: xml.\n# This tag requires that the tag GENERATE_XML is set to YES.\n\nXML_OUTPUT             = xml\n\n# If the XML_PROGRAMLISTING tag is set to YES, doxygen will dump the program\n# listings (including syntax highlighting and cross-referencing information) to\n# the XML output. Note that enabling this will significantly increase the size\n# of the XML output.\n# The default value is: YES.\n# This tag requires that the tag GENERATE_XML is set to YES.\n\nXML_PROGRAMLISTING     = YES\n\n# If the XML_NS_MEMB_FILE_SCOPE tag is set to YES, doxygen will include\n# namespace members in file scope as well, matching the HTML output.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_XML is set to YES.\n\nXML_NS_MEMB_FILE_SCOPE = NO\n\n#---------------------------------------------------------------------------\n# Configuration options related to the DOCBOOK output\n#---------------------------------------------------------------------------\n\n# If the GENERATE_DOCBOOK tag is set to YES, doxygen will generate Docbook files\n# that can be used to generate PDF.\n# The default value is: NO.\n\nGENERATE_DOCBOOK       = NO\n\n# The DOCBOOK_OUTPUT tag is used to specify where the Docbook pages will be put.\n# If a relative path is entered the value of OUTPUT_DIRECTORY will be put in\n# front of it.\n# The default directory is: docbook.\n# This tag requires that the tag GENERATE_DOCBOOK is set to YES.\n\nDOCBOOK_OUTPUT         = docbook\n\n# If the DOCBOOK_PROGRAMLISTING tag is set to YES, doxygen will include the\n# program listings (including syntax highlighting and cross-referencing\n# information) to the DOCBOOK output. Note that enabling this will significantly\n# increase the size of the DOCBOOK output.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_DOCBOOK is set to YES.\n\nDOCBOOK_PROGRAMLISTING = NO\n\n#---------------------------------------------------------------------------\n# Configuration options for the AutoGen Definitions output\n#---------------------------------------------------------------------------\n\n# If the GENERATE_AUTOGEN_DEF tag is set to YES, doxygen will generate an\n# AutoGen Definitions (see http://autogen.sourceforge.net/) file that captures\n# the structure of the code including all documentation. Note that this feature\n# is still experimental and incomplete at the moment.\n# The default value is: NO.\n\nGENERATE_AUTOGEN_DEF   = NO\n\n#---------------------------------------------------------------------------\n# Configuration options related to the Perl module output\n#---------------------------------------------------------------------------\n\n# If the GENERATE_PERLMOD tag is set to YES, doxygen will generate a Perl module\n# file that captures the structure of the code including all documentation.\n#\n# Note that this feature is still experimental and incomplete at the moment.\n# The default value is: NO.\n\nGENERATE_PERLMOD       = NO\n\n# If the PERLMOD_LATEX tag is set to YES, doxygen will generate the necessary\n# Makefile rules, Perl scripts and LaTeX code to be able to generate PDF and DVI\n# output from the Perl module output.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_PERLMOD is set to YES.\n\nPERLMOD_LATEX          = NO\n\n# If the PERLMOD_PRETTY tag is set to YES, the Perl module output will be nicely\n# formatted so it can be parsed by a human reader. This is useful if you want to\n# understand what is going on. On the other hand, if this tag is set to NO, the\n# size of the Perl module output will be much smaller and Perl will parse it\n# just the same.\n# The default value is: YES.\n# This tag requires that the tag GENERATE_PERLMOD is set to YES.\n\nPERLMOD_PRETTY         = YES\n\n# The names of the make variables in the generated doxyrules.make file are\n# prefixed with the string contained in PERLMOD_MAKEVAR_PREFIX. This is useful\n# so different doxyrules.make files included by the same Makefile don't\n# overwrite each other's variables.\n# This tag requires that the tag GENERATE_PERLMOD is set to YES.\n\nPERLMOD_MAKEVAR_PREFIX =\n\n#---------------------------------------------------------------------------\n# Configuration options related to the preprocessor\n#---------------------------------------------------------------------------\n\n# If the ENABLE_PREPROCESSING tag is set to YES, doxygen will evaluate all\n# C-preprocessor directives found in the sources and include files.\n# The default value is: YES.\n\nENABLE_PREPROCESSING   = YES\n\n# If the MACRO_EXPANSION tag is set to YES, doxygen will expand all macro names\n# in the source code. If set to NO, only conditional compilation will be\n# performed. Macro expansion can be done in a controlled way by setting\n# EXPAND_ONLY_PREDEF to YES.\n# The default value is: NO.\n# This tag requires that the tag ENABLE_PREPROCESSING is set to YES.\n\nMACRO_EXPANSION        = NO\n\n# If the EXPAND_ONLY_PREDEF and MACRO_EXPANSION tags are both set to YES then\n# the macro expansion is limited to the macros specified with the PREDEFINED and\n# EXPAND_AS_DEFINED tags.\n# The default value is: NO.\n# This tag requires that the tag ENABLE_PREPROCESSING is set to YES.\n\nEXPAND_ONLY_PREDEF     = NO\n\n# If the SEARCH_INCLUDES tag is set to YES, the include files in the\n# INCLUDE_PATH will be searched if a #include is found.\n# The default value is: YES.\n# This tag requires that the tag ENABLE_PREPROCESSING is set to YES.\n\nSEARCH_INCLUDES        = YES\n\n# The INCLUDE_PATH tag can be used to specify one or more directories that\n# contain include files that are not input files but should be processed by the\n# preprocessor.\n# This tag requires that the tag SEARCH_INCLUDES is set to YES.\n\nINCLUDE_PATH           =\n\n# You can use the INCLUDE_FILE_PATTERNS tag to specify one or more wildcard\n# patterns (like *.h and *.hpp) to filter out the header-files in the\n# directories. If left blank, the patterns specified with FILE_PATTERNS will be\n# used.\n# This tag requires that the tag ENABLE_PREPROCESSING is set to YES.\n\nINCLUDE_FILE_PATTERNS  =\n\n# The PREDEFINED tag can be used to specify one or more macro names that are\n# defined before the preprocessor is started (similar to the -D option of e.g.\n# gcc). The argument of the tag is a list of macros of the form: name or\n# name=definition (no spaces). If the definition and the \"=\" are omitted, \"=1\"\n# is assumed. To prevent a macro definition from being undefined via #undef or\n# recursively expanded use the := operator instead of the = operator.\n# This tag requires that the tag ENABLE_PREPROCESSING is set to YES.\n\nPREDEFINED             = NFQ IPFW HAVE_PFRING HAVE_AF_PACKET HAVE_NAPATECH HAVE_DAG PROFILING UNITTESTS\n\n# If the MACRO_EXPANSION and EXPAND_ONLY_PREDEF tags are set to YES then this\n# tag can be used to specify a list of macro names that should be expanded. The\n# macro definition that is found in the sources will be used. Use the PREDEFINED\n# tag if you want to use a different macro definition that overrules the\n# definition found in the source code.\n# This tag requires that the tag ENABLE_PREPROCESSING is set to YES.\n\nEXPAND_AS_DEFINED      =\n\n# If the SKIP_FUNCTION_MACROS tag is set to YES then doxygen's preprocessor will\n# remove all references to function-like macros that are alone on a line, have\n# an all uppercase name, and do not end with a semicolon. Such function macros\n# are typically used for boiler-plate code, and will confuse the parser if not\n# removed.\n# The default value is: YES.\n# This tag requires that the tag ENABLE_PREPROCESSING is set to YES.\n\nSKIP_FUNCTION_MACROS   = YES\n\n#---------------------------------------------------------------------------\n# Configuration options related to external references\n#---------------------------------------------------------------------------\n\n# The TAGFILES tag can be used to specify one or more tag files. For each tag\n# file the location of the external documentation should be added. The format of\n# a tag file without this location is as follows:\n# TAGFILES = file1 file2 ...\n# Adding location for the tag files is done as follows:\n# TAGFILES = file1=loc1 \"file2 = loc2\" ...\n# where loc1 and loc2 can be relative or absolute paths or URLs. See the\n# section \"Linking to external documentation\" for more information about the use\n# of tag files.\n# Note: Each tag file must have a unique name (where the name does NOT include\n# the path). If a tag file is not located in the directory in which doxygen is\n# run, you must also specify the path to the tagfile here.\n\nTAGFILES               =\n\n# When a file name is specified after GENERATE_TAGFILE, doxygen will create a\n# tag file that is based on the input files it reads. See section \"Linking to\n# external documentation\" for more information about the usage of tag files.\n\nGENERATE_TAGFILE       =\n\n# If the ALLEXTERNALS tag is set to YES, all external class will be listed in\n# the class index. If set to NO, only the inherited external classes will be\n# listed.\n# The default value is: NO.\n\nALLEXTERNALS           = NO\n\n# If the EXTERNAL_GROUPS tag is set to YES, all external groups will be listed\n# in the modules index. If set to NO, only the current project's groups will be\n# listed.\n# The default value is: YES.\n\nEXTERNAL_GROUPS        = YES\n\n# If the EXTERNAL_PAGES tag is set to YES, all external pages will be listed in\n# the related pages index. If set to NO, only the current project's pages will\n# be listed.\n# The default value is: YES.\n\nEXTERNAL_PAGES         = YES\n\n#---------------------------------------------------------------------------\n# Configuration options related to the dot tool\n#---------------------------------------------------------------------------\n\n# If the CLASS_DIAGRAMS tag is set to YES, doxygen will generate a class diagram\n# (in HTML and LaTeX) for classes with base or super classes. Setting the tag to\n# NO turns the diagrams off. Note that this option also works with HAVE_DOT\n# disabled, but it is recommended to install and use dot, since it yields more\n# powerful graphs.\n# The default value is: YES.\n\nCLASS_DIAGRAMS         = YES\n\n# You can include diagrams made with dia in doxygen documentation. Doxygen will\n# then run dia to produce the diagram and insert it in the documentation. The\n# DIA_PATH tag allows you to specify the directory where the dia binary resides.\n# If left empty dia is assumed to be found in the default search path.\n\nDIA_PATH               =\n\n# If set to YES the inheritance and collaboration graphs will hide inheritance\n# and usage relations if the target is undocumented or is not a class.\n# The default value is: YES.\n\nHIDE_UNDOC_RELATIONS   = YES\n\n# If you set the HAVE_DOT tag to YES then doxygen will assume the dot tool is\n# available from the path. This tool is part of Graphviz (see:\n# http://www.graphviz.org/), a graph visualization toolkit from AT&T and Lucent\n# Bell Labs. The other options in this section have no effect if this option is\n# set to NO\n# The default value is: NO.\n\nHAVE_DOT               = YES\n\n# The DOT_NUM_THREADS specifies the number of dot invocations doxygen is allowed\n# to run in parallel. When set to 0 doxygen will base this on the number of\n# processors available in the system. You can set it explicitly to a value\n# larger than 0 to get control over the balance between CPU load and processing\n# speed.\n# Minimum value: 0, maximum value: 32, default value: 0.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nDOT_NUM_THREADS        = 0\n\n# When you want a differently looking font in the dot files that doxygen\n# generates you can specify the font name using DOT_FONTNAME. You need to make\n# sure dot is able to find the font, which can be done by putting it in a\n# standard location or by setting the DOTFONTPATH environment variable or by\n# setting DOT_FONTPATH to the directory containing the font.\n# The default value is: Helvetica.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nDOT_FONTNAME           = Helvetica\n\n# The DOT_FONTSIZE tag can be used to set the size (in points) of the font of\n# dot graphs.\n# Minimum value: 4, maximum value: 24, default value: 10.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nDOT_FONTSIZE           = 10\n\n# By default doxygen will tell dot to use the default font as specified with\n# DOT_FONTNAME. If you specify a different font using DOT_FONTNAME you can set\n# the path where dot can find it using this tag.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nDOT_FONTPATH           =\n\n# If the CLASS_GRAPH tag is set to YES then doxygen will generate a graph for\n# each documented class showing the direct and indirect inheritance relations.\n# Setting this tag to YES will force the CLASS_DIAGRAMS tag to NO.\n# The default value is: YES.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nCLASS_GRAPH            = YES\n\n# If the COLLABORATION_GRAPH tag is set to YES then doxygen will generate a\n# graph for each documented class showing the direct and indirect implementation\n# dependencies (inheritance, containment, and class references variables) of the\n# class with other documented classes.\n# The default value is: YES.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nCOLLABORATION_GRAPH    = YES\n\n# If the GROUP_GRAPHS tag is set to YES then doxygen will generate a graph for\n# groups, showing the direct groups dependencies.\n# The default value is: YES.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nGROUP_GRAPHS           = YES\n\n# If the UML_LOOK tag is set to YES, doxygen will generate inheritance and\n# collaboration diagrams in a style similar to the OMG's Unified Modeling\n# Language.\n# The default value is: NO.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nUML_LOOK               = NO\n\n# If the UML_LOOK tag is enabled, the fields and methods are shown inside the\n# class node. If there are many fields or methods and many nodes the graph may\n# become too big to be useful. The UML_LIMIT_NUM_FIELDS threshold limits the\n# number of items for each type to make the size more manageable. Set this to 0\n# for no limit. Note that the threshold may be exceeded by 50% before the limit\n# is enforced. So when you set the threshold to 10, up to 15 fields may appear,\n# but if the number exceeds 15, the total amount of fields shown is limited to\n# 10.\n# Minimum value: 0, maximum value: 100, default value: 10.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nUML_LIMIT_NUM_FIELDS   = 10\n\n# If the TEMPLATE_RELATIONS tag is set to YES then the inheritance and\n# collaboration graphs will show the relations between templates and their\n# instances.\n# The default value is: NO.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nTEMPLATE_RELATIONS     = NO\n\n# If the INCLUDE_GRAPH, ENABLE_PREPROCESSING and SEARCH_INCLUDES tags are set to\n# YES then doxygen will generate a graph for each documented file showing the\n# direct and indirect include dependencies of the file with other documented\n# files.\n# The default value is: YES.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nINCLUDE_GRAPH          = YES\n\n# If the INCLUDED_BY_GRAPH, ENABLE_PREPROCESSING and SEARCH_INCLUDES tags are\n# set to YES then doxygen will generate a graph for each documented file showing\n# the direct and indirect include dependencies of the file with other documented\n# files.\n# The default value is: YES.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nINCLUDED_BY_GRAPH      = YES\n\n# If the CALL_GRAPH tag is set to YES then doxygen will generate a call\n# dependency graph for every global function or class method.\n#\n# Note that enabling this option will significantly increase the time of a run.\n# So in most cases it will be better to enable call graphs for selected\n# functions only using the \\callgraph command. Disabling a call graph can be\n# accomplished by means of the command \\hidecallgraph.\n# The default value is: NO.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nCALL_GRAPH             = YES\n\n# If the CALLER_GRAPH tag is set to YES then doxygen will generate a caller\n# dependency graph for every global function or class method.\n#\n# Note that enabling this option will significantly increase the time of a run.\n# So in most cases it will be better to enable caller graphs for selected\n# functions only using the \\callergraph command. Disabling a caller graph can be\n# accomplished by means of the command \\hidecallergraph.\n# The default value is: NO.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nCALLER_GRAPH           = YES\n\n# If the GRAPHICAL_HIERARCHY tag is set to YES then doxygen will graphical\n# hierarchy of all classes instead of a textual one.\n# The default value is: YES.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nGRAPHICAL_HIERARCHY    = YES\n\n# If the DIRECTORY_GRAPH tag is set to YES then doxygen will show the\n# dependencies a directory has on other directories in a graphical way. The\n# dependency relations are determined by the #include relations between the\n# files in the directories.\n# The default value is: YES.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nDIRECTORY_GRAPH        = YES\n\n# The DOT_IMAGE_FORMAT tag can be used to set the image format of the images\n# generated by dot. For an explanation of the image formats see the section\n# output formats in the documentation of the dot tool (Graphviz (see:\n# http://www.graphviz.org/)).\n# Note: If you choose svg you need to set HTML_FILE_EXTENSION to xhtml in order\n# to make the SVG files visible in IE 9+ (other browsers do not have this\n# requirement).\n# Possible values are: png, jpg, gif, svg, png:gd, png:gd:gd, png:cairo,\n# png:cairo:gd, png:cairo:cairo, png:cairo:gdiplus, png:gdiplus and\n# png:gdiplus:gdiplus.\n# The default value is: png.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nDOT_IMAGE_FORMAT       = png\n\n# If DOT_IMAGE_FORMAT is set to svg, then this option can be set to YES to\n# enable generation of interactive SVG images that allow zooming and panning.\n#\n# Note that this requires a modern browser other than Internet Explorer. Tested\n# and working are Firefox, Chrome, Safari, and Opera.\n# Note: For IE 9+ you need to set HTML_FILE_EXTENSION to xhtml in order to make\n# the SVG files visible. Older versions of IE do not have SVG support.\n# The default value is: NO.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nINTERACTIVE_SVG        = NO\n\n# The DOT_PATH tag can be used to specify the path where the dot tool can be\n# found. If left blank, it is assumed the dot tool can be found in the path.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nDOT_PATH               =\n\n# The DOTFILE_DIRS tag can be used to specify one or more directories that\n# contain dot files that are included in the documentation (see the \\dotfile\n# command).\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nDOTFILE_DIRS           =\n\n# The MSCFILE_DIRS tag can be used to specify one or more directories that\n# contain msc files that are included in the documentation (see the \\mscfile\n# command).\n\nMSCFILE_DIRS           =\n\n# The DIAFILE_DIRS tag can be used to specify one or more directories that\n# contain dia files that are included in the documentation (see the \\diafile\n# command).\n\nDIAFILE_DIRS           =\n\n# When using plantuml, the PLANTUML_JAR_PATH tag should be used to specify the\n# path where java can find the plantuml.jar file. If left blank, it is assumed\n# PlantUML is not used or called during a preprocessing step. Doxygen will\n# generate a warning when it encounters a \\startuml command in this case and\n# will not generate output for the diagram.\n\nPLANTUML_JAR_PATH      =\n\n# When using plantuml, the PLANTUML_CFG_FILE tag can be used to specify a\n# configuration file for plantuml.\n\nPLANTUML_CFG_FILE      =\n\n# When using plantuml, the specified paths are searched for files specified by\n# the !include statement in a plantuml block.\n\nPLANTUML_INCLUDE_PATH  =\n\n# The DOT_GRAPH_MAX_NODES tag can be used to set the maximum number of nodes\n# that will be shown in the graph. If the number of nodes in a graph becomes\n# larger than this value, doxygen will truncate the graph, which is visualized\n# by representing a node as a red box. Note that doxygen if the number of direct\n# children of the root node in a graph is already larger than\n# DOT_GRAPH_MAX_NODES then the graph will not be shown at all. Also note that\n# the size of a graph can be further restricted by MAX_DOT_GRAPH_DEPTH.\n# Minimum value: 0, maximum value: 10000, default value: 50.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nDOT_GRAPH_MAX_NODES    = 50\n\n# The MAX_DOT_GRAPH_DEPTH tag can be used to set the maximum depth of the graphs\n# generated by dot. A depth value of 3 means that only nodes reachable from the\n# root by following a path via at most 3 edges will be shown. Nodes that lay\n# further from the root node will be omitted. Note that setting this option to 1\n# or 2 may greatly reduce the computation time needed for large code bases. Also\n# note that the size of a graph can be further restricted by\n# DOT_GRAPH_MAX_NODES. Using a depth of 0 means no depth restriction.\n# Minimum value: 0, maximum value: 1000, default value: 0.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nMAX_DOT_GRAPH_DEPTH    = 0\n\n# Set the DOT_TRANSPARENT tag to YES to generate images with a transparent\n# background. This is disabled by default, because dot on Windows does not seem\n# to support this out of the box.\n#\n# Warning: Depending on the platform used, enabling this option may lead to\n# badly anti-aliased labels on the edges of a graph (i.e. they become hard to\n# read).\n# The default value is: NO.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nDOT_TRANSPARENT        = NO\n\n# Set the DOT_MULTI_TARGETS tag to YES to allow dot to generate multiple output\n# files in one run (i.e. multiple -o and -T options on the command line). This\n# makes dot run faster, but since only newer versions of dot (>1.8.10) support\n# this, this feature is disabled by default.\n# The default value is: NO.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nDOT_MULTI_TARGETS      = NO\n\n# If the GENERATE_LEGEND tag is set to YES doxygen will generate a legend page\n# explaining the meaning of the various boxes and arrows in the dot generated\n# graphs.\n# The default value is: YES.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nGENERATE_LEGEND        = YES\n\n# If the DOT_CLEANUP tag is set to YES, doxygen will remove the intermediate dot\n# files that are used to generate the various graphs.\n# The default value is: YES.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nDOT_CLEANUP            = YES\n"
        },
        {
          "name": "ebpf",
          "type": "tree",
          "content": null
        },
        {
          "name": "etc",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "git-templates",
          "type": "tree",
          "content": null
        },
        {
          "name": "libsuricata-config.in",
          "type": "blob",
          "size": 1.2880859375,
          "content": "#! /bin/sh\n\nprefix=\"@prefix@\"\nexec_prefix=\"@exec_prefix@\"\nincludedir=\"@includedir@\"\nlibdir=\"@libdir@\"\nLIBS=\"@LIBS@ @RUST_LDADD@\"\n\nshared_lib=\"-lsuricata\"\nstatic_lib=\"-lsuricata_c -lsuricata_rust\"\n\nenable_non_bundled_htp=\"@enable_non_bundled_htp@\"\n\nlib=\"$shared_lib\"\n\nshow_libs=\"no\"\nshow_cflags=\"no\"\nuse_static=\"no\"\n\nif [ \"$#\" = 0 ]; then\n    echo \"usage: suricata-config [--cflags] [--libs] [--static]\"\n    exit 0\nfi\n\nwhile [ \"$#\" != 0 ]\ndo\n    case \"$1\" in\n        --libs)\n            show_libs=\"yes\"\n            ;;\n        --cflags)\n            show_cflags=\"yes\"\n            ;;\n        --static)\n            lib=\"$static_lib\"\n            use_status=\"yes\"\n            ;;\n    esac\n    shift\ndone\n\n# If --static wasn't provided, use the static library if the shared\n# library is not available.\nif [ \"$use_static\" = \"no\" ]; then\n    if ! test -e \"$libdir/libsuricata.so\"; then\n        lib=\"$static_lib\"\n    fi\nfi\n\n# If we're using a bundled htp, add it to the libs as well. It will\n# already be present if we're use a non-bundled libhtp.\nif [ \"$enable_non_bundled_htp\" = \"no\" ]; then\n    lib=\"${lib} -lhtp\"\nfi\n\noutput=\"\"\n\nif [ \"$show_cflags\" = \"yes\" ]; then\n    output=\"${output} -DHAVE_CONFIG_H -I$includedir/suricata\"\nfi\n\nif [ \"$show_libs\" = \"yes\" ]; then\n    output=\"${output} -L$libdir $lib $LIBS\"\nfi\n\necho \"$output\"\n"
        },
        {
          "name": "lua",
          "type": "tree",
          "content": null
        },
        {
          "name": "plugins",
          "type": "tree",
          "content": null
        },
        {
          "name": "python",
          "type": "tree",
          "content": null
        },
        {
          "name": "qa",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.1962890625,
          "content": "# Specify libhtp and suricata-update requirements.\n#\n# Format:\n#\n#   name {repo} {branch|tag}\nlibhtp https://github.com/OISF/libhtp 0.5.x\nsuricata-update https://github.com/OISF/suricata-update master\n"
        },
        {
          "name": "rules",
          "type": "tree",
          "content": null
        },
        {
          "name": "rust",
          "type": "tree",
          "content": null
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "src",
          "type": "tree",
          "content": null
        },
        {
          "name": "suricata-update",
          "type": "tree",
          "content": null
        },
        {
          "name": "suricata.yaml.in",
          "type": "blob",
          "size": 85.9892578125,
          "content": "%YAML 1.1\n---\n\n# Suricata configuration file. In addition to the comments describing all\n# options in this file, full documentation can be found at:\n# https://docs.suricata.io/en/latest/configuration/suricata-yaml.html\n\n# This configuration file was generated by Suricata @PACKAGE_VERSION@.\nsuricata-version: \"@MAJOR_MINOR@\"\n\n##\n## Step 1: Inform Suricata about your network\n##\n\nvars:\n  # more specific is better for alert accuracy and performance\n  address-groups:\n    HOME_NET: \"[192.168.0.0/16,10.0.0.0/8,172.16.0.0/12]\"\n    #HOME_NET: \"[192.168.0.0/16]\"\n    #HOME_NET: \"[10.0.0.0/8]\"\n    #HOME_NET: \"[172.16.0.0/12]\"\n    #HOME_NET: \"any\"\n\n    EXTERNAL_NET: \"!$HOME_NET\"\n    #EXTERNAL_NET: \"any\"\n\n    HTTP_SERVERS: \"$HOME_NET\"\n    SMTP_SERVERS: \"$HOME_NET\"\n    SQL_SERVERS: \"$HOME_NET\"\n    DNS_SERVERS: \"$HOME_NET\"\n    TELNET_SERVERS: \"$HOME_NET\"\n    AIM_SERVERS: \"$EXTERNAL_NET\"\n    DC_SERVERS: \"$HOME_NET\"\n    DNP3_SERVER: \"$HOME_NET\"\n    DNP3_CLIENT: \"$HOME_NET\"\n    MODBUS_CLIENT: \"$HOME_NET\"\n    MODBUS_SERVER: \"$HOME_NET\"\n    ENIP_CLIENT: \"$HOME_NET\"\n    ENIP_SERVER: \"$HOME_NET\"\n\n  port-groups:\n    HTTP_PORTS: \"80\"\n    SHELLCODE_PORTS: \"!80\"\n    ORACLE_PORTS: 1521\n    SSH_PORTS: 22\n    DNP3_PORTS: 20000\n    MODBUS_PORTS: 502\n    FILE_DATA_PORTS: \"[$HTTP_PORTS,110,143]\"\n    FTP_PORTS: 21\n    GENEVE_PORTS: 6081\n    VXLAN_PORTS: 4789\n    TEREDO_PORTS: 3544\n    SIP_PORTS: \"[5060, 5061]\"\n\n##\n## Step 2: Select outputs to enable\n##\n\n# The default logging directory.  Any log or output file will be\n# placed here if it's not specified with a full path name. This can be\n# overridden with the -l command line parameter.\ndefault-log-dir: @e_logdir@\n\n# Global stats configuration\nstats:\n  enabled: yes\n  # The interval field (in seconds) controls the interval at\n  # which stats are updated in the log.\n  interval: 8\n  # Add decode events to stats.\n  #decoder-events: true\n  # Decoder event prefix in stats. Has been 'decoder' before, but that leads\n  # to missing events in the eve.stats records. See issue #2225.\n  #decoder-events-prefix: \"decoder.event\"\n  # Add stream events as stats.\n  #stream-events: false\n  exception-policy:\n    #per-app-proto-errors: false  # default: false. True will log errors for\n                                  # each app-proto. Warning: VERY verbose\n\n# Plugins -- Experimental -- specify the filename for each plugin shared object\nplugins:\n  @pfring_comment@- @prefix@/lib/@PACKAGE_NAME@/pfring.so\n  @napatech_comment@- @prefix@/lib/@PACKAGE_NAME@/napatech.so\n# - /path/to/plugin.so\n\n# Configure the type of alert (and other) logging you would like.\noutputs:\n  # a line based alerts log similar to Snort's fast.log\n  - fast:\n      enabled: yes\n      filename: fast.log\n      append: yes\n      #filetype: regular # 'regular', 'unix_stream' or 'unix_dgram'\n\n  # Extensible Event Format (nicknamed EVE) event log in JSON format\n  - eve-log:\n      enabled: @e_enable_evelog@\n      filetype: regular #regular|syslog|unix_dgram|unix_stream|redis\n      filename: eve.json\n      # Enable for multi-threaded eve.json output; output files are amended with\n      # an identifier, e.g., eve.9.json\n      #threaded: false\n      #prefix: \"@cee: \" # prefix to prepend to each log entry\n      # the following are valid when type: syslog above\n      #identity: \"suricata\"\n      #facility: local5\n      #level: Info ## possible levels: Emergency, Alert, Critical,\n                   ## Error, Warning, Notice, Info, Debug\n      #ethernet: no  # log ethernet header in events when available\n      #redis:\n      #  server: 127.0.0.1\n      #  port: 6379\n      #  async: true ## if redis replies are read asynchronously\n      #  mode: list ## possible values: list|lpush (default), rpush, channel|publish, xadd|stream\n      #             ## lpush and rpush are using a Redis list. \"list\" is an alias for lpush\n      #             ## publish is using a Redis channel. \"channel\" is an alias for publish\n      #             ## xadd is using a Redis stream. \"stream\" is an alias for xadd\n      #  key: suricata ## string denoting the key/channel/stream to use (default to suricata)\n      #  stream-maxlen: 100000        ## Automatically trims the stream length to at most\n                                      ## this number of events. Set to 0 to disable trimming.\n                                      ## Only used when mode is set to xadd/stream.\n      #  stream-trim-exact: false     ## Trim exactly to the maximum stream length above.\n                                      ## Default: use inexact trimming (inexact by a few\n                                      ## tens of items)\n                                      ## Only used when mode is set to xadd/stream.\n      # Redis pipelining set up. This will enable to only do a query every\n      # 'batch-size' events. This should lower the latency induced by network\n      # connection at the cost of some memory. There is no flushing implemented\n      # so this setting should be reserved to high traffic Suricata deployments.\n      #  pipelining:\n      #    enabled: yes ## set enable to yes to enable query pipelining\n      #    batch-size: 10 ## number of entries to keep in buffer\n\n      # Include top level metadata. Default yes.\n      #metadata: no\n\n      # include the name of the input pcap file in pcap file processing mode\n      pcap-file: false\n\n      # Community Flow ID\n      # Adds a 'community_id' field to EVE records. These are meant to give\n      # records a predictable flow ID that can be used to match records to\n      # output of other tools such as Zeek (Bro).\n      #\n      # Takes a 'seed' that needs to be same across sensors and tools\n      # to make the id less predictable.\n\n      # enable/disable the community id feature.\n      community-id: false\n      # Seed value for the ID output. Valid values are 0-65535.\n      community-id-seed: 0\n\n      # HTTP X-Forwarded-For support by adding an extra field or overwriting\n      # the source or destination IP address (depending on flow direction)\n      # with the one reported in the X-Forwarded-For HTTP header. This is\n      # helpful when reviewing alerts for traffic that is being reverse\n      # or forward proxied.\n      xff:\n        enabled: no\n        # Two operation modes are available: \"extra-data\" and \"overwrite\".\n        mode: extra-data\n        # Two proxy deployments are supported: \"reverse\" and \"forward\". In\n        # a \"reverse\" deployment the IP address used is the last one, in a\n        # \"forward\" deployment the first IP address is used.\n        deployment: reverse\n        # Header name where the actual IP address will be reported. If more\n        # than one IP address is present, the last IP address will be the\n        # one taken into consideration.\n        header: X-Forwarded-For\n\n      types:\n        - alert:\n            # payload: yes             # enable dumping payload in Base64\n            # payload-buffer-size: 4 KiB  # max size of payload buffer to output in eve-log\n            # payload-printable: yes   # enable dumping payload in printable (lossy) format\n            # payload-length: yes      # enable dumping payload length, including the gaps\n            # packet: yes              # enable dumping of packet (without stream segments)\n            # metadata: no             # enable inclusion of app layer metadata with alert. Default yes\n            # If you want metadata, use:\n            # metadata:\n              # Include the decoded application layer (ie. http, dns)\n              #app-layer: true\n              # Log the current state of the flow record.\n              #flow: true\n              #rule:\n                # Log the metadata field from the rule in a structured\n                # format.\n                #metadata: true\n                # Log the raw rule text.\n                #raw: false\n                #reference: false      # include reference information from the rule\n            # http-body: yes           # Requires metadata; enable dumping of HTTP body in Base64\n            # http-body-printable: yes # Requires metadata; enable dumping of HTTP body in printable format\n            # websocket-payload: yes   # Requires metadata; enable dumping of WebSocket Payload in Base64\n            # websocket-payload-printable: yes # Requires metadata; enable dumping of WebSocket Payload in printable format\n\n            # Enable the logging of tagged packets for rules using the\n            # \"tag\" keyword.\n            tagged-packets: yes\n            # Enable logging the final action taken on a packet by the engine\n            # (e.g: the alert may have action 'allowed' but the verdict be\n            # 'drop' due to another alert. That's the engine's verdict)\n            # verdict: yes\n        # app layer frames\n        - frame:\n            # disabled by default as this is very verbose.\n            enabled: no\n            # payload-buffer-size: 4 KiB # max size of frame payload buffer to output in eve-log\n        - anomaly:\n            # Anomaly log records describe unexpected conditions such\n            # as truncated packets, packets with invalid IP/UDP/TCP\n            # length values, and other events that render the packet\n            # invalid for further processing or describe unexpected\n            # behavior on an established stream. Networks which\n            # experience high occurrences of anomalies may experience\n            # packet processing degradation.\n            #\n            # Anomalies are reported for the following:\n            # 1. Decode: Values and conditions that are detected while\n            # decoding individual packets. This includes invalid or\n            # unexpected values for low-level protocol lengths as well\n            # as stream related events (TCP 3-way handshake issues,\n            # unexpected sequence number, etc).\n            # 2. Stream: This includes stream related events (TCP\n            # 3-way handshake issues, unexpected sequence number,\n            # etc).\n            # 3. Application layer: These denote application layer\n            # specific conditions that are unexpected, invalid or are\n            # unexpected given the application monitoring state.\n            #\n            # By default, anomaly logging is enabled. When anomaly\n            # logging is enabled, applayer anomaly reporting is\n            # also enabled.\n            enabled: yes\n            #\n            # Choose one or more types of anomaly logging and whether to enable\n            # logging of the packet header for packet anomalies.\n            types:\n              # decode: no\n              # stream: no\n              # applayer: yes\n            #packethdr: no\n        - http:\n            extended: yes     # enable this for extended logging information\n            # custom allows additional HTTP fields to be included in eve-log.\n            # the example below adds three additional fields when uncommented\n            #custom: [Accept-Encoding, Accept-Language, Authorization]\n            # set this value to one and only one from {both, request, response}\n            # to dump all HTTP headers for every HTTP request and/or response\n            # dump-all-headers: none\n        - dns:\n            # Suricata 8.0 uses a new DNS logging format, to keep with\n            # the old format while you upgrade the version can be set\n            # to 2. See https://docs.suricata.io/en/latest/upgrade/8.0-dns-logging-changes.html\n            #version: 3\n\n            # Enable/disable this logger. Default: enabled.\n            #enabled: yes\n\n            # Control logging of requests and responses:\n            # - requests: enable logging of DNS queries\n            # - responses: enable logging of DNS answers\n            # By default both requests and responses are logged.\n            #requests: no\n            #responses: no\n\n            # Format of answer logging:\n            # - detailed: array item per answer\n            # - grouped: answers aggregated by type\n            # Default: all\n            #formats: [detailed, grouped]\n\n            # DNS record types to log, based on the query type.\n            # Default: all.\n            #types: [a, aaaa, cname, mx, ns, ptr, txt]\n        - tls:\n            extended: yes     # enable this for extended logging information\n            # output TLS transaction where the session is resumed using a\n            # session id\n            #session-resumption: no\n            # custom controls which TLS fields that are included in eve-log\n            # WARNING: enabling custom disables extended logging.\n            #custom: [subject, issuer, session_resumed, serial, fingerprint, sni, version, not_before, not_after, certificate, chain, ja3, ja3s, ja4, subjectaltname, client, client_certificate, client_chain, client_alpns, server_alpns]\n        - files:\n            force-magic: no   # force logging magic on all logged files\n            # force logging of checksums, available hash functions are md5,\n            # sha1 and sha256\n            #force-hash: [md5]\n        #- drop:\n        #    alerts: yes      # log alerts that caused drops\n        #    flows: all       # start or all: 'start' logs only a single drop\n        #                     # per flow direction. All logs each dropped pkt.\n            # Enable logging the final action taken on a packet by the engine\n            # (will show more information in case of a drop caused by 'reject')\n            # verdict: yes\n        - smtp:\n            #extended: yes # enable this for extended logging information\n            # this includes: bcc, message-id, subject, x_mailer, user-agent\n            # custom fields logging from the list:\n            #  reply-to, bcc, message-id, subject, x-mailer, user-agent, received,\n            #  x-originating-ip, in-reply-to, references, importance, priority,\n            #  sensitivity, organization, content-md5, date\n            #custom: [received, x-mailer, x-originating-ip, relays, reply-to, bcc]\n            # output md5 of fields: body, subject\n            # for the body you need to set app-layer.protocols.smtp.mime.body-md5\n            # to yes\n            #md5: [body, subject]\n\n        #- dnp3\n        - websocket\n        #- enip\n        - ftp\n        - rdp\n        - nfs\n        - smb\n        - tftp\n        - ike\n        - dcerpc\n        - krb5\n        - bittorrent-dht\n        - snmp\n        - rfb\n        - sip\n        - quic\n        - ldap\n        - arp:\n            enabled: no        # Many events can be logged. Disabled by default\n        - dhcp:\n            enabled: yes\n            # When extended mode is on, all DHCP messages are logged\n            # with full detail. When extended mode is off (the\n            # default), just enough information to map a MAC address\n            # to an IP address is logged.\n            extended: no\n        - ssh\n        - mqtt:\n            # passwords: yes           # enable output of passwords\n            # string-log-limit: 1KiB   # limit size of logged strings in bytes.\n                                       # Can be specified in KiB, MiB, GiB. Just a number\n                                       # is parsed as bytes. Default is 1 KiB.\n                                       # Use a value of 0 to disable limiting.\n                                       # Note that the size is also bounded by\n                                       # the maximum parsed message size (see\n                                       # app-layer configuration)\n        - http2\n        # dns over http2\n        - doh2\n        - pgsql:\n            enabled: no\n            # passwords: yes           # enable output of passwords. Disabled by default\n        - stats:\n            totals: yes       # stats for all threads merged together\n            threads: no       # per thread stats\n            deltas: no        # include delta values\n            # Don't log stats counters that are zero. Default: true\n            #null-values: false    # False will NOT log stats counters: 0\n        # bi-directional flows\n        - flow\n        # uni-directional flows\n        #- netflow\n\n        # Metadata event type. Triggered whenever a pktvar is saved\n        # and will include the pktvars, flowvars, flowbits and\n        # flowints.\n        #- metadata\n\n        # EXPERIMENTAL per packet output giving TCP state tracking details\n        # including internal state, flags, etc.\n        # This output is experimental, meant for debugging and subject to\n        # change in both config and output without any notice.\n        #- stream:\n        #   all: false                      # log all TCP packets\n        #   event-set: false                # log packets that have a decoder/stream event\n        #   state-update: false             # log packets triggering a TCP state update\n        #   spurious-retransmission: false  # log spurious retransmission packets\n\n  # output module to store certificates chain to disk\n  - tls-store:\n      enabled: no\n      #certs-log-dir: certs # directory to store the certificates files\n\n  # Packet log... log packets in pcap format. 2 modes of operation: \"normal\"\n  # and \"multi\".\n  #\n  # In normal mode a pcap file \"filename\" is created in the default-log-dir,\n  # or as specified by \"dir\".\n  # In multi mode, a file is created per thread. This will perform much\n  # better, but will create multiple files where 'normal' would create one.\n  # In multi mode the filename takes a few special variables:\n  # - %n -- thread number\n  # - %i -- thread id\n  # - %t -- timestamp (secs or secs.usecs based on 'ts-format'\n  # E.g. filename: pcap.%n.%t\n  #\n  # Note that it's possible to use directories, but the directories are not\n  # created by Suricata. E.g. filename: pcaps/%n/log.%s will log into the\n  # per thread directory.\n  #\n  # Also note that the limit and max-files settings are enforced per thread.\n  # So the size limit when using 8 threads with 1000 MiB files and 2000 files\n  # is: 8*1000*2000 ~ 16TiB.\n  #\n  # By default all packets are logged except:\n  # - TCP streams beyond stream.reassembly.depth\n  # - encrypted streams after the key exchange\n  #\n  - pcap-log:\n      enabled: no\n      filename: log.pcap\n\n      # File size limit.  Can be specified in kb, mb, gb.  Just a number\n      # is parsed as bytes.\n      limit: 1000 MiB\n\n      # If set to a value, ring buffer mode is enabled. Will keep maximum of\n      # \"max-files\" of size \"limit\"\n      max-files: 2000\n\n      # Compression algorithm for pcap files. Possible values: none, lz4.\n      # Note also that on Windows, enabling compression will *increase* disk I/O.\n      compression: none\n\n      # Further options for lz4 compression. The compression level can be set\n      # to a value between 0 and 16, where higher values result in higher\n      # compression.\n      #lz4-checksum: no\n      #lz4-level: 0\n\n      mode: normal # normal or multi\n\n      # Directory to place pcap files. If not provided the default log\n      # directory will be used.\n      #dir: /nsm_data/\n\n      #ts-format: usec # sec or usec second format (default) is filename.sec usec is filename.sec.usec\n      use-stream-depth: no #If set to \"yes\" packets seen after reaching stream inspection depth are ignored. \"no\" logs all packets\n      honor-pass-rules: no # If set to \"yes\", flows in which a pass rule matched will stop being logged.\n      # Use \"all\" to log all packets or use \"alerts\" to log only alerted packets and flows or \"tag\"\n      # to log only flow tagged via the \"tag\" keyword\n      #conditional: all\n\n  # a full alert log containing much information for signature writers\n  # or for investigating suspected false positives.\n  - alert-debug:\n      enabled: no\n      filename: alert-debug.log\n      append: yes\n      #filetype: regular # 'regular', 'unix_stream' or 'unix_dgram'\n\n  # Stats.log contains data from various counters of the Suricata engine.\n  - stats:\n      enabled: yes\n      filename: stats.log\n      append: yes       # append to file (yes) or overwrite it (no)\n      totals: yes       # stats for all threads merged together\n      threads: no       # per thread stats\n      #null-values: yes  # print counters that have value 0. Default: no\n\n  # Output module for storing files on disk. Files are stored in\n  # directory names consisting of the first 2 characters of the\n  # SHA256 of the file. Each file is given its SHA256 as a filename.\n  #\n  # When a duplicate file is found, the timestamps on the existing file\n  # are updated.\n  #\n  # Unlike the older filestore, metadata is not written by default\n  # as each file should already have a \"fileinfo\" record in the\n  # eve-log. If write-fileinfo is set to yes, then each file will have\n  # one more associated .json files that consist of the fileinfo\n  # record. A fileinfo file will be written for each occurrence of the\n  # file seen using a filename suffix to ensure uniqueness.\n  #\n  # To prune the filestore directory see the \"suricatactl filestore\n  # prune\" command which can delete files over a certain age.\n  - file-store:\n      version: 2\n      enabled: no\n\n      # Set the directory for the filestore. Relative pathnames\n      # are contained within the \"default-log-dir\".\n      #dir: filestore\n\n      # Write out a fileinfo record for each occurrence of a file.\n      # Disabled by default as each occurrence is already logged\n      # as a fileinfo record to the main eve-log.\n      #write-fileinfo: yes\n\n      # Force storing of all files. Default: no.\n      #force-filestore: yes\n\n      # Override the global stream-depth for sessions in which we want\n      # to perform file extraction. Set to 0 for unlimited; otherwise,\n      # must be greater than the global stream-depth value to be used.\n      #stream-depth: 0\n\n      # Uncomment the following variable to define how many files can\n      # remain open for filestore by Suricata. Default value is 0 which\n      # means files get closed after each write to the file.\n      #max-open-files: 1000\n\n      # Force logging of checksums: available hash functions are md5,\n      # sha1 and sha256. Note that SHA256 is automatically forced by\n      # the use of this output module as it uses the SHA256 as the\n      # file naming scheme.\n      #force-hash: [sha1, md5]\n      # NOTE: X-Forwarded configuration is ignored if write-fileinfo is disabled\n      # HTTP X-Forwarded-For support by adding an extra field or overwriting\n      # the source or destination IP address (depending on flow direction)\n      # with the one reported in the X-Forwarded-For HTTP header. This is\n      # helpful when reviewing alerts for traffic that is being reverse\n      # or forward proxied.\n      xff:\n        enabled: no\n        # Two operation modes are available, \"extra-data\" and \"overwrite\".\n        mode: extra-data\n        # Two proxy deployments are supported, \"reverse\" and \"forward\". In\n        # a \"reverse\" deployment the IP address used is the last one, in a\n        # \"forward\" deployment the first IP address is used.\n        deployment: reverse\n        # Header name where the actual IP address will be reported. If more\n        # than one IP address is present, the last IP address will be the\n        # one taken into consideration.\n        header: X-Forwarded-For\n\n  # Log TCP data after stream normalization\n  # Two types: file or dir:\n  #     - file logs into a single logfile.\n  #     - dir creates 2 files per TCP session and stores the raw TCP\n  #            data into them.\n  # Use 'both' to enable both file and dir modes.\n  #\n  # Note: limited by \"stream.reassembly.depth\"\n  - tcp-data:\n      enabled: no\n      type: file\n      filename: tcp-data.log\n\n  # Log HTTP body data after normalization, de-chunking and unzipping.\n  # Two types: file or dir.\n  #     - file logs into a single logfile.\n  #     - dir creates 2 files per HTTP session and stores the\n  #           normalized data into them.\n  # Use 'both' to enable both file and dir modes.\n  #\n  # Note: limited by the body limit settings\n  - http-body-data:\n      enabled: no\n      type: file\n      filename: http-data.log\n\n  # Lua Output Support - execute lua script to generate alert and event\n  # output.\n  # Documented at:\n  # https://docs.suricata.io/en/latest/output/lua-output.html\n  - lua:\n      enabled: no\n      #scripts-dir: /etc/suricata/lua-output/\n      scripts:\n      #   - script1.lua\n\n# Logging configuration.  This is not about logging IDS alerts/events, but\n# output about what Suricata is doing, like startup messages, errors, etc.\nlogging:\n  # The default log level: can be overridden in an output section.\n  # Note that debug level logging will only be emitted if Suricata was\n  # compiled with the --enable-debug configure option.\n  #\n  # This value is overridden by the SC_LOG_LEVEL env var.\n  default-log-level: notice\n\n  # The default output format.  Optional parameter, should default to\n  # something reasonable if not provided.  Can be overridden in an\n  # output section.  You can leave this out to get the default.\n  #\n  # This console log format value can be overridden by the SC_LOG_FORMAT env var.\n  #default-log-format: \"%D: %S: %M\"\n  #\n  # For the pre-7.0 log format use:\n  #default-log-format: \"[%i] %t [%S] - (%f:%l) <%d> (%n) -- \"\n\n  # A regex to filter output.  Can be overridden in an output section.\n  # Defaults to empty (no filter).\n  #\n  # This value is overridden by the SC_LOG_OP_FILTER env var.\n  default-output-filter:\n\n  # Requires libunwind to be available when Suricata is configured and built.\n  # If a signal unexpectedly terminates Suricata, displays a brief diagnostic\n  # message with the offending stacktrace if enabled.\n  #stacktrace-on-signal: on\n\n  # Define your logging outputs.  If none are defined, or they are all\n  # disabled you will get the default: console output.\n  outputs:\n  - console:\n      enabled: yes\n      # type: json\n  - file:\n      enabled: yes\n      level: info\n      filename: suricata.log\n      # format: \"[%i - %m] %z %d: %S: %M\"\n      # type: json\n  - syslog:\n      enabled: no\n      facility: local5\n      format: \"[%i] <%d> -- \"\n      # type: json\n\n\n##\n## Step 3: Configure common capture settings\n##\n## See \"Advanced Capture Options\" below for more options, including Netmap\n## and PF_RING.\n##\n\n# Linux high speed capture support\naf-packet:\n  - interface: eth0\n    # Number of receive threads. \"auto\" uses the number of cores\n    #threads: auto\n    # Default clusterid. AF_PACKET will load balance packets based on flow.\n    cluster-id: 99\n    # Default AF_PACKET cluster type. AF_PACKET can load balance per flow or per hash.\n    # This is only supported for Linux kernel > 3.1\n    # possible value are:\n    #  * cluster_flow: all packets of a given flow are sent to the same socket\n    #  * cluster_cpu: all packets treated in kernel by a CPU are sent to the same socket\n    #  * cluster_qm: all packets linked by network card to a RSS queue are sent to the same\n    #  socket. Requires at least Linux 3.14.\n    #  * cluster_ebpf: eBPF file load balancing. See doc/userguide/capture-hardware/ebpf-xdp.rst for\n    #  more info.\n    # Recommended modes are cluster_flow on most boxes and cluster_cpu or cluster_qm on system\n    # with capture card using RSS (requires cpu affinity tuning and system IRQ tuning)\n    # cluster_rollover has been deprecated; if used, it'll be replaced with cluster_flow.\n    cluster-type: cluster_flow\n    # In some fragmentation cases, the hash can not be computed. If \"defrag\" is set\n    # to yes, the kernel will do the needed defragmentation before sending the packets.\n    defrag: yes\n    # To use the ring feature of AF_PACKET, set 'use-mmap' to yes\n    #use-mmap: yes\n    # Lock memory map to avoid it being swapped. Be careful that over\n    # subscribing could lock your system\n    #mmap-locked: yes\n    # Use tpacket_v3 capture mode, only active if use-mmap is true\n    # Don't use it in IPS or TAP mode as it causes severe latency\n    #tpacket-v3: yes\n    # Ring size will be computed with respect to \"max-pending-packets\" and number\n    # of threads. You can set manually the ring size in number of packets by setting\n    # the following value. If you are using flow \"cluster-type\" and have really network\n    # intensive single-flow you may want to set the \"ring-size\" independently of the number\n    # of threads:\n    #ring-size: 2048\n    # Block size is used by tpacket_v3 only. It should set to a value high enough to contain\n    # a decent number of packets. Size is in bytes so please consider your MTU. It should be\n    # a power of 2 and it must be multiple of page size (usually 4096).\n    #block-size: 32768\n    # tpacket_v3 block timeout: an open block is passed to userspace if it is not\n    # filled after block-timeout milliseconds.\n    #block-timeout: 10\n    # On busy systems, set it to yes to help recover from a packet drop\n    # phase. This will result in some packets (at max a ring flush) not being inspected.\n    #use-emergency-flush: yes\n    # recv buffer size, increased value could improve performance\n    # buffer-size: 32768\n    # Set to yes to disable promiscuous mode\n    # disable-promisc: no\n    # Choose checksum verification mode for the interface. At the moment\n    # of the capture, some packets may have an invalid checksum due to\n    # the checksum computation being offloaded to the network card.\n    # Possible values are:\n    #  - kernel: use indication sent by kernel for each packet (default)\n    #  - yes: checksum validation is forced\n    #  - no: checksum validation is disabled\n    #  - auto: Suricata uses a statistical approach to detect when\n    #  checksum off-loading is used.\n    # Warning: 'capture.checksum-validation' must be set to yes to have any validation\n    #checksum-checks: kernel\n    # BPF filter to apply to this interface. The pcap filter syntax applies here.\n    #bpf-filter: port 80 or udp\n    # You can use the following variables to activate AF_PACKET tap or IPS mode.\n    # If copy-mode is set to ips or tap, the traffic coming to the current\n    # interface will be copied to the copy-iface interface. If 'tap' is set, the\n    # copy is complete. If 'ips' is set, the packet matching a 'drop' action\n    # will not be copied.\n    #copy-mode: ips\n    #copy-iface: eth1\n    #  For eBPF and XDP setup including bypass, filter and load balancing, please\n    #  see doc/userguide/capture-hardware/ebpf-xdp.rst for more info.\n\n  # Put default values here. These will be used for an interface that is not\n  # in the list above.\n  - interface: default\n    #threads: auto\n    #use-mmap: no\n    #tpacket-v3: yes\n\n# Linux high speed af-xdp capture support\naf-xdp:\n  - interface: default\n    # Number of receive threads. \"auto\" uses least between the number\n    # of cores and RX queues\n    #threads: auto\n    #disable-promisc: false\n    # XDP_DRV mode can be chosen when the driver supports XDP\n    # XDP_SKB mode can be chosen when the driver does not support XDP\n    # Possible values are:\n    #  - drv: enable XDP_DRV mode\n    #  - skb: enable XDP_SKB mode\n    #  - none: disable (kernel in charge of applying mode)\n    #force-xdp-mode: none\n    # During socket binding the kernel will attempt zero-copy, if this\n    # fails it will fallback to copy. If this fails, the bind fails.\n    # The bind can be explicitly configured using the option below.\n    # If configured, the bind will fail if not successful (no fallback).\n    # Possible values are:\n    #  - zero: enable zero-copy mode\n    #  - copy: enable copy mode\n    #  - none: disable (kernel in charge of applying mode)\n    #force-bind-mode: none\n    # Memory alignment mode can vary between two modes, aligned and\n    # unaligned chunk modes. By default, aligned chunk mode is selected.\n    # select 'yes' to enable unaligned chunk mode.\n    # Note: unaligned chunk mode uses hugepages, so the required number\n    # of pages must be available.\n    #mem-unaligned: no\n    # The following options configure the prefer-busy-polling socket\n    # options. The polling time and budget can be edited here.\n    # Possible values are:\n    #  - yes: enable (default)\n    #  - no: disable\n    #enable-busy-poll: yes\n    # busy-poll-time sets the approximate time in microseconds to busy\n    # poll on a blocking receive when there is no data.\n    #busy-poll-time: 20\n    # busy-poll-budget is the budget allowed for packet batches\n    #busy-poll-budget: 64\n    # These two tunables are used to configure the Linux OS's NAPI\n    # context. Their purpose is to defer enabling of interrupts and\n    # instead schedule the NAPI context from a watchdog timer.\n    # The softirq NAPI will exit early, allowing busy polling to be\n    # performed. Successfully setting these tunables alongside busy-polling\n    # should improve performance.\n    # Defaults are:\n    #gro-flush-timeout: 2000000\n    #napi-defer-hard-irq: 2\n\ndpdk:\n  eal-params:\n    proc-type: primary\n\n  # DPDK capture support\n  # RX queues (and TX queues in IPS mode) are assigned to cores in 1:1 ratio\n  interfaces:\n    - interface: 0000:3b:00.0 # PCIe address of the NIC port\n      # Threading: possible values are either \"auto\" or number of threads\n      # - auto takes all cores\n      # in IPS mode it is required to specify the number of cores and the numbers on both interfaces must match\n      threads: auto\n      # interrupt-mode: false # true to switch to interrupt mode \n      promisc: true # promiscuous mode - capture all packets\n      multicast: true # enables also detection on multicast packets\n      checksum-checks: true # if Suricata should validate checksums\n      checksum-checks-offload: true # if possible offload checksum validation to the NIC (saves Suricata resources)\n      mtu: 1500 # Set MTU of the device in bytes\n      vlan-strip-offload: false # if possible enable hardware vlan stripping\n      # rss-hash-functions: 0x0 # advanced configuration option, use only if you use untested NIC card and experience RSS warnings,\n      # For `rss-hash-functions` use hexadecimal 0x01ab format to specify RSS hash function flags - DumpRssFlags can help (you can see output if you use -vvv option during Suri startup)\n      # setting auto to rss_hf sets the default RSS hash functions (based on IP addresses)\n\n      # To approximately calculate required amount of space (in bytes) for interface's mempool: mempool-size * mtu\n      # Make sure you have enough allocated hugepages.\n      # The optimum size for the packet memory pool (in terms of memory usage) is power of two minus one: n = (2^q - 1)\n      mempool-size: 65535 # The number of elements in the mbuf pool\n\n      # Mempool cache size must be lower or equal to:\n      #     - RTE_MEMPOOL_CACHE_MAX_SIZE (by default 512) and\n      #     - \"mempool-size / 1.5\"\n      # It is advised to choose cache_size to have \"mempool-size modulo cache_size == 0\".\n      # If this is not the case, some elements will always stay in the pool and will never be used.\n      # The cache can be disabled if the cache_size argument is set to 0, can be useful to avoid losing objects in cache\n      # If the value is empty or set to \"auto\", Suricata will attempt to set cache size of the mempool to a value\n      # that matches the previously mentioned recommendations\n      mempool-cache-size: 257\n      rx-descriptors: 1024\n      tx-descriptors: 1024\n      #\n      # IPS mode for Suricata works in 3 modes - none, tap, ips\n      # - none: IDS mode only - disables IPS functionality (does not further forward packets)\n      # - tap: forwards all packets and generates alerts (omits DROP action) This is not DPDK TAP\n      # - ips: the same as tap mode but it also drops packets that are flagged by rules to be dropped\n      copy-mode: none\n      copy-iface: none # or PCIe address of the second interface\n\n    - interface: default\n      threads: auto\n      promisc: true\n      multicast: true\n      checksum-checks: true\n      checksum-checks-offload: true\n      mtu: 1500\n      vlan-strip-offload: false\n      rss-hash-functions: auto\n      mempool-size: 65535\n      mempool-cache-size: 257\n      rx-descriptors: 1024\n      tx-descriptors: 1024\n      copy-mode: none\n      copy-iface: none\n\n\n# Cross platform libpcap capture support\npcap:\n  - interface: eth0\n    # On Linux, pcap will try to use mmap'ed capture and will use \"buffer-size\"\n    # as total memory used by the ring. So set this to something bigger\n    # than 1% of your bandwidth.\n    #buffer-size: 16777216\n    #bpf-filter: \"tcp and port 25\"\n    # Choose checksum verification mode for the interface. At the moment\n    # of the capture, some packets may have an invalid checksum due to\n    # the checksum computation being offloaded to the network card.\n    # Possible values are:\n    #  - yes: checksum validation is forced\n    #  - no: checksum validation is disabled\n    #  - auto: Suricata uses a statistical approach to detect when\n    #  checksum off-loading is used. (default)\n    # Warning: 'capture.checksum-validation' must be set to yes to have any validation\n    #checksum-checks: auto\n    # With some accelerator cards using a modified libpcap (like Myricom), you\n    # may want to have the same number of capture threads as the number of capture\n    # rings. In this case, set up the threads variable to N to start N threads\n    # listening on the same interface.\n    #threads: 16\n    # set to no to disable promiscuous mode:\n    #promisc: no\n    # set snaplen, if not set it defaults to MTU if MTU can be known\n    # via ioctl call and to full capture if not.\n    #snaplen: 1518\n  # Put default values here\n  - interface: default\n    #checksum-checks: auto\n\n# Settings for reading pcap files\npcap-file:\n  # Possible values are:\n  #  - yes: checksum validation is forced\n  #  - no: checksum validation is disabled\n  #  - auto: Suricata uses a statistical approach to detect when\n  #  checksum off-loading is used. (default)\n  # Warning: 'checksum-validation' must be set to yes to have checksum tested\n  checksum-checks: auto\n  # Read buffer size set using setvbuf. Max value is 64 MiB. Linux only.\n  #buffer-size: 128 KiB\n\n# See \"Advanced Capture Options\" below for more options, including Netmap\n# and PF_RING.\n\n\n##\n## Step 4: App Layer Protocol configuration\n##\n\n# Configure the app-layer parsers.\n#\n# The exception policy error-policy setting applies to all app-layer parsers.\n#  Values can be \"drop-flow\", \"pass-flow\", \"bypass\", \"drop-packet\", \"pass-packet\",\n#  \"reject\" or \"ignore\" (the default).\n#\n# The protocol's section details each protocol.\n#\n# The option \"enabled\" takes 3 values - \"yes\", \"no\", \"detection-only\".\n# \"yes\" enables both detection and the parser, \"no\" disables both, and\n# \"detection-only\" enables protocol detection only (parser disabled).\napp-layer:\n  # error-policy: ignore\n  protocols:\n    telnet:\n      enabled: yes\n    rfb:\n      enabled: yes\n      detection-ports:\n        dp: 5900, 5901, 5902, 5903, 5904, 5905, 5906, 5907, 5908, 5909\n    mqtt:\n      enabled: yes\n      # max-msg-length: 1 MiB\n      # subscribe-topic-match-limit: 100\n      # unsubscribe-topic-match-limit: 100\n      # Maximum number of live MQTT transactions per flow\n      # max-tx: 4096\n    krb5:\n      enabled: yes\n    bittorrent-dht:\n      enabled: yes\n    snmp:\n      enabled: yes\n    ike:\n      enabled: yes\n    tls:\n      enabled: yes\n      detection-ports:\n        dp: 443\n\n      # Generate JA3/JA4 fingerprints from client hello. If not specified it\n      # will be disabled by default, but enabled if rules require it.\n      #ja3-fingerprints: auto\n      #ja4-fingerprints: auto\n\n      # What to do when the encrypted communications start:\n      # - default: keep tracking TLS session, check for protocol anomalies,\n      #            inspect tls_* keywords. Disables inspection of unmodified\n      #            'content' signatures.\n      # - bypass:  stop processing this flow as much as possible. No further\n      #            TLS parsing and inspection. Offload flow bypass to kernel\n      #            or hardware if possible.\n      # - full:    keep tracking and inspection as normal. Unmodified content\n      #            keyword signatures are inspected as well.\n      #\n      # For best performance, select 'bypass'.\n      #\n      #encryption-handling: default\n\n    pgsql:\n      enabled: no\n      # Stream reassembly size for PostgreSQL. By default, track it completely.\n      stream-depth: 0\n      # Maximum number of live PostgreSQL transactions per flow\n      # max-tx: 1024\n    dcerpc:\n      enabled: yes\n      # Maximum number of live DCERPC transactions per flow\n      # max-tx: 1024\n    ftp:\n      enabled: yes\n      # memcap: 64 MiB\n    websocket:\n      #enabled: yes\n      # Maximum used payload size, the rest is skipped\n      # max-payload-size: 64 KiB\n    rdp:\n      #enabled: yes\n    ssh:\n      enabled: yes\n      #hassh: yes\n    doh2:\n      enabled: yes\n    http2:\n      enabled: yes\n      # Maximum number of live HTTP2 streams in a flow\n      #max-streams: 4096\n      # Maximum headers table size\n      #max-table-size: 65536\n      # Maximum reassembly size for header + continuation frames\n      #max-reassembly-size: 102400\n    smtp:\n      enabled: yes\n      raw-extraction: no\n      # Maximum number of live SMTP transactions per flow\n      # max-tx: 256\n      # Configure SMTP-MIME Decoder\n      mime:\n        # Decode MIME messages from SMTP transactions\n        # (may be resource intensive)\n        # This field supersedes all others because it turns the entire\n        # process on or off\n        decode-mime: yes\n\n        # Decode MIME entity bodies (ie. Base64, quoted-printable, etc.)\n        decode-base64: yes\n        decode-quoted-printable: yes\n\n        # Maximum bytes per header data value stored in the data structure\n        # (default is 2000)\n        header-value-depth: 2000\n\n        # Extract URLs and save in state data structure\n        extract-urls: yes\n        # Scheme of URLs to extract\n        # (default is [http])\n        #extract-urls-schemes: [http, https, ftp, mailto]\n        # Log the scheme of URLs that are extracted\n        # (default is no)\n        #log-url-scheme: yes\n        # Set to yes to compute the md5 of the mail body. You will then\n        # be able to journalize it.\n        body-md5: no\n      # Configure inspected-tracker for file_data keyword\n      inspected-tracker:\n        content-limit: 100000\n        content-inspect-min-size: 32768\n        content-inspect-window: 4096\n    imap:\n      enabled: detection-only\n    pop3:\n      enabled: detection-only\n    smb:\n      enabled: yes\n      detection-ports:\n        dp: 139, 445\n      # Maximum number of live SMB transactions per flow\n      # max-tx: 1024\n\n      # Stream reassembly size for SMB streams. By default track it completely.\n      #stream-depth: 0\n\n    nfs:\n      enabled: yes\n      # max-tx: 1024\n    tftp:\n      enabled: yes\n    dns:\n      tcp:\n        enabled: yes\n        detection-ports:\n          dp: 53\n      udp:\n        enabled: yes\n        detection-ports:\n          dp: 53\n    http:\n      enabled: yes\n\n      # Byte Range Containers default settings\n      # byterange:\n      #   memcap: 100 MiB\n      #   timeout: 60\n\n      # memcap:                   Maximum memory capacity for HTTP\n      #                           Default is unlimited, values can be 64 MiB, e.g.\n\n      # default-config:           Used when no server-config matches\n      #   personality:            List of personalities used by default\n      #   request-body-limit:     Limit reassembly of request body for inspection\n      #                           by http_client_body & pcre /P option.\n      #   response-body-limit:    Limit reassembly of response body for inspection\n      #                           by file_data, http_server_body & pcre /Q option.\n      #\n      #   For advanced options, see the user guide\n\n\n      # server-config:            List of server configurations to use if address matches\n      #   address:                List of IP addresses or networks for this block\n      #   personality:            List of personalities used by this block\n      #\n      #                           Then, all the fields from default-config can be overloaded\n      #\n      # Currently Available Personalities:\n      #   Minimal, Generic, IDS (default), IIS_4_0, IIS_5_0, IIS_5_1, IIS_6_0,\n      #   IIS_7_0, IIS_7_5, Apache_2\n      libhtp:\n         default-config:\n           personality: IDS\n\n           # Can be specified in KiB, MiB, GiB.  Just a number indicates\n           # it's in bytes.\n           request-body-limit: 100 KiB\n           response-body-limit: 100 KiB\n\n           # inspection limits\n           request-body-minimal-inspect-size: 32 KiB\n           request-body-inspect-window: 4 KiB\n           response-body-minimal-inspect-size: 40 KiB\n           response-body-inspect-window: 16 KiB\n\n           # response body decompression (0 disables)\n           response-body-decompress-layer-limit: 2\n\n           # auto will use http-body-inline mode in IPS mode, yes or no set it statically\n           http-body-inline: auto\n\n           # Decompress SWF files. Disabled by default.\n           # Two types: 'deflate', 'lzma', 'both' will decompress deflate and lzma\n           # compress-depth:\n           # Specifies the maximum amount of data to decompress,\n           # set 0 for unlimited.\n           # decompress-depth:\n           # Specifies the maximum amount of decompressed data to obtain,\n           # set 0 for unlimited.\n           swf-decompression:\n             enabled: no\n             type: both\n             compress-depth: 100 KiB\n             decompress-depth: 100 KiB\n\n           # Use a random value for inspection sizes around the specified value.\n           # This lowers the risk of some evasion techniques but could lead\n           # to detection change between runs. It is set to 'yes' by default.\n           #randomize-inspection-sizes: yes\n           # If \"randomize-inspection-sizes\" is active, the value of various\n           # inspection size will be chosen from the [1 - range%, 1 + range%]\n           # range\n           # Default value of \"randomize-inspection-range\" is 10.\n           #randomize-inspection-range: 10\n\n           # decoding\n           double-decode-path: no\n           double-decode-query: no\n\n           # Can enable LZMA decompression\n           #lzma-enabled: false\n           # Memory limit usage for LZMA decompression dictionary\n           # Data is decompressed until dictionary reaches this size\n           #lzma-memlimit: 1 MiB\n           # Maximum decompressed size with a compression ratio\n           # above 2048 (only LZMA can reach this ratio, deflate cannot)\n           #compression-bomb-limit: 1 MiB\n           # Maximum time spent decompressing a single transaction in usec\n           #decompression-time-limit: 100000\n           # Maximum number of live transactions per flow\n           #max-tx: 512\n           # Maximum used number of HTTP1 headers in one request or response\n           #headers-limit: 1024\n\n         server-config:\n\n           #- apache:\n           #    address: [192.168.1.0/24, 127.0.0.0/8, \"::1\"]\n           #    personality: Apache_2\n           #    # Can be specified in KiB, MiB, GiB.  Just a number indicates\n           #    # it's in bytes.\n           #    request-body-limit: 4096\n           #    response-body-limit: 4096\n           #    double-decode-path: no\n           #    double-decode-query: no\n\n           #- iis7:\n           #    address:\n           #      - 192.168.0.0/24\n           #      - 192.168.10.0/24\n           #    personality: IIS_7_0\n           #    # Can be specified in KiB, MiB, GiB.  Just a number indicates\n           #    # it's in bytes.\n           #    request-body-limit: 4096\n           #    response-body-limit: 4096\n           #    double-decode-path: no\n           #    double-decode-query: no\n\n    # Note: Modbus probe parser is minimalist due to the limited usage in the field.\n    # Only Modbus message length (greater than Modbus header length)\n    # and protocol ID (equal to 0) are checked in probing parser\n    # It is important to enable detection port and define Modbus port\n    # to avoid false positives\n    modbus:\n      # How many unanswered Modbus requests are considered a flood.\n      # If the limit is reached, the app-layer-event:modbus.flooded; will match.\n      #request-flood: 500\n\n      enabled: no\n      detection-ports:\n        dp: 502\n      # According to MODBUS Messaging on TCP/IP Implementation Guide V1.0b, it\n      # is recommended to keep the TCP connection opened with a remote device\n      # and not to open and close it for each MODBUS/TCP transaction. In that\n      # case, it is important to set the depth of the stream reassembling as\n      # unlimited (stream.reassembly.depth: 0)\n\n      # Stream reassembly size for modbus. By default track it completely.\n      stream-depth: 0\n\n    # DNP3\n    dnp3:\n      enabled: no\n      detection-ports:\n        dp: 20000\n\n    # SCADA EtherNet/IP and CIP protocol support\n    enip:\n      enabled: no\n      detection-ports:\n        dp: 44818\n        sp: 44818\n\n    ntp:\n      enabled: yes\n\n    quic:\n      enabled: yes\n\n    dhcp:\n      enabled: yes\n\n    sip:\n      #enabled: yes\n\n    ldap:\n      tcp:\n        enabled: yes\n        detection-ports:\n          dp: 389, 3268\n      udp:\n        enabled: yes\n        detection-ports:\n          dp: 389, 3268\n      # Maximum number of live LDAP transactions per flow\n      # max-tx: 1024\n\n# Limit for the maximum number of asn1 frames to decode (default 256)\nasn1-max-frames: 256\n\n# Datasets default settings\ndatasets:\n  # Default fallback memcap and hashsize values for datasets in case these\n  # were not explicitly defined.\n  defaults:\n    #memcap: 100 MiB\n    #hashsize: 2048\n\n  rules:\n    # Set to true to allow absolute filenames and filenames that use\n    # \"..\" components to reference parent directories in rules that specify\n    # their filenames.\n    #allow-absolute-filenames: false\n\n    # Allow datasets in rules write access for \"save\" and\n    # \"state\". This is enabled by default, however write access is\n    # limited to the data directory.\n    #allow-write: true\n\n##############################################################################\n##\n## Advanced settings below\n##\n##############################################################################\n\n##\n## Run Options\n##\n\n# Run Suricata with a specific user-id and group-id:\n#run-as:\n#  user: suri\n#  group: suri\n\nsecurity:\n  # if true, prevents process creation from Suricata by calling\n  # setrlimit(RLIMIT_NPROC, 0)\n  limit-noproc: true\n  # Use landlock security module under Linux\n  landlock:\n    enabled: no\n    directories:\n      #write:\n      #  - @e_rundir@\n      # /usr and /etc folders are added to read list to allow\n      # file magic to be used.\n      read:\n        - /usr/\n        - /etc/\n        - @e_sysconfdir@\n\n  lua:\n    # Allow Lua rules. Disabled by default.\n    #allow-rules: false\n\n# Some logging modules will use that name in event as identifier. The default\n# value is the hostname\n#sensor-name: suricata\n\n# Default location of the pid file. The pid file is only used in\n# daemon mode (start Suricata with -D). If not running in daemon mode\n# the --pidfile command line option must be used to create a pid file.\n#pid-file: @e_rundir@suricata.pid\n\n# Daemon working directory\n# Suricata will change directory to this one if provided\n# Default: \"/\"\n#daemon-directory: \"/\"\n\n# Umask.\n# Suricata will use this umask if it is provided. By default it will use the\n# umask passed on by the shell.\n#umask: 022\n\n# Suricata core dump configuration. Limits the size of the core dump file to\n# approximately max-dump. The actual core dump size will be a multiple of the\n# page size. Core dumps that would be larger than max-dump are truncated. On\n# Linux, the actual core dump size may be a few pages larger than max-dump.\n# Setting max-dump to 0 disables core dumping.\n# Setting max-dump to 'unlimited' will give the full core dump file.\n# On 32-bit Linux, a max-dump value >= ULONG_MAX may cause the core dump size\n# to be 'unlimited'.\n\ncoredump:\n  max-dump: unlimited\n\n# If the Suricata box is a router for the sniffed networks, set it to 'router'. If\n# it is a pure sniffing setup, set it to 'sniffer-only'.\n# If set to auto, the variable is internally switched to 'router' in IPS mode\n# and 'sniffer-only' in IDS mode.\n# This feature is currently only used by the reject* keywords.\nhost-mode: auto\n\n# Number of packets preallocated per thread. The default is 1024. A higher number \n# will make sure each CPU will be more easily kept busy, but may negatively \n# impact caching.\n#max-pending-packets: 1024\n\n# Runmode the engine should use. Please check --list-runmodes to get the available\n# runmodes for each packet acquisition method. Default depends on selected capture\n# method. 'workers' generally gives best performance.\n#runmode: autofp\n\n# Specifies the kind of flow load balancer used by the flow pinned autofp mode.\n#\n# Supported schedulers are:\n#\n# hash     - Flow assigned to threads using the 5-7 tuple hash.\n# ippair   - Flow assigned to threads using addresses only.\n# ftp-hash - Flow assigned to threads using the hash, except for FTP, so that\n#            ftp-data flows will be handled by the same thread\n#\n#autofp-scheduler: hash\n\n# Preallocated size for each packet. Default is 1514 which is the classical\n# size for pcap on Ethernet. You should adjust this value to the highest\n# packet size (MTU + hardware header) on your system.\n#default-packet-size: 1514\n\n# Unix command socket that can be used to pass commands to Suricata.\n# An external tool can then connect to get information from Suricata\n# or trigger some modifications of the engine. Set enabled to yes\n# to activate the feature. In auto mode, the feature will only be\n# activated in live capture mode. You can use the filename variable to set\n# the file name of the socket.\nunix-command:\n  enabled: auto\n  #filename: custom.socket\n\n# Magic file. The extension .mgc is added to the value here.\n#magic-file: /usr/share/file/magic\n@e_magic_file_comment@magic-file: @e_magic_file@\n\n# GeoIP2 database file. Specify path and filename of GeoIP2 database\n# if using rules with \"geoip\" rule option.\n#geoip-database: /usr/local/share/GeoLite2/GeoLite2-Country.mmdb\n\nlegacy:\n  uricontent: enabled\n\n##\n## Detection settings\n##\n\n# Set the order of alerts based on actions\n# The default order is pass, drop, reject, alert\n# action-order:\n#   - pass\n#   - drop\n#   - reject\n#   - alert\n\n# Define maximum number of possible alerts that can be triggered for the same\n# packet. Default is 15\n#packet-alert-max: 15\n\n# Exception Policies\n#\n# Define a common behavior for all exception policies.\n# In IPS mode, the default is drop-flow. For cases when that's not possible, the\n# engine will fall to drop-packet. To fallback to old behavior (setting each of\n# them individually, or ignoring all), set this to ignore.\n# All values available for exception policies can be used, and there is one\n# extra option: auto - which means drop-flow or drop-packet (as explained above)\n# in IPS mode, and ignore in IDS mode. Exception policy values are: drop-packet,\n# drop-flow, reject, bypass, pass-packet, pass-flow, ignore (disable).\nexception-policy: auto\n\n# IP Reputation\n#reputation-categories-file: @e_sysconfdir@iprep/categories.txt\n#default-reputation-path: @e_sysconfdir@iprep\n#reputation-files:\n# - reputation.list\n\n# When run with the option --engine-analysis, the engine will read each of\n# the parameters below, and print reports for each of the enabled sections\n# and exit.  The reports are printed to a file in the default log dir\n# given by the parameter \"default-log-dir\", with engine reporting\n# subsection below printing reports in its own report file.\nengine-analysis:\n  # enables printing reports for fast-pattern for every rule.\n  rules-fast-pattern: yes\n  # enables printing reports for each rule\n  rules: yes\n\n#recursion and match limits for PCRE where supported\npcre:\n  match-limit: 3500\n  match-limit-recursion: 1500\n\n##\n## Advanced Traffic Tracking and Reconstruction Settings\n##\n\n# Host specific policies for defragmentation and TCP stream\n# reassembly. The host OS lookup is done using a radix tree, just\n# like a routing table so the most specific entry matches.\nhost-os-policy:\n  # Make the default policy windows.\n  windows: [0.0.0.0/0]\n  bsd: []\n  bsd-right: []\n  old-linux: []\n  linux: []\n  old-solaris: []\n  solaris: []\n  hpux10: []\n  hpux11: []\n  irix: []\n  macos: []\n  vista: []\n  windows2k3: []\n\n# Defrag settings:\n\n# The exception policy memcap-policy value can be \"drop-packet\", \"pass-packet\",\n#  \"reject\" or \"ignore\" (which is the default).\ndefrag:\n  memcap: 32 MiB\n  # memcap-policy: ignore\n  hash-size: 65536\n  trackers: 65535 # number of defragmented flows to follow\n  max-frags: 65535 # number of fragments to keep (higher than trackers)\n  prealloc: yes\n  timeout: 60\n\n# Enable defrag per host settings\n#  host-config:\n#\n#    - dmz:\n#        timeout: 30\n#        address: [192.168.1.0/24, 127.0.0.0/8, 1.1.1.0/24, 2.2.2.0/24, \"1.1.1.1\", \"2.2.2.2\", \"::1\"]\n#\n#    - lan:\n#        timeout: 45\n#        address:\n#          - 192.168.0.0/24\n#          - 192.168.10.0/24\n#          - 172.16.14.0/24\n\n# Flow settings:\n# By default, the reserved memory (memcap) for flows is 32 MiB. This is the limit\n# for flow allocation inside the engine. You can change this value to allow\n# more memory usage for flows.\n# The hash-size determines the size of the hash used to identify flows inside\n# the engine, and by default the value is 65536.\n# At startup, the engine can preallocate a number of flows, to get better\n# performance. The number of flows preallocated is 10000 by default.\n# emergency-recovery is the percentage of flows that the engine needs to\n# prune before clearing the emergency state. The emergency state is activated\n# when the memcap limit is reached, allowing new flows to be created, but\n# pruning them with the emergency timeouts (they are defined below).\n# If the memcap is reached, the engine will try to prune flows\n# with the default timeouts. If it doesn't find a flow to prune, it will set\n# the emergency bit and it will try again with more aggressive timeouts.\n# If that doesn't work, then it will try to kill the oldest flows using\n# last time seen flows.\n# The memcap can be specified in KiB, MiB, GiB. Just a number indicates it's\n# in bytes.\n# The exception policy memcap-policy can be \"drop-packet\", \"pass-packet\",\n#  \"reject\" or \"ignore\" (which is the default).\n\nflow:\n  memcap: 128 MiB\n  #memcap-policy: ignore\n  hash-size: 65536\n  prealloc: 10000\n  emergency-recovery: 30\n  #managers: 1 # default to one flow manager\n  #recyclers: 1 # default to one flow recycler thread\n\n# This option controls the use of VLAN ids in the flow (and defrag)\n# hashing. Normally this should be enabled, but in some (broken)\n# setups where both sides of a flow are not tagged with the same VLAN\n# tag, we can ignore the VLAN id's in the flow hashing.\nvlan:\n  use-for-tracking: true\n\n# This option controls the use of livedev ids in the flow (and defrag)\n# hashing. This is enabled by default and should be disabled if\n# multiple live devices are used to capture traffic from the same network\nlivedev:\n  use-for-tracking: true\n\n# Specific timeouts for flows. Here you can specify the timeouts that the\n# active flows will wait to transit from the current state to another, on each\n# protocol. The value of \"new\" determines the seconds to wait after a handshake or\n# stream startup before the engine frees the data of that flow it doesn't\n# change the state to established (usually if we don't receive more packets\n# of that flow). The value of \"established\" is the amount of\n# seconds that the engine will wait to free the flow if that time elapses\n# without receiving new packets or closing the connection. \"closed\" is the\n# amount of time to wait after a flow is closed (usually zero). \"bypassed\"\n# timeout controls locally bypassed flows. For these flows we don't do any other\n# tracking. If no packets have been seen after this timeout, the flow is discarded.\n#\n# There's an emergency mode that will become active under attack circumstances,\n# making the engine to check flow status faster. This configuration variables\n# use the prefix \"emergency-\" and work similar as the normal ones.\n# Some timeouts doesn't apply to all the protocols, like \"closed\", for udp and\n# icmp.\n\nflow-timeouts:\n\n  default:\n    new: 30\n    established: 300\n    closed: 0\n    bypassed: 100\n    emergency-new: 10\n    emergency-established: 100\n    emergency-closed: 0\n    emergency-bypassed: 50\n  tcp:\n    new: 60\n    established: 600\n    closed: 60\n    bypassed: 100\n    emergency-new: 5\n    emergency-established: 100\n    emergency-closed: 10\n    emergency-bypassed: 50\n  udp:\n    new: 30\n    established: 300\n    bypassed: 100\n    emergency-new: 10\n    emergency-established: 100\n    emergency-bypassed: 50\n  icmp:\n    new: 30\n    established: 300\n    bypassed: 100\n    emergency-new: 10\n    emergency-established: 100\n    emergency-bypassed: 50\n\n# Stream engine settings. Here the TCP stream tracking and reassembly\n# engine is configured.\n#\n# stream:\n#   memcap: 64 MiB              # Can be specified in KiB, MiB, GiB.  Just a\n#                               # number indicates it's in bytes.\n#   memcap-policy: ignore       # The exception policy value can be \"drop-flow\",\n#                               # \"pass-flow\", \"bypass\", \"drop-packet\",\n#                               # \"pass-packet\", \"reject\" or \"ignore\" default is \"ignore\"\n#   checksum-validation: yes    # To validate the checksum of received\n#                               # packet. If csum validation is specified as\n#                               # \"yes\", then packets with invalid csum values will not\n#                               # be processed by the engine stream/app layer.\n#                               # Warning: locally generated traffic can be\n#                               # generated without checksum due to hardware offload\n#                               # of checksum. You can control the handling of checksum\n#                               # on a per-interface basis via the 'checksum-checks'\n#                               # option\n#   prealloc-sessions: 2048     # 2k sessions prealloc'd per stream thread\n#   midstream: false            # don't allow midstream session pickups\n#   midstream-policy: ignore    # The exception policy value can be \"drop-flow\",\n#                               # \"pass-flow\", \"bypass\", \"drop-packet\",\n#                               # \"pass-packet\", \"reject\" or \"ignore\" default is \"ignore\"\n#   async-oneside: false        # don't enable async stream handling\n#   inline: no                  # stream inline mode\n#   drop-invalid: yes           # in inline mode, drop packets that are invalid with regards to streaming engine\n#   max-syn-queued: 10          # Max different SYNs to queue\n#   max-synack-queued: 5        # Max different SYN/ACKs to queue\n#   bypass: no                  # Bypass packets when stream.reassembly.depth is reached.\n#                               # Warning: first side to reach this triggers\n#                               # the bypass.\n#   liberal-timestamps: false   # Treat all timestamps as if the Linux policy applies. This\n#                               # means it's slightly more permissive. Enabled by default.\n#\n#   reassembly:\n#     memcap: 256 MiB           # Can be specified in KiB, MiB, GiB. Just a number\n#                               # indicates it's in bytes.\n#     memcap-policy: ignore     # The exception policy value can be \"drop-flow\",\n#                               # \"pass-flow\", \"bypass\", \"drop-packet\", \"pass-packet\",\n#                               # \"reject\" or \"ignore\" default is \"ignore\"\n#     depth: 1 MiB               # Can be specified in KiB, MiB, GiB.  Just a number\n#                               # indicates it's in bytes.\n#     toserver-chunk-size: 2560 # inspect raw stream in chunks of at least\n#                               # this size.  Can be specified in KiB, MiB, GiB.\n#                               # Just a number indicates it's in bytes.\n#     toclient-chunk-size: 2560 # inspect raw stream in chunks of at least\n#                               # this size.  Can be specified in KiB, MiB, GiB.\n#                               # Just a number indicates it's in bytes.\n#     randomize-chunk-size: yes # Take a random value for chunk size around the specified value.\n#                               # This lowers the risk of some evasion techniques but could lead\n#                               # to detection change between runs. It is set to 'yes' by default.\n#     randomize-chunk-range: 10 # If randomize-chunk-size is active, the value of chunk-size is\n#                               # a random value between (1 - randomize-chunk-range/100)*toserver-chunk-size\n#                               # and (1 + randomize-chunk-range/100)*toserver-chunk-size and the same\n#                               # calculation for toclient-chunk-size.\n#                               # Default value of randomize-chunk-range is 10.\n#\n#     raw: yes                  # 'Raw' reassembly enabled or disabled.\n#                               # raw is for content inspection by detection\n#                               # engine.\n#\n#     segment-prealloc: 2048    # number of segments preallocated per thread\n#\n#     check-overlap-different-data: true|false\n#                               # check if a segment contains different data\n#                               # than what we've already seen for that\n#                               # position in the stream.\n#                               # This is enabled automatically if inline mode\n#                               # is used or when stream-event:reassembly_overlap_different_data;\n#                               # is used in a rule.\n#\nstream:\n  memcap: 64 MiB\n  #memcap-policy: ignore\n  checksum-validation: yes      # reject incorrect csums\n  #midstream: false\n  #midstream-policy: ignore\n  inline: auto                  # auto will use inline mode in IPS mode, yes or no set it statically\n  reassembly:\n    urgent:\n      policy: oob              # drop, inline, oob (1 byte, see RFC 6093, 3.1), gap\n      oob-limit-policy: drop\n    memcap: 256 MiB\n    #memcap-policy: ignore\n    depth: 1 MiB                # reassemble 1 MiB into a stream\n    toserver-chunk-size: 2560\n    toclient-chunk-size: 2560\n    randomize-chunk-size: yes\n    #randomize-chunk-range: 10\n    #raw: yes\n    #segment-prealloc: 2048\n    #check-overlap-different-data: true\n\n# Host table:\n#\n# Host table is used by the tagging and per host thresholding subsystems.\n#\nhost:\n  hash-size: 4096\n  prealloc: 1000\n  memcap: 32 MiB\n\n# IP Pair table:\n#\n# Used by xbits 'ippair' tracking.\n#\n#ippair:\n#  hash-size: 4096\n#  prealloc: 1000\n#  memcap: 32 MiB\n\n# Decoder settings\n\ndecoder:\n  # Teredo decoder is known to not be completely accurate\n  # as it will sometimes detect non-teredo as teredo.\n  teredo:\n    enabled: true\n    # ports to look for Teredo. Max 4 ports. If no ports are given, or\n    # the value is set to 'any', Teredo detection runs on _all_ UDP packets.\n    ports: $TEREDO_PORTS # syntax: '[3544, 1234]' or '3533' or 'any'.\n\n  # VXLAN decoder is assigned to up to 4 UDP ports. By default only the\n  # IANA assigned port 4789 is enabled.\n  vxlan:\n    enabled: true\n    ports: $VXLAN_PORTS # syntax: '[8472, 4789]' or '4789'.\n\n  # Geneve decoder is assigned to up to 4 UDP ports. By default only the\n  # IANA assigned port 6081 is enabled.\n  geneve:\n    enabled: true\n    ports: $GENEVE_PORTS # syntax: '[6081, 1234]' or '6081'.\n\n  # maximum number of decoder layers for a packet\n  # max-layers: 16\n\n##\n## Performance tuning and profiling\n##\n\n# The detection engine builds internal groups of signatures. The engine\n# allows us to specify the profile to use for them, to manage memory in an\n# efficient way keeping good performance. For the profile keyword you\n# can use the words \"low\", \"medium\", \"high\" or \"custom\". If you use custom,\n# make sure to define the values in the \"custom-values\" section.\n# Usually you would prefer medium/high/low.\n#\n# \"sgh mpm-context\", indicates how the staging should allot mpm contexts for\n# the signature groups.  \"single\" indicates the use of a single context for\n# all the signature group heads.  \"full\" indicates a mpm-context for each\n# group head.  \"auto\" lets the engine decide the distribution of contexts\n# based on the information the engine gathers on the patterns from each\n# group head.\n#\n# The option inspection-recursion-limit is used to limit the recursive calls\n# in the content inspection code.  For certain payload-sig combinations, we\n# might end up taking too much time in the content inspection code.\n# If the argument specified is 0, the engine uses an internally defined\n# default limit.  When a value is not specified, there are no limits on the recursion.\ndetect:\n  profile: medium\n  custom-values:\n    toclient-groups: 3\n    toserver-groups: 25\n  sgh-mpm-context: auto\n  inspection-recursion-limit: 3000\n  # maximum number of times a tx will get logged for rules without app-layer keywords\n  # stream-tx-log-limit: 4\n  # Try to guess an app-layer transaction for rules without app-layer keywords,\n  # ONLY IF there is just one live transaction for the flow.\n  # This allows logging app-layer metadata in alert - the transaction may not\n  # be the relevant one for the alert.\n  # guess-applayer-tx: no\n  # If set to yes, the loading of signatures will be made after the capture\n  # is started. This will limit the downtime in IPS mode.\n  #delayed-detect: yes\n\n  prefilter:\n    # default prefiltering setting. \"mpm\" only creates MPM/fast_pattern\n    # engines. \"auto\" also sets up prefilter engines for other keywords.\n    # Use --list-keywords=all to see which keywords support prefiltering.\n    default: mpm\n\n  # the grouping values above control how many groups are created per\n  # direction. Port priority setting forces that port to get its own group.\n  # Very common ports will benefit, as well as ports with many expensive\n  # rules.\n  grouping:\n    #tcp-priority-ports: 53, 80, 139, 443, 445, 1433, 3306, 3389, 6666, 6667, 8080\n    #udp-priority-ports: 53, 135, 5060\n\n  # Thresholding hash table settings.\n  thresholds:\n    hash-size: 16384\n    memcap: 16 MiB\n\n  profiling:\n    # Log the rules that made it past the prefilter stage, per packet\n    # default is off. The threshold setting determines how many rules\n    # must have made it past pre-filter for that rule to trigger the\n    # logging.\n    #inspect-logging-threshold: 200\n    grouping:\n      dump-to-disk: false\n      include-rules: false      # very verbose\n      include-mpm-stats: false\n\n# Select the multi pattern algorithm you want to run for scan/search the\n# in the engine.\n#\n# The supported algorithms are:\n# \"ac\"      - Aho-Corasick, default implementation\n# \"ac-bs\"   - Aho-Corasick, reduced memory implementation\n# \"ac-ks\"   - Aho-Corasick, \"Ken Steele\" variant\n# \"hs\"      - Hyperscan, available when built with Hyperscan support\n#\n# The default mpm-algo value of \"auto\" will use \"hs\" if Hyperscan is\n# available, \"ac\" otherwise.\n#\n# The mpm you choose also decides the distribution of mpm contexts for\n# signature groups, specified by the conf - \"detect.sgh-mpm-context\".\n# Selecting \"ac\" as the mpm would require \"detect.sgh-mpm-context\"\n# to be set to \"single\", because of ac's memory requirements, unless the\n# ruleset is small enough to fit in memory, in which case one can\n# use \"full\" with \"ac\".  The rest of the mpms can be run in \"full\" mode.\n\nmpm-algo: auto\n\n# Select the matching algorithm you want to use for single-pattern searches.\n#\n# Supported algorithms are \"bm\" (Boyer-Moore) and \"hs\" (Hyperscan, only\n# available if Suricata has been built with Hyperscan support).\n#\n# The default of \"auto\" will use \"hs\" if available, otherwise \"bm\".\n\nspm-algo: auto\n\n# Suricata is multi-threaded. Here the threading can be influenced.\nthreading:\n  set-cpu-affinity: no\n  # Tune cpu affinity of threads. Each family of threads can be bound\n  # to specific CPUs.\n  #\n  # These 2 apply to the all runmodes:\n  # management-cpu-set is used for flow timeout handling, counters\n  # worker-cpu-set is used for 'worker' threads\n  #\n  # Additionally, for autofp these apply:\n  # receive-cpu-set is used for capture threads\n  # verdict-cpu-set is used for IPS verdict threads\n  #\n  cpu-affinity:\n    - management-cpu-set:\n        cpu: [ 0 ]  # include only these CPUs in affinity settings\n    - receive-cpu-set:\n        cpu: [ 0 ]  # include only these CPUs in affinity settings\n    - worker-cpu-set:\n        cpu: [ \"all\" ]\n        mode: \"exclusive\"\n        # Use explicitly 3 threads and don't compute number by using\n        # detect-thread-ratio variable:\n        # threads: 3\n        prio:\n          low: [ 0 ]\n          medium: [ \"1-2\" ]\n          high: [ 3 ]\n          default: \"medium\"\n    #- verdict-cpu-set:\n    #    cpu: [ 0 ]\n    #    prio:\n    #      default: \"high\"\n  #\n  # By default Suricata creates one \"detect\" thread per available CPU/CPU core.\n  # This setting allows controlling this behaviour. A ratio setting of 2 will\n  # create 2 detect threads for each CPU/CPU core. So for a dual core CPU this\n  # will result in 4 detect threads. If values below 1 are used, less threads\n  # are created. So on a dual core CPU a setting of 0.5 results in 1 detect\n  # thread being created. Regardless of the setting at a minimum 1 detect\n  # thread will always be created.\n  #\n  detect-thread-ratio: 1.0\n  #\n  # By default, the per-thread stack size is left to its default setting. If\n  # the default thread stack size is too small, use the following configuration\n  # setting to change the size. Note that if any thread's stack size cannot be\n  # set to this value, a fatal error occurs.\n  #\n  # Generally, the per-thread stack-size should not exceed 8MB.\n  #stack-size: 8 MiB\n\n# Profiling settings. Only effective if Suricata has been built with\n# the --enable-profiling configure flag.\n#\nprofiling:\n  # Run profiling for every X-th packet. The default is 1, which means we\n  # profile every packet. If set to 1024, one packet is profiled for every\n  # 1024 received. The sample rate must be a power of 2.\n  #sample-rate: 1024\n\n  # rule profiling\n  rules:\n\n    # Profiling can be disabled here, but it will still have a\n    # performance impact if compiled in.\n    enabled: yes\n    filename: rule_perf.log\n    append: yes\n    # Set active to yes to enable rules profiling at start\n    # if set to no (default), the rules profiling will have to be started\n    # via unix socket commands.\n    #active:no\n\n    # Sort options: ticks, avgticks, checks, matches, maxticks\n    # If commented out all the sort options will be used.\n    #sort: avgticks\n\n    # Limit the number of sids for which stats are shown at exit (per sort).\n    limit: 10\n\n    # output to json\n    json: @e_enable_evelog@\n\n  # per keyword profiling\n  keywords:\n    enabled: yes\n    filename: keyword_perf.log\n    append: yes\n\n  prefilter:\n    enabled: yes\n    filename: prefilter_perf.log\n    append: yes\n\n  # per rulegroup profiling\n  rulegroups:\n    enabled: yes\n    filename: rule_group_perf.log\n    append: yes\n\n  # packet profiling\n  packets:\n\n    # Profiling can be disabled here, but it will still have a\n    # performance impact if compiled in.\n    enabled: yes\n    filename: packet_stats.log\n    append: yes\n\n    # per packet csv output\n    csv:\n\n      # Output can be disabled here, but it will still have a\n      # performance impact if compiled in.\n      enabled: no\n      filename: packet_stats.csv\n\n  # profiling of locking. Only available when Suricata was built with\n  # --enable-profiling-locks.\n  locks:\n    enabled: no\n    filename: lock_stats.log\n    append: yes\n\n  pcap-log:\n    enabled: no\n    filename: pcaplog_stats.log\n    append: yes\n\n##\n## Netfilter integration\n##\n\n# When running in NFQ inline mode, it is possible to use a simulated\n# non-terminal NFQUEUE verdict.\n# This permits sending all needed packet to Suricata via this rule:\n#        iptables -I FORWARD -m mark ! --mark $MARK/$MASK -j NFQUEUE\n# And below, you can have your standard filtering ruleset. To activate\n# this mode, you need to set mode to 'repeat'\n# If you want a packet to be sent to another queue after an ACCEPT decision\n# set the mode to 'route' and set next-queue value.\n# On Linux >= 3.1, you can set batchcount to a value > 1 to improve performance\n# by processing several packets before sending a verdict (worker runmode only).\n# On Linux >= 3.6, you can set the fail-open option to yes to have the kernel\n# accept the packet if Suricata is not able to keep pace.\n# bypass mark and mask can be used to implement NFQ bypass. If bypass mark is\n# set then the NFQ bypass is activated. Suricata will set the bypass mark/mask\n# on packet of a flow that need to be bypassed. The Netfilter ruleset has to\n# directly accept all packets of a flow once a packet has been marked.\nnfq:\n#  mode: accept\n#  repeat-mark: 1\n#  repeat-mask: 1\n#  bypass-mark: 1\n#  bypass-mask: 1\n#  route-queue: 2\n#  batchcount: 20\n#  fail-open: yes\n\n#nflog support\nnflog:\n    # netlink multicast group\n    # (the same as the iptables --nflog-group param)\n    # Group 0 is used by the kernel, so you can't use it\n  - group: 2\n    # netlink buffer size\n    buffer-size: 18432\n    # put default value here\n  - group: default\n    # set number of packets to queue inside kernel\n    qthreshold: 1\n    # set the delay before flushing packet in the kernel's queue\n    qtimeout: 100\n    # netlink max buffer size\n    max-size: 20000\n\n##\n## Advanced Capture Options\n##\n\n# General settings affecting packet capture\ncapture:\n  # disable NIC offloading. It's restored when Suricata exits.\n  # Enabled by default.\n  #disable-offloading: false\n  #\n  # disable checksum validation. Same as setting '-k none' on the\n  # command-line.\n  #checksum-validation: none\n\n# Netmap support\n#\n# Netmap operates with NIC directly in driver, so you need FreeBSD 11+ which has\n# built-in Netmap support or compile and install the Netmap module and appropriate\n# NIC driver for your Linux system.\n# To reach maximum throughput disable all receive-, segmentation-,\n# checksum- offloading on your NIC (using ethtool or similar).\n# Disabling TX checksum offloading is *required* for connecting OS endpoint\n# with NIC endpoint.\n# You can find more information at https://github.com/luigirizzo/netmap\n#\nnetmap:\n   # To specify OS endpoint add plus sign at the end (e.g. \"eth0+\")\n - interface: eth2\n   # Number of capture threads. \"auto\" uses number of RSS queues on interface.\n   # Warning: unless the RSS hashing is symmetrical, this will lead to\n   # accuracy issues.\n   #threads: auto\n   # You can use the following variables to activate netmap tap or IPS mode.\n   # If copy-mode is set to ips or tap, the traffic coming to the current\n   # interface will be copied to the copy-iface interface. If 'tap' is set, the\n   # copy is complete. If 'ips' is set, the packet matching a 'drop' action\n   # will not be copied.\n   # To specify the OS as the copy-iface (so the OS can route packets, or forward\n   # to a service running on the same machine) add a plus sign at the end\n   # (e.g. \"copy-iface: eth0+\"). Don't forget to set up a symmetrical eth0+ -> eth0\n   # for return packets. Hardware checksumming must be *off* on the interface if\n   # using an OS endpoint (e.g. 'ifconfig eth0 -rxcsum -txcsum -rxcsum6 -txcsum6' for FreeBSD\n   # or 'ethtool -K eth0 tx off rx off' for Linux).\n   #copy-mode: tap\n   #copy-iface: eth3\n   # Set to yes to disable promiscuous mode\n   # disable-promisc: no\n   # Choose checksum verification mode for the interface. At the moment\n   # of the capture, some packets may have an invalid checksum due to\n   # the checksum computation being offloaded to the network card.\n   # Possible values are:\n   #  - yes: checksum validation is forced\n   #  - no: checksum validation is disabled\n   #  - auto: Suricata uses a statistical approach to detect when\n   #  checksum off-loading is used.\n   # Warning: 'checksum-validation' must be set to yes to have any validation\n   #checksum-checks: auto\n   # BPF filter to apply to this interface. The pcap filter syntax apply here.\n   #bpf-filter: port 80 or udp\n #- interface: eth3\n   #threads: auto\n   #copy-mode: tap\n   #copy-iface: eth2\n   # Put default values here\n - interface: default\n\n# PF_RING configuration: for use with native PF_RING support\n# for more info see http://www.ntop.org/products/pf_ring/\npfring:\n  - interface: eth0\n    # Number of receive threads. If set to 'auto' Suricata will first try\n    # to use CPU (core) count and otherwise RSS queue count.\n    threads: auto\n\n    # Default clusterid.  PF_RING will load balance packets based on flow.\n    # All threads/processes that will participate need to have the same\n    # clusterid.\n    cluster-id: 99\n\n    # Default PF_RING cluster type. PF_RING can load balance per flow.\n    # Possible values are:\n    # - cluster_flow:               6-tuple: <src ip, src_port, dst ip, dst port, proto, vlan>\n    # - cluster_inner_flow:         6-tuple: <src ip, src port, dst ip, dst port, proto, vlan>\n    # - cluster_inner_flow_2_tuple: 2-tuple: <src ip,           dst ip                       >\n    # - cluster_inner_flow_4_tuple: 4-tuple: <src ip, src port, dst ip, dst port             >\n    # - cluster_inner_flow_5_tuple: 5-tuple: <src ip, src port, dst ip, dst port, proto      >\n    # - cluster_round_robin (NOT RECOMMENDED)\n    cluster-type: cluster_flow\n\n    # bpf filter for this interface\n    #bpf-filter: tcp\n\n    # If bypass is set then the PF_RING hw bypass is activated, when supported\n    # by the network interface. Suricata will instruct the interface to bypass\n    # all future packets for a flow that need to be bypassed.\n    #bypass: yes\n\n    # Choose checksum verification mode for the interface. At the moment\n    # of the capture, some packets may have an invalid checksum due to\n    # the checksum computation being offloaded to the network card.\n    # Possible values are:\n    #  - rxonly: only compute checksum for packets received by network card.\n    #  - yes: checksum validation is forced\n    #  - no: checksum validation is disabled\n    #  - auto: Suricata uses a statistical approach to detect when\n    #  checksum off-loading is used. (default)\n    # Warning: 'checksum-validation' must be set to yes to have any validation\n    #checksum-checks: auto\n  # Second interface\n  #- interface: eth1\n  #  threads: 3\n  #  cluster-id: 93\n  #  cluster-type: cluster_flow\n  # Put default values here\n  - interface: default\n    #threads: 2\n\n# For FreeBSD ipfw(8) divert(4) support.\n# Please make sure you have ipfw_load=\"YES\" and ipdivert_load=\"YES\"\n# in /etc/loader.conf or kldload'ing the appropriate kernel modules.\n# Additionally, you need to have an ipfw rule for the engine to see\n# the packets from ipfw.  For Example:\n#\n#   ipfw add 100 divert 8000 ip from any to any\n#\n# N.B. This example uses \"8000\" -- this number must mach the values\n# you passed on the command line, i.e., -d 8000\n#\nipfw:\n\n  # Reinject packets at the specified ipfw rule number.  This config\n  # option is the ipfw rule number AT WHICH rule processing continues\n  # in the ipfw processing system after the engine has finished\n  # inspecting the packet for acceptance.  If no rule number is specified,\n  # accepted packets are reinjected at the divert rule which they entered\n  # and IPFW rule processing continues.  No check is done to verify\n  # this will rule makes sense so care must be taken to avoid loops in ipfw.\n  #\n  ## The following example tells the engine to reinject packets\n  # back into the ipfw firewall AT rule number 5500:\n  #\n  # ipfw-reinjection-rule-number: 5500\n\n\nnapatech:\n    # When use_all_streams is set to \"yes\" the initialization code will query\n    # the Napatech service for all configured streams and listen on all of them.\n    # When set to \"no\" the streams config array will be used.\n    #\n    # This option necessitates running the appropriate NTPL commands to create\n    # the desired streams prior to running Suricata.\n    #use-all-streams: no\n\n    # The streams to listen on when auto-config is disabled or when and threading\n    # cpu-affinity is disabled.  This can be either:\n    #   an individual stream (e.g. streams: [0])\n    # or\n    #   a range of streams (e.g. streams: [\"0-3\"])\n    #\n    streams: [\"0-3\"]\n\n    # Stream stats can be enabled to provide fine grain packet and byte counters\n    # for each thread/stream that is configured.\n    #\n    enable-stream-stats: no\n\n    # When auto-config is enabled the streams will be created and assigned\n    # automatically to the NUMA node where the thread resides.  If cpu-affinity\n    # is enabled in the threading section.  Then the streams will be created\n    # according to the number of worker threads specified in the worker-cpu-set.\n    # Otherwise, the streams array is used to define the streams.\n    #\n    # This option is intended primarily to support legacy configurations.\n    #\n    # This option cannot be used simultaneously with either \"use-all-streams\"\n    # or \"hardware-bypass\".\n    #\n    auto-config: yes\n\n    # Enable hardware level flow bypass.\n    #\n    hardware-bypass: yes\n\n    # Enable inline operation.  When enabled traffic arriving on a given port is\n    # automatically forwarded out its peer port after analysis by Suricata.\n    #\n    inline: no\n\n    # Ports indicates which Napatech ports are to be used in auto-config mode.\n    # these are the port IDs of the ports that will be merged prior to the\n    # traffic being distributed to the streams.\n    #\n    # When hardware-bypass is enabled the ports must be configured as a segment.\n    # specify the port(s) on which upstream and downstream traffic will arrive.\n    # This information is necessary for the hardware to properly process flows.\n    #\n    # When using a tap configuration one of the ports will receive inbound traffic\n    # for the network and the other will receive outbound traffic. The two ports on a\n    # given segment must reside on the same network adapter.\n    #\n    # When using a SPAN-port configuration the upstream and downstream traffic\n    # arrives on a single port. This is configured by setting the two sides of the\n    # segment to reference the same port.  (e.g. 0-0 to configure a SPAN port on\n    # port 0).\n    #\n    # port segments are specified in the form:\n    #    ports: [0-1,2-3,4-5,6-6,7-7]\n    #\n    # For legacy systems when hardware-bypass is disabled this can be specified in any\n    # of the following ways:\n    #\n    #   a list of individual ports (e.g. ports: [0,1,2,3])\n    #\n    #   a range of ports (e.g. ports: [0-3])\n    #\n    #   \"all\" to indicate that all ports are to be merged together\n    #   (e.g. ports: [all])\n    #\n    # This parameter has no effect if auto-config is disabled.\n    #\n    ports: [0-1,2-3]\n\n    # When auto-config is enabled the hashmode specifies the algorithm for\n    # determining to which stream a given packet is to be delivered.\n    # This can be any valid Napatech NTPL hashmode command.\n    #\n    # The most common hashmode commands are:  hash2tuple, hash2tuplesorted,\n    # hash5tuple, hash5tuplesorted and roundrobin.\n    #\n    # See Napatech NTPL documentation other hashmodes and details on their use.\n    #\n    # This parameter has no effect if auto-config is disabled.\n    #\n    hashmode: hash5tuplesorted\n\n##\n## Configure Suricata to load Suricata-Update managed rules.\n##\n\ndefault-rule-path: @e_defaultruledir@\n\nrule-files:\n  - suricata.rules\n\n##\n## Auxiliary configuration files.\n##\n\nclassification-file: @e_sysconfdir@classification.config\nreference-config-file: @e_sysconfdir@reference.config\n# threshold-file: @e_sysconfdir@threshold.config\n\n##\n## Include other configs\n##\n\n# Includes:  Files included here will be handled as if they were in-lined\n# in this configuration file. Files with relative pathnames will be\n# searched for in the same directory as this configuration file. You may\n# use absolute pathnames too.\n#include:\n#  - include1.yaml\n#  - include2.yaml\n"
        },
        {
          "name": "threshold.config",
          "type": "blob",
          "size": 1.6044921875,
          "content": "# Thresholding:\n#\n# This feature is used to reduce the number of logged alerts for noisy rules.\n# Thresholding commands limit the number of times a particular event is logged\n# during a specified time interval.\n#\n# The syntax is the following:\n#\n# threshold gen_id <gen_id>, sig_id <sig_id>, type <limit|threshold|both>, track <by_src|by_dst>, count <n>, seconds <t>\n#\n# event_filter gen_id <gen_id>, sig_id <sig_id>, type <limit|threshold|both>, track <by_src|by_dst>, count <n>, seconds <t>\n#\n# suppress gen_id <gid>, sig_id <sid>\n# suppress gen_id <gid>, sig_id <sid>, track <by_src|by_dst>, ip <ip|subnet>\n#\n# The options are documented at https://docs.suricata.io/en/latest/configuration/global-thresholds.html\n#\n# Please note that thresholding can also be set inside a signature. The interaction between rule based thresholds\n# and global thresholds is documented here:\n# https://docs.suricata.io/en/latest/configuration/global-thresholds.html#global-thresholds-vs-rule-thresholds\n\n# Limit to 10 alerts every 10 seconds for each source host\n#threshold gen_id 0, sig_id 0, type limit, track by_src, count 10, seconds 10\n\n# Limit to 1 alert every 10 seconds for signature with sid 2404000 per destination host\n#threshold gen_id 1, sig_id 2404000, type limit, track by_dst, count 1, seconds 10\n\n# Avoid to alert on f-secure update\n# Example taken from https://blog.inliniac.net/2012/03/07/f-secure-av-updates-and-suricata-ips/\n#suppress gen_id 1, sig_id 2009557, track by_src, ip 217.110.97.128/25\n#suppress gen_id 1, sig_id 2012086, track by_src, ip 217.110.97.128/25\n#suppress gen_id 1, sig_id 2003614, track by_src, ip 217.110.97.128/25\n"
        }
      ]
    }
  ]
}