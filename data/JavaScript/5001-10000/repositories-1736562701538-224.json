{
  "metadata": {
    "timestamp": 1736562701538,
    "page": 224,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjI2MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "porsager/postgres",
      "stars": 7647,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".eslintrc.json",
          "type": "blob",
          "size": 5.8857421875,
          "content": "{\n  \"root\": true,\n  \"env\": {\n    \"es2020\": true,\n    \"node\": true\n  },\n  \"parserOptions\": {\n    \"ecmaVersion\": 2020,\n    \"sourceType\": \"module\"\n  },\n  \"rules\": {\n    \"comma-dangle\": 2,\n    \"no-cond-assign\": 2,\n    \"no-console\": 1,\n    \"no-constant-condition\": 2,\n    \"no-control-regex\": 2,\n    \"no-debugger\": 2,\n    \"no-dupe-args\": 2,\n    \"no-dupe-keys\": 2,\n    \"no-duplicate-case\": 2,\n    \"no-empty\": 2,\n    \"no-empty-character-class\": 2,\n    \"no-ex-assign\": 2,\n    \"no-extra-boolean-cast\": 2,\n    \"no-extra-semi\": 2,\n    \"no-func-assign\": 2,\n    \"no-inner-declarations\": 2,\n    \"no-invalid-regexp\": 2,\n    \"no-irregular-whitespace\": 2,\n    \"no-negated-in-lhs\": 0,\n    \"no-obj-calls\": 2,\n    \"no-regex-spaces\": 2,\n    \"no-sparse-arrays\": 2,\n    \"no-unexpected-multiline\": 2,\n    \"no-unreachable\": 2,\n    \"use-isnan\": 2,\n    \"valid-typeof\": 2,\n    \"accessor-pairs\": 2,\n    \"array-callback-return\": 0,\n    \"consistent-return\": 0,\n    \"curly\": [\n      2,\n      \"multi-or-nest\",\n      \"consistent\"\n    ],\n    \"default-case\": 2,\n    \"dot-location\": [\n      2,\n      \"property\"\n    ],\n    \"dot-notation\": [\n      2,\n      {\n        \"allowPattern\": \"^[a-z]+(_[a-z]+)+$\"\n      }\n    ],\n    \"eqeqeq\": [\n      \"error\",\n      \"always\",\n      {\n        \"null\": \"ignore\"\n      }\n    ],\n    \"no-alert\": 2,\n    \"no-caller\": 2,\n    \"no-case-declarations\": 2,\n    \"no-div-regex\": 2,\n    \"no-else-return\": 2,\n    \"no-empty-function\": 2,\n    \"no-empty-pattern\": 2,\n    \"no-eq-null\": 0,\n    \"no-eval\": 2,\n    \"no-extend-native\": 2,\n    \"no-extra-bind\": 2,\n    \"no-extra-label\": 2,\n    \"no-fallthrough\": 2,\n    \"no-floating-decimal\": 2,\n    \"no-implicit-coercion\": 0,\n    \"no-implicit-globals\": 2,\n    \"no-implied-eval\": 2,\n    \"no-invalid-this\": 2,\n    \"no-iterator\": 2,\n    \"no-labels\": 2,\n    \"no-lone-blocks\": 2,\n    \"no-loop-func\": 2,\n    \"no-magic-numbers\": 0,\n    \"no-multi-spaces\": [\n      2,\n      {\n        \"ignoreEOLComments\": true,\n        \"exceptions\": {\n          \"Array\": true,\n          \"Property\": true,\n          \"VariableDeclarator\": true,\n          \"ImportDeclaration\": true,\n          \"TernaryExpressions\": true,\n          \"Comments\": true\n        }\n      }\n    ],\n    \"no-multi-str\": 2,\n    \"no-native-reassign\": 2,\n    \"no-new\": 2,\n    \"no-new-func\": 2,\n    \"no-new-wrappers\": 2,\n    \"no-octal\": 2,\n    \"no-octal-escape\": 2,\n    \"no-param-reassign\": 0,\n    \"no-proto\": 2,\n    \"no-redeclare\": 2,\n    \"no-return-assign\": 0,\n    \"no-script-url\": 2,\n    \"no-self-assign\": 2,\n    \"no-self-compare\": 2,\n    \"no-sequences\": 0,\n    \"no-throw-literal\": 2,\n    \"no-unmodified-loop-condition\": 2,\n    \"no-unused-expressions\": 0,\n    \"no-unused-labels\": 2,\n    \"no-useless-call\": 2,\n    \"no-useless-concat\": 2,\n    \"no-useless-escape\": 2,\n    \"no-void\": 2,\n    \"no-with\": 2,\n    \"wrap-iife\": 2,\n    \"no-delete-var\": 2,\n    \"no-label-var\": 2,\n    \"no-restricted-globals\": 2,\n    \"no-shadow\": 0,\n    \"no-shadow-restricted-names\": 2,\n    \"no-undef\": 2,\n    \"no-undef-init\": 2,\n    \"no-unused-vars\": 2,\n    \"no-use-before-define\": [\n      2,\n      {\n        \"functions\": false,\n        \"variables\": false\n      }\n    ],\n    \"callback-return\": 0,\n    \"global-require\": 2,\n    \"handle-callback-err\": 2,\n    \"no-mixed-requires\": 2,\n    \"no-new-require\": 2,\n    \"no-path-concat\": 2,\n    \"no-process-env\": 2,\n    \"no-process-exit\": 2,\n    \"array-bracket-spacing\": [\n      2,\n      \"never\"\n    ],\n    \"block-spacing\": [\n      2,\n      \"always\"\n    ],\n    \"brace-style\": [\n      2,\n      \"1tbs\",\n      {\n        \"allowSingleLine\": true\n      }\n    ],\n    \"camelcase\": 0,\n    \"comma-spacing\": 2,\n    \"comma-style\": [\n      2,\n      \"first\",\n      {\n        \"exceptions\": {\n          \"ArrayExpression\": true,\n          \"ObjectExpression\": true\n        }\n      }\n    ],\n    \"consistent-this\": [\n      2,\n      \"self\"\n    ],\n    \"eol-last\": 2,\n    \"indent\": [\n      2,\n      2,\n      {\n        \"MemberExpression\": \"off\",\n        \"flatTernaryExpressions\": true,\n        \"VariableDeclarator\": {\n          \"const\": 2\n        },\n        \"FunctionExpression\": {\n          \"parameters\": \"first\"\n        },\n        \"CallExpression\": {\n          \"arguments\": \"off\"\n        },\n        \"ArrayExpression\": \"first\",\n        \"ObjectExpression\": \"first\"\n      }\n    ],\n    \"key-spacing\": [\n      0,\n      {\n        \"beforeColon\": false,\n        \"afterColon\": true,\n        \"mode\": \"minimum\"\n      }\n    ],\n    \"keyword-spacing\": 2,\n    \"linebreak-style\": 2,\n    \"lines-around-comment\": 0,\n    \"max-depth\": [\n      2,\n      5\n    ],\n    \"max-len\": [\n      2,\n      150\n    ],\n    \"max-nested-callbacks\": [\n      2,\n      5\n    ],\n    \"max-params\": [\n      2,\n      5\n    ],\n    \"max-statements-per-line\": 0,\n    \"new-cap\": [\n      2,\n      {\n        \"capIsNew\": false\n      }\n    ],\n    \"new-parens\": 2,\n    \"newline-after-var\": 0,\n    \"newline-before-return\": 0,\n    \"no-array-constructor\": 2,\n    \"no-bitwise\": 0,\n    \"no-continue\": 2,\n    \"no-lonely-if\": 2,\n    \"no-mixed-spaces-and-tabs\": 2,\n    \"no-negated-condition\": 0,\n    \"no-new-object\": 2,\n    \"no-spaced-func\": 2,\n    \"no-trailing-spaces\": 1,\n    \"no-unneeded-ternary\": 2,\n    \"no-whitespace-before-property\": 2,\n    \"object-curly-spacing\": [\n      2,\n      \"always\"\n    ],\n    \"one-var-declaration-per-line\": [\n      2,\n      \"always\"\n    ],\n    \"quote-props\": [\n      2,\n      \"as-needed\"\n    ],\n    \"quotes\": [\n      2,\n      \"single\"\n    ],\n    \"semi\": [\n      2,\n      \"never\"\n    ],\n    \"space-before-blocks\": 2,\n    \"space-before-function-paren\": [\n      2,\n      \"never\"\n    ],\n    \"space-infix-ops\": 2,\n    \"space-unary-ops\": 2,\n    \"spaced-comment\": 2,\n    \"arrow-spacing\": 2,\n    \"constructor-super\": 2,\n    \"no-class-assign\": 2,\n    \"no-confusing-arrow\": 0,\n    \"no-const-assign\": 2,\n    \"no-dupe-class-members\": 2,\n    \"no-duplicate-imports\": 2,\n    \"no-new-symbol\": 2,\n    \"no-this-before-super\": 2,\n    \"no-useless-constructor\": 2,\n    \"no-var\": 2,\n    \"object-shorthand\": 0,\n    \"prefer-arrow-callback\": 0,\n    \"prefer-const\": 2,\n    \"prefer-rest-params\": 0,\n    \"prefer-spread\": 0\n  }\n}\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": "CHANGELOG.md",
          "type": "blob",
          "size": 6.318359375,
          "content": "# Changelog\n\n## v3.2.4 - 25 May 2022\n- Allow setting keep_alive: false  bee62f3\n- Fix support for null in arrays - fixes #371  b04c853\n\n## v3.2.3 - 23 May 2022\n- Fix Only use setKeepAlive in Deno if available  28fbbaf\n- Fix wrong helper match on multiple occurances  02f3854\n\n#### Typescript related\n- Fix Deno assertRejects compatibility (#365)  0f0af92\n- Fix include missing boolean type in JSONValue union (#373)  1817387\n\n## v3.2.2 - 15 May 2022\n- Properly handle errors thrown on commit  99ddae4\n\n## v3.2.1 - 15 May 2022\n- Exclude target_session_attrs from connection obj  43f1442\n\n## v3.2.0 - 15 May 2022\n- Add `sslmode=verify-full` support  e67da29\n- Add support for array of fragments  342bf55\n- Add uri decode of host in url - fixes #346 1adc113\n- Add passing of rest url params to connection (ootb support cockroach urls)  41ed84f\n- Fix Deno partial writes  452a30d\n- Fix `as` dynamic helper  3300c40\n- Fix some nested fragments usage  9bfa902\n- Fix missing columns on `Result` when using simple protocol - fixes #350  1e2e298\n- Fix fragments in transactions - fixes #333  75914c7\n\n#### Typescript related\n- Upgrade/fix types (#357)  1e6d312\n- Add optional `onlisten` callback to `listen()` on TypeScript (#360)  6b749b2\n- Add implicit custom type inference (#361)  28512bf\n- Fix and improve sql() helper types (#338)  c1de3d8\n- Fix update query type def for `.writable()` and `.readable()` to return promises (#347)  51269ce\n- Add bigint to typescript Serializable - fixes #330  f1e41c3\n\n## v3.1.0 - 22 Apr 2022\n- Add close method to close but not end connections forever  94fea8f\n- Add .values() method to return rows as arrays of values  56873c2\n- Support transform.undefined - fixes #314  eab71e5\n- Support nested fragments values and dynamics - fixes #326  86445ca\n- Fix deno close sequence  f76af24\n- Fix subscribe reconnect and add onsubscribe method - fixes #315  5097345\n- Deno ts fix - fixes #327  50403a1\n\n## v3.0.6 - 19 Apr 2022\n- Properly close connections in Deno  cbc6a75\n- Only write end message if socket is open  13950af\n- Improve query cancellation  01c2c68\n- Use monotonically increasing time for timeout - fixes #316  9d7a21d\n- Add support for dynamic columns with `returning` - fixes #317  04644c0\n- Fix type errors in TypeScript deno projects (#313)  822fb21\n- Execute forEach instantly  44e9fbe\n\n## v3.0.5 - 6 Apr 2022\n- Fix transaction execution timing  28bb0b3\n- Add optional onlisten function to listen  1dc2fd2\n- Fix dynamic in helper after insert #305  4d63a59\n\n## v3.0.4 - 5 Apr 2022\n- Ensure drain only dequeues if ready - fixes #303  2e5f017\n\n## v3.0.3 - 4 Apr 2022\n- Run tests with github actions  b536d0d\n- Add custom socket option - fixes #284  5413f0c\n- Fix sql function overload type inference (#294)  3c4e90a\n- Update deno std to 0.132 and enable last tests  50762d4\n- Send proper client-encoding - Fixes #288  e5b8554\n\n## v3.0.2 - 31 Mar 2022\n- Fix BigInt handling  36a70df\n- Fix unsubscribing  (#300)  b6c597f\n- Parse update properly with identity full - Fixes #296  3ed11e7\n\n## v3.0.1 - 30 Mar 2022\n - Improve connection queue handling + fix leak cee1a57\n - Use publications option - fixes #295 b5ceecc\n - Throw proper query error if destroyed e148a0a\n - Transaction rejects with rethrown error - fixes #289 f7c8ae6\n - Only create origin stacktrace for tagged and debug - fixes #290 a782edf\n - Include types and readme in deno release - fixes #287 9068820\n - Disable fetch_types for Subscribe options 72e0cdb\n - Update TypeScript types with v3 changes (#293) db05836\n\n## v3.0.0 - 24 Mar 2022\nThis is a complete rewrite to better support all the features that I was trying to get into v2. There are a few breaking changes from v2 beta, which some (myself included) was using in production, so I'm skipping a stable v2 release and going straight to v3.\n\nHere are some of the new things available, but check the updated docs.\n- Dynamic query builder based on raw sql\n- Realtime subscribe to db changes through logical replication\n- Multi-host support for High Availability setups\n- Postgres input parameter types from `ParameterDescription`\n- Deno support\n- Cursors as async iterators\n- `.describe()` to only get query input types and column definitions\n- Support for Large Objects\n- `max_lifetime` for connections\n- Cancellation of requests\n- Converted to ESM (with CJS support)\n- Typescript support (Credit @minigugus)\n\n### Breaking changes from v2 -> v3\n- Cursors are always called with `Result` arrays (previously cursor 1 would return a row object, where > 1 would return an array of rows)\n- `.writable()` and `.readable()` is now async (returns a Promise that resolves to the stream)\n- Queries now returns a lazy promise instead of being executed immediately. This means the query won't be sent until awaited (.then, .catch, .finally is called) or until `.execute()` is manually called.\n- `.stream()` is renamed to `.forEach`\n- Returned results are now it's own `Result` class extending `Array` instead of an Array with extra properties (actually shouldn't be breaking unless you're doing something funny)\n- Parameters are now cast using the types returned from Postgres ParameterDescription with a fallback to the previously inferred types\n- Only tested with node v12 and up\n- Implicit array value to multiple parameter expansion removed (use sql([...]) instead)\n\n### Breaking changes from v1 -> v2 (v2 never moved on from beta)\n- All identifiers from `sql()` in queries are now always quoted\n- Undefined parameters are no longer allowed\n- Rename timeout option to `idle_timeout`\n- Default to 10 connections instead of number of CPUs\n- Numbers that cannot be safely cast to JS Number are returned as string. This happens for eg, `select count(*)` because `count()` returns a 64 bit integer (int8), so if you know your `count()` won't be too big for a js number just cast in your query to int4 like `select count(*)::int`\n\n## v1.0.2 - 21 Jan 2020\n\n- Fix standard postgres user env var (#20)  cce5ad7\n- Ensure url or options is not falsy  bc549b0\n- Add support for dynamic password  b2ab9fb\n- Fix hiding pass from options  3f76b98\n\n\n## v1.0.1 - 3 Jan 2020\n\n- Fix #3 url without db and trailing slash  45d4233\n- Fix stream promise - resolve with correct result  730df2c\n- Fix return value of unsafe query with multiple statements  748f198\n- Fix destroy before connected  f682ca1\n- Fix params usage for file() call without options  e4f12a4\n- Various Performance improvements\n\n## v1.0.0 - 22 Dec 2019\n\n- Initial release\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 48.654296875,
          "content": "<img align=\"left\" width=\"440\" height=\"180\" alt=\"Fastest full PostgreSQL nodejs client\" src=\"https://raw.githubusercontent.com/porsager/postgres/master/postgresjs.svg?sanitize=true\">\n\n- [🚀 Fastest full-featured node & deno client](https://github.com/porsager/postgres-benchmarks#results)\n- 🏷 ES6 Tagged Template Strings at the core\n- 🏄‍♀️ Simple surface API\n- 🖊️ Dynamic query support\n- 💬 Chat and help on [Gitter](https://gitter.im/porsager/postgres)\n- 🐦 Follow on [Twitter](https://twitter.com/rporsager)\n\n<br>\n\n## Getting started\n\n<br>\n<img height=\"220\" width=\"458\" alt=\"Good UX with Postgres.js\" src=\"https://raw.githubusercontent.com/porsager/postgres/master/demo.gif\">\n<br>\n\n### Installation\n```bash\n$ npm install postgres\n```\n\n### Usage\nCreate your `sql` database instance\n```js\n// db.js\nimport postgres from 'postgres'\n\nconst sql = postgres({ /* options */ }) // will use psql environment variables\n\nexport default sql\n```\n\nSimply import for use elsewhere\n```js\n// users.js\nimport sql from './db.js'\n\nasync function getUsersOver(age) {\n  const users = await sql`\n    select\n      name,\n      age\n    from users\n    where age > ${ age }\n  `\n  // users = Result [{ name: \"Walter\", age: 80 }, { name: 'Murray', age: 68 }, ...]\n  return users\n}\n\n\nasync function insertUser({ name, age }) {\n  const users = await sql`\n    insert into users\n      (name, age)\n    values\n      (${ name }, ${ age })\n    returning name, age\n  `\n  // users = Result [{ name: \"Murray\", age: 68 }]\n  return users\n}\n```\n\n#### ESM dynamic imports\n\nThe library can be used with ESM dynamic imports as well as shown here.\n\n```js\nconst { default: postgres } = await import('postgres')\n```\n\n## Table of Contents\n\n* [Connection](#connection)\n* [Queries](#queries)\n* [Building queries](#building-queries)\n* [Advanced query methods](#advanced-query-methods)\n* [Transactions](#transactions)\n* [Data Transformation](#data-transformation)\n* [Listen & notify](#listen--notify)\n* [Realtime subscribe](#realtime-subscribe)\n* [Numbers, bigint, numeric](#numbers-bigint-numeric)\n* [Result Array](#result-array)\n* [Connection details](#connection-details)\n* [Custom Types](#custom-types)\n* [Teardown / Cleanup](#teardown--cleanup)\n* [Error handling](#error-handling)\n* [TypeScript support](#typescript-support)\n* [Reserving connections](#reserving-connections)\n* [Changelog](./CHANGELOG.md)\n\n\n## Connection\n\n### `postgres([url], [options])`\n\nYou can use either a `postgres://` url connection string or the options to define your database connection properties. Options in the object will override any present in the url. Options will fall back to the same environment variables as psql.\n\n```js\nconst sql = postgres('postgres://username:password@host:port/database', {\n  host                 : '',            // Postgres ip address[s] or domain name[s]\n  port                 : 5432,          // Postgres server port[s]\n  database             : '',            // Name of database to connect to\n  username             : '',            // Username of database user\n  password             : '',            // Password of database user\n  ...and more\n})\n```\n\nMore options can be found in the [Connection details section](#connection-details).\n\n## Queries\n\n### ```await sql`...` -> Result[]```\n\nPostgres.js utilizes [Tagged template functions](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Template_literals#Tagged_templates) to process query parameters **before** interpolation. Using tagged template literals benefits developers by:\n\n1. **Enforcing** safe query generation\n2. Giving the ` sql`` ` function powerful [utility](#dynamic-inserts) and [query building](#building-queries) features.\n\nAny generic value will be serialized according to an inferred type, and replaced by a PostgreSQL protocol placeholder `$1, $2, ...`. The parameters are then sent separately to the database which handles escaping & casting.\n\nAll queries will return a `Result` array, with objects mapping column names to each row.\n\n```js\nconst xs = await sql`\n  insert into users (\n    name, age\n  ) values (\n    'Murray', 68\n  )\n\n  returning *\n`\n\n// xs = [{ user_id: 1, name: 'Murray', age: 68 }]\n```\n\n> Please note that queries are first executed when `awaited` – or instantly by using [`.execute()`](#execute).\n\n### Query parameters\n\nParameters are automatically extracted and handled by the database so that SQL injection isn't possible. No special handling is necessary, simply use tagged template literals as usual.\n\n```js\nconst name = 'Mur'\n    , age = 60\n\nconst users = await sql`\n  select\n    name,\n    age\n  from users\n  where\n    name like ${ name + '%' }\n    and age > ${ age }\n`\n// users = [{ name: 'Murray', age: 68 }]\n```\n\n> Be careful with quotation marks here. Because Postgres infers column types, you do not need to wrap your interpolated parameters in quotes like `'${name}'`. This will cause an error because the tagged template replaces `${name}` with `$1` in the query string, leaving Postgres to do the interpolation. If you wrap that in a string, Postgres will see `'$1'` and interpret it as a string as opposed to a parameter.\n\n### Dynamic column selection\n\n```js\nconst columns = ['name', 'age']\n\nawait sql`\n  select\n    ${ sql(columns) }\n  from users\n`\n\n// Which results in:\nselect \"name\", \"age\" from users\n```\n\n### Dynamic inserts\n\n```js\nconst user = {\n  name: 'Murray',\n  age: 68\n}\n\nawait sql`\n  insert into users ${\n    sql(user, 'name', 'age')\n  }\n`\n\n// Which results in:\ninsert into users (\"name\", \"age\") values ($1, $2)\n\n// The columns can also be given with an array\nconst columns = ['name', 'age']\n\nawait sql`\n  insert into users ${\n    sql(user, columns)\n  }\n`\n```\n\n**You can omit column names and simply execute `sql(user)` to get all the fields from the object as columns**. Be careful not to allow users to supply columns that you do not want to be inserted.\n\n#### Multiple inserts in one query\nIf you need to insert multiple rows at the same time it's also much faster to do it with a single `insert`. Simply pass an array of objects to `sql()`.\n\n```js\nconst users = [{\n  name: 'Murray',\n  age: 68,\n  garbage: 'ignore'\n},\n{\n  name: 'Walter',\n  age: 80\n}]\n\nawait sql`insert into users ${ sql(users, 'name', 'age') }`\n\n// Is translated to:\ninsert into users (\"name\", \"age\") values ($1, $2), ($3, $4)\n\n// Here you can also omit column names which will use object keys as columns\nawait sql`insert into users ${ sql(users) }`\n\n// Which results in:\ninsert into users (\"name\", \"age\") values ($1, $2), ($3, $4)\n```\n\n### Dynamic columns in updates\nThis is also useful for update queries\n```js\nconst user = {\n  id: 1,\n  name: 'Murray',\n  age: 68\n}\n\nawait sql`\n  update users set ${\n    sql(user, 'name', 'age')\n  }\n  where user_id = ${ user.id }\n`\n\n// Which results in:\nupdate users set \"name\" = $1, \"age\" = $2 where user_id = $3\n\n// The columns can also be given with an array\nconst columns = ['name', 'age']\n\nawait sql`\n  update users set ${\n    sql(user, columns)\n  }\n  where user_id = ${ user.id }\n`\n```\n\n### Multiple updates in one query\nTo create multiple updates in a single query, it is necessary to use arrays instead of objects to ensure that the order of the items correspond with the column names.\n```js\nconst users = [\n  [1, 'John', 34],\n  [2, 'Jane', 27],\n]\n\nawait sql`\n  update users set name = update_data.name, age = (update_data.age)::int\n  from (values ${sql(users)}) as update_data (id, name, age)\n  where users.id = (update_data.id)::int\n  returning users.id, users.name, users.age\n`\n```\n\n### Dynamic values and `where in`\nValue lists can also be created dynamically, making `where in` queries simple too.\n```js\nconst users = await sql`\n  select\n    *\n  from users\n  where age in ${ sql([68, 75, 23]) }\n`\n```\n\nor\n```js\nconst [{ a, b, c }] = await sql`\n  select\n    *\n  from (values ${ sql(['a', 'b', 'c']) }) as x(a, b, c)\n`\n```\n\n## Building queries\n\nPostgres.js features a simple dynamic query builder by conditionally appending/omitting query fragments.\nIt works by nesting ` sql`` ` fragments within other ` sql`` ` calls or fragments. This allows you to build dynamic queries safely without risking sql injections through usual string concatenation.\n\n### Partial queries\n```js\nconst olderThan = x => sql`and age > ${ x }`\n\nconst filterAge = true\n\nawait sql`\n  select\n   *\n  from users\n  where name is not null ${\n    filterAge\n      ? olderThan(50)\n      : sql``\n  }\n`\n// Which results in:\nselect * from users where name is not null\n// Or\nselect * from users where name is not null and age > 50\n```\n\n### Dynamic filters\n```js\nawait sql`\n  select\n    *\n  from users ${\n    id\n      ? sql`where user_id = ${ id }`\n      : sql``\n  }\n`\n\n// Which results in:\nselect * from users\n// Or\nselect * from users where user_id = $1\n```\n\n### SQL functions\nUsing keywords or calling functions dynamically is also possible by using ``` sql`` ``` fragments.\n```js\nconst date = null\n\nawait sql`\n  update users set updated_at = ${ date || sql`now()` }\n`\n\n// Which results in:\nupdate users set updated_at = now()\n```\n\n### Table names\nDynamic identifiers like table names and column names is also supported like so:\n```js\nconst table = 'users'\n    , column = 'id'\n\nawait sql`\n  select ${ sql(column) } from ${ sql(table) }\n`\n\n// Which results in:\nselect \"id\" from \"users\"\n```\n\n### Quick primer on interpolation\n\nHere's a quick oversight over all the ways to do interpolation in a query template string:\n\n| Interpolation syntax       | Usage                         | Example                                                   |\n| -------------              | -------------                 | -------------                                             |\n| `${ sql`` }`               | for keywords or sql fragments | ``await sql`SELECT * FROM users ${sql`order by age desc` }` ``  |\n| `${ sql(string) }`         | for identifiers               | ``await sql`SELECT * FROM ${sql('table_name')` ``               |\n| `${ sql([] or {}, ...) }`  | for helpers                   | ``await sql`INSERT INTO users ${sql({ name: 'Peter'})}` ``      |\n| `${ 'somevalue' }`         | for values                    | ``await sql`SELECT * FROM users WHERE age = ${42}` ``           |\n\n## Advanced query methods\n\n### Cursors\n\n#### ```await sql``.cursor([rows = 1], [fn])```\n\nUse cursors if you need to throttle the amount of rows being returned from a query. You can use a cursor either as an [async iterable](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Statements/for-await...of) or with a callback function. For a callback function new results won't be requested until the promise / async callback function has resolved.\n\n##### callback function\n```js\nawait sql`\n  select\n    *\n  from generate_series(1,4) as x\n`.cursor(async([row]) => {\n  // row = { x: 1 }\n  await http.request('https://example.com/wat', { row })\n})\n```\n\n##### for await...of\n```js\n// for await...of\nconst cursor = sql`select * from generate_series(1,4) as x`.cursor()\n\nfor await (const [row] of cursor) {\n  // row = { x: 1 }\n  await http.request('https://example.com/wat', { row })\n}\n```\n\nA single row will be returned by default, but you can also request batches by setting the number of rows desired in each batch as the first argument to `.cursor`:\n```js\nawait sql`\n  select\n    *\n  from generate_series(1,1000) as x\n`.cursor(10, async rows => {\n  // rows = [{ x: 1 }, { x: 2 }, ... ]\n  await Promise.all(rows.map(row =>\n    http.request('https://example.com/wat', { row })\n  ))\n})\n```\n\nIf an error is thrown inside the callback function no more rows will be requested and the outer promise will reject with the thrown error.\n\nYou can close the cursor early either by calling `break` in the `for await...of` loop, or by returning the token `sql.CLOSE` from the callback function.\n\n```js\nawait sql`\n  select * from generate_series(1,1000) as x\n`.cursor(row => {\n  return Math.random() > 0.9 && sql.CLOSE // or sql.END\n})\n```\n\n### Instant iteration\n\n#### ```await sql``.forEach(fn)```\n\nIf you want to handle rows returned by a query one by one, you can use `.forEach` which returns a promise that resolves once there are no more rows.\n```js\nawait sql`\n  select created_at, name from events\n`.forEach(row => {\n  // row = { created_at: '2019-11-22T14:22:00Z', name: 'connected' }\n})\n\n// No more rows\n```\n\n### Query Descriptions\n#### ```await sql``.describe() -> Result[]```\n\nRather than executing a given query, `.describe` will return information utilized in the query process. This information can include the query identifier, column types, etc.\n\nThis is useful for debugging and analyzing your Postgres queries. Furthermore, **`.describe` will give you access to the final generated query string that would be executed.**\n\n### Rows as Array of Values\n#### ```sql``.values()```\n\nUsing `.values` will return rows as an array of values for each column, instead of objects.\n\nThis can be useful to receive identically named columns, or for specific performance/transformation reasons. The column definitions are still included on the result array, plus access to parsers for each column.\n\n### Rows as Raw Array of Buffers\n#### ```sql``.raw()```\n\nUsing `.raw` will return rows as an array with `Buffer` values for each column, instead of objects.\n\nThis can be useful for specific performance/transformation reasons. The column definitions are still included on the result array, plus access to parsers for each column.\n\n### Queries in Files\n#### `await sql.file(path, [args], [options]) -> Result[]`\n\nUsing a file for a query is also supported with optional parameters to use if the file includes `$1, $2, etc`\n\n```js\nconst result = await sql.file('query.sql', ['Murray', 68])\n```\n\n### Multiple statements in one query\n#### ```await sql``.simple()```\n\nThe postgres wire protocol supports [\"simple\"](https://www.postgresql.org/docs/current/protocol-flow.html#id-1.10.6.7.4) and [\"extended\"](https://www.postgresql.org/docs/current/protocol-flow.html#PROTOCOL-FLOW-EXT-QUERY) queries. \"simple\" queries supports multiple statements, but does not support any dynamic parameters. \"extended\" queries support parameters but only one statement. To use \"simple\" queries you can use\n```sql``.simple()```. That will create it as a simple query.\n\n```js\nawait sql`select 1; select 2;`.simple()\n```\n\n### Copy to/from as Streams\n\nPostgres.js supports [`COPY ...`](https://www.postgresql.org/docs/14/sql-copy.html) queries, which are exposed as [Node.js streams](https://nodejs.org/api/stream.html).\n\n#### ```await sql`copy ... from stdin`.writable() -> Writable```\n\n```js\nimport { pipeline } from 'node:stream/promises'\n\n// Stream of users with the default tab delimitated cells and new-line delimitated rows\nconst userStream = Readable.from([\n  'Murray\\t68\\n',\n  'Walter\\t80\\n'\n])\n\nconst query = await sql`copy users (name, age) from stdin`.writable()\nawait pipeline(userStream, query);\n```\n\n#### ```await sql`copy ... to stdout`.readable() -> Readable```\n\n##### Using Stream Pipeline\n```js\nimport { pipeline } from 'node:stream/promises'\nimport { createWriteStream } from 'node:fs'\n\nconst readableStream = await sql`copy users (name, age) to stdout`.readable()\nawait pipeline(readableStream, createWriteStream('output.tsv'))\n// output.tsv content: `Murray\\t68\\nWalter\\t80\\n`\n```\n\n##### Using `for await...of`\n```js\nconst readableStream = await sql`\n  copy (\n    select name, age\n    from users\n    where age = 68\n  ) to stdout\n`.readable()\nfor await (const chunk of readableStream) {\n  // chunk.toString() === `Murray\\t68\\n`\n}\n```\n\n> **NOTE** This is a low-level API which does not provide any type safety. To make this work, you must match your [`copy query` parameters](https://www.postgresql.org/docs/14/sql-copy.html) correctly to your [Node.js stream read or write](https://nodejs.org/api/stream.html) code. Ensure [Node.js stream backpressure](https://nodejs.org/en/learn/modules/backpressuring-in-streams) is handled correctly to avoid memory exhaustion.\n\n### Canceling Queries in Progress\n\nPostgres.js supports, [canceling queries in progress](https://www.postgresql.org/docs/7.1/protocol-protocol.html#AEN39000). It works by opening a new connection with a protocol level startup message to cancel the current query running on a specific connection. That means there is no guarantee that the query will be canceled, and due to the possible race conditions it might even result in canceling another query. This is fine for long running queries, but in the case of high load and fast queries it might be better to simply ignore results instead of canceling.\n\n```js\nconst query = sql`select pg_sleep 100`.execute()\nsetTimeout(() => query.cancel(), 100)\nconst result = await query\n```\n\n### Execute\n\n#### ```await sql``.execute()```\n\nThe lazy Promise implementation in Postgres.js is what allows it to distinguish [Nested Fragments](#building-queries) from the main outer query. This also means that queries are always executed at the earliest in the following tick. If you have a specific need to execute the query in the same tick, you can call `.execute()`\n\n### Unsafe raw string queries\n\n<details>\n<summary>Advanced unsafe use cases</summary>\n\n### `await sql.unsafe(query, [args], [options]) -> Result[]`\n\nIf you know what you're doing, you can use `unsafe` to pass any string you'd like to postgres. Please note that this can lead to SQL injection if you're not careful.\n\n```js\nsql.unsafe('select ' + danger + ' from users where id = ' + dragons)\n```\n\nYou can also nest `sql.unsafe` within a safe `sql` expression.  This is useful if only part of your fraction has unsafe elements.\n\n```js\nconst triggerName = 'friend_created'\nconst triggerFnName = 'on_friend_created'\nconst eventType = 'insert'\nconst schema_name = 'app'\nconst table_name = 'friends'\n\nawait sql`\n  create or replace trigger ${sql(triggerName)}\n  after ${sql.unsafe(eventType)} on ${sql.unsafe(`${schema_name}.${table_name}`)}\n  for each row\n  execute function ${sql(triggerFnName)}()\n`\n\nawait sql`\n  create role friend_service with login password ${sql.unsafe(`'${password}'`)}\n`\n```\n\n</details>\n\n## Transactions\n\n#### BEGIN / COMMIT `await sql.begin([options = ''], fn) -> fn()`\n\nUse `sql.begin` to start a new transaction. Postgres.js will reserve a connection for the transaction and supply a scoped `sql` instance for all transaction uses in the callback function. `sql.begin` will resolve with the returned value from the callback function.\n\n`BEGIN` is automatically sent with the optional options, and if anything fails `ROLLBACK` will be called so the connection can be released and execution can continue.\n\n```js\nconst [user, account] = await sql.begin(async sql => {\n  const [user] = await sql`\n    insert into users (\n      name\n    ) values (\n      'Murray'\n    )\n    returning *\n  `\n\n  const [account] = await sql`\n    insert into accounts (\n      user_id\n    ) values (\n      ${ user.user_id }\n    )\n    returning *\n  `\n\n  return [user, account]\n})\n```\n\nDo note that you can often achieve the same result using [`WITH` queries (Common Table Expressions)](https://www.postgresql.org/docs/current/queries-with.html) instead of using transactions.\n\nIt's also possible to pipeline the requests in a transaction if needed by returning an array with queries from the callback function like this:\n\n```js\nconst result = await sql.begin(sql => [\n  sql`update ...`,\n  sql`update ...`,\n  sql`insert ...`\n])\n```\n\n#### SAVEPOINT `await sql.savepoint([name], fn) -> fn()`\n\n```js\nsql.begin('read write', async sql => {\n  const [user] = await sql`\n    insert into users (\n      name\n    ) values (\n      'Murray'\n    )\n  `\n\n  const [account] = (await sql.savepoint(sql =>\n    sql`\n      insert into accounts (\n        user_id\n      ) values (\n        ${ user.user_id }\n      )\n    `\n  ).catch(err => {\n    // Account could not be created. ROLLBACK SAVEPOINT is called because we caught the rejection.\n  })) || []\n\n  return [user, account]\n})\n.then(([user, account]) => {\n  // great success - COMMIT succeeded\n})\n.catch(() => {\n  // not so good - ROLLBACK was called\n})\n```\n\n\n#### PREPARE TRANSACTION `await sql.prepare([name]) -> fn()`\n\nIndicates that the transactions should be prepared using the [`PREPARE TRANSACTION [NAME]`](https://www.postgresql.org/docs/current/sql-prepare-transaction.html) statement\ninstead of being committed.\n\n```js\nsql.begin('read write', async sql => {\n  const [user] = await sql`\n    insert into users (\n      name\n    ) values (\n      'Murray'\n    )\n  `\n\n  await sql.prepare('tx1')\n})\n```\n\n## Data Transformation\n\nPostgres.js allows for transformation of the data passed to or returned from a query by using the `transform` option.\n\nBuilt in transformation functions are:\n\n* For camelCase - `postgres.camel`, `postgres.toCamel`, `postgres.fromCamel`\n* For PascalCase - `postgres.pascal`, `postgres.toPascal`, `postgres.fromPascal`\n* For Kebab-Case - `postgres.kebab`, `postgres.toKebab`, `postgres.fromKebab`\n\nThese built in transformations will only convert to/from snake_case. For example, using `{ transform: postgres.toCamel }` will convert the column names to camelCase only if the column names are in snake_case to begin with. `{ transform: postgres.fromCamel }` will convert camelCase only to snake_case.\n\nBy default, using `postgres.camel`, `postgres.pascal` and `postgres.kebab` will perform a two-way transformation - both the data passed to the query and the data returned by the query will be transformed:\n\n```js\n// Transform the column names to and from camel case\nconst sql = postgres({ transform: postgres.camel })\n\nawait sql`CREATE TABLE IF NOT EXISTS camel_case (a_test INTEGER, b_test TEXT)`\nawait sql`INSERT INTO camel_case ${ sql([{ aTest: 1, bTest: 1 }]) }`\nconst data = await sql`SELECT ${ sql('aTest', 'bTest') } FROM camel_case`\n\nconsole.log(data) // [ { aTest: 1, bTest: '1' } ]\n```\n\nTo only perform half of the transformation (eg. only the transformation **to** or **from** camel case), use the other transformation functions:\n\n```js\n// Transform the column names only to camel case\n// (for the results that are returned from the query)\npostgres({ transform: postgres.toCamel })\n\nawait sql`CREATE TABLE IF NOT EXISTS camel_case (a_test INTEGER)`\nawait sql`INSERT INTO camel_case ${ sql([{ a_test: 1 }]) }`\nconst data = await sql`SELECT a_test FROM camel_case`\n\nconsole.log(data) // [ { aTest: 1 } ]\n```\n\n```js\n// Transform the column names only from camel case\n// (for interpolated inserts, updates, and selects)\nconst sql = postgres({ transform: postgres.fromCamel })\n\nawait sql`CREATE TABLE IF NOT EXISTS camel_case (a_test INTEGER)`\nawait sql`INSERT INTO camel_case ${ sql([{ aTest: 1 }]) }`\nconst data = await sql`SELECT ${ sql('aTest') } FROM camel_case`\n\nconsole.log(data) // [ { a_test: 1 } ]\n```\n\n> Note that Postgres.js does not rewrite the static parts of the tagged template strings. So to transform column names in your queries, the `sql()` helper must be used - eg. `${ sql('columnName') }` as in the examples above.\n\n### Transform `undefined` Values\n\nBy default, Postgres.js will throw the error `UNDEFINED_VALUE: Undefined values are not allowed` when undefined values are passed\n\n```js\n// Transform the column names to and from camel case\nconst sql = postgres({\n  transform: {\n    undefined: null\n  }\n})\n\nawait sql`CREATE TABLE IF NOT EXISTS transform_undefined (a_test INTEGER)`\nawait sql`INSERT INTO transform_undefined ${ sql([{ a_test: undefined }]) }`\nconst data = await sql`SELECT a_test FROM transform_undefined`\n\nconsole.log(data) // [ { a_test: null } ]\n```\n\nTo combine with the built in transform functions, spread the transform in the `transform` object:\n\n```js\n// Transform the column names to and from camel case\nconst sql = postgres({\n  transform: {\n    ...postgres.camel,\n    undefined: null\n  }\n})\n\nawait sql`CREATE TABLE IF NOT EXISTS transform_undefined (a_test INTEGER)`\nawait sql`INSERT INTO transform_undefined ${ sql([{ aTest: undefined }]) }`\nconst data = await sql`SELECT ${ sql('aTest') } FROM transform_undefined`\n\nconsole.log(data) // [ { aTest: null } ]\n```\n\n### Custom Transform Functions\n\nTo specify your own transformation functions, you can use the `column`, `value` and `row` options inside of `transform`, each an object possibly including `to` and `from` keys:\n\n* `to`: The function to transform the outgoing query column name to, i.e `SELECT ${ sql('aName') }` to `SELECT a_name` when using `postgres.toCamel`.\n* `from`: The function to transform the incoming query result column name to, see example below.\n\n> Both parameters are optional, if not provided, the default transformation function will be used.\n\n```js\n// Implement your own functions, look at postgres.toCamel, etc\n// as a reference:\n// https://github.com/porsager/postgres/blob/4241824ffd7aa94ffb482e54ca9f585d9d0a4eea/src/types.js#L310-L328\nfunction transformColumnToDatabase() { /* ... */ }\nfunction transformColumnFromDatabase() { /* ... */ }\n\nconst sql = postgres({\n  transform: {\n    column: {\n      to: transformColumnToDatabase,\n      from: transformColumnFromDatabase,\n    },\n    value: { /* ... */ },\n    row: { /* ... */ }\n  }\n})\n```\n\n## Listen & notify\n\nWhen you call `.listen`, a dedicated connection will be created to ensure that you receive notifications instantly. This connection will be used for any further calls to `.listen`. The connection will automatically reconnect according to a backoff reconnection pattern to not overload the database server.\n\n### Listen `await sql.listen(channel, onnotify, [onlisten]) -> { state }`\n`.listen` takes the channel name, a function to handle each notify, and an optional function to run every time listen is registered and ready (happens on initial connect and reconnects). It returns a promise which resolves once the `LISTEN` query to Postgres completes, or if there is already a listener active.\n\n```js\nawait sql.listen('news', payload => {\n  const json = JSON.parse(payload)\n  console.log(json.this) // logs 'is'\n})\n```\n\nThe optional `onlisten` method is great to use for a very simply queue mechanism:\n\n```js\nawait sql.listen(\n  'jobs',\n  (x) => run(JSON.parse(x)),\n  ( ) => sql`select unfinished_jobs()`.forEach(run)\n)\n\nfunction run(job) {\n  // And here you do the work you please\n}\n```\n### Notify `await sql.notify(channel, payload) -> Result[]`\nNotify can be done as usual in SQL, or by using the `sql.notify` method.\n```js\nsql.notify('news', JSON.stringify({ no: 'this', is: 'news' }))\n```\n\n## Realtime subscribe\n\nPostgres.js implements the logical replication protocol of PostgreSQL to support subscription to real-time updates of `insert`, `update` and `delete` operations.\n\n> **NOTE** To make this work you must [create the proper publications in your database](https://www.postgresql.org/docs/current/sql-createpublication.html), enable logical replication by setting `wal_level = logical` in `postgresql.conf` and connect using either a replication or superuser.\n\n### Quick start\n\n#### Create a publication (eg. in migration)\n```sql\nCREATE PUBLICATION alltables FOR ALL TABLES\n```\n\n#### Subscribe to updates\n```js\nconst sql = postgres({ publications: 'alltables' })\n\nconst { unsubscribe } = await sql.subscribe(\n  'insert:events',\n  (row, { command, relation, key, old }) => {\n    // Callback function for each row change\n    // tell about new event row over eg. websockets or do something else\n  },\n  () => {\n    // Callback on initial connect and potential reconnects\n  }\n)\n```\n\n### Subscribe pattern\n\nYou can subscribe to specific operations, tables, or even rows with primary keys.\n\n#### `operation`      `:` `schema` `.` `table` `=` `primary_key`\n\n**`operation`** is one of ``` * | insert | update | delete ``` and defaults to `*`\n\n**`schema`** defaults to `public`\n\n**`table`** is a specific table name and defaults to `*`\n\n**`primary_key`** can be used to only subscribe to specific rows\n\n### Examples\n\n```js\nsql.subscribe('*',                () => /* everything */ )\nsql.subscribe('insert',           () => /* all inserts */ )\nsql.subscribe('*:users',          () => /* all operations on the public.users table */ )\nsql.subscribe('delete:users',     () => /* all deletes on the public.users table */ )\nsql.subscribe('update:users=1',   () => /* all updates on the users row with a primary key = 1 */ )\n```\n\n## Numbers, bigint, numeric\n\n`Number` in javascript is only able to represent 2<sup>53</sup>-1 safely which means that types in PostgreSQLs like `bigint` and `numeric` won't fit into `Number`.\n\nSince Node.js v10.4 we can use [`BigInt`](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/BigInt) to match the PostgreSQL type `bigint` which is returned for eg. `count(*)`. Unfortunately, it doesn't work with `JSON.stringify` out of the box, so Postgres.js will return it as a string.\n\nIf you want to use `BigInt` you can add this custom type:\n\n```js\nconst sql = postgres({\n  types: {\n    bigint: postgres.BigInt\n  }\n})\n```\n\nThere is currently no guaranteed way to handle `numeric` / `decimal` types in native Javascript. **These [and similar] types will be returned as a `string`**. The best way in this case is to use  [custom types](#custom-types).\n\n## Result Array\n\nThe `Result` Array returned from queries is a custom array allowing for easy destructuring or passing on directly to JSON.stringify or general Array usage. It includes the following properties.\n\n### .count\n\nThe `count` property is the number of affected rows returned by the database. This is useful for insert, update and delete operations to know the number of rows since .length will be 0 in these cases if not using `RETURNING ...`.\n\n### .command\n\nThe `command` run by the query - eg. one of `SELECT`, `UPDATE`, `INSERT`, `DELETE`\n\n### .columns\n\nThe `columns` returned by the query useful to determine types, or map to the result values when using `.values()`\n\n```js\n{\n  name  : String,    // Column name,\n  type  : oid,       // PostgreSQL oid column type\n  parser: Function   // The function used by Postgres.js for parsing\n}\n```\n\n### .statement\n\nThe `statement` contains information about the statement implicitly created by Postgres.js.\n\n```js\n{\n  name    : String,  // The auto generated statement name\n  string  : String,  // The actual query string executed\n  types   : [oid],   // An array of oid expected as input parameters\n  columns : [Column] // Array of columns - same as Result.columns\n}\n```\n\n### .state\n\nThis is the state `{ pid, secret }` of the connection that executed the query.\n\n## Connection details\n\n### All Postgres options\n\n```js\nconst sql = postgres('postgres://username:password@host:port/database', {\n  host                 : '',            // Postgres ip address[es] or domain name[s]\n  port                 : 5432,          // Postgres server port[s]\n  path                 : '',            // unix socket path (usually '/tmp')\n  database             : '',            // Name of database to connect to\n  username             : '',            // Username of database user\n  password             : '',            // Password of database user\n  ssl                  : false,         // true, prefer, require, tls.connect options\n  max                  : 10,            // Max number of connections\n  max_lifetime         : null,          // Max lifetime in seconds (more info below)\n  idle_timeout         : 0,             // Idle connection timeout in seconds\n  connect_timeout      : 30,            // Connect timeout in seconds\n  prepare              : true,          // Automatic creation of prepared statements\n  types                : [],            // Array of custom types, see more below\n  onnotice             : fn,            // Default console.log, set false to silence NOTICE\n  onparameter          : fn,            // (key, value) when server param change\n  debug                : fn,            // Is called with (connection, query, params, types)\n  socket               : fn,            // fn returning custom socket to use\n  transform            : {\n    undefined          : undefined,     // Transforms undefined values (eg. to null)\n    column             : fn,            // Transforms incoming column names\n    value              : fn,            // Transforms incoming row values\n    row                : fn             // Transforms entire rows\n  },\n  connection           : {\n    application_name   : 'postgres.js', // Default application_name\n    ...                                 // Other connection parameters, see https://www.postgresql.org/docs/current/runtime-config-client.html\n  },\n  target_session_attrs : null,          // Use 'read-write' with multiple hosts to\n                                        // ensure only connecting to primary\n  fetch_types          : true,          // Automatically fetches types on connect\n                                        // on initial connection.\n})\n```\n\nNote that `max_lifetime = 60 * (30 + Math.random() * 30)` by default. This resolves to an interval between 30 and 60 minutes to optimize for the benefits of prepared statements **and** working nicely with Linux's OOM killer.\n\n### Dynamic passwords\n\nWhen clients need to use alternative authentication schemes such as access tokens or connections to databases with rotating passwords, provide either a synchronous or asynchronous function that will resolve the dynamic password value at connection time.\n\n```js\nconst sql = postgres(url, {\n  // Other connection config\n  ...\n  // Password function for the database user\n  password : async () => await signer.getAuthToken(),\n})\n```\n\n### SSL\n\nAlthough [vulnerable to MITM attacks](https://security.stackexchange.com/a/229297/174913), a common configuration for the `ssl` option for some cloud providers is to set `rejectUnauthorized` to `false` (if `NODE_ENV` is `production`):\n\n```js\nconst sql =\n  process.env.NODE_ENV === 'production'\n    ? // \"Unless you're using a Private or Shield Heroku Postgres database, Heroku Postgres does not currently support verifiable certificates\"\n      // https://help.heroku.com/3DELT3RK/why-can-t-my-third-party-utility-connect-to-heroku-postgres-with-ssl\n      postgres({ ssl: { rejectUnauthorized: false } })\n    : postgres()\n```\n\nFor more information regarding `ssl` with `postgres`, check out the [Node.js documentation for tls](https://nodejs.org/dist/latest-v16.x/docs/api/tls.html#new-tlstlssocketsocket-options).\n\n\n### Multi-host connections - High Availability (HA)\n\nMultiple connection strings can be passed to `postgres()` in the form of `postgres('postgres://localhost:5432,localhost:5433', ...)`. This works the same as native the `psql` command. Read more at [multiple host URIs](https://www.postgresql.org/docs/13/libpq-connect.html#LIBPQ-MULTIPLE-HOSTS).\n\nConnections will be attempted in order of the specified hosts/ports. On a successful connection, all retries will be reset. This ensures that hosts can come up and down seamlessly.\n\nIf you specify `target_session_attrs: 'primary'` or `PGTARGETSESSIONATTRS=primary` Postgres.js will only connect to the primary host, allowing for zero downtime failovers.\n\n### The Connection Pool\n\nConnections are created lazily once a query is created. This means that simply doing const `sql = postgres(...)` won't have any effect other than instantiating a new `sql` instance.\n\n> No connection will be made until a query is made.\n\nFor example:\n\n```js\nconst sql = postgres() // no connections are opened\n\nawait sql`...` // one connection is now opened\nawait sql`...` // previous opened connection is reused\n\n// two connections are opened now\nawait Promise.all([\n  sql`...`,\n  sql`...`\n])\n```\n\n> When there are high amount of concurrent queries, `postgres` will open as many connections as needed up until `max` number of connections is reached. By default `max` is 10. This can be changed by setting `max` in the `postgres()` call. Example - `postgres('connectionURL', { max: 20 })`.\n\nThis means that we get a much simpler story for error handling and reconnections. Queries will be sent over the wire immediately on the next available connection in the pool. Connections are automatically taken out of the pool if you start a transaction using `sql.begin()`, and automatically returned to the pool once your transaction is done.\n\nAny query which was already sent over the wire will be rejected if the connection is lost. It'll automatically defer to the error handling you have for that query, and since connections are lazy it'll automatically try to reconnect the next time a query is made. The benefit of this is no weird generic \"onerror\" handler that tries to get things back to normal, and also simpler application code since you don't have to handle errors out of context.\n\nThere are no guarantees about queries executing in order unless using a transaction with `sql.begin()` or setting `max: 1`. Of course doing a series of queries, one awaiting the other will work as expected, but that's just due to the nature of js async/promise handling, so it's not necessary for this library to be concerned with ordering.\n\nSince this library automatically creates prepared statements, it also has a default max lifetime for connections to prevent memory bloat on the database itself. This is a random interval for each connection between 45 and 90 minutes. This allows multiple connections to independently come up and down without affecting the service.\n\n### Connection timeout\n\nBy default, connections will not close until `.end()` is called. However, it may be useful to have them close automatically when:\n\n- re-instantiating multiple ` sql`` ` instances\n- using Postgres.js in a Serverless environment (Lambda, etc.)\n- using Postgres.js with a database service that automatically closes connections after some time (see [`ECONNRESET` issue](https://github.com/porsager/postgres/issues/179))\n\nThis can be done using the `idle_timeout` or `max_lifetime` options. These configuration options specify the number of seconds to wait before automatically closing an idle connection and the maximum time a connection can exist, respectively.\n\nFor example, to close a connection that has either been idle for 20 seconds or existed for more than 30 minutes:\n\n```js\nconst sql = postgres({\n  idle_timeout: 20,\n  max_lifetime: 60 * 30\n})\n```\n\n### Cloudflare Workers support\n\nPostgres.js has built-in support for the [TCP socket API](https://developers.cloudflare.com/workers/runtime-apis/tcp-sockets/) in Cloudflare Workers, which is [on-track](https://github.com/wintercg/proposal-sockets-api) to be standardized and adopted in Node.js and other JavaScript runtimes, such as Deno.\n\nYou can use Postgres.js directly in a Worker, or to benefit from connection pooling and query caching, via the [Hyperdrive](https://developers.cloudflare.com/hyperdrive/learning/connect-to-postgres/#driver-examples) service available to Workers by passing the Hyperdrive `connectionString` when creating a new `postgres` client as follows:\n\n```ts\n// Requires Postgres.js 3.4.0 or later\nimport postgres from 'postgres'\n\ninterface Env {\n    HYPERDRIVE: Hyperdrive;\n}\n\nexport default async fetch(req: Request, env: Env, ctx: ExecutionContext) {\n    // The Postgres.js library accepts a connection string directly\n    const sql = postgres(env.HYPERDRIVE.connectionString)\n    const results = await sql`SELECT * FROM users LIMIT 10`\n    return Response.json(results)\n}\n```\n\nIn `wrangler.toml` you will need to enable the `nodejs_compat` compatibility flag to allow Postgres.js to operate in the Workers environment:\n\n```toml\ncompatibility_flags = [\"nodejs_compat\"]\n```\n\n### Auto fetching of array types\n\nPostgres.js will automatically fetch table/array-type information when it first connects to a database.\n\nIf you have revoked access to `pg_catalog` this feature will no longer work and will need to be disabled.\n\nYou can disable this feature by setting `fetch_types` to `false`.\n\n### Environmental variables\n\nIt is also possible to connect to the database without a connection string or any options. Postgres.js will fall back to the common environment variables used by `psql` as in the table below:\n\n```js\nconst sql = postgres()\n```\n\n| Option             | Environment Variables    |\n| ------------------ | ------------------------ |\n| `host`             | `PGHOST`                 |\n| `port`             | `PGPORT`                 |\n| `database`         | `PGDATABASE`             |\n| `username`         | `PGUSERNAME` or `PGUSER` |\n| `password`         | `PGPASSWORD`             |\n| `application_name` | `PGAPPNAME`              |\n| `idle_timeout`     | `PGIDLE_TIMEOUT`         |\n| `connect_timeout`  | `PGCONNECT_TIMEOUT`      |\n\n### Prepared statements\n\nPrepared statements will automatically be created for any queries where it can be inferred that the query is static. This can be disabled by using the `prepare: false` option. For instance — this is useful when [using PGBouncer in `transaction mode`](https://github.com/porsager/postgres/issues/93#issuecomment-656290493).\n\n**update**: [since 1.21.0](https://www.pgbouncer.org/2023/10/pgbouncer-1-21-0)\nPGBouncer supports protocol-level named prepared statements when [configured\nproperly](https://www.pgbouncer.org/config.html#max_prepared_statements)\n\n## Custom Types\n\nYou can add ergonomic support for custom types, or simply use `sql.typed(value, type)` inline, where type is the PostgreSQL `oid` for the type and the correctly serialized string. _(`oid` values for types can be found in the `pg_catalog.pg_type` table.)_\n\nAdding Query helpers is the cleanest approach which can be done like this:\n\n```js\nconst sql = postgres({\n  types: {\n    rect: {\n      // The pg_types oid to pass to the db along with the serialized value.\n      to        : 1337,\n\n      // An array of pg_types oids to handle when parsing values coming from the db.\n      from      : [1337],\n\n      //Function that transform values before sending them to the db.\n      serialize : ({ x, y, width, height }) => [x, y, width, height],\n\n      // Function that transforms values coming from the db.\n      parse     : ([x, y, width, height]) => { x, y, width, height }\n    }\n  }\n})\n\n// Now you can use sql.typed.rect() as specified above\nconst [custom] = await sql`\n  insert into rectangles (\n    name,\n    rect\n  ) values (\n    'wat',\n    ${ sql.typed.rect({ x: 13, y: 37, width: 42, height: 80 }) }\n  )\n  returning *\n`\n\n// custom = { name: 'wat', rect: { x: 13, y: 37, width: 42, height: 80 } }\n\n```\n\n### Custom socket\n\nEasily do in-process ssh tunneling to your database by providing a custom socket for Postgres.js to use. The function (optionally async) must return a socket-like duplex stream.\n\nHere's a sample using [ssh2](https://github.com/mscdex/ssh2)\n\n```js\nimport ssh2 from 'ssh2'\n\nconst sql = postgres({\n  ...options,\n  socket: ({ host: [host], port: [port] }) => new Promise((resolve, reject) => {\n    const ssh = new ssh2.Client()\n    ssh\n    .on('error', reject)\n    .on('ready', () =>\n      ssh.forwardOut('127.0.0.1', 12345, host, port,\n        (err, socket) => err ? reject(err) : resolve(socket)\n      )\n    )\n    .connect(sshOptions)\n  })\n})\n```\n\n## Teardown / Cleanup\n\nTo ensure proper teardown and cleanup on server restarts use `await sql.end()` before `process.exit()`.\n\nCalling `sql.end()` will reject new queries and return a Promise which resolves when all queries are finished and the underlying connections are closed. If a `{ timeout }` option is provided any pending queries will be rejected once the timeout (in seconds) is reached and the connections will be destroyed.\n\n#### Sample shutdown using [Prexit](https://github.com/porsager/prexit)\n\n```js\nimport prexit from 'prexit'\n\nprexit(async () => {\n  await sql.end({ timeout: 5 })\n  await new Promise(r => server.close(r))\n})\n```\n\n## Reserving connections\n\n### `await sql.reserve()`\n\nThe `reserve` method pulls out a connection from the pool, and returns a client that wraps the single connection. This can be used for running queries on an isolated connection.\n\n```ts\nconst reserved = await sql.reserve()\nawait reserved`select * from users`\nawait reserved.release()\n```\n\n### `reserved.release()`\n\nOnce you have finished with the reserved connection, call `release` to add it back to the pool.\n\n## Error handling\n\nErrors are all thrown to related queries and never globally. Errors coming from database itself are always in the [native Postgres format](https://www.postgresql.org/docs/current/errcodes-appendix.html), and the same goes for any [Node.js errors](https://nodejs.org/api/errors.html#errors_common_system_errors) eg. coming from the underlying connection.\n\nQuery errors will contain a stored error with the origin of the query to aid in tracing errors.\n\nQuery errors will also contain the `query` string and the `parameters`. These are not enumerable to avoid accidentally leaking confidential information in logs. To log these it is required to specifically access `error.query` and `error.parameters`, or set `debug: true` in options.\n\nThere are also the following errors specifically for this library.\n\n##### UNSAFE_TRANSACTION\n> Only use sql.begin or max: 1\n\nTo ensure statements in a transaction runs on the same connection (which is required for them to run inside the transaction), you must use [`sql.begin(...)`](#transactions) or only allow a single connection in options (`max: 1`).\n\n##### UNDEFINED_VALUE\n> Undefined values are not allowed\n\nPostgres.js won't accept `undefined` as values in tagged template queries since it becomes ambiguous what to do with the value. If you want to set something to null, use `null` explicitly.\n\n##### MESSAGE_NOT_SUPPORTED\n> X (X) is not supported\n\nWhenever a message is received from Postgres which is not supported by this library. Feel free to file an issue if you think something is missing.\n\n##### MAX_PARAMETERS_EXCEEDED\n> Max number of parameters (65534) exceeded\n\nThe postgres protocol doesn't allow more than 65534 (16bit) parameters. If you run into this issue there are various workarounds such as using `sql([...])` to escape values instead of passing them as parameters.\n\n##### SASL_SIGNATURE_MISMATCH\n> Message type X not supported\n\nWhen using SASL authentication the server responds with a signature at the end of the authentication flow which needs to match the one on the client. This is to avoid [man-in-the-middle attacks](https://en.wikipedia.org/wiki/Man-in-the-middle_attack). If you receive this error the connection was canceled because the server did not reply with the expected signature.\n\n##### NOT_TAGGED_CALL\n> Query not called as a tagged template literal\n\nMaking queries has to be done using the sql function as a [tagged template](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Template_literals#Tagged_templates). This is to ensure parameters are serialized and passed to Postgres as query parameters with correct types and to avoid SQL injection.\n\n##### AUTH_TYPE_NOT_IMPLEMENTED\n> Auth type X not implemented\n\nPostgres supports many different authentication types. This one is not supported.\n\n##### CONNECTION_CLOSED\n> write CONNECTION_CLOSED host:port\n\nThis error is thrown if the connection was closed without an error. This should not happen during normal operations, so please create an issue if this was unexpected.\n\n##### CONNECTION_ENDED\n> write CONNECTION_ENDED host:port\n\nThis error is thrown if the user has called [`sql.end()`](#teardown--cleanup) and performed a query afterward.\n\n##### CONNECTION_DESTROYED\n> write CONNECTION_DESTROYED host:port\n\nThis error is thrown for any queries that were pending when the timeout to [`sql.end({ timeout: X })`](#teardown--cleanup) was reached.\n\n##### CONNECTION_CONNECT_TIMEOUT\n> write CONNECTION_CONNECT_TIMEOUT host:port\n\nThis error is thrown if the startup phase of the connection (tcp, protocol negotiation, and auth) took more than the default 30 seconds or what was specified using `connect_timeout` or `PGCONNECT_TIMEOUT`.\n\n## TypeScript support\n\n`postgres` has TypeScript support. You can pass a row list type for your queries in this way:\n```ts\ninterface User {\n  id: number\n  name: string\n}\n\nconst users = await sql<User[]>`SELECT * FROM users`\nusers[0].id // ok => number\nusers[1].name // ok => string\nusers[0].invalid // fails: `invalid` does not exists on `User`\n```\n\nHowever, be sure to check the array length to avoid accessing properties of `undefined` rows:\n```ts\nconst users = await sql<User[]>`SELECT * FROM users WHERE id = ${id}`\nif (!users.length)\n  throw new Error('Not found')\nreturn users[0]\n```\n\nYou can also prefer destructuring when you only care about a fixed number of rows.\nIn this case, we recommend you to prefer using tuples to handle `undefined` properly:\n```ts\nconst [user]: [User?] = await sql`SELECT * FROM users WHERE id = ${id}`\nif (!user) // => User | undefined\n  throw new Error('Not found')\nreturn user // => User\n\n// NOTE:\nconst [first, second]: [User?] = await sql`SELECT * FROM users WHERE id = ${id}` // fails: `second` does not exist on `[User?]`\nconst [first, second] = await sql<[User?]>`SELECT * FROM users WHERE id = ${id}` // don't fail : `second: User | undefined`\n```\n\nWe do our best to type all the public API, however types are not always updated when features are added or changed. Feel free to open an issue if you have trouble with types.\n\n## Migration tools\n\nPostgres.js doesn't come with any migration solution since it's way out of scope, but here are some modules that support Postgres.js for migrations:\n\n- https://github.com/porsager/postgres-shift\n- https://github.com/lukeed/ley\n- https://github.com/JAForbes/pgmg\n\n## Thank you\n\nA really big thank you to [@JAForbes](https://twitter.com/jmsfbs) who introduced me to Postgres and still holds my hand navigating all the great opportunities we have.\n\nThanks to [@ACXgit](https://twitter.com/andreacoiutti) for initial tests and dogfooding.\n\nAlso thanks to [Ryan Dahl](https://github.com/ry) for letting me have the `postgres` npm package name.\n"
        },
        {
          "name": "UNLICENSE",
          "type": "blob",
          "size": 1.18359375,
          "content": "This is free and unencumbered software released into the public domain.\n\nAnyone is free to copy, modify, publish, use, compile, sell, or\ndistribute this software, either in source code form or as a compiled\nbinary, for any purpose, commercial or non-commercial, and by any\nmeans.\n\nIn jurisdictions that recognize copyright laws, the author or authors\nof this software dedicate any and all copyright interest in the\nsoftware to the public domain. We make this dedication for the benefit\nof the public at large and to the detriment of our heirs and\nsuccessors. We intend this dedication to be an overt act of\nrelinquishment in perpetuity of all present and future rights to this\nsoftware under copyright law.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\nEXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\nIN NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY CLAIM, DAMAGES OR\nOTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\nARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\nOTHER DEALINGS IN THE SOFTWARE.\n\nFor more information, please refer to <https://unlicense.org/>\n"
        },
        {
          "name": "cf",
          "type": "tree",
          "content": null
        },
        {
          "name": "cjs",
          "type": "tree",
          "content": null
        },
        {
          "name": "demo.gif",
          "type": "blob",
          "size": 372.1953125,
          "content": null
        },
        {
          "name": "deno",
          "type": "tree",
          "content": null
        },
        {
          "name": "package.json",
          "type": "blob",
          "size": 1.7724609375,
          "content": "{\n  \"name\": \"postgres\",\n  \"version\": \"3.4.5\",\n  \"description\": \"Fastest full featured PostgreSQL client for Node.js\",\n  \"type\": \"module\",\n  \"module\": \"src/index.js\",\n  \"main\": \"cjs/src/index.js\",\n  \"exports\": {\n    \"types\": \"./types/index.d.ts\",\n    \"bun\": \"./src/index.js\",\n    \"workerd\": \"./cf/src/index.js\",\n    \"import\": \"./src/index.js\",\n    \"default\": \"./cjs/src/index.js\"\n  },\n  \"types\": \"types/index.d.ts\",\n  \"typings\": \"types/index.d.ts\",\n  \"engines\": {\n    \"node\": \">=12\"\n  },\n  \"scripts\": {\n    \"build\": \"npm run build:cjs && npm run build:deno && npm run build:cf\",\n    \"build:cjs\": \"node transpile.cjs\",\n    \"build:deno\": \"node transpile.deno.js\",\n    \"build:cf\": \"node transpile.cf.js\",\n    \"test\": \"npm run test:esm && npm run test:cjs && npm run test:deno\",\n    \"test:esm\": \"node tests/index.js\",\n    \"test:cjs\": \"npm run build:cjs && cd cjs/tests && node index.js && cd ../../\",\n    \"test:deno\": \"npm run build:deno && cd deno/tests && deno run --unstable --allow-all --unsafely-ignore-certificate-errors index.js && cd ../../\",\n    \"lint\": \"eslint src && eslint tests\",\n    \"prepare\": \"npm run build\",\n    \"prepublishOnly\": \"npm run lint\"\n  },\n  \"files\": [\n    \"/cf/src\",\n    \"/cf/polyfills.js\",\n    \"/cjs/src\",\n    \"/cjs/package.json\",\n    \"/src\",\n    \"/types\"\n  ],\n  \"author\": \"Rasmus Porsager <rasmus@porsager.com> (https://www.porsager.com)\",\n  \"funding\": {\n    \"type\": \"individual\",\n    \"url\": \"https://github.com/sponsors/porsager\"\n  },\n  \"license\": \"Unlicense\",\n  \"repository\": \"porsager/postgres\",\n  \"homepage\": \"https://github.com/porsager/postgres\",\n  \"bugs\": \"https://github.com/porsager/postgres/issues\",\n  \"keywords\": [\n    \"driver\",\n    \"postgresql\",\n    \"postgres.js\",\n    \"postgres\",\n    \"postrges\",\n    \"postgre\",\n    \"client\",\n    \"sql\",\n    \"db\",\n    \"pg\",\n    \"database\"\n  ]\n}\n"
        },
        {
          "name": "postgresjs.svg",
          "type": "blob",
          "size": 6.4169921875,
          "content": "<svg width=\"440\" height=\"180\" viewBox=\"0 0 440 180\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\n    <rect y=\"29\" width=\"358\" height=\"121\" rx=\"24\" fill=\"white\" />\n    <path d=\"M90.1 120.5C89.3 120.5 88.5 120.1 88 119.3C87.3 118.1 87.6 116.6 88.8 115.8L93.7 112.7H93.8C96.4 111.2 98.1 108.3 98.1 105.3V73C98.1 70 96.5 67.1 93.8 65.6C93.8 65.6 93.8 65.6 93.7 65.6L65.8 48.2C64.6 47.5 63.5 48 63.1 48.2C62.7 48.4 61.7 49.2 61.7 50.6V87.1C61.7 89.8 63.1 92.2 65.3 93.6C67.6 95 70.3 95.1 72.7 93.9L79.6 90.5C80.8 89.9 82.3 90.4 83 91.6C83.6 92.8 83.1 94.3 81.9 95L75 98.4C71.1 100.4 66.5 100.2 62.7 97.8C59 95.5 56.7 91.5 56.7 87.1V50.6C56.7 47.8 58.2 45.3 60.6 43.9C63 42.5 65.9 42.5 68.4 43.9C68.4 43.9 68.4 43.9 68.5 43.9L96.4 61.3C100.6 63.7 103.1 68.2 103.1 73V105.4C103.1 110.2 100.5 114.7 96.4 117.1L91.5 120.2C91 120.4 90.6 120.5 90.1 120.5Z\" fill=\"black\" />\n    <path d=\"M53.9 132.9C52.6 132.9 51.3 132.6 50.1 131.9L26.4 118.5C21.9 115.9 19 111 19 105.7V83C19 76.3 22.6 70.1 28.3 66.8L45 57C46.2 56.3 47.7 56.7 48.4 57.9C49.1 59.1 48.7 60.6 47.5 61.3L30.8 71.1C26.6 73.5 24 78.1 24 83V105.7C24 109.2 25.9 112.4 28.8 114.1L52.5 127.5C53.7 128.2 54.8 127.7 55.2 127.5C55.6 127.3 56.5 126.6 56.5 125.2V108.1C56.5 106.7 57.6 105.6 59 105.6C60.4 105.6 61.5 106.7 61.5 108.1V125.2C61.5 128 60.1 130.5 57.7 131.9C56.6 132.6 55.2 132.9 53.9 132.9Z\" fill=\"black\" />\n    <path d=\"M90.3 90C91.6807 90 92.8 88.8807 92.8 87.5C92.8 86.1193 91.6807 85 90.3 85C88.9193 85 87.8 86.1193 87.8 87.5C87.8 88.8807 88.9193 90 90.3 90Z\" fill=\"black\" />\n    <path d=\"M128.5 74.9C128.5 74.4 128.9 74 129.4 74H139.8C145.6 74 150.4 78.7 150.4 84.4C150.4 90.3 145.6 95 139.9 95H132.1V106.1C132.1 106.6 131.7 107 131.2 107H129.4C128.9 107 128.5 106.6 128.5 106.1V74.9V74.9ZM139.6 91.6C143.5 91.6 146.8 88.7 146.8 84.4C146.8 80.2 143.5 77.6 139.6 77.6H132.1V91.6H139.6Z\" fill=\"black\" />\n    <path d=\"M164.2 84.8C170.2 84.8 175.2 90 175.2 96.1C175.2 102.3 170.3 107.6 164.2 107.6C158.2 107.6 153.2 102.4 153.2 96.1C153.2 90 158.1 84.8 164.2 84.8ZM164.2 104.4C168.5 104.4 171.7 100.6 171.7 96.1C171.7 91.7 168.5 87.9 164.2 87.9C159.9 87.9 156.7 91.6 156.7 96.1C156.7 100.6 159.9 104.4 164.2 104.4Z\" fill=\"black\" />\n    <path d=\"M180 104.9C179.8 104.7 179.7 104.3 179.9 104L180.6 102.8C180.9 102.3 181.3 102.1 181.9 102.5C182.7 103.1 184.5 104.4 187.3 104.4C189.5 104.4 191 103.4 191 101.7C191 99.6 189.3 98.8 186.1 97.2C183.2 95.8 179.9 94.2 179.9 90.5C179.9 88.2 181.6 84.8 186.7 84.8C189.5 84.8 191.9 85.7 192.9 86.5C193.4 86.9 193.5 87.3 193.1 88L192.6 88.9C192.3 89.5 191.6 89.6 191.1 89.3C190.2 88.7 188.6 88 186.6 88C184.2 88 183.4 89.4 183.4 90.4C183.4 92.3 184.9 93 187.6 94.3C191.4 96 194.6 97.5 194.6 101.4C194.6 104.8 191.6 107.6 187.2 107.6C183.5 107.6 181.1 106 180 104.9Z\" fill=\"black\" />\n    <path d=\"M198.8 88.3C198.1 88.3 197.7 87.8 197.7 87.3V86.4C197.7 85.8 198.1 85.4 198.8 85.4H201.7V79.4C201.7 78.8 202.2 78.4 202.7 78.4L204.2 78.3C204.8 78.3 205.2 78.8 205.2 79.3V85.4H211.6C212.2 85.4 212.7 85.8 212.7 86.4V87.3C212.7 87.9 212.2 88.3 211.6 88.3H205.2V101.3C205.2 103.6 205.9 104.4 207.1 104.4C209.1 104.4 210.5 103.6 211.4 103C212.2 102.5 212.5 102.9 212.8 103.4L213.2 104.4C213.5 105.1 213.4 105.4 212.8 105.8C211.9 106.5 209.6 107.5 206.7 107.5C202.5 107.5 201.7 104.9 201.7 101.5V88.3H198.8Z\" fill=\"black\" />\n    <path d=\"M236.6 107.2C236.6 114.7 232 117.4 227.1 117.4C223.7 117.4 220.6 115.6 219.5 114.6C219.1 114.3 219 113.7 219.3 113.3L220.2 111.9C220.5 111.4 221.1 111.4 221.6 111.9C222.8 112.9 224.7 114.2 227.1 114.2C230.5 114.2 233.2 112.3 233.2 107.2V105.6C233.2 105.6 230.7 107.5 227.2 107.5C221.2 107.5 216.9 102.4 216.9 96.1C216.9 89.9 221.1 84.7 227.1 84.7C231 84.7 233.6 86.9 233.6 86.9L233.9 85.9C234.1 85.5 234.4 85.1 234.9 85.1H235.6C236.2 85.1 236.7 85.6 236.7 86.1L236.6 107.2ZM227.6 104.3C230.6 104.3 233.1 102.5 233.1 102.5V90.6C233.1 90.6 230.9 88 227.3 88C223 88 220.3 91.9 220.3 96.1C220.3 100.8 223.4 104.3 227.6 104.3Z\" fill=\"black\" />\n    <path d=\"M244 86.3C244 85.8 244.5 85.3 245 85.3H245.8C246.2 85.3 246.6 85.5 246.7 86.1L247.1 87.7C247.1 87.7 249.3 84.9 253.8 84.9C255.5 84.9 258.1 85.1 257.6 86.6L257 88.3C256.8 89 256.2 89.1 255.7 88.8C255.2 88.6 254.4 88.2 253.3 88.2C249.1 88.2 247.4 91 247.4 91V106C247.4 106.6 246.9 107 246.4 107H245C244.4 107 244 106.5 244 106V86.3Z\" fill=\"black\" />\n    <path d=\"M270.7 84.8C276.4 84.8 280.6 89 280.6 94.7C280.6 95.1 280.6 95.6 280.5 96C280.5 96.6 280 96.9 279.5 96.9H263.6C263.7 100.5 266.8 104.4 270.7 104.4C273.6 104.4 275.4 103.4 276.6 102.6C277.4 102.1 277.7 101.9 278.3 102.7L278.9 103.5C279.3 104.2 279.5 104.4 278.7 105C277.4 106 274.5 107.5 270.7 107.5C264.1 107.5 260.1 102.3 260.1 96.1C260.1 90 264.1 84.8 270.7 84.8ZM277.1 94.2C277 91 274.5 88 270.7 88C267 88 264.1 90.8 263.7 94.2H277.1Z\" fill=\"black\" />\n    <path d=\"M285.1 104.9C284.9 104.7 284.8 104.3 285 104L285.7 102.8C286 102.3 286.4 102.1 287 102.5C287.8 103.1 289.6 104.4 292.4 104.4C294.6 104.4 296.1 103.4 296.1 101.7C296.1 99.6 294.4 98.8 291.2 97.2C288.3 95.8 285 94.2 285 90.5C285 88.2 286.7 84.8 291.8 84.8C294.6 84.8 297 85.7 298 86.5C298.5 86.9 298.6 87.3 298.2 88L297.7 88.9C297.4 89.5 296.7 89.6 296.2 89.3C295.3 88.7 293.7 88 291.7 88C289.3 88 288.5 89.4 288.5 90.4C288.5 92.3 290 93 292.7 94.3C296.5 96 299.7 97.5 299.7 101.4C299.7 104.8 296.7 107.6 292.3 107.6C288.7 107.6 286.3 106 285.1 104.9Z\" fill=\"black\" />\n    <path d=\"M307.5 101.3C309.2 101.3 310.6 102.7 310.6 104.4C310.6 106.1 309.2 107.6 307.5 107.6C305.8 107.6 304.4 106.1 304.4 104.4C304.4 102.7 305.8 101.3 307.5 101.3Z\" fill=\"black\" />\n    <path d=\"M320.4 109.4C320.4 115.3 317.8 118 314.2 118.7C313.7 118.8 313.4 118.6 313.2 118.1L312.7 116.7C312.5 116 312.8 115.6 313.4 115.4C315.6 114.6 316.9 113.4 316.9 109.6V86.3C316.9 85.7 317.4 85.3 317.9 85.3H319.3C319.8 85.3 320.3 85.7 320.3 86.3V109.4H320.4ZM316 77.1C316 75.6 317.1 74.5 318.6 74.5C320 74.5 321.1 75.6 321.1 77.1C321.1 78.5 320 79.6 318.6 79.6C317.1 79.6 316 78.4 316 77.1Z\" fill=\"black\" />\n    <path d=\"M326.6 104.9C326.4 104.7 326.3 104.3 326.5 104L327.2 102.8C327.5 102.3 327.9 102.1 328.5 102.5C329.3 103.1 331.1 104.4 333.9 104.4C336.1 104.4 337.6 103.4 337.6 101.7C337.6 99.6 335.9 98.8 332.7 97.2C329.8 95.8 326.5 94.2 326.5 90.5C326.5 88.2 328.2 84.8 333.3 84.8C336.1 84.8 338.5 85.7 339.5 86.5C340 86.9 340.1 87.3 339.7 88L339.2 88.9C338.9 89.5 338.2 89.6 337.7 89.3C336.8 88.7 335.2 88 333.2 88C330.8 88 330 89.4 330 90.4C330 92.3 331.5 93 334.2 94.3C338 96 341.2 97.5 341.2 101.4C341.2 104.8 338.2 107.6 333.8 107.6C330.2 107.6 327.8 106 326.6 104.9Z\" fill=\"black\" />\n</svg>\n"
        },
        {
          "name": "src",
          "type": "tree",
          "content": null
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "transpile.cf.js",
          "type": "blob",
          "size": 1.384765625,
          "content": "import fs from 'fs'\nimport path from 'path'\n\nconst empty = x => fs.readdirSync(x).forEach(f => fs.unlinkSync(path.join(x, f)))\n    , ensureEmpty = x => !fs.existsSync(x) ? fs.mkdirSync(x) : empty(x)\n    , root = 'cf'\n    , src = path.join(root, 'src')\n\nensureEmpty(src)\n\nfs.readdirSync('src').forEach(name =>\n  fs.writeFileSync(\n    path.join(src, name),\n    transpile(fs.readFileSync(path.join('src', name), 'utf8'), name, 'src')\n  )\n)\n\nfunction transpile(x) {\n  const timers = x.includes('setImmediate')\n    ? 'import { setImmediate, clearImmediate } from \\'../polyfills.js\\'\\n'\n    : ''\n\n  const process = x.includes('process.')\n    ? 'import { process } from \\'../polyfills.js\\'\\n'\n    : ''\n\n  const buffer = x.includes('Buffer')\n    ? 'import { Buffer } from \\'node:buffer\\'\\n'\n    : ''\n\n  return process + buffer + timers + x\n    .replace('import net from \\'net\\'', 'import { net } from \\'../polyfills.js\\'')\n    .replace('import tls from \\'tls\\'', 'import { tls } from \\'../polyfills.js\\'')\n    .replace('import crypto from \\'crypto\\'', 'import { crypto } from \\'../polyfills.js\\'')\n    .replace('import os from \\'os\\'', 'import { os } from \\'../polyfills.js\\'')\n    .replace('import fs from \\'fs\\'', 'import { fs } from \\'../polyfills.js\\'')\n    .replace('import { performance } from \\'perf_hooks\\'', 'import { performance } from \\'../polyfills.js\\'')\n    .replace(/ from '([a-z_]+)'/g, ' from \\'node:$1\\'')\n}\n"
        },
        {
          "name": "transpile.cjs",
          "type": "blob",
          "size": 1.650390625,
          "content": "const fs = require('fs')\n    , path = require('path')\n\nconst empty = x => fs.readdirSync(x).forEach(f => fs.unlinkSync(path.join(x, f)))\n    , ensureEmpty = x => !fs.existsSync(x) ? fs.mkdirSync(x) : empty(x)\n    , root = 'cjs'\n    , src = path.join(root, 'src')\n    , tests = path.join(root, 'tests')\n\n!fs.existsSync(root) && fs.mkdirSync(root)\nensureEmpty(src)\nensureEmpty(tests)\n\nfs.readdirSync('src').forEach(name =>\n  fs.writeFileSync(\n    path.join(src, name),\n    transpile(fs.readFileSync(path.join('src', name), 'utf8'))\n  )\n)\n\nfs.readdirSync('tests').forEach(name =>\n  fs.writeFileSync(\n    path.join(tests, name),\n    name.endsWith('.js')\n      ? transpile(fs.readFileSync(path.join('tests', name), 'utf8'))\n      : fs.readFileSync(path.join('tests', name), 'utf8')\n  )\n)\n\nfs.writeFileSync(path.join(root, 'package.json'), JSON.stringify({ type: 'commonjs' }))\n\nfunction transpile(x) {\n  return x.replace(/export default function ([^(]+)/, 'module.exports = $1;function $1')\n          .replace(/export class ([a-z0-9_$]+)/gi, 'const $1 = module.exports.$1 = class $1')\n          .replace(/export default /, 'module.exports = ')\n          .replace(/export {/g, 'module.exports = {')\n          .replace(/export const ([a-z0-9_$]+)/gi, 'const $1 = module.exports.$1')\n          .replace(/export function ([a-z0-9_$]+)/gi, 'module.exports.$1 = $1;function $1')\n          .replace(/import {([^{}]*?)} from (['\"].*?['\"])/gi, 'const {$1} = require($2)')\n          .replace(/import (.*?) from (['\"].*?['\"])/gi, 'const $1 = require($2)')\n          .replace(/import (['\"].*?['\"])/gi, 'require($1)')\n          .replace('new URL(x, import.meta.url)', 'require(\"path\").join(__dirname, x)')\n}\n"
        },
        {
          "name": "transpile.deno.js",
          "type": "blob",
          "size": 3,
          "content": "import fs from 'fs'\nimport path from 'path'\n\nconst std = 'https://deno.land/std@0.132.0/'\n    , empty = x => fs.readdirSync(x).forEach(f => fs.unlinkSync(path.join(x, f)))\n    , ensureEmpty = x => !fs.existsSync(x) ? fs.mkdirSync(x) : empty(x)\n    , root = 'deno'\n    , src = path.join(root, 'src')\n    , types = path.join(root, 'types')\n    , tests = path.join(root, 'tests')\n\nensureEmpty(src)\nensureEmpty(types)\nensureEmpty(tests)\n\nfs.writeFileSync(\n  path.join(types, 'index.d.ts'),\n  transpile(fs.readFileSync(path.join('types', 'index.d.ts'), 'utf8'), 'index.d.ts', 'types')\n)\n\nfs.writeFileSync(\n  path.join(root, 'README.md'),\n  fs.readFileSync('README.md', 'utf8')\n    .replace(/### Installation(\\n.*){4}/, '')\n    .replace(\n      'import postgres from \\'postgres\\'',\n      'import postgres from \\'https://deno.land/x/postgresjs/mod.js\\''\n    )\n)\n\nfs.readdirSync('src').forEach(name =>\n  fs.writeFileSync(\n    path.join(src, name),\n    transpile(fs.readFileSync(path.join('src', name), 'utf8'), name, 'src')\n  )\n)\n\nfs.readdirSync('tests').forEach(name =>\n  fs.writeFileSync(\n    path.join(tests, name),\n    name.endsWith('.js')\n      ? transpile(fs.readFileSync(path.join('tests', name), 'utf8'), name, 'tests')\n      : fs.readFileSync(path.join('tests', name), 'utf8')\n  )\n)\n\nfs.writeFileSync(path.join(root, 'package.json'), JSON.stringify({ type: 'commonjs' }))\n\nfunction transpile(x, name, folder) {\n  if (folder === 'tests') {\n    if (name === 'bootstrap.js') {\n      x = x.replace('export function exec(', 'function ignore(')\n           .replace('async function execAsync(', 'export async function exec(')\n           .replace(/\\nexec\\(/g, '\\nawait exec(')\n           .replace('{ spawnSync }', '{ spawn }')\n    }\n    if (name === 'index.js')\n      x += '\\n;globalThis.addEventListener(\"unload\", () => Deno.exit(process.exitCode))'\n  }\n\n  const buffer = x.includes('Buffer')\n    ? 'import { Buffer } from \\'' + std + 'node/buffer.ts\\'\\n'\n    : ''\n\n  const process = x.includes('process.')\n    ? 'import process from \\'' + std + 'node/process.ts\\'\\n'\n    : ''\n\n  const timers = x.includes('setImmediate')\n    ? 'import { setImmediate, clearImmediate } from \\'../polyfills.js\\'\\n'\n    : ''\n\n  const hmac = x.includes('createHmac')\n    ? 'import { HmacSha256 } from \\'' + std + 'hash/sha256.ts\\'\\n'\n    : ''\n\n  return hmac + buffer + process + timers + x\n    .replace(\n      'crypto.createHmac(\\'sha256\\', key).update(x).digest()',\n      'Buffer.from(new HmacSha256(key).update(x).digest())'\n    )\n    .replace(\n      'query.writable.push({ chunk, callback })',\n      '(query.writable.push({ chunk }), callback())'\n    )\n    .replace('socket.setKeepAlive(true, 1000 * keep_alive)', 'socket.setKeepAlive(true)')\n    .replace('node:stream', std + 'node/stream.ts')\n    .replace('import net from \\'net\\'', 'import { net } from \\'../polyfills.js\\'')\n    .replace('import tls from \\'tls\\'', 'import { tls } from \\'../polyfills.js\\'')\n    .replace('import { performance } from \\'perf_hooks\\'', '')\n    .replace(/ from '([a-z_]+)'/g, ' from \\'' + std + 'node/$1.ts\\'')\n}\n"
        },
        {
          "name": "types",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}