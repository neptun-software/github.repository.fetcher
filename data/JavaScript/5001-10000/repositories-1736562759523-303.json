{
  "metadata": {
    "timestamp": 1736562759523,
    "page": 303,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjM0MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "thunlp/WantWords",
      "stars": 7028,
      "defaultBranch": "main",
      "files": [
        {
          "name": "README.md",
          "type": "blob",
          "size": 2.744140625,
          "content": "### [中](README_ZH.md)|En\n<p align=\"center\">\n  <a href=\"https://wantwords.thunlp.org/\">\n  \t<img src=\"resources/wantwords_logo.svg\" width = \"300\"  alt=\"WantWords Logo\"  />\n  </a>\n</p>\n<h3 align=\"center\">An Open-source Online Reverse Dictionary [<a href=\"https://wantwords.net/\">link</a>] </h3>\n\n## News\n\nThe WantWords MiniProgram has been launched. Welcome to scan the following QR code to try it!\n<div align=center>\n<img src=\"resources/miniprogram.jpg\" width = \"300\"  alt=\"MiniProgram QR code\"/>\n</div>\n\n\n\n## What Is a Reverse Dictionary?\nOpposite to a regular (forward) dictionary that provides definitions for query words, a reverse dictionary returns words semantically matching the query descriptions.\n\n<div align=center>\n<img src=\"resources/rd_example.png\" alt=\"rd_example\" width = \"600\"/>\n</div>\n\n## What Can a Reverse Dictionary Do?\n* Solve the *tip-of-the-tongue problem*, the phenomenon of failing to retrieve a word from memory\n* Help new language learners\n* Help word selection (or word dictionary) anomia patients, people who can recognize and describe an object but fail to name it due to neurological disorder\n\n## Our System\n### Workflow\n\n<div align=center>\n<img src=\"resources/workflow.png\" alt=\"workflow\" width = \"500\"  />\n</div>\n\n### Core Model\n\nThe core model of WantWords is based on our proposed **Multi-channel Reverse Dictionary Model** [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/5365/5221)] [[code](https://github.com/thunlp/MultiRD)], as illustrate in the following figure.\n\n<div align=center>\n<img src=\"resources/MRD_model.png\" alt=\"model\" width = \"500\" />\n</div>\n\n\n### Pre-trained Models and Data\n\nYou can [download](https://cloud.tsinghua.edu.cn/d/811dcb428ed24480bc60/) and decompress the pre-trained models and data to `BASE_PATH/website_RD/` to reimplement the system.\n\n### Key Requirements\n\n* Django==2.2.5\n* django-cors-headers==3.5.0\n* numpy==1.17.2\n* pytorch-transformers==1.2.0\n* requests==2.22.0\n* scikit-learn==0.22.1\n* scipy==1.4.1\n* thulac==0.2.0\n* torch==1.2.0\n* urllib3==1.25.6\n* uWSGI==2.0.18\n* uwsgitop==0.11\n\n## Cite\n\nIf the code or data help you, please cite the following two papers.\n\n```\n@inproceedings{qi2020wantwords,\n  title={WantWords: An Open-source Online Reverse Dictionary System},\n  author={Qi, Fanchao and Zhang, Lei and Yang, Yanhui and Liu, Zhiyuan and Sun, Maosong},\n  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},\n  pages={175--181},\n  year={2020}\n}\n\n@inproceedings{zhang2020multi,\n  title={Multi-channel reverse dictionary model},\n  author={Zhang, Lei and Qi, Fanchao and Liu, Zhiyuan and Wang, Yasheng and Liu, Qun and Sun, Maosong},\n  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},\n  pages={312--319},\n  year={2020}\n}\n```\n\n\n\n"
        },
        {
          "name": "README_ZH.md",
          "type": "blob",
          "size": 2.9873046875,
          "content": "### 中|[En](README.md)\n<p align=\"center\">\n  <a href=\"https://wantwords.thunlp.org/\">\n  \t<img src=\"resources/wantwords_logo.svg\" width = \"300\"  alt=\"WantWords Logo\"/>\n  </a>\n</p>\n<h3 align=\"center\">首个支持中文及跨语言查询的开源在线反向词典 [<a href=\"https://wantwords.net/\">点击访问</a>] </h3>\n\n## 新闻\n\nWantWords小程序已上线，欢迎扫码体验！\n<div align=center>\n<img src=\"resources/miniprogram.jpg\" width = \"300\"  alt=\"MiniProgram QR code\"/>\n</div>\n\n\n## 什么是反向词典?\n\n普通的词典告诉你某个词语的定义，而反向词典恰好相反，可以告诉你哪些词语符合你输入描述的意思。下图为WantWords在线反向词典的页面截图，其中演示了反向查词的一个示例，输入“山非常高”，系统将返回一系列模型认为表达“山非常高”意思的词语，例如“高峻”、“巍峨”等。\n\n<div align=center>\n<img src=\"resources/screenshot.png\" alt=\"rd_example\" width = \"700\"  />\n</div>\n\n## 反向词典可以用来做什么?\n* 解决“舌尖现象”（*tip-of-the-tongue*，又称话到嘴边说不出来），即暂时性忘词的问题\n* 帮助语言学习者学习、巩固词汇\n* 改善选词性失语者患者的生活质量，该病的症状是可以识别并描述一个物体，但是无法记起该物体的名字\n\n## 系统架构\n### 工作流\n\n<div align=center>\n<img src=\"resources/workflow_ZH.png\" alt=\"workflow\" width = \"500\"/>\n</div>\n\n### 核心模型\n\nWantWords的核心模型为我们此前发表在AAAI-20上的一篇论文提出的多通道反向词典模型：**Multi-channel Reverse Dictionary Model** [[论文](https://ojs.aaai.org/index.php/AAAI/article/view/5365/5221)] [[代码](https://github.com/thunlp/MultiRD)]，其模型架构如下所示。\n\n<div align=center>\n<img src=\"resources/MRD_model_ZH.png\" alt=\"model\" width = \"500\" />\n</div>\n\n\n### 模型和数据\n\n可从[此处](https://cloud.tsinghua.edu.cn/d/811dcb428ed24480bc60/)下载并解压模型和数据到 `BASE_PATH/website_RD/` 以构建此系统。\n\n### 关键依赖\n\n* Django==2.2.5\n* django-cors-headers==3.5.0\n* numpy==1.17.2\n* pytorch-transformers==1.2.0\n* requests==2.22.0\n* scikit-learn==0.22.1\n* scipy==1.4.1\n* thulac==0.2.0\n* torch==1.2.0\n* urllib3==1.25.6\n* uWSGI==2.0.18\n* uwsgitop==0.11\n\n## 引用\n\n如果本项目的代码或者数据帮到你，请引用以下两篇论文：\n\n```\n@inproceedings{qi2020wantwords,\n  title={WantWords: An Open-source Online Reverse Dictionary System},\n  author={Qi, Fanchao and Zhang, Lei and Yang, Yanhui and Liu, Zhiyuan and Sun, Maosong},\n  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},\n  pages={175--181},\n  year={2020}\n}\n\n@inproceedings{zhang2020multi,\n  title={Multi-channel reverse dictionary model},\n  author={Zhang, Lei and Qi, Fanchao and Liu, Zhiyuan and Wang, Yasheng and Liu, Qun and Sun, Maosong},\n  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},\n  pages={312--319},\n  year={2020}\n}\n```\n\n\n\n"
        },
        {
          "name": "manage.py",
          "type": "blob",
          "size": 0.615234375,
          "content": "#!/usr/bin/env python\n\"\"\"Django's command-line utility for administrative tasks.\"\"\"\nimport os\nimport sys\n\n\ndef main():\n    os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'website_RD.settings')\n    try:\n        from django.core.management import execute_from_command_line\n    except ImportError as exc:\n        raise ImportError(\n            \"Couldn't import Django. Are you sure it's installed and \"\n            \"available on your PYTHONPATH environment variable? Did you \"\n            \"forget to activate a virtual environment?\"\n        ) from exc\n    execute_from_command_line(sys.argv)\n\n\nif __name__ == '__main__':\n    main()\n"
        },
        {
          "name": "model.py",
          "type": "blob",
          "size": 5.2314453125,
          "content": "import torch\r\n\r\nclass Encoder(torch.nn.Module):\r\n    def __init__(self, vocab_size, embed_dim, hidden_dim, layers, class_num, encoder, sememe_num, chara_num, mode):\r\n        super().__init__()\r\n        self.vocab_size = vocab_size\r\n        self.embed_dim = 200\r\n        self.hidden_dim = 768\r\n        self.layers = layers\r\n        self.class_num = class_num\r\n        self.sememe_num = sememe_num\r\n        self.chara_num = chara_num\r\n        self.embedding = torch.nn.Embedding(self.vocab_size, self.embed_dim, padding_idx=0, max_norm=5, sparse=True)\r\n        self.embedding.weight.requires_grad = False\r\n        self.embedding_dropout = torch.nn.Dropout(0.2)\r\n        self.encoder = encoder\r\n        self.fc = torch.nn.Linear(self.hidden_dim, self.embed_dim)\r\n        self.loss = torch.nn.CrossEntropyLoss()\r\n        self.relu = torch.nn.ReLU()\r\n        if 'P' in mode:\r\n            self.fc2 = torch.nn.Linear(self.hidden_dim, 13)\r\n        if 's' in mode:\r\n            self.fc1 = torch.nn.Linear(self.hidden_dim, self.sememe_num)\r\n        if 'c' in mode:\r\n            self.fc3 = torch.nn.Linear(self.hidden_dim, self.chara_num)\r\n        if 'C' in mode:\r\n            self.fc_C1 = torch.nn.Linear(self.hidden_dim, 12)\r\n            self.fc_C2 = torch.nn.Linear(self.hidden_dim, 95)\r\n            self.fc_C3 = torch.nn.Linear(self.hidden_dim, 1425)\r\n        \r\n    def forward(self, operation, x=None, w=None, ws=None, wP=None, wc=None, wC=None, msk_s=None, msk_c=None, mode=None):\r\n        # x: T(bat, max_word_num)\r\n        # w: T(bat)\r\n        # x_embedding: T(bat, max_word_num, embed_dim)\r\n        x = x.long()\r\n        attention_mask = torch.gt(x, 0).to(torch.int64)\r\n        h = self.encoder(x, attention_mask=attention_mask)[0]\r\n        h_1 = self.embedding_dropout(h[:,0,:])\r\n        vd = self.fc(h_1)\r\n        # score0: T(bat, 30000) = [bat, emb] .mm [class_num, emb].t()\r\n        score0 = vd.mm(self.embedding.weight.data[[range(self.class_num)]].t())\r\n        score = score0\r\n        \r\n        if 'C' in mode:\r\n            # scC[i]: T(bat, Ci_size)\r\n            # 词林的层次分类训练的慢，其实这样不公平，不平衡，因为词预测先收敛了，而cilin的分类还没效果，其他信息的利用也有同样的问题，不一定同时收敛！！！\r\n            scC = [self.fc_C1(h_1), self.fc_C2(h_1), self.fc_C3(h_1)]\r\n            score2 = torch.zeros((score0.shape[0], score0.shape[1]), dtype=torch.float32)\r\n            rank = 0.6\r\n            for i in range(3):\r\n                # wC[i]: T(class_num, Ci_size)\r\n                # C_sc: T(bat, class_num)\r\n                score2 += self.relu(scC[i].mm(wC[i].t())*(rank**i))\r\n            #----------add mean cilin-class score to those who have no cilin-class\r\n            mean_cilin_sc = torch.mean(score2, 1)\r\n            score2 = score2*(1-msk_c) + mean_cilin_sc.unsqueeze(1).mm(msk_c.unsqueeze(0))\r\n            #----------\r\n            score = score + score2/2\r\n        if 'P' in mode:\r\n            ## POS prediction\r\n            # score_POS: T(bat, 13) pos_num=12+1\r\n            score_POS = self.fc2(h_1)\r\n            # s: (class_num, 13) multi-hot\r\n            # weight_sc: T(bat, class_num) = [bat, 13] .mm [class_num, 13].t()\r\n            weight_sc = self.relu(score_POS.mm(wP.t()))\r\n            #print(torch.max(weight_sc), torch.min(weight_sc))\r\n            score = score + weight_sc\r\n        if 's' in mode:\r\n            ## sememe prediction\r\n            # pos_score: T(bat, max_word_num, sememe_num)\r\n            pos_score = self.fc1(h)\r\n            # sem_score: T(bat, sememe_num)\r\n            sem_score, _ = torch.max(pos_score, dim=1)\r\n            # score: T(bat, class_num) = [bat, sememe_num] .mm [class_num, sememe_num].t()\r\n            score1 = self.relu(sem_score.mm(ws.t()))\r\n            #----------add mean sememe score to those who have no sememes\r\n            # mean_sem_sc: T(bat)\r\n            mean_sem_sc = torch.mean(score1, 1)\r\n            # msk: T(class_num)\r\n            score1 = score1 + mean_sem_sc.unsqueeze(1).mm(msk_s.unsqueeze(0))\r\n            #----------\r\n            score = score + score1\r\n        if 'c' in mode:\r\n            ## character prediction\r\n            # pos_score: T(bat, max_word_num, sememe_num)\r\n            pos_score = self.fc3(h)\r\n            # chara_score: T(bat, chara_num)\r\n            chara_score, _ = torch.max(pos_score, dim=1)\r\n            #chara_score = torch.sum(pos_score * alpha, 1)\r\n            # score: T(bat, class_num) = [bat, sememe_num] .mm [class_num, sememe_num].t()\r\n            score3 = self.relu(chara_score.mm(wc.t()))\r\n            score = score + score3\r\n        '''\r\n        if RD_mode ==  'CC':\r\n            # fine-tune depended on the target word shouldn't exist in the definition.\r\n            #score_res = score.clone().detach()\r\n            mask1 = torch.lt(x, self.class_num).to(torch.int64)\r\n            mask2 = torch.ones((score.shape[0], score.shape[1]), dtype=torch.float32)\r\n            for i in range(x.shape[0]):\r\n                mask2[i][x[i]*mask1[i]] = 0.\r\n            score = score * mask2 + (-1e6)*(1-mask2)\r\n        '''\r\n        #_, indices = torch.sort(score, descending=True)\r\n        if operation == 'train':\r\n            loss = self.loss(score, w.long())\r\n            return loss, score, indices\r\n        elif operation == 'test':\r\n            return score #, indices\r\n\r\n"
        },
        {
          "name": "model_en.py",
          "type": "blob",
          "size": 3.7724609375,
          "content": "import torch\n\nclass Encoder(torch.nn.Module):\n    def __init__(self, vocab_size, embed_dim, hidden_dim, layers, class_num, sememe_num, lexname_num, rootaffix_num, encoder):\n        super().__init__()\n        self.vocab_size = vocab_size\n        self.embed_dim = 300\n        self.hidden_dim = 768\n        self.layers = layers\n        self.class_num = class_num\n        self.sememe_num = sememe_num\n        self.lexname_num = lexname_num\n        self.rootaffix_num = rootaffix_num\n        self.embedding = torch.nn.Embedding(self.vocab_size, self.embed_dim, padding_idx=0, max_norm=5, sparse=True)\n        self.embedding.weight.requires_grad = False\n        self.embedding_dropout = torch.nn.Dropout(0.2)\n        self.encoder = encoder\n        self.fc = torch.nn.Linear(self.hidden_dim, self.embed_dim)\n        self.fc_s = torch.nn.Linear(self.hidden_dim, self.sememe_num)\n        self.fc_l = torch.nn.Linear(self.hidden_dim, self.lexname_num)\n        self.fc_r = torch.nn.Linear(self.hidden_dim, self.rootaffix_num)\n        self.loss = torch.nn.CrossEntropyLoss()\n        self.relu = torch.nn.ReLU()\n\n        \n    def forward(self, operation, x=None, w=None, ws=None, wl=None, wr=None, msk_s=None, msk_l=None, msk_r=None, mode=None):\n        # x: T(bat, max_word_num)\n        # w: T(bat)\n        # h: T(bat, max_word_num, 768)\n        attention_mask = torch.gt(x, 0).to(torch.int64)\n        h = self.encoder(x, attention_mask=attention_mask)[0]\n        #h = self.encoder(x)[0]\n        #h = self.embedding_dropout(h)\n        \n        ## word prediction\n        # vd: T(bat, embed_dim)\n        #h_1 = torch.max(h, dim=1)[0]\n        #h_1 = h[:,0,:] # The first token of every sequence is always a special classification token ([CLS]). The final hidden state corresponding to this token is used as the aggregate sequence representation for classification tasks.\n        h_1 = self.embedding_dropout(h[:,0,:])\n        vd = self.fc(h_1)\n        # score0: T(bat, 30000) = [bat, emb] .mm [class_num, emb].t()\n        score0 = vd.mm(self.embedding.weight.data[[range(self.class_num)]].t())\n        # BertVec: 30000, class_num: 50477+2\n        score = score0\n        \n        if 's' in mode:\n            ## sememe prediction\n            # pos_score: T(bat, max_word_num, sememe_num)\n            pos_score = self.fc_s(h)\n            # sem_score: T(bat, sememe_num)\n            sem_score, _ = torch.max(pos_score, dim=1)\n            #sem_score = torch.sum(pos_score * alpha, 1)\n            # score: T(bat, class_num) = [bat, sememe_num] .mm [class_num, sememe_num].t()\n            score_s = self.relu(sem_score.mm(ws.t()))\n            #----------add mean sememe score to those who have no sememes\n            # mean_sem_sc: T(bat)\n            mean_sem_sc = torch.mean(score_s, 1)\n            # msk: T(class_num)\n            score_s = score_s + mean_sem_sc.unsqueeze(1).mm(msk_s.unsqueeze(0))\n            #----------\n            score = score + score_s\n        if 'r' in mode:\n            ## root-affix prediction\n            pos_score_ = self.fc_r(h)\n            ra_score, _ = torch.max(pos_score_, dim=1)\n            score_r = self.relu(ra_score.mm(wr.t()))\n            mean_ra_sc = torch.mean(score_r, 1)\n            score_r = score_r + mean_ra_sc.unsqueeze(1).mm(msk_r.unsqueeze(0))\n            score = score + score_r\n        if 'l' in mode:\n            ## lexname prediction\n            lex_score = self.fc_l(h_1)\n            score_l = self.relu(lex_score.mm(wl.t()))\n            mean_lex_sc = torch.mean(score_l, 1)\n            score_l = score_l + mean_lex_sc.unsqueeze(1).mm(msk_l.unsqueeze(0))\n            score = score + score_l\n        #_, indices = torch.sort(score, descending=True)\n        if operation == 'train':\n            loss = self.loss(score, w)\n            return loss, score, indices\n        elif operation == 'test':\n            return score#, indices\n"
        },
        {
          "name": "resources",
          "type": "tree",
          "content": null
        },
        {
          "name": "static",
          "type": "tree",
          "content": null
        },
        {
          "name": "templates",
          "type": "tree",
          "content": null
        },
        {
          "name": "uwsgi.ini",
          "type": "blob",
          "size": 0.4765625,
          "content": "# website_RD_uwsgi.ini file\n[uwsgi]\n\n# Django-related settings\n\nsocket = 127.0.0.1:8000\n\n# the base directory (full path)\nchdir           = /home/zhanglei/website_RD/\n\n# Django s wsgi file\nmodule          = website_RD.wsgi\n\n# process-related settings\n# master\nmaster          = true\n\n# maximum number of worker processes\nprocesses       = 2\nthreads         = 2\n\n# ... with appropriate permissions - may be needed\n# chmod-socket    = 664\n# clear environment on exit\nvacuum          = true\n"
        },
        {
          "name": "website_RD",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}