{
  "metadata": {
    "timestamp": 1736562750298,
    "page": 290,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjMzMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "carpedm20/DCGAN-tensorflow",
      "stars": 7157,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.9658203125,
          "content": "# Data\ndata\nsamples\n*.zip\nlogs\ntest*\n\nweb/js/gen_layers.js\n\n# checkpoint\ncheckpoint\n\n# trash\n.dropbox\n.DS_Store\n\n# Created by https://www.gitignore.io/api/python,vim\n\n### Python ###\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nenv/\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\n*.egg-info/\n.installed.cfg\n*.egg\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*,cover\n.hypothesis/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n\n### Vim ###\n[._]*.s[a-w][a-z]\n[._]s[a-w][a-z]\n*.un~\nSession.vim\n.netrwhist\n*~\n\n"
        },
        {
          "name": "DCGAN.png",
          "type": "blob",
          "size": 147.0859375,
          "content": null
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.052734375,
          "content": "The MIT License (MIT)\n\nCopyright (c) 2016 Taehoon Kim\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 3.630859375,
          "content": "# DCGAN in Tensorflow\n\nTensorflow implementation of [Deep Convolutional Generative Adversarial Networks](http://arxiv.org/abs/1511.06434) which is a stabilize Generative Adversarial Networks. The referenced torch code can be found [here](https://github.com/soumith/dcgan.torch).\n\n![alt tag](DCGAN.png)\n\n* [Brandon Amos](http://bamos.github.io/) wrote an excellent [blog post](http://bamos.github.io/2016/08/09/deep-completion/) and [image completion code](https://github.com/bamos/dcgan-completion.tensorflow) based on this repo.\n* *To avoid the fast convergence of D (discriminator) network, G (generator) network is updated twice for each D network update, which differs from original paper.*\n\n\n## Online Demo\n\n[<img src=\"https://raw.githubusercontent.com/carpedm20/blog/master/content/images/face.png\">](http://carpedm20.github.io/faces/)\n\n[link](http://carpedm20.github.io/faces/)\n\n\n## Prerequisites\n\n- Python 2.7 or Python 3.3+\n- [Tensorflow 0.12.1](https://github.com/tensorflow/tensorflow/tree/r0.12)\n- [SciPy](http://www.scipy.org/install.html)\n- [pillow](https://github.com/python-pillow/Pillow)\n- [tqdm](https://pypi.org/project/tqdm/)\n- (Optional) [moviepy](https://github.com/Zulko/moviepy) (for visualization)\n- (Optional) [Align&Cropped Images.zip](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html) : Large-scale CelebFaces Dataset\n\n\n## Usage\n\nFirst, download dataset with:\n\n    $ python download.py mnist celebA\n\nTo train a model with downloaded dataset:\n\n    $ python main.py --dataset mnist --input_height=28 --output_height=28 --train\n    $ python main.py --dataset celebA --input_height=108 --train --crop\n\nTo test with an existing model:\n\n    $ python main.py --dataset mnist --input_height=28 --output_height=28\n    $ python main.py --dataset celebA --input_height=108 --crop\n\nOr, you can use your own dataset (without central crop) by:\n\n    $ mkdir data/DATASET_NAME\n    ... add images to data/DATASET_NAME ...\n    $ python main.py --dataset DATASET_NAME --train\n    $ python main.py --dataset DATASET_NAME\n    $ # example\n    $ python main.py --dataset=eyes --input_fname_pattern=\"*_cropped.png\" --train\n\nIf your dataset is located in a different root directory:\n\n    $ python main.py --dataset DATASET_NAME --data_dir DATASET_ROOT_DIR --train\n    $ python main.py --dataset DATASET_NAME --data_dir DATASET_ROOT_DIR\n    $ # example\n    $ python main.py --dataset=eyes --data_dir ../datasets/ --input_fname_pattern=\"*_cropped.png\" --train\n    \n\n## Results\n\n![result](assets/training.gif)\n\n### celebA\n\nAfter 6th epoch:\n\n![result3](assets/result_16_01_04_.png)\n\nAfter 10th epoch:\n\n![result4](assets/test_2016-01-27%2015:08:54.png)\n\n### Asian face dataset\n\n![custom_result1](web/img/change5.png)\n\n![custom_result1](web/img/change2.png)\n\n![custom_result2](web/img/change4.png)\n\n### MNIST\n\nMNIST codes are written by [@PhoenixDai](https://github.com/PhoenixDai).\n\n![mnist_result1](assets/mnist1.png)\n\n![mnist_result2](assets/mnist2.png)\n\n![mnist_result3](assets/mnist3.png)\n\nMore results can be found [here](./assets/) and [here](./web/img/).\n\n\n## Training details\n\nDetails of the loss of Discriminator and Generator (with custom dataset not celebA).\n\n![d_loss](assets/d_loss.png)\n\n![g_loss](assets/g_loss.png)\n\nDetails of the histogram of true and fake result of discriminator (with custom dataset not celebA).\n\n![d_hist](assets/d_hist.png)\n\n![d__hist](assets/d__hist.png)\n\n\n## Related works\n\n- [BEGAN-tensorflow](https://github.com/carpedm20/BEGAN-tensorflow)\n- [DiscoGAN-pytorch](https://github.com/carpedm20/DiscoGAN-pytorch)\n- [simulated-unsupervised-tensorflow](https://github.com/carpedm20/simulated-unsupervised-tensorflow)\n\n\n## Author\n\nTaehoon Kim / [@carpedm20](http://carpedm20.github.io/)\n"
        },
        {
          "name": "assets",
          "type": "tree",
          "content": null
        },
        {
          "name": "average.png",
          "type": "blob",
          "size": 8.4013671875,
          "content": null
        },
        {
          "name": "download.py",
          "type": "blob",
          "size": 5.2490234375,
          "content": "\"\"\"\nModification of https://github.com/stanfordnlp/treelstm/blob/master/scripts/download.py\n\nDownloads the following:\n- Celeb-A dataset\n- LSUN dataset\n- MNIST dataset\n\"\"\"\n\nfrom __future__ import print_function\nimport os\nimport sys\nimport gzip\nimport json\nimport shutil\nimport zipfile\nimport argparse\nimport requests\nimport subprocess\nfrom tqdm import tqdm\nfrom six.moves import urllib\n\nparser = argparse.ArgumentParser(description='Download dataset for DCGAN.')\nparser.add_argument('datasets', metavar='N', type=str, nargs='+', choices=['celebA', 'lsun', 'mnist'],\n           help='name of dataset to download [celebA, lsun, mnist]')\n\ndef download(url, dirpath):\n  filename = url.split('/')[-1]\n  filepath = os.path.join(dirpath, filename)\n  u = urllib.request.urlopen(url)\n  f = open(filepath, 'wb')\n  filesize = int(u.headers[\"Content-Length\"])\n  print(\"Downloading: %s Bytes: %s\" % (filename, filesize))\n\n  downloaded = 0\n  block_sz = 8192\n  status_width = 70\n  while True:\n    buf = u.read(block_sz)\n    if not buf:\n      print('')\n      break\n    else:\n      print('', end='\\r')\n    downloaded += len(buf)\n    f.write(buf)\n    status = ((\"[%-\" + str(status_width + 1) + \"s] %3.2f%%\") %\n      ('=' * int(float(downloaded) / filesize * status_width) + '>', downloaded * 100. / filesize))\n    print(status, end='')\n    sys.stdout.flush()\n  f.close()\n  return filepath\n\ndef download_file_from_google_drive(id, destination):\n  URL = \"https://docs.google.com/uc?export=download\"\n  session = requests.Session()\n\n  response = session.get(URL, params={ 'id': id }, stream=True)\n  token = get_confirm_token(response)\n\n  if token:\n    params = { 'id' : id, 'confirm' : token }\n    response = session.get(URL, params=params, stream=True)\n\n  save_response_content(response, destination)\n\ndef get_confirm_token(response):\n  for key, value in response.cookies.items():\n    if key.startswith('download_warning'):\n      return value\n  return None\n\ndef save_response_content(response, destination, chunk_size=32*1024):\n  total_size = int(response.headers.get('content-length', 0))\n  with open(destination, \"wb\") as f:\n    for chunk in tqdm(response.iter_content(chunk_size), total=total_size,\n              unit='B', unit_scale=True, desc=destination):\n      if chunk: # filter out keep-alive new chunks\n        f.write(chunk)\n\ndef unzip(filepath):\n  print(\"Extracting: \" + filepath)\n  dirpath = os.path.dirname(filepath)\n  with zipfile.ZipFile(filepath) as zf:\n    zf.extractall(dirpath)\n  os.remove(filepath)\n\ndef download_celeb_a(dirpath):\n  data_dir = 'celebA'\n  if os.path.exists(os.path.join(dirpath, data_dir)):\n    print('Found Celeb-A - skip')\n    return\n\n  filename, drive_id  = \"img_align_celeba.zip\", \"0B7EVK8r0v71pZjFTYXZWM3FlRnM\"\n  save_path = os.path.join(dirpath, filename)\n\n  if os.path.exists(save_path):\n    print('[*] {} already exists'.format(save_path))\n  else:\n    download_file_from_google_drive(drive_id, save_path)\n\n  zip_dir = ''\n  with zipfile.ZipFile(save_path) as zf:\n    zip_dir = zf.namelist()[0]\n    zf.extractall(dirpath)\n  os.remove(save_path)\n  os.rename(os.path.join(dirpath, zip_dir), os.path.join(dirpath, data_dir))\n\ndef _list_categories(tag):\n  url = 'http://lsun.cs.princeton.edu/htbin/list.cgi?tag=' + tag\n  f = urllib.request.urlopen(url)\n  return json.loads(f.read())\n\ndef _download_lsun(out_dir, category, set_name, tag):\n  url = 'http://lsun.cs.princeton.edu/htbin/download.cgi?tag={tag}' \\\n      '&category={category}&set={set_name}'.format(**locals())\n  print(url)\n  if set_name == 'test':\n    out_name = 'test_lmdb.zip'\n  else:\n    out_name = '{category}_{set_name}_lmdb.zip'.format(**locals())\n  out_path = os.path.join(out_dir, out_name)\n  cmd = ['curl', url, '-o', out_path]\n  print('Downloading', category, set_name, 'set')\n  subprocess.call(cmd)\n\ndef download_lsun(dirpath):\n  data_dir = os.path.join(dirpath, 'lsun')\n  if os.path.exists(data_dir):\n    print('Found LSUN - skip')\n    return\n  else:\n    os.mkdir(data_dir)\n\n  tag = 'latest'\n  #categories = _list_categories(tag)\n  categories = ['bedroom']\n\n  for category in categories:\n    _download_lsun(data_dir, category, 'train', tag)\n    _download_lsun(data_dir, category, 'val', tag)\n  _download_lsun(data_dir, '', 'test', tag)\n\ndef download_mnist(dirpath):\n  data_dir = os.path.join(dirpath, 'mnist')\n  if os.path.exists(data_dir):\n    print('Found MNIST - skip')\n    return\n  else:\n    os.mkdir(data_dir)\n  url_base = 'http://yann.lecun.com/exdb/mnist/'\n  file_names = ['train-images-idx3-ubyte.gz',\n                'train-labels-idx1-ubyte.gz',\n                't10k-images-idx3-ubyte.gz',\n                't10k-labels-idx1-ubyte.gz']\n  for file_name in file_names:\n    url = (url_base+file_name).format(**locals())\n    print(url)\n    out_path = os.path.join(data_dir,file_name)\n    cmd = ['curl', url, '-o', out_path]\n    print('Downloading ', file_name)\n    subprocess.call(cmd)\n    cmd = ['gzip', '-d', out_path]\n    print('Decompressing ', file_name)\n    subprocess.call(cmd)\n\ndef prepare_data_dir(path = './data'):\n  if not os.path.exists(path):\n    os.mkdir(path)\n\nif __name__ == '__main__':\n  args = parser.parse_args()\n  prepare_data_dir()\n\n  if any(name in args.datasets for name in ['CelebA', 'celebA', 'celebA']):\n    download_celeb_a('./data')\n  if 'lsun' in args.datasets:\n    download_lsun('./data')\n  if 'mnist' in args.datasets:\n    download_mnist('./data')\n"
        },
        {
          "name": "main.py",
          "type": "blob",
          "size": 6.7236328125,
          "content": "import os\nimport scipy.misc\nimport numpy as np\nimport json\n\nfrom model import DCGAN\nfrom utils import pp, visualize, to_json, show_all_variables, expand_path, timestamp\n\nimport tensorflow as tf\n\nflags = tf.app.flags\nflags.DEFINE_integer(\"epoch\", 25, \"Epoch to train [25]\")\nflags.DEFINE_float(\"learning_rate\", 0.0002, \"Learning rate of for adam [0.0002]\")\nflags.DEFINE_float(\"beta1\", 0.5, \"Momentum term of adam [0.5]\")\nflags.DEFINE_float(\"train_size\", np.inf, \"The size of train images [np.inf]\")\nflags.DEFINE_integer(\"batch_size\", 64, \"The size of batch images [64]\")\nflags.DEFINE_integer(\"input_height\", 108, \"The size of image to use (will be center cropped). [108]\")\nflags.DEFINE_integer(\"input_width\", None, \"The size of image to use (will be center cropped). If None, same value as input_height [None]\")\nflags.DEFINE_integer(\"output_height\", 64, \"The size of the output images to produce [64]\")\nflags.DEFINE_integer(\"output_width\", None, \"The size of the output images to produce. If None, same value as output_height [None]\")\nflags.DEFINE_string(\"dataset\", \"celebA\", \"The name of dataset [celebA, mnist, lsun]\")\nflags.DEFINE_string(\"input_fname_pattern\", \"*.jpg\", \"Glob pattern of filename of input images [*]\")\nflags.DEFINE_string(\"data_dir\", \"./data\", \"path to datasets [e.g. $HOME/data]\")\nflags.DEFINE_string(\"out_dir\", \"./out\", \"Root directory for outputs [e.g. $HOME/out]\")\nflags.DEFINE_string(\"out_name\", \"\", \"Folder (under out_root_dir) for all outputs. Generated automatically if left blank []\")\nflags.DEFINE_string(\"checkpoint_dir\", \"checkpoint\", \"Folder (under out_root_dir/out_name) to save checkpoints [checkpoint]\")\nflags.DEFINE_string(\"sample_dir\", \"samples\", \"Folder (under out_root_dir/out_name) to save samples [samples]\")\nflags.DEFINE_boolean(\"train\", False, \"True for training, False for testing [False]\")\nflags.DEFINE_boolean(\"crop\", False, \"True for training, False for testing [False]\")\nflags.DEFINE_boolean(\"visualize\", False, \"True for visualizing, False for nothing [False]\")\nflags.DEFINE_boolean(\"export\", False, \"True for exporting with new batch size\")\nflags.DEFINE_boolean(\"freeze\", False, \"True for exporting with new batch size\")\nflags.DEFINE_integer(\"max_to_keep\", 1, \"maximum number of checkpoints to keep\")\nflags.DEFINE_integer(\"sample_freq\", 200, \"sample every this many iterations\")\nflags.DEFINE_integer(\"ckpt_freq\", 200, \"save checkpoint every this many iterations\")\nflags.DEFINE_integer(\"z_dim\", 100, \"dimensions of z\")\nflags.DEFINE_string(\"z_dist\", \"uniform_signed\", \"'normal01' or 'uniform_unsigned' or uniform_signed\")\nflags.DEFINE_boolean(\"G_img_sum\", False, \"Save generator image summaries in log\")\n#flags.DEFINE_integer(\"generate_test_images\", 100, \"Number of images to generate during test. [100]\")\nFLAGS = flags.FLAGS\n\ndef main(_):\n  pp.pprint(flags.FLAGS.__flags)\n  \n  # expand user name and environment variables\n  FLAGS.data_dir = expand_path(FLAGS.data_dir)\n  FLAGS.out_dir = expand_path(FLAGS.out_dir)\n  FLAGS.out_name = expand_path(FLAGS.out_name)\n  FLAGS.checkpoint_dir = expand_path(FLAGS.checkpoint_dir)\n  FLAGS.sample_dir = expand_path(FLAGS.sample_dir)\n\n  if FLAGS.output_height is None: FLAGS.output_height = FLAGS.input_height\n  if FLAGS.input_width is None: FLAGS.input_width = FLAGS.input_height\n  if FLAGS.output_width is None: FLAGS.output_width = FLAGS.output_height\n\n  # output folders\n  if FLAGS.out_name == \"\":\n      FLAGS.out_name = '{} - {} - {}'.format(timestamp(), FLAGS.data_dir.split('/')[-1], FLAGS.dataset) # penultimate folder of path\n      if FLAGS.train:\n        FLAGS.out_name += ' - x{}.z{}.{}.y{}.b{}'.format(FLAGS.input_width, FLAGS.z_dim, FLAGS.z_dist, FLAGS.output_width, FLAGS.batch_size)\n\n  FLAGS.out_dir = os.path.join(FLAGS.out_dir, FLAGS.out_name)\n  FLAGS.checkpoint_dir = os.path.join(FLAGS.out_dir, FLAGS.checkpoint_dir)\n  FLAGS.sample_dir = os.path.join(FLAGS.out_dir, FLAGS.sample_dir)\n\n  if not os.path.exists(FLAGS.checkpoint_dir): os.makedirs(FLAGS.checkpoint_dir)\n  if not os.path.exists(FLAGS.sample_dir): os.makedirs(FLAGS.sample_dir)\n\n  with open(os.path.join(FLAGS.out_dir, 'FLAGS.json'), 'w') as f:\n    flags_dict = {k:FLAGS[k].value for k in FLAGS}\n    json.dump(flags_dict, f, indent=4, sort_keys=True, ensure_ascii=False)\n  \n\n  #gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\n  run_config = tf.ConfigProto()\n  run_config.gpu_options.allow_growth=True\n\n  with tf.Session(config=run_config) as sess:\n    if FLAGS.dataset == 'mnist':\n      dcgan = DCGAN(\n          sess,\n          input_width=FLAGS.input_width,\n          input_height=FLAGS.input_height,\n          output_width=FLAGS.output_width,\n          output_height=FLAGS.output_height,\n          batch_size=FLAGS.batch_size,\n          sample_num=FLAGS.batch_size,\n          y_dim=10,\n          z_dim=FLAGS.z_dim,\n          dataset_name=FLAGS.dataset,\n          input_fname_pattern=FLAGS.input_fname_pattern,\n          crop=FLAGS.crop,\n          checkpoint_dir=FLAGS.checkpoint_dir,\n          sample_dir=FLAGS.sample_dir,\n          data_dir=FLAGS.data_dir,\n          out_dir=FLAGS.out_dir,\n          max_to_keep=FLAGS.max_to_keep)\n    else:\n      dcgan = DCGAN(\n          sess,\n          input_width=FLAGS.input_width,\n          input_height=FLAGS.input_height,\n          output_width=FLAGS.output_width,\n          output_height=FLAGS.output_height,\n          batch_size=FLAGS.batch_size,\n          sample_num=FLAGS.batch_size,\n          z_dim=FLAGS.z_dim,\n          dataset_name=FLAGS.dataset,\n          input_fname_pattern=FLAGS.input_fname_pattern,\n          crop=FLAGS.crop,\n          checkpoint_dir=FLAGS.checkpoint_dir,\n          sample_dir=FLAGS.sample_dir,\n          data_dir=FLAGS.data_dir,\n          out_dir=FLAGS.out_dir,\n          max_to_keep=FLAGS.max_to_keep)\n\n    show_all_variables()\n\n    if FLAGS.train:\n      dcgan.train(FLAGS)\n    else:\n      load_success, load_counter = dcgan.load(FLAGS.checkpoint_dir)\n      if not load_success:\n        raise Exception(\"Checkpoint not found in \" + FLAGS.checkpoint_dir)\n\n\n    # to_json(\"./web/js/layers.js\", [dcgan.h0_w, dcgan.h0_b, dcgan.g_bn0],\n    #                 [dcgan.h1_w, dcgan.h1_b, dcgan.g_bn1],\n    #                 [dcgan.h2_w, dcgan.h2_b, dcgan.g_bn2],\n    #                 [dcgan.h3_w, dcgan.h3_b, dcgan.g_bn3],\n    #                 [dcgan.h4_w, dcgan.h4_b, None])\n\n    # Below is codes for visualization\n      if FLAGS.export:\n        export_dir = os.path.join(FLAGS.checkpoint_dir, 'export_b'+str(FLAGS.batch_size))\n        dcgan.save(export_dir, load_counter, ckpt=True, frozen=False)\n\n      if FLAGS.freeze:\n        export_dir = os.path.join(FLAGS.checkpoint_dir, 'frozen_b'+str(FLAGS.batch_size))\n        dcgan.save(export_dir, load_counter, ckpt=False, frozen=True)\n\n      if FLAGS.visualize:\n        OPTION = 1\n        visualize(sess, dcgan, FLAGS, OPTION, FLAGS.sample_dir)\n\nif __name__ == '__main__':\n  tf.app.run()\n"
        },
        {
          "name": "model.py",
          "type": "blob",
          "size": 20.892578125,
          "content": "from __future__ import division\nfrom __future__ import print_function\nimport os\nimport time\nimport math\nfrom glob import glob\nimport tensorflow as tf\nimport numpy as np\nfrom six.moves import xrange\n\nfrom ops import *\nfrom utils import *\n\ndef conv_out_size_same(size, stride):\n  return int(math.ceil(float(size) / float(stride)))\n\ndef gen_random(mode, size):\n    if mode=='normal01': return np.random.normal(0,1,size=size)\n    if mode=='uniform_signed': return np.random.uniform(-1,1,size=size)\n    if mode=='uniform_unsigned': return np.random.uniform(0,1,size=size)\n\n\nclass DCGAN(object):\n  def __init__(self, sess, input_height=108, input_width=108, crop=True,\n         batch_size=64, sample_num = 64, output_height=64, output_width=64,\n         y_dim=None, z_dim=100, gf_dim=64, df_dim=64,\n         gfc_dim=1024, dfc_dim=1024, c_dim=3, dataset_name='default',\n         max_to_keep=1,\n         input_fname_pattern='*.jpg', checkpoint_dir='ckpts', sample_dir='samples', out_dir='./out', data_dir='./data'):\n    \"\"\"\n\n    Args:\n      sess: TensorFlow session\n      batch_size: The size of batch. Should be specified before training.\n      y_dim: (optional) Dimension of dim for y. [None]\n      z_dim: (optional) Dimension of dim for Z. [100]\n      gf_dim: (optional) Dimension of gen filters in first conv layer. [64]\n      df_dim: (optional) Dimension of discrim filters in first conv layer. [64]\n      gfc_dim: (optional) Dimension of gen units for for fully connected layer. [1024]\n      dfc_dim: (optional) Dimension of discrim units for fully connected layer. [1024]\n      c_dim: (optional) Dimension of image color. For grayscale input, set to 1. [3]\n    \"\"\"\n    self.sess = sess\n    self.crop = crop\n\n    self.batch_size = batch_size\n    self.sample_num = sample_num\n\n    self.input_height = input_height\n    self.input_width = input_width\n    self.output_height = output_height\n    self.output_width = output_width\n\n    self.y_dim = y_dim\n    self.z_dim = z_dim\n\n    self.gf_dim = gf_dim\n    self.df_dim = df_dim\n\n    self.gfc_dim = gfc_dim\n    self.dfc_dim = dfc_dim\n\n    # batch normalization : deals with poor initialization helps gradient flow\n    self.d_bn1 = batch_norm(name='d_bn1')\n    self.d_bn2 = batch_norm(name='d_bn2')\n\n    if not self.y_dim:\n      self.d_bn3 = batch_norm(name='d_bn3')\n\n    self.g_bn0 = batch_norm(name='g_bn0')\n    self.g_bn1 = batch_norm(name='g_bn1')\n    self.g_bn2 = batch_norm(name='g_bn2')\n\n    if not self.y_dim:\n      self.g_bn3 = batch_norm(name='g_bn3')\n\n    self.dataset_name = dataset_name\n    self.input_fname_pattern = input_fname_pattern\n    self.checkpoint_dir = checkpoint_dir\n    self.data_dir = data_dir\n    self.out_dir = out_dir\n    self.max_to_keep = max_to_keep\n\n    if self.dataset_name == 'mnist':\n      self.data_X, self.data_y = self.load_mnist()\n      self.c_dim = self.data_X[0].shape[-1]\n    else:\n      data_path = os.path.join(self.data_dir, self.dataset_name, self.input_fname_pattern)\n      self.data = glob(data_path)\n      if len(self.data) == 0:\n        raise Exception(\"[!] No data found in '\" + data_path + \"'\")\n      np.random.shuffle(self.data)\n      imreadImg = imread(self.data[0])\n      if len(imreadImg.shape) >= 3: #check if image is a non-grayscale image by checking channel number\n        self.c_dim = imread(self.data[0]).shape[-1]\n      else:\n        self.c_dim = 1\n\n      if len(self.data) < self.batch_size:\n        raise Exception(\"[!] Entire dataset size is less than the configured batch_size\")\n    \n    self.grayscale = (self.c_dim == 1)\n\n    self.build_model()\n\n  def build_model(self):\n    if self.y_dim:\n      self.y = tf.placeholder(tf.float32, [self.batch_size, self.y_dim], name='y')\n    else:\n      self.y = None\n\n    if self.crop:\n      image_dims = [self.output_height, self.output_width, self.c_dim]\n    else:\n      image_dims = [self.input_height, self.input_width, self.c_dim]\n\n    self.inputs = tf.placeholder(\n      tf.float32, [self.batch_size] + image_dims, name='real_images')\n\n    inputs = self.inputs\n\n    self.z = tf.placeholder(\n      tf.float32, [None, self.z_dim], name='z')\n    self.z_sum = histogram_summary(\"z\", self.z)\n\n    self.G                  = self.generator(self.z, self.y)\n    self.D, self.D_logits   = self.discriminator(inputs, self.y, reuse=False)\n    self.sampler            = self.sampler(self.z, self.y)\n    self.D_, self.D_logits_ = self.discriminator(self.G, self.y, reuse=True)\n    \n    self.d_sum = histogram_summary(\"d\", self.D)\n    self.d__sum = histogram_summary(\"d_\", self.D_)\n    self.G_sum = image_summary(\"G\", self.G)\n\n    def sigmoid_cross_entropy_with_logits(x, y):\n      try:\n        return tf.nn.sigmoid_cross_entropy_with_logits(logits=x, labels=y)\n      except:\n        return tf.nn.sigmoid_cross_entropy_with_logits(logits=x, targets=y)\n\n    self.d_loss_real = tf.reduce_mean(\n      sigmoid_cross_entropy_with_logits(self.D_logits, tf.ones_like(self.D)))\n    self.d_loss_fake = tf.reduce_mean(\n      sigmoid_cross_entropy_with_logits(self.D_logits_, tf.zeros_like(self.D_)))\n    self.g_loss = tf.reduce_mean(\n      sigmoid_cross_entropy_with_logits(self.D_logits_, tf.ones_like(self.D_)))\n\n    self.d_loss_real_sum = scalar_summary(\"d_loss_real\", self.d_loss_real)\n    self.d_loss_fake_sum = scalar_summary(\"d_loss_fake\", self.d_loss_fake)\n                          \n    self.d_loss = self.d_loss_real + self.d_loss_fake\n\n    self.g_loss_sum = scalar_summary(\"g_loss\", self.g_loss)\n    self.d_loss_sum = scalar_summary(\"d_loss\", self.d_loss)\n\n    t_vars = tf.trainable_variables()\n\n    self.d_vars = [var for var in t_vars if 'd_' in var.name]\n    self.g_vars = [var for var in t_vars if 'g_' in var.name]\n\n    self.saver = tf.train.Saver(max_to_keep=self.max_to_keep)\n\n  def train(self, config):\n    d_optim = tf.train.AdamOptimizer(config.learning_rate, beta1=config.beta1) \\\n              .minimize(self.d_loss, var_list=self.d_vars)\n    g_optim = tf.train.AdamOptimizer(config.learning_rate, beta1=config.beta1) \\\n              .minimize(self.g_loss, var_list=self.g_vars)\n    try:\n      tf.global_variables_initializer().run()\n    except:\n      tf.initialize_all_variables().run()\n\n    if config.G_img_sum:\n      self.g_sum = merge_summary([self.z_sum, self.d__sum, self.G_sum, self.d_loss_fake_sum, self.g_loss_sum])\n    else:\n      self.g_sum = merge_summary([self.z_sum, self.d__sum, self.d_loss_fake_sum, self.g_loss_sum])\n    self.d_sum = merge_summary(\n        [self.z_sum, self.d_sum, self.d_loss_real_sum, self.d_loss_sum])\n    self.writer = SummaryWriter(os.path.join(self.out_dir, \"logs\"), self.sess.graph)\n\n    sample_z = gen_random(config.z_dist, size=(self.sample_num , self.z_dim))\n    \n    if config.dataset == 'mnist':\n      sample_inputs = self.data_X[0:self.sample_num]\n      sample_labels = self.data_y[0:self.sample_num]\n    else:\n      sample_files = self.data[0:self.sample_num]\n      sample = [\n          get_image(sample_file,\n                    input_height=self.input_height,\n                    input_width=self.input_width,\n                    resize_height=self.output_height,\n                    resize_width=self.output_width,\n                    crop=self.crop,\n                    grayscale=self.grayscale) for sample_file in sample_files]\n      if (self.grayscale):\n        sample_inputs = np.array(sample).astype(np.float32)[:, :, :, None]\n      else:\n        sample_inputs = np.array(sample).astype(np.float32)\n  \n    counter = 1\n    start_time = time.time()\n    could_load, checkpoint_counter = self.load(self.checkpoint_dir)\n    if could_load:\n      counter = checkpoint_counter\n      print(\" [*] Load SUCCESS\")\n    else:\n      print(\" [!] Load failed...\")\n\n    for epoch in xrange(config.epoch):\n      if config.dataset == 'mnist':\n        batch_idxs = min(len(self.data_X), config.train_size) // config.batch_size\n      else:      \n        self.data = glob(os.path.join(\n          config.data_dir, config.dataset, self.input_fname_pattern))\n        np.random.shuffle(self.data)\n        batch_idxs = min(len(self.data), config.train_size) // config.batch_size\n\n      for idx in xrange(0, int(batch_idxs)):\n        if config.dataset == 'mnist':\n          batch_images = self.data_X[idx*config.batch_size:(idx+1)*config.batch_size]\n          batch_labels = self.data_y[idx*config.batch_size:(idx+1)*config.batch_size]\n        else:\n          batch_files = self.data[idx*config.batch_size:(idx+1)*config.batch_size]\n          batch = [\n              get_image(batch_file,\n                        input_height=self.input_height,\n                        input_width=self.input_width,\n                        resize_height=self.output_height,\n                        resize_width=self.output_width,\n                        crop=self.crop,\n                        grayscale=self.grayscale) for batch_file in batch_files]\n          if self.grayscale:\n            batch_images = np.array(batch).astype(np.float32)[:, :, :, None]\n          else:\n            batch_images = np.array(batch).astype(np.float32)\n\n        batch_z = gen_random(config.z_dist, size=[config.batch_size, self.z_dim]) \\\n              .astype(np.float32)\n\n        if config.dataset == 'mnist':\n          # Update D network\n          _, summary_str = self.sess.run([d_optim, self.d_sum],\n            feed_dict={ \n              self.inputs: batch_images,\n              self.z: batch_z,\n              self.y:batch_labels,\n            })\n          self.writer.add_summary(summary_str, counter)\n\n          # Update G network\n          _, summary_str = self.sess.run([g_optim, self.g_sum],\n            feed_dict={\n              self.z: batch_z, \n              self.y:batch_labels,\n            })\n          self.writer.add_summary(summary_str, counter)\n\n          # Run g_optim twice to make sure that d_loss does not go to zero (different from paper)\n          _, summary_str = self.sess.run([g_optim, self.g_sum],\n            feed_dict={ self.z: batch_z, self.y:batch_labels })\n          self.writer.add_summary(summary_str, counter)\n          \n          errD_fake = self.d_loss_fake.eval({\n              self.z: batch_z, \n              self.y:batch_labels\n          })\n          errD_real = self.d_loss_real.eval({\n              self.inputs: batch_images,\n              self.y:batch_labels\n          })\n          errG = self.g_loss.eval({\n              self.z: batch_z,\n              self.y: batch_labels\n          })\n        else:\n          # Update D network\n          _, summary_str = self.sess.run([d_optim, self.d_sum],\n            feed_dict={ self.inputs: batch_images, self.z: batch_z })\n          self.writer.add_summary(summary_str, counter)\n\n          # Update G network\n          _, summary_str = self.sess.run([g_optim, self.g_sum],\n            feed_dict={ self.z: batch_z })\n          self.writer.add_summary(summary_str, counter)\n\n          # Run g_optim twice to make sure that d_loss does not go to zero (different from paper)\n          _, summary_str = self.sess.run([g_optim, self.g_sum],\n            feed_dict={ self.z: batch_z })\n          self.writer.add_summary(summary_str, counter)\n          \n          errD_fake = self.d_loss_fake.eval({ self.z: batch_z })\n          errD_real = self.d_loss_real.eval({ self.inputs: batch_images })\n          errG = self.g_loss.eval({self.z: batch_z})\n\n        print(\"[%8d Epoch:[%2d/%2d] [%4d/%4d] time: %4.4f, d_loss: %.8f, g_loss: %.8f\" \\\n          % (counter, epoch, config.epoch, idx, batch_idxs,\n            time.time() - start_time, errD_fake+errD_real, errG))\n\n        if np.mod(counter, config.sample_freq) == 0:\n          if config.dataset == 'mnist':\n            samples, d_loss, g_loss = self.sess.run(\n              [self.sampler, self.d_loss, self.g_loss],\n              feed_dict={\n                  self.z: sample_z,\n                  self.inputs: sample_inputs,\n                  self.y:sample_labels,\n              }\n            )\n            save_images(samples, image_manifold_size(samples.shape[0]),\n                  './{}/train_{:08d}.png'.format(config.sample_dir, counter))\n            print(\"[Sample] d_loss: %.8f, g_loss: %.8f\" % (d_loss, g_loss)) \n          else:\n            try:\n              samples, d_loss, g_loss = self.sess.run(\n                [self.sampler, self.d_loss, self.g_loss],\n                feed_dict={\n                    self.z: sample_z,\n                    self.inputs: sample_inputs,\n                },\n              )\n              save_images(samples, image_manifold_size(samples.shape[0]),\n                    './{}/train_{:08d}.png'.format(config.sample_dir, counter))\n              print(\"[Sample] d_loss: %.8f, g_loss: %.8f\" % (d_loss, g_loss)) \n            except:\n              print(\"one pic error!...\")\n\n        if np.mod(counter, config.ckpt_freq) == 0:\n          self.save(config.checkpoint_dir, counter)\n        \n        counter += 1\n        \n  def discriminator(self, image, y=None, reuse=False):\n    with tf.variable_scope(\"discriminator\") as scope:\n      if reuse:\n        scope.reuse_variables()\n\n      if not self.y_dim:\n        h0 = lrelu(conv2d(image, self.df_dim, name='d_h0_conv'))\n        h1 = lrelu(self.d_bn1(conv2d(h0, self.df_dim*2, name='d_h1_conv')))\n        h2 = lrelu(self.d_bn2(conv2d(h1, self.df_dim*4, name='d_h2_conv')))\n        h3 = lrelu(self.d_bn3(conv2d(h2, self.df_dim*8, name='d_h3_conv')))\n        h4 = linear(tf.reshape(h3, [self.batch_size, -1]), 1, 'd_h4_lin')\n\n        return tf.nn.sigmoid(h4), h4\n      else:\n        yb = tf.reshape(y, [self.batch_size, 1, 1, self.y_dim])\n        x = conv_cond_concat(image, yb)\n\n        h0 = lrelu(conv2d(x, self.c_dim + self.y_dim, name='d_h0_conv'))\n        h0 = conv_cond_concat(h0, yb)\n\n        h1 = lrelu(self.d_bn1(conv2d(h0, self.df_dim + self.y_dim, name='d_h1_conv')))\n        h1 = tf.reshape(h1, [self.batch_size, -1])      \n        h1 = concat([h1, y], 1)\n        \n        h2 = lrelu(self.d_bn2(linear(h1, self.dfc_dim, 'd_h2_lin')))\n        h2 = concat([h2, y], 1)\n\n        h3 = linear(h2, 1, 'd_h3_lin')\n        \n        return tf.nn.sigmoid(h3), h3\n\n  def generator(self, z, y=None):\n    with tf.variable_scope(\"generator\") as scope:\n      if not self.y_dim:\n        s_h, s_w = self.output_height, self.output_width\n        s_h2, s_w2 = conv_out_size_same(s_h, 2), conv_out_size_same(s_w, 2)\n        s_h4, s_w4 = conv_out_size_same(s_h2, 2), conv_out_size_same(s_w2, 2)\n        s_h8, s_w8 = conv_out_size_same(s_h4, 2), conv_out_size_same(s_w4, 2)\n        s_h16, s_w16 = conv_out_size_same(s_h8, 2), conv_out_size_same(s_w8, 2)\n\n        # project `z` and reshape\n        self.z_, self.h0_w, self.h0_b = linear(\n            z, self.gf_dim*8*s_h16*s_w16, 'g_h0_lin', with_w=True)\n\n        self.h0 = tf.reshape(\n            self.z_, [-1, s_h16, s_w16, self.gf_dim * 8])\n        h0 = tf.nn.relu(self.g_bn0(self.h0))\n\n        self.h1, self.h1_w, self.h1_b = deconv2d(\n            h0, [self.batch_size, s_h8, s_w8, self.gf_dim*4], name='g_h1', with_w=True)\n        h1 = tf.nn.relu(self.g_bn1(self.h1))\n\n        h2, self.h2_w, self.h2_b = deconv2d(\n            h1, [self.batch_size, s_h4, s_w4, self.gf_dim*2], name='g_h2', with_w=True)\n        h2 = tf.nn.relu(self.g_bn2(h2))\n\n        h3, self.h3_w, self.h3_b = deconv2d(\n            h2, [self.batch_size, s_h2, s_w2, self.gf_dim*1], name='g_h3', with_w=True)\n        h3 = tf.nn.relu(self.g_bn3(h3))\n\n        h4, self.h4_w, self.h4_b = deconv2d(\n            h3, [self.batch_size, s_h, s_w, self.c_dim], name='g_h4', with_w=True)\n\n        return tf.nn.tanh(h4)\n      else:\n        s_h, s_w = self.output_height, self.output_width\n        s_h2, s_h4 = int(s_h/2), int(s_h/4)\n        s_w2, s_w4 = int(s_w/2), int(s_w/4)\n\n        # yb = tf.expand_dims(tf.expand_dims(y, 1),2)\n        yb = tf.reshape(y, [self.batch_size, 1, 1, self.y_dim])\n        z = concat([z, y], 1)\n\n        h0 = tf.nn.relu(\n            self.g_bn0(linear(z, self.gfc_dim, 'g_h0_lin')))\n        h0 = concat([h0, y], 1)\n\n        h1 = tf.nn.relu(self.g_bn1(\n            linear(h0, self.gf_dim*2*s_h4*s_w4, 'g_h1_lin')))\n        h1 = tf.reshape(h1, [self.batch_size, s_h4, s_w4, self.gf_dim * 2])\n\n        h1 = conv_cond_concat(h1, yb)\n\n        h2 = tf.nn.relu(self.g_bn2(deconv2d(h1,\n            [self.batch_size, s_h2, s_w2, self.gf_dim * 2], name='g_h2')))\n        h2 = conv_cond_concat(h2, yb)\n\n        return tf.nn.sigmoid(\n            deconv2d(h2, [self.batch_size, s_h, s_w, self.c_dim], name='g_h3'))\n\n  def sampler(self, z, y=None):\n    with tf.variable_scope(\"generator\") as scope:\n      scope.reuse_variables()\n\n      if not self.y_dim:\n        s_h, s_w = self.output_height, self.output_width\n        s_h2, s_w2 = conv_out_size_same(s_h, 2), conv_out_size_same(s_w, 2)\n        s_h4, s_w4 = conv_out_size_same(s_h2, 2), conv_out_size_same(s_w2, 2)\n        s_h8, s_w8 = conv_out_size_same(s_h4, 2), conv_out_size_same(s_w4, 2)\n        s_h16, s_w16 = conv_out_size_same(s_h8, 2), conv_out_size_same(s_w8, 2)\n\n        # project `z` and reshape\n        h0 = tf.reshape(\n            linear(z, self.gf_dim*8*s_h16*s_w16, 'g_h0_lin'),\n            [-1, s_h16, s_w16, self.gf_dim * 8])\n        h0 = tf.nn.relu(self.g_bn0(h0, train=False))\n\n        h1 = deconv2d(h0, [self.batch_size, s_h8, s_w8, self.gf_dim*4], name='g_h1')\n        h1 = tf.nn.relu(self.g_bn1(h1, train=False))\n\n        h2 = deconv2d(h1, [self.batch_size, s_h4, s_w4, self.gf_dim*2], name='g_h2')\n        h2 = tf.nn.relu(self.g_bn2(h2, train=False))\n\n        h3 = deconv2d(h2, [self.batch_size, s_h2, s_w2, self.gf_dim*1], name='g_h3')\n        h3 = tf.nn.relu(self.g_bn3(h3, train=False))\n\n        h4 = deconv2d(h3, [self.batch_size, s_h, s_w, self.c_dim], name='g_h4')\n\n        return tf.nn.tanh(h4)\n      else:\n        s_h, s_w = self.output_height, self.output_width\n        s_h2, s_h4 = int(s_h/2), int(s_h/4)\n        s_w2, s_w4 = int(s_w/2), int(s_w/4)\n\n        # yb = tf.reshape(y, [-1, 1, 1, self.y_dim])\n        yb = tf.reshape(y, [self.batch_size, 1, 1, self.y_dim])\n        z = concat([z, y], 1)\n\n        h0 = tf.nn.relu(self.g_bn0(linear(z, self.gfc_dim, 'g_h0_lin'), train=False))\n        h0 = concat([h0, y], 1)\n\n        h1 = tf.nn.relu(self.g_bn1(\n            linear(h0, self.gf_dim*2*s_h4*s_w4, 'g_h1_lin'), train=False))\n        h1 = tf.reshape(h1, [self.batch_size, s_h4, s_w4, self.gf_dim * 2])\n        h1 = conv_cond_concat(h1, yb)\n\n        h2 = tf.nn.relu(self.g_bn2(\n            deconv2d(h1, [self.batch_size, s_h2, s_w2, self.gf_dim * 2], name='g_h2'), train=False))\n        h2 = conv_cond_concat(h2, yb)\n\n        return tf.nn.sigmoid(deconv2d(h2, [self.batch_size, s_h, s_w, self.c_dim], name='g_h3'))\n\n  def load_mnist(self):\n    data_dir = os.path.join(self.data_dir, self.dataset_name)\n    \n    fd = open(os.path.join(data_dir,'train-images-idx3-ubyte'))\n    loaded = np.fromfile(file=fd,dtype=np.uint8)\n    trX = loaded[16:].reshape((60000,28,28,1)).astype(np.float)\n\n    fd = open(os.path.join(data_dir,'train-labels-idx1-ubyte'))\n    loaded = np.fromfile(file=fd,dtype=np.uint8)\n    trY = loaded[8:].reshape((60000)).astype(np.float)\n\n    fd = open(os.path.join(data_dir,'t10k-images-idx3-ubyte'))\n    loaded = np.fromfile(file=fd,dtype=np.uint8)\n    teX = loaded[16:].reshape((10000,28,28,1)).astype(np.float)\n\n    fd = open(os.path.join(data_dir,'t10k-labels-idx1-ubyte'))\n    loaded = np.fromfile(file=fd,dtype=np.uint8)\n    teY = loaded[8:].reshape((10000)).astype(np.float)\n\n    trY = np.asarray(trY)\n    teY = np.asarray(teY)\n    \n    X = np.concatenate((trX, teX), axis=0)\n    y = np.concatenate((trY, teY), axis=0).astype(np.int)\n    \n    seed = 547\n    np.random.seed(seed)\n    np.random.shuffle(X)\n    np.random.seed(seed)\n    np.random.shuffle(y)\n    \n    y_vec = np.zeros((len(y), self.y_dim), dtype=np.float)\n    for i, label in enumerate(y):\n      y_vec[i,y[i]] = 1.0\n    \n    return X/255.,y_vec\n\n  @property\n  def model_dir(self):\n    return \"{}_{}_{}_{}\".format(\n        self.dataset_name, self.batch_size,\n        self.output_height, self.output_width)\n\n  def save(self, checkpoint_dir, step, filename='model', ckpt=True, frozen=False):\n    # model_name = \"DCGAN.model\"\n    # checkpoint_dir = os.path.join(checkpoint_dir, self.model_dir)\n\n    filename += '.b' + str(self.batch_size)\n    if not os.path.exists(checkpoint_dir):\n      os.makedirs(checkpoint_dir)\n\n    if ckpt:\n      self.saver.save(self.sess,\n              os.path.join(checkpoint_dir, filename),\n              global_step=step)\n\n    if frozen:\n      tf.train.write_graph(\n              tf.graph_util.convert_variables_to_constants(self.sess, self.sess.graph_def, [\"generator_1/Tanh\"]),\n              checkpoint_dir,\n              '{}-{:06d}_frz.pb'.format(filename, step),\n              as_text=False)\n\n  def load(self, checkpoint_dir):\n    #import re\n    print(\" [*] Reading checkpoints...\", checkpoint_dir)\n    # checkpoint_dir = os.path.join(checkpoint_dir, self.model_dir)\n    # print(\"     ->\", checkpoint_dir)\n\n    ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n    if ckpt and ckpt.model_checkpoint_path:\n      ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n      self.saver.restore(self.sess, os.path.join(checkpoint_dir, ckpt_name))\n      #counter = int(next(re.finditer(\"(\\d+)(?!.*\\d)\",ckpt_name)).group(0))\n      counter = int(ckpt_name.split('-')[-1])\n      print(\" [*] Success to read {}\".format(ckpt_name))\n      return True, counter\n    else:\n      print(\" [*] Failed to find a checkpoint\")\n      return False, 0\n"
        },
        {
          "name": "ops.py",
          "type": "blob",
          "size": 3.8125,
          "content": "import math\nimport numpy as np \nimport tensorflow as tf\n\nfrom tensorflow.python.framework import ops\n\nfrom utils import *\n\ntry:\n  image_summary = tf.image_summary\n  scalar_summary = tf.scalar_summary\n  histogram_summary = tf.histogram_summary\n  merge_summary = tf.merge_summary\n  SummaryWriter = tf.train.SummaryWriter\nexcept:\n  image_summary = tf.summary.image\n  scalar_summary = tf.summary.scalar\n  histogram_summary = tf.summary.histogram\n  merge_summary = tf.summary.merge\n  SummaryWriter = tf.summary.FileWriter\n\nif \"concat_v2\" in dir(tf):\n  def concat(tensors, axis, *args, **kwargs):\n    return tf.concat_v2(tensors, axis, *args, **kwargs)\nelse:\n  def concat(tensors, axis, *args, **kwargs):\n    return tf.concat(tensors, axis, *args, **kwargs)\n\nclass batch_norm(object):\n  def __init__(self, epsilon=1e-5, momentum = 0.9, name=\"batch_norm\"):\n    with tf.variable_scope(name):\n      self.epsilon  = epsilon\n      self.momentum = momentum\n      self.name = name\n\n  def __call__(self, x, train=True):\n    return tf.contrib.layers.batch_norm(x,\n                      decay=self.momentum, \n                      updates_collections=None,\n                      epsilon=self.epsilon,\n                      scale=True,\n                      is_training=train,\n                      scope=self.name)\n\ndef conv_cond_concat(x, y):\n  \"\"\"Concatenate conditioning vector on feature map axis.\"\"\"\n  x_shapes = x.get_shape()\n  y_shapes = y.get_shape()\n  return concat([\n    x, y*tf.ones([x_shapes[0], x_shapes[1], x_shapes[2], y_shapes[3]])], 3)\n\ndef conv2d(input_, output_dim, \n       k_h=5, k_w=5, d_h=2, d_w=2, stddev=0.02,\n       name=\"conv2d\"):\n  with tf.variable_scope(name):\n    w = tf.get_variable('w', [k_h, k_w, input_.get_shape()[-1], output_dim],\n              initializer=tf.truncated_normal_initializer(stddev=stddev))\n    conv = tf.nn.conv2d(input_, w, strides=[1, d_h, d_w, 1], padding='SAME')\n\n    biases = tf.get_variable('biases', [output_dim], initializer=tf.constant_initializer(0.0))\n    conv = tf.reshape(tf.nn.bias_add(conv, biases), conv.get_shape())\n\n    return conv\n\ndef deconv2d(input_, output_shape,\n       k_h=5, k_w=5, d_h=2, d_w=2, stddev=0.02,\n       name=\"deconv2d\", with_w=False):\n  with tf.variable_scope(name):\n    # filter : [height, width, output_channels, in_channels]\n    w = tf.get_variable('w', [k_h, k_w, output_shape[-1], input_.get_shape()[-1]],\n              initializer=tf.random_normal_initializer(stddev=stddev))\n    \n    try:\n      deconv = tf.nn.conv2d_transpose(input_, w, output_shape=output_shape,\n                strides=[1, d_h, d_w, 1])\n\n    # Support for verisons of TensorFlow before 0.7.0\n    except AttributeError:\n      deconv = tf.nn.deconv2d(input_, w, output_shape=output_shape,\n                strides=[1, d_h, d_w, 1])\n\n    biases = tf.get_variable('biases', [output_shape[-1]], initializer=tf.constant_initializer(0.0))\n    deconv = tf.reshape(tf.nn.bias_add(deconv, biases), deconv.get_shape())\n\n    if with_w:\n      return deconv, w, biases\n    else:\n      return deconv\n     \ndef lrelu(x, leak=0.2, name=\"lrelu\"):\n  return tf.maximum(x, leak*x)\n\ndef linear(input_, output_size, scope=None, stddev=0.02, bias_start=0.0, with_w=False):\n  shape = input_.get_shape().as_list()\n\n  with tf.variable_scope(scope or \"Linear\"):\n    try:\n      matrix = tf.get_variable(\"Matrix\", [shape[1], output_size], tf.float32,\n                 tf.random_normal_initializer(stddev=stddev))\n    except ValueError as err:\n        msg = \"NOTE: Usually, this is due to an issue with the image dimensions.  Did you correctly set '--crop' or '--input_height' or '--output_height'?\"\n        err.args = err.args + (msg,)\n        raise\n    bias = tf.get_variable(\"bias\", [output_size],\n      initializer=tf.constant_initializer(bias_start))\n    if with_w:\n      return tf.matmul(input_, matrix) + bias, matrix, bias\n    else:\n      return tf.matmul(input_, matrix) + bias\n"
        },
        {
          "name": "utils.py",
          "type": "blob",
          "size": 9.1435546875,
          "content": "\"\"\"\nSome codes from https://github.com/Newmu/dcgan_code\n\"\"\"\nfrom __future__ import division\nimport math\nimport json\nimport random\nimport pprint\nimport scipy.misc\nimport cv2\nimport numpy as np\nimport os\nimport time\nimport datetime\nfrom time import gmtime, strftime\nfrom six.moves import xrange\nfrom PIL import Image\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\npp = pprint.PrettyPrinter()\n\nget_stddev = lambda x, k_h, k_w: 1/math.sqrt(k_w*k_h*x.get_shape()[-1])\n\n\ndef expand_path(path):\n  return os.path.expanduser(os.path.expandvars(path))\n\ndef timestamp(s='%Y%m%d.%H%M%S', ts=None):\n  if not ts: ts = time.time()\n  st = datetime.datetime.fromtimestamp(ts).strftime(s)\n  return st\n  \ndef show_all_variables():\n  model_vars = tf.trainable_variables()\n  slim.model_analyzer.analyze_vars(model_vars, print_info=True)\n\ndef get_image(image_path, input_height, input_width,\n              resize_height=64, resize_width=64,\n              crop=True, grayscale=False):\n  image = imread(image_path, grayscale)\n  return transform(image, input_height, input_width,\n                   resize_height, resize_width, crop)\n\ndef save_images(images, size, image_path):\n  return imsave(inverse_transform(images), size, image_path)\n\ndef imread(path, grayscale = False):\n  if (grayscale):\n    return scipy.misc.imread(path, flatten = True).astype(np.float)\n  else:\n    # Reference: https://github.com/carpedm20/DCGAN-tensorflow/issues/162#issuecomment-315519747\n    img_bgr = cv2.imread(path)\n    # Reference: https://stackoverflow.com/a/15074748/\n    img_rgb = img_bgr[..., ::-1]\n    return img_rgb.astype(np.float)\n\ndef merge_images(images, size):\n  return inverse_transform(images)\n\ndef merge(images, size):\n  h, w = images.shape[1], images.shape[2]\n  if (images.shape[3] in (3,4)):\n    c = images.shape[3]\n    img = np.zeros((h * size[0], w * size[1], c))\n    for idx, image in enumerate(images):\n      i = idx % size[1]\n      j = idx // size[1]\n      img[j * h:j * h + h, i * w:i * w + w, :] = image\n    return img\n  elif images.shape[3]==1:\n    img = np.zeros((h * size[0], w * size[1]))\n    for idx, image in enumerate(images):\n      i = idx % size[1]\n      j = idx // size[1]\n      img[j * h:j * h + h, i * w:i * w + w] = image[:,:,0]\n    return img\n  else:\n    raise ValueError('in merge(images,size) images parameter '\n                     'must have dimensions: HxW or HxWx3 or HxWx4')\n\ndef imsave(images, size, path):\n  image = np.squeeze(merge(images, size))\n  return scipy.misc.imsave(path, image)\n\ndef center_crop(x, crop_h, crop_w,\n                resize_h=64, resize_w=64):\n  if crop_w is None:\n    crop_w = crop_h\n  h, w = x.shape[:2]\n  j = int(round((h - crop_h)/2.))\n  i = int(round((w - crop_w)/2.))\n  im = Image.fromarray(x[j:j+crop_h, i:i+crop_w])\n  return np.array(im.resize([resize_h, resize_w]), PIL.Image.BILINEAR)\n\ndef transform(image, input_height, input_width, \n              resize_height=64, resize_width=64, crop=True):\n  if crop:\n    cropped_image = center_crop(\n      image, input_height, input_width, \n      resize_height, resize_width)\n  else:\n    im = Image.fromarray(image[j:j+crop_h, i:i+crop_w])\n  return np.array(im.resize([resize_h, resize_w]), PIL.Image.BILINEAR)/127.5 - 1.\n\ndef inverse_transform(images):\n  return (images+1.)/2.\n\ndef to_json(output_path, *layers):\n  with open(output_path, \"w\") as layer_f:\n    lines = \"\"\n    for w, b, bn in layers:\n      layer_idx = w.name.split('/')[0].split('h')[1]\n\n      B = b.eval()\n\n      if \"lin/\" in w.name:\n        W = w.eval()\n        depth = W.shape[1]\n      else:\n        W = np.rollaxis(w.eval(), 2, 0)\n        depth = W.shape[0]\n\n      biases = {\"sy\": 1, \"sx\": 1, \"depth\": depth, \"w\": ['%.2f' % elem for elem in list(B)]}\n      if bn != None:\n        gamma = bn.gamma.eval()\n        beta = bn.beta.eval()\n\n        gamma = {\"sy\": 1, \"sx\": 1, \"depth\": depth, \"w\": ['%.2f' % elem for elem in list(gamma)]}\n        beta = {\"sy\": 1, \"sx\": 1, \"depth\": depth, \"w\": ['%.2f' % elem for elem in list(beta)]}\n      else:\n        gamma = {\"sy\": 1, \"sx\": 1, \"depth\": 0, \"w\": []}\n        beta = {\"sy\": 1, \"sx\": 1, \"depth\": 0, \"w\": []}\n\n      if \"lin/\" in w.name:\n        fs = []\n        for w in W.T:\n          fs.append({\"sy\": 1, \"sx\": 1, \"depth\": W.shape[0], \"w\": ['%.2f' % elem for elem in list(w)]})\n\n        lines += \"\"\"\n          var layer_%s = {\n            \"layer_type\": \"fc\", \n            \"sy\": 1, \"sx\": 1, \n            \"out_sx\": 1, \"out_sy\": 1,\n            \"stride\": 1, \"pad\": 0,\n            \"out_depth\": %s, \"in_depth\": %s,\n            \"biases\": %s,\n            \"gamma\": %s,\n            \"beta\": %s,\n            \"filters\": %s\n          };\"\"\" % (layer_idx.split('_')[0], W.shape[1], W.shape[0], biases, gamma, beta, fs)\n      else:\n        fs = []\n        for w_ in W:\n          fs.append({\"sy\": 5, \"sx\": 5, \"depth\": W.shape[3], \"w\": ['%.2f' % elem for elem in list(w_.flatten())]})\n\n        lines += \"\"\"\n          var layer_%s = {\n            \"layer_type\": \"deconv\", \n            \"sy\": 5, \"sx\": 5,\n            \"out_sx\": %s, \"out_sy\": %s,\n            \"stride\": 2, \"pad\": 1,\n            \"out_depth\": %s, \"in_depth\": %s,\n            \"biases\": %s,\n            \"gamma\": %s,\n            \"beta\": %s,\n            \"filters\": %s\n          };\"\"\" % (layer_idx, 2**(int(layer_idx)+2), 2**(int(layer_idx)+2),\n               W.shape[0], W.shape[3], biases, gamma, beta, fs)\n    layer_f.write(\" \".join(lines.replace(\"'\",\"\").split()))\n\ndef make_gif(images, fname, duration=2, true_image=False):\n  import moviepy.editor as mpy\n\n  def make_frame(t):\n    try:\n      x = images[int(len(images)/duration*t)]\n    except:\n      x = images[-1]\n\n    if true_image:\n      return x.astype(np.uint8)\n    else:\n      return ((x+1)/2*255).astype(np.uint8)\n\n  clip = mpy.VideoClip(make_frame, duration=duration)\n  clip.write_gif(fname, fps = len(images) / duration)\n\ndef visualize(sess, dcgan, config, option, sample_dir='samples'):\n  image_frame_dim = int(math.ceil(config.batch_size**.5))\n  if option == 0:\n    z_sample = np.random.uniform(-0.5, 0.5, size=(config.batch_size, dcgan.z_dim))\n    samples = sess.run(dcgan.sampler, feed_dict={dcgan.z: z_sample})\n    save_images(samples, [image_frame_dim, image_frame_dim], os.path.join(sample_dir, 'test_%s.png' % strftime(\"%Y%m%d%H%M%S\", gmtime() )))\n  elif option == 1:\n    values = np.arange(0, 1, 1./config.batch_size)\n    for idx in xrange(dcgan.z_dim):\n      print(\" [*] %d\" % idx)\n      z_sample = np.random.uniform(-1, 1, size=(config.batch_size , dcgan.z_dim))\n      for kdx, z in enumerate(z_sample):\n        z[idx] = values[kdx]\n\n      if config.dataset == \"mnist\":\n        y = np.random.choice(10, config.batch_size)\n        y_one_hot = np.zeros((config.batch_size, 10))\n        y_one_hot[np.arange(config.batch_size), y] = 1\n\n        samples = sess.run(dcgan.sampler, feed_dict={dcgan.z: z_sample, dcgan.y: y_one_hot})\n      else:\n        samples = sess.run(dcgan.sampler, feed_dict={dcgan.z: z_sample})\n\n      save_images(samples, [image_frame_dim, image_frame_dim], os.path.join(sample_dir, 'test_arange_%s.png' % (idx)))\n  elif option == 2:\n    values = np.arange(0, 1, 1./config.batch_size)\n    for idx in [random.randint(0, dcgan.z_dim - 1) for _ in xrange(dcgan.z_dim)]:\n      print(\" [*] %d\" % idx)\n      z = np.random.uniform(-0.2, 0.2, size=(dcgan.z_dim))\n      z_sample = np.tile(z, (config.batch_size, 1))\n      #z_sample = np.zeros([config.batch_size, dcgan.z_dim])\n      for kdx, z in enumerate(z_sample):\n        z[idx] = values[kdx]\n\n      if config.dataset == \"mnist\":\n        y = np.random.choice(10, config.batch_size)\n        y_one_hot = np.zeros((config.batch_size, 10))\n        y_one_hot[np.arange(config.batch_size), y] = 1\n\n        samples = sess.run(dcgan.sampler, feed_dict={dcgan.z: z_sample, dcgan.y: y_one_hot})\n      else:\n        samples = sess.run(dcgan.sampler, feed_dict={dcgan.z: z_sample})\n\n      try:\n        make_gif(samples, './samples/test_gif_%s.gif' % (idx))\n      except:\n        save_images(samples, [image_frame_dim, image_frame_dim], os.path.join(sample_dir, 'test_%s.png' % strftime(\"%Y%m%d%H%M%S\", gmtime() )))\n  elif option == 3:\n    values = np.arange(0, 1, 1./config.batch_size)\n    for idx in xrange(dcgan.z_dim):\n      print(\" [*] %d\" % idx)\n      z_sample = np.zeros([config.batch_size, dcgan.z_dim])\n      for kdx, z in enumerate(z_sample):\n        z[idx] = values[kdx]\n\n      samples = sess.run(dcgan.sampler, feed_dict={dcgan.z: z_sample})\n      make_gif(samples, os.path.join(sample_dir, 'test_gif_%s.gif' % (idx)))\n  elif option == 4:\n    image_set = []\n    values = np.arange(0, 1, 1./config.batch_size)\n\n    for idx in xrange(dcgan.z_dim):\n      print(\" [*] %d\" % idx)\n      z_sample = np.zeros([config.batch_size, dcgan.z_dim])\n      for kdx, z in enumerate(z_sample): z[idx] = values[kdx]\n\n      image_set.append(sess.run(dcgan.sampler, feed_dict={dcgan.z: z_sample}))\n      make_gif(image_set[-1], os.path.join(sample_dir, 'test_gif_%s.gif' % (idx)))\n\n    new_image_set = [merge(np.array([images[idx] for images in image_set]), [10, 10]) \\\n        for idx in range(64) + range(63, -1, -1)]\n    make_gif(new_image_set, './samples/test_gif_merged.gif', duration=8)\n\n\ndef image_manifold_size(num_images):\n  manifold_h = int(np.floor(np.sqrt(num_images)))\n  manifold_w = int(np.ceil(np.sqrt(num_images)))\n  assert manifold_h * manifold_w == num_images\n  return manifold_h, manifold_w\n"
        },
        {
          "name": "web",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}