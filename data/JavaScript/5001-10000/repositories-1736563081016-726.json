{
  "metadata": {
    "timestamp": 1736563081016,
    "page": 726,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjc3MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "affinelayer/pix2pix-tensorflow",
      "stars": 5079,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.0283203125,
          "content": "*.pyc\ntags\ntest\nmodels\n.idea\n"
        },
        {
          "name": "LICENSE.txt",
          "type": "blob",
          "size": 1.048828125,
          "content": "MIT License\n\nCopyright (c) 2017 Christopher Hesse\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 10.572265625,
          "content": "# pix2pix-tensorflow\n\nBased on [pix2pix](https://phillipi.github.io/pix2pix/) by Isola et al.\n\n[Article about this implemention](https://affinelayer.com/pix2pix/)\n\n[Interactive Demo](https://affinelayer.com/pixsrv/)\n\nTensorflow implementation of pix2pix.  Learns a mapping from input images to output images, like these examples from the original paper:\n\n<img src=\"docs/examples.jpg\" width=\"900px\"/>\n\nThis port is based directly on the torch implementation, and not on an existing Tensorflow implementation.  It is meant to be a faithful implementation of the original work and so does not add anything.  The processing speed on a GPU with cuDNN was equivalent to the Torch implementation in testing.\n\n## Setup\n\n### Prerequisites\n- Tensorflow 1.4.1\n\n### Recommended\n- Linux with Tensorflow GPU edition + cuDNN\n\n### Getting Started\n\n```sh\n# clone this repo\ngit clone https://github.com/affinelayer/pix2pix-tensorflow.git\ncd pix2pix-tensorflow\n# download the CMP Facades dataset (generated from http://cmp.felk.cvut.cz/~tylecr1/facade/)\npython tools/download-dataset.py facades\n# train the model (this may take 1-8 hours depending on GPU, on CPU you will be waiting for a bit)\npython pix2pix.py \\\n  --mode train \\\n  --output_dir facades_train \\\n  --max_epochs 200 \\\n  --input_dir facades/train \\\n  --which_direction BtoA\n# test the model\npython pix2pix.py \\\n  --mode test \\\n  --output_dir facades_test \\\n  --input_dir facades/val \\\n  --checkpoint facades_train\n```\n\nThe test run will output an HTML file at `facades_test/index.html` that shows input/output/target image sets.\n\nIf you have Docker installed, you can use the provided Docker image to run pix2pix without installing the correct version of Tensorflow:\n\n```sh\n# train the model\npython tools/dockrun.py python pix2pix.py \\\n      --mode train \\\n      --output_dir facades_train \\\n      --max_epochs 200 \\\n      --input_dir facades/train \\\n      --which_direction BtoA\n# test the model\npython tools/dockrun.py python pix2pix.py \\\n      --mode test \\\n      --output_dir facades_test \\\n      --input_dir facades/val \\\n      --checkpoint facades_train\n```\n\n## Datasets and Trained Models\n\nThe data format used by this program is the same as the original pix2pix format, which consists of images of input and desired output side by side like:\n\n<img src=\"docs/ab.png\" width=\"256px\"/>\n\nFor example:\n\n<img src=\"docs/418.png\" width=\"256px\"/>\n\nSome datasets have been made available by the authors of the pix2pix paper.  To download those datasets, use the included script `tools/download-dataset.py`.  There are also links to pre-trained models alongside each dataset, note that these pre-trained models require the current version of pix2pix.py:\n\n| dataset | example |\n| --- | --- |\n| `python tools/download-dataset.py facades` <br> 400 images from [CMP Facades dataset](http://cmp.felk.cvut.cz/~tylecr1/facade/). (31MB) <br> Pre-trained: [BtoA](https://mega.nz/#!H0AmER7Y!pBHcH4M11eiHBmJEWvGr-E_jxK4jluKBUlbfyLSKgpY)  | <img src=\"docs/facades.jpg\" width=\"256px\"/> |\n| `python tools/download-dataset.py cityscapes` <br> 2975 images from the [Cityscapes training set](https://www.cityscapes-dataset.com/). (113M) <br> Pre-trained: [AtoB](https://mega.nz/#!K1hXlbJA!rrZuEnL3nqOcRhjb-AnSkK0Ggf9NibhDymLOkhzwuQk) [BtoA](https://mega.nz/#!y1YxxB5D!1817IXQFcydjDdhk_ILbCourhA6WSYRttKLrGE97q7k) | <img src=\"docs/cityscapes.jpg\" width=\"256px\"/> |\n| `python tools/download-dataset.py maps` <br> 1096 training images scraped from Google Maps (246M) <br> Pre-trained: [AtoB](https://mega.nz/#!7oxklCzZ!8fRZoF3jMRS_rylCfw2RNBeewp4DFPVE_tSCjCKr-TI) [BtoA](https://mega.nz/#!S4AGzQJD!UH7B5SV7DJSTqKvtbFKqFkjdAh60kpdhTk9WerI-Q1I) | <img src=\"docs/maps.jpg\" width=\"256px\"/> |\n| `python tools/download-dataset.py edges2shoes` <br> 50k training images from [UT Zappos50K dataset](http://vision.cs.utexas.edu/projects/finegrained/utzap50k/). Edges are computed by [HED](https://github.com/s9xie/hed) edge detector + post-processing. (2.2GB) <br> Pre-trained: [AtoB](https://mega.nz/#!u9pnmC4Q!2uHCZvHsCkHBJhHZ7xo5wI-mfekTwOK8hFPy0uBOrb4) | <img src=\"docs/edges2shoes.jpg\" width=\"256px\"/>  |\n| `python tools/download-dataset.py edges2handbags` <br> 137K Amazon Handbag images from [iGAN project](https://github.com/junyanz/iGAN). Edges are computed by [HED](https://github.com/s9xie/hed) edge detector + post-processing. (8.6GB) <br> Pre-trained: [AtoB](https://mega.nz/#!G1xlDCIS!sFDN3ZXKLUWU1TX6Kqt7UG4Yp-eLcinmf6HVRuSHjrM) | <img src=\"docs/edges2handbags.jpg\" width=\"256px\"/> |\n\nThe `facades` dataset is the smallest and easiest to get started with.\n\n### Creating your own dataset\n\n#### Example: creating images with blank centers for [inpainting](https://people.eecs.berkeley.edu/~pathak/context_encoder/)\n\n<img src=\"docs/combine.png\" width=\"900px\"/>\n\n```sh\n# Resize source images\npython tools/process.py \\\n  --input_dir photos/original \\\n  --operation resize \\\n  --output_dir photos/resized\n# Create images with blank centers\npython tools/process.py \\\n  --input_dir photos/resized \\\n  --operation blank \\\n  --output_dir photos/blank\n# Combine resized images with blanked images\npython tools/process.py \\\n  --input_dir photos/resized \\\n  --b_dir photos/blank \\\n  --operation combine \\\n  --output_dir photos/combined\n# Split into train/val set\npython tools/split.py \\\n  --dir photos/combined\n```\n\nThe folder `photos/combined` will now have `train` and `val` subfolders that you can use for training and testing.\n\n#### Creating image pairs from existing images\n\nIf you have two directories `a` and `b`, with corresponding images (same name, same dimensions, different data) you can combine them with `process.py`:\n\n```sh\npython tools/process.py \\\n  --input_dir a \\\n  --b_dir b \\\n  --operation combine \\\n  --output_dir c\n```\n\nThis puts the images in a side-by-side combined image that `pix2pix.py` expects.\n\n#### Colorization\n\nFor colorization, your images should ideally all be the same aspect ratio.  You can resize and crop them with the resize command:\n```sh\npython tools/process.py \\\n  --input_dir photos/original \\\n  --operation resize \\\n  --output_dir photos/resized\n```\n\nNo other processing is required, the colorization mode (see Training section below) uses single images instead of image pairs.\n\n## Training\n\n### Image Pairs\n\nFor normal training with image pairs, you need to specify which directory contains the training images, and which direction to train on.  The direction options are `AtoB` or `BtoA`\n```sh\npython pix2pix.py \\\n  --mode train \\\n  --output_dir facades_train \\\n  --max_epochs 200 \\\n  --input_dir facades/train \\\n  --which_direction BtoA\n```\n\n### Colorization\n\n`pix2pix.py` includes special code to handle colorization with single images instead of pairs, using that looks like this:\n\n```sh\npython pix2pix.py \\\n  --mode train \\\n  --output_dir photos_train \\\n  --max_epochs 200 \\\n  --input_dir photos/train \\\n  --lab_colorization\n```\n\nIn this mode, image A is the black and white image (lightness only), and image B contains the color channels of that image (no lightness information).\n\n### Tips\n\nYou can look at the loss and computation graph using tensorboard:\n```sh\ntensorboard --logdir=facades_train\n```\n\n<img src=\"docs/tensorboard-scalar.png\" width=\"250px\"/> <img src=\"docs/tensorboard-image.png\" width=\"250px\"/> <img src=\"docs/tensorboard-graph.png\" width=\"250px\"/>\n\nIf you wish to write in-progress pictures as the network is training, use `--display_freq 50`.  This will update `facades_train/index.html` every 50 steps with the current training inputs and outputs.\n\n## Testing\n\nTesting is done with `--mode test`.  You should specify the checkpoint to use with `--checkpoint`, this should point to the `output_dir` that you created previously with `--mode train`:\n\n```sh\npython pix2pix.py \\\n  --mode test \\\n  --output_dir facades_test \\\n  --input_dir facades/val \\\n  --checkpoint facades_train\n```\n\nThe testing mode will load some of the configuration options from the checkpoint provided so you do not need to specify `which_direction` for instance.\n\nThe test run will output an HTML file at `facades_test/index.html` that shows input/output/target image sets:\n\n<img src=\"docs/test-html.png\" width=\"300px\"/>\n\n## Code Validation\n\nValidation of the code was performed on a Linux machine with a ~1.3 TFLOPS Nvidia GTX 750 Ti GPU and an Azure NC6 instance with a K80 GPU.\n\n```sh\ngit clone https://github.com/affinelayer/pix2pix-tensorflow.git\ncd pix2pix-tensorflow\npython tools/download-dataset.py facades\nsudo nvidia-docker run \\\n  --volume $PWD:/prj \\\n  --workdir /prj \\\n  --env PYTHONUNBUFFERED=x \\\n  affinelayer/pix2pix-tensorflow \\\n    python pix2pix.py \\\n      --mode train \\\n      --output_dir facades_train \\\n      --max_epochs 200 \\\n      --input_dir facades/train \\\n      --which_direction BtoA\nsudo nvidia-docker run \\\n  --volume $PWD:/prj \\\n  --workdir /prj \\\n  --env PYTHONUNBUFFERED=x \\\n  affinelayer/pix2pix-tensorflow \\\n    python pix2pix.py \\\n      --mode test \\\n      --output_dir facades_test \\\n      --input_dir facades/val \\\n      --checkpoint facades_train\n```\n\nComparison on facades dataset:\n\n| Input | Tensorflow | Torch | Target |\n| --- | --- | --- | --- |\n| <img src=\"docs/1-inputs.png\" width=\"256px\"> | <img src=\"docs/1-tensorflow.png\" width=\"256px\"> | <img src=\"docs/1-torch.jpg\" width=\"256px\"> | <img src=\"docs/1-targets.png\" width=\"256px\"> |\n| <img src=\"docs/5-inputs.png\" width=\"256px\"> | <img src=\"docs/5-tensorflow.png\" width=\"256px\"> | <img src=\"docs/5-torch.jpg\" width=\"256px\"> | <img src=\"docs/5-targets.png\" width=\"256px\"> |\n| <img src=\"docs/51-inputs.png\" width=\"256px\"> | <img src=\"docs/51-tensorflow.png\" width=\"256px\"> | <img src=\"docs/51-torch.jpg\" width=\"256px\"> | <img src=\"docs/51-targets.png\" width=\"256px\"> |\n| <img src=\"docs/95-inputs.png\" width=\"256px\"> | <img src=\"docs/95-tensorflow.png\" width=\"256px\"> | <img src=\"docs/95-torch.jpg\" width=\"256px\"> | <img src=\"docs/95-targets.png\" width=\"256px\"> |\n\n## Unimplemented Features\n\nThe following models have not been implemented:\n- defineG_encoder_decoder\n- defineG_unet_128\n- defineD_pixelGAN\n\n## Citation\nIf you use this code for your research, please cite the paper this code is based on: <a href=\"https://arxiv.org/pdf/1611.07004v1.pdf\">Image-to-Image Translation Using Conditional Adversarial Networks</a>:\n\n```\n@article{pix2pix2016,\n  title={Image-to-Image Translation with Conditional Adversarial Networks},\n  author={Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A},\n  journal={arxiv},\n  year={2016}\n}\n```\n\n## Acknowledgments\nThis is a port of [pix2pix](https://github.com/phillipi/pix2pix) from Torch to Tensorflow.  It also contains colorspace conversion code ported from Torch.  Thanks to the Tensorflow team for making such a quality library!  And special thanks to Phillip Isola for answering my questions about the pix2pix code.\n"
        },
        {
          "name": "docker",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "pix2pix.py",
          "type": "blob",
          "size": 34.7744140625,
          "content": "from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport numpy as np\nimport argparse\nimport os\nimport json\nimport glob\nimport random\nimport collections\nimport math\nimport time\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--input_dir\", help=\"path to folder containing images\")\nparser.add_argument(\"--mode\", required=True, choices=[\"train\", \"test\", \"export\"])\nparser.add_argument(\"--output_dir\", required=True, help=\"where to put output files\")\nparser.add_argument(\"--seed\", type=int)\nparser.add_argument(\"--checkpoint\", default=None, help=\"directory with checkpoint to resume training from or use for testing\")\n\nparser.add_argument(\"--max_steps\", type=int, help=\"number of training steps (0 to disable)\")\nparser.add_argument(\"--max_epochs\", type=int, help=\"number of training epochs\")\nparser.add_argument(\"--summary_freq\", type=int, default=100, help=\"update summaries every summary_freq steps\")\nparser.add_argument(\"--progress_freq\", type=int, default=50, help=\"display progress every progress_freq steps\")\nparser.add_argument(\"--trace_freq\", type=int, default=0, help=\"trace execution every trace_freq steps\")\nparser.add_argument(\"--display_freq\", type=int, default=0, help=\"write current training images every display_freq steps\")\nparser.add_argument(\"--save_freq\", type=int, default=5000, help=\"save model every save_freq steps, 0 to disable\")\n\nparser.add_argument(\"--separable_conv\", action=\"store_true\", help=\"use separable convolutions in the generator\")\nparser.add_argument(\"--aspect_ratio\", type=float, default=1.0, help=\"aspect ratio of output images (width/height)\")\nparser.add_argument(\"--lab_colorization\", action=\"store_true\", help=\"split input image into brightness (A) and color (B)\")\nparser.add_argument(\"--batch_size\", type=int, default=1, help=\"number of images in batch\")\nparser.add_argument(\"--which_direction\", type=str, default=\"AtoB\", choices=[\"AtoB\", \"BtoA\"])\nparser.add_argument(\"--ngf\", type=int, default=64, help=\"number of generator filters in first conv layer\")\nparser.add_argument(\"--ndf\", type=int, default=64, help=\"number of discriminator filters in first conv layer\")\nparser.add_argument(\"--scale_size\", type=int, default=286, help=\"scale images to this size before cropping to 256x256\")\nparser.add_argument(\"--flip\", dest=\"flip\", action=\"store_true\", help=\"flip images horizontally\")\nparser.add_argument(\"--no_flip\", dest=\"flip\", action=\"store_false\", help=\"don't flip images horizontally\")\nparser.set_defaults(flip=True)\nparser.add_argument(\"--lr\", type=float, default=0.0002, help=\"initial learning rate for adam\")\nparser.add_argument(\"--beta1\", type=float, default=0.5, help=\"momentum term of adam\")\nparser.add_argument(\"--l1_weight\", type=float, default=100.0, help=\"weight on L1 term for generator gradient\")\nparser.add_argument(\"--gan_weight\", type=float, default=1.0, help=\"weight on GAN term for generator gradient\")\n\n# export options\nparser.add_argument(\"--output_filetype\", default=\"png\", choices=[\"png\", \"jpeg\"])\na = parser.parse_args()\n\nEPS = 1e-12\nCROP_SIZE = 256\n\nExamples = collections.namedtuple(\"Examples\", \"paths, inputs, targets, count, steps_per_epoch\")\nModel = collections.namedtuple(\"Model\", \"outputs, predict_real, predict_fake, discrim_loss, discrim_grads_and_vars, gen_loss_GAN, gen_loss_L1, gen_grads_and_vars, train\")\n\n\ndef preprocess(image):\n    with tf.name_scope(\"preprocess\"):\n        # [0, 1] => [-1, 1]\n        return image * 2 - 1\n\n\ndef deprocess(image):\n    with tf.name_scope(\"deprocess\"):\n        # [-1, 1] => [0, 1]\n        return (image + 1) / 2\n\n\ndef preprocess_lab(lab):\n    with tf.name_scope(\"preprocess_lab\"):\n        L_chan, a_chan, b_chan = tf.unstack(lab, axis=2)\n        # L_chan: black and white with input range [0, 100]\n        # a_chan/b_chan: color channels with input range ~[-110, 110], not exact\n        # [0, 100] => [-1, 1],  ~[-110, 110] => [-1, 1]\n        return [L_chan / 50 - 1, a_chan / 110, b_chan / 110]\n\n\ndef deprocess_lab(L_chan, a_chan, b_chan):\n    with tf.name_scope(\"deprocess_lab\"):\n        # this is axis=3 instead of axis=2 because we process individual images but deprocess batches\n        return tf.stack([(L_chan + 1) / 2 * 100, a_chan * 110, b_chan * 110], axis=3)\n\n\ndef augment(image, brightness):\n    # (a, b) color channels, combine with L channel and convert to rgb\n    a_chan, b_chan = tf.unstack(image, axis=3)\n    L_chan = tf.squeeze(brightness, axis=3)\n    lab = deprocess_lab(L_chan, a_chan, b_chan)\n    rgb = lab_to_rgb(lab)\n    return rgb\n\n\ndef discrim_conv(batch_input, out_channels, stride):\n    padded_input = tf.pad(batch_input, [[0, 0], [1, 1], [1, 1], [0, 0]], mode=\"CONSTANT\")\n    return tf.layers.conv2d(padded_input, out_channels, kernel_size=4, strides=(stride, stride), padding=\"valid\", kernel_initializer=tf.random_normal_initializer(0, 0.02))\n\n\ndef gen_conv(batch_input, out_channels):\n    # [batch, in_height, in_width, in_channels] => [batch, out_height, out_width, out_channels]\n    initializer = tf.random_normal_initializer(0, 0.02)\n    if a.separable_conv:\n        return tf.layers.separable_conv2d(batch_input, out_channels, kernel_size=4, strides=(2, 2), padding=\"same\", depthwise_initializer=initializer, pointwise_initializer=initializer)\n    else:\n        return tf.layers.conv2d(batch_input, out_channels, kernel_size=4, strides=(2, 2), padding=\"same\", kernel_initializer=initializer)\n\n\ndef gen_deconv(batch_input, out_channels):\n    # [batch, in_height, in_width, in_channels] => [batch, out_height, out_width, out_channels]\n    initializer = tf.random_normal_initializer(0, 0.02)\n    if a.separable_conv:\n        _b, h, w, _c = batch_input.shape\n        resized_input = tf.image.resize_images(batch_input, [h * 2, w * 2], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n        return tf.layers.separable_conv2d(resized_input, out_channels, kernel_size=4, strides=(1, 1), padding=\"same\", depthwise_initializer=initializer, pointwise_initializer=initializer)\n    else:\n        return tf.layers.conv2d_transpose(batch_input, out_channels, kernel_size=4, strides=(2, 2), padding=\"same\", kernel_initializer=initializer)\n\n\ndef lrelu(x, a):\n    with tf.name_scope(\"lrelu\"):\n        # adding these together creates the leak part and linear part\n        # then cancels them out by subtracting/adding an absolute value term\n        # leak: a*x/2 - a*abs(x)/2\n        # linear: x/2 + abs(x)/2\n\n        # this block looks like it has 2 inputs on the graph unless we do this\n        x = tf.identity(x)\n        return (0.5 * (1 + a)) * x + (0.5 * (1 - a)) * tf.abs(x)\n\n\ndef batchnorm(inputs):\n    return tf.layers.batch_normalization(inputs, axis=3, epsilon=1e-5, momentum=0.1, training=True, gamma_initializer=tf.random_normal_initializer(1.0, 0.02))\n\n\ndef check_image(image):\n    assertion = tf.assert_equal(tf.shape(image)[-1], 3, message=\"image must have 3 color channels\")\n    with tf.control_dependencies([assertion]):\n        image = tf.identity(image)\n\n    if image.get_shape().ndims not in (3, 4):\n        raise ValueError(\"image must be either 3 or 4 dimensions\")\n\n    # make the last dimension 3 so that you can unstack the colors\n    shape = list(image.get_shape())\n    shape[-1] = 3\n    image.set_shape(shape)\n    return image\n\n# based on https://github.com/torch/image/blob/9f65c30167b2048ecbe8b7befdc6b2d6d12baee9/generic/image.c\ndef rgb_to_lab(srgb):\n    with tf.name_scope(\"rgb_to_lab\"):\n        srgb = check_image(srgb)\n        srgb_pixels = tf.reshape(srgb, [-1, 3])\n\n        with tf.name_scope(\"srgb_to_xyz\"):\n            linear_mask = tf.cast(srgb_pixels <= 0.04045, dtype=tf.float32)\n            exponential_mask = tf.cast(srgb_pixels > 0.04045, dtype=tf.float32)\n            rgb_pixels = (srgb_pixels / 12.92 * linear_mask) + (((srgb_pixels + 0.055) / 1.055) ** 2.4) * exponential_mask\n            rgb_to_xyz = tf.constant([\n                #    X        Y          Z\n                [0.412453, 0.212671, 0.019334], # R\n                [0.357580, 0.715160, 0.119193], # G\n                [0.180423, 0.072169, 0.950227], # B\n            ])\n            xyz_pixels = tf.matmul(rgb_pixels, rgb_to_xyz)\n\n        # https://en.wikipedia.org/wiki/Lab_color_space#CIELAB-CIEXYZ_conversions\n        with tf.name_scope(\"xyz_to_cielab\"):\n            # convert to fx = f(X/Xn), fy = f(Y/Yn), fz = f(Z/Zn)\n\n            # normalize for D65 white point\n            xyz_normalized_pixels = tf.multiply(xyz_pixels, [1/0.950456, 1.0, 1/1.088754])\n\n            epsilon = 6/29\n            linear_mask = tf.cast(xyz_normalized_pixels <= (epsilon**3), dtype=tf.float32)\n            exponential_mask = tf.cast(xyz_normalized_pixels > (epsilon**3), dtype=tf.float32)\n            fxfyfz_pixels = (xyz_normalized_pixels / (3 * epsilon**2) + 4/29) * linear_mask + (xyz_normalized_pixels ** (1/3)) * exponential_mask\n\n            # convert to lab\n            fxfyfz_to_lab = tf.constant([\n                #  l       a       b\n                [  0.0,  500.0,    0.0], # fx\n                [116.0, -500.0,  200.0], # fy\n                [  0.0,    0.0, -200.0], # fz\n            ])\n            lab_pixels = tf.matmul(fxfyfz_pixels, fxfyfz_to_lab) + tf.constant([-16.0, 0.0, 0.0])\n\n        return tf.reshape(lab_pixels, tf.shape(srgb))\n\n\ndef lab_to_rgb(lab):\n    with tf.name_scope(\"lab_to_rgb\"):\n        lab = check_image(lab)\n        lab_pixels = tf.reshape(lab, [-1, 3])\n\n        # https://en.wikipedia.org/wiki/Lab_color_space#CIELAB-CIEXYZ_conversions\n        with tf.name_scope(\"cielab_to_xyz\"):\n            # convert to fxfyfz\n            lab_to_fxfyfz = tf.constant([\n                #   fx      fy        fz\n                [1/116.0, 1/116.0,  1/116.0], # l\n                [1/500.0,     0.0,      0.0], # a\n                [    0.0,     0.0, -1/200.0], # b\n            ])\n            fxfyfz_pixels = tf.matmul(lab_pixels + tf.constant([16.0, 0.0, 0.0]), lab_to_fxfyfz)\n\n            # convert to xyz\n            epsilon = 6/29\n            linear_mask = tf.cast(fxfyfz_pixels <= epsilon, dtype=tf.float32)\n            exponential_mask = tf.cast(fxfyfz_pixels > epsilon, dtype=tf.float32)\n            xyz_pixels = (3 * epsilon**2 * (fxfyfz_pixels - 4/29)) * linear_mask + (fxfyfz_pixels ** 3) * exponential_mask\n\n            # denormalize for D65 white point\n            xyz_pixels = tf.multiply(xyz_pixels, [0.950456, 1.0, 1.088754])\n\n        with tf.name_scope(\"xyz_to_srgb\"):\n            xyz_to_rgb = tf.constant([\n                #     r           g          b\n                [ 3.2404542, -0.9692660,  0.0556434], # x\n                [-1.5371385,  1.8760108, -0.2040259], # y\n                [-0.4985314,  0.0415560,  1.0572252], # z\n            ])\n            rgb_pixels = tf.matmul(xyz_pixels, xyz_to_rgb)\n            # avoid a slightly negative number messing up the conversion\n            rgb_pixels = tf.clip_by_value(rgb_pixels, 0.0, 1.0)\n            linear_mask = tf.cast(rgb_pixels <= 0.0031308, dtype=tf.float32)\n            exponential_mask = tf.cast(rgb_pixels > 0.0031308, dtype=tf.float32)\n            srgb_pixels = (rgb_pixels * 12.92 * linear_mask) + ((rgb_pixels ** (1/2.4) * 1.055) - 0.055) * exponential_mask\n\n        return tf.reshape(srgb_pixels, tf.shape(lab))\n\n\ndef load_examples():\n    if a.input_dir is None or not os.path.exists(a.input_dir):\n        raise Exception(\"input_dir does not exist\")\n\n    input_paths = glob.glob(os.path.join(a.input_dir, \"*.jpg\"))\n    decode = tf.image.decode_jpeg\n    if len(input_paths) == 0:\n        input_paths = glob.glob(os.path.join(a.input_dir, \"*.png\"))\n        decode = tf.image.decode_png\n\n    if len(input_paths) == 0:\n        raise Exception(\"input_dir contains no image files\")\n\n    def get_name(path):\n        name, _ = os.path.splitext(os.path.basename(path))\n        return name\n\n    # if the image names are numbers, sort by the value rather than asciibetically\n    # having sorted inputs means that the outputs are sorted in test mode\n    if all(get_name(path).isdigit() for path in input_paths):\n        input_paths = sorted(input_paths, key=lambda path: int(get_name(path)))\n    else:\n        input_paths = sorted(input_paths)\n\n    with tf.name_scope(\"load_images\"):\n        path_queue = tf.train.string_input_producer(input_paths, shuffle=a.mode == \"train\")\n        reader = tf.WholeFileReader()\n        paths, contents = reader.read(path_queue)\n        raw_input = decode(contents)\n        raw_input = tf.image.convert_image_dtype(raw_input, dtype=tf.float32)\n\n        assertion = tf.assert_equal(tf.shape(raw_input)[2], 3, message=\"image does not have 3 channels\")\n        with tf.control_dependencies([assertion]):\n            raw_input = tf.identity(raw_input)\n\n        raw_input.set_shape([None, None, 3])\n\n        if a.lab_colorization:\n            # load color and brightness from image, no B image exists here\n            lab = rgb_to_lab(raw_input)\n            L_chan, a_chan, b_chan = preprocess_lab(lab)\n            a_images = tf.expand_dims(L_chan, axis=2)\n            b_images = tf.stack([a_chan, b_chan], axis=2)\n        else:\n            # break apart image pair and move to range [-1, 1]\n            width = tf.shape(raw_input)[1] # [height, width, channels]\n            a_images = preprocess(raw_input[:,:width//2,:])\n            b_images = preprocess(raw_input[:,width//2:,:])\n\n    if a.which_direction == \"AtoB\":\n        inputs, targets = [a_images, b_images]\n    elif a.which_direction == \"BtoA\":\n        inputs, targets = [b_images, a_images]\n    else:\n        raise Exception(\"invalid direction\")\n\n    # synchronize seed for image operations so that we do the same operations to both\n    # input and output images\n    seed = random.randint(0, 2**31 - 1)\n    def transform(image):\n        r = image\n        if a.flip:\n            r = tf.image.random_flip_left_right(r, seed=seed)\n\n        # area produces a nice downscaling, but does nearest neighbor for upscaling\n        # assume we're going to be doing downscaling here\n        r = tf.image.resize_images(r, [a.scale_size, a.scale_size], method=tf.image.ResizeMethod.AREA)\n\n        offset = tf.cast(tf.floor(tf.random_uniform([2], 0, a.scale_size - CROP_SIZE + 1, seed=seed)), dtype=tf.int32)\n        if a.scale_size > CROP_SIZE:\n            r = tf.image.crop_to_bounding_box(r, offset[0], offset[1], CROP_SIZE, CROP_SIZE)\n        elif a.scale_size < CROP_SIZE:\n            raise Exception(\"scale size cannot be less than crop size\")\n        return r\n\n    with tf.name_scope(\"input_images\"):\n        input_images = transform(inputs)\n\n    with tf.name_scope(\"target_images\"):\n        target_images = transform(targets)\n\n    paths_batch, inputs_batch, targets_batch = tf.train.batch([paths, input_images, target_images], batch_size=a.batch_size)\n    steps_per_epoch = int(math.ceil(len(input_paths) / a.batch_size))\n\n    return Examples(\n        paths=paths_batch,\n        inputs=inputs_batch,\n        targets=targets_batch,\n        count=len(input_paths),\n        steps_per_epoch=steps_per_epoch,\n    )\n\n\ndef create_generator(generator_inputs, generator_outputs_channels):\n    layers = []\n\n    # encoder_1: [batch, 256, 256, in_channels] => [batch, 128, 128, ngf]\n    with tf.variable_scope(\"encoder_1\"):\n        output = gen_conv(generator_inputs, a.ngf)\n        layers.append(output)\n\n    layer_specs = [\n        a.ngf * 2, # encoder_2: [batch, 128, 128, ngf] => [batch, 64, 64, ngf * 2]\n        a.ngf * 4, # encoder_3: [batch, 64, 64, ngf * 2] => [batch, 32, 32, ngf * 4]\n        a.ngf * 8, # encoder_4: [batch, 32, 32, ngf * 4] => [batch, 16, 16, ngf * 8]\n        a.ngf * 8, # encoder_5: [batch, 16, 16, ngf * 8] => [batch, 8, 8, ngf * 8]\n        a.ngf * 8, # encoder_6: [batch, 8, 8, ngf * 8] => [batch, 4, 4, ngf * 8]\n        a.ngf * 8, # encoder_7: [batch, 4, 4, ngf * 8] => [batch, 2, 2, ngf * 8]\n        a.ngf * 8, # encoder_8: [batch, 2, 2, ngf * 8] => [batch, 1, 1, ngf * 8]\n    ]\n\n    for out_channels in layer_specs:\n        with tf.variable_scope(\"encoder_%d\" % (len(layers) + 1)):\n            rectified = lrelu(layers[-1], 0.2)\n            # [batch, in_height, in_width, in_channels] => [batch, in_height/2, in_width/2, out_channels]\n            convolved = gen_conv(rectified, out_channels)\n            output = batchnorm(convolved)\n            layers.append(output)\n\n    layer_specs = [\n        (a.ngf * 8, 0.5),   # decoder_8: [batch, 1, 1, ngf * 8] => [batch, 2, 2, ngf * 8 * 2]\n        (a.ngf * 8, 0.5),   # decoder_7: [batch, 2, 2, ngf * 8 * 2] => [batch, 4, 4, ngf * 8 * 2]\n        (a.ngf * 8, 0.5),   # decoder_6: [batch, 4, 4, ngf * 8 * 2] => [batch, 8, 8, ngf * 8 * 2]\n        (a.ngf * 8, 0.0),   # decoder_5: [batch, 8, 8, ngf * 8 * 2] => [batch, 16, 16, ngf * 8 * 2]\n        (a.ngf * 4, 0.0),   # decoder_4: [batch, 16, 16, ngf * 8 * 2] => [batch, 32, 32, ngf * 4 * 2]\n        (a.ngf * 2, 0.0),   # decoder_3: [batch, 32, 32, ngf * 4 * 2] => [batch, 64, 64, ngf * 2 * 2]\n        (a.ngf, 0.0),       # decoder_2: [batch, 64, 64, ngf * 2 * 2] => [batch, 128, 128, ngf * 2]\n    ]\n\n    num_encoder_layers = len(layers)\n    for decoder_layer, (out_channels, dropout) in enumerate(layer_specs):\n        skip_layer = num_encoder_layers - decoder_layer - 1\n        with tf.variable_scope(\"decoder_%d\" % (skip_layer + 1)):\n            if decoder_layer == 0:\n                # first decoder layer doesn't have skip connections\n                # since it is directly connected to the skip_layer\n                input = layers[-1]\n            else:\n                input = tf.concat([layers[-1], layers[skip_layer]], axis=3)\n\n            rectified = tf.nn.relu(input)\n            # [batch, in_height, in_width, in_channels] => [batch, in_height*2, in_width*2, out_channels]\n            output = gen_deconv(rectified, out_channels)\n            output = batchnorm(output)\n\n            if dropout > 0.0:\n                output = tf.nn.dropout(output, keep_prob=1 - dropout)\n\n            layers.append(output)\n\n    # decoder_1: [batch, 128, 128, ngf * 2] => [batch, 256, 256, generator_outputs_channels]\n    with tf.variable_scope(\"decoder_1\"):\n        input = tf.concat([layers[-1], layers[0]], axis=3)\n        rectified = tf.nn.relu(input)\n        output = gen_deconv(rectified, generator_outputs_channels)\n        output = tf.tanh(output)\n        layers.append(output)\n\n    return layers[-1]\n\n\ndef create_model(inputs, targets):\n    def create_discriminator(discrim_inputs, discrim_targets):\n        n_layers = 3\n        layers = []\n\n        # 2x [batch, height, width, in_channels] => [batch, height, width, in_channels * 2]\n        input = tf.concat([discrim_inputs, discrim_targets], axis=3)\n\n        # layer_1: [batch, 256, 256, in_channels * 2] => [batch, 128, 128, ndf]\n        with tf.variable_scope(\"layer_1\"):\n            convolved = discrim_conv(input, a.ndf, stride=2)\n            rectified = lrelu(convolved, 0.2)\n            layers.append(rectified)\n\n        # layer_2: [batch, 128, 128, ndf] => [batch, 64, 64, ndf * 2]\n        # layer_3: [batch, 64, 64, ndf * 2] => [batch, 32, 32, ndf * 4]\n        # layer_4: [batch, 32, 32, ndf * 4] => [batch, 31, 31, ndf * 8]\n        for i in range(n_layers):\n            with tf.variable_scope(\"layer_%d\" % (len(layers) + 1)):\n                out_channels = a.ndf * min(2**(i+1), 8)\n                stride = 1 if i == n_layers - 1 else 2  # last layer here has stride 1\n                convolved = discrim_conv(layers[-1], out_channels, stride=stride)\n                normalized = batchnorm(convolved)\n                rectified = lrelu(normalized, 0.2)\n                layers.append(rectified)\n\n        # layer_5: [batch, 31, 31, ndf * 8] => [batch, 30, 30, 1]\n        with tf.variable_scope(\"layer_%d\" % (len(layers) + 1)):\n            convolved = discrim_conv(rectified, out_channels=1, stride=1)\n            output = tf.sigmoid(convolved)\n            layers.append(output)\n\n        return layers[-1]\n\n    with tf.variable_scope(\"generator\"):\n        out_channels = int(targets.get_shape()[-1])\n        outputs = create_generator(inputs, out_channels)\n\n    # create two copies of discriminator, one for real pairs and one for fake pairs\n    # they share the same underlying variables\n    with tf.name_scope(\"real_discriminator\"):\n        with tf.variable_scope(\"discriminator\"):\n            # 2x [batch, height, width, channels] => [batch, 30, 30, 1]\n            predict_real = create_discriminator(inputs, targets)\n\n    with tf.name_scope(\"fake_discriminator\"):\n        with tf.variable_scope(\"discriminator\", reuse=True):\n            # 2x [batch, height, width, channels] => [batch, 30, 30, 1]\n            predict_fake = create_discriminator(inputs, outputs)\n\n    with tf.name_scope(\"discriminator_loss\"):\n        # minimizing -tf.log will try to get inputs to 1\n        # predict_real => 1\n        # predict_fake => 0\n        discrim_loss = tf.reduce_mean(-(tf.log(predict_real + EPS) + tf.log(1 - predict_fake + EPS)))\n\n    with tf.name_scope(\"generator_loss\"):\n        # predict_fake => 1\n        # abs(targets - outputs) => 0\n        gen_loss_GAN = tf.reduce_mean(-tf.log(predict_fake + EPS))\n        gen_loss_L1 = tf.reduce_mean(tf.abs(targets - outputs))\n        gen_loss = gen_loss_GAN * a.gan_weight + gen_loss_L1 * a.l1_weight\n\n    with tf.name_scope(\"discriminator_train\"):\n        discrim_tvars = [var for var in tf.trainable_variables() if var.name.startswith(\"discriminator\")]\n        discrim_optim = tf.train.AdamOptimizer(a.lr, a.beta1)\n        discrim_grads_and_vars = discrim_optim.compute_gradients(discrim_loss, var_list=discrim_tvars)\n        discrim_train = discrim_optim.apply_gradients(discrim_grads_and_vars)\n\n    with tf.name_scope(\"generator_train\"):\n        with tf.control_dependencies([discrim_train]):\n            gen_tvars = [var for var in tf.trainable_variables() if var.name.startswith(\"generator\")]\n            gen_optim = tf.train.AdamOptimizer(a.lr, a.beta1)\n            gen_grads_and_vars = gen_optim.compute_gradients(gen_loss, var_list=gen_tvars)\n            gen_train = gen_optim.apply_gradients(gen_grads_and_vars)\n\n    ema = tf.train.ExponentialMovingAverage(decay=0.99)\n    update_losses = ema.apply([discrim_loss, gen_loss_GAN, gen_loss_L1])\n\n    global_step = tf.train.get_or_create_global_step()\n    incr_global_step = tf.assign(global_step, global_step+1)\n\n    return Model(\n        predict_real=predict_real,\n        predict_fake=predict_fake,\n        discrim_loss=ema.average(discrim_loss),\n        discrim_grads_and_vars=discrim_grads_and_vars,\n        gen_loss_GAN=ema.average(gen_loss_GAN),\n        gen_loss_L1=ema.average(gen_loss_L1),\n        gen_grads_and_vars=gen_grads_and_vars,\n        outputs=outputs,\n        train=tf.group(update_losses, incr_global_step, gen_train),\n    )\n\n\ndef save_images(fetches, step=None):\n    image_dir = os.path.join(a.output_dir, \"images\")\n    if not os.path.exists(image_dir):\n        os.makedirs(image_dir)\n\n    filesets = []\n    for i, in_path in enumerate(fetches[\"paths\"]):\n        name, _ = os.path.splitext(os.path.basename(in_path.decode(\"utf8\")))\n        fileset = {\"name\": name, \"step\": step}\n        for kind in [\"inputs\", \"outputs\", \"targets\"]:\n            filename = name + \"-\" + kind + \".png\"\n            if step is not None:\n                filename = \"%08d-%s\" % (step, filename)\n            fileset[kind] = filename\n            out_path = os.path.join(image_dir, filename)\n            contents = fetches[kind][i]\n            with open(out_path, \"wb\") as f:\n                f.write(contents)\n        filesets.append(fileset)\n    return filesets\n\n\ndef append_index(filesets, step=False):\n    index_path = os.path.join(a.output_dir, \"index.html\")\n    if os.path.exists(index_path):\n        index = open(index_path, \"a\")\n    else:\n        index = open(index_path, \"w\")\n        index.write(\"<html><body><table><tr>\")\n        if step:\n            index.write(\"<th>step</th>\")\n        index.write(\"<th>name</th><th>input</th><th>output</th><th>target</th></tr>\")\n\n    for fileset in filesets:\n        index.write(\"<tr>\")\n\n        if step:\n            index.write(\"<td>%d</td>\" % fileset[\"step\"])\n        index.write(\"<td>%s</td>\" % fileset[\"name\"])\n\n        for kind in [\"inputs\", \"outputs\", \"targets\"]:\n            index.write(\"<td><img src='images/%s'></td>\" % fileset[kind])\n\n        index.write(\"</tr>\")\n    return index_path\n\n\ndef main():\n    if a.seed is None:\n        a.seed = random.randint(0, 2**31 - 1)\n\n    tf.set_random_seed(a.seed)\n    np.random.seed(a.seed)\n    random.seed(a.seed)\n\n    if not os.path.exists(a.output_dir):\n        os.makedirs(a.output_dir)\n\n    if a.mode == \"test\" or a.mode == \"export\":\n        if a.checkpoint is None:\n            raise Exception(\"checkpoint required for test mode\")\n\n        # load some options from the checkpoint\n        options = {\"which_direction\", \"ngf\", \"ndf\", \"lab_colorization\"}\n        with open(os.path.join(a.checkpoint, \"options.json\")) as f:\n            for key, val in json.loads(f.read()).items():\n                if key in options:\n                    print(\"loaded\", key, \"=\", val)\n                    setattr(a, key, val)\n        # disable these features in test mode\n        a.scale_size = CROP_SIZE\n        a.flip = False\n\n    for k, v in a._get_kwargs():\n        print(k, \"=\", v)\n\n    with open(os.path.join(a.output_dir, \"options.json\"), \"w\") as f:\n        f.write(json.dumps(vars(a), sort_keys=True, indent=4))\n\n    if a.mode == \"export\":\n        # export the generator to a meta graph that can be imported later for standalone generation\n        if a.lab_colorization:\n            raise Exception(\"export not supported for lab_colorization\")\n\n        input = tf.placeholder(tf.string, shape=[1])\n        input_data = tf.decode_base64(input[0])\n        input_image = tf.image.decode_png(input_data)\n\n        # remove alpha channel if present\n        input_image = tf.cond(tf.equal(tf.shape(input_image)[2], 4), lambda: input_image[:,:,:3], lambda: input_image)\n        # convert grayscale to RGB\n        input_image = tf.cond(tf.equal(tf.shape(input_image)[2], 1), lambda: tf.image.grayscale_to_rgb(input_image), lambda: input_image)\n\n        input_image = tf.image.convert_image_dtype(input_image, dtype=tf.float32)\n        input_image.set_shape([CROP_SIZE, CROP_SIZE, 3])\n        batch_input = tf.expand_dims(input_image, axis=0)\n\n        with tf.variable_scope(\"generator\"):\n            batch_output = deprocess(create_generator(preprocess(batch_input), 3))\n\n        output_image = tf.image.convert_image_dtype(batch_output, dtype=tf.uint8)[0]\n        if a.output_filetype == \"png\":\n            output_data = tf.image.encode_png(output_image)\n        elif a.output_filetype == \"jpeg\":\n            output_data = tf.image.encode_jpeg(output_image, quality=80)\n        else:\n            raise Exception(\"invalid filetype\")\n        output = tf.convert_to_tensor([tf.encode_base64(output_data)])\n\n        key = tf.placeholder(tf.string, shape=[1])\n        inputs = {\n            \"key\": key.name,\n            \"input\": input.name\n        }\n        tf.add_to_collection(\"inputs\", json.dumps(inputs))\n        outputs = {\n            \"key\":  tf.identity(key).name,\n            \"output\": output.name,\n        }\n        tf.add_to_collection(\"outputs\", json.dumps(outputs))\n\n        init_op = tf.global_variables_initializer()\n        restore_saver = tf.train.Saver()\n        export_saver = tf.train.Saver()\n\n        with tf.Session() as sess:\n            sess.run(init_op)\n            print(\"loading model from checkpoint\")\n            checkpoint = tf.train.latest_checkpoint(a.checkpoint)\n            restore_saver.restore(sess, checkpoint)\n            print(\"exporting model\")\n            export_saver.export_meta_graph(filename=os.path.join(a.output_dir, \"export.meta\"))\n            export_saver.save(sess, os.path.join(a.output_dir, \"export\"), write_meta_graph=False)\n\n        return\n\n    examples = load_examples()\n    print(\"examples count = %d\" % examples.count)\n\n    # inputs and targets are [batch_size, height, width, channels]\n    model = create_model(examples.inputs, examples.targets)\n\n    # undo colorization splitting on images that we use for display/output\n    if a.lab_colorization:\n        if a.which_direction == \"AtoB\":\n            # inputs is brightness, this will be handled fine as a grayscale image\n            # need to augment targets and outputs with brightness\n            targets = augment(examples.targets, examples.inputs)\n            outputs = augment(model.outputs, examples.inputs)\n            # inputs can be deprocessed normally and handled as if they are single channel\n            # grayscale images\n            inputs = deprocess(examples.inputs)\n        elif a.which_direction == \"BtoA\":\n            # inputs will be color channels only, get brightness from targets\n            inputs = augment(examples.inputs, examples.targets)\n            targets = deprocess(examples.targets)\n            outputs = deprocess(model.outputs)\n        else:\n            raise Exception(\"invalid direction\")\n    else:\n        inputs = deprocess(examples.inputs)\n        targets = deprocess(examples.targets)\n        outputs = deprocess(model.outputs)\n\n    def convert(image):\n        if a.aspect_ratio != 1.0:\n            # upscale to correct aspect ratio\n            size = [CROP_SIZE, int(round(CROP_SIZE * a.aspect_ratio))]\n            image = tf.image.resize_images(image, size=size, method=tf.image.ResizeMethod.BICUBIC)\n\n        return tf.image.convert_image_dtype(image, dtype=tf.uint8, saturate=True)\n\n    # reverse any processing on images so they can be written to disk or displayed to user\n    with tf.name_scope(\"convert_inputs\"):\n        converted_inputs = convert(inputs)\n\n    with tf.name_scope(\"convert_targets\"):\n        converted_targets = convert(targets)\n\n    with tf.name_scope(\"convert_outputs\"):\n        converted_outputs = convert(outputs)\n\n    with tf.name_scope(\"encode_images\"):\n        display_fetches = {\n            \"paths\": examples.paths,\n            \"inputs\": tf.map_fn(tf.image.encode_png, converted_inputs, dtype=tf.string, name=\"input_pngs\"),\n            \"targets\": tf.map_fn(tf.image.encode_png, converted_targets, dtype=tf.string, name=\"target_pngs\"),\n            \"outputs\": tf.map_fn(tf.image.encode_png, converted_outputs, dtype=tf.string, name=\"output_pngs\"),\n        }\n\n    # summaries\n    with tf.name_scope(\"inputs_summary\"):\n        tf.summary.image(\"inputs\", converted_inputs)\n\n    with tf.name_scope(\"targets_summary\"):\n        tf.summary.image(\"targets\", converted_targets)\n\n    with tf.name_scope(\"outputs_summary\"):\n        tf.summary.image(\"outputs\", converted_outputs)\n\n    with tf.name_scope(\"predict_real_summary\"):\n        tf.summary.image(\"predict_real\", tf.image.convert_image_dtype(model.predict_real, dtype=tf.uint8))\n\n    with tf.name_scope(\"predict_fake_summary\"):\n        tf.summary.image(\"predict_fake\", tf.image.convert_image_dtype(model.predict_fake, dtype=tf.uint8))\n\n    tf.summary.scalar(\"discriminator_loss\", model.discrim_loss)\n    tf.summary.scalar(\"generator_loss_GAN\", model.gen_loss_GAN)\n    tf.summary.scalar(\"generator_loss_L1\", model.gen_loss_L1)\n\n    for var in tf.trainable_variables():\n        tf.summary.histogram(var.op.name + \"/values\", var)\n\n    for grad, var in model.discrim_grads_and_vars + model.gen_grads_and_vars:\n        tf.summary.histogram(var.op.name + \"/gradients\", grad)\n\n    with tf.name_scope(\"parameter_count\"):\n        parameter_count = tf.reduce_sum([tf.reduce_prod(tf.shape(v)) for v in tf.trainable_variables()])\n\n    saver = tf.train.Saver(max_to_keep=1)\n\n    logdir = a.output_dir if (a.trace_freq > 0 or a.summary_freq > 0) else None\n    sv = tf.train.Supervisor(logdir=logdir, save_summaries_secs=0, saver=None)\n    with sv.managed_session() as sess:\n        print(\"parameter_count =\", sess.run(parameter_count))\n\n        if a.checkpoint is not None:\n            print(\"loading model from checkpoint\")\n            checkpoint = tf.train.latest_checkpoint(a.checkpoint)\n            saver.restore(sess, checkpoint)\n\n        max_steps = 2**32\n        if a.max_epochs is not None:\n            max_steps = examples.steps_per_epoch * a.max_epochs\n        if a.max_steps is not None:\n            max_steps = a.max_steps\n\n        if a.mode == \"test\":\n            # testing\n            # at most, process the test data once\n            start = time.time()\n            max_steps = min(examples.steps_per_epoch, max_steps)\n            for step in range(max_steps):\n                results = sess.run(display_fetches)\n                filesets = save_images(results)\n                for i, f in enumerate(filesets):\n                    print(\"evaluated image\", f[\"name\"])\n                index_path = append_index(filesets)\n            print(\"wrote index at\", index_path)\n            print(\"rate\", (time.time() - start) / max_steps)\n        else:\n            # training\n            start = time.time()\n\n            for step in range(max_steps):\n                def should(freq):\n                    return freq > 0 and ((step + 1) % freq == 0 or step == max_steps - 1)\n\n                options = None\n                run_metadata = None\n                if should(a.trace_freq):\n                    options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n                    run_metadata = tf.RunMetadata()\n\n                fetches = {\n                    \"train\": model.train,\n                    \"global_step\": sv.global_step,\n                }\n\n                if should(a.progress_freq):\n                    fetches[\"discrim_loss\"] = model.discrim_loss\n                    fetches[\"gen_loss_GAN\"] = model.gen_loss_GAN\n                    fetches[\"gen_loss_L1\"] = model.gen_loss_L1\n\n                if should(a.summary_freq):\n                    fetches[\"summary\"] = sv.summary_op\n\n                if should(a.display_freq):\n                    fetches[\"display\"] = display_fetches\n\n                results = sess.run(fetches, options=options, run_metadata=run_metadata)\n\n                if should(a.summary_freq):\n                    print(\"recording summary\")\n                    sv.summary_writer.add_summary(results[\"summary\"], results[\"global_step\"])\n\n                if should(a.display_freq):\n                    print(\"saving display images\")\n                    filesets = save_images(results[\"display\"], step=results[\"global_step\"])\n                    append_index(filesets, step=True)\n\n                if should(a.trace_freq):\n                    print(\"recording trace\")\n                    sv.summary_writer.add_run_metadata(run_metadata, \"step_%d\" % results[\"global_step\"])\n\n                if should(a.progress_freq):\n                    # global_step will have the correct step count if we resume from a checkpoint\n                    train_epoch = math.ceil(results[\"global_step\"] / examples.steps_per_epoch)\n                    train_step = (results[\"global_step\"] - 1) % examples.steps_per_epoch + 1\n                    rate = (step + 1) * a.batch_size / (time.time() - start)\n                    remaining = (max_steps - step) * a.batch_size / rate\n                    print(\"progress  epoch %d  step %d  image/sec %0.1f  remaining %dm\" % (train_epoch, train_step, rate, remaining / 60))\n                    print(\"discrim_loss\", results[\"discrim_loss\"])\n                    print(\"gen_loss_GAN\", results[\"gen_loss_GAN\"])\n                    print(\"gen_loss_L1\", results[\"gen_loss_L1\"])\n\n                if should(a.save_freq):\n                    print(\"saving model\")\n                    saver.save(sess, os.path.join(a.output_dir, \"model\"), global_step=sv.global_step)\n\n                if sv.should_stop():\n                    break\n\n\nmain()\n"
        },
        {
          "name": "server",
          "type": "tree",
          "content": null
        },
        {
          "name": "tools",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}