{
  "metadata": {
    "timestamp": 1736561888122,
    "page": 571,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjU4MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "0hq/WebGPT",
      "stars": 3666,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0.083984375,
          "content": "*.bin filter=lfs diff=lfs merge=lfs -text\n*.json filter=lfs diff=lfs merge=lfs -text\n "
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.01953125,
          "content": "weights/large-models"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.205078125,
          "content": "IFCOOLTELLME License\n\nCopyright (c) 2023 Will DePue\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nIf this software is used for any purpose that is substantially epic, awesome, or\nincredible, notice is required to the Author, reachable at will@depue.net. \n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 3.3818359375,
          "content": "# WebGPT\n\n![webGPT](other/misc/header.png)\n\nAfter six years of development, WebGPU is about to launch across most major web browsers. This is massive: web applications now have near-native access to the GPU, with the added capacity of compute shaders.\n\nWebGPT is a vanilla JS and HTML implementation of a transformer model, intended as a proof-of-concept as well as educational resource. WebGPT has been tested to be working with models up to 500 M parameters, though could likely support far more with further testing/optimization.\n\n### Current Stats\n2020 M1 Mac: 3ms/token at 5M parameters with f32 precision.  \n2020 M1 Mac: 30ms/token at 117M parameters with f32 precision.  \n2020 M1 Mac: 70ms/token at 377M parameters with f32 precision.  \n2020 M1 Mac: 120ms/token at 775M parameters with f32 precision.  \n1.5B is working but unstable, sitting around 1000ms/token due to inefficiencies.  \n\n## Running WebGPT\n\nRunning WebGPT is remarkably simple, as it's just a set of HTML + JS files. Since WebGPU is still in the process of being released, you'll need to open with a compatible browser. WebGPU is currently available on Chrome v113 but the most straightforward way to ensure proper functionality is to install [Chrome Canary](https://www.google.com/chrome/canary/) or Edge Canary.\n\nI've included two different models: a toy GPT-Shakespeare model (which is severly undertrained haha) and GPT-2 117M. See main.js for more information on how to run these models. If you want to import custom models, take a look at misc/conversion_scripts.\n\nIf you want to try out WebGPT, visit the demo website here [KMeans.org](https://www.kmeans.org). I'd generally reccomend cloning the repo and running locally, just because loading the weights remotely is significantly slower.  \nNote: **You'll need to use Git LFS** to download the model files, after cloning the repository.\n\n![file sizes](other/misc/files.png)\n\n## Roadmap / Fixing Stupid Decisions\n\n- [x] Embeddings / de-embeddings on GPU.\n- [x] Initializing pipelines on every step is incredibly inefficient.\n- [x] Key-value caching.\n- [x] Reuse buffers.\n- [x] Kernel shared memory for matmul!\n- [x] Destroy buffers after use!\n- [x] Create kernel instruction classes + optimize pipeline creation.\n- [X] Fuse all kernels.\n- [X] Optimize all other kernels.\n- [X] Compute pass splitting for larger models _(maxStorageBufferBindingSize)_\n- [ ] Run selection ops on GPU (topk, selection softmax)\n- [ ] Attention kernel is optimized for small models, not for large models where each head having it's own matmul is more efficient.\n- [ ] Investigate why attention cache isn't giving proper speed-ups.\n- [ ] Make simple instructional version without special stuff.\n- [ ] Optimize workgroup sizes, specifically for single row/col operations.\n- [ ] Convert into a package.\n- [ ] Write better comments + make Youtube explainer.\n\n## Acknowledgements\n\nWhen I started this project I had no idea how transformers worked or how to implement them (or GPUs or matmul kernels or WebGPU or tokenization for that matter), so Andrej Karpathy's series on neural networks and building GPT from scratch were invaluable: [Andrej's Youtube](https://www.youtube.com/@AndrejKarpathy). I've also used some code as well from the nanoGPT repository: [nanoGPT](https://github.com/karpathy/nanoGPT).\n\nI copied from LatitudeGames' implementation of OpenAI's GPT-3 tokenizer in Javascript: [GPT-3-Encoder](https://github.com/latitudegames/GPT-3-Encoder).\n"
        },
        {
          "name": "condensed",
          "type": "tree",
          "content": null
        },
        {
          "name": "globals.js",
          "type": "blob",
          "size": 3.185546875,
          "content": "const FastMatMulBlock = new FastMatMulBlockClass();\nconst AttentionBlock = new AttentionBlockClass();\nconst ResidualBlock = new ResidualBlockClass();\nconst EmbedBlock = new EmbedBlockClass();\nconst DeEmbedBlock = new DeEmbedBlockClass();\nconst GeluBlock = new GeluBlockClass();\nconst LayerNormBlock = new LayerNormBlockClass();\nconst SoftmaxBlock = new SoftmaxBlockClass();\n\n// Needed for deletion.\nlet operations = [FastMatMulBlock, AttentionBlock, ResidualBlock, EmbedBlock, DeEmbedBlock, GeluBlock, LayerNormBlock, SoftmaxBlock];\n\nfunction initializeOperations(device) {\n  for (const operation of operations) operation.initialize(device);\n}\n\nfunction destroyOperationBuffers() {\n  for (const operation of operations) operation.destroyBuffers();\n}\n\nfunction clearOperationCache() {\n  for (const operation of operations) operation.clearBufferCache();\n}\n\nfunction destroyOperations() {\n  for (const operation of operations) operation.destroy();\n}\n\nconst bufferUsageDict = {\n  copy_from: GPUBufferUsage.COPY_SRC,\n  copy_to: GPUBufferUsage.COPY_DST,\n  storage: GPUBufferUsage.STORAGE,\n  uniform: GPUBufferUsage.UNIFORM,\n  map_read: GPUBufferUsage.MAP_READ,\n};\n\n// ---------------- Helper Functions ----------------\n\nasync function fetchBin(url) {\n  const response = await fetch(url);\n  const buffer = await response.arrayBuffer();\n  return new Float32Array(buffer);\n}\n\nconst wgSize = (dim, size) => Math.min(Math.ceil(dim / size), Infinity);\n\nfunction sampleFromDistribution(probs) {\n  const rand = Math.random();\n  let cumulativeProb = 0;\n  for (let i = 0; i < probs.length; i++) {\n    cumulativeProb += probs[i];\n    if (rand < cumulativeProb) {\n      return i;\n    }\n  }\n  return probs.length - 1;\n}\n\nfunction cpuSoftmax(logits, temperature = 1.0) {\n  const maxLogit = Math.max(...logits);\n  const expLogits = logits.map((logit) => Math.exp((logit - maxLogit) / temperature));\n  const sumExpLogits = expLogits.reduce((a, b) => a + b, 0);\n  return expLogits.map((expLogit) => expLogit / sumExpLogits);\n}\n\nfunction selectTopK(probs, top_k) {\n  const sortedIndices = Array.from(probs)\n    .map((value, index) => ({ value, index }))\n    .sort((a, b) => b.value - a.value)\n    .map(({ index }) => index);\n  const topKIndices = sortedIndices.slice(0, top_k);\n  const topKProbs = topKIndices.map((index) => probs[index]);\n  return { topKIndices, topKProbs };\n}\n\n// ----------------------- Matrix Operations -----------------------\n\nconst zeros = (dim) => new Float32Array(dim).fill(0);\n\nfunction transpose(array, input_rows, input_cols) {\n  if (array.length !== input_rows * input_cols) {\n    console.log(array.length, input_rows, input_cols);\n    throw new Error(\"Transpose dims failed\");\n  }\n\n  const transpose = [];\n  for (let col = 0; col < input_cols; col++) {\n    for (let row = 0; row < input_rows; row++) {\n      transpose.push(array[row * input_cols + col]);\n    }\n  }\n\n  return new Float32Array(transpose);\n}\n\nfunction leastPrimeFactor(n, start = 2) {\n  for (let i = start; i <= Math.sqrt(n); i++) {\n    if (n % i === 0) return i;\n  }\n  return n;\n}\n\nfunction formatAsMatrix(floatArray, dimA, dimB) {\n  const resultMatrix = [];\n  for (let i = 0; i < dimA; i++) {\n    resultMatrix.push(floatArray.slice(i * dimB, (i + 1) * dimB));\n  }\n  return resultMatrix;\n}\n"
        },
        {
          "name": "index.html",
          "type": "blob",
          "size": 6.3271484375,
          "content": "<!DOCTYPE html>\n<html>\n  <head>\n    <title>WebGPU GPT Model Demo</title>\n    <meta\n      http-equiv=\"origin-trial\"\n      content=\"Anx9P4m5tzLOL/wLICKy/mA+DRyoSsTkyqmnK5t+S7oyw7A2SeBI2jO4LKqoQiQgChP2MTtqMNKofelMwvGtPQsAAABKeyJvcmlnaW4iOiJodHRwczovL2ttZWFucy5vcmc6NDQzIiwiZmVhdHVyZSI6IldlYkdQVSIsImV4cGlyeSI6MTY5MTcxMTk5OX0=\"\n    />\n    <script src=\"tokenizer.js\"></script>\n    <script src=\"instructions.js\"></script>\n    <script src=\"model.js\"></script>\n    <script src=\"visuals.js\"></script>\n    <script src=\"globals.js\"></script>\n  </head>\n  <body>\n    <h1>WebGPU GPT Model Demo</h1>\n    <p id=\"webgpuSupportMessage\">Checking WebGPU support...</p>\n    <p>\n      <i>PS: Loading models is 5x slower on the web rather than running locally. Just <a href=\"https://github.com/0hq/WebGPT\">clone the repo</a> and open!</i>\n    </p>\n\n    <button class=\"loadModelButton\" onclick=\"loadModel('better_shakespeare', 'char')\" disabled>Load Shakespeare Model</button>\n    <button class=\"loadModelButton\" onclick=\"loadModel('gpt2', 'bpe')\" disabled>Load GPT2 117M Model</button>\n    <button id=\"destroyCacheButton\" onclick=\"destroyCache()\" disabled>Destroy Cache</button>\n\n    <p>\n      <i>Special models (download required):</i>\n    </p>\n    <button class=\"loadModelButton\" onclick=\"loadModel('large-models/gpt2-medium', 'bpe')\" disabled>Load GPT2 377M Model</button>\n    <button class=\"loadModelButton\" onclick=\"loadModel('large-models/gpt2-large', 'bpe')\" disabled>Load GPT2 777M Model</button>\n    <button class=\"loadModelButton\" onclick=\"loadModel('large-models/gpt2-xl', 'bpe')\" disabled>Load GPT2 1.5B Model</button>\n\n    <br />\n    <br />\n    <label for=\"tokens\">Number of tokens:</label>\n    <input type=\"number\" min=\"1\" max=\"300\" value=\"100\" id=\"tokensInput\" disabled />\n    <br /><br />\n    <label for=\"topK\">Top K:</label>\n    <input type=\"number\" min=\"1\" max=\"100\" value=\"2\" id=\"topKInput\" disabled />\n    <br /><br />\n    <label for=\"temperature\">Temperature:</label>\n    <input type=\"number\" step=\"0.01\" min=\"0.1\" max=\"2\" value=\"1\" id=\"temperatureInput\" disabled />\n    <br /><br />\n    <button id=\"generateButton\" onclick=\"startGeneration()\" disabled>Generate Text</button>\n    <br /><br />\n    <textarea id=\"prompt\" rows=\"15\" cols=\"50\" disabled>\nWILL:\nAh, how dare you challenge me?\nHave you forgotten I built WebGPT?&#13;&#10;</textarea\n    >\n    <br /><br />\n    <button id=\"updateVisuals\" onclick=\"updateVisuals()\" disabled>Force Update Visuals</button>\n    <br /><br />\n    <div id=\"visualsContainer\"></div>\n    <script>\n      const webgpuSupportMessage = document.getElementById(\"webgpuSupportMessage\");\n      const loadModelButton = document.getElementsByClassName(\"loadModelButton\");\n      const setModelButtonDisabled = (bool) => {\n        for (let i = 0; i < loadModelButton.length; i++) loadModelButton[i].disabled = bool;\n      };\n      const destroyCacheButton = document.getElementById(\"destroyCacheButton\");\n\n      const tokensInput = document.getElementById(\"tokensInput\");\n      const topKInput = document.getElementById(\"topKInput\");\n      const temperatureInput = document.getElementById(\"temperatureInput\");\n      const generateButton = document.getElementById(\"generateButton\");\n      const promptTextarea = document.getElementById(\"prompt\");\n      const updateVisualsButton = document.getElementById(\"updateVisuals\");\n\n      let GPTModel = null;\n      let visuals = null;\n\n      // Check for WebGPU support\n      if (!navigator.gpu) {\n        webgpuSupportMessage.innerHTML =\n          \"WebGPU is not supported in your browser yet. Update Chrome to v113 or download <a href='https://www.google.com/chrome/canary/'>Chrome Canary</a>. Also available on <a href='https://www.microsoftedgeinsider.com/en-us/download/canary'>Edge Canary</a>.\";\n        console.error(\"WebGPU is not supported\");\n      } else {\n        webgpuSupportMessage.innerHTML = \"WebGPU is supported in your browser!\";\n        setModelButtonDisabled(false);\n      }\n\n      async function startGeneration() {\n        setTextareaDisabled(true);\n\n        const prompt = promptTextarea.value || \" \";\n        const numTokens = tokensInput.value;\n\n        const topK = topKInput.value;\n        const temperature = temperatureInput.value;\n        const textStream = GPTModel.generate(prompt, numTokens, topK, temperature);\n\n        for await (const text of textStream) {\n          promptTextarea.value += text;\n          visuals.render();\n        }\n\n        setTextareaDisabled(false);\n      }\n\n      async function loadModel(folder, tokenizer) {\n        setModelButtonDisabled(true);\n\n        GPTModel = new GPT(folder, tokenizer);\n        await GPTModel.initialize();\n\n\n        promptTextarea.value = GPTModel.defaultPrompt;\n        topKInput.value = GPTModel.defaultTopK;\n        tokensInput.value = GPTModel.defaultTokens;\n        temperatureInput.value = GPTModel.defaultTemperature;\n\n        setTextareaDisabled(false);\n        tokensInput.disabled = false;\n        topKInput.disabled = false;\n        temperatureInput.disabled = false;\n\n        destroyCacheButton.disabled = false;\n        updateVisualsButton.disabled = false;\n\n        if (!visuals) {\n          visuals = new Visuals(GPTModel);\n          visuals.init();\n          visuals.render();\n        }\n      }\n\n      function setTextareaDisabled(bool) {\n        promptTextarea.disabled = bool;\n        generateButton.disabled = bool;\n      }\n\n      async function continueGeneration() {\n        setTextareaDisabled(true);\n\n        const prompt = outputDiv.innerHTML || \" \";\n        const numTokens = tokensInput.value;\n\n        outputDiv.innerHTML = prompt;\n\n        const topK = topKInput.value;\n        const temperature = temperatureInput.value;\n        const textStream = generate(prompt, numTokens, 10, topK, temperature);\n\n        for await (const text of textStream) {\n          outputDiv.innerHTML += text;\n        }\n\n        setTextareaDisabled(false);\n      }\n\n      function destroyCache() {\n        GPTModel.unloadBuffers();\n        destroyOperations();\n\n        GPTModel = null;\n\n        visuals.destroy();\n        visuals = null;\n\n        setModelButtonDisabled(false);\n\n        destroyCacheButton.disabled = true;\n        tokensInput.disabled = true;\n        topKInput.disabled = true;\n        temperatureInput.disabled = true;\n        updateVisualsButton.disabled = true;\n        setTextareaDisabled(true);\n      }\n\n      function updateVisuals() {\n          visuals.render();\n      }\n    </script>\n  </body>\n</html>\n"
        },
        {
          "name": "instructions.js",
          "type": "blob",
          "size": 41.1650390625,
          "content": "// --------------------- Instructions --------------------- //\n\n// All have been vectorized, but not all have been optimized with shared memory.\n// Soon: quantize to int8, possible int4.\n// Tuning needed and more detailed benchmarking, but decent for now.\n\nclass Block {\n  constructor() {\n    this.name = \"\";\n  }\n\n  initialize(device) {\n    this.device = device;\n    this.initBindGroups();\n    this.pipelineCache = new Map();\n    this.boundBufferCache = new Map();\n    this.unboundBufferCache = new Map();\n  }\n\n  initBindGroup(layout, buffers, label = \"\") {\n    return this.device.createBindGroup({\n      layout,\n      entries: buffers.map((buffer, i) => ({\n        binding: i,\n        resource: { buffer },\n      })),\n      label,\n    });\n  }\n\n  initUniform(size, values) {\n    const key = `uniform_${values.map((v) => v[1].toString()).join(\",\")}`;\n    if (this.unboundBufferCache.has(key)) {\n      const buffer = this.unboundBufferCache.get(key).pop();\n      if (this.unboundBufferCache.get(key).length === 0) this.unboundBufferCache.delete(key);\n      return buffer;\n    }\n    const buffer = this.device.createBuffer({\n      size: size * 4,\n      usage: bufferUsageDict[\"uniform\"],\n      mappedAtCreation: true,\n    });\n    const mappedRange = buffer.getMappedRange();\n    for (const value of values) {\n      if (value[1].constructor.name === \"Float32Array\") {\n        new Float32Array(mappedRange, value[0]).set(value[1]);\n      } else if (value[1].constructor.name === \"Uint32Array\") {\n        new Uint32Array(mappedRange, value[0]).set(value[1]);\n      }\n    }\n    buffer.unmap();\n    if (!this.boundBufferCache.has(key)) this.boundBufferCache.set(key, []);\n    this.boundBufferCache.get(key).push(buffer);\n    return buffer;\n  }\n\n  initResultBuffer(dims) {\n    const key = `result_${dims.join(\",\")}`;\n    if (this.unboundBufferCache.has(key)) {\n      const buffer = this.unboundBufferCache.get(key).pop();\n      if (this.unboundBufferCache.get(key).length === 0) this.unboundBufferCache.delete(key);\n      return buffer;\n    }\n    return this.initBuffer([\"storage\", \"copy_from\"], dims, key);\n  }\n\n  initBuffer(ops, dims, key = \"\") {\n    const buffer = this.device.createBuffer({\n      size: this.bufferSize(dims[0], dims[1] || 1, dims[2] || 1),\n      usage: ops.map((u) => bufferUsageDict[u]).reduce((a, b) => a | b),\n    });\n    if (!this.boundBufferCache.has(key)) this.boundBufferCache.set(key, []);\n    this.boundBufferCache.get(key).push(buffer);\n    return buffer;\n  }\n\n  bufferSize(dimA, dimB = 1) {\n    const size = Math.ceil((dimA * dimB * Float32Array.BYTES_PER_ELEMENT) / 1) * 1;\n    if (size > this.device.limits.maxStorageBufferBindingSize) throw new Error(\"Buffer size exceeds GPU limit.\");\n    return size;\n  }\n\n  // Could be removed with auto bind groups, currently initializing everytime so probably slowing things down.\n  initBindGroups() {\n    const bg = (types) =>\n      this.device.createBindGroupLayout({\n        entries: types.map((entry, i) => ({\n          binding: i,\n          visibility: GPUShaderStage.COMPUTE,\n          buffer: { type: entry },\n        })),\n      });\n\n    this.r_r_r_r_Layout = bg([\"read-only-storage\", \"read-only-storage\", \"read-only-storage\", \"read-only-storage\"]);\n    this.r_r_r_Layout = bg([\"read-only-storage\", \"read-only-storage\", \"read-only-storage\"]);\n    this.r_r_Layout = bg([\"read-only-storage\", \"read-only-storage\"]);\n    this.r_Layout = bg([\"read-only-storage\"]);\n    this.u_s_Layout = bg([\"uniform\", \"storage\"]);\n    this.u_s_s_s_Layout = bg([\"uniform\", \"storage\", \"storage\", \"storage\"]);\n  }\n\n  initPipeline(code, bindGroupLayouts, label = \"\", constants = {}) {\n    return this.device.createComputePipeline({\n      layout: this.device.createPipelineLayout({ bindGroupLayouts }),\n      compute: {\n        module: this.device.createShaderModule({ code }),\n        entryPoint: \"main\",\n        constants,\n      },\n      label,\n    });\n  }\n\n  destroy() {\n    this.destroyPipelineCache();\n    this.destroyBuffers();\n  }\n\n  destroyPipelineCache() {\n    this.pipelineCache = null;\n  }\n\n  clearBufferCache() {\n    // delete all unbound buffers\n    this.unboundBufferCache.forEach((buffers) => buffers.map((buffer) => buffer.destroy()));\n    // set unbound buffers to bound buffers\n    this.unboundBufferCache = this.boundBufferCache;\n    // clear bound buffers\n    this.boundBufferCache = new Map();\n  }\n\n  destroyBuffers() {\n    this.unboundBufferCache.forEach((buffers) => buffers.map((buffer) => buffer.destroy()));\n    this.unboundBufferCache = new Map();\n\n    this.boundBufferCache.forEach((buffers) => buffers.map((buffer) => buffer.destroy()));\n    this.boundBufferCache = new Map();\n  }\n}\n\nclass FastMatMulBlockClass extends Block {\n  constructor() {\n    super();\n    this.name = \"fastMatMul\";\n  }\n\n  getPipeline(rows) {\n    const settings = rows % 4 !== 0 ? \"withCheck\" : \"noCheck\";\n    const pipelineCacheKey = `${this.name}_${settings}}`;\n    if (this.pipelineCache.has(pipelineCacheKey)) return this.pipelineCache.get(pipelineCacheKey);\n    const kernel = this.fastMatMul(settings);\n    const pipeline = this.initPipeline(kernel, [this.u_s_Layout, this.r_r_r_Layout], `${this.name}_Pipeline_${pipelineCacheKey}`);\n    this.pipelineCache.set(pipelineCacheKey, pipeline);\n    return pipeline;\n  }\n\n  newInstance(rows, cols, shared, inputBuffer, weightsBuffer, biasBuffer) {\n    if (cols % 8 !== 0) throw new Error(\"Cols must be divisible by 16.\"); // Is this not 8? Or is it 16?\n    const pipeline = this.getPipeline(rows);\n    const uniformBuffer = this.initUniform(4, [[0, new Uint32Array([rows, cols, Math.ceil(cols / 4), Math.ceil(shared / 4)])]]);\n    const resultBuffer = this.initResultBuffer([rows, cols]);\n    const opBindGroup = this.initBindGroup(this.u_s_Layout, [uniformBuffer, resultBuffer], `${this.name}_OpG`);\n    const inputBindGroup = this.initBindGroup(this.r_r_r_Layout, [inputBuffer, weightsBuffer, biasBuffer], `${this.name}_InputG`);\n    const workgroups = { x: wgSize(cols, 8 * 8), y: wgSize(rows, 4 * 8) };\n\n    return {\n      resultBuffer,\n      passes: [\n        {\n          flag: \"compute\",\n          pipeline,\n          groups: [opBindGroup, inputBindGroup],\n          workgroups,\n        },\n      ],\n    };\n  }\n\n  fastMatMul(flag) {\n    const outputCode = {\n      withCheck: `\n      if (y * 4u + 0u < M) {\n        array_c[x * 2u + 0u + (y * 4u + 0u) * ND4] = sum00;\n        array_c[x * 2u + 1u + (y * 4u + 0u) * ND4] = sum10;\n      }\n      if (y * 4u + 1u < M) {\n        array_c[x * 2u + 0u + (y * 4u + 1u) * ND4] = sum01;\n        array_c[x * 2u + 1u + (y * 4u + 1u) * ND4] = sum11;\n      }\n      if (y * 4u + 2u < M) {\n        array_c[x * 2u + 0u + (y * 4u + 2u) * ND4] = sum02;\n        array_c[x * 2u + 1u + (y * 4u + 2u) * ND4] = sum12;\n      }\n      if (y * 4u + 3u < M) {\n        array_c[x * 2u + 0u + (y * 4u + 3u) * ND4] = sum03;\n        array_c[x * 2u + 1u + (y * 4u + 3u) * ND4] = sum13;\n      }\n    `,\n      noCheck: `\n      array_c[x * 2u + 0u + (y * 4u + 0u) * ND4] = sum00;\n      array_c[x * 2u + 1u + (y * 4u + 0u) * ND4] = sum10;\n      array_c[x * 2u + 0u + (y * 4u + 1u) * ND4] = sum01;\n      array_c[x * 2u + 1u + (y * 4u + 1u) * ND4] = sum11;\n      array_c[x * 2u + 0u + (y * 4u + 2u) * ND4] = sum02;\n      array_c[x * 2u + 1u + (y * 4u + 2u) * ND4] = sum12;\n      array_c[x * 2u + 0u + (y * 4u + 3u) * ND4] = sum03;\n      array_c[x * 2u + 1u + (y * 4u + 3u) * ND4] = sum13;\n    `,\n    };\n\n    return `\n    struct Meta {\n      M: u32,\n      N: u32,\n      ND4: u32,\n      KD4: u32,\n    }\n\n    @group(1) @binding(0) var<storage,read> array_a: array<vec4<f32>>;\n    @group(1) @binding(1) var<storage,read> array_b: array<vec4<f32>>;\n    @group(1) @binding(2) var<storage,read> array_bias: array<vec4<f32>>;\n\n    @group(0) @binding(0) var<uniform> uniforms: Meta;\n    @group(0) @binding(1) var<storage,read_write> array_c: array<vec4<f32>>;\n\n    @compute @workgroup_size(8, 8)\n    fn main(@builtin(global_invocation_id) global_id: vec3<u32>) {\n      var M: u32 = uniforms.M;\n      var N: u32 = uniforms.N;\n      var ND4: u32 = uniforms.ND4;\n      var KD4: u32 = uniforms.KD4;\n      var x: u32 = global_id.x;\n      var y: u32 = global_id.y;\n\n      if (x * 8 >= N || y * 4 >= M) {\n        return;\n      }\n\n      var sum00: vec4<f32> = vec4<f32>();\n      var sum01: vec4<f32> = vec4<f32>();\n      var sum02: vec4<f32> = vec4<f32>();\n      var sum03: vec4<f32> = vec4<f32>();\n      var sum10: vec4<f32> = vec4<f32>();\n      var sum11: vec4<f32> = vec4<f32>();\n      var sum12: vec4<f32> = vec4<f32>();\n      var sum13: vec4<f32> = vec4<f32>();\n\n      for(var k: u32 = 0u; k < KD4; k = k + 1u) {\n        var arow0: vec4<f32> = array_a[(y * 4u + 0u) * KD4 + k];\n        var arow1: vec4<f32> = array_a[(y * 4u + 1u) * KD4 + k];\n        var arow2: vec4<f32> = array_a[(y * 4u + 2u) * KD4 + k];\n        var arow3: vec4<f32> = array_a[(y * 4u + 3u) * KD4 + k];\n        var brow: vec4<f32>;\n\n        brow = array_b[(k * 4u + 0u) * ND4 + x * 2u + 0u];\n        sum00 = vec4<f32>(arow0.x) * brow + sum00;\n        sum01 = vec4<f32>(arow1.x) * brow + sum01;\n        sum02 = vec4<f32>(arow2.x) * brow + sum02;\n        sum03 = vec4<f32>(arow3.x) * brow + sum03;\n\n        brow = array_b[(k * 4u + 0u) * ND4 + x * 2u + 1u];\n        sum10 = vec4<f32>(arow0.x) * brow + sum10;\n        sum11 = vec4<f32>(arow1.x) * brow + sum11;\n        sum12 = vec4<f32>(arow2.x) * brow + sum12;\n        sum13 = vec4<f32>(arow3.x) * brow + sum13;\n\n        brow = array_b[(k * 4u + 1u) * ND4 + x * 2u + 0u];\n        sum00 = vec4<f32>(arow0.y) * brow + sum00;\n        sum01 = vec4<f32>(arow1.y) * brow + sum01;\n        sum02 = vec4<f32>(arow2.y) * brow + sum02;\n        sum03 = vec4<f32>(arow3.y) * brow + sum03;\n\n        brow = array_b[(k * 4u + 1u) * ND4 + x * 2u + 1u];\n        sum10 = vec4<f32>(arow0.y) * brow + sum10;\n        sum11 = vec4<f32>(arow1.y) * brow + sum11;\n        sum12 = vec4<f32>(arow2.y) * brow + sum12;\n        sum13 = vec4<f32>(arow3.y) * brow + sum13;\n\n        brow = array_b[(k * 4u + 2u) * ND4 + x * 2u + 0u];\n        sum00 = vec4<f32>(arow0.z) * brow + sum00;\n        sum01 = vec4<f32>(arow1.z) * brow + sum01;\n        sum02 = vec4<f32>(arow2.z) * brow + sum02;\n        sum03 = vec4<f32>(arow3.z) * brow + sum03;\n\n        brow = array_b[(k * 4u + 2u) * ND4 + x * 2u + 1u];\n        sum10 = vec4<f32>(arow0.z) * brow + sum10;\n        sum11 = vec4<f32>(arow1.z) * brow + sum11;\n        sum12 = vec4<f32>(arow2.z) * brow + sum12;\n        sum13 = vec4<f32>(arow3.z) * brow + sum13;\n\n        brow = array_b[(k * 4u + 3u) * ND4 + x * 2u + 0u];\n        sum00 = vec4<f32>(arow0.w) * brow + sum00;\n        sum01 = vec4<f32>(arow1.w) * brow + sum01;\n        sum02 = vec4<f32>(arow2.w) * brow + sum02;\n        sum03 = vec4<f32>(arow3.w) * brow + sum03;\n\n        brow = array_b[(k * 4u + 3u) * ND4 + x * 2u + 1u];\n        sum10 = vec4<f32>(arow0.w) * brow + sum10;\n        sum11 = vec4<f32>(arow1.w) * brow + sum11;\n        sum12 = vec4<f32>(arow2.w) * brow + sum12;\n        sum13 = vec4<f32>(arow3.w) * brow + sum13;\n      }\n\n      var array_bias_1: vec4<f32> = array_bias[x * 2u + 0u];\n      sum00 = sum00 + array_bias_1;\n      sum01 = sum01 + array_bias_1;\n      sum02 = sum02 + array_bias_1;\n      sum03 = sum03 + array_bias_1;\n\n      var array_bias_2: vec4<f32> = array_bias[x * 2u + 1u];\n      sum10 = sum10 + array_bias_2;\n      sum11 = sum11 + array_bias_2;\n      sum12 = sum12 + array_bias_2;\n      sum13 = sum13 + array_bias_2;\n\n      ${outputCode[flag]}\n    }\n  `;\n  }\n}\n\nclass ResidualBlockClass extends Block {\n  constructor() {\n    super();\n    this.name = \"residual\";\n  }\n\n  getPipeline() {\n    const pipelineCacheKey = this.name; // No param optimization.\n    if (this.pipelineCache.has(pipelineCacheKey)) return this.pipelineCache.get(pipelineCacheKey);\n    const pipeline = this.initPipeline(this.elementWiseAdditionShader, [this.u_s_Layout, this.r_r_Layout], `${this.name}_Pipeline`);\n    this.pipelineCache.set(pipelineCacheKey, pipeline);\n    return pipeline;\n  }\n\n  newInstance(rows, cols, outputBuf, residualBuf) {\n    if (cols % 4 !== 0) throw new Error(\"Cols must be divisible by 4.\");\n    const pipeline = this.getPipeline();\n    const uniformBuffer = this.initUniform(4, [[0, new Uint32Array([rows, Math.ceil(cols / 4)])]]);\n    const resultBuffer = this.initResultBuffer([rows, cols]);\n    const opBindGroup = this.initBindGroup(this.u_s_Layout, [uniformBuffer, resultBuffer], `${this.name}_OpG`);\n    const inputBindGroup = this.initBindGroup(this.r_r_Layout, [outputBuf, residualBuf], `${this.name}_InputG`);\n    const workgroups = { x: wgSize(cols, 32), y: wgSize(rows, 8), z: 1 };\n\n    return {\n      resultBuffer,\n      passes: [\n        {\n          flag: \"compute\",\n          pipeline,\n          groups: [opBindGroup, inputBindGroup],\n          workgroups,\n        },\n      ],\n    };\n  }\n\n  elementWiseAdditionShader = `\n    struct Meta {\n      M: u32,\n      ND4: u32,\n    }\n\n    @group(1) @binding(0) var<storage, read> layer_out_array: array<vec4<f32>>;\n    @group(1) @binding(1) var<storage, read> residual_array: array<vec4<f32>>;\n\n    @group(0) @binding(0) var<uniform> uniforms: Meta;\n    @group(0) @binding(1) var<storage, read_write> result_array: array<vec4<f32>>;\n\n    @compute @workgroup_size(8, 8)\n    fn main (@builtin(global_invocation_id) global_id: vec3<u32>) {\n      var col: u32 = global_id.x;\n      var row: u32 = global_id.y;\n      var ND4: u32 = uniforms.ND4;\n      var M: u32 = uniforms.M;\n\n      if (row >= M || col >= ND4) {\n        return;\n      }\n      \n      let index = row * ND4 + col;\n      result_array[index] =  layer_out_array[index] + residual_array[index];\n    }\n  `;\n}\n\nclass LayerNormBlockClass extends Block {\n  constructor() {\n    super();\n    this.name = \"layerNorm\";\n  }\n\n  getStatsPipeline(workgroups) {\n    const pipelineCacheKey = `${this.name}_stats_${workgroups}`;\n    if (this.pipelineCache.has(pipelineCacheKey)) return this.pipelineCache.get(pipelineCacheKey);\n    const pipeline = this.initPipeline(this.normStatsShader(workgroups), [this.u_s_Layout, this.r_Layout], `${this.name}_Pipeline_Stats`);\n    this.pipelineCache.set(pipelineCacheKey, pipeline);\n    return pipeline;\n  }\n\n  getNormPipeline() {\n    const pipelineCacheKey = `${this.name}_norm`; // No param optimization.\n    if (this.pipelineCache.has(pipelineCacheKey)) return this.pipelineCache.get(pipelineCacheKey);\n    const pipeline = this.initPipeline(this.normShader, [this.u_s_Layout, this.r_r_r_r_Layout], `${this.name}_Pipeline_Norm`);\n    this.pipelineCache.set(pipelineCacheKey, pipeline);\n    return pipeline;\n  }\n\n  newInstance(rows, cols, inputBuffer, gammaBuffer, betaBuffer) {\n    if (cols % 4 !== 0) throw new Error(\"Cols must be divisible by 4.\");\n\n    const workgroupsX = cols > 4096 ? 256 : 64;\n    const statsPipeline = this.getStatsPipeline(workgroupsX);\n    const statsUniformBuffer = this.initUniform(4, [[0, new Uint32Array([cols])]]);\n    const statsResultBuffer = this.initResultBuffer([rows, 2]);\n    const statsBindGroup = this.initBindGroup(this.u_s_Layout, [statsUniformBuffer, statsResultBuffer], `${this.name}_BindGroup_stats`);\n    const statsInputBindGroup = this.initBindGroup(this.r_Layout, [inputBuffer], `${this.name}_InputG`);\n    const statsWorkgroups = { x: 1, y: wgSize(rows, 1), z: 1 };\n\n    const normPipeline = this.getNormPipeline();\n    const normUniformBuffer = this.initUniform(4, [[0, new Uint32Array([rows, Math.ceil(cols / 4)])]]);\n    const normResultBuffer = this.initResultBuffer([rows, cols]);\n    const normBindGroup = this.initBindGroup(this.u_s_Layout, [normUniformBuffer, normResultBuffer], `${this.name}_BindGroup_norm`);\n    const normInputBindGroup = this.initBindGroup(\n      this.r_r_r_r_Layout,\n      [inputBuffer, gammaBuffer, betaBuffer, statsResultBuffer],\n      `${this.name}_InputBindGroup_norm`\n    );\n    const normWorkgroups = { x: wgSize(cols, 32), y: wgSize(rows, 8), z: 1 };\n\n    return {\n      resultBuffer: normResultBuffer,\n      passes: [\n        {\n          flag: \"compute\",\n          pipeline: statsPipeline,\n          groups: [statsBindGroup, statsInputBindGroup],\n          workgroups: statsWorkgroups,\n        },\n        {\n          flag: \"compute\",\n          pipeline: normPipeline,\n          groups: [normBindGroup, normInputBindGroup],\n          workgroups: normWorkgroups,\n        },\n      ],\n    };\n  }\n\n  // I'm pretty sure this breaks for col < workgroupSize? Needs testing.\n  normStatsShader = (wg_size) => `\n    struct Meta {\n      N: u32, \n    };\n\n    const wg_size: u32 = ${wg_size};\n\n    @group(1) @binding(0) var<storage, read> input_array: array<f32>;\n\n    @group(0) @binding(0) var<uniform> uniforms: Meta;\n    @group(0) @binding(1) var<storage, read_write> result_array: array<f32>;\n\n    var<workgroup> row_mean: f32;\n    var<workgroup> op_buffer: array<f32, ${wg_size}>;\n\n    @compute @workgroup_size(${wg_size})\n    fn main (@builtin(global_invocation_id) global_id: vec3<u32>) {\n      let col: u32 = global_id.x;\n      let row: u32 = global_id.y;\n      let N: u32 = uniforms.N;\n\n      // Condense.\n      var threadSum: f32 = 0.0;\n      for (var i: u32 = col; i < N; i = i + wg_size) {\n        threadSum = threadSum + input_array[row * N + i];\n      }\n      op_buffer[col] = threadSum;\n      workgroupBarrier();\n      \n      // Reduce to one value sum. Optimize with bit shifts.\n      for (var i: u32 = wg_size >> 1; i > 0; i = i >> 1) {\n        if (col < i) {\n          op_buffer[col] = op_buffer[col] + op_buffer[col + i];\n        }\n        workgroupBarrier();\n      }\n      \n      if (col == 0) {\n        row_mean = op_buffer[0] / f32(N);\n      }\n      workgroupBarrier();\n\n      // Condense.\n      var threadVariance: f32 = 0.0;\n      for (var i: u32 = col; i < N; i = i + wg_size) {\n        threadVariance = threadVariance + pow(input_array[row * N + i] - row_mean, 2);\n      }\n      op_buffer[col] = threadVariance / f32(N);\n      workgroupBarrier();\n      \n      for (var i: u32 = wg_size >> 1; i > 0; i = i >> 1) {\n        if (col < i) {\n          op_buffer[col] = op_buffer[col] + op_buffer[col + i];\n        }\n        workgroupBarrier();\n      }\n      \n      var stdev: f32 = sqrt(op_buffer[0] + 1e-5);\n\n      if (col == 0) {\n        result_array[row * 2] = row_mean;\n        result_array[row * 2 + 1] = stdev;\n      }\n    }\n  `;\n\n  normShader = `\n    struct Meta {\n      M: u32,\n      ND4: u32,\n    }\n\n    @group(0) @binding(0) var<uniform> uniforms: Meta;\n    @group(0) @binding(1) var<storage, read_write> result_array: array<vec4<f32>>;\n\n    @group(1) @binding(0) var<storage, read> input_array: array<vec4<f32>>;\n    @group(1) @binding(1) var<storage, read> gamma_param: array<vec4<f32>>;\n    @group(1) @binding(2) var<storage, read> beta_param: array<vec4<f32>>;\n    @group(1) @binding(3) var<storage, read> stats_param: array<f32>;\n\n    @compute @workgroup_size(8, 8)\n    fn main (@builtin(global_invocation_id) global_id: vec3<u32>) {\n      var col: u32 = global_id.x;\n      var row: u32 = global_id.y;\n      var ND4: u32 = uniforms.ND4;\n      var M: u32 = uniforms.M;\n\n      if (row >= M || col >= ND4) {\n        return;\n      }\n\n      let mean = stats_param[row * 2];\n      let stdev = stats_param[row * 2 + 1];\n      let output = (input_array[row * ND4 + col] - mean) / stdev;\n      let gamma = gamma_param[col];\n      let beta = beta_param[col];\n      let shift = gamma * output + beta;\n      result_array[row * ND4 + col] = shift;\n    }\n  `;\n}\n\nclass SoftmaxBlockClass extends Block {\n  constructor() {\n    super();\n    this.name = \"softmax\";\n  }\n\n  getFusedPipeline(workgroups, transpose) {\n    const pipelineCacheKey = `${this.name}_fused_${workgroups}_${transpose}`;\n    if (this.pipelineCache.has(pipelineCacheKey)) return this.pipelineCache.get(pipelineCacheKey);\n    const pipeline = this.initPipeline(this.fusedShader(workgroups, transpose), [this.u_s_Layout, this.r_Layout], `${this.name}_Pipeline_Div`);\n    this.pipelineCache.set(pipelineCacheKey, pipeline);\n    return pipeline;\n  }\n\n  newInstance(rows, cols, inputBuffer, transpose = false) {\n    const workgroupsX = cols > 4096 ? 256 : 64;\n    const fusedPipeline = this.getFusedPipeline(workgroupsX, transpose);\n\n    const uniformBuffer = this.initUniform(4, [[0, new Uint32Array([rows, cols])]]);\n    const resultBuffer = this.initResultBuffer([rows, cols], `${this.name}_ResultBuffer_`);\n    const bindGroup = this.initBindGroup(this.u_s_Layout, [uniformBuffer, resultBuffer], `${this.name}_BindGroup_`);\n    const inputBindGroup = this.initBindGroup(this.r_Layout, [inputBuffer], `${this.name}_BindGroup__Input`);\n    const workgroups = { x: 1, y: wgSize(rows, 1), z: 1 };\n\n    return {\n      resultBuffer: resultBuffer,\n      passes: [\n        {\n          flag: \"compute\",\n          pipeline: fusedPipeline,\n          groups: [bindGroup, inputBindGroup],\n          workgroups: workgroups,\n        },\n      ],\n    };\n  }\n\n  /*\n    Possible improvements: Vectorization? Modify input buffer to coalesce better?\n  */\n  fusedShader(wg_size, transpose) {\n    const outputIndex = transpose ? \"i * uniforms.M + row\" : \"row * N + i\";\n    return `\n    struct Meta {\n      M: u32,\n      N: u32, \n    };\n\n    const minFloat: f32 = -3.402823e+38f;\n    const wg_size: u32 = ${wg_size};\n\n    @group(0) @binding(0) var<uniform> uniforms: Meta;\n    @group(0) @binding(1) var<storage, read_write> result_array: array<f32>;\n    @group(1) @binding(0) var<storage, read> input_array: array<f32>;\n\n    var<workgroup> max_row: f32;\n    var<workgroup> sum_row: f32;\n    var<workgroup> op_buffer: array<f32, ${wg_size}>;\n\n    @compute @workgroup_size(${wg_size})\n    fn main (@builtin(global_invocation_id) global_id: vec3<u32>) {\n      let col: u32 = global_id.x;\n      let row: u32 = global_id.y;\n      let N: u32 = uniforms.N;\n\n      // Condense into 256 col max.\n      var thread_max = minFloat;\n      for (var i: u32 = col; i < N; i = i + wg_size) {\n        thread_max = max(thread_max, input_array[row * N + i]);\n      }\n      if (col < N) {\n        op_buffer[col] = thread_max;\n      }\n      workgroupBarrier();\n      \n      // Reduce to one value max. Optimize with bit shifts.\n      var reductionSize: u32 = min(N, wg_size);\n      for (var i: u32 = reductionSize >> 1; i > 0; i = reductionSize >> 1) {\n        reductionSize = i + (reductionSize & 1); // Ensure odd numbers are rounded up.\n        if (col < i) {\n          op_buffer[col] = max(op_buffer[col], op_buffer[col + reductionSize]);\n        }\n        workgroupBarrier();\n      }\n      if (col == 0) {\n        max_row = op_buffer[0];\n      }\n      workgroupBarrier();\n\n      var threadSum: f32 = 0.0;\n      for (var i: u32 = col; i < N; i = i + wg_size) {\n        threadSum = threadSum + exp(input_array[row * N + i] - max_row);\n      }\n      op_buffer[col] = threadSum;\n      workgroupBarrier();\n      \n      for (var i: u32 = wg_size >> 1; i > 0; i = i >> 1) {\n        if (col < i) {\n          op_buffer[col] = op_buffer[col] + op_buffer[col + i];\n        }\n        workgroupBarrier();\n      }\n      \n      if (col == 0) {\n        sum_row = op_buffer[0];\n      }\n      workgroupBarrier();\n\n      for (var i: u32 = col; i < N; i = i + wg_size) {\n        result_array[${outputIndex}] = exp(input_array[row * N + i] - max_row) / sum_row;\n      }\n    }\n  `;\n  }\n}\n\nclass GeluBlockClass extends Block {\n  constructor() {\n    super();\n    this.name = \"gelu\";\n  }\n\n  getPipeline() {\n    const pipelineCacheKey = this.name; // No param optimization.\n    if (this.pipelineCache.has(pipelineCacheKey)) return this.pipelineCache.get(pipelineCacheKey);\n    const pipeline = this.initPipeline(this.GELUShader, [this.u_s_Layout, this.r_Layout], `${this.name}_Pipeline`);\n    this.pipelineCache.set(pipelineCacheKey, pipeline);\n    return pipeline;\n  }\n\n  newInstance(rows, cols, inputBuf) {\n    if (cols % 4 !== 0) throw new Error(\"Cols must be divisible by 4.\");\n    const pipeline = this.getPipeline();\n    const uniformBuffer = this.initUniform(4, [[0, new Uint32Array([rows, Math.ceil(cols / 4)])]]);\n    const resultBuffer = this.initResultBuffer([rows, cols]);\n    const opBindGroup = this.initBindGroup(this.u_s_Layout, [uniformBuffer, resultBuffer], `${this.name}_OpG`);\n    const inputBindGroup = this.initBindGroup(this.r_Layout, [inputBuf], `${this.name}_InputG`);\n    const workgroups = { x: wgSize(cols, 32), y: wgSize(rows, 8), z: 1 };\n\n    return {\n      resultBuffer,\n      passes: [\n        {\n          flag: \"compute\",\n          pipeline,\n          groups: [opBindGroup, inputBindGroup],\n          workgroups,\n        },\n      ],\n    };\n  }\n\n  GELUShader = `\n    struct Meta {\n      M: u32,\n      ND4: u32,\n    }\n\n    const LOW_THRESHOLD: vec4<f32> = vec4<f32>(-10.0);\n    const HIGH_THRESHOLD: vec4<f32> = vec4<f32>(10.0);\n    const ZERO: vec4<f32> = vec4<f32>(0.0);\n    const HALF: vec4<f32> = vec4<f32>(0.5);\n    const SQRPI: vec4<f32> = vec4<f32>(0.7978845608);\n    const COEFF: vec4<f32> = vec4<f32>(0.044715);\n    \n    fn gelu_vec4(x: vec4<f32>) -> vec4<f32> {\n      let x_cubed: vec4<f32> = pow(x, vec4<f32>(3.0));\n      let cdf_approx: vec4<f32> = HALF * (vec4<f32>(1.0) + tanh(SQRPI * (x + COEFF * x_cubed)));\n  \n      let result: vec4<f32> = x * cdf_approx;\n  \n      let lt_mask: vec4<bool> = x < LOW_THRESHOLD;\n      let gt_mask: vec4<bool> = x > HIGH_THRESHOLD;\n  \n      return select(select(result, ZERO, lt_mask), x, gt_mask);\n    }\n\n    @group(1) @binding(0) var<storage,read> array_matrix: array<vec4<f32>>;\n\n    @group(0) @binding(0) var<uniform> uniforms: Meta;\n    @group(0) @binding(1) var<storage,read_write> array_output: array<vec4<f32>>;\n\n    @compute @workgroup_size(8, 8)\n    fn main (@builtin(global_invocation_id) global_id: vec3<u32>) {\n      var col: u32 = global_id.x;\n      var row: u32 = global_id.y;\n      var ND4: u32 = uniforms.ND4;\n      var M: u32 = uniforms.M;\n      \n      if (row >= M || col >= ND4) {\n        return;\n      }\n\n      array_output[row * ND4 + col] = gelu_vec4(array_matrix[row * ND4 + col]);\n    }\n  `;\n}\n\nclass EmbedBlockClass extends Block {\n  constructor() {\n    super();\n    this.name = \"embed\";\n  }\n\n  staticLoad(embdOutputBuffer, posEmbdOutputBuffer, embdBuffers, posEmbdBuffer, idx, seq_length, n_embd, vocab_chunk_size) {\n    // Can build a cache later.\n    const embdCopyCommands = Array(seq_length)\n      .fill()\n      .map((_, i) => {\n        const chunkOffset = Math.floor(idx[i] / vocab_chunk_size);\n        const chunkIdx = idx[i] % vocab_chunk_size;\n\n        return {\n          flag: \"copy\",\n          src: embdBuffers[chunkOffset],\n          srcOffset: this.bufferSize(n_embd) * chunkIdx,\n          dst: embdOutputBuffer,\n          dstOffset: this.bufferSize(n_embd) * i,\n          size: this.bufferSize(n_embd),\n        };\n      });\n\n    // Also can be cached.\n    const posCopyCommand = {\n      flag: \"copy\",\n      src: posEmbdBuffer,\n      srcOffset: 0,\n      dst: posEmbdOutputBuffer,\n      dstOffset: 0,\n      size: this.bufferSize(seq_length, n_embd),\n    };\n\n    return {\n      passes: [...embdCopyCommands, posCopyCommand],\n    };\n  }\n\n  newInstance(idx, seq_length, n_embd, vocab_chunk_size, embdBuffers, posEmbdBuffer, ResidualBlock) {\n    const embdOutputBuffer = this.initBuffer([\"storage\", \"copy_to\"], [seq_length, n_embd]);\n    const posEmbdOutputBuffer = this.initBuffer([\"storage\", \"copy_to\"], [seq_length, n_embd]);\n\n    // Can build a cache later.\n    const embdCopyCommands = Array(seq_length)\n      .fill()\n      .map((_, i) => {\n        const chunkOffset = Math.floor(idx[i] / vocab_chunk_size);\n        const chunkIdx = idx[i] % vocab_chunk_size;\n\n        return {\n          flag: \"copy\",\n          src: embdBuffers[chunkOffset],\n          srcOffset: this.bufferSize(n_embd) * chunkIdx,\n          dst: embdOutputBuffer,\n          dstOffset: this.bufferSize(n_embd) * i,\n          size: this.bufferSize(n_embd),\n        };\n      });\n\n    // Also can be cached.\n    const posCopyCommand = {\n      flag: \"copy\",\n      src: posEmbdBuffer,\n      srcOffset: 0,\n      dst: posEmbdOutputBuffer,\n      dstOffset: 0,\n      size: this.bufferSize(seq_length, n_embd),\n    };\n\n    const { resultBuffer: residualResult, passes: residualPasses } = ResidualBlock.newInstance(seq_length, n_embd, embdOutputBuffer, posEmbdOutputBuffer);\n\n    return {\n      resultBuffer: residualResult,\n      passes: [...embdCopyCommands, posCopyCommand, ...residualPasses],\n    };\n  }\n}\n\nclass DeEmbedBlockClass extends Block {\n  constructor() {\n    super();\n    this.name = \"deembed\";\n  }\n\n  getPipeline() {\n    const pipelineCacheKey = this.name; // No param optimization.\n    if (this.pipelineCache.has(pipelineCacheKey)) return this.pipelineCache.get(pipelineCacheKey);\n    const pipeline = this.initPipeline(this.deEmbedShader, [this.u_s_Layout, this.r_r_Layout], `${this.name}_Pipeline`);\n    this.pipelineCache.set(pipelineCacheKey, pipeline);\n    return pipeline;\n  }\n\n  newInstance(n_embd, vocab_size, padded_vocab_size, seq_length, vocab_chunk_size, embedBuffer, deEmbeddingsBuffers) {\n    const deEmbedPipeline = this.getPipeline();\n    const slicedEmbedOutputBuffer = this.initBuffer([\"storage\", \"copy_to\"], [n_embd]);\n    const deEmbedOutputBuffer = this.initBuffer([\"map_read\", \"copy_to\"], [vocab_size]);\n\n    const sliceEmbedCopyCommand = {\n      flag: \"copy\",\n      src: embedBuffer,\n      srcOffset: this.bufferSize(seq_length - 1, n_embd),\n      dst: slicedEmbedOutputBuffer,\n      dstOffset: 0,\n      size: this.bufferSize(1, n_embd),\n    };\n\n    const deEmbedPasses = deEmbeddingsBuffers.flatMap((embdBuffer, i) => {\n      // Some future optimizations where we can assume that vocab_size is consistent.\n      const uniformBuffer = this.initUniform(4, [[0, new Uint32Array([vocab_chunk_size, Math.ceil(vocab_chunk_size / 4), Math.ceil(n_embd / 4)])]]);\n      const resultBuffer = this.initResultBuffer([vocab_chunk_size]);\n      const opBindGroup = this.initBindGroup(this.u_s_Layout, [uniformBuffer, resultBuffer], `${this.name}_OpG`);\n      const inputBindGroup = this.initBindGroup(this.r_r_Layout, [slicedEmbedOutputBuffer, embdBuffer], `${this.name}_InputG`);\n      const workgroups = { x: wgSize(vocab_chunk_size, 32), y: 1, z: 1 };\n\n      return [\n        {\n          flag: \"compute\",\n          pipeline: deEmbedPipeline,\n          groups: [opBindGroup, inputBindGroup],\n          workgroups,\n        },\n        {\n          flag: \"copy\",\n          src: resultBuffer,\n          srcOffset: 0,\n          dst: deEmbedOutputBuffer,\n          dstOffset: i * this.bufferSize(vocab_chunk_size),\n          size: i == deEmbeddingsBuffers.length - 1 ? this.bufferSize(vocab_chunk_size - (padded_vocab_size - vocab_size)) : this.bufferSize(vocab_chunk_size),\n        },\n      ];\n    });\n\n    return {\n      resultBuffer: deEmbedOutputBuffer,\n      passes: [sliceEmbedCopyCommand, ...deEmbedPasses],\n    };\n  }\n\n  deEmbedShader = `\n    struct Meta {\n      N: u32,\n      ND4: u32,\n      KD4: u32,\n    }\n\n    @group(1) @binding(0) var<storage,read> embed_vector: array<vec4<f32>>;\n    @group(1) @binding(1) var<storage,read> deembed_matrix: array<vec4<f32>>;\n    @group(0) @binding(0) var<uniform> uniforms: Meta;\n    @group(0) @binding(1) var<storage,read_write> array_output: array<vec4<f32>>;\n\n    @compute @workgroup_size(4)\n    fn main(@builtin(global_invocation_id) global_id: vec3<u32>) {\n      var N: u32 = uniforms.N;\n      var ND4: u32 = uniforms.ND4;\n      var KD4: u32 = uniforms.KD4;\n      var colD8: u32 = global_id.x;\n\n      if (colD8 * 8 >= N) {\n        return;\n      }\n\n      var sum00: vec4<f32> = vec4<f32>();\n      var sum10: vec4<f32> = vec4<f32>();\n\n      for(var k: u32 = 0u; k < KD4; k = k + 1u) {\n        var arow0: vec4<f32> = embed_vector[k];\n        var brow: vec4<f32>;\n\n        brow = deembed_matrix[(k * 4u + 0u) * ND4 + colD8 * 2u + 0u];\n        sum00 = vec4<f32>(arow0.x) * brow + sum00;\n\n        brow = deembed_matrix[(k * 4u + 0u) * ND4 + colD8 * 2u + 1u];\n        sum10 = vec4<f32>(arow0.x) * brow + sum10;\n       \n        brow = deembed_matrix[(k * 4u + 1u) * ND4 + colD8 * 2u + 0u];\n        sum00 = vec4<f32>(arow0.y) * brow + sum00;\n       \n        brow = deembed_matrix[(k * 4u + 1u) * ND4 + colD8 * 2u + 1u];\n        sum10 = vec4<f32>(arow0.y) * brow + sum10;\n       \n        brow = deembed_matrix[(k * 4u + 2u) * ND4 + colD8 * 2u + 0u];\n        sum00 = vec4<f32>(arow0.z) * brow + sum00;\n        \n        brow = deembed_matrix[(k * 4u + 2u) * ND4 + colD8 * 2u + 1u];\n        sum10 = vec4<f32>(arow0.z) * brow + sum10;\n       \n        brow = deembed_matrix[(k * 4u + 3u) * ND4 + colD8 * 2u + 0u];\n        sum00 = vec4<f32>(arow0.w) * brow + sum00;\n\n        brow = deembed_matrix[(k * 4u + 3u) * ND4 + colD8 * 2u + 1u];\n        sum10 = vec4<f32>(arow0.w) * brow + sum10;\n      }\n\n      if (colD8 * 8u + 0u < N) {\n        array_output[colD8 * 2u + 0u] = sum00;\n      }\n      if (colD8 * 8u + 4u < N) {\n        array_output[colD8 * 2u + 1u] = sum10;\n      }\n    }\n  `;\n}\n\nclass AttentionBlockClass extends Block {\n  constructor() {\n    super();\n    this.name = \"attention\";\n  }\n\n  getNewAttentionWeightsPipeline() {\n    const pipelineCacheKey = `${this.name}_weights_fused`; // No param optimization.\n    if (this.pipelineCache.has(pipelineCacheKey)) return this.pipelineCache.get(pipelineCacheKey);\n    const pipeline = this.initPipeline(this.fusedWeightsShader, [this.u_s_Layout, this.r_r_Layout], `${this.name}_Pipeline_AttWeights`);\n    this.pipelineCache.set(pipelineCacheKey, pipeline);\n    return pipeline;\n  }\n\n  getNewAttentionValuesPipeline() {\n    const pipelineCacheKey = `${this.name}_values_fused`; // No param optimization.\n    if (this.pipelineCache.has(pipelineCacheKey)) return this.pipelineCache.get(pipelineCacheKey);\n    const pipeline = this.initPipeline(this.newAttentionValuesShader, [this.u_s_Layout, this.r_r_Layout], `${this.name}_Pipeline_AttValues`);\n    this.pipelineCache.set(pipelineCacheKey, pipeline);\n    return pipeline;\n  }\n\n  getFormatQPipeline() {\n    const pipelineCacheKey = `${this.name}_format_q`; // No param optimization.\n    if (this.pipelineCache.has(pipelineCacheKey)) return this.pipelineCache.get(pipelineCacheKey);\n    const pipeline = this.initPipeline(this.formatQShader, [this.u_s_Layout, this.r_Layout], `${this.name}_Pipeline_formatQ`);\n    this.pipelineCache.set(pipelineCacheKey, pipeline);\n    return pipeline;\n  }\n\n  newFusedInstance(\n    seq_length,\n    n_embd,\n    attentionDotProductScale,\n    n_head,\n    head_size,\n    inputBuffer,\n    qWeightsBuffer,\n    qBiasBuffer,\n    kWeightsBuffer,\n    kBiasBuffer,\n    vWeightsBuffer,\n    vBiasBuffer,\n    linearWeightsBuffer,\n    linearBiasBuffer,\n    FastMatMulBlock,\n    SoftmaxBlock\n  ) {\n    const { resultBuffer: QResultBuffer, passes: QMLPPasses } = FastMatMulBlock.newInstance(\n      seq_length,\n      n_embd,\n      n_embd,\n      inputBuffer,\n      qWeightsBuffer,\n      qBiasBuffer\n    );\n    const { resultBuffer: KResultBuffer, passes: KMLPPasses } = FastMatMulBlock.newInstance(\n      seq_length,\n      n_embd,\n      n_embd,\n      inputBuffer,\n      kWeightsBuffer,\n      kBiasBuffer\n    );\n    const { resultBuffer: VResultBuffer, passes: VMLPPasses } = FastMatMulBlock.newInstance(\n      seq_length,\n      n_embd,\n      n_embd,\n      inputBuffer,\n      vWeightsBuffer,\n      vBiasBuffer\n    );\n\n    const formatQPipeline = this.getFormatQPipeline();\n    const formatQUniformBuffer = this.initUniform(4, [[0, new Uint32Array([seq_length, n_embd, head_size])]]);\n    const formatQResultBuffer = this.initResultBuffer([seq_length * n_head, head_size]);\n    const formatQBindGroup = this.initBindGroup(this.u_s_Layout, [formatQUniformBuffer, formatQResultBuffer]);\n    const formatQInputBindGroup = this.initBindGroup(this.r_Layout, [QResultBuffer], `${this.name}_formatQInputG`);\n    const formatQWorkgroups = { x: wgSize(n_embd, 8), y: wgSize(seq_length, 8), z: 1 };\n\n    const attentionWeightsPipeline = this.getNewAttentionWeightsPipeline();\n    const attentionWeightsUniformBuffer = this.initUniform(8, [\n      [0, new Uint32Array([seq_length * n_head, seq_length, n_embd / 4, head_size / 4])],\n      [16, new Float32Array([attentionDotProductScale])],\n    ]);\n    const attentionWeightsResultBuffer = this.initResultBuffer([seq_length * n_head, seq_length]);\n    const attentionWeightsBindGroup = this.initBindGroup(\n      this.u_s_Layout,\n      [attentionWeightsUniformBuffer, attentionWeightsResultBuffer],\n      `${this.name}_AttentionWeightsG`\n    );\n    const attentionWeightsInputBindGroup = this.initBindGroup(this.r_r_Layout, [formatQResultBuffer, KResultBuffer], `${this.name}_AttentionWeightsInputG`);\n    const attentionWeightsWorkgroups = { x: wgSize(seq_length, 8), y: wgSize(seq_length * n_head, 8), z: 1 };\n\n    const { resultBuffer: softmaxOutputBuffer, passes: softmaxPasses } = SoftmaxBlock.newInstance(\n      seq_length * n_head,\n      seq_length,\n      attentionWeightsResultBuffer\n    );\n\n    const attentionValuesPipeline = this.getNewAttentionValuesPipeline();\n    const attentionValuesUniformBuffer = this.initUniform(4, [[0, new Uint32Array([seq_length, n_embd / 4, head_size / 4])]]);\n    const attentionValuesResultBuffer = this.initResultBuffer([seq_length, n_embd]);\n    const attentionValuesBindGroup = this.initBindGroup(this.u_s_Layout, [attentionValuesUniformBuffer, attentionValuesResultBuffer]);\n    const attentionValuesInputBindGroup = this.initBindGroup(this.r_r_Layout, [softmaxOutputBuffer, VResultBuffer], `${this.name}_AttentionValuesInputG`);\n    const attentionValuesWorkgroups = { x: wgSize(n_embd, 32), y: wgSize(seq_length, 8), z: 1 };\n\n    const { resultBuffer: linearMLPResult, passes: linearMLPPasses } = FastMatMulBlock.newInstance(\n      seq_length,\n      n_embd,\n      n_embd,\n      attentionValuesResultBuffer,\n      linearWeightsBuffer,\n      linearBiasBuffer\n    );\n\n    return {\n      resultBuffer: linearMLPResult,\n      passes: [\n        ...QMLPPasses,\n        ...KMLPPasses,\n        ...VMLPPasses,\n        {\n          flag: \"compute\",\n          pipeline: formatQPipeline,\n          groups: [formatQBindGroup, formatQInputBindGroup],\n          workgroups: formatQWorkgroups,\n        },\n        {\n          flag: \"compute\",\n          pipeline: attentionWeightsPipeline,\n          groups: [attentionWeightsBindGroup, attentionWeightsInputBindGroup],\n          workgroups: attentionWeightsWorkgroups,\n        },\n        ...softmaxPasses,\n        {\n          flag: \"compute\",\n          pipeline: attentionValuesPipeline,\n          groups: [attentionValuesBindGroup, attentionValuesInputBindGroup],\n          workgroups: attentionValuesWorkgroups,\n        },\n        ...linearMLPPasses,\n      ],\n    };\n  }\n\n  formatQShader = `\n    struct Meta {\n      M: u32,\n      N: u32,\n      HSize: u32,\n    }\n\n    @group(1) @binding(0) var<storage, read> input_array: array<f32>;\n\n    @group(0) @binding(0) var<uniform> uniforms: Meta;\n    @group(0) @binding(1) var<storage, read_write> result_array: array<f32>;\n\n    var<workgroup> tile: array<array<f32, 8>, 8>;\n\n    @compute @workgroup_size(8, 8)\n    fn main (@builtin(local_invocation_id) local_id: vec3<u32>, @builtin(workgroup_id) workgroup_id: vec3<u32>) {\n      let col: u32 = workgroup_id.x * 8 + local_id.x;\n      let row: u32 = workgroup_id.y * 8 + local_id.y;\n      let N: u32 = uniforms.N;\n      let M: u32 = uniforms.M;\n\n      // Load a tile from input_array to shared memory tile\n      if (row < M && col < N) {\n          tile[local_id.y][local_id.x] = input_array[row * N + col];\n      }\n\n      workgroupBarrier(); // Ensure all threads have finished writing to the shared memory before proceeding\n\n      let HSize: u32 = uniforms.HSize;\n      let xOffset: u32 = col % HSize;\n      let yOffset: u32 = row * HSize + (col / HSize) * HSize * M;\n\n      // Write the tile to result_array\n      if (row < M && col < N) {\n          result_array[yOffset + xOffset] = tile[local_id.y][local_id.x];\n      }\n    } \n  `;\n\n  // Sequence length invariant, no padding needed.\n  fusedWeightsShader = `\n    struct Meta {\n      M: u32, // seq_length * n_heads\n      N: u32, // seq_length\n      ED4: u32, // hsize * n_heads\n      HD4: u32,\n      attentionScale: f32,\n    };\n\n    @group(1) @binding(0) var<storage, read> query_array: array<vec4<f32>>;\n    @group(1) @binding(1) var<storage, read> key_array: array<vec4<f32>>;\n\n    @group(0) @binding(0) var<uniform> uniforms: Meta;\n    @group(0) @binding(1) var<storage, read_write> result_array: array<f32>;\n\n    @compute @workgroup_size(8, 8)\n    fn main (@builtin(global_invocation_id) global_id: vec3<u32>) {\n      let col: u32 = global_id.x;\n      let row: u32 = global_id.y;\n      let N: u32 = uniforms.N;\n      let HD4: u32 = uniforms.HD4;\n\n      if (row >= uniforms.M || col >= N) {\n        return;\n      }\n\n      let head: u32 = row / N;\n      var sum: f32 = 0.0;\n      for (var i: u32 = 0; i < HD4; i = i + 1) {\n        sum = sum + dot(query_array[row * HD4 + i], key_array[col * uniforms.ED4 + i + head * HD4]);\n      }\n\n      // Causal attention step.\n      let rowMask: u32 = row % N;\n      let causalMask: bool = (col <= rowMask);\n      result_array[row * N + col] = select(-1e9, sum * uniforms.attentionScale, causalMask);\n    }\n  `;\n\n  newAttentionValuesShader = `\n    struct Meta {\n      M: u32,\n      ND4: u32,\n      HD4: u32,\n    }\n\n    @group(0) @binding(0) var<uniform> uniforms: Meta;\n    @group(0) @binding(1) var<storage, read_write> result_array: array<vec4<f32>>;\n\n    @group(1) @binding(0) var<storage, read> weights_array: array<f32>;\n    @group(1) @binding(1) var<storage, read> values_array: array<vec4<f32>>;\n\n    @compute @workgroup_size(8, 8)\n    fn main (@builtin(global_invocation_id) global_id: vec3<u32>) {\n      var col: u32 = global_id.x;\n      var row: u32 = global_id.y;\n      var M: u32 = uniforms.M;\n      var ND4: u32 = uniforms.ND4;\n      var HD4: u32 = uniforms.HD4;\n\n      if (row >= M || col >= ND4) {\n        return;\n      }\n\n      let head: u32 = col / HD4;\n      var sum: vec4<f32> = vec4<f32>(0.0);\n      for (var i: u32 = 0; i < M; i = i + 1) {\n        var weight = weights_array[row * M + i + head * M * M]; // weights is M * M\n        sum = sum + values_array[i * ND4 + col] * weight;\n      }\n\n      result_array[row * ND4 + col] = sum;\n    }\n  `;\n}\n"
        },
        {
          "name": "model.js",
          "type": "blob",
          "size": 20.3193359375,
          "content": "class GPT {\n  constructor(folder, type) {\n    this.folder = folder;\n    this.tokenizerType = type;\n    this.initialized = false;\n\n    this.device;\n    this.model;\n    this.tokenizer;\n    this.params;\n    this.minBufferOffset = 1;\n\n    this.defaultPrompt;\n    this.defaultTopK;\n    this.defaultTemperature;\n    this.defaultTokens;\n\n    this.externalBuffer;\n\n    this.unloadDeletionStack = [];\n  }\n\n  async initialize() {\n    if (this.initialized) return console.error(\"Model already initialized\");\n    if (!navigator.gpu) throw new Error(\"WebGPU is not supported\");\n\n    const adapter = await navigator.gpu.requestAdapter();\n    this.device = await adapter.requestDevice();\n\n    initializeOperations(this.device);\n\n    [this.model, this.params] = await this.loadModel(this.folder);\n    this.tokenizer = this.tokenizerType == \"bpe\" ? new GPT2Tokenizer() : new SimpleTokenizer();\n    await this.tokenizer.load();\n\n    if (this.tokenizerType == \"bpe\") {\n      this.defaultPrompt = `What is the answer to life, the universe, and everything?\\n`;\n      this.defaultTopK = 3;\n      this.defaultTemperature = 1;\n      this.defaultTokens = 30;\n    } else {\n      this.defaultPrompt = `WILL:\\nAh, how dare you challenge me?\\nHave you forgotten I built WebGPT?\\n`;\n      this.defaultTopK = 2;\n      this.defaultTemperature = 1;\n      this.defaultTokens = 80;\n    }\n\n    this.initialized = true;\n\n    console.log(\"Model initialized\");\n  }\n\n  async *generate(prompt, max_new_tokens, top_k, temperature) {\n    if (!this.initialized) {\n      console.error(\"Model not loaded yet\");\n      return;\n    }\n\n    // Buffer size (321644800) exceeds the max buffer size limit (268435456).\n    //  - While calling [Device].CreateBuffer([BufferDescriptor]).\n\n    let history = this.tokenizer.encode(prompt);\n    console.log(`Prompt (${history.length} tokens):\\n${prompt}`);\n\n    const warmupRuns = 3;\n    let totalTime = 0;\n\n    for (let i = 0; i < max_new_tokens; i++) {\n      const idx_cond = history.slice(-this.params.n_ctx);\n      const useAttCache = i !== 0 && history.length <= this.params.n_ctx;\n\n      const startTime = performance.now();\n      const logits = await this.run(idx_cond, useAttCache);\n      const endTime = performance.now();\n\n      // console.log(`\\nIteration ${i + 1} of ${max_new_tokens}`);\n      const lapsedTime = endTime - startTime;\n      console.log(`Kernel execution time: ${lapsedTime} ms`);\n      i >= warmupRuns && (totalTime += lapsedTime);\n\n      const { topKIndices, topKProbs } = selectTopK(logits, top_k);\n      const probs = cpuSoftmax(topKProbs, temperature);\n      const idx_next = topKIndices[sampleFromDistribution(probs)];\n\n      history = history.concat(idx_next);\n\n      // console.log(`Output:\\n${this.tokenizer.decode(history)}`);\n\n      // const totalProbs = cpuSoftmax(logits, temperature);\n      // const tokenProbsString = Array.from(totalProbs)\n      //   .map((value, index) => ({ value, index }))\n      //   .sort((a, b) => b.value - a.value)\n      //   .slice(0, 8)\n      //   .map((prob) => `{ ${this.tokenizer.decode([prob.index]).replace(/(\\r\\n|\\n|\\r)/gm, \"newline\")} } : ${prob.value.toPrecision(3)}`)\n      //   .join(\" | \");\n      // console.log(\"Top 8 token probs:\", tokenProbsString);\n\n      yield this.tokenizer.decode([idx_next]);\n    }\n\n    console.log(`Average kernel execution time: ${totalTime / (max_new_tokens - warmupRuns)} ms`);\n  }\n\n  async run(idx) {\n    const { posEmbdBuffer, layer_buffers, normGammaBuffer, normBetaBuffer, embeddingsBuffers, deEmbeddingsBuffers } = this.model;\n    const { attention_scale, n_embd, n_head, head_size, n_layer, vocab_size, hidden_size, vocab_chunk_size, vocab_chunk_instances } = this.params;\n    const seq_length = idx.length;\n\n    // ---------------- Create Passes ---------------- //\n    // Note: These are re-initialized because everytime seq_length changes buffers are different sizes.\n\n    // Pipeline creation is major bottleneck to spin up speed! Also buffer re-use.\n\n    this.computePasses = [];\n    let intermediateBuffer;\n    let residualBuffer;\n    {\n      const { passes, resultBuffer } = EmbedBlock.newInstance(idx, seq_length, n_embd, vocab_chunk_size, embeddingsBuffers, posEmbdBuffer, ResidualBlock);\n      intermediateBuffer = resultBuffer;\n      residualBuffer = resultBuffer;\n      this.computePasses.push(...passes);\n    }\n    for (let i = 0; i < n_layer; i++) {\n      const buffers = layer_buffers[i];\n      {\n        const { passes, resultBuffer } = LayerNormBlock.newInstance(\n          seq_length,\n          n_embd,\n          intermediateBuffer,\n          buffers.normAttentionGammaBuffer,\n          buffers.normAttentionBetaBuffer\n        );\n        intermediateBuffer = resultBuffer;\n        this.computePasses.push(...passes);\n      }\n      {\n        const { passes, resultBuffer } = AttentionBlock.newFusedInstance(\n          seq_length,\n          n_embd,\n          attention_scale,\n          n_head,\n          head_size,\n          intermediateBuffer,\n          buffers.qkvWeightArray[0],\n          buffers.qkvBiasArray[0],\n          buffers.qkvWeightArray[1],\n          buffers.qkvBiasArray[1],\n          buffers.qkvWeightArray[2],\n          buffers.qkvBiasArray[2],\n          buffers.linearWeightsBuffer,\n          buffers.linearBiasBuffer,\n          FastMatMulBlock,\n          SoftmaxBlock\n        );\n        intermediateBuffer = resultBuffer;\n        this.computePasses.push(...passes);\n      }\n      {\n        const { passes, resultBuffer } = ResidualBlock.newInstance(seq_length, n_embd, intermediateBuffer, residualBuffer);\n        intermediateBuffer = resultBuffer;\n        residualBuffer = resultBuffer;\n        this.computePasses.push(...passes);\n      }\n      {\n        const { passes, resultBuffer } = LayerNormBlock.newInstance(\n          seq_length,\n          n_embd,\n          intermediateBuffer,\n          buffers.normLinearGammaBuffer,\n          buffers.normLinearBetaBuffer\n        );\n        intermediateBuffer = resultBuffer;\n        this.computePasses.push(...passes);\n      }\n      {\n        const { resultBuffer, passes } = FastMatMulBlock.newInstance(\n          seq_length,\n          hidden_size,\n          n_embd,\n          intermediateBuffer,\n          buffers.firstLayerWeightsBuffer,\n          buffers.firstLayerBiasBuffer\n        );\n        intermediateBuffer = resultBuffer;\n        this.computePasses.push(...passes);\n      }\n      {\n        const { resultBuffer, passes } = GeluBlock.newInstance(seq_length, hidden_size, intermediateBuffer);\n        intermediateBuffer = resultBuffer;\n        this.computePasses.push(...passes);\n      }\n      {\n        const { resultBuffer, passes } = FastMatMulBlock.newInstance(\n          seq_length,\n          n_embd,\n          hidden_size,\n          intermediateBuffer,\n          buffers.secondLayerWeightsBuffer,\n          buffers.secondLayerBiasBuffer\n        );\n        intermediateBuffer = resultBuffer;\n        this.computePasses.push(...passes);\n      }\n      {\n        const { passes, resultBuffer } = ResidualBlock.newInstance(seq_length, n_embd, intermediateBuffer, residualBuffer);\n        intermediateBuffer = resultBuffer;\n        residualBuffer = resultBuffer;\n        this.computePasses.push(...passes);\n      }\n    }\n    {\n      if (this.externalBuffer) {\n        this.computePasses.push({\n          flag: \"copy\",\n          src: intermediateBuffer,\n          srcOffset: 0,\n          dst: this.externalBuffer,\n          dstOffset: 0,\n          size: this.bufferSize(seq_length, n_embd),\n        });\n      }\n    }\n    {\n      const { passes, resultBuffer } = LayerNormBlock.newInstance(seq_length, n_embd, intermediateBuffer, normGammaBuffer, normBetaBuffer);\n      intermediateBuffer = resultBuffer;\n      this.computePasses.push(...passes);\n    }\n    {\n      const { passes, resultBuffer } = DeEmbedBlock.newInstance(\n        n_embd,\n        vocab_size,\n        vocab_chunk_size * vocab_chunk_instances,\n        seq_length,\n        vocab_chunk_size,\n        intermediateBuffer,\n        deEmbeddingsBuffers\n      );\n      intermediateBuffer = resultBuffer;\n      this.computePasses.push(...passes);\n    }\n    const resultBuffer = intermediateBuffer;\n\n    // ---------------- Compute Passes ----------------\n\n    const commandEncoder = this.device.createCommandEncoder();\n    for (const pass of this.computePasses) {\n      if (pass.flag === \"compute\") {\n        const passEncoder = commandEncoder.beginComputePass();\n        passEncoder.setPipeline(pass.pipeline);\n        for (let i = 0; i < pass.groups.length; i++) passEncoder.setBindGroup(i, pass.groups[i]);\n        passEncoder.dispatchWorkgroups(pass.workgroups.x, pass.workgroups.y);\n        passEncoder.end();\n      } else if (pass.flag === \"copy\") {\n        commandEncoder.copyBufferToBuffer(pass.src, pass.srcOffset, pass.dst, pass.dstOffset, pass.size);\n      }\n    }\n    this.device.queue.submit([commandEncoder.finish()]);\n\n    // ---------------- Read Results ----------------\n\n    await resultBuffer.mapAsync(GPUMapMode.READ);\n    const output = resultBuffer.getMappedRange();\n    const outputArray = new Float32Array(output).slice(0); // Copy the array, otherwise it'll be destroyed.\n\n    clearOperationCache();\n\n    return outputArray;\n  }\n\n  async loadModel(folder) {\n    if (this.initialized) return console.error(\"Model already loaded\");\n\n    console.log(\"Loading model from folder:\", folder);\n    const weightsFolder = `weights/${folder}/`;\n\n    const params = await this.loadParameters(weightsFolder);\n    const { embeddingsBuffers, deEmbeddingsBuffers } = await this.loadEmbeddings(params, weightsFolder);\n    const { posEmbdBuffer } = await this.loadPositionalEmbeddings(params, weightsFolder);\n    const layer_buffers = await this.loadLayers(params, weightsFolder);\n\n    console.log(\"Loading final layer norm...\");\n    const { normGammaBuffer, normBetaBuffer } = await this.loadFinalLayerNorm(params, weightsFolder);\n\n    const output = { layer_buffers, embeddingsBuffers, deEmbeddingsBuffers, posEmbdBuffer, normGammaBuffer, normBetaBuffer };\n    console.log(\"Finished loading model.\", output, params);\n    return [output, params];\n  }\n\n  async loadParameters(weightsFolder) {\n    console.log(\"Loading params...\");\n    const params = await (await fetch(`${weightsFolder}/params_gpt.json`)).json();\n\n    // Did you enable GitHub LFS? Won't work without it.\n    if (params.n_embd % 4 !== 0) throw new Error(\"Model load failed: n_embd must be divisible by 4.\");\n    if (params.n_embd % params.n_head !== 0) throw new Error(\"Model load failed: n_embd must be divisible by n_head.\");\n    // I'm unsure if this is a reasonable requirement here. At worst, I can figure out some padding method.\n    if ((params.n_embd / params.n_head) % 4 !== 0) throw new Error(\"Model load failed: n_embd / n_head must be divisible by 4.\");\n    const tokenParam = this.bufferSize(params.vocab_size, params.n_embd);\n    let minSplits = Math.ceil(tokenParam / this.device.limits.maxStorageBufferBindingSize);\n    function vocabChunkSizeCalc(vocab_size, n_embd, splits, maxStorageBufferBindingSize) {\n      // Possibly could be better? Needs actual benchmarking to know what approach is best.\n      const optimisticSize = Math.ceil(vocab_size / splits / 4) * 4 * n_embd;\n      const pessimiticSize = Math.floor(vocab_size / splits / 4) * 4 * n_embd;\n      let vocab_chunk_size = optimisticSize;\n      if (optimisticSize > maxStorageBufferBindingSize) {\n        vocab_chunk_size = pessimiticSize;\n        if (pessimiticSize * splits < tokenParam) {\n          return vocabChunkSizeCalc(vocab_size, n_embd, splits + 1, maxStorageBufferBindingSize);\n        }\n      }\n      return { vocab_chunk_size: vocab_chunk_size / n_embd, splits };\n    }\n    const { vocab_chunk_size, splits } = vocabChunkSizeCalc(params.vocab_size, params.n_embd, minSplits, this.device.limits.maxStorageBufferBindingSize);\n    if (splits > minSplits) console.warn(`Non-optimal number of vocab splits. Optimal: ${minSplits}, Selected: ${splits}`);\n\n    // Set derived parameters\n    params.vocab_chunk_size = vocab_chunk_size;\n    params.vocab_chunk_instances = splits;\n    params.head_size = params.n_embd / params.n_head;\n    params.hidden_size = params.n_embd * 4;\n    params.attention_scale = 1 / Math.sqrt(params.n_embd / params.n_head);\n    params.bias = params.bias == undefined ? true : params.bias;\n\n    // Check for overflow in buffers larger than maxStorageBufferBindingSize\n    const maxBufferSize = this.device.limits.maxStorageBufferBindingSize / 4;\n    if (params.n_embd * params.n_ctx > maxBufferSize) console.warn(\"Model load failed: n_embd * n_ctx must be less than maxStorageBufferBindingSize.\");\n    if (params.n_embd * params.hidden_size > maxBufferSize)\n      console.warn(\"Model load failed: n_embd * hidden_size must be less than maxStorageBufferBindingSize.\");\n    if (params.n_ctx * params.n_ctx * params.n_head > maxBufferSize)\n      console.warn(\"Model load failed: n_ctx * n_ctx must be less than maxStorageBufferBindingSize.\");\n    if (params.n_embd * params.n_embd * 3 > maxBufferSize)\n      console.warn(\"Model load failed: n_embd * n_embd * 3 must be less than maxStorageBufferBindingSize.\");\n\n    console.log(\"Params:\", params);\n\n    return params;\n  }\n\n  async loadEmbeddings(params, weightsFolder) {\n    console.log(\"Loading token embeddings...\");\n    const embeddingWeights = await fetchBin(`${weightsFolder}/transformer.wte.weight_gpt.bin`);\n\n    // Chunks are stored in row-major order and are of dimensions n_embd x vocab_chunk_size.\n    // Embedding weights are imported in column-major order and are of dimensions vocab_size x n_embd.\n    // We pre-transpose the chunk for the deEmbedding process for the matmul. Could do this on GPU later.\n    const embeddingsBuffers = [];\n    const deEmbeddingsBuffers = [];\n    for (let i = 0; i < params.vocab_chunk_instances; i++) {\n      console.log(`Loading deEmbedding chunk ${i + 1}/${params.vocab_chunk_instances}...`);\n      const offset = i * params.vocab_chunk_size;\n      let size = params.vocab_chunk_size;\n\n      const paddedArray = new Float32Array(params.vocab_chunk_size * params.n_embd);\n      if (i === params.vocab_chunk_instances - 1) {\n        size = params.vocab_size - offset;\n        paddedArray.set(size * params.n_embd, zeros((params.vocab_chunk_size * params.vocab_chunk_instances - params.vocab_size) * params.n_embd));\n      }\n      paddedArray.set(embeddingWeights.subarray(offset * params.n_embd, offset * params.n_embd + size * params.n_embd));\n\n      embeddingsBuffers.push(this.initTensor(paddedArray, [params.vocab_chunk_size, params.n_embd], [\"copy_from\"]));\n\n      const chunk = transpose(paddedArray, params.vocab_chunk_size, params.n_embd); // Use GPU perhaps?\n      deEmbeddingsBuffers.push(this.initTensor(chunk, [params.n_embd, params.vocab_chunk_size], [\"storage\"]));\n    }\n\n    return { embeddingsBuffers, deEmbeddingsBuffers };\n  }\n\n  async loadPositionalEmbeddings(params, weightsFolder) {\n    console.log(\"Loading positional embeddings...\");\n    const posEmbeddings = await fetchBin(`${weightsFolder}/transformer.wpe.weight_gpt.bin`);\n    const posEmbdBuffer = this.initTensor(posEmbeddings, [params.n_ctx, params.n_embd], [\"copy_from\"]);\n\n    return { posEmbdBuffer };\n  }\n\n  async loadFinalLayerNorm(params, weightsFolder) {\n    console.log(\"Loading final norm...\");\n    const prefix = `${weightsFolder}/transformer.ln_f.`;\n\n    const tensorPromises = [\n      this.fetchAndInitTensor(`${prefix}weight_gpt.bin`, [params.n_embd], [\"storage\"]),\n      this.fetchAndInitTensor(`${prefix}bias_gpt.bin`, [params.n_embd], [\"storage\"]),\n    ];\n\n    const [normGammaBuffer, normBetaBuffer] = await Promise.all(tensorPromises);\n\n    return { normGammaBuffer, normBetaBuffer };\n  }\n\n  async loadLayers(params, weightsFolder) {\n    console.log(\"Loading layers...\");\n    const layerPromises = [];\n\n    for (let i = 0; i < params.n_layer; i++) {\n      layerPromises.push(this.loadLayer(params, weightsFolder, i));\n    }\n\n    const layer_buffers = await Promise.all(layerPromises);\n    return layer_buffers;\n  }\n\n  async loadLayer(params, weightsFolder, layerIndex) {\n    console.log(\"Starting to load layer...\", layerIndex);\n    const prefix = `${weightsFolder}transformer.h.${layerIndex}.`;\n\n    // Create an array of promises for fetching and initializing the tensors\n    const tensorPromises = [\n      this.fetchAndInitTensor(`${prefix}ln_1.weight_gpt.bin`, [params.n_embd], [\"storage\"]),\n      this.fetchAndInitTensor(`${prefix}ln_1.bias_gpt.bin`, [params.n_embd], [\"storage\"]),\n      this.fetchAndSplitQKVWeightTensors(`${prefix}attn.c_attn.weight_gpt.bin`, [params.n_embd, 3 * params.n_embd], [\"storage\"]),\n      this.fetchAndSplitQKVBiasTensors(`${prefix}attn.c_attn.bias_gpt.bin`, [params.n_embd], [\"storage\"]),\n      this.fetchAndInitTensor(`${prefix}attn.c_proj.weight_gpt.bin`, [params.n_embd, params.n_embd], [\"storage\"]),\n      this.fetchAndInitTensor(`${prefix}attn.c_proj.bias_gpt.bin`, [params.n_embd], [\"storage\"]),\n      this.fetchAndInitTensor(`${prefix}ln_2.weight_gpt.bin`, [params.n_embd], [\"storage\"]),\n      this.fetchAndInitTensor(`${prefix}ln_2.bias_gpt.bin`, [params.n_embd], [\"storage\"]),\n      this.fetchAndInitTensor(`${prefix}mlp.c_fc.weight_gpt.bin`, [params.n_embd, params.hidden_size], [\"storage\"]),\n      this.fetchAndInitTensor(`${prefix}mlp.c_fc.bias_gpt.bin`, [params.hidden_size], [\"storage\"]),\n      this.fetchAndInitTensor(`${prefix}mlp.c_proj.weight_gpt.bin`, [params.hidden_size, params.n_embd], [\"storage\"]),\n      this.fetchAndInitTensor(`${prefix}mlp.c_proj.bias_gpt.bin`, [params.n_embd], [\"storage\"]),\n    ];\n\n    // Wait for all tensors to be fetched and initialized\n    const [\n      normAttentionGammaBuffer,\n      normAttentionBetaBuffer,\n      qkvWeightArray,\n      qkvBiasArray,\n      linearWeightsBuffer,\n      linearBiasBuffer,\n      normLinearGammaBuffer,\n      normLinearBetaBuffer,\n      firstLayerWeightsBuffer,\n      firstLayerBiasBuffer,\n      secondLayerWeightsBuffer,\n      secondLayerBiasBuffer,\n    ] = await Promise.all(tensorPromises);\n\n    // Process the fetched data and return the layer buffers\n    return {\n      normAttentionGammaBuffer,\n      normAttentionBetaBuffer,\n      qkvWeightArray,\n      qkvBiasArray,\n      linearWeightsBuffer,\n      linearBiasBuffer,\n      normLinearGammaBuffer,\n      normLinearBetaBuffer,\n      firstLayerWeightsBuffer,\n      firstLayerBiasBuffer,\n      secondLayerWeightsBuffer,\n      secondLayerBiasBuffer,\n    };\n  }\n\n  async fetchAndSplitQKVWeightTensors(url, dims, ops) {\n    const data = transpose(await fetchBin(url), dims[0], dims[1]);\n\n    const qWeights = transpose(data.subarray(0, dims[0] * dims[0]), dims[0], dims[0]);\n    const kWeights = transpose(data.subarray(dims[0] * dims[0], dims[0] * dims[0] * 2), dims[0], dims[0]);\n    const vWeights = transpose(data.subarray(dims[0] * dims[0] * 2, dims[0] * dims[0] * 3), dims[0], dims[0]);\n\n    const qWeightsBuffer = this.initTensor(qWeights, [dims[0], dims[0]], ops);\n    const kWeightsBuffer = this.initTensor(kWeights, [dims[0], dims[0]], ops);\n    const vWeightsBuffer = this.initTensor(vWeights, [dims[0], dims[0]], ops);\n\n    return [qWeightsBuffer, kWeightsBuffer, vWeightsBuffer];\n  }\n\n  async fetchAndSplitQKVBiasTensors(url, dims, ops) {\n    const data = await fetchBin(url);\n\n    const qBias = data.subarray(0, dims[0]);\n    const kBias = data.subarray(dims[0], dims[0] * 2);\n    const vBias = data.subarray(dims[0] * 2, dims[0] * 3);\n\n    const qBiasBuffer = this.initTensor(qBias, [dims[0]], ops);\n    const kBiasBuffer = this.initTensor(kBias, [dims[0]], ops);\n    const vBiasBuffer = this.initTensor(vBias, [dims[0]], ops);\n\n    return [qBiasBuffer, kBiasBuffer, vBiasBuffer];\n  }\n\n  async fetchAndInitTensor(url, dims, ops) {\n    console.log(\"Fetching and initializing tensor...\", url);\n    const data = await fetchBin(url);\n    return this.initTensor(data, dims, ops);\n  }\n\n  initTensor(data, dims, ops) {\n    const buffer = this.device.createBuffer({\n      size: this.bufferSize(dims[0], dims[1] || 1, dims[2] || 1),\n      usage: ops.map((u) => bufferUsageDict[u]).reduce((a, b) => a | b),\n      mappedAtCreation: true,\n    });\n    new Float32Array(buffer.getMappedRange()).set(data);\n    buffer.unmap();\n    this.unloadDeletionStack.push(buffer);\n    return buffer;\n  }\n\n  unloadBuffers() {\n    this.unloadDeletionStack.map((buffer) => buffer.destroy());\n    this.unloadDeletionStack = [];\n  }\n\n  bufferSize(dimX, dimY = 1, dimZ = 1) {\n    const size = Math.ceil((dimX * dimY * dimZ * Float32Array.BYTES_PER_ELEMENT) / this.minBufferOffset) * this.minBufferOffset;\n    if (size > this.device.limits.maxStorageBufferBindingSize)\n      console.warn(\"Warning: Buffer size calc result exceeds GPU limit, are you using this value for a tensor size?\", dimX, dimY, dimZ, size);\n    return size;\n  }\n}\n"
        },
        {
          "name": "other",
          "type": "tree",
          "content": null
        },
        {
          "name": "tokenizer.js",
          "type": "blob",
          "size": 5.4345703125,
          "content": "class Tokenizer {\n  constructor() {\n    this.encoder = undefined;\n    this.decoder = undefined;\n    this.vocab_size = undefined;\n  }\n\n  async load() {\n    throw new Error(\"Not implemented.\");\n  }\n\n  getVocabSize() {\n    return this.vocab_size;\n  }\n\n  encode(str) {\n    throw new Error(\"Not implemented.\");\n  }\n\n  decode(arr) {\n    throw new Error(\"Not implemented.\");\n  }\n}\n\nclass SimpleTokenizer extends Tokenizer {\n  constructor() {\n    super();\n  }\n\n  async load() {\n    console.log(\"Loading simple tokenizer...\");\n    this.encoder = await (await fetch(\"weights/tokenization/simple_tokens.json\")).json();\n    this.decoder = Object.keys(this.encoder).reduce((acc, x) => ({ ...acc, [this.encoder[x]]: x }), {});\n    this.vocab_size = Object.keys(this.encoder).length;\n  }\n\n  encode(str) {\n    return str.split(\"\").map((x) => this.encoder[x]);\n  }\n\n  decode(arr) {\n    return arr.map((x) => this.decoder[x]).join(\"\");\n  }\n}\n\n// ------------------ GPT Tokenizer ------------------\n// Credit to https://github.com/latitudegames/GPT-3-Encoder\n\nclass GPT2Tokenizer extends Tokenizer {\n  constructor() {\n    super();\n    this.pat = /'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+/gu;\n    this.textEncoder = new TextEncoder(); // always utf-8 by spec\n    this.textDecoder = new TextDecoder(\"utf-8\");\n  }\n\n  async load() {\n    console.log(\"Loading GPT2 tokenizer...\");\n\n    const bpe_file = await (await fetch(\"weights/tokenization/vocab.bpe\")).text();\n    const encoder = await (await fetch(\"weights/tokenization/gpt_tokens.json\")).json();\n    this.encoder = encoder;\n\n    console.log(\"Building decoder...\");\n    const decoder = {};\n    Object.keys(encoder).map((x) => {\n      decoder[encoder[x]] = x;\n    });\n    this.decoder = decoder;\n\n    const lines = bpe_file.split(\"\\n\");\n    const bpe_merges = lines.slice(1, lines.length - 1).map((x) => {\n      return x.split(/(\\s+)/).filter(function (e) {\n        return e.trim().length > 0;\n      });\n    });\n\n    const byte_encoder = bytes_to_unicode();\n    const byte_decoder = {};\n    Object.keys(byte_encoder).map((x) => {\n      byte_decoder[byte_encoder[x]] = x;\n    });\n    this.byte_encoder = byte_encoder;\n    this.byte_decoder = byte_decoder;\n\n    this.bpe_ranks = dictZip(bpe_merges, range(0, bpe_merges.length));\n    this.cache = new Map();\n    this.vocab_size = Object.keys(encoder).length;\n  }\n\n  encode(text) {\n    if (!this.byte_encoder) throw new Error(\"Tokenizer not loaded.\");\n    let bpe_tokens = [];\n    const matches = Array.from(text.matchAll(this.pat)).map((x) => x[0]);\n    for (let token of matches) {\n      const encoded_bytes = this.textEncoder.encode(token);\n      let bytes = [];\n      for (let i = 0; i < encoded_bytes.length; i++) {\n        bytes.push(this.byte_encoder[encoded_bytes[i].toString()]);\n      }\n      token = bytes.join(\"\");\n\n      const new_tokens = this.bpe(token)\n        .split(\" \")\n        .map((x) => this.encoder[x]);\n      bpe_tokens = bpe_tokens.concat(new_tokens);\n    }\n    return bpe_tokens;\n  }\n\n  decode(tokens) {\n    if (!this.byte_decoder) throw new Error(\"Tokenizer not loaded.\");\n    let text = tokens.map((x) => this.decoder[x]).join(\"\");\n    text = this.textDecoder.decode(new Uint8Array(text.split(\"\").map((x) => this.byte_decoder[x])));\n    return text;\n  }\n\n  bpe(token) {\n    if (this.cache.has(token)) return this.cache.get(token);\n    let word = token.split(\"\");\n    let pairs = get_pairs(word);\n    if (!pairs) return token;\n    while (true) {\n      const minPairs = {};\n      pairs.forEach(pair => {\n        const rank = this.bpe_ranks[pair];\n        minPairs[isNaN(rank) ? 10e10 : rank] = pair;\n      });\n      const keys = Object.keys(minPairs).map((x) => parseInt(x));\n      const bigram = minPairs[Math.min(...keys)];\n      if (!Object.hasOwn(this.bpe_ranks, bigram)) break;\n      const first = bigram[0];\n      const second = bigram[1];\n      let new_word = [];\n      let i = 0;\n      while (i < word.length) {\n        const j = word.indexOf(first, i);\n        if (j === -1) {\n          new_word = new_word.concat(word.slice(i));\n          break;\n        }\n        new_word = new_word.concat(word.slice(i, j));\n        i = j;\n        if (word[i] === first && i < word.length - 1 && word[i + 1] === second) {\n          new_word.push(first + second);\n          i = i + 2;\n        } else {\n          new_word.push(word[i]);\n          i = i + 1;\n        }\n      }\n      word = new_word;\n      if (word.length === 1) break;\n      else pairs = get_pairs(word);\n    }\n    word = word.join(\" \");\n    this.cache.set(token, word);\n    return word;\n  }\n}\n\nconst range = (x, y) => {\n  const res = [];\n  for (let i = x; i < y; i++) { res.push(i) }\n  return res;\n};\n\nconst ord = (x) => {\n  return x.charCodeAt(0);\n};\n\nconst dictZip = (x, y) => {\n  const result = {};\n  x.map((_, i) => {\n    result[x[i]] = y[i];\n  });\n  return result;\n};\n\nconst bytes_to_unicode = () => {\n  const bs = range(ord(\"!\"), ord(\"~\") + 1).concat(range(ord(\"¡\"), ord(\"¬\") + 1), range(ord(\"®\"), ord(\"ÿ\") + 1));\n  let cs = bs.slice();\n  let n = 0;\n  for (let b = 0; b < 2 ** 8; b++) {\n    if (!bs.includes(b)) {\n      bs.push(b);\n      cs.push(2 ** 8 + n);\n      n = n + 1;\n    }\n  }\n  cs = cs.map((x) => String.fromCharCode(x));\n  const result = {};\n  bs.map((_, i) => {\n    result[bs[i]] = cs[i];\n  });\n  return result;\n};\n\nconst get_pairs = (word) => {\n  const pairs = new Set();\n  let prev_char = word[0];\n  for (let i = 1; i < word.length; i++) {\n    const char = word[i];\n    pairs.add([prev_char, char]);\n    prev_char = char;\n  }\n  return pairs;\n};\n"
        },
        {
          "name": "visuals.js",
          "type": "blob",
          "size": 6.4072265625,
          "content": "class Visuals {\n\n  initialized = false;\n\n  constructor(model) {\n    this.model = model;\n    this.device = model.device;\n    this.params = model.params;\n  }\n\n  init() {\n    this.initFoundation();\n    this.initUniforms();\n    this.initLayoutAndPipeline();\n    this.initBuffersAndBindGroup();\n    this.updateModelBuffer();\n\n    this.initialized = true;\n  }\n\n  initFoundation() {\n    const containerEl = document.getElementById(\"visualsContainer\");\n    const gpuCanvasEl = document.createElement(\"canvas\");\n\n    containerEl.style.width = this.params.n_embd + \"px\";\n    containerEl.style.height = this.params.n_ctx + \"px\";\n    gpuCanvasEl.style.width = \"100%\";\n    gpuCanvasEl.style.height = \"100%\";\n    gpuCanvasEl.width = this.params.n_embd;\n    gpuCanvasEl.height = this.params.n_ctx;\n\n    const gpuContext = gpuCanvasEl.getContext(\"webgpu\");\n    const gpuCanvasFormat = navigator.gpu.getPreferredCanvasFormat();\n\n    gpuContext.configure({\n      device: this.device,\n      format: gpuCanvasFormat,\n    });\n\n    containerEl.appendChild(gpuCanvasEl);\n\n    this.containerEl = containerEl;\n    this.gpuCanvasFormat = gpuCanvasFormat;\n    this.gpuCanvasEl = gpuCanvasEl;\n    this.gpuContext = gpuContext;\n  }\n\n  updateModelBuffer() {\n    this.model.externalBuffer = this.embeddingsBuffer;\n  }\n\n  initUniforms() {\n    this.uniforms = {\n      width: this.model.params.n_embd,\n      height: this.model.params.n_ctx,\n    };\n  }\n\n  initLayoutAndPipeline() {\n    this.bindGroupLayout = this.device.createBindGroupLayout({\n      entries: [\n        {\n          binding: 0,\n          visibility: GPUShaderStage.FRAGMENT,\n          buffer: {\n            type: \"uniform\",\n          }\n        },\n        {\n          binding: 1,\n          visibility: GPUShaderStage.FRAGMENT,\n          buffer: {\n            type: \"read-only-storage\",\n          }\n        },\n      ]\n    });\n\n    this.renderShaderModule = this.device.createShaderModule({\n      label: 'visuals',\n      code: `\n        struct UniformData {\n          width: f32,\n          height: f32,\n        }\n\n        @vertex\n        fn vsMain(@builtin(vertex_index) vertexIndex: u32) -> @builtin(position) vec4<f32> {\n          var positions = array<vec2<f32>, 6>(\n            vec2<f32>(-1.0, -1.0), // bottom left\n            vec2<f32>( 1.0, -1.0), // bottom right\n            vec2<f32>(-1.0,  1.0), // top left\n            vec2<f32>(-1.0,  1.0), // top left\n            vec2<f32>( 1.0, -1.0), // bottom right\n            vec2<f32>( 1.0,  1.0)  // top right\n          );\n          return vec4<f32>(positions[vertexIndex], 0.0, 1.0);\n        }\n\n        @group(0) @binding(0) var<uniform> uniformData: UniformData;\n        @group(0) @binding(1) var<storage, read> embeddingsBuffer: array<f32>;\n\n        @fragment\n        fn fsMain(@builtin(position) fragCoord: vec4<f32>) -> @location(0) vec4<f32> {\n          let xNormalized = fragCoord.x / uniformData.width;\n          let yNormalized = fragCoord.y / uniformData.height;\n\n          let xIndex = xNormalized * ${this.params.n_embd};\n          let yIndex = yNormalized * ${this.params.n_ctx};\n          let index = u32(yIndex) * ${this.params.n_embd} + u32(xIndex);\n\n          let vectorValue = embeddingsBuffer[index];\n\n          var outColor = vec4<f32>(0.0);\n          outColor = hdrColorMapping(outColor, 1.0, vectorValue * 0.1);\n\n          return outColor;\n        }\n\n        fn hdrColorMapping(colorRef: vec4<f32>, hdrThreshold: f32, vectorValue: f32) -> vec4<f32> {\n          var color = colorRef;\n\n          if (vectorValue < 0.0) {\n            color.b = -vectorValue;\n            if (vectorValue < -hdrThreshold) {\n              color.g = -vectorValue - hdrThreshold;\n            }\n          } else {\n            color.r = vectorValue;\n            if (vectorValue > hdrThreshold) {\n              color.g = vectorValue - hdrThreshold;\n            }\n          }\n          color.g = min(color.g, 0.7);\n          return color;\n        }\n      `\n    });\n\n    this.renderPipeline = this.device.createRenderPipeline({\n      layout: this.device.createPipelineLayout({\n        bindGroupLayouts: [this.bindGroupLayout],\n      }),\n      vertex: {\n        module: this.renderShaderModule,\n        entryPoint: 'vsMain',\n        buffers: []\n      },\n      fragment: {\n        module: this.renderShaderModule,\n        entryPoint: 'fsMain',\n        targets: [\n          {\n            format: this.gpuCanvasFormat,\n          },\n        ],\n      },\n    });\n  }\n\n  initBuffersAndBindGroup() {\n    const uniformCount = Object.values(this.uniforms).length;\n\n    this.uniformBuffer = this.device.createBuffer({\n      size: uniformCount * Float32Array.BYTES_PER_ELEMENT,\n      usage: GPUBufferUsage.UNIFORM | GPUBufferUsage.COPY_DST,\n    });\n\n    this.embeddingsBuffer = this.device.createBuffer({\n      size: this.model.bufferSize(this.params.n_ctx, this.params.n_embd),\n      usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST,\n    });\n\n    this.bindGroup = this.device.createBindGroup({\n      layout: this.bindGroupLayout,\n      entries: [\n        {\n          binding: 0,\n          resource: {\n            buffer: this.uniformBuffer,\n          }\n        },\n        {\n          binding: 1,\n          resource: {\n            buffer: this.embeddingsBuffer,\n          }\n        },\n      ]\n    });\n\n    this.updateUniforms();\n  }\n\n  updateUniforms() {\n    this.uniforms.width = this.gpuCanvasEl.width;\n    this.uniforms.height = this.gpuCanvasEl.height;\n\n    const uniformArray = new Float32Array([\n      this.uniforms.width,\n      this.uniforms.height,\n    ]);\n\n    this.device.queue.writeBuffer(\n      this.uniformBuffer,\n      0,\n      uniformArray.buffer,\n      uniformArray.byteOffset,\n      uniformArray.byteLength,\n    );\n  }\n\n  render(existingCommandEncoder) {\n    const commandEncoder = existingCommandEncoder ?? this.device.createCommandEncoder();\n\n    const textureView = this.gpuContext.getCurrentTexture().createView();\n\n    const renderPassDescriptor = {\n      colorAttachments: [\n        {\n          view: textureView,\n          loadOp: 'clear',\n          loadValue: { r: 0.0, g: 0.0, b: 0.0, a: 1.0 },\n          storeOp: 'store',\n        }\n      ]\n    };\n\n    const passEncoder = commandEncoder.beginRenderPass(renderPassDescriptor);\n    passEncoder.setPipeline(this.renderPipeline);\n    passEncoder.setBindGroup(0, this.bindGroup);\n    passEncoder.draw(6, 1, 0, 0);\n    passEncoder.end();\n\n    this.device.queue.submit([commandEncoder.finish()]);\n  }\n\n  destroy() {\n    this.gpuCanvasEl.remove();\n    this.uniformBuffer.destroy();\n    this.embeddingsBuffer.destroy();\n  }\n}"
        },
        {
          "name": "weights",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}