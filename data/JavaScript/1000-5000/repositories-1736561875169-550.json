{
  "metadata": {
    "timestamp": 1736561875169,
    "page": 550,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjU1MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "ruipgil/scraperjs",
      "stars": 3712,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".editorconfig",
          "type": "blob",
          "size": 0.1845703125,
          "content": "root = true\n\n\n[*]\n\nindent_style = tab\nindent_size = 2\n\nend_of_line = lf\ncharset = utf-8\ntrim_trailing_whitespace = true\ninsert_final_newline = true\n\n[*.md]\ntrim_trailing_whitespace = false\n"
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.1787109375,
          "content": ".DS_Store\n*.swp\n.project\n.DS_Store?\n._*\n.Spotlight-V100\n.Trashes\nIcon?\nehthumbs.db\nThumbs.db\n*.sublime-project\n*.sublime-workspace\n*.ignore\nnode_modules\nbower_components\ndocs\ncoverage"
        },
        {
          "name": ".jshintrc",
          "type": "blob",
          "size": 0.3857421875,
          "content": "{\n  \"bitwise\": false,\n  \"camelcase\": true,\n  \"curly\": true,\n  \"eqeqeq\": false,\n  \"es3\": false,\n  \"freeze\": true,\n  \"immed\": true,\n  \"latedef\": true,\n  \"newcap\": true,\n  \"noarg\": false,\n  \"noempty\": true,\n  \"nonbsp\": true,\n  \"nonew\": true,\n  \"plusplus\": false,\n  \"quotmark\": \"single\",\n  \"undef\": true,\n  \"unused\": true,\n  \n  \"trailing\": true,\n  \"boss\": true,\n  \"browser\": false,\n  \"node\": true\n}\n"
        },
        {
          "name": ".travis.yml",
          "type": "blob",
          "size": 0.1181640625,
          "content": "sudo: false\nlanguage: node_js\nnode_js:\n  - \"0.12\"\n  - \"4.0\"\n  - \"0.11\"\n  - \"0.10\"\nscript: grunt coveralls && grunt clean\n"
        },
        {
          "name": "CHANGELOG.md",
          "type": "blob",
          "size": 1.6083984375,
          "content": "# Change Log\n\n## [1.2.0] - 2015-12-14\n- Change order of parameters when continuing promise chain.\n- ``` async ```'s callback function receives (err, result) parameters.\n- Last result is passed to ``` done ``` promise.\n\n## [1.1.0] - 2015-12-12\n- Result of last promise is passed as first parameter to the ``` async ``` promise.\n\n## [1.0.2] - 2015-12-10\n- Dependency bump\n\n## [1.0.0] - 2015-10-17\n- ``` catch ``` promise is the new standard way to deal with errors. ``` onError ``` is being deprecated, the two work the same way.\n- ``` then ``` promise receives the value returned in the last promise as the first parameter, the second parameter is the ``` utils ``` object.\n- Errors generated inside the dynamic scraper's scraping function will fire the ``` catch ``` promise.\n\n## [0.4.1] - 2015-10-04\n- Url of the page being scraped can now be easily accessed using ``` utils.url ```.\n- Added error handling example.\n\n## [0.4.0] - 2015-09-19\n- Passing utils to the error callback by @rvernica\n- Add an 'options' argument to DynamicScraper that get passed to Phantom by @vdraceil\n- Updated dependencies\n\n## [0.3.4] - 2015-04-26\n- Minor fixes related with documentation.\n- Fixed ``` async ``` promise works. It can receive values to be passed to the next promise. Internally it now uses this mechanism.\n- Support for node 0.12.\n- Changes in the command-line interface.\n\n## [0.3.3] - 2015-02-11\n- Fixed bug where no argument was given to the ```done``` promise when there was an error.\n- Added experimental support for command-line interface.\n- Added example.\n\n## [0.3.2] - 2014-12-20\n- The ```lastResult``` is now made accessible to a ```Router```.\n"
        },
        {
          "name": "Gruntfile.js",
          "type": "blob",
          "size": 2.8701171875,
          "content": "var testServer = require('./test/setupServer');\n\nvar MOCHA_TIMEOUT_S = 10,\n\tMOCHA_TIMEOUT_MS = MOCHA_TIMEOUT_S * 1000,\n\tMOCHA_OPTIONS = {\n\t\treporter: 'spec',\n\t\ttimeout: MOCHA_TIMEOUT_MS\n\t},\n\tCOVERAGE_THRESHOLD = 95;\n\nmodule.exports = function(grunt) {\n\tgrunt.loadNpmTasks('grunt-mocha-test');\n\tgrunt.loadNpmTasks('grunt-contrib-jshint');\n\tgrunt.loadNpmTasks('grunt-contrib-watch');\n\tgrunt.loadNpmTasks('grunt-contrib-clean');\n\tgrunt.loadNpmTasks('grunt-exec');\n\n\tgrunt.initConfig({\n\t\texec: {\n\t\t\tcoverage: {\n\t\t\t\tcommand: 'istanbul cover ./node_modules/mocha/bin/_mocha -x src/PhantomWrapper.js -- -t ' + MOCHA_TIMEOUT_MS + ' --root src/ test/'\n\t\t\t},\n\t\t\tcoveralls: {\n\t\t\t\tcommand: 'istanbul cover ./node_modules/mocha/bin/_mocha -x src/PhantomWrapper.js --report lcovonly -- -t ' + MOCHA_TIMEOUT_MS + ' -x src/PhantomWrapper.js --root src/ test/ && cat ./coverage/lcov.info | ./node_modules/coveralls/bin/coveralls.js'\n\t\t\t},\n\t\t\t'check-coverage': {\n\t\t\t\tcommand: 'istanbul check-coverage --lines ' + COVERAGE_THRESHOLD + ' --statements ' + COVERAGE_THRESHOLD + ' --functions ' + COVERAGE_THRESHOLD + ' --branches ' + COVERAGE_THRESHOLD + ' ./coverage/coverage.json'\n\t\t\t}\n\t\t},\n\t\tclean: {\n\t\t\tcoverage: {\n\t\t\t\tsrc: ['coverage/']\n\t\t\t}\n\t\t},\n\t\twatch: {\n\t\t\tcommon: {\n\t\t\t\tfiles: ['src/**/*.js', 'test/**/*.js', 'Gruntfile.js'],\n\t\t\t\ttasks: ['test']\n\t\t\t}\n\t\t},\n\t\tjshint: {\n\t\t\tall: ['src/**/*.js', 'test/**/*.js']\n\t\t},\n\t\tmochaTest: {\n\t\t\tabstractScraper: {\n\t\t\t\tsrc: 'test/AbstractScraper.js',\n\t\t\t\toptions: MOCHA_OPTIONS\n\t\t\t},\n\t\t\tstaticScraper: {\n\t\t\t\tsrc: 'test/StaticScraper.js',\n\t\t\t\toptions: MOCHA_OPTIONS\n\t\t\t},\n\t\t\tdynamicScraper: {\n\t\t\t\tsrc: 'test/DynamicScraper.js',\n\t\t\t\toptions: MOCHA_OPTIONS\n\t\t\t},\n\t\t\tscraperPromise: {\n\t\t\t\tsrc: 'test/ScraperPromise.js',\n\t\t\t\toptions: MOCHA_OPTIONS\n\t\t\t},\n\t\t\trouter: {\n\t\t\t\tsrc: 'test/Router.js',\n\t\t\t\toptions: MOCHA_OPTIONS\n\t\t\t},\n\t\t\tscraperError: {\n\t\t\t\tsrc: 'test/ScraperError.js',\n\t\t\t\toptions: MOCHA_OPTIONS\n\t\t\t},\n\t\t\tcommandLine: {\n\t\t\t\tsrc: 'test/commandLine.js',\n\t\t\t\toptions: MOCHA_OPTIONS\n\t\t\t},\n\t\t\tall: {\n\t\t\t\tsrc: ['test/AbstractScraper.js', 'test/StaticScraper.js', 'test/DynamicScraper.js', 'test/ScraperPromise.js', 'test/Router.js', 'test/ScraperError.js', 'test/commandLine.js'],\n\t\t\t\toptions: MOCHA_OPTIONS\n\t\t\t}\n\t\t}\n\t});\n\n\tvar server;\n\n\tgrunt.registerTask('serve', 'Starts express testing server', function() {\n\t\tserver = testServer(grunt);\n\t});\n\n\tgrunt.registerTask('unserve', function() {\n\t\tif (server) {\n\t\t\tserver.close();\n\t\t}\n\t});\n\n\tgrunt.registerTask('serve-and-test', ['serve', 'mochaTest:all', 'unserve']);\n\n\tgrunt.registerTask('coverage', ['clean', 'jshint', 'serve', 'exec:coverage', 'exec:check-coverage', 'unserve']);\n\tgrunt.registerTask('coveralls', ['clean', 'jshint', 'serve', 'exec:coveralls', 'exec:check-coverage', 'unserve']);\n\n\tgrunt.registerTask('unit', ['jshint', 'serve-and-test']);\n\tgrunt.registerTask('test', ['coverage']);\n\n\tgrunt.registerTask('watch-all', ['serve', 'watch', 'unserve']);\n};"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0302734375,
          "content": "\nCopyright (C) 2013 Rui Gil\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is \nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in \nall copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, \nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER \nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE."
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 12.8359375,
          "content": "# Scraperjs\n[![Build Status](https://travis-ci.org/ruipgil/scraperjs.svg?branch=master)](https://travis-ci.org/ruipgil/scraperjs) [![Dependency Status](https://gemnasium.com/ruipgil/scraperjs.svg)](https://gemnasium.com/ruipgil/scraperjs) [![Coverage Status](https://coveralls.io/repos/ruipgil/scraperjs/badge.svg?branch=master)](https://coveralls.io/r/ruipgil/scraperjs?branch=master) [![NPM version](https://badge.fury.io/js/scraperjs.svg)](http://badge.fury.io/js/scraperjs) [![Inline docs](http://inch-ci.org/github/ruipgil/scraperjs.svg?branch=master)](http://inch-ci.org/github/ruipgil/scraperjs)\n\nScraperjs is a web scraper module that make scraping the web an easy job.\n\n## Installing\n\n```\nnpm install scraperjs\n```\n\nIf you would like to test (this is optional and requires the installation with the ``` --save-dev ``` tag),\n```\ngrunt test\n```\n\n*To use some features you’ll need to install [phantomjs](http://phantomjs.org/download.html), if you haven’t already*\n\n# Getting started\n\nScraperjs exposes two different scrapers,\n+ a **StaticScraper**, that is light fast and with a low footprint, however it doesn't allow for more complex situations, like scraping dynamic content.\n+ a **DynamicScraper**, that is a bit more heavy, but allows you to scrape dynamic content, like in the browser console.\nboth scrapers expose a *very* similar API, with some minor differences when it comes to scraping.\n\n## Lets scrape [Hacker News](https://news.ycombinator.com/), with both scrapers.\n\nTry to spot the differences.\n\n### Static Scraper\n\n```javascript\nvar scraperjs = require('scraperjs');\nscraperjs.StaticScraper.create('https://news.ycombinator.com/')\n\t.scrape(function($) {\n\t\treturn $(\".title a\").map(function() {\n\t\t\treturn $(this).text();\n\t\t}).get();\n\t})\n\t.then(function(news) {\n\t\tconsole.log(news);\n\t})\n```\n\nThe ```scrape``` promise receives a function that will scrape the page and return the result, it only receives jQuery a parameter to scrape the page. Still, very powerful. It uses [cheerio](https://github.com/cheeriojs/cheerio) to do the magic behind the scenes.\n\n### Dynamic Scraper\n\n```javascript\nvar scraperjs = require('scraperjs');\nscraperjs.DynamicScraper.create('https://news.ycombinator.com/')\n\t.scrape(function($) {\n\t\treturn $(\".title a\").map(function() {\n\t\t\treturn $(this).text();\n\t\t}).get();\n\t})\n\t.then(function(news) {\n\t\tconsole.log(news);\n\t})\n```\n\nAgain, the ```scrape``` promise receives a function to scrape the page, the only difference is that, because we're using a dynamic scraper, the scraping function is [sandboxed](https://github.com/sgentle/phantomjs-node/wiki#evaluating-pages) only with the page scope, so **no closures!** This means that in *this* (and only in this) scraper you can't call a function that has not been defined inside the scraping function. Also, the result of the scraping function must be [JSON-serializable](https://github.com/sgentle/phantomjs-node/wiki#evaluating-pages).\nWe use [phantom](https://github.com/sgentle/phantomjs-node) and [phantomjs](https://github.com/ariya/phantomjs) to make it happen, we also inject jQuery for you.\n\nHowever, it's possible to [pass JSON-serializable data](**Example**) to *any* scraper.\n\n*The ```$``` varible received by the scraping function is, only for the dynamic scraper, hardcoded.*\n\n## Show me the way! (aka Routes)\n\nFor a more flexible scraping and crawling of the web sometimes we need to go through multiple web sites and we don't want map every possible url format. For that scraperjs provides the Router class.\n\n### Example\n\n```javascript\nvar scraperjs = require('scraperjs'),\n\trouter = new scraperjs.Router();\n\nrouter\n\t.otherwise(function(url) {\n\tconsole.log(\"Url '\"+url+\"' couldn't be routed.\");\n});\n\nvar path = {};\n\nrouter.on('https?://(www.)?youtube.com/watch/:id')\n\t.createStatic()\n\t.scrape(function($) {\n\t\treturn $(\"a\").map(function() {\n\t\t\treturn $(this).attr(\"href\");\n\t\t}).get();\n\t})\n\t.then(function(links, utils) {\n\t\tpath[utils.params.id] = links\n\t})\n\nrouter.route(\"https://www.youtube.com/watch/YE7VzlLtp-4\", function() {\n\tconsole.log(\"i'm done\");\n});\n```\n\nCode that allows for parameters in paths is from the project [Routes.js](https://github.com/aaronblohowiak/routes.js), information about the [path formating](https://github.com/aaronblohowiak/routes.js#path-formats) is there too.\n\n# API overview\n\nScraperjs uses promises whenever possible.\n\n#### StaticScraper, DynamicScraper and ScraperPromise\n\nSo, the scrapers should be used with the ScraperPromise. By creating a scraper\n```javascript\nvar scraperPromise = scraperjs.StaticScraper.create() // or DynamicScraper\n```\nThe following promises can be made over it, they all return a scraper promise,\n+ ```onStatusCode(code:number, callback:function(utils:Object))```, executes the callback when the status code is equal to the code,\n+ ```onStatusCode(callback:function(code:number, utils:Object))```, executes the callback when receives the status code. The callback receives the current status code,\n+ ```delay(time:number, callback:function(last:?, utils:Object))```, delays the execution of the chain by time (in milliseconds),\n+ ```timeout(time:number, callback:function(last:?, utils:Object))```, executes the callback function after time (in milliseconds),\n+ ```then(lastResult:?, callback:function(last:?, utils:Object))```, executes the callback after the last promise,\n+ ```async(callback:function(last:?, done:function(result:?, err:?), utils))```, executes the callback, stopping the promise chain, resuming it when the ```done``` function is called. You can provide a result to be passed down the promise chain, or an error to trigger the catch promise,\n+ ```catch(callback:function(error:Error, utils:Object))```, executes the callback when there was an error, errors block the execution of the chain even if the promise was not defined,\n+ ```done(callback:function(last:?, utils:Object))```, executes the callback at the end of the promise chain, this is always executed, even if there was an error,\n+ ```get(url:string)```, makes a simple HTTP GET request to the url. This promise should be used only once per scraper.\n+ ```request(options:Object)```, makes a (possibly) more complex HTTP request, scraperjs uses the [request](https://github.com/mikeal/request) module, and this method is a simple wrapper of ```request.request()```. This promise should be used only once per scraper.\n+ ```scrape(scrapeFn:function(...?), callback:function(result:?, utils:Object)=, ...?)```, scrapes the page. It executes the scrapeFn and passes it's result to the callback. When using the StaticScraper, the scrapeFn receives a jQuery function that is used to scrape the page. When using the DynamicScraper, the scrapeFn doesn't receive anything and can only return a [JSON-serializable](https://github.com/sgentle/phantomjs-node/wiki#evaluating-pages) type. Optionally an arbitrary number of arguments can be passed to the scraping function. The callback may be omitted, if so, the result of the scraping may be accessed with the ``` then ``` promise or ``` utils.lastReturn ``` in the next promise.\n\nAll callback functions receive as their last parameter a utils object, with it the parameters of an url from a router can be accessed. Also the chain can be stopped.\n```javascript\nDynamicScraper.create()\n\t.get(\"http://news.ycombinator.com\")\n\t.then(function(_, utils) {\n\t\tutils.stop();\n\t\t// utils.params.paramName\n\t});\n```\n\nThe promise chain is fired with the same sequence it was declared, with the exception of the promises get and request that fire the chain when they've received a valid response, and the promises ``` done ``` and ``` catch ```, which were explained above.\n\nYou can also waterfall values between promises by returning them (with the exception of the promise ```timeout```, that will always return ```undefined```) and it can be access through ```utils.lastReturn```.\n\n##### The ``` utils ``` object\n\nYou've seen the ``` utils ``` object that is passed to promises, it provides useful information and methods to your promises. Here's what you can do with it:\n+ ``` .lastResult ```, value returned in the last promise\n+ ``` .stop() ```, function to stop the promise chain,\n+ ``` .url ```, url provided to do the scraper,\n+ ``` .params ```, object with the parameters defined in the router matching pattern.\n\n\n##### A more powerful DynamicScraper.\n\nWhen lots of instances of DynamicScraper are needed, it's creation gets really heavy on resources and takes a lot of time. To make this more lighter you can use a *factory*, that will create only one PhantomJS instance, and every DynamicScraper will request a page to work with. To use it you must start the factory before any DynamicSrcaper is created, ``` scraperjs.DynamicScraper.startFactory() ``` and then close the factory after the execution of your program, ``` scraperjs.DynamicScraper.closeFactory() ```.\nTo make the scraping function more robust you can inject code into the page,\n```js\nvar ds = scraperjs.DynamicScraper\n\t.create('http://news.ycombinator.com')\n\t.async(function(_, done, utils) {\n\t\tutils.scraper.inject(__dirname+'/path/to/code.js', function(err) {\n\t\t\t// in this case if there was an error won't fire catch promise.\n\t\t\tif(err) {\n\t\t\t\tdone(err);\n\t\t\t} else {\n\t\t\t\tdone();\n\t\t\t}\n\t\t});\n\t})\n\t.scrape(function() {\n\t\treturn functionInTheCodeInjected();\n\t})\n\t.then(function(result) {\n\t\tconsole.log(result);\n\t});\n```\n\n#### Router\n\nThe router should be initialized like a class\n```javascript\nvar router = new scraperjs.Router(options);\n```\n\nThe options object is optional, and these are the options:\n+ ``` firstMatch ```, a boolean, if true the routing will stop once the first path is matched, the default is false.\n\nThe following promises can be made over it,\n+ ```on(path:string|RegExp|function(url:string))```, makes the promise for the match url or regular expression, alternatively you can use a function to accept or not a passed url. The promises ```get``` or ```request``` and ```createStatic``` or ```createDynamic``` are expected after the on promise.\n+ ```get()```, makes so that the page matched will be requested with a simple HTTP request,\n+ ```request(options:Object)```, makes so that the page matched will be requested with a possible more complex HTTP request, , scraperjs uses the [request](https://github.com/mikeal/request) module, and this method is a simple wrapper of [request.request()](https://github.com/mikeal/request#requestoptions-callback),\n+ ```createStatic()```, associates a static scraper to use to scrape the matched page, this returns ScraperPromise, so any promise made from now on will be made over a ScraperPromise of a StaticScraper. Also the ```done``` promise of the scraper will not be available.\n+ ```createDynamic()```, associates a dynamic scraper to use to scrape the matched page, this returns ScraperPromise, so any promise made from now on will be made over a ScraperPromise of a DynamicScraper. Also the ```done``` promise of the scraper will not be available.\n+ ```route(url:string, callback:function(boolean))```, routes an url through all matched paths, calls the callback when it's executed, true is passed if the route was successful, false otherwise.\n+ ```use(scraperInstance:ScraperPromise)```, uses a ScraperPromise already instantiated.\n+ ```otherwise(callback:function(url:string))```, executes the callback function if the routing url didn't match any path.\n+ ```catch(callback:function(url:string, error:Error))```, executes the callback when an error occurred on the routing scope, not on any scraper, for that situations you should use the ```catch``` promise of the scraper.\n\n#### Notes\n\n* Scraperjs **always** fetches the document with `request`, and then when using a DynamicScraper, leverages phantom's `setContent()` to set the body of the page object. This will result in subtly different processing of web pages compared to directly loading a URL in PhantomJS.\n\n#### More\n\nCheck the [examples](./doc/examples), the [tests](./test) or just dig into the code, it's well documented and it's simple to understand.\n\n# Dependencies\n\nAs mentioned above, scraperjs is uses some dependencies to do the the heavy work, such as\n+ [```async```](https://github.com/caolan/async), for flow control\n+ [```request```](https://github.com/mikeal/request), to make HTTP requests, again, if you want more complex requests see it's [documentation](https://github.com/mikeal/request#requestoptions-callback)\n+ [```phantom```](https://github.com/sgentle/phantomjs-node) + [```phantomjs```](https://github.com/ariya/phantomjs), phantom is an awesome module that links node to phantom, used in the DynamicScraper\n+ [```cheerio```](https://github.com/cheeriojs/cheerio), light and fast DOM manipulation, used to implement the StaticScraper\n+ [```jquery```](https://github.com/jquery/jquery), to include jquery in the DynamicScraper\n+ although [```Routes.js```](https://github.com/aaronblohowiak/routes.js) is great, scraperjs doesn't use it to maintain it's \"interface layout\", but the code to transform the path given on the on promise to regular expressions is from them\n\n# License\n\nThis project is under the [MIT](./LICENSE) license. \n"
        },
        {
          "name": "bin",
          "type": "tree",
          "content": null
        },
        {
          "name": "doc",
          "type": "tree",
          "content": null
        },
        {
          "name": "package.json",
          "type": "blob",
          "size": 1.2119140625,
          "content": "{\n  \"name\": \"scraperjs\",\n  \"version\": \"1.2.0\",\n  \"description\": \"A complete and versatile web scraper.\",\n  \"main\": \"./src/Scraper.js\",\n  \"keywords\": [\n    \"scraper\",\n    \"scraping\",\n    \"web\"\n  ],\n  \"bin\": \"./bin/scraperjs\",\n  \"scripts\": {\n    \"test\": \"grunt test\"\n  },\n  \"repository\": {\n    \"type\": \"git\",\n    \"url\": \"git://github.com/ruipgil/scraperjs.git\"\n  },\n  \"bugs\": {\n    \"url\": \"https://github.com/ruipgil/scraperjs/issues\"\n  },\n  \"gitHead\": \"c58be022438e49564597bbb3ad7c036d610744f8\",\n  \"homepage\": \"https://github.com/ruipgil/scraperjs\",\n  \"author\": \"Rui Gil\",\n  \"license\": \"MIT\",\n  \"dependencies\": {\n    \"async\": \"^1.5.0\",\n    \"cheerio\": \"^0.19.0\",\n    \"jquery\": \"^2.1.4\",\n    \"phantom\": \"^0.8.4\",\n    \"request\": \"^2.67.0\",\n    \"commander\": \"^2.9.0\"\n  },\n  \"readmeFilename\": \"README.md\",\n  \"directories\": {\n    \"test\": \"test\"\n  },\n  \"devDependencies\": {\n    \"coveralls\": \"^2.11.1\",\n    \"express\": \"^4.8.3\",\n    \"grunt\": \"^0.4.5\",\n    \"grunt-cli\": \"~0.1.9\",\n    \"grunt-contrib-clean\": \"^0.6.0\",\n    \"grunt-contrib-jshint\": \"^0.10.0\",\n    \"grunt-contrib-watch\": \"^0.6.1\",\n    \"grunt-exec\": \"^0.4.6\",\n    \"grunt-mocha-test\": \"^0.11.0\",\n    \"istanbul\": \"^0.3.0\",\n    \"mocha\": \"^1.21.4\"\n  },\n  \"engines\": {\n    \"node\": \">=0.10\"\n  }\n}\n"
        },
        {
          "name": "src",
          "type": "tree",
          "content": null
        },
        {
          "name": "test",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}