{
  "metadata": {
    "timestamp": 1736557359480,
    "page": 325,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjM1MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "pandas-dev/pandas",
      "stars": 44245,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".circleci",
          "type": "tree",
          "content": null
        },
        {
          "name": ".devcontainer.json",
          "type": "blob",
          "size": 0.95,
          "content": "// For format details, see https://aka.ms/vscode-remote/devcontainer.json or the definition README at\n// https://github.com/microsoft/vscode-dev-containers/tree/master/containers/python-3-miniconda\n{\n\t\"name\": \"pandas\",\n\t\"context\": \".\",\n\t\"dockerFile\": \"Dockerfile\",\n\n\t// Use 'settings' to set *default* container specific settings.json values on container create.\n\t// You can edit these settings after create using File > Preferences > Settings > Remote.\n\t\"settings\": {\n\t\t\"python.pythonPath\": \"/usr/local/bin/python\",\n\t\t\"python.formatting.provider\": \"black\",\n\t\t\"python.linting.enabled\": true,\n\t\t\"python.linting.flake8Enabled\": true,\n\t\t\"python.linting.pylintEnabled\": false,\n\t\t\"python.linting.mypyEnabled\": true,\n\t\t\"python.testing.pytestEnabled\": true,\n\t\t\"python.testing.pytestArgs\": [\n\t\t\t\"pandas\"\n\t\t]\n\t},\n\n\t// Add the IDs of extensions you want installed when the container is created in the array below.\n\t\"extensions\": [\n\t\t\"ms-python.python\",\n\t\t\"ms-vscode.cpptools\"\n\t]\n}\n"
        },
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 1.85,
          "content": "* text=auto\n# enforce text on certain files\n*.py text\n*.pyx text\n*.pyd text\n*.c text\n*.h text\n*.html text\n*.csv text\n*.json text\n*.pickle binary\n*.h5 binary\n*.dta binary\n*.xls binary\n*.xlsx binary\npandas/_version.py export-subst\n\n\n*.bz2 export-ignore\n*.csv export-ignore\n*.data export-ignore\n*.dta export-ignore\n*.feather export-ignore\n*.tar export-ignore\n*.gz export-ignore\n*.h5 export-ignore\n*.html export-ignore\n*.json export-ignore\n*.jsonl export-ignore\n*.kml export-ignore\n*.msgpack export-ignore\n*.pdf export-ignore\n*.parquet export-ignore\n*.pickle export-ignore\n*.pkl export-ignore\n*.png export-ignore\n*.pptx export-ignore\n*.ods export-ignore\n*.odt export-ignore\n*.orc export-ignore\n*.sas7bdat export-ignore\n*.sav export-ignore\n*.so export-ignore\n*.txt export-ignore\n*.xls export-ignore\n*.xlsb export-ignore\n*.xlsm export-ignore\n*.xlsx export-ignore\n*.xpt export-ignore\n*.cpt export-ignore\n*.xml export-ignore\n*.xsl export-ignore\n*.xz export-ignore\n*.zip export-ignore\n*.zst export-ignore\n*~ export-ignore\n.DS_Store export-ignore\n.git* export-ignore\n\n*.py[ocd] export-ignore\n*.pxi export-ignore\n\n# Ignoring stuff from the top level\n.circleci export-ignore\n.github export-ignore\nasv_bench export-ignore\nci export-ignore\ndoc export-ignore\ngitpod export-ignore\nMANIFEST.in export-ignore\nscripts/** export-ignore\ntypings export-ignore\nweb export-ignore\nCITATION.cff export-ignore\ncodecov.yml export-ignore\nDockerfile export-ignore\nenvironment.yml export-ignore\nsetup.py export-ignore\n\n\n# GH 39321\n# csv_dir_path fixture checks the existence of the directory\n# exclude the whole directory to avoid running related tests in sdist\npandas/tests/io/parser/data export-ignore\n\n# Include cibw script in sdist since it's needed for building wheels\nscripts/cibw_before_build.sh -export-ignore\nscripts/cibw_before_build_windows.sh -export-ignore\nscripts/cibw_before_test_windows.sh -export-ignore\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.87,
          "content": "#########################################\n# Editor temporary/working/backup files #\n.#*\n*\\#*\\#\n[#]*#\n*~\n*$\n*.bak\n*flymake*\n*.iml\n*.kdev4\n*.log\n*.swp\n*.pdb\n*.zip\n.project\n.pydevproject\n.settings\n.idea\n.vagrant\n.noseids\n.ipynb_checkpoints\n.tags\n.cache/\n.vscode/\n\n# Compiled source #\n###################\n*.a\n*.com\n*.class\n*.dll\n*.exe\n*.pxi\n*.o\n*.py[ocd]\n*.so\n.build_cache_dir\n.mesonpy-native-file.ini\nMANIFEST\ncompile_commands.json\ndebug\n.debug\n\n# Python files #\n################\n# setup.py working directory\nbuild\n# sphinx build directory\ndoc/_build\n# setup.py dist directory\ndist\n# Egg metadata\n*.egg-info\n.eggs\n.pypirc\n# type checkers\npandas/py.typed\n\n# pyenv\n.python-version\n\n# tox testing tool\n.tox\n# rope\n.ropeproject\n# wheel files\n*.whl\n**/wheelhouse/*\npip-wheel-metadata\n# coverage\n.coverage\ncoverage.xml\ncoverage_html_report\n.mypy_cache\n*.pytest_cache\n.ruff_cache\n# hypothesis test database\n.hypothesis/\n__pycache__\n# pytest-monkeytype\nmonkeytype.sqlite3\n# meson editable install folder\n.mesonpy\n\n\n# OS generated files #\n######################\n.directory\n.gdb_history\n.DS_Store\nehthumbs.db\nIcon?\nThumbs.db\n\n# Data files #\n##############\n*.dta\n*.xpt\n*.h5\npandas/io/*.dat\npandas/io/*.json\nscikits\n\n# Generated Sources #\n#####################\n!skts.c\n*.c\n*.cpp\n!pandas/_libs/src/**/*.c\n!pandas/_libs/src/**/*.h\n!pandas/_libs/include/**/*.h\n\n# Unit / Performance Testing #\n##############################\nasv_bench/env/\nasv_bench/html/\nasv_bench/results/\nasv_bench/pandas/\ntest-data.xml\n\n# Documentation generated files #\n#################################\ndoc/source/generated\ndoc/source/user_guide/styled.xlsx\ndoc/source/reference/api\ndoc/source/_static/*.html\ndoc/source/vbench\ndoc/source/vbench.rst\ndoc/source/index.rst\ndoc/build/html/index.html\n# Windows specific leftover:\ndoc/tmp.sv\nenv/\ndoc/source/savefig/\n\n# Interactive terminal generated files #\n########################################\n.jupyterlite.doit.db\n"
        },
        {
          "name": ".gitpod.yml",
          "type": "blob",
          "size": 2.23,
          "content": "# Building pandas on init\n# Might delegate this later to prebuild with Q2 improvements on gitpod\n# https://www.gitpod.io/docs/config-start-tasks/#configuring-the-terminal\n# -------------------------------------------------------------------------\n\n# images for gitpod pandas are in https://hub.docker.com/r/pandas/pandas-gitpod/tags\n# we're using the Dockerfile in the base of the repo\nimage:\n  file: Dockerfile\ntasks:\n  - name: Prepare development environment\n    init: |\n      mkdir -p .vscode\n      cp gitpod/settings.json .vscode/settings.json\n      git fetch --tags\n      python -m pip install -ve . --no-build-isolation -Ceditable-verbose=true\n      pre-commit install --install-hooks\n    command: |\n      python -m pip install -ve . --no-build-isolation -Ceditable-verbose=true\n      echo \"✨ Pre-build complete! You can close this terminal ✨ \"\n\n# --------------------------------------------------------\n# exposing ports for liveserve\nports:\n  - port: 5500\n    onOpen: notify\n\n# --------------------------------------------------------\n# some useful extensions to have\nvscode:\n  extensions:\n    - ms-python.python\n    - yzhang.markdown-all-in-one\n    - eamodio.gitlens\n    - lextudio.restructuredtext\n    - ritwickdey.liveserver\n    # add or remove what you think is generally useful to most contributors\n    # avoid adding too many. they each open a pop-up window\n\n# --------------------------------------------------------\n# Using prebuilds for the container\n# With this configuration the prebuild will happen on push to main\ngithub:\n  prebuilds:\n    # enable for main/default branch\n    main: true\n    # enable for other branches (defaults to false)\n    branches: false\n    # enable for pull requests coming from this repo (defaults to true)\n    pullRequests: false\n    # enable for pull requests coming from forks (defaults to false)\n    pullRequestsFromForks: false\n    # add a check to pull requests (defaults to true)\n    addCheck: false\n    # add a \"Review in Gitpod\" button as a comment to pull requests (defaults to false)\n    addComment: false\n    # add a \"Review in Gitpod\" button to the pull request's description (defaults to false)\n    addBadge: false\n    # add a label once the prebuild is ready to pull requests (defaults to false)\n    addLabel: false\n"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 10.82,
          "content": "minimum_pre_commit_version: 2.15.0\nexclude: ^LICENSES/|\\.(html|csv|svg)$\n# reserve \"manual\" for relatively slow hooks which we still want to run in CI\ndefault_stages: [\n    pre-commit,\n    pre-merge-commit,\n    pre-push,\n    prepare-commit-msg,\n    commit-msg,\n    post-checkout,\n    post-commit,\n    post-merge,\n    post-rewrite\n]\nci:\n    autofix_prs: false\n    autoupdate_schedule: monthly\n    # manual stage hooks\n    skip: [pyright, mypy]\nrepos:\n-   repo: https://github.com/astral-sh/ruff-pre-commit\n    rev: v0.8.6\n    hooks:\n    -   id: ruff\n        args: [--exit-non-zero-on-fix]\n        exclude: ^pandas/tests/frame/test_query_eval.py\n    -   id: ruff\n        # TODO: remove autofixe-only rules when they are checked by ruff\n        name: ruff-selected-autofixes\n        alias: ruff-selected-autofixes\n        files: ^pandas\n        exclude: ^pandas/tests\n        args: [--select, \"ANN001,ANN2\", --fix-only, --exit-non-zero-on-fix]\n    -   id: ruff-format\n        exclude: ^scripts|^pandas/tests/frame/test_query_eval.py\n-   repo: https://github.com/jendrikseipp/vulture\n    rev: 'v2.14'\n    hooks:\n      - id: vulture\n        entry: python scripts/run_vulture.py\n        pass_filenames: true\n        require_serial: false\n-   repo: https://github.com/codespell-project/codespell\n    rev: v2.3.0\n    hooks:\n    -   id: codespell\n        types_or: [python, rst, markdown, cython, c]\n        additional_dependencies: [tomli]\n-   repo: https://github.com/MarcoGorelli/cython-lint\n    rev: v0.16.6\n    hooks:\n    -   id: cython-lint\n    -   id: double-quote-cython-strings\n-   repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v5.0.0\n    hooks:\n    -   id: check-case-conflict\n    -   id: check-toml\n    -   id: check-xml\n    -   id: check-yaml\n        exclude: ^ci/meta.yaml$\n    -   id: end-of-file-fixer\n        exclude: \\.txt$\n    -   id: mixed-line-ending\n        args: [--fix=auto]\n        exclude: ^pandas/tests/io/parser/data/utf16_ex.txt$\n    -   id: fix-byte-order-marker\n    -   id: fix-encoding-pragma\n        args: [--remove]\n    -   id: trailing-whitespace\n        args: [--markdown-linebreak-ext=md]\n-   repo: https://github.com/PyCQA/isort\n    rev: 5.13.2\n    hooks:\n    -   id: isort\n-   repo: https://github.com/asottile/pyupgrade\n    rev: v3.19.1\n    hooks:\n    -   id: pyupgrade\n        args: [--py310-plus]\n-   repo: https://github.com/pre-commit/pygrep-hooks\n    rev: v1.10.0\n    hooks:\n      - id: rst-backticks\n      - id: rst-directive-colons\n        types: [text]  # overwrite types: [rst]\n        types_or: [python, rst]\n      - id: rst-inline-touching-normal\n        exclude: ^pandas/tests/frame/test_query_eval.py\n        types: [text]  # overwrite types: [rst]\n        types_or: [python, rst]\n-   repo: https://github.com/sphinx-contrib/sphinx-lint\n    rev: v1.0.0\n    hooks:\n    - id: sphinx-lint\n      args: [\"--enable\", \"all\", \"--disable\", \"line-too-long\"]\n-   repo: https://github.com/pre-commit/mirrors-clang-format\n    rev: v19.1.6\n    hooks:\n    - id: clang-format\n      files: ^pandas/_libs/src|^pandas/_libs/include\n      args: [-i]\n      types_or: [c, c++]\n-   repo: https://github.com/trim21/pre-commit-mirror-meson\n    rev: v1.6.1\n    hooks:\n    - id: meson-fmt\n      args: ['--inplace']\n-   repo: local\n    hooks:\n    -   id: pyright\n        # note: assumes python env is setup and activated\n        name: pyright\n        entry: pyright\n        language: node\n        pass_filenames: false\n        types: [python]\n        stages: [manual]\n        additional_dependencies: &pyright_dependencies\n        - pyright@1.1.383\n    -   id: pyright\n        # note: assumes python env is setup and activated\n        name: pyright reportGeneralTypeIssues\n        entry: pyright -p pyright_reportGeneralTypeIssues.json --level warning\n        language: node\n        pass_filenames: false\n        types: [python]\n        stages: [manual]\n        additional_dependencies: *pyright_dependencies\n    -   id: mypy\n        # note: assumes python env is setup and activated\n        name: mypy\n        entry: mypy\n        language: system\n        pass_filenames: false\n        types: [python]\n        stages: [manual]\n    -   id: mypy\n        # note: assumes python env is setup and activated\n        # note: requires pandas dev to be installed\n        name: mypy (stubtest)\n        entry: python\n        language: system\n        pass_filenames: false\n        types: [pyi]\n        args: [scripts/run_stubtest.py]\n        stages: [manual]\n    -   id: inconsistent-namespace-usage\n        name: 'Check for inconsistent use of pandas namespace'\n        entry: python scripts/check_for_inconsistent_pandas_namespace.py\n        exclude: ^pandas/core/interchange/\n        language: python\n        types: [python]\n    -   id: unwanted-patterns\n        name: Unwanted patterns\n        language: pygrep\n        entry: |\n            (?x)\n            # outdated annotation syntax\n            \\#\\ type:\\ (?!ignore)\n\n            # foo._class__ instead of type(foo)\n            |\\.__class__\n\n            # Numpy\n            |from\\ numpy\\ import\\ random\n            |from\\ numpy\\.random\\ import\n\n            # Incorrect code-block / IPython directives\n            |\\.\\.\\ code-block\\ ::\n            |\\.\\.\\ ipython\\ ::\n            # directive should not have a space before ::\n            |\\.\\.\\ \\w+\\ ::\n\n            # Check for deprecated messages without sphinx directive\n            |(DEPRECATED|DEPRECATE|Deprecated)(:|,|\\.)\n\n            # builtin filter function\n            |(?<!def)[\\(\\s]filter\\(\n        types_or: [python, cython, rst]\n        exclude: ^doc/source/development/code_style\\.rst  # contains examples of patterns to avoid\n    -   id: incorrect-backticks\n        name: Check for backticks incorrectly rendering because of missing spaces\n        language: pygrep\n        entry: '[a-zA-Z0-9]\\`\\`?[a-zA-Z0-9]'\n        types: [rst]\n        files: ^doc/source/\n    -   id: seed-check-asv\n        name: Check for unnecessary random seeds in asv benchmarks\n        language: pygrep\n        entry: 'np\\.random\\.seed'\n        files: ^asv_bench/benchmarks\n        exclude: ^asv_bench/benchmarks/pandas_vb_common\\.py\n    -   id: unwanted-patterns-in-tests\n        name: Unwanted patterns in tests\n        language: pygrep\n        entry: |\n            (?x)\n            # imports from pandas._testing instead of `import pandas._testing as tm`\n            from\\ pandas\\._testing\\ import\n            |from\\ pandas\\ import\\ _testing\\ as\\ tm\n\n            # pandas.testing instead of tm\n            |pd\\.testing\\.\n\n            # pd.api.types instead of from pandas.api.types import ...\n            |(pd|pandas)\\.api\\.types\\.\n\n            # np.array_equal\n            |(numpy|np)\\.array_equal\n\n            # pytest raises without context\n            |\\s\\ pytest.raises\n\n            # Unseeded numpy default_rng\n            |default_rng\\(\\)\n        files: ^pandas/tests/\n        types_or: [python, cython, rst]\n    -   id: unwanted-patterns-in-cython\n        name: Unwanted patterns in Cython code\n        language: pygrep\n        entry: |\n            (?x)\n            # `<type>obj` as opposed to `<type> obj`\n            [a-zA-Z0-9*]>[ ]\n        types: [cython]\n    -   id: pip-to-conda\n        name: Generate pip dependency from conda\n        language: python\n        entry: python scripts/generate_pip_deps_from_conda.py\n        files: ^(environment.yml|requirements-dev.txt)$\n        pass_filenames: false\n        additional_dependencies: [tomli, pyyaml]\n    -   id: title-capitalization\n        name: Validate correct capitalization among titles in documentation\n        entry: python scripts/validate_rst_title_capitalization.py\n        language: python\n        types: [rst]\n        files: ^doc/source/(development|reference)/\n    -   id: unwanted-patterns-private-function-across-module\n        name: Check for use of private functions across modules\n        language: python\n        entry: python scripts/validate_unwanted_patterns.py --validation-type=\"private_function_across_module\"\n        types: [python]\n        exclude: ^(asv_bench|pandas/tests|doc)/\n    -   id: unwanted-patterns-private-import-across-module\n        name: Check for import of private attributes across modules\n        language: python\n        entry: python scripts/validate_unwanted_patterns.py --validation-type=\"private_import_across_module\"\n        types: [python]\n        exclude: |\n            (?x)\n            ^(asv_bench|pandas/tests|doc)/\n            |scripts/validate_min_versions_in_sync\\.py$\n    -   id: unwanted-patterns-strings-with-misplaced-whitespace\n        name: Check for strings with misplaced spaces\n        language: python\n        entry: python scripts/validate_unwanted_patterns.py --validation-type=\"strings_with_wrong_placed_whitespace\"\n        types_or: [python, cython]\n    -   id: unwanted-patterns-nodefault-used-not-only-for-typing\n        name: Check that `pandas._libs.lib.NoDefault` is used only for typing\n        language: python\n        entry: python scripts/validate_unwanted_patterns.py --validation-type=\"nodefault_used_not_only_for_typing\"\n        types: [python]\n    -   id: no-return-exception\n        name: Use raise instead of return for exceptions\n        language: pygrep\n        entry: 'return [A-Za-z]+(Error|Exit|Interrupt|Exception|Iteration)'\n        files: ^pandas/\n        types: [python]\n        exclude: ^pandas/tests/\n    -   id: pandas-errors-documented\n        name: Ensure pandas errors are documented in doc/source/reference/testing.rst\n        entry: python scripts/pandas_errors_documented.py\n        language: python\n        files: ^pandas/errors/__init__.py$\n    -   id: pg8000-not-installed-CI\n        name: Check for pg8000 not installed on CI for test_pg8000_sqlalchemy_passthrough_error\n        language: pygrep\n        entry: 'pg8000'\n        files: ^ci/deps\n        types: [yaml]\n    -   id: validate-min-versions-in-sync\n        name: Check minimum version of dependencies are aligned\n        entry: python -m scripts.validate_min_versions_in_sync\n        language: python\n        files: ^(ci/deps/actions-.*-minimum_versions\\.yaml|pandas/compat/_optional\\.py)$\n        additional_dependencies: [tomli, pyyaml]\n        pass_filenames: false\n    -   id: validate-errors-locations\n        name: Validate errors locations\n        description: Validate errors are in appropriate locations.\n        entry: python scripts/validate_exception_location.py\n        language: python\n        files: ^pandas/\n        exclude: ^(pandas/_libs/|pandas/tests/|pandas/errors/__init__.py$|pandas/_version.py)\n        types: [python]\n    -   id: check-test-naming\n        name: check that test names start with 'test'\n        entry: python -m scripts.check_test_naming\n        types: [python]\n        files: ^pandas/tests\n        language: python\n    -   id: sort-whatsnew-items\n        name: sort whatsnew entries alphabetically\n        entry: python -m scripts.sort_whatsnew_note\n        types: [rst]\n        language: python\n        files: ^doc/source/whatsnew/v\n        exclude: ^doc/source/whatsnew/v(0|1|2\\.0\\.0)\n"
        },
        {
          "name": "AUTHORS.md",
          "type": "blob",
          "size": 2.23,
          "content": "About the Copyright Holders\n===========================\n\n*   Copyright (c) 2008-2011 AQR Capital Management, LLC\n\n    AQR Capital Management began pandas development in 2008. Development was\n    led by Wes McKinney. AQR released the source under this license in 2009.\n*   Copyright (c) 2011-2012, Lambda Foundry, Inc.\n\n    Wes is now an employee of Lambda Foundry, and remains the pandas project\n    lead.\n*   Copyright (c) 2011-2012, PyData Development Team\n\n    The PyData Development Team is the collection of developers of the PyData\n    project. This includes all of the PyData sub-projects, including pandas. The\n    core team that coordinates development on GitHub can be found here:\n    https://github.com/pydata.\n\nFull credits for pandas contributors can be found in the documentation.\n\nOur Copyright Policy\n====================\n\nPyData uses a shared copyright model. Each contributor maintains copyright\nover their contributions to PyData. However, it is important to note that\nthese contributions are typically only changes to the repositories. Thus,\nthe PyData source code, in its entirety, is not the copyright of any single\nperson or institution. Instead, it is the collective copyright of the\nentire PyData Development Team. If individual contributors want to maintain\na record of what changes/contributions they have specific copyright on,\nthey should indicate their copyright in the commit message of the change\nwhen they commit the change to one of the PyData repositories.\n\nWith this in mind, the following banner should be used in any source code\nfile to indicate the copyright and license terms:\n\n```\n#-----------------------------------------------------------------------------\n# Copyright (c) 2012, PyData Development Team\n# All rights reserved.\n#\n# Distributed under the terms of the BSD Simplified License.\n#\n# The full license is in the LICENSE file, distributed with this software.\n#-----------------------------------------------------------------------------\n```\n\nOther licenses can be found in the LICENSES directory.\n\nLicense\n=======\n\npandas is distributed under a 3-clause (\"Simplified\" or \"New\") BSD\nlicense. Parts of NumPy, SciPy, numpydoc, bottleneck, which all have\nBSD-compatible licenses, are included. Their licenses follow the pandas\nlicense.\n"
        },
        {
          "name": "CITATION.cff",
          "type": "blob",
          "size": 1.5,
          "content": "cff-version: 1.2.0\ntitle: 'pandas-dev/pandas: Pandas'\nmessage: 'If you use this software, please cite it as below.'\nauthors:\n  - name: \"The pandas development team\"\n    website: \"https://pandas.pydata.org/about/team.html\"\nabstract: \"Pandas is a powerful data structures for data analysis, time series, and statistics.\"\ndoi: 10.5281/zenodo.3509134\nlicense: BSD-3-Clause\nlicense-url: \"https://github.com/pandas-dev/pandas/blob/main/LICENSE\"\nrepository-code: \"https://github.com/pandas-dev/pandas\"\nkeywords:\n  - python\n  - data science\n  - flexible\n  - pandas\n  - alignment\n  - data analysis\ntype: software\nurl: \"https://pandas.pydata.org/\"\nreferences:\n  - type: article\n    authors:\n      - given-names: Wes\n        family-names: McKinney\n        affiliation: AQR Capital Management, LLC\n        email: wesmckinn@gmail.com\n    title: Data Structures for Statistical Computing in Python\n    doi: 10.25080/Majora-92bf1922-00a\n    license: CC-BY-3.0\n    start: 56\n    end: 61\n    year: 2010\n    collection-title: Proceedings of the 9th Python in Science Conference\n    collection-doi: 10.25080/Majora-92bf1922-012\n    collection-type: proceedings\n    editors:\n      - given-names: Stéfan\n        name-particle: van der\n        family-names: Walt\n      - given-names: Jarrod\n        family-names: Millman\n    conference:\n      name: 9th Python in Science Conference (SciPy 2010)\n      city: Austin, TX\n      country: US\n      date-start: \"2010-06-28\"\n      date-end: \"2010-07-03\"\n    keywords:\n      - data structure\n      - statistics\n      - R\n"
        },
        {
          "name": "Dockerfile",
          "type": "blob",
          "size": 0.48,
          "content": "FROM python:3.10.8\nWORKDIR /home/pandas\n\nRUN apt-get update && apt-get -y upgrade\nRUN apt-get install -y build-essential bash-completion\n\n# hdf5 needed for pytables installation\n# libgles2-mesa needed for pytest-qt\nRUN apt-get install -y libhdf5-dev libgles2-mesa-dev\n\nRUN python -m pip install --upgrade pip\nCOPY requirements-dev.txt /tmp\nRUN python -m pip install -r /tmp/requirements-dev.txt\nRUN git config --global --add safe.directory /home/pandas\n\nENV SHELL \"/bin/bash\"\nCMD [\"/bin/bash\"]\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.6,
          "content": "BSD 3-Clause License\n\nCopyright (c) 2008-2011, AQR Capital Management, LLC, Lambda Foundry, Inc. and PyData Development Team\nAll rights reserved.\n\nCopyright (c) 2011-2025, Open source contributors.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n* Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n\n* Redistributions in binary form must reproduce the above copyright notice,\n  this list of conditions and the following disclaimer in the documentation\n  and/or other materials provided with the distribution.\n\n* Neither the name of the copyright holder nor the names of its\n  contributors may be used to endorse or promote products derived from\n  this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n"
        },
        {
          "name": "LICENSES",
          "type": "tree",
          "content": null
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 1.5,
          "content": "graft doc\nprune doc/build\n\ngraft LICENSES\n\ngraft pandas\n\nglobal-exclude *.bz2\nglobal-exclude *.csv\nglobal-exclude *.data\nglobal-exclude *.dta\nglobal-exclude *.feather\nglobal-exclude *.tar\nglobal-exclude *.gz\nglobal-exclude *.h5\nglobal-exclude *.html\nglobal-exclude *.json\nglobal-exclude *.jsonl\nglobal-exclude *.kml\nglobal-exclude *.msgpack\nglobal-exclude *.pdf\nglobal-exclude *.parquet\nglobal-exclude *.pickle\nglobal-exclude *.pkl\nglobal-exclude *.png\nglobal-exclude *.pptx\nglobal-exclude *.ods\nglobal-exclude *.odt\nglobal-exclude *.orc\nglobal-exclude *.sas7bdat\nglobal-exclude *.sav\nglobal-exclude *.so\nglobal-exclude *.txt\nglobal-exclude *.xls\nglobal-exclude *.xlsb\nglobal-exclude *.xlsm\nglobal-exclude *.xlsx\nglobal-exclude *.xpt\nglobal-exclude *.cpt\nglobal-exclude *.xml\nglobal-exclude *.xsl\nglobal-exclude *.xz\nglobal-exclude *.zip\nglobal-exclude *.zst\nglobal-exclude *~\nglobal-exclude .DS_Store\nglobal-exclude .git*\nglobal-exclude \\#*\n\nglobal-exclude *.c\nglobal-exclude *.cpp\nglobal-exclude *.h\n\nglobal-exclude *.py[ocd]\nglobal-exclude *.pxi\n\n# GH 39321\n# csv_dir_path fixture checks the existence of the directory\n# exclude the whole directory to avoid running related tests in sdist\nprune pandas/tests/io/parser/data\n\n# Selectively re-add *.cxx files that were excluded above\ngraft pandas/_libs/src\ngraft pandas/_libs/include\n\n# Include cibw script in sdist since it's needed for building wheels\ninclude scripts/cibw_before_build.sh\ninclude scripts/cibw_before_build_windows.sh\ninclude scripts/cibw_before_test_windows.sh\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 11.39,
          "content": "<picture align=\"center\">\n  <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://pandas.pydata.org/static/img/pandas_white.svg\">\n  <img alt=\"Pandas Logo\" src=\"https://pandas.pydata.org/static/img/pandas.svg\">\n</picture>\n\n-----------------\n\n# pandas: powerful Python data analysis toolkit\n\n| | |\n| --- | --- |\n| Testing | [![CI - Test](https://github.com/pandas-dev/pandas/actions/workflows/unit-tests.yml/badge.svg)](https://github.com/pandas-dev/pandas/actions/workflows/unit-tests.yml) [![Coverage](https://codecov.io/github/pandas-dev/pandas/coverage.svg?branch=main)](https://codecov.io/gh/pandas-dev/pandas) |\n| Package | [![PyPI Latest Release](https://img.shields.io/pypi/v/pandas.svg)](https://pypi.org/project/pandas/) [![PyPI Downloads](https://img.shields.io/pypi/dm/pandas.svg?label=PyPI%20downloads)](https://pypi.org/project/pandas/) [![Conda Latest Release](https://anaconda.org/conda-forge/pandas/badges/version.svg)](https://anaconda.org/conda-forge/pandas) [![Conda Downloads](https://img.shields.io/conda/dn/conda-forge/pandas.svg?label=Conda%20downloads)](https://anaconda.org/conda-forge/pandas) |\n| Meta | [![Powered by NumFOCUS](https://img.shields.io/badge/powered%20by-NumFOCUS-orange.svg?style=flat&colorA=E1523D&colorB=007D8A)](https://numfocus.org) [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3509134.svg)](https://doi.org/10.5281/zenodo.3509134) [![License - BSD 3-Clause](https://img.shields.io/pypi/l/pandas.svg)](https://github.com/pandas-dev/pandas/blob/main/LICENSE) [![Slack](https://img.shields.io/badge/join_Slack-information-brightgreen.svg?logo=slack)](https://pandas.pydata.org/docs/dev/development/community.html?highlight=slack#community-slack) |\n\n\n## What is it?\n\n**pandas** is a Python package that provides fast, flexible, and expressive data\nstructures designed to make working with \"relational\" or \"labeled\" data both\neasy and intuitive. It aims to be the fundamental high-level building block for\ndoing practical, **real world** data analysis in Python. Additionally, it has\nthe broader goal of becoming **the most powerful and flexible open source data\nanalysis / manipulation tool available in any language**. It is already well on\nits way towards this goal.\n\n## Table of Contents\n\n- [Main Features](#main-features)\n- [Where to get it](#where-to-get-it)\n- [Dependencies](#dependencies)\n- [Installation from sources](#installation-from-sources)\n- [License](#license)\n- [Documentation](#documentation)\n- [Background](#background)\n- [Getting Help](#getting-help)\n- [Discussion and Development](#discussion-and-development)\n- [Contributing to pandas](#contributing-to-pandas)\n\n## Main Features\nHere are just a few of the things that pandas does well:\n\n  - Easy handling of [**missing data**][missing-data] (represented as\n    `NaN`, `NA`, or `NaT`) in floating point as well as non-floating point data\n  - Size mutability: columns can be [**inserted and\n    deleted**][insertion-deletion] from DataFrame and higher dimensional\n    objects\n  - Automatic and explicit [**data alignment**][alignment]: objects can\n    be explicitly aligned to a set of labels, or the user can simply\n    ignore the labels and let `Series`, `DataFrame`, etc. automatically\n    align the data for you in computations\n  - Powerful, flexible [**group by**][groupby] functionality to perform\n    split-apply-combine operations on data sets, for both aggregating\n    and transforming data\n  - Make it [**easy to convert**][conversion] ragged,\n    differently-indexed data in other Python and NumPy data structures\n    into DataFrame objects\n  - Intelligent label-based [**slicing**][slicing], [**fancy\n    indexing**][fancy-indexing], and [**subsetting**][subsetting] of\n    large data sets\n  - Intuitive [**merging**][merging] and [**joining**][joining] data\n    sets\n  - Flexible [**reshaping**][reshape] and [**pivoting**][pivot-table] of\n    data sets\n  - [**Hierarchical**][mi] labeling of axes (possible to have multiple\n    labels per tick)\n  - Robust IO tools for loading data from [**flat files**][flat-files]\n    (CSV and delimited), [**Excel files**][excel], [**databases**][db],\n    and saving/loading data from the ultrafast [**HDF5 format**][hdfstore]\n  - [**Time series**][timeseries]-specific functionality: date range\n    generation and frequency conversion, moving window statistics,\n    date shifting and lagging\n\n\n   [missing-data]: https://pandas.pydata.org/pandas-docs/stable/user_guide/missing_data.html\n   [insertion-deletion]: https://pandas.pydata.org/pandas-docs/stable/user_guide/dsintro.html#column-selection-addition-deletion\n   [alignment]: https://pandas.pydata.org/pandas-docs/stable/user_guide/dsintro.html?highlight=alignment#intro-to-data-structures\n   [groupby]: https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html#group-by-split-apply-combine\n   [conversion]: https://pandas.pydata.org/pandas-docs/stable/user_guide/dsintro.html#dataframe\n   [slicing]: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#slicing-ranges\n   [fancy-indexing]: https://pandas.pydata.org/pandas-docs/stable/user_guide/advanced.html#advanced\n   [subsetting]: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#boolean-indexing\n   [merging]: https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html#database-style-dataframe-or-named-series-joining-merging\n   [joining]: https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html#joining-on-index\n   [reshape]: https://pandas.pydata.org/pandas-docs/stable/user_guide/reshaping.html\n   [pivot-table]: https://pandas.pydata.org/pandas-docs/stable/user_guide/reshaping.html\n   [mi]: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#hierarchical-indexing-multiindex\n   [flat-files]: https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#csv-text-files\n   [excel]: https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#excel-files\n   [db]: https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#sql-queries\n   [hdfstore]: https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#hdf5-pytables\n   [timeseries]: https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#time-series-date-functionality\n\n## Where to get it\nThe source code is currently hosted on GitHub at:\nhttps://github.com/pandas-dev/pandas\n\nBinary installers for the latest released version are available at the [Python\nPackage Index (PyPI)](https://pypi.org/project/pandas) and on [Conda](https://anaconda.org/conda-forge/pandas).\n\n```sh\n# conda\nconda install -c conda-forge pandas\n```\n\n```sh\n# or PyPI\npip install pandas\n```\n\nThe list of changes to pandas between each release can be found\n[here](https://pandas.pydata.org/pandas-docs/stable/whatsnew/index.html). For full\ndetails, see the commit logs at https://github.com/pandas-dev/pandas.\n\n## Dependencies\n- [NumPy - Adds support for large, multi-dimensional arrays, matrices and high-level mathematical functions to operate on these arrays](https://www.numpy.org)\n- [python-dateutil - Provides powerful extensions to the standard datetime module](https://dateutil.readthedocs.io/en/stable/index.html)\n- [pytz - Brings the Olson tz database into Python which allows accurate and cross platform timezone calculations](https://github.com/stub42/pytz)\n\nSee the [full installation instructions](https://pandas.pydata.org/pandas-docs/stable/install.html#dependencies) for minimum supported versions of required, recommended and optional dependencies.\n\n## Installation from sources\nTo install pandas from source you need [Cython](https://cython.org/) in addition to the normal\ndependencies above. Cython can be installed from PyPI:\n\n```sh\npip install cython\n```\n\nIn the `pandas` directory (same one where you found this file after\ncloning the git repo), execute:\n\n```sh\npip install .\n```\n\nor for installing in [development mode](https://pip.pypa.io/en/latest/cli/pip_install/#install-editable):\n\n\n```sh\npython -m pip install -ve . --no-build-isolation -Ceditable-verbose=true\n```\n\nSee the full instructions for [installing from source](https://pandas.pydata.org/docs/dev/development/contributing_environment.html).\n\n## License\n[BSD 3](LICENSE)\n\n## Documentation\nThe official documentation is hosted on [PyData.org](https://pandas.pydata.org/pandas-docs/stable/).\n\n## Background\nWork on ``pandas`` started at [AQR](https://www.aqr.com/) (a quantitative hedge fund) in 2008 and\nhas been under active development since then.\n\n## Getting Help\n\nFor usage questions, the best place to go to is [StackOverflow](https://stackoverflow.com/questions/tagged/pandas).\nFurther, general questions and discussions can also take place on the [pydata mailing list](https://groups.google.com/forum/?fromgroups#!forum/pydata).\n\n## Discussion and Development\nMost development discussions take place on GitHub in this repo, via the [GitHub issue tracker](https://github.com/pandas-dev/pandas/issues).\n\nFurther, the [pandas-dev mailing list](https://mail.python.org/mailman/listinfo/pandas-dev) can also be used for specialized discussions or design issues, and a [Slack channel](https://pandas.pydata.org/docs/dev/development/community.html?highlight=slack#community-slack) is available for quick development related questions.\n\nThere are also frequent [community meetings](https://pandas.pydata.org/docs/dev/development/community.html#community-meeting) for project maintainers open to the community as well as monthly [new contributor meetings](https://pandas.pydata.org/docs/dev/development/community.html#new-contributor-meeting) to help support new contributors.\n\nAdditional information on the communication channels can be found on the [contributor community](https://pandas.pydata.org/docs/development/community.html) page.\n\n## Contributing to pandas\n\n[![Open Source Helpers](https://www.codetriage.com/pandas-dev/pandas/badges/users.svg)](https://www.codetriage.com/pandas-dev/pandas)\n\nAll contributions, bug reports, bug fixes, documentation improvements, enhancements, and ideas are welcome.\n\nA detailed overview on how to contribute can be found in the **[contributing guide](https://pandas.pydata.org/docs/dev/development/contributing.html)**.\n\nIf you are simply looking to start working with the pandas codebase, navigate to the [GitHub \"issues\" tab](https://github.com/pandas-dev/pandas/issues) and start looking through interesting issues. There are a number of issues listed under [Docs](https://github.com/pandas-dev/pandas/issues?labels=Docs&sort=updated&state=open) and [good first issue](https://github.com/pandas-dev/pandas/issues?labels=good+first+issue&sort=updated&state=open) where you could start out.\n\nYou can also triage issues which may include reproducing bug reports, or asking for vital information such as version numbers or reproduction instructions. If you would like to start triaging issues, one easy way to get started is to [subscribe to pandas on CodeTriage](https://www.codetriage.com/pandas-dev/pandas).\n\nOr maybe through using pandas you have an idea of your own or are looking for something in the documentation and thinking ‘this can be improved’...you can do something about it!\n\nFeel free to ask questions on the [mailing list](https://groups.google.com/forum/?fromgroups#!forum/pydata) or on [Slack](https://pandas.pydata.org/docs/dev/development/community.html?highlight=slack#community-slack).\n\nAs contributors and maintainers to this project, you are expected to abide by pandas' code of conduct. More information can be found at: [Contributor Code of Conduct](https://github.com/pandas-dev/.github/blob/master/CODE_OF_CONDUCT.md)\n\n<hr>\n\n[Go to Top](#table-of-contents)\n"
        },
        {
          "name": "asv_bench",
          "type": "tree",
          "content": null
        },
        {
          "name": "ci",
          "type": "tree",
          "content": null
        },
        {
          "name": "codecov.yml",
          "type": "blob",
          "size": 0.25,
          "content": "codecov:\n  branch: main\n  notify:\n    after_n_builds: 10\ncomment: false\n\ncoverage:\n  status:\n    project:\n      default:\n        target: '82'\n    patch:\n      default:\n        target: '50'\n        informational: true\n\ngithub_checks:\n    annotations: false\n"
        },
        {
          "name": "doc",
          "type": "tree",
          "content": null
        },
        {
          "name": "environment.yml",
          "type": "blob",
          "size": 2.56,
          "content": "# Local development dependencies including docs building, website upload, ASV benchmark\nname: pandas-dev\nchannels:\n  - conda-forge\ndependencies:\n  - python=3.10\n  - pip\n\n  # build dependencies\n  - versioneer\n  - cython~=3.0.5\n  - meson=1.2.1\n  - meson-python=0.13.1\n\n  # test dependencies\n  - pytest>=7.3.2\n  - pytest-cov\n  - pytest-xdist>=3.4.0\n  - pytest-qt>=4.4.0\n  - pytest-localserver\n  - pyqt>=5.15.9\n  - coverage\n\n  # required dependencies\n  - python-dateutil\n  - numpy<2\n\n  # optional dependencies\n  - beautifulsoup4>=4.11.2\n  - blosc\n  - bottleneck>=1.3.6\n  - fastparquet>=2023.10.0\n  - fsspec>=2022.11.0\n  - html5lib>=1.1\n  - hypothesis>=6.84.0\n  - gcsfs>=2022.11.0\n  - ipython\n  - pickleshare  # Needed for IPython Sphinx directive in the docs GH#60429\n  - jinja2>=3.1.2\n  - lxml>=4.9.2\n  - matplotlib>=3.6.3\n  - numba>=0.56.4\n  - numexpr>=2.8.4\n  - openpyxl>=3.1.0\n  - odfpy>=1.4.1\n  - py\n  - psycopg2>=2.9.6\n  - pyarrow>=10.0.1\n  - pymysql>=1.0.2\n  - pyreadstat>=1.2.0\n  - pytables>=3.8.0\n  - python-calamine>=0.1.7\n  - pytz>=2023.4\n  - pyxlsb>=1.0.10\n  - s3fs>=2022.11.0\n  - scipy>=1.10.0\n  - sqlalchemy>=2.0.0\n  - tabulate>=0.9.0\n  - xarray>=2022.12.0, <=2024.9.0\n  - xlrd>=2.0.1\n  - xlsxwriter>=3.0.5\n  - zstandard>=0.19.0\n\n  # downstream packages\n  - dask-core\n  - seaborn-base\n\n  # local testing dependencies\n  - moto\n  - flask\n\n  # benchmarks\n  - asv>=0.6.1\n\n  ## The compiler packages are meta-packages and install the correct compiler (activation) packages on the respective platforms.\n  - c-compiler\n  - cxx-compiler\n\n  # code checks\n  - flake8=7.1.0  # run in subprocess over docstring examples\n  - mypy=1.13.0  # pre-commit uses locally installed mypy\n  - tokenize-rt  # scripts/check_for_inconsistent_pandas_namespace.py\n  - pre-commit>=4.0.1\n\n  # documentation\n  - gitpython  # obtain contributors from git for whatsnew\n  - gitdb\n  - google-auth\n  - natsort  # DataFrame.sort_values doctest\n  - numpydoc\n  - pydata-sphinx-theme=0.16\n  - pytest-cython  # doctest\n  - sphinx\n  - sphinx-design\n  - sphinx-copybutton\n  - types-python-dateutil\n  - types-PyMySQL\n  - types-pytz\n  - types-PyYAML\n  - types-setuptools\n\n  # documentation (jupyter notebooks)\n  - nbconvert>=7.11.0\n  - nbsphinx\n  - pandoc\n  - ipywidgets\n  - nbformat\n  - notebook>=7.0.6\n  - ipykernel\n\n  # web\n  # - jinja2  # already listed in optional dependencies, but documented here for reference\n  - markdown\n  - feedparser\n  - pyyaml\n  - requests\n  - pygments # Code highlighting\n\n  - pip:\n      - adbc-driver-postgresql>=0.10.0\n      - adbc-driver-sqlite>=0.8.0\n      - typing_extensions; python_version<\"3.11\"\n      - tzdata>=2022.7\n"
        },
        {
          "name": "generate_pxi.py",
          "type": "blob",
          "size": 0.85,
          "content": "import argparse\nimport os\n\nfrom Cython import Tempita\n\n\ndef process_tempita(pxifile, outfile) -> None:\n    with open(pxifile, encoding=\"utf-8\") as f:\n        tmpl = f.read()\n    pyxcontent = Tempita.sub(tmpl)\n\n    with open(outfile, \"w\", encoding=\"utf-8\") as f:\n        f.write(pyxcontent)\n\n\ndef main() -> None:\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"infile\", type=str, help=\"Path to the input file\")\n    parser.add_argument(\"-o\", \"--outdir\", type=str, help=\"Path to the output directory\")\n    args = parser.parse_args()\n\n    if not args.infile.endswith(\".in\"):\n        raise ValueError(f\"Unexpected extension: {args.infile}\")\n\n    outdir_abs = os.path.join(os.getcwd(), args.outdir)\n    outfile = os.path.join(\n        outdir_abs, os.path.splitext(os.path.split(args.infile)[1])[0]\n    )\n\n    process_tempita(args.infile, outfile)\n\n\nmain()\n"
        },
        {
          "name": "generate_version.py",
          "type": "blob",
          "size": 1.67,
          "content": "#!/usr/bin/env python3\n\n# Note: This file has to live next to setup.py or versioneer will not work\nimport argparse\nimport os\nimport sys\n\nimport versioneer\n\nsys.path.insert(0, \"\")\n\n\ndef write_version_info(path) -> None:\n    version = None\n    git_version = None\n\n    try:\n        import _version_meson\n\n        version = _version_meson.__version__\n        git_version = _version_meson.__git_version__\n    except ImportError:\n        version = versioneer.get_version()\n        git_version = versioneer.get_versions()[\"full-revisionid\"]\n    if os.environ.get(\"MESON_DIST_ROOT\"):\n        path = os.path.join(os.environ.get(\"MESON_DIST_ROOT\"), path)\n    with open(path, \"w\", encoding=\"utf-8\") as file:\n        file.write(f'__version__=\"{version}\"\\n')\n        file.write(f'__git_version__=\"{git_version}\"\\n')\n\n\ndef main() -> None:\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"-o\",\n        \"--outfile\",\n        type=str,\n        help=\"Path to write version info to\",\n        required=False,\n    )\n    parser.add_argument(\n        \"--print\",\n        default=False,\n        action=\"store_true\",\n        help=\"Whether to print out the version\",\n        required=False,\n    )\n    args = parser.parse_args()\n\n    if args.outfile:\n        if not args.outfile.endswith(\".py\"):\n            raise ValueError(\n                f\"Output file must be a Python file. \"\n                f\"Got: {args.outfile} as filename instead\"\n            )\n\n        write_version_info(args.outfile)\n\n    if args.print:\n        try:\n            import _version_meson\n\n            version = _version_meson.__version__\n        except ImportError:\n            version = versioneer.get_version()\n        print(version)\n\n\nmain()\n"
        },
        {
          "name": "gitpod",
          "type": "tree",
          "content": null
        },
        {
          "name": "meson.build",
          "type": "blob",
          "size": 1.73,
          "content": "# This file is adapted from https://github.com/scipy/scipy/blob/main/meson.build\nproject(\n    'pandas',\n    'c',\n    'cpp',\n    'cython',\n    version: run_command(['generate_version.py', '--print'], check: true).stdout().strip(),\n    license: 'BSD-3',\n    meson_version: '>=1.2.1',\n    default_options: ['buildtype=release', 'c_std=c11', 'warning_level=2'],\n)\n\nfs = import('fs')\npy = import('python').find_installation(pure: false)\ntempita = files('generate_pxi.py')\nversioneer = files('generate_version.py')\n\n\nadd_project_arguments('-DNPY_NO_DEPRECATED_API=0', language: 'c')\nadd_project_arguments('-DNPY_NO_DEPRECATED_API=0', language: 'cpp')\n\n# Allow supporting older numpys than the version compiled against\n# Set the define to the min supported version of numpy for pandas\n# e.g. right now this is targeting numpy 1.21+\nadd_project_arguments('-DNPY_TARGET_VERSION=NPY_1_21_API_VERSION', language: 'c')\nadd_project_arguments(\n    '-DNPY_TARGET_VERSION=NPY_1_21_API_VERSION',\n    language: 'cpp',\n)\n\n\nif fs.exists('_version_meson.py')\n    py.install_sources('_version_meson.py', subdir: 'pandas')\nelse\n    custom_target(\n        'write_version_file',\n        output: '_version_meson.py',\n        command: [py, versioneer, '-o', '@OUTPUT@'],\n        build_by_default: true,\n        build_always_stale: true,\n        install: true,\n        install_dir: py.get_install_dir() / 'pandas',\n    )\n    meson.add_dist_script(py, versioneer, '-o', '_version_meson.py')\nendif\n\ncy = meson.get_compiler('cython')\nif cy.version().version_compare('>=3.1.0')\n    add_project_arguments('-Xfreethreading_compatible=true', language: 'cython')\nendif\n\n# Needed by pandas.test() when it looks for the pytest ini options\npy.install_sources('pyproject.toml', subdir: 'pandas')\n\nsubdir('pandas')\n"
        },
        {
          "name": "pandas",
          "type": "tree",
          "content": null
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 22.44,
          "content": "[build-system]\n# Minimum requirements for the build system to execute.\n# See https://github.com/scipy/scipy/pull/12940 for the AIX issue.\nrequires = [\n    \"meson-python>=0.13.1\",\n    \"meson>=1.2.1,<2\",\n    \"wheel\",\n    \"Cython~=3.0.5\",  # Note: sync with setup.py, environment.yml and asv.conf.json\n    # Force numpy higher than 2.0rc1, so that built wheels are compatible\n    # with both numpy 1 and 2\n    \"numpy>=2.0.0rc1\",\n    \"versioneer[toml]\"\n]\n\nbuild-backend = \"mesonpy\"\n\n[project]\nname = 'pandas'\ndynamic = [\n  'version'\n]\ndescription = 'Powerful data structures for data analysis, time series, and statistics'\nreadme = 'README.md'\nauthors = [\n  { name = 'The Pandas Development Team', email='pandas-dev@python.org' },\n]\nlicense = {file = 'LICENSE'}\nrequires-python = '>=3.10'\ndependencies = [\n  \"numpy>=1.23.5; python_version<'3.12'\",\n  \"numpy>=1.26.0; python_version>='3.12'\",\n  \"python-dateutil>=2.8.2\",\n  \"tzdata>=2022.7\"\n]\nclassifiers = [\n    'Development Status :: 5 - Production/Stable',\n    'Environment :: Console',\n    'Intended Audience :: Science/Research',\n    'License :: OSI Approved :: BSD License',\n    'Operating System :: OS Independent',\n    'Programming Language :: Cython',\n    'Programming Language :: Python',\n    'Programming Language :: Python :: 3',\n    'Programming Language :: Python :: 3 :: Only',\n    'Programming Language :: Python :: 3.10',\n    'Programming Language :: Python :: 3.11',\n    'Programming Language :: Python :: 3.12',\n    'Programming Language :: Python :: 3.13',\n    'Topic :: Scientific/Engineering'\n]\n\n[project.urls]\nhomepage = 'https://pandas.pydata.org'\ndocumentation = 'https://pandas.pydata.org/docs/'\nrepository = 'https://github.com/pandas-dev/pandas'\n\n[project.entry-points.\"pandas_plotting_backends\"]\nmatplotlib = \"pandas:plotting._matplotlib\"\n\n[project.optional-dependencies]\ntest = ['hypothesis>=6.84.0', 'pytest>=7.3.2', 'pytest-xdist>=3.4.0']\npyarrow = ['pyarrow>=10.0.1']\nperformance = ['bottleneck>=1.3.6', 'numba>=0.56.4', 'numexpr>=2.8.4']\ncomputation = ['scipy>=1.10.0', 'xarray>=2022.12.0']\nfss = ['fsspec>=2022.11.0']\naws = ['s3fs>=2022.11.0']\ngcp = ['gcsfs>=2022.11.0']\nexcel = ['odfpy>=1.4.1', 'openpyxl>=3.1.0', 'python-calamine>=0.1.7', 'pyxlsb>=1.0.10', 'xlrd>=2.0.1', 'xlsxwriter>=3.0.5']\nparquet = ['pyarrow>=10.0.1']\nfeather = ['pyarrow>=10.0.1']\nhdf5 = [# blosc only available on conda (https://github.com/Blosc/python-blosc/issues/297)\n        #'blosc>=1.20.1',\n        'tables>=3.8.0']\nspss = ['pyreadstat>=1.2.0']\npostgresql = ['SQLAlchemy>=2.0.0', 'psycopg2>=2.9.6', 'adbc-driver-postgresql>=0.10.0']\nmysql = ['SQLAlchemy>=2.0.0', 'pymysql>=1.0.2']\nsql-other = ['SQLAlchemy>=2.0.0', 'adbc-driver-postgresql>=0.10.0', 'adbc-driver-sqlite>=0.8.0']\nhtml = ['beautifulsoup4>=4.11.2', 'html5lib>=1.1', 'lxml>=4.9.2']\nxml = ['lxml>=4.9.2']\nplot = ['matplotlib>=3.6.3']\noutput-formatting = ['jinja2>=3.1.2', 'tabulate>=0.9.0']\nclipboard = ['PyQt5>=5.15.9', 'qtpy>=2.3.0']\ncompression = ['zstandard>=0.19.0']\ntimezone = ['pytz>=2023.4']\nall = ['adbc-driver-postgresql>=0.10.0',\n       'adbc-driver-sqlite>=0.8.0',\n       'beautifulsoup4>=4.11.2',\n       # blosc only available on conda (https://github.com/Blosc/python-blosc/issues/297)\n       #'blosc>=1.21.3',\n       'bottleneck>=1.3.6',\n       'fastparquet>=2023.10.0',\n       'fsspec>=2022.11.0',\n       'gcsfs>=2022.11.0',\n       'html5lib>=1.1',\n       'hypothesis>=6.84.0',\n       'jinja2>=3.1.2',\n       'lxml>=4.9.2',\n       'matplotlib>=3.6.3',\n       'numba>=0.56.4',\n       'numexpr>=2.8.4',\n       'odfpy>=1.4.1',\n       'openpyxl>=3.1.0',\n       'psycopg2>=2.9.6',\n       'pyarrow>=10.0.1',\n       'pymysql>=1.0.2',\n       'PyQt5>=5.15.9',\n       'pyreadstat>=1.2.0',\n       'pytest>=7.3.2',\n       'pytest-xdist>=3.4.0',\n       'python-calamine>=0.1.7',\n       'pytz>=2023.4',\n       'pyxlsb>=1.0.10',\n       'qtpy>=2.3.0',\n       'scipy>=1.10.0',\n       's3fs>=2022.11.0',\n       'SQLAlchemy>=2.0.0',\n       'tables>=3.8.0',\n       'tabulate>=0.9.0',\n       'xarray>=2022.12.0',\n       'xlrd>=2.0.1',\n       'xlsxwriter>=3.0.5',\n       'zstandard>=0.19.0']\n\n# TODO: Remove after setuptools support is dropped.\n[tool.setuptools]\ninclude-package-data = true\n\n[tool.setuptools.packages.find]\ninclude = [\"pandas\", \"pandas.*\"]\nnamespaces = false\n\n[tool.setuptools.exclude-package-data]\n\"*\" = [\"*.c\", \"*.h\"]\n\n# See the docstring in versioneer.py for instructions. Note that you must\n# re-run 'versioneer.py setup' after changing this section, and commit the\n# resulting files.\n[tool.versioneer]\nVCS = \"git\"\nstyle = \"pep440\"\nversionfile_source = \"pandas/_version.py\"\nversionfile_build = \"pandas/_version.py\"\ntag_prefix = \"v\"\nparentdir_prefix = \"pandas-\"\n\n[tool.meson-python.args]\nsetup = ['--vsenv'] # For Windows\n\n[tool.cibuildwheel]\nskip = \"cp36-* cp37-* cp38-* cp39-* pp* *_i686 *_ppc64le *_s390x\"\nbuild-verbosity = \"3\"\nenvironment = {LDFLAGS=\"-Wl,--strip-all\"}\ntest-requires = \"hypothesis>=6.84.0 pytest>=7.3.2 pytest-xdist>=3.4.0\"\ntest-command = \"\"\"\n  PANDAS_CI='1' python -c 'import pandas as pd; \\\n  pd.test(extra_args=[\"-m not clipboard and not single_cpu and not slow and not network and not db\", \"-n 2\", \"--no-strict-data-files\"]); \\\n  pd.test(extra_args=[\"-m not clipboard and single_cpu and not slow and not network and not db\", \"--no-strict-data-files\"]);' \\\n  \"\"\"\nfree-threaded-support = true\nbefore-build = \"PACKAGE_DIR={package} bash {package}/scripts/cibw_before_build.sh\"\n\n[tool.cibuildwheel.windows]\nbefore-build = \"pip install delvewheel && bash {package}/scripts/cibw_before_build_windows.sh\"\nbefore-test = \"bash {package}/scripts/cibw_before_test_windows.sh\"\ntest-command = \"\"\"\n  set PANDAS_CI='1' && \\\n  python -c \"import pandas as pd; \\\n  pd.test(extra_args=['--no-strict-data-files', '-m not clipboard and not single_cpu and not slow and not network and not db']);\" \\\n  \"\"\"\nrepair-wheel-command = \"delvewheel repair -w {dest_dir} {wheel}\"\n\n[[tool.cibuildwheel.overrides]]\nselect = \"*-manylinux_aarch64*\"\ntest-command = \"\"\"\n  PANDAS_CI='1' python -c 'import pandas as pd; \\\n  pd.test(extra_args=[\"-m not clipboard and not single_cpu and not slow and not network and not db and not fails_arm_wheels\", \"-n 2\", \"--no-strict-data-files\"]); \\\n  pd.test(extra_args=[\"-m not clipboard and single_cpu and not slow and not network and not db\", \"--no-strict-data-files\"]);' \\\n  \"\"\"\n\n[[tool.cibuildwheel.overrides]]\nselect = \"*-musllinux*\"\nbefore-test = \"apk update && apk add musl-locales\"\n\n[[tool.cibuildwheel.overrides]]\n# Don't strip wheels on macOS.\n# macOS doesn't support stripping wheels with linker\n# https://github.com/MacPython/numpy-wheels/pull/87#issuecomment-624878264\nselect = \"*-macosx*\"\nenvironment = {CFLAGS=\"-g0\"}\n\n[[tool.cibuildwheel.overrides]]\nselect = \"*pyodide*\"\ntest-requires = \"pytest>=7.3.2 hypothesis>=6.84.0\"\n# Pyodide repairs wheels on its own, using auditwheel-emscripten\nrepair-wheel-command = \"\"\ntest-command = \"\"\"\n  PANDAS_CI='1' python -c 'import pandas as pd; \\\n  pd.test(extra_args=[\"-m not clipboard and not single_cpu and not slow and not network and not db\", \"--no-strict-data-files\"]);' \\\n  \"\"\"\n\n[tool.ruff]\nline-length = 88\ntarget-version = \"py310\"\nfix = true\n\n[tool.ruff.lint]\nunfixable = []\ntyping-modules = [\"pandas._typing\"]\n\nselect = [\n  # pyflakes\n  \"F\",\n  # pycodestyle\n  \"E\", \"W\",\n  # flake8-2020\n  \"YTT\",\n  # flake8-bugbear\n  \"B\",\n  # flake8-quotes\n  \"Q\",\n  # flake8-debugger\n  \"T10\",\n  # flake8-gettext\n  \"INT\",\n  # pylint\n  \"PL\",\n  # flake8-pytest-style\n  \"PT\",\n  # misc lints\n  \"PIE\",\n  # flake8-pyi\n  \"PYI\",\n  # tidy imports\n  \"TID\",\n  # implicit string concatenation\n  \"ISC\",\n  # type-checking imports\n  \"TCH\",\n  # comprehensions\n  \"C4\",\n  # pygrep-hooks\n  \"PGH\",\n  # Ruff-specific rules\n  \"RUF\",\n  # flake8-bandit: exec-builtin\n  \"S102\",\n  # numpy-legacy-random\n  \"NPY002\",\n  # Perflint\n  \"PERF\",\n  # flynt\n  \"FLY\",\n  # flake8-logging-format\n  \"G\",\n  # flake8-future-annotations\n  \"FA\",\n  # unconventional-import-alias\n  \"ICN001\",\n  # flake8-slots\n  \"SLOT\",\n  # flake8-raise\n  \"RSE\"\n]\n\nignore = [\n  ### Intentionally disabled\n  # module level import not at top of file\n  \"E402\",\n  # do not assign a lambda expression, use a def\n  \"E731\",\n  # controversial\n  \"B007\",\n  # controversial\n  \"B008\",\n  # setattr is used to side-step mypy\n  \"B009\",\n  # getattr is used to side-step mypy\n  \"B010\",\n  # tests use comparisons but not their returned value\n  \"B015\",\n  # Function definition does not bind loop variable\n  \"B023\",\n  # Only works with python >=3.10\n  \"B905\",\n  # Too many arguments to function call\n  \"PLR0913\",\n  # Too many returns\n  \"PLR0911\",\n  # Too many branches\n  \"PLR0912\",\n  # Too many statements\n  \"PLR0915\",\n  # Redefined loop name\n  \"PLW2901\",\n  # Global statements are discouraged\n  \"PLW0603\",\n  # Use `typing.NamedTuple` instead of `collections.namedtuple`\n  \"PYI024\",\n  # Use of possibly insecure function; consider using ast.literal_eval\n  \"S307\",\n  # while int | float can be shortened to float, the former is more explicit\n  \"PYI041\",\n  # incorrect-dict-iterator, flags valid Series.items usage\n  \"PERF102\",\n  # try-except-in-loop, becomes useless in Python 3.11\n  \"PERF203\",\n  # pytest-parametrize-names-wrong-type\n  \"PT006\",\n  # pytest-parametrize-values-wrong-type\n  \"PT007\",\n  # pytest-patch-with-lambda\n  \"PT008\",\n  # pytest-raises-with-multiple-statements\n  \"PT012\",\n  # pytest-assert-in-except\n  \"PT017\",\n  # pytest-composite-assertion\n  \"PT018\",\n  # pytest-fixture-param-without-value\n  \"PT019\",\n  # The following rules may cause conflicts when used with the formatter:\n  \"ISC001\",\n  # if-stmt-min-max\n  \"PLR1730\",\n\n  ### TODO: Enable gradually\n  # Useless statement\n  \"B018\",\n  # Magic number\n  \"PLR2004\",\n  # comparison-with-itself\n  \"PLR0124\",\n  # collection-literal-concatenation\n  \"RUF005\",\n  # pairwise-over-zipped (>=PY310 only)\n  \"RUF007\",\n  # mutable-class-default\n  \"RUF012\",\n  # type-comparison\n  \"E721\",\n  # repeated-equality-comparison\n  \"PLR1714\",\n  # self-or-cls-assignment\n  \"PLW0642\",\n  # literal-membership\n  \"PLR6201\", # 847 errors\n  # Method could be a function, class method, or static method\n  \"PLR6301\", # 11411 errors\n  # Private name import\n  \"PLC2701\", # 27 errors\n  # Too many positional arguments (6/5)\n  \"PLR0917\", # 470 errors\n  # compare-to-empty-string\n  \"PLC1901\",\n  # `tempfile.NamedTemporaryFile` in text mode without explicit `encoding` argument\n  \"PLW1514\", # 1 error\n  # Object does not implement `__hash__` method\n  \"PLW1641\", # 16 errors\n  # Bad or misspelled dunder method name\n  \"PLW3201\", # 69 errors, seems to be all false positive\n  # Unnecessary lookup of dictionary value by key\n  \"PLR1733\", # 5 errors, it seems like we wannt to ignore these\n  # Unnecessary lookup of list item by index\n  \"PLR1736\", # 4 errors, we're currently having inline pylint ignore\n  # empty-comment\n  \"PLR2044\", # autofixable\n  # Unpacking a dictionary in iteration without calling `.items()`\n  \"PLE1141\", # autofixable\n  # import-outside-toplevel\n  \"PLC0415\",\n  # unnecessary-dunder-call\n  \"PLC2801\",\n  # comparison-with-itself\n  \"PLR0124\",\n  # too-many-public-methods\n  \"PLR0904\",\n  # too-many-return-statements\n  \"PLR0911\",\n  # too-many-branches\n  \"PLR0912\",\n  # too-many-arguments\n  \"PLR0913\",\n  # too-many-locals\n  \"PLR0914\",\n  # too-many-statements\n  \"PLR0915\",\n  # too-many-boolean-expressions\n  \"PLR0916\",\n  # too-many-nested-blocks\n  \"PLR1702\",\n  # redefined-argument-from-local\n  \"PLR1704\",\n  # unnecessary-lambda\n  \"PLW0108\",\n  # global-statement\n  \"PLW0603\",\n]\n\nexclude = [\n  \"doc/sphinxext/*.py\",\n  \"doc/build/*.py\",\n  \"doc/temp/*.py\",\n  \".eggs/*.py\",\n  # vendored files\n  \"pandas/util/version/*\",\n  \"pandas/io/clipboard/__init__.py\",\n  # exclude asv benchmark environments from linting\n  \"env\",\n]\n\n[tool.ruff.lint.flake8-tidy-imports.banned-api]\n\"urllib.request.urlopen\".msg = \"Use pandas.io.common.urlopen instead of urllib.request.urlopen\"\n# numpy.random is banned but np.random is not. Is this intentional?\n# \"numpy.random\".msg = \"Do not use numpy.random\"\n\"pytest.warns\".msg = \"Use tm.assert_produces_warning instead of pytest.warns\"\n\"pytest.xfail\".msg = \"Use pytest.mark.xfail instead of pytest.xfail\"\n\"conftest\".msg = \"No direct imports from conftest\"\n\"numpy.testing\".msg = \"Do not use numpy.testing\"\n# \"numpy.array_equal\".msg = \"Do not use numpy.array_equal\" # Used in pandas/core\n\"unittest.mock\".msg = \"use pytest builtin monkeypatch fixture instead\"\n\"os.remove\".msg = \"Do not use os.remove\"\n\n\n\n[tool.ruff.lint.flake8-import-conventions.aliases]\n\"pandas.core.construction.array\" = \"pd_array\"\n\n[tool.ruff.lint.per-file-ignores]\n# relative imports allowed for asv_bench\n\"asv_bench/*\" = [\"TID\", \"NPY002\"]\n# to be enabled gradually\n\"pandas/core/*\" = [\"PLR5501\"]\n\"pandas/tests/*\" = [\"B028\", \"FLY\"]\n\"scripts/*\" = [\"B028\"]\n# Keep this one enabled\n\"pandas/_typing.py\" = [\"TCH\"]\n\n[tool.ruff.lint.flake8-pytest-style]\nfixture-parentheses = false\nmark-parentheses = false\n\n[tool.ruff.format]\ndocstring-code-format = true\n\n[tool.pytest.ini_options]\n# sync minversion with pyproject.toml & install.rst\nminversion = \"7.3.2\"\naddopts = \"--strict-markers --strict-config --capture=no --durations=30 --junitxml=test-data.xml\"\nempty_parameter_set_mark = \"fail_at_collect\"\nxfail_strict = true\ntestpaths = \"pandas\"\ndoctest_optionflags = [\n  \"NORMALIZE_WHITESPACE\",\n  \"IGNORE_EXCEPTION_DETAIL\",\n  \"ELLIPSIS\",\n]\nfilterwarnings = [\n  \"error:::pandas\",\n  \"error::ResourceWarning\",\n  \"error::pytest.PytestUnraisableExceptionWarning\",\n  # TODO(PY311-minimum): Specify EncodingWarning\n  # Ignore 3rd party EncodingWarning but raise on pandas'\n  \"ignore:.*encoding.* argument not specified\",\n  \"error:.*encoding.* argument not specified::pandas\",\n  \"ignore:.*ssl.SSLSocket:pytest.PytestUnraisableExceptionWarning\",\n  \"ignore:.*ssl.SSLSocket:ResourceWarning\",\n  # GH 44844: Can remove once minimum matplotlib version >= 3.7\n  \"ignore:.*FileIO:pytest.PytestUnraisableExceptionWarning\",\n  \"ignore:.*BufferedRandom:ResourceWarning\",\n  \"ignore::ResourceWarning:asyncio\",\n  # From plotting doctests\n  \"ignore:More than 20 figures have been opened:RuntimeWarning\",\n  \"ignore:.*urllib3:DeprecationWarning:botocore\",\n  \"ignore:Setuptools is replacing distutils.:UserWarning:_distutils_hack\",\n  # https://github.com/PyTables/PyTables/issues/822\n  \"ignore:a closed node found in the registry:UserWarning:tables\",\n]\njunit_family = \"xunit2\"\nmarkers = [\n  \"single_cpu: tests that should run on a single cpu only\",\n  \"slow: mark a test as slow\",\n  \"network: mark a test as network\",\n  \"db: tests requiring a database (mysql or postgres)\",\n  \"clipboard: mark a pd.read_clipboard test\",\n  \"arm_slow: mark a test as slow for arm64 architecture\",\n  \"skip_ubsan: Tests known to fail UBSAN check\",\n  # TODO: someone should investigate this ...\n  # these tests only fail in the wheel builder and don't fail in regular\n  # ARM CI\n  \"fails_arm_wheels: Tests that fail in the ARM wheel build only\",\n]\n\n[tool.mypy]\n# Import discovery\nmypy_path = \"typings\"\nfiles = [\"pandas\", \"typings\"]\nnamespace_packages = false\nexplicit_package_bases = false\nignore_missing_imports = true\nfollow_imports = \"normal\"\nfollow_imports_for_stubs = false\nno_site_packages = false\nno_silence_site_packages = false\n# Platform configuration\npython_version = \"3.11\"\nplatform = \"linux-64\"\n# Disallow dynamic typing\ndisallow_any_unimported = false # TODO\ndisallow_any_expr = false # TODO\ndisallow_any_decorated = false # TODO\ndisallow_any_explicit = false # TODO\ndisallow_any_generics = false # TODO\ndisallow_subclassing_any = false # TODO\n# Untyped definitions and calls\ndisallow_untyped_calls = true\ndisallow_untyped_defs = true\ndisallow_incomplete_defs = true\ncheck_untyped_defs = true\ndisallow_untyped_decorators = true\n# None and Optional handling\nno_implicit_optional = true\nstrict_optional = true\n# Configuring warnings\nwarn_redundant_casts = true\nwarn_unused_ignores = true\nwarn_no_return = true\nwarn_return_any = false # TODO\nwarn_unreachable = false # GH#27396\n# Suppressing errors\nignore_errors = false\nenable_error_code = \"ignore-without-code\"\n# Miscellaneous strictness flags\nallow_untyped_globals = false\nallow_redefinition = false\nlocal_partial_types = false\nimplicit_reexport = true\nstrict_equality = true\n# Configuring error messages\nshow_error_context = false\nshow_column_numbers = false\nshow_error_codes = true\n\n[[tool.mypy.overrides]]\nmodule = [\n  \"pandas._config.config\", # TODO\n  \"pandas._libs.*\",\n  \"pandas._testing.*\", # TODO\n  \"pandas.compat.numpy.function\", # TODO\n  \"pandas.core._numba.executor\", # TODO\n  \"pandas.core.array_algos.masked_reductions\", # TODO\n  \"pandas.core.array_algos.putmask\", # TODO\n  \"pandas.core.array_algos.quantile\", # TODO\n  \"pandas.core.array_algos.replace\", # TODO\n  \"pandas.core.array_algos.take\", # TODO\n  \"pandas.core.arrays.*\", # TODO\n  \"pandas.core.computation.*\", # TODO\n  \"pandas.core.dtypes.astype\", # TODO\n  \"pandas.core.dtypes.cast\", # TODO\n  \"pandas.core.dtypes.common\", # TODO\n  \"pandas.core.dtypes.concat\", # TODO\n  \"pandas.core.dtypes.dtypes\", # TODO\n  \"pandas.core.dtypes.generic\", # TODO\n  \"pandas.core.dtypes.missing\", # TODO\n  \"pandas.core.groupby.generic\", # TODO\n  \"pandas.core.groupby.grouper\", # TODO\n  \"pandas.core.groupby.groupby\", # TODO\n  \"pandas.core.groupby.ops\", # TODO\n  \"pandas.core.indexers.*\", # TODO\n  \"pandas.core.indexes.*\", # TODO\n  \"pandas.core.interchange.column\", # TODO\n  \"pandas.core.interchange.dataframe_protocol\", # TODO\n  \"pandas.core.interchange.from_dataframe\", # TODO\n  \"pandas.core.internals.*\", # TODO\n  \"pandas.core.ops.array_ops\", # TODO\n  \"pandas.core.ops.common\", # TODO\n  \"pandas.core.ops.missing\", # TODO\n  \"pandas.core.reshape.*\", # TODO\n  \"pandas.core.strings.*\", # TODO\n  \"pandas.core.tools.*\", # TODO\n  \"pandas.core.window.common\", # TODO\n  \"pandas.core.window.ewm\", # TODO\n  \"pandas.core.window.expanding\", # TODO\n  \"pandas.core.window.numba_\", # TODO\n  \"pandas.core.window.online\", # TODO\n  \"pandas.core.window.rolling\", # TODO\n  \"pandas.core.accessor\", # TODO\n  \"pandas.core.algorithms\", # TODO\n  \"pandas.core.apply\", # TODO\n  \"pandas.core.arraylike\", # TODO\n  \"pandas.core.base\", # TODO\n  \"pandas.core.common\", # TODO\n  \"pandas.core.construction\", # TODO\n  \"pandas.core.flags\", # TODO\n  \"pandas.core.frame\", # TODO\n  \"pandas.core.generic\", # TODO\n  \"pandas.core.indexing\", # TODO\n  \"pandas.core.missing\", # TODO\n  \"pandas.core.nanops\", # TODO\n  \"pandas.core.resample\", # TODO\n  \"pandas.core.roperator\", # TODO\n  \"pandas.core.sample\", # TODO\n  \"pandas.core.series\", # TODO\n  \"pandas.core.sorting\", # TODO\n  \"pandas.errors\", # TODO\n  \"pandas.io.clipboard\", # TODO\n  \"pandas.io.excel._base\", # TODO\n  \"pandas.io.excel._odfreader\", # TODO\n  \"pandas.io.excel._openpyxl\", # TODO\n  \"pandas.io.excel._pyxlsb\", # TODO\n  \"pandas.io.excel._xlrd\", # TODO\n  \"pandas.io.excel._xlsxwriter\", # TODO\n  \"pandas.io.formats.excel\", # TODO\n  \"pandas.io.formats.format\", # TODO\n  \"pandas.io.formats.style\", # TODO\n  \"pandas.io.formats.style_render\", # TODO\n  \"pandas.io.formats.xml\", # TODO\n  \"pandas.io.json.*\", # TODO\n  \"pandas.io.parsers.*\", # TODO\n  \"pandas.io.sas.sas_xport\", # TODO\n  \"pandas.io.sas.sas7bdat\", # TODO\n  \"pandas.io.clipboards\", # TODO\n  \"pandas.io.html\", # TODO\n  \"pandas.io.parquet\", # TODO\n  \"pandas.io.pytables\", # TODO\n  \"pandas.io.sql\", # TODO\n  \"pandas.io.xml\", # TODO\n  \"pandas.plotting.*\", # TODO\n  \"pandas.tests.*\",\n  \"pandas.tseries.frequencies\", # TODO\n  \"pandas.tseries.holiday\", # TODO\n  \"pandas.util._decorators\", # TODO\n  \"pandas.util._doctools\", # TODO\n  \"pandas.util._test_decorators\", # TODO\n  \"pandas.util._validators\", # TODO\n  \"pandas.util\", # TODO\n  \"pandas._version\",\n  \"pandas.conftest\",\n  \"pandas\"\n]\ndisallow_untyped_calls = false\ndisallow_untyped_defs = false\ndisallow_incomplete_defs = false\n\n[[tool.mypy.overrides]]\nmodule = [\n  \"pandas.tests.*\",\n  \"pandas._version\",\n  \"pandas.io.clipboard\",\n]\ncheck_untyped_defs = false\n\n[[tool.mypy.overrides]]\nmodule = [\n  \"pandas.tests.apply.test_series_apply\",\n  \"pandas.tests.arithmetic.conftest\",\n  \"pandas.tests.arrays.sparse.test_combine_concat\",\n  \"pandas.tests.dtypes.test_common\",\n  \"pandas.tests.frame.methods.test_to_records\",\n  \"pandas.tests.groupby.test_rank\",\n  \"pandas.tests.groupby.transform.test_transform\",\n  \"pandas.tests.indexes.interval.test_interval\",\n  \"pandas.tests.indexing.test_categorical\",\n  \"pandas.tests.io.excel.test_writers\",\n  \"pandas.tests.reductions.test_reductions\",\n  \"pandas.tests.test_expressions\",\n]\nignore_errors = true\n\n# To be kept consistent with \"Import Formatting\" section in contributing.rst\n[tool.isort]\nknown_pre_libs = \"pandas._config\"\nknown_pre_core = [\"pandas._libs\", \"pandas._typing\", \"pandas.util._*\", \"pandas.compat\", \"pandas.errors\"]\nknown_dtypes = \"pandas.core.dtypes\"\nknown_post_core = [\"pandas.tseries\", \"pandas.io\", \"pandas.plotting\"]\nsections = [\"FUTURE\", \"STDLIB\", \"THIRDPARTY\" ,\"PRE_LIBS\" , \"PRE_CORE\", \"DTYPES\", \"FIRSTPARTY\", \"POST_CORE\", \"LOCALFOLDER\"]\nprofile = \"black\"\ncombine_as_imports = true\nforce_grid_wrap = 2\nforce_sort_within_sections = true\nskip_glob = \"env\"\nskip = \"pandas/__init__.py\"\n\n[tool.pyright]\npythonVersion = \"3.11\"\ntypeCheckingMode = \"basic\"\nuseLibraryCodeForTypes = false\ninclude = [\"pandas\", \"typings\"]\nexclude = [\"pandas/tests\", \"pandas/io/clipboard\", \"pandas/util/version\", \"pandas/core/_numba/extensions.py\"]\n# enable subset of \"strict\"\nreportDuplicateImport = true\nreportInconsistentConstructor = true\nreportInvalidStubStatement = true\nreportOverlappingOverload = true\nreportPropertyTypeMismatch = true\nreportUntypedClassDecorator = true\nreportUntypedFunctionDecorator = true\nreportUntypedNamedTuple = true\nreportUnusedImport = true\ndisableBytesTypePromotions = true\n# disable subset of \"basic\"\nreportArgumentType = false\nreportAssignmentType = false\nreportAttributeAccessIssue = false\nreportCallIssue = false\nreportGeneralTypeIssues = false\nreportIndexIssue = false\nreportMissingModuleSource = false\nreportOperatorIssue = false\nreportOptionalCall = false\nreportOptionalIterable = false\nreportOptionalMemberAccess = false\nreportOptionalOperand = false\nreportOptionalSubscript = false\nreportPrivateImportUsage = false\nreportRedeclaration = false\nreportReturnType = false\nreportUnboundVariable = false\n\n[tool.coverage.run]\nbranch = true\nomit = [\"pandas/_typing.py\", \"pandas/_version.py\"]\nplugins = [\"Cython.Coverage\"]\nsource = [\"pandas\"]\n\n[tool.coverage.report]\nignore_errors = false\nshow_missing = true\nomit = [\"pandas/_version.py\"]\nexclude_lines = [\n  # Have to re-enable the standard pragma\n  \"pragma: no cover\",\n  # Don't complain about missing debug-only code:s\n  \"def __repr__\",\n  \"if self.debug\",\n  # Don't complain if tests don't hit defensive assertion code:\n  \"raise AssertionError\",\n  \"raise NotImplementedError\",\n  \"AbstractMethodError\",\n  # Don't complain if non-runnable code isn't run:\n  \"if 0:\",\n  \"if __name__ == .__main__.:\",\n  \"if TYPE_CHECKING:\",\n]\n\n[tool.coverage.html]\ndirectory = \"coverage_html_report\"\n\n[tool.codespell]\nignore-words-list = \"blocs, coo, hist, nd, sav, ser, recuse, nin, timere, expec, expecs, indext, SME, NotIn, tructures, tru\"\nignore-regex = 'https://([\\w/\\.])+'\n"
        },
        {
          "name": "pyright_reportGeneralTypeIssues.json",
          "type": "blob",
          "size": 4.21,
          "content": "{\n    \"typeCheckingMode\": \"off\",\n    \"reportArgumentType\": true,\n    \"reportAssignmentType\": true,\n    \"reportAttributeAccessIssue\": true,\n    \"reportCallIssue\": true,\n    \"reportGeneralTypeIssues\": true,\n    \"reportIndexIssue\": true,\n    \"reportOperatorIssue\": true,\n    \"reportRedeclaration\": true,\n    \"reportReturnType\": true,\n    \"useLibraryCodeForTypes\": false,\n    \"analyzeUnannotatedFunctions\": false,\n    \"disableBytesTypePromotions\": true,\n    \"include\":\n      [\n        \"pandas\",\n        \"typings\"\n      ],\n    \"exclude\":\n      [\n        \"pandas/tests\",\n\n        \"pandas/io/clipboard\",\n        \"pandas/util/version\",\n\n        \"pandas/_testing/__init__.py\",\n        \"pandas/_testing/_io.py\",\n        \"pandas/compat/pickle_compat.py\",\n        \"pandas/core/_numba/extensions.py\",\n        \"pandas/core/_numba/kernels/sum_.py\",\n        \"pandas/core/_numba/kernels/var_.py\",\n        \"pandas/core/algorithms.py\",\n        \"pandas/core/apply.py\",\n        \"pandas/core/array_algos/take.py\",\n        \"pandas/core/arrays/_mixins.py\",\n        \"pandas/core/arrays/arrow/array.py\",\n        \"pandas/core/arrays/base.py\",\n        \"pandas/core/arrays/boolean.py\",\n        \"pandas/core/arrays/categorical.py\",\n        \"pandas/core/arrays/datetimelike.py\",\n        \"pandas/core/arrays/datetimes.py\",\n        \"pandas/core/arrays/interval.py\",\n        \"pandas/core/arrays/masked.py\",\n        \"pandas/core/arrays/period.py\",\n        \"pandas/core/arrays/sparse/accessor.py\",\n        \"pandas/core/arrays/sparse/array.py\",\n        \"pandas/core/arrays/string_.py\",\n        \"pandas/core/arrays/string_arrow.py\",\n        \"pandas/core/arrays/timedeltas.py\",\n        \"pandas/core/computation/align.py\",\n        \"pandas/core/construction.py\",\n        \"pandas/core/dtypes/cast.py\",\n        \"pandas/core/dtypes/common.py\",\n        \"pandas/core/dtypes/concat.py\",\n        \"pandas/core/dtypes/dtypes.py\",\n        \"pandas/core/frame.py\",\n        \"pandas/core/generic.py\",\n        \"pandas/core/groupby/generic.py\",\n        \"pandas/core/groupby/groupby.py\",\n        \"pandas/core/groupby/grouper.py\",\n        \"pandas/core/groupby/ops.py\",\n        \"pandas/core/indexers/utils.py\",\n        \"pandas/core/indexes/base.py\",\n        \"pandas/core/indexes/category.py\",\n        \"pandas/core/indexes/datetimelike.py\",\n        \"pandas/core/indexes/datetimes.py\",\n        \"pandas/core/indexes/extension.py\",\n        \"pandas/core/indexes/interval.py\",\n        \"pandas/core/indexes/multi.py\",\n        \"pandas/core/indexes/period.py\",\n        \"pandas/core/indexing.py\",\n        \"pandas/core/internals/api.py\",\n        \"pandas/core/internals/blocks.py\",\n        \"pandas/core/internals/construction.py\",\n        \"pandas/core/internals/managers.py\",\n        \"pandas/core/missing.py\",\n        \"pandas/core/nanops.py\",\n        \"pandas/core/ops/array_ops.py\",\n        \"pandas/core/resample.py\",\n        \"pandas/core/reshape/concat.py\",\n        \"pandas/core/reshape/merge.py\",\n        \"pandas/core/reshape/pivot.py\",\n        \"pandas/core/reshape/tile.py\",\n        \"pandas/core/series.py\",\n        \"pandas/core/sorting.py\",\n        \"pandas/core/strings/accessor.py\",\n        \"pandas/core/tools/datetimes.py\",\n        \"pandas/core/tools/numeric.py\",\n        \"pandas/core/util/hashing.py\",\n        \"pandas/core/window/ewm.py\",\n        \"pandas/core/window/rolling.py\",\n        \"pandas/io/common.py\",\n        \"pandas/io/excel/_base.py\",\n        \"pandas/io/excel/_odfreader.py\",\n        \"pandas/io/formats/excel.py\",\n        \"pandas/io/formats/format.py\",\n        \"pandas/io/formats/info.py\",\n        \"pandas/io/formats/printing.py\",\n        \"pandas/io/formats/style.py\",\n        \"pandas/io/formats/style_render.py\",\n        \"pandas/io/json/_json.py\",\n        \"pandas/io/json/_normalize.py\",\n        \"pandas/io/parsers/arrow_parser_wrapper.py\",\n        \"pandas/io/parsers/base_parser.py\",\n        \"pandas/io/parsers/c_parser_wrapper.py\",\n        \"pandas/io/pytables.py\",\n        \"pandas/io/sql.py\",\n        \"pandas/io/stata.py\",\n        \"pandas/plotting/_matplotlib/boxplot.py\",\n        \"pandas/plotting/_matplotlib/core.py\",\n        \"pandas/plotting/_matplotlib/misc.py\",\n        \"pandas/plotting/_matplotlib/timeseries.py\",\n        \"pandas/plotting/_matplotlib/tools.py\",\n        \"pandas/tseries/frequencies.py\",\n        \"pandas/tseries/holiday.py\",\n      ],\n}\n"
        },
        {
          "name": "requirements-dev.txt",
          "type": "blob",
          "size": 1.38,
          "content": "# This file is auto-generated from environment.yml, do not modify.\n# See that file for comments about the need/usage of each dependency.\n\npip\nversioneer[toml]\ncython~=3.0.5\nmeson[ninja]==1.2.1\nmeson-python==0.13.1\npytest>=7.3.2\npytest-cov\npytest-xdist>=3.4.0\npytest-qt>=4.4.0\npytest-localserver\nPyQt5>=5.15.9\ncoverage\npython-dateutil\nnumpy<2\nbeautifulsoup4>=4.11.2\nblosc\nbottleneck>=1.3.6\nfastparquet>=2023.10.0\nfsspec>=2022.11.0\nhtml5lib>=1.1\nhypothesis>=6.84.0\ngcsfs>=2022.11.0\nipython\npickleshare\njinja2>=3.1.2\nlxml>=4.9.2\nmatplotlib>=3.6.3\nnumba>=0.56.4\nnumexpr>=2.8.4\nopenpyxl>=3.1.0\nodfpy>=1.4.1\npy\npsycopg2-binary>=2.9.6\npyarrow>=10.0.1\npymysql>=1.0.2\npyreadstat>=1.2.0\ntables>=3.8.0\npython-calamine>=0.1.7\npytz>=2023.4\npyxlsb>=1.0.10\ns3fs>=2022.11.0\nscipy>=1.10.0\nSQLAlchemy>=2.0.0\ntabulate>=0.9.0\nxarray>=2022.12.0, <=2024.9.0\nxlrd>=2.0.1\nxlsxwriter>=3.0.5\nzstandard>=0.19.0\ndask\nseaborn\nmoto\nflask\nasv>=0.6.1\nflake8==7.1.0\nmypy==1.13.0\ntokenize-rt\npre-commit>=4.0.1\ngitpython\ngitdb\ngoogle-auth\nnatsort\nnumpydoc\npydata-sphinx-theme==0.16\npytest-cython\nsphinx\nsphinx-design\nsphinx-copybutton\ntypes-python-dateutil\ntypes-PyMySQL\ntypes-pytz\ntypes-PyYAML\ntypes-setuptools\nnbconvert>=7.11.0\nnbsphinx\npandoc\nipywidgets\nnbformat\nnotebook>=7.0.6\nipykernel\nmarkdown\nfeedparser\npyyaml\nrequests\npygments\nadbc-driver-postgresql>=0.10.0\nadbc-driver-sqlite>=0.8.0\ntyping_extensions; python_version<\"3.11\"\ntzdata>=2022.7\n"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 21.45,
          "content": "#!/usr/bin/env python3\n\n\"\"\"\nParts of this file were taken from the pyzmq project\n(https://github.com/zeromq/pyzmq) which have been permitted for use under the\nBSD license. Parts are from lxml (https://github.com/lxml/lxml)\n\"\"\"\n\nimport argparse\nimport multiprocessing\nimport os\nfrom os.path import join as pjoin\nimport platform\nimport shutil\nimport sys\nfrom sysconfig import get_config_vars\n\nimport numpy\nfrom pkg_resources import parse_version\nfrom setuptools import (\n    Command,\n    Extension,\n    setup,\n)\nfrom setuptools.command.build_ext import build_ext as _build_ext\nimport versioneer\n\ncmdclass = versioneer.get_cmdclass()\n\n\ndef is_platform_windows():\n    return sys.platform in (\"win32\", \"cygwin\")\n\n\ndef is_platform_mac():\n    return sys.platform == \"darwin\"\n\n\n# note: sync with pyproject.toml, environment.yml and asv.conf.json\nmin_cython_ver = \"3.0\"\n\ntry:\n    from Cython import (\n        Tempita,\n        __version__ as _CYTHON_VERSION,\n    )\n    from Cython.Build import cythonize\n\n    _CYTHON_INSTALLED = parse_version(_CYTHON_VERSION) >= parse_version(min_cython_ver)\nexcept ImportError:\n    _CYTHON_VERSION = None\n    _CYTHON_INSTALLED = False\n    cythonize = lambda x, *args, **kwargs: x  # dummy func\n\n\n_pxi_dep_template = {\n    \"algos\": [\"_libs/algos_common_helper.pxi.in\", \"_libs/algos_take_helper.pxi.in\"],\n    \"hashtable\": [\n        \"_libs/hashtable_class_helper.pxi.in\",\n        \"_libs/hashtable_func_helper.pxi.in\",\n        \"_libs/khash_for_primitive_helper.pxi.in\",\n    ],\n    \"index\": [\"_libs/index_class_helper.pxi.in\"],\n    \"sparse\": [\"_libs/sparse_op_helper.pxi.in\"],\n    \"interval\": [\"_libs/intervaltree.pxi.in\"],\n}\n\n_pxifiles = []\n_pxi_dep = {}\nfor module, files in _pxi_dep_template.items():\n    pxi_files = [pjoin(\"pandas\", x) for x in files]\n    _pxifiles.extend(pxi_files)\n    _pxi_dep[module] = pxi_files\n\n\nclass build_ext(_build_ext):\n    @classmethod\n    def render_templates(cls, pxifiles) -> None:\n        for pxifile in pxifiles:\n            # build pxifiles first, template extension must be .pxi.in\n            assert pxifile.endswith(\".pxi.in\")\n            outfile = pxifile[:-3]\n\n            if (\n                os.path.exists(outfile)\n                and os.stat(pxifile).st_mtime < os.stat(outfile).st_mtime\n            ):\n                # if .pxi.in is not updated, no need to output .pxi\n                continue\n\n            with open(pxifile, encoding=\"utf-8\") as f:\n                tmpl = f.read()\n            pyxcontent = Tempita.sub(tmpl)\n\n            with open(outfile, \"w\", encoding=\"utf-8\") as f:\n                f.write(pyxcontent)\n\n    def build_extensions(self) -> None:\n        # if building from c files, don't need to\n        # generate template output\n        if _CYTHON_INSTALLED:\n            self.render_templates(_pxifiles)\n\n        super().build_extensions()\n\n\nclass CleanCommand(Command):\n    \"\"\"Custom command to clean the .so and .pyc files.\"\"\"\n\n    user_options = [(\"all\", \"a\", \"\")]\n\n    def initialize_options(self) -> None:\n        self.all = True\n        self._clean_me = []\n        self._clean_trees = []\n\n        base = pjoin(\"pandas\", \"_libs\", \"src\")\n        parser = pjoin(base, \"parser\")\n        vendored = pjoin(base, \"vendored\")\n        dt = pjoin(base, \"datetime\")\n        ujson_python = pjoin(vendored, \"ujson\", \"python\")\n        ujson_lib = pjoin(vendored, \"ujson\", \"lib\")\n        self._clean_exclude = [\n            pjoin(vendored, \"numpy\", \"datetime\", \"np_datetime.c\"),\n            pjoin(vendored, \"numpy\", \"datetime\", \"np_datetime_strings.c\"),\n            pjoin(dt, \"date_conversions.c\"),\n            pjoin(parser, \"tokenizer.c\"),\n            pjoin(parser, \"io.c\"),\n            pjoin(ujson_python, \"ujson.c\"),\n            pjoin(ujson_python, \"objToJSON.c\"),\n            pjoin(ujson_python, \"JSONtoObj.c\"),\n            pjoin(ujson_lib, \"ultrajsonenc.c\"),\n            pjoin(ujson_lib, \"ultrajsondec.c\"),\n            pjoin(dt, \"pd_datetime.c\"),\n            pjoin(parser, \"pd_parser.c\"),\n        ]\n\n        for root, dirs, files in os.walk(\"pandas\"):\n            for f in files:\n                filepath = pjoin(root, f)\n                if filepath in self._clean_exclude:\n                    continue\n\n                if os.path.splitext(f)[-1] in (\n                    \".pyc\",\n                    \".so\",\n                    \".o\",\n                    \".pyo\",\n                    \".pyd\",\n                    \".c\",\n                    \".cpp\",\n                    \".orig\",\n                ):\n                    self._clean_me.append(filepath)\n            self._clean_trees.append(pjoin(root, d) for d in dirs if d == \"__pycache__\")\n\n        # clean the generated pxi files\n        for pxifile in _pxifiles:\n            pxifile_replaced = pxifile.replace(\".pxi.in\", \".pxi\")\n            self._clean_me.append(pxifile_replaced)\n\n        self._clean_trees.append(d for d in (\"build\", \"dist\") if os.path.exists(d))\n\n    def finalize_options(self) -> None:\n        pass\n\n    def run(self) -> None:\n        for clean_me in self._clean_me:\n            try:\n                os.unlink(clean_me)\n            except OSError:\n                pass\n        for clean_tree in self._clean_trees:\n            try:\n                shutil.rmtree(clean_tree)\n            except OSError:\n                pass\n\n\n# we need to inherit from the versioneer\n# class as it encodes the version info\nsdist_class = cmdclass[\"sdist\"]\n\n\nclass CheckSDist(sdist_class):\n    \"\"\"Custom sdist that ensures Cython has compiled all pyx files to c.\"\"\"\n\n    _pyxfiles = [\n        \"pandas/_libs/arrays.pyx\",\n        \"pandas/_libs/lib.pyx\",\n        \"pandas/_libs/hashtable.pyx\",\n        \"pandas/_libs/tslib.pyx\",\n        \"pandas/_libs/index.pyx\",\n        \"pandas/_libs/internals.pyx\",\n        \"pandas/_libs/algos.pyx\",\n        \"pandas/_libs/join.pyx\",\n        \"pandas/_libs/indexing.pyx\",\n        \"pandas/_libs/interval.pyx\",\n        \"pandas/_libs/hashing.pyx\",\n        \"pandas/_libs/missing.pyx\",\n        \"pandas/_libs/testing.pyx\",\n        \"pandas/_libs/sparse.pyx\",\n        \"pandas/_libs/ops.pyx\",\n        \"pandas/_libs/parsers.pyx\",\n        \"pandas/_libs/tslibs/base.pyx\",\n        \"pandas/_libs/tslibs/ccalendar.pyx\",\n        \"pandas/_libs/tslibs/dtypes.pyx\",\n        \"pandas/_libs/tslibs/period.pyx\",\n        \"pandas/_libs/tslibs/strptime.pyx\",\n        \"pandas/_libs/tslibs/np_datetime.pyx\",\n        \"pandas/_libs/tslibs/timedeltas.pyx\",\n        \"pandas/_libs/tslibs/timestamps.pyx\",\n        \"pandas/_libs/tslibs/timezones.pyx\",\n        \"pandas/_libs/tslibs/conversion.pyx\",\n        \"pandas/_libs/tslibs/fields.pyx\",\n        \"pandas/_libs/tslibs/offsets.pyx\",\n        \"pandas/_libs/tslibs/parsing.pyx\",\n        \"pandas/_libs/tslibs/tzconversion.pyx\",\n        \"pandas/_libs/tslibs/vectorized.pyx\",\n        \"pandas/_libs/window/indexers.pyx\",\n        \"pandas/_libs/writers.pyx\",\n        \"pandas/_libs/sas.pyx\",\n        \"pandas/_libs/byteswap.pyx\",\n    ]\n\n    _cpp_pyxfiles = [\n        \"pandas/_libs/window/aggregations.pyx\",\n    ]\n\n    def initialize_options(self) -> None:\n        sdist_class.initialize_options(self)\n\n    def run(self) -> None:\n        if \"cython\" in cmdclass:\n            self.run_command(\"cython\")\n        else:\n            # If we are not running cython then\n            # compile the extensions correctly\n            pyx_files = [(self._pyxfiles, \"c\"), (self._cpp_pyxfiles, \"cpp\")]\n\n            for pyxfiles, extension in pyx_files:\n                for pyxfile in pyxfiles:\n                    sourcefile = pyxfile[:-3] + extension\n                    msg = (\n                        f\"{extension}-source file '{sourcefile}' not found.\\n\"\n                        \"Run 'setup.py cython' before sdist.\"\n                    )\n                    assert os.path.isfile(sourcefile), msg\n        sdist_class.run(self)\n\n\nclass CheckingBuildExt(build_ext):\n    \"\"\"\n    Subclass build_ext to get clearer report if Cython is necessary.\n    \"\"\"\n\n    def check_cython_extensions(self, extensions) -> None:\n        for ext in extensions:\n            for src in ext.sources:\n                if not os.path.exists(src):\n                    print(f\"{ext.name}: -> [{ext.sources}]\")\n                    raise Exception(\n                        f\"\"\"Cython-generated file '{src}' not found.\n                Cython is required to compile pandas from a development branch.\n                Please install Cython or download a release package of pandas.\n                \"\"\"\n                    )\n\n    def build_extensions(self) -> None:\n        self.check_cython_extensions(self.extensions)\n        build_ext.build_extensions(self)\n\n\nclass CythonCommand(build_ext):\n    \"\"\"\n    Custom command subclassed from Cython.Distutils.build_ext\n    to compile pyx->c, and stop there. All this does is override the\n    C-compile method build_extension() with a no-op.\n    \"\"\"\n\n    def build_extension(self, ext) -> None:\n        pass\n\n\nclass DummyBuildSrc(Command):\n    \"\"\"numpy's build_src command interferes with Cython's build_ext.\"\"\"\n\n    user_options = []\n\n    def initialize_options(self) -> None:\n        self.py_modules_dict = {}\n\n    def finalize_options(self) -> None:\n        pass\n\n    def run(self) -> None:\n        pass\n\n\ncmdclass[\"clean\"] = CleanCommand\ncmdclass[\"build_ext\"] = CheckingBuildExt\n\nif _CYTHON_INSTALLED:\n    suffix = \".pyx\"\n    cmdclass[\"cython\"] = CythonCommand\nelse:\n    suffix = \".c\"\n    cmdclass[\"build_src\"] = DummyBuildSrc\n\n# ----------------------------------------------------------------------\n# Preparation of compiler arguments\n\ndebugging_symbols_requested = \"--with-debugging-symbols\" in sys.argv\nif debugging_symbols_requested:\n    sys.argv.remove(\"--with-debugging-symbols\")\n\n\nif sys.byteorder == \"big\":\n    endian_macro = [(\"__BIG_ENDIAN__\", \"1\")]\nelse:\n    endian_macro = [(\"__LITTLE_ENDIAN__\", \"1\")]\n\n\nextra_compile_args = []\nextra_link_args = []\nif is_platform_windows():\n    if debugging_symbols_requested:\n        extra_compile_args.append(\"/Z7\")\n        extra_link_args.append(\"/DEBUG\")\nelse:\n    # PANDAS_CI=1 is set in CI\n    if os.environ.get(\"PANDAS_CI\", \"0\") == \"1\":\n        extra_compile_args.append(\"-Werror\")\n    if debugging_symbols_requested:\n        extra_compile_args.append(\"-g3\")\n        extra_compile_args.append(\"-UNDEBUG\")\n        extra_compile_args.append(\"-O0\")\n\n# Build for at least macOS 10.9 when compiling on a 10.9 system or above,\n# overriding CPython distuitls behaviour which is to target the version that\n# python was built for. This may be overridden by setting\n# MACOSX_DEPLOYMENT_TARGET before calling setup.py\nif is_platform_mac():\n    if \"MACOSX_DEPLOYMENT_TARGET\" not in os.environ:\n        current_system = platform.mac_ver()[0]\n        python_target = get_config_vars().get(\n            \"MACOSX_DEPLOYMENT_TARGET\", current_system\n        )\n        target_macos_version = \"10.9\"\n        parsed_macos_version = parse_version(target_macos_version)\n        if (\n            parse_version(str(python_target))\n            < parsed_macos_version\n            <= parse_version(current_system)\n        ):\n            os.environ[\"MACOSX_DEPLOYMENT_TARGET\"] = target_macos_version\n\n    if sys.version_info[:2] == (3, 8):  # GH 33239\n        extra_compile_args.append(\"-Wno-error=deprecated-declarations\")\n\n    # https://github.com/pandas-dev/pandas/issues/35559\n    extra_compile_args.append(\"-Wno-error=unreachable-code\")\n\n# enable coverage by building cython files by setting the environment variable\n# \"PANDAS_CYTHON_COVERAGE\" (with a Truthy value) or by running build_ext\n# with `--with-cython-coverage`enabled\nlinetrace = os.environ.get(\"PANDAS_CYTHON_COVERAGE\", False)\nif \"--with-cython-coverage\" in sys.argv:\n    linetrace = True\n    sys.argv.remove(\"--with-cython-coverage\")\n\n# Note: if not using `cythonize`, coverage can be enabled by\n# pinning `ext.cython_directives = directives` to each ext in extensions.\n# github.com/cython/cython/wiki/enhancements-compilerdirectives#in-setuppy\ndirectives = {\"linetrace\": False, \"language_level\": 3, \"always_allow_keywords\": True}\nmacros = []\nif linetrace:\n    # https://pypkg.com/pypi/pytest-cython/f/tests/example-project/setup.py\n    directives[\"linetrace\"] = True\n    macros = [(\"CYTHON_TRACE\", \"1\"), (\"CYTHON_TRACE_NOGIL\", \"1\")]\n\n# silence build warnings about deprecated API usage\n# we can't do anything about these warnings because they stem from\n# cython+numpy version mismatches.\nmacros.append((\"NPY_NO_DEPRECATED_API\", \"0\"))\n\n\n# ----------------------------------------------------------------------\n# Specification of Dependencies\n\n\n# TODO(cython#4518): Need to check to see if e.g. `linetrace` has changed and\n#  possibly re-compile.\ndef maybe_cythonize(extensions, *args, **kwargs):\n    \"\"\"\n    Render tempita templates before calling cythonize. This is skipped for\n\n    * clean\n    * sdist\n    \"\"\"\n    if \"clean\" in sys.argv or \"sdist\" in sys.argv:\n        # See https://github.com/cython/cython/issues/1495\n        return extensions\n\n    elif not _CYTHON_INSTALLED:\n        # GH#28836 raise a helfpul error message\n        if _CYTHON_VERSION:\n            raise RuntimeError(\n                f\"Cannot cythonize with old Cython version ({_CYTHON_VERSION} \"\n                f\"installed, needs {min_cython_ver})\"\n            )\n        raise RuntimeError(\"Cannot cythonize without Cython installed.\")\n\n    # reuse any parallel arguments provided for compilation to cythonize\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--parallel\", \"-j\", type=int, default=1)\n    parsed, _ = parser.parse_known_args()\n\n    kwargs[\"nthreads\"] = parsed.parallel\n    build_ext.render_templates(_pxifiles)\n    if debugging_symbols_requested:\n        kwargs[\"gdb_debug\"] = True\n\n    return cythonize(extensions, *args, **kwargs)\n\n\ndef srcpath(name=None, suffix=\".pyx\", subdir=\"src\"):\n    return pjoin(\"pandas\", subdir, name + suffix)\n\n\nlib_depends = [\"pandas/_libs/include/pandas/parse_helper.h\"]\n\ntseries_depends = [\n    \"pandas/_libs/include/pandas/datetime/pd_datetime.h\",\n]\n\next_data = {\n    \"_libs.algos\": {\n        \"pyxfile\": \"_libs/algos\",\n        \"depends\": _pxi_dep[\"algos\"],\n    },\n    \"_libs.arrays\": {\"pyxfile\": \"_libs/arrays\"},\n    \"_libs.groupby\": {\"pyxfile\": \"_libs/groupby\"},\n    \"_libs.hashing\": {\"pyxfile\": \"_libs/hashing\", \"depends\": []},\n    \"_libs.hashtable\": {\n        \"pyxfile\": \"_libs/hashtable\",\n        \"depends\": (\n            [\n                \"pandas/_libs/include/pandas/vendored/klib/khash_python.h\",\n                \"pandas/_libs/include/pandas/vendored/klib/khash.h\",\n            ]\n            + _pxi_dep[\"hashtable\"]\n        ),\n    },\n    \"_libs.index\": {\n        \"pyxfile\": \"_libs/index\",\n        \"depends\": _pxi_dep[\"index\"],\n    },\n    \"_libs.indexing\": {\"pyxfile\": \"_libs/indexing\"},\n    \"_libs.internals\": {\"pyxfile\": \"_libs/internals\"},\n    \"_libs.interval\": {\n        \"pyxfile\": \"_libs/interval\",\n        \"depends\": _pxi_dep[\"interval\"],\n    },\n    \"_libs.join\": {\"pyxfile\": \"_libs/join\"},\n    \"_libs.lib\": {\n        \"pyxfile\": \"_libs/lib\",\n        \"depends\": lib_depends + tseries_depends,\n    },\n    \"_libs.missing\": {\"pyxfile\": \"_libs/missing\", \"depends\": tseries_depends},\n    \"_libs.parsers\": {\n        \"pyxfile\": \"_libs/parsers\",\n        \"depends\": [\n            \"pandas/_libs/src/parser/tokenizer.h\",\n            \"pandas/_libs/src/parser/io.h\",\n            \"pandas/_libs/src/pd_parser.h\",\n        ],\n    },\n    \"_libs.ops\": {\"pyxfile\": \"_libs/ops\"},\n    \"_libs.ops_dispatch\": {\"pyxfile\": \"_libs/ops_dispatch\"},\n    \"_libs.properties\": {\"pyxfile\": \"_libs/properties\"},\n    \"_libs.reshape\": {\"pyxfile\": \"_libs/reshape\", \"depends\": []},\n    \"_libs.sparse\": {\"pyxfile\": \"_libs/sparse\", \"depends\": _pxi_dep[\"sparse\"]},\n    \"_libs.tslib\": {\n        \"pyxfile\": \"_libs/tslib\",\n        \"depends\": tseries_depends,\n    },\n    \"_libs.tslibs.base\": {\"pyxfile\": \"_libs/tslibs/base\"},\n    \"_libs.tslibs.ccalendar\": {\"pyxfile\": \"_libs/tslibs/ccalendar\"},\n    \"_libs.tslibs.dtypes\": {\"pyxfile\": \"_libs/tslibs/dtypes\"},\n    \"_libs.tslibs.conversion\": {\n        \"pyxfile\": \"_libs/tslibs/conversion\",\n        \"depends\": tseries_depends,\n    },\n    \"_libs.tslibs.fields\": {\n        \"pyxfile\": \"_libs/tslibs/fields\",\n        \"depends\": tseries_depends,\n    },\n    \"_libs.tslibs.nattype\": {\"pyxfile\": \"_libs/tslibs/nattype\"},\n    \"_libs.tslibs.np_datetime\": {\n        \"pyxfile\": \"_libs/tslibs/np_datetime\",\n        \"depends\": tseries_depends,\n    },\n    \"_libs.tslibs.offsets\": {\n        \"pyxfile\": \"_libs/tslibs/offsets\",\n        \"depends\": tseries_depends,\n    },\n    \"_libs.tslibs.parsing\": {\n        \"pyxfile\": \"_libs/tslibs/parsing\",\n        \"sources\": [\"pandas/_libs/src/parser/tokenizer.c\"],\n    },\n    \"_libs.tslibs.period\": {\n        \"pyxfile\": \"_libs/tslibs/period\",\n        \"depends\": tseries_depends,\n    },\n    \"_libs.tslibs.strptime\": {\n        \"pyxfile\": \"_libs/tslibs/strptime\",\n        \"depends\": tseries_depends,\n    },\n    \"_libs.tslibs.timedeltas\": {\n        \"pyxfile\": \"_libs/tslibs/timedeltas\",\n        \"depends\": tseries_depends,\n    },\n    \"_libs.tslibs.timestamps\": {\n        \"pyxfile\": \"_libs/tslibs/timestamps\",\n        \"depends\": tseries_depends,\n    },\n    \"_libs.tslibs.timezones\": {\"pyxfile\": \"_libs/tslibs/timezones\"},\n    \"_libs.tslibs.tzconversion\": {\n        \"pyxfile\": \"_libs/tslibs/tzconversion\",\n        \"depends\": tseries_depends,\n    },\n    \"_libs.tslibs.vectorized\": {\n        \"pyxfile\": \"_libs/tslibs/vectorized\",\n        \"depends\": tseries_depends,\n    },\n    \"_libs.testing\": {\"pyxfile\": \"_libs/testing\"},\n    \"_libs.window.aggregations\": {\n        \"pyxfile\": \"_libs/window/aggregations\",\n        \"language\": \"c++\",\n        \"suffix\": \".cpp\",\n        \"depends\": [\"pandas/_libs/include/pandas/skiplist.h\"],\n    },\n    \"_libs.window.indexers\": {\"pyxfile\": \"_libs/window/indexers\"},\n    \"_libs.writers\": {\"pyxfile\": \"_libs/writers\"},\n    \"_libs.sas\": {\"pyxfile\": \"_libs/sas\"},\n    \"_libs.byteswap\": {\"pyxfile\": \"_libs/byteswap\"},\n}\n\nextensions = []\n\nfor name, data in ext_data.items():\n    source_suffix = suffix if suffix == \".pyx\" else data.get(\"suffix\", \".c\")\n\n    sources = [srcpath(data[\"pyxfile\"], suffix=source_suffix, subdir=\"\")]\n\n    sources.extend(data.get(\"sources\", []))\n\n    include = [\"pandas/_libs/include\", numpy.get_include()]\n\n    undef_macros = []\n\n    if (\n        sys.platform == \"zos\"\n        and data.get(\"language\") == \"c++\"\n        and os.path.basename(os.environ.get(\"CXX\", \"/bin/xlc++\")) in (\"xlc\", \"xlc++\")\n    ):\n        data.get(\"macros\", macros).append((\"__s390__\", \"1\"))\n        extra_compile_args.append(\"-qlanglvl=extended0x:nolibext\")\n        undef_macros.append(\"_POSIX_THREADS\")\n\n    obj = Extension(\n        f\"pandas.{name}\",\n        sources=sources,\n        depends=data.get(\"depends\", []),\n        include_dirs=include,\n        language=data.get(\"language\", \"c\"),\n        define_macros=data.get(\"macros\", macros),\n        extra_compile_args=extra_compile_args,\n        extra_link_args=extra_link_args,\n        undef_macros=undef_macros,\n    )\n\n    extensions.append(obj)\n\n# ----------------------------------------------------------------------\n# ujson\n\nif suffix == \".pyx\":\n    # undo dumb setuptools bug clobbering .pyx sources back to .c\n    for ext in extensions:\n        if ext.sources[0].endswith((\".c\", \".cpp\")):\n            root, _ = os.path.splitext(ext.sources[0])\n            ext.sources[0] = root + suffix\n\nujson_ext = Extension(\n    \"pandas._libs.json\",\n    depends=[\n        \"pandas/_libs/include/pandas/vendored/ujson/lib/ultrajson.h\",\n        \"pandas/_libs/include/pandas/datetime/pd_datetime.h\",\n    ],\n    sources=(\n        [\n            \"pandas/_libs/src/vendored/ujson/python/ujson.c\",\n            \"pandas/_libs/src/vendored/ujson/python/objToJSON.c\",\n            \"pandas/_libs/src/vendored/ujson/python/JSONtoObj.c\",\n            \"pandas/_libs/src/vendored/ujson/lib/ultrajsonenc.c\",\n            \"pandas/_libs/src/vendored/ujson/lib/ultrajsondec.c\",\n        ]\n    ),\n    include_dirs=[\n        \"pandas/_libs/include\",\n        numpy.get_include(),\n    ],\n    extra_compile_args=(extra_compile_args),\n    extra_link_args=extra_link_args,\n    define_macros=macros,\n)\n\n\nextensions.append(ujson_ext)\n\n# ----------------------------------------------------------------------\n\n# ----------------------------------------------------------------------\n# pd_datetime\npd_dt_ext = Extension(\n    \"pandas._libs.pandas_datetime\",\n    depends=[\"pandas/_libs/tslibs/datetime/pd_datetime.h\"],\n    sources=(\n        [\n            \"pandas/_libs/src/vendored/numpy/datetime/np_datetime.c\",\n            \"pandas/_libs/src/vendored/numpy/datetime/np_datetime_strings.c\",\n            \"pandas/_libs/src/datetime/date_conversions.c\",\n            \"pandas/_libs/src/datetime/pd_datetime.c\",\n        ]\n    ),\n    include_dirs=[\n        \"pandas/_libs/include\",\n        numpy.get_include(),\n    ],\n    extra_compile_args=(extra_compile_args),\n    extra_link_args=extra_link_args,\n    define_macros=macros,\n)\n\n\nextensions.append(pd_dt_ext)\n\n# ----------------------------------------------------------------------\n\n# ----------------------------------------------------------------------\n# pd_datetime\npd_parser_ext = Extension(\n    \"pandas._libs.pandas_parser\",\n    depends=[\"pandas/_libs/include/pandas/parser/pd_parser.h\"],\n    sources=(\n        [\n            \"pandas/_libs/src/parser/tokenizer.c\",\n            \"pandas/_libs/src/parser/io.c\",\n            \"pandas/_libs/src/parser/pd_parser.c\",\n        ]\n    ),\n    include_dirs=[\n        \"pandas/_libs/include\",\n    ],\n    extra_compile_args=(extra_compile_args),\n    extra_link_args=extra_link_args,\n    define_macros=macros,\n)\n\n\nextensions.append(pd_parser_ext)\n\n\n# ----------------------------------------------------------------------\n\n\nif __name__ == \"__main__\":\n    # Freeze to support parallel compilation when using spawn instead of fork\n    multiprocessing.freeze_support()\n    setup(\n        version=versioneer.get_version(),\n        ext_modules=maybe_cythonize(extensions, compiler_directives=directives),\n        cmdclass=cmdclass,\n    )\n"
        },
        {
          "name": "tooling",
          "type": "tree",
          "content": null
        },
        {
          "name": "typings",
          "type": "tree",
          "content": null
        },
        {
          "name": "web",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}