{
  "metadata": {
    "timestamp": 1736555000387,
    "page": 82,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjkw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "pytorch/pytorch",
      "stars": 85714,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".bazelignore",
          "type": "blob",
          "size": 0.17,
          "content": "# We do not use this library in our Bazel build. It contains an\n# infinitely recursing symlink that makes Bazel very unhappy.\nthird_party/ittapi/\nthird_party/opentelemetry-cpp\n"
        },
        {
          "name": ".bazelrc",
          "type": "blob",
          "size": 5.53,
          "content": "build --cxxopt=--std=c++17\nbuild --copt=-I.\n# Bazel does not support including its cc_library targets as system\n# headers. We work around this for generated code\n# (e.g. c10/macros/cmake_macros.h) by making the generated directory a\n# system include path.\nbuild --copt=-isystem --copt bazel-out/k8-fastbuild/bin\nbuild --copt=-isystem --copt bazel-out/darwin-fastbuild/bin\nbuild --experimental_ui_max_stdouterr_bytes=2048576\n\n# Configuration to disable tty features for environments like CI\nbuild:no-tty --curses no\nbuild:no-tty --progress_report_interval 10\nbuild:no-tty --show_progress_rate_limit 10\n\n# Build with GPU support by default.\nbuild --define=cuda=true\n# rules_cuda configuration\nbuild --@rules_cuda//cuda:enable_cuda\nbuild --@rules_cuda//cuda:cuda_targets=sm_52\nbuild --@rules_cuda//cuda:compiler=nvcc\nbuild --repo_env=CUDA_PATH=/usr/local/cuda\n\n# Configuration to build without GPU support\nbuild:cpu-only --define=cuda=false\n# define a separate build folder for faster switching between configs\nbuild:cpu-only --platform_suffix=-cpu-only\n# See the note on the config-less build for details about why we are\n# doing this. We must also do it for the \"-cpu-only\" platform suffix.\nbuild --copt=-isystem --copt=bazel-out/k8-fastbuild-cpu-only/bin\n# rules_cuda configuration\nbuild:cpu-only --@rules_cuda//cuda:enable_cuda=False\n\n# Definition of --config=shell\n# interactive shell immediately before execution\nbuild:shell --run_under=\"//tools/bazel_tools:shellwrap\"\n\n# Disable all warnings for external repositories. We don't care about\n# their warnings.\nbuild --per_file_copt=^external/@-w\n\n# Set additional warnings to error level.\n#\n# Implementation notes:\n#  * we use file extensions to determine if we are using the C++\n#    compiler or the cuda compiler\n#  * we use ^// at the start of the regex to only permit matching\n#    PyTorch files. This excludes external repos.\n#\n# Note that because this is logically a command-line flag, it is\n# considered the word on what warnings are enabled. This has the\n# unfortunate consequence of preventing us from disabling an error at\n# the target level because those flags will come before these flags in\n# the action invocation. Instead we provide per-file exceptions after\n# this.\n#\n# On the bright side, this means we don't have to more broadly apply\n# the exceptions to an entire target.\n#\n# Looking for CUDA flags? We have a cu_library macro that we can edit\n# directly. Look in //tools/rules:cu.bzl for details. Editing the\n# macro over this has the following advantages:\n#  * making changes does not require discarding the Bazel analysis\n#    cache\n#  * it allows for selective overrides on individual targets since the\n#    macro-level opts will come earlier than target level overrides\n\nbuild --per_file_copt='^//.*\\.(cpp|cc)$'@-Werror=all\n# The following warnings come from -Wall. We downgrade them from error\n# to warnings here.\n#\n# We intentionally use #pragma unroll, which is compiler specific.\nbuild --per_file_copt='^//.*\\.(cpp|cc)$'@-Wno-error=unknown-pragmas\n\nbuild --per_file_copt='^//.*\\.(cpp|cc)$'@-Werror=extra\n# The following warnings come from -Wextra. We downgrade them from error\n# to warnings here.\n#\n# unused-parameter-compare has a tremendous amount of violations in the\n# codebase. It will be a lot of work to fix them, just disable it for\n# now.\nbuild --per_file_copt='^//.*\\.(cpp|cc)$'@-Wno-unused-parameter\n# missing-field-parameters has both a large number of violations in\n# the codebase, but it also is used pervasively in the Python C\n# API. There are a couple of catches though:\n# * we use multiple versions of the Python API and hence have\n#   potentially multiple different versions of each relevant\n#   struct. They may have different numbers of fields. It will be\n#   unwieldy to support multiple versions in the same source file.\n# * Python itself for many of these structs recommends only\n#   initializing a subset of the fields. We should respect the API\n#   usage conventions of our dependencies.\n#\n# Hence, we just disable this warning altogether. We may want to clean\n# up some of the clear-cut cases that could be risky, but we still\n# likely want to have this disabled for the most part.\nbuild --per_file_copt='^//.*\\.(cpp|cc)$'@-Wno-missing-field-initializers\n\nbuild --per_file_copt='^//.*\\.(cpp|cc)$'@-Wno-unused-function\nbuild --per_file_copt='^//.*\\.(cpp|cc)$'@-Wno-unused-variable\n\nbuild --per_file_copt='//:aten/src/ATen/RegisterCompositeExplicitAutograd\\.cpp$'@-Wno-error=unused-function\nbuild --per_file_copt='//:aten/src/ATen/RegisterCompositeImplicitAutograd\\.cpp$'@-Wno-error=unused-function\nbuild --per_file_copt='//:aten/src/ATen/RegisterMkldnnCPU\\.cpp$'@-Wno-error=unused-function\nbuild --per_file_copt='//:aten/src/ATen/RegisterNestedTensorCPU\\.cpp$'@-Wno-error=unused-function\nbuild --per_file_copt='//:aten/src/ATen/RegisterQuantizedCPU\\.cpp$'@-Wno-error=unused-function\nbuild --per_file_copt='//:aten/src/ATen/RegisterSparseCPU\\.cpp$'@-Wno-error=unused-function\nbuild --per_file_copt='//:aten/src/ATen/RegisterSparseCsrCPU\\.cpp$'@-Wno-error=unused-function\nbuild --per_file_copt='//:aten/src/ATen/RegisterNestedTensorMeta\\.cpp$'@-Wno-error=unused-function\nbuild --per_file_copt='//:aten/src/ATen/RegisterSparseMeta\\.cpp$'@-Wno-error=unused-function\nbuild --per_file_copt='//:aten/src/ATen/RegisterQuantizedMeta\\.cpp$'@-Wno-error=unused-function\nbuild --per_file_copt='//:aten/src/ATen/RegisterZeroTensor\\.cpp$'@-Wno-error=unused-function\nbuild --per_file_copt='//:torch/csrc/lazy/generated/RegisterAutogradLazy\\.cpp$'@-Wno-error=unused-function\nbuild --per_file_copt='//:torch/csrc/lazy/generated/RegisterLazy\\.cpp$'@-Wno-error=unused-function\n"
        },
        {
          "name": ".bazelversion",
          "type": "blob",
          "size": 0.01,
          "content": "6.5.0\n"
        },
        {
          "name": ".ci",
          "type": "tree",
          "content": null
        },
        {
          "name": ".circleci",
          "type": "tree",
          "content": null
        },
        {
          "name": ".clang-format",
          "type": "blob",
          "size": 3.34,
          "content": "---\nAccessModifierOffset: -1\nAlignAfterOpenBracket: AlwaysBreak\nAlignConsecutiveAssignments: false\nAlignConsecutiveDeclarations: false\nAlignEscapedNewlinesLeft: true\nAlignOperands:   false\nAlignTrailingComments: false\nAllowAllParametersOfDeclarationOnNextLine: false\nAllowShortBlocksOnASingleLine: false\nAllowShortCaseLabelsOnASingleLine: false\nAllowShortFunctionsOnASingleLine: Empty\nAllowShortIfStatementsOnASingleLine: false\nAllowShortLoopsOnASingleLine: false\nAlwaysBreakAfterReturnType: None\nAlwaysBreakBeforeMultilineStrings: true\nAlwaysBreakTemplateDeclarations: true\nBinPackArguments: false\nBinPackParameters: false\nBraceWrapping:\n  AfterClass:      false\n  AfterControlStatement: false\n  AfterEnum:       false\n  AfterFunction:   false\n  AfterNamespace:  false\n  AfterObjCDeclaration: false\n  AfterStruct:     false\n  AfterUnion:      false\n  BeforeCatch:     false\n  BeforeElse:      false\n  IndentBraces:    false\nBreakBeforeBinaryOperators: None\nBreakBeforeBraces: Attach\nBreakBeforeTernaryOperators: true\nBreakConstructorInitializersBeforeComma: false\nBreakAfterJavaFieldAnnotations: false\nBreakStringLiterals: false\nColumnLimit:     80\nCommentPragmas:  '^ IWYU pragma:'\nCompactNamespaces: false\nConstructorInitializerAllOnOneLineOrOnePerLine: true\nConstructorInitializerIndentWidth: 4\nContinuationIndentWidth: 4\nCpp11BracedListStyle: true\nDerivePointerAlignment: false\nDisableFormat:   false\nForEachMacros:\n  - FOR_EACH_RANGE\n  - FOR_EACH\nIncludeCategories:\n  - Regex:           '^<.*\\.h(pp)?>'\n    Priority:        1\n  - Regex:           '^<.*'\n    Priority:        2\n  - Regex:           '.*'\n    Priority:        3\nIndentCaseLabels: true\nIndentWidth:     2\nIndentWrappedFunctionNames: false\nKeepEmptyLinesAtTheStartOfBlocks: false\nMacroBlockBegin: ''\nMacroBlockEnd:   ''\nMacros:\n  - >-\n    PyObject_HEAD_INIT(type)={\n        /* this is not exactly match with PyObject_HEAD_INIT in Python source code\n         * but it is enough for clang-format */\n        { 0xFFFFFFFF },\n        (type)\n    },\n  - >-\n    PyVarObject_HEAD_INIT(type, size)={\n        {\n            /* manually expand PyObject_HEAD_INIT(type) above\n             * because clang-format do not support recursive expansion */\n            { 0xFFFFFFFF },\n            (type)\n        },\n        (size)\n    },\nMaxEmptyLinesToKeep: 1\nNamespaceIndentation: None\nPenaltyBreakBeforeFirstCallParameter: 1\nPenaltyBreakComment: 300\nPenaltyBreakFirstLessLess: 120\nPenaltyBreakString: 1000\nPenaltyExcessCharacter: 1000000\nPenaltyReturnTypeOnItsOwnLine: 2000000\nPointerAlignment: Left\nReflowComments:  true\nSortIncludes:    true\nSpaceAfterCStyleCast: false\nSpaceBeforeAssignmentOperators: true\nSpaceBeforeParens: ControlStatements\nSpaceInEmptyParentheses: false\nSpacesBeforeTrailingComments: 1\nSpacesInAngles:  false\nSpacesInContainerLiterals: true\nSpacesInCStyleCastParentheses: false\nSpacesInParentheses: false\nSpacesInSquareBrackets: false\nStandard:        c++17\nStatementMacros:\n  - C10_DEFINE_bool\n  - C10_DEFINE_int\n  - C10_DEFINE_int32\n  - C10_DEFINE_int64\n  - C10_DEFINE_string\n  - C10_DEFINE_REGISTRY_WITHOUT_WARNING\n  - C10_REGISTER_CREATOR\n  - DEFINE_BINARY\n  - PyObject_HEAD\n  - PyObject_VAR_HEAD\n  - PyException_HEAD\n  - TORCH_DECLARE_bool\n\nTabWidth:        8\nUseTab:          Never\n---\nLanguage: ObjC\nColumnLimit: 120\nAlignAfterOpenBracket: Align\nObjCBlockIndentWidth: 2\nObjCSpaceAfterProperty: false\nObjCSpaceBeforeProtocolList: false\n...\n"
        },
        {
          "name": ".clang-tidy",
          "type": "blob",
          "size": 2.57,
          "content": "---\n# NOTE there must be no spaces before the '-', so put the comma last.\n# The check bugprone-unchecked-optional-access is also turned on.\n# Note that it can cause clang-tidy to hang randomly. The tracking issue\n# can be found at https://github.com/llvm/llvm-project/issues/69369.\n# When that happens, we can disable it on the problematic code by NOLINT.\nInheritParentConfig: true\nChecks: '\nbugprone-*,\n-bugprone-easily-swappable-parameters,\n-bugprone-forward-declaration-namespace,\n-bugprone-macro-parentheses,\n-bugprone-lambda-function-name,\n-bugprone-reserved-identifier,\n-bugprone-swapped-arguments,\nclang-analyzer-core.*,\nclang-analyzer-cplusplus.*,\nclang-analyzer-nullability.*,\nclang-analyzer-deadcode.*,\nclang-diagnostic-missing-prototypes,\ncppcoreguidelines-*,\n-cppcoreguidelines-avoid-do-while,\n-cppcoreguidelines-avoid-magic-numbers,\n-cppcoreguidelines-avoid-non-const-global-variables,\n-cppcoreguidelines-interfaces-global-init,\n-cppcoreguidelines-macro-usage,\n-cppcoreguidelines-owning-memory,\n-cppcoreguidelines-pro-bounds-array-to-pointer-decay,\n-cppcoreguidelines-pro-bounds-constant-array-index,\n-cppcoreguidelines-pro-bounds-pointer-arithmetic,\n-cppcoreguidelines-pro-type-cstyle-cast,\n-cppcoreguidelines-pro-type-reinterpret-cast,\n-cppcoreguidelines-pro-type-static-cast-downcast,\n-cppcoreguidelines-pro-type-union-access,\n-cppcoreguidelines-pro-type-vararg,\n-cppcoreguidelines-non-private-member-variables-in-classes,\n-facebook-hte-RelativeInclude,\nhicpp-exception-baseclass,\nhicpp-avoid-goto,\nmisc-*,\n-misc-confusable-identifiers,\n-misc-const-correctness,\n-misc-include-cleaner,\n-misc-use-anonymous-namespace,\n-misc-unused-parameters,\n-misc-no-recursion,\n-misc-non-private-member-variables-in-classes,\n-misc-unused-using-decls,\nmodernize-*,\n-modernize-macro-to-enum,\n-modernize-return-braced-init-list,\n-modernize-use-auto,\n-modernize-use-default-member-init,\n-modernize-use-using,\n-modernize-use-trailing-return-type,\n-modernize-use-nodiscard,\nperformance-*,\nreadability-container-size-empty,\nreadability-delete-null-pointer,\nreadability-duplicate-include\nreadability-misplaced-array-index,\nreadability-redundant*\nreadability-simplify-subscript-expr,\nreadability-string-compare,\n-readability-redundant-access-specifiers,\n-readability-redundant-control-flow,\n'\nHeaderFilterRegex: '^(aten/|c10/|torch/).*$'\nWarningsAsErrors: '*'\nCheckOptions:\n  cppcoreguidelines-special-member-functions.AllowSoleDefaultDtor: true\n  cppcoreguidelines-special-member-functions.AllowImplicitlyDeletedCopyOrMove: true\n  misc-header-include-cycle.IgnoredFilesList: 'format.h;ivalue.h;custom_class.h;Dict.h;List.h;IListRef.h'\n...\n"
        },
        {
          "name": ".cmakelintrc",
          "type": "blob",
          "size": 0.24,
          "content": "filter=-convention/filename,-linelength,-package/consistency,-readability/logic,-readability/mixedcase,-readability/wonkycase,-syntax,-whitespace/eol,+whitespace/extra,-whitespace/indent,-whitespace/mismatch,-whitespace/newline,-whitespace/tabs\n"
        },
        {
          "name": ".coveragerc",
          "type": "blob",
          "size": 0.18,
          "content": "[run]\nplugins =\n    coverage_plugins.jit_plugin\nomit =\n    */tmp*\n    */Temp/*\n    */usr/local/lib*\n    *test/*\n\n[report]\nomit =\n    */tmp*\n    */Temp/*\n    */usr/local/lib*\n    *test/*\n"
        },
        {
          "name": ".ctags.d",
          "type": "tree",
          "content": null
        },
        {
          "name": ".devcontainer",
          "type": "tree",
          "content": null
        },
        {
          "name": ".dockerignore",
          "type": "blob",
          "size": 0.01,
          "content": ".gitignore"
        },
        {
          "name": ".flake8",
          "type": "blob",
          "size": 3.11,
          "content": "[flake8]\n# NOTE: **Mirror any changes** to this file the [tool.ruff] config in pyproject.toml\n# before we can fully move to use ruff\nenable-extensions = G\nselect = B,C,E,F,G,P,SIM1,SIM911,T4,W,B9,TOR0,TOR1,TOR2,TOR9\nmax-line-length = 120\n# C408 ignored because we like the dict keyword argument syntax\n# E501 is not flexible enough, we're using B950 instead\nignore =\n    E203,E305,E402,E501,E704,E721,E741,F405,F841,F999,W503,W504,C408,E302,W291,E303,\n    # shebang has extra meaning in fbcode lints, so I think it's not worth trying\n    # to line this up with executable bit\n    EXE001,\n    # these ignores are from flake8-bugbear; please fix!\n    B007,B008,B017,B019,B023,B028,B903,B904,B905,B906,B907\n    # these ignores are from flake8-comprehensions; please fix!\n    C407,\n    # these ignores are from flake8-logging-format; please fix!\n    G100,G101,G200\n    # these ignores are from flake8-simplify. please fix or ignore with commented reason\n    SIM105,SIM108,SIM110,SIM111,SIM113,SIM114,SIM115,SIM116,SIM117,SIM118,SIM119,SIM12,\n    # flake8-simplify code styles\n    SIM102,SIM103,SIM106,SIM112,\n    # TorchFix codes that don't make sense for PyTorch itself:\n    # removed and deprecated PyTorch functions.\n    TOR001,TOR101,\n    # TODO(kit1980): fix all TOR102 issues\n    # `torch.load` without `weights_only` parameter is unsafe\n    TOR102,\n    # TODO(kit1980): resolve all TOR003 issues\n    # pass `use_reentrant` explicitly to `checkpoint`.\n    TOR003\nper-file-ignores =\n    __init__.py: F401\n    test/**: F821\n    test/**/__init__.py: F401,F821\n    torch/utils/cpp_extension.py: B950\n    torchgen/api/types/__init__.py: F401,F403\n    torchgen/executorch/api/types/__init__.py: F401,F403\n    test/dynamo/test_higher_order_ops.py: B950\n    torch/testing/_internal/dynamo_test_failures.py: B950\n    # TOR901 is only for test, we want to ignore it for everything else.\n    # It's not easy to configure this without affecting other per-file-ignores,\n    # so we explicitly list every file where it's violated outside of test.\n    torch/__init__.py: F401,TOR901\n    torch/_custom_op/impl.py: TOR901\n    torch/_export/serde/upgrade.py: TOR901\n    torch/_functorch/vmap.py: TOR901\n    torch/_inductor/test_operators.py: TOR901\n    torch/_library/abstract_impl.py: TOR901\n    torch/_meta_registrations.py: TOR901\n    torch/_prims/__init__.py: F401,TOR901\n    torch/_prims/rng_prims.py: TOR901\n    torch/ao/quantization/fx/_decomposed.py: TOR901\n    torch/distributed/_functional_collectives.py: TOR901\n    torch/distributed/_spmd/data_parallel.py: TOR901\n    torch/distributed/_tensor/_collective_utils.py: TOR901\n    # This is a full package that happen to live within the test\n    # folder, so ok to skip\n    test/cpp_extensions/open_registration_extension/pytorch_openreg/_aten_impl.py: TOR901\noptional-ascii-coding = True\nexclude =\n    ./.git,\n    ./build_test_custom_build,\n    ./build,\n    ./caffe2,\n    ./docs/caffe2,\n    ./docs/cpp/src,\n    ./docs/src,\n    ./functorch/docs,\n    ./functorch/examples,\n    ./functorch/notebooks,\n    ./scripts,\n    ./test/generated_type_hints_smoketest.py,\n    ./third_party,\n    ./torch/include,\n    ./torch/lib,\n    ./venv,\n    *.pyi\n"
        },
        {
          "name": ".gdbinit",
          "type": "blob",
          "size": 0.64,
          "content": "# automatically load the pytoch-gdb extension.\n#\n# gdb automatically tries to load this file whenever it is executed from the\n# root of the pytorch repo, but by default it is not allowed to do so due to\n# security reasons. If you want to use pytorch-gdb, please add the following\n# line to your ~/.gdbinit (i.e., the .gdbinit file which is in your home\n# directory, NOT this file):\n#    add-auto-load-safe-path /path/to/pytorch/.gdbinit\n#\n# Alternatively, you can manually load the pytorch-gdb commands into your\n# existing gdb session by doing the following:\n#    (gdb) source /path/to/pytorch/tools/gdb/pytorch-gdb.py\n\nsource tools/gdb/pytorch-gdb.py\n"
        },
        {
          "name": ".git-blame-ignore-revs",
          "type": "blob",
          "size": 2.08,
          "content": "# 2020-11-12 Enabled ShellCheck on `.jenkins/pytorch`\n65d5004b09fd8d5deac173a3aaa259f46eaa0d67\n# 2021-01-20 Replaced `   ` with `...` in many doctests\nc147aa306c6386a753fdff24b48d04e803070a63\n# 2021-03-05 Removed all trailing whitespace\n8c798e062216278673a75bac0848ea69a8bd3f03\n# 2021-03-30 Normalized trailing newlines\n5bcbbf537327f6e8328289c25a3a453a2444d984\n# 2021-03-31 Autogenerated Markdown ToCs\na74b10def961ab090385f291ee06e66db99c1a2f\n# 2021-04-02 Enabled more ShellCheck warnings\n09670c7d43b9abce862a6bf71d8cc89e64764bdb\n# 2021-04-08 Removed all non-breaking spaces\ncc11aaaa60aadf28e3ec278bce26a42c1cd68a4f\n# 2021-04-13 Expanded many wildcard imports\n4753100a3baa96273204c361c8452afb7b59836f\n# 2021-04-19 Removed all unqualified `noqa`\ne3900d2ba5c9f91a24a9ce34520794c8366d5c54\n# 2021-04-21 Removed all unqualified `type: ignore`\n75024e228ca441290b6a1c2e564300ad507d7af6\n# 2021-04-30 [PyTorch] Autoformat c10\n44cc873fba5e5ffc4d4d4eef3bd370b653ce1ce1\n# 2021-05-14 Removed all versionless Python shebangs\n2e26976ad3b06ce95dd6afccfdbe124802edf28f\n# 2021-06-07 Strictly typed everything in `.github` and `tools`\n737d920b21db9b4292d056ee1329945990656304\n# 2022-06-09 Apply clang-format to ATen headers\n95b15c266baaf989ef7b6bbd7c23a2d90bacf687\n# 2022-06-11 [lint] autoformat test/cpp and torch/csrc\n30fb2c4abaaaa966999eab11674f25b18460e609\n# 2023-06-06 clang-format on Foreach / Multi-Tensor-Apply\n515c4279416f13fcc3c898e560f8ae8f15139a03\n# 2023-07-25 Apply UFMT to all files in benchmarks\ndd3a77bc965adf9fe8ba582ee13bb7f14c9661b0\n# 2023-07-26 Enable UFMT on a bunch of low traffic Python files outside of main files\nf70844bec783bfce43c950ccf180dc494e86f2bf\n# 2023-07-28 Apply UFMT to all non test/torch files\ne6ec0efaf87703c5f889cfc20b29be455885d58d\n# 2023-07-31 [optim][BE] split test file into logical parts: SWA, LR, optim\na53cda1ddc15336dc1ff0ce1eff2a49cdc5f882e\n# 2024-01-02 clangformat: fused adam #116583\n9dc68d1aa9e554d09344a10fff69f7b50b2d23a0\n# 2024-06-28 enable UFMT in `torch/storage.py`\nd80939e5e9337e8078f11489afefec59fd42f93b\n# 2024-06-28 enable UFMT in `torch.utils.data`\n7cf0b90e49689d45be91aa539fdf54cf2ea8a9a3\n"
        },
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0.4,
          "content": "*.bat text eol=crlf\n.circleci/config.yml linguist-generated=true\n.github/workflows/generated-*.yml linguist-generated=true\n.github/generated-* linguist-generated=true\n.github/scripts/gql_mocks.json linguist-generated=true\nthird_party/LICENSES_BUNDLED.txt linguist-generated=true\ntools/build/bazel/requirements.txt linguist-generated=true\ntorch/csrc/utils/generated_serialization_types.h linguist-generated=true\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 5.97,
          "content": "# READ THIS BEFORE YOU REFACTOR ME\n#\n# setup.py uses the list of patterns in this file to decide\n# what to delete, but it's not 100% sound.  So, for example,\n# if you delete aten/build/ because it's redundant with build/,\n# aten/build/ will stop being cleaned.  So be careful when\n# refactoring this file!\n\n## PyTorch\n\n.coverage\ncoverage.xml\n.dmypy.json\n.gradle\n.hypothesis\n.mypy_cache\n.additional_ci_files\n.lintrunner.private.toml\n/.extracted_scripts/\n**/.pytorch_specified_test_cases.csv\n**/.pytorch-disabled-tests.json\n*/*.pyc\n*/*.so*\n*/**/__pycache__\n*/**/*.dylib*\n*/**/*.pyc\n*/**/*.pyd\n*/**/*.so*\n*/**/**/*.pyc\n*/**/**/**/*.pyc\n*/**/**/**/**/*.pyc\naten/build/\naten/src/ATen/Config.h\naten/src/ATen/cuda/CUDAConfig.h\nbenchmarks/.data\ncaffe2/cpp_test/\ndist/\ndocs/build/\ndocs/cpp/src\ndocs/src/**/*\ndocs/cpp/build\ndocs/cpp/source/api\ndocs/cpp/source/html/\ndocs/cpp/source/latex/\ndocs/source/compile/generated/\ndocs/source/generated/\ndocs/source/compile/generated/\nlog\nusage_log.txt\ntest-reports/\ntest/*.bak\ntest/**/*.bak\ntest/.coverage\ntest/.hypothesis/\ntest/cpp/api/mnist\ntest/custom_operator/model.pt\ntest/debug/\ntest/jit_hooks/*.pt\ntest/data/legacy_modules.t7\ntest/data/*.pt\ntest/forward_backward_compatibility/nightly_schemas.txt\ndropout_model.pt\ntest/generated_type_hints_smoketest.py\ntest/htmlcov\ntest/cpp_extensions/install/\nthird_party/build/\ntools/coverage_plugins_package/pip-wheel-metadata/\ntools/shared/_utils_internal.py\ntools/fast_nvcc/wrap_nvcc.sh\ntools/fast_nvcc/wrap_nvcc.bat\ntools/fast_nvcc/tmp/\ntorch.egg-info/\ntorch/_C/__init__.pyi\ntorch/_C/_nn.pyi\ntorch/_C/_VariableFunctions.pyi\ntorch/_VF.pyi\ntorch/return_types.pyi\ntorch/nn/functional.pyi\ntorch/utils/data/datapipes/datapipe.pyi\ntorch/csrc/autograd/generated/*\ntorch/csrc/lazy/generated/*.[!m]*\ntorch_compile_debug/\n# Listed manually because some files in this directory are not generated\ntorch/testing/_internal/generated/annotated_fn_args.py\ntorch/testing/_internal/data/*.pt\ntorch/csrc/api/include/torch/version.h\ntorch/csrc/cudnn/cuDNN.cpp\ntorch/csrc/generated\ntorch/csrc/generic/TensorMethods.cpp\ntorch/csrc/inductor/aoti_torch/generated/*.cpp\ntorch/csrc/inductor/aoti_torch/generated/extend/*\ntorch/csrc/jit/generated/*\ntorch/csrc/jit/fuser/config.h\ntorch/csrc/nn/THCUNN.cpp\ntorch/csrc/nn/THCUNN.cwrap\ntorch/bin/\ntorch/cmake/\ntorch/lib/*.a*\ntorch/lib/*.dll*\ntorch/lib/*.exe*\ntorch/lib/*.dylib*\ntorch/lib/*.h\ntorch/lib/*.lib\ntorch/lib/*.pdb\ntorch/lib/*.so*\ntorch/lib/protobuf*.pc\ntorch/lib/build\ntorch/lib/caffe2/\ntorch/lib/cmake\ntorch/lib/include\ntorch/lib/pkgconfig\ntorch/lib/protoc\ntorch/lib/protobuf/\ntorch/lib/tmp_install\ntorch/lib/torch_shm_manager\ntorch/lib/site-packages/\ntorch/lib/python*\ntorch/lib64\ntorch/include/\ntorch/share/\ntorch/test/\ntorch/utils/benchmark/utils/valgrind_wrapper/callgrind.h\ntorch/utils/benchmark/utils/valgrind_wrapper/valgrind.h\ntorch/version.py\nminifier_launcher.py\n# Root level file used in CI to specify certain env configs.\n# E.g., see .circleci/config.yaml\nenv\n.circleci/scripts/COMMIT_MSG\nscripts/release_notes/*.json\nsccache-stats*.json\nlint.json\nmerge_record.json\n\n# These files get copied over on invoking setup.py\ntorchgen/packaged/*\n!torchgen/packaged/README.md\n\n# IPython notebook checkpoints\n.ipynb_checkpoints\n\n# Editor temporaries\n*.swa\n*.swb\n*.swc\n*.swd\n*.swe\n*.swf\n*.swg\n*.swh\n*.swi\n*.swj\n*.swk\n*.swl\n*.swm\n*.swn\n*.swo\n*.swp\n*~\n.~lock.*\n\n# macOS dir files\n.DS_Store\n\n# Ninja files\n.ninja_deps\n.ninja_log\ncompile_commands.json\n*.egg-info/\ndocs/source/scripts/activation_images/\ndocs/source/scripts/quantization_backend_configs/\n\n## General\n\n# Compiled Object files\n*.slo\n*.lo\n*.o\n*.cuo\n*.obj\n\n# Compiled Dynamic libraries\n*.so\n*.dylib\n*.dll\n\n# Compiled Static libraries\n*.lai\n*.la\n*.a\n*.lib\n\n# Compiled protocol buffers\n*.pb.h\n*.pb.cc\n*_pb2.py\n\n# Compiled python\n*.pyc\n*.pyd\n\n# Compiled MATLAB\n*.mex*\n\n# IPython notebook checkpoints\n.ipynb_checkpoints\n\n# Editor temporaries\n*.swn\n*.swo\n*.swp\n*~\n\n# NFS handle files\n**/.nfs*\n\n# Sublime Text settings\n*.sublime-workspace\n*.sublime-project\n\n# Eclipse Project settings\n*.*project\n.settings\n\n# QtCreator files\n*.user\n\n# PyCharm files\n.idea\n\n# GDB history\n.gdb_history\n\n## Caffe2\n\n# build, distribute, and bins (+ python proto bindings)\nbuild/\n# Allow tools/build/ for build support.\n!tools/build/\nbuild_host_protoc\nbuild_android\nbuild_ios\n.build_debug/*\n.build_release/*\n.build_profile/*\ndistribute/*\n*.testbin\n*.bin\ncmake_build\n.cmake_build\ngen\n.setuptools-cmake-build\n.pytest_cache\naten/build/*\n\n# Bram\nplsdontbreak\n\n# Generated documentation\ndocs/_site\ndocs/gathered\n_site\ndoxygen\ndocs/dev\n\n# LevelDB files\n*.sst\n*.ldb\nLOCK\nCURRENT\nMANIFEST-*\n\n# generated version file\ncaffe2/version.py\n\n# setup.py intermediates\n.eggs\ncaffe2.egg-info\nMANIFEST\n\n# Atom/Watchman required file\n.watchmanconfig\n.watchman\n\n# Files generated by CLion\ncmake-build-debug\n\n# BEGIN NOT-CLEAN-FILES (setup.py handles this marker. Do not change.)\n#\n# Below files are not deleted by \"setup.py clean\".\n\n# Downloaded bazel\ntools/bazel\n\n# Visual Studio Code files\n.vs\n/.vscode/*\n!/.vscode/extensions.json\n!/.vscode/settings_recommended.json\n\n# YouCompleteMe config file\n.ycm_extra_conf.py\n\n# Files generated when a patch is rejected\n*.orig\n*.rej\n\n# Files generated by ctags\nCTAGS\nGTAGS\nGRTAGS\nGSYMS\nGPATH\ntags\nTAGS\n\n\n# ccls file\n.ccls-cache/\n\n# clang tooling storage location\n.clang-format-bin\n.clang-tidy-bin\n.lintbin\n\n# clangd background index\n.clangd/\n.cache/\n\n# bazel symlinks\nbazel-*\n\n# xla repo\nxla/\n\n# direnv, posh-direnv\n.env\n.envrc\n.psenvrc\n\n# generated shellcheck directories\n.shellcheck_generated*/\n\n# zip archives\n*.zip\n\n# core dump files\n**/core.[1-9]*\n\n# Generated if you use the pre-commit script for clang-tidy\npr.diff\n\n# coverage files\n*/**/.coverage.*\n\n# buck generated files\n.buckd/\n.lsp-buck-out/\n.lsp.buckd/\nbuck-out/\n\n# Downloaded libraries\nthird_party/ruy/\nthird_party/glog/\n\n# Virtualenv\nvenv/\n\n# Log files\n*.log\nsweep/\n\n# Android build artifacts\nandroid/pytorch_android/.cxx\nandroid/pytorch_android_torchvision/.cxx\n\n# Pyre configs (for internal usage)\n.pyre_configuration\n.pyre_configuration.codenav\n.arcconfig\n.stable_pyre_client\n.pyre_client\n"
        },
        {
          "name": ".gitmodules",
          "type": "blob",
          "size": 4.72,
          "content": "[submodule \"third_party/pybind11\"]\n    ignore = dirty\n    path = third_party/pybind11\n    url = https://github.com/pybind/pybind11.git\n[submodule \"third_party/eigen\"]\n    ignore = dirty\n    path = third_party/eigen\n    url = https://gitlab.com/libeigen/eigen.git\n[submodule \"third_party/googletest\"]\n    ignore = dirty\n    path = third_party/googletest\n    url = https://github.com/google/googletest.git\n[submodule \"third_party/benchmark\"]\n    ignore = dirty\n    path = third_party/benchmark\n    url = https://github.com/google/benchmark.git\n[submodule \"third_party/protobuf\"]\n    ignore = dirty\n    path = third_party/protobuf\n    url = https://github.com/protocolbuffers/protobuf.git\n[submodule \"third_party/NNPACK\"]\n    ignore = dirty\n    path = third_party/NNPACK\n    url = https://github.com/Maratyszcza/NNPACK.git\n[submodule \"third_party/gloo\"]\n    ignore = dirty\n    path = third_party/gloo\n    url = https://github.com/facebookincubator/gloo\n[submodule \"third_party/NNPACK_deps/pthreadpool\"]\n    ignore = dirty\n    path = third_party/pthreadpool\n    url = https://github.com/Maratyszcza/pthreadpool.git\n[submodule \"third_party/NNPACK_deps/FXdiv\"]\n    ignore = dirty\n    path = third_party/FXdiv\n    url = https://github.com/Maratyszcza/FXdiv.git\n[submodule \"third_party/NNPACK_deps/FP16\"]\n    ignore = dirty\n    path = third_party/FP16\n    url = https://github.com/Maratyszcza/FP16.git\n[submodule \"third_party/NNPACK_deps/psimd\"]\n    ignore = dirty\n    path = third_party/psimd\n    url = https://github.com/Maratyszcza/psimd.git\n[submodule \"third_party/cpuinfo\"]\n    ignore = dirty\n    path = third_party/cpuinfo\n    url = https://github.com/pytorch/cpuinfo.git\n[submodule \"third_party/python-peachpy\"]\n    ignore = dirty\n    path = third_party/python-peachpy\n    url = https://github.com/malfet/PeachPy.git\n[submodule \"third_party/onnx\"]\n    ignore = dirty\n    path = third_party/onnx\n    url = https://github.com/onnx/onnx.git\n[submodule \"third_party/sleef\"]\n    ignore = dirty\n    path = third_party/sleef\n    url = https://github.com/shibatch/sleef\n[submodule \"third_party/ideep\"]\n    ignore = dirty\n    path = third_party/ideep\n    url = https://github.com/intel/ideep\n[submodule \"third_party/nccl/nccl\"]\n    ignore = dirty\n    path = third_party/nccl/nccl\n    url = https://github.com/NVIDIA/nccl\n[submodule \"third_party/gemmlowp/gemmlowp\"]\n    ignore = dirty\n    path = third_party/gemmlowp/gemmlowp\n    url = https://github.com/google/gemmlowp.git\n[submodule \"third_party/fbgemm\"]\n    ignore = dirty\n    path = third_party/fbgemm\n    url = https://github.com/pytorch/fbgemm\n[submodule \"android/libs/fbjni\"]\n    ignore = dirty\n    path = android/libs/fbjni\n    url = https://github.com/facebookincubator/fbjni.git\n[submodule \"third_party/XNNPACK\"]\n    ignore = dirty\n    path = third_party/XNNPACK\n    url = https://github.com/google/XNNPACK.git\n[submodule \"third_party/fmt\"]\n    ignore = dirty\n    path = third_party/fmt\n    url = https://github.com/fmtlib/fmt.git\n[submodule \"third_party/tensorpipe\"]\n    ignore = dirty\n    path = third_party/tensorpipe\n    url = https://github.com/pytorch/tensorpipe.git\n[submodule \"third_party/cudnn_frontend\"]\n\tpath = third_party/cudnn_frontend\n\turl = https://github.com/NVIDIA/cudnn-frontend.git\n[submodule \"third_party/kineto\"]\n    path = third_party/kineto\n    url = https://github.com/pytorch/kineto\n[submodule \"third_party/pocketfft\"]\n\tpath = third_party/pocketfft\n\turl = https://github.com/mreineck/pocketfft\n[submodule \"third_party/ittapi\"]\n\tpath = third_party/ittapi\n\turl = https://github.com/intel/ittapi.git\n[submodule \"third_party/flatbuffers\"]\n\tpath = third_party/flatbuffers\n\turl = https://github.com/google/flatbuffers.git\n[submodule \"third_party/nlohmann\"]\n\tpath = third_party/nlohmann\n\turl = https://github.com/nlohmann/json.git\n[submodule \"third_party/VulkanMemoryAllocator\"]\n\tpath = third_party/VulkanMemoryAllocator\n\turl = https://github.com/GPUOpen-LibrariesAndSDKs/VulkanMemoryAllocator.git\n[submodule \"third_party/cutlass\"]\n\tpath = third_party/cutlass\n\turl = https://github.com/NVIDIA/cutlass.git\n[submodule \"third_party/mimalloc\"]\n\tpath = third_party/mimalloc\n\turl = https://github.com/microsoft/mimalloc.git\n[submodule \"third_party/opentelemetry-cpp\"]\n\tpath = third_party/opentelemetry-cpp\n\turl = https://github.com/open-telemetry/opentelemetry-cpp.git\n[submodule \"third_party/cpp-httplib\"]\n\tpath = third_party/cpp-httplib\n\turl = https://github.com/yhirose/cpp-httplib.git\n\tbranch = v0.15.3\n[submodule \"third_party/NVTX\"]\n\tpath = third_party/NVTX\n\turl = https://github.com/NVIDIA/NVTX.git\n[submodule \"third_party/composable_kernel\"]\n\tpath = third_party/composable_kernel\n\turl = https://github.com/ROCm/composable_kernel.git\n\tbranch = develop\n[submodule \"third_party/kleidiai\"]\n\tpath = third_party/kleidiai\n\turl = https://git.gitlab.arm.com/kleidi/kleidiai.git\n"
        },
        {
          "name": ".lintrunner.toml",
          "type": "blob",
          "size": 52.47,
          "content": "[[linter]]\ncode = 'FLAKE8'\ninclude_patterns = ['**/*.py']\nexclude_patterns = [\n    '.git/**',\n    'build_test_custom_build/**',\n    'build/**',\n    'caffe2/**',\n    'docs/caffe2/**',\n    'docs/cpp/src/**',\n    'docs/src/**',\n    'fb/**',\n    '**/fb/**',\n    'functorch/docs/**',\n    'functorch/examples/**',\n    'functorch/notebooks/**',\n    'torch/_inductor/fx_passes/serialized_patterns/**',\n    'torch/_inductor/autoheuristic/artifacts/**',\n    'scripts/**',\n    'test/generated_type_hints_smoketest.py',\n    # Tests from the NumPy test suite\n    'test/torch_np/numpy_test/**/*.py',\n    'third_party/**',\n    'torch/include/**',\n    'torch/lib/**',\n    'venv/**',\n    '**/*.pyi',\n    'tools/test/test_selective_build.py',\n]\ncommand = [\n    'python3',\n    'tools/linter/adapters/flake8_linter.py',\n    '--',\n    '@{{PATHSFILE}}'\n]\ninit_command = [\n    'python3',\n    'tools/linter/adapters/pip_init.py',\n    '--dry-run={{DRYRUN}}',\n    'flake8==6.1.0',\n    'flake8-bugbear==23.3.23',\n    'flake8-comprehensions==3.15.0',\n    'flake8-executable==2.1.3',\n    'flake8-logging-format==0.9.0',\n    'flake8-pyi==23.3.1',\n    'flake8-simplify==0.19.3',\n    'mccabe==0.7.0',\n    'pycodestyle==2.11.1',\n    'pyflakes==3.1.0',\n    'torchfix==0.4.0 ; python_version >= \"3.9\" and python_version < \"3.13\"',\n]\n\n\n[[linter]]\ncode = 'CLANGFORMAT'\ninclude_patterns = [\n    'aten/src/ATen/*.h',\n    'aten/src/ATen/mps/**/*.mm',\n    'aten/src/ATen/mps/**/*.h',\n    'aten/src/ATen/xpu/**/*.h',\n    'aten/src/ATen/xpu/**/*.cpp',\n    'aten/src/ATen/core/boxing/**/*.h',\n    'aten/src/ATen/core/dispatch/**/*.h',\n    'aten/src/ATen/native/mps/**/*.metal',\n    'aten/src/ATen/native/mps/**/*.mm',\n    'aten/src/ATen/native/mps/**/*.h',\n    'aten/src/ATen/native/vulkan/**/*.h',\n    'aten/src/ATen/native/vulkan/**/*.cpp',\n    'aten/src/ATen/native/cuda/MultiTensorApply.cuh',\n    'aten/src/ATen/native/**/Foreach*.*',\n    'aten/src/ATen/native/cuda/fused*.*',\n    'aten/src/ATen/native/cuda/Fused*.cu',\n    'aten/src/ATen/native/cudnn/*.h',\n    'aten/src/ATen/native/cudnn/*.cpp',\n    'aten/src/ATen/native/mkldnn/xpu/**/*.h',\n    'aten/src/ATen/native/mkldnn/xpu/**/*.cpp',\n    'aten/src/ATen/native/Tensor*.h',\n    'aten/src/ATen/native/Tensor*.cpp',\n    'c10/**/*.h',\n    'c10/**/*.cpp',\n    'torch/csrc/**/*.h',\n    'torch/csrc/**/*.hpp',\n    'torch/csrc/**/*.cpp',\n    'test/cpp/**/*.h',\n    'test/cpp/**/*.cpp',\n]\nexclude_patterns = [\n    'aten/src/ATen/native/vulkan/api/vk_mem_alloc.h',\n    'aten/src/ATen/native/mps/kernels/Quantized.metal',\n    'c10/util/strong_type.h',\n    '**/fb/**',\n    'torch/csrc/inductor/aoti_torch/generated/**',\n    'torch/csrc/jit/serialization/mobile_bytecode_generated.h',\n    'torch/csrc/utils/pythoncapi_compat.h',\n    'aten/src/ATen/dlpack.h',\n]\ninit_command = [\n    'python3',\n    'tools/linter/adapters/s3_init.py',\n    '--config-json=tools/linter/adapters/s3_init_config.json',\n    '--linter=clang-format',\n    '--dry-run={{DRYRUN}}',\n    '--output-dir=.lintbin',\n    '--output-name=clang-format',\n]\ncommand = [\n    'python3',\n    'tools/linter/adapters/clangformat_linter.py',\n    '--binary=.lintbin/clang-format',\n    '--',\n    '@{{PATHSFILE}}'\n]\nis_formatter = true\n\n[[linter]]\ncode = 'MYPY'\ninclude_patterns = [\n    'torch/**/*.py',\n    'torch/**/*.pyi',\n    'caffe2/**/*.py',\n    'caffe2/**/*.pyi',\n    'test/test_bundled_images.py',\n    'test/test_bundled_inputs.py',\n    'test/test_complex.py',\n    'test/test_datapipe.py',\n    'test/test_futures.py',\n    # 'test/test_numpy_interop.py',\n    'test/test_torch.py',\n    'test/test_type_hints.py',\n    'test/test_type_info.py',\n    'test/test_utils.py',\n    'scripts/release/upload_metadata_file.py',\n]\nexclude_patterns = [\n    '**/fb/**',\n]\ncommand = [\n    'python3',\n    'tools/linter/adapters/mypy_linter.py',\n    '--config=mypy.ini',\n    '--',\n    '@{{PATHSFILE}}'\n]\ninit_command = [\n    'python3',\n    'tools/linter/adapters/pip_init.py',\n    '--dry-run={{DRYRUN}}',\n    'numpy==1.26.4 ; python_version >= \"3.9\" and python_version <= \"3.11\"',\n    'numpy==2.1.0 ; python_version >= \"3.12\"',\n    'expecttest==0.3.0',\n    'mypy==1.13.0',\n    'sympy==1.13.0 ; python_version >= \"3.9\"',\n    'types-requests==2.27.25',\n    'types-PyYAML==6.0.7',\n    'types-tabulate==0.8.8',\n    'types-protobuf==3.19.18',\n    'types-pkg-resources==0.1.3',\n    'types-Jinja2==2.11.9',\n    'types-colorama==0.4.6',\n    'filelock==3.13.1',\n    'junitparser==2.1.1',\n    'rich==10.9.0',\n    'pyyaml==6.0.1',\n    'optree==0.13.0',\n]\n\n[[linter]]\ncode = 'MYPYSTRICT'\ninclude_patterns = [\n    '.github/**/*.py',\n    'benchmarks/instruction_counts/**/*.py',\n    'tools/**/*.py',\n    'torchgen/**/*.py',\n    'torch/utils/_pytree.py',\n    'torch/utils/_cxx_pytree.py',\n    'torch/utils/benchmark/utils/common.py',\n    'torch/utils/benchmark/utils/timer.py',\n    'torch/utils/benchmark/utils/valgrind_wrapper/**/*.py',\n]\nexclude_patterns = [\n    # (linbinyu) copied from internal repo\n    '**/fb/**',\n    'tools/code_analyzer/gen_operators_yaml.py',\n    'tools/dynamo/verify_dynamo.py',\n    'tools/gen_vulkan_spv.py',\n    'tools/test/gen_operators_yaml_test.py',\n    'tools/test/gen_oplist_test.py',\n    'tools/test/test_selective_build.py',\n]\ncommand = [\n    'python3',\n    'tools/linter/adapters/mypy_linter.py',\n    '--config=mypy-strict.ini',\n    '--code=MYPYSTRICT',\n    '--',\n    '@{{PATHSFILE}}'\n]\n\n[[linter]]\ncode = 'CLANGTIDY'\ninclude_patterns = [\n    # Enable coverage of headers in aten/src/ATen\n    # and excluding most sub-directories for now.\n    'aten/src/ATen/*.h',\n    'aten/src/ATen/*.cpp',\n    'aten/src/ATen/cuda/*.cpp',\n    'aten/src/ATen/cpu/*.h',\n    'aten/src/ATen/cpu/*.cpp',\n    'aten/src/ATen/core/*.h',\n    'aten/src/ATen/core/*.cpp',\n    'aten/src/ATen/cudnn/*.h',\n    'aten/src/ATen/cudnn/*.cpp',\n    'aten/src/ATen/native/mkldnn/xpu/**/*.h',\n    'aten/src/ATen/native/mkldnn/xpu/**/*.cpp',\n    'aten/src/ATen/detail/*',\n    'aten/src/ATen/functorch/*.h',\n    'aten/src/ATen/functorch/*.cpp',\n    'aten/src/ATen/native/nested/cuda/*.cpp',\n    'aten/src/ATen/native/nested/cuda/*.h',\n    'aten/src/ATen/native/nested/*.cpp',\n    'aten/src/ATen/native/nested/*.h',\n    'c10/**/*.cpp',\n    'c10/**/*.h',\n    'torch/*.h',\n    'torch/csrc/*.h',\n    'torch/csrc/*.cpp',\n    'torch/csrc/**/*.h',\n    'torch/csrc/**/*.cpp',\n    'torch/csrc/jit/serialization/*.h',\n    'torch/csrc/jit/serialization/*.cpp',\n]\nexclude_patterns = [\n    # The negative filters below are to exclude files that include onnx_pb.h or\n    # caffe2_pb.h, otherwise we'd have to build protos as part of this CI job.\n    # CUDA files are also excluded.\n    '**/fb/**',\n    '**/generated/**',\n    '**/*pb.h',\n    '**/*inl.h',\n    'aten/src/ATen/cpu/FlushDenormal.cpp',\n    'aten/src/ATen/cpu/Utils.cpp',\n    'aten/src/ATen/cpu/vml.h',\n    'aten/src/ATen/CPUFixedAllocator.h',\n    'aten/src/ATen/Parallel*.h',\n    'c10/xpu/**/*.h',\n    'c10/xpu/**/*.cpp',\n    'c10/benchmark/intrusive_ptr_benchmark.cpp',\n    'c10/cuda/CUDAAlgorithm.h',\n    'c10/util/complex_math.h',\n    'c10/util/complex_utils.h',\n    'c10/util/flat_hash_map.h',\n    'c10/util/logging*.h',\n    'c10/util/hash.h',\n    'c10/util/strong_type.h',\n    'c10/util/SmallVector.h',\n    'c10/util/win32-headers.h',\n    'c10/test/**/*.h',\n    'third_party/**/*',\n    'torch/csrc/api/include/torch/nn/modules/common.h',\n    'torch/csrc/api/include/torch/linalg.h',\n    'torch/csrc/autograd/generated/**',\n    'torch/csrc/distributed/**/*.cu',\n    'torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp',\n    'torch/csrc/distributed/c10d/WinSockUtils.hpp',\n    'torch/csrc/distributed/c10d/quantization/quantization_gpu.h',\n    'torch/csrc/dynamo/eval_frame.h',\n    'torch/csrc/inductor/aoti_torch/c/shim.h',\n    'torch/csrc/jit/**/*',\n    'torch/csrc/jit/serialization/mobile_bytecode_generated.h',\n    'torch/csrc/utils/generated_serialization_types.h',\n    'torch/csrc/utils/pythoncapi_compat.h',\n    'torch/csrc/inductor/aoti_runtime/sycl_runtime_wrappers.h',\n]\ninit_command = [\n    'python3',\n    'tools/linter/adapters/s3_init.py',\n    '--config-json=tools/linter/adapters/s3_init_config.json',\n    '--linter=clang-tidy',\n    '--dry-run={{DRYRUN}}',\n    '--output-dir=.lintbin',\n    '--output-name=clang-tidy',\n]\ncommand = [\n    'python3',\n    'tools/linter/adapters/clangtidy_linter.py',\n    '--binary=.lintbin/clang-tidy',\n    '--build_dir=./build',\n    '--',\n    '@{{PATHSFILE}}'\n]\n\n[[linter]]\ncode = 'TYPEIGNORE'\ninclude_patterns = ['**/*.py', '**/*.pyi']\nexclude_patterns = [\n    'fb/**',\n    '**/fb/**',\n    'test/test_jit.py',\n]\ncommand = [\n    'python3',\n    'tools/linter/adapters/grep_linter.py',\n    '--pattern=# type:\\s*ignore([^\\[]|$)',\n    '--linter-name=TYPEIGNORE',\n    '--error-name=unqualified type: ignore',\n    \"\"\"--error-description=\\\n        This line has an unqualified `type: ignore`; \\\n        please convert it to `type: ignore[xxxx]`\\\n    \"\"\",\n    '--',\n    '@{{PATHSFILE}}'\n]\n\n[[linter]]\ncode = 'TYPENOSKIP'\ninclude_patterns = ['mypy.ini']\ncommand = [\n    'python3',\n    'tools/linter/adapters/grep_linter.py',\n    '--pattern=follow_imports\\s*=\\s*skip',\n    '--linter-name=TYPENOSKIP',\n    '--error-name=use of follow_imports = skip',\n    \"\"\"--error-description=\\\n        follow_imports = skip is forbidden from mypy.ini configuration as it \\\n        is extremely easy to accidentally turn off type checking unintentionally.  If \\\n        you need to suppress type errors, use a top level # mypy: ignore-errors.  \\\n        Do not rely on automatic Any substitution; instead, manually # type: ignore \\\n        at use sites or define a pyi type stub with more relaxed types. \\\n    \"\"\",\n    '--',\n    '@{{PATHSFILE}}'\n]\n\n[[linter]]\ncode = 'NOQA'\ninclude_patterns = ['**/*.py', '**/*.pyi']\nexclude_patterns = [\n    'caffe2/**',\n    'fb/**',\n    '**/fb/**'\n    ]\ncommand = [\n    'python3',\n    'tools/linter/adapters/grep_linter.py',\n    '--pattern=# noqa([^:]|$)',\n    '--linter-name=NOQA',\n    '--error-name=unqualified noqa',\n    \"\"\"--error-description=\\\n        This line has an unqualified `noqa`; \\\n        please convert it to `noqa: XXXX`\\\n    \"\"\",\n    '--',\n    '@{{PATHSFILE}}'\n]\n\n[[linter]]\ncode = 'NATIVEFUNCTIONS'\ninclude_patterns=['aten/src/ATen/native/native_functions.yaml']\ncommand = [\n    'python3',\n    'tools/linter/adapters/nativefunctions_linter.py',\n    '--native-functions-yml=aten/src/ATen/native/native_functions.yaml',\n]\ninit_command = [\n    'python3',\n    'tools/linter/adapters/pip_init.py',\n    '--dry-run={{DRYRUN}}',\n    'ruamel.yaml==0.17.4',\n]\nis_formatter = true\n\n[[linter]]\ncode = 'GHA'\ninclude_patterns=['.github/workflows/**/*.yml']\ncommand = [\n    'python3',\n    'tools/linter/adapters/gha_linter.py',\n    '--',\n    '@{{PATHSFILE}}'\n]\n\n[[linter]]\ncode = 'NEWLINE'\ninclude_patterns=['**']\nexclude_patterns=[\n    '**/contrib/**',\n    'third_party/**',\n    '**/*.bat',\n    '**/*.expect',\n    '**/*.ipynb',\n    '**/*.ps1',\n    '**/*.ptl',\n    'fb/**',\n    '**/fb/**',\n    'tools/clang_format_hash/**',\n    'test/cpp/jit/upgrader_models/*.ptl',\n    'test/cpp/jit/upgrader_models/*.ptl.ff',\n    '**/*.png',\n    '**/*.gz',\n]\ncommand = [\n    'python3',\n    'tools/linter/adapters/newlines_linter.py',\n    '--',\n    '@{{PATHSFILE}}',\n]\nis_formatter = true\n\n[[linter]]\ncode = 'SPACES'\ninclude_patterns = ['**']\nexclude_patterns = [\n    '**/contrib/**',\n    '**/*.diff',\n    '**/*.patch',\n    'third_party/**',\n    'aten/src/ATen/native/vulkan/api/vk_mem_alloc.h',\n    'fb/**',\n    '**/fb/**',\n    'test/cpp/jit/upgrader_models/*.ptl',\n    'test/cpp/jit/upgrader_models/*.ptl.ff',\n]\ncommand = [\n    'python3',\n    'tools/linter/adapters/grep_linter.py',\n    '--pattern=[[:blank:]]$',\n    '--linter-name=SPACES',\n    '--error-name=trailing spaces',\n    '--replace-pattern=s/[[:blank:]]+$//',\n    \"\"\"--error-description=\\\n        This line has trailing spaces; please remove them.\\\n    \"\"\",\n    '--',\n    '@{{PATHSFILE}}'\n]\n\n[[linter]]\ncode = 'TABS'\ninclude_patterns = ['**']\nexclude_patterns = [\n    '**/*.svg',\n    '**/*Makefile',\n    '**/contrib/**',\n    'third_party/**',\n    '**/.gitattributes',\n    '**/.gitmodules',\n    'fb/**',\n    '**/fb/**',\n    'aten/src/ATen/native/vulkan/api/vk_mem_alloc.h',\n    'test/cpp/jit/upgrader_models/*.ptl',\n    'test/cpp/jit/upgrader_models/*.ptl.ff',\n    '.ci/docker/common/install_rocm_drm.sh',\n    '.lintrunner.toml',\n    '.ci/magma/package_files/*.patch',\n]\ncommand = [\n    'python3',\n    'tools/linter/adapters/grep_linter.py',\n    # @lint-ignore TXT2\n    '--pattern=\t',\n    '--linter-name=TABS',\n    '--error-name=saw some tabs',\n    '--replace-pattern=s/\\t/    /',\n    \"\"\"--error-description=\\\n        This line has tabs; please replace them with spaces.\\\n    \"\"\",\n    '--',\n    '@{{PATHSFILE}}'\n]\n\n[[linter]]\ncode = 'C10_UNUSED'\ninclude_patterns = [\n    '**/*.cpp',\n    '**/*.h',\n]\nexclude_patterns = [\n    'c10/macros/Macros.h',\n]\ncommand = [\n    'python3',\n    'tools/linter/adapters/grep_linter.py',\n    '--pattern=C10_UNUSED',\n    '--linter-name=C10_UNUSED',\n    '--error-name=deprecated C10_UNUSED macro',\n    '--replace-pattern=s/C10_UNUSED/[[maybe_unused]]/',\n    \"\"\"--error-description=\\\n        Deprecated macro, use [[maybe_unused]] directly\\\n    \"\"\",\n    '--',\n    '@{{PATHSFILE}}'\n]\n\n[[linter]]\ncode = 'C10_NODISCARD'\ninclude_patterns = [\n    '**/*.cpp',\n    '**/*.h',\n]\nexclude_patterns = [\n    'c10/macros/Macros.h',\n]\ncommand = [\n    'python3',\n    'tools/linter/adapters/grep_linter.py',\n    '--pattern=C10_NODISCARD',\n    '--linter-name=C10_NODISCARD',\n    '--error-name=deprecated C10_NODISCARD macro',\n    '--replace-pattern=s/C10_NODISCARD/[[nodiscard]]/',\n    \"\"\"--error-description=\\\n        Deprecated macro, use [[nodiscard]] directly\\\n    \"\"\",\n    '--',\n    '@{{PATHSFILE}}'\n]\n\n[[linter]]\ncode = 'INCLUDE'\ninclude_patterns = [\n    'c10/**',\n    'aten/**',\n    'torch/csrc/**',\n]\nexclude_patterns = [\n    'aten/src/ATen/native/quantized/cpu/qnnpack/**',\n    'aten/src/ATen/native/vulkan/api/vk_mem_alloc.h',\n    'aten/src/ATen/native/vulkan/glsl/**',\n    '**/fb/**',\n    'torch/csrc/jit/serialization/mobile_bytecode_generated.h',\n    'torch/csrc/utils/pythoncapi_compat.h',\n]\ncommand = [\n    'python3',\n    'tools/linter/adapters/grep_linter.py',\n    '--pattern=#include \"',\n    '--linter-name=INCLUDE',\n    '--error-name=quoted include',\n    '--replace-pattern=s/#include \"(.*)\"$/#include <\\1>/',\n    \"\"\"--error-description=\\\n        This #include uses quotes; please convert it to #include <xxxx>\\\n    \"\"\",\n    '--',\n    '@{{PATHSFILE}}'\n]\n\n[[linter]]\ncode = 'PYBIND11_INCLUDE'\ninclude_patterns = [\n    '**/*.cpp',\n    '**/*.h',\n]\nexclude_patterns = [\n    'torch/csrc/utils/pybind.h',\n    'torch/utils/benchmark/utils/valgrind_wrapper/compat_bindings.cpp',\n    'caffe2/**/*',\n]\ncommand = [\n    'python3',\n    'tools/linter/adapters/grep_linter.py',\n    '--pattern=#include <pybind11\\/',\n    '--allowlist-pattern=#include <torch\\/csrc\\/utils\\/pybind.h>',\n    '--linter-name=PYBIND11_INCLUDE',\n    '--match-first-only',\n    '--error-name=direct include of pybind11',\n    # https://stackoverflow.com/a/33416489/23845\n    # NB: this won't work if the pybind11 include is on the first line;\n    # but that's fine because it will just mean the lint will still fail\n    # after applying the change and you will have to fix it manually\n    '--replace-pattern=1,/(#include <pybind11\\/)/ s/(#include <pybind11\\/)/#include <torch\\/csrc\\/utils\\/pybind.h>\\n\\1/',\n    \"\"\"--error-description=\\\n        This #include directly includes pybind11 without also including \\\n        #include <torch/csrc/utils/pybind.h>;  this means some important \\\n        specializations may not be included.\\\n    \"\"\",\n    '--',\n    '@{{PATHSFILE}}'\n]\n\n[[linter]]\ncode = 'ERROR_PRONE_ISINSTANCE'\ninclude_patterns = [\n    'torch/_refs/**/*.py',\n    'torch/_prims/**/*.py',\n    'torch/_prims_common/**/*.py',\n    'torch/_decomp/**/*.py',\n    'torch/_meta_registrations.py',\n]\nexclude_patterns = [\n    '**/fb/**',\n]\ncommand = [\n    'python3',\n    'tools/linter/adapters/grep_linter.py',\n    '--pattern=isinstance\\([^)]+(int|float)\\)',\n    '--linter-name=ERROR_PRONE_ISINSTANCE',\n    '--error-name=error prone isinstance',\n    \"\"\"--error-description=\\\n        This line has an isinstance call that directly refers to \\\n        int or float.  This is error-prone because you may also \\\n        have wanted to allow SymInt or SymFloat in your test.  \\\n        To suppress this lint, use an appropriate type alias defined \\\n        in torch._prims_common; use IntLike/FloatLike when you would accept \\\n        both regular and symbolic numbers, Dim for ints representing \\\n        dimensions, or IntWithoutSymInt/FloatWithoutSymFloat if you really \\\n        meant to exclude symbolic numbers.\n    \"\"\",\n    '--',\n    '@{{PATHSFILE}}'\n]\n\n[[linter]]\ncode = 'PYBIND11_SPECIALIZATION'\ninclude_patterns = [\n    '**/*.cpp',\n    '**/*.h',\n]\nexclude_patterns = [\n    # The place for all orphan specializations\n    'torch/csrc/utils/pybind.h',\n    # These specializations are non-orphan\n    'torch/csrc/distributed/c10d/init.cpp',\n    'torch/csrc/jit/python/pybind.h',\n    'fb/**',\n    '**/fb/**',\n    # These are safe to exclude as they do not have Python\n    'c10/**/*',\n]\ncommand = [\n    'python3',\n    'tools/linter/adapters/grep_linter.py',\n    '--pattern=PYBIND11_DECLARE_HOLDER_TYPE',\n    '--linter-name=PYBIND11_SPECIALIZATION',\n    '--error-name=pybind11 specialization in non-standard location',\n    \"\"\"--error-description=\\\n        This pybind11 specialization (PYBIND11_DECLARE_HOLDER_TYPE) should \\\n        be placed in torch/csrc/utils/pybind.h so that it is guaranteed to be \\\n        included at any site that may potentially make use of it via py::cast. \\\n        If your specialization is in the same header file as the definition \\\n        of the holder type, you can ignore this lint by adding your header to \\\n        the exclude_patterns for this lint in .lintrunner.toml.  For more \\\n        information see https://github.com/pybind/pybind11/issues/4099 \\\n    \"\"\",\n    '--',\n    '@{{PATHSFILE}}'\n]\n\n[[linter]]\ncode = 'PYPIDEP'\ninclude_patterns = ['.github/**']\nexclude_patterns = [\n    '**/*.rst',\n    '**/*.py',\n    '**/*.md',\n    '**/*.diff',\n    '**/fb/**',\n]\ncommand = [\n    'python3',\n    'tools/linter/adapters/grep_linter.py',\n    \"\"\"--pattern=\\\n    (pip|pip3|python -m pip|python3 -m pip|python3 -mpip|python -mpip) \\\n    install ([a-zA-Z0-9][A-Za-z0-9\\\\._\\\\-]+)([^/=<>~!]+)[A-Za-z0-9\\\\._\\\\-\\\\*\\\\+\\\\!]*$\\\n    \"\"\",\n    '--linter-name=PYPIDEP',\n    '--error-name=unpinned PyPI install',\n    \"\"\"--error-description=\\\n        This line has unpinned PyPi installs; \\\n        please pin them to a specific version: e.g. 'thepackage==1.2'\\\n    \"\"\",\n    '--',\n    '@{{PATHSFILE}}'\n]\n\n[[linter]]\ncode = 'EXEC'\ninclude_patterns = ['**']\nexclude_patterns = [\n    'third_party/**',\n    'torch/bin/**',\n    '**/*.so',\n    '**/*.py',\n    '**/*.sh',\n    '**/*.bash',\n    '**/git-pre-commit',\n    '**/git-clang-format',\n    '**/gradlew',\n    'fb/**',\n    '**/fb/**',\n]\ncommand = [\n    'python3',\n    'tools/linter/adapters/exec_linter.py',\n    '--',\n    '@{{PATHSFILE}}',\n]\n\n[[linter]]\ncode = 'CUBINCLUDE'\ninclude_patterns = ['aten/**']\nexclude_patterns = [\n    'aten/src/ATen/cuda/cub*.cuh',\n    '**/fb/**',\n]\ncommand = [\n    'python3',\n    'tools/linter/adapters/grep_linter.py',\n    '--pattern=#include <cub/',\n    '--linter-name=CUBINCLUDE',\n    '--error-name=direct cub include',\n    \"\"\"--error-description=\\\n        This line has a direct cub include; please include \\\n        ATen/cuda/cub.cuh instead and wrap your cub calls in \\\n        at::native namespace if necessary.\n    \"\"\",\n    '--',\n    '@{{PATHSFILE}}'\n]\n\n[[linter]]\ncode = 'RAWCUDA'\ninclude_patterns = [\n    'aten/**',\n    'c10/**',\n]\nexclude_patterns = [\n    'aten/src/ATen/test/**',\n    'c10/cuda/CUDAFunctions.h',\n    'c10/cuda/CUDACachingAllocator.cpp',\n    '**/fb/**',\n]\ncommand = [\n    'python3',\n    'tools/linter/adapters/grep_linter.py',\n    '--pattern=cudaStreamSynchronize',\n    '--linter-name=RAWCUDA',\n    '--error-name=raw CUDA API usage',\n    \"\"\"--error-description=\\\n        This line calls raw CUDA APIs directly; please use at::cuda wrappers instead.\n    \"\"\",\n    '--',\n    '@{{PATHSFILE}}'\n]\n\n[[linter]]\ncode = 'RAWCUDADEVICE'\ninclude_patterns = [\n    'aten/**',\n    'c10/**',\n    'torch/csrc/**',\n]\nexclude_patterns = [\n    'aten/src/ATen/cuda/CUDAContext.cpp',\n    'aten/src/ATen/cuda/CUDAGeneratorImpl.cpp',\n    'aten/src/ATen/test/**',\n    'c10/core/impl/InlineDeviceGuard.h',\n    'c10/cuda/CUDAFunctions.cpp',\n    'c10/cuda/CUDAGuard.h',\n    'c10/cuda/impl/CUDATest.cpp',\n    'torch/csrc/cuda/nccl.cpp',\n    '**/fb/**',\n]\ncommand = [\n    'python3',\n    'tools/linter/adapters/grep_linter.py',\n    '--pattern=cudaSetDevice(',\n    '--pattern=cudaGetDevice(',\n    '--linter-name=RAWCUDADEVICE',\n    '--error-name=raw CUDA API usage',\n    \"\"\"--error-description=\\\n        This line calls raw CUDA APIs directly; please use c10::cuda wrappers instead.\n    \"\"\",\n    '--',\n    '@{{PATHSFILE}}'\n]\n\n[[linter]]\ncode = 'ROOT_LOGGING'\ninclude_patterns = [\n    '**/*.py',\n]\n# These are not library code, but scripts in their own right, and so\n# therefore are permitted to use logging\nexclude_patterns = [\n    'tools/**',\n    'test/**',\n    'benchmarks/**',\n    'torch/distributed/run.py',\n    'functorch/benchmarks/**',\n    # Grandfathered in\n    'caffe2/**',\n    'fb/**',\n    '**/fb/**',\n]\ncommand = [\n    'python3',\n    'tools/linter/adapters/grep_linter.py',\n    '--pattern=logging\\.(debug|info|warn|warning|error|critical|log|exception)\\(',\n    '--replace-pattern=s/logging\\.(debug|info|warn|warning|error|critical|log|exception)\\(/log.\\1(/',\n    '--linter-name=ROOT_LOGGING',\n    '--error-name=use of root logger',\n    \"\"\"--error-description=\\\n        Do not use root logger (logging.info, etc) directly; instead \\\n        define 'log = logging.getLogger(__name__)' and call, e.g., log.info().\n    \"\"\",\n    '--',\n    '@{{PATHSFILE}}'\n]\n\n[[linter]]\ncode = 'DEPLOY_DETECTION'\ninclude_patterns = [\n    '**/*.py',\n]\ncommand = [\n    'python3',\n    'tools/linter/adapters/grep_linter.py',\n    '--pattern=sys\\.executable == .torch_deploy.',\n    '--replace-pattern=s/sys\\.executable == .torch_deploy./torch._running_with_deploy\\(\\)/',\n    '--linter-name=DEPLOY_DETECTION',\n    '--error-name=properly detect deploy runner',\n    \"\"\"--error-description=\\\n        Do not use sys.executable to detect if running within deploy/multipy, use torch._running_with_deploy().\n    \"\"\",\n    '--',\n    '@{{PATHSFILE}}'\n]\n\n[[linter]]\ncode = 'CMAKE'\ninclude_patterns = [\n    \"**/*.cmake\",\n    \"**/*.cmake.in\",\n    \"**/CMakeLists.txt\",\n]\nexclude_patterns = [\n    'cmake/Modules/**',\n    'cmake/Modules_CUDA_fix/**',\n    'cmake/Caffe2Config.cmake.in',\n    'aten/src/ATen/ATenConfig.cmake.in',\n    'cmake/TorchConfig.cmake.in',\n    'cmake/TorchConfigVersion.cmake.in',\n    'cmake/cmake_uninstall.cmake.i',\n    'fb/**',\n    '**/fb/**',\n]\ncommand = [\n    'python3',\n    'tools/linter/adapters/cmake_linter.py',\n    '--config=.cmakelintrc',\n    '--',\n    '@{{PATHSFILE}}',\n]\ninit_command = [\n    'python3',\n    'tools/linter/adapters/pip_init.py',\n    '--dry-run={{DRYRUN}}',\n    'cmakelint==1.4.1',\n]\n\n[[linter]]\ncode = 'SHELLCHECK'\ninclude_patterns = [\n    '.ci/pytorch/**/*.sh'\n]\nexclude_patterns = [\n    '**/fb/**',\n]\ncommand = [\n    'python3',\n    'tools/linter/adapters/shellcheck_linter.py',\n    '--',\n    '@{{PATHSFILE}}',\n]\ninit_command = [\n    'python3',\n    'tools/linter/adapters/pip_init.py',\n    '--dry-run={{DRYRUN}}',\n    'shellcheck-py==0.7.2.1',\n]\n\n[[linter]]\ncode = 'ACTIONLINT'\ninclude_patterns = [\n    '.github/workflows/*.yml',\n    '.github/workflows/*.yaml',\n    # actionlint does not support composite actions yet\n    # '.github/actions/**/*.yml',\n    # '.github/actions/**/*.yaml',\n]\nexclude_patterns = [\n    '**/fb/**',\n]\ncommand = [\n    'python3',\n    'tools/linter/adapters/actionlint_linter.py',\n    '--binary=.lintbin/actionlint',\n    '--',\n    '@{{PATHSFILE}}',\n]\ninit_command = [\n    'python3',\n    'tools/linter/adapters/s3_init.py',\n    '--config-json=tools/linter/adapters/s3_init_config.json',\n    '--linter=actionlint',\n    '--dry-run={{DRYRUN}}',\n    '--output-dir=.lintbin',\n    '--output-name=actionlint',\n]\n\n[[linter]]\ncode = 'TESTOWNERS'\ninclude_patterns = [\n    'test/**/test_*.py',\n    'test/**/*_test.py',\n]\nexclude_patterns = [\n    'test/run_test.py',\n    '**/fb/**',\n]\ncommand = [\n    'python3',\n    'tools/linter/adapters/testowners_linter.py',\n    '--',\n    '@{{PATHSFILE}}',\n]\n\n[[linter]]\ncode = 'TEST_HAS_MAIN'\ninclude_patterns = [\n    'test/**/test_*.py',\n]\nexclude_patterns = [\n    'test/run_test.py',\n    '**/fb/**',\n    'test/quantization/**',  # should be run through test/test_quantization.py\n    'test/jit/**',  # should be run through test/test_jit.py\n    'test/ao/sparsity/**',  # should be run through test/test_ao_sparsity.py\n    'test/fx/**',  # should be run through test/test_fx.py\n    'test/bottleneck_test/**',  # excluded by test/run_test.py\n    'test/package/**',  # excluded by test/run_test.py\n    'test/distributed/argparse_util_test.py',\n    'test/distributed/bin/test_script.py',\n    'test/distributed/elastic/agent/server/test/local_elastic_agent_test.py',\n    'test/distributed/elastic/multiprocessing/bin/test_script.py',\n    'test/distributed/elastic/multiprocessing/bin/zombie_test.py',\n    'test/distributed/elastic/multiprocessing/errors/api_test.py',\n    'test/distributed/elastic/multiprocessing/errors/error_handler_test.py',\n    'test/distributed/elastic/multiprocessing/redirects_test.py',\n    'test/distributed/elastic/multiprocessing/tail_log_test.py',\n    'test/distributed/elastic/rendezvous/api_test.py',\n    'test/distributed/elastic/rendezvous/c10d_rendezvous_backend_test.py',\n    'test/distributed/elastic/rendezvous/dynamic_rendezvous_test.py',\n    'test/distributed/elastic/rendezvous/etcd_rendezvous_backend_test.py',\n    'test/distributed/elastic/rendezvous/etcd_rendezvous_test.py',\n    'test/distributed/elastic/rendezvous/etcd_server_test.py',\n    'test/distributed/elastic/rendezvous/rendezvous_backend_test.py',\n    'test/distributed/elastic/rendezvous/static_rendezvous_test.py',\n    'test/distributed/elastic/rendezvous/utils_test.py',\n    'test/distributed/elastic/timer/api_test.py',\n    'test/distributed/elastic/utils/data/cycling_iterator_test.py',\n    'test/distributed/launcher/api_test.py',\n    'test/distributed/launcher/bin/test_script.py',\n    'test/distributed/launcher/bin/test_script_init_method.py',\n    'test/distributed/launcher/bin/test_script_is_torchelastic_launched.py',\n    'test/distributed/launcher/bin/test_script_local_rank.py',\n    'test/distributed/launcher/launch_test.py',\n    'test/distributed/launcher/run_test.py',\n    'test/distributed/optim/test_apply_optimizer_in_backward.py',\n    'test/distributed/optim/test_named_optimizer.py',\n    'test/distributed/test_c10d_spawn.py',\n    'test/distributed/test_collective_utils.py',\n    'test/distributions/test_distributions.py',\n    'test/inductor/test_aot_inductor_utils.py',\n    'test/lazy/test_bindings.py',\n    'test/lazy/test_extract_compiled_graph.py',\n    'test/lazy/test_meta_kernel.py',\n    'test/nn/test_init.py',\n    'test/onnx/model_defs/op_test.py',\n    'test/onnx/test_models_quantized_onnxruntime.py',\n    'test/onnx/test_onnxscript_no_runtime.py',\n    'test/onnx_caffe2/test_caffe2_common.py',\n    'test/optim/test_lrscheduler.py',\n    'test/optim/test_optim.py',\n    'test/optim/test_swa_utils.py',\n    'test/run_test.py',\n    'test/test_bundled_images.py',\n    'test/test_cuda_expandable_segments.py',\n    'test/test_hub.py',\n]\ncommand = [\n    'python3',\n    'tools/linter/adapters/test_has_main_linter.py',\n    '--',\n    '@{{PATHSFILE}}',\n]\n\n[[linter]]\ncode = 'CALL_ONCE'\ninclude_patterns = [\n    'c10/**',\n    'aten/**',\n    'torch/csrc/**',\n]\nexclude_patterns = [\n    'c10/util/CallOnce.h',\n    '**/fb/**',\n]\ncommand = [\n    'python3',\n    'tools/linter/adapters/grep_linter.py',\n    '--pattern=std::call_once',\n    '--linter-name=CALL_ONCE',\n    '--error-name=invalid call_once',\n    '--replace-pattern=s/std::call_once/c10::call_once/',\n    \"\"\"--error-description=\\\n        Use of std::call_once is forbidden and should be replaced with c10::call_once\\\n    \"\"\",\n    '--',\n    '@{{PATHSFILE}}'\n]\n\n[[linter]]\ncode = 'CONTEXT_DECORATOR'\ninclude_patterns = [\n    'torch/**',\n]\ncommand = [\n    'python3',\n    'tools/linter/adapters/grep_linter.py',\n    '--pattern=@.*(dynamo_timed|preserve_rng_state|clear_frame|with_fresh_cache_if_config|use_lazy_graph_module|_disable_current_modes)',\n    '--linter-name=CONTEXT_DECORATOR',\n    '--error-name=avoid context decorator',\n    \"\"\"--error-description=\\\n        Do not use context manager as decorator as it breaks cProfile traces.  Use it as \\\n        a context manager instead\\\n    \"\"\",\n    '--',\n    '@{{PATHSFILE}}'\n]\n\n[[linter]]\ncode = 'ONCE_FLAG'\ninclude_patterns = [\n    'c10/**',\n    'aten/**',\n    'torch/csrc/**',\n]\nexclude_patterns = [\n    '**/fb/**',\n]\ncommand = [\n    'python3',\n    'tools/linter/adapters/grep_linter.py',\n    '--pattern=std::once_flag',\n    '--linter-name=ONCE_FLAG',\n    '--error-name=invalid once_flag',\n    '--replace-pattern=s/std::once_flag/c10::once_flag/',\n    \"\"\"--error-description=\\\n        Use of std::once_flag is forbidden and should be replaced with c10::once_flag\\\n    \"\"\",\n    '--',\n    '@{{PATHSFILE}}'\n]\n\n[[linter]]\ncode = 'WORKFLOWSYNC'\ninclude_patterns = [\n    '.github/workflows/pull.yml',\n    '.github/workflows/trunk.yml',\n    '.github/workflows/periodic.yml',\n    '.github/workflows/mac-mps.yml',\n    '.github/workflows/slow.yml',\n]\ncommand = [\n    'python3',\n    'tools/linter/adapters/workflow_consistency_linter.py',\n    '--',\n    '@{{PATHSFILE}}'\n]\ninit_command = [\n    'python3',\n    'tools/linter/adapters/pip_init.py',\n    '--dry-run={{DRYRUN}}',\n    'PyYAML==6.0.1',\n]\n\n[[linter]]\ncode = 'NO_WORKFLOWS_ON_FORK'\ninclude_patterns = [\n    '.github/**/*.yml',\n    '.github/**/*.yaml',\n]\nexclude_patterns = [\n    '**/fb/**',\n]\ncommand = [\n    'python3',\n    'tools/linter/adapters/no_workflows_on_fork.py',\n    '--',\n    '@{{PATHSFILE}}',\n]\ninit_command = [\n    'python3',\n    'tools/linter/adapters/pip_init.py',\n    '--dry-run={{DRYRUN}}',\n    'PyYAML==6.0.1',\n]\n\n# usort + ruff-format\n[[linter]]\ncode = 'PYFMT'\ninclude_patterns = [\n    '**/*.py',\n    '**/*.pyi',\n]\ncommand = [\n    'python3',\n    'tools/linter/adapters/pyfmt_linter.py',\n    '--',\n    '@{{PATHSFILE}}'\n]\nexclude_patterns = [\n    'tools/gen_vulkan_spv.py',\n    # We don't care too much about files in this directory, don't enforce\n    # formatting on them\n    'caffe2/**/*.py',\n    'caffe2/**/*.pyi',\n    'fb/**',\n    '**/fb/**',\n    'third_party/**/*.py',\n    'third_party/**/*.pyi',\n    'torch/_vendor/**',\n    'torch/_inductor/fx_passes/serialized_patterns/**',\n    'torch/_inductor/autoheuristic/artifacts/**',\n    # These files are all grandfathered in, feel free to remove from this list\n    # as necessary\n    'test/_nvfuser/__init__.py',\n    'test/_nvfuser/test_dynamo.py',\n    'test/_nvfuser/test_python_frontend.py',\n    'test/_nvfuser/test_torchscript.py',\n    'test/delete.py',\n    'test/expect/__init__.py',\n    'test/quantization/__init__.py',\n    'test/quantization/core/__init__.py',\n    'test/quantization/core/experimental/apot_fx_graph_mode_ptq.py',\n    'test/quantization/core/experimental/apot_fx_graph_mode_qat.py',\n    'test/quantization/core/experimental/quantization_util.py',\n    'test/quantization/core/experimental/test_bits.py',\n    'test/quantization/core/experimental/test_fake_quantize.py',\n    'test/quantization/core/experimental/test_linear.py',\n    'test/quantization/core/experimental/test_nonuniform_observer.py',\n    'test/quantization/core/experimental/test_quantized_tensor.py',\n    'test/quantization/core/experimental/test_quantizer.py',\n    'test/quantization/core/test_backend_config.py',\n    'test/quantization/core/test_docs.py',\n    'test/quantization/core/test_quantized_functional.py',\n    'test/quantization/core/test_quantized_module.py',\n    'test/quantization/core/test_quantized_op.py',\n    'test/quantization/core/test_quantized_tensor.py',\n    'test/quantization/core/test_top_level_apis.py',\n    'test/quantization/core/test_utils.py',\n    'test/quantization/core/test_workflow_module.py',\n    'test/quantization/core/test_workflow_ops.py',\n    'test/quantization/eager/__init__.py',\n    'test/quantization/eager/test_bias_correction_eager.py',\n    'test/quantization/eager/test_equalize_eager.py',\n    'test/quantization/eager/test_fuse_eager.py',\n    'test/quantization/eager/test_model_numerics.py',\n    'test/quantization/eager/test_numeric_suite_eager.py',\n    'test/quantization/eager/test_quantize_eager_ptq.py',\n    'test/quantization/eager/test_quantize_eager_qat.py',\n    'test/quantization/fx/__init__.py',\n    'test/quantization/fx/test_equalize_fx.py',\n    'test/quantization/fx/test_model_report_fx.py',\n    'test/quantization/fx/test_numeric_suite_fx.py',\n    'test/quantization/fx/test_quantize_fx.py',\n    'test/quantization/fx/test_subgraph_rewriter.py',\n    'test/test_fake_tensor.py',\n    'test/test_flop_counter.py',\n    'test/test_function_schema.py',\n    'test/test_functional_autograd_benchmark.py',\n    'test/test_functional_optim.py',\n    'test/test_functionalization_of_rng_ops.py',\n    'test/test_datapipe.py',\n    'test/test_futures.py',\n    'test/test_fx.py',\n    'test/test_fx_experimental.py',\n    'test/test_fx_passes.py',\n    'test/test_fx_reinplace_pass.py',\n    'test/test_import_stats.py',\n    'test/test_itt.py',\n    'test/test_jit.py',\n    'test/test_jit_autocast.py',\n    'test/test_jit_cuda_fuser.py',\n    'test/test_jit_disabled.py',\n    'test/test_jit_fuser.py',\n    'test/test_jit_fuser_legacy.py',\n    'test/test_jit_legacy.py',\n    'test/test_jit_llga_fuser.py',\n    'test/test_jit_profiling.py',\n    'test/test_jit_simple.py',\n    'test/test_jit_string.py',\n    'test/test_jiterator.py',\n    'test/test_kernel_launch_checks.py',\n    'test/test_linalg.py',\n    'test/test_masked.py',\n    'test/test_maskedtensor.py',\n    'test/test_matmul_cuda.py',\n    'test/test_meta.py',\n    'test/test_metal.py',\n    'test/test_mkl_verbose.py',\n    'test/test_mkldnn.py',\n    'test/test_mkldnn_fusion.py',\n    'test/test_mkldnn_verbose.py',\n    'test/test_mobile_optimizer.py',\n    'test/test_model_dump.py',\n    'test/test_modules.py',\n    'test/test_monitor.py',\n    'test/test_mps.py',\n    'test/test_multiprocessing_spawn.py',\n    'test/test_namedtensor.py',\n    'test/test_namedtuple_return_api.py',\n    'test/test_native_functions.py',\n    'test/test_native_mha.py',\n    'test/test_nn.py',\n    'test/test_out_dtype_op.py',\n    'test/test_overrides.py',\n    'test/test_prims.py',\n    'test/test_proxy_tensor.py',\n    'test/test_pruning_op.py',\n    'test/test_quantization.py',\n    'test/test_reductions.py',\n    'test/test_scatter_gather_ops.py',\n    'test/test_schema_check.py',\n    'test/test_segment_reductions.py',\n    'test/test_serialization.py',\n    'test/test_set_default_mobile_cpu_allocator.py',\n    'test/test_sparse.py',\n    'test/test_sparse_csr.py',\n    'test/test_sparse_semi_structured.py',\n    'test/test_spectral_ops.py',\n    'test/test_stateless.py',\n    'test/test_static_runtime.py',\n    'test/test_subclass.py',\n    'test/test_sympy_utils.py',\n    'test/test_tensor_creation_ops.py',\n    'test/test_tensorboard.py',\n    'test/test_tensorexpr.py',\n    'test/test_tensorexpr_pybind.py',\n    'test/test_testing.py',\n    'test/test_torch.py',\n    'test/test_transformers.py',\n    'test/test_type_promotion.py',\n    'test/test_unary_ufuncs.py',\n    'test/test_vulkan.py',\n    'torch/_awaits/__init__.py',\n    'torch/_custom_op/__init__.py',\n    'torch/_custom_op/autograd.py',\n    'torch/_custom_op/functional.py',\n    'torch/_custom_op/impl.py',\n    'torch/_export/__init__.py',\n    'torch/_export/constraints.py',\n    'torch/_export/db/__init__.py',\n    'torch/_export/db/case.py',\n    'torch/_export/db/examples/__init__.py',\n    'torch/_export/db/examples/assume_constant_result.py',\n    'torch/_export/db/examples/autograd_function.py',\n    'torch/_export/db/examples/class_method.py',\n    'torch/_export/db/examples/cond_branch_class_method.py',\n    'torch/_export/db/examples/cond_branch_nested_function.py',\n    'torch/_export/db/examples/cond_branch_nonlocal_variables.py',\n    'torch/_export/db/examples/cond_closed_over_variable.py',\n    'torch/_export/db/examples/cond_operands.py',\n    'torch/_export/db/examples/cond_predicate.py',\n    'torch/_export/db/examples/decorator.py',\n    'torch/_export/db/examples/dictionary.py',\n    'torch/_export/db/examples/dynamic_shape_assert.py',\n    'torch/_export/db/examples/dynamic_shape_constructor.py',\n    'torch/_export/db/examples/dynamic_shape_if_guard.py',\n    'torch/_export/db/examples/dynamic_shape_map.py',\n    'torch/_export/db/examples/dynamic_shape_round.py',\n    'torch/_export/db/examples/dynamic_shape_slicing.py',\n    'torch/_export/db/examples/dynamic_shape_view.py',\n    'torch/_export/db/examples/fn_with_kwargs.py',\n    'torch/_export/db/examples/list_contains.py',\n    'torch/_export/db/examples/list_unpack.py',\n    'torch/_export/db/examples/nested_function.py',\n    'torch/_export/db/examples/null_context_manager.py',\n    'torch/_export/db/examples/pytree_flatten.py',\n    'torch/_export/db/examples/scalar_output.py',\n    'torch/_export/db/examples/specialized_attribute.py',\n    'torch/_export/db/examples/static_for_loop.py',\n    'torch/_export/db/examples/static_if.py',\n    'torch/_export/db/examples/tensor_setattr.py',\n    'torch/_export/db/examples/type_reflection_method.py',\n    'torch/_export/db/gen_example.py',\n    'torch/_export/db/logging.py',\n    'torch/_export/error.py',\n    'torch/_export/exported_program.py',\n    'torch/_export/pass_base.py',\n    'torch/_export/pass_infra/__init__.py',\n    'torch/_export/pass_infra/node_metadata.py',\n    'torch/_export/pass_infra/proxy_value.py',\n    'torch/_export/passes/__init__.py',\n    'torch/_export/passes/add_runtime_assertions_for_constraints_pass.py',\n    'torch/_export/passes/const_prop_pass.py',\n    'torch/_export/passes/functionalize_side_effectful_ops_pass.py',\n    'torch/_export/passes/replace_sym_size_ops_pass.py',\n    'torch/_export/passes/replace_view_ops_with_view_copy_ops_pass.py',\n    'torch/_export/serde/__init__.py',\n    'torch/_export/serde/schema.py',\n    'torch/_export/serde/serialize.py',\n    'torch/_export/serde/upgrade.py',\n    'torch/_export/trace.py',\n    'torch/_export/verifier.py',\n    'torch/testing/_internal/__init__.py',\n    'torch/testing/_internal/autocast_test_lists.py',\n    'torch/testing/_internal/autograd_function_db.py',\n    'torch/testing/_internal/check_kernel_launches.py',\n    'torch/testing/_internal/codegen/__init__.py',\n    'torch/testing/_internal/codegen/random_topo_test.py',\n    'torch/testing/_internal/common_cuda.py',\n    'torch/testing/_internal/common_distributed.py',\n    'torch/testing/_internal/common_jit.py',\n    'torch/testing/_internal/common_methods_invocations.py',\n    'torch/testing/_internal/common_modules.py',\n    'torch/testing/_internal/common_nn.py',\n    'torch/testing/_internal/common_pruning.py',\n    'torch/testing/_internal/common_quantization.py',\n    'torch/testing/_internal/common_quantized.py',\n    'torch/testing/_internal/common_subclass.py',\n    'torch/testing/_internal/common_utils.py',\n    'torch/testing/_internal/composite_compliance.py',\n    'torch/testing/_internal/hop_db.py',\n    'torch/testing/_internal/custom_op_db.py',\n    'torch/testing/_internal/data/__init__.py',\n    'torch/testing/_internal/data/network1.py',\n    'torch/testing/_internal/data/network2.py',\n    'torch/testing/_internal/dist_utils.py',\n    'torch/testing/_internal/distributed/__init__.py',\n    'torch/testing/_internal/distributed/_shard/__init__.py',\n    'torch/testing/_internal/distributed/_shard/sharded_tensor/__init__.py',\n    'torch/testing/_internal/distributed/_shard/sharded_tensor/_test_ops_common.py',\n    'torch/testing/_internal/distributed/_shard/sharded_tensor/_test_st_common.py',\n    'torch/testing/_internal/distributed/_shard/test_common.py',\n    'torch/testing/_internal/distributed/_tensor/__init__.py',\n    'torch/testing/_internal/distributed/_tensor/common_dtensor.py',\n    'torch/testing/_internal/distributed/ddp_under_dist_autograd_test.py',\n    'torch/testing/_internal/distributed/distributed_test.py',\n    'torch/testing/_internal/distributed/distributed_utils.py',\n    'torch/testing/_internal/distributed/fake_pg.py',\n    'torch/testing/_internal/distributed/multi_threaded_pg.py',\n    'torch/testing/_internal/distributed/nn/__init__.py',\n    'torch/testing/_internal/distributed/nn/api/__init__.py',\n    'torch/testing/_internal/distributed/nn/api/remote_module_test.py',\n    'torch/testing/_internal/distributed/rpc/__init__.py',\n    'torch/testing/_internal/distributed/rpc/dist_autograd_test.py',\n    'torch/testing/_internal/distributed/rpc/dist_optimizer_test.py',\n    'torch/testing/_internal/distributed/rpc/examples/__init__.py',\n    'torch/testing/_internal/distributed/rpc/examples/parameter_server_test.py',\n    'torch/testing/_internal/distributed/rpc/examples/reinforcement_learning_rpc_test.py',\n    'torch/testing/_internal/distributed/rpc/faulty_agent_rpc_test.py',\n    'torch/testing/_internal/distributed/rpc/faulty_rpc_agent_test_fixture.py',\n    'torch/testing/_internal/distributed/rpc/jit/__init__.py',\n    'torch/testing/_internal/distributed/rpc/jit/dist_autograd_test.py',\n    'torch/testing/_internal/distributed/rpc/jit/rpc_test.py',\n    'torch/testing/_internal/distributed/rpc/jit/rpc_test_faulty.py',\n    'torch/testing/_internal/distributed/rpc/rpc_agent_test_fixture.py',\n    'torch/testing/_internal/distributed/rpc/rpc_test.py',\n    'torch/testing/_internal/distributed/rpc/tensorpipe_rpc_agent_test_fixture.py',\n    'torch/testing/_internal/distributed/rpc_utils.py',\n    'torch/testing/_internal/generated/__init__.py',\n    'torch/testing/_internal/hypothesis_utils.py',\n    'torch/testing/_internal/inductor_utils.py',\n    'torch/testing/_internal/jit_metaprogramming_utils.py',\n    'torch/testing/_internal/jit_utils.py',\n    'torch/testing/_internal/logging_tensor.py',\n    'torch/testing/_internal/logging_utils.py',\n    'torch/testing/_internal/optests/__init__.py',\n    'torch/testing/_internal/optests/aot_autograd.py',\n    'torch/testing/_internal/optests/compile_check.py',\n    'torch/testing/_internal/optests/fake_tensor.py',\n    'torch/testing/_internal/optests/make_fx.py',\n    'torch/testing/_internal/quantization_torch_package_models.py',\n    'torch/testing/_internal/test_module/__init__.py',\n    'torch/testing/_internal/test_module/future_div.py',\n    'torch/testing/_internal/test_module/no_future_div.py',\n    'torch/utils/_contextlib.py',\n    'torch/utils/_cpp_extension_versioner.py',\n    'torch/utils/_crash_handler.py',\n    'torch/utils/_device.py',\n    'torch/utils/_foreach_utils.py',\n    'torch/utils/_freeze.py',\n    'torch/utils/_mode_utils.py',\n    'torch/utils/_python_dispatch.py',\n    'torch/utils/_stats.py',\n    'torch/utils/_traceback.py',\n    'torch/utils/_zip.py',\n    'torch/utils/backcompat/__init__.py',\n    'torch/utils/backend_registration.py',\n    'torch/utils/benchmark/__init__.py',\n    'torch/utils/benchmark/examples/__init__.py',\n    'torch/utils/benchmark/examples/blas_compare.py',\n    'torch/utils/benchmark/examples/blas_compare_setup.py',\n    'torch/utils/benchmark/examples/compare.py',\n    'torch/utils/benchmark/examples/end_to_end.py',\n    'torch/utils/benchmark/examples/fuzzer.py',\n    'torch/utils/benchmark/examples/op_benchmark.py',\n    'torch/utils/benchmark/examples/simple_timeit.py',\n    'torch/utils/benchmark/examples/sparse/compare.py',\n    'torch/utils/benchmark/examples/sparse/fuzzer.py',\n    'torch/utils/benchmark/examples/sparse/op_benchmark.py',\n    'torch/utils/benchmark/examples/spectral_ops_fuzz_test.py',\n    'torch/utils/benchmark/op_fuzzers/__init__.py',\n    'torch/utils/benchmark/op_fuzzers/binary.py',\n    'torch/utils/benchmark/op_fuzzers/sparse_binary.py',\n    'torch/utils/benchmark/op_fuzzers/sparse_unary.py',\n    'torch/utils/benchmark/op_fuzzers/spectral.py',\n    'torch/utils/benchmark/op_fuzzers/unary.py',\n    'torch/utils/benchmark/utils/__init__.py',\n    'torch/utils/benchmark/utils/_stubs.py',\n    'torch/utils/benchmark/utils/common.py',\n    'torch/utils/benchmark/utils/compare.py',\n    'torch/utils/benchmark/utils/compile.py',\n    'torch/utils/benchmark/utils/cpp_jit.py',\n    'torch/utils/benchmark/utils/fuzzer.py',\n    'torch/utils/benchmark/utils/sparse_fuzzer.py',\n    'torch/utils/benchmark/utils/timer.py',\n    'torch/utils/benchmark/utils/valgrind_wrapper/__init__.py',\n    'torch/utils/benchmark/utils/valgrind_wrapper/timer_interface.py',\n    'torch/utils/bottleneck/__init__.py',\n    'torch/utils/bottleneck/__main__.py',\n    'torch/utils/bundled_inputs.py',\n    'torch/utils/checkpoint.py',\n    'torch/utils/collect_env.py',\n    'torch/utils/cpp_backtrace.py',\n    'torch/utils/cpp_extension.py',\n    'torch/utils/dlpack.py',\n    'torch/utils/file_baton.py',\n    'torch/utils/flop_counter.py',\n    'torch/utils/hipify/__init__.py',\n    'torch/utils/hipify/constants.py',\n    'torch/utils/hipify/cuda_to_hip_mappings.py',\n    'torch/utils/hipify/hipify_python.py',\n    'torch/utils/hipify/version.py',\n    'torch/utils/hooks.py',\n    'torch/utils/jit/__init__.py',\n    'torch/utils/jit/log_extract.py',\n    'torch/utils/mkldnn.py',\n    'torch/utils/mobile_optimizer.py',\n    'torch/utils/model_dump/__init__.py',\n    'torch/utils/model_dump/__main__.py',\n    'torch/utils/model_zoo.py',\n    'torch/utils/show_pickle.py',\n    'torch/utils/tensorboard/__init__.py',\n    'torch/utils/tensorboard/_caffe2_graph.py',\n    'torch/utils/tensorboard/_convert_np.py',\n    'torch/utils/tensorboard/_embedding.py',\n    'torch/utils/tensorboard/_onnx_graph.py',\n    'torch/utils/tensorboard/_proto_graph.py',\n    'torch/utils/tensorboard/_pytorch_graph.py',\n    'torch/utils/tensorboard/_utils.py',\n    'torch/utils/tensorboard/summary.py',\n    'torch/utils/tensorboard/writer.py',\n    'torch/utils/throughput_benchmark.py',\n    'torch/utils/viz/__init__.py',\n    'torch/utils/viz/_cycles.py',\n    'torch/utils/weak.py',\n]\ninit_command = [\n    'python3',\n    'tools/linter/adapters/pip_init.py',\n    '--dry-run={{DRYRUN}}',\n    '--no-black-binary',\n    'black==23.12.1',\n    'usort==1.0.8.post1',\n    'isort==5.13.2',\n    'ruff==0.8.4',  # sync with RUFF\n]\nis_formatter = true\n\n[[linter]]\ncode = 'COPYRIGHT'\ninclude_patterns = ['**']\nexclude_patterns = [\n    '.lintrunner.toml',\n    'fb/**',\n    '**/fb/**',\n]\ncommand = [\n    'python3',\n    'tools/linter/adapters/grep_linter.py',\n    '--pattern=Confidential and proprietary',\n    '--linter-name=COPYRIGHT',\n    '--error-name=Confidential Code',\n    \"\"\"--error-description=\\\n        Proprietary and confidential source code\\\n        should not be contributed to PyTorch codebase\\\n    \"\"\",\n    '--',\n    '@{{PATHSFILE}}'\n]\n\n[[linter]]\ncode = 'BAZEL_LINTER'\ninclude_patterns = ['WORKSPACE']\ncommand = [\n    'python3',\n    'tools/linter/adapters/bazel_linter.py',\n    '--binary=.lintbin/bazel',\n    '--',\n    '@{{PATHSFILE}}'\n]\ninit_command = [\n    'python3',\n    'tools/linter/adapters/s3_init.py',\n    '--config-json=tools/linter/adapters/s3_init_config.json',\n    '--linter=bazel',\n    '--dry-run={{DRYRUN}}',\n    '--output-dir=.lintbin',\n    '--output-name=bazel',\n]\nis_formatter = true\n\n[[linter]]\ncode = 'LINTRUNNER_VERSION'\ninclude_patterns = ['**']\nexclude_patterns = [\n    'fb/**',\n    '**/fb/**',\n]\ncommand = [\n    'python3',\n    'tools/linter/adapters/lintrunner_version_linter.py'\n]\n\n[[linter]]\ncode = 'RUFF'\ninclude_patterns = ['**/*.py', '**/*.pyi']\nexclude_patterns = [\n    'caffe2/**',\n    'functorch/docs/**',\n    'functorch/notebooks/**',\n    'torch/_inductor/fx_passes/serialized_patterns/**',\n    'torch/_inductor/autoheuristic/artifacts/**',\n    'scripts/**',\n    'third_party/**',\n    'fb/**',\n    '**/fb/**',\n]\ncommand = [\n    'python3',\n    'tools/linter/adapters/ruff_linter.py',\n    '--config=pyproject.toml',\n    '--show-disable',\n    '--',\n    '@{{PATHSFILE}}'\n]\ninit_command = [\n    'python3',\n    'tools/linter/adapters/pip_init.py',\n    '--dry-run={{DRYRUN}}',\n    'ruff==0.8.4',  # sync with PYFMT\n]\nis_formatter = true\n\n# This linter prevents merge conlicts in csv files in pytorch by enforcing\n# three lines of whitespace between entries such that unless people are modifying\n# the same line, merge conflicts should not arise in git or hg\n[[linter]]\ncode = 'MERGE_CONFLICTLESS_CSV'\ninclude_patterns = ['benchmarks/dynamo/ci_expected_accuracy/*.csv']\ncommand = [\n    'python3',\n    'tools/linter/adapters/no_merge_conflict_csv_linter.py',\n    '--',\n    '@{{PATHSFILE}}'\n]\nis_formatter = true\n\n\n[[linter]]\ncode = 'META_NO_CREATE_UNBACKED'\ninclude_patterns = [\n  \"torch/_meta_registrations.py\"\n]\ncommand = [\n    'python3',\n    'tools/linter/adapters/grep_linter.py',\n    '--pattern=create_unbacked',\n    '--linter-name=META_NO_CREATE_UNBACKED',\n    '--error-name=no create_unbacked in meta registrations',\n    \"\"\"--error-description=\\\n        Data-dependent operators should have their meta \\\n        registration in torch/_subclasses/fake_impls.py, \\\n        not torch/_meta_registrations.py\n    \"\"\",\n    '--',\n    '@{{PATHSFILE}}'\n]\n\n[[linter]]\ncode = 'ATEN_CPU_GPU_AGNOSTIC'\ninclude_patterns = [\n    # aten source\n    \"aten/src/ATen/*.cpp\",\n    \"aten/src/ATen/cpu/*.cpp\",\n    \"aten/src/ATen/functorch/**/*.cpp\",\n    \"aten/src/ATen/nnapi/*.cpp\",\n    \"aten/src/ATen/quantized/*.cpp\",\n    \"aten/src/ATen/vulkan/*.cpp\",\n    \"aten/src/ATen/metal/*.cpp\",\n    \"aten/src/ATen/detail/CPUGuardImpl.cpp\",\n    \"aten/src/ATen/detail/MetaGuardImpl.cpp\",\n    # aten native source\n    \"aten/src/ATen/native/cpu/*.cpp\",\n    \"aten/src/ATen/native/ao_sparse/cpu/kernels/*.cpp\",\n    \"aten/src/ATen/native/ao_sparse/quantized/cpu/kernels/*.cpp\",\n    \"aten/src/ATen/native/quantized/cpu/kernels/*.cpp\",\n    \"aten/src/ATen/native/*.cpp\",\n    \"aten/src/ATen/native/cpu/**/*.cpp\",\n    \"aten/src/ATen/native/ao_sparse/*.cpp\",\n    \"aten/src/ATen/native/ao_sparse/**/*.cpp\",\n    \"aten/src/ATen/native/ao_sparse/quantized/*.cpp\",\n    \"aten/src/ATen/native/ao_sparse/quantized/**/*.cpp\",\n    \"aten/src/ATen/native/nested/*.cpp\",\n    \"aten/src/ATen/native/quantized/*.cpp\",\n    \"aten/src/ATen/native/quantized/**/*.cpp\",\n    \"aten/src/ATen/native/sparse/*.cpp\",\n    \"aten/src/ATen/native/transformers/*.cpp\",\n    \"aten/src/ATen/native/utils/*.cpp\",\n    \"aten/src/ATen/native/xnnpack/*.cpp\",\n    \"aten/src/ATen/native/metal/MetalPrepackOpRegister.cpp\",\n    # aten headers\n    \"aten/src/ATen/*.h\",\n    \"aten/src/ATen/functorch/**/*.h\",\n    \"aten/src/ATen/ops/*.h\",\n    \"aten/src/ATen/cpu/**/*.h\",\n    \"aten/src/ATen/nnapi/*.h\",\n    \"aten/src/ATen/quantized/*.h\",\n    \"aten/src/ATen/vulkan/*.h\",\n    \"aten/src/ATen/metal/*.h\",\n    \"aten/src/ATen/mps/*.h\",\n    # aten native headers\n    \"aten/src/ATen/native/*.h\",\n    \"aten/src/ATen/native/cpu/**/*.h\",\n    \"aten/src/ATen/native/nested/*.h\",\n    \"aten/src/ATen/native/sparse/*.h\",\n    \"aten/src/ATen/native/ao_sparse/*.h\",\n    \"aten/src/ATen/native/ao_sparse/cpu/*.h\",\n    \"aten/src/ATen/native/ao_sparse/quantized/*.h\",\n    \"aten/src/ATen/native/ao_sparse/quantized/cpu/*.h\",\n    \"aten/src/ATen/native/quantized/*.h\",\n    \"aten/src/ATen/native/quantized/cpu/*.h\",\n    \"aten/src/ATen/native/transformers/*.h\",\n    \"aten/src/ATen/native/quantized/cpu/qnnpack/include/*.h\",\n    \"aten/src/ATen/native/utils/*.h\",\n    \"aten/src/ATen/native/vulkan/ops/*.h\",\n    \"aten/src/ATen/native/xnnpack/*.h\",\n    \"aten/src/ATen/native/metal/MetalPrepackOpContext.h\",\n    \"aten/src/ATen/native/mps/Copy.h\",\n    \"aten/src/ATen/native/mkldnn/**/*.h\",\n]\nexclude_patterns = [\n    \"aten/src/ATen/Context.h\",\n    \"aten/src/ATen/Context.cpp\",\n    \"aten/src/ATen/DLConvertor.cpp\",\n    \"aten/src/ATen/core/Array.h\",\n    \"aten/src/ATen/native/quantized/ConvUtils.h\",\n    \"aten/src/ATen/native/sparse/SparseBlasImpl.cpp\",  # triton implementation\n    \"aten/src/ATen/native/transformers/attention.cpp\",\n    \"aten/src/ATen/native/**/cudnn/**\",  # cudnn is cuda specific\n]\ncommand = [\n    'python3',\n    'tools/linter/adapters/grep_linter.py',\n    '--pattern=(^#if.*USE_ROCM.*)|(^#if.*USE_CUDA.*)',\n    '--linter-name=ATEN_CPU',\n    '--error-name=aten-cpu should be gpu agnostic',\n    \"\"\"--error-description=\\\n        We strongly discourage the compile-time divergence \\\n        on ATen-CPU code for different GPU code. This \\\n        disallows sharing the same aten-cpu shared object \\\n        between different GPU backends \\\n    \"\"\",\n    '--',\n    '@{{PATHSFILE}}'\n]\nis_formatter = true\n\n# `set_linter` detects occurrences of built-in `set` in areas of Python code like\n# _inductor where the instability of iteration in `set` has proven a problem.\n\n[[linter]]\ncode = 'SET_LINTER'\ncommand = [\n    'python3',\n    'tools/linter/adapters/set_linter.py',\n    '--lintrunner',\n    '--',\n    '@{{PATHSFILE}}'\n]\ninclude_patterns = [\n    \"torch/_inductor/**/*.py\",\n]\nis_formatter = true\n\n# `docstring_linter` reports on long Python classes, methods, and functions\n# whose definitions have very small docstrings or none at all.\n#\n[[linter]]\ncode = 'DOCSTRING_LINTER'\ncommand = [\n    'python3',\n    'tools/linter/adapters/docstring_linter.py',\n    '--lintrunner',\n    '--',\n    '@{{PATHSFILE}}'\n]\ninclude_patterns = [\n   'torch/**/not-exist.py'\n]\nis_formatter = false\n\n# `import_linter` reports on importing disallowed third party libraries.\n[[linter]]\ncode = 'IMPORT_LINTER'\ncommand = [\n    'python3',\n    'tools/linter/adapters/import_linter.py',\n    '--',\n    '@{{PATHSFILE}}'\n]\ninclude_patterns = [\n   'torch/_dynamo/**',\n]\nis_formatter = false\n"
        },
        {
          "name": ".lldbinit",
          "type": "blob",
          "size": 0.77,
          "content": "# automatically load the pytorch_lldb extension.\n#\n# lldb automatically tries to load this file whenever it is executed from the\n# root of the pytorch repo, but by default it is not allowed to do so due to\n# security reasons. If you want to use pytorch_lldb, please add the following\n# line to your ~/.lldbinit (i.e., the .lldbinit file which is in your home\n# directory, NOT this file):\n#    settings set target.load-cwd-lldbinit true\n#    setting set escape-non-printables false\n#\n# Alternatively, you can manually load the pytorch_lldb  commands into your\n# existing lldb session by doing the following:\n#    (lldb) command script import tools/lldb/pytorch_lldb.py\n\ncommand script import tools/lldb/pytorch_lldb.py\nsetting set escape-non-printables false\ntype category enable torch\n"
        },
        {
          "name": ".vscode",
          "type": "tree",
          "content": null
        },
        {
          "name": "BUCK.oss",
          "type": "blob",
          "size": 4.01,
          "content": "load(\"//tools/build_defs:glob_defs.bzl\", \"subdir_glob\")\nload(\n    \":pt_ops.bzl\",\n    \"pt_operator_library\",\n)\nload(\":buckbuild.bzl\",\n    \"define_buck_targets\",\n    \"get_pt_operator_registry_dict\",\n)\n\n# define shared buck targets\ndefine_buck_targets()\n\n# define OSS only targets\ncxx_library(\n    name = \"pthreadpool\",\n    srcs = ['caffe2/utils/threadpool/pthreadpool.cc', 'caffe2/utils/threadpool/pthreadpool_impl.cc', 'caffe2/utils/threadpool/pthreadpool-cpp.cc', 'caffe2/utils/threadpool/thread_pool_guard.cpp', 'caffe2/utils/threadpool/ThreadPool.cc'],\n    deps = [':caffe2_headers', '//third_party:cpuinfo', '//third_party:glog', '//c10:c10', '//third_party:FXdiv'],\n    exported_deps = ['//third_party:pthreadpool'],\n    compiler_flags = ['-Wno-unused-function'],\n    preferred_linkage = \"static\",\n    exported_headers = subdir_glob([(\"\", \"caffe2/utils/threadpool/*.h\")]),\n    exported_preprocessor_flags = ['-DUSE_PTHREADPOOL'],\n    header_namespace = \"\",\n    headers = [],\n    link_whole = True,\n    platform_preprocessor_flags = [['windows', ['-D_WINDOWS', '-D_WIN32', '-DWIN32', '-DNOMINMAX', '-D_CRT_SECURE_NO_WARNINGS', '-D_USE_MATH_DEFINES']], ['windows.*64$', ['-D_WIN64']]],\n    visibility = ['PUBLIC'],\n)\n\ncxx_library(\n    name = \"caffe2_headers\",\n    deps = ['//c10:c10'],\n    exported_headers = subdir_glob(\n        [\n            (\"\", \"caffe2/**/*.h\"),\n            (\"\", \"binaries/**/*.h\"),\n            (\"modules\", \"**/*.h\"),\n            (\"aten/src\", \"ATen/core/**/*.h\"),\n        ],\n        exclude = [\n            \"caffe2/fb/**/*.h\",\n            \"caffe2/mobile/contrib/libopencl-stub/**/*.h\",\n            \"caffe2/mobile/contrib/libvulkan-stub/**/*.h\",\n            \"caffe2/mobile/contrib/nnapi/**/*.h\",\n            \"caffe2/mobile/fb/binary/**/*.h\",\n            \"caffe2/mobile/fb/snpe_so/**/*.h\",\n            \"caffe2/mobile/fb/boltnn/bolt_lib/include/**/*.h\",\n            \"caffe2/mobile/contrib/snpe/**/*.h\",\n            \"caffe2/mobile/fb/qpl/jni/QuickPerformanceLogger.h\",\n            \"caffe2/share/fb/x3d/ldi/*.h\",\n            \"**/*.pb.h\",\n        ],\n    ),\n    compiler_flags = ['-Os', '-fexceptions', '-frtti', '-Wno-shadow', '-Wno-unknown-pragmas', '-Wno-unused-variable', '-Wno-sign-compare', '-Icaffe2', '-Imodules', '-DEIGEN_NO_DEBUG', '-DCAFFE2_USE_LITE_PROTO', '-DCAFFE2_USE_GOOGLE_GLOG', '-DCAFFE2_RNN_NO_TEXT_FORMAT', '-DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK=1', '-DCAFFE2_IS_XPLAT_BUILD', '-DSTRIP_ERROR_MESSAGES', '-DUSE_INTERNAL_PTHREADPOOL_IMPL', '-DCAFFE2_USE_HPTT'],\n    preferred_linkage = \"static\",\n    platform_preprocessor_flags = [['windows', ['-D_WINDOWS', '-D_WIN32', '-DWIN32', '-DNOMINMAX', '-D_CRT_SECURE_NO_WARNINGS', '-D_USE_MATH_DEFINES']], ['windows.*64$', ['-D_WIN64']]],\n    preprocessor_flags = ['-DUSE_INTERNAL_PTHREADPOOL_IMPL'],\n    visibility = ['PUBLIC'],\n)\n\ncxx_library(\n    name = \"caffe2_serialize\",\n    srcs = [\n        \"caffe2/serialize/file_adapter.cc\",\n        \"caffe2/serialize/inline_container.cc\",\n        \"caffe2/serialize/istream_adapter.cc\",\n        \"caffe2/serialize/read_adapter_interface.cc\",\n    ],\n    visibility = [\"PUBLIC\"],\n    deps = [\n        \":caffe2_headers\",\n        \"//third_party:glog\",\n        \"//c10:c10\",\n        \"//third_party:miniz\",\n    ],\n)\n\npt_operator_library(\n    name = \"torch_mobile_ops_full_dev\",\n    include_all_operators = True,\n)\n\ncxx_library(\n    name = \"pt_ops_full\",\n    **get_pt_operator_registry_dict(\n        name = \"pt_ops_full\",\n        deps = [\n            \":torch_mobile_ops_full_dev\",\n        ],\n    )\n)\n\ncxx_binary(\n    name = 'ptmobile_benchmark',\n    srcs = [\n        'binaries/speed_benchmark_torch.cc',\n    ],\n    compiler_flags = [\n        \"-fexceptions\",\n        \"-frtti\",\n        \"-Wno-deprecated-declarations\",\n    ],\n    preprocessor_flags = [\n        \"-DBUILD_LITE_INTERPRETER\",\n    ],\n    platform_linker_flags = [\n        (\n            \"^linux.*$\",\n            [\n                \"-Wl,--no-as-needed\",\n                \"-ldl\",\n                \"-pthread\",\n            ],\n        ),\n    ],\n    deps = [\n        \":torch_mobile_core\",\n        \":pt_ops_full\",\n        \"//c10:c10\",\n    ],\n)\n"
        },
        {
          "name": "BUILD.bazel",
          "type": "blob",
          "size": 28.16,
          "content": "load(\"@bazel_skylib//lib:paths.bzl\", \"paths\")\nload(\"@pybind11_bazel//:build_defs.bzl\", \"pybind_extension\")\nload(\"@rules_cc//cc:defs.bzl\", \"cc_binary\", \"cc_library\", \"cc_test\")\nload(\"@rules_python//python:defs.bzl\", \"py_library\", \"py_test\")\nload(\"@pytorch//third_party:substitution.bzl\", \"header_template_rule\", \"template_rule\")\nload(\"@pytorch//:tools/bazel.bzl\", \"rules\")\nload(\"@pytorch//tools/rules:cu.bzl\", \"cu_library\")\nload(\"@pytorch//tools/config:defs.bzl\", \"if_cuda\")\nload(\"@pytorch//:aten.bzl\", \"generate_aten\", \"intern_build_aten_ops\")\nload(\":build.bzl\", \"GENERATED_AUTOGRAD_CPP\", \"GENERATED_AUTOGRAD_PYTHON\", \"define_targets\")\nload(\":build_variables.bzl\", \"jit_core_sources\", \"lazy_tensor_ts_sources\", \"libtorch_core_sources\", \"libtorch_cuda_sources\", \"libtorch_distributed_sources\", \"libtorch_extra_sources\", \"libtorch_python_core_sources\", \"torch_cpp_srcs\", \"libtorch_python_cuda_sources\", \"libtorch_python_distributed_sources\")\nload(\":ufunc_defs.bzl\", \"aten_ufunc_generated_cpu_kernel_sources\", \"aten_ufunc_generated_cpu_sources\", \"aten_ufunc_generated_cuda_sources\")\nload(\"//:tools/bazel.bzl\", \"rules\")\n\ndefine_targets(rules = rules)\n\nCOMMON_COPTS = [\n    \"-DHAVE_MALLOC_USABLE_SIZE=1\",\n    \"-DHAVE_MMAP=1\",\n    \"-DHAVE_SHM_OPEN=1\",\n    \"-DHAVE_SHM_UNLINK=1\",\n    \"-D_FILE_OFFSET_BITS=64\",\n    \"-DUSE_FBGEMM\",\n    \"-DUSE_DISTRIBUTED\",\n    \"-DAT_PER_OPERATOR_HEADERS\",\n    \"-DATEN_THREADING=NATIVE\",\n    \"-DNO_CUDNN_DESTROY_HANDLE\",\n] + if_cuda([\n    \"-DUSE_CUDA\",\n    \"-DUSE_CUDNN\",\n    # TODO: This should be passed only when building for CUDA-11.5 or newer\n    # use cub in a safe manner, see:\n    # https://github.com/pytorch/pytorch/pull/55292\n    \"-DCUB_WRAPPED_NAMESPACE=at_cuda_detail\",\n])\n\naten_generation_srcs = [\"aten/src/ATen/native/native_functions.yaml\"] + [\"aten/src/ATen/native/tags.yaml\"] + glob([\"aten/src/ATen/templates/**\"])\n\ngenerated_cpu_cpp = [\n    \"aten/src/ATen/RegisterBackendSelect.cpp\",\n    \"aten/src/ATen/RegisterCPU_0.cpp\",\n    \"aten/src/ATen/RegisterCPU_1.cpp\",\n    \"aten/src/ATen/RegisterCPU_2.cpp\",\n    \"aten/src/ATen/RegisterCPU_3.cpp\",\n    \"aten/src/ATen/RegisterFunctionalization_0.cpp\",\n    \"aten/src/ATen/RegisterFunctionalization_1.cpp\",\n    \"aten/src/ATen/RegisterFunctionalization_2.cpp\",\n    \"aten/src/ATen/RegisterFunctionalization_3.cpp\",\n    # \"aten/src/ATen/RegisterFunctionalizationEverything.cpp\",\n    \"aten/src/ATen/RegisterMkldnnCPU_0.cpp\",\n    \"aten/src/ATen/RegisterNestedTensorCPU_0.cpp\",\n    \"aten/src/ATen/RegisterQuantizedCPU_0.cpp\",\n    \"aten/src/ATen/RegisterSparseCPU_0.cpp\",\n    \"aten/src/ATen/RegisterSparseCsrCPU_0.cpp\",\n    \"aten/src/ATen/RegisterZeroTensor_0.cpp\",\n    \"aten/src/ATen/RegisterCompositeImplicitAutograd_0.cpp\",\n    \"aten/src/ATen/RegisterCompositeImplicitAutogradNestedTensor_0.cpp\",\n    \"aten/src/ATen/RegisterCompositeExplicitAutograd_0.cpp\",\n    \"aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp\",\n    \"aten/src/ATen/RegisterMeta_0.cpp\",\n    \"aten/src/ATen/RegisterSparseMeta_0.cpp\",\n    \"aten/src/ATen/RegisterQuantizedMeta_0.cpp\",\n    \"aten/src/ATen/RegisterNestedTensorMeta_0.cpp\",\n    \"aten/src/ATen/RegisterSchema.cpp\",\n    \"aten/src/ATen/CPUFunctions.h\",\n    \"aten/src/ATen/CPUFunctions_inl.h\",\n    \"aten/src/ATen/CompositeExplicitAutogradFunctions.h\",\n    \"aten/src/ATen/CompositeExplicitAutogradFunctions_inl.h\",\n    \"aten/src/ATen/CompositeExplicitAutogradNonFunctionalFunctions.h\",\n    \"aten/src/ATen/CompositeExplicitAutogradNonFunctionalFunctions_inl.h\",\n    \"aten/src/ATen/CompositeImplicitAutogradFunctions.h\",\n    \"aten/src/ATen/CompositeImplicitAutogradFunctions_inl.h\",\n    \"aten/src/ATen/CompositeImplicitAutogradNestedTensorFunctions.h\",\n    \"aten/src/ATen/CompositeImplicitAutogradNestedTensorFunctions_inl.h\",\n    \"aten/src/ATen/CompositeViewCopyKernels.cpp\",\n    \"aten/src/ATen/FunctionalInverses.h\",\n    \"aten/src/ATen/Functions.h\",\n    \"aten/src/ATen/Functions.cpp\",\n    \"aten/src/ATen/RedispatchFunctions.h\",\n    \"aten/src/ATen/Operators.h\",\n    \"aten/src/ATen/Operators_0.cpp\",\n    \"aten/src/ATen/Operators_1.cpp\",\n    \"aten/src/ATen/Operators_2.cpp\",\n    \"aten/src/ATen/Operators_3.cpp\",\n    \"aten/src/ATen/Operators_4.cpp\",\n    \"aten/src/ATen/NativeFunctions.h\",\n    \"aten/src/ATen/MetaFunctions.h\",\n    \"aten/src/ATen/MetaFunctions_inl.h\",\n    \"aten/src/ATen/MethodOperators.h\",\n    \"aten/src/ATen/NativeMetaFunctions.h\",\n    \"aten/src/ATen/RegistrationDeclarations.h\",\n    \"aten/src/ATen/VmapGeneratedPlumbing.h\",\n    \"aten/src/ATen/core/aten_interned_strings.h\",\n    \"aten/src/ATen/core/enum_tag.h\",\n    \"aten/src/ATen/core/TensorBody.h\",\n    \"aten/src/ATen/core/TensorMethods.cpp\",\n    \"aten/src/ATen/core/ATenOpList.cpp\",\n]\n\ngenerated_cuda_cpp = [\n    \"aten/src/ATen/CUDAFunctions.h\",\n    \"aten/src/ATen/CUDAFunctions_inl.h\",\n    \"aten/src/ATen/RegisterCUDA_0.cpp\",\n    \"aten/src/ATen/RegisterNestedTensorCUDA_0.cpp\",\n    \"aten/src/ATen/RegisterQuantizedCUDA_0.cpp\",\n    \"aten/src/ATen/RegisterSparseCUDA_0.cpp\",\n    \"aten/src/ATen/RegisterSparseCsrCUDA_0.cpp\",\n]\n\ngenerate_aten(\n    name = \"generated_aten_cpp\",\n    srcs = aten_generation_srcs,\n    outs = (\n        generated_cpu_cpp +\n        generated_cuda_cpp +\n        aten_ufunc_generated_cpu_sources(\"aten/src/ATen/{}\") +\n        aten_ufunc_generated_cpu_kernel_sources(\"aten/src/ATen/{}\") +\n        aten_ufunc_generated_cuda_sources(\"aten/src/ATen/{}\") + [\n            \"aten/src/ATen/Declarations.yaml\",\n        ]\n    ),\n    generator = \"//torchgen:gen\",\n)\n\nfilegroup(\n    name = \"cpp_generated_code\",\n    srcs = GENERATED_AUTOGRAD_CPP,\n    data = [\":generate-code\"],\n)\n\n# ATen\nfilegroup(\n    name = \"aten_base_cpp\",\n    srcs = glob([\n        \"aten/src/ATen/*.cpp\",\n        \"aten/src/ATen/functorch/*.cpp\",\n        \"aten/src/ATen/detail/*.cpp\",\n        \"aten/src/ATen/cpu/*.cpp\",\n    ]),\n)\n\nfilegroup(\n    name = \"ATen_CORE_SRCS\",\n    srcs = glob(\n        [\n            \"aten/src/ATen/core/**/*.cpp\",\n        ],\n        exclude = [\n            \"aten/src/ATen/core/**/*_test.cpp\",\n        ],\n    ),\n)\n\nfilegroup(\n    name = \"aten_native_cpp\",\n    srcs = glob([\"aten/src/ATen/native/*.cpp\"]),\n)\n\nfilegroup(\n    name = \"aten_native_sparse_cpp\",\n    srcs = glob([\"aten/src/ATen/native/sparse/*.cpp\"]),\n)\n\nfilegroup(\n    name = \"aten_native_nested_cpp\",\n    srcs = glob([\"aten/src/ATen/native/nested/*.cpp\"]),\n)\n\nfilegroup(\n    name = \"aten_native_quantized_cpp\",\n    srcs = glob(\n        [\n            \"aten/src/ATen/native/quantized/*.cpp\",\n            \"aten/src/ATen/native/quantized/cpu/*.cpp\",\n        ],\n    ),\n)\n\nfilegroup(\n    name = \"aten_native_transformers_cpp\",\n    srcs = glob([\"aten/src/ATen/native/transformers/*.cpp\"]),\n)\n\nfilegroup(\n    name = \"aten_native_mkl_cpp\",\n    srcs = glob([\n        \"aten/src/ATen/native/mkl/*.cpp\",\n        \"aten/src/ATen/mkl/*.cpp\",\n    ]),\n)\n\nfilegroup(\n    name = \"aten_native_mkldnn_cpp\",\n    srcs = glob([\"aten/src/ATen/native/mkldnn/*.cpp\"]),\n)\n\nfilegroup(\n    name = \"aten_native_xnnpack\",\n    srcs = glob([\"aten/src/ATen/native/xnnpack/*.cpp\"]),\n)\n\nfilegroup(\n    name = \"aten_base_vulkan\",\n    srcs = glob([\"aten/src/ATen/vulkan/*.cpp\"]),\n)\n\nfilegroup(\n    name = \"aten_base_metal\",\n    srcs = glob([\"aten/src/ATen/metal/*.cpp\"]),\n)\n\nfilegroup(\n    name = \"ATen_QUANTIZED_SRCS\",\n    srcs = glob(\n        [\n            \"aten/src/ATen/quantized/**/*.cpp\",\n        ],\n        exclude = [\n            \"aten/src/ATen/quantized/**/*_test.cpp\",\n        ],\n    ),\n)\n\nfilegroup(\n    name = \"aten_cuda_cpp_srcs\",\n    srcs = glob(\n        [\n            \"aten/src/ATen/cuda/*.cpp\",\n            \"aten/src/ATen/cuda/detail/*.cpp\",\n            \"aten/src/ATen/cuda/tunable/*.cpp\",\n            \"aten/src/ATen/cudnn/*.cpp\",\n            \"aten/src/ATen/native/cuda/*.cpp\",\n            \"aten/src/ATen/native/cuda/linalg/*.cpp\",\n            \"aten/src/ATen/native/cudnn/*.cpp\",\n            \"aten/src/ATen/native/miopen/*.cpp\",\n            \"aten/src/ATen/native/nested/cuda/*.cpp\",\n            \"aten/src/ATen/native/quantized/cuda/*.cpp\",\n            \"aten/src/ATen/native/quantized/cudnn/*.cpp\",\n            \"aten/src/ATen/native/sparse/cuda/*.cpp\",\n            \"aten/src/ATen/native/transformers/cuda/*.cpp\",\n        ],\n    ),\n)\n\nfilegroup(\n    name = \"aten_cu_srcs\",\n    srcs = glob([\n        \"aten/src/ATen/cuda/*.cu\",\n        \"aten/src/ATen/cuda/detail/*.cu\",\n        \"aten/src/ATen/native/cuda/*.cu\",\n        \"aten/src/ATen/native/nested/cuda/*.cu\",\n        \"aten/src/ATen/native/quantized/cuda/*.cu\",\n        \"aten/src/ATen/native/sparse/cuda/*.cu\",\n        \"aten/src/ATen/native/transformers/cuda/*.cu\",\n    ]) + aten_ufunc_generated_cuda_sources(\"aten/src/ATen/{}\"),\n    # It's a bit puzzling to me why it's not necessary to declare the\n    # target that generates these sources...\n)\n\n# TODO: Enable support for KleidiAI bazel build\nheader_template_rule(\n    name = \"aten_src_ATen_config\",\n    src = \"aten/src/ATen/Config.h.in\",\n    out = \"aten/src/ATen/Config.h\",\n    include = \"aten/src\",\n    substitutions = {\n        \"@AT_MKLDNN_ENABLED@\": \"1\",\n        \"@AT_MKLDNN_ACL_ENABLED@\": \"0\",\n        \"@AT_MKL_ENABLED@\": \"1\",\n        \"@AT_MKL_SEQUENTIAL@\": \"0\",\n        \"@AT_POCKETFFT_ENABLED@\": \"0\",\n        \"@AT_NNPACK_ENABLED@\": \"0\",\n        \"@CAFFE2_STATIC_LINK_CUDA_INT@\": \"0\",\n        \"@AT_BUILD_WITH_BLAS@\": \"1\",\n        \"@AT_BUILD_WITH_LAPACK@\": \"1\",\n        \"@AT_PARALLEL_OPENMP@\": \"0\",\n        \"@AT_PARALLEL_NATIVE@\": \"1\",\n        \"@AT_BLAS_F2C@\": \"0\",\n        \"@AT_BLAS_USE_CBLAS_DOT@\": \"1\",\n        \"@AT_KLEIDIAI_ENABLED@\": \"0\",\n    },\n)\n\nheader_template_rule(\n    name = \"aten_src_ATen_cuda_config\",\n    src = \"aten/src/ATen/cuda/CUDAConfig.h.in\",\n    out = \"aten/src/ATen/cuda/CUDAConfig.h\",\n    include = \"aten/src\",\n    substitutions = {\n        \"@AT_CUDNN_ENABLED@\": \"1\",\n        \"@AT_CUSPARSELT_ENABLED@\": \"0\",\n        \"@AT_ROCM_ENABLED@\": \"0\",\n        \"@AT_MAGMA_ENABLED@\": \"0\",\n        \"@NVCC_FLAGS_EXTRA@\": \"\",\n    },\n)\n\ncc_library(\n    name = \"aten_headers\",\n    hdrs = [\n        \"torch/csrc/Export.h\",\n        \"torch/csrc/jit/frontend/function_schema_parser.h\",\n    ] + glob(\n        [\n            \"aten/src/**/*.h\",\n            \"aten/src/**/*.hpp\",\n            \"aten/src/ATen/cuda/**/*.cuh\",\n            \"aten/src/ATen/native/**/*.cuh\",\n            \"aten/src/THC/*.cuh\",\n        ],\n    ) + [\n        \":aten_src_ATen_config\",\n        \":generated_aten_cpp\",\n    ],\n    includes = [\n        \"aten/src\",\n    ],\n    deps = [\n        \"//c10\",\n    ],\n)\n\nATEN_COPTS = COMMON_COPTS + [\n    \"-DCAFFE2_BUILD_MAIN_LIBS\",\n    \"-DHAVE_AVX_CPU_DEFINITION\",\n    \"-DHAVE_AVX2_CPU_DEFINITION\",\n    \"-fvisibility-inlines-hidden\",\n    \"-fno-math-errno\",\n    \"-fno-trapping-math\",\n]\n\nintern_build_aten_ops(\n    copts = ATEN_COPTS,\n    extra_impls = aten_ufunc_generated_cpu_kernel_sources(\"aten/src/ATen/{}\"),\n    deps = [\n        \":aten_headers\",\n        \"@fbgemm\",\n        \"@mkl\",\n        \"@sleef\",\n        \"@mkl_dnn//:mkl-dnn\",\n    ],\n)\n\ncc_library(\n    name = \"aten\",\n    srcs = [\n        \":ATen_CORE_SRCS\",\n        \":ATen_QUANTIZED_SRCS\",\n        \":aten_base_cpp\",\n        \":aten_base_metal\",\n        \":aten_base_vulkan\",\n        \":aten_native_cpp\",\n        \":aten_native_mkl_cpp\",\n        \":aten_native_mkldnn_cpp\",\n        \":aten_native_nested_cpp\",\n        \":aten_native_quantized_cpp\",\n        \":aten_native_sparse_cpp\",\n        \":aten_native_transformers_cpp\",\n        \":aten_native_xnnpack\",\n        \":aten_src_ATen_config\",\n    ] + generated_cpu_cpp + aten_ufunc_generated_cpu_sources(\"aten/src/ATen/{}\"),\n    copts = ATEN_COPTS,\n    linkopts = [\n      \"-ldl\",\n    ],\n    data = if_cuda(\n        [\":libcaffe2_nvrtc.so\"],\n        [],\n    ),\n    visibility = [\"//visibility:public\"],\n    deps = [\n        \":ATen_CPU\",\n        \":aten_headers\",\n        \":caffe2_for_aten_headers\",\n        \":torch_headers\",\n        \"@fbgemm\",\n        \"@ideep\",\n    ],\n    alwayslink = True,\n)\n\ncc_library(\n    name = \"aten_nvrtc\",\n    srcs = glob([\n        \"aten/src/ATen/cuda/nvrtc_stub/*.cpp\",\n    ]),\n    copts = ATEN_COPTS,\n    linkstatic = True,\n    visibility = [\"//visibility:public\"],\n    deps = [\n        \":aten_headers\",\n        \"//c10\",\n        \"@cuda\",\n        \"@cuda//:cuda_driver\",\n        \"@cuda//:nvrtc\",\n    ],\n    alwayslink = True,\n)\n\ncc_binary(\n    name = \"libcaffe2_nvrtc.so\",\n    linkshared = True,\n    visibility = [\"//visibility:public\"],\n    deps = [\n        \":aten_nvrtc\",\n    ],\n)\n\ncc_library(\n    name = \"aten_cuda_cpp\",\n    srcs = [\":aten_cuda_cpp_srcs\"] + generated_cuda_cpp,\n    hdrs = [\":aten_src_ATen_cuda_config\"],\n    copts = ATEN_COPTS,\n    visibility = [\"//visibility:public\"],\n    deps = [\n        \":aten\",\n        \"@cuda\",\n        \"@cuda//:cusolver\",\n        \"@cuda//:nvrtc\",\n        \"@cudnn\",\n        \"@cudnn_frontend\",\n    ],\n    alwayslink = True,\n)\n\ntorch_cuda_half_options = [\n    \"-DCUDA_HAS_FP16=1\",\n    \"-D__CUDA_NO_HALF_OPERATORS__\",\n    \"-D__CUDA_NO_HALF_CONVERSIONS__\",\n    \"-D__CUDA_NO_BFLOAT16_CONVERSIONS__\",\n    \"-D__CUDA_NO_HALF2_OPERATORS__\",\n]\n\ncu_library(\n    name = \"aten_cuda\",\n    srcs = [\":aten_cu_srcs\"],\n    copts = ATEN_COPTS + torch_cuda_half_options,\n    visibility = [\"//visibility:public\"],\n    deps = [\n        \":aten_cuda_cpp\",\n        \"//c10/util:bit_cast\",\n        \"@cuda//:cublas\",\n        \"@cuda//:cufft\",\n        \"@cuda//:cusparse\",\n        \"@cutlass\",\n    ],\n    alwayslink = True,\n)\n\n# caffe2\nCAFFE2_COPTS = COMMON_COPTS + [\n    \"-Dcaffe2_EXPORTS\",\n    \"-DCAFFE2_USE_CUDNN\",\n    \"-DCAFFE2_BUILD_MAIN_LIB\",\n    \"-fvisibility-inlines-hidden\",\n    \"-fno-math-errno\",\n    \"-fno-trapping-math\",\n]\n\nfilegroup(\n    name = \"caffe2_core_srcs\",\n    srcs = [\n        \"caffe2/core/common.cc\",\n    ],\n)\n\nfilegroup(\n    name = \"caffe2_perfkernels_srcs\",\n    srcs = [\n        \"caffe2/perfkernels/embedding_lookup_idx.cc\",\n    ],\n)\n\n\nfilegroup(\n    name = \"caffe2_serialize_srcs\",\n    srcs = [\n        \"caffe2/serialize/file_adapter.cc\",\n        \"caffe2/serialize/inline_container.cc\",\n        \"caffe2/serialize/istream_adapter.cc\",\n        \"caffe2/serialize/read_adapter_interface.cc\",\n    ],\n)\n\nfilegroup(\n    name = \"caffe2_utils_srcs\",\n    srcs = [\n        \"caffe2/utils/proto_wrap.cc\",\n        \"caffe2/utils/string_utils.cc\",\n        \"caffe2/utils/threadpool/ThreadPool.cc\",\n        \"caffe2/utils/threadpool/pthreadpool.cc\",\n        \"caffe2/utils/threadpool/pthreadpool_impl.cc\",\n        \"caffe2/utils/threadpool/thread_pool_guard.cpp\",\n    ],\n)\n\n# To achieve finer granularity and make debug easier, caffe2 is split into three libraries:\n# ATen, caffe2 and caffe2_for_aten_headers. ATen lib group up source codes under\n# aten/ directory and caffe2 contains most files under `caffe2/` directory. Since the\n# ATen lib and the caffe2 lib would depend on each other, `caffe2_for_aten_headers` is splitted\n# out from `caffe2` to avoid dependency cycle.\ncc_library(\n    name = \"caffe2_for_aten_headers\",\n    hdrs = [\n        \"caffe2/core/common.h\",\n        \"caffe2/perfkernels/common.h\",\n        \"caffe2/perfkernels/embedding_lookup_idx.h\",\n        \"caffe2/utils/fixed_divisor.h\",\n    ] + glob([\n        \"caffe2/utils/threadpool/*.h\",\n    ]),\n    copts = CAFFE2_COPTS,\n    visibility = [\"//visibility:public\"],\n    deps = [\n        \":caffe2_core_macros\",\n        \"//c10\",\n    ],\n)\n\ncc_library(\n    name = \"caffe2_headers\",\n    hdrs = glob(\n        [\n            \"caffe2/perfkernels/*.h\",\n            \"caffe2/serialize/*.h\",\n            \"caffe2/utils/*.h\",\n            \"caffe2/utils/threadpool/*.h\",\n            \"modules/**/*.h\",\n        ],\n        exclude = [\n            \"caffe2/core/macros.h\",\n        ],\n    ) + if_cuda(glob([\n        \"caffe2/**/*.cuh\",\n    ])),\n    copts = CAFFE2_COPTS,\n    visibility = [\"//visibility:public\"],\n    deps = [\n        \":caffe2_core_macros\",\n        \":caffe2_for_aten_headers\",\n    ],\n)\n\ncc_library(\n    name = \"caffe2\",\n    srcs = [\n        \":caffe2_core_srcs\",\n        \":caffe2_perfkernels_srcs\",\n        \":caffe2_serialize_srcs\",\n        \":caffe2_utils_srcs\",\n    ],\n    copts = CAFFE2_COPTS + [\"-mf16c\"],\n    linkstatic = 1,\n    visibility = [\"//visibility:public\"],\n    deps = [\n        \":caffe2_core_macros\",\n        \":caffe2_headers\",\n        \":caffe2_perfkernels_avx\",\n        \":caffe2_perfkernels_avx2\",\n        \"//third_party/miniz-3.0.2:miniz\",\n        \"@com_google_protobuf//:protobuf\",\n        \"@eigen\",\n        \"@fbgemm//:fbgemm_src_headers\",\n        \"@fmt\",\n        \"@onnx\",\n    ] + if_cuda(\n        [\n            \":aten_cuda\",\n            \"@tensorpipe//:tensorpipe_cuda\",\n        ],\n        [\n            \":aten\",\n            \"@tensorpipe//:tensorpipe_cpu\",\n        ],\n    ),\n    alwayslink = True,\n)\n\ncu_library(\n    name = \"torch_cuda\",\n    srcs = [\n        \"torch/csrc/distributed/c10d/intra_node_comm.cu\",\n        \"torch/csrc/distributed/c10d/NanCheck.cu\",\n        \"torch/csrc/distributed/c10d/quantization/quantization_gpu.cu\",\n    ],\n    copts = torch_cuda_half_options,\n    visibility = [\"//visibility:public\"],\n    deps = [\n        \":aten\",\n        \"@cuda//:cublas\",\n        \"@cuda//:curand\",\n        \"@cudnn\",\n        \"@eigen\",\n        \"@tensorpipe//:tensorpipe_cuda\",\n    ],\n    alwayslink = True,\n)\n\nPERF_COPTS = [\n    \"-DHAVE_AVX_CPU_DEFINITION\",\n    \"-DHAVE_AVX2_CPU_DEFINITION\",\n    \"-DENABLE_ALIAS=1\",\n    \"-DHAVE_MALLOC_USABLE_SIZE=1\",\n    \"-DHAVE_MMAP=1\",\n    \"-DHAVE_SHM_OPEN=1\",\n    \"-DHAVE_SHM_UNLINK=1\",\n    \"-DSLEEF_STATIC_LIBS=1\",\n    \"-DTH_BALS_MKL\",\n    \"-D_FILE_OFFSET_BITS=64\",\n    \"-DUSE_FBGEMM\",\n    \"-fvisibility-inlines-hidden\",\n    \"-Wunused-parameter\",\n    \"-fno-math-errno\",\n    \"-fno-trapping-math\",\n    \"-mf16c\",\n]\n\nPERF_HEADERS = glob([\n    \"caffe2/perfkernels/*.h\",\n    \"caffe2/core/*.h\",\n])\n\ncc_library(\n    name = \"caffe2_perfkernels_avx\",\n    srcs = glob([\n        \"caffe2/perfkernels/*_avx.cc\",\n    ]),\n    hdrs = PERF_HEADERS,\n    copts = PERF_COPTS + [\n        \"-mavx\",\n    ],\n    visibility = [\"//visibility:public\"],\n    deps = [\n        \":caffe2_headers\",\n        \"//c10\",\n    ],\n    alwayslink = True,\n)\n\ncc_library(\n    name = \"caffe2_perfkernels_avx2\",\n    srcs = glob([\n        \"caffe2/perfkernels/*_avx2.cc\",\n    ]),\n    hdrs = PERF_HEADERS,\n    copts = PERF_COPTS + [\n        \"-mavx2\",\n        \"-mfma\",\n        \"-mavx\",\n    ],\n    visibility = [\"//visibility:public\"],\n    deps = [\n        \":caffe2_headers\",\n        \"//c10\",\n    ],\n    alwayslink = True,\n)\n\n# torch\ntorch_cuda_headers = glob([\"torch/csrc/cuda/*.h\"])\n\ncc_library(\n    name = \"torch_headers\",\n    hdrs = if_cuda(\n        torch_cuda_headers,\n    ) + glob(\n        [\n            \"torch/*.h\",\n            \"torch/csrc/**/*.h\",\n            \"torch/csrc/distributed/c10d/**/*.hpp\",\n            \"torch/lib/libshm/*.h\",\n        ],\n        exclude = [\n            \"torch/csrc/*/generated/*.h\",\n        ] + torch_cuda_headers,\n    ) + GENERATED_AUTOGRAD_CPP + [\":version_h\"],\n    includes = [\n        \"third_party/kineto/libkineto/include\",\n        \"torch/csrc\",\n        \"torch/csrc/api/include\",\n        \"torch/csrc/distributed\",\n        \"torch/lib\",\n        \"torch/lib/libshm\",\n    ],\n    visibility = [\"//visibility:public\"],\n    deps = [\n        \":aten_headers\",\n        \":caffe2_headers\",\n        \"//c10\",\n        \"@com_github_google_flatbuffers//:flatbuffers\",\n        \"@local_config_python//:python_headers\",\n        \"@onnx\",\n    ],\n    alwayslink = True,\n)\n\nTORCH_COPTS = COMMON_COPTS + [\n    \"-Dtorch_EXPORTS\",\n    \"-DHAVE_AVX_CPU_DEFINITION\",\n    \"-DHAVE_AVX2_CPU_DEFINITION\",\n    \"-DCAFFE2_USE_GLOO\",\n    \"-fvisibility-inlines-hidden\",\n    \"-fno-math-errno \",\n    \"-fno-trapping-math\",\n    \"-Wno-error=unused-function\",\n]\n\ntorch_sources = {\n    k: \"\"\n    for k in (\n        libtorch_core_sources +\n        libtorch_distributed_sources +\n        torch_cpp_srcs +\n        libtorch_extra_sources +\n        jit_core_sources +\n        lazy_tensor_ts_sources +\n        GENERATED_AUTOGRAD_CPP\n    )\n}.keys()\n\ncc_library(\n    name = \"torch\",\n    srcs = if_cuda(glob(\n        libtorch_cuda_sources,\n        exclude = [\n            \"torch/csrc/cuda/python_nccl.cpp\",\n            \"torch/csrc/cuda/nccl.cpp\",\n            \"torch/csrc/distributed/c10d/intra_node_comm.cu\",\n            \"torch/csrc/distributed/c10d/CUDASymmetricMemory.cu\",\n            \"torch/csrc/distributed/c10d/CUDASymmetricMemoryOps.cu\",\n            \"torch/csrc/distributed/c10d/cuda/AsyncMM.cu\",\n            \"torch/csrc/distributed/c10d/NanCheck.cu\",\n            \"torch/csrc/distributed/c10d/quantization/quantization_gpu.cu\",\n        ],\n    )) + torch_sources,\n    copts = TORCH_COPTS,\n    linkopts = [\n      \"-lrt\",\n    ],\n    defines = [\n        \"CAFFE2_NIGHTLY_VERSION=20200115\",\n    ],\n    visibility = [\"//visibility:public\"],\n    deps = [\n        \":caffe2\",\n        \":torch_headers\",\n        \"@kineto\",\n        \"@cpp-httplib\",\n        \"@nlohmann\",\n    ] + if_cuda([\n        \"@cuda//:nvToolsExt\",\n        \"@cutlass\",\n        \":torch_cuda\",\n    ]),\n    alwayslink = True,\n)\n\ncc_library(\n    name = \"shm\",\n    srcs = glob([\"torch/lib/libshm/*.cpp\"]),\n    linkopts = [\n      \"-lrt\",\n    ],\n    deps = [\n        \":torch\",\n    ],\n)\n\ncc_library(\n    name = \"libtorch_headers\",\n    hdrs = glob([\n        \"**/*.h\",\n        \"**/*.cuh\",\n    ]) + [\n        # We need the filegroup here because the raw list causes Bazel\n        # to see duplicate files. It knows how to deduplicate with the\n        # filegroup.\n        \":cpp_generated_code\",\n    ],\n    includes = [\n        \"torch/csrc/api/include\",\n        \"torch/csrc/distributed\",\n        \"torch/lib\",\n        \"torch/lib/libshm\",\n    ],\n    visibility = [\"//visibility:public\"],\n    deps = [\n        \":torch_headers\",\n    ],\n)\n\ncc_library(\n    name = \"torch_python\",\n    srcs = libtorch_python_core_sources\n        + if_cuda(libtorch_python_cuda_sources)\n        + if_cuda(libtorch_python_distributed_sources)\n        + GENERATED_AUTOGRAD_PYTHON,\n    hdrs = glob([\n        \"torch/csrc/generic/*.cpp\",\n    ]),\n    copts = COMMON_COPTS + if_cuda([\"-DUSE_CUDA=1\"]),\n    deps = [\n        \":torch\",\n        \":shm\",\n        \"@pybind11\",\n    ],\n)\n\npybind_extension(\n    name = \"torch/_C\",\n    srcs = [\"torch/csrc/stub.c\"],\n    deps = [\n        \":torch_python\",\n        \":aten_nvrtc\",\n    ],\n)\n\ncc_library(\n    name = \"functorch\",\n    hdrs = glob([\n        \"functorch/csrc/dim/*.h\",\n    ]),\n    srcs = glob([\n        \"functorch/csrc/dim/*.cpp\",\n    ]),\n    deps = [\n        \":aten_nvrtc\",\n        \":torch_python\",\n        \"@pybind11\",\n    ],\n)\n\npybind_extension(\n    name = \"functorch/_C\",\n    copts=[\n        \"-DTORCH_EXTENSION_NAME=_C\"\n    ],\n    srcs = [\n        \"functorch/csrc/init_dim_only.cpp\",\n    ],\n    deps = [\n        \":functorch\",\n        \":torch_python\",\n        \":aten_nvrtc\",\n    ],\n)\n\ncc_binary(\n    name = \"torch/bin/torch_shm_manager\",\n    srcs = [\n        \"torch/lib/libshm/manager.cpp\",\n    ],\n    deps = [\n        \":shm\",\n    ],\n    linkstatic = False,\n)\n\ntemplate_rule(\n    name = \"gen_version_py\",\n    src = \":torch/version.py.tpl\",\n    out = \"torch/version.py\",\n    substitutions = if_cuda({\n        # Set default to 11.2. Otherwise Torchvision complains about incompatibility.\n        \"{{CUDA_VERSION}}\": \"11.2\",\n        \"{{VERSION}}\": \"2.0.0\",\n    }, {\n        \"{{CUDA_VERSION}}\": \"None\",\n        \"{{VERSION}}\": \"2.0.0\",\n    }),\n)\n\npy_library(\n    name = \"pytorch_py\",\n    visibility = [\"//visibility:public\"],\n    srcs = glob([\"torch/**/*.py\"], exclude = [\"torch/version.py\"]) + [\":torch/version.py\"] + glob([\"functorch/**/*.py\"]),\n    deps = [\n        rules.requirement(\"numpy\"),\n        rules.requirement(\"pyyaml\"),\n        rules.requirement(\"requests\"),\n        rules.requirement(\"setuptools\"),\n        rules.requirement(\"sympy\"),\n        rules.requirement(\"typing_extensions\"),\n        \"//torchgen\",\n    ],\n    data = [\n        \":torch/_C.so\",\n        \":functorch/_C.so\",\n        \":torch/bin/torch_shm_manager\",\n    ],\n)\n\n# cpp api tests\ncc_library(\n    name = \"test_support\",\n    testonly = True,\n    srcs = [\n        \"test/cpp/api/support.cpp\",\n    ],\n    hdrs = [\n        \"test/cpp/api/init_baseline.h\",\n        \"test/cpp/api/optim_baseline.h\",\n        \"test/cpp/api/support.h\",\n        \"test/cpp/common/support.h\",\n    ],\n    deps = [\n        \":torch\",\n        \"@com_google_googletest//:gtest_main\",\n    ],\n)\n\n# Torch integration tests rely on a labeled data set from the MNIST database.\n# http://yann.lecun.com/exdb/mnist/\n\ncpp_api_tests = glob(\n    [\"test/cpp/api/*.cpp\"],\n    exclude = [\n        \"test/cpp/api/imethod.cpp\",\n        \"test/cpp/api/integration.cpp\",\n    ],\n)\n\ncc_test(\n    name = \"integration_test\",\n    size = \"medium\",\n    srcs = [\"test/cpp/api/integration.cpp\"],\n    data = [\n        \":download_mnist\",\n    ],\n    tags = [\n        \"gpu-required\",\n    ],\n    deps = [\n        \":test_support\",\n        \"@com_google_googletest//:gtest_main\",\n    ],\n)\n\n[\n    cc_test(\n        name = paths.split_extension(paths.basename(filename))[0].replace(\"-\", \"_\") + \"_test\",\n        size = \"medium\",\n        srcs = [filename],\n        deps = [\n            \":test_support\",\n            \"@com_google_googletest//:gtest_main\",\n        ],\n    )\n    for filename in cpp_api_tests\n]\n\ntest_suite(\n    name = \"api_tests\",\n    tests = [\n        \"any_test\",\n        \"autograd_test\",\n        \"dataloader_test\",\n        \"enum_test\",\n        \"expanding_array_test\",\n        \"functional_test\",\n        \"init_test\",\n        \"integration_test\",\n        \"jit_test\",\n        \"memory_test\",\n        \"misc_test\",\n        \"module_test\",\n        \"modulelist_test\",\n        \"modules_test\",\n        \"nn_utils_test\",\n        \"optim_test\",\n        \"ordered_dict_test\",\n        \"rnn_test\",\n        \"sequential_test\",\n        \"serialize_test\",\n        \"static_test\",\n        \"tensor_options_test\",\n        \"tensor_test\",\n        \"torch_include_test\",\n    ],\n)\n\n# dist autograd tests\ncc_test(\n    name = \"torch_dist_autograd_test\",\n    size = \"small\",\n    srcs = [\"test/cpp/dist_autograd/test_dist_autograd.cpp\"],\n    tags = [\n        \"exclusive\",\n        \"gpu-required\",\n    ],\n    deps = [\n        \":torch\",\n        \"@com_google_googletest//:gtest_main\",\n    ],\n)\n\n# jit tests\n# Because these individual unit tests require custom registering,\n# it is easier to mimic the cmake build by globing together a single test.\ncc_test(\n    name = \"jit_tests\",\n    size = \"small\",\n    srcs = glob(\n        [\n            \"test/cpp/jit/*.cpp\",\n            \"test/cpp/jit/*.h\",\n            \"test/cpp/tensorexpr/*.cpp\",\n            \"test/cpp/tensorexpr/*.h\",\n        ],\n        exclude = [\n            # skip this since <pybind11/embed.h> is not found in OSS build\n            \"test/cpp/jit/test_exception.cpp\",\n        ],\n    ),\n    linkstatic = True,\n    tags = [\n        \"exclusive\",\n        \"gpu-required\",\n    ],\n    deps = [\n        \":torch\",\n        \"@com_google_googletest//:gtest_main\",\n    ],\n)\n\ncc_test(\n    name = \"lazy_tests\",\n    size = \"small\",\n    srcs = glob(\n        [\n            \"test/cpp/lazy/*.cpp\",\n            \"test/cpp/lazy/*.h\",\n        ],\n        exclude = [\n            # skip these since they depend on generated LazyIr.h which isn't available in bazel yet\n            \"test/cpp/lazy/test_ir.cpp\",\n            \"test/cpp/lazy/test_lazy_ops.cpp\",\n            \"test/cpp/lazy/test_lazy_ops_util.cpp\",\n        ],\n    ),\n    linkstatic = True,\n    tags = [\n        \"exclusive\",\n    ],\n    deps = [\n        \":torch\",\n        \"@com_google_googletest//:gtest_main\",\n    ],\n)\n\n# python api tests\n\npy_test(\n    name = \"test_bazel\",\n    srcs = [\"test/_test_bazel.py\"],\n    main = \"test/_test_bazel.py\",\n    deps = [\n        \":pytorch_py\",\n        rules.requirement(\"networkx\"),\n    ],\n)\n\n# all tests\ntest_suite(\n    name = \"all_tests\",\n    tests = [\n        \"api_tests\",\n        \"jit_tests\",\n        \"torch_dist_autograd_test\",\n        \"//c10/test:tests\",\n    ],\n)\n\n# An internal genrule that we are converging with refers to these file\n# as if they are from this package, so we alias them for\n# compatibility.\n\n[\n    alias(\n        name = paths.basename(path),\n        actual = path,\n    )\n    for path in [\n        \"aten/src/ATen/templates/DispatchKeyNativeFunctions.cpp\",\n        \"aten/src/ATen/templates/DispatchKeyNativeFunctions.h\",\n        \"aten/src/ATen/templates/LazyIr.h\",\n        \"aten/src/ATen/templates/LazyNonNativeIr.h\",\n        \"aten/src/ATen/templates/RegisterDispatchKey.cpp\",\n        \"aten/src/ATen/templates/RegisterDispatchDefinitions.ini\",\n        \"aten/src/ATen/native/native_functions.yaml\",\n        \"aten/src/ATen/native/tags.yaml\",\n        \"aten/src/ATen/native/ts_native_functions.yaml\",\n        \"torch/csrc/lazy/core/shape_inference.h\",\n        \"torch/csrc/lazy/ts_backend/ts_native_functions.cpp\",\n    ]\n]\n\ngenrule(\n    name = \"download_mnist\",\n    srcs = [\"//:tools/download_mnist.py\"],\n    outs = [\n        \"mnist/train-images-idx3-ubyte\",\n        \"mnist/train-labels-idx1-ubyte\",\n        \"mnist/t10k-images-idx3-ubyte\",\n        \"mnist/t10k-labels-idx1-ubyte\",\n    ],\n    cmd = \"python3 tools/download_mnist.py -d $(RULEDIR)/mnist\",\n)\n"
        },
        {
          "name": "CITATION.cff",
          "type": "blob",
          "size": 3.14,
          "content": "cff-version: 1.2.0\nmessage: If you use this software, please cite it as below.\ntitle: PyTorch\nauthors:\n  - family-names: PyTorch Team\nurl: https://pytorch.org\npreferred-citation:\n  type: conference-paper\n  title: \"PyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation\"\n  authors:\n    - family-names: Ansel\n      given-names: Jason\n    - family-names: Yang\n      given-names: Edward\n    - family-names: He\n      given-names: Horace\n    - family-names: Gimelshein\n      given-names: Natalia\n    - family-names: Jain\n      given-names: Animesh\n    - family-names: Voznesensky\n      given-names: Michael\n    - family-names: Bao\n      given-names: Bin\n    - family-names: Bell\n      given-names: Peter\n    - family-names: Berard\n      given-names: David\n    - family-names: Burovski\n      given-names: Evgeni\n    - family-names: Chauhan\n      given-names: Geeta\n    - family-names: Chourdia\n      given-names: Anjali\n    - family-names: Constable\n      given-names: Will\n    - family-names: Desmaison\n      given-names: Alban\n    - family-names: DeVito\n      given-names: Zachary\n    - family-names: Ellison\n      given-names: Elias\n    - family-names: Feng\n      given-names: Will\n    - family-names: Gong\n      given-names: Jiong\n    - family-names: Gschwind\n      given-names: Michael\n    - family-names: Hirsh\n      given-names: Brian\n    - family-names: Huang\n      given-names: Sherlock\n    - family-names: Kalambarkar\n      given-names: Kshiteej\n    - family-names: Kirsch\n      given-names: Laurent\n    - family-names: Lazos\n      given-names: Michael\n    - family-names: Lezcano\n      given-names: Mario\n    - family-names: Liang\n      given-names: Yanbo\n    - family-names: Liang\n      given-names: Jason\n    - family-names: Lu\n      given-names: Yinghai\n    - family-names: Luk\n      given-names: CK\n    - family-names: Maher\n      given-names: Bert\n    - family-names: Pan\n      given-names: Yunjie\n    - family-names: Puhrsch\n      given-names: Christian\n    - family-names: Reso\n      given-names: Matthias\n    - family-names: Saroufim\n      given-names: Mark\n    - family-names: Siraichi\n      given-names: Marcos Yukio\n    - family-names: Suk\n      given-names: Helen\n    - family-names: Suo\n      given-names: Michael\n    - family-names: Tillet\n      given-names: Phil\n    - family-names: Wang\n      given-names: Eikan\n    - family-names: Wang\n      given-names: Xiaodong\n    - family-names: Wen\n      given-names: William\n    - family-names: Zhang\n      given-names: Shunting\n    - family-names: Zhao\n      given-names: Xu\n    - family-names: Zhou\n      given-names: Keren\n    - family-names: Zou\n      given-names: Richard\n    - family-names: Mathews\n      given-names: Ajit\n    - family-names: Chanan\n      given-names: Gregory\n    - family-names: Wu\n      given-names: Peng\n    - family-names: Chintala\n      given-names: Soumith\n  collection-title: \"29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2 (ASPLOS '24)\"\n  collection-type: proceedings\n  month: 4\n  year: 2024\n  publisher:\n    name: ACM\n  doi: \"10.1145/3620665.3640366\"\n  url: \"https://pytorch.org/assets/pytorch2-2.pdf\"\n"
        },
        {
          "name": "CMakeLists.txt",
          "type": "blob",
          "size": 52.6,
          "content": "cmake_minimum_required(VERSION 3.18 FATAL_ERROR)\n# cmake_policy(SET CMP0022 NEW) cmake_policy(SET CMP0023 NEW)\n\n# Use compiler ID \"AppleClang\" instead of \"Clang\" for XCode. Not setting this\n# sometimes makes XCode C compiler gets detected as \"Clang\", even when the C++\n# one is detected as \"AppleClang\".\ncmake_policy(SET CMP0010 NEW)\ncmake_policy(SET CMP0025 NEW)\n\n# Enables CMake to set LTO on compilers other than Intel.\ncmake_policy(SET CMP0069 NEW)\n# Enable the policy for CMake subprojects. protobuf currently causes issues\n# set(CMAKE_POLICY_DEFAULT_CMP0069 NEW)\n\n# Suppress warning flags in default MSVC configuration.  It's not mandatory that\n# we do this (and we don't if cmake is old), but it's nice when it's possible,\n# and it's possible on our Windows configs.\ncmake_policy(SET CMP0092 NEW)\n\n# Prohibit in-source builds\nif(${CMAKE_SOURCE_DIR} STREQUAL ${CMAKE_BINARY_DIR})\n  message(FATAL_ERROR \"In-source build are not supported\")\nendif()\n\n# ---[ Project and semantic versioning.\nproject(Torch CXX C)\n\nif(${CMAKE_SYSTEM_NAME} STREQUAL \"Linux\")\n  set(LINUX TRUE)\nelse()\n  set(LINUX FALSE)\nendif()\n\nset(CMAKE_INSTALL_MESSAGE NEVER)\n\n# check and set CMAKE_CXX_STANDARD\nstring(FIND \"${CMAKE_CXX_FLAGS}\" \"-std=c++\" env_cxx_standard)\nif(env_cxx_standard GREATER -1)\n  message(\n    WARNING\n      \"C++ standard version definition detected in environment variable.\"\n      \"PyTorch requires -std=c++17. Please remove -std=c++ settings in your environment.\"\n  )\nendif()\nset(CMAKE_CXX_STANDARD\n    17\n    CACHE STRING\n          \"The C++ standard whose features are requested to build this target.\")\nset(CMAKE_C_STANDARD\n    11\n    CACHE STRING\n          \"The C standard whose features are requested to build this target.\")\n\n# ---[ Utils\ninclude(cmake/public/utils.cmake)\n\n# --- [ Check that minimal gcc version is 9.3+\nif(CMAKE_COMPILER_IS_GNUCXX AND CMAKE_CXX_COMPILER_VERSION VERSION_LESS 9.3)\n  message(\n    FATAL_ERROR\n      \"GCC-9.3 or newer is required to compile PyTorch, but found ${CMAKE_CXX_COMPILER_VERSION}\"\n  )\nendif()\n\n# This define is needed to preserve behavior given anticpated changes to\n# cccl/thrust\n# https://nvidia.github.io/libcudacxx/standard_api/numerics_library/complex.html\nstring(APPEND CMAKE_CUDA_FLAGS\n       \" -DLIBCUDACXX_ENABLE_SIMPLIFIED_COMPLEX_OPERATIONS\")\n\nif(LINUX)\n  include(cmake/CheckAbi.cmake)\n  string(APPEND CMAKE_CXX_FLAGS\n         \" -D_GLIBCXX_USE_CXX11_ABI=${GLIBCXX_USE_CXX11_ABI}\")\n  string(APPEND CMAKE_CUDA_FLAGS\n         \" -D_GLIBCXX_USE_CXX11_ABI=${GLIBCXX_USE_CXX11_ABI}\")\n  if(${GLIBCXX_USE_CXX11_ABI} EQUAL 1)\n    set(CXX_STANDARD_REQUIRED ON)\n  else()\n    # Please note this is required in order to ensure compatibility between gcc\n    # 9 and gcc 7 This could be removed when all Linux PyTorch binary builds are\n    # compiled by the same toolchain again\n    append_cxx_flag_if_supported(\"-fabi-version=11\" CMAKE_CXX_FLAGS)\n  endif()\nendif()\n\nset(CMAKE_EXPORT_COMPILE_COMMANDS ON)\nset(CMAKE_LINK_WHAT_YOU_USE TRUE)\n\n# One variable that determines whether the current cmake process is being run\n# with the main Caffe2 library. This is useful for building modules - if modules\n# are built with the main Caffe2 library then one does not need to do find\n# caffe2 in the cmake script. One can usually guard it in some way like if(NOT\n# CAFFE2_CMAKE_BUILDING_WITH_MAIN_REPO) find_package(Caffe2 REQUIRED) endif()\nset(CAFFE2_CMAKE_BUILDING_WITH_MAIN_REPO ON)\n\n# Googletest's cmake files are going to set it on once they are processed. Let's\n# set it at the very beginning so that the entire build is deterministic.\nset(THREADS_PREFER_PTHREAD_FLAG ON)\n\nif(NOT DEFINED BLAS_SET_BY_USER)\n  if(DEFINED BLAS)\n    set(BLAS_SET_BY_USER TRUE)\n  else()\n    message(STATUS \"Not forcing any particular BLAS to be found\")\n    set(BLAS_SET_BY_USER FALSE)\n  endif()\n  set(BLAS_SET_BY_USER\n      ${BLAS_SET_BY_USER}\n      CACHE STRING\n            \"Marks whether BLAS was manually set by user or auto-detected\")\nendif()\n\n# Apple specific\nif(APPLE)\n  # These lines are an attempt to make find_package(cuda) pick up libcuda.dylib,\n  # and not cuda.framework.  It doesn't work all the time, but it seems to help\n  # for some users. TODO: replace this with a more robust fix\n  set(CMAKE_FIND_FRAMEWORK LAST)\n  set(CMAKE_FIND_APPBUNDLE LAST)\n\n  # Get clang version on macOS\n  execute_process(COMMAND ${CMAKE_CXX_COMPILER} --version\n                  OUTPUT_VARIABLE clang_full_version_string)\n  string(REGEX REPLACE \"Apple (.*) version ([0-9]+\\\\.[0-9]+).*\" \"\\\\2\"\n                       CLANG_VERSION_STRING ${clang_full_version_string})\n  message(STATUS \"CLANG_VERSION_STRING:         \" ${CLANG_VERSION_STRING})\n\n  # RPATH stuff\n  set(CMAKE_MACOSX_RPATH ON)\n  if(NOT IOS)\n    # Determine if we can link against MPSGraph\n    set(MPS_FOUND OFF)\n    execute_process(\n      COMMAND bash -c \"xcrun --sdk macosx --show-sdk-version\"\n      RESULT_VARIABLE _exit_code\n      OUTPUT_VARIABLE _macosx_sdk_version\n      OUTPUT_STRIP_TRAILING_WHITESPACE)\n    if(_exit_code EQUAL 0)\n      set(_MPS_supported_os_version OFF)\n      if(_macosx_sdk_version VERSION_GREATER_EQUAL 12.3)\n        set(_MPS_supported_os_version ON)\n      endif()\n      message(\n        STATUS\n          \"sdk version: ${_macosx_sdk_version}, mps supported: ${_MPS_supported_os_version}\"\n      )\n      execute_process(\n        COMMAND bash -c \"xcrun --sdk macosx --show-sdk-path\"\n        OUTPUT_VARIABLE _macosx_sdk_path\n        OUTPUT_STRIP_TRAILING_WHITESPACE)\n      set(_SDK_SEARCH_PATH \"${_macosx_sdk_path}/System/Library/Frameworks/\")\n      set(_FRAMEWORK_SEARCH_PATH \"/System/Library/Frameworks/\")\n\n      find_library(\n        _MPS_fwrk_path_\n        NAMES MetalPerformanceShadersGraph MetalPerformanceShaders\n        PATHS ${_FRAMEWORK_SEARCH_PATH}\n        NO_DEFAULT_PATH)\n      find_library(\n        _MPS_sdk_path_\n        NAMES MetalPerformanceShadersGraph MetalPerformanceShaders\n        PATHS ${_SDK_SEARCH_PATH}\n        NO_DEFAULT_PATH)\n\n      if(_MPS_supported_os_version\n         AND _MPS_fwrk_path_\n         AND _MPS_sdk_path_)\n        set(MPS_FOUND ON)\n        message(STATUS \"MPSGraph framework found\")\n      else()\n        message(STATUS \"MPSGraph framework not found\")\n      endif()\n    else()\n      message(STATUS \"MPS: unable to get MacOS sdk version\")\n      message(STATUS \"MPSGraph framework not found\")\n    endif()\n  endif()\nendif()\n\nset(CPU_AARCH64 OFF)\nset(CPU_INTEL OFF)\n\nif(CMAKE_SYSTEM_PROCESSOR MATCHES \"(AMD64|x86_64)\")\n  set(CPU_INTEL ON)\nelseif(CMAKE_SYSTEM_PROCESSOR MATCHES \"^(aarch64|arm64)\")\n  set(CPU_AARCH64 ON)\nendif()\n\n# For non-supported platforms, turn USE_DISTRIBUTED off by default. It is not\n# tested and likely won't work without additional changes.\nif(NOT LINUX AND NOT WIN32)\n  set(USE_DISTRIBUTED\n      OFF\n      CACHE STRING \"Use distributed\")\n  # On macOS, if USE_DISTRIBUTED is enabled (specified by the user), then make\n  # Gloo build with the libuv transport.\n  if(APPLE AND USE_DISTRIBUTED)\n    set(USE_LIBUV\n        ON\n        CACHE STRING \"\")\n  endif()\nendif()\n\n# ---[ Options. Note to developers: if you add an option below, make sure you\n# also add it to cmake/Summary.cmake so that the summary prints out the option\n# values.\ninclude(CMakeDependentOption)\noption(ATEN_NO_TEST \"Do not build ATen test binaries\" OFF)\noption(BUILD_BINARY \"Build C++ binaries\" OFF)\noption(BUILD_CUSTOM_PROTOBUF\n       \"Build and use Caffe2's own protobuf under third_party\" ON)\noption(BUILD_PYTHON \"Build Python binaries\" ON)\noption(BUILD_LITE_INTERPRETER \"Master flag to build Lite Interpreter\" OFF)\noption(BUILD_SHARED_LIBS \"Build libcaffe2.so\" ON)\ncmake_dependent_option(\n  CAFFE2_LINK_LOCAL_PROTOBUF \"If set, build protobuf inside libcaffe2.so.\" ON\n  \"BUILD_SHARED_LIBS AND BUILD_CUSTOM_PROTOBUF\" OFF)\ncmake_dependent_option(\n  CAFFE2_USE_MSVC_STATIC_RUNTIME \"Using MSVC static runtime libraries\" ON\n  \"NOT BUILD_SHARED_LIBS\" OFF)\noption(BUILD_TEST \"Build C++ test binaries (need gtest and gbenchmark)\" OFF)\noption(BUILD_AOT_INDUCTOR_TEST \"Build C++ test binaries for aot-inductor\" OFF)\noption(BUILD_STATIC_RUNTIME_BENCHMARK\n       \"Build C++ binaries for static runtime benchmarks (need gbenchmark)\" OFF)\noption(\n  BUILD_MOBILE_BENCHMARK\n  \"Build C++ test binaries for mobile (ARM) targets(need gtest and gbenchmark)\"\n  OFF)\noption(\n  BUILD_MOBILE_TEST\n  \"Build C++ test binaries for mobile (ARM) targets(need gtest and gbenchmark)\"\n  OFF)\noption(BUILD_JNI \"Build JNI bindings\" OFF)\noption(BUILD_MOBILE_AUTOGRAD\n       \"Build autograd function in mobile build (in development)\" OFF)\ncmake_dependent_option(INSTALL_TEST \"Install test binaries if BUILD_TEST is on\"\n                       ON \"BUILD_TEST\" OFF)\noption(USE_CPP_CODE_COVERAGE \"Compile C/C++ with code coverage flags\" OFF)\noption(USE_COLORIZE_OUTPUT \"Colorize output during compilation\" ON)\noption(USE_ASAN \"Use Address+Undefined Sanitizers\" OFF)\noption(USE_TSAN \"Use Thread Sanitizer\" OFF)\noption(USE_CUDA \"Use CUDA\" ON)\noption(USE_XPU \"Use XPU\" ON)\ncmake_dependent_option(\n  BUILD_LAZY_CUDA_LINALG \"Build cuda linalg ops as separate library\" ON\n  \"USE_CUDA AND LINUX AND BUILD_PYTHON\" OFF)\ncmake_dependent_option(USE_ROCM \"Use ROCm\" ON \"LINUX\" OFF)\noption(CAFFE2_STATIC_LINK_CUDA \"Statically link CUDA libraries\" OFF)\ncmake_dependent_option(USE_CUDNN \"Use cuDNN\" ON \"USE_CUDA\" OFF)\ncmake_dependent_option(USE_STATIC_CUDNN \"Use cuDNN static libraries\" OFF\n                       \"USE_CUDNN\" OFF)\ncmake_dependent_option(USE_CUSPARSELT \"Use cuSPARSELt\" ON \"USE_CUDA\" OFF)\ncmake_dependent_option(USE_CUDSS \"Use cuDSS\" ON \"USE_CUDA\" OFF)\n# Binary builds will fail for cufile due to https://github.com/pytorch/builder/issues/1924\n# Using TH_BINARY_BUILD to check whether is binary build.\n# USE_ROCM is guarded against in Dependencies.cmake because USE_ROCM is not properly defined here\nif(DEFINED ENV{TH_BINARY_BUILD})\n  cmake_dependent_option(USE_CUFILE \"Use cuFile\" OFF\n                         \"USE_CUDA AND NOT $ENV{TH_BINARY_BUILD} AND NOT WIN32\" OFF)\nelse()\n  cmake_dependent_option(USE_CUFILE \"Use cuFile\" OFF \"USE_CUDA AND NOT WIN32\" OFF)\nendif()\noption(USE_FBGEMM \"Use FBGEMM (quantized 8-bit server operators)\" ON)\noption(USE_KINETO \"Use Kineto profiling library\" ON)\noption(USE_CUPTI_SO \"Use CUPTI as a shared library\" ON)\noption(USE_FAKELOWP \"Use FakeLowp operators\" OFF)\noption(USE_GFLAGS \"Use GFLAGS\" OFF)\noption(USE_GLOG \"Use GLOG\" OFF)\noption(USE_LITE_PROTO \"Use lite protobuf instead of full.\" OFF)\noption(USE_MAGMA \"Use MAGMA\" ON)\noption(USE_PYTORCH_METAL \"Use Metal for PyTorch iOS build\" OFF)\noption(USE_PYTORCH_METAL_EXPORT \"Export Metal models on MacOSX desktop\" OFF)\noption(USE_NATIVE_ARCH \"Use -march=native\" OFF)\ncmake_dependent_option(USE_MPS \"Use MPS for macOS build\" ON \"MPS_FOUND\" OFF)\ncmake_dependent_option(USE_NCCL \"Use NCCL\" ON\n                       \"USE_CUDA OR USE_ROCM;UNIX;NOT APPLE\" OFF)\ncmake_dependent_option(USE_RCCL \"Use RCCL\" ON USE_NCCL OFF)\ncmake_dependent_option(USE_STATIC_NCCL \"Use static NCCL\" OFF \"USE_NCCL\" OFF)\ncmake_dependent_option(USE_SYSTEM_NCCL \"Use system-wide NCCL\" OFF \"USE_NCCL\"\n                       OFF)\noption(USE_NNAPI \"Use NNAPI\" OFF)\noption(USE_NNPACK \"Use NNPACK\" ON)\ncmake_dependent_option(USE_NUMA \"Use NUMA. Only available on Linux.\" ON \"LINUX\"\n                       OFF)\ncmake_dependent_option(USE_NVRTC \"Use NVRTC. Only available if USE_CUDA is on.\"\n                       OFF \"USE_CUDA\" OFF)\noption(USE_NUMPY \"Use NumPy\" ON)\noption(USE_OBSERVERS \"Use observers module.\" OFF)\noption(USE_OPENCL \"Use OpenCL\" OFF)\noption(USE_OPENMP \"Use OpenMP for parallel code\" ON)\noption(USE_PRECOMPILED_HEADERS \"Use pre-compiled headers to accelerate build.\"\n       OFF)\n\noption(USE_PROF \"Use profiling\" OFF)\noption(USE_PYTORCH_QNNPACK \"Use ATen/QNNPACK (quantized 8-bit operators)\" ON)\noption(USE_SNPE \"Use Qualcomm's SNPE library\" OFF)\noption(USE_SYSTEM_EIGEN_INSTALL\n    \"Use system Eigen instead of the one under third_party\" OFF)\ncmake_dependent_option(\n    USE_VALGRIND \"Use Valgrind. Only available on Linux.\" ON\n    \"LINUX\" OFF)\n\nif(NOT DEFINED USE_VULKAN)\n  cmake_dependent_option(USE_VULKAN \"Use Vulkan GPU backend\" ON \"ANDROID\" OFF)\nendif()\n\noption(USE_SOURCE_DEBUG_ON_MOBILE \"Enable\" ON)\noption(USE_LITE_INTERPRETER_PROFILER \"Enable\" ON)\ncmake_dependent_option(\n  USE_LITE_AOTI \"Include AOTI sources\" OFF\n  \"BUILD_LITE_INTERPRETER\" OFF)\noption(USE_VULKAN_FP16_INFERENCE \"Vulkan - Use fp16 inference\" OFF)\noption(USE_VULKAN_RELAXED_PRECISION\n       \"Vulkan - Use relaxed precision math in the kernels (mediump)\" OFF)\n# option USE_XNNPACK: try to enable xnnpack by default.\noption(USE_XNNPACK \"Use XNNPACK\" ON)\noption(USE_ROCM_KERNEL_ASSERT \"Use Kernel Assert for ROCm\" OFF)\n# Ensure that an ITT build is the default for x86 CPUs\ncmake_dependent_option(USE_ITT \"Use Intel(R) VTune Profiler ITT functionality\"\n                       ON \"CPU_INTEL\" OFF)\n# Ensure that an MKLDNN build is the default for x86 CPUs but optional for\n# AArch64 (dependent on -DUSE_MKLDNN).\ncmake_dependent_option(\n  USE_MKLDNN \"Use MKLDNN. Only available on x86, x86_64, and AArch64.\"\n  \"${CPU_INTEL}\" \"CPU_INTEL OR CPU_AARCH64\" OFF)\ncmake_dependent_option(\n  USE_MKLDNN_ACL \"Use Compute Library for the Arm architecture.\" OFF\n  \"USE_MKLDNN AND CPU_AARCH64\" OFF)\nset(MKLDNN_ENABLE_CONCURRENT_EXEC ${USE_MKLDNN})\ncmake_dependent_option(USE_MKLDNN_CBLAS \"Use CBLAS in MKLDNN\" OFF \"USE_MKLDNN\"\n                       OFF)\noption(USE_STATIC_MKL \"Prefer to link with MKL statically (Unix only)\" OFF)\noption(USE_DISTRIBUTED \"Use distributed\" ON)\ncmake_dependent_option(\n  USE_MPI \"Use MPI for Caffe2. Only available if USE_DISTRIBUTED is on.\" ON\n  \"USE_DISTRIBUTED\" OFF)\ncmake_dependent_option(\n  USE_UCC \"Use UCC. Only available if USE_DISTRIBUTED is on.\" OFF\n  \"USE_DISTRIBUTED\" OFF)\ncmake_dependent_option(USE_SYSTEM_UCC \"Use system-wide UCC\" OFF \"USE_UCC\" OFF)\ncmake_dependent_option(USE_C10D_UCC \"USE C10D UCC\" ON \"USE_DISTRIBUTED;USE_UCC\"\n                       OFF)\ncmake_dependent_option(\n    USE_GLOO \"Use Gloo. Only available if USE_DISTRIBUTED is on.\" ON\n    \"USE_DISTRIBUTED\" OFF)\ncmake_dependent_option(\n  USE_GLOO_WITH_OPENSSL \"Use Gloo with OpenSSL. Only available if USE_GLOO is on.\" OFF\n    \"USE_GLOO AND LINUX AND NOT INTERN_BUILD_MOBILE\" OFF)\ncmake_dependent_option(\n    USE_C10D_GLOO \"USE C10D GLOO\" ON \"USE_DISTRIBUTED;USE_GLOO\" OFF)\ncmake_dependent_option(\n    USE_C10D_NCCL \"USE C10D NCCL\" ON \"USE_DISTRIBUTED;USE_NCCL\" OFF)\ncmake_dependent_option(\n    USE_C10D_MPI \"USE C10D MPI\" ON \"USE_DISTRIBUTED;USE_MPI\" OFF)\ncmake_dependent_option(\n    USE_TENSORPIPE \"Use TensorPipe. Only available if USE_DISTRIBUTED is on.\" ON\n    \"USE_DISTRIBUTED AND NOT WIN32\" OFF)\noption(ONNX_ML \"Enable traditional ONNX ML API.\" ON)\noption(HAVE_SOVERSION \"Whether to add SOVERSION to the shared objects\" OFF)\noption(BUILD_LIBTORCH_CPU_WITH_DEBUG\n       \"Enable RelWithDebInfo for libtorch_cpu target only\" OFF)\ncmake_dependent_option(\n  USE_CCACHE \"Attempt using CCache to wrap the compilation\" ON \"UNIX\" OFF)\noption(WERROR \"Build with -Werror supported by the compiler\" OFF)\noption(\n  DEBUG_CUDA\n  \"When compiling DEBUG, also attempt to compile CUDA with debug flags (may cause nvcc to OOM)\"\n  OFF)\noption(USE_COREML_DELEGATE \"Use the CoreML backend through delegate APIs\" OFF)\noption(USE_PER_OPERATOR_HEADERS\n       \"Whether ATen should generate separate headers for each operator\" ON)\ncmake_dependent_option(\n  BUILD_LAZY_TS_BACKEND\n  \"Build the lazy Torchscript backend, not compatible with mobile builds\" ON\n  \"NOT INTERN_BUILD_MOBILE\" OFF)\ncmake_dependent_option(BUILD_FUNCTORCH \"Build Functorch\" ON \"BUILD_PYTHON\" OFF)\ncmake_dependent_option(BUILD_BUNDLE_PTXAS \"Bundle PTX into torch/bin fodler\"\n                       OFF \"USE_CUDA\" OFF)\ncmake_dependent_option(USE_KLEIDIAI \"Use KleidiAI for the ARM CPU & AARCH64 architecture.\" ON\n                        \"CPU_AARCH64\" OFF)\n\noption(USE_MIMALLOC \"Use mimalloc\" OFF)\n# Enable third party mimalloc library to improve memory allocation performance\n# on Windows.\noption(USE_MIMALLOC_ON_MKL \"Use mimalloc on MKL\" OFF)\nif(WIN32)\n  set(USE_MIMALLOC ON)\n\n  # Not enable USE_MIMALLOC_ON_MKL due to it caused issue:\n  # https://github.com/pytorch/pytorch/issues/138994\n  # Will turn on when we can fix USE_STATIC_MKL lost functionality:\n  # https://github.com/pytorch/pytorch/pull/138996\n  # set(USE_MIMALLOC_ON_MKL ON)\nendif()\n\nif(USE_CCACHE)\n  find_program(CCACHE_PROGRAM ccache)\n  if(CCACHE_PROGRAM)\n    set(CMAKE_C_COMPILER_LAUNCHER\n        \"${CCACHE_PROGRAM}\"\n        CACHE STRING \"C compiler launcher\")\n    set(CMAKE_CXX_COMPILER_LAUNCHER\n        \"${CCACHE_PROGRAM}\"\n        CACHE STRING \"CXX compiler launcher\")\n    set(CMAKE_CUDA_COMPILER_LAUNCHER\n        \"${CCACHE_PROGRAM}\"\n        CACHE STRING \"CUDA compiler launcher\")\n  else()\n    message(\n      STATUS\n        \"Could not find ccache. Consider installing ccache to speed up compilation.\"\n    )\n  endif()\nendif()\n\n# Since TensorPipe does not support Windows, set it to OFF when WIN32 detected\n# On Windows platform, if user does not install libuv in build conda env and\n# does not set libuv_ROOT environment variable. Set USE_DISTRIBUTED to OFF.\nif(WIN32)\n  set(USE_TENSORPIPE OFF)\n  message(WARNING \"TensorPipe cannot be used on Windows. Set it to OFF\")\n  set(USE_KLEIDIAI OFF)\n  message(WARNING \"KleidiAI cannot be used on Windows. Set it to OFF\")\n\n  if(USE_DISTRIBUTED AND NOT DEFINED ENV{libuv_ROOT})\n    find_library(\n      libuv_tmp_LIBRARY\n      NAMES uv libuv\n      HINTS $ENV{CONDA_PREFIX}\\\\Library $ENV{PREFIX}\\\\Library\n      PATH_SUFFIXES lib\n      NO_DEFAULT_PATH)\n    if(NOT libuv_tmp_LIBRARY)\n      set(USE_DISTRIBUTED OFF)\n      set(USE_GLOO OFF)\n      message(\n        WARNING\n          \"Libuv is not installed in current conda env. Set USE_DISTRIBUTED to OFF. \"\n          \"Please run command 'conda install -c conda-forge libuv=1.39' to install libuv.\"\n      )\n    else()\n      set(ENV{libuv_ROOT} ${libuv_tmp_LIBRARY}/../../)\n    endif()\n  endif()\nendif()\n\nif(USE_GLOO_WITH_OPENSSL)\n  set(USE_TCP_OPENSSL_LOAD\n      ON\n      CACHE STRING \"\")\nendif()\n\n# Linux distributions do not want too many embedded sources, in that sense we\n# need to be able to build pytorch with an (almost) empty third_party directory.\n# USE_SYSTEM_LIBS is a shortcut variable to toggle all the # USE_SYSTEM_*\n# variables on. Individual USE_SYSTEM_* variables can be toggled with\n# USE_SYSTEM_LIBS being \"OFF\".\noption(USE_SYSTEM_LIBS \"Use all available system-provided libraries.\" OFF)\noption(USE_SYSTEM_CPUINFO \"Use system-provided cpuinfo.\" OFF)\noption(USE_SYSTEM_SLEEF \"Use system-provided sleef.\" OFF)\noption(USE_SYSTEM_GLOO \"Use system-provided gloo.\" OFF)\noption(USE_SYSTEM_FP16 \"Use system-provided fp16.\" OFF)\noption(USE_SYSTEM_PYBIND11 \"Use system-provided PyBind11.\" OFF)\noption(USE_SYSTEM_PTHREADPOOL \"Use system-provided pthreadpool.\" OFF)\noption(USE_SYSTEM_PSIMD \"Use system-provided psimd.\" OFF)\noption(USE_SYSTEM_FXDIV \"Use system-provided fxdiv.\" OFF)\noption(USE_SYSTEM_BENCHMARK \"Use system-provided google benchmark.\" OFF)\noption(USE_SYSTEM_ONNX \"Use system-provided onnx.\" OFF)\noption(USE_SYSTEM_XNNPACK \"Use system-provided xnnpack.\" OFF)\nOPTION(USE_SYSTEM_NVTX \"Use system-provided nvtx.\" OFF)\noption(USE_GOLD_LINKER \"Use ld.gold to link\" OFF)\nif(USE_SYSTEM_LIBS)\n  set(USE_SYSTEM_CPUINFO ON)\n  set(USE_SYSTEM_SLEEF ON)\n  set(USE_SYSTEM_GLOO ON)\n  set(BUILD_CUSTOM_PROTOBUF OFF)\n  set(USE_SYSTEM_EIGEN_INSTALL ON)\n  set(USE_SYSTEM_FP16 ON)\n  set(USE_SYSTEM_PTHREADPOOL ON)\n  set(USE_SYSTEM_PSIMD ON)\n  set(USE_SYSTEM_FXDIV ON)\n  set(USE_SYSTEM_BENCHMARK ON)\n  set(USE_SYSTEM_ONNX ON)\n  set(USE_SYSTEM_XNNPACK ON)\n  set(USE_SYSTEM_PYBIND11 ON)\n  if(USE_NCCL)\n    set(USE_SYSTEM_NCCL ON)\n  endif()\n  set(USE_SYSTEM_NVTX ON)\nendif()\n\n# /Z7 override option When generating debug symbols, CMake default to use the\n# flag /Zi. However, it is not compatible with sccache. So we rewrite it off.\n# But some users don't use sccache; this override is for them.\ncmake_dependent_option(\n  MSVC_Z7_OVERRIDE\n  \"Work around sccache bug by replacing /Zi and /ZI with /Z7 when using MSVC (if you are not using sccache, you can turn this OFF)\"\n  ON\n  \"MSVC\"\n  OFF)\n\nif(NOT USE_SYSTEM_ONNX)\n  set(ONNX_NAMESPACE\n      \"onnx_torch\"\n      CACHE\n        STRING\n        \"A namespace for ONNX; needed to build with other frameworks that share ONNX.\"\n  )\nelse()\n  set(ONNX_NAMESPACE\n      \"onnx\"\n      CACHE\n        STRING\n        \"A namespace for ONNX; needed to build with other frameworks that share ONNX.\"\n  )\nendif()\nset(SELECTED_OP_LIST\n    \"\"\n    CACHE\n      STRING\n      \"Path to the yaml file that contains the list of operators to include for custom build. Include all operators by default.\"\n)\noption(\n  STATIC_DISPATCH_BACKEND\n  \"Name of the backend for which static dispatch code is generated, e.g.: CPU.\"\n  \"\")\noption(\n  USE_LIGHTWEIGHT_DISPATCH\n  \"Enable codegen unboxing for ATen ops, need to work with static dispatch in order to work properly.\"\n  OFF)\nif(USE_LIGHTWEIGHT_DISPATCH AND NOT STATIC_DISPATCH_BACKEND)\n  message(\n    FATAL_ERROR\n      \"Need to enable static dispatch after enabling USE_LIGHTWEIGHT_DISPATCH.\")\nendif()\noption(TRACING_BASED\n       \"Master flag to build Lite Interpreter with tracing build option\" OFF)\noption(BUILD_EXECUTORCH \"Master flag to build Executorch\" ON)\n# This is a fix for a rare build issue on Ubuntu: symbol lookup error:\n# miniconda3/envs/pytorch-py3.7/lib/libmkl_intel_lp64.so: undefined symbol:\n# mkl_blas_dsyrk\n# https://software.intel.com/en-us/articles/symbol-lookup-error-when-linking-intel-mkl-with-gcc-on-ubuntu\nif(LINUX)\n  set(CMAKE_SHARED_LINKER_FLAGS\n      \"${CMAKE_SHARED_LINKER_FLAGS} -Wl,--no-as-needed\")\n\n  set(ENV_LDFLAGS \"$ENV{LDFLAGS}\")\n  string(STRIP \"${ENV_LDFLAGS}\" ENV_LDFLAGS)\n  # Do not append linker flags passed via env var if they already there\n  if(NOT ${CMAKE_SHARED_LINKER_FLAGS} MATCHES \"${ENV_LDFLAGS}\")\n     set(CMAKE_SHARED_LINKER_FLAGS\n         \"${CMAKE_SHARED_LINKER_FLAGS} ${ENV_LDFLAGS}\")\n  endif()\nendif()\n\nif(MSVC)\n  # MSVC by default does not apply the correct __cplusplus version as specified\n  # by the C++ standard because MSVC is not a completely compliant\n  # implementation. This option forces MSVC to use the appropriate value given\n  # the requested --std option. This fixes a compilation issue mismatch between\n  # GCC/Clang and MSVC.\n  #\n  # See: *\n  # https://learn.microsoft.com/en-us/cpp/build/reference/zc-cplusplus?view=msvc-170\n  # * https://en.cppreference.com/w/cpp/preprocessor/replace#Predefined_macros\n  set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} /Zc:__cplusplus\")\n  set(CMAKE_CUDA_FLAGS \"${CMAKE_CUDA_FLAGS} -Xcompiler  /Zc:__cplusplus\")\n\n  set(CMAKE_NINJA_CMCLDEPS_RC OFF)\n  foreach(\n    flag_var\n    CMAKE_C_FLAGS\n    CMAKE_C_FLAGS_DEBUG\n    CMAKE_C_FLAGS_RELEASE\n    CMAKE_C_FLAGS_MINSIZEREL\n    CMAKE_C_FLAGS_RELWITHDEBINFO\n    CMAKE_CXX_FLAGS\n    CMAKE_CXX_FLAGS_DEBUG\n    CMAKE_CXX_FLAGS_RELEASE\n    CMAKE_CXX_FLAGS_MINSIZEREL\n    CMAKE_CXX_FLAGS_RELWITHDEBINFO)\n    # Replace /Zi and /ZI with /Z7\n    if(MSVC_Z7_OVERRIDE)\n      if(${flag_var} MATCHES \"/Z[iI]\")\n        string(REGEX REPLACE \"/Z[iI]\" \"/Z7\" ${flag_var} \"${${flag_var}}\")\n      endif(${flag_var} MATCHES \"/Z[iI]\")\n    endif(MSVC_Z7_OVERRIDE)\n\n    if(${CAFFE2_USE_MSVC_STATIC_RUNTIME})\n      if(${flag_var} MATCHES \"/MD\")\n        string(REGEX REPLACE \"/MD\" \"/MT\" ${flag_var} \"${${flag_var}}\")\n      endif(${flag_var} MATCHES \"/MD\")\n    else()\n      if(${flag_var} MATCHES \"/MT\")\n        string(REGEX REPLACE \"/MT\" \"/MD\" ${flag_var} \"${${flag_var}}\")\n      endif()\n    endif()\n\n    # /bigobj increases number of sections in .obj file, which is needed to link\n    # against libraries in Python 2.7 under Windows For Visual Studio\n    # generators, if /MP is not added, then we may need to add /MP to the flags.\n    # For other generators like ninja, we don't need to add /MP because it is\n    # already handled by the generator itself.\n    if(CMAKE_GENERATOR MATCHES \"Visual Studio\" AND NOT ${flag_var} MATCHES\n                                                   \"/MP\")\n      set(${flag_var} \"${${flag_var}} /MP /bigobj\")\n    else()\n      set(${flag_var} \"${${flag_var}} /bigobj\")\n    endif()\n  endforeach(flag_var)\n\n  foreach(flag_var\n          CMAKE_C_FLAGS CMAKE_C_FLAGS_RELEASE CMAKE_C_FLAGS_MINSIZEREL\n          CMAKE_CXX_FLAGS CMAKE_CXX_FLAGS_RELEASE CMAKE_CXX_FLAGS_MINSIZEREL)\n    if(${flag_var} MATCHES \"/Z[iI7]\")\n      string(REGEX REPLACE \"/Z[iI7]\" \"\" ${flag_var} \"${${flag_var}}\")\n    endif()\n  endforeach(flag_var)\n\n  foreach(\n    flag_var\n    CMAKE_SHARED_LINKER_FLAGS_RELWITHDEBINFO\n    CMAKE_STATIC_LINKER_FLAGS_RELWITHDEBINFO\n    CMAKE_EXE_LINKER_FLAGS_RELWITHDEBINFO\n    CMAKE_MODULE_LINKER_FLAGS_RELWITHDEBINFO\n    CMAKE_SHARED_LINKER_FLAGS_DEBUG\n    CMAKE_STATIC_LINKER_FLAGS_DEBUG\n    CMAKE_EXE_LINKER_FLAGS_DEBUG\n    CMAKE_MODULE_LINKER_FLAGS_DEBUG)\n    # Switch off incremental linking in debug/relwithdebinfo builds\n    if(${flag_var} MATCHES \"/INCREMENTAL\" AND NOT ${flag_var} MATCHES\n                                              \"/INCREMENTAL:NO\")\n      string(REGEX REPLACE \"/INCREMENTAL\" \"/INCREMENTAL:NO\" ${flag_var}\n                           \"${${flag_var}}\")\n    endif()\n  endforeach(flag_var)\n\n  foreach(flag_var CMAKE_SHARED_LINKER_FLAGS CMAKE_STATIC_LINKER_FLAGS\n                   CMAKE_EXE_LINKER_FLAGS CMAKE_MODULE_LINKER_FLAGS)\n    string(APPEND ${flag_var} \" /ignore:4049 /ignore:4217 /ignore:4099\")\n  endforeach(flag_var)\n\n  foreach(flag_var CMAKE_SHARED_LINKER_FLAGS)\n    # https://github.com/pytorch/pytorch/issues/91933: Don't set the manifest\n    # filename explicitly helps fix the linker error when linking\n    # torch_python.dll. The manifest file would still be there in the correct\n    # format torch_python.dll.manifest\n    if(${flag_var} MATCHES \"/MANIFESTFILE:.*\\\\.manifest\")\n      string(REGEX REPLACE \"/MANIFESTFILE:.*\\\\.manifest\" \"\" ${flag_var}\n                           \"${${flag_var}}\")\n    endif()\n  endforeach(flag_var)\n\n  # Try harder\n  string(APPEND CMAKE_CUDA_FLAGS \" -Xcompiler /w -w\")\n\n  string(APPEND CMAKE_CXX_FLAGS \" /FS\")\n  string(APPEND CMAKE_CUDA_FLAGS \" -Xcompiler /FS\")\nendif(MSVC)\n\nstring(APPEND CMAKE_CUDA_FLAGS \" -Xfatbin -compress-all\")\n\n# Set INTERN_BUILD_MOBILE for all mobile builds. Components that are not\n# applicable to mobile are disabled by this variable. Setting\n# `BUILD_PYTORCH_MOBILE_WITH_HOST_TOOLCHAIN` environment variable can force it\n# to do mobile build with host toolchain - which is useful for testing purpose.\nif(ANDROID\n   OR IOS\n   OR DEFINED ENV{BUILD_PYTORCH_MOBILE_WITH_HOST_TOOLCHAIN})\n  set(INTERN_BUILD_MOBILE ON)\n  message(WARNING \"INTERN_BUILD_MOBILE is on, disabling BUILD_LAZY_TS_BACKEND\")\n  set(BUILD_LAZY_TS_BACKEND OFF)\n\n  set(USE_KLEIDIAI OFF)\n  message(WARNING \"KleidiAI cannot be used on Mobile builds. Set it to OFF\")\n\n  # Set -ffunction-sections and -fdata-sections so that each method has its own\n  # text section. This allows the linker to remove unused section when the flag\n  # -Wl,-gc-sections is provided at link time.\n  string(APPEND CMAKE_CXX_FLAGS \" -ffunction-sections\")\n  string(APPEND CMAKE_C_FLAGS \" -ffunction-sections\")\n  string(APPEND CMAKE_CXX_FLAGS \" -fdata-sections\")\n  string(APPEND CMAKE_C_FLAGS \" -fdata-sections\")\n\n  # Please note that the use of the following flags is required when linking\n  # against libtorch_cpu.a for mobile builds. -Wl,--whole-archive -ltorch_cpu\n  # -Wl,--no-whole-archive\n  #\n  # This allows global constructors to be included and run. Global constructors\n  # are used for operator/kernel registration with the PyTorch Dispatcher.\n\n  if(DEFINED ENV{BUILD_PYTORCH_MOBILE_WITH_HOST_TOOLCHAIN})\n    # C10_MOBILE is derived from Android/iOS toolchain macros in\n    # c10/macros/Macros.h, so it needs to be explicitly set here.\n    string(APPEND CMAKE_CXX_FLAGS \" -DC10_MOBILE\")\n  endif()\n\n  if(DEFINED ENV{PYTORCH_MOBILE_TRIM_DISPATCH_KEY_SET})\n    # If PYTORCH_MOBILE_TRIM_DISPATCH_KEY_SET is defined (env var), then define\n    # C10_MOBILE_TRIM_DISPATCH_KEYS, which limits the number of dispatch keys in\n    # OperatorEntry::dispatchTable_ to reduce peak memory during library\n    # initialization.\n    string(APPEND CMAKE_CXX_FLAGS \" -DC10_MOBILE_TRIM_DISPATCH_KEYS\")\n  endif()\nendif()\n\n# INTERN_BUILD_ATEN_OPS is used to control whether to build ATen/TH operators.\nset(INTERN_BUILD_ATEN_OPS ON)\n\nif(NOT DEFINED USE_BLAS)\n  set(USE_BLAS ON)\nendif()\n\n# Build libtorch mobile library, which contains ATen/TH ops and native support\n# for TorchScript model, but doesn't contain not-yet-unified caffe2 ops;\nif(INTERN_BUILD_MOBILE)\n  if(NOT BUILD_SHARED_LIBS AND NOT \"${SELECTED_OP_LIST}\" STREQUAL \"\")\n    string(APPEND CMAKE_CXX_FLAGS \" -DNO_EXPORT\")\n  endif()\n  if(BUILD_MOBILE_AUTOGRAD)\n    set(INTERN_DISABLE_AUTOGRAD OFF)\n  else()\n    set(INTERN_DISABLE_AUTOGRAD ON)\n  endif()\n  set(BUILD_PYTHON OFF)\n  set(BUILD_FUNCTORCH OFF)\n  set(USE_DISTRIBUTED OFF)\n  set(NO_API ON)\n  set(USE_FBGEMM OFF)\n  set(INTERN_DISABLE_ONNX ON)\n  if(USE_BLAS)\n    set(INTERN_USE_EIGEN_BLAS ON)\n  else()\n    set(INTERN_USE_EIGEN_BLAS OFF)\n  endif()\n  # Disable developing mobile interpreter for actual mobile build. Enable it\n  # elsewhere to capture build error.\n  set(INTERN_DISABLE_MOBILE_INTERP ON)\nendif()\n\n# ---[ Version numbers for generated libraries\nfile(READ version.txt TORCH_DEFAULT_VERSION)\n# Strip trailing newline\nstring(REGEX REPLACE \"\\n$\" \"\" TORCH_DEFAULT_VERSION \"${TORCH_DEFAULT_VERSION}\")\nif(\"${TORCH_DEFAULT_VERSION} \" STREQUAL \" \")\n  message(WARNING \"Could not get version from base 'version.txt'\")\n  # If we can't get the version from the version file we should probably set it\n  # to something non-sensical like 0.0.0\n  set(TORCH_DEFAULT_VERSION, \"0.0.0\")\nendif()\nset(TORCH_BUILD_VERSION\n    \"${TORCH_DEFAULT_VERSION}\"\n    CACHE STRING \"Torch build version\")\nif(DEFINED ENV{PYTORCH_BUILD_VERSION})\n  set(TORCH_BUILD_VERSION\n      \"$ENV{PYTORCH_BUILD_VERSION}\"\n      CACHE STRING \"Torch build version\" FORCE)\nendif()\nif(NOT TORCH_BUILD_VERSION)\n  # An empty string was specified so force version to the default\n  set(TORCH_BUILD_VERSION\n      \"${TORCH_DEFAULT_VERSION}\"\n      CACHE STRING \"Torch build version\" FORCE)\nendif()\ncaffe2_parse_version_str(TORCH ${TORCH_BUILD_VERSION})\nset(TORCH_SOVERSION \"${TORCH_VERSION_MAJOR}.${TORCH_VERSION_MINOR}\")\n\n# ---[ CMake scripts + modules\nlist(APPEND CMAKE_MODULE_PATH ${PROJECT_SOURCE_DIR}/cmake/Modules)\n\n# ---[ CMake build directories\nset(CMAKE_ARCHIVE_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}/lib)\nset(CMAKE_LIBRARY_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}/lib)\nset(CMAKE_RUNTIME_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}/bin)\n\nenable_testing()\n\n# ---[ Build variables set within the cmake tree\ninclude(cmake/BuildVariables.cmake)\nset(CAFFE2_ALLOWLIST\n    \"\"\n    CACHE STRING \"A allowlist file of files that one should build.\")\n\n# Set default build type\nif(NOT CMAKE_BUILD_TYPE)\n  message(STATUS \"Build type not set - defaulting to Release\")\n  set(CMAKE_BUILD_TYPE\n      \"Release\"\n      CACHE\n        STRING\n        \"Choose the type of build from: Debug Release RelWithDebInfo MinSizeRel Coverage.\"\n        FORCE)\nendif()\n\n# The below means we are cross compiling for arm64 or x86_64 on MacOSX\nif(NOT IOS\n   AND CMAKE_SYSTEM_NAME STREQUAL \"Darwin\"\n   AND CMAKE_OSX_ARCHITECTURES MATCHES \"^(x86_64|arm64)$\")\n  set(CROSS_COMPILING_MACOSX TRUE)\n  # We need to compile a universal protoc to not fail protobuf build We set\n  # CMAKE_TRY_COMPILE_TARGET_TYPE to STATIC_LIBRARY (vs executable) to succeed\n  # the cmake compiler check for cross-compiling\n  set(protoc_build_command\n      \"./scripts/build_host_protoc.sh --other-flags -DCMAKE_OSX_ARCHITECTURES=\\\"x86_64;arm64\\\" -DCMAKE_TRY_COMPILE_TARGET_TYPE=STATIC_LIBRARY -DCMAKE_C_COMPILER_WORKS=1 -DCMAKE_CXX_COMPILER_WORKS=1\"\n  )\n  # We write to a temp scriptfile because CMake COMMAND dislikes double quotes\n  # in commands\n  file(WRITE ${PROJECT_SOURCE_DIR}/tmp_protoc_script.sh\n       \"#!/bin/bash\\n${protoc_build_command}\")\n  file(\n    COPY ${PROJECT_SOURCE_DIR}/tmp_protoc_script.sh\n    DESTINATION ${PROJECT_SOURCE_DIR}/scripts/\n    FILE_PERMISSIONS OWNER_EXECUTE OWNER_WRITE OWNER_READ)\n  execute_process(\n    COMMAND ./scripts/tmp_protoc_script.sh\n    WORKING_DIRECTORY ${PROJECT_SOURCE_DIR}\n    RESULT_VARIABLE BUILD_HOST_PROTOC_RESULT)\n  file(REMOVE ${PROJECT_SOURCE_DIR}/tmp_protoc_script.sh\n       ${PROJECT_SOURCE_DIR}/scripts/tmp_protoc_script.sh)\n  if(NOT BUILD_HOST_PROTOC_RESULT EQUAL \"0\")\n    message(FATAL_ERROR \"Could not compile universal protoc.\")\n  endif()\n  set(PROTOBUF_PROTOC_EXECUTABLE\n      \"${PROJECT_SOURCE_DIR}/build_host_protoc/bin/protoc\")\n  set(CAFFE2_CUSTOM_PROTOC_EXECUTABLE\n      \"${PROJECT_SOURCE_DIR}/build_host_protoc/bin/protoc\")\nendif()\n\n# ---[ Misc checks to cope with various compiler modes\ninclude(cmake/MiscCheck.cmake)\n\n# External projects\ninclude(ExternalProject)\n\n# ---[ Dependencies ---[ FBGEMM doesn't work on x86 32bit and\n# CMAKE_SYSTEM_PROCESSOR thinks its 64bit\nif(USE_FBGEMM\n   AND((CMAKE_SYSTEM_PROCESSOR STREQUAL \"x86_64\" AND CMAKE_SIZEOF_VOID_P EQUAL\n                                                      4)\n        OR CMAKE_SYSTEM_PROCESSOR STREQUAL \"x86\"))\n  set(USE_FBGEMM OFF)\nendif()\n\nset(BUILD_ONEDNN_GRAPH OFF)\n\nif(MSVC)\n  # The source code is in utf-8 encoding\n  append_cxx_flag_if_supported(\"/utf-8\" CMAKE_CXX_FLAGS)\nendif()\n\n# Note for ROCM platform: 1. USE_ROCM is always ON until\n# include(cmake/Dependencies.cmake) 2. USE_CUDA will become OFF during\n# re-configuration Truth Table: CUDA 1st pass: USE_CUDA=True;USE_ROCM=True,\n# FLASH evaluates to ON by default CUDA 2nd pass: USE_CUDA=True;USE_ROCM=False,\n# FLASH evaluates to ON by default ROCM 1st pass: USE_CUDA=True;USE_ROCM=True,\n# FLASH evaluates to ON by default ROCM 2nd pass: USE_CUDA=False;USE_ROCM=True,\n# FLASH evaluates to ON by default CPU 1st pass: USE_CUDA=False(Cmd\n# Option);USE_ROCM=True, FLASH evaluates to OFF by default CPU 2nd pass:\n# USE_CUDA=False(Cmd Option);USE_ROCM=False, FLASH evaluates to OFF by default\n# Thus we cannot tell ROCM 2nd pass and CPU 1st pass\n#\n# The only solution is to include(cmake/Dependencies.cmake), and defer the\n# aotriton build decision later.\n\ninclude(cmake/Dependencies.cmake)\n\ncmake_dependent_option(\n  USE_FLASH_ATTENTION\n  \"Whether to build the flash_attention kernel for scaled dot product attention.\\\n  Will be disabled if not supported by the platform\"\n  ON\n  \"USE_CUDA OR USE_ROCM;NOT MSVC\"\n  OFF)\n\n# We are currenlty not using alibi attention for Flash So we disable this\n# feature by default We dont currently document this feature because we don't\n# Suspect users building from source will need this\nadd_definitions(-DFLASHATTENTION_DISABLE_ALIBI)\n\n# CAVEAT: Again, Flash Attention2 will error while building for sm52 while Mem\n# Eff Attention won't\ncmake_dependent_option(\n  USE_MEM_EFF_ATTENTION\n  \"Enable memory-efficient attention for scaled dot product attention.\\\n  Will be disabled if not supported by the platform\" ON\n  \"USE_CUDA OR USE_ROCM\" OFF)\n\n#\n# Cannot be put into Dependencies.cmake due circular dependency:\n# USE_FLASH_ATTENTION -> USE_ROCM -> Dependencies.cmake -> aotriton.cmake\n#\nif(USE_ROCM)\n  if(UNIX AND (USE_FLASH_ATTENTION OR USE_MEM_EFF_ATTENTION))\n    include(cmake/External/aotriton.cmake)\n  endif()\nendif()\n\nif(DEBUG_CUDA)\n  string(APPEND CMAKE_CUDA_FLAGS_DEBUG \" -lineinfo\")\n  string(APPEND CMAKE_CUDA_FLAGS_RELWITHDEBINFO \" -lineinfo\")\n  # CUDA-12.1 crashes when trying to compile with --source-in-ptx See\n  # https://github.com/pytorch/pytorch/issues/102372#issuecomment-1572526893\n  if(CMAKE_CUDA_COMPILER_VERSION VERSION_LESS 12.1)\n    string(APPEND CMAKE_CUDA_FLAGS_DEBUG \" --source-in-ptx\")\n    string(APPEND CMAKE_CUDA_FLAGS_RELWITHDEBINFO \" --source-in-ptx\")\n  endif()\nendif(DEBUG_CUDA)\n\nif(USE_FBGEMM)\n  string(APPEND CMAKE_CXX_FLAGS \" -DUSE_FBGEMM\")\nendif()\n\nif(USE_PYTORCH_QNNPACK)\n  string(APPEND CMAKE_CXX_FLAGS \" -DUSE_PYTORCH_QNNPACK\")\nendif()\n\n# Enable sleef on macOS with Apple silicon by default\nif((${CMAKE_SYSTEM_NAME} STREQUAL \"Darwin\") AND (\"${CMAKE_SYSTEM_PROCESSOR}\" STREQUAL \"arm64\"))\n  message(STATUS \"Running on macOS with Apple silicon\")\n  string(APPEND CMAKE_CXX_FLAGS \" -DAT_BUILD_ARM_VEC256_WITH_SLEEF\")\n  add_definitions(-DAT_BUILD_ARM_VEC256_WITH_SLEEF)\nendif()\n\n# Enable sleef on Arm(R) architecture by default (except Android)\nif((NOT ${CMAKE_SYSTEM_NAME} STREQUAL \"Android\")\n  AND(\"${CMAKE_SYSTEM_PROCESSOR}\" MATCHES \"aarch64\"))\n  string(APPEND CMAKE_CXX_FLAGS \" -DAT_BUILD_ARM_VEC256_WITH_SLEEF\")\n  add_definitions(-DAT_BUILD_ARM_VEC256_WITH_SLEEF)\nendif()\n\n\nif(USE_XNNPACK)\n  string(APPEND CMAKE_CXX_FLAGS \" -DUSE_XNNPACK\")\nendif()\n\nif(USE_VULKAN)\n  string(APPEND CMAKE_CXX_FLAGS \" -DUSE_VULKAN\")\n  string(APPEND CMAKE_CXX_FLAGS \" -DUSE_VULKAN_API\")\n\n  if(USE_VULKAN_FP16_INFERENCE)\n    string(APPEND CMAKE_CXX_FLAGS \" -DUSE_VULKAN_FP16_INFERENCE\")\n  endif()\n\n  if(USE_VULKAN_RELAXED_PRECISION)\n    string(APPEND CMAKE_CXX_FLAGS \" -DUSE_VULKAN_RELAXED_PRECISION\")\n  endif()\n\nendif()\n\nif(BUILD_LITE_INTERPRETER)\n  string(APPEND CMAKE_CXX_FLAGS \" -DBUILD_LITE_INTERPRETER\")\nendif()\n\nif(TRACING_BASED)\n  string(APPEND CMAKE_CXX_FLAGS \" -DTRACING_BASED\")\nendif()\n\nif(USE_PYTORCH_METAL)\n  string(APPEND CMAKE_CXX_FLAGS \" -DUSE_PYTORCH_METAL\")\nendif()\n\nif(USE_PYTORCH_METAL_EXPORT)\n  string(APPEND CMAKE_CXX_FLAGS \" -DUSE_PYTORCH_METAL_EXPORT\")\nendif()\n\nif(USE_SOURCE_DEBUG_ON_MOBILE)\n  string(APPEND CMAKE_CXX_FLAGS \" -DSYMBOLICATE_MOBILE_DEBUG_HANDLE\")\nendif()\n\nif(BUILD_LITE_INTERPRETER AND USE_LITE_INTERPRETER_PROFILER)\n  string(APPEND CMAKE_CXX_FLAGS \" -DEDGE_PROFILER_USE_KINETO\")\nendif()\n\nif(USE_COREML_DELEGATE)\n  string(APPEND CMAKE_CXX_FLAGS \" -DUSE_COREML_DELEGATE\")\nendif()\n\n# ---[ Allowlist file if allowlist is specified\ninclude(cmake/Allowlist.cmake)\n\n# ---[ Set link flag, handle additional deps for gcc 4.8 and above\nif(CMAKE_COMPILER_IS_GNUCXX AND NOT ANDROID)\n  message(\n    STATUS\n      \"GCC ${CMAKE_CXX_COMPILER_VERSION}: Adding gcc and gcc_s libs to link line\"\n  )\n  list(APPEND Caffe2_DEPENDENCY_LIBS gcc_s gcc)\nendif()\n\n# ---[ Build flags Re-include to override append_cxx_flag_if_supported from\n# third_party/FBGEMM\ninclude(cmake/public/utils.cmake)\nif(NOT MSVC)\n  string(APPEND CMAKE_CXX_FLAGS \" -O2 -fPIC\")\n  # Eigen fails to build with some versions, so convert this to a warning\n  # Details at http://eigen.tuxfamily.org/bz/show_bug.cgi?id=1459\n  string(APPEND CMAKE_CXX_FLAGS \" -Wall\")\n  string(APPEND CMAKE_CXX_FLAGS \" -Wextra\")\n  append_cxx_flag_if_supported(\"-Werror=return-type\" CMAKE_CXX_FLAGS)\n  append_cxx_flag_if_supported(\"-Werror=non-virtual-dtor\" CMAKE_CXX_FLAGS)\n  append_cxx_flag_if_supported(\"-Werror=braced-scalar-init\" CMAKE_CXX_FLAGS)\n  append_cxx_flag_if_supported(\"-Werror=range-loop-construct\" CMAKE_CXX_FLAGS)\n  append_cxx_flag_if_supported(\"-Werror=bool-operation\" CMAKE_CXX_FLAGS)\n  append_cxx_flag_if_supported(\"-Wnarrowing\" CMAKE_CXX_FLAGS)\n  append_cxx_flag_if_supported(\"-Wno-missing-field-initializers\"\n                               CMAKE_CXX_FLAGS)\n  append_cxx_flag_if_supported(\"-Wno-unknown-pragmas\" CMAKE_CXX_FLAGS)\n  append_cxx_flag_if_supported(\"-Wno-unused-parameter\" CMAKE_CXX_FLAGS)\n  append_cxx_flag_if_supported(\"-Wno-strict-overflow\" CMAKE_CXX_FLAGS)\n  append_cxx_flag_if_supported(\"-Wno-strict-aliasing\" CMAKE_CXX_FLAGS)\n  append_cxx_flag_if_supported(\"-Wno-stringop-overflow\" CMAKE_CXX_FLAGS)\n  append_cxx_flag_if_supported(\"-Wvla-extension\" CMAKE_CXX_FLAGS)\n  append_cxx_flag_if_supported(\"-Wsuggest-override\" CMAKE_CXX_FLAGS)\n  append_cxx_flag_if_supported(\"-Wnewline-eof\" CMAKE_CXX_FLAGS)\n  append_cxx_flag_if_supported(\"-Winconsistent-missing-override\"\n                               CMAKE_CXX_FLAGS)\n  append_cxx_flag_if_supported(\"-Winconsistent-missing-destructor-override\"\n                               CMAKE_CXX_FLAGS)\n  if(\"${CMAKE_CXX_COMPILER_ID}\" MATCHES \"Clang\")\n    string(APPEND CMAKE_CXX_FLAGS \" -Wno-pass-failed\")\n  endif()\n  if(CMAKE_COMPILER_IS_GNUCXX)\n    # Suppress \"The ABI for passing parameters with 64-byte alignment has\n    # changed in GCC 4.6\"\n    string(APPEND CMAKE_CXX_FLAGS \" -Wno-psabi\")\n  endif()\n\n  # Use ld.gold if available, fall back to ld.bfd (the default ld) if not\n  if(USE_GOLD_LINKER)\n    if(USE_DISTRIBUTED AND USE_MPI)\n      # Same issue as here with default MPI on Ubuntu\n      # https://bugs.launchpad.net/ubuntu/+source/deal.ii/+bug/1841577\n      message(WARNING \"Refusing to use gold when USE_MPI=1\")\n    else()\n      execute_process(\n        COMMAND \"${CMAKE_C_COMPILER}\" -fuse-ld=gold -Wl,--version\n        ERROR_QUIET\n        OUTPUT_VARIABLE LD_VERSION)\n      if(NOT \"${LD_VERSION}\" MATCHES \"GNU gold\")\n        message(\n          WARNING\n            \"USE_GOLD_LINKER was set but ld.gold isn't available, turning it off\"\n        )\n        set(USE_GOLD_LINKER OFF)\n      else()\n        message(STATUS \"ld.gold is available, using it to link\")\n        set(CMAKE_EXE_LINKER_FLAGS \"${CMAKE_EXE_LINKER_FLAGS} -fuse-ld=gold\")\n        set(CMAKE_SHARED_LINKER_FLAGS\n            \"${CMAKE_SHARED_LINKER_FLAGS} -fuse-ld=gold\")\n        set(CMAKE_MODULE_LINKER_FLAGS\n            \"${CMAKE_MODULE_LINKER_FLAGS} -fuse-ld=gold\")\n      endif()\n    endif()\n  endif()\n\n  append_cxx_flag_if_supported(\"-Wno-error=old-style-cast\" CMAKE_CXX_FLAGS)\n  append_cxx_flag_if_supported(\"-Wconstant-conversion\" CMAKE_CXX_FLAGS)\n  append_cxx_flag_if_supported(\"-Wno-aligned-allocation-unavailable\"\n                               CMAKE_CXX_FLAGS)\n  append_cxx_flag_if_supported(\"-Qunused-arguments\" CMAKE_CXX_FLAGS)\n\n  if(${USE_COLORIZE_OUTPUT})\n    # Why compiler checks are necessary even when `try_compile` is used Because\n    # of the bug in ccache that can incorrectly identify `-fcolor-diagnostics`\n    # As supported by GCC, see https://github.com/ccache/ccache/issues/740 (for\n    # older ccache) and https://github.com/ccache/ccache/issues/1275 (for newer\n    # ones)\n    if(\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"GNU\")\n      append_cxx_flag_if_supported(\"-fdiagnostics-color=always\" CMAKE_CXX_FLAGS)\n    else()\n      append_cxx_flag_if_supported(\"-fcolor-diagnostics\" CMAKE_CXX_FLAGS)\n    endif()\n  endif()\n\n  append_cxx_flag_if_supported(\"-faligned-new\" CMAKE_CXX_FLAGS)\n\n  if(WERROR)\n    append_cxx_flag_if_supported(\"-Werror\" CMAKE_CXX_FLAGS)\n    if(NOT COMPILER_SUPPORT_WERROR)\n      set(WERROR FALSE)\n    endif()\n  endif()\n  append_cxx_flag_if_supported(\"-Wno-maybe-uninitialized\" CMAKE_CXX_FLAGS)\n  append_cxx_flag_if_supported(\"-fstandalone-debug\" CMAKE_CXX_FLAGS_DEBUG)\n  if(CMAKE_SYSTEM_PROCESSOR MATCHES \"aarch64\" AND CMAKE_CXX_COMPILER_ID MATCHES \"GNU\")\n    if(CMAKE_BUILD_TYPE MATCHES Debug)\n      message(Warning \"Applying -Og optimization for aarch64 GCC debug build to workaround ICE\")\n    endif()\n    string(APPEND CMAKE_CXX_FLAGS_DEBUG \" -fno-omit-frame-pointer -Og\")\n    string(APPEND CMAKE_LINKER_FLAGS_DEBUG \" -fno-omit-frame-pointer -Og\")\n  else()\n    string(APPEND CMAKE_CXX_FLAGS_DEBUG \" -fno-omit-frame-pointer -O0\")\n    string(APPEND CMAKE_LINKER_FLAGS_DEBUG \" -fno-omit-frame-pointer -O0\")\n  endif()\n  append_cxx_flag_if_supported(\"-fno-math-errno\" CMAKE_CXX_FLAGS)\n  append_cxx_flag_if_supported(\"-fno-trapping-math\" CMAKE_CXX_FLAGS)\n  append_cxx_flag_if_supported(\"-Werror=format\" CMAKE_CXX_FLAGS)\n  if(CMAKE_COMPILER_IS_GNUCXX AND CMAKE_CXX_COMPILER_VERSION VERSION_GREATER_EQUAL 13)\n    append_cxx_flag_if_supported(\"-Wno-dangling-reference\" CMAKE_CXX_FLAGS)\n    append_cxx_flag_if_supported(\"-Wno-error=dangling-reference\" CMAKE_CXX_FLAGS)\n    append_cxx_flag_if_supported(\"-Wno-error=redundant-move\" CMAKE_CXX_FLAGS)\n  endif()\nelse()\n  # skip unwanted includes from windows.h\n  add_compile_definitions(WIN32_LEAN_AND_MEAN)\n  # Windows SDK broke compatibility since version 25131, but introduced this\n  # define for backward compatibility.\n  add_compile_definitions(_UCRT_LEGACY_INFINITY)\n  # disable min/max macros\n  add_compile_definitions(NOMINMAX)\n  # Turn off these warnings on Windows. destructor was implicitly defined as\n  # delete\n  append_cxx_flag_if_supported(\"/wd4624\" CMAKE_CXX_FLAGS)\n  # unknown pragma\n  append_cxx_flag_if_supported(\"/wd4068\" CMAKE_CXX_FLAGS)\n  # unexpected tokens following preprocessor directive - expected a newline\n  append_cxx_flag_if_supported(\"/wd4067\" CMAKE_CXX_FLAGS)\n  # conversion from 'size_t' to 'unsigned int', possible loss of data\n  append_cxx_flag_if_supported(\"/wd4267\" CMAKE_CXX_FLAGS)\n  # no suitable definition provided for explicit template instantiation request\n  append_cxx_flag_if_supported(\"/wd4661\" CMAKE_CXX_FLAGS)\n  # recursive on all control paths, function will cause runtime stack overflow\n  append_cxx_flag_if_supported(\"/wd4717\" CMAKE_CXX_FLAGS)\n  # conversion from '_Ty' to '_Ty', possible loss of data\n  append_cxx_flag_if_supported(\"/wd4244\" CMAKE_CXX_FLAGS)\n  # unsafe use of type 'bool' in operation\n  append_cxx_flag_if_supported(\"/wd4804\" CMAKE_CXX_FLAGS)\n  # inconsistent dll linkage\n  append_cxx_flag_if_supported(\"/wd4273\" CMAKE_CXX_FLAGS)\nendif()\n\nif(CMAKE_SYSTEM_PROCESSOR MATCHES \"aarch64\")\n  include(CheckCSourceCompiles)\n  check_c_source_compiles(\n    \"#include <arm_neon.h>\nint main() {\n  float a[] = {1.0, 1.0};\n  float32x4x2_t v;\n  v.val[0] = vcombine_f32 (vcreate_f32 (0UL), vcreate_f32 (0UL));\n  v.val[1] = vcombine_f32 (vcreate_f32 (0UL), vcreate_f32 (0UL));\n  vst1q_f32_x2(a, v);\n  return 0;\n}\"\n    HAS_VST1)\n\n  if(NOT HAS_VST1)\n    string(APPEND CMAKE_CXX_FLAGS \" -DMISSING_ARM_VST1\")\n  endif()\nendif()\n\nif(CMAKE_SYSTEM_PROCESSOR MATCHES \"aarch64\")\n  include(CheckCSourceCompiles)\n  check_c_source_compiles(\n    \"#include <arm_neon.h>\nint main() {\n  float a[] = {1.0, 1.0};\n  vld1q_f32_x2(a);\n  return 0;\n}\"\n    HAS_VLD1)\n\n  if(NOT HAS_VLD1)\n    string(APPEND CMAKE_CXX_FLAGS \" -DMISSING_ARM_VLD1\")\n  endif()\nendif()\n\n# Add code coverage flags to supported compilers\nif(USE_CPP_CODE_COVERAGE)\n  if(\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"GNU\")\n    string(APPEND CMAKE_C_FLAGS \" --coverage -fprofile-abs-path\")\n    string(APPEND CMAKE_CXX_FLAGS \" --coverage -fprofile-abs-path\")\n  elseif(\"${CMAKE_CXX_COMPILER_ID}\" MATCHES \"Clang\")\n    string(APPEND CMAKE_C_FLAGS \" -fprofile-instr-generate -fcoverage-mapping\")\n    string(APPEND CMAKE_CXX_FLAGS\n           \" -fprofile-instr-generate -fcoverage-mapping\")\n  else()\n    message(\n      ERROR\n      \"Code coverage for compiler ${CMAKE_CXX_COMPILER_ID} is unsupported\")\n  endif()\n\nendif()\n\nif(APPLE)\n  if(USE_MPS)\n    string(APPEND CMAKE_OBJCXX_FLAGS \" -DUSE_MPS -fno-objc-arc\")\n    string(APPEND CMAKE_CXX_FLAGS \" -DUSE_MPS\")\n    string(\n      APPEND\n      CMAKE_SHARED_LINKER_FLAGS\n      \" -weak_framework Foundation -weak_framework MetalPerformanceShaders -weak_framework MetalPerformanceShadersGraph -weak_framework Metal\"\n    )\n    # To suppress MPSGraph availability warnings\n    append_cxx_flag_if_supported(\"-Wno-unguarded-availability-new\"\n                                 CMAKE_OBJCXX_FLAGS)\n  endif()\n  append_cxx_flag_if_supported(\"-Wno-missing-braces\" CMAKE_CXX_FLAGS)\nendif()\n\nif(USE_XPU)\n  string(APPEND CMAKE_CXX_FLAGS \" -DUSE_XPU\")\nendif()\n\nif(EMSCRIPTEN)\n  string(\n    APPEND\n    CMAKE_CXX_FLAGS\n    \" -Wno-implicit-function-declaration -DEMSCRIPTEN -s DISABLE_EXCEPTION_CATCHING=0\"\n  )\nendif()\n\nappend_cxx_flag_if_supported(\"-Wno-stringop-overflow\" CMAKE_CXX_FLAGS)\n\nif(ANDROID AND (NOT ANDROID_DEBUG_SYMBOLS))\n  if(CMAKE_COMPILER_IS_GNUCXX)\n    string(APPEND CMAKE_CXX_FLAGS \" -s\")\n  elseif(\"${CMAKE_CXX_COMPILER_ID}\" MATCHES \"Clang\")\n    string(APPEND CMAKE_CXX_FLAGS \" -g0\")\n  else()\n    string(APPEND CMAKE_EXE_LINKER_FLAGS \" -s\")\n  endif()\nendif()\n\nif(NOT APPLE AND UNIX)\n  list(APPEND Caffe2_DEPENDENCY_LIBS dl)\nendif()\n\n# Prefix path to Caffe2 headers. If a directory containing installed Caffe2\n# headers was inadvertently added to the list of include directories, prefixing\n# PROJECT_SOURCE_DIR means this source tree always takes precedence.\ninclude_directories(BEFORE ${PROJECT_SOURCE_DIR})\n\n# Prefix path to generated Caffe2 headers. These need to take precedence over\n# their empty counterparts located in PROJECT_SOURCE_DIR.\ninclude_directories(BEFORE ${PROJECT_BINARY_DIR})\n\ninclude_directories(BEFORE ${PROJECT_SOURCE_DIR}/aten/src/)\ninclude_directories(BEFORE ${CMAKE_BINARY_DIR}/aten/src/)\n\nif(USE_MIMALLOC)\n  set(MI_OVERRIDE OFF)\n  set(MI_BUILD_SHARED OFF)\n  set(MI_BUILD_OBJECT OFF)\n  set(MI_BUILD_TESTS OFF)\n  add_definitions(-DUSE_MIMALLOC)\n  add_subdirectory(third_party/mimalloc)\n  include_directories(third_party/mimalloc/include)\nendif()\n\nif(USE_MIMALLOC AND USE_MIMALLOC_ON_MKL)\n  add_definitions(-DUSE_MIMALLOC_ON_MKL)\nendif()\n\n# ---[ Main build\nadd_subdirectory(c10)\nadd_subdirectory(caffe2)\n\n# ---[ CMake related files Uninistall option.\nif(NOT TARGET caffe2_uninstall)\n  configure_file(\n    ${CMAKE_CURRENT_SOURCE_DIR}/cmake/cmake_uninstall.cmake.in\n    ${CMAKE_CURRENT_BINARY_DIR}/cmake_uninstall.cmake IMMEDIATE @ONLY)\n\n  add_custom_target(\n    caffe2_uninstall COMMAND ${CMAKE_COMMAND} -P\n                             ${CMAKE_CURRENT_BINARY_DIR}/cmake_uninstall.cmake)\nendif()\n\n# ---[ Make configuration files for cmake to allow dependent libraries easier\n# access to Caffe2.\n\nif((NOT USE_GLOG)\n   OR(NOT USE_GFLAGS)\n   OR BUILD_CUSTOM_PROTOBUF)\n  message(WARNING \"Generated cmake files are only fully tested if one builds \"\n                  \"with system glog, gflags, and protobuf. Other settings may \"\n                  \"generate files that are not well tested.\")\nendif()\n\nif(USE_CUDA OR USE_ROCM)\n  # TODO: check if we should include other cuda dependency libraries to the\n  # interface as well.\n\nendif()\n\n# Note(jiayq): when building static libraries, all PRIVATE dependencies will\n# also become interface libraries, and as a result if there are any dependency\n# libraries that are not exported, the following install export script will\n# fail. As a result, we will only provide the targets cmake files for shared lib\n# installation. For more info, read:\n# https://cmake.org/pipermail/cmake/2016-May/063400.html\nif(BUILD_SHARED_LIBS)\n  configure_file(${PROJECT_SOURCE_DIR}/cmake/Caffe2Config.cmake.in\n                 ${PROJECT_BINARY_DIR}/Caffe2Config.cmake @ONLY)\n  install(\n    FILES ${PROJECT_BINARY_DIR}/Caffe2Config.cmake\n    DESTINATION share/cmake/Caffe2\n    COMPONENT dev)\n  install(\n    FILES ${PROJECT_SOURCE_DIR}/cmake/public/cuda.cmake\n          ${PROJECT_SOURCE_DIR}/cmake/public/xpu.cmake\n          ${PROJECT_SOURCE_DIR}/cmake/public/glog.cmake\n          ${PROJECT_SOURCE_DIR}/cmake/public/gflags.cmake\n          ${PROJECT_SOURCE_DIR}/cmake/public/mkl.cmake\n          ${PROJECT_SOURCE_DIR}/cmake/public/mkldnn.cmake\n          ${PROJECT_SOURCE_DIR}/cmake/public/protobuf.cmake\n          ${PROJECT_SOURCE_DIR}/cmake/public/utils.cmake\n          ${PROJECT_SOURCE_DIR}/cmake/public/LoadHIP.cmake\n    DESTINATION share/cmake/Caffe2/public\n    COMPONENT dev)\n  install(\n    DIRECTORY ${PROJECT_SOURCE_DIR}/cmake/Modules_CUDA_fix\n    DESTINATION share/cmake/Caffe2/\n    COMPONENT dev)\n  install(\n    FILES ${PROJECT_SOURCE_DIR}/cmake/Modules/FindCUDAToolkit.cmake\n    DESTINATION share/cmake/Caffe2/\n    COMPONENT dev)\n  install(\n    FILES ${PROJECT_SOURCE_DIR}/cmake/Modules/FindCUSPARSELT.cmake\n    DESTINATION share/cmake/Caffe2/\n    COMPONENT dev)\n  install(\n    FILES ${PROJECT_SOURCE_DIR}/cmake/Modules/FindCUDSS.cmake\n    DESTINATION share/cmake/Caffe2/\n    COMPONENT dev)\n  install(\n    FILES ${PROJECT_SOURCE_DIR}/cmake/Modules/FindSYCLToolkit.cmake\n    DESTINATION share/cmake/Caffe2/\n    COMPONENT dev)\n  if(NOT BUILD_LIBTORCHLESS)\n    install(\n      EXPORT Caffe2Targets\n      DESTINATION share/cmake/Caffe2\n      FILE Caffe2Targets.cmake\n      COMPONENT dev)\n  endif()\nelse()\n  message(WARNING \"Generated cmake files are only available when building \"\n                  \"shared libs.\")\nendif()\n\n# ---[ Binaries Binaries will be built after the Caffe2 main libraries and the\n# modules are built. For the binaries, they will be linked to the Caffe2 main\n# libraries, as well as all the modules that are built with Caffe2 (the ones\n# built in the previous Modules section above).\nif(BUILD_BINARY)\n  add_subdirectory(binaries)\nendif()\n\n# ---[ JNI\nif(BUILD_JNI)\n  if(NOT MSVC)\n    string(APPEND CMAKE_CXX_FLAGS \" -Wno-unused-variable\")\n  endif()\n  set(BUILD_LIBTORCH_WITH_JNI 1)\n  set(FBJNI_SKIP_TESTS 1)\n  add_subdirectory(android/pytorch_android)\nendif()\n\ninclude(cmake/Summary.cmake)\ncaffe2_print_configuration_summary()\n\nif(BUILD_FUNCTORCH)\n  add_subdirectory(functorch)\nendif()\n\n# Parse custom debug info\nif(DEFINED USE_CUSTOM_DEBINFO)\n  string(REPLACE \";\" \" \" SOURCE_FILES \"${USE_CUSTOM_DEBINFO}\")\n  message(STATUS \"Source files with custom debug infos: ${SOURCE_FILES}\")\n\n  string(REGEX REPLACE \" +\" \";\" SOURCE_FILES_LIST \"${SOURCE_FILES}\")\n\n  # Set the COMPILE_FLAGS property for each source file\n  foreach(SOURCE_FILE ${SOURCE_FILES_LIST})\n    # We have to specify the scope here. We do this by specifying the targets we\n    # care about and caffe2/ for all test targets defined there\n    if(BUILD_LIBTORCHLESS)\n      caffe2_update_option(USE_CUDA OFF)\n      set(ALL_PT_TARGETS \"torch_python;${C10_LIB};${TORCH_CPU_LIB};${TORCH_LIB}\")\n    else()\n      # @todo test if we can remove this\n      set(ALL_PT_TARGETS \"torch_python;c10;torch_cpu;torch\")\n    endif()\n    set_source_files_properties(\n      ${SOURCE_FILE} DIRECTORY \"caffe2/\" TARGET_DIRECTORY ${ALL_PT_TARGETS}\n      PROPERTIES COMPILE_FLAGS \"-g\")\n  endforeach()\n\n  # Link everything with debug info when any file is in debug mode\n  set(CMAKE_EXE_LINKER_FLAGS \"${CMAKE_EXE_LINKER_FLAGS} -g\")\n  set(CMAKE_SHARED_LINKER_FLAGS \"${CMAKE_SHARED_LINKER_FLAGS} -g\")\nendif()\n\n# Bundle PTXAS if needed\nif(BUILD_BUNDLE_PTXAS AND USE_CUDA)\n  if(NOT EXISTS \"${PROJECT_SOURCE_DIR}/build/bin/ptxas\")\n    message(STATUS \"Copying PTXAS into the bin folder\")\n    file(COPY \"${CUDAToolkit_BIN_DIR}/ptxas\"\n         DESTINATION \"${PROJECT_BINARY_DIR}\")\n  endif()\n  install(PROGRAMS \"${PROJECT_BINARY_DIR}/ptxas\"\n          DESTINATION \"${CMAKE_INSTALL_BINDIR}\")\nendif()\n"
        },
        {
          "name": "CODEOWNERS",
          "type": "blob",
          "size": 6.31,
          "content": "# IMPORTANT:\n# This file is ONLY used to subscribe for notifications for PRs\n# related to a specific file path. Approvals from people in this\n# file are not required for merges.\n\n# This is a comment.\n# Each line is a file pattern followed by one or more owners.\n# For module labels => owners mapping, please see https://github.com/pytorch/pytorch/issues/24422.\n\n/torch/utils/cpp_extension.py @fmassa @soumith @ezyang\n\n# Not there to strictly require the approval, but to be tagged as a reviewer\n# on the PRs to push them into a high priority inbox.\n/torch/csrc/autograd/ @albanD @soulitzer\n/torch/autograd/ @albanD @soulitzer\n/tools/autograd/ @albanD @soulitzer\n/torch/nn/ @albanD @jbschlosser @mikaylagawarecki\n/torch/optim/ @albanD @janeyx99\n/test/test_public_bindings.py @albanD\n/test/allowlist_for_publicAPI.json @albanD\n/test/forward_backward_compatibility/check_forward_backward_compatibility.py @larryliu0820\n/docs/source/conf.py @albanD\n/aten/src/ATen/native/tags.yaml @ezyang\n\n# Architecture Optimization (quantization, sparsity, etc.)\n/aten/src/ATen/native/ao_sparse @salilsdesai @kimishpatel @digantdesai @jianyuh\n/aten/src/ATen/native/quantized @jerryzh168 @salilsdesai @kimishpatel @digantdesai @jianyuh\n/aten/src/ATen/native/quantized/cpu @jerryzh168 @salilsdesai @kimishpatel @digantdesai @jianyuh\n/aten/src/ATen/native/quantized/cuda @jerryzh168\n/aten/src/ATen/native/quantized/cudnn @jerryzh168\n/test/test_quantization.py @jerryzh168\n/test/ao/ @jerryzh168 @hdcharles\n/test/quantization/ @jerryzh168\n/torch/quantization/ @jerryzh168\nao/sparisty/ @hdcharles\nao/quantization/ @jerryzh168\nnn/intrinsic/ @jerryzh168\nnn/quantized/ @jerryzh168\nnn/quantizable/ @jerryzh168\nnn/qat/ @jerryzh168\n\n# Tensorpipe RPC Agent.\n/torch/csrc/distributed/rpc/tensorpipe_agent.cpp @jiayisuse @osalpekar @lw\n/torch/csrc/distributed/rpc/tensorpipe_agent.h @jiayisuse @osalpekar @lw\n\n# Distributed\n# c10d backend APIs\n/torch/csrc/distributed/c10d/Backend.* @kwen2501\n/torch/csrc/distributed/c10d/Ops.* @kwen2501\n\n# ONNX Export\n/torch/_dynamo/backends/onnxrt.py @wschin @xadupre\n/torch/csrc/jit/passes/onnx.h @titaiwangms @shubhambhokare1 @xadupre\n/torch/csrc/jit/passes/onnx.cpp @titaiwangms @shubhambhokare1 @xadupre\n/torch/csrc/jit/passes/onnx/ @titaiwangms @shubhambhokare1 @xadupre\n/torch/onnx/ @titaiwangms @shubhambhokare1 @justinchuby @wschin @xadupre\n/test/onnx/  @titaiwangms @shubhambhokare1 @justinchuby @wschin @xadupre\n\n# CI\n/.ci  @pytorch/pytorch-dev-infra\n\n# Docker\n/.ci/docker/ @jeffdaily\n/.ci/docker/ci_commit_pins/triton.txt @desertfire @Chillee @eellison @shunting314 @bertmaher @jeffdaily @jataylo @jithunnair-amd @pruthvistony\n/.ci/docker/ci_commit_pins/triton-xpu.txt @EikanWang @gujinghui\n\n# Github Actions\n# This list is for people wanting to be notified every time there's a change\n# related to Github Actions\n/.github/ @pytorch/pytorch-dev-infra\n\n# Custom Test Infrastructure\n/test/run_test.py @pytorch/pytorch-dev-infra\n/torch/testing/_internal/common_device_type.py @mruberry\n/torch/testing/_internal/common_utils.py @pytorch/pytorch-dev-infra\n/torch/testing/_internal/hop_db.py @tugsbayasgalan @zou3519 @ydwu4\n\n# Parametrizations\n/torch/nn/utils/parametriz*.py @lezcano\n\n# torch.linalg\n# docs\n/torch/linalg/ @lezcano @IvanYashchuk\n# code\n/aten/src/ATen/native/**/*LinearAlgebra* @lezcano @nikitaved @IvanYashchuk\n# tests\n/test/test_linalg.py @lezcano @nikitaved @IvanYashchuk\n\n# OpInfo-related files\n/torch/testing/_internal/common_methods_invocations.py @mruberry\n/torch/testing/_internal/common_device_type.py @mruberry\ntest/test_ops.py @mruberry\ntest/test_ops_gradients.py @mruberry @soulitzer\ntest/test_ops_fwd_gradients.py @mruberry @soulitzer\ntest/test_unary_ufuncs.py @mruberry\ntest/test_binary_ufuncs.py @mruberry\ntest/test_reductions.py @mruberry\ntest/test_type_promotion.py @mruberry\n\n# functorch-related things\n# This list is for people wanting to be notified every time there's a change\n# Useful for e.g. auditing xfails that other folks add to tests\ntest/functorch/test_ops.py @zou3519 @chillee @kshitij12345\ntest/functorch/test_vmap.py @zou3519 @chillee @kshitij12345\n\n# HOPs\ntorch/_higher_order_ops/*.py @zou3519\ntorch/_dynamo/variables/higher_order_ops.py @zou3519\n\n# AOTAutograd\ntorch/_functorch/_aot_autograd/*.py @bdhirsh\ntorch/_functorch/aot_autograd.py @bdhirsh\n\n# torch MPS\ntest/test_mps.py @kulinseth @malfet\naten/src/ATen/mps/ @kulinseth @malfet\naten/src/ATen/native/mps/ @kulinseth @malfet\n\n# MTIA\naten/src/ATen/detail/MTIAHooksInterface.h @egienvalue\ntorch/csrc/mtia/ @egienvalue\n\n# Profiler\ntorch/csrc/autograd/profiler* @sraikund16\ntorch/autograd/profiler* @sraikund16\ntorch/csrc/profiler/ @sraikund16\ntorch/profiler/ @sraikund16\n\n# AOTDispatch tests\ntest/functorch/test_aotdispatch.py @ezyang @Chillee\n\n# Dataloader\ntorch/utils/data/ @andrewkho @divyanshk\n\n# hipify\ntorch/utils/hipify/ @jeffdaily @jithunnair-amd\ntools/amd_build/ @jeffdaily @jithunnair-amd\n\n# ROCm-specific files\naten/src/ATen/hip/ @jeffdaily @jithunnair-amd\naten/src/ATen/miopen/ @jeffdaily @jithunnair-amd\naten/src/ATen/native/miopen/ @jeffdaily @jithunnair-amd\nc10/hip @jeffdaily @jithunnair-amd\ncaffe2/core/hip @jeffdaily @jithunnair-amd\ncaffe2/operators/hip @jeffdaily @jithunnair-amd\ncaffe2/operators/rnn/hip @jeffdaily @jithunnair-amd\ncaffe2/utils/hip @jeffdaily @jithunnair-amd\n\n# XPU-specific files\n/aten/src/ATen/xpu/ @EikanWang @gujinghui\n/c10/xpu/ @EikanWang @gujinghui\n/torch/csrc/xpu/ @EikanWang @gujinghui\n/torch/xpu/ @EikanWang @gujinghui\n/test/xpu/ @EikanWang @gujinghui\n/test/test_xpu.py @EikanWang @gujinghui\n/third_party/xpu.txt @EikanWang @gujinghui\n\n# torch.export\n/torch/export/ @avikchaudhuri @tugsbayasgalan @zhxchen17 @ydwu4 @angelayi\n/torch/_export/ @avikchaudhuri @tugsbayasgalan @zhxchen17 @ydwu4 @angelayi\n\n# serialization-related files\n/aten/src/ATen/MapAllocator* @mikaylagawarecki\n/caffe2/serialize/ @mikaylagawarecki\n/torch/serialization.py @mikaylagawarecki\n/torch/storage.py @mikaylagawarecki\n/torch/csrc/Storage* @mikaylagawarecki\n# subscribing for PyTorchFileWriter/PyTorchFileReader changes\n/torch/csrc/jit/python/init.cpp @mikaylagawarecki\n\n# CUDA and CUDA math libraries\naten/src/ATen/cuda/ @eqy @syed-ahmed\naten/src/ATen/cudnn/ @eqy @syed-ahmed\naten/src/ATen/native/cuda/ @eqy @syed-ahmed\naten/src/ATen/native/cudnn/ @eqy @syed-ahmed\nc10/cuda @eqy @syed-ahmed\ntorch/cuda/ @eqy @syed-ahmed\ntorch/csrc/cuda/ @eqy @syed-ahmed\ntorch/backends/cuda/ @eqy @syed-ahmed\ntorch/backends/cudnn/ @eqy @syed-ahmed\n"
        },
        {
          "name": "CODE_OF_CONDUCT.md",
          "type": "blob",
          "size": 3.26,
          "content": "# Code of Conduct\n\n## Our Pledge\n\nIn the interest of fostering an open and welcoming environment, we as\ncontributors and maintainers pledge to make participation in our project and\nour community a harassment-free experience for everyone, regardless of age, body\nsize, disability, ethnicity, sex characteristics, gender identity and expression,\nlevel of experience, education, socio-economic status, nationality, personal\nappearance, race, religion, or sexual identity and orientation.\n\n## Our Standards\n\nExamples of behavior that contributes to creating a positive environment\ninclude:\n\n* Using welcoming and inclusive language\n* Being respectful of differing viewpoints and experiences\n* Gracefully accepting constructive criticism\n* Focusing on what is best for the community\n* Showing empathy towards other community members\n\nExamples of unacceptable behavior by participants include:\n\n* The use of sexualized language or imagery and unwelcome sexual attention or\nadvances\n* Trolling, insulting/derogatory comments, and personal or political attacks\n* Public or private harassment\n* Publishing others' private information, such as a physical or electronic\naddress, without explicit permission\n* Other conduct which could reasonably be considered inappropriate in a\nprofessional setting\n\n## Our Responsibilities\n\nProject maintainers are responsible for clarifying the standards of acceptable\nbehavior and are expected to take appropriate and fair corrective action in\nresponse to any instances of unacceptable behavior.\n\nProject maintainers have the right and responsibility to remove, edit, or\nreject comments, commits, code, wiki edits, issues, and other contributions\nthat are not aligned to this Code of Conduct, or to ban temporarily or\npermanently any contributor for other behaviors that they deem inappropriate,\nthreatening, offensive, or harmful.\n\n## Scope\n\nThis Code of Conduct applies within all project spaces, and it also applies when\nan individual is representing the project or its community in public spaces.\nExamples of representing a project or community include using an official\nproject e-mail address, posting via an official social media account, or acting\nas an appointed representative at an online or offline event. Representation of\na project may be further defined and clarified by project maintainers.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported by contacting the project team at <conduct@pytorch.org>. All\ncomplaints will be reviewed and investigated and will result in a response that\nis deemed necessary and appropriate to the circumstances. The project team is\nobligated to maintain confidentiality with regard to the reporter of an incident.\nFurther details of specific enforcement policies may be posted separately.\n\nProject maintainers who do not follow or enforce the Code of Conduct in good\nfaith may face temporary or permanent repercussions as determined by other\nmembers of the project's leadership.\n\n## Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant][homepage], version 1.4,\navailable at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html\n\n[homepage]: https://www.contributor-covenant.org\n\nFor answers to common questions about this code of conduct, see\nhttps://www.contributor-covenant.org/faq\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 57.13,
          "content": "Thank you for your interest in contributing to PyTorch!\nIf you're a new contributor, please first take a read through our\n[Contributing Guide](https://github.com/pytorch/pytorch/wiki/The-Ultimate-Guide-to-PyTorch-Contributions), specifically the [Submitting a Change](https://github.com/pytorch/pytorch/wiki/The-Ultimate-Guide-to-PyTorch-Contributions#submitting-a-change) section\nthat walks through the process of contributing a change to PyTorch.\n\nThe rest of this document (CONTRIBUTING.md) covers some of the more technical\naspects of contributing to PyTorch.\n\n# Table of Contents\n\n<!-- toc -->\n\n- [Developing PyTorch](#developing-pytorch)\n  - [Setup the development environment](#setup-the-development-environment)\n  - [Tips and Debugging](#tips-and-debugging)\n- [Nightly Checkout & Pull](#nightly-checkout--pull)\n- [Codebase structure](#codebase-structure)\n- [Unit testing](#unit-testing)\n  - [Python Unit Testing](#python-unit-testing)\n  - [Better local unit tests with `pytest`](#better-local-unit-tests-with-pytest)\n  - [Local linting](#local-linting)\n    - [Running `mypy`](#running-mypy)\n  - [C++ Unit Testing](#c-unit-testing)\n  - [Run Specific CI Jobs](#run-specific-ci-jobs)\n- [Merging your Change](#merging-your-change)\n- [Writing documentation](#writing-documentation)\n  - [Docstring type formatting](#docstring-type-formatting)\n  - [Building documentation](#building-documentation)\n    - [Tips](#tips)\n    - [Building C++ Documentation](#building-c-documentation)\n  - [Previewing changes locally](#previewing-changes-locally)\n  - [Previewing documentation on PRs](#previewing-documentation-on-prs)\n  - [Adding documentation tests](#adding-documentation-tests)\n- [Profiling with `py-spy`](#profiling-with-py-spy)\n- [Managing multiple build trees](#managing-multiple-build-trees)\n- [C++ development tips](#c-development-tips)\n  - [Build only what you need](#build-only-what-you-need)\n  - [Code completion and IDE support](#code-completion-and-ide-support)\n  - [Make no-op build fast](#make-no-op-build-fast)\n    - [Use Ninja](#use-ninja)\n    - [Use CCache](#use-ccache)\n    - [Use a faster linker](#use-a-faster-linker)\n    - [Use pre-compiled headers](#use-pre-compiled-headers)\n    - [Workaround for header dependency bug in nvcc](#workaround-for-header-dependency-bug-in-nvcc)\n  - [Rebuild few files with debug information](#rebuild-few-files-with-debug-information)\n  - [C++ frontend development tips](#c-frontend-development-tips)\n  - [GDB integration](#gdb-integration)\n  - [C++ stacktraces](#c-stacktraces)\n- [CUDA development tips](#cuda-development-tips)\n- [Windows development tips](#windows-development-tips)\n  - [Known MSVC (and MSVC with NVCC) bugs](#known-msvc-and-msvc-with-nvcc-bugs)\n  - [Building on legacy code and CUDA](#building-on-legacy-code-and-cuda)\n- [Pre-commit tidy/linting hook](#pre-commit-tidylinting-hook)\n- [Building PyTorch with ASAN](#building-pytorch-with-asan)\n  - [Getting `ccache` to work](#getting-ccache-to-work)\n  - [Why this stuff with `LD_PRELOAD` and `LIBASAN_RT`?](#why-this-stuff-with-ld_preload-and-libasan_rt)\n  - [Why LD_PRELOAD in the build function?](#why-ld_preload-in-the-build-function)\n  - [Why no leak detection?](#why-no-leak-detection)\n- [Caffe2 notes](#caffe2-notes)\n- [CI failure tips](#ci-failure-tips)\n  - [Which commit is used in CI?](#which-commit-is-used-in-ci)\n- [Dev Infra Office Hours](#dev-infra-office-hours)\n\n<!-- tocstop -->\n\n## Developing PyTorch\n\nFollow the instructions for [installing PyTorch from source](https://github.com/pytorch/pytorch#from-source). If you get stuck when developing PyTorch on your machine, check out the [tips and debugging](#tips-and-debugging) section below for common solutions.\n\n### Setup the development environment\n\nFirst, you need to [fork the PyTorch project on GitHub](https://github.com/pytorch/pytorch/fork) and follow the instructions at [Connecting to GitHub with SSH](https://docs.github.com/en/authentication/connecting-to-github-with-ssh) to setup your SSH authentication credentials.\n\nThen clone the PyTorch project and setup the development environment:\n\n```bash\ngit clone git@github.com:<USERNAME>/pytorch.git\ncd pytorch\ngit remote add upstream git@github.com:pytorch/pytorch.git\n\nmake setup-env\n# Or run `make setup-env-cuda` for pre-built CUDA binaries\n# Or run `make setup-env-rocm` for pre-built ROCm binaries\nsource venv/bin/activate  # or `& .\\venv\\Scripts\\Activate.ps1` on Windows\n```\n\n### Tips and Debugging\n\n* If you want to have no-op incremental rebuilds (which are fast), see [Make no-op build fast](#make-no-op-build-fast) below.\n\n* When installing with `python setup.py develop` (in contrast to `python setup.py install`) Python runtime will use\n  the current local source-tree when importing `torch` package. (This is done by creating [`.egg-link`](https://wiki.python.org/moin/PythonPackagingTerminology#egg-link) file in `site-packages` folder)\n  This way you do not need to repeatedly install after modifying Python files (`.py`).\n  However, you would need to reinstall if you modify Python interface (`.pyi`, `.pyi.in`) or\n   non-Python files (`.cpp`, `.cc`, `.cu`, `.h`, ...).\n\n\n  One way to avoid running `python setup.py develop` every time one makes a change to C++/CUDA/ObjectiveC files on Linux/Mac,\n  is to create a symbolic link from `build` folder to `torch/lib`, for example, by issuing following:\n  ```bash\n   pushd torch/lib; sh -c \"ln -sf ../../build/lib/libtorch_cpu.* .\"; popd\n  ```\n   Afterwards rebuilding a library (for example to rebuild `libtorch_cpu.so` issue `ninja torch_cpu` from `build` folder),\n   would be sufficient to make change visible in `torch` package.\n\n\n  To reinstall, first uninstall all existing PyTorch installs. You may need to run `pip\n  uninstall torch` multiple times. You'll know `torch` is fully\n  uninstalled when you see `WARNING: Skipping torch as it is not\n  installed`. (You should only have to `pip uninstall` a few times, but\n  you can always `uninstall` with `timeout` or in a loop if you're feeling\n  lazy.)\n\n  ```bash\n  conda uninstall pytorch -y\n  yes | pip uninstall torch\n  ```\n\n  Next run `python setup.py clean`. After that, you can install in `develop` mode again.\n\n* If you run into errors when running `python setup.py develop`, here are some debugging steps:\n  1. Run `printf '#include <stdio.h>\\nint main() { printf(\"Hello World\");}'|clang -x c -; ./a.out` to make sure\n  your CMake works and can compile this simple Hello World program without errors.\n  2. Nuke your `build` directory. The `setup.py` script compiles binaries into the `build` folder and caches many\n  details along the way, which saves time the next time you build. If you're running into issues, you can always\n  `rm -rf build` from the toplevel `pytorch` directory and start over.\n  3. If you have made edits to the PyTorch repo, commit any change you'd like to keep and clean the repo with the\n  following commands (note that clean _really_ removes all untracked files and changes.):\n      ```bash\n      git submodule deinit -f .\n      git clean -xdf\n      python setup.py clean\n      git submodule update --init --recursive # very important to sync the submodules\n      python setup.py develop                 # then try running the command again\n      ```\n  4. The main step within `python setup.py develop` is running `make` from the `build` directory. If you want to\n    experiment with some environment variables, you can pass them into the command:\n      ```bash\n      ENV_KEY1=ENV_VAL1[, ENV_KEY2=ENV_VAL2]* python setup.py develop\n      ```\n\n* If you run into issue running `git submodule update --init --recursive`. Please try the following:\n  - If you encounter an error such as\n    ```\n    error: Submodule 'third_party/pybind11' could not be updated\n    ```\n    check whether your Git local or global config file contains any `submodule.*` settings. If yes, remove them and try again.\n    (please reference [this doc](https://git-scm.com/docs/git-config#Documentation/git-config.txt-submoduleltnamegturl) for more info).\n\n  - If you encounter an error such as\n    ```\n    fatal: unable to access 'https://github.com/pybind11/pybind11.git': could not load PEM client certificate ...\n    ```\n    this is likely that you are using HTTP proxying and the certificate expired. To check if the certificate is valid, run\n    `git config --global --list` and search for config like `http.proxysslcert=<cert_file>`. Then check certificate valid date by running\n    ```bash\n    openssl x509 -noout -in <cert_file> -dates\n    ```\n\n  - If you encounter an error that some third_party modules are not checked out correctly, such as\n    ```\n    Could not find .../pytorch/third_party/pybind11/CMakeLists.txt\n    ```\n    remove any `submodule.*` settings in your local git config (`.git/config` of your pytorch repo) and try again.\n* If you're a Windows contributor, please check out [Best Practices](https://github.com/pytorch/pytorch/wiki/Best-Practices-to-Edit-and-Compile-Pytorch-Source-Code-On-Windows).\n* For help with any part of the contributing process, please dont hesitate to utilize our Zoom office hours! See details [here](https://github.com/pytorch/pytorch/wiki/Dev-Infra-Office-Hours)\n\n## Nightly Checkout & Pull\n\nThe `tools/nightly.py` script is provided to ease pure Python development of\nPyTorch. This uses `venv` and `git` to check out the nightly development\nversion of PyTorch and installs pre-built binaries into the current repository.\nThis is like a development or editable install, but without needing the ability\nto compile any C++ code.\n\nYou can use this script to check out a new nightly branch with the following:\n\n```bash\n./tools/nightly.py checkout -b my-nightly-branch\nsource venv/bin/activate  # or `& .\\venv\\Scripts\\Activate.ps1` on Windows\n```\n\nOr if you would like to re-use an existing conda environment, you can pass in\nthe prefix argument (`--prefix`):\n\n```bash\n./tools/nightly.py checkout -b my-nightly-branch -p my-env\nsource my-env/bin/activate  # or `& .\\my-env\\Scripts\\Activate.ps1` on Windows\n```\n\nTo install the nightly binaries built with CUDA, you can pass in the flag `--cuda`:\n\n```bash\n./tools/nightly.py checkout -b my-nightly-branch --cuda\nsource venv/bin/activate  # or `& .\\venv\\Scripts\\Activate.ps1` on Windows\n```\n\nTo install the nightly binaries built with ROCm, you can pass in the flag `--rocm`:\n\n```bash\n./tools/nightly.py checkout -b my-nightly-branch --rocm\nsource venv/bin/activate  # or `& .\\venv\\Scripts\\Activate.ps1` on Windows\n```\n\nYou can also use this tool to pull the nightly commits into the current branch:\n\n```bash\n./tools/nightly.py pull -p my-env\nsource my-env/bin/activate  # or `& .\\venv\\Scripts\\Activate.ps1` on Windows\n```\n\nPulling will recreate a fresh virtual environment and reinstall the development\ndependencies as well as the nightly binaries into the repo directory.\n\n## Codebase structure\n\n* [c10](c10) - Core library files that work everywhere, both server\n  and mobile. We are slowly moving pieces from [ATen/core](aten/src/ATen/core)\n  here. This library is intended only to contain essential functionality,\n  and appropriate to use in settings where binary size matters. (But\n  you'll have a lot of missing functionality if you try to use it\n  directly.)\n* [aten](aten) - C++ tensor library for PyTorch (no autograd support)\n  * [src](aten/src) - [README](aten/src/README.md)\n    * [ATen](aten/src/ATen)\n      * [core](aten/src/ATen/core) - Core functionality of ATen. This\n        is migrating to top-level c10 folder.\n      * [native](aten/src/ATen/native) - Modern implementations of\n        operators. If you want to write a new operator, here is where\n        it should go. Most CPU operators go in the top level directory,\n        except for operators which need to be compiled specially; see\n        cpu below.\n        * [cpu](aten/src/ATen/native/cpu) - Not actually CPU\n          implementations of operators, but specifically implementations\n          which are compiled with processor-specific instructions, like\n          AVX. See the [README](aten/src/ATen/native/cpu/README.md) for more\n          details.\n        * [cuda](aten/src/ATen/native/cuda) - CUDA implementations of\n          operators.\n        * [sparse](aten/src/ATen/native/sparse) - CPU and CUDA\n          implementations of COO sparse tensor operations\n        * [mkl](aten/src/ATen/native/mkl) [mkldnn](aten/src/ATen/native/mkldnn)\n          [miopen](aten/src/ATen/native/miopen) [cudnn](aten/src/ATen/native/cudnn)\n          - implementations of operators which simply bind to some\n            backend library.\n        * [quantized](aten/src/ATen/native/quantized/) - Quantized tensor (i.e. QTensor) operation implementations. [README](aten/src/ATen/native/quantized/README.md) contains details including how to implement native quantized operations.\n* [torch](torch) - The actual PyTorch library. Everything that is not\n  in [csrc](torch/csrc) is a Python module, following the PyTorch Python\n  frontend module structure.\n  * [csrc](torch/csrc) - C++ files composing the PyTorch library. Files\n    in this directory tree are a mix of Python binding code, and C++\n    heavy lifting. Consult `setup.py` for the canonical list of Python\n    binding files; conventionally, they are often prefixed with\n    `python_`. [README](torch/csrc/README.md)\n    * [jit](torch/csrc/jit) - Compiler and frontend for TorchScript JIT\n      frontend. [README](torch/csrc/jit/README.md)\n    * [autograd](torch/csrc/autograd) - Implementation of reverse-mode automatic differentiation. [README](torch/csrc/autograd/README.md)\n    * [api](torch/csrc/api) - The PyTorch C++ frontend.\n    * [distributed](torch/csrc/distributed) - Distributed training\n      support for PyTorch.\n* [tools](tools) - Code generation scripts for the PyTorch library.\n  See [README](tools/README.md) of this directory for more details.\n* [test](test) - Python unit tests for PyTorch Python frontend.\n  * [test_torch.py](test/test_torch.py) - Basic tests for PyTorch\n    functionality.\n  * [test_autograd.py](test/test_autograd.py) - Tests for non-NN\n    automatic differentiation support.\n  * [test_nn.py](test/test_nn.py) - Tests for NN operators and\n    their automatic differentiation.\n  * [test_jit.py](test/test_jit.py) - Tests for the JIT compiler\n    and TorchScript.\n  * ...\n  * [cpp](test/cpp) - C++ unit tests for PyTorch C++ frontend.\n    * [api](test/cpp/api) - [README](test/cpp/api/README.md)\n    * [jit](test/cpp/jit) - [README](test/cpp/jit/README.md)\n    * [tensorexpr](test/cpp/tensorexpr) - [README](test/cpp/tensorexpr/README.md)\n  * [expect](test/expect) - Automatically generated \"expect\" files\n    which are used to compare against expected output.\n  * [onnx](test/onnx) - Tests for ONNX export functionality,\n    using both PyTorch and Caffe2.\n* [caffe2](caffe2) - The Caffe2 library.\n  * [core](caffe2/core) - Core files of Caffe2, e.g., tensor, workspace,\n    blobs, etc.\n  * [operators](caffe2/operators) - Operators of Caffe2.\n  * [python](caffe2/python) - Python bindings to Caffe2.\n  * ...\n* [.circleci](.circleci) - CircleCI configuration management. [README](.circleci/README.md)\n\n## Unit testing\n\n### Python Unit Testing\n\n**Prerequisites**:\nThe following packages should be installed with either `conda` or `pip`:\n- `expecttest` and `hypothesis` - required to run tests\n- `mypy` - recommended for linting\n- `pytest` - recommended to run tests more selectively\nRunning\n```\npip install -r requirements.txt\n```\nwill install these dependencies for you.\n\nAll PyTorch test suites are located in the `test` folder and start with\n`test_`. Run the entire test\nsuite with\n\n```bash\npython test/run_test.py\n```\n\nor run individual test suites using the command `python test/FILENAME.py`,\nwhere `FILENAME` represents the file containing the test suite you wish\nto run.\n\nFor example, to run all the TorchScript JIT tests (located at\n`test/test_jit.py`), you would run:\n\n```bash\npython test/test_jit.py\n```\n\nYou can narrow down what you're testing even further by specifying the\nname of an individual test with `TESTCLASSNAME.TESTNAME`. Here,\n`TESTNAME` is the name of the test you want to run, and `TESTCLASSNAME`\nis the name of the class in which it is defined.\n\nGoing off the above example, let's say you want to run\n`test_Sequential`, which is defined as part of the `TestJit` class\nin `test/test_jit.py`. Your command would be:\n\n```bash\npython test/test_jit.py TestJit.test_Sequential\n```\n\n**Weird note:** In our CI (Continuous Integration) jobs, we actually run the tests from the `test` folder and **not** the root of the repo, since there are various dependencies we set up for CI that expects the tests to be run from the test folder. As such, there may be some inconsistencies between local testing and CI testing--if you observe an inconsistency, please [file an issue](https://github.com/pytorch/pytorch/issues/new/choose).\n\n### Better local unit tests with `pytest`\n\nWe don't officially support `pytest`, but it works well with our\n`unittest` tests and offers a number of useful features for local\ndeveloping. Install it via `pip install pytest`.\n\nIf you want to just run tests that contain a specific substring, you can\nuse the `-k` flag:\n\n```bash\npytest test/test_nn.py -k Loss -v\n```\n\nThe above is an example of testing a change to all Loss functions: this\ncommand runs tests such as `TestNN.test_BCELoss` and\n`TestNN.test_MSELoss` and can be useful to save keystrokes.\n\n### Local linting\n\nInstall all prerequisites by running\n\n```bash\nmake setup-lint\n```\n\nYou can now run the same linting steps that are used in CI locally via `make`:\n\n```bash\nmake lint\n```\n\nLearn more about the linter on the [lintrunner wiki page](https://github.com/pytorch/pytorch/wiki/lintrunner)\n\n#### Running `mypy`\n\n`mypy` is an optional static type checker for Python. We have multiple `mypy`\nconfigs for the PyTorch codebase that are automatically validated against whenever the linter is run.\n\nSee [Guide for adding type annotations to\nPyTorch](https://github.com/pytorch/pytorch/wiki/Guide-for-adding-type-annotations-to-PyTorch)\nfor more information on how to set up `mypy` and tackle type annotation\ntasks.\n\n### C++ Unit Testing\n\nPyTorch offers a series of tests located in the `test/cpp` folder.\nThese tests are written in C++ and use the Google Test testing framework.\nAfter compiling PyTorch from source, the test runner binaries will be\nwritten to the `build/bin` folder. The command to run one of these tests\nis `./build/bin/FILENAME --gtest_filter=TESTSUITE.TESTNAME`, where\n`TESTNAME` is the name of the test you'd like to run and `TESTSUITE` is\nthe suite that test is defined in.\n\nFor example, if you wanted to run the test `MayContainAlias`, which\nis part of the test suite `ContainerAliasingTest` in the file\n`test/cpp/jit/test_alias_analysis.cpp`, the command would be:\n\n```bash\n./build/bin/test_jit --gtest_filter=ContainerAliasingTest.MayContainAlias\n```\n\n\n### Run Specific CI Jobs\n\nYou can generate a commit that limits the CI to only run a specific job by using\n`tools/testing/explicit_ci_jobs.py` like so:\n\n```bash\n# --job: specify one or more times to filter to a specific job + its dependencies\n# --filter-gha: specify github actions workflows to keep\n# --make-commit: commit CI changes to git with a message explaining the change\npython tools/testing/explicit_ci_jobs.py --job binary_linux_manywheel_3_6m_cpu_devtoolset7_nightly_test --filter-gha '*generated*gcc5.4*' --make-commit\n\n# Make your changes\n\nghstack submit\n```\n\n**NB**: It is not recommended to use this workflow unless you are also using\n[`ghstack`](https://github.com/ezyang/ghstack). It creates a large commit that is\nof very low signal to reviewers.\n\n## Merging your Change\nIf you know the right people or team that should approve your PR (and you have the required permissions to do so), add them to the Reviewers list.\n\nIf not, leave the Reviewers section empty. Our triage squad will review your PR, add a module label, and assign it to the appropriate reviewer in a couple business days.  The reviewer will then look at your PR and respond.\n\nOccasionally, things might fall through the cracks (sorry!). In case your PR either doesn't get assigned to a reviewer or doesn't get any response from the reviewer for 4 business days, please leave comment on the PR (mentioning the reviewer if one has been assigned). That'll get it nudged back onto people's radar.\n\nIf that still doesn't help, come see us during [our office hours](https://github.com/pytorch/pytorch/wiki/Dev-Infra-Office-Hours)\n\nOnce your PR is approved, you can merge it in by entering a comment with the content `@pytorchmergebot merge` ([what's this bot?](https://github.com/pytorch/pytorch/wiki/Bot-commands))\n\n## Writing documentation\n\nSo you want to write some documentation and don't know where to start?\nPyTorch has two main types of documentation:\n- **User facing documentation**:\nThese are the docs that you see over at [our docs website](https://pytorch.org/docs).\n- **Developer facing documentation**:\nDeveloper facing documentation is spread around our READMEs in our codebase and in\nthe [PyTorch Developer Wiki](https://pytorch.org/wiki).\nIf you're interested in adding new developer docs, please read this [page on the wiki](https://github.com/pytorch/pytorch/wiki/Where-or-how-should-I-add-documentation) on our best practices for where to put it.\n\nThe rest of this section is about user-facing documentation.\n\nPyTorch uses [Google style](https://www.sphinx-doc.org/en/master/usage/extensions/example_google.html)\nfor formatting docstrings. Each line inside a docstrings block must be limited to 80 characters so that it fits into Jupyter documentation popups.\n\n\n### Docstring type formatting\n\nIn addition to the standard Google Style docstring formatting rules, the following guidelines should be followed for docstring types (docstring types are the type information contained in the round brackets after the variable name):\n\n* The \"`Callable`\", \"`Any`\", \"`Iterable`\", \"`Iterator`\", \"`Generator`\" types should have their first letter capitalized.\n\n* The \"`list`\" and \"`tuple`\" types should be completely lowercase.\n\n* Types should not be made plural. For example: `tuple of int` should be used instead of `tuple of ints`.\n\n* The only acceptable delimiter words for types are `or` and `of`. No other non-type words should be used other than `optional`.\n\n* The word `optional` should only be used after the types, and it is only used if the user does not have to specify a value for the variable. Default values are listed after the variable description. Example:\n\n    ```\n    my_var (int, optional): Variable description. Default: 1\n    ```\n\n* Basic Python types should match their type name so that the [Intersphinx](https://www.sphinx-doc.org/en/master/usage/extensions/intersphinx.html) extension can correctly identify them. For example:\n    * Use `str` instead of `string`.\n    * Use `bool` instead of `boolean`.\n    * Use `dict` instead of `dictionary`.\n\n* Square brackets should be used for the dictionary type. For example:\n\n    ```\n    my_var (dict[str, int]): Variable description.\n    ```\n\n* If a variable has two different possible types, then the word `or` should be used without a comma. Otherwise variables with 3 or more types should use commas to separate the types. Example:\n\n    ```\n    x (type1 or type2): Variable description.\n    y (type1, type2, or type3): Variable description.\n    ```\n\n\n### Building documentation\n\nTo build the documentation:\n\n1. Build and install PyTorch\n\n2. Install the prerequisites\n\n```bash\ncd docs\npip install -r requirements.txt\n# `katex` must also be available in your PATH.\n# You can either install katex globally if you have properly configured npm:\n# npm install -g katex\n# Or if you prefer an uncontaminated global executable environment or do not want to go through the node configuration:\n# npm install katex && export PATH=\"$PATH:$(pwd)/node_modules/.bin\"\n```\n> Note: if you installed `nodejs` with a different package manager (e.g.,\n`conda`) then `npm` will probably install a version of `katex` that is not\ncompatible with your version of `nodejs` and doc builds will fail.\nA combination of versions that is known to work is `node@6.13.1` and\n`katex@0.13.18`. To install the latter with `npm` you can run\n```npm install -g katex@0.13.18```\n\n\n> Note that if you are a Facebook employee using a devserver, yarn may be more convenient to install katex:\n\n```bash\nyarn global add katex\n```\n> If a specific version is required you can use for example `yarn global add katex@0.13.18`.\n\n3. Generate the documentation HTML files. The generated files will be in `docs/build/html`.\n\n```bash\nmake html\n```\n\n#### Tips\n\nThe `.rst` source files live in [docs/source](docs/source). Some of the `.rst`\nfiles pull in docstrings from PyTorch Python code (for example, via\nthe `autofunction` or `autoclass` directives). To vastly shorten doc build times,\nit is helpful to remove the files you are not working on, only keeping the base\n`index.rst` file and the files you are editing. The Sphinx build will produce\nmissing file warnings but will still complete. For example, to work on `jit.rst`:\n\n```bash\ncd docs/source\nfind . -type f | grep rst | grep -v index | grep -v jit | xargs rm\n\n# Make your changes, build the docs, etc.\n\n# Don't commit the deletions!\ngit add index.rst jit.rst\n...\n```\n\n#### Building C++ Documentation\n\nFor C++ documentation (https://pytorch.org/cppdocs), we use\n[Doxygen](http://www.doxygen.nl/) and then convert it to\n[Sphinx](http://www.sphinx-doc.org/) via\n[Breathe](https://github.com/michaeljones/breathe) and\n[Exhale](https://github.com/svenevs/exhale). Check the [Doxygen\nreference](https://www.doxygen.nl/manual/) for more\ninformation on the documentation syntax.\n\nWe run Doxygen in CI (Travis) to verify that you do not use invalid Doxygen\ncommands. To run this check locally, run `./check-doxygen.sh` from inside\n`docs/cpp/source`.\n\nTo build the documentation, follow the same steps as above, but run them from\n`docs/cpp` instead of `docs`.\n\n### Previewing changes locally\n\nTo view HTML files locally, you can open the files in your web browser. For example,\nnavigate to `file:///your_pytorch_folder/docs/build/html/index.html` in a web\nbrowser.\n\nIf you are developing on a remote machine, you can set up an SSH tunnel so that\nyou can access the HTTP server on the remote machine from your local machine. To map\nremote port 8000 to local port 8000, use either of the following commands.\n\n```bash\n# For SSH\nssh my_machine -L 8000:my_machine:8000\n\n# For Eternal Terminal\net my_machine -t=\"8000:8000\"\n```\n\nThen navigate to `localhost:8000` in your web browser.\n\n**Tip:**\nYou can start a lightweight HTTP server on the remote machine with:\n\n```bash\npython -m http.server 8000 <path_to_html_output>\n```\n\nAlternatively, you can run `rsync` on your local machine to copy the files from\nyour remote machine:\n\n```bash\nmkdir -p build cpp/build\nrsync -az me@my_machine:/path/to/pytorch/docs/build/html build\nrsync -az me@my_machine:/path/to/pytorch/docs/cpp/build/html cpp/build\n```\n\n### Previewing documentation on PRs\n\nPyTorch will host documentation previews at `https://docs-preview.pytorch.org/pytorch/pytorch/<pr number>/index.html` once the\n`pytorch_python_doc_build` GitHub Actions job has completed on your PR. You can visit that page directly\nor find its link in the automated Dr. CI comment on your PR.\n\n### Adding documentation tests\n\nIt is easy for code snippets in docstrings and `.rst` files to get out of date. The docs\nbuild includes the [Sphinx Doctest Extension](https://www.sphinx-doc.org/en/master/usage/extensions/doctest.html),\nwhich can run code in documentation as a unit test. To use the extension, use\nthe `.. testcode::` directive in your `.rst` and docstrings.\n\nTo manually run these tests, follow steps 1 and 2 above, then run:\n\n```bash\ncd docs\nmake doctest\n```\n\n## Profiling with `py-spy`\n\nEvaluating the performance impact of code changes in PyTorch can be complicated,\nparticularly if code changes happen in compiled code. One simple way to profile\nboth Python and C++ code in PyTorch is to use\n[`py-spy`](https://github.com/benfred/py-spy), a sampling profiler for Python\nthat has the ability to profile native code and Python code in the same session.\n\n`py-spy` can be installed via `pip`:\n\n```bash\npip install py-spy\n```\n\nTo use `py-spy`, first write a Python test script that exercises the\nfunctionality you would like to profile. For example, this script profiles\n`torch.add`:\n\n```python\nimport torch\n\nt1 = torch.tensor([[1, 1], [1, 1.]])\nt2 = torch.tensor([[0, 0], [0, 0.]])\n\nfor _ in range(1000000):\n    torch.add(t1, t2)\n```\n\nSince the `torch.add` operation happens in microseconds, we repeat it a large\nnumber of times to get good statistics. The most straightforward way to use\n`py-spy` with such a script is to generate a [flame\ngraph](http://www.brendangregg.com/flamegraphs.html):\n\n```bash\npy-spy record -o profile.svg --native -- python test_tensor_tensor_add.py\n```\n\nThis will output a file named `profile.svg` containing a flame graph you can\nview in a web browser or SVG viewer. Individual stack frame entries in the graph\ncan be selected interactively with your mouse to zoom in on a particular part of\nthe program execution timeline. The `--native` command-line option tells\n`py-spy` to record stack frame entries for PyTorch C++ code. To get line numbers\nfor C++ code it may be necessary to compile PyTorch in debug mode by prepending\nyour `setup.py develop` call to compile PyTorch with `DEBUG=1`. Depending on\nyour operating system it may also be necessary to run `py-spy` with root\nprivileges.\n\n`py-spy` can also work in an `htop`-like \"live profiling\" mode and can be\ntweaked to adjust the stack sampling rate, see the `py-spy` readme for more\ndetails.\n\n## Managing multiple build trees\n\nOne downside to using `python setup.py develop` is that your development\nversion of PyTorch will be installed globally on your account (e.g., if\nyou run `import torch` anywhere else, the development version will be\nused.\n\nIf you want to manage multiple builds of PyTorch, you can make use of\n[conda environments](https://conda.io/docs/using/envs.html) to maintain\nseparate Python package environments, each of which can be tied to a\nspecific build of PyTorch. To set one up:\n\n```bash\nconda create -n pytorch-myfeature\nsource activate pytorch-myfeature\n# if you run python now, torch will NOT be installed\npython setup.py develop\n```\n\n## C++ development tips\n\nIf you are working on the C++ code, there are a few important things that you\nwill want to keep in mind:\n\n1. How to rebuild only the code you are working on.\n2. How to make rebuilds in the absence of changes go faster.\n\n### Build only what you need\n\n`python setup.py build` will build everything by default, but sometimes you are\nonly interested in a specific component.\n\n- Working on a test binary? Run `(cd build && ninja bin/test_binary_name)` to\n  rebuild only that test binary (without rerunning cmake). (Replace `ninja` with\n  `make` if you don't have ninja installed).\n\nOn the initial build, you can also speed things up with the environment\nvariables `DEBUG`, `USE_DISTRIBUTED`, `USE_MKLDNN`, `USE_CUDA`, `USE_FLASH_ATTENTION`, `USE_MEM_EFF_ATTENTION`, `BUILD_TEST`, `USE_FBGEMM`, `USE_NNPACK` and `USE_QNNPACK`.\n\n- `DEBUG=1` will enable debug builds (-g -O0)\n- `REL_WITH_DEB_INFO=1` will enable debug symbols with optimizations (-g -O3)\n- `USE_DISTRIBUTED=0` will disable distributed (c10d, gloo, mpi, etc.) build.\n- `USE_MKLDNN=0` will disable using MKL-DNN.\n- `USE_CUDA=0` will disable compiling CUDA (in case you are developing on something not CUDA related), to save compile time.\n- `BUILD_TEST=0` will disable building C++ test binaries.\n- `USE_FBGEMM=0` will disable using FBGEMM (quantized 8-bit server operators).\n- `USE_NNPACK=0` will disable compiling with NNPACK.\n- `USE_QNNPACK=0` will disable QNNPACK build (quantized 8-bit operators).\n- `USE_XNNPACK=0` will disable compiling with XNNPACK.\n- `USE_FLASH_ATTENTION=0` and `USE_MEM_EFF_ATTENTION=0` will disable compiling flash attention and memory efficient kernels respectively\n\nFor example:\n\n```bash\nDEBUG=1 USE_DISTRIBUTED=0 USE_MKLDNN=0 USE_CUDA=0 BUILD_TEST=0 USE_FBGEMM=0 USE_NNPACK=0 USE_QNNPACK=0 USE_XNNPACK=0 python setup.py develop\n```\n\nFor subsequent builds (i.e., when `build/CMakeCache.txt` exists), the build\noptions passed for the first time will persist; please run `ccmake build/`, run\n`cmake-gui build/`, or directly edit `build/CMakeCache.txt` to adapt build\noptions.\n\n### Code completion and IDE support\n\nWhen using `python setup.py develop`, PyTorch will generate\na `compile_commands.json` file that can be used by many editors\nto provide command completion and error highlighting for PyTorch's\nC++ code. You need to `pip install ninja` to generate accurate\ninformation for the code in `torch/csrc`. More information at:\n- https://sarcasm.github.io/notes/dev/compilation-database.html\n\n### Make no-op build fast\n\n#### Use Ninja\n\nBy default, cmake will use its Makefile generator to generate your build\nsystem.  You can get faster builds if you install the ninja build system\nwith `pip install ninja`.  If PyTorch was already built, you will need\nto run `python setup.py clean` once after installing ninja for builds to\nsucceed.\n\nNote: Make sure to use a machine with a larger number of CPU cores, this will significantly reduce your build times.\n\n#### Use CCache\n\nEven when dependencies are tracked with file modification, there are many\nsituations where files get rebuilt when a previous compilation was exactly the\nsame. Using ccache in a situation like this is a real time-saver.\n\nBefore building pytorch, install ccache from your package manager of choice:\n\n```bash\nconda install ccache -c conda-forge\nsudo apt install ccache\nsudo yum install ccache\nbrew install ccache\n```\n\nYou may also find the default cache size in ccache is too small to be useful.\nThe cache sizes can be increased from the command line:\n\n```bash\n# config: cache dir is ~/.ccache, conf file ~/.ccache/ccache.conf\n# max size of cache\nccache -M 25Gi  # -M 0 for unlimited\n# unlimited number of files\nccache -F 0\n```\n\nTo check this is working, do two clean builds of pytorch in a row. The second\nbuild should be substantially and noticeably faster than the first build. If\nthis doesn't seem to be the case, check the `CMAKE_<LANG>_COMPILER_LAUNCHER`\nrules in `build/CMakeCache.txt`, where `<LANG>` is `C`, `CXX` and `CUDA`.\nEach of these 3 variables should contain ccache, e.g.\n\n```\n//CXX compiler launcher\nCMAKE_CXX_COMPILER_LAUNCHER:STRING=/usr/bin/ccache\n```\n\nIf not, you can define these variables on the command line before invoking `setup.py`.\n\n```bash\nexport CMAKE_C_COMPILER_LAUNCHER=ccache\nexport CMAKE_CXX_COMPILER_LAUNCHER=ccache\nexport CMAKE_CUDA_COMPILER_LAUNCHER=ccache\npython setup.py develop\n```\n\n#### Use a faster linker\n\nIf you are editing a single file and rebuilding in a tight loop, the time spent\nlinking will dominate. The system linker available in most Linux distributions\n(GNU `ld`) is quite slow. Use a faster linker, like [lld](https://lld.llvm.org/).\n\nPeople on Mac, follow [this guide](https://stackoverflow.com/questions/42730345/how-to-install-llvm-for-mac) instead.\n\nThe easiest way to use `lld` this is download the\n[latest LLVM binaries](http://releases.llvm.org/download.html#8.0.0) and run:\n\n```bash\nln -s /path/to/downloaded/ld.lld /usr/local/bin/ld\n```\n\n#### Use pre-compiled headers\n\nSometimes there's no way of getting around rebuilding lots of files, for example\nediting `native_functions.yaml` usually means 1000+ files being rebuilt. If\nyou're using CMake newer than 3.16, you can enable pre-compiled headers by\nsetting `USE_PRECOMPILED_HEADERS=1` either on first setup, or in the\n`CMakeCache.txt` file.\n\n```sh\nUSE_PRECOMPILED_HEADERS=1 python setup.py develop\n```\n\nThis adds a build step where the compiler takes `<ATen/ATen.h>` and essentially\ndumps its internal AST to a file so the compiler can avoid repeating itself for\nevery `.cpp` file.\n\nOne caveat is that when enabled, this header gets included in every file by default.\nWhich may change what code is legal, for example:\n- internal functions can never alias existing names in `<ATen/ATen.h>`\n- names in `<ATen/ATen.h>` will work even if you don't explicitly include it.\n\n#### Workaround for header dependency bug in nvcc\nIf re-building without modifying any files results in several CUDA files being\nre-compiled, you may be running into an `nvcc` bug where header dependencies are\nnot converted to absolute paths before reporting it to the build system. This\nmakes `ninja` think one of the header files has been deleted, so it runs the\nbuild again.\n\nA compiler-wrapper to fix this is provided in `tools/nvcc_fix_deps.py`. You can use\nthis as a compiler launcher, similar to `ccache`\n```bash\nexport CMAKE_CUDA_COMPILER_LAUNCHER=\"python;`pwd`/tools/nvcc_fix_deps.py;ccache\"\npython setup.py develop\n```\n\n### Rebuild few files with debug information\n\nWhile debugging a problem one often had to maintain a debug build in a separate folder.\nBut often only a few files needs to be rebuild with debug info to get a symbolicated backtrace or enable source debugging\nOne can easily solve this with the help of `tools/build_with_debinfo.py`\n\nFor example, suppose one wants to debug what is going on while tensor index is selected, which can be achieved by setting a breakpoint at `applySelect` function:\n```\n% lldb -o \"b applySelect\" -o \"process launch\" -- python3 -c \"import torch;print(torch.rand(5)[3])\"\n(lldb) target create \"python\"\nCurrent executable set to '/usr/bin/python3' (arm64).\n(lldb) settings set -- target.run-args  \"-c\" \"import torch;print(torch.rand(5)[3])\"\n(lldb) b applySelect\nBreakpoint 1: no locations (pending).\nWARNING:  Unable to resolve breakpoint to any actual locations.\n(lldb) process launch\n2 locations added to breakpoint 1\nProcess 87729 stopped\n* thread #1, queue = 'com.apple.main-thread', stop reason = breakpoint 1.1\n    frame #0: 0x00000001023d55a8 libtorch_python.dylib`at::indexing::impl::applySelect(at::Tensor const&, long long, c10::SymInt, long long, c10::Device const&, std::__1::optional<c10::ArrayRef<c10::SymInt>> const&)\nlibtorch_python.dylib`at::indexing::impl::applySelect:\n->  0x1023d55a8 <+0>:  sub    sp, sp, #0xd0\n    0x1023d55ac <+4>:  stp    x24, x23, [sp, #0x90]\n    0x1023d55b0 <+8>:  stp    x22, x21, [sp, #0xa0]\n    0x1023d55b4 <+12>: stp    x20, x19, [sp, #0xb0]\nTarget 0: (python) stopped.\nProcess 87729 launched: '/usr/bin/python' (arm64)\n```\nWhich is not very informative, but can be easily remedied by rebuilding `python_variable_indexing.cpp` with debug information\n```\n% ./tools/build_with_debinfo.py torch/csrc/autograd/python_variable_indexing.cpp\n[1 / 2] Building caffe2/torch/CMakeFiles/torch_python.dir/csrc/autograd/python_variable_indexing.cpp.o\n[2 / 2] Building lib/libtorch_python.dylib\n```\nAnd afterwards:\n```\n% lldb -o \"b applySelect\" -o \"process launch\" -- python3 -c \"import torch;print(torch.rand(5)[3])\"\n(lldb) target create \"python\"\nCurrent executable set to '/usr/bin/python3' (arm64).\n(lldb) settings set -- target.run-args  \"-c\" \"import torch;print(torch.rand(5)[3])\"\n(lldb) b applySelect\nBreakpoint 1: no locations (pending).\nWARNING:  Unable to resolve breakpoint to any actual locations.\n(lldb) process launch\n2 locations added to breakpoint 1\nProcess 87741 stopped\n* thread #1, queue = 'com.apple.main-thread', stop reason = breakpoint 1.1\n    frame #0: 0x00000001024e2628 libtorch_python.dylib`at::indexing::impl::applySelect(self=0x00000001004ee8a8, dim=0, index=(data_ = 3), real_dim=0, (null)=0x000000016fdfe535, self_sizes= Has Value=true ) at TensorIndexing.h:239:7\n   236         const at::Device& /*self_device*/,\n   237         const std::optional<SymIntArrayRef>& self_sizes) {\n   238       // See NOTE [nested tensor size for indexing]\n-> 239       if (self_sizes.has_value()) {\n   240         auto maybe_index = index.maybe_as_int();\n   241         if (maybe_index.has_value()) {\n   242           TORCH_CHECK_INDEX(\nTarget 0: (python) stopped.\nProcess 87741 launched: '/usr/bin/python3' (arm64)\n```\nWhich is much more useful, isn't it?\n\n### C++ frontend development tips\n\nWe have very extensive tests in the [test/cpp/api](test/cpp/api) folder. The\ntests are a great way to see how certain components are intended to be used.\nWhen compiling PyTorch from source, the test runner binary will be written to\n`build/bin/test_api`. The tests use the [GoogleTest](https://github.com/google/googletest/blob/master/googletest)\nframework, which you can read up about to learn how to configure the test runner. When\nsubmitting a new feature, we care very much that you write appropriate tests.\nPlease follow the lead of the other tests to see how to write a new test case.\n\n### GDB integration\n\nIf you are debugging pytorch inside GDB, you might be interested in\n[pytorch-gdb](tools/gdb/pytorch-gdb.py). This script introduces some\npytorch-specific commands which you can use from the GDB prompt. In\nparticular, `torch-tensor-repr` prints a human-readable repr of an at::Tensor\nobject. Example of usage:\n\n```\n$ gdb python\nGNU gdb (GDB) 9.2\n[...]\n(gdb) # insert a breakpoint when we call .neg()\n(gdb) break at::Tensor::neg\nFunction \"at::Tensor::neg\" not defined.\nMake breakpoint pending on future shared library load? (y or [n]) y\nBreakpoint 1 (at::Tensor::neg) pending.\n\n(gdb) run\n[...]\n>>> import torch\n>>> t = torch.tensor([1, 2, 3, 4], dtype=torch.float64)\n>>> t\ntensor([1., 2., 3., 4.], dtype=torch.float64)\n>>> t.neg()\n\nThread 1 \"python\" hit Breakpoint 1, at::Tensor::neg (this=0x7ffb118a9c88) at aten/src/ATen/core/TensorBody.h:3295\n3295    inline at::Tensor Tensor::neg() const {\n(gdb) # the default repr of 'this' is not very useful\n(gdb) p this\n$1 = (const at::Tensor * const) 0x7ffb118a9c88\n(gdb) p *this\n$2 = {impl_ = {target_ = 0x55629b5cd330}}\n(gdb) torch-tensor-repr *this\nPython-level repr of *this:\ntensor([1., 2., 3., 4.], dtype=torch.float64)\n```\n\nGDB tries to automatically load `pytorch-gdb` thanks to the\n[.gdbinit](.gdbinit) at the root of the pytorch repo. However, auto-loadings is disabled by default, because of security reasons:\n\n```bash\n$ gdb\nwarning: File \"/path/to/pytorch/.gdbinit\" auto-loading has been declined by your `auto-load safe-path' set to \"$debugdir:$datadir/auto-load\".\nTo enable execution of this file add\n        add-auto-load-safe-path /path/to/pytorch/.gdbinit\nline to your configuration file \"/home/YOUR-USERNAME/.gdbinit\".\nTo completely disable this security protection add\n        set auto-load safe-path /\nline to your configuration file \"/home/YOUR-USERNAME/.gdbinit\".\nFor more information about this security protection see the\n\"Auto-loading safe path\" section in the GDB manual.  E.g., run from the shell:\n        info \"(gdb)Auto-loading safe path\"\n(gdb)\n```\n\nAs gdb itself suggests, the best way to enable auto-loading of `pytorch-gdb`\nis to add the following line to your `~/.gdbinit` (i.e., the `.gdbinit` file\nwhich is in your home directory, **not** `/path/to/pytorch/.gdbinit`):\n\n```bash\nadd-auto-load-safe-path /path/to/pytorch/.gdbinit\n```\n\n### C++ stacktraces\nSet `TORCH_SHOW_CPP_STACKTRACES=1` to get the C++ stacktrace when an error occurs in Python.\n\n## CUDA development tips\n\nIf you are working on the CUDA code, here are some useful CUDA debugging tips:\n\n1. `CUDA_DEVICE_DEBUG=1` will enable CUDA device function debug symbols (`-g -G`).\n    This will be particularly helpful in debugging device code. However, it will\n    slow down the build process for about 50% (compared to only `DEBUG=1`), so use wisely.\n2. `cuda-gdb` and `cuda-memcheck` are your best CUDA debugging friends. Unlike`gdb`,\n   `cuda-gdb` can display actual values in a CUDA tensor (rather than all zeros).\n3. CUDA supports a lot of C++11/14 features such as, `std::numeric_limits`, `std::nextafter`,\n   `std::tuple` etc. in device code. Many of such features are possible because of the\n   [--expt-relaxed-constexpr](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#constexpr-functions)\n   nvcc flag. There is a known [issue](https://github.com/ROCm-Developer-Tools/HIP/issues/374)\n   that ROCm errors out on device code, which uses such stl functions.\n4. A good performance metric for a CUDA kernel is the\n   [Effective Memory Bandwidth](https://devblogs.nvidia.com/how-implement-performance-metrics-cuda-cc/).\n   It is useful for you to measure this metric whenever you are writing/optimizing a CUDA\n   kernel. Following script shows how we can measure the effective bandwidth of CUDA `uniform_`\n   kernel.\n   ```python\n   import torch\n   from torch.utils.benchmark import Timer\n   size = 128*512\n   nrep = 100\n   nbytes_read_write = 4 # this is number of bytes read + written by a kernel. Change this to fit your kernel.\n\n   for i in range(10):\n       a=torch.empty(size).cuda().uniform_()\n       torch.cuda.synchronize()\n       out = a.uniform_()\n       torch.cuda.synchronize()\n       t = Timer(stmt=\"a.uniform_()\", globals=globals())\n       res = t.blocked_autorange()\n       timec = res.median\n       print(\"uniform, size, elements\", size, \"forward\", timec, \"bandwidth (GB/s)\", size*(nbytes_read_write)*1e-9/timec)\n       size *=2\n   ```\n\n  See more cuda development tips [here](https://github.com/pytorch/pytorch/wiki/CUDA-basics)\n\n## Windows development tips\n\nFor building from source on Windows, consult\n[our documentation](https://pytorch.org/docs/stable/notes/windows.html) on it.\n\nOccasionally, you will write a patch which works on Linux, but fails CI on Windows.\nThere are a few aspects in which MSVC (the Windows compiler toolchain we use) is stricter\nthan Linux, which are worth keeping in mind when fixing these problems.\n\n1. Symbols are NOT exported by default on Windows; instead, you have to explicitly\n   mark a symbol as exported/imported in a header file with `__declspec(dllexport)` /\n   `__declspec(dllimport)`. We have codified this pattern into a set of macros\n   which follow the convention `*_API`, e.g., `TORCH_API` inside Caffe2, Aten and Torch.\n   (Every separate shared library needs a unique macro name, because symbol visibility\n   is on a per shared library basis. See c10/macros/Macros.h for more details.)\n\n   The upshot is if you see an \"unresolved external\" error in your Windows build, this\n   is probably because you forgot to mark a function with `*_API`. However, there is\n   one important counterexample to this principle: if you want a *templated* function\n   to be instantiated at the call site, do NOT mark it with `*_API` (if you do mark it,\n   you'll have to explicitly instantiate all of the specializations used by the call\n   sites.)\n\n2. If you link against a library, this does not make its dependencies transitively\n   visible. You must explicitly specify a link dependency against every library whose\n   symbols you use. (This is different from Linux where in most environments,\n   transitive dependencies can be used to fulfill unresolved symbols.)\n\n3. If you have a Windows box (we have a few on EC2 which you can request access to) and\n   you want to run the build, the easiest way is to just run `.ci/pytorch/win-build.sh`.\n   If you need to rebuild, run `REBUILD=1 .ci/pytorch/win-build.sh` (this will avoid\n   blowing away your Conda environment.)\n\nEven if you don't know anything about MSVC, you can use cmake to build simple programs on\nWindows; this can be helpful if you want to learn more about some peculiar linking behavior\nby reproducing it on a small example. Here's a simple example cmake file that defines\ntwo dynamic libraries, one linking with the other:\n\n```CMake\nproject(myproject CXX)\nset(CMAKE_CXX_STANDARD 14)\nadd_library(foo SHARED foo.cpp)\nadd_library(bar SHARED bar.cpp)\n# NB: don't forget to __declspec(dllexport) at least one symbol from foo,\n# otherwise foo.lib will not be created.\ntarget_link_libraries(bar PUBLIC foo)\n```\n\nYou can build it with:\n\n```bash\nmkdir build\ncd build\ncmake ..\ncmake --build .\n```\n\n### Known MSVC (and MSVC with NVCC) bugs\n\nThe PyTorch codebase sometimes likes to use exciting C++ features, and\nthese exciting features lead to exciting bugs in Windows compilers.\nTo add insult to injury, the error messages will often not tell you\nwhich line of code actually induced the erroring template instantiation.\n\nWe've found the most effective way to debug these problems is to\ncarefully read over diffs, keeping in mind known bugs in MSVC/NVCC.\nHere are a few well known pitfalls and workarounds:\n\n* This is not actually a bug per se, but in general, code generated by MSVC\n  is more sensitive to memory errors; you may have written some code\n  that does a use-after-free or stack overflows; on Linux the code\n  might work, but on Windows your program will crash. ASAN may not\n  catch all of these problems: stay vigilant to the possibility that\n  your crash is due to a real memory problem.\n\n* `constexpr` generally works less well on MSVC.\n\n  * The idiom `static_assert(f() == f())` to test if `f` is constexpr\n    does not work; you'll get \"error C2131: expression did not evaluate\n    to a constant\". Don't use these asserts on Windows.\n    (Example: `c10/util/intrusive_ptr.h`)\n\n* (NVCC) Code you access inside a `static_assert` will eagerly be\n  evaluated as if it were device code, and so you might get an error\n  that the code is \"not accessible\".\n\n```cpp\nclass A {\n  static A singleton_;\n  static constexpr inline A* singleton() {\n    return &singleton_;\n  }\n};\nstatic_assert(std::is_same(A*, decltype(A::singleton()))::value, \"hmm\");\n```\n\n* The compiler will run out of heap space if you attempt to compile files that\n  are too large. Splitting such files into separate files helps.\n  (Example: `THTensorMath`, `THTensorMoreMath`, `THTensorEvenMoreMath`.)\n\n* MSVC's preprocessor (but not the standard compiler) has a bug\n  where it incorrectly tokenizes raw string literals, ending when it sees a `\"`.\n  This causes preprocessor tokens inside the literal like an`#endif`  to be incorrectly\n  treated as preprocessor directives. See https://godbolt.org/z/eVTIJq as an example.\n\n* Either MSVC or the Windows headers have a PURE macro defined and will replace\n  any occurrences of the PURE token in code with an empty string. This is why\n  we have AliasAnalysisKind::PURE_FUNCTION and not AliasAnalysisKind::PURE.\n  The same is likely true for other identifiers that we just didn't try to use yet.\n\n### Building on legacy code and CUDA\n\nCUDA, MSVC, and PyTorch versions are interdependent; please install matching versions from this table:\n| CUDA version | Newest supported VS version                             | PyTorch version |\n| ------------ | ------------------------------------------------------- | --------------- |\n| 10.1         | Visual Studio 2019 (16.X) (`_MSC_VER` < 1930)           |  1.3.0 ~ 1.7.0  |\n| 10.2         | Visual Studio 2019 (16.X) (`_MSC_VER` < 1930)           |  1.5.0 ~ 1.7.0  |\n| 11.0         | Visual Studio 2019 (16.X) (`_MSC_VER` < 1930)           |      1.7.0      |\n\nNote: There's a [compilation issue](https://github.com/oneapi-src/oneDNN/issues/812) in several Visual Studio 2019 versions since 16.7.1, so please make sure your Visual Studio 2019 version is not in 16.7.1 ~ 16.7.5\n\n## Pre-commit tidy/linting hook\n\nWe use clang-tidy to perform additional\nformatting and semantic checking of code. We provide a pre-commit git hook for\nperforming these checks, before a commit is created:\n\n  ```bash\n  ln -s ../../tools/git-pre-commit .git/hooks/pre-commit\n  ```\n\nIf you have already committed files and\nCI reports `flake8` errors, you can run the check locally in your PR branch with:\n\n  ```bash\n  flake8 $(git diff --name-only $(git merge-base --fork-point main))\n  ```\n\nYou'll need to install an appropriately configured flake8; see\n[Lint as you type](https://github.com/pytorch/pytorch/wiki/Lint-as-you-type)\nfor documentation on how to do this.\n\nFix the code so that no errors are reported when you re-run the above check again,\nand then commit the fix.\n\n## Building PyTorch with ASAN\n\n[ASAN](https://github.com/google/sanitizers/wiki/AddressSanitizer) is very\nuseful for debugging memory errors in C++. We run it in CI, but here's how to\nget the same thing to run on your local machine.\n\nFirst, install LLVM 8. The easiest way is to get [prebuilt\nbinaries](http://releases.llvm.org/download.html#8.0.0) and extract them to\nfolder (later called `$LLVM_ROOT`).\n\nThen set up the appropriate scripts. You can put this in your `.bashrc`:\n\n```bash\nLLVM_ROOT=<wherever your llvm install is>\nPYTORCH_ROOT=<wherever your pytorch checkout is>\n\nLIBASAN_RT=\"$LLVM_ROOT/lib/clang/8.0.0/lib/linux/libclang_rt.asan-x86_64.so\"\nbuild_with_asan()\n{\n  LD_PRELOAD=${LIBASAN_RT} \\\n  CC=\"$LLVM_ROOT/bin/clang\" \\\n  CXX=\"$LLVM_ROOT/bin/clang++\" \\\n  LDSHARED=\"clang --shared\" \\\n  LDFLAGS=\"-stdlib=libstdc++\" \\\n  CFLAGS=\"-fsanitize=address -fno-sanitize-recover=all -shared-libasan -pthread\" \\\n  CXX_FLAGS=\"-pthread\" \\\n  USE_CUDA=0 USE_OPENMP=0 USE_DISTRIBUTED=0 DEBUG=1 \\\n  python setup.py develop\n}\n\nrun_with_asan()\n{\n  LD_PRELOAD=${LIBASAN_RT} $@\n}\n\n# you can look at build-asan.sh to find the latest options the CI uses\nexport ASAN_OPTIONS=detect_leaks=0:symbolize=1:strict_init_order=true\nexport UBSAN_OPTIONS=print_stacktrace=1:suppressions=$PYTORCH_ROOT/ubsan.supp\nexport ASAN_SYMBOLIZER_PATH=$LLVM_ROOT/bin/llvm-symbolizer\n```\n\nThen you can use the scripts like:\n\n```\nsuo-devfair ~/pytorch  build_with_asan\nsuo-devfair ~/pytorch  run_with_asan python test/test_jit.py\n```\n\n### Getting `ccache` to work\n\nThe scripts above specify the `clang` and `clang++` binaries directly, which\nbypasses `ccache`. Here's how to get `ccache` to work:\n\n1. Make sure the ccache symlinks for `clang` and `clang++` are set up (see\n   CONTRIBUTING.md)\n2. Make sure `$LLVM_ROOT/bin` is available on your `$PATH`.\n3. Change the `CC` and `CXX` variables in `build_with_asan()` to point\n   directly to `clang` and `clang++`.\n\n### Why this stuff with `LD_PRELOAD` and `LIBASAN_RT`?\n\nThe standard workflow for ASAN assumes you have a standalone binary:\n\n1. Recompile your binary with `-fsanitize=address`.\n2. Run the binary, and ASAN will report whatever errors it find.\n\nUnfortunately, PyTorch is a distributed as a shared library that is loaded by\na third-party executable (Python). Its too much of a hassle to recompile all\nof Python every time we want to use ASAN. Luckily, the ASAN folks have a\nworkaround for cases like this:\n\n1. Recompile your library with `-fsanitize=address -shared-libasan`. The\n   extra `-shared-libasan` tells the compiler to ask for the shared ASAN\n   runtime library.\n2. Use `LD_PRELOAD` to tell the dynamic linker to load the ASAN runtime\n   library before anything else.\n\nMore information can be found\n[here](https://github.com/google/sanitizers/wiki/AddressSanitizerAsDso).\n\n### Why LD_PRELOAD in the build function?\n\nWe need `LD_PRELOAD` because there is a cmake check that ensures that a\nsimple program builds and runs. If we are building with ASAN as a shared\nlibrary, we need to `LD_PRELOAD` the runtime library, otherwise there will\ndynamic linker errors and the check will fail.\n\nWe dont actually need either of these if we fix the cmake checks.\n\n### Why no leak detection?\n\nPython leaks a lot of memory. Possibly we could configure a suppression file,\nbut we havent gotten around to it.\n\n## Caffe2 notes\n\nIn 2018, we merged Caffe2 into the PyTorch source repository. While the\nsteady state aspiration is that Caffe2 and PyTorch share code freely,\nin the meantime there will be some separation.\n\nThere are a few \"unusual\" directories which, for historical reasons,\nare Caffe2/PyTorch specific. Here they are:\n\n- `CMakeLists.txt`, `Makefile`, `binaries`, `cmake`, `conda`, `modules`,\n  `scripts` are Caffe2-specific. Don't put PyTorch code in them without\n  extra coordination.\n\n- `mypy*`, `requirements.txt`, `setup.py`, `test`, `tools` are\n  PyTorch-specific. Don't put Caffe2 code in them without extra\n  coordination.\n\n## CI failure tips\n\nOnce you submit a PR or push a new commit to a branch that is in\nan active PR, CI jobs will be run automatically. Some of these may\nfail and you will need to find out why, by looking at the logs.\n\nFairly often, a CI failure might be unrelated to your changes. You can\nconfirm by going to our [HUD](https://hud.pytorch.org) and seeing if the CI job\nis failing upstream already. In this case, you\ncan usually ignore the failure. See [the following\nsubsection](#which-commit-is-used-in-ci) for more details.\n\nSome failures might be related to specific hardware or environment\nconfigurations. In this case, if you're a Meta employee, you can ssh into\nthe job's session to perform manual debugging following the instructions in\nour [CI wiki](https://github.com/pytorch/pytorch/wiki/Debugging-using-with-ssh-for-Github-Actions).\n\n\n### Which commit is used in CI?\n\nFor CI run on `main`, this repository is checked out for a given `main`\ncommit, and CI is run on that commit (there isn't really any other choice).\n\nFor PRs, however, it's a bit more complicated. Consider this commit graph, where\n`main` is at commit `A`, and the branch for PR #42 (just a placeholder) is at\ncommit `B`:\n\n```\n       o---o---B (refs/pull/42/head)\n      /         \\\n     /           C (refs/pull/42/merge)\n    /           /\n---o---o---o---A (merge-destination) - usually main\n```\n\nThere are two possible choices for which commit to use:\n\n1. Checkout commit `B`, the head of the PR (manually committed by the PR\n   author).\n2. Checkout commit `C`, the hypothetical result of what would happen if the PR\n   were merged into its destination (usually `main`).\n\nFor all practical purposes, most people can think of the commit being used as\ncommit `B` (choice **1**).\n\nHowever, if workflow files (which govern CI behavior) were modified (either by your PR or since dev branch were created ) there's\na nuance to know about:\nThe workflow files themselves get taken from checkpoint `C`, the merger of your\nPR and the `main` branch. But only the workflow files get taken from that merged\ncheckpoint. Everything else (tests, code, etc) all get taken directly from your\nPR's commit (commit `B`). Please note, this scenario would never affect PRs authored by `ghstack` as they would not automatically ingest the updates from default branch.\n\n\n## Dev Infra Office Hours\n[Dev Infra Office Hours](https://github.com/pytorch/pytorch/wiki/Dev-Infra-Office-Hours) are hosted every Friday to answer any questions regarding developer experience, Green HUD, and CI.\n"
        },
        {
          "name": "Dockerfile",
          "type": "blob",
          "size": 4.09,
          "content": "# syntax=docker/dockerfile:1\n\n# NOTE: Building this image require's docker version >= 23.0.\n#\n# For reference:\n# - https://docs.docker.com/build/dockerfile/frontend/#stable-channel\n\nARG BASE_IMAGE=ubuntu:22.04\nARG PYTHON_VERSION=3.11\n\nFROM ${BASE_IMAGE} as dev-base\nRUN apt-get update && DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends \\\n        build-essential \\\n        ca-certificates \\\n        ccache \\\n        cmake \\\n        curl \\\n        git \\\n        libjpeg-dev \\\n        libpng-dev && \\\n    rm -rf /var/lib/apt/lists/*\nRUN /usr/sbin/update-ccache-symlinks\nRUN mkdir /opt/ccache && ccache --set-config=cache_dir=/opt/ccache\nENV PATH /opt/conda/bin:$PATH\n\nFROM dev-base as conda\nARG PYTHON_VERSION=3.11\n# Automatically set by buildx\nARG TARGETPLATFORM\n# translating Docker's TARGETPLATFORM into miniconda arches\nRUN case ${TARGETPLATFORM} in \\\n         \"linux/arm64\")  MINICONDA_ARCH=aarch64  ;; \\\n         *)              MINICONDA_ARCH=x86_64   ;; \\\n    esac && \\\n    curl -fsSL -v -o ~/miniconda.sh -O  \"https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-Linux-${MINICONDA_ARCH}.sh\"\nCOPY requirements.txt .\n# Manually invoke bash on miniconda script per https://github.com/conda/conda/issues/10431\nRUN chmod +x ~/miniconda.sh && \\\n    bash ~/miniconda.sh -b -p /opt/conda && \\\n    rm ~/miniconda.sh && \\\n    /opt/conda/bin/conda install -y python=${PYTHON_VERSION} cmake conda-build pyyaml numpy ipython && \\\n    /opt/conda/bin/python -mpip install -r requirements.txt && \\\n    /opt/conda/bin/conda clean -ya\n\nFROM dev-base as submodule-update\nWORKDIR /opt/pytorch\nCOPY . .\nRUN git submodule update --init --recursive\n\nFROM conda as build\nARG CMAKE_VARS\nWORKDIR /opt/pytorch\nCOPY --from=conda /opt/conda /opt/conda\nCOPY --from=submodule-update /opt/pytorch /opt/pytorch\nRUN make triton\nRUN --mount=type=cache,target=/opt/ccache \\\n    export eval ${CMAKE_VARS} && \\\n    TORCH_CUDA_ARCH_LIST=\"7.0 7.2 7.5 8.0 8.6 8.7 8.9 9.0 9.0a\" TORCH_NVCC_FLAGS=\"-Xfatbin -compress-all\" \\\n    CMAKE_PREFIX_PATH=\"$(dirname $(which conda))/../\" \\\n    python setup.py install\n\nFROM conda as conda-installs\nARG PYTHON_VERSION=3.11\nARG CUDA_PATH=cu121\nARG CUDA_CHANNEL=nvidia\nARG INSTALL_CHANNEL=whl/nightly\n# Automatically set by buildx\nRUN /opt/conda/bin/conda update -y -n base -c defaults conda\nRUN /opt/conda/bin/conda install -y python=${PYTHON_VERSION}\n\nARG TARGETPLATFORM\n\n# INSTALL_CHANNEL whl - release, whl/nightly - nightly, whle/test - test channels\nRUN case ${TARGETPLATFORM} in \\\n         \"linux/arm64\")  pip install --extra-index-url https://download.pytorch.org/whl/cpu/ torch torchvision torchaudio ;; \\\n         *)              pip install --index-url https://download.pytorch.org/${INSTALL_CHANNEL}/${CUDA_PATH#.}/ torch torchvision torchaudio ;; \\\n    esac && \\\n    /opt/conda/bin/conda clean -ya\nRUN /opt/conda/bin/pip install torchelastic\nRUN IS_CUDA=$(python -c 'import torch ; print(torch.cuda._is_compiled())'); \\\n    echo \"Is torch compiled with cuda: ${IS_CUDA}\"; \\\n    if test \"${IS_CUDA}\" != \"True\" -a ! -z \"${CUDA_VERSION}\"; then \\\n        exit 1; \\\n    fi\n\nFROM ${BASE_IMAGE} as official\nARG PYTORCH_VERSION\nARG TRITON_VERSION\nARG TARGETPLATFORM\nARG CUDA_VERSION\nLABEL com.nvidia.volumes.needed=\"nvidia_driver\"\nRUN apt-get update && DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends \\\n        ca-certificates \\\n        libjpeg-dev \\\n        libpng-dev \\\n        && rm -rf /var/lib/apt/lists/*\nCOPY --from=conda-installs /opt/conda /opt/conda\nRUN if test -n \"${TRITON_VERSION}\" -a \"${TARGETPLATFORM}\" != \"linux/arm64\"; then \\\n        DEBIAN_FRONTEND=noninteractive apt install -y --no-install-recommends gcc; \\\n        rm -rf /var/lib/apt/lists/*; \\\n    fi\nENV PATH /opt/conda/bin:$PATH\nENV NVIDIA_VISIBLE_DEVICES all\nENV NVIDIA_DRIVER_CAPABILITIES compute,utility\nENV LD_LIBRARY_PATH /usr/local/nvidia/lib:/usr/local/nvidia/lib64\nENV PATH /usr/local/nvidia/bin:/usr/local/cuda/bin:$PATH\nENV PYTORCH_VERSION ${PYTORCH_VERSION}\nWORKDIR /workspace\n\nFROM official as dev\n# Should override the already installed version from the official-image stage\nCOPY --from=build /opt/conda /opt/conda\n"
        },
        {
          "name": "GLOSSARY.md",
          "type": "blob",
          "size": 2.41,
          "content": "# PyTorch Glossary\n\n<!-- toc -->\n\n- [Operation and Kernel](#operation-and-kernel)\n  - [ATen](#aten)\n  - [Operation](#operation)\n  - [Native Operation](#native-operation)\n  - [Custom Operation](#custom-operation)\n  - [Kernel](#kernel)\n  - [Compound Operation](#compound-operation)\n  - [Composite Operation](#composite-operation)\n  - [Non-Leaf Operation](#non-leaf-operation)\n  - [Leaf Operation](#leaf-operation)\n  - [Device Kernel](#device-kernel)\n  - [Compound Kernel](#compound-kernel)\n- [JIT Compilation](#jit-compilation)\n  - [JIT](#jit)\n  - [TorchScript](#torchscript)\n  - [Tracing](#tracing)\n  - [Scripting](#scripting)\n\n<!-- tocstop -->\n\n# Operation and Kernel\n\n## ATen\nShort for \"A Tensor Library\". The foundational tensor and mathematical\noperation library on which all else is built.\n\n## Operation\nA unit of work. For example, the work of matrix multiplication is an operation\ncalled aten::matmul.\n\n## Native Operation\nAn operation that comes natively with PyTorch ATen, for example aten::matmul.\n\n## Custom Operation\nAn Operation that is defined by users and is usually a Compound Operation.\nFor example, this\n[tutorial](https://pytorch.org/docs/stable/notes/extending.html) details how\nto create Custom Operations.\n\n## Kernel\nImplementation of a PyTorch operation, specifying what should be done when an\noperation executes.\n\n## Compound Operation\nA Compound Operation is composed of other operations. Its kernel is usually\ndevice-agnostic. Normally it doesn't have its own derivative functions defined.\nInstead, AutoGrad automatically computes its derivative based on operations it\nuses.\n\n## Composite Operation\nSame as Compound Operation.\n\n## Non-Leaf Operation\nSame as Compound Operation.\n\n## Leaf Operation\nAn operation that's considered a basic operation, as opposed to a Compound\nOperation. Leaf Operation always has dispatch functions defined, usually has a\nderivative function defined as well.\n\n## Device Kernel\nDevice-specific kernel of a leaf operation.\n\n## Compound Kernel\nOpposed to Device Kernels, Compound kernels are usually device-agnostic and belong to Compound Operations.\n\n# JIT Compilation\n\n## JIT\nJust-In-Time Compilation.\n\n## TorchScript\nAn interface to the TorchScript JIT compiler and interpreter.\n\n## Tracing\nUsing `torch.jit.trace` on a function to get an executable that can be optimized\nusing just-in-time compilation.\n\n## Scripting\nUsing `torch.jit.script` on a function to inspect source code and compile it as\nTorchScript code.\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 3.38,
          "content": "From PyTorch:\n\nCopyright (c) 2016-     Facebook, Inc            (Adam Paszke)\nCopyright (c) 2014-     Facebook, Inc            (Soumith Chintala)\nCopyright (c) 2011-2014 Idiap Research Institute (Ronan Collobert)\nCopyright (c) 2012-2014 Deepmind Technologies    (Koray Kavukcuoglu)\nCopyright (c) 2011-2012 NEC Laboratories America (Koray Kavukcuoglu)\nCopyright (c) 2011-2013 NYU                      (Clement Farabet)\nCopyright (c) 2006-2010 NEC Laboratories America (Ronan Collobert, Leon Bottou, Iain Melvin, Jason Weston)\nCopyright (c) 2006      Idiap Research Institute (Samy Bengio)\nCopyright (c) 2001-2004 Idiap Research Institute (Ronan Collobert, Samy Bengio, Johnny Mariethoz)\n\nFrom Caffe2:\n\nCopyright (c) 2016-present, Facebook Inc. All rights reserved.\n\nAll contributions by Facebook:\nCopyright (c) 2016 Facebook Inc.\n\nAll contributions by Google:\nCopyright (c) 2015 Google Inc.\nAll rights reserved.\n\nAll contributions by Yangqing Jia:\nCopyright (c) 2015 Yangqing Jia\nAll rights reserved.\n\nAll contributions by Kakao Brain:\nCopyright 2019-2020 Kakao Brain\n\nAll contributions by Cruise LLC:\nCopyright (c) 2022 Cruise LLC.\nAll rights reserved.\n\nAll contributions by Tri Dao:\nCopyright (c) 2024 Tri Dao.\nAll rights reserved.\n\nAll contributions by Arm:\nCopyright (c) 2021, 2023-2024 Arm Limited and/or its affiliates\n\nAll contributions from Caffe:\nCopyright(c) 2013, 2014, 2015, the respective contributors\nAll rights reserved.\n\nAll other contributions:\nCopyright(c) 2015, 2016 the respective contributors\nAll rights reserved.\n\nCaffe2 uses a copyright model similar to Caffe: each contributor holds\ncopyright over their contributions to Caffe2. The project versioning records\nall such contribution and copyright details. If a contributor wants to further\nmark their specific copyright on a particular contribution, they should\nindicate their copyright solely in the commit message of the change when it is\ncommitted.\n\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n1. Redistributions of source code must retain the above copyright\n   notice, this list of conditions and the following disclaimer.\n\n2. Redistributions in binary form must reproduce the above copyright\n   notice, this list of conditions and the following disclaimer in the\n   documentation and/or other materials provided with the distribution.\n\n3. Neither the names of Facebook, Deepmind Technologies, NYU, NEC Laboratories America\n   and IDIAP Research Institute nor the names of its contributors may be\n   used to endorse or promote products derived from this software without\n   specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\nARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE\nLIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\nCONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\nSUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\nINTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\nCONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\nARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\nPOSSIBILITY OF SUCH DAMAGE.\n"
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.81,
          "content": "include MANIFEST.in\ninclude CMakeLists.txt\ninclude CITATION.cff\ninclude LICENSE\ninclude NOTICE\ninclude .gitmodules\ninclude build_variables.bzl\ninclude mypy.ini\ninclude requirements.txt\ninclude ufunc_defs.bzl\ninclude version.txt\nrecursive-include android *.*\nrecursive-include aten *.*\nrecursive-include binaries *.*\nrecursive-include c10 *.*\nrecursive-include caffe2 *.*\nrecursive-include cmake *.*\nrecursive-include torch *.*\nrecursive-include tools *.*\nrecursive-include test *.*\nrecursive-include docs *.*\nrecursive-include ios *.*\nrecursive-include third_party *\nrecursive-include test *.*\nrecursive-include benchmarks *.*\nrecursive-include scripts *.*\nrecursive-include mypy_plugins *.*\nrecursive-include modules *.*\nrecursive-include functorch *.*\nprune */__pycache__\nglobal-exclude *.o *.so *.dylib *.a .git *.pyc *.swp\n"
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 1.3,
          "content": "# This makefile does nothing but delegating the actual building to cmake.\nPYTHON = python3\nPIP = $(PYTHON) -m pip\nNIGHTLY_TOOL_OPTS := pull\n\nall:\n\t@mkdir -p build && cd build && cmake .. $(shell $(PYTHON) ./scripts/get_python_cmake_flags.py) && $(MAKE)\n\nlocal:\n\t@./scripts/build_local.sh\n\nandroid:\n\t@./scripts/build_android.sh\n\nios:\n\t@./scripts/build_ios.sh\n\nclean: # This will remove ALL build folders.\n\t@rm -r build*/\n\nlinecount:\n\t@cloc --read-lang-def=caffe.cloc caffe2 || \\\n\t\techo \"Cloc is not available on the machine. You can install cloc with \" && \\\n\t\techo \"    sudo apt-get install cloc\"\n\nensure-branch-clean:\n\t@if [ -n \"$(shell git status --porcelain)\" ]; then \\\n\t\techo \"Please commit or stash all changes before running this script\"; \\\n\t\texit 1; \\\n\tfi\n\nsetup-env: ensure-branch-clean\n\t$(PYTHON) tools/nightly.py $(NIGHTLY_TOOL_OPTS)\n\nsetup-env-cuda:\n\t$(MAKE) setup-env PYTHON=\"$(PYTHON)\" NIGHTLY_TOOL_OPTS=\"$(NIGHTLY_TOOL_OPTS) --cuda\"\n\nsetup-env-rocm:\n\t$(MAKE) setup-env PYTHON=\"$(PYTHON)\" NIGHTLY_TOOL_OPTS=\"$(NIGHTLY_TOOL_OPTS) --rocm\"\n\nsetup_env: setup-env\nsetup_env_cuda: setup-env-cuda\nsetup_env_rocm: setup-env-rocm\n\nsetup-lint:\n\t$(PIP) install lintrunner\n\tlintrunner init\n\nsetup_lint: setup-lint\n\nlint:\n\tlintrunner\n\nquicklint:\n\tlintrunner\n\ntriton:\n\t$(PIP) uninstall -y triton\n\t@./scripts/install_triton_wheel.sh\n"
        },
        {
          "name": "NOTICE",
          "type": "blob",
          "size": 23.08,
          "content": "=======================================================================\nSoftware under third_party\n=======================================================================\nSoftware libraries under third_party are provided as github submodule\nlinks, and their content is not part of the Caffe2 codebase. Their\nlicences can be found under the respective software repositories.\n\n=======================================================================\nEarlier BSD License\n=======================================================================\nEarly development of Caffe2 in 2015 and early 2016 is licensed under the\nBSD license. The license is attached below:\n\nAll contributions by Facebook:\nCopyright (c) 2016 Facebook Inc.\n\nAll contributions by Google:\nCopyright (c) 2015 Google Inc.\nAll rights reserved.\n\nAll contributions by Yangqing Jia:\nCopyright (c) 2015 Yangqing Jia\nAll rights reserved.\n\nAll contributions by Kakao Brain:\nCopyright 2019-2020 Kakao Brain\n\nAll other contributions:\nCopyright(c) 2015, 2016 the respective contributors\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n1. Redistributions of source code must retain the above copyright notice, this\n   list of conditions and the following disclaimer.\n2. Redistributions in binary form must reproduce the above copyright notice,\n   this list of conditions and the following disclaimer in the documentation\n   and/or other materials provided with the distribution.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\nANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR\nANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\nLOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\nON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\n=======================================================================\nCaffe's BSD License\n=======================================================================\nSome parts of the caffe2 code is derived from the original Caffe code, which is\ncreated by Yangqing Jia and is now a BSD-licensed open-source project. The Caffe\nlicense is as follows:\n\nCOPYRIGHT\n\nAll contributions by the University of California:\nCopyright (c) 2014, The Regents of the University of California (Regents)\nAll rights reserved.\n\nAll other contributions:\nCopyright (c) 2014, the respective contributors\nAll rights reserved.\n\nCaffe uses a shared copyright model: each contributor holds copyright over\ntheir contributions to Caffe. The project versioning records all such\ncontribution and copyright details. If a contributor wants to further mark\ntheir specific copyright on a particular contribution, they should indicate\ntheir copyright solely in the commit message of the change when it is\ncommitted.\n\nLICENSE\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n1. Redistributions of source code must retain the above copyright notice, this\n   list of conditions and the following disclaimer.\n2. Redistributions in binary form must reproduce the above copyright notice,\n   this list of conditions and the following disclaimer in the documentation\n   and/or other materials provided with the distribution.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\nANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR\nANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\nLOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\nON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\nCONTRIBUTION AGREEMENT\n\nBy contributing to the BVLC/caffe repository through pull-request, comment,\nor otherwise, the contributor releases their content to the\nlicense and copyright terms herein.\n\n=======================================================================\nCaffe2's Apache License\n=======================================================================\n\nThis repo contains Caffe2 code, which was previously licensed under\nApache License Version 2.0:\n\n                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n=======================================================================\nCephes's 3-Clause BSD License\n=======================================================================\n\nCode derived from implementations in the Cephes Math Library should mention\nits derivation and reference the following license:\n\n   3-Clause BSD License for the Cephes Math Library\n   Copyright (c) 2018, Steven Moshier\n   All rights reserved.\n\n   Redistribution and use in source and binary forms, with or without\n   modification, are permitted provided that the following conditions are met:\n\n   * Redistributions of source code must retain the above copyright\n   notice, this list of conditions and the following disclaimer.\n\n   * Redistributions in binary form must reproduce the above copyright\n   notice, this list of conditions and the following disclaimer in the\n   documentation and/or other materials provided with the distribution.\n\n   * Neither the name of the nor the\n   names of its contributors may be used to endorse or promote products\n   derived from this software without specific prior written permission.\n\n   THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n   ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n   WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n   DISCLAIMED. IN NO EVENT SHALL Steven Moshier BE LIABLE FOR ANY\n   DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n   (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n   LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n   ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n   (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n   SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\n=======================================================================\nSciPy's 3-Clause BSD License\n=======================================================================\n\nCode derived from implementations in SciPy should mention its derivation\nand reference the following license:\n\n   Copyright (c) 2001-2002 Enthought, Inc.  2003-2019, SciPy Developers.\n   All rights reserved.\n\n   Redistribution and use in source and binary forms, with or without\n   modification, are permitted provided that the following conditions\n   are met:\n\n   1. Redistributions of source code must retain the above copyright\n     notice, this list of conditions and the following disclaimer.\n\n   2. Redistributions in binary form must reproduce the above\n     copyright notice, this list of conditions and the following\n     disclaimer in the documentation and/or other materials provided\n     with the distribution.\n\n   3. Neither the name of the copyright holder nor the names of its\n     contributors may be used to endorse or promote products derived\n     from this software without specific prior written permission.\n\n   THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n   \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n   LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n   A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n   OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n   SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n   LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n   DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n   THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n   (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n   OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n=======================================================================\nBoost's 1.0 Software License\n=======================================================================\n\nCode derived from implementations in Boost 1.0 should mention its\nderivation and reference the following license:\n\n   Boost Software License - Version 1.0 - August 17th, 2003\n\n   Permission is hereby granted, free of charge, to any person or organization\n   obtaining a copy of the software and accompanying documentation covered by\n   this license (the \"Software\") to use, reproduce, display, distribute,\n   execute, and transmit the Software, and to prepare derivative works of the\n   Software, and to permit third-parties to whom the Software is furnished to\n   do so, all subject to the following:\n\n   The copyright notices in the Software and this entire statement, including\n   the above license grant, this restriction and the following disclaimer,\n   must be included in all copies of the Software, in whole or in part, and\n   all derivative works of the Software, unless such copies or derivative\n   works are solely in the form of machine-executable object code generated by\n   a source language processor.\n\n   THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n   FITNESS FOR A PARTICULAR PURPOSE, TITLE AND NON-INFRINGEMENT. IN NO EVENT\n   SHALL THE COPYRIGHT HOLDERS OR ANYONE DISTRIBUTING THE SOFTWARE BE LIABLE\n   FOR ANY DAMAGES OR OTHER LIABILITY, WHETHER IN CONTRACT, TORT OR OTHERWISE,\n   ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n   DEALINGS IN THE SOFTWARE.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n\n=======================================================================\nPILLOW-SIMD Software License\n=======================================================================\n\nCode derived from implementations in PILLOW-SIMD should mention its derivation\nand reference the following license:\n\n    The Python Imaging Library (PIL) is\n\n        Copyright  1997-2011 by Secret Labs AB\n        Copyright  1995-2011 by Fredrik Lundh\n\n    Pillow is the friendly PIL fork. It is\n\n        Copyright  2010-2022 by Alex Clark and contributors\n\n    Like PIL, Pillow is licensed under the open source HPND License:\n\n    By obtaining, using, and/or copying this software and/or its associated\n    documentation, you agree that you have read, understood, and will comply\n    with the following terms and conditions:\n\n    Permission to use, copy, modify, and distribute this software and its\n    associated documentation for any purpose and without fee is hereby granted,\n    provided that the above copyright notice appears in all copies, and that\n    both that copyright notice and this permission notice appear in supporting\n    documentation, and that the name of Secret Labs AB or the author not be\n    used in advertising or publicity pertaining to distribution of the software\n    without specific, written prior permission.\n\n    SECRET LABS AB AND THE AUTHOR DISCLAIMS ALL WARRANTIES WITH REGARD TO THIS\n    SOFTWARE, INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS.\n    IN NO EVENT SHALL SECRET LABS AB OR THE AUTHOR BE LIABLE FOR ANY SPECIAL,\n    INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING FROM\n    LOSS OF USE, DATA OR PROFITS, WHETHER IN AN ACTION OF CONTRACT, NEGLIGENCE\n    OR OTHER TORTIOUS ACTION, ARISING OUT OF OR IN CONNECTION WITH THE USE OR\n    PERFORMANCE OF THIS SOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 24.33,
          "content": "![PyTorch Logo](https://github.com/pytorch/pytorch/raw/main/docs/source/_static/img/pytorch-logo-dark.png)\n\n--------------------------------------------------------------------------------\n\nPyTorch is a Python package that provides two high-level features:\n- Tensor computation (like NumPy) with strong GPU acceleration\n- Deep neural networks built on a tape-based autograd system\n\nYou can reuse your favorite Python packages such as NumPy, SciPy, and Cython to extend PyTorch when needed.\n\nOur trunk health (Continuous Integration signals) can be found at [hud.pytorch.org](https://hud.pytorch.org/ci/pytorch/pytorch/main).\n\n<!-- toc -->\n\n- [More About PyTorch](#more-about-pytorch)\n  - [A GPU-Ready Tensor Library](#a-gpu-ready-tensor-library)\n  - [Dynamic Neural Networks: Tape-Based Autograd](#dynamic-neural-networks-tape-based-autograd)\n  - [Python First](#python-first)\n  - [Imperative Experiences](#imperative-experiences)\n  - [Fast and Lean](#fast-and-lean)\n  - [Extensions Without Pain](#extensions-without-pain)\n- [Installation](#installation)\n  - [Binaries](#binaries)\n    - [NVIDIA Jetson Platforms](#nvidia-jetson-platforms)\n  - [From Source](#from-source)\n    - [Prerequisites](#prerequisites)\n      - [NVIDIA CUDA Support](#nvidia-cuda-support)\n      - [AMD ROCm Support](#amd-rocm-support)\n      - [Intel GPU Support](#intel-gpu-support)\n    - [Get the PyTorch Source](#get-the-pytorch-source)\n    - [Install Dependencies](#install-dependencies)\n    - [Install PyTorch](#install-pytorch)\n      - [Adjust Build Options (Optional)](#adjust-build-options-optional)\n  - [Docker Image](#docker-image)\n    - [Using pre-built images](#using-pre-built-images)\n    - [Building the image yourself](#building-the-image-yourself)\n  - [Building the Documentation](#building-the-documentation)\n  - [Previous Versions](#previous-versions)\n- [Getting Started](#getting-started)\n- [Resources](#resources)\n- [Communication](#communication)\n- [Releases and Contributing](#releases-and-contributing)\n- [The Team](#the-team)\n- [License](#license)\n\n<!-- tocstop -->\n\n## More About PyTorch\n\n[Learn the basics of PyTorch](https://pytorch.org/tutorials/beginner/basics/intro.html)\n\nAt a granular level, PyTorch is a library that consists of the following components:\n\n| Component | Description |\n| ---- | --- |\n| [**torch**](https://pytorch.org/docs/stable/torch.html) | A Tensor library like NumPy, with strong GPU support |\n| [**torch.autograd**](https://pytorch.org/docs/stable/autograd.html) | A tape-based automatic differentiation library that supports all differentiable Tensor operations in torch |\n| [**torch.jit**](https://pytorch.org/docs/stable/jit.html) | A compilation stack (TorchScript) to create serializable and optimizable models from PyTorch code  |\n| [**torch.nn**](https://pytorch.org/docs/stable/nn.html) | A neural networks library deeply integrated with autograd designed for maximum flexibility |\n| [**torch.multiprocessing**](https://pytorch.org/docs/stable/multiprocessing.html) | Python multiprocessing, but with magical memory sharing of torch Tensors across processes. Useful for data loading and Hogwild training |\n| [**torch.utils**](https://pytorch.org/docs/stable/data.html) | DataLoader and other utility functions for convenience |\n\nUsually, PyTorch is used either as:\n\n- A replacement for NumPy to use the power of GPUs.\n- A deep learning research platform that provides maximum flexibility and speed.\n\nElaborating Further:\n\n### A GPU-Ready Tensor Library\n\nIf you use NumPy, then you have used Tensors (a.k.a. ndarray).\n\n![Tensor illustration](./docs/source/_static/img/tensor_illustration.png)\n\nPyTorch provides Tensors that can live either on the CPU or the GPU and accelerates the\ncomputation by a huge amount.\n\nWe provide a wide variety of tensor routines to accelerate and fit your scientific computation needs\nsuch as slicing, indexing, mathematical operations, linear algebra, reductions.\nAnd they are fast!\n\n### Dynamic Neural Networks: Tape-Based Autograd\n\nPyTorch has a unique way of building neural networks: using and replaying a tape recorder.\n\nMost frameworks such as TensorFlow, Theano, Caffe, and CNTK have a static view of the world.\nOne has to build a neural network and reuse the same structure again and again.\nChanging the way the network behaves means that one has to start from scratch.\n\nWith PyTorch, we use a technique called reverse-mode auto-differentiation, which allows you to\nchange the way your network behaves arbitrarily with zero lag or overhead. Our inspiration comes\nfrom several research papers on this topic, as well as current and past work such as\n[torch-autograd](https://github.com/twitter/torch-autograd),\n[autograd](https://github.com/HIPS/autograd),\n[Chainer](https://chainer.org), etc.\n\nWhile this technique is not unique to PyTorch, it's one of the fastest implementations of it to date.\nYou get the best of speed and flexibility for your crazy research.\n\n![Dynamic graph](https://github.com/pytorch/pytorch/raw/main/docs/source/_static/img/dynamic_graph.gif)\n\n### Python First\n\nPyTorch is not a Python binding into a monolithic C++ framework.\nIt is built to be deeply integrated into Python.\nYou can use it naturally like you would use [NumPy](https://www.numpy.org/) / [SciPy](https://www.scipy.org/) / [scikit-learn](https://scikit-learn.org) etc.\nYou can write your new neural network layers in Python itself, using your favorite libraries\nand use packages such as [Cython](https://cython.org/) and [Numba](http://numba.pydata.org/).\nOur goal is to not reinvent the wheel where appropriate.\n\n### Imperative Experiences\n\nPyTorch is designed to be intuitive, linear in thought, and easy to use.\nWhen you execute a line of code, it gets executed. There isn't an asynchronous view of the world.\nWhen you drop into a debugger or receive error messages and stack traces, understanding them is straightforward.\nThe stack trace points to exactly where your code was defined.\nWe hope you never spend hours debugging your code because of bad stack traces or asynchronous and opaque execution engines.\n\n### Fast and Lean\n\nPyTorch has minimal framework overhead. We integrate acceleration libraries\nsuch as [Intel MKL](https://software.intel.com/mkl) and NVIDIA ([cuDNN](https://developer.nvidia.com/cudnn), [NCCL](https://developer.nvidia.com/nccl)) to maximize speed.\nAt the core, its CPU and GPU Tensor and neural network backends\nare mature and have been tested for years.\n\nHence, PyTorch is quite fast  whether you run small or large neural networks.\n\nThe memory usage in PyTorch is extremely efficient compared to Torch or some of the alternatives.\nWe've written custom memory allocators for the GPU to make sure that\nyour deep learning models are maximally memory efficient.\nThis enables you to train bigger deep learning models than before.\n\n### Extensions Without Pain\n\nWriting new neural network modules, or interfacing with PyTorch's Tensor API was designed to be straightforward\nand with minimal abstractions.\n\nYou can write new neural network layers in Python using the torch API\n[or your favorite NumPy-based libraries such as SciPy](https://pytorch.org/tutorials/advanced/numpy_extensions_tutorial.html).\n\nIf you want to write your layers in C/C++, we provide a convenient extension API that is efficient and with minimal boilerplate.\nNo wrapper code needs to be written. You can see [a tutorial here](https://pytorch.org/tutorials/advanced/cpp_extension.html) and [an example here](https://github.com/pytorch/extension-cpp).\n\n\n## Installation\n\n### Binaries\nCommands to install binaries via Conda or pip wheels are on our website: [https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/)\n\n\n#### NVIDIA Jetson Platforms\n\nPython wheels for NVIDIA's Jetson Nano, Jetson TX1/TX2, Jetson Xavier NX/AGX, and Jetson AGX Orin are provided [here](https://forums.developer.nvidia.com/t/pytorch-for-jetson-version-1-10-now-available/72048) and the L4T container is published [here](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/l4t-pytorch)\n\nThey require JetPack 4.2 and above, and [@dusty-nv](https://github.com/dusty-nv) and [@ptrblck](https://github.com/ptrblck) are maintaining them.\n\n\n### From Source\n\n#### Prerequisites\nIf you are installing from source, you will need:\n- Python 3.9 or later\n- A compiler that fully supports C++17, such as clang or gcc (gcc 9.4.0 or newer is required, on Linux)\n- Visual Studio or Visual Studio Build Tool (Windows only)\n\n\\* PyTorch CI uses Visual C++ BuildTools, which come with Visual Studio Enterprise,\nProfessional, or Community Editions. You can also install the build tools from\nhttps://visualstudio.microsoft.com/visual-cpp-build-tools/. The build tools *do not*\ncome with Visual Studio Code by default.\n\n\\* We highly recommend installing an [Anaconda](https://www.anaconda.com/download) environment. You will get a high-quality BLAS library (MKL) and you get controlled dependency versions regardless of your Linux distro.\n\nAn example of environment setup is shown below:\n\n* Linux:\n\n```bash\n$ source <CONDA_INSTALL_DIR>/bin/activate\n$ conda create -y -n <CONDA_NAME>\n$ conda activate <CONDA_NAME>\n```\n\n* Windows:\n\n```bash\n$ source <CONDA_INSTALL_DIR>\\Scripts\\activate.bat\n$ conda create -y -n <CONDA_NAME>\n$ conda activate <CONDA_NAME>\n$ call \"C:\\Program Files\\Microsoft Visual Studio\\<VERSION>\\Community\\VC\\Auxiliary\\Build\\vcvarsall.bat\" x64\n```\n\n##### NVIDIA CUDA Support\nIf you want to compile with CUDA support, [select a supported version of CUDA from our support matrix](https://pytorch.org/get-started/locally/), then install the following:\n- [NVIDIA CUDA](https://developer.nvidia.com/cuda-downloads)\n- [NVIDIA cuDNN](https://developer.nvidia.com/cudnn) v8.5 or above\n- [Compiler](https://gist.github.com/ax3l/9489132) compatible with CUDA\n\nNote: You could refer to the [cuDNN Support Matrix](https://docs.nvidia.com/deeplearning/cudnn/reference/support-matrix.html) for cuDNN versions with the various supported CUDA, CUDA driver and NVIDIA hardware\n\nIf you want to disable CUDA support, export the environment variable `USE_CUDA=0`.\nOther potentially useful environment variables may be found in `setup.py`.\n\nIf you are building for NVIDIA's Jetson platforms (Jetson Nano, TX1, TX2, AGX Xavier), Instructions to install PyTorch for Jetson Nano are [available here](https://devtalk.nvidia.com/default/topic/1049071/jetson-nano/pytorch-for-jetson-nano/)\n\n##### AMD ROCm Support\nIf you want to compile with ROCm support, install\n- [AMD ROCm](https://rocm.docs.amd.com/en/latest/deploy/linux/quick_start.html) 4.0 and above installation\n- ROCm is currently supported only for Linux systems.\n\nBy default the build system expects ROCm to be installed in `/opt/rocm`. If ROCm is installed in a different directory, the `ROCM_PATH` environment variable must be set to the ROCm installation directory. The build system automatically detects the AMD GPU architecture. Optionally, the AMD GPU architecture can be explicitly set with the `PYTORCH_ROCM_ARCH` environment variable [AMD GPU architecture](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html#supported-gpus)\n\nIf you want to disable ROCm support, export the environment variable `USE_ROCM=0`.\nOther potentially useful environment variables may be found in `setup.py`.\n\n##### Intel GPU Support\nIf you want to compile with Intel GPU support, follow these\n- [PyTorch Prerequisites for Intel GPUs](https://www.intel.com/content/www/us/en/developer/articles/tool/pytorch-prerequisites-for-intel-gpus.html) instructions.\n- Intel GPU is supported for Linux and Windows.\n\nIf you want to disable Intel GPU support, export the environment variable `USE_XPU=0`.\nOther potentially useful environment variables may be found in `setup.py`.\n\n#### Get the PyTorch Source\n```bash\ngit clone --recursive https://github.com/pytorch/pytorch\ncd pytorch\n# if you are updating an existing checkout\ngit submodule sync\ngit submodule update --init --recursive\n```\n\n#### Install Dependencies\n\n**Common**\n\n```bash\nconda install cmake ninja\n# Run this command from the PyTorch directory after cloning the source code using the Get the PyTorch Source section below\npip install -r requirements.txt\n```\n\n**On Linux**\n\n```bash\npip install mkl-static mkl-include\n# CUDA only: Add LAPACK support for the GPU if needed\nconda install -c pytorch magma-cuda121  # or the magma-cuda* that matches your CUDA version from https://anaconda.org/pytorch/repo\n\n# (optional) If using torch.compile with inductor/triton, install the matching version of triton\n# Run from the pytorch directory after cloning\n# For Intel GPU support, please explicitly `export USE_XPU=1` before running command.\nmake triton\n```\n\n**On MacOS**\n\n```bash\n# Add this package on intel x86 processor machines only\npip install mkl-static mkl-include\n# Add these packages if torch.distributed is needed\nconda install pkg-config libuv\n```\n\n**On Windows**\n\n```bash\npip install mkl-static mkl-include\n# Add these packages if torch.distributed is needed.\n# Distributed package support on Windows is a prototype feature and is subject to changes.\nconda install -c conda-forge libuv=1.39\n```\n\n#### Install PyTorch\n**On Linux**\n\nIf you would like to compile PyTorch with [new C++ ABI](https://gcc.gnu.org/onlinedocs/libstdc++/manual/using_dual_abi.html) enabled, then first run this command:\n```bash\nexport _GLIBCXX_USE_CXX11_ABI=1\n```\n\nPlease **note** that starting from PyTorch 2.5, the PyTorch build with XPU supports both new and old C++ ABIs. Previously, XPU only supported the new C++ ABI. If you want to compile with Intel GPU support, please follow [Intel GPU Support](#intel-gpu-support).\n\nIf you're compiling for AMD ROCm then first run this command:\n```bash\n# Only run this if you're compiling for ROCm\npython tools/amd_build/build_amd.py\n```\n\nInstall PyTorch\n```bash\nexport CMAKE_PREFIX_PATH=\"${CONDA_PREFIX:-'$(dirname $(which conda))/../'}:${CMAKE_PREFIX_PATH}\"\npython setup.py develop\n```\n\n**On macOS**\n\n```bash\npython3 setup.py develop\n```\n\n**On Windows**\n\nIf you want to build legacy python code, please refer to [Building on legacy code and CUDA](https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#building-on-legacy-code-and-cuda)\n\n**CPU-only builds**\n\nIn this mode PyTorch computations will run on your CPU, not your GPU\n\n```cmd\npython setup.py develop\n```\n\nNote on OpenMP: The desired OpenMP implementation is Intel OpenMP (iomp). In order to link against iomp, you'll need to manually download the library and set up the building environment by tweaking `CMAKE_INCLUDE_PATH` and `LIB`. The instruction [here](https://github.com/pytorch/pytorch/blob/main/docs/source/notes/windows.rst#building-from-source) is an example for setting up both MKL and Intel OpenMP. Without these configurations for CMake, Microsoft Visual C OpenMP runtime (vcomp) will be used.\n\n**CUDA based build**\n\nIn this mode PyTorch computations will leverage your GPU via CUDA for faster number crunching\n\n[NVTX](https://docs.nvidia.com/gameworks/content/gameworkslibrary/nvtx/nvidia_tools_extension_library_nvtx.htm) is needed to build Pytorch with CUDA.\nNVTX is a part of CUDA distributive, where it is called \"Nsight Compute\". To install it onto an already installed CUDA run CUDA installation once again and check the corresponding checkbox.\nMake sure that CUDA with Nsight Compute is installed after Visual Studio.\n\nCurrently, VS 2017 / 2019, and Ninja are supported as the generator of CMake. If `ninja.exe` is detected in `PATH`, then Ninja will be used as the default generator, otherwise, it will use VS 2017 / 2019.\n<br/> If Ninja is selected as the generator, the latest MSVC will get selected as the underlying toolchain.\n\nAdditional libraries such as\n[Magma](https://developer.nvidia.com/magma), [oneDNN, a.k.a. MKLDNN or DNNL](https://github.com/oneapi-src/oneDNN), and [Sccache](https://github.com/mozilla/sccache) are often needed. Please refer to the [installation-helper](https://github.com/pytorch/pytorch/tree/main/.ci/pytorch/win-test-helpers/installation-helpers) to install them.\n\nYou can refer to the [build_pytorch.bat](https://github.com/pytorch/pytorch/blob/main/.ci/pytorch/win-test-helpers/build_pytorch.bat) script for some other environment variables configurations\n\n\n```cmd\ncmd\n\n:: Set the environment variables after you have downloaded and unzipped the mkl package,\n:: else CMake would throw an error as `Could NOT find OpenMP`.\nset CMAKE_INCLUDE_PATH={Your directory}\\mkl\\include\nset LIB={Your directory}\\mkl\\lib;%LIB%\n\n:: Read the content in the previous section carefully before you proceed.\n:: [Optional] If you want to override the underlying toolset used by Ninja and Visual Studio with CUDA, please run the following script block.\n:: \"Visual Studio 2019 Developer Command Prompt\" will be run automatically.\n:: Make sure you have CMake >= 3.12 before you do this when you use the Visual Studio generator.\nset CMAKE_GENERATOR_TOOLSET_VERSION=14.27\nset DISTUTILS_USE_SDK=1\nfor /f \"usebackq tokens=*\" %i in (`\"%ProgramFiles(x86)%\\Microsoft Visual Studio\\Installer\\vswhere.exe\" -version [15^,17^) -products * -latest -property installationPath`) do call \"%i\\VC\\Auxiliary\\Build\\vcvarsall.bat\" x64 -vcvars_ver=%CMAKE_GENERATOR_TOOLSET_VERSION%\n\n:: [Optional] If you want to override the CUDA host compiler\nset CUDAHOSTCXX=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.27.29110\\bin\\HostX64\\x64\\cl.exe\n\npython setup.py develop\n\n```\n\n##### Adjust Build Options (Optional)\n\nYou can adjust the configuration of cmake variables optionally (without building first), by doing\nthe following. For example, adjusting the pre-detected directories for CuDNN or BLAS can be done\nwith such a step.\n\nOn Linux\n```bash\nexport CMAKE_PREFIX_PATH=\"${CONDA_PREFIX:-'$(dirname $(which conda))/../'}:${CMAKE_PREFIX_PATH}\"\npython setup.py build --cmake-only\nccmake build  # or cmake-gui build\n```\n\nOn macOS\n```bash\nexport CMAKE_PREFIX_PATH=\"${CONDA_PREFIX:-'$(dirname $(which conda))/../'}:${CMAKE_PREFIX_PATH}\"\nMACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ python setup.py build --cmake-only\nccmake build  # or cmake-gui build\n```\n\n### Docker Image\n\n#### Using pre-built images\n\nYou can also pull a pre-built docker image from Docker Hub and run with docker v19.03+\n\n```bash\ndocker run --gpus all --rm -ti --ipc=host pytorch/pytorch:latest\n```\n\nPlease note that PyTorch uses shared memory to share data between processes, so if torch multiprocessing is used (e.g.\nfor multithreaded data loaders) the default shared memory segment size that container runs with is not enough, and you\nshould increase shared memory size either with `--ipc=host` or `--shm-size` command line options to `nvidia-docker run`.\n\n#### Building the image yourself\n\n**NOTE:** Must be built with a docker version > 18.06\n\nThe `Dockerfile` is supplied to build images with CUDA 11.1 support and cuDNN v8.\nYou can pass `PYTHON_VERSION=x.y` make variable to specify which Python version is to be used by Miniconda, or leave it\nunset to use the default.\n\n```bash\nmake -f docker.Makefile\n# images are tagged as docker.io/${your_docker_username}/pytorch\n```\n\nYou can also pass the `CMAKE_VARS=\"...\"` environment variable to specify additional CMake variables to be passed to CMake during the build.\nSee [setup.py](./setup.py) for the list of available variables.\n\n```bash\nmake -f docker.Makefile\n```\n\n### Building the Documentation\n\nTo build documentation in various formats, you will need [Sphinx](http://www.sphinx-doc.org) and the\nreadthedocs theme.\n\n```bash\ncd docs/\npip install -r requirements.txt\nmake html\nmake serve\n```\n\nRun `make` to get a list of all available output formats.\n\nIf you get a katex error run `npm install katex`.  If it persists, try\n`npm install -g katex`\n\n> Note: if you installed `nodejs` with a different package manager (e.g.,\n`conda`) then `npm` will probably install a version of `katex` that is not\ncompatible with your version of `nodejs` and doc builds will fail.\nA combination of versions that is known to work is `node@6.13.1` and\n`katex@0.13.18`. To install the latter with `npm` you can run\n```npm install -g katex@0.13.18```\n\n### Previous Versions\n\nInstallation instructions and binaries for previous PyTorch versions may be found\non [our website](https://pytorch.org/previous-versions).\n\n\n## Getting Started\n\nThree-pointers to get you started:\n- [Tutorials: get you started with understanding and using PyTorch](https://pytorch.org/tutorials/)\n- [Examples: easy to understand PyTorch code across all domains](https://github.com/pytorch/examples)\n- [The API Reference](https://pytorch.org/docs/)\n- [Glossary](https://github.com/pytorch/pytorch/blob/main/GLOSSARY.md)\n\n## Resources\n\n* [PyTorch.org](https://pytorch.org/)\n* [PyTorch Tutorials](https://pytorch.org/tutorials/)\n* [PyTorch Examples](https://github.com/pytorch/examples)\n* [PyTorch Models](https://pytorch.org/hub/)\n* [Intro to Deep Learning with PyTorch from Udacity](https://www.udacity.com/course/deep-learning-pytorch--ud188)\n* [Intro to Machine Learning with PyTorch from Udacity](https://www.udacity.com/course/intro-to-machine-learning-nanodegree--nd229)\n* [Deep Neural Networks with PyTorch from Coursera](https://www.coursera.org/learn/deep-neural-networks-with-pytorch)\n* [PyTorch Twitter](https://twitter.com/PyTorch)\n* [PyTorch Blog](https://pytorch.org/blog/)\n* [PyTorch YouTube](https://www.youtube.com/channel/UCWXI5YeOsh03QvJ59PMaXFw)\n\n## Communication\n* Forums: Discuss implementations, research, etc. https://discuss.pytorch.org\n* GitHub Issues: Bug reports, feature requests, install issues, RFCs, thoughts, etc.\n* Slack: The [PyTorch Slack](https://pytorch.slack.com/) hosts a primary audience of moderate to experienced PyTorch users and developers for general chat, online discussions, collaboration, etc. If you are a beginner looking for help, the primary medium is [PyTorch Forums](https://discuss.pytorch.org). If you need a slack invite, please fill this form: https://goo.gl/forms/PP1AGvNHpSaJP8to1\n* Newsletter: No-noise, a one-way email newsletter with important announcements about PyTorch. You can sign-up here: https://eepurl.com/cbG0rv\n* Facebook Page: Important announcements about PyTorch. https://www.facebook.com/pytorch\n* For brand guidelines, please visit our website at [pytorch.org](https://pytorch.org/)\n\n## Releases and Contributing\n\nTypically, PyTorch has three minor releases a year. Please let us know if you encounter a bug by [filing an issue](https://github.com/pytorch/pytorch/issues).\n\nWe appreciate all contributions. If you are planning to contribute back bug-fixes, please do so without any further discussion.\n\nIf you plan to contribute new features, utility functions, or extensions to the core, please first open an issue and discuss the feature with us.\nSending a PR without discussion might end up resulting in a rejected PR because we might be taking the core in a different direction than you might be aware of.\n\nTo learn more about making a contribution to Pytorch, please see our [Contribution page](CONTRIBUTING.md). For more information about PyTorch releases, see [Release page](RELEASE.md).\n\n## The Team\n\nPyTorch is a community-driven project with several skillful engineers and researchers contributing to it.\n\nPyTorch is currently maintained by [Soumith Chintala](http://soumith.ch), [Gregory Chanan](https://github.com/gchanan), [Dmytro Dzhulgakov](https://github.com/dzhulgakov), [Edward Yang](https://github.com/ezyang), and [Nikita Shulga](https://github.com/malfet) with major contributions coming from hundreds of talented individuals in various forms and means.\nA non-exhaustive but growing list needs to mention: [Trevor Killeen](https://github.com/killeent), [Sasank Chilamkurthy](https://github.com/chsasank), [Sergey Zagoruyko](https://github.com/szagoruyko), [Adam Lerer](https://github.com/adamlerer), [Francisco Massa](https://github.com/fmassa), [Alykhan Tejani](https://github.com/alykhantejani), [Luca Antiga](https://github.com/lantiga), [Alban Desmaison](https://github.com/albanD), [Andreas Koepf](https://github.com/andreaskoepf), [James Bradbury](https://github.com/jamesb93), [Zeming Lin](https://github.com/ebetica), [Yuandong Tian](https://github.com/yuandong-tian), [Guillaume Lample](https://github.com/glample), [Marat Dukhan](https://github.com/Maratyszcza), [Natalia Gimelshein](https://github.com/ngimel), [Christian Sarofeen](https://github.com/csarofeen), [Martin Raison](https://github.com/martinraison), [Edward Yang](https://github.com/ezyang), [Zachary Devito](https://github.com/zdevito).\n\nNote: This project is unrelated to [hughperkins/pytorch](https://github.com/hughperkins/pytorch) with the same name. Hugh is a valuable contributor to the Torch community and has helped with many things Torch and PyTorch.\n\n## License\n\nPyTorch has a BSD-style license, as found in the [LICENSE](LICENSE) file.\n"
        },
        {
          "name": "RELEASE.md",
          "type": "blob",
          "size": 29.5,
          "content": "# Releasing PyTorch\n\n<!-- toc -->\n\n  - [Release Compatibility Matrix](#release-compatibility-matrix)\n  - [Release Cadence](#release-cadence)\n  - [General Overview](#general-overview)\n    - [Frequently Asked Questions](#frequently-asked-questions)\n  - [Cutting a release branch preparations](#cutting-a-release-branch-preparations)\n  - [Cutting release branches](#cutting-release-branches)\n    - [`pytorch/pytorch`](#pytorchpytorch)\n    - [`pytorch/builder` / PyTorch domain libraries](#pytorchbuilder--pytorch-domain-libraries)\n    - [Making release branch specific changes for PyTorch](#making-release-branch-specific-changes-for-pytorch)\n    - [Making release branch specific changes for domain libraries](#making-release-branch-specific-changes-for-domain-libraries)\n  - [Running Launch Execution team Core XFN sync](#running-launch-execution-team-core-xfn-sync)\n  - [Drafting RCs (Release Candidates) for PyTorch and domain libraries](#drafting-rcs-release-candidates-for-pytorch-and-domain-libraries)\n    - [Release Candidate Storage](#release-candidate-storage)\n    - [Release Candidate health validation](#release-candidate-health-validation)\n    - [Cherry Picking Fixes](#cherry-picking-fixes)\n      - [How to do Cherry Picking](#how-to-do-cherry-picking)\n    - [Cherry Picking Reverts](#cherry-picking-reverts)\n  - [Preparing and Creating Final Release candidate](#preparing-and-creating-final-release-candidate)\n  - [Promoting RCs to Stable](#promoting-rcs-to-stable)\n  - [Additional Steps to prepare for release day](#additional-steps-to-prepare-for-release-day)\n    - [Modify release matrix](#modify-release-matrix)\n    - [Open Google Colab issue](#open-google-colab-issue)\n- [Patch Releases](#patch-releases)\n  - [Patch Release Criteria](#patch-release-criteria)\n  - [Patch Release Process](#patch-release-process)\n    - [Patch Release Process Description](#patch-release-process-description)\n    - [Triage](#triage)\n    - [Issue Tracker for Patch releases](#issue-tracker-for-patch-releases)\n    - [Building a release schedule / cherry picking](#building-a-release-schedule--cherry-picking)\n    - [Building Binaries / Promotion to Stable](#building-binaries--promotion-to-stable)\n- [Hardware / Software Support in Binary Build Matrix](#hardware--software-support-in-binary-build-matrix)\n  - [Python](#python)\n  - [Accelerator Software](#accelerator-software)\n    - [Special support cases](#special-support-cases)\n  - [Operating Systems](#operating-systems)\n- [Submitting Tutorials](#submitting-tutorials)\n- [Special Topics](#special-topics)\n  - [Updating submodules for a release](#updating-submodules-for-a-release)\n  - [Triton dependency for the release](#triton-dependency-for-the-release)\n\n<!-- tocstop -->\n\n## Release Compatibility Matrix\n\nFollowing is the Release Compatibility Matrix for PyTorch releases:\n\n| PyTorch version | Python | C++ | Stable CUDA | Experimental CUDA | Stable ROCm |\n| --- | --- | --- | --- | --- | --- |\n| 2.6 | >=3.9, <=3.13, (3.13t experimental) | C++17 | CUDA 11.8, CUDA 12.4 (CUDNN 9.1.0.70), CUDA 12.6 (CUDNN 9.5.1.17) | None | ROCm 6.2.4 |\n| 2.5 | >=3.9, <=3.12, (3.13 experimental) | C++17 | CUDA 11.8, CUDA 12.1, CUDA 12.4, CUDNN 9.1.0.70  | None | ROCm 6.2 |\n| 2.4 | >=3.8, <=3.12 | C++17 | CUDA 11.8, CUDA 12.1, CUDNN 9.1.0.70  | CUDA 12.4, CUDNN 9.1.0.70 | ROCm 6.1 |\n| 2.3 | >=3.8, <=3.11, (3.12 experimental) | C++17 | CUDA 11.8, CUDNN 8.7.0.84 | CUDA 12.1, CUDNN 8.9.2.26 | ROCm 6.0 |\n| 2.2 | >=3.8, <=3.11, (3.12 experimental) | C++17 | CUDA 11.8, CUDNN 8.7.0.84 | CUDA 12.1, CUDNN 8.9.2.26 | ROCm 5.7 |\n| 2.1 | >=3.8, <=3.11 | C++17 | CUDA 11.8, CUDNN 8.7.0.84 | CUDA 12.1, CUDNN 8.9.2.26 | ROCm 5.6 |\n| 2.0 | >=3.8, <=3.11 | C++14 | CUDA 11.7, CUDNN 8.5.0.96 | CUDA 11.8, CUDNN 8.7.0.84 | ROCm 5.4 |\n| 1.13 | >=3.7, <=3.10 | C++14 | CUDA 11.6, CUDNN 8.3.2.44 | CUDA 11.7, CUDNN 8.5.0.96 | ROCm 5.2 |\n| 1.12 | >=3.7, <=3.10 | C++14 | CUDA 11.3, CUDNN 8.3.2.44 | CUDA 11.6, CUDNN 8.3.2.44 | ROCm 5.0 |\n\n## Release Cadence\n\nFollowing is the release cadence for year 2023/2024. All dates below are tentative, for latest updates on the release scheduled please follow [dev discuss](https://dev-discuss.pytorch.org/c/release-announcements/27). Please note: Patch Releases are optional.\n\n| Minor Version | Release branch cut | Release date | First patch release date | Second patch release date|\n| --- | --- | --- | --- | --- |\n| 2.1 | Aug 2023 | Oct 2023 | Nov 2023 | Dec 2023 |\n| 2.2 | Dec 2023 | Jan 2024 | Feb 2024 | Mar 2024 |\n| 2.3 | Mar 2024 | Apr 2024 | Jun 2024 | Not planned |\n| 2.4 | Jun 2024 | Jul 2024 | (Sept 2024) | Not planned |\n| 2.5 | Sep 2024 | Oct 2024 | (Nov 2024) | (Dec 2024) |\n| 2.6 | Dec 2024 | Jan 2025 | (Feb 2025) | (Mar 2025) |\n| 2.7 | Mar 2025 | Apr 2025 | (May 2025) | (Jun 2025) |\n| 2.8 | Jun 2025 | Jul 2025 | (Aug 2025) | (Sep 2025) |\n| 2.9 | Aug 2025 | Oct 2025 | (Nov 2025) | (Dec 2025) |\n\n## General Overview\n\nReleasing a new version of PyTorch generally entails 3 major steps:\n\n0. Cutting a release branch preparations\n1. Cutting a release branch and making release branch specific changes\n2. Drafting RCs (Release Candidates), and merging cherry picks\n3. Preparing and Creating Final Release Candidate\n4. Promoting Final RC to stable and performing release day tasks\n\n### Frequently Asked Questions\n\n* Q: What is release branch cut  ?\n  * A: When bulk of the tracked features merged into the main branch, the primary release engineer starts the release process of cutting the release branch by creating a new git branch based off of the current `main` development branch of PyTorch. This allows PyTorch development flow on `main` to continue uninterrupted, while the release engineering team focuses on stabilizing the release branch in order to release a series of release candidates (RC). The activities in the release branch include both regression and performance testing as well as polishing new features and fixing release-specific bugs. In general, new features *are not* added to the release branch after it was created.\n\n* Q: What is cherry-pick ?\n  * A: A cherry pick is a process of propagating commits from the main into the release branch, utilizing git's built in [cherry-pick feature](https://git-scm.com/docs/git-cherry-pick). These commits are typically limited to small fixes or documentation updates to ensure that the release engineering team has sufficient time to complete a thorough round of testing on the release branch. To nominate a fix for cherry-picking, a separate pull request must be created against the respective release branch and then mentioned in the Release Tracker issue (example: https://github.com/pytorch/pytorch/issues/94937) following the template from the issue description. The comment nominating a particular cherry-pick for inclusion in the release should include the committed PR against main branch, the newly created cherry-pick PR, as well as the acceptance criteria for why the cherry-pick is needed in the first place.\n\n## Cutting a release branch preparations\n\nFollowing Requirements needs to be met prior to cutting a release branch:\n\n* Resolve all outstanding issues in the milestones(for example [1.11.0](https://github.com/pytorch/pytorch/milestone/28))before first RC cut is completed. After RC cut is completed following script should be executed from builder repo in order to validate the presence of the fixes in the release branch :\n``` python github_analyze.py --repo-path ~/local/pytorch --remote upstream --branch release/1.11 --milestone-id 26 --missing-in-branch ```\n* Validate that all new workflows have been created in the PyTorch and domain libraries included in the release. Validate it against all dimensions of release matrix, including operating systems(Linux, MacOS, Windows), Python versions as well as CPU architectures(x86 and arm) and accelerator versions(CUDA, ROCm).\n* All the nightly jobs for pytorch and domain libraries should be green. Validate this using following HUD links:\n  * [Pytorch](https://hud.pytorch.org/hud/pytorch/pytorch/nightly)\n  * [TorchVision](https://hud.pytorch.org/hud/pytorch/vision/nightly)\n  * [TorchAudio](https://hud.pytorch.org/hud/pytorch/audio/nightly)\n\n## Cutting release branches\n\n### `pytorch/pytorch`\n\nRelease branches are typically cut from the branch [`viable/strict`](https://github.com/pytorch/pytorch/tree/viable/strict) as to ensure that tests are passing on the release branch.\n\nThere's a convenience script to create release branches from current `viable/strict`. Perform following actions :\n* Perform a fresh clone of pytorch repo using\n```bash\ngit clone git@github.com:pytorch/pytorch.git\n```\n\n* Execute following command from PyTorch repository root folder:\n```bash\nDRY_RUN=disabled scripts/release/cut-release-branch.sh\n```\nThis script should create 2 branches:\n* `release/{MAJOR}.{MINOR}`\n* `orig/release/{MAJOR}.{MINOR}`\n\n### `pytorch/builder` / PyTorch domain libraries\n\n*Note*:  Release branches for individual domain libraries should be created after first release candidate build of PyTorch is available in staging channels (which happens about a week after PyTorch release branch has been created). This is absolutely required to allow sufficient testing time for each of the domain library. Domain libraries branch cut is performed by Domain Library POC.\nBuilder branch cut should be performed at the same time as Pytorch core branch cut. Convenience script can also be used domains as well as `pytorch/builder`\n\n> NOTE: RELEASE_VERSION only needs to be specified if version.txt is not available in root directory\n\n```bash\nDRY_RUN=disabled GIT_BRANCH_TO_CUT_FROM=main RELEASE_VERSION=1.11 scripts/release/cut-release-branch.sh\n```\n\n### Making release branch specific changes for PyTorch\n\nThese are examples of changes that should be made to release branches so that CI / tooling can function normally on\nthem:\n\n* Update backwards compatibility tests to use RC binaries instead of nightlies\n  * Example: https://github.com/pytorch/pytorch/pull/77983 and https://github.com/pytorch/pytorch/pull/77986\n* A release branches should also be created in [`pytorch/xla`](https://github.com/pytorch/xla) and [`pytorch/builder`](https://github.com/pytorch/builder) repos and pinned in `pytorch/pytorch`\n  * Example: https://github.com/pytorch/pytorch/pull/86290 and https://github.com/pytorch/pytorch/pull/90506\n* Update branch used in composite actions from trunk to release (for example, can be done by running `for i in .github/workflows/*.yml; do sed -i -e s#@main#@release/2.0# $i; done`\n  * Example: https://github.com/pytorch/pytorch/commit/17f400404f2ca07ea5ac864428e3d08149de2304\n\nThese are examples of changes that should be made to the *default* branch after a release branch is cut\n\n* Nightly versions should be updated in all version files to the next MINOR release (i.e. 0.9.0 -> 0.10.0) in the default branch:\n  * Example: https://github.com/pytorch/pytorch/pull/77984\n\n### Making release branch specific changes for domain libraries\n\nDomain library branch cut is done a week after branch cut for the `pytorch/pytorch`. The branch cut is performed by the Domain Library POC.\nAfter the branch cut is performed, the Pytorch Dev Infra member should be informed of the branch cut and Domain Library specific change is required before Drafting RC for this domain library.\n\nFollow these examples of PR that updates the version and sets RC Candidate upload channel:\n* torchvision : https://github.com/pytorch/vision/pull/5400\n* torchaudio: https://github.com/pytorch/audio/pull/2210\n\n## Running Launch Execution team Core XFN sync\n\nThe series of meetings for Core XFN sync should be organized. The goal of these meetings are the following:\n1. Establish release POC's from each of the workstreams\n2. Cover the tactical phase of releasing minor releases to the market\n3. Discuss possible release blockers\n\nFollowing POC's should be assigned from each of the workstreams:\n* Core/Marketing\n* Release Eng\n* Doc Eng\n* Release notes\n* Partner\n\n**NOTE**: The meetings should start after the release branch is created and should continue until the week of the release.\n\n## Drafting RCs (Release Candidates) for PyTorch and domain libraries\n\nTo draft RCs, a user with the necessary permissions can push a git tag to the main `pytorch/pytorch` git repository. Please note: exactly same process is used for each of the domain library\n\nThe git tag for a release candidate must follow the following format:\n```\nv{MAJOR}.{MINOR}.{PATCH}-rc{RC_NUMBER}\n```\n\nAn example of this would look like:\n```\nv1.12.0-rc1\n```\nYou can use following commands to perform tag from pytorch core repo (not fork):\n* Checkout and validate the repo history before tagging\n```\ngit checkout release/1.12\ngit log --oneline\n```\n* Perform tag and push it to github (this will trigger the binary release build)\n```\ngit tag -f  v1.12.0-rc2\ngit push origin  v1.12.0-rc2\n```\n\nPushing a release candidate should trigger the `binary_builds` workflow within CircleCI using [`pytorch/pytorch-probot`](https://github.com/pytorch/pytorch-probot)'s [`trigger-circleci-workflows`](trigger-circleci-workflows) functionality.\n\nThis trigger functionality is configured here: [`pytorch-circleci-labels.yml`](https://github.com/pytorch/pytorch/blob/main/.github/pytorch-circleci-labels.yml)\n\nTo view the state of the release build, please navigate to [HUD](https://hud.pytorch.org/hud/pytorch/pytorch/release%2F1.12). And make sure all binary builds are successful.\n### Release Candidate Storage\n\nRelease candidates are currently stored in the following places:\n\n* Wheels: https://download.pytorch.org/whl/test/\n* Conda: https://anaconda.org/pytorch-test\n* Libtorch: https://download.pytorch.org/libtorch/test\n\nBackups are stored in a non-public S3 bucket at [`s3://pytorch-backup`](https://s3.console.aws.amazon.com/s3/buckets/pytorch-backup?region=us-east-1&tab=objects)\n\n### Release Candidate health validation\n\nValidate the release jobs for pytorch and domain libraries should be green. Validate this using following HUD links:\n  * [Pytorch](https://hud.pytorch.org/hud/pytorch/pytorch/release%2F1.12)\n  * [TorchVision](https://hud.pytorch.org/hud/pytorch/vision/release%2F1.12)\n  * [TorchAudio](https://hud.pytorch.org/hud/pytorch/audio/release%2F1.12)\n\nValidate that the documentation build has completed and generated entry corresponding to the release in  [docs repository](https://github.com/pytorch/docs/tree/main/).\n\n### Cherry Picking Fixes\n\nTypically, within a release cycle fixes are necessary for regressions, test fixes, etc.\n\nFor fixes that are to go into a release after the release branch has been cut we typically employ the use of a cherry pick tracker.\n\nAn example of this would look like:\n* https://github.com/pytorch/pytorch/issues/128436\n\nPlease also make sure to add milestone target to the PR/issue, especially if it needs to be considered for inclusion into the dot release.\n\n**NOTE**: The cherry pick process is not an invitation to add new features, it is mainly there to fix regressions\n\n#### How to do Cherry Picking\n\nYou can now use `pytorchbot` to cherry pick a PyTorch PR that has been committed\nto the main branch using `@pytorchbot cherry-pick` command as follows (make sure\nthat the cherry-pick tracker issue for the target release labelled as \"release tracker\" -\nthis will allow the bot to find it and post comments).\n\n```\nusage: @pytorchbot cherry-pick --onto ONTO [--fixes FIXES] -c\n                               {regression,critical,fixnewfeature,docs,release}\n\nCherry pick a pull request onto a release branch for inclusion in a release\n\noptional arguments:\n  --onto ONTO           Branch you would like to cherry pick onto (Example: release/2.2)\n  --fixes FIXES         Link to the issue that your PR fixes (i.e. https://github.com/pytorch/pytorch/issues/110666)\n  -c {regression,critical,fixnewfeature,docs,release}\n                        A machine-friendly classification of the cherry-pick reason.\n```\n\nFor example, [#120567](https://github.com/pytorch/pytorch/pull/120567#issuecomment-1978964376)\ncreated a cherry pick PR [#121232](https://github.com/pytorch/pytorch/pull/121232) onto `release/2.2`\nbranch to fix a regression issue. You can then refer to the original\nand the cherry-picked PRs on the release tracker issue. Please note\nthat the cherry-picked PR will still need to be reviewed by PyTorch\nRelEng team before it can go into the release branch. This feature\nrequires `pytorchbot`, so it's only available in PyTorch atm.\n\n### Cherry Picking Reverts\n\nIf PR that has been cherry-picked into release branch has been reverted, its cherry-pick must be reverted as well.\n\nReverts for changes that was committed into the main branch prior to the branch cut, must be propagated into release branch as well.\n\n## Preparing and Creating Final Release candidate\n\nThe following requirements need to be met prior to creating final Release Candidate :\n\n* Resolve all outstanding open issues in the milestone. There should be no open issues/PRs (for example [2.1.2](https://github.com/pytorch/pytorch/milestone/39)). The issue should either be closed or de-milestoned.\n\n* Validate that all closed milestone PRs are present in the release branch. Confirm this by running:\n``` python github_analyze.py --repo-path ~/local/pytorch --remote upstream --branch release/2.2 --milestone-id 40 --missing-in-branch ```\n\n* No outstanding cherry-picks that need to be reviewed in the issue tracker: https://github.com/pytorch/pytorch/issues/115300\n\n* Perform [Release Candidate health validation](#release-candidate-health-validation). CI should have the green signal.\n\nAfter the final RC is created. The following tasks should be performed :\n\n* Perform [Release Candidate health validation](#release-candidate-health-validation). CI should have the green signal.\n\n* Run and inspect the output [Validate Binaries](https://github.com/pytorch/builder/actions/workflows/validate-binaries.yml) workflow.\n\n* All the closed issues from [milestone](https://github.com/pytorch/pytorch/milestone/39) need to be validated. Confirm the validation by commenting on the issue: https://github.com/pytorch/pytorch/issues/113568#issuecomment-1851031064\n\n* Create validation issue for the release, see for example [Validations for 2.1.2 release](https://github.com/pytorch/pytorch/issues/114904) and perform required validations.\n\n* Run performance tests in [benchmark repository](https://github.com/pytorch/benchmark). Make sure there are no performance regressions.\n\n* Prepare and stage PyPI binaries for promotion. This is done with this script:\n[`pytorch/builder:release/pypi/promote_pypi_to_staging.sh`](https://github.com/pytorch/builder/blob/main/release/pypi/promote_pypi_to_staging.sh)\n\n* Validate staged PyPI binaries. Make sure generated packages are correct and package size does not exceeds maximum allowed PyPI package size.\n\n## Promoting RCs to Stable\n\nPromotion of RCs to stable is done with this script:\n[`pytorch/builder:release/promote.sh`](https://github.com/pytorch/builder/blob/main/release/promote.sh)\n\nUsers of that script should take care to update the versions necessary for the specific packages you are attempting to promote.\n\nPromotion should occur in two steps:\n* Promote S3 artifacts (wheels, libtorch) and Conda packages\n* Promote S3 wheels to PyPI\n\n**NOTE**: The promotion of wheels to PyPI can only be done once so take caution when attempting to promote wheels to PyPI, (see https://github.com/pypa/warehouse/issues/726 for a discussion on potential draft releases within PyPI)\n\n## Additional Steps to prepare for release day\n\nThe following should be prepared for the release day\n\n### Modify release matrix\n\nNeed to modify release matrix for get started page. See following [PR](https://github.com/pytorch/test-infra/pull/4611) as reference.\n\nThe PR to update published_versions.json and quick-start-module.js is auto generated. See following [PR](https://github.com/pytorch/pytorch.github.io/pull/1467) as reference.\n\nPlease note: This PR needs to be merged on the release day and hence it should be absolutely free of any failures. To test this PR, open another test PR but pointing to the Release candidate location as above [Release Candidate Storage](RELEASE.md#release-candidate-storage)\n\n### Open Google Colab issue\n\nThis is normally done right after the release is completed. We would need to create Google Colab Issue see following [PR](https://github.com/googlecolab/colabtools/issues/2372)\n\n# Patch Releases\n\nA patch release is a maintenance release of PyTorch that includes fixes for regressions found in a previous minor release. Patch releases typically will bump the `patch` version from semver (i.e. `[major].[minor].[patch]`).\n\nPlease note: Starting from 2.1 one can expect up to 2 patch releases after every minor ones. Patch releases would only be published for latest minor release.\n\n## Patch Release Criteria\n\nPatch releases should be considered if a regression meets the following criteria:\n\n1. Does the regression break core functionality (stable / beta features) including functionality in first party domain libraries?\n    * First party domain libraries:\n        * [pytorch/vision](https://github.com/pytorch/vision)\n        * [pytorch/audio](https://github.com/pytorch/audio)\n3. Is there not a viable workaround?\n    * Can the regression be solved simply or is it not overcomable?\n\n> *NOTE*: Patch releases should only be considered when functionality is broken, documentation does not typically fall within this category\n\n## Patch Release Process\n\n### Patch Release Process Description\n\n> Main POC: Patch Release Managers, Triage Reviewers\n\nPatch releases should follow these high-level phases. This process starts immediately after the previous release has completed.\nPatch release process takes around 4-5 weeks to complete.\n\n1. Triage, is a process where issues are identified, graded, compared to Patch Release Criteria and added to Patch Release milestone. This process normally takes 2 weeks after the release completion.\n2. Go/No Go meeting between PyTorch Releng, PyTorch Core and Project Managers where potential issues triggering a release in milestones are reviewed, and following decisions are made:\n  * Should the new patch Release be created ?\n  * Timeline execution for the patch release\n3. Cherry picking phase starts after the decision is made to create patch release. At this point a new release tracker for the patch release is created, and an announcement will be made on official channels [example announcement](https://dev-discuss.pytorch.org/t/pytorch-release-2-0-1-important-information/1176). The authors of the fixes to regressions will be asked to create their own cherry picks. This process normally takes 2 weeks.\n4. Building Binaries, Promotion to Stable and testing. After all cherry picks have been merged, Release Managers trigger new build and produce new release candidate. Announcement is made on the official channel about the RC availability at this point. This process normally takes 2 weeks.\n5. General Availability\n\n### Triage\n\n> Main POC: Triage Reviewers\n\n1. Tag issues / pull requests that are candidates for a potential patch release with `triage review`\n    * ![adding triage review label](https://user-images.githubusercontent.com/1700823/132589089-a9210a14-6159-409d-95e5-f79067f6fa38.png)\n2. Triage reviewers will then check if the regression / fix identified fits within above mentioned [Patch Release Criteria](#patch-release-criteria)\n3. Triage reviewers will then add the issue / pull request to the related milestone (i.e. `1.9.1`) if the regressions is found to be within the [Patch Release Criteria](#patch-release-criteria)\n    * ![adding to milestone](https://user-images.githubusercontent.com/1700823/131175980-148ff38d-44c3-4611-8a1f-cd2fd1f4c49d.png)\n\n### Issue Tracker for Patch releases\n\nFor patch releases issue tracker needs to be created. For patch release, we require all cherry-pick changes to have links to either a high-priority GitHub issue or a CI failure from previous RC. An example of this would look like:\n* https://github.com/pytorch/pytorch/issues/128436\n\nOnly following issues are accepted:\n1. Fixes to regressions against previous major version (e.g. regressions introduced in 1.13.0 from 1.12.0 are pickable for 1.13.1)\n2. Low risk critical fixes for: silent correctness, backwards compatibility, crashes, deadlocks, (large) memory leaks\n3. Fixes to new features being introduced in this release\n4. Documentation improvements\n5. Release branch specific changes (e.g. blocking ci fixes, change version identifiers)\n\n### Building a release schedule / cherry picking\n\n> Main POC: Patch Release Managers\n\n1. After regressions / fixes have been triaged Patch Release Managers will work together and build /announce a schedule for the patch release\n    * *NOTE*: Ideally this should be ~2-3 weeks after a regression has been identified to allow other regressions to be identified\n2. Patch Release Managers will work with the authors of the regressions / fixes to cherry pick their change into the related release branch (i.e. `release/1.9` for `1.9.1`)\n    * *NOTE*: Patch release managers should notify authors of the regressions to post a cherry picks for their changes. It is up to authors of the regressions to post a cherry pick. If cherry pick is not posted the issue will not be included in the release.\n3. If cherry picking deadline is missed by cherry pick author, patch release managers will not accept any requests after the fact.\n\n### Building Binaries / Promotion to Stable\n\n> Main POC: Patch Release managers\n\n1. Patch Release Managers will follow the process of [Drafting RCs (Release Candidates)](#drafting-rcs-release-candidates-for-pytorch-and-domain-libraries)\n2. Patch Release Managers will follow the process of [Promoting RCs to Stable](#promoting-rcs-to-stable)\n\n# Hardware / Software Support in Binary Build Matrix\n\nPyTorch has a support matrix across a couple of different axis. This section should be used as a decision making framework to drive hardware / software support decisions\n\n## Python\n\nPyTorch supports all minor versions of CPython that are not EOL: https://devguide.python.org/versions/\n\nFor each minor release independently, we only support patch releases as follows:\n- If the latest patch release is a bugfix release, we only support this one.\n- Otherwise, we support all the non-bugfix patch releases.\n\nSee https://github.com/pytorch/rfcs/blob/master/RFC-0038-cpython-support.md for details on the rules and process for upgrade and sunset of each version.\n\n## Accelerator Software\n\nFor accelerator software like CUDA and ROCm we will typically use the following criteria:\n* Support latest 2 minor versions\n\n### Special support cases\n\nIn some instances support for a particular version of software will continue if a need is found. For example, our CUDA 11 binaries do not currently meet\nthe size restrictions for publishing on PyPI so the default version that is published to PyPI is CUDA 10.2.\n\nThese special support cases will be handled on a case by case basis and support may be continued if current PyTorch maintainers feel as though there may still be a\nneed to support these particular versions of software.\n\n## Operating Systems\nSupported OS flavors are summarized in the table below:\n| Operating System family | Architecture | Notes |\n| --- | --- | --- |\n| Linux | aarch64, x86_64 | Wheels are manylinux2014 compatible, i.e. they should be runnable on any Linux system with glibc-2.17 or above. |\n| MacOS | arm64 | Builds should be compatible with MacOS 11 (Big Sur) or newer, but are actively tested against MacOS 14 (Sonoma). MPS support is enabled on MacOS 13 (Ventura) or later. |\n| Windows | x86_64 | Builds are compatible with Windows-10 or newer. |\n\n# Submitting Tutorials\n\nTutorials in support of a release feature must be submitted to the [pytorch/tutorials](https://github.com/pytorch/tutorials) repo at least two weeks before the release date to allow for editorial and technical review. There is no cherry-pick process for tutorials. All tutorials will be merged around the release day and published at [pytorch.org/tutorials](https://pytorch.org/tutorials/).\n\n# Special Topics\n\n## Updating submodules for a release\n\nIn the event a submodule cannot be fast forwarded, and a patch must be applied we can take two different approaches:\n\n* (preferred) Fork the said repository under the pytorch GitHub organization, apply the patches we need there, and then switch our submodule to accept our fork.\n* Get the dependencies maintainers to support a release branch for us\n\nEditing submodule remotes can be easily done with: (running from the root of the git repository)\n```\ngit config --file=.gitmodules -e\n```\n\nAn example of this process can be found here:\n\n* https://github.com/pytorch/pytorch/pull/48312\n\n## Triton dependency for the release\n\nIn nightly builds for conda and wheels pytorch depend on Triton build by this workflow: https://hud.pytorch.org/hud/pytorch/pytorch/nightly/1?per_page=50&name_filter=Build%20Triton%20Wheel. The pinned version of triton used by this workflow is specified here:  https://github.com/pytorch/pytorch/blob/main/.ci/docker/ci_commit_pins/triton.txt .\n\nIn Nightly builds we have following configuration:\n* Conda builds, depend on: https://anaconda.org/pytorch-nightly/torchtriton\n* Wheel builds, depend on : https://download.pytorch.org/whl/nightly/pytorch-triton/\n* Rocm wheel builds, depend on : https://download.pytorch.org/whl/nightly/pytorch-triton-rocm/\n\nHowever for release we have following :\n* Conda builds, depend on: https://anaconda.org/pytorch-test/torchtriton for test and https://anaconda.org/pytorch/torchtriton for release\n* Wheel builds, depend only triton pypi package: https://pypi.org/project/triton/ for both test and release\n* Rocm wheel builds, depend on : https://download.pytorch.org/whl/test/pytorch-triton-rocm/ for test and https://download.pytorch.org/whl/pytorch-triton-rocm/ for release\n\nImportant: The release of https://pypi.org/project/triton/ needs to be requested from OpenAI once branch cut is completed. Please include the release PIN hash in the request: https://github.com/pytorch/pytorch/blob/release/2.1/.ci/docker/ci_commit_pins/triton.txt .\n"
        },
        {
          "name": "SECURITY.md",
          "type": "blob",
          "size": 9.41,
          "content": "# Security Policy\n\n - [**Reporting a Vulnerability**](#reporting-a-vulnerability)\n - [**Using Pytorch Securely**](#using-pytorch-securely)\n   - [Untrusted models](#untrusted-models)\n   - [Untrusted inputs](#untrusted-inputs)\n   - [Data privacy](#data-privacy)\n   - [Using distributed features](#using-distributed-features)\n- [**CI/CD security principles**](#cicd-security-principles)\n## Reporting Security Issues\n\nBeware that none of the topics under [Using Pytorch Securely](#using-pytorch-securely) are considered vulnerabilities of Pytorch.\n\nHowever, if you believe you have found a security vulnerability in PyTorch, we encourage you to let us know right away. We will investigate all legitimate reports and do our best to quickly fix the problem.\n\nPlease report security issues using https://github.com/pytorch/pytorch/security/advisories/new\n\nPlease refer to the following page for our responsible disclosure policy, reward guidelines, and those things that should not be reported:\n\nhttps://www.facebook.com/whitehat\n\n\n## Using Pytorch Securely\n**Pytorch models are programs**, so treat its security seriously -- running untrusted models is equivalent to running untrusted code. In general we recommend that model weights and the python code for the model are distributed independently. That said, be careful about where you get the python code from and who wrote it (preferentially check for a provenance or checksums, do not run any pip installed package).\n\n### Untrusted models\nBe careful when running untrusted models. This classification includes models created by unknown developers or utilizing data obtained from unknown sources[^data-poisoning-sources].\n\n**Prefer to execute untrusted models within a secure, isolated environment such as a sandbox** (e.g., containers, virtual machines). This helps protect your system from potentially malicious code. You can find further details and instructions in [this page](https://developers.google.com/code-sandboxing).\n\n**Be mindful of risky model formats**. Give preference to share and load weights with the appropriate format for your use case. [safetensors](https://huggingface.co/docs/safetensors/en/index) gives the most safety but is the most restricted in what it supports. [`torch.load`](https://pytorch.org/docs/stable/generated/torch.load.html#torch.load) with `weights_only=True` is also secure to our knowledge even though it offers significantly larger surface of attack. Loading un-trusted checkpoint with `weights_only=False` MUST never be done.\n\n\n\nImportant Note: The trustworthiness of a model is not binary. You must always determine the proper level of caution depending on the specific model and how it matches your use case and risk tolerance.\n\n[^data-poisoning-sources]: To understand risks of utilization of data from unknown sources, read the following Cornell papers on Data poisoning:\n    https://arxiv.org/abs/2312.04748\n    https://arxiv.org/abs/2401.05566\n\n### Untrusted inputs during training and prediction\n\nIf you plan to open your model to untrusted inputs, be aware that inputs can also be used as vectors by malicious agents. To minimize risks, make sure to give your model only the permissions strictly required, and keep your libraries updated with the latest security patches.\n\nIf applicable, prepare your model against bad inputs and prompt injections. Some recommendations:\n- Pre-analysis: check how the model performs by default when exposed to prompt injection (e.g. using fuzzing for prompt injection).\n- Input Sanitation: Before feeding data to the model, sanitize inputs rigorously. This involves techniques such as:\n    - Validation: Enforce strict rules on allowed characters and data types.\n    - Filtering: Remove potentially malicious scripts or code fragments.\n    - Encoding: Convert special characters into safe representations.\n    - Verification: Run tooling that identifies potential script injections (e.g. [models that detect prompt injection attempts](https://python.langchain.com/docs/guides/safety/hugging_face_prompt_injection)).\n\n### Data privacy\n\n**Take special security measures if your model if you train models with sensitive data**. Prioritize [sandboxing](https://developers.google.com/code-sandboxing) your models and:\n- Do not feed sensitive data to untrusted model (even if runs in a sandboxed environment)\n- If you consider publishing a model that was partially trained with sensitive data, be aware that data can potentially be recovered from the trained weights (especially if model overfits).\n\n### Using distributed features\n\nPyTorch can be used for distributed computing, and as such there is a `torch.distributed` package. PyTorch Distributed features are intended for internal communication only. They are not built for use in untrusted environments or networks.\n\nFor performance reasons, none of the PyTorch Distributed primitives (including c10d, RPC, and TCPStore) include any authorization protocol and will send messages unencrypted. They accept connections from anywhere, and execute the workload sent without performing any checks. Therefore, if you run a PyTorch Distributed program on your network, anybody with access to the network can execute arbitrary code with the privileges of the user running PyTorch.\n\n## CI/CD security principles\n_Audience_: Contributors and reviewers, especially if modifying the workflow files/build system.\n\nPyTorch CI/CD security philosophy is based on finding a balance between open and transparent CI pipelines while keeping the environment efficient and safe.\n\nPyTorch testing requirements are complex, and a large part of the code base can only be tested on specialized powerful hardware, such as GPU, making it a lucrative target for resource misuse. To prevent this, we require workflow run approval for PRs from non-member contributors. To keep the volume of those approvals relatively low, we easily extend write permissions to the repository to regular contributors.\n\nMore widespread write access to the repo presents challenges when it comes to reviewing changes, merging code into trunk, and creating releases. [Protected branches](https://docs.github.com/en/repositories/configuring-branches-and-merges-in-your-repository/managing-protected-branches/about-protected-branches) are used to restrict the ability to merge to the trunk/release branches only to the repository administrators and merge bot. The merge bot is responsible for mechanistically merging the change and validating reviews against the path-based rules defined in [merge_rules.yml](https://github.com/pytorch/pytorch/blob/main/.github/merge_rules.yaml). Once a PR has been reviewed by person(s) mentioned in these rules, leaving a `@pytorchbot merge` comment on the PR will initiate the merge process. To protect merge bot credentials from leaking, merge actions must be executed only on ephemeral runners (see definition below) using a specialized deployment environment.\n\nTo speed up the CI system, build steps of the workflow rely on the distributed caching mechanism backed by [sccache](https://github.com/mozilla/sccache), making them susceptible to cache corruption compromises. For that reason binary artifacts generated during CI should not be executed in an environment that contains an access to any sensitive/non-public information and should not be published for use by general audience. One should not have any expectation about the lifetime of those artifacts, although in practice they likely remain accessible for about two weeks after the PR has been closed.\n\nTo speed up CI system setup, PyTorch relies heavily on Docker to pre-build and pre-install the dependencies. To prevent a potentially malicious PR from altering ones that were published in the past, ECR has been configured to use immutable tags.\n\nTo improve runner availability and more efficient resource utilization, some of the CI runners are non-ephemeral, i.e., workflow steps from completely unrelated PRs could be scheduled sequentially on the same runner, making them susceptible to reverse shell attacks. For that reason, PyTorch does not rely on the repository secrets mechanism, as these can easily be compromised in such attacks.\n\n### Release pipelines security\n\nTo ensure safe binary releases, PyTorch release pipelines are built on the following principles:\n - All binary builds/upload jobs must be run on ephemeral runners, i.e., on a machine that is allocated from the cloud to do the build and released back to the cloud after the build is finished. This protects those builds from interference from external actors, who potentially can get reverse shell access to a non-ephemeral runner and wait there for a binary build.\n - All binary builds are cold-start builds, i.e., distributed caching/incremental builds are not permitted. This renders builds much slower than incremental CI builds but isolates them from potential compromises of the intermediate artifacts caching systems.\n - All upload jobs are executed in a [deployment environments](https://docs.github.com/en/actions/deployment/targeting-different-environments/using-environments-for-deployment) that are restricted to protected branches\n - Security credentials needed to upload binaries to PyPI/conda or stable indexes `download.pytorch.org/whl` are never uploaded to repo secrets storage/environment. This requires an extra manual step to publish the release but ensures that access to those would not be compromised by deliberate/accidental leaks of secrets stored in the cloud.\n - No binary artifacts should be published to GitHub releases pages, as these are overwritable by anyone with write permission to the repo.\n"
        },
        {
          "name": "WORKSPACE",
          "type": "blob",
          "size": 9.37,
          "content": "workspace(name = \"pytorch\")\n\nload(\"@bazel_tools//tools/build_defs/repo:http.bzl\", \"http_archive\")\nload(\"//tools/rules:workspace.bzl\", \"new_patched_local_repository\")\n\nhttp_archive(\n    name = \"rules_cc\",\n    patches = [\n        \"//:tools/rules_cc/cuda_support.patch\",\n    ],\n    strip_prefix = \"rules_cc-40548a2974f1aea06215272d9c2b47a14a24e556\",\n    urls = [\n        \"https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/40548a2974f1aea06215272d9c2b47a14a24e556.tar.gz\",\n        \"https://github.com/bazelbuild/rules_cc/archive/40548a2974f1aea06215272d9c2b47a14a24e556.tar.gz\",\n    ],\n)\n\nhttp_archive(\n    name = \"rules_cuda\",\n    strip_prefix = \"runtime-b1c7cce21ba4661c17ac72421c6a0e2015e7bef3/third_party/rules_cuda\",\n    urls = [\"https://github.com/tensorflow/runtime/archive/b1c7cce21ba4661c17ac72421c6a0e2015e7bef3.tar.gz\"],\n)\n\nhttp_archive(\n    name = \"platforms\",\n    urls = [\n        \"https://mirror.bazel.build/github.com/bazelbuild/platforms/releases/download/0.0.10/platforms-0.0.10.tar.gz\",\n        # TODO Fix bazel linter to support hashes for release tarballs.\n        # \"https://github.com/bazelbuild/platforms/releases/download/0.0.10/platforms-0.0.10.tar.gz\",\n    ],\n    # sha256 = \"218efe8ee736d26a3572663b374a253c012b716d8af0c07e842e82f238a0a7ee\",\n)\n\nload(\"@rules_cuda//cuda:dependencies.bzl\", \"rules_cuda_dependencies\")\n\nrules_cuda_dependencies(with_rules_cc = False)\n\nload(\"@rules_cc//cc:repositories.bzl\", \"rules_cc_toolchains\")\n\nrules_cc_toolchains()\n\nhttp_archive(\n    name = \"bazel_skylib\",\n    urls = [\n        \"https://github.com/bazelbuild/bazel-skylib/releases/download/1.0.2/bazel-skylib-1.0.2.tar.gz\",\n    ],\n)\n\nhttp_archive(\n    name = \"pybind11_bazel\",\n    strip_prefix = \"pybind11_bazel-b162c7c88a253e3f6b673df0c621aca27596ce6b\",\n    urls = [\"https://github.com/pybind/pybind11_bazel/archive/b162c7c88a253e3f6b673df0c621aca27596ce6b.zip\"],\n)\n\nnew_local_repository(\n    name = \"pybind11\",\n    build_file = \"@pybind11_bazel//:pybind11.BUILD\",\n    path = \"third_party/pybind11\",\n)\n\nhttp_archive(\n    name = \"com_github_glog\",\n    build_file_content = \"\"\"\nlicenses(['notice'])\n\nload(':bazel/glog.bzl', 'glog_library')\n# TODO: figure out why enabling gflags leads to SIGSEV on the logging init\nglog_library(with_gflags=0)\n    \"\"\",\n    strip_prefix = \"glog-0.4.0\",\n    urls = [\n        \"https://github.com/google/glog/archive/v0.4.0.tar.gz\",\n    ],\n)\n\nhttp_archive(\n    name = \"com_github_gflags_gflags\",\n    strip_prefix = \"gflags-2.2.2\",\n    urls = [\n        \"https://github.com/gflags/gflags/archive/v2.2.2.tar.gz\",\n    ],\n)\n\nhttp_archive(\n    name = \"com_github_opentelemetry-cpp\",\n    urls = [\n        \"https://github.com/open-telemetry/opentelemetry-cpp/archive/refs/tags/v1.14.2.tar.gz\",\n    ],\n)\n\nnew_local_repository(\n    name = \"gloo\",\n    build_file = \"//third_party:gloo.BUILD\",\n    path = \"third_party/gloo\",\n)\n\nnew_local_repository(\n    name = \"onnx\",\n    build_file = \"//third_party:onnx.BUILD\",\n    path = \"third_party/onnx\",\n)\n\nlocal_repository(\n    name = \"com_google_protobuf\",\n    path = \"third_party/protobuf\",\n)\n\nnew_local_repository(\n    name = \"eigen\",\n    build_file = \"//third_party:eigen.BUILD\",\n    path = \"third_party/eigen\",\n)\n\nnew_local_repository(\n    name = \"cutlass\",\n    build_file = \"//third_party:cutlass.BUILD\",\n    path = \"third_party/cutlass\",\n)\n\nnew_local_repository(\n    name = \"fbgemm\",\n    build_file = \"//third_party:fbgemm/BUILD.bazel\",\n    path = \"third_party/fbgemm\",\n    repo_mapping = {\"@cpuinfo\": \"@org_pytorch_cpuinfo\"},\n)\n\nnew_local_repository(\n    name = \"ideep\",\n    build_file = \"//third_party:ideep.BUILD\",\n    path = \"third_party/ideep\",\n)\n\nnew_local_repository(\n    name = \"mkl_dnn\",\n    build_file = \"//third_party:mkl-dnn.BUILD\",\n    path = \"third_party/ideep/mkl-dnn\",\n)\n\nnew_local_repository(\n    name = \"org_pytorch_cpuinfo\",\n    build_file = \"//third_party:cpuinfo/BUILD.bazel\",\n    path = \"third_party/cpuinfo\",\n)\n\nnew_local_repository(\n    name = \"asmjit\",\n    build_file = \"//third_party:fbgemm/third_party/asmjit.BUILD\",\n    path = \"third_party/fbgemm/third_party/asmjit\",\n)\n\nnew_local_repository(\n    name = \"sleef\",\n    build_file = \"//third_party:sleef.BUILD\",\n    path = \"third_party/sleef\",\n)\n\nnew_local_repository(\n    name = \"fmt\",\n    build_file = \"//third_party:fmt.BUILD\",\n    path = \"third_party/fmt\",\n)\n\nnew_local_repository(\n    name = \"kineto\",\n    build_file = \"//third_party:kineto.BUILD\",\n    path = \"third_party/kineto\",\n)\n\nnew_local_repository(\n    name = \"opentelemetry-cpp\",\n    build_file = \"//third_party::opentelemetry-cpp.BUILD\",\n    path = \"third_party/opentelemetry-cpp\",\n)\n\nnew_local_repository(\n    name = \"cpp-httplib\",\n    build_file = \"//third_party:cpp-httplib.BUILD\",\n    path = \"third_party/cpp-httplib\",\n)\n\nnew_local_repository(\n    name = \"nlohmann\",\n    build_file = \"//third_party:nlohmann.BUILD\",\n    path = \"third_party/nlohmann\",\n)\n\nnew_local_repository(\n    name = \"tensorpipe\",\n    build_file = \"//third_party:tensorpipe.BUILD\",\n    path = \"third_party/tensorpipe\",\n)\n\nhttp_archive(\n    name = \"mkl\",\n    build_file = \"//third_party:mkl.BUILD\",\n    sha256 = \"59154b30dd74561e90d547f9a3af26c75b6f4546210888f09c9d4db8f4bf9d4c\",\n    strip_prefix = \"lib\",\n    urls = [\n        \"https://anaconda.org/anaconda/mkl/2020.0/download/linux-64/mkl-2020.0-166.tar.bz2\",\n    ],\n)\n\nhttp_archive(\n    name = \"mkl_headers\",\n    build_file = \"//third_party:mkl_headers.BUILD\",\n    sha256 = \"2af3494a4bebe5ddccfdc43bacc80fcd78d14c1954b81d2c8e3d73b55527af90\",\n    urls = [\n        \"https://anaconda.org/anaconda/mkl-include/2020.0/download/linux-64/mkl-include-2020.0-166.tar.bz2\",\n    ],\n)\n\nhttp_archive(\n    name = \"rules_python\",\n    # TODO Fix bazel linter to support hashes for release tarballs.\n    #\n    # sha256 = \"94750828b18044533e98a129003b6a68001204038dc4749f40b195b24c38f49f\",\n    strip_prefix = \"rules_python-0.21.0\",\n    url = \"https://github.com/bazelbuild/rules_python/releases/download/0.21.0/rules_python-0.21.0.tar.gz\",\n)\n\nload(\"@rules_python//python:repositories.bzl\", \"py_repositories\")\n\npy_repositories()\n\nload(\"@rules_python//python:repositories.bzl\", \"python_register_toolchains\")\n\npython_register_toolchains(\n    name = \"python3_10\",\n    python_version = \"3.10\",\n)\n\nload(\"@python3_10//:defs.bzl\", \"interpreter\")\nload(\"@rules_python//python:pip.bzl\", \"pip_parse\")\n\npip_parse(\n    name = \"pip_deps\",\n    python_interpreter_target = interpreter,\n    requirements_lock = \"//:tools/build/bazel/requirements.txt\",\n)\n\nload(\"@pip_deps//:requirements.bzl\", \"install_deps\")\n\ninstall_deps()\n\nload(\"@pybind11_bazel//:python_configure.bzl\", \"python_configure\")\n\npython_configure(\n    name = \"local_config_python\",\n    python_interpreter_target = interpreter,\n)\n\nload(\"@com_google_protobuf//:protobuf_deps.bzl\", \"protobuf_deps\")\n\nprotobuf_deps()\n\nnew_local_repository(\n    name = \"cuda\",\n    build_file = \"@//third_party:cuda.BUILD\",\n    path = \"/usr/local/cuda\",\n)\n\nnew_local_repository(\n    name = \"cudnn\",\n    build_file = \"@//third_party:cudnn.BUILD\",\n    path = \"/usr/local/cuda\",\n)\n\nnew_local_repository(\n    name = \"cudnn_frontend\",\n    build_file = \"@//third_party:cudnn_frontend.BUILD\",\n    path = \"third_party/cudnn_frontend/\",\n)\n\nlocal_repository(\n    name = \"com_github_google_flatbuffers\",\n    path = \"third_party/flatbuffers\",\n)\n\nlocal_repository(\n    name = \"google_benchmark\",\n    path = \"third_party/benchmark\",\n)\n\nlocal_repository(\n    name = \"com_google_googletest\",\n    path = \"third_party/googletest\",\n)\n\nlocal_repository(\n    name = \"pthreadpool\",\n    path = \"third_party/pthreadpool\",\n    repo_mapping = {\"@com_google_benchmark\": \"@google_benchmark\"},\n)\n\nlocal_repository(\n    name = \"FXdiv\",\n    path = \"third_party/FXdiv\",\n    repo_mapping = {\"@com_google_benchmark\": \"@google_benchmark\"},\n)\n\nlocal_repository(\n    name = \"XNNPACK\",\n    path = \"third_party/XNNPACK\",\n    repo_mapping = {\"@com_google_benchmark\": \"@google_benchmark\"},\n)\n\nlocal_repository(\n    name = \"gemmlowp\",\n    path = \"third_party/gemmlowp/gemmlowp\",\n)\n\nlocal_repository(\n    name = \"kleidiai\",\n    path = \"third_party/kleidiai\",\n    repo_mapping = {\"@com_google_googletest\": \"@com_google_benchmark\"},\n)\n\n### Unused repos start\n\n# `unused` repos are defined to hide bazel files from submodules of submodules.\n# This allows us to run `bazel build //...` and not worry about the submodules madness.\n# Otherwise everything traverses recursively and a lot of submodules of submodules have\n# they own bazel build files.\n\nlocal_repository(\n    name = \"unused_tensorpipe_googletest\",\n    path = \"third_party/tensorpipe/third_party/googletest\",\n)\n\nlocal_repository(\n    name = \"unused_fbgemm\",\n    path = \"third_party/fbgemm\",\n)\n\nlocal_repository(\n    name = \"unused_ftm_bazel\",\n    path = \"third_party/fmt/support/bazel\",\n)\n\nlocal_repository(\n    name = \"unused_kineto_fmt_bazel\",\n    path = \"third_party/kineto/libkineto/third_party/fmt/support/bazel\",\n)\n\nlocal_repository(\n    name = \"unused_kineto_dynolog_googletest\",\n    path = \"third_party/kineto/libkineto/third_party/dynolog/third_party/googletest\",\n)\n\nlocal_repository(\n    name = \"unused_kineto_dynolog_gflags\",\n    path = \"third_party/kineto/libkineto/third_party/dynolog/third_party/gflags\",\n)\n\nlocal_repository(\n    name = \"unused_kineto_dynolog_glog\",\n    path = \"third_party/kineto/libkineto/third_party/dynolog/third_party/glog\",\n)\n\nlocal_repository(\n    name = \"unused_kineto_googletest\",\n    path = \"third_party/kineto/libkineto/third_party/googletest\",\n)\n\nlocal_repository(\n    name = \"unused_onnx_benchmark\",\n    path = \"third_party/onnx/third_party/benchmark\",\n)\n\n### Unused repos end\n"
        },
        {
          "name": "android",
          "type": "tree",
          "content": null
        },
        {
          "name": "aten.bzl",
          "type": "blob",
          "size": 2.77,
          "content": "load(\"@bazel_skylib//lib:paths.bzl\", \"paths\")\nload(\"@rules_cc//cc:defs.bzl\", \"cc_library\")\n\nCPU_CAPABILITY_NAMES = [\"DEFAULT\", \"AVX2\"]\nCAPABILITY_COMPILER_FLAGS = {\n    \"AVX2\": [\"-mavx2\", \"-mfma\", \"-mf16c\"],\n    \"DEFAULT\": [],\n}\n\nPREFIX = \"aten/src/ATen/native/\"\nEXTRA_PREFIX = \"aten/src/ATen/\"\n\ndef intern_build_aten_ops(copts, deps, extra_impls):\n    for cpu_capability in CPU_CAPABILITY_NAMES:\n        srcs = []\n        for impl in native.glob(\n            [\n                PREFIX + \"cpu/*.cpp\",\n                PREFIX + \"quantized/cpu/kernels/*.cpp\",\n            ],\n        ):\n            name = impl.replace(PREFIX, \"\")\n            out = PREFIX + name + \".\" + cpu_capability + \".cpp\"\n            native.genrule(\n                name = name + \"_\" + cpu_capability + \"_cp\",\n                srcs = [impl],\n                outs = [out],\n                cmd = \"cp $< $@\",\n            )\n            srcs.append(out)\n\n        for impl in extra_impls:\n            name = impl.replace(EXTRA_PREFIX, \"\")\n            out = EXTRA_PREFIX + name + \".\" + cpu_capability + \".cpp\"\n            native.genrule(\n                name = name + \"_\" + cpu_capability + \"_cp\",\n                srcs = [impl],\n                outs = [out],\n                cmd = \"cp $< $@\",\n            )\n            srcs.append(out)\n\n        cc_library(\n            name = \"ATen_CPU_\" + cpu_capability,\n            srcs = srcs,\n            copts = copts + [\n                \"-DCPU_CAPABILITY=\" + cpu_capability,\n                \"-DCPU_CAPABILITY_\" + cpu_capability,\n            ] + CAPABILITY_COMPILER_FLAGS[cpu_capability],\n            deps = deps,\n            linkstatic = 1,\n        )\n    cc_library(\n        name = \"ATen_CPU\",\n        deps = [\":ATen_CPU_\" + cpu_capability for cpu_capability in CPU_CAPABILITY_NAMES],\n        linkstatic = 1,\n    )\n\ndef generate_aten_impl(ctx):\n    # Declare the entire ATen/ops/ directory as an output\n    ops_dir = ctx.actions.declare_directory(\"aten/src/ATen/ops\")\n    outputs = [ops_dir] + ctx.outputs.outs\n\n    install_dir = paths.dirname(ops_dir.path)\n    ctx.actions.run(\n        outputs = outputs,\n        inputs = ctx.files.srcs,\n        executable = ctx.executable.generator,\n        arguments = [\n            \"--source-path\",\n            \"aten/src/ATen\",\n            \"--per-operator-headers\",\n            \"--install_dir\",\n            install_dir,\n        ],\n        use_default_shell_env = True,\n        mnemonic = \"GenerateAten\",\n    )\n    return [DefaultInfo(files = depset(outputs))]\n\ngenerate_aten = rule(\n    implementation = generate_aten_impl,\n    attrs = {\n        \"generator\": attr.label(\n            executable = True,\n            allow_files = True,\n            mandatory = True,\n            cfg = \"exec\",\n        ),\n        \"outs\": attr.output_list(),\n        \"srcs\": attr.label_list(allow_files = True),\n    },\n)\n"
        },
        {
          "name": "aten",
          "type": "tree",
          "content": null
        },
        {
          "name": "benchmarks",
          "type": "tree",
          "content": null
        },
        {
          "name": "binaries",
          "type": "tree",
          "content": null
        },
        {
          "name": "buckbuild.bzl",
          "type": "blob",
          "size": 83.59,
          "content": "# NOTE: This file is shared by internal and OSS BUCK build.\n# These load paths point to different files in internal and OSS environment\n\nload(\"@bazel_skylib//lib:paths.bzl\", \"paths\")\nload(\"//tools/build_defs:fb_native_wrapper.bzl\", \"fb_native\")\nload(\"//tools/build_defs:fb_xplat_cxx_library.bzl\", \"fb_xplat_cxx_library\")\nload(\"//tools/build_defs:fb_xplat_genrule.bzl\", \"fb_xplat_genrule\")\nload(\"//tools/build_defs/windows:windows_flag_map.bzl\", \"windows_convert_gcc_clang_flags\")\nload(\"//tools/build_defs:fbsource_utils.bzl\", \"is_arvr_mode\")\nload(\"//tools/build_defs:glob_defs.bzl\", \"subdir_glob\")\nload(\"//tools/build_defs:platform_defs.bzl\", \"APPLETVOS\", \"IOS\", \"MACOSX\")\nload(\"//tools/build_defs:type_defs.bzl\", \"is_list\", \"is_string\")\nload(\"//tools/build_defs/android:build_mode_defs.bzl\", is_production_build_android = \"is_production_build\")\nload(\"//tools/build_defs/apple:build_mode_defs.bzl\", is_production_build_ios = \"is_production_build\")\nload(\n    \":build_variables.bzl\",\n    \"aten_cpu_source_list\",\n    \"aten_native_source_list\",\n    \"core_sources_common\",\n    \"core_sources_full_mobile_no_backend_interface_xplat\",\n    \"core_trainer_sources\",\n    \"jit_core_headers\",\n    \"jit_core_sources\",\n    \"libtorch_profiler_sources\",\n    \"torch_cpp_srcs\",\n    \"torch_mobile_tracer_sources\",\n)\nload(\n    \":pt_ops.bzl\",\n    \"USED_PT_BACKENDS\",\n)\nload(\n    \":pt_template_srcs.bzl\",\n    \"METAL_MASKRCNN_SOURCE_LIST\",\n    \"METAL_SOURCE_LIST\",\n    \"TEMPLATE_MASKRCNN_SOURCE_LIST\",\n    \"TEMPLATE_SOURCE_LIST\",\n    \"aten_ufunc_generated_all_cpu_sources\",\n    \"get_gen_oplist_outs\",\n    \"get_generate_code_bin_outs\",\n    \"get_metal_registration_files_outs\",\n    \"get_metal_registration_files_outs_windows\",\n    \"get_metal_source_dict\",\n    \"get_template_registration_file_rules\",\n    \"get_template_registration_files_outs\",\n    \"get_template_source_dict\",\n)\nload(\n    \":ufunc_defs.bzl\",\n    \"aten_ufunc_generated_cpu_kernel_sources\",\n    \"aten_ufunc_generated_cpu_sources\",\n    \"aten_ufunc_generated_cuda_sources\",\n)\n\ndef read_bool(section, field, default, required = True):\n    val = read_config(section, field)\n    if val != None:\n        if val in [\"true\", \"True\", \"1\"]:\n            return True\n        elif val in [\"false\", \"False\", \"0\"]:\n            return False\n        else:\n            fail(\n                \"`{}:{}`: must be one of (0, 1, true, false, True, False), but was {}\".format(section, field, val),\n            )\n    elif default != None:\n        return default\n    elif not required:\n        return None\n    else:\n        fail(\"`{}:{}`: no value set\".format(section, field))\n\ndef _is_build_mode_dev():\n    if is_production_build_android():\n        # Android Prod builds\n        return False\n    if is_production_build_ios():\n        # iOS Prod builds\n        return False\n\n    return True\n\ndef _get_enable_lightweight_dispatch():\n    return read_bool(\"pt\", \"enable_lightweight_dispatch\", False)\n\ndef _get_enable_record_kernel_dtype():\n    return read_bool(\"pt\", \"enable_record_kernel_dtype\", False)\n\ndef get_enable_mobile_dispatch_keys_trimming():\n    return read_bool(\"pt\", \"enable_mobile_dispatch_keys_trimming\", False)\n\ndef get_disable_per_op_profiling():\n    return read_bool(\"pt\", \"disable_per_op_profiling\", True)\n\ndef get_strip_error_messages():\n    if IS_OSS:\n        return True  # always strip in OSS CI to expose potential issues\n    return read_bool(\"pt\", \"strip_error_messages\", not _is_build_mode_dev())\n\ndef get_disable_warn():\n    return read_bool(\"pt\", \"disable_warn\", False)\n\ndef get_enable_eager_symbolication():\n    return read_bool(\"pt\", \"enable_eager_symbolication\", default = False, required = False)\n\ndef get_static_dispatch_backend():\n    static_dispatch_backend = native.read_config(\"pt\", \"static_dispatch_backend\", None)\n    if static_dispatch_backend == None:\n        return []\n    return static_dispatch_backend.split(\";\")\n\ndef get_glsl_image_format():\n    if read_config(\"pt\", \"vulkan_full_precision\", \"0\") == \"0\":\n        return \"rgba16f\"\n    return \"rgba32f\"\n\ndef get_glsl_paths():\n    paths = [\n        \"//xplat/caffe2:aten_vulkan_glsl_src_path\",\n        \"aten/src/ATen/native/vulkan/glsl\",\n    ] + [\n        p\n        for p in read_config(\"gen_vulkan_spv\", \"additional_glsl_paths\", \"\").split(\" \")\n        if p\n    ]\n\n    if len(paths) % 2 != 0:\n        fail(\n            \"gen_vulkan_spv.additional_glsl_paths must contain an even number of elements\",\n        )\n\n    return \" \".join(\n        [\n            \"$(location {})/{}\".format(\n                paths[i],\n                paths[i + 1],\n            )\n            for i in range(\n                0,\n                len(paths),\n                2,\n            )\n        ],\n    )\n\ndef spv_shader_library():\n    pass\n\nIS_OSS = read_config(\"pt\", \"is_oss\", \"0\") == \"1\"  # True for OSS BUCK build, and False for internal BUCK build\n\nNOT_OSS = not IS_OSS\n\n# for targets in caffe2 root path\nROOT = \"//\" if IS_OSS else \"//xplat/caffe2\"\n\n# for targets in subfolders\nROOT_PATH = \"//\" if IS_OSS else \"//xplat/caffe2/\"\n\nC10 = \"//c10:c10\" if IS_OSS else \"//xplat/caffe2/c10:c10\"\n\n# a dictionary maps third party library name to fbsource and oss target\nTHIRD_PARTY_LIBS = {\n    \"FP16\": [\"//xplat/third-party/FP16:FP16\", \"//third_party:FP16\"],\n    \"FXdiv\": [\"//xplat/third-party/FXdiv:FXdiv\", \"//third_party:FXdiv\"],\n    \"XNNPACK\": [\"//xplat/third-party/XNNPACK:XNNPACK\", \"//third_party:XNNPACK\"],\n    \"clog\": [\"//xplat/third-party/clog:clog\", \"//third_party:clog\"],\n    \"cpuinfo\": [\"//third-party/cpuinfo:cpuinfo\", \"//third_party:cpuinfo\"],\n    \"flatbuffers-api\": [\"//third-party/flatbuffers/fbsource_namespace:flatbuffers-api\", \"//third_party:flatbuffers-api\"],\n    \"flatc\": [\"//third-party/flatbuffers/fbsource_namespace:flatc\", \"//third_party:flatc\"],\n    \"fmt\": [\"//third-party/fmt:fmt\", \"//third_party:fmt\"],\n    \"glog\": [\"//third-party/glog:glog\", \"//third_party:glog\"],\n    \"gmock\": [\"//third-party/googletest:gmock_main\", \"//third_party:gmock\"],\n    \"gtest\": [\"//third-party/googletest:gtest_main\", \"//third_party:gtest\"],\n    \"kineto\": [\"//xplat/kineto/libkineto:libkineto\", \"//third_party:libkineto\"],\n    \"libkineto_headers\": [\"//xplat/kineto/libkineto:libkineto_headers\", \"//third_party:libkineto_headers\"],\n    \"omp\": [\"//xplat/third-party/linker_lib:omp\", \"//third_party:no-op\"],\n    \"pocketfft\": [\"//third-party/pocket_fft:pocketfft\", \"//third_party:pocketfft_header\"],\n    \"psimd\": [\"//xplat/third-party/psimd:psimd\", \"//third_party:psimd\"],\n    \"pthreadpool\": [\"//xplat/third-party/pthreadpool:pthreadpool\", \"//third_party:pthreadpool\"],\n    \"pthreadpool_header\": [\"//xplat/third-party/pthreadpool:pthreadpool_header\", \"//third_party:pthreadpool_header\"],\n    \"pyyaml\": [\"//third-party/pyyaml:pyyaml\", \"//third_party:pyyaml\"],\n    \"rt\": [\"//xplat/third-party/linker_lib:rt\", \"//third_party:rt\"],\n    \"ruy\": [\"//third-party/ruy:ruy_xplat_lib\", \"//third_party:ruy_lib\"],\n    \"sleef_arm\": [\"//third-party/sleef:sleef_arm\", \"//third_party:sleef_arm\"],\n    \"typing-extensions\": [\"//third-party/typing-extensions:typing-extensions\", \"//third_party:typing-extensions\"],\n}\n\ndef third_party(name):\n    if name not in THIRD_PARTY_LIBS:\n        fail(\"Cannot find third party library \" + name + \", please register it in THIRD_PARTY_LIBS first!\")\n    return THIRD_PARTY_LIBS[name][1] if IS_OSS else THIRD_PARTY_LIBS[name][0]\n\ndef get_pt_compiler_flags():\n    return select({\n        \"DEFAULT\": _PT_COMPILER_FLAGS,\n        \"ovr_config//compiler:cl\": windows_convert_gcc_clang_flags(_PT_COMPILER_FLAGS),\n    })\n\n_PT_COMPILER_FLAGS = [\n    \"-fexceptions\",\n    \"-frtti\",\n    \"-Os\",\n    \"-Wno-unknown-pragmas\",\n    \"-Wno-write-strings\",\n    \"-Wno-unused-variable\",\n    \"-Wno-unused-function\",\n    \"-Wno-deprecated-declarations\",\n    \"-Wno-shadow\",\n    \"-Wno-global-constructors\",\n    \"-Wno-missing-prototypes\",\n]\n\nATEN_COMPILER_FLAGS = [\n    \"-fexceptions\",\n    \"-frtti\",\n    \"-Os\",\n    \"-Wno-absolute-value\",\n    \"-Wno-deprecated-declarations\",\n    \"-Wno-macro-redefined\",\n    \"-Wno-tautological-constant-out-of-range-compare\",\n    \"-Wno-unknown-pragmas\",\n    \"-Wno-unknown-warning-option\",\n    \"-Wno-unused-function\",\n    \"-Wno-unused-variable\",\n    \"-Wno-pass-failed\",\n    \"-Wno-shadow\",\n] + select({\n    # Not supported by clang on Windows\n    \"DEFAULT\": [\"-fPIC\"],\n    \"ovr_config//compiler:clang-windows\": [],\n})\n\ndef get_aten_compiler_flags():\n    return select({\n        \"DEFAULT\": ATEN_COMPILER_FLAGS,\n        \"ovr_config//compiler:cl\": windows_convert_gcc_clang_flags(ATEN_COMPILER_FLAGS),\n    })\n\n_COMMON_PREPROCESSOR_FLAGS = [\n    \"-DC10_MOBILE\",\n    \"-DNO_EXPORT\",\n] + (\n    [\"-DC10_MOBILE_TRIM_DISPATCH_KEYS\"] if get_enable_mobile_dispatch_keys_trimming() else []\n) + (\n    [\"-DSTRIP_ERROR_MESSAGES\"] if get_strip_error_messages() else []\n) + (\n    [\"-DDISABLE_WARN\"] if get_disable_warn() else []\n)\n\ndef get_aten_preprocessor_flags():\n    # read_config is not allowed outside of function in Starlark\n    ATEN_PREPROCESSOR_FLAGS = _COMMON_PREPROCESSOR_FLAGS + [\n        \"-DCPU_CAPABILITY_DEFAULT\",\n        \"-DCPU_CAPABILITY=DEFAULT\",\n        \"-DCAFFE2_USE_LITE_PROTO\",\n        \"-DATEN_CUDNN_ENABLED_FBXPLAT=0\",\n        \"-DATEN_MKLDNN_ENABLED_FBXPLAT=0\",\n        \"-DATEN_MKLDNN_ACL_ENABLED_FBXPLAT=0\",\n        \"-DATEN_NNPACK_ENABLED_FBXPLAT=0\",\n        \"-DATEN_MKL_ENABLED_FBXPLAT=0\",\n        \"-DATEN_MKL_SEQUENTIAL_FBXPLAT=0\",\n        \"-DUSE_PYTORCH_METAL\",\n        \"-DUSE_PYTORCH_QNNPACK\",\n        \"-DUSE_XNNPACK\",\n        \"-DPYTORCH_QNNPACK_RUNTIME_QUANTIZATION\",\n        \"-DAT_PARALLEL_OPENMP_FBXPLAT=0\",\n        \"-DAT_PARALLEL_NATIVE_FBXPLAT=1\",\n        \"-DUSE_LAPACK_FBXPLAT=0\",\n        \"-DAT_BLAS_F2C_FBXPLAT=0\",\n        \"-DAT_BLAS_USE_CBLAS_DOT_FBXPLAT=0\",\n        \"-DUSE_RUY_QMATMUL\",\n    ]\n    if get_disable_per_op_profiling():\n        ATEN_PREPROCESSOR_FLAGS.append(\"-DPYTORCH_DISABLE_PER_OP_PROFILING\")\n    if _get_enable_record_kernel_dtype():\n        ATEN_PREPROCESSOR_FLAGS.append(\"-DENABLE_RECORD_KERNEL_FUNCTION_DTYPE\")\n    return ATEN_PREPROCESSOR_FLAGS\n\ndef get_pt_preprocessor_flags():\n    # read_config is not allowed outside of function in Starlark\n    PT_PREPROCESSOR_FLAGS = _COMMON_PREPROCESSOR_FLAGS + [\n        \"-D_THP_CORE\",\n        \"-DUSE_SCALARS\",\n        \"-DNO_CUDNN_DESTROY_HANDLE\",\n    ]\n\n    if _is_build_mode_dev():\n        PT_PREPROCESSOR_FLAGS.append(\"-DENABLE_PYTORCH_NON_PRODUCTION_BUILDS\")\n    return PT_PREPROCESSOR_FLAGS\n\n# This needs to be kept in sync with https://github.com/pytorch/pytorch/blob/release/1.9/torchgen/gen.py#L892\nPT_BACKEND_HEADERS = [\n    \"CPU\",\n    \"CUDA\",\n    \"CompositeExplicitAutograd\",\n    \"CompositeExplicitAutogradNonFunctional\",\n    \"CompositeImplicitAutograd\",\n    \"CompositeImplicitAutogradNestedTensor\",\n    \"Meta\",\n]\n\ndef get_aten_static_dispatch_backend_headers(existing_headers):\n    static_backends = get_static_dispatch_backend()\n    for backend in static_backends:\n        if backend != \"CPU\":\n            existing_headers[\"{}Functions.h\".format(backend)] = \":gen_aten[{}Functions.h]\".format(backend)\n            existing_headers[\"{}Functions_inl.h\".format(backend)] = \":gen_aten[{}Functions_inl.h]\".format(backend)\n    return existing_headers\n\ndef get_aten_codegen_extra_params(backends):\n    extra_params = {\n        \"force_schema_registration\": True,\n    }\n    static_backends = get_static_dispatch_backend()\n    if static_backends:\n        extra_params[\"static_dispatch_backend\"] = static_backends\n        extra_params[\"enabled_backends\"] = static_backends\n    else:\n        extra_params[\"enabled_backends\"] = backends\n    return extra_params\n\ndef get_jit_codegen_params():\n    return []\n\ndef get_unboxing_generated_files():\n    srcs = []\n    if _get_enable_lightweight_dispatch():\n        srcs = [\n            \"UnboxingFunctions.h\",\n            \"UnboxingFunctions_0.cpp\",\n            \"UnboxingFunctions_1.cpp\",\n            \"UnboxingFunctions_2.cpp\",\n            \"UnboxingFunctions_3.cpp\",\n            \"UnboxingFunctions_4.cpp\",\n            \"RegisterCodegenUnboxedKernels_0.cpp\",\n            \"RegisterCodegenUnboxedKernels_1.cpp\",\n            \"RegisterCodegenUnboxedKernels_2.cpp\",\n            \"RegisterCodegenUnboxedKernels_3.cpp\",\n            \"RegisterCodegenUnboxedKernels_4.cpp\",\n            \"RegisterCodegenUnboxedKernels_5.cpp\",\n            \"RegisterCodegenUnboxedKernels_6.cpp\",\n            \"RegisterCodegenUnboxedKernels_7.cpp\",\n            \"RegisterCodegenUnboxedKernels_8.cpp\",\n            \"RegisterCodegenUnboxedKernels_9.cpp\",\n        ]\n    res = {}\n    for file_name in srcs:\n        res[file_name] = [file_name]\n    return res\n\ndef get_aten_generated_files(enabled_backends):\n    # NB: RegisterMeta counts as an optionally enabled backend,\n    # and is intentionally omitted from here\n    src_files = [\n        \"RegisterBackendSelect.cpp\",\n        \"RegisterCompositeImplicitAutograd_0.cpp\",\n        \"RegisterCompositeImplicitAutogradNestedTensor_0.cpp\",\n        \"RegisterCompositeExplicitAutograd_0.cpp\",\n        \"RegisterCompositeExplicitAutogradNonFunctional_0.cpp\",\n        \"CompositeViewCopyKernels.cpp\",\n        \"RegisterSchema.cpp\",\n        \"Declarations.yaml\",\n        \"Functions.cpp\",\n        \"Functions.h\",\n        \"RedispatchFunctions.h\",\n        \"NativeFunctions.h\",\n        \"NativeMetaFunctions.h\",\n        \"MethodOperators.h\",\n        \"FunctionalInverses.h\",\n        \"Operators.h\",\n        \"Operators_0.cpp\",\n        \"Operators_1.cpp\",\n        \"Operators_2.cpp\",\n        \"Operators_3.cpp\",\n        \"Operators_4.cpp\",\n        \"CompositeImplicitAutogradFunctions.h\",\n        \"CompositeImplicitAutogradFunctions_inl.h\",\n        \"CompositeImplicitAutogradNestedTensorFunctions.h\",\n        \"CompositeImplicitAutogradNestedTensorFunctions_inl.h\",\n        \"CompositeExplicitAutogradFunctions.h\",\n        \"CompositeExplicitAutogradFunctions_inl.h\",\n        \"CompositeExplicitAutogradNonFunctionalFunctions.h\",\n        \"CompositeExplicitAutogradNonFunctionalFunctions_inl.h\",\n        \"VmapGeneratedPlumbing.h\",\n        \"core/ATenOpList.cpp\",\n        \"core/TensorBody.h\",\n        \"core/TensorMethods.cpp\",\n        \"core/aten_interned_strings.h\",\n        \"core/enum_tag.h\",\n        \"torch/csrc/inductor/aoti_torch/generated/c_shim_cpu.cpp\",\n    ] + get_aten_derived_type_srcs(enabled_backends)\n\n    # This is tiresome.  A better strategy would be to unconditionally\n    # generate these files, and then only actually COMPILE them depended\n    # on the generated set.  C'est la vie...\n    if \"CPU\" in enabled_backends:\n        src_files.extend(aten_ufunc_generated_cpu_sources())\n        src_files.extend(aten_ufunc_generated_cpu_kernel_sources())\n    if \"CUDA\" in enabled_backends:\n        # Cannot unconditionally include this, because in the Edge selective\n        # build CUDA is not enabled and thus the ufunc codegen for CUDA gets\n        # skipped\n        src_files.extend(aten_ufunc_generated_cuda_sources())\n\n    res = {}\n    for file_name in src_files:\n        res[file_name] = [file_name]\n    return res\n\ndef get_aten_derived_type_src_rules(aten_rule_name, enabled_backends):\n    return [\n        \":{}[{}]\".format(aten_rule_name, \"Register\" + backend + \"_0.cpp\")\n        for backend in enabled_backends if backend != \"CPU\"\n    ] + ([\n        \":{}[RegisterCPU_{}.cpp]\".format(aten_rule_name, x) for x in range(4)\n    ] if \"CPU\" in enabled_backends else [])\n\ndef get_aten_selective_cpp_rules(aten_rule_name, enabled_backends):\n    return [\n        \":{}[{}]\".format(aten_rule_name, f)\n        for f in [\"RegisterCompositeImplicitAutograd_0.cpp\", \"RegisterCompositeImplicitAutogradNestedTensor_0.cpp\", \"RegisterCompositeExplicitAutograd_0.cpp\", \"RegisterCompositeExplicitAutogradNonFunctional_0.cpp\", \"RegisterSchema.cpp\", \"RegisterBackendSelect.cpp\", \"CompositeViewCopyKernels.cpp\"]\n    ] + get_aten_derived_type_src_rules(aten_rule_name, enabled_backends)\n\ndef get_aten_derived_type_srcs(enabled_backends):\n    return [\n        \"Register\" + derived_type + \"_0.cpp\"\n        for derived_type in enabled_backends if derived_type != \"CPU\"\n    ] + [\n        derived_type + \"Functions.h\"\n        for derived_type in enabled_backends\n        if derived_type in PT_BACKEND_HEADERS or derived_type in get_static_dispatch_backend()\n    ] + [\n        derived_type + \"Functions_inl.h\"\n        for derived_type in enabled_backends\n        if derived_type in PT_BACKEND_HEADERS or derived_type in get_static_dispatch_backend()\n    ] + ([\n        \"RegisterCPU_{}.cpp\".format(x) for x in range(4)\n    ] if \"CPU\" in enabled_backends else [])\n\ndef gen_aten_files(\n        name,\n        extra_flags = {},\n        visibility = [],\n        compatible_with = [],\n        apple_sdks = None):\n    extra_params = []\n    force_schema_registration = extra_flags.get(\"force_schema_registration\", False)\n    op_registration_allowlist = extra_flags.get(\"op_registration_allowlist\", None)\n    op_selection_yaml_path = extra_flags.get(\"op_selection_yaml_path\", None)\n    enabled_backends = extra_flags.get(\"enabled_backends\", None)\n    static_dispatch_backend = extra_flags.get(\"static_dispatch_backend\", None)\n\n    if force_schema_registration:\n        extra_params.append(\"--force_schema_registration\")\n    if op_registration_allowlist != None and is_string(op_registration_allowlist):\n        extra_params.append(\"--op_registration_whitelist\")\n        extra_params.append(op_registration_allowlist)\n    if op_selection_yaml_path != None and is_string(op_selection_yaml_path):\n        extra_params.append(\"--op_selection_yaml_path\")\n        extra_params.append(op_selection_yaml_path)\n    if enabled_backends != None and is_list(enabled_backends):\n        extra_params.append(\"--backend_whitelist\")\n        extra_params.extend(enabled_backends)\n    if _get_enable_lightweight_dispatch():\n        extra_params.append(\"--skip_dispatcher_op_registration\")\n    if static_dispatch_backend:\n        extra_params.append(\"--static_dispatch_backend\")\n        extra_params.extend(static_dispatch_backend)\n        backends = static_dispatch_backend\n    else:\n        backends = enabled_backends\n    fb_xplat_genrule(\n        name = name,\n        default_outs = [\".\"],\n        outs = get_aten_generated_files(backends),\n        cmd = \"$(exe {}torchgen:gen) \".format(ROOT_PATH) + \" \".join([\n            \"--source-path $(location {}:aten_src_path)/aten/src/ATen\".format(ROOT),\n            \"--install_dir $OUT\",\n            \"--aoti_install_dir $OUT/torch/csrc/inductor/aoti_torch/generated\"\n        ] + extra_params),\n        visibility = visibility,\n        compatible_with = compatible_with,\n        apple_sdks = apple_sdks,\n    )\n\ndef gen_aten_unboxing_files(\n        genrule_name,\n        extra_flags = {}):\n    extra_params = []\n    op_selection_yaml_path = extra_flags.get(\"op_selection_yaml_path\", None)\n    op_registration_allowlist = extra_flags.get(\"op_registration_allowlist\", None)\n    if op_selection_yaml_path != None and is_string(op_selection_yaml_path):\n        extra_params.append(\"--op_selection_yaml_path\")\n        extra_params.append(op_selection_yaml_path)\n    if op_registration_allowlist != None and is_string(op_registration_allowlist):\n        extra_params.append(\"--op_registration_allowlist\")\n        extra_params.append(op_registration_allowlist)\n\n    fb_xplat_genrule(\n        name = genrule_name,\n        default_outs = [\".\"],\n        outs = get_unboxing_generated_files(),\n        cmd = \"$(exe {}tools:gen_unboxing_bin) \".format(ROOT_PATH) + \" \".join([\n            \"--source-path $(location {}:aten_src_path)/aten/src/ATen\".format(ROOT),\n            \"--install_dir $OUT\",\n        ] + extra_params),\n        visibility = [\"PUBLIC\"],\n    )\n\ndef copy_template_registration_files(name, apple_sdks = None):\n    cmd = []\n    cmd_exe = []\n\n    template_source_dict = get_template_source_dict()\n\n    # Ideally, we would run one copy command for a single source directory along\n    # with all its child directories, but it's somewhat hard to know if a directory\n    # is a child of another just bu looking at the metadata (directory relative\n    # path) that we currently have since 1 directory could look like a parent of\n    # another and yet come from a different filegroup() rule.\n    #\n    for (path_prefix, file_paths) in template_source_dict.items():\n        cmd.append(\"mkdir -p $OUT/{}\".format(path_prefix))\n        cmd_exe.append(\"md $OUT/{}\".format(path_prefix))\n\n        # Adding *.cpp is a workaround to prevent cp from thrown an error when it\n        # encounters a directory (since -r was not specified). If files with an\n        # extension other than .cpp need to be copied, then the command below\n        # will not work and will need to be updated.\n        #\n        cmd.append(\"cp -f $(location {0}:templated_selective_build_srcs)/{1}/*.cpp $OUT/{1}/\".format(ROOT, path_prefix))\n        cmd_exe.append(\"robocopy /E $(location {0}:templated_selective_build_srcs)/{1} $OUT/{1}\".format(ROOT, path_prefix))\n\n    if NOT_OSS:\n        for file_path in TEMPLATE_MASKRCNN_SOURCE_LIST:\n            maskrcnn_file = \"$(location //xplat/caffe2/fb/custom_ops/maskrcnn:templated_selective_build_srcs)/\" + file_path\n            cmd.append(\"cp -f \" + maskrcnn_file + \" $OUT\")\n            cmd_exe.append(\"copy \" + maskrcnn_file + \" $OUT\")\n\n    cmd.append(\"mkdir -p $OUT/aten/src/ATen\")\n    cmd_exe.append(\"md $OUT/aten/src/ATen\")\n\n    # NB: CUDA is skipped here because this is selective build and CUDA is not\n    # supported for selective build\n    for ufunc_file in aten_ufunc_generated_all_cpu_sources(\"$(location \" + ROOT + \":gen_aten[{}])\"):\n        cmd.append(\"cp -f \" + ufunc_file + \" $OUT/aten/src/ATen\")\n        cmd_exe.append(\"copy \" + ufunc_file + \" $OUT/aten/src/ATen\")\n\n    if NOT_OSS:\n        pvd_batch_box_cox_file = \"$(location //xplat/caffe2/fb/custom_ops/batch_box_cox:templated_selective_build_srcs)/register_batch_box_cox_ops.cpp\"\n        cmd.append(\"cp -f \" + pvd_batch_box_cox_file + \" $OUT\")\n        cmd_exe.append(\"copy \" + pvd_batch_box_cox_file + \" $OUT\")\n\n    fb_xplat_genrule(\n        name = name,\n        cmd = \" && \".join(cmd),\n        cmd_exe = \"@powershell -Command \" + (\"; \".join(cmd_exe)),\n        outs = get_template_registration_files_outs(IS_OSS),\n        default_outs = [\".\"],\n        apple_sdks = apple_sdks,\n    )\n\ndef get_feature_tracer_source_list():\n    \"\"\"\n    Return just the Feature specific handlers used in the model tracer.\n    \"\"\"\n    sources = []\n    for s in torch_mobile_tracer_sources:\n        if s.endswith(\"Tracer.cpp\"):\n            sources.append(s)\n    return sources\n\ndef pt_operator_query_codegen(\n        name,\n        deps = [],\n        train = False,\n        enforce_traced_op_list = False,\n        pt_allow_forced_schema_registration = True,\n        compatible_with = [],\n        apple_sdks = None):\n    oplist_dir_name = name + \"_pt_oplist\"\n\n    # @lint-ignore BUCKLINT\n    fb_native.genrule(\n        name = oplist_dir_name,\n        cmd = (\"$(exe {}tools:gen_oplist) \".format(ROOT_PATH) +\n               \"--model_file_list_path $(@query_outputs 'attrfilter(labels, pt_operator_library, deps(set({deps})))') \" +\n               (\"\" if enforce_traced_op_list else \"--allow_include_all_overloads \") +\n               \"--output_dir $OUT \").format(deps = \" \".join([\"\\\"{}\\\"\".format(d) for d in deps])),\n        outs = get_gen_oplist_outs(),\n        default_outs = [\".\"],\n        compatible_with = compatible_with,\n    )\n\n    # Aten files\n    aten_genrule = name + \"_aten\"\n    extra_flags = {\n        \"enabled_backends\": USED_PT_BACKENDS,\n        \"op_selection_yaml_path\": \"$(location :{}[selected_operators.yaml])\".format(oplist_dir_name),\n    }\n\n    if train and pt_allow_forced_schema_registration:\n        extra_flags[\"force_schema_registration\"] = True\n\n    unboxing_genrule = name + \"_unboxing\"\n    if _get_enable_lightweight_dispatch():\n        gen_aten_unboxing_files(\n            unboxing_genrule,\n            extra_flags = extra_flags,\n        )\n\n    static_dispatch_backend = get_static_dispatch_backend()\n    if static_dispatch_backend:\n        extra_flags[\"static_dispatch_backend\"] = static_dispatch_backend\n\n    gen_aten_files(\n        aten_genrule,\n        extra_flags = extra_flags,\n        compatible_with = compatible_with,\n        apple_sdks = apple_sdks,\n    )\n\n    # unboxing_wrappers files\n    extra_params = [\n        \"--operators_yaml_path\",\n        \"$(location :\" + oplist_dir_name + \"[selected_operators.yaml])\",\n    ]\n    unboxing_and_autograd_genrule = name + \"_unboxing_and_autograd\"\n    gen_aten_libtorch_files(\n        unboxing_and_autograd_genrule,\n        extra_params,\n        compatible_with,\n        apple_sdks = apple_sdks,\n    )\n\n    # Template runtime files (prim ops, etc)\n    template_registration_genrule = name + \"_template_registration\"\n    copy_template_registration_files(template_registration_genrule, apple_sdks = apple_sdks)\n\n    # Files needed for metal\n    if NOT_OSS:\n        metal_genrule = name + \"_metal\"\n        copy_metal(metal_genrule, apple_sdks = apple_sdks)\n\n    srcs = get_aten_selective_cpp_rules(\n        aten_genrule,\n        static_dispatch_backend if static_dispatch_backend else USED_PT_BACKENDS,\n    ) + get_template_registration_file_rules(template_registration_genrule, IS_OSS) + ([\n        \":{}[autograd/generated/VariableType_0.cpp]\".format(unboxing_and_autograd_genrule),\n        \":{}[autograd/generated/VariableType_1.cpp]\".format(unboxing_and_autograd_genrule),\n        \":{}[autograd/generated/VariableType_2.cpp]\".format(unboxing_and_autograd_genrule),\n        \":{}[autograd/generated/VariableType_3.cpp]\".format(unboxing_and_autograd_genrule),\n        \":{}[autograd/generated/VariableType_4.cpp]\".format(unboxing_and_autograd_genrule),\n        \":{}[autograd/generated/ADInplaceOrViewType_0.cpp]\".format(unboxing_and_autograd_genrule),\n        \":{}[autograd/generated/ADInplaceOrViewType_1.cpp]\".format(unboxing_and_autograd_genrule),\n    ] if train else []) + ([\n        \":{}[SupportedMobileModelsRegistration.cpp]\".format(oplist_dir_name),\n    ] if NOT_OSS else [])\n\n    headers = {\n        \"selected_mobile_ops.h\": \":{}[selected_mobile_ops.h]\".format(oplist_dir_name),\n    }\n\n    if _get_enable_lightweight_dispatch():\n        srcs.extend([\n            \":{}[UnboxingFunctions_0.cpp]\".format(unboxing_genrule),\n            \":{}[UnboxingFunctions_1.cpp]\".format(unboxing_genrule),\n            \":{}[UnboxingFunctions_2.cpp]\".format(unboxing_genrule),\n            \":{}[UnboxingFunctions_3.cpp]\".format(unboxing_genrule),\n            \":{}[UnboxingFunctions_4.cpp]\".format(unboxing_genrule),\n            \":{}[RegisterCodegenUnboxedKernels_0.cpp]\".format(unboxing_genrule),\n            \":{}[RegisterCodegenUnboxedKernels_1.cpp]\".format(unboxing_genrule),\n            \":{}[RegisterCodegenUnboxedKernels_2.cpp]\".format(unboxing_genrule),\n            \":{}[RegisterCodegenUnboxedKernels_3.cpp]\".format(unboxing_genrule),\n            \":{}[RegisterCodegenUnboxedKernels_4.cpp]\".format(unboxing_genrule),\n            \":{}[RegisterCodegenUnboxedKernels_5.cpp]\".format(unboxing_genrule),\n            \":{}[RegisterCodegenUnboxedKernels_6.cpp]\".format(unboxing_genrule),\n            \":{}[RegisterCodegenUnboxedKernels_7.cpp]\".format(unboxing_genrule),\n            \":{}[RegisterCodegenUnboxedKernels_8.cpp]\".format(unboxing_genrule),\n            \":{}[RegisterCodegenUnboxedKernels_9.cpp]\".format(unboxing_genrule),\n        ])\n        headers[\"UnboxingFunctions.h\"] = \":{}[UnboxingFunctions.h]\".format(unboxing_genrule)\n    return {\"headers\": headers, \"srcs\": srcs}\n\ndef gen_aten_libtorch_files(name, extra_params = [], compatible_with = [], apple_sdks = None):\n    fb_xplat_genrule(\n        name = name,\n        outs = get_generate_code_bin_outs(),\n        default_outs = [\".\"],\n        bash = \"mkdir -p tools && \" +\n               \"$(exe {}tools:generate_code_bin) \".format(ROOT_PATH) + \" \".join(\n            # Mobile build only needs libtorch - skip python bindings for now, except\n            # for ovrsource, which needs Python bindings.\n            ([\"--subset libtorch\"] if not is_arvr_mode() else []) + [\n                \"--native-functions-path $(location {}:aten_src_path)/aten/src/ATen/native/native_functions.yaml\".format(ROOT),\n                \"--tags-path $(location {}:aten_src_path)/aten/src/ATen/native/tags.yaml\".format(ROOT),\n                \"--install_dir $OUT\",\n            ] + extra_params,\n        ),\n        cmd_exe = \"@powershell -Command New-Item -Path tools -ItemType Directory -Force; \" +\n                  \"$(exe {}tools:generate_code_bin) \".format(ROOT_PATH) + \" \".join(\n            # Mobile build only needs libtorch - skip python bindings for now, except\n            # for ovrsource, which needs Python bindings.\n            ([\"--subset libtorch\"] if not is_arvr_mode() else []) + [\n                \"--native-functions-path $(location {}:aten_src_path)/aten/src/ATen/native/native_functions.yaml\".format(ROOT),\n                \"--tags-path $(location {}:aten_src_path)/aten/src/ATen/native/tags.yaml\".format(ROOT),\n                \"--install_dir $OUT\",\n            ] + extra_params,\n        ),\n        compatible_with = compatible_with,\n        apple_sdks = apple_sdks,\n    )\n\ndef vulkan_spv_shader_library(name, spv_filegroup):\n    genrule_cmd = [\n        \"$(exe //xplat/caffe2/tools:gen_aten_vulkan_spv_bin)\",\n        \"--glsl-paths $(location {})\".format(spv_filegroup),\n        \"--output-path $OUT --env FLOAT_IMAGE_FORMAT={}\".format(get_glsl_image_format()),\n        \"--glslc-path=$(exe //xplat/caffe2/fb/vulkan/dotslash:glslc)\",\n        \"--tmp-dir-path=$TMP\",\n    ]\n\n    genrule_name = \"gen_{}_cpp\".format(name)\n    fb_xplat_genrule(\n        name = \"gen_{}_cpp\".format(name),\n        outs = {\n            \"{}.cpp\".format(name): [\"spv.cpp\"],\n        },\n        cmd = \" \".join(genrule_cmd),\n        default_outs = [\".\"],\n        labels = [\"uses_dotslash\"],\n    )\n\n    fb_xplat_cxx_library(\n        name = name,\n        srcs = [\n            \":{}[{}.cpp]\".format(genrule_name, name),\n        ],\n        # Static initialization is used to register shaders to the global shader registry,\n        # therefore link_whole must be True to make sure unused symbols are not discarded.\n        # @lint-ignore BUCKLINT: Avoid `link_whole=True`\n        link_whole = True,\n        # Define a soname that can be used for dynamic loading in Java, Python, etc.\n        soname = \"lib{}.$(ext)\".format(name),\n        visibility = [\"PUBLIC\"],\n        exported_deps = [\n            \"//xplat/caffe2:torch_vulkan_api\",\n        ],\n    )\n\ndef copy_metal(name, apple_sdks = None):\n    cmd = []\n    cmd_exe = []\n    metal_source_dict = get_metal_source_dict()\n\n    # Copy all source files over to bring them into the per app build\n    for path_prefix in sorted(metal_source_dict.keys()):\n        cmd.append(\"mkdir -p $OUT/{}\".format(path_prefix))\n        cmd_exe.append(\"mkdir -Force $OUT/{0}\".format(path_prefix))\n\n        # Not every directory has a mm or cpp file so '2>/dev/null || :' are tricks to suppress the error messages and codes.\n        cmd.append(\"cp -f {0}/{1}/*.mm $OUT/{1}/ 2>/dev/null || :\".format(\"$(location //xplat/caffe2:metal_build_srcs)\", path_prefix))\n        cmd.append(\"cp -f {0}/{1}/*.cpp $OUT/{1}/ 2>/dev/null || :\".format(\"$(location //xplat/caffe2:metal_build_srcs)\", path_prefix))\n\n        # Robocopy has a default success code of 1 which buck treats as failure so the echo masks that problem\n        cmd_exe.append(\"(robocopy /E /NFL /NDL /NJH /NJS {0}/{1} $OUT/{1}) || ECHO robocopy failed\".format(\"$(location //xplat/caffe2:metal_build_srcs)\", path_prefix))\n\n    # Metal custom ops currently have to be brought into selective build because they directly reference metal ops instead of\n    # going through the dispatcher. There is some weird issues with the genrule and these files locations on windows though, so\n    # for now we simply skip building them for windows where they very likely arent needed anyway.\n    # Metal MaskRCNN custom op\n    for full_path in METAL_MASKRCNN_SOURCE_LIST:\n        path_prefix = paths.dirname(full_path)\n        cmd.append(\"mkdir -p $OUT/{}\".format(path_prefix))\n        cmd.append(\"cp -f {0}/{1}/*.mm $OUT/{1}/ 2>/dev/null || :\".format(\"$(location //xplat/caffe2/fb/metal:metal_maskrcnn_sources)\", path_prefix))\n\n    # Unet Metal Prepack Custom op\n    unet_metal_prepack_file = \"$(location //xplat/caffe2/fb/custom_ops/unet_metal_prepack:unet_metal_prepack_sources)\"\n    cmd.append(\"cp -f \" + unet_metal_prepack_file + \"/unet_metal_prepack.cpp\" + \" $OUT\")\n    cmd.append(\"cp -f \" + unet_metal_prepack_file + \"/unet_metal_prepack.mm\" + \" $OUT\")\n\n    fb_xplat_genrule(\n        name = name,\n        cmd = \" && \".join(cmd),\n        cmd_exe = \"@powershell -Command \" + (\"; \".join(cmd_exe)),\n        # due to an obscure bug certain custom ops werent being copied correctly on windows. ARVR also sometimes builds android targets on windows,\n        # so we just exclude those targets from being copied for those platforms (They end up uncompiled anyway).\n        outs = select({\n            \"DEFAULT\": get_metal_registration_files_outs(),\n            \"ovr_config//os:android\": get_metal_registration_files_outs_windows(),\n            \"ovr_config//os:windows\": get_metal_registration_files_outs_windows(),\n        }),\n        default_outs = [\".\"],\n        apple_sdks = apple_sdks,\n    )\n\ndef get_pt_operator_registry_dict(\n        name,\n        deps = [],\n        train = False,\n        labels = [],\n        env = [],\n        template_select = True,\n        enforce_traced_op_list = False,\n        pt_allow_forced_schema_registration = True,\n        enable_flatbuffer = False,\n        **kwargs):\n    code_gen_files = pt_operator_query_codegen(\n        name,\n        deps = deps,\n        train = train,\n        enforce_traced_op_list = enforce_traced_op_list,\n        pt_allow_forced_schema_registration = pt_allow_forced_schema_registration,\n        compatible_with = kwargs.get(\"compatible_with\", []),\n        apple_sdks = kwargs.get(\"apple_sdks\"),\n    )\n\n    return dict(\n        srcs = code_gen_files[\"srcs\"],\n        linker_flags = [\n            \"-Wl,--no-as-needed\",\n        ],\n        # @lint-ignore BUCKLINT link_whole\n        link_whole = True,\n        soname = \"libtorch-code-gen.$(ext)\",\n        header_namespace = \"ATen\",\n        compiler_flags = get_aten_compiler_flags(),\n        exported_headers = code_gen_files[\"headers\"],\n        exported_preprocessor_flags = get_aten_preprocessor_flags() + ([\"-DTEMPLATE_SELECTIVE_BUILD\"] if template_select else []),\n        headers = kwargs.pop(\"headers\", []),\n        labels = kwargs.pop(\"labels\", []) + [\n            # This library has multiple sources with the same file name\n            # and does not work with Buck filegroup used in bad practices.\n            # Opt out of the bad practices check with the below label.\n            \"bad_practices_ignore_override\",\n            \"pt_operator_registry\",\n        ],\n        deps = [\n            # need absolute path here\n            ROOT + \":torch_mobile_core\",\n            ROOT + \":aten_cpu\",\n            ROOT + \":aten_metal_prepack_header\",\n            third_party(\"glog\"),\n            C10,\n        ] + ([ROOT + \":torch_mobile_train\"] if train else []),\n        **kwargs\n    )\n\n# these targets are shared by internal and OSS BUCK\ndef define_buck_targets(\n        aten_default_args = dict(),\n        pt_xplat_cxx_library = fb_xplat_cxx_library,\n        c2_fbandroid_xplat_compiler_flags = [],\n        labels = []):\n    # @lint-ignore BUCKLINT\n    fb_native.filegroup(\n        name = \"metal_build_srcs\",\n        srcs = glob(METAL_SOURCE_LIST),\n        visibility = [\n            \"PUBLIC\",\n        ],\n    )\n\n    # @lint-ignore BUCKLINT\n    fb_native.filegroup(\n        name = \"templated_selective_build_srcs\",\n        # NB: no glob here, there are generated targets in this list!\n        srcs = glob(TEMPLATE_SOURCE_LIST) + aten_ufunc_generated_all_cpu_sources(\":gen_aten[{}]\"),\n        visibility = [\n            \"PUBLIC\",\n        ],\n    )\n\n    fb_xplat_cxx_library(\n        name = \"aten_header\",\n        header_namespace = \"\",\n        exported_headers = subdir_glob([\n            # ATen Core\n            (\"aten/src\", \"ATen/core/**/*.h\"),\n            (\"aten/src\", \"ATen/ops/*.h\"),\n            # ATen Base\n            (\"aten/src\", \"ATen/*.h\"),\n            (\"aten/src\", \"ATen/cpu/**/*.h\"),\n            (\"aten/src\", \"ATen/detail/*.h\"),\n            (\"aten/src\", \"ATen/functorch/**/*.h\"),\n            (\"aten/src\", \"ATen/quantized/*.h\"),\n            (\"aten/src\", \"ATen/vulkan/*.h\"),\n            (\"aten/src\", \"ATen/metal/*.h\"),\n            (\"aten/src\", \"ATen/nnapi/*.h\"),\n            # ATen Native\n            (\"aten/src\", \"ATen/native/*.h\"),\n            (\"aten/src\", \"ATen/native/ao_sparse/quantized/cpu/*.h\"),\n            (\"aten/src\", \"ATen/native/cpu/**/*.h\"),\n            (\"aten/src\", \"ATen/native/sparse/*.h\"),\n            (\"aten/src\", \"ATen/native/nested/*.h\"),\n            (\"aten/src\", \"ATen/native/quantized/*.h\"),\n            (\"aten/src\", \"ATen/native/quantized/cpu/*.h\"),\n            (\"aten/src\", \"ATen/native/transformers/*.h\"),\n            (\"aten/src\", \"ATen/native/ufunc/*.h\"),\n            (\"aten/src\", \"ATen/native/utils/*.h\"),\n            (\"aten/src\", \"ATen/native/vulkan/ops/*.h\"),\n            (\"aten/src\", \"ATen/native/xnnpack/*.h\"),\n            (\"aten/src\", \"ATen/mps/*.h\"),\n            (\"aten/src\", \"ATen/native/mps/*.h\"),\n            # Remove the following after modifying codegen for mobile.\n            (\"aten/src\", \"ATen/mkl/*.h\"),\n            (\"aten/src\", \"ATen/native/mkl/*.h\"),\n            (\"aten/src\", \"ATen/native/mkldnn/*.h\"),\n        ]),\n        visibility = [\"PUBLIC\"],\n        labels = labels,\n    )\n\n    fb_xplat_cxx_library(\n        name = \"aten_vulkan_header\",\n        header_namespace = \"\",\n        exported_headers = subdir_glob([\n            (\"aten/src\", \"ATen/native/vulkan/*.h\"),\n            (\"aten/src\", \"ATen/native/vulkan/ops/*.h\"),\n            (\"aten/src\", \"ATen/vulkan/*.h\"),\n        ]),\n        labels = labels,\n        visibility = [\"PUBLIC\"],\n    )\n\n    fb_xplat_cxx_library(\n        name = \"jit_core_headers\",\n        header_namespace = \"\",\n        exported_headers = subdir_glob([(\"\", x) for x in jit_core_headers]),\n        labels = labels,\n    )\n\n    fb_xplat_cxx_library(\n        name = \"torch_headers\",\n        header_namespace = \"\",\n        exported_headers = subdir_glob(\n            [\n                (\"torch/csrc/api/include\", \"torch/**/*.h\"),\n                (\"\", \"torch/csrc/**/*.h\"),\n                (\"\", \"torch/script.h\"),\n                (\"\", \"torch/library.h\"),\n                (\"\", \"torch/custom_class.h\"),\n                (\"\", \"torch/custom_class_detail.h\"),\n                # Add again due to namespace difference from aten_header.\n                (\"\", \"aten/src/ATen/*.h\"),\n                (\"\", \"aten/src/ATen/functorch/**/*.h\"),\n                (\"\", \"aten/src/ATen/quantized/*.h\"),\n            ],\n            exclude = [\n                # Don't need on mobile.\n                \"torch/csrc/Exceptions.h\",\n                \"torch/csrc/python_headers.h\",\n                \"torch/csrc/jit/serialization/mobile_bytecode_generated.h\",\n            ],\n        ),\n        labels = labels,\n        visibility = [\"PUBLIC\"],\n        deps = [\n            \":generated-version-header\",\n        ],\n    )\n\n    fb_xplat_cxx_library(\n        name = \"aten_test_header\",\n        header_namespace = \"\",\n        exported_headers = subdir_glob([\n            (\"aten/src\", \"ATen/test/*.h\"),\n        ]),\n    )\n\n    fb_xplat_cxx_library(\n        name = \"aten_metal_prepack_header\",\n        header_namespace = \"\",\n        exported_headers = subdir_glob([\n            (\"aten/src\", \"ATen/native/metal/MetalPrepackOpContext.h\"),\n        ]),\n        labels = labels,\n        visibility = [\"PUBLIC\"],\n    )\n\n    fb_xplat_cxx_library(\n        name = \"torch_mobile_headers\",\n        header_namespace = \"\",\n        exported_headers = subdir_glob(\n            [\n                (\"\", \"torch/csrc/jit/mobile/*.h\"),\n            ],\n        ),\n        labels = labels,\n        visibility = [\"PUBLIC\"],\n    )\n\n    fb_xplat_cxx_library(\n        name = \"generated_aten_config_header\",\n        header_namespace = \"ATen\",\n        exported_headers = {\n            \"Config.h\": \":generate_aten_config[Config.h]\",\n        },\n        labels = labels,\n    )\n\n    fb_xplat_cxx_library(\n        name = \"generated-autograd-headers\",\n        header_namespace = \"torch/csrc/autograd/generated\",\n        exported_headers = {\n            \"Functions.h\": \":gen_aten_libtorch[autograd/generated/Functions.h]\",\n            \"VariableType.h\": \":gen_aten_libtorch[autograd/generated/VariableType.h]\",\n            \"variable_factories.h\": \":gen_aten_libtorch[autograd/generated/variable_factories.h]\",\n            \"ViewFuncs.h\": \":gen_aten_libtorch[autograd/generated/ViewFuncs.h]\",\n            # Don't build python bindings on mobile.\n            #\"python_functions.h\",\n        },\n        labels = labels,\n        visibility = [\"PUBLIC\"],\n    )\n\n    fb_xplat_cxx_library(\n        name = \"generated-version-header\",\n        header_namespace = \"torch\",\n        exported_headers = {\n            \"version.h\": \":generate-version-header[version.h]\",\n        },\n        labels = labels,\n    )\n\n    # @lint-ignore BUCKLINT\n    fb_native.genrule(\n        name = \"generate-version-header\",\n        srcs = [\n            \"torch/csrc/api/include/torch/version.h.in\",\n            \"version.txt\",\n        ],\n        cmd = \"$(exe {}tools:gen-version-header) \".format(ROOT_PATH) + \" \".join([\n            \"--template-path\",\n            \"torch/csrc/api/include/torch/version.h.in\",\n            \"--version-path\",\n            \"version.txt\",\n            \"--output-path\",\n            \"$OUT/version.h\",\n        ]),\n        outs = {\n            \"version.h\": [\"version.h\"],\n        },\n        default_outs = [\".\"],\n    )\n\n    # @lint-ignore BUCKLINT\n    fb_native.filegroup(\n        name = \"aten_src_path\",\n        srcs = [\n            \"aten/src/ATen/native/native_functions.yaml\",\n            \"aten/src/ATen/native/tags.yaml\",\n        ] + glob([\"aten/src/ATen/templates/*\"]),\n        visibility = [\n            \"PUBLIC\",\n        ],\n    )\n\n    fb_xplat_cxx_library(\n        name = \"common_core\",\n        srcs = [\n            \"caffe2/core/common.cc\",\n        ],\n        apple_sdks = (IOS, MACOSX, APPLETVOS),\n        compiler_flags = get_pt_compiler_flags(),\n        labels = labels,\n        # @lint-ignore BUCKLINT link_whole\n        link_whole = True,\n        visibility = [\"PUBLIC\"],\n        windows_preferred_linkage = \"static\" if is_arvr_mode() else None,\n        deps = [\n            \":caffe2_headers\",\n            C10,\n        ],\n    )\n\n    # TODO: Enable support for KleidiAI bazel build\n    # @lint-ignore BUCKLINT\n    fb_native.genrule(\n        name = \"generate_aten_config\",\n        srcs = [\n            \"aten/src/ATen/Config.h.in\",\n        ],\n        cmd = \"$(exe {}tools:substitute) \".format(ROOT_PATH) + \" \".join([\n            \"--install_dir\",\n            \"$OUT\",\n            \"--input-file\",\n            \"aten/src/ATen/Config.h.in\",\n            \"--output-file\",\n            \"Config.h\",\n            \"--replace\",\n            \"@AT_MKLDNN_ENABLED@\",\n            \"ATEN_MKLDNN_ENABLED_FBXPLAT\",\n            \"--replace\",\n            \"@AT_MKLDNN_ACL_ENABLED@\",\n            \"ATEN_MKLDNN_ACL_ENABLED_FBXPLAT\",\n            \"--replace\",\n            \"@AT_MKL_ENABLED@\",\n            \"ATEN_MKL_ENABLED_FBXPLAT\",\n            \"--replace\",\n            \"@AT_MKL_SEQUENTIAL@\",\n            \"ATEN_MKL_SEQUENTIAL_FBXPLAT\",\n            \"--replace\",\n            \"@AT_POCKETFFT_ENABLED@\",\n            \"1\",\n            \"--replace\",\n            \"@AT_NNPACK_ENABLED@\",\n            \"ATEN_NNPACK_ENABLED_FBXPLAT\",\n            \"--replace\",\n            \"@CAFFE2_STATIC_LINK_CUDA_INT@\",\n            \"CAFFE2_STATIC_LINK_CUDA_FBXPLAT\",\n            \"--replace\",\n            \"@AT_BUILD_WITH_BLAS@\",\n            \"USE_BLAS_FBXPLAT\",\n            \"--replace\",\n            \"@AT_PARALLEL_OPENMP@\",\n            \"AT_PARALLEL_OPENMP_FBXPLAT\",\n            \"--replace\",\n            \"@AT_PARALLEL_NATIVE@\",\n            \"AT_PARALLEL_NATIVE_FBXPLAT\",\n            \"--replace\",\n            \"@AT_BUILD_WITH_LAPACK@\",\n            \"USE_LAPACK_FBXPLAT\",\n            \"--replace\",\n            \"@AT_BLAS_F2C@\",\n            \"AT_BLAS_F2C_FBXPLAT\",\n            \"--replace\",\n            \"@AT_BLAS_USE_CBLAS_DOT@\",\n            \"AT_BLAS_USE_CBLAS_DOT_FBXPLAT\",\n            \"--replace\",\n            \"@AT_KLEIDIAI_ENABLED@\",\n            \"0\",\n        ]),\n        outs = {\n            \"Config.h\": [\"Config.h\"],\n        },\n        default_outs = [\".\"],\n    )\n\n    gen_aten_files(\n        name = \"gen_aten\",\n        extra_flags = get_aten_codegen_extra_params(USED_PT_BACKENDS),\n        visibility = [\"PUBLIC\"],\n    )\n\n    gen_aten_libtorch_files(name = \"gen_aten_libtorch\")\n\n    gen_aten_libtorch_files(\n        name = \"gen_aten_libtorch_lite\",\n        extra_params = get_jit_codegen_params(),\n    )\n\n    fb_xplat_cxx_library(\n        name = \"generated_aten_headers_cpu\",\n        header_namespace = \"ATen\",\n        exported_headers = get_aten_static_dispatch_backend_headers({\n            \"CPUFunctions.h\": \":gen_aten[CPUFunctions.h]\",\n            \"CPUFunctions_inl.h\": \":gen_aten[CPUFunctions_inl.h]\",\n            \"CompositeExplicitAutogradFunctions.h\": \":gen_aten[CompositeExplicitAutogradFunctions.h]\",\n            \"CompositeExplicitAutogradFunctions_inl.h\": \":gen_aten[CompositeExplicitAutogradFunctions_inl.h]\",\n            \"CompositeExplicitAutogradNonFunctionalFunctions.h\": \":gen_aten[CompositeExplicitAutogradNonFunctionalFunctions.h]\",\n            \"CompositeExplicitAutogradNonFunctionalFunctions_inl.h\": \":gen_aten[CompositeExplicitAutogradNonFunctionalFunctions_inl.h]\",\n            \"CompositeImplicitAutogradFunctions.h\": \":gen_aten[CompositeImplicitAutogradFunctions.h]\",\n            \"CompositeImplicitAutogradFunctions_inl.h\": \":gen_aten[CompositeImplicitAutogradFunctions_inl.h]\",\n            \"CompositeImplicitAutogradNestedTensorFunctions.h\": \":gen_aten[CompositeImplicitAutogradNestedTensorFunctions.h]\",\n            \"CompositeImplicitAutogradNestedTensorFunctions_inl.h\": \":gen_aten[CompositeImplicitAutogradNestedTensorFunctions_inl.h]\",\n            \"FunctionalInverses.h\": \":gen_aten[FunctionalInverses.h]\",\n            \"Functions.h\": \":gen_aten[Functions.h]\",\n            \"MethodOperators.h\": \":gen_aten[MethodOperators.h]\",\n            \"NativeFunctions.h\": \":gen_aten[NativeFunctions.h]\",\n            \"NativeMetaFunctions.h\": \":gen_aten[NativeMetaFunctions.h]\",\n            \"Operators.h\": \":gen_aten[Operators.h]\",\n            \"RedispatchFunctions.h\": \":gen_aten[RedispatchFunctions.h]\",\n            \"core/TensorBody.h\": \":gen_aten[core/TensorBody.h]\",\n            \"core/aten_interned_strings.h\": \":gen_aten[core/aten_interned_strings.h]\",\n            \"core/enum_tag.h\": \":gen_aten[core/enum_tag.h]\",\n        }),\n        labels = labels,\n    )\n\n    fb_xplat_cxx_library(\n        name = \"torch_mobile_observer\",\n        srcs = [\n            \"torch/csrc/jit/mobile/observer.cpp\",\n        ] + ([] if IS_OSS else [\"torch/fb/observers/MobileObserverUtil.cpp\"]),\n        compiler_flags = [\"-fexceptions\"],\n        header_namespace = \"\",\n        exported_headers = subdir_glob(\n            [\n                (\"\", \"torch/csrc/jit/mobile/observer.h\"),\n            ] + ([] if IS_OSS else [\n                (\"\", \"torch/fb/observers/ObserverUtil.h\"),\n                (\"\", \"torch/fb/observers/MobileObserverUtil.h\"),\n            ]),\n        ),\n        fbobjc_compiler_flags = [\n            \"-Wno-missing-prototypes\",\n        ],\n        labels = labels,\n        visibility = [\"PUBLIC\"],\n        deps = [\n            C10,\n        ],\n    )\n\n    # Base library shared by lite-interpreter and full-jit.\n    pt_xplat_cxx_library(\n        name = \"torch_common\",\n        srcs = core_sources_common,\n        compiler_flags = get_pt_compiler_flags(),\n        exported_preprocessor_flags = get_pt_preprocessor_flags(),\n        # @lint-ignore BUCKLINT link_whole\n        link_whole = True,\n        visibility = [\"PUBLIC\"],\n        deps = [\n            \":aten_cpu\",\n            \":generated-autograd-headers\",\n            \":torch_headers\",\n            C10,\n            third_party(\"libkineto_headers\"),\n        ],\n    )\n\n    pt_xplat_cxx_library(\n        name = \"torch_mobile_deserialize_common\",\n        srcs = [\n            \"torch/csrc/jit/mobile/parse_bytecode.cpp\",\n            \"torch/csrc/jit/mobile/parse_operators.cpp\",\n            \"torch/csrc/jit/mobile/upgrader_mobile.cpp\",\n            \"torch/csrc/jit/serialization/import_read.cpp\",\n            \"torch/csrc/jit/serialization/unpickler.cpp\",\n        ],\n        header_namespace = \"\",\n        exported_headers = [\n            \"torch/csrc/jit/serialization/import_read.h\",\n            \"torch/csrc/jit/serialization/unpickler.h\",\n        ],\n        compiler_flags = get_pt_compiler_flags(),\n        exported_preprocessor_flags = get_pt_preprocessor_flags(),\n        extra_flags = {\n            \"fbandroid_compiler_flags\": [\"-frtti\"],\n        },\n        # torch_mobile_deserialize brings in sources neccessary to read a module\n        # which depends on mobile module definition\n        # link_whole is enable so that all symbols neccessary for mobile module are compiled\n        # instead of only symbols used while loading; this prevents symbol\n        # found definied in runtime\n        # @lint-ignore BUCKLINT link_whole\n        link_whole = True,\n        linker_flags = [\"-Wl,--no-as-needed\"],\n        visibility = [\"PUBLIC\"],\n        exported_deps = [\n            \":aten_cpu\",\n            \":caffe2_headers\",\n            \":caffe2_serialize\",\n            \":torch_common\",\n            \":torch_headers\",\n            \":torch_mobile_headers\",\n            \":torch_mobile_module\",\n            \":torch_mobile_observer\",\n            C10,\n        ],\n    )\n\n    pt_xplat_cxx_library(\n        name = \"torch_mobile_module\",\n        srcs = [\n            \"torch/csrc/jit/mobile/function.cpp\",\n            \"torch/csrc/jit/mobile/interpreter.cpp\",\n            \"torch/csrc/jit/mobile/module.cpp\",\n        ],\n        header_namespace = \"\",\n        exported_headers = [\n        ],\n        compiler_flags = get_pt_compiler_flags(),\n        exported_preprocessor_flags = get_pt_preprocessor_flags() + ([\"-DSYMBOLICATE_MOBILE_DEBUG_HANDLE\"] if get_enable_eager_symbolication() else []),\n        extra_flags = {\n            \"fbandroid_compiler_flags\": [\"-frtti\"],\n        },\n        # @lint-ignore BUCKLINT link_whole\n        link_whole = True,\n        linker_flags = [\n            \"-Wl,--no-as-needed\",\n        ],\n        visibility = [\"PUBLIC\"],\n        exported_deps = [\n            \":aten_cpu\",\n            \":caffe2_headers\",\n            \":torch_common\",\n            \":torch_headers\",\n            \":torch_mobile_headers\",\n            \":torch_mobile_observer\",\n            C10,\n        ],\n    )\n\n    pt_xplat_cxx_library(\n        name = \"torch_mobile_debug_symbolication\",\n        srcs = [\n            # included in aten_cpu \"torch/csrc/jit/frontend/source_range.cpp\",\n            \"torch/csrc/jit/ir/scope.cpp\",\n            \"torch/csrc/jit/mobile/debug_info.cpp\",\n            \"torch/csrc/jit/serialization/callstack_debug_info_serialization.cpp\",\n            \"torch/csrc/jit/serialization/source_range_serialization.cpp\",\n            \"torch/csrc/jit/serialization/pickle.cpp\",\n            # pickler.cpp doesn't seem to be needed.\n            # \"torch/csrc/jit/serialization/pickler.cpp\",\n            # included in core_sources_common \"torch/csrc/jit/serialization/unpickler.cpp\",\n        ],\n        compiler_flags = get_pt_compiler_flags(),\n        exported_preprocessor_flags = get_pt_preprocessor_flags(),\n        header_namespace = \"\",\n        # @lint-ignore BUCKLINT link_whole\n        link_whole = True,\n        linker_flags = [\n            \"-Wl,--no-as-needed\",\n        ],\n        visibility = [\"PUBLIC\"],\n        deps = [\n            \":torch_mobile_deserialize\",\n        ],\n        exported_deps = [\n            \":torch_common\",\n        ],\n    )\n\n    pt_xplat_cxx_library(\n        name = \"torch_model_tracer\",\n        srcs = [\n            \"torch/csrc/jit/mobile/model_tracer/TracerRunner.cpp\",\n        ] + get_feature_tracer_source_list(),\n        header_namespace = \"\",\n        compiler_flags = get_pt_compiler_flags(),\n        exported_preprocessor_flags = get_pt_preprocessor_flags() + ([\"-DSYMBOLICATE_MOBILE_DEBUG_HANDLE\"] if get_enable_eager_symbolication() else []),\n        # @lint-ignore BUCKLINT link_whole\n        link_whole = True,\n        linker_flags = [\n            \"-Wl,--no-as-needed\",\n        ],\n        visibility = [\"PUBLIC\"],\n        deps = [\n            \":generated-autograd-headers\",\n            \":torch_mobile_deserialize\",\n            \":torch_mobile_headers\",\n            \":torch_mobile_observer\",\n        ] + ([] if IS_OSS else [\"//xplat/folly:molly\"]),\n        exported_deps = [\n            \":aten_cpu\",\n            \":torch_common\",\n        ] + ([] if IS_OSS else [\n            \"//xplat/caffe2/fb/custom_ops/batch_box_cox:batch_box_cox\",\n            \"//xplat/caffe2/fb/custom_ops/maskrcnn:maskrcnn\",\n        ]),\n    )\n\n    pt_xplat_cxx_library(\n        name = \"torch_mobile_deserialize\",\n        srcs = [\n            \"torch/csrc/jit/mobile/import.cpp\",\n            \"torch/csrc/jit/mobile/flatbuffer_loader.cpp\",\n        ],\n        compiler_flags = get_pt_compiler_flags(),\n        exported_preprocessor_flags = get_pt_preprocessor_flags() + ([\"-DFB_XPLAT_BUILD\"] if not IS_OSS else []),\n        header_namespace = \"\",\n        exported_headers = [\n            \"torch/csrc/jit/mobile/import.h\",\n            \"torch/csrc/jit/mobile/flatbuffer_loader.h\",\n        ],\n        # torch_mobile_deserialize brings in sources neccessary to read a module\n        # which depends on mobile module definition\n        # link_whole is enable so that all symbols neccessary for mobile module are compiled\n        # instead of only symbols used while loading; this prevents symbol\n        # found definied in runtime\n        # @lint-ignore BUCKLINT link_whole\n        link_whole = True,\n        linker_flags = [\n            \"-Wl,--no-as-needed\",\n        ],\n        visibility = [\"PUBLIC\"],\n        exported_deps = [\n            \":aten_cpu\",\n            \":caffe2_headers\",\n            \":caffe2_serialize\",\n            \":torch_common\",\n            \":torch_headers\",\n            \":torch_mobile_headers\",\n            \":torch_mobile_module\",\n            \":torch_mobile_observer\",\n            \":torch_mobile_deserialize_common\",\n            \":mobile_bytecode\",\n            C10,\n        ],\n    )\n\n    pt_xplat_cxx_library(\n        name = \"torch_mobile_core\",\n        srcs = [],\n        header_namespace = \"\",\n        exported_headers = [],\n        compiler_flags = get_pt_compiler_flags(),\n        exported_preprocessor_flags = get_pt_preprocessor_flags() + ([\"-DSYMBOLICATE_MOBILE_DEBUG_HANDLE\"] if get_enable_eager_symbolication() else []),\n        # torch_mobile_core brings in sources neccessary to read and run a module\n        # link_whole is enabled so that all symbols linked\n        # operators, registerations and other few symbols are need in runtime\n        # @lint-ignore BUCKLINT link_whole\n        link_whole = True,\n        linker_flags = [\n            \"-Wl,--no-as-needed\",\n        ],\n        visibility = [\"PUBLIC\"],\n        deps = [\n            \":generated-autograd-headers\",\n            \":torch_mobile_headers\",\n            \":torch_mobile_observer\",\n        ],\n        exported_deps = [\n            \":aten_cpu\",\n            \":torch_common\",\n            \":torch_mobile_deserialize\",\n            \":torch_supported_mobile_models\",\n        ],\n    )\n\n    pt_xplat_cxx_library(\n        name = \"torch_mobile_core_pickle_and_flatbuffer\",\n        compiler_flags = get_pt_compiler_flags(),\n        exported_preprocessor_flags = get_pt_preprocessor_flags(),\n        visibility = [\"PUBLIC\"],\n        exported_deps = [\n            \":flatbuffers_mobile\",\n            \":torch_mobile_core\",\n        ],\n    )\n\n    pt_xplat_cxx_library(\n        name = \"torch_cpp_cpu\",\n        srcs = torch_cpp_srcs,\n        headers = native.glob([\"torch/csrc/api/include/**/*.h\"]) + [\"torch/script.h\"],\n        compiler_flags = get_pt_compiler_flags(),\n        exported_preprocessor_flags = get_pt_preprocessor_flags(),\n        visibility = [\"PUBLIC\"],\n        exported_deps = [\n            \":torch\",\n            \":torch_mobile_deserialize_common\",  # for torch/csrc/api/src/serialize/input-archive.cpp\n        ],\n    )\n\n    pt_xplat_cxx_library(\n        name = \"torch_core\",\n        srcs = core_sources_full_mobile_no_backend_interface_xplat,\n        compiler_flags = get_pt_compiler_flags(),\n        exported_preprocessor_flags = get_pt_preprocessor_flags(),\n        visibility = [\n            \"//xplat/caffe2/android/...\",\n            \"//xplat/caffe2/fb/...\",\n            \"//xplat/caffe2/fb/model_tracer/...\",\n        ],\n        deps = [\n            \":aten_cpu\",\n            \":backend_interface_lib\",\n            \":generated-autograd-headers\",\n            \":torch_headers\",\n            \":torch_mobile_deserialize\",\n            third_party(\"glog\"),\n            third_party(\"rt\"),\n            C10,\n        ] + ([] if IS_OSS else [\n            \"//xplat/caffe2/fb/custom_ops/batch_box_cox:batch_box_cox\",\n            \"//xplat/caffe2/fb/custom_ops/maskrcnn:maskrcnn\",\n        ]),\n        exported_deps = [\n            \":torch_common\",\n            \":torch_mobile_train\",\n        ],\n    )\n\n    pt_xplat_cxx_library(\n        name = \"torch_train\",\n        srcs = [\n            \"torch/csrc/api/src/data/samplers/random.cpp\",\n            \"torch/csrc/api/src/data/samplers/sequential.cpp\",\n            \"torch/csrc/api/src/optim/optimizer.cpp\",\n            \"torch/csrc/api/src/optim/serialize.cpp\",\n            \"torch/csrc/api/src/optim/sgd.cpp\",\n            \"torch/csrc/api/src/serialize/input-archive.cpp\",\n            \"torch/csrc/api/src/serialize/output-archive.cpp\",\n            \"torch/csrc/jit/api/module_save.cpp\",\n        ],\n        compiler_flags = get_pt_compiler_flags(),\n        exported_preprocessor_flags = get_pt_preprocessor_flags(),\n        visibility = [\"PUBLIC\"],\n        deps = [\n            \":aten_cpu\",\n            \":torch_headers\",\n            \":torch\",\n            \":torch_core\",\n            \":torch_mobile_deserialize\",\n            \":torch_mobile_train\",\n            \":jit_module_saving\",\n            C10,\n        ],\n    )\n\n    pt_xplat_cxx_library(\n        name = \"torch_mobile_train\",\n        srcs = core_trainer_sources + [\n            \"torch/csrc/autograd/VariableTypeManual.cpp\",\n            \"torch/csrc/autograd/FunctionsManual.cpp\",\n            \"torch/csrc/api/src/data/datasets/mnist.cpp\",\n            \"torch/csrc/jit/mobile/quantization.cpp\",\n            \"torch/csrc/jit/mobile/train/export_data.cpp\",\n            \"torch/csrc/jit/mobile/train/optim/sgd.cpp\",\n            \"torch/csrc/jit/mobile/train/random.cpp\",\n            \"torch/csrc/jit/mobile/train/sequential.cpp\",\n            \":gen_aten_libtorch[autograd/generated/Functions.cpp]\",\n            \":gen_aten_libtorch[autograd/generated/ViewFuncs.cpp]\",\n        ],\n        compiler_flags = get_pt_compiler_flags(),\n        exported_preprocessor_flags = get_pt_preprocessor_flags() + [\"-DUSE_MOBILE_CLASSTYPE\"],\n        # torch_mobile_train brings in sources neccessary to read and run a mobile\n        # and save and load mobile params along with autograd\n        # link_whole is enabled so that all symbols linked\n        # operators, registerations and autograd related symbols are need in runtime\n        # @lint-ignore BUCKLINT link_whole\n        link_whole = True,\n        visibility = [\"PUBLIC\"],\n        deps = [\n            \":aten_cpu\",\n            \":generated-autograd-headers\",\n            \":torch_headers\",\n            \":torch_mobile_deserialize\",\n            \":flatbuffers_serializer_mobile\",\n            C10,\n        ],\n    )\n\n    pt_xplat_cxx_library(\n        name = \"torch\",\n        srcs = [\n            \"torch/csrc/jit/runtime/register_c10_ops.cpp\",\n            \"torch/csrc/jit/runtime/register_prim_ops_fulljit.cpp\",\n        ],\n        compiler_flags = get_pt_compiler_flags(),\n        exported_preprocessor_flags = get_pt_preprocessor_flags(),\n        # torch brings in all sources neccessary to read and run a mobile module/jit module\n        # link_whole is enabled so that all symbols linked\n        # operators, registerations and other few symbols are need in runtime\n        # @lint-ignore BUCKLINT link_whole\n        link_whole = True,\n        visibility = [\"PUBLIC\"],\n        deps = [\n            # This is to have autograd profiler available\n            # in xplat/caffe2:torch which some builds are using\n            # notable xplate/facegen:testsAndroid\n            \":torch_headers\",\n            \":torch_kineto_profiling\",\n        ],\n        exported_deps = [\n            \":aten_cpu\",\n            \":torch_core\",\n            C10,\n        ],\n    )\n\n    pt_xplat_cxx_library(\n        name = \"torch_mobile_train_import_data\",\n        srcs = [\n            \"torch/csrc/jit/mobile/import_data.cpp\",\n        ],\n        compiler_flags = get_pt_compiler_flags(),\n        exported_preprocessor_flags = get_pt_preprocessor_flags() + [\"-DUSE_MOBILE_CLASSTYPE\"],\n        # torch_mobile_train_import_data brings in sources neccessary to read a mobile module\n        # link_whole is enabled so that all symbols linked\n        # operators other few symbols are need in runtime\n        # @lint-ignore BUCKLINT link_whole\n        link_whole = True,\n        visibility = [\"PUBLIC\"],\n        deps = [\n            \":torch_headers\",\n            \":torch_mobile_observer\",\n            \":torch_mobile_core\",\n            \":torch_mobile_train\",\n        ],\n    )\n\n    fb_xplat_cxx_library(\n        name = \"torch_mobile_compatibility\",\n        srcs = [\n            # These .cpp brought in through core_sources_common\n            # \"torch/csrc/jit/mobile/compatibility/runtime_compatibility.cpp\",\n            # \"torch/csrc/jit/serialization/unpickler.cpp\",\n            \"torch/csrc/jit/mobile/compatibility/model_compatibility.cpp\",\n        ],\n        header_namespace = \"\",\n        exported_headers = [\n            \"torch/csrc/jit/mobile/compatibility/backport.h\",\n            \"torch/csrc/jit/mobile/compatibility/backport_manager.h\",\n            \"torch/csrc/jit/mobile/compatibility/model_compatibility.h\",\n            \"torch/csrc/jit/mobile/compatibility/runtime_compatibility.h\",\n        ],\n        compiler_flags = [\n            \"-fexceptions\",\n            \"-frtti\",\n            \"-Wno-deprecated-declarations\",\n            \"-Wno-global-constructors\",\n        ],\n        labels = labels,\n        visibility = [\"PUBLIC\"],\n        deps = [\n            \":torch_mobile_deserialize\",\n        ],\n    )\n\n    pt_xplat_cxx_library(\n        name = \"jit_module_saving\",\n        srcs = [\n            \"torch/csrc/jit/api/module_save.cpp\",\n            \"torch/csrc/jit/serialization/export_bytecode.cpp\",\n            \"torch/csrc/jit/serialization/export_module.cpp\",\n        ],\n        compiler_flags = get_pt_compiler_flags(),\n        exported_preprocessor_flags = get_pt_preprocessor_flags() +\n                                      ([\"-DFB_XPLAT_BUILD\"] if not IS_OSS else []),\n        exported_headers = [\n            \"torch/csrc/jit/serialization/export.h\",\n        ],\n        visibility = [\"PUBLIC\"],\n        deps = [\n            \":torch\",\n            \":torch_mobile_core\",\n            \":flatbuffers_serializer_mobile\",\n        ],\n    )\n\n    pt_xplat_cxx_library(\n        name = \"torch_mobile_model_tracer\",\n        srcs = [\n            \"torch/csrc/jit/mobile/model_tracer/MobileModelRunner.cpp\",\n            \"torch/csrc/jit/mobile/model_tracer/TensorUtils.cpp\",\n        ],\n        headers = [\n            \"torch/csrc/jit/mobile/model_tracer/MobileModelRunner.h\",\n            \"torch/csrc/jit/mobile/model_tracer/TensorUtils.h\",\n        ],\n        header_namespace = \"\",\n        exported_headers = [\n            \"torch/csrc/jit/mobile/model_tracer/MobileModelRunner.h\",\n        ],\n        compiler_flags = get_pt_compiler_flags(),\n        exported_preprocessor_flags = get_pt_preprocessor_flags() + ([\"-DSYMBOLICATE_MOBILE_DEBUG_HANDLE\"] if get_enable_eager_symbolication() else []),\n        # torch_mobile_model_tracer brings in sources neccessary to read and run a jit module\n        # and trace the ops\n        # link_whole is enabled so that all symbols linked\n        # operators, registerations and other few symbols are need in runtime\n        # @lint-ignore BUCKLINT link_whole\n        link_whole = True,\n        linker_flags = [\n            \"-Wl,--no-as-needed\",\n        ],\n        visibility = [\"PUBLIC\"],\n        deps = [\n            \":caffe2_serialize\",\n            \":generated-autograd-headers\",\n            \":torch_mobile_headers\",\n            \":torch_mobile_observer\",\n            \":torch_mobile_core\",\n        ] + ([] if IS_OSS else [\"//xplat/folly:molly\"]),\n        exported_deps = [\n            \":aten_cpu\",\n            \":torch_common\",\n        ] + ([] if IS_OSS else [\n            \"//xplat/caffe2/fb/custom_ops/batch_box_cox:batch_box_cox\",\n            \"//xplat/caffe2/fb/custom_ops/maskrcnn:maskrcnn\",\n            \"//xplat/caffe2/fb/custom_ops/sparsenn:sparsenn-all\",\n        ]),\n    )\n\n    #TODO(qihan) delete\n    pt_xplat_cxx_library(\n        name = \"torch_mobile_core_flatbuffer\",\n        srcs = [],\n        header_namespace = \"\",\n        exported_headers = [],\n        compiler_flags = get_pt_compiler_flags(),\n        exported_preprocessor_flags = get_pt_preprocessor_flags() + ([\"-DSYMBOLICATE_MOBILE_DEBUG_HANDLE\"] if get_enable_eager_symbolication() else []),\n        # @lint-ignore BUCKLINT link_whole\n        link_whole = True,\n        linker_flags = [\n            \"-Wl,--no-as-needed\",\n        ],\n        visibility = [\"PUBLIC\"],\n        deps = [\n            \":generated-autograd-headers\",\n            \":torch_mobile_headers\",\n            \":torch_mobile_observer\",\n        ],\n        exported_deps = [\n            \":aten_cpu\",\n            \":torch_common\",\n        ],\n    )\n\n    fb_xplat_cxx_library(\n        name = \"backend_interface_lib\",\n        srcs = [\n            \"torch/csrc/jit/backends/backend_debug_info.cpp\",\n            \"torch/csrc/jit/backends/backend_interface.cpp\",\n        ],\n        compiler_flags = get_pt_compiler_flags(),\n        fbandroid_compiler_flags = c2_fbandroid_xplat_compiler_flags,\n        # @lint-ignore BUCKLINT link_whole\n        link_whole = True,\n        linker_flags = [\n            \"-Wl,--no-as-needed\",\n        ],\n        visibility = [\"PUBLIC\"],\n        exported_deps = [\n            \":aten_cpu\",\n            \":torch_common\",\n        ],\n    )\n\n    pt_xplat_cxx_library(\n        name = \"torch_kineto_profiling\",\n        srcs = libtorch_profiler_sources,\n        compiler_flags = get_pt_compiler_flags() + [\"-Wno-error\"],\n        exported_preprocessor_flags = get_pt_preprocessor_flags() + [\n            \"-DUSE_KINETO\",\n            # Need this otherwise USE_KINETO is undefed\n            # for mobile\n            \"-DEDGE_PROFILER_USE_KINETO\",\n        ],\n        # @lint-ignore BUCKLINT link_whole\n        link_whole = True,\n        linker_flags = [\n            \"-Wl,--no-as-needed\",\n        ],\n        visibility = [\"PUBLIC\"],\n        deps = [\n            third_party(\"glog\"),\n            third_party(\"kineto\"),\n        ],\n        exported_deps = [\n            \":aten_cpu\",\n            \":torch_common\",\n        ],\n    )\n\n    pt_xplat_cxx_library(\n        name = \"torch_edge_profiling\",\n        srcs = [\"torch/csrc/jit/mobile/profiler_edge.cpp\"],\n        compiler_flags = get_pt_compiler_flags() + [\"-Wno-error\"],\n        exported_preprocessor_flags = get_pt_preprocessor_flags() + [\n            \"-DUSE_KINETO\",\n            \"-DEDGE_PROFILER_USE_KINETO\",\n        ],\n        # @lint-ignore BUCKLINT link_whole\n        link_whole = True,\n        linker_flags = [\n            \"-Wl,--no-as-needed\",\n        ],\n        visibility = [\"PUBLIC\"],\n        exported_deps = [\n            \":torch_common\",\n            \":torch_kineto_profiling\",\n            \":torch_mobile_core\",\n        ],\n    )\n\n    fb_xplat_genrule(\n        name = \"mobile_bytecode_header\",\n        srcs = [\n            \"torch/csrc/jit/serialization/mobile_bytecode.fbs\",\n        ],\n        outs = {\n            \"mobile_bytecode_generated_fbsource.h\": [\"mobile_bytecode_generated.h\"],\n        },\n        cmd = \"$(exe {})\".format(third_party(\"flatc\")) +\n              \" --cpp --gen-mutable --scoped-enums -o ${OUT} ${SRCS}\",\n        default_outs = [\".\"],\n        visibility = [\n            \"{}:mobile_bytecode\".format(ROOT),\n        ],\n    )\n\n    # Users of this target will need to add third_party(\"flatbuffers-api\") as a\n    # dep.\n    fb_xplat_cxx_library(\n        name = \"mobile_bytecode\",\n        header_namespace = \"\",\n        exported_headers = {\n            (\"torch/csrc/jit/serialization/mobile_bytecode_generated.h\" if IS_OSS else \"torch/csrc/jit/serialization/mobile_bytecode_generated_fbsource.h\"): \":mobile_bytecode_header[mobile_bytecode_generated_fbsource.h]\",\n        },\n        # Avoid leaking implementation details by only exposing this header to\n        # the internals of the loader/serializer layer.\n        visibility = [\n            \"{}:flatbuffer_loader\".format(ROOT),\n            \"{}:flatbuffers_serializer_mobile\".format(ROOT),\n        ],\n        exported_deps = [\n            third_party(\"flatbuffers-api\"),\n        ],\n    )\n\n    fb_xplat_cxx_library(\n        name = \"flatbuffers_serializer_mobile\",\n        srcs = [\"torch/csrc/jit/serialization/flatbuffer_serializer.cpp\"],\n        exported_headers = [\n            \"torch/csrc/jit/serialization/flatbuffer_serializer.h\",\n        ],\n        compiler_flags = [\n            \"-g0\",\n            \"-O3\",\n            \"-fexceptions\",\n            \"-frtti\",\n            \"-Wno-deprecated-declarations\",\n        ] + ([\"-DFB_XPLAT_BUILD\"] if not IS_OSS else []),\n        visibility = [\"PUBLIC\"],\n        deps = [\n            \":mobile_bytecode\",\n            \":torch_mobile_module\",\n            C10,\n        ],\n        exported_deps = [\n            \":torch_mobile_deserialize\",\n            \":mobile_bytecode\",\n        ],\n    )\n\n    # TODO (qihan) delete\n    pt_xplat_cxx_library(\n        name = \"flatbuffer_loader\",\n        srcs = [\n        ],\n        exported_headers = [\n            \"torch/csrc/jit/mobile/flatbuffer_loader.h\",\n        ],\n        compiler_flags = get_pt_compiler_flags() + [\"-Wno-error\"],\n        exported_preprocessor_flags = get_pt_preprocessor_flags() + [\n            \"-DUSE_KINETO\",\n            # Need this otherwise USE_KINETO is undefed\n            # for mobile\n            \"-DEDGE_PROFILER_USE_KINETO\",\n        ] + ([\"-DFB_XPLAT_BUILD\"] if not IS_OSS else []),\n        extra_flags = {\n            \"fbandroid_compiler_flags\": [\"-frtti\"],\n        },\n        # torch_mobile_deserialize brings in sources neccessary to read a module\n        # which depends on mobile module definition\n        # link_whole is enable so that all symbols neccessary for mobile module are compiled\n        # instead of only symbols used while loading; this prevents symbol\n        # found definied in runtime\n        # @lint-ignore BUCKLINT link_whole\n        link_whole = True,\n        linker_flags = [\n            \"-Wl,--no-as-needed\",\n        ],\n        visibility = [\"PUBLIC\"],\n        deps = [\n            \":mobile_bytecode\",\n        ],\n        exported_deps = [\n            C10,\n        ],\n    )\n\n    # TODO(qihan) delete\n    fb_xplat_cxx_library(\n        name = \"flatbuffers_serializer_jit\",\n        compiler_flags = [\n            \"-g0\",\n            \"-O3\",\n            \"-fexceptions\",\n            \"-frtti\",\n            \"-Wno-deprecated-declarations\",\n        ],\n        headers = [\n            \"torch/csrc/jit/serialization/flatbuffer_serializer_jit.h\",\n        ],\n        srcs = [\n            \"torch/csrc/jit/serialization/flatbuffer_serializer_jit.cpp\",\n        ],\n        linker_flags = [\n            \"-Wl,--no-as-needed\",\n        ],\n        visibility = [\"PUBLIC\"],\n        deps = [\n            \":flatbuffer_loader\",\n            \":flatbuffers_serializer_mobile\",\n            \":torch_core\",\n            \":torch_mobile_module\",\n            C10,\n        ],\n    )\n\n    fb_xplat_cxx_library(\n        name = \"flatbuffers_jit\",\n        visibility = [\"PUBLIC\"],\n        exported_deps = [\n            \":flatbuffer_loader\",\n            \":flatbuffers_serializer_mobile\",\n            \":flatbuffers_serializer_jit\",\n        ],\n    )\n\n    fb_xplat_cxx_library(\n        name = \"flatbuffers_mobile\",\n        visibility = [\"PUBLIC\"],\n        exported_deps = [\n            \":flatbuffer_loader\",\n            \":flatbuffers_serializer_mobile\",\n            \":torch_mobile_train\",\n        ],\n    )\n\n    pt_xplat_cxx_library(\n        name = \"torch_supported_mobile_models\",\n        srcs = [\n            \"fb/supported_mobile_models/SupportedMobileModels.cpp\",\n        ] if NOT_OSS else [],\n        header_namespace = \"\",\n        exported_headers = [\"fb/supported_mobile_models/SupportedMobileModels.h\"] if NOT_OSS else [],\n        compiler_flags = get_pt_compiler_flags() + [\"-Wno-error\"],\n        exported_preprocessor_flags = get_pt_preprocessor_flags() + ([\"-DSYMBOLICATE_MOBILE_DEBUG_HANDLE\"] if get_enable_eager_symbolication() else []),\n        # @lint-ignore BUCKLINT link_whole\n        link_whole = True,\n        linker_flags = [\n            \"-Wl,--no-as-needed\",\n        ],\n        visibility = [\"PUBLIC\"],\n        deps = [],\n        exported_deps = [\n            \"//xplat/caffe2/fb/custom_ops/batch_box_cox:batch_box_cox\",\n            \"//xplat/caffe2/fb/custom_ops/maskrcnn:maskrcnn\",\n        ] if NOT_OSS else [],\n    )\n\n    fb_xplat_cxx_library(\n        name = \"static_runtime\",\n        srcs = [\n            \"torch/csrc/jit/runtime/static/fusion.cpp\",\n            \"torch/csrc/jit/runtime/static/generated_ops.cpp\",\n            \"torch/csrc/jit/runtime/static/impl.cpp\",\n            \"torch/csrc/jit/runtime/static/memory_planner.cpp\",\n            \"torch/csrc/jit/runtime/static/native_ops.cpp\",\n            \"torch/csrc/jit/runtime/static/ops.cpp\",\n            \"torch/csrc/jit/runtime/static/passes.cpp\",\n            \"torch/csrc/jit/runtime/static/te_wrapper.cpp\",\n        ],\n        compiler_flags = [\"-fexceptions\"],\n        labels = labels,\n        # @lint-ignore BUCKLINT link_whole\n        link_whole = True,\n        visibility = [\"PUBLIC\"],\n        windows_preferred_linkage = \"static\" if is_arvr_mode() else None,\n        deps = [\n            \":aten_cpu\",\n            \":caffe2_headers\",\n            \":torch_core\",\n            C10,\n        ],\n    )\n\n    # aten_cpu and aten_native_cpu\n    for name, srcs in [\n        (\"aten_cpu\", jit_core_sources + aten_cpu_source_list + [\n            # Generated\n            \":gen_aten[Functions.cpp]\",\n            \":gen_aten[Operators_0.cpp]\",\n            \":gen_aten[Operators_1.cpp]\",\n            \":gen_aten[Operators_2.cpp]\",\n            \":gen_aten[Operators_3.cpp]\",\n            \":gen_aten[Operators_4.cpp]\",\n            \":gen_aten[core/ATenOpList.cpp]\",\n            \":gen_aten[core/TensorMethods.cpp]\",\n            # Needed by ATen/native/EmbeddingBag.cpp\n            \"caffe2/perfkernels/embedding_lookup_idx.cc\",\n        ]),\n        (\"aten_native_cpu\", aten_native_source_list),\n    ]:\n        fb_xplat_cxx_library(\n            name = name,\n            srcs = srcs,\n            header_namespace = \"\",\n            # @lint-ignore BUCKLINT\n            link_whole = True,\n            visibility = [\"PUBLIC\"],\n            deps = [\n                third_party(\"omp\"),\n                third_party(\"cpuinfo\"),\n                third_party(\"glog\"),\n                third_party(\"XNNPACK\"),\n                third_party(\"pocketfft\"),\n            ] + select({\n                \"DEFAULT\": [],\n                \"ovr_config//runtime:fbcode-arm64\": [\n                    third_party(\"sleef_arm\"),\n                ],\n            }),\n            compiler_flags = get_aten_compiler_flags(),\n            exported_preprocessor_flags = get_aten_preprocessor_flags(),\n            exported_deps = [\n                \":aten_header\",\n                \":caffe2_headers\",\n                \":common_core\",\n                \":generated_aten_config_header\",\n                \":generated_aten_headers_cpu\",\n                \":jit_core_headers\",\n                \":pthreadpool\",\n                third_party(\"fmt\"),\n                third_party(\"ruy\"),\n                C10,\n                ROOT_PATH + \"aten/src/ATen/native/quantized/cpu/qnnpack:pytorch_qnnpack\",\n            ],\n            labels = labels,\n            **aten_default_args\n        )\n\n    fb_xplat_cxx_library(\n        name = \"lean_runtime_with_flatbuffer\",\n        srcs = [\n            \"aten/src/ATen/core/DeprecatedTypePropertiesRegistry.cpp\",\n            \"torch/csrc/jit/mobile/import.cpp\",\n            \"torch/csrc/jit/mobile/module.cpp\",\n            \"torch/csrc/jit/mobile/observer.cpp\",\n            \"torch/csrc/jit/serialization/import_read.cpp\",\n        ],\n        header_namespace = \"\",\n        exported_headers = subdir_glob(\n            [\n                (\"\", \"torch/csrc/jit/ir/*.h\"),\n                (\"\", \"caffe2/serialize/*.h\"),\n                (\"\", \"caffe2/utils/*.h\"),\n                (\"\", \"caffe2/core/*.h\"),\n                (\"\", \"torch/csrc/*.h\"),\n                (\"\", \"torch/csrc/api/include/torch/*.h\"),\n                (\"\", \"torch/csrc/autograd/*.h\"),\n                (\"\", \"torch/csrc/autograd/*/*.h\"),\n                (\"\", \"torch/csrc/jit/api/*.h\"),\n                (\"\", \"torch/csrc/jit/backends/*.h\"),\n                (\"\", \"torch/csrc/jit/mobile/*.h\"),\n                (\"\", \"torch/csrc/jit/runtime/*.h\"),\n                (\"\", \"torch/csrc/jit/passes/*.h\"),\n                (\"\", \"torch/csrc/jit/python/*.h\"),\n                (\"\", \"torch/csrc/jit/frontend/*.h\"),\n                (\"\", \"torch/csrc/jit/serialization/*.h\"),\n                (\"\", \"torch/csrc/profiler/**/*.h\"),\n                (\"\", \"torch/csrc/utils/*.h\"),\n                (\"\", \"aten/src/ATen/quantized/*.h\"),\n            ] + ([\n                (\"third_party/miniz-3.0.2\", \"*.h\"),\n            ] if NOT_OSS else []),\n            exclude = [\n                \"torch/csrc/jit/serialization/mobile_bytecode_generated.h\",\n            ],\n        ),\n        compiler_flags = get_pt_compiler_flags() + select({\n            \"DEFAULT\": [],\n            \"ovr_config//os:xtensa-xos\": [\n                \"-fdata-sections\",\n                \"-ffunction-sections\",\n            ],\n        }),\n        exported_preprocessor_flags = get_pt_preprocessor_flags() + [\n            \"-DMIN_EDGE_RUNTIME\",\n        ],\n        linker_flags = [\n            \"-Wl,--no-as-needed\",\n        ] + select({\n            \"DEFAULT\": [],\n            \"ovr_config//os:macos\": [\n                \"-dead_strip\",\n            ],\n            \"ovr_config//os:xtensa-xos\": [\n                \"-Wl,--gc-sections\",\n            ],\n        }),\n        visibility = [\"PUBLIC\"],\n        exported_deps = [\n            \":lean_runtime_with_tensor\",\n        ],\n    )\n\n    pt_xplat_cxx_library(\n        name = \"lean_runtime_with_tensor\",\n        srcs = [\n            \"aten/src/ATen/Context.cpp\",\n            \"aten/src/ATen/EmptyTensor.cpp\",\n            \"aten/src/ATen/Utils.cpp\",\n            \"aten/src/ATen/detail/CUDAHooksInterface.cpp\",\n            \"aten/src/ATen/detail/PrivateUse1HooksInterface.cpp\",\n            \":gen_aten[Operators_0.cpp]\",\n            \":gen_aten[Operators_1.cpp]\",\n            \":gen_aten[Operators_2.cpp]\",\n            \":gen_aten[Operators_3.cpp]\",\n            \":gen_aten[Operators_4.cpp]\",\n            \":gen_aten[core/TensorMethods.cpp]\",\n        ],\n        header_namespace = \"\",\n        exported_headers = [\n            \"torch/csrc/jit/runtime/custom_operator.h\",\n            \":gen_aten[core/TensorBody.h]\",\n        ],\n        compiler_flags = get_pt_compiler_flags() + select({\n            \"DEFAULT\": [],\n            \"ovr_config//os:xtensa-xos\": [\n                \"-fdata-sections\",\n                \"-ffunction-sections\",\n            ],\n        }),\n        exported_preprocessor_flags = get_pt_preprocessor_flags() + [\"-DMIN_EDGE_RUNTIME\"] + select({\n            \"DEFAULT\": [],\n            \"ovr_config//os:xtensa-xos\": [\n                \"-Dthread_local=\",\n            ],\n        }),\n        # @lint-ignore BUCKLINT link_whole\n        link_whole = True,\n        linker_flags = [\n            \"-Wl,--no-as-needed\",\n        ],\n        visibility = [\"PUBLIC\"],\n        exported_deps = [\n            \":generated_aten_config_header\",\n            \":lean_runtime_with_op\",\n            \":aten_header\",\n            C10,\n        ] + ([\"//xplat/caffe2/fb/embedded:experimental\"] if NOT_OSS else []),\n    )\n\n    pt_xplat_cxx_library(\n        name = \"lean_runtime_with_op\",\n        srcs = [\n            \"aten/src/ATen/SequenceNumber.cpp\",\n            \"aten/src/ATen/core/boxing/KernelFunction.cpp\",\n            \"aten/src/ATen/core/custom_class.cpp\",\n            \"aten/src/ATen/core/dispatch/DispatchKeyExtractor.cpp\",\n            \"aten/src/ATen/core/dispatch/Dispatcher.cpp\",\n            \"aten/src/ATen/core/dispatch/ObservedOperators.cpp\",\n            \"aten/src/ATen/core/dispatch/OperatorEntry.cpp\",\n            \"aten/src/ATen/core/PythonOpRegistrationTrampoline.cpp\",\n            \"aten/src/ATen/core/interned_strings.cpp\",\n            \"aten/src/ATen/core/library.cpp\",\n            \"aten/src/ATen/core/op_registration/infer_schema.cpp\",\n            \"aten/src/ATen/core/function_schema.cpp\",\n            \"aten/src/ATen/core/operator_name.cpp\",\n            \"aten/src/ATen/core/register_symbols.cpp\",\n            \"aten/src/ATen/core/tensor_type.cpp\",\n            \"aten/src/ATen/core/union_type.cpp\",\n            \"aten/src/ATen/record_function.cpp\",\n            \"torch/csrc/jit/frontend/edit_distance.cpp\",\n            \"torch/csrc/jit/frontend/error_report.cpp\",\n            \"torch/csrc/jit/frontend/function_schema_parser.cpp\",\n            \"torch/csrc/jit/frontend/lexer.cpp\",\n            \"torch/csrc/jit/frontend/schema_type_parser.cpp\",\n            \"torch/csrc/jit/frontend/source_range.cpp\",\n            \"torch/csrc/jit/frontend/strtod.cpp\",\n            \"torch/csrc/jit/mobile/parse_operators.cpp\",\n            \"torch/csrc/jit/mobile/prim_ops_registery.cpp\",\n            \"torch/csrc/jit/runtime/operator.cpp\",\n            \"torch/csrc/jit/runtime/slice_indices_adjust.cpp\",\n        ],\n        header_namespace = \"\",\n        exported_headers = [\n            \"torch/csrc/jit/frontend/edit_distance.h\",\n            \"torch/csrc/jit/runtime/slice_indices_adjust.h\",\n        ],\n        compiler_flags = get_pt_compiler_flags() + select({\n            \"DEFAULT\": [],\n            \"ovr_config//os:xtensa-xos\": [\n                \"-fdata-sections\",\n                \"-ffunction-sections\",\n            ],\n        }),\n        exported_preprocessor_flags = get_pt_preprocessor_flags() + [\"-DMIN_EDGE_RUNTIME\"] + select({\n            \"DEFAULT\": [],\n            \"ovr_config//os:xtensa-xos\": [\n                \"-Dthread_local=\",\n            ],\n        }),\n        # @lint-ignore BUCKLINT link_whole\n        link_whole = True,\n        linker_flags = [\n            \"-Wl,--no-as-needed\",\n        ],\n        visibility = [\"PUBLIC\"],\n        exported_deps = [\n            \":min_runtime_lib\",\n            C10,\n        ],\n    )\n\n    pt_xplat_cxx_library(\n        name = \"min_runtime_lib\",\n        srcs = [\n            \"aten/src/ATen/ScalarOps.cpp\",\n            \"aten/src/ATen/core/Dict.cpp\",\n            \"aten/src/ATen/core/List.cpp\",\n            \"aten/src/ATen/core/class_type.cpp\",\n            \"aten/src/ATen/core/dynamic_type.cpp\",\n            \"aten/src/ATen/core/ivalue.cpp\",\n            \"aten/src/ATen/core/type.cpp\",\n            \"aten/src/ATen/core/type_factory.cpp\",\n            \"aten/src/ATen/native/prim_native_functions.cpp\",\n            \"torch/csrc/jit/mobile/function.cpp\",\n            \"torch/csrc/jit/mobile/interpreter.cpp\",\n            \"torch/csrc/jit/mobile/parse_bytecode.cpp\",\n            \"torch/csrc/jit/mobile/promoted_prim_ops.cpp\",\n            \"torch/csrc/jit/mobile/register_ops_common_utils.cpp\",\n            \"torch/csrc/jit/mobile/type_parser.cpp\",\n            \"torch/csrc/jit/runtime/instruction.cpp\",\n            \"torch/csrc/jit/runtime/jit_exception.cpp\",\n            \"torch/csrc/jit/runtime/vararg_functions.cpp\",\n        ],\n        header_namespace = \"\",\n        exported_headers = [\n            \"caffe2/serialize/versions.h\",\n            \"torch/csrc/jit/backends/backend_exception.h\",\n            \"torch/csrc/jit/mobile/register_ops_common_utils.h\",\n            \"torch/csrc/jit/runtime/instruction.h\",\n            \"torch/csrc/jit/runtime/jit_exception.h\",\n            \"torch/csrc/jit/runtime/operator.h\",\n            \"torch/csrc/jit/runtime/operator_options.h\",\n            \"torch/csrc/jit/runtime/vararg_functions.h\",\n            \"torch/csrc/jit/serialization/import_export_constants.h\",\n            \"torch/csrc/jit/serialization/import_export_functions.h\",\n        ],\n        compiler_flags = get_pt_compiler_flags() + select({\n            \"DEFAULT\": [],\n            \"ovr_config//os:xtensa-xos\": [\n                \"-fexceptions\",\n                \"-fdata-sections\",\n                \"-ffunction-sections\",\n            ],\n        }),\n        exported_preprocessor_flags = get_pt_preprocessor_flags() + [\"-DMIN_EDGE_RUNTIME\"] + select({\n            \"DEFAULT\": [],\n            \"ovr_config//os:xtensa-xos\": [\n                \"-Dthread_local=\",\n            ],\n        }),\n        # @lint-ignore BUCKLINT link_whole\n        link_whole = True,\n        linker_flags = [\n            \"-Wl,--no-as-needed\",\n        ],\n        visibility = [\"PUBLIC\"],\n        exported_deps = [\n            \":aten_header\",\n            \":generated_aten_headers_cpu\",\n            \":jit_core_headers\",\n            \":torch_mobile_headers\",\n            C10,\n        ],\n    )\n"
        },
        {
          "name": "build.bzl",
          "type": "blob",
          "size": 11.33,
          "content": "load(\n    \":ufunc_defs.bzl\",\n    \"aten_ufunc_generated_cpu_kernel_sources\",\n    \"aten_ufunc_generated_cpu_sources\",\n    \"aten_ufunc_generated_cuda_sources\",\n)\n\ndef define_targets(rules):\n    rules.cc_library(\n        name = \"caffe2_core_macros\",\n        hdrs = [\":caffe2_core_macros_h\"],\n    )\n\n    rules.cmake_configure_file(\n        name = \"caffe2_core_macros_h\",\n        src = \"caffe2/core/macros.h.in\",\n        out = \"caffe2/core/macros.h\",\n        definitions = [\n            \"CAFFE2_BUILD_SHARED_LIBS\",\n            \"CAFFE2_PERF_WITH_AVX\",\n            \"CAFFE2_PERF_WITH_AVX2\",\n            \"CAFFE2_USE_EXCEPTION_PTR\",\n            \"CAFFE2_USE_CUDNN\",\n            \"USE_MKLDNN\",\n            \"CAFFE2_USE_ITT\",\n            \"USE_ROCM_KERNEL_ASSERT\",\n            \"EIGEN_MPL2_ONLY\",\n        ],\n    )\n\n    rules.cc_library(\n        name = \"caffe2_serialize\",\n        srcs = [\n            \"caffe2/serialize/file_adapter.cc\",\n            \"caffe2/serialize/inline_container.cc\",\n            \"caffe2/serialize/istream_adapter.cc\",\n            \"caffe2/serialize/read_adapter_interface.cc\",\n        ],\n        copts = [\"-fexceptions\", \"-DFBCODE_CAFFE2\"],\n        tags = [\n            \"-fbcode\",\n            \"supermodule:android/default/pytorch\",\n            \"supermodule:ios/default/public.pytorch\",\n            \"xplat\",\n        ],\n        visibility = [\"//visibility:public\"],\n        deps = [\n            \":caffe2_headers\",\n            \"//c10\",\n            \"//third_party/miniz-3.0.2:miniz\",\n            \"@com_github_glog//:glog\",\n        ],\n    )\n\n    #\n    # ATen generated code\n    # You need to keep this is sync with the files written out\n    # by gen.py (in the cmake build system, we track generated files\n    # via generated_cpp.txt and generated_cpp.txt-cuda\n    #\n    # Sure would be nice to use gen.py to create this list dynamically\n    # instead of hardcoding, no? Well, we can't, as discussed in this\n    # thread:\n    # https://fb.facebook.com/groups/askbuck/permalink/1924258337622772/\n\n    gen_aten_srcs = [\n        \"aten/src/ATen/native/native_functions.yaml\",\n        \"aten/src/ATen/native/tags.yaml\",\n    ] + rules.glob([\"aten/src/ATen/templates/*\"])\n\n    gen_aten_cmd = \" \".join([\n        \"$(execpath //torchgen:gen)\",\n        \"--install_dir=$(RULEDIR)\",\n        \"--source-path aten/src/ATen\",\n        \"--aoti_install_dir=$(RULEDIR)/torch/csrc/inductor/aoti_torch/generated\"\n    ] + ([\"--static_dispatch_backend CPU\"] if rules.is_cpu_static_dispatch_build() else []))\n\n    gen_aten_outs_cuda = (\n        GENERATED_H_CUDA + GENERATED_CPP_CUDA + GENERATED_AOTI_CUDA_CPP +\n        aten_ufunc_generated_cuda_sources()\n    )\n\n    gen_aten_outs = (\n        GENERATED_H + GENERATED_H_CORE +\n        GENERATED_CPP + GENERATED_CPP_CORE +\n        GENERATED_AOTI_CPP +\n        aten_ufunc_generated_cpu_sources() +\n        aten_ufunc_generated_cpu_kernel_sources() + [\n            \"Declarations.yaml\",\n        ] + gen_aten_outs_cuda\n    )\n\n    rules.genrule(\n        name = \"gen_aten\",\n        srcs = gen_aten_srcs,\n        outs = gen_aten_outs,\n        cmd = gen_aten_cmd,\n        tools = [\"//torchgen:gen\"],\n    )\n\n    rules.genrule(\n        name = \"gen_aten_hip\",\n        srcs = gen_aten_srcs,\n        outs = gen_aten_outs_cuda,\n        cmd = gen_aten_cmd + \" --rocm\",\n        features = [\"-create_bazel_outputs\"],\n        tags = [\"-bazel\"],\n        tools = [\"//torchgen:gen\"],\n    )\n\n    rules.genrule(\n        name = \"generate-code\",\n        srcs = [\n            \":DispatchKeyNativeFunctions.cpp\",\n            \":DispatchKeyNativeFunctions.h\",\n            \":LazyIr.h\",\n            \":LazyNonNativeIr.h\",\n            \":RegisterDispatchDefinitions.ini\",\n            \":RegisterDispatchKey.cpp\",\n            \":native_functions.yaml\",\n            \":shape_inference.h\",\n            \":tags.yaml\",\n            \":ts_native_functions.cpp\",\n            \":ts_native_functions.yaml\",\n        ],\n        outs = GENERATED_AUTOGRAD_CPP + GENERATED_AUTOGRAD_PYTHON + GENERATED_TESTING_PY,\n        cmd = \"$(execpath //tools/setup_helpers:generate_code) \" +\n              \"--gen-dir=$(RULEDIR) \" +\n              \"--native-functions-path $(location :native_functions.yaml) \" +\n              \"--tags-path=$(location :tags.yaml) \" +\n              \"--gen_lazy_ts_backend\",\n        tools = [\"//tools/setup_helpers:generate_code\"],\n    )\n\n    rules.cc_library(\n        name = \"generated-autograd-headers\",\n        hdrs = [\":{}\".format(h) for h in _GENERATED_AUTOGRAD_CPP_HEADERS + _GENERATED_AUTOGRAD_PYTHON_HEADERS],\n        visibility = [\"//visibility:public\"],\n    )\n\n    rules.genrule(\n        name = \"version_h\",\n        srcs = [\n            \":torch/csrc/api/include/torch/version.h.in\",\n            \":version.txt\",\n        ],\n        outs = [\"torch/csrc/api/include/torch/version.h\"],\n        cmd = \"$(execpath //tools/setup_helpers:gen_version_header) \" +\n              \"--template-path $(location :torch/csrc/api/include/torch/version.h.in) \" +\n              \"--version-path $(location :version.txt) --output-path $@ \",\n        tools = [\"//tools/setup_helpers:gen_version_header\"],\n    )\n\n#\n# ATen generated code\n# You need to keep this is sync with the files written out\n# by gen.py (in the cmake build system, we track generated files\n# via generated_cpp.txt and generated_cpp.txt-cuda\n#\n# Sure would be nice to use gen.py to create this list dynamically\n# instead of hardcoding, no? Well, we can't, as discussed in this\n# thread:\n# https://fb.facebook.com/groups/askbuck/permalink/1924258337622772/\n\nGENERATED_H = [\n    \"Functions.h\",\n    \"NativeFunctions.h\",\n    \"NativeMetaFunctions.h\",\n    \"FunctionalInverses.h\",\n    \"RedispatchFunctions.h\",\n    \"RegistrationDeclarations.h\",\n    \"VmapGeneratedPlumbing.h\",\n]\n\nGENERATED_H_CORE = [\n    \"Operators.h\",\n    # CPUFunctions.h (and likely similar headers) need to be part of core because\n    # of the static dispatch build: TensorBody.h directly includes CPUFunctions.h.\n    # The disinction looks pretty arbitrary though; maybe will can kill core\n    # and merge the two?\n    \"CPUFunctions.h\",\n    \"CPUFunctions_inl.h\",\n    \"CompositeExplicitAutogradFunctions.h\",\n    \"CompositeExplicitAutogradFunctions_inl.h\",\n    \"CompositeExplicitAutogradNonFunctionalFunctions.h\",\n    \"CompositeExplicitAutogradNonFunctionalFunctions_inl.h\",\n    \"CompositeImplicitAutogradFunctions.h\",\n    \"CompositeImplicitAutogradFunctions_inl.h\",\n    \"CompositeImplicitAutogradNestedTensorFunctions.h\",\n    \"CompositeImplicitAutogradNestedTensorFunctions_inl.h\",\n    \"MetaFunctions.h\",\n    \"MetaFunctions_inl.h\",\n    \"core/TensorBody.h\",\n    \"MethodOperators.h\",\n    \"core/aten_interned_strings.h\",\n    \"core/enum_tag.h\",\n]\n\nGENERATED_H_CUDA = [\n    \"CUDAFunctions.h\",\n    \"CUDAFunctions_inl.h\",\n]\n\nGENERATED_CPP_CUDA = [\n    \"RegisterCUDA_0.cpp\",\n    \"RegisterNestedTensorCUDA_0.cpp\",\n    \"RegisterSparseCUDA_0.cpp\",\n    \"RegisterSparseCsrCUDA_0.cpp\",\n    \"RegisterQuantizedCUDA_0.cpp\",\n]\n\nGENERATED_CPP = [\n    \"Functions.cpp\",\n    \"RegisterBackendSelect.cpp\",\n    \"RegisterCPU_0.cpp\",\n    \"RegisterCPU_1.cpp\",\n    \"RegisterCPU_2.cpp\",\n    \"RegisterCPU_3.cpp\",\n    \"RegisterQuantizedCPU_0.cpp\",\n    \"RegisterNestedTensorCPU_0.cpp\",\n    \"RegisterSparseCPU_0.cpp\",\n    \"RegisterSparseCsrCPU_0.cpp\",\n    \"RegisterMkldnnCPU_0.cpp\",\n    \"RegisterCompositeImplicitAutograd_0.cpp\",\n    \"RegisterCompositeImplicitAutogradNestedTensor_0.cpp\",\n    \"RegisterZeroTensor_0.cpp\",\n    \"RegisterMeta_0.cpp\",\n    \"RegisterQuantizedMeta_0.cpp\",\n    \"RegisterNestedTensorMeta_0.cpp\",\n    \"RegisterSparseMeta_0.cpp\",\n    \"RegisterCompositeExplicitAutograd_0.cpp\",\n    \"RegisterCompositeExplicitAutogradNonFunctional_0.cpp\",\n    \"CompositeViewCopyKernels.cpp\",\n    \"RegisterSchema.cpp\",\n    \"RegisterFunctionalization_0.cpp\",\n    \"RegisterFunctionalization_1.cpp\",\n    \"RegisterFunctionalization_2.cpp\",\n    \"RegisterFunctionalization_3.cpp\",\n]\n\nGENERATED_CPP_CORE = [\n    \"Operators_0.cpp\",\n    \"Operators_1.cpp\",\n    \"Operators_2.cpp\",\n    \"Operators_3.cpp\",\n    \"Operators_4.cpp\",\n    \"core/ATenOpList.cpp\",\n    \"core/TensorMethods.cpp\",\n]\n\n# These lists are temporarily living in and exported from the shared\n# structure so that an internal build that lives under a different\n# root can access them. These could technically live in a separate\n# file in the same directory but that would require extra work to\n# ensure that file is synced to both Meta internal repositories and\n# GitHub. This problem will go away when the targets downstream of\n# generate-code that use these lists are moved into the shared\n# structure as well.\n\n_GENERATED_AUTOGRAD_PYTHON_HEADERS = [\n    \"torch/csrc/autograd/generated/python_functions.h\",\n    \"torch/csrc/autograd/generated/python_return_types.h\",\n]\n\n_GENERATED_AUTOGRAD_CPP_HEADERS = [\n    \"torch/csrc/autograd/generated/Functions.h\",\n    \"torch/csrc/autograd/generated/VariableType.h\",\n    \"torch/csrc/autograd/generated/ViewFuncs.h\",\n    \"torch/csrc/autograd/generated/variable_factories.h\",\n]\n\nGENERATED_TESTING_PY = [\n    \"torch/testing/_internal/generated/annotated_fn_args.py\",\n]\n\nGENERATED_LAZY_H = [\n    \"torch/csrc/lazy/generated/LazyIr.h\",\n    \"torch/csrc/lazy/generated/LazyNonNativeIr.h\",\n    \"torch/csrc/lazy/generated/LazyNativeFunctions.h\",\n]\n\n_GENERATED_AUTOGRAD_PYTHON_CPP = [\n    \"torch/csrc/autograd/generated/python_functions_0.cpp\",\n    \"torch/csrc/autograd/generated/python_functions_1.cpp\",\n    \"torch/csrc/autograd/generated/python_functions_2.cpp\",\n    \"torch/csrc/autograd/generated/python_functions_3.cpp\",\n    \"torch/csrc/autograd/generated/python_functions_4.cpp\",\n    \"torch/csrc/autograd/generated/python_nn_functions.cpp\",\n    \"torch/csrc/autograd/generated/python_nested_functions.cpp\",\n    \"torch/csrc/autograd/generated/python_fft_functions.cpp\",\n    \"torch/csrc/autograd/generated/python_linalg_functions.cpp\",\n    \"torch/csrc/autograd/generated/python_return_types.cpp\",\n    \"torch/csrc/autograd/generated/python_enum_tag.cpp\",\n    \"torch/csrc/autograd/generated/python_sparse_functions.cpp\",\n    \"torch/csrc/autograd/generated/python_special_functions.cpp\",\n    \"torch/csrc/autograd/generated/python_torch_functions_0.cpp\",\n    \"torch/csrc/autograd/generated/python_torch_functions_1.cpp\",\n    \"torch/csrc/autograd/generated/python_torch_functions_2.cpp\",\n    \"torch/csrc/autograd/generated/python_variable_methods.cpp\",\n]\n\nGENERATED_AUTOGRAD_PYTHON = _GENERATED_AUTOGRAD_PYTHON_HEADERS + _GENERATED_AUTOGRAD_PYTHON_CPP\n\nGENERATED_AUTOGRAD_CPP = [\n    \"torch/csrc/autograd/generated/Functions.cpp\",\n    \"torch/csrc/autograd/generated/VariableType_0.cpp\",\n    \"torch/csrc/autograd/generated/VariableType_1.cpp\",\n    \"torch/csrc/autograd/generated/VariableType_2.cpp\",\n    \"torch/csrc/autograd/generated/VariableType_3.cpp\",\n    \"torch/csrc/autograd/generated/VariableType_4.cpp\",\n    \"torch/csrc/autograd/generated/ViewFuncs.cpp\",\n    \"torch/csrc/autograd/generated/TraceType_0.cpp\",\n    \"torch/csrc/autograd/generated/TraceType_1.cpp\",\n    \"torch/csrc/autograd/generated/TraceType_2.cpp\",\n    \"torch/csrc/autograd/generated/TraceType_3.cpp\",\n    \"torch/csrc/autograd/generated/TraceType_4.cpp\",\n    \"torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp\",\n    \"torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp\",\n    \"torch/csrc/lazy/generated/LazyNativeFunctions.cpp\",\n    \"torch/csrc/lazy/generated/RegisterAutogradLazy.cpp\",\n    \"torch/csrc/lazy/generated/RegisterLazy.cpp\",\n] + _GENERATED_AUTOGRAD_CPP_HEADERS + GENERATED_LAZY_H\n\nGENERATED_AOTI_CPP = [\n    \"torch/csrc/inductor/aoti_torch/generated/c_shim_cpu.cpp\",\n]\n\nGENERATED_AOTI_CUDA_CPP = [\n    \"torch/csrc/inductor/aoti_torch/generated/c_shim_cuda.cpp\",\n]\n"
        },
        {
          "name": "build_variables.bzl",
          "type": "blob",
          "size": 69.8,
          "content": "# WARNING: the contents of this file must BOTH be valid Starlark (for Buck and\n\n# Bazel) as well as valid Python (for our cmake build).  This means that\n# load() directives are not allowed (as they are not recognized by Python).\n# If you want to fix this, figure out how run this file from cmake with a proper\n# Starlark interpreter as part of the default OSS build process.  If you need\n# some nontrivial Starlark features, make a separate bzl file (remember that\n\n# bzl files are not exported via ShipIt by default, so you may also need to\n# update PyTorch's ShipIt config)\n\n# This is duplicated in caffe2/CMakeLists.txt for now and not yet used in buck\nGENERATED_LAZY_TS_CPP = [\n    \"lazy/generated/LazyNativeFunctions.cpp\",\n    \"lazy/generated/RegisterAutogradLazy.cpp\",\n    \"lazy/generated/RegisterLazy.cpp\",\n]\n\ndef libtorch_generated_sources(gencode_pattern):\n    return [gencode_pattern.format(name) for name in [\n        \"torch/csrc/autograd/generated/Functions.cpp\",\n        \"torch/csrc/autograd/generated/VariableType_0.cpp\",\n        \"torch/csrc/autograd/generated/VariableType_1.cpp\",\n        \"torch/csrc/autograd/generated/VariableType_2.cpp\",\n        \"torch/csrc/autograd/generated/VariableType_3.cpp\",\n        \"torch/csrc/autograd/generated/VariableType_4.cpp\",\n        \"torch/csrc/autograd/generated/ViewFuncs.cpp\",\n        \"torch/csrc/autograd/generated/TraceType_0.cpp\",\n        \"torch/csrc/autograd/generated/TraceType_1.cpp\",\n        \"torch/csrc/autograd/generated/TraceType_2.cpp\",\n        \"torch/csrc/autograd/generated/TraceType_3.cpp\",\n        \"torch/csrc/autograd/generated/TraceType_4.cpp\",\n        \"torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp\",\n        \"torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp\",\n    ]]\n\n# copied from https://github.com/pytorch/pytorch/blob/f99a693cd9ff7a9b5fdc71357dac66b8192786d3/aten/src/ATen/core/CMakeLists.txt\njit_core_headers = [\n    \"torch/csrc/Export.h\",\n    \"torch/csrc/jit/frontend/source_range.h\",\n    \"torch/csrc/jit/serialization/callstack_debug_info_serialization.h\",\n    \"torch/csrc/jit/serialization/source_range_serialization.h\",\n    \"torch/csrc/jit/frontend/lexer.h\",\n    \"torch/csrc/jit/frontend/strtod.h\",\n    \"torch/csrc/jit/frontend/parser_constants.h\",\n    \"torch/csrc/jit/frontend/function_schema_parser.h\",\n    \"torch/csrc/jit/frontend/parse_string_literal.h\",\n    \"torch/csrc/jit/frontend/schema_type_parser.h\",\n    \"torch/csrc/jit/frontend/error_report.h\",\n    \"torch/csrc/jit/frontend/tree.h\",\n    \"torch/custom_class.h\",\n    \"torch/custom_class_detail.h\",\n    \"torch/library.h\",\n]\n\njit_core_sources = [\n    \"torch/csrc/jit/frontend/error_report.cpp\",\n    \"torch/csrc/jit/frontend/function_schema_parser.cpp\",\n    \"torch/csrc/jit/frontend/lexer.cpp\",\n    \"torch/csrc/jit/frontend/schema_type_parser.cpp\",\n    \"torch/csrc/jit/frontend/strtod.cpp\",\n    \"torch/csrc/jit/frontend/source_range.cpp\",\n]\n\n# copied from https://github.com/pytorch/pytorch/blob/0bde610c14b92d351b968a0228df29e92442b1cc/torch/CMakeLists.txt\n# There are some common files used in both internal lite-interpreter and full-jit. Making a separate\n# list for the shared files.\n\ncore_sources_common = [\n    \"torch/csrc/autograd/autograd_meta.cpp\",\n    \"torch/csrc/autograd/forward_grad.cpp\",\n    \"torch/csrc/jit/frontend/edit_distance.cpp\",\n    \"torch/csrc/jit/mobile/compatibility/runtime_compatibility.cpp\",\n    \"torch/csrc/jit/mobile/type_parser.cpp\",\n    \"torch/csrc/jit/operator_upgraders/version_map.cpp\",\n    \"torch/csrc/jit/runtime/instruction.cpp\",\n    \"torch/csrc/jit/runtime/jit_exception.cpp\",\n    \"torch/csrc/jit/runtime/operator.cpp\",\n    \"torch/csrc/jit/mobile/register_ops_common_utils.cpp\",\n    \"torch/csrc/jit/runtime/print_handler.cpp\",\n    \"torch/csrc/jit/runtime/slice_indices_adjust.cpp\",\n    \"torch/csrc/jit/runtime/register_ops_utils.cpp\",\n    \"torch/csrc/jit/runtime/vararg_functions.cpp\",\n    \"torch/csrc/jit/mobile/promoted_prim_ops.cpp\",\n    \"torch/csrc/jit/mobile/prim_ops_registery.cpp\",\n    \"torch/csrc/profiler/util.cpp\",\n]\n\ntorch_unpickler_common = [\n    \"torch/csrc/jit/serialization/import_read.cpp\",\n    \"torch/csrc/jit/serialization/unpickler.cpp\",\n]\n\nlibtorch_sources_common = sorted(core_sources_common + torch_unpickler_common)\n\n# The profilers are not needed in the lite interpreter build.\nlibtorch_profiler_sources = [\n    \"torch/csrc/autograd/profiler_legacy.cpp\",\n    \"torch/csrc/autograd/profiler_kineto.cpp\",\n    \"torch/csrc/profiler/collection.cpp\",\n    \"torch/csrc/profiler/data_flow.cpp\",\n    \"torch/csrc/profiler/kineto_shim.cpp\",\n    \"torch/csrc/profiler/kineto_client_interface.cpp\",\n    \"torch/csrc/profiler/orchestration/observer.cpp\",\n    \"torch/csrc/profiler/orchestration/python_tracer.cpp\",\n    \"torch/csrc/profiler/standalone/execution_trace_observer.cpp\",\n    \"torch/csrc/profiler/standalone/itt_observer.cpp\",\n    \"torch/csrc/profiler/standalone/nvtx_observer.cpp\",\n    \"torch/csrc/profiler/standalone/privateuse1_observer.cpp\",\n    \"torch/csrc/profiler/stubs/base.cpp\",\n    \"torch/csrc/profiler/orchestration/vulkan.cpp\",\n    \"torch/csrc/profiler/perf.cpp\",\n    \"torch/csrc/monitor/counters.cpp\",\n    \"torch/csrc/monitor/events.cpp\",\n]\n\nlibtorch_edge_profiler_sources = libtorch_profiler_sources + [\n    \"torch/csrc/jit/mobile/profiler_edge.cpp\",\n]\n\ncore_trainer_sources = [\n    \"torch/csrc/autograd/anomaly_mode.cpp\",\n    \"torch/csrc/autograd/autograd.cpp\",\n    \"torch/csrc/autograd/autograd_not_implemented_fallback.cpp\",\n    \"torch/csrc/autograd/cpp_hook.cpp\",\n    \"torch/csrc/autograd/custom_function.cpp\",\n    \"torch/csrc/autograd/variable_info.cpp\",\n    \"torch/csrc/autograd/engine.cpp\",\n    \"torch/csrc/autograd/function.cpp\",\n    \"torch/csrc/autograd/input_metadata.cpp\",\n    \"torch/csrc/autograd/functions/accumulate_grad.cpp\",\n    \"torch/csrc/autograd/functions/basic_ops.cpp\",\n    \"torch/csrc/autograd/functions/tensor.cpp\",\n    \"torch/csrc/autograd/functions/utils.cpp\",\n    \"torch/csrc/autograd/input_buffer.cpp\",\n    \"torch/csrc/autograd/record_function_ops.cpp\",\n    \"torch/csrc/autograd/saved_variable.cpp\",\n    \"torch/csrc/autograd/variable.cpp\",\n    \"torch/csrc/autograd/utils/warnings.cpp\",\n    \"torch/csrc/autograd/jit_decomp_interface.cpp\",\n    \"torch/csrc/jit/frontend/name_mangler.cpp\",\n    \"torch/csrc/jit/ir/type_hashing.cpp\",\n    \"torch/csrc/jit/serialization/pickler.cpp\",\n    \"torch/csrc/jit/serialization/type_name_uniquer.cpp\",\n]\n\ntorch_mobile_core = [\n    # backend_debug_info.cpp provides\n    # __torch__.torch.classes.backend.BackendDebugInfo class\n    # This should not be needed eventually.\n    # TODO: Remove this dependency\n    \"torch/csrc/jit/backends/backend_debug_info.cpp\",\n    \"torch/csrc/jit/mobile/compatibility/model_compatibility.cpp\",\n    \"torch/csrc/jit/mobile/function.cpp\",\n    \"torch/csrc/jit/mobile/import.cpp\",\n    \"torch/csrc/jit/mobile/flatbuffer_loader.cpp\",\n    \"torch/csrc/jit/mobile/interpreter.cpp\",\n    \"torch/csrc/jit/mobile/module.cpp\",\n    \"torch/csrc/jit/mobile/observer.cpp\",\n    \"torch/csrc/jit/mobile/parse_bytecode.cpp\",\n    \"torch/csrc/jit/mobile/parse_operators.cpp\",\n    \"torch/csrc/jit/mobile/quantization.cpp\",\n    \"torch/csrc/jit/mobile/upgrader_mobile.cpp\",\n    \"torch/csrc/jit/runtime/register_prim_ops.cpp\",\n    \"torch/csrc/jit/runtime/register_special_ops.cpp\",\n]\n\ncore_sources_full_mobile_no_backend_interface_xplat = [\n    \"torch/csrc/jit/api/function_impl.cpp\",\n    \"torch/csrc/jit/api/module.cpp\",\n    \"torch/csrc/jit/api/object.cpp\",\n    \"torch/csrc/jit/backends/backend_debug_handler.cpp\",\n    \"torch/csrc/jit/backends/backend_detail.cpp\",\n    \"torch/csrc/jit/backends/backend_resolver.cpp\",\n    \"torch/csrc/jit/codegen/fuser/codegen.cpp\",\n    \"torch/csrc/jit/codegen/fuser/compiler.cpp\",\n    \"torch/csrc/jit/codegen/fuser/executor.cpp\",\n    \"torch/csrc/jit/codegen/fuser/fallback.cpp\",\n    \"torch/csrc/jit/codegen/fuser/interface.cpp\",\n    \"torch/csrc/jit/codegen/fuser/kernel_cache.cpp\",\n    \"torch/csrc/jit/frontend/builtin_functions.cpp\",\n    \"torch/csrc/jit/frontend/versioned_symbols.cpp\",\n    \"torch/csrc/jit/frontend/canonicalize_modified_loop.cpp\",\n    \"torch/csrc/jit/frontend/convert_to_ssa.cpp\",\n    \"torch/csrc/jit/frontend/exit_transforms.cpp\",\n    \"torch/csrc/jit/frontend/inline_loop_condition.cpp\",\n    \"torch/csrc/jit/frontend/ir_emitter.cpp\",\n    \"torch/csrc/jit/frontend/parser.cpp\",\n    \"torch/csrc/jit/frontend/schema_matching.cpp\",\n    \"torch/csrc/jit/frontend/script_type_parser.cpp\",\n    \"torch/csrc/jit/frontend/sugared_value.cpp\",\n    \"torch/csrc/jit/frontend/tracer.cpp\",\n    \"torch/csrc/jit/ir/alias_analysis.cpp\",\n    \"torch/csrc/jit/ir/attributes.cpp\",\n    \"torch/csrc/jit/ir/constants.cpp\",\n    \"torch/csrc/jit/ir/ir.cpp\",\n    \"torch/csrc/jit/ir/irparser.cpp\",\n    \"torch/csrc/jit/ir/node_hashing.cpp\",\n    \"torch/csrc/jit/ir/scope.cpp\",\n    \"torch/csrc/jit/ir/subgraph_matcher.cpp\",\n    \"torch/csrc/jit/ir/graph_utils.cpp\",\n    \"torch/csrc/jit/jit_log.cpp\",\n    \"torch/csrc/jit/jit_opt_limit.cpp\",\n    \"torch/csrc/jit/mobile/nnc/aot_compiler.cpp\",\n    \"torch/csrc/jit/mobile/nnc/backend.cpp\",\n    \"torch/csrc/jit/mobile/nnc/context.cpp\",\n    \"torch/csrc/jit/mobile/nnc/registry.cpp\",\n    \"torch/csrc/jit/operator_upgraders/utils.cpp\",\n    \"torch/csrc/jit/operator_upgraders/upgraders.cpp\",\n    \"torch/csrc/jit/operator_upgraders/upgraders_entry.cpp\",\n    \"torch/csrc/jit/passes/add_if_then_else.cpp\",\n    \"torch/csrc/jit/passes/annotate_warns.cpp\",\n    \"torch/csrc/jit/passes/bailout_graph.cpp\",\n    \"torch/csrc/jit/passes/check_strict_fusion.cpp\",\n    \"torch/csrc/jit/passes/batch_mm.cpp\",\n    \"torch/csrc/jit/passes/canonicalize.cpp\",\n    \"torch/csrc/jit/passes/canonicalize_graph_fuser_ops.cpp\",\n    \"torch/csrc/jit/passes/clear_profiling.cpp\",\n    \"torch/csrc/jit/passes/clear_undefinedness.cpp\",\n    \"torch/csrc/jit/passes/common_subexpression_elimination.cpp\",\n    \"torch/csrc/jit/passes/concat_opt.cpp\",\n    \"torch/csrc/jit/passes/constant_pooling.cpp\",\n    \"torch/csrc/jit/passes/constant_propagation.cpp\",\n    \"torch/csrc/jit/passes/restore_mutation.cpp\",\n    \"torch/csrc/jit/passes/create_autodiff_subgraphs.cpp\",\n    \"torch/csrc/jit/passes/dead_code_elimination.cpp\",\n    \"torch/csrc/jit/passes/eliminate_no_ops.cpp\",\n    \"torch/csrc/jit/passes/remove_redundant_profiles.cpp\",\n    \"torch/csrc/jit/passes/remove_exceptions.cpp\",\n    \"torch/csrc/jit/passes/decompose_ops.cpp\",\n    \"torch/csrc/jit/passes/dtype_analysis.cpp\",\n    \"torch/csrc/jit/passes/device_type_analysis.cpp\",\n    \"torch/csrc/jit/passes/erase_number_types.cpp\",\n    \"torch/csrc/jit/passes/fixup_trace_scope_blocks.cpp\",\n    \"torch/csrc/jit/passes/freeze_module.cpp\",\n    \"torch/csrc/jit/passes/fuse_linear.cpp\",\n    \"torch/csrc/jit/passes/fuse_relu.cpp\",\n    \"torch/csrc/jit/passes/graph_fuser.cpp\",\n    \"torch/csrc/jit/passes/graph_rewrite_helper.cpp\",\n    \"torch/csrc/jit/passes/guard_elimination.cpp\",\n    \"torch/csrc/jit/passes/hoist_conv_packed_params.cpp\",\n    \"torch/csrc/jit/passes/inline_autodiff_subgraphs.cpp\",\n    \"torch/csrc/jit/passes/inline_forked_closures.cpp\",\n    \"torch/csrc/jit/passes/inline_fork_wait.cpp\",\n    \"torch/csrc/jit/passes/inliner.cpp\",\n    \"torch/csrc/jit/passes/inplace_check.cpp\",\n    \"torch/csrc/jit/passes/insert_guards.cpp\",\n    \"torch/csrc/jit/passes/lift_closures.cpp\",\n    \"torch/csrc/jit/passes/liveness.cpp\",\n    \"torch/csrc/jit/passes/loop_unrolling.cpp\",\n    \"torch/csrc/jit/passes/lower_grad_of.cpp\",\n    \"torch/csrc/jit/passes/lower_tuples.cpp\",\n    \"torch/csrc/jit/passes/normalize_ops.cpp\",\n    \"torch/csrc/jit/passes/peephole_dict_idioms.cpp\",\n    \"torch/csrc/jit/passes/peephole_list_idioms.cpp\",\n    \"torch/csrc/jit/passes/value_refinement_utils.cpp\",\n    \"torch/csrc/jit/passes/peephole_alias_sensitive.cpp\",\n    \"torch/csrc/jit/passes/pass_manager.cpp\",\n    \"torch/csrc/jit/passes/peephole.cpp\",\n    \"torch/csrc/jit/passes/peephole_non_tensor.cpp\",\n    \"torch/csrc/jit/passes/create_functional_graphs.cpp\",\n    \"torch/csrc/jit/passes/refine_tuple_types.cpp\",\n    \"torch/csrc/jit/passes/remove_mutation.cpp\",\n    \"torch/csrc/jit/passes/prepack_folding.cpp\",\n    \"torch/csrc/jit/passes/fold_conv_bn.cpp\",\n    \"torch/csrc/jit/passes/fold_linear_bn.cpp\",\n    \"torch/csrc/jit/passes/dbr_quantization/remove_redundant_aliases.cpp\",\n    \"torch/csrc/jit/passes/frozen_concat_linear.cpp\",\n    \"torch/csrc/jit/passes/frozen_conv_add_relu_fusion.cpp\",\n    \"torch/csrc/jit/passes/frozen_conv_folding.cpp\",\n    \"torch/csrc/jit/passes/frozen_linear_folding.cpp\",\n    \"torch/csrc/jit/passes/frozen_linear_transpose.cpp\",\n    \"torch/csrc/jit/passes/frozen_ops_to_mkldnn.cpp\",\n    \"torch/csrc/jit/passes/frozen_graph_optimizations.cpp\",\n    \"torch/csrc/jit/passes/remove_expands.cpp\",\n    \"torch/csrc/jit/passes/remove_dropout.cpp\",\n    \"torch/csrc/jit/passes/requires_grad_analysis.cpp\",\n    \"torch/csrc/jit/passes/shape_analysis.cpp\",\n    \"torch/csrc/jit/passes/integer_value_refinement.cpp\",\n    \"torch/csrc/jit/passes/replacement_of_old_operators.cpp\",\n    \"torch/csrc/jit/passes/symbolic_shape_analysis.cpp\",\n    \"torch/csrc/jit/passes/symbolic_shape_cache.cpp\",\n    \"torch/csrc/jit/passes/symbolic_shape_runtime_fusion.cpp\",\n    \"torch/csrc/jit/passes/specialize_autogradzero.cpp\",\n    \"torch/csrc/jit/passes/update_differentiable_graph_requires_grad.cpp\",\n    \"torch/csrc/jit/passes/variadic_ops.cpp\",\n    \"torch/csrc/jit/passes/subgraph_rewrite.cpp\",\n    \"torch/csrc/jit/passes/tensorexpr_fuser.cpp\",\n    \"torch/csrc/jit/passes/utils/memory_dag.cpp\",\n    \"torch/csrc/jit/passes/utils/subgraph_utils.cpp\",\n    \"torch/csrc/jit/passes/utils/optimization_utils.cpp\",\n    \"torch/csrc/jit/passes/utils/op_registry.cpp\",\n    \"torch/csrc/jit/passes/mkldnn_rewrite.cpp\",\n    \"torch/csrc/jit/passes/xnnpack_rewrite.cpp\",\n    \"torch/csrc/jit/passes/vulkan_rewrite.cpp\",\n    \"torch/csrc/jit/passes/metal_rewrite.cpp\",\n    \"torch/csrc/jit/passes/quantization/helper.cpp\",\n    \"torch/csrc/jit/passes/quantization/quantization_type.cpp\",\n    \"torch/csrc/jit/passes/quantization/insert_observers.cpp\",\n    \"torch/csrc/jit/passes/quantization/insert_quant_dequant.cpp\",\n    \"torch/csrc/jit/passes/quantization/dedup_module_uses.cpp\",\n    \"torch/csrc/jit/passes/quantization/finalize.cpp\",\n    \"torch/csrc/jit/passes/quantization/fusion_passes.cpp\",\n    \"torch/csrc/jit/passes/quantization/register_packed_params.cpp\",\n    \"torch/csrc/jit/python/update_graph_executor_opt.cpp\",\n    \"torch/csrc/jit/python/utf8_decoding_ignore.cpp\",\n    \"torch/csrc/jit/runtime/argument_spec.cpp\",\n    \"torch/csrc/jit/runtime/autodiff.cpp\",\n    \"torch/csrc/jit/runtime/graph_executor.cpp\",\n    \"torch/csrc/jit/runtime/interpreter/frame.cpp\",\n    \"torch/csrc/jit/runtime/interpreter/preprocess_graph.cpp\",\n    \"torch/csrc/jit/runtime/interpreter.cpp\",\n    \"torch/csrc/jit/runtime/logging.cpp\",\n    \"torch/csrc/jit/runtime/simple_graph_executor_impl.cpp\",\n    \"torch/csrc/jit/runtime/profiling_graph_executor_impl.cpp\",\n    \"torch/csrc/jit/runtime/profiling_record.cpp\",\n    \"torch/csrc/jit/runtime/script_profile.cpp\",\n    \"torch/csrc/jit/runtime/symbolic_script.cpp\",\n    \"torch/csrc/jit/runtime/symbolic_shape_registry.cpp\",\n    \"torch/csrc/jit/runtime/decomposition_registry.cpp\",\n    \"torch/csrc/jit/runtime/decomposition_registry_util.cpp\",\n    \"torch/csrc/jit/runtime/serialized_shape_function_registry.cpp\",\n    \"torch/csrc/jit/runtime/symbolic_shape_registry_util.cpp\",\n    \"torch/csrc/jit/runtime/jit_trace.cpp\",\n    \"torch/csrc/jit/serialization/callstack_debug_info_serialization.cpp\",\n    \"torch/csrc/jit/serialization/import.cpp\",\n    \"torch/csrc/jit/serialization/import_export_helpers.cpp\",\n    \"torch/csrc/jit/serialization/import_source.cpp\",\n    \"torch/csrc/jit/serialization/pickle.cpp\",\n    \"torch/csrc/jit/serialization/python_print.cpp\",\n    \"torch/csrc/jit/serialization/source_range_serialization.cpp\",\n    \"torch/csrc/jit/tensorexpr/block_codegen.cpp\",\n    \"torch/csrc/jit/tensorexpr/bounds_inference.cpp\",\n    \"torch/csrc/jit/tensorexpr/bounds_overlap.cpp\",\n    \"torch/csrc/jit/tensorexpr/codegen.cpp\",\n    \"torch/csrc/jit/tensorexpr/cpp_codegen.cpp\",\n    \"torch/csrc/jit/tensorexpr/eval.cpp\",\n    \"torch/csrc/jit/tensorexpr/expr.cpp\",\n    \"torch/csrc/jit/tensorexpr/external_functions_core.cpp\",\n    \"torch/csrc/jit/tensorexpr/external_functions_registry.cpp\",\n    \"torch/csrc/jit/tensorexpr/graph_opt.cpp\",\n    \"torch/csrc/jit/tensorexpr/hash_provider.cpp\",\n    \"torch/csrc/jit/tensorexpr/intrinsic_symbols.cpp\",\n    \"torch/csrc/jit/tensorexpr/ir.cpp\",\n    \"torch/csrc/jit/tensorexpr/ir_cloner.cpp\",\n    \"torch/csrc/jit/tensorexpr/ir_mutator.cpp\",\n    \"torch/csrc/jit/tensorexpr/ir_printer.cpp\",\n    \"torch/csrc/jit/tensorexpr/ir_simplifier.cpp\",\n    \"torch/csrc/jit/tensorexpr/ir_verifier.cpp\",\n    \"torch/csrc/jit/tensorexpr/ir_visitor.cpp\",\n    \"torch/csrc/jit/tensorexpr/kernel.cpp\",\n    \"torch/csrc/jit/tensorexpr/llvm_codegen.cpp\",\n    \"torch/csrc/jit/tensorexpr/llvm_jit.cpp\",\n    \"torch/csrc/jit/tensorexpr/loopnest.cpp\",\n    \"torch/csrc/jit/tensorexpr/loopnest_randomization.cpp\",\n    \"torch/csrc/jit/tensorexpr/lowerings.cpp\",\n    \"torch/csrc/jit/tensorexpr/mem_dependency_checker.cpp\",\n    \"torch/csrc/jit/tensorexpr/operators/conv2d.cpp\",\n    \"torch/csrc/jit/tensorexpr/operators/matmul.cpp\",\n    \"torch/csrc/jit/tensorexpr/operators/misc.cpp\",\n    \"torch/csrc/jit/tensorexpr/operators/norm.cpp\",\n    \"torch/csrc/jit/tensorexpr/operators/pointwise.cpp\",\n    \"torch/csrc/jit/tensorexpr/operators/quantization.cpp\",\n    \"torch/csrc/jit/tensorexpr/operators/reduction.cpp\",\n    \"torch/csrc/jit/tensorexpr/operators/softmax.cpp\",\n    \"torch/csrc/jit/tensorexpr/reduction.cpp\",\n    \"torch/csrc/jit/tensorexpr/registerizer.cpp\",\n    \"torch/csrc/jit/tensorexpr/tensor.cpp\",\n    \"torch/csrc/jit/tensorexpr/types.cpp\",\n    \"torch/csrc/jit/tensorexpr/unique_name_manager.cpp\",\n    \"torch/csrc/jit/testing/file_check.cpp\",\n    \"torch/csrc/profiler/unwind/unwind.cpp\",\n    \"torch/csrc/profiler/unwind/unwind_fb.cpp\",\n    \"torch/csrc/profiler/combined_traceback.cpp\",\n    \"torch/csrc/jit/testing/hooks_for_testing.cpp\",\n    \"torch/csrc/utils/cpp_stacktraces.cpp\",\n    \"torch/csrc/utils/schema_info.cpp\",\n    \"torch/csrc/utils/tensor_flatten.cpp\",\n    \"torch/csrc/utils/variadic.cpp\",\n]\n\ncore_sources_full_mobile_no_backend_interface = core_sources_full_mobile_no_backend_interface_xplat + [\n    # backend_debug_info.cpp provides\n    # __torch__.torch.classes.backend.BackendDebugInfo class\n    # This should not be needed eventually.\n    # TODO: Remove this dependency\n    \"torch/csrc/jit/backends/backend_debug_info.cpp\",\n    \"torch/csrc/jit/mobile/compatibility/model_compatibility.cpp\",\n    \"torch/csrc/jit/mobile/function.cpp\",\n    \"torch/csrc/jit/mobile/import.cpp\",\n    \"torch/csrc/jit/mobile/flatbuffer_loader.cpp\",\n    \"torch/csrc/jit/mobile/interpreter.cpp\",\n    \"torch/csrc/jit/mobile/module.cpp\",\n    \"torch/csrc/jit/mobile/observer.cpp\",\n    \"torch/csrc/jit/mobile/parse_bytecode.cpp\",\n    \"torch/csrc/jit/mobile/parse_operators.cpp\",\n    \"torch/csrc/jit/mobile/quantization.cpp\",\n    \"torch/csrc/jit/mobile/upgrader_mobile.cpp\",\n]\n\ncore_sources_full_mobile = core_sources_full_mobile_no_backend_interface + [\n    \"torch/csrc/jit/backends/backend_debug_info.cpp\",\n    \"torch/csrc/jit/backends/backend_interface.cpp\",\n]\n\ncore_sources_full = core_sources_full_mobile + [\n    \"torch/csrc/jit/runtime/static/fusion.cpp\",\n    \"torch/csrc/jit/runtime/static/generated_ops.cpp\",\n    \"torch/csrc/jit/runtime/static/impl.cpp\",\n    \"torch/csrc/jit/runtime/static/memory_planner.cpp\",\n    \"torch/csrc/jit/runtime/static/native_ops.cpp\",\n    \"torch/csrc/jit/runtime/static/ops.cpp\",\n    \"torch/csrc/jit/runtime/static/passes.cpp\",\n    \"torch/csrc/jit/runtime/static/te_wrapper.cpp\",\n    \"torch/csrc/jit/tensorexpr/external_functions.cpp\",\n    \"torch/csrc/jit/tensorexpr/external_functions_codegen.cpp\",\n]\n\nlazy_tensor_core_sources = [\n    \"torch/csrc/lazy/backend/backend_device.cpp\",\n    \"torch/csrc/lazy/backend/backend_interface.cpp\",\n    \"torch/csrc/lazy/backend/lowering_context.cpp\",\n    \"torch/csrc/lazy/core/config.cpp\",\n    \"torch/csrc/lazy/core/debug_util.cpp\",\n    \"torch/csrc/lazy/core/hash.cpp\",\n    \"torch/csrc/lazy/core/helpers.cpp\",\n    \"torch/csrc/lazy/core/ir.cpp\",\n    \"torch/csrc/lazy/core/ir_dump_util.cpp\",\n    \"torch/csrc/lazy/core/ir_metadata.cpp\",\n    \"torch/csrc/lazy/core/ir_util.cpp\",\n    \"torch/csrc/lazy/core/lazy_graph_executor.cpp\",\n    \"torch/csrc/lazy/core/metrics.cpp\",\n    \"torch/csrc/lazy/core/multi_wait.cpp\",\n    \"torch/csrc/lazy/core/ops/arithmetic_ir_ops.cpp\",\n    \"torch/csrc/lazy/core/ops/utils.cpp\",\n    \"torch/csrc/lazy/core/permutation_util.cpp\",\n    \"torch/csrc/lazy/core/shape.cpp\",\n    \"torch/csrc/lazy/core/shape_inference.cpp\",\n    \"torch/csrc/lazy/core/tensor.cpp\",\n    \"torch/csrc/lazy/core/tensor_impl.cpp\",\n    \"torch/csrc/lazy/core/tensor_util.cpp\",\n    \"torch/csrc/lazy/core/thread_pool.cpp\",\n    \"torch/csrc/lazy/core/trie.cpp\",\n]\n\n# We can't build all of the ts backend under certain build configurations, e.g. mobile,\n# since it depends on things like autograd, meta functions, which may be disabled\nlazy_tensor_ts_sources = [\n    \"torch/csrc/lazy/ts_backend/dynamic_ir.cpp\",\n    \"torch/csrc/lazy/ts_backend/config.cpp\",\n    \"torch/csrc/lazy/ts_backend/ops/device_data.cpp\",\n    \"torch/csrc/lazy/ts_backend/ops/generic.cpp\",\n    \"torch/csrc/lazy/ts_backend/tensor_aten_ops.cpp\",\n    \"torch/csrc/lazy/ts_backend/ts_autograd_functions.cpp\",\n    \"torch/csrc/lazy/ts_backend/ts_backend_impl.cpp\",\n    \"torch/csrc/lazy/ts_backend/ts_eager_fallback.cpp\",\n    \"torch/csrc/lazy/ts_backend/ts_lowering_context.cpp\",\n    \"torch/csrc/lazy/ts_backend/ts_native_functions.cpp\",\n    \"torch/csrc/lazy/ts_backend/ts_node.cpp\",\n    \"torch/csrc/lazy/ts_backend/ts_node_lowering.cpp\",\n]\n\nlazy_tensor_core_python_sources = [\n    \"torch/csrc/lazy/python/init.cpp\",\n    \"torch/csrc/lazy/python/python_util.cpp\",\n]\n\ninductor_core_resources = [\n    \"torch/csrc/inductor/aoti_package/model_package_loader.cpp\",\n    \"torch/csrc/inductor/aoti_runner/model_container_runner.cpp\",\n    \"torch/csrc/inductor/aoti_runner/model_container_runner_cpu.cpp\",\n    \"torch/csrc/inductor/aoti_torch/shim_common.cpp\",\n    \"torch/csrc/inductor/aoti_torch/shim_mkldnn.cpp\",\n    \"torch/csrc/inductor/aoti_torch/tensor_converter.cpp\",\n    \"torch/csrc/inductor/aoti_torch/mkldnn_tensor.cpp\",\n    \"torch/csrc/inductor/aoti_torch/oss_proxy_executor.cpp\",\n    \"torch/csrc/inductor/inductor_ops.cpp\",\n    \"torch/csrc/jit/serialization/pickle.cpp\",\n]\n\nlibtorch_core_sources = sorted(\n    core_sources_common +\n    torch_unpickler_common +\n    core_sources_full +\n    core_trainer_sources +\n    inductor_core_resources +\n    libtorch_profiler_sources +\n    lazy_tensor_core_sources,\n)\n\n# These files are the only ones that are supported on Windows.\nlibtorch_distributed_base_sources = [\n    \"torch/csrc/distributed/c10d/Backend.cpp\",\n    \"torch/csrc/distributed/c10d/Backoff.cpp\",\n    \"torch/csrc/distributed/c10d/DMAConnectivity.cpp\",\n    \"torch/csrc/distributed/c10d/control_collectives/StoreCollectives.cpp\",\n    \"torch/csrc/distributed/c10d/FileStore.cpp\",\n    \"torch/csrc/distributed/c10d/Functional.cpp\",\n    \"torch/csrc/distributed/c10d/GlooDeviceFactory.cpp\",\n    \"torch/csrc/distributed/c10d/GroupRegistry.cpp\",\n    \"torch/csrc/distributed/c10d/Ops.cpp\",\n    \"torch/csrc/distributed/c10d/ParamCommsUtils.cpp\",\n    \"torch/csrc/distributed/c10d/PrefixStore.cpp\",\n    \"torch/csrc/distributed/c10d/ProcessGroup.cpp\",\n    \"torch/csrc/distributed/c10d/ProcessGroupGloo.cpp\",\n    \"torch/csrc/distributed/c10d/ProcessGroupMPI.cpp\",\n    \"torch/csrc/distributed/c10d/ProcessGroupWrapper.cpp\",\n    \"torch/csrc/distributed/c10d/Store.cpp\",\n    \"torch/csrc/distributed/c10d/SymmetricMemory.cpp\",\n    \"torch/csrc/distributed/c10d/TCPStore.cpp\",\n    \"torch/csrc/distributed/c10d/TCPStoreBackend.cpp\",\n    \"torch/csrc/distributed/c10d/TCPStoreLibUvBackend.cpp\",\n    \"torch/csrc/distributed/c10d/Utils.cpp\",\n    \"torch/csrc/distributed/c10d/comm.cpp\",\n    \"torch/csrc/distributed/c10d/debug.cpp\",\n    \"torch/csrc/distributed/c10d/default_comm_hooks.cpp\",\n    \"torch/csrc/distributed/c10d/logger.cpp\",\n    \"torch/csrc/distributed/c10d/logging.cpp\",\n    \"torch/csrc/distributed/c10d/quantization/quantization.cpp\",\n    \"torch/csrc/distributed/c10d/reducer.cpp\",\n    \"torch/csrc/distributed/c10d/sequence_num.cpp\",\n    \"torch/csrc/distributed/c10d/socket.cpp\",\n    \"torch/csrc/distributed/c10d/Work.cpp\",\n    \"torch/csrc/distributed/c10d/control_plane/Handlers.cpp\",\n    \"torch/csrc/distributed/c10d/control_plane/WorkerServer.cpp\",\n]\n\n# These files are only supported on Linux (and others) but not on Windows.\nlibtorch_distributed_extra_sources = [\n    \"torch/csrc/distributed/autograd/autograd.cpp\",\n    \"torch/csrc/distributed/autograd/utils.cpp\",\n    \"torch/csrc/distributed/autograd/context/container.cpp\",\n    \"torch/csrc/distributed/autograd/context/context.cpp\",\n    \"torch/csrc/distributed/autograd/engine/dist_engine.cpp\",\n    \"torch/csrc/distributed/autograd/functions/recvrpc_backward.cpp\",\n    \"torch/csrc/distributed/autograd/functions/sendrpc_backward.cpp\",\n    \"torch/csrc/distributed/autograd/rpc_messages/autograd_metadata.cpp\",\n    \"torch/csrc/distributed/autograd/rpc_messages/propagate_gradients_req.cpp\",\n    \"torch/csrc/distributed/autograd/rpc_messages/propagate_gradients_resp.cpp\",\n    \"torch/csrc/distributed/autograd/rpc_messages/cleanup_autograd_context_req.cpp\",\n    \"torch/csrc/distributed/autograd/rpc_messages/cleanup_autograd_context_resp.cpp\",\n    \"torch/csrc/distributed/autograd/rpc_messages/rpc_with_autograd.cpp\",\n    \"torch/csrc/distributed/autograd/rpc_messages/rpc_with_profiling_req.cpp\",\n    \"torch/csrc/distributed/autograd/rpc_messages/rpc_with_profiling_resp.cpp\",\n    \"torch/csrc/distributed/autograd/rpc_messages/rref_backward_req.cpp\",\n    \"torch/csrc/distributed/autograd/rpc_messages/rref_backward_resp.cpp\",\n    \"torch/csrc/distributed/c10d/HashStore.cpp\",\n    \"torch/csrc/distributed/rpc/agent_utils.cpp\",\n    \"torch/csrc/distributed/rpc/message.cpp\",\n    \"torch/csrc/distributed/rpc/profiler/remote_profiler_manager.cpp\",\n    \"torch/csrc/distributed/rpc/profiler/server_process_global_profiler.cpp\",\n    \"torch/csrc/distributed/rpc/python_call.cpp\",\n    \"torch/csrc/distributed/rpc/python_remote_call.cpp\",\n    \"torch/csrc/distributed/rpc/python_resp.cpp\",\n    \"torch/csrc/distributed/rpc/request_callback.cpp\",\n    \"torch/csrc/distributed/rpc/request_callback_no_python.cpp\",\n    \"torch/csrc/distributed/rpc/rpc_agent.cpp\",\n    \"torch/csrc/distributed/rpc/rref_context.cpp\",\n    \"torch/csrc/distributed/rpc/rref_impl.cpp\",\n    \"torch/csrc/distributed/rpc/rref_proto.cpp\",\n    \"torch/csrc/distributed/rpc/script_call.cpp\",\n    \"torch/csrc/distributed/rpc/script_remote_call.cpp\",\n    \"torch/csrc/distributed/rpc/script_resp.cpp\",\n    \"torch/csrc/distributed/rpc/tensorpipe_agent.cpp\",\n    \"torch/csrc/distributed/rpc/tensorpipe_utils.cpp\",\n    \"torch/csrc/distributed/rpc/testing/faulty_tensorpipe_agent.cpp\",\n    \"torch/csrc/distributed/rpc/torchscript_functions.cpp\",\n    \"torch/csrc/distributed/rpc/types.cpp\",\n    \"torch/csrc/distributed/rpc/utils.cpp\",\n]\n\nlibtorch_distributed_sources = libtorch_distributed_base_sources + libtorch_distributed_extra_sources\n\njit_sources_full = [\n    \"torch/csrc/jit/codegen/cuda/interface.cpp\",\n    \"torch/csrc/jit/passes/lower_graph.cpp\",\n    \"torch/csrc/jit/runtime/register_c10_ops.cpp\",\n    \"torch/csrc/jit/runtime/register_prim_ops.cpp\",\n    \"torch/csrc/jit/runtime/register_prim_ops_fulljit.cpp\",\n    \"torch/csrc/jit/runtime/register_special_ops.cpp\",\n    \"torch/csrc/jit/passes/remove_inplace_ops.cpp\",\n    \"torch/csrc/jit/passes/utils/check_alias_annotation.cpp\",\n    \"torch/csrc/jit/passes/autocast.cpp\",\n]\n\nlibtorch_core_jit_sources = sorted(jit_sources_full)\n\ntorch_mobile_tracer_sources = [\n    \"torch/csrc/jit/mobile/model_tracer/tracer.cpp\",\n    \"torch/csrc/jit/mobile/model_tracer/TensorUtils.cpp\",\n    \"torch/csrc/jit/mobile/model_tracer/TracerRunner.cpp\",\n    \"torch/csrc/jit/mobile/model_tracer/MobileModelRunner.cpp\",\n    \"torch/csrc/jit/mobile/model_tracer/OperatorCallTracer.cpp\",\n    \"torch/csrc/jit/mobile/model_tracer/KernelDTypeTracer.cpp\",\n    \"torch/csrc/jit/mobile/model_tracer/CustomClassTracer.cpp\",\n    \"torch/csrc/jit/mobile/model_tracer/BuildFeatureTracer.cpp\",\n]\n\nlibtorch_lite_eager_symbolication = [\n    \"torch/csrc/jit/frontend/source_range.cpp\",\n    \"torch/csrc/jit/ir/scope.cpp\",\n    \"torch/csrc/jit/mobile/debug_info.cpp\",\n    \"torch/csrc/jit/serialization/callstack_debug_info_serialization.cpp\",\n    \"torch/csrc/jit/serialization/source_range_serialization.cpp\",\n    # Later we can split serialization and deserialization logic\n    # to have better separation within build and only build relevant parts.\n    \"torch/csrc/jit/serialization/pickle.cpp\",\n    \"torch/csrc/jit/serialization/pickler.cpp\",\n    \"torch/csrc/jit/serialization/unpickler.cpp\",\n]\n\n# TODO: core_trainer_sources is not necessary for libtorch lite\nlibtorch_lite_cmake_sources = sorted(\n    core_trainer_sources +\n    core_sources_common +\n    torch_unpickler_common +\n    torch_mobile_core,\n)\n\nlibtorch_cmake_sources = libtorch_core_sources + libtorch_core_jit_sources\n\nlibtorch_extra_sources = libtorch_core_jit_sources + [\n    \"torch/csrc/autograd/TraceTypeManual.cpp\",\n    \"torch/csrc/autograd/VariableTypeManual.cpp\",\n    \"torch/csrc/autograd/FunctionsManual.cpp\",\n    \"torch/csrc/jit/api/module_save.cpp\",\n    \"torch/csrc/jit/codegen/fuser/cpu/fused_kernel.cpp\",\n    \"torch/csrc/jit/mobile/compatibility/backport.cpp\",\n    \"torch/csrc/jit/mobile/compatibility/backport_manager.cpp\",\n    \"torch/csrc/jit/mobile/compatibility/model_compatibility.cpp\",\n    # To be included for eager symbolication in lite interpreter\n    # when it is built in libtorch\n    \"torch/csrc/jit/mobile/debug_info.cpp\",\n    \"torch/csrc/jit/mobile/function.cpp\",\n    \"torch/csrc/jit/mobile/flatbuffer_loader.cpp\",\n    \"torch/csrc/jit/mobile/import.cpp\",\n    \"torch/csrc/jit/mobile/import_data.cpp\",\n    \"torch/csrc/jit/mobile/interpreter.cpp\",\n    \"torch/csrc/jit/mobile/module.cpp\",\n    \"torch/csrc/jit/mobile/observer.cpp\",\n    \"torch/csrc/jit/mobile/parse_bytecode.cpp\",\n    \"torch/csrc/jit/mobile/parse_operators.cpp\",\n    \"torch/csrc/jit/mobile/quantization.cpp\",\n    \"torch/csrc/jit/mobile/train/export_data.cpp\",\n    \"torch/csrc/jit/mobile/train/optim/sgd.cpp\",\n    \"torch/csrc/jit/mobile/train/random.cpp\",\n    \"torch/csrc/jit/mobile/train/sequential.cpp\",\n    \"torch/csrc/jit/mobile/upgrader_mobile.cpp\",\n    \"torch/csrc/jit/serialization/onnx.cpp\",\n    \"torch/csrc/jit/serialization/export.cpp\",\n    \"torch/csrc/jit/serialization/export_bytecode.cpp\",\n    \"torch/csrc/jit/serialization/export_module.cpp\",\n    \"torch/csrc/jit/serialization/flatbuffer_serializer.cpp\",\n    \"torch/csrc/utils/byte_order.cpp\",\n    \"torch/csrc/utils/out_types.cpp\",\n]\n\ndef libtorch_sources(gencode_pattern = \":generate-code[{}]\"):\n    return (\n        libtorch_generated_sources(gencode_pattern) + libtorch_core_sources + libtorch_distributed_sources + libtorch_extra_sources\n    )\n\nlibtorch_cuda_core_sources = [\n    \"torch/csrc/CudaIPCTypes.cpp\",\n    \"torch/csrc/cuda/comm.cpp\",\n    \"torch/csrc/cuda/memory_snapshot.cpp\",\n    \"torch/csrc/cuda/CUDAPluggableAllocator.cpp\",\n    \"torch/csrc/inductor/aoti_runner/model_container_runner_cuda.cpp\",\n    \"torch/csrc/inductor/aoti_torch/shim_cuda.cpp\",\n    \"torch/csrc/jit/codegen/fuser/cuda/fused_kernel.cpp\",\n    \"torch/csrc/profiler/stubs/cuda.cpp\",\n    \"torch/csrc/autograd/functions/comm.cpp\",\n    \"torch/csrc/jit/passes/frozen_conv_add_relu_fusion_cuda.cpp\",\n    \"torch/csrc/jit/tensorexpr/cuda_codegen.cpp\",\n    \"torch/csrc/jit/runtime/register_cuda_ops.cpp\",\n]\n\n# These files are the only ones that are supported on Windows.\nlibtorch_cuda_distributed_base_sources = [\n    \"torch/csrc/distributed/c10d/reducer_cuda.cpp\",\n]\n\n# These files are only supported on Linux (and others) but not on Windows.\nlibtorch_cuda_distributed_extra_sources = [\n    \"torch/csrc/distributed/c10d/CudaDMAConnectivity.cpp\",\n    \"torch/csrc/distributed/c10d/NCCLUtils.cpp\",\n    \"torch/csrc/distributed/c10d/FlightRecorder.cpp\",\n    \"torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp\",\n    \"torch/csrc/distributed/c10d/ProcessGroupUCC.cpp\",\n    \"torch/csrc/distributed/c10d/UCCTracing.cpp\",\n    \"torch/csrc/distributed/c10d/UCCUtils.cpp\",\n    \"torch/csrc/distributed/c10d/intra_node_comm.cpp\",\n    \"torch/csrc/distributed/c10d/intra_node_comm.cu\",\n    \"torch/csrc/distributed/c10d/CUDASymmetricMemory.cu\",\n    \"torch/csrc/distributed/c10d/CUDASymmetricMemoryOps.cu\",\n    \"torch/csrc/distributed/c10d/cuda/AsyncMM.cu\",\n    \"torch/csrc/distributed/c10d/NanCheck.cu\",\n    \"torch/csrc/distributed/rpc/tensorpipe_cuda.cpp\",\n    \"torch/csrc/distributed/c10d/quantization/quantization_gpu.cu\",\n]\n\nlibtorch_cuda_distributed_sources = libtorch_cuda_distributed_base_sources + libtorch_cuda_distributed_extra_sources\n\nlibtorch_cuda_sources = libtorch_cuda_core_sources + libtorch_cuda_distributed_sources + [\n    \"torch/csrc/cuda/nccl.cpp\",\n]\n\ntorch_cpp_srcs = [\n    \"torch/csrc/api/src/cuda.cpp\",  # this just forwards stuff, no real CUDA\n    \"torch/csrc/api/src/data/datasets/mnist.cpp\",\n    \"torch/csrc/api/src/data/samplers/distributed.cpp\",\n    \"torch/csrc/api/src/data/samplers/random.cpp\",\n    \"torch/csrc/api/src/data/samplers/sequential.cpp\",\n    \"torch/csrc/api/src/data/samplers/stream.cpp\",\n    \"torch/csrc/api/src/enum.cpp\",\n    \"torch/csrc/api/src/imethod.cpp\",\n    \"torch/csrc/api/src/jit.cpp\",\n    \"torch/csrc/api/src/mps.cpp\",\n    \"torch/csrc/api/src/serialize.cpp\",\n    \"torch/csrc/api/src/nn/init.cpp\",\n    \"torch/csrc/api/src/nn/module.cpp\",\n    \"torch/csrc/api/src/nn/modules/_functions.cpp\",\n    \"torch/csrc/api/src/nn/modules/activation.cpp\",\n    \"torch/csrc/api/src/nn/modules/adaptive.cpp\",\n    \"torch/csrc/api/src/nn/modules/batchnorm.cpp\",\n    \"torch/csrc/api/src/nn/modules/normalization.cpp\",\n    \"torch/csrc/api/src/nn/modules/instancenorm.cpp\",\n    \"torch/csrc/api/src/nn/modules/conv.cpp\",\n    \"torch/csrc/api/src/nn/modules/dropout.cpp\",\n    \"torch/csrc/api/src/nn/modules/distance.cpp\",\n    \"torch/csrc/api/src/nn/modules/embedding.cpp\",\n    \"torch/csrc/api/src/nn/modules/fold.cpp\",\n    \"torch/csrc/api/src/nn/modules/linear.cpp\",\n    \"torch/csrc/api/src/nn/modules/loss.cpp\",\n    \"torch/csrc/api/src/nn/modules/padding.cpp\",\n    \"torch/csrc/api/src/nn/modules/pixelshuffle.cpp\",\n    \"torch/csrc/api/src/nn/modules/pooling.cpp\",\n    \"torch/csrc/api/src/nn/modules/rnn.cpp\",\n    \"torch/csrc/api/src/nn/modules/upsampling.cpp\",\n    \"torch/csrc/api/src/nn/modules/transformer.cpp\",\n    \"torch/csrc/api/src/nn/modules/container/functional.cpp\",\n    \"torch/csrc/api/src/nn/options/activation.cpp\",\n    \"torch/csrc/api/src/nn/options/adaptive.cpp\",\n    \"torch/csrc/api/src/nn/options/batchnorm.cpp\",\n    \"torch/csrc/api/src/nn/options/conv.cpp\",\n    \"torch/csrc/api/src/nn/options/dropout.cpp\",\n    \"torch/csrc/api/src/nn/options/instancenorm.cpp\",\n    \"torch/csrc/api/src/nn/options/linear.cpp\",\n    \"torch/csrc/api/src/nn/options/normalization.cpp\",\n    \"torch/csrc/api/src/nn/options/embedding.cpp\",\n    \"torch/csrc/api/src/nn/options/padding.cpp\",\n    \"torch/csrc/api/src/nn/options/pooling.cpp\",\n    \"torch/csrc/api/src/nn/options/rnn.cpp\",\n    \"torch/csrc/api/src/nn/options/vision.cpp\",\n    \"torch/csrc/api/src/nn/options/transformer.cpp\",\n    \"torch/csrc/api/src/optim/adagrad.cpp\",\n    \"torch/csrc/api/src/optim/adam.cpp\",\n    \"torch/csrc/api/src/optim/adamw.cpp\",\n    \"torch/csrc/api/src/optim/lbfgs.cpp\",\n    \"torch/csrc/api/src/optim/optimizer.cpp\",\n    \"torch/csrc/api/src/optim/rmsprop.cpp\",\n    \"torch/csrc/api/src/optim/serialize.cpp\",\n    \"torch/csrc/api/src/optim/sgd.cpp\",\n    \"torch/csrc/api/src/optim/schedulers/lr_scheduler.cpp\",\n    \"torch/csrc/api/src/optim/schedulers/reduce_on_plateau_scheduler.cpp\",\n    \"torch/csrc/api/src/optim/schedulers/step_lr.cpp\",\n    \"torch/csrc/api/src/serialize/input-archive.cpp\",\n    \"torch/csrc/api/src/serialize/output-archive.cpp\",\n    \"torch/csrc/api/src/xpu.cpp\",\n]\n\nlibtorch_python_cuda_core_sources = [\n    \"torch/csrc/cuda/Event.cpp\",\n    \"torch/csrc/cuda/Module.cpp\",\n    \"torch/csrc/cuda/python_comm.cpp\",\n    \"torch/csrc/cuda/Stream.cpp\",\n    \"torch/csrc/cuda/Graph.cpp\",\n    \"torch/csrc/cuda/MemPool.cpp\",\n    \"torch/csrc/cuda/shared/cudart.cpp\",\n    \"torch/csrc/cuda/shared/nvtx.cpp\",\n    \"torch/csrc/cuda/utils.cpp\",\n    \"torch/csrc/cuda/GdsFile.cpp\",\n]\n\nlibtorch_python_cuda_sources = libtorch_python_cuda_core_sources + [\n    \"torch/csrc/cuda/python_nccl.cpp\",\n    \"torch/csrc/cuda/shared/cudnn.cpp\",\n    \"torch/csrc/cuda/shared/cusparselt.cpp\",\n    \"torch/csrc/cuda/Tensor.cpp\",\n]\n\nlibtorch_python_xpu_sources = [\n    \"torch/csrc/xpu/Event.cpp\",\n    \"torch/csrc/xpu/Module.cpp\",\n    \"torch/csrc/xpu/Stream.cpp\",\n    \"torch/csrc/inductor/aoti_runner/model_container_runner_xpu.cpp\",\n    \"torch/csrc/inductor/aoti_torch/shim_xpu.cpp\",\n]\n\nlibtorch_xpu_sources = libtorch_python_xpu_sources\n\nlibtorch_python_core_sources = [\n    \"torch/csrc/DataLoader.cpp\",\n    \"torch/csrc/DeviceAccelerator.cpp\",\n    \"torch/csrc/Device.cpp\",\n    \"torch/csrc/Dtype.cpp\",\n    \"torch/csrc/DynamicTypes.cpp\",\n    \"torch/csrc/Exceptions.cpp\",\n    \"torch/csrc/Generator.cpp\",\n    \"torch/csrc/Layout.cpp\",\n    \"torch/csrc/MemoryFormat.cpp\",\n    \"torch/csrc/QScheme.cpp\",\n    \"torch/csrc/Module.cpp\",\n    \"torch/csrc/PyInterpreter.cpp\",\n    \"torch/csrc/python_dimname.cpp\",\n    \"torch/csrc/Size.cpp\",\n    \"torch/csrc/Storage.cpp\",\n    \"torch/csrc/StorageMethods.cpp\",\n    \"torch/csrc/StorageSharing.cpp\",\n    \"torch/csrc/Stream.cpp\",\n    \"torch/csrc/Event.cpp\",\n    \"torch/csrc/TypeInfo.cpp\",\n    \"torch/csrc/api/src/python/init.cpp\",\n    \"torch/csrc/autograd/functions/init.cpp\",\n    \"torch/csrc/autograd/init.cpp\",\n    \"torch/csrc/autograd/profiler_python.cpp\",\n    \"torch/csrc/autograd/python_anomaly_mode.cpp\",\n    \"torch/csrc/autograd/python_saved_variable_hooks.cpp\",\n    \"torch/csrc/autograd/python_cpp_function.cpp\",\n    \"torch/csrc/autograd/python_engine.cpp\",\n    \"torch/csrc/autograd/python_function.cpp\",\n    \"torch/csrc/autograd/python_hook.cpp\",\n    \"torch/csrc/autograd/python_legacy_variable.cpp\",\n    \"torch/csrc/autograd/python_nested_functions_manual.cpp\",\n    \"torch/csrc/autograd/python_torch_functions_manual.cpp\",\n    \"torch/csrc/autograd/python_variable.cpp\",\n    \"torch/csrc/autograd/python_variable_indexing.cpp\",\n    \"torch/csrc/dynamo/python_compiled_autograd.cpp\",\n    \"torch/csrc/dynamo/cache_entry.cpp\",\n    \"torch/csrc/dynamo/cpp_shim.cpp\",\n    \"torch/csrc/dynamo/cpython_defs.c\",\n    \"torch/csrc/dynamo/eval_frame.c\",\n    \"torch/csrc/dynamo/extra_state.cpp\",\n    \"torch/csrc/dynamo/framelocals_mapping.cpp\",\n    \"torch/csrc/dynamo/guards.cpp\",\n    \"torch/csrc/dynamo/utils.cpp\",\n    \"torch/csrc/dynamo/init.cpp\",\n    \"torch/csrc/functorch/init.cpp\",\n    \"torch/csrc/fx/node.cpp\",\n    \"torch/csrc/mps/Module.cpp\",\n    \"torch/csrc/mtia/Module.cpp\",\n    \"torch/csrc/export/pybind.cpp\",\n    \"torch/csrc/inductor/aoti_package/pybind.cpp\",\n    \"torch/csrc/inductor/aoti_runner/pybind.cpp\",\n    \"torch/csrc/inductor/aoti_eager/kernel_holder.cpp\",\n    \"torch/csrc/inductor/aoti_eager/kernel_meta_info.cpp\",\n    \"torch/csrc/inductor/resize_storage_bytes.cpp\",\n    \"torch/csrc/jit/backends/backend_init.cpp\",\n    \"torch/csrc/jit/python/init.cpp\",\n    \"torch/csrc/jit/passes/onnx.cpp\",\n    \"torch/csrc/jit/passes/onnx/cast_all_constant_to_floating.cpp\",\n    \"torch/csrc/jit/passes/onnx/deduplicate_initializers.cpp\",\n    \"torch/csrc/jit/passes/onnx/eval_peephole.cpp\",\n    \"torch/csrc/jit/passes/onnx/constant_fold.cpp\",\n    \"torch/csrc/jit/passes/onnx/constant_map.cpp\",\n    \"torch/csrc/jit/passes/onnx/eliminate_unused_items.cpp\",\n    \"torch/csrc/jit/passes/onnx/fixup_onnx_controlflow.cpp\",\n    \"torch/csrc/jit/passes/onnx/list_model_parameters.cpp\",\n    \"torch/csrc/jit/passes/onnx/function_substitution.cpp\",\n    \"torch/csrc/jit/passes/onnx/helper.cpp\",\n    \"torch/csrc/jit/passes/onnx/peephole.cpp\",\n    \"torch/csrc/jit/passes/onnx/preprocess_for_onnx.cpp\",\n    \"torch/csrc/jit/passes/onnx/prepare_division_for_onnx.cpp\",\n    \"torch/csrc/jit/passes/onnx/scalar_type_analysis.cpp\",\n    \"torch/csrc/jit/passes/onnx/unpack_quantized_weights.cpp\",\n    \"torch/csrc/jit/passes/onnx/remove_inplace_ops_for_onnx.cpp\",\n    \"torch/csrc/jit/passes/onnx/shape_type_inference.cpp\",\n    \"torch/csrc/jit/passes/onnx/function_extraction.cpp\",\n    \"torch/csrc/jit/passes/onnx/onnx_log.cpp\",\n    \"torch/csrc/jit/passes/onnx/naming.cpp\",\n    \"torch/csrc/jit/python/pybind_utils.cpp\",\n    \"torch/csrc/jit/passes/onnx/pattern_conversion/autograd_function_process.cpp\",\n    \"torch/csrc/jit/passes/onnx/pattern_conversion/common.cpp\",\n    \"torch/csrc/jit/passes/onnx/pattern_conversion/pattern_encapsulation.cpp\",\n    \"torch/csrc/jit/passes/onnx/pattern_conversion/pattern_conversion.cpp\",\n    \"torch/csrc/jit/python/python_arg_flatten.cpp\",\n    \"torch/csrc/jit/python/python_custom_class.cpp\",\n    \"torch/csrc/jit/python/python_dict.cpp\",\n    \"torch/csrc/jit/python/python_interpreter.cpp\",\n    \"torch/csrc/jit/python/python_ir.cpp\",\n    \"torch/csrc/jit/python/python_list.cpp\",\n    \"torch/csrc/jit/python/python_tracer.cpp\",\n    \"torch/csrc/jit/python/script_init.cpp\",\n    \"torch/csrc/jit/frontend/concrete_module_type.cpp\",\n    \"torch/csrc/jit/frontend/tree_views.cpp\",\n    \"torch/csrc/jit/python/python_sugared_value.cpp\",\n    \"torch/csrc/jit/python/python_tree_views.cpp\",\n    \"torch/csrc/jit/runtime/static/init.cpp\",\n    \"torch/csrc/jit/tensorexpr/tensorexpr_init.cpp\",\n    \"torch/csrc/monitor/python_init.cpp\",\n    \"torch/csrc/multiprocessing/init.cpp\",\n    \"torch/csrc/onnx/init.cpp\",\n    \"torch/csrc/profiler/python/init.cpp\",\n    \"torch/csrc/profiler/python/combined_traceback.cpp\",\n    \"torch/csrc/serialization.cpp\",\n    \"torch/csrc/tensor/python_tensor.cpp\",\n    \"torch/csrc/utils/init.cpp\",\n    \"torch/csrc/utils/throughput_benchmark.cpp\",\n    \"torch/csrc/utils.cpp\",\n    \"torch/csrc/utils/device_lazy_init.cpp\",\n    \"torch/csrc/utils/invalid_arguments.cpp\",\n    \"torch/csrc/utils/nested.cpp\",\n    \"torch/csrc/utils/object_ptr.cpp\",\n    \"torch/csrc/utils/python_arg_parser.cpp\",\n    \"torch/csrc/utils/python_dispatch.cpp\",\n    \"torch/csrc/utils/python_symnode.cpp\",\n    \"torch/csrc/utils/pybind.cpp\",\n    \"torch/csrc/utils/pyobject_preservation.cpp\",\n    \"torch/csrc/utils/structseq.cpp\",\n    \"torch/csrc/utils/tensor_apply.cpp\",\n    \"torch/csrc/utils/tensor_dtypes.cpp\",\n    \"torch/csrc/utils/tensor_layouts.cpp\",\n    \"torch/csrc/utils/tensor_memoryformats.cpp\",\n    \"torch/csrc/utils/tensor_qschemes.cpp\",\n    \"torch/csrc/utils/tensor_list.cpp\",\n    \"torch/csrc/utils/tensor_new.cpp\",\n    \"torch/csrc/utils/tensor_numpy.cpp\",\n    \"torch/csrc/utils/tensor_types.cpp\",\n    \"torch/csrc/utils/disable_torch_function.cpp\",\n    \"torch/csrc/utils/verbose.cpp\",\n    \"torch/csrc/cpu/Module.cpp\",\n    \"torch/csrc/instruction_counter/Module.cpp\",\n] + lazy_tensor_core_python_sources\n\nlibtorch_python_distributed_core_sources = [\n    \"torch/csrc/distributed/c10d/init.cpp\",\n    \"torch/csrc/distributed/c10d/python_comm_hook.cpp\",\n]\n\nlibtorch_python_distributed_sources = libtorch_python_distributed_core_sources + [\n    \"torch/csrc/distributed/autograd/init.cpp\",\n    \"torch/csrc/distributed/rpc/init.cpp\",\n    \"torch/csrc/distributed/rpc/py_rref.cpp\",\n    \"torch/csrc/distributed/rpc/python_functions.cpp\",\n    \"torch/csrc/distributed/rpc/python_rpc_handler.cpp\",\n    \"torch/csrc/distributed/rpc/request_callback_impl.cpp\",\n    \"torch/csrc/distributed/rpc/testing/init.cpp\",\n    \"torch/csrc/distributed/rpc/unpickled_python_call.cpp\",\n    \"torch/csrc/distributed/rpc/unpickled_python_remote_call.cpp\",\n    \"torch/csrc/jit/runtime/register_distributed_ops.cpp\",\n    \"torch/csrc/distributed/c10d/control_plane/PythonHandlers.cpp\",\n]\n\ndef glob_libtorch_python_sources(gencode_pattern = \":generate-code[{}]\"):\n    _libtorch_python_sources = [gencode_pattern.format(name) for name in [\n        \"torch/csrc/autograd/generated/python_functions_0.cpp\",\n        \"torch/csrc/autograd/generated/python_functions_1.cpp\",\n        \"torch/csrc/autograd/generated/python_functions_2.cpp\",\n        \"torch/csrc/autograd/generated/python_functions_3.cpp\",\n        \"torch/csrc/autograd/generated/python_functions_4.cpp\",\n        \"torch/csrc/autograd/generated/python_nested_functions.cpp\",\n        \"torch/csrc/autograd/generated/python_nn_functions.cpp\",\n        \"torch/csrc/autograd/generated/python_fft_functions.cpp\",\n        \"torch/csrc/autograd/generated/python_linalg_functions.cpp\",\n        \"torch/csrc/autograd/generated/python_enum_tag.cpp\",\n        \"torch/csrc/autograd/generated/python_return_types.cpp\",\n        \"torch/csrc/autograd/generated/python_sparse_functions.cpp\",\n        \"torch/csrc/autograd/generated/python_special_functions.cpp\",\n        \"torch/csrc/autograd/generated/python_torch_functions_0.cpp\",\n        \"torch/csrc/autograd/generated/python_torch_functions_1.cpp\",\n        \"torch/csrc/autograd/generated/python_torch_functions_2.cpp\",\n        \"torch/csrc/autograd/generated/python_variable_methods.cpp\",\n    ]]\n\n    _libtorch_python_sources.extend(libtorch_python_core_sources)\n    _libtorch_python_sources.extend(libtorch_python_distributed_sources)\n\n    return _libtorch_python_sources\n\n# List of non-globed source used to build ATen core internally\naten_cpu_non_globed_sources = [\n    \"aten/src/ATen/detail/CUDAHooksInterface.cpp\",\n    \"aten/src/ATen/detail/HIPHooksInterface.cpp\",\n    \"aten/src/ATen/detail/HPUHooksInterface.cpp\",\n    \"aten/src/ATen/detail/MPSHooksInterface.cpp\",\n    \"aten/src/ATen/detail/MAIAHooksInterface.cpp\",\n    \"aten/src/ATen/detail/PrivateUse1HooksInterface.cpp\",\n    \"aten/src/ATen/detail/XPUHooksInterface.cpp\",\n    \"aten/src/ATen/detail/MTIAHooksInterface.cpp\",\n    \"aten/src/ATen/detail/IPUHooksInterface.cpp\",\n    \"aten/src/ATen/record_function.cpp\",\n    \"aten/src/ATen/Dispatch.cpp\",\n    \"aten/src/ATen/SequenceNumber.cpp\",\n]\n\naten_cpu_non_globed_headers = [\n    \"aten/src/ATen/CPUGeneratorImpl.h\",\n    \"aten/src/ATen/NumericUtils.h\",\n    \"aten/src/ATen/detail/AcceleratorHooksInterface.h\",\n    \"aten/src/ATen/detail/CUDAHooksInterface.h\",\n    \"aten/src/ATen/detail/MPSHooksInterface.h\",\n    \"aten/src/ATen/detail/HIPHooksInterface.h\",\n    \"aten/src/ATen/detail/HPUHooksInterface.h\",\n    \"aten/src/ATen/detail/MAIAHooksInterface.h\",\n    \"aten/src/ATen/detail/PrivateUse1HooksInterface.h\",\n    \"aten/src/ATen/detail/XPUHooksInterface.h\",\n    \"aten/src/ATen/detail/MTIAHooksInterface.h\",\n    \"aten/src/ATen/detail/IPUHooksInterface.h\",\n]\n\naten_cpu_source_non_codegen_list = [\n    \"aten/src/ATen/AccumulateType.cpp\",\n    \"aten/src/ATen/LegacyBatchedTensorImpl.cpp\",\n    \"aten/src/ATen/CPUGeneratorImpl.cpp\",\n    \"aten/src/ATen/DeviceAccelerator.cpp\",\n    \"aten/src/ATen/Context.cpp\",\n    \"aten/src/ATen/DLConvertor.cpp\",\n    \"aten/src/ATen/EmptyTensor.cpp\",\n    \"aten/src/ATen/ExpandUtils.cpp\",\n    \"aten/src/ATen/CachedTensorUtils.cpp\",\n    \"aten/src/ATen/FunctionalInverses.cpp\",\n    \"aten/src/ATen/FunctionalStorageImpl.cpp\",\n    \"aten/src/ATen/FunctionalTensorWrapper.cpp\",\n    \"aten/src/ATen/FunctionalizeFallbackKernel.cpp\",\n    \"aten/src/ATen/MemoryOverlap.cpp\",\n    \"aten/src/ATen/MapAllocator.cpp\",\n    \"aten/src/ATen/NamedTensorUtils.cpp\",\n    \"aten/src/ATen/NestedTensorImpl.cpp\",\n    \"aten/src/ATen/ParallelCommon.cpp\",\n    \"aten/src/ATen/ParallelNative.cpp\",\n    \"aten/src/ATen/ParallelOpenMP.cpp\",\n    \"aten/src/ATen/ParallelThreadPoolNative.cpp\",\n    \"aten/src/ATen/PythonTorchFunctionTLS.cpp\",\n    \"aten/src/ATen/ThreadLocalPythonObjects.cpp\",\n    \"aten/src/ATen/ScalarOps.cpp\",\n    \"aten/src/ATen/SparseTensorImpl.cpp\",\n    \"aten/src/ATen/SparseCsrTensorImpl.cpp\",\n    \"aten/src/ATen/TensorGeometry.cpp\",\n    \"aten/src/ATen/TensorIndexing.cpp\",\n    \"aten/src/ATen/TensorMeta.cpp\",\n    \"aten/src/ATen/TensorNames.cpp\",\n    \"aten/src/ATen/TensorUtils.cpp\",\n    \"aten/src/ATen/ThreadLocalState.cpp\",\n    \"aten/src/ATen/FuncTorchTLS.cpp\",\n    \"aten/src/ATen/Utils.cpp\",\n    \"aten/src/ATen/Version.cpp\",\n    \"aten/src/ATen/LegacyVmapMode.cpp\",\n    \"aten/src/ATen/LegacyVmapTransforms.cpp\",\n    \"aten/src/ATen/core/BackendSelectFallbackKernel.cpp\",\n    \"aten/src/ATen/core/DeprecatedTypeProperties.cpp\",\n    \"aten/src/ATen/core/DeprecatedTypePropertiesRegistry.cpp\",\n    \"aten/src/ATen/core/Dict.cpp\",\n    \"aten/src/ATen/core/Dimname.cpp\",\n    \"aten/src/ATen/core/Formatting.cpp\",\n    \"aten/src/ATen/core/function_schema.cpp\",\n    \"aten/src/ATen/core/Generator.cpp\",\n    \"aten/src/ATen/core/PythonOpRegistrationTrampoline.cpp\",\n    \"aten/src/ATen/core/List.cpp\",\n    \"aten/src/ATen/core/NamedTensor.cpp\",\n    \"aten/src/ATen/core/Tensor.cpp\",\n    \"aten/src/ATen/core/VariableFallbackKernel.cpp\",\n    \"aten/src/ATen/core/VariableHooksInterface.cpp\",\n    \"aten/src/ATen/core/Vitals.cpp\",\n    \"aten/src/ATen/core/boxing/KernelFunction.cpp\",\n    \"aten/src/ATen/core/custom_class.cpp\",\n    \"aten/src/ATen/core/dispatch/DispatchKeyExtractor.cpp\",\n    \"aten/src/ATen/core/dispatch/Dispatcher.cpp\",\n    \"aten/src/ATen/core/dispatch/ObservedOperators.cpp\",\n    \"aten/src/ATen/core/dispatch/OperatorEntry.cpp\",\n    \"aten/src/ATen/core/interned_strings.cpp\",\n    \"aten/src/ATen/core/ivalue.cpp\",\n    \"aten/src/ATen/core/library.cpp\",\n    \"aten/src/ATen/core/op_registration/infer_schema.cpp\",\n    \"aten/src/ATen/core/op_registration/op_registration.cpp\",\n    \"aten/src/ATen/core/operator_name.cpp\",\n    \"aten/src/ATen/core/TorchDispatchUtils.cpp\",\n    \"aten/src/ATen/core/register_symbols.cpp\",\n    \"aten/src/ATen/core/NestedIntSymNodeImpl.cpp\",\n    \"aten/src/ATen/core/class_type.cpp\",\n    \"aten/src/ATen/core/type.cpp\",\n    \"aten/src/ATen/core/type_factory.cpp\",\n    \"aten/src/ATen/core/dynamic_type.cpp\",\n    \"aten/src/ATen/core/tensor_type.cpp\",\n    \"aten/src/ATen/core/union_type.cpp\",\n    \"aten/src/ATen/cpu/FlushDenormal.cpp\",\n    \"aten/src/ATen/detail/CPUGuardImpl.cpp\",\n    \"aten/src/ATen/metal/Context.cpp\",\n    \"aten/src/ATen/native/AutogradComposite.cpp\",\n    \"aten/src/ATen/native/ComparisonUtils.cpp\",\n    \"aten/src/ATen/native/DispatchStub.cpp\",\n    \"aten/src/ATen/native/UpSample.cpp\",\n    \"aten/src/ATen/native/mkldnn/BinaryOps.cpp\",\n    \"aten/src/ATen/native/mkldnn/Conv.cpp\",\n    \"aten/src/ATen/native/mkldnn/ConvPrepack.cpp\",\n    \"aten/src/ATen/native/mkldnn/Copy.cpp\",\n    \"aten/src/ATen/native/mkldnn/Gelu.cpp\",\n    \"aten/src/ATen/native/mkldnn/IDeepRegistration.cpp\",\n    \"aten/src/ATen/native/mkldnn/Linear.cpp\",\n    \"aten/src/ATen/native/mkldnn/MKLDNNCommon.cpp\",\n    \"aten/src/ATen/native/mkldnn/MKLDNNConversions.cpp\",\n    \"aten/src/ATen/native/mkldnn/MkldnnTensorMath.cpp\",\n    \"aten/src/ATen/native/mkldnn/Normalization.cpp\",\n    \"aten/src/ATen/native/mkldnn/OpContext.cpp\",\n    \"aten/src/ATen/native/mkldnn/Pooling.cpp\",\n    \"aten/src/ATen/native/mkldnn/Prelu.cpp\",\n    \"aten/src/ATen/native/mkldnn/RegisterMkldnnOpContextClass.cpp\",\n    \"aten/src/ATen/native/mkldnn/Relu.cpp\",\n    \"aten/src/ATen/native/mkldnn/RNN.cpp\",\n    \"aten/src/ATen/native/mkldnn/SoftMax.cpp\",\n    \"aten/src/ATen/native/mkldnn/TensorFactories.cpp\",\n    \"aten/src/ATen/native/mkldnn/TensorShape.cpp\",\n    \"aten/src/ATen/native/mkldnn/UnaryOps.cpp\",\n    \"aten/src/ATen/native/mkldnn/Utils.cpp\",\n    \"aten/src/ATen/native/mkldnn/Matmul.cpp\",\n    \"aten/src/ATen/native/quantized/cpu/init_qnnpack.cpp\",\n    # This is moved to aten_cpu because some of the custom ops use empty_with_tail_padding\n    # which was available only within aten_native_cpu. Ideally the right fix is to make\n    # empty_with_tail_padding into an op and use dispatcher with it. But exposing it as an op\n    # has limited use and hence does not seem to really make sense.\n    \"aten/src/ATen/native/utils/Factory.cpp\",\n    \"aten/src/ATen/SavedTensorHooks.cpp\",\n    \"aten/src/ATen/vulkan/Context.cpp\",\n    \"aten/src/ATen/native/prim_native_functions.cpp\",\n    \"aten/src/ATen/native/verbose_wrapper.cpp\",\n    \"aten/src/ATen/cpu/Utils.cpp\",\n] + aten_cpu_non_globed_sources\n\naten_cpu_source_codegen_list = [\n    \"aten/src/ATen/native/cpu/AdaptiveAvgPoolKernel.cpp\",\n    \"aten/src/ATen/native/cpu/AdaptiveMaxPoolKernel.cpp\",\n]\n\naten_ufunc_headers = [\n    \"aten/src/ATen/native/ufunc/add.h\",\n]\n\n# When building lite interpreter in OSS, \"aten/src/ATen/native/cpu/AdaptiveAvgPoolKernel.cpp\" will go through\n# codegen process. The codegen version of this file, like Activation.cpp.DEFAULT.cpp, will be included\n# in ${cpu_kernel_cpp} in aten/src/ATen/CMakeLists.txt. As a result, in aten/src/ATen/CMakeLists.txt,\n# only aten_cpu_source_non_codegen_list need to be added to ${all_cpu_cpp}.\naten_cpu_source_list = sorted(aten_cpu_source_non_codegen_list + aten_cpu_source_codegen_list)\n\n# Same as ${aten_cpu_source_codegen_list}, this list will go through aten codegen, and be included in\n# ${cpu_kernel_cpp} in aten/src/ATen/CMakeLists.txt.\naten_native_source_codegen_list = [\n    \"aten/src/ATen/native/cpu/Activation.cpp\",\n    \"aten/src/ATen/native/cpu/AvgPoolKernel.cpp\",\n    \"aten/src/ATen/native/cpu/BinaryOpsKernel.cpp\",\n    \"aten/src/ATen/native/cpu/BlasKernel.cpp\",\n    \"aten/src/ATen/native/cpu/CatKernel.cpp\",\n    \"aten/src/ATen/native/cpu/ChannelShuffleKernel.cpp\",\n    \"aten/src/ATen/native/cpu/ComplexKernel.cpp\",\n    \"aten/src/ATen/native/cpu/CopyKernel.cpp\",\n    \"aten/src/ATen/native/cpu/CrossKernel.cpp\",\n    \"aten/src/ATen/native/cpu/DepthwiseConvKernel.cpp\",\n    \"aten/src/ATen/native/cpu/DistanceOpsKernel.cpp\",\n    \"aten/src/ATen/native/cpu/DistributionKernels.cpp\",\n    \"aten/src/ATen/native/cpu/FlashAttentionKernel.cpp\",\n    \"aten/src/ATen/native/cpu/FillKernel.cpp\",\n    \"aten/src/ATen/native/cpu/FunctionOfAMatrixUtilsKernel.cpp\",\n    \"aten/src/ATen/native/cpu/GridSamplerKernel.cpp\",\n    \"aten/src/ATen/native/cpu/HistogramKernel.cpp\",\n    \"aten/src/ATen/native/cpu/IndexKernel.cpp\",\n    \"aten/src/ATen/native/cpu/LerpKernel.cpp\",\n    \"aten/src/ATen/native/cpu/LinearAlgebraKernel.cpp\",\n    \"aten/src/ATen/native/cpu/MaxPoolKernel.cpp\",\n    \"aten/src/ATen/native/cpu/MaxPooling.cpp\",\n    \"aten/src/ATen/native/cpu/MaxUnpoolKernel.cpp\",\n    \"aten/src/ATen/native/cpu/MultinomialKernel.cpp\",\n    \"aten/src/ATen/native/cpu/NativeMultiheadAttnKernel.cpp\",\n    \"aten/src/ATen/native/cpu/PaddingKernel.cpp\",\n    \"aten/src/ATen/native/cpu/PixelShuffleKernel.cpp\",\n    \"aten/src/ATen/native/cpu/PointwiseOpsKernel.cpp\",\n    \"aten/src/ATen/native/cpu/PowKernel.cpp\",\n    \"aten/src/ATen/native/cpu/RangeFactoriesKernel.cpp\",\n    \"aten/src/ATen/native/cpu/ReduceAllOpsKernel.cpp\",\n    \"aten/src/ATen/native/cpu/ReduceOpsKernel.cpp\",\n    \"aten/src/ATen/native/cpu/ReducedPrecisionFloatGemvFastPathKernel.cpp\",\n    \"aten/src/ATen/native/cpu/RenormKernel.cpp\",\n    \"aten/src/ATen/native/cpu/ScatterGatherKernel.cpp\",\n    \"aten/src/ATen/native/cpu/SoftMaxKernel.cpp\",\n    \"aten/src/ATen/native/cpu/SortingKernel.cpp\",\n    \"aten/src/ATen/native/cpu/StackKernel.cpp\",\n    \"aten/src/ATen/native/cpu/SumKernel.cpp\",\n    \"aten/src/ATen/native/cpu/TensorCompareKernel.cpp\",\n    \"aten/src/ATen/native/cpu/UnaryOpsKernel.cpp\",\n    \"aten/src/ATen/native/cpu/Unfold2d.cpp\",\n    \"aten/src/ATen/native/cpu/UnfoldBackwardKernel.cpp\",\n    \"aten/src/ATen/native/cpu/UpSampleKernel.cpp\",\n    \"aten/src/ATen/native/cpu/UpSampleMoreKernel.cpp\",\n    \"aten/src/ATen/native/cpu/WeightNormKernel.cpp\",\n    \"aten/src/ATen/native/cpu/airy_ai.cpp\",\n    \"aten/src/ATen/native/cpu/batch_norm_kernel.cpp\",\n    \"aten/src/ATen/native/cpu/group_norm_kernel.cpp\",\n    \"aten/src/ATen/native/cpu/int4mm_kernel.cpp\",\n    \"aten/src/ATen/native/cpu/int8mm_kernel.cpp\",\n    \"aten/src/ATen/native/cpu/layer_norm_kernel.cpp\",\n    \"aten/src/ATen/native/cpu/AmpGradScalerKernels.cpp\",\n    \"aten/src/ATen/native/cpu/scaled_modified_bessel_k0.cpp\",\n    \"aten/src/ATen/native/cpu/scaled_modified_bessel_k1.cpp\",\n    \"aten/src/ATen/native/cpu/spherical_bessel_j0.cpp\",\n    \"aten/src/ATen/native/cpu/SampledAddmmKernel.cpp\",\n    \"aten/src/ATen/native/cpu/SpmmReduceKernel.cpp\",\n    \"aten/src/ATen/native/cpu/SparseFactories.cpp\",\n    \"aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp\",\n    \"aten/src/ATen/native/cpu/FusedAdamKernel.cpp\",\n    \"aten/src/ATen/native/cpu/FusedSGDKernel.cpp\",\n    \"aten/src/ATen/native/cpu/FusedAdagradKernel.cpp\",\n]\n\n# This aten native source file list will not go through aten codegen process\naten_native_source_non_codegen_list = [\n    \"aten/src/ATen/native/ao_sparse/library.cpp\",\n    \"aten/src/ATen/native/ao_sparse/quantized/cpu/fbgemm_utils.cpp\",\n    \"aten/src/ATen/native/ao_sparse/quantized/cpu/qlinear.cpp\",\n    \"aten/src/ATen/native/ao_sparse/quantized/cpu/qlinear_deserialize.cpp\",\n    \"aten/src/ATen/native/ao_sparse/quantized/cpu/qlinear_dynamic.cpp\",\n    \"aten/src/ATen/native/ao_sparse/quantized/cpu/qlinear_prepack.cpp\",\n    \"aten/src/ATen/native/ao_sparse/quantized/cpu/qlinear_serialize.cpp\",\n    \"aten/src/ATen/native/ao_sparse/quantized/cpu/qlinear_unpack.cpp\",\n    \"aten/src/ATen/native/quantized/cpu/fbgemm_utils.cpp\",\n    \"aten/src/ATen/native/quantized/cpu/fused_obs_fake_quant.cpp\",\n    \"aten/src/ATen/native/quantized/cpu/IntReprQuant.cpp\",\n    \"aten/src/ATen/native/quantized/cpu/MakePerTensorQuantizedTensor.cpp\",\n    \"aten/src/ATen/native/quantized/cpu/AdaptiveAveragePooling.cpp\",\n    \"aten/src/ATen/native/quantized/cpu/AveragePool2d.cpp\",\n    \"aten/src/ATen/native/quantized/cpu/AveragePool3d.cpp\",\n    \"aten/src/ATen/native/quantized/cpu/BinaryOps.cpp\",\n    \"aten/src/ATen/native/quantized/cpu/Normalization.cpp\",\n    \"aten/src/ATen/native/quantized/cpu/ChannelShuffle.cpp\",\n    \"aten/src/ATen/native/quantized/cpu/qclamp.cpp\",\n    \"aten/src/ATen/native/quantized/cpu/TensorShape.cpp\",\n    \"aten/src/ATen/native/quantized/cpu/qconv.cpp\",\n    \"aten/src/ATen/native/quantized/cpu/qconv_prepack.cpp\",\n    \"aten/src/ATen/native/quantized/cpu/qconv_unpack_impl.cpp\",\n    \"aten/src/ATen/native/quantized/cpu/qelu.cpp\",\n    \"aten/src/ATen/native/quantized/cpu/qembeddingbag.cpp\",\n    \"aten/src/ATen/native/quantized/cpu/qembeddingbag_prepack.cpp\",\n    \"aten/src/ATen/native/quantized/cpu/qembeddingbag_unpack.cpp\",\n    \"aten/src/ATen/native/quantized/cpu/qgelu.cpp\",\n    \"aten/src/ATen/native/quantized/cpu/qhardsigmoid.cpp\",\n    \"aten/src/ATen/native/quantized/cpu/qhardswish.cpp\",\n    \"aten/src/ATen/native/quantized/cpu/qlinear.cpp\",\n    \"aten/src/ATen/native/quantized/cpu/qlinear_dynamic.cpp\",\n    \"aten/src/ATen/native/quantized/cpu/qconv_dynamic.cpp\",\n    \"aten/src/ATen/native/quantized/cpu/qlinear_prepack.cpp\",\n    \"aten/src/ATen/native/quantized/cpu/LinearUnpackImpl.cpp\",\n    \"aten/src/ATen/native/quantized/cpu/qmatmul.cpp\",\n    \"aten/src/ATen/native/quantized/cpu/qmul.cpp\",\n    \"aten/src/ATen/native/quantized/cpu/qnormalization.cpp\",\n    \"aten/src/ATen/native/quantized/cpu/Pooling.cpp\",\n    \"aten/src/ATen/native/quantized/cpu/ReduceOps.cpp\",\n    \"aten/src/ATen/native/quantized/cpu/qrelu.cpp\",\n    \"aten/src/ATen/native/quantized/cpu/qsigmoid.cpp\",\n    \"aten/src/ATen/native/quantized/cpu/qsoftmax.cpp\",\n    \"aten/src/ATen/native/quantized/cpu/Sorting.cpp\",\n    \"aten/src/ATen/native/quantized/cpu/qtanh.cpp\",\n    \"aten/src/ATen/native/quantized/cpu/qthreshold.cpp\",\n    \"aten/src/ATen/native/quantized/cpu/UpSampleBilinear2d.cpp\",\n    \"aten/src/ATen/native/quantized/cpu/UpSampleNearest2d.cpp\",\n    \"aten/src/ATen/native/quantized/cpu/UpSampleNearest3d.cpp\",\n    \"aten/src/ATen/native/quantized/cpu/TensorOperators.cpp\",\n    \"aten/src/ATen/native/quantized/Copy.cpp\",\n    \"aten/src/ATen/native/quantized/QTensor.cpp\",\n    \"aten/src/ATen/native/quantized/TensorCompare.cpp\",\n    \"aten/src/ATen/native/quantized/TensorFactories.cpp\",\n    \"aten/src/ATen/native/quantized/AffineQuantizer.cpp\",\n    \"aten/src/ATen/native/quantized/AffineQuantizerBase.cpp\",\n    \"aten/src/ATen/native/quantized/FakeQuantPerChannelAffine.cpp\",\n    \"aten/src/ATen/native/quantized/FakeQuantPerTensorAffine.cpp\",\n    \"aten/src/ATen/native/quantized/library.cpp\",\n    \"aten/src/ATen/native/quantized/TensorAdvancedIndexing.cpp\",\n    \"aten/src/ATen/native/quantized/cpu/RuyUtils.cpp\",\n    \"aten/src/ATen/native/quantized/cpu/XnnpackUtils.cpp\",\n    \"aten/src/ATen/native/quantized/qlinear_unpack.cpp\",\n    \"aten/src/ATen/quantized/QTensorImpl.cpp\",\n    \"aten/src/ATen/quantized/Quantizer.cpp\",\n    \"aten/src/ATen/native/Activation.cpp\",\n    \"aten/src/ATen/native/AdaptiveAveragePooling.cpp\",\n    \"aten/src/ATen/native/AdaptiveAveragePooling3d.cpp\",\n    \"aten/src/ATen/native/AdaptiveMaxPooling2d.cpp\",\n    \"aten/src/ATen/native/AdaptiveMaxPooling3d.cpp\",\n    \"aten/src/ATen/native/AffineGridGenerator.cpp\",\n    \"aten/src/ATen/native/AveragePool2d.cpp\",\n    \"aten/src/ATen/native/AveragePool3d.cpp\",\n    \"aten/src/ATen/native/BatchLinearAlgebra.cpp\",\n    \"aten/src/ATen/native/BatchLinearAlgebraKernel.cpp\",\n    \"aten/src/ATen/native/LegacyBatching.cpp\",\n    \"aten/src/ATen/native/BinaryOps.cpp\",\n    \"aten/src/ATen/native/Blas.cpp\",\n    \"aten/src/ATen/native/BlasKernel.cpp\",\n    \"aten/src/ATen/native/Bucketization.cpp\",\n    \"aten/src/ATen/native/CPUBlas.cpp\",\n    \"aten/src/ATen/native/ChanelShuffle.cpp\",\n    \"aten/src/ATen/native/Col2Im.cpp\",\n    \"aten/src/ATen/native/PadNd.cpp\",\n    \"aten/src/ATen/native/Constraints.cpp\",\n    \"aten/src/ATen/native/Convolution.cpp\",\n    \"aten/src/ATen/native/ConvolutionMM2d.cpp\",\n    \"aten/src/ATen/native/ConvolutionMM3d.cpp\",\n    \"aten/src/ATen/native/ConvolutionTBC.cpp\",\n    \"aten/src/ATen/native/Copy.cpp\",\n    \"aten/src/ATen/native/Correlation.cpp\",\n    \"aten/src/ATen/native/CPUFallback.cpp\",\n    \"aten/src/ATen/native/Cross.cpp\",\n    \"aten/src/ATen/native/DilatedMaxPool2d.cpp\",\n    \"aten/src/ATen/native/DilatedMaxPool3d.cpp\",\n    # Referenced by both native and ATen/Version.cpp. Does not reference to other native symbols\n    # \"aten/src/ATen/native/DispatchStub.cpp\",\n    # \"aten/src/ATen/native/quantized/cpu/init_qnnpack.cpp\",\n    \"aten/src/ATen/native/Distance.cpp\",\n    \"aten/src/ATen/native/Distributions.cpp\",\n    \"aten/src/ATen/native/Dropout.cpp\",\n    \"aten/src/ATen/native/Embedding.cpp\",\n    \"aten/src/ATen/native/EmbeddingBag.cpp\",\n    \"aten/src/ATen/native/Fill.cpp\",\n    \"aten/src/ATen/native/ForeachOpsKernels.cpp\",\n    \"aten/src/ATen/native/FractionalMaxPool2d.cpp\",\n    \"aten/src/ATen/native/FractionalMaxPool3d.cpp\",\n    \"aten/src/ATen/native/FunctionOfAMatrixUtils.cpp\",\n    \"aten/src/ATen/native/GatedLinearUnit.cpp\",\n    \"aten/src/ATen/native/GridSampler.cpp\",\n    \"aten/src/ATen/native/Histogram.cpp\",\n    \"aten/src/ATen/native/Im2Col.cpp\",\n    \"aten/src/ATen/native/IndexingUtils.cpp\",\n    \"aten/src/ATen/native/Integration.cpp\",\n    \"aten/src/ATen/native/Itertools.cpp\",\n    \"aten/src/ATen/native/Lerp.cpp\",\n    \"aten/src/ATen/native/Linear.cpp\",\n    \"aten/src/ATen/native/LinearAlgebra.cpp\",\n    \"aten/src/ATen/native/Loss.cpp\",\n    \"aten/src/ATen/native/LossCTC.cpp\",\n    \"aten/src/ATen/native/LossMultiLabelMargin.cpp\",\n    \"aten/src/ATen/native/LossMultiMargin.cpp\",\n    \"aten/src/ATen/native/LossNLL.cpp\",\n    \"aten/src/ATen/native/LossNLL2d.cpp\",\n    \"aten/src/ATen/native/MaxPooling.cpp\",\n    \"aten/src/ATen/native/MaxUnpooling.cpp\",\n    \"aten/src/ATen/native/Memory.cpp\",\n    \"aten/src/ATen/native/MetaTensor.cpp\",\n    \"aten/src/ATen/native/NNPACK.cpp\",\n    \"aten/src/ATen/native/NaiveConvolutionTranspose2d.cpp\",\n    \"aten/src/ATen/native/NaiveConvolutionTranspose3d.cpp\",\n    \"aten/src/ATen/native/NaiveDilatedConvolution.cpp\",\n    \"aten/src/ATen/native/NamedTensor.cpp\",\n    \"aten/src/ATen/native/Normalization.cpp\",\n    \"aten/src/ATen/native/Onehot.cpp\",\n    \"aten/src/ATen/native/PackedSequence.cpp\",\n    \"aten/src/ATen/native/PixelShuffle.cpp\",\n    \"aten/src/ATen/native/PointwiseOps.cpp\",\n    \"aten/src/ATen/native/Pooling.cpp\",\n    \"aten/src/ATen/native/Pow.cpp\",\n    \"aten/src/ATen/native/QuantizedLinear.cpp\",\n    \"aten/src/ATen/native/RNN.cpp\",\n    \"aten/src/ATen/native/RangeFactories.cpp\",\n    \"aten/src/ATen/native/ReduceAllOps.cpp\",\n    \"aten/src/ATen/native/ReduceOps.cpp\",\n    \"aten/src/ATen/native/ReflectionPad.cpp\",\n    \"aten/src/ATen/native/Repeat.cpp\",\n    \"aten/src/ATen/native/ReplicationPadding.cpp\",\n    \"aten/src/ATen/native/Resize.cpp\",\n    \"aten/src/ATen/native/RowwisePrune.cpp\",\n    \"aten/src/ATen/native/SegmentReduce.cpp\",\n    \"aten/src/ATen/native/Scalar.cpp\",\n    \"aten/src/ATen/native/SobolEngineOps.cpp\",\n    \"aten/src/ATen/native/SobolEngineOpsUtils.cpp\",\n    \"aten/src/ATen/native/SoftMax.cpp\",\n    \"aten/src/ATen/native/Sorting.cpp\",\n    \"aten/src/ATen/native/SparseTensorUtils.cpp\",\n    \"aten/src/ATen/native/SpectralOps.cpp\",\n    \"aten/src/ATen/native/SummaryOps.cpp\",\n    \"aten/src/ATen/native/TensorAdvancedIndexing.cpp\",\n    \"aten/src/ATen/native/TensorCompare.cpp\",\n    \"aten/src/ATen/native/TensorConversions.cpp\",\n    \"aten/src/ATen/native/TensorFactories.cpp\",\n    \"aten/src/ATen/native/TensorIteratorReduce.cpp\",\n    \"aten/src/ATen/native/TensorProperties.cpp\",\n    \"aten/src/ATen/native/TensorShape.cpp\",\n    \"aten/src/ATen/native/TensorTransformations.cpp\",\n    \"aten/src/ATen/native/TestOps.cpp\",\n    \"aten/src/ATen/native/TriangularOps.cpp\",\n    \"aten/src/ATen/native/TypeProperties.cpp\",\n    \"aten/src/ATen/native/UnaryOps.cpp\",\n    \"aten/src/ATen/native/Unfold2d.cpp\",\n    \"aten/src/ATen/native/Unfold3d.cpp\",\n    \"aten/src/ATen/native/UnfoldBackward.cpp\",\n    \"aten/src/ATen/native/Unique.cpp\",\n    # Low-level functions that can be directly referenced\n    # \"aten/src/ATen/native/UpSample.cpp\",\n    \"aten/src/ATen/native/UpSampleBicubic2d.cpp\",\n    \"aten/src/ATen/native/UpSampleBilinear2d.cpp\",\n    \"aten/src/ATen/native/UpSampleLinear1d.cpp\",\n    \"aten/src/ATen/native/UpSampleNearest1d.cpp\",\n    \"aten/src/ATen/native/UpSampleNearest2d.cpp\",\n    \"aten/src/ATen/native/UpSampleNearest3d.cpp\",\n    \"aten/src/ATen/native/UpSampleTrilinear3d.cpp\",\n    \"aten/src/ATen/native/VariableMethodStubs.cpp\",\n    \"aten/src/ATen/native/WeightNorm.cpp\",\n    \"aten/src/ATen/native/group_norm.cpp\",\n    \"aten/src/ATen/native/layer_norm.cpp\",\n    \"aten/src/ATen/native/AmpKernels.cpp\",\n    \"aten/src/ATen/native/mkl/LinearAlgebra.cpp\",\n    \"aten/src/ATen/native/mkl/SparseBlasImpl.cpp\",\n    \"aten/src/ATen/native/mkl/SparseCsrLinearAlgebra.cpp\",\n    \"aten/src/ATen/native/mkl/SpectralOps.cpp\",\n    \"aten/src/ATen/native/nested/NestedTensorAliases.cpp\",\n    \"aten/src/ATen/native/nested/NestedTensorBackward.cpp\",\n    \"aten/src/ATen/native/nested/NestedTensorBinaryOps.cpp\",\n    \"aten/src/ATen/native/nested/NestedTensorFactories.cpp\",\n    \"aten/src/ATen/native/nested/NestedTensorMath.cpp\",\n    \"aten/src/ATen/native/nested/NestedTensorMatmul.cpp\",\n    \"aten/src/ATen/native/nested/NestedTensorTransformerFunctions.cpp\",\n    \"aten/src/ATen/native/nested/NestedTensorUnaryOps.cpp\",\n    \"aten/src/ATen/native/nested/NestedTensorUtils.cpp\",\n    \"aten/src/ATen/native/sparse/ParamUtils.cpp\",\n    \"aten/src/ATen/native/sparse/SoftMax.cpp\",\n    \"aten/src/ATen/native/sparse/SparseBlas.cpp\",\n    \"aten/src/ATen/native/sparse/SparseBlasImpl.cpp\",\n    \"aten/src/ATen/native/sparse/SparseMatMul.cpp\",\n    \"aten/src/ATen/native/sparse/SparseTensor.cpp\",\n    \"aten/src/ATen/native/sparse/SparseCsrTensor.cpp\",\n    \"aten/src/ATen/native/sparse/SparseTensorMath.cpp\",\n    \"aten/src/ATen/native/sparse/SparseUnaryOps.cpp\",\n    \"aten/src/ATen/native/sparse/SparseCsrTensorMath.cpp\",\n    \"aten/src/ATen/native/sparse/SparseFactories.cpp\",\n    \"aten/src/ATen/native/sparse/ValidateCompressedIndicesKernel.cpp\",\n    \"aten/src/ATen/native/sparse/SparseBinaryOpIntersectionKernel.cpp\",\n    \"aten/src/ATen/native/sparse/FlattenIndicesKernel.cpp\",\n    \"aten/src/ATen/native/transformers/attention.cpp\",\n    \"aten/src/ATen/native/transformers/sdp_utils_cpp.cpp\",\n    \"aten/src/ATen/native/transformers/transformer.cpp\",\n    \"aten/src/ATen/native/xnnpack/Activation.cpp\",\n    \"aten/src/ATen/native/xnnpack/ChannelShuffle.cpp\",\n    \"aten/src/ATen/native/xnnpack/Convolution.cpp\",\n    \"aten/src/ATen/native/xnnpack/AveragePooling.cpp\",\n    \"aten/src/ATen/native/xnnpack/Init.cpp\",\n    \"aten/src/ATen/native/xnnpack/Linear.cpp\",\n    \"aten/src/ATen/native/xnnpack/MaxPooling.cpp\",\n    \"aten/src/ATen/native/xnnpack/OpContext.cpp\",\n    \"aten/src/ATen/native/xnnpack/RegisterOpContextClass.cpp\",\n    \"aten/src/ATen/native/xnnpack/Shim.cpp\",\n    \"aten/src/ATen/native/FusedAdam.cpp\",\n    \"aten/src/ATen/native/FusedSGD.cpp\",\n    \"aten/src/ATen/native/FusedAdagrad.cpp\",\n    # Files not in native, but depends on native symbols\n    # \"aten/src/ATen/TensorIndexing.cpp\",\n    \"aten/src/ATen/TensorIterator.cpp\",\n]\n\n# 1. Files in ATen/native with a few exceptions\n# TODO: move the exceptions to proper locations\n# 2. The whole aten native source list includes the list with and without aten codegen process.\naten_native_source_list = sorted(aten_native_source_non_codegen_list + aten_native_source_codegen_list)\n\n# These are cpp files which need to go in the torch_cuda_cu library\n# .cu files can be found via glob\naten_cuda_cu_source_list = [\n    \"aten/src/ATen/cuda/CUDABlas.cpp\",\n    \"aten/src/ATen/cuda/CUDASparseBlas.cpp\",\n    \"aten/src/ATen/cuda/CublasHandlePool.cpp\",\n    \"aten/src/ATen/native/cuda/linalg/CudssHandlePool.cpp\",\n    \"aten/src/ATen/cuda/tunable/StreamTimer.cpp\",\n    \"aten/src/ATen/cuda/tunable/Tunable.cpp\",\n    \"aten/src/ATen/native/cuda/Activation.cpp\",\n    \"aten/src/ATen/native/cuda/LinearAlgebraStubs.cpp\",\n    \"aten/src/ATen/native/cuda/Blas.cpp\",\n    \"aten/src/ATen/native/cuda/Distributions.cpp\",\n    \"aten/src/ATen/native/cuda/Equal.cpp\",\n    \"aten/src/ATen/native/cuda/GridSampler.cpp\",\n    \"aten/src/ATen/native/cuda/IndexKernel.cpp\",\n    \"aten/src/ATen/native/cuda/ReduceOps.cpp\",\n    \"aten/src/ATen/native/cuda/ScanKernels.cpp\",\n    \"aten/src/ATen/native/cuda/Sort.cpp\",\n    \"aten/src/ATen/native/cuda/Sorting.cpp\",\n    \"aten/src/ATen/native/cuda/TensorModeKernel.cpp\",\n    \"aten/src/ATen/native/cuda/TensorShapeCUDA.cpp\",\n    \"aten/src/ATen/native/cuda/TensorTopK.cpp\",\n    \"aten/src/ATen/native/cuda/jit_utils.cpp\",\n    \"aten/src/ATen/native/nested/cuda/NestedTensorTransformerFunctions.cpp\",\n    \"aten/src/ATen/native/sparse/cuda/SparseBlas.cpp\",\n    \"aten/src/ATen/native/sparse/cuda/SparseBlasImpl.cpp\",\n    \"aten/src/ATen/native/sparse/cuda/SparseBlasLegacy.cpp\",\n    \"aten/src/ATen/native/sparse/cuda/SparseCUDABlas.cpp\",\n    \"aten/src/ATen/native/transformers/cuda/flash_attn/flash_api.cpp\",\n]\n\n# Files using thrust::sort_by_key need to be linked last\naten_cuda_with_sort_by_key_source_list = [\n    # empty_cuda is needed by torch_cuda_cpp\n    \"aten/src/ATen/native/cuda/TensorFactories.cu\",\n]\n\naten_cuda_cu_with_sort_by_key_source_list = [\n    \"aten/src/ATen/native/cuda/Unique.cu\",\n]\n\n# Followings are source code for xnnpack delegate\n\nxnnpack_delegate_serializer_header = [\n    \"torch/csrc/jit/backends/xnnpack/serialization/serializer.h\",\n]\n\nxnnpack_delegate_serializer_source_list = [\n    \"torch/csrc/jit/backends/xnnpack/serialization/serializer.cpp\",\n]\n\nxnnpack_delegate_core_source_list = [\n    \"torch/csrc/jit/backends/xnnpack/compiler/xnn_compiler.cpp\",\n]\n\nxnnpack_delegate_core_header = [\n    \"torch/csrc/jit/backends/xnnpack/compiler/xnn_compiler.h\",\n    \"torch/csrc/jit/backends/xnnpack/executor/xnn_executor.h\",\n]\n\nxnnpack_backend_header = [\n    \"torch/csrc/jit/backends/xnnpack/xnnpack_graph_builder.h\",\n] + xnnpack_delegate_core_header\n\nxnnpack_backend_source_list = [\n    \"torch/csrc/jit/backends/xnnpack/compiler/xnn_compiler.cpp\",\n    \"torch/csrc/jit/backends/xnnpack/xnnpack_backend_lib.cpp\",\n    \"torch/csrc/jit/backends/xnnpack/xnnpack_backend_preprocess.cpp\",\n    \"torch/csrc/jit/backends/xnnpack/xnnpack_graph_builder.cpp\",\n] + xnnpack_delegate_core_source_list\n"
        },
        {
          "name": "c10",
          "type": "tree",
          "content": null
        },
        {
          "name": "caffe2",
          "type": "tree",
          "content": null
        },
        {
          "name": "cmake",
          "type": "tree",
          "content": null
        },
        {
          "name": "defs.bzl",
          "type": "blob",
          "size": 2.66,
          "content": "def get_blas_gomp_arch_deps():\n    return [\n        (\"x86_64\", [\n            \"fbsource//third-party/mkl:{}\".format(native.read_config(\"fbcode\", \"mkl_lp64\", \"mkl_lp64_omp\")),\n        ]),\n        (\"aarch64\", [\n            \"third-party//OpenBLAS:OpenBLAS\",\n            \"third-party//openmp:omp\",\n        ]),\n    ]\n\ndefault_compiler_flags = [\n    \"-Wall\",\n    \"-Wextra\",\n    \"-Wno-unused-function\",\n    \"-Wno-unused-parameter\",\n    \"-Wno-error=strict-aliasing\",\n    \"-Wno-shadow-compatible-local\",\n    \"-Wno-maybe-uninitialized\",  # aten is built with gcc as part of HHVM\n    \"-Wno-unknown-pragmas\",\n    \"-Wno-strict-overflow\",\n    # See https://fb.facebook.com/groups/fbcode/permalink/1813348245368673/\n    # These trigger on platform007\n    \"-Wno-stringop-overflow\",\n    \"-Wno-class-memaccess\",\n    \"-DHAVE_MMAP\",\n    \"-DUSE_GCC_ATOMICS=1\",\n    \"-D_FILE_OFFSET_BITS=64\",\n    \"-DHAVE_SHM_OPEN=1\",\n    \"-DHAVE_SHM_UNLINK=1\",\n    \"-DHAVE_MALLOC_USABLE_SIZE=1\",\n    \"-DCPU_CAPABILITY_DEFAULT\",\n    \"-DTH_INDEX_BASE=0\",\n    \"-DMAGMA_V2\",\n    \"-DNO_CUDNN_DESTROY_HANDLE\",\n    \"-DUSE_FBGEMM\",\n    \"-DUSE_PYTORCH_QNNPACK\",\n    # The dynamically loaded NVRTC trick doesn't work in fbcode,\n    # and it's not necessary anyway, because we have a stub\n    # nvrtc library which we load canonically anyway\n    \"-DUSE_DIRECT_NVRTC\",\n    \"-DUSE_RUY_QMATMUL\",\n] + select({\n    # XNNPACK depends on an updated version of pthreadpool interface, whose implementation\n    # includes <pthread.h> - a header not available on Windows.\n    \"DEFAULT\": [\"-DUSE_XNNPACK\"],\n    \"ovr_config//os:windows\": [],\n}) + ([\"-O1\"] if native.read_config(\"fbcode\", \"build_mode_test_label\", \"\") == \"dev-nosan\" else [])\n\ncompiler_specific_flags = {\n    \"clang\": [\n        \"-Wno-absolute-value\",\n        \"-Wno-pass-failed\",\n        \"-Wno-braced-scalar-init\",\n    ],\n    \"gcc\": [\n        \"-Wno-error=array-bounds\",\n    ],\n}\n\ndef get_cpu_parallel_backend_flags():\n    parallel_backend = native.read_config(\"pytorch\", \"parallel_backend\", \"openmp\")\n    defs = []\n    if parallel_backend == \"openmp\":\n        defs.append(\"-DAT_PARALLEL_OPENMP_FBCODE=1\")\n    elif parallel_backend == \"native\":\n        defs.append(\"-DAT_PARALLEL_NATIVE_FBCODE=1\")\n    else:\n        fail(\"Unsupported parallel backend: \" + parallel_backend)\n    if native.read_config(\"pytorch\", \"exp_single_thread_pool\", \"0\") == \"1\":\n        defs.append(\"-DAT_EXPERIMENTAL_SINGLE_THREAD_POOL=1\")\n    mkl_ver = native.read_config(\"fbcode\", \"mkl_lp64\", \"mkl_lp64_omp\")\n    if mkl_ver == \"mkl_lp64_seq\":\n        defs.append(\"-DATEN_MKL_SEQUENTIAL_FBCODE=1\")\n    return defs\n\ndef is_cpu_static_dispatch_build():\n    mode = native.read_config(\"fbcode\", \"caffe2_static_dispatch_mode\", \"none\")\n    return mode == \"cpu\"\n"
        },
        {
          "name": "docker.Makefile",
          "type": "blob",
          "size": 3.8,
          "content": "DOCKER_REGISTRY          ?= docker.io\nDOCKER_ORG               ?= $(shell docker info 2>/dev/null | sed '/Username:/!d;s/.* //')\nDOCKER_IMAGE             ?= pytorch\nDOCKER_FULL_NAME          = $(DOCKER_REGISTRY)/$(DOCKER_ORG)/$(DOCKER_IMAGE)\n\nifeq (\"$(DOCKER_ORG)\",\"\")\n$(warning WARNING: No docker user found using results from whoami)\nDOCKER_ORG                = $(shell whoami)\nendif\n\nCUDA_VERSION_SHORT       ?= 12.1\nCUDA_VERSION             ?= 12.1.1\nCUDNN_VERSION            ?= 9\nBASE_RUNTIME              = ubuntu:22.04\nBASE_DEVEL                = nvidia/cuda:$(CUDA_VERSION)-devel-ubuntu22.04\nCMAKE_VARS               ?=\n\n# The conda channel to use to install cudatoolkit\nCUDA_CHANNEL              = nvidia\n# The conda channel to use to install pytorch / torchvision\nINSTALL_CHANNEL          ?= whl\n\nCUDA_PATH                ?= cpu\nifneq (\"$(CUDA_VERSION_SHORT)\",\"cpu\")\nCUDA_PATH                = cu$(subst .,,$(CUDA_VERSION_SHORT))\nendif\n\nPYTHON_VERSION           ?= 3.11\n# Match versions that start with v followed by a number, to avoid matching with tags like ciflow\nPYTORCH_VERSION          ?= $(shell git describe --tags --always --match \"v[1-9]*.*\")\n# Can be either official / dev\nBUILD_TYPE               ?= dev\nBUILD_PROGRESS           ?= auto\n# Intentionally left blank\nTRITON_VERSION           ?=\nBUILD_ARGS                = --build-arg BASE_IMAGE=$(BASE_IMAGE) \\\n\t\t\t\t\t\t\t--build-arg PYTHON_VERSION=$(PYTHON_VERSION) \\\n\t\t\t\t\t\t\t--build-arg CUDA_VERSION=$(CUDA_VERSION) \\\n\t\t\t\t\t\t\t--build-arg CUDA_PATH=$(CUDA_PATH) \\\n\t\t\t\t\t\t\t--build-arg PYTORCH_VERSION=$(PYTORCH_VERSION) \\\n\t\t\t\t\t\t\t--build-arg INSTALL_CHANNEL=$(INSTALL_CHANNEL) \\\n\t\t\t\t\t\t\t--build-arg TRITON_VERSION=$(TRITON_VERSION) \\\n\t\t\t\t\t\t\t--build-arg CMAKE_VARS=\"$(CMAKE_VARS)\"\nEXTRA_DOCKER_BUILD_FLAGS ?=\n\nBUILD                    ?= build\n# Intentionally left blank\nPLATFORMS_FLAG           ?=\nPUSH_FLAG                ?=\nUSE_BUILDX               ?=\nBUILD_PLATFORMS          ?=\nWITH_PUSH                ?= false\n# Setup buildx flags\nifneq (\"$(USE_BUILDX)\",\"\")\nBUILD                     = buildx build\nifneq (\"$(BUILD_PLATFORMS)\",\"\")\nPLATFORMS_FLAG            = --platform=\"$(BUILD_PLATFORMS)\"\nendif\n# Only set platforms flags if using buildx\nifeq (\"$(WITH_PUSH)\",\"true\")\nPUSH_FLAG                 = --push\nendif\nendif\n\nDOCKER_BUILD              = docker $(BUILD) \\\n\t\t\t\t\t\t\t\t--progress=$(BUILD_PROGRESS) \\\n\t\t\t\t\t\t\t\t$(EXTRA_DOCKER_BUILD_FLAGS) \\\n\t\t\t\t\t\t\t\t$(PLATFORMS_FLAG) \\\n\t\t\t\t\t\t\t\t$(PUSH_FLAG) \\\n\t\t\t\t\t\t\t\t--target $(BUILD_TYPE) \\\n\t\t\t\t\t\t\t\t-t $(DOCKER_FULL_NAME):$(DOCKER_TAG) \\\n\t\t\t\t\t\t\t\t$(BUILD_ARGS) .\nDOCKER_PUSH               = docker push $(DOCKER_FULL_NAME):$(DOCKER_TAG)\n\n.PHONY: all\nall: devel-image\n\n.PHONY: devel-image\ndevel-image: BASE_IMAGE := $(BASE_DEVEL)\ndevel-image: DOCKER_TAG := $(PYTORCH_VERSION)-cuda$(CUDA_VERSION_SHORT)-cudnn$(CUDNN_VERSION)-devel\ndevel-image:\n\t$(DOCKER_BUILD)\n\n.PHONY: devel-push\ndevel-push: BASE_IMAGE := $(BASE_DEVEL)\ndevel-push: DOCKER_TAG := $(PYTORCH_VERSION)-cuda$(CUDA_VERSION_SHORT)-cudnn$(CUDNN_VERSION)-devel\ndevel-push:\n\t$(DOCKER_PUSH)\n\nifeq (\"$(CUDA_VERSION_SHORT)\",\"cpu\")\n\n.PHONY: runtime-image\nruntime-image: BASE_IMAGE := $(BASE_RUNTIME)\nruntime-image: DOCKER_TAG := $(PYTORCH_VERSION)-runtime\nruntime-image:\n\t$(DOCKER_BUILD)\n\n.PHONY: runtime-push\nruntime-push: BASE_IMAGE := $(BASE_RUNTIME)\nruntime-push: DOCKER_TAG := $(PYTORCH_VERSION)-runtime\nruntime-push:\n\t$(DOCKER_PUSH)\n\nelse\n\n.PHONY: runtime-image\nruntime-image: BASE_IMAGE := $(BASE_RUNTIME)\nruntime-image: DOCKER_TAG := $(PYTORCH_VERSION)-cuda$(CUDA_VERSION_SHORT)-cudnn$(CUDNN_VERSION)-runtime\nruntime-image:\n\t$(DOCKER_BUILD)\n\n.PHONY: runtime-push\nruntime-push: BASE_IMAGE := $(BASE_RUNTIME)\nruntime-push: DOCKER_TAG := $(PYTORCH_VERSION)-cuda$(CUDA_VERSION_SHORT)-cudnn$(CUDNN_VERSION)-runtime\nruntime-push:\n\t$(DOCKER_PUSH)\n\nendif\n\n.PHONY: clean\nclean:\n\t-docker rmi -f $(shell docker images -q $(DOCKER_FULL_NAME))\n"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "functorch",
          "type": "tree",
          "content": null
        },
        {
          "name": "mypy-strict.ini",
          "type": "blob",
          "size": 1.6,
          "content": "# This is the PyTorch mypy-strict.ini file (note: don't change this line! -\n# test_run_mypy in test/test_type_hints.py uses this string)\n\n# Unlike mypy.ini, it enforces very strict typing rules. The intention is for\n# this config file to be used to ENFORCE that people are using mypy on codegen\n# files.\n\n[mypy]\npython_version = 3.9\nplugins = mypy_plugins/check_mypy_version.py, numpy.typing.mypy_plugin\n\ncache_dir = .mypy_cache/strict\nallow_redefinition = True\nstrict_optional = True\nshow_error_codes = True\nshow_column_numbers = True\nwarn_no_return = True\ndisallow_any_unimported = True\n\nstrict = True\nimplicit_reexport = False\n\n# do not reenable this:\n# https://github.com/pytorch/pytorch/pull/60006#issuecomment-866130657\nwarn_unused_ignores = False\n\nfiles =\n    .github,\n    benchmarks/instruction_counts,\n    tools,\n    torch/profiler/_memory_profiler.py,\n    torch/utils/_pytree.py,\n    torch/utils/_cxx_pytree.py,\n    torch/utils/benchmark/utils/common.py,\n    torch/utils/benchmark/utils/timer.py,\n    torch/utils/benchmark/utils/valgrind_wrapper\n\n# Specifically enable imports of benchmark utils. As more of `torch` becomes\n# strict compliant, those modules can be enabled as well.\n[mypy-torch.utils.benchmark.utils.*]\nfollow_imports = normal\n\n# Don't follow imports as much of `torch` is not strict compliant.\n[mypy-torch]\nfollow_imports = skip\n\n[mypy-torch.*]\nfollow_imports = skip\n\n# Missing stubs.\n\n[mypy-numpy]\nignore_missing_imports = True\n\n[mypy-sympy]\nignore_missing_imports = True\n\n[mypy-sympy.*]\nignore_missing_imports = True\n\n[mypy-mypy.*]\nignore_missing_imports = True\n\n[mypy-usort.*]\nignore_missing_imports = True\n"
        },
        {
          "name": "mypy.ini",
          "type": "blob",
          "size": 6.18,
          "content": "# This is the PyTorch mypy.ini file (note: don't change this line! -\n# test_run_mypy in test/test_type_hints.py uses this string)\n\n[mypy]\nplugins = mypy_plugins/check_mypy_version.py, mypy_plugins/sympy_mypy_plugin.py, numpy.typing.mypy_plugin\n\ncache_dir = .mypy_cache/normal\nallow_redefinition = True\nwarn_unused_configs = True\nwarn_redundant_casts = True\nshow_error_codes = True\nshow_column_numbers = True\ncheck_untyped_defs = True\ndisallow_untyped_defs = True\ndisallow_untyped_decorators = True\nfollow_imports = normal\nlocal_partial_types = True\nenable_error_code = possibly-undefined\n\n# do not reenable this:\n# https://github.com/pytorch/pytorch/pull/60006#issuecomment-866130657\nwarn_unused_ignores = False\n\n#\n# Note: test/ still has syntax errors so can't be added\n#\n# Typing tests is low priority, but enabling type checking on the\n# untyped test functions (using `--check-untyped-defs`) is still\n# high-value because it helps test the typing.\n#\n\nfiles =\n    torch,\n    caffe2,\n    test/test_bundled_images.py,\n    test/test_bundled_inputs.py,\n    test/test_complex.py,\n    test/test_datapipe.py,\n    test/test_futures.py,\n    test/test_numpy_interop.py,\n    test/test_torch.py,\n    test/test_type_hints.py,\n    test/test_type_info.py,\n    test/test_utils.py\n\n#\n# `exclude` is a regex, not a list of paths like `files` (sigh)\n#\nexclude = torch/include/|torch/csrc/|torch/distributed/elastic/agent/server/api.py|torch/testing/_internal|torch/distributed/fsdp/fully_sharded_data_parallel.py\n\npython_version = 3.11\n\n\n#\n# Extension modules without stubs.\n#\n\n[mypy-torch._C._jit_tree_views]\nignore_missing_imports = True\n\n[mypy-torch.for_onnx.onnx]\nignore_missing_imports = True\n\n[mypy-torch.ao.quantization.experimental.apot_utils]\nignore_missing_imports = True\n\n[mypy-torch.ao.quantization.experimental.quantizer]\nignore_missing_imports = True\n\n[mypy-torch.ao.quantization.experimental.observer]\nignore_missing_imports = True\n\n[mypy-torch.ao.quantization.experimental.APoT_tensor]\nignore_missing_imports = True\n\n[mypy-torch.ao.quantization.experimental.fake_quantize_function]\nignore_missing_imports = True\n\n[mypy-torch.ao.quantization.experimental.fake_quantize]\nignore_missing_imports = True\n\n[mypy-torch.ao.quantization.pt2e._affine_quantization]\nignore_errors = True\n\n#\n# Files with various errors. Mostly real errors, possibly some false\n# positives as well.\n#\n\n[mypy-test_torch]\ncheck_untyped_defs = False\n\n# Excluded from mypy due to OpInfos being annoying to type\n[mypy-torch.testing._internal.common_methods_invocations.*]\nignore_errors = True\n\n[mypy-torch.testing._internal.hypothesis_utils.*]\nignore_errors = True\n\n[mypy-torch.testing._internal.common_quantization.*]\nignore_errors = True\n\n[mypy-torch.testing._internal.generated.*]\nignore_errors = True\n\n[mypy-torch.testing._internal.distributed.*]\nignore_errors = True\n\n[mypy-torch.nn.modules.pooling]\nignore_errors = True\n\n[mypy-torch.nn.parallel._functions]\nignore_errors = True\n\n[mypy-torch._appdirs]\nignore_errors = True\n\n[mypy-torch.multiprocessing.pool]\nignore_errors = True\n\n[mypy-torch.overrides]\nignore_errors = True\n\n#\n# Files with 'type: ignore' comments that are needed if checked with mypy-strict.ini\n#\n\n[mypy-tools.render_junit]\nwarn_unused_ignores = False\n\n[mypy-tools.generate_torch_version]\nwarn_unused_ignores = False\n\n#\n# Adding type annotations to caffe2 is probably not worth the effort\n# only work on this if you have a specific reason for it, otherwise\n# leave these ignores as they are.\n#\n\n[mypy-caffe2.python.*]\nignore_errors = True\n\n[mypy-caffe2.proto.*]\nignore_errors = True\n\n[mypy-caffe2.distributed.store_ops_test_util]\nignore_errors = True\n\n[mypy-caffe2.experiments.*]\nignore_errors = True\n\n[mypy-caffe2.contrib.*]\nignore_errors = True\n\n[mypy-caffe2.quantization.server.*]\nignore_errors = True\n\n#\n# Third party dependencies that don't have types.\n#\n\n[mypy-triton.*]\nignore_missing_imports = True\n\n[mypy-tensorflow.*]\nignore_missing_imports = True\n\n[mypy-tensorboard.*]\nignore_missing_imports = True\n\n[mypy-matplotlib.*]\nignore_missing_imports = True\n\n[mypy-numpy.*]\nignore_missing_imports = True\n\n[mypy-sympy]\nignore_missing_imports = True\n\n[mypy-sympy.*]\nignore_missing_imports = True\n\n[mypy-hypothesis.*]\nignore_missing_imports = True\n\n[mypy-tqdm.*]\nignore_missing_imports = True\n\n[mypy-multiprocessing.*]\nignore_missing_imports = True\n\n[mypy-setuptools.*]\nignore_missing_imports = True\n\n[mypy-distutils.*]\nignore_missing_imports = True\n\n[mypy-nvd3.*]\nignore_missing_imports = True\n\n[mypy-future.utils]\nignore_missing_imports = True\n\n[mypy-past.builtins]\nignore_missing_imports = True\n\n[mypy-numba.*]\nignore_missing_imports = True\n\n[mypy-PIL.*]\nignore_missing_imports = True\n\n[mypy-moviepy.*]\nignore_missing_imports = True\n\n[mypy-cv2.*]\nignore_missing_imports = True\n\n[mypy-torchvision.*]\nignore_missing_imports = True\n\n[mypy-pycuda.*]\nignore_missing_imports = True\n\n[mypy-tensorrt.*]\nignore_missing_imports = True\n\n[mypy-tornado.*]\nignore_missing_imports = True\n\n[mypy-pydot.*]\nignore_missing_imports = True\n\n[mypy-networkx.*]\nignore_missing_imports = True\n\n[mypy-scipy.*]\nignore_missing_imports = True\n\n[mypy-IPython.*]\nignore_missing_imports = True\n\n[mypy-google.protobuf.textformat]\nignore_missing_imports = True\n\n[mypy-lmdb.*]\nignore_missing_imports = True\n\n[mypy-mpi4py.*]\nignore_missing_imports = True\n\n[mypy-skimage.*]\nignore_missing_imports = True\n\n[mypy-librosa.*]\nignore_missing_imports = True\n\n[mypy-mypy.*]\nignore_missing_imports = True\n\n[mypy-xml.*]\nignore_missing_imports = True\n\n[mypy-boto3.*]\nignore_missing_imports = True\n\n[mypy-dill.*]\nignore_missing_imports = True\n\n[mypy-usort.*]\nignore_missing_imports = True\n\n[mypy-torch._inductor.*]\ndisallow_any_generics = True\n\n[mypy-torch._dynamo.*]\ndisallow_any_generics = True\n\n[mypy-cutlass_library.*]\nignore_missing_imports = True\n\n[mypy-deeplearning.*]\nignore_missing_imports = True\n\n[mypy-einops.*]\nignore_missing_imports = True\n\n[mypy-libfb.*]\nignore_missing_imports = True\n\n[mypy-torch.*.fb.*]\nignore_missing_imports = True\n\n[mypy-torch.fb.*]\nignore_missing_imports = True\n\n[mypy-torch_xla.*]\nignore_missing_imports = True\n\n#\n# Third party dependencies that are optional.\n#\n\n[mypy-onnx.*]\nignore_missing_imports = True\n\n[mypy-onnxruntime.*]\nignore_missing_imports = True\n\n[mypy-onnxscript.*]\nignore_missing_imports = True\n\n[mypy-redis]\nignore_missing_imports = True"
        },
        {
          "name": "mypy_plugins",
          "type": "tree",
          "content": null
        },
        {
          "name": "pt_ops.bzl",
          "type": "blob",
          "size": 19.87,
          "content": "load(\"//tools/build_defs:expect.bzl\", \"expect\")\nload(\"//tools/build_defs:fb_native_wrapper.bzl\", \"fb_native\")\nload(\"//tools/build_defs:fb_xplat_genrule.bzl\", \"fb_xplat_genrule\")\nload(\"//tools/build_defs:type_defs.bzl\", \"is_list\", \"is_string\")\n\nIS_OSS = read_config(\"pt\", \"is_oss\", \"0\") == \"1\"  # True for OSS BUCK build, and False for internal BUCK build\n\nUSED_PT_BACKENDS = [\n    \"CPU\",\n    \"QuantizedCPU\",\n    \"SparseCPU\",  # brings ~20 kb size regression\n]\n\ndef pt_operator_library(\n        name,\n        ops = [],\n        exported_deps = [],\n        check_decl = True,\n        train = False,\n        model = None,\n        include_all_operators = False,\n        include_base_operators = True,\n        **kwargs):\n    (model_name, model_versions, model_assets, model_traced_backends) = validate_and_extract_model_information(\n        name,\n        model,\n    )\n\n    ops = [op.strip() for op in ops]\n\n    # If ops are specified, then we are in static selective build mode, so we append\n    # base ops to this list to avoid additional special case logic in subsequent code,\n    # unless include_base_operators is explicitly set to False (the default is True)\n    if len(ops) > 0 and include_base_operators:\n        ops.extend(PT_BASE_OPS)\n\n    labels = kwargs.pop(\"labels\", [])\n    visibility = kwargs.pop(\"visibility\", [\"PUBLIC\"])\n\n    # Sanity check the model name and versions.  While the input to both is an array, the\n    # codegen script only ever outputs a single item in the array so we can just assume that\n    # here. If you ever need to depends on more than one assets, just break it up into a separate\n    # BUCK targets.\n    if model_assets or model_versions:\n        if len(model_assets) != 1:\n            fail(\"Model assets must be of size 1\")\n        if len(model_versions) != 1:\n            fail(\"Model versions must be of size 1\")\n\n    # Is this a traced operator therefore has a YAML file with ops?\n    yaml_option = \"\"\n    if model_assets and len(model_assets) > 0:\n        # We know these lists are only of length 1 via earlier assert.\n        model_asset = model_assets[0]\n        model_version = model_versions[0]\n\n        # Pass the YAML file from this asset to the genrule below.\n        yaml_dep = \"{}_v{}_yaml\".format(model_asset, model_version)\n        fb_native.filegroup(\n            name = yaml_dep,\n            srcs = [\n                model_asset + \".yaml\",\n            ],\n            # The visibility is not set to PUBLIC as this an internal detail.  If you see this error\n            # in your buck build flow, you are trying to use a hand-crafted \"pt_operator_library\" that\n            # with parameters not supported outside of codegen targets!\n        )\n\n        # Since all selective traced ops are created by automation, we can assume they\n        # have a YAML file at this very location.  If it doesn't exist, it means the targets\n        # was hand-crafted which is not a support workflow for traced ops.\n        yaml_option = \"--models_yaml_path $(location fbsource//xplat/pytorch_models/build/{}/v{}:{})/{}.yaml\".format(model_name, model_version, yaml_dep, model_asset)\n\n    not_include_all_overloads_static_root_ops = kwargs.pop(\n        \"not_include_all_overloads_static_root_ops\",\n        False,\n    )\n\n    not_include_all_overloads_closure_ops = kwargs.pop(\"not_include_all_overloads_closure_ops\", False)\n\n    if False:\n        # TODO(nga): `yaml_option` is never `None`, but it is checked against `None` below.\n        #   Typechecker (`--unstable-typecheck`) catches it.\n        yaml_option = None\n\n    fb_xplat_genrule(\n        name = name,\n        out = \"model_operators.yaml\",\n        cmd = (\n            \"$(exe {exe}) \" +\n            \"{optionally_root_ops} \" +\n            \"{optionally_training_root_ops} \" +\n            \"--rule_name {rule_name} \" +\n            \"--output_path \\\"${{OUT}}\\\" \" +\n            \"--model_name {model_name} \" +\n            \"--dep_graph_yaml_path {dep_graph_yaml} \" +\n            \"{optionally_model_yamls} \" +\n            \"{optionally_model_versions} \" +\n            \"{optionally_model_assets} \" +\n            \"{optionally_model_traced_backends} \" +\n            \"{optionally_include_all_operators}\" +\n            \"{not_include_all_overloads_static_root_ops}\" +\n            \"{not_include_all_overloads_closure_ops}\"\n        ).format(\n            exe = \"//tools:gen_operators_yaml\" if IS_OSS else \"fbsource//xplat/caffe2/tools:gen_operators_yaml\",\n            rule_name = name,\n            model_name = model_name,\n            dep_graph_yaml = \"none\" if IS_OSS else \"$(location fbsource//xplat/caffe2:pytorch_op_deps)/fb/pytorch_op_deps.yaml \",\n            optionally_model_yamls = \"\" if (IS_OSS or yaml_option == None) else yaml_option,\n            optionally_root_ops = \"--root_ops \" + (\",\".join(ops)) if len(ops) > 0 else \"\",\n            optionally_training_root_ops = \"--training_root_ops \" + (\",\".join(ops)) if len(ops) > 0 and train else \"\",\n            optionally_model_versions = \"--model_versions \" + (\",\".join(model_versions)) if model_versions != None else \"\",\n            optionally_model_assets = \"--model_assets \" + (\",\".join(model_assets)) if model_assets != None else \"\",\n            optionally_model_traced_backends = \"--model_traced_backends \" + (\",\".join(model_traced_backends)) if model_traced_backends != None else \"\",\n            optionally_include_all_operators = \"--include_all_operators \" if include_all_operators else \"\",\n            not_include_all_overloads_static_root_ops = \"--not_include_all_overloads_static_root_ops \" if not_include_all_overloads_static_root_ops else \"\",\n            not_include_all_overloads_closure_ops = \"--not_include_all_overloads_closure_ops \" if not_include_all_overloads_closure_ops else \"\",\n        ),\n        labels = labels + [\n            \"pt_operator_library\",\n            \"supermodule:android/default/pytorch\",\n            \"supermodule:ios/default/public.pytorch\",\n        ] + ([\"pt_train_operator_library\"] if train else []),\n        visibility = visibility,\n        **kwargs\n    )\n\ndef validate_and_extract_model_information(name, model):\n    model_name = name\n    model_versions = None\n    model_assets = None\n    model_traced_backends = None\n\n    if model != None:\n        model_name = model.get(\"name\")\n        expect(model_name != None, \"Expected Model Name to be present\")\n        model_versions = model.get(\"versions\")\n        expect(is_list(model_versions), \"Expected model versions to be a list of string\")\n        for ver in model_versions or []:\n            expect(is_string(ver), \"Expected version '{}' to be string\".format(str(ver)))\n        model_assets = model.get(\"assets\")\n        expect(\n            model_assets == None or is_list(model_assets),\n            \"Expected model assets to be a list of string if specified\",\n        )\n        for asset_name in model_assets or []:\n            expect(is_string(asset_name), \"Expected asset_name '{}' to be string\".format(str(asset_name)))\n        model_traced_backends = model.get(\"traced_backends\")\n        expect(\n            model_traced_backends == None or is_list(model_traced_backends),\n            \"Expected model traced backends to be a list of string if specified\",\n        )\n\n        if model_traced_backends != None:\n            for backend in model_traced_backends:\n                expect(is_string(backend), \"Expected backend name '{}' to be string\".format(str(backend)))\n                expect(\n                    backend in USED_PT_BACKENDS,\n                    \"Expected backend name ({}) to be in set: {}\".format(backend, \",\".join(USED_PT_BACKENDS)),\n                )\n\n    return (model_name, model_versions, model_assets, model_traced_backends)\n\n# This file keeps a list of PyTorch operators used by any targets in\n# @fbsource//xplat/...\n# The purpose of the list is to avoid generating large number of unused\n# operator registration code / BUCK rules at build time.\n# See more detail at: https://fb.quip.com/ZVh1AgOKW8Vv\n\nPT_OPS_PRIM = [\n    \"aten::str\",\n    \"aten::list\",\n    \"aten::__range_length\",\n    \"aten::__derive_index\",\n    \"prim::TupleUnpack\",\n    \"prim::unchecked_cast\",\n    \"aten::IntImplicit\",\n    \"aten::FloatImplicit\",\n    \"aten::ScalarImplicit\",\n    \"aten::Bool.Tensor\",\n    \"aten::Bool.int\",\n    \"aten::Bool.float\",\n    \"aten::Int.Tensor\",\n    \"aten::Int.Scalar\",\n    \"aten::Int.int\",\n    \"aten::Int.bool\",\n    \"aten::Int.str\",\n    \"aten::Float.Tensor\",\n    \"aten::Float.Scalar\",\n    \"aten::Float.int\",\n    \"aten::Float.bool\",\n    \"aten::Float.str\",\n    \"aten::format\",\n    \"prim::NumToTensor.Scalar\",\n    \"prim::RaiseException\",\n    \"aten::Size\",\n    \"aten::size\",\n    \"prim::EnumName\",\n    \"prim::EnumValue.int\",\n    \"prim::EnumValue.float\",\n    \"prim::EnumValue.str\",\n    \"prim::TupleIndex\",\n    \"aten::ne.int_list\",\n    \"prim::unchecked_unwrap_optional\",\n    \"prim::device\",\n    \"prim::dtype\",\n    \"aten::__not__\",\n    \"aten::__is__\",\n    \"aten::__isnot__\",\n    \"aten::element_size\",\n    \"aten::numel\",\n    \"aten::dim\",\n    \"aten::get_device\",\n    \"aten::storage_offset\",\n    \"aten::is_contiguous\",\n    \"aten::select.t\",\n    \"aten::__getitem__.t\",\n    \"aten::append.t\",\n    \"aten::reverse.t\",\n    \"aten::extend.t\",\n    \"aten::copy.t\",\n    \"aten::_set_item.t\",\n    \"aten::clear.t\",\n    \"aten::Delete.t\",\n    \"aten::insert.t\",\n    \"aten::pop.t\",\n    \"aten::add.t\",\n    \"aten::add_.t\",\n    \"aten::slice.t\",\n    \"aten::list.t\",\n    \"aten::mul.left_t\",\n    \"aten::mul.right_\",\n    \"aten::mul_.t\",\n    \"aten::len.t\",\n    \"aten::eq.int_list\",\n    \"prim::Uninitialized\",\n    \"prim::Print\",\n    \"aten::eq.enum\",\n    \"aten::ne.enum\",\n    \"aten::dequantize.tensor\",\n    \"aten::dequantize.any\",\n    \"aten::add.str\",\n    \"aten::eq.int\",\n    \"aten::eq.float\",\n    \"aten::eq.int_float\",\n    \"aten::eq.float_int\",\n    \"aten::eq\",\n    \"aten::eq.str\",\n    \"aten::ne.int\",\n    \"aten::ne.float\",\n    \"aten::ne.int_float\",\n    \"aten::ne.float_int\",\n    \"aten::ne\",\n    \"aten::ne.str\",\n    \"aten::lt.int\",\n    \"aten::lt.float\",\n    \"aten::lt.int_float\",\n    \"aten::lt.float_int\",\n    \"aten::lt\",\n    \"aten::lt.str\",\n    \"aten::gt.int\",\n    \"aten::gt.float\",\n    \"aten::gt.int_float\",\n    \"aten::gt.float_int\",\n    \"aten::gt\",\n    \"aten::gt.str\",\n    \"aten::le.int\",\n    \"aten::le.float\",\n    \"aten::le.int_float\",\n    \"aten::le.float_int\",\n    \"aten::le\",\n    \"aten::le.str\",\n    \"aten::ge.int\",\n    \"aten::ge.float\",\n    \"aten::ge.int_float\",\n    \"aten::ge.float_int\",\n    \"aten::ge\",\n    \"aten::ge.str\",\n    \"aten::add.int\",\n    \"aten::add.float\",\n    \"aten::add.int_float\",\n    \"aten::add.float_int\",\n    \"aten::add\",\n    \"aten::sub.int\",\n    \"aten::sub.float\",\n    \"aten::sub.int_float\",\n    \"aten::sub.float_int\",\n    \"aten::sub\",\n    \"aten::mul.int\",\n    \"aten::mul.float\",\n    \"aten::mul.int_float\",\n    \"aten::mul.float_int\",\n    \"aten::mul\",\n    \"aten::__and__.bool\",\n    \"aten::__or__.bool\",\n    \"aten::__xor__.bool\",\n    \"aten::floor.int\",\n    \"aten::floor.float\",\n    \"aten::floor.Scalar\",\n    \"aten::ceil.int\",\n    \"aten::ceil.float\",\n    \"aten::ceil.Scalar\",\n    \"aten::neg.int\",\n    \"aten::neg.float\",\n    \"aten::neg.Scalar\",\n    \"aten::exp.int\",\n    \"aten::exp.float\",\n    \"aten::exp.Scalar\",\n    \"aten::remainder.int\",\n    \"aten::remainder.float\",\n    \"aten::remainder.int_float\",\n    \"aten::remainder.float_int\",\n    \"aten::remainder\",\n    \"aten::div.int\",\n    \"aten::div.float\",\n    \"aten::div\",\n    \"aten::floordiv.int\",\n    \"aten::floordiv.float\",\n    \"aten::floordiv.int_float\",\n    \"aten::floordiv.float_int\",\n    \"aten::floordiv\",\n    \"aten::pow.int\",\n    \"aten::pow.float\",\n    \"aten::pow.int_float\",\n    \"aten::pow.float_int\",\n    \"aten::pow.Scalar_Scalar\",\n    \"aten::pow.int_to_int\",\n    \"prim::min.int\",\n    \"prim::min.float\",\n    \"prim::min.int_float\",\n    \"prim::min.float_int\",\n    \"prim::min\",\n    \"prim::max.int\",\n    \"prim::max.float\",\n    \"prim::max.int_float\",\n    \"prim::max.float_int\",\n    \"prim::max\",\n    \"prim::type\",\n    \"aten::len.Tensor\",\n    \"aten::ord\",\n    \"aten::lower\",\n    \"aten::__contains__.str_list\",\n    \"aten::len.str\",\n    \"aten::__getitem__.str\",\n    \"aten::copy_.Tensor\",\n    \"aten::copy_.int\",\n    \"aten::copy_.float\",\n    \"aten::backward\",\n    \"aten::index.Tensor_hacked_twin\",\n    \"aten::_unsafe_index.Tensor_hacked_twin\",\n    \"aten::_index_put_impl_.hacked_twin\",\n    \"aten::index_put_.hacked_twin\",\n    \"aten::index_put.hacked_twin\",\n    \"aten::_unsafe_index_put.hacked_twin\",\n    \"aten::to.prim_Device\",\n    \"aten::to.prim_dtype\",\n    \"prim::is_cuda\",\n    \"prim::data\",\n    \"prim::min.int_list\",\n    \"prim::max.int_list\",\n    \"prim::min.self_int\",\n    \"prim::max.self_int\",\n    \"prim::min.float_list\",\n    \"prim::max.float_list\",\n    \"prim::min.self_float\",\n    \"prim::max.self_float\",\n    \"prim::min.bool_list\",\n    \"prim::max.bool_list\",\n    \"prim::min.self_bool\",\n    \"prim::max.self_bool\",\n    \"aten::len.Dict_str\",\n    \"aten::keys.str\",\n    \"aten::values.str\",\n    \"aten::__getitem__.Dict_str\",\n    \"aten::get.str\",\n    \"aten::get.default_str\",\n    \"aten::setdefault.str\",\n    \"aten::Delete.Dict_str\",\n    \"aten::pop.Dict_str\",\n    \"aten::pop.Dict_default_str\",\n    \"aten::popitem.str\",\n    \"aten::clear.str\",\n    \"aten::update.str\",\n    \"aten::items.str\",\n    \"aten::copy.Dict_str\",\n    \"aten::__contains__.str\",\n    \"aten::_set_item.str\",\n    \"aten::dict.str\",\n    \"aten::len.Dict_int\",\n    \"aten::keys.int\",\n    \"aten::values.int\",\n    \"aten::__getitem__.Dict_int\",\n    \"aten::get.int\",\n    \"aten::get.default_int\",\n    \"aten::setdefault.int\",\n    \"aten::Delete.Dict_int\",\n    \"aten::pop.Dict_int\",\n    \"aten::pop.Dict_default_int\",\n    \"aten::popitem.int\",\n    \"aten::clear.int\",\n    \"aten::update.int\",\n    \"aten::items.int\",\n    \"aten::copy.Dict_int\",\n    \"aten::__contains__.int\",\n    \"aten::_set_item.int\",\n    \"aten::dict.int\",\n    \"aten::len.Dict_bool\",\n    \"aten::keys.bool\",\n    \"aten::values.bool\",\n    \"aten::__getitem__.Dict_bool\",\n    \"aten::get.bool\",\n    \"aten::get.default_bool\",\n    \"aten::setdefault.bool\",\n    \"aten::Delete.Dict_bool\",\n    \"aten::pop.Dict_bool\",\n    \"aten::pop.Dict_default_bool\",\n    \"aten::popitem.bool\",\n    \"aten::clear.bool\",\n    \"aten::update.bool\",\n    \"aten::items.bool\",\n    \"aten::copy.Dict_bool\",\n    \"aten::__contains__.bool\",\n    \"aten::_set_item.bool\",\n    \"aten::dict.bool\",\n    \"aten::len.Dict_float\",\n    \"aten::keys.float\",\n    \"aten::values.float\",\n    \"aten::__getitem__.Dict_float\",\n    \"aten::get.float\",\n    \"aten::get.default_float\",\n    \"aten::setdefault.float\",\n    \"aten::Delete.Dict_float\",\n    \"aten::pop.Dict_float\",\n    \"aten::pop.Dict_default_float\",\n    \"aten::popitem.float\",\n    \"aten::clear.float\",\n    \"aten::update.float\",\n    \"aten::items.float\",\n    \"aten::copy.Dict_float\",\n    \"aten::__contains__.float\",\n    \"aten::_set_item.float\",\n    \"aten::dict.float\",\n    \"aten::len.Dict_Tensor\",\n    \"aten::keys.Tensor\",\n    \"aten::values.Tensor\",\n    \"aten::__getitem__.Dict_Tensor\",\n    \"aten::get.Tensor\",\n    \"aten::get.default_Tensor\",\n    \"aten::setdefault.Tensor\",\n    \"aten::Delete.Dict_Tensor\",\n    \"aten::pop.Dict_Tensor\",\n    \"aten::pop.Dict_default_Tensor\",\n    \"aten::popitem.Tensor\",\n    \"aten::clear.Tensor\",\n    \"aten::update.Tensor\",\n    \"aten::items.Tensor\",\n    \"aten::copy.Dict_Tensor\",\n    \"aten::__contains__.Tensor\",\n    \"aten::_set_item.Tensor\",\n    \"aten::dict.Tensor\",\n    \"aten::__round_to_zero_floordiv.int\",\n    \"aten::mathremainder.int\",\n    \"aten::mathremainder.float\",\n    \"aten::mathremainder.int_float\",\n    \"aten::mathremainder.float_int\",\n    \"aten::mathremainder\",\n    \"aten::__and__.int\",\n    \"aten::__or__.int\",\n    \"aten::__xor__.int\",\n    \"aten::__lshift__.int\",\n    \"aten::__rshift__.int\",\n    \"aten::round.int\",\n    \"aten::round.float\",\n    \"aten::round.Scalar\",\n    \"aten::log.int\",\n    \"aten::log.float\",\n    \"aten::log.Scalar\",\n    \"aten::log.int_int\",\n    \"aten::log.float_float\",\n    \"aten::log.int_float\",\n    \"aten::log.float_int\",\n    \"aten::log.Scalar_Scalar\",\n    \"aten::log1p.int\",\n    \"aten::log1p.float\",\n    \"aten::log1p.Scalar\",\n    \"aten::log10.int\",\n    \"aten::log10.float\",\n    \"aten::log10.Scalar\",\n    \"aten::sqrt.int\",\n    \"aten::sqrt.float\",\n    \"aten::sqrt.Scalar\",\n    \"aten::acos.int\",\n    \"aten::acos.float\",\n    \"aten::acos.Scalar\",\n    \"aten::asin.int\",\n    \"aten::asin.float\",\n    \"aten::asin.Scalar\",\n    \"aten::atan.int\",\n    \"aten::atan.float\",\n    \"aten::atan.Scalar\",\n    \"aten::atan2.int\",\n    \"aten::atan2.float\",\n    \"aten::atan2.int_float\",\n    \"aten::atan2.float_int\",\n    \"aten::atan2.Scalar_Scalar\",\n    \"aten::cos.int\",\n    \"aten::cos.float\",\n    \"aten::cos.Scalar\",\n    \"aten::sin.int\",\n    \"aten::sin.float\",\n    \"aten::sin.Scalar\",\n    \"aten::tan.int\",\n    \"aten::tan.float\",\n    \"aten::tan.Scalar\",\n    \"aten::asinh.int\",\n    \"aten::asinh.float\",\n    \"aten::asinh.Scalar\",\n    \"aten::atanh.int\",\n    \"aten::atanh.float\",\n    \"aten::atanh.Scalar\",\n    \"aten::acosh.int\",\n    \"aten::acosh.float\",\n    \"aten::acosh.Scalar\",\n    \"aten::sinh.int\",\n    \"aten::sinh.float\",\n    \"aten::sinh.Scalar\",\n    \"aten::cosh.int\",\n    \"aten::cosh.float\",\n    \"aten::cosh.Scalar\",\n    \"aten::tanh.int\",\n    \"aten::tanh.float\",\n    \"aten::tanh.Scalar\",\n    \"aten::degrees.int\",\n    \"aten::degrees.float\",\n    \"aten::degrees.Scalar\",\n    \"aten::radians.int\",\n    \"aten::radians.float\",\n    \"aten::radians.Scalar\",\n    \"aten::fmod.int\",\n    \"aten::fmod.float\",\n    \"aten::fmod.int_float\",\n    \"aten::fmod.float_int\",\n    \"aten::fmod\",\n    \"aten::factorial.int\",\n    \"aten::isnan.float\",\n    \"aten::isfinite.float\",\n    \"aten::isinf.float\",\n    \"aten::gamma.int\",\n    \"aten::gamma.float\",\n    \"aten::gamma.Scalar\",\n    \"aten::erf.int\",\n    \"aten::erf.float\",\n    \"aten::erf.Scalar\",\n    \"aten::erfc.int\",\n    \"aten::erfc.float\",\n    \"aten::erfc.Scalar\",\n    \"aten::expm1.int\",\n    \"aten::expm1.float\",\n    \"aten::expm1.Scalar\",\n    \"aten::fabs.int\",\n    \"aten::fabs.float\",\n    \"aten::fabs.Scalar\",\n    \"aten::lgamma.int\",\n    \"aten::lgamma.float\",\n    \"aten::lgamma.Scalar\",\n    \"prim::abs.int\",\n    \"prim::abs.float\",\n    \"prim::abs.Scalar\",\n    \"aten::gcd.int\",\n    \"aten::copysign.int\",\n    \"aten::copysign.float\",\n    \"aten::copysign.int_float\",\n    \"aten::copysign.float_int\",\n    \"aten::copysign\",\n    \"aten::split\",\n    \"aten::tensor.float\",\n    \"aten::as_tensor.float\",\n    \"aten::tensor.int\",\n    \"aten::as_tensor.int\",\n    \"aten::tensor.bool\",\n    \"aten::as_tensor.bool\",\n    \"aten::_infer_size\",\n    \"aten::_no_grad_embedding_renorm_\",\n    \"aten::tensor\",\n    \"aten::as_tensor\",\n    \"aten::as_tensor.list\",\n    \"aten::_pack_sequence\",\n    \"aten::_get_tracing_state\",\n    \"aten::is_scripting\",\n    \"aten::_no_grad_uniform_\",\n    \"aten::_no_grad_normal_\",\n    \"aten::_no_grad_fill_\",\n    \"aten::_no_grad_zero_\",\n]\n\nPT_BASE_OPS = [\n    \"aten::_coalesced_\",\n    \"aten::_copy_from\",\n    \"aten::_empty_affine_quantized\",\n    \"aten::_empty_per_channel_affine_quantized\",\n    \"aten::_indices\",\n    \"aten::_nnz\",\n    \"aten::_values\",\n    \"aten::add\",\n    \"aten::add_\",\n    \"aten::arange\",\n    \"aten::as_strided\",\n    \"aten::as_strided_\",\n    \"aten::cat\",\n    \"aten::clone\",\n    \"aten::coalesce\",\n    \"aten::contiguous\",\n    \"aten::copy_\",\n    \"aten::copy_sparse_to_sparse_\",\n    \"aten::dense_dim\",\n    \"aten::dequantize\",\n    \"aten::div\",\n    \"aten::div_\",\n    \"aten::empty\",\n    \"aten::empty_like\",\n    \"aten::empty_strided\",\n    \"aten::eq\",\n    \"aten::equal\",\n    \"aten::expand\",\n    \"aten::fill_\",\n    \"aten::is_coalesced\",\n    \"aten::is_complex\",\n    \"aten::is_floating_point\",\n    \"aten::is_leaf\",\n    \"aten::is_nonzero\",\n    \"aten::item\",\n    \"aten::max\",\n    \"aten::min\",\n    \"aten::mul\",\n    \"aten::mul_\",\n    \"aten::narrow\",\n    \"aten::ne\",\n    \"aten::permute\",\n    \"aten::q_per_channel_axis\",\n    \"aten::q_per_channel_scales\",\n    \"aten::q_per_channel_zero_points\",\n    \"aten::q_scale\",\n    \"aten::q_zero_point\",\n    \"aten::qscheme\",\n    \"aten::quantize_per_tensor\",\n    \"aten::reshape\",\n    \"aten::_reshape_alias\",\n    \"aten::resize_\",\n    \"aten::resize_as_\",\n    \"aten::scalar_tensor\",\n    \"aten::select\",\n    \"aten::set_\",\n    \"aten::size\",\n    \"aten::slice\",\n    \"aten::sparse_dim\",\n    \"aten::sparse_resize_and_clear_\",\n    \"aten::squeeze\",\n    \"aten::squeeze_\",\n    \"aten::stride\",\n    \"aten::sub\",\n    \"aten::sub_\",\n    \"aten::sum\",\n    \"aten::t\",\n    \"aten::to\",\n    \"aten::_to_copy\",\n    \"aten::unsqueeze\",\n    \"aten::view\",\n    \"aten::zero_\",\n    \"aten::zeros\",\n    \"aten::zeros_like\",\n]\n"
        },
        {
          "name": "pt_template_srcs.bzl",
          "type": "blob",
          "size": 11.13,
          "content": "# This file keeps a list of PyTorch source files that are used for templated selective build.\n# NB: as this is PyTorch Edge selective build, we assume only CPU targets are\n# being built\n\nload(\"@bazel_skylib//lib:paths.bzl\", \"paths\")\nload(\"//tools/build_defs:fbsource_utils.bzl\", \"is_arvr_mode\")\nload(\":build_variables.bzl\", \"aten_native_source_list\")\nload(\n    \":ufunc_defs.bzl\",\n    \"aten_ufunc_generated_cpu_kernel_sources\",\n    \"aten_ufunc_generated_cpu_sources\",\n)\n\n# Files in this list are supposed to be built separately for each app,\n# for different operator allow lists.\nTEMPLATE_SOURCE_LIST = [\n    \"torch/csrc/jit/runtime/register_prim_ops.cpp\",\n    \"torch/csrc/jit/runtime/register_special_ops.cpp\",\n] + aten_native_source_list\n\n# For selective build, we can lump the CPU and CPU kernel sources altogether\n# because there is only ever one vectorization variant that is compiled\ndef aten_ufunc_generated_all_cpu_sources(gencode_pattern = \"{}\"):\n    return (\n        aten_ufunc_generated_cpu_sources(gencode_pattern) +\n        aten_ufunc_generated_cpu_kernel_sources(gencode_pattern)\n    )\n\nTEMPLATE_MASKRCNN_SOURCE_LIST = [\n    \"register_maskrcnn_ops.cpp\",\n]\n\nTEMPLATE_BATCH_BOX_COX_SOURCE_LIST = [\n    \"register_batch_box_cox_ops.cpp\",\n]\n\nMETAL_SOURCE_LIST = [\n    \"aten/src/ATen/native/metal/MetalAten.mm\",\n    \"aten/src/ATen/native/metal/MetalGuardImpl.cpp\",\n    \"aten/src/ATen/native/metal/MetalPrepackOpRegister.cpp\",\n    \"aten/src/ATen/native/metal/MetalCommandBuffer.mm\",\n    \"aten/src/ATen/native/metal/MetalContext.mm\",\n    \"aten/src/ATen/native/metal/MetalConvParams.mm\",\n    \"aten/src/ATen/native/metal/MetalTensorImplStorage.mm\",\n    \"aten/src/ATen/native/metal/MetalTensorUtils.mm\",\n    \"aten/src/ATen/native/metal/mpscnn/MPSCNNClampOp.mm\",\n    \"aten/src/ATen/native/metal/mpscnn/MPSCNNConvOp.mm\",\n    \"aten/src/ATen/native/metal/mpscnn/MPSCNNFullyConnectedOp.mm\",\n    \"aten/src/ATen/native/metal/mpscnn/MPSCNNNeuronOp.mm\",\n    \"aten/src/ATen/native/metal/mpscnn/MPSCNNUtils.mm\",\n    \"aten/src/ATen/native/metal/mpscnn/MPSImage+Tensor.mm\",\n    \"aten/src/ATen/native/metal/mpscnn/MPSImageUtils.mm\",\n    \"aten/src/ATen/native/metal/mpscnn/MPSImageWrapper.mm\",\n    \"aten/src/ATen/native/metal/ops/MetalAddmm.mm\",\n    \"aten/src/ATen/native/metal/ops/MetalBinaryElementwise.mm\",\n    \"aten/src/ATen/native/metal/ops/MetalChunk.mm\",\n    \"aten/src/ATen/native/metal/ops/MetalClamp.mm\",\n    \"aten/src/ATen/native/metal/ops/MetalConcat.mm\",\n    \"aten/src/ATen/native/metal/ops/MetalConvolution.mm\",\n    \"aten/src/ATen/native/metal/ops/MetalCopy.mm\",\n    \"aten/src/ATen/native/metal/ops/MetalHardswish.mm\",\n    \"aten/src/ATen/native/metal/ops/MetalHardshrink.mm\",\n    \"aten/src/ATen/native/metal/ops/MetalLeakyReLU.mm\",\n    \"aten/src/ATen/native/metal/ops/MetalNeurons.mm\",\n    \"aten/src/ATen/native/metal/ops/MetalPadding.mm\",\n    \"aten/src/ATen/native/metal/ops/MetalPooling.mm\",\n    \"aten/src/ATen/native/metal/ops/MetalReduce.mm\",\n    \"aten/src/ATen/native/metal/ops/MetalReshape.mm\",\n    \"aten/src/ATen/native/metal/ops/MetalSoftmax.mm\",\n    \"aten/src/ATen/native/metal/ops/MetalTranspose.mm\",\n    \"aten/src/ATen/native/metal/ops/MetalUpsamplingNearest.mm\",\n]\n\nUNET_METAL_PREPACK_SOURCE_LIST = [\n    \"unet_metal_prepack.cpp\",\n    \"unet_metal_prepack.mm\",\n]\n\nMETAL_MASKRCNN_SOURCE_LIST = [\n    \"maskrcnn/srcs/GenerateProposals.mm\",\n    \"maskrcnn/srcs/RoIAlign.mm\",\n]\n\n# The get_template_source_dict() returns a dict containing a path prefix\n# and a list of .cpp source files containing operator definitions and\n# registrations that should get selected via templated selective build.\n# The file selected_mobile_ops.h has the list of selected top level\n# operators.\n# NB: doesn't include generated files; copy_template_registration_files\n# handles those specially\ndef get_template_source_dict():\n    ret = {}\n    for file_path in TEMPLATE_SOURCE_LIST:\n        path_prefix = paths.dirname(file_path)\n        if path_prefix not in ret:\n            ret[path_prefix] = []\n        ret[path_prefix].append(file_path)\n    return ret\n\ndef get_gen_oplist_outs():\n    return {\n        \"SupportedMobileModelsRegistration.cpp\": [\n            \"SupportedMobileModelsRegistration.cpp\",\n        ],\n        \"selected_mobile_ops.h\": [\n            \"selected_mobile_ops.h\",\n        ],\n        \"selected_operators.yaml\": [\n            \"selected_operators.yaml\",\n        ],\n    }\n\ndef get_generate_code_bin_outs():\n    outs = {\n        \"autograd/generated/ADInplaceOrViewTypeEverything.cpp\": [\"autograd/generated/ADInplaceOrViewTypeEverything.cpp\"],\n        \"autograd/generated/ADInplaceOrViewType_0.cpp\": [\"autograd/generated/ADInplaceOrViewType_0.cpp\"],\n        \"autograd/generated/ADInplaceOrViewType_1.cpp\": [\"autograd/generated/ADInplaceOrViewType_1.cpp\"],\n        \"autograd/generated/Functions.cpp\": [\"autograd/generated/Functions.cpp\"],\n        \"autograd/generated/Functions.h\": [\"autograd/generated/Functions.h\"],\n        \"autograd/generated/TraceTypeEverything.cpp\": [\"autograd/generated/TraceTypeEverything.cpp\"],\n        \"autograd/generated/TraceType_0.cpp\": [\"autograd/generated/TraceType_0.cpp\"],\n        \"autograd/generated/TraceType_1.cpp\": [\"autograd/generated/TraceType_1.cpp\"],\n        \"autograd/generated/TraceType_2.cpp\": [\"autograd/generated/TraceType_2.cpp\"],\n        \"autograd/generated/TraceType_3.cpp\": [\"autograd/generated/TraceType_3.cpp\"],\n        \"autograd/generated/TraceType_4.cpp\": [\"autograd/generated/TraceType_4.cpp\"],\n        \"autograd/generated/VariableType.h\": [\"autograd/generated/VariableType.h\"],\n        \"autograd/generated/VariableTypeEverything.cpp\": [\"autograd/generated/VariableTypeEverything.cpp\"],\n        \"autograd/generated/VariableType_0.cpp\": [\"autograd/generated/VariableType_0.cpp\"],\n        \"autograd/generated/VariableType_1.cpp\": [\"autograd/generated/VariableType_1.cpp\"],\n        \"autograd/generated/VariableType_2.cpp\": [\"autograd/generated/VariableType_2.cpp\"],\n        \"autograd/generated/VariableType_3.cpp\": [\"autograd/generated/VariableType_3.cpp\"],\n        \"autograd/generated/VariableType_4.cpp\": [\"autograd/generated/VariableType_4.cpp\"],\n        \"autograd/generated/variable_factories.h\": [\"autograd/generated/variable_factories.h\"],\n        \"autograd/generated/ViewFuncs.cpp\": [\"autograd/generated/ViewFuncs.cpp\"],\n        \"autograd/generated/ViewFuncs.h\": [\"autograd/generated/ViewFuncs.h\"],\n    }\n\n    if is_arvr_mode():\n        outs.update({\n            \"autograd/generated/python_enum_tag.cpp\": [\"autograd/generated/python_enum_tag.cpp\"],\n            \"autograd/generated/python_fft_functions.cpp\": [\"autograd/generated/python_fft_functions.cpp\"],\n            \"autograd/generated/python_functions.h\": [\"autograd/generated/python_functions.h\"],\n            \"autograd/generated/python_functions_0.cpp\": [\"autograd/generated/python_functions_0.cpp\"],\n            \"autograd/generated/python_functions_1.cpp\": [\"autograd/generated/python_functions_1.cpp\"],\n            \"autograd/generated/python_functions_2.cpp\": [\"autograd/generated/python_functions_2.cpp\"],\n            \"autograd/generated/python_functions_3.cpp\": [\"autograd/generated/python_functions_3.cpp\"],\n            \"autograd/generated/python_functions_4.cpp\": [\"autograd/generated/python_functions_4.cpp\"],\n            \"autograd/generated/python_linalg_functions.cpp\": [\"autograd/generated/python_linalg_functions.cpp\"],\n            \"autograd/generated/python_nested_functions.cpp\": [\"autograd/generated/python_nested_functions.cpp\"],\n            \"autograd/generated/python_nn_functions.cpp\": [\"autograd/generated/python_nn_functions.cpp\"],\n            \"autograd/generated/python_return_types.h\": [\"autograd/generated/python_return_types.h\"],\n            \"autograd/generated/python_return_types.cpp\": [\"autograd/generated/python_return_types.cpp\"],\n            \"autograd/generated/python_sparse_functions.cpp\": [\"autograd/generated/python_sparse_functions.cpp\"],\n            \"autograd/generated/python_special_functions.cpp\": [\"autograd/generated/python_special_functions.cpp\"],\n            \"autograd/generated/python_torch_functions_0.cpp\": [\"autograd/generated/python_torch_functions_0.cpp\"],\n            \"autograd/generated/python_torch_functions_1.cpp\": [\"autograd/generated/python_torch_functions_1.cpp\"],\n            \"autograd/generated/python_torch_functions_2.cpp\": [\"autograd/generated/python_torch_functions_2.cpp\"],\n            \"autograd/generated/python_variable_methods.cpp\": [\"autograd/generated/python_variable_methods.cpp\"],\n        })\n    return outs\n\ndef get_template_registration_files_outs(is_oss = False):\n    outs = {}\n    if not is_oss:\n        for file_path in TEMPLATE_MASKRCNN_SOURCE_LIST:\n            outs[file_path] = [file_path]\n\n        for file_path in TEMPLATE_BATCH_BOX_COX_SOURCE_LIST:\n            outs[file_path] = [file_path]\n\n    for file_path in TEMPLATE_SOURCE_LIST:\n        outs[file_path] = [file_path]\n\n    for base_name in aten_ufunc_generated_all_cpu_sources():\n        file_path = \"aten/src/ATen/{}\".format(base_name)\n        outs[file_path] = [file_path]\n\n    return outs\n\ndef get_template_registration_file_rules(rule_name, is_oss = False):\n    rules = []\n    for file_path in TEMPLATE_SOURCE_LIST if is_oss else (TEMPLATE_SOURCE_LIST + TEMPLATE_MASKRCNN_SOURCE_LIST + TEMPLATE_BATCH_BOX_COX_SOURCE_LIST):\n        rules.append(\":{}[{}]\".format(rule_name, file_path))\n    for file_path in aten_ufunc_generated_all_cpu_sources():\n        rules.append(\":{}[aten/src/ATen/{}]\".format(rule_name, file_path))\n\n    return rules\n\n# ---------------------METAL RULES---------------------\ndef get_metal_source_dict():\n    ret = {}\n    for file_path in METAL_SOURCE_LIST:\n        path_prefix = paths.dirname(file_path)\n        if path_prefix not in ret:\n            ret[path_prefix] = []\n        ret[path_prefix].append(file_path)\n    return ret\n\ndef get_metal_registration_files_outs():\n    outs = {}\n    for file_path in METAL_SOURCE_LIST:\n        outs[file_path] = [file_path]\n\n    for file_path in UNET_METAL_PREPACK_SOURCE_LIST:\n        outs[file_path] = [file_path]\n\n    for file_path in METAL_MASKRCNN_SOURCE_LIST:\n        outs[file_path] = [file_path]\n    return outs\n\n# There is a really weird issue with the arvr windows builds where\n# the custom op files are breaking them. See https://fburl.com/za87443c\n# The hack is just to not build them for that platform and pray they arent needed.\ndef get_metal_registration_files_outs_windows():\n    outs = {}\n    for file_path in METAL_SOURCE_LIST:\n        outs[file_path] = [file_path]\n    return outs\n\ndef get_metal_registration_files_rules(rule_name):\n    ret = {}\n    objc_rules = []\n    cxx_rules = []\n\n    for file_path in METAL_SOURCE_LIST + METAL_MASKRCNN_SOURCE_LIST + UNET_METAL_PREPACK_SOURCE_LIST:\n        if \".cpp\" not in file_path:\n            objc_rules.append(\":{}[{}]\".format(rule_name, file_path))\n        else:\n            cxx_rules.append(\":{}[{}]\".format(rule_name, file_path))\n    ret[\"objc\"] = objc_rules\n    ret[\"cxx\"] = cxx_rules\n    return ret\n\ndef get_metal_registration_files_rules_windows(rule_name):\n    ret = {}\n    objc_rules = []\n    cxx_rules = []\n\n    for file_path in METAL_SOURCE_LIST:\n        if \".cpp\" not in file_path:\n            objc_rules.append(\":{}[{}]\".format(rule_name, file_path))\n        else:\n            cxx_rules.append(\":{}[{}]\".format(rule_name, file_path))\n    ret[\"objc\"] = objc_rules\n    ret[\"cxx\"] = cxx_rules\n    return ret\n"
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 5.13,
          "content": "[build-system]\nrequires = [\n    \"setuptools\",\n    \"wheel\",\n    \"astunparse\",\n    \"numpy\",\n    \"ninja\",\n    \"pyyaml\",\n    \"cmake\",\n    \"typing-extensions>=4.10.0\",\n    \"requests\",\n]\n# Use legacy backend to import local packages in setup.py\nbuild-backend = \"setuptools.build_meta:__legacy__\"\n\n\n[tool.black]\nline-length = 88\ntarget-version = [\"py38\"]\n\n\n[tool.isort]\nsrc_paths = [\"caffe2\", \"torch\", \"torchgen\", \"functorch\", \"test\"]\nextra_standard_library = [\"typing_extensions\"]\nskip_gitignore = true\nskip_glob = [\"third_party/*\"]\natomic = true\nprofile = \"black\"\nindent = 4\nline_length = 88\nlines_after_imports = 2\nmulti_line_output = 3\ninclude_trailing_comma = true\ncombine_as_imports = true\n\n\n[tool.usort.known]\nfirst_party = [\"caffe2\", \"torch\", \"torchgen\", \"functorch\", \"test\"]\nstandard_library = [\"typing_extensions\"]\n\n\n[tool.ruff]\ntarget-version = \"py38\"\nline-length = 88\nsrc = [\"caffe2\", \"torch\", \"torchgen\", \"functorch\", \"test\"]\n\n[tool.ruff.format]\ndocstring-code-format = true\nquote-style = \"double\"\n\n[tool.ruff.lint]\n# NOTE: Synchoronize the ignores with .flake8\nignore = [\n    # these ignores are from flake8-bugbear; please fix!\n    \"B007\", \"B008\", \"B017\",\n    \"B018\", # Useless expression\n    \"B023\",\n    \"B028\", # No explicit `stacklevel` keyword argument found\n    \"E402\",\n    \"C408\", # C408 ignored because we like the dict keyword argument syntax\n    \"E501\", # E501 is not flexible enough, we're using B950 instead\n    \"E721\",\n    \"E731\", # Assign lambda expression\n    \"E741\",\n    \"EXE001\",\n    \"F405\",\n    # these ignores are from flake8-logging-format; please fix!\n    \"G101\",\n    # these ignores are from ruff NPY; please fix!\n    \"NPY002\",\n    # these ignores are from ruff PERF; please fix!\n    \"PERF203\",\n    \"PERF401\",\n    \"PERF403\",\n    # these ignores are from PYI; please fix!\n    \"PYI024\",\n    \"PYI036\",\n    \"PYI041\",\n    \"PYI056\",\n    \"SIM102\", \"SIM103\", \"SIM112\", # flake8-simplify code styles\n    \"SIM113\", # please fix\n    \"SIM105\", # these ignores are from flake8-simplify. please fix or ignore with commented reason\n    \"SIM108\", # SIM108 ignored because we prefer if-else-block instead of ternary expression\n    \"SIM110\",\n    \"SIM114\", # Combine `if` branches using logical `or` operator\n    \"SIM115\",\n    \"SIM116\", # Disable Use a dictionary instead of consecutive `if` statements\n    \"SIM117\",\n    \"SIM118\",\n    \"UP006\", # keep-runtime-typing\n    \"UP007\", # keep-runtime-typing\n]\nselect = [\n    \"B\",\n    \"B904\", # Re-raised error without specifying the cause via the from keyword\n    \"C4\",\n    \"G\",\n    \"E\",\n    \"EXE\",\n    \"F\",\n    \"SIM1\",\n    \"SIM911\",\n    \"W\",\n    # Not included in flake8\n    \"FURB\",\n    \"LOG\",\n    \"NPY\",\n    \"PERF\",\n    \"PGH004\",\n    \"PIE790\",\n    \"PIE794\",\n    \"PIE800\",\n    \"PIE804\",\n    \"PIE807\",\n    \"PIE810\",\n    \"PLC0131\", # type bivariance\n    \"PLC0132\", # type param mismatch\n    \"PLC0205\", # string as __slots__\n    \"PLC3002\", # unnecessary-direct-lambda-call\n    \"PLE\",\n    \"PLR0133\", # constant comparison\n    \"PLR0206\", # property with params\n    \"PLR1722\", # use sys exit\n    \"PLR1736\", # unnecessary list index\n    \"PLW0129\", # assert on string literal\n    \"PLW0133\", # useless exception statement\n    \"PLW0406\", # import self\n    \"PLW0711\", # binary op exception\n    \"PLW1509\", # preexec_fn not safe with threads\n    \"PLW2101\", # useless lock statement\n    \"PLW3301\", # nested min max\n    \"PT006\", # TODO: enable more PT rules\n    \"PT022\",\n    \"PT023\",\n    \"PT024\",\n    \"PT025\",\n    \"PT026\",\n    \"PYI\",\n    \"Q003\",  # avoidable escaped quote\n    \"Q004\",  # unnecessary escaped quote\n    \"RSE\",\n    \"RUF008\", # mutable dataclass default\n    \"RUF013\", # ban implicit optional\n    \"RUF015\", # access first ele in constant time\n    \"RUF016\", # type error non-integer index\n    \"RUF017\",\n    \"RUF018\", # no assignment in assert\n    \"RUF019\", # unnecessary-key-check\n    \"RUF024\", # from keys mutable\n    \"RUF026\", # default factory kwarg\n    \"TCH\",\n    \"TRY002\", # ban vanilla raise (todo fix NOQAs)\n    \"TRY203\",\n    \"TRY401\", # verbose-log-message\n    \"UP\",\n]\n\n[tool.ruff.lint.per-file-ignores]\n\"__init__.py\" = [\n    \"F401\",\n]\n\"functorch/notebooks/**\" = [\n    \"F401\",\n]\n\"test/typing/reveal/**\" = [\n    \"F821\",\n]\n\"test/torch_np/numpy_tests/**\" = [\n    \"F821\",\n    \"NPY201\",\n]\n\"test/dynamo/test_bytecode_utils.py\" = [\n    \"F821\",\n]\n\"test/dynamo/test_debug_utils.py\" = [\n    \"UP037\",\n]\n\"test/jit/**\" = [\n    \"PLR0133\", # tests require this for JIT\n    \"PYI\",\n    \"RUF015\",\n    \"UP\", # We don't want to modify the jit test as they test specify syntax\n]\n\"test/test_jit.py\" = [\n    \"PLR0133\", # tests require this for JIT\n    \"PYI\",\n    \"RUF015\",\n    \"UP\", # We don't want to modify the jit test as they test specify syntax\n]\n\"test/inductor/test_torchinductor.py\" = [\n    \"UP037\",\n]\n# autogenerated #TODO figure out why file level noqa is ignored\n\"torch/_inductor/fx_passes/serialized_patterns/**\" = [\"F401\", \"F501\"]\n\"torch/_inductor/autoheuristic/artifacts/**\" = [\"F401\", \"F501\"]\n\"torchgen/api/types/__init__.py\" = [\n    \"F401\",\n    \"F403\",\n]\n\"torchgen/executorch/api/types/__init__.py\" = [\n    \"F401\",\n    \"F403\",\n]\n\"torch/utils/collect_env.py\" = [\n    \"UP\", # collect_env.py needs to work with older versions of Python\n]\n\"torch/_vendor/**\" = [\n    \"UP\", # No need to mess with _vendor\n]\n"
        },
        {
          "name": "pytest.ini",
          "type": "blob",
          "size": 0.82,
          "content": "[pytest]\naddopts =\n    # show summary of all tests that did not pass\n    -rEfX\n    # Make tracebacks shorter\n    --tb=native\n    # capture only Python print and C++ py::print, but not C output (low-level Python errors)\n    --capture=sys\n    # don't suppress warnings, but don't shove them all to the end either\n    -p no:warnings\n    # Use custom pytest shard located in test/pytest_shard_custom.py instead\n    -p no:pytest-shard\n    # don't rewrite assertions (usually not a problem in CI due to differences in imports, see #95844)\n    --assert=plain\ntestpaths =\n    test\njunit_logging_reruns = all\nfilterwarnings =\n    ignore:Module already imported so cannot be rewritten.*hypothesis:pytest.PytestAssertRewriteWarning\n\nxfail_strict = True\n\nmarkers =\n    serial: marks tests as needs to be run serially (deselect with '-m \"not serial\"')\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.29,
          "content": "# Python dependencies required for development\nastunparse\ncmake\nexpecttest>=0.3.0\nfilelock\nfsspec\nhypothesis\njinja2\nlintrunner ; platform_machine != \"s390x\"\nnetworkx\nninja\nnumpy\noptree>=0.13.0\npackaging\npsutil\npyyaml\nrequests\nsetuptools\nsympy==1.13.3\ntypes-dataclasses\ntyping-extensions>=4.10.0\n"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 51.15,
          "content": "# Welcome to the PyTorch setup.py.\n# Environment variables you are probably interested in:\n#\n#   DEBUG\n#     build with -O0 and -g (debug symbols)\n#\n#   REL_WITH_DEB_INFO\n#     build with optimizations and -g (debug symbols)\n#\n#   USE_CUSTOM_DEBINFO=\"path/to/file1.cpp;path/to/file2.cpp\"\n#     build with debug info only for specified files\n#\n#   MAX_JOBS\n#     maximum number of compile jobs we should use to compile your code\n#\n#   USE_CUDA=0\n#     disables CUDA build\n#\n#   CFLAGS\n#     flags to apply to both C and C++ files to be compiled (a quirk of setup.py\n#     which we have faithfully adhered to in our build system is that CFLAGS\n#     also applies to C++ files (unless CXXFLAGS is set), in contrast to the\n#     default behavior of autogoo and cmake build systems.)\n#\n#   CC\n#     the C/C++ compiler to use\n#\n# Environment variables for feature toggles:\n#\n#   DEBUG_CUDA=1\n#     if used in conjunction with DEBUG or REL_WITH_DEB_INFO, will also\n#     build CUDA kernels with -lineinfo --source-in-ptx.  Note that\n#     on CUDA 12 this may cause nvcc to OOM, so this is disabled by default.\n\n#   USE_CUDNN=0\n#     disables the cuDNN build\n#\n#   USE_CUSPARSELT=0\n#     disables the cuSPARSELt build\n#\n#   USE_CUDSS=0\n#     disables the cuDSS build\n#\n#   USE_CUFILE=0\n#     disables the cuFile build\n#\n#   USE_FBGEMM=0\n#     disables the FBGEMM build\n#\n#   USE_KINETO=0\n#     disables usage of libkineto library for profiling\n#\n#   USE_NUMPY=0\n#     disables the NumPy build\n#\n#   BUILD_TEST=0\n#     disables the test build\n#\n#   USE_MKLDNN=0\n#     disables use of MKLDNN\n#\n#   USE_MKLDNN_ACL\n#     enables use of Compute Library backend for MKLDNN on Arm;\n#     USE_MKLDNN must be explicitly enabled.\n#\n#   MKLDNN_CPU_RUNTIME\n#     MKL-DNN threading mode: TBB or OMP (default)\n#\n#   USE_STATIC_MKL\n#     Prefer to link with MKL statically - Unix only\n#   USE_ITT=0\n#     disable use of Intel(R) VTune Profiler's ITT functionality\n#\n#   USE_NNPACK=0\n#     disables NNPACK build\n#\n#   USE_DISTRIBUTED=0\n#     disables distributed (c10d, gloo, mpi, etc.) build\n#\n#   USE_TENSORPIPE=0\n#     disables distributed Tensorpipe backend build\n#\n#   USE_GLOO=0\n#     disables distributed gloo backend build\n#\n#   USE_MPI=0\n#     disables distributed MPI backend build\n#\n#   USE_SYSTEM_NCCL=0\n#     disables use of system-wide nccl (we will use our submoduled\n#     copy in third_party/nccl)\n#\n#   USE_OPENMP=0\n#     disables use of OpenMP for parallelization\n#\n#   USE_FLASH_ATTENTION=0\n#     disables building flash attention for scaled dot product attention\n#\n#   USE_MEM_EFF_ATTENTION=0\n#    disables building memory efficient attention for scaled dot product attention\n#\n#   BUILD_BINARY\n#     enables the additional binaries/ build\n#\n#   ATEN_AVX512_256=TRUE\n#     ATen AVX2 kernels can use 32 ymm registers, instead of the default 16.\n#     This option can be used if AVX512 doesn't perform well on a machine.\n#     The FBGEMM library also uses AVX512_256 kernels on Xeon D processors,\n#     but it also has some (optimized) assembly code.\n#\n#   PYTORCH_BUILD_VERSION\n#   PYTORCH_BUILD_NUMBER\n#     specify the version of PyTorch, rather than the hard-coded version\n#     in this file; used when we're building binaries for distribution\n#\n#   TORCH_CUDA_ARCH_LIST\n#     specify which CUDA architectures to build for.\n#     ie `TORCH_CUDA_ARCH_LIST=\"6.0;7.0\"`\n#     These are not CUDA versions, instead, they specify what\n#     classes of NVIDIA hardware we should generate PTX for.\n#\n#   TORCH_XPU_ARCH_LIST\n#     specify which XPU architectures to build for.\n#     ie `TORCH_XPU_ARCH_LIST=\"ats-m150,lnl-m\"`\n#\n#   PYTORCH_ROCM_ARCH\n#     specify which AMD GPU targets to build for.\n#     ie `PYTORCH_ROCM_ARCH=\"gfx900;gfx906\"`\n#\n#   ONNX_NAMESPACE\n#     specify a namespace for ONNX built here rather than the hard-coded\n#     one in this file; needed to build with other frameworks that share ONNX.\n#\n#   BLAS\n#     BLAS to be used by Caffe2. Can be MKL, Eigen, ATLAS, FlexiBLAS, or OpenBLAS. If set\n#     then the build will fail if the requested BLAS is not found, otherwise\n#     the BLAS will be chosen based on what is found on your system.\n#\n#   MKL_THREADING\n#     MKL threading mode: SEQ, TBB or OMP (default)\n#\n#   USE_ROCM_KERNEL_ASSERT=1\n#     Enable kernel assert in ROCm platform\n#\n# Environment variables we respect (these environment variables are\n# conventional and are often understood/set by other software.)\n#\n#   CUDA_HOME (Linux/OS X)\n#   CUDA_PATH (Windows)\n#     specify where CUDA is installed; usually /usr/local/cuda or\n#     /usr/local/cuda-x.y\n#   CUDAHOSTCXX\n#     specify a different compiler than the system one to use as the CUDA\n#     host compiler for nvcc.\n#\n#   CUDA_NVCC_EXECUTABLE\n#     Specify a NVCC to use. This is used in our CI to point to a cached nvcc\n#\n#   CUDNN_LIB_DIR\n#   CUDNN_INCLUDE_DIR\n#   CUDNN_LIBRARY\n#     specify where cuDNN is installed\n#\n#   MIOPEN_LIB_DIR\n#   MIOPEN_INCLUDE_DIR\n#   MIOPEN_LIBRARY\n#     specify where MIOpen is installed\n#\n#   NCCL_ROOT\n#   NCCL_LIB_DIR\n#   NCCL_INCLUDE_DIR\n#     specify where nccl is installed\n#\n#   ACL_ROOT_DIR\n#     specify where Compute Library is installed\n#\n#   LIBRARY_PATH\n#   LD_LIBRARY_PATH\n#     we will search for libraries in these paths\n#\n#   ATEN_THREADING\n#     ATen parallel backend to use for intra- and inter-op parallelism\n#     possible values:\n#       OMP - use OpenMP for intra-op and native backend for inter-op tasks\n#       NATIVE - use native thread pool for both intra- and inter-op tasks\n#\n#   USE_SYSTEM_LIBS (work in progress)\n#      Use system-provided libraries to satisfy the build dependencies.\n#      When turned on, the following cmake variables will be toggled as well:\n#        USE_SYSTEM_CPUINFO=ON\n#        USE_SYSTEM_SLEEF=ON\n#        USE_SYSTEM_GLOO=ON\n#        BUILD_CUSTOM_PROTOBUF=OFF\n#        USE_SYSTEM_EIGEN_INSTALL=ON\n#        USE_SYSTEM_FP16=ON\n#        USE_SYSTEM_PTHREADPOOL=ON\n#        USE_SYSTEM_PSIMD=ON\n#        USE_SYSTEM_FXDIV=ON\n#        USE_SYSTEM_BENCHMARK=ON\n#        USE_SYSTEM_ONNX=ON\n#        USE_SYSTEM_XNNPACK=ON\n#        USE_SYSTEM_PYBIND11=ON\n#        USE_SYSTEM_NCCL=ON\n#        USE_SYSTEM_NVTX=ON\n#\n#   USE_MIMALLOC\n#      Static link mimalloc into C10, and use mimalloc in alloc_cpu & alloc_free.\n#      By default, It is only enabled on Windows.\n#\n#   USE_PRIORITIZED_TEXT_FOR_LD\n#      Uses prioritized text form cmake/prioritized_text.txt for LD\n#\n#   BUILD_LIBTORCH_WHL\n#      Builds libtorch.so and its dependencies as a wheel\n#\n#   BUILD_PYTHON_ONLY\n#      Builds pytorch as a wheel using libtorch.so from a seperate wheel\n\nimport os\nimport sys\n\n\nif sys.platform == \"win32\" and sys.maxsize.bit_length() == 31:\n    print(\n        \"32-bit Windows Python runtime is not supported. Please switch to 64-bit Python.\"\n    )\n    sys.exit(-1)\n\nimport platform\n\n\nBUILD_LIBTORCH_WHL = os.getenv(\"BUILD_LIBTORCH_WHL\", \"0\") == \"1\"\nBUILD_PYTHON_ONLY = os.getenv(\"BUILD_PYTHON_ONLY\", \"0\") == \"1\"\n\npython_min_version = (3, 9, 0)\npython_min_version_str = \".\".join(map(str, python_min_version))\nif sys.version_info < python_min_version:\n    print(\n        f\"You are using Python {platform.python_version()}. Python >={python_min_version_str} is required.\"\n    )\n    sys.exit(-1)\n\nimport filecmp\nimport glob\nimport importlib\nimport importlib.util\nimport json\nimport shutil\nimport subprocess\nimport sysconfig\nimport time\nfrom collections import defaultdict\n\nimport setuptools.command.build_ext\nimport setuptools.command.install\nimport setuptools.command.sdist\nfrom setuptools import Extension, find_packages, setup\nfrom setuptools.dist import Distribution\nfrom tools.build_pytorch_libs import build_pytorch\nfrom tools.generate_torch_version import get_torch_version\nfrom tools.setup_helpers.cmake import CMake\nfrom tools.setup_helpers.env import build_type, IS_DARWIN, IS_LINUX, IS_WINDOWS\nfrom tools.setup_helpers.generate_linker_script import gen_linker_script\n\n\ndef _get_package_path(package_name):\n    spec = importlib.util.find_spec(package_name)\n    if spec:\n        # The package might be a namespace package, so get_data may fail\n        try:\n            loader = spec.loader\n            if loader is not None:\n                file_path = loader.get_filename()  # type: ignore[attr-defined]\n                return os.path.dirname(file_path)\n        except AttributeError:\n            pass\n    return None\n\n\n# set up appropriate env variables\nif BUILD_LIBTORCH_WHL:\n    # Set up environment variables for ONLY building libtorch.so and not libtorch_python.so\n    # functorch is not supported without python\n    os.environ[\"BUILD_FUNCTORCH\"] = \"OFF\"\n\n\nif BUILD_PYTHON_ONLY:\n    os.environ[\"BUILD_LIBTORCHLESS\"] = \"ON\"\n    os.environ[\"LIBTORCH_LIB_PATH\"] = f\"{_get_package_path('torch')}/lib\"\n\n################################################################################\n# Parameters parsed from environment\n################################################################################\n\nVERBOSE_SCRIPT = True\nRUN_BUILD_DEPS = True\n# see if the user passed a quiet flag to setup.py arguments and respect\n# that in our parts of the build\nEMIT_BUILD_WARNING = False\nRERUN_CMAKE = False\nCMAKE_ONLY = False\nfiltered_args = []\nfor i, arg in enumerate(sys.argv):\n    if arg == \"--cmake\":\n        RERUN_CMAKE = True\n        continue\n    if arg == \"--cmake-only\":\n        # Stop once cmake terminates. Leave users a chance to adjust build\n        # options.\n        CMAKE_ONLY = True\n        continue\n    if arg == \"rebuild\" or arg == \"build\":\n        arg = \"build\"  # rebuild is gone, make it build\n        EMIT_BUILD_WARNING = True\n    if arg == \"--\":\n        filtered_args += sys.argv[i:]\n        break\n    if arg == \"-q\" or arg == \"--quiet\":\n        VERBOSE_SCRIPT = False\n    if arg in [\"clean\", \"egg_info\", \"sdist\"]:\n        RUN_BUILD_DEPS = False\n    filtered_args.append(arg)\nsys.argv = filtered_args\n\nif VERBOSE_SCRIPT:\n\n    def report(*args):\n        print(*args)\n\nelse:\n\n    def report(*args):\n        pass\n\n    # Make distutils respect --quiet too\n    setuptools.distutils.log.warn = report\n\n# Constant known variables used throughout this file\ncwd = os.path.dirname(os.path.abspath(__file__))\nlib_path = os.path.join(cwd, \"torch\", \"lib\")\nthird_party_path = os.path.join(cwd, \"third_party\")\n\n# CMAKE: full path to python library\nif IS_WINDOWS:\n    cmake_python_library = \"{}/libs/python{}.lib\".format(\n        sysconfig.get_config_var(\"prefix\"), sysconfig.get_config_var(\"VERSION\")\n    )\n    # Fix virtualenv builds\n    if not os.path.exists(cmake_python_library):\n        cmake_python_library = \"{}/libs/python{}.lib\".format(\n            sys.base_prefix, sysconfig.get_config_var(\"VERSION\")\n        )\nelse:\n    cmake_python_library = \"{}/{}\".format(\n        sysconfig.get_config_var(\"LIBDIR\"), sysconfig.get_config_var(\"INSTSONAME\")\n    )\ncmake_python_include_dir = sysconfig.get_path(\"include\")\n\n\n################################################################################\n# Version, create_version_file, and package_name\n################################################################################\n\npackage_name = os.getenv(\"TORCH_PACKAGE_NAME\", \"torch\")\nLIBTORCH_PKG_NAME = os.getenv(\"LIBTORCH_PACKAGE_NAME\", \"torch_no_python\")\nif BUILD_LIBTORCH_WHL:\n    package_name = LIBTORCH_PKG_NAME\n\n\npackage_type = os.getenv(\"PACKAGE_TYPE\", \"wheel\")\nversion = get_torch_version()\nreport(f\"Building wheel {package_name}-{version}\")\n\ncmake = CMake()\n\n\ndef get_submodule_folders():\n    git_modules_path = os.path.join(cwd, \".gitmodules\")\n    default_modules_path = [\n        os.path.join(third_party_path, name)\n        for name in [\n            \"gloo\",\n            \"cpuinfo\",\n            \"onnx\",\n            \"fbgemm\",\n            \"cutlass\",\n        ]\n    ]\n    if not os.path.exists(git_modules_path):\n        return default_modules_path\n    with open(git_modules_path) as f:\n        return [\n            os.path.join(cwd, line.split(\"=\", 1)[1].strip())\n            for line in f\n            if line.strip().startswith(\"path\")\n        ]\n\n\ndef check_submodules():\n    def check_for_files(folder, files):\n        if not any(os.path.exists(os.path.join(folder, f)) for f in files):\n            report(\"Could not find any of {} in {}\".format(\", \".join(files), folder))\n            report(\"Did you run 'git submodule update --init --recursive'?\")\n            sys.exit(1)\n\n    def not_exists_or_empty(folder):\n        return not os.path.exists(folder) or (\n            os.path.isdir(folder) and len(os.listdir(folder)) == 0\n        )\n\n    if bool(os.getenv(\"USE_SYSTEM_LIBS\", False)):\n        return\n    folders = get_submodule_folders()\n    # If none of the submodule folders exists, try to initialize them\n    if all(not_exists_or_empty(folder) for folder in folders):\n        try:\n            print(\" --- Trying to initialize submodules\")\n            start = time.time()\n            subprocess.check_call(\n                [\"git\", \"submodule\", \"update\", \"--init\", \"--recursive\"], cwd=cwd\n            )\n            end = time.time()\n            print(f\" --- Submodule initialization took {end - start:.2f} sec\")\n        except Exception:\n            print(\" --- Submodule initalization failed\")\n            print(\"Please run:\\n\\tgit submodule update --init --recursive\")\n            sys.exit(1)\n    for folder in folders:\n        check_for_files(\n            folder,\n            [\n                \"CMakeLists.txt\",\n                \"Makefile\",\n                \"setup.py\",\n                \"LICENSE\",\n                \"LICENSE.md\",\n                \"LICENSE.txt\",\n            ],\n        )\n    check_for_files(\n        os.path.join(third_party_path, \"fbgemm\", \"third_party\", \"asmjit\"),\n        [\"CMakeLists.txt\"],\n    )\n\n\n# Windows has very bad support for symbolic links.\n# Instead of using symlinks, we're going to copy files over\ndef mirror_files_into_torchgen():\n    # (new_path, orig_path)\n    # Directories are OK and are recursively mirrored.\n    paths = [\n        (\n            \"torchgen/packaged/ATen/native/native_functions.yaml\",\n            \"aten/src/ATen/native/native_functions.yaml\",\n        ),\n        (\"torchgen/packaged/ATen/native/tags.yaml\", \"aten/src/ATen/native/tags.yaml\"),\n        (\"torchgen/packaged/ATen/templates\", \"aten/src/ATen/templates\"),\n        (\"torchgen/packaged/autograd\", \"tools/autograd\"),\n        (\"torchgen/packaged/autograd/templates\", \"tools/autograd/templates\"),\n    ]\n    for new_path, orig_path in paths:\n        # Create the dirs involved in new_path if they don't exist\n        if not os.path.exists(new_path):\n            os.makedirs(os.path.dirname(new_path), exist_ok=True)\n\n        # Copy the files from the orig location to the new location\n        if os.path.isfile(orig_path):\n            shutil.copyfile(orig_path, new_path)\n            continue\n        if os.path.isdir(orig_path):\n            if os.path.exists(new_path):\n                # copytree fails if the tree exists already, so remove it.\n                shutil.rmtree(new_path)\n            shutil.copytree(orig_path, new_path)\n            continue\n        raise RuntimeError(\"Check the file paths in `mirror_files_into_torchgen()`\")\n\n\n# all the work we need to do _before_ setup runs\ndef build_deps():\n    report(\"-- Building version \" + version)\n\n    check_submodules()\n    check_pydep(\"yaml\", \"pyyaml\")\n    build_python = not BUILD_LIBTORCH_WHL\n    build_pytorch(\n        version=version,\n        cmake_python_library=cmake_python_library,\n        build_python=build_python,\n        rerun_cmake=RERUN_CMAKE,\n        cmake_only=CMAKE_ONLY,\n        cmake=cmake,\n    )\n\n    if CMAKE_ONLY:\n        report(\n            'Finished running cmake. Run \"ccmake build\" or '\n            '\"cmake-gui build\" to adjust build options and '\n            '\"python setup.py install\" to build.'\n        )\n        sys.exit()\n\n    # Use copies instead of symbolic files.\n    # Windows has very poor support for them.\n    sym_files = [\n        \"tools/shared/_utils_internal.py\",\n        \"torch/utils/benchmark/utils/valgrind_wrapper/callgrind.h\",\n        \"torch/utils/benchmark/utils/valgrind_wrapper/valgrind.h\",\n    ]\n    orig_files = [\n        \"torch/_utils_internal.py\",\n        \"third_party/valgrind-headers/callgrind.h\",\n        \"third_party/valgrind-headers/valgrind.h\",\n    ]\n    for sym_file, orig_file in zip(sym_files, orig_files):\n        same = False\n        if os.path.exists(sym_file):\n            if filecmp.cmp(sym_file, orig_file):\n                same = True\n            else:\n                os.remove(sym_file)\n        if not same:\n            shutil.copyfile(orig_file, sym_file)\n\n\n################################################################################\n# Building dependent libraries\n################################################################################\n\nmissing_pydep = \"\"\"\nMissing build dependency: Unable to `import {importname}`.\nPlease install it via `conda install {module}` or `pip install {module}`\n\"\"\".strip()\n\n\ndef check_pydep(importname, module):\n    try:\n        importlib.import_module(importname)\n    except ImportError as e:\n        raise RuntimeError(\n            missing_pydep.format(importname=importname, module=module)\n        ) from e\n\n\nclass build_ext(setuptools.command.build_ext.build_ext):\n    def _embed_libomp(self):\n        # Copy libiomp5.dylib/libomp.dylib inside the wheel package on MacOS\n        lib_dir = os.path.join(self.build_lib, \"torch\", \"lib\")\n        libtorch_cpu_path = os.path.join(lib_dir, \"libtorch_cpu.dylib\")\n        if not os.path.exists(libtorch_cpu_path):\n            return\n        # Parse libtorch_cpu load commands\n        otool_cmds = (\n            subprocess.check_output([\"otool\", \"-l\", libtorch_cpu_path])\n            .decode(\"utf-8\")\n            .split(\"\\n\")\n        )\n        rpaths, libs = [], []\n        for idx, line in enumerate(otool_cmds):\n            if line.strip() == \"cmd LC_LOAD_DYLIB\":\n                lib_name = otool_cmds[idx + 2].strip()\n                assert lib_name.startswith(\"name \")\n                libs.append(lib_name.split(\" \", 1)[1].rsplit(\"(\", 1)[0][:-1])\n\n            if line.strip() == \"cmd LC_RPATH\":\n                rpath = otool_cmds[idx + 2].strip()\n                assert rpath.startswith(\"path \")\n                rpaths.append(rpath.split(\" \", 1)[1].rsplit(\"(\", 1)[0][:-1])\n\n        omp_lib_name = (\n            \"libomp.dylib\" if os.uname().machine == \"arm64\" else \"libiomp5.dylib\"\n        )\n        omp_rpath_lib_path = os.path.join(\"@rpath\", omp_lib_name)\n        if omp_rpath_lib_path not in libs:\n            return\n\n        # Copy libomp/libiomp5 from rpath locations\n        for rpath in rpaths:\n            source_lib = os.path.join(rpath, omp_lib_name)\n            if not os.path.exists(source_lib):\n                continue\n            target_lib = os.path.join(self.build_lib, \"torch\", \"lib\", omp_lib_name)\n            self.copy_file(source_lib, target_lib)\n            # Delete old rpath and add @loader_lib to the rpath\n            # This should prevent delocate from attempting to package another instance\n            # of OpenMP library in torch wheel as well as loading two libomp.dylib into\n            # the address space, as libraries are cached by their unresolved names\n            subprocess.check_call(\n                [\n                    \"install_name_tool\",\n                    \"-rpath\",\n                    rpath,\n                    \"@loader_path\",\n                    libtorch_cpu_path,\n                ]\n            )\n            break\n\n        # Copy omp.h from OpenMP_C_FLAGS and copy it into include folder\n        omp_cflags = get_cmake_cache_vars()[\"OpenMP_C_FLAGS\"]\n        if not omp_cflags:\n            return\n        for include_dir in [f[2:] for f in omp_cflags.split(\" \") if f.startswith(\"-I\")]:\n            omp_h = os.path.join(include_dir, \"omp.h\")\n            if not os.path.exists(omp_h):\n                continue\n            target_omp_h = os.path.join(self.build_lib, \"torch\", \"include\", \"omp.h\")\n            self.copy_file(omp_h, target_omp_h)\n            break\n\n    def run(self):\n        # Report build options. This is run after the build completes so # `CMakeCache.txt` exists and we can get an\n        # accurate report on what is used and what is not.\n        cmake_cache_vars = defaultdict(lambda: False, cmake.get_cmake_cache_variables())\n        if cmake_cache_vars[\"USE_NUMPY\"]:\n            report(\"-- Building with NumPy bindings\")\n        else:\n            report(\"-- NumPy not found\")\n        if cmake_cache_vars[\"USE_CUDNN\"]:\n            report(\n                \"-- Detected cuDNN at \"\n                + cmake_cache_vars[\"CUDNN_LIBRARY\"]\n                + \", \"\n                + cmake_cache_vars[\"CUDNN_INCLUDE_DIR\"]\n            )\n        else:\n            report(\"-- Not using cuDNN\")\n        if cmake_cache_vars[\"USE_CUDA\"]:\n            report(\"-- Detected CUDA at \" + cmake_cache_vars[\"CUDA_TOOLKIT_ROOT_DIR\"])\n        else:\n            report(\"-- Not using CUDA\")\n        if cmake_cache_vars[\"USE_XPU\"]:\n            report(\"-- Detected XPU runtime at \" + cmake_cache_vars[\"SYCL_LIBRARY_DIR\"])\n        else:\n            report(\"-- Not using XPU\")\n        if cmake_cache_vars[\"USE_MKLDNN\"]:\n            report(\"-- Using MKLDNN\")\n            if cmake_cache_vars[\"USE_MKLDNN_ACL\"]:\n                report(\"-- Using Compute Library for the Arm architecture with MKLDNN\")\n            else:\n                report(\n                    \"-- Not using Compute Library for the Arm architecture with MKLDNN\"\n                )\n            if cmake_cache_vars[\"USE_MKLDNN_CBLAS\"]:\n                report(\"-- Using CBLAS in MKLDNN\")\n            else:\n                report(\"-- Not using CBLAS in MKLDNN\")\n        else:\n            report(\"-- Not using MKLDNN\")\n        if cmake_cache_vars[\"USE_NCCL\"] and cmake_cache_vars[\"USE_SYSTEM_NCCL\"]:\n            report(\n                \"-- Using system provided NCCL library at {}, {}\".format(\n                    cmake_cache_vars[\"NCCL_LIBRARIES\"],\n                    cmake_cache_vars[\"NCCL_INCLUDE_DIRS\"],\n                )\n            )\n        elif cmake_cache_vars[\"USE_NCCL\"]:\n            report(\"-- Building NCCL library\")\n        else:\n            report(\"-- Not using NCCL\")\n        if cmake_cache_vars[\"USE_DISTRIBUTED\"]:\n            if IS_WINDOWS:\n                report(\"-- Building without distributed package\")\n            else:\n                report(\"-- Building with distributed package: \")\n                report(\n                    \"  -- USE_TENSORPIPE={}\".format(cmake_cache_vars[\"USE_TENSORPIPE\"])\n                )\n                report(\"  -- USE_GLOO={}\".format(cmake_cache_vars[\"USE_GLOO\"]))\n                report(\"  -- USE_MPI={}\".format(cmake_cache_vars[\"USE_OPENMPI\"]))\n        else:\n            report(\"-- Building without distributed package\")\n        if cmake_cache_vars[\"STATIC_DISPATCH_BACKEND\"]:\n            report(\n                \"-- Using static dispatch with backend {}\".format(\n                    cmake_cache_vars[\"STATIC_DISPATCH_BACKEND\"]\n                )\n            )\n        if cmake_cache_vars[\"USE_LIGHTWEIGHT_DISPATCH\"]:\n            report(\"-- Using lightweight dispatch\")\n        if cmake_cache_vars[\"BUILD_EXECUTORCH\"]:\n            report(\"-- Building Executorch\")\n\n        if cmake_cache_vars[\"USE_ITT\"]:\n            report(\"-- Using ITT\")\n        else:\n            report(\"-- Not using ITT\")\n\n        # Do not use clang to compile extensions if `-fstack-clash-protection` is defined\n        # in system CFLAGS\n        c_flags = str(os.getenv(\"CFLAGS\", \"\"))\n        if (\n            IS_LINUX\n            and \"-fstack-clash-protection\" in c_flags\n            and \"clang\" in os.environ.get(\"CC\", \"\")\n        ):\n            os.environ[\"CC\"] = str(os.environ[\"CC\"])\n\n        # It's an old-style class in Python 2.7...\n        setuptools.command.build_ext.build_ext.run(self)\n\n        if IS_DARWIN and package_type != \"conda\":\n            self._embed_libomp()\n\n        # Copy the essential export library to compile C++ extensions.\n        if IS_WINDOWS:\n            build_temp = self.build_temp\n\n            ext_filename = self.get_ext_filename(\"_C\")\n            lib_filename = \".\".join(ext_filename.split(\".\")[:-1]) + \".lib\"\n\n            export_lib = os.path.join(\n                build_temp, \"torch\", \"csrc\", lib_filename\n            ).replace(\"\\\\\", \"/\")\n\n            build_lib = self.build_lib\n\n            target_lib = os.path.join(build_lib, \"torch\", \"lib\", \"_C.lib\").replace(\n                \"\\\\\", \"/\"\n            )\n\n            # Create \"torch/lib\" directory if not exists.\n            # (It is not created yet in \"develop\" mode.)\n            target_dir = os.path.dirname(target_lib)\n            if not os.path.exists(target_dir):\n                os.makedirs(target_dir)\n\n            self.copy_file(export_lib, target_lib)\n\n    def build_extensions(self):\n        self.create_compile_commands()\n\n        # Copy functorch extension\n        for i, ext in enumerate(self.extensions):\n            if ext.name != \"functorch._C\":\n                continue\n            fullname = self.get_ext_fullname(ext.name)\n            filename = self.get_ext_filename(fullname)\n            fileext = os.path.splitext(filename)[1]\n            src = os.path.join(os.path.dirname(filename), \"functorch\" + fileext)\n            dst = os.path.join(os.path.realpath(self.build_lib), filename)\n            if os.path.exists(src):\n                report(f\"Copying {ext.name} from {src} to {dst}\")\n                dst_dir = os.path.dirname(dst)\n                if not os.path.exists(dst_dir):\n                    os.makedirs(dst_dir)\n                self.copy_file(src, dst)\n\n        setuptools.command.build_ext.build_ext.build_extensions(self)\n\n    def get_outputs(self):\n        outputs = setuptools.command.build_ext.build_ext.get_outputs(self)\n        outputs.append(os.path.join(self.build_lib, \"caffe2\"))\n        report(f\"setup.py::get_outputs returning {outputs}\")\n        return outputs\n\n    def create_compile_commands(self):\n        def load(filename):\n            with open(filename) as f:\n                return json.load(f)\n\n        ninja_files = glob.glob(\"build/*compile_commands.json\")\n        cmake_files = glob.glob(\"torch/lib/build/*/compile_commands.json\")\n        all_commands = [entry for f in ninja_files + cmake_files for entry in load(f)]\n\n        # cquery does not like c++ compiles that start with gcc.\n        # It forgets to include the c++ header directories.\n        # We can work around this by replacing the gcc calls that python\n        # setup.py generates with g++ calls instead\n        for command in all_commands:\n            if command[\"command\"].startswith(\"gcc \"):\n                command[\"command\"] = \"g++ \" + command[\"command\"][4:]\n\n        new_contents = json.dumps(all_commands, indent=2)\n        contents = \"\"\n        if os.path.exists(\"compile_commands.json\"):\n            with open(\"compile_commands.json\") as f:\n                contents = f.read()\n        if contents != new_contents:\n            with open(\"compile_commands.json\", \"w\") as f:\n                f.write(new_contents)\n\n\nclass concat_license_files:\n    \"\"\"Merge LICENSE and LICENSES_BUNDLED.txt as a context manager\n\n    LICENSE is the main PyTorch license, LICENSES_BUNDLED.txt is auto-generated\n    from all the licenses found in ./third_party/. We concatenate them so there\n    is a single license file in the sdist and wheels with all of the necessary\n    licensing info.\n    \"\"\"\n\n    def __init__(self, include_files=False):\n        self.f1 = \"LICENSE\"\n        self.f2 = \"third_party/LICENSES_BUNDLED.txt\"\n        self.include_files = include_files\n\n    def __enter__(self):\n        \"\"\"Concatenate files\"\"\"\n\n        old_path = sys.path\n        sys.path.append(third_party_path)\n        try:\n            from build_bundled import create_bundled\n        finally:\n            sys.path = old_path\n\n        with open(self.f1) as f1:\n            self.bsd_text = f1.read()\n\n        with open(self.f1, \"a\") as f1:\n            f1.write(\"\\n\\n\")\n            create_bundled(\n                os.path.relpath(third_party_path), f1, include_files=self.include_files\n            )\n\n    def __exit__(self, exception_type, exception_value, traceback):\n        \"\"\"Restore content of f1\"\"\"\n        with open(self.f1, \"w\") as f:\n            f.write(self.bsd_text)\n\n\ntry:\n    from wheel.bdist_wheel import bdist_wheel\nexcept ImportError:\n    # This is useful when wheel is not installed and bdist_wheel is not\n    # specified on the command line. If it _is_ specified, parsing the command\n    # line will fail before wheel_concatenate is needed\n    wheel_concatenate = None\nelse:\n    # Need to create the proper LICENSE.txt for the wheel\n    class wheel_concatenate(bdist_wheel):\n        \"\"\"check submodules on sdist to prevent incomplete tarballs\"\"\"\n\n        def run(self):\n            with concat_license_files(include_files=True):\n                super().run()\n\n        def write_wheelfile(self, *args, **kwargs):\n            super().write_wheelfile(*args, **kwargs)\n\n            if BUILD_LIBTORCH_WHL:\n                # Remove extraneneous files in the libtorch wheel\n                for root, dirs, files in os.walk(self.bdist_dir):\n                    for file in files:\n                        if file.endswith((\".a\", \".so\")) and os.path.isfile(\n                            os.path.join(self.bdist_dir, file)\n                        ):\n                            os.remove(os.path.join(root, file))\n                        elif file.endswith(\".py\"):\n                            os.remove(os.path.join(root, file))\n                # need an __init__.py file otherwise we wouldn't have a package\n                open(os.path.join(self.bdist_dir, \"torch\", \"__init__.py\"), \"w\").close()\n\n\nclass install(setuptools.command.install.install):\n    def run(self):\n        super().run()\n\n\nclass clean(setuptools.Command):\n    user_options = []\n\n    def initialize_options(self):\n        pass\n\n    def finalize_options(self):\n        pass\n\n    def run(self):\n        import glob\n        import re\n\n        with open(\".gitignore\") as f:\n            ignores = f.read()\n            pat = re.compile(r\"^#( BEGIN NOT-CLEAN-FILES )?\")\n            for wildcard in filter(None, ignores.split(\"\\n\")):\n                match = pat.match(wildcard)\n                if match:\n                    if match.group(1):\n                        # Marker is found and stop reading .gitignore.\n                        break\n                    # Ignore lines which begin with '#'.\n                else:\n                    # Don't remove absolute paths from the system\n                    wildcard = wildcard.lstrip(\"./\")\n\n                    for filename in glob.glob(wildcard):\n                        try:\n                            os.remove(filename)\n                        except OSError:\n                            shutil.rmtree(filename, ignore_errors=True)\n\n\nclass sdist(setuptools.command.sdist.sdist):\n    def run(self):\n        with concat_license_files():\n            super().run()\n\n\ndef get_cmake_cache_vars():\n    try:\n        return defaultdict(lambda: False, cmake.get_cmake_cache_variables())\n    except FileNotFoundError:\n        # CMakeCache.txt does not exist. Probably running \"python setup.py clean\" over a clean directory.\n        return defaultdict(lambda: False)\n\n\ndef configure_extension_build():\n    r\"\"\"Configures extension build options according to system environment and user's choice.\n\n    Returns:\n      The input to parameters ext_modules, cmdclass, packages, and entry_points as required in setuptools.setup.\n    \"\"\"\n\n    cmake_cache_vars = get_cmake_cache_vars()\n\n    ################################################################################\n    # Configure compile flags\n    ################################################################################\n\n    library_dirs = []\n    extra_install_requires = []\n\n    if IS_WINDOWS:\n        # /NODEFAULTLIB makes sure we only link to DLL runtime\n        # and matches the flags set for protobuf and ONNX\n        extra_link_args = [\"/NODEFAULTLIB:LIBCMT.LIB\"]\n        # /MD links against DLL runtime\n        # and matches the flags set for protobuf and ONNX\n        # /EHsc is about standard C++ exception handling\n        extra_compile_args = [\"/MD\", \"/FS\", \"/EHsc\"]\n    else:\n        extra_link_args = []\n        extra_compile_args = [\n            \"-Wall\",\n            \"-Wextra\",\n            \"-Wno-strict-overflow\",\n            \"-Wno-unused-parameter\",\n            \"-Wno-missing-field-initializers\",\n            \"-Wno-unknown-pragmas\",\n            # Python 2.6 requires -fno-strict-aliasing, see\n            # http://legacy.python.org/dev/peps/pep-3123/\n            # We also depend on it in our code (even Python 3).\n            \"-fno-strict-aliasing\",\n        ]\n\n    library_dirs.append(lib_path)\n\n    main_compile_args = []\n    main_libraries = [\"torch_python\"]\n\n    main_link_args = []\n    main_sources = [\"torch/csrc/stub.c\"]\n\n    if BUILD_LIBTORCH_WHL:\n        main_libraries = [\"torch\"]\n        main_sources = []\n\n    if build_type.is_debug():\n        if IS_WINDOWS:\n            extra_compile_args.append(\"/Z7\")\n            extra_link_args.append(\"/DEBUG:FULL\")\n        else:\n            extra_compile_args += [\"-O0\", \"-g\"]\n            extra_link_args += [\"-O0\", \"-g\"]\n\n    if build_type.is_rel_with_deb_info():\n        if IS_WINDOWS:\n            extra_compile_args.append(\"/Z7\")\n            extra_link_args.append(\"/DEBUG:FULL\")\n        else:\n            extra_compile_args += [\"-g\"]\n            extra_link_args += [\"-g\"]\n\n    # pypi cuda package that requires installation of cuda runtime, cudnn and cublas\n    # should be included in all wheels uploaded to pypi\n    pytorch_extra_install_requirements = os.getenv(\n        \"PYTORCH_EXTRA_INSTALL_REQUIREMENTS\", \"\"\n    )\n    if pytorch_extra_install_requirements:\n        report(\n            f\"pytorch_extra_install_requirements: {pytorch_extra_install_requirements}\"\n        )\n        extra_install_requires += pytorch_extra_install_requirements.split(\"|\")\n\n    # Cross-compile for M1\n    if IS_DARWIN:\n        macos_target_arch = os.getenv(\"CMAKE_OSX_ARCHITECTURES\", \"\")\n        if macos_target_arch in [\"arm64\", \"x86_64\"]:\n            macos_sysroot_path = os.getenv(\"CMAKE_OSX_SYSROOT\")\n            if macos_sysroot_path is None:\n                macos_sysroot_path = (\n                    subprocess.check_output(\n                        [\"xcrun\", \"--show-sdk-path\", \"--sdk\", \"macosx\"]\n                    )\n                    .decode(\"utf-8\")\n                    .strip()\n                )\n            extra_compile_args += [\n                \"-arch\",\n                macos_target_arch,\n                \"-isysroot\",\n                macos_sysroot_path,\n            ]\n            extra_link_args += [\"-arch\", macos_target_arch]\n\n    def make_relative_rpath_args(path):\n        if IS_DARWIN:\n            return [\"-Wl,-rpath,@loader_path/\" + path]\n        elif IS_WINDOWS:\n            return []\n        else:\n            return [\"-Wl,-rpath,$ORIGIN/\" + path]\n\n    ################################################################################\n    # Declare extensions and package\n    ################################################################################\n\n    extensions = []\n    excludes = [\"tools\", \"tools.*\", \"caffe2\", \"caffe2.*\"]\n    if not cmake_cache_vars[\"BUILD_FUNCTORCH\"]:\n        excludes.extend([\"functorch\", \"functorch.*\"])\n    packages = find_packages(exclude=excludes)\n    C = Extension(\n        \"torch._C\",\n        libraries=main_libraries,\n        sources=main_sources,\n        language=\"c\",\n        extra_compile_args=main_compile_args + extra_compile_args,\n        include_dirs=[],\n        library_dirs=library_dirs,\n        extra_link_args=extra_link_args\n        + main_link_args\n        + make_relative_rpath_args(\"lib\"),\n    )\n    extensions.append(C)\n\n    # These extensions are built by cmake and copied manually in build_extensions()\n    # inside the build_ext implementation\n    if cmake_cache_vars[\"BUILD_FUNCTORCH\"]:\n        extensions.append(\n            Extension(name=\"functorch._C\", sources=[]),\n        )\n\n    cmdclass = {\n        \"bdist_wheel\": wheel_concatenate,\n        \"build_ext\": build_ext,\n        \"clean\": clean,\n        \"install\": install,\n        \"sdist\": sdist,\n    }\n\n    entry_points = {\n        \"console_scripts\": [\n            \"torchrun = torch.distributed.run:main\",\n        ],\n        \"torchrun.logs_specs\": [\n            \"default = torch.distributed.elastic.multiprocessing:DefaultLogsSpecs\",\n        ],\n    }\n\n    if cmake_cache_vars[\"USE_DISTRIBUTED\"]:\n        # Only enable fr_trace command if distributed is enabled\n        entry_points[\"console_scripts\"].append(\n            \"torchfrtrace = tools.flight_recorder.fr_trace:main\",\n        )\n    return extensions, cmdclass, packages, entry_points, extra_install_requires\n\n\n# post run, warnings, printed at the end to make them more visible\nbuild_update_message = \"\"\"\n    It is no longer necessary to use the 'build' or 'rebuild' targets\n\n    To install:\n      $ python setup.py install\n    To develop locally:\n      $ python setup.py develop\n    To force cmake to re-generate native build files (off by default):\n      $ python setup.py develop --cmake\n\"\"\"\n\n\ndef print_box(msg):\n    lines = msg.split(\"\\n\")\n    size = max(len(l) + 1 for l in lines)\n    print(\"-\" * (size + 2))\n    for l in lines:\n        print(\"|{}{}|\".format(l, \" \" * (size - len(l))))\n    print(\"-\" * (size + 2))\n\n\ndef main():\n    if BUILD_LIBTORCH_WHL and BUILD_PYTHON_ONLY:\n        raise RuntimeError(\n            \"Conflict: 'BUILD_LIBTORCH_WHL' and 'BUILD_PYTHON_ONLY' can't both be 1. Set one to 0 and rerun.\"\n        )\n    install_requires = [\n        \"filelock\",\n        \"typing-extensions>=4.10.0\",\n        'setuptools ; python_version >= \"3.12\"',\n        'sympy==1.13.1 ; python_version >= \"3.9\"',\n        \"networkx\",\n        \"jinja2\",\n        \"fsspec\",\n    ]\n\n    if BUILD_PYTHON_ONLY:\n        install_requires.append(f\"{LIBTORCH_PKG_NAME}=={get_torch_version()}\")\n\n    use_prioritized_text = str(os.getenv(\"USE_PRIORITIZED_TEXT_FOR_LD\", \"\"))\n    if (\n        use_prioritized_text == \"\"\n        and platform.system() == \"Linux\"\n        and platform.processor() == \"aarch64\"\n    ):\n        print_box(\n            \"\"\"\n            WARNING: we strongly recommend enabling linker script optimization for ARM + CUDA.\n            To do so please export USE_PRIORITIZED_TEXT_FOR_LD=1\n            \"\"\"\n        )\n    if use_prioritized_text == \"1\" or use_prioritized_text == \"True\":\n        gen_linker_script(\n            filein=\"cmake/prioritized_text.txt\", fout=\"cmake/linker_script.ld\"\n        )\n        linker_script_path = os.path.abspath(\"cmake/linker_script.ld\")\n        os.environ[\"LDFLAGS\"] = os.getenv(\"LDFLAGS\", \"\") + f\" -T{linker_script_path}\"\n        os.environ[\"CFLAGS\"] = (\n            os.getenv(\"CFLAGS\", \"\") + \" -ffunction-sections -fdata-sections\"\n        )\n        os.environ[\"CXXFLAGS\"] = (\n            os.getenv(\"CXXFLAGS\", \"\") + \" -ffunction-sections -fdata-sections\"\n        )\n\n    # Parse the command line and check the arguments before we proceed with\n    # building deps and setup. We need to set values so `--help` works.\n    dist = Distribution()\n    dist.script_name = os.path.basename(sys.argv[0])\n    dist.script_args = sys.argv[1:]\n    try:\n        dist.parse_command_line()\n    except setuptools.distutils.errors.DistutilsArgError as e:\n        print(e)\n        sys.exit(1)\n\n    mirror_files_into_torchgen()\n    if RUN_BUILD_DEPS:\n        build_deps()\n\n    (\n        extensions,\n        cmdclass,\n        packages,\n        entry_points,\n        extra_install_requires,\n    ) = configure_extension_build()\n    install_requires += extra_install_requires\n\n    extras_require = {\n        \"optree\": [\"optree>=0.13.0\"],\n        \"opt-einsum\": [\"opt-einsum>=3.3\"],\n    }\n\n    # Read in README.md for our long_description\n    with open(os.path.join(cwd, \"README.md\"), encoding=\"utf-8\") as f:\n        long_description = f.read()\n\n    version_range_max = max(sys.version_info[1], 13) + 1\n    torch_package_data = [\n        \"py.typed\",\n        \"bin/*\",\n        \"test/*\",\n        \"*.pyi\",\n        \"_C/*.pyi\",\n        \"cuda/*.pyi\",\n        \"fx/*.pyi\",\n        \"optim/*.pyi\",\n        \"autograd/*.pyi\",\n        \"jit/*.pyi\",\n        \"nn/*.pyi\",\n        \"nn/modules/*.pyi\",\n        \"nn/parallel/*.pyi\",\n        \"utils/data/*.pyi\",\n        \"utils/data/datapipes/*.pyi\",\n        \"lib/*.pdb\",\n        \"lib/*shm*\",\n        \"lib/torch_shm_manager\",\n        \"lib/*.h\",\n        \"include/*.h\",\n        \"include/ATen/*.h\",\n        \"include/ATen/cpu/*.h\",\n        \"include/ATen/cpu/vec/vec128/*.h\",\n        \"include/ATen/cpu/vec/vec256/*.h\",\n        \"include/ATen/cpu/vec/vec256/vsx/*.h\",\n        \"include/ATen/cpu/vec/vec256/zarch/*.h\",\n        \"include/ATen/cpu/vec/vec512/*.h\",\n        \"include/ATen/cpu/vec/*.h\",\n        \"include/ATen/cpu/vec/sve/*.h\",\n        \"include/ATen/core/*.h\",\n        \"include/ATen/cuda/*.cuh\",\n        \"include/ATen/cuda/*.h\",\n        \"include/ATen/cuda/detail/*.cuh\",\n        \"include/ATen/cuda/detail/*.h\",\n        \"include/ATen/cuda/tunable/*.h\",\n        \"include/ATen/cudnn/*.h\",\n        \"include/ATen/functorch/*.h\",\n        \"include/ATen/ops/*.h\",\n        \"include/ATen/hip/*.cuh\",\n        \"include/ATen/hip/*.h\",\n        \"include/ATen/hip/detail/*.cuh\",\n        \"include/ATen/hip/detail/*.h\",\n        \"include/ATen/hip/impl/*.h\",\n        \"include/ATen/hip/tunable/*.h\",\n        \"include/ATen/mps/*.h\",\n        \"include/ATen/miopen/*.h\",\n        \"include/ATen/detail/*.h\",\n        \"include/ATen/native/*.h\",\n        \"include/ATen/native/cpu/*.h\",\n        \"include/ATen/native/cuda/*.h\",\n        \"include/ATen/native/cuda/*.cuh\",\n        \"include/ATen/native/hip/*.h\",\n        \"include/ATen/native/hip/*.cuh\",\n        \"include/ATen/native/kleidiai/*.h\",\n        \"include/ATen/native/mps/*.h\",\n        \"include/ATen/native/mkldnn/xpu/*.h\",\n        \"include/ATen/native/mkldnn/xpu/detail/*.h\",\n        \"include/ATen/native/nested/*.h\",\n        \"include/ATen/native/quantized/*.h\",\n        \"include/ATen/native/quantized/cpu/*.h\",\n        \"include/ATen/native/transformers/*.h\",\n        \"include/ATen/native/sparse/*.h\",\n        \"include/ATen/native/utils/*.h\",\n        \"include/ATen/quantized/*.h\",\n        \"include/ATen/xpu/*.h\",\n        \"include/ATen/xpu/detail/*.h\",\n        \"include/caffe2/serialize/*.h\",\n        \"include/c10/*.h\",\n        \"include/c10/macros/*.h\",\n        \"include/c10/core/*.h\",\n        \"include/ATen/core/boxing/*.h\",\n        \"include/ATen/core/boxing/impl/*.h\",\n        \"include/ATen/core/dispatch/*.h\",\n        \"include/ATen/core/op_registration/*.h\",\n        \"include/c10/core/impl/*.h\",\n        \"include/c10/util/*.h\",\n        \"include/c10/cuda/*.h\",\n        \"include/c10/cuda/impl/*.h\",\n        \"include/c10/hip/*.h\",\n        \"include/c10/hip/impl/*.h\",\n        \"include/c10/xpu/*.h\",\n        \"include/c10/xpu/impl/*.h\",\n        \"include/torch/*.h\",\n        \"include/torch/csrc/*.h\",\n        \"include/torch/csrc/api/include/torch/*.h\",\n        \"include/torch/csrc/api/include/torch/data/*.h\",\n        \"include/torch/csrc/api/include/torch/data/dataloader/*.h\",\n        \"include/torch/csrc/api/include/torch/data/datasets/*.h\",\n        \"include/torch/csrc/api/include/torch/data/detail/*.h\",\n        \"include/torch/csrc/api/include/torch/data/samplers/*.h\",\n        \"include/torch/csrc/api/include/torch/data/transforms/*.h\",\n        \"include/torch/csrc/api/include/torch/detail/*.h\",\n        \"include/torch/csrc/api/include/torch/detail/ordered_dict.h\",\n        \"include/torch/csrc/api/include/torch/nn/*.h\",\n        \"include/torch/csrc/api/include/torch/nn/functional/*.h\",\n        \"include/torch/csrc/api/include/torch/nn/options/*.h\",\n        \"include/torch/csrc/api/include/torch/nn/modules/*.h\",\n        \"include/torch/csrc/api/include/torch/nn/modules/container/*.h\",\n        \"include/torch/csrc/api/include/torch/nn/parallel/*.h\",\n        \"include/torch/csrc/api/include/torch/nn/utils/*.h\",\n        \"include/torch/csrc/api/include/torch/optim/*.h\",\n        \"include/torch/csrc/api/include/torch/optim/schedulers/*.h\",\n        \"include/torch/csrc/api/include/torch/serialize/*.h\",\n        \"include/torch/csrc/autograd/*.h\",\n        \"include/torch/csrc/autograd/functions/*.h\",\n        \"include/torch/csrc/autograd/generated/*.h\",\n        \"include/torch/csrc/autograd/utils/*.h\",\n        \"include/torch/csrc/cuda/*.h\",\n        \"include/torch/csrc/distributed/c10d/*.h\",\n        \"include/torch/csrc/distributed/c10d/*.hpp\",\n        \"include/torch/csrc/distributed/rpc/*.h\",\n        \"include/torch/csrc/distributed/autograd/context/*.h\",\n        \"include/torch/csrc/distributed/autograd/functions/*.h\",\n        \"include/torch/csrc/distributed/autograd/rpc_messages/*.h\",\n        \"include/torch/csrc/dynamo/*.h\",\n        \"include/torch/csrc/inductor/*.h\",\n        \"include/torch/csrc/inductor/aoti_package/*.h\",\n        \"include/torch/csrc/inductor/aoti_runner/*.h\",\n        \"include/torch/csrc/inductor/aoti_runtime/*.h\",\n        \"include/torch/csrc/inductor/aoti_torch/*.h\",\n        \"include/torch/csrc/inductor/aoti_torch/c/*.h\",\n        \"include/torch/csrc/inductor/aoti_torch/generated/*.h\",\n        \"include/torch/csrc/inductor/aoti_torch/generated/extend/*.h\",\n        \"include/torch/csrc/jit/*.h\",\n        \"include/torch/csrc/jit/backends/*.h\",\n        \"include/torch/csrc/jit/generated/*.h\",\n        \"include/torch/csrc/jit/passes/*.h\",\n        \"include/torch/csrc/jit/passes/quantization/*.h\",\n        \"include/torch/csrc/jit/passes/utils/*.h\",\n        \"include/torch/csrc/jit/runtime/*.h\",\n        \"include/torch/csrc/jit/ir/*.h\",\n        \"include/torch/csrc/jit/frontend/*.h\",\n        \"include/torch/csrc/jit/api/*.h\",\n        \"include/torch/csrc/jit/serialization/*.h\",\n        \"include/torch/csrc/jit/python/*.h\",\n        \"include/torch/csrc/jit/mobile/*.h\",\n        \"include/torch/csrc/jit/testing/*.h\",\n        \"include/torch/csrc/jit/tensorexpr/*.h\",\n        \"include/torch/csrc/jit/tensorexpr/operators/*.h\",\n        \"include/torch/csrc/jit/codegen/cuda/*.h\",\n        \"include/torch/csrc/onnx/*.h\",\n        \"include/torch/csrc/profiler/*.h\",\n        \"include/torch/csrc/profiler/orchestration/*.h\",\n        \"include/torch/csrc/profiler/standalone/*.h\",\n        \"include/torch/csrc/profiler/stubs/*.h\",\n        \"include/torch/csrc/profiler/unwind/*.h\",\n        \"include/torch/csrc/profiler/python/*.h\",\n        \"include/torch/csrc/utils/*.h\",\n        \"include/torch/csrc/tensor/*.h\",\n        \"include/torch/csrc/lazy/backend/*.h\",\n        \"include/torch/csrc/lazy/core/*.h\",\n        \"include/torch/csrc/lazy/core/internal_ops/*.h\",\n        \"include/torch/csrc/lazy/core/ops/*.h\",\n        \"include/torch/csrc/lazy/python/python_util.h\",\n        \"include/torch/csrc/lazy/ts_backend/*.h\",\n        \"include/torch/csrc/xpu/*.h\",\n        \"include/pybind11/*.h\",\n        \"include/pybind11/detail/*.h\",\n        \"include/pybind11/eigen/*.h\",\n        \"include/TH/*.h*\",\n        \"include/TH/generic/*.h*\",\n        \"include/THC/*.cuh\",\n        \"include/THC/*.h*\",\n        \"include/THC/generic/*.h\",\n        \"include/THH/*.cuh\",\n        \"include/THH/*.h*\",\n        \"include/THH/generic/*.h\",\n        \"include/sleef.h\",\n        \"_inductor/codegen/*.h\",\n        \"_inductor/codegen/aoti_runtime/*.cpp\",\n        \"_export/serde/*.yaml\",\n        \"_export/serde/*.thrift\",\n        \"share/cmake/ATen/*.cmake\",\n        \"share/cmake/Caffe2/*.cmake\",\n        \"share/cmake/Caffe2/public/*.cmake\",\n        \"share/cmake/Caffe2/Modules_CUDA_fix/*.cmake\",\n        \"share/cmake/Caffe2/Modules_CUDA_fix/upstream/*.cmake\",\n        \"share/cmake/Caffe2/Modules_CUDA_fix/upstream/FindCUDA/*.cmake\",\n        \"share/cmake/Gloo/*.cmake\",\n        \"share/cmake/Tensorpipe/*.cmake\",\n        \"share/cmake/Torch/*.cmake\",\n        \"utils/benchmark/utils/*.cpp\",\n        \"utils/benchmark/utils/valgrind_wrapper/*.cpp\",\n        \"utils/benchmark/utils/valgrind_wrapper/*.h\",\n        \"utils/model_dump/skeleton.html\",\n        \"utils/model_dump/code.js\",\n        \"utils/model_dump/*.mjs\",\n    ]\n\n    if not BUILD_LIBTORCH_WHL:\n        torch_package_data.extend(\n            [\n                \"lib/libtorch_python.so\",\n                \"lib/libtorch_python.dylib\",\n                \"lib/libtorch_python.dll\",\n            ]\n        )\n    if not BUILD_PYTHON_ONLY:\n        torch_package_data.extend(\n            [\n                \"lib/*.so*\",\n                \"lib/*.dylib*\",\n                \"lib/*.dll\",\n                \"lib/*.lib\",\n            ]\n        )\n        aotriton_image_path = os.path.join(lib_path, \"aotriton.images\")\n        aks2_files = []\n        for root, dirs, files in os.walk(aotriton_image_path):\n            subpath = os.path.relpath(root, start=aotriton_image_path)\n            for fn in files:\n                aks2_files.append(os.path.join(\"lib/aotriton.images\", subpath, fn))\n        torch_package_data += aks2_files\n    if get_cmake_cache_vars()[\"USE_TENSORPIPE\"]:\n        torch_package_data.extend(\n            [\n                \"include/tensorpipe/*.h\",\n                \"include/tensorpipe/channel/*.h\",\n                \"include/tensorpipe/channel/basic/*.h\",\n                \"include/tensorpipe/channel/cma/*.h\",\n                \"include/tensorpipe/channel/mpt/*.h\",\n                \"include/tensorpipe/channel/xth/*.h\",\n                \"include/tensorpipe/common/*.h\",\n                \"include/tensorpipe/core/*.h\",\n                \"include/tensorpipe/transport/*.h\",\n                \"include/tensorpipe/transport/ibv/*.h\",\n                \"include/tensorpipe/transport/shm/*.h\",\n                \"include/tensorpipe/transport/uv/*.h\",\n            ]\n        )\n    if get_cmake_cache_vars()[\"USE_KINETO\"]:\n        torch_package_data.extend(\n            [\n                \"include/kineto/*.h\",\n            ]\n        )\n    torchgen_package_data = [\n        # Recursive glob doesn't work in setup.py,\n        # https://github.com/pypa/setuptools/issues/1806\n        # To make this robust we should replace it with some code that\n        # returns a list of everything under packaged/\n        \"packaged/ATen/*\",\n        \"packaged/ATen/native/*\",\n        \"packaged/ATen/templates/*\",\n        \"packaged/autograd/*\",\n        \"packaged/autograd/templates/*\",\n    ]\n    package_data = {\n        \"torch\": torch_package_data,\n    }\n\n    if not BUILD_LIBTORCH_WHL:\n        package_data[\"torchgen\"] = torchgen_package_data\n    else:\n        # no extensions in BUILD_LIBTORCH_WHL mode\n        extensions = []\n\n    setup(\n        name=package_name,\n        version=version,\n        description=(\n            \"Tensors and Dynamic neural networks in \"\n            \"Python with strong GPU acceleration\"\n        ),\n        long_description=long_description,\n        long_description_content_type=\"text/markdown\",\n        ext_modules=extensions,\n        cmdclass=cmdclass,\n        packages=packages,\n        entry_points=entry_points,\n        install_requires=install_requires,\n        extras_require=extras_require,\n        package_data=package_data,\n        url=\"https://pytorch.org/\",\n        download_url=\"https://github.com/pytorch/pytorch/tags\",\n        author=\"PyTorch Team\",\n        author_email=\"packages@pytorch.org\",\n        python_requires=f\">={python_min_version_str}\",\n        # PyPI package information.\n        classifiers=[\n            \"Development Status :: 5 - Production/Stable\",\n            \"Intended Audience :: Developers\",\n            \"Intended Audience :: Education\",\n            \"Intended Audience :: Science/Research\",\n            \"License :: OSI Approved :: BSD License\",\n            \"Topic :: Scientific/Engineering\",\n            \"Topic :: Scientific/Engineering :: Mathematics\",\n            \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\n            \"Topic :: Software Development\",\n            \"Topic :: Software Development :: Libraries\",\n            \"Topic :: Software Development :: Libraries :: Python Modules\",\n            \"Programming Language :: C++\",\n            \"Programming Language :: Python :: 3\",\n        ]\n        + [\n            f\"Programming Language :: Python :: 3.{i}\"\n            for i in range(python_min_version[1], version_range_max)\n        ],\n        license=\"BSD-3-Clause\",\n        keywords=\"pytorch, machine learning\",\n    )\n    if EMIT_BUILD_WARNING:\n        print_box(build_update_message)\n\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "test",
          "type": "tree",
          "content": null
        },
        {
          "name": "third_party",
          "type": "tree",
          "content": null
        },
        {
          "name": "tools",
          "type": "tree",
          "content": null
        },
        {
          "name": "torch",
          "type": "tree",
          "content": null
        },
        {
          "name": "torchgen",
          "type": "tree",
          "content": null
        },
        {
          "name": "ubsan.supp",
          "type": "blob",
          "size": 0.04,
          "content": "vptr:pybind11::detail::translate_exception"
        },
        {
          "name": "ufunc_defs.bzl",
          "type": "blob",
          "size": 0.78,
          "content": "load(\"@bazel_skylib//lib:paths.bzl\", \"paths\")\nload(\":build_variables.bzl\", \"aten_ufunc_headers\")\n\naten_ufunc_names = [\n    paths.split_extension(paths.basename(h))[0]\n    for h in aten_ufunc_headers\n]\n\ndef aten_ufunc_generated_cpu_sources(gencode_pattern = \"{}\"):\n    return [gencode_pattern.format(name) for name in [\n        \"UfuncCPU_{}.cpp\".format(n)\n        for n in aten_ufunc_names\n    ]]\n\ndef aten_ufunc_generated_cpu_kernel_sources(gencode_pattern = \"{}\"):\n    return [gencode_pattern.format(name) for name in [\n        \"UfuncCPUKernel_{}.cpp\".format(n)\n        for n in aten_ufunc_names\n    ]]\n\ndef aten_ufunc_generated_cuda_sources(gencode_pattern = \"{}\"):\n    return [gencode_pattern.format(name) for name in [\n        \"UfuncCUDA_{}.cu\".format(n)\n        for n in aten_ufunc_names\n    ]]\n"
        },
        {
          "name": "version.txt",
          "type": "blob",
          "size": 0.01,
          "content": "2.7.0a0\n"
        }
      ]
    }
  ]
}