{
  "metadata": {
    "timestamp": 1736557469490,
    "page": 497,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjUyMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "babysor/MockingBird",
      "stars": 35629,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".dockerignore",
          "type": "blob",
          "size": 0.09,
          "content": "*/saved_models\n!vocoder/saved_models/pretrained/**\n!encoder/saved_models/pretrained.pt\n/datasets"
        },
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0.03,
          "content": "*.ipynb linguist-vendored\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.32,
          "content": "*.pyc\n*.aux\n*.log\n*.out\n*.synctex.gz\n*.suo\n*__pycache__\n*.idea\n*.ipynb_checkpoints\n*.pickle\n*.npy\n*.blg\n*.bbl\n*.bcf\n*.toc\n*.sh\ndata/ckpt/*/*\n!data/ckpt/encoder/pretrained.pt\n!data/ckpt/vocoder/pretrained/\nwavs\nlog\n!/docker-entrypoint.sh\n!/datasets_download/*.sh\n/datasets\nmonotonic_align/build\nmonotonic_align/monotonic_align"
        },
        {
          "name": ".vscode",
          "type": "tree",
          "content": null
        },
        {
          "name": "CODE_OF_CONDUCT.md",
          "type": "blob",
          "size": 5.13,
          "content": "# Contributor Covenant Code of Conduct\n## First of all\nDon't be evil, never\n\n## Our Pledge\n\nWe as members, contributors, and leaders pledge to make participation in our\ncommunity a harassment-free experience for everyone, regardless of age, body\nsize, visible or invisible disability, ethnicity, sex characteristics, gender\nidentity and expression, level of experience, education, socio-economic status,\nnationality, personal appearance, race, religion, or sexual identity\nand orientation.\n\nWe pledge to act and interact in ways that contribute to an open, welcoming,\ndiverse, inclusive, and healthy community.\n\n## Our Standards\n\nExamples of behavior that contributes to a positive environment for our\ncommunity include:\n\n* Demonstrating empathy and kindness toward other people\n* Being respectful of differing opinions, viewpoints, and experiences\n* Giving and gracefully accepting constructive feedback\n* Accepting responsibility and apologizing to those affected by our mistakes,\n  and learning from the experience\n* Focusing on what is best not just for us as individuals, but for the\n  overall community\n\nExamples of unacceptable behavior include:\n\n* The use of sexualized language or imagery, and sexual attention or\n  advances of any kind\n* Trolling, insulting or derogatory comments, and personal or political attacks\n* Public or private harassment\n* Publishing others' private information, such as a physical or email\n  address, without their explicit permission\n* Other conduct which could reasonably be considered inappropriate in a\n  professional setting\n\n## Enforcement Responsibilities\n\nCommunity leaders are responsible for clarifying and enforcing our standards of\nacceptable behavior and will take appropriate and fair corrective action in\nresponse to any behavior that they deem inappropriate, threatening, offensive,\nor harmful.\n\nCommunity leaders have the right and responsibility to remove, edit, or reject\ncomments, commits, code, wiki edits, issues, and other contributions that are\nnot aligned to this Code of Conduct, and will communicate reasons for moderation\ndecisions when appropriate.\n\n## Scope\n\nThis Code of Conduct applies within all community spaces, and also applies when\nan individual is officially representing the community in public spaces.\nExamples of representing our community include using an official e-mail address,\nposting via an official social media account, or acting as an appointed\nrepresentative at an online or offline event.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported to the community leaders responsible for enforcement at\nbabysor00@gmail.com.\nAll complaints will be reviewed and investigated promptly and fairly.\n\nAll community leaders are obligated to respect the privacy and security of the\nreporter of any incident.\n\n## Enforcement Guidelines\n\nCommunity leaders will follow these Community Impact Guidelines in determining\nthe consequences for any action they deem in violation of this Code of Conduct:\n\n### 1. Correction\n\n**Community Impact**: Use of inappropriate language or other behavior deemed\nunprofessional or unwelcome in the community.\n\n**Consequence**: A private, written warning from community leaders, providing\nclarity around the nature of the violation and an explanation of why the\nbehavior was inappropriate. A public apology may be requested.\n\n### 2. Warning\n\n**Community Impact**: A violation through a single incident or series\nof actions.\n\n**Consequence**: A warning with consequences for continued behavior. No\ninteraction with the people involved, including unsolicited interaction with\nthose enforcing the Code of Conduct, for a specified period of time. This\nincludes avoiding interactions in community spaces as well as external channels\nlike social media. Violating these terms may lead to a temporary or\npermanent ban.\n\n### 3. Temporary Ban\n\n**Community Impact**: A serious violation of community standards, including\nsustained inappropriate behavior.\n\n**Consequence**: A temporary ban from any sort of interaction or public\ncommunication with the community for a specified period of time. No public or\nprivate interaction with the people involved, including unsolicited interaction\nwith those enforcing the Code of Conduct, is allowed during this period.\nViolating these terms may lead to a permanent ban.\n\n### 4. Permanent Ban\n\n**Community Impact**: Demonstrating a pattern of violation of community\nstandards, including sustained inappropriate behavior,  harassment of an\nindividual, or aggression toward or disparagement of classes of individuals.\n\n**Consequence**: A permanent ban from any sort of public interaction within\nthe community.\n\n## Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant][homepage],\nversion 2.0, available at\nhttps://www.contributor-covenant.org/version/2/0/code_of_conduct.html.\n\nCommunity Impact Guidelines were inspired by [Mozilla's code of conduct\nenforcement ladder](https://github.com/mozilla/diversity).\n\n[homepage]: https://www.contributor-covenant.org\n\nFor answers to common questions about this code of conduct, see the FAQ at\nhttps://www.contributor-covenant.org/faq. Translations are available at\nhttps://www.contributor-covenant.org/translations.\n"
        },
        {
          "name": "Dockerfile",
          "type": "blob",
          "size": 0.52,
          "content": "FROM pytorch/pytorch:latest\n\nRUN apt-get update && apt-get install -y build-essential ffmpeg parallel aria2 && apt-get clean\n\nCOPY ./requirements.txt /workspace/requirements.txt\n\nRUN pip install -r requirements.txt && pip install webrtcvad-wheels\n\nCOPY . /workspace\n\nVOLUME [ \"/datasets\", \"/workspace/synthesizer/saved_models/\" ]\n\nENV DATASET_MIRROR=default FORCE_RETRAIN=false TRAIN_DATASETS=aidatatang_200zh\\ magicdata\\ aishell3\\ data_aishell TRAIN_SKIP_EXISTING=true\n\nEXPOSE 8080\n\nENTRYPOINT [ \"/workspace/docker-entrypoint.sh\" ]\n"
        },
        {
          "name": "LICENSE.txt",
          "type": "blob",
          "size": 1.32,
          "content": "MIT License\n\nModified & original work Copyright (c) 2019 Corentin Jemine (https://github.com/CorentinJ)\nOriginal work Copyright (c) 2018 Rayhane Mama (https://github.com/Rayhane-mamah)\nOriginal work Copyright (c) 2019 fatchord (https://github.com/fatchord)\nOriginal work Copyright (c) 2015 braindead (https://github.com/braindead)\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README-CN.md",
          "type": "blob",
          "size": 17.13,
          "content": "## å®æ—¶è¯­éŸ³å…‹éš† - ä¸­æ–‡/æ™®é€šè¯\n![mockingbird](https://user-images.githubusercontent.com/12797292/131216767-6eb251d6-14fc-4951-8324-2722f0cd4c63.jpg)\n\n[![MIT License](https://img.shields.io/badge/license-MIT-blue.svg?style=flat)](http://choosealicense.com/licenses/mit/)\n\n### [English](README.md)  | ä¸­æ–‡\n\n### [DEMO VIDEO](https://www.bilibili.com/video/BV17Q4y1B7mY/) | [Wikiæ•™ç¨‹](https://github.com/babysor/MockingBird/wiki/Quick-Start-(Newbie)) ï½œ [è®­ç»ƒæ•™ç¨‹](https://vaj2fgg8yn.feishu.cn/docs/doccn7kAbr3SJz0KM0SIDJ0Xnhd)\n\n## ç‰¹æ€§\nğŸŒ **ä¸­æ–‡** æ”¯æŒæ™®é€šè¯å¹¶ä½¿ç”¨å¤šç§ä¸­æ–‡æ•°æ®é›†è¿›è¡Œæµ‹è¯•ï¼šaidatatang_200zh, magicdata, aishell3, biaobei, MozillaCommonVoice, data_aishell ç­‰\n\nğŸ¤© **PyTorch** é€‚ç”¨äº pytorchï¼Œå·²åœ¨ 1.9.0 ç‰ˆæœ¬ï¼ˆæœ€æ–°äº 2021 å¹´ 8 æœˆï¼‰ä¸­æµ‹è¯•ï¼ŒGPU Tesla T4 å’Œ GTX 2060\n\nğŸŒ **Windows + Linux** å¯åœ¨ Windows æ“ä½œç³»ç»Ÿå’Œ linux æ“ä½œç³»ç»Ÿä¸­è¿è¡Œï¼ˆè‹¹æœç³»ç»ŸM1ç‰ˆä¹Ÿæœ‰ç¤¾åŒºæˆåŠŸè¿è¡Œæ¡ˆä¾‹ï¼‰\n\nğŸ¤© **Easy & Awesome** ä»…éœ€ä¸‹è½½æˆ–æ–°è®­ç»ƒåˆæˆå™¨ï¼ˆsynthesizerï¼‰å°±æœ‰è‰¯å¥½æ•ˆæœï¼Œå¤ç”¨é¢„è®­ç»ƒçš„ç¼–ç å™¨/å£°ç å™¨ï¼Œæˆ–å®æ—¶çš„HiFi-GANä½œä¸ºvocoder\n\nğŸŒ **Webserver Ready** å¯ä¼ºæœä½ çš„è®­ç»ƒç»“æœï¼Œä¾›è¿œç¨‹è°ƒç”¨\n\n\n## å¼€å§‹\n### 1. å®‰è£…è¦æ±‚\n#### 1.1 é€šç”¨é…ç½®\n> æŒ‰ç…§åŸå§‹å­˜å‚¨åº“æµ‹è¯•æ‚¨æ˜¯å¦å·²å‡†å¤‡å¥½æ‰€æœ‰ç¯å¢ƒã€‚\nè¿è¡Œå·¥å…·ç®±(demo_toolbox.py)éœ€è¦ **Python 3.7 æˆ–æ›´é«˜ç‰ˆæœ¬** ã€‚\n\n* å®‰è£… [PyTorch](https://pytorch.org/get-started/locally/)ã€‚\n> å¦‚æœåœ¨ç”¨ pip æ–¹å¼å®‰è£…çš„æ—¶å€™å‡ºç° `ERROR: Could not find a version that satisfies the requirement torch==1.9.0+cu102 (from versions: 0.1.2, 0.1.2.post1, 0.1.2.post2)` è¿™ä¸ªé”™è¯¯å¯èƒ½æ˜¯ python ç‰ˆæœ¬è¿‡ä½ï¼Œ3.9 å¯ä»¥å®‰è£…æˆåŠŸ\n* å®‰è£… [ffmpeg](https://ffmpeg.org/download.html#get-packages)ã€‚\n* è¿è¡Œ`pip install -r requirements.txt` æ¥å®‰è£…å‰©ä½™çš„å¿…è¦åŒ…ã€‚\n> è¿™é‡Œçš„ç¯å¢ƒå»ºè®®ä½¿ç”¨ `Repo Tag 0.0.1` `Pytorch1.9.0 with Torchvision0.10.0 and cudatoolkit10.2` `requirements.txt` `webrtcvad-wheels` å› ä¸º `requiremants.txt` æ˜¯åœ¨å‡ ä¸ªæœˆå‰å¯¼å‡ºçš„ï¼Œæ‰€ä»¥ä¸é€‚é…æ–°ç‰ˆæœ¬\n* å®‰è£… webrtcvad `pip install webrtcvad-wheels`ã€‚\n\næˆ–è€…\n- ç”¨`conda`Â æˆ–è€…Â `mamba`Â å®‰è£…ä¾èµ–\n\n  ```conda env create -n env_name -f env.yml```\n\n  ```mamba env create -n env_name -f env.yml```\n\n  ä¼šåˆ›å»ºæ–°ç¯å¢ƒå®‰è£…å¿…é¡»çš„ä¾èµ–. ä¹‹åç”¨Â `conda activate env_name`Â åˆ‡æ¢ç¯å¢ƒå°±å®Œæˆäº†.\n  > env.ymlåªåŒ…å«äº†è¿è¡Œæ—¶å¿…è¦çš„ä¾èµ–ï¼Œæš‚æ—¶ä¸åŒ…æ‹¬monotonic-alignï¼Œå¦‚æœæƒ³è¦è£…GPUç‰ˆæœ¬çš„pytorchå¯ä»¥æŸ¥çœ‹å®˜ç½‘æ•™ç¨‹ã€‚\n\n#### 1.2 M1èŠ¯ç‰‡Macç¯å¢ƒé…ç½®ï¼ˆInference Time)\n> ä»¥ä¸‹ç¯å¢ƒæŒ‰x86-64æ­å»ºï¼Œä½¿ç”¨åŸç”Ÿçš„`demo_toolbox.py`ï¼Œå¯ä½œä¸ºåœ¨ä¸æ”¹ä»£ç æƒ…å†µä¸‹å¿«é€Ÿä½¿ç”¨çš„workaroundã€‚\n>\n  >  å¦‚éœ€ä½¿ç”¨M1èŠ¯ç‰‡è®­ç»ƒï¼Œå› `demo_toolbox.py`ä¾èµ–çš„`PyQt5`ä¸æ”¯æŒM1ï¼Œåˆ™åº”æŒ‰éœ€ä¿®æ”¹ä»£ç ï¼Œæˆ–è€…å°è¯•ä½¿ç”¨`web.py`ã€‚\n\n* å®‰è£…`PyQt5`ï¼Œå‚è€ƒ[è¿™ä¸ªé“¾æ¥](https://stackoverflow.com/a/68038451/20455983)\n  * ç”¨Rosettaæ‰“å¼€Terminalï¼Œå‚è€ƒ[è¿™ä¸ªé“¾æ¥](https://dev.to/courier/tips-and-tricks-to-setup-your-apple-m1-for-development-547g)\n  * ç”¨ç³»ç»ŸPythonåˆ›å»ºé¡¹ç›®è™šæ‹Ÿç¯å¢ƒ\n    ```\n    /usr/bin/python3 -m venv /PathToMockingBird/venv\n    source /PathToMockingBird/venv/bin/activate\n    ```\n  * å‡çº§pipå¹¶å®‰è£…`PyQt5`\n    ```\n    pip install --upgrade pip\n    pip install pyqt5\n    ```\n* å®‰è£…`pyworld`å’Œ`ctc-segmentation`\n  > è¿™é‡Œä¸¤ä¸ªæ–‡ä»¶ç›´æ¥`pip install`çš„æ—¶å€™æ‰¾ä¸åˆ°wheelï¼Œå°è¯•ä»cé‡Œbuildæ—¶æ‰¾ä¸åˆ°`Python.h`æŠ¥é”™\n  * å®‰è£…`pyworld`\n    * `brew install python` é€šè¿‡brewå®‰è£…pythonæ—¶ä¼šè‡ªåŠ¨å®‰è£…`Python.h`\n    * `export CPLUS_INCLUDE_PATH=/opt/homebrew/Frameworks/Python.framework/Headers` å¯¹äºM1ï¼Œbrewå®‰è£…`Python.h`åˆ°ä¸Šè¿°è·¯å¾„ã€‚æŠŠè·¯å¾„æ·»åŠ åˆ°ç¯å¢ƒå˜é‡é‡Œ\n    * `pip install pyworld`\n\n  * å®‰è£…`ctc-segmentation`\n    > å› ä¸Šè¿°æ–¹æ³•æ²¡æœ‰æˆåŠŸï¼Œé€‰æ‹©ä»[github](https://github.com/lumaku/ctc-segmentation) cloneæºç æ‰‹åŠ¨ç¼–è¯‘\n    * `git clone https://github.com/lumaku/ctc-segmentation.git` å…‹éš†åˆ°ä»»æ„ä½ç½®\n    * `cd ctc-segmentation`\n    * `source /PathToMockingBird/venv/bin/activate` å‡è®¾ä¸€å¼€å§‹æœªå¼€å¯ï¼Œæ‰“å¼€MockingBirdé¡¹ç›®çš„è™šæ‹Ÿç¯å¢ƒ\n    * `cythonize -3 ctc_segmentation/ctc_segmentation_dyn.pyx`\n    * `/usr/bin/arch -x86_64 python setup.py build` è¦æ³¨æ„æ˜ç¡®ç”¨x86-64æ¶æ„ç¼–è¯‘\n    * `/usr/bin/arch -x86_64 python setup.py install --optimize=1 --skip-build`ç”¨x86-64æ¶æ„å®‰è£…\n\n* å®‰è£…å…¶ä»–ä¾èµ–\n    * `/usr/bin/arch -x86_64 pip install torch torchvision torchaudio` è¿™é‡Œç”¨pipå®‰è£…`PyTorch`ï¼Œæ˜ç¡®æ¶æ„æ˜¯x86\n    * `pip install ffmpeg`  å®‰è£…ffmpeg\n    * `pip install -r requirements.txt`\n\n* è¿è¡Œ\n  > å‚è€ƒ[è¿™ä¸ªé“¾æ¥](https://youtrack.jetbrains.com/issue/PY-46290/Allow-running-Python-under-Rosetta-2-in-PyCharm-for-Apple-Silicon)\n  ï¼Œè®©é¡¹ç›®è·‘åœ¨x86æ¶æ„ç¯å¢ƒä¸Š\n  * `vim /PathToMockingBird/venv/bin/pythonM1`\n  * å†™å…¥ä»¥ä¸‹ä»£ç \n    ```\n    #!/usr/bin/env zsh\n    mydir=${0:a:h}\n    /usr/bin/arch -x86_64 $mydir/python \"$@\"\n    ```\n  * `chmod +x pythonM1` è®¾ä¸ºå¯æ‰§è¡Œæ–‡ä»¶\n  * å¦‚æœä½¿ç”¨PyCharmï¼Œåˆ™æŠŠInterpreteræŒ‡å‘`pythonM1`ï¼Œå¦åˆ™ä¹Ÿå¯å‘½ä»¤è¡Œè¿è¡Œ`/PathToMockingBird/venv/bin/pythonM1 demo_toolbox.py`\n\n### 2. å‡†å¤‡é¢„è®­ç»ƒæ¨¡å‹\nè€ƒè™‘è®­ç»ƒæ‚¨è‡ªå·±ä¸“å±çš„æ¨¡å‹æˆ–è€…ä¸‹è½½ç¤¾åŒºä»–äººè®­ç»ƒå¥½çš„æ¨¡å‹:\n> è¿‘æœŸåˆ›å»ºäº†[çŸ¥ä¹ä¸“é¢˜](https://www.zhihu.com/column/c_1425605280340504576) å°†ä¸å®šæœŸæ›´æ–°ç‚¼ä¸¹å°æŠ€å·§orå¿ƒå¾—ï¼Œä¹Ÿæ¬¢è¿æé—®\n#### 2.1 ä½¿ç”¨æ•°æ®é›†è‡ªå·±è®­ç»ƒencoderæ¨¡å‹ (å¯é€‰)\n\n* è¿›è¡ŒéŸ³é¢‘å’Œæ¢…å°”é¢‘è°±å›¾é¢„å¤„ç†ï¼š\n`python encoder_preprocess.py <datasets_root>`\nä½¿ç”¨`-d {dataset}` æŒ‡å®šæ•°æ®é›†ï¼Œæ”¯æŒ librispeech_otherï¼Œvoxceleb1ï¼Œaidatatang_200zhï¼Œä½¿ç”¨é€—å·åˆ†å‰²å¤„ç†å¤šæ•°æ®é›†ã€‚\n* è®­ç»ƒencoder: `python encoder_train.py my_run <datasets_root>/SV2TTS/encoder`\n> è®­ç»ƒencoderä½¿ç”¨äº†visdomã€‚ä½ å¯ä»¥åŠ ä¸Š`-no_visdom`ç¦ç”¨visdomï¼Œä½†æ˜¯æœ‰å¯è§†åŒ–ä¼šæ›´å¥½ã€‚åœ¨å•ç‹¬çš„å‘½ä»¤è¡Œ/è¿›ç¨‹ä¸­è¿è¡Œ\"visdom\"æ¥å¯åŠ¨visdomæœåŠ¡å™¨ã€‚\n\n#### 2.2 ä½¿ç”¨æ•°æ®é›†è‡ªå·±è®­ç»ƒåˆæˆå™¨æ¨¡å‹ï¼ˆä¸2.3äºŒé€‰ä¸€ï¼‰\n* ä¸‹è½½ æ•°æ®é›†å¹¶è§£å‹ï¼šç¡®ä¿æ‚¨å¯ä»¥è®¿é—® *train* æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰éŸ³é¢‘æ–‡ä»¶ï¼ˆå¦‚.wavï¼‰\n* è¿›è¡ŒéŸ³é¢‘å’Œæ¢…å°”é¢‘è°±å›¾é¢„å¤„ç†ï¼š\n`python pre.py <datasets_root> -d {dataset} -n {number}`\nå¯ä¼ å…¥å‚æ•°ï¼š\n* `-d {dataset}` æŒ‡å®šæ•°æ®é›†ï¼Œæ”¯æŒ aidatatang_200zh, magicdata, aishell3, data_aishell, ä¸ä¼ é»˜è®¤ä¸ºaidatatang_200zh\n* `-n {number}` æŒ‡å®šå¹¶è¡Œæ•°ï¼ŒCPU 11770k + 32GBå®æµ‹10æ²¡æœ‰é—®é¢˜\n> å‡å¦‚ä½ ä¸‹è½½çš„ `aidatatang_200zh`æ–‡ä»¶æ”¾åœ¨Dç›˜ï¼Œ`train`æ–‡ä»¶è·¯å¾„ä¸º `D:\\data\\aidatatang_200zh\\corpus\\train` , ä½ çš„`datasets_root`å°±æ˜¯ `D:\\data\\`\n\n* è®­ç»ƒåˆæˆå™¨ï¼š\n`python ./control/cli/synthesizer_train.py mandarin <datasets_root>/SV2TTS/synthesizer`\n\n* å½“æ‚¨åœ¨è®­ç»ƒæ–‡ä»¶å¤¹ *synthesizer/saved_models/* ä¸­çœ‹åˆ°æ³¨æ„çº¿æ˜¾ç¤ºå’ŒæŸå¤±æ»¡è¶³æ‚¨çš„éœ€è¦æ—¶ï¼Œè¯·è½¬åˆ°`å¯åŠ¨ç¨‹åº`ä¸€æ­¥ã€‚\n\n#### 2.3ä½¿ç”¨ç¤¾åŒºé¢„å…ˆè®­ç»ƒå¥½çš„åˆæˆå™¨ï¼ˆä¸2.2äºŒé€‰ä¸€ï¼‰\n> å½“å®åœ¨æ²¡æœ‰è®¾å¤‡æˆ–è€…ä¸æƒ³æ…¢æ…¢è°ƒè¯•ï¼Œå¯ä»¥ä½¿ç”¨ç¤¾åŒºè´¡çŒ®çš„æ¨¡å‹(æ¬¢è¿æŒç»­åˆ†äº«):\n\n| ä½œè€… | ä¸‹è½½é“¾æ¥ | æ•ˆæœé¢„è§ˆ | ä¿¡æ¯ |\n| --- | ----------- | ----- | ----- |\n| ä½œè€… | https://pan.baidu.com/s/1iONvRxmkI-t1nHqxKytY3g  [ç™¾åº¦ç›˜é“¾æ¥](https://pan.baidu.com/s/1iONvRxmkI-t1nHqxKytY3g) 4j5d |  | 75k steps ç”¨3ä¸ªå¼€æºæ•°æ®é›†æ··åˆè®­ç»ƒ\n| ä½œè€… | https://pan.baidu.com/s/1fMh9IlgKJlL2PIiRTYDUvw  [ç™¾åº¦ç›˜é“¾æ¥](https://pan.baidu.com/s/1fMh9IlgKJlL2PIiRTYDUvw) æå–ç ï¼šom7f |  | 25k steps ç”¨3ä¸ªå¼€æºæ•°æ®é›†æ··åˆè®­ç»ƒ, åˆ‡æ¢åˆ°tag v0.0.1ä½¿ç”¨\n|@FawenYo | https://yisiou-my.sharepoint.com/:u:/g/personal/lawrence_cheng_fawenyo_onmicrosoft_com/EWFWDHzee-NNg9TWdKckCc4BC7bK2j9cCbOWn0-_tK0nOg?e=n0gGgC  | [input](https://github.com/babysor/MockingBird/wiki/audio/self_test.mp3) [output](https://github.com/babysor/MockingBird/wiki/audio/export.wav) | 200k steps å°æ¹¾å£éŸ³éœ€åˆ‡æ¢åˆ°tag v0.0.1ä½¿ç”¨\n|@miven| https://pan.baidu.com/s/1PI-hM3sn5wbeChRryX-RCQ æå–ç ï¼š2021 | https://www.bilibili.com/video/BV1uh411B7AD/ | 150k steps æ³¨æ„ï¼šæ ¹æ®[issue](https://github.com/babysor/MockingBird/issues/37)ä¿®å¤ å¹¶åˆ‡æ¢åˆ°tag v0.0.1ä½¿ç”¨\n\n#### 2.4è®­ç»ƒå£°ç å™¨ (å¯é€‰)\nå¯¹æ•ˆæœå½±å“ä¸å¤§ï¼Œå·²ç»é¢„ç½®3æ¬¾ï¼Œå¦‚æœå¸Œæœ›è‡ªå·±è®­ç»ƒå¯ä»¥å‚è€ƒä»¥ä¸‹å‘½ä»¤ã€‚\n* é¢„å¤„ç†æ•°æ®:\n`python vocoder_preprocess.py <datasets_root> -m <synthesizer_model_path>`\n> `<datasets_root>`æ›¿æ¢ä¸ºä½ çš„æ•°æ®é›†ç›®å½•ï¼Œ`<synthesizer_model_path>`æ›¿æ¢ä¸ºä¸€ä¸ªä½ æœ€å¥½çš„synthesizeræ¨¡å‹ç›®å½•ï¼Œä¾‹å¦‚ *sythensizer\\saved_models\\xxx*\n\n\n* è®­ç»ƒwavernnå£°ç å™¨:\n`python ./control/cli/vocoder_train.py <trainid> <datasets_root>`\n> `<trainid>`æ›¿æ¢ä¸ºä½ æƒ³è¦çš„æ ‡è¯†ï¼ŒåŒä¸€æ ‡è¯†å†æ¬¡è®­ç»ƒæ—¶ä¼šå»¶ç»­åŸæ¨¡å‹\n\n* è®­ç»ƒhifiganå£°ç å™¨:\n`python ./control/cli/vocoder_train.py <trainid> <datasets_root> hifigan`\n> `<trainid>`æ›¿æ¢ä¸ºä½ æƒ³è¦çš„æ ‡è¯†ï¼ŒåŒä¸€æ ‡è¯†å†æ¬¡è®­ç»ƒæ—¶ä¼šå»¶ç»­åŸæ¨¡å‹\n* è®­ç»ƒfreganå£°ç å™¨:\n`python ./control/cli/vocoder_train.py <trainid> <datasets_root> --config config.json fregan`\n> `<trainid>`æ›¿æ¢ä¸ºä½ æƒ³è¦çš„æ ‡è¯†ï¼ŒåŒä¸€æ ‡è¯†å†æ¬¡è®­ç»ƒæ—¶ä¼šå»¶ç»­åŸæ¨¡å‹\n* å°†GANå£°ç å™¨çš„è®­ç»ƒåˆ‡æ¢ä¸ºå¤šGPUæ¨¡å¼ï¼šä¿®æ”¹GANæ–‡ä»¶å¤¹ä¸‹.jsonæ–‡ä»¶ä¸­çš„\"num_gpus\"å‚æ•°\n### 3. å¯åŠ¨ç¨‹åºæˆ–å·¥å…·ç®±\næ‚¨å¯ä»¥å°è¯•ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ï¼š\n\n### 3.1 å¯åŠ¨Webç¨‹åºï¼ˆv2ï¼‰ï¼š\n`python web.py`\nè¿è¡ŒæˆåŠŸååœ¨æµè§ˆå™¨æ‰“å¼€åœ°å€, é»˜è®¤ä¸º `http://localhost:8080`\n> * ä»…æ”¯æŒæ‰‹åŠ¨æ–°å½•éŸ³ï¼ˆ16khzï¼‰, ä¸æ”¯æŒè¶…è¿‡4MBçš„å½•éŸ³ï¼Œæœ€ä½³é•¿åº¦åœ¨5~15ç§’\n\n### 3.2 å¯åŠ¨å·¥å…·ç®±ï¼š\n`python demo_toolbox.py -d <datasets_root>`\n> è¯·æŒ‡å®šä¸€ä¸ªå¯ç”¨çš„æ•°æ®é›†æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœæœ‰æ”¯æŒçš„æ•°æ®é›†åˆ™ä¼šè‡ªåŠ¨åŠ è½½ä¾›è°ƒè¯•ï¼Œä¹ŸåŒæ—¶ä¼šä½œä¸ºæ‰‹åŠ¨å½•åˆ¶éŸ³é¢‘çš„å­˜å‚¨ç›®å½•ã€‚\n\n<img width=\"1042\" alt=\"d48ea37adf3660e657cfb047c10edbc\" src=\"https://user-images.githubusercontent.com/7423248/134275227-c1ddf154-f118-4b77-8949-8c4c7daf25f0.png\">\n\n### 4. ç•ªå¤–ï¼šè¯­éŸ³è½¬æ¢Voice Conversion(PPG based)\næƒ³åƒæŸ¯å—æ‹¿ç€å˜å£°å™¨ç„¶åå‘å‡ºæ¯›åˆ©å°äº”éƒçš„å£°éŸ³å—ï¼Ÿæœ¬é¡¹ç›®ç°åŸºäºPPG-VCï¼Œå¼•å…¥é¢å¤–ä¸¤ä¸ªæ¨¡å—ï¼ˆPPG extractor + PPG2Melï¼‰, å¯ä»¥å®ç°å˜å£°åŠŸèƒ½ã€‚ï¼ˆæ–‡æ¡£ä¸å…¨ï¼Œå°¤å…¶æ˜¯è®­ç»ƒéƒ¨åˆ†ï¼Œæ­£åœ¨åŠªåŠ›è¡¥å……ä¸­ï¼‰\n#### 4.0 å‡†å¤‡ç¯å¢ƒ\n* ç¡®ä¿é¡¹ç›®ä»¥ä¸Šç¯å¢ƒå·²ç»å®‰è£…okï¼Œè¿è¡Œ`pip install espnet` æ¥å®‰è£…å‰©ä½™çš„å¿…è¦åŒ…ã€‚\n* ä¸‹è½½ä»¥ä¸‹æ¨¡å‹ é“¾æ¥ï¼šhttps://pan.baidu.com/s/1bl_x_DHJSAUyN2fma-Q_Wg\næå–ç ï¼šgh41\n  * 24Ké‡‡æ ·ç‡ä¸“ç”¨çš„vocoderï¼ˆhifiganï¼‰åˆ° *vocoder\\saved_models\\xxx*\n  * é¢„è®­ç»ƒçš„ppgç‰¹å¾encoder(ppg_extractor)åˆ° *ppg_extractor\\saved_models\\xxx*\n  * é¢„è®­ç»ƒçš„PPG2Melåˆ° *ppg2mel\\saved_models\\xxx*\n\n#### 4.1 ä½¿ç”¨æ•°æ®é›†è‡ªå·±è®­ç»ƒPPG2Melæ¨¡å‹ (å¯é€‰)\n\n* ä¸‹è½½aidatatang_200zhæ•°æ®é›†å¹¶è§£å‹ï¼šç¡®ä¿æ‚¨å¯ä»¥è®¿é—® *train* æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰éŸ³é¢‘æ–‡ä»¶ï¼ˆå¦‚.wavï¼‰\n* è¿›è¡ŒéŸ³é¢‘å’Œæ¢…å°”é¢‘è°±å›¾é¢„å¤„ç†ï¼š\n`python ./control/cli/pre4ppg.py <datasets_root> -d {dataset} -n {number}`\nå¯ä¼ å…¥å‚æ•°ï¼š\n* `-d {dataset}` æŒ‡å®šæ•°æ®é›†ï¼Œæ”¯æŒ aidatatang_200zh, ä¸ä¼ é»˜è®¤ä¸ºaidatatang_200zh\n* `-n {number}` æŒ‡å®šå¹¶è¡Œæ•°ï¼ŒCPU 11700kåœ¨8çš„æƒ…å†µä¸‹ï¼Œéœ€è¦è¿è¡Œ12åˆ°18å°æ—¶ï¼å¾…ä¼˜åŒ–\n> å‡å¦‚ä½ ä¸‹è½½çš„ `aidatatang_200zh`æ–‡ä»¶æ”¾åœ¨Dç›˜ï¼Œ`train`æ–‡ä»¶è·¯å¾„ä¸º `D:\\data\\aidatatang_200zh\\corpus\\train` , ä½ çš„`datasets_root`å°±æ˜¯ `D:\\data\\`\n\n* è®­ç»ƒåˆæˆå™¨, æ³¨æ„åœ¨ä¸Šä¸€æ­¥å…ˆä¸‹è½½å¥½`ppg2mel.yaml`, ä¿®æ”¹é‡Œé¢çš„åœ°å€æŒ‡å‘é¢„è®­ç»ƒå¥½çš„æ–‡ä»¶å¤¹ï¼š\n`python ./control/cli/ppg2mel_train.py --config .\\ppg2mel\\saved_models\\ppg2mel.yaml --oneshotvc `\n* å¦‚æœæƒ³è¦ç»§ç»­ä¸Šä¸€æ¬¡çš„è®­ç»ƒï¼Œå¯ä»¥é€šè¿‡`--load .\\ppg2mel\\saved_models\\<old_pt_file>` å‚æ•°æŒ‡å®šä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹æ–‡ä»¶ã€‚\n\n#### 4.2 å¯åŠ¨å·¥å…·ç®±VCæ¨¡å¼\næ‚¨å¯ä»¥å°è¯•ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ï¼š\n`python demo_toolbox.py -vc -d <datasets_root>`\n> è¯·æŒ‡å®šä¸€ä¸ªå¯ç”¨çš„æ•°æ®é›†æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœæœ‰æ”¯æŒçš„æ•°æ®é›†åˆ™ä¼šè‡ªåŠ¨åŠ è½½ä¾›è°ƒè¯•ï¼Œä¹ŸåŒæ—¶ä¼šä½œä¸ºæ‰‹åŠ¨å½•åˆ¶éŸ³é¢‘çš„å­˜å‚¨ç›®å½•ã€‚\n<img width=\"971\" alt=\"å¾®ä¿¡å›¾ç‰‡_20220305005351\" src=\"https://user-images.githubusercontent.com/7423248/156805733-2b093dbc-d989-4e68-8609-db11f365886a.png\">\n\n## å¼•ç”¨åŠè®ºæ–‡\n> è¯¥åº“ä¸€å¼€å§‹ä»ä»…æ”¯æŒè‹±è¯­çš„[Real-Time-Voice-Cloning](https://github.com/CorentinJ/Real-Time-Voice-Cloning) åˆ†å‰å‡ºæ¥çš„ï¼Œé¸£è°¢ä½œè€…ã€‚\n\n| URL | Designation | æ ‡é¢˜ | å®ç°æºç  |\n| --- | ----------- | ----- | --------------------- |\n| [1803.09017](https://arxiv.org/abs/1803.09017) | GlobalStyleToken (synthesizer)| Style Tokens: Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesis | æœ¬ä»£ç åº“ |\n| [2010.05646](https://arxiv.org/abs/2010.05646) | HiFi-GAN (vocoder)| Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis | æœ¬ä»£ç åº“ |\n| [2106.02297](https://arxiv.org/abs/2106.02297) | Fre-GAN (vocoder)| Fre-GAN: Adversarial Frequency-consistent Audio Synthesis | æœ¬ä»£ç åº“ |\n|[**1806.04558**](https://arxiv.org/pdf/1806.04558.pdf) | SV2TTS | Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis | æœ¬ä»£ç åº“ |\n|[1802.08435](https://arxiv.org/pdf/1802.08435.pdf) | WaveRNN (vocoder) | Efficient Neural Audio Synthesis | [fatchord/WaveRNN](https://github.com/fatchord/WaveRNN) |\n|[1703.10135](https://arxiv.org/pdf/1703.10135.pdf) | Tacotron (synthesizer) | Tacotron: Towards End-to-End Speech Synthesis | [fatchord/WaveRNN](https://github.com/fatchord/WaveRNN)\n|[1710.10467](https://arxiv.org/pdf/1710.10467.pdf) | GE2E (encoder)| Generalized End-To-End Loss for Speaker Verification | æœ¬ä»£ç åº“ |\n\n## å¸¸è§é—®é¢˜(FQ&A)\n#### 1.æ•°æ®é›†åœ¨å“ªé‡Œä¸‹è½½?\n| æ•°æ®é›† | OpenSLRåœ°å€ | å…¶ä»–æº (Google Drive, Baiduç½‘ç›˜ç­‰) |\n| --- | ----------- | ---------------|\n| aidatatang_200zh | [OpenSLR](http://www.openslr.org/62/) | [Google Drive](https://drive.google.com/file/d/110A11KZoVe7vy6kXlLb6zVPLb_J91I_t/view?usp=sharing) |\n| magicdata | [OpenSLR](http://www.openslr.org/68/) | [Google Drive (Dev set)](https://drive.google.com/file/d/1g5bWRUSNH68ycC6eNvtwh07nX3QhOOlo/view?usp=sharing) |\n| aishell3 | [OpenSLR](https://www.openslr.org/93/) | [Google Drive](https://drive.google.com/file/d/1shYp_o4Z0X0cZSKQDtFirct2luFUwKzZ/view?usp=sharing) |\n| data_aishell | [OpenSLR](https://www.openslr.org/33/) |  |\n> è§£å‹ aidatatang_200zh åï¼Œè¿˜éœ€å°† `aidatatang_200zh\\corpus\\train`ä¸‹çš„æ–‡ä»¶å…¨é€‰è§£å‹ç¼©\n\n#### 2.`<datasets_root>`æ˜¯ä»€éº¼æ„æ€?\nå‡å¦‚æ•°æ®é›†è·¯å¾„ä¸º `D:\\data\\aidatatang_200zh`ï¼Œé‚£ä¹ˆ `<datasets_root>`å°±æ˜¯ `D:\\data`\n\n#### 3.è®­ç»ƒæ¨¡å‹æ˜¾å­˜ä¸è¶³\nè®­ç»ƒåˆæˆå™¨æ—¶ï¼šå°† `synthesizer/hparams.py`ä¸­çš„batch_sizeå‚æ•°è°ƒå°\n```\n//è°ƒæ•´å‰\ntts_schedule = [(2,  1e-3,  20_000,  12),   # Progressive training schedule\n                (2,  5e-4,  40_000,  12),   # (r, lr, step, batch_size)\n                (2,  2e-4,  80_000,  12),   #\n                (2,  1e-4, 160_000,  12),   # r = reduction factor (# of mel frames\n                (2,  3e-5, 320_000,  12),   #     synthesized for each decoder iteration)\n                (2,  1e-5, 640_000,  12)],  # lr = learning rate\n//è°ƒæ•´å\ntts_schedule = [(2,  1e-3,  20_000,  8),   # Progressive training schedule\n                (2,  5e-4,  40_000,  8),   # (r, lr, step, batch_size)\n                (2,  2e-4,  80_000,  8),   #\n                (2,  1e-4, 160_000,  8),   # r = reduction factor (# of mel frames\n                (2,  3e-5, 320_000,  8),   #     synthesized for each decoder iteration)\n                (2,  1e-5, 640_000,  8)],  # lr = learning rate\n```\n\nå£°ç å™¨-é¢„å¤„ç†æ•°æ®é›†æ—¶ï¼šå°† `synthesizer/hparams.py`ä¸­çš„batch_sizeå‚æ•°è°ƒå°\n```\n//è°ƒæ•´å‰\n### Data Preprocessing\n        max_mel_frames = 900,\n        rescale = True,\n        rescaling_max = 0.9,\n        synthesis_batch_size = 16,                  # For vocoder preprocessing and inference.\n//è°ƒæ•´å\n### Data Preprocessing\n        max_mel_frames = 900,\n        rescale = True,\n        rescaling_max = 0.9,\n        synthesis_batch_size = 8,                  # For vocoder preprocessing and inference.\n```\n\nå£°ç å™¨-è®­ç»ƒå£°ç å™¨æ—¶ï¼šå°† `vocoder/wavernn/hparams.py`ä¸­çš„batch_sizeå‚æ•°è°ƒå°\n```\n//è°ƒæ•´å‰\n# Training\nvoc_batch_size = 100\nvoc_lr = 1e-4\nvoc_gen_at_checkpoint = 5\nvoc_pad = 2\n\n//è°ƒæ•´å\n# Training\nvoc_batch_size = 6\nvoc_lr = 1e-4\nvoc_gen_at_checkpoint = 5\nvoc_pad =2\n```\n\n#### 4.ç¢°åˆ°`RuntimeError: Error(s) in loading state_dict for Tacotron: size mismatch for encoder.embedding.weight: copying a param with shape torch.Size([70, 512]) from checkpoint, the shape in current model is torch.Size([75, 512]).`\nè¯·å‚ç…§ issue [#37](https://github.com/babysor/MockingBird/issues/37)\n\n#### 5.å¦‚ä½•æ”¹å–„CPUã€GPUå ç”¨ç‡?\nè§†æƒ…å†µè°ƒæ•´batch_sizeå‚æ•°æ¥æ”¹å–„\n\n#### 6.å‘ç”Ÿ `é¡µé¢æ–‡ä»¶å¤ªå°ï¼Œæ— æ³•å®Œæˆæ“ä½œ`\nè¯·å‚è€ƒè¿™ç¯‡[æ–‡ç« ](https://blog.csdn.net/qq_17755303/article/details/112564030)ï¼Œå°†è™šæ‹Ÿå†…å­˜æ›´æ”¹ä¸º100G(102400)ï¼Œä¾‹å¦‚:æ–‡ä»¶æ”¾ç½®Dç›˜å°±æ›´æ”¹Dç›˜çš„è™šæ‹Ÿå†…å­˜\n\n#### 7.ä»€ä¹ˆæ—¶å€™ç®—è®­ç»ƒå®Œæˆï¼Ÿ\né¦–å…ˆä¸€å®šè¦å‡ºç°æ³¨æ„åŠ›æ¨¡å‹ï¼Œå…¶æ¬¡æ˜¯lossè¶³å¤Ÿä½ï¼Œå–å†³äºç¡¬ä»¶è®¾å¤‡å’Œæ•°æ®é›†ã€‚æ‹¿æœ¬äººçš„ä¾›å‚è€ƒï¼Œæˆ‘çš„æ³¨æ„åŠ›æ˜¯åœ¨ 18k æ­¥ä¹‹åå‡ºç°çš„ï¼Œå¹¶ä¸”åœ¨ 50k æ­¥ä¹‹åæŸå¤±å˜å¾—ä½äº 0.4\n![attention_step_20500_sample_1](https://user-images.githubusercontent.com/7423248/128587252-f669f05a-f411-4811-8784-222156ea5e9d.png)\n\n![step-135500-mel-spectrogram_sample_1](https://user-images.githubusercontent.com/7423248/128587255-4945faa0-5517-46ea-b173-928eff999330.png)\n"
        },
        {
          "name": "README-LINUX-CN.md",
          "type": "blob",
          "size": 8.83,
          "content": "## å®æ—¶è¯­éŸ³å…‹éš† - ä¸­æ–‡/æ™®é€šè¯\n![mockingbird](https://user-images.githubusercontent.com/12797292/131216767-6eb251d6-14fc-4951-8324-2722f0cd4c63.jpg)\n\n[![MIT License](https://img.shields.io/badge/license-MIT-blue.svg?style=flat)](http://choosealicense.com/licenses/mit/)\n\n### [English](README.md)  | ä¸­æ–‡\n\n### [DEMO VIDEO](https://www.bilibili.com/video/BV17Q4y1B7mY/) | [Wikiæ•™ç¨‹](https://github.com/babysor/MockingBird/wiki/Quick-Start-(Newbie)) ï½œ [è®­ç»ƒæ•™ç¨‹](https://vaj2fgg8yn.feishu.cn/docs/doccn7kAbr3SJz0KM0SIDJ0Xnhd)\n\n## ç‰¹æ€§\nğŸŒ **ä¸­æ–‡** æ”¯æŒæ™®é€šè¯å¹¶ä½¿ç”¨å¤šç§ä¸­æ–‡æ•°æ®é›†è¿›è¡Œæµ‹è¯•ï¼šaidatatang_200zh, magicdata, aishell3, biaobei, MozillaCommonVoice, data_aishell ç­‰\n\nğŸ¤© **Easy & Awesome** ä»…éœ€ä¸‹è½½æˆ–æ–°è®­ç»ƒåˆæˆå™¨ï¼ˆsynthesizerï¼‰å°±æœ‰è‰¯å¥½æ•ˆæœï¼Œå¤ç”¨é¢„è®­ç»ƒçš„ç¼–ç å™¨/å£°ç å™¨ï¼Œæˆ–å®æ—¶çš„HiFi-GANä½œä¸ºvocoder\n\nğŸŒ **Webserver Ready** å¯ä¼ºæœä½ çš„è®­ç»ƒç»“æœï¼Œä¾›è¿œç¨‹è°ƒç”¨ã€‚\n\nğŸ¤© **æ„Ÿè°¢å„ä½å°ä¼™ä¼´çš„æ”¯æŒï¼Œæœ¬é¡¹ç›®å°†å¼€å¯æ–°ä¸€è½®çš„æ›´æ–°**\n\n## 1.å¿«é€Ÿå¼€å§‹\n### 1.1 å»ºè®®ç¯å¢ƒ\n- Ubuntu 18.04 \n- Cuda 11.7 && CuDNN 8.5.0 \n- Python 3.8 æˆ– 3.9 \n- Pytorch 2.0.1 <post cuda-11.7>\n### 1.2 ç¯å¢ƒé…ç½®\n```shell\n# ä¸‹è½½å‰å»ºè®®æ›´æ¢å›½å†…é•œåƒæº\n\nconda create -n sound python=3.9\n\nconda activate sound\n\ngit clone https://github.com/babysor/MockingBird.git\n\ncd MockingBird\n\npip install -r requirements.txt\n\npip install webrtcvad-wheels\n\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117\n```\n\n### 1.3 æ¨¡å‹å‡†å¤‡\n> å½“å®åœ¨æ²¡æœ‰è®¾å¤‡æˆ–è€…ä¸æƒ³æ…¢æ…¢è°ƒè¯•ï¼Œå¯ä»¥ä½¿ç”¨ç¤¾åŒºè´¡çŒ®çš„æ¨¡å‹(æ¬¢è¿æŒç»­åˆ†äº«):\n\n| ä½œè€… | ä¸‹è½½é“¾æ¥ | æ•ˆæœé¢„è§ˆ | ä¿¡æ¯ |\n| --- | ----------- | ----- | ----- |\n| ä½œè€… | https://pan.baidu.com/s/1iONvRxmkI-t1nHqxKytY3g  [ç™¾åº¦ç›˜é“¾æ¥](https://pan.baidu.com/s/1iONvRxmkI-t1nHqxKytY3g) 4j5d |  | 75k steps ç”¨3ä¸ªå¼€æºæ•°æ®é›†æ··åˆè®­ç»ƒ\n| ä½œè€… | https://pan.baidu.com/s/1fMh9IlgKJlL2PIiRTYDUvw  [ç™¾åº¦ç›˜é“¾æ¥](https://pan.baidu.com/s/1fMh9IlgKJlL2PIiRTYDUvw) æå–ç ï¼šom7f |  | 25k steps ç”¨3ä¸ªå¼€æºæ•°æ®é›†æ··åˆè®­ç»ƒ, åˆ‡æ¢åˆ°tag v0.0.1ä½¿ç”¨\n|@FawenYo | https://drive.google.com/file/d/1H-YGOUHpmqKxJ9FRc6vAjPuqQki24UbC/view?usp=sharing [ç™¾åº¦ç›˜é“¾æ¥](https://pan.baidu.com/s/1vSYXO4wsLyjnF3Unl-Xoxg) æå–ç ï¼š1024  | [input](https://github.com/babysor/MockingBird/wiki/audio/self_test.mp3) [output](https://github.com/babysor/MockingBird/wiki/audio/export.wav) | 200k steps å°æ¹¾å£éŸ³éœ€åˆ‡æ¢åˆ°tag v0.0.1ä½¿ç”¨\n|@miven| https://pan.baidu.com/s/1PI-hM3sn5wbeChRryX-RCQ æå–ç ï¼š2021 | https://www.bilibili.com/video/BV1uh411B7AD/ | 150k steps æ³¨æ„ï¼šæ ¹æ®[issue](https://github.com/babysor/MockingBird/issues/37)ä¿®å¤ å¹¶åˆ‡æ¢åˆ°tag v0.0.1ä½¿ç”¨\n\n### 1.4 æ–‡ä»¶ç»“æ„å‡†å¤‡\næ–‡ä»¶ç»“æ„å‡†å¤‡å¦‚ä¸‹æ‰€ç¤ºï¼Œç®—æ³•å°†è‡ªåŠ¨éå†synthesizerä¸‹çš„.ptæ¨¡å‹æ–‡ä»¶ã€‚\n``` \n#  ä»¥ç¬¬ä¸€ä¸ª pretrained-11-7-21_75k.pt ä¸ºä¾‹\n\nâ””â”€â”€ data\n      â””â”€â”€ ckpt \n            â””â”€â”€ synthesizer \n                     â””â”€â”€ pretrained-11-7-21_75k.pt\n```\n### 1.5 è¿è¡Œ\n```\npython web.py \n```\n\n## 2.æ¨¡å‹è®­ç»ƒ\n### 2.1 æ•°æ®å‡†å¤‡\n#### 2.1.1 æ•°æ®ä¸‹è½½\n``` shell\n# aidatatang_200zh \n \nwget https://openslr.elda.org/resources/62/aidatatang_200zh.tgz\n```\n``` shell\n# MAGICDATA  \n\nwget https://openslr.magicdatatech.com/resources/68/train_set.tar.gz\n\nwget https://openslr.magicdatatech.com/resources/68/dev_set.tar.gz\n\nwget https://openslr.magicdatatech.com/resources/68/test_set.tar.gz\n```\n``` shell\n# AISHELL-3 \n\nwget https://openslr.elda.org/resources/93/data_aishell3.tgz\n```\n```shell\n# Aishell  \n\nwget https://openslr.elda.org/resources/33/data_aishell.tgz\n```\n#### 2.1.2 æ•°æ®æ‰¹é‡è§£å‹\n```shell\n# è¯¥æŒ‡ä»¤ä¸ºè§£å‹å½“å‰ç›®å½•ä¸‹çš„æ‰€æœ‰å‹ç¼©æ–‡ä»¶ \n\nfor gz in *.gz; do tar -zxvf $gz; done\n```\n### 2.2 encoderæ¨¡å‹è®­ç»ƒ\n#### 2.2.1 æ•°æ®é¢„å¤„ç†ï¼š\néœ€è¦å…ˆåœ¨`pre.py `å¤´éƒ¨åŠ å…¥ï¼š\n```python\nimport torch\ntorch.multiprocessing.set_start_method('spawn', force=True)\n```\nä½¿ç”¨ä»¥ä¸‹æŒ‡ä»¤å¯¹æ•°æ®é¢„å¤„ç†ï¼š\n```shell\npython pre.py <datasets_root> \\\n           -d <datasets_name> \n```\nå…¶ä¸­`<datasets_root>`ä¸ºåŸæ•°æ®é›†è·¯å¾„ï¼Œ`<datasets_name>` ä¸ºæ•°æ®é›†åç§°ã€‚\n\næ”¯æŒ `librispeech_other`ï¼Œ`voxceleb1`ï¼Œ`aidatatang_200zh`ï¼Œä½¿ç”¨é€—å·åˆ†å‰²å¤„ç†å¤šæ•°æ®é›†ã€‚\n\n### 2.2.2 encoderæ¨¡å‹è®­ç»ƒï¼š\nè¶…å‚æ•°æ–‡ä»¶è·¯å¾„ï¼š`models/encoder/hparams.py`\n```shell\npython encoder_train.py <name> \\\n                        <datasets_root>/SV2TTS/encoder\n```\nå…¶ä¸­ `<name>` æ˜¯è®­ç»ƒäº§ç”Ÿæ–‡ä»¶çš„åç§°ï¼Œå¯è‡ªè¡Œä¿®æ”¹ã€‚\n\nå…¶ä¸­ `<datasets_root>` æ˜¯ç»è¿‡ `Step 2.1.1` å¤„ç†è¿‡åçš„æ•°æ®é›†è·¯å¾„ã€‚\n#### 2.2.3 å¼€å¯encoderæ¨¡å‹è®­ç»ƒæ•°æ®å¯è§†åŒ–ï¼ˆå¯é€‰ï¼‰\n```shell\nvisdom\n```\n\n### 2.3 synthesizeræ¨¡å‹è®­ç»ƒ\n#### 2.3.1 æ•°æ®é¢„å¤„ç†ï¼š\n```shell\npython pre.py    <datasets_root> \\\n              -d <datasets_name> \\\n              -o <datasets_path> \\\n              -n <number>\n```\n`<datasets_root>` ä¸ºåŸæ•°æ®é›†è·¯å¾„ï¼Œå½“ä½ çš„`aidatatang_200zh`è·¯å¾„ä¸º`/data/aidatatang_200zh/corpus/train`æ—¶ï¼Œ`<datasets_root>` ä¸º `/data/`ã€‚\n \n`<datasets_name>` ä¸ºæ•°æ®é›†åç§°ã€‚\n \n`<datasets_path>` ä¸ºæ•°æ®é›†å¤„ç†åçš„ä¿å­˜è·¯å¾„ã€‚\n\n`<number>` ä¸ºæ•°æ®é›†å¤„ç†æ—¶è¿›ç¨‹æ•°ï¼Œæ ¹æ®CPUæƒ…å†µè°ƒæ•´å¤§å°ã€‚\n\n#### 2.3.2 æ–°å¢æ•°æ®é¢„å¤„ç†ï¼š\n```shell\npython pre.py    <datasets_root> \\\n              -d <datasets_name> \\\n              -o <datasets_path> \\\n              -n <number> \\\n              -s\n```\nå½“æ–°å¢æ•°æ®é›†æ—¶ï¼Œåº”åŠ  `-s` é€‰æ‹©æ•°æ®æ‹¼æ¥ï¼Œä¸åŠ åˆ™ä¸ºè¦†ç›–ã€‚\n#### 2.3.3 synthesizeræ¨¡å‹è®­ç»ƒï¼š\nè¶…å‚æ•°æ–‡ä»¶è·¯å¾„ï¼š`models/synthesizer/hparams.py`ï¼Œéœ€å°†`MockingBird/control/cli/synthesizer_train.py`ç§»æˆ`MockingBird/synthesizer_train.py`ç»“æ„ã€‚\n```shell\npython synthesizer_train.py <name> <datasets_path> \\\n                                -m <out_dir>\n```\nå…¶ä¸­ `<name>` æ˜¯è®­ç»ƒäº§ç”Ÿæ–‡ä»¶çš„åç§°ï¼Œå¯è‡ªè¡Œä¿®æ”¹ã€‚\n\nå…¶ä¸­ `<datasets_path>` æ˜¯ç»è¿‡ `Step 2.2.1` å¤„ç†è¿‡åçš„æ•°æ®é›†è·¯å¾„ã€‚\n\nå…¶ä¸­ `<out_dir> `ä¸ºè®­ç»ƒæ—¶æ‰€æœ‰æ•°æ®çš„ä¿å­˜è·¯å¾„ã€‚\n\n### 2.4 vocoderæ¨¡å‹è®­ç»ƒ\nvocoderæ¨¡å‹å¯¹ç”Ÿæˆæ•ˆæœå½±å“ä¸å¤§ï¼Œå·²é¢„ç½®3æ¬¾ã€‚\n#### 2.4.1 æ•°æ®é¢„å¤„ç†\n```shell\npython vocoder_preprocess.py <datasets_root> \\\n                          -m <synthesizer_model_path>\n```\n\nå…¶ä¸­`<datasets_root>`ä¸ºä½ æ•°æ®é›†è·¯å¾„ã€‚\n\nå…¶ä¸­ `<synthesizer_model_path>`ä¸ºsynthesizeræ¨¡å‹åœ°å€ã€‚\n\n#### 2.4.2 wavernnå£°ç å™¨è®­ç»ƒ:\n```\npython vocoder_train.py <name> <datasets_root>\n```\n#### 2.4.3 hifiganå£°ç å™¨è®­ç»ƒ:\n```\npython vocoder_train.py <name> <datasets_root> hifigan\n```\n#### 2.4.4 freganå£°ç å™¨è®­ç»ƒ:\n```\npython vocoder_train.py <name> <datasets_root> \\\n                        --config config.json fregan\n```\nå°†GANå£°ç å™¨çš„è®­ç»ƒåˆ‡æ¢ä¸ºå¤šGPUæ¨¡å¼ï¼šä¿®æ”¹`GAN`æ–‡ä»¶å¤¹ä¸‹`.json`æ–‡ä»¶ä¸­çš„`num_gpus`å‚æ•°ã€‚\n\n\n\n\n\n## 3.è‡´è°¢\n### 3.1 é¡¹ç›®è‡´è°¢\nè¯¥åº“ä¸€å¼€å§‹ä»ä»…æ”¯æŒè‹±è¯­çš„[Real-Time-Voice-Cloning](https://github.com/CorentinJ/Real-Time-Voice-Cloning) åˆ†å‰å‡ºæ¥çš„ï¼Œé¸£è°¢ä½œè€…ã€‚\n### 3.2 è®ºæ–‡è‡´è°¢\n| URL | Designation | æ ‡é¢˜ | å®ç°æºç  |\n| --- | ----------- | ----- | --------------------- |\n| [1803.09017](https://arxiv.org/abs/1803.09017) | GlobalStyleToken (synthesizer)| Style Tokens: Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesis | æœ¬ä»£ç åº“ |\n| [2010.05646](https://arxiv.org/abs/2010.05646) | HiFi-GAN (vocoder)| Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis | æœ¬ä»£ç åº“ |\n| [2106.02297](https://arxiv.org/abs/2106.02297) | Fre-GAN (vocoder)| Fre-GAN: Adversarial Frequency-consistent Audio Synthesis | æœ¬ä»£ç åº“ |\n|[**1806.04558**](https://arxiv.org/pdf/1806.04558.pdf) | SV2TTS | Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis | æœ¬ä»£ç åº“ |\n|[1802.08435](https://arxiv.org/pdf/1802.08435.pdf) | WaveRNN (vocoder) | Efficient Neural Audio Synthesis | [fatchord/WaveRNN](https://github.com/fatchord/WaveRNN) |\n|[1703.10135](https://arxiv.org/pdf/1703.10135.pdf) | Tacotron (synthesizer) | Tacotron: Towards End-to-End Speech Synthesis | [fatchord/WaveRNN](https://github.com/fatchord/WaveRNN)\n|[1710.10467](https://arxiv.org/pdf/1710.10467.pdf) | GE2E (encoder)| Generalized End-To-End Loss for Speaker Verification | æœ¬ä»£ç åº“ |\n\n### 3.3 å¼€å‘è€…è‡´è°¢\n\nä½œä¸ºAIé¢†åŸŸçš„ä»ä¸šè€…ï¼Œæˆ‘ä»¬ä¸ä»…ä¹äºå¼€å‘ä¸€äº›å…·æœ‰é‡Œç¨‹ç¢‘æ„ä¹‰çš„ç®—æ³•é¡¹ç›®ï¼ŒåŒæ—¶ä¹Ÿä¹äºåˆ†äº«é¡¹ç›®ä»¥åŠå¼€å‘è¿‡ç¨‹ä¸­æ”¶è·çš„å–œæ‚¦ã€‚\n\nå› æ­¤ï¼Œä½ ä»¬çš„ä½¿ç”¨æ˜¯å¯¹æˆ‘ä»¬é¡¹ç›®çš„æœ€å¤§è®¤å¯ã€‚åŒæ—¶å½“ä½ ä»¬åœ¨é¡¹ç›®ä½¿ç”¨ä¸­é‡åˆ°ä¸€äº›é—®é¢˜æ—¶ï¼Œæ¬¢è¿ä½ ä»¬éšæ—¶åœ¨issueä¸Šç•™è¨€ã€‚ä½ ä»¬çš„æŒ‡æ­£è¿™å¯¹äºé¡¹ç›®çš„åç»­ä¼˜åŒ–å…·æœ‰ååˆ†é‡å¤§çš„çš„æ„ä¹‰ã€‚\n\nä¸ºäº†è¡¨ç¤ºæ„Ÿè°¢ï¼Œæˆ‘ä»¬å°†åœ¨æœ¬é¡¹ç›®ä¸­ç•™ä¸‹å„ä½å¼€å‘è€…ä¿¡æ¯ä»¥åŠç›¸å¯¹åº”çš„è´¡çŒ®ã€‚\n\n- ------------------------------------------------  å¼€ å‘ è€… è´¡ çŒ® å†… å®¹  ---------------------------------------------------------------------------------\n\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 14.78,
          "content": "> ğŸš§ While I no longer actively update this repo, you can find me continuously pushing this tech forward to good side and open-source. I'm also building an optimized and cloud hosted version: https://noiz.ai/ and it's free but not ready for commersial use now.\n>\n![mockingbird](https://user-images.githubusercontent.com/12797292/131216767-6eb251d6-14fc-4951-8324-2722f0cd4c63.jpg)\n\n\n[![MIT License](https://img.shields.io/badge/license-MIT-blue.svg?style=flat)](http://choosealicense.com/licenses/mit/)\n\n> English | [ä¸­æ–‡](README-CN.md)| [ä¸­æ–‡Linux](README-LINUX-CN.md)\n\n## Features\nğŸŒ **Chinese** supported mandarin and tested with multiple datasets: aidatatang_200zh, magicdata, aishell3, data_aishell, and etc.\n\nğŸ¤© **PyTorch** worked for pytorch, tested in version of 1.9.0(latest in August 2021), with GPU Tesla T4 and GTX 2060\n\nğŸŒ **Windows + Linux** run in both Windows OS and linux OS (even in M1 MACOS)\n\nğŸ¤© **Easy & Awesome** effect with only newly-trained synthesizer, by reusing the pretrained encoder/vocoder\n\nğŸŒ **Webserver Ready** to serve your result with remote calling\n\n### [DEMO VIDEO](https://www.bilibili.com/video/BV17Q4y1B7mY/)\n\n## Quick Start\n\n### 1. Install Requirements\n#### 1.1 General Setup\n> Follow the original repo to test if you got all environment ready.\n**Python 3.7 or higher ** is needed to run the toolbox.\n\n* Install [PyTorch](https://pytorch.org/get-started/locally/).\n> If you get an `ERROR: Could not find a version that satisfies the requirement torch==1.9.0+cu102 (from versions: 0.1.2, 0.1.2.post1, 0.1.2.post2 )` This error is probably due to a low version of python, try using 3.9 and it will install successfully\n* Install [ffmpeg](https://ffmpeg.org/download.html#get-packages).\n* Run `pip install -r requirements.txt` to install the remaining necessary packages.\n> The recommended environment here is `Repo Tag 0.0.1` `Pytorch1.9.0 with Torchvision0.10.0 and cudatoolkit10.2` `requirements.txt` `webrtcvad-wheels` because `requirements. txt` was exported a few months ago, so it doesn't work with newer versions\n* Install webrtcvad `pip install webrtcvad-wheels`(If you need)\n\nor\n- install dependencies withÂ `conda`Â orÂ `mamba`\n\n  ```conda env create -n env_name -f env.yml```\n\n  ```mamba env create -n env_name -f env.yml```\n\n  will create a virtual environment where necessary dependencies are installed. Switch to the new environment byÂ `conda activate env_name`Â and enjoy it.\n  > env.yml only includes the necessary dependencies to run the projectï¼Œtemporarily without monotonic-align. You can check the official website to install the GPU version of pytorch.\n\n#### 1.2 Setup with a M1 Mac\n> The following steps are a workaround to directly use the original `demo_toolbox.py`without the changing of codes.\n>\n  >  Since the major issue comes with the PyQt5 packages used in `demo_toolbox.py` not compatible with M1 chips, were one to attempt on training models with the M1 chip, either that person can forgo `demo_toolbox.py`, or one can try the `web.py` in the project.\n\n##### 1.2.1 Install `PyQt5`, with [ref](https://stackoverflow.com/a/68038451/20455983) here.\n  * Create and open a Rosetta Terminal, with [ref](https://dev.to/courier/tips-and-tricks-to-setup-your-apple-m1-for-development-547g) here.\n  * Use system Python to create a virtual environment for the project\n    ```\n    /usr/bin/python3 -m venv /PathToMockingBird/venv\n    source /PathToMockingBird/venv/bin/activate\n    ```\n  * Upgrade pip and install `PyQt5`\n    ```\n    pip install --upgrade pip\n    pip install pyqt5\n    ```\n##### 1.2.2 Install `pyworld` and `ctc-segmentation`\n\n> Both packages seem to be unique to this project and are not seen in the original [Real-Time Voice Cloning](https://github.com/CorentinJ/Real-Time-Voice-Cloning) project. When installing with `pip install`, both packages lack wheels so the program tries to directly compile from c code and could not find `Python.h`.\n\n  * Install `pyworld`\n      * `brew install python` `Python.h` can come with Python installed by brew\n      * `export CPLUS_INCLUDE_PATH=/opt/homebrew/Frameworks/Python.framework/Headers` The filepath of brew-installed `Python.h` is unique to M1 MacOS and listed above. One needs to manually add the path to the environment variables.\n      * `pip install pyworld` that should do.\n\n\n  * Install`ctc-segmentation`\n    > Same method does not apply to `ctc-segmentation`, and one needs to compile it from the source code on [github](https://github.com/lumaku/ctc-segmentation).\n    * `git clone https://github.com/lumaku/ctc-segmentation.git`\n    * `cd ctc-segmentation`\n    * `source /PathToMockingBird/venv/bin/activate` If the virtual environment hasn't been deployed, activate it.\n    * `cythonize -3 ctc_segmentation/ctc_segmentation_dyn.pyx`\n    * `/usr/bin/arch -x86_64 python setup.py build` Build with x86 architecture.\n    * `/usr/bin/arch -x86_64 python setup.py install --optimize=1 --skip-build`Install with x86 architecture.\n\n##### 1.2.3 Other dependencies\n  * `/usr/bin/arch -x86_64 pip install torch torchvision torchaudio` Pip installing `PyTorch` as an example, articulate that it's installed with x86 architecture\n  * `pip install ffmpeg`  Install ffmpeg\n  * `pip install -r requirements.txt` Install other requirements.\n\n##### 1.2.4 Run the Inference Time (with Toolbox)\n  > To run the project on x86 architecture. [ref](https://youtrack.jetbrains.com/issue/PY-46290/Allow-running-Python-under-Rosetta-2-in-PyCharm-for-Apple-Silicon).\n  * `vim /PathToMockingBird/venv/bin/pythonM1` Create an executable file `pythonM1` to condition python interpreter at `/PathToMockingBird/venv/bin`.\n  * Write in the following content:\n    ```\n    #!/usr/bin/env zsh\n    mydir=${0:a:h}\n    /usr/bin/arch -x86_64 $mydir/python \"$@\"\n    ```\n  * `chmod +x pythonM1` Set the file as executable.\n  * If using PyCharm IDE, configure project interpreter to `pythonM1`([steps here](https://www.jetbrains.com/help/pycharm/configuring-python-interpreter.html#add-existing-interpreter)), if using command line python, run `/PathToMockingBird/venv/bin/pythonM1 demo_toolbox.py`\n\n\n### 2. Prepare your models\n> Note that we are using the pretrained encoder/vocoder but not synthesizer, since the original model is incompatible with the Chinese symbols. It means the demo_cli is not working at this moment, so additional synthesizer models are required.\n\nYou can either train your models or use existing ones:\n\n#### 2.1 Train encoder with your dataset (Optional)\n\n* Preprocess with the audios and the mel spectrograms:\n`python encoder_preprocess.py <datasets_root>` Allowing parameter `--dataset {dataset}` to support the datasets you want to preprocess. Only the train set of these datasets will be used. Possible names: librispeech_other, voxceleb1, voxceleb2. Use comma to sperate multiple datasets.\n\n* Train the encoder: `python encoder_train.py my_run <datasets_root>/SV2TTS/encoder`\n> For training, the encoder uses visdom. You can disable it with `--no_visdom`, but it's nice to have. Run \"visdom\" in a separate CLI/process to start your visdom server.\n\n#### 2.2 Train synthesizer with your dataset\n* Download dataset and unzip: make sure you can access all .wav in folder\n* Preprocess with the audios and the mel spectrograms:\n`python pre.py <datasets_root>`\nAllowing parameter `--dataset {dataset}` to support aidatatang_200zh, magicdata, aishell3, data_aishell, etc.If this parameter is not passed, the default dataset will be aidatatang_200zh.\n\n* Train the synthesizer:\n`python train.py --type=synth mandarin <datasets_root>/SV2TTS/synthesizer`\n\n* Go to next step when you see attention line show and loss meet your need in training folder *synthesizer/saved_models/*.\n\n#### 2.3 Use pretrained model of synthesizer\n> Thanks to the community, some models will be shared:\n\n| author | Download link | Preview Video | Info |\n| --- | ----------- | ----- |----- |\n| @author | https://pan.baidu.com/s/1iONvRxmkI-t1nHqxKytY3g  [Baidu](https://pan.baidu.com/s/1iONvRxmkI-t1nHqxKytY3g) 4j5d  |  | 75k steps trained by multiple datasets\n| @author | https://pan.baidu.com/s/1fMh9IlgKJlL2PIiRTYDUvw  [Baidu](https://pan.baidu.com/s/1fMh9IlgKJlL2PIiRTYDUvw) codeï¼šom7f  |  | 25k steps trained by multiple datasets, only works under version 0.0.1\n|@FawenYo | https://yisiou-my.sharepoint.com/:u:/g/personal/lawrence_cheng_fawenyo_onmicrosoft_com/EWFWDHzee-NNg9TWdKckCc4BC7bK2j9cCbOWn0-_tK0nOg?e=n0gGgC  | [input](https://github.com/babysor/MockingBird/wiki/audio/self_test.mp3) [output](https://github.com/babysor/MockingBird/wiki/audio/export.wav) | 200k steps with local accent of Taiwan, only works under version 0.0.1\n|@miven| https://pan.baidu.com/s/1PI-hM3sn5wbeChRryX-RCQ code: 2021 https://www.aliyundrive.com/s/AwPsbo8mcSP code: z2m0 | https://www.bilibili.com/video/BV1uh411B7AD/ | only works under version 0.0.1\n\n#### 2.4 Train vocoder (Optional)\n> note: vocoder has little difference in effect, so you may not need to train a new one.\n* Preprocess the data:\n`python vocoder_preprocess.py <datasets_root> -m <synthesizer_model_path>`\n> `<datasets_root>` replace with your dataset rootï¼Œ`<synthesizer_model_path>`replace with directory of your best trained models of sythensizer, e.g. *sythensizer\\saved_mode\\xxx*\n\n* Train the wavernn vocoder:\n`python vocoder_train.py mandarin <datasets_root>`\n\n* Train the hifigan vocoder\n`python vocoder_train.py mandarin <datasets_root> hifigan`\n\n### 3. Launch\n#### 3.1 Using the web server\nYou can then try to run:`python web.py` and open it in browser, default as `http://localhost:8080`\n\n#### 3.2 Using the Toolbox\nYou can then try the toolbox:\n`python demo_toolbox.py -d <datasets_root>`\n\n#### 3.3 Using the command line\nYou can then try the command:\n`python gen_voice.py <text_file.txt> your_wav_file.wav`\nyou may need to install cn2an by \"pip install cn2an\" for better digital number result.\n\n## Reference\n> This repository is forked from [Real-Time-Voice-Cloning](https://github.com/CorentinJ/Real-Time-Voice-Cloning) which only support English.\n\n| URL | Designation | Title | Implementation source |\n| --- | ----------- | ----- | --------------------- |\n| [1803.09017](https://arxiv.org/abs/1803.09017) | GlobalStyleToken (synthesizer)| Style Tokens: Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesis | This repo |\n| [2010.05646](https://arxiv.org/abs/2010.05646) | HiFi-GAN (vocoder)| Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis | This repo |\n| [2106.02297](https://arxiv.org/abs/2106.02297) | Fre-GAN (vocoder)| Fre-GAN: Adversarial Frequency-consistent Audio Synthesis | This repo |\n|[**1806.04558**](https://arxiv.org/pdf/1806.04558.pdf) | **SV2TTS** | **Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis** | This repo |\n|[1802.08435](https://arxiv.org/pdf/1802.08435.pdf) | WaveRNN (vocoder) | Efficient Neural Audio Synthesis | [fatchord/WaveRNN](https://github.com/fatchord/WaveRNN) |\n|[1703.10135](https://arxiv.org/pdf/1703.10135.pdf) | Tacotron (synthesizer) | Tacotron: Towards End-to-End Speech Synthesis | [fatchord/WaveRNN](https://github.com/fatchord/WaveRNN)\n|[1710.10467](https://arxiv.org/pdf/1710.10467.pdf) | GE2E (encoder)| Generalized End-To-End Loss for Speaker Verification | This repo |\n\n## F Q&A\n#### 1.Where can I download the dataset?\n| Dataset | Original Source | Alternative Sources |\n| --- | ----------- | ---------------|\n| aidatatang_200zh | [OpenSLR](http://www.openslr.org/62/) | [Google Drive](https://drive.google.com/file/d/110A11KZoVe7vy6kXlLb6zVPLb_J91I_t/view?usp=sharing) |\n| magicdata | [OpenSLR](http://www.openslr.org/68/) | [Google Drive (Dev set)](https://drive.google.com/file/d/1g5bWRUSNH68ycC6eNvtwh07nX3QhOOlo/view?usp=sharing) |\n| aishell3 | [OpenSLR](https://www.openslr.org/93/) | [Google Drive](https://drive.google.com/file/d/1shYp_o4Z0X0cZSKQDtFirct2luFUwKzZ/view?usp=sharing) |\n| data_aishell | [OpenSLR](https://www.openslr.org/33/) |  |\n> After unzip aidatatang_200zh, you need to unzip all the files under `aidatatang_200zh\\corpus\\train`\n\n#### 2.What is`<datasets_root>`?\nIf the dataset path is `D:\\data\\aidatatang_200zh`,then `<datasets_root>` is`D:\\data`\n\n#### 3.Not enough VRAM\nTrain the synthesizerï¼šadjust the batch_size in `synthesizer/hparams.py`\n```\n//Before\ntts_schedule = [(2,  1e-3,  20_000,  12),   # Progressive training schedule\n                (2,  5e-4,  40_000,  12),   # (r, lr, step, batch_size)\n                (2,  2e-4,  80_000,  12),   #\n                (2,  1e-4, 160_000,  12),   # r = reduction factor (# of mel frames\n                (2,  3e-5, 320_000,  12),   #     synthesized for each decoder iteration)\n                (2,  1e-5, 640_000,  12)],  # lr = learning rate\n//After\ntts_schedule = [(2,  1e-3,  20_000,  8),   # Progressive training schedule\n                (2,  5e-4,  40_000,  8),   # (r, lr, step, batch_size)\n                (2,  2e-4,  80_000,  8),   #\n                (2,  1e-4, 160_000,  8),   # r = reduction factor (# of mel frames\n                (2,  3e-5, 320_000,  8),   #     synthesized for each decoder iteration)\n                (2,  1e-5, 640_000,  8)],  # lr = learning rate\n```\n\nTrain Vocoder-Preprocess the dataï¼šadjust the batch_size in `synthesizer/hparams.py`\n```\n//Before\n### Data Preprocessing\n        max_mel_frames = 900,\n        rescale = True,\n        rescaling_max = 0.9,\n        synthesis_batch_size = 16,                  # For vocoder preprocessing and inference.\n//After\n### Data Preprocessing\n        max_mel_frames = 900,\n        rescale = True,\n        rescaling_max = 0.9,\n        synthesis_batch_size = 8,                  # For vocoder preprocessing and inference.\n```\n\nTrain Vocoder-Train the vocoderï¼šadjust the batch_size in `vocoder/wavernn/hparams.py`\n```\n//Before\n# Training\nvoc_batch_size = 100\nvoc_lr = 1e-4\nvoc_gen_at_checkpoint = 5\nvoc_pad = 2\n\n//After\n# Training\nvoc_batch_size = 6\nvoc_lr = 1e-4\nvoc_gen_at_checkpoint = 5\nvoc_pad =2\n```\n\n#### 4.If it happens `RuntimeError: Error(s) in loading state_dict for Tacotron: size mismatch for encoder.embedding.weight: copying a param with shape torch.Size([70, 512]) from checkpoint, the shape in current model is torch.Size([75, 512]).`\nPlease refer to issue [#37](https://github.com/babysor/MockingBird/issues/37)\n\n#### 5. How to improve CPU and GPU occupancy rate?\nAdjust the batch_size as appropriate to improve\n\n\n#### 6. What if it happens `the page file is too small to complete the operation`\nPlease refer to this [video](https://www.youtube.com/watch?v=Oh6dga-Oy10&ab_channel=CodeProf) and change the virtual memory to 100G (102400), for example : When the file is placed in the D disk, the virtual memory of the D disk is changed.\n\n#### 7. When should I stop during training?\nFYI, my attention came after 18k steps and loss became lower than 0.4 after 50k steps.\n![attention_step_20500_sample_1](https://user-images.githubusercontent.com/7423248/128587252-f669f05a-f411-4811-8784-222156ea5e9d.png)\n![step-135500-mel-spectrogram_sample_1](https://user-images.githubusercontent.com/7423248/128587255-4945faa0-5517-46ea-b173-928eff999330.png)\n"
        },
        {
          "name": "archived_untest_files",
          "type": "tree",
          "content": null
        },
        {
          "name": "control",
          "type": "tree",
          "content": null
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "datasets_download",
          "type": "tree",
          "content": null
        },
        {
          "name": "demo_toolbox.py",
          "type": "blob",
          "size": 2.43,
          "content": "from pathlib import Path\nfrom control.toolbox import Toolbox\nfrom utils.argutils import print_args\nfrom utils.modelutils import check_model_paths\nimport argparse\nimport os\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(\n        description=\"Runs the toolbox\",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n    )\n    \n    parser.add_argument(\"-d\", \"--datasets_root\", type=Path, help= \\\n        \"Path to the directory containing your datasets. See toolbox/__init__.py for a list of \"\n        \"supported datasets.\", default=None)\n    parser.add_argument(\"-vc\", \"--vc_mode\", action=\"store_true\", \n                        help=\"Voice Conversion Mode(PPG based)\")\n    parser.add_argument(\"-e\", \"--enc_models_dir\", type=Path, default=f\"data{os.sep}ckpt{os.sep}encoder\", \n                        help=\"Directory containing saved encoder models\")\n    parser.add_argument(\"-s\", \"--syn_models_dir\", type=Path, default=f\"data{os.sep}ckpt{os.sep}synthesizer\", \n                        help=\"Directory containing saved synthesizer models\")\n    parser.add_argument(\"-v\", \"--voc_models_dir\", type=Path, default=f\"data{os.sep}ckpt{os.sep}vocoder\", \n                        help=\"Directory containing saved vocoder models\")\n    parser.add_argument(\"-ex\", \"--extractor_models_dir\", type=Path, default=f\"data{os.sep}ckpt{os.sep}ppg_extractor\", \n                        help=\"Directory containing saved extrator models\")\n    parser.add_argument(\"-cv\", \"--convertor_models_dir\", type=Path, default=f\"data{os.sep}ckpt{os.sep}ppg2mel\", \n                        help=\"Directory containing saved convert models\")\n    parser.add_argument(\"--cpu\", action=\"store_true\", help=\\\n        \"If True, processing is done on CPU, even when a GPU is available.\")\n    parser.add_argument(\"--seed\", type=int, default=None, help=\\\n        \"Optional random number seed value to make toolbox deterministic.\")\n    parser.add_argument(\"--no_mp3_support\", action=\"store_true\", help=\\\n        \"If True, no mp3 files are allowed.\")\n    args = parser.parse_args()\n    print_args(args, parser)\n\n    if args.cpu:\n        # Hide GPUs from Pytorch to force CPU processing\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n    del args.cpu\n\n    ## Remind the user to download pretrained models if needed\n    check_model_paths(encoder_path=args.enc_models_dir, synthesizer_path=args.syn_models_dir,\n                      vocoder_path=args.voc_models_dir)\n\n    # Launch the toolbox\n    Toolbox(**vars(args))    \n"
        },
        {
          "name": "docker-compose.yml",
          "type": "blob",
          "size": 0.55,
          "content": "version: '3.8'\n\nservices:\n  server:\n    image: mockingbird:latest\n    build: .\n    volumes:\n      - ./datasets:/datasets\n      - ./synthesizer/saved_models:/workspace/synthesizer/saved_models\n    environment:\n      - DATASET_MIRROR=US\n      - FORCE_RETRAIN=false\n      - TRAIN_DATASETS=aidatatang_200zh magicdata aishell3 data_aishell\n      - TRAIN_SKIP_EXISTING=true\n    ports:\n      - 8080:8080\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              device_ids: [ '0' ]\n              capabilities: [ gpu ]\n"
        },
        {
          "name": "docker-entrypoint.sh",
          "type": "blob",
          "size": 0.55,
          "content": "#!/usr/bin/env bash\n\nif [ -z \"$(ls -A /workspace/synthesizer/saved_models)\" ] || [ \"$FORCE_RETRAIN\" = true ] ; then\n    /workspace/datasets_download/download.sh\n    /workspace/datasets_download/extract.sh\n    for DATASET in ${TRAIN_DATASETS}\n    do\n        if [ \"$TRAIN_SKIP_EXISTING\" = true ] ; then\n            python pre.py /datasets -d ${DATASET} -n $(nproc) --skip_existing\n        else\n            python pre.py /datasets -d ${DATASET} -n $(nproc)\n        fi\n    done\n    python synthesizer_train.py mandarin /datasets/SV2TTS/synthesizer\nfi\n\npython web.py\n"
        },
        {
          "name": "env.yml",
          "type": "blob",
          "size": 1.06,
          "content": "ï»¿channels:\r\n  - pytorch\r\n  - conda-forge\r\ndependencies:\r\n  - umap-learn\r\n  - visdom\r\n  - librosa\r\n  - matplotlib>=3.3.0\r\n  - numpy\r\n  - scipy>=1.0.0\r\n  - tqdm\r\n  - python-sounddevice\r\n  - pysoundfile\r\n  - unidecode\r\n  - inflect\r\n  - pyqt5-sip\r\n  - multiprocess\r\n  - numba\r\n  - pypinyin\r\n  - flask\r\n  - flask-wtf\r\n  - flask_cors\r\n  - gevent\r\n  - flask-restx\r\n  - tensorboard\r\n  - streamlit\r\n  - pyyaml\r\n  - torch-complex\r\n  - espnet\r\n  - pywavelets\r\n  - fastapi\r\n  - loguru\r\n  - typer[all]\r\n  - click\r\n  - webrtcvad\r\n  - ffmpeg\r\n  - transformers"
        },
        {
          "name": "gen_voice.py",
          "type": "blob",
          "size": 4.34,
          "content": "from models.synthesizer.inference import Synthesizer\nfrom models.encoder import inference as encoder\nfrom models.vocoder.hifigan import inference as gan_vocoder\nfrom pathlib import Path\nimport numpy as np\nimport soundfile as sf\nimport torch\nimport sys\nimport os\nimport re\nimport cn2an\n\nvocoder = gan_vocoder\n\ndef gen_one_wav(synthesizer, in_fpath, embed, texts, file_name, seq):\n    embeds = [embed] * len(texts)\n    # If you know what the attention layer alignments are, you can retrieve them here by\n    # passing return_alignments=True\n    specs = synthesizer.synthesize_spectrograms(texts, embeds, style_idx=-1, min_stop_token=4, steps=400)\n    #spec = specs[0]\n    breaks = [spec.shape[1] for spec in specs]\n    spec = np.concatenate(specs, axis=1)\n\n    # If seed is specified, reset torch seed and reload vocoder\n    # Synthesizing the waveform is fairly straightforward. Remember that the longer the\n    # spectrogram, the more time-efficient the vocoder.\n    generated_wav, output_sample_rate = vocoder.infer_waveform(spec)\n    \n    # Add breaks\n    b_ends = np.cumsum(np.array(breaks) * synthesizer.hparams.hop_size)\n    b_starts = np.concatenate(([0], b_ends[:-1]))\n    wavs = [generated_wav[start:end] for start, end, in zip(b_starts, b_ends)]\n    breaks = [np.zeros(int(0.15 * synthesizer.sample_rate))] * len(breaks)\n    generated_wav = np.concatenate([i for w, b in zip(wavs, breaks) for i in (w, b)])\n    \n    ## Post-generation\n    # There's a bug with sounddevice that makes the audio cut one second earlier, so we\n    # pad it.\n\n    # Trim excess silences to compensate for gaps in spectrograms (issue #53)\n    generated_wav = encoder.preprocess_wav(generated_wav)\n    generated_wav = generated_wav / np.abs(generated_wav).max() * 0.97\n        \n    # Save it on the disk\n    model=os.path.basename(in_fpath)\n    filename = \"%s_%d_%s.wav\" %(file_name, seq, model)\n    sf.write(filename, generated_wav, synthesizer.sample_rate)\n\n    print(\"\\nSaved output as %s\\n\\n\" % filename)\n    \n    \ndef generate_wav(enc_model_fpath, syn_model_fpath, voc_model_fpath, in_fpath, input_txt, file_name): \n    if torch.cuda.is_available():\n        device_id = torch.cuda.current_device()\n        gpu_properties = torch.cuda.get_device_properties(device_id)\n        ## Print some environment information (for debugging purposes)\n        print(\"Found %d GPUs available. Using GPU %d (%s) of compute capability %d.%d with \"\n            \"%.1fGb total memory.\\n\" % \n            (torch.cuda.device_count(),\n            device_id,\n            gpu_properties.name,\n            gpu_properties.major,\n            gpu_properties.minor,\n            gpu_properties.total_memory / 1e9))\n    else:\n        print(\"Using CPU for inference.\\n\")\n\n    print(\"Preparing the encoder, the synthesizer and the vocoder...\")\n    encoder.load_model(enc_model_fpath)\n    synthesizer = Synthesizer(syn_model_fpath)\n    vocoder.load_model(voc_model_fpath)\n\n    encoder_wav = synthesizer.load_preprocess_wav(in_fpath)\n    embed, partial_embeds, _ = encoder.embed_utterance(encoder_wav, return_partials=True)\n\n    texts = input_txt.split(\"\\n\")\n    seq=0\n    each_num=1500\n    \n    punctuation = 'ï¼ï¼Œã€‚ã€,' # punctuate and split/clean text\n    processed_texts = []\n    cur_num = 0\n    for text in texts:\n      for processed_text in re.sub(r'[{}]+'.format(punctuation), '\\n', text).split('\\n'):\n        if processed_text:\n            processed_texts.append(processed_text.strip())\n            cur_num += len(processed_text.strip())\n      if cur_num > each_num:\n        seq = seq +1\n        gen_one_wav(synthesizer, in_fpath, embed, processed_texts, file_name, seq)\n        processed_texts = []\n        cur_num = 0\n    \n    if len(processed_texts)>0:\n      seq = seq +1\n      gen_one_wav(synthesizer, in_fpath, embed, processed_texts, file_name, seq)\n\nif (len(sys.argv)>=3):\n    my_txt = \"\"\n    print(\"reading from :\", sys.argv[1])\n    with open(sys.argv[1], \"r\") as f:\n        for line in f.readlines():\n            #line = line.strip('\\n')\n            my_txt += line\n    txt_file_name = sys.argv[1]\n    wav_file_name = sys.argv[2]\n\n    output = cn2an.transform(my_txt, \"an2cn\")\n    print(output)\n    generate_wav(\n    Path(\"encoder/saved_models/pretrained.pt\"),\n    Path(\"synthesizer/saved_models/mandarin.pt\"),\n    Path(\"vocoder/saved_models/pretrained/g_hifigan.pt\"), wav_file_name, output, txt_file_name\n    )\n\nelse:\n    print(\"please input the file name\")\n    exit(1)\n\n\n"
        },
        {
          "name": "models",
          "type": "tree",
          "content": null
        },
        {
          "name": "monotonic_align",
          "type": "tree",
          "content": null
        },
        {
          "name": "pre.py",
          "type": "blob",
          "size": 3.97,
          "content": "from models.synthesizer.preprocess import create_embeddings, preprocess_dataset, create_emo\nfrom models.synthesizer.hparams import hparams\nfrom pathlib import Path\nimport argparse\n\nrecognized_datasets = [\n    \"aidatatang_200zh\",\n    \"aidatatang_200zh_s\",\n    \"magicdata\",\n    \"aishell3\",\n    \"data_aishell\"\n]\n\n#TODO: add for emotional data \nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\n        description=\"Preprocesses audio files from datasets, encodes them as mel spectrograms \"\n                    \"and writes them to  the disk. Audio files are also saved, to be used by the \"\n                    \"vocoder for training.\",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n    )\n    parser.add_argument(\"datasets_root\", type=Path, help=\\\n        \"Path to the directory containing your datasets.\")\n    parser.add_argument(\"-o\", \"--out_dir\", type=Path, default=argparse.SUPPRESS, help=\\\n        \"Path to the output directory that will contain the mel spectrograms, the audios and the \"\n        \"embeds. Defaults to <datasets_root>/SV2TTS/synthesizer/\")\n    parser.add_argument(\"-n\", \"--n_processes\", type=int, default=1, help=\\\n        \"Number of processes in parallel.\")\n    parser.add_argument(\"-s\", \"--skip_existing\", action=\"store_true\", help=\\\n        \"Whether to overwrite existing files with the same name. Useful if the preprocessing was \"\n        \"interrupted. \")\n    parser.add_argument(\"--hparams\", type=str, default=\"\", help=\\\n        \"Hyperparameter overrides as a comma-separated list of name-value pairs\")\n    parser.add_argument(\"--no_trim\", action=\"store_true\", help=\\\n        \"Preprocess audio without trimming silences (not recommended).\")\n    parser.add_argument(\"--no_alignments\", action=\"store_true\", help=\\\n        \"Use this option when dataset does not include alignments\\\n        (these are used to split long audio files into sub-utterances.)\")\n    parser.add_argument(\"-d\", \"--dataset\", type=str, default=\"aidatatang_200zh\", help=\\\n        \"Name of the dataset to process, allowing values: magicdata, aidatatang_200zh, aishell3, data_aishell.\")\n    parser.add_argument(\"-e\", \"--encoder_model_fpath\", type=Path, default=\"data/ckpt/encoder/pretrained.pt\", help=\\\n        \"Path your trained encoder model.\")\n    parser.add_argument(\"-ne\", \"--n_processes_embed\", type=int, default=1, help=\\\n        \"Number of processes in parallel.An encoder is created for each, so you may need to lower \"\n        \"this value on GPUs with low memory. Set it to 1 if CUDA is unhappy\")\n    parser.add_argument(\"-ee\",\"--emotion_extract\", action=\"store_true\", help=\\\n        \"Preprocess audio to extract emotional numpy (for emotional vits).\")\n    args = parser.parse_args()\n\n    # Process the arguments\n    if not hasattr(args, \"out_dir\"):\n        args.out_dir = args.datasets_root.joinpath(\"SV2TTS\", \"synthesizer\")\n    assert args.dataset in recognized_datasets, 'is not supported, please vote for it in https://github.com/babysor/MockingBird/issues/10'\n    # Create directories\n    assert args.datasets_root.exists()\n    args.out_dir.mkdir(exist_ok=True, parents=True)\n\n    # Verify webrtcvad is available\n    if not args.no_trim:\n        try:\n            import webrtcvad\n        except:\n            raise ModuleNotFoundError(\"Package 'webrtcvad' not found. This package enables \"\n                \"noise removal and is recommended. Please install and try again. If installation fails, \"\n                \"use --no_trim to disable this error message.\")\n    encoder_model_fpath = args.encoder_model_fpath\n    del args.no_trim\n   \n    args.hparams = hparams.parse(args.hparams)\n    n_processes_embed = args.n_processes_embed\n    del args.n_processes_embed\n    preprocess_dataset(**vars(args))\n    \n    create_embeddings(synthesizer_root=args.out_dir, n_processes=n_processes_embed, encoder_model_fpath=encoder_model_fpath, skip_existing=args.skip_existing)\n    \n    if args.emotion_extract:\n        create_emo(synthesizer_root=args.out_dir, n_processes=n_processes_embed, skip_existing=args.skip_existing, hparams=args.hparams)\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.44,
          "content": "umap-learn\nvisdom\nlibrosa\nmatplotlib>=3.3.0\nnumpy==1.19.3; platform_system == \"Windows\"\nnumpy==1.20.3; platform_system != \"Windows\"\nscipy>=1.0.0\ntqdm\nsounddevice\nSoundFile\nUnidecode\ninflect\nPyQt5\nmultiprocess\nnumba\nwebrtcvad; platform_system != \"Windows\"\npypinyin==0.44.0\nflask\nflask_wtf\nflask_cors\ngevent\nflask_restx\ntensorboard\nstreamlit\nPyYAML\ntorch_complex\nespnet\nPyWavelets\nfastapi\nloguru\nclick==8.0.4\ntyper\nmonotonic-align==1.0.0\ntransformers\n"
        },
        {
          "name": "run.py",
          "type": "blob",
          "size": 4.52,
          "content": "import time\nimport os\nimport argparse\nimport torch\nimport glob\nfrom pathlib import Path\nfrom tqdm import tqdm\nfrom models.ppg_extractor import load_model\nimport librosa\nimport soundfile as sf\nfrom utils.hparams import HpsYaml\n\nfrom models.encoder.audio import preprocess_wav\nfrom models.encoder import inference as speacker_encoder\nfrom models.vocoder.hifigan import inference as vocoder\nfrom models.ppg2mel import MelDecoderMOLv2\nfrom utils.f0_utils import compute_f0, f02lf0, compute_mean_std, get_converted_lf0uv\n\n\ndef _build_ppg2mel_model(model_config, model_file, device):\n    ppg2mel_model = MelDecoderMOLv2(\n        **model_config[\"model\"]\n    ).to(device)\n    ckpt = torch.load(model_file, map_location=device)\n    ppg2mel_model.load_state_dict(ckpt[\"model\"])\n    ppg2mel_model.eval()\n    return ppg2mel_model\n\n\n@torch.no_grad()\ndef convert(args):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    output_dir = args.output_dir\n    os.makedirs(output_dir, exist_ok=True)\n\n    step = os.path.basename(args.ppg2mel_model_file)[:-4].split(\"_\")[-1]\n\n    # Build models\n    print(\"Load PPG-model, PPG2Mel-model, Vocoder-model...\")\n    ppg_model = load_model(\n        Path('./ppg_extractor/saved_models/24epoch.pt'),\n        device,\n    )\n    ppg2mel_model = _build_ppg2mel_model(HpsYaml(args.ppg2mel_model_train_config), args.ppg2mel_model_file, device) \n    # vocoder.load_model('./vocoder/saved_models/pretrained/g_hifigan.pt', \"./vocoder/hifigan/config_16k_.json\")\n    vocoder.load_model('./vocoder/saved_models/24k/g_02830000.pt')\n    # Data related\n    ref_wav_path = args.ref_wav_path\n    ref_wav = preprocess_wav(ref_wav_path)\n    ref_fid = os.path.basename(ref_wav_path)[:-4]\n    \n    # TODO: specify encoder\n    speacker_encoder.load_model(Path(\"encoder/saved_models/pretrained_bak_5805000.pt\"))\n    ref_spk_dvec = speacker_encoder.embed_utterance(ref_wav)\n    ref_spk_dvec = torch.from_numpy(ref_spk_dvec).unsqueeze(0).to(device)\n    ref_lf0_mean, ref_lf0_std = compute_mean_std(f02lf0(compute_f0(ref_wav)))\n    \n    source_file_list = sorted(glob.glob(f\"{args.wav_dir}/*.wav\"))\n    print(f\"Number of source utterances: {len(source_file_list)}.\")\n    \n    total_rtf = 0.0\n    cnt = 0\n    for src_wav_path in tqdm(source_file_list):\n        # Load the audio to a numpy array:\n        src_wav, _ = librosa.load(src_wav_path, sr=16000)\n        src_wav_tensor = torch.from_numpy(src_wav).unsqueeze(0).float().to(device)\n        src_wav_lengths = torch.LongTensor([len(src_wav)]).to(device)\n        ppg = ppg_model(src_wav_tensor, src_wav_lengths)\n\n        lf0_uv = get_converted_lf0uv(src_wav, ref_lf0_mean, ref_lf0_std, convert=True)\n        min_len = min(ppg.shape[1], len(lf0_uv))\n\n        ppg = ppg[:, :min_len]\n        lf0_uv = lf0_uv[:min_len]\n        \n        start = time.time()\n        _, mel_pred, att_ws = ppg2mel_model.inference(\n            ppg,\n            logf0_uv=torch.from_numpy(lf0_uv).unsqueeze(0).float().to(device),\n            spembs=ref_spk_dvec,\n        )\n        src_fid = os.path.basename(src_wav_path)[:-4]\n        wav_fname = f\"{output_dir}/vc_{src_fid}_ref_{ref_fid}_step{step}.wav\"\n        mel_len = mel_pred.shape[0]\n        rtf = (time.time() - start) / (0.01 * mel_len)\n        total_rtf += rtf\n        cnt += 1\n        # continue\n        mel_pred= mel_pred.transpose(0, 1)\n        y, output_sample_rate = vocoder.infer_waveform(mel_pred.cpu())\n        sf.write(wav_fname, y.squeeze(), output_sample_rate, \"PCM_16\")\n    \n    print(\"RTF:\")\n    print(total_rtf / cnt)\n\n\ndef get_parser():\n    parser = argparse.ArgumentParser(description=\"Conversion from wave input\")\n    parser.add_argument(\n        \"--wav_dir\",\n        type=str,\n        default=None,\n        required=True,\n        help=\"Source wave directory.\",\n    )\n    parser.add_argument(\n        \"--ref_wav_path\",\n        type=str,\n        required=True,\n        help=\"Reference wave file path.\",\n    )\n    parser.add_argument(\n        \"--ppg2mel_model_train_config\", \"-c\",\n        type=str,\n        default=None,\n        required=True,\n        help=\"Training config file (yaml file)\",\n    )\n    parser.add_argument(\n        \"--ppg2mel_model_file\", \"-m\",\n        type=str,\n        default=None,\n        required=True,\n        help=\"ppg2mel model checkpoint file path\"\n    )\n    parser.add_argument(\n        \"--output_dir\", \"-o\",\n        type=str,\n        default=\"vc_gens_vctk_oneshot\",\n        help=\"Output folder to save the converted wave.\"\n    )\n    \n    return parser\n\ndef main():\n    parser = get_parser()\n    args = parser.parse_args()\n    convert(args)\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "train.py",
          "type": "blob",
          "size": 0.53,
          "content": "import argparse\n\ndef main():\n    # Arguments\n    preparser = argparse.ArgumentParser(description=\n            'Training model.')\n    preparser.add_argument('--type', type=str, \n                        help='type of training ')\n\n    ###\n    paras, _ = preparser.parse_known_args()\n    if paras.type == \"synth\":\n        from control.cli.synthesizer_train import new_train\n        new_train()\n    if paras.type == \"vits\":\n        from models.synthesizer.train_vits import new_train\n        new_train()\n\nif __name__ == \"__main__\":\n    main()   \n"
        },
        {
          "name": "utils",
          "type": "tree",
          "content": null
        },
        {
          "name": "vits.ipynb",
          "type": "blob",
          "size": 19.04,
          "content": "{\n \"cells\": [\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"from utils.hparams import load_hparams_json\\n\",\n    \"from utils.util import intersperse\\n\",\n    \"import json\\n\",\n    \"from models.synthesizer.models.vits import Vits\\n\",\n    \"import torch\\n\",\n    \"import numpy as np\\n\",\n    \"import IPython.display as ipd\\n\",\n    \"from models.synthesizer.utils.symbols import symbols\\n\",\n    \"from models.synthesizer.utils.text import text_to_sequence\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"hps = load_hparams_json(\\\"data/ckpt/synthesizer/vits5/config.json\\\")\\n\",\n    \"print(hps.train)\\n\",\n    \"model = Vits(\\n\",\n    \"    len(symbols),\\n\",\n    \"    hps[\\\"data\\\"][\\\"filter_length\\\"] // 2 + 1,\\n\",\n    \"    hps[\\\"train\\\"][\\\"segment_size\\\"] // hps[\\\"data\\\"][\\\"hop_length\\\"],\\n\",\n    \"    n_speakers=hps[\\\"data\\\"][\\\"n_speakers\\\"],\\n\",\n    \"    **hps[\\\"model\\\"])\\n\",\n    \"_ = model.eval()\\n\",\n    \"device = torch.device(\\\"cpu\\\")\\n\",\n    \"checkpoint = torch.load(str(\\\"data/ckpt/synthesizer/vits5/G_56000.pth\\\"), map_location=device)\\n\",\n    \"if \\\"model_state\\\" in checkpoint:\\n\",\n    \"    state = checkpoint[\\\"model_state\\\"]\\n\",\n    \"else:\\n\",\n    \"    state = checkpoint[\\\"model\\\"]\\n\",\n    \"model.load_state_dict(state, strict=False)\\n\",\n    \"\\n\",\n    \"# éšæœºæŠ½å–æƒ…æ„Ÿå‚è€ƒéŸ³é¢‘çš„æ ¹ç›®å½•\\n\",\n    \"random_emotion_root = \\\"D:\\\\\\\\audiodata\\\\\\\\SV2TTS\\\\\\\\synthesizer\\\\\\\\emo\\\\\\\\\\\"\\n\",\n    \"import random, re\\n\",\n    \"from pypinyin import lazy_pinyin, Style\\n\",\n    \"\\n\",\n    \"import os\\n\",\n    \"\\n\",\n    \"def tts(txt, emotion, sid=0):\\n\",\n    \"    txt = \\\" \\\".join(lazy_pinyin(txt, style=Style.TONE3, neutral_tone_with_five=False))\\n\",\n    \"    text_norm = text_to_sequence(txt, hps[\\\"data\\\"][\\\"text_cleaners\\\"])\\n\",\n    \"    # if hps[\\\"data\\\"][\\\"add_blank\\\"]:\\n\",\n    \"    # text_norm = intersperse(text_norm, 0)\\n\",\n    \"    stn_tst = torch.LongTensor(text_norm)\\n\",\n    \"\\n\",\n    \"    with torch.no_grad(): #inference mode\\n\",\n    \"        x_tst = stn_tst.unsqueeze(0)\\n\",\n    \"        x_tst_lengths = torch.LongTensor([stn_tst.size(0)])\\n\",\n    \"        sid = torch.LongTensor([sid])\\n\",\n    \"        if emotion.endswith(\\\"wav\\\"):\\n\",\n    \"            from models.synthesizer.preprocess_audio import extract_emo\\n\",\n    \"            import librosa\\n\",\n    \"            wav, sr = librosa.load(emotion, 16000)\\n\",\n    \"            emo = torch.FloatTensor(extract_emo(np.expand_dims(wav, 0), sr, embeddings=True))\\n\",\n    \"        elif emotion == \\\"random_sample\\\":\\n\",\n    \"            rand_emo = random.sample(os.listdir(random_emotion_root), 1)[0]\\n\",\n    \"            print(rand_emo)\\n\",\n    \"            emo = torch.FloatTensor(np.load(f\\\"{random_emotion_root}\\\\\\\\{rand_emo}\\\")).unsqueeze(0)\\n\",\n    \"        elif emotion.endswith(\\\"npy\\\"):\\n\",\n    \"            print(emotion)\\n\",\n    \"            emo = torch.FloatTensor(np.load(f\\\"{random_emotion_root}\\\\\\\\{emotion}\\\")).unsqueeze(0)\\n\",\n    \"        else:\\n\",\n    \"            print(\\\"emotionå‚æ•°ä¸æ­£ç¡®\\\")\\n\",\n    \"\\n\",\n    \"        audio = model.infer(x_tst, x_tst_lengths, sid=sid, noise_scale=0.667, noise_scale_w=0.8, length_scale=1, emo=emo)[0][0,0].data.float().numpy()\\n\",\n    \"    ipd.display(ipd.Audio(audio, rate=hps[\\\"data\\\"][\\\"sampling_rate\\\"], normalize=False))\\n\",\n    \"\\n\",\n    \"\\n\"\n   ]\n  },\n  {\n   \"attachments\": {},\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"æ¨ç†ï¼š\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"txt = \\\"æˆ‘ä»¬å°†å…¶æ‹“å±•åˆ°æ–‡æœ¬é©±åŠ¨æ•°å­—äººå½¢è±¡é¢†åŸŸ\\\"\\n\",\n    \"#æ­£å¸¸: \\n\",\n    \"tts(txt, emotion='emo-T0055G4906S0052.wav_00.npy', sid=100)\\n\",\n    \"#å¿«é€Ÿï¼šemo-T0055G2323S0179.wav_00.npy\\n\",\n    \"\\n\",\n    \"#éš¾è¿‡ï¼š\\n\",\n    \"tts(txt, emotion='emo-15_4581_20170825202626.wav_00.npy', sid=100)\\n\",\n    \"\\n\",\n    \"#å¼€å¿ƒï¼šT0055G2412S0498.wav\\n\",\n    \"tts(txt, emotion='emo-T0055G2412S0498.wav_00.npy', sid=100)\\n\",\n    \"\\n\",\n    \"#æ„¤æ€’ T0055G1371S0363.wav T0055G1344S0160.wav\\n\",\n    \"tts(txt, emotion='emo-T0055G1344S0160.wav_00.npy', sid=100)\\n\",\n    \"\\n\",\n    \"#ç–²æƒ«\\n\",\n    \"tts(txt, emotion='emo-T0055G2294S0476.wav_00.npy', sid=100)\\n\",\n    \"\\n\",\n    \"#ç€æ€¥\\n\",\n    \"tts(txt, emotion='emo-T0055G1671S0170.wav_00.npy', sid=100)\\n\",\n    \"\\n\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"txt = \\\"æˆ‘ä»¬å°†å…¶æ‹“å±•åˆ°æ–‡æœ¬é©±åŠ¨æ•°å­—äººå½¢è±¡é¢†åŸŸ\\\"\\n\",\n    \"tts(txt, emotion='random_sample', sid=100)\\n\",\n    \"tts(txt, emotion='random_sample', sid=100)\\n\",\n    \"tts(txt, emotion='random_sample', sid=100)\\n\",\n    \"tts(txt, emotion='random_sample', sid=100)\\n\",\n    \"tts(txt, emotion='random_sample', sid=100)\\n\",\n    \"tts(txt, emotion='random_sample', sid=100)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"txt = \\\"æˆ‘ä»¬å°†å…¶æ‹“å±•åˆ°æ–‡æœ¬é©±åŠ¨æ•°å­—äººå½¢è±¡é¢†åŸŸ\\\"\\n\",\n    \"types = [\\\"å¹³æ·¡\\\", \\\"æ¿€åŠ¨\\\", \\\"ç–²æƒ«\\\", \\\"å…´å¥‹\\\", \\\"æ²®ä¸§\\\", \\\"å¼€å¿ƒ\\\"]\\n\",\n    \"for t in types:\\n\",\n    \"    print(t)\\n\",\n    \"    tts(txt, emotion=f'C:\\\\\\\\Users\\\\\\\\babys\\\\\\\\Music\\\\\\\\{t}.wav', sid=100)\\n\",\n    \"# tts(txt, emotion='D:\\\\\\\\audiodata\\\\\\\\aidatatang_200zh\\\\\\\\corpus\\\\\\\\train\\\\\\\\G1858\\\\\\\\T0055G1858S0342.wav', sid=5)\"\n   ]\n  },\n  {\n   \"attachments\": {},\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"é¢„å¤„ç†ï¼š\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"from models.synthesizer.preprocess import preprocess_dataset\\n\",\n    \"from pathlib import Path\\n\",\n    \"from utils.hparams import HParams\\n\",\n    \"datasets_root = Path(\\\"../audiodata/\\\")\\n\",\n    \"hparams = HParams(\\n\",\n    \"        n_fft = 1024, # filter_length\\n\",\n    \"        num_mels = 80,\\n\",\n    \"        hop_size = 256,                             # Tacotron uses 12.5 ms frame shift (set to sample_rate * 0.0125)\\n\",\n    \"        win_size = 1024,                             # Tacotron uses 50 ms frame length (set to sample_rate * 0.050)\\n\",\n    \"        fmin = 55,\\n\",\n    \"        min_level_db = -100,\\n\",\n    \"        ref_level_db = 20,\\n\",\n    \"        max_abs_value = 4.,                         # Gradient explodes if too big, premature convergence if too small.\\n\",\n    \"        sample_rate = 16000,\\n\",\n    \"        rescale = True,\\n\",\n    \"        max_mel_frames = 900,\\n\",\n    \"        rescaling_max = 0.9,        \\n\",\n    \"        preemphasis = 0.97,                         # Filter coefficient to use if preemphasize is True\\n\",\n    \"        preemphasize = True,\\n\",\n    \"        ### Mel Visualization and Griffin-Lim\\n\",\n    \"        signal_normalization = True,\\n\",\n    \"\\n\",\n    \"        utterance_min_duration = 1.6,               # Duration in seconds below which utterances are discarded\\n\",\n    \"        ### Audio processing options\\n\",\n    \"        fmax = 7600,                                # Should not exceed (sample_rate // 2)\\n\",\n    \"        allow_clipping_in_normalization = True,     # Used when signal_normalization = True\\n\",\n    \"        clip_mels_length = True,                    # If true, discards samples exceeding max_mel_frames\\n\",\n    \"        use_lws = False,                            # \\\"Fast spectrogram phase recovery using local weighted sums\\\"\\n\",\n    \"        symmetric_mels = True,                      # Sets mel range to [-max_abs_value, max_abs_value] if True,\\n\",\n    \"                                                    #               and [0, max_abs_value] if False\\n\",\n    \"        trim_silence = False,                        # Use with sample_rate of 16000 for best results\\n\",\n    \"\\n\",\n    \")\\n\",\n    \"preprocess_dataset(datasets_root=datasets_root, \\n\",\n    \"        out_dir=datasets_root.joinpath(\\\"SV2TTS\\\", \\\"synthesizer\\\"),\\n\",\n    \"        n_processes=8,\\n\",\n    \"        skip_existing=True, \\n\",\n    \"        hparams=hparams, \\n\",\n    \"        no_alignments=False, \\n\",\n    \"        dataset=\\\"aidatatang_200zh\\\", \\n\",\n    \"        emotion_extract=True)\"\n   ]\n  },\n  {\n   \"attachments\": {},\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"è®­ç»ƒï¼š\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"from models.synthesizer.train_vits import run\\n\",\n    \"from pathlib import Path\\n\",\n    \"from utils.hparams import HParams\\n\",\n    \"import torch, os\\n\",\n    \"import torch.multiprocessing as mp\\n\",\n    \"\\n\",\n    \"datasets_root = Path(\\\"../audiodata/SV2TTS/synthesizer\\\")\\n\",\n    \"hparams= HParams(\\n\",\n    \"  model_dir = \\\"data/ckpt/synthesizer/vits\\\",\\n\",\n    \")\\n\",\n    \"hparams.loadJson(Path(hparams.model_dir).joinpath(\\\"config.json\\\"))\\n\",\n    \"hparams.data[\\\"training_files\\\"] = str(datasets_root.joinpath(\\\"train.txt\\\"))\\n\",\n    \"hparams.data[\\\"validation_files\\\"] = str(datasets_root.joinpath(\\\"train.txt\\\"))\\n\",\n    \"hparams.data[\\\"datasets_root\\\"] = str(datasets_root)\\n\",\n    \"\\n\",\n    \"n_gpus = torch.cuda.device_count()\\n\",\n    \"# for spawn\\n\",\n    \"os.environ['MASTER_ADDR'] = 'localhost'\\n\",\n    \"os.environ['MASTER_PORT'] = '8899'\\n\",\n    \"mp.spawn(run, nprocs=n_gpus, args=(n_gpus, hparams))\"\n   ]\n  },\n  {\n   \"attachments\": {},\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"æŒ‘é€‰åªæœ‰å¯¹åº”emoæ–‡ä»¶çš„metaæ•°æ®\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"from pathlib import Path\\n\",\n    \"import os\\n\",\n    \"root = Path('../audiodata/SV2TTS/synthesizer')\\n\",\n    \"dict_info = []\\n\",\n    \"with open(root.joinpath(\\\"train.txt\\\"), \\\"r\\\", encoding=\\\"utf-8\\\") as dict_meta:\\n\",\n    \"    for raw in dict_meta:\\n\",\n    \"        if not raw:\\n\",\n    \"            continue\\n\",\n    \"        v = raw.split(\\\"|\\\")[0].replace(\\\"audio\\\",\\\"emo\\\")\\n\",\n    \"        emo_fpath = root.joinpath(\\\"emo\\\").joinpath(v)\\n\",\n    \"        if emo_fpath.exists():\\n\",\n    \"            dict_info.append(raw)\\n\",\n    \"        # else:\\n\",\n    \"        #     print(emo_fpath)\\n\",\n    \"# Iterate over each wav\\n\",\n    \"meta2 = Path('../audiodata/SV2TTS/synthesizer/train2.txt')\\n\",\n    \"metadata_file = meta2.open(\\\"w\\\", encoding=\\\"utf-8\\\")\\n\",\n    \"for new_info in dict_info:\\n\",\n    \"    metadata_file.write(new_info)\\n\",\n    \"metadata_file.close()\"\n   ]\n  },\n  {\n   \"attachments\": {},\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"ä»è®­ç»ƒé›†ä¸­æŠ½å–10%ä½œä¸ºæµ‹è¯•é›†\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"from pathlib import Path\\n\",\n    \"root = Path('../audiodata/SV2TTS/synthesizer')\\n\",\n    \"dict_info1 = []\\n\",\n    \"dict_info2 = []\\n\",\n    \"count = 1\\n\",\n    \"with open(root.joinpath(\\\"train.txt\\\"), \\\"r\\\", encoding=\\\"utf-8\\\") as dict_meta:\\n\",\n    \"    for raw in dict_meta:\\n\",\n    \"        if not raw:\\n\",\n    \"            continue\\n\",\n    \"        if count % 10 == 0:\\n\",\n    \"            dict_info2.append(raw)\\n\",\n    \"        else:\\n\",\n    \"            dict_info1.append(raw)\\n\",\n    \"        count += 1\\n\",\n    \"# Iterate over each wav\\n\",\n    \"meta1 = Path('../audiodata/SV2TTS/synthesizer/train1.txt')\\n\",\n    \"metadata_file = meta1.open(\\\"w\\\", encoding=\\\"utf-8\\\")\\n\",\n    \"for new_info in dict_info1:\\n\",\n    \"    metadata_file.write(new_info)\\n\",\n    \"metadata_file.close()\\n\",\n    \"\\n\",\n    \"meta2 = Path('../audiodata/SV2TTS/synthesizer/eval.txt')\\n\",\n    \"metadata_file = meta2.open(\\\"w\\\", encoding=\\\"utf-8\\\")\\n\",\n    \"for new_info in dict_info2:\\n\",\n    \"    metadata_file.write(new_info)\\n\",\n    \"metadata_file.close()\"\n   ]\n  },\n  {\n   \"attachments\": {},\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"evaluation\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"from pathlib import Path\\n\",\n    \"root = Path('../audiodata/SV2TTS/synthesizer')\\n\",\n    \"spks = []\\n\",\n    \"spk_id = {}\\n\",\n    \"rows = []\\n\",\n    \"with open(root.joinpath(\\\"eval.txt\\\"), \\\"r\\\", encoding=\\\"utf-8\\\") as dict_meta:\\n\",\n    \"    for raw in dict_meta:\\n\",\n    \"        speaker_name = raw.split(\\\"-\\\")[1][6:10]\\n\",\n    \"        if speaker_name not in spk_id:\\n\",\n    \"            spks.append(speaker_name)\\n\",\n    \"            spk_id[speaker_name] = 1\\n\",\n    \"        rows.append(raw)\\n\",\n    \"i = 0\\n\",\n    \"spks.sort()\\n\",\n    \"\\n\",\n    \"for sp in spks:\\n\",\n    \"    spk_id[sp] = str(i)\\n\",\n    \"    i = i + 1\\n\",\n    \"print(len(spks))\\n\",\n    \"meta2 = Path('../audiodata/SV2TTS/synthesizer/eval2.txt')\\n\",\n    \"metadata_file = meta2.open(\\\"w\\\", encoding=\\\"utf-8\\\")\\n\",\n    \"for row in rows:\\n\",\n    \"    speaker_n = row.split(\\\"-\\\")[1][6:10]\\n\",\n    \"    metadata_file.write(row.strip()+\\\"|\\\"+spk_id[speaker_n]+\\\"\\\\n\\\")\\n\",\n    \"metadata_file.close()\\n\"\n   ]\n  },\n  {\n   \"attachments\": {},\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"[Not Recommended]\\n\",\n    \"Try to transcript map to detailed format:\\n\",\n    \"ni3 hao3 -> n i3 <pad> h ao3\\n\",\n    \"\\n\",\n    \"After couple of tests, I think this method will not improve the quality of result and may cause the crash of monotonic alignment.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"\\n\",\n    \"from pathlib import Path\\n\",\n    \"datasets_root = Path(\\\"../audiodata/SV2TTS/synthesizer/\\\")\\n\",\n    \"\\n\",\n    \"dictionary_fp = Path(\\\"../audiodata/ProDiff/processed/mandarin_pinyin.dict\\\")\\n\",\n    \"dict_map = {}\\n\",\n    \"for l in open(dictionary_fp, encoding='utf-8').readlines():\\n\",\n    \"    item = l.split(\\\"\\\\t\\\")\\n\",\n    \"    dict_map[item[0]] = item[1].replace(\\\"\\\\n\\\",\\\"\\\")\\n\",\n    \"\\n\",\n    \"with datasets_root.joinpath('train2.txt').open(\\\"w+\\\", encoding='utf-8') as f:\\n\",\n    \"    for l in open(datasets_root.joinpath('train.txt'), encoding='utf-8').readlines():\\n\",\n    \"        items = l.strip().replace(\\\"\\\\n\\\",\\\"\\\").replace(\\\"\\\\t\\\",\\\" \\\").split(\\\"|\\\")\\n\",\n    \"        phs_str = \\\"\\\"\\n\",\n    \"        for word in items[5].split(\\\" \\\"):\\n\",\n    \"            if word in dict_map:\\n\",\n    \"                phs_str += dict_map[word] \\n\",\n    \"            else:\\n\",\n    \"                phs_str += word\\n\",\n    \"            phs_str += \\\" _ \\\"\\n\",\n    \"        items[5] = phs_str\\n\",\n    \"        # if not os.path.exists(mfa_input_root.joinpath('train.txt')):\\n\",\n    \"        #     with open(mfa_input_root.joinpath(fileName + 'lab'), 'w+', encoding=\\\"utf-8\\\") as f:\\n\",\n    \"        f.write(\\\"|\\\".join(items) + \\\"\\\\n\\\")\"\n   ]\n  },\n  {\n   \"attachments\": {},\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"é¢„å¤„ç†åçš„æ•°æ®å¯è§†åŒ–\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"import matplotlib.pyplot as plt\\n\",\n    \"import librosa.display\\n\",\n    \"import librosa, torch\\n\",\n    \"import numpy as np\\n\",\n    \"from utils.audio_utils import spectrogram, mel_spectrogram, load_wav_to_torch, spec_to_mel\\n\",\n    \"\\n\",\n    \"# x, sr = librosa.load(\\\"D:\\\\audiodata\\\\SV2TTS\\\\synthesizer\\\\audio\\\\audio-T0055G2333S0196.wav_00.npy\\\")\\n\",\n    \"x = np.load(\\\"D:\\\\\\\\audiodata\\\\\\\\SV2TTS\\\\\\\\synthesizer\\\\\\\\audio\\\\\\\\audio-T0055G1858S0342.wav_00.npy\\\")\\n\",\n    \"\\n\",\n    \"plt.figure(figsize=(14, 5))\\n\",\n    \"librosa.display.waveplot(x)\\n\",\n    \"\\n\",\n    \"X = librosa.stft(x)\\n\",\n    \"Xdb = librosa.amplitude_to_db(abs(X))\\n\",\n    \"plt.figure(figsize=(14, 5))\\n\",\n    \"librosa.display.specshow(Xdb,  x_axis='time', y_axis='hz')\\n\",\n    \"\\n\",\n    \"# spectrogram = np.load(\\\"D:\\\\\\\\audiodata\\\\\\\\SV2TTS\\\\\\\\synthesizer\\\\\\\\mels\\\\\\\\mel-T0055G1858S0342.wav_00.npy\\\")\\n\",\n    \"audio = torch.from_numpy(x.astype(np.float32))\\n\",\n    \"\\n\",\n    \"# audio, sampling_rate = load_wav_to_torch(\\\"D:\\\\\\\\audiodata\\\\\\\\aidatatang_200zh\\\\\\\\corpus\\\\\\\\train\\\\\\\\G1858\\\\\\\\T0055G1858S0342.wav\\\")\\n\",\n    \"# audio_norm = audio / 32768.0\\n\",\n    \"audio_norm = audio.unsqueeze(0)\\n\",\n    \"spec = spectrogram(audio_norm, 1024, 256, 1024, center=False)\\n\",\n    \"# spec = spec_to_mel()\\n\",\n    \"spec = torch.squeeze(spec, 0)\\n\",\n    \"mel = spec_to_mel(spec, 1024, 80, 16000, 0, None)\\n\",\n    \"\\n\",\n    \"fig = plt.figure(figsize=(10, 8))\\n\",\n    \"ax2 = fig.add_subplot(211)\\n\",\n    \"im = ax2.imshow(mel, interpolation=\\\"none\\\")\"\n   ]\n  },\n  {\n   \"attachments\": {},\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"æƒ…æ„Ÿèšç±»\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"\\n\",\n    \"# from sklearn import metrics\\n\",\n    \"# from sklearn.mixture import GaussianMixture  # é«˜æ–¯æ··åˆæ¨¡å‹\\n\",\n    \"import os\\n\",\n    \"import numpy as np\\n\",\n    \"import librosa\\n\",\n    \"import IPython.display as ipd\\n\",\n    \"from random import sample\\n\",\n    \"\\n\",\n    \"embs = []\\n\",\n    \"wavnames = []\\n\",\n    \"emo_root_path = \\\"D:\\\\\\\\audiodata\\\\\\\\SV2TTS\\\\\\\\synthesizer\\\\\\\\emo\\\\\\\\\\\"\\n\",\n    \"wav_root_path = \\\"D:\\\\\\\\audiodata\\\\\\\\aidatatang_200zh\\\\\\\\corpus\\\\\\\\train\\\\\\\\\\\"\\n\",\n    \"for idx, emo_fpath in enumerate(sample(os.listdir(emo_root_path), 10000)):\\n\",\n    \"    if emo_fpath.endswith(\\\".npy\\\") and emo_fpath.startswith(\\\"emo-T\\\"):\\n\",\n    \"        embs.append(np.expand_dims(np.load(emo_root_path + emo_fpath), axis=0))\\n\",\n    \"        wav_fpath = wav_root_path + emo_fpath[9:14] + \\\"\\\\\\\\\\\" + emo_fpath.split(\\\"_00\\\")[0][4:]\\n\",\n    \"        wavnames.append(wav_fpath)\\n\",\n    \"print(len(embs))\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"x = np.concatenate(embs, axis=0)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# èšç±»ç®—æ³•ç±»çš„æ•°é‡\\n\",\n    \"n_clusters = 20\\n\",\n    \"from sklearn.cluster import *\\n\",\n    \"# model = KMeans(n_clusters=n_clusters, random_state=10)\\n\",\n    \"# model = DBSCAN(eps=0.002, min_samples=2)\\n\",\n    \"# å¯ä»¥è‡ªè¡Œå°è¯•å„ç§ä¸åŒçš„èšç±»ç®—æ³•\\n\",\n    \"# model = Birch(n_clusters= n_clusters, threshold= 0.2)\\n\",\n    \"# model = SpectralClustering(n_clusters=n_clusters)\\n\",\n    \"model = AgglomerativeClustering(n_clusters= n_clusters)\\n\",\n    \"import random\\n\",\n    \"\\n\",\n    \"y_predict = model.fit_predict(x)\\n\",\n    \"\\n\",\n    \"def disp(wavname):\\n\",\n    \"    wav, sr =librosa.load(wavname, 16000)\\n\",\n    \"    display(ipd.Audio(wav, rate=sr))\\n\",\n    \"\\n\",\n    \"classes=[[] for i in range(y_predict.max()+1)]\\n\",\n    \"\\n\",\n    \"for idx, wavname in enumerate(wavnames):\\n\",\n    \"    classes[y_predict[idx]].append(wavname)\\n\",\n    \"\\n\",\n    \"for i in range(y_predict.max()+1):\\n\",\n    \"    print(\\\"ç±»åˆ«:\\\", i, \\\"æœ¬ç±»ä¸­æ ·æœ¬æ•°é‡:\\\", len(classes[i]))\\n\",\n    \"    \\\"\\\"\\\"æ¯ä¸€ä¸ªç±»åªé¢„è§ˆ2æ¡éŸ³é¢‘\\\"\\\"\\\"\\n\",\n    \"    for j in range(2):\\n\",\n    \"        idx = random.randint(0, len(classes[i]) - 1)\\n\",\n    \"        print(classes[i][idx])\\n\",\n    \"        disp(classes[i][idx])\"\n   ]\n  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"mo\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.9.7\"\n  },\n  \"vscode\": {\n   \"interpreter\": {\n    \"hash\": \"788ab866da3baa6c99886d56abb59fe71b6a552bf52c65473ecf96c784704db8\"\n   }\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 4\n}\n"
        },
        {
          "name": "web.py",
          "type": "blob",
          "size": 0.53,
          "content": "import os\nimport sys\nimport typer\n\ncli = typer.Typer()\n\n@cli.command()\ndef launch(port: int = typer.Option(8080, \"--port\", \"-p\")) -> None:\n    \"\"\"Start a graphical UI server for the opyrator.\n\n    The UI is auto-generated from the input- and output-schema of the given function.\n    \"\"\"\n    # Add the current working directory to the sys path\n    # This is required to resolve the opyrator path\n    sys.path.append(os.getcwd())\n\n    from control.mkgui.base.ui.streamlit_ui import launch_ui\n    launch_ui(port)\n\nif __name__ == \"__main__\":\n    cli()"
        }
      ]
    }
  ]
}