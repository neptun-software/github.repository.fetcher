{
  "metadata": {
    "timestamp": 1736711913265,
    "page": 308,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjMxMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "spatie/crawler",
      "stars": 2568,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".editorconfig",
          "type": "blob",
          "size": 0.21484375,
          "content": "root = true\n\n[*]\ncharset = utf-8\nindent_size = 4\nindent_style = space\nend_of_line = lf\ninsert_final_newline = true\ntrim_trailing_whitespace = true\n\n[*.md]\ntrim_trailing_whitespace = false\n\n[*.{yml,yaml}]\nindent_size = 2\n"
        },
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0.5517578125,
          "content": "# Path-based git attributes\n# https://www.kernel.org/pub/software/scm/git/docs/gitattributes.html\n\n# Ignore all test and documentation with \"export-ignore\".\n/.github            export-ignore\n/.gitattributes     export-ignore\n/.gitignore         export-ignore\n/.editorconfig      export-ignore\n/.travis.yml        export-ignore\n/phpunit.xml.dist   export-ignore\n/.scrutinizer.yml   export-ignore\n/tests              export-ignore\n/CrawlRealSite.php  export-ignore\n/.php_cs.dist.php   export-ignore\n/CHANGELOG.md       export-ignore\nUPGRADING.md        export-ignore\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.111328125,
          "content": ".idea\n.php_cs\n.php_cs.cache\n.phpunit.result.cache\nbuild\ncomposer.lock\ncoverage\ndocs\npsalm.xml\nvendor\nnode_modules\n"
        },
        {
          "name": "CHANGELOG.md",
          "type": "blob",
          "size": 11.078125,
          "content": "# Changelog\n\nAll notable changes to `spatie/crawler` will be documented in this file.\n\n## 8.4.0 - 2024-12-16\n\n### What's Changed\n\n* Add execution time limit by @VincentLanglet in https://github.com/spatie/crawler/pull/480\n\n### New Contributors\n\n* @VincentLanglet made their first contribution in https://github.com/spatie/crawler/pull/480\n\n**Full Changelog**: https://github.com/spatie/crawler/compare/8.3.1...8.4.0\n\n## 8.3.1 - 2024-12-09\n\n### What's Changed\n\n* Upgrade spatie/browsershot to 5.0 by @hasansoyalan in https://github.com/spatie/crawler/pull/478\n\n### New Contributors\n\n* @hasansoyalan made their first contribution in https://github.com/spatie/crawler/pull/478\n\n**Full Changelog**: https://github.com/spatie/crawler/compare/8.3.0...8.3.1\n\n## 8.3.0 - 2024-12-02\n\n### What's Changed\n\n* Add support for PHP 8.4 by @pascalbaljet in https://github.com/spatie/crawler/pull/477\n\n**Full Changelog**: https://github.com/spatie/crawler/compare/8.2.3...8.3.0\n\n## 8.2.3 - 2024-07-31\n\n### What's Changed\n\n* Fix setParsableMimeTypes() by @superpenguin612 in https://github.com/spatie/crawler/pull/470\n\n**Full Changelog**: https://github.com/spatie/crawler/compare/8.2.2...8.2.3\n\n## 8.2.1 - 2024-07-16\n\n### What's Changed\n\n* Check original URL against depth tree when visited link is a redirect by @superpenguin612 in https://github.com/spatie/crawler/pull/467\n\n### New Contributors\n\n* @superpenguin612 made their first contribution in https://github.com/spatie/crawler/pull/467\n\n**Full Changelog**: https://github.com/spatie/crawler/compare/8.2.0...8.2.1\n\n## 8.2.0 - 2024-02-15\n\n### What's Changed\n\n* Fix wording in documentation by @adamtomat in https://github.com/spatie/crawler/pull/460\n* Add Laravel/Illuminate 11 Support by @Jubeki in https://github.com/spatie/crawler/pull/461\n\n### New Contributors\n\n* @adamtomat made their first contribution in https://github.com/spatie/crawler/pull/460\n* @Jubeki made their first contribution in https://github.com/spatie/crawler/pull/461\n\n**Full Changelog**: https://github.com/spatie/crawler/compare/8.1.0...8.2.0\n\n## 8.1.0 - 2024-01-02\n\n### What's Changed\n\n* feat: custom link parser by @Velka-DEV in https://github.com/spatie/crawler/pull/458\n\n### New Contributors\n\n* @Velka-DEV made their first contribution in https://github.com/spatie/crawler/pull/458\n\n**Full Changelog**: https://github.com/spatie/crawler/compare/8.0.4...8.1.0\n\n## 8.0.4 - 2023-12-29\n\n- allow Browsershot v4\n\n## 8.0.3 - 2023-11-22\n\n### What's Changed\n\n- Fix return type by @riesjart in https://github.com/spatie/crawler/pull/452\n\n### New Contributors\n\n- @riesjart made their first contribution in https://github.com/spatie/crawler/pull/452\n\n**Full Changelog**: https://github.com/spatie/crawler/compare/8.0.2...8.0.3\n\n## 8.0.2 - 2023-11-20\n\n### What's Changed\n\n- Define only needed methods in observer implementation by @buismaarten in https://github.com/spatie/crawler/pull/449\n\n### New Contributors\n\n- @buismaarten made their first contribution in https://github.com/spatie/crawler/pull/449\n\n**Full Changelog**: https://github.com/spatie/crawler/compare/8.0.1...8.0.2\n\n## 8.0.1 - 2023-07-19\n\n### What's Changed\n\n- Check if rel attribute contains nofollow by @robbinbenard in https://github.com/spatie/crawler/pull/445\n\n### New Contributors\n\n- @robbinbenard made their first contribution in https://github.com/spatie/crawler/pull/445\n\n**Full Changelog**: https://github.com/spatie/crawler/compare/8.0.0...8.0.1\n\n## 8.0.0 - 2023-06-04\n\n- add linkText to crawl observer methods\n- upgrade dependencies\n\n## 7.1.3 - 2023-01-24\n\n- support Laravel 10\n\n## 7.1.2 - 2022-05-30\n\n### What's Changed\n\n- Feat/convert phpunit tests to pest by @mansoorkhan96 in https://github.com/spatie/crawler/pull/401\n- Add the ability to change the default baseUrl scheme by @arnissolle in https://github.com/spatie/crawler/pull/402\n\n### New Contributors\n\n- @arnissolle made their first contribution in https://github.com/spatie/crawler/pull/402\n\n**Full Changelog**: https://github.com/spatie/crawler/compare/7.1.1...7.1.2\n\n## 7.1.1 - 2022-03-20\n\n## What's Changed\n\n- Fix issue #395 by @BrokenSourceCode in https://github.com/spatie/crawler/pull/396\n\n## New Contributors\n\n- @BrokenSourceCode made their first contribution in https://github.com/spatie/crawler/pull/396\n\n**Full Changelog**: https://github.com/spatie/crawler/compare/7.1.0...7.1.1\n\n## 7.1.0 - 2022-01-14\n\n- allow Laravel 9 collections\n\n## 7.0.5 - 2021-11-15\n\n## What's Changed\n\n- Keep only guzzlehttp/psr7 v2.0 by @flangofas in https://github.com/spatie/crawler/pull/392\n\n## New Contributors\n\n- @flangofas made their first contribution in https://github.com/spatie/crawler/pull/392\n\n**Full Changelog**: https://github.com/spatie/crawler/compare/7.0.4...7.0.5\n\n## 7.0.2 - 2021-09-14\n\n- allow psr7 v2\n\n## 7.0.1 - 2021-08-01\n\n- change response type hint (#371)\n\n## 7.0.0 - 2021-04-27\n\n- require PHP 8+\n- drop support for PHP 7.x\n- convert syntax to PHP 8\n- no API changes have been made\n\n## 6.0.1 - 2021-02-26\n\n- bugfix: infinite loops when a CrawlProfile prevents crawling (#358)\n\n## 6.0.0 - 2020-12-02\n\n- add `setCurrentCrawlLimit` and `setTotalCrawlLimit`\n- internal refactors\n\n## 5.0.2 - 2020-11-27\n\n- add support for PHP 8.0\n\n## 5.0.1 - 2020-10-09\n\n- tweak variable naming in `ArrayCrawlQueue` (#326)\n\n## 5.0.0 - 2020-09-29\n\n- improve chucked reading of response\n- move observer / profiles / queues to separate namespaces\n- typehint all the things\n- use laravel/collections instead of tightenco package\n- remove support for anything below PHP 7.4\n- remove all deprecated functions and classes\n\n## 4.7.5 - 2020-09-12\n\n- treat connection exceptions as request exceptions\n\n## 4.7.4 - 2020-07-15\n\n- fix: method and property name error (#311)\n\n## 4.7.3 - 2020-07-15\n\n- add crawler option to allow crawl links with rel=\"nofollow\" (#310)\n\n## 4.7.2 - 2020-05-06\n\n- only crawl links that are completely parsed\n\n## 4.7.1 - 2020-04-14\n\n- fix curl streaming responses (#295)\n\n## 4.7.0 - 2020-04-14\n\n- add `setParseableMimeTypes()` (#293)\n\n## 4.6.9 - 2020-04-11\n\n- fix LinkAdder not receiving the updated DOM (#292)\n\n## 4.6.8 - 2020-03-12\n\n- allow tightenco/collect 7 (#282)\n\n## 4.6.7 - 2020-03-09\n\n- respect maximum response size when checking Robots Meta tags (#281)\n\n## 4.6.6 - 2020-01-30\n\n- allow Guzzle 7\n\n## 4.6.5 - 2019-11-23\n\n- allow symfony 5 components\n\n## 4.6.4 - 2019-09-10\n\n- allow tightenco/collect 6.0 and up (#261)\n\n## 4.6.3 - 2019-09-09\n\n- fix crash when `CrawlRequestFailed` receives an exception other than `RequestException`\n\n## 4.6.2 - 2019-08-08\n\n- case-insensitive user agent bugfix (#249)\n\n## 4.6.1 - 2019-08-08\n\n- fix bugs in `hasAlreadyBeenProcessed`\n\n## 4.6.0 - 2019-08-07\n\n**THIS VERSION CONTAINS A CRITICAL BUG, DO NOT USE**\n\n- added `ArrayCrawlQueue`; this is now the default queue\n- deprecated `CollectionCrawlQueue`\n\n## 4.5.0 - 2019-07-22\n\n- Make user agent configurable (#246)\n\n## 4.4.3 - 2019-06-22\n\n- `delayBetweenRequests` now uses `int` instead of `float` everywhere\n\n## 4.4.2 - 2019-06-20\n\n- remove incorrect docblock\n\n## 4.4.1 - 2019-06-06\n\n- handle relative paths after redirects correctly\n\n## 4.4.0 - 2019-04-05\n\n- add `getUrls` and `getPendingUrls`\n\n## 4.3.2 - 2019-04-04\n\n- Respect maximumDepth in combination with robots (#181)\n\n## 4.3.1 - 2019-04-03\n\n- Properly handle `noindex,follow` urls.\n\n## 4.3.0 - 2019-03-11\n\n- added capability of crawling links with rel= next or prev\n\n## 4.2.0 - 2018-10-31\n\n- add `setDelayBetweenRequests`\n\n## 4.1.7 - 2018-07-27\n\n- fix an issue where the node in the depthtree could be null\n\n## 4.1.6 - 2018-06-26\n\n- improve performance by only building the depth three when needed\n- handlers will get html after JavaScript has been processed\n\n## 4.1.5 - 2018-06-25\n\n- refactor to improve extendability\n\n## 4.1.4 - 2018-06-09\n\n- always add links to pool if robots shouldn't be respected\n\n## 4.1.3 - 2018-06-05\n\n- refactor of internals\n\n## 4.1.2 - 2018-06-03\n\n- make it possible to override `$defaultClientOptions`\n\n## 4.1.1 - 2018-05-22\n\n- Bump minimum required version of `spatie/robots-txt` to `1.0.1`.\n\n## 4.1.0 - 2018-05-08\n\n- Respect robots.txt\n\n## 4.0.5 - 2018-04-30\n\n- improved extensibility by removing php native type hinting of url, queue and crawler pool Closures\n\n## 4.0.4 - 2018-03-20\n\n- do not follow links that have attribute `rel` set to `nofollow`\n\n## 4.0.3 - 2018-03-02\n\n- Support both `Illuminate`'s and `Tighten`'s `Collection`.\n\n## 4.0.2 - 2018-03-01\n\n- fix bugs when installing into a Laravel app\n\n## 4.0.0 - 2018-03-01\n\n- the `CrawlObserver` and `CrawlProfile` are upgraded from interfaces to abstract classes\n- don't crawl `tel:` links\n\n## 3.2.1 - 2018-02-21\n\n- fix endless loop\n\n## 3.2.0 - 2018-01-25\n\n- add `setCrawlObservers`, `addCrawlObserver`\n\n## 3.1.3 - 2018-01-19\n\n- fix `setMaximumResponseSize` (someday we'll get this right)\n\n## 3.1.2 - 2018-01-19\n\n**CONTAINS BUGS, DO NOT USE THIS VERSION**\n\n- fix `setMaximumResponseSize`\n\n## 3.1.1 - 2018-01-17\n\n**CONTAINS BUGS, DO NOT USE THIS VERSION**\n\n- fix `setMaximumResponseSize`\n\n## 3.1.0 - 2018-01-11\n\n**CONTAINS BUGS, DO NOT USE THIS VERSION**\n\n- add `setMaximumResponseSize`\n\n## 3.0.1 - 2018-01-02\n\n- fix for exception being thrown when encountering a malformatted url\n\n## 3.0.0 - 2017-12-22\n\n- use `\\Psr\\Http\\Message\\UriInterface` for all urls\n- use Puppeteer\n- drop support from PHP 7.0\n\n## 2.7.1 - 2017-12-13\n\n- allow symfony 4 crawler\n\n## 2.7.0 - 2017-12-10\n\n- added the ability to change the crawl queue\n\n## 2.6.2 - 2017-12-10\n\n- more performance improvements\n\n## 2.6.1 - 2017-12-10\n\n- performance improvements\n\n## 2.6.0 - 2017-10-16\n\n- add `CrawlSubdomains` profile\n\n## 2.5.0 - 2017-09-27\n\n- add crawl count limit\n\n## 2.4.0 - 2017-09-21\n\n- add depth limit\n\n## 2.3.0 - 2017-09-21\n\n- add JavaScript execution\n\n## 2.2.1 - 2017-09-07\n\n- fix deps for PHP 7.2\n\n## 2.2.0 - 2017-08-03\n\n- add `EmptyCrawlObserver`\n\n## 2.1.2 - 2017-03-06\n\n- refactor to make use of Symfony Crawler's `link` function\n\n## 2.1.1 - 2017-03-03\n\n- fix bugs around relative urls\n\n## 2.1.0 - 2017-01-27\n\n- add `CrawlInternalUrls`\n\n## 2.0.7 - 2016-12-30\n\n- make sure the passed client options are being used\n\n## 2.0.6 - 2016-12-15\n\n- second attempt to fix detection of redirects\n\n## 2.0.5 - 2016-12-15\n\n- fix detection of redirects\n\n## 2.0.4 - 2016-12-15\n\n- fix the default timeout of 5 seconds\n\n## 2.0.3 - 2016-12-13\n\n- set a default timeout of 5 seconds\n\n## 2.0.2 - 2016-12-05\n\n- fix for non responding hosts\n\n## 2.0.1 - 2016-12-05\n\n- fix for the accidental crawling of mailto-links\n\n## 2.0.0 - 2016-12-05\n\n- improve performance by concurrent crawling\n- make it possible to determine on which url a url was found\n\n## 1.3.1 - 2015-09-13\n\n- Ignore `tel:` links when crawling\n\n## 1.3.0 - 2015-08-18\n\n- Added `path`, `segment` and `segments` functions to `Url`\n\n## 1.2.3 - 2015-08-12\n\n- Updated the required version of Guzzle to a secure version\n\n## 1.2.2 - 2015-03-08\n\n- Fixed a bug where the crawler would not take query strings into account\n\n## 1.2.1 - 2015-03-01\n\n- Fixed a bug where the crawler tries to follow JavaScript links\n\n## 1.2.0 - 2015-12-21\n\n- Add support for DomCrawler 3.x\n\n## 1.1.1 - 2015-11-16\n\n- Fix for normalizing relative links when using non-80 ports\n\n## 1.1.0 - 2015-11-16\n\n- Add support for custom ports\n\n## 1.0.2 - 2015-11-05\n\n- Lower required php version to 5.5\n\n## 1.0.1 - 2015-11-03\n\n- Make url's case sensitive\n\n## 1.0.0 - 2015-11-03\n\n- First release\n"
        },
        {
          "name": "LICENSE.md",
          "type": "blob",
          "size": 1.064453125,
          "content": "The MIT License (MIT)\n\nCopyright (c) Spatie bvba <info@spatie.be>\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 16.361328125,
          "content": "# ðŸ•¸ Crawl the web using PHP ðŸ•·\n\n[![Latest Version on Packagist](https://img.shields.io/packagist/v/spatie/crawler.svg?style=flat-square)](https://packagist.org/packages/spatie/crawler)\n[![MIT Licensed](https://img.shields.io/badge/license-MIT-brightgreen.svg?style=flat-square)](LICENSE.md)\n![Tests](https://github.com/spatie/crawler/workflows/Tests/badge.svg)\n[![Total Downloads](https://img.shields.io/packagist/dt/spatie/crawler.svg?style=flat-square)](https://packagist.org/packages/spatie/crawler)\n\nThis package provides a class to crawl links on a website. Under the hood Guzzle promises are used to [crawl multiple urls concurrently](http://docs.guzzlephp.org/en/latest/quickstart.html?highlight=pool#concurrent-requests).\n\nBecause the crawler can execute JavaScript, it can crawl JavaScript rendered sites. Under the hood [Chrome and Puppeteer](https://github.com/spatie/browsershot) are used to power this feature.\n\n## Support us\n\n[<img src=\"https://github-ads.s3.eu-central-1.amazonaws.com/crawler.jpg?t=1\" width=\"419px\" />](https://spatie.be/github-ad-click/crawler)\n\nWe invest a lot of resources into creating [best in class open source packages](https://spatie.be/open-source). You can support us by [buying one of our paid products](https://spatie.be/open-source/support-us).\n\nWe highly appreciate you sending us a postcard from your hometown, mentioning which of our package(s) you are using. You'll find our address on [our contact page](https://spatie.be/about-us). We publish all received postcards on [our virtual postcard wall](https://spatie.be/open-source/postcards).\n\n## Installation\n\nThis package can be installed via Composer:\n\n``` bash\ncomposer require spatie/crawler\n```\n\n## Usage\n\nThe crawler can be instantiated like this\n\n```php\nuse Spatie\\Crawler\\Crawler;\n\nCrawler::create()\n    ->setCrawlObserver(<class that extends \\Spatie\\Crawler\\CrawlObservers\\CrawlObserver>)\n    ->startCrawling($url);\n```\n\nThe argument passed to `setCrawlObserver` must be an object that extends the `\\Spatie\\Crawler\\CrawlObservers\\CrawlObserver` abstract class:\n\n```php\nnamespace Spatie\\Crawler\\CrawlObservers;\n\nuse GuzzleHttp\\Exception\\RequestException;\nuse Psr\\Http\\Message\\ResponseInterface;\nuse Psr\\Http\\Message\\UriInterface;\n\nabstract class CrawlObserver\n{\n    /*\n     * Called when the crawler will crawl the url.\n     */\n    public function willCrawl(UriInterface $url, ?string $linkText): void\n    {\n    }\n\n    /*\n     * Called when the crawler has crawled the given url successfully.\n     */\n    abstract public function crawled(\n        UriInterface $url,\n        ResponseInterface $response,\n        ?UriInterface $foundOnUrl = null,\n        ?string $linkText,\n    ): void;\n\n    /*\n     * Called when the crawler had a problem crawling the given url.\n     */\n    abstract public function crawlFailed(\n        UriInterface $url,\n        RequestException $requestException,\n        ?UriInterface $foundOnUrl = null,\n        ?string $linkText = null,\n    ): void;\n\n    /**\n     * Called when the crawl has ended.\n     */\n    public function finishedCrawling(): void\n    {\n    }\n}\n```\n\n### Using multiple observers\n\nYou can set multiple observers with `setCrawlObservers`:\n\n```php\nCrawler::create()\n    ->setCrawlObservers([\n        <class that extends \\Spatie\\Crawler\\CrawlObservers\\CrawlObserver>,\n        <class that extends \\Spatie\\Crawler\\CrawlObservers\\CrawlObserver>,\n        ...\n     ])\n    ->startCrawling($url);\n```\n\nAlternatively you can set multiple observers one by one with `addCrawlObserver`:\n\n```php\nCrawler::create()\n    ->addCrawlObserver(<class that extends \\Spatie\\Crawler\\CrawlObservers\\CrawlObserver>)\n    ->addCrawlObserver(<class that extends \\Spatie\\Crawler\\CrawlObservers\\CrawlObserver>)\n    ->addCrawlObserver(<class that extends \\Spatie\\Crawler\\CrawlObservers\\CrawlObserver>)\n    ->startCrawling($url);\n```\n\n### Executing JavaScript\n\nBy default, the crawler will not execute JavaScript. This is how you can enable the execution of JavaScript:\n\n```php\nCrawler::create()\n    ->executeJavaScript()\n    ...\n```\n\nIn order to make it possible to get the body html after the javascript has been executed, this package depends on\nour [Browsershot](https://github.com/spatie/browsershot) package.\nThis package uses [Puppeteer](https://github.com/puppeteer/puppeteer) under the hood. Here are some pointers on [how to install it on your system](https://spatie.be/docs/browsershot/v2/requirements).\n\nBrowsershot will make an educated guess as to where its dependencies are installed on your system.\nBy default, the Crawler will instantiate a new Browsershot instance. You may find the need to set a custom created instance using the `setBrowsershot(Browsershot $browsershot)` method.\n\n```php\nCrawler::create()\n    ->setBrowsershot($browsershot)\n    ->executeJavaScript()\n    ...\n```\n\nNote that the crawler will still work even if you don't have the system dependencies required by Browsershot.\nThese system dependencies are only required if you're calling `executeJavaScript()`.\n\n### Filtering certain urls\n\nYou can tell the crawler not to visit certain urls by using the `setCrawlProfile`-function. That function expects\nan object that extends `Spatie\\Crawler\\CrawlProfiles\\CrawlProfile`:\n\n```php\n/*\n * Determine if the given url should be crawled.\n */\npublic function shouldCrawl(UriInterface $url): bool;\n```\n\nThis package comes with three `CrawlProfiles` out of the box:\n\n- `CrawlAllUrls`: this profile will crawl all urls on all pages including urls to an external site.\n- `CrawlInternalUrls`: this profile will only crawl the internal urls on the pages of a host.\n- `CrawlSubdomains`: this profile will only crawl the internal urls and its subdomains on the pages of a host.\n\n### Custom link extraction\n\nYou can customize how links are extracted from a page by passing a custom `UrlParser` to the crawler.\n\n```php\nCrawler::create()\n    ->setUrlParserClass(<class that implements \\Spatie\\Crawler\\UrlParsers\\UrlParser>::class)\n    ...\n```\n\nBy default, the `LinkUrlParser` is used. This parser will extract all links from the `href` attribute of `a` tags.\n\nThere is also a built-in `SitemapUrlParser` that will extract & crawl all links from a sitemap. It does support sitemap index files.\n\n```php\nCrawler::create()\n    ->setUrlParserClass(SitemapUrlParser::class)\n    ...\n```\n\n### Ignoring robots.txt and robots meta\n\nBy default, the crawler will respect robots data. It is possible to disable these checks like so:\n\n```php\nCrawler::create()\n    ->ignoreRobots()\n    ...\n```\n\nRobots data can come from either a `robots.txt` file, meta tags or response headers.\nMore information on the spec can be found here: [http://www.robotstxt.org/](http://www.robotstxt.org/).\n\nParsing robots data is done by our package [spatie/robots-txt](https://github.com/spatie/robots-txt).\n\n### Accept links with rel=\"nofollow\" attribute\n\nBy default, the crawler will reject all links containing attribute rel=\"nofollow\". It is possible to disable these checks like so:\n\n```php\nCrawler::create()\n    ->acceptNofollowLinks()\n    ...\n```\n\n### Using a custom User Agent ###\n\nIn order to respect robots.txt rules for a custom User Agent you can specify your own custom User Agent.\n\n```php\nCrawler::create()\n    ->setUserAgent('my-agent')\n```\n\nYou can add your specific crawl rule group for 'my-agent' in robots.txt. This example disallows crawling the entire site for crawlers identified by 'my-agent'.\n\n```txt\n// Disallow crawling for my-agent\nUser-agent: my-agent\nDisallow: /\n```\n\n## Setting the number of concurrent requests\n\nTo improve the speed of the crawl the package concurrently crawls 10 urls by default. If you want to change that number you can use the `setConcurrency` method.\n\n```php\nCrawler::create()\n    ->setConcurrency(1) // now all urls will be crawled one by one\n```\n\n## Defining Crawl and Time Limits\n\nBy default, the crawler continues until it has crawled every page it can find. This behavior might cause issues if you are working in an environment with limitations such as a serverless environment.\n\nThe crawl behavior can be controlled with the following two options:\n\n - **Total Crawl Limit** (`setTotalCrawlLimit`): This limit defines the maximal count of URLs to crawl.\n - **Current Crawl Limit** (`setCurrentCrawlLimit`): This defines how many URLs are processed during the current crawl.\n - **Total Execution Time Limit** (`setTotalExecutionTimeLimit`): This limit defines the maximal execution time of the crawl.\n - **Current Execution Time Limit** (`setCurrentExecutionTimeLimit`): This limits the execution time of the current crawl.\n\nLet's take a look at some examples to clarify the difference between `setTotalCrawlLimit` and `setCurrentCrawlLimit`.\nThe difference between `setTotalExecutionTimeLimit` and `setCurrentExecutionTimeLimit` will be the same.\n\n### Example 1: Using the total crawl limit\n\nThe `setTotalCrawlLimit` method allows you to limit the total number of URLs to crawl, no matter how often you call the crawler.\n\n```php\n$queue = <your selection/implementation of a queue>;\n\n// Crawls 5 URLs and ends.\nCrawler::create()\n    ->setCrawlQueue($queue)\n    ->setTotalCrawlLimit(5)\n    ->startCrawling($url);\n\n// Doesn't crawl further as the total limit is reached.\nCrawler::create()\n    ->setCrawlQueue($queue)\n    ->setTotalCrawlLimit(5)\n    ->startCrawling($url);\n```\n\n### Example 2: Using the current crawl limit\n\nThe `setCurrentCrawlLimit` will set a limit on how many URls will be crawled per execution. This piece of code will process 5 pages with each execution, without a total limit of pages to crawl.\n\n```php\n$queue = <your selection/implementation of a queue>;\n\n// Crawls 5 URLs and ends.\nCrawler::create()\n    ->setCrawlQueue($queue)\n    ->setCurrentCrawlLimit(5)\n    ->startCrawling($url);\n\n// Crawls the next 5 URLs and ends.\nCrawler::create()\n    ->setCrawlQueue($queue)\n    ->setCurrentCrawlLimit(5)\n    ->startCrawling($url);\n```\n\n### Example 3: Combining the total and crawl limit\n\nBoth limits can be combined to control the crawler:\n\n```php\n$queue = <your selection/implementation of a queue>;\n\n// Crawls 5 URLs and ends.\nCrawler::create()\n    ->setCrawlQueue($queue)\n    ->setTotalCrawlLimit(10)\n    ->setCurrentCrawlLimit(5)\n    ->startCrawling($url);\n\n// Crawls the next 5 URLs and ends.\nCrawler::create()\n    ->setCrawlQueue($queue)\n    ->setTotalCrawlLimit(10)\n    ->setCurrentCrawlLimit(5)\n    ->startCrawling($url);\n\n// Doesn't crawl further as the total limit is reached.\nCrawler::create()\n    ->setCrawlQueue($queue)\n    ->setTotalCrawlLimit(10)\n    ->setCurrentCrawlLimit(5)\n    ->startCrawling($url);\n```\n\n### Example 4: Crawling across requests\n\nYou can use the `setCurrentCrawlLimit` to break up long running crawls. The following example demonstrates a (simplified) approach. It's made up of an initial request and any number of follow-up requests continuing the crawl.\n\n#### Initial Request\n\nTo start crawling across different requests, you will need to create a new queue of your selected queue-driver. Start by passing the queue-instance to the crawler. The crawler will start filling the queue as pages are processed and new URLs are discovered. Serialize and store the queue reference after the crawler has finished (using the current crawl limit).\n\n```php\n// Create a queue using your queue-driver.\n$queue = <your selection/implementation of a queue>;\n\n// Crawl the first set of URLs\nCrawler::create()\n    ->setCrawlQueue($queue)\n    ->setCurrentCrawlLimit(10)\n    ->startCrawling($url);\n\n// Serialize and store your queue\n$serializedQueue = serialize($queue);\n```\n\n#### Subsequent Requests\n\nFor any following requests you will need to unserialize your original queue and pass it to the crawler:\n\n```php\n// Unserialize queue\n$queue = unserialize($serializedQueue);\n\n// Crawls the next set of URLs\nCrawler::create()\n    ->setCrawlQueue($queue)\n    ->setCurrentCrawlLimit(10)\n    ->startCrawling($url);\n\n// Serialize and store your queue\n$serialized_queue = serialize($queue);\n```\n\nThe behavior is based on the information in the queue. Only if the same queue-instance is passed in the behavior works as described. When a completely new queue is passed in, the limits of previous crawls -even for the same website- won't apply.\n\nAn example with more details can be found [here](https://github.com/spekulatius/spatie-crawler-cached-queue-example).\n\n## Setting the maximum crawl depth\n\nBy default, the crawler continues until it has crawled every page of the supplied URL. If you want to limit the depth of the crawler you can use the `setMaximumDepth` method.\n\n```php\nCrawler::create()\n    ->setMaximumDepth(2)\n```\n\n## Setting the maximum response size\n\nMost html pages are quite small. But the crawler could accidentally pick up on large files such as PDFs and MP3s. To keep memory usage low in such cases the crawler will only use the responses that are smaller than 2 MB. If, when streaming a response, it becomes larger than 2 MB, the crawler will stop streaming the response. An empty response body will be assumed.\n\nYou can change the maximum response size.\n\n```php\n// let's use a 3 MB maximum.\nCrawler::create()\n    ->setMaximumResponseSize(1024 * 1024 * 3)\n```\n\n## Add a delay between requests\n\nIn some cases you might get rate-limited when crawling too aggressively. To circumvent this, you can use the `setDelayBetweenRequests()` method to add a pause between every request. This value is expressed in milliseconds.\n\n```php\nCrawler::create()\n    ->setDelayBetweenRequests(150) // After every page crawled, the crawler will wait for 150ms\n```\n\n## Limiting which content-types to parse\n\nBy default, every found page will be downloaded (up to `setMaximumResponseSize()` in size) and parsed for additional links. You can limit which content-types should be downloaded and parsed by setting the `setParseableMimeTypes()` with an array of allowed types.\n\n```php\nCrawler::create()\n    ->setParseableMimeTypes(['text/html', 'text/plain'])\n```\n\nThis will prevent downloading the body of pages that have different mime types, like binary files, audio/video, ... that are unlikely to have links embedded in them. This feature mostly saves bandwidth.\n\n## Using a custom crawl queue\n\nWhen crawling a site the crawler will put urls to be crawled in a queue. By default, this queue is stored in memory using the built-in `ArrayCrawlQueue`.\n\nWhen a site is very large you may want to store that queue elsewhere, maybe a database. In such cases, you can write your own crawl queue.\n\nA valid crawl queue is any class that implements the `Spatie\\Crawler\\CrawlQueues\\CrawlQueue`-interface. You can pass your custom crawl queue via the `setCrawlQueue` method on the crawler.\n\n```php\nCrawler::create()\n    ->setCrawlQueue(<implementation of \\Spatie\\Crawler\\CrawlQueues\\CrawlQueue>)\n```\n\nHere\n\n- [ArrayCrawlQueue](https://github.com/spatie/crawler/blob/master/src/CrawlQueues/ArrayCrawlQueue.php)\n- [RedisCrawlQueue (third-party package)](https://github.com/repat/spatie-crawler-redis)\n- [CacheCrawlQueue for Laravel (third-party package)](https://github.com/spekulatius/spatie-crawler-toolkit-for-laravel)\n- [Laravel Model as Queue (third-party example app)](https://github.com/insign/spatie-crawler-queue-with-laravel-model)\n\n## Change the default base url scheme\n\nBy default, the crawler will set the base url scheme to `http` if none. You have the ability to change that with `setDefaultScheme`.\n\n```php\nCrawler::create()\n    ->setDefaultScheme('https')\n```\n\n## Changelog\n\nPlease see [CHANGELOG](CHANGELOG.md) for more information what has changed recently.\n\n## Contributing\n\nPlease see [CONTRIBUTING](https://github.com/spatie/.github/blob/main/CONTRIBUTING.md) for details.\n\n## Testing\n\nFirst, install the Puppeteer dependency, or your tests will fail.\n\n```\nnpm install puppeteer\n```\n\nTo run the tests you'll have to start the included node based server first in a separate terminal window.\n\n```bash\ncd tests/server\nnpm install\nnode server.js\n```\n\nWith the server running, you can start testing.\n```bash\ncomposer test\n```\n\n## Security\n\nIf you've found a bug regarding security please mail [security@spatie.be](mailto:security@spatie.be) instead of using the issue tracker.\n\n## Postcardware\n\nYou're free to use this package, but if it makes it to your production environment we highly appreciate you sending us a postcard from your hometown, mentioning which of our package(s) you are using.\n\nOur address is: Spatie, Kruikstraat 22, 2018 Antwerp, Belgium.\n\nWe publish all received postcards [on our company website](https://spatie.be/en/opensource/postcards).\n\n## Credits\n\n- [Freek Van der Herten](https://github.com/freekmurze)\n- [All Contributors](../../contributors)\n\n## License\n\nThe MIT License (MIT). Please see [License File](LICENSE.md) for more information.\n"
        },
        {
          "name": "UPGRADING.md",
          "type": "blob",
          "size": 0.974609375,
          "content": "# Upgrading\n\n## From v7 to v8\n\nThe signatures of the `willCrawl`, `crawled` and `crawlFailed` all gained an extra parameter `$linkText` `CrawlObserver`. You should add that parameter to the methods in your custom crawl observers.\n\n## From v5 to v6\n\n- There are no breaking changes to the API. Internally, we shuffled around some checks around crawl limit that might affected some edge cases\n\n## From v3 to v4\n\n- The `CrawlObserver` and `CrawlProfile` are upgraded from interfaces to abstract classes, so you have to convert your old observers and profiles. `crawled` now receives every successfully crawled uri, `crawlFailed` every failed one.\n\n## From v2 to v3\n\n- PHP 7.1 is now required as a minimum version.\n- Instead of using our custom `\\Spatie\\Crawler\\Url` object, we're now using the `Psr\\Http\\Message\\UriInterface`. \nCustom Profiles and Observers will need to be changed to have the correct arguments and return types.\nWe're using `\\GuzzleHttp\\Psr7\\Uri` as the concrete URI implementation.\n"
        },
        {
          "name": "composer.json",
          "type": "blob",
          "size": 1.2236328125,
          "content": "{\n    \"name\": \"spatie/crawler\",\n    \"description\": \"Crawl all internal links found on a website\",\n    \"keywords\": [\n        \"spatie\",\n        \"crawler\",\n        \"link\",\n        \"website\"\n    ],\n    \"homepage\": \"https://github.com/spatie/crawler\",\n    \"license\": \"MIT\",\n    \"authors\": [\n        {\n            \"name\": \"Freek Van der Herten\",\n            \"email\": \"freek@spatie.be\"\n        }\n    ],\n    \"require\": {\n        \"php\": \"^8.1\",\n        \"guzzlehttp/guzzle\": \"^7.3\",\n        \"guzzlehttp/psr7\": \"^2.0\",\n        \"illuminate/collections\": \"^10.0|^11.0\",\n        \"nicmart/tree\": \"^0.9\",\n        \"spatie/browsershot\": \"^3.45|^4.0|^5.0\",\n        \"spatie/robots-txt\": \"^2.0\",\n        \"symfony/dom-crawler\": \"^6.0|^7.0\"\n    },\n    \"require-dev\": {\n        \"pestphp/pest\": \"^2.0\",\n        \"spatie/ray\": \"^1.37\"\n    },\n    \"config\": {\n        \"sort-packages\": true,\n        \"allow-plugins\": {\n            \"pestphp/pest-plugin\": true,\n            \"phpstan/extension-installer\": true\n        }\n    },\n    \"autoload\": {\n        \"psr-4\": {\n            \"Spatie\\\\Crawler\\\\\": \"src\"\n        }\n    },\n    \"autoload-dev\": {\n        \"psr-4\": {\n            \"Spatie\\\\Crawler\\\\Test\\\\\": \"tests\"\n        }\n    },\n    \"minimum-stability\": \"dev\",\n    \"prefer-stable\": true\n}\n"
        },
        {
          "name": "phpunit.xml",
          "type": "blob",
          "size": 0.474609375,
          "content": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<phpunit xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n         xsi:noNamespaceSchemaLocation=\"vendor/phpunit/phpunit/phpunit.xsd\"\n         bootstrap=\"vendor/autoload.php\"\n         colors=\"true\"\n>\n    <testsuites>\n        <testsuite name=\"all\">\n            <directory>tests</directory>\n        </testsuite>\n    </testsuites>\n    <source>\n        <include>\n            <directory>src</directory>\n        </include>\n    </source>\n</phpunit>\n"
        },
        {
          "name": "src",
          "type": "tree",
          "content": null
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}