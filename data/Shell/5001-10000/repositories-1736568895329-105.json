{
  "metadata": {
    "timestamp": 1736568895329,
    "page": 105,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjExMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "six2dez/reconftw",
      "stars": 5871,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.14453125,
          "content": "#Ignoring output directories\nRecon/\noutput/\n.obsidian/\ntest/\n.trunk/\n\n#Ignoring compressed files\n*.tar\n*.tar.*\n*.zip\n.DS_Store\nreconftw.cfg-personal"
        },
        {
          "name": "CODE_OF_CONDUCT.md",
          "type": "blob",
          "size": 3.2568359375,
          "content": "# Contributor Covenant Code of Conduct\n\n## Our Pledge\n\nIn the interest of fostering an open and welcoming environment, we as\ncontributors and maintainers pledge to making participation in our project and\nour community a harassment-free experience for everyone, regardless of age, body\nsize, disability, ethnicity, sex characteristics, gender identity and expression,\nlevel of experience, education, socio-economic status, nationality, personal\nappearance, race, religion, or sexual identity and orientation.\n\n## Our Standards\n\nExamples of behavior that contributes to creating a positive environment\ninclude:\n\n* Using welcoming and inclusive language\n* Being respectful of differing viewpoints and experiences\n* Gracefully accepting constructive criticism\n* Focusing on what is best for the community\n* Showing empathy towards other community members\n\nExamples of unacceptable behavior by participants include:\n\n* The use of sexualized language or imagery and unwelcome sexual attention or\n advances\n* Trolling, insulting/derogatory comments, and personal or political attacks\n* Public or private harassment\n* Publishing others' private information, such as a physical or electronic\n address, without explicit permission\n* Other conduct which could reasonably be considered inappropriate in a\n professional setting\n\n## Our Responsibilities\n\nProject maintainers are responsible for clarifying the standards of acceptable\nbehavior and are expected to take appropriate and fair corrective action in\nresponse to any instances of unacceptable behavior.\n\nProject maintainers have the right and responsibility to remove, edit, or\nreject comments, commits, code, wiki edits, issues, and other contributions\nthat are not aligned to this Code of Conduct, or to ban temporarily or\npermanently any contributor for other behaviors that they deem inappropriate,\nthreatening, offensive, or harmful.\n\n## Scope\n\nThis Code of Conduct applies both within project spaces and in public spaces\nwhen an individual is representing the project or its community. Examples of\nrepresenting a project or community include using an official project e-mail\naddress, posting via an official social media account, or acting as an appointed\nrepresentative at an online or offline event. Representation of a project may be\nfurther defined and clarified by project maintainers.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported by contacting me at six2dez@gmail.com. All\ncomplaints will be reviewed and investigated and will result in a response that\nis deemed necessary and appropriate to the circumstances. The project team is\nobligated to maintain confidentiality with regard to the reporter of an incident.\nFurther details of specific enforcement policies may be posted separately.\n\nProject maintainers who do not follow or enforce the Code of Conduct in good\nfaith may face temporary or permanent repercussions as determined by other\nmembers of the project's leadership.\n\n## Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant][homepage], version 1.4,\navailable at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html\n\n[homepage]: https://www.contributor-covenant.org\n\nFor answers to common questions about this code of conduct, see\nhttps://www.contributor-covenant.org/faq\n"
        },
        {
          "name": "Docker",
          "type": "tree",
          "content": null
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0390625,
          "content": "MIT License\n\nCopyright (c) 2023 six2dez\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 1.232421875,
          "content": "GH_CLI := $(shell command -v gh 2> /dev/null)\nPRIVATE_REPO := $(shell echo $${PRIV_REPO-reconftw-data})\n\n.PHONY: sync upload bootstrap rm\n\n# bootstrap a private repo to store data\nbootstrap:\n\t@if [ -z $(GH_CLI) ]; then echo \"github cli is missing. please install\"; exit 2; fi\n\tgh repo create $(PRIVATE_REPO) --private\n\tgh repo clone $(PRIVATE_REPO) ~/$(PRIVATE_REPO)\n\tcd ~/$(PRIVATE_REPO) && git commit --allow-empty -m \"Empty commit\" && \\\n\t\tgit remote add upstream https://github.com/six2dez/reconftw && \\\n\t\tgit fetch upstream && \\\n\t\tgit rebase upstream/main $(shell git symbolic-ref refs/remotes/origin/HEAD | sed 's@^refs/remotes/origin/@@') && \\\n\t\tmkdir Recon && \\\n\t\tgit push origin $(shell git symbolic-ref refs/remotes/origin/HEAD | sed 's@^refs/remotes/origin/@@')\n\t@echo \"Done!\"\n\t@echo \"Initialized private repo: $(PRIVATE_REPO)\"\n\nrm:\n\tgh repo delete $(PRIVATE_REPO) --yes\n\trm -rf ~/$(PRIVATE_REPO)\n\nsync:\n\tcd ~/$(PRIVATE_REPO) && git fetch upstream && git rebase upstream/main $(shell git symbolic-ref refs/remotes/origin/HEAD | sed 's@^refs/remotes/origin/@@')\n\nupload:\n\tcd ~/$(PRIVATE_REPO) && \\\n\t\tgit add . && \\\n\t\tgit commit -m \"Data upload\" && \\\n\t\tgit push origin $(shell git symbolic-ref refs/remotes/origin/HEAD | sed 's@^refs/remotes/origin/@@')\n"
        },
        {
          "name": "Proxmox",
          "type": "tree",
          "content": null
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 29.2099609375,
          "content": "<h1 align=\"center\">\r\n  <br>\r\n  <a href=\"https://github.com/six2dez/reconftw\"><img src=\"https://github.com/six2dez/reconftw/blob/main/images/banner.png\" alt=\"reconftw\"></a>\r\n  <br>\r\n  reconFTW\r\n  <br>\r\n</h1>\r\n\r\n<p align=\"center\">\r\n  <a href=\"https://github.com/six2dez/reconftw/releases/tag/v2.9\">\r\n    <img src=\"https://img.shields.io/badge/release-v2.9-green\">\r\n  </a>\r\n   </a>\r\n  <a href=\"https://opensource.org/licenses/MIT\">\r\n      <img src=\"https://img.shields.io/badge/License-MIT-yellow.svg\">\r\n  </a>\r\n  <a href=\"https://twitter.com/Six2dez1\">\r\n    <img src=\"https://img.shields.io/badge/twitter-%40Six2dez1-blue\">\r\n  </a>\r\n    <a href=\"https://github.com/six2dez/reconftw/issues?q=is%3Aissue+is%3Aclosed\">\r\n    <img src=\"https://img.shields.io/github/issues-closed-raw/six2dez/reconftw.svg\">\r\n  </a>\r\n  <a href=\"https://github.com/six2dez/reconftw/wiki\">\r\n    <img src=\"https://img.shields.io/badge/doc-wiki-blue.svg\">\r\n  </a>\r\n  <a href=\"https://t.me/joinchat/H5bAaw3YbzzmI5co\">\r\n    <img src=\"https://img.shields.io/badge/telegram-@ReconFTW-blue.svg\">\r\n  </a>\r\n  <a href=\"https://discord.gg/R5DdXVEdTy\">\r\n    <img src=\"https://img.shields.io/discord/1048623782912340038.svg?logo=discord\">\r\n  </a>\r\n</p>\r\n\r\n<h3 align=\"center\">Summary</h3>\r\n\r\n**reconFTW** automates the entire process of reconnaissance for you. It outperforms the work of subdomain enumeration along with various vulnerability checks and obtaining maximum information about your target.\r\n\r\nreconFTW uses a lot of techniques (passive, bruteforce, permutations, certificate transparency, source code scraping, analytics, DNS records...) for subdomain enumeration which helps you to get the maximum and the most interesting subdomains so that you be ahead of the competition.\r\n\r\nIt also performs various vulnerability checks like XSS, Open Redirects, SSRF, CRLF, LFI, SQLi, SSL tests, SSTI, DNS zone transfers, and much more. Along with these, it performs OSINT techniques, directory fuzzing, dorking, ports scanning, screenshots, nuclei scan on your target.\r\n\r\nSo, what are you waiting for? Go! Go! Go! :boom:\r\n\r\n## üìî Table of Contents\r\n\r\n-----------------\r\n\r\n- [‚öôÔ∏è Config file](#Ô∏è-config-file)\r\n- [Usage](#usage)\r\n  - [TARGET OPTIONS](#target-options)\r\n  - [MODE OPTIONS](#mode-options)\r\n  - [GENERAL OPTIONS](#general-options)\r\n  - [Example Usage](#example-usage)\r\n    - [To perform a full recon on single target](#to-perform-a-full-recon-on-single-target)\r\n    - [To perform a full recon on a list of targets](#to-perform-a-full-recon-on-a-list-of-targets)\r\n    - [Perform full recon with more time intense tasks *(VPS intended only)*](#perform-full-recon-with-more-time-intense-tasks-vps-intended-only)\r\n    - [Perform recon in a multi domain target](#perform-recon-in-a-multi-domain-target)\r\n    - [Perform recon with axiom integration](#perform-recon-with-axiom-integration)\r\n    - [Perform all steps (whole recon + all attacks) a.k.a. YOLO mode](#perform-all-steps-whole-recon--all-attacks-aka-yolo-mode)\r\n    - [Show help section](#show-help-section)\r\n- [Axiom Support :cloud:](#axiom-support-cloud)\r\n- [Sample video](#sample-video)\r\n- [:fire: Features :fire:](#fire-features-fire)\r\n  - [Osint](#osint)\r\n  - [Subdomains](#subdomains)\r\n  - [Hosts](#hosts)\r\n  - [Webs](#webs)\r\n  - [Vulnerability checks](#vulnerability-checks)\r\n  - [Extras](#extras)\r\n  - [Mindmap/Workflow](#mindmapworkflow)\r\n  - [Data Keep](#data-keep)\r\n    - [Makefile](#makefile)\r\n    - [Manual](#manual)\r\n    - [Main commands](#main-commands)\r\n  - [How to contribute](#how-to-contribute)\r\n  - [Need help? :information\\_source:](#need-help-information_source)\r\n  - [Support this project](#support-this-project)\r\n    - [Buymeacoffee](#buymeacoffee)\r\n    - [DigitalOcean referral link](#digitalocean-referral-link)\r\n    - [GitHub sponsorship](#github-sponsorship)\r\n  - [Thanks :pray:](#thanks-pray)\r\n  - [Disclaimer](#disclaimer)\r\n\r\n-----------------\r\n\r\n## üíø Installation\r\n\r\n## a) Using a PC/VPS/VM\r\n\r\n> You can check out our wiki for the installation guide [Installation Guide](https://github.com/six2dez/reconftw/wiki/0.-Installation-Guide) :book:\r\n\r\n- Requires [Golang](https://golang.org/dl/) > **1.15.0+** installed and paths correctly set (**$GOPATH**, **$GOROOT**)\r\n\r\nImportant: if you are not running reconftw as root, run `sudo echo \"${USERNAME}  ALL=(ALL:ALL) NOPASSWD: ALL\" | sudo tee -a /etc/sudoers.d/reconFTW`, to make sure no sudo prompts are required to run the tool and to avoid any permission issues.\r\n\r\n```bash\r\ngit clone https://github.com/six2dez/reconftw\r\ncd reconftw/\r\n./install.sh\r\n./reconftw.sh -d target.com -r\r\n```\r\n\r\n## b) Docker Image üê≥ (3 options)\r\n\r\n- Pull the image\r\n\r\n```bash\r\ndocker pull six2dez/reconftw:main\r\n```\r\n\r\n- Run the container\r\n\r\n```bash\r\ndocker run -it --rm \\\r\n-v \"${PWD}/OutputFolder/\":'/reconftw/Recon/' \\\r\nsix2dez/reconftw:main -d example.com -r\r\n```\r\n\r\n- View results (they're NOT in the Docker container)\r\n\r\n  - As the folder you cloned earlier (named `reconftw`) is being renamed to `OutputFolder`, you'll have to go to that folder to view results.\r\n\r\nIf you wish to:\r\n\r\n1. Dynamically modify the behaviour & function of the image\r\n2. Build your own container\r\n3. Build an Axiom Controller on top of the official image\r\n\r\nPlease refer to the [Docker](https://github.com/six2dez/reconftw/wiki/4.-Docker) documentation.\r\n\r\n## c) Terraform + Ansible\r\n\r\nYes! reconFTW can also be easily deployed with Terraform and Ansible to AWS, if you want to know how to do it, you can check the guide [here](Terraform/README.md)\r\n\r\n# ‚öôÔ∏è Config file\r\n>\r\n> You can find a detailed explanation of the configuration file [here](https://github.com/six2dez/reconftw/wiki/3.-Configuration-file) :book:\r\n\r\n- Through ```reconftw.cfg``` file the whole execution of the tool can be controlled.\r\n- Hunters can set various scanning modes, execution preferences, tools, config files, APIs/TOKENS, personalized wordlists and much more.\r\n\r\n<details>\r\n <br><br>\r\n <summary> :point_right: Click here to view default config file :point_left: </summary>\r\n\r\n```yaml\r\n#############################################\r\n#\t\t\treconFTW config file\t\t\t#\r\n#############################################\r\n\r\n# General values\r\ntools=~/Tools   # Path installed tools\r\nSCRIPTPATH=\"$( cd \"$(dirname \"$0\")\" >/dev/null 2>&1 ; pwd -P )\" # Get current script's path\r\nprofile_shell=\".$(basename $(echo $SHELL))rc\" # Get current shell profile\r\nreconftw_version=$(git rev-parse --abbrev-ref HEAD)-$(git describe --tags) # Fetch current reconftw version\r\ngenerate_resolvers=false # Generate custom resolvers with dnsvalidator\r\nupdate_resolvers=true # Fetch and rewrite resolvers from trickest/resolvers before DNS resolution\r\nresolvers_url=\"https://raw.githubusercontent.com/trickest/resolvers/main/resolvers.txt\"\r\nresolvers_trusted_url=\"https://gist.githubusercontent.com/six2dez/ae9ed7e5c786461868abd3f2344401b6/raw/trusted_resolvers.txt\"\r\nfuzzing_remote_list=\"https://raw.githubusercontent.com/six2dez/OneListForAll/main/onelistforallmicro.txt\" # Used to send to axiom(if used) on fuzzing \r\nproxy_url=\"http://127.0.0.1:8080/\" # Proxy url\r\ninstall_golang=true # Set it to false if you already have Golang configured and ready\r\nupgrade_tools=true\r\nupgrade_before_running=false # Upgrade tools before running\r\n#dir_output=/custom/output/path\r\n\r\n# Golang Vars (Comment or change on your own)\r\nexport GOROOT=/usr/local/go\r\nexport GOPATH=$HOME/go\r\nexport PATH=$GOPATH/bin:$GOROOT/bin:$HOME/.local/bin:$PATH\r\n\r\n# Tools config files\r\n#NOTIFY_CONFIG=~/.config/notify/provider-config.yaml # No need to define\r\nGITHUB_TOKENS=${tools}/.github_tokens\r\nGITLAB_TOKENS=${tools}/.gitlab_tokens\r\n#CUSTOM_CONFIG=custom_config_path.txt # In case you use a custom config file, uncomment this line and set your files path\r\n\r\n# APIs/TOKENS - Uncomment the lines you want removing the '#' at the beginning of the line\r\n#SHODAN_API_KEY=\"XXXXXXXXXXXXX\"\r\n#WHOISXML_API=\"XXXXXXXXXX\"\r\n#XSS_SERVER=\"XXXXXXXXXXXXXXXXX\"\r\n#COLLAB_SERVER=\"XXXXXXXXXXXXXXXXX\"\r\n#slack_channel=\"XXXXXXXX\"\r\n#slack_auth=\"xoXX-XXX-XXX-XXX\"\r\n\r\n# File descriptors\r\nDEBUG_STD=\"&>/dev/null\" # Skips STD output on installer\r\nDEBUG_ERROR=\"2>/dev/null\" # Skips ERR output on installer\r\n\r\n# Osint\r\nOSINT=true # Enable or disable the whole OSINT module\r\nGOOGLE_DORKS=true\r\nGITHUB_DORKS=true\r\nGITHUB_REPOS=true\r\nMETADATA=true # Fetch metadata from indexed office documents\r\nEMAILS=true # Fetch emails from differents sites \r\nDOMAIN_INFO=true # whois info\r\nIP_INFO=true    # Reverse IP search, geolocation and whois\r\nAPI_LEAKS=true # Check for API leaks\r\nTHIRD_PARTIES=true # Check for 3rd parties misconfigs\r\nSPOOF=true # Check spoofable domains\r\nMETAFINDER_LIMIT=20 # Max 250\r\n\r\n# Subdomains\r\nSUBDOMAINS_GENERAL=true # Enable or disable the whole Subdomains module\r\nSUBPASSIVE=true # Passive subdomains search\r\nSUBCRT=true # crtsh search\r\nCTR_LIMIT=999999 # Limit the number of results\r\nSUBNOERROR=false # Check DNS NOERROR response and BF on them\r\nSUBANALYTICS=true # Google Analytics search\r\nSUBBRUTE=true # DNS bruteforcing\r\nSUBSCRAPING=true # Subdomains extraction from web crawling\r\nSUBPERMUTE=true # DNS permutations\r\nSUBREGEXPERMUTE=true # Permutations by regex analysis\r\nPERMUTATIONS_OPTION=gotator # The alternative is \"ripgen\" (faster, not deeper)\r\nGOTATOR_FLAGS=\" -depth 1 -numbers 3 -mindup -adv -md\" # Flags for gotator\r\nSUBTAKEOVER=true # Check subdomain takeovers, false by default cuz nuclei already check this\r\nSUB_RECURSIVE_PASSIVE=false # Uses a lot of API keys queries\r\nDEEP_RECURSIVE_PASSIVE=10 # Number of top subdomains for recursion\r\nSUB_RECURSIVE_BRUTE=false # Needs big disk space and time to resolve\r\nZONETRANSFER=true # Check zone transfer\r\nS3BUCKETS=true # Check S3 buckets misconfigs\r\nREVERSE_IP=false # Check reverse IP subdomain search (set True if your target is CIDR/IP)\r\nTLS_PORTS=\"21,22,25,80,110,135,143,261,271,324,443,448,465,563,614,631,636,664,684,695,832,853,854,990,993,989,992,994,995,1129,1131,1184,2083,2087,2089,2096,2221,2252,2376,2381,2478,2479,2482,2484,2679,2762,3077,3078,3183,3191,3220,3269,3306,3410,3424,3471,3496,3509,3529,3539,3535,3660,36611,3713,3747,3766,3864,3885,3995,3896,4031,4036,4062,4064,4081,4083,4116,4335,4336,4536,4590,4740,4843,4849,5443,5007,5061,5321,5349,5671,5783,5868,5986,5989,5990,6209,6251,6443,6513,6514,6619,6697,6771,7202,7443,7673,7674,7677,7775,8243,8443,8991,8989,9089,9295,9318,9443,9444,9614,9802,10161,10162,11751,12013,12109,14143,15002,16995,41230,16993,20003\"\r\nINSCOPE=false # Uses inscope tool to filter the scope, requires .scope file in reconftw folder \r\n\r\n# Web detection\r\nWEBPROBESIMPLE=true # Web probing on 80/443\r\nWEBPROBEFULL=true # Web probing in a large port list\r\nWEBSCREENSHOT=true # Webs screenshooting\r\nVIRTUALHOSTS=false # Check virtualhosts by fuzzing HOST header\r\nUNCOMMON_PORTS_WEB=\"81,300,591,593,832,981,1010,1311,1099,2082,2095,2096,2480,3000,3001,3002,3003,3128,3333,4243,4567,4711,4712,4993,5000,5104,5108,5280,5281,5601,5800,6543,7000,7001,7396,7474,8000,8001,8008,8014,8042,8060,8069,8080,8081,8083,8088,8090,8091,8095,8118,8123,8172,8181,8222,8243,8280,8281,8333,8337,8443,8500,8834,8880,8888,8983,9000,9001,9043,9060,9080,9090,9091,9092,9200,9443,9502,9800,9981,10000,10250,11371,12443,15672,16080,17778,18091,18092,20720,32000,55440,55672\"\r\n\r\n# Host\r\nFAVICON=true # Check Favicon domain discovery\r\nPORTSCANNER=true # Enable or disable the whole Port scanner module \r\nGEO_INFO=true # Fetch Geolocalization info\r\nPORTSCAN_PASSIVE=true # Port scanner with Shodan\r\nPORTSCAN_ACTIVE=true # Port scanner with nmap\r\nPORTSCAN_ACTIVE_OPTIONS=\"--top-ports 200 -sV -n -Pn --open --max-retries 2 --script vulners\"\r\nCDN_IP=true # Check which IPs belongs to CDN\r\n\r\n# Web analysis\r\nWAF_DETECTION=true # Detect WAFs\r\nNUCLEICHECK=true # Enable or disable nuclei\r\nNUCLEI_TEMPLATES_PATH=\"$HOME/nuclei-templates\" # Set nuclei templates path\r\nNUCLEI_SEVERITY=\"info,low,medium,high,critical\" # Set templates criticity\r\nNUCLEI_FLAGS=\" -silent -t ${NUCLEI_TEMPLATES_PATH}/ -retries 2\" # Additional nuclei extra flags, don't set the severity here but the exclusions like \" -etags openssh\"\r\nNUCLEI_FLAGS_JS=\" -silent -tags exposure,token -severity info,low,medium,high,critical\" # Additional nuclei extra flags for js secrets\r\nURL_CHECK=true # Enable or disable URL collection\r\nURL_CHECK_PASSIVE=true # Search for urls, passive methods from Archive, OTX, CommonCrawl, etc\r\nURL_CHECK_ACTIVE=true # Search for urls by crawling the websites\r\nURL_GF=true # Url patterns classification\r\nURL_EXT=true # Returns a list of files divided by extension\r\nJSCHECKS=true # JS analysis\r\nFUZZ=true # Web fuzzing\r\nIIS_SHORTNAME=true\r\nCMS_SCANNER=true # CMS scanner\r\nWORDLIST=true # Wordlist generation\r\nROBOTSWORDLIST=true # Check historic disallow entries on waybackMachine\r\nPASSWORD_DICT=true # Generate password dictionary\r\nPASSWORD_MIN_LENGTH=5 # Min password length\r\nPASSWORD_MAX_LENGTH=14 # Max password length\r\n\r\n# Vulns\r\nVULNS_GENERAL=false # Enable or disable the vulnerability module (very intrusive and slow)\r\nXSS=true # Check for xss with dalfox\r\nCORS=true # CORS misconfigs\r\nTEST_SSL=true # SSL misconfigs\r\nOPEN_REDIRECT=true # Check open redirects\r\nSSRF_CHECKS=true # SSRF checks\r\nCRLF_CHECKS=true # CRLF checks\r\nLFI=true # LFI by fuzzing\r\nSSTI=true # SSTI by fuzzing\r\nSQLI=true # Check SQLI\r\nSQLMAP=true # Check SQLI with sqlmap\r\nGHAURI=false # Check SQLI with ghauri\r\nBROKENLINKS=true # Check for brokenlinks\r\nSPRAY=true # Performs password spraying\r\nCOMM_INJ=true # Check for command injections with commix\r\nPROTO_POLLUTION=true # Check for prototype pollution flaws\r\nSMUGGLING=true # Check for HTTP request smuggling flaws\r\nWEBCACHE=true # Check for Web Cache issues\r\nBYPASSER4XX=true # Check for 4XX bypasses\r\nFUZZPARAMS=true # Fuzz parameters values\r\n\r\n# Extra features\r\nNOTIFICATION=false # Notification for every function\r\nSOFT_NOTIFICATION=false # Only for start/end\r\nDEEP=false # DEEP mode, really slow and don't care about the number of results\r\nDEEP_LIMIT=500 # First limit to not run unless you run DEEP\r\nDEEP_LIMIT2=1500 # Second limit to not run unless you run DEEP\r\nDIFF=false # Diff function, run every module over an already scanned target, printing only new findings (but save everything)\r\nREMOVETMP=true # Delete temporary files after execution (to free up space)\r\nREMOVELOG=false # Delete logs after execution\r\nPROXY=false # Send to proxy the websites found\r\nSENDZIPNOTIFY=false # Send to zip the results (over notify)\r\nPRESERVE=true      # set to true to avoid deleting the .called_fn files on really large scans\r\nFFUF_FLAGS=\" -mc all -fc 404 -sf -noninteractive -of json\" # Ffuf flags\r\nHTTPX_FLAGS=\" -follow-redirects -random-agent -status-code -silent -title -web-server -tech-detect -location -content-length\" # Httpx flags for simple web probing\r\n\r\n# HTTP options\r\nHEADER=\"User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:72.0) Gecko/20100101 Firefox/72.0\" # Default header\r\n\r\n# Threads\r\nFFUF_THREADS=40\r\nHTTPX_THREADS=50\r\nHTTPX_UNCOMMONPORTS_THREADS=100\r\nKATANA_THREADS=20\r\nBRUTESPRAY_THREADS=20\r\nBRUTESPRAY_CONCURRENCE=10\r\nGAU_THREADS=10\r\nDNSTAKE_THREADS=100\r\nDALFOX_THREADS=200\r\nPUREDNS_PUBLIC_LIMIT=0 # Set between 2000 - 10000 if your router blows up, 0 means unlimited\r\nPUREDNS_TRUSTED_LIMIT=400\r\nPUREDNS_WILDCARDTEST_LIMIT=30\r\nPUREDNS_WILDCARDBATCH_LIMIT=1500000\r\nRESOLVE_DOMAINS_THREADS=150\r\nDNSVALIDATOR_THREADS=200\r\nINTERLACE_THREADS=10\r\nTLSX_THREADS=1000\r\nXNLINKFINDER_DEPTH=3\r\n\r\n# Rate limits\r\nHTTPX_RATELIMIT=150\r\nNUCLEI_RATELIMIT=150\r\nFFUF_RATELIMIT=0\r\n\r\n# Timeouts\r\nSUBFINDER_ENUM_TIMEOUT=180          # Minutes\r\nCMSSCAN_TIMEOUT=3600            # Seconds\r\nFFUF_MAXTIME=900                # Seconds\r\nHTTPX_TIMEOUT=10                # Seconds\r\nHTTPX_UNCOMMONPORTS_TIMEOUT=10  # Seconds\r\nPERMUTATIONS_LIMIT=21474836480  # Bytes, default is 20 GB\r\n\r\n# lists\r\nfuzz_wordlist=${tools}/fuzz_wordlist.txt\r\nlfi_wordlist=${tools}/lfi_wordlist.txt\r\nssti_wordlist=${tools}/ssti_wordlist.txt\r\nsubs_wordlist=${tools}/subdomains.txt\r\nsubs_wordlist_big=${tools}/subdomains_n0kovo_big.txt\r\nresolvers=${tools}/resolvers.txt\r\nresolvers_trusted=${tools}/resolvers_trusted.txt\r\n\r\n# Axiom Fleet\r\n# Will not start a new fleet if one exist w/ same name and size (or larger)\r\n# AXIOM=false Uncomment only to overwrite command line flags\r\nAXIOM_FLEET_LAUNCH=true # Enable or disable spin up a new fleet, if false it will use the current fleet with the AXIOM_FLEET_NAME prefix\r\nAXIOM_FLEET_NAME=\"reconFTW\" # Fleet's prefix name\r\nAXIOM_FLEET_COUNT=10 # Fleet's number\r\nAXIOM_FLEET_REGIONS=\"eu-central\" # Fleet's region\r\nAXIOM_FLEET_SHUTDOWN=true # # Enable or disable delete the fleet after the execution\r\n# This is a script on your reconftw host that might prep things your way...\r\n#AXIOM_POST_START=\"~/Tools/axiom_config.sh\" # Useful  to send your config files to the fleet\r\nAXIOM_EXTRA_ARGS=\"\" # Leave empty if you don't want to add extra arguments\r\n#AXIOM_EXTRA_ARGS=\" --rm-logs\" # Example\r\n\r\n# TERM COLORS\r\nbred='\\033[1;31m'\r\nbblue='\\033[1;34m'\r\nbgreen='\\033[1;32m'\r\nbyellow='\\033[1;33m'\r\nred='\\033[0;31m'\r\nblue='\\033[0;34m'\r\ngreen='\\033[0;32m'\r\nyellow='\\033[0;33m'\r\nreset='\\033[0m'\r\n\r\n```\r\n\r\n</details>\r\n\r\n# Usage\r\n\r\n> Check out the wiki section to know which flag performs what all steps/attacks [Usage Guide](https://github.com/six2dez/reconftw/wiki/2.-Usage-Guide) :book:\r\n\r\n## TARGET OPTIONS\r\n\r\n| Flag | Description |\r\n|------|-------------|\r\n| -d | Single Target domain *(example.com)*  |\r\n| -l | List of targets *(one per line)* |\r\n| -m | Multiple domain target *(companyName)*  |\r\n| -x | Exclude subdomains list *(Out Of Scope)* |\r\n| -i | Include subdomains list *(In Scope)* |\r\n\r\n## MODE OPTIONS\r\n\r\n| Flag | Description |\r\n|------|-------------|\r\n| -r | Recon - Full recon process (without attacks like sqli,ssrf,xss,ssti,lfi etc.) |\r\n| -s | Subdomains - Perform only subdomain enumeration, web probing, subdomain takeovers |\r\n| -p | Passive - Perform only passive steps |\r\n| -a | All - Perform whole recon and all active attacks |\r\n| -w | Web - Perform only vulnerability checks/attacks on particular target |\r\n| -n | OSINT - Performs an OSINT scan (no subdomain enumeration and attacks) |\r\n| -c | Custom - Launches specific function against target |\r\n| -h | Help - Show this help menu |\r\n\r\n## GENERAL OPTIONS\r\n\r\n| Flag | Description |\r\n|------|-------------|\r\n| --deep | Deep scan (Enable some slow options for deeper scan, *vps intended mode*) |\r\n| -f | Custom config file path |\r\n| -o | Output directory |\r\n| -v | Axiom distributed VPS |\r\n| -q | Rate limit in requests per second |\r\n\r\n## Example Usage\r\n\r\n**NOTE: this is applicable when you've installed reconFTW on the host (e.g. VM/VPS/cloud) and not in a Docker container.**\r\n\r\n### To perform a full recon on single target\r\n\r\n```bash\r\n./reconftw.sh -d target.com -r\r\n```\r\n\r\n### To perform a full recon on a list of targets\r\n\r\n```bash\r\n./reconftw.sh -l sites.txt -r -o /output/directory/\r\n```\r\n\r\n### Perform full recon with more time intense tasks *(VPS intended only)*\r\n\r\n```bash\r\n./reconftw.sh -d target.com -r --deep -o /output/directory/\r\n```\r\n\r\n### Perform recon in a multi domain target\r\n\r\n```bash\r\n./reconftw.sh -m company -l domains_list.txt -r\r\n```\r\n\r\n### Perform recon with axiom integration\r\n\r\n```bash\r\n./reconftw.sh -d target.com -r -v\r\n```\r\n\r\n### Perform all steps (whole recon + all attacks) a.k.a. YOLO mode\r\n\r\n```bash\r\n./reconftw.sh -d target.com -a\r\n```\r\n\r\n### Show help section\r\n\r\n```bash\r\n./reconftw.sh -h\r\n```\r\n\r\n# Axiom Support :cloud:\r\n\r\n![](https://i.ibb.co/Jzrgkqt/axiom-readme.png)\r\n> Check out the wiki section for more info [Axiom Support](https://github.com/six2dez/reconftw/wiki/5.-Axiom-version)\r\n\r\n- As reconFTW actively hits the target with a lot of web traffic, hence there was a need to move to Axiom distributing the work load among various instances leading to reduction of execution time.\r\n- During the configuration of axiom you need to select `reconftw` as provisoner.\r\n- You can create your own axiom's fleet before running reconFTW or let reconFTW to create and destroy it automatically just modifying reconftw.cfg file.\r\n\r\n# Sample video\r\n\r\n![Video](images/reconFTW.gif)\r\n\r\n# :fire: Features :fire:\r\n\r\n## Osint\r\n\r\n- Domain information ([whois](https://github.com/rfc1036/whois))\r\n- Emails addresses and passwords leaks ([emailfinder](https://github.com/Josue87/EmailFinder) and [LeakSearch](https://github.com/JoelGMSec/LeakSearch))\r\n- Metadata finder ([MetaFinder](https://github.com/Josue87/MetaFinder))\r\n- API leaks search ([porch-pirate](https://github.com/MandConsultingGroup/porch-pirate) and [SwaggerSpy](https://github.com/UndeadSec/SwaggerSpy))\r\n- Google Dorks ([dorks_hunter](https://github.com/six2dez/dorks_hunter))\r\n- Github Dorks ([gitdorks_go](https://github.com/damit5/gitdorks_go))\r\n- GitHub org analysis ([enumerepo](https://github.com/trickest/enumerepo), [trufflehog](https://github.com/trufflesecurity/trufflehog) and [gitleaks](https://github.com/gitleaks/gitleaks))\r\n- 3rd parties misconfigurations([misconfig-mapper](https://github.com/intigriti/misconfig-mapper))\r\n- Spoofable domains ([spoofcheck](https://github.com/MattKeeley/Spoofy))\r\n\r\n## Subdomains\r\n\r\n- Passive ([subfinder](https://github.com/projectdiscovery/subfinder) and [github-subdomains](https://github.com/gwen001/github-subdomains))\r\n- Certificate transparency ([crt](https://github.com/cemulus/crt))\r\n- NOERROR subdomain discovery ([dnsx](https://github.com/projectdiscovery/dnsx), more info [here](https://www.securesystems.de/blog/enhancing-subdomain-enumeration-ents-and-noerror/))\r\n- Bruteforce ([puredns](https://github.com/d3mondev/puredns))\r\n- Permutations ([Gotator](https://github.com/Josue87/gotator), [ripgen](https://github.com/resyncgg/ripgen) and [regulator](https://github.com/cramppet/regulator))\r\n- JS files & Source Code Scraping ([katana](https://github.com/projectdiscovery/katana))\r\n- DNS Records ([dnsx](https://github.com/projectdiscovery/dnsx))\r\n- Google Analytics ID ([AnalyticsRelationships](https://github.com/Josue87/AnalyticsRelationships))\r\n- TLS handshake ([tlsx](https://github.com/projectdiscovery/tlsx))\r\n- Recursive search ([dsieve](https://github.com/trickest/dsieve)).\r\n- Subdomains takeover ([nuclei](https://github.com/projectdiscovery/nuclei))\r\n- DNS takeover ([dnstake](https://github.com/pwnesia/dnstake))\r\n- DNS Zone Transfer ([dig](https://linux.die.net/man/1/dig))\r\n- Cloud checkers ([S3Scanner](https://github.com/sa7mon/S3Scanner) and [cloud_enum](https://github.com/initstring/cloud_enum))\r\n\r\n## Hosts\r\n\r\n- IP info ([ipinfo](https://www.ipinfo.io/))\r\n- CDN checker ([ipcdn](https://github.com/six2dez/ipcdn))\r\n- WAF checker ([wafw00f](https://github.com/EnableSecurity/wafw00f))\r\n- Port Scanner (Active with [nmap](https://github.com/nmap/nmap) and passive with [smap](https://github.com/s0md3v/Smap))\r\n- Port services vulnerability checks ([vulners](https://github.com/vulnersCom/nmap-vulners))\r\n- Password spraying ([brutespray](https://github.com/x90skysn3k/brutespray))\r\n- Geolocalization info (ipapi.co)\r\n\r\n## Webs\r\n\r\n- Web Prober ([httpx](https://github.com/projectdiscovery/httpx))\r\n- Web screenshoting ([nuclei](https://github.com/projectdiscovery/nuclei))\r\n- Web templates scanner ([nuclei](https://github.com/projectdiscovery/nuclei) and [nuclei geeknik](https://github.com/geeknik/the-nuclei-templates.git))\r\n- CMS Scanner ([CMSeeK](https://github.com/Tuhinshubhra/CMSeeK))\r\n- Url extraction ([gau](https://github.com/lc/gau),[waymore](https://github.com/xnl-h4ck3r/waymore), [katana](https://github.com/projectdiscovery/katana), [github-endpoints](https://gist.github.com/six2dez/d1d516b606557526e9a78d7dd49cacd3) and [JSA](https://github.com/w9w/JSA))\r\n- URL patterns Search and filtering ([urless](https://github.com/xnl-h4ck3r/urless), [gf](https://github.com/tomnomnom/gf) and [gf-patterns](https://github.com/1ndianl33t/Gf-Patterns))\r\n- Favicon Real IP ([fav-up](https://github.com/pielco11/fav-up))\r\n- Javascript analysis ([subjs](https://github.com/lc/subjs), [JSA](https://github.com/w9w/JSA), [xnLinkFinder](https://github.com/xnl-h4ck3r/xnLinkFinder), [getjswords](https://github.com/m4ll0k/BBTz), [mantra](https://github.com/MrEmpy/mantra), [jsluice](https://github.com/BishopFox/jsluice))\r\n- Sourcemap JS extraction ([sourcemapper](https://github.com/denandz/sourcemapper))\r\n- Fuzzing ([ffuf](https://github.com/ffuf/ffuf))\r\n- URL sorting by extension\r\n- Wordlist generation\r\n- Passwords dictionary creation ([pydictor](https://github.com/LandGrey/pydictor))\r\n\r\n## Vulnerability checks\r\n\r\n- XSS ([dalfox](https://github.com/hahwul/dalfox))\r\n- Open redirect ([Oralyzer](https://github.com/r0075h3ll/Oralyzer))\r\n- SSRF (headers [interactsh](https://github.com/projectdiscovery/interactsh) and param values with [ffuf](https://github.com/ffuf/ffuf))\r\n- CRLF ([crlfuzz](https://github.com/dwisiswant0/crlfuzz))\r\n- Cors ([Corsy](https://github.com/s0md3v/Corsy))\r\n- LFI Checks ([ffuf](https://github.com/ffuf/ffuf))\r\n- SQLi Check ([SQLMap](https://github.com/sqlmapproject/sqlmap) and [ghauri](https://github.com/r0oth3x49/ghauri))\r\n- SSTI ([ffuf](https://github.com/ffuf/ffuf))\r\n- SSL tests ([testssl](https://github.com/drwetter/testssl.sh))\r\n- Broken Links Checker ([katana](https://github.com/projectdiscovery/katana))\r\n- Prototype Pollution ([ppmap](https://github.com/kleiton0x00/ppmap))\r\n- Web Cache Vulnerabilities ([Web-Cache-Vulnerability-Scanner](https://github.com/Hackmanit/Web-Cache-Vulnerability-Scanner))\r\n- 4XX Bypasser ([nomore403](https://github.com/devploit/nomore403))\r\n\r\n## Extras\r\n\r\n- Multithreading ([Interlace](https://github.com/codingo/Interlace))\r\n- Custom resolvers generated list ([dnsvalidator](https://github.com/vortexau/dnsvalidator))\r\n- Docker container included and [DockerHub](https://hub.docker.com/r/six2dez/reconftw) integration\r\n- Ansible + Terraform deployment over AWS\r\n- Allows IP/CIDR as target\r\n- Resume the scan from last performed step\r\n- Custom output folder option\r\n- All in one installer/updater script compatible with most distros\r\n- Diff support for continuous running (cron mode)\r\n- Support for targets with multiple domains\r\n- Raspberry Pi/ARM support\r\n- 6 modes (recon, passive, subdomains, web, osint and all)\r\n- Out of Scope Support + optional [inscope](https://github.com/tomnomnom/hacks/tree/master/inscope) support\r\n- Notification system with Slack, Discord and Telegram ([notify](https://github.com/projectdiscovery/notify)) and sending zipped results support\r\n\r\n## Mindmap/Workflow\r\n\r\n![Mindmap](images/mindmap_obsidian.png)\r\n\r\n## Data Keep\r\n\r\nFollow these simple steps to end up with a private repository with your `API Keys` and `/Recon` data.\r\n\r\n### Makefile\r\n\r\nA `Makefile` is provided to quickly bootstrap a private repo. To use it, you'll need the [Github CLI](https://cli.github.com/) installed.\r\n\r\nOnce done, just run:\r\n\r\n```bash\r\n# below line is optional, the default is ~/reconftw-data\r\nexport PRIV_REPO=\"$HOME/reconftw-data\"\r\nmake bootstrap\r\n```\r\n\r\nTo sync your private repo with upstream:\r\n\r\n```bash\r\nmake sync\r\n```\r\n\r\nTo upload juicy recon data:\r\n\r\n```bash\r\nmake upload\r\n```\r\n\r\n### Manual\r\n\r\n- Create a private **blank** repository on `Git(Hub|Lab)` (Take into account size limits regarding Recon data upload)\r\n\r\n- Clone your project: `git clone https://gitlab.com/example/reconftw-data`\r\n- Get inside the cloned repository: `cd reconftw-data`\r\n- Create a new branch with an empty commit: `git commit --allow-empty -m \"Empty commit\"`\r\n- Add the official repo as a new remote: `git remote add upstream https://github.com/six2dez/reconftw` (`upstream` is an example)\r\n- Update upstream's repo: `git fetch upstream`\r\n- Rebase current branch with the official one: `git rebase upstream/main master`\r\n\r\n### Main commands\r\n\r\n- Upload changes to your personal repo: `git add . && git commit -m \"Data upload\" && git push origin master`\r\n- Update tool anytime: `git fetch upstream && git rebase upstream/main master`\r\n\r\n## How to contribute\r\n\r\nIf you want to contribute to this project, you can do it in multiple ways:\r\n\r\n- Submitting an [issue](https://github.com/six2dez/reconftw/issues/new/choose) because you have found a bug or you have any suggestion or request.\r\n- Making a Pull Request from [dev](https://github.com/six2dez/reconftw/tree/dev) branch because you want to improve the code or add something to the script.\r\n\r\n## Need help? :information_source:\r\n\r\n- Take a look at the [wiki](https://github.com/six2dez/reconftw/wiki) section.\r\n- Check [FAQ](https://github.com/six2dez/reconftw/wiki/7.-FAQs) for commonly asked questions.\r\n- Join our [Discord server](https://discord.gg/R5DdXVEdTy)\r\n- Ask for help in the [Telegram group](https://t.me/joinchat/TO_R8NYFhhbmI5co)\r\n\r\n## Support this project\r\n\r\n### Buymeacoffee\r\n\r\n[<img src=\"https://cdn.buymeacoffee.com/buttons/v2/default-green.png\">](https://www.buymeacoffee.com/six2dez)\r\n\r\n### DigitalOcean referral link\r\n\r\n<a href=\"https://www.digitalocean.com/?refcode=f362a6e193a1&utm_campaign=Referral_Invite&utm_medium=Referral_Program&utm_source=badge\"><img src=\"https://web-platforms.sfo2.cdn.digitaloceanspaces.com/WWW/Badge%201.svg\" alt=\"DigitalOcean Referral Badge\" /></a>\r\n\r\n### GitHub sponsorship\r\n\r\n[Sponsor](https://github.com/sponsors/six2dez)\r\n\r\n## Thanks :pray:\r\n\r\n- Thank you for lending a helping hand towards the development of the project!\r\n\r\n- [C99](https://api.c99.nl/)\r\n- [CIRCL](https://www.circl.lu/)\r\n- [NetworksDB](https://networksdb.io/)\r\n- [ipinfo](https://ipinfo.io/)\r\n- [hackertarget](https://hackertarget.com/)\r\n- [Censys](https://censys.io/)\r\n- [Fofa](https://fofa.info/)\r\n- [intelx](https://intelx.io/)\r\n- [Whoxy](https://www.whoxy.com/)\r\n\r\n## Disclaimer\r\n\r\nUsage of this program for attacking targets without consent is illegal. It is the user's responsibility to obey all applicable laws. The developer assumes no liability and is not responsible for any misuse or damage caused by this program. Please use responsibly.\r\n\r\nThe material contained in this repository is licensed under MIT.\r\n"
        },
        {
          "name": "Terraform",
          "type": "tree",
          "content": null
        },
        {
          "name": "banners.txt",
          "type": "blob",
          "size": 10.2763671875,
          "content": "banner1=\"\"\" ‚ñà‚ñà‚ñÄ‚ñà‚ñà‚ñà  ‚ñì‚ñà‚ñà‚ñà‚ñà‚ñà  ‚ñÑ‚ñà‚ñà‚ñà‚ñà‚ñÑ   ‚ñí‚ñà‚ñà‚ñà‚ñà‚ñà   ‚ñà‚ñà‚ñà‚ñÑ    ‚ñà   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñí‚ñÑ‚ñÑ‚ñÑ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñì ‚ñà     ‚ñà‚ñë   \n‚ñì‚ñà‚ñà ‚ñí ‚ñà‚ñà‚ñí‚ñì‚ñà   ‚ñÄ ‚ñí‚ñà‚ñà‚ñÄ ‚ñÄ‚ñà  ‚ñí‚ñà‚ñà‚ñí  ‚ñà‚ñà‚ñí ‚ñà‚ñà ‚ñÄ‚ñà   ‚ñà ‚ñì‚ñà‚ñà   ‚ñí ‚ñì  ‚ñà‚ñà‚ñí ‚ñì‚ñí‚ñì‚ñà‚ñë ‚ñà ‚ñë‚ñà‚ñë   \n‚ñì‚ñà‚ñà ‚ñë‚ñÑ‚ñà ‚ñí‚ñí‚ñà‚ñà‚ñà   ‚ñí‚ñì‚ñà    ‚ñÑ ‚ñí‚ñà‚ñà‚ñë  ‚ñà‚ñà‚ñí‚ñì‚ñà‚ñà  ‚ñÄ‚ñà ‚ñà‚ñà‚ñí‚ñí‚ñà‚ñà‚ñà‚ñà ‚ñë ‚ñí ‚ñì‚ñà‚ñà‚ñë ‚ñí‚ñë‚ñí‚ñà‚ñë ‚ñà ‚ñë‚ñà    \n‚ñí‚ñà‚ñà‚ñÄ‚ñÄ‚ñà‚ñÑ  ‚ñí‚ñì‚ñà  ‚ñÑ ‚ñí‚ñì‚ñì‚ñÑ ‚ñÑ‚ñà‚ñà‚ñí‚ñí‚ñà‚ñà   ‚ñà‚ñà‚ñë‚ñì‚ñà‚ñà‚ñí  ‚ñê‚ñå‚ñà‚ñà‚ñí‚ñë‚ñì‚ñà‚ñí  ‚ñë ‚ñë ‚ñì‚ñà‚ñà‚ñì ‚ñë ‚ñë‚ñà‚ñë ‚ñà ‚ñë‚ñà    \n‚ñë‚ñà‚ñà‚ñì ‚ñí‚ñà‚ñà‚ñí‚ñë‚ñí‚ñà‚ñà‚ñà‚ñà‚ñí‚ñí ‚ñì‚ñà‚ñà‚ñà‚ñÄ ‚ñë‚ñë ‚ñà‚ñà‚ñà‚ñà‚ñì‚ñí‚ñë‚ñí‚ñà‚ñà‚ñë   ‚ñì‚ñà‚ñà‚ñë‚ñë‚ñí‚ñà‚ñë      ‚ñí‚ñà‚ñà‚ñí ‚ñë ‚ñë‚ñë‚ñà‚ñà‚ñí‚ñà‚ñà‚ñì    \n‚ñë ‚ñí‚ñì ‚ñë‚ñí‚ñì‚ñë‚ñë‚ñë ‚ñí‚ñë ‚ñë‚ñë ‚ñë‚ñí ‚ñí  ‚ñë‚ñë ‚ñí‚ñë‚ñí‚ñë‚ñí‚ñë ‚ñë ‚ñí‚ñë   ‚ñí ‚ñí  ‚ñí ‚ñë      ‚ñí ‚ñë‚ñë   ‚ñë ‚ñì‚ñë‚ñí ‚ñí     \n  ‚ñë‚ñí ‚ñë ‚ñí‚ñë ‚ñë ‚ñë  ‚ñë  ‚ñë  ‚ñí     ‚ñë ‚ñí ‚ñí‚ñë ‚ñë ‚ñë‚ñë   ‚ñë ‚ñí‚ñë ‚ñë          ‚ñë      ‚ñí ‚ñë ‚ñë     \n  ‚ñë‚ñë   ‚ñë    ‚ñë   ‚ñë        ‚ñë ‚ñë ‚ñë ‚ñí     ‚ñë   ‚ñë ‚ñë  ‚ñë ‚ñë      ‚ñë        ‚ñë   ‚ñë     \n   ‚ñë        ‚ñë  ‚ñë‚ñë ‚ñë          ‚ñë ‚ñë           ‚ñë                      ‚ñë       \n                ‚ñë                                                         \n\"\"\"\n\nbanner2=\"\"\"  _____                      ______ _________          __ \n |  __ \\                    |  ____|__   __\\ \\        / / \n | |__) |___  ___ ___  _ __ | |__     | |   \\ \\  /\\  / /  \n |  _  // _ \\/ __/ _ \\| '_ \\|  __|    | |    \\ \\/  \\/ /   \n | | \\ \\  __/ (_| (_) | | | | |       | |     \\  /\\  /    \n |_|  \\_\\___|\\___\\___/|_| |_|_|       |_|      \\/  \\/     \n\"\"\"\n\nbanner3=\"\"\" ____  ____  ___  _____  _  _  ____  ____  _    _   \n(  _ \\( ___)/ __)(  _  )( \\( )( ___)(_  _)( \\/\\/ )  \n )   / )__)( (__  )(_)(  )  (  )__)   )(   )    (   \n(_)\\_)(____)\\___)(_____)(_)\\_)(__)   (__) (__/\\__)  \n\n\"\"\"\nbanner4=\"\"\"########  ########  ######   #######  ##    ## ######## ######## ##      ##    \n##     ## ##       ##    ## ##     ## ###   ## ##          ##    ##  ##  ##    \n##     ## ##       ##       ##     ## ####  ## ##          ##    ##  ##  ##    \n########  ######   ##       ##     ## ## ## ## ######      ##    ##  ##  ##    \n##   ##   ##       ##       ##     ## ##  #### ##          ##    ##  ##  ##    \n##    ##  ##       ##    ## ##     ## ##   ### ##          ##    ##  ##  ##    \n##     ## ########  ######   #######  ##    ## ##          ##     ###  ###     \n\"\"\"\n\nbanner5=\"\"\" _______  _______  _______  _______  _        _______ _________           \n(  ____ )(  ____ \\(  ____ \\(  ___  )( (    /|(  ____ \\\\__   __/|\\     /|  \n| (    )|| (    \\/| (    \\/| (   ) ||  \\  ( || (    \\/   ) (   | )   ( |  \n| (____)|| (__    | |      | |   | ||   \\ | || (__       | |   | | _ | |  \n|     __)|  __)   | |      | |   | || (\\ \\) ||  __)      | |   | |( )| |  \n| (\\ (   | (      | |      | |   | || | \\   || (         | |   | || || |  \n| ) \\ \\__| (____/\\| (____/\\| (___) || )  \\  || )         | |   | () () |  \n|/   \\__/(_______/(_______/(_______)|/    )_)|/          )_(   (_______)  \n\"\"\"\n\nbanner6=\"\"\"__________                           ________________________      __  \n\\______   \\ ____   ____  ____   ____ \\_   _____/\\__    ___/  \\    /  \\ \n |       _// __ \\_/ ___\\/  _ \\ /    \\ |    __)    |    |  \\   \\/\\/   / \n |    |   \\  ___/\\  \\__(  <_> )   |  \\|     \\     |    |   \\        /  \n |____|_  /\\___  >\\___  >____/|___|  /\\___  /     |____|    \\__/\\  /   \n        \\/     \\/     \\/           \\/     \\/                     \\/    \n\"\"\"\n\nbanner7=\"\"\"   __                        ___  _____  __    __  \n  /__\\ ___  ___ ___  _ __   / __\\/__   \\/ / /\\ \\ \\ \n / \\/// _ \\/ __/ _ \\| '_ \\ / _\\    / /\\/\\ \\/  \\/ / \n/ _  \\  __/ (_| (_) | | | / /     / /    \\  /\\  /  \n\\/ \\_/\\___|\\___\\___/|_| |_\\/      \\/      \\/  \\/                                                      \n\"\"\"\n\nbanner8=\"\"\"‚ï¶‚ïê‚ïó‚îå‚îÄ‚îê‚îå‚îÄ‚îê‚îå‚îÄ‚îê‚îå‚îê‚îå‚ïî‚ïê‚ïó‚ïî‚ï¶‚ïó‚ï¶ ‚ï¶  \n‚ï†‚ï¶‚ïù‚îú‚î§ ‚îÇ  ‚îÇ ‚îÇ‚îÇ‚îÇ‚îÇ‚ï†‚ï£  ‚ïë ‚ïë‚ïë‚ïë  \n‚ï©‚ïö‚ïê‚îî‚îÄ‚îò‚îî‚îÄ‚îò‚îî‚îÄ‚îò‚îò‚îî‚îò‚ïö   ‚ï© ‚ïö‚ï©‚ïù  \n\"\"\"\n\nbanner9=\"\"\" ‚ñÑ‚ñÄ‚ñÄ‚ñÑ‚ñÄ‚ñÄ‚ñÄ‚ñÑ  ‚ñÑ‚ñÄ‚ñÄ‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÑ  ‚ñÑ‚ñÄ‚ñÑ‚ñÑ‚ñÑ‚ñÑ   ‚ñÑ‚ñÄ‚ñÄ‚ñÄ‚ñÄ‚ñÑ   ‚ñÑ‚ñÄ‚ñÄ‚ñÑ ‚ñÄ‚ñÑ  ‚ñÑ‚ñÄ‚ñÄ‚ñÄ‚ñà‚ñÑ    ‚ñÑ‚ñÄ‚ñÄ‚ñÄ‚ñà‚ñÄ‚ñÄ‚ñÑ  ‚ñÑ‚ñÄ‚ñÄ‚ñÑ    ‚ñÑ‚ñÄ‚ñÄ‚ñÑ     \n‚ñà   ‚ñà   ‚ñà ‚ñê  ‚ñÑ‚ñÄ   ‚ñê ‚ñà ‚ñà    ‚ñå ‚ñà      ‚ñà ‚ñà  ‚ñà ‚ñà ‚ñà ‚ñà  ‚ñÑ‚ñÄ  ‚ñÄ‚ñÑ ‚ñà    ‚ñà  ‚ñê ‚ñà   ‚ñà    ‚ñê  ‚ñà     \n‚ñê  ‚ñà‚ñÄ‚ñÄ‚ñà‚ñÄ    ‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ  ‚ñê ‚ñà      ‚ñà      ‚ñà ‚ñê  ‚ñà  ‚ñÄ‚ñà ‚ñê ‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÑ   ‚ñê   ‚ñà     ‚ñê  ‚ñà        ‚ñà     \n ‚ñÑ‚ñÄ    ‚ñà    ‚ñà    ‚ñå    ‚ñà      ‚ñÄ‚ñÑ    ‚ñÑ‚ñÄ   ‚ñà   ‚ñà   ‚ñà    ‚ñê      ‚ñà        ‚ñà   ‚ñÑ    ‚ñà      \n‚ñà     ‚ñà    ‚ñÑ‚ñÄ‚ñÑ‚ñÑ‚ñÑ‚ñÑ    ‚ñÑ‚ñÄ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÄ   ‚ñÄ‚ñÄ‚ñÄ‚ñÄ   ‚ñÑ‚ñÄ   ‚ñà    ‚ñà         ‚ñÑ‚ñÄ          ‚ñÄ‚ñÑ‚ñÄ ‚ñÄ‚ñÑ ‚ñÑ‚ñÄ      \n‚ñê     ‚ñê    ‚ñà    ‚ñê   ‚ñà     ‚ñê           ‚ñà    ‚ñê   ‚ñà         ‚ñà                  ‚ñÄ        \n           ‚ñê        ‚ñê                 ‚ñê        ‚ñê         ‚ñê                           \n\"\"\"\n\nbanner10=\"\"\"                                                                                     \n    //   ) )                                      //   / / /__  ___/ ||   / |  / /   \n   //___/ /   ___      ___      ___       __     //___       / /     ||  /  | / /    \n  / ___ (   //___) ) //   ) ) //   ) ) //   ) ) / ___       / /      || / /||/ /     \n //   | |  //       //       //   / / //   / / //          / /       ||/ / |  /      \n//    | | ((____   ((____   ((___/ / //   / / //          / /        |  /  | /       \n\"\"\"\n\nbanner11=\"\"\"    ____                        _____________       __   \n   / __ \\___  _________  ____  / ____/_  __/ |     / /   \n  / /_/ / _ \\/ ___/ __ \\/ __ \\/ /_    / /  | | /| / /    \n / _, _/  __/ /__/ /_/ / / / / __/   / /   | |/ |/ /     \n/_/ |_|\\___/\\___/\\____/_/ /_/_/     /_/    |__/|__/                                                              \n\"\"\"\n\nbanner12=\"\"\"                                    ####### ####### #     # \n #####  ######  ####   ####  #    # #          #    #  #  # \n #    # #      #    # #    # ##   # #          #    #  #  # \n #    # #####  #      #    # # #  # #####      #    #  #  # \n #####  #      #      #    # #  # # #          #    #  #  # \n #   #  #      #    # #    # #   ## #          #    #  #  # \n #    # ######  ####   ####  #    # #          #     ## ##                                                          \n\"\"\"\n\nbanner13=\"\"\" ___   ____  __    ___   _      ____ _____  _      \n| |_) | |_  / /\\`  / / \\ | |\\ | | |_   | |  \\ \\    /\n|_| \\ |_|__ \\_\\_, \\_\\_/ |_| \\| |_|    |_|   \\_\\/\\/ \n\"\"\"\n\nbanner14=\"\"\"  ______ _______ _______  _____  __   _ _______ _______ _  _  _     \n |_____/ |______ |       |     | | \\  | |______    |    |  |  |     \n |    \\_ |______ |_____  |_____| |  \\_| |          |    |__|__|                                                                         \n\"\"\"\n\nbanner15=\"\"\" ____   ____   ___   ___   __  __  ____ ______ __    __   \n || \\\\ ||     //    // \\\\  ||\\ || ||    | || | ||    ||   \n ||_// ||==  ((    ((   )) ||\\\\|| ||==    ||   \\\\ /\\ //   \n || \\\\ ||___  \\\\__  \\\\_//  || \\|| ||      ||    \\V/\\V/                                                           \n\"\"\"\n\nbanner16=\"\"\" ______                         _______ _______ _  _  _   \n(_____ \\                       (_______|_______|_)(_)(_)  \n _____) )_____  ____ ___  ____  _____      _    _  _  _   \n|  __  /| ___ |/ ___) _ \\|  _ \\|  ___)    | |  | || || |  \n| |  \\ \\| ____( (__| |_| | | | | |        | |  | || || |  \n|_|   |_|_____)\\____)___/|_| |_|_|        |_|   \\_____/                                                             \n\"\"\"\n\nbanner17=\"\"\" ____ ____ ____ ____ ____ ____ ____ ____ _________ \n||R |||e |||c |||o |||n |||F |||T |||W |||       ||\n||__|||__|||__|||__|||__|||__|||__|||__|||_______||\n|/__\\|/__\\|/__\\|/__\\|/__\\|/__\\|/__\\|/__\\|/_______\\|\n\"\"\"\n\nbanner18=\"\"\" __                 ___ ___          \n )_) _   _  _   _   )_   ) \\  X  /   \n/ \\ )_) (_ (_) ) ) (    (   \\/ \\/    \n   (_                                \n\"\"\"\n\nbanner19=\"\"\"                           ______ _________          __\n                          |  ____|__   __\\ \\        / /\n  _ __ ___  ___ ___  _ __ | |__     | |   \\ \\  /\\  / / \n | '__/ _ \\/ __/ _ \\| '_ \\|  __|    | |    \\ \\/  \\/ /  \n | | |  __/ (_| (_) | | | | |       | |     \\  /\\  /   \n |_|  \\___|\\___\\___/|_| |_|_|       |_|      \\/  \\/                                                           \n\"\"\"\n\nbanner20=\"\"\" :::====  :::===== :::===== :::====  :::= === :::===== :::==== :::  ===  ===\n :::  === :::      :::      :::  === :::===== :::      :::==== :::  ===  ===\n =======  ======   ===      ===  === ======== ======     ===   ===  ===  ===\n === ===  ===      ===      ===  === === ==== ===        ===    =========== \n ===  === ========  =======  ======  ===  === ===        ===     ==== ====  \n\"\"\"\n\nbanner21=\"\"\"   _   _   _   _   _   _   _   _  \n  / \\ / \\ / \\ / \\ / \\ / \\ / \\ / \\ \n ( r | e | c | o | n | F | T | W )\n  \\_/ \\_/ \\_/ \\_/ \\_/ \\_/ \\_/ \\_/ \n\"\"\"          \n\nbanner22=\"\"\"                              _______ _______ ________ \n.----.-----.----.-----.-----.|    ___|_     _|  |  |  |\n|   _|  -__|  __|  _  |     ||    ___| |   | |  |  |  |\n|__| |_____|____|_____|__|__||___|     |___| |________|                                                      \n\"\"\"\n\nbanner23=\"\"\"                     ________) ______) __       __) \n                    (, /      (, /    (, )  |  /    \n __   _  _  _____     /___,     /        | /| /     \n/ (__(/_(__(_) / (_) /       ) /         |/ |/      \n                  (_/       (_/          /  |       \n\"\"\"\n\nbanner24=\"\"\"                                                   ________  ________  __       __ \n                                                  /        |/        |/  |  _  /  |\n  ______    ______    _______   ______   _______  $$$$$$$$/ $$$$$$$$/ $$ | / \\ $$ |\n /      \\  /      \\  /       | /      \\ /       \\ $$ |__       $$ |   $$ |/$  \\$$ |\n/$$$$$$  |/$$$$$$  |/$$$$$$$/ /$$$$$$  |$$$$$$$  |$$    |      $$ |   $$ /$$$  $$ |\n$$ |  $$/ $$    $$ |$$ |      $$ |  $$ |$$ |  $$ |$$$$$/       $$ |   $$ $$/$$ $$ |\n$$ |      $$$$$$$$/ $$ \\_____ $$ \\__$$ |$$ |  $$ |$$ |         $$ |   $$$$/  $$$$ |\n$$ |      $$       |$$       |$$    $$/ $$ |  $$ |$$ |         $$ |   $$$/    $$$ |\n$$/        $$$$$$$/  $$$$$$$/  $$$$$$/  $$/   $$/ $$/          $$/    $$/      $$/ \n\"\"\"\n"
        },
        {
          "name": "images",
          "type": "tree",
          "content": null
        },
        {
          "name": "install.sh",
          "type": "blob",
          "size": 27.6875,
          "content": "#!/usr/bin/env bash\n\n# Enable strict error handling\n#IFS=$'\\n\\t'\n\n# Load main configuration\nCONFIG_FILE=\"./reconftw.cfg\"\n\nif [[ ! -f $CONFIG_FILE ]]; then\n\techo -e \"${bred}[!] Config file reconftw.cfg not found.${reset}\"\n\texit 1\nfi\n\nsource \"$CONFIG_FILE\"\n\n# Initialize variables\ndir=\"${tools}\"\ndouble_check=false\n\n# ARM Detection\nARCH=$(uname -m)\ncase \"$ARCH\" in\namd64 | x86_64)\n\tIS_ARM=\"False\"\n\t;;\narm64 | armv6l | aarch64)\n\tIS_ARM=\"True\"\n\tif [[ $ARCH == \"arm64\" ]]; then\n\t\tRPI_4=\"True\"\n\t\tRPI_3=\"False\"\n\telse\n\t\tRPI_4=\"False\"\n\t\tRPI_3=\"True\"\n\tfi\n\t;;\n*)\n\tIS_ARM=\"False\"\n\t;;\nesac\n\n# macOS Detection\nIS_MAC=$([[ $OSTYPE == \"darwin\"* ]] && echo \"True\" || echo \"False\")\n\n# Check Bash version\nBASH_VERSION_NUM=$(bash --version | awk 'NR==1{print $4}' | cut -d'.' -f1)\nif [[ $BASH_VERSION_NUM -lt 4 ]]; then\n\techo -e \"${bred}Your Bash version is lower than 4, please update.${reset}\"\n\tif [[ $IS_MAC == \"True\" ]]; then\n\t\techo -e \"${yellow}For macOS, run 'brew install bash' and rerun the installer in a new terminal.${reset}\"\n\tfi\n\texit 1\nfi\n\n# Declare Go tools and their installation commands\ndeclare -A gotools=(\n\t[\"gf\"]=\"go install -v github.com/tomnomnom/gf@latest\"\n\t[\"brutespray\"]=\"go install -v github.com/x90skysn3k/brutespray@latest\"\n\t[\"qsreplace\"]=\"go install -v github.com/tomnomnom/qsreplace@latest\"\n\t[\"ffuf\"]=\"go install -v github.com/ffuf/ffuf/v2@latest\"\n\t[\"github-subdomains\"]=\"go install -v github.com/gwen001/github-subdomains@latest\"\n\t[\"gitlab-subdomains\"]=\"go install -v github.com/gwen001/gitlab-subdomains@latest\"\n\t[\"nuclei\"]=\"go install -v github.com/projectdiscovery/nuclei/v3/cmd/nuclei@latest\"\n\t[\"anew\"]=\"go install -v github.com/tomnomnom/anew@latest\"\n\t[\"notify\"]=\"go install -v github.com/projectdiscovery/notify/cmd/notify@latest\"\n\t[\"unfurl\"]=\"go install -v github.com/tomnomnom/unfurl@v0.3.0\"\n\t[\"httpx\"]=\"go install -v github.com/projectdiscovery/httpx/cmd/httpx@latest\"\n\t[\"github-endpoints\"]=\"go install -v github.com/gwen001/github-endpoints@latest\"\n\t[\"dnsx\"]=\"go install -v github.com/projectdiscovery/dnsx/cmd/dnsx@latest\"\n\t[\"subjs\"]=\"go install -v github.com/lc/subjs@latest\"\n\t[\"Gxss\"]=\"go install -v github.com/KathanP19/Gxss@latest\"\n\t[\"katana\"]=\"go install -v github.com/projectdiscovery/katana/cmd/katana@latest\"\n\t[\"crlfuzz\"]=\"go install -v github.com/dwisiswant0/crlfuzz/cmd/crlfuzz@latest\"\n\t[\"dalfox\"]=\"go install -v github.com/hahwul/dalfox/v2@latest\"\n\t[\"puredns\"]=\"go install -v github.com/d3mondev/puredns/v2@latest\"\n\t[\"interactsh-client\"]=\"go install -v github.com/projectdiscovery/interactsh/cmd/interactsh-client@latest\"\n\t[\"analyticsrelationships\"]=\"go install -v github.com/Josue87/analyticsrelationships@latest\"\n\t[\"gotator\"]=\"go install -v github.com/Josue87/gotator@latest\"\n\t[\"roboxtractor\"]=\"go install -v github.com/Josue87/roboxtractor@latest\"\n\t[\"mapcidr\"]=\"go install -v github.com/projectdiscovery/mapcidr/cmd/mapcidr@latest\"\n\t[\"cdncheck\"]=\"go install -v github.com/projectdiscovery/cdncheck/cmd/cdncheck@latest\"\n\t[\"dnstake\"]=\"go install -v github.com/pwnesia/dnstake/cmd/dnstake@latest\"\n\t[\"tlsx\"]=\"go install -v github.com/projectdiscovery/tlsx/cmd/tlsx@latest\"\n\t[\"gitdorks_go\"]=\"go install -v github.com/damit5/gitdorks_go@latest\"\n\t[\"smap\"]=\"go install -v github.com/s0md3v/smap/cmd/smap@latest\"\n\t[\"dsieve\"]=\"go install -v github.com/trickest/dsieve@master\"\n\t[\"inscope\"]=\"go install -v github.com/tomnomnom/hacks/inscope@latest\"\n\t[\"enumerepo\"]=\"go install -v github.com/trickest/enumerepo@latest\"\n\t[\"Web-Cache-Vulnerability-Scanner\"]=\"go install -v github.com/Hackmanit/Web-Cache-Vulnerability-Scanner@latest\"\n\t[\"subfinder\"]=\"go install -v github.com/projectdiscovery/subfinder/v2/cmd/subfinder@latest\"\n\t[\"hakip2host\"]=\"go install -v github.com/hakluke/hakip2host@latest\"\n\t[\"gau\"]=\"go install -v github.com/lc/gau/v2/cmd/gau@latest\"\n\t[\"mantra\"]=\"go install -v github.com/MrEmpy/mantra@latest\"\n\t[\"crt\"]=\"go install -v github.com/cemulus/crt@latest\"\n\t[\"s3scanner\"]=\"go install -v github.com/sa7mon/s3scanner@latest\"\n\t[\"nmapurls\"]=\"go install -v github.com/sdcampbell/nmapurls@latest\"\n\t[\"shortscan\"]=\"go install -v github.com/bitquark/shortscan/cmd/shortscan@latest\"\n\t[\"sns\"]=\"go install github.com/sw33tLie/sns@latest\"\n\t[\"ppmap\"]=\"go install -v github.com/kleiton0x00/ppmap@latest\"\n\t[\"sourcemapper\"]=\"go install -v github.com/denandz/sourcemapper@latest\"\n\t[\"jsluice\"]=\"go install -v github.com/BishopFox/jsluice/cmd/jsluice@latest\"\n\t[\"urlfinder\"]=\"go install -v github.com/projectdiscovery/urlfinder/cmd/urlfinder@latest\"\n)\n\n# Declare repositories and their paths\ndeclare -A repos=(\n\t[\"dorks_hunter\"]=\"six2dez/dorks_hunter\"\n\t[\"dnsvalidator\"]=\"vortexau/dnsvalidator\"\n\t[\"interlace\"]=\"codingo/Interlace\"\n\t[\"wafw00f\"]=\"EnableSecurity/wafw00f\"\n\t[\"gf\"]=\"tomnomnom/gf\"\n\t[\"Gf-Patterns\"]=\"1ndianl33t/Gf-Patterns\"\n\t[\"Corsy\"]=\"s0md3v/Corsy\"\n\t[\"CMSeeK\"]=\"Tuhinshubhra/CMSeeK\"\n\t[\"fav-up\"]=\"pielco11/fav-up\"\n\t[\"massdns\"]=\"blechschmidt/massdns\"\n\t[\"Oralyzer\"]=\"r0075h3ll/Oralyzer\"\n\t[\"testssl\"]=\"drwetter/testssl.sh\"\n\t[\"commix\"]=\"commixproject/commix\"\n\t[\"JSA\"]=\"w9w/JSA\"\n\t[\"CloudHunter\"]=\"belane/CloudHunter\"\n\t[\"ultimate-nmap-parser\"]=\"shifty0g/ultimate-nmap-parser\"\n\t[\"pydictor\"]=\"LandGrey/pydictor\"\n\t[\"gitdorks_go\"]=\"damit5/gitdorks_go\"\n\t[\"urless\"]=\"xnl-h4ck3r/urless\"\n\t[\"smuggler\"]=\"defparam/smuggler\"\n\t[\"Web-Cache-Vulnerability-Scanner\"]=\"Hackmanit/Web-Cache-Vulnerability-Scanner\"\n\t[\"regulator\"]=\"cramppet/regulator\"\n\t[\"ghauri\"]=\"r0oth3x49/ghauri\"\n\t[\"gitleaks\"]=\"gitleaks/gitleaks\"\n\t[\"trufflehog\"]=\"trufflesecurity/trufflehog\"\n\t[\"nomore403\"]=\"devploit/nomore403\"\n\t[\"SwaggerSpy\"]=\"UndeadSec/SwaggerSpy\"\n\t[\"LeakSearch\"]=\"JoelGMSec/LeakSearch\"\n\t[\"ffufPostprocessing\"]=\"Damian89/ffufPostprocessing\"\n\t[\"misconfig-mapper\"]=\"intigriti/misconfig-mapper\"\n\t[\"Spoofy\"]=\"MattKeeley/Spoofy\"\n\t[\"xnLinkFinder\"]=\"xnl-h4ck3r/xnLinkFinder\"\n\t[\"porch-pirate\"]=\"MandConsultingGroup/porch-pirate\"\n\t[\"MetaFinder\"]=\"Josue87/MetaFinder\"\n\t[\"EmailFinder\"]=\"Josue87/EmailFinder\"\n)\n\n# Function to display the banner\nfunction banner() {\n\ttput clear\n\tcat <<\"EOF\"\n\n  ‚ñà‚ñà‚ñÄ‚ñà‚ñà‚ñà  ‚ñì‚ñà‚ñà‚ñà‚ñà‚ñà  ‚ñÑ‚ñà‚ñà‚ñà‚ñà‚ñÑ   ‚ñí‚ñà‚ñà‚ñà‚ñà‚ñà   ‚ñà‚ñà‚ñà‚ñÑ    ‚ñà   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñí‚ñÑ‚ñÑ‚ñÑ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñì ‚ñà     ‚ñà‚ñë\n ‚ñì‚ñà‚ñà ‚ñí ‚ñà‚ñà‚ñí‚ñì‚ñà   ‚ñÄ ‚ñí‚ñà‚ñà‚ñÄ ‚ñÄ‚ñà  ‚ñí‚ñà‚ñà‚ñí  ‚ñà‚ñà‚ñí ‚ñà‚ñà ‚ñÄ‚ñà   ‚ñà ‚ñì‚ñà‚ñà   ‚ñí ‚ñì  ‚ñà‚ñà‚ñí ‚ñì‚ñí‚ñì‚ñà‚ñë ‚ñà ‚ñë‚ñà‚ñë\n ‚ñì‚ñà‚ñà ‚ñë‚ñÑ‚ñà ‚ñí‚ñí‚ñà‚ñà‚ñà   ‚ñí‚ñì‚ñà    ‚ñÑ ‚ñí‚ñà‚ñà‚ñë  ‚ñà‚ñà‚ñí‚ñì‚ñà‚ñà  ‚ñÄ‚ñà ‚ñà‚ñà‚ñí‚ñí‚ñà‚ñà‚ñà‚ñà ‚ñë ‚ñí ‚ñì‚ñà‚ñà‚ñë ‚ñí‚ñë‚ñí‚ñà‚ñë ‚ñà ‚ñë‚ñà\n ‚ñí‚ñà‚ñà‚ñÄ‚ñÄ‚ñà‚ñÑ  ‚ñí‚ñì‚ñà  ‚ñÑ ‚ñí‚ñì‚ñì‚ñÑ ‚ñÑ‚ñà‚ñà‚ñí‚ñí‚ñà‚ñà   ‚ñà‚ñà‚ñë‚ñì‚ñà‚ñà‚ñí  ‚ñê‚ñå‚ñà‚ñà‚ñí‚ñë‚ñì‚ñà‚ñí  ‚ñë ‚ñë ‚ñì‚ñà‚ñà‚ñì ‚ñë ‚ñë‚ñà‚ñë ‚ñà ‚ñë‚ñà\n ‚ñë‚ñà‚ñà‚ñì ‚ñí‚ñà‚ñà‚ñí‚ñë‚ñí‚ñà‚ñà‚ñà‚ñà‚ñí‚ñí ‚ñì‚ñà‚ñà‚ñà‚ñÄ ‚ñë‚ñë ‚ñà‚ñà‚ñà‚ñà‚ñì‚ñí‚ñë‚ñí‚ñà‚ñà‚ñë   ‚ñì‚ñà‚ñà‚ñë‚ñë‚ñí‚ñà‚ñë      ‚ñí‚ñà‚ñà‚ñí ‚ñë ‚ñë‚ñë‚ñà‚ñà‚ñí‚ñà‚ñà‚ñì\n ‚ñë ‚ñí‚ñì ‚ñë‚ñí‚ñì‚ñë‚ñë‚ñë ‚ñí‚ñë ‚ñë‚ñë ‚ñë‚ñí ‚ñí  ‚ñë‚ñë ‚ñí‚ñë‚ñí‚ñë‚ñí‚ñë ‚ñë ‚ñí‚ñë   ‚ñí ‚ñí  ‚ñí ‚ñë      ‚ñí ‚ñë‚ñë   ‚ñë ‚ñì‚ñë‚ñí ‚ñí\n   ‚ñë‚ñí ‚ñë ‚ñí‚ñë ‚ñë ‚ñë  ‚ñë  ‚ñë  ‚ñí     ‚ñë ‚ñí ‚ñí‚ñë ‚ñë ‚ñë‚ñë   ‚ñë ‚ñí‚ñë ‚ñë          ‚ñë      ‚ñí ‚ñë ‚ñë\n   ‚ñë‚ñë   ‚ñë    ‚ñë   ‚ñë        ‚ñë ‚ñë ‚ñë ‚ñí     ‚ñë   ‚ñë ‚ñë  ‚ñë ‚ñë      ‚ñë        ‚ñë   ‚ñë\n    ‚ñë        ‚ñë  ‚ñë‚ñë ‚ñë          ‚ñë ‚ñë           ‚ñë                      ‚ñë\n\n                 ${reconftw_version}                                         by @six2dez\n\nEOF\n}\n\n# Function to install Go tools\nfunction install_tools() {\n\techo -e \"${bblue}Running: Installing Golang tools (${#gotools[@]})${reset}\\n\"\n\n\tlocal go_step=0\n\tlocal failed_tools=()\n\tfor gotool in \"${!gotools[@]}\"; do\n\t\t((go_step++))\n\t\tif [[ $upgrade_tools == \"false\" ]]; then\n\t\t\tif command -v \"$gotool\" &>/dev/null; then\n\t\t\t\techo -e \"[${yellow}SKIPPING${reset}] $gotool already installed at $(command -v \"$gotool\")\"\n\t\t\t\tcontinue\n\t\t\tfi\n\t\tfi\n\n\t\t# Install the Go tool\n\t\teval \"${gotools[$gotool]}\" &>/dev/null\n\t\texit_status=$?\n\t\tif [[ $exit_status -eq 0 ]]; then\n\t\t\techo -e \"${yellow}$gotool installed (${go_step}/${#gotools[@]})${reset}\"\n\t\telse\n\t\t\techo -e \"${red}Unable to install $gotool, try manually (${go_step}/${#gotools[@]})${reset}\"\n\t\t\tfailed_tools+=(\"$gotool\")\n\t\t\tdouble_check=true\n\t\tfi\n\tdone\n\n\techo -e \"\\n${bblue}Running: Installing repositories (${#repos[@]})${reset}\\n\"\n\n\tlocal repos_step=0\n\tlocal failed_repos=()\n\n\tfor repo in \"${!repos[@]}\"; do\n\t\t((repos_step++))\n\t\tif [[ $upgrade_tools == \"false\" ]]; then\n\t\t\tif [[ -d \"${dir}/${repo}\" ]]; then\n\t\t\t\techo -e \"[${yellow}SKIPPING${reset}] Repository $repo already cloned in ${dir}/${repo}\"\n\t\t\t\tcontinue\n\t\t\tfi\n\t\tfi\n\t\t# Clone the repository\n\t\tif [[ ! -d \"${dir}/${repo}\" || -z \"$(ls -A \"${dir}/${repo}\")\" ]]; then\n\t\t\tgit clone --filter=\"blob:none\" \"https://github.com/${repos[$repo]}\" \"${dir}/${repo}\" &>/dev/null\n\t\t\texit_status=$?\n\t\t\tif [[ $exit_status -ne 0 ]]; then\n\t\t\t\techo -e \"${red}Unable to clone repository $repo.${reset}\"\n\t\t\t\tfailed_repos+=(\"$repo\")\n\t\t\t\tdouble_check=true\n\t\t\t\tcontinue\n\t\t\tfi\n\t\tfi\n\n\t\t# Navigate to the repository directory\n\t\tcd \"${dir}/${repo}\" || {\n\t\t\techo -e \"${red}Failed to navigate to directory '${dir}/${repo}'${reset}\"\n\t\t\tfailed_repos+=(\"$repo\")\n\t\t\tdouble_check=true\n\t\t\tcontinue\n\t\t}\n\n\t\t# Pull the latest changes\n\t\tgit pull &>/dev/null\n\t\texit_status=$?\n\t\tif [[ $exit_status -ne 0 ]]; then\n\t\t\techo -e \"${red}Failed to pull updates for repository $repo.${reset}\"\n\t\t\tfailed_repos+=(\"$repo\")\n\t\t\tdouble_check=true\n\t\t\tcontinue\n\t\tfi\n\n\t\t# Install dependencies if setup.py exists\n\t\tif [[ -f \"setup.py\" ]]; then\n\t\t\teval \"$SUDO pip3 install . $DEBUG_STD\" &>/dev/null\n\t\tfi\n\n\t\t# Special handling for certain repositories\n\t\tcase \"$repo\" in\n\t\t\"massdns\")\n\t\t\tmake &>/dev/null && strip -s bin/massdns && \"$SUDO\" cp bin/massdns /usr/local/bin/ &>/dev/null\n\t\t\t;;\n\t\t\"gitleaks\")\n\t\t\tmake build &>/dev/null && \"$SUDO\" cp ./gitleaks /usr/local/bin/ &>/dev/null\n\t\t\t;;\n\t\t\"nomore403\")\n\t\t\tgo get &>/dev/null\n\t\t\tgo build &>/dev/null\n\t\t\tchmod +x ./nomore403\n\t\t\t;;\n\t\t\"ffufPostprocessing\")\n\t\t\tgit reset --hard origin/main &>/dev/null\n\t\t\tgit pull &>/dev/null\n\t\t\tgo build -o ffufPostprocessing main.go &>/dev/null\n\t\t\tchmod +x ./ffufPostprocessing\n\t\t\t;;\n\t\t\"misconfig-mapper\")\n\t\t\tgit reset --hard origin/main &>/dev/null\n\t\t\tgit pull &>/dev/null\n\t\t\tgo build -o misconfig-mapper &>/dev/null\n\t\t\tchmod +x ./misconfig-mapper\n\t\t\t;;\n\t\tesac\n\n\t\t# Copy gf patterns if applicable\n\t\tif [[ $repo == \"gf\" ]]; then\n\t\t\tcp -r examples ${HOME}/.gf &>/dev/null\n\t\telif [[ $repo == \"Gf-Patterns\" ]]; then\n\t\t\tmv ./*.json ${HOME}/.gf &>/dev/null\n\t\tfi\n\n\t\t# Return to the main directory\n\t\tcd \"$dir\" || {\n\t\t\techo -e \"${red}Failed to navigate back to directory '$dir'.${reset}\"\n\t\t\texit 1\n\t\t}\n\n\t\techo -e \"${yellow}$repo installed (${repos_step}/${#repos[@]})${reset}\"\n\tdone\n\n\t# Notify and ensure subfinder is installed twice (as per original script)\n\tnotify &>/dev/null\n\tsubfinder &>/dev/null\n\tsubfinder &>/dev/null\n\n\t# Handle failed installations\n\tif [[ ${#failed_tools[@]} -ne 0 ]]; then\n\t\techo -e \"\\n${red}Failed to install the following Go tools: ${failed_tools[*]}${reset}\"\n\tfi\n\n\tif [[ ${#failed_repos[@]} -ne 0 ]]; then\n\t\techo -e \"\\n${red}Failed to clone or update the following repositories:\\n${failed_repos[*]}${reset}\"\n\tfi\n}\n\n# Function to reset git proxy settings\nfunction reset_git_proxies() {\n\tgit config --global --unset http.proxy || true\n\tgit config --global --unset https.proxy || true\n}\n\n# Function to check for updates\nfunction check_updates() {\n\techo -e \"${bblue}Running: Looking for new reconFTW version${reset}\\n\"\n\n\tif timeout 10 git fetch; then\n\t\tBRANCH=$(git rev-parse --abbrev-ref HEAD)\n\t\tHEADHASH=$(git rev-parse HEAD)\n\t\tUPSTREAMHASH=$(git rev-parse \"${BRANCH}@{upstream}\")\n\n\t\tif [[ $HEADHASH != \"$UPSTREAMHASH\" ]]; then\n\t\t\techo -e \"${yellow}A new version is available. Updating...${reset}\\n\"\n\t\t\tif git status --porcelain | grep -q 'reconftw.cfg$'; then\n\t\t\t\tmv reconftw.cfg reconftw.cfg_bck\n\t\t\t\techo -e \"${yellow}reconftw.cfg has been backed up to reconftw.cfg_bck${reset}\\n\"\n\t\t\tfi\n\t\t\tgit reset --hard &>/dev/null\n\t\t\tgit pull &>/dev/null\n\t\t\techo -e \"${bgreen}Updated! Running the new installer version...${reset}\\n\"\n\t\telse\n\t\t\techo -e \"${bgreen}reconFTW is already up to date!${reset}\\n\"\n\t\tfi\n\telse\n\t\techo -e \"\\n${bred}[!] Unable to check for updates.${reset}\\n\"\n\tfi\n}\n\n# Function to install Golang\nfunction install_golang_version() {\n\tlocal version=\"go1.20.7\"\n\tlocal latest_version\n\tlatest_version=$(curl -s https://go.dev/VERSION?m=text | head -1 || echo \"go1.20.7\")\n\tif [[ $latest_version == g* ]]; then\n\t\tversion=\"$latest_version\"\n\tfi\n\n\techo -e \"${bblue}Running: Installing/Updating Golang($version) ${reset}\\n\"\n\n\tif [[ $install_golang == \"true\" ]]; then\n\t\tif command -v go &>/dev/null && [[ $version == \"$(go version | awk '{print $3}')\" ]]; then\n\t\t\techo -e \"${bgreen}Golang is already installed and up to date.${reset}\\n\"\n\t\telse\n\t\t\t\"$SUDO\" rm -rf /usr/local/go &>/dev/null || true\n\n\t\t\tcase \"$ARCH\" in\n\t\t\tarm64 | aarch64)\n\t\t\t\tif [[ $RPI_4 == \"True\" ]]; then\n\t\t\t\t\twget \"https://dl.google.com/go/${version}.linux-arm64.tar.gz\" -O \"/tmp/${version}.linux-arm64.tar.gz\" &>/dev/null\n\t\t\t\t\t\"$SUDO\" tar -C /usr/local -xzf \"/tmp/${version}.linux-arm64.tar.gz\" &>/dev/null\n\t\t\t\telif [[ $RPI_3 == \"True\" ]]; then\n\t\t\t\t\twget \"https://dl.google.com/go/${version}.linux-armv6l.tar.gz\" -O \"/tmp/${version}.linux-armv6l.tar.gz\" &>/dev/null\n\t\t\t\t\t\"$SUDO\" tar -C /usr/local -xzf \"/tmp/${version}.linux-armv6l.tar.gz\" &>/dev/null\n\t\t\t\tfi\n\t\t\t\t;;\n\t\t\t*)\n\t\t\t\tif [[ $IS_MAC == \"True\" ]]; then\n\t\t\t\t\tif [[ $IS_ARM == \"True\" ]]; then\n\t\t\t\t\t\twget \"https://dl.google.com/go/${version}.darwin-arm64.tar.gz\" -O \"/tmp/${version}.darwin-arm64.tar.gz\" &>/dev/null\n\t\t\t\t\t\t\"$SUDO\" tar -C /usr/local -xzf \"/tmp/${version}.darwin-arm64.tar.gz\" &>/dev/null\n\t\t\t\t\telse\n\t\t\t\t\t\twget \"https://dl.google.com/go/${version}.darwin-amd64.tar.gz\" -O \"/tmp/${version}.darwin-amd64.tar.gz\" &>/dev/null\n\t\t\t\t\t\t\"$SUDO\" tar -C /usr/local -xzf \"/tmp/${version}.darwin-amd64.tar.gz\" &>/dev/null\n\t\t\t\t\tfi\n\t\t\t\telse\n\t\t\t\t\twget \"https://dl.google.com/go/${version}.linux-amd64.tar.gz\" -O \"/tmp/${version}.linux-amd64.tar.gz\" &>/dev/null\n\t\t\t\t\t\"$SUDO\" tar -C /usr/local -xzf \"/tmp/${version}.linux-amd64.tar.gz\" &>/dev/null\n\t\t\t\tfi\n\t\t\t\t;;\n\t\t\tesac\n\n\t\t\t\"$SUDO\" ln -sf /usr/local/go/bin/go /usr/local/bin/ 2>/dev/null\n\t\t\texport GOROOT=/usr/local/go\n\t\t\texport GOPATH=\"${HOME}/go\"\n\t\t\texport PATH=\"$GOPATH/bin:$GOROOT/bin:$HOME/.local/bin:$PATH\"\n\n\t\t\t# Append Go environment variables to shell profile\n\t\t\tcat <<EOF >>${HOME}/\"${profile_shell}\"\n\n# Golang environment variables\nexport GOROOT=/usr/local/go\nexport GOPATH=\\$HOME/go\nexport PATH=\\$GOPATH/bin:\\$GOROOT/bin:\\$HOME/.local/bin:\\$PATH\nEOF\n\t\tfi\n\telse\n\t\techo -e \"${byellow}Golang will not be configured according to the user's preferences (install_golang=false in reconftw.cfg).${reset}\\n\"\n\tfi\n\n\t# Validate Go environment variables\n\tif [[ -z ${GOPATH-} ]]; then\n\t\techo -e \"${bred}GOPATH environment variable not detected. Add Golang environment variables to your \\$HOME/.bashrc or \\$HOME/.zshrc:${reset}\"\n\t\techo -e \"export GOROOT=/usr/local/go\"\n\t\techo -e 'export GOPATH=$HOME/go'\n\t\techo -e \"export PATH=\\$GOPATH/bin:\\$GOROOT/bin:\\$PATH\\n\"\n\t\texit 1\n\tfi\n\n\tif [[ -z ${GOROOT-} ]]; then\n\t\techo -e \"${bred}GOROOT environment variable not detected. Add Golang environment variables to your \\$HOME/.bashrc or \\$HOME/.zshrc:${reset}\"\n\t\techo -e \"export GOROOT=/usr/local/go\"\n\t\techo -e 'export GOPATH=$HOME/go'\n\t\techo -e \"export PATH=\\$GOPATH/bin:\\$GOROOT/bin:\\$PATH\\n\"\n\t\texit 1\n\tfi\n}\n\n# Function to install system packages based on OS\nfunction install_system_packages() {\n\n\tif [[ -f /etc/debian_version ]]; then\n\t\tinstall_apt\n\telif [[ -f /etc/redhat-release ]]; then\n\t\tinstall_yum\n\telif [[ -f /etc/arch-release ]]; then\n\t\tinstall_pacman\n\telif [[ $IS_MAC == \"True\" ]]; then\n\t\tinstall_brew\n\telif [[ -f /etc/os-release ]]; then\n\t\tinstall_yum # Assuming RedHat-based\n\telse\n\t\techo -e \"${bred}[!] Unsupported OS. Please install dependencies manually.${reset}\"\n\t\texit 1\n\tfi\n}\n\n# Function to install required packages for Debian-based systems\nfunction install_apt() {\n\t\"$SUDO\" apt update -y &>/dev/null\n\t\"$SUDO\" DEBIAN_FRONTEND=\"noninteractive\" apt install -y chromium-browser python3 python3-pip python3-virtualenv build-essential gcc cmake ruby whois git curl libpcap-dev wget zip python3-dev pv dnsutils libssl-dev libffi-dev libxml2-dev libxslt1-dev zlib1g-dev nmap jq apt-transport-https lynx medusa xvfb libxml2-utils procps bsdmainutils libdata-hexdump-perl libnss3 libatk1.0-0 libatk-bridge2.0-0 libcups2 libxkbcommon-x11-0 libxcomposite-dev libxdamage1 libxrandr2 libgbm-dev libpangocairo-1.0-0 libasound2 &>/dev/null\n\tcurl https://sh.rustup.rs -sSf | sh -s -- -y >/dev/null 2>&1\n\tsource \"${HOME}/.cargo/env\"\n\tcargo install ripgen &>/dev/null\n}\n\n# Function to install required packages for macOS\nfunction install_brew() {\n\tif command -v brew &>/dev/null; then\n\t\techo -e \"${bgreen}brew is already installed.${reset}\\n\"\n\telse\n\t\t/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n\tfi\n\tbrew update &>/dev/null\n\tbrew install --cask chromium &>/dev/null\n\tbrew install bash coreutils python massdns jq gcc cmake ruby git curl libpcap-dev wget zip python3-dev pv dnsutils whois libssl-dev libffi-dev libxml2-dev libxslt-dev zlib libnss3 atk bridge2.0 cups xkbcommon xcomposite xdamage xrandr gbm pangocairo alsa libxml2-utils &>/dev/null\n\tbrew install rustup &>/dev/null\n\trustup-init -y &>/dev/null\n\tcargo install ripgen &>/dev/null\n}\n\n# Function to install required packages for RedHat-based systems\nfunction install_yum() {\n\t\"$SUDO\" yum groupinstall \"Development Tools\" -y &>/dev/null\n\t\"$SUDO\" yum install -y python3 python3-pip gcc cmake ruby git curl libpcap whois wget zip pv bind-utils openssl-devel libffi-devel libxml2-devel libxslt-devel zlib-devel nmap jq lynx medusa xorg-x11-server-xvfb &>/dev/null\n\tcurl https://sh.rustup.rs -sSf | sh -s -- -y >/dev/null 2>&1\n\tsource \"${HOME}/.cargo/env\"\n\tcargo install ripgen &>/dev/null\n}\n\n# Function to install required packages for Arch-based systems\nfunction install_pacman() {\n\t\"$SUDO\" pacman -Sy --noconfirm python python-pip base-devel gcc cmake ruby git curl libpcap whois wget zip pv bind openssl libffi libxml2 libxslt zlib nmap jq lynx medusa xorg-server-xvfb &>/dev/null\n\tcurl https://sh.rustup.rs -sSf | sh -s -- -y >/dev/null 2>&1\n\tsource \"${HOME}/.cargo/env\"\n\tcargo install ripgen &>/dev/null\n}\n\n# Function to perform initial setup\nfunction initial_setup() {\n\tbanner\n\treset_git_proxies\n\n\techo -e \"${bblue}Running: Checking for updates${reset}\\n\"\n\tcheck_updates\n\n\techo -e \"${bblue}Running: Installing system packages${reset}\\n\"\n\tinstall_system_packages\n\n\tinstall_golang_version\n\n\techo -e \"${bblue}Running: Installing Python requirements${reset}\\n\"\n\tmkdir -p ${HOME}/.gf\n\tmkdir -p \"$tools\"\n\tmkdir -p ${HOME}/.config/notify/\n\tmkdir -p ${HOME}/.config/nuclei/\n\ttouch \"${dir}/.github_tokens\"\n\ttouch \"${dir}/.gitlab_tokens\"\n\n\twget -N -c https://bootstrap.pypa.io/get-pip.py -O /tmp/get-pip.py &>/dev/null\n\tpython3 /tmp/get-pip.py &>/dev/null\n\trm -f /tmp/get-pip.py\n\n\tinstall_tools\n\n\t# Repositorios con configuraciones especiales\n\tprintf \"${bblue}\\nRunning: Configuring special repositories${reset}\\n\"\n\n\t# Nuclei Templates\n\tif [[ ! -d ${NUCLEI_TEMPLATES_PATH} ]]; then\n\t\t#printf \"${yellow}Cloning Nuclei templates...${reset}\\n\"\n\t\teval git clone https://github.com/projectdiscovery/nuclei-templates.git \"${NUCLEI_TEMPLATES_PATH}\" $DEBUG_STD\n\t\teval git clone https://github.com/geeknik/the-nuclei-templates.git \"${NUCLEI_TEMPLATES_PATH}/extra_templates\" $DEBUG_STD\n\t\teval git clone https://github.com/projectdiscovery/fuzzing-templates ${tools}/fuzzing-templates $DEBUG_STD\n\t\teval nuclei -update-templates update-template-dir \"${NUCLEI_TEMPLATES_PATH}\" $DEBUG_STD\n\telse\n\t\t#printf \"${yellow}Updating Nuclei templates...${reset}\\n\"\n\t\teval git -C \"${NUCLEI_TEMPLATES_PATH}\" pull $DEBUG_STD\n\t\teval git -C \"${NUCLEI_TEMPLATES_PATH}/extra_templates\" pull $DEBUG_STD\n\t\teval git -C \"${tools}/fuzzing-templates\" pull $DEBUG_STD\n\tfi\n\n\t# sqlmap\n\tif [[ ! -d \"${dir}/sqlmap\" ]]; then\n\t\t#printf \"${yellow}Cloning sqlmap...${reset}\\n\"\n\t\teval git clone --depth 1 https://github.com/sqlmapproject/sqlmap.git \"${dir}/sqlmap\" $DEBUG_STD\n\telse\n\t\t#printf \"${yellow}Updating sqlmap...${reset}\\n\"\n\t\teval git -C \"${dir}/sqlmap\" pull $DEBUG_STD\n\tfi\n\n\t# testssl.sh\n\tif [[ ! -d \"${dir}/testssl.sh\" ]]; then\n\t\t#printf \"${yellow}Cloning testssl.sh...${reset}\\n\"\n\t\teval git clone --depth 1 https://github.com/drwetter/testssl.sh.git \"${dir}/testssl.sh\" $DEBUG_STD\n\telse\n\t\t#printf \"${yellow}Updating testssl.sh...${reset}\\n\"\n\t\teval git -C \"${dir}/testssl.sh\" pull $DEBUG_STD\n\tfi\n\n\t# massdns\n\tif [[ ! -d \"${dir}/massdns\" ]]; then\n\t\t#printf \"${yellow}Cloning and compiling massdns...${reset}\\n\"\n\t\teval git clone https://github.com/blechschmidt/massdns.git \"${dir}/massdns\" $DEBUG_STD\n\t\teval make -C \"${dir}/massdns\" $DEBUG_STD\n\t\teval strip -s \"${dir}/massdns/bin/massdns\" $DEBUG_ERROR\n\t\teval $SUDO cp \"${dir}/massdns/bin/massdns\" /usr/local/bin/ $DEBUG_ERROR\n\telse\n\t\t#printf \"${yellow}Updating massdns...${reset}\\n\"\n\t\teval git -C \"${dir}/massdns\" pull $DEBUG_STD\n\tfi\n\n\t# Interlace\n\tif [[ ! -d \"${dir}/interlace\" ]]; then\n\t\t#printf \"${yellow}Cloning Interlace...${reset}\\n\"\n\t\teval git clone https://github.com/codingo/Interlace.git \"${dir}/interlace\" $DEBUG_STD\n\t\teval cd \"${dir}/interlace\" && eval $SUDO python3 setup.py install $DEBUG_STD\n\telse\n\t\t#printf \"${yellow}Updating Interlace...${reset}\\n\"\n\t\teval git -C \"${dir}/interlace\" pull $DEBUG_STD\n\tfi\n\n\t# wafw00f\n\tif [[ ! -d \"${dir}/wafw00f\" ]]; then\n\t\t#printf \"${yellow}Cloning wafw00f...${reset}\\n\"\n\t\teval git clone https://github.com/EnableSecurity/wafw00f.git \"${dir}/wafw00f\" $DEBUG_STD\n\t\teval cd \"${dir}/wafw00f\" && eval $SUDO python3 setup.py install $DEBUG_STD\n\telse\n\t\t#printf \"${yellow}Updating wafw00f...${reset}\\n\"\n\t\teval git -C \"${dir}/wafw00f\" pull $DEBUG_STD\n\tfi\n\n\t# gf patterns\n\tif [[ ! -d \"$HOME/.gf\" ]]; then\n\t\t#printf \"${yellow}Installing gf patterns...${reset}\\n\"\n\t\teval git clone https://github.com/tomnomnom/gf.git \"${dir}/gf\" $DEBUG_STD\n\t\teval cp -r \"${dir}/gf/examples\" ~/.gf $DEBUG_ERROR\n\t\teval git clone https://github.com/1ndianl33t/Gf-Patterns \"${dir}/Gf-Patterns\" $DEBUG_STD\n\t\teval cp \"${dir}/Gf-Patterns\"/*.json ~/.gf/ $DEBUG_ERROR\n\telse\n\t\t#printf \"${yellow}Updating gf patterns...${reset}\\n\"\n\t\teval git -C \"${dir}/Gf-Patterns\" pull $DEBUG_STD\n\tfi\n\n\techo -e \"\\n${bblue}Running: Downloading required files${reset}\\n\"\n\n\t# Download required files with error handling\n\tdeclare -A downloads=(\n\t\t[\"notify_provider_config\"]=\"https://gist.githubusercontent.com/six2dez/23a996bca189a11e88251367e6583053/raw ${HOME}/.config/notify/provider-config.yaml\"\n\t\t[\"getjswords\"]=\"https://raw.githubusercontent.com/m4ll0k/Bug-Bounty-Toolz/master/getjswords.py ${tools}/getjswords.py\"\n\t\t[\"subdomains_huge\"]=\"https://raw.githubusercontent.com/n0kovo/n0kovo_subdomains/main/n0kovo_subdomains_huge.txt ${subs_wordlist_big}\"\n\t\t[\"trusted_resolvers\"]=\"https://gist.githubusercontent.com/six2dez/ae9ed7e5c786461868abd3f2344401b6/raw ${resolvers_trusted}\"\n\t\t[\"resolvers\"]=\"https://raw.githubusercontent.com/trickest/resolvers/main/resolvers.txt ${resolvers}\"\n\t\t[\"subs_wordlist\"]=\"https://gist.github.com/six2dez/a307a04a222fab5a57466c51e1569acf/raw ${subs_wordlist}\"\n\t\t[\"permutations_list\"]=\"https://gist.github.com/six2dez/ffc2b14d283e8f8eff6ac83e20a3c4b4/raw ${tools}/permutations_list.txt\"\n\t\t[\"fuzz_wordlist\"]=\"https://raw.githubusercontent.com/six2dez/OneListForAll/main/onelistforallmicro.txt ${fuzz_wordlist}\"\n\t\t[\"lfi_wordlist\"]=\"https://gist.githubusercontent.com/six2dez/a89a0c7861d49bb61a09822d272d5395/raw ${lfi_wordlist}\"\n\t\t[\"ssti_wordlist\"]=\"https://gist.githubusercontent.com/six2dez/ab5277b11da7369bf4e9db72b49ad3c1/raw ${ssti_wordlist}\"\n\t\t[\"headers_inject\"]=\"https://gist.github.com/six2dez/d62ab8f8ffd28e1c206d401081d977ae/raw ${tools}/headers_inject.txt\"\n\t\t[\"axiom_config\"]=\"https://gist.githubusercontent.com/six2dez/6e2d9f4932fd38d84610eb851014b26e/raw ${tools}/axiom_config.sh\"\n\t)\n\n\tfor key in \"${!downloads[@]}\"; do\n\t\turl=\"${downloads[$key]% *}\"\n\t\tdestination=\"${downloads[$key]#* }\"\n\t\twget -q -O \"$destination\" \"$url\" || {\n\t\t\techo -e \"${red}[!] Failed to download $key from $url.${reset}\"\n\t\t\tcontinue\n\t\t}\n\tdone\n\n\t# Make axiom_config.sh executable\n\tchmod +x \"${tools}/axiom_config.sh\" || {\n\t\techo -e \"${red}[!] Failed to make axiom_config.sh executable.${reset}\"\n\t}\n\n\techo -e \"${bblue}Running: Performing last configurations${reset}\\n\"\n\n\t# Update resolvers if generate_resolvers is true\n\tif [[ $generate_resolvers == true ]]; then\n\t\tif [[ ! -s $resolvers || $(find \"$resolvers\" -mtime +1 -print) ]]; then\n\t\t\techo -e \"${yellow}Checking resolvers lists...\\nAccurate resolvers are the key to great results.\\nThis may take around 10 minutes if it's not updated.${reset}\\n\"\n\t\t\trm -f \"$resolvers\" &>/dev/null\n\t\t\tdnsvalidator -tL https://public-dns.info/nameservers.txt -threads \"$DNSVALIDATOR_THREADS\" -o \"$resolvers\" &>/dev/null\n\t\t\tdnsvalidator -tL https://raw.githubusercontent.com/blechschmidt/massdns/master/lists/resolvers.txt -threads \"$DNSVALIDATOR_THREADS\" -o tmp_resolvers &>/dev/null\n\n\t\t\tif [[ -s \"tmp_resolvers\" ]]; then\n\t\t\t\tcat tmp_resolvers | anew -q \"$resolvers\"\n\t\t\t\trm -f tmp_resolvers &>/dev/null\n\t\t\tfi\n\n\t\t\t[[ ! -s $resolvers ]] && wget -q -O \"$resolvers\" https://raw.githubusercontent.com/trickest/resolvers/main/resolvers.txt\n\t\t\t[[ ! -s $resolvers_trusted ]] && wget -q -O \"$resolvers_trusted\" https://gist.githubusercontent.com/six2dez/ae9ed7e5c786461868abd3f2344401b6/raw/trusted_resolvers.txt\n\t\t\techo -e \"${yellow}Resolvers updated.${reset}\\n\"\n\t\tfi\n\t\tgenerate_resolvers=false\n\telse\n\t\tif [[ -s $resolvers && $(find \"$resolvers\" -mtime +1 -print) ]]; then\n\t\t\techo -e \"${yellow}Checking resolvers lists...\\nAccurate resolvers are the key to great results.\\nDownloading new resolvers.${reset}\\n\"\n\t\t\twget -q -O \"$resolvers\" https://raw.githubusercontent.com/trickest/resolvers/main/resolvers.txt\n\t\t\twget -q -O \"$resolvers_trusted\" https://gist.githubusercontent.com/six2dez/ae9ed7e5c786461868abd3f2344401b6/raw/trusted_resolvers.txt\n\t\t\techo -e \"${yellow}Resolvers updated.${reset}\\n\"\n\t\tfi\n\tfi\n\n\t# Strip all Go binaries and copy to /usr/local/bin\n\tstrip -s \"${GOPATH}/bin/\"* &>/dev/null || true\n\t\"$SUDO\" cp \"${GOPATH}/bin/\"* /usr/local/bin/ &>/dev/null || true\n\n\t# Final reminders\n\techo -e \"${yellow}Remember to set your API keys:\\n- subfinder (${HOME}/.config/subfinder/provider-config.yaml)\\n- GitHub (${HOME}/Tools/.github_tokens)\\n- GitLab (${HOME}/Tools/.gitlab_tokens)\\n- SSRF Server (COLLAB_SERVER in reconftw.cfg or env var)\\n- Blind XSS Server (XSS_SERVER in reconftw.cfg or env var)\\n- notify (${HOME}/.config/notify/provider-config.yaml)\\n- WHOISXML API (WHOISXML_API in reconftw.cfg or env var)\\n${reset}\"\n\techo -e \"${bgreen}Finished!${reset}\\n\"\n\techo -e \"${bgreen}#######################################################################${reset}\"\n}\n\n# Function to display additional help\nfunction show_additional_help() {\n\techo \"Usage: $0 [OPTION]\"\n\techo \"Run the script with specified options.\"\n\techo \"\"\n\techo \"  -h, --help       Display this help and exit.\"\n\techo \"  --tools          Install the tools before running, useful for upgrading.\"\n\techo \"\"\n\techo \"  ****             Without any arguments, the script will update reconftw\"\n\techo \"                   and install all dependencies and requirements.\"\n\texit 0\n}\n\n# Function to handle installation arguments\nfunction handle_install_arguments() {\n\techo -e \"\\n${bgreen}reconFTW installer/updater script${reset}\\n\"\n\n\twhile [[ $# -gt 0 ]]; do\n\t\tcase \"$1\" in\n\t\t-h | --help)\n\t\t\tshow_additional_help\n\t\t\t;;\n\t\t--tools)\n\t\t\tinstall_tools\n\t\t\tshift\n\t\t\t;;\n\t\t*)\n\t\t\techo -e \"${bred}Error: Invalid argument '$1'${reset}\"\n\t\t\techo \"Use -h or --help for usage information.\"\n\t\t\texit 1\n\t\t\t;;\n\t\tesac\n\tdone\n\n\techo -e \"${yellow}This may take some time. Grab a coffee!${reset}\\n\"\n\n\t# Determine if the script is run as root\n\tif [[ \"$(id -u)\" -eq 0 ]]; then\n\t\tSUDO=\"\"\n\telse\n\t\tif ! sudo -n true 2>/dev/null; then\n\t\t\techo -e \"${bred}It is strongly recommended to add your user to sudoers.${reset}\"\n\t\t\techo -e \"${bred}This will avoid prompts for sudo password during installation and scans.${reset}\"\n\t\t\techo -e \"${bred}Run the following command to add your user to sudoers:${reset}\"\n\t\t\techo -e \"${bred}echo \\\"${USER}  ALL=(ALL:ALL) NOPASSWD: ALL\\\" | sudo tee /etc/sudoers.d/reconFTW${reset}\\n\"\n\t\tfi\n\t\tSUDO=\"sudo\"\n\tfi\n}\n\n# Invoke main functions\nhandle_install_arguments \"$@\"\ninitial_setup\n"
        },
        {
          "name": "reconftw.cfg",
          "type": "blob",
          "size": 10.8447265625,
          "content": "#############################################\n#\t\t\treconFTW config file\t\t\t#\n#############################################\n\n# General values\ntools=$HOME/Tools   # Path installed tools\nSCRIPTPATH=\"$( cd \"$(dirname \"$0\")\" >/dev/null 2>&1 ; pwd -P )\" # Get current script's path\nprofile_shell=\".$(basename $(echo $SHELL))rc\" # Get current shell profile\nreconftw_version=$(git rev-parse --abbrev-ref HEAD)-$(git describe --tags) # Fetch current reconftw version\ngenerate_resolvers=false # Generate custom resolvers with dnsvalidator\nupdate_resolvers=true # Fetch and rewrite resolvers from trickest/resolvers before DNS resolution\nresolvers_url=\"https://raw.githubusercontent.com/trickest/resolvers/main/resolvers.txt\"\nresolvers_trusted_url=\"https://gist.githubusercontent.com/six2dez/ae9ed7e5c786461868abd3f2344401b6/raw/trusted_resolvers.txt\"\nfuzzing_remote_list=\"https://raw.githubusercontent.com/six2dez/OneListForAll/main/onelistforallmicro.txt\" # Used to send to axiom(if used) on fuzzing \nproxy_url=\"http://127.0.0.1:8080/\" # Proxy url\ninstall_golang=true # Set it to false if you already have Golang configured and ready\nupgrade_tools=true\nupgrade_before_running=false # Upgrade tools before running\n#dir_output=/custom/output/path\n\n# Golang Vars (Comment or change on your own)\nexport GOROOT=/usr/local/go\nexport GOPATH=$HOME/go\nexport PATH=$GOPATH/bin:$GOROOT/bin:$HOME/.local/bin:$PATH\n\n# Tools config files\n#NOTIFY_CONFIG=~/.config/notify/provider-config.yaml # No need to define\nGITHUB_TOKENS=${tools}/.github_tokens\nGITLAB_TOKENS=${tools}/.gitlab_tokens\n#CUSTOM_CONFIG=custom_config_path.txt # In case you use a custom config file, uncomment this line and set your files path\n\n# APIs/TOKENS - Uncomment the lines you want removing the '#' at the beginning of the line\n#SHODAN_API_KEY=\"XXXXXXXXXXXXX\"\n#WHOISXML_API=\"XXXXXXXXXX\"\n#XSS_SERVER=\"XXXXXXXXXXXXXXXXX\"\n#COLLAB_SERVER=\"XXXXXXXXXXXXXXXXX\"\n#slack_channel=\"XXXXXXXX\"\n#slack_auth=\"xoXX-XXX-XXX-XXX\"\n\n# File descriptors\nDEBUG_STD=\"&>/dev/null\" # Skips STD output on installer\nDEBUG_ERROR=\"2>/dev/null\" # Skips ERR output on installer\n\n# Osint\nOSINT=true # Enable or disable the whole OSINT module\nGOOGLE_DORKS=true\nGITHUB_DORKS=true\nGITHUB_REPOS=true\nMETADATA=true # Fetch metadata from indexed office documents\nEMAILS=true # Fetch emails from differents sites \nDOMAIN_INFO=true # whois info\nIP_INFO=true    # Reverse IP search, geolocation and whois\nAPI_LEAKS=true # Check for API leaks\nTHIRD_PARTIES=true # Check for 3rd parties misconfigs\nSPOOF=true # Check spoofable domains\nMETAFINDER_LIMIT=20 # Max 250\n\n# Subdomains\nSUBDOMAINS_GENERAL=true # Enable or disable the whole Subdomains module\nSUBPASSIVE=true # Passive subdomains search\nSUBCRT=true # crtsh search\nCTR_LIMIT=999999 # Limit the number of results\nSUBNOERROR=false # Check DNS NOERROR response and BF on them\nSUBANALYTICS=true # Google Analytics search\nSUBBRUTE=true # DNS bruteforcing\nSUBSCRAPING=true # Subdomains extraction from web crawling\nSUBPERMUTE=true # DNS permutations\nSUBREGEXPERMUTE=true # Permutations by regex analysis\nPERMUTATIONS_OPTION=gotator # The alternative is \"ripgen\" (faster, not deeper)\nGOTATOR_FLAGS=\" -depth 1 -numbers 3 -mindup -adv -md\" # Flags for gotator\nSUBTAKEOVER=true # Check subdomain takeovers, false by default cuz nuclei already check this\nSUB_RECURSIVE_PASSIVE=false # Uses a lot of API keys queries\nDEEP_RECURSIVE_PASSIVE=10 # Number of top subdomains for recursion\nSUB_RECURSIVE_BRUTE=false # Needs big disk space and time to resolve\nZONETRANSFER=true # Check zone transfer\nS3BUCKETS=true # Check S3 buckets misconfigs\nREVERSE_IP=false # Check reverse IP subdomain search (set True if your target is CIDR/IP)\nTLS_PORTS=\"21,22,25,80,110,135,143,261,271,324,443,448,465,563,614,631,636,664,684,695,832,853,854,990,993,989,992,994,995,1129,1131,1184,2083,2087,2089,2096,2221,2252,2376,2381,2478,2479,2482,2484,2679,2762,3077,3078,3183,3191,3220,3269,3306,3410,3424,3471,3496,3509,3529,3539,3535,3660,36611,3713,3747,3766,3864,3885,3995,3896,4031,4036,4062,4064,4081,4083,4116,4335,4336,4536,4590,4740,4843,4849,5443,5007,5061,5321,5349,5671,5783,5868,5986,5989,5990,6209,6251,6443,6513,6514,6619,6697,6771,7202,7443,7673,7674,7677,7775,8243,8443,8991,8989,9089,9295,9318,9443,9444,9614,9802,10161,10162,11751,12013,12109,14143,15002,16995,41230,16993,20003\"\nINSCOPE=false # Uses inscope tool to filter the scope, requires .scope file in reconftw folder \n\n# Web detection\nWEBPROBESIMPLE=true # Web probing on 80/443\nWEBPROBEFULL=true # Web probing in a large port list\nWEBSCREENSHOT=true # Webs screenshooting\nVIRTUALHOSTS=false # Check virtualhosts by fuzzing HOST header\nUNCOMMON_PORTS_WEB=\"81,300,591,593,832,981,1010,1311,1099,2082,2095,2096,2480,3000,3001,3002,3003,3128,3333,4243,4567,4711,4712,4993,5000,5104,5108,5280,5281,5601,5800,6543,7000,7001,7396,7474,8000,8001,8008,8014,8042,8060,8069,8080,8081,8083,8088,8090,8091,8095,8118,8123,8172,8181,8222,8243,8280,8281,8333,8337,8443,8500,8834,8880,8888,8983,9000,9001,9043,9060,9080,9090,9091,9092,9200,9443,9502,9800,9981,10000,10250,11371,12443,15672,16080,17778,18091,18092,20720,32000,55440,55672\"\n\n# Host\nFAVICON=true # Check Favicon domain discovery\nPORTSCANNER=true # Enable or disable the whole Port scanner module \nGEO_INFO=true # Fetch Geolocalization info\nPORTSCAN_PASSIVE=true # Port scanner with Shodan\nPORTSCAN_ACTIVE=true # Port scanner with nmap\nPORTSCAN_ACTIVE_OPTIONS=\"--top-ports 200 -sV -n -Pn --open --max-retries 2 --script vulners\"\nCDN_IP=true # Check which IPs belongs to CDN\n\n# Web analysis\nWAF_DETECTION=true # Detect WAFs\nNUCLEICHECK=true # Enable or disable nuclei\nNUCLEI_TEMPLATES_PATH=\"$HOME/nuclei-templates\" # Set nuclei templates path\nNUCLEI_SEVERITY=\"info,low,medium,high,critical\" # Set templates criticity\nNUCLEI_EXTRA_ARGS=\"\" # Additional nuclei extra flags, don't set the severity here but the exclusions like \" -etags openssh\"\nNUCLEI_FLAGS=\" -silent -t ${NUCLEI_TEMPLATES_PATH}/ -retries 2\" # Additional nuclei extra flags, don't set the severity here but the exclusions like \" -etags openssh\"\nNUCLEI_FLAGS_JS=\" -silent -tags exposure,token -severity info,low,medium,high,critical\" # Additional nuclei extra flags for js secrets\nURL_CHECK=true # Enable or disable URL collection\nURL_CHECK_PASSIVE=true # Search for urls, passive methods from Archive, OTX, CommonCrawl, etc\nURL_CHECK_ACTIVE=true # Search for urls by crawling the websites\nURL_GF=true # Url patterns classification\nURL_EXT=true # Returns a list of files divided by extension\nJSCHECKS=true # JS analysis\nFUZZ=true # Web fuzzing\nIIS_SHORTNAME=true\nCMS_SCANNER=true # CMS scanner\nWORDLIST=true # Wordlist generation\nROBOTSWORDLIST=true # Check historic disallow entries on waybackMachine\nPASSWORD_DICT=true # Generate password dictionary\nPASSWORD_MIN_LENGTH=5 # Min password length\nPASSWORD_MAX_LENGTH=14 # Max password length\nCLOUDHUNTER_PERMUTATION=NORMAL # Options: DEEP (very slow), NORMAL (slow), NONE \nNUCLEI_FUZZING_TEMPLATES_PATH=\"${tools}/fuzzing-templates\" # Set nuclei templates path\n\n# Vulns\nVULNS_GENERAL=false # Enable or disable the vulnerability module (very intrusive and slow)\nXSS=true # Check for xss with dalfox\nCORS=true # CORS misconfigs\nTEST_SSL=true # SSL misconfigs\nOPEN_REDIRECT=true # Check open redirects\nSSRF_CHECKS=true # SSRF checks\nCRLF_CHECKS=true # CRLF checks\nLFI=true # LFI by fuzzing\nSSTI=true # SSTI by fuzzing\nSQLI=true # Check SQLI\nSQLMAP=true # Check SQLI with sqlmap\nGHAURI=false # Check SQLI with ghauri\nBROKENLINKS=true # Check for brokenlinks\nSPRAY=true # Performs password spraying\nCOMM_INJ=true # Check for command injections with commix\nPROTO_POLLUTION=true # Check for prototype pollution flaws\nSMUGGLING=true # Check for HTTP request smuggling flaws\nWEBCACHE=true # Check for Web Cache issues\nBYPASSER4XX=true # Check for 4XX bypasses\nFUZZPARAMS=true # Fuzz parameters values\n\n# Extra features\nNOTIFICATION=false # Notification for every function\nSOFT_NOTIFICATION=false # Only for start/end\nDEEP=false # DEEP mode, really slow and don't care about the number of results\nDEEP_LIMIT=500 # First limit to not run unless you run DEEP\nDEEP_LIMIT2=1500 # Second limit to not run unless you run DEEP\nDIFF=false # Diff function, run every module over an already scanned target, printing only new findings (but save everything)\nREMOVETMP=true # Delete temporary files after execution (to free up space)\nREMOVELOG=false # Delete logs after execution\nPROXY=false # Send to proxy the websites found\nSENDZIPNOTIFY=false # Send to zip the results (over notify)\nPRESERVE=true      # set to true to avoid deleting the .called_fn files on really large scans\nFFUF_FLAGS=\" -mc all -fc 404 -sf -noninteractive -of json\" # Ffuf flags\nHTTPX_FLAGS=\" -follow-redirects -random-agent -status-code -silent -title -web-server -tech-detect -location -content-length\" # Httpx flags for simple web probing\n\n# HTTP options\nHEADER=\"User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:72.0) Gecko/20100101 Firefox/72.0\" # Default header\n\n# Threads\nFFUF_THREADS=40\nHTTPX_THREADS=50\nHTTPX_UNCOMMONPORTS_THREADS=100\nKATANA_THREADS=20\nBRUTESPRAY_THREADS=20\nBRUTESPRAY_CONCURRENCE=10\nGAU_THREADS=10\nDNSTAKE_THREADS=100\nDALFOX_THREADS=200\nPUREDNS_PUBLIC_LIMIT=0 # Set between 2000 - 10000 if your router blows up, 0 means unlimited\nPUREDNS_TRUSTED_LIMIT=400\nPUREDNS_WILDCARDTEST_LIMIT=30\nPUREDNS_WILDCARDBATCH_LIMIT=1500000\nRESOLVE_DOMAINS_THREADS=150\nDNSVALIDATOR_THREADS=200\nINTERLACE_THREADS=10\nTLSX_THREADS=1000\nXNLINKFINDER_DEPTH=3\n\n# Rate limits\nHTTPX_RATELIMIT=150\nNUCLEI_RATELIMIT=150\nFFUF_RATELIMIT=0\n\n# Timeouts\nSUBFINDER_ENUM_TIMEOUT=180          # Minutes\nCMSSCAN_TIMEOUT=3600            # Seconds\nFFUF_MAXTIME=900                # Seconds\nHTTPX_TIMEOUT=10                # Seconds\nHTTPX_UNCOMMONPORTS_TIMEOUT=10  # Seconds\nPERMUTATIONS_LIMIT=21474836480  # Bytes, default is 20 GB\n\n# lists\nfuzz_wordlist=${tools}/fuzz_wordlist.txt\nlfi_wordlist=${tools}/lfi_wordlist.txt\nssti_wordlist=${tools}/ssti_wordlist.txt\nsubs_wordlist=${tools}/subdomains.txt\nsubs_wordlist_big=${tools}/subdomains_n0kovo_big.txt\nresolvers=${tools}/resolvers.txt\nresolvers_trusted=${tools}/resolvers_trusted.txt\n\n# Axiom Fleet\n# Will not start a new fleet if one exist w/ same name and size (or larger)\n# AXIOM=false Uncomment only to overwrite command line flags\nAXIOM_FLEET_LAUNCH=true # Enable or disable spin up a new fleet, if false it will use the current fleet with the AXIOM_FLEET_NAME prefix\nAXIOM_FLEET_NAME=\"reconFTW\" # Fleet's prefix name\nAXIOM_FLEET_COUNT=10 # Fleet's number\nAXIOM_FLEET_REGIONS=\"eu-central\" # Fleet's region\nAXIOM_FLEET_SHUTDOWN=true # # Enable or disable delete the fleet after the execution\n# This is a script on your reconftw host that might prep things your way...\n#AXIOM_POST_START=\"~/Tools/axiom_config.sh\" # Useful  to send your config files to the fleet\nAXIOM_EXTRA_ARGS=\"\" # Leave empty if you don't want to add extra arguments\n#AXIOM_EXTRA_ARGS=\" --rm-logs\" # Example\n\n# TERM COLORS\nbred='\\033[1;31m'\nbblue='\\033[1;34m'\nbgreen='\\033[1;32m'\nbyellow='\\033[1;33m'\nred='\\033[0;31m'\nblue='\\033[0;34m'\ngreen='\\033[0;32m'\nyellow='\\033[0;33m'\nreset='\\033[0m'"
        },
        {
          "name": "reconftw.sh",
          "type": "blob",
          "size": 235.021484375,
          "content": "#!/usr/bin/env bash\n\n# Welcome to reconFTW main script\n#\t ‚ñà‚ñà‚ñÄ‚ñà‚ñà‚ñà  ‚ñì‚ñà‚ñà‚ñà‚ñà‚ñà  ‚ñÑ‚ñà‚ñà‚ñà‚ñà‚ñÑ   ‚ñí‚ñà‚ñà‚ñà‚ñà‚ñà   ‚ñà‚ñà‚ñà‚ñÑ    ‚ñà   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñí‚ñÑ‚ñÑ‚ñÑ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñì ‚ñà     ‚ñà‚ñë\n#\t‚ñì‚ñà‚ñà ‚ñí ‚ñà‚ñà‚ñí‚ñì‚ñà   ‚ñÄ ‚ñí‚ñà‚ñà‚ñÄ ‚ñÄ‚ñà  ‚ñí‚ñà‚ñà‚ñí  ‚ñà‚ñà‚ñí ‚ñà‚ñà ‚ñÄ‚ñà   ‚ñà ‚ñì‚ñà‚ñà   ‚ñí ‚ñì  ‚ñà‚ñà‚ñí ‚ñì‚ñí‚ñì‚ñà‚ñë ‚ñà ‚ñë‚ñà‚ñë\n#\t‚ñì‚ñà‚ñà ‚ñë‚ñÑ‚ñà ‚ñí‚ñí‚ñà‚ñà‚ñà   ‚ñí‚ñì‚ñà    ‚ñÑ ‚ñí‚ñà‚ñà‚ñë  ‚ñà‚ñà‚ñí‚ñì‚ñà‚ñà  ‚ñÄ‚ñà ‚ñà‚ñà‚ñí‚ñí‚ñà‚ñà‚ñà‚ñà ‚ñë ‚ñí ‚ñì‚ñà‚ñà‚ñë ‚ñí‚ñë‚ñí‚ñà‚ñë ‚ñà ‚ñë‚ñà\n#\t‚ñí‚ñà‚ñà‚ñÄ‚ñÄ‚ñà‚ñÑ  ‚ñí‚ñì‚ñà  ‚ñÑ ‚ñí‚ñì‚ñì‚ñÑ ‚ñÑ‚ñà‚ñà‚ñí‚ñí‚ñà‚ñà   ‚ñà‚ñà‚ñë‚ñì‚ñà‚ñà‚ñí  ‚ñê‚ñå‚ñà‚ñà‚ñí‚ñë‚ñì‚ñà‚ñí  ‚ñë ‚ñë ‚ñì‚ñà‚ñà‚ñì ‚ñë ‚ñë‚ñà‚ñë ‚ñà ‚ñë‚ñà\n#\t‚ñë‚ñà‚ñà‚ñì ‚ñí‚ñà‚ñà‚ñí‚ñë‚ñí‚ñà‚ñà‚ñà‚ñà‚ñí‚ñí ‚ñì‚ñà‚ñà‚ñà‚ñÄ ‚ñë‚ñë ‚ñà‚ñà‚ñà‚ñà‚ñì‚ñí‚ñë‚ñí‚ñà‚ñà‚ñë   ‚ñì‚ñà‚ñà‚ñë‚ñë‚ñí‚ñà‚ñë      ‚ñí‚ñà‚ñà‚ñí ‚ñë ‚ñë‚ñë‚ñà‚ñà‚ñí‚ñà‚ñà‚ñì\n#\t‚ñë ‚ñí‚ñì ‚ñë‚ñí‚ñì‚ñë‚ñë‚ñë ‚ñí‚ñë ‚ñë‚ñë ‚ñë‚ñí ‚ñí  ‚ñë‚ñë ‚ñí‚ñë‚ñí‚ñë‚ñí‚ñë ‚ñë ‚ñí‚ñë   ‚ñí ‚ñí  ‚ñí ‚ñë      ‚ñí ‚ñë‚ñë   ‚ñë ‚ñì‚ñë‚ñí ‚ñí\n#\t  ‚ñë‚ñí ‚ñë ‚ñí‚ñë ‚ñë ‚ñë  ‚ñë  ‚ñë  ‚ñí     ‚ñë ‚ñí ‚ñí‚ñë ‚ñë ‚ñë‚ñë   ‚ñë ‚ñí‚ñë ‚ñë          ‚ñë      ‚ñí ‚ñë ‚ñë\n#\t  ‚ñë‚ñë   ‚ñë    ‚ñë   ‚ñë        ‚ñë ‚ñë ‚ñë ‚ñí     ‚ñë   ‚ñë ‚ñë  ‚ñë ‚ñë      ‚ñë        ‚ñë   ‚ñë\n#\t   ‚ñë        ‚ñë  ‚ñë‚ñë ‚ñë          ‚ñë ‚ñë           ‚ñë                      ‚ñë\n#\n\nfunction banner_grabber() {\n\tlocal banner_file=\"${SCRIPTPATH}/banners.txt\"\n\n\t# Check if the banner file exists\n\tif [[ ! -f $banner_file ]]; then\n\t\techo \"Banner file not found: $banner_file\" >&2\n\t\treturn 1\n\tfi\n\n\t# Source the banner file\n\tsource \"$banner_file\"\n\n\t# Collect all banner variable names\n\tmapfile -t banner_vars < <(compgen -A variable | grep '^banner[0-9]\\+$')\n\n\t# Check if any banners are available\n\tif [[ ${#banner_vars[@]} -eq 0 ]]; then\n\t\techo \"No banners found in $banner_file\" >&2\n\t\treturn 1\n\tfi\n\n\t# Select a random banner\n\tlocal rand_index=$((RANDOM % ${#banner_vars[@]}))\n\tlocal banner_var=\"${banner_vars[$rand_index]}\"\n\tlocal banner_code=\"${!banner_var}\"\n\n\t# Output the banner code\n\tprintf \"%b\\n\" \"$banner_code\"\n}\n\nfunction banner() {\n\tlocal banner_code\n\tif banner_code=$(banner_grabber); then\n\t\tprintf \"\\n%b%s\" \"$bgreen\" \"$banner_code\"\n\t\tprintf \"\\n %s                                 by @six2dez%b\\n\" \"$reconftw_version\" \"$reset\"\n\telse\n\t\tprintf \"\\n%bFailed to load banner.%b\\n\" \"$bgreen\" \"$reset\"\n\tfi\n}\n\n###############################################################################################################\n################################################### TOOLS #####################################################\n###############################################################################################################\n\nfunction check_version() {\n\n\t# Check if git is installed\n\tif ! command -v git >/dev/null 2>&1; then\n\t\tprintf \"\\n%bGit is not installed. Cannot check for updates.%b\\n\\n\" \"$bred\" \"$reset\"\n\t\treturn 1\n\tfi\n\n\t# Check if current directory is a git repository\n\tif ! git rev-parse --is-inside-work-tree >/dev/null 2>&1; then\n\t\tprintf \"\\n%bCurrent directory is not a git repository. Cannot check for updates.%b\\n\\n\" \"$bred\" \"$reset\"\n\t\treturn 1\n\tfi\n\n\t# Fetch updates with a timeout\n\tif ! timeout 10 git fetch >/dev/null 2>&1; then\n\t\tprintf \"\\n%bUnable to check updates (git fetch timed out).%b\\n\\n\" \"$bred\" \"$reset\"\n\t\treturn 1\n\tfi\n\n\t# Get current branch name\n\tlocal BRANCH\n\tBRANCH=$(git rev-parse --abbrev-ref HEAD)\n\n\t# Get upstream branch\n\tlocal UPSTREAM\n\tUPSTREAM=$(git rev-parse --abbrev-ref --symbolic-full-name \"@{u}\" 2>/dev/null)\n\tif [[ -z $UPSTREAM ]]; then\n\t\tprintf \"\\n%bNo upstream branch set for '%s'. Cannot check for updates.%b\\n\\n\" \"$bred\" \"$BRANCH\" \"$reset\"\n\t\treturn 1\n\tfi\n\n\t# Get local and remote commit hashes\n\tlocal LOCAL REMOTE\n\tLOCAL=$(git rev-parse HEAD)\n\tREMOTE=$(git rev-parse \"$UPSTREAM\")\n\n\t# Compare local and remote hashes\n\tif [[ $LOCAL != \"$REMOTE\" ]]; then\n\t\tprintf \"\\n%bThere is a new version available. Run ./install.sh to get the latest version.%b\\n\\n\" \"$yellow\" \"$reset\"\n\tfi\n}\n\nfunction tools_installed() {\n\t# Check if all tools are installed\n\tprintf \"\\n\\n%b#######################################################################%b\\n\" \"$bgreen\" \"$reset\"\n\tprintf \"%b[%s] Checking installed tools %b\\n\\n\" \"$bblue\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"$reset\"\n\n\tlocal all_installed=true\n\tlocal missing_tools=()\n\n\t# Check environment variables\n\tlocal env_vars=(\"GOPATH\" \"GOROOT\" \"PATH\")\n\tfor var in \"${env_vars[@]}\"; do\n\t\tif [[ -z ${!var} ]]; then\n\t\t\tprintf \"%b [*] %s variable\\t\\t[NO]%b\\n\" \"$bred\" \"$var\" \"$reset\"\n\t\t\tall_installed=false\n\t\t\tmissing_tools+=(\"$var environment variable\")\n\t\tfi\n\tdone\n\n\t# Define tools and their paths/commands\n\tdeclare -A tools_files=(\n\t\t[\"dorks_hunter\"]=\"${tools}/dorks_hunter/dorks_hunter.py\"\n\t\t[\"fav-up\"]=\"${tools}/fav-up/favUp.py\"\n\t\t[\"Corsy\"]=\"${tools}/Corsy/corsy.py\"\n\t\t[\"testssl\"]=\"${tools}/testssl.sh/testssl.sh\"\n\t\t[\"CMSeeK\"]=\"${tools}/CMSeeK/cmseek.py\"\n\t\t[\"OneListForAll\"]=\"$fuzz_wordlist\"\n\t\t[\"lfi_wordlist\"]=\"$lfi_wordlist\"\n\t\t[\"ssti_wordlist\"]=\"$ssti_wordlist\"\n\t\t[\"subs_wordlist\"]=\"$subs_wordlist\"\n\t\t[\"subs_wordlist_big\"]=\"$subs_wordlist_big\"\n\t\t[\"resolvers\"]=\"$resolvers\"\n\t\t[\"resolvers_trusted\"]=\"$resolvers_trusted\"\n\t\t[\"commix\"]=\"${tools}/commix/commix.py\"\n\t\t[\"getjswords\"]=\"${tools}/getjswords.py\"\n\t\t[\"JSA\"]=\"${tools}/JSA/jsa.py\"\n\t\t[\"CloudHunter\"]=\"${tools}/CloudHunter/cloudhunter.py\"\n\t\t[\"nmap-parse-output\"]=\"${tools}/ultimate-nmap-parser/ultimate-nmap-parser.sh\"\n\t\t[\"pydictor\"]=\"${tools}/pydictor/pydictor.py\"\n\t\t[\"urless\"]=\"${tools}/urless/urless/urless.py\"\n\t\t[\"smuggler\"]=\"${tools}/smuggler/smuggler.py\"\n\t\t[\"regulator\"]=\"${tools}/regulator/main.py\"\n\t\t[\"nomore403\"]=\"${tools}/nomore403/nomore403\"\n\t\t[\"ffufPostprocessing\"]=\"${tools}/ffufPostprocessing/ffufPostprocessing\"\n\t\t[\"misconfig-mapper\"]=\"${tools}/misconfig-mapper/misconfig-mapper\"\n\t\t[\"spoofy\"]=\"${tools}/Spoofy/spoofy.py\"\n\t\t[\"swaggerspy\"]=\"${tools}/SwaggerSpy/swaggerspy.py\"\n\t\t[\"LeakSearch\"]=\"${tools}/LeakSearch/LeakSearch.py\"\n\t)\n\n\tdeclare -A tools_folders=(\n\t\t[\"NUCLEI_TEMPLATES_PATH\"]=\"${NUCLEI_TEMPLATES_PATH}\"\n\t\t[\"NUCLEI_FUZZING_TEMPLATES_PATH\"]=\"${NUCLEI_FUZZING_TEMPLATES_PATH}\"\n\t)\n\n\tdeclare -A tools_commands=(\n\t\t[\"brutespray\"]=\"brutespray\"\n\t\t[\"xnLinkFinder\"]=\"xnLinkFinder\"\n\t\t[\"urlfinder\"]=\"urlfinder\"\n\t\t[\"github-endpoints\"]=\"github-endpoints\"\n\t\t[\"github-subdomains\"]=\"github-subdomains\"\n\t\t[\"gitlab-subdomains\"]=\"gitlab-subdomains\"\n\t\t[\"katana\"]=\"katana\"\n\t\t[\"wafw00f\"]=\"wafw00f\"\n\t\t[\"dnsvalidator\"]=\"dnsvalidator\"\n\t\t[\"metafinder\"]=\"metafinder\"\n\t\t[\"whois\"]=\"whois\"\n\t\t[\"dnsx\"]=\"dnsx\"\n\t\t[\"gotator\"]=\"gotator\"\n\t\t[\"Nuclei\"]=\"nuclei\"\n\t\t[\"gf\"]=\"gf\"\n\t\t[\"Gxss\"]=\"Gxss\"\n\t\t[\"subjs\"]=\"subjs\"\n\t\t[\"ffuf\"]=\"ffuf\"\n\t\t[\"Massdns\"]=\"massdns\"\n\t\t[\"qsreplace\"]=\"qsreplace\"\n\t\t[\"interlace\"]=\"interlace\"\n\t\t[\"Anew\"]=\"anew\"\n\t\t[\"unfurl\"]=\"unfurl\"\n\t\t[\"crlfuzz\"]=\"crlfuzz\"\n\t\t[\"Httpx\"]=\"httpx\"\n\t\t[\"jq\"]=\"jq\"\n\t\t[\"notify\"]=\"notify\"\n\t\t[\"dalfox\"]=\"dalfox\"\n\t\t[\"puredns\"]=\"puredns\"\n\t\t[\"emailfinder\"]=\"emailfinder\"\n\t\t[\"analyticsrelationships\"]=\"analyticsrelationships\"\n\t\t[\"mapcidr\"]=\"mapcidr\"\n\t\t[\"ppmap\"]=\"ppmap\"\n\t\t[\"cdncheck\"]=\"cdncheck\"\n\t\t[\"interactsh-client\"]=\"interactsh-client\"\n\t\t[\"tlsx\"]=\"tlsx\"\n\t\t[\"smap\"]=\"smap\"\n\t\t[\"gitdorks_go\"]=\"gitdorks_go\"\n\t\t[\"ripgen\"]=\"ripgen\"\n\t\t[\"dsieve\"]=\"dsieve\"\n\t\t[\"inscope\"]=\"inscope\"\n\t\t[\"enumerepo\"]=\"enumerepo\"\n\t\t[\"Web-Cache-Vulnerability-Scanner\"]=\"Web-Cache-Vulnerability-Scanner\"\n\t\t[\"subfinder\"]=\"subfinder\"\n\t\t[\"ghauri\"]=\"ghauri\"\n\t\t[\"hakip2host\"]=\"hakip2host\"\n\t\t[\"gau\"]=\"gau\"\n\t\t[\"crt\"]=\"crt\"\n\t\t[\"gitleaks\"]=\"gitleaks\"\n\t\t[\"trufflehog\"]=\"trufflehog\"\n\t\t[\"s3scanner\"]=\"s3scanner\"\n\t\t[\"mantra\"]=\"mantra\"\n\t\t[\"nmapurls\"]=\"nmapurls\"\n\t\t[\"porch-pirate\"]=\"porch-pirate\"\n\t\t[\"shortscan\"]=\"shortscan\"\n\t\t[\"sns\"]=\"sns\"\n\t\t[\"sourcemapper\"]=\"sourcemapper\"\n\t\t[\"jsluice\"]=\"jsluice\"\n\t)\n\n\t# Check for tool files\n\tfor tool in \"${!tools_files[@]}\"; do\n\t\tif [[ ! -f ${tools_files[$tool]} ]]; then\n\t\t\t#\t\t\tprintf \"%b [*] %s\\t\\t[NO]%b\\n\" \"$bred\" \"$tool\" \"$reset\"\n\t\t\tall_installed=false\n\t\t\tmissing_tools+=(\"$tool\")\n\t\tfi\n\tdone\n\n\t# Check for tool folders\n\tfor folder in \"${!tools_folders[@]}\"; do\n\t\tif [[ ! -d ${tools_folders[$folder]} ]]; then\n\t\t\t# printf \"%b [*] %s\\t\\t[NO]%b\\n\" \"$bred\" \"$folder\" \"$reset\"\n\t\t\tall_installed=false\n\t\t\tmissing_tools+=(\"$folder\") # Correctly pushing the folder name\n\t\tfi\n\tdone\n\n\t# Check for tool commands\n\tfor tool in \"${!tools_commands[@]}\"; do\n\t\tif ! command -v \"${tools_commands[$tool]}\" >/dev/null 2>&1; then\n\t\t\t#\t\t\tprintf \"%b [*] %s\\t\\t[NO]%b\\n\" \"$bred\" \"$tool\" \"$reset\"\n\t\t\tall_installed=false\n\t\t\tmissing_tools+=(\"$tool\")\n\t\tfi\n\tdone\n\n\tif [[ $all_installed == true ]]; then\n\t\tprintf \"%b\\n Good! All tools are installed! %b\\n\\n\" \"$bgreen\" \"$reset\"\n\telse\n\t\tprintf \"\\n%bSome tools or directories are missing:%b\\n\\n\" \"$yellow\" \"$reset\"\n\t\tfor tool in \"${missing_tools[@]}\"; do\n\t\t\tprintf \"%b - %s %b\\n\" \"$bred\" \"$tool\" \"$reset\"\n\t\tdone\n\t\tprintf \"\\n%bTry running the installer script again: ./install.sh%b\\n\" \"$yellow\" \"$reset\"\n\t\tprintf \"%bIf it fails, try installing the missing tools manually.%b\\n\" \"$yellow\" \"$reset\"\n\t\tprintf \"%bEnsure that the %b\\$tools%b variable is correctly set at the start of this script.%b\\n\" \"$yellow\" \"$bred\" \"$yellow\" \"$reset\"\n\t\tprintf \"%bIf you need assistance, feel free to contact me! :D%b\\n\\n\" \"$yellow\" \"$reset\"\n\tfi\n\n\tprintf \"%b[%s] Tools check finished%b\\n\" \"$bblue\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"$reset\"\n\tprintf \"%b#######################################################################\\n%b\" \"$bgreen\" \"$reset\"\n}\n\n#####################################################################cc##########################################\n################################################### OSINT #####################################################\n###############################################################################################################\n\nfunction google_dorks() {\n\tmkdir -p osint\n\n\tif { [[ ! -f \"$called_fn_dir/.${FUNCNAME[0]}\" ]] || [[ $DIFF == true ]]; } && [[ $GOOGLE_DORKS == true ]] && [[ $OSINT == true ]]; then\n\t\tstart_func \"${FUNCNAME[0]}\" \"Running: Google Dorks in process\"\n\n\t\tpython3 \"${tools}/dorks_hunter/dorks_hunter.py\" -d \"$domain\" -o \"osint/dorks.txt\"\n\t\tend_func \"Results are saved in $domain/osint/dorks.txt\" \"${FUNCNAME[0]}\"\n\telse\n\t\tif [[ $GOOGLE_DORKS == false ]] || [[ $OSINT == false ]]; then\n\t\t\tprintf \"\\n%b[%s] %s skipped due to mode or configuration settings.%b\\n\" \"$yellow\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"${FUNCNAME[0]}\" \"$reset\"\n\t\telse\n\t\t\tprintf \"%b[%s] %s has already been processed. To force execution, delete:\\n    %s/.%s %b\\n\\n\" \"$yellow\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"${FUNCNAME[0]}\" \"$called_fn_dir\" \"${FUNCNAME[0]}\" \"$reset\"\n\t\tfi\n\tfi\n}\n\nfunction github_dorks() {\n\tmkdir -p osint\n\n\tif { [[ ! -f \"$called_fn_dir/.${FUNCNAME[0]}\" ]] || [[ $DIFF == true ]]; } && [[ $GITHUB_DORKS == true ]] && [[ $OSINT == true ]]; then\n\t\tstart_func \"${FUNCNAME[0]}\" \"Running: Github Dorks in process\"\n\n\t\tif [[ -s $GITHUB_TOKENS ]]; then\n\t\t\tif [[ $DEEP == true ]]; then\n\t\t\t\tif ! gitdorks_go -gd \"${tools}/gitdorks_go/Dorks/medium_dorks.txt\" -nws 20 -target \"$domain\" -tf \"$GITHUB_TOKENS\" -ew 3 | anew -q osint/gitdorks.txt; then\n\t\t\t\t\tprintf \"%b[!] gitdorks_go command failed.%b\\n\" \"$bred\" \"$reset\"\n\t\t\t\t\treturn 1\n\t\t\t\tfi\n\t\t\telse\n\t\t\t\tif ! gitdorks_go -gd \"${tools}/gitdorks_go/Dorks/smalldorks.txt\" -nws 20 -target \"$domain\" -tf \"$GITHUB_TOKENS\" -ew 3 | anew -q osint/gitdorks.txt; then\n\t\t\t\t\tprintf \"%b[!] gitdorks_go command failed.%b\\n\" \"$bred\" \"$reset\"\n\t\t\t\t\treturn 1\n\t\t\t\tfi\n\t\t\tfi\n\t\telse\n\t\t\tprintf \"\\n%b[%s] Required file %s does not exist or is empty.%b\\n\" \"$bred\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"$GITHUB_TOKENS\" \"$reset\"\n\t\t\treturn 1\n\t\tfi\n\t\tend_func \"Results are saved in $domain/osint/gitdorks.txt\" \"${FUNCNAME[0]}\"\n\telse\n\t\tif [[ $GITHUB_DORKS == false ]] || [[ $OSINT == false ]]; then\n\t\t\tprintf \"\\n%b[%s] %s skipped due to mode or configuration settings.%b\\n\" \"$yellow\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"${FUNCNAME[0]}\" \"$reset\"\n\t\telse\n\t\t\tprintf \"%b[%s] %s has already been processed. To force execution, delete:\\n    %s/.%s %b\\n\\n\" \"$yellow\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"${FUNCNAME[0]}\" \"$called_fn_dir\" \"${FUNCNAME[0]}\" \"$reset\"\n\t\tfi\n\tfi\n}\n\nfunction github_repos() {\n\tmkdir -p .tmp\n\n\tif { [[ ! -f \"$called_fn_dir/.${FUNCNAME[0]}\" ]] || [[ $DIFF == true ]]; } && [[ $GITHUB_REPOS == true ]] && [[ $OSINT == true ]]; then\n\t\tstart_func \"${FUNCNAME[0]}\" \"Github Repos analysis in process\"\n\n\t\tif [[ -s $GITHUB_TOKENS ]]; then\n\t\t\tGH_TOKEN=$(head -n 1 \"$GITHUB_TOKENS\")\n\t\t\techo \"$domain\" | unfurl format %r >.tmp/company_name.txt\n\n\t\t\tif ! enumerepo -token-string \"$GH_TOKEN\" -usernames .tmp/company_name.txt -o .tmp/company_repos.txt 2>>\"$LOGFILE\" >/dev/null; then\n\t\t\t\tprintf \"%b[!] enumerepo command failed.%b\\n\" \"$bred\" \"$reset\"\n\t\t\tfi\n\n\t\t\tif [[ -s \".tmp/company_repos.txt\" ]]; then\n\t\t\t\tif ! jq -r '.[].repos[]|.url' <.tmp/company_repos.txt >.tmp/company_repos_url.txt 2>>\"$LOGFILE\"; then\n\t\t\t\t\tprintf \"%b[!] jq command failed.%b\\n\" \"$bred\" \"$reset\"\n\t\t\t\tfi\n\t\t\tfi\n\n\t\t\tmkdir -p .tmp/github_repos 2>>\"$LOGFILE\"\n\t\t\tmkdir -p .tmp/github 2>>\"$LOGFILE\"\n\n\t\t\tif [[ -s \".tmp/company_repos_url.txt\" ]]; then\n\t\t\t\tif ! interlace -tL .tmp/company_repos_url.txt -threads \"$INTERLACE_THREADS\" -c \"git clone _target_ .tmp/github_repos/_cleantarget_\" 2>>\"$LOGFILE\" >/dev/null; then\n\t\t\t\t\tprintf \"%b[!] interlace git clone command failed.%b\\n\" \"$bred\" \"$reset\"\n\t\t\t\t\treturn 1\n\t\t\t\tfi\n\t\t\telse\n\t\t\t\tend_func \"Results are saved in $domain/osint/github_company_secrets.json\" \"${FUNCNAME[0]}\"\n\t\t\t\treturn 1\n\t\t\tfi\n\n\t\t\tif [[ -d \".tmp/github_repos/\" ]]; then\n\t\t\t\tls .tmp/github_repos >.tmp/github_repos_folders.txt\n\t\t\telse\n\t\t\t\tend_func \"Results are saved in $domain/osint/github_company_secrets.json\" \"${FUNCNAME[0]}\"\n\t\t\t\treturn 1\n\t\t\tfi\n\n\t\t\tif [[ -s \".tmp/github_repos_folders.txt\" ]]; then\n\t\t\t\tif ! interlace -tL .tmp/github_repos_folders.txt -threads \"$INTERLACE_THREADS\" -c \"gitleaks detect --source .tmp/github_repos/_target_ --no-banner --no-color -r .tmp/github/gh_secret_cleantarget_.json\" 2>>\"$LOGFILE\" >/dev/null; then\n\t\t\t\t\tprintf \"%b[!] interlace gitleaks command failed.%b\\n\" \"$bred\" \"$reset\"\n\t\t\t\t\tend_func \"Results are saved in $domain/osint/github_company_secrets.json\" \"${FUNCNAME[0]}\"\n\t\t\t\t\treturn 1\n\t\t\t\tfi\n\t\t\telse\n\t\t\t\tend_func \"Results are saved in $domain/osint/github_company_secrets.json\" \"${FUNCNAME[0]}\"\n\t\t\t\treturn 1\n\t\t\tfi\n\n\t\t\tif [[ -s \".tmp/company_repos_url.txt\" ]]; then\n\t\t\t\tif ! interlace -tL .tmp/company_repos_url.txt -threads \"$INTERLACE_THREADS\" -c \"trufflehog git _target_ -j 2>&1 | jq -c > _output_/_cleantarget_\" -o .tmp/github/ 2>>\"$LOGFILE\" >/dev/null; then\n\t\t\t\t\tprintf \"%b[!] interlace trufflehog command failed.%b\\n\" \"$bred\" \"$reset\"\n\t\t\t\t\treturn 1\n\t\t\t\tfi\n\t\t\tfi\n\n\t\t\tif [[ -d \".tmp/github/\" ]]; then\n\t\t\t\tif ! cat .tmp/github/* 2>/dev/null | jq -c | jq -r >\"osint/github_company_secrets.json\" 2>>\"$LOGFILE\"; then\n\t\t\t\t\tprintf \"%b[!] Error combining results.%b\\n\" \"$bred\" \"$reset\"\n\t\t\t\t\treturn 1\n\t\t\t\tfi\n\t\t\telse\n\t\t\t\tend_func \"Results are saved in $domain/osint/github_company_secrets.json\" \"${FUNCNAME[0]}\"\n\t\t\t\treturn 1\n\t\t\tfi\n\n\t\t\tend_func \"Results are saved in $domain/osint/github_company_secrets.json\" \"${FUNCNAME[0]}\"\n\t\telse\n\t\t\tprintf \"\\n%s[%s] Required file %s does not exist or is empty.%b\\n\" \"$bred\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"$GITHUB_TOKENS\" \"$reset\"\n\t\t\treturn 1\n\t\tfi\n\telse\n\t\tif [[ $GITHUB_REPOS == false ]] || [[ $OSINT == false ]]; then\n\t\t\tprintf \"\\n%b[%s] %s skipped due to mode or configuration settings.%b\\n\" \"$yellow\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"${FUNCNAME[0]}\" \"$reset\"\n\t\telse\n\t\t\tprintf \"%b[%s] %s has already been processed. To force execution, delete:\\n    %s/.%s %b\\n\\n\" \"$yellow\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"${FUNCNAME[0]}\" \"$called_fn_dir\" \"${FUNCNAME[0]}\" \"$reset\"\n\t\tfi\n\tfi\n}\n\nfunction metadata() {\n\tmkdir -p osint\n\n\t# Check if the function should run\n\tif { [[ ! -f \"${called_fn_dir}/.${FUNCNAME[0]}\" ]] || [[ ${DIFF} == true ]]; } && [[ ${METADATA} == true ]] && [[ ${OSINT} == true ]] && ! [[ ${domain} =~ ^[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+$ ]]; then\n\t\tstart_func \"${FUNCNAME[0]}\" \"Scanning metadata in public files\"\n\n\t\t# Run metafinder and check for errors\n\t\tif ! metafinder -d \"${domain}\" -l \"${METAFINDER_LIMIT}\" -o osint -go -bi &>>\"${LOGFILE}\"; then\n\t\t\tprintf \"%b[!] metafinder command failed.%b\\n\" \"${bred}\" \"${reset}\"\n\t\t\treturn 1\n\t\tfi\n\n\t\t# Move result files and check for errors\n\t\tif [ -d \"osint/${domain}\" ] && [ \"$(ls -A \"osint/${domain}\")\" ]; then\n\t\t\tif ! mv \"osint/${domain}/\"*.txt \"osint/\" 2>>\"${LOGFILE}\"; then\n\t\t\t\tprintf \"%b[!] Failed to move metadata files.%b\\n\" \"${bred}\" \"${reset}\"\n\t\t\t\treturn 1\n\t\t\tfi\n\t\tfi\n\n\t\t# Remove temporary directory and check for errors\n\t\tif ! rm -rf \"osint/${domain}\" 2>>\"${LOGFILE}\"; then\n\t\t\tprintf \"%b[!] Failed to remove temporary directory.%b\\n\" \"${bred}\" \"${reset}\"\n\t\t\treturn 1\n\t\tfi\n\n\t\tend_func \"Results are saved in ${domain}/osint/[software/authors/metadata_results].txt\" \"${FUNCNAME[0]}\"\n\telse\n\t\tif [[ ${METADATA} == false ]] || [[ ${OSINT} == false ]]; then\n\t\t\tprintf \"\\n%b[%s] %s skipped due to mode or configuration settings.%b\\n\" \"${yellow}\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"${FUNCNAME[0]}\" \"${reset}\"\n\t\telif [[ ${domain} =~ ^[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+$ ]]; then\n\t\t\treturn\n\t\telse\n\t\t\tprintf \"%b[%s] %s has already been processed. To force execution, delete:\\n    %s/.%s %b\\n\\n\" \"${yellow}\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"${FUNCNAME[0]}\" \"${called_fn_dir}\" \"${FUNCNAME[0]}\" \"${reset}\"\n\t\tfi\n\tfi\n}\n\nfunction apileaks() {\n\tmkdir -p osint\n\n\t# Check if the function should run\n\tif { [[ ! -f \"$called_fn_dir/.${FUNCNAME[0]}\" ]] || [[ $DIFF == true ]]; } &&\n\t\t[[ $API_LEAKS == true ]] && [[ $OSINT == true ]] &&\n\t\t! [[ $domain =~ ^[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+$ ]]; then\n\n\t\tstart_func \"${FUNCNAME[0]}\" \"Scanning for leaks in public API directories\"\n\n\t\t# Run porch-pirate and handle errors\n\t\tporch-pirate -s \"$domain\" --dump 2>>\"$LOGFILE\" >\"${dir}/osint/postman_leaks.txt\"\n\n\t\t# Change directory to SwaggerSpy\n\t\tif ! pushd \"${tools}/SwaggerSpy\" >/dev/null; then\n\t\t\tprintf \"%b[!] Failed to change directory to %s in %s at line %s.%b\\n\" \"$bred\" \"${tools}/SwaggerSpy\" \"${FUNCNAME[0]}\" \"$LINENO\" \"$reset\"\n\t\t\treturn 1\n\t\tfi\n\n\t\t# Run swaggerspy.py and handle errors\n\t\tpython3 swaggerspy.py \"$domain\" 2>>\"$LOGFILE\" | grep -i \"[*]\\|URL\" >\"${dir}/osint/swagger_leaks.txt\"\n\n\t\t# Return to the previous directory\n\t\tif ! popd >/dev/null; then\n\t\t\tprintf \"%b[!] Failed to return to the previous directory in %s at line %s.%b\\n\" \"$bred\" \"${FUNCNAME[0]}\" \"$LINENO\" \"$reset\"\n\t\t\treturn 1\n\t\tfi\n\n\t\t# Analyze leaks with trufflehog\n\t\tif [[ -s \"${dir}/osint/postman_leaks.txt\" ]]; then\n\t\t\ttrufflehog filesystem \"${dir}/osint/postman_leaks.txt\" -j 2>/dev/null | jq -c | anew -q \"${dir}/osint/postman_leaks_trufflehog.json\"\n\t\tfi\n\n\t\tif [[ -s \"${dir}/osint/swagger_leaks.txt\" ]]; then\n\t\t\ttrufflehog filesystem \"${dir}/osint/swagger_leaks.txt\" -j 2>/dev/null | jq -c | anew -q \"${dir}/osint/swagger_leaks_trufflehog.json\"\n\t\tfi\n\n\t\tend_func \"Results are saved in $domain/osint/[postman_leaks_trufflehog.json, swagger_leaks_trufflehog.json]\" \"${FUNCNAME[0]}\"\n\telse\n\t\tif [[ $API_LEAKS == false ]] || [[ $OSINT == false ]]; then\n\t\t\tprintf \"\\n%b[%s] %s skipped due to mode or configuration settings.%b\\n\" \"$yellow\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"${FUNCNAME[0]}\" \"$reset\"\n\t\telif [[ $domain =~ ^[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+$ ]]; then\n\t\t\treturn\n\t\telse\n\t\t\tprintf \"%b[%s] %s has already been processed. To force execution, delete:\\n    %s/.%s %b\\n\\n\" \"$yellow\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"${FUNCNAME[0]}\" \"$called_fn_dir\" \"${FUNCNAME[0]}\" \"$reset\"\n\t\tfi\n\tfi\n}\n\nfunction emails() {\n\tmkdir -p .tmp osint\n\n\t# Check if the function should run\n\tif { [[ ! -f \"$called_fn_dir/.${FUNCNAME[0]}\" ]] || [[ $DIFF == true ]]; } &&\n\t\t[[ $EMAILS == true ]] && [[ $OSINT == true ]] &&\n\t\t! [[ $domain =~ ^[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+$ ]]; then\n\n\t\tstart_func \"${FUNCNAME[0]}\" \"Searching for emails/users/passwords leaks\"\n\n\t\t# Run emailfinder and handle errors\n\t\temailfinder -d \"$domain\" 2>>\"$LOGFILE\" | anew -q .tmp/emailfinder.txt\n\n\t\t# Process emailfinder results\n\t\tif [[ -s \".tmp/emailfinder.txt\" ]]; then\n\t\t\tgrep \"@\" .tmp/emailfinder.txt | grep -iv \"|_\" | anew -q osint/emails.txt\n\t\tfi\n\n\t\t# Change directory to LeakSearch\n\t\tif ! pushd \"${tools}/LeakSearch\" >/dev/null; then\n\t\t\tprintf \"%b[!] Failed to change directory to %s in %s at line %s.%b\\n\" \"$bred\" \"${tools}/LeakSearch\" \"${FUNCNAME[0]}\" \"$LINENO\" \"$reset\"\n\t\t\treturn 1\n\t\tfi\n\n\t\t# Run LeakSearch.py and handle errors\n\t\tpython3 LeakSearch.py -k \"$domain\" -o \"${dir}/.tmp/passwords.txt\" 1>>\"$LOGFILE\"\n\n\t\t# Return to the previous directory\n\t\tif ! popd >/dev/null; then\n\t\t\tprintf \"%b[!] Failed to return to the previous directory in %s at line %s.%b\\n\" \"$bred\" \"${FUNCNAME[0]}\" \"$LINENO\" \"$reset\"\n\t\t\treturn 1\n\t\tfi\n\n\t\t# Process passwords.txt\n\t\tif [[ -s \"${dir}/.tmp/passwords.txt\" ]]; then\n\t\t\tanew -q osint/passwords.txt <\"${dir}/.tmp/passwords.txt\"\n\t\tfi\n\n\t\tend_func \"Results are saved in $domain/osint/emails.txt and passwords.txt\" \"${FUNCNAME[0]}\"\n\telse\n\t\tif [[ $EMAILS == false ]] || [[ $OSINT == false ]]; then\n\t\t\tprintf \"\\n%b[%s] %s skipped due to mode or configuration settings.%b\\n\" \"$yellow\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"${FUNCNAME[0]}\" \"$reset\"\n\t\telif [[ $domain =~ ^[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+$ ]]; then\n\t\t\treturn\n\t\telse\n\t\t\tprintf \"%b[%s] %s has already been processed. To force execution, delete:\\n    %s/.%s%b\\n\\n\" \"$yellow\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"${FUNCNAME[0]}\" \"$called_fn_dir\" \"/.${FUNCNAME[0]}\" \"$reset\"\n\t\tfi\n\tfi\n}\n\nfunction domain_info() {\n\n\tmkdir -p osint\n\n\t# Check if the function should run\n\tif { [[ ! -f \"$called_fn_dir/.${FUNCNAME[0]}\" ]] || [[ $DIFF == true ]]; } &&\n\t\t[[ $DOMAIN_INFO == true ]] && [[ $OSINT == true ]] &&\n\t\t! [[ $domain =~ ^[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+$ ]]; then\n\n\t\tstart_func \"${FUNCNAME[0]}\" \"Searching domain info (whois, registrant name/email domains)\"\n\n\t\t# Run whois command and check for errors\n\t\twhois -H \"$domain\" >\"osint/domain_info_general.txt\"\n\n\t\t# Fetch tenant info using curl and check for errors\n\t\tcurl -s \"https://aadinternals.azurewebsites.net/api/tenantinfo?domainName=${domain}\" \\\n\t\t\t-H \"Origin: https://aadinternals.com\" \\\n\t\t\t-H \"Referer: https://aadinternals.com/\" \\\n\t\t\t-H \"User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:72.0) Gecko/20100101 Firefox/72.0\" |\n\t\t\tjq -r '.domains[].name' >\"osint/azure_tenant_domains.txt\"\n\n\t\tend_func \"Results are saved in ${domain}/osint/domain_info_[general/azure_tenant_domains].txt\" \"${FUNCNAME[0]}\"\n\n\telse\n\t\tif [[ $DOMAIN_INFO == false ]] || [[ $OSINT == false ]]; then\n\t\t\tprintf \"\\n%b[%s] %s skipped due to mode or configuration settings.%b\\n\" \\\n\t\t\t\t\"$yellow\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"${FUNCNAME[0]}\" \"$reset\"\n\t\telif [[ $domain =~ ^[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+$ ]]; then\n\t\t\treturn\n\t\telse\n\t\t\tprintf \"%b[%s] %s has already been processed. To force execution, delete:\\n    %s/.%s%b\\n\\n\" \\\n\t\t\t\t\"$yellow\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"${FUNCNAME[0]}\" \"$called_fn_dir\" \"/.${FUNCNAME[0]}\" \"$reset\"\n\t\tfi\n\tfi\n}\n\nfunction third_party_misconfigs() {\n\tmkdir -p osint\n\n\t# Check if the function should run\n\tif { [[ ! -f \"$called_fn_dir/.${FUNCNAME[0]}\" ]] || [[ $DIFF == true ]]; } &&\n\t\t[[ $THIRD_PARTIES == true ]] && [[ $OSINT == true ]] &&\n\t\t! [[ $domain =~ ^[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+$ ]]; then\n\n\t\tstart_func \"${FUNCNAME[0]}\" \"Searching for third parties misconfigurations\"\n\n\t\t# Extract company name from domain\n\t\tcompany_name=$(unfurl format %r <<<\"$domain\")\n\n\t\t# Change directory to misconfig-mapper tool\n\t\tif ! pushd \"${tools}/misconfig-mapper\" >/dev/null; then\n\t\t\tprintf \"%b[!] Failed to change directory to %s in %s at line %s.%b\\n\" \\\n\t\t\t\t\"$bred\" \"${tools}/misconfig-mapper\" \"${FUNCNAME[0]}\" \"$LINENO\" \"$reset\"\n\t\t\treturn 1\n\t\tfi\n\n\t\t# Run misconfig-mapper and handle errors\n\t\t./misconfig-mapper -target \"$company_name\" -service \"*\" 2>&1 | grep -v \"\\-\\]\" | grep -v \"Failed\" >\"${dir}/osint/3rdparts_misconfigurations.txt\"\n\n\t\t# Return to the previous directory\n\t\tif ! popd >/dev/null; then\n\t\t\tprintf \"%b[!] Failed to return to previous directory in %s at line %s.%b\\n\" \\\n\t\t\t\t\"$bred\" \"${FUNCNAME[0]}\" \"$LINENO\" \"$reset\"\n\t\t\treturn 1\n\t\tfi\n\n\t\tend_func \"Results are saved in $domain/osint/3rdparts_misconfigurations.txt\" \"${FUNCNAME[0]}\"\n\n\telse\n\t\tif [[ $THIRD_PARTIES == false ]] || [[ $OSINT == false ]]; then\n\t\t\tprintf \"\\n%b[%s] %s skipped due to mode or configuration settings.%b\\n\" \\\n\t\t\t\t\"$yellow\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"${FUNCNAME[0]}\" \"$reset\"\n\t\telif [[ $domain =~ ^[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+$ ]]; then\n\t\t\treturn\n\t\telse\n\t\t\tprintf \"%b[%s] %s has already been processed. To force execution, delete:\\n    %s/.%s%b\\n\\n\" \\\n\t\t\t\t\"$yellow\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"${FUNCNAME[0]}\" \"$called_fn_dir\" \"/.${FUNCNAME[0]}\" \"$reset\"\n\t\tfi\n\tfi\n}\n\nfunction spoof() {\n\tmkdir -p osint\n\n\t# Check if the function should run\n\tif { [[ ! -f \"$called_fn_dir/.${FUNCNAME[0]}\" ]] || [[ $DIFF == true ]]; } &&\n\t\t[[ $SPOOF == true ]] && [[ $OSINT == true ]] &&\n\t\t! [[ $domain =~ ^[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+$ ]]; then\n\n\t\tstart_func \"${FUNCNAME[0]}\" \"Searching for spoofable domains\"\n\n\t\t# Change directory to Spoofy tool\n\t\tif ! pushd \"${tools}/Spoofy\" >/dev/null; then\n\t\t\tprintf \"%b[!] Failed to change directory to %s in %s at line %s.%b\\n\" \\\n\t\t\t\t\"$bred\" \"${tools}/Spoofy\" \"${FUNCNAME[0]}\" \"$LINENO\" \"$reset\"\n\t\t\treturn 1\n\t\tfi\n\n\t\t# Run spoofy.py and handle errors\n\t\t./spoofy.py -d \"$domain\" >\"${dir}/osint/spoof.txt\"\n\n\t\t# Return to the previous directory\n\t\tif ! popd >/dev/null; then\n\t\t\tprintf \"%b[!] Failed to return to previous directory in %s at line %s.%b\\n\" \\\n\t\t\t\t\"$bred\" \"${FUNCNAME[0]}\" \"$LINENO\" \"$reset\"\n\t\t\treturn 1\n\t\tfi\n\n\t\tend_func \"Results are saved in $domain/osint/spoof.txt\" \"${FUNCNAME[0]}\"\n\n\telse\n\t\tif [[ $SPOOF == false ]] || [[ $OSINT == false ]]; then\n\t\t\tprintf \"\\n%b[%s] %s skipped due to mode or configuration settings.%b\\n\" \\\n\t\t\t\t\"$yellow\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"${FUNCNAME[0]}\" \"$reset\"\n\t\telif [[ $domain =~ ^[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+$ ]]; then\n\t\t\treturn\n\t\telse\n\t\t\tprintf \"%b[%s] %s has already been processed. To force execution, delete:\\n    %s/.%s%b\\n\\n\" \\\n\t\t\t\t\"$yellow\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"${FUNCNAME[0]}\" \"$called_fn_dir\" \"/.${FUNCNAME[0]}\" \"$reset\"\n\t\tfi\n\tfi\n}\n\nfunction ip_info() {\n\n\tmkdir -p osint\n\n\t# Check if the function should run\n\tif { [[ ! -f \"$called_fn_dir/.${FUNCNAME[0]}\" ]] || [[ $DIFF == true ]]; } &&\n\t\t[[ $IP_INFO == true ]] && [[ $OSINT == true ]] &&\n\t\t[[ $domain =~ ^[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+$ ]]; then\n\n\t\tstart_func \"${FUNCNAME[0]}\" \"Searching IP info\"\n\n\t\tif [[ -n $WHOISXML_API ]]; then\n\n\t\t\t# Reverse IP lookup\n\t\t\tcurl -s \"https://reverse-ip.whoisxmlapi.com/api/v1?apiKey=${WHOISXML_API}&ip=${domain}\" |\n\t\t\t\tjq -r '.result[].name' 2>>\"$LOGFILE\" |\n\t\t\t\tsed -e \"s/$/ ${domain}/\" |\n\t\t\t\tanew -q \"osint/ip_${domain}_relations.txt\"\n\n\t\t\t# WHOIS lookup\n\t\t\tcurl -s \"https://www.whoisxmlapi.com/whoisserver/WhoisService?apiKey=${WHOISXML_API}&domainName=${domain}&outputFormat=json&da=2&registryRawText=1&registrarRawText=1&ignoreRawTexts=1\" |\n\t\t\t\tjq 2>>\"$LOGFILE\" |\n\t\t\t\tanew -q \"osint/ip_${domain}_whois.txt\"\n\n\t\t\t# IP Geolocation\n\t\t\tcurl -s \"https://ip-geolocation.whoisxmlapi.com/api/v1?apiKey=${WHOISXML_API}&ipAddress=${domain}\" |\n\t\t\t\tjq -r '.ip,.location' 2>>\"$LOGFILE\" |\n\t\t\t\tanew -q \"osint/ip_${domain}_location.txt\"\n\n\t\t\tend_func \"Results are saved in ${domain}/osint/ip_[domain_relations|whois|location].txt\" \"${FUNCNAME[0]}\"\n\n\t\telse\n\t\t\tprintf \"\\n%s[%s] WHOISXML_API variable is not defined. Skipping function.%b\\n\" \\\n\t\t\t\t\"$yellow\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"$reset\"\n\t\tfi\n\n\telse\n\t\tif [[ $IP_INFO == false ]] || [[ $OSINT == false ]]; then\n\t\t\tprintf \"\\n%b[%s] %s skipped due to mode or configuration settings.%b\\n\" \\\n\t\t\t\t\"$yellow\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"${FUNCNAME[0]}\" \"$reset\"\n\t\telif ! [[ $domain =~ ^[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+$ ]]; then\n\t\t\treturn\n\t\telse\n\t\t\tprintf \"%b[%s] %s has already been processed. To force execution, delete:\\n    %s/.%s%b\\n\\n\" \\\n\t\t\t\t\"$yellow\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"${FUNCNAME[0]}\" \"$called_fn_dir\" \"/.${FUNCNAME[0]}\" \"$reset\"\n\t\tfi\n\tfi\n\n}\n\n###############################################################################################################\n############################################### SUBDOMAINS ####################################################\n###############################################################################################################\n\nfunction subdomains_full() {\n\n\t# Create necessary directories\n\tif ! mkdir -p .tmp webs subdomains; then\n\t\tprintf \"%b[!] Failed to create directories.%b\\n\" \"$bred\" \"$reset\"\n\t\treturn 1\n\tfi\n\n\tNUMOFLINES_subs=\"0\"\n\tNUMOFLINES_probed=\"0\"\n\n\tprintf \"%b#######################################################################%b\\n\\n\" \"$bgreen\" \"$reset\"\n\n\t# Check if domain is an IP address\n\tif [[ $domain =~ ^[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+$ ]]; then\n\t\tprintf \"%b[%s] Scanning IP %s%b\\n\\n\" \"$bblue\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"$domain\" \"$reset\"\n\telse\n\t\tprintf \"%b[%s] Subdomain Enumeration %s%b\\n\\n\" \"$bblue\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"$domain\" \"$reset\"\n\tfi\n\n\t# Backup existing subdomains and webs\n\tif [[ -s \"subdomains/subdomains.txt\" ]]; then\n\t\tif ! cp \"subdomains/subdomains.txt\" \".tmp/subdomains_old.txt\"; then\n\t\t\tprintf \"%b[!] Failed to backup subdomains.txt.%b\\n\" \"$bred\" \"$reset\"\n\t\tfi\n\tfi\n\n\tif [[ -s \"webs/webs.txt\" ]]; then\n\t\tif ! cp \"webs/webs.txt\" \".tmp/probed_old.txt\"; then\n\t\t\tprintf \"%b[!] Failed to backup webs.txt.%b\\n\" \"$bred\" \"$reset\"\n\t\tfi\n\tfi\n\n\t# Update resolvers if necessary\n\tif { [[ ! -f \"$called_fn_dir/.sub_active\" ]] || [[ ! -f \"$called_fn_dir/.sub_brute\" ]] || [[ ! -f \"$called_fn_dir/.sub_permut\" ]] || [[ ! -f \"$called_fn_dir/.sub_recursive_brute\" ]]; } || [[ $DIFF == true ]]; then\n\t\tif ! resolvers_update; then\n\t\t\tprintf \"%b[!] Failed to update resolvers.%b\\n\" \"$bred\" \"$reset\"\n\t\t\treturn 1\n\t\tfi\n\tfi\n\n\t# Add in-scope subdomains\n\tif [[ -s $inScope_file ]]; then\n\t\tif ! cat \"$inScope_file\" | anew -q subdomains/subdomains.txt; then\n\t\t\tprintf \"%b[!] Failed to update subdomains.txt with in-scope domains.%b\\n\" \"$bred\" \"$reset\"\n\t\tfi\n\tfi\n\n\t# Subdomain enumeration\n\tif [[ ! $domain =~ ^[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+$ ]] && [[ $SUBDOMAINS_GENERAL == true ]]; then\n\t\tsub_passive\n\t\tsub_crt\n\t\tsub_active\n\t\tsub_tls\n\t\tsub_noerror\n\t\tsub_brute\n\t\tsub_permut\n\t\tsub_regex_permut\n\t\t# sub_gpt (commented out)\n\t\tsub_recursive_passive\n\t\tsub_recursive_brute\n\t\tsub_dns\n\t\tsub_scraping\n\t\tsub_analytics\n\telse\n\t\tnotification \"IP/CIDR detected, subdomains search skipped\" \"info\"\n\t\tif ! printf \"%b\\n\" \"$domain\" | anew -q subdomains/subdomains.txt; then\n\t\t\tprintf \"%b[!] Failed to add domain to subdomains.txt.%b\\n\" \"$bred\" \"$reset\"\n\t\tfi\n\tfi\n\n\t# Web probing\n\tif ! webprobe_simple; then\n\t\tprintf \"%b[!] webprobe_simple function failed.%b\\n\" \"$bred\" \"$reset\"\n\tfi\n\n\t# Process subdomains\n\tif [[ -s \"subdomains/subdomains.txt\" ]]; then\n\t\tif [[ -s $outOfScope_file ]]; then\n\t\t\tif ! deleteOutScoped \"$outOfScope_file\" \"subdomains/subdomains.txt\"; then\n\t\t\t\tprintf \"%b[!] Failed to remove out-of-scope subdomains.%b\\n\" \"$bred\" \"$reset\"\n\t\t\tfi\n\t\tfi\n\t\tif ! NUMOFLINES_subs=$(cat \"subdomains/subdomains.txt\" 2>>\"$LOGFILE\" | anew \".tmp/subdomains_old.txt\" | sed '/^$/d' | wc -l); then\n\t\t\tprintf \"%b[!] Failed to count new subdomains.%b\\n\" \"$bred\" \"$reset\"\n\t\t\tNUMOFLINES_subs=\"0\"\n\t\tfi\n\tfi\n\n\t# Process webs\n\tif [[ -s \"webs/webs.txt\" ]]; then\n\t\tif [[ -s $outOfScope_file ]]; then\n\t\t\tif ! deleteOutScoped \"$outOfScope_file\" \"webs/webs.txt\"; then\n\t\t\t\tprintf \"%b[!] Failed to remove out-of-scope webs.%b\\n\" \"$bred\" \"$reset\"\n\t\t\tfi\n\t\tfi\n\t\tif ! NUMOFLINES_probed=$(cat \"webs/webs.txt\" 2>>\"$LOGFILE\" | anew \".tmp/probed_old.txt\" | sed '/^$/d' | wc -l); then\n\t\t\tprintf \"%b[!] Failed to count new probed webs.%b\\n\" \"$bred\" \"$reset\"\n\t\t\tNUMOFLINES_probed=\"0\"\n\t\tfi\n\tfi\n\n\t# Display results\n\tprintf \"%b\\n[%s] Total subdomains:%b\\n\\n\" \"$bblue\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"$reset\"\n\tnotification \"- ${NUMOFLINES_subs} alive\" \"good\"\n\n\tif [[ -s \"subdomains/subdomains.txt\" ]]; then\n\t\tif ! sort \"subdomains/subdomains.txt\"; then\n\t\t\tprintf \"%b[!] Failed to sort subdomains.txt.%b\\n\" \"$bred\" \"$reset\"\n\t\tfi\n\tfi\n\n\tnotification \"- ${NUMOFLINES_probed} new web probed\" \"good\"\n\n\tif [[ -s \"webs/webs.txt\" ]]; then\n\t\tif ! sort \"webs/webs.txt\"; then\n\t\t\tprintf \"%b[!] Failed to sort webs.txt.%b\\n\" \"$bred\" \"$reset\"\n\t\tfi\n\tfi\n\n\tnotification \"Subdomain Enumeration Finished\" \"good\"\n\tprintf \"%b[%s] Results are saved in %s/subdomains/subdomains.txt and webs/webs.txt%b\\n\" \"$bblue\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"$domain\" \"$reset\"\n\tprintf \"%b#######################################################################%b\\n\\n\" \"$bgreen\" \"$reset\"\n\n}\n\nfunction sub_passive() {\n\n\tmkdir -p .tmp\n\n\tif { [[ ! -f \"$called_fn_dir/.${FUNCNAME[0]}\" ]] || [[ $DIFF == true ]]; } && [[ $SUBPASSIVE == true ]]; then\n\t\tstart_subfunc \"${FUNCNAME[0]}\" \"Running: Passive Subdomain Enumeration\"\n\n\t\t# Run subfinder and check for errors\n\t\tsubfinder -all -d \"$domain\" -max-time \"$SUBFINDER_ENUM_TIMEOUT\" -silent -o .tmp/subfinder_psub.txt 2>>\"$LOGFILE\" >/dev/null\n\t\tmerklemap-cli search $domain 2>/dev/null | awk -F' ' '{for(i=1;i<=NF;i++) if($i ~ /^domain=/) {split($i,a,\"=\"); print a[2]}}' | anew -q .tmp/subfinder_psub.txt 2>>\"$LOGFILE\" >/dev/null\n\n\t\t# Run github-subdomains if GITHUB_TOKENS is set and file is not empty\n\t\tif [[ -s $GITHUB_TOKENS ]]; then\n\t\t\tif [[ $DEEP == true ]]; then\n\t\t\t\tgithub-subdomains -d \"$domain\" -t \"$GITHUB_TOKENS\" -o .tmp/github_subdomains_psub.txt 2>>\"$LOGFILE\" >/dev/null\n\t\t\telse\n\t\t\t\tgithub-subdomains -d \"$domain\" -k -q -t \"$GITHUB_TOKENS\" -o .tmp/github_subdomains_psub.txt 2>>\"$LOGFILE\" >/dev/null\n\t\t\tfi\n\t\tfi\n\n\t\t# Run gitlab-subdomains if GITLAB_TOKENS is set and file is not empty\n\t\tif [[ -s $GITLAB_TOKENS ]]; then\n\t\t\tgitlab-subdomains -d \"$domain\" -t \"$GITLAB_TOKENS\" 2>>\"$LOGFILE\" | tee .tmp/gitlab_subdomains_psub.txt >/dev/null\n\t\tfi\n\n\t\t# Check if INSCOPE is true and run check_inscope\n\t\tif [[ $INSCOPE == true ]]; then\n\t\t\tcheck_inscope .tmp/subfinder_psub.txt 2>>\"$LOGFILE\" >/dev/null\n\t\t\tcheck_inscope .tmp/github_subdomains_psub.txt 2>>\"$LOGFILE\" >/dev/null\n\t\t\tcheck_inscope .tmp/gitlab_subdomains_psub.txt 2>>\"$LOGFILE\" >/dev/null\n\t\tfi\n\n\t\t# Combine results and count new lines\n\t\tNUMOFLINES=$(find .tmp -type f -iname \"*_psub.txt\" -exec cat {} + | sed \"s/^\\*\\.//\" | anew .tmp/passive_subs.txt | sed '/^$/d' | wc -l)\n\t\tend_subfunc \"${NUMOFLINES} new subs (passive)\" \"${FUNCNAME[0]}\"\n\n\telse\n\t\tif [[ $SUBPASSIVE == false ]]; then\n\t\t\tprintf \"\\n%b[%s] %s skipped due to mode or configuration settings.%b\\n\" \"$yellow\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"${FUNCNAME[0]}\" \"$reset\"\n\t\telse\n\t\t\tprintf \"%b[%s] %s has already been processed. To force execution, delete:\\n    %s/.%s%b\\n\\n\" \"$yellow\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"${FUNCNAME[0]}\" \"$called_fn_dir\" \"/.${FUNCNAME[0]}\" \"$reset\"\n\t\tfi\n\tfi\n\n}\n\nfunction sub_crt() {\n\n\tmkdir -p .tmp subdomains\n\n\tif { [[ ! -f \"$called_fn_dir/.${FUNCNAME[0]}\" ]] || [[ $DIFF == true ]]; } && [[ $SUBCRT == true ]]; then\n\t\tstart_subfunc \"${FUNCNAME[0]}\" \"Running: Crtsh Subdomain Enumeration\"\n\n\t\t# Run crt command and check for errors\n\t\tcrt -s -json -l \"${CTR_LIMIT}\" \"$domain\" 2>>\"$LOGFILE\" |\n\t\t\tjq -r '.[].subdomain' 2>>\"$LOGFILE\" |\n\t\t\tsed -e 's/^\\*\\.//' >.tmp/crtsh_subdomains.txt\n\n\t\t# Use anew to get new subdomains\n\t\tcat .tmp/crtsh_subdomains.txt | anew -q .tmp/crtsh_subs_tmp.txt\n\n\t\t# If INSCOPE is true, check inscope\n\t\tif [[ $INSCOPE == true ]]; then\n\t\t\tif ! check_inscope .tmp/crtsh_subs_tmp.txt 2>>\"$LOGFILE\" >/dev/null; then\n\t\t\t\tprintf \"%b[!] check_inscope command failed.%b\\n\" \"$bred\" \"$reset\"\n\t\t\t\treturn 1\n\t\t\tfi\n\t\tfi\n\n\t\t# Process subdomains and append new ones to .tmp/crtsh_subs.txt, count new lines\n\t\tNUMOFLINES=$(sed 's/^\\*\\.//' .tmp/crtsh_subs_tmp.txt | sed '/^$/d' | anew .tmp/crtsh_subs.txt | wc -l)\n\n\t\tend_subfunc \"${NUMOFLINES} new subs (cert transparency)\" \"${FUNCNAME[0]}\"\n\telse\n\t\tif [[ $SUBCRT == false ]]; then\n\t\t\tprintf \"\\n%b[%s] %s skipped due to mode or defined in reconftw.cfg.%b\\n\" \\\n\t\t\t\t\"$yellow\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"${FUNCNAME[0]}\" \"$reset\"\n\t\telse\n\t\t\tprintf \"%b[%s] %s has already been processed. To force execution, delete:\\n    %s/.%s%b\\n\\n\" \\\n\t\t\t\t\"$yellow\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"${FUNCNAME[0]}\" \"$called_fn_dir\" \"/.${FUNCNAME[0]}\" \"$reset\"\n\t\tfi\n\tfi\n}\n\nfunction sub_active() {\n\n\tmkdir -p .tmp subdomains\n\n\tif [[ ! -f \"$called_fn_dir/.${FUNCNAME[0]}\" ]] || [[ $DIFF == true ]]; then\n\t\tstart_subfunc \"${FUNCNAME[0]}\" \"Running: Active Subdomain Enumeration\"\n\n\t\t# Combine subdomain files into subs_no_resolved.txt\n\t\tif ! find .tmp -type f -iname \"*_subs.txt\" -exec cat {} + | anew -q .tmp/subs_no_resolved.txt; then\n\t\t\tprintf \"%b[!] Failed to collect subdomains into subs_no_resolved.txt.%b\\n\" \"$bred\" \"$reset\"\n\t\t\treturn 1\n\t\tfi\n\n\t\t# Delete out-of-scope domains if outOfScope_file exists\n\t\tif [[ -s $outOfScope_file ]]; then\n\t\t\tif ! deleteOutScoped \"$outOfScope_file\" .tmp/subs_no_resolved.txt; then\n\t\t\t\tprintf \"%b[!] deleteOutScoped command failed.%b\\n\" \"$bred\" \"$reset\"\n\t\t\t\treturn 1\n\t\t\tfi\n\t\tfi\n\n\t\tif [[ $AXIOM != true ]]; then\n\t\t\t# Update resolvers locally\n\t\t\tif ! resolvers_update_quick_local; then\n\t\t\t\tprintf \"%b[!] resolvers_update_quick_local command failed.%b\\n\" \"$bred\" \"$reset\"\n\t\t\t\treturn 1\n\t\t\tfi\n\n\t\t\t# Resolve subdomains using puredns\n\t\t\tif [[ -s \".tmp/subs_no_resolved.txt\" ]]; then\n\t\t\t\tpuredns resolve .tmp/subs_no_resolved.txt -w .tmp/subdomains_tmp.txt \\\n\t\t\t\t\t-r \"$resolvers\" --resolvers-trusted \"$resolvers_trusted\" \\\n\t\t\t\t\t-l \"$PUREDNS_PUBLIC_LIMIT\" --rate-limit-trusted \"$PUREDNS_TRUSTED_LIMIT\" \\\n\t\t\t\t\t--wildcard-tests \"$PUREDNS_WILDCARDTEST_LIMIT\" \\\n\t\t\t\t\t--wildcard-batch \"$PUREDNS_WILDCARDBATCH_LIMIT\" \\\n\t\t\t\t\t2>>\"$LOGFILE\" >/dev/null\n\t\t\tfi\n\t\telse\n\t\t\t# Update resolvers using axiom\n\t\t\tif ! resolvers_update_quick_axiom; then\n\t\t\t\tprintf \"%b[!] resolvers_update_quick_axiom command failed.%b\\n\" \"$bred\" \"$reset\"\n\t\t\t\treturn 1\n\t\t\tfi\n\n\t\t\t# Resolve subdomains using axiom-scan\n\t\t\tif [[ -s \".tmp/subs_no_resolved.txt\" ]]; then\n\t\t\t\taxiom-scan .tmp/subs_no_resolved.txt -m puredns-resolve \\\n\t\t\t\t\t-r /home/op/lists/resolvers.txt \\\n\t\t\t\t\t--resolvers-trusted /home/op/lists/resolvers_trusted.txt \\\n\t\t\t\t\t--wildcard-tests \"$PUREDNS_WILDCARDTEST_LIMIT\" \\\n\t\t\t\t\t--wildcard-batch \"$PUREDNS_WILDCARDBATCH_LIMIT\" \\\n\t\t\t\t\t-o .tmp/subdomains_tmp.txt $AXIOM_EXTRA_ARGS \\\n\t\t\t\t\t2>>\"$LOGFILE\" >/dev/null\n\t\t\tfi\n\t\tfi\n\n\t\t# Add the domain itself to the list if it resolves\n\t\techo \"$domain\" | dnsx -retry 3 -silent -r \"$resolvers_trusted\" \\\n\t\t\t2>>\"$LOGFILE\" | anew -q .tmp/subdomains_tmp.txt\n\n\t\t# If INSCOPE is true, check inscope\n\t\tif [[ $INSCOPE == true ]]; then\n\t\t\tif ! check_inscope .tmp/subdomains_tmp.txt 2>>\"$LOGFILE\" >/dev/null; then\n\t\t\t\tprintf \"%b[!] check_inscope command failed.%b\\n\" \"$bred\" \"$reset\"\n\t\t\t\treturn 1\n\t\t\tfi\n\t\tfi\n\n\t\t# Process subdomains and append new ones to subdomains.txt, count new lines\n\t\tif ! NUMOFLINES=$(grep \"\\.$domain$\\|^$domain$\" .tmp/subdomains_tmp.txt 2>>\"$LOGFILE\" |\n\t\t\tgrep -E '^([a-zA-Z0-9\\.\\-]+\\.)+[a-zA-Z]{1,}$' |\n\t\t\tanew subdomains/subdomains.txt | sed '/^$/d' | wc -l); then\n\t\t\tprintf \"%b[!] Failed to process subdomains.%b\\n\" \"$bred\" \"$reset\"\n\t\t\treturn 1\n\t\tfi\n\n\t\tend_subfunc \"${NUMOFLINES} subs DNS resolved from passive\" \"${FUNCNAME[0]}\"\n\telse\n\t\tprintf \"%b[%s] %s has already been processed. To force execution, delete:\\n    %s/.%s%b\\n\\n\" \\\n\t\t\t\"$yellow\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"${FUNCNAME[0]}\" \\\n\t\t\t\"$called_fn_dir\" \"/.${FUNCNAME[0]}\" \"$reset\"\n\tfi\n}\n\nfunction sub_tls() {\n\tmkdir -p .tmp subdomains\n\n\tif [[ ! -f \"$called_fn_dir/.${FUNCNAME[0]}\" ]] || [[ $DIFF == true ]]; then\n\t\tstart_subfunc \"${FUNCNAME[0]}\" \"Running: TLS Active Subdomain Enumeration\"\n\n\t\tif [[ $DEEP == true ]]; then\n\t\t\tif [[ $AXIOM != true ]]; then\n\t\t\t\ttlsx -san -cn -silent -ro -c \"$TLSX_THREADS\" \\\n\t\t\t\t\t-p \"$TLS_PORTS\" -o .tmp/subdomains_tlsx.txt <subdomains/subdomains.txt \\\n\t\t\t\t\t2>>\"$LOGFILE\" >/dev/null\n\t\t\telse\n\t\t\t\taxiom-scan subdomains/subdomains.txt -m tlsx \\\n\t\t\t\t\t-san -cn -silent -ro -c \"$TLSX_THREADS\" -p \"$TLS_PORTS\" \\\n\t\t\t\t\t-o .tmp/subdomains_tlsx.txt $AXIOM_EXTRA_ARGS 2>>\"$LOGFILE\" >/dev/null\n\t\t\tfi\n\t\telse\n\t\t\tif [[ $AXIOM != true ]]; then\n\t\t\t\ttlsx -san -cn -silent -ro -c \"$TLSX_THREADS\" <subdomains/subdomains.txt >.tmp/subdomains_tlsx.txt 2>>\"$LOGFILE\"\n\t\t\telse\n\t\t\t\taxiom-scan subdomains/subdomains.txt -m tlsx \\\n\t\t\t\t\t-san -cn -silent -ro -c \"$TLSX_THREADS\" \\\n\t\t\t\t\t-o .tmp/subdomains_tlsx.txt $AXIOM_EXTRA_ARGS 2>>\"$LOGFILE\" >/dev/null\n\t\t\tfi\n\t\tfi\n\n\t\tif [[ -s \".tmp/subdomains_tlsx.txt\" ]]; then\n\t\t\tgrep \"\\.$domain$\\|^$domain$\" .tmp/subdomains_tlsx.txt |\n\t\t\t\tgrep -E '^((http|https):\\/\\/)?([a-zA-Z0-9]([a-zA-Z0-9\\-]*[a-zA-Z0-9])?\\.)+[a-zA-Z]{1,}(\\/.*)?$' |\n\t\t\t\tsed \"s/|__ //\" | anew -q .tmp/subdomains_tlsx_clean.txt\n\t\tfi\n\n\t\tif [[ $AXIOM != true ]]; then\n\t\t\tif ! resolvers_update_quick_local; then\n\t\t\t\tprintf \"%b[!] resolvers_update_quick_local command failed.%b\\n\" \"$bred\" \"$reset\"\n\t\t\t\treturn 1\n\t\t\tfi\n\t\t\tif [[ -s \".tmp/subdomains_tlsx_clean.txt\" ]]; then\n\t\t\t\tpuredns resolve .tmp/subdomains_tlsx_clean.txt -w .tmp/subdomains_tlsx_resolved.txt \\\n\t\t\t\t\t-r \"$resolvers\" --resolvers-trusted \"$resolvers_trusted\" \\\n\t\t\t\t\t-l \"$PUREDNS_PUBLIC_LIMIT\" --rate-limit-trusted \"$PUREDNS_TRUSTED_LIMIT\" \\\n\t\t\t\t\t--wildcard-tests \"$PUREDNS_WILDCARDTEST_LIMIT\" --wildcard-batch \"$PUREDNS_WILDCARDBATCH_LIMIT\" \\\n\t\t\t\t\t2>>\"$LOGFILE\" >/dev/null\n\t\t\tfi\n\t\telse\n\t\t\tif ! resolvers_update_quick_axiom; then\n\t\t\t\tprintf \"%b[!] resolvers_update_quick_axiom command failed.%b\\n\" \"$bred\" \"$reset\"\n\t\t\t\treturn 1\n\t\t\tfi\n\t\t\tif [[ -s \".tmp/subdomains_tlsx_clean.txt\" ]]; then\n\t\t\t\taxiom-scan .tmp/subdomains_tlsx_clean.txt -m puredns-resolve \\\n\t\t\t\t\t-r /home/op/lists/resolvers.txt --resolvers-trusted /home/op/lists/resolvers_trusted.txt \\\n\t\t\t\t\t--wildcard-tests \"$PUREDNS_WILDCARDTEST_LIMIT\" --wildcard-batch \"$PUREDNS_WILDCARDBATCH_LIMIT\" \\\n\t\t\t\t\t-o .tmp/subdomains_tlsx_resolved.txt $AXIOM_EXTRA_ARGS \\\n\t\t\t\t\t2>>\"$LOGFILE\" >/dev/null\n\t\t\tfi\n\t\tfi\n\n\t\tif [[ $INSCOPE == true ]]; then\n\t\t\tif ! check_inscope .tmp/subdomains_tlsx_resolved.txt 2>>\"$LOGFILE\" >/dev/null; then\n\t\t\t\tprintf \"%b[!] check_inscope command failed.%b\\n\" \"$bred\" \"$reset\"\n\t\t\t\treturn 1\n\t\t\tfi\n\t\tfi\n\n\t\tif ! NUMOFLINES=$(anew subdomains/subdomains.txt <.tmp/subdomains_tlsx_resolved.txt | sed '/^$/d' | wc -l); then\n\t\t\tprintf \"%b[!] Counting new subdomains failed.%b\\n\" \"$bred\" \"$reset\"\n\t\t\treturn 1\n\t\tfi\n\n\t\tend_subfunc \"${NUMOFLINES} new subs (tls active enum)\" \"${FUNCNAME[0]}\"\n\telse\n\t\tprintf \"%b[%s] %s has already been processed. To force execution, delete:\\n    %s/.%s%b\\n\\n\" \\\n\t\t\t\"$yellow\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"${FUNCNAME[0]}\" \\\n\t\t\t\"$called_fn_dir\" \"/.${FUNCNAME[0]}\" \"$reset\"\n\tfi\n}\n\nfunction sub_noerror() {\n\n\tmkdir -p .tmp subdomains\n\n\tif { [[ ! -f \"$called_fn_dir/.${FUNCNAME[0]}\" ]] || [[ $DIFF == true ]]; } && [[ $SUBNOERROR == true ]]; then\n\t\tstart_subfunc \"${FUNCNAME[0]}\" \"Running: Checking NOERROR DNS response\"\n\n\t\t# Check for DNSSEC black lies\n\t\trandom_subdomain=\"${RANDOM}thistotallynotexist${RANDOM}.$domain\"\n\t\tdns_response=$(echo \"$random_subdomain\" | dnsx -r \"$resolvers\" -rcode noerror,nxdomain -retry 3 -silent | cut -d' ' -f2)\n\n\t\tif [[ $dns_response == \"[NXDOMAIN]\" ]]; then\n\t\t\tif ! resolvers_update_quick_local; then\n\t\t\t\tprintf \"%b[!] Failed to update resolvers.%b\\n\" \"$bred\" \"$reset\"\n\t\t\t\treturn 1\n\t\t\tfi\n\n\t\t\t# Determine wordlist based on DEEP setting\n\t\t\tif [[ $DEEP == true ]]; then\n\t\t\t\twordlist=\"$subs_wordlist_big\"\n\t\t\telse\n\t\t\t\twordlist=\"$subs_wordlist\"\n\t\t\tfi\n\n\t\t\t# Run dnsx and check for errors\n\t\t\tdnsx -d \"$domain\" -r \"$resolvers\" -silent \\\n\t\t\t\t-rcode noerror -w \"$wordlist\" \\\n\t\t\t\t2>>\"$LOGFILE\" | cut -d' ' -f1 | anew -q .tmp/subs_noerror.txt >/dev/null\n\n\t\t\t# Check inscope if INSCOPE is true\n\t\t\tif [[ $INSCOPE == true ]]; then\n\t\t\t\tif ! check_inscope .tmp/subs_noerror.txt 2>>\"$LOGFILE\" >/dev/null; then\n\t\t\t\t\tprintf \"%b[!] check_inscope command failed.%b\\n\" \"$bred\" \"$reset\"\n\t\t\t\t\treturn 1\n\t\t\t\tfi\n\t\t\tfi\n\n\t\t\t# Process subdomains and append new ones to subdomains.txt, count new lines\n\t\t\tif ! NUMOFLINES=$(grep \"\\.$domain$\\|^$domain$\" .tmp/subs_noerror.txt 2>>\"$LOGFILE\" |\n\t\t\t\tgrep -E '^([a-zA-Z0-9\\.\\-]+\\.)+[a-zA-Z]{1,}$' |\n\t\t\t\tsed 's/^\\*\\.//' | anew subdomains/subdomains.txt | sed '/^$/d' | wc -l); then\n\t\t\t\tprintf \"%b[!] Failed to process subdomains.%b\\n\" \"$bred\" \"$reset\"\n\t\t\t\treturn 1\n\t\t\tfi\n\n\t\t\tend_subfunc \"${NUMOFLINES} new subs (DNS noerror)\" \"${FUNCNAME[0]}\"\n\n\t\telse\n\t\t\tprintf \"\\n%s[%s] Detected DNSSEC black lies, skipping this technique.%b\\n\" \\\n\t\t\t\t\"$yellow\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"$reset\"\n\t\tfi\n\n\telse\n\t\tif [[ $SUBNOERROR == false ]]; then\n\t\t\tprintf \"\\n%b[%s] %s skipped due to mode or defined in reconftw.cfg.%b\\n\" \\\n\t\t\t\t\"$yellow\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"${FUNCNAME[0]}\" \"$reset\"\n\t\telse\n\t\t\tprintf \"%b[%s] %s has already been processed. To force execution, delete:\\n    %s/.%s%b\\n\\n\" \\\n\t\t\t\t\"$yellow\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"${FUNCNAME[0]}\" \\\n\t\t\t\t\"$called_fn_dir\" \"/.${FUNCNAME[0]}\" \"$reset\"\n\t\tfi\n\tfi\n\n}\n\nfunction sub_dns() {\n\tmkdir -p .tmp subdomains\n\n\tif [[ ! -f \"$called_fn_dir/.${FUNCNAME[0]}\" ]] || [[ $DIFF == true ]]; then\n\t\tstart_subfunc \"${FUNCNAME[0]}\" \"Running: DNS Subdomain Enumeration and PTR search\"\n\n\t\tif [[ $AXIOM != true ]]; then\n\t\t\tif [[ -s \"subdomains/subdomains.txt\" ]]; then\n\t\t\t\tdnsx -r \"$resolvers_trusted\" -a -aaaa -cname -ns -ptr -mx -soa -silent -retry 3 -json \\\n\t\t\t\t\t-o \"subdomains/subdomains_dnsregs.json\" <\"subdomains/subdomains.txt\" 2>>\"$LOGFILE\" >/dev/null\n\t\t\tfi\n\n\t\t\tif [[ -s \"subdomains/subdomains_dnsregs.json\" ]]; then\n\t\t\t\t# Extract various DNS records and process them\n\t\t\t\tjq -r 'try .a[], try .aaaa[], try .cname[], try .ns[], try .ptr[], try .mx[], try .soa[]' \\\n\t\t\t\t\t<\"subdomains/subdomains_dnsregs.json\" 2>/dev/null |\n\t\t\t\t\tgrep \"\\.$domain$\" |\n\t\t\t\t\tgrep -E '^([a-zA-Z0-9\\-\\.]+\\.)+[a-zA-Z]{1,}$' |\n\t\t\t\t\tanew -q .tmp/subdomains_dns.txt\n\n\t\t\t\tjq -r 'try .a[]' <\"subdomains/subdomains_dnsregs.json\" | sort -u |\n\t\t\t\t\thakip2host | awk '{print $3}' | unfurl -u domains |\n\t\t\t\t\tsed -e 's/^\\*\\.//' -e 's/\\.$//' -e '/\\./!d' |\n\t\t\t\t\tgrep \"\\.$domain$\" |\n\t\t\t\t\tgrep -E '^([a-zA-Z0-9\\-\\.]+\\.)+[a-zA-Z]{1,}$' |\n\t\t\t\t\tanew -q .tmp/subdomains_dns.txt\n\n\t\t\t\tjq -r 'try \"\\(.host) - \\(.a[])\"' <\"subdomains/subdomains_dnsregs.json\" 2>/dev/null |\n\t\t\t\t\tsort -u -k2 | anew -q \"subdomains/subdomains_ips.txt\"\n\t\t\tfi\n\n\t\t\tif ! resolvers_update_quick_local; then\n\t\t\t\tprintf \"%b[!] Failed to update resolvers.%b\\n\" \"$bred\" \"$reset\"\n\t\t\tfi\n\n\t\t\tif [[ -s \".tmp/subdomains_dns.txt\" ]]; then\n\t\t\t\tpuredns resolve .tmp/subdomains_dns.txt -w .tmp/subdomains_dns_resolved.txt \\\n\t\t\t\t\t-r \"$resolvers\" --resolvers-trusted \"$resolvers_trusted\" \\\n\t\t\t\t\t-l \"$PUREDNS_PUBLIC_LIMIT\" --rate-limit-trusted \"$PUREDNS_TRUSTED_LIMIT\" \\\n\t\t\t\t\t--wildcard-tests \"$PUREDNS_WILDCARDTEST_LIMIT\" --wildcard-batch \"$PUREDNS_WILDCARDBATCH_LIMIT\" \\\n\t\t\t\t\t2>>\"$LOGFILE\" >/dev/null\n\t\t\tfi\n\t\telse\n\t\t\tif [[ -s \"subdomains/subdomains.txt\" ]]; then\n\t\t\t\taxiom-scan \"subdomains/subdomains.txt\" -m dnsx -retry 3 -a -aaaa -cname -ns -ptr -mx -soa -json \\\n\t\t\t\t\t-o \"subdomains/subdomains_dnsregs.json\" \"$AXIOM_EXTRA_ARGS\" 2>>\"$LOGFILE\" >/dev/null\n\t\t\tfi\n\n\t\t\tif [[ -s \"subdomains/subdomains_dnsregs.json\" ]]; then\n\t\t\t\tjq -r 'try .a[]' <\"subdomains/subdomains_dnsregs.json\" | sort -u |\n\t\t\t\t\tanew -q .tmp/subdomains_dns_a_records.txt\n\n\t\t\t\tjq -r 'try .a[]' <\"subdomains/subdomains_dnsregs.json\" | sort -u |\n\t\t\t\t\thakip2host | awk '{print $3}' | unfurl -u domains |\n\t\t\t\t\tsed -e 's/^\\*\\.//' -e 's/\\.$//' -e '/\\./!d' |\n\t\t\t\t\tgrep \"\\.$domain$\" |\n\t\t\t\t\tgrep -E '^([a-zA-Z0-9\\-\\.]+\\.)+[a-zA-Z]{1,}$' |\n\t\t\t\t\tanew -q .tmp/subdomains_dns.txt\n\n\t\t\t\tjq -r 'try .a[], try .aaaa[], try .cname[], try .ns[], try .ptr[], try .mx[], try .soa[]' \\\n\t\t\t\t\t<\"subdomains/subdomains_dnsregs.json\" 2>/dev/null |\n\t\t\t\t\tgrep \"\\.$domain$\" |\n\t\t\t\t\tgrep -E '^([a-zA-Z0-9\\-\\.]+\\.)+[a-zA-Z]{1,}$' |\n\t\t\t\t\tanew -q .tmp/subdomains_dns.txt\n\n\t\t\t\tjq -r 'try \"\\(.host) - \\(.a[])\"' <\"subdomains/subdomains_dnsregs.json\" 2>/dev/null |\n\t\t\t\t\tsort -u -k2 | anew -q \"subdomains/subdomains_ips.txt\"\n\t\t\tfi\n\n\t\t\tif ! resolvers_update_quick_axiom; then\n\t\t\t\tprintf \"%b[!] Failed to update resolvers.%b\\n\" \"$bred\" \"$reset\"\n\t\t\tfi\n\n\t\t\tif [[ -s \".tmp/subdomains_dns.txt\" ]]; then\n\t\t\t\taxiom-scan .tmp/subdomains_dns.txt -m puredns-resolve \\\n\t\t\t\t\t-r \"/home/op/lists/resolvers.txt\" --resolvers-trusted \"/home/op/lists/resolvers_trusted.txt\" \\\n\t\t\t\t\t--wildcard-tests \"$PUREDNS_WILDCARDTEST_LIMIT\" --wildcard-batch \"$PUREDNS_WILDCARDBATCH_LIMIT\" \\\n\t\t\t\t\t-o .tmp/subdomains_dns_resolved.txt \"$AXIOM_EXTRA_ARGS\" \\\n\t\t\t\t\t2>>\"$LOGFILE\" >/dev/null\n\t\t\tfi\n\t\tfi\n\n\t\tif [[ $INSCOPE == true ]]; then\n\t\t\tif ! check_inscope .tmp/subdomains_dns_resolved.txt 2>>\"$LOGFILE\" >/dev/null; then\n\t\t\t\tprintf \"%b[!] check_inscope command failed.%b\\n\" \"$bred\" \"$reset\"\n\t\t\tfi\n\t\tfi\n\n\t\tif ! NUMOFLINES=$(grep \"\\.$domain$\\|^$domain$\" .tmp/subdomains_dns_resolved.txt 2>>\"$LOGFILE\" |\n\t\t\tgrep -E '^([a-zA-Z0-9\\-\\.]+\\.)+[a-zA-Z]{1,}$' |\n\t\t\tanew subdomains/subdomains.txt | sed '/^$/d' | wc -l); then\n\t\t\tprintf \"%b[!] Failed to count new subdomains.%b\\n\" \"$bred\" \"$reset\"\n\t\t\treturn 1\n\t\tfi\n\n\t\tend_subfunc \"${NUMOFLINES} new subs (dns resolution)\" \"${FUNCNAME[0]}\"\n\telse\n\t\tprintf \"\\n%b[%s] %s has already been processed. To force execution, delete:\\n    %s/.%s%b\\n\\n\" \\\n\t\t\t\"$yellow\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"${FUNCNAME[0]}\" \"$called_fn_dir\" \"/.${FUNCNAME[0]}\" \"$reset\"\n\tfi\n}\n\nfunction sub_brute() {\n\n\tmkdir -p .tmp subdomains\n\n\tif { [[ ! -f \"$called_fn_dir/.${FUNCNAME[0]}\" ]] || [[ $DIFF == true ]]; } && [[ $SUBBRUTE == true ]]; then\n\t\tstart_subfunc \"${FUNCNAME[0]}\" \"Running: Bruteforce Subdomain Enumeration\"\n\n\t\tif [[ $AXIOM != true ]]; then\n\t\t\tif ! resolvers_update_quick_local; then\n\t\t\t\tprintf \"%b[!] Failed to update resolvers.%b\\n\" \"$bred\" \"$reset\"\n\t\t\t\treturn 1\n\t\t\tfi\n\n\t\t\twordlist=\"$subs_wordlist\"\n\t\t\t[[ $DEEP == true ]] && wordlist=\"$subs_wordlist_big\"\n\n\t\t\t# Run puredns bruteforce\n\t\t\tpuredns bruteforce \"$wordlist\" \"$domain\" -w .tmp/subs_brute.txt -r \"$resolvers\" --resolvers-trusted \"$resolvers_trusted\" \\\n\t\t\t\t-l \"$PUREDNS_PUBLIC_LIMIT\" --rate-limit-trusted \"$PUREDNS_TRUSTED_LIMIT\" \\\n\t\t\t\t--wildcard-tests \"$PUREDNS_WILDCARDTEST_LIMIT\" --wildcard-batch \"$PUREDNS_WILDCARDBATCH_LIMIT\" \\\n\t\t\t\t2>>\"$LOGFILE\" >/dev/null\n\n\t\t\t# Resolve the subdomains\n\t\t\tif [[ -s \".tmp/subs_brute.txt\" ]]; then\n\t\t\t\tpuredns resolve .tmp/subs_brute.txt -w .tmp/subs_brute_valid.txt -r \"$resolvers\" --resolvers-trusted \"$resolvers_trusted\" \\\n\t\t\t\t\t-l \"$PUREDNS_PUBLIC_LIMIT\" --rate-limit-trusted \"$PUREDNS_TRUSTED_LIMIT\" \\\n\t\t\t\t\t--wildcard-tests \"$PUREDNS_WILDCARDTEST_LIMIT\" --wildcard-batch \"$PUREDNS_WILDCARDBATCH_LIMIT\" \\\n\t\t\t\t\t2>>\"$LOGFILE\" >/dev/null\n\t\t\tfi\n\n\t\telse\n\t\t\tif ! resolvers_update_quick_axiom; then\n\t\t\t\tprintf \"%b[!] Failed to update resolvers on axiom.%b\\n\" \"$bred\" \"$reset\"\n\t\t\t\treturn 1\n\t\t\tfi\n\n\t\t\twordlist=\"$subs_wordlist\"\n\t\t\t[[ $DEEP == true ]] && wordlist=\"$subs_wordlist_big\"\n\n\t\t\t# Run axiom-scan with puredns-single\n\t\t\taxiom-scan \"$wordlist\" -m puredns-single \"$domain\" -r /home/op/lists/resolvers.txt \\\n\t\t\t\t--resolvers-trusted /home/op/lists/resolvers_trusted.txt \\\n\t\t\t\t--wildcard-tests \"$PUREDNS_WILDCARDTEST_LIMIT\" --wildcard-batch \"$PUREDNS_WILDCARDBATCH_LIMIT\" \\\n\t\t\t\t-o .tmp/subs_brute.txt \"$AXIOM_EXTRA_ARGS\" 2>>\"$LOGFILE\" >/dev/null\n\n\t\t\t# Resolve the subdomains using axiom-scan\n\t\t\tif [[ -s \".tmp/subs_brute.txt\" ]]; then\n\t\t\t\taxiom-scan .tmp/subs_brute.txt -m puredns-resolve -r /home/op/lists/resolvers.txt \\\n\t\t\t\t\t--resolvers-trusted /home/op/lists/resolvers_trusted.txt \\\n\t\t\t\t\t--wildcard-tests \"$PUREDNS_WILDCARDTEST_LIMIT\" --wildcard-batch \"$PUREDNS_WILDCARDBATCH_LIMIT\" \\\n\t\t\t\t\t-o .tmp/subs_brute_valid.txt \"$AXIOM_EXTRA_ARGS\" 2>>\"$LOGFILE\" >/dev/null\n\t\t\tfi\n\t\tfi\n\n\t\t# Check inscope if INSCOPE is true\n\t\tif [[ $INSCOPE == true ]]; then\n\t\t\tif ! check_inscope .tmp/subs_brute_valid.txt 2>>\"$LOGFILE\" >/dev/null; then\n\t\t\t\tprintf \"%b[!] check_inscope command failed.%b\\n\" \"$bred\" \"$reset\"\n\t\t\t\treturn 1\n\t\t\tfi\n\t\tfi\n\n\t\t# Process subdomains and append new ones to subdomains.txt, count new lines\n\t\tif ! NUMOFLINES=$(grep \"\\.$domain$\\|^$domain$\" .tmp/subs_brute_valid.txt 2>>\"$LOGFILE\" |\n\t\t\tgrep -E '^([a-zA-Z0-9\\.\\-]+\\.)+[a-zA-Z]{1,}$' |\n\t\t\tsed 's/^\\*\\.//' | anew subdomains/subdomains.txt | sed '/^$/d' | wc -l); then\n\t\t\tprintf \"%b[!] Failed to process subdomains.%b\\n\" \"$bred\" \"$reset\"\n\t\t\treturn 1\n\t\tfi\n\n\t\tend_subfunc \"${NUMOFLINES} new subs (bruteforce)\" \"${FUNCNAME[0]}\"\n\n\telse\n\t\tif [[ $SUBBRUTE == false ]]; then\n\t\t\tprintf \"\\n%b[%s] %s skipped due to mode or defined in reconftw.cfg.%b\\n\" \\\n\t\t\t\t\"$yellow\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"${FUNCNAME[0]}\" \"$reset\"\n\t\telse\n\t\t\tprintf \"%b[%s] %s has already been processed. To force execution, delete:\\n    %s/.%s%b\\n\\n\" \\\n\t\t\t\t\"$yellow\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"${FUNCNAME[0]}\" \\\n\t\t\t\t\"$called_fn_dir\" \"/.${FUNCNAME[0]}\" \"$reset\"\n\t\tfi\n\tfi\n\n}\n\nfunction sub_scraping() {\n\n\t# Create necessary directories\n\tif ! mkdir -p .tmp subdomains; then\n\t\tprintf \"%b[!] Failed to create directories.%b\\n\" \"$bred\" \"$reset\"\n\t\treturn 1\n\tfi\n\n\t# Check if the function should run\n\tif { [[ ! -f \"$called_fn_dir/.${FUNCNAME[0]}\" ]] || [[ $DIFF == true ]]; } && [[ $SUBSCRAPING == true ]]; then\n\t\tstart_subfunc \"${FUNCNAME[0]}\" \"Running: Source code scraping subdomain search\"\n\n\t\t# Initialize scrap_subs.txt\n\t\tif ! touch .tmp/scrap_subs.txt; then\n\t\t\tprintf \"%b[!] Failed to create .tmp/scrap_subs.txt.%b\\n\" \"$bred\" \"$reset\"\n\t\t\treturn 1\n\t\tfi\n\n\t\t# If in multi mode and subdomains.txt doesn't exist, create it\n\t\tif [[ -n $multi ]] && [[ ! -f \"$dir/subdomains/subdomains.txt\" ]]; then\n\t\t\tif ! printf \"%b\\n\" \"$domain\" >\"$dir/subdomains/subdomains.txt\"; then\n\t\t\t\tprintf \"%b[!] Failed to create subdomains.txt.%b\\n\" \"$bred\" \"$reset\"\n\t\t\t\treturn 1\n\t\t\tfi\n\t\tfi\n\n\t\t# Check if subdomains.txt exists and is not empty\n\t\tif [[ -s \"$dir/subdomains/subdomains.txt\" ]]; then\n\n\t\t\tsubdomains_count=$(wc -l <\"$dir/subdomains/subdomains.txt\")\n\t\t\tif [[ $subdomains_count -le $DEEP_LIMIT ]] || [[ $DEEP == true ]]; then\n\n\t\t\t\tif [[ $AXIOM != true ]]; then\n\t\t\t\t\tif ! resolvers_update_quick_local; then\n\t\t\t\t\t\tprintf \"%b[!] Failed to update resolvers locally.%b\\n\" \"$bred\" \"$reset\"\n\t\t\t\t\t\treturn 1\n\t\t\t\t\tfi\n\n\t\t\t\t\t# Run httpx to gather web info\n\t\t\t\t\thttpx -follow-host-redirects -status-code -threads \"$HTTPX_THREADS\" -rl \"$HTTPX_RATELIMIT\" \\\n\t\t\t\t\t\t-timeout \"$HTTPX_TIMEOUT\" -silent -retries 2 -title -web-server -tech-detect -location \\\n\t\t\t\t\t\t-no-color -json -o .tmp/web_full_info1.txt \\\n\t\t\t\t\t\t<subdomains/subdomains.txt 2>>\"$LOGFILE\" >/dev/null\n\n\t\t\t\t\tif [[ -s \".tmp/web_full_info1.txt\" ]]; then\n\t\t\t\t\t\tcat .tmp/web_full_info1.txt | jq -r 'try .url' 2>/dev/null |\n\t\t\t\t\t\t\tgrep \"$domain\" |\n\t\t\t\t\t\t\tgrep -E '^((http|https):\\/\\/)?([a-zA-Z0-9\\-\\.]+\\.)+[a-zA-Z]{1,}(\\/.*)?$' |\n\t\t\t\t\t\t\tsed \"s/^\\*\\.//\" |\n\t\t\t\t\t\t\tanew .tmp/probed_tmp_scrap.txt |\n\t\t\t\t\t\t\tunfurl -u domains 2>>\"$LOGFILE\" |\n\t\t\t\t\t\t\tanew -q .tmp/scrap_subs.txt\n\t\t\t\t\tfi\n\n\t\t\t\t\tif [[ -s \".tmp/probed_tmp_scrap.txt\" ]]; then\n\t\t\t\t\t\ttimeout -k 1m 10m httpx -l .tmp/probed_tmp_scrap.txt -tls-grab -tls-probe -csp-probe \\\n\t\t\t\t\t\t\t-status-code -threads \"$HTTPX_THREADS\" -rl \"$HTTPX_RATELIMIT\" -timeout \"$HTTPX_TIMEOUT\" \\\n\t\t\t\t\t\t\t-silent -retries 2 -no-color -json -o .tmp/web_full_info2.txt \\\n\t\t\t\t\t\t\t2>>\"$LOGFILE\" >/dev/null\n\t\t\t\t\tfi\n\n\t\t\t\t\tif [[ -s \".tmp/web_full_info2.txt\" ]]; then\n\t\t\t\t\t\tcat .tmp/web_full_info2.txt | jq -r 'try .\"tls-grab\".\"dns_names\"[], try .csp.domains[], try .url' 2>/dev/null |\n\t\t\t\t\t\t\tgrep \"$domain\" |\n\t\t\t\t\t\t\tgrep -E '^((http|https):\\/\\/)?([a-zA-Z0-9\\-\\.]+\\.)+[a-zA-Z]{1,}(\\/.*)?$' |\n\t\t\t\t\t\t\tsed \"s/^\\*\\.//\" |\n\t\t\t\t\t\t\tsort -u |\n\t\t\t\t\t\t\thttpx -silent |\n\t\t\t\t\t\t\tanew .tmp/probed_tmp_scrap.txt |\n\t\t\t\t\t\t\tunfurl -u domains 2>>\"$LOGFILE\" |\n\t\t\t\t\t\t\tanew -q .tmp/scrap_subs.txt\n\t\t\t\t\tfi\n\n\t\t\t\t\tif [[ -s \".tmp/probed_tmp_scrap.txt\" ]]; then\n\t\t\t\t\t\tif [[ $DEEP == true ]]; then\n\t\t\t\t\t\t\tkatana_depth=3\n\t\t\t\t\t\telse\n\t\t\t\t\t\t\tkatana_depth=2\n\t\t\t\t\t\tfi\n\n\t\t\t\t\t\tkatana -silent -list .tmp/probed_tmp_scrap.txt -jc -kf all -c \"$KATANA_THREADS\" -d \"$katana_depth\" \\\n\t\t\t\t\t\t\t-fs rdn -o .tmp/katana.txt 2>>\"$LOGFILE\" >/dev/null\n\t\t\t\t\tfi\n\n\t\t\t\telse\n\t\t\t\t\t# AXIOM mode\n\t\t\t\t\tif ! resolvers_update_quick_axiom; then\n\t\t\t\t\t\tprintf \"%b[!] Failed to update resolvers on axiom.%b\\n\" \"$bred\" \"$reset\"\n\t\t\t\t\t\treturn 1\n\t\t\t\t\tfi\n\n\t\t\t\t\taxiom-scan subdomains/subdomains.txt -m httpx -follow-host-redirects -random-agent -status-code \\\n\t\t\t\t\t\t-threads \"$HTTPX_THREADS\" -rl \"$HTTPX_RATELIMIT\" -timeout \"$HTTPX_TIMEOUT\" -silent -retries 2 \\\n\t\t\t\t\t\t-title -web-server -tech-detect -location -no-color -json -o .tmp/web_full_info1.txt \\\n\t\t\t\t\t\t$AXIOM_EXTRA_ARGS 2>>\"$LOGFILE\" >/dev/null\n\n\t\t\t\t\tif [[ -s \".tmp/web_full_info1.txt\" ]]; then\n\t\t\t\t\t\tcat .tmp/web_full_info1.txt | jq -r 'try .url' 2>/dev/null |\n\t\t\t\t\t\t\tgrep \"$domain\" |\n\t\t\t\t\t\t\tgrep -E '^((http|https):\\/\\/)?([a-zA-Z0-9\\-\\.]+\\.)+[a-zA-Z]{1,}(\\/.*)?$' |\n\t\t\t\t\t\t\tsed \"s/^\\*\\.//\" |\n\t\t\t\t\t\t\tanew .tmp/probed_tmp_scrap.txt |\n\t\t\t\t\t\t\tunfurl -u domains 2>>\"$LOGFILE\" |\n\t\t\t\t\t\t\tanew -q .tmp/scrap_subs.txt\n\t\t\t\t\tfi\n\n\t\t\t\t\tif [[ -s \".tmp/probed_tmp_scrap.txt\" ]]; then\n\t\t\t\t\t\ttimeout -k 1m 10m axiom-scan .tmp/probed_tmp_scrap.txt -m httpx -tls-grab -tls-probe -csp-probe \\\n\t\t\t\t\t\t\t-random-agent -status-code -threads \"$HTTPX_THREADS\" -rl \"$HTTPX_RATELIMIT\" -timeout \"$HTTPX_TIMEOUT\" \\\n\t\t\t\t\t\t\t-silent -retries 2 -title -web-server -tech-detect -location -no-color -json -o .tmp/web_full_info2.txt \\\n\t\t\t\t\t\t\t$AXIOM_EXTRA_ARGS 2>>\"$LOGFILE\" >/dev/null\n\t\t\t\t\tfi\n\n\t\t\t\t\tif [[ -s \".tmp/web_full_info2.txt\" ]]; then\n\t\t\t\t\t\tcat .tmp/web_full_info2.txt | jq -r 'try .\"tls-grab\".\"dns_names\"[], try .csp.domains[], try .url' 2>/dev/null |\n\t\t\t\t\t\t\tgrep \"$domain\" |\n\t\t\t\t\t\t\tgrep -E '^((http|https):\\/\\/)?([a-zA-Z0-9\\-\\.]+\\.)+[a-zA-Z]{1,}(\\/.*)?$' |\n\t\t\t\t\t\t\tsed \"s/^\\*\\.//\" |\n\t\t\t\t\t\t\tsort -u |\n\t\t\t\t\t\t\thttpx -silent |\n\t\t\t\t\t\t\tanew .tmp/probed_tmp_scrap.txt |\n\t\t\t\t\t\t\tunfurl -u domains 2>>\"$LOGFILE\" |\n\t\t\t\t\t\t\tanew -q .tmp/scrap_subs.txt\n\t\t\t\t\tfi\n\n\t\t\t\t\tif [[ -s \".tmp/probed_tmp_scrap.txt\" ]]; then\n\t\t\t\t\t\tif [[ $DEEP == true ]]; then\n\t\t\t\t\t\t\tkatana_depth=3\n\t\t\t\t\t\telse\n\t\t\t\t\t\t\tkatana_depth=2\n\t\t\t\t\t\tfi\n\n\t\t\t\t\t\taxiom-scan .tmp/probed_tmp_scrap.txt -m katana -jc -kf all -d \"$katana_depth\" -fs rdn \\\n\t\t\t\t\t\t\t-o .tmp/katana.txt $AXIOM_EXTRA_ARGS 2>>\"$LOGFILE\" >/dev/null\n\t\t\t\t\tfi\n\t\t\t\tfi\n\n\t\t\t\tif [[ -s \".tmp/katana.txt\" ]]; then\n\t\t\t\t\tsed -i '/^.\\{2048\\}./d' .tmp/katana.txt\n\n\t\t\t\t\tcat .tmp/katana.txt | unfurl -u domains 2>>\"$LOGFILE\" |\n\t\t\t\t\t\tgrep \"\\.$domain$\" |\n\t\t\t\t\t\tgrep -E '^([a-zA-Z0-9\\.\\-]+\\.)+[a-zA-Z]{1,}$' |\n\t\t\t\t\t\tanew -q .tmp/scrap_subs.txt\n\t\t\t\tfi\n\n\t\t\t\tif [[ -s \".tmp/scrap_subs.txt\" ]]; then\n\t\t\t\t\tpuredns resolve .tmp/scrap_subs.txt -w .tmp/scrap_subs_resolved.txt -r \"$resolvers\" \\\n\t\t\t\t\t\t--resolvers-trusted \"$resolvers_trusted\" -l \"$PUREDNS_PUBLIC_LIMIT\" \\\n\t\t\t\t\t\t--rate-limit-trusted \"$PUREDNS_TRUSTED_LIMIT\" --wildcard-tests \"$PUREDNS_WILDCARDTEST_LIMIT\" \\\n\t\t\t\t\t\t--wildcard-batch \"$PUREDNS_WILDCARDBATCH_LIMIT\" 2>>\"$LOGFILE\" >/dev/null\n\t\t\t\tfi\n\n\t\t\t\tif [[ $INSCOPE == true ]]; then\n\t\t\t\t\tif ! check_inscope .tmp/scrap_subs_resolved.txt 2>>\"$LOGFILE\" >/dev/null; then\n\t\t\t\t\t\tprintf \"%b[!] check_inscope command failed.%b\\n\" \"$bred\" \"$reset\"\n\t\t\t\t\tfi\n\t\t\t\tfi\n\n\t\t\t\tif [[ -s \".tmp/scrap_subs_resolved.txt\" ]]; then\n\t\t\t\t\tif ! NUMOFLINES=$(cat .tmp/scrap_subs_resolved.txt 2>>\"$LOGFILE\" |\n\t\t\t\t\t\tgrep \"\\.$domain$\\|^$domain$\" |\n\t\t\t\t\t\tgrep -E '^([a-zA-Z0-9\\.\\-]+\\.)+[a-zA-Z]{1,}$' |\n\t\t\t\t\t\tanew subdomains/subdomains.txt |\n\t\t\t\t\t\ttee .tmp/diff_scrap.txt |\n\t\t\t\t\t\tsed '/^$/d' | wc -l); then\n\t\t\t\t\t\tprintf \"%b[!] Failed to count new subdomains.%b\\n\" \"$bred\" \"$reset\"\n\t\t\t\t\t\tNUMOFLINES=0\n\t\t\t\t\tfi\n\t\t\t\telse\n\t\t\t\t\tNUMOFLINES=0\n\t\t\t\tfi\n\n\t\t\t\tif [[ -s \".tmp/diff_scrap.txt\" ]]; then\n\t\t\t\t\thttpx -follow-host-redirects -random-agent -status-code -threads \"$HTTPX_THREADS\" \\\n\t\t\t\t\t\t-rl \"$HTTPX_RATELIMIT\" -timeout \"$HTTPX_TIMEOUT\" -silent -retries 2 -title -web-server \\\n\t\t\t\t\t\t-tech-detect -location -no-color -json -o .tmp/web_full_info3.txt \\\n\t\t\t\t\t\t<.tmp/diff_scrap.txt 2>>\"$LOGFILE\" >/dev/null\n\n\t\t\t\t\tif [[ -s \".tmp/web_full_info3.txt\" ]]; then\n\t\t\t\t\t\tcat .tmp/web_full_info3.txt | jq -r 'try .url' 2>/dev/null |\n\t\t\t\t\t\t\tgrep \"$domain\" |\n\t\t\t\t\t\t\tgrep -E '^((http|https):\\/\\/)?([a-zA-Z0-9\\-\\.]+\\.)+[a-zA-Z]{1,}(\\/.*)?$' |\n\t\t\t\t\t\t\tsed \"s/^\\*\\.//\" |\n\t\t\t\t\t\t\tanew .tmp/probed_tmp_scrap.txt |\n\t\t\t\t\t\t\tunfurl -u domains 2>>\"$LOGFILE\" |\n\t\t\t\t\t\t\tanew -q .tmp/scrap_subs.txt\n\t\t\t\t\tfi\n\t\t\t\tfi\n\n\t\t\t\tcat .tmp/web_full_info1.txt .tmp/web_full_info2.txt .tmp/web_full_info3.txt 2>>\"$LOGFILE\" |\n\t\t\t\t\tjq -s 'try .' | jq 'try unique_by(.input)' | jq 'try .[]' 2>>\"$LOGFILE\" >.tmp/web_full_info.txt\n\n\t\t\t\tend_subfunc \"${NUMOFLINES} new subs (code scraping)\" \"${FUNCNAME[0]}\"\n\n\t\t\telse\n\t\t\t\tend_subfunc \"Skipping Subdomains Web Scraping: Too Many Subdomains\" \"${FUNCNAME[0]}\"\n\t\t\tfi\n\t\tfi\n\n\telse\n\t\tif [[ $SUBSCRAPING == false ]]; then\n\t\t\tprintf \"\\n%b[%s] %s skipped due to mode or defined in reconftw.cfg.%b\\n\" \\\n\t\t\t\t\"$yellow\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"${FUNCNAME[0]}\" \"$reset\"\n\t\telse\n\t\t\tprintf \"%b[%s] %s has already been processed. To force execution, delete:\\n    %s/.%s%b\\n\\n\" \\\n\t\t\t\t\"$yellow\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"${FUNCNAME[0]}\" \\\n\t\t\t\t\"$called_fn_dir\" \"/.${FUNCNAME[0]}\" \"$reset\"\n\t\tfi\n\tfi\n\n}\n\nfunction sub_analytics() {\n\n\t# Create necessary directories\n\tif ! mkdir -p .tmp subdomains; then\n\t\tprintf \"%b[!] Failed to create directories.%b\\n\" \"$bred\" \"$reset\"\n\t\treturn 1\n\tfi\n\n\t# Check if the function should run\n\tif { [[ ! -f \"$called_fn_dir/.${FUNCNAME[0]}\" ]] || [[ $DIFF == true ]]; } && [[ $SUBANALYTICS == true ]]; then\n\t\tstart_subfunc \"${FUNCNAME[0]}\" \"Running: Analytics Subdomain Enumeration\"\n\n\t\tif [[ -s \".tmp/probed_tmp_scrap.txt\" ]]; then\n\t\t\t# Run analyticsrelationships and check for errors\n\t\t\tanalyticsrelationships -ch <.tmp/probed_tmp_scrap.txt >>.tmp/analytics_subs_tmp.txt 2>>\"$LOGFILE\"\n\n\t\t\tif [[ -s \".tmp/analytics_subs_tmp.txt\" ]]; then\n\t\t\t\tgrep \"\\.$domain$\\|^$domain$\" .tmp/analytics_subs_tmp.txt |\n\t\t\t\t\tgrep -E '^([a-zA-Z0-9\\.\\-]+\\.)+[a-zA-Z]{1,}$' |\n\t\t\t\t\tsed \"s/|__ //\" | anew -q .tmp/analytics_subs_clean.txt\n\n\t\t\t\tif [[ $AXIOM != true ]]; then\n\t\t\t\t\tif ! resolvers_update_quick_local; then\n\t\t\t\t\t\tprintf \"%b[!] Failed to update resolvers locally.%b\\n\" \"$bred\" \"$reset\"\n\t\t\t\t\t\treturn 1\n\t\t\t\t\tfi\n\n\t\t\t\t\tif [[ -s \".tmp/analytics_subs_clean.txt\" ]]; then\n\t\t\t\t\t\tpuredns resolve .tmp/analytics_subs_clean.txt -w .tmp/analytics_subs_resolved.txt \\\n\t\t\t\t\t\t\t-r \"$resolvers\" --resolvers-trusted \"$resolvers_trusted\" \\\n\t\t\t\t\t\t\t-l \"$PUREDNS_PUBLIC_LIMIT\" --rate-limit-trusted \"$PUREDNS_TRUSTED_LIMIT\" \\\n\t\t\t\t\t\t\t--wildcard-tests \"$PUREDNS_WILDCARDTEST_LIMIT\" --wildcard-batch \"$PUREDNS_WILDCARDBATCH_LIMIT\" \\\n\t\t\t\t\t\t\t2>>\"$LOGFILE\" >/dev/null\n\t\t\t\t\tfi\n\t\t\t\telse\n\t\t\t\t\tif ! resolvers_update_quick_axiom; then\n\t\t\t\t\t\tprintf \"%b[!] Failed to update resolvers on Axiom.%b\\n\" \"$bred\" \"$reset\"\n\t\t\t\t\t\treturn 1\n\t\t\t\t\tfi\n\n\t\t\t\t\tif [[ -s \".tmp/analytics_subs_clean.txt\" ]]; then\n\t\t\t\t\t\taxiom-scan .tmp/analytics_subs_clean.txt -m puredns-resolve \\\n\t\t\t\t\t\t\t-r /home/op/lists/resolvers.txt --resolvers-trusted /home/op/lists/resolvers_trusted.txt \\\n\t\t\t\t\t\t\t--wildcard-tests \"$PUREDNS_WILDCARDTEST_LIMIT\" --wildcard-batch \"$PUREDNS_WILDCARDBATCH_LIMIT\" \\\n\t\t\t\t\t\t\t-o .tmp/analytics_subs_resolved.txt $AXIOM_EXTRA_ARGS \\\n\t\t\t\t\t\t\t2>>\"$LOGFILE\" >/dev/null\n\t\t\t\t\tfi\n\t\t\t\tfi\n\t\t\tfi\n\t\tfi\n\n\t\tif [[ $INSCOPE == true ]]; then\n\t\t\tif ! check_inscope .tmp/analytics_subs_resolved.txt 2>>\"$LOGFILE\" >/dev/null; then\n\t\t\t\tprintf \"%b[!] check_inscope command failed.%b\\n\" \"$bred\" \"$reset\"\n\t\t\tfi\n\t\tfi\n\n\t\tif ! NUMOFLINES=$(anew subdomains/subdomains.txt 2>/dev/null <.tmp/analytics_subs_resolved.txt 2>/dev/null | sed '/^$/d' | wc -l); then\n\t\t\tprintf \"%b[!] Failed to count new subdomains.%b\\n\" \"$bred\" \"$reset\"\n\t\t\tNUMOFLINES=0\n\t\tfi\n\n\t\tend_subfunc \"${NUMOFLINES} new subs (analytics relationship)\" \"${FUNCNAME[0]}\"\n\n\telse\n\t\tif [[ $SUBANALYTICS == false ]]; then\n\t\t\tprintf \"\\n%b[%s] %s skipped due to mode or defined in reconftw.cfg.%b\\n\" \\\n\t\t\t\t\"$yellow\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"${FUNCNAME[0]}\" \"$reset\"\n\t\telse\n\t\t\tprintf \"%b[%s] %s has already been processed. To force execution, delete:\\n    %s/.%s%b\\n\\n\" \\\n\t\t\t\t\"$yellow\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"${FUNCNAME[0]}\" \"$called_fn_dir\" \"/.${FUNCNAME[0]}\" \"$reset\"\n\t\tfi\n\tfi\n}\n\nfunction sub_permut() {\n\n\tmkdir -p .tmp subdomains\n\n\tif { [[ ! -f \"$called_fn_dir/.${FUNCNAME[0]}\" ]] || [[ $DIFF == true ]]; } && [[ $SUBPERMUTE == true ]]; then\n\t\tstart_subfunc \"${FUNCNAME[0]}\" \"Running: Permutations Subdomain Enumeration\"\n\n\t\t# If in multi mode and subdomains.txt doesn't exist, create it with the domain\n\t\tif [[ -n $multi ]] && [[ ! -f \"$dir/subdomains/subdomains.txt\" ]]; then\n\t\t\techo \"$domain\" >\"$dir/subdomains/subdomains.txt\"\n\t\tfi\n\n\t\t# Determine the number of subdomains\n\t\tsubdomain_count=$(wc -l <subdomains/subdomains.txt)\n\n\t\t# Check if DEEP mode is enabled or subdomains are within DEEP_LIMIT\n\t\tif [[ $DEEP == true ]] || [[ $subdomain_count -le $DEEP_LIMIT ]]; then\n\n\t\t\t# Select the permutations tool\n\t\t\tif [[ $PERMUTATIONS_OPTION == \"gotator\" ]]; then\n\t\t\t\tif [[ -s \"subdomains/subdomains.txt\" ]]; then\n\t\t\t\t\tgotator -sub subdomains/subdomains.txt -perm \"${tools}/permutations_list.txt\" $GOTATOR_FLAGS \\\n\t\t\t\t\t\t-silent 2>>\"$LOGFILE\" | head -c \"$PERMUTATIONS_LIMIT\" >.tmp/gotator1.txt\n\t\t\t\tfi\n\t\t\telse\n\t\t\t\tif [[ -s \"subdomains/subdomains.txt\" ]]; then\n\t\t\t\t\tripgen -d subdomains/subdomains.txt -w \"${tools}/permutations_list.txt\" \\\n\t\t\t\t\t\t2>>\"$LOGFILE\" | head -c \"$PERMUTATIONS_LIMIT\" >.tmp/gotator1.txt\n\t\t\t\tfi\n\t\t\tfi\n\n\t\telif [[ \"$(wc -l <.tmp/subs_no_resolved.txt)\" -le $DEEP_LIMIT2 ]]; then\n\n\t\t\tif [[ $PERMUTATIONS_OPTION == \"gotator\" ]]; then\n\t\t\t\tif [[ -s \".tmp/subs_no_resolved.txt\" ]]; then\n\t\t\t\t\tgotator -sub .tmp/subs_no_resolved.txt -perm \"${tools}/permutations_list.txt\" $GOTATOR_FLAGS \\\n\t\t\t\t\t\t-silent 2>>\"$LOGFILE\" | head -c \"$PERMUTATIONS_LIMIT\" >.tmp/gotator1.txt\n\t\t\t\tfi\n\t\t\telse\n\t\t\t\tif [[ -s \".tmp/subs_no_resolved.txt\" ]]; then\n\t\t\t\t\tripgen -d .tmp/subs_no_resolved.txt -w \"${tools}/permutations_list.txt\" \\\n\t\t\t\t\t\t2>>\"$LOGFILE\" | head -c \"$PERMUTATIONS_LIMIT\" >.tmp/gotator1.txt\n\t\t\t\tfi\n\t\t\tfi\n\n\t\telse\n\t\t\tend_subfunc \"Skipping Permutations: Too Many Subdomains\" \"${FUNCNAME[0]}\"\n\t\t\treturn 0\n\t\tfi\n\n\t\t# Resolve the permutations\n\t\tif [[ $AXIOM != true ]]; then\n\t\t\tif ! resolvers_update_quick_local; then\n\t\t\t\tprintf \"%b[!] Failed to update resolvers.%b\\n\" \"$bred\" \"$reset\"\n\t\t\t\treturn 1\n\t\t\tfi\n\t\t\tif [[ -s \".tmp/gotator1.txt\" ]]; then\n\t\t\t\tpuredns resolve .tmp/gotator1.txt -w .tmp/permute1.txt -r \"$resolvers\" --resolvers-trusted \"$resolvers_trusted\" \\\n\t\t\t\t\t-l \"$PUREDNS_PUBLIC_LIMIT\" --rate-limit-trusted \"$PUREDNS_TRUSTED_LIMIT\" \\\n\t\t\t\t\t--wildcard-tests \"$PUREDNS_WILDCARDTEST_LIMIT\" --wildcard-batch \"$PUREDNS_WILDCARDBATCH_LIMIT\" \\\n\t\t\t\t\t2>>\"$LOGFILE\" >/dev/null\n\t\t\tfi\n\t\telse\n\t\t\tif ! resolvers_update_quick_axiom; then\n\t\t\t\tprintf \"%b[!] Failed to update resolvers on axiom.%b\\n\" \"$bred\" \"$reset\"\n\t\t\t\treturn 1\n\t\t\tfi\n\t\t\tif [[ -s \".tmp/gotator1.txt\" ]]; then\n\t\t\t\taxiom-scan .tmp/gotator1.txt -m puredns-resolve -r /home/op/lists/resolvers.txt \\\n\t\t\t\t\t--resolvers-trusted /home/op/lists/resolvers_trusted.txt \\\n\t\t\t\t\t--wildcard-tests \"$PUREDNS_WILDCARDTEST_LIMIT\" --wildcard-batch \"$PUREDNS_WILDCARDBATCH_LIMIT\" \\\n\t\t\t\t\t-o .tmp/permute1.txt $AXIOM_EXTRA_ARGS 2>>\"$LOGFILE\" >/dev/null\n\t\t\tfi\n\t\tfi\n\n\t\t# Generate second round of permutations\n\t\tif [[ $PERMUTATIONS_OPTION == \"gotator\" ]]; then\n\t\t\tif [[ -s \".tmp/permute1.txt\" ]]; then\n\t\t\t\tgotator -sub .tmp/permute1.txt -perm \"${tools}/permutations_list.txt\" \\\n\t\t\t\t\t$GOTATOR_FLAGS -silent 2>>\"$LOGFILE\" | head -c \"$PERMUTATIONS_LIMIT\" >.tmp/gotator2.txt\n\t\t\tfi\n\t\telse\n\t\t\tif [[ -s \".tmp/permute1.txt\" ]]; then\n\t\t\t\tripgen -d .tmp/permute1.txt -w \"${tools}/permutations_list.txt\" \\\n\t\t\t\t\t2>>\"$LOGFILE\" | head -c \"$PERMUTATIONS_LIMIT\" >.tmp/gotator2.txt\n\t\t\tfi\n\t\tfi\n\n\t\t# Resolve the second round of permutations\n\t\tif [[ $AXIOM != true ]]; then\n\t\t\tif [[ -s \".tmp/gotator2.txt\" ]]; then\n\t\t\t\tpuredns resolve .tmp/gotator2.txt -w .tmp/permute2.txt -r \"$resolvers\" --resolvers-trusted \"$resolvers_trusted\" \\\n\t\t\t\t\t-l \"$PUREDNS_PUBLIC_LIMIT\" --rate-limit-trusted \"$PUREDNS_TRUSTED_LIMIT\" \\\n\t\t\t\t\t--wildcard-tests \"$PUREDNS_WILDCARDTEST_LIMIT\" --wildcard-batch \"$PUREDNS_WILDCARDBATCH_LIMIT\" \\\n\t\t\t\t\t2>>\"$LOGFILE\" >/dev/null\n\t\t\tfi\n\t\telse\n\t\t\tif [[ -s \".tmp/gotator2.txt\" ]]; then\n\t\t\t\taxiom-scan .tmp/gotator2.txt -m puredns-resolve -r /home/op/lists/resolvers.txt \\\n\t\t\t\t\t--resolvers-trusted /home/op/lists/resolvers_trusted.txt \\\n\t\t\t\t\t--wildcard-tests \"$PUREDNS_WILDCARDTEST_LIMIT\" --wildcard-batch \"$PUREDNS_WILDCARDBATCH_LIMIT\" \\\n\t\t\t\t\t-o .tmp/permute2.txt $AXIOM_EXTRA_ARGS 2>>\"$LOGFILE\" >/dev/null\n\t\t\tfi\n\t\tfi\n\n\t\t# Combine results\n\t\tif [[ -s \".tmp/permute1.txt\" ]] || [[ -s \".tmp/permute2.txt\" ]]; then\n\t\t\tcat .tmp/permute1.txt .tmp/permute2.txt 2>>\"$LOGFILE\" | anew -q .tmp/permute_subs.txt\n\n\t\t\t# Remove out-of-scope domains if applicable\n\t\t\tif [[ -s $outOfScope_file ]]; then\n\t\t\t\tif ! deleteOutScoped \"$outOfScope_file\" .tmp/permute_subs.txt; then\n\t\t\t\t\tprintf \"%b[!] deleteOutScoped command failed.%b\\n\" \"$bred\" \"$reset\"\n\t\t\t\tfi\n\t\t\tfi\n\n\t\t\t# Check inscope if INSCOPE is true\n\t\t\tif [[ $INSCOPE == true ]]; then\n\t\t\t\tif ! check_inscope .tmp/permute_subs.txt 2>>\"$LOGFILE\" >/dev/null; then\n\t\t\t\t\tprintf \"%b[!] check_inscope command failed.%b\\n\" \"$bred\" \"$reset\"\n\t\t\t\tfi\n\t\t\tfi\n\n\t\t\t# Process subdomains and append new ones to subdomains.txt, count new lines\n\t\t\tif ! NUMOFLINES=$(grep \"\\.$domain$\\|^$domain$\" .tmp/permute_subs.txt 2>>\"$LOGFILE\" |\n\t\t\t\tgrep -E '^([a-zA-Z0-9\\.\\-]+\\.)+[a-zA-Z]{1,}$' |\n\t\t\t\tanew subdomains/subdomains.txt | sed '/^$/d' | wc -l); then\n\t\t\t\tprintf \"%b[!] Failed to process subdomains.%b\\n\" \"$bred\" \"$reset\"\n\t\t\t\treturn 1\n\t\t\tfi\n\t\telse\n\t\t\tNUMOFLINES=0\n\t\tfi\n\n\t\tend_subfunc \"${NUMOFLINES} new subs (permutations)\" \"${FUNCNAME[0]}\"\n\n\telse\n\t\tif [[ $SUBPERMUTE == false ]]; then\n\t\t\tprintf \"\\n%b[%s] %s skipped due to mode or defined in reconftw.cfg.%b\\n\" \\\n\t\t\t\t\"$yellow\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"${FUNCNAME[0]}\" \"$reset\"\n\t\telse\n\t\t\tprintf \"%b[%s] %s has already been processed. To force execution, delete:\\n    %s/.%s%b\\n\\n\" \\\n\t\t\t\t\"$yellow\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"${FUNCNAME[0]}\" \\\n\t\t\t\t\"$called_fn_dir\" \"/.${FUNCNAME[0]}\" \"$reset\"\n\t\tfi\n\tfi\n\n}\n\nfunction sub_regex_permut() {\n\n\t# Create necessary directories\n\tif ! mkdir -p .tmp subdomains; then\n\t\tprintf \"%b[!] Failed to create directories.%b\\n\" \"$bred\" \"$reset\"\n\t\treturn 1\n\tfi\n\n\t# Check if the function should run\n\tif { [[ ! -f \"$called_fn_dir/.${FUNCNAME[0]}\" ]] || [[ $DIFF == true ]]; } && [[ $SUBREGEXPERMUTE == true ]]; then\n\t\tstart_subfunc \"${FUNCNAME[0]}\" \"Running: Permutations by regex analysis\"\n\n\t\t# Change to the regulator directory\n\t\tif ! pushd \"${tools}/regulator\" >/dev/null; then\n\t\t\tprintf \"%b[!] Failed to change directory to %s.%b\\n\" \"$bred\" \"${tools}/regulator\" \"$reset\"\n\t\t\treturn 1\n\t\tfi\n\n\t\t# If in multi mode and subdomains.txt doesn't exist, create it\n\t\tif [[ -n $multi ]] && [[ ! -f \"$dir/subdomains/subdomains.txt\" ]]; then\n\t\t\tprintf \"%b\\n\" \"$domain\" >\"$dir/subdomains/subdomains.txt\"\n\t\tfi\n\n\t\t# Run the main.py script\n\t\tpython3 main.py -t \"$domain\" -f \"${dir}/subdomains/subdomains.txt\" -o \"${dir}/.tmp/${domain}.brute\" \\\n\t\t\t2>>\"$LOGFILE\" >/dev/null\n\n\t\t# Return to the previous directory\n\t\tif ! popd >/dev/null; then\n\t\t\tprintf \"%b[!] Failed to return to previous directory.%b\\n\" \"$bred\" \"$reset\"\n\t\t\treturn 1\n\t\tfi\n\n\t\t# Resolve the generated domains\n\t\tif [[ $AXIOM != true ]]; then\n\t\t\tif ! resolvers_update_quick_local; then\n\t\t\t\tprintf \"%b[!] Failed to update resolvers locally.%b\\n\" \"$bred\" \"$reset\"\n\t\t\t\treturn 1\n\t\t\tfi\n\n\t\t\tif [[ -s \".tmp/${domain}.brute\" ]]; then\n\t\t\t\tpuredns resolve \".tmp/${domain}.brute\" -w .tmp/regulator.txt -r \"$resolvers\" --resolvers-trusted \"$resolvers_trusted\" \\\n\t\t\t\t\t-l \"$PUREDNS_PUBLIC_LIMIT\" --rate-limit-trusted \"$PUREDNS_TRUSTED_LIMIT\" \\\n\t\t\t\t\t--wildcard-tests \"$PUREDNS_WILDCARDTEST_LIMIT\" --wildcard-batch \"$PUREDNS_WILDCARDBATCH_LIMIT\" \\\n\t\t\t\t\t2>>\"$LOGFILE\" >/dev/null\n\t\t\tfi\n\t\telse\n\t\t\tif ! resolvers_update_quick_axiom; then\n\t\t\t\tprintf \"%b[!] Failed to update resolvers on Axiom.%b\\n\" \"$bred\" \"$reset\"\n\t\t\t\treturn 1\n\t\t\tfi\n\n\t\t\tif [[ -s \".tmp/${domain}.brute\" ]]; then\n\t\t\t\taxiom-scan \".tmp/${domain}.brute\" -m puredns-resolve \\\n\t\t\t\t\t-r /home/op/lists/resolvers.txt --resolvers-trusted /home/op/lists/resolvers_trusted.txt \\\n\t\t\t\t\t--wildcard-tests \"$PUREDNS_WILDCARDTEST_LIMIT\" --wildcard-batch \"$PUREDNS_WILDCARDBATCH_LIMIT\" \\\n\t\t\t\t\t-o .tmp/regulator.txt $AXIOM_EXTRA_ARGS \\\n\t\t\t\t\t2>>\"$LOGFILE\" >/dev/null\n\t\t\tfi\n\t\tfi\n\n\t\t# Process the resolved domains\n\t\tif [[ -s \".tmp/regulator.txt\" ]]; then\n\t\t\tif [[ -s $outOfScope_file ]]; then\n\t\t\t\tif ! deleteOutScoped \"$outOfScope_file\" .tmp/regulator.txt; then\n\t\t\t\t\tprintf \"%b[!] deleteOutScoped command failed.%b\\n\" \"$bred\" \"$reset\"\n\t\t\t\tfi\n\t\t\tfi\n\n\t\t\tif [[ $INSCOPE == true ]]; then\n\t\t\t\tif ! check_inscope .tmp/regulator.txt 2>>\"$LOGFILE\" >/dev/null; then\n\t\t\t\t\tprintf \"%b[!] check_inscope command failed.%b\\n\" \"$bred\" \"$reset\"\n\t\t\t\tfi\n\t\t\tfi\n\n\t\t\tif ! NUMOFLINES=$(grep \"\\.$domain$\\|^$domain$\" .tmp/regulator.txt 2>>\"$LOGFILE\" |\n\t\t\t\tgrep -E '^([a-zA-Z0-9\\.\\-]+\\.)+[a-zA-Z]{1,}$' |\n\t\t\t\tanew subdomains/subdomains.txt |\n\t\t\t\tsed '/^$/d' |\n\t\t\t\twc -l); then\n\t\t\t\tprintf \"%b[!] Failed to count new subdomains.%b\\n\" \"$bred\" \"$reset\"\n\t\t\t\tNUMOFLINES=0\n\t\t\tfi\n\t\telse\n\t\t\tNUMOFLINES=0\n\t\tfi\n\n\t\tend_subfunc \"${NUMOFLINES} new subs (permutations by regex)\" \"${FUNCNAME[0]}\"\n\n\telse\n\t\tif [[ $SUBREGEXPERMUTE == false ]]; then\n\t\t\tprintf \"\\n%b[%s] %s skipped due to mode or defined in reconftw.cfg.%b\\n\" \\\n\t\t\t\t\"$yellow\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"${FUNCNAME[0]}\" \"$reset\"\n\t\telse\n\t\t\tprintf \"%b[%s] %s has already been processed. To force execution, delete:\\n    %s/.%s%b\\n\\n\" \\\n\t\t\t\t\"$yellow\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"${FUNCNAME[0]}\" \"$called_fn_dir\" \"/.${FUNCNAME[0]}\" \"$reset\"\n\t\tfi\n\tfi\n\n}\n\nfunction sub_recursive_passive() {\n\n\t# Create necessary directories\n\tif ! mkdir -p .tmp subdomains; then\n\t\tprintf \"%b[!] Failed to create directories.%b\\n\" \"$bred\" \"$reset\"\n\t\treturn 1\n\tfi\n\n\t# Check if the function should run\n\tif { [[ ! -f \"$called_fn_dir/.${FUNCNAME[0]}\" ]] || [[ $DIFF == true ]]; } && [[ $SUB_RECURSIVE_PASSIVE == true ]] && [[ -s \"subdomains/subdomains.txt\" ]]; then\n\t\tstart_subfunc \"${FUNCNAME[0]}\" \"Running: Subdomains recursive search passive\"\n\n\t\t# If in multi mode and subdomains.txt doesn't exist, create it with the domain\n\t\tif [[ -n $multi ]] && [[ ! -f \"$dir/subdomains/subdomains.txt\" ]]; then\n\t\t\tprintf \"%b\\n\" \"$domain\" >\"$dir/subdomains/subdomains.txt\"\n\t\tfi\n\n\t\t# Passive recursive\n\t\tif [[ -s \"subdomains/subdomains.txt\" ]]; then\n\t\t\tdsieve -if subdomains/subdomains.txt -f 3 -top \"$DEEP_RECURSIVE_PASSIVE\" >.tmp/subdomains_recurs_top.txt\n\t\tfi\n\n\t\tif [[ $AXIOM != true ]]; then\n\t\t\tif ! resolvers_update_quick_local; then\n\t\t\t\tprintf \"%b[!] Failed to update resolvers locally.%b\\n\" \"$bred\" \"$reset\"\n\t\t\t\treturn 1\n\t\t\tfi\n\n\t\t\tif [[ -s \".tmp/subdomains_recurs_top.txt\" ]]; then\n\t\t\t\tsubfinder -all -dL .tmp/subdomains_recurs_top.txt -max-time \"${SUBFINDER_ENUM_TIMEOUT}\" \\\n\t\t\t\t\t-silent -o .tmp/passive_recursive_tmp.txt 2>>\"$LOGFILE\"\n\t\t\telse\n\t\t\t\treturn 1\n\t\t\tfi\n\n\t\t\tif [[ -s \".tmp/passive_recursive_tmp.txt\" ]]; then\n\t\t\t\tcat .tmp/passive_recursive_tmp.txt | anew -q .tmp/passive_recursive.txt\n\t\t\tfi\n\n\t\t\tif [[ -s \".tmp/passive_recursive.txt\" ]]; then\n\t\t\t\tpuredns resolve .tmp/passive_recursive.txt -w .tmp/passive_recurs_tmp.txt -r \"$resolvers\" --resolvers-trusted \"$resolvers_trusted\" \\\n\t\t\t\t\t-l \"$PUREDNS_PUBLIC_LIMIT\" --rate-limit-trusted \"$PUREDNS_TRUSTED_LIMIT\" \\\n\t\t\t\t\t--wildcard-tests \"$PUREDNS_WILDCARDTEST_LIMIT\" --wildcard-batch \"$PUREDNS_WILDCARDBATCH_LIMIT\" \\\n\t\t\t\t\t2>>\"$LOGFILE\" >/dev/null\n\t\t\tfi\n\n\t\telse\n\t\t\tif ! resolvers_update_quick_axiom; then\n\t\t\t\tprintf \"%b[!] Failed to update resolvers on Axiom.%b\\n\" \"$bred\" \"$reset\"\n\t\t\t\treturn 1\n\t\t\tfi\n\n\t\t\tif [[ -s \".tmp/subdomains_recurs_top.txt\" ]]; then\n\t\t\t\taxiom-scan .tmp/subdomains_recurs_top.txt -m subfinder -all -o .tmp/subfinder_prec.txt \"$AXIOM_EXTRA_ARGS\" 2>>\"$LOGFILE\" >/dev/null\n\t\t\telse\n\t\t\t\treturn 1\n\t\t\tfi\n\n\t\t\tif [[ -s \".tmp/subfinder_prec.txt\" ]]; then\n\t\t\t\tcat .tmp/subfinder_prec.txt | anew -q .tmp/passive_recursive.txt\n\t\t\tfi\n\n\t\t\tif [[ -s \".tmp/passive_recursive.txt\" ]]; then\n\t\t\t\taxiom-scan .tmp/passive_recursive.txt -m puredns-resolve \\\n\t\t\t\t\t-r /home/op/lists/resolvers.txt --resolvers-trusted /home/op/lists/resolvers_trusted.txt \\\n\t\t\t\t\t--wildcard-tests \"$PUREDNS_WILDCARDTEST_LIMIT\" --wildcard-batch \"$PUREDNS_WILDCARDBATCH_LIMIT\" \\\n\t\t\t\t\t-o .tmp/passive_recurs_tmp.txt \"$AXIOM_EXTRA_ARGS\" 2>>\"$LOGFILE\" >/dev/null\n\t\t\tfi\n\t\tfi\n\n\t\tif [[ $INSCOPE == true ]]; then\n\t\t\tif ! check_inscope .tmp/passive_recurs_tmp.txt 2>>\"$LOGFILE\" >/dev/null; then\n\t\t\t\tprintf \"%b[!] check_inscope command failed.%b\\n\" \"$bred\" \"$reset\"\n\t\t\tfi\n\t\tfi\n\n\t\tif [[ -s \".tmp/passive_recurs_tmp.txt\" ]]; then\n\t\t\tif ! NUMOFLINES=$(grep \"\\.$domain$\\|^$domain$\" .tmp/passive_recurs_tmp.txt 2>>\"$LOGFILE\" |\n\t\t\t\tgrep -E '^([a-zA-Z0-9\\.\\-]+\\.)+[a-zA-Z]{1,}$' |\n\t\t\t\tsed '/^$/d' |\n\t\t\t\tanew subdomains/subdomains.txt |\n\t\t\t\twc -l); then\n\t\t\t\tprintf \"%b[!] Failed to count new subdomains.%b\\n\" \"$bred\" \"$reset\"\n\t\t\t\tNUMOFLINES=0\n\t\t\tfi\n\t\telse\n\t\t\tNUMOFLINES=0\n\t\tfi\n\n\t\tend_subfunc \"${NUMOFLINES} new subs (recursive)\" \"${FUNCNAME[0]}\"\n\n\telse\n\t\tif [[ $SUB_RECURSIVE_PASSIVE == false ]]; then\n\t\t\tprintf \"\\n%b[%s] %s skipped due to mode or defined in reconftw.cfg.%b\\n\" \\\n\t\t\t\t\"$yellow\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"${FUNCNAME[0]}\" \"$reset\"\n\t\telif [[ ! -s \"subdomains/subdomains.txt\" ]]; then\n\t\t\tprintf \"\\n%s[%s] No subdomains to process.%b\\n\" \\\n\t\t\t\t\"$yellow\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"$reset\"\n\t\telse\n\t\t\tprintf \"%b[%s] %s has already been processed. To force execution, delete:\\n    %s/.%s%b\\n\\n\" \\\n\t\t\t\t\"$yellow\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"${FUNCNAME[0]}\" \"$called_fn_dir\" \"/.${FUNCNAME[0]}\" \"$reset\"\n\t\tfi\n\tfi\n\n}\n\nfunction sub_recursive_brute() {\n\t# Create necessary directories\n\tif ! mkdir -p .tmp subdomains; then\n\t\tprintf \"%b[!] Failed to create directories.%b\\n\" \"$bred\" \"$reset\"\n\t\treturn 1\n\tfi\n\n\t# Check if the function should run\n\tif { [[ ! -f \"$called_fn_dir/.${FUNCNAME[0]}\" ]] || [[ $DIFF == true ]]; } && [[ $SUB_RECURSIVE_BRUTE == true ]] && [[ -s \"subdomains/subdomains.txt\" ]]; then\n\t\tstart_subfunc \"${FUNCNAME[0]}\" \"Running: Subdomains recursive search active\"\n\n\t\t# If in multi mode and subdomains.txt doesn't exist, create it with the domain\n\t\tif [[ -n $multi ]] && [[ ! -f \"$dir/subdomains/subdomains.txt\" ]]; then\n\t\t\techo \"$domain\" >\"$dir/subdomains/subdomains.txt\"\n\t\tfi\n\n\t\t# Check the number of subdomains\n\t\tsubdomain_count=$(wc -l <subdomains/subdomains.txt)\n\t\tif [[ $subdomain_count -le $DEEP_LIMIT ]]; then\n\t\t\t# Generate top subdomains if not already done\n\t\t\tif [[ ! -s \".tmp/subdomains_recurs_top.txt\" ]]; then\n\t\t\t\tdsieve -if subdomains/subdomains.txt -f 3 -top \"$DEEP_RECURSIVE_PASSIVE\" >.tmp/subdomains_recurs_top.txt\n\t\t\tfi\n\n\t\t\t# Generate brute recursive wordlist\n\t\t\tripgen -d .tmp/subdomains_recurs_top.txt -w \"$subs_wordlist\" >.tmp/brute_recursive_wordlist.txt\n\n\t\t\tif [[ $AXIOM != true ]]; then\n\t\t\t\tif ! resolvers_update_quick_local; then\n\t\t\t\t\tprintf \"%b[!] Failed to update resolvers locally.%b\\n\" \"$bred\" \"$reset\"\n\t\t\t\t\treturn 1\n\t\t\t\tfi\n\n\t\t\t\tif [[ -s \".tmp/brute_recursive_wordlist.txt\" ]]; then\n\t\t\t\t\tpuredns resolve .tmp/brute_recursive_wordlist.txt -r \"$resolvers\" --resolvers-trusted \"$resolvers_trusted\" \\\n\t\t\t\t\t\t-l \"$PUREDNS_PUBLIC_LIMIT\" --rate-limit-trusted \"$PUREDNS_TRUSTED_LIMIT\" \\\n\t\t\t\t\t\t--wildcard-tests \"$PUREDNS_WILDCARDTEST_LIMIT\" --wildcard-batch \"$PUREDNS_WILDCARDBATCH_LIMIT\" \\\n\t\t\t\t\t\t-w .tmp/brute_recursive_result.txt 2>>\"$LOGFILE\" >/dev/null\n\t\t\t\tfi\n\t\t\telse\n\t\t\t\tif ! resolvers_update_quick_axiom; then\n\t\t\t\t\tprintf \"%b[!] Failed to update resolvers on axiom.%b\\n\" \"$bred\" \"$reset\"\n\t\t\t\t\treturn 1\n\t\t\t\tfi\n\n\t\t\t\tif [[ -s \".tmp/brute_recursive_wordlist.txt\" ]]; then\n\t\t\t\t\taxiom-scan .tmp/brute_recursive_wordlist.txt -m puredns-resolve \\\n\t\t\t\t\t\t-r /home/op/lists/resolvers.txt --resolvers-trusted /home/op/lists/resolvers_trusted.txt \\\n\t\t\t\t\t\t--wildcard-tests \"$PUREDNS_WILDCARDTEST_LIMIT\" --wildcard-batch \"$PUREDNS_WILDCARDBATCH_LIMIT\" \\\n\t\t\t\t\t\t-o .tmp/brute_recursive_result.txt $AXIOM_EXTRA_ARGS \\\n\t\t\t\t\t\t2>>\"$LOGFILE\" >/dev/null\n\t\t\t\tfi\n\t\t\tfi\n\n\t\t\tif [[ -s \".tmp/brute_recursive_result.txt\" ]]; then\n\t\t\t\tcat .tmp/brute_recursive_result.txt | anew -q .tmp/brute_recursive.txt\n\t\t\tfi\n\n\t\t\t# Generate permutations\n\t\t\tif [[ $PERMUTATIONS_OPTION == \"gotator\" ]]; then\n\t\t\t\tif [[ -s \".tmp/brute_recursive.txt\" ]]; then\n\t\t\t\t\tgotator -sub .tmp/brute_recursive.txt -perm \"${tools}/permutations_list.txt\" $GOTATOR_FLAGS -silent \\\n\t\t\t\t\t\t2>>\"$LOGFILE\" | head -c \"$PERMUTATIONS_LIMIT\" >.tmp/gotator1_recursive.txt\n\t\t\t\tfi\n\t\t\telse\n\t\t\t\tif [[ -s \".tmp/brute_recursive.txt\" ]]; then\n\t\t\t\t\tripgen -d .tmp/brute_recursive.txt -w \"${tools}/permutations_list.txt\" \\\n\t\t\t\t\t\t2>>\"$LOGFILE\" | head -c \"$PERMUTATIONS_LIMIT\" >.tmp/gotator1_recursive.txt\n\t\t\t\tfi\n\t\t\tfi\n\n\t\t\t# Resolve permutations\n\t\t\tif [[ $AXIOM != true ]]; then\n\t\t\t\tif [[ -s \".tmp/gotator1_recursive.txt\" ]]; then\n\t\t\t\t\tpuredns resolve .tmp/gotator1_recursive.txt -w .tmp/permute1_recursive.txt \\\n\t\t\t\t\t\t-r \"$resolvers\" --resolvers-trusted \"$resolvers_trusted\" \\\n\t\t\t\t\t\t-l \"$PUREDNS_PUBLIC_LIMIT\" --rate-limit-trusted \"$PUREDNS_TRUSTED_LIMIT\" \\\n\t\t\t\t\t\t--wildcard-tests \"$PUREDNS_WILDCARDTEST_LIMIT\" --wildcard-batch \"$PUREDNS_WILDCARDBATCH_LIMIT\" \\\n\t\t\t\t\t\t2>>\"$LOGFILE\" >/dev/null\n\t\t\t\tfi\n\t\t\telse\n\t\t\t\tif [[ -s \".tmp/gotator1_recursive.txt\" ]]; then\n\t\t\t\t\taxiom-scan .tmp/gotator1_recursive.txt -m puredns-resolve \\\n\t\t\t\t\t\t-r /home/op/lists/resolvers.txt --resolvers-trusted /home/op/lists/resolvers_trusted.txt \\\n\t\t\t\t\t\t--wildcard-tests \"$PUREDNS_WILDCARDTEST_LIMIT\" --wildcard-batch \"$PUREDNS_WILDCARDBATCH_LIMIT\" \\\n\t\t\t\t\t\t-o .tmp/permute1_recursive.txt $AXIOM_EXTRA_ARGS \\\n\t\t\t\t\t\t2>>\"$LOGFILE\" >/dev/null\n\t\t\t\tfi\n\t\t\tfi\n\n\t\t\t# Second round of permutations\n\t\t\tif [[ $PERMUTATIONS_OPTION == \"gotator\" ]]; then\n\t\t\t\tif [[ -s \".tmp/permute1_recursive.txt\" ]]; then\n\t\t\t\t\tgotator -sub .tmp/permute1_recursive.txt -perm \"${tools}/permutations_list.txt\" $GOTATOR_FLAGS -silent \\\n\t\t\t\t\t\t2>>\"$LOGFILE\" | head -c \"$PERMUTATIONS_LIMIT\" >.tmp/gotator2_recursive.txt\n\t\t\t\tfi\n\t\t\telse\n\t\t\t\tif [[ -s \".tmp/permute1_recursive.txt\" ]]; then\n\t\t\t\t\tripgen -d .tmp/permute1_recursive.txt -w \"${tools}/permutations_list.txt\" \\\n\t\t\t\t\t\t2>>\"$LOGFILE\" | head -c \"$PERMUTATIONS_LIMIT\" >.tmp/gotator2_recursive.txt\n\t\t\t\tfi\n\t\t\tfi\n\n\t\t\t# Resolve second round of permutations\n\t\t\tif [[ $AXIOM != true ]]; then\n\t\t\t\tif [[ -s \".tmp/gotator2_recursive.txt\" ]]; then\n\t\t\t\t\tpuredns resolve .tmp/gotator2_recursive.txt -w .tmp/permute2_recursive.txt \\\n\t\t\t\t\t\t-r \"$resolvers\" --resolvers-trusted \"$resolvers_trusted\" \\\n\t\t\t\t\t\t-l \"$PUREDNS_PUBLIC_LIMIT\" --rate-limit-trusted \"$PUREDNS_TRUSTED_LIMIT\" \\\n\t\t\t\t\t\t--wildcard-tests \"$PUREDNS_WILDCARDTEST_LIMIT\" --wildcard-batch \"$PUREDNS_WILDCARDBATCH_LIMIT\" \\\n\t\t\t\t\t\t2>>\"$LOGFILE\" >/dev/null\n\t\t\t\tfi\n\t\t\telse\n\t\t\t\tif [[ -s \".tmp/gotator2_recursive.txt\" ]]; then\n\t\t\t\t\taxiom-scan .tmp/gotator2_recursive.txt -m puredns-resolve \\\n\t\t\t\t\t\t-r /home/op/lists/resolvers.txt --resolvers-trusted /home/op/lists/resolvers_trusted.txt \\\n\t\t\t\t\t\t--wildcard-tests \"$PUREDNS_WILDCARDTEST_LIMIT\" --wildcard-batch \"$PUREDNS_WILDCARDBATCH_LIMIT\" \\\n\t\t\t\t\t\t-o .tmp/permute2_recursive.txt $AXIOM_EXTRA_ARGS \\\n\t\t\t\t\t\t2>>\"$LOGFILE\" >/dev/null\n\t\t\t\tfi\n\t\t\tfi\n\n\t\t\t# Combine permutations\n\t\t\tif [[ -s \".tmp/permute1_recursive.txt\" ]] || [[ -s \".tmp/permute2_recursive.txt\" ]]; then\n\t\t\t\tcat .tmp/permute1_recursive.txt .tmp/permute2_recursive.txt 2>>\"$LOGFILE\" | anew -q .tmp/permute_recursive.txt\n\t\t\tfi\n\t\telse\n\t\t\tend_subfunc \"Skipping recursive search: Too many subdomains\" \"${FUNCNAME[0]}\"\n\t\t\treturn 0\n\t\tfi\n\n\t\t# Check inscope if applicable\n\t\tif [[ $INSCOPE == true ]]; then\n\t\t\tif [[ -s \".tmp/permute_recursive.txt\" ]]; then\n\t\t\t\tif ! check_inscope .tmp/permute_recursive.txt 2>>\"$LOGFILE\" >/dev/null; then\n\t\t\t\t\tprintf \"%b[!] check_inscope command failed on permute_recursive.txt.%b\\n\" \"$bred\" \"$reset\"\n\t\t\t\tfi\n\t\t\tfi\n\t\t\tif [[ -s \".tmp/brute_recursive.txt\" ]]; then\n\t\t\t\tif ! check_inscope .tmp/brute_recursive.txt 2>>\"$LOGFILE\" >/dev/null; then\n\t\t\t\t\tprintf \"%b[!] check_inscope command failed on brute_recursive.txt.%b\\n\" \"$bred\" \"$reset\"\n\t\t\t\tfi\n\t\t\tfi\n\t\tfi\n\n\t\t# Combine results for final validation\n\t\tif [[ -s \".tmp/permute_recursive.txt\" ]] || [[ -s \".tmp/brute_recursive.txt\" ]]; then\n\t\t\tif ! cat .tmp/permute_recursive.txt .tmp/brute_recursive.txt 2>>\"$LOGFILE\" | anew -q .tmp/brute_perm_recursive.txt; then\n\t\t\t\tprintf \"%b[!] Failed to combine final results.%b\\n\" \"$bred\" \"$reset\"\n\t\t\t\treturn 1\n\t\t\tfi\n\t\tfi\n\n\t\t# Final resolve\n\t\tif [[ $AXIOM != true ]]; then\n\t\t\tif [[ -s \".tmp/brute_perm_recursive.txt\" ]]; then\n\t\t\t\tpuredns resolve .tmp/brute_perm_recursive.txt -w .tmp/brute_perm_recursive_final.txt \\\n\t\t\t\t\t-r \"$resolvers\" --resolvers-trusted \"$resolvers_trusted\" \\\n\t\t\t\t\t-l \"$PUREDNS_PUBLIC_LIMIT\" --rate-limit-trusted \"$PUREDNS_TRUSTED_LIMIT\" \\\n\t\t\t\t\t--wildcard-tests \"$PUREDNS_WILDCARDTEST_LIMIT\" --wildcard-batch \"$PUREDNS_WILDCARDBATCH_LIMIT\" \\\n\t\t\t\t\t2>>\"$LOGFILE\" >/dev/null\n\t\t\tfi\n\t\telse\n\t\t\tif [[ -s \".tmp/brute_perm_recursive.txt\" ]]; then\n\t\t\t\taxiom-scan .tmp/brute_perm_recursive.txt -m puredns-resolve \\\n\t\t\t\t\t-r /home/op/lists/resolvers.txt --resolvers-trusted /home/op/lists/resolvers_trusted.txt \\\n\t\t\t\t\t--wildcard-tests \"$PUREDNS_WILDCARDTEST_LIMIT\" --wildcard-batch \"$PUREDNS_WILDCARDBATCH_LIMIT\" \\\n\t\t\t\t\t-o .tmp/brute_perm_recursive_final.txt $AXIOM_EXTRA_ARGS \\\n\t\t\t\t\t2>>\"$LOGFILE\" >/dev/null\n\t\t\tfi\n\t\tfi\n\n\t\t# Process final results\n\t\tif [[ -s \".tmp/brute_perm_recursive_final.txt\" ]]; then\n\t\t\tif ! NUMOFLINES=$(grep \"\\.$domain$\\|^$domain$\" .tmp/brute_perm_recursive_final.txt 2>>\"$LOGFILE\" |\n\t\t\t\tgrep -E '^([a-zA-Z0-9\\.\\-]+\\.)+[a-zA-Z]{1,}$' |\n\t\t\t\tsed '/^$/d' |\n\t\t\t\tanew subdomains/subdomains.txt |\n\t\t\t\twc -l); then\n\t\t\t\tprintf \"%b[!] Failed to count new subdomains.%b\\n\" \"$bred\" \"$reset\"\n\t\t\t\tNUMOFLINES=0\n\t\t\tfi\n\t\telse\n\t\t\tNUMOFLINES=0\n\t\tfi\n\n\t\tend_subfunc \"${NUMOFLINES} new subs (recursive active)\" \"${FUNCNAME[0]}\"\n\n\telse\n\t\tif [[ $SUB_RECURSIVE_BRUTE == false ]]; then\n\t\t\tprintf \"\\n%b[%s] %s skipped due to mode or defined in reconftw.cfg.%b\\n\" \\\n\t\t\t\t\"$yellow\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"${FUNCNAME[0]}\" \"$reset\"\n\t\telif [[ ! -s \"subdomains/subdomains.txt\" ]]; then\n\t\t\tprintf \"\\n%s[%s] No subdomains to process.%b\\n\" \\\n\t\t\t\t\"$yellow\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"$reset\"\n\t\telse\n\t\t\tprintf \"%b[%s] %s has already been processed. To force execution, delete:\\n    %s/.%s%b\\n\\n\" \\\n\t\t\t\t\"$yellow\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"${FUNCNAME[0]}\" \\\n\t\t\t\t\"$called_fn_dir\" \"/.${FUNCNAME[0]}\" \"$reset\"\n\t\tfi\n\tfi\n}\n\nfunction subtakeover() {\n\n\t# Create necessary directories\n\tif ! mkdir -p .tmp webs subdomains; then\n\t\tprintf \"%b[!] Failed to create directories.%b\\n\" \"$bred\" \"$reset\"\n\t\treturn 1\n\tfi\n\n\t# Check if the function should run\n\tif { [[ ! -f \"$called_fn_dir/.${FUNCNAME[0]}\" ]] || [[ $DIFF == true ]]; } && [[ $SUBTAKEOVER == true ]]; then\n\t\tstart_func \"${FUNCNAME[0]}\" \"Looking for possible subdomain and DNS takeover\"\n\n\t\t# Initialize takeover file\n\t\tif ! touch .tmp/tko.txt; then\n\t\t\tprintf \"%b[!] Failed to create .tmp/tko.txt.%b\\n\" \"$bred\" \"$reset\"\n\t\t\treturn 1\n\t\tfi\n\n\t\t# Combine webs.txt and webs_uncommon_ports.txt if webs_all.txt doesn't exist\n\t\tif [[ ! -s \"webs/webs_all.txt\" ]]; then\n\t\t\tcat webs/webs.txt webs/webs_uncommon_ports.txt 2>/dev/null | anew -q webs/webs_all.txt\n\t\tfi\n\n\t\tif [[ $AXIOM != true ]]; then\n\t\t\tif ! nuclei -update 2>>\"$LOGFILE\" >/dev/null; then\n\t\t\t\tprintf \"%b[!] Failed to update nuclei.%b\\n\" \"$bred\" \"$reset\"\n\t\t\tfi\n\t\t\tcat subdomains/subdomains.txt webs/webs_all.txt 2>/dev/null | nuclei -silent -nh -tags takeover \\\n\t\t\t\t-severity info,low,medium,high,critical -retries 3 -rl \"$NUCLEI_RATELIMIT\" \\\n\t\t\t\t-t \"${NUCLEI_TEMPLATES_PATH}\" -o .tmp/tko.txt\n\t\telse\n\t\t\tcat subdomains/subdomains.txt webs/webs_all.txt 2>>\"$LOGFILE\" | sed '/^$/d' | anew -q .tmp/webs_subs.txt\n\t\t\tif [[ -s \".tmp/webs_subs.txt\" ]]; then\n\t\t\t\taxiom-scan .tmp/webs_subs.txt -m nuclei --nuclei-templates \"${NUCLEI_TEMPLATES_PATH}\" \\\n\t\t\t\t\t-tags takeover -nh -severity info,low,medium,high,critical -retries 3 -rl \"$NUCLEI_RATELIMIT\" \\\n\t\t\t\t\t-t \"${NUCLEI_TEMPLATES_PATH}\" -o .tmp/tko.txt $AXIOM_EXTRA_ARGS 2>>\"$LOGFILE\" >/dev/null\n\t\t\tfi\n\t\tfi\n\n\t\t# DNS Takeover\n\t\tcat .tmp/subs_no_resolved.txt .tmp/subdomains_dns.txt .tmp/scrap_subs.txt \\\n\t\t\t.tmp/analytics_subs_clean.txt .tmp/passive_recursive.txt 2>/dev/null | anew -q .tmp/subs_dns_tko.txt\n\n\t\tif [[ -s \".tmp/subs_dns_tko.txt\" ]]; then\n\t\t\tcat .tmp/subs_dns_tko.txt 2>/dev/null | dnstake -c \"$DNSTAKE_THREADS\" -s 2>>\"$LOGFILE\" |\n\t\t\t\tsed '/^$/d' | anew -q .tmp/tko.txt\n\t\tfi\n\n\t\t# Remove empty lines from tko.txt\n\t\tsed -i '/^$/d' .tmp/tko.txt\n\n\t\t# Count new takeover entries\n\t\tif ! NUMOFLINES=$(cat .tmp/tko.txt 2>>\"$LOGFILE\" | anew webs/takeover.txt | sed '/^$/d' | wc -l); then\n\t\t\tprintf \"%b[!] Failed to count takeover entries.%b\\n\" \"$bred\" \"$reset\"\n\t\t\tNUMOFLINES=0\n\t\tfi\n\n\t\tif [[ $NUMOFLINES -gt 0 ]]; then\n\t\t\tnotification \"${NUMOFLINES} new possible takeovers found\" info\n\t\tfi\n\n\t\tend_func \"Results are saved in $domain/webs/takeover.txt\" \"${FUNCNAME[0]}\"\n\n\telse\n\t\tif [[ $SUBTAKEOVER == false ]]; then\n\t\t\tprintf \"\\n%b[%s] %s skipped due to mode or defined in reconftw.cfg.%b\\n\" \\\n\t\t\t\t\"$yellow\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"${FUNCNAME[0]}\" \"$reset\"\n\t\telse\n\t\t\tprintf \"%b[%s] %s has already been processed. To force execution, delete:\\n    %s/.%s %b\\n\\n\" \\\n\t\t\t\t\"$yellow\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"${FUNCNAME[0]}\" \"$called_fn_dir\" \".${FUNCNAME[0]}\" \"$reset\"\n\t\tfi\n\tfi\n\n}\n\nfunction zonetransfer() {\n\n\t# Create necessary directories\n\tif ! mkdir -p subdomains; then\n\t\tprintf \"%b[!] Failed to create subdomains directory.%b\\n\" \"$bred\" \"$reset\"\n\t\treturn 1\n\tfi\n\n\t# Check if the function should run\n\tif { [[ ! -f \"$called_fn_dir/.${FUNCNAME[0]}\" ]] || [[ $DIFF == true ]]; } && [[ $ZONETRANSFER == true ]] && ! [[ $domain =~ ^[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+$ ]]; then\n\t\tstart_func \"${FUNCNAME[0]}\" \"Zone transfer check\"\n\n\t\t# Initialize output file\n\t\tif ! : >\"subdomains/zonetransfer.txt\"; then\n\t\t\tprintf \"%b[!] Failed to create zonetransfer.txt.%b\\n\" \"$bred\" \"$reset\"\n\t\t\treturn 1\n\t\tfi\n\n\t\t# Perform zone transfer check\n\t\tfor ns in $(dig +short ns \"$domain\"); do\n\t\t\tdig axfr \"$domain\" @\"$ns\" >>\"subdomains/zonetransfer.txt\" 2>>\"$LOGFILE\"\n\t\tdone\n\n\t\t# Check if zone transfer was successful\n\t\tif [[ -s \"subdomains/zonetransfer.txt\" ]]; then\n\t\t\tif ! grep -q \"Transfer failed\" \"subdomains/zonetransfer.txt\"; then\n\t\t\t\tnotification \"Zone transfer found on ${domain}!\" \"info\"\n\t\t\tfi\n\t\tfi\n\n\t\tend_func \"Results are saved in $domain/subdomains/zonetransfer.txt\" \"${FUNCNAME[0]}\"\n\n\telse\n\t\tif [[ $ZONETRANSFER == false ]]; then\n\t\t\tprintf \"\\n%b[%s] %s skipped due to mode or defined in reconftw.cfg.%b\\n\" \\\n\t\t\t\t\"$yellow\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"${FUNCNAME[0]}\" \"$reset\"\n\t\telif [[ $domain =~ ^[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+$ ]]; then\n\t\t\tprintf \"\\n%s[%s] Domain is an IP address; skipping zone transfer.%b\\n\" \\\n\t\t\t\t\"$yellow\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"$reset\"\n\t\telse\n\t\t\tprintf \"%b[%s] %s has already been processed. To force execution, delete:\\n    %s/.%s%b\\n\\n\" \\\n\t\t\t\t\"$yellow\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"${FUNCNAME[0]}\" \"$called_fn_dir\" \".${FUNCNAME[0]}\" \"$reset\"\n\t\tfi\n\tfi\n\n}\n\nfunction s3buckets() {\n\t# Create necessary directories\n\tif ! mkdir -p .tmp subdomains; then\n\t\tprintf \"%b[!] Failed to create directories.%b\\n\" \"$bred\" \"$reset\"\n\t\treturn 1\n\tfi\n\n\t# Check if the function should run\n\tif { [[ ! -f \"$called_fn_dir/.${FUNCNAME[0]}\" ]] || [[ $DIFF == true ]]; } && [[ $S3BUCKETS == true ]] && ! [[ $domain =~ ^[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+$ ]]; then\n\t\tstart_func \"${FUNCNAME[0]}\" \"AWS S3 buckets search\"\n\n\t\t# If in multi mode and subdomains.txt doesn't exist, create it\n\t\tif [[ -n $multi ]] && [[ ! -f \"$dir/subdomains/subdomains.txt\" ]]; then\n\t\t\tif ! printf \"%b\\n\" \"$domain\" >\"$dir/subdomains/subdomains.txt\"; then\n\t\t\t\tprintf \"%b[!] Failed to create subdomains.txt.%b\\n\" \"$bred\" \"$reset\"\n\t\t\t\treturn 1\n\t\t\tfi\n\t\tfi\n\n\t\t# Debug: Print current directory and tools variable\n\t\tprintf \"Current directory: %s\\n\" \"$(pwd)\" >>\"$LOGFILE\"\n\t\tprintf \"Tools directory: %s\\n\" \"$tools\" >>\"$LOGFILE\"\n\n\t\t# S3Scanner\n\t\tif [[ $AXIOM != true ]]; then\n\t\t\tif [[ -s \"subdomains/subdomains.txt\" ]]; then\n\t\t\t\ts3scanner scan -f subdomains/subdomains.txt 2>>\"$LOGFILE\" | anew -q .tmp/s3buckets.txt\n\t\t\tfi\n\t\telse\n\t\t\taxiom-scan subdomains/subdomains.txt -m s3scanner -o .tmp/s3buckets_tmp.txt \"$AXIOM_EXTRA_ARGS\" 2>>\"$LOGFILE\" >/dev/null\n\n\t\t\tif [[ -s \".tmp/s3buckets_tmp.txt\" ]]; then\n\t\t\t\tif ! cat .tmp/s3buckets_tmp.txt .tmp/s3buckets_tmp2.txt 2>>\"$LOGFILE\" | anew -q .tmp/s3buckets.txt; then\n\t\t\t\t\tprintf \"%b[!] Failed to process s3buckets_tmp.txt.%b\\n\" \"$bred\" \"$reset\"\n\t\t\t\tfi\n\t\t\t\tif ! sed -i '/^$/d' .tmp/s3buckets.txt; then\n\t\t\t\t\tprintf \"%b[!] Failed to clean s3buckets.txt.%b\\n\" \"$bred\" \"$reset\"\n\t\t\t\tfi\n\t\t\tfi\n\t\tfi\n\n\t\t# Include root domain in the process\n\t\tif ! printf \"%b\\n\" \"$domain\" >webs/full_webs.txt; then\n\t\t\tprintf \"%b[!] Failed to create webs/full_webs.txt.%b\\n\" \"$bred\" \"$reset\"\n\t\tfi\n\n\t\tif [[ -s \"webs/webs_all.txt\" ]]; then\n\t\t\tif ! cat webs/webs_all.txt >>webs/full_webs.txt; then\n\t\t\t\tprintf \"%b[!] Failed to append webs_all.txt to full_webs.txt.%b\\n\" \"$bred\" \"$reset\"\n\t\t\tfi\n\t\tfi\n\n\t\t# Initialize the output file in the subdomains folder\n\t\tif ! : >subdomains/cloudhunter_open_buckets.txt; then\n\t\t\tprintf \"%b[!] Failed to initialize cloudhunter_open_buckets.txt.%b\\n\" \"$bred\" \"$reset\"\n\t\tfi\n\n\t\t# Determine the CloudHunter permutations flag based on the config\n\t\tPERMUTATION_FLAG=\"\"\n\t\tcase \"$CLOUDHUNTER_PERMUTATION\" in\n\t\tDEEP)\n\t\t\tPERMUTATION_FLAG=\"-p $tools/CloudHunter/permutations-big.txt\"\n\t\t\t;;\n\t\tNORMAL)\n\t\t\tPERMUTATION_FLAG=\"-p $tools/CloudHunter/permutations.txt\"\n\t\t\t;;\n\t\tNONE)\n\t\t\tPERMUTATION_FLAG=\"\"\n\t\t\t;;\n\t\t*)\n\t\t\tprintf \"%b[!] Invalid value for CLOUDHUNTER_PERMUTATION: %s.%b\\n\" \"$bred\" \"$CLOUDHUNTER_PERMUTATION\" \"$reset\"\n\t\t\treturn 1\n\t\t\t;;\n\t\tesac\n\n\t\t# Debug: Print the full CloudHunter command\n\t\tprintf \"CloudHunter command: python3 %s/cloudhunter.py %s -r %s/resolvers.txt -t 50 [URL]\\n\" \"$tools/CloudHunter\" \"$PERMUTATION_FLAG\" \"$tools/CloudHunter\" >>\"$LOGFILE\"\n\n\t\t# Debug: Check if files exist\n\t\tif [[ -f \"$tools/CloudHunter/cloudhunter.py\" ]]; then\n\t\t\tprintf \"cloudhunter.py exists\\n\" >>\"$LOGFILE\"\n\t\telse\n\t\t\tprintf \"cloudhunter.py not found\\n\" >>\"$LOGFILE\"\n\t\tfi\n\n\t\tif [[ -n $PERMUTATION_FLAG ]]; then\n\t\t\tpermutation_file=\"${PERMUTATION_FLAG#-p }\"\n\t\t\tif [[ -f $permutation_file ]]; then\n\t\t\t\tprintf \"Permutations file exists\\n\" >>\"$LOGFILE\"\n\t\t\telse\n\t\t\t\tprintf \"Permutations file not found: %s\\n\" \"$permutation_file\" >>\"$LOGFILE\"\n\t\t\tfi\n\t\tfi\n\n\t\tif [[ -f \"$tools/CloudHunter/resolvers.txt\" ]]; then\n\t\t\tprintf \"resolvers.txt exists\\n\" >>\"$LOGFILE\"\n\t\telse\n\t\t\tprintf \"resolvers.txt not found\\n\" >>\"$LOGFILE\"\n\t\tfi\n\n\t\t# Run CloudHunter on each URL in webs/full_webs.txt and append the output to the file in the subdomains folder\n\t\twhile IFS= read -r url; do\n\t\t\tprintf \"Processing URL: %s\\n\" \"$url\" >>\"$LOGFILE\"\n\t\t\t(\n\t\t\t\tif ! cd \"$tools/CloudHunter\"; then\n\t\t\t\t\tprintf \"%b[!] Failed to cd to %s.%b\\n\" \"$bred\" \"$tools/CloudHunter\" \"$reset\"\n\t\t\t\t\treturn 1\n\t\t\t\tfi\n\t\t\t\tif ! python3 ./cloudhunter.py ${PERMUTATION_FLAG#-p } -r ./resolvers.txt -t 50 \"$url\"; then\n\t\t\t\t\tprintf \"%b[!] CloudHunter command failed for URL %s.%b\\n\" \"$bred\" \"$url\" \"$reset\"\n\t\t\t\tfi\n\t\t\t) >>\"$dir/subdomains/cloudhunter_open_buckets.txt\" 2>>\"$LOGFILE\"\n\t\tdone <webs/full_webs.txt\n\n\t\t# Remove the full_webs.txt file after CloudHunter processing\n\t\tif ! rm webs/full_webs.txt; then\n\t\t\tprintf \"%b[!] Failed to remove webs/full_webs.txt.%b\\n\" \"$bred\" \"$reset\"\n\t\tfi\n\n\t\t# Process CloudHunter results\n\t\tif [[ -s \"subdomains/cloudhunter_open_buckets.txt\" ]]; then\n\t\t\tif ! NUMOFLINES1=$(cat subdomains/cloudhunter_open_buckets.txt 2>>\"$LOGFILE\" | anew subdomains/cloud_assets.txt | wc -l); then\n\t\t\t\tprintf \"%b[!] Failed to process cloudhunter_open_buckets.txt.%b\\n\" \"$bred\" \"$reset\"\n\t\t\t\tNUMOFLINES1=0\n\t\t\tfi\n\t\t\tif [[ $NUMOFLINES1 -gt 0 ]]; then\n\t\t\t\tnotification \"${NUMOFLINES1} new cloud assets found\" \"info\"\n\t\t\tfi\n\t\telse\n\t\t\tNUMOFLINES1=0\n\t\tfi\n\n\t\t# Process s3buckets results\n\t\tif [[ -s \".tmp/s3buckets.txt\" ]]; then\n\t\t\tif ! NUMOFLINES2=$(cat .tmp/s3buckets.txt 2>>\"$LOGFILE\" | grep -aiv \"not_exist\" | grep -aiv \"Warning:\" | grep -aiv \"invalid_name\" | grep -aiv \"^http\" | awk 'NF' | anew subdomains/s3buckets.txt | sed '/^$/d' | wc -l); then\n\t\t\t\tprintf \"%b[!] Failed to process s3buckets.txt.%b\\n\" \"$bred\" \"$reset\"\n\t\t\t\tNUMOFLINES2=0\n\t\t\tfi\n\t\t\tif [[ $NUMOFLINES2 -gt 0 ]]; then\n\t\t\t\tnotification \"${NUMOFLINES2} new S3 buckets found\" \"info\"\n\t\t\tfi\n\t\telse\n\t\t\tNUMOFLINES2=0\n\t\tfi\n\n\t\t# Run trufflehog for S3 buckets\n\t\tif [[ -s \"subdomains/s3buckets.txt\" ]]; then\n\t\t\twhile IFS= read -r bucket; do\n\t\t\t\ttrufflehog s3 --bucket=\"$bucket\" -j 2>/dev/null | jq -c | anew -q subdomains/s3buckets_trufflehog.txt\n\t\t\tdone <subdomains/s3buckets.txt\n\t\tfi\n\n\t\t# Run trufflehog for open buckets found by CloudHunter\n\t\tif [[ -s \"subdomains/cloudhunter_open_buckets.txt\" ]]; then\n\t\t\twhile IFS= read -r line; do\n\t\t\t\tif echo \"$line\" | grep -q \"Aws Cloud\"; then\n\t\t\t\t\t# AWS S3 Bucket\n\t\t\t\t\tbucket_name=$(echo \"$line\" | awk '{print $3}')\n\t\t\t\t\ttrufflehog s3 --bucket=\"$bucket_name\" -j 2>/dev/null | jq -c | anew -q subdomains/cloudhunter_buckets_trufflehog.txt\n\t\t\t\telif echo \"$line\" | grep -q \"Google Cloud\"; then\n\t\t\t\t\t# Google Cloud Storage\n\t\t\t\t\tbucket_name=$(echo \"$line\" | awk '{print $3}')\n\t\t\t\t\ttrufflehog gcs --bucket=\"$bucket_name\" -j 2>/dev/null | jq -c | anew -q subdomains/cloudhunter_buckets_trufflehog.txt\n\t\t\t\tfi\n\t\t\tdone <subdomains/cloudhunter_open_buckets.txt\n\t\tfi\n\n\t\tend_func \"Results are saved in subdomains/s3buckets.txt, subdomains/cloud_assets.txt, subdomains/s3buckets_trufflehog.txt, and subdomains/cloudhunter_buckets_trufflehog.txt\" \"${FUNCNAME[0]}\"\n\telse\n\t\tif [[ $S3BUCKETS == false ]]; then\n\t\t\tprintf \"\\n%b[%s] %s skipped due to mode or defined in reconftw.cfg.%b\\n\" \"$yellow\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"${FUNCNAME[0]}\" \"$reset\"\n\t\telif [[ $domain =~ ^[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+$ ]]; then\n\t\t\tprintf \"\\n%s[%s] Domain is an IP address; skipping S3 buckets search.%b\\n\" \"$yellow\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"$reset\"\n\t\t\treturn 0\n\t\telse\n\t\t\tprintf \"%b[%s] %s has already been processed. To force execution, delete:\\n    %s/.%s%b\\n\\n\" \"$yellow\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"${FUNCNAME[0]}\" \"$called_fn_dir\" \".${FUNCNAME[0]}\" \"$reset\"\n\t\tfi\n\tfi\n}\n\n###############################################################################################################\n############################################# GEOLOCALIZATION INFO ############################################\n###############################################################################################################\n\nfunction geo_info() {\n\n\t# Create necessary directories\n\tif ! mkdir -p hosts; then\n\t\tprintf \"%b[!] Failed to create hosts directory.%b\\n\" \"$bred\" \"$reset\"\n\t\treturn 1\n\tfi\n\n\t# Check if the function should run\n\tif { [[ ! -f \"$called_fn_dir/.${FUNCNAME[0]}\" ]] || [[ $DIFF == true ]]; } && [[ $GEO_INFO == true ]]; then\n\t\tstart_func \"${FUNCNAME[0]}\" \"Running: ipinfo\"\n\n\t\tips_file=\"${dir}/hosts/ips.txt\"\n\n\t\t# Check if ips.txt exists or is empty; if so, attempt to generate it\n\t\tif [[ ! -s $ips_file ]]; then\n\t\t\t# Attempt to generate hosts/ips.txt\n\t\t\tif ! [[ $domain =~ ^[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+$ ]]; then\n\t\t\t\tif [[ -s \"subdomains/subdomains_dnsregs.json\" ]]; then\n\t\t\t\t\tjq -r 'try . | \"\\(.host) \\(.a[0])\"' \"subdomains/subdomains_dnsregs.json\" | anew -q .tmp/subs_ips.txt\n\t\t\t\tfi\n\t\t\t\tif [[ -s \".tmp/subs_ips.txt\" ]]; then\n\t\t\t\t\tawk '{ print $2 \" \" $1}' .tmp/subs_ips.txt | sort -k2 -n | anew -q hosts/subs_ips_vhosts.txt\n\t\t\t\tfi\n\t\t\t\tif [[ -s \"hosts/subs_ips_vhosts.txt\" ]]; then\n\t\t\t\t\tcut -d ' ' -f1 hosts/subs_ips_vhosts.txt |\n\t\t\t\t\t\tgrep -aEiv \"^(127|10|169\\.254|172\\.1[6-9]|172\\.2[0-9]|172\\.3[0-1]|192\\.168)\\.\" |\n\t\t\t\t\t\tgrep -oE \"\\b([0-9]{1,3}\\.){3}[0-9]{1,3}\\b\" |\n\t\t\t\t\t\tanew -q hosts/ips.txt\n\t\t\t\tfi\n\t\t\telse\n\t\t\t\tprintf \"%b\\n\" \"$domain\" |\n\t\t\t\t\tgrep -aEiv \"^(127|10|169\\.254|172\\.1[6-9]|172\\.2[0-9]|172\\.3[0-1]|192\\.168)\\.\" |\n\t\t\t\t\tgrep -oE \"\\b([0-9]{1,3}\\.){3}[0-9]{1,3}\\b\" |\n\t\t\t\t\tanew -q hosts/ips.txt\n\t\t\tfi\n\t\tfi\n\n\t\tif [[ -s $ips_file ]]; then\n\t\t\tif ! touch \"${dir}/hosts/ipinfo.txt\"; then\n\t\t\t\tprintf \"%b[!] Failed to create ipinfo.txt.%b\\n\" \"$bred\" \"$reset\"\n\t\t\tfi\n\n\t\t\twhile IFS= read -r ip; do\n\t\t\t\tcurl -s \"https://ipinfo.io/widget/demo/$ip\" >>\"${dir}/hosts/ipinfo.txt\"\n\t\t\tdone <\"$ips_file\"\n\t\tfi\n\n\t\tend_func \"Results are saved in hosts/ipinfo.txt\" \"${FUNCNAME[0]}\"\n\telse\n\t\tif [[ $GEO_INFO == false ]]; then\n\t\t\tprintf \"\\n%b[%s] %s skipped due to mode or defined in reconftw.cfg.%b\\n\" \\\n\t\t\t\t\"$yellow\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"${FUNCNAME[0]}\" \"$reset\"\n\t\telse\n\t\t\tprintf \"%b[%s] %s has already been processed. To force execution, delete:\\n    %s/.%s%b\\n\\n\" \\\n\t\t\t\t\"$yellow\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"${FUNCNAME[0]}\" \"$called_fn_dir\" \".${FUNCNAME[0]}\" \"$reset\"\n\t\tfi\n\tfi\n\n}\n\n###############################################################################################################\n########################################### WEB DETECTION #####################################################\n###############################################################################################################\n\nfunction webprobe_simple() {\n\n\t# Create necessary directories\n\tif ! mkdir -p .tmp webs subdomains; then\n\t\tprintf \"%b[!] Failed to create directories.%b\\n\" \"$bred\" \"$reset\"\n\t\treturn 1\n\tfi\n\n\t# Check if the function should run\n\tif { [[ ! -f \"$called_fn_dir/.${FUNCNAME[0]}\" ]] || [[ $DIFF == true ]]; } && [[ $WEBPROBESIMPLE == true ]]; then\n\t\tstart_subfunc \"${FUNCNAME[0]}\" \"Running: HTTP probing $domain\"\n\n\t\t# If in multi mode and subdomains.txt doesn't exist, create it\n\t\tif [[ -n $multi ]] && [[ ! -f \"$dir/subdomains/subdomains.txt\" ]]; then\n\t\t\tprintf \"%b\\n\" \"$domain\" >\"$dir/subdomains/subdomains.txt\"\n\t\t\ttouch .tmp/web_full_info.txt webs/web_full_info.txt\n\t\tfi\n\n\t\t# Run httpx or axiom-scan\n\t\tif [[ $AXIOM != true ]]; then\n\t\t\thttpx ${HTTPX_FLAGS} -no-color -json -random-agent -threads \"$HTTPX_THREADS\" -rl \"$HTTPX_RATELIMIT\" \\\n\t\t\t\t-retries 2 -timeout \"$HTTPX_TIMEOUT\" -o .tmp/web_full_info_probe.txt \\\n\t\t\t\t<subdomains/subdomains.txt 2>>\"$LOGFILE\" >/dev/null\n\t\telse\n\t\t\taxiom-scan subdomains/subdomains.txt -m httpx ${HTTPX_FLAGS} -no-color -json -random-agent \\\n\t\t\t\t-threads \"$HTTPX_THREADS\" -rl \"$HTTPX_RATELIMIT\" -retries 2 -timeout \"$HTTPX_TIMEOUT\" \\\n\t\t\t\t-o .tmp/web_full_info_probe.txt \"$AXIOM_EXTRA_ARGS\" 2>>\"$LOGFILE\" >/dev/null\n\t\tfi\n\n\t\t# Merge web_full_info files\n\t\tcat .tmp/web_full_info.txt .tmp/web_full_info_probe.txt webs/web_full_info.txt 2>>\"$LOGFILE\" |\n\t\t\tjq -s 'try .' | jq 'try unique_by(.input)' | jq 'try .[]' 2>>\"$LOGFILE\" >webs/web_full_info.txt\n\n\t\t# Extract URLs\n\t\tif [[ -s \"webs/web_full_info.txt\" ]]; then\n\t\t\tjq -r 'try .url' webs/web_full_info.txt 2>/dev/null |\n\t\t\t\tgrep \"$domain\" |\n\t\t\t\tgrep -E '^((http|https):\\/\\/)?([a-zA-Z0-9]([a-zA-Z0-9\\-]*[a-zA-Z0-9])?\\.)+[a-zA-Z]{1,}(\\/.*)?$' |\n\t\t\t\tsed 's/*.//' | anew -q .tmp/probed_tmp.txt\n\t\tfi\n\n\t\t# Extract web info to plain text\n\t\tif [[ -s \"webs/web_full_info.txt\" ]]; then\n\t\t\tjq -r 'try . |\"\\(.url) [\\(.status_code)] [\\(.title)] [\\(.webserver)] \\(.tech)\"' webs/web_full_info.txt |\n\t\t\t\tgrep \"$domain\" | anew -q webs/web_full_info_plain.txt\n\t\tfi\n\n\t\t# Remove out-of-scope entries\n\t\tif [[ -s $outOfScope_file ]]; then\n\t\t\tif ! deleteOutScoped \"$outOfScope_file\" .tmp/probed_tmp.txt; then\n\t\t\t\tprintf \"%b[!] Failed to delete out-of-scope entries.%b\\n\" \"$bred\" \"$reset\"\n\t\t\tfi\n\t\tfi\n\n\t\t# Count new websites\n\t\tif ! NUMOFLINES=$(anew webs/webs.txt <.tmp/probed_tmp.txt 2>/dev/null | sed '/^$/d' | wc -l); then\n\t\t\tprintf \"%b[!] Failed to count new websites.%b\\n\" \"$bred\" \"$reset\"\n\t\t\tNUMOFLINES=0\n\t\tfi\n\n\t\t# Update webs_all.txt\n\t\tcat webs/webs.txt webs/webs_uncommon_ports.txt 2>/dev/null | anew -q webs/webs_all.txt\n\n\t\tend_subfunc \"${NUMOFLINES} new websites resolved\" \"${FUNCNAME[0]}\"\n\n\t\t# Send websites to proxy if conditions met\n\t\tif [[ $PROXY == true ]] && [[ -n $proxy_url ]] && [[ $(wc -l <webs/webs.txt) -le $DEEP_LIMIT2 ]]; then\n\t\t\tnotification \"Sending websites to proxy\" \"info\"\n\t\t\tffuf -mc all -w webs/webs.txt -u FUZZ -replay-proxy \"$proxy_url\" 2>>\"$LOGFILE\" >/dev/null\n\t\tfi\n\n\telse\n\t\tif [[ $WEBPROBESIMPLE == false ]]; then\n\t\t\tprintf \"\\n%b[%s] %s skipped due to mode or defined in reconftw.cfg.%b\\n\" \\\n\t\t\t\t\"$yellow\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"${FUNCNAME[0]}\" \"$reset\"\n\t\telse\n\t\t\tprintf \"%b[%s] %s has already been processed. To force execution, delete:\\n    %s/.%s%b\\n\\n\" \\\n\t\t\t\t\"$yellow\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"${FUNCNAME[0]}\" \"$called_fn_dir\" \".${FUNCNAME[0]}\" \"$reset\"\n\t\tfi\n\tfi\n\n}\n\nfunction webprobe_full() {\n\n\t# Create necessary directories\n\tif ! mkdir -p .tmp webs subdomains; then\n\t\tprintf \"%b[!] Failed to create directories.%b\\n\" \"$bred\" \"$reset\"\n\t\treturn 1\n\tfi\n\n\t# Check if the function should run\n\tif { [[ ! -f \"$called_fn_dir/.${FUNCNAME[0]}\" ]] || [[ $DIFF == true ]]; } && [[ $WEBPROBEFULL == true ]]; then\n\t\tstart_func \"${FUNCNAME[0]}\" \"HTTP Probing Non-Standard Ports\"\n\n\t\t# If in multi mode and subdomains.txt doesn't exist, create it\n\t\tif [[ -n $multi ]] && [[ ! -f \"$dir/subdomains/subdomains.txt\" ]]; then\n\t\t\tprintf \"%b\\n\" \"$domain\" >\"$dir/subdomains/subdomains.txt\"\n\t\t\ttouch webs/webs.txt\n\t\tfi\n\n\t\t# Check if subdomains.txt is non-empty\n\t\tif [[ -s \"subdomains/subdomains.txt\" ]]; then\n\t\t\tif [[ $AXIOM != true ]]; then\n\t\t\t\t# Run httpx on subdomains.txt\n\t\t\t\thttpx -follow-host-redirects -random-agent -status-code \\\n\t\t\t\t\t-p \"$UNCOMMON_PORTS_WEB\" -threads \"$HTTPX_UNCOMMONPORTS_THREADS\" \\\n\t\t\t\t\t-timeout \"$HTTPX_UNCOMMONPORTS_TIMEOUT\" -silent -retries 2 \\\n\t\t\t\t\t-title -web-server -tech-detect -location -no-color -json \\\n\t\t\t\t\t-o .tmp/web_full_info_uncommon.txt <subdomains/subdomains.txt 2>>\"$LOGFILE\" >/dev/null\n\t\t\telse\n\t\t\t\t# Run axiom-scan with httpx module on subdomains.txt\n\t\t\t\taxiom-scan subdomains/subdomains.txt -m httpx -follow-host-redirects \\\n\t\t\t\t\t-H \"${HEADER}\" -status-code -p \"$UNCOMMON_PORTS_WEB\" \\\n\t\t\t\t\t-threads \"$HTTPX_UNCOMMONPORTS_THREADS\" -timeout \"$HTTPX_UNCOMMONPORTS_TIMEOUT\" \\\n\t\t\t\t\t-silent -retries 2 -title -web-server -tech-detect -location -no-color -json \\\n\t\t\t\t\t-o .tmp/web_full_info_uncommon.txt \"$AXIOM_EXTRA_ARGS\" 2>>\"$LOGFILE\" >/dev/null\n\t\t\tfi\n\t\tfi\n\n\t\t# Process web_full_info_uncommon.txt\n\t\tif [[ -s \".tmp/web_full_info_uncommon.txt\" ]]; then\n\t\t\t# Extract URLs\n\t\t\tjq -r 'try .url' .tmp/web_full_info_uncommon.txt 2>/dev/null |\n\t\t\t\tgrep \"$domain\" |\n\t\t\t\tgrep -E '^((http|https):\\/\\/)?([a-zA-Z0-9]([a-zA-Z0-9\\-]*[a-zA-Z0-9])?\\.)+[a-zA-Z]{1,}(\\/.*)?' |\n\t\t\t\tsed 's/*.//' |\n\t\t\t\tanew -q .tmp/probed_uncommon_ports_tmp.txt\n\n\t\t\t# Extract plain web info\n\t\t\tjq -r 'try . | \"\\(.url) [\\(.status_code)] [\\(.title)] [\\(.webserver)] \\(.tech)\"' .tmp/web_full_info_uncommon.txt |\n\t\t\t\tgrep \"$domain\" |\n\t\t\t\tanew -q webs/web_full_info_uncommon_plain.txt\n\n\t\t\t# Update webs_full_info_uncommon.txt based on whether domain is IP\n\t\t\tif [[ $domain =~ ^[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+$ ]]; then\n\t\t\t\tcat .tmp/web_full_info_uncommon.txt 2>>\"$LOGFILE\" | anew -q webs/web_full_info_uncommon.txt\n\t\t\telse\n\t\t\t\tgrep \"$domain\" .tmp/web_full_info_uncommon.txt | anew -q webs/web_full_info_uncommon.txt\n\t\t\tfi\n\n\t\t\t# Count new websites\n\t\t\tif ! NUMOFLINES=$(anew webs/webs_uncommon_ports.txt <.tmp/probed_uncommon_ports_tmp.txt | sed '/^$/d' | wc -l); then\n\t\t\t\tprintf \"%b[!] Failed to count new websites.%b\\n\" \"$bred\" \"$reset\"\n\t\t\t\tNUMOFLINES=0\n\t\t\tfi\n\n\t\t\t# Notify user\n\t\t\tnotification \"Uncommon web ports: ${NUMOFLINES} new websites\" \"good\"\n\n\t\t\t# Display new uncommon ports websites\n\t\t\tif [[ -s \"webs/webs_uncommon_ports.txt\" ]]; then\n\t\t\t\tcat \"webs/webs_uncommon_ports.txt\"\n\t\t\tfi\n\n\t\t\t# Update webs_all.txt\n\t\t\tcat webs/webs.txt webs/webs_uncommon_ports.txt 2>/dev/null | anew -q webs/webs_all.txt\n\n\t\t\tend_func \"Results are saved in $domain/webs/webs_uncommon_ports.txt\" \"${FUNCNAME[0]}\"\n\n\t\t\t# Send to proxy if conditions met\n\t\t\tif [[ $PROXY == true ]] && [[ -n $proxy_url ]] && [[ $(wc -l <webs/webs_uncommon_ports.txt) -le $DEEP_LIMIT2 ]]; then\n\t\t\t\tnotification \"Sending websites with uncommon ports to proxy\" \"info\"\n\t\t\t\tffuf -mc all -w webs/webs_uncommon_ports.txt -u FUZZ -replay-proxy \"$proxy_url\" 2>>\"$LOGFILE\" >/dev/null\n\t\t\tfi\n\t\tfi\n\telse\n\t\tif [[ $WEBPROBEFULL == false ]]; then\n\t\t\tprintf \"\\n%b[%s] %s skipped due to mode or defined in reconftw.cfg.%b\\n\" \\\n\t\t\t\t\"$yellow\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"${FUNCNAME[0]}\" \"$reset\"\n\t\telse\n\t\t\tprintf \"%b[%s] %s has already been processed. To force execution, delete:\\n    %s/.%s%b\\n\\n\" \\\n\t\t\t\t\"$yellow\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"${FUNCNAME[0]}\" \"$called_fn_dir\" \".${FUNCNAME[0]}\" \"$reset\"\n\t\tfi\n\tfi\n\n}\n\nfunction screenshot() {\n\n\t# Create necessary directories\n\tif ! mkdir -p webs screenshots; then\n\t\tprintf \"%b[!] Failed to create directories.%b\\n\" \"$bred\" \"$reset\"\n\t\treturn 1\n\tfi\n\n\t# Check if the function should run\n\tif { [[ ! -f \"$called_fn_dir/.${FUNCNAME[0]}\" ]] || [[ $DIFF == true ]]; } && [[ $WEBSCREENSHOT == true ]]; then\n\t\tstart_func \"${FUNCNAME[0]}\" \"Web Screenshots\"\n\n\t\t# Combine webs.txt and webs_uncommon_ports.txt into webs_all.txt if it doesn't exist\n\t\tif [[ ! -s \"webs/webs_all.txt\" ]]; then\n\t\t\tcat webs/webs.txt webs/webs_uncommon_ports.txt 2>/dev/null | anew -q webs/webs_all.txt\n\t\tfi\n\n\t\t# Run nuclei or axiom-scan based on AXIOM flag\n\t\tif [[ $AXIOM != true ]]; then\n\t\t\tif [[ -s \"webs/webs_all.txt\" ]]; then\n\t\t\t\tnuclei -headless -id screenshot -V dir='screenshots' <webs/webs_all.txt 2>>\"$LOGFILE\"\n\t\t\tfi\n\t\telse\n\t\t\tif [[ -s \"webs/webs_all.txt\" ]]; then\n\t\t\t\taxiom-scan webs/webs_all.txt -m nuclei-screenshots -o screenshots \"$AXIOM_EXTRA_ARGS\" 2>>\"$LOGFILE\" >/dev/null\n\t\t\tfi\n\t\tfi\n\n\t\t# Extract and process URLs from web_full_info_uncommon.txt\n\t\tif [[ -s \".tmp/web_full_info_uncommon.txt\" ]]; then\n\t\t\tjq -r 'try .url' .tmp/web_full_info_uncommon.txt 2>/dev/null |\n\t\t\t\tgrep \"$domain\" |\n\t\t\t\tgrep -E '^((http|https):\\/\\/)?([a-zA-Z0-9]([a-zA-Z0-9\\-]*[a-zA-Z0-9])?\\.)+[a-zA-Z]{1,}(\\/.*)?' |\n\t\t\t\tsed 's/*.//' |\n\t\t\t\tanew -q .tmp/probed_uncommon_ports_tmp.txt\n\n\t\t\tjq -r 'try . | \"\\(.url) [\\(.status_code)] [\\(.title)] [\\(.webserver)] \\(.tech)\"' .tmp/web_full_info_uncommon.txt |\n\t\t\t\tgrep \"$domain\" |\n\t\t\t\tanew -q webs/web_full_info_uncommon_plain.txt\n\n\t\t\t# Update webs_full_info_uncommon.txt based on whether domain is IP\n\t\t\tif [[ $domain =~ ^[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+$ ]]; then\n\t\t\t\tcat .tmp/web_full_info_uncommon.txt 2>>\"$LOGFILE\" | anew -q webs/web_full_info_uncommon.txt\n\t\t\telse\n\t\t\t\tgrep \"$domain\" .tmp/web_full_info_uncommon.txt | anew -q webs/web_full_info_uncommon.txt\n\t\t\tfi\n\n\t\t\t# Count new websites\n\t\t\tif ! NUMOFLINES=$(anew webs/webs_uncommon_ports.txt <.tmp/probed_uncommon_ports_tmp.txt 2>>\"$LOGFILE\" | sed '/^$/d' | wc -l); then\n\t\t\t\tprintf \"%b[!] Failed to count new websites.%b\\n\" \"$bred\" \"$reset\"\n\t\t\t\tNUMOFLINES=0\n\t\t\tfi\n\n\t\t\t# Notify user\n\t\t\tnotification \"Uncommon web ports: ${NUMOFLINES} new websites\" \"good\"\n\n\t\t\t# Display new uncommon ports websites\n\t\t\tif [[ -s \"webs/webs_uncommon_ports.txt\" ]]; then\n\t\t\t\tcat \"webs/webs_uncommon_ports.txt\"\n\t\t\tfi\n\n\t\t\t# Update webs_all.txt\n\t\t\tcat webs/webs.txt webs/webs_uncommon_ports.txt 2>/dev/null | anew -q webs/webs_all.txt\n\n\t\t\tend_func \"Results are saved in $domain/screenshots folder\" \"${FUNCNAME[0]}\"\n\n\t\t\t# Send to proxy if conditions met\n\t\t\tif [[ $PROXY == true ]] && [[ -n $proxy_url ]] && [[ $(wc -l <webs/webs_uncommon_ports.txt) -le $DEEP_LIMIT2 ]]; then\n\t\t\t\tnotification \"Sending websites with uncommon ports to proxy\" \"info\"\n\t\t\t\tffuf -mc all -w webs/webs_uncommon_ports.txt -u FUZZ -replay-proxy \"$proxy_url\" 2>>\"$LOGFILE\" >/dev/null\n\t\t\tfi\n\t\tfi\n\telse\n\t\tif [[ $WEBSCREENSHOT == false ]]; then\n\t\t\tprintf \"\\n%b[%s] %s skipped due to mode or defined in reconftw.cfg.%b\\n\" \\\n\t\t\t\t\"$yellow\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"${FUNCNAME[0]}\" \"$reset\"\n\t\telse\n\t\t\tprintf \"%b[%s] %s has already been processed. To force execution, delete:\\n    %s/.%s%b\\n\\n\" \\\n\t\t\t\t\"$yellow\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"${FUNCNAME[0]}\" \"$called_fn_dir\" \".${FUNCNAME[0]}\" \"$reset\"\n\t\tfi\n\tfi\n\n}\n\nfunction virtualhosts() {\n\n\t# Create necessary directories\n\tif ! mkdir -p .tmp/virtualhosts virtualhosts webs; then\n\t\tprintf \"%b[!] Failed to create directories.%b\\n\" \"$bred\" \"$reset\"\n\t\treturn 1\n\tfi\n\n\t# Check if the function should run\n\tif { [[ ! -f \"$called_fn_dir/.${FUNCNAME[0]}\" ]] || [[ $DIFF == true ]]; } && [[ $VIRTUALHOSTS == true ]]; then\n\t\tstart_func \"${FUNCNAME[0]}\" \"Virtual Hosts Discovery\"\n\n\t\t# Combine webs.txt and webs_uncommon_ports.txt into webs_all.txt if it doesn't exist\n\t\tif [[ ! -s \"webs/webs_all.txt\" ]]; then\n\t\t\tcat webs/webs.txt webs/webs_uncommon_ports.txt 2>/dev/null | anew -q webs/webs_all.txt\n\t\tfi\n\n\t\t# Proceed only if webs_all.txt exists and is non-empty\n\t\tif [[ -s \"webs/webs_all.txt\" ]]; then\n\t\t\tif [[ $AXIOM != true ]]; then\n\t\t\t\t# Run ffuf using interlace\n\t\t\t\tinterlace -tL webs/webs_all.txt -threads \"$INTERLACE_THREADS\" \\\n\t\t\t\t\t-c \"ffuf -ac -t ${FFUF_THREADS} -rate ${FFUF_RATELIMIT} \\\n\t\t\t\t\t-H \\\"${HEADER}\\\" -H \\\"Host: FUZZ._cleantarget_\\\" \\\n\t\t\t\t\t-w ${fuzz_wordlist} -maxtime ${FFUF_MAXTIME} \\\n\t\t\t\t\t-u _target_ -of json -o _output_/_cleantarget_.json\" \\\n\t\t\t\t\t-o .tmp/virtualhosts 2>>\"$LOGFILE\" >/dev/null\n\t\t\telse\n\t\t\t\t# Run axiom-scan with nuclei-screenshots module\n\t\t\t\taxiom-scan webs/webs_all.txt -m nuclei-screenshots \\\n\t\t\t\t\t-o virtualhosts \"$AXIOM_EXTRA_ARGS\" 2>>\"$LOGFILE\" >/dev/null\n\t\t\tfi\n\n\t\t\t# Process ffuf output\n\t\t\twhile IFS= read -r sub; do\n\t\t\t\tsub_out=$(echo \"$sub\" | sed -e 's|^[^/]*//||' -e 's|/.*$||')\n\t\t\t\tjson_file=\"$dir/.tmp/virtualhosts/${sub_out}.json\"\n\t\t\t\ttxt_file=\"$dir/virtualhosts/${sub_out}.txt\"\n\n\t\t\t\tif [[ -s $json_file ]]; then\n\t\t\t\t\tjq -r 'try .results[] | \"\\(.status) \\(.length) \\(.url)\"' \"$json_file\" | sort | anew -q \"$txt_file\"\n\t\t\t\tfi\n\t\t\tdone\n\n\t\t\t# Merge all virtual host txt files into virtualhosts_full.txt\n\t\t\tfind \"$dir/virtualhosts/\" -type f -iname \"*.txt\" -exec cat {} + 2>>\"$LOGFILE\" | anew -q \"$dir/virtualhosts/virtualhosts_full.txt\"\n\n\t\t\tend_func \"Results are saved in $domain/virtualhosts/*subdomain*.txt\" \"${FUNCNAME[0]}\"\n\n\t\telse\n\t\t\tend_func \"No webs/webs_all.txt file found, virtualhosts skipped.\" \"${FUNCNAME[0]}\"\n\t\tfi\n\n\t\t# Optionally send to proxy if conditions are met\n\t\tif [[ $PROXY == true ]] && [[ -n $proxy_url ]] && [[ $(wc -l <webs/webs_uncommon_ports.txt) -le $DEEP_LIMIT2 ]]; then\n\t\t\tnotification \"Sending websites with uncommon ports to proxy\" \"info\"\n\t\t\tffuf -mc all -w webs/webs_uncommon_ports.txt -u FUZZ -replay-proxy \"$proxy_url\" 2>>\"$LOGFILE\" >/dev/null\n\t\tfi\n\n\telse\n\t\tif [[ $VIRTUALHOSTS == false ]]; then\n\t\t\tprintf \"\\n${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] ${FUNCNAME[0]} skipped in this mode or defined in reconftw.cfg ${reset}\\n\"\n\t\telse\n\t\t\tprintf \"${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] ${FUNCNAME[0]} has already been processed. To force execution, delete:\\n    $called_fn_dir/.${FUNCNAME[0]} ${reset}\\n\\n\"\n\t\tfi\n\tfi\n\n}\n\n###############################################################################################################\n############################################# HOST SCAN #######################################################\n###############################################################################################################\n\nfunction favicon() {\n\n\t# Create necessary directories\n\tif ! mkdir -p hosts .tmp/virtualhosts; then\n\t\tprintf \"%b[!] Failed to create directories.%b\\n\" \"$bred\" \"$reset\"\n\t\treturn 1\n\tfi\n\n\t# Check if the function should run\n\tif { [[ ! -f \"$called_fn_dir/.${FUNCNAME[0]}\" ]] || [[ $DIFF == true ]]; } &&\n\t\t[[ $FAVICON == true ]] &&\n\t\t! [[ $domain =~ ^[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+$ ]]; then\n\n\t\tstart_func \"${FUNCNAME[0]}\" \"Favicon IP Lookup\"\n\n\t\t# Navigate to the fav-up tool directory\n\t\tif ! pushd \"${tools}/fav-up\" >/dev/null; then\n\t\t\tprintf \"%b[!] Failed to change directory to %s in %s @ line %s.%b\\n\" \\\n\t\t\t\t\"$bred\" \"${tools}/fav-up\" \"${FUNCNAME[0]}\" \"${LINENO}\" \"$reset\"\n\t\t\treturn 1\n\t\tfi\n\n\t\t# Run the favicon IP lookup tool\n\t\tpython3 favUp.py -w \"$domain\" -sc -o favicontest.json 2>>\"$LOGFILE\" >/dev/null\n\n\t\t# Process the results if favicontest.json exists and is not empty\n\t\tif [[ -s \"favicontest.json\" ]]; then\n\t\t\tjq -r 'try .found_ips' favicontest.json 2>>\"$LOGFILE\" |\n\t\t\t\tgrep -v \"not-found\" >favicontest.txt\n\n\t\t\t# Replace '|' with newlines\n\t\t\tsed -i \"s/|/\\n/g\" favicontest.txt\n\n\t\t\t# Move the processed IPs to the hosts directory\n\t\t\tmv favicontest.txt \"$dir/hosts/favicontest.txt\" 2>>\"$LOGFILE\"\n\n\t\t\t# Remove the JSON file\n\t\t\trm -f favicontest.json 2>>\"$LOGFILE\"\n\t\tfi\n\n\t\t# Return to the original directory\n\t\tif ! popd >/dev/null; then\n\t\t\tprintf \"%b[!] Failed to return to the previous directory in %s @ line %s.%b\\n\" \\\n\t\t\t\t\"$bred\" \"${FUNCNAME[0]}\" \"${LINENO}\" \"$reset\"\n\t\tfi\n\n\t\tend_func \"Results are saved in hosts/favicontest.txt\" \"${FUNCNAME[0]}\"\n\n\telse\n\t\tif [[ $FAVICON == false ]]; then\n\t\t\tprintf \"\\n${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] ${FUNCNAME[0]} skipped due to configuration settings.${reset}\\n\"\n\t\telif [[ $domain =~ ^[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+$ ]]; then\n\t\t\t# Domain is an IP, do nothing\n\t\t\treturn\n\t\telse\n\t\t\tprintf \"${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] ${FUNCNAME[0]} has already been processed. To force execution, delete:\\n    $called_fn_dir/.${FUNCNAME[0]}${reset}\\n\\n\"\n\t\tfi\n\tfi\n\n}\n\nfunction portscan() {\n\n\t# Create necessary directories\n\tif ! mkdir -p .tmp subdomains hosts; then\n\t\tprintf \"%b[!] Failed to create directories.%b\\n\" \"$bred\" \"$reset\"\n\t\treturn 1\n\tfi\n\n\t# Check if the function should run\n\tif { [[ ! -f \"$called_fn_dir/.${FUNCNAME[0]}\" ]] || [[ $DIFF == true ]]; } && [[ $PORTSCANNER == true ]]; then\n\t\tstart_func \"${FUNCNAME[0]}\" \"Port scan\"\n\n\t\t# Determine if domain is IP address or domain name\n\t\tif ! [[ $domain =~ ^[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+$ ]]; then\n\t\t\t# Not an IP address\n\t\t\tif [[ -s \"subdomains/subdomains_dnsregs.json\" ]]; then\n\t\t\t\t# Extract host and IP from JSON\n\t\t\t\tjq -r 'try . | \"\\(.host) \\(.a[0])\"' \"subdomains/subdomains_dnsregs.json\" | anew -q .tmp/subs_ips.txt\n\t\t\tfi\n\n\t\t\tif [[ -s \".tmp/subs_ips.txt\" ]]; then\n\t\t\t\t# Reorder fields and sort\n\t\t\t\tawk '{ print $2 \" \" $1}' \".tmp/subs_ips.txt\" | sort -k2 -n | anew -q hosts/subs_ips_vhosts.txt\n\t\t\tfi\n\n\t\t\tif [[ -s \"hosts/subs_ips_vhosts.txt\" ]]; then\n\t\t\t\t# Extract IPs, filter out private ranges\n\t\t\t\tawk '{print $1}' \"hosts/subs_ips_vhosts.txt\" | grep -aEiv \"^(127|10|169\\.254|172\\.1[6-9]|172\\.2[0-9]|172\\.3[0-1]|192\\.168)\\.\" | grep -oE '\\b([0-9]{1,3}\\.){3}[0-9]{1,3}\\b' | anew -q hosts/ips.txt\n\t\t\tfi\n\n\t\telse\n\t\t\t# Domain is an IP address\n\t\t\tprintf \"%b\\n\" \"$domain\" | grep -aEiv \"^(127|10|169\\.254|172\\.1[6-9]|172\\.2[0-9]|172\\.3[0-1]|192\\.168)\\.\" | grep -oE '\\b([0-9]{1,3}\\.){3}[0-9]{1,3}\\b' | anew -q hosts/ips.txt\n\t\tfi\n\n\t\t# Check for CDN providers\n\t\tif [[ ! -s \"hosts/cdn_providers.txt\" ]]; then\n\t\t\tif [[ -s \"hosts/ips.txt\" ]]; then\n\t\t\t\tcdncheck -silent -resp -cdn -waf -nc <hosts/ips.txt 2>/dev/null >hosts/cdn_providers.txt\n\t\t\tfi\n\t\tfi\n\n\t\tif [[ -s \"hosts/ips.txt\" ]]; then\n\t\t\t# Remove CDN IPs\n\t\t\tcomm -23 <(sort -u hosts/ips.txt) <(cut -d'[' -f1 hosts/cdn_providers.txt | sed 's/[[:space:]]*$//' | sort -u) |\n\t\t\t\tgrep -aEiv \"^(127|10|169\\.254|172\\.1[6-9]|172\\.2[0-9]|172\\.3[0-1]|192\\.168)\\.\" | grep -oE '\\b([0-9]{1,3}\\.){3}[0-9]{1,3}\\b' |\n\t\t\t\tsort -u | anew -q .tmp/ips_nocdn.txt\n\t\tfi\n\n\t\t# Display resolved IPs without CDN\n\t\tprintf \"%b\\n[%s] Resolved IP addresses (No CDN):%b\\n\\n\" \"$bblue\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"$reset\"\n\t\tif [[ -s \".tmp/ips_nocdn.txt\" ]]; then\n\t\t\tsort \".tmp/ips_nocdn.txt\"\n\t\tfi\n\n\t\tprintf \"%b\\n[%s] Scanning ports...%b\\n\\n\" \"$bblue\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"$reset\"\n\n\t\tips_file=\"${dir}/hosts/ips.txt\"\n\n\t\tif [[ $PORTSCAN_PASSIVE == true ]]; then\n\t\t\tif [[ ! -f $ips_file ]]; then\n\t\t\t\tprintf \"%b[!] File %s does not exist.%b\\n\" \"$bred\" \"$ips_file\" \"$reset\"\n\t\t\telse\n\t\t\t\tjson_array=()\n\t\t\t\twhile IFS= read -r cip; do\n\t\t\t\t\tif ! json_result=$(curl -s \"https://internetdb.shodan.io/${cip}\"); then\n\t\t\t\t\t\tprintf \"%b[!] Failed to retrieve data for IP %s.%b\\n\" \"$bred\" \"$cip\" \"$reset\"\n\t\t\t\t\telse\n\t\t\t\t\t\tjson_array+=(\"$json_result\")\n\t\t\t\t\tfi\n\t\t\t\tdone <\"$ips_file\"\n\t\t\t\tformatted_json=\"[\"\n\t\t\t\tfor ((i = 0; i < ${#json_array[@]}; i++)); do\n\t\t\t\t\tformatted_json+=\"$(echo \"${json_array[i]}\" | tr -d '\\n')\"\n\t\t\t\t\tif [ $i -lt $((${#json_array[@]} - 1)) ]; then\n\t\t\t\t\t\tformatted_json+=\", \"\n\t\t\t\t\tfi\n\t\t\t\tdone\n\t\t\t\tformatted_json+=\"]\"\n\t\t\t\tif ! echo \"$formatted_json\" >\"${dir}/hosts/portscan_shodan.txt\"; then\n\t\t\t\t\tprintf \"%b[!] Failed to write portscan_shodan.txt.%b\\n\" \"$bred\" \"$reset\"\n\t\t\t\tfi\n\t\t\tfi\n\t\tfi\n\n\t\tif [[ $PORTSCAN_PASSIVE == true ]] && [[ ! -f \"hosts/portscan_passive.txt\" ]] && [[ -s \".tmp/ips_nocdn.txt\" ]]; then\n\t\t\tsmap -iL .tmp/ips_nocdn.txt >hosts/portscan_passive.txt\n\t\tfi\n\n\t\tif [[ $PORTSCAN_ACTIVE == true ]]; then\n\t\t\tif [[ $AXIOM != true ]]; then\n\t\t\t\tif [[ -s \".tmp/ips_nocdn.txt\" ]]; then\n\t\t\t\t\t\"$SUDO\" nmap $PORTSCAN_ACTIVE_OPTIONS -iL .tmp/ips_nocdn.txt -oA hosts/portscan_active 2>>\"$LOGFILE\" >/dev/null\n\t\t\t\tfi\n\t\t\telse\n\t\t\t\tif [[ -s \".tmp/ips_nocdn.txt\" ]]; then\n\t\t\t\t\taxiom-scan .tmp/ips_nocdn.txt -m nmapx $PORTSCAN_ACTIVE_OPTIONS \\\n\t\t\t\t\t\t-oA hosts/portscan_active $AXIOM_EXTRA_ARGS 2>>\"$LOGFILE\" >/dev/null\n\t\t\t\tfi\n\t\t\tfi\n\t\tfi\n\n\t\tif [[ -s \"hosts/portscan_active.xml\" ]]; then\n\t\t\tnmapurls <hosts/portscan_active.xml 2>>\"$LOGFILE\" | anew -q hosts/webs.txt\n\t\tfi\n\n\t\tif [[ -s \"hosts/webs.txt\" ]]; then\n\t\t\tif ! NUMOFLINES=$(wc -l <hosts/webs.txt); then\n\t\t\t\tprintf \"%b[!] Failed to count lines in hosts/webs.txt.%b\\n\" \"$bred\" \"$reset\"\n\t\t\t\tNUMOFLINES=0\n\t\t\tfi\n\t\t\tnotification \"Webs detected from port scan: ${NUMOFLINES} new websites\" \"good\"\n\t\t\tcat hosts/webs.txt\n\t\tfi\n\n\t\tend_func \"Results are saved in hosts/portscan_[passive|active|shodan].[txt|xml]\" \"${FUNCNAME[0]}\"\n\n\telse\n\t\tif [[ $PORTSCANNER == false ]]; then\n\t\t\tprintf \"\\n%b[%s] %s skipped due to mode or defined in reconftw.cfg.%b\\n\" \\\n\t\t\t\t\"$yellow\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"${FUNCNAME[0]}\" \"$reset\"\n\t\telse\n\t\t\tprintf \"%b[%s] %s has already been processed. To force execution, delete:\\n    %s/.%s%b\\n\\n\" \\\n\t\t\t\t\"$yellow\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"${FUNCNAME[0]}\" \"$called_fn_dir\" \".${FUNCNAME[0]}\" \"$reset\"\n\t\tfi\n\tfi\n\n}\n\nfunction cdnprovider() {\n\n\t# Create necessary directories\n\tif ! mkdir -p .tmp subdomains hosts; then\n\t\tprintf \"%b[!] Failed to create directories.%b\\n\" \"$bred\" \"$reset\"\n\t\treturn 1\n\tfi\n\n\t# Check if the function should run\n\tif { [[ ! -f \"$called_fn_dir/.${FUNCNAME[0]}\" ]] || [[ $DIFF == true ]]; } &&\n\t\t[[ $CDN_IP == true ]] &&\n\t\t! [[ $domain =~ ^[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+$ ]]; then\n\n\t\tstart_func \"${FUNCNAME[0]}\" \"CDN Provider Check\"\n\n\t\t# Check if subdomains_dnsregs.json exists and is not empty\n\t\tif [[ -s \"subdomains/subdomains_dnsregs.json\" ]]; then\n\t\t\t# Extract IPs from .a[] fields, exclude private IPs, extract IPs, sort uniquely\n\t\t\tjq -r 'try . | .a[]' \"subdomains/subdomains_dnsregs.json\" |\n\t\t\t\tgrep -aEiv \"^(127|10|169\\.254|172\\.(1[6-9]|2[0-9]|3[01])|192\\.168)\\.\" |\n\t\t\t\tgrep -oE \"\\b([0-9]{1,3}\\.){3}[0-9]{1,3}\\b\" |\n\t\t\t\tsort -u >.tmp/ips_cdn.txt\n\t\tfi\n\n\t\t# Check if ips_cdn.txt exists and is not empty\n\t\tif [[ -s \".tmp/ips_cdn.txt\" ]]; then\n\t\t\t# Run cdncheck on the IPs and save to cdn_providers.txt\n\t\t\tcdncheck -silent -resp -nc <.tmp/ips_cdn.txt | anew -q \"$dir/hosts/cdn_providers.txt\"\n\t\tfi\n\n\t\tend_func \"Results are saved in hosts/cdn_providers.txt\" \"${FUNCNAME[0]}\"\n\n\telse\n\t\tif [[ $CDN_IP == false ]]; then\n\t\t\tprintf \"\\n${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] ${FUNCNAME[0]} skipped due to configuration settings.${reset}\\n\"\n\t\telif [[ $domain =~ ^[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+$ ]]; then\n\t\t\t# Domain is an IP, do nothing\n\t\t\treturn\n\t\telse\n\t\t\tprintf \"${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] ${FUNCNAME[0]} has already been processed. To force execution, delete:\\n    $called_fn_dir/.${FUNCNAME[0]}${reset}\\n\\n\"\n\t\tfi\n\tfi\n\n}\n\n###############################################################################################################\n############################################# WEB SCAN ########################################################\n###############################################################################################################\n\nfunction waf_checks() {\n\n\t# Create necessary directories\n\tif ! mkdir -p .tmp webs; then\n\t\tprintf \"%b[!] Failed to create directories.%b\\n\" \"$bred\" \"$reset\"\n\t\treturn 1\n\tfi\n\n\t# Check if the function should run\n\tif { [[ ! -f \"$called_fn_dir/.${FUNCNAME[0]}\" ]] || [[ $DIFF == true ]]; } && [[ $WAF_DETECTION == true ]]; then\n\t\tstart_func \"${FUNCNAME[0]}\" \"Website's WAF Detection\"\n\n\t\t# Combine webs.txt and webs_uncommon_ports.txt into webs_all.txt if it doesn't exist\n\t\tif [[ ! -s \"webs/webs_all.txt\" ]]; then\n\t\t\tcat webs/webs.txt webs/webs_uncommon_ports.txt 2>/dev/null | anew -q webs/webs_all.txt\n\t\tfi\n\n\t\t# Proceed only if webs_all.txt exists and is non-empty\n\t\tif [[ -s \"webs/webs_all.txt\" ]]; then\n\t\t\tif [[ $AXIOM != true ]]; then\n\t\t\t\t# Run wafw00f on webs_all.txt\n\t\t\t\twafw00f -i \"webs/webs_all.txt\" -o \".tmp/wafs.txt\" 2>>\"$LOGFILE\" >/dev/null\n\t\t\telse\n\t\t\t\t# Run axiom-scan with wafw00f module on webs_all.txt\n\t\t\t\taxiom-scan \"webs/webs_all.txt\" -m wafw00f -o \".tmp/wafs.txt\" \"$AXIOM_EXTRA_ARGS\" 2>>\"$LOGFILE\" >/dev/null\n\t\t\tfi\n\n\t\t\t# Process wafs.txt if it exists and is not empty\n\t\t\tif [[ -s \".tmp/wafs.txt\" ]]; then\n\t\t\t\t# Format the wafs.txt file\n\t\t\t\tsed -e 's/^[ \\t]*//' -e 's/ \\+ /\\t/g' -e '/(None)/d' \".tmp/wafs.txt\" | tr -s \"\\t\" \";\" >\"webs/webs_wafs.txt\"\n\n\t\t\t\t# Count the number of websites protected by WAF\n\t\t\t\tif ! NUMOFLINES=$(sed '/^$/d' \"webs/webs_wafs.txt\" 2>>\"$LOGFILE\" | wc -l); then\n\t\t\t\t\tprintf \"%b[!] Failed to count lines in webs_wafs.txt.%b\\n\" \"$bred\" \"$reset\"\n\t\t\t\t\tNUMOFLINES=0\n\t\t\t\tfi\n\n\t\t\t\t# Send a notification about the number of WAF-protected websites\n\t\t\t\tnotification \"${NUMOFLINES} websites protected by WAF\" \"info\"\n\n\t\t\t\t# End the function with a success message\n\t\t\t\tend_func \"Results are saved in webs/webs_wafs.txt\" \"${FUNCNAME[0]}\"\n\t\t\telse\n\t\t\t\t# End the function indicating no results were found\n\t\t\t\tend_func \"No results found\" \"${FUNCNAME[0]}\"\n\t\t\tfi\n\t\telse\n\t\t\t# End the function indicating there are no websites to scan\n\t\t\tend_func \"No websites to scan\" \"${FUNCNAME[0]}\"\n\t\tfi\n\telse\n\t\t# Handle cases where WAF_DETECTION is false or the function has already been processed\n\t\tif [[ $WAF_DETECTION == false ]]; then\n\t\t\tprintf \"\\n${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] ${FUNCNAME[0]} skipped due to configuration settings.${reset}\\n\"\n\t\telif [[ $domain =~ ^[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+$ ]]; then\n\t\t\t# Domain is an IP address; skip the function\n\t\t\treturn\n\t\telse\n\t\t\tprintf \"${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] ${FUNCNAME[0]} has already been processed. To force execution, delete:\\n    $called_fn_dir/.${FUNCNAME[0]}${reset}\\n\\n\"\n\t\tfi\n\tfi\n\n}\n\nfunction nuclei_check() {\n\n\t# Create necessary directories\n\tif ! mkdir -p .tmp webs subdomains nuclei_output; then\n\t\tprintf \"%b[!] Failed to create directories.%b\\n\" \"$bred\" \"$reset\"\n\t\treturn 1\n\tfi\n\n\t# Check if the function should run\n\tif { [[ ! -f \"$called_fn_dir/.${FUNCNAME[0]}\" ]] || [[ $DIFF == true ]]; } && [[ $NUCLEICHECK == true ]]; then\n\t\tstart_func \"${FUNCNAME[0]}\" \"Templates-based Web Scanner\"\n\n\t\t# Update nuclei templates\n\t\tnuclei -update 2>>\"$LOGFILE\" >/dev/null\n\n\t\t# Handle multi mode and initialize subdomains.txt if necessary\n\t\tif [[ -n $multi ]] && [[ ! -f \"$dir/subdomains/subdomains.txt\" ]]; then\n\t\t\tprintf \"%b\\n\" \"$domain\" >\"$dir/subdomains/subdomains.txt\"\n\t\t\ttouch webs/webs.txt webs/webs_uncommon_ports.txt\n\t\tfi\n\n\t\t# Combine webs.txt and webs_uncommon_ports.txt into webs_all.txt if it doesn't exist\n\t\tif [[ ! -s \"webs/webs_all.txt\" ]]; then\n\t\t\tcat webs/webs.txt webs/webs_uncommon_ports.txt 2>/dev/null | anew -q webs/webs_all.txt\n\t\tfi\n\n\t\t# Combine url_extract_nodupes.txt, subdomains.txt, and webs_all.txt into webs_subs.txt if it doesn't exist\n\t\tif [[ ! -s \".tmp/webs_subs.txt\" ]]; then\n\t\t\tcat webs/url_extract_nodupes.txt subdomains/subdomains.txt webs/webs_all.txt 2>>\"$LOGFILE\" | anew -q .tmp/webs_subs.txt\n\t\tfi\n\n\t\t# If fuzzing_full.txt exists, process it and create webs_fuzz.txt\n\t\tif [[ -s \"$dir/fuzzing/fuzzing_full.txt\" ]]; then\n\t\t\tgrep \"^200\" \"$dir/fuzzing/fuzzing_full.txt\" | cut -d \" \" -f3 | anew -q .tmp/webs_fuzz.txt\n\t\tfi\n\n\t\t# Combine webs_subs.txt and webs_fuzz.txt into webs_nuclei.txt and duplicate it\n\t\tcat .tmp/webs_subs.txt .tmp/webs_fuzz.txt 2>>\"$LOGFILE\" | anew -q .tmp/webs_nuclei.txt | tee -a webs/webs_nuclei.txt\n\n\t\t# Check if AXIOM is enabled\n\t\tif [[ $AXIOM != true ]]; then\n\t\t\t# Split severity levels into an array\n\t\t\tIFS=',' read -ra severity_array <<<\"$NUCLEI_SEVERITY\"\n\n\t\t\tfor crit in \"${severity_array[@]}\"; do\n\t\t\t\tprintf \"${yellow}\\n[$(date +'%Y-%m-%d %H:%M:%S')] Running: Nuclei Severity: $crit ${reset}\\n\\n\"\n\n\t\t\t\t# Run nuclei for each severity level\n\t\t\t\tnuclei $NUCLEI_FLAGS -severity \"$crit\" -nh -rl \"$NUCLEI_RATELIMIT\" \"$NUCLEI_EXTRA_ARGS\" -o \"nuclei_output/${crit}.txt\" <.tmp/webs_nuclei.txt\n\t\t\tdone\n\t\t\tprintf \"\\n\\n\"\n\t\telse\n\t\t\t# Check if webs_nuclei.txt exists and is not empty\n\t\t\tif [[ -s \".tmp/webs_nuclei.txt\" ]]; then\n\t\t\t\t# Split severity levels into an array\n\t\t\t\tIFS=',' read -ra severity_array <<<\"$NUCLEI_SEVERITY\"\n\n\t\t\t\tfor crit in \"${severity_array[@]}\"; do\n\t\t\t\t\tprintf \"${yellow}\\n[$(date +'%Y-%m-%d %H:%M:%S')] Running: Axiom Nuclei Severity: $crit. Check results in nuclei_output folder.${reset}\\n\\n\"\n\t\t\t\t\t# Run axiom-scan with nuclei module for each severity level\n\t\t\t\t\taxiom-scan .tmp/webs_nuclei.txt -m nuclei \\\n\t\t\t\t\t\t--nuclei-templates \"$NUCLEI_TEMPLATES_PATH\" \\\n\t\t\t\t\t\t-severity \"$crit\" -nh -rl \"$NUCLEI_RATELIMIT\" \\\n\t\t\t\t\t\t\"$NUCLEI_EXTRA_ARGS\" -o \"nuclei_output/${crit}.txt\" \"$AXIOM_EXTRA_ARGS\" 2>>\"$LOGFILE\" >/dev/null\n\n\t\t\t\t\t# Display the results if the output file exists and is not empty\n\t\t\t\t\tif [[ -s \"nuclei_output/${crit}.txt\" ]]; then\n\t\t\t\t\t\tcat \"nuclei_output/${crit}.txt\"\n\t\t\t\t\tfi\n\t\t\t\tdone\n\t\t\t\tprintf \"\\n\\n\"\n\t\t\tfi\n\t\tfi\n\n\t\tend_func \"Results are saved in $domain/nuclei_output folder\" \"${FUNCNAME[0]}\"\n\telse\n\t\t# Handle cases where NUCLEICHECK is false or the function has already been processed\n\t\tif [[ $NUCLEICHECK == false ]]; then\n\t\t\tprintf \"\\n${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] ${FUNCNAME[0]} skipped due to configuration settings.${reset}\\n\"\n\t\telif [[ $domain =~ ^[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+$ ]]; then\n\t\t\t# Domain is an IP address; skip the function\n\t\t\treturn\n\t\telse\n\t\t\tprintf \"${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] ${FUNCNAME[0]} has already been processed. To force execution, delete:\\n    $called_fn_dir/.${FUNCNAME[0]}${reset}\\n\\n\"\n\t\tfi\n\tfi\n\n}\n\nfunction fuzz() {\n\n\t# Create necessary directories\n\tmkdir -p .tmp/fuzzing webs fuzzing nuclei_output\n\n\t# Check if the function should run\n\tif { [[ ! -f \"$called_fn_dir/.${FUNCNAME[0]}\" ]] || [[ $DIFF == true ]]; } && [[ $FUZZ == true ]] &&\n\t\t! [[ $domain =~ ^[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+$ ]]; then\n\n\t\tstart_func \"${FUNCNAME[0]}\" \"Web Directory Fuzzing\"\n\n\t\t# Handle multi mode and initialize subdomains.txt if necessary\n\t\tif [[ -n $multi ]] && [[ ! -f \"$dir/webs/webs.txt\" ]]; then\n\t\t\tif ! printf \"%b\\n\" \"$domain\" >\"$dir/webs/webs.txt\"; then\n\t\t\t\tprintf \"%b[!] Failed to create webs.txt.%b\\n\" \"$bred\" \"$reset\"\n\t\t\tfi\n\t\t\tif ! touch webs/webs_uncommon_ports.txt; then\n\t\t\t\tprintf \"%b[!] Failed to initialize webs_uncommon_ports.txt.%b\\n\" \"$bred\" \"$reset\"\n\t\t\tfi\n\t\tfi\n\n\t\t# Combine webs.txt and webs_uncommon_ports.txt into webs_all.txt if it doesn't exist\n\t\tif [[ ! -s \"webs/webs_all.txt\" ]]; then\n\t\t\tcat webs/webs.txt webs/webs_uncommon_ports.txt 2>/dev/null | anew -q webs/webs_all.txt\n\t\tfi\n\n\t\tif [[ -s \"webs/webs_all.txt\" ]]; then\n\t\t\tif [[ $AXIOM != true ]]; then\n\t\t\t\tinterlace -tL webs/webs_all.txt -threads ${INTERLACE_THREADS} -c \"ffuf ${FFUF_FLAGS} -t ${FFUF_THREADS} -rate ${FFUF_RATELIMIT} -H \\\"${HEADER}\\\" -w ${fuzz_wordlist} -maxtime ${FFUF_MAXTIME} -u _target_/FUZZ -o _output_/_cleantarget_.json\" -o $dir/.tmp/fuzzing 2>>\"$LOGFILE\" >/dev/null\n\t\t\t\tfor sub in $(cat webs/webs_all.txt); do\n\t\t\t\t\tsub_out=$(echo $sub | sed -e 's|^[^/]*//||' -e 's|/.*$||')\n\n\t\t\t\t\tpushd \"${tools}/ffufPostprocessing\" >/dev/null || {\n\t\t\t\t\t\techo \"Failed to cd directory in ${FUNCNAME[0]} @ line ${LINENO}\"\n\t\t\t\t\t}\n\t\t\t\t\t./ffufPostprocessing -result-file $dir/.tmp/fuzzing/${sub_out}.json -overwrite-result-file 2>>\"$LOGFILE\" >/dev/null\n\t\t\t\t\tpopd >/dev/null || {\n\t\t\t\t\t\techo \"Failed to popd in ${FUNCNAME[0]} @ line ${LINENO}\"\n\t\t\t\t\t}\n\n\t\t\t\t\t[ -s \"$dir/.tmp/fuzzing/${sub_out}.json\" ] && cat $dir/.tmp/fuzzing/${sub_out}.json | jq -r 'try .results[] | \"\\(.status) \\(.length) \\(.url)\"' | sort -k1 | anew -q $dir/fuzzing/${sub_out}.txt\n\t\t\t\tdone\n\t\t\t\tfind $dir/fuzzing/ -type f -iname \"*.txt\" -exec cat {} + 2>>\"$LOGFILE\" | sort -k1 | anew -q $dir/fuzzing/fuzzing_full.txt\n\t\t\telse\n\t\t\t\taxiom-exec \"mkdir -p /home/op/lists/seclists/Discovery/Web-Content/\" &>/dev/null\n\t\t\t\taxiom-exec \"wget -q -O - ${fuzzing_remote_list} > /home/op/lists/fuzz_wordlist.txt\" &>/dev/null\n\t\t\t\taxiom-exec \"wget -q -O - ${fuzzing_remote_list} > /home/op/lists/seclists/Discovery/Web-Content/big.txt\" &>/dev/null\n\t\t\t\taxiom-scan webs/webs_all.txt -m ffuf_base -H \"${HEADER}\" $FFUF_FLAGS -s -maxtime $FFUF_MAXTIME -o $dir/.tmp/ffuf-content.json $AXIOM_EXTRA_ARGS 2>>\"$LOGFILE\" >/dev/null\n\t\t\t\tpushd \"${tools}/ffufPostprocessing\" >/dev/null || {\n\t\t\t\t\techo \"Failed to cd directory in ${FUNCNAME[0]} @ line ${LINENO}\"\n\t\t\t\t}\n\t\t\t\t[ -s \"$dir/.tmp/ffuf-content.json\" ] && ./ffufPostprocessing -result-file $dir/.tmp/ffuf-content.json -overwrite-result-file 2>>\"$LOGFILE\" >/dev/null\n\t\t\t\tpopd >/dev/null || {\n\t\t\t\t\techo \"Failed to popd in ${FUNCNAME[0]} @ line ${LINENO}\"\n\t\t\t\t}\n\t\t\t\tfor sub in $(cat webs/webs_all.txt); do\n\t\t\t\t\tsub_out=$(echo $sub | sed -e 's|^[^/]*//||' -e 's|/.*$||')\n\t\t\t\t\t[ -s \"$dir/.tmp/ffuf-content.json\" ] && cat .tmp/ffuf-content.json | jq -r 'try .results[] | \"\\(.status) \\(.length) \\(.url)\"' | grep $sub | sort -k1 | anew -q fuzzing/${sub_out}.txt\n\t\t\t\tdone\n\t\t\t\tfind $dir/fuzzing/ -type f -iname \"*.txt\" -exec cat {} + 2>>\"$LOGFILE\" | sort -k1 | anew -q $dir/fuzzing/fuzzing_full.txt\n\t\t\tfi\n\t\t\tend_func \"Results are saved in $domain/fuzzing/*subdomain*.txt\" ${FUNCNAME[0]}\n\t\telse\n\t\t\tend_func \"No $domain/web/webs.txts file found, fuzzing skipped \" ${FUNCNAME[0]}\n\t\tfi\n\n\t\tend_func \"Results are saved in $domain/nuclei_output folder\" \"${FUNCNAME[0]}\"\n\telse\n\t\tif [[ $FUZZ == false ]]; then\n\t\t\tprintf \"\\n${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] ${FUNCNAME[0]} skipped in this mode or defined in reconftw.cfg ${reset}\\n\"\n\t\telse\n\t\t\tprintf \"${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] ${FUNCNAME[0]} is already processed, to force executing ${FUNCNAME[0]} delete\\n    $called_fn_dir/.${FUNCNAME[0]} ${reset}\\n\\n\"\n\t\tfi\n\tfi\n\n}\n\nfunction iishortname() {\n\n\t# Check if the function should run\n\tif { [[ ! -f \"$called_fn_dir/.${FUNCNAME[0]}\" ]] || [[ $DIFF == true ]]; } && [[ $IIS_SHORTNAME == true ]]; then\n\t\tstart_func \"${FUNCNAME[0]}\" \"IIS Shortname Scanner\"\n\n\t\t# Ensure nuclei_output/info.txt exists and is not empty\n\t\tif [[ -s \"nuclei_output/info.txt\" ]]; then\n\t\t\t# Extract IIS version information and save to .tmp/iis_sites.txt\n\t\t\tgrep \"iis-version\" \"nuclei_output/info.txt\" | cut -d \" \" -f4 >.tmp/iis_sites.txt\n\t\tfi\n\n\t\t# Proceed only if iis_sites.txt exists and is non-empty\n\t\tif [[ -s \".tmp/iis_sites.txt\" ]]; then\n\t\t\t# Create necessary directories\n\t\t\tmkdir -p \"$dir/vulns/iis-shortname-shortscan/\" \"$dir/vulns/iis-shortname-sns/\"\n\n\t\t\t# Run shortscan using interlace\n\t\t\tinterlace -tL .tmp/iis_sites.txt -threads \"$INTERLACE_THREADS\" \\\n\t\t\t\t-c \"shortscan _target_ -F -s -p 1 > _output_/_cleantarget_.txt\" \\\n\t\t\t\t-o \"$dir/vulns/iis-shortname-shortscan/\" 2>>\"$LOGFILE\" >/dev/null\n\n\t\t\t# Remove non-vulnerable shortscan results\n\t\t\tfind \"$dir/vulns/iis-shortname-shortscan/\" -type f -iname \"*.txt\" -print0 |\n\t\t\t\txargs --null grep -Z -L 'Vulnerable: Yes' |\n\t\t\t\txargs --null rm 2>>\"$LOGFILE\" >/dev/null\n\n\t\t\t# Run sns using interlace\n\t\t\tinterlace -tL .tmp/iis_sites.txt -threads \"$INTERLACE_THREADS\" \\\n\t\t\t\t-c \"sns -u _target_ > _output_/_cleantarget_.txt\" \\\n\t\t\t\t-o \"$dir/vulns/iis-shortname-sns/\" 2>>\"$LOGFILE\" >/dev/null\n\n\t\t\t# Remove non-vulnerable sns results\n\t\t\tfind \"$dir/vulns/iis-shortname-sns/\" -type f -iname \"*.txt\" -print0 |\n\t\t\t\txargs --null grep -Z 'Target is not vulnerable' |\n\t\t\t\txargs --null rm 2>>\"$LOGFILE\" >/dev/null\n\n\t\tfi\n\t\tend_func \"Results are saved in vulns/iis-shortname/\" \"${FUNCNAME[0]}\"\n\telse\n\t\t# Handle cases where IIS_SHORTNAME is false or the function has already been processed\n\t\tif [[ $IIS_SHORTNAME == false ]]; then\n\t\t\tprintf \"\\n${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] ${FUNCNAME[0]} skipped due to configuration settings.${reset}\\n\"\n\t\telif [[ $domain =~ ^[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+$ ]]; then\n\t\t\t# Domain is an IP address; skip the function\n\t\t\treturn\n\t\telse\n\t\t\tprintf \"${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] ${FUNCNAME[0]} has already been processed. To force execution, delete:\\n    $called_fn_dir/.${FUNCNAME[0]}${reset}\\n\\n\"\n\t\tfi\n\tfi\n\n}\n\nfunction cms_scanner() {\n\n\t# Create necessary directories\n\tif ! mkdir -p .tmp/fuzzing webs cms; then\n\t\tprintf \"%b[!] Failed to create directories.%b\\n\" \"$bred\" \"$reset\"\n\t\treturn 1\n\tfi\n\n\t# Check if the function should run\n\tif { [[ ! -f \"$called_fn_dir/.${FUNCNAME[0]}\" ]] || [[ $DIFF == true ]]; } && [[ $CMS_SCANNER == true ]] &&\n\t\t! [[ $domain =~ ^[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+$ ]]; then\n\n\t\tstart_func \"${FUNCNAME[0]}\" \"CMS Scanner\"\n\n\t\trm -rf \"$dir/cms/\"*\n\n\t\t# Handle multi mode and initialize webs.txt if necessary\n\t\tif [[ -n $multi ]] && [[ ! -f \"$dir/webs/webs.txt\" ]]; then\n\t\t\tprintf \"%b\\n\" \"$domain\" >\"$dir/webs/webs.txt\"\n\t\t\ttouch webs/webs_uncommon_ports.txt\n\t\tfi\n\n\t\t# Combine webs.txt and webs_uncommon_ports.txt into webs_all.txt if it doesn't exist\n\t\tif [[ ! -s \"webs/webs_all.txt\" ]]; then\n\t\t\tcat webs/webs.txt webs/webs_uncommon_ports.txt 2>/dev/null | anew -q webs/webs_all.txt\n\t\tfi\n\n\t\t# Combine webs_all.txt into .tmp/cms.txt as a comma-separated list\n\t\tif [[ -s \"webs/webs_all.txt\" ]]; then\n\t\t\ttr '\\n' ',' <webs/webs_all.txt >.tmp/cms.txt 2>>\"$LOGFILE\"\n\t\telse\n\t\t\tend_func \"No webs/webs_all.txt file found, cms scanner skipped.\" \"${FUNCNAME[0]}\"\n\t\t\treturn\n\t\tfi\n\n\t\t# Run CMSeeK with timeout\n\t\tif ! timeout -k 1m \"${CMSSCAN_TIMEOUT}s\" python3 \"${tools}/CMSeeK/cmseek.py\" -l .tmp/cms.txt --batch -r &>>\"$LOGFILE\"; then\n\t\t\texit_status=$?\n\t\t\tif [[ ${exit_status} -eq 124 || ${exit_status} -eq 137 ]]; then\n\t\t\t\techo \"TIMEOUT cmseek.py - investigate manually for $dir\" >>\"$LOGFILE\"\n\t\t\t\tend_func \"TIMEOUT cmseek.py - investigate manually for $dir\" \"${FUNCNAME[0]}\"\n\t\t\t\treturn\n\t\t\telif [[ ${exit_status} -ne 0 ]]; then\n\t\t\t\techo \"ERROR cmseek.py - investigate manually for $dir\" >>\"$LOGFILE\"\n\t\t\t\tend_func \"ERROR cmseek.py - investigate manually for $dir\" \"${FUNCNAME[0]}\"\n\t\t\t\treturn\n\t\t\tfi\n\t\tfi\n\n\t\t# Process CMSeeK results\n\t\twhile IFS= read -r sub; do\n\t\t\tsub_out=$(echo \"$sub\" | sed -e 's|^[^/]*//||' -e 's|/.*$||')\n\t\t\tcms_json_path=\"${tools}/CMSeeK/Result/${sub_out}/cms.json\"\n\n\t\t\tif [[ -s $cms_json_path ]]; then\n\t\t\t\tcms_id=$(jq -r 'try .cms_id' \"$cms_json_path\")\n\t\t\t\tif [[ -n $cms_id ]]; then\n\t\t\t\t\tmv -f \"${tools}/CMSeeK/Result/${sub_out}\" \"$dir/cms/\" 2>>\"$LOGFILE\"\n\t\t\t\telse\n\t\t\t\t\trm -rf \"${tools}/CMSeeK/Result/${sub_out}\" 2>>\"$LOGFILE\"\n\t\t\t\tfi\n\t\t\tfi\n\t\tdone <\"webs/webs_all.txt\"\n\n\t\tend_func \"Results are saved in $domain/cms/*subdomain* folder\" \"${FUNCNAME[0]}\"\n\telse\n\t\t# Handle cases where CMS_SCANNER is false or the function has already been processed\n\t\tif [[ $CMS_SCANNER == false ]]; then\n\t\t\tprintf \"\\n${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] ${FUNCNAME[0]} skipped due to configuration settings.${reset}\\n\"\n\t\telif [[ $domain =~ ^[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+$ ]]; then\n\t\t\t# Domain is an IP address; skip the function\n\t\t\treturn\n\t\telse\n\t\t\tprintf \"${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] ${FUNCNAME[0]} has already been processed. To force execution, delete:\\n    $called_fn_dir/.${FUNCNAME[0]}${reset}\\n\\n\"\n\t\tfi\n\tfi\n}\n\nfunction urlchecks() {\n\n\t# Create necessary directories\n\tif ! mkdir -p .tmp webs; then\n\t\tprintf \"%b[!] Failed to create directories.%b\\n\" \"$bred\" \"$reset\"\n\t\treturn 1\n\tfi\n\n\t# Check if the function should run\n\tif { [[ ! -f \"$called_fn_dir/.${FUNCNAME[0]}\" ]] || [[ $DIFF == true ]]; } && [[ $URL_CHECK == true ]]; then\n\t\tstart_func \"${FUNCNAME[0]}\" \"URL Extraction\"\n\n\t\t# Combine webs.txt and webs_uncommon_ports.txt if webs_all.txt doesn't exist\n\t\tif [[ ! -s \"webs/webs_all.txt\" ]]; then\n\t\t\tcat webs/webs.txt webs/webs_uncommon_ports.txt 2>/dev/null | anew -q webs/webs_all.txt\n\t\tfi\n\n\t\tif [[ -s \"webs/webs_all.txt\" ]]; then\n\n\t\t\tif [[ $URL_CHECK_PASSIVE == true ]]; then\n\t\t\t\turlfinder -d $domain -o .tmp/url_extract_tmp.txt 2>>\"$LOGFILE\" >/dev/null\n\t\t\t\tif [[ -s $GITHUB_TOKENS ]]; then\n\t\t\t\t\tgithub-endpoints -q -k -d \"$domain\" -t \"$GITHUB_TOKENS\" -o .tmp/github-endpoints.txt 2>>\"$LOGFILE\" >/dev/null\n\t\t\t\t\tif [[ -s \".tmp/github-endpoints.txt\" ]]; then\n\t\t\t\t\t\tcat .tmp/github-endpoints.txt | anew -q .tmp/url_extract_tmp.txt\n\t\t\t\t\tfi\n\t\t\t\tfi\n\t\t\tfi\n\n\t\t\tif [[ $AXIOM != true ]]; then\n\t\t\t\tdiff_webs=$(diff <(sort -u .tmp/probed_tmp.txt 2>>\"$LOGFILE\") <(sort -u webs/webs_all.txt 2>>\"$LOGFILE\") | wc -l)\n\t\t\t\tif [[ $diff_webs != \"0\" ]] || [[ ! -s \".tmp/katana.txt\" ]]; then\n\t\t\t\t\tif [[ $URL_CHECK_ACTIVE == true ]]; then\n\t\t\t\t\t\tif [[ $DEEP == true ]]; then\n\t\t\t\t\t\t\tkatana -silent -list webs/webs_all.txt -jc -kf all -c \"$KATANA_THREADS\" -d 3 -fs rdn -o .tmp/katana.txt 2>>\"$LOGFILE\" >/dev/null\n\t\t\t\t\t\telse\n\t\t\t\t\t\t\tkatana -silent -list webs/webs_all.txt -jc -kf all -c \"$KATANA_THREADS\" -d 2 -fs rdn -o .tmp/katana.txt 2>>\"$LOGFILE\" >/dev/null\n\t\t\t\t\t\tfi\n\t\t\t\t\tfi\n\t\t\t\tfi\n\t\t\telse\n\t\t\t\tdiff_webs=$(diff <(sort -u .tmp/probed_tmp.txt) <(sort -u webs/webs_all.txt) | wc -l)\n\t\t\t\tif [[ $diff_webs != \"0\" ]] || [[ ! -s \".tmp/katana.txt\" ]]; then\n\t\t\t\t\tif [[ $URL_CHECK_ACTIVE == true ]]; then\n\t\t\t\t\t\tif [[ $DEEP == true ]]; then\n\t\t\t\t\t\t\taxiom-scan webs/webs_all.txt -m katana -jc -kf all -d 3 -fs rdn -o .tmp/katana.txt \"$AXIOM_EXTRA_ARGS\" 2>>\"$LOGFILE\" >/dev/null\n\t\t\t\t\t\telse\n\t\t\t\t\t\t\taxiom-scan webs/webs_all.txt -m katana -jc -kf all -d 2 -fs rdn -o .tmp/katana.txt \"$AXIOM_EXTRA_ARGS\" 2>>\"$LOGFILE\" >/dev/null\n\t\t\t\t\t\tfi\n\t\t\t\t\tfi\n\t\t\t\tfi\n\t\t\tfi\n\n\t\t\tif [[ -s \".tmp/katana.txt\" ]]; then\n\t\t\t\tsed -i '/^.\\{2048\\}./d' .tmp/katana.txt\n\t\t\t\tcat .tmp/katana.txt | anew -q .tmp/url_extract_tmp.txt\n\t\t\tfi\n\n\t\t\tif [[ -s \".tmp/url_extract_tmp.txt\" ]]; then\n\t\t\t\tgrep \"$domain\" .tmp/url_extract_tmp.txt | grep -E '^((http|https):\\/\\/)?([a-zA-Z0-9\\-\\.]+\\.)+[a-zA-Z]{1,}(\\/.*)?$' | grep -aEi \"\\.js$\" | anew -q .tmp/url_extract_js.txt\n\t\t\t\tgrep \"$domain\" .tmp/url_extract_tmp.txt | grep -E '^((http|https):\\/\\/)?([a-zA-Z0-9\\-\\.]+\\.)+[a-zA-Z]{1,}(\\/.*)?$' | grep -aEi \"\\.js\\.map$\" | anew -q .tmp/url_extract_jsmap.txt\n\t\t\t\tif [[ $DEEP == true ]] && [[ -s \".tmp/url_extract_js.txt\" ]]; then\n\t\t\t\t\tinterlace -tL .tmp/url_extract_js.txt -threads 10 -c \"python3 ${tools}/JSA/jsa.py -f _target_ | anew -q .tmp/url_extract_tmp.txt\" &>/dev/null\n\t\t\t\tfi\n\n\t\t\t\tgrep \"$domain\" .tmp/url_extract_tmp.txt | grep -E '^((http|https):\\/\\/)?([a-zA-Z0-9\\-\\.]+\\.)+[a-zA-Z]{1,}(\\/.*)?$' | grep \"=\" | qsreplace -a 2>>\"$LOGFILE\" | grep -aEiv \"\\.(eot|jpg|jpeg|gif|css|tif|tiff|png|ttf|otf|woff|woff2|ico|pdf|svg|txt|js)$\" | anew -q .tmp/url_extract_tmp2.txt\n\n\t\t\t\tif [[ -s \".tmp/url_extract_tmp2.txt\" ]]; then\n\t\t\t\t\tpython3 \"${tools}/urless/urless/urless.py\" <.tmp/url_extract_tmp2.txt | anew -q .tmp/url_extract_uddup.txt 2>>\"$LOGFILE\" >/dev/null\n\t\t\t\tfi\n\n\t\t\t\tif [[ -s \".tmp/url_extract_uddup.txt\" ]]; then\n\t\t\t\t\tif ! NUMOFLINES=$(anew webs/url_extract.txt <.tmp/url_extract_uddup.txt | sed '/^$/d' | wc -l); then\n\t\t\t\t\t\tprintf \"%b[!] Failed to update url_extract.txt.%b\\n\" \"$bred\" \"$reset\"\n\t\t\t\t\t\tNUMOFLINES=0\n\t\t\t\t\tfi\n\t\t\t\t\tnotification \"${NUMOFLINES} new URLs with parameters\" \"info\"\n\t\t\t\telse\n\t\t\t\t\tNUMOFLINES=0\n\t\t\t\tfi\n\n\t\t\t\tend_func \"Results are saved in $domain/webs/url_extract.txt\" \"${FUNCNAME[0]}\"\n\n\t\t\t\tp1radup -i webs/url_extract.txt -o webs/url_extract_nodupes.txt -s 2>>\"$LOGFILE\" >/dev/null\n\n\t\t\t\tif [[ $PROXY == true ]] && [[ -n $proxy_url ]] && [[ $(wc -l <webs/url_extract.txt) -le $DEEP_LIMIT2 ]]; then\n\t\t\t\t\tnotification \"Sending URLs to proxy\" \"info\"\n\t\t\t\t\tffuf -mc all -w webs/url_extract.txt -u FUZZ -replay-proxy \"$proxy_url\" 2>>\"$LOGFILE\" >/dev/null\n\t\t\t\tfi\n\t\t\tfi\n\t\tfi\n\telse\n\t\tif [[ $URL_CHECK == false ]]; then\n\t\t\tprintf \"\\n%b[%s] %s skipped due to mode or defined in reconftw.cfg.%b\\n\" \\\n\t\t\t\t\"$yellow\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"${FUNCNAME[0]}\" \"$reset\"\n\t\telse\n\t\t\tprintf \"%b[%s] %s has already been processed. To force execution, delete:\\n    %s/.%s%b\\n\\n\" \\\n\t\t\t\t\"$yellow\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"${FUNCNAME[0]}\" \"$called_fn_dir\" \".${FUNCNAME[0]}\" \"$reset\"\n\t\tfi\n\tfi\n\n}\n\nfunction url_gf() {\n\n\t# Create necessary directories\n\tif ! mkdir -p .tmp webs gf; then\n\t\tprintf \"%b[!] Failed to create directories.%b\\n\" \"$bred\" \"$reset\"\n\t\treturn 1\n\tfi\n\n\t# Check if the function should run\n\tif { [[ ! -f \"$called_fn_dir/.${FUNCNAME[0]}\" ]] || [[ $DIFF == true ]]; } && [[ $URL_GF == true ]] &&\n\t\t! [[ $domain =~ ^[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+$ ]]; then\n\n\t\tstart_func \"${FUNCNAME[0]}\" \"Vulnerable Pattern Search\"\n\n\t\t# Ensure webs_nuclei.txt exists and is not empty\n\t\tif [[ -s \"webs/webs_nuclei.txt\" ]]; then\n\t\t\t# Define an array of GF patterns\n\t\t\tdeclare -A gf_patterns=(\n\t\t\t\t[\"xss\"]=\"gf/xss.txt\"\n\t\t\t\t[\"ssti\"]=\"gf/ssti.txt\"\n\t\t\t\t[\"ssrf\"]=\"gf/ssrf.txt\"\n\t\t\t\t[\"sqli\"]=\"gf/sqli.txt\"\n\t\t\t\t[\"redirect\"]=\"gf/redirect.txt\"\n\t\t\t\t[\"rce\"]=\"gf/rce.txt\"\n\t\t\t\t[\"potential\"]=\"gf/potential.txt\"\n\t\t\t\t[\"lfi\"]=\"gf/lfi.txt\"\n\t\t\t)\n\n\t\t\t# Iterate over GF patterns and process each\n\t\t\tfor pattern in \"${!gf_patterns[@]}\"; do\n\t\t\t\toutput_file=\"${gf_patterns[$pattern]}\"\n\t\t\t\tprintf \"${yellow}\\n[$(date +'%Y-%m-%d %H:%M:%S')] Running: GF Pattern '$pattern'${reset}\\n\\n\"\n\t\t\t\tif [[ $pattern == \"potential\" ]]; then\n\t\t\t\t\t# Special handling for 'potential' pattern\n\t\t\t\t\tgf \"$pattern\" \"webs/webs_nuclei.txt\" | cut -d ':' -f3-5 | anew -q \"$output_file\"\n\t\t\t\telif [[ $pattern == \"redirect\" && -s \"gf/ssrf.txt\" ]]; then\n\t\t\t\t\t# Append SSFR results to redirect if ssrf.txt exists\n\t\t\t\t\tgf \"$pattern\" \"webs/webs_nuclei.txt\" | anew -q \"$output_file\"\n\t\t\t\telse\n\t\t\t\t\t# General handling for other patterns\n\t\t\t\t\tgf \"$pattern\" \"webs/webs_nuclei.txt\" | anew -q \"$output_file\"\n\t\t\t\tfi\n\t\t\tdone\n\n\t\t\t# Process endpoints extraction\n\t\t\tif [[ -s \".tmp/url_extract_tmp.txt\" ]]; then\n\t\t\t\tprintf \"${yellow}\\n[$(date +'%Y-%m-%d %H:%M:%S')] Extracting endpoints...${reset}\\n\\n\"\n\t\t\t\tgrep -aEiv \"\\.(eot|jpg|jpeg|gif|css|tif|tiff|png|ttf|otf|woff|woff2|ico|pdf|svg|txt|js)$\" \".tmp/url_extract_tmp.txt\" |\n\t\t\t\t\tunfurl -u format '%s://%d%p' 2>>\"$LOGFILE\" | anew -q \"gf/endpoints.txt\"\n\t\t\tfi\n\n\t\telse\n\t\t\tend_func \"No webs/webs_nuclei.txt file found, URL_GF check skipped.\" \"${FUNCNAME[0]}\"\n\t\t\treturn\n\t\tfi\n\n\t\tend_func \"Results are saved in $domain/gf folder\" \"${FUNCNAME[0]}\"\n\telse\n\t\t# Handle cases where URL_GF is false or the function has already been processed\n\t\tif [[ $URL_GF == false ]]; then\n\t\t\tprintf \"\\n${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] ${FUNCNAME[0]} skipped due to configuration settings.${reset}\\n\"\n\t\telif [[ $domain =~ ^[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+$ ]]; then\n\t\t\t# Domain is an IP address; skip the function\n\t\t\treturn\n\t\telse\n\t\t\tprintf \"${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] ${FUNCNAME[0]} has already been processed. To force execution, delete:\\n    $called_fn_dir/.${FUNCNAME[0]}${reset}\\n\\n\"\n\t\tfi\n\tfi\n\n}\n\nfunction url_ext() {\n\n\t# Create necessary directories\n\tif ! mkdir -p .tmp webs gf; then\n\t\tprintf \"%b[!] Failed to create directories.%b\\n\" \"$bred\" \"$reset\"\n\t\treturn 1\n\tfi\n\n\t# Check if the function should run\n\tif { [[ ! -f \"$called_fn_dir/.${FUNCNAME[0]}\" ]] || [[ $DIFF == true ]]; } && [[ $URL_EXT == true ]] &&\n\t\t! [[ $domain =~ ^[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+$ ]]; then\n\n\t\tif [[ -s \".tmp/url_extract_tmp.txt\" ]]; then\n\t\t\tstart_func \"${FUNCNAME[0]}\" \"Vulnerable Pattern Search\"\n\n\t\t\t# Define an array of file extensions\n\t\t\text=(\"7z\" \"achee\" \"action\" \"adr\" \"apk\" \"arj\" \"ascx\" \"asmx\" \"asp\" \"aspx\" \"axd\" \"backup\" \"bak\" \"bat\" \"bin\" \"bkf\" \"bkp\" \"bok\" \"cab\" \"cer\" \"cfg\" \"cfm\" \"cnf\" \"conf\" \"config\" \"cpl\" \"crt\" \"csr\" \"csv\" \"dat\" \"db\" \"dbf\" \"deb\" \"dmg\" \"dmp\" \"doc\" \"docx\" \"drv\" \"email\" \"eml\" \"emlx\" \"env\" \"exe\" \"gadget\" \"gz\" \"html\" \"ica\" \"inf\" \"ini\" \"iso\" \"jar\" \"java\" \"jhtml\" \"json\" \"jsp\" \"key\" \"log\" \"lst\" \"mai\" \"mbox\" \"mbx\" \"md\" \"mdb\" \"msg\" \"msi\" \"nsf\" \"ods\" \"oft\" \"old\" \"ora\" \"ost\" \"pac\" \"passwd\" \"pcf\" \"pdf\" \"pem\" \"pgp\" \"php\" \"php3\" \"php4\" \"php5\" \"phtm\" \"phtml\" \"pkg\" \"pl\" \"plist\" \"pst\" \"pwd\" \"py\" \"rar\" \"rb\" \"rdp\" \"reg\" \"rpm\" \"rtf\" \"sav\" \"sh\" \"shtm\" \"shtml\" \"skr\" \"sql\" \"swf\" \"sys\" \"tar\" \"tar.gz\" \"tmp\" \"toast\" \"tpl\" \"txt\" \"url\" \"vcd\" \"vcf\" \"wml\" \"wpd\" \"wsdl\" \"wsf\" \"xls\" \"xlsm\" \"xlsx\" \"xml\" \"xsd\" \"yaml\" \"yml\" \"z\" \"zip\")\n\n\t\t\t# Initialize the output file\n\t\t\tif ! : >webs/urls_by_ext.txt; then\n\t\t\t\tprintf \"%b[!] Failed to initialize webs/urls_by_ext.txt.%b\\n\" \"$bred\" \"$reset\"\n\t\t\tfi\n\n\t\t\t# Iterate over extensions and extract matching URLs\n\t\t\tfor t in \"${ext[@]}\"; do\n\n\t\t\t\t# Extract unique matching URLs\n\t\t\t\tmatches=$(grep -aEi \"\\.(${t})($|/|\\?)\" \".tmp/url_extract_tmp.txt\" | sort -u | sed '/^$/d')\n\n\t\t\t\tNUMOFLINES=$(echo \"$matches\" | wc -l)\n\n\t\t\t\tif [[ $NUMOFLINES -gt 0 ]]; then\n\t\t\t\t\tprintf \"\\n############################\\n + %s + \\n############################\\n\" \"$t\" >>webs/urls_by_ext.txt\n\t\t\t\t\techo \"$matches\" >>webs/urls_by_ext.txt\n\t\t\t\tfi\n\t\t\tdone\n\n\t\t\t# Append ssrf.txt to redirect.txt if ssrf.txt exists and is not empty\n\t\t\tif [[ -s \"gf/ssrf.txt\" ]]; then\n\t\t\t\tcat \"gf/ssrf.txt\" | anew -q \"gf/redirect.txt\"\n\t\t\tfi\n\n\t\t\tend_func \"Results are saved in $domain/webs/urls_by_ext.txt\" \"${FUNCNAME[0]}\"\n\n\t\telse\n\t\t\tend_func \"No .tmp/url_extract_tmp.txt file found, URL_EXT check skipped.\" \"${FUNCNAME[0]}\"\n\t\tfi\n\n\telse\n\t\t# Handle cases where URL_EXT is false or function already processed\n\t\tif [[ $URL_EXT == false ]]; then\n\t\t\tprintf \"\\n${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] ${FUNCNAME[0]} skipped in this mode or defined in reconftw.cfg ${reset}\\n\"\n\t\telif [[ $domain =~ ^[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+$ ]]; then\n\t\t\t# Domain is an IP address; skip the function\n\t\t\treturn\n\t\telse\n\t\t\tprintf \"${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] ${FUNCNAME[0]} has already been processed. To force execution, delete:\\n    $called_fn_dir/.${FUNCNAME[0]}${reset}\\n\\n\"\n\t\tfi\n\tfi\n\n}\n\nfunction jschecks() {\n\n\t# Create necessary directories\n\tif ! mkdir -p .tmp webs subdomains js; then\n\t\tprintf \"%b[!] Failed to create directories.%b\\n\" \"$bred\" \"$reset\"\n\t\treturn 1\n\tfi\n\n\t# Check if the function should run\n\tif { [[ ! -f \"$called_fn_dir/.${FUNCNAME[0]}\" ]] || [[ $DIFF == true ]]; } && [[ $JSCHECKS == true ]]; then\n\t\tstart_func \"${FUNCNAME[0]}\" \"JavaScript Scan\"\n\n\t\tif [[ -s \".tmp/url_extract_js.txt\" ]]; then\n\n\t\t\tprintf \"%bRunning: Fetching URLs 1/6%b\\n\" \"$yellow\" \"$reset\"\n\t\t\tif [[ $AXIOM != true ]]; then\n\t\t\t\tsubjs -ua \"Mozilla/5.0 (X11; Linux x86_64; rv:72.0) Gecko/20100101 Firefox/72.0\" -c 40 <.tmp/url_extract_js.txt |\n\t\t\t\t\tgrep \"$domain\" |\n\t\t\t\t\tgrep -E '^((http|https):\\/\\/)?([a-zA-Z0-9]([a-zA-Z0-9\\-]*[a-zA-Z0-9])?\\.)+[a-zA-Z]{1,}(\\/.*)?$' |\n\t\t\t\t\tanew -q .tmp/subjslinks.txt\n\t\t\telse\n\t\t\t\taxiom-scan .tmp/url_extract_js.txt -m subjs -o .tmp/subjslinks.txt \"$AXIOM_EXTRA_ARGS\" 2>>\"$LOGFILE\" >/dev/null\n\t\t\tfi\n\n\t\t\tif [[ -s \".tmp/subjslinks.txt\" ]]; then\n\t\t\t\tgrep -Eiv \"\\.(eot|jpg|jpeg|gif|css|tif|tiff|png|ttf|otf|woff|woff2|ico|pdf|svg|txt|js)$\" .tmp/subjslinks.txt |\n\t\t\t\t\tanew -q js/nojs_links.txt\n\t\t\t\tgrep -iE \"\\.js($|\\?)\" .tmp/subjslinks.txt | anew -q .tmp/url_extract_js.txt\n\t\t\tfi\n\n\t\t\tpython3 \"${tools}/urless/urless/urless.py\" <.tmp/url_extract_js.txt |\n\t\t\t\tanew -q js/url_extract_js.txt 2>>\"$LOGFILE\" >/dev/null\n\n\t\t\tprintf \"%bRunning: Resolving JS URLs 2/6%b\\n\" \"$yellow\" \"$reset\"\n\t\t\tif [[ $AXIOM != true ]]; then\n\t\t\t\tif [[ -s \"js/url_extract_js.txt\" ]]; then\n\t\t\t\t\thttpx -follow-redirects -random-agent -silent -timeout \"$HTTPX_TIMEOUT\" -threads \"$HTTPX_THREADS\" \\\n\t\t\t\t\t\t-rl \"$HTTPX_RATELIMIT\" -status-code -content-type -retries 2 -no-color <js/url_extract_js.txt |\n\t\t\t\t\t\tgrep \"[200]\" | grep \"javascript\" | cut -d ' ' -f1 | anew -q js/js_livelinks.txt\n\t\t\t\tfi\n\t\t\telse\n\t\t\t\tif [[ -s \"js/url_extract_js.txt\" ]]; then\n\t\t\t\t\taxiom-scan js/url_extract_js.txt -m httpx -follow-host-redirects -H \"$HEADER\" -status-code \\\n\t\t\t\t\t\t-threads \"$HTTPX_THREADS\" -rl \"$HTTPX_RATELIMIT\" -timeout \"$HTTPX_TIMEOUT\" -silent \\\n\t\t\t\t\t\t-content-type -retries 2 -no-color -o .tmp/js_livelinks.txt \"$AXIOM_EXTRA_ARGS\" 2>>\"$LOGFILE\" >/dev/null\n\t\t\t\t\tif [[ -s \".tmp/js_livelinks.txt\" ]]; then\n\t\t\t\t\t\tcat .tmp/js_livelinks.txt | anew .tmp/web_full_info.txt |\n\t\t\t\t\t\t\tgrep \"[200]\" | grep \"javascript\" | cut -d ' ' -f1 | anew -q js/js_livelinks.txt\n\t\t\t\t\tfi\n\t\t\t\tfi\n\t\t\tfi\n\n\t\t\tprintf \"%bRunning: Extracting JS from sourcemaps 3/6%b\\n\" \"$yellow\" \"$reset\"\n\t\t\tif ! mkdir -p .tmp/sourcemapper; then\n\t\t\t\tprintf \"%b[!] Failed to create sourcemapper directory.%b\\n\" \"$bred\" \"$reset\"\n\t\t\tfi\n\t\t\tif [[ -s \"js/js_livelinks.txt\" ]]; then\n\t\t\t\tinterlace -tL js/js_livelinks.txt -threads \"$INTERLACE_THREADS\" \\\n\t\t\t\t\t-c \"sourcemapper -jsurl '_target_' -output _output_/_cleantarget_\" \\\n\t\t\t\t\t-o .tmp/sourcemapper 2>>\"$LOGFILE\" >/dev/null\n\t\t\tfi\n\n\t\t\tif [[ -s \".tmp/url_extract_jsmap.txt\" ]]; then\n\t\t\t\tinterlace -tL js/js_livelinks.txt -threads \"$INTERLACE_THREADS\" \\\n\t\t\t\t\t-c \"sourcemapper -url '_target_' -output _output_/_cleantarget_\" \\\n\t\t\t\t\t-o .tmp/sourcemapper 2>>\"$LOGFILE\" >/dev/null\n\t\t\tfi\n\n\t\t\tfind .tmp/sourcemapper/ \\( -name \"*.js\" -o -name \"*.ts\" \\) -type f |\n\t\t\t\tjsluice urls | jq -r .url | anew -q .tmp/js_endpoints.txt\n\n\t\t\tprintf \"%bRunning: Gathering endpoints 4/6%b\\n\" \"$yellow\" \"$reset\"\n\t\t\tif [[ -s \"js/js_livelinks.txt\" ]]; then\n\t\t\t\txnLinkFinder -i js/js_livelinks.txt -sf subdomains/subdomains.txt -d \"$XNLINKFINDER_DEPTH\" \\\n\t\t\t\t\t-o .tmp/js_endpoints.txt 2>>\"$LOGFILE\" >/dev/null\n\t\t\tfi\n\n\t\t\tif [[ -s \".tmp/js_endpoints.txt\" ]]; then\n\t\t\t\tsed -i '/^\\//!d' .tmp/js_endpoints.txt\n\t\t\t\tcat .tmp/js_endpoints.txt | anew -q js/js_endpoints.txt\n\t\t\tfi\n\n\t\t\tprintf \"%bRunning: Gathering secrets 5/6%b\\n\" \"$yellow\" \"$reset\"\n\t\t\tif [[ -s \"js/js_livelinks.txt\" ]]; then\n\t\t\t\tif [[ $AXIOM != true ]]; then\n\t\t\t\t\tcat js/js_livelinks.txt | mantra -ua \"$HEADER\" -s -o js/js_secrets.txt 2>>\"$LOGFILE\" >/dev/null\n\t\t\t\telse\n\t\t\t\t\taxiom-scan js/js_livelinks.txt -m mantra -ua \"$HEADER\" -s -o js/js_secrets.txt \"$AXIOM_EXTRA_ARGS\" &>/dev/null\n\t\t\t\tfi\n\t\t\t\tif [[ -s \"js/js_secrets.txt\" ]]; then\n\t\t\t\t\ttrufflehog filesystem js/js_secrets.txt -j 2>/dev/null |\n\t\t\t\t\t\tjq -c | anew -q js/js_secrets_trufflehog.txt\n\t\t\t\t\ttrufflehog filesystem .tmp/sourcemapper/ -j 2>/dev/null |\n\t\t\t\t\t\tjq -c | anew -q js/js_secrets_trufflehog.txt\n\t\t\t\t\tsed -r \"s/\\x1B\\[([0-9]{1,3}(;[0-9]{1,2};?)?)?[mGK]//g\" -i js/js_secrets.txt\n\t\t\t\tfi\n\t\t\tfi\n\n\t\t\tprintf \"%bRunning: Building wordlist 6/6%b\\n\" \"$yellow\" \"$reset\"\n\t\t\tif [[ -s \"js/js_livelinks.txt\" ]]; then\n\t\t\t\tinterlace -tL js/js_livelinks.txt -threads \"$INTERLACE_THREADS\" \\\n\t\t\t\t\t-c \"python3 ${tools}/getjswords.py '_target_' | anew -q webs/dict_words.txt\" 2>>\"$LOGFILE\" >/dev/null\n\t\t\tfi\n\t\t\tend_func \"Results are saved in $domain/js folder\" \"${FUNCNAME[0]}\"\n\t\tfi\n\telse\n\t\tif [[ $JSCHECKS == false ]]; then\n\t\t\tprintf \"\\n%b[%s] %s skipped due to mode or defined in reconftw.cfg.%b\\n\" \\\n\t\t\t\t\"$yellow\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"${FUNCNAME[0]}\" \"$reset\"\n\t\telse\n\t\t\tprintf \"%b[%s] %s has already been processed. To force execution, delete:\\n    %s/.%s%b\\n\\n\" \\\n\t\t\t\t\"$yellow\" \"$(date +'%Y-%m-%d %H:%M:%S')\" \"${FUNCNAME[0]}\" \"$called_fn_dir\" \".${FUNCNAME[0]}\" \"$reset\"\n\t\tfi\n\tfi\n\n}\n\nfunction wordlist_gen() {\n\n\t# Create necessary directories\n\tif ! mkdir -p .tmp webs; then\n\t\tprintf \"%b[!] Failed to create directories.%b\\n\" \"$bred\" \"$reset\"\n\t\treturn 1\n\tfi\n\n\t# Check if the function should run\n\tif { [[ ! -f \"$called_fn_dir/.${FUNCNAME[0]}\" ]] || [[ $DIFF == true ]]; } && [[ $WORDLIST == true ]] &&\n\t\t! [[ $domain =~ ^[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+$ ]]; then\n\n\t\tstart_func \"${FUNCNAME[0]}\" \"Wordlist Generation\"\n\n\t\t# Ensure url_extract_tmp.txt exists and is not empty\n\t\tif [[ -s \".tmp/url_extract_tmp.txt\" ]]; then\n\t\t\t# Define patterns for keys and values\n\t\t\tpatterns=(\"keys\" \"values\")\n\n\t\t\tfor pattern in \"${patterns[@]}\"; do\n\t\t\t\toutput_file=\"webs/dict_${pattern}.txt\"\n\t\t\t\tprintf \"${yellow}\\n[$(date +'%Y-%m-%d %H:%M:%S')] Extracting ${pattern}...${reset}\\n\"\n\n\t\t\t\tif [[ $pattern == \"keys\" || $pattern == \"values\" ]]; then\n\t\t\t\t\tunfurl -u \"$pattern\" \".tmp/url_extract_tmp.txt\" 2>>\"$LOGFILE\" |\n\t\t\t\t\t\tsed 's/[][]//g' | sed 's/[#]//g' | sed 's/[}{]//g' |\n\t\t\t\t\t\tanew -q \"$output_file\"\n\t\t\t\tfi\n\t\t\tdone\n\n\t\t\t# Extract words by removing punctuation\n\t\t\tprintf \"${yellow}\\n[$(date +'%Y-%m-%d %H:%M:%S')] Extracting words...${reset}\\n\"\n\t\t\ttr \"[:punct:]\" \"\\n\" <\".tmp/url_extract_tmp.txt\" | anew -q \"webs/dict_words.txt\"\n\t\tfi\n\n\t\tend_func \"Results are saved in $domain/webs/dict_[words|paths].txt\" \"${FUNCNAME[0]}\"\n\n\telse\n\t\t# Handle cases where WORDLIST is false or function already processed\n\t\tif [[ $WORDLIST == false ]]; then\n\t\t\tprintf \"\\n${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] ${FUNCNAME[0]} skipped in this mode or defined in reconftw.cfg ${reset}\\n\"\n\t\telif [[ $domain =~ ^[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+$ ]]; then\n\t\t\t# Domain is an IP address; skip the function\n\t\t\treturn\n\t\telse\n\t\t\tprintf \"${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] ${FUNCNAME[0]} has already been processed. To force execution, delete:\\n    $called_fn_dir/.${FUNCNAME[0]}${reset}\\n\\n\"\n\t\tfi\n\tfi\n\n}\n\nfunction wordlist_gen_roboxtractor() {\n\n\t# Create necessary directories\n\tif ! mkdir -p .tmp webs gf; then\n\t\tprintf \"%b[!] Failed to create directories.%b\\n\" \"$bred\" \"$reset\"\n\t\treturn 1\n\tfi\n\n\t# Check if the function should run\n\tif { [[ ! -f \"$called_fn_dir/.${FUNCNAME[0]}\" ]] || [[ $DIFF == true ]]; } && [[ $ROBOTSWORDLIST == true ]] &&\n\t\t! [[ $domain =~ ^[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+$ ]]; then\n\n\t\tstart_func \"${FUNCNAME[0]}\" \"Robots Wordlist Generation\"\n\n\t\t# Combine webs.txt and webs_uncommon_ports.txt into webs_all.txt if it doesn't exist\n\t\tif [[ ! -s \"webs/webs_all.txt\" ]]; then\n\t\t\tcat webs/webs.txt webs/webs_uncommon_ports.txt 2>/dev/null | anew -q \"webs/webs_all.txt\"\n\t\tfi\n\n\t\t# Proceed only if webs_all.txt exists and is non-empty\n\t\tif [[ -s \"webs/webs_all.txt\" ]]; then\n\t\t\t# Extract URLs using roboxtractor and append unique entries to robots_wordlist.txt\n\t\t\tprintf \"${yellow}\\n[$(date +'%Y-%m-%d %H:%M:%S')] Running: Roboxtractor for Robots Wordlist${reset}\\n\\n\"\n\t\t\troboxtractor -m 1 -wb <\"webs/webs_all.txt\" 2>>\"$LOGFILE\" | anew -q \"webs/robots_wordlist.txt\"\n\t\telse\n\t\t\tend_func \"No webs/webs_all.txt file found, Robots Wordlist generation skipped.\" \"${FUNCNAME[0]}\"\n\t\t\treturn\n\t\tfi\n\n\t\tend_func \"Results are saved in $domain/webs/robots_wordlist.txt\" \"${FUNCNAME[0]}\"\n\n\t\t# Handle Proxy if conditions are met\n\t\tif [[ $PROXY == true ]] && [[ -n $proxy_url ]] && [[ \"$(wc -l <\"webs/robots_wordlist.txt\")\" -le $DEEP_LIMIT2 ]]; then\n\t\t\tnotification \"Sending URLs to proxy\" info\n\t\t\tffuf -mc all -w \"webs/robots_wordlist.txt\" -u \"FUZZ\" -replay-proxy \"$proxy_url\" 2>>\"$LOGFILE\" >/dev/null\n\t\tfi\n\n\telse\n\t\t# Handle cases where ROBOTSWORDLIST is false or function already processed\n\t\tif [[ $ROBOTSWORDLIST == false ]]; then\n\t\t\tprintf \"\\n${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] ${FUNCNAME[0]} skipped due to configuration settings.${reset}\\n\"\n\t\telif [[ $domain =~ ^[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+$ ]]; then\n\t\t\t# Domain is an IP address; skip the function\n\t\t\treturn\n\t\telse\n\t\t\tprintf \"${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] ${FUNCNAME[0]} has already been processed. To force execution, delete:\\n    $called_fn_dir/.${FUNCNAME[0]}${reset}\\n\\n\"\n\t\tfi\n\tfi\n\n}\n\nfunction password_dict() {\n\n\t# Create necessary directories\n\tif ! mkdir -p \"$dir/webs\"; then\n\t\tprintf \"%b[!] Failed to create directories.%b\\n\" \"$bred\" \"$reset\"\n\t\treturn 1\n\tfi\n\n\t# Check if the function should run\n\tif { [[ ! -f \"$called_fn_dir/.${FUNCNAME[0]}\" ]] || [[ $DIFF == true ]]; } && [[ $PASSWORD_DICT == true ]] &&\n\t\t! [[ $domain =~ ^[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+$ ]]; then\n\n\t\tstart_func \"${FUNCNAME[0]}\" \"Password Dictionary Generation\"\n\n\t\t# Extract the first part of the domain\n\t\tword=\"${domain%%.*}\"\n\n\t\t# Run pydictor.py with specified parameters\n\t\tpython3 \"${tools}/pydictor/pydictor.py\" -extend \"$word\" --leet 0 1 2 11 21 --len \"$PASSWORD_MIN_LENGTH\" \"$PASSWORD_MAX_LENGTH\" -o \"$dir/webs/password_dict.txt\" 2>>\"$LOGFILE\" >/dev/null\n\t\tend_func \"Results are saved in $domain/webs/password_dict.txt\" \"${FUNCNAME[0]}\"\n\n\t\t# Optionally, create a marker file to indicate the function has been processed\n\t\ttouch \"$called_fn_dir/.${FUNCNAME[0]}\"\n\n\telse\n\t\t# Handle cases where PASSWORD_DICT is false or function already processed\n\t\tif [[ $PASSWORD_DICT == false ]]; then\n\t\t\tprintf \"\\n${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] ${FUNCNAME[0]} skipped due to configuration settings.${reset}\\n\"\n\t\telif [[ $domain =~ ^[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+$ ]]; then\n\t\t\t# Domain is an IP address; skip the function\n\t\t\treturn\n\t\telse\n\t\t\tprintf \"${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] ${FUNCNAME[0]} has already been processed. To force execution, delete:\\n    $called_fn_dir/.${FUNCNAME[0]}${reset}\\n\\n\"\n\t\tfi\n\tfi\n\n}\n\n###############################################################################################################\n######################################### VULNERABILITIES #####################################################\n###############################################################################################################\n\nfunction brokenLinks() {\n\n\t# Create necessary directories\n\tif ! mkdir -p .tmp webs vulns; then\n\t\tprintf \"%b[!] Failed to create directories.%b\\n\" \"$bred\" \"$reset\"\n\t\treturn 1\n\tfi\n\n\t# Check if the function should run\n\tif { [[ ! -f \"$called_fn_dir/.${FUNCNAME[0]}\" ]] || [[ $DIFF == true ]]; } && [[ $BROKENLINKS == true ]] &&\n\t\t! [[ $domain =~ ^[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+$ ]]; then\n\n\t\tstart_func \"${FUNCNAME[0]}\" \"Broken Links Checks\"\n\n\t\t# Combine webs.txt and webs_uncommon_ports.txt into webs_all.txt if it doesn't exist\n\t\tif [[ ! -s \"webs/webs_all.txt\" ]]; then\n\t\t\tcat webs/webs.txt webs/webs_uncommon_ports.txt 2>/dev/null | anew -q \"webs/webs_all.txt\"\n\t\tfi\n\n\t\t# Check if webs_all.txt exists and is not empty\n\t\tif [[ -s \"webs/webs_all.txt\" ]]; then\n\t\t\tif [[ $AXIOM != true ]]; then\n\t\t\t\t# Use katana for scanning\n\t\t\t\tif [[ ! -s \".tmp/katana.txt\" ]]; then\n\t\t\t\t\tif [[ $DEEP == true ]]; then\n\t\t\t\t\t\tkatana -silent -list \"webs/webs_all.txt\" -jc -kf all -c \"$KATANA_THREADS\" -d 3 -o \".tmp/katana.txt\" 2>>\"$LOGFILE\" >/dev/null\n\t\t\t\t\telse\n\t\t\t\t\t\tkatana -silent -list \"webs/webs_all.txt\" -jc -kf all -c \"$KATANA_THREADS\" -d 2 -o \".tmp/katana.txt\" 2>>\"$LOGFILE\" >/dev/null\n\t\t\t\t\tfi\n\t\t\t\tfi\n\t\t\t\t# Remove lines longer than 2048 characters\n\t\t\t\tif [[ -s \".tmp/katana.txt\" ]]; then\n\t\t\t\t\tsed -i '/^.\\{2048\\}./d' \".tmp/katana.txt\"\n\t\t\t\tfi\n\t\t\telse\n\t\t\t\t# Use axiom-scan for scanning\n\t\t\t\tif [[ ! -s \".tmp/katana.txt\" ]]; then\n\t\t\t\t\tif [[ $DEEP == true ]]; then\n\t\t\t\t\t\taxiom-scan \"webs/webs_all.txt\" -m katana -jc -kf all -d 3 -o \".tmp/katana.txt\" $AXIOM_EXTRA_ARGS 2>>\"$LOGFILE\" >/dev/null\n\t\t\t\t\telse\n\t\t\t\t\t\taxiom-scan \"webs/webs_all.txt\" -m katana -jc -kf all -d 2 -o \".tmp/katana.txt\" $AXIOM_EXTRA_ARGS 2>>\"$LOGFILE\" >/dev/null\n\t\t\t\t\tfi\n\t\t\t\t\t# Remove lines longer than 2048 characters\n\t\t\t\t\tif [[ -s \".tmp/katana.txt\" ]]; then\n\t\t\t\t\t\tsed -i '/^.\\{2048\\}./d' \".tmp/katana.txt\"\n\t\t\t\t\tfi\n\t\t\t\tfi\n\t\t\tfi\n\n\t\t\t# Process katana.txt to find broken links\n\t\t\tif [[ -s \".tmp/katana.txt\" ]]; then\n\t\t\t\thttpx -follow-redirects -random-agent -status-code -threads \"$HTTPX_THREADS\" -rl \"$HTTPX_RATELIMIT\" -timeout \"$HTTPX_TIMEOUT\" -silent -retries 2 -no-color <\".tmp/katana.txt\" 2>>\"$LOGFILE\" |\n\t\t\t\t\tgrep \"\\[4\" | cut -d ' ' -f1 | anew -q \".tmp/brokenLinks_total.txt\"\n\t\t\tfi\n\n\t\t\t# Update brokenLinks.txt with unique entries\n\t\t\tif [[ -s \".tmp/brokenLinks_total.txt\" ]]; then\n\t\t\t\tNUMOFLINES=$(wc -l <\".tmp/brokenLinks_total.txt\" 2>>\"$LOGFILE\" | awk '{print $1}')\n\t\t\t\tcat .tmp/brokenLinks_total.txt | anew -q \"vulns/brokenLinks.txt\"\n\t\t\t\tNUMOFLINES=$(sed '/^$/d' \"vulns/brokenLinks.txt\" | wc -l)\n\t\t\t\tnotification \"${NUMOFLINES} new broken links found\" info\n\t\t\tfi\n\n\t\t\tend_func \"Results are saved in vulns/brokenLinks.txt\" \"${FUNCNAME[0]}\"\n\t\telse\n\t\t\tend_func \"No webs/webs_all.txt file found, Broken Links check skipped.\" \"${FUNCNAME[0]}\"\n\t\t\treturn\n\t\tfi\n\telse\n\t\t# Handle cases where BROKENLINKS is false or function already processed\n\t\tif [[ $BROKENLINKS == false ]]; then\n\t\t\tprintf \"\\n${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] ${FUNCNAME[0]} skipped due to configuration settings.${reset}\\n\"\n\t\telif [[ $domain =~ ^[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+$ ]]; then\n\t\t\t# Domain is an IP address; skip the function\n\t\t\treturn\n\t\telse\n\t\t\tprintf \"${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] ${FUNCNAME[0]} has already been processed. To force execution, delete:\\n    $called_fn_dir/.${FUNCNAME[0]}${reset}\\n\\n\"\n\t\tfi\n\tfi\n\n}\n\nfunction xss() {\n\n\t# Create necessary directories\n\tif ! mkdir -p .tmp webs vulns; then\n\t\tprintf \"%b[!] Failed to create directories.%b\\n\" \"$bred\" \"$reset\"\n\t\treturn 1\n\tfi\n\n\t# Check if the function should run\n\tif { [[ ! -f \"$called_fn_dir/.${FUNCNAME[0]}\" ]] || [[ $DIFF == true ]]; } && [[ $XSS == true ]] && [[ -s \"gf/xss.txt\" ]] &&\n\t\t! [[ $domain =~ ^[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+$ ]]; then\n\n\t\tstart_func \"${FUNCNAME[0]}\" \"XSS Analysis\"\n\n\t\t# Process gf/xss.txt with qsreplace and Gxss\n\t\tif [[ -s \"gf/xss.txt\" ]]; then\n\t\t\tprintf \"${yellow}\\n[$(date +'%Y-%m-%d %H:%M:%S')] Running: XSS Payload Generation${reset}\\n\\n\"\n\t\t\tqsreplace FUZZ <\"gf/xss.txt\" | sed '/FUZZ/!d' | Gxss -c 100 -p Xss | qsreplace FUZZ | sed '/FUZZ/!d' |\n\t\t\t\tanew -q \".tmp/xss_reflected.txt\"\n\t\tfi\n\n\t\t# Determine whether to use Axiom or Katana for scanning\n\t\tif [[ $AXIOM != true ]]; then\n\t\t\t# Using Katana\n\t\t\tif [[ $DEEP == true ]]; then\n\t\t\t\tDEPTH=3\n\t\t\telse\n\t\t\t\tDEPTH=2\n\t\t\tfi\n\n\t\t\tif [[ -n $XSS_SERVER ]]; then\n\t\t\t\tOPTIONS=\"-b ${XSS_SERVER} -w $DALFOX_THREADS\"\n\t\t\telse\n\t\t\t\tprintf \"${yellow}\\n[$(date +'%Y-%m-%d %H:%M:%S')] No XSS_SERVER defined, blind XSS skipped\\n\\n\"\n\t\t\t\tOPTIONS=\"-w $DALFOX_THREADS\"\n\t\t\tfi\n\n\t\t\t# Run Dalfox with Katana output\n\t\t\tif [[ -s \".tmp/xss_reflected.txt\" ]]; then\n\t\t\t\tprintf \"${yellow}\\n[$(date +'%Y-%m-%d %H:%M:%S')] Running: Dalfox with Katana${reset}\\n\\n\"\n\t\t\t\tdalfox pipe --silence --no-color --no-spinner --only-poc r --ignore-return 302,404,403 --skip-bav $OPTIONS -d \"$DEPTH\" <\".tmp/xss_reflected.txt\" 2>>\"$LOGFILE\" |\n\t\t\t\t\tanew -q \"vulns/xss.txt\"\n\t\t\tfi\n\t\telse\n\t\t\t# Using Axiom\n\t\t\tif [[ $DEEP == true ]]; then\n\t\t\t\tDEPTH=3\n\t\t\t\tAXIOM_ARGS=\"$AXIOM_EXTRA_ARGS\"\n\t\t\telse\n\t\t\t\tDEPTH=2\n\t\t\t\tAXIOM_ARGS=\"$AXIOM_EXTRA_ARGS\"\n\t\t\tfi\n\n\t\t\tif [[ -n $XSS_SERVER ]]; then\n\t\t\t\tOPTIONS=\"-b ${XSS_SERVER} -w $DALFOX_THREADS\"\n\t\t\telse\n\t\t\t\tprintf \"${yellow}\\n[$(date +'%Y-%m-%d %H:%M:%S')] No XSS_SERVER defined, blind XSS skipped\\n\\n\"\n\t\t\t\tOPTIONS=\"-w $DALFOX_THREADS\"\n\t\t\tfi\n\n\t\t\t# Run Dalfox with Axiom-scan output\n\t\t\tif [[ -s \".tmp/xss_reflected.txt\" ]]; then\n\t\t\t\tprintf \"${yellow}\\n[$(date +'%Y-%m-%d %H:%M:%S')] Running: Dalfox with Axiom${reset}\\n\\n\"\n\t\t\t\taxiom-scan \".tmp/xss_reflected.txt\" -m dalfox --skip-bav $OPTIONS -d \"$DEPTH\" -o \"vulns/xss.txt\" $AXIOM_ARGS 2>>\"$LOGFILE\" >/dev/null\n\t\t\tfi\n\t\tfi\n\n\t\tend_func \"Results are saved in vulns/xss.txt\" \"${FUNCNAME[0]}\"\n\telse\n\t\t# Handle cases where XSS is false, no vulnerable URLs, or already processed\n\t\tif [[ $XSS == false ]]; then\n\t\t\tprintf \"\\n${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] ${FUNCNAME[0]} skipped in this mode or defined in reconftw.cfg ${reset}\\n\"\n\t\telif [[ ! -s \"gf/xss.txt\" ]]; then\n\t\t\tprintf \"\\n${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] ${FUNCNAME[0]} skipped: No URLs potentially vulnerable to XSS ${reset}\\n\\n\"\n\t\telse\n\t\t\tprintf \"${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] ${FUNCNAME[0]} has already been processed. To force execution, delete:\\n    $called_fn_dir/.${FUNCNAME[0]} ${reset}\\n\\n\"\n\t\tfi\n\tfi\n\n}\n\nfunction cors() {\n\n\t# Create necessary directories\n\tif ! mkdir -p .tmp webs vulns; then\n\t\tprintf \"%b[!] Failed to create directories.%b\\n\" \"$bred\" \"$reset\"\n\t\treturn 1\n\tfi\n\n\t# Check if the function should run\n\tif { [[ ! -f \"$called_fn_dir/.${FUNCNAME[0]}\" ]] || [[ $DIFF == true ]]; } && [[ $CORS == true ]] &&\n\t\t! [[ $domain =~ ^[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+$ ]]; then\n\n\t\tstart_func \"${FUNCNAME[0]}\" \"CORS Scan\"\n\n\t\t# Combine webs.txt and webs_uncommon_ports.txt into webs_all.txt if it doesn't exist\n\t\tif [[ ! -s \"webs/webs_all.txt\" ]]; then\n\t\t\tcat webs/webs.txt webs/webs_uncommon_ports.txt 2>/dev/null | anew -q \"webs/webs_all.txt\"\n\t\tfi\n\n\t\t# Proceed only if webs_all.txt exists and is non-empty\n\t\tif [[ -s \"webs/webs_all.txt\" ]]; then\n\t\t\tprintf \"${yellow}\\n[$(date +'%Y-%m-%d %H:%M:%S')] Running: Corsy for CORS Scan${reset}\\n\\n\"\n\t\t\tpython3 \"${tools}/Corsy/corsy.py\" -i \"webs/webs_all.txt\" -o \"vulns/cors.txt\" 2>>\"$LOGFILE\" >/dev/null\n\t\telse\n\t\t\tend_func \"No webs/webs_all.txt file found, CORS Scan skipped.\" \"${FUNCNAME[0]}\"\n\t\t\treturn\n\t\tfi\n\n\t\tend_func \"Results are saved in vulns/cors.txt\" \"${FUNCNAME[0]}\"\n\n\telse\n\t\t# Handle cases where CORS is false, no vulnerable URLs, or already processed\n\t\tif [[ $CORS == false ]]; then\n\t\t\tprintf \"\\n${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] ${FUNCNAME[0]} skipped due to configuration settings.${reset}\\n\"\n\t\telif [[ ! -s \"gf/xss.txt\" ]]; then\n\t\t\tprintf \"\\n${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] ${FUNCNAME[0]} skipped: No URLs available for CORS Scan.${reset}\\n\\n\"\n\t\telse\n\t\t\tprintf \"${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] ${FUNCNAME[0]} has already been processed. To force execution, delete:\\n    $called_fn_dir/.${FUNCNAME[0]} ${reset}\\n\\n\"\n\t\tfi\n\tfi\n\n}\n\nfunction open_redirect() {\n\n\t# Create necessary directories\n\tif ! mkdir -p .tmp webs vulns; then\n\t\tprintf \"%b[!] Failed to create directories.%b\\n\" \"$bred\" \"$reset\"\n\t\treturn 1\n\tfi\n\n\t# Check if the function should run\n\tif { [[ ! -f \"$called_fn_dir/.${FUNCNAME[0]}\" ]] || [[ $DIFF == true ]]; } && [[ $OPEN_REDIRECT == true ]] &&\n\t\t[[ -s \"gf/redirect.txt\" ]] && ! [[ $domain =~ ^[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+$ ]]; then\n\n\t\tstart_func \"${FUNCNAME[0]}\" \"Open Redirects Checks\"\n\n\t\t# Determine whether to proceed based on DEEP flag or number of URLs\n\t\tURL_COUNT=$(wc -l <\"gf/redirect.txt\")\n\t\tif [[ $DEEP == true ]] || [[ $URL_COUNT -le $DEEP_LIMIT ]]; then\n\n\t\t\tprintf \"${yellow}\\n[$(date +'%Y-%m-%d %H:%M:%S')] Running: Open Redirects Payload Generation${reset}\\n\\n\"\n\n\t\t\t# Process redirect.txt with qsreplace and filter lines containing 'FUZZ'\n\t\t\tqsreplace FUZZ <\"gf/redirect.txt\" | sed '/FUZZ/!d' | anew -q \".tmp/tmp_redirect.txt\"\n\n\t\t\t# Run Oralyzer with the generated payloads\n\t\t\tpython3 \"${tools}/Oralyzer/oralyzer.py\" -l \".tmp/tmp_redirect.txt\" -p \"${tools}/Oralyzer/payloads.txt\" >\"vulns/redirect.txt\" 2>>\"$LOGFILE\" >/dev/null\n\n\t\t\t# Remove ANSI color codes from the output\n\t\t\tsed -r -i \"s/\\x1B\\[([0-9]{1,3}(;[0-9]{1,2})?)?[mGK]//g\" \"vulns/redirect.txt\"\n\n\t\t\tend_func \"Results are saved in vulns/redirect.txt\" \"${FUNCNAME[0]}\"\n\t\telse\n\t\t\tend_func \"Skipping Open Redirects: Too many URLs to test, try with --deep flag.\" \"${FUNCNAME[0]}\"\n\t\t\tprintf \"${bgreen}#######################################################################${reset}\\n\"\n\t\tfi\n\telse\n\t\t# Handle cases where OPEN_REDIRECT is false, no vulnerable URLs, or already processed\n\t\tif [[ $OPEN_REDIRECT == false ]]; then\n\t\t\tprintf \"\\n${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] ${FUNCNAME[0]} skipped due to configuration settings.${reset}\\n\"\n\t\telif [[ ! -s \"gf/redirect.txt\" ]]; then\n\t\t\tprintf \"\\n${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] ${FUNCNAME[0]} skipped: No URLs potentially vulnerable to Open Redirect.${reset}\\n\\n\"\n\t\telse\n\t\t\tprintf \"${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] ${FUNCNAME[0]} has already been processed. To force execution, delete:\\n    $called_fn_dir/.${FUNCNAME[0]} ${reset}\\n\\n\"\n\t\tfi\n\tfi\n\n}\n\nfunction ssrf_checks() {\n\n\t# Create necessary directories\n\tif ! mkdir -p .tmp gf vulns; then\n\t\tprintf \"%b[!] Failed to create directories.%b\\n\" \"$bred\" \"$reset\"\n\t\treturn 1\n\tfi\n\n\t# Check if the function should run\n\tif { [[ ! -f \"$called_fn_dir/.${FUNCNAME[0]}\" ]] || [[ $DIFF == true ]]; } && [[ $SSRF_CHECKS == true ]] &&\n\t\t[[ -s \"gf/ssrf.txt\" ]] && ! [[ $domain =~ ^[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+$ ]]; then\n\n\t\tstart_func \"${FUNCNAME[0]}\" \"SSRF Checks\"\n\n\t\t# Handle COLLAB_SERVER configuration\n\t\tif [[ -z $COLLAB_SERVER ]]; then\n\t\t\tinteractsh-client &>.tmp/ssrf_callback.txt &\n\t\t\tsleep 2\n\n\t\t\t# Extract FFUFHASH from interactsh_callback.txt\n\t\t\tCOLLAB_SERVER_FIX=\"FFUFHASH.$(tail -n1 .tmp/ssrf_callback.txt | cut -c 16-)\"\n\t\t\tCOLLAB_SERVER_URL=\"http://$COLLAB_SERVER_FIX\"\n\t\t\tINTERACT=true\n\t\telse\n\t\t\tCOLLAB_SERVER_FIX=\"FFUFHASH.$(echo \"$COLLAB_SERVER\" | sed -r \"s|https?://||\")\"\n\t\t\tINTERACT=false\n\t\tfi\n\n\t\t# Determine whether to proceed based on DEEP flag or URL count\n\t\tURL_COUNT=$(wc -l <\"gf/ssrf.txt\")\n\t\tif [[ $DEEP == true ]] || [[ $URL_COUNT -le $DEEP_LIMIT ]]; then\n\n\t\t\tprintf \"${yellow}\\n[$(date +'%Y-%m-%d %H:%M:%S')] Running: SSRF Payload Generation${reset}\\n\\n\"\n\n\t\t\t# Generate temporary SSRF payloads\n\t\t\tqsreplace \"$COLLAB_SERVER_FIX\" <\"gf/ssrf.txt\" | sed '/FUZZ/!d' | anew -q \".tmp/tmp_ssrf.txt\"\n\n\t\t\tqsreplace \"$COLLAB_SERVER_URL\" <\"gf/ssrf.txt\" | sed '/FUZZ/!d' | anew -q \".tmp/tmp_ssrf.txt\"\n\n\t\t\t# Run FFUF to find requested URLs\n\t\t\tprintf \"${yellow}\\n[$(date +'%Y-%m-%d %H:%M:%S')] Running: FFUF for SSRF Requested URLs${reset}\\n\\n\"\n\t\t\tffuf -v -H \"${HEADER}\" -t \"$FFUF_THREADS\" -rate \"$FFUF_RATELIMIT\" -w \".tmp/tmp_ssrf.txt\" -u \"FUZZ\" 2>/dev/null |\n\t\t\t\tgrep \"URL\" | sed 's/| URL | //' | anew -q \"vulns/ssrf_requested_url.txt\"\n\n\t\t\t# Run FFUF with header injection for SSRF\n\t\t\tprintf \"${yellow}\\n[$(date +'%Y-%m-%d %H:%M:%S')] Running: FFUF for SSRF Requested Headers with COLLAB_SERVER_FIX${reset}\\n\\n\"\n\t\t\tffuf -v -w \".tmp/tmp_ssrf.txt:W1,${tools}/headers_inject.txt:W2\" -H \"${HEADER}\" -H \"W2: ${COLLAB_SERVER_FIX}\" -t \"$FFUF_THREADS\" \\\n\t\t\t\t-rate \"$FFUF_RATELIMIT\" -u \"W1\" 2>/dev/null | anew -q \"vulns/ssrf_requested_headers.txt\"\n\n\t\t\tprintf \"${yellow}\\n[$(date +'%Y-%m-%d %H:%M:%S')] Running: FFUF for SSRF Requested Headers with COLLAB_SERVER_URL${reset}\\n\\n\"\n\t\t\tffuf -v -w \".tmp/tmp_ssrf.txt:W1,${tools}/headers_inject.txt:W2\" -H \"${HEADER}\" -H \"W2: ${COLLAB_SERVER_URL}\" -t \"$FFUF_THREADS\" \\\n\t\t\t\t-rate \"$FFUF_RATELIMIT\" -u \"W1\" 2>/dev/null | anew -q \"vulns/ssrf_requested_headers.txt\"\n\n\t\t\t# Allow time for callbacks to be received\n\t\t\tsleep 5\n\n\t\t\t# Process SSRF callback results if INTERACT is enabled\n\t\t\tif [[ $INTERACT == true ]] && [[ -s \".tmp/ssrf_callback.txt\" ]]; then\n\t\t\t\ttail -n +11 .tmp/ssrf_callback.txt | anew -q \"vulns/ssrf_callback.txt\"\n\t\t\t\tNUMOFLINES=$(tail -n +12 .tmp/ssrf_callback.txt | sed '/^$/d' | wc -l)\n\t\t\t\tnotification \"SSRF: ${NUMOFLINES} callbacks received\" info\n\t\t\tfi\n\n\t\t\tend_func \"Results are saved in vulns/ssrf_*\" \"${FUNCNAME[0]}\"\n\t\telse\n\t\t\tend_func \"Skipping SSRF: Too many URLs to test, try with --deep flag.\" \"${FUNCNAME[0]}\"\n\t\t\tprintf \"${bgreen}#######################################################################${reset}\\n\"\n\t\tfi\n\n\t\t# Terminate interactsh-client if it was started\n\t\tif [[ $INTERACT == true ]]; then\n\t\t\tpkill -f interactsh-client &\n\t\tfi\n\n\telse\n\t\t# Handle cases where SSRF_CHECKS is false, no vulnerable URLs, or already processed\n\t\tif [[ $SSRF_CHECKS == false ]]; then\n\t\t\tprintf \"\\n${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] ${FUNCNAME[0]} skipped in this mode or defined in reconftw.cfg ${reset}\\n\"\n\t\telif [[ ! -s \"gf/ssrf.txt\" ]]; then\n\t\t\tprintf \"\\n${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] ${FUNCNAME[0]} skipped: No URLs potentially vulnerable to SSRF.${reset}\\n\\n\"\n\t\telse\n\t\t\tprintf \"${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] ${FUNCNAME[0]} has already been processed. To force execution, delete:\\n    $called_fn_dir/.${FUNCNAME[0]} ${reset}\\n\\n\"\n\t\tfi\n\tfi\n\n}\n\nfunction crlf_checks() {\n\n\t# Create necessary directories\n\tif ! mkdir -p webs vulns; then\n\t\tprintf \"%b[!] Failed to create directories.%b\\n\" \"$bred\" \"$reset\"\n\t\treturn 1\n\tfi\n\n\t# Check if the function should run\n\tif { [[ ! -f \"$called_fn_dir/.${FUNCNAME[0]}\" ]] || [[ $DIFF == true ]]; } && [[ $CRLF_CHECKS == true ]] &&\n\t\t! [[ $domain =~ ^[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+$ ]]; then\n\n\t\tstart_func \"${FUNCNAME[0]}\" \"CRLF Checks\"\n\n\t\t# Combine webs.txt and webs_uncommon_ports.txt into webs_all.txt if it doesn't exist\n\t\tif [[ ! -s \"webs/webs_all.txt\" ]]; then\n\t\t\tcat webs/webs.txt webs/webs_uncommon_ports.txt 2>/dev/null | anew -q \"webs/webs_all.txt\"\n\t\tfi\n\n\t\t# Determine whether to proceed based on DEEP flag or number of URLs\n\t\tURL_COUNT=$(wc -l <\"webs/webs_all.txt\")\n\t\tif [[ $DEEP == true ]] || [[ $URL_COUNT -le $DEEP_LIMIT ]]; then\n\n\t\t\tprintf \"${yellow}\\n[$(date +'%Y-%m-%d %H:%M:%S')] Running: CRLF Fuzzing${reset}\\n\\n\"\n\n\t\t\t# Run CRLFuzz\n\t\t\tcrlfuzz -l \"webs/webs_all.txt\" -o \"vulns/crlf.txt\" 2>>\"$LOGFILE\" >/dev/null\n\n\t\t\tend_func \"Results are saved in vulns/crlf.txt\" \"${FUNCNAME[0]}\"\n\t\telse\n\t\t\tend_func \"Skipping CRLF: Too many URLs to test, try with --deep flag.\" \"${FUNCNAME[0]}\"\n\t\tfi\n\telse\n\t\t# Handle cases where CRLF_CHECKS is false, no vulnerable URLs, or already processed\n\t\tif [[ $CRLF_CHECKS == false ]]; then\n\t\t\tprintf \"\\n${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] ${FUNCNAME[0]} skipped due to configuration settings.${reset}\\n\"\n\t\telif [[ ! -s \"gf/crlf.txt\" ]]; then\n\t\t\tprintf \"\\n${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] ${FUNCNAME[0]} skipped: No URLs potentially vulnerable to CRLF.${reset}\\n\\n\"\n\t\telse\n\t\t\tprintf \"${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] ${FUNCNAME[0]} has already been processed. To force execution, delete:\\n    $called_fn_dir/.${FUNCNAME[0]} ${reset}\\n\\n\"\n\t\tfi\n\tfi\n\n}\n\nfunction lfi() {\n\n\t# Create necessary directories\n\tif ! mkdir -p .tmp gf vulns; then\n\t\tprintf \"%b[!] Failed to create directories.%b\\n\" \"$bred\" \"$reset\"\n\t\treturn 1\n\tfi\n\n\t# Check if the function should run\n\tif { [[ ! -f \"$called_fn_dir/.${FUNCNAME[0]}\" ]] || [[ $DIFF == true ]]; } && [[ $LFI == true ]] &&\n\t\t[[ -s \"gf/lfi.txt\" ]] && ! [[ $domain =~ ^[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+$ ]]; then\n\n\t\tstart_func \"${FUNCNAME[0]}\" \"LFI Checks\"\n\n\t\t# Ensure gf/lfi.txt is not empty\n\t\tif [[ -s \"gf/lfi.txt\" ]]; then\n\t\t\tprintf \"${yellow}\\n[$(date +'%Y-%m-%d %H:%M:%S')] Running: LFI Payload Generation${reset}\\n\\n\"\n\n\t\t\t# Process lfi.txt with qsreplace and filter lines containing 'FUZZ'\n\t\t\tqsreplace \"FUZZ\" <\"gf/lfi.txt\" | sed '/FUZZ/!d' | anew -q \".tmp/tmp_lfi.txt\"\n\n\t\t\t# Determine whether to proceed based on DEEP flag or number of URLs\n\t\t\tURL_COUNT=$(wc -l <\".tmp/tmp_lfi.txt\")\n\t\t\tif [[ $DEEP == true ]] || [[ $URL_COUNT -le $DEEP_LIMIT ]]; then\n\n\t\t\t\tprintf \"${yellow}\\n[$(date +'%Y-%m-%d %H:%M:%S')] Running: LFI Fuzzing with FFUF${reset}\\n\\n\"\n\n\t\t\t\t# Use Interlace to parallelize FFUF scanning\n\t\t\t\tinterlace -tL \".tmp/tmp_lfi.txt\" -threads \"$INTERLACE_THREADS\" -c \"ffuf -v -r -t ${FFUF_THREADS} -rate ${FFUF_RATELIMIT} -H \\\"${HEADER}\\\" -w \\\"${lfi_wordlist}\\\" -u \\\"_target_\\\" -mr \\\"root:\\\" \" 2>>\"$LOGFILE\" |\n\t\t\t\t\tgrep \"URL\" | sed 's/| URL | //' | anew -q \"vulns/lfi.txt\"\n\n\t\t\t\tend_func \"Results are saved in vulns/lfi.txt\" \"${FUNCNAME[0]}\"\n\t\t\telse\n\t\t\t\tend_func \"Skipping LFI: Too many URLs to test, try with --deep flag.\" \"${FUNCNAME[0]}\"\n\t\t\tfi\n\t\telse\n\t\t\tend_func \"No gf/lfi.txt file found, LFI Checks skipped.\" \"${FUNCNAME[0]}\"\n\t\t\treturn\n\t\tfi\n\telse\n\t\t# Handle cases where LFI is false, no vulnerable URLs, or already processed\n\t\tif [[ $LFI == false ]]; then\n\t\t\tprintf \"\\n${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] ${FUNCNAME[0]} skipped due to configuration settings.${reset}\\n\"\n\t\telif [[ ! -s \"gf/lfi.txt\" ]]; then\n\t\t\tprintf \"\\n${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] ${FUNCNAME[0]} skipped: No URLs potentially vulnerable to LFI.${reset}\\n\\n\"\n\t\telse\n\t\t\tprintf \"${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] ${FUNCNAME[0]} has already been processed. To force execution, delete:\\n    $called_fn_dir/.${FUNCNAME[0]} ${reset}\\n\\n\"\n\t\tfi\n\tfi\n\n}\n\nfunction ssti() {\n\n\t# Create necessary directories\n\tif ! mkdir -p .tmp gf vulns; then\n\t\tprintf \"%b[!] Failed to create directories.%b\\n\" \"$bred\" \"$reset\"\n\t\treturn 1\n\tfi\n\n\t# Check if the function should run\n\tif { [[ ! -f \"$called_fn_dir/.${FUNCNAME[0]}\" ]] || [[ $DIFF == true ]]; } && [[ $SSTI == true ]] &&\n\t\t[[ -s \"gf/ssti.txt\" ]] && ! [[ $domain =~ ^[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+$ ]]; then\n\n\t\tstart_func \"${FUNCNAME[0]}\" \"SSTI Checks\"\n\n\t\t# Ensure gf/ssti.txt is not empty\n\t\tif [[ -s \"gf/ssti.txt\" ]]; then\n\t\t\tprintf \"${yellow}\\n[$(date +'%Y-%m-%d %H:%M:%S')] Running: SSTI Payload Generation${reset}\\n\\n\"\n\n\t\t\t# Process ssti.txt with qsreplace and filter lines containing 'FUZZ'\n\t\t\tqsreplace \"FUZZ\" <\"gf/ssti.txt\" | sed '/FUZZ/!d' | anew -q \".tmp/tmp_ssti.txt\"\n\n\t\t\t# Determine whether to proceed based on DEEP flag or number of URLs\n\t\t\tURL_COUNT=$(wc -l <\".tmp/tmp_ssti.txt\")\n\t\t\tif [[ $DEEP == true ]] || [[ $URL_COUNT -le $DEEP_LIMIT ]]; then\n\n\t\t\t\tprintf \"${yellow}\\n[$(date +'%Y-%m-%d %H:%M:%S')] Running: SSTI Fuzzing with FFUF${reset}\\n\\n\"\n\n\t\t\t\t# Use Interlace to parallelize FFUF scanning\n\t\t\t\tinterlace -tL \".tmp/tmp_ssti.txt\" -threads \"$INTERLACE_THREADS\" -c \"ffuf -v -r -t ${FFUF_THREADS} -rate ${FFUF_RATELIMIT} -H \\\"${HEADER}\\\" -w \\\"${ssti_wordlist}\\\" -u \\\"_target_\\\" -mr \\\"ssti49\\\"\" 2>>\"$LOGFILE\" |\n\t\t\t\t\tgrep \"URL\" | sed 's/| URL | //' | anew -q \"vulns/ssti.txt\"\n\n\t\t\t\tend_func \"Results are saved in vulns/ssti.txt\" \"${FUNCNAME[0]}\"\n\t\t\telse\n\t\t\t\tend_func \"Skipping SSTI: Too many URLs to test, try with --deep flag.\" \"${FUNCNAME[0]}\"\n\t\t\tfi\n\t\telse\n\t\t\tend_func \"No gf/ssti.txt file found, SSTI Checks skipped.\" \"${FUNCNAME[0]}\"\n\t\t\treturn\n\t\tfi\n\telse\n\t\t# Handle cases where SSTI is false, no vulnerable URLs, or already processed\n\t\tif [[ $SSTI == false ]]; then\n\t\t\tprintf \"\\n${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] ${FUNCNAME[0]} skipped due to configuration settings.${reset}\\n\"\n\t\telif [[ ! -s \"gf/ssti.txt\" ]]; then\n\t\t\tprintf \"\\n${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] ${FUNCNAME[0]} skipped: No URLs potentially vulnerable to SSTI.${reset}\\n\\n\"\n\t\telse\n\t\t\tprintf \"${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] ${FUNCNAME[0]} has already been processed. To force execution, delete:\\n    $called_fn_dir/.${FUNCNAME[0]} ${reset}\\n\\n\"\n\t\tfi\n\tfi\n\n}\n\nfunction sqli() {\n\n\t# Create necessary directories\n\tif ! mkdir -p .tmp gf vulns; then\n\t\tprintf \"%b[!] Failed to create directories.%b\\n\" \"$bred\" \"$reset\"\n\t\treturn 1\n\tfi\n\n\t# Check if the function should run\n\tif { [[ ! -f \"$called_fn_dir/.${FUNCNAME[0]}\" ]] || [[ $DIFF == true ]]; } && [[ $SQLI == true ]] &&\n\t\t[[ -s \"gf/sqli.txt\" ]] && ! [[ $domain =~ ^[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+$ ]]; then\n\n\t\tstart_func \"${FUNCNAME[0]}\" \"SQLi Checks\"\n\n\t\t# Ensure gf/sqli.txt is not empty\n\t\tif [[ -s \"gf/sqli.txt\" ]]; then\n\t\t\tprintf \"${yellow}\\n[$(date +'%Y-%m-%d %H:%M:%S')] Running: SQLi Payload Generation${reset}\\n\\n\"\n\n\t\t\t# Process sqli.txt with qsreplace and filter lines containing 'FUZZ'\n\t\t\tqsreplace \"FUZZ\" <\"gf/sqli.txt\" | sed '/FUZZ/!d' | anew -q \".tmp/tmp_sqli.txt\"\n\n\t\t\t# Determine whether to proceed based on DEEP flag or number of URLs\n\t\t\tURL_COUNT=$(wc -l <\".tmp/tmp_sqli.txt\")\n\t\t\tif [[ $DEEP == true ]] || [[ $URL_COUNT -le $DEEP_LIMIT ]]; then\n\n\t\t\t\t# Check if SQLMAP is enabled and run SQLMap\n\t\t\t\tif [[ $SQLMAP == true ]]; then\n\t\t\t\t\tprintf \"${yellow}\\n[$(date +'%Y-%m-%d %H:%M:%S')] Running: SQLMap for SQLi Checks${reset}\\n\\n\"\n\t\t\t\t\tpython3 \"${tools}/sqlmap/sqlmap.py\" -m \".tmp/tmp_sqli.txt\" -b -o --smart \\\n\t\t\t\t\t\t--batch --disable-coloring --random-agent --output-dir=\"vulns/sqlmap\" 2>>\"$LOGFILE\" >/dev/null\n\t\t\t\tfi\n\n\t\t\t\t# Check if GHAURI is enabled and run Ghauri\n\t\t\t\tif [[ $GHAURI == true ]]; then\n\t\t\t\t\tprintf \"${yellow}\\n[$(date +'%Y-%m-%d %H:%M:%S')] Running: Ghauri for SQLi Checks${reset}\\n\\n\"\n\t\t\t\t\tinterlace -tL \".tmp/tmp_sqli.txt\" -threads \"$INTERLACE_THREADS\" -c \"ghauri -u _target_ --batch -H \\\"${HEADER}\\\" --force-ssl >> vulns/ghauri_log.txt\" 2>>\"$LOGFILE\" >/dev/null\n\t\t\t\tfi\n\n\t\t\t\tend_func \"Results are saved in vulns/sqlmap folder\" \"${FUNCNAME[0]}\"\n\t\t\telse\n\t\t\t\tend_func \"Skipping SQLi: Too many URLs to test, try with --deep flag.\" \"${FUNCNAME[0]}\"\n\t\t\tfi\n\t\telse\n\t\t\tend_func \"No gf/sqli.txt file found, SQLi Checks skipped.\" \"${FUNCNAME[0]}\"\n\t\t\treturn\n\t\tfi\n\telse\n\t\t# Handle cases where SQLI is false, no vulnerable URLs, or already processed\n\t\tif [[ $SQLI == false ]]; then\n\t\t\tprintf \"\\n${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] ${FUNCNAME[0]} skipped due to configuration settings.${reset}\\n\"\n\t\telif [[ ! -s \"gf/sqli.txt\" ]]; then\n\t\t\tprintf \"\\n${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] ${FUNCNAME[0]} skipped: No URLs potentially vulnerable to SQLi.${reset}\\n\\n\"\n\t\telse\n\t\t\tprintf \"${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] ${FUNCNAME[0]} has already been processed. To force execution, delete:\\n    $called_fn_dir/.${FUNCNAME[0]} ${reset}\\n\\n\"\n\t\tfi\n\tfi\n\n}\n\nfunction test_ssl() {\n\n\t# Create necessary directories\n\tif ! mkdir -p hosts vulns; then\n\t\tprintf \"%b[!] Failed to create directories.%b\\n\" \"$bred\" \"$reset\"\n\t\treturn 1\n\tfi\n\n\t# Check if the function should run\n\tif { [[ ! -f \"$called_fn_dir/.${FUNCNAME[0]}\" ]] || [[ $DIFF == true ]]; } && [[ $TEST_SSL == true ]] &&\n\t\t! [[ $domain =~ ^[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+$ ]]; then\n\n\t\tstart_func \"${FUNCNAME[0]}\" \"SSL Test\"\n\n\t\t# Handle multi-domain scenarios\n\t\tif [[ -n $multi ]] && [[ ! -f \"$dir/hosts/ips.txt\" ]]; then\n\t\t\techo \"$domain\" >\"$dir/hosts/ips.txt\"\n\t\tfi\n\n\t\t# Run testssl.sh\n\t\tprintf \"${yellow}\\n[$(date +'%Y-%m-%d %H:%M:%S')] Running: SSL Test with testssl.sh${reset}\\n\\n\"\n\t\t\"${tools}/testssl.sh/testssl.sh\" --quiet --color 0 -U -iL \"hosts/ips.txt\" 2>>\"$LOGFILE\" >\"vulns/testssl.txt\"\n\n\t\tend_func \"Results are saved in vulns/testssl.txt\" \"${FUNCNAME[0]}\"\n\n\telse\n\t\t# Handle cases where TEST_SSL is false, no vulnerable URLs, or already processed\n\t\tif [[ $TEST_SSL == false ]]; then\n\t\t\tprintf \"\\n${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] ${FUNCNAME[0]} skipped due to configuration settings.${reset}\\n\"\n\t\telif [[ ! -s \"gf/testssl.txt\" ]]; then\n\t\t\tprintf \"\\n${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] ${FUNCNAME[0]} skipped: No URLs potentially vulnerable to SSL issues.${reset}\\n\\n\"\n\t\telse\n\t\t\tprintf \"${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] ${FUNCNAME[0]} has already been processed. To force execution, delete:\\n    $called_fn_dir/.${FUNCNAME[0]} ${reset}\\n\\n\"\n\t\tfi\n\tfi\n\n}\n\nfunction spraying() {\n\n\t# Create necessary directories\n\tif ! mkdir -p \"vulns\"; then\n\t\tprintf \"%b[!] Failed to create directories.%b\\n\" \"$bred\" \"$reset\"\n\t\treturn 1\n\tfi\n\n\t# Check if the function should run\n\tif { [[ ! -f \"$called_fn_dir/.${FUNCNAME[0]}\" ]] || [[ $DIFF == true ]]; } && [[ $SPRAY == true ]] &&\n\t\t[[ -s \"$dir/hosts/portscan_active.gnmap\" ]] && ! [[ $domain =~ ^[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+$ ]]; then\n\n\t\tstart_func \"${FUNCNAME[0]}\" \"Password Spraying\"\n\n\t\t# Ensure portscan_active.gnmap exists and is not empty\n\t\tif [[ ! -s \"$dir/hosts/portscan_active.gnmap\" ]]; then\n\t\t\tprintf \"%b[!] File $dir/hosts/portscan_active.gnmap does not exist or is empty.%b\\n\" \"$bred\" \"$reset\"\n\t\t\tend_func \"Port scan results missing. Password Spraying aborted.\" \"${FUNCNAME[0]}\"\n\t\t\treturn 1\n\t\tfi\n\n\t\tprintf \"${yellow}\\n[$(date +'%Y-%m-%d %H:%M:%S')] Running: Password Spraying with BruteSpray${reset}\\n\\n\"\n\n\t\t# Run BruteSpray for password spraying\n\t\tbrutespray -f \"$dir/hosts/portscan_active.gnmap\" -T \"$BRUTESPRAY_CONCURRENCE\" -o \"$dir/vulns/brutespray\" 2>>\"$LOGFILE\" >/dev/null\n\n\t\tend_func \"Results are saved in vulns/brutespray folder\" \"${FUNCNAME[0]}\"\n\n\telse\n\t\t# Handle cases where SPRAY is false, required files are missing, or already processed\n\t\tif [[ $SPRAY == false ]]; then\n\t\t\tprintf \"\\n${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] ${FUNCNAME[0]} skipped due to configuration settings.${reset}\\n\"\n\t\telif [[ ! -s \"$dir/hosts/portscan_active.gnmap\" ]]; then\n\t\t\tprintf \"\\n${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] ${FUNCNAME[0]} skipped: No active port scan results found.${reset}\\n\\n\"\n\t\telse\n\t\t\tprintf \"${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] ${FUNCNAME[0]} has already been processed. To force execution, delete:\\n    $called_fn_dir/.${FUNCNAME[0]} ${reset}\\n\\n\"\n\t\tfi\n\tfi\n\n}\n\nfunction command_injection() {\n\n\t# Create necessary directories\n\tif ! mkdir -p .tmp gf vulns; then\n\t\tprintf \"%b[!] Failed to create directories.%b\\n\" \"$bred\" \"$reset\"\n\t\treturn 1\n\tfi\n\n\t# Check if the function should run\n\tif { [[ ! -f \"$called_fn_dir/.${FUNCNAME[0]}\" ]] || [[ $DIFF == true ]]; } && [[ $COMM_INJ == true ]] &&\n\t\t[[ -s \"gf/rce.txt\" ]] && ! [[ $domain =~ ^[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+$ ]]; then\n\n\t\tstart_func \"${FUNCNAME[0]}\" \"Command Injection Checks\"\n\n\t\t# Ensure gf/rce.txt is not empty and process it\n\t\tif [[ -s \"gf/rce.txt\" ]]; then\n\t\t\tprintf \"${yellow}\\n[$(date +'%Y-%m-%d %H:%M:%S')] Running: Command Injection Payload Generation${reset}\\n\\n\"\n\n\t\t\t# Process rce.txt with qsreplace and filter lines containing 'FUZZ'\n\t\t\tqsreplace \"FUZZ\" <\"gf/rce.txt\" | sed '/FUZZ/!d' | anew -q \".tmp/tmp_rce.txt\"\n\n\t\t\t# Determine whether to proceed based on DEEP flag or number of URLs\n\t\t\tURL_COUNT=$(wc -l <\".tmp/tmp_rce.txt\")\n\t\t\tif [[ $DEEP == true ]] || [[ $URL_COUNT -le $DEEP_LIMIT ]]; then\n\n\t\t\t\t# Run Commix if enabled\n\t\t\t\tif [[ $SQLMAP == true ]]; then\n\t\t\t\t\tprintf \"${yellow}\\n[$(date +'%Y-%m-%d %H:%M:%S')] Running: Commix for Command Injection Checks${reset}\\n\\n\"\n\t\t\t\t\tpython3 \"${tools}/commix/commix.py\" --batch -m \".tmp/tmp_rce.txt\" --output-dir \"vulns/command_injection\" 2>>\"$LOGFILE\" >/dev/null\n\t\t\t\tfi\n\n\t\t\t\t# Additional tools can be integrated here (e.g., Ghauri, sqlmap)\n\n\t\t\t\tend_func \"Results are saved in vulns/command_injection folder\" \"${FUNCNAME[0]}\"\n\t\t\telse\n\t\t\t\tend_func \"Skipping Command Injection: Too many URLs to test, try with --deep flag.\" \"${FUNCNAME[0]}\"\n\t\t\tfi\n\t\telse\n\t\t\tend_func \"No gf/rce.txt file found, Command Injection Checks skipped.\" \"${FUNCNAME[0]}\"\n\t\t\treturn\n\t\tfi\n\telse\n\t\t# Handle cases where COMM_INJ is false, no vulnerable URLs, or already processed\n\t\tif [[ $COMM_INJ == false ]]; then\n\t\t\tprintf \"\\n${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] ${FUNCNAME[0]} skipped due to configuration settings.${reset}\\n\"\n\t\telif [[ ! -s \"gf/rce.txt\" ]]; then\n\t\t\tprintf \"\\n${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] ${FUNCNAME[0]} skipped: No URLs potentially vulnerable to Command Injection.${reset}\\n\\n\"\n\t\telse\n\t\t\tprintf \"${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] ${FUNCNAME[0]} has already been processed. To force execution, delete:\\n    $called_fn_dir/.${FUNCNAME[0]} ${reset}\\n\\n\"\n\t\tfi\n\tfi\n\n}\n\nfunction 4xxbypass() {\n\n\t# Create necessary directories\n\tif ! mkdir -p .tmp fuzzing vulns; then\n\t\tprintf \"%b[!] Failed to create directories.%b\\n\" \"$bred\" \"$reset\"\n\t\treturn 1\n\tfi\n\n\t# Check if the function should run\n\tif { [[ ! -f \"$called_fn_dir/.${FUNCNAME[0]}\" ]] || [[ $DIFF == true ]]; } && [[ $BYPASSER4XX == true ]] &&\n\t\t! [[ $domain =~ ^[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+$ ]]; then\n\n\t\t# Extract relevant URLs starting with 4xx but not 404\n\t\tprintf \"${yellow}\\n[$(date +'%Y-%m-%d %H:%M:%S')] Running: 403 Bypass${reset}\\n\\n\"\n\t\tgrep -E '^4' \"fuzzing/fuzzing_full.txt\" 2>/dev/null | grep -Ev '^404' | awk '{print $3}' | anew -q \".tmp/403test.txt\"\n\n\t\t# Count the number of URLs to process\n\t\tURL_COUNT=$(wc -l <\".tmp/403test.txt\")\n\t\tif [[ $DEEP == true ]] || [[ $URL_COUNT -le $DEEP_LIMIT ]]; then\n\n\t\t\tstart_func \"${FUNCNAME[0]}\" \"403 Bypass\"\n\n\t\t\t# Navigate to nomore403 tool directory\n\t\t\tif ! pushd \"${tools}/nomore403\" >/dev/null; then\n\t\t\t\tprintf \"%b[!] Failed to navigate to nomore403 directory.%b\\n\" \"$bred\" \"$reset\"\n\t\t\t\tend_func \"Failed to navigate to nomore403 directory during 403 Bypass.\" \"${FUNCNAME[0]}\"\n\t\t\t\treturn 1\n\t\t\tfi\n\n\t\t\t# Run nomore403 on the processed URLs\n\t\t\t./nomore403 <\"$dir/.tmp/403test.txt\" >\"$dir/.tmp/4xxbypass.txt\" 2>>\"$LOGFILE\"\n\n\t\t\t# Return to the original directory\n\t\t\tif ! popd >/dev/null; then\n\t\t\t\tprintf \"%b[!] Failed to return to the original directory.%b\\n\" \"$bred\" \"$reset\"\n\t\t\t\tend_func \"Failed to return to the original directory during 403 Bypass.\" \"${FUNCNAME[0]}\"\n\t\t\t\treturn 1\n\t\t\tfi\n\n\t\t\t# Append unique bypassed URLs to the vulns directory\n\t\t\tif [[ -s \"$dir/.tmp/4xxbypass.txt\" ]]; then\n\t\t\t\tcat \"$dir/.tmp/4xxbypass.txt\" | anew -q \"vulns/4xxbypass.txt\"\n\t\t\tfi\n\n\t\t\tend_func \"Results are saved in vulns/4xxbypass.txt\" \"${FUNCNAME[0]}\"\n\n\t\telse\n\t\t\tnotification \"Too many URLs to bypass, skipping\" warn\n\t\t\tend_func \"Skipping Command Injection: Too many URLs to test, try with --deep flag.\" \"${FUNCNAME[0]}\"\n\t\tfi\n\n\telse\n\t\t# Handle cases where BYPASSER4XX is false, no vulnerable URLs, or already processed\n\t\tif [[ $BYPASSER4XX == false ]]; then\n\t\t\tprintf \"\\n${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] ${FUNCNAME[0]} skipped due to configuration settings.${reset}\\n\"\n\t\telif [[ ! -s \"fuzzing/fuzzing_full.txt\" ]]; then\n\t\t\tprintf \"\\n${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] ${FUNCNAME[0]} skipped: No URLs potentially vulnerable to 4xx bypass.${reset}\\n\\n\"\n\t\telse\n\t\t\tprintf \"${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] ${FUNCNAME[0]} has already been processed. To force execution, delete:\\n    $called_fn_dir/.${FUNCNAME[0]} ${reset}\\n\\n\"\n\t\tfi\n\tfi\n\n}\n\nfunction prototype_pollution() {\n\n\t# Create necessary directories\n\tif ! mkdir -p .tmp webs vulns; then\n\t\tprintf \"%b[!] Failed to create directories.%b\\n\" \"$bred\" \"$reset\"\n\t\treturn 1\n\tfi\n\n\t# Check if the function should run\n\tif { [[ ! -f \"$called_fn_dir/.${FUNCNAME[0]}\" ]] || [[ $DIFF == true ]]; } && [[ $PROTO_POLLUTION == true ]] &&\n\t\t! [[ $domain =~ ^[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+$ ]]; then\n\n\t\tstart_func \"${FUNCNAME[0]}\" \"Prototype Pollution Checks\"\n\n\t\t# Determine whether to proceed based on DEEP flag or number of URLs\n\t\tURL_COUNT=$(wc -l <\"webs/url_extract_nodupes.txt\" 2>/dev/null)\n\t\tif [[ $DEEP == true ]] || [[ $URL_COUNT -le $DEEP_LIMIT ]]; then\n\n\t\t\t# Ensure fuzzing_full.txt exists and has content\n\t\t\tif [[ -s \"webs/url_extract_nodupes.txt\" ]]; then\n\t\t\t\tprintf \"${yellow}\\n[$(date +'%Y-%m-%d %H:%M:%S')] Running: Prototype Pollution Mapping${reset}\\n\\n\"\n\n\t\t\t\t# Process URL list with ppmap and save results\n\t\t\t\tppmap <\"webs/url_extract_nodupes.txt\" >\".tmp/prototype_pollution.txt\" 2>>\"$LOGFILE\"\n\n\t\t\t\t# Filter and save relevant results\n\t\t\t\tif [[ -s \".tmp/prototype_pollution.txt\" ]]; then\n\t\t\t\t\tgrep \"EXPL\" \".tmp/prototype_pollution.txt\" | anew -q \"vulns/prototype_pollution.txt\"\n\t\t\t\tfi\n\n\t\t\t\tend_func \"Results are saved in vulns/prototype_pollution.txt\" \"${FUNCNAME[0]}\"\n\t\t\telse\n\t\t\t\tprintf \"%b[!] File webs/url_extract_nodupes.txt is missing or empty.%b\\n\" \"$bred\" \"$reset\"\n\t\t\t\tend_func \"File webs/url_extract_nodupes.txt is missing or empty.\" \"${FUNCNAME[0]}\"\n\t\t\t\treturn 1\n\t\t\tfi\n\n\t\telse\n\t\t\tend_func \"Skipping Prototype Pollution: Too many URLs to test, try with --deep flag.\" \"${FUNCNAME[0]}\"\n\t\tfi\n\n\telse\n\t\t# Handle cases where PROTO_POLLUTION is false, no vulnerable URLs, or already processed\n\t\tif [[ $PROTO_POLLUTION == false ]]; then\n\t\t\tprintf \"\\n${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] ${FUNCNAME[0]} skipped due to configuration settings.${reset}\\n\"\n\t\telif [[ ! -s \"webs/url_extract_nodupes.txt\" ]]; then\n\t\t\tprintf \"\\n${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] ${FUNCNAME[0]} skipped: No URLs potentially vulnerable to Prototype Pollution.${reset}\\n\\n\"\n\t\telse\n\t\t\tprintf \"${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] ${FUNCNAME[0]} has already been processed. To force execution, delete:\\n    $called_fn_dir/.${FUNCNAME[0]} ${reset}\\n\\n\"\n\t\tfi\n\tfi\n\n}\n\nfunction smuggling() {\n\n\t# Create necessary directories\n\tif ! mkdir -p .tmp webs vulns/smuggling; then\n\t\tprintf \"%b[!] Failed to create directories.%b\\n\" \"$bred\" \"$reset\"\n\t\treturn 1\n\tfi\n\n\t# Check if the function should run\n\tif { [[ ! -f \"$called_fn_dir/.${FUNCNAME[0]}\" ]] || [[ $DIFF == true ]]; } && [[ $SMUGGLING == true ]] &&\n\t\t! [[ $domain =~ ^[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+$ ]]; then\n\n\t\tstart_func \"${FUNCNAME[0]}\" \"HTTP Request Smuggling Checks\"\n\n\t\t# Combine webs.txt and webs_uncommon_ports.txt into webs_all.txt if it doesn't exist\n\t\tif [[ ! -s \"webs/webs_all.txt\" ]]; then\n\t\t\tcat \"webs/webs.txt\" \"webs/webs_uncommon_ports.txt\" 2>/dev/null | anew -q \"webs/webs_all.txt\"\n\t\tfi\n\n\t\t# Determine whether to proceed based on DEEP flag or number of URLs\n\t\tURL_COUNT=$(wc -l <\"webs/webs_all.txt\")\n\t\tif [[ $DEEP == true ]] || [[ $URL_COUNT -le $DEEP_LIMIT ]]; then\n\n\t\t\tprintf \"${yellow}\\n[$(date +'%Y-%m-%d %H:%M:%S')] Running: HTTP Request Smuggling Checks${reset}\\n\\n\"\n\n\t\t\t# Navigate to smuggler tool directory\n\t\t\tif ! pushd \"${tools}/smuggler\" >/dev/null; then\n\t\t\t\tprintf \"%b[!] Failed to navigate to smuggler directory.%b\\n\" \"$bred\" \"$reset\"\n\t\t\t\tend_func \"Failed to navigate to smuggler directory during HTTP Request Smuggling Checks.\" \"${FUNCNAME[0]}\"\n\t\t\t\treturn 1\n\t\t\tfi\n\n\t\t\t# Run smuggler.py on the list of URLs\n\t\t\tpython3 \"smuggler.py\" -f \"$dir/webs/webs_all.txt\" -o \"$dir/.tmp/smuggling.txt\" 2>>\"$LOGFILE\" >/dev/null\n\n\t\t\t# Move payload files to vulns/smuggling/\n\t\t\tfind \"payloads\" -type f ! -name \"README*\" -exec mv {} \"$dir/vulns/smuggling/\" \\;\n\n\t\t\t# Return to the original directory\n\t\t\tif ! popd >/dev/null; then\n\t\t\t\tprintf \"%b[!] Failed to return to the original directory.%b\\n\" \"$bred\" \"$reset\"\n\t\t\t\tend_func \"Failed to return to the original directory during HTTP Request Smuggling Checks.\" \"${FUNCNAME[0]}\"\n\t\t\t\treturn 1\n\t\t\tfi\n\n\t\t\t# Append unique smuggling results to vulns directory\n\t\t\tif [[ -s \"$dir/.tmp/smuggling.txt\" ]]; then\n\t\t\t\tcat \"$dir/.tmp/smuggling.txt\" | grep \"EXPL\" | anew -q \"vulns/prototype_pollution.txt\"\n\t\t\tfi\n\n\t\t\tend_func \"Results are saved in vulns/smuggling_log.txt and findings in vulns/smuggling/\" \"${FUNCNAME[0]}\"\n\n\t\telse\n\t\t\tnotification \"Too many URLs to bypass, skipping\" warn\n\t\t\tend_func \"Skipping HTTP Request Smuggling: Too many URLs to test, try with --deep flag.\" \"${FUNCNAME[0]}\"\n\t\tfi\n\n\telse\n\t\t# Handle cases where SMUGGLING is false, no vulnerable URLs, or already processed\n\t\tif [[ $SMUGGLING == false ]]; then\n\t\t\tprintf \"\\n${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] ${FUNCNAME[0]} skipped in this mode or defined in reconftw.cfg ${reset}\\n\"\n\t\telif [[ ! -s \"webs/webs_all.txt\" ]]; then\n\t\t\tprintf \"\\n${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] ${FUNCNAME[0]} skipped: No URLs potentially vulnerable to HTTP Request Smuggling.${reset}\\n\\n\"\n\t\telse\n\t\t\tprintf \"${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] ${FUNCNAME[0]} has already been processed. To force execution, delete:\\n    $called_fn_dir/.${FUNCNAME[0]} ${reset}\\n\\n\"\n\t\tfi\n\tfi\n\n}\n\nfunction webcache() {\n\n\t# Create necessary directories\n\tif ! mkdir -p .tmp webs vulns; then\n\t\tprintf \"%b[!] Failed to create directories.%b\\n\" \"$bred\" \"$reset\"\n\t\treturn 1\n\tfi\n\n\t# Check if the function should run\n\tif { [[ ! -f \"$called_fn_dir/.${FUNCNAME[0]}\" ]] || [[ $DIFF == true ]]; } && [[ $WEBCACHE == true ]] &&\n\t\t! [[ $domain =~ ^[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+$ ]]; then\n\n\t\tstart_func \"${FUNCNAME[0]}\" \"Web Cache Poisoning Checks\"\n\n\t\t# Combine webs.txt and webs_uncommon_ports.txt into webs_all.txt if it doesn't exist\n\t\tif [[ ! -s \"webs/webs_all.txt\" ]]; then\n\t\t\tcat webs/webs.txt webs/webs_uncommon_ports.txt 2>/dev/null | anew -q \"webs/webs_all.txt\"\n\t\tfi\n\n\t\t# Determine whether to proceed based on DEEP flag or number of URLs\n\t\tURL_COUNT=$(wc -l <\"webs/webs_all.txt\")\n\t\tif [[ $DEEP == true ]] || [[ $URL_COUNT -le $DEEP_LIMIT ]]; then\n\n\t\t\tprintf \"${yellow}\\n[$(date +'%Y-%m-%d %H:%M:%S')] Running: Web Cache Poisoning Checks${reset}\\n\\n\"\n\n\t\t\t# Navigate to Web-Cache-Vulnerability-Scanner tool directory\n\t\t\tif ! pushd \"${tools}/Web-Cache-Vulnerability-Scanner\" >/dev/null; then\n\t\t\t\tprintf \"%b[!] Failed to navigate to Web-Cache-Vulnerability-Scanner directory.%b\\n\" \"$bred\" \"$reset\"\n\t\t\t\tend_func \"Failed to navigate to Web-Cache-Vulnerability-Scanner directory during Web Cache Poisoning Checks.\" \"${FUNCNAME[0]}\"\n\t\t\t\treturn 1\n\t\t\tfi\n\n\t\t\t# Run the Web-Cache-Vulnerability-Scanner\n\t\t\t./Web-Cache-Vulnerability-Scanner -u \"file:$dir/webs/webs_all.txt\" -v 0 2>>\"$LOGFILE\" |\n\t\t\t\tanew -q \"$dir/.tmp/webcache.txt\"\n\n\t\t\t# Return to the original directory\n\t\t\tif ! popd >/dev/null; then\n\t\t\t\tprintf \"%b[!] Failed to return to the original directory.%b\\n\" \"$bred\" \"$reset\"\n\t\t\t\tend_func \"Failed to return to the original directory during Web Cache Poisoning Checks.\" \"${FUNCNAME[0]}\"\n\t\t\t\treturn 1\n\t\t\tfi\n\n\t\t\t# Append unique findings to vulns/webcache.txt\n\t\t\tif [[ -s \"$dir/.tmp/webcache.txt\" ]]; then\n\t\t\t\tcat \"$dir/.tmp/webcache.txt\" | anew -q \"vulns/webcache.txt\"\n\t\t\tfi\n\n\t\t\tend_func \"Results are saved in vulns/webcache.txt\" \"${FUNCNAME[0]}\"\n\n\t\telse\n\t\t\tend_func \"Skipping Web Cache Poisoning: Too many URLs to test, try with --deep flag.\" \"${FUNCNAME[0]}\"\n\t\tfi\n\n\telse\n\t\t# Handle cases where WEBCACHE is false, no vulnerable URLs, or already processed\n\t\tif [[ $WEBCACHE == false ]]; then\n\t\t\tprintf \"\\n${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] ${FUNCNAME[0]} skipped due to configuration settings.${reset}\\n\"\n\t\telif [[ ! -s \"fuzzing/fuzzing_full.txt\" ]]; then\n\t\t\tprintf \"\\n${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] ${FUNCNAME[0]} skipped: No URLs potentially vulnerable to Web Cache Poisoning.${reset}\\n\\n\"\n\t\telse\n\t\t\tprintf \"${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] ${FUNCNAME[0]} has already been processed. To force execution, delete:\\n    $called_fn_dir/.${FUNCNAME[0]} ${reset}\\n\\n\"\n\t\tfi\n\tfi\n\n}\n\nfunction fuzzparams() {\n\n\t# Create necessary directories\n\tif ! mkdir -p .tmp webs vulns; then\n\t\tprintf \"%b[!] Failed to create directories.%b\\n\" \"$bred\" \"$reset\"\n\t\treturn 1\n\tfi\n\n\t# Check if the function should run\n\tif { [[ ! -f \"$called_fn_dir/.${FUNCNAME[0]}\" ]] || [[ $DIFF == true ]]; } && [[ $FUZZPARAMS == true ]] &&\n\t\t! [[ $domain =~ ^[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+$ ]]; then\n\n\t\tstart_func \"${FUNCNAME[0]}\" \"Fuzzing Parameters Values Checks\"\n\n\t\t# Determine if we should proceed based on DEEP flag or number of URLs\n\t\tURL_COUNT=$(wc -l <\"webs/url_extract_nodupes.txt\")\n\t\tif [[ $DEEP == true ]] || [[ $URL_COUNT -le $DEEP_LIMIT2 ]]; then\n\n\t\t\tif [[ $AXIOM != true ]]; then\n\t\t\t\tprintf \"${yellow}\\n[$(date +'%Y-%m-%d %H:%M:%S')] Running: Nuclei Setup and Execution${reset}\\n\\n\"\n\n\t\t\t\t# Update Nuclei\n\t\t\t\tif ! nuclei -update 2>>\"$LOGFILE\" >/dev/null; then\n\t\t\t\t\tprintf \"%b[!] Nuclei update failed.%b\\n\" \"$bred\" \"$reset\"\n\t\t\t\t\tend_func \"Nuclei update failed.\" \"${FUNCNAME[0]}\"\n\t\t\t\t\treturn 1\n\t\t\t\tfi\n\n\t\t\t\t# Pull latest fuzzing templates\n\t\t\t\tif ! git -C ${NUCLEI_FUZZING_TEMPLATES_PATH} pull 2>>\"$LOGFILE\"; then\n\t\t\t\t\tprintf \"%b[!] Failed to pull latest fuzzing templates.%b\\n\" \"$bred\" \"$reset\"\n\t\t\t\t\tend_func \"Failed to pull latest fuzzing templates.\" \"${FUNCNAME[0]}\"\n\t\t\t\t\treturn 1\n\t\t\t\tfi\n\n\t\t\t\t# Execute Nuclei with the fuzzing templates\n\t\t\t\tnuclei -silent -retries 3 -rl \"$NUCLEI_RATELIMIT\" -t ${NUCLEI_FUZZING_TEMPLATES_PATH} -dast -o \".tmp/fuzzparams.txt\" <\"webs/url_extract_nodupes.txt\" 2>>\"$LOGFILE\"\n\n\t\t\telse\n\t\t\t\tprintf \"${yellow}\\n[$(date +'%Y-%m-%d %H:%M:%S')] Running: Axiom with Nuclei${reset}\\n\\n\"\n\n\t\t\t\t# Clone fuzzing-templates if not already present\n\t\t\t\tif [[ ! -d \"/home/op/fuzzing-templates\" ]]; then\n\t\t\t\t\taxiom-exec \"git clone https://github.com/projectdiscovery/fuzzing-templates /home/op/fuzzing-templates\" &>/dev/null\n\t\t\t\tfi\n\n\t\t\t\t# Execute Axiom scan with Nuclei\n\t\t\t\taxiom-scan \"webs/url_extract_nodupes.txt\" -m nuclei -nh -retries 3 -w \"/home/op/fuzzing-templates\" -rl \"$NUCLEI_RATELIMIT\" -dast -o \".tmp/fuzzparams.txt\" $AXIOM_EXTRA_ARGS 2>>\"$LOGFILE\" >/dev/null\n\t\t\tfi\n\n\t\t\t# Append unique results to vulns/fuzzparams.txt\n\t\t\tif [[ -s \".tmp/fuzzparams.txt\" ]]; then\n\t\t\t\tcat \".tmp/fuzzparams.txt\" | anew -q \"vulns/fuzzparams.txt\"\n\t\t\tfi\n\n\t\t\tend_func \"Results are saved in vulns/fuzzparams.txt\" \"${FUNCNAME[0]}\"\n\n\t\telse\n\t\t\tend_func \"Fuzzing Parameters Values: Too many entries to test, try with --deep flag\" \"${FUNCNAME[0]}\"\n\t\tfi\n\n\telse\n\t\t# Handle cases where FUZZPARAMS is false, no vulnerable URLs, or already processed\n\t\tif [[ $FUZZPARAMS == false ]]; then\n\t\t\tprintf \"\\n${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] ${FUNCNAME[0]} skipped due to configuration settings.${reset}\\n\"\n\t\telif [[ ! -s \"webs/url_extract_nodupes.txt\" ]]; then\n\t\t\tprintf \"\\n${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] ${FUNCNAME[0]} skipped: No URLs potentially vulnerable to Fuzzing Parameters.${reset}\\n\\n\"\n\t\telse\n\t\t\tprintf \"${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] ${FUNCNAME[0]} has already been processed. To force execution, delete:\\n    $called_fn_dir/.${FUNCNAME[0]} ${reset}\\n\\n\"\n\t\tfi\n\tfi\n\n}\n\n###############################################################################################################\n########################################## OPTIONS & MGMT #####################################################\n###############################################################################################################\n\nfunction deleteOutScoped() {\n\tif [[ -s $1 ]]; then\n\t\tcat $1 | while read outscoped; do\n\t\t\tif grep -q \"^[*]\" <<<$outscoped; then\n\t\t\t\toutscoped=\"${outscoped:1}\"\n\t\t\t\tsed -i /\"$outscoped$\"/d $2\n\t\t\telse\n\t\t\t\tsed -i /$outscoped/d $2\n\t\t\tfi\n\t\tdone\n\tfi\n}\n\nfunction getElapsedTime {\n\truntime=\"\"\n\tlocal T=$2-$1\n\tlocal D=$((T / 60 / 60 / 24))\n\tlocal H=$((T / 60 / 60 % 24))\n\tlocal M=$((T / 60 % 60))\n\tlocal S=$((T % 60))\n\t((D > 0)) && runtime=\"$runtime$D days, \"\n\t((H > 0)) && runtime=\"$runtime$H hours, \"\n\t((M > 0)) && runtime=\"$runtime$M minutes, \"\n\truntime=\"$runtime$S seconds.\"\n}\n\nfunction zipSnedOutputFolder {\n\tzip_name1=$(date +\"%Y_%m_%d-%H.%M.%S\")\n\tzip_name=\"${zip_name1}_${domain}.zip\" 2>>\"$LOGFILE\" >/dev/null\n\t(cd \"$dir\" && zip -r \"$zip_name\" .) 2>>\"$LOGFILE\" >/dev/null\n\techo \"Sending zip file \"${dir}/${zip_name}\"\"\n\tif [[ -s \"${dir}/$zip_name\" ]]; then\n\t\tsendToNotify \"$dir/$zip_name\"\n\t\trm -f \"${dir}/$zip_name\"\n\telse\n\t\tnotification \"No Zip file to send\" warn\n\tfi\n}\n\nfunction isAsciiText {\n\tIS_ASCII=\"False\"\n\tif [[ $(file $1 | grep -o 'ASCII text$') == \"ASCII text\" ]]; then\n\t\tIS_ASCII=\"True\"\n\telse\n\t\tIS_ASCII=\"False\"\n\tfi\n}\n\nfunction output() {\n\tmkdir -p $dir_output\n\tcp -r $dir $dir_output\n\t[[ \"$(dirname $dir)\" != \"$dir_output\" ]] && rm -rf \"$dir\"\n}\n\nfunction remove_big_files() {\n\teval rm -rf .tmp/gotator*.txt 2>>\"$LOGFILE\"\n\teval rm -rf .tmp/brute_recursive_wordlist.txt 2>>\"$LOGFILE\"\n\teval rm -rf .tmp/subs_dns_tko.txt 2>>\"$LOGFILE\"\n\teval rm -rf .tmp/subs_no_resolved.txt .tmp/subdomains_dns.txt .tmp/brute_dns_tko.txt .tmp/scrap_subs.txt .tmp/analytics_subs_clean.txt .tmp/gotator1.txt .tmp/gotator2.txt .tmp/passive_recursive.txt .tmp/brute_recursive_wordlist.txt .tmp/gotator1_recursive.txt .tmp/gotator2_recursive.txt 2>>\"$LOGFILE\"\n\teval find .tmp -type f -size +200M -exec rm -f {} + 2>>\"$LOGFILE\"\n}\n\nfunction notification() {\n\tif [[ -n $1 ]] && [[ -n $2 ]]; then\n\t\tif [[ $NOTIFICATION == true ]]; then\n\t\t\tNOTIFY=\"notify -silent\"\n\t\telse\n\t\t\tNOTIFY=\"\"\n\t\tfi\n\t\tif [[ -z $3 ]]; then\n\t\t\tcurrent_date=$(date +'%Y-%m-%d %H:%M:%S')\n\t\telse\n\t\t\tcurrent_date=\"$3\"\n\t\tfi\n\n\t\tcase $2 in\n\t\tinfo)\n\t\t\ttext=\"\\n${bblue}[$current_date] ${1} ${reset}\"\n\t\t\t;;\n\t\twarn)\n\t\t\ttext=\"\\n${yellow}[$current_date] ${1} ${reset}\"\n\t\t\t;;\n\t\terror)\n\t\t\ttext=\"\\n${bred}[$current_date] ${1} ${reset}\"\n\t\t\t;;\n\t\tgood)\n\t\t\ttext=\"\\n${bgreen}[$current_date] ${1} ${reset}\"\n\t\t\t;;\n\t\tesac\n\n\t\t# Print to terminal\n\t\tprintf \"${text}\\n\"\n\n\t\t# Send to notify if notifications are enabled\n\t\tif [[ -n $NOTIFY ]]; then\n\t\t\t# Remove color codes for the notification\n\t\t\tclean_text=$(echo -e \"${text} - ${domain}\" | sed 's/\\x1B\\[[0-9;]*[JKmsu]//g')\n\t\t\techo -e \"${clean_text}\" | $NOTIFY >/dev/null 2>&1\n\t\tfi\n\tfi\n}\n\nfunction transfer {\n\tif [[ $# -eq 0 ]]; then\n\t\techo \"No arguments specified.\\nUsage:\\n transfer <file|directory>\\n ... | transfer <file_name>\" >&2\n\t\treturn 1\n\tfi\n\tif tty -s; then\n\t\tfile=\"$1\"\n\t\tfile_name=$(basename \"$file\")\n\t\tif [[ ! -e $file ]]; then\n\t\t\techo \"$file: No such file or directory\" >&2\n\t\t\treturn 1\n\t\tfi\n\t\tif [[ -d $file ]]; then\n\t\t\tfile_name=\"$file_name.zip\"\n\t\t\t(cd \"$file\" && zip -r -q - .) | curl --progress-bar --upload-file \"-\" \"https://oshi.at/${file_name}\" | tee /dev/null\n\t\telse\n\t\t\tcat \"$file\" | curl --progress-bar --upload-file \"-\" \"https://oshi.at/${file_name}\" | tee /dev/null\n\t\tfi\n\telse\n\t\tfile_name=$1\n\t\tcurl --progress-bar --upload-file \"-\" \"https://oshi.at/${file_name}\" | tee /dev/null\n\tfi\n}\n\nfunction sendToNotify {\n\tif [[ -z $1 ]]; then\n\t\tprintf \"\\n${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] No file provided to send ${reset}\\n\"\n\telse\n\t\tif [[ -z $NOTIFY_CONFIG ]]; then\n\t\t\tNOTIFY_CONFIG=~/.config/notify/provider-config.yaml\n\t\tfi\n\t\tif [[ -n \"$(find \"${1}\" -prune -size +8000000c)\" ]]; then\n\t\t\tprintf '%s is larger than 8MB, sending over oshi.at\\n' \"${1}\"\n\t\t\ttransfer \"${1}\" | notify -silent\n\t\t\treturn 0\n\t\tfi\n\t\tif grep -q '^ telegram\\|^telegram\\|^    telegram' $NOTIFY_CONFIG; then\n\t\t\tnotification \"[$(date +'%Y-%m-%d %H:%M:%S')] Sending ${domain} data over Telegram\" info\n\t\t\ttelegram_chat_id=$(sed -n '/^telegram:/,/^[^ ]/p' ${NOTIFY_CONFIG} | sed -n 's/^[ ]*telegram_chat_id:[ ]*\"\\([^\"]*\\)\".*/\\1/p')\n\t\t\ttelegram_key=$(sed -n '/^telegram:/,/^[^ ]/p' ${NOTIFY_CONFIG} | sed -n 's/^[ ]*telegram_api_key:[ ]*\"\\([^\"]*\\)\".*/\\1/p')\n\t\t\tcurl -F \"chat_id=${telegram_chat_id}\" -F \"document=@${1}\" https://api.telegram.org/bot${telegram_key}/sendDocument 2>>\"$LOGFILE\" >/dev/null\n\t\tfi\n\t\tif grep -q '^ discord\\|^discord\\|^    discord' $NOTIFY_CONFIG; then\n\t\t\tnotification \"[$(date +'%Y-%m-%d %H:%M:%S')] Sending ${domain} data over Discord\" info\n\t\t\tdiscord_url=$(sed -n '/^discord:/,/^[^ ]/p' ${NOTIFY_CONFIG} | sed -n 's/^[ ]*discord_webhook_url:[ ]*\"\\([^\"]*\\)\".*/\\1/p')\n\t\t\tcurl -v -i -H \"Accept: application/json\" -H \"Content-Type: multipart/form-data\" -X POST -F 'payload_json={\"username\": \"test\", \"content\": \"hello\"}' -F file1=@${1} $discord_url 2>>\"$LOGFILE\" >/dev/null\n\t\tfi\n\t\tif [[ -n $slack_channel ]] && [[ -n $slack_auth ]]; then\n\t\t\tnotification \"[$(date +'%Y-%m-%d %H:%M:%S')] Sending ${domain} data over Slack\" info\n\t\t\tcurl -F file=@${1} -F \"initial_comment=reconftw zip file\" -F channels=${slack_channel} -H \"Authorization: Bearer ${slack_auth}\" https://slack.com/api/files.upload 2>>\"$LOGFILE\" >/dev/null\n\t\tfi\n\tfi\n}\n\nfunction start_func() {\n\tprintf \"${bgreen}#######################################################################\"\n\tnotification \"${2}\" info\n\techo \"[$current_date] Start function: ${1} \" >>\"${LOGFILE}\"\n\tstart=$(date +%s)\n}\n\nfunction end_func() {\n\ttouch $called_fn_dir/.${2}\n\tend=$(date +%s)\n\tgetElapsedTime $start $end\n\tnotification \"${2} Finished in ${runtime}\" info\n\techo \"[$current_date] End function: ${2} \" >>\"${LOGFILE}\"\n\tprintf \"${bblue}[$current_date] ${1} ${reset}\\n\"\n\tprintf \"${bgreen}#######################################################################${reset}\\n\"\n}\n\nfunction start_subfunc() {\n\tnotification \"     ${2}\" info\n\techo \"[$current_date] Start subfunction: ${1} \" >>\"${LOGFILE}\"\n\tstart_sub=$(date +%s)\n}\n\nfunction end_subfunc() {\n\ttouch $called_fn_dir/.${2}\n\tend_sub=$(date +%s)\n\tgetElapsedTime $start_sub $end_sub\n\tnotification \"     ${1} in ${runtime}\" good\n\techo \"[$current_date] End subfunction: ${1} \" >>\"${LOGFILE}\"\n}\n\nfunction check_inscope() {\n\tcat $1 | inscope >$1_tmp && cp $1_tmp $1 && rm -f $1_tmp\n}\n\nfunction resolvers_update() {\n\n\tif [[ $generate_resolvers == true ]]; then\n\t\tif [[ $AXIOM != true ]]; then\n\t\t\tif [[ ! -s $resolvers ]] || [[ $(find \"$resolvers\" -mtime +1 -print) ]]; then\n\t\t\t\tnotification \"Resolvers seem older than 1 day\\n Generating custom resolvers...\" warn\n\t\t\t\teval rm -f $resolvers 2>>\"$LOGFILE\"\n\t\t\t\tdnsvalidator -tL https://public-dns.info/nameservers.txt -threads $DNSVALIDATOR_THREADS -o $resolvers 2>>\"$LOGFILE\" >/dev/null\n\t\t\t\tdnsvalidator -tL https://raw.githubusercontent.com/blechschmidt/massdns/master/lists/resolvers.txt -threads $DNSVALIDATOR_THREADS -o tmp_resolvers 2>>\"$LOGFILE\" >/dev/null\n\t\t\t\t[ -s \"tmp_resolvers\" ] && cat tmp_resolvers | anew -q $resolvers\n\t\t\t\t[ -s \"tmp_resolvers\" ] && rm -f tmp_resolvers 2>>\"$LOGFILE\" >/dev/null\n\t\t\t\t[ ! -s \"$resolvers\" ] && wget -q -O - ${resolvers_url} >$resolvers\n\t\t\t\t[ ! -s \"$resolvers_trusted\" ] && wget -q -O - ${resolvers_trusted_url} >$resolvers_trusted\n\t\t\t\tnotification \"Updated\\n\" good\n\t\t\tfi\n\t\telse\n\t\t\tnotification \"Checking resolvers lists...\\n Accurate resolvers are the key to great results\\n This may take around 10 minutes if it's not updated\" warn\n\t\t\t# shellcheck disable=SC2016\n\t\t\taxiom-exec 'if [[ $(find \"/home/op/lists/resolvers.txt\" -mtime +1 -print) ]] || [[ $(cat /home/op/lists/resolvers.txt | wc -l) -le 40 ] ; then dnsvalidator -tL https://public-dns.info/nameservers.txt -threads 200 -o /home/op/lists/resolvers.txt ; fi' &>/dev/null\n\t\t\taxiom-exec \"wget -q -O - ${resolvers_url} > /home/op/lists/resolvers.txt\" 2>>\"$LOGFILE\" >/dev/null\n\t\t\taxiom-exec \"wget -q -O - ${resolvers_trusted_url} > /home/op/lists/resolvers_trusted.txt\" 2>>\"$LOGFILE\" >/dev/null\n\t\t\tnotification \"Updated\\n\" good\n\t\tfi\n\t\tgenerate_resolvers=false\n\telse\n\n\t\tif [[ ! -s $resolvers ]] || [[ $(find \"$resolvers\" -mtime +1 -print) ]]; then\n\t\t\tnotification \"Resolvers seem older than 1 day\\n Downloading new resolvers...\" warn\n\t\t\twget -q -O - ${resolvers_url} >$resolvers\n\t\t\twget -q -O - ${resolvers_trusted_url} >$resolvers_trusted\n\t\t\tnotification \"Resolvers updated\\n\" good\n\t\tfi\n\tfi\n\n}\n\nfunction resolvers_update_quick_local() {\n\tif [[ $update_resolvers == true ]]; then\n\t\twget -q -O - ${resolvers_url} >$resolvers\n\t\twget -q -O - ${resolvers_trusted_url} >$resolvers_trusted\n\tfi\n}\n\nfunction resolvers_update_quick_axiom() {\n\taxiom-exec \"wget -q -O - ${resolvers_url} > /home/op/lists/resolvers.txt\" 2>>\"$LOGFILE\" >/dev/null\n\taxiom-exec \"wget -q -O - ${resolvers_trusted_url} > /home/op/lists/resolvers_trusted.txt\" 2>>\"$LOGFILE\" >/dev/null\n}\n\nfunction ipcidr_target() {\n\tIP_CIDR_REGEX='(((25[0-5]|2[0-4][0-9]|1?[0-9][0-9]?)\\.){3}(25[0-5]|2[0-4][0-9]|1?[0-9][0-9]?))(\\/([8-9]|[1-2][0-9]|3[0-2]))([^0-9.]|$)|(((25[0-5]|2[0-4][0-9]|1?[0-9][0-9]?)\\.){3}(25[0-5]|2[0-4][0-9]|1?[0-9][0-9]?)$)'\n\tif [[ $1 =~ ^$IP_CIDR_REGEX ]]; then\n\t\techo $1 | mapcidr -silent | anew -q target_reconftw_ipcidr.txt\n\t\tif [[ -s \"./target_reconftw_ipcidr.txt\" ]]; then\n\t\t\t[ \"$REVERSE_IP\" = true ] && cat ./target_reconftw_ipcidr.txt | hakip2host | cut -d' ' -f 3 | unfurl -u domains 2>/dev/null | sed -e 's/*\\.//' -e 's/\\.$//' -e '/\\./!d' | anew -q ./target_reconftw_ipcidr.txt\n\t\t\tif [[ $(cat ./target_reconftw_ipcidr.txt | wc -l) -eq 1 ]]; then\n\t\t\t\tdomain=$(cat ./target_reconftw_ipcidr.txt)\n\t\t\telif [[ $(cat ./target_reconftw_ipcidr.txt | wc -l) -gt 1 ]]; then\n\t\t\t\tunset domain\n\t\t\t\tlist=${PWD}/target_reconftw_ipcidr.txt\n\t\t\tfi\n\t\tfi\n\t\tif [[ -n $2 ]]; then\n\t\t\tcat $list | anew -q $2\n\t\t\tsed -i '/\\/[0-9]*$/d' $2\n\t\tfi\n\tfi\n}\n\nfunction axiom_launch() {\n\t# let's fire up a FLEET!\n\tif [[ $AXIOM_FLEET_LAUNCH == true ]] && [[ -n $AXIOM_FLEET_NAME ]] && [[ -n $AXIOM_FLEET_COUNT ]]; then\n\t\tstart_func ${FUNCNAME[0]} \"Launching our Axiom fleet\"\n\n\t\t# Check to see if we have a fleet already, if so, SKIP THIS!\n\t\tNUMOFNODES=$(timeout 30 axiom-ls | grep -c \"$AXIOM_FLEET_NAME\" || true)\n\t\tif [[ $NUMOFNODES -ge $AXIOM_FLEET_COUNT ]]; then\n\t\t\taxiom-select \"$AXIOM_FLEET_NAME*\"\n\t\t\tend_func \"Axiom fleet $AXIOM_FLEET_NAME already has $NUMOFNODES instances\" info\n\t\telse\n\t\t\tif [[ $NUMOFNODES -eq 0 ]]; then\n\t\t\t\tstartcount=$AXIOM_FLEET_COUNT\n\t\t\telse\n\t\t\t\tstartcount=$((AXIOM_FLEET_COUNT - NUMOFNODES))\n\t\t\tfi\n\t\t\tAXIOM_ARGS=\" -i $startcount\"\n\t\t\t# Temporarily disabled multiple axiom regions\n\t\t\t# [ -n \"$AXIOM_FLEET_REGIONS\" ] && axiom_args=\"$axiom_args --regions=\\\"$AXIOM_FLEET_REGIONS\\\" \"\n\n\t\t\techo \"axiom-fleet ${AXIOM_FLEET_NAME} ${AXIOM_ARGS}\"\n\t\t\taxiom-fleet ${AXIOM_FLEET_NAME} ${AXIOM_ARGS}\n\t\t\taxiom-select \"$AXIOM_FLEET_NAME*\"\n\t\t\tif [[ -n $AXIOM_POST_START ]]; then\n\t\t\t\teval \"$AXIOM_POST_START\" 2>>\"$LOGFILE\" >/dev/null\n\t\t\tfi\n\n\t\t\tNUMOFNODES=$(timeout 30 axiom-ls | grep -c \"$AXIOM_FLEET_NAME\" || true)\n\t\t\tend_func \"Axiom fleet $AXIOM_FLEET_NAME launched $NUMOFNODES instances\" info\n\t\tfi\n\tfi\n}\n\nfunction axiom_shutdown() {\n\tif [[ $AXIOM_FLEET_LAUNCH == true ]] && [[ $AXIOM_FLEET_SHUTDOWN == true ]] && [[ -n $AXIOM_FLEET_NAME ]]; then\n\t\t#if [[ \"$mode\" == \"subs_menu\" ]] || [[ \"$mode\" == \"list_recon\" ]] || [[ \"$mode\" == \"passive\" ]] || [[ \"$mode\" == \"all\" ]]; then\n\t\tif [[ $mode == \"subs_menu\" ]] || [[ $mode == \"passive\" ]] || [[ $mode == \"all\" ]]; then\n\t\t\tnotification \"Automatic Axiom fleet shutdown is not enabled in this mode\" info\n\t\t\treturn\n\t\tfi\n\t\teval axiom-rm -f \"$AXIOM_FLEET_NAME*\" || true\n\t\taxiom-ls | grep \"$AXIOM_FLEET_NAME\" || true\n\t\tnotification \"Axiom fleet $AXIOM_FLEET_NAME shutdown\" info\n\tfi\n}\n\nfunction axiom_selected() {\n\n\tif [[ ! $(axiom-ls | tail -n +2 | sed '$ d' | wc -l) -gt 0 ]]; then\n\t\tnotification \"No axiom instances running ${reset}\\n\\n\" error\n\t\texit\n\tfi\n\n\tif [[ ! $(cat ~/.axiom/selected.conf | sed '/^\\s*$/d' | wc -l) -gt 0 ]]; then\n\t\tnotification \"No axiom instances selected ${reset}\\n\\n\" error\n\t\texit\n\tfi\n}\n\nfunction start() {\n\n\tglobal_start=$(date +%s)\n\n\tprintf \"\\n${bgreen}#######################################################################${reset}\"\n\tnotification \"Recon succesfully started on ${domain}\" \"good\" \"$(date +'%Y-%m-%d %H:%M:%S')\"\n\t[ \"$SOFT_NOTIFICATION\" = true ] && echo \"$(date +'%Y-%m-%d %H:%M:%S') Recon succesfully started on ${domain}\" | notify -silent\n\tprintf \"${bgreen}#######################################################################${reset}\\n\"\n\tif [[ $upgrade_before_running == true ]]; then\n\t\t${SCRIPTPATH}/install.sh --tools\n\tfi\n\ttools_installed\n\n\t#[[ -n \"$domain\" ]] && ipcidr_target $domain\n\n\tif [[ -z $domain ]]; then\n\t\tif [[ -n $list ]]; then\n\t\t\tif [[ -z $domain ]]; then\n\t\t\t\tdomain=\"Multi\"\n\t\t\t\tdir=\"${SCRIPTPATH}/Recon/$domain\"\n\t\t\t\tcalled_fn_dir=\"$dir\"/.called_fn\n\t\t\tfi\n\t\t\tif [[ $list == /* ]]; then\n\t\t\t\tinstall -D \"$list\" \"$dir\"/webs/webs.txt\n\t\t\telse\n\t\t\t\tinstall -D \"${SCRIPTPATH}\"/\"$list\" \"$dir\"/webs/webs.txt\n\t\t\tfi\n\t\tfi\n\telse\n\t\tdir=\"${SCRIPTPATH}/Recon/$domain\"\n\t\tcalled_fn_dir=\"$dir\"/.called_fn\n\tfi\n\n\tif [[ -z $domain ]]; then\n\t\tnotification \"${bred} No domain or list provided ${reset}\\n\\n\" error\n\t\texit\n\tfi\n\n\tif [[ ! -d $called_fn_dir ]]; then\n\t\tmkdir -p \"$called_fn_dir\"\n\tfi\n\tmkdir -p \"$dir\"\n\tcd \"$dir\" || {\n\t\techo \"Failed to cd directory in ${FUNCNAME[0]} @ line ${LINENO}\"\n\t\texit 1\n\t}\n\tif [[ $AXIOM == true ]]; then\n\t\tif [[ -n $domain ]]; then\n\t\t\techo \"$domain\" | anew -q target.txt\n\t\t\tlist=\"${dir}/target.txt\"\n\t\tfi\n\tfi\n\tmkdir -p {.log,.tmp,webs,hosts,vulns,osint,screenshots,subdomains}\n\n\tNOW=$(date +\"%F\")\n\tNOWT=$(date +\"%T\")\n\tLOGFILE=\"${dir}/.log/${NOW}_${NOWT}.txt\"\n\ttouch .log/${NOW}_${NOWT}.txt\n\techo \"[$(date +'%Y-%m-%d %H:%M:%S')] Start ${NOW} ${NOWT}\" >\"${LOGFILE}\"\n\n\tprintf \"\\n\"\n\tprintf \"${bred}[$(date +'%Y-%m-%d %H:%M:%S')] Target: ${domain}\\n\\n\"\n}\n\nfunction end() {\n\n\tfind $dir -type f -empty -print | grep -v '.called_fn' | grep -v '.log' | grep -v '.tmp' | xargs rm -f 2>>\"$LOGFILE\" >/dev/null\n\tfind $dir -type d -empty -print -delete 2>>\"$LOGFILE\" >/dev/null\n\n\techo \"[$(date +'%Y-%m-%d %H:%M:%S')] End\" >>\"${LOGFILE}\"\n\n\tif [[ $PRESERVE != true ]]; then\n\t\tfind $dir -type f -empty | grep -v \"called_fn\" | xargs rm -f 2>>\"$LOGFILE\" >/dev/null\n\t\tfind $dir -type d -empty | grep -v \"called_fn\" | xargs rm -rf 2>>\"$LOGFILE\" >/dev/null\n\tfi\n\n\tif [[ $REMOVETMP == true ]]; then\n\t\trm -rf $dir/.tmp\n\tfi\n\n\tif [[ $REMOVELOG == true ]]; then\n\t\trm -rf $dir/.log\n\tfi\n\n\tif [[ -n $dir_output ]]; then\n\t\toutput\n\t\tfinaldir=$dir_output\n\telse\n\t\tfinaldir=$dir\n\tfi\n\t#Zip the output folder and send it via tg/discord/slack\n\tif [[ $SENDZIPNOTIFY == true ]]; then\n\t\tzipSnedOutputFolder\n\tfi\n\tglobal_end=$(date +%s)\n\tgetElapsedTime $global_start $global_end\n\tprintf \"${bgreen}#######################################################################${reset}\\n\"\n\tnotification \"Finished Recon on: ${domain} under ${finaldir} in: ${runtime}\" good \"$(date +'%Y-%m-%d %H:%M:%S')\"\n\t[ \"$SOFT_NOTIFICATION\" = true ] && echo \"[$(date +'%Y-%m-%d %H:%M:%S')] Finished Recon on: ${domain} under ${finaldir} in: ${runtime}\" | notify -silent\n\tprintf \"${bgreen}#######################################################################${reset}\\n\"\n\t#Separator for more clear messges in telegram_Bot\n\tnotification echo \"[$(date +'%Y-%m-%d %H:%M:%S')] ******  Stay safe ü¶† and secure üîê  ******\" info\n\n}\n\n###############################################################################################################\n########################################### MODES & MENUS #####################################################\n###############################################################################################################\n\nfunction passive() {\n\tstart\n\tdomain_info\n\tip_info\n\temails\n\tgoogle_dorks\n\t#github_dorks\n\tgithub_repos\n\tmetadata\n\tapileaks\n\tthird_party_misconfigs\n\tSUBNOERROR=false\n\tSUBANALYTICS=false\n\tSUBBRUTE=false\n\tSUBSCRAPING=false\n\tSUBPERMUTE=false\n\tSUBREGEXPERMUTE=false\n\tSUB_RECURSIVE_BRUTE=false\n\tWEBPROBESIMPLE=false\n\tif [[ $AXIOM == true ]]; then\n\t\taxiom_launch\n\t\taxiom_selected\n\tfi\n\n\tsubdomains_full\n\tremove_big_files\n\tfavicon\n\tcdnprovider\n\tPORTSCAN_ACTIVE=false\n\tportscan\n\tgeo_info\n\n\tif [[ $AXIOM == true ]]; then\n\t\taxiom_shutdown\n\tfi\n\n\tend\n}\n\nfunction all() {\n\tstart\n\trecon\n\tvulns\n\tend\n}\n\nfunction osint() {\n\tdomain_info\n\tip_info\n\temails\n\tgoogle_dorks\n\t#github_dorks\n\tgithub_repos\n\tmetadata\n\tapileaks\n\tthird_party_misconfigs\n\tzonetransfer\n\tfavicon\n}\n\nfunction vulns() {\n\tif [[ $VULNS_GENERAL == true ]]; then\n\t\tcors\n\t\topen_redirect\n\t\tssrf_checks\n\t\tcrlf_checks\n\t\tlfi\n\t\tssti\n\t\tsqli\n\t\txss\n\t\tcommand_injection\n\t\tprototype_pollution\n\t\tsmuggling\n\t\twebcache\n\t\tspraying\n\t\tbrokenLinks\n\t\tfuzzparams\n\t\t4xxbypass\n\t\ttest_ssl\n\tfi\n}\n\nfunction multi_osint() {\n\n\tglobal_start=$(date +%s)\n\n\t#[[ -n \"$domain\" ]] && ipcidr_target $domain\n\n\tif [[ -s $list ]]; then\n\t\tsed -i 's/\\r$//' $list\n\t\ttargets=$(cat $list)\n\telse\n\t\tnotification \"Target list not provided\" error\n\t\texit\n\tfi\n\n\tworkdir=${SCRIPTPATH}/Recon/$multi\n\tmkdir -p $workdir || {\n\t\techo \"Failed to create directory '$workdir' in ${FUNCNAME[0]} @ line ${LINENO}\"\n\t\texit 1\n\t}\n\tcd \"$workdir\" || {\n\t\techo \"Failed to cd directory '$workdir' in ${FUNCNAME[0]} @ line ${LINENO}\"\n\t\texit 1\n\t}\n\tmkdir -p {.log,.tmp,webs,hosts,vulns,osint,screenshots,subdomains}\n\n\tNOW=$(date +\"%F\")\n\tNOWT=$(date +\"%T\")\n\tLOGFILE=\"${workdir}/.log/${NOW}_${NOWT}.txt\"\n\ttouch .log/${NOW}_${NOWT}.txt\n\techo \"[$(date +'%Y-%m-%d %H:%M:%S')] Start ${NOW} ${NOWT}\" >\"${LOGFILE}\"\n\n\tfor domain in $targets; do\n\t\tdir=$workdir/targets/$domain\n\t\tcalled_fn_dir=$dir/.called_fn\n\t\tmkdir -p $dir\n\t\tcd \"$dir\" || {\n\t\t\techo \"Failed to cd directory '$dir' in ${FUNCNAME[0]} @ line ${LINENO}\"\n\t\t\texit 1\n\t\t}\n\t\tmkdir -p {.log,.tmp,webs,hosts,vulns,osint,screenshots,subdomains}\n\t\tNOW=$(date +\"%F\")\n\t\tNOWT=$(date +\"%T\")\n\t\tLOGFILE=\"${dir}/.log/${NOW}_${NOWT}.txt\"\n\t\ttouch .log/${NOW}_${NOWT}.txt\n\t\techo \"[$(date +'%Y-%m-%d %H:%M:%S')] Start ${NOW} ${NOWT}\" >\"${LOGFILE}\"\n\t\tdomain_info\n\t\tip_info\n\t\temails\n\t\tgoogle_dorks\n\t\t#github_dorks\n\t\tgithub_repos\n\t\tmetadata\n\t\tapileaks\n\t\tthird_party_misconfigs\n\t\tzonetransfer\n\t\tfavicon\n\tdone\n\tcd \"$workdir\" || {\n\t\techo \"Failed to cd directory '$workdir' in ${FUNCNAME[0]} @ line ${LINENO}\"\n\t\texit 1\n\t}\n\tdir=$workdir\n\tdomain=$multi\n\tend\n}\n\nfunction recon() {\n\tdomain_info\n\tip_info\n\temails\n\tgoogle_dorks\n\t#github_dorks\n\tgithub_repos\n\tmetadata\n\tapileaks\n\tthird_party_misconfigs\n\tzonetransfer\n\tfavicon\n\n\tif [[ $AXIOM == true ]]; then\n\t\taxiom_launch\n\t\taxiom_selected\n\tfi\n\n\tsubdomains_full\n\twebprobe_full\n\tsubtakeover\n\tremove_big_files\n\ts3buckets\n\tscreenshot\n\t#\tvirtualhosts\n\tcdnprovider\n\tportscan\n\tgeo_info\n\twaf_checks\n\tfuzz\n\tiishortname\n\turlchecks\n\tjschecks\n\tnuclei_check\n\n\tif [[ $AXIOM == true ]]; then\n\t\taxiom_shutdown\n\tfi\n\n\tcms_scanner\n\turl_gf\n\twordlist_gen\n\twordlist_gen_roboxtractor\n\tpassword_dict\n\turl_ext\n}\n\nfunction multi_recon() {\n\n\t[ \"$SOFT_NOTIFICATION\" = true ] && echo \"$(date +'%Y-%m-%d %H:%M:%S') Recon successfully started on ${multi}\" | notify -silent\n\n\tglobal_start=$(date +%s)\n\n\t#[[ -n \"$domain\" ]] && ipcidr_target $domain\n\n\tif [[ -s $list ]]; then\n\t\tsed -i 's/\\r$//' $list\n\t\ttargets=$(cat $list)\n\telse\n\t\tnotification \"Target list not provided\" error\n\t\texit\n\tfi\n\n\tworkdir=${SCRIPTPATH}/Recon/$multi\n\tmkdir -p $workdir || {\n\t\techo \"Failed to create directory '$workdir' in ${FUNCNAME[0]} @ line ${LINENO}\"\n\t\texit 1\n\t}\n\tcd \"$workdir\" || {\n\t\techo \"Failed to cd directory '$workdir' in ${FUNCNAME[0]} @ line ${LINENO}\"\n\t\texit 1\n\t}\n\n\tmkdir -p {.log,.tmp,webs,hosts,vulns,osint,screenshots,subdomains}\n\tNOW=$(date +\"%F\")\n\tNOWT=$(date +\"%T\")\n\tLOGFILE=\"${workdir}/.log/${NOW}_${NOWT}.txt\"\n\ttouch .log/${NOW}_${NOWT}.txt\n\techo \"[$(date +'%Y-%m-%d %H:%M:%S')] Start ${NOW} ${NOWT}\" >\"${LOGFILE}\"\n\n\t[ -n \"$flist\" ] && LISTTOTAL=$(cat \"$flist\" | wc -l)\n\n\tfor domain in $targets; do\n\t\tdir=$workdir/targets/$domain\n\t\tcalled_fn_dir=$dir/.called_fn\n\n\t\t# Ensure directories exist\n\t\tmkdir -p \"$dir\" || {\n\t\t\techo \"Failed to create directory '$dir' in ${FUNCNAME[0]} @ line ${LINENO}\"\n\t\t\texit 1\n\t\t}\n\t\tmkdir -p \"$called_fn_dir\" || {\n\t\t\techo \"Failed to create directory '$called_fn_dir' in ${FUNCNAME[0]} @ line ${LINENO}\"\n\t\t\texit 1\n\t\t}\n\n\t\tcd \"$dir\" || {\n\t\t\techo \"Failed to cd to directory '$dir' in ${FUNCNAME[0]} @ line ${LINENO}\"\n\t\t\texit 1\n\t\t}\n\n\t\tmkdir -p {.log,.tmp,webs,hosts,vulns,osint,screenshots,subdomains}\n\n\t\tNOW=$(date +\"%F\")\n\t\tNOWT=$(date +\"%T\")\n\t\tLOGFILE=\"${dir}/.log/${NOW}_${NOWT}.txt\"\n\n\t\t# Ensure the .log directory exists before touching the file\n\t\tmkdir -p .log\n\n\t\ttouch \"$LOGFILE\" || {\n\t\t\techo \"Failed to create log file: $LOGFILE\"\n\t\t\texit 1\n\t\t}\n\t\techo \"[$(date +'%Y-%m-%d %H:%M:%S')] Start ${NOW} ${NOWT}\" >\"$LOGFILE\"\n\t\tloopstart=$(date +%s)\n\n\t\tdomain_info\n\t\tip_info\n\t\temails\n\t\tgoogle_dorks\n\t\t#github_dorks\n\t\tgithub_repos\n\t\tmetadata\n\t\tapileaks\n\t\tthird_party_misconfigs\n\t\tzonetransfer\n\t\tfavicon\n\t\tcurrently=$(date +\"%H:%M:%S\")\n\t\tloopend=$(date +%s)\n\t\tgetElapsedTime $loopstart $loopend\n\t\tprintf \"${bgreen}#######################################################################${reset}\\n\"\n\t\tprintf \"${bgreen}[$(date +'%Y-%m-%d %H:%M:%S')] $domain finished 1st loop in ${runtime} $currently ${reset}\\n\"\n\t\tif [[ -n $flist ]]; then\n\t\t\tPOSINLIST=$(eval grep -nrE \"^$domain$\" \"$flist\" | cut -f1 -d':')\n\t\t\tprintf \"\\n${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] $domain is $POSINLIST of $LISTTOTAL${reset}\\n\"\n\t\tfi\n\t\tprintf \"${bgreen}#######################################################################${reset}\\n\"\n\tdone\n\tcd \"$workdir\" || {\n\t\techo \"Failed to cd directory '$workdir' in ${FUNCNAME[0]} @ line ${LINENO}\"\n\t\texit 1\n\t}\n\n\tif [[ $AXIOM == true ]]; then\n\t\taxiom_launch\n\t\taxiom_selected\n\tfi\n\n\tfor domain in $targets; do\n\t\tloopstart=$(date +%s)\n\t\tdir=$workdir/targets/$domain\n\t\tcalled_fn_dir=$dir/.called_fn\n\t\tcd \"$dir\" || {\n\t\t\techo \"Failed to cd directory '$dir' in ${FUNCNAME[0]} @ line ${LINENO}\"\n\t\t\texit 1\n\t\t}\n\t\tsubdomains_full\n\t\twebprobe_full\n\t\tsubtakeover\n\t\tremove_big_files\n\t\tscreenshot\n\t\t#\t\tvirtualhosts\n\t\tcdnprovider\n\t\tportscan\n\t\tgeo_info\n\t\tcurrently=$(date +\"%H:%M:%S\")\n\t\tloopend=$(date +%s)\n\t\tgetElapsedTime $loopstart $loopend\n\t\tprintf \"${bgreen}#######################################################################${reset}\\n\"\n\t\tprintf \"${bgreen}[$(date +'%Y-%m-%d %H:%M:%S')] $domain finished 2nd loop in ${runtime} $currently ${reset}\\n\"\n\t\tif [[ -n $flist ]]; then\n\t\t\tPOSINLIST=$(eval grep -nrE \"^$domain$\" \"$flist\" | cut -f1 -d':')\n\t\t\tprintf \"\\n${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] $domain is $POSINLIST of $LISTTOTAL${reset}\\n\"\n\t\tfi\n\t\tprintf \"${bgreen}#######################################################################${reset}\\n\"\n\tdone\n\tcd \"$workdir\" || {\n\t\techo \"Failed to cd directory '$workdir' in ${FUNCNAME[0]} @ line ${LINENO}\"\n\t\texit 1\n\t}\n\n\tnotification \"############################# Total data ############################\" info\n\tNUMOFLINES_users_total=$(find . -type f -name 'users.txt' -exec cat {} + | anew osint/users.txt | sed '/^$/d' | wc -l)\n\tNUMOFLINES_pwndb_total=$(find . -type f -name 'passwords.txt' -exec cat {} + | anew osint/passwords.txt | sed '/^$/d' | wc -l)\n\tNUMOFLINES_software_total=$(find . -type f -name 'software.txt' -exec cat {} + | anew osint/software.txt | sed '/^$/d' | wc -l)\n\tNUMOFLINES_authors_total=$(find . -type f -name 'authors.txt' -exec cat {} + | anew osint/authors.txt | sed '/^$/d' | wc -l)\n\tNUMOFLINES_subs_total=$(find . -type f -name 'subdomains.txt' -exec cat {} + | anew subdomains/subdomains.txt | sed '/^$/d' | wc -l)\n\tNUMOFLINES_subtko_total=$(find . -type f -name 'takeover.txt' -exec cat {} + | anew webs/takeover.txt | sed '/^$/d' | wc -l)\n\tNUMOFLINES_webs_total=$(find . -type f -name 'webs.txt' -exec cat {} + | anew webs/webs.txt | sed '/^$/d' | wc -l)\n\tNUMOFLINES_webs_total=$(find . -type f -name 'webs_uncommon_ports.txt' -exec cat {} + | anew webs/webs_uncommon_ports.txt | sed '/^$/d' | wc -l)\n\tNUMOFLINES_ips_total=$(find . -type f -name 'ips.txt' -exec cat {} + | anew hosts/ips.txt | sed '/^$/d' | wc -l)\n\tNUMOFLINES_cloudsprov_total=$(find . -type f -name 'cdn_providers.txt' -exec cat {} + | anew hosts/cdn_providers.txt | sed '/^$/d' | wc -l)\n\tfind . -type f -name 'portscan_active.txt' -exec cat {} + | tee -a hosts/portscan_active.txt >>\"$LOGFILE\" 2>&1 >/dev/null\n\tfind . -type f -name 'portscan_active.gnmap' -exec cat {} + | tee hosts/portscan_active.gnmap 2>>\"$LOGFILE\" >/dev/null\n\tfind . -type f -name 'portscan_passive.txt' -exec cat {} + | tee hosts/portscan_passive.txt 2>&1 >>\"$LOGFILE\" >/dev/null\n\n\tnotification \"- ${NUMOFLINES_users_total} total users found\" good\n\tnotification \"- ${NUMOFLINES_pwndb_total} total creds leaked\" good\n\tnotification \"- ${NUMOFLINES_software_total} total software found\" good\n\tnotification \"- ${NUMOFLINES_authors_total} total authors found\" good\n\tnotification \"- ${NUMOFLINES_subs_total} total subdomains\" good\n\tnotification \"- ${NUMOFLINES_subtko_total} total probably subdomain takeovers\" good\n\tnotification \"- ${NUMOFLINES_webs_total} total websites\" good\n\tnotification \"- ${NUMOFLINES_ips_total} total ips\" good\n\tnotification \"- ${NUMOFLINES_cloudsprov_total} total IPs belongs to cloud\" good\n\ts3buckets\n\twaf_checks\n\tfor domain in $targets; do\n\t\tloopstart=$(date +%s)\n\t\tdir=$workdir/targets/$domain\n\t\tcalled_fn_dir=$dir/.called_fn\n\t\tcd \"$dir\" || {\n\t\t\techo \"Failed to cd directory '$dir' in ${FUNCNAME[0]} @ line ${LINENO}\"\n\t\t\texit 1\n\t\t}\n\t\tloopstart=$(date +%s)\n\t\tfuzz\n\t\tiishortname\n\t\turlchecks\n\t\tjschecks\n\t\tcurrently=$(date +\"%H:%M:%S\")\n\t\tloopend=$(date +%s)\n\t\tgetElapsedTime $loopstart $loopend\n\t\tprintf \"${bgreen}#######################################################################${reset}\\n\"\n\t\tprintf \"${bgreen}[$(date +'%Y-%m-%d %H:%M:%S')] $domain finished 3rd loop in ${runtime} $currently ${reset}\\n\"\n\t\tif [[ -n $flist ]]; then\n\t\t\tPOSINLIST=$(eval grep -nrE \"^$domain$\" \"$flist\" | cut -f1 -d':')\n\t\t\tprintf \"\\n${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] $domain is $POSINLIST of $LISTTOTAL${reset}\\n\"\n\t\tfi\n\t\tprintf \"${bgreen}#######################################################################${reset}\\n\"\n\tdone\n\tnuclei_check\n\tif [[ $AXIOM == true ]]; then\n\t\taxiom_shutdown\n\tfi\n\n\tfor domain in $targets; do\n\t\tloopstart=$(date +%s)\n\t\tdir=$workdir/targets/$domain\n\t\tcalled_fn_dir=$dir/.called_fn\n\t\tcd \"$dir\" || {\n\t\t\techo \"Failed to cd directory '$dir' in ${FUNCNAME[0]} @ line ${LINENO}\"\n\t\t\texit 1\n\t\t}\n\t\tcms_scanner\n\t\turl_gf\n\t\twordlist_gen\n\t\twordlist_gen_roboxtractor\n\t\tpassword_dict\n\t\turl_ext\n\t\tcurrently=$(date +\"%H:%M:%S\")\n\t\tloopend=$(date +%s)\n\t\tgetElapsedTime $loopstart $loopend\n\t\tprintf \"${bgreen}#######################################################################${reset}\\n\"\n\t\tprintf \"${bgreen}[$(date +'%Y-%m-%d %H:%M:%S')] $domain finished final loop in ${runtime} $currently ${reset}\\n\"\n\t\tif [[ -n $flist ]]; then\n\t\t\tPOSINLIST=$(eval grep -nrE \"^$domain$\" \"$flist\" | cut -f1 -d':')\n\t\t\tprintf \"\\n${yellow}[$(date +'%Y-%m-%d %H:%M:%S')] $domain is $POSINLIST of $LISTTOTAL${reset}\\n\"\n\t\tfi\n\t\tprintf \"${bgreen}#######################################################################${reset}\\n\"\n\tdone\n\tcd \"$workdir\" || {\n\t\techo \"Failed to cd directory '$workdir' in ${FUNCNAME[0]} @ line ${LINENO}\"\n\t\texit 1\n\t}\n\tdir=$workdir\n\tdomain=$multi\n\tend\n\t[ \"$SOFT_NOTIFICATION\" = true ] && echo \"$(date +'%Y-%m-%d %H:%M:%S') Finished Recon on: ${multi} in ${runtime}\" | notify -silent\n}\n\nfunction multi_custom() {\n\n\tglobal_start=$(date +%s)\n\n\tif [[ -s $list ]]; then\n\t\tsed -i 's/\\r$//' $list\n\t\ttargets=$(cat $list)\n\telse\n\t\tnotification \"Target list not provided\" error\n\t\texit\n\tfi\n\n\tdir=${SCRIPTPATH}/Recon/$multi\n\trm -rf $dir\n\tmkdir -p $dir || {\n\t\techo \"Failed to create directory '$dir' in ${FUNCNAME[0]} @ line ${LINENO}\"\n\t\texit 1\n\t}\n\tcd \"$dir\" || {\n\t\techo \"Failed to cd directory '$dir' in ${FUNCNAME[0]} @ line ${LINENO}\"\n\t\texit 1\n\t}\n\n\tmkdir -p {.called_fn,.log}\n\tcalled_fn_dir=$dir/.called_fn\n\tNOW=$(date +\"%F\")\n\tNOWT=$(date +\"%T\")\n\tLOGFILE=\"${dir}/.log/${NOW}_${NOWT}.txt\"\n\ttouch .log/${NOW}_${NOWT}.txt\n\techo \"[$(date +'%Y-%m-%d %H:%M:%S')] Start ${NOW} ${NOWT}\" >\"${LOGFILE}\"\n\n\t[ -n \"$flist\" ] && entries=$(cat \"$flist\" | wc -l)\n\n\tif [[ $AXIOM == true ]]; then\n\t\taxiom_launch\n\t\taxiom_selected\n\tfi\n\n\tcustom_function_list=$(echo $custom_function | tr ',' '\\n')\n\tfunc_total=$(echo \"$custom_function_list\" | wc -l)\n\n\tfunc_count=0\n\tdomain=$(cat $flist)\n\tfor custom_f in $custom_function_list; do\n\t\t((func_count = func_count + 1))\n\n\t\tloopstart=$(date +%s)\n\n\t\t$custom_f\n\n\t\tcurrently=$(date +\"%H:%M:%S\")\n\t\tloopend=$(date +%s)\n\t\tgetElapsedTime $loopstart $loopend\n\t\tprintf \"${bgreen}#######################################################################${reset}\\n\"\n\t\tprintf \"${bgreen}[$(date +'%Y-%m-%d %H:%M:%S')] Finished $custom_f ($func_count/$func_total) for $entries entries in ${runtime} $currently ${reset}\\n\"\n\t\tprintf \"${bgreen}#######################################################################${reset}\\n\"\n\tdone\n\n\tif [[ $AXIOM == true ]]; then\n\t\taxiom_shutdown\n\tfi\n\n\tend\n}\n\nfunction subs_menu() {\n\tstart\n\n\tif [[ $AXIOM == true ]]; then\n\t\taxiom_launch\n\t\taxiom_selected\n\tfi\n\n\tsubdomains_full\n\twebprobe_full\n\tsubtakeover\n\tremove_big_files\n\tscreenshot\n\t#\tvirtualhosts\n\tzonetransfer\n\ts3buckets\n\n\tif [[ $AXIOM == true ]]; then\n\t\taxiom_shutdown\n\tfi\n\n\tend\n}\n\nfunction webs_menu() {\n\tsubtakeover\n\tremove_big_files\n\tscreenshot\n\t#\tvirtualhosts\n\twaf_checks\n\tfuzz\n\tcms_scanner\n\tiishortname\n\turlchecks\n\tjschecks\n\turl_gf\n\tnuclei_check\n\twordlist_gen\n\twordlist_gen_roboxtractor\n\tpassword_dict\n\turl_ext\n\tvulns\n\tend\n}\n\nfunction zen_menu() {\n\tstart\n\tif [[ $AXIOM == true ]]; then\n\t\taxiom_launch\n\t\taxiom_selected\n\tfi\n\tsubdomains_full\n\twebprobe_full\n\tsubtakeover\n\tremove_big_files\n\ts3buckets\n\tscreenshot\n\t#\tvirtualhosts\n\tcdnprovider\n\twaf_checks\n\tfuzz\n\tiishortname\n\tnuclei_check\n\n\tif [[ $AXIOM == true ]]; then\n\t\taxiom_shutdown\n\tfi\n\tcms_scanner\n\tend\n}\n\nfunction help() {\n\tprintf \"\\n Usage: $0 [-d domain.tld] [-m name] [-l list.txt] [-x oos.txt] [-i in.txt] \"\n\tprintf \"\\n           \t      [-r] [-s] [-p] [-a] [-w] [-n] [-i] [-h] [-f] [--deep] [-o OUTPUT]\\n\\n\"\n\tprintf \" ${bblue}TARGET OPTIONS${reset}\\n\"\n\tprintf \"   -d domain.tld     Target domain\\n\"\n\tprintf \"   -m company        Target company name\\n\"\n\tprintf \"   -l list.txt       Targets list (One on each line)\\n\"\n\tprintf \"   -x oos.txt        Excludes subdomains list (Out Of Scope)\\n\"\n\tprintf \"   -i in.txt         Includes subdomains list\\n\"\n\tprintf \" \\n\"\n\tprintf \" ${bblue}MODE OPTIONS${reset}\\n\"\n\tprintf \"   -r, --recon       Recon - Performs full recon process (without attacks)\\n\"\n\tprintf \"   -s, --subdomains  Subdomains - Performs Subdomain Enumeration, Web probing and check for sub-tko\\n\"\n\tprintf \"   -p, --passive     Passive - Performs only passive steps\\n\"\n\tprintf \"   -a, --all         All - Performs all checks and active exploitations\\n\"\n\tprintf \"   -w, --web         Web - Performs web checks from list of subdomains\\n\"\n\tprintf \"   -n, --osint       OSINT - Checks for public intel data\\n\"\n\tprintf \"   -z, --zen         Zen - Performs a recon process covering the basics and some vulns \\n\"\n\tprintf \"   -c, --custom      Custom - Launches specific function against target, u need to know the function name first\\n\"\n\tprintf \"   -h                Help - Show help section\\n\"\n\tprintf \" \\n\"\n\tprintf \" ${bblue}GENERAL OPTIONS${reset}\\n\"\n\tprintf \"   --deep            Deep scan (Enable some slow options for deeper scan)\\n\"\n\tprintf \"   -f config_file    Alternate reconftw.cfg file\\n\"\n\tprintf \"   -o output/path    Define output folder\\n\"\n\tprintf \"   -v, --vps         Axiom distributed VPS \\n\"\n\tprintf \"   -q                Rate limit in requests per second \\n\"\n\tprintf \" \\n\"\n\tprintf \" ${bblue}USAGE EXAMPLES${reset}\\n\"\n\tprintf \" ${byellow}Perform full recon (without attacks):${reset}\\n\"\n\tprintf \" ./reconftw.sh -d example.com -r\\n\"\n\tprintf \" \\n\"\n\tprintf \" ${byellow}Perform subdomain enumeration on multiple targets:${reset}\\n\"\n\tprintf \" ./reconftw.sh -l targets.txt -s\\n\"\n\tprintf \" \\n\"\n\tprintf \" ${byellow}Perform Web based scanning on a subdomains list:${reset}\\n\"\n\tprintf \" ./reconftw.sh -d example.com -l targets.txt -w\\n\"\n\tprintf \" \\n\"\n\tprintf \" ${byellow}Multidomain recon:${reset}\\n\"\n\tprintf \" ./reconftw.sh -m company -l domainlist.txt -r\\n\"\n\tprintf \" \\n\"\n\tprintf \" ${byellow}Perform full recon (with active attacks) along Out-Of-Scope subdomains list:${reset}\\n\"\n\tprintf \" ./reconftw.sh -d example.com -x out.txt -a\\n\"\n\tprintf \" \\n\"\n\tprintf \" ${byellow}Perform full recon and store output to specified directory:${reset}\\n\"\n\tprintf \" ./reconftw.sh -d example.com -r -o custom/path\\n\"\n\tprintf \" \\n\"\n\tprintf \" ${byellow}Run custom function:${reset}\\n\"\n\tprintf \" ./reconftw.sh -d example.com -c nuclei_check \\n\"\n}\n\n###############################################################################################################\n########################################### START SCRIPT  #####################################################\n###############################################################################################################\n\n# macOS PATH initialization, thanks @0xtavian <3\nif [[ $OSTYPE == \"darwin\"* ]]; then\n\tPATH=\"/usr/local/opt/gnu-getopt/bin:$PATH\"\n\tPATH=\"/usr/local/opt/coreutils/libexec/gnubin:$PATH\"\nfi\n\nPROGARGS=$(getopt -o 'd:m:l:x:i:o:f:q:c:z:rspanwvh::' --long 'domain:,list:,recon,subdomains,passive,all,web,osint,zen,deep,help,vps' -n 'reconFTW' -- \"$@\")\n\n# Note the quotes around \"$PROGARGS\": they are essential!\neval set -- \"$PROGARGS\"\nunset PROGARGS\n\nwhile true; do\n\tcase \"$1\" in\n\t'-d' | '--domain')\n\t\tdomain=$2\n\t\tipcidr_target $2\n\t\tshift 2\n\t\tcontinue\n\t\t;;\n\t'-m')\n\t\tmulti=$2\n\t\tshift 2\n\t\tcontinue\n\t\t;;\n\t'-l' | '--list')\n\t\tlist=$2\n\t\tfor t in $(cat $list); do\n\t\t\tipcidr_target $t $list\n\t\tdone\n\t\tshift 2\n\t\tcontinue\n\t\t;;\n\t'-x')\n\t\toutOfScope_file=$2\n\t\tshift 2\n\t\tcontinue\n\t\t;;\n\t'-i')\n\t\tinScope_file=$2\n\t\tshift 2\n\t\tcontinue\n\t\t;;\n\t# modes\n\t'-r' | '--recon')\n\t\topt_mode='r'\n\t\tshift\n\t\tcontinue\n\t\t;;\n\t'-s' | '--subdomains')\n\t\topt_mode='s'\n\t\tshift\n\t\tcontinue\n\t\t;;\n\t'-p' | '--passive')\n\t\topt_mode='p'\n\t\tshift\n\t\tcontinue\n\t\t;;\n\t'-a' | '--all')\n\t\topt_mode='a'\n\t\tshift\n\t\tcontinue\n\t\t;;\n\t'-w' | '--web')\n\t\topt_mode='w'\n\t\tshift\n\t\tcontinue\n\t\t;;\n\t'-n' | '--osint')\n\t\topt_mode='n'\n\t\tshift\n\t\tcontinue\n\t\t;;\n\t'-c' | '--custom')\n\t\tcustom_function=$2\n\t\topt_mode='c'\n\t\tshift 2\n\t\tcontinue\n\t\t;;\n\t'-z' | '--zen')\n\t\topt_mode='z'\n\t\tshift\n\t\tcontinue\n\t\t;;\n\t# extra stuff\n\t'-o')\n\t\tif [[ $2 != /* ]]; then\n\t\t\tdir_output=$PWD/$2\n\t\telse\n\t\t\tdir_output=$2\n\t\tfi\n\t\tshift 2\n\t\tcontinue\n\t\t;;\n\t'-v' | '--vps')\n\t\tcommand -v axiom-ls &>/dev/null || {\n\t\t\tprintf \"\\n Axiom is needed for this mode and is not installed \\n You have to install it manually \\n\" && exit\n\t\t}\n\t\tAXIOM=true\n\t\tshift\n\t\tcontinue\n\t\t;;\n\t'-f')\n\t\tCUSTOM_CONFIG=$2\n\t\tshift 2\n\t\tcontinue\n\t\t;;\n\t'-q')\n\t\trate_limit=$2\n\t\tshift 2\n\t\tcontinue\n\t\t;;\n\t'--deep')\n\t\topt_deep=true\n\t\tshift\n\t\tcontinue\n\t\t;;\n\n\t'--')\n\t\tshift\n\t\tbreak\n\t\t;;\n\t'--help' | '-h' | *)\n\t\t# echo \"Unknown argument: $1\"\n\t\t. ./reconftw.cfg\n\t\tbanner\n\t\thelp\n\t\ttools_installed\n\t\texit 1\n\t\t;;\n\tesac\ndone\n\n# Initialize some variables\n#opt_deep=\"${opt_deep:=false}\"\n#rate_limit=\"${rate_limit:=0}\"\n#outOfScope_file=\"${outOfScope_file:=}\"\n#inScope_file=\"${inScope_file:=}\"\n#domain=\"${domain:=}\"\n#multi=\"${multi:=}\"\n#list=\"${list:=}\"\n#opt_mode=\"${opt_mode:=}\"\n#custom_function=\"${custom_function:=}\"\n#AXIOM=\"${AXIOM:=false}\"\n#AXIOM_POST_START=\"${AXIOM_POST_START:=}\"\n#CUSTOM_CONFIG=\"${CUSTOM_CONFIG:=}\"\n\n# This is the first thing to do to read in alternate config\nSCRIPTPATH=\"$(\n\tcd \"$(dirname \"$0\")\" >/dev/null 2>&1 || exit\n\tpwd -P\n)\"\n. \"${SCRIPTPATH}\"/reconftw.cfg || {\n\techo \"Error importing reconftw.ctg\"\n\texit 1\n}\n\nif [[ -s $CUSTOM_CONFIG ]]; then\n\t# shellcheck source=/home/six2dez/Tools/reconftw/custom_config.cfg\n\t. \"${CUSTOM_CONFIG}\" || {\n\t\techo \"Error importing reconftw.ctg\"\n\t\texit 1\n\t}\nfi\n\nif [[ $opt_deep ]]; then\n\tDEEP=true\nfi\n\nif [[ $rate_limit ]]; then\n\tNUCLEI_RATELIMIT=$rate_limit\n\tFFUF_RATELIMIT=$rate_limit\n\tHTTPX_RATELIMIT=$rate_limit\nfi\n\nif [[ -n $outOfScope_file ]]; then\n\tisAsciiText $outOfScope_file\n\tif [[ \"False\" == \"$IS_ASCII\" ]]; then\n\t\tprintf \"\\n\\n${bred} Out of Scope file is not a text file${reset}\\n\\n\"\n\t\texit\n\tfi\nfi\n\nif [[ -n $inScope_file ]]; then\n\tisAsciiText $inScope_file\n\tif [[ \"False\" == \"$IS_ASCII\" ]]; then\n\t\tprintf \"\\n\\n${bred} In Scope file is not a text file${reset}\\n\\n\"\n\t\texit\n\tfi\nfi\n\nif [[ $(id -u | grep -o '^0$') == \"0\" ]]; then\n\tSUDO=\" \"\nelse\n\tSUDO=\"sudo\"\nfi\n\nstartdir=${PWD}\n\nbanner\n\ncheck_version\n\nstartdir=${PWD}\nif [[ -n $list ]]; then\n\tif [[ $list == ./* ]]; then\n\t\tflist=\"${startdir}/${list:2}\"\n\telif [[ $list == ~* ]]; then\n\t\tflist=\"${HOME}/${list:2}\"\n\telif [[ $list == /* ]]; then\n\t\tflist=$list\n\telse\n\t\tflist=\"$startdir/$list\"\n\tfi\nelse\n\tflist=''\nfi\n\ncase $opt_mode in\n'r')\n\tif [[ -n $multi ]]; then\n\t\tif [[ $AXIOM == true ]]; then\n\t\t\tmode=\"multi_recon\"\n\t\tfi\n\t\tmulti_recon\n\t\texit\n\tfi\n\tif [[ -n $list ]]; then\n\t\tif [[ $AXIOM == true ]]; then\n\t\t\tmode=\"list_recon\"\n\t\tfi\n\t\tsed -i 's/\\r$//' $list\n\t\tfor domain in $(cat $list); do\n\t\t\tstart\n\t\t\trecon\n\t\t\tend\n\t\tdone\n\telse\n\t\tif [[ $AXIOM == true ]]; then\n\t\t\tmode=\"recon\"\n\t\tfi\n\t\tstart\n\t\trecon\n\t\tend\n\tfi\n\t;;\n's')\n\tif [[ -n $list ]]; then\n\t\tif [[ $AXIOM == true ]]; then\n\t\t\tmode=\"subs_menu\"\n\t\tfi\n\t\tsed -i 's/\\r$//' $list\n\t\tfor domain in $(cat $list); do\n\t\t\tsubs_menu\n\t\tdone\n\telse\n\t\tsubs_menu\n\tfi\n\t;;\n'p')\n\tif [[ -n $list ]]; then\n\t\tif [[ $AXIOM == true ]]; then\n\t\t\tmode=\"passive\"\n\t\tfi\n\t\tsed -i 's/\\r$//' $list\n\t\tfor domain in $(cat $list); do\n\t\t\tpassive\n\t\tdone\n\telse\n\t\tpassive\n\tfi\n\t;;\n'a')\n\texport VULNS_GENERAL=true\n\tif [[ -n $list ]]; then\n\t\tif [[ $AXIOM == true ]]; then\n\t\t\tmode=\"all\"\n\t\tfi\n\t\tsed -i 's/\\r$//' $list\n\t\tfor domain in $(cat $list); do\n\t\t\tall\n\t\tdone\n\telse\n\t\tall\n\tfi\n\t;;\n'w')\n\tif [[ -n $list ]]; then\n\t\tstart\n\t\tif [[ $list == /* ]]; then\n\t\t\tcp $list $dir/webs/webs.txt\n\t\telse\n\t\t\tcp ${SCRIPTPATH}/$list $dir/webs/webs.txt\n\t\tfi\n\telse\n\t\tprintf \"\\n\\n${bred} Web mode needs a website list file as target (./reconftw.sh -l target.txt -w) ${reset}\\n\\n\"\n\t\texit\n\tfi\n\twebs_menu\n\texit\n\t;;\n'n')\n\tPRESERVE=true\n\tif [[ -n $multi ]]; then\n\t\tmulti_osint\n\t\texit\n\tfi\n\tif [[ -n $list ]]; then\n\t\tsed -i 's/\\r$//' $list\n\t\twhile IFS= read -r domain; do\n\t\t\tstart\n\t\t\tosint\n\t\t\tend\n\t\tdone\n\telse\n\t\tstart\n\t\tosint\n\t\tend\n\tfi\n\t;;\n'z')\n\tif [[ -n $list ]]; then\n\t\tif [[ $AXIOM == true ]]; then\n\t\t\tmode=\"zen_menu\"\n\t\tfi\n\t\tsed -i 's/\\r$//' $list\n\t\tfor domain in $(cat $list); do\n\t\t\tzen_menu\n\t\tdone\n\telse\n\t\tzen_menu\n\tfi\n\t;;\n'c')\n\tif [[ -n $multi ]]; then\n\t\tif [[ $AXIOM == true ]]; then\n\t\t\tmode=\"multi_custom\"\n\t\tfi\n\t\tmulti_custom\n\telse\n\t\texport DIFF=true\n\t\tdir=\"${SCRIPTPATH}/Recon/$domain\"\n\t\tcd $dir || {\n\t\t\techo \"Failed to cd directory '$dir'\"\n\t\t\texit 1\n\t\t}\n\t\tLOGFILE=\"${dir}/.log/${NOW}_${NOWT}.txt\"\n\t\tcalled_fn_dir=$dir/.called_fn\n\t\t$custom_function\n\t\tcd ${SCRIPTPATH} || {\n\t\t\techo \"Failed to cd directory '$dir'\"\n\t\t\texit 1\n\t\t}\n\tfi\n\texit\n\t;;\n\t# No mode selected.  EXIT!\n*)\n\thelp\n\ttools_installed\n\texit 1\n\t;;\nesac\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 1.4306640625,
          "content": "aiohttp                 # sqlmap\nargcomplete             # brutespray\nargparse                # multiple\nbeautifulsoup4          # multiple\nbs4                     # multiple\ncensys                  # multiple\ncertifi                 # multiple\nchardet                 # ghauri\ncolorama                # ghauri\ncolorclass              # dnsvalidator\ndank                    # regulator\ndatetime                # JSA\ndatrie                  # regulator\ndnspython               # multiple\nemailfinder             # Tool\neditdistance            # regulator\nfake-useragent          # fav-up\nfuture                  # multiple\ngoogle                  # dorks_hunter\nipwhois                 # fav-up\nipaddress               # dnsvalidator\nmetafinder              # Tool\nmmh3                    # fav-up\nnetaddr                 # dnsvalidator\npsutil                  # xnLinkFinder\nPySocks                 # pwndb\nrequests                # multiple\nrequests_futures        # cloud_enums\nretrying                # multiple\nsetuptools              # multiple\nshodan                  # favUp\ntermcolor               # xnLinkFinder\ntldextract              # dorks_hunter\ntqdm                    # multiple\nujson                   # multiple\nurllib3                 # multiple\nxmltodict               # CloudHunter\nporch-pirate            # Tool\np1radup                 # Tool\njsbeautifier            # Tool\ngit+https://github.com/xnl-h4ck3r/xnLinkFinder.git            # Tool\n"
        }
      ]
    }
  ]
}