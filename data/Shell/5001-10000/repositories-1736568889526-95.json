{
  "metadata": {
    "timestamp": 1736568889526,
    "page": 95,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjEwMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "HariSekhon/DevOps-Bash-tools",
      "stars": 6093,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".appveyor.yml",
          "type": "blob",
          "size": 3.8701171875,
          "content": "#\n#  Author: Hari Sekhon\n#  Date: 2020-02-24 16:19:35 +0000 (Mon, 24 Feb 2020)\n#\n#  vim:ts=2:sts=2:sw=2:et\n#\n#  https://github.com/HariSekhon/DevOps-Bash-tools\n#\n#  License: see accompanying Hari Sekhon LICENSE file\n#\n#  If you're using my code you're welcome to connect with me on LinkedIn and optionally send me feedback to help steer this or other code I publish\n#\n#  https://www.linkedin.com/in/HariSekhon\n#\n\n# ============================================================================ #\n#                             A p p V e y o r   C I\n# ============================================================================ #\n\n# https://www.appveyor.com/docs/appveyor-yml/\n\nimage: Ubuntu\n\n# workaround for default JDK9 have old CA certs:\n#\n#   https://github.com/appveyor/ci/issues/3833\n#\n#   https://www.appveyor.com/docs/getting-started-with-appveyor-for-linux/#configuring-language-stack\n#\nstack: jdk 15\n\nskip_commits:\n  files:\n    - docs/*\n    - '**/*.md'\n\n# https://www.appveyor.com/docs/how-to/ssh-to-build-worker/\nenvironment:\n  APPVEYOR_SSH_KEY: ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEAvihSRU+YjBKvKiacDfUoZ7ghoVMcwNh4cWIYUNFGZosXOzNtyOcBpIb71TCgLFhOd+aMWKXCEC67BpNSIjt+a/FLD27AwmgVHv6cPlE3G0JJ9zmIrNmx9511dshTsxUW2O0SbYG+3InuO7FUkSrld+kA1OucyjgmZU7/+Cs9shpAEOaIVYmGlpDGRucAHpwtckvdgRTtnA3WNZ/Qg1vU6Ik4Xm03vjrW6lSiuTffYO1kbdcMQ4IZBlzfmovOtXQ0PomvN5NMCpgOyQuoNlvyS11tOXoqNiWOkiLE15XEzAQth9hHbNiH8jHJbAtkHqWWh0KK4IUyNGvoL6QfNxsTlw== hari@anotherdimension\n\n# enable SSH session accessible via my public key\n#init:\n#  - sh: curl -sflL 'https://raw.githubusercontent.com/appveyor/ci/master/scripts/enable-ssh.sh' | bash -e -\n\n# more useful at end to leverage .appveyor.yml tweaks like disabling broken mssql repo/dependencies, checking out project and building the core stuff happen first so we don't have to do all that manually in SSH session\non_finish:\n  # set this in Settings -> Environment dynamically instead of here\n  #- sh: export APPVEYOR_SSH_BLOCK=true\n  #\n  # workaround for https://github.com/appveyor/ci/issues/3373\n  #            and https://github.com/appveyor/ci/issues/3384\n  #\n  # has since been added to AppVeyor's own scripts:\n  #\n  #     https://github.com/appveyor/ci/pull/3385\n  #\n  #- sh: curl -sflL 'https://raw.githubusercontent.com/HariSekhon/DevOps-Bash-tools/master/install/install_openssh.sh' | bash -e -\n  #\n  # https://www.appveyor.com/docs/how-to/ssh-to-build-worker/\n  - sh: if [ \"$APPVEYOR_SSH_BLOCK\" = true ]; then curl -sflL 'https://raw.githubusercontent.com/appveyor/ci/master/scripts/enable-ssh.sh' | bash -e -; fi\n\ninstall:\n  # workaround for:\n  # Some packages could not be installed. This may mean that you have\n  # requested an impossible situation or if you are using the unstable\n  # distribution that some required packages have not yet been created\n  # or been moved out of Incoming.\n  # The following information may help to resolve the situation:\n  #\n  # The following packages have unmet dependencies:\n  #  mssql-server : Depends: libsasl2-modules-gssapi-mit but it is not going to be installed\n  #  E: Error, pkgProblemResolver::Resolve generated breaks, this may be caused by held packages.\n  #  bash-tools/Makefile.in:272: recipe for target 'apt-packages' failed\n  #  make[2]: *** [apt-packages] Error 123\n  #  make[2]: Leaving directory '/home/appveyor/projects/pylib'\n  #  bash-tools/Makefile.in:212: recipe for target 'system-packages' failed\n  #\n  #  adding \"|| :\" to the end of these commands causes them to be silently ignored!\n  - sudo sed -i '/https:\\/\\/packages.microsoft.com\\/ubuntu\\/.*\\/mssql-server/d' /etc/apt/sources.list\n  - sudo apt purge -yq --allow-change-held-packages mssql-server\n  # this prevents conflicts installing default-jdk - see https://github.com/appveyor/ci/issues/3411\n  #- dpkg -l | awk '/openjdk/{print $2}' | DEBIAN_FRONTEND=noninteractive xargs sudo apt-get remove -y --allow-change-held-packages\n  - setup/ci_bootstrap.sh\n  - make\n\ntest_script:\n  - make test\n\nbuild: off\n"
        },
        {
          "name": ".bash.d",
          "type": "tree",
          "content": null
        },
        {
          "name": ".bash_logout",
          "type": "blob",
          "size": 1.00390625,
          "content": "#!/usr/bin/env bash\n#  vim:ts=4:sts=4:sw=4:et\n#\n#  Author: Hari Sekhon\n#  Date: 2006-06-28 23:25:09 +0100 (Wed, 28 Jun 2006)\n#\n#  https://github.com/HariSekhon/DevOps-Bash-tools\n#\n#  License: see accompanying Hari Sekhon LICENSE file\n#\n#  If you're using my code you're welcome to connect with me on LinkedIn and optionally send me feedback to help steer this or other code I publish\n#\n#  https://www.linkedin.com/in/HariSekhon\n#\n\n# ============================================================================ #\n#                             B a s h   L o g o u t\n# ============================================================================ #\n\n# read all history lines not already read from the history file and append them to the history list\nhistory -n\n\n# destroy kerberos tickets in all caches (-A), quietly don't beep (-q)\nkdestroy -A -q\n\nclear\n\n# From Ubuntu\n## when leaving the console clear the screen to increase privacy\n#\n#if [ \"$SHLVL\" = 1 ]; then\n#    [ -x /usr/bin/clear_console ] && /usr/bin/clear_console -q\n#fi\n"
        },
        {
          "name": ".bash_profile",
          "type": "blob",
          "size": 1.423828125,
          "content": "#\n#  Author: Hari Sekhon\n#  Date: 2006-06-28 23:25:09 +0100 (Wed, 28 Jun 2006)\n#\n\n# ~/.bash_profile: executed by bash(1) for login shells.\n# see /usr/share/doc/bash/examples/startup-files for examples.\n# the files are located in the bash-doc package.\n\ntrap clear EXIT\n\n# the default umask is set in /etc/login.defs\n#umask 022\n\nif [ -f ~/.bashrc ]; then\n    . ~/.bashrc\nfi\n\n# not supported in the tmux terminal in GCP Cloud Shell\n#if ! is_mac &&\n#   ! isGoogleCloudShell &&\n#   [ \"${TERM:-}\" != \"xterm-256color\" ]; then\n#    setterm -blank 0\n#fi\n\n# prints a cool spinning welcome message which shows the time of last login\n# this is available in the Python / Perl and Golang Devops Tools repos,\n# as well as a function in .bash.d/welcome.sh in the DevOps Bash Tools repo\n#if type welcome &>/dev/null; then\n#    welcome\n#fi\n\n#eval \"$(rbenv init -)\"\n\n# from brew install bash-completion\n[[ -r \"/usr/local/etc/profile.d/bash_completion.sh\" ]] && . \"/usr/local/etc/profile.d/bash_completion.sh\"\n\n#sudo setmixer -V pcm 100\n\ncomplete -C /usr/local/bin/terragrunt terragrunt\n\n# ============================================================================ #\n# This should be automatically added to ~/.bash_profile when you install SDKman (install/install_sdkman.sh):\n#\n#THIS MUST BE AT THE END OF THE FILE FOR SDKMAN TO WORK!!!\nexport SDKMAN_DIR=\"$HOME/.sdkman\"\n[[ -s \"/Users/<user>/.sdkman/bin/sdkman-init.sh\" ]] && source \"/Users/<user>/.sdkman/bin/sdkman-init.sh\"\n"
        },
        {
          "name": ".bashrc",
          "type": "blob",
          "size": 5.5107421875,
          "content": "#!/usr/bin/env bash\n#  shellcheck disable=SC1091\n#  vim:ts=4:sts=4:sw=4:et\n#\n#  Author: Hari Sekhon\n#  Date: 2006-06-28 23:25:09 +0100 (Wed, 28 Jun 2006)\n#\n#  https://github.com/HariSekhon/DevOps-Bash-tools\n#\n#  License: see accompanying Hari Sekhon LICENSE file\n#\n#  If you're using my code you're welcome to connect with me on LinkedIn and optionally send me feedback to help steer this or other code I publish\n#\n#  https://www.linkedin.com/in/HariSekhon\n#\n\n# ============================================================================ #\n#                     BASH - Heavily Customized Environment\n# ============================================================================ #\n\n# Sources thousands of lines of Bash code written over the course of ~15+ years\n# some of which is now found in this GitHub repo's .bash.d/*.sh\n\n# ============================================================================ #\n#\n# put this at the top of your ~/.bashrc to inherit the goodness here (assuming you've checked out this repo to ~/github/bash-tools):\n#\n#   if [ -f ~/github/bash-tools/.bashrc ]; then\n#       . ~/github/bash-tools/.bashrc\n#   fi\n#\n# ============================================================================ #\n\n\n# Use with PS4 further down + profile-bash.pl (still in private repos) for performance profiling this bashrc\n#set -x\n\n# If not running interactively, don't do anything:\n[ -z \"${PS1:-}\" ] && return\n\n[ -n \"${PERLBREW_PERL:-}\" ] && return\n\n# Another alternative\n#case $- in\n#   *i*) ;;\n#     *) return 0;;\n#esac\n\n# Another variation\n#if [[ $- != *i* ]] ; then\n#    # Shell is non-interactive.  Be done now!\n#    return\n#fi\n\n# ============================================================================ #\n\n# after cleanshell, not even $HOME is set, this messes up things that base off $HOME, like SDKman\nif [ -z \"${HOME:-}\" ]; then\n    export HOME=~\nfi\n\nbash_tools=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\"\n# needed to inherit by things like vim script execution from within files using libraries rooted at this location\nexport bash_tools\n\n# shellcheck disable=SC1090,SC1091\n. \"$bash_tools/.bash.d/os_detection.sh\"\n\n# enable color support for ls\nif [ \"$TERM\" != \"dumb\" ] && \\\n   ! is_mac; then\n    eval \"$(dircolors -b)\"\nfi\n\n# shut up Mac, Bash still rocks\nexport BASH_SILENCE_DEPRECATION_WARNING=1\n\n# ============================================================================ #\n\n# technically should get called only for new login shells\n#[ -f /etc/profile     ] && . /etc/profile\n[ -f /etc/bash/bashrc ] && . /etc/bash/bashrc\n[ -f /etc/bashrc      ] && . /etc/bashrc\n\n[ -f /etc/bash_completion ] && . /etc/bash_completion\n\n[ -x /usr/bin/lesspipe ] && eval \"$(SHELL=/bin/sh lesspipe)\"\n\n# shellcheck disable=SC1090,SC1091\n[ -f \"$HOME/.aliases\" ] && source \"$HOME/.aliases\"\n\n# ============================================================================ #\n\n# SECURITY TO STOP STUFF BEING WRITTEN TO DISK\n#unset HISTFILE\n#unset HISTFILESIZE\nexport HISTSIZE=50000\nexport HISTFILESIZE=50000\n\nrmhist(){ history -d \"$1\"; }\nhistrm(){ rmhist \"$1\"; }\nhistrmlast(){ history -d \"$(history | tail -n 2 | head -n 1 | awk '{print $1}')\"; }\n\n# This adds a time format of \"YYYY-mm-dd hh:mm:ss  command\" to the bash history\nexport HISTTIMEFORMAT=\"%F %T  \"\n\n# stop logging duplicate successive commands to history\n#HISTCONTROL=ignoredups:ignorespace\nHISTCONTROL=ignoredups\n\n# Neat trick \"[ \\t]*\" to exclude any command by just prefixing it with a space. Fast way of going stealth for pw entering on cli\n# & here means any duplicate patterns, others are simple things like built-ins and ls and stuff you don't need history for\n#export HISTIGNORE=\"[ \\t]*:&:ls:[bf]g:exit\"\n\n# append rather than overwrite history\nshopt -s histappend\n\n# check window size and update $LINES and $COLUMNS after each command\nshopt -s checkwinsize\n\nshopt -s cdspell\n\n# prevent core dumps which can leak sensitive information\nulimit -c 0\n\n# tighten permissions except for root where library installations become inaccessible to my user account\nif [ $EUID = 0 ]; then\n    umask 0022\nelse\n    # caused no end of problems when doing sudo command which retained 0077 and broke library access for user accounts\n    #umask 0077\n    umask 0022\nfi\n\n# make less more friendly for non-text input files, see lesspipe(1)\n[ -x /usr/bin/lesspipe ] && eval \"$(SHELL=/bin/sh lesspipe)\"\n\n# ============================================================================ #\n\nsudo=sudo\nif [ $EUID -eq 0 ]; then\n    # used throughout .bash.d/*.sh\n    # shellcheck disable=SC2034\n    sudo=\"\"\nfi\n\n# shellcheck disable=SC1090,SC1091\ntype add_PATH &>/dev/null || . \"$bash_tools/.bash.d/paths.sh\"\n\n# ============================================================================ #\n\n# want this to fail is there is no match because we should always have local .bash.d/*.sh in this repo\n# shopt -s nullglob\nfor src in \"$bash_tools/.bash.d/\"*.sh; do\n    # shellcheck disable=SC1090,SC1091\n    . \"$src\"\ndone\n# shopt -u nullglob\n\n# added by travis gem - should be in ~/.bashrc so not needed to duplicate here\n#[ -f /Users/hari.sekhon/.travis/travis.sh ] && source /Users/hari.sekhon/.travis/travis.sh\n\n# shellcheck disable=SC1090,SC1091\n[ -f \"$HOME/.bashrc.local\" ] && . \"$HOME/.bashrc.local\"\nif [ -d \"$HOME/.bash.d\" ]; then\n    shopt -s nullglob\n    for src in \"$HOME/.bash.d/\"*.sh; do\n        # shellcheck disable=SC1090,SC1091\n        . \"$src\"\n    done\n    shopt -u nullglob\nfi\nif [ -d \"$HOME/.bash.autocomplete.d\" ]; then\n    shopt -s nullglob\n    for src in \"$HOME/.bash.autocomplete.d/\"*.sh; do\n        # shellcheck disable=SC1090,SC1091\n        . \"$src\"\n    done\n    shopt -u nullglob\nfi\n"
        },
        {
          "name": ".buildkite",
          "type": "tree",
          "content": null
        },
        {
          "name": ".circleci",
          "type": "tree",
          "content": null
        },
        {
          "name": ".cirrus.yml",
          "type": "blob",
          "size": 0.7939453125,
          "content": "#\n#  Author: Hari Sekhon\n#  Date: 2020-02-24 16:55:36 +0000 (Mon, 24 Feb 2020)\n#\n#  vim:ts=2:sts=2:sw=2:et\n#\n#  https://github.com/HariSekhon/DevOps-Bash-tools\n#\n#  License: see accompanying Hari Sekhon LICENSE file\n#\n#  If you're using my code you're welcome to connect with me on LinkedIn and optionally send me feedback to help steer this or other code I publish\n#\n#  https://www.linkedin.com/in/HariSekhon\n#\n\n# ============================================================================ #\n#                               C i r r u s   C I\n# ============================================================================ #\n\n# https://cirrus-ci.org/guide/writing-tasks/\n\ncontainer:\n  image: ubuntu:18.04\n\ntask:\n  env:\n    TMPDIR: /var/tmp\n  script:\n    - setup/ci_bootstrap.sh\n    - make init\n    - make ci test\n"
        },
        {
          "name": ".dockerignore",
          "type": "blob",
          "size": 84.5341796875,
          "content": "#\n#  Author: Hari Sekhon\n#  Date: Tue Sep 12 13:06:25 2017 +0200\n#\n#  vim: filetype=conf\n#\n\n# ============================================================================ #\n#                           . d o c k e r i g n o r e\n# ============================================================================ #\n\n# https://docs.docker.com/engine/reference/builder/#dockerignore-file\n\n# =================================================\n# Don't send things you don't need to Docker server and avoid uploading your secrets / keys!!\n#\n# XXX: RULES:\n#      - .dockerignore must be at top-level root context from which you invoke 'docker build'\n#      - unfortunately .dockerignore doesn't match basenames like .gitignore so you need to prefix **/ for recursive matching to be safe\n#      - Last Match Wins - you must put more specific whitelisting matches after the general exclusion pattern\n\n# ====================================================================\n# Smaller more concise .dockerignore files are found in the builds at:\n#\n#   https://github.com/HariSekhon/Dockerfiles\n\n# =========================================================\n# This is a huge list of exclusions which covers most cases\n#\n# XXX: Best Practice - enable '*' ignore and create short concise whitelist of inclusions\n#\n#      *\n#\n#      Second option is to rely on this pretty extensive blacklist, which excludes most known credentials files, all hidden dot files and major RCS repos like Git / Mercurial / Subversion\n#\n# Sloppy Docker builds which do 'COPY/ADD .' are a ** Security Risk ** - see Dockerfiles repo's tests/ which checks for their existence in all Dockerfile's\n#\n#   https://github.com/HariSekhon/Dockerfiles\n\n# *\n\n# =========================================================================================\n# Whitelist - must come after the more general blacklist pattern * above as Last Match Wins\n#\n# always include package.json and requirements.txt for standard dependency installations on NodeJS and Python\n!package.json\n!requirements.txt\n!LICENSE\n\n# ===============\n# Docker specific\n\n# still sent to the daemon as needed to do the job but aren't included in the image by ADD/COPY . commands\n**/Dockerfile\n**/.dockerignore\n\n**/docker-compose.yml\n**/.gitignore\n**/.gcloudignore\n**/.ssh/\n# don't accidentally publish your whole code base via Docker like Twitter Vine did!!\n**/.git/\n**/.svn/\n**/.hg/\n**/git/\n**/github/\n**/gitolite*/\n**/gitroot/\n**/mercurial/\n**/hg/\n**/hgroot/\n**/svn/\n**/svnroot/\n\n**/node_modules/\n**/dist/\n**/fatpacks/\n**/logs/\n**/vendor/\n**/vagrant/\n**/venv/\n**/wordlists/\n**/pytools_checks/\n**/debs/\n**/rpms/\n**/drive/\n**/Google Drive/\n**/Dropbox/\n\n# XXX: don't include any hidden files unless we explicitly override and include them with a !.filename\n**/.*\n\n# don't include CI configs not covered by ignoring dotfiles\n**/Jenkinsfile\n**/azure-pipelines.y*ml\n**/bitbucket-pipelines.y*ml\n**/buddy.y*ml\n**/codefresh.y*ml\n**/shippable.y*ml\n**/wercker.y*ml\n**/gocd_config_repo.json\n**/jenkins-job.xml\n**/hadolint.y*ml\n**/scalastyle_config.xml\n**/yamllint/\n# contains webhook URL which should not be committed publicly\n**/buildkite-pipeline*.json\n\n# leave our README.md in case we want to include it in the image but filter out other .md files\n*.md\n!README.md\n\n# ========================================\n# Based on the massive adjacent .gitignore, modified for Docker ignore's Go filepath.Match() function\n*#*#\n**/*.a\n**/*.avi\n**/*.bak\n**/*.bak.*\n**/*.bin\n**/*.bkp\n**/*.class\n**/*.dump\n**/*.flv\n**/*.kdb\n**/*.lock\n**/*.log\n**/*.macports-saved_*\n**/*.mp3\n**/*.mp4\n**/*.mpeg\n**/*.mpg\n**/*.o\n**/*.orig\n**/*.out\n**/*.part\n**/*.pyc\n**/*.pyo\n**/*.stderr\n**/*.stdout\n**/*.swo\n**/*.swp\n**/*.tmp\n**/*.wmv\n**/*~\n**/tmp.*\n**/~*\n\n**/*.doc\n**/*.docx\n**/*.msg\n**/*.pages\n**/*.ppt\n**/*.pptx\n**/*.rtf\n**/*.wpd\n**/*.wps\n**/*.xls\n**/*.xlsx\n\n# ============================================================================ #\n#\n# regenerate all sections below in to a single arg for API call via:\n#\n#       grep '[C]reated by https://' .dockerignore | sed 's,.*/,,' | tr ',' '\\n' | sort -u | tr '\\n' ',' | sed 's/,$//' | xargs echo gitignore.io_api.sh\n#\n# then pipe through perl to add recursive prefix '**/':\n#\n#       gitignore.io_api.sh ansible,apachehadoop,appcode,appengine,archive,archives,archlinuxpackages,audio,autotools,backup,basic,bittorrent,c,c++,certificates,chefcookbook,clojure,cloud9,cmake,code,code-java,codeblocks,compressed,compressedarchive,compression,data,database,datarecovery,diff,direnv,diskimage,docfx,docpress,docz,dotenv,dotfilessh,dotsettings,dropbox,eclipse,emacs,erlang,executable,firebase,flask,git,gitbook,go,gpg,gradle,grails,groovy,grunt,haskell,helm,homebrew,hugo,images,intellij,intellij+all,intellij+iml,java,java-web,jenv,jetbrains,jetbrains+all,jetbrains+iml,jmeter,julia,jupyternotebooks,kotlin,lamp,latex,less,linux,lua,macos,matlab,maven,mercurial,microsoftoffice,node,octave,osx,packer,patch,perl,perl6,phpunit,powershell,puppet,putty,pycharm,pycharm+all,pycharm+iml,pydev,python,r,rails,react,reactnative,redis,root,ruby,rust,sbt,scala,serverless,sonar,sonarqube,spark,splunk,spreadsheet,ssh,sublimetext,svn,terraform,terragrunt,tortoisegit,vagrant,venv,virtualenv,visualstudio,visualstudiocode,vs,vscode,vue,vuejs,waf,windows,xcode,xcodeinjection,zsh | perl -p -e 's/^([^#\\s\\/])/**\\/$1/; s/^\\//**\\//' >> .dockerignore\n#\n# Find new or missing tags you aren't using yet:\n#\n#       grep '[C]reated by https://' .dockerignore | sed 's,.*/,,' | tr ',' '\\n' | sort -u | tr '\\n' ',' | sed 's/,$//' | gitignore.io_api.sh missing\n#\n# ============================================================================ #\n\n\n# Created by https://www.toptal.com/developers/gitignore/api/ansible,apachehadoop,appcode,appengine,archive,archives,archlinuxpackages,audio,autotools,backup,basic,bittorrent,c,c++,certificates,chefcookbook,clojure,cloud9,cmake,code,code-java,codeblocks,compressed,compressedarchive,compression,data,database,datarecovery,diff,direnv,diskimage,docfx,docpress,docz,dotenv,dotfilessh,dotsettings,dropbox,eclipse,emacs,erlang,executable,firebase,flask,git,gitbook,go,gpg,gradle,grails,groovy,grunt,haskell,helm,homebrew,hugo,images,intellij,intellij+all,intellij+iml,java,java-web,jenv,jetbrains,jetbrains+all,jetbrains+iml,jmeter,julia,jupyternotebooks,kotlin,lamp,latex,less,linux,lua,macos,matlab,maven,mercurial,microsoftoffice,node,octave,osx,packer,patch,perl,perl6,phpunit,powershell,puppet,putty,pycharm,pycharm+all,pycharm+iml,pydev,python,r,rails,react,reactnative,redis,root,ruby,rust,sbt,scala,serverless,sonar,sonarqube,spark,splunk,spreadsheet,ssh,sublimetext,svn,terraform,terragrunt,tortoisegit,vagrant,venv,virtualenv,visualstudio,visualstudiocode,vs,vscode,vue,vuejs,waf,windows,xcode,xcodeinjection,zsh\n# Edit at https://www.toptal.com/developers/gitignore?templates=ansible,apachehadoop,appcode,appengine,archive,archives,archlinuxpackages,audio,autotools,backup,basic,bittorrent,c,c++,certificates,chefcookbook,clojure,cloud9,cmake,code,code-java,codeblocks,compressed,compressedarchive,compression,data,database,datarecovery,diff,direnv,diskimage,docfx,docpress,docz,dotenv,dotfilessh,dotsettings,dropbox,eclipse,emacs,erlang,executable,firebase,flask,git,gitbook,go,gpg,gradle,grails,groovy,grunt,haskell,helm,homebrew,hugo,images,intellij,intellij+all,intellij+iml,java,java-web,jenv,jetbrains,jetbrains+all,jetbrains+iml,jmeter,julia,jupyternotebooks,kotlin,lamp,latex,less,linux,lua,macos,matlab,maven,mercurial,microsoftoffice,node,octave,osx,packer,patch,perl,perl6,phpunit,powershell,puppet,putty,pycharm,pycharm+all,pycharm+iml,pydev,python,r,rails,react,reactnative,redis,root,ruby,rust,sbt,scala,serverless,sonar,sonarqube,spark,splunk,spreadsheet,ssh,sublimetext,svn,terraform,terragrunt,tortoisegit,vagrant,venv,virtualenv,visualstudio,visualstudiocode,vs,vscode,vue,vuejs,waf,windows,xcode,xcodeinjection,zsh\n\n### Ansible ###\n**/*.retry\n\n### ApacheHadoop ###\n**/*.iml\n**/*.ipr\n**/*.iws\n**/*.orig\n**/*.rej\n**/.idea\n**/.svn\n**/.classpath\n**/.project\n**/.settings\n**/target\n**/hadoop-common-project/hadoop-kms/downloads/\n**/hadoop-hdfs-project/hadoop-hdfs/downloads\n**/hadoop-hdfs-project/hadoop-hdfs-httpfs/downloads\n**/hadoop-common-project/hadoop-common/src/test/resources/contract-test-options.xml\n**/hadoop-tools/hadoop-openstack/src/test/resources/contract-test-options.xml\n\n### AppCode ###\n# Covers JetBrains IDEs: IntelliJ, RubyMine, PhpStorm, AppCode, PyCharm, CLion, Android Studio, WebStorm and Rider\n# Reference: https://intellij-support.jetbrains.com/hc/en-us/articles/206544839\n\n# User-specific stuff\n**/.idea/**/workspace.xml\n**/.idea/**/tasks.xml\n**/.idea/**/usage.statistics.xml\n**/.idea/**/dictionaries\n**/.idea/**/shelf\n\n# Generated files\n**/.idea/**/contentModel.xml\n\n# Sensitive or high-churn files\n**/.idea/**/dataSources/\n**/.idea/**/dataSources.ids\n**/.idea/**/dataSources.local.xml\n**/.idea/**/sqlDataSources.xml\n**/.idea/**/dynamic.xml\n**/.idea/**/uiDesigner.xml\n**/.idea/**/dbnavigator.xml\n\n# Gradle\n**/.idea/**/gradle.xml\n**/.idea/**/libraries\n\n# Gradle and Maven with auto-import\n# When using Gradle or Maven with auto-import, you should exclude module files,\n# since they will be recreated, and may cause churn.  Uncomment if using\n# auto-import.\n# .idea/artifacts\n# .idea/compiler.xml\n# .idea/jarRepositories.xml\n# .idea/modules.xml\n# .idea/*.iml\n# .idea/modules\n# *.iml\n# *.ipr\n\n# CMake\n**/cmake-build-*/\n\n# Mongo Explorer plugin\n**/.idea/**/mongoSettings.xml\n\n# File-based project format\n\n# IntelliJ\n**/out/\n\n# mpeltonen/sbt-idea plugin\n**/.idea_modules/\n\n# JIRA plugin\n**/atlassian-ide-plugin.xml\n\n# Cursive Clojure plugin\n**/.idea/replstate.xml\n\n# Crashlytics plugin (for Android Studio and IntelliJ)\n**/com_crashlytics_export_strings.xml\n**/crashlytics.properties\n**/crashlytics-build.properties\n**/fabric.properties\n\n# Editor-based Rest Client\n**/.idea/httpRequests\n\n# Android studio 3.1+ serialized cache file\n**/.idea/caches/build_file_checksums.ser\n\n### AppCode Patch ###\n# Comment Reason: https://github.com/joeblau/gitignore.io/issues/186#issuecomment-215987721\n\n# *.iml\n# modules.xml\n# .idea/misc.xml\n# *.ipr\n\n# Sonarlint plugin\n# https://plugins.jetbrains.com/plugin/7973-sonarlint\n**/.idea/**/sonarlint/\n\n# SonarQube Plugin\n# https://plugins.jetbrains.com/plugin/7238-sonarqube-community-plugin\n**/.idea/**/sonarIssues.xml\n\n# Markdown Navigator plugin\n# https://plugins.jetbrains.com/plugin/7896-markdown-navigator-enhanced\n**/.idea/**/markdown-navigator.xml\n**/.idea/**/markdown-navigator-enh.xml\n**/.idea/**/markdown-navigator/\n\n# Cache file creation bug\n# See https://youtrack.jetbrains.com/issue/JBR-2257\n**/.idea/$CACHE_FILE$\n\n# CodeStream plugin\n# https://plugins.jetbrains.com/plugin/12206-codestream\n**/.idea/codestream.xml\n\n### AppEngine ###\n# Google App Engine generated folder\n**/appengine-generated/\n\n### Archive ###\n\n### Mostly from https://en.wikipedia.org/wiki/List_of_archive_formats\n\n## Archiving only\n# The traditional archive format on Unix-like systems, now used mainly for the creation of static libraries.\n**/*.a\n**/*.ar\n# RPM files consist of metadata concatenated with (usually) a cpio archive. Newer RPM systems also support other archives, as cpio is becoming obsolete. cpio is also used with initramfs.\n**/*.cpio\n\n# A self-extracting archive that uses the Bourne shell (sh).\n**/*.shar\n# A system for storing multiple files. LBR archives typically contained files processed by SQ, or the archive itself was compressed with SQ. LBR archives that were compressed with SQ ended with the extension .LQR\n**/*.LBR\n# An archive format originally used mainly for archiving and distribution of the exact, nearly-exact, or custom-modified contents of an optical storage medium such as a CD-ROM or DVD-ROM. However, it can be used to archive the contents of other storage media, selected partitions, folders, and/or files. The resulting archive is typically optimized for convenient rendering to (re-)writable CD or DVD media.\n**/*.iso\n# A library format used primarily on the Commodore 64 and 128 lines of computers. This bears no resemblance to the DOS LBR format. While library files were quick to implement (a number of programs exist to work with them) they are crippled in that they cannot grow with use: once a file has been created it cannot be amended (files added, changed or deleted) without recreating the entire file.\n**/*.lbr\n# An archive format used by Mozilla for storing binary diffs. Used in conjunction with bzip2.\n**/*.mar\n# A common archive format used on Unix-like systems. Generally used in conjunction with compressors such as gzip, bzip2, compress or xz to create .tar.gz, .tar.bz2, .tar.Z or tar.xz files.\n**/*.tar\n\n# Package managers\n# Red Hat Package Manager\n**/*.rpm\n# Debian package\n**/*.deb\n# MicroSoft Installer\n**/*.msi\n**/*.msm\n**/*.msp\n# Mozilla package installer\n**/*.xpi\n# Ruby Package\n**/*.gem\n\n\n### Archives ###\n# It's better to unpack these files and commit the raw source because\n# git has its own built in compression methods.\n**/*.7z\n**/*.jar\n**/*.rar\n**/*.zip\n**/*.gz\n**/*.gzip\n**/*.tgz\n**/*.bzip\n**/*.bzip2\n**/*.bz2\n**/*.xz\n**/*.lzma\n**/*.cab\n**/*.xar\n\n# Packing-only formats\n\n# Package management formats\n**/*.dmg\n**/*.egg\n**/*.txz\n\n### ArchLinuxPackages ###\n**/*.tar.*\n**/*.exe\n**/*.log\n**/*.log.*\n**/*.sig\n\n**/pkg/\n**/src/\n\n### Audio ###\n**/*.aif\n**/*.iff\n**/*.m3u\n**/*.m4a\n**/*.mid\n**/*.mp3\n**/*.mpa\n**/*.ra\n**/*.wav\n**/*.wma\n**/*.ogg\n**/*.flac\n\n### Autotools ###\n# http://www.gnu.org/software/automake\n\n**/Makefile.in\n**/ar-lib\n**/mdate-sh\n**/py-compile\n**/test-driver\n**/ylwrap\n**/.deps/\n\n# http://www.gnu.org/software/autoconf\n\n**/autom4te.cache\n**/autoscan.log\n**/autoscan-*.log\n**/aclocal.m4\n**/compile\n**/config.guess\n**/config.h.in\n**/config.log\n**/config.status\n**/config.sub\n**/configure\n**/configure.scan\n**/depcomp\n**/install-sh\n**/missing\n**/stamp-h1\n\n# https://www.gnu.org/software/libtool/\n\n**/ltmain.sh\n\n# http://www.gnu.org/software/texinfo\n\n**/texinfo.tex\n\n# http://www.gnu.org/software/m4/\n\n**/m4/libtool.m4\n**/m4/ltoptions.m4\n**/m4/ltsugar.m4\n**/m4/ltversion.m4\n**/m4/lt~obsolete.m4\n\n# Generated Makefile\n# (meta build system like autotools,\n# can automatically generate from config.status script\n# (which is called by configure script))\n**/Makefile\n\n### Autotools Patch ###\n\n### Backup ###\n**/*.bak\n**/*.gho\n**/*.ori\n**/*.tmp\n\n### Basic ###\n# Apples Build\n**/*.build\n**/*.apples\n\n# Initialized files\n**/*.ini\n**/*.basic\n\n### BitTorrent ###\n**/*.torrent\n\n### C ###\n# Prerequisites\n**/*.d\n\n# Object files\n**/*.o\n**/*.ko\n**/*.obj\n**/*.elf\n\n# Linker output\n**/*.ilk\n**/*.map\n**/*.exp\n\n# Precompiled Headers\n**/*.gch\n**/*.pch\n\n# Libraries\n**/*.lib\n**/*.la\n**/*.lo\n\n# Shared objects (inc. Windows DLLs)\n**/*.dll\n**/*.so\n**/*.so.*\n**/*.dylib\n\n# Executables\n**/*.out\n**/*.app\n**/*.i*86\n**/*.x86_64\n**/*.hex\n\n# Debug files\n**/*.dSYM/\n**/*.su\n**/*.idb\n**/*.pdb\n\n# Kernel Module Compile Results\n**/*.mod*\n**/*.cmd\n**/.tmp_versions/\n**/modules.order\n**/Module.symvers\n**/Mkfile.old\n**/dkms.conf\n\n### C++ ###\n# Prerequisites\n\n# Compiled Object files\n**/*.slo\n\n# Precompiled Headers\n\n# Compiled Dynamic libraries\n\n# Fortran module files\n**/*.mod\n**/*.smod\n\n# Compiled Static libraries\n**/*.lai\n\n# Executables\n\n### Zsh ###\n# Zsh compiled script + zrecompile backup\n**/*.zwc\n**/*.zwc.old\n\n# Zsh completion-optimization dumpfile\n**/*zcompdump*\n\n# Zsh zcalc history\n**/.zcalc_history\n\n# A popular plugin manager's files\n**/._zplugin\n**/.zplugin_lstupd\n\n# zdharma/zshelldoc tool's files\n**/zsdoc/data\n\n# robbyrussell/oh-my-zsh/plugins/per-directory-history plugin's files\n# (when set-up to store the history in the local directory)\n**/.directory_history\n\n# MichaelAquilina/zsh-autoswitch-virtualenv plugin's files\n# (for Zsh plugins using Python)\n**/.venv\n\n# Zunit tests' output\n**/tests/_output/*\n**/!/tests/_output/.gitkeep\n\n### certificates ###\n**/*.pem\n**/*.key\n**/*.crt\n**/*.cer\n**/*.priv\n\n### ChefCookbook ###\n**/.vagrant\n**/cookbooks\n\n# Bundler\n**/bin/*\n**/.bundle/*\n\n**/.kitchen/\n**/.kitchen.local.yml\n**/.kitchen.*.local.yml\n**/kitchen.local.yml\n**/kitchen.*.local.yml\n\n### CMake ###\n**/CMakeLists.txt.user\n**/CMakeCache.txt\n**/CMakeFiles\n**/CMakeScripts\n**/Testing\n**/cmake_install.cmake\n**/install_manifest.txt\n**/compile_commands.json\n**/CTestTestfile.cmake\n**/_deps\n\n### CMake Patch ###\n# External projects\n**/*-prefix/\n\n### Clojure ###\n**/pom.xml\n**/pom.xml.asc\n**/*.class\n**/lib/\n**/classes/\n**/target/\n**/checkouts/\n**/.lein-deps-sum\n**/.lein-repl-history\n**/.lein-plugins/\n**/.lein-failures\n**/.nrepl-port\n**/.cpcache/\n\n### Code-Java ###\n# Language Support for Java(TM) by Red Hat extension for Visual Studio Code - https://marketplace.visualstudio.com/items?itemName=redhat.java\n\n**/factoryConfiguration.json\n\n### Cloud9 ###\n# Cloud9 IDE - http://c9.io\n**/.c9revisions\n**/.c9\n\n### Compressed ###\n**/*.pkg\n**/*.sit\n**/*.sitx\n**/*.zipx\n\n### CompressedArchive ###\n\n\n## Archiving and compression\n# Open source file format. Used by 7-Zip.\n# Mac OS X, restoration on different platforms is possible although not immediate \tYes \tBased on 7z. Preserves Spotlight metadata, resource forks, owner/group information, dates and other data which would be otherwise lost with compression.\n**/*.s7z\n# Old archive versions only \tProprietary format\n**/*.ace\n# A format that compresses and doubly encrypt the data (AES256 and CAS256) avoiding brute force attacks, also hide files in an AFA file. It has two ways to safeguard data integrity and subsequent repair of the file if has an error (repair with AstroA2P (online) or Astrotite (offline)).\n**/*.afa\n# A mainly Korean format designed for very large archives.\n**/*.alz\n# Android application package (variant of JAR file format).\n**/*.apk\n# ??\n**/*.arc\n# Originally DOS, now multiple\n**/*.arj\n# Open archive format, used by B1 Free Archiver (http://dev.b1.org/standard/archive-format.html)\n**/*.b1\n# Binary Archive with external header\n**/*.ba\n# Proprietary format from the ZipTV Compression Components\n**/*.bh\n# The Microsoft Windows native archive format, which is also used by many commercial installers such as InstallShield and WISE.\n# Originally DOS, now DOS and Windows \tCreated by Yaakov Gringeler; released last in 2003 (Compressia 1.0.0.1 beta), now apparently defunct. Free trial of 30 days lets user create and extract archives; after that it is possible to extract, but not to create.\n**/*.car\n# Open source file format.\n**/*.cfs\n# Compact Pro archive, a common archiver used on Mac platforms until about Mac OS 7.5.x. Competed with StuffIt; now obsolete.\n**/*.cpt\n# Windows, Unix-like, Mac OS X Open source file format. Files are compressed individually with either gzip, bzip2 or lzo.\n**/*.dar\n# DiskDoubler \tMac OS \t\t\tobsolete\n**/*.dd\n# ??\n**/*.dgc\n# Apple Disk Image upports \"Internet-enabled\" disk images, which, once downloaded, are automatically decompressed, mounted, have the contents extracted, and thrown away. Currently, Safari is the only browser that supports this form of extraction; however, the images can be manually extracted as well. This format can also be password-protected or encrypted with 128-bit or 256-bit AES encryption.\n# Enterprise Java Archive archive\n**/*.ear\n# ETSoft compressed archive\n# The predecessor of DGCA.\n**/*.gca\n# Originally DOS \tYes, but may be covered by patents \tDOS era format; uses arithmetic/Markov coding\n**/*.ha\n# MS Windows \tHKI\n**/*.hki\n# Produced by ICEOWS program. Excels at text file compression.\n**/*.ice\n# Java archive, compatible with ZIP files\n# Open sourced archiver with compression using the PAQ family of algorithms and optional encryption.\n**/*.kgb\n# Originally DOS, now multiple \tMultiple \tYes \tThe standard format on Amiga.\n**/*.lzh\n**/*.lha\n# Archiver originally used on The Amiga. Now copied by Microsoft to use in their .cab and .chm files.\n**/*.lzx\n# file format from NoGate Consultings, a rival from ARC-Compressor.\n**/*.pak\n# A disk image archive format that supports several compression methods as well as splitting the archive into smaller pieces.\n**/*.partimg\n# An experimental open source packager (http://mattmahoney.net/dc)\n**/*.paq*\n# Open source archiver supporting authenticated encryption, volume spanning, customizable object level and volume level integrity checks (form CRCs to SHA-512 and Whirlpool hashes), fast deflate based compression\n**/*.pea\n# The format from the PIM - a freeware compression tool by Ilia Muraviev. It uses an LZP-based compression algorithm with set of filters for executable, image and audio files.\n**/*.pim\n# PackIt \tMac OS \t\t\tobsolete\n**/*.pit\n# Used for data in games written using the Quadruple D library for Delphi. Uses byte pair compression.\n**/*.qda\n# A proprietary archive format, second in popularity to .zip files.\n# The format from a commercial archiving package. Odd among commercial packages in that they focus on incorporating experimental algorithms with the highest possible compression (at the expense of speed and memory), such as PAQ, PPMD and PPMZ (PPMD with unlimited-length strings), as well as a proprietary algorithms.\n**/*.rk\n# Self Dissolving ARChive \tCommodore 64, Commodore 128 \tCommodore 64, Commodore 128 \tYes \tSDAs refer to Self Dissolving ARC files, and are based on the Commodore 64 and Commodore 128 versions of ARC, originally written by Chris Smeets. While the files share the same extension, they are not compatible between platforms. That is, an SDA created on a Commodore 64 but run on a Commodore 128 in Commodore 128 mode will crash the machine, and vice versa. The intended successor to SDA is SFX.\n**/*.sda\n# A pre-Mac OS X Self-Extracting Archive format. StuffIt, Compact Pro, Disk Doubler and others could create .sea files, though the StuffIt versions were the most common.\n**/*.sea\n# Scifer Archive with internal header\n**/*.sen\n# Commodore 64, Commodore 128 \tSFX is a Self Extracting Archive which uses the LHArc compression algorithm. It was originally developed by Chris Smeets on the Commodore platform, and runs primarily using the CS-DOS extension for the Commodore 128. Unlike its predecessor SDA, SFX files will run on both the Commodore 64 and Commodore 128 regardless of which machine they were created on.\n**/*.sfx\n# An archive format designed for the Apple II series of computers. The canonical implementation is ShrinkIt, which can operate on disk images as well as files. Preferred compression algorithm is a combination of RLE and 12-bit LZW. Archives can be manipulated with the command-line NuLib tool, or the Windows-based CiderPress.\n**/*.shk\n# A compression format common on Apple Macintosh computers. The free StuffIt Expander is available for Windows and OS X.\n# The replacement for the .sit format that supports more compression methods, UNIX file permissions, long file names, very large files, more encryption options, data specific compressors (JPEG, Zip, PDF, 24-bit image, MP3). The free StuffIt Expander is available for Windows and OS X.\n# A royalty-free compressing format\n**/*.sqx\n# The \"tarball\" format combines tar archives with a file-based compression scheme (usually gzip). Commonly used for source and binary distribution on Unix-like platforms, widely available elsewhere.\n**/*.tar.gz\n**/*.tar.Z\n**/*.tar.bz2\n**/*.tbz2\n**/*.tar.lzma\n**/*.tlz\n# UltraCompressor 2.3 was developed to act as an alternative to the then popular PKZIP application. The main feature of the application is its ability to create large archives. This means that compressed archives with the UC2 file extension can hold almost 1 million files.\n**/*.uc\n**/*.uc0\n**/*.uc2\n**/*.ucn\n**/*.ur2\n**/*.ue2\n# Based on PAQ, RZM, CSC, CCM, and 7zip. The format consists of a PAQ, RZM, CSC, or CCM compressed file and a manifest with compression settings stored in a 7z archive.\n**/*.uca\n# A high compression rate archive format originally for DOS.\n**/*.uha\n# Web Application archive (Java-based web app)\n**/*.war\n# File-based disk image format developed to deploy Microsoft Windows.\n**/*.wim\n# XAR\n# Native format of the Open Source KiriKiri Visual Novel engine. Uses combination of block splitting and zlib compression. The filenames and pathes are stored in UTF-16 format. For integrity check, the Adler-32 hashsum is used. For many commercial games, the files are encrypted (and decoded on runtime) via so-called \"cxdec\" module, which implements xor-based encryption.\n**/*.xp3\n# Yamazaki zipper archive. Compression format used in DeepFreezer archiver utility created by Yamazaki Satoshi. Read and write support exists in TUGZip, IZArc and ZipZag\n**/*.yz1\n# The most widely used compression format on Microsoft Windows. Commonly used on Macintosh and Unix systems as well.\n# application/x-zoo \tzoo \tMultiple \tMultiple \tYes\n**/*.zoo\n# Journaling (append-only) archive format with rollback capability. Supports deduplication and incremental update based on last-modified dates. Multi-threaded. Compresses in LZ77, BWT, and context mixing formats. Open source.\n**/*.zpaq\n# Archiver with a compression algorithm based on the Burrows-Wheeler transform method.\n**/*.zz\n\n\n### Compression ###\n\n### From https://en.wikipedia.org/wiki/List_of_archive_formats\n\n## Compression only\n# An open source, patent- and royalty-free compression format. The compression algorithm is a Burrows-Wheeler transform followed by a move-to-front transform and finally Huffman coding\n# Old compressor for QNX4 OS. The compression algorithm is a modified LZSS, with an adaptive Huffman coding.\n**/*.F\n# GNU Zip, the primary compression format used by Unix-like systems. The compression algorithm is DEFLATE.\n# An alternate LZMA algorithm implementation, with support for checksums and ident bytes.\n**/*.lz\n# The LZMA compression algorithm as used by 7-Zip\n# An implementation of the LZO data compression algorithm\n**/*.lzo\n# A compression program designed to do particularly well on very large files containing long distance redundancy.\n**/*.rz\n# Windows compress/decompress- Linux and Mac OS X decompress only \tA compression program designed to do high compression on SF2 files (SoundFont)\n**/*.sfark\n# A compression format invented by Google and open-sourced in 2011. Snappy aims for very high speeds, reasonable compression, and maximum stability rather than maximum compression or compatibility with any other compression library.\n**/*.sz\n# Squeeze: A program which compressed files. A file which was \"squeezed\" had the middle initial of the name changed to \"Q\", so that a squeezed text file would end with .TQT, a squeezed executable would end with .CQM or .EQE. Typically used with .LBR archives, either by storing the squeezed files in the archive, or by storing the files decompressed and then compressing the archive, which would have a name ending in \".LQR\".\n**/*.?Q?\n# A compression program written by Steven Greenberg implementing the LZW algorithm. For several years in the CP/M world when no implementation was available of ARC, CRUNCHed files stored in .LBR archives were very popular. CRUNCH's implementation of LZW had a somewhat unique feature of modifying and occasionally clearing the code table in memory when it became full, resulting in a few percent better compression on many files.\n**/*.?Z?\n# A compression format using LZMA2 to yield very high compression ratios.\n# The traditional Huffman coding compression format.\n**/*.z\n# The traditional LZW compression format.\n**/*.Z\n# Joke compression program, actually increasing file size\n**/*.infl\n# Compression format(s) used by some DOS and Windows install programs. MS-DOS includes expand.exe to decompress its install files. The compressed files are created with a matching compress.exe command. The compression algorithm is LZSS.\n**/*.??_\n\n\n### Data ###\n**/*.csv\n**/*.dat\n**/*.efx\n**/*.gbr\n**/*.pps\n**/*.ppt\n**/*.pptx\n**/*.sdf\n**/*.tax2010\n**/*.vcf\n**/*.xml\n\n### Code ###\n**/.vscode/*\n**/!.vscode/settings.json\n**/!.vscode/tasks.json\n**/!.vscode/launch.json\n**/!.vscode/extensions.json\n**/*.code-workspace\n\n### DataRecovery ###\n\n\n## Data recovery\n# File format used by dvdisaster to be used for data recovery when discs become damaged or partially unreadable.\n**/*.ecc\n# File format used in conjunction with any archive format to provide redundancy and data recovery, most often in newsgroup distribution of binary files.\n**/*.par\n**/*.par2\n\n\n### Diff ###\n**/*.patch\n**/*.diff\n\n### direnv ###\n**/.direnv\n**/.envrc\n\n### CodeBlocks ###\n# specific to CodeBlocks IDE\n**/*.layout\n**/*.depend\n# generated directories\n**/bin/\n**/obj/\n\n### DocFx ###\n**/.cache\n**/**/_site/\n\n### Docpress ###\n# docpress documentation generator: https://docpress.github.io/index.html\n\n**/_docpress/\n\n### Docz ###\n**/.docz\n\n\n### dotenv ###\n**/.env\n\n### DotfilesSh ###\n**/local-patch\n**/patched-src\n\n### DotSettings ###\n**/*.DotSettings\n\n### Dropbox ###\n# Dropbox settings and caches\n**/.dropbox\n**/.dropbox.attr\n**/.dropbox.cache\n\n### Eclipse ###\n**/.metadata\n**/tmp/\n**/*.swp\n**/*~.nib\n**/local.properties\n**/.settings/\n**/.loadpath\n**/.recommenders\n\n# External tool builders\n**/.externalToolBuilders/\n\n# Locally stored \"Eclipse launch configurations\"\n**/*.launch\n\n# PyDev specific (Python IDE for Eclipse)\n**/*.pydevproject\n\n# CDT-specific (C/C++ Development Tooling)\n**/.cproject\n\n# CDT- autotools\n**/.autotools\n\n# Java annotation processor (APT)\n**/.factorypath\n\n# PDT-specific (PHP Development Tools)\n**/.buildpath\n\n# sbteclipse plugin\n**/.target\n\n# Tern plugin\n**/.tern-project\n\n# TeXlipse plugin\n**/.texlipse\n\n# STS (Spring Tool Suite)\n**/.springBeans\n\n# Code Recommenders\n**/.recommenders/\n\n# Annotation Processing\n**/.apt_generated/\n**/.apt_generated_test/\n\n# Scala IDE specific (Scala & Java development for Eclipse)\n**/.cache-main\n**/.scala_dependencies\n**/.worksheet\n\n# Uncomment this line if you wish to ignore the project description file.\n# Typically, this file would be tracked if it contains build/dependency configurations:\n#.project\n\n### Eclipse Patch ###\n# Spring Boot Tooling\n**/.sts4-cache/\n\n### Emacs ###\n# -*- mode: gitignore; -*-\n**/*~\n**/\\#*\\#\n**/.emacs.desktop\n**/.emacs.desktop.lock\n**/*.elc\n**/auto-save-list\n**/tramp\n**/.\\#*\n\n# Org-mode\n**/.org-id-locations\n**/*_archive\n\n# flymake-mode\n**/*_flymake.*\n\n# eshell files\n**/eshell/history\n**/eshell/lastdir\n\n# elpa packages\n**/elpa/\n\n# reftex files\n**/*.rel\n\n# AUCTeX auto folder\n**/auto/\n\n# cask packages\n**/.cask/\n**/dist/\n\n# Flycheck\n**/flycheck_*.el\n\n# server auth directory\n**/server/\n\n# projectiles files\n**/.projectile\n\n# directory configuration\n**/.dir-locals.el\n\n# network security\n**/network-security.data\n\n\n### Database ###\n**/*.accdb\n**/*.db\n**/*.dbf\n**/*.mdb\n**/*.sqlite3\n\n### Executable ###\n**/*.bat\n**/*.cgi\n**/*.com\n**/*.gadget\n**/*.pif\n**/*.vb\n**/*.wsf\n\n### Firebase ###\n**/**/node_modules/*\n**/**/.firebaserc\n\n### Firebase Patch ###\n**/.runtimeconfig.json\n**/.firebase/\n\n### Flask ###\n**/instance/*\n**/!instance/.gitignore\n**/.webassets-cache\n\n### Flask.Python Stack ###\n# Byte-compiled / optimized / DLL files\n**/__pycache__/\n**/*.py[cod]\n**/*$py.class\n\n# C extensions\n\n# Distribution / packaging\n**/.Python\n**/build/\n**/develop-eggs/\n**/downloads/\n**/eggs/\n**/.eggs/\n**/lib/\n**/lib64/\n**/parts/\n**/sdist/\n**/var/\n**/wheels/\n**/pip-wheel-metadata/\n**/share/python-wheels/\n**/*.egg-info/\n**/.installed.cfg\n**/MANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n**/*.manifest\n**/*.spec\n\n# Installer logs\n**/pip-log.txt\n**/pip-delete-this-directory.txt\n\n# Unit test / coverage reports\n**/htmlcov/\n**/.tox/\n**/.nox/\n**/.coverage\n**/.coverage.*\n**/nosetests.xml\n**/coverage.xml\n**/*.cover\n**/*.py,cover\n**/.hypothesis/\n**/.pytest_cache/\n**/pytestdebug.log\n\n# Translations\n**/*.mo\n**/*.pot\n\n# Django stuff:\n**/local_settings.py\n**/db.sqlite3\n**/db.sqlite3-journal\n\n# Flask stuff:\n**/instance/\n\n# Scrapy stuff:\n**/.scrapy\n\n# Sphinx documentation\n**/docs/_build/\n**/doc/_build/\n\n# PyBuilder\n**/target/\n\n# Jupyter Notebook\n**/.ipynb_checkpoints\n\n# IPython\n**/profile_default/\n**/ipython_config.py\n\n# pyenv\n**/.python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow\n**/__pypackages__/\n\n# Celery stuff\n**/celerybeat-schedule\n**/celerybeat.pid\n\n# SageMath parsed files\n**/*.sage.py\n\n# Environments\n**/env/\n**/venv/\n**/ENV/\n**/env.bak/\n**/venv.bak/\n**/pythonenv*\n\n# Spyder project settings\n**/.spyderproject\n**/.spyproject\n\n# Rope project settings\n**/.ropeproject\n\n# mkdocs documentation\n**/site\n\n# mypy\n**/.mypy_cache/\n**/.dmypy.json\n**/dmypy.json\n\n# Pyre type checker\n**/.pyre/\n\n# pytype static type analyzer\n**/.pytype/\n\n# profiling data\n**/.prof\n\n### Git ###\n# Created by git for backups. To disable backups in Git:\n# $ git config --global mergetool.keepBackup false\n\n# Created by git when using merge tools for conflicts\n**/*.BACKUP.*\n**/*.BASE.*\n**/*.LOCAL.*\n**/*.REMOTE.*\n**/*_BACKUP_*.txt\n**/*_BASE_*.txt\n**/*_LOCAL_*.txt\n**/*_REMOTE_*.txt\n\n### GitBook ###\n# Node rules:\n## Grunt intermediate storage (http://gruntjs.com/creating-plugins#storing-task-files)\n**/.grunt\n\n## Dependency directory\n## Commenting this out is preferred by some people, see\n## https://docs.npmjs.com/misc/faq#should-i-check-my-node_modules-folder-into-git\n**/node_modules\n\n# Book build output\n**/_book\n\n# eBook build output\n**/*.epub\n**/*.mobi\n**/*.pdf\n\n### Go ###\n# Binaries for programs and plugins\n**/*.exe~\n\n# Test binary, built with `go test -c`\n**/*.test\n\n# Output of the go coverage tool, specifically when used with LiteIDE\n\n# Dependency directories (remove the comment below to include it)\n# vendor/\n\n### Go Patch ###\n**/vendor/\n**/Godeps/\n\n### GPG ###\n**/secring.*\n\n\n### DiskImage ###\n**/*.toast\n**/*.vcd\n\n### Grails ###\n# .gitignore for Grails 1.2 and 1.3\n# Although this should work for most versions of grails, it is\n# suggested that you use the \"grails integrate-with --git\" command\n# to generate your .gitignore file.\n\n# web application files\n**/web-app/WEB-INF/classes\n\n# default HSQL database files for production mode\n**/prodDb.*\n\n# general HSQL database files\n**/*Db.properties\n**/*Db.script\n\n# logs\n**/stacktrace.log\n**/test/reports\n**/logs\n\n# project release file\n**/*.war\n\n# plugin release files\n**/*.zip\n**/plugin.xml\n\n# older plugin install locations\n**/plugins\n**/web-app/plugins\n\n# \"temporary\" build files\n**/target\n\n### Groovy ###\n# .gitignore created from Groovy contributors in https://github.com/apache/groovy/blob/master/.gitignore\n\n**/user.gradle\n**/.gradle/\n\n**/*.DS_Store\n\n**/.shelf\n\n\n\n### grunt ###\n# Grunt usually compiles files inside this directory\n\n# Grunt usually preprocesses files such as coffeescript, compass... inside the .tmp directory\n**/.tmp/\n\n### Haskell ###\n**/dist\n**/dist-*\n**/cabal-dev\n**/*.hi\n**/*.hie\n**/*.chi\n**/*.chs.h\n**/*.dyn_o\n**/*.dyn_hi\n**/.hpc\n**/.hsenv\n**/.cabal-sandbox/\n**/cabal.sandbox.config\n**/*.prof\n**/*.aux\n**/*.hp\n**/*.eventlog\n**/.stack-work/\n**/cabal.project.local\n**/cabal.project.local~\n**/.HTF/\n**/.ghc.environment.*\n\n### Helm ###\n# Chart dependencies\n**/**/charts/*.tgz\n\n### Homebrew ###\n**/Brewfile.lock.json\n\n### Hugo ###\n# Generated files by hugo\n**/public/\n**/resources/_gen/\n**/hugo_stats.json\n\n# Executable may be added to repository\n**/hugo.exe\n**/hugo.darwin\n**/hugo.linux\n\n### Images ###\n# JPEG\n**/*.jpg\n**/*.jpeg\n**/*.jpe\n**/*.jif\n**/*.jfif\n**/*.jfi\n\n# JPEG 2000\n**/*.jp2\n**/*.j2k\n**/*.jpf\n**/*.jpx\n**/*.jpm\n**/*.mj2\n\n# JPEG XR\n**/*.jxr\n**/*.hdp\n**/*.wdp\n\n# Graphics Interchange Format\n**/*.gif\n\n# RAW\n**/*.raw\n\n# Web P\n**/*.webp\n\n# Portable Network Graphics\n**/*.png\n\n# Animated Portable Network Graphics\n**/*.apng\n\n# Multiple-image Network Graphics\n**/*.mng\n\n# Tagged Image File Format\n**/*.tiff\n**/*.tif\n\n# Scalable Vector Graphics\n**/*.svg\n**/*.svgz\n\n# Portable Document Format\n\n# X BitMap\n**/*.xbm\n\n# BMP\n**/*.bmp\n**/*.dib\n\n# ICO\n**/*.ico\n\n# 3D Images\n**/*.3dm\n**/*.max\n\n### Intellij ###\n# Covers JetBrains IDEs: IntelliJ, RubyMine, PhpStorm, AppCode, PyCharm, CLion, Android Studio, WebStorm and Rider\n# Reference: https://intellij-support.jetbrains.com/hc/en-us/articles/206544839\n\n# User-specific stuff\n\n# Generated files\n\n# Sensitive or high-churn files\n\n# Gradle\n\n# Gradle and Maven with auto-import\n# When using Gradle or Maven with auto-import, you should exclude module files,\n# since they will be recreated, and may cause churn.  Uncomment if using\n# auto-import.\n# .idea/artifacts\n# .idea/compiler.xml\n# .idea/jarRepositories.xml\n# .idea/modules.xml\n# .idea/*.iml\n# .idea/modules\n# *.iml\n# *.ipr\n\n# CMake\n\n# Mongo Explorer plugin\n\n# File-based project format\n\n# IntelliJ\n\n# mpeltonen/sbt-idea plugin\n\n# JIRA plugin\n\n# Cursive Clojure plugin\n\n# Crashlytics plugin (for Android Studio and IntelliJ)\n\n# Editor-based Rest Client\n\n# Android studio 3.1+ serialized cache file\n\n### Intellij Patch ###\n# Comment Reason: https://github.com/joeblau/gitignore.io/issues/186#issuecomment-215987721\n\n# *.iml\n# modules.xml\n# .idea/misc.xml\n# *.ipr\n\n# Sonarlint plugin\n# https://plugins.jetbrains.com/plugin/7973-sonarlint\n\n# SonarQube Plugin\n# https://plugins.jetbrains.com/plugin/7238-sonarqube-community-plugin\n\n# Markdown Navigator plugin\n# https://plugins.jetbrains.com/plugin/7896-markdown-navigator-enhanced\n\n# Cache file creation bug\n# See https://youtrack.jetbrains.com/issue/JBR-2257\n\n# CodeStream plugin\n# https://plugins.jetbrains.com/plugin/12206-codestream\n\n### Intellij+all ###\n# Covers JetBrains IDEs: IntelliJ, RubyMine, PhpStorm, AppCode, PyCharm, CLion, Android Studio, WebStorm and Rider\n# Reference: https://intellij-support.jetbrains.com/hc/en-us/articles/206544839\n\n# User-specific stuff\n\n# Generated files\n\n# Sensitive or high-churn files\n\n# Gradle\n\n# Gradle and Maven with auto-import\n# When using Gradle or Maven with auto-import, you should exclude module files,\n# since they will be recreated, and may cause churn.  Uncomment if using\n# auto-import.\n# .idea/artifacts\n# .idea/compiler.xml\n# .idea/jarRepositories.xml\n# .idea/modules.xml\n# .idea/*.iml\n# .idea/modules\n# *.iml\n# *.ipr\n\n# CMake\n\n# Mongo Explorer plugin\n\n# File-based project format\n\n# IntelliJ\n\n# mpeltonen/sbt-idea plugin\n\n# JIRA plugin\n\n# Cursive Clojure plugin\n\n# Crashlytics plugin (for Android Studio and IntelliJ)\n\n# Editor-based Rest Client\n\n# Android studio 3.1+ serialized cache file\n\n### Intellij+all Patch ###\n# Ignores the whole .idea folder and all .iml files\n# See https://github.com/joeblau/gitignore.io/issues/186 and https://github.com/joeblau/gitignore.io/issues/360\n\n**/.idea/\n\n# Reason: https://github.com/joeblau/gitignore.io/issues/186#issuecomment-249601023\n\n**/modules.xml\n**/.idea/misc.xml\n\n# Sonarlint plugin\n**/.idea/sonarlint\n\n### Intellij+iml ###\n# Covers JetBrains IDEs: IntelliJ, RubyMine, PhpStorm, AppCode, PyCharm, CLion, Android Studio, WebStorm and Rider\n# Reference: https://intellij-support.jetbrains.com/hc/en-us/articles/206544839\n\n# User-specific stuff\n\n# Generated files\n\n# Sensitive or high-churn files\n\n# Gradle\n\n# Gradle and Maven with auto-import\n# When using Gradle or Maven with auto-import, you should exclude module files,\n# since they will be recreated, and may cause churn.  Uncomment if using\n# auto-import.\n# .idea/artifacts\n# .idea/compiler.xml\n# .idea/jarRepositories.xml\n# .idea/modules.xml\n# .idea/*.iml\n# .idea/modules\n# *.iml\n# *.ipr\n\n# CMake\n\n# Mongo Explorer plugin\n\n# File-based project format\n\n# IntelliJ\n\n# mpeltonen/sbt-idea plugin\n\n# JIRA plugin\n\n# Cursive Clojure plugin\n\n# Crashlytics plugin (for Android Studio and IntelliJ)\n\n# Editor-based Rest Client\n\n# Android studio 3.1+ serialized cache file\n\n### Intellij+iml Patch ###\n# Reason: https://github.com/joeblau/gitignore.io/issues/186#issuecomment-249601023\n\n\n### Java ###\n# Compiled class file\n\n# Log file\n\n# BlueJ files\n**/*.ctxt\n\n# Mobile Tools for Java (J2ME)\n**/.mtj.tmp/\n\n# Package Files #\n**/*.nar\n\n# virtual machine crash logs, see http://www.java.com/en/download/help/error_hotspot.xml\n**/hs_err_pid*\n\n### Java-Web ###\n## ignoring target file\n\n### JEnv ###\n# JEnv local Java version configuration file\n**/.java-version\n\n# Used by previous versions of JEnv\n**/.jenv-version\n\n### JetBrains ###\n# Covers JetBrains IDEs: IntelliJ, RubyMine, PhpStorm, AppCode, PyCharm, CLion, Android Studio, WebStorm and Rider\n# Reference: https://intellij-support.jetbrains.com/hc/en-us/articles/206544839\n\n# User-specific stuff\n\n# Generated files\n\n# Sensitive or high-churn files\n\n# Gradle\n\n# Gradle and Maven with auto-import\n# When using Gradle or Maven with auto-import, you should exclude module files,\n# since they will be recreated, and may cause churn.  Uncomment if using\n# auto-import.\n# .idea/artifacts\n# .idea/compiler.xml\n# .idea/jarRepositories.xml\n# .idea/modules.xml\n# .idea/*.iml\n# .idea/modules\n# *.iml\n# *.ipr\n\n# CMake\n\n# Mongo Explorer plugin\n\n# File-based project format\n\n# IntelliJ\n\n# mpeltonen/sbt-idea plugin\n\n# JIRA plugin\n\n# Cursive Clojure plugin\n\n# Crashlytics plugin (for Android Studio and IntelliJ)\n\n# Editor-based Rest Client\n\n# Android studio 3.1+ serialized cache file\n\n### JetBrains Patch ###\n# Comment Reason: https://github.com/joeblau/gitignore.io/issues/186#issuecomment-215987721\n\n# *.iml\n# modules.xml\n# .idea/misc.xml\n# *.ipr\n\n# Sonarlint plugin\n# https://plugins.jetbrains.com/plugin/7973-sonarlint\n\n# SonarQube Plugin\n# https://plugins.jetbrains.com/plugin/7238-sonarqube-community-plugin\n\n# Markdown Navigator plugin\n# https://plugins.jetbrains.com/plugin/7896-markdown-navigator-enhanced\n\n# Cache file creation bug\n# See https://youtrack.jetbrains.com/issue/JBR-2257\n\n# CodeStream plugin\n# https://plugins.jetbrains.com/plugin/12206-codestream\n\n### JetBrains+all ###\n# Covers JetBrains IDEs: IntelliJ, RubyMine, PhpStorm, AppCode, PyCharm, CLion, Android Studio, WebStorm and Rider\n# Reference: https://intellij-support.jetbrains.com/hc/en-us/articles/206544839\n\n# User-specific stuff\n\n# Generated files\n\n# Sensitive or high-churn files\n\n# Gradle\n\n# Gradle and Maven with auto-import\n# When using Gradle or Maven with auto-import, you should exclude module files,\n# since they will be recreated, and may cause churn.  Uncomment if using\n# auto-import.\n# .idea/artifacts\n# .idea/compiler.xml\n# .idea/jarRepositories.xml\n# .idea/modules.xml\n# .idea/*.iml\n# .idea/modules\n# *.iml\n# *.ipr\n\n# CMake\n\n# Mongo Explorer plugin\n\n# File-based project format\n\n# IntelliJ\n\n# mpeltonen/sbt-idea plugin\n\n# JIRA plugin\n\n# Cursive Clojure plugin\n\n# Crashlytics plugin (for Android Studio and IntelliJ)\n\n# Editor-based Rest Client\n\n# Android studio 3.1+ serialized cache file\n\n### JetBrains+all Patch ###\n# Ignores the whole .idea folder and all .iml files\n# See https://github.com/joeblau/gitignore.io/issues/186 and https://github.com/joeblau/gitignore.io/issues/360\n\n\n# Reason: https://github.com/joeblau/gitignore.io/issues/186#issuecomment-249601023\n\n\n# Sonarlint plugin\n\n### JetBrains+iml ###\n# Covers JetBrains IDEs: IntelliJ, RubyMine, PhpStorm, AppCode, PyCharm, CLion, Android Studio, WebStorm and Rider\n# Reference: https://intellij-support.jetbrains.com/hc/en-us/articles/206544839\n\n# User-specific stuff\n\n# Generated files\n\n# Sensitive or high-churn files\n\n# Gradle\n\n# Gradle and Maven with auto-import\n# When using Gradle or Maven with auto-import, you should exclude module files,\n# since they will be recreated, and may cause churn.  Uncomment if using\n# auto-import.\n# .idea/artifacts\n# .idea/compiler.xml\n# .idea/jarRepositories.xml\n# .idea/modules.xml\n# .idea/*.iml\n# .idea/modules\n# *.iml\n# *.ipr\n\n# CMake\n\n# Mongo Explorer plugin\n\n# File-based project format\n\n# IntelliJ\n\n# mpeltonen/sbt-idea plugin\n\n# JIRA plugin\n\n# Cursive Clojure plugin\n\n# Crashlytics plugin (for Android Studio and IntelliJ)\n\n# Editor-based Rest Client\n\n# Android studio 3.1+ serialized cache file\n\n### JetBrains+iml Patch ###\n# Reason: https://github.com/joeblau/gitignore.io/issues/186#issuecomment-249601023\n\n\n### JMeter ###\n# JMeter common ignore files\n# http://jmeter.apache.org/\n\n# Ignore Summary/Aggregrate reports\n**/*.jtl\n\n# Ignore log files\n\n# Ignore customized user.properties\n**/user.properties\n\n### Erlang ###\n**/.eunit\n**/*.beam\n**/*.plt\n**/erl_crash.dump\n**/.concrete/DEV_MODE\n\n# rebar 2.x\n**/.rebar\n**/rel/example_project\n**/ebin/*.beam\n**/deps\n\n# rebar 3\n**/.rebar3\n**/_build/\n**/_checkouts/\n\n### JupyterNotebooks ###\n# gitignore template for Jupyter Notebooks\n# website: http://jupyter.org/\n\n**/*/.ipynb_checkpoints/*\n\n# IPython\n\n# Remove previous ipynb_checkpoints\n#   git rm -r .ipynb_checkpoints/\n\n### Kotlin ###\n# Compiled class file\n\n# Log file\n\n# BlueJ files\n\n# Mobile Tools for Java (J2ME)\n\n# Package Files #\n\n# virtual machine crash logs, see http://www.java.com/en/download/help/error_hotspot.xml\n\n### LAMP ###\n# LAMP Stack Base\n\n### LAMP.Linux Stack ###\n\n# temporary files which can be created if a process still has a handle open of a deleted file\n**/.fuse_hidden*\n\n# KDE directory preferences\n**/.directory\n\n# Linux trash folder which might appear on any partition or disk\n**/.Trash-*\n\n# .nfs files are created when an open file is removed but is still being accessed\n**/.nfs*\n\n### LAMP.PHP Stack ###\n# Covers JetBrains IDEs: IntelliJ, RubyMine, PhpStorm, AppCode, PyCharm, CLion, Android Studio, WebStorm and Rider\n# Reference: https://intellij-support.jetbrains.com/hc/en-us/articles/206544839\n\n# User-specific stuff\n\n# Generated files\n\n# Sensitive or high-churn files\n\n# Gradle\n\n# Gradle and Maven with auto-import\n# When using Gradle or Maven with auto-import, you should exclude module files,\n# since they will be recreated, and may cause churn.  Uncomment if using\n# auto-import.\n# .idea/artifacts\n# .idea/compiler.xml\n# .idea/jarRepositories.xml\n# .idea/modules.xml\n# .idea/*.iml\n# .idea/modules\n# *.iml\n# *.ipr\n\n# CMake\n\n# Mongo Explorer plugin\n\n# File-based project format\n\n# IntelliJ\n\n# mpeltonen/sbt-idea plugin\n\n# JIRA plugin\n\n# Cursive Clojure plugin\n\n# Crashlytics plugin (for Android Studio and IntelliJ)\n\n# Editor-based Rest Client\n\n# Android studio 3.1+ serialized cache file\n\n### LaTeX ###\n## Core latex/pdflatex auxiliary files:\n**/*.lof\n**/*.lot\n**/*.fls\n**/*.toc\n**/*.fmt\n**/*.fot\n**/*.cb\n**/*.cb2\n**/.*.lb\n\n## Intermediate documents:\n**/*.dvi\n**/*.xdv\n**/*-converted-to.*\n# these rules might exclude image files for figures etc.\n# *.ps\n# *.eps\n# *.pdf\n\n## Generated if empty string is given at \"Please type another file name for output:\"\n**/.pdf\n\n## Bibliography auxiliary files (bibtex/biblatex/biber):\n**/*.bbl\n**/*.bcf\n**/*.blg\n**/*-blx.aux\n**/*-blx.bib\n**/*.run.xml\n\n## Build tool auxiliary files:\n**/*.fdb_latexmk\n**/*.synctex\n**/*.synctex(busy)\n**/*.synctex.gz\n**/*.synctex.gz(busy)\n**/*.pdfsync\n\n## Build tool directories for auxiliary files\n# latexrun\n**/latex.out/\n\n## Auxiliary and intermediate files from other packages:\n# algorithms\n**/*.alg\n**/*.loa\n\n# achemso\n**/acs-*.bib\n\n# amsthm\n**/*.thm\n\n# beamer\n**/*.nav\n**/*.pre\n**/*.snm\n**/*.vrb\n\n# changes\n**/*.soc\n\n# comment\n**/*.cut\n\n# cprotect\n\n# elsarticle (documentclass of Elsevier journals)\n**/*.spl\n\n# endnotes\n**/*.ent\n\n# fixme\n**/*.lox\n\n# feynmf/feynmp\n**/*.mf\n**/*.mp\n**/*.t[1-9]\n**/*.t[1-9][0-9]\n**/*.tfm\n\n#(r)(e)ledmac/(r)(e)ledpar\n**/*.end\n**/*.?end\n**/*.[1-9]\n**/*.[1-9][0-9]\n**/*.[1-9][0-9][0-9]\n**/*.[1-9]R\n**/*.[1-9][0-9]R\n**/*.[1-9][0-9][0-9]R\n**/*.eledsec[1-9]\n**/*.eledsec[1-9]R\n**/*.eledsec[1-9][0-9]\n**/*.eledsec[1-9][0-9]R\n**/*.eledsec[1-9][0-9][0-9]\n**/*.eledsec[1-9][0-9][0-9]R\n\n# glossaries\n**/*.acn\n**/*.acr\n**/*.glg\n**/*.glo\n**/*.gls\n**/*.glsdefs\n**/*.lzs\n\n# uncomment this for glossaries-extra (will ignore makeindex's style files!)\n# *.ist\n\n# gnuplottex\n**/*-gnuplottex-*\n\n# gregoriotex\n**/*.gaux\n**/*.gtex\n\n# htlatex\n**/*.4ct\n**/*.4tc\n**/*.idv\n**/*.lg\n**/*.trc\n**/*.xref\n\n# hyperref\n**/*.brf\n\n# knitr\n**/*-concordance.tex\n# TODO Comment the next line if you want to keep your tikz graphics files\n**/*.tikz\n**/*-tikzDictionary\n\n# listings\n**/*.lol\n\n# luatexja-ruby\n**/*.ltjruby\n\n# makeidx\n**/*.idx\n**/*.ilg\n**/*.ind\n\n# minitoc\n**/*.maf\n**/*.mlf\n**/*.mlt\n**/*.mtc\n**/*.mtc[0-9]*\n**/*.slf[0-9]*\n**/*.slt[0-9]*\n**/*.stc[0-9]*\n\n# minted\n**/_minted*\n**/*.pyg\n\n# morewrites\n**/*.mw\n\n# nomencl\n**/*.nlg\n**/*.nlo\n**/*.nls\n\n# pax\n**/*.pax\n\n# pdfpcnotes\n**/*.pdfpc\n\n# sagetex\n**/*.sagetex.sage\n**/*.sagetex.py\n**/*.sagetex.scmd\n\n# scrwfile\n**/*.wrt\n\n# sympy\n**/*.sout\n**/*.sympy\n**/sympy-plots-for-*.tex/\n\n# pdfcomment\n**/*.upa\n**/*.upb\n\n# pythontex\n**/*.pytxcode\n**/pythontex-files-*/\n\n# tcolorbox\n**/*.listing\n\n# thmtools\n**/*.loe\n\n# TikZ & PGF\n**/*.dpth\n**/*.md5\n**/*.auxlock\n\n# todonotes\n**/*.tdo\n\n# vhistory\n**/*.hst\n**/*.ver\n\n# easy-todo\n**/*.lod\n\n# xcolor\n**/*.xcp\n\n# xmpincl\n**/*.xmpi\n\n# xindy\n**/*.xdy\n\n# xypic precompiled matrices and outlines\n**/*.xyc\n**/*.xyd\n\n# endfloat\n**/*.ttt\n**/*.fff\n\n# Latexian\n**/TSWLatexianTemp*\n\n## Editors:\n# WinEdt\n**/*.sav\n\n# Texpad\n**/.texpadtmp\n\n# LyX\n**/*.lyx~\n\n# Kile\n**/*.backup\n\n# gummi\n**/.*.swp\n\n# KBibTeX\n**/*~[0-9]*\n\n# TeXnicCenter\n**/*.tps\n\n# auto folder when using emacs and auctex\n**/./auto/*\n**/*.el\n\n# expex forward references with \\gathertags\n**/*-tags.tex\n\n# standalone packages\n**/*.sta\n\n# Makeindex log files\n**/*.lpz\n\n# REVTeX puts footnotes in the bibliography by default, unless the nofootinbib\n# option is specified. Footnotes are the stored in a file with suffix Notes.bib.\n# Uncomment the next line to have this generated file ignored.\n#*Notes.bib\n\n### LaTeX Patch ###\n# LIPIcs / OASIcs\n**/*.vtc\n\n# glossaries\n**/*.glstex\n\n### Less ###\n**/*.less\n\n### Linux ###\n\n# temporary files which can be created if a process still has a handle open of a deleted file\n\n# KDE directory preferences\n\n# Linux trash folder which might appear on any partition or disk\n\n# .nfs files are created when an open file is removed but is still being accessed\n\n### Lua ###\n# Compiled Lua sources\n**/luac.out\n\n# luarocks build files\n**/*.src.rock\n\n# Object files\n**/*.os\n\n# Precompiled Headers\n\n# Libraries\n**/*.def\n\n# Shared objects (inc. Windows DLLs)\n\n# Executables\n\n\n### macOS ###\n# General\n**/.DS_Store\n**/.AppleDouble\n**/.LSOverride\n\n# Icon must end with two \\r\n**/Icon\r\n\n# Thumbnails\n**/._*\n\n# Files that might appear in the root of a volume\n**/.DocumentRevisions-V100\n**/.fseventsd\n**/.Spotlight-V100\n**/.TemporaryItems\n**/.Trashes\n**/.VolumeIcon.icns\n**/.com.apple.timemachine.donotpresent\n\n# Directories potentially created on remote AFP share\n**/.AppleDB\n**/.AppleDesktop\n**/Network Trash Folder\n**/Temporary Items\n**/.apdisk\n\n### MATLAB ###\n# Windows default autosave extension\n**/*.asv\n\n# OSX / *nix default autosave extension\n**/*.m~\n\n# Compiled MEX binaries (all platforms)\n**/*.mex*\n\n# Packaged app and toolbox files\n**/*.mlappinstall\n**/*.mltbx\n\n# Generated helpsearch folders\n**/helpsearch*/\n\n# Simulink code generation folders\n**/slprj/\n**/sccprj/\n\n# Matlab code generation folders\n**/codegen/\n\n# Simulink autosave extension\n**/*.autosave\n\n# Simulink cache files\n**/*.slxc\n\n# Octave session info\n**/octave-workspace\n\n### Maven ###\n**/pom.xml.tag\n**/pom.xml.releaseBackup\n**/pom.xml.versionsBackup\n**/pom.xml.next\n**/release.properties\n**/dependency-reduced-pom.xml\n**/buildNumber.properties\n**/.mvn/timing.properties\n# https://github.com/takari/maven-wrapper#usage-without-binary-jar\n**/.mvn/wrapper/maven-wrapper.jar\n\n### Mercurial ###\n**/.hg/\n**/.hgignore\n**/.hgsigs\n**/.hgsub\n**/.hgsubstate\n**/.hgtags\n\n### MicrosoftOffice ###\n\n# Word temporary\n**/~$*.doc*\n\n# Word Auto Backup File\n**/Backup of *.doc*\n\n# Excel temporary\n**/~$*.xls*\n\n# Excel Backup File\n**/*.xlk\n\n# PowerPoint temporary\n**/~$*.ppt*\n\n# Visio autosave temporary files\n**/*.~vsd*\n\n### Node ###\n# Logs\n**/logs\n**/npm-debug.log*\n**/yarn-debug.log*\n**/yarn-error.log*\n**/lerna-debug.log*\n\n# Diagnostic reports (https://nodejs.org/api/report.html)\n**/report.[0-9]*.[0-9]*.[0-9]*.[0-9]*.json\n\n# Runtime data\n**/pids\n**/*.pid\n**/*.seed\n**/*.pid.lock\n\n# Directory for instrumented libs generated by jscoverage/JSCover\n**/lib-cov\n\n# Coverage directory used by tools like istanbul\n**/coverage\n**/*.lcov\n\n# nyc test coverage\n**/.nyc_output\n\n# Grunt intermediate storage (https://gruntjs.com/creating-plugins#storing-task-files)\n\n# Bower dependency directory (https://bower.io/)\n**/bower_components\n\n# node-waf configuration\n**/.lock-wscript\n\n# Compiled binary addons (https://nodejs.org/api/addons.html)\n**/build/Release\n\n# Dependency directories\n**/node_modules/\n**/jspm_packages/\n\n# TypeScript v1 declaration files\n**/typings/\n\n# TypeScript cache\n**/*.tsbuildinfo\n\n# Optional npm cache directory\n**/.npm\n\n# Optional eslint cache\n**/.eslintcache\n\n# Microbundle cache\n**/.rpt2_cache/\n**/.rts2_cache_cjs/\n**/.rts2_cache_es/\n**/.rts2_cache_umd/\n\n# Optional REPL history\n**/.node_repl_history\n\n# Output of 'npm pack'\n\n# Yarn Integrity file\n**/.yarn-integrity\n\n# dotenv environment variables file\n**/.env.test\n**/.env*.local\n\n# parcel-bundler cache (https://parceljs.org/)\n**/.parcel-cache\n\n# Next.js build output\n**/.next\n\n# Nuxt.js build / generate output\n**/.nuxt\n\n# Gatsby files\n**/.cache/\n# Comment in the public line in if your project uses Gatsby and not Next.js\n# https://nextjs.org/blog/next-9-1#public-directory-support\n# public\n\n# vuepress build output\n**/.vuepress/dist\n\n# Serverless directories\n**/.serverless/\n\n# FuseBox cache\n**/.fusebox/\n\n# DynamoDB Local files\n**/.dynamodb/\n\n# TernJS port file\n**/.tern-port\n\n# Stores VSCode versions used for testing VSCode extensions\n**/.vscode-test\n\n### Octave ###\n# Windows default autosave extension\n\n# OSX / *nix default autosave extension\n\n# Compiled MEX binaries (all platforms)\n\n# Packaged app and toolbox files\n\n# Generated helpsearch folders\n\n# Simulink code generation folders\n\n# Matlab code generation folders\n\n# Simulink autosave extension\n\n# Simulink cache files\n\n# Octave session info\n\n### OSX ###\n# General\n\n# Icon must end with two \\r\n\n# Thumbnails\n\n# Files that might appear in the root of a volume\n\n# Directories potentially created on remote AFP share\n\n### Packer ###\n# Cache objects\n**/packer_cache/\n\n# Crash log\n**/crash.log\n\n# For built boxes\n**/*.box\n\n### Patch ###\n\n### Perl ###\n**/!Build/\n**/.last_cover_stats\n**/META.yml\n**/META.json\n**/MYMETA.*\n**/*.pm.tdy\n**/*.bs\n\n# Devel::Cover\n**/cover_db/\n\n# Devel::NYTProf\n**/nytprof.out\n\n# Dizt::Zilla\n**/.build/\n\n# Module::Build\n**/Build\n**/Build.bat\n\n# Module::Install\n**/inc/\n\n# ExtUtils::MakeMaker\n**/blib/\n**/_eumm/\n**/*.gz\n**/Makefile\n**/Makefile.old\n**/MANIFEST.bak\n**/pm_to_blib\n\n### Perl6 ###\n# Gitignore for Perl 6 (http://www.perl6.org)\n# As part of https://github.com/github/gitignore\n\n# precompiled files\n**/.precomp\n**/lib/.precomp\n\n\n### PHPUnit ###\n# Covers PHPUnit\n# Reference: https://phpunit.de/\n\n# Generated files\n**/.phpunit.result.cache\n\n# PHPUnit\n**/app/phpunit.xml\n**/phpunit.xml\n\n# Build data\n**/build/\n\n### PowerShell ###\n# Exclude packaged modules\n\n# Exclude .NET assemblies from source\n\n### Puppet ###\n# gitignore template for Puppet modules\n# website: https://forge.puppet.com/\n\n# Built packages\n**/pkg/*\n\n# Should run on multiple platforms so don't check in\n**/Gemfile.lock\n\n# Tests\n**/spec/fixtures/*\n**/coverage/*\n\n# Third-party\n**/vendor/*\n\n### PuTTY ###\n# Private key\n**/*.ppk\n\n### PyCharm ###\n# Covers JetBrains IDEs: IntelliJ, RubyMine, PhpStorm, AppCode, PyCharm, CLion, Android Studio, WebStorm and Rider\n# Reference: https://intellij-support.jetbrains.com/hc/en-us/articles/206544839\n\n# User-specific stuff\n\n# Generated files\n\n# Sensitive or high-churn files\n\n# Gradle\n\n# Gradle and Maven with auto-import\n# When using Gradle or Maven with auto-import, you should exclude module files,\n# since they will be recreated, and may cause churn.  Uncomment if using\n# auto-import.\n# .idea/artifacts\n# .idea/compiler.xml\n# .idea/jarRepositories.xml\n# .idea/modules.xml\n# .idea/*.iml\n# .idea/modules\n# *.iml\n# *.ipr\n\n# CMake\n\n# Mongo Explorer plugin\n\n# File-based project format\n\n# IntelliJ\n\n# mpeltonen/sbt-idea plugin\n\n# JIRA plugin\n\n# Cursive Clojure plugin\n\n# Crashlytics plugin (for Android Studio and IntelliJ)\n\n# Editor-based Rest Client\n\n# Android studio 3.1+ serialized cache file\n\n### PyCharm Patch ###\n# Comment Reason: https://github.com/joeblau/gitignore.io/issues/186#issuecomment-215987721\n\n# *.iml\n# modules.xml\n# .idea/misc.xml\n# *.ipr\n\n# Sonarlint plugin\n# https://plugins.jetbrains.com/plugin/7973-sonarlint\n\n# SonarQube Plugin\n# https://plugins.jetbrains.com/plugin/7238-sonarqube-community-plugin\n\n# Markdown Navigator plugin\n# https://plugins.jetbrains.com/plugin/7896-markdown-navigator-enhanced\n\n# Cache file creation bug\n# See https://youtrack.jetbrains.com/issue/JBR-2257\n\n# CodeStream plugin\n# https://plugins.jetbrains.com/plugin/12206-codestream\n\n### PyCharm+all ###\n# Covers JetBrains IDEs: IntelliJ, RubyMine, PhpStorm, AppCode, PyCharm, CLion, Android Studio, WebStorm and Rider\n# Reference: https://intellij-support.jetbrains.com/hc/en-us/articles/206544839\n\n# User-specific stuff\n\n# Generated files\n\n# Sensitive or high-churn files\n\n# Gradle\n\n# Gradle and Maven with auto-import\n# When using Gradle or Maven with auto-import, you should exclude module files,\n# since they will be recreated, and may cause churn.  Uncomment if using\n# auto-import.\n# .idea/artifacts\n# .idea/compiler.xml\n# .idea/jarRepositories.xml\n# .idea/modules.xml\n# .idea/*.iml\n# .idea/modules\n# *.iml\n# *.ipr\n\n# CMake\n\n# Mongo Explorer plugin\n\n# File-based project format\n\n# IntelliJ\n\n# mpeltonen/sbt-idea plugin\n\n# JIRA plugin\n\n# Cursive Clojure plugin\n\n# Crashlytics plugin (for Android Studio and IntelliJ)\n\n# Editor-based Rest Client\n\n# Android studio 3.1+ serialized cache file\n\n### PyCharm+all Patch ###\n# Ignores the whole .idea folder and all .iml files\n# See https://github.com/joeblau/gitignore.io/issues/186 and https://github.com/joeblau/gitignore.io/issues/360\n\n\n# Reason: https://github.com/joeblau/gitignore.io/issues/186#issuecomment-249601023\n\n\n# Sonarlint plugin\n\n### PyCharm+iml ###\n# Covers JetBrains IDEs: IntelliJ, RubyMine, PhpStorm, AppCode, PyCharm, CLion, Android Studio, WebStorm and Rider\n# Reference: https://intellij-support.jetbrains.com/hc/en-us/articles/206544839\n\n# User-specific stuff\n\n# Generated files\n\n# Sensitive or high-churn files\n\n# Gradle\n\n# Gradle and Maven with auto-import\n# When using Gradle or Maven with auto-import, you should exclude module files,\n# since they will be recreated, and may cause churn.  Uncomment if using\n# auto-import.\n# .idea/artifacts\n# .idea/compiler.xml\n# .idea/jarRepositories.xml\n# .idea/modules.xml\n# .idea/*.iml\n# .idea/modules\n# *.iml\n# *.ipr\n\n# CMake\n\n# Mongo Explorer plugin\n\n# File-based project format\n\n# IntelliJ\n\n# mpeltonen/sbt-idea plugin\n\n# JIRA plugin\n\n# Cursive Clojure plugin\n\n# Crashlytics plugin (for Android Studio and IntelliJ)\n\n# Editor-based Rest Client\n\n# Android studio 3.1+ serialized cache file\n\n### PyCharm+iml Patch ###\n# Reason: https://github.com/joeblau/gitignore.io/issues/186#issuecomment-249601023\n\n\n### pydev ###\n**/.pydevproject\n\n### Python ###\n# Byte-compiled / optimized / DLL files\n\n# C extensions\n\n# Distribution / packaging\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n\n# Installer logs\n\n# Unit test / coverage reports\n\n# Translations\n\n# Django stuff:\n\n# Flask stuff:\n\n# Scrapy stuff:\n\n# Sphinx documentation\n\n# PyBuilder\n\n# Jupyter Notebook\n\n# IPython\n\n# pyenv\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow\n\n# Celery stuff\n\n# SageMath parsed files\n\n# Environments\n\n# Spyder project settings\n\n# Rope project settings\n\n# mkdocs documentation\n\n# mypy\n\n# Pyre type checker\n\n# pytype static type analyzer\n\n# profiling data\n\n### R ###\n# History files\n**/.Rhistory\n**/.Rapp.history\n\n# Session Data files\n**/.RData\n\n# User-specific files\n**/.Ruserdata\n\n# Example code in package build process\n**/*-Ex.R\n\n# Output files from R CMD build\n**/*.tar.gz\n\n# Output files from R CMD check\n**/*.Rcheck/\n\n# RStudio files\n**/.Rproj.user/\n\n# produced vignettes\n**/vignettes/*.html\n**/vignettes/*.pdf\n\n# OAuth2 token, see https://github.com/hadley/httr/releases/tag/v0.3\n**/.httr-oauth\n\n# knitr and R markdown default cache directories\n**/*_cache/\n**/cache/\n\n# Temporary files created by R markdown\n**/*.utf8.md\n**/*.knit.md\n\n# R Environment Variables\n**/.Renviron\n\n### R.Bookdown Stack ###\n# R package: bookdown caching files\n**/*_files/\n\n### Rails ###\n**/*.rbc\n**/capybara-*.html\n**/.rspec\n**/db/*.sqlite3\n**/db/*.sqlite3-journal\n**/db/*.sqlite3-[0-9]*\n**/public/system\n**/coverage/\n**/spec/tmp\n**/rerun.txt\n**/pickle-email-*.html\n\n# Ignore all logfiles and tempfiles.\n**/log/*\n**/tmp/*\n**/!/log/.keep\n**/!/tmp/.keep\n\n# TODO Comment out this rule if you are OK with secrets being uploaded to the repo\n**/config/initializers/secret_token.rb\n**/config/master.key\n\n# Only include if you have production secrets in this file, which is no longer a Rails default\n# config/secrets.yml\n\n# dotenv, dotenv-rails\n# TODO Comment out these rules if environment variables can be committed\n**/.env.*\n\n## Environment normalization:\n**/.bundle\n**/vendor/bundle\n\n# these should all be checked in to normalize the environment:\n# Gemfile.lock, .ruby-version, .ruby-gemset\n\n# unless supporting rvm < 1.11.0 or doing something fancy, ignore this:\n**/.rvmrc\n\n# if using bower-rails ignore default bower_components path bower.json files\n**/vendor/assets/bower_components\n**/*.bowerrc\n**/bower.json\n\n# Ignore pow environment settings\n**/.powenv\n\n# Ignore Byebug command history file.\n**/.byebug_history\n\n# Ignore node_modules\n\n# Ignore precompiled javascript packs\n**/public/packs\n**/public/packs-test\n**/public/assets\n\n# Ignore yarn files\n**/yarn-error.log\n\n# Ignore uploaded files in development\n**/storage/*\n**/!/storage/.keep\n\n### react ###\n**/.DS_*\n**/**/*.backup.*\n**/**/*.back.*\n\n\n**/*.sublime*\n\n**/psd\n**/thumb\n**/sketch\n\n### ReactNative ###\n# React Native Stack Base\n\n**/.expo\n**/__generated__\n\n### ReactNative.Android Stack ###\n# Built application files\n**/*.aar\n**/*.ap_\n**/*.aab\n\n# Files for the ART/Dalvik VM\n**/*.dex\n\n# Java class files\n\n# Generated files\n**/gen/\n#  Uncomment the following line in case you need and you don't have the release build type files in your app\n# release/\n\n# Gradle files\n\n# Local configuration file (sdk path, etc)\n\n# Proguard folder generated by Eclipse\n**/proguard/\n\n# Log Files\n\n# Android Studio Navigation editor temp files\n**/.navigation/\n\n# Android Studio captures folder\n**/captures/\n\n# IntelliJ\n**/.idea/workspace.xml\n**/.idea/tasks.xml\n**/.idea/gradle.xml\n**/.idea/assetWizardSettings.xml\n**/.idea/dictionaries\n**/.idea/libraries\n# Android Studio 3 in .gitignore file.\n**/.idea/caches\n**/.idea/modules.xml\n# Comment next line if keeping position of elements in Navigation Editor is relevant for you\n**/.idea/navEditor.xml\n\n# Keystore files\n# Uncomment the following lines if you do not want to check your keystore files in.\n#*.jks\n#*.keystore\n\n# External native build folder generated in Android Studio 2.2 and later\n**/.externalNativeBuild\n**/.cxx/\n\n# Google Services (e.g. APIs or Firebase)\n# google-services.json\n\n# Freeline\n**/freeline.py\n**/freeline/\n**/freeline_project_description.json\n\n# fastlane\n**/fastlane/report.xml\n**/fastlane/Preview.html\n**/fastlane/screenshots\n**/fastlane/test_output\n**/fastlane/readme.md\n\n# Version control\n**/vcs.xml\n\n# lint\n**/lint/intermediates/\n**/lint/generated/\n**/lint/outputs/\n**/lint/tmp/\n# lint/reports/\n\n### ReactNative.Buck Stack ###\n**/buck-out/\n**/.buckconfig.local\n**/.buckd/\n**/.buckversion\n**/.fakebuckversion\n\n### ReactNative.Gradle Stack ###\n**/.gradle\n\n# Ignore Gradle GUI config\n**/gradle-app.setting\n\n# Avoid ignoring Gradle wrapper jar file (.jar files are usually ignored)\n**/!gradle-wrapper.jar\n\n# Cache of project\n**/.gradletasknamecache\n\n# # Work around https://youtrack.jetbrains.com/issue/IDEA-116898\n# gradle/wrapper/gradle-wrapper.properties\n\n### ReactNative.Linux Stack ###\n\n# temporary files which can be created if a process still has a handle open of a deleted file\n\n# KDE directory preferences\n\n# Linux trash folder which might appear on any partition or disk\n\n# .nfs files are created when an open file is removed but is still being accessed\n\n### ReactNative.Node Stack ###\n# Logs\n\n# Diagnostic reports (https://nodejs.org/api/report.html)\n\n# Runtime data\n\n# Directory for instrumented libs generated by jscoverage/JSCover\n\n# Coverage directory used by tools like istanbul\n\n# nyc test coverage\n\n# Grunt intermediate storage (https://gruntjs.com/creating-plugins#storing-task-files)\n\n# Bower dependency directory (https://bower.io/)\n\n# node-waf configuration\n\n# Compiled binary addons (https://nodejs.org/api/addons.html)\n\n# Dependency directories\n\n# TypeScript v1 declaration files\n\n# TypeScript cache\n\n# Optional npm cache directory\n\n# Optional eslint cache\n\n# Microbundle cache\n\n# Optional REPL history\n\n# Output of 'npm pack'\n\n# Yarn Integrity file\n\n# dotenv environment variables file\n\n# parcel-bundler cache (https://parceljs.org/)\n\n# Next.js build output\n\n# Nuxt.js build / generate output\n\n# Gatsby files\n# Comment in the public line in if your project uses Gatsby and not Next.js\n# https://nextjs.org/blog/next-9-1#public-directory-support\n# public\n\n# vuepress build output\n\n# Serverless directories\n\n# FuseBox cache\n\n# DynamoDB Local files\n\n# TernJS port file\n\n# Stores VSCode versions used for testing VSCode extensions\n\n### ReactNative.Xcode Stack ###\n# Xcode\n#\n# gitignore contributors: remember to update Global/Xcode.gitignore, Objective-C.gitignore & Swift.gitignore\n\n## User settings\n**/xcuserdata/\n\n## compatibility with Xcode 8 and earlier (ignoring not required starting Xcode 9)\n**/*.xcscmblueprint\n**/*.xccheckout\n\n## compatibility with Xcode 3 and earlier (ignoring not required starting Xcode 4)\n**/DerivedData/\n**/*.moved-aside\n**/*.pbxuser\n**/!default.pbxuser\n**/*.mode1v3\n**/!default.mode1v3\n**/*.mode2v3\n**/!default.mode2v3\n**/*.perspectivev3\n**/!default.perspectivev3\n\n## Gcc Patch\n**/*.gcno\n\n### ReactNative.macOS Stack ###\n# General\n\n# Icon must end with two \\r\n**/Icon\r\r\n\n# Thumbnails\n\n# Files that might appear in the root of a volume\n\n# Directories potentially created on remote AFP share\n\n### Redis ###\n# Ignore redis binary dump (dump.rdb) files\n\n**/*.rdb\n\n### ROOT ###\n# ROOT Home Page : https://root.cern.ch/\n# ROOT Used by Experimental Physicists, not necessarily HEP\n# ROOT based on C++\n\n# Files generated by ROOT, observed with v6.xy\n\n**/*.pcm\n\n\n\n### Ruby ###\n**/.config\n**/InstalledFiles\n**/pkg/\n**/spec/reports/\n**/spec/examples.txt\n**/test/tmp/\n**/test/version_tmp/\n**/tmp/\n\n# Used by dotenv library to load environment variables.\n# .env\n\n# Ignore Byebug command history file.\n\n## Specific to RubyMotion:\n**/.dat*\n**/.repl_history\n**/*.bridgesupport\n**/build-iPhoneOS/\n**/build-iPhoneSimulator/\n\n## Specific to RubyMotion (use of CocoaPods):\n# We recommend against adding the Pods directory to your .gitignore. However\n# you should judge for yourself, the pros and cons are mentioned at:\n# https://guides.cocoapods.org/using/using-cocoapods.html#should-i-check-the-pods-directory-into-source-control\n# vendor/Pods/\n\n## Documentation cache and generated files:\n**/.yardoc/\n**/_yardoc/\n**/doc/\n**/rdoc/\n\n**/.bundle/\n**/lib/bundler/man/\n\n# for a library or gem, you might want to ignore these files since the code is\n# intended to run in multiple environments; otherwise, check them in:\n# Gemfile.lock\n# .ruby-version\n# .ruby-gemset\n\n# unless supporting rvm < 1.11.0 or doing something fancy, ignore this:\n\n# Used by RuboCop. Remote config files pulled in from inherit_from directive.\n# .rubocop-https?--*\n\n### Ruby Patch ###\n# Used by RuboCop. Remote config files pulled in from inherit_from directive.\n# .rubocop-https?--*\n\n### Rust ###\n# Generated by Cargo\n# will have compiled files and executables\n\n# Remove Cargo.lock from gitignore if creating an executable, leave it for libraries\n# More information here https://doc.rust-lang.org/cargo/guide/cargo-toml-vs-cargo-lock.html\n**/Cargo.lock\n\n### SBT ###\n# Simple Build Tool\n# http://www.scala-sbt.org/release/docs/Getting-Started/Directories.html#configuring-version-control\n\n**/dist/*\n**/lib_managed/\n**/src_managed/\n**/project/boot/\n**/project/plugins/project/\n**/.history\n**/.lib/\n\n### Scala ###\n\n### Serverless ###\n# Ignore build directory\n**/.serverless\n\n### Sonar ###\n#Sonar generated dir\n**/.sonar/\n\n### SonarQube ###\n# SonarQube ignore files.\n# https://docs.sonarqube.org/display/SCAN/Analyzing+with+SonarQube+Scanner\n# Sonar Scanner working directories\n**/.sonar/\n**/.scannerwork/\n\n# http://www.sonarlint.org/commandline/\n# SonarLint working directories, configuration files (including credentials)\n**/.sonarlint/\n\n### Spark ###\n**/*#*#\n**/*.#*\n**/*.pyc\n**/*.pyo\n**/.ensime\n**/.ensime_cache/\n**/.ensime_lucene\n**/.generated-mima*\n**/R-unit-tests.log\n**/R/unit-tests.out\n**/R/cran-check.out\n**/R/pkg/vignettes/sparkr-vignettes.html\n**/R/pkg/tests/fulltests/Rplots.pdf\n**/build/*.jar\n**/build/apache-maven*\n**/build/scala*\n**/build/zinc*\n**/cache\n**/checkpoint\n**/conf/*.cmd\n**/conf/*.conf\n**/conf/*.properties\n**/conf/*.sh\n**/conf/*.xml\n**/conf/java-opts\n**/conf/slaves\n**/derby.log\n**/dev/create-release/*final\n**/dev/create-release/*txt\n**/dev/pr-deps/\n**/docs/_site\n**/docs/api\n**/sql/docs\n**/sql/site\n**/lint-r-report.log\n**/log/\n**/logs/\n**/project/build/target/\n**/project/plugins/lib_managed/\n**/project/plugins/project/build.properties\n**/project/plugins/src_managed/\n**/project/plugins/target/\n**/python/lib/pyspark.zip\n**/python/deps\n**/python/test_coverage/coverage_data\n**/python/test_coverage/htmlcov\n**/python/pyspark/python\n**/reports/\n**/scalastyle-on-compile.generated.xml\n**/scalastyle-output.xml\n**/scalastyle.txt\n**/spark-*-bin-*.tgz\n**/spark-tests.log\n**/streaming-tests.log\n**/unit-tests.log\n**/work/\n**/docs/.jekyll-metadata\n\n# For Hive\n**/TempStatsStore/\n**/metastore/\n**/metastore_db/\n**/sql/hive-thriftserver/test_warehouses\n**/warehouse/\n**/spark-warehouse/\n\n# For R session data\n**/.RHistory\n**/*.Rproj\n**/*.Rproj.*\n\n**/.Rproj.user\n\n# For SBT\n**/.jvmopts\n\n\n### Splunk ###\n# gitignore template for Splunk apps\n# documentation: http://docs.splunk.com/Documentation/Splunk/6.2.3/admin/Defaultmetaconf\n\n# Splunk local meta file\n**/local.meta\n\n# Splunk local folder\n**/local\n\n### Spreadsheet ###\n**/*.xlr\n**/*.xls\n**/*.xlsx\n\n### SSH ###\n**/**/.ssh/id_*\n**/**/.ssh/*_id_*\n**/**/.ssh/known_hosts\n\n### SublimeText ###\n# Cache files for Sublime Text\n**/*.tmlanguage.cache\n**/*.tmPreferences.cache\n**/*.stTheme.cache\n\n# Workspace files are user-specific\n**/*.sublime-workspace\n\n# Project files should be checked into the repository, unless a significant\n# proportion of contributors will probably not be using Sublime Text\n# *.sublime-project\n\n# SFTP configuration file\n**/sftp-config.json\n\n# Package control specific files\n**/Package Control.last-run\n**/Package Control.ca-list\n**/Package Control.ca-bundle\n**/Package Control.system-ca-bundle\n**/Package Control.cache/\n**/Package Control.ca-certs/\n**/Package Control.merged-ca-bundle\n**/Package Control.user-ca-bundle\n**/oscrypto-ca-bundle.crt\n**/bh_unicode_properties.cache\n\n# Sublime-github package stores a github token in this file\n# https://packagecontrol.io/packages/sublime-github\n**/GitHub.sublime-settings\n\n### SVN ###\n**/.svn/\n\n### Terraform ###\n# Local .terraform directories\n**/**/.terraform/*\n\n# .tfstate files\n**/*.tfstate\n**/*.tfstate.*\n\n# Crash log files\n\n# Ignore any .tfvars files that are generated automatically for each Terraform run. Most\n# .tfvars files are managed as part of configuration and so should be included in\n# version control.\n# example.tfvars\n\n# Ignore override files as they are usually used to override resources locally and so\n# are not checked in\n**/override.tf\n**/override.tf.json\n**/*_override.tf\n**/*_override.tf.json\n\n# Include override files you do wish to add to version control using negated pattern\n# !example_override.tf\n\n# Include tfplan files to ignore the plan output of command: terraform plan -out=tfplan\n# example: *tfplan*\n\n### Terragrunt ###\n# terragrunt cache directories\n**/**/.terragrunt-cache/*\n\n### TortoiseGit ###\n# Project-level settings\n**/.tgitconfig\n\n### Vagrant ###\n# General\n**/.vagrant/\n\n# Log files (if you are creating logs in debug mode, uncomment this)\n# *.log\n\n### Vagrant Patch ###\n\n### venv ###\n# Virtualenv\n# http://iamzed.com/2009/05/07/a-primer-on-virtualenv/\n**/[Bb]in\n**/[Ii]nclude\n**/[Ll]ib\n**/[Ll]ib64\n**/[Ll]ocal\n**/[Ss]cripts\n**/pyvenv.cfg\n**/pip-selfcheck.json\n\n### VirtualEnv ###\n# Virtualenv\n# http://iamzed.com/2009/05/07/a-primer-on-virtualenv/\n\n### Julia ###\n# Files generated by invoking Julia with --code-coverage\n**/*.jl.cov\n**/*.jl.*.cov\n\n# Files generated by invoking Julia with --track-allocation\n**/*.jl.mem\n\n# System-specific files and directories generated by the BinaryProvider and BinDeps packages\n# They contain absolute paths specific to the host computer, and so should not be committed\n**/deps/deps.jl\n**/deps/build.log\n**/deps/downloads/\n**/deps/usr/\n**/deps/src/\n\n# Build artifacts for creating documentation generated by the Documenter package\n**/docs/build/\n**/docs/site/\n\n# File generated by Pkg, the package manager, based on a corresponding Project.toml\n# It records a fixed state of all packages used by the project. As such, it should not be\n# committed for packages, but should be committed for applications that require a static\n# environment.\n**/Manifest.toml\n\n### VisualStudioCode ###\n\n### VisualStudioCode Patch ###\n# Ignore all local history of files\n**/.ionide\n\n### vs ###\n## Ignore Visual Studio temporary files, build results, and\n## files generated by popular Visual Studio add-ons.\n##\n## Get latest from https://github.com/github/gitignore/blob/master/VisualStudio.gitignore\n\n# User-specific files\n**/*.rsuser\n**/*.suo\n**/*.user\n**/*.userosscache\n**/*.sln.docstates\n\n# User-specific files (MonoDevelop/Xamarin Studio)\n**/*.userprefs\n\n# Mono auto generated files\n**/mono_crash.*\n\n# Build results\n**/[Dd]ebug/\n**/[Dd]ebugPublic/\n**/[Rr]elease/\n**/[Rr]eleases/\n**/x64/\n**/x86/\n**/[Aa][Rr][Mm]/\n**/[Aa][Rr][Mm]64/\n**/bld/\n**/[Bb]in/\n**/[Oo]bj/\n**/[Ll]og/\n**/[Ll]ogs/\n\n# Visual Studio 2015/2017 cache/options directory\n**/.vs/\n# Uncomment if you have tasks that create the project's static files in wwwroot\n#wwwroot/\n\n# Visual Studio 2017 auto generated files\n**/Generated\\ Files/\n\n# MSTest test Results\n**/[Tt]est[Rr]esult*/\n**/[Bb]uild[Ll]og.*\n\n# NUnit\n**/*.VisualState.xml\n**/TestResult.xml\n**/nunit-*.xml\n\n# Build Results of an ATL Project\n**/[Dd]ebugPS/\n**/[Rr]eleasePS/\n**/dlldata.c\n\n# Benchmark Results\n**/BenchmarkDotNet.Artifacts/\n\n# .NET Core\n**/project.lock.json\n**/project.fragment.lock.json\n**/artifacts/\n\n# StyleCop\n**/StyleCopReport.xml\n\n# Files built by Visual Studio\n**/*_i.c\n**/*_p.c\n**/*_h.h\n**/*.meta\n**/*.iobj\n**/*.ipdb\n**/*.pgc\n**/*.pgd\n**/*.rsp\n**/*.sbr\n**/*.tlb\n**/*.tli\n**/*.tlh\n**/*.tmp_proj\n**/*_wpftmp.csproj\n**/*.vspscc\n**/*.vssscc\n**/.builds\n**/*.pidb\n**/*.svclog\n**/*.scc\n\n# Chutzpah Test files\n**/_Chutzpah*\n\n# Visual C++ cache files\n**/ipch/\n**/*.aps\n**/*.ncb\n**/*.opendb\n**/*.opensdf\n**/*.cachefile\n**/*.VC.db\n**/*.VC.VC.opendb\n\n# Visual Studio profiler\n**/*.psess\n**/*.vsp\n**/*.vspx\n**/*.sap\n\n# Visual Studio Trace Files\n**/*.e2e\n\n# TFS 2012 Local Workspace\n**/$tf/\n\n# Guidance Automation Toolkit\n**/*.gpState\n\n# ReSharper is a .NET coding add-in\n**/_ReSharper*/\n**/*.[Rr]e[Ss]harper\n**/*.DotSettings.user\n\n# TeamCity is a build add-in\n**/_TeamCity*\n\n# DotCover is a Code Coverage Tool\n**/*.dotCover\n\n# AxoCover is a Code Coverage Tool\n**/.axoCover/*\n**/!.axoCover/settings.json\n\n# Coverlet is a free, cross platform Code Coverage Tool\n**/coverage*[.json, .xml, .info]\n\n# Visual Studio code coverage results\n**/*.coverage\n**/*.coveragexml\n\n# NCrunch\n**/_NCrunch_*\n**/.*crunch*.local.xml\n**/nCrunchTemp_*\n\n# MightyMoose\n**/*.mm.*\n**/AutoTest.Net/\n\n# Web workbench (sass)\n**/.sass-cache/\n\n# Installshield output folder\n**/[Ee]xpress/\n\n# DocProject is a documentation generator add-in\n**/DocProject/buildhelp/\n**/DocProject/Help/*.HxT\n**/DocProject/Help/*.HxC\n**/DocProject/Help/*.hhc\n**/DocProject/Help/*.hhk\n**/DocProject/Help/*.hhp\n**/DocProject/Help/Html2\n**/DocProject/Help/html\n\n# Click-Once directory\n**/publish/\n\n# Publish Web Output\n**/*.[Pp]ublish.xml\n**/*.azurePubxml\n# Note: Comment the next line if you want to checkin your web deploy settings,\n# but database connection strings (with potential passwords) will be unencrypted\n**/*.pubxml\n**/*.publishproj\n\n# Microsoft Azure Web App publish settings. Comment the next line if you want to\n# checkin your Azure Web App publish settings, but sensitive information contained\n# in these scripts will be unencrypted\n**/PublishScripts/\n\n# NuGet Packages\n**/*.nupkg\n# NuGet Symbol Packages\n**/*.snupkg\n# The packages folder can be ignored because of Package Restore\n**/**/[Pp]ackages/*\n# except build/, which is used as an MSBuild target.\n**/!**/[Pp]ackages/build/\n# Uncomment if necessary however generally it will be regenerated when needed\n#!**/[Pp]ackages/repositories.config\n# NuGet v3's project.json files produces more ignorable files\n**/*.nuget.props\n**/*.nuget.targets\n\n# Microsoft Azure Build Output\n**/csx/\n**/*.build.csdef\n\n# Microsoft Azure Emulator\n**/ecf/\n**/rcf/\n\n# Windows Store app package directories and files\n**/AppPackages/\n**/BundleArtifacts/\n**/Package.StoreAssociation.xml\n**/_pkginfo.txt\n**/*.appx\n**/*.appxbundle\n**/*.appxupload\n\n# Visual Studio cache files\n# files ending in .cache can be ignored\n**/*.[Cc]ache\n# but keep track of directories ending in .cache\n**/!?*.[Cc]ache/\n\n# Others\n**/ClientBin/\n**/~$*\n**/*.dbmdl\n**/*.dbproj.schemaview\n**/*.jfm\n**/*.pfx\n**/*.publishsettings\n**/orleans.codegen.cs\n\n# Including strong name files can present a security risk\n# (https://github.com/github/gitignore/pull/2483#issue-259490424)\n#*.snk\n\n# Since there are multiple workflows, uncomment next line to ignore bower_components\n# (https://github.com/github/gitignore/pull/1529#issuecomment-104372622)\n#bower_components/\n\n# RIA/Silverlight projects\n**/Generated_Code/\n\n# Backup & report files from converting an old project file\n# to a newer Visual Studio version. Backup files are not needed,\n# because we have git ;-)\n**/_UpgradeReport_Files/\n**/Backup*/\n**/UpgradeLog*.XML\n**/UpgradeLog*.htm\n**/ServiceFabricBackup/\n**/*.rptproj.bak\n\n# SQL Server files\n**/*.mdf\n**/*.ldf\n**/*.ndf\n\n# Business Intelligence projects\n**/*.rdl.data\n**/*.bim.layout\n**/*.bim_*.settings\n**/*.rptproj.rsuser\n**/*- [Bb]ackup.rdl\n**/*- [Bb]ackup ([0-9]).rdl\n**/*- [Bb]ackup ([0-9][0-9]).rdl\n\n# Microsoft Fakes\n**/FakesAssemblies/\n\n# GhostDoc plugin setting file\n**/*.GhostDoc.xml\n\n# Node.js Tools for Visual Studio\n**/.ntvs_analysis.dat\n\n# Visual Studio 6 build log\n**/*.plg\n\n# Visual Studio 6 workspace options file\n**/*.opt\n\n# Visual Studio 6 auto-generated workspace file (contains which files were open etc.)\n**/*.vbw\n\n# Visual Studio LightSwitch build output\n**/**/*.HTMLClient/GeneratedArtifacts\n**/**/*.DesktopClient/GeneratedArtifacts\n**/**/*.DesktopClient/ModelManifest.xml\n**/**/*.Server/GeneratedArtifacts\n**/**/*.Server/ModelManifest.xml\n**/_Pvt_Extensions\n\n# Paket dependency manager\n**/.paket/paket.exe\n**/paket-files/\n\n# FAKE - F# Make\n**/.fake/\n\n# CodeRush personal settings\n**/.cr/personal\n\n# Python Tools for Visual Studio (PTVS)\n\n# Cake - Uncomment if you are using it\n# tools/**\n# !tools/packages.config\n\n# Tabs Studio\n**/*.tss\n\n# Telerik's JustMock configuration file\n**/*.jmconfig\n\n# BizTalk build output\n**/*.btp.cs\n**/*.btm.cs\n**/*.odx.cs\n**/*.xsd.cs\n\n# OpenCover UI analysis results\n**/OpenCover/\n\n# Azure Stream Analytics local run output\n**/ASALocalRun/\n\n# MSBuild Binary and Structured Log\n**/*.binlog\n\n# NVidia Nsight GPU debugger configuration file\n**/*.nvuser\n\n# MFractors (Xamarin productivity tool) working folder\n**/.mfractor/\n\n# Local History for Visual Studio\n**/.localhistory/\n\n# BeatPulse healthcheck temp database\n**/healthchecksdb\n\n# Backup folder for Package Reference Convert tool in Visual Studio 2017\n**/MigrationBackup/\n\n# Ionide (cross platform F# VS Code tools) working folder\n**/.ionide/\n\n### vscode ###\n\n### Vue ###\n# gitignore template for Vue.js projects\n# Recommended template: Node.gitignore\n\n# TODO: where does this rule come from?\n**/docs/_book\n\n# TODO: where does this rule come from?\n**/test/\n\n### Vuejs ###\n# Recommended template: Node.gitignore\n\n**/npm-debug.log\n**/yarn-error.log\n\n### Waf ###\n# For projects that use the Waf build system: https://waf.io/\n# Dot-hidden on Unix-like systems\n**/.waf-*-*/\n**/.waf3-*-*/\n# Hidden directory on Windows (no dot)\n**/waf-*-*/\n**/waf3-*-*/\n# Lockfile\n**/.lock-waf_*_build\n\n### Windows ###\n# Windows thumbnail cache files\n**/Thumbs.db\n**/Thumbs.db:encryptable\n**/ehthumbs.db\n**/ehthumbs_vista.db\n\n# Dump file\n**/*.stackdump\n\n# Folder config file\n**/[Dd]esktop.ini\n\n# Recycle Bin used on file shares\n**/$RECYCLE.BIN/\n\n# Windows Installer files\n**/*.msix\n\n# Windows shortcuts\n**/*.lnk\n\n### Xcode ###\n# Xcode\n# gitignore contributors: remember to update Global/Xcode.gitignore, Objective-C.gitignore & Swift.gitignore\n\n\n\n\n\n### Xcode Patch ###\n**/*.xcodeproj/*\n**/!*.xcodeproj/project.pbxproj\n**/!*.xcodeproj/xcshareddata/\n**/!*.xcworkspace/contents.xcworkspacedata\n**/**/xcshareddata/WorkspaceSettings.xcsettings\n\n### XcodeInjection ###\n# Code Injection\n# After new code Injection tools there's a generated folder /iOSInjectionProject\n# https://github.com/johnno1962/injectionforxcode\n\n**/iOSInjectionProject/\n\n### Gradle ###\n\n# Ignore Gradle GUI config\n\n# Avoid ignoring Gradle wrapper jar file (.jar files are usually ignored)\n\n# Cache of project\n\n# # Work around https://youtrack.jetbrains.com/issue/IDEA-116898\n# gradle/wrapper/gradle-wrapper.properties\n\n### Gradle Patch ###\n**/**/build/\n\n### VisualStudio ###\n\n# User-specific files\n\n# User-specific files (MonoDevelop/Xamarin Studio)\n\n# Mono auto generated files\n\n# Build results\n\n# Visual Studio 2015/2017 cache/options directory\n# Uncomment if you have tasks that create the project's static files in wwwroot\n\n# Visual Studio 2017 auto generated files\n\n# MSTest test Results\n\n# NUnit\n\n# Build Results of an ATL Project\n\n# Benchmark Results\n\n# .NET Core\n\n# StyleCop\n\n# Files built by Visual Studio\n\n# Chutzpah Test files\n\n# Visual C++ cache files\n\n# Visual Studio profiler\n\n# Visual Studio Trace Files\n\n# TFS 2012 Local Workspace\n\n# Guidance Automation Toolkit\n\n# ReSharper is a .NET coding add-in\n\n# TeamCity is a build add-in\n\n# DotCover is a Code Coverage Tool\n\n# AxoCover is a Code Coverage Tool\n\n# Coverlet is a free, cross platform Code Coverage Tool\n\n# Visual Studio code coverage results\n\n# NCrunch\n\n# MightyMoose\n\n# Web workbench (sass)\n\n# Installshield output folder\n\n# DocProject is a documentation generator add-in\n\n# Click-Once directory\n\n# Publish Web Output\n# Note: Comment the next line if you want to checkin your web deploy settings,\n# but database connection strings (with potential passwords) will be unencrypted\n\n# Microsoft Azure Web App publish settings. Comment the next line if you want to\n# checkin your Azure Web App publish settings, but sensitive information contained\n# in these scripts will be unencrypted\n\n# NuGet Packages\n# NuGet Symbol Packages\n# The packages folder can be ignored because of Package Restore\n# except build/, which is used as an MSBuild target.\n# Uncomment if necessary however generally it will be regenerated when needed\n# NuGet v3's project.json files produces more ignorable files\n\n# Microsoft Azure Build Output\n\n# Microsoft Azure Emulator\n\n# Windows Store app package directories and files\n\n# Visual Studio cache files\n# files ending in .cache can be ignored\n# but keep track of directories ending in .cache\n\n# Others\n\n# Including strong name files can present a security risk\n# (https://github.com/github/gitignore/pull/2483#issue-259490424)\n\n# Since there are multiple workflows, uncomment next line to ignore bower_components\n# (https://github.com/github/gitignore/pull/1529#issuecomment-104372622)\n\n# RIA/Silverlight projects\n\n# Backup & report files from converting an old project file\n# to a newer Visual Studio version. Backup files are not needed,\n# because we have git ;-)\n\n# SQL Server files\n\n# Business Intelligence projects\n\n# Microsoft Fakes\n\n# GhostDoc plugin setting file\n\n# Node.js Tools for Visual Studio\n\n# Visual Studio 6 build log\n\n# Visual Studio 6 workspace options file\n\n# Visual Studio 6 auto-generated workspace file (contains which files were open etc.)\n\n# Visual Studio LightSwitch build output\n\n# Paket dependency manager\n\n# FAKE - F# Make\n\n# CodeRush personal settings\n\n# Python Tools for Visual Studio (PTVS)\n\n# Cake - Uncomment if you are using it\n# tools/**\n# !tools/packages.config\n\n# Tabs Studio\n\n# Telerik's JustMock configuration file\n\n# BizTalk build output\n\n# OpenCover UI analysis results\n\n# Azure Stream Analytics local run output\n\n# MSBuild Binary and Structured Log\n\n# NVidia Nsight GPU debugger configuration file\n\n# MFractors (Xamarin productivity tool) working folder\n\n# Local History for Visual Studio\n\n# BeatPulse healthcheck temp database\n\n# Backup folder for Package Reference Convert tool in Visual Studio 2017\n\n# Ionide (cross platform F# VS Code tools) working folder\n\n# End of https://www.toptal.com/developers/gitignore/api/ansible,apachehadoop,appcode,appengine,archive,archives,archlinuxpackages,audio,autotools,backup,basic,bittorrent,c,c++,certificates,chefcookbook,clojure,cloud9,cmake,code,code-java,codeblocks,compressed,compressedarchive,compression,data,database,datarecovery,diff,direnv,diskimage,docfx,docpress,docz,dotenv,dotfilessh,dotsettings,dropbox,eclipse,emacs,erlang,executable,firebase,flask,git,gitbook,go,gpg,gradle,grails,groovy,grunt,haskell,helm,homebrew,hugo,images,intellij,intellij+all,intellij+iml,java,java-web,jenv,jetbrains,jetbrains+all,jetbrains+iml,jmeter,julia,jupyternotebooks,kotlin,lamp,latex,less,linux,lua,macos,matlab,maven,mercurial,microsoftoffice,node,octave,osx,packer,patch,perl,perl6,phpunit,powershell,puppet,putty,pycharm,pycharm+all,pycharm+iml,pydev,python,r,rails,react,reactnative,redis,root,ruby,rust,sbt,scala,serverless,sonar,sonarqube,spark,splunk,spreadsheet,ssh,sublimetext,svn,terraform,terragrunt,tortoisegit,vagrant,venv,virtualenv,visualstudio,visualstudiocode,vs,vscode,vue,vuejs,waf,windows,xcode,xcodeinjection,zsh\n"
        },
        {
          "name": ".drone.yml",
          "type": "blob",
          "size": 1.15234375,
          "content": "---\n# XXX: putting this separator further down with code causes a parsing bug in drone lint\n#\n#  Author: Hari Sekhon\n#  Date: 2020-02-29 12:05:52 +0000 (Sat, 29 Feb 2020)\n#\n#  vim:ts=2:sts=2:sw=2:et\n#\n#  https://github.com/HariSekhon/DevOps-Bash-tools\n#\n#  License: see accompanying Hari Sekhon LICENSE file\n#\n#  If you're using my code you're welcome to connect with me on LinkedIn and optionally send me feedback to help steer this or other code I publish\n#\n#  https://www.linkedin.com/in/HariSekhon\n#\n\n# ============================================================================ #\n#                                D r o n e   C I\n# ============================================================================ #\n\n# https://docs.drone.io/quickstart/cli/\n#\n# https://docs.drone.io/cli/install/\n#\n# brew install drone-cli\n#\n# cd to this directory\n#\n# drone exec [--pipeline default] [--include=thisstep] [--exclude=thatstep]\n\nkind: pipeline\ntype: docker\nname: default\n\nsteps:\n  - name: build\n    image: ubuntu:18.04\n    #environment:\n    #  DEBUG: 1\n    commands:\n      - setup/ci_bootstrap.sh\n      - make init\n      - make ci\n      - make test\n\ntrigger:\n  branch:\n    - master\n"
        },
        {
          "name": ".editorconfig",
          "type": "blob",
          "size": 1.9072265625,
          "content": "#  vim:ts=4:sts=4:sw=4:et\n#\n#  Author: Hari Sekhon\n#  Date: 2015-10-31 19:04:34 +0000 (Sat, 31 Oct 2015)\n#\n#  https://github.com/HariSekhon/DevOps-Bash-tools\n#\n#  License: see accompanying Hari Sekhon LICENSE file\n#\n#  If you're using my code you're welcome to connect with me on LinkedIn and optionally send me feedback\n#  to help improve or steer this or other code I publish\n#\n#  https://www.linkedin.com/in/HariSekhon\n#\n\n# http://EditorConfig.org\n\n# stop recursing upwards for other .editorconfig files\nroot = true\n\n# Unix-style newlines with a newline ending every file\n[*]\nindent_size = 4\nindent_style = space\nend_of_line = lf\ntrim_trailing_whitespace = true\ninsert_final_newline = true\n\n[*.go]\nindent_size = 4\nindent_style = tab\nend_of_line = lf\ntrim_trailing_whitespace = true\ninsert_final_newline = true\n\n[Makefile]\nindent_size = 4\nindent_style = tab\nend_of_line = lf\ntrim_trailing_whitespace = true\ninsert_final_newline = true\n\n[{*.md,*.hcl,*.tf,*.tfvars}]\nindent_size = 2\nindent_style = space\nend_of_line = lf\ntrim_trailing_whitespace = true\ninsert_final_newline = true\n\n[*.yml,*.yaml]\nindent_size = 2\nindent_style = space\nend_of_line = lf\ntrim_trailing_whitespace = true\ninsert_final_newline = true\n\n[.*]\nindent_size = 4\nindent_style = space\nend_of_line = lf\ntrim_trailing_whitespace = true\ninsert_final_newline = true\n\n# ============================================================================ #\n#                  Older Stuff, don't think I use this any more\n# ============================================================================ #\n\n# Matches multiple files with brace expansion notation\n# Set default charset\n#[*.{js,py}]\n#charset = utf-8\n\n# Indentation override for all JS under lib directory\n#[lib/**.js]\n#indent_style = space\n#indent_size = 2\n\n# Matches the exact files either package.json or .travis.yml\n#[{package.json,.travis.yml}]\n#indent_style = space\n#indent_size = 2\n\n#[*.xml]\n#indent_style = space\n#indent_size = 2\n"
        },
        {
          "name": ".envrc",
          "type": "blob",
          "size": 6.9462890625,
          "content": "#!/usr/bin/env bash\n#  vim:ts=4:sts=4:sw=4:et\n#\n#  Author: Hari Sekhon\n#  Date: Mon Feb 22 17:42:01 2021 +0000\n#\n#  https://github.com/HariSekhon/DevOps-Bash-tools\n#\n#  License: see accompanying Hari Sekhon LICENSE file\n#\n#  If you're using my code you're welcome to connect with me on LinkedIn and optionally send me feedback to help steer this or other code I publish\n#\n#  https://www.linkedin.com/in/HariSekhon\n#\n\n# ============================================================================ #\n#                                  D i r E n v\n# ============================================================================ #\n\n# https://direnv.net/man/direnv-stdlib.1.html\n\n# See Also:\n#\n#   .envrc-aws\n#   .envrc-gcp\n#   .envrc-kubernetes\n\n# direnv stdlib - loads .envrc from parent dir up to /\n#\n# useful to accumulate parent and child directory .envrc settings eg. adding Kubernetes namespace, ArgoCD app etc.\n#\n# bypasses security authorization though - use with care\n#source_up\n#\n# source_up must be loaded before set -u otherwise gets this error:\n#\n#   direnv: loading .envrc\n#   /bin/bash: line 226: $1: unbound variable\n#\n# source_up causes this error is up .envrc is found in parent directories:\n#\n#   direnv: No ancestor .envrc found\n\nset -euo pipefail\n[ -n \"${DEBUG:-}\" ] && set -x\nsrc=\"$(readlink -f \"${BASH_SOURCE[0]}\")\"\nsrcdir=\"$(cd \"$(dirname \"$src\")\" && pwd)\"\n\n# ============================================================================ #\n#                              P r e - C o m m i t\n# ============================================================================ #\n\n# Automatically install Pre-Commit Git hooks if not already present\n\nif ! type -P pre-commit &>/dev/null; then\n    if uname -s | grep -q Darwin &&\n       type -P brew &>/dev/null; then\n        echo\n        echo \"Pre-commit is not installed - installing now using Homebrew...\"\n        echo\n        brew install pre-commit\n        echo\n    elif type -P pip &>/dev/null; then\n        echo\n        echo \"Pre-commit is not installed - installing now using Pip...\"\n        echo\n        pip install pre-commit\n    fi\nfi\n\nif [ -f .pre-commit-config.yaml ] &&\n   type -P pre-commit &>/dev/null &&\n   git rev-parse --is-inside-work-tree &>/dev/null; then\n    if ! [ -f \"$(git rev-parse --show-toplevel)/.git/hooks/pre-commit\" ]; then\n        echo\n        echo \"Pre-commit hook is not installed in local Git repo checkout - installing now...\"\n        echo\n        pre-commit install\n    fi\nfi\n\n# ============================================================================ #\n#                          D o c k e r   C o m p o s e\n# ============================================================================ #\n\nexport COMPOSE_PROJECT_NAME=\"bash-tools\"\n\n# ============================================================================ #\n#                                  G i t H u b\n# ============================================================================ #\n\n#export GITHUB_ORGANIZATION=HariSekhon\n\n# ============================================================================ #\n#                                 A n s i b l e\n# ============================================================================ #\n\n# use the local repo's ansible.cfg rather than:\n#\n#   $PWD/ansible.cfg\n#   ~/.ansible.cfg\n#   /etc/ansible/ansible.cfg\n#\n# set this in project repos to ensure user environment ANSIBLE_CONFIG doesn't get used\n#export ANSIBLE_CONFIG=\"/path/to/ansible.cfg\"\n\n# ============================================================================ #\n#                              C l o u d f l a r e\n# ============================================================================ #\n\n#export CLOUDFLARE_EMAIL=hari@...\n#export CLOUDFLARE_API_KEY=...  # generate here: https://dash.cloudflare.com/profile/api-tokens\n#export CLOUDFLARE_TOKEN=...    # used by cloudflare_api.sh but not by terraform module\n\n# export the variables for terraform\n#export TF_VAR_cloudflare_email=\"$CLOUDFLARE_EMAIL\"\n#export TF_VAR_cloudflare_api_key=\"$CLOUDFLARE_API_KEY\"  # must be a key, not a token using the link above\n\n# ============================================================================ #\n#                   Load External Envrc Files If Present\n# ============================================================================ #\n\n# XXX: safer to bring all these external .envrc inline if you're worried about changes\n#      to it bypassing 'direnv allow' authorization\nload_if_exists(){\n    # first arg is a path to a .envrc\n    # all other args are passed to the sourcing of .envrc - used by .envrc-kubernetes\n    # to pass the context name 'docker-desktop' to switch to\n    local envrc=\"$1\"\n    shift\n    if ! [[ \"$envrc\" =~ ^/ ]]; then\n        envrc=\"$srcdir/$envrc\"\n    fi\n    if [ -f \"$envrc\" ]; then\n        # prevent looping on symlinks to this .envrc if given\n        if [ \"$(readlink \"$envrc\")\" = \"$src\" ]; then\n            return\n        fi\n        echo\n        echo \"Loading $envrc\"\n        # shellcheck disable=SC1090,SC1091\n        . \"$envrc\" \"$@\"\n    fi\n}\n\n# don't do this it may lead to an infinite loop if 'make link' symlinking ~/.envrc to this repo's .envrc\n# (which I do to keep Python virtual automatically loaded at all times because recent pip on Python refuses\n# to install to system Python)\n#load_if_exists ~/.envrc\n\n# ============================================================================ #\n#                                  P y t h o n\n# ============================================================================ #\n\n    #.envrc-aws \\\n    #.envrc-gcp \\\n    #.envrc-terraform \\\n# shellcheck disable=SC2043\nfor envrc in \\\n    .envrc-python \\\n    ; do\n    load_if_exists \"$envrc\"\ndone\n\n# ============================================================================ #\n#                                     A W S\n# ============================================================================ #\n\nif [[ \"$PWD\" =~ /aws/ ]]; then\n    load_if_exists .envrc-aws\nfi\n\n# ============================================================================ #\n#                                     G C P\n# ============================================================================ #\n\nif [[ \"$PWD\" =~ /gcp/ ]]; then\n    load_if_exists .envrc-gcp\nfi\n\n# ============================================================================ #\n#                               T e r r a f o r m\n# ============================================================================ #\n\nif [[ \"$PWD\" =~ /(terra(form)?|tf)(/|$) ]]; then\n    load_if_exists .envrc-terraform\nfi\n\n# ============================================================================ #\n#                              K u b e r n e t e s\n# ============================================================================ #\n\nif [ -f \"$srcdir/.envrc-kubernetes\" ]; then\n    load_if_exists .envrc-kubernetes docker-desktop\nfi\n\n# ============================================================================ #\n#                                    . E n v\n# ============================================================================ #\n\necho\n# read .env too\n#dotenv\n\nload_if_exists .envrc.local\n"
        },
        {
          "name": ".envrc-aws",
          "type": "blob",
          "size": 6.3515625,
          "content": "#!/usr/bin/env bash\n#  vim:ts=4:sts=4:sw=4:et\n#\n#  Author: Hari Sekhon\n#  Date: 2021-07-27 12:42:32 +0100 (Tue, 27 Jul 2021)\n#\n#  https://github.com/HariSekhon/DevOps-Bash-tools\n#\n#  License: see accompanying Hari Sekhon LICENSE file\n#\n#  If you're using my code you're welcome to connect with me on LinkedIn and optionally send me feedback to help steer this or other code I publish\n#\n#  https://www.linkedin.com/in/HariSekhon\n#\n\n# ============================================================================ #\n#                              A W S   D i r E n v\n# ============================================================================ #\n\n# https://direnv.net/man/direnv-stdlib.1.html\n\n# See Also:\n#\n#   .envrc\n#   .envrc-gcp\n#   .envrc-kubernetes\n\n# direnv stdlib - loads .envrc from parent dir up to /\n#\n# useful to accumulate parent and child directory .envrc settings eg. adding Kubernetes namespace, ArgoCD app etc.\n#\n# bypasses security authorization though - use with care\n#source_up\n#\n# source_up must be loaded before set -u otherwise gets this error:\n#\n#   direnv: loading .envrc\n#   /bin/bash: line 226: $1: unbound variable\n\nset -euo pipefail\n[ -n \"${DEBUG:-}\" ] && set -x\nsrcdir=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\"\n\nif [ -n \"${CI:-}\" ]; then\n    exit 0\nfi\n\n# XXX: Edit - crucial to set to the right environment, the rest of the inferred settings below depend on this\nif [ -z \"${AWS_PROFILE:-}\" ]; then\n    exit 0\nfi\n\naws configure list 2>/dev/null || :\necho\n\n# If not logged in:\n#\n# - and we know the AWS_PROFILE\n# - and AWS_NO_AUTOLOGIN is not set\n# - check for SSO key in config section for this profile\n# - if found then do an automatic 'aws sso login'\n#\nif ! aws sts get-caller-identity --output table; then\n    if [ -n \"${AWS_PROFILE:-}\" ] &&\n       [ -z \"${AWS_NO_AUTOLOGIN:-}\" ]; then\n        # assumes you're not putting a blank line until the next section block\n        #if sed -n \"/profile.*$AWS_PROFILE/,/^[[:space:]]*$/p\" ~/.aws/config | grep -q sso_start_url; then\n        # goes until the next [profile ...] section instead, should be more reliable\n        if sed -n \"/profile.*$AWS_PROFILE/,/^[[:space:]]*\\[.+\\]/p\" ~/.aws/config | grep -q sso_start_url; then\n            echo\n            aws sso login\n        fi\n    fi\nfi\necho\n\n# 'aws sts get-caller-identity --query Account' succeeds in returning the account id\n# from the ~/.aws/config even if 'aws sso login' has expired\nAWS_ACCOUNT_ID=\"$(\n    aws sts get-caller-identity --query Account --output text ||\n    aws configure get sso_account_id ||\n    :\n)\"\necho \"AWS Account ID: $AWS_ACCOUNT_ID\"\nexport AWS_ACCOUNT_ID\necho\n\n# might not have permissions to the Organizations in which case this will error instead of return\nAWS_ACCOUNT=\"$(aws organizations describe-account --account-id \"$AWS_ACCOUNT_ID\" 2>/dev/null)\"\nif [ -n \"$AWS_ACCOUNT\" ]; then\n    echo \"AWS Account: $AWS_ACCOUNT\"\n    export AWS_ACCOUNT\n    echo\nfi\n\nAWS_DEFAULT_REGION=\"$(aws configure get region || :)\"  # use region configured in profile by default\nAWS_DEFAULT_REGION=\"${AWS_DEFAULT_REGION:-eu-west-1}\"  # XXX: Edit default fallback region\nexport AWS_DEFAULT_REGION\necho \"AWS Region: $AWS_DEFAULT_REGION\"\necho\n\nexport AWS_DEFAULT_OUTPUT=json\n\n# XXX: Edit, or remove if only have 1 cluster in account, will auto-determine below\nexport EKS_CLUSTER=\"mycluster\"\n\n# safer but slower\n#eks_clusters=()\n#while IFS='' read -r line; do\n#    eks_clusters+=(\"$line\")\n##done < <(aws eks list-clusters --output=json | jq -r '.clusters[]')\n#done < <(aws eks list-clusters --query 'clusters[]' --output text)\n#if [ \"${#eks_clusters[@]}\" -eq 1 ]; then\n#    export EKS_CLUSTER=\"${eks_clusters[*]}\"\n#fi\n\neks_clusters=\"$(\n    aws eks list-clusters --query 'clusters' --output text |\n    tr '[:space:]' '\\n' |\n    sed '/^[[:space:]]*$/d'\n)\"\n\nif [ -n \"$eks_clusters\" ]; then\n    num_eks_clusters=\"$(grep -c . <<< \"$eks_clusters\")\"\n    echo \"EKS Clusters ($num_eks_clusters):\"\n    echo\n    echo \"$eks_clusters\"\n    echo\n    # If EKS_CLUSTER isn't set and there is only one EKS cluster in this account and region, then use it\n    if [ -z \"${EKS_CLUSTER:-}\" ]; then\n        if [ \"$num_eks_clusters\" = 1 ]; then\n            EKS_CLUSTER=\"$eks_clusters\"\n        fi\n    fi\nelse\n    num_eks_clusters=0\nfi\n\nif [ -n \"${EKS_CLUSTER:-}\" ]; then\n    # kubectl context is easily created by running adjacent aws_kube_creds.sh script first\n    export EKS_CONTEXT=\"arn:aws:eks:$AWS_DEFAULT_REGION:$AWS_ACCOUNT_ID:cluster/$EKS_CLUSTER\"\n\n    if command -v kubectl &>/dev/null; then\n        if ! kubectl config get-clusters | grep -Fxq \"$EKS_CONTEXT\"; then\n            echo \"EKS Cluster '$EKS_CLUSTER' not configured, configuring now\"\n            aws eks update-kubeconfig --name \"$EKS_CLUSTER\"\n            echo\n        fi\n    fi\n\n    # XXX: safer to inline .envrc-kubernetes if you're worried about changes to it bypassing 'direnv allow' authorization\n    # shellcheck disable=SC1090,SC1091\n    . \"$srcdir/.envrc-kubernetes\" \"$EKS_CONTEXT\" ${EKS_NAMESPACE:+\"$EKS_NAMESPACE\"}\nfi\n\nif [ \"$num_eks_clusters\" = 1 ]; then\n    if grep -q '^[[:space:]]*export[[:space:]]*EKS_CLUSTER' .envrc &&\n     ! grep -q \"^export EKS_CLUSTER=$eks_clusters$\" .envrc; then\n        echo\n        echo \"Updating EKS_CLUSTER in .envrc from:\"\n        echo\n        grep '^[[:space:]]*export[[:space:]]*EKS_CLUSTER' .envrc\n        echo\n        echo \"to\"\n        echo\n        echo \"export EKS_CLUSTER=$eks_clusters\"\n        echo\n        perl -pi -e \"s/^\\\\s*export\\s+EKS_CLUSTER=.*/export EKS_CLUSTER=$eks_clusters/\" .envrc\n        echo\n    fi\nfi\n\n# better to load this dynamically from credentials, using functions in .bash.d/aws.sh\n#export AWS_ACCESS_KEY_ID=...\n#export AWS_SECRET_ACCESS_KEY=...\n#export AWS_SESSION_TOKEN=...\n\n#export AWS_CONFIG_FILE=~/.aws/config\n#export AWS_SHARED_CREDENTIALS_FILE=~/.aws/credentials\n#export AWS_MAX_ATTEMPTS=3\n\n# to quickly export prefixed AWS environment keys if they exist for simple overrides, see examples below\naws_access_key_env(){\n    env=\"$1\"\n    for key in AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY; do\n        varname=\"${env}_${key}\"\n        if [ -n \"${!varname:-}\" ]; then\n            export \"$key\"=\"${!varname}\"\n        fi\n    done\n}\n\n#aws_access_key_env \"DEV\"\n#aws_access_key_env \"STAGING\"\n#aws_access_key_env \"PROD\"\n#aws_access_key_env \"MGMT\"\n\n# pull the secret using this command whenever you need it:\n#\n#   aws_secret_get.sh \"$JENKINS_ADMIN_PASSWORD_AWS_SECRET\" | copy_to_clipboard.sh\n#\nexport JENKINS_ADMIN_PASSWORD_AWS_SECRET=\"jenkins-admin-password\"\n"
        },
        {
          "name": ".envrc-gcp",
          "type": "blob",
          "size": 5.4765625,
          "content": "#!/usr/bin/env bash\n#  vim:ts=4:sts=4:sw=4:et\n#\n#  Author: Hari Sekhon\n#  Date: Mon Feb 22 17:42:01 2021 +0000\n#\n#  https://github.com/HariSekhon/DevOps-Bash-tools\n#\n#  License: see accompanying Hari Sekhon LICENSE file\n#\n#  If you're using my code you're welcome to connect with me on LinkedIn and optionally send me feedback to help steer this or other code I publish\n#\n#  https://www.linkedin.com/in/HariSekhon\n#\n\n# ============================================================================ #\n#                              G C P   D i r E n v\n# ============================================================================ #\n\n# https://direnv.net/man/direnv-stdlib.1.html\n\n# See Also:\n#\n#   .envrc\n#   .envrc-aws\n#   .envrc-kubernetes\n\n# direnv stdlib - loads .envrc from parent dir up to /\n#\n# useful to accumulate parent and child directory .envrc settings eg. adding Kubernetes namespace, ArgoCD app etc.\n#\n# bypasses security authorization though - use with care\n#source_up\n#\n# source_up must be loaded before set -u otherwise gets this error:\n#\n#   direnv: loading .envrc\n#   /bin/bash: line 226: $1: unbound variable\n\nset -euo pipefail\n[ -n \"${DEBUG:-}\" ] && set -x\nsrcdir=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\"\n\nif [ -n \"${CI:-}\" ]; then\n    exit 0\nfi\n\n# https://cloud.google.com/sdk/gcloud/reference/config\n\n# If using other services, infer the environment variables to put below by reading:\n#\n#   gcloud topic configurations\n#       or\n#   gcloud config set --help\n\ngcloud_config(){\n    local config=\"${1:-}\"\n    if [ -z \"$config\" ]; then\n        echo \"no config passed to gcloud_config() function\" >&2\n        return 1\n    fi\n    if [ -z \"${CI:-}\" ]; then\n        return\n    fi\n    # don't waste time if not using GCloud SDK, ie. not found in $PATH\n    if type -P gcloud; then\n        # protect from setting this if the config does exist as this can cause auth problems by unsetting the core.account\n        if gcloud config configurations list --format='get(name)' | grep -q \"^$config$\"; then\n            export CLOUDSDK_ACTIVE_CONFIG_NAME=\"$config\"\n        fi\n    fi\n}\n\n#gcloud_config dev\n#gcloud_config staging\n#gcloud_config production\n\n# XXX: Edit\nexport CLOUDSDK_CORE_PROJECT=myproject\n\necho \"CLOUDSDK_CORE_PROJECT=$CLOUDSDK_CORE_PROJECT\"\necho\n\n# XXX: Edit\nexport CLOUDSDK_COMPUTE_REGION=\"${CLOUDSDK_COMPUTE_REGION:-eu-west-2}\"\n\necho \"CLOUDSDK_COMPUTE_REGION=$CLOUDSDK_COMPUTE_REGION\"\necho\n\nREGION=\"$CLOUDSDK_COMPUTE_REGION\"\n\n# you should probably not set CLOUDSDK_COMPUTE_ZONE\n#\n# 'gcloud compute ssh' will auto-determine the zone\n#\n# setting CLOUDSDK_COMPUTE_ZONE explicitly breaks the above command in 2/3 cases due to a VM being in a different zone:\n#\n#    ERROR: (gcloud.compute.ssh) Could not fetch resource:\n#     - The resource 'projects/<MY_PROJECT>/zones/<ZONE>/instances/<VM_NAME>' was not found\n#\n# gcp/gce_ssh.sh script in this repo can work around that if you do set this\n#\n#export CLOUDSDK_COMPUTE_ZONE=\"${REGION}-a\" # or b or c\n\nexport CLOUDSDK_AI_REGION=\"$REGION\"\nexport CLOUDSDK_AI_PLATFORM_REGION=\"$REGION\"\nexport CLOUDSDK_DATAPROC_REGION=\"$REGION\"\nexport CLOUDSDK_DEPLOY_REGION=\"$REGION\"\nexport CLOUDSDK_FILESTORE_REGION=\"$REGION\"\nexport CLOUDSDK_FUNCTIONS_REGION=\"$REGION\"\nexport CLOUDSDK_MEMCACHE_REGION=\"$REGION\"\nexport CLOUDSDK_REDIS_REGION=\"$REGION\"\nexport CLOUDSDK_RUN_REGION=\"$REGION\"\nexport CLOUDSDK_RUN_CLUSTER_LOCATION=\"$REGION\"\nexport CLOUDSDK_VMWARE_REGION=\"$REGION\"\n\n# XXX: Edit\nexport CLOUDSDK_RUN_PLATFORM=managed\n#export CLOUDSDK_RUN_PLATFORM=gke\n#export CLOUDSDK_RUN_PLATFORM=kubernetes\n#export CLOUDSDK_RUN_CLUSTER=mycluster\n\nexport CLOUDSDK_GCLOUDIGNORE_ENABLED=True\n#export CLOUDSDK_BUILDS_USE_KANIKO=True\n\n# XXX: Edit, or remove if only have 1 cluster in project, will auto-determine below\nexport CLOUDSDK_CONTAINER_CLUSTER=mycluster  # GKE cluster name\n\n# safer but slower\n#gke_clusters=()\n#while IFS='' read -r line; do\n#    gke_clusters+=(\"$line\")\n#done < <(gcloud container clusters list --format='get(name)')\n#if [ \"${#gke_clusters[@]}\" -eq 1 ]; then\n#    export CLOUDSDK_CONTAINER_CLUSTER=\"${gke_clusters[*]}\"\n#fi\n\ngke_clusters=\"$(\n    gcloud container clusters list --format='get(name)' |\n    sed '/^[[:space:]]*$/d'\n)\"\n\nif [ -n \"$gke_clusters\" ]; then\n    num_gke_clusters=\"$(grep -c . <<< \"$gke_clusters\")\"\n    echo \"GKE Clusters ($num_gke_clusters):\"\n    echo\n    echo \"$gke_clusters\"\n    echo\n    # If GKE_CLUSTER isn't set and there is only one GKE cluster in this account and region, then use it\n    if [ -z \"${GKE_CLUSTER:-}\" ]; then\n        if [ \"$num_gke_clusters\" = 1 ]; then\n            GKE_CLUSTER=\"$gke_clusters\"\n        fi\n    fi\nelse\n    num_gke_clusters=0\nfi\n\n# alternatively call gke_kube_context() function in .envrc-kubernetes which will do this\n# and comment out auto-running kube_context() on sourcing .envrc-kubernetes\nif [ -n \"${CLOUDSDK_CONTAINER_CLUSTER:-}\" ]; then\n    echo \"CLOUDSDK_CONTAINER_CLUSTER=$CLOUDSDK_CONTAINER_CLUSTER\"\n    echo\n    # kubectl context is easily created by running adjacent aws_kube_creds.sh script first\n\n    export GKE_CONTEXT=\"gke_${CLOUDSDK_CORE_PROJECT}_${CLOUDSDK_COMPUTE_REGION}_${CLOUDSDK_CONTAINER_CLUSTER}\"\n\n    # XXX: safer to inline .envrc-kubernetes if you're worried about changes to it bypassing 'direnv allow' authorization\n    # shellcheck disable=SC1090,SC1091\n    . \"$srcdir/.envrc-kubernetes\" \"$GKE_CONTEXT\" ${GKE_NAMESPACE:+\"$GKE_NAMESPACE\"}\nfi\n\n# pull the secret using this command whenever you need it:\n#\n#   gcp_secret_get.sh \"$JENKINS_ADMIN_PASSWORD_GCP_SECRET\" | copy_to_clipboard.sh\n#\nexport JENKINS_ADMIN_PASSWORD_GCP_SECRET=\"jenkins-admin-password\"\n"
        },
        {
          "name": ".envrc-kubernetes",
          "type": "blob",
          "size": 6.0439453125,
          "content": "#!/usr/bin/env bash\n#  vim:ts=4:sts=4:sw=4:et\n#\n#  Author: Hari Sekhon\n#  Date: 2021-02-25 10:10:53 +0000 (Thu, 25 Feb 2021)\n#\n#  https://github.com/HariSekhon/DevOps-Bash-tools\n#\n#  License: see accompanying Hari Sekhon LICENSE file\n#\n#  If you're using my code you're welcome to connect with me on LinkedIn and optionally send me feedback to help steer this or other code I publish\n#\n#  https://www.linkedin.com/in/HariSekhon\n#\n\n# ============================================================================ #\n#                       K u b e r n e t e s   D i r E n v\n# ============================================================================ #\n\n# https://direnv.net/man/direnv-stdlib.1.html\n\n# See Also:\n#\n#   .envrc\n#   .envrc-aws\n#   .envrc-gcp\n\n# direnv stdlib - loads .envrc from parent dir up to /\n#\n# useful to accumulate parent and child directory .envrc settings eg. adding Kubernetes namespace, ArgoCD app etc.\n#\n# bypasses security authorization though - use with care\n#source_up\n#\n# source_up must be loaded before set -u otherwise gets this error:\n#\n#   direnv: loading .envrc\n#   /bin/bash: line 226: $1: unbound variable\n\nset -euo pipefail\n[ -n \"${DEBUG:-}\" ] && set -x\n\narg=\"${1:-}\"\nif [ \"${arg##*/}\" = \"${BASH_SOURCE[0]##*/}\" ]; then\n    shift\nfi\n\n# XXX: Edit this - hardcode for localized convenience\nCONTEXT=\"${1:-docker-desktop}\"\n# if set will also set the namespace for extra convenience\nNAMESPACE=\"${2:-}\"\n#NAMESPACE=\"jenkins\"\n\n# function so can place in topdir .envrc and have subdirs 'source_up' or simply . ../.envrc to reuse this code among many .envrc environments\nkube_context(){\n    local context=\"$1\"\n    local namespace=\"${2:-}\"\n    if command -v kubectl &>/dev/null; then\n        local tmpdir=\"/tmp/.kube\"\n\n        mkdir -pv \"$tmpdir\"\n\n        local default_kubeconfig=\"${HOME:-$(cd ~ && pwd)}/.kube/config\"\n        local original_kubeconfig=\"${KUBECONFIG:-$default_kubeconfig}\"\n\n        # reload safety - do not source from new tmpdir - not necessary for direnv but useful for local sourcing tests\n        #if [[ \"$original_kubeconfig\" =~ $tmpdir ]]; then\n        #    echo \"ignoring \\$KUBECONFIG=$original_kubeconfig, using default home location $default_kubeconfig\"\n        #    original_kubeconfig=\"$default_kubeconfig\"\n        #fi\n\n        # isolate the kubernetes context to avoid a race condition affecting any other shells or scripts\n        # epoch is added because $$ and $PPID are direnv sub-processes and may be reused later, so using epoch to add uniqueness\n        local epoch\n        epoch=\"$(date +%s)\"\n        export KUBECONFIG=\"$tmpdir/config.${EUID:-${UID:-$(id -u)}}.$$.$epoch\"\n\n        # load your real kube config to isolated staging area to source the context info\n        local src_kubeconfig=\"\"\n        local kubeconfig_source_locations=\"\n            $original_kubeconfig\n            $default_kubeconfig\n            $PWD/.kube/config\n            /etc/rancher/k3s/k3s.yaml\"\n        for kubeconfig in $kubeconfig_source_locations; do\n            if [ -f \"$kubeconfig\" ]; then\n                src_kubeconfig=\"$kubeconfig\"\n                break\n            fi\n        done\n        if [ -n \"$src_kubeconfig\" ]; then\n            if [ \"$src_kubeconfig\" != \"$KUBECONFIG\" ]; then\n                cp -f -- \"$src_kubeconfig\" \"$KUBECONFIG\"\n            fi\n        else\n            if [[ \"$PWD\" =~ k8|kube ]]; then\n                echo \"WARNING: failed to find one of:\" >&2\n                echo \"$kubeconfig_source_locations\" | sort -u >&2\n                echo >&2\n            fi\n        fi\n\n        # race condition - 'kubectl config get-contexts' fails to find the context and switch in many runs without this sleep\n        context_found=0\n        local i\n        for ((i=0; i < 5; i++)); do\n            # surprisingly unreliable - if kubectl config get-contexts -o name | grep -Fxq \"$context\" can miss even after these succeed\n            #if [ -s \"$KUBECONFIG\" ]; then\n            #if cmp --quiet \"$from_kubeconfig\" \"$KUBECONFIG\"; then\n            if kubectl config get-contexts -o name | grep -Fxq \"$context\"; then\n                context_found=1\n                break\n            fi\n            sleep 0.1\n        done\n\n        # this randomly misses the context, and not even 'sync; sync; sleep 1' is reliable to stop that happening in testing\n        #if kubectl config get-contexts -o name 2>/dev/null | grep -Fxq \"$context\"; then\n        if [ \"$context_found\" = 1 ]; then\n            kubectl config use-context \"$CONTEXT\"\n            echo\n\n            if [ -n \"${namespace:-}\" ]; then\n                kubectl config set-context \"$context\" --namespace \"$namespace\"\n                echo\n            fi\n        fi\n    fi\n}\n\ngke_kube_context(){\n    local CONTEXT\n    for _ in CLOUDSDK_CORE_PROJECT CLOUDSDK_COMPUTE_REGION CLOUDSDK_CONTAINER_CLUSTER; do\n        if [ -z \"${!_}\" ]; then\n            echo \"WARNING: \\$$_ is not set\" >&2\n        fi\n    done\n    # if CLOUDSDK_CONTAINER_CLUSTER and it's generated as a naming convention such as \"${CLOUDSDK_CORE_PROJECT}-${CLOUDSDK_COMPUTE_REGION}\"\n    #export CLOUDSDK_CONTAINER_CLUSTER=\"${CLOUDSDK_CONTAINER_CLUSTER:-${CLOUDSDK_CORE_PROJECT}-${CLOUDSDK_COMPUTE_REGION}}\"\n\n    # the context naming convention for GKE clusters imported via:\n    #\n    #   gcloud container clusters get-credentials \"$cluster\" --zone \"$zone\"\n    #\n    # use gke_kube_creds.sh to auto-populate this for all GKE clusters in the current project\n    # and gcp_foreach_project.sh to do this for all GCP projects. Both scripts are found here:\n    #\n    #   https://github.com/HariSekhon/DevOps-Bash-tools\n    #\n    # should be using a regional cluster\n    CONTEXT=\"gke_${CLOUDSDK_CORE_PROJECT}_${CLOUDSDK_COMPUTE_REGION}_${CLOUDSDK_CONTAINER_CLUSTER}\"\n    # not a zonal cluster\n    #CONTEXT=\"gke_${CLOUDSDK_CORE_PROJECT}_${CLOUDSDK_COMPUTE_ZONE}_${CLOUDSDK_CONTAINER_CLUSTER}\"\n    kube_context \"$CONTEXT\" \"${NAMESPACE:-}\"\n}\n\nkube_context \"$CONTEXT\" \"$NAMESPACE\"\n\n#export ARGOCD_SERVER=\"argocd.mycompany.com\"\n#export ARGOCD_OPTS=\"${ARGOCD_OPTS:-} --grpc-web\"\n#if [ -n \"${ARGOCD_AUTH_TOKEN_MYCOMPANY_OR_ENV:-}\" ]; then\n#    export ARGOCD_AUTH_TOKEN=\"$ARGOCD_AUTH_TOKEN_MYCOMPANY_OR_ENV\"\n#fi\n#export ARGOCD_APP=\"myapp\"\n"
        },
        {
          "name": ".envrc-python",
          "type": "blob",
          "size": 1.3056640625,
          "content": "#!/usr/bin/env bash\n#  vim:ts=4:sts=4:sw=4:et\n#\n#  Author: Hari Sekhon\n#  Date: Mon Feb 22 17:42:01 2021 +0000\n#\n#  https://github.com/HariSekhon/DevOps-Bash-tools\n#\n#  License: see accompanying Hari Sekhon LICENSE file\n#\n#  If you're using my code you're welcome to connect with me on LinkedIn and optionally send me feedback to help steer this or other code I publish\n#\n#  https://www.linkedin.com/in/HariSekhon\n#\n\n# ============================================================================ #\n#                           P y t h o n   D i r E n v\n# ============================================================================ #\n\n# .envrc to auto-load the virtualenv inside the 'venv' directory if present\n\n# https://direnv.net/man/direnv-stdlib.1.html\n\nset -euo pipefail\n[ -n \"${DEBUG:-}\" ] && set -x\n#srcdir=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\"\n\n# this is necessary because newer versions of pip no longer allow you to install PyPI packages in system-packages by default\nfor venv in \"$PWD/venv\" \"$HOME/venv\"; do\n    if [ -f \"$venv/bin/activate\" ]; then\n        echo\n        echo \"Virtualenv directory found in: $venv\"\n        echo\n        echo \"Activating Virtualenv inside the directory: $venv\"\n\n        # shellcheck disable=SC1091\n        source \"$venv/bin/activate\"\n        break\n    fi\ndone\n\n# read .env too\n#dotenv\n"
        },
        {
          "name": ".envrc-terraform",
          "type": "blob",
          "size": 2.4248046875,
          "content": "#!/usr/bin/env bash\n#  vim:ts=4:sts=4:sw=4:et\n#\n#  Author: Hari Sekhon\n#  Date: Mon Feb 22 17:42:01 2021 +0000\n#\n#  https://github.com/HariSekhon/DevOps-Bash-tools\n#\n#  License: see accompanying Hari Sekhon LICENSE file\n#\n#  If you're using my code you're welcome to connect with me on LinkedIn and optionally send me feedback to help steer this or other code I publish\n#\n#  https://www.linkedin.com/in/HariSekhon\n#\n\n# ============================================================================ #\n#                        T e r r a f o r m   D i r E n v\n# ============================================================================ #\n\nset -euo pipefail\n[ -n \"${DEBUG:-}\" ] && set -x\n#srcdir=\"$(dirname \"${BASH_SOURCE[0]}\")\"\n\n# XXX: beware that tfenv and tgswitch replace the terraform / terragrunt in the path and this is a race condition between different shells\n#\t   it is not as good as the KUBECONFIG trick done in the .envrc-kubernetes\n\n# would auto-determine the version from the state file, but this commands seems to always return the version of your local binary pulling the state file, not the version from the actual terraform_version field in state file if you see when opening it up in the cloud bucket\n# terraform state pull | jq -r .terraform_version\nexport TERRAFORM_VERSION=1.1.9\n\n# overrides .terraform-version file to make this single source of truth\nexport TFENV_TERRAFORM_VERSION=\"$TERRAFORM_VERSION\"\nexport TFENV_AUTO_INSTALL=true\n\n# Terragrunt\nexport TG_VERSION=0.39.2\n\n# if tgswitch is installed, trigger it to use the above TF_VERSION environment variable and switch to the correct version of Terragrunt\n# better than adding the ugly shell hook from the docs - https://github.com/warrensbox/tgswitch?tab=readme-ov-file#get-the-version-from-a-subdirectory\nif type -P tgswitch &>/dev/null; then\n    tgswitch\nfi\n\n# XXX: set these or other variables for Terraform code to find\nexport CLOUDFLARE_EMAIL=hari@...\nexport CLOUDFLARE_API_KEY=...  # generate here: https://dash.cloudflare.com/profile/api-tokens\n#export CLOUDFLARE_TOKEN=...   # used by cloudflare_api.sh but not by terraform module\n\n# export the variables for terraform\nexport TF_VAR_cloudflare_email=\"$CLOUDFLARE_EMAIL\"\nexport TF_VAR_cloudflare_api_key=\"$CLOUDFLARE_API_KEY\"  # must be a key, not a token using the link above\n\n# GITHUB_* environment variables may interfere with GitHub provider, so unset them\nfor env_var in $(env | awk -F= '$1 ~ /GITHUB/ {print $1}'); do\n    unset \"$env_var\"\ndone\n"
        },
        {
          "name": ".git-templates",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitconfig",
          "type": "blob",
          "size": 9.7685546875,
          "content": "#\n#  Author: Hari Sekhon\n#  Date: 2012-01-31 14:08:42 +0000 (Tue, 31 Jan 2012)\n#\n#  vim:ts=4:sts=4:sw=4:et\n\n# configure your user name and email in ~/.gitconfig.local\n#[user]\n#    name = Hari Sekhon\n#    email = harisekhon@gmail.com\n\n[include]\n  path = .gitconfig.local\n\n[core]\n    # detects filemode changes\n    filemode = true\n    # defaults to $VISUAL or $EDITOR which is set in .bash.d/env.sh\n    #editor = vim\n    # global .gitignore\n    excludesfile = ~/.gitignore\n    # stops unicode chars coming out as \\xxx and double quoted filenames in git status (used in .bash.d/git.sh git_rm_untracked function)\n    quotePath = false\n\n    #whitespace = trailing-space,space-before-tab\n\n[pull]\n    rebase = false\n\n[push]\n#   default = current\n    default = simple\n\n[alias]\n    name = config --get user.name\n    email = config --get user.email\n    who = !git config -l | grep -E '^user\\\\.(name|email)'\n    whoami = who\n    co  = checkout\n    ci  = commit\n    p   = push\n    st  = status\n    stq = !git_foreach_repo.sh git status | grep --color=no -e \"=======\" -e branch -e GitHub\n    br  = branch\n    ba  = branch -a\n    bav = branch -a -vvv\n    cp  = cherry-pick\n    ls  = ls-files\n    root = rev-parse --show-toplevel\n\n    rem      = remote -v\n    remotes  = remote -v\n    tags     = tag -l\n    branches = branch -a\n    prune-branches = ! git remote prune origin && git branch -vv | cut -c 3- | awk '$4 ~ /gone\\\\]/ {print $1}' | xargs git branch -d\n\n    # the results with multi-origin remotes are not reliable without a git pull first, even when only pulling from the primary GitHub origin\n    in      = ! git pull && git log HEAD..FETCH_HEAD\n    out     = ! git pull && git log FETCH_HEAD..HEAD\n    inp     = ! git pull && git log -p HEAD..FETCH_HEAD\n    outp    = ! git pull && git log -p FETCH_HEAD..HEAD\n    ind     = ! git pull && git diff HEAD..FETCH_HEAD\n    outd    = ! git pull && git diff FETCH_HEAD..HEAD\n    age     = for-each-ref --format '%(authordate:iso) %(refname:short)' --sort=-authordate refs/remotes refs/heads\n\n    unstage = reset HEAD --\n    last = log -1 HEAD\n\n    # Show files ignored by git:\n    ign = ls-files -o -i --exclude-standard\n    ignored = !git clean -ndX | sed -e 's/^Would remove //' | sed 's/^Would skip repository //'\n    untracked = ls-files --others --exclude-standard\n\n    # how to use commands inside git aliases\n    visual = !gitk\n\n    df = diff\n    dc = diff --cached\n    lg = log -p\n    lol = log --graph --decorate --pretty=oneline --abbrev-commit\n    lola = log --graph --decorate --pretty=oneline --abbrev-commit --all\n\n    # avoid diff-so-fancy so we can create patches\n    patch = !git --no-pager diff --no-color\n\n    #ffm     = merge --ff-only\n    #ffp     = pull --ff-only\n    #fp      = fetch --prune\n\n    #mp      = merge --no-commit --no-ff\n    #ma      = merge --abort\n    #dno     = diff --name-only\n    #gone    = !git branch -vv | grep ': gone'\n    #gd      = !git branch -vv | awk '/: gone/ {print $1}' | xargs --no-run-if-empty -n1 git branch -D\n\n[help]\n    # autocorrects git commands and executes the inferred command\n    # dangerous this just autocorrected my git rename to git rebase, lucky it errored out...\n    autocorrect = 0\n\n[homebrew]\n    donationmessage = false\n\n# ============================================================================ #\n#                               G i t   C o l o r\n# ============================================================================ #\n\n# colors: normal, black, red, green, yellow, blue, magenta, cyan, white\n# effects: bold, dim, ul, blink and reverse\n\n# if 2 colours given - 1st is foreground, 2nd is background\n\n[color]\n    ui     = auto\n    diff   = auto\n    grep   = auto\n    # do not set always here, use:\n    # -c color.status=always\n    # for specific overrides, otherwise may break gitci and related functions\n    status = auto\n    branch = auto\n\n[color \"branch\"]\n    current     = green ul\n    local       = yellow\n    remote      = red        # default\n    plain       = white\n\n[color \"diff\"]\n    new         = green\n    old         = red        # default\n    plain       = white\n    whitespace  = yellow reverse\n    func        = yellow\n    #frag        = cyan       # default\n    #meta        = green bold # default\n    # from diff-so-fancy\n    meta = 11\n    frag = magenta bold\n\n[color \"grep\"]\n    context     = white\n    filename    = cyan\n    function    = yellow\n    linenumber  = white\n    match       = white magenta\n    selected    = green\n    separator   = white\n\n[color \"status\"]\n    added       = white blue\n    changed     = magenta\n    untracked   = cyan\n    branch      = magenta blink\n    nobranch    = red blink\n\n# ============================================================================ #\n#                           D i f f - s o - f a n c y\n# ============================================================================ #\n\n# detected if installed and set via $GIT_PAGER in .bash.d/git.sh\n#[core]\n#   pager = diff-so-fancy | less --tabs=4 -RFX\n#[pager]\n    # don't set --pattern, overrides -F and doesn't quit less automatically for short diffs\n    #diff = diff-so-fancy | less --tabs=4 -RFX --pattern '^(Date|added|deleted|modified): '\n    #diff = diff-so-fancy | less --tabs=4 -RFX\n\n    # truncate lines in less, only for 'git blame'\n    #blame = less -S\n\n\n[color \"diff-highlight\"]\n  #oldNormal = red bold\n  oldHighlight = white red  # black doesn't contrast well with red bg, use white\n  #newNormal = green bold\n  newHighlight = white magenta\n\n[diff-so-fancy]\n  stripLeadingSymbols = false\n\n# diff-so-fancy but I prefer most of my old preferences\n#[color \"diff\"]\n#  meta = 11\n#  frag = magenta bold\n#  commit = yellow bold\n#  old = red bold\n#  new = green bold\n#  whitespace = red reverse\n\n# ============================================================================ #\n\n#[difftool \"sourcetree\"]\n#    cmd = opendiff \\\"$LOCAL\\\" \\\"$REMOTE\\\"\n#    path =\n#\n#[mergetool \"sourcetree\"]\n#    cmd = /Applications/SourceTree.app/Contents/Resources/opendiff-w.sh \\\"$LOCAL\\\" \\\"$REMOTE\\\" -ancestor \\\"$BASE\\\" -merge \\\"$MERGED\\\"\n#    trustExitCode = true\n\n#[filter \"media\"]\n#   clean = git media clean %f\n#   smudge = git media smudge %f\n#   required = true\n\n# ============================================================================ #\n#                   C r e d e n t i a l s   H e l p e r s\n# ============================================================================ #\n\n# more specific credential addresses below take priority, so if you want to paste in a GitHub token\n# you will need to comment out the GitHub credential help sections below\n[credential]\n    helper = store\n\n# When prompted, enter your username and PAT token, not password, otherwise you'll get this error:\n#\n# remote: Support for password authentication was removed on August 13, 2021.\n# remote: Please see https://docs.github.com/get-started/getting-started-with-git/about-remote-repositories#cloning-with-https-urls for information on currently recommended modes of authentication.\n\n# generated by git/git_remotes_set_https_creds_helpers.sh\n\n[credential \"https://github.com\"]\n    helper = \"!f() { sleep 1; echo \\\"username=${GITHUB_USER}\\\"; echo \\\"password=${GH_TOKEN:-${GITHUB_TOKEN}}\\\"; }; f\"\n\n[credential \"https://gist.github.com\"]\n    helper = \"!f() { sleep 1; echo \\\"username=${GITHUB_USER}\\\"; echo \\\"password=${GH_TOKEN:-${GITHUB_TOKEN}}\\\"; }; f\"\n\n[credential \"https://gitlab.com\"]\n    helper = \"!f() { sleep 1; echo \\\"password=${GITLAB_TOKEN}\\\"; }; f\"\n\n[credential \"https://bitbucket.org\"]\n    helper = \"!f() { sleep 1; echo \\\"password=${BITBUCKET_TOKEN}\\\"; }; f\"\n\n[credential \"https://dev.azure.com\"]\n    helper = \"!f() { sleep 1; echo \\\"password=${AZURE_DEVOPS_TOKEN}\\\"; }; f\"\n\n# ============================================================================ #\n#                             A W S   S e c r e t s\n# ============================================================================ #\n\n# AWS Secrets prevents committing secrets in to Git\n\n# install git-secrets hooks in any repo initialized or cloned to prevent credential leak\n[init]\n    templateDir = ~/.git-templates/git-secrets\n\n[secrets]\n    providers = git secrets --aws-provider\n    patterns = (A3T[A-Z0-9]|AKIA|AGPA|AIDA|AROA|AIPA|ANPA|ANVA|ASIA)[A-Z0-9]{16}\n    patterns = (\\\"|')?(AWS|aws|Aws)?_?(SECRET|secret|Secret)?_?(ACCESS|access|Access)?_?(KEY|key|Key)(\\\"|')?\\\\s*(:|=>|=)\\\\s*(\\\"|')?[A-Za-z0-9/\\\\+=]{40}(\\\"|')?\n    patterns = (\\\"|')?(AWS|aws|Aws)?_?(ACCOUNT|account|Account)_?(ID|id|Id)?(\\\"|')?\\\\s*(:|=>|=)\\\\s*(\\\"|')?[0-9]{4}\\\\-?[0-9]{4}\\\\-?[0-9]{4}(\\\"|')?\n    # doesn't work, doesn't support (?! ) negative lookahead regex\n    #patterns = (\\bhari|sekhon\\b)(!.*@gmail.com)\n    # only applies to contents, not metadata to prevent wrong author commits\n    #patterns = hari|sekhon\n    # These are sample keys so ignore false positives from scanning tools\n    # trivy:ignore:aws-access-key-id\n    allowed = AKIAIOSFODNN7EXAMPLE\n    # trivy:ignore:aws-access-key-id\n    allowed = wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\n    # better to add to private repos .git/config only\n    #allowed = AWS_ACCOUNT_ID\n\n# ============================================================================ #\n#                 G C P   S o u r c e   R e p o s i t o r i e s\n# ============================================================================ #\n\n# GCloud SDK command clone and sets up the repo auth:\n#\n#   gcloud source repos clone \"$repo\" --project=\"$project\"\n#\n# Remote origin:\n#\n#   https://source.developers.google.com/p/$project/r/$repo\n#\n# GCloud SDK adds this to .git/config in a repo cloned via:\n#\n# # having a blank helper before the real help prevents this error when pushing:\n# # bad input: ..........\n#[credential \"https://source.developers.google.com/\"]\n#    helper =\n#    helper = !gcloud auth git-helper --account=hari@<project>.iam.gserviceaccount.com --ignore-unknown $@\n[filter \"lfs\"]\n    clean = git-lfs clean -- %f\n    smudge = git-lfs smudge -- %f\n    process = git-lfs filter-process\n    required = true\n"
        },
        {
          "name": ".gitconfig.local",
          "type": "blob",
          "size": 0.1416015625,
          "content": "#\n#  Author: Hari Sekhon\n#  Date: 2012-01-31 14:08:42 +0000 (Tue, 31 Jan 2012)\n#\n\n[user]\n    name = Hari Sekhon\n    email = harisekhon@gmail.com\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 84.859375,
          "content": "#\n#  Author: Hari Sekhon\n#  Date: 2012-06-24 22:03:34 +0000 (Sun, 24 Jun 2012)\n#\n#  vim: filetype=conf\n#\n\n# ============================================================================ #\n#                              . g i t i g n o r e\n# ============================================================================ #\n\n# https://git-scm.com/docs/gitignore\n\n# ===================================================================\n# Ignores any file in any subdirectory with matching basename or path\n#\n# (can use **/filename but not necessary)\n\n# ===================================================================\n# Once in a while you should check which files have been ignored via:\n#\n#       git status --ignored\n#\n# to check that there aren't some legit files you need to commit\n\n# =================================================================\n# To find out which line is causing a given file to be ignored, do:\n#\n#       git check-ignore -v $filename\n\n*#*#\n*.a\n*.avi\n*.bak\n*.bak.*\n*.bin\n*.bkp\n*.bz2\n*.class\n*.dump\n*.flv\n*.gif\n*.gpg\n*.gz\n*.img\n*.jar\n*.jks\n*.jpeg\n*.jpg\n*.kdb\n*.lock\n*.log\n*.lzo\n*.macports-saved_*\n*.mp3\n*.mp4\n*.mpeg\n*.mpg\n*.o\n*.orig\n*.out\n*.p12\n*.part\n*.pyc\n*.pyo\n*.stderr\n*.stdout\n*.swo\n*.swp\n*.tar\n*.tbz2\n*.tgz\n*.tmp\n*.wmv\n*.zip\n*~\n~*\n\n.a\n.activator/\n.adobe\n.agent.env\n.aliaslists\n.android/\n.anyconnect\n.ApacheDirectoryStudio/\n.atftp_history\n.atom/\n.audacious/.thumbs\n.audacious/accels\n.audacious/log\n.audacious/playlist.xspf\n.awless/\n# AWS Access + Secret keys are stored in here\n.aws/credentials\n.aws/config\n.aws/token\n# SSO Access + Secret Keys & STS are stored in .json files in here\n.aws/cli/cache/\n.aws/shell/\n.bashrc_dynamichosts\n.bashrc_dynamichosts.src\n.bash_history\n.bash_sessions\n.bash_vars\n.boot2docker/\n.cache\n.cassandra/\n.cassandra/cqlshrc\n.cassandra/cqlsh_history\n.cassandra/nodetool_history\n.cbq_history\n.ccm/\n.CFUserTextEncoding\n# Codefresh config contains API Key\n.cfconfig\n.circleci/build_agent_settings.json\n# Circle CI API Token stored in here\n.circleci/cli.yml\n.circleci/update_check.yml\n.Codefresh/agent/\n.compiz\n.conda/\n#.config\n# contains repo_token / COVERALLS_REPO_TOKEN\n.coveralls.yml\n.cpan/build/\n.cpan/FTPstats.yml\n.cpan/histfile\n.cpan/Metadata\n.cpan/sources/\n.cpanm/\n.cups/\n.data\n.dbshell\n.dbus\n.dcos/\n.DCOPserver_*\n.devcenter/.metadata/\n.devcenter/DevCenter/.default/\n.devcenter/DevCenter/.metadata/\n.devcenter/logging/\n.docker_vars\n.docker/machine/\n.dropbox/\n.DS_Store\n.dvdcss/\n# contains things like auth tokens\n#.envrc*\n.erlang.cookie\n.evolution\n.fluxbox/backgrounds\n.fluxbox/BEST\n.fluxbox/best-styles/styles-backup\n.fluxbox/DIVISION2\n.fluxbox/fbrun_history\n.fluxbox/lastwallpaper\n.fluxbox/menu\n.fluxbox/NOBG\n.fluxbox/OTHERTHEMES\n.fluxbox/startup.log\n# contains Concourse bearer auth token\n.flyrc\n# contains GCP credentials for Ruby Fog library (like Boto)\n.fog\n.fontconfig\n.fseventsd\n.gaim/accels\n.gaim/icons\n.gaim/status.xml\n.gconf\n.gconfd\n.gem\n.gimp-*\n.github_actions_runner/\n.gitk\n.gmvault/\n.gnome\n.gnome2\n.gnome2_private\n.gnupg/gpg-agent-info-*\n.gnupg/private-keys-*\n.gnuplot_history\n.gpg-agent.env\n.gradle\n.groovy\n.gstreamer-*\n.gvfs\n.hg\n#.htoprc\n.ICEauthority\n.IdeaIC*\n.idea\n.inkscape-etc/\n.ion3\n.ipython/\n.irb-save-history\n.irb_history\n# contains creds\n.iredisrc\n.ivy2/\n.jline-jython.history\n.kde\n.kodos\n.kube/\n.ldapvi_history\n.lesshst\n.links2\n.local\n.m2/\n.macports/\n.macromedia\n.matplotlib/\n.mcop\n.mcoprc\n.minikube/\n.minishift/\n.minishift.env\n.mozilla\n.mtools/\n.mysql_history\n.nbprofiler/\n.neo4j_shell_history\n.npm/\n.octave_hist\n.openoffice.org\n.openoffice.org2\n.oracle_jre_usage/\n.ovftool.ssldb\n.Qsync/\n.parallel/\n# PostgreSQL password file\n.pgpass\n# stores credentials\n.pig_history\n.pki\n.pentaho/\n.psql_history\n.pulse\n.puppet/\n.pwm3\n.PyCharm*/\n.pylint.d\n.python-eggs/\n.python_history\n.qt\n.qicon\n.qnicon\n# contains usernames and passwords\n.rabbitmqadmin.conf\n.RData\n.recently-used\n.recently-used.xbel\n.rediscli_history\n.Rhistory\n.rnd\n.rstudio-desktop/\n.rbenv/\n# 'rbenv local' $PWD version file\n.ruby-version\n.sbt/[[:digit:]].[[:digit:]]*/\n.sbt/boot/\n.sbt/preloaded/\n.sbt/repositories\n.sbt/*/plugins/project/target/\n.sbt/*/plugins/target/\n.scala_history\n.sdkman/archives/\n.sdkman/bin/\n.sdkman/candidates/\n.sdkman/etc/config\n.sdkman/ext/\n.sdkman/src/\n.sdkman/tmp/\n.sdkman/var/\n# Semaphore CI - contains auth token\n.sem.yaml\n.serverauth.*\n# $PWD/.serverless local service artifacts\n.serverless/\n# $HOME/.serverless installation\n.serverless/bin/\n.sh_history\n# contains client id and secret for Shpotify\n.shpotify.cfg\n.Skype\n.snowsql\n# Snowflake password stored in plaintext in here\n.snowsql/config\n.spark_history\n.Spotlight-*\n.spumux/\n.sqlite_history\n.sqlline/history\n.ssh/known_hosts\n.ssh-agent.env\n.subversion/auth\n.svn\n.swatch_script.*\n.TemporaryItems\n.terraform.d/\n.terragrunt\n.themes\n.thumbnails\n.tilda/locks/*\n.tmux/\n.tomboy\n.tomboy.log\n.Trash\n.Trashes\n.travis/\n.vagrant\n.vagrant.d/\n.vboxclient-autoresize.pid\n.vboxclient-clipboard.pid\n.vboxclient-seamless.pid\n.vim/\n.viminfo\n.vnc/\n.wapi\n.wget-hsts\n.wine\n.wireshark-etc/\n.wireshark/\n.wmii-*\n.Xauthority\n.xine\n.xmms\n.xsession-errors\n.zenmap-etc/pango/pangorc\n\n# GCP credentials\napplication_default_credentials.json\n*keyfile.json\n*credentials.json\n\nabs-guide.pdf\nandroid-sdks/\nApplications/\nAT.postflight.*\n# contains OAuth token\nauth.json\nbin/altfirewalls\nbin/altnames\nbin/contrib/*\nbin/desktops\nbin/firewalls\nbin/servers\n#bitbucket/\n#bitbucket/*\nboxes/\nBox Documents/\n# contains webhook URL which should not be committed publicly\nbuildkite-pipeline*.json\n.buildkite-pipelines/\nc\nchinook.*sql*\nchinook.psql\nCalibre Library/\ncli_junkie.gif\ndebs/\nDesktop/\nDocuments/\nDownloads/\ndrive/\nDropbox/\neclipse/\nenterprise\nfatpacks/\nfatlib/\nfluxbox-themes\nfluxbox/debian007.jpg\n# internal repos\n#git/\n#git/*\n# public repos - clashes with TeamCity-CI repo's .teamcity/GitHub\n#github/\n#github/*\ngitolite-admin/\ngitroot/\nGNUstep\nGoogle*Drive\ngo/bin/\ngo/src/github.com/\ngoogle-cloud-sdk/\nhadoop-sources/\nhgroot/\nIdeaProjects/\nImages\ninfrastructure/puppetinfrastructure/\n#jython*\nLibrary\nmbox\nmcollective-plugins/\nMovies\nMusic\nnorev\nNS-GUISettings\n*OneDrive*/\nops/\noriginal-tars\noriginals_tars\notherpics\nQsync/\nperl5/\nPictures\nPublic\npuppet.git/\nPycharmProjects/\npytools_checks/\nrpms/\nsetup/mac_settings/\nshm/\nsiege.log\nSites\n# golang libraries\nsrc/github.com\nsrc/golang.org\nsubversion/\nsvnroot/\ntesting/\ntmp.*\n# common vagrant stuff\nvagrant/boxes/\nvagrant/data/\nvagrant/gems/\nvagrant/insecure_private_key\nvagrant/rgloader/\nvagrant/setup_version\nvagrant/tmp/\n# custom stuff found in vagrant\nvagrant/basho_bench/\nvagrant/id_rsa.pub\nvagrant/jce_policy-6\nvagrant/jython\nvagrant/kibana/\nvagrant/lib\nvagrant/mrepo\nvagrant/mx4j-*\nvagrant/mysql-connector-*\nvagrant/thrift-*\nvenv/\nVirtualBox VMs/\nVirtualBoxShared\nvisualvm*\nwindows/bin2\nwordlists/*.lower\nwordlists/hosts.large\nwordlists/hosts.medium\nwordlists/hosts.small\nwordlists/pw.medium\nwordlists/pw.small\nwordlists/pw.tiny\nwordlists/snmp\nwordlists/ultimate.*\nwordlists/users.large\nwordlists/users.medium\nwordlists/users.small\nwordlists/users.tiny\nwordlists/users.weighted\n\n*.doc\n*.docx\n*.xls\n*.xlsx\n*.log\n*.msg\n*.pages\n*.rtf\n*.wpd\n*.wps\n\n# ============================================================================ #\n#\n# Regenerate all sections below in to a single arg for API call via:\n#\n#       grep '[C]reated by https://' .gitignore | sed 's,.*/,,' | tr ',' '\\n' | sort -u | tr '\\n' ',' | sed 's/,$//' | xargs echo gitignore.io_api.sh\n#\n#   eg.\n#\n#       gitignore.io_api.sh ansible,apachehadoop,appcode,appengine,archive,archives,archlinuxpackages,audio,autotools,backup,basic,bittorrent,c,c++,certificates,chefcookbook,clojure,cloud9,cmake,code,code-java,codeblocks,compressed,compressedarchive,compression,data,database,datarecovery,diff,direnv,diskimage,docfx,docpress,docz,dotenv,dotfilessh,dotsettings,dropbox,eclipse,emacs,erlang,executable,firebase,flask,git,gitbook,go,gpg,gradle,grails,groovy,grunt,haskell,helm,homebrew,hugo,images,intellij,intellij+all,intellij+iml,java,java-web,jenv,jetbrains,jetbrains+all,jetbrains+iml,jmeter,julia,jupyternotebooks,kotlin,lamp,latex,less,linux,lua,macos,matlab,maven,mercurial,microsoftoffice,node,octave,osx,packer,patch,perl,perl6,phpunit,powershell,puppet,putty,pycharm,pycharm+all,pycharm+iml,pydev,python,r,rails,react,reactnative,redis,root,ruby,rust,sbt,scala,serverless,sonar,sonarqube,spark,splunk,spreadsheet,ssh,sublimetext,svn,terraform,terragrunt,tortoisegit,vagrant,venv,virtualenv,visualstudio,visualstudiocode,vs,vscode,vue,vuejs,waf,windows,xcode,xcodeinjection,zsh >> .gitignore\n#\n# Find new or missing tags you aren't using yet:\n#\n#       grep '[C]reated by https://' .gitignore | sed 's,.*/,,' | tr ',' '\\n' | sort -u | tr '\\n' ',' | sed 's/,$//' | gitignore.io_api.sh missing\n#\n# ============================================================================ #\n\n\n\n# Created by https://www.toptal.com/developers/gitignore/api/ansible,apachehadoop,appcode,appengine,archive,archives,archlinuxpackages,audio,autotools,backup,basic,bittorrent,c,c++,certificates,chefcookbook,clojure,cloud9,cmake,code,code-java,codeblocks,compressed,compressedarchive,compression,data,database,datarecovery,diff,direnv,diskimage,docfx,docpress,docz,dotenv,dotfilessh,dotsettings,dropbox,eclipse,emacs,erlang,executable,firebase,flask,git,gitbook,go,gpg,gradle,grails,groovy,grunt,haskell,helm,homebrew,hugo,images,intellij,intellij+all,intellij+iml,java,java-web,jenv,jetbrains,jetbrains+all,jetbrains+iml,jmeter,julia,jupyternotebooks,kotlin,lamp,latex,less,linux,lua,macos,matlab,maven,mercurial,microsoftoffice,node,octave,osx,packer,patch,perl,perl6,phpunit,powershell,puppet,putty,pycharm,pycharm+all,pycharm+iml,pydev,python,r,rails,react,reactnative,redis,root,ruby,rust,sbt,scala,serverless,sonar,sonarqube,spark,splunk,spreadsheet,ssh,sublimetext,svn,terraform,terragrunt,tortoisegit,vagrant,venv,virtualenv,visualstudio,visualstudiocode,vs,vscode,vue,vuejs,waf,windows,xcode,xcodeinjection,zsh\n# Edit at https://www.toptal.com/developers/gitignore?templates=ansible,apachehadoop,appcode,appengine,archive,archives,archlinuxpackages,audio,autotools,backup,basic,bittorrent,c,c++,certificates,chefcookbook,clojure,cloud9,cmake,code,code-java,codeblocks,compressed,compressedarchive,compression,data,database,datarecovery,diff,direnv,diskimage,docfx,docpress,docz,dotenv,dotfilessh,dotsettings,dropbox,eclipse,emacs,erlang,executable,firebase,flask,git,gitbook,go,gpg,gradle,grails,groovy,grunt,haskell,helm,homebrew,hugo,images,intellij,intellij+all,intellij+iml,java,java-web,jenv,jetbrains,jetbrains+all,jetbrains+iml,jmeter,julia,jupyternotebooks,kotlin,lamp,latex,less,linux,lua,macos,matlab,maven,mercurial,microsoftoffice,node,octave,osx,packer,patch,perl,perl6,phpunit,powershell,puppet,putty,pycharm,pycharm+all,pycharm+iml,pydev,python,r,rails,react,reactnative,redis,root,ruby,rust,sbt,scala,serverless,sonar,sonarqube,spark,splunk,spreadsheet,ssh,sublimetext,svn,terraform,terragrunt,tortoisegit,vagrant,venv,virtualenv,visualstudio,visualstudiocode,vs,vscode,vue,vuejs,waf,windows,xcode,xcodeinjection,zsh\n\n### Ansible ###\n*.retry\n\n### ApacheHadoop ###\n*.iml\n*.ipr\n*.iws\n*.orig\n*.rej\n.idea\n.svn\n.classpath\n.project\n.settings\ntarget\nhadoop-common-project/hadoop-kms/downloads/\nhadoop-hdfs-project/hadoop-hdfs/downloads\nhadoop-hdfs-project/hadoop-hdfs-httpfs/downloads\nhadoop-common-project/hadoop-common/src/test/resources/contract-test-options.xml\nhadoop-tools/hadoop-openstack/src/test/resources/contract-test-options.xml\n\n### AppCode ###\n# Covers JetBrains IDEs: IntelliJ, RubyMine, PhpStorm, AppCode, PyCharm, CLion, Android Studio, WebStorm and Rider\n# Reference: https://intellij-support.jetbrains.com/hc/en-us/articles/206544839\n\n# User-specific stuff\n.idea/**/workspace.xml\n.idea/**/tasks.xml\n.idea/**/usage.statistics.xml\n.idea/**/dictionaries\n.idea/**/shelf\n\n# Generated files\n.idea/**/contentModel.xml\n\n# Sensitive or high-churn files\n.idea/**/dataSources/\n.idea/**/dataSources.ids\n.idea/**/dataSources.local.xml\n.idea/**/sqlDataSources.xml\n.idea/**/dynamic.xml\n.idea/**/uiDesigner.xml\n.idea/**/dbnavigator.xml\n\n# Gradle\n.idea/**/gradle.xml\n.idea/**/libraries\n\n# Gradle and Maven with auto-import\n# When using Gradle or Maven with auto-import, you should exclude module files,\n# since they will be recreated, and may cause churn.  Uncomment if using\n# auto-import.\n# .idea/artifacts\n# .idea/compiler.xml\n# .idea/jarRepositories.xml\n# .idea/modules.xml\n# .idea/*.iml\n# .idea/modules\n# *.iml\n# *.ipr\n\n# CMake\ncmake-build-*/\n\n# Mongo Explorer plugin\n.idea/**/mongoSettings.xml\n\n# File-based project format\n\n# IntelliJ\nout/\n\n# mpeltonen/sbt-idea plugin\n.idea_modules/\n\n# JIRA plugin\natlassian-ide-plugin.xml\n\n# Cursive Clojure plugin\n.idea/replstate.xml\n\n# Crashlytics plugin (for Android Studio and IntelliJ)\ncom_crashlytics_export_strings.xml\ncrashlytics.properties\ncrashlytics-build.properties\nfabric.properties\n\n# Editor-based Rest Client\n.idea/httpRequests\n\n# Android studio 3.1+ serialized cache file\n.idea/caches/build_file_checksums.ser\n\n### AppCode Patch ###\n# Comment Reason: https://github.com/joeblau/gitignore.io/issues/186#issuecomment-215987721\n\n# *.iml\n# modules.xml\n# .idea/misc.xml\n# *.ipr\n\n# Sonarlint plugin\n# https://plugins.jetbrains.com/plugin/7973-sonarlint\n.idea/**/sonarlint/\n\n# SonarQube Plugin\n# https://plugins.jetbrains.com/plugin/7238-sonarqube-community-plugin\n.idea/**/sonarIssues.xml\n\n# Markdown Navigator plugin\n# https://plugins.jetbrains.com/plugin/7896-markdown-navigator-enhanced\n.idea/**/markdown-navigator.xml\n.idea/**/markdown-navigator-enh.xml\n.idea/**/markdown-navigator/\n\n# Cache file creation bug\n# See https://youtrack.jetbrains.com/issue/JBR-2257\n.idea/$CACHE_FILE$\n\n# CodeStream plugin\n# https://plugins.jetbrains.com/plugin/12206-codestream\n.idea/codestream.xml\n\n### AppEngine ###\n# Google App Engine generated folder\nappengine-generated/\n\n### Archive ###\n\n### Mostly from https://en.wikipedia.org/wiki/List_of_archive_formats\n\n## Archiving only\n# The traditional archive format on Unix-like systems, now used mainly for the creation of static libraries.\n*.a\n*.ar\n# RPM files consist of metadata concatenated with (usually) a cpio archive. Newer RPM systems also support other archives, as cpio is becoming obsolete. cpio is also used with initramfs.\n*.cpio\n\n# A self-extracting archive that uses the Bourne shell (sh).\n*.shar\n# A system for storing multiple files. LBR archives typically contained files processed by SQ, or the archive itself was compressed with SQ. LBR archives that were compressed with SQ ended with the extension .LQR\n*.LBR\n# An archive format originally used mainly for archiving and distribution of the exact, nearly-exact, or custom-modified contents of an optical storage medium such as a CD-ROM or DVD-ROM. However, it can be used to archive the contents of other storage media, selected partitions, folders, and/or files. The resulting archive is typically optimized for convenient rendering to (re-)writable CD or DVD media.\n*.iso\n# A library format used primarily on the Commodore 64 and 128 lines of computers. This bears no resemblance to the DOS LBR format. While library files were quick to implement (a number of programs exist to work with them) they are crippled in that they cannot grow with use: once a file has been created it cannot be amended (files added, changed or deleted) without recreating the entire file.\n*.lbr\n# An archive format used by Mozilla for storing binary diffs. Used in conjunction with bzip2.\n*.mar\n# A common archive format used on Unix-like systems. Generally used in conjunction with compressors such as gzip, bzip2, compress or xz to create .tar.gz, .tar.bz2, .tar.Z or tar.xz files.\n*.tar\n\n# Package managers\n# Red Hat Package Manager\n*.rpm\n# Debian package\n*.deb\n# MicroSoft Installer\n*.msi\n*.msm\n*.msp\n# Mozilla package installer\n*.xpi\n# Ruby Package\n*.gem\n\n\n### Archives ###\n# It's better to unpack these files and commit the raw source because\n# git has its own built in compression methods.\n*.7z\n*.jar\n*.rar\n*.zip\n*.gz\n*.gzip\n*.tgz\n*.bzip\n*.bzip2\n*.bz2\n*.xz\n*.lzma\n*.cab\n*.xar\n\n# Packing-only formats\n\n# Package management formats\n*.dmg\n*.egg\n*.txz\n\n### ArchLinuxPackages ###\n*.tar.*\n*.exe\n*.log\n*.log.*\n*.sig\n\npkg/\n# XXX: might conflict with standard java golang src/ directory structure\n#src/\n\n### Audio ###\n*.aif\n*.iff\n*.m3u\n*.m4a\n*.mid\n*.mp3\n*.mpa\n*.ra\n*.wav\n*.wma\n*.ogg\n*.flac\n\n### Autotools ###\n# http://www.gnu.org/software/automake\n\n# XXX: I track a master Makefile.in in DevOps-Bash-tools to inherit in other projects\n#Makefile.in\n/ar-lib\n/mdate-sh\n/py-compile\n/test-driver\n/ylwrap\n.deps/\n\n# http://www.gnu.org/software/autoconf\n\nautom4te.cache\n/autoscan.log\n/autoscan-*.log\n/aclocal.m4\n/compile\n/config.guess\n/config.h.in\n/config.log\n/config.status\n/config.sub\n/configure\n/configure.scan\n/depcomp\n/install-sh\n/missing\n/stamp-h1\n\n# https://www.gnu.org/software/libtool/\n\n/ltmain.sh\n\n# http://www.gnu.org/software/texinfo\n\n/texinfo.tex\n\n# http://www.gnu.org/software/m4/\n\nm4/libtool.m4\nm4/ltoptions.m4\nm4/ltsugar.m4\nm4/ltversion.m4\nm4/lt~obsolete.m4\n\n# Generated Makefile\n# (meta build system like autotools,\n# can automatically generate from config.status script\n# (which is called by configure script))\n#\n# XXX: always want to commit Makefiles\n#Makefile\n\n### Autotools Patch ###\n\n### Backup ###\n*.bak\n*.gho\n*.ori\n*.tmp\n\n### Basic ###\n# Apples Build\n*.build\n*.apples\n\n# Initialized files\n#*.ini  # used by Ansible for inventory.ini\n*.basic\n\n### BitTorrent ###\n*.torrent\n\n### C ###\n# Prerequisites\n# XXX: clashes with .bash.d/* and .conf.d/* type stuff\n#*.d\n\n# Object files\n*.o\n*.ko\n*.obj\n*.elf\n\n# Linker output\n*.ilk\n*.map\n*.exp\n\n# Precompiled Headers\n*.gch\n*.pch\n\n# Libraries\n*.lib\n*.la\n*.lo\n\n# Shared objects (inc. Windows DLLs)\n*.dll\n*.so\n*.so.*\n*.dylib\n\n# Executables\n*.out\n*.app\n*.i*86\n*.x86_64\n*.hex\n\n# Debug files\n*.dSYM/\n*.su\n*.idb\n*.pdb\n\n# Kernel Module Compile Results\n# XXX: *.mod* changed to avoid clashing with Golang's new module system go.mod\n*.mod?*\n.tmp_versions/\nmodules.order\nModule.symvers\nMkfile.old\ndkms.conf\n\n### Zsh ###\n# Zsh compiled script + zrecompile backup\n*.zwc\n*.zwc.old\n\n# Zsh completion-optimization dumpfile\n*zcompdump*\n\n# Zsh zcalc history\n.zcalc_history\n\n# A popular plugin manager's files\n._zplugin\n.zplugin_lstupd\n\n# zdharma/zshelldoc tool's files\nzsdoc/data\n\n# robbyrussell/oh-my-zsh/plugins/per-directory-history plugin's files\n# (when set-up to store the history in the local directory)\n.directory_history\n\n# MichaelAquilina/zsh-autoswitch-virtualenv plugin's files\n# (for Zsh plugins using Python)\n.venv\n\n# Zunit tests' output\n/tests/_output/*\n!/tests/_output/.gitkeep\n\n### C++ ###\n# Prerequisites\n\n# Compiled Object files\n*.slo\n\n# Precompiled Headers\n\n# Compiled Dynamic libraries\n\n# Fortran module files\n# XXX: would conflict with Golang's go.mod\n#*.mod\n*.smod\n\n# Compiled Static libraries\n*.lai\n\n# Executables\n\n### certificates ###\n*.pem\n*.key\n*.crt\n*.cer\n*.priv\n\n### Cloud9 ###\n# Cloud9 IDE - http://c9.io\n.c9revisions\n.c9\n\n### ChefCookbook ###\n.vagrant\n/cookbooks\n\n# Bundler\n# XXX: would prevent tracking ~/bin scripts\n#bin/*\n.bundle/*\n\n.kitchen/\n.kitchen.local.yml\n.kitchen.*.local.yml\nkitchen.local.yml\nkitchen.*.local.yml\n\n### Code ###\n.vscode/*\n!.vscode/settings.json\n!.vscode/tasks.json\n!.vscode/launch.json\n!.vscode/extensions.json\n*.code-workspace\n\n### Code-Java ###\n# Language Support for Java(TM) by Red Hat extension for Visual Studio Code - https://marketplace.visualstudio.com/items?itemName=redhat.java\n\nfactoryConfiguration.json\n\n### Clojure ###\n# XXX: always want to commit pom.xml\n#pom.xml\npom.xml.asc\n*.class\n# XXX: would break lib submodule updates\n#/lib/\n/classes/\n/target/\n/checkouts/\n.lein-deps-sum\n.lein-repl-history\n.lein-plugins/\n.lein-failures\n.nrepl-port\n.cpcache/\n\n### Compressed ###\n*.pkg\n*.sit\n*.sitx\n*.zipx\n\n### CompressedArchive ###\n\n\n## Archiving and compression\n# Open source file format. Used by 7-Zip.\n# Mac OS X, restoration on different platforms is possible although not immediate \tYes \tBased on 7z. Preserves Spotlight metadata, resource forks, owner/group information, dates and other data which would be otherwise lost with compression.\n*.s7z\n# Old archive versions only \tProprietary format\n*.ace\n# A format that compresses and doubly encrypt the data (AES256 and CAS256) avoiding brute force attacks, also hide files in an AFA file. It has two ways to safeguard data integrity and subsequent repair of the file if has an error (repair with AstroA2P (online) or Astrotite (offline)).\n*.afa\n# A mainly Korean format designed for very large archives.\n*.alz\n# Android application package (variant of JAR file format).\n*.apk\n# ??\n*.arc\n# Originally DOS, now multiple\n*.arj\n# Open archive format, used by B1 Free Archiver (http://dev.b1.org/standard/archive-format.html)\n*.b1\n# Binary Archive with external header\n*.ba\n# Proprietary format from the ZipTV Compression Components\n*.bh\n# The Microsoft Windows native archive format, which is also used by many commercial installers such as InstallShield and WISE.\n# Originally DOS, now DOS and Windows \tCreated by Yaakov Gringeler; released last in 2003 (Compressia 1.0.0.1 beta), now apparently defunct. Free trial of 30 days lets user create and extract archives; after that it is possible to extract, but not to create.\n*.car\n# Open source file format.\n*.cfs\n# Compact Pro archive, a common archiver used on Mac platforms until about Mac OS 7.5.x. Competed with StuffIt; now obsolete.\n*.cpt\n# Windows, Unix-like, Mac OS X Open source file format. Files are compressed individually with either gzip, bzip2 or lzo.\n*.dar\n# DiskDoubler \tMac OS \t\t\tobsolete\n*.dd\n# ??\n*.dgc\n# Apple Disk Image upports \"Internet-enabled\" disk images, which, once downloaded, are automatically decompressed, mounted, have the contents extracted, and thrown away. Currently, Safari is the only browser that supports this form of extraction; however, the images can be manually extracted as well. This format can also be password-protected or encrypted with 128-bit or 256-bit AES encryption.\n# Enterprise Java Archive archive\n*.ear\n# ETSoft compressed archive\n# The predecessor of DGCA.\n*.gca\n# Originally DOS \tYes, but may be covered by patents \tDOS era format; uses arithmetic/Markov coding\n*.ha\n# MS Windows \tHKI\n*.hki\n# Produced by ICEOWS program. Excels at text file compression.\n*.ice\n# Java archive, compatible with ZIP files\n# Open sourced archiver with compression using the PAQ family of algorithms and optional encryption.\n*.kgb\n# Originally DOS, now multiple \tMultiple \tYes \tThe standard format on Amiga.\n*.lzh\n*.lha\n# Archiver originally used on The Amiga. Now copied by Microsoft to use in their .cab and .chm files.\n*.lzx\n# file format from NoGate Consultings, a rival from ARC-Compressor.\n*.pak\n# A disk image archive format that supports several compression methods as well as splitting the archive into smaller pieces.\n*.partimg\n# An experimental open source packager (http://mattmahoney.net/dc)\n*.paq*\n# Open source archiver supporting authenticated encryption, volume spanning, customizable object level and volume level integrity checks (form CRCs to SHA-512 and Whirlpool hashes), fast deflate based compression\n*.pea\n# The format from the PIM - a freeware compression tool by Ilia Muraviev. It uses an LZP-based compression algorithm with set of filters for executable, image and audio files.\n*.pim\n# PackIt \tMac OS \t\t\tobsolete\n*.pit\n# Used for data in games written using the Quadruple D library for Delphi. Uses byte pair compression.\n*.qda\n# A proprietary archive format, second in popularity to .zip files.\n# The format from a commercial archiving package. Odd among commercial packages in that they focus on incorporating experimental algorithms with the highest possible compression (at the expense of speed and memory), such as PAQ, PPMD and PPMZ (PPMD with unlimited-length strings), as well as a proprietary algorithms.\n*.rk\n# Self Dissolving ARChive \tCommodore 64, Commodore 128 \tCommodore 64, Commodore 128 \tYes \tSDAs refer to Self Dissolving ARC files, and are based on the Commodore 64 and Commodore 128 versions of ARC, originally written by Chris Smeets. While the files share the same extension, they are not compatible between platforms. That is, an SDA created on a Commodore 64 but run on a Commodore 128 in Commodore 128 mode will crash the machine, and vice versa. The intended successor to SDA is SFX.\n*.sda\n# A pre-Mac OS X Self-Extracting Archive format. StuffIt, Compact Pro, Disk Doubler and others could create .sea files, though the StuffIt versions were the most common.\n*.sea\n# Scifer Archive with internal header\n*.sen\n# Commodore 64, Commodore 128 \tSFX is a Self Extracting Archive which uses the LHArc compression algorithm. It was originally developed by Chris Smeets on the Commodore platform, and runs primarily using the CS-DOS extension for the Commodore 128. Unlike its predecessor SDA, SFX files will run on both the Commodore 64 and Commodore 128 regardless of which machine they were created on.\n*.sfx\n# An archive format designed for the Apple II series of computers. The canonical implementation is ShrinkIt, which can operate on disk images as well as files. Preferred compression algorithm is a combination of RLE and 12-bit LZW. Archives can be manipulated with the command-line NuLib tool, or the Windows-based CiderPress.\n*.shk\n# A compression format common on Apple Macintosh computers. The free StuffIt Expander is available for Windows and OS X.\n# The replacement for the .sit format that supports more compression methods, UNIX file permissions, long file names, very large files, more encryption options, data specific compressors (JPEG, Zip, PDF, 24-bit image, MP3). The free StuffIt Expander is available for Windows and OS X.\n# A royalty-free compressing format\n*.sqx\n# The \"tarball\" format combines tar archives with a file-based compression scheme (usually gzip). Commonly used for source and binary distribution on Unix-like platforms, widely available elsewhere.\n*.tar.gz\n*.tar.Z\n*.tar.bz2\n*.tbz2\n*.tar.lzma\n*.tlz\n# UltraCompressor 2.3 was developed to act as an alternative to the then popular PKZIP application. The main feature of the application is its ability to create large archives. This means that compressed archives with the UC2 file extension can hold almost 1 million files.\n*.uc\n*.uc0\n*.uc2\n*.ucn\n*.ur2\n*.ue2\n# Based on PAQ, RZM, CSC, CCM, and 7zip. The format consists of a PAQ, RZM, CSC, or CCM compressed file and a manifest with compression settings stored in a 7z archive.\n*.uca\n# A high compression rate archive format originally for DOS.\n*.uha\n# Web Application archive (Java-based web app)\n*.war\n# File-based disk image format developed to deploy Microsoft Windows.\n*.wim\n# XAR\n# Native format of the Open Source KiriKiri Visual Novel engine. Uses combination of block splitting and zlib compression. The filenames and pathes are stored in UTF-16 format. For integrity check, the Adler-32 hashsum is used. For many commercial games, the files are encrypted (and decoded on runtime) via so-called \"cxdec\" module, which implements xor-based encryption.\n*.xp3\n# Yamazaki zipper archive. Compression format used in DeepFreezer archiver utility created by Yamazaki Satoshi. Read and write support exists in TUGZip, IZArc and ZipZag\n*.yz1\n# The most widely used compression format on Microsoft Windows. Commonly used on Macintosh and Unix systems as well.\n# application/x-zoo \tzoo \tMultiple \tMultiple \tYes\n*.zoo\n# Journaling (append-only) archive format with rollback capability. Supports deduplication and incremental update based on last-modified dates. Multi-threaded. Compresses in LZ77, BWT, and context mixing formats. Open source.\n*.zpaq\n# Archiver with a compression algorithm based on the Burrows-Wheeler transform method.\n*.zz\n\n\n### Compression ###\n\n### From https://en.wikipedia.org/wiki/List_of_archive_formats\n\n## Compression only\n# An open source, patent- and royalty-free compression format. The compression algorithm is a Burrows-Wheeler transform followed by a move-to-front transform and finally Huffman coding\n# Old compressor for QNX4 OS. The compression algorithm is a modified LZSS, with an adaptive Huffman coding.\n*.F\n# GNU Zip, the primary compression format used by Unix-like systems. The compression algorithm is DEFLATE.\n# An alternate LZMA algorithm implementation, with support for checksums and ident bytes.\n*.lz\n# The LZMA compression algorithm as used by 7-Zip\n# An implementation of the LZO data compression algorithm\n*.lzo\n# A compression program designed to do particularly well on very large files containing long distance redundancy.\n*.rz\n# Windows compress/decompress- Linux and Mac OS X decompress only \tA compression program designed to do high compression on SF2 files (SoundFont)\n*.sfark\n# A compression format invented by Google and open-sourced in 2011. Snappy aims for very high speeds, reasonable compression, and maximum stability rather than maximum compression or compatibility with any other compression library.\n*.sz\n# Squeeze: A program which compressed files. A file which was \"squeezed\" had the middle initial of the name changed to \"Q\", so that a squeezed text file would end with .TQT, a squeezed executable would end with .CQM or .EQE. Typically used with .LBR archives, either by storing the squeezed files in the archive, or by storing the files decompressed and then compressing the archive, which would have a name ending in \".LQR\".\n# XXX: this prevents .sql files from being tracked\n#*.?Q?\n# A compression program written by Steven Greenberg implementing the LZW algorithm. For several years in the CP/M world when no implementation was available of ARC, CRUNCHed files stored in .LBR archives were very popular. CRUNCH's implementation of LZW had a somewhat unique feature of modifying and occasionally clearing the code table in memory when it became full, resulting in a few percent better compression on many files.\n*.?Z?\n# A compression format using LZMA2 to yield very high compression ratios.\n# The traditional Huffman coding compression format.\n*.z\n# The traditional LZW compression format.\n*.Z\n# Joke compression program, actually increasing file size\n*.infl\n# Compression format(s) used by some DOS and Windows install programs. MS-DOS includes expand.exe to decompress its install files. The compressed files are created with a matching compress.exe command. The compression algorithm is LZSS.\n*.??_\n\n\n### CMake ###\nCMakeLists.txt.user\nCMakeCache.txt\nCMakeFiles\nCMakeScripts\nTesting\ncmake_install.cmake\ninstall_manifest.txt\ncompile_commands.json\nCTestTestfile.cmake\n_deps\n\n### CMake Patch ###\n# External projects\n*-prefix/\n\n### Database ###\n*.accdb\n*.db\n*.dbf\n*.mdb\n*.sqlite3\n\n### DataRecovery ###\n\n\n## Data recovery\n# File format used by dvdisaster to be used for data recovery when discs become damaged or partially unreadable.\n*.ecc\n# File format used in conjunction with any archive format to provide redundancy and data recovery, most often in newsgroup distribution of binary files.\n*.par\n*.par2\n\n\n### Diff ###\n*.patch\n*.diff\n\n### direnv ###\n.direnv\n#.envrc\n\n### CodeBlocks ###\n# specific to CodeBlocks IDE\n*.layout\n*.depend\n# generated directories\n# XXX: would prevent tracking ~/bin scripts\n#bin/\nobj/\n\n### DocFx ###\n.cache\n/**/_site/\n\n### Docpress ###\n# docpress documentation generator: https://docpress.github.io/index.html\n\n_docpress/\n\n### Docz ###\n.docz\n\n\n### dotenv ###\n.env\n\n### DotfilesSh ###\nlocal-patch\npatched-src\n\n### DotSettings ###\n*.DotSettings\n\n### Dropbox ###\n# Dropbox settings and caches\n.dropbox\n.dropbox.attr\n.dropbox.cache\n\n### Eclipse ###\n.metadata\ntmp/\n*.swp\n*~.nib\nlocal.properties\n.settings/\n.loadpath\n.recommenders\n\n# External tool builders\n.externalToolBuilders/\n\n# Locally stored \"Eclipse launch configurations\"\n*.launch\n\n# PyDev specific (Python IDE for Eclipse)\n*.pydevproject\n\n# CDT-specific (C/C++ Development Tooling)\n.cproject\n\n# CDT- autotools\n.autotools\n\n# Java annotation processor (APT)\n.factorypath\n\n# PDT-specific (PHP Development Tools)\n.buildpath\n\n# sbteclipse plugin\n.target\n\n# Tern plugin\n.tern-project\n\n# TeXlipse plugin\n.texlipse\n\n# STS (Spring Tool Suite)\n.springBeans\n\n# Code Recommenders\n.recommenders/\n\n# Annotation Processing\n.apt_generated/\n.apt_generated_test/\n\n# Scala IDE specific (Scala & Java development for Eclipse)\n.cache-main\n.scala_dependencies\n.worksheet\n\n# Uncomment this line if you wish to ignore the project description file.\n# Typically, this file would be tracked if it contains build/dependency configurations:\n#.project\n\n### Eclipse Patch ###\n# Spring Boot Tooling\n.sts4-cache/\n\n### Data ###\n# XXX: would prevent tracking sample test csv data\n#*.csv\n*.dat\n*.efx\n*.gbr\n*.pps\n*.ppt\n*.pptx\n*.sdf\n*.tax2010\n*.vcf\n# XXX: would prevent tracking pom.xml\n#*.xml\n\n### Erlang ###\n.eunit\n*.beam\n*.plt\nerl_crash.dump\n.concrete/DEV_MODE\n\n# rebar 2.x\n.rebar\nrel/example_project\nebin/*.beam\ndeps\n\n# rebar 3\n.rebar3\n_build/\n_checkouts/\n\n### Executable ###\n# XXX: .cgi scripts are old but we might want to track them\n#*.cgi\n*.com\n*.gadget\n*.pif\n# XXX: would prevent tracking Windows Scripting files\n#*.vb\n#*.wsf\n\n### Firebase ###\n**/node_modules/*\n**/.firebaserc\n\n### Firebase Patch ###\n.runtimeconfig.json\n.firebase/\n\n### Flask ###\ninstance/*\n!instance/.gitignore\n.webassets-cache\n\n### Flask.Python Stack ###\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\n# XXX: breaks lib submodule updates\n#lib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\npip-wheel-metadata/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\npytestdebug.log\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\ndoc/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n.python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\npythonenv*\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n\n# pytype static type analyzer\n.pytype/\n\n# profiling data\n.prof\n\n### Git ###\n# Created by git for backups. To disable backups in Git:\n# $ git config --global mergetool.keepBackup false\n\n# Created by git when using merge tools for conflicts\n*.BACKUP.*\n*.BASE.*\n*.LOCAL.*\n*.REMOTE.*\n*_BACKUP_*.txt\n*_BASE_*.txt\n*_LOCAL_*.txt\n*_REMOTE_*.txt\n\n### GitBook ###\n# Node rules:\n## Grunt intermediate storage (http://gruntjs.com/creating-plugins#storing-task-files)\n.grunt\n\n## Dependency directory\n## Commenting this out is preferred by some people, see\n## https://docs.npmjs.com/misc/faq#should-i-check-my-node_modules-folder-into-git\nnode_modules\n\n# Book build output\n_book\n\n# eBook build output\n*.epub\n*.mobi\n*.pdf\n\n### Go ###\n# Binaries for programs and plugins\n*.exe~\n\n# Test binary, built with `go test -c`\n*.test\n\n# Output of the go coverage tool, specifically when used with LiteIDE\n\n# Dependency directories (remove the comment below to include it)\n# vendor/\n\n### Go Patch ###\n/vendor/\n/Godeps/\n\n### GPG ###\nsecring.*\n\n\n### DiskImage ###\n*.toast\n*.vcd\n\n### Grails ###\n# .gitignore for Grails 1.2 and 1.3\n# Although this should work for most versions of grails, it is\n# suggested that you use the \"grails integrate-with --git\" command\n# to generate your .gitignore file.\n\n# web application files\n/web-app/WEB-INF/classes\n\n# default HSQL database files for production mode\n/prodDb.*\n\n# general HSQL database files\n*Db.properties\n*Db.script\n\n# logs\n/stacktrace.log\n/test/reports\n/logs\n\n# project release file\n/*.war\n\n# plugin release files\n/*.zip\n/plugin.xml\n\n# older plugin install locations\n/plugins\n/web-app/plugins\n\n# \"temporary\" build files\n/target\n\n### Groovy ###\n# .gitignore created from Groovy contributors in https://github.com/apache/groovy/blob/master/.gitignore\n\nuser.gradle\n.gradle/\n\n*.DS_Store\n*~\n\n.shelf\n\n\n\n### grunt ###\n# Grunt usually compiles files inside this directory\n\n# Grunt usually preprocesses files such as coffeescript, compass... inside the .tmp directory\n.tmp/\n\n### Haskell ###\ndist\ndist-*\ncabal-dev\n*.hi\n*.hie\n*.chi\n*.chs.h\n*.dyn_o\n*.dyn_hi\n.hpc\n.hsenv\n.cabal-sandbox/\ncabal.sandbox.config\n*.prof\n*.aux\n*.hp\n*.eventlog\n.stack-work/\ncabal.project.local\ncabal.project.local~\n.HTF/\n.ghc.environment.*\n\n### Helm ###\n# Chart dependencies\n**/charts/*.tgz\n\n### Homebrew ###\nBrewfile.lock.json\n\n### Hugo ###\n# Generated files by hugo\n# XXX: might interfere with projects code structure\n#/public/\n/resources/_gen/\nhugo_stats.json\n\n# Executable may be added to repository\nhugo.exe\nhugo.darwin\nhugo.linux\n\n### Images ###\n# JPEG\n*.jpg\n*.jpeg\n*.jpe\n*.jif\n*.jfif\n*.jfi\n\n# JPEG 2000\n*.jp2\n*.j2k\n*.jpf\n*.jpx\n*.jpm\n*.mj2\n\n# JPEG XR\n*.jxr\n*.hdp\n*.wdp\n\n# Graphics Interchange Format\n*.gif\n\n# RAW\n*.raw\n\n# Web P\n*.webp\n\n# Portable Network Graphics\n*.png\n\n# Animated Portable Network Graphics\n*.apng\n\n# Multiple-image Network Graphics\n*.mng\n\n# Tagged Image File Format\n*.tiff\n*.tif\n\n# Scalable Vector Graphics\n*.svg\n*.svgz\n\n# Portable Document Format\n\n# X BitMap\n*.xbm\n\n# BMP\n*.bmp\n*.dib\n\n# ICO\n*.ico\n\n# 3D Images\n*.3dm\n*.max\n\n### Intellij ###\n# Covers JetBrains IDEs: IntelliJ, RubyMine, PhpStorm, AppCode, PyCharm, CLion, Android Studio, WebStorm and Rider\n# Reference: https://intellij-support.jetbrains.com/hc/en-us/articles/206544839\n\n# User-specific stuff\n\n# Generated files\n\n# Sensitive or high-churn files\n\n# Gradle\n\n# Gradle and Maven with auto-import\n# When using Gradle or Maven with auto-import, you should exclude module files,\n# since they will be recreated, and may cause churn.  Uncomment if using\n# auto-import.\n# .idea/artifacts\n# .idea/compiler.xml\n# .idea/jarRepositories.xml\n# .idea/modules.xml\n# .idea/*.iml\n# .idea/modules\n# *.iml\n# *.ipr\n\n# CMake\n\n# Mongo Explorer plugin\n\n# File-based project format\n\n# IntelliJ\n\n# mpeltonen/sbt-idea plugin\n\n# JIRA plugin\n\n# Cursive Clojure plugin\n\n# Crashlytics plugin (for Android Studio and IntelliJ)\n\n# Editor-based Rest Client\n\n# Android studio 3.1+ serialized cache file\n\n### Intellij Patch ###\n# Comment Reason: https://github.com/joeblau/gitignore.io/issues/186#issuecomment-215987721\n\n# *.iml\n# modules.xml\n# .idea/misc.xml\n# *.ipr\n\n# Sonarlint plugin\n# https://plugins.jetbrains.com/plugin/7973-sonarlint\n\n# SonarQube Plugin\n# https://plugins.jetbrains.com/plugin/7238-sonarqube-community-plugin\n\n# Markdown Navigator plugin\n# https://plugins.jetbrains.com/plugin/7896-markdown-navigator-enhanced\n\n# Cache file creation bug\n# See https://youtrack.jetbrains.com/issue/JBR-2257\n\n# CodeStream plugin\n# https://plugins.jetbrains.com/plugin/12206-codestream\n\n### Intellij+all ###\n# Covers JetBrains IDEs: IntelliJ, RubyMine, PhpStorm, AppCode, PyCharm, CLion, Android Studio, WebStorm and Rider\n# Reference: https://intellij-support.jetbrains.com/hc/en-us/articles/206544839\n\n# User-specific stuff\n\n# Generated files\n\n# Sensitive or high-churn files\n\n# Gradle\n\n# Gradle and Maven with auto-import\n# When using Gradle or Maven with auto-import, you should exclude module files,\n# since they will be recreated, and may cause churn.  Uncomment if using\n# auto-import.\n# .idea/artifacts\n# .idea/compiler.xml\n# .idea/jarRepositories.xml\n# .idea/modules.xml\n# .idea/*.iml\n# .idea/modules\n# *.iml\n# *.ipr\n\n# CMake\n\n# Mongo Explorer plugin\n\n# File-based project format\n\n# IntelliJ\n\n# mpeltonen/sbt-idea plugin\n\n# JIRA plugin\n\n# Cursive Clojure plugin\n\n# Crashlytics plugin (for Android Studio and IntelliJ)\n\n# Editor-based Rest Client\n\n# Android studio 3.1+ serialized cache file\n\n### Intellij+all Patch ###\n# Ignores the whole .idea folder and all .iml files\n# See https://github.com/joeblau/gitignore.io/issues/186 and https://github.com/joeblau/gitignore.io/issues/360\n\n.idea/\n\n# Reason: https://github.com/joeblau/gitignore.io/issues/186#issuecomment-249601023\n\nmodules.xml\n.idea/misc.xml\n\n# Sonarlint plugin\n.idea/sonarlint\n\n### Intellij+iml ###\n# Covers JetBrains IDEs: IntelliJ, RubyMine, PhpStorm, AppCode, PyCharm, CLion, Android Studio, WebStorm and Rider\n# Reference: https://intellij-support.jetbrains.com/hc/en-us/articles/206544839\n\n# User-specific stuff\n\n# Generated files\n\n# Sensitive or high-churn files\n\n# Gradle\n\n# Gradle and Maven with auto-import\n# When using Gradle or Maven with auto-import, you should exclude module files,\n# since they will be recreated, and may cause churn.  Uncomment if using\n# auto-import.\n# .idea/artifacts\n# .idea/compiler.xml\n# .idea/jarRepositories.xml\n# .idea/modules.xml\n# .idea/*.iml\n# .idea/modules\n# *.iml\n# *.ipr\n\n# CMake\n\n# Mongo Explorer plugin\n\n# File-based project format\n\n# IntelliJ\n\n# mpeltonen/sbt-idea plugin\n\n# JIRA plugin\n\n# Cursive Clojure plugin\n\n# Crashlytics plugin (for Android Studio and IntelliJ)\n\n# Editor-based Rest Client\n\n# Android studio 3.1+ serialized cache file\n\n### Intellij+iml Patch ###\n# Reason: https://github.com/joeblau/gitignore.io/issues/186#issuecomment-249601023\n\n\n### Java ###\n# Compiled class file\n\n# Log file\n\n# BlueJ files\n*.ctxt\n\n# Mobile Tools for Java (J2ME)\n.mtj.tmp/\n\n# Package Files #\n*.nar\n\n# virtual machine crash logs, see http://www.java.com/en/download/help/error_hotspot.xml\nhs_err_pid*\n\n### Java-Web ###\n## ignoring target file\n\n### JEnv ###\n# JEnv local Java version configuration file\n.java-version\n\n# Used by previous versions of JEnv\n.jenv-version\n\n### JetBrains ###\n# Covers JetBrains IDEs: IntelliJ, RubyMine, PhpStorm, AppCode, PyCharm, CLion, Android Studio, WebStorm and Rider\n# Reference: https://intellij-support.jetbrains.com/hc/en-us/articles/206544839\n\n# User-specific stuff\n\n# Generated files\n\n# Sensitive or high-churn files\n\n# Gradle\n\n# Gradle and Maven with auto-import\n# When using Gradle or Maven with auto-import, you should exclude module files,\n# since they will be recreated, and may cause churn.  Uncomment if using\n# auto-import.\n# .idea/artifacts\n# .idea/compiler.xml\n# .idea/jarRepositories.xml\n# .idea/modules.xml\n# .idea/*.iml\n# .idea/modules\n# *.iml\n# *.ipr\n\n# CMake\n\n# Mongo Explorer plugin\n\n# File-based project format\n\n# IntelliJ\n\n# mpeltonen/sbt-idea plugin\n\n# JIRA plugin\n\n# Cursive Clojure plugin\n\n# Crashlytics plugin (for Android Studio and IntelliJ)\n\n# Editor-based Rest Client\n\n# Android studio 3.1+ serialized cache file\n\n### JetBrains Patch ###\n# Comment Reason: https://github.com/joeblau/gitignore.io/issues/186#issuecomment-215987721\n\n# *.iml\n# modules.xml\n# .idea/misc.xml\n# *.ipr\n\n# Sonarlint plugin\n# https://plugins.jetbrains.com/plugin/7973-sonarlint\n\n# SonarQube Plugin\n# https://plugins.jetbrains.com/plugin/7238-sonarqube-community-plugin\n\n# Markdown Navigator plugin\n# https://plugins.jetbrains.com/plugin/7896-markdown-navigator-enhanced\n\n# Cache file creation bug\n# See https://youtrack.jetbrains.com/issue/JBR-2257\n\n# CodeStream plugin\n# https://plugins.jetbrains.com/plugin/12206-codestream\n\n### JetBrains+all ###\n# Covers JetBrains IDEs: IntelliJ, RubyMine, PhpStorm, AppCode, PyCharm, CLion, Android Studio, WebStorm and Rider\n# Reference: https://intellij-support.jetbrains.com/hc/en-us/articles/206544839\n\n# User-specific stuff\n\n# Generated files\n\n# Sensitive or high-churn files\n\n# Gradle\n\n# Gradle and Maven with auto-import\n# When using Gradle or Maven with auto-import, you should exclude module files,\n# since they will be recreated, and may cause churn.  Uncomment if using\n# auto-import.\n# .idea/artifacts\n# .idea/compiler.xml\n# .idea/jarRepositories.xml\n# .idea/modules.xml\n# .idea/*.iml\n# .idea/modules\n# *.iml\n# *.ipr\n\n# CMake\n\n# Mongo Explorer plugin\n\n# File-based project format\n\n# IntelliJ\n\n# mpeltonen/sbt-idea plugin\n\n# JIRA plugin\n\n# Cursive Clojure plugin\n\n# Crashlytics plugin (for Android Studio and IntelliJ)\n\n# Editor-based Rest Client\n\n# Android studio 3.1+ serialized cache file\n\n### JetBrains+all Patch ###\n# Ignores the whole .idea folder and all .iml files\n# See https://github.com/joeblau/gitignore.io/issues/186 and https://github.com/joeblau/gitignore.io/issues/360\n\n\n# Reason: https://github.com/joeblau/gitignore.io/issues/186#issuecomment-249601023\n\n\n# Sonarlint plugin\n\n### JetBrains+iml ###\n# Covers JetBrains IDEs: IntelliJ, RubyMine, PhpStorm, AppCode, PyCharm, CLion, Android Studio, WebStorm and Rider\n# Reference: https://intellij-support.jetbrains.com/hc/en-us/articles/206544839\n\n# User-specific stuff\n\n# Generated files\n\n# Sensitive or high-churn files\n\n# Gradle\n\n# Gradle and Maven with auto-import\n# When using Gradle or Maven with auto-import, you should exclude module files,\n# since they will be recreated, and may cause churn.  Uncomment if using\n# auto-import.\n# .idea/artifacts\n# .idea/compiler.xml\n# .idea/jarRepositories.xml\n# .idea/modules.xml\n# .idea/*.iml\n# .idea/modules\n# *.iml\n# *.ipr\n\n# CMake\n\n# Mongo Explorer plugin\n\n# File-based project format\n\n# IntelliJ\n\n# mpeltonen/sbt-idea plugin\n\n# JIRA plugin\n\n# Cursive Clojure plugin\n\n# Crashlytics plugin (for Android Studio and IntelliJ)\n\n# Editor-based Rest Client\n\n# Android studio 3.1+ serialized cache file\n\n### JetBrains+iml Patch ###\n# Reason: https://github.com/joeblau/gitignore.io/issues/186#issuecomment-249601023\n\n\n### JMeter ###\n# JMeter common ignore files\n# http://jmeter.apache.org/\n\n# Ignore Summary/Aggregrate reports\n*.jtl\n\n# Ignore log files\n\n# Ignore customized user.properties\nuser.properties\n\n### Emacs ###\n# -*- mode: gitignore; -*-\n\\#*\\#\n/.emacs.desktop\n/.emacs.desktop.lock\n*.elc\nauto-save-list\ntramp\n.\\#*\n\n# Org-mode\n.org-id-locations\n*_archive\n\n# flymake-mode\n*_flymake.*\n\n# eshell files\n/eshell/history\n/eshell/lastdir\n\n# elpa packages\n/elpa/\n\n# reftex files\n*.rel\n\n# AUCTeX auto folder\n/auto/\n\n# cask packages\n.cask/\n\n# Flycheck\nflycheck_*.el\n\n# server auth directory\n/server/\n\n# projectiles files\n.projectile\n\n# directory configuration\n.dir-locals.el\n\n# network security\n/network-security.data\n\n\n### JupyterNotebooks ###\n# gitignore template for Jupyter Notebooks\n# website: http://jupyter.org/\n\n*/.ipynb_checkpoints/*\n\n# IPython\n\n# Remove previous ipynb_checkpoints\n#   git rm -r .ipynb_checkpoints/\n\n\n### Kotlin ###\n# Compiled class file\n\n# Log file\n\n# BlueJ files\n\n# Mobile Tools for Java (J2ME)\n\n# Package Files #\n\n# virtual machine crash logs, see http://www.java.com/en/download/help/error_hotspot.xml\n\n### LAMP ###\n# LAMP Stack Base\n\n### LAMP.Linux Stack ###\n\n# temporary files which can be created if a process still has a handle open of a deleted file\n.fuse_hidden*\n\n# KDE directory preferences\n.directory\n\n# Linux trash folder which might appear on any partition or disk\n.Trash-*\n\n# .nfs files are created when an open file is removed but is still being accessed\n.nfs*\n\n### LAMP.PHP Stack ###\n# Covers JetBrains IDEs: IntelliJ, RubyMine, PhpStorm, AppCode, PyCharm, CLion, Android Studio, WebStorm and Rider\n# Reference: https://intellij-support.jetbrains.com/hc/en-us/articles/206544839\n\n# User-specific stuff\n\n# Generated files\n\n# Sensitive or high-churn files\n\n# Gradle\n\n# Gradle and Maven with auto-import\n# When using Gradle or Maven with auto-import, you should exclude module files,\n# since they will be recreated, and may cause churn.  Uncomment if using\n# auto-import.\n# .idea/artifacts\n# .idea/compiler.xml\n# .idea/jarRepositories.xml\n# .idea/modules.xml\n# .idea/*.iml\n# .idea/modules\n# *.iml\n# *.ipr\n\n# CMake\n\n# Mongo Explorer plugin\n\n# File-based project format\n\n# IntelliJ\n\n# mpeltonen/sbt-idea plugin\n\n# JIRA plugin\n\n# Cursive Clojure plugin\n\n# Crashlytics plugin (for Android Studio and IntelliJ)\n\n# Editor-based Rest Client\n\n# Android studio 3.1+ serialized cache file\n\n### LaTeX ###\n## Core latex/pdflatex auxiliary files:\n*.lof\n*.lot\n*.fls\n*.toc\n*.fmt\n*.fot\n*.cb\n*.cb2\n.*.lb\n\n## Intermediate documents:\n*.dvi\n*.xdv\n*-converted-to.*\n# these rules might exclude image files for figures etc.\n# *.ps\n# *.eps\n# *.pdf\n\n## Generated if empty string is given at \"Please type another file name for output:\"\n.pdf\n\n## Bibliography auxiliary files (bibtex/biblatex/biber):\n*.bbl\n*.bcf\n*.blg\n*-blx.aux\n*-blx.bib\n*.run.xml\n\n## Build tool auxiliary files:\n*.fdb_latexmk\n*.synctex\n*.synctex(busy)\n*.synctex.gz\n*.synctex.gz(busy)\n*.pdfsync\n\n## Build tool directories for auxiliary files\n# latexrun\nlatex.out/\n\n## Auxiliary and intermediate files from other packages:\n# algorithms\n*.alg\n*.loa\n\n# achemso\nacs-*.bib\n\n# amsthm\n*.thm\n\n# beamer\n*.nav\n*.pre\n*.snm\n*.vrb\n\n# changes\n*.soc\n\n# comment\n*.cut\n\n# cprotect\n\n# elsarticle (documentclass of Elsevier journals)\n*.spl\n\n# endnotes\n*.ent\n\n# fixme\n*.lox\n\n# feynmf/feynmp\n*.mf\n*.mp\n*.t[1-9]\n*.t[1-9][0-9]\n*.tfm\n\n#(r)(e)ledmac/(r)(e)ledpar\n*.end\n*.?end\n*.[1-9]\n*.[1-9][0-9]\n*.[1-9][0-9][0-9]\n*.[1-9]R\n*.[1-9][0-9]R\n*.[1-9][0-9][0-9]R\n*.eledsec[1-9]\n*.eledsec[1-9]R\n*.eledsec[1-9][0-9]\n*.eledsec[1-9][0-9]R\n*.eledsec[1-9][0-9][0-9]\n*.eledsec[1-9][0-9][0-9]R\n\n# glossaries\n*.acn\n*.acr\n*.glg\n*.glo\n*.gls\n*.glsdefs\n*.lzs\n\n# uncomment this for glossaries-extra (will ignore makeindex's style files!)\n# *.ist\n\n# gnuplottex\n*-gnuplottex-*\n\n# gregoriotex\n*.gaux\n*.gtex\n\n# htlatex\n*.4ct\n*.4tc\n*.idv\n*.lg\n*.trc\n*.xref\n\n# hyperref\n*.brf\n\n# knitr\n*-concordance.tex\n# TODO Comment the next line if you want to keep your tikz graphics files\n*.tikz\n*-tikzDictionary\n\n# listings\n*.lol\n\n# luatexja-ruby\n*.ltjruby\n\n# makeidx\n*.idx\n*.ilg\n*.ind\n\n# minitoc\n*.maf\n*.mlf\n*.mlt\n*.mtc\n*.mtc[0-9]*\n*.slf[0-9]*\n*.slt[0-9]*\n*.stc[0-9]*\n\n# minted\n_minted*\n*.pyg\n\n# morewrites\n*.mw\n\n# nomencl\n*.nlg\n*.nlo\n*.nls\n\n# pax\n*.pax\n\n# pdfpcnotes\n*.pdfpc\n\n# sagetex\n*.sagetex.sage\n*.sagetex.py\n*.sagetex.scmd\n\n# scrwfile\n*.wrt\n\n# sympy\n*.sout\n*.sympy\nsympy-plots-for-*.tex/\n\n# pdfcomment\n*.upa\n*.upb\n\n# pythontex\n*.pytxcode\npythontex-files-*/\n\n# tcolorbox\n*.listing\n\n# thmtools\n*.loe\n\n# TikZ & PGF\n*.dpth\n*.md5\n*.auxlock\n\n# todonotes\n*.tdo\n\n# vhistory\n*.hst\n*.ver\n\n# easy-todo\n*.lod\n\n# xcolor\n*.xcp\n\n# xmpincl\n*.xmpi\n\n# xindy\n*.xdy\n\n# xypic precompiled matrices and outlines\n*.xyc\n*.xyd\n\n# endfloat\n*.ttt\n*.fff\n\n# Latexian\nTSWLatexianTemp*\n\n## Editors:\n# WinEdt\n*.sav\n\n# Texpad\n.texpadtmp\n\n# LyX\n*.lyx~\n\n# Kile\n*.backup\n\n# gummi\n.*.swp\n\n# KBibTeX\n*~[0-9]*\n\n# TeXnicCenter\n*.tps\n\n# auto folder when using emacs and auctex\n./auto/*\n*.el\n\n# expex forward references with \\gathertags\n*-tags.tex\n\n# standalone packages\n*.sta\n\n# Makeindex log files\n*.lpz\n\n# REVTeX puts footnotes in the bibliography by default, unless the nofootinbib\n# option is specified. Footnotes are the stored in a file with suffix Notes.bib.\n# Uncomment the next line to have this generated file ignored.\n#*Notes.bib\n\n### LaTeX Patch ###\n# LIPIcs / OASIcs\n*.vtc\n\n# glossaries\n*.glstex\n\n### Less ###\n*.less\n\n### Linux ###\n\n# temporary files which can be created if a process still has a handle open of a deleted file\n\n# KDE directory preferences\n\n# Linux trash folder which might appear on any partition or disk\n\n# .nfs files are created when an open file is removed but is still being accessed\n\n### Lua ###\n# Compiled Lua sources\nluac.out\n\n# luarocks build files\n*.src.rock\n\n# Object files\n*.os\n\n# Precompiled Headers\n\n# Libraries\n*.def\n\n# Shared objects (inc. Windows DLLs)\n\n# Executables\n\n\n### macOS ###\n# General\n.DS_Store\n.AppleDouble\n.LSOverride\n\n# Icon must end with two \\r\nIcon\n\n# Thumbnails\n._*\n\n# Files that might appear in the root of a volume\n.DocumentRevisions-V100\n.fseventsd\n.Spotlight-V100\n.TemporaryItems\n.Trashes\n.VolumeIcon.icns\n.com.apple.timemachine.donotpresent\n\n# Directories potentially created on remote AFP share\n.AppleDB\n.AppleDesktop\nNetwork Trash Folder\nTemporary Items\n.apdisk\n\n### MATLAB ###\n# Windows default autosave extension\n*.asv\n\n# OSX / *nix default autosave extension\n*.m~\n\n# Compiled MEX binaries (all platforms)\n*.mex*\n\n# Packaged app and toolbox files\n*.mlappinstall\n*.mltbx\n\n# Generated helpsearch folders\nhelpsearch*/\n\n# Simulink code generation folders\nslprj/\nsccprj/\n\n# Matlab code generation folders\ncodegen/\n\n# Simulink autosave extension\n*.autosave\n\n# Simulink cache files\n*.slxc\n\n# Octave session info\noctave-workspace\n\n### Maven ###\npom.xml.tag\npom.xml.releaseBackup\npom.xml.versionsBackup\npom.xml.next\nrelease.properties\ndependency-reduced-pom.xml\nbuildNumber.properties\n.mvn/timing.properties\n# https://github.com/takari/maven-wrapper#usage-without-binary-jar\n.mvn/wrapper/maven-wrapper.jar\n\n### Mercurial ###\n.hg/\n.hgignore\n.hgsigs\n.hgsub\n.hgsubstate\n.hgtags\n\n### MicrosoftOffice ###\n\n# Word temporary\n~$*.doc*\n\n# Word Auto Backup File\nBackup of *.doc*\n\n# Excel temporary\n~$*.xls*\n\n# Excel Backup File\n*.xlk\n\n# PowerPoint temporary\n~$*.ppt*\n\n# Visio autosave temporary files\n*.~vsd*\n\n### Node ###\n# Logs\nlogs\nnpm-debug.log*\nyarn-debug.log*\nyarn-error.log*\nlerna-debug.log*\n\n# Diagnostic reports (https://nodejs.org/api/report.html)\nreport.[0-9]*.[0-9]*.[0-9]*.[0-9]*.json\n\n# Runtime data\npids\n*.pid\n*.seed\n*.pid.lock\n\n# Directory for instrumented libs generated by jscoverage/JSCover\nlib-cov\n\n# Coverage directory used by tools like istanbul\ncoverage\n*.lcov\n\n# nyc test coverage\n.nyc_output\n\n# Grunt intermediate storage (https://gruntjs.com/creating-plugins#storing-task-files)\n\n# Bower dependency directory (https://bower.io/)\nbower_components\n\n# node-waf configuration\n.lock-wscript\n\n# Compiled binary addons (https://nodejs.org/api/addons.html)\nbuild/Release\n\n# Dependency directories\nnode_modules/\njspm_packages/\n\n# TypeScript v1 declaration files\ntypings/\n\n# TypeScript cache\n*.tsbuildinfo\n\n# Optional npm cache directory\n.npm\n\n# Optional eslint cache\n.eslintcache\n\n# Microbundle cache\n.rpt2_cache/\n.rts2_cache_cjs/\n.rts2_cache_es/\n.rts2_cache_umd/\n\n# Optional REPL history\n.node_repl_history\n\n# Output of 'npm pack'\n\n# Yarn Integrity file\n.yarn-integrity\n\n# dotenv environment variables file\n.env.test\n.env*.local\n\n# parcel-bundler cache (https://parceljs.org/)\n.parcel-cache\n\n# Next.js build output\n.next\n\n# Nuxt.js build / generate output\n.nuxt\n\n# Gatsby files\n.cache/\n# Comment in the public line in if your project uses Gatsby and not Next.js\n# https://nextjs.org/blog/next-9-1#public-directory-support\n# public\n\n# vuepress build output\n.vuepress/dist\n\n# Serverless directories\n.serverless/\n\n# FuseBox cache\n.fusebox/\n\n# DynamoDB Local files\n.dynamodb/\n\n# TernJS port file\n.tern-port\n\n# Stores VSCode versions used for testing VSCode extensions\n.vscode-test\n\n### Octave ###\n# Windows default autosave extension\n\n# OSX / *nix default autosave extension\n\n# Compiled MEX binaries (all platforms)\n\n# Packaged app and toolbox files\n\n# Generated helpsearch folders\n\n# Simulink code generation folders\n\n# Matlab code generation folders\n\n# Simulink autosave extension\n\n# Simulink cache files\n\n# Octave session info\n\n### OSX ###\n# General\n\n# Icon must end with two \\r\n\n# Thumbnails\n\n# Files that might appear in the root of a volume\n\n# Directories potentially created on remote AFP share\n\n### Packer ###\n# Cache objects\npacker_cache/\n\n# Crash log\ncrash.log\n\n# For built boxes\n*.box\n\n### Patch ###\n\n### Perl ###\n!Build/\n.last_cover_stats\n/META.yml\n/META.json\n/MYMETA.*\n*.pm.tdy\n*.bs\n\n# Devel::Cover\ncover_db/\n\n# Devel::NYTProf\nnytprof.out\n\n# Dizt::Zilla\n/.build/\n\n# Module::Build\nBuild\nBuild.bat\n\n# Module::Install\ninc/\n\n# ExtUtils::MakeMaker\n/blib/\n/_eumm/\n/*.gz\n# XXX: always want to commit Makefiles\n#/Makefile\n/Makefile.old\n/MANIFEST.bak\n/pm_to_blib\n\n### Perl6 ###\n# Gitignore for Perl 6 (http://www.perl6.org)\n# As part of https://github.com/github/gitignore\n\n# precompiled files\n.precomp\nlib/.precomp\n\n\n### PHPUnit ###\n# Covers PHPUnit\n# Reference: https://phpunit.de/\n\n# Generated files\n.phpunit.result.cache\n\n# PHPUnit\n/app/phpunit.xml\n/phpunit.xml\n\n# Build data\n/build/\n\n### PowerShell ###\n# Exclude packaged modules\n\n# Exclude .NET assemblies from source\n\n### Puppet ###\n# gitignore template for Puppet modules\n# website: https://forge.puppet.com/\n\n# Built packages\npkg/*\n\n# Should run on multiple platforms so don't check in\nGemfile.lock\n\n# Tests\nspec/fixtures/*\ncoverage/*\n\n# Third-party\nvendor/*\n\n### PuTTY ###\n# Private key\n*.ppk\n\n### PyCharm ###\n# Covers JetBrains IDEs: IntelliJ, RubyMine, PhpStorm, AppCode, PyCharm, CLion, Android Studio, WebStorm and Rider\n# Reference: https://intellij-support.jetbrains.com/hc/en-us/articles/206544839\n\n# User-specific stuff\n\n# Generated files\n\n# Sensitive or high-churn files\n\n# Gradle\n\n# Gradle and Maven with auto-import\n# When using Gradle or Maven with auto-import, you should exclude module files,\n# since they will be recreated, and may cause churn.  Uncomment if using\n# auto-import.\n# .idea/artifacts\n# .idea/compiler.xml\n# .idea/jarRepositories.xml\n# .idea/modules.xml\n# .idea/*.iml\n# .idea/modules\n# *.iml\n# *.ipr\n\n# CMake\n\n# Mongo Explorer plugin\n\n# File-based project format\n\n# IntelliJ\n\n# mpeltonen/sbt-idea plugin\n\n# JIRA plugin\n\n# Cursive Clojure plugin\n\n# Crashlytics plugin (for Android Studio and IntelliJ)\n\n# Editor-based Rest Client\n\n# Android studio 3.1+ serialized cache file\n\n### PyCharm Patch ###\n# Comment Reason: https://github.com/joeblau/gitignore.io/issues/186#issuecomment-215987721\n\n# *.iml\n# modules.xml\n# .idea/misc.xml\n# *.ipr\n\n# Sonarlint plugin\n# https://plugins.jetbrains.com/plugin/7973-sonarlint\n\n# SonarQube Plugin\n# https://plugins.jetbrains.com/plugin/7238-sonarqube-community-plugin\n\n# Markdown Navigator plugin\n# https://plugins.jetbrains.com/plugin/7896-markdown-navigator-enhanced\n\n# Cache file creation bug\n# See https://youtrack.jetbrains.com/issue/JBR-2257\n\n# CodeStream plugin\n# https://plugins.jetbrains.com/plugin/12206-codestream\n\n### PyCharm+all ###\n# Covers JetBrains IDEs: IntelliJ, RubyMine, PhpStorm, AppCode, PyCharm, CLion, Android Studio, WebStorm and Rider\n# Reference: https://intellij-support.jetbrains.com/hc/en-us/articles/206544839\n\n# User-specific stuff\n\n# Generated files\n\n# Sensitive or high-churn files\n\n# Gradle\n\n# Gradle and Maven with auto-import\n# When using Gradle or Maven with auto-import, you should exclude module files,\n# since they will be recreated, and may cause churn.  Uncomment if using\n# auto-import.\n# .idea/artifacts\n# .idea/compiler.xml\n# .idea/jarRepositories.xml\n# .idea/modules.xml\n# .idea/*.iml\n# .idea/modules\n# *.iml\n# *.ipr\n\n# CMake\n\n# Mongo Explorer plugin\n\n# File-based project format\n\n# IntelliJ\n\n# mpeltonen/sbt-idea plugin\n\n# JIRA plugin\n\n# Cursive Clojure plugin\n\n# Crashlytics plugin (for Android Studio and IntelliJ)\n\n# Editor-based Rest Client\n\n# Android studio 3.1+ serialized cache file\n\n### PyCharm+all Patch ###\n# Ignores the whole .idea folder and all .iml files\n# See https://github.com/joeblau/gitignore.io/issues/186 and https://github.com/joeblau/gitignore.io/issues/360\n\n\n# Reason: https://github.com/joeblau/gitignore.io/issues/186#issuecomment-249601023\n\n\n# Sonarlint plugin\n\n### PyCharm+iml ###\n# Covers JetBrains IDEs: IntelliJ, RubyMine, PhpStorm, AppCode, PyCharm, CLion, Android Studio, WebStorm and Rider\n# Reference: https://intellij-support.jetbrains.com/hc/en-us/articles/206544839\n\n# User-specific stuff\n\n# Generated files\n\n# Sensitive or high-churn files\n\n# Gradle\n\n# Gradle and Maven with auto-import\n# When using Gradle or Maven with auto-import, you should exclude module files,\n# since they will be recreated, and may cause churn.  Uncomment if using\n# auto-import.\n# .idea/artifacts\n# .idea/compiler.xml\n# .idea/jarRepositories.xml\n# .idea/modules.xml\n# .idea/*.iml\n# .idea/modules\n# *.iml\n# *.ipr\n\n# CMake\n\n# Mongo Explorer plugin\n\n# File-based project format\n\n# IntelliJ\n\n# mpeltonen/sbt-idea plugin\n\n# JIRA plugin\n\n# Cursive Clojure plugin\n\n# Crashlytics plugin (for Android Studio and IntelliJ)\n\n# Editor-based Rest Client\n\n# Android studio 3.1+ serialized cache file\n\n### PyCharm+iml Patch ###\n# Reason: https://github.com/joeblau/gitignore.io/issues/186#issuecomment-249601023\n\n\n### pydev ###\n.pydevproject\n\n### Python ###\n# Byte-compiled / optimized / DLL files\n\n# C extensions\n\n# Distribution / packaging\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n\n# Installer logs\n\n# Unit test / coverage reports\n\n# Translations\n\n# Django stuff:\n\n# Flask stuff:\n\n# Scrapy stuff:\n\n# Sphinx documentation\n\n# PyBuilder\n\n# Jupyter Notebook\n\n# IPython\n\n# pyenv\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow\n\n# Celery stuff\n\n# SageMath parsed files\n\n# Environments\n\n# Spyder project settings\n\n# Rope project settings\n\n# mkdocs documentation\n\n# mypy\n\n# Pyre type checker\n\n# pytype static type analyzer\n\n# profiling data\n\n### R ###\n# History files\n.Rhistory\n.Rapp.history\n\n# Session Data files\n.RData\n\n# User-specific files\n.Ruserdata\n\n# Example code in package build process\n*-Ex.R\n\n# Output files from R CMD build\n/*.tar.gz\n\n# Output files from R CMD check\n/*.Rcheck/\n\n# RStudio files\n.Rproj.user/\n\n# produced vignettes\nvignettes/*.html\nvignettes/*.pdf\n\n# OAuth2 token, see https://github.com/hadley/httr/releases/tag/v0.3\n.httr-oauth\n\n# knitr and R markdown default cache directories\n*_cache/\n/cache/\n\n# Temporary files created by R markdown\n*.utf8.md\n*.knit.md\n\n# R Environment Variables\n.Renviron\n\n### R.Bookdown Stack ###\n# R package: bookdown caching files\n/*_files/\n\n### Rails ###\n*.rbc\ncapybara-*.html\n.rspec\n/db/*.sqlite3\n/db/*.sqlite3-journal\n/db/*.sqlite3-[0-9]*\n/public/system\n/coverage/\n/spec/tmp\nrerun.txt\npickle-email-*.html\n\n# Ignore all logfiles and tempfiles.\n/log/*\n/tmp/*\n!/log/.keep\n!/tmp/.keep\n\n# TODO Comment out this rule if you are OK with secrets being uploaded to the repo\nconfig/initializers/secret_token.rb\nconfig/master.key\n\n# Only include if you have production secrets in this file, which is no longer a Rails default\n# config/secrets.yml\n\n# dotenv, dotenv-rails\n# TODO Comment out these rules if environment variables can be committed\n.env.*\n\n## Environment normalization:\n/.bundle\n/vendor/bundle\n\n# these should all be checked in to normalize the environment:\n# Gemfile.lock, .ruby-version, .ruby-gemset\n\n# unless supporting rvm < 1.11.0 or doing something fancy, ignore this:\n.rvmrc\n\n# if using bower-rails ignore default bower_components path bower.json files\n/vendor/assets/bower_components\n*.bowerrc\nbower.json\n\n# Ignore pow environment settings\n.powenv\n\n# Ignore Byebug command history file.\n.byebug_history\n\n# Ignore node_modules\n\n# Ignore precompiled javascript packs\n/public/packs\n/public/packs-test\n/public/assets\n\n# Ignore yarn files\n/yarn-error.log\n\n# Ignore uploaded files in development\n/storage/*\n!/storage/.keep\n\n### react ###\n.DS_*\n**/*.backup.*\n**/*.back.*\n\n\n*.sublime*\n\npsd\nthumb\nsketch\n\n### ReactNative ###\n# React Native Stack Base\n\n.expo\n__generated__\n\n### ReactNative.Android Stack ###\n# Built application files\n*.aar\n*.ap_\n*.aab\n\n# Files for the ART/Dalvik VM\n*.dex\n\n# Java class files\n\n# Generated files\ngen/\n#  Uncomment the following line in case you need and you don't have the release build type files in your app\n# release/\n\n# Gradle files\n\n# Local configuration file (sdk path, etc)\n\n# Proguard folder generated by Eclipse\nproguard/\n\n# Log Files\n\n# Android Studio Navigation editor temp files\n.navigation/\n\n# Android Studio captures folder\ncaptures/\n\n# IntelliJ\n.idea/workspace.xml\n.idea/tasks.xml\n.idea/gradle.xml\n.idea/assetWizardSettings.xml\n.idea/dictionaries\n.idea/libraries\n# Android Studio 3 in .gitignore file.\n.idea/caches\n.idea/modules.xml\n# Comment next line if keeping position of elements in Navigation Editor is relevant for you\n.idea/navEditor.xml\n\n# Keystore files\n# Uncomment the following lines if you do not want to check your keystore files in.\n#*.jks\n#*.keystore\n\n# External native build folder generated in Android Studio 2.2 and later\n.externalNativeBuild\n.cxx/\n\n# Google Services (e.g. APIs or Firebase)\n# google-services.json\n\n# Freeline\nfreeline.py\nfreeline/\nfreeline_project_description.json\n\n# fastlane\nfastlane/report.xml\nfastlane/Preview.html\nfastlane/screenshots\nfastlane/test_output\nfastlane/readme.md\n\n# Version control\nvcs.xml\n\n# lint\nlint/intermediates/\nlint/generated/\nlint/outputs/\nlint/tmp/\n# lint/reports/\n\n### ReactNative.Buck Stack ###\nbuck-out/\n.buckconfig.local\n.buckd/\n.buckversion\n.fakebuckversion\n\n### ReactNative.Gradle Stack ###\n.gradle\n\n# Ignore Gradle GUI config\ngradle-app.setting\n\n# Avoid ignoring Gradle wrapper jar file (.jar files are usually ignored)\n!gradle-wrapper.jar\n\n# Cache of project\n.gradletasknamecache\n\n# # Work around https://youtrack.jetbrains.com/issue/IDEA-116898\n# gradle/wrapper/gradle-wrapper.properties\n\n### ReactNative.Linux Stack ###\n\n# temporary files which can be created if a process still has a handle open of a deleted file\n\n# KDE directory preferences\n\n# Linux trash folder which might appear on any partition or disk\n\n# .nfs files are created when an open file is removed but is still being accessed\n\n### ReactNative.Node Stack ###\n# Logs\n\n# Diagnostic reports (https://nodejs.org/api/report.html)\n\n# Runtime data\n\n# Directory for instrumented libs generated by jscoverage/JSCover\n\n# Coverage directory used by tools like istanbul\n\n# nyc test coverage\n\n# Grunt intermediate storage (https://gruntjs.com/creating-plugins#storing-task-files)\n\n# Bower dependency directory (https://bower.io/)\n\n# node-waf configuration\n\n# Compiled binary addons (https://nodejs.org/api/addons.html)\n\n# Dependency directories\n\n# TypeScript v1 declaration files\n\n# TypeScript cache\n\n# Optional npm cache directory\n\n# Optional eslint cache\n\n# Microbundle cache\n\n# Optional REPL history\n\n# Output of 'npm pack'\n\n# Yarn Integrity file\n\n# dotenv environment variables file\n\n# parcel-bundler cache (https://parceljs.org/)\n\n# Next.js build output\n\n# Nuxt.js build / generate output\n\n# Gatsby files\n# Comment in the public line in if your project uses Gatsby and not Next.js\n# https://nextjs.org/blog/next-9-1#public-directory-support\n# public\n\n# vuepress build output\n\n# Serverless directories\n\n# FuseBox cache\n\n# DynamoDB Local files\n\n# TernJS port file\n\n# Stores VSCode versions used for testing VSCode extensions\n\n### ReactNative.Xcode Stack ###\n# Xcode\n#\n# gitignore contributors: remember to update Global/Xcode.gitignore, Objective-C.gitignore & Swift.gitignore\n\n## User settings\nxcuserdata/\n\n## compatibility with Xcode 8 and earlier (ignoring not required starting Xcode 9)\n*.xcscmblueprint\n*.xccheckout\n\n## compatibility with Xcode 3 and earlier (ignoring not required starting Xcode 4)\nDerivedData/\n*.moved-aside\n*.pbxuser\n!default.pbxuser\n*.mode1v3\n!default.mode1v3\n*.mode2v3\n!default.mode2v3\n*.perspectivev3\n!default.perspectivev3\n\n## Gcc Patch\n/*.gcno\n\n### ReactNative.macOS Stack ###\n# General\n\n# Icon must end with two \\r\nIcon\n\n# Thumbnails\n\n# Files that might appear in the root of a volume\n\n# Directories potentially created on remote AFP share\n\n### Redis ###\n# Ignore redis binary dump (dump.rdb) files\n\n*.rdb\n\n### ROOT ###\n# ROOT Home Page : https://root.cern.ch/\n# ROOT Used by Experimental Physicists, not necessarily HEP\n# ROOT based on C++\n\n# Files generated by ROOT, observed with v6.xy\n\n*.pcm\n\n\n\n### Ruby ###\n/.config\n/InstalledFiles\n/pkg/\n/spec/reports/\n/spec/examples.txt\n/test/tmp/\n/test/version_tmp/\n/tmp/\n\n# Used by dotenv library to load environment variables.\n# .env\n\n# Ignore Byebug command history file.\n\n## Specific to RubyMotion:\n.dat*\n.repl_history\n*.bridgesupport\nbuild-iPhoneOS/\nbuild-iPhoneSimulator/\n\n## Specific to RubyMotion (use of CocoaPods):\n# We recommend against adding the Pods directory to your .gitignore. However\n# you should judge for yourself, the pros and cons are mentioned at:\n# https://guides.cocoapods.org/using/using-cocoapods.html#should-i-check-the-pods-directory-into-source-control\n# vendor/Pods/\n\n## Documentation cache and generated files:\n/.yardoc/\n/_yardoc/\n/doc/\n/rdoc/\n\n/.bundle/\n/lib/bundler/man/\n\n# for a library or gem, you might want to ignore these files since the code is\n# intended to run in multiple environments; otherwise, check them in:\n# Gemfile.lock\n# .ruby-version\n# .ruby-gemset\n\n# unless supporting rvm < 1.11.0 or doing something fancy, ignore this:\n\n# Used by RuboCop. Remote config files pulled in from inherit_from directive.\n# .rubocop-https?--*\n\n### Ruby Patch ###\n# Used by RuboCop. Remote config files pulled in from inherit_from directive.\n# .rubocop-https?--*\n\n### Rust ###\n# Generated by Cargo\n# will have compiled files and executables\n\n# Remove Cargo.lock from gitignore if creating an executable, leave it for libraries\n# More information here https://doc.rust-lang.org/cargo/guide/cargo-toml-vs-cargo-lock.html\nCargo.lock\n\n### SBT ###\n# Simple Build Tool\n# http://www.scala-sbt.org/release/docs/Getting-Started/Directories.html#configuring-version-control\n\ndist/*\nlib_managed/\nsrc_managed/\nproject/boot/\nproject/plugins/project/\n.history\n.lib/\n\n### Scala ###\n\n### Serverless ###\n# Ignore build directory\n.serverless\n\n### Sonar ###\n#Sonar generated dir\n/.sonar/\n\n### SonarQube ###\n# SonarQube ignore files.\n# https://docs.sonarqube.org/display/SCAN/Analyzing+with+SonarQube+Scanner\n# Sonar Scanner working directories\n.sonar/\n.scannerwork/\n\n# http://www.sonarlint.org/commandline/\n# SonarLint working directories, configuration files (including credentials)\n.sonarlint/\n\n### Spark ###\n*#*#\n*.#*\n*.pyc\n*.pyo\n.ensime\n.ensime_cache/\n.ensime_lucene\n.generated-mima*\nR-unit-tests.log\nR/unit-tests.out\nR/cran-check.out\nR/pkg/vignettes/sparkr-vignettes.html\nR/pkg/tests/fulltests/Rplots.pdf\nbuild/*.jar\nbuild/apache-maven*\nbuild/scala*\nbuild/zinc*\ncache\ncheckpoint\nconf/*.cmd\nconf/*.conf\nconf/*.properties\nconf/*.sh\nconf/*.xml\nconf/java-opts\nconf/slaves\nderby.log\ndev/create-release/*final\ndev/create-release/*txt\ndev/pr-deps/\ndocs/_site\ndocs/api\nsql/docs\nsql/site\nlint-r-report.log\nlog/\nlogs/\nproject/build/target/\nproject/plugins/lib_managed/\nproject/plugins/project/build.properties\nproject/plugins/src_managed/\nproject/plugins/target/\npython/lib/pyspark.zip\npython/deps\npython/test_coverage/coverage_data\npython/test_coverage/htmlcov\npython/pyspark/python\nreports/\nscalastyle-on-compile.generated.xml\nscalastyle-output.xml\nscalastyle.txt\nspark-*-bin-*.tgz\nspark-tests.log\nstreaming-tests.log\nunit-tests.log\nwork/\ndocs/.jekyll-metadata\n\n# For Hive\nTempStatsStore/\nmetastore/\nmetastore_db/\nsql/hive-thriftserver/test_warehouses\nwarehouse/\nspark-warehouse/\n\n# For R session data\n.RHistory\n*.Rproj\n*.Rproj.*\n\n.Rproj.user\n\n# For SBT\n.jvmopts\n\n\n### Splunk ###\n# gitignore template for Splunk apps\n# documentation: http://docs.splunk.com/Documentation/Splunk/6.2.3/admin/Defaultmetaconf\n\n# Splunk local meta file\nlocal.meta\n\n# Splunk local folder\nlocal\n\n### Spreadsheet ###\n*.xlr\n*.xls\n*.xlsx\n\n### SSH ###\n**/.ssh/id_*\n**/.ssh/*_id_*\n**/.ssh/known_hosts\n\n### SublimeText ###\n# Cache files for Sublime Text\n*.tmlanguage.cache\n*.tmPreferences.cache\n*.stTheme.cache\n\n# Workspace files are user-specific\n*.sublime-workspace\n\n# Project files should be checked into the repository, unless a significant\n# proportion of contributors will probably not be using Sublime Text\n# *.sublime-project\n\n# SFTP configuration file\nsftp-config.json\n\n# Package control specific files\nPackage Control.last-run\nPackage Control.ca-list\nPackage Control.ca-bundle\nPackage Control.system-ca-bundle\nPackage Control.cache/\nPackage Control.ca-certs/\nPackage Control.merged-ca-bundle\nPackage Control.user-ca-bundle\noscrypto-ca-bundle.crt\nbh_unicode_properties.cache\n\n# Sublime-github package stores a github token in this file\n# https://packagecontrol.io/packages/sublime-github\nGitHub.sublime-settings\n\n### SVN ###\n.svn/\n\n### Terraform ###\n# Local .terraform directories\n**/.terraform/*\n\n# .tfstate files\n*.tfstate\n*.tfstate.*\n\n# Crash log files\n\n# Ignore any .tfvars files that are generated automatically for each Terraform run. Most\n# .tfvars files are managed as part of configuration and so should be included in\n# version control.\n# example.tfvars\n\n# Ignore override files as they are usually used to override resources locally and so\n# are not checked in\noverride.tf\noverride.tf.json\n*_override.tf\n*_override.tf.json\n\n# Include override files you do wish to add to version control using negated pattern\n# !example_override.tf\n\n# Include tfplan files to ignore the plan output of command: terraform plan -out=tfplan\n# example: *tfplan*\n\n### Terragrunt ###\n# terragrunt cache directories\n**/.terragrunt-cache/*\n\n### TortoiseGit ###\n# Project-level settings\n/.tgitconfig\n\n### Vagrant ###\n# General\n.vagrant/\n\n# Log files (if you are creating logs in debug mode, uncomment this)\n# *.log\n\n### Vagrant Patch ###\n\n### venv ###\n# Virtualenv\n# http://iamzed.com/2009/05/07/a-primer-on-virtualenv/\n# XXX: prevents committing scripts at ~/bin\n#[Bb]in\n[Ii]nclude\n# XXX: prevents committing submodule lib\n#[Ll]ib\n[Ll]ib64\n[Ll]ocal\n#[Ss]cripts\npyvenv.cfg\npip-selfcheck.json\n\n### VirtualEnv ###\n# Virtualenv\n# http://iamzed.com/2009/05/07/a-primer-on-virtualenv/\n\n### Julia ###\n# Files generated by invoking Julia with --code-coverage\n*.jl.cov\n*.jl.*.cov\n\n# Files generated by invoking Julia with --track-allocation\n*.jl.mem\n\n# System-specific files and directories generated by the BinaryProvider and BinDeps packages\n# They contain absolute paths specific to the host computer, and so should not be committed\ndeps/deps.jl\ndeps/build.log\ndeps/downloads/\ndeps/usr/\ndeps/src/\n\n# Build artifacts for creating documentation generated by the Documenter package\ndocs/build/\ndocs/site/\n\n# File generated by Pkg, the package manager, based on a corresponding Project.toml\n# It records a fixed state of all packages used by the project. As such, it should not be\n# committed for packages, but should be committed for applications that require a static\n# environment.\nManifest.toml\n\n### VisualStudioCode ###\n\n### VisualStudioCode Patch ###\n# Ignore all local history of files\n.ionide\n\n### vs ###\n## Ignore Visual Studio temporary files, build results, and\n## files generated by popular Visual Studio add-ons.\n##\n## Get latest from https://github.com/github/gitignore/blob/master/VisualStudio.gitignore\n\n# User-specific files\n*.rsuser\n*.suo\n*.user\n*.userosscache\n*.sln.docstates\n\n# User-specific files (MonoDevelop/Xamarin Studio)\n*.userprefs\n\n# Mono auto generated files\nmono_crash.*\n\n# Build results\n[Dd]ebug/\n[Dd]ebugPublic/\n[Rr]elease/\n[Rr]eleases/\nx64/\nx86/\n[Aa][Rr][Mm]/\n[Aa][Rr][Mm]64/\nbld/\n# XXX: prevents committing scripts at ~/bin\n#[Bb]in/\n[Oo]bj/\n[Ll]og/\n[Ll]ogs/\n\n# Visual Studio 2015/2017 cache/options directory\n.vs/\n# Uncomment if you have tasks that create the project's static files in wwwroot\n#wwwroot/\n\n# Visual Studio 2017 auto generated files\nGenerated\\ Files/\n\n# MSTest test Results\n[Tt]est[Rr]esult*/\n[Bb]uild[Ll]og.*\n\n# NUnit\n*.VisualState.xml\nTestResult.xml\nnunit-*.xml\n\n# Build Results of an ATL Project\n[Dd]ebugPS/\n[Rr]eleasePS/\ndlldata.c\n\n# Benchmark Results\nBenchmarkDotNet.Artifacts/\n\n# .NET Core\nproject.lock.json\nproject.fragment.lock.json\nartifacts/\n\n# StyleCop\nStyleCopReport.xml\n\n# Files built by Visual Studio\n*_i.c\n*_p.c\n*_h.h\n*.meta\n*.iobj\n*.ipdb\n*.pgc\n*.pgd\n*.rsp\n*.sbr\n*.tlb\n*.tli\n*.tlh\n*.tmp_proj\n*_wpftmp.csproj\n*.vspscc\n*.vssscc\n.builds\n*.pidb\n*.svclog\n*.scc\n\n# Chutzpah Test files\n_Chutzpah*\n\n# Visual C++ cache files\nipch/\n*.aps\n*.ncb\n*.opendb\n*.opensdf\n*.cachefile\n*.VC.db\n*.VC.VC.opendb\n\n# Visual Studio profiler\n*.psess\n*.vsp\n*.vspx\n*.sap\n\n# Visual Studio Trace Files\n*.e2e\n\n# TFS 2012 Local Workspace\n$tf/\n\n# Guidance Automation Toolkit\n*.gpState\n\n# ReSharper is a .NET coding add-in\n_ReSharper*/\n*.[Rr]e[Ss]harper\n*.DotSettings.user\n\n# TeamCity is a build add-in\n_TeamCity*\n\n# DotCover is a Code Coverage Tool\n*.dotCover\n\n# AxoCover is a Code Coverage Tool\n.axoCover/*\n!.axoCover/settings.json\n\n# Coverlet is a free, cross platform Code Coverage Tool\ncoverage*[.json, .xml, .info]\n\n# Visual Studio code coverage results\n*.coverage\n*.coveragexml\n\n# NCrunch\n_NCrunch_*\n.*crunch*.local.xml\nnCrunchTemp_*\n\n# MightyMoose\n*.mm.*\nAutoTest.Net/\n\n# Web workbench (sass)\n.sass-cache/\n\n# Installshield output folder\n[Ee]xpress/\n\n# DocProject is a documentation generator add-in\nDocProject/buildhelp/\nDocProject/Help/*.HxT\nDocProject/Help/*.HxC\nDocProject/Help/*.hhc\nDocProject/Help/*.hhk\nDocProject/Help/*.hhp\nDocProject/Help/Html2\nDocProject/Help/html\n\n# Click-Once directory\npublish/\n\n# Publish Web Output\n*.[Pp]ublish.xml\n*.azurePubxml\n# Note: Comment the next line if you want to checkin your web deploy settings,\n# but database connection strings (with potential passwords) will be unencrypted\n*.pubxml\n*.publishproj\n\n# Microsoft Azure Web App publish settings. Comment the next line if you want to\n# checkin your Azure Web App publish settings, but sensitive information contained\n# in these scripts will be unencrypted\nPublishScripts/\n\n# NuGet Packages\n*.nupkg\n# NuGet Symbol Packages\n*.snupkg\n# The packages folder can be ignored because of Package Restore\n**/[Pp]ackages/*\n# except build/, which is used as an MSBuild target.\n!**/[Pp]ackages/build/\n# Uncomment if necessary however generally it will be regenerated when needed\n#!**/[Pp]ackages/repositories.config\n# NuGet v3's project.json files produces more ignorable files\n*.nuget.props\n*.nuget.targets\n\n# Microsoft Azure Build Output\ncsx/\n*.build.csdef\n\n# Microsoft Azure Emulator\necf/\nrcf/\n\n# Windows Store app package directories and files\nAppPackages/\nBundleArtifacts/\nPackage.StoreAssociation.xml\n_pkginfo.txt\n*.appx\n*.appxbundle\n*.appxupload\n\n# Visual Studio cache files\n# files ending in .cache can be ignored\n*.[Cc]ache\n# but keep track of directories ending in .cache\n!?*.[Cc]ache/\n\n# Others\nClientBin/\n~$*\n*.dbmdl\n*.dbproj.schemaview\n*.jfm\n*.pfx\n*.publishsettings\norleans.codegen.cs\n\n# Including strong name files can present a security risk\n# (https://github.com/github/gitignore/pull/2483#issue-259490424)\n#*.snk\n\n# Since there are multiple workflows, uncomment next line to ignore bower_components\n# (https://github.com/github/gitignore/pull/1529#issuecomment-104372622)\n#bower_components/\n\n# RIA/Silverlight projects\nGenerated_Code/\n\n# Backup & report files from converting an old project file\n# to a newer Visual Studio version. Backup files are not needed,\n# because we have git ;-)\n_UpgradeReport_Files/\nBackup*/\nUpgradeLog*.XML\nUpgradeLog*.htm\nServiceFabricBackup/\n*.rptproj.bak\n\n# SQL Server files\n*.mdf\n*.ldf\n*.ndf\n\n# Business Intelligence projects\n*.rdl.data\n*.bim.layout\n*.bim_*.settings\n*.rptproj.rsuser\n*- [Bb]ackup.rdl\n*- [Bb]ackup ([0-9]).rdl\n*- [Bb]ackup ([0-9][0-9]).rdl\n\n# Microsoft Fakes\nFakesAssemblies/\n\n# GhostDoc plugin setting file\n*.GhostDoc.xml\n\n# Node.js Tools for Visual Studio\n.ntvs_analysis.dat\n\n# Visual Studio 6 build log\n*.plg\n\n# Visual Studio 6 workspace options file\n*.opt\n\n# Visual Studio 6 auto-generated workspace file (contains which files were open etc.)\n*.vbw\n\n# Visual Studio LightSwitch build output\n**/*.HTMLClient/GeneratedArtifacts\n**/*.DesktopClient/GeneratedArtifacts\n**/*.DesktopClient/ModelManifest.xml\n**/*.Server/GeneratedArtifacts\n**/*.Server/ModelManifest.xml\n_Pvt_Extensions\n\n# Paket dependency manager\n.paket/paket.exe\npaket-files/\n\n# FAKE - F# Make\n.fake/\n\n# CodeRush personal settings\n.cr/personal\n\n# Python Tools for Visual Studio (PTVS)\n\n# Cake - Uncomment if you are using it\n# tools/**\n# !tools/packages.config\n\n# Tabs Studio\n*.tss\n\n# Telerik's JustMock configuration file\n*.jmconfig\n\n# BizTalk build output\n*.btp.cs\n*.btm.cs\n*.odx.cs\n*.xsd.cs\n\n# OpenCover UI analysis results\nOpenCover/\n\n# Azure Stream Analytics local run output\nASALocalRun/\n\n# MSBuild Binary and Structured Log\n*.binlog\n\n# NVidia Nsight GPU debugger configuration file\n*.nvuser\n\n# MFractors (Xamarin productivity tool) working folder\n.mfractor/\n\n# Local History for Visual Studio\n.localhistory/\n\n# BeatPulse healthcheck temp database\nhealthchecksdb\n\n# Backup folder for Package Reference Convert tool in Visual Studio 2017\nMigrationBackup/\n\n# Ionide (cross platform F# VS Code tools) working folder\n.ionide/\n\n### vscode ###\n\n### Vue ###\n# gitignore template for Vue.js projects\n# Recommended template: Node.gitignore\n\n# TODO: where does this rule come from?\ndocs/_book\n\n# TODO: where does this rule come from?\n# XXX: covers up standard Python unit test path\n# test/\n\n### Vuejs ###\n# Recommended template: Node.gitignore\n\nnpm-debug.log\nyarn-error.log\n\n### Waf ###\n# For projects that use the Waf build system: https://waf.io/\n# Dot-hidden on Unix-like systems\n.waf-*-*/\n.waf3-*-*/\n# Hidden directory on Windows (no dot)\nwaf-*-*/\nwaf3-*-*/\n# Lockfile\n.lock-waf_*_build\n\n### Windows ###\n# Windows thumbnail cache files\nThumbs.db\nThumbs.db:encryptable\nehthumbs.db\nehthumbs_vista.db\n\n# Dump file\n*.stackdump\n\n# Folder config file\n[Dd]esktop.ini\n\n# Recycle Bin used on file shares\n$RECYCLE.BIN/\n\n# Windows Installer files\n*.msix\n\n# Windows shortcuts\n*.lnk\n\n### Xcode ###\n# Xcode\n# gitignore contributors: remember to update Global/Xcode.gitignore, Objective-C.gitignore & Swift.gitignore\n\n\n\n\n\n### Xcode Patch ###\n*.xcodeproj/*\n!*.xcodeproj/project.pbxproj\n!*.xcodeproj/xcshareddata/\n!*.xcworkspace/contents.xcworkspacedata\n**/xcshareddata/WorkspaceSettings.xcsettings\n\n### XcodeInjection ###\n# Code Injection\n# After new code Injection tools there's a generated folder /iOSInjectionProject\n# https://github.com/johnno1962/injectionforxcode\n\niOSInjectionProject/\n\n### Gradle ###\n\n# Ignore Gradle GUI config\n\n# Avoid ignoring Gradle wrapper jar file (.jar files are usually ignored)\n\n# Cache of project\n\n# # Work around https://youtrack.jetbrains.com/issue/IDEA-116898\n# gradle/wrapper/gradle-wrapper.properties\n\n### Gradle Patch ###\n**/build/\n\n### VisualStudio ###\n\n# User-specific files\n\n# User-specific files (MonoDevelop/Xamarin Studio)\n\n# Mono auto generated files\n\n# Build results\n\n# Visual Studio 2015/2017 cache/options directory\n# Uncomment if you have tasks that create the project's static files in wwwroot\n\n# Visual Studio 2017 auto generated files\n\n# MSTest test Results\n\n# NUnit\n\n# Build Results of an ATL Project\n\n# Benchmark Results\n\n# .NET Core\n\n# StyleCop\n\n# Files built by Visual Studio\n\n# Chutzpah Test files\n\n# Visual C++ cache files\n\n# Visual Studio profiler\n\n# Visual Studio Trace Files\n\n# TFS 2012 Local Workspace\n\n# Guidance Automation Toolkit\n\n# ReSharper is a .NET coding add-in\n\n# TeamCity is a build add-in\n\n# DotCover is a Code Coverage Tool\n\n# AxoCover is a Code Coverage Tool\n\n# Coverlet is a free, cross platform Code Coverage Tool\n\n# Visual Studio code coverage results\n\n# NCrunch\n\n# MightyMoose\n\n# Web workbench (sass)\n\n# Installshield output folder\n\n# DocProject is a documentation generator add-in\n\n# Click-Once directory\n\n# Publish Web Output\n# Note: Comment the next line if you want to checkin your web deploy settings,\n# but database connection strings (with potential passwords) will be unencrypted\n\n# Microsoft Azure Web App publish settings. Comment the next line if you want to\n# checkin your Azure Web App publish settings, but sensitive information contained\n# in these scripts will be unencrypted\n\n# NuGet Packages\n# NuGet Symbol Packages\n# The packages folder can be ignored because of Package Restore\n# except build/, which is used as an MSBuild target.\n# Uncomment if necessary however generally it will be regenerated when needed\n# NuGet v3's project.json files produces more ignorable files\n\n# Microsoft Azure Build Output\n\n# Microsoft Azure Emulator\n\n# Windows Store app package directories and files\n\n# Visual Studio cache files\n# files ending in .cache can be ignored\n# but keep track of directories ending in .cache\n\n# Others\n\n# Including strong name files can present a security risk\n# (https://github.com/github/gitignore/pull/2483#issue-259490424)\n\n# Since there are multiple workflows, uncomment next line to ignore bower_components\n# (https://github.com/github/gitignore/pull/1529#issuecomment-104372622)\n\n# RIA/Silverlight projects\n\n# Backup & report files from converting an old project file\n# to a newer Visual Studio version. Backup files are not needed,\n# because we have git ;-)\n\n# SQL Server files\n\n# Business Intelligence projects\n\n# Microsoft Fakes\n\n# GhostDoc plugin setting file\n\n# Node.js Tools for Visual Studio\n\n# Visual Studio 6 build log\n\n# Visual Studio 6 workspace options file\n\n# Visual Studio 6 auto-generated workspace file (contains which files were open etc.)\n\n# Visual Studio LightSwitch build output\n\n# Paket dependency manager\n\n# FAKE - F# Make\n\n# CodeRush personal settings\n\n# Python Tools for Visual Studio (PTVS)\n\n# Cake - Uncomment if you are using it\n# tools/**\n# !tools/packages.config\n\n# Tabs Studio\n\n# Telerik's JustMock configuration file\n\n# BizTalk build output\n\n# OpenCover UI analysis results\n\n# Azure Stream Analytics local run output\n\n# MSBuild Binary and Structured Log\n\n# NVidia Nsight GPU debugger configuration file\n\n# MFractors (Xamarin productivity tool) working folder\n\n# Local History for Visual Studio\n\n# BeatPulse healthcheck temp database\n\n# Backup folder for Package Reference Convert tool in Visual Studio 2017\n\n# Ionide (cross platform F# VS Code tools) working folder\n\n# End of https://www.toptal.com/developers/gitignore/api/ansible,apachehadoop,appcode,appengine,archive,archives,archlinuxpackages,audio,autotools,backup,basic,batch,bittorrent,c,c++,certificates,chefcookbook,clojure,cloud9,cmake,code,code-java,codeblocks,compressed,compressedarchive,compression,data,database,datarecovery,diff,direnv,diskimage,docfx,docpress,docz,dotenv,dotfilessh,dotsettings,dropbox,eclipse,emacs,erlang,executable,firebase,flask,git,gitbook,go,gpg,gradle,grails,groovy,grunt,haskell,helm,homebrew,hugo,images,intellij,intellij+all,intellij+iml,java,java-web,jenv,jetbrains,jetbrains+all,jetbrains+iml,jmeter,julia,jupyternotebooks,kotlin,lamp,latex,less,linux,lua,macos,matlab,maven,mercurial,microsoftoffice,node,octave,osx,packer,patch,perl,perl6,phpunit,powershell,puppet,putty,pycharm,pycharm+all,pycharm+iml,pydev,python,r,rails,react,reactnative,redis,root,ruby,rust,sbt,scala,serverless,sonar,sonarqube,spark,splunk,spreadsheet,ssh,sublimetext,svn,terraform,terragrunt,tortoisegit,vagrant,venv,virtualenv,visualstudio,visualstudiocode,vs,vscode,vue,vuejs,waf,windows,xcode,xcodeinjection,zsh\n"
        },
        {
          "name": ".gitlab-ci.yml",
          "type": "blob",
          "size": 0.7958984375,
          "content": "#  vim:ts=2:sts=2:sw=2:et\n#\n#  Author: Hari Sekhon\n#  Date: Sun Feb 23 19:02:10 2020 +0000\n#\n#  https://github.com/HariSekhon/DevOps-Bash-tools\n#\n#  License: see accompanying Hari Sekhon LICENSE file\n#\n#  If you're using my code you're welcome to connect with me on LinkedIn and optionally send me feedback\n#  to help improve or steer this or other code I publish\n#\n#  https://www.linkedin.com/in/HariSekhon\n#\n\n# ============================================================================ #\n#                               G i t L a b   C I\n# ============================================================================ #\n\n# https://docs.gitlab.com/ee/ci/yaml/README.html\n\n#include: '.gitlab/*.y*ml'\n\nimage: ubuntu:18.04\n\njob:\n  before_script:\n    - setup/ci_bootstrap.sh\n  script:\n    - make init && make ci test\n"
        },
        {
          "name": ".gitmodules",
          "type": "blob",
          "size": 0.6298828125,
          "content": "[submodule \"SQL\"]\n\tpath = SQL\n\turl = https://github.com/HariSekhon/SQL-scripts\n[submodule \"sql\"]\n\tpath = sql\n\turl = https://github.com/HariSekhon/SQL-scripts\n[submodule \"templates\"]\n\tpath = templates\n\turl = https://github.com/HariSekhon/Templates\n[submodule \"vagrant-configs\"]\n\tpath = vagrant-configs\n\turl = https://github.com/HariSekhon/Vagrant-templates\n[submodule \"packer\"]\n\tpath = packer\n\turl = https://github.com/HariSekhon/Packer-templates\n[submodule \"kubernetes-configs\"]\n\tpath = kubernetes-configs\n\turl = https://github.com/HariSekhon/Kubernetes-configs\n[submodule \"ansible\"]\n\tpath = ansible\n\turl = https://github.com/HariSekhon/Ansible\n"
        },
        {
          "name": ".hound.yml",
          "type": "blob",
          "size": 0.728515625,
          "content": "#\n#  Author: Hari Sekhon\n#  Date: 2020-09-23 10:28:21 +0100 (Wed, 23 Sep 2020)\n#\n#  vim:ts=2:sts=2:sw=2:et\n#\n#  https://github.com/HariSekhon/DevOps-Bash-tools\n#\n#  License: see accompanying Hari Sekhon LICENSE file\n#\n#  If you're using my code you're welcome to connect with me on LinkedIn and optionally send me feedback to help steer this or other code I publish\n#\n#  https://www.linkedin.com/in/HariSekhon\n#\n\n---\n# https://intercom.help/hound/en/articles/2138537-flake8\nflake8:\n  enabled: true\n  #config_file: .flake8\n\n# http://help.houndci.com/en/articles/2138564-shellcheck\nshellcheck:\n  enabled: true\n  #config_file: .shellcheck.yml\n\n# http://help.houndci.com/en/articles/2138524-golint\ngolint:\n  enabled: false\n\n#fail_on_violations: true\n"
        },
        {
          "name": ".mdl.rb",
          "type": "blob",
          "size": 1.056640625,
          "content": "#!/usr/bin/env ruby\n#  vim:ts=4:sts=4:sw=4:et:filetype=ruby\n#\n#  Author: Hari Sekhon\n#  Date: 2024-08-22 01:58:12 +0200 (Thu, 22 Aug 2024)\n#\n#  https///github.com/HariSekhon/DevOps-Bash-tools\n#\n#  License: see accompanying Hari Sekhon LICENSE file\n#\n#  If you're using my code you're welcome to connect with me on LinkedIn and optionally send me feedback to help steer this or other code I publish\n#\n#  https://www.linkedin.com/in/HariSekhon\n#\n\nall\n#exclude_rule 'MD001'\n#exclude_rule 'MD003'\n#exclude_rule 'MD005'\nexclude_rule 'MD007'  # leave 2 space indentation for lists, 3 space is ugly af\n#exclude_rule 'MD012'\nexclude_rule 'MD013'  # long lines cannot be split if they are URLs\n#exclude_rule 'MD022'\n#exclude_rule 'MD025'\nexclude_rule 'MD026'  # Trailing punctuation in header - sometimes I want to do etc. or ... at the end of a heading\n#exclude_rule 'MD031'\n#exclude_rule 'MD032'\nexclude_rule 'MD033'  # inline HTML is important for formatting\nexclude_rule 'MD036'  # emphasis used instead of header for footer Ported from lines\n#exclude_rule 'MD039'\n#exclude_rule 'MD056'\n"
        },
        {
          "name": ".mdlrc",
          "type": "blob",
          "size": 0.1064453125,
          "content": "mdlrc_dir = File.expand_path('..', __FILE__)\n\nstyle_file = File.join(mdlrc_dir, '.mdl.rb')\n\nstyle style_file\n"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 2.193359375,
          "content": "#\n#  Author: Hari Sekhon\n#  Date: 2024-08-08 17:34:56 +0300 (Thu, 08 Aug 2024)\n#\n#  vim:ts=2:sts=2:sw=2:et\n#\n#  https///github.com/HariSekhon/DevOps-Bash-tools\n#\n#  License: see accompanying Hari Sekhon LICENSE file\n#\n#  If you're using my code you're welcome to connect with me on LinkedIn and optionally send me feedback to help steer this or other code I publish\n#\n#  https://www.linkedin.com/in/HariSekhon\n#\n\n# ============================================================================ #\n#                              P r e - C o m m i t\n# ============================================================================ #\n\n---\nfail_fast: false\n#exclude: *.tmp$\n\nrepos:\n\n    # will accept anything that 'git clone' understands\n    # this means you can set this to a local git repo to develop your own hook repos interactively\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v4.6.0\n    hooks:\n      - id: check-yaml\n      # Common errors\n      #- id: end-of-file-fixer  # ruins .gitignore Icon\\r\n      - id: trailing-whitespace\n        args: [--markdown-linebreak-ext=md]\n      # Git style\n      - id: check-added-large-files\n      - id: check-merge-conflict\n      - id: check-vcs-permalinks\n      #- id: forbid-new-submodules\n      # Cross platform\n      - id: check-case-conflict\n      - id: mixed-line-ending\n        args: [--fix=lf]\n      # Security\n      - id: detect-aws-credentials\n        args: ['--allow-missing-credentials']\n\n  # rewrites python files with useless changes like changing single quotes to double quotes\n  #- repo: https://github.com/psf/black\n  #  rev: 24.8.0\n  #  hooks:\n  #    - id: black\n\n  # Git secrets Leaks\n  - repo: https://github.com/awslabs/git-secrets.git\n    # the release tags for 1.2.0, 1.2.1 and 1.3.0 are broken with this error:\n    #\n    #   /Users/hari/.cache/pre-commit/repo......./.pre-commit-hooks.yaml is not a file\n    #\n    rev: 5357e18\n    hooks:\n      - id: git-secrets\n\n  - repo: https://github.com/markdownlint/markdownlint\n    rev: v0.12.0\n    hooks:\n      - id: markdownlint\n        name: Markdownlint\n        description: Run markdownlint on your Markdown files\n        entry: mdl\n        args: [-s, .mdl.rb]\n        language: ruby\n        files: \\.(md|mdown|markdown)$\n"
        },
        {
          "name": ".pylintrc",
          "type": "blob",
          "size": 21.4833984375,
          "content": "#  vim:ts=4:sts=4:sw=4:et\n#\n#  Author: Hari Sekhon\n#  Date: 2006-06-28 23:25:09 +0100 (Wed, 28 Jun 2006)\n#\n#  https://github.com/HariSekhon/DevOps-Bash-tools\n#\n#  License: see accompanying Hari Sekhon LICENSE file\n#\n#  If you're using my code you're welcome to connect with me on LinkedIn and optionally send me feedback to help steer this or other code I publish\n#\n#  https://www.linkedin.com/in/HariSekhon\n#\n\n# ============================================================================ #\n#                           P y L i n t   C o n f i g\n# ============================================================================ #\n\n# pylint --generate-rcfile >> .pylintrc\n\n[MAIN]\n\n# Analyse import fallback blocks. This can be used to support both Python 2 and\n# 3 compatible code, which means that the block might have code that exists\n# only in one or another interpreter, leading to false positives when analysed.\nanalyse-fallback-blocks=no\n\n# Clear in-memory caches upon conclusion of linting. Useful if running pylint\n# in a server-like mode.\nclear-cache-post-run=no\n\n# Load and enable all available extensions. Use --list-extensions to see a list\n# all available extensions.\n#enable-all-extensions=\n\n# In error mode, messages with a category besides ERROR or FATAL are\n# suppressed, and no reports are done by default. Error mode is compatible with\n# disabling specific errors.\n#errors-only=\n\n# Always return a 0 (non-error) status code, even if lint errors are found.\n# This is primarily useful in continuous integration scripts.\n#exit-zero=\n\n# A comma-separated list of package or module names from where C extensions may\n# be loaded. Extensions are loading into the active Python interpreter and may\n# run arbitrary code.\nextension-pkg-allow-list=\n\n# A comma-separated list of package or module names from where C extensions may\n# be loaded. Extensions are loading into the active Python interpreter and may\n# run arbitrary code. (This is an alternative name to extension-pkg-allow-list\n# for backward compatibility.)\nextension-pkg-whitelist=\n\n# Return non-zero exit code if any of these messages/categories are detected,\n# even if score is above --fail-under value. Syntax same as enable. Messages\n# specified are enabled, while categories only check already-enabled messages.\nfail-on=\n\n# Specify a score threshold under which the program will exit with error.\nfail-under=10\n\n# Interpret the stdin as a python script, whose filename needs to be passed as\n# the module_or_package argument.\n#from-stdin=\n\n# Files or directories to be skipped. They should be base names, not paths.\nignore=CVS\n\n# Add files or directories matching the regular expressions patterns to the\n# ignore-list. The regex matches against paths and can be in Posix or Windows\n# format. Because '\\\\' represents the directory delimiter on Windows systems,\n# it can't be used as an escape character.\nignore-paths=\n\n# Files or directories matching the regular expression patterns are skipped.\n# The regex matches against base names, not paths. The default value ignores\n# Emacs file locks\nignore-patterns=^\\.#\n\n# List of module names for which member attributes should not be checked\n# (useful for modules/projects where namespaces are manipulated during runtime\n# and thus existing member attributes cannot be deduced by static analysis). It\n# supports qualified module names, as well as Unix pattern matching.\nignored-modules=\n\n# Python code to execute, usually for sys.path manipulation such as\n# pygtk.require().\n#init-hook=\n\n# Use multiple processes to speed up Pylint. Specifying 0 will auto-detect the\n# number of processors available to use, and will cap the count on Windows to\n# avoid hangs.\njobs=1\n\n# Control the amount of potential inferred values when inferring a single\n# object. This can help the performance when dealing with large functions or\n# complex, nested conditions.\nlimit-inference-results=100\n\n# List of plugins (as comma separated values of python module names) to load,\n# usually to register additional checkers.\nload-plugins=\n\n# Pickle collected data for later comparisons.\npersistent=yes\n\n# Minimum Python version to use for version dependent checks. Will default to\n# the version used to run pylint.\npy-version=3.11\n\n# Discover python modules and packages in the file system subtree.\nrecursive=no\n\n# Add paths to the list of the source roots. Supports globbing patterns. The\n# source root is an absolute path or a path relative to the current working\n# directory used to determine a package namespace for modules located under the\n# source root.\nsource-roots=\n\n# When enabled, pylint would attempt to guess common misconfiguration and emit\n# user-friendly hints instead of false-positive error messages.\nsuggestion-mode=yes\n\n# Allow loading of arbitrary C extensions. Extensions are imported into the\n# active Python interpreter and may run arbitrary code.\nunsafe-load-any-extension=no\n\n# In verbose mode, extra non-checker-related info will be displayed.\n#verbose=\n\n\n[BASIC]\n\n# Naming style matching correct argument names.\nargument-naming-style=snake_case\n\n# Regular expression matching correct argument names. Overrides argument-\n# naming-style. If left empty, argument names will be checked with the set\n# naming style.\n#argument-rgx=\n\n# Naming style matching correct attribute names.\nattr-naming-style=snake_case\n\n# Regular expression matching correct attribute names. Overrides attr-naming-\n# style. If left empty, attribute names will be checked with the set naming\n# style.\n#attr-rgx=\n\n# Bad variable names which should always be refused, separated by a comma.\nbad-names=foo,\n          bar,\n          baz,\n          toto,\n          tutu,\n          tata\n\n# Bad variable names regexes, separated by a comma. If names match any regex,\n# they will always be refused\nbad-names-rgxs=\n\n# Naming style matching correct class attribute names.\nclass-attribute-naming-style=any\n\n# Regular expression matching correct class attribute names. Overrides class-\n# attribute-naming-style. If left empty, class attribute names will be checked\n# with the set naming style.\n#class-attribute-rgx=\n\n# Naming style matching correct class constant names.\nclass-const-naming-style=UPPER_CASE\n\n# Regular expression matching correct class constant names. Overrides class-\n# const-naming-style. If left empty, class constant names will be checked with\n# the set naming style.\n#class-const-rgx=\n\n# Naming style matching correct class names.\nclass-naming-style=PascalCase\n\n# Regular expression matching correct class names. Overrides class-naming-\n# style. If left empty, class names will be checked with the set naming style.\n#class-rgx=\n\n# Naming style matching correct constant names.\nconst-naming-style=UPPER_CASE\n\n# Regular expression matching correct constant names. Overrides const-naming-\n# style. If left empty, constant names will be checked with the set naming\n# style.\n#const-rgx=\n\n# Minimum line length for functions/classes that require docstrings, shorter\n# ones are exempt.\ndocstring-min-length=-1\n\n# Naming style matching correct function names.\nfunction-naming-style=snake_case\n\n# Regular expression matching correct function names. Overrides function-\n# naming-style. If left empty, function names will be checked with the set\n# naming style.\n#function-rgx=\n\n# Good variable names which should always be accepted, separated by a comma.\ngood-names=i,\n           j,\n           k,\n           ex,\n           Run,\n           _\n\n# Good variable names regexes, separated by a comma. If names match any regex,\n# they will always be accepted\ngood-names-rgxs=\n\n# Include a hint for the correct naming format with invalid-name.\ninclude-naming-hint=no\n\n# Naming style matching correct inline iteration names.\ninlinevar-naming-style=any\n\n# Regular expression matching correct inline iteration names. Overrides\n# inlinevar-naming-style. If left empty, inline iteration names will be checked\n# with the set naming style.\n#inlinevar-rgx=\n\n# Naming style matching correct method names.\nmethod-naming-style=snake_case\n\n# Regular expression matching correct method names. Overrides method-naming-\n# style. If left empty, method names will be checked with the set naming style.\n#method-rgx=\n\n# Naming style matching correct module names.\nmodule-naming-style=snake_case\n\n# Regular expression matching correct module names. Overrides module-naming-\n# style. If left empty, module names will be checked with the set naming style.\n#module-rgx=\n\n# Colon-delimited sets of names that determine each other's naming style when\n# the name regexes allow several styles.\nname-group=\n\n# Regular expression which should only match function or class names that do\n# not require a docstring.\nno-docstring-rgx=^_\n\n# List of decorators that produce properties, such as abc.abstractproperty. Add\n# to this list to register other decorators that produce valid properties.\n# These decorators are taken in consideration only for invalid-name.\nproperty-classes=abc.abstractproperty\n\n# Regular expression matching correct type alias names. If left empty, type\n# alias names will be checked with the set naming style.\n#typealias-rgx=\n\n# Regular expression matching correct type variable names. If left empty, type\n# variable names will be checked with the set naming style.\n#typevar-rgx=\n\n# Naming style matching correct variable names.\nvariable-naming-style=snake_case\n\n# Regular expression matching correct variable names. Overrides variable-\n# naming-style. If left empty, variable names will be checked with the set\n# naming style.\n#variable-rgx=\n\n\n[CLASSES]\n\n# Warn about protected attribute access inside special methods\ncheck-protected-access-in-special-methods=no\n\n# List of method names used to declare (i.e. assign) instance attributes.\ndefining-attr-methods=__init__,\n                      __new__,\n                      setUp,\n                      asyncSetUp,\n                      __post_init__\n\n# List of member names, which should be excluded from the protected access\n# warning.\nexclude-protected=_asdict,_fields,_replace,_source,_make,os._exit\n\n# List of valid names for the first argument in a class method.\nvalid-classmethod-first-arg=cls\n\n# List of valid names for the first argument in a metaclass class method.\nvalid-metaclass-classmethod-first-arg=mcs\n\n\n[DESIGN]\n\n# List of regular expressions of class ancestor names to ignore when counting\n# public methods (see R0903)\nexclude-too-few-public-methods=\n\n# List of qualified class names to ignore when counting class parents (see\n# R0901)\nignored-parents=\n\n# Maximum number of arguments for function / method.\nmax-args=5\n\n# Maximum number of attributes for a class (see R0902).\nmax-attributes=7\n\n# Maximum number of boolean expressions in an if statement (see R0916).\nmax-bool-expr=5\n\n# Maximum number of branch for function / method body.\nmax-branches=12\n\n# Maximum number of locals for function / method body.\nmax-locals=15\n\n# Maximum number of parents for a class (see R0901).\nmax-parents=7\n\n# Maximum number of public methods for a class (see R0904).\nmax-public-methods=20\n\n# Maximum number of return / yield for function / method body.\nmax-returns=6\n\n# Maximum number of statements in function / method body.\nmax-statements=50\n\n# Minimum number of public methods for a class (see R0903).\nmin-public-methods=2\n\n\n[EXCEPTIONS]\n\n# Exceptions that will emit a warning when caught.\novergeneral-exceptions=builtins.BaseException,builtins.Exception\n\n\n[FORMAT]\n\n# Expected format of line ending, e.g. empty (any line ending), LF or CRLF.\nexpected-line-ending-format=\n\n# Regexp for a line that is allowed to be longer than the limit.\nignore-long-lines=^\\s*(# )?<?https?://\\S+>?$\n\n# Number of spaces of indent required inside a hanging or continued line.\nindent-after-paren=4\n\n# String used as indentation unit. This is usually \"    \" (4 spaces) or \"\\t\" (1\n# tab).\nindent-string='    '\n\n# Maximum number of characters on a single line.\nmax-line-length=120\n\n# Maximum number of lines in a module.\nmax-module-lines=1000\n\n# Allow the body of a class to be on the same line as the declaration if body\n# contains single statement.\nsingle-line-class-stmt=no\n\n# Allow the body of an if to be on the same line as the test if there is no\n# else.\nsingle-line-if-stmt=no\n\n\n[IMPORTS]\n\n# List of modules that can be imported at any level, not just the top level\n# one.\nallow-any-import-level=\n\n# Allow explicit reexports by alias from a package __init__.\nallow-reexport-from-package=no\n\n# Allow wildcard imports from modules that define __all__.\nallow-wildcard-with-all=no\n\n# Deprecated modules which should not be used, separated by a comma.\ndeprecated-modules=\n\n# Output a graph (.gv or any supported image format) of external dependencies\n# to the given file (report RP0402 must not be disabled).\next-import-graph=\n\n# Output a graph (.gv or any supported image format) of all (i.e. internal and\n# external) dependencies to the given file (report RP0402 must not be\n# disabled).\nimport-graph=\n\n# Output a graph (.gv or any supported image format) of internal dependencies\n# to the given file (report RP0402 must not be disabled).\nint-import-graph=\n\n# Force import order to recognize a module as part of the standard\n# compatibility libraries.\nknown-standard-library=\n\n# Force import order to recognize a module as part of a third party library.\nknown-third-party=enchant\n\n# Couples of modules and preferred modules, separated by a comma.\npreferred-modules=\n\n\n[LOGGING]\n\n# The type of string formatting that logging methods do. `old` means using %\n# formatting, `new` is for `{}` formatting.\nlogging-format-style=old\n\n# Logging modules to check that the string format arguments are in logging\n# function parameter format.\nlogging-modules=logging\n\n\n[MESSAGES CONTROL]\n\n# Only show warnings with the listed confidence levels. Leave empty to show\n# all. Valid levels: HIGH, CONTROL_FLOW, INFERENCE, INFERENCE_FAILURE,\n# UNDEFINED.\nconfidence=HIGH,\n           CONTROL_FLOW,\n           INFERENCE,\n           INFERENCE_FAILURE,\n           UNDEFINED\n\n# Disable the message, report, category or checker with the given id(s). You\n# can either give multiple identifiers separated by comma (,) or put this\n# option multiple times (only on the command line, not in the configuration\n# file where it should appear only once). You can also use \"--disable=all\" to\n# disable everything first and then re-enable specific checks. For example, if\n# you want to run only the similarities checker, you can use \"--disable=all\n# --enable=similarities\". If you want to run only the classes checker, but have\n# no Warning level messages displayed, use \"--disable=all --enable=classes\n# --disable=W\".\ndisable=raw-checker-failed,\n        bad-inline-option,\n        locally-disabled,\n        file-ignored,\n        suppressed-message,\n        useless-suppression,\n        deprecated-pragma,\n        use-symbolic-message-instead,\n        missing-class-docstring,\n        missing-function-docstring,\n        super-with-arguments,\n        consider-using-f-string\n\n# Enable the message, report, category or checker with the given id(s). You can\n# either give multiple identifier separated by comma (,) or put this option\n# multiple time (only on the command line, not in the configuration file where\n# it should appear only once). See also the \"--disable\" option for examples.\nenable=c-extension-no-member\n\n\n[METHOD_ARGS]\n\n# List of qualified names (i.e., library.method) which require a timeout\n# parameter e.g. 'requests.api.get,requests.api.post'\ntimeout-methods=requests.api.delete,requests.api.get,requests.api.head,requests.api.options,requests.api.patch,requests.api.post,requests.api.put,requests.api.request\n\n\n[MISCELLANEOUS]\n\n# List of note tags to take in consideration, separated by a comma.\nnotes=FIXME,\n      XXX,\n      TODO\n\n# Regular expression of note tags to take in consideration.\nnotes-rgx=\n\n\n[REFACTORING]\n\n# Maximum number of nested blocks for function / method body\nmax-nested-blocks=5\n\n# Complete name of functions that never returns. When checking for\n# inconsistent-return-statements if a never returning function is called then\n# it will be considered as an explicit return statement and no message will be\n# printed.\nnever-returning-functions=sys.exit,argparse.parse_error\n\n\n[REPORTS]\n\n# Python expression which should return a score less than or equal to 10. You\n# have access to the variables 'fatal', 'error', 'warning', 'refactor',\n# 'convention', and 'info' which contain the number of messages in each\n# category, as well as 'statement' which is the total number of statements\n# analyzed. This score is used by the global evaluation report (RP0004).\nevaluation=max(0, 0 if fatal else 10.0 - ((float(5 * error + warning + refactor + convention) / statement) * 10))\n\n# Template used to display messages. This is a python new-style format string\n# used to format the message information. See doc for all details.\nmsg-template=\n\n# Set the output format. Available formats are text, parseable, colorized, json\n# and msvs (visual studio). You can also give a reporter class, e.g.\n# mypackage.mymodule.MyReporterClass.\n#output-format=\n\n# Tells whether to display a full report or only the messages.\nreports=no\n\n# Activate the evaluation score.\nscore=yes\n\n\n[SIMILARITIES]\n\n# Comments are removed from the similarity computation\nignore-comments=yes\n\n# Docstrings are removed from the similarity computation\nignore-docstrings=yes\n\n# Imports are removed from the similarity computation\nignore-imports=yes\n\n# Signatures are removed from the similarity computation\nignore-signatures=yes\n\n# Minimum lines number of a similarity.\nmin-similarity-lines=4\n\n\n[SPELLING]\n\n# Limits count of emitted suggestions for spelling mistakes.\nmax-spelling-suggestions=4\n\n# Spelling dictionary name. No available dictionaries : You need to install\n# both the python package and the system dependency for enchant to work..\nspelling-dict=\n\n# List of comma separated words that should be considered directives if they\n# appear at the beginning of a comment and should not be checked.\nspelling-ignore-comment-directives=fmt: on,fmt: off,noqa:,noqa,nosec,isort:skip,mypy:\n\n# List of comma separated words that should not be checked.\nspelling-ignore-words=\n\n# A path to a file that contains the private dictionary; one word per line.\nspelling-private-dict-file=\n\n# Tells whether to store unknown words to the private dictionary (see the\n# --spelling-private-dict-file option) instead of raising a message.\nspelling-store-unknown-words=no\n\n\n[STRING]\n\n# This flag controls whether inconsistent-quotes generates a warning when the\n# character used as a quote delimiter is used inconsistently within a module.\ncheck-quote-consistency=no\n\n# This flag controls whether the implicit-str-concat should generate a warning\n# on implicit string concatenation in sequences defined over several lines.\ncheck-str-concat-over-line-jumps=no\n\n\n[TYPECHECK]\n\n# List of decorators that produce context managers, such as\n# contextlib.contextmanager. Add to this list to register other decorators that\n# produce valid context managers.\ncontextmanager-decorators=contextlib.contextmanager\n\n# List of members which are set dynamically and missed by pylint inference\n# system, and so shouldn't trigger E1101 when accessed. Python regular\n# expressions are accepted.\ngenerated-members=\n\n# Tells whether to warn about missing members when the owner of the attribute\n# is inferred to be None.\nignore-none=yes\n\n# This flag controls whether pylint should warn about no-member and similar\n# checks whenever an opaque object is returned when inferring. The inference\n# can return multiple potential results while evaluating a Python object, but\n# some branches might not be evaluated, which results in partial inference. In\n# that case, it might be useful to still emit no-member and other checks for\n# the rest of the inferred objects.\nignore-on-opaque-inference=yes\n\n# List of symbolic message names to ignore for Mixin members.\nignored-checks-for-mixins=no-member,\n                          not-async-context-manager,\n                          not-context-manager,\n                          attribute-defined-outside-init\n\n# List of class names for which member attributes should not be checked (useful\n# for classes with dynamically set attributes). This supports the use of\n# qualified names.\nignored-classes=optparse.Values,thread._local,_thread._local,argparse.Namespace\n\n# Show a hint with possible names when a member name was not found. The aspect\n# of finding the hint is based on edit distance.\nmissing-member-hint=yes\n\n# The minimum edit distance a name should have in order to be considered a\n# similar match for a missing member name.\nmissing-member-hint-distance=1\n\n# The total number of similar names that should be taken in consideration when\n# showing a hint for a missing member.\nmissing-member-max-choices=1\n\n# Regex pattern to define which classes are considered mixins.\nmixin-class-rgx=.*[Mm]ixin\n\n# List of decorators that change the signature of a decorated function.\nsignature-mutators=\n\n\n[VARIABLES]\n\n# List of additional names supposed to be defined in builtins. Remember that\n# you should avoid defining new builtins when possible.\nadditional-builtins=\n\n# Tells whether unused global variables should be treated as a violation.\nallow-global-unused-variables=yes\n\n# List of names allowed to shadow builtins\nallowed-redefined-builtins=\n\n# List of strings which can identify a callback function by name. A callback\n# name must start or end with one of those strings.\ncallbacks=cb_,\n          _cb\n\n# A regular expression matching the name of dummy variables (i.e. expected to\n# not be used).\ndummy-variables-rgx=_+$|(_[a-zA-Z0-9_]*[a-zA-Z0-9]+?$)|dummy|^ignored_|^unused_\n\n# Argument names that match this expression will be ignored.\nignored-argument-names=_.*|^ignored_|^unused_\n\n# Tells whether we should check for unused import in __init__ files.\ninit-import=no\n\n# List of qualified module names which can have objects that can redefine\n# builtins.\nredefining-builtins-modules=six.moves,past.builtins,future.builtins,builtins,io\n"
        },
        {
          "name": ".scrutinizer.yml",
          "type": "blob",
          "size": 0.64453125,
          "content": "#\n#  Author: Hari Sekhon\n#  Date: 2020-03-17 11:41:13 +0000 (Tue, 17 Mar 2020)\n#\n#  vim:ts=2:sts=2:sw=2:et\n#\n#  https://github.com/HariSekhon/DevOps-Bash-tools\n#\n#  License: see accompanying Hari Sekhon LICENSE file\n#\n#  If you're using my code you're welcome to connect with me on LinkedIn and optionally send me feedback to help steer this or other code I publish\n#\n#  https://www.linkedin.com/in/HariSekhon\n#\n\nbuild:\n  image: default-bionic\n  nodes:\n    auto:\n      commands:\n        - repo=\"${SCRUTINIZER_PROJECT#*/}\"; git clone \"https://github.com/$repo\" build\n        - cd ~/build\n        - pwd\n        - ls -l\n        - make init\n        - make ci test\n"
        },
        {
          "name": ".semaphore",
          "type": "tree",
          "content": null
        },
        {
          "name": ".sonarcloud.properties",
          "type": "blob",
          "size": 0.0361328125,
          "content": "sonar.host.url=https://sonarcloud.io\n"
        },
        {
          "name": ".terraformignore",
          "type": "blob",
          "size": 0.87890625,
          "content": "#\n#  Author: Hari Sekhon\n#  Date: 2020-09-24 17:08:01 +0100 (Thu, 24 Sep 2020)\n#\n#  vim:ts=4:sts=4:sw=4:et\n#\n#  https://github.com/HariSekhon/DevOps-Bash-tools\n#\n#  License: see accompanying Hari Sekhon LICENSE file\n#\n#  If you're using my code you're welcome to connect with me on LinkedIn and optionally send me feedback to help steer this or other code I publish\n#\n#  https://www.linkedin.com/in/HariSekhon\n#\n\n# Requires Terraform 0.12.11+\n#\n# Prevents upload of paths to Terraform Cloud\n#\n# Same format as .gitignore\n# - directories must end with forward slash /\n# - negate matches using !\n#\n# Only works at the root of the config directory\n\n# https://www.terraform.io/docs/backends/types/remote.html#excluding-files-from-upload-with-terraformignore\n\n# defaults\n.git/\n.terraform/\n\n# custom\n.hg/\n.svn/\n.ssh/\ngithub/\ngitroot/\nmercurial/\nhg/\nhgroot/\nsvn/\nsvnroot/\n\n# exclude all hidden dot files\n.*\n"
        },
        {
          "name": ".trivyignore",
          "type": "blob",
          "size": 0.0537109375,
          "content": "#aws-access-key-id\n#aws-account-id\ngcp-service-account\n"
        },
        {
          "name": ".zlogin",
          "type": "blob",
          "size": 0.6279296875,
          "content": "#!/usr/bin/env bash\n#  vim:ts=4:sts=4:sw=4:et\n#\n#  Author: Hari Sekhon\n#  Date: 2020-03-13 18:58:03 +0000 (Fri, 13 Mar 2020)\n#\n#  https://github.com/HariSekhon/DevOps-Bash-tools\n#\n#  License: see accompanying Hari Sekhon LICENSE file\n#\n#  If you're using my code you're welcome to connect with me on LinkedIn and optionally send me feedback to help steer this or other code I publish\n#\n#  https://www.linkedin.com/in/HariSekhon\n#\n\n# ============================================================================ #\n#                               Z S H   L o g i n\n# ============================================================================ #\n"
        },
        {
          "name": ".zlogout",
          "type": "blob",
          "size": 0.62890625,
          "content": "#!/usr/bin/env bash\n#  vim:ts=4:sts=4:sw=4:et\n#\n#  Author: Hari Sekhon\n#  Date: 2020-03-13 18:58:03 +0000 (Fri, 13 Mar 2020)\n#\n#  https://github.com/HariSekhon/DevOps-Bash-tools\n#\n#  License: see accompanying Hari Sekhon LICENSE file\n#\n#  If you're using my code you're welcome to connect with me on LinkedIn and optionally send me feedback to help steer this or other code I publish\n#\n#  https://www.linkedin.com/in/HariSekhon\n#\n\n# ============================================================================ #\n#                              Z S H   L o g o u t\n# ============================================================================ #\n"
        },
        {
          "name": ".zprofile",
          "type": "blob",
          "size": 0.4365234375,
          "content": "#\n#  Author: Hari Sekhon\n#  Date: 2006-06-28 23:25:09 +0100 (Wed, 28 Jun 2006)\n#  (forked from .bash_profile)\n\n# ============================================================================ #\n#                             Z S H   P r o f i l e\n# ============================================================================ #\n\n# goes horribly wrong - too much advanced bash\n#if [[ -e ~/.profile  ]]; then\n#    emulate sh -c 'source ~/.profile'\n#fi\n"
        },
        {
          "name": ".zshenv",
          "type": "blob",
          "size": 0.7099609375,
          "content": "#!/usr/bin/env bash\n#  vim:ts=4:sts=4:sw=4:et\n#\n#  Author: Hari Sekhon\n#  Date: 2020-03-13 18:58:03 +0000 (Fri, 13 Mar 2020)\n#\n#  https://github.com/HariSekhon/DevOps-Bash-tools\n#\n#  License: see accompanying Hari Sekhon LICENSE file\n#\n#  If you're using my code you're welcome to connect with me on LinkedIn and optionally send me feedback to help steer this or other code I publish\n#\n#  https://www.linkedin.com/in/HariSekhon\n#\n\n# ============================================================================ #\n#                                 Z S H   E n v\n# ============================================================================ #\n\n# sourced by both interactive shells and scripts\n#\n# be careful with you put in here\n"
        },
        {
          "name": ".zshrc",
          "type": "blob",
          "size": 7.328125,
          "content": "#!/usr/bin/env bash\n#  shellcheck disable=SC1091\n#  vim:ts=4:sts=4:sw=4:et\n#\n#  Author: Hari Sekhon\n#  Date: 2006-06-28 23:25:09 +0100 (Wed, 28 Jun 2006)\n#  (forked from .bashrc)\n#\n#  https://github.com/HariSekhon/DevOps-Bash-tools\n#\n#  License: see accompanying Hari Sekhon LICENSE file\n#\n#  If you're using my code you're welcome to connect with me on LinkedIn and optionally send me feedback to help steer this or other code I publish\n#\n#  https://www.linkedin.com/in/HariSekhon\n#\n\n# ============================================================================ #\n#                                     Z S H\n# ============================================================================ #\n\n# https://wiki.archlinux.org/index.php/Zsh\n\n# goes horribly wrong - too much advanced bash\n#if [[ -e ~/.bashrc  ]]; then\n#    emulate sh -c 'source ~/.bashrc'\n#fi\n\nautoload -Uz compinit promptinit\ncompinit  # completes ssh/scp/sftp hostnames as long as HashKnownHosts not set in ~/.ssh/config\npromptinit\n\n# prompt -l - list themes\n# prompt -p - preview themes\n#prompt suse\n\n# install Oh-My-ZSH\n# sh -c \"$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\"\n\n# custom themes:\n# mkdir ~/.zprompts\n# fpath=(\"$HOME/.zprompts\" \"$fpath[@]\")\n\n# uniq all items in $PATH and $path array\ntypeset -U PATH path\npath=(\"$HOME/.local/bin\" \"$HOME/bin\" \"$path[@]\")\nexport PATH\n\n# autocompletion with an arrow-key driven interface - tab twice to enable\nzstyle ':completion:*' menu select\n\n# autocompletions with sudo\n# allows zsh completion scripts run commands with sudo privileges - do not enable if using untrusted autocompletion scripts!!\n#zstyle ':completion::complete:*' gain-privileges 1\n\n# ============================================================================ #\n#                                S e t t i n g s\n# ============================================================================ #\n\n# compatible style with other shells\n#set -o AUTO_CD\n# casse insensitive, underscores stripped\nsetopt AUTO_CD\n\nsetopt COMPLETE_ALIASES\n\nsetopt CORRECT\nexport SPROMPT=\"Correct %R to %r? [Yes, No, Abort, Edit] \"\n\nautoload U colors && colors\n\n# expand wilcard expansion on unquoted variables like Bash\nsetopt GLOB_SUBST\n\nexport PATH=\"$PATH:/opt/homebrew/bin/\"\n\n# ============================================================================ #\n#                                   Oh-My-ZSH\n# ============================================================================ #\n\n# If you come from bash you might have to change your $PATH.\n# export PATH=$HOME/bin:/usr/local/bin:$PATH\n\n# Path to your oh-my-zsh installation.\nexport ZSH=\"/Users/hari.sekhon/.oh-my-zsh\"\n\n# Set name of the theme to load --- if set to \"random\", it will\n# load a random theme each time oh-my-zsh is loaded, in which case,\n# to know which specific one was loaded, run: echo $RANDOM_THEME\n# See https://github.com/ohmyzsh/ohmyzsh/wiki/Themes\nZSH_THEME=\"robbyrussell\"\n\n# also messed up\n#ZSH_THEME=\"agnoster\"\n\n# messes up both Terminal and iTerm2 from both brew and git cloned installations\n#if [ -f /usr/local/opt/powerlevel9k/powerlevel9k.zsh-theme ]; then\n#    source /usr/local/opt/powerlevel9k/powerlevel9k.zsh-theme\n#fi\n#\n# Oh-My-ZSH ~/.oh-my-zsh/custom/themes/powerlevel9k\n#ZSH_THEME=\"powerlevel9k/powerlevel9k\"\n\n# Set list of themes to pick from when loading at random\n# Setting this variable when ZSH_THEME=random will cause zsh to load\n# a theme from this variable instead of looking in ~/.oh-my-zsh/themes/\n# If set to an empty array, this variable will have no effect.\n# ZSH_THEME_RANDOM_CANDIDATES=( \"robbyrussell\" \"agnoster\" )\n\n# Uncomment the following line to use case-sensitive completion.\n# CASE_SENSITIVE=\"true\"\n\n# Uncomment the following line to use hyphen-insensitive completion.\n# Case-sensitive completion must be off. _ and - will be interchangeable.\n# HYPHEN_INSENSITIVE=\"true\"\n\n# Uncomment the following line to disable bi-weekly auto-update checks.\n# DISABLE_AUTO_UPDATE=\"true\"\n\n# Uncomment the following line to automatically update without prompting.\n# DISABLE_UPDATE_PROMPT=\"true\"\n\n# Uncomment the following line to change how often to auto-update (in days).\n# export UPDATE_ZSH_DAYS=13\n\n# Uncomment the following line if pasting URLs and other text is messed up.\n# DISABLE_MAGIC_FUNCTIONS=true\n\n# Uncomment the following line to disable colors in ls.\n# DISABLE_LS_COLORS=\"true\"\n\n# Uncomment the following line to disable auto-setting terminal title.\n# DISABLE_AUTO_TITLE=\"true\"\n\n# Uncomment the following line to enable command auto-correction.\n# ENABLE_CORRECTION=\"true\"\n\n# Uncomment the following line to display red dots whilst waiting for completion.\n# COMPLETION_WAITING_DOTS=\"true\"\n\n# Uncomment the following line if you want to disable marking untracked files\n# under VCS as dirty. This makes repository status check for large repositories\n# much, much faster.\n# DISABLE_UNTRACKED_FILES_DIRTY=\"true\"\n\n# Uncomment the following line if you want to change the command execution time\n# stamp shown in the history command output.\n# You can set one of the optional three formats:\n# \"mm/dd/yyyy\"|\"dd.mm.yyyy\"|\"yyyy-mm-dd\"\n# or set a custom format using the strftime function format specifications,\n# see 'man strftime' for details.\n# HIST_STAMPS=\"mm/dd/yyyy\"\n\n# Would you like to use another custom folder than $ZSH/custom?\n# ZSH_CUSTOM=/path/to/new-custom-folder\n\n# Which plugins would you like to load?\n# Standard plugins can be found in ~/.oh-my-zsh/plugins/*\n# Custom plugins may be added to ~/.oh-my-zsh/custom/plugins/\n# Example format: plugins=(rails git textmate ruby lighthouse)\n# Add wisely, as too many plugins slow down shell startup.\nplugins=(git)\n\nsource $ZSH/oh-my-zsh.sh\n\n# User configuration\n\n# export MANPATH=\"/usr/local/man:$MANPATH\"\n\n# You may need to manually set your language environment\n# export LANG=en_US.UTF-8\n\n# Preferred editor for local and remote sessions\n# if [[ -n $SSH_CONNECTION ]]; then\n#   export EDITOR='vim'\n# else\n#   export EDITOR='mvim'\n# fi\n\n# Compilation flags\n# export ARCHFLAGS=\"-arch x86_64\"\n\n# Set personal aliases, overriding those provided by oh-my-zsh libs,\n# plugins, and themes. Aliases can be placed here, though oh-my-zsh\n# users are encouraged to define aliases within the ZSH_CUSTOM folder.\n# For a full list of active aliases, run `alias`.\n#\n# Example aliases\n# alias zshconfig=\"mate ~/.zshrc\"\n# alias ohmyzsh=\"mate ~/.oh-my-zsh\"\n\n# ============================================================================ #\n\n# Lines configured by zsh-newuser-install\nHISTFILE=~/.histfile\nHISTSIZE=1000\nSAVEHIST=1000\nsetopt appendhistory autocd extendedglob nomatch notify\nunsetopt beep\nbindkey -e\n# End of lines configured by zsh-newuser-install\n# The following lines were added by compinstall\nzstyle :compinstall filename '/home/hari/.zshrc'\nautoload -Uz compinit\ncompinit\n# End of lines added by compinstall\n\n# added by travis gem - Travis is legacy now, don't bother with this\n#[ -f /Users/hari/.travis/travis.sh ] && source /Users/hari/.travis/travis.sh\n\nif type -P direnv &>/dev/null; then\n    eval \"$(direnv hook zsh)\"\nfi\n\nautoload -U +X bashcompinit && bashcompinit\ncomplete -o nospace -C /Users/hari/bin/terraform terraform\ncomplete -o nospace -C /Users/hari/bin/terraform tf\n\ncomplete -o nospace -C /usr/local/bin/terragrunt terragrunt\n\n#THIS MUST BE AT THE END OF THE FILE FOR SDKMAN TO WORK!!!\nexport SDKMAN_DIR=\"/Users/hari/.sdkman\"\n[[ -s \"/Users/hari/.sdkman/bin/sdkman-init.sh\" ]] && source \"/Users/hari/.sdkman/bin/sdkman-init.sh\"\n"
        },
        {
          "name": "DOCKER_STATUS.md",
          "type": "blob",
          "size": 17.7080078125,
          "content": "# Docker Status Page\n\ngenerated by `docker_generate_status_page.sh` in [HariSekhon/DevOps-Bash-tools](https://github.com/HariSekhon/DevOps-Bash-tools)\n\nThis page relies on shields.io which is slow so a lot of it may not load properly the first time so you may need to do one or more page reloads to get all the badges to load.\n\n50 docker repos - `:latest` tag build status:\n\n[![Docker Build Status](https://img.shields.io/docker/cloud/build/harisekhon/alluxio.svg)](https://hub.docker.com/r/harisekhon/alluxio/builds)\n[![DockerHub Pulls](https://img.shields.io/docker/pulls/harisekhon/alluxio.svg)](https://hub.docker.com/r/harisekhon/alluxio) -\n[harisekhon/alluxio](https://hub.docker.com/r/harisekhon/alluxio)\n\n[![Docker Build Status](https://img.shields.io/docker/cloud/build/harisekhon/alpine-dev.svg)](https://hub.docker.com/r/harisekhon/alpine-dev/builds)\n[![DockerHub Pulls](https://img.shields.io/docker/pulls/harisekhon/alpine-dev.svg)](https://hub.docker.com/r/harisekhon/alpine-dev) -\n[harisekhon/alpine-dev](https://hub.docker.com/r/harisekhon/alpine-dev)\n\n[![Docker Build Status](https://img.shields.io/docker/cloud/build/harisekhon/alpine-github.svg)](https://hub.docker.com/r/harisekhon/alpine-github/builds)\n[![DockerHub Pulls](https://img.shields.io/docker/pulls/harisekhon/alpine-github.svg)](https://hub.docker.com/r/harisekhon/alpine-github) -\n[harisekhon/alpine-github](https://hub.docker.com/r/harisekhon/alpine-github)\n\n[![Docker Build Status](https://img.shields.io/docker/cloud/build/harisekhon/apache-drill.svg)](https://hub.docker.com/r/harisekhon/apache-drill/builds)\n[![DockerHub Pulls](https://img.shields.io/docker/pulls/harisekhon/apache-drill.svg)](https://hub.docker.com/r/harisekhon/apache-drill) -\n[harisekhon/apache-drill](https://hub.docker.com/r/harisekhon/apache-drill)\n\n[![Docker Build Status](https://img.shields.io/docker/cloud/build/harisekhon/cassandra-dev.svg)](https://hub.docker.com/r/harisekhon/cassandra-dev/builds)\n[![DockerHub Pulls](https://img.shields.io/docker/pulls/harisekhon/cassandra-dev.svg)](https://hub.docker.com/r/harisekhon/cassandra-dev) -\n[harisekhon/cassandra-dev](https://hub.docker.com/r/harisekhon/cassandra-dev)\n\n[![Docker Build Status](https://img.shields.io/docker/cloud/build/harisekhon/centos-dev.svg)](https://hub.docker.com/r/harisekhon/centos-dev/builds)\n[![DockerHub Pulls](https://img.shields.io/docker/pulls/harisekhon/centos-dev.svg)](https://hub.docker.com/r/harisekhon/centos-dev) -\n[harisekhon/centos-dev](https://hub.docker.com/r/harisekhon/centos-dev)\n\n[![Docker Build Status](https://img.shields.io/docker/cloud/build/harisekhon/centos-github.svg)](https://hub.docker.com/r/harisekhon/centos-github/builds)\n[![DockerHub Pulls](https://img.shields.io/docker/pulls/harisekhon/centos-github.svg)](https://hub.docker.com/r/harisekhon/centos-github) -\n[harisekhon/centos-github](https://hub.docker.com/r/harisekhon/centos-github)\n\n[![Docker Build Status](https://img.shields.io/docker/cloud/build/harisekhon/centos-java.svg)](https://hub.docker.com/r/harisekhon/centos-java/builds)\n[![DockerHub Pulls](https://img.shields.io/docker/pulls/harisekhon/centos-java.svg)](https://hub.docker.com/r/harisekhon/centos-java) -\n[harisekhon/centos-java](https://hub.docker.com/r/harisekhon/centos-java)\n\n[![Docker Build Status](https://img.shields.io/docker/cloud/build/harisekhon/centos-scala.svg)](https://hub.docker.com/r/harisekhon/centos-scala/builds)\n[![DockerHub Pulls](https://img.shields.io/docker/pulls/harisekhon/centos-scala.svg)](https://hub.docker.com/r/harisekhon/centos-scala) -\n[harisekhon/centos-scala](https://hub.docker.com/r/harisekhon/centos-scala)\n\n[![Docker Build Status](https://img.shields.io/docker/cloud/build/harisekhon/ci_intentionally_broken_test_do_not_use.svg)](https://hub.docker.com/r/harisekhon/ci_intentionally_broken_test_do_not_use/builds)\n[![DockerHub Pulls](https://img.shields.io/docker/pulls/harisekhon/ci_intentionally_broken_test_do_not_use.svg)](https://hub.docker.com/r/harisekhon/ci_intentionally_broken_test_do_not_use) -\n[harisekhon/ci_intentionally_broken_test_do_not_use](https://hub.docker.com/r/harisekhon/ci_intentionally_broken_test_do_not_use)\n\n[![Docker Build Status](https://img.shields.io/docker/cloud/build/harisekhon/collectd.svg)](https://hub.docker.com/r/harisekhon/collectd/builds)\n[![DockerHub Pulls](https://img.shields.io/docker/pulls/harisekhon/collectd.svg)](https://hub.docker.com/r/harisekhon/collectd) -\n[harisekhon/collectd](https://hub.docker.com/r/harisekhon/collectd)\n\n[![Docker Build Status](https://img.shields.io/docker/cloud/build/harisekhon/consul.svg)](https://hub.docker.com/r/harisekhon/consul/builds)\n[![DockerHub Pulls](https://img.shields.io/docker/pulls/harisekhon/consul.svg)](https://hub.docker.com/r/harisekhon/consul) -\n[harisekhon/consul](https://hub.docker.com/r/harisekhon/consul)\n\n[![Docker Build Status](https://img.shields.io/docker/cloud/build/harisekhon/consul-dev.svg)](https://hub.docker.com/r/harisekhon/consul-dev/builds)\n[![DockerHub Pulls](https://img.shields.io/docker/pulls/harisekhon/consul-dev.svg)](https://hub.docker.com/r/harisekhon/consul-dev) -\n[harisekhon/consul-dev](https://hub.docker.com/r/harisekhon/consul-dev)\n\n[![Docker Build Status](https://img.shields.io/docker/cloud/build/harisekhon/debian-dev.svg)](https://hub.docker.com/r/harisekhon/debian-dev/builds)\n[![DockerHub Pulls](https://img.shields.io/docker/pulls/harisekhon/debian-dev.svg)](https://hub.docker.com/r/harisekhon/debian-dev) -\n[harisekhon/debian-dev](https://hub.docker.com/r/harisekhon/debian-dev)\n\n[![Docker Build Status](https://img.shields.io/docker/cloud/build/harisekhon/debian-github.svg)](https://hub.docker.com/r/harisekhon/debian-github/builds)\n[![DockerHub Pulls](https://img.shields.io/docker/pulls/harisekhon/debian-github.svg)](https://hub.docker.com/r/harisekhon/debian-github) -\n[harisekhon/debian-github](https://hub.docker.com/r/harisekhon/debian-github)\n\n[![Docker Build Status](https://img.shields.io/docker/cloud/build/harisekhon/debian-java.svg)](https://hub.docker.com/r/harisekhon/debian-java/builds)\n[![DockerHub Pulls](https://img.shields.io/docker/pulls/harisekhon/debian-java.svg)](https://hub.docker.com/r/harisekhon/debian-java) -\n[harisekhon/debian-java](https://hub.docker.com/r/harisekhon/debian-java)\n\n[![Docker Build Status](https://img.shields.io/docker/cloud/build/harisekhon/h2o.svg)](https://hub.docker.com/r/harisekhon/h2o/builds)\n[![DockerHub Pulls](https://img.shields.io/docker/pulls/harisekhon/h2o.svg)](https://hub.docker.com/r/harisekhon/h2o) -\n[harisekhon/h2o](https://hub.docker.com/r/harisekhon/h2o)\n\n[![Docker Build Status](https://img.shields.io/docker/cloud/build/harisekhon/hadoop.svg)](https://hub.docker.com/r/harisekhon/hadoop/builds)\n[![DockerHub Pulls](https://img.shields.io/docker/pulls/harisekhon/hadoop.svg)](https://hub.docker.com/r/harisekhon/hadoop) -\n[harisekhon/hadoop](https://hub.docker.com/r/harisekhon/hadoop)\n\n[![Docker Build Status](https://img.shields.io/docker/cloud/build/harisekhon/hadoop-dev.svg)](https://hub.docker.com/r/harisekhon/hadoop-dev/builds)\n[![DockerHub Pulls](https://img.shields.io/docker/pulls/harisekhon/hadoop-dev.svg)](https://hub.docker.com/r/harisekhon/hadoop-dev) -\n[harisekhon/hadoop-dev](https://hub.docker.com/r/harisekhon/hadoop-dev)\n\n[![Docker Build Status](https://img.shields.io/docker/cloud/build/harisekhon/hbase.svg)](https://hub.docker.com/r/harisekhon/hbase/builds)\n[![DockerHub Pulls](https://img.shields.io/docker/pulls/harisekhon/hbase.svg)](https://hub.docker.com/r/harisekhon/hbase) -\n[harisekhon/hbase](https://hub.docker.com/r/harisekhon/hbase)\n\n[![Docker Build Status](https://img.shields.io/docker/cloud/build/harisekhon/hbase-dev.svg)](https://hub.docker.com/r/harisekhon/hbase-dev/builds)\n[![DockerHub Pulls](https://img.shields.io/docker/pulls/harisekhon/hbase-dev.svg)](https://hub.docker.com/r/harisekhon/hbase-dev) -\n[harisekhon/hbase-dev](https://hub.docker.com/r/harisekhon/hbase-dev)\n\n[![Docker Build Status](https://img.shields.io/docker/cloud/build/harisekhon/jython.svg)](https://hub.docker.com/r/harisekhon/jython/builds)\n[![DockerHub Pulls](https://img.shields.io/docker/pulls/harisekhon/jython.svg)](https://hub.docker.com/r/harisekhon/jython) -\n[harisekhon/jython](https://hub.docker.com/r/harisekhon/jython)\n\n[![Docker Build Status](https://img.shields.io/docker/cloud/build/harisekhon/kafka.svg)](https://hub.docker.com/r/harisekhon/kafka/builds)\n[![DockerHub Pulls](https://img.shields.io/docker/pulls/harisekhon/kafka.svg)](https://hub.docker.com/r/harisekhon/kafka) -\n[harisekhon/kafka](https://hub.docker.com/r/harisekhon/kafka)\n\n[![Docker Build Status](https://img.shields.io/docker/cloud/build/harisekhon/mesos.svg)](https://hub.docker.com/r/harisekhon/mesos/builds)\n[![DockerHub Pulls](https://img.shields.io/docker/pulls/harisekhon/mesos.svg)](https://hub.docker.com/r/harisekhon/mesos) -\n[harisekhon/mesos](https://hub.docker.com/r/harisekhon/mesos)\n\n[![Docker Build Status](https://img.shields.io/docker/cloud/build/harisekhon/nagios-plugin-kafka.svg)](https://hub.docker.com/r/harisekhon/nagios-plugin-kafka/builds)\n[![DockerHub Pulls](https://img.shields.io/docker/pulls/harisekhon/nagios-plugin-kafka.svg)](https://hub.docker.com/r/harisekhon/nagios-plugin-kafka) -\n[harisekhon/nagios-plugin-kafka](https://hub.docker.com/r/harisekhon/nagios-plugin-kafka)\n\n[![Docker Build Status](https://img.shields.io/docker/cloud/build/harisekhon/nagios-plugins.svg)](https://hub.docker.com/r/harisekhon/nagios-plugins/builds)\n[![DockerHub Pulls](https://img.shields.io/docker/pulls/harisekhon/nagios-plugins.svg)](https://hub.docker.com/r/harisekhon/nagios-plugins) -\n[harisekhon/nagios-plugins](https://hub.docker.com/r/harisekhon/nagios-plugins)\n\n[![Docker Build Status](https://img.shields.io/docker/cloud/build/harisekhon/nifi.svg)](https://hub.docker.com/r/harisekhon/nifi/builds)\n[![DockerHub Pulls](https://img.shields.io/docker/pulls/harisekhon/nifi.svg)](https://hub.docker.com/r/harisekhon/nifi) -\n[harisekhon/nifi](https://hub.docker.com/r/harisekhon/nifi)\n\n[![Docker Build Status](https://img.shields.io/docker/cloud/build/harisekhon/presto.svg)](https://hub.docker.com/r/harisekhon/presto/builds)\n[![DockerHub Pulls](https://img.shields.io/docker/pulls/harisekhon/presto.svg)](https://hub.docker.com/r/harisekhon/presto) -\n[harisekhon/presto](https://hub.docker.com/r/harisekhon/presto)\n\n[![Docker Build Status](https://img.shields.io/docker/cloud/build/harisekhon/presto-cli.svg)](https://hub.docker.com/r/harisekhon/presto-cli/builds)\n[![DockerHub Pulls](https://img.shields.io/docker/pulls/harisekhon/presto-cli.svg)](https://hub.docker.com/r/harisekhon/presto-cli) -\n[harisekhon/presto-cli](https://hub.docker.com/r/harisekhon/presto-cli)\n\n[![Docker Build Status](https://img.shields.io/docker/cloud/build/harisekhon/presto-cli-dev.svg)](https://hub.docker.com/r/harisekhon/presto-cli-dev/builds)\n[![DockerHub Pulls](https://img.shields.io/docker/pulls/harisekhon/presto-cli-dev.svg)](https://hub.docker.com/r/harisekhon/presto-cli-dev) -\n[harisekhon/presto-cli-dev](https://hub.docker.com/r/harisekhon/presto-cli-dev)\n\n[![Docker Build Status](https://img.shields.io/docker/cloud/build/harisekhon/presto-dev.svg)](https://hub.docker.com/r/harisekhon/presto-dev/builds)\n[![DockerHub Pulls](https://img.shields.io/docker/pulls/harisekhon/presto-dev.svg)](https://hub.docker.com/r/harisekhon/presto-dev) -\n[harisekhon/presto-dev](https://hub.docker.com/r/harisekhon/presto-dev)\n\n[![Docker Build Status](https://img.shields.io/docker/cloud/build/harisekhon/pytools.svg)](https://hub.docker.com/r/harisekhon/pytools/builds)\n[![DockerHub Pulls](https://img.shields.io/docker/pulls/harisekhon/pytools.svg)](https://hub.docker.com/r/harisekhon/pytools) -\n[harisekhon/pytools](https://hub.docker.com/r/harisekhon/pytools)\n\n[![Docker Build Status](https://img.shields.io/docker/cloud/build/harisekhon/rabbitmq-cluster.svg)](https://hub.docker.com/r/harisekhon/rabbitmq-cluster/builds)\n[![DockerHub Pulls](https://img.shields.io/docker/pulls/harisekhon/rabbitmq-cluster.svg)](https://hub.docker.com/r/harisekhon/rabbitmq-cluster) -\n[harisekhon/rabbitmq-cluster](https://hub.docker.com/r/harisekhon/rabbitmq-cluster)\n\n[![Docker Build Status](https://img.shields.io/docker/cloud/build/harisekhon/riak.svg)](https://hub.docker.com/r/harisekhon/riak/builds)\n[![DockerHub Pulls](https://img.shields.io/docker/pulls/harisekhon/riak.svg)](https://hub.docker.com/r/harisekhon/riak) -\n[harisekhon/riak](https://hub.docker.com/r/harisekhon/riak)\n\n[![Docker Build Status](https://img.shields.io/docker/cloud/build/harisekhon/riak-dev.svg)](https://hub.docker.com/r/harisekhon/riak-dev/builds)\n[![DockerHub Pulls](https://img.shields.io/docker/pulls/harisekhon/riak-dev.svg)](https://hub.docker.com/r/harisekhon/riak-dev) -\n[harisekhon/riak-dev](https://hub.docker.com/r/harisekhon/riak-dev)\n\n[![Docker Build Status](https://img.shields.io/docker/cloud/build/harisekhon/serf.svg)](https://hub.docker.com/r/harisekhon/serf/builds)\n[![DockerHub Pulls](https://img.shields.io/docker/pulls/harisekhon/serf.svg)](https://hub.docker.com/r/harisekhon/serf) -\n[harisekhon/serf](https://hub.docker.com/r/harisekhon/serf)\n\n[![Docker Build Status](https://img.shields.io/docker/cloud/build/harisekhon/solr.svg)](https://hub.docker.com/r/harisekhon/solr/builds)\n[![DockerHub Pulls](https://img.shields.io/docker/pulls/harisekhon/solr.svg)](https://hub.docker.com/r/harisekhon/solr) -\n[harisekhon/solr](https://hub.docker.com/r/harisekhon/solr)\n\n[![Docker Build Status](https://img.shields.io/docker/cloud/build/harisekhon/solrcloud.svg)](https://hub.docker.com/r/harisekhon/solrcloud/builds)\n[![DockerHub Pulls](https://img.shields.io/docker/pulls/harisekhon/solrcloud.svg)](https://hub.docker.com/r/harisekhon/solrcloud) -\n[harisekhon/solrcloud](https://hub.docker.com/r/harisekhon/solrcloud)\n\n[![Docker Build Status](https://img.shields.io/docker/cloud/build/harisekhon/solrcloud-dev.svg)](https://hub.docker.com/r/harisekhon/solrcloud-dev/builds)\n[![DockerHub Pulls](https://img.shields.io/docker/pulls/harisekhon/solrcloud-dev.svg)](https://hub.docker.com/r/harisekhon/solrcloud-dev) -\n[harisekhon/solrcloud-dev](https://hub.docker.com/r/harisekhon/solrcloud-dev)\n\n[![Docker Build Status](https://img.shields.io/docker/cloud/build/harisekhon/spark.svg)](https://hub.docker.com/r/harisekhon/spark/builds)\n[![DockerHub Pulls](https://img.shields.io/docker/pulls/harisekhon/spark.svg)](https://hub.docker.com/r/harisekhon/spark) -\n[harisekhon/spark](https://hub.docker.com/r/harisekhon/spark)\n\n[![Docker Build Status](https://img.shields.io/docker/cloud/build/harisekhon/spotify-tools.svg)](https://hub.docker.com/r/harisekhon/spotify-tools/builds)\n[![DockerHub Pulls](https://img.shields.io/docker/pulls/harisekhon/spotify-tools.svg)](https://hub.docker.com/r/harisekhon/spotify-tools) -\n[harisekhon/spotify-tools](https://hub.docker.com/r/harisekhon/spotify-tools)\n\n[![Docker Build Status](https://img.shields.io/docker/cloud/build/harisekhon/superset.svg)](https://hub.docker.com/r/harisekhon/superset/builds)\n[![DockerHub Pulls](https://img.shields.io/docker/pulls/harisekhon/superset.svg)](https://hub.docker.com/r/harisekhon/superset) -\n[harisekhon/superset](https://hub.docker.com/r/harisekhon/superset)\n\n[![Docker Build Status](https://img.shields.io/docker/cloud/build/harisekhon/tachyon.svg)](https://hub.docker.com/r/harisekhon/tachyon/builds)\n[![DockerHub Pulls](https://img.shields.io/docker/pulls/harisekhon/tachyon.svg)](https://hub.docker.com/r/harisekhon/tachyon) -\n[harisekhon/tachyon](https://hub.docker.com/r/harisekhon/tachyon)\n\n[![Docker Build Status](https://img.shields.io/docker/cloud/build/harisekhon/tcollector.svg)](https://hub.docker.com/r/harisekhon/tcollector/builds)\n[![DockerHub Pulls](https://img.shields.io/docker/pulls/harisekhon/tcollector.svg)](https://hub.docker.com/r/harisekhon/tcollector) -\n[harisekhon/tcollector](https://hub.docker.com/r/harisekhon/tcollector)\n\n[![Docker Build Status](https://img.shields.io/docker/cloud/build/harisekhon/tools.svg)](https://hub.docker.com/r/harisekhon/tools/builds)\n[![DockerHub Pulls](https://img.shields.io/docker/pulls/harisekhon/tools.svg)](https://hub.docker.com/r/harisekhon/tools) -\n[harisekhon/tools](https://hub.docker.com/r/harisekhon/tools)\n\n[![Docker Build Status](https://img.shields.io/docker/cloud/build/harisekhon/ubuntu-dev.svg)](https://hub.docker.com/r/harisekhon/ubuntu-dev/builds)\n[![DockerHub Pulls](https://img.shields.io/docker/pulls/harisekhon/ubuntu-dev.svg)](https://hub.docker.com/r/harisekhon/ubuntu-dev) -\n[harisekhon/ubuntu-dev](https://hub.docker.com/r/harisekhon/ubuntu-dev)\n\n[![Docker Build Status](https://img.shields.io/docker/cloud/build/harisekhon/ubuntu-github.svg)](https://hub.docker.com/r/harisekhon/ubuntu-github/builds)\n[![DockerHub Pulls](https://img.shields.io/docker/pulls/harisekhon/ubuntu-github.svg)](https://hub.docker.com/r/harisekhon/ubuntu-github) -\n[harisekhon/ubuntu-github](https://hub.docker.com/r/harisekhon/ubuntu-github)\n\n[![Docker Build Status](https://img.shields.io/docker/cloud/build/harisekhon/ubuntu-java.svg)](https://hub.docker.com/r/harisekhon/ubuntu-java/builds)\n[![DockerHub Pulls](https://img.shields.io/docker/pulls/harisekhon/ubuntu-java.svg)](https://hub.docker.com/r/harisekhon/ubuntu-java) -\n[harisekhon/ubuntu-java](https://hub.docker.com/r/harisekhon/ubuntu-java)\n\n[![Docker Build Status](https://img.shields.io/docker/cloud/build/harisekhon/zookeeper.svg)](https://hub.docker.com/r/harisekhon/zookeeper/builds)\n[![DockerHub Pulls](https://img.shields.io/docker/pulls/harisekhon/zookeeper.svg)](https://hub.docker.com/r/harisekhon/zookeeper) -\n[harisekhon/zookeeper](https://hub.docker.com/r/harisekhon/zookeeper)\n\n[![Docker Build Status](https://img.shields.io/docker/cloud/build/harisekhon/zookeeper-dev.svg)](https://hub.docker.com/r/harisekhon/zookeeper-dev/builds)\n[![DockerHub Pulls](https://img.shields.io/docker/pulls/harisekhon/zookeeper-dev.svg)](https://hub.docker.com/r/harisekhon/zookeeper-dev) -\n[harisekhon/zookeeper-dev](https://hub.docker.com/r/harisekhon/zookeeper-dev)\n\n"
        },
        {
          "name": "Gemfile",
          "type": "blob",
          "size": 0.8955078125,
          "content": "#\n#  Author: Hari Sekhon\n#  Date: 2022-05-13 15:25:18 +0100 (Fri, 13 May 2022)\n#\n#  vim:ts=4:sts=4:sw=4:et\n#\n#  https://github.com/HariSekhon/DevOps-Bash-tools\n#\n#  License: see accompanying Hari Sekhon LICENSE file\n#\n#  If you're using my code you're welcome to connect with me on LinkedIn and optionally send me feedback to help steer this or other code I publish\n#\n#  https://www.linkedin.com/in/HariSekhon\n#\n\n# ============================================================================ #\n#                                 G e m f i l e\n# ============================================================================ #\n\n# https://bundler.io/gemfile.html\n\n# This isn't automatically installed since ruby code is not much used in this repo\n\n# see also:\n#\n#   setup/gem-packages.txt\n#   setup/gem-packages-desktop.txt  # optional for desktop use\n\nsource 'https://rubygems.org'\n\ngem 'cfn-nag'\ngem 'json'\ngem 'gitlab'\n"
        },
        {
          "name": "Jenkinsfile",
          "type": "blob",
          "size": 2.2158203125,
          "content": "//  vim:ts=4:sts=4:sw=4:et:filetype=groovy:syntax=groovy\n//\n//  Author: Hari Sekhon\n//  Date: 2017-06-28 12:39:02 +0200 (Wed, 28 Jun 2017)\n//\n//  https://github.com/HariSekhon/DevOps-Bash-tools\n//\n//  License: see accompanying Hari Sekhon LICENSE file\n//\n//  If you're using my code you're welcome to connect with me on LinkedIn and optionally send me feedback to help steer this or other code I publish\n//\n//  https://www.linkedin.com/in/HariSekhon\n//\n\n// ========================================================================== //\n//                        J e n k i n s   P i p e l i n e\n// ========================================================================== //\n\n// Epic Jenkinsfile template:\n//\n// https://github.com/HariSekhon/Templates/blob/master/Jenkinsfile\n\n\n// Official Documentation:\n//\n// https://jenkins.io/doc/book/pipeline/syntax/\n//\n// https://www.jenkins.io/doc/pipeline/steps/\n//\n// https://www.jenkins.io/doc/pipeline/steps/workflow-basic-steps/\n\n\npipeline {\n  // to run on Docker or Kubernetes, see the master Jenkinsfile template listed at the top\n  agent any\n\n  options {\n    timestamps()\n\n    timeout(time: 2, unit: 'HOURS')\n  }\n\n  triggers {\n    cron('H 10 * * 1-5')\n    pollSCM('H/2 * * * *')\n  }\n\n  stages {\n    stage ('Checkout') {\n      steps {\n        checkout([$class: 'GitSCM', branches: [[name: '*/master']], doGenerateSubmoduleConfigurations: false, extensions: [], submoduleCfg: [], userRemoteConfigs: [[credentialsId: '', url: 'https://github.com/HariSekhon/DevOps-Bash-tools']]])\n      }\n    }\n\n    stage('Build') {\n      steps {\n        echo \"Running ${env.JOB_NAME} Build ${env.BUILD_ID} on ${env.JENKINS_URL}\"\n        echo 'Building...'\n        timeout(time: 10, unit: 'MINUTES') {\n          retry(3) {\n//            sh 'apt update -q'\n//            sh 'apt install -qy make'\n//            sh 'make init'\n            sh \"\"\"\n              setup/ci_bootstrap.sh &&\n              make init\n            \"\"\"\n          }\n        }\n        timeout(time: 180, unit: 'MINUTES') {\n          sh 'make ci'\n        }\n      }\n    }\n\n    stage('Test') {\n      options {\n        retry(2)\n      }\n      steps {\n        echo 'Testing...'\n        timeout(time: 120, unit: 'MINUTES') {\n          sh 'make test'\n        }\n      }\n    }\n  }\n}\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0263671875,
          "content": "Copyright 2016 Hari Sekhon\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n"
        },
        {
          "name": "Library",
          "type": "tree",
          "content": null
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 12.6044921875,
          "content": "#\n#  Author: Hari Sekhon\n#  Date: 2016-01-17 12:56:53 +0000 (Sun, 17 Jan 2016)\n#\n#  vim:ts=4:sts=4:sw=4:noet\n#\n#  https://github.com/HariSekhon/DevOps-Bash-tools\n#\n#  If you're using my code you're welcome to connect with me on LinkedIn and optionally send me feedback\n#\n#  https://www.linkedin.com/in/HariSekhon\n#\n\ninclude Makefile.in\n\nREPO := HariSekhon/DevOps-Bash-tools\n\nCONF_FILES := $(shell sed \"s/\\#.*//; /^[[:space:]]*$$/d\" setup/files.txt)\n\n#CODE_FILES := $(shell find . -type f -name '*.sh' -o -type f -name '.bash*' | sort)\n#CODE_FILES := $(shell git ls-files | grep -E -e '\\.sh$$' -e '\\.bash[^/]*$$' -e '\\.groovy$$' | sort)\nCODE_FILES := $(shell \\\n\tif type git >/dev/null 2>&1; then \\\n\t\tgit ls-files | \\\n\t\tgrep -E -e '\\.sh$$' -e '\\.bash[^/]*$$' -e '\\.groovy$$' | \\\n\t\tsort | \\\n\t\twhile read -r filepath; do \\\n\t\t\ttest -f \"$$filepath\" || continue; \\\n\t\t\ttest -d \"$$filepath\" && continue; \\\n\t\t\ttest -L \"$$filepath\" && continue; \\\n\t\t\techo \"$$filepath\"; \\\n\t\tdone; \\\n\telse \\\n\t\tfind . -type f; \\\n\tfi \\\n)\n\n\nBASH_PROFILE_FILES := $(shell echo .bashrc .bash_profile .bash.d/*.sh)\n\n#.PHONY: *\n\nCURRENT_BRANCH := $(shell git rev-parse --abbrev-ref HEAD)\nTRUNK_BRANCH := $(shell git symbolic-ref refs/remotes/origin/HEAD | sed 's|.*/||')\n\nDEFAULT_TITLE := [GD-00] - merge $(CURRENT_BRANCH) to $(TRUNK_BRANCH)\n\ntitle ?= $(DEFAULT_TITLE)\n\n# ===================\ndefine MAKEFILE_USAGE\n\n  Repo specific options:\n\n    make install                builds all script dependencies, installs AWS CLI, GitHub CLI, symlinks all config files to $$HOME and adds sourcing of bash profile\n\n    make link                   symlinks all config files to $$HOME and adds sourcing of bash profile\n    make unlink                 removes all symlinks pointing to this repo's config files and removes the sourcing lines from .bashrc and .bash_profile\n\n    make python-desktop         installs all Python Pip packages for desktop workstation listed in setup/pip-packages-desktop.txt\n    make perl-desktop           installs all Perl CPAN packages for desktop workstation listed in setup/cpan-packages-desktop.txt\n    make ruby-desktop           installs all Ruby Gem packages for desktop workstation listed in setup/gem-packages-desktop.txt\n    make golang-desktop         installs all Golang packages for desktop workstation listed in setup/go-packages-desktop.txt\n    make nodejs-desktop         installs all NodeJS packages for desktop workstation listed in setup/npm-packages-desktop.txt\n\n    make desktop                installs all of the above + many desktop OS packages listed in setup/\n\n    make mac-desktop            all of the above + installs a bunch of major common workstation software packages like Ansible, Terraform, MiniKube, MiniShift, SDKman, Travis CI, CCMenu, Parquet tools etc.\n    make linux-desktop\n\n    make ls-scripts             print list of scripts in this project, ignoring code libraries in lib/ and .bash.d/\n\n    make github-cli             installs GitHub CLI\n    make kubernetes             installs Kubernetes kubectl and kustomize to ~/bin/\n    make terraform              installs Terraform to ~/bin/\n    make vim                    installs Vundle and plugins\n    make tmux                   installs TMUX TPM and plugin for kubernetes context\n    make ccmenu                 installs and (re)configures CCMenu to watch this and all other major HariSekhon GitHub repos\n    make status                 open the Github Status page of all my repos build statuses across all CI platforms\n\n    make aws                    installs AWS CLI tools\n    make azure                  installs Azure CLI\n    make gcp                    installs Google Cloud SDK\n\n    make aws-shell              sets up AWS Cloud Shell: installs core packages and links configs\n                                (maintains itself across future Cloud Shells via .aws_customize_environment hook)\n    make gcp-shell              sets up GCP Cloud Shell: installs core packages and links configs\n                                (maintains itself across future Cloud Shells via .customize_environment hook)\n    make azure-shell            sets up Azure Cloud Shell (limited compared to gcp-shell, doesn't install OS packages since there is no sudo)\nendef\n\n# not including azure here because it requires interactive prompt and hangs automatic testing of make docker-*\n.PHONY: build\nbuild:\n\t@echo ================\n\t@echo Bash Tools Build\n\t@echo ================\n\t@$(MAKE) git-summary\n\t@$(MAKE) init\n\t@$(MAKE) system-packages\n\t@$(MAKE) aws github-cli\n\n.PHONY: init\ninit: git\n\t@echo \"running init:\"\n\tgit submodule update --init --recursive\n\t@echo\n\n.PHONY: install\ninstall: build\n\t@$(MAKE) link\n\t@$(MAKE) aws\n\t@$(MAKE) gcp\n\t@$(MAKE) github-cli\n\t@$(MAKE) pip\n\n.PHONY: uninstall\nuninstall: unlink\n\t@echo \"Not removing any system packages for safety\"\n\n.PHONY: bash\nbash: link\n\t@:\n\n.PHONY: link\nlink:\n\t@setup/shell_link.sh\n\n.PHONY: unlink\nunlink:\n\t@setup/shell_unlink.sh\n\n.PHONY: mac-desktop\nmac-desktop: desktop\n\t@setup/mac_desktop.sh\n\n.PHONY: mac\nmac: mac-desktop\n\t@:\n\n.PHONY: linux-desktop\nlinux-desktop: desktop\n\t@setup/linux_desktop.sh\n\n.PHONY: linux\nlinux: linux-desktop\n\t@:\n\n.PHONY:\nccmenu:\n\t@setup/ccmenu_setup.sh\n\n.PHONY: desktop\ndesktop: install\n\t@if [ -x /sbin/apk ];        then $(MAKE) apk-packages-desktop; fi\n\t@if [ -x /usr/bin/apt-get ]; then $(MAKE) apt-packages-desktop; fi\n\t@if [ -x /usr/bin/yum ];     then $(MAKE) yum-packages-desktop; fi\n\t@if [ `uname` = Darwin ]; then \\\n\t\tif type brew >/dev/null 2>/dev/null; then \\\n\t\t\t$(MAKE) homebrew-packages-desktop; \\\n\t\tfi; \\\n\tfi\n\t@# do these late so that we have the above system packages installed first to take priority and not install from source where we don't need to\n\t@$(MAKE) perl-desktop\n\t@$(MAKE) golang-desktop\n\t@$(MAKE) nodejs-desktop\n\t@$(MAKE) ruby-desktop\n\t@# no packages any more since jgrep is no longer found\n\t@#$(MAKE) ruby-desktop\n\n.PHONY: apk-packages-desktop\napk-packages-desktop: system-packages\n\t@echo \"Alpine desktop not supported at this time\"\n\t@exit 1\n\n.PHONY: apt-packages-desktop\napt-packages-desktop: system-packages\n\tNO_FAIL=1 NO_UPDATE=1 $(BASH_TOOLS)/packages/apt_install_packages.sh setup/deb-packages-desktop.txt\n\n.PHONY: yum-packages-desktop\nyum-packages-desktop: system-packages\n\tNO_FAIL=1 NO_UPDATE=1 $(BASH_TOOLS)/packages/yum_install_packages.sh setup/rpm-packages-desktop.txt\n\n.PHONY: homebrew-packages-desktop\nhomebrew-packages-desktop: system-packages homebrew\n\t@:\n\n.PHONY: brew-packages-desktop\nbrew-packages-desktop: homebrew-packages-desktop\n\t@:\n\n.PHONY: homebrew\nhomebrew: system-packages brew\n\t@:\n\n.PHONY: brew\nbrew:\n\twhich -a brew || install/install_homebrew.sh\n\twhich -a wget || brew install wget\n\tNO_FAIL=1 NO_UPDATE=1 $(BASH_TOOLS)/packages/brew_install_packages_if_absent.sh setup/brew-packages-desktop.txt\n\tNO_FAIL=1 NO_UPDATE=1 CASK=1 $(BASH_TOOLS)/packages/brew_install_packages_if_absent.sh setup/brew-packages-desktop-casks.txt\n\t@# doesn't pass the packages correctly yet\n\t@#NO_FAIL=1 NO_UPDATE=1 TAP=1 $(BASH_TOOLS)/packages/brew_install_packages.sh setup/brew-packages-desktop-taps.txt\n\tNO_FAIL=1 NO_UPDATE=1 TAP=1 $(BASH_TOOLS)/packages/brew_install_packages.sh setup/brew-packages-desktop-taps.txt\n\n.PHONY: perl-desktop\nperl-desktop: system-packages cpan-desktop\n\t@:\n\n.PHONY: cpan-desktop\ncpan-desktop: cpan\n\tNO_FAIL=1 NO_UPDATE=1 $(BASH_TOOLS)/perl/perl_cpanm_install_if_absent.sh setup/cpan-packages-desktop.txt\n\n.PHONY: golang-desktop\ngolang-desktop: system-packages go-desktop\n\t@:\n\n.PHONY: go-desktop\ngo-desktop: system-packages go\n\t@:\n\n.PHONY: go\ngo:\n\tNO_FAIL=1 $(BASH_TOOLS)/packages/golang_install_if_absent.sh setup/go-packages-desktop.txt\n\n.PHONY: ruby-desktop\nruby-desktop: system-packages gem-desktop\n\t@:\n\n.PHONY: gem-desktop\ngem-desktop: gem\n\tNO_FAIL=1 $(BASH_TOOLS)/packages/ruby_gem_install_if_absent.sh setup/gem-packages-desktop.txt\n\n.PHONY: python-desktop\npython-desktop: system-packages pip-desktop\n\n.PHONY: pip\npip-desktop: pip\n\tPIP=$(PIP) ./python/python_pip_install_if_absent.sh setup/pip-packages-desktop.txt\n\tif uname -s | grep -q Darwin; then \\\n\t\tPIP=$(PIP) ./python/python_pip_install_if_absent.sh setup/pip-packages-mac.txt; \\\n\tfi\n\n.PHONY: nodejs-desktop\nnodejs-desktop: system-packages npm-desktop\n\n.PHONY: npm-desktop\nnpm-desktop: npm\n\t$(BASH_TOOLS)/packages/nodejs_npm_install_if_absent.sh $(BASH_TOOLS)/setup/npm-packages-desktop.txt\n\n.PHONY: aws\naws: system-packages python-version\n\t@if ! command -v aws; then install/install_aws_cli.sh; fi\n#    @$(MAKE) codecommit\n#\n#.PHONY: codecommit\n#codecommit:\n\t@# needed for github_mirror_repos_to_aws_codecommit.sh and dependent GitHub Actions workflows\n\t@if uname -s | grep -q Darwin; then \\\n\t\txargs(){ \\\n\t\t\tgxargs \"$$@\"; \\\n\t\t}; \\\n\tfi; \\\n\tgrep '^git-remote-codecommit' requirements.txt | \\\n\tPIP=$(PIP) xargs --no-run-if-empty ./python/python_pip_install_if_absent.sh || :\n\n.PHONY: aws-shell\naws-shell:\n\t@if [ \"${AWS_EXECUTION_ENV:-}\" != \"CloudShell\" ]; then echo \"Not running inside AWS Cloud Shell\"; exit 1; fi\n\t@$(MAKE) system-packages aws link\n\n.PHONY: azure\nazure: system-packages\n\t@install/install_azure_cli.sh\n\n.PHONY: azure-shell\nazure-shell: link\n\t:\n\n.PHONY: gcp\ngcp: system-packages\n\t@./install/install_gcloud_sdk.sh\n\t@./install/install_cloud_sql_proxy.sh\n\n.PHONY: gcp-shell\ngcp-shell:\n\t@if [ -z \"${DEVSHELL_PROJECT_ID:-}\" ]; then echo \"Not running inside Google Cloud Shell\"; exit 1; fi\n\t@$(MAKE) system-packages link\n\n.PHONY: github-cli\ngithub-cli: ~/bin/gh\n\t@:\n\n~/bin/gh:\n\tinstall/install_github_cli.sh\n\n.PHONY:\ndigital-ocean: ~/bin/doctl\n\t@:\n\n~/bin/doctl:\n\tinstall/install_doctl.sh\n\n.PHONY: kubernetes\nkubernetes: kubectl kustomize\n\t@:\n\n.PHONY: k8s\nk8s: kubernetes\n\t@:\n\n.PHONY: kubectl\nkubectl: ~/bin/kubectl\n\t@:\n\n~/bin/kubectl:\n\tinstall/install_kubectl.sh\n\n.PHONY: kustomize\nkustomize: ~/bin/kustomize\n\t@:\n\n~/bin/kustomize:\n\tinstall/install_kustomize.sh\n\n.PHONY: vim\nvim: ~/.vim/bundle/Vundle.vim\n\t@:\n\n~/.vim/bundle/Vundle.vim:\n\tinstall/install_vundle.sh\n\n.PHONY: tmux\ntmux: ~/.tmux/plugins/tpm ~/.tmux/plugins/kube.tmux\n\t@:\n\n~/.tmux/plugins/tpm:\n\tgit clone https://github.com/tmux-plugins/tpm ~/.tmux/plugins/tpm\n\n~/.tmux/plugins/kube.tmux:\n\twget -O ~/.tmux/plugins/kube.tmux https://raw.githubusercontent.com/jonmosco/kube-tmux/master/kube.tmux\n\n.PHONY: test\ntest:\n\t./checks/check_all.sh\n\n.PHONY: clean\nclean:\n\t@rm -fv -- setup/terraform.zip\n\n.PHONY: ls-scripts\nls-scripts:\n\t@$(MAKE) ls | grep -v -e 'lib/' -e '\\.bash'\n\n.PHONY: ls-scripts2\nls-scripts2:\n\t@$(MAKE) ls | grep -v -e 'lib/' -e '\\.bash' -e 'setup/'\n\n.PHONY: wcbashrc\nwcbashrc:\n\t@wc $(BASH_PROFILE_FILES)\n\t@printf \"Total Bash Profile files: \"\n\t@ls $(BASH_PROFILE_FILES) | wc -l\n\n.PHONY: wcbash\nwcbash: wcbashrc\n\t@:\n\n.PHONY: wcbashrc2\nwcbashrc2:\n\t@printf \"Total Bash Profile files: \"\n\t@ls $(BASH_PROFILE_FILES) | wc -l\n\t@printf \"Total line count without # comments: \"\n\t@ls $(BASH_PROFILE_FILES) | xargs sed 's/#.*//;/^[[:space:]]*$$/d' | wc -l\n\n.PHONY: wcbash2\nwcbash2: wcbashrc2\n\t@:\n\n.PHONY: pipreqs-mapping\npipreqs-mapping:\n\t#wget -O resources/pipreqs_mapping.txt https://raw.githubusercontent.com/HariSekhon/pipreqs/mysql-python/pipreqs/mapping\n\twget -O resources/pipreqs_mapping.txt https://raw.githubusercontent.com/bndr/pipreqs/master/pipreqs/mapping\n.PHONY: pip-mapping\npip-mapping: pipreqs-mapping\n\t@:\n\n.PHONY: status-page\nstatus-page:\n\t./cicd/generate_status_page.sh; . .bash.d/git.sh; gitu STATUS.md\n\n.PHONY: dialog-install\ndialog-install:\n\tinstall/install_packages.sh dialog\n\n# Raise Pull Requests from the command line like this:\n#\n#\tYou need GitHub CLI installed ('make' installs it for you) and authenticated eg.:\n#\n#\t\tgh auth login\n#\n#\t\t# https://cli.github.com/manual/gh_auth_login\n#\n#\tExample:\n#\n#\t\tmake pr title=\"Hari code to avoid clicking\"\n#\n.PHONY: pr\npr: dialog-install\n\tgit push --set-upstream origin \"$(CURRENT_BRANCH)\"\n\tif [ -z \"$$GITHUB_PULL_REQUEST_TITLE\" ]; then \\\n\t\tif [ \"$(title)\" = \"$(DEFAULT_TITLE)\" ]; then \\\n\t\t\tGITHUB_PULL_REQUEST_TITLE=\"$$(dialog --inputbox \"Pull Request Title:\" 8 40 \"$(DEFAULT_TITLE)\" 3>&1 1>&2 2>&3)\"; \\\n\t\telse \\\n\t\t\tGITHUB_PULL_REQUEST_TITLE=\"$(title)\"; \\\n\t\tfi; \\\n\tfi; \\\n\texport GITHUB_PULL_REQUEST_TITLE; \\\n\tgithub_pull_request_create.sh \\\n\t\t\"$(REPO)\" \\\n\t\t\"$(CURRENT_BRANCH)\" \\\n\t\t\"$(TRUNK_BRANCH)\"\n\n# raise a PR in one command with Auto-Merge enabled - use this for trivial PRs of low / no impact like MkDocs updates\n.PHONY: auto-pr\nauto-pr: update\n\t@# - if GITHUB_PULL_REQUEST_AUTO_MERGE=true then marks the PR for auto-merge once it is approved and passes pre-requisite checks\n\t@# - if GITHUB_PULL_REQUEST_SQUASH=true while GITHUB_PULL_REQUEST_AUTO_MERGE=true then it marks\n\t@#   the PR's auto-merge to be done using a squash commit to avoid any CLI prompt for how to merge it\n\tGITHUB_PULL_REQUEST_AUTO_MERGE=true \\\n\tGITHUB_PULL_REQUEST_SQUASH=true \\\n\t$(MAKE) pr\n\n# Example:\n#\n#\tmake autopr title=\"Documented something\"\n#\n.PHONY: autopr\nautopr: auto-pr\n\t@:\n\n.PHONY: sync\nsync:\n\tsync_configs_to_adjacent_repos.sh\n"
        },
        {
          "name": "Makefile.in",
          "type": "blob",
          "size": 27.970703125,
          "content": "#\n#  Author: Hari Sekhon\n#  Date: 2013-02-03 10:25:36 +0000 (Sun, 03 Feb 2013)\n#\n#  https://github.com/HariSekhon/DevOps-Bash-tools\n#\n#  License: see accompanying Hari Sekhon LICENSE file\n#\n#  If you're using my code you're welcome to connect with me on LinkedIn and optionally send me feedback\n#  to help improve or steer this or other code I publish\n#\n#  https://www.linkedin.com/in/HariSekhon\n#\n\nifneq (\"$(wildcard bash-tools)\", \"\")\n\tBASH_TOOLS := bash-tools\nelse\n\tBASH_TOOLS := .\nendif\n\n# would fail bootstrapping on Alpine\n#SHELL := /usr/bin/env bash\n\nexport PATH := $(PATH):/bin:/usr/bin:/sbin:/usr/sbin:/usr/local/bin:/opt/homebrew/bin\n\n# Python breaking backwards compatibility as usual - set this to use historic behaviour and treat the user like an adult\n# this way if the install is run for --user instead of in a virtualenv it'll still work\nexport PIP_BREAK_SYSTEM_PACKAGES := 1\n\nDOCKER_IMAGE := harisekhon/github\n\nifneq (\"$(wildcard /.dockerenv)\", \"\")\n\tINSIDE_DOCKER := 1\nelse\n\tINSIDE_DOCKER :=\nendif\n\nCODE_FILES := $(shell \\\n\tif type git >/dev/null 2>&1; then \\\n\t\tgit ls-files | \\\n\t\twhile read filepath; do \\\n\t\t\ttest -f \"$$filepath\" || continue; \\\n\t\t\ttest -d \"$$filepath\" & continue; \\\n\t\t\ttest -L \"$$filepath\" & continue; \\\n\t\t\techo \"$$filepath\"; \\\n\t\tdone; \\\n\tfi \\\n)\n\nCPANM := cpanm\nexport PIP := pip3\nexport PYTHON := python3\n\nFATPACKS_DIR := fatpacks\n\nSUDO := sudo\nSUDO_PIP := sudo -H\nSUDO_PERL := sudo\n\nPYTHON_VIRTUALENV :=\n\nifdef PERLBREW_PERL\n\t# can't put this here, nor @commented, otherwise gets error - \"commands commence before first target.  Stop.\"\n\t#echo \"Perlbrew environment detected, not calling sudo\"\n\tSUDO_PERL =\nelse\n\tPERLBREW_PERL :=\nendif\n\n# Travis has custom python install earlier in $PATH even in Perl builds so need to install PyPI modules locally to non-system python otherwise they're not found by programs.\n# Perms not set correctly on custom python install in Travis perl build so workaround is done to chown to travis user in .travis.yml\n# Better than modifying $PATH to put /usr/bin first which is likely to affect many other things including potentially not finding the perlbrew installation first\n# Looks like Perl travis builds are now using system Python - do not use TRAVIS env\nifdef VIRTUAL_ENV\n\t#echo \"Virtual Env / Conda detected, not calling sudo\"\n\tSUDO_PIP :=\n\tPYTHON_VIRTUALENV := 1\nendif\nifdef CONDA_DEFAULT_ENV\n\tSUDO_PIP :=\n\tPYTHON_VIRTUALENV := 1\nendif\n\n# must come after to reset SUDO_PERL/SUDO_PIP to blank if root\n# EUID / UID not exported in Make\n# USER not populated in Docker\nifeq '$(shell id -u)' '0'\n\t#echo \"root UID detected, not calling sudo\"\n\tSUDO :=\n\tSUDO_PERL :=\n\tSUDO_PIP :=\nendif\n\n# placeholders to silence check_makefile.sh warnings - should be set in client Makefiles after sourcing\nifndef REPO\n\tREPO := NOTSET\nendif\nifndef ARGS\n\tARGS := NOTSET\nendif\nifndef CONF_FILES\n\tCONF_FILES := NOTSET\nendif\n\ndefine MAKEFILE_USAGE_COMMON\n\n Usage:\n\n  Common Options:\n\n    make help                   show this message\n    make build                  installs all dependencies - OS packages and any language libraries via native tools eg. pip, cpanm, gem, go etc that are not available via OS packages\n    make build-retry            retries 'make build' x 3 until success to try to mitigate temporary upstream repo failures triggering false alerts in CI systems\n    make ci                     prints env, then runs 'build-retry' for more resilient CI builds with debugging\n    make printenv               prints environment variables, CPU cores, OS release, $$PWD, Git branch, hashref etc. Useful for CI debugging\n    make system-packages        installs OS packages only (detects OS via whichever package manager is available)\n    make test                   run tests\n    make clean                  removes compiled / generated files, downloaded tarballs, temporary files etc.\n\n    make submodules             initialize and update submodules to the right release (done automatically by build / system-packages)\n    make init                   same as above, often useful to do in CI systems to get access to additional submodule provided targets such as 'make ci'\n\n    make cpan                   install any modules listed in any cpan-requirements.txt files if not already installed\n    make gem                    install any modules listed in any gem-requirements.txt files if not already installed\n    make npm                    install any modules listed in any npm-requirements.txt files if not already installed\n    make pip                    install any modules listed in any requirements.txt files if not already installed\n\n    make python-compile         compile any python files found in the current directory and 1 level of subdirectory\n    make pycompile\n\n    make github                 open browser at github project\n    make readme                 open browser at github's README\n    make github-url             print github url and copy to clipboard\n    make status                 open browser at Github CI Builds overview Status page for all projects\n\n    make ls                     print list of code files in project\n    make wc                     show counts of files and lines\n\nendef\n    #make ${VENV}                make a virtualenv in the base directory (see VENV)\n    #make pip-install            install python packages in requirements.txt\n    #make git-config             set local git configuration\nexport MAKEFILE_USAGE_COMMON\nexport MAKEFILE_USAGE\n\n# doesn't seem to work\n#.DEFAULT: build\n#\t@echo running default\n#\t$(MAKE) build\n\n# won't be run the first time - will default to first target which will only then initialize submodules\n.PHONY: default\ndefault: git printenv\n\t@$(MAKE) main\n\n.PHONY: printenv\nprintenv: git\n\t@ printf \"CPU Cores: \"; nproc 2>/dev/null || sysctl -n hw.ncpu 2>/dev/null; :\n\t@ # $$USER not always set in sh\n\t@ # printf \"Git hashref: \"; git rev-parse HEAD\n\t@ # printf \"Git hashref: \"; git log --pretty=format:'%H' -n 1\n\t@ # printf \"Git branch:  \"; git branch --show-current # doesn't work on Alpine\n\t@ # printf \"Git branch:  \"; git show-branch --current # prints too many branches\n\t@ # sort --ignore-case switch not available on Alpine, must use sort -f which is available on both Mac and all Linux distros\n\t@ . $(BASH_TOOLS)/lib/ci.sh || : ; \\\n\t\tif is_CI || test -f /.dockerenv; then \\\n\t\t\techo; \\\n\t\t\techo \"USER = `whoami`\"; \\\n\t\t\techo \"PWD = $$PWD\"; \\\n\t\t\techo; \\\n\t\t\tprintf \"Git branch: \"; git rev-parse --abbrev-ref HEAD; \\\n\t\t\tprintf \"Git commit: \"; git log --pretty=format:\"%ai  %cn  %H  %s\" -n 1; echo; \\\n\t\t\techo; \\\n\t\t\tif [ -f /.dockerenv  ]; then \\\n\t\t\t\techo \"Running inside Docker:\"; \\\n\t\t\t\tls -l /.dockerenv 2>/dev/null; \\\n\t\t\tfi; \\\n\t\t\techo; \\\n\t\t\techo \"OS RELEASE:\"; \\\n\t\t\techo; \\\n\t\t\tuname -a || : ; \\\n\t\t\techo; \\\n\t\t\tcat /etc/*release || : ; \\\n\t\t\techo; \\\n\t\t\tunset MAKEFILE_USAGE; \\\n\t\t\tunset MAKEFILE_USAGE_COMMON; \\\n\t\t\tunset TERMCAP; \\\n\t\t\techo; \\\n\t\t\techo \"CI ENVIRONMENT:\"; \\\n\t\t\techo; \\\n\t\t\tenv | grep -vi -e PASS -e TOKEN -e KEY -e SECRET | sort -f; \\\n\t\t\techo; \\\n\t\t\techo; \\\n\t\t\tif which java 2>/dev/null; then \\\n\t\t\t\twhich java; \\\n\t\t\t\tjava -version; \\\n\t\t\t\techo; \\\n\t\t\tfi; \\\n\t\t\techo \"PATH:\"; echo \"$$PATH\" | tr ':' '\\n'; \\\n\t\t\techo; \\\n\t\telse \\\n\t\t\tenv | grep -E 'BUILD|PIPELINE|JOB|STAGE|\\<CI_|^CI=' | grep -v TOKEN || : ; \\\n\t\tfi | cat # stops git commands from entering pager\n\n.PHONY: git-summary\ngit-summary: init\n\t@echo\n\t@echo \"Git summary:\"\n\t@$(BASH_TOOLS)/git/git_summary_line.sh\n\t@echo\n\n.PHONY: ci\nci: printenv\n\t$(MAKE) build-retry\n\n.PHONY: build-retry\nbuild-retry: git\n\t$(BASH_TOOLS)/bin/retry.sh $(MAKE) build\n\n# won't be run the first time - will default to first target which will only then initialize submodules\n.PHONY: main\nmain: printenv\n\t@$(MAKE) build\n\n.PHONY: help\nhelp:\n\t@# this doesn't work because the macro insertion of a multiline literal breaks the line-based make format so we have to export to an env var instead of using natively\n\t@# even the unescaped macro literal in a commented breaks make\n\t@echo \"$$MAKEFILE_USAGE_COMMON $$MAKEFILE_USAGE\" # | less -RFXig # don't use less it will make target tests hang\n\t@echo\n\t@echo \"Now exiting usage help with status code 3 to explicitly prevent silent build failures from stray 'help' arguments\"\n\t@exit 3\n\n.PHONY: usage\nusage: help\n\t@#:\n\n# clever but breaks 'make -n <target>' tests because the exit 3 doesn't actually get called and leads make to think there is a matching target, which then fail to execute\n# catchall - any unrecognized target will print usage\n#%::\n#\t@# don't use less, it will make target tests hang\n#\t@echo Unrecognized option $@; \\\n#\techo; \\\n#\t$(MAKE) usage;\n\n.PHONY: quick\nquick:\n\tQUICK=1 $(MAKE) build\n\n.PHONY: git\ngit:\n\t@# not using install_packages_if_absent.sh as we don't need a package on Mac, it comes with XCode\n\ttype git 2>/dev/null || $(BASH_TOOLS)/packages/install_packages.sh git\n\n.PHONY: submodules\nsubmodules: git\n\t@echo \"checking out any git submodules:\"\n\tgit submodule update --init --recursive\n\t@echo\n\n.PHONY: git-clean\ngit-clean: git\n\t@git clean -n -d\n\t@printf \"\\n\\n%s\" \"If you're happy with this list, run:\"\n\t@printf \"\\n\\n%s\\n\\n\" \"git clean -f -d\"\n\n.PHONY: gitignore\ngitignore:\n\t$(BASH_TOOLS)/git/update_gitignore.io.sh\n\n.PHONY: btest\nbtest: bash-test\n\t@:\n\n.PHONY: bash-test\nbash-test:\n\t$(BASH_TOOLS)/checks/check_all.sh\n\n.PHONY: test\n#test: precommit\ntest: bash-test\n\t@:\n\nprecommit: pre-commit\n\t@:\n\npre-commit:\n\tpre-commit run --all-files\n\n.PHONY: push\npush: test\n\tgit push\n\n.PHONY: system-packages\nsystem-packages: submodules\n\tif [ -x /sbin/apk ];        then $(MAKE) apk-packages; fi\n\tif [ -x /usr/bin/apt-get ]; then $(MAKE) apt-packages; fi\n\t@# /usr/bin/yum is a symlink to dnf-3 on newer RHEL systems, so fails -x /usr/bin/yum\n\tif [ -e /usr/bin/yum ];     then $(MAKE) yum-packages; fi\n\t@# /usr/local/bin/brew    on older macOS\n\t@# /opt/homebrew/bin/brew on newer macOS\n\tif which -a brew && [ `uname` = Darwin ]; then $(MAKE) homebrew-packages; fi\n\n.PHONY: system-packages-perl\nsystem-packages-perl: system-packages\n\tif [ -x /sbin/apk ];        then $(MAKE) apk-packages-perl; fi\n\tif [ -x /usr/bin/apt-get ]; then $(MAKE) apt-packages-perl; fi\n\t@# /usr/bin/yum is a symlink to dnf-3 on newer RHEL systems, so fails -x /usr/bin/yum\n\tif [ -e /usr/bin/yum ];     then $(MAKE) yum-packages-perl; fi\n\n.PHONY: system-packages-python\nsystem-packages-python: system-packages\n\tif [ -x /sbin/apk ];        then $(MAKE) apk-packages-python; fi\n\tif [ -x /usr/bin/apt-get ]; then $(MAKE) apt-packages-python; fi\n\t@# /usr/bin/yum is a symlink to dnf-3 on newer RHEL systems, so fails -x /usr/bin/yum\n\tif [ -e /usr/bin/yum ];     then $(MAKE) yum-packages-python; fi\n\n.PHONY: apk-packages\napk-packages:\n\t# not portable in Alpine sh\n\t#for x in apk-packages{,-perl,-python}{,-dev}.txt; do \\\n\n\tfor x in apk-packages.txt apk-packages-dev.txt; do \\\n\t\tfind . -maxdepth 3 -path \"*/setup/$$x\"; \\\n\tdone | xargs \"$(BASH_TOOLS)/packages/apk_install_packages.sh\"\n\t#for x in apk-packages-{optional,cpan,pip}.txt; do \\\n\n\tfor x in apk-packages-optional.txt; do \\\n\t\tfind . -maxdepth 3 -path \"*/setup/$$x\"; \\\n\tdone | NO_FAIL=1 NO_UPDATE=1 xargs \"$(BASH_TOOLS)/packages/apk_install_packages.sh\"\n\n.PHONY: apk-packages-perl\napk-packages-perl:\n\tfor x in apk-packages-perl.txt apk-packages-perl-dev.txt; do \\\n\t\tfind . -maxdepth 3 -path \"*/setup/$$x\"; \\\n\tdone | xargs \"$(BASH_TOOLS)/packages/apk_install_packages.sh\"\n\t#for x in apk-packages-{optional,cpan,pip}.txt; do \\\n\n\t# don't put comments inside the for loop, breaks syntax expecting 'done'\n\t# no point installing system cpan packages if using perlbrew as they won't be found inside perlbrew\n\tfor x in apk-packages-cpan.txt; do \\\n\t\tif [ -z \"$(PERLBREW_PERL)\" ]; then \\\n\t\t\tfind . -maxdepth 3 -path \"*/setup/$$x\"; \\\n\t\tfi; \\\n\tdone | NO_FAIL=1 NO_UPDATE=1 xargs \"$(BASH_TOOLS)/packages/apk_install_packages.sh\"\n\n.PHONY: apk-packages-python\napk-packages-python:\n\tfor x in apk-packages-python.txt apk-packages-python-dev.txt; do \\\n\t\tfind . -maxdepth 3 -path \"*/setup/$$x\"; \\\n\tdone | xargs \"$(BASH_TOOLS)/packages/apk_install_packages.sh\"\n\t# no point installing system pip packages when they won't be found in virtualenv and will need to be pip installed anyway\n\tfor x in apk-packages-pip.txt; do \\\n\t\tif [ -z \"$(PYTHON_VIRTUALENV)\" ]; then \\\n\t\t\tfind . -maxdepth 3 -path \"*/setup/$$x\"; \\\n\t\tfi; \\\n\tdone | NO_FAIL=1 NO_UPDATE=1 xargs \"$(BASH_TOOLS)/packages/apk_install_packages.sh\"\n\n.PHONY: apt-packages\napt-packages:\n\t#for x in deb-packages{,-perl,-python}{,-dev}.txt; do \\\n\n\tfor x in deb-packages.txt deb-packages-dev.txt; do \\\n\t\tfind . -maxdepth 3 -path \"*/setup/$$x\"; \\\n\tdone | xargs \"$(BASH_TOOLS)/packages/apt_install_packages.sh\"\n\t#for x in deb-packages-{optional,cpan,pip}.txt; do \\\n\n\tfor x in deb-packages-optional.txt; do \\\n\t\tfind . -maxdepth 3 -path \"*/setup/$$x\"; \\\n\tdone | NO_FAIL=1 NO_UPDATE=1 xargs \"$(BASH_TOOLS)/packages/apt_install_packages.sh\"\n\n.PHONY: apt-packages-perl\napt-packages-perl:\n\tfor x in deb-packages-perl.txt deb-packages-perl-dev.txt; do \\\n\t\tfind . -maxdepth 3 -path \"*/setup/$$x\"; \\\n\tdone | xargs \"$(BASH_TOOLS)/packages/apt_install_packages.sh\"\n\tfor x in deb-packages-cpan.txt; do \\\n\t\tif [ -z \"$(PERLBREW_PERL)\" ] && \\\n\t\t   [ -z \"$(GOOGLE_CLOUD_SHELL)\" ]; then \\\n\t\t\tfind . -maxdepth 3 -path \"*/setup/$$x\"; \\\n\t\tfi; \\\n\tdone | NO_FAIL=1 NO_UPDATE=1 xargs \"$(BASH_TOOLS)/packages/apt_install_packages.sh\"\n\n.PHONY: apt-packages-python\napt-packages-python:\n\tfor x in deb-packages-python.txt deb-packages-python-dev.txt; do \\\n\t\tfind . -maxdepth 3 -path \"*/setup/$$x\"; \\\n\tdone | xargs \"$(BASH_TOOLS)/packages/apt_install_packages.sh\"\n\tfor x in deb-packages-pip.txt; do \\\n\t\tif [ -z \"$(PYTHON_VIRTUALENV)\" ] && \\\n\t\t   [ -z \"$(GOOGLE_CLOUD_SHELL)\" ]; then \\\n\t\t\tfind . -maxdepth 3 -path \"*/setup/$$x\"; \\\n\t\tfi; \\\n\tdone  | NO_FAIL=1 NO_UPDATE=1 xargs \"$(BASH_TOOLS)/packages/apt_install_packages.sh\"\n\n.PHONY: yum-packages\nyum-packages:\n\t# needed for Fedora to have find and xargs to use below\n\t\"$(BASH_TOOLS)/packages/yum_install_packages.sh\" findutils\n\n\t# if on Amazon Linux 2 install epel this way\n\tif type -P amazon-linux-extras; then \\\n\t\t$(SUDO) amazon-linux-extras install epel -y; \\\n\tfi\n\t$(BASH_TOOLS)/install/install_epel_repo.sh\n\n\t# installing packages individually to catch package install failure, otherwise yum succeeds even if it misses a package\n\tfor x in rpm-packages.txt rpm-packages-dev.txt; do \\\n\t\tfind . -maxdepth 3 -path \"*/setup/$$x\"; \\\n\tdone | xargs \"$(BASH_TOOLS)/packages/yum_install_packages.sh\"\n\tfor x in rpm-packages-optional.txt; do \\\n\t\tfind . -maxdepth 3 -path \"*/setup/$$x\"; \\\n\tdone | NO_FAIL=1 xargs \"$(BASH_TOOLS)/packages/yum_install_packages.sh\"\n\n.PHONY: yum-packages-perl\nyum-packages-perl:\n\t# installing packages individually to catch package install failure, otherwise yum succeeds even if it misses a package\n\tfor x in rpm-packages-perl.txt rpm-packages-perl-dev.txt; do \\\n\t\tfind . -maxdepth 3 -path \"*/setup/$$x\"; \\\n\tdone | xargs \"$(BASH_TOOLS)/packages/yum_install_packages.sh\"\n\tfor x in rpm-packages-cpan.txt; do \\\n\t\tif [ -z \"$(PERLBREW_PERL)\" ]; then \\\n\t\t\tfind . -maxdepth 3 -path \"*/setup/$$x\"; \\\n\t\tfi; \\\n\tdone | NO_FAIL=1 xargs \"$(BASH_TOOLS)/packages/yum_install_packages.sh\"\n\n.PHONY: yum-packages-python\nyum-packages-python:\n\t# installing packages individually to catch package install failure, otherwise yum succeeds even if it misses a package\n\tfor x in rpm-packages-python.txt rpm-packages-python-dev.txt; do \\\n\t\tfind . -maxdepth 3 -path \"*/setup/$$x\"; \\\n\tdone | xargs \"$(BASH_TOOLS)/packages/yum_install_packages.sh\"\n\t. \"$(BASH_TOOLS)/lib/python.sh\"; \\\n\tset +o pipefail || : ; \\\n\tif ! inside_virtualenv; then \\\n\t\tfor x in rpm-packages-pip.txt; do \\\n\t\t\tfind . -maxdepth 3 -path \"*/setup/$$x\"; \\\n\t\tdone | NO_FAIL=1 xargs \"$(BASH_TOOLS)/packages/yum_install_packages.sh\"; \\\n\tfi\n\n.PHONY: homebrew-packages\nhomebrew-packages:\n\t# Fails if any of the packages are already installed, ignore and continue - if it's a problem the latest build steps will fail with missing headers\n\tfor x in brew-packages.txt; do \\\n\t\tfind . -maxdepth 3 -path \"*/setup/$$x\"; \\\n\tdone | NO_FAIL=1 xargs \"$(BASH_TOOLS)/packages/brew_install_packages.sh\"\n\t@# fix for OpenSSL 1.0 -> 1.1 library linkage breaking python -c 'import hashlib', which break pips, eg:\n\t@# https://stackoverflow.com/questions/20399331/error-importing-hashlib-with-python-2-7-but-not-with-2-6\n\t$(BASH_TOOLS)/setup/brew_fix_openssl_dependencies.sh\n\n.PHONY: system-packages-remove\nsystem-packages-remove:\n\tif [ -x /sbin/apk ];        then $(MAKE) apk-packages-remove; fi\n\tif [ -x /usr/bin/apt-get ]; then $(MAKE) apt-packages-remove; fi\n\tif [ -x /usr/bin/yum ];     then $(MAKE) yum-packages-remove; fi\n\n.PHONY: apk-packages-remove\napk-packages-remove:\n\tfor x in apk-packages-{,perl-,python-}dev.txt; do \\\n\t\tfind . -maxdepth 3 -path \"*/setup/$$x\"; \\\n\tdone | NO_FAIL=1 xargs \"$(BASH_TOOLS)/packages/apk_remove_packages.sh\"\n\t$(SUDO) rm -fr -- /var/cache/apk/*\n\n.PHONY: apt-packages-remove\napt-packages-remove:\n\tfor x in deb-packages-{,perl-,python-}dev.txt; do \\\n\t\tfind . -maxdepth 3 -path \"*/setup/$$x\"; \\\n\tdone | NO_FAIL=1 xargs \"$(BASH_TOOLS)/packages/apt_remove_packages.sh\"\n\n.PHONY: yum-packages-remove\nyum-packages-remove:\n\tfor x in rpm-packages-{,perl-,python-}dev.txt; do \\\n\t\tfind . -maxdepth 3 -path \"*/setup/$$x\"; \\\n\tdone | NO_FAIL=1 xargs \"$(BASH_TOOLS)/packages/yum_remove_packages.sh\"\n\n.PHONY: cpan\ncpan::\n\tfind . -maxdepth 3 -path '*/setup/cpan-requirements*.txt' | grep -v cpan-requirements-optional.txt | xargs --no-run-if-empty \"$(BASH_TOOLS)/perl/perl_cpanm_install_if_absent.sh\"\n\t@$(MAKE) cpan-optional\n\n.PHONY: cpan-optional\ncpan-optional::\n\tfind . -maxdepth 3 -path '*/setup/cpan-requirements-optional.txt' | NO_FAIL=1 xargs --no-run-if-empty \"$(BASH_TOOLS)/perl/perl_cpanm_install_if_absent.sh\"\n\n.PHONY: gems\ngems:: gem\n\t@:\n\n.PHONY: gem\ngem::\n\tfind . -maxdepth 3 -path '*/setup/gem-packages.txt' | xargs --no-run-if-empty \"$(BASH_TOOLS)/packages/ruby_gem_install_if_absent.sh\"\n\t@$(MAKE) gem-optional\n\n.PHONY: gem-optional\ngem-optional::\n\tfind . -maxdepth 3 -path '*/setup/gem-packages-optional.txt' | NO_FAIL=1 PIP=\"$(PIP)\" xargs --no-run-if-empty \"$(BASH_TOOLS)/packages/ruby_gem_install_if_absent.sh\"\n\n.PHONY: npm\nnpm::\n\tfind . -maxdepth 3 -path '*/setup/npm-requirements.txt' -o -path '*/setup/npm-packages.txt' | xargs --no-run-if-empty \"$(BASH_TOOLS)/packages/nodejs_npm_install_if_absent.sh\"\n\t@$(MAKE) npm-optional\n\n.PHONY: npm-optional\nnpm-optional::\n\tfind . -maxdepth 3 -path '*/setup/npm-requirements-optional.txt' | NO_FAIL=1 xargs --no-run-if-empty \"$(BASH_TOOLS)/packages/nodejs_npm_install_if_absent.sh\"\n\n.PHONY: pip\npip::\n\tfind . -maxdepth 3 -path '*/requirements.txt' | PIP=\"$(PIP)\" xargs --no-run-if-empty \"$(BASH_TOOLS)/python/python_pip_install_if_absent.sh\"\n\t@$(MAKE) pip-optional\n\n.PHONY: pip-optional\npip-optional::\n\tfind . -maxdepth 3 -path '*/requirements-optional.txt' | NO_FAIL=1 PIP=\"$(PIP)\" xargs --no-run-if-empty \"$(BASH_TOOLS)/python/python_pip_install_if_absent.sh\"\n\n.PHONY: pip-user\npip-user::\n\tPYTHON_USER_INSTALL=1 $(MAKE) pip\n\n.PHONY: fatpacks\nfatpacks:\n\t$(BASH_TOOLS)/perl/perl_generate_fatpacks.sh *.pl\n\t@echo\n\t@if [ -d lib/resources ]; then \\\n\t\tcp -av -- lib/resources fatpacks/; \\\n\tfi\n\t@if $(MAKE) -n fatpacks-local >/dev/null 2>&1; then \\\n\t\techo; \\\n\t\techo \"fatpacks-local target detected, running:\"; \\\n\t\t$(MAKE) fatpacks-local; \\\n\tfi\n\t@echo\n\t@if [ -n \"`ls \"$(FATPACKS_DIR)\"`\" ]; then \\\n\t\ttar czvf fatpacks.tar.gz \"$(FATPACKS_DIR)\"; \\\n\t\techo; \\\n\t\techo \"Generated fatpacks.tar.gz containing $(FATPACKS_DIR)/ directory of perl scripts with all dependencies bundled\"; \\\n\tfi\n\n.PHONY: fatpack\nfatpack: fatpacks\n\t@:\n\n.PHONY: python-compile\npython-compile:\n\t$(BASH_TOOLS)/python/python_compile.sh\n\n.PHONY: pycompile\npycompile: python-compile\n\t@:\n\n.PHONY: python-version\npython-version:\n\t$(BASH_TOOLS)/setup/which_python_installed.sh\n\n.PHONY: golang-version\ngolang-version:\n\t@echo && \\\n\twhich go && \\\n\tls -l `which go` && \\\n\techo && \\\n\tgo version || : ; \\\n\techo\n\n.PHONY: go-version\ngo-version: golang-version\n\t@:\n\n.PHONY: golang-clean\ngolang-clean:\n\t@$(BASH_TOOLS)/packages/golang_rm_binaries.sh\n\n.PHONY: go-clean\ngo-clean: golang-clean\n\t@:\n\n# =======================\n# Nice tricks for pure Python projects\n# - borrowed from https://gist.github.com/bsmith89/c6811893c1cbd2a72cc1d144a197bef2#file-makefile\n\n#VENV = .venv\n#export VIRTUAL_ENV := $(abspath ${VENV})\n\n# putting the venv/bin at the start of the path means that the venv python will be called\n# and the venv libraries used automatically, so no need to 'source .venv/bin/activate' first\n# although it misses the hash flush 'hash -r' that the venv activate script does\n#export PATH := ${VIRTUAL_ENV}/bin:${PATH}\n\n#${VENV}:\n#    python3 -m venv \"$@\"\n\n#pip-install: requirements.txt | ${VENV}\n#    pip install --upgrade -r requirements.txt\n# =======================\n\n.PHONY: sonar\nsonar:\n\tsonar-scanner\n\n.PHONY: update\nupdate: update2\n\t@# putting this here instead of inline dep because otherwise check_makefile.sh will fail the target as build target doesn't exist in this Makefile.in\n\t@$(MAKE) build\n\n.PHONY: update2\nupdate2: update-no-recompile\n\t@:\n\n.PHONY: update-no-recompile\nupdate-no-recompile:\n\tgit pull --no-edit\n\t$(MAKE) submodules\n\n.PHONY: update-submodules\nupdate-submodules:\n\tgit submodule update --init --remote\n.PHONY: updatem\nupdatem: update-submodules\n\t@:\n\n.PHONY: docker-run\ndocker-run:\n\tdocker run -ti --rm ${DOCKER_IMAGE} ${ARGS}\n\n.PHONY: run\nrun: docker-run\n\t@:\n\n.PHONY: concourse\nconcourse:\n\t$(BASH_TOOLS)/cicd/concourse.sh\n\n.PHONY: fly\nfly: concourse\n\t@:\n\n.PHONY: docker-mount\ndocker-mount:\n\t# --privileged=true is needed to be able to:\n\t# mount -t tmpfs -o size=1m tmpfs /mnt/ramdisk\n\tdocker run -ti --rm --privileged=true -v $$PWD:/code ${DOCKER_IMAGE} bash -c \"cd /code; exec bash\"\n\n.PHONY: docker-mount-alpine\ndocker-mount-alpine:\n\t# --privileged=true is needed to be able to:\n\t# mount -t tmpfs -o size=1m tmpfs /mnt/ramdisk\n\tdocker run -ti --rm --privileged=true -v $$PWD:/code ${DOCKER_IMAGE}:alpine bash -c \"cd /code; exec bash\"\n\n.PHONY: docker-mount-debian\ndocker-mount-debian:\n\t# --privileged=true is needed to be able to:\n\t# mount -t tmpfs -o size=1m tmpfs /mnt/ramdisk\n\tdocker run -ti --rm --privileged=true -v $$PWD:/code ${DOCKER_IMAGE}:debian bash -c \"cd /code; exec bash\"\n\n.PHONY: docker-mount-centos\ndocker-mount-centos:\n\t# --privileged=true is needed to be able to:\n\t# mount -t tmpfs -o size=1m tmpfs /mnt/ramdisk\n\tdocker run -ti --rm --privileged=true -v $$PWD:/code ${DOCKER_IMAGE}:centos bash -c \"cd /code; exec bash\"\n\n.PHONY: docker-mount-ubuntu\ndocker-mount-ubuntu:\n\t# --privileged=true is needed to be able to:\n\t# mount -t tmpfs -o size=1m tmpfs /mnt/ramdisk\n\tdocker run -ti --rm --privileged=true -v $$PWD:/code ${DOCKER_IMAGE}:ubuntu bash -c \"cd /code; exec bash\"\n\n.PHONY: mount\nmount: docker-mount\n\t@:\n\n.PHONY: mount-alpine\nmount-alpine: docker-mount-alpine\n\t@:\n\n.PHONY: mount-debian\nmount-debian: docker-mount-debian\n\t@:\n\n.PHONY: mount-centos\nmount-centos: docker-mount-centos\n\t@:\n\n.PHONY: mount-ubuntu\nmount-ubuntu: docker-mount-ubuntu\n\t@:\n\n# checks dockerhub build status for this repo - needs check_dockerhub_repo_build_status.py from Advanced Nagios Plugins Collection to be in $PATH\n.PHONY: dockerhub-status\ndockerhub-status:\n\tcheck_dockerhub_repo_build_status.py -r \"$(DOCKER_IMAGE)\"\n\n# For quick testing only - for actual Dockerfile builds see https://hub.docker.com/u/harisekhon and Dockerfiles source repo https://github.com/HariSekhon/Dockerfiles\n.PHONY: docker-alpine\ndocker-alpine:\n\t$(BASH_TOOLS)/docker/docker_mount_build_exec.sh alpine\n\n.PHONY: docker-debian\ndocker-debian:\n\t$(BASH_TOOLS)/docker/docker_mount_build_exec.sh debian\n\n.PHONY: docker-centos\ndocker-centos:\n\t$(BASH_TOOLS)/docker/docker_mount_build_exec.sh centos\n\n.PHONY: docker-fedora\ndocker-fedora:\n\t$(BASH_TOOLS)/docker/docker_mount_build_exec.sh fedora\n\n.PHONY: docker-ubuntu\ndocker-ubuntu:\n\t$(BASH_TOOLS)/docker/docker_mount_build_exec.sh ubuntu\n\n.PHONY: travis\ntravis:\n\t@. $(BASH_TOOLS)/.bash.d/network.sh; browser \"https://travis-ci.org/$(REPO)\"\n\n.PHONY: travis-log\ntravis-log:\n\ttravis_last_log.py --failed $(REPO)\n\n.PHONY: travis-debug\ntravis-debug:\n\ttravis_debug_session.py $(REPO)\n\n.PHONY: browse\nbrowse: github\n\t@:\n\n.PHONY: commitcount\ncommitcount:\n\t@# interestingly, even on 10,000 commit repos, there are no duplicate short hashes shown from:\n\t@# git log --all --pretty=format:\"%h\" | sort | uniq -d\n\t@git log --all --pretty=format:\"%h\" | wc -l\n\n.PHONY: github\ngithub:\n\t@. $(BASH_TOOLS)/.bash.d/network.sh; browser \"https://github.com/$(REPO)\"\n\n.PHONY: github-url\ngithub-url:\n\t@. $(BASH_TOOLS)/.bash.d/functions.sh; echo \"https://github.com/$(REPO)\" | tee /dev/stderr | tr -d '\\n' | paste_clipboard\n\n.PHONY: gitlab\ngitlab:\n\t@. $(BASH_TOOLS)/.bash.d/network.sh; browser \"https://gitlab.com/$(REPO)\"\n\n.PHONY: gitlab-url\ngitlab-url:\n\t@. $(BASH_TOOLS)/.bash.d/functions.sh; echo \"https://gitlab.com/$(REPO)\" | tee /dev/stderr | tr -d '\\n' | paste_clipboard\n\n.PHONY: bitbucket\nbitbucket:\n\t@. $(BASH_TOOLS)/.bash.d/network.sh; browser \"https://bitbucket.org/$(REPO)/src/master/\"\n\n.PHONY: bitbucket-url\nbitbucket-url:\n\t@. $(BASH_TOOLS)/.bash.d/functions.sh; echo \"https://bitbucket.org/$(REPO)/src/master/\" | tee /dev/stderr | tr -d '\\n' | paste_clipboard\n\n.PHONY: status\nstatus:\n\t@. $(BASH_TOOLS)/.bash.d/network.sh; browser \"https://bitbucket.org/HariSekhon/DevOps-Bash-tools/src/master/STATUS.md\"\n\n.PHONY: readme\nreadme:\n\t@. $(BASH_TOOLS)/.bash.d/network.sh; browser \"https://github.com/$(REPO)/blob/master/README.md\"\n\n.PHONY: issues\nissues:\n\t@. $(BASH_TOOLS)/.bash.d/network.sh; browser \"https://github.com/$(REPO)/issues\"\n\n.PHONY: github\ndockerhub:\n\t@. $(BASH_TOOLS)/.bash.d/network.sh; browser \"https://hub.docker.com/u/harisekhon\"\n\n.PHONY: dockerhub-url\ndockerhub-url:\n\t@. $(BASH_TOOLS)/.bash.d/functions.sh; echo \"https://hub.docker.com/u/harisekhon\" | tee /dev/stderr | tr -d '\\n' | paste_clipboard\n\n.PHONY: startrack\nstartrack:\n\t@echo \"Don't run this too much, you will hit an API limit against your IP\"\n\t@. $(BASH_TOOLS)/.bash.d/network.sh; \\\n\tbrowser \"https://seladb.github.io/StarTrack-js/?\\\n\tu=$$(sed 's/\\/.*//' <<< \"$(REPO)\")\\\n\t&r=$$(sed 's/.*\\///' <<< \"$(REPO)\")\"\n\n.PHONY: star\nstar: startrack\n\t@:\n\n.PHONY: allstars\nallstars:\n\t@echo \"Takes a while, don't run this all the time or you will hit an API limit against your IP\"\n\t@REPOS=\"Nagios-Plugins Dockerfiles DevOps-Python-tools DevOps-Perl-tools DevOps-Bash-Tools Nagios-Plugin-Kafka HAProxy-configs\"; \\\n\t. $(BASH_TOOLS)/.bash.d/network.sh; \\\n\tbrowser \"https://seladb.github.io/StarTrack-js/#/preload?\\\n\t$$(\\\n\t\tfor repo in $$REPOS; do \\\n\t\t\tprintf \"%s\" \"&r=HariSekhon,$$repo\"; \\\n\t\tdone | \\\n\t\tsed 's/\\&//'\\\n\t)\"\n\n.PHONY: ls\nls:\n\t@echo $(CODE_FILES) | tr ' ' '\\n' | sort\n\n.PHONY: wc\nwc:\n\tif [ -x wc.sh ]; then ./wc.sh; exit 1; fi\n\t@# CODE_FILES := definitions in Makefiles must not be quoted or will get wc error 'open: File name too long'\n\t@wc -l $(CODE_FILES)\n\t@printf 'Total Lines:\\t\\t\\t'\n\t@cat $(CODE_FILES) | wc -l | sed 's/[[:space:]]//g'\n\t@printf 'Total Lines without # comments:\\t'\n\t@sed 's/#.*//;/^[[:space:]]*$$/d' $(CODE_FILES) | wc -l | sed 's/[[:space:]]//g'\n\t@printf 'Total Files:\\t\\t\\t'\n\t@tr ' ' '\\n' <<< \"$(CODE_FILES)\" | wc -l | sed 's/[[:space:]]//g'\n\t@printf 'of which not the following:\\t'\n\t@tr ' ' '\\n' <<< \"$(CODE_FILES)\" | grep -Ev -e '\\.bash' \\\n\t\t\t\t\t\t\t\t\t\t\t\t-e lib/ \\\n\t\t\t\t\t\t\t\t\t\t\t\t-e install/ \\\n\t\t\t\t\t\t\t\t\t\t\t\t-e setup/ \\\n\t\t\t\t\t\t\t\t\t\t\t\t-e 'tests?/' \\\n\t\t\t\t\t\t\t\t\t\t\t\t-e vagrant/ \\\n\t\t\t\t\t\t\t\t\t\t\t\t| wc -l | sed 's/[[:space:]]//g'\n\t@printf 'of which .bash*:\\t\\t'\n\t@tr ' ' '\\n' <<< \"$(CODE_FILES)\" | grep -c '\\.bash'\n\t@printf 'of which lib/:\\t\\t\\t'\n\t@tr ' ' '\\n' <<< \"$(CODE_FILES)\" | grep -c lib/\n\t@printf 'of which install/:\\t\\t'\n\t@tr ' ' '\\n' <<< \"$(CODE_FILES)\" | grep -c install/\n\t@printf 'of which setup/:\\t\\t'\n\t@tr ' ' '\\n' <<< \"$(CODE_FILES)\" | grep -c setup/\n\t@printf 'of which test(s)/:\\t\\t'\n\t@tr ' ' '\\n' <<< \"$(CODE_FILES)\" | grep -Eec 'tests?/'\n\t@printf 'of which vagrant/:\\t\\t'\n\t@tr ' ' '\\n' <<< \"$(CODE_FILES)\" | grep -c vagrant/\n\nmdl:\n\t@echo \"Checking Markdown for issues\"\n\t@echo\n\t@if .mdl.rb; then \\\n\t\tmdl -s .mdl.rb *.md; \\\n\telse \\\n\t\tmdl *.md; \\\n\tfi\n\n# finds .swp, would need to port out code lists\n#.PHONY: wcall\n#wcall:\n#\tfind . -type f --not -path '*.git*' -exec cat {} \\; | wc -l\n#\n#.PHONY: wcall\n#wcall:\n#\tfind . -type f -not -path '*.git*' -exec sed 's/#.*//;/^[[:space:]]*$$/d' {} \\; | wc -l\n\n.PHONY: repos\nrepos:\n\t@if ! grep -q \"OTHER_REPOS_START\" README.md || \\\n\t\t! grep -q \"OTHER_REPOS_END\"   README.md || \\\n\t\t! grep -q \"More Core Repos\"   README.md; then \\\n\t\techo \"Adding More Core Repos section to README.md\"; \\\n\t\tprintf '\\n## More Core Repos\\n\\n<!-- OTHER_REPOS_START -->\\n\\n<!-- OTHER_REPOS_END -->\\n' >> README.md; \\\n\tfi; \\\n\tmarkdown_replace_repos.sh\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 201.71875,
          "content": "# Hari Sekhon - DevOps Bash Tools\n\n[![GitHub stars](https://img.shields.io/github/stars/harisekhon/devops-bash-tools?logo=github)](https://github.com/HariSekhon/DevOps-Bash-tools/stargazers)\n[![GitHub forks](https://img.shields.io/github/forks/harisekhon/devops-bash-tools?logo=github)](https://github.com/HariSekhon/DevOps-Bash-tools/network)\n[![Lines of Code](https://img.shields.io/badge/lines%20of%20code-96k-lightgrey?logo=codecademy)](#hari-sekhon---devops-bash-tools)\n[![License](https://img.shields.io/badge/license-MIT-green)](https://github.com/HariSekhon/DevOps-Bash-tools/blob/master/LICENSE)\n[![My LinkedIn](https://img.shields.io/badge/LinkedIn%20Profile-HariSekhon-blue?logo=data:image/svg%2bxml;base64,PHN2ZyByb2xlPSJpbWciIGZpbGw9IiNmZmZmZmYiIHZpZXdCb3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48dGl0bGU+TGlua2VkSW48L3RpdGxlPjxwYXRoIGQ9Ik0yMC40NDcgMjAuNDUyaC0zLjU1NHYtNS41NjljMC0xLjMyOC0uMDI3LTMuMDM3LTEuODUyLTMuMDM3LTEuODUzIDAtMi4xMzYgMS40NDUtMi4xMzYgMi45Mzl2NS42NjdIOS4zNTFWOWgzLjQxNHYxLjU2MWguMDQ2Yy40NzctLjkgMS42MzctMS44NSAzLjM3LTEuODUgMy42MDEgMCA0LjI2NyAyLjM3IDQuMjY3IDUuNDU1djYuMjg2ek01LjMzNyA3LjQzM2MtMS4xNDQgMC0yLjA2My0uOTI2LTIuMDYzLTIuMDY1IDAtMS4xMzguOTItMi4wNjMgMi4wNjMtMi4wNjMgMS4xNCAwIDIuMDY0LjkyNSAyLjA2NCAyLjA2MyAwIDEuMTM5LS45MjUgMi4wNjUtMi4wNjQgMi4wNjV6bTEuNzgyIDEzLjAxOUgzLjU1NVY5aDMuNTY0djExLjQ1MnpNMjIuMjI1IDBIMS43NzFDLjc5MiAwIDAgLjc3NCAwIDEuNzI5djIwLjU0MkMwIDIzLjIyNy43OTIgMjQgMS43NzEgMjRoMjAuNDUxQzIzLjIgMjQgMjQgMjMuMjI3IDI0IDIyLjI3MVYxLjcyOUMyNCAuNzc0IDIzLjIgMCAyMi4yMjIgMGguMDAzeiIvPjwvc3ZnPgo=)](https://www.linkedin.com/in/HariSekhon/)\n[![GitHub Last Commit](https://img.shields.io/github/last-commit/HariSekhon/DevOps-Bash-tools?logo=github)](https://github.com/HariSekhon/DevOps-Bash-tools/commits/master)\n\n[![Codacy](https://app.codacy.com/project/badge/Grade/dffc1bfd13404c95b5a0ab97fd47974e)](https://www.codacy.com/gh/HariSekhon/DevOps-Bash-tools/dashboard)\n[![CodeFactor](https://www.codefactor.io/repository/github/harisekhon/devops-bash-tools/badge)](https://www.codefactor.io/repository/github/harisekhon/devops-bash-tools)\n[![Quality Gate Status](https://sonarcloud.io/api/project_badges/measure?project=HariSekhon_DevOps-Bash-tools&metric=alert_status)](https://sonarcloud.io/dashboard?id=HariSekhon_DevOps-Bash-tools)\n[![Maintainability Rating](https://sonarcloud.io/api/project_badges/measure?project=HariSekhon_DevOps-Bash-tools&metric=sqale_rating)](https://sonarcloud.io/dashboard?id=HariSekhon_DevOps-Bash-tools)\n[![Reliability Rating](https://sonarcloud.io/api/project_badges/measure?project=HariSekhon_DevOps-Bash-tools&metric=reliability_rating)](https://sonarcloud.io/dashboard?id=HariSekhon_DevOps-Bash-tools)\n[![Security Rating](https://sonarcloud.io/api/project_badges/measure?project=HariSekhon_DevOps-Bash-tools&metric=security_rating)](https://sonarcloud.io/dashboard?id=HariSekhon_DevOps-Bash-tools)\n[![Vulnerabilities](https://sonarcloud.io/api/project_badges/measure?project=HariSekhon_DevOps-Bash-tools&metric=vulnerabilities)](https://sonarcloud.io/summary/new_code?id=HariSekhon_DevOps-Bash-tools)\n\n<!--\nBitBucket exposes HTML comments - open issue - works properly on GitHub/GitLab\ndoesn't detect shell code properly\n[![Lines of Code](https://sonarcloud.io/api/project_badges/measure?project=HariSekhon_DevOps-Bash-tools&metric=ncloc)](https://sonarcloud.io/dashboard?id=HariSekhon_DevOps-Bash-tools)\n-->\n\n[![Linux](https://img.shields.io/badge/OS-Linux-blue?logo=linux)](#hari-sekhon---devops-bash-tools)\n[![Mac](https://img.shields.io/badge/OS-Mac-blue?logo=apple)](#hari-sekhon---devops-bash-tools)\n[![Docker](https://img.shields.io/badge/container-Docker-blue?logo=docker&logoColor=white)](https://hub.docker.com/r/harisekhon/bash-tools)\n[![Dockerfile](https://img.shields.io/badge/repo-Dockerfiles-blue?logo=docker&logoColor=white)](https://github.com/HariSekhon/Dockerfiles)\n[![DockerHub Pulls](https://img.shields.io/docker/pulls/harisekhon/bash-tools?label=DockerHub%20pulls&logo=docker&logoColor=white)](https://hub.docker.com/r/harisekhon/bash-tools)\n[![StarTrack](https://img.shields.io/badge/Star-Track-blue?logo=github)](https://seladb.github.io/StarTrack-js/#/preload?r=HariSekhon,Nagios-Plugins&r=HariSekhon,Dockerfiles&r=HariSekhon,DevOps-Python-tools&r=HariSekhon,DevOps-Perl-tools&r=HariSekhon,DevOps-Bash-tools&r=HariSekhon,HAProxy-configs&r=HariSekhon,SQL-scripts)\n[![StarCharts](https://img.shields.io/badge/Star-Charts-blue?logo=github)](https://github.com/HariSekhon/DevOps-Bash-tools/blob/master/STARCHARTS.md)\n\n[![Mac Homebrew](https://img.shields.io/badge/Mac-Homebrew-999999?logo=apple&logoColor=white)](https://brew.sh/)\n[![Alpine](https://img.shields.io/badge/Linux-Alpine-0D597F?logo=alpine%20linux)](https://alpinelinux.org/)\n[![CentOS](https://img.shields.io/badge/Linux-CentOS-262577?logo=centos&logoColor=white)](https://www.centos.org/)\n[![Debian](https://img.shields.io/badge/Linux-Debian-A81D33?logo=debian)](https://www.debian.org/)\n[![Fedora](https://img.shields.io/badge/Linux-Fedora-294172?logo=fedora&logoColor=white)](https://getfedora.org/)\n[![Redhat](https://img.shields.io/badge/Linux-Redhat-EE0000?logo=red%20hat)](https://www.redhat.com/en)\n[![Rocky](https://img.shields.io/badge/Linux-Rocky-10B981?logo=rockylinux&logoColor=white)](https://rockylinux.org/)\n[![Ubuntu](https://img.shields.io/badge/Linux-Ubuntu-E95420?logo=ubuntu&logoColor=white)](https://ubuntu.com/)\n\n<!-- TODO: fix\n[![DockerHub Build Automated](https://img.shields.io/docker/automated/harisekhon/bash-tools?logo=docker&logoColor=white)](https://hub.docker.com/r/harisekhon/bash-tools)\n[![Docker Build Status](https://img.shields.io/docker/cloud/build/harisekhon/bash-tools?logo=docker&logoColor=white)](https://hub.docker.com/r/harisekhon/bash-tools/builds)\n-->\n\n<!--\nofficial badges without logos to differentiate them\n\nthis one I don't trust it'll stick around so using shields version instead\n[![Build Status](https://badges.herokuapp.com/travis/HariSekhon/DevOps-Bash-tools?label=Travis%20CI)](https://travis-ci.org/HariSekhon/DevOps-Bash-tools)\n\nawkward URLs more nicely replaced with shields.io\n\n[![AppVeyor](https://ci.appveyor.com/api/projects/status/u6f97cskcgb30sce/branch/master?svg=true)](https://ci.appveyor.com/project/HariSekhon/devops-bash-tools/branch/master)\n[![Drone](https://cloud.drone.io/api/badges/HariSekhon/DevOps-Bash-tools/status.svg)](https://cloud.drone.io/HariSekhon/DevOps-Bash-tools)\n-->\n\n[![CI Builds Overview](https://img.shields.io/badge/CI%20Builds-Overview%20Page-blue?logo=circleci)](https://harisekhon.github.io/CI-CD/)\n[![Jenkins](https://img.shields.io/badge/Jenkins-ready-blue?logo=jenkins&logoColor=white)](https://github.com/HariSekhon/DevOps-Bash-tools/blob/master/Jenkinsfile)\n[![Concourse](https://img.shields.io/badge/Concourse-ready-blue?logo=concourse&logoColor=white)](https://github.com/HariSekhon/DevOps-Bash-tools/blob/master/cicd/.concourse.yml)\n[![GoCD](https://img.shields.io/badge/GoCD-ready-blue?logo=go&logoColor=white)](https://github.com/HariSekhon/DevOps-Bash-tools/blob/master/cicd/.gocd.yml)\n[![TeamCity](https://img.shields.io/badge/TeamCity-ready-blue?logo=teamcity)](https://github.com/HariSekhon/TeamCity-CI)\n\n[![CircleCI](https://circleci.com/gh/HariSekhon/DevOps-Bash-tools.svg?style=svg)](https://circleci.com/gh/HariSekhon/DevOps-Bash-tools)\n[![BuildKite](https://img.shields.io/buildkite/f11bdd9690a9bac9a8edc6094dc2f2b9af3218a7a15d4ec17d/master?label=BuildKite&logo=buildkite)](https://buildkite.com/hari-sekhon/devops-bash-tools)\n[![AppVeyor](https://img.shields.io/appveyor/build/harisekhon/devops-bash-tools/master?logo=appveyor&label=AppVeyor)](https://ci.appveyor.com/project/HariSekhon/devops-bash-tools/branch/master)\n[![Drone](https://img.shields.io/drone/build/HariSekhon/DevOps-Bash-tools/master?logo=drone&label=Drone)](https://cloud.drone.io/HariSekhon/DevOps-Bash-tools)\n[![Codefresh](https://g.codefresh.io/api/badges/pipeline/harisekhon/GitHub%2FDevOps-Bash-tools?branch=master&key=eyJhbGciOiJIUzI1NiJ9.NWU1MmM5OGNiM2FiOWUzM2Y3ZDZmYjM3.O69674cW7vYom3v5JOGKXDbYgCVIJU9EWhXUMHl3zwA&type=cf-1)](https://g.codefresh.io/pipelines/edit/new/builds?id=5e53eaeea284e010982eaa6e&pipeline=DevOps-Bash-tools&projects=GitHub&projectId=5e52ca8ea284e00f882ea992&context=github&filter=page:1;pageSize:10;timeFrameStart:week)\n[![Cirrus CI](https://img.shields.io/cirrus/github/HariSekhon/DevOps-Bash-tools/master?logo=Cirrus%20CI&label=Cirrus%20CI)](https://cirrus-ci.com/github/HariSekhon/DevOps-Bash-tools)\n[![Semaphore](https://harisekhon.semaphoreci.com/badges/DevOps-Bash-tools.svg)](https://harisekhon.semaphoreci.com/projects/DevOps-Bash-tools)\n[![Buddy](https://img.shields.io/badge/Buddy-ready-1A86FD?logo=buddy)](https://github.com/HariSekhon/DevOps-Bash-tools/blob/master/buddy.yml)\n[![Shippable](https://img.shields.io/badge/Shippable-legacy-lightgrey?logo=jfrog&label=Shippable)](https://github.com/HariSekhon/DevOps-Bash-tools/blob/master/shippable.yml)\n[![Travis CI](https://img.shields.io/badge/TravisCI-ready-blue?logo=travis&label=Travis%20CI)](https://github.com/HariSekhon/DevOps-Bash-tools/blob/master/travis/.travis.yml)\n[![Reviewed by Hound](https://img.shields.io/badge/Reviewed%20by-Hound-8E64B0.svg)](https://houndci.com)\n\n[![Repo on GitHub](https://img.shields.io/badge/repo-GitHub-2088FF?logo=github)](https://github.com/HariSekhon/DevOps-Bash-tools)\n[![Repo on GitLab](https://img.shields.io/badge/repo-GitLab-FCA121?logo=gitlab)](https://gitlab.com/HariSekhon/DevOps-Bash-tools)\n[![Repo on Azure DevOps](https://img.shields.io/badge/repo-Azure%20DevOps-0078D7?logo=azure%20devops)](https://dev.azure.com/harisekhon/GitHub/_git/DevOps-Bash-tools)\n[![Repo on BitBucket](https://img.shields.io/badge/repo-BitBucket-0052CC?logo=bitbucket)](https://bitbucket.org/HariSekhon/DevOps-Bash-tools)\n\n[![Azure DevOps Pipeline](https://dev.azure.com/harisekhon/GitHub/_apis/build/status/HariSekhon.DevOps-Bash-tools?branchName=master)](https://dev.azure.com/harisekhon/GitHub/_build/latest?definitionId=1&branchName=master)\n[![GitLab Pipeline](https://img.shields.io/badge/GitLab%20CI-legacy-lightgrey?logo=gitlab)](https://gitlab.com/HariSekhon/DevOps-Bash-tools/pipelines)\n[![BitBucket Pipeline](https://img.shields.io/badge/Bitbucket%20CI-legacy-lightgrey?logo=bitbucket)](https://bitbucket.org/harisekhon/devops-bash-tools/addon/pipelines/home#!/)\n[![AWS CodeBuild](https://img.shields.io/badge/AWS%20CodeBuild-ready-blue?logo=amazon%20aws)](https://github.com/HariSekhon/DevOps-Bash-tools/blob/master/cicd/buildspec.yml)\n[![GCP Cloud Build](https://img.shields.io/badge/GCP%20Cloud%20Build-ready-blue?logo=google%20cloud&logoColor=white)](https://github.com/HariSekhon/DevOps-Bash-tools/blob/master/cicd/cloudbuild.yaml)\n\n[![ShellCheck](https://github.com/HariSekhon/DevOps-Bash-tools/actions/workflows/shellcheck.yaml/badge.svg)](https://github.com/HariSekhon/DevOps-Bash-tools/actions/workflows/shellcheck.yaml)\n[![JSON](https://github.com/HariSekhon/DevOps-Bash-tools/actions/workflows/json.yaml/badge.svg)](https://github.com/HariSekhon/DevOps-Bash-tools/actions/workflows/json.yaml)\n[![YAML](https://github.com/HariSekhon/DevOps-Bash-tools/actions/workflows/yaml.yaml/badge.svg)](https://github.com/HariSekhon/DevOps-Bash-tools/actions/workflows/yaml.yaml)\n[![XML](https://github.com/HariSekhon/DevOps-Bash-tools/actions/workflows/xml.yaml/badge.svg)](https://github.com/HariSekhon/DevOps-Bash-tools/actions/workflows/xml.yaml)\n[![Markdown](https://github.com/HariSekhon/DevOps-Bash-tools/actions/workflows/markdown.yaml/badge.svg)](https://github.com/HariSekhon/DevOps-Bash-tools/actions/workflows/markdown.yaml)\n[![Validation](https://github.com/HariSekhon/DevOps-Bash-tools/actions/workflows/validate.yaml/badge.svg)](https://github.com/HariSekhon/DevOps-Bash-tools/actions/workflows/validate.yaml)\n[![Kics](https://github.com/HariSekhon/DevOps-Bash-tools/actions/workflows/kics.yaml/badge.svg)](https://github.com/HariSekhon/DevOps-Bash-tools/actions/workflows/kics.yaml)\n[![Grype](https://github.com/HariSekhon/DevOps-Bash-tools/actions/workflows/grype.yaml/badge.svg)](https://github.com/HariSekhon/DevOps-Bash-tools/actions/workflows/grype.yaml)\n[![Semgrep](https://github.com/HariSekhon/DevOps-Bash-tools/actions/workflows/semgrep.yaml/badge.svg)](https://github.com/HariSekhon/DevOps-Bash-tools/actions/workflows/semgrep.yaml)\n[![Semgrep Cloud](https://github.com/HariSekhon/DevOps-Bash-tools/actions/workflows/semgrep-cloud.yaml/badge.svg)](https://github.com/HariSekhon/DevOps-Bash-tools/actions/workflows/semgrep-cloud.yaml)\n[![Trivy](https://github.com/HariSekhon/DevOps-Bash-tools/actions/workflows/trivy.yaml/badge.svg)](https://github.com/HariSekhon/DevOps-Bash-tools/actions/workflows/trivy.yaml)\n\n[![Docker Build (Alpine)](https://github.com/HariSekhon/DevOps-Bash-tools/actions/workflows/docker_bash_alpine.yaml/badge.svg)](https://github.com/HariSekhon/DevOps-Bash-tools/actions/workflows/docker_bash_alpine.yaml)\n[![Docker Build (Debian)](https://github.com/HariSekhon/DevOps-Bash-tools/actions/workflows/docker_bash_debian.yaml/badge.svg)](https://github.com/HariSekhon/DevOps-Bash-tools/actions/workflows/docker_bash_debian.yaml)\n[![Docker Build (Fedora)](https://github.com/HariSekhon/DevOps-Bash-tools/actions/workflows/docker_bash_fedora.yaml/badge.svg)](https://github.com/HariSekhon/DevOps-Bash-tools/actions/workflows/docker_bash_fedora.yaml)\n[![Docker Build (Ubuntu)](https://github.com/HariSekhon/DevOps-Bash-tools/actions/workflows/docker_bash_ubuntu.yaml/badge.svg)](https://github.com/HariSekhon/DevOps-Bash-tools/actions/workflows/docker_bash_ubuntu.yaml)\n\n[![GitHub Actions Ubuntu](https://github.com/HariSekhon/DevOps-Bash-tools/workflows/GitHub%20Actions%20Ubuntu/badge.svg)](https://github.com/HariSekhon/DevOps-Bash-tools/actions?query=workflow%3A%22GitHub+Actions+Ubuntu%22)\n[![Mac](https://github.com/HariSekhon/DevOps-Bash-tools/actions/workflows/mac.yaml/badge.svg)](https://github.com/HariSekhon/DevOps-Bash-tools/actions/workflows/mac.yaml)\n[![Mac 11](https://github.com/HariSekhon/DevOps-Bash-tools/actions/workflows/mac_11.yaml/badge.svg)](https://github.com/HariSekhon/DevOps-Bash-tools/actions/workflows/mac_11.yaml)\n[![Mac 12](https://github.com/HariSekhon/DevOps-Bash-tools/actions/workflows/mac_12.yaml/badge.svg)](https://github.com/HariSekhon/DevOps-Bash-tools/actions/workflows/mac_12.yaml)\n[![Ubuntu](https://github.com/HariSekhon/DevOps-Bash-tools/workflows/Ubuntu/badge.svg)](https://github.com/HariSekhon/DevOps-Bash-tools/actions?query=workflow%3A%22Ubuntu%22)\n[![Ubuntu 20.04](https://github.com/HariSekhon/DevOps-Bash-tools/workflows/Ubuntu%2020.04/badge.svg)](https://github.com/HariSekhon/DevOps-Bash-tools/actions?query=workflow%3A%22Ubuntu+20.04%22)\n[![Ubuntu 22.04](https://github.com/HariSekhon/DevOps-Bash-tools/workflows/Ubuntu%2022.04/badge.svg)](https://github.com/HariSekhon/DevOps-Bash-tools/actions?query=workflow%3A%22Ubuntu+22.04%22)\n[![Debian](https://github.com/HariSekhon/DevOps-Bash-tools/workflows/Debian/badge.svg)](https://github.com/HariSekhon/DevOps-Bash-tools/actions?query=workflow%3A%22Debian%22)\n[![Debian 10](https://github.com/HariSekhon/DevOps-Bash-tools/workflows/Debian%2010/badge.svg)](https://github.com/HariSekhon/DevOps-Bash-tools/actions?query=workflow%3A%22Debian+10%22)\n[![Debian 11](https://github.com/HariSekhon/DevOps-Bash-tools/workflows/Debian%2011/badge.svg)](https://github.com/HariSekhon/DevOps-Bash-tools/actions?query=workflow%3A%22Debian+11%22)\n[![Debian 12](https://github.com/HariSekhon/DevOps-Bash-tools/workflows/Debian%2012/badge.svg)](https://github.com/HariSekhon/DevOps-Bash-tools/actions?query=workflow%3A%22Debian+12%22)\n[![Fedora](https://github.com/HariSekhon/DevOps-Bash-tools/workflows/Fedora/badge.svg)](https://github.com/HariSekhon/DevOps-Bash-tools/actions?query=workflow%3A%22Fedora%22)\n[![Alpine](https://github.com/HariSekhon/DevOps-Bash-tools/workflows/Alpine/badge.svg)](https://github.com/HariSekhon/DevOps-Bash-tools/actions?query=workflow%3A%22Alpine%22)\n[![Alpine 3](https://github.com/HariSekhon/DevOps-Bash-tools/workflows/Alpine%203/badge.svg)](https://github.com/HariSekhon/DevOps-Bash-tools/actions?query=workflow%3A%22Alpine+3%22)\n\n[![Python 3.7](https://github.com/HariSekhon/DevOps-Bash-tools/workflows/Python%203.7/badge.svg)](https://github.com/HariSekhon/DevOps-Bash-tools/actions?query=workflow%3A%22Python+3.7%22)\n[![Python 3.8](https://github.com/HariSekhon/DevOps-Bash-tools/workflows/Python%203.8/badge.svg)](https://github.com/HariSekhon/DevOps-Bash-tools/actions?query=workflow%3A%22Python+3.8%22)\n[![Python 3.9](https://github.com/HariSekhon/DevOps-Bash-tools/workflows/Python%203.9/badge.svg)](https://github.com/HariSekhon/DevOps-Bash-tools/actions?query=workflow%3A%22Python+3.9%22)\n[![Python 3.10](https://github.com/HariSekhon/DevOps-Bash-tools/workflows/Python%203.10/badge.svg)](https://github.com/HariSekhon/DevOps-Bash-tools/actions?query=workflow%3A%22Python+3.10%22)\n[![Python 3.11](https://github.com/HariSekhon/DevOps-Bash-tools/workflows/Python%203.11/badge.svg)](https://github.com/HariSekhon/DevOps-Bash-tools/actions?query=workflow%3A%22Python+3.11%22)\n<!--\n[![Self Hosted](https://github.com/HariSekhon/DevOps-Bash-tools/workflows/Self%20Hosted/badge.svg)](https://github.com/HariSekhon/DevOps-Bash-tools/actions?query=workflow%3A%22Self+Hosted%22)\n-->\n\n<!-- TODO: https://codecov.io, https://coveralls.io -->\n\n[git.io/bash-tools](https://git.io/bash-tools)\n\n1000+ DevOps Shell Scripts and Advanced Bash environment.\n\nFast, Advanced Systems Engineering, Automation, APIs, shorter CLIs, etc.\n\nHeavily used in many [GitHub repos](https://github.com/search?o=desc&q=user%3Aharisekhon+type%3Arepository&type=Repositories), dozens of [DockerHub builds](https://hub.docker.com/r/harisekhon) ([Dockerfiles](https://github.com/HariSekhon/Dockerfiles)) and 600+ [CI builds](https://harisekhon.github.io/CI-CD/).\n\n## Summary\n\n- Scripts for many popular DevOps technologies, see [Index](#index) below for more details\n- Advanced configs for common tools like [Git](https://git-scm.com/), [vim](https://www.vim.org/), [screen](https://www.gnu.org/software/screen/), [tmux](https://github.com/tmux/tmux/wiki), [PostgreSQL psql](https://www.postgresql.org/) etc...\n- CI configs for most major Continuous Integration products (see [CI builds](https://harisekhon.github.io/CI-CD/) page)\n- CI scripts for a drop-in framework of standard checks to run in all [CI builds](https://harisekhon.github.io/CI-CD/), CI detection, accounting for installation differences across CI environments, root vs user, virtualenvs etc.\n- API scripts auto-handling authentication, tokens and other details to quickly query popular APIs with a few keystrokes just supplying the `/path/endpoint`\n- Advanced Bash environment - `.bashrc` + `.bash.d/*.sh` - aliases, functions, colouring, dynamic Git & shell behaviour enhancements, automatic pathing for installations and major languages like Python, Perl, Ruby, NodeJS, Golang across Linux distributions and Mac. See [.bash.d/README.md](https://github.com/HariSekhon/DevOps-Bash-tools/blob/master/.bash.d/README.md)\n- Installs the best systems packages -\n  [AWS CLI](https://aws.amazon.com/cli/),\n  [Azure CLI](https://docs.microsoft.com/en-us/cli/azure/?view=azure-cli-latest),\n  [GCloud SDK](https://cloud.google.com/sdk),\n  [Digital Ocean CLI](https://docs.digitalocean.com/reference/doctl/),\n  [Terraform](https://www.terraform.io/),\n  [Terragrunt](https://terragrunt.gruntwork.io/),\n  [GitHub CLI](https://github.com/cli/cli),\n  [Kubernetes](https://kubernetes.io/)\n  [kubectl](https://kubernetes.io/docs/reference/kubectl/overview/) &\n  [kustomize](https://kustomize.io/),\n  [Helm](https://helm.sh/),\n  [eksctl](https://eksctl.io/),\n  [Docker-Compose](https://docs.docker.com/compose/),\n  [jq](https://stedolan.github.io/jq/)\n  and many others... extensive package lists for servers and desktops for most major Linux distributions package managers and Mac\n  - `install/` - contains many installation scripts for popular open source software and direct binary downloads from GitHub releases\n  - `configs/` - contains many dot configs for common technologies like ViM, top, Screen, Tmux, MySQL, PostgreSQL etc.\n  - `setup/` - contains setup scripts, package lists, extra configs, Mac OS X settings etc.\n- Utility Libraries used by many hundreds of scripts and [builds](https://harisekhon.github.io/CI-CD/) across [repos](https://github.com/search?o=desc&q=user%3Aharisekhon+type%3Arepository&type=Repositories):\n  - `.bash.d/` - interactive library\n  - `lib/` - scripting and CI library\n- [SQL Scripts](https://github.com/HariSekhon/SQL-scripts) - 100+ scripts for [PostgreSQL](https://www.postgresql.org/), [MySQL](https://www.mysql.com/), [AWS Athena](https://aws.amazon.com/athena/) + [CloudTrail](https://aws.amazon.com/cloudtrail/), [Google BigQuery](https://cloud.google.com/bigquery)\n- [Templates](https://github.com/HariSekhon/Templates) - templates for common programming languages and build configs\n- [Kubernetes Configs](https://github.com/HariSekhon/Kubernetes-configs) - Kubernetes YAML configs for most common scenarios, including Production Best Practices, Tips & Tricks\n\nSee Also: [similar DevOps repos](https://github.com/HariSekhon/DevOps-Bash-tools/blob/master/README.md#see-also) in other languages\n\nHari Sekhon\n\nCloud & Big Data Contractor, United Kingdom\n\n(ex-Cloudera, former Hortonworks Consultant)\n\n[![My LinkedIn](https://img.shields.io/badge/LinkedIn%20Profile-HariSekhon-blue?logo=data:image/svg%2bxml;base64,PHN2ZyByb2xlPSJpbWciIGZpbGw9IiNmZmZmZmYiIHZpZXdCb3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48dGl0bGU+TGlua2VkSW48L3RpdGxlPjxwYXRoIGQ9Ik0yMC40NDcgMjAuNDUyaC0zLjU1NHYtNS41NjljMC0xLjMyOC0uMDI3LTMuMDM3LTEuODUyLTMuMDM3LTEuODUzIDAtMi4xMzYgMS40NDUtMi4xMzYgMi45Mzl2NS42NjdIOS4zNTFWOWgzLjQxNHYxLjU2MWguMDQ2Yy40NzctLjkgMS42MzctMS44NSAzLjM3LTEuODUgMy42MDEgMCA0LjI2NyAyLjM3IDQuMjY3IDUuNDU1djYuMjg2ek01LjMzNyA3LjQzM2MtMS4xNDQgMC0yLjA2My0uOTI2LTIuMDYzLTIuMDY1IDAtMS4xMzguOTItMi4wNjMgMi4wNjMtMi4wNjMgMS4xNCAwIDIuMDY0LjkyNSAyLjA2NCAyLjA2MyAwIDEuMTM5LS45MjUgMi4wNjUtMi4wNjQgMi4wNjV6bTEuNzgyIDEzLjAxOUgzLjU1NVY5aDMuNTY0djExLjQ1MnpNMjIuMjI1IDBIMS43NzFDLjc5MiAwIDAgLjc3NCAwIDEuNzI5djIwLjU0MkMwIDIzLjIyNy43OTIgMjQgMS43NzEgMjRoMjAuNDUxQzIzLjIgMjQgMjQgMjMuMjI3IDI0IDIyLjI3MVYxLjcyOUMyNCAuNzc0IDIzLjIgMCAyMi4yMjIgMGguMDAzeiIvPjwvc3ZnPgo=)](https://www.linkedin.com/in/HariSekhon/)\n<br>*(you're welcome to connect with me on LinkedIn)*\n\n### Quick Setup\n\nTo bootstrap, install packages and link in to your shell profile to inherit all configs, do:\n\n```bash\ncurl -L https://git.io/bash-bootstrap | sh\n```\n\n- Adds sourcing to `.bashrc`/`.bash_profile` to automatically inherit all `.bash.d/*.sh` environment enhancements for all technologies (see [Inventory](#index) below)\n- Symlinks `.*` config dotfiles to `$HOME` for [git](https://git-scm.com/), [vim](https://www.vim.org/), top, [htop](https://hisham.hm/htop/), [screen](https://www.gnu.org/software/screen/), [tmux](https://github.com/tmux/tmux/wiki), [editorconfig](https://editorconfig.org/), [Ansible](https://www.ansible.com/), [PostgreSQL](https://www.postgresql.org/) `.psqlrc` etc. (only when they don't already exist so there is no conflict with your own configs)\n- Installs OS package dependencies for all scripts (detects the OS and installs the right RPMs, Debs, Apk or Mac HomeBrew packages)\n- Installs Python packages\n- Installs [AWS CLI](https://aws.amazon.com/cli/)\n\nTo only install package dependencies to run scripts, simply `cd` to the git clone directory and run `make`:\n\n```shell\ngit clone https://github.com/HariSekhon/DevOps-Bash-tools bash-tools\ncd bash-tools\nmake\n```\n\n`make install` sets your shell profile to source this repo. See [Individual Setup Parts](#individual-setup-parts) below for more install/uninstall options.\n\n## Index\n\n- [Dot Configs](#dot-configs) - `.gitconfig`, `.vimrc`, `.screenrc`, `.tmux.conf`, `.toprc`, `.gitignore`...\n- [Bash Environment & Libraries](#bash-environment--libraries) - `.bashrc`, `.bash.d/` interactive library, `lib/` scripting library\n- [Installation Scripts](#installation-scripts) for many popular open source technologies\n- [Linux & Mac](#linux--mac) - curl OAuth / JWT, LDAP, find duplicate files, SSL certificate get/validate, URL encoding/decoding, Vagrant\n- [Mac & AppleScript](#mac--applescript) - Mac settings and UI automation scripts, send keystrokes, mouse clicks,\n  detect foreground app, switch app, detect locked screen or screensaver, activate screensaver\n- [Monitoring](#monitoring) - Grafana, Prometheus, Node Exporter, scripted collection of common Linux & Mac cli\n  monitoring stats and log locations for quick generation of vendor support tarball bundles both locally and over SSH\n- [AWS - Amazon Web Services](#aws---amazon-web-services) - AWS account summary, lots of IAM reports, CIS Benchmark config hardening, EC2, ECR, EKS, Spot termination, S3 access logging, KMS key rotation info, SSM, CloudTrail, CloudWatch billing alarm with SNS notification topic and subscription for email alerts\n- [GCP - Google Cloud Platform](#gcp---google-cloud-platform) - massive GCP auto-inventory, scripts for GCE, GKE, GCR, Secret Manager, BigQuery, Cloud SQL, Cloud Scheduler, Terraform service account creation\n- [Kubernetes](#kubernetes) - massive Kubernetes auto-inventory, cluster management scripts & tricks\n- [Docker](#docker) - Docker API, Dockerhub API, Quay.io API scripts\n- [Databases](#databases) - fast CLI wrappers, instant Docker sandboxes (PostgreSQL, MySQL, MariaDB, SQLite), [SQL scripts](https://github.com/HariSekhon/SQL-scripts), SQL script testers against all versions of a DB, advanced `.psqlrc`\n- [Data](#data) - data tools, converters and format validators for Avro, Parquet, CSV, JSON, INI / Properties files (Java), LDAP LDIF, XML, YAML\n- [Big Data & NoSQL](#big-data--nosql) - Kafka, Hadoop, HDFS, Hive, Impala, ZooKeeper, Cloudera Manager API & Cloudera Navigator API scripts\n- [Git - GitHub, GitLab, Bitbucket, Azure DevOps](#git---github-gitlab-bitbucket-azure-devops) - scripts for Git local & mirror management, GitHub, GitLab & BitBucket APIs\n- [CI/CD - Continuous Integration / Continuous Delivery](#cicd---continuous-integration--continuous-deployment) - API scripts & build pipeline configs for most major CI systems:\n  - Jenkins, Concourse, GoCD, TeamCity - one-touch boot & build\n  - Azure DevOps Pipelines, GitHub Actions Workflows, GitLab CI, BitBucket Pipelines, AppVeyor, BuildKite, Travis CI, Circle CI, Codefresh, CodeShip, Drone.io, Semaphore CI, Shippable ...\n  - Terraform Cloud, Octopus Deploy\n  - Checkov / Bridgecrew Cloud\n- [AI & IPaaS](#ai--ipaas) - OpenAI (ChatGPT), Make.com\n- [Internet Services](#internet-services) - Cloudflare, DataDog, Digital Ocean, Kong API Gateway, GitGuardian, Jira, NGrok, Traefik, Pingdom, Wordpress and various pastebins and file upload sites\n- [Java](#java) - Java utilies to debug running Java programs or decompile Java JAR code for deeper debugging\n- [Python](#python) - Python utilities & library management\n- [Perl](#perl) - Perl utilities & library management\n- [Golang](#golang) - Golang utilities\n- [Media](#media) - MP3 metadata editing, grouping and ordering of albums and audiobooks, mkv/avi to mp4 converters, YouTube channel download\n- [Spotify](#spotify) - 40+ Spotify API scripts for backups, managing playlists, track deduplication, URI conversion, search, add/delete, liked tracks, followed artists, top artists, top tracks etc.\n- [More Linux & Mac](#more-linux--mac) - more systems administration scripts, package installation automation\n- [Builds, Languages & Linting](#builds-languages--linting) - programming language, build system & CI linting\n- [Templates](https://github.com/HariSekhon/Templates) - Templates for AWS, GCP, Terraform, Docker, Jenkins, Cloud Build, Vagrant, Puppet, Python, Bash, Go, Perl, Java, Scala, Groovy, Maven, SBT, Gradle, Make, GitHub Actions, CircleCI, Jenkinsfile, Makefile, Dockerfile, docker-compose.yml etc.\n- [Kubernetes Configs](https://github.com/HariSekhon/Kubernetes-configs) - Kubernetes YAML configs for most common scenarios, including Production Best Practices, Tips & Tricks\n\n### Dot Configs\n\nTop-level dotfiles and `configs/` directory:\n\n- `.*` - dot conf files for lots of common software eg. advanced `.vimrc`, `.gitconfig`, massive `.gitignore`, `.editorconfig`, `.screenrc`, `.tmux.conf` etc.\n  - `.vimrc` - contains many awesome [vim](https://www.vim.org/) tweaks, plus hotkeys for linting lots of different file types in place, including Python, Perl, Bash / Shell, Dockerfiles, JSON, YAML, XML, CSV, INI / Properties files, LDAP LDIF etc without leaving the editor!\n  - `.screenrc` - fancy [screen](https://www.gnu.org/software/screen/) configuration including advanced colour bar, large history, hotkey reloading, auto-blanking etc.\n  - `.tmux.conf` - fancy [tmux](https://github.com/tmux/tmux/wiki) configuration include advanced colour bar and plugins, settings, hotkey reloading etc.\n  - [Git](https://git-scm.com/):\n    - `.gitconfig` - advanced Git configuration\n    - `.gitignore` - extensive Git ignore of trivial files you shouldn't commit\n    - enhanced Git diffs\n    - protections against committing AWS secret keys or merge conflict unresolved files\n\n### Bash Environment & Libraries\n\nTop-level `.bashrc` and `.bash.d/` directory:\n\n- `.bashrc` - shell tuning and sourcing of `.bash.d/*.sh`\n- `.bash.d/*.sh` - thousands of lines of advanced bashrc code, aliases, functions and environment variables for:\n  - [Linux](https://en.wikipedia.org/wiki/Linux) & [Mac](https://en.wikipedia.org/wiki/MacOS)\n  - SCM - [Git](https://git-scm.com/), [Mercurial](https://www.mercurial-scm.org/), [Svn](https://subversion.apache.org)\n  - [AWS](https://aws.amazon.com/)\n  - [GCP](https://cloud.google.com/)\n  - [Docker](https://www.docker.com/)\n  - [Kubernetes](https://kubernetes.io/)\n  - [Kafka](http://kafka.apache.org/)\n  - [Vagrant](https://www.vagrantup.com/)\n  - automatic GPG and SSH agent handling for handling encrypted private keys without re-entering passwords, and lazy evaluation to only prompt key load the first time SSH is called\n  - and lots more - see [.bash.d/README](https://github.com/HariSekhon/DevOps-Bash-tools/blob/master/.bash.d/README.md) for a more detailed list\n  - run `make bash` to link `.bashrc`/`.bash_profile` and the `.*` dot config files to your `$HOME` directory to auto-inherit everything\n- `lib/*.sh` - Bash utility libraries full of functions for\n  [Docker](https://www.docker.com/),\n  environment,\n  CI detection ([Travis CI](https://travis-ci.org/), [Jenkins](https://jenkins.io/) etc),\n  port and HTTP url availability content checks etc.\n  Sourced from all my other [GitHub repos](https://github.com/harisekhon) to make setting up Dockerized tests easier.\n\n### Installation Scripts\n\n- `install/install_*.sh` - various simple to use installation scripts for common technologies like:\n  - [AWS CLI](https://aws.amazon.com/cli/)\n  - [Azure CLI](https://docs.microsoft.com/en-us/cli/azure/?view=azure-cli-latest)\n  - [GCloud SDK](https://cloud.google.com/sdk)\n  - [GitHub CLI](https://cli.github.com/)\n  - [Terraform](https://www.terraform.io/)\n  - [Terragrunt](https://terragrunt.gruntwork.io/)\n  - [Direnv](https://direnv.net/)\n  - [Ansible](https://www.ansible.com/)\n  - [K3s](https://k3s.io)\n  - [MiniKube](https://kubernetes.io/docs/setup/learning-environment/minikube/) (Kubernetes)\n  - [MiniShift](https://www.okd.io/minishift/)\n    ([Redhat OpenShift](https://www.openshift.com/) / [OKD](https://www.okd.io/) dev VMs)\n  - [Maven](https://maven.apache.org/)\n  - [Gradle](https://gradle.org/)\n  - [SBT](https://www.scala-sbt.org/)\n  - [EPEL](https://fedoraproject.org/wiki/EPEL)\n  - [RPMforge](http://repoforge.org/)\n  - [Homebrew](https://brew.sh/)\n  - [Travis CI](https://travis-ci.org/)\n  - [Circle CI](https://circleci.com/)\n  - [AppVeyor](https://www.appveyor.com/)\n  - [BuildKite](https://buildkite.com)\n  - [Avro Tools](https://avro.apache.org/)\n  - [Parquet Tools](https://github.com/apache/parquet-mr/tree/master/parquet-tools)\n  - [Prometheus](https://prometheus.io/)\n  - various JDKs and RDBMS JDBC connector jars\n  - and many more...\n\n### Linux & Mac\n\n`bin/` directory:\n\n- `login.sh` - logs to major Cloud platforms if their credentials are found in the environment, CLIs such as AWS, GCP, Azure, GitHub... Docker registries: DockerHub, GHCR, ECR, GCR, GAR, ACR, Gitlab, Quay...\n- `clean_caches.sh` - cleans out OS package and programming language caches - useful to save space or reduce Docker image size\n- `delete_duplicate_files.sh` - deletes duplicate files with (N) suffixes, commonly caused by web browser downloads,\n  in the given or current directory. Checks they're exact duplicates of a matching basename file without the (N) suffix with\n  the exact same checksum for safety. Prompts to delete per file. To auto-accept deletions, do\n  `yes | delete_duplicate_files.sh`. This is a fast way of cleaning up your `~/Downloads` directory and can be put your\n  user crontab\n- `download_url_file.sh` - downloads a file from a URL using wget with no clobber and continue support, or curl with atomic replacement to avoid race conditions. Used by `github/github_download_release_file.sh`, `github_download_release_jar.sh`, and `install/download_*_jar.sh`\n- `curl_auth.sh` - shortens `curl` command by auto-loading your OAuth2 / JWT API token or username & password from environment variables or interactive starred password prompt through a ram file descriptor to avoid placing them on the command line (which would expose your credentials in the process list or OS audit log files). Used by many other adjacent API querying scripts\n- `find_duplicate_files*.sh` - finds duplicate files by size and/or checksum in given directory trees. Checksums are only done on files that already have matching byte counts for efficiency\n- `find_broken_links.sh` - find broken links with delays to avoid tripping defenses\n- `find_broken_symlinks.sh` - find broken symlinks pointing to non-existent files/directories\n- `find_lock.sh` - tries to find if a lockfile is used in the given or current working directory by taking snapshots of the file list before and after a prompt in which you should open/close an application\n- `foreach_path_bin.sh` - runs each binary of the given name found in `$PATH` with the args given. Useful to find all the installed versions of a program in different paths eg. `~/bin/` vs `/usr/local/bin/` eg. `foreach_path_bin.sh terraform --version`\n- `http_duplicate_urls.sh` - find duplicate URLs in a given web page\n- `ldapsearch.sh` - shortens `ldapsearch` command by inferring switches from environment variables\n- `ldap_user_recurse.sh` / `ldap_group_recurse.sh` - recurse Active Directory LDAP users upwards to find all parent groups, or groups downwards to find all nested users (useful for debugging LDAP integration and group-based permissions)\n- `linux_distro_versions.sh` - quickly returns the list of major versions for a given Linux distro\n- `diff_line_threshold.sh` - compares two files vs a line count diff threshold to determine if they are radically different. Used to avoid overwriting files which are not mere updates but completely different files\n- `organize_downloads.sh` - moves files of well-known extensions in the `$HOME/Downloads` directory older than 1 week to capitalized subdirectories of their type to keep the `$HOME/Downloads/` directory tidy\n- `copy_to_clipboard.sh` - copies stdin or string arg to system clipboard on Linux or Mac\n- `paste_from_clipboard.sh` - pastes from system clipboard to stdout on Linux or Mac\n- `paste_diff_settings.sh` - takes snapshots of before and after clipboard changes and diffs them to show config changes\n- `processes_ram_sum.sh` - sums the RAM usage of all processes matching a given regex in GB to one decimal place\n- `pldd.sh` - parses `/proc` on Linux to show the runtime `.so` loaded dynamic shared libraries a program pid is using. Runtime equivalent of the classic static `ldd` command and because the system `pldd` command often fails to attach to a process\n- `random_select.sh` - selects one of given args at random. Useful for sampling, running randomized subsets of large test suites etc.\n- `random_number.sh` - prints a random integer between two integer arguments (inclusive)\n- `random_string.sh` - prints a random alphanumeric string of a given length\n- `shields_embed_logo.sh` - base64 encodes a given icon file or url and prints the `logo=...` url parameter you need to add the [shields.io](https://shields.io/) badge url\n- `shred_file.sh` - overwrites a file 7 times to DoD standards before deleting it to prevent recovery of sensitive information\n- `shred_free_space.sh` - overwrites free space to prevent recovery of sensitive information for files that have already been deleted\n- `split.sh` - split large files into N parts (defaults to the number of your CPU cores) to parallelize operations on them\n- `ssl_get_cert.sh` - gets a remote `host:port` server's SSL cert in a format you can pipe, save and use locally, for example in Java truststores\n- `ssl_verify_cert.sh` - verifies a remote SSL certificate (battle tested more feature-rich version `check_ssl_cert.pl` exists in the [Advanced Nagios Plugins](https://github.com/HariSekhon/Nagios-Plugins) repo)\n- `ssl_verify_cert_by_ip.sh` - verifies SSL certificates on specific IP addresses, useful to test SSL source addresses for CDNs, such as Cloudflare Proxied sources before enabling SSL Full-Strict Mode for end-to-end, or Kubernetes ingresses (see also `curl_k8s_ingress.sh`)\n- `ttygif.sh` - creates a Gif from running terminal commands using `ttyrec` and `ttygif` and then opens the resulting gif\n- `asciinema.sh` - creates a Gif from running terminal commands using `asciinema` and `agg` and then opens the resulting gif\n- `terminalizer.sh` - creates a Gif from running terminal commands using Terminalizer and then opens the resulting gif\n- `urlencode.sh` / `urldecode.sh` - URL encode/decode quickly on the command line, in pipes etc.\n- `urlextract.sh` - extracts the URLs from a given string arg, file or standard input\n- `url_extract_redirects.sh` - extracts the URLs from a given string arg, file or standard input, queries each one and outputs the redirected urls instead to stdout\n- `url_replace_redirects.sh` - extracts the URLs from a given string arg, file or standard input, queries each one and outputs the entire contents to stdout with the urls replaced by the redirected urls\n- `urlopen.sh` - opens the URL given as an arg, or first URL found from stdin or a given file.\n  Uses the system's default browser\n- `vagrant_hosts.sh` - generate `/etc/hosts` output from a `Vagrantfile`\n- `vagrant_total_mb.sh` - calculate the RAM committed to VMs in a `Vagrantfile`\n\nSee also [Knowledge Base notes for Linux](https://github.com/HariSekhon/Knowledge-Base/blob/main/linux.md)\nand [Mac](https://github.com/HariSekhon/Knowledge-Base/blob/main/mac.md).\n\n### Mac & AppleScript\n\nMac automation scripts to automate the Mac UI and settings\n\n`bin/` directory:\n\n- `mac_diff_settings.sh` - takes before and after snapshots of UI setting changes and diffs them to make it easy to find `defaults` keys to add to `setup/mac_settings.sh` to save settings\n- `mac_iso_to_usb.sh` - converts a given ISO file to a USB bootable image and burns it onto a given or detected inserted USB drive\n- `copy_to_clipboard.sh` - copies stdin or string arg to system clipboard on Linux or Mac\n- `paste_from_clipboard.sh` - pastes from system clipboard to stdout on Linux or Mac\n- `paste_diff_settings.sh` - Takes snapshots of before and after clipboard changes and diffs them to show config changes\n\n`applescript/` directory:\n\n- `keystrokes.sh` - send N keystroke combinations\n- `mouse_clicks.sh` - send N mouse click combinations to sequence of screen coordinates\n  - `get_mouse_coordinates.sh` - print the current mouse coordinates - to know what to pass to above script\n- `mouse_clicks_remote_desktop.sh` - switches to Microsoft Remote Desktop, waits 10 seconds and then clicks the mouse once a minute to prevent the screensaver from coming on. Workaround to Active Directory Group Policies that don't let you disable the screensaver. Point your mouse to an area that will have no mouse click effect, the Cmd-Tab to Terminal and run this\n- `get_frontmost_process_title.scpt` - detect the frontmost window\n  - to detect if you should send keystrokes / mouse clicks)\n- `set_frontmost_process.scpt` - switch to bring the given app to the foreground to send keystrokes / mouse clicks to it\n  - `browser_get_default.scpt` - get the default configured browser in format passable to Applescript (for  above script)\n- `is_screen_locked.py` - detect if the screen is locked to stop sending keystrokes or mouse clicks\n- `is_screensaver_running.scpt` - detect if the screensaver is running to stop sending keystrokes or mouse clicks\n- `screensaver_activate.scpt` - activate screensaver\n\nSee also [Mac](https://github.com/HariSekhon/Knowledge-Base/blob/main/mac.md) page\nin [HariSekhon/Knowledge-Base](https://github.com/HariSekhon/Knowledge-Base).\n\n### Monitoring\n\n`monitoring/` directory:\n\n- `dump_stats.sh` - dumps common command outputs to text files in a local tarball. Useful to collect support information\n  for vendor support cases\n- `grafana_api.sh` - queries the [Grafana](https://grafana.com/) API with authentication\n- `log_timestamp_large_intervals.sh` - finds log lines whose timestamp intervals exceed the given number of seconds and\n  outputs those log lines with the difference between the last and current timestamps. Useful to find actions that are\n  taking a long time from log files such as CI/CD logs\n- `prometheus.sh` - starts [Prometheus](https://prometheus.io/) locally, downloading it if not found in `$PATH`\n- `prometheus_docker.sh` - starts [Prometheus](https://prometheus.io/) in Docker using `docker-compose`\n- `prometheus_node_exporter.sh` - starts Prometheus `node_exporter` locally, downloading it if not found in `$PATH`\n- `ssh_dump_stats.sh` - uses SSH and `dump_stats.sh` to dump common command outputs from remote servers to a local\n  tarball. Useful for vendor support cases\n- `ssh_dump_logs.sh` - Uses SSH to dump logs from server to local text files for uploading to vendor support cases\n\nSee doc pages in [HariSekhon/Knowledge-Base](https://github.com/HariSekhon/Knowledge-Base) on Grafana,\nPrometheus, OpenTSDB, InfluxDB etc.\n\n### Databases\n\n`mysql/`, `postgres/`, `sql/` and `bin/` directories:\n\n- [sql/](https://github.com/HariSekhon/SQL-scripts) - 100+ SQL scripts for [PostgreSQL](https://www.postgresql.org/), [MySQL](https://www.mysql.com/), [Google BigQuery](https://cloud.google.com/bigquery) and [AWS Athena](https://aws.amazon.com/athena/) [CloudTrail](https://aws.amazon.com/cloudtrail/) logs integration\n- `sqlite.sh` - one-touch [SQLite](https://www.sqlite.org/index.html), starts sqlite3 shell with sample 'chinook' database loaded\n- `mysql*.sh` - [MySQL](https://www.mysql.com/) scripts:\n  - `mysql.sh` - shortens `mysql` command to connect to [MySQL](https://www.mysql.com/) by auto-populating switches from both standard environment variables like `$MYSQL_TCP_PORT`, `$DBI_USER`, `$MYSQL_PWD` (see [doc](https://dev.mysql.com/doc/refman/8.0/en/environment-variables.html)) and other common environment variables like `$MYSQL_HOST` / `$HOST`, `$MYSQL_USER` / `$USER`, `$MYSQL_PASSWORD` / `$PASSWORD`, `$MYSQL_DATABASE` / `$DATABASE`\n  - `mysql_foreach_table.sh` - executes a SQL query against every table, replacing `{db}` and `{table}` in each iteration eg. `select count(*) from {table}`\n  - `mysql_*.sh` - various scripts using `mysql.sh` for row counts, iterating each table, or outputting clean lists of databases and tables for quick scripting\n  - `mysqld.sh` - one-touch [MySQL](https://www.mysql.com/), boots docker container + drops in to `mysql` shell, with `/sql` scripts mounted in container for easy sourcing eg. `source /sql/<name>.sql`. Optionally loads sample 'chinook' database\n  - see also the [SQL Scripts](https://github.com/HariSekhon/SQL-scripts) repo for many more straight MySQL SQL scripts\n- `mariadb.sh` - one-touch [MariaDB](https://mariadb.org/), boots docker container + drops in to `mysql` shell, with `/sql` scripts mounted in container for easy sourcing eg. `source /sql/<name>.sql`. Optionally loads sample 'chinook' database\n- `postgres*.sh` / `psql.sh` - [PostgreSQL](https://www.postgresql.org/) scripts:\n  - `postgres.sh` - one-touch [PostgreSQL](https://www.postgresql.org/), boots docker container + drops in to `psql` shell, with `/sql` scripts mounted in container for easy sourcing eg. `\\i /sql/<name>.sql`. Optionally loads sample 'chinook' database\n  - `psql.sh` - shortens `psql` command to connect to [PostreSQL](https://www.postgresql.org/) by auto-populating switches from environment variables, using both standard postgres supported environment variables like `$PG*` (see [doc](https://www.postgresql.org/docs/12/libpq-envars.html)) as well as other common environment variables like `$POSTGRESQL_HOST` / `$POSTGRES_HOST` / `$HOST`, `$POSTGRESQL_USER` / `$POSTGRES_USER` / `$USER`, `$POSTGRESQL_PASSWORD` / `$POSTGRES_PASSWORD` / `$PASSWORD`, `$POSTGRESQL_DATABASE` / `$POSTGRES_DATABASE` / `$DATABASE`\n  - `postgres_foreach_table.sh` - executes a SQL query against every table, replacing `{db}`, `{schema}` and `{table}` in each iteration eg. `select count(*) from {table}`\n  - `postgres_*.sh` - various scripts using `psql.sh` for row counts, iterating each table, or outputting clean lists of databases, schemas and tables for quick scripting\n  - `checks/check_sqlfluff.sh` - recursively iterates all SQL code files found in the given or current directory and runs SQLFluff linter against them, inferring the different SQL dialects from each path/filename/extension\n\n### AWS - Amazon Web Services\n\n`aws/` directory:\n\n- [AWS](https://aws.amazon.com/) scripts - `aws_*.sh`:\n  - `aws_profile.sh` - switches to an AWS Profile given as an arg or prompts the user with a convenient interactive menu list of AWS profiles to choose from - useful when you have lots of AWS work profiles\n  - `aws_cli_create_credential.sh` - creates an AWS service account user for CI/CD or CLI with Admin permissions (or other group or policy), creates an AWS Access Key, saves a credentials CSV and even prints the shell export commands and aws credentials file config to configure your environment to start using it. Useful trick to avoid CLI reauth to `aws sso login` every day.\n  - `aws_terraform_create_credential.sh` - creates a AWS terraform service account with Administrator permissions for Terraform Cloud or other CI/CD systems to run Terraform plan and apply, since no CI/CD systems can work with AWS SSO workflows. Stores the access key as both CSV and prints shell export commands and credentials file config as above\n  - `.envrc-aws` - copy to `.envrc` for [direnv](https://direnv.net/) to auto-load AWS configuration settings such as AWS Profile, Compute Region, EKS cluster kubectl context etc.\n    - calls `.envrc-kubernetes` to set the `kubectl` context isolated to current shell to prevent race conditions between shells and scripts caused by otherwise naively changing the global `~/.kube/config` context\n  - `aws_sso_ssh.sh` - launches local AWS SSO authentication pop-up (if not already authenticated), then scp's the latest resultant `~/.aws/sso/cache/` file to the remote server and SSH's there so that you can use AWS CLI or kubectl to EKS remotely on that server easily, without having to copy and paste the token from remote aws sso login to your local web browser\n  - `aws_terraform_create_s3_bucket.sh` - creates a Terraform S3 bucket for storing the backend state, locks out public access, enables versioning, encryption, and locks out Power Users role and optionally any given user/group/role ARNs via a bucket policy for safety\n  - `aws_terraform_create_dynamodb_table.sh` - creates a Terraform locking table in DynamoDB for use with the S3 backend, plus custom IAM policy which can be applied to less privileged accounts\n  - `aws_terraform_create_all.sh` - runs all of the above, plus also applies the custom DynamoDB IAM policy to the user to ensure if the account is less privileged it can still get the Terraform lock (useful for GitHub Actions environment secret for a read only user to generate Terraform Plans in Pull Request without needing approval)\n  - `aws_terraform_iam_grant_s3_dynamodb.sh` - creates IAM policies to access any S3 buckets and DynamoDB tables with `terraform-state` or `tf-state` in their names, and attaches them to the given user. Useful for limited permissions CI/CD accounts that run Terraform Plan eg. in GitHub Actions pull requests\n  - `aws_account_summary.sh` - prints AWS account summary in `key = value` pairs for easy viewing / grepping of things like `AccountMFAEnabled`, `AccountAccessKeysPresent`, useful for checking whether the root account has MFA enabled and no access keys, comparing number of users vs number of MFA devices etc. (see also `check_aws_root_account.py` in [Advanced Nagios Plugins](https://github.com/HariSekhon/Nagios-Plugins))\n  - `aws_billing_alarm.sh` - creates a [CloudWatch](https://aws.amazon.com/cloudwatch/) billing alarm and [SNS](https://aws.amazon.com/sns/) topic with subscription to email you when you incur charges above a given threshold. This is often the first thing you want to do on an account\n  - `aws_budget_alarm.sh` - creates an [AWS Budgets](https://aws.amazon.com/cloudwatch/) billing alarm and [SNS](https://aws.amazon.com/sns/) topic with subscription to email you when both when you start incurring forecasted charges of over 80% of your budget, and 90% actual usage. This is often the first thing you want to do on an account\n  - `aws_batch_stale_jobs.sh` - lists [AWS Batch](https://aws.amazon.com/batch/) jobs that are older than N hours in a given queue\n  - `aws_batch_kill_stale_jobs.sh` - finds and kills [AWS Batch](https://aws.amazon.com/batch/) jobs that are older than N hours in a given queue\n  - `aws_cloudfront_distribution_for_origin.sh` - returns the AWS CloudFront ARN of the distribution which serves origins containing a given substring. Useful for quickly finding the CloudFront ARN needed to give permissions to a private S3 bucket exposed via CloudFront\n  - `aws_cloudtrails_cloudwatch.sh` - lists [Cloud Trails](https://aws.amazon.com/cloudtrail/) and their last delivery to [CloudWatch](https://aws.amazon.com/cloudwatch/features/) Logs (should be recent)\n  - `aws_cloudtrails_event_selectors.sh` - lists [Cloud Trails](https://aws.amazon.com/cloudtrail/) and their event selectors to check each one has at least one event selector\n  - `aws_cloudtrails_s3_accesslogging.sh` - lists [Cloud Trails](https://aws.amazon.com/cloudtrail/) buckets and their Access Logging prefix and target bucket. Checks [S3 access logging](https://docs.aws.amazon.com/AmazonS3/latest/dev/ServerLogs.html) is enabled\n  - `aws_cloudtrails_s3_kms.sh` - lists [Cloud Trails](https://aws.amazon.com/cloudtrail/) and whether their [S3](https://aws.amazon.com/s3/) buckets are [KMS](https://aws.amazon.com/kms/) secured\n  - `aws_cloudtrails_status.sh` - lists [Cloud Trails](https://aws.amazon.com/cloudtrail/) status - if logging, multi-region and log file validation enabled\n  - `aws_config_all_types.sh` - lists [AWS Config](https://aws.amazon.com/config/) recorders, checking all resource types are supported (should be true) and includes global resources (should be true)\n  - `aws_config_recording.sh` - lists [AWS Config](https://aws.amazon.com/config/) recorders, their recording status (should be true) and their last status (should be success)\n  - `aws_csv_creds.sh` - prints AWS credentials from a CSV file as shell export statements. Useful to quickly switch your shell to some exported credentials from a service account for testing permissions or pipe to upload to a CI/CD system via an API (eg. `jenkins_cred_add*.sh`, `github_actions_repo*_set_secret.sh`, `gitlab_*_set_env_vars.sh`, `circleci_*_set_env_vars.sh`, `bitbucket_*_set_env_vars.sh`, `terraform_cloud_*_set_vars.sh`, `kubectl_kv_to_secret.sh`). Supports new user and new access key csv file formats.\n  - `aws_codecommit_csv_creds.sh` - prints AWS [CodeCommit](https://aws.amazon.com/codecommit/) Git credentials from a CSV file as shell export statements. Similar use case and chaining as above\n  - `aws_ec2_instance_name_to_id.sh` - looks up an EC2 instance ID from an instance name with extra safety checks that only a single instance ID is returned and a reverse lookup on that instance ID to re-verify it matches the name. If an instance ID is passed, returns it as is for convenience. Used by adjacent scripts\n  - `aws_ec2_instances.sh` - lists AWS EC2 instances, their DNS names and States in an easy to read table output\n  - `aws_ec2_terminate_instance_by_name.sh` - terminate an AWS EC2 instance by name\n  - `aws_ec2_create_ami_from_instance.sh` - creates an AWS EC2 AMI from an EC2 instance and waits for it to become available for use\n  - `aws_ec2_clone_instance.sh` - clones an AWS EC2 instance by creating an AMI from the original and then booting a new instance from the AMI with the same settings as the original instance. Useful to testing risky things on a separate EC2 instance, such as Server Administrator recovery of Tableau\n  - `aws_ec2_amis.sh` - list AWS EC2 AMIs belonging to your account in an easy to read table output\n  - `aws_ec2_ami_ids.sh` - lists AWS EC2 AMI IDs only, one per line, to be used in adjacent scripts that creating mapping tables and translate AMI IDs to names in inventory scripts `aws_info_ec2*.sh`\n  - `aws_ec2_ebs_*.sh` - AWS EC2 [EBS](https://aws.amazon.com/ebs/) scripts:\n    - `aws_ec2_ebs_volumes.sh` - list EC2 instances and their EBS volumes in the current region\n    - `aws_ec2_ebs_create_snapshot_and_wait.sh - creates a snapshot of a given EBS volume ID and waits for it to complete with exponential backoff\n    - `aws_ec2_ebs_resize_and_wait.sh - resizes an EBS volume and waits for it to complete modifying and optionally optimizing with exponential backoff\n    - `aws_ec2_ebs_volumes_unattached.sh` - list an unattached EBS volumes in a table format\n  - `aws_ec2_launch_templates_ami_id.sh` - for each Launch Template lists the AMI ID of the latest version. Useful to check EKS upgrades of node groups via Terragrunt have taken effect\n  - `aws_ecr_*.sh` - AWS [ECR](https://aws.amazon.com/ecr/) docker image management scripts:\n    - `aws_ecr_docker_login.sh` - authenticates Docker to AWS ECR, inferring the ECR registry from the current AWS Account ID and Region\n    - `aws_ecr_docker_build_push.sh` - builds a docker image and pushes it to ECR with not just the `latest` docker tag but also the current Git hashref and Git tags\n    - `aws_ecr_list_repos.sh` - lists ECR repos, and their docker image mutability and whether image scanning is enabled\n    - `aws_ecr_list_tags.sh` - lists all the tags for a given ECR docker image\n    - `aws_ecr_newest_image_tags.sh` - lists the tags for the given ECR docker image with the newest creation date (can use this to determine which image version to tag as `latest`)\n    - `aws_ecr_alternate_tags.sh` - lists all the tags for a given ECR docker `image:tag` (use arg `<image>:latest` to see what version / build hashref / date tag has been tagged as `latest`)\n    - `aws_ecr_tag_image.sh` - tags an ECR image with another tag without pulling and pushing it\n    - `aws_ecr_tag_image_by_digest.sh` - same as above but tags an ECR image found via digest (more accurate as reference by existing tag can be a moving target). Useful to recover images that have become untagged\n    - `aws_ecr_tag_latest.sh` - tags a given ECR docker `image:tag` as `latest` without pulling or pushing the docker image\n    - `aws_ecr_tag_branch.sh` - tags a given ECR `image:tag` with the current Git branch without pulling or pushing the docker image\n    - `aws_ecr_tag_datetime.sh` - tags a given ECR docker image with its creation date and UTC timestamp (when it was uploaded to ECR) without pulling or pushing the docker image\n    - `aws_ecr_tag_newest_image_as_latest.sh` - finds and tags the newest build of a given ECR docker image as `latest` without pulling or pushing the docker image\n    - `aws_ecr_tags_timestamps.sh` - lists all the tags and their timestamps for a given ECR docker image\n    - `aws_ecr_tags_old.sh` - lists tags older than N days for a given ECR docker image\n    - `aws_ecr_delete_old_tags.sh` - deletes tags older than N days for a given ECR docker image. Lists the image:tags to be deleted and prompts for confirmation safety\n  - `aws_emr_clusters_last_steps.sh` - shows the last N steps executed on each EMR cluster and their EndTime to find idle clusters that should be removed. Also checks CloudWatch for number of steps running within the last few months to catch directly submitted jobs such as Spark, Hive, Glue or Athena which won't show up in the native steps list\n  - `aws_foreach_profile.sh` - executes a templated command across all AWS named profiles configured in AWS CLIv2, replacing `{profile}` in each iteration. Combine with other scripts for powerful functionality, auditing, setup etc. eg. `aws_kube_creds.sh` to configure `kubectl` config to all EKS clusters in all environments\n  - `aws_foreach_region.sh` - executes a templated command against each AWS region enabled for the current account, replacing `{region}` in each iteration. Combine with AWS CLI or scripts to find resources across regions\n  - `aws_iam_*.sh` - AWS [IAM](https://aws.amazon.com/iam/) scripts:\n    - `aws_iam_password_policy.sh` - prints [AWS password policy](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_passwords_account-policy.html) in `key = value` pairs for easy viewing / grepping (used by `aws_harden_password_policy.sh` before and after to show the differences)\n    - `aws_iam_harden_password_policy.sh` - strengthens [AWS password policy](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_passwords_account-policy.html) according to [CIS Foundations Benchmark](https://d1.awsstatic.com/whitepapers/compliance/AWS_CIS_Foundations_Benchmark.pdf) recommendations\n    - `aws_iam_replace_access_key.sh` - replaces the non-current IAM access key (Inactive, Not Used, longer time since used, or an explicitly given key), outputting the new key as shell export statements (useful for piping to the same tools listed for `aws_csv_creds.sh` above)\n    - `aws_iam_policies_attached_to_users.sh` - finds [AWS IAM policies](https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_manage.html) directly attached to users (anti-best practice) instead of groups\n    - `aws_iam_policies_granting_full_access.sh` - finds [AWS IAM policies](https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_manage.html) granting full access (anti-best practice)\n    - `aws_iam_policies_unattached.sh` - lists unattached [AWS IAM policies](https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_manage.html)\n    - `aws_iam_policy_attachments.sh` - finds all users, groups and roles where a given IAM policy is attached, so that you can remove all these references in your Terraform code and avoid this error `Error: error deleting IAM policy arn:aws:iam::***:policy/mypolicy: DeleteConflict: Cannot delete a policy attached to entities.`\n    - `aws_iam_policy_delete.sh` - deletes an IAM policy, by first handling all prerequisite steps of deleting all prior versions and all detaching all users, groups and roles\n    - `aws_iam_generate_credentials_report_wait.sh` - generates an AWS IAM [credentials report](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_getting-report.html)\n    - `aws_iam_users.sh` - list your IAM users\n    - `aws_iam_users_access_key_age.sh` - prints AWS users [access key](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html) status and age (see also `aws_users_access_key_age.py` in [DevOps Python tools](https://github.com/HariSekhon/DevOps-Python-tools) which can filter by age and status)\n    - `aws_iam_users_access_key_age_report.sh` - prints AWS users [access key](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html) status and age using a bulk credentials report (faster for many users)\n    - `aws_iam_users_access_key_last_used.sh` - prints AWS users [access keys](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html) last used date\n    - `aws_iam_users_access_key_last_used_report.sh` - same as above using bulk credentials report (faster for many users)\n    - `aws_iam_users_last_used_report.sh` - lists AWS users password/access keys last used dates\n    - `aws_iam_users_mfa_active_report.sh` - lists AWS users password enabled and [MFA](https://aws.amazon.com/iam/features/mfa/) enabled status\n    - `aws_iam_users_without_mfa.sh` - lists AWS users with password enabled but no MFA\n    - `aws_iam_users_mfa_serials.sh` - lists AWS users [MFA](https://aws.amazon.com/iam/features/mfa/) serial numbers (differentiates Virtual vs Hardware MFAs)\n    - `aws_iam_users_pw_last_used.sh` - lists AWS users and their password last used date\n  - `aws_ip_ranges.sh` - get all AWS IP ranges for a given Region and/or Service using the IP range API\n  - `aws_info*.sh`:\n    - `aws_info_all_profiles.sh` - calls `aws_info.sh` for all AWS profiles using `aws_foreach_profile.sh`\n      - `aws_info.sh` - lists AWS deployed resources in the current or specified AWS account profile\n        - `aws_info_ec2.sh` - lists AWS EC2 Instances resources deployed in the current AWS account\n    - `aws_info_ec2_csv.sh` - lists AWS EC2 Instances in quoted CSV format in the current AWS account\n    - `aws_info_ec2_all_profiles_csv.sh` - lists AWS EC2 Instances in quoted CSV format across all configured AWS profiles for their configured region\n  - `aws_eks_cloudwatch_logs.sh` - enables and fetches AWS EKS Master logs via CloudWatch\n  - `aws_eks_ssh_dump_logs.sh` - fetch system logs from EKS Worker Nodes EC2 VMs (eg. for support debug requests by vendors)\n  - `aws_eks_cluster_versions.sh` - iterates EKS clusters to list each AWS EKS cluster name and version in the current account. Combine with `aws_foreach_profile.sh` and `aws_foreach_region.sh` to audit your EKS cluster versions across accounts and regions\n  - `aws_eks_addon_versions.sh` - lists the EKS addon versions available for the given cluster by checking its version before checking addons\n  - `aws_eks_available_ips.sh` - lists the number of available IP addresses in the EKS subnets for the given cluster (5 required for an EKS upgrade)\n  - `aws_kms_key_rotation_enabled.sh` - lists [AWS KMS](https://aws.amazon.com/kms/) keys and whether they have key rotation enabled\n  - `aws_kube_creds.sh` - auto-loads all [AWS EKS](https://aws.amazon.com/eks/) clusters credentials in the current --profile and --region so your kubectl is ready to rock on AWS\n  - `aws_kubectl.sh` - runs kubectl commands safely fixed to a given [AWS EKS](https://aws.amazon.com/eks/) cluster using config isolation to avoid concurrency race conditions\n  - `aws_logs_*.sh` - some useful log queries in last N hours (24 hours by default):\n    - `aws_logs_batch_jobs.sh` - lists AWS Batch job submission requests and their callers\n    - `aws_logs_ec2_spot.sh` - lists AWS EC2 Spot fleet creation requests, their caller and first tag value for origin hint\n    - `aws_logs_ecs_tasks.sh` - lists AWS ECS task run requests, their callers and job definitions\n  - `aws_meta.sh` - AWS [EC2 Metadata API](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html) query shortcut. See also the official [ec2-metadata](https://aws.amazon.com/code/ec2-instance-metadata-query-tool/) shell script with more features\n  - `aws_nat_gateways_public_ips.sh` - lists the public IPs of all NAT gateways. Useful to give to clients to permit through firewalls for webhooks or similar calls\n  - `aws_rds_list.sh` - list RDS instances with select fields - Name, Status, Engine, AZ, Instance Type, Storage\n  - `aws_rds_open_port_to_my_ip.sh` - adds a security group to an RDS DB instance to open its native database SQL port to your public IP address\n  - `aws_rds_get_version.sh` - quickly retrieve the version of an RDS database to know which JDBC jar version to download using `install/download_*_jdbc.sh` when setting up connections\n  - `aws_route53_check_ns_records.sh` - checks AWS [Route 53](https://aws.amazon.com/route53/) public hosted zones NS servers are delegated in the public DNS hierarchy and that there are no rogue NS servers delegated not matching the Route 53 zone configuration\n  - `aws_sso_accounts.sh` - lists all AWS SSO accounts the current SSO user has access to\n  - `aws_sso_configs.sh` - generates AWS SSO configs for all AWS SSO accounts the currently logged in SSO user has access to\n  - `aws_sso_configs_save.sh` - saves AWS SSO configs generated by `aws_sso_configs.sh` to `~/.aws/config` if they're not already found\n  - `aws_sso_config_duplicate_sections.sh` - lists duplicate AWS SSO config sections that are using the same sso_account_id. Useful to deduplicate configs containing a mix of hand crafted and automatically generated `aws_sso_configs.sh`\n  - `aws_sso_config_duplicate_profile_names.sh` - lists duplicate AWS SSO config profile names that are using the same sso_account_id\n  - `aws_sso_env_creds.sh` - retrieves AWS SSO session credentials in the format of environment export commands for copying to other systems like Terraform Cloud\n  - `aws_sso_role_arn.sh` - prints the currently authenticated AWS SSO user's role ARN in IAM policy usable format\n  - `aws_sso_role_arns.sh` - prints all AWS SSO role ARNs in IAM policy usable format\n  - `aws_profile_config_add_if_missing.sh` - reads AWS profile config blocks from stdin and appends them to the `~/.aws/config` file if the profile section is not found\n  - `aws_profile_generate_direnvs.sh` - generates subdirectories containing the `config.ini` and `.envrc` for every AWS profile found in the given file or `$AWS_CONFIG_FILE` or `~/.aws/config`. Useful to take a large generated AWS `config.ini` from `aws_sso_configs.sh` and then split it into subdirectories for direnvs\n  - `aws_s3_bucket.sh` - creates an S3 bucket, blocks public access, enables versioning, encryption, and optionally locks out any given user/group/role ARNs via a bucket policy for safety (eg. to stop Power Users accessing a sensitive bucket like Terraform state)\n  - `aws_s3_buckets_block_public_access.sh` - blocks public access to one or more given S3 buckets or files containing bucket names, one per line\n  - `aws_s3_account_block_public_access.sh` - blocks S3 public access at the AWS account level\n  - `aws_s3_check_buckets_public_blocked.sh` - iterates each S3 bucket and checks it has public access fully blocked via policy. Parallelized for speedup\n  - `aws_s3_check_account_public_blocked.sh` - checks S3 public access is blocked at the AWS account level\n  - `aws_s3_sync.sh` - syncs multiple AWS S3 URLs from file lists. Validates S3 URLs, source and destination list lengths matches, and optionally that path suffixes match, to prevent off-by-one human errors spraying data all over the wrong destination paths\n  - `aws_s3_access_logging.sh` - lists [AWS S3](https://aws.amazon.com/s3/) buckets and their [access logging](https://docs.aws.amazon.com/AmazonS3/latest/dev/ServerLogs.html) status\n  - `aws_s3_delete_bucket_with_versions.sh` - deletes a bucket including all versions. Use with caution!\n  - `aws_spot_when_terminated.sh` - executes commands when the [AWS EC2](https://aws.amazon.com/ec2/) instance running this script is notified of [Spot Termination](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html), acts as a latch mechanism that can be set any time after boot\n  - `aws_sqs_check.sh` - sends a test message to an [AWS SQS](https://aws.amazon.com/sqs/) queue, retrieves it to check and then deletes it via the receipt handle id\n  - `aws_sqs_delete_message.sh` - deletes 1-10 messages from a given [AWS SQS](https://aws.amazon.com/sqs/) queue (to help clear out test messages)\n  - `aws_ssm_put_param.sh` - reads a value from a command line argument or non-echo prompt and saves it to AWS [Systems Manager Parameter Store](https://docs.aws.amazon.com/systems-manager/latest/userguide/what-is-systems-manager.html). Useful for uploading a password without exposing it on your screen\n  - `aws_secret*.sh` - AWS [Secrets Manager](https://aws.amazon.com/secrets-manager/) scripts:\n    - `aws_secret_list.sh` - returns the list of secrets, one per line\n    - `aws_secret_add.sh` - reads a value from a command line argument or non-echo prompt and saves it to Secrets Manager. Useful for uploading a password without exposing it on your screen\n    - `aws_secret_add_binary.sh` - base64 encodes a given file's contents and saves it to Secrets Manager as a binary secret. Useful for uploading things like QR code screenshots for sharing MFA to recovery admin accounts\n    - `aws_secret_update.sh` - reads a value from a command line argument or non-echo prompt and updates a given Secrets Manager secret. Useful for updating a password without exposing it on your screen\n    - `aws_secret_update_binary.sh` - base64 encodes a given file's contents and updates a given Secrets Manager secret. Useful for updating a QR code screenshot for a root account\n    - `aws_secret_get.sh` - gets a secret value for a given secret from Secrets Manager, retrieving either a secure string or secure binary depending on which is available\n  - `eksctl_cluster.sh` - downloads [eksctl](https://eksctl.io/) and creates an [AWS EKS](https://aws.amazon.com/eks/) Kubernetes cluster\n\nSee also [Knowledge Base notes for AWS](https://github.com/HariSekhon/Knowledge-Base/blob/main/aws.md).\n\n### GCP - Google Cloud Platform\n\n`gcp/` directory:\n\n- [Google Cloud](https://cloud.google.com/) scripts - `gcp_*.sh` / `gce_*.sh` / `gke_*.sh` / `gcr_*.sh` / `bigquery_*.sh`:\n  - `.envrc-gcp` - copy to `.envrc` for [direnv](https://direnv.net/) to auto-load GCP configuration settings such as Project, Region, Zone, GKE cluster kubectl context or any other GCloud SDK settings to shorten `gcloud` commands. Applies to the local shell environment only to avoid race conditions caused by naively changing the global gcloud config at `~/.config/gcloud/active_config`\n    - calls `.envrc-kubernetes` to set the `kubectl` context isolated to current shell to prevent race conditions between shells and scripts caused by otherwise naively changing the global `~/.kube/config` context\n  - `gcp_terraform_create_credential.sh` - creates a service account for [Terraform](https://www.terraform.io/) with full permissions, creates and downloads a credential key json and even prints the `export GOOGLE_CREDENTIALS` command to configure your environment to start using Terraform immediately. Run once for each project and combine with [direnv](https://direnv.net/) for fast easy management of multiple GCP projects\n  - `gcp_ansible_create_credential.sh` - creates an [Ansible](https://www.ansible.com/) service account with permissions on the current project, creates and downloads a credential key json and prints the environment variable to immediately use it\n  - `gcp_cli_create_credential.sh` - creates a GCloud SDK CLI service account with full owner permissions to all projects, creates and downloads a credential key json and even prints the `export GOOGLE_CREDENTIALS` command to configure your environment to start using it. Avoids having to reauth to `gcloud auth login` every day.\n  - `gcp_spinnaker_create_credential.sh` - creates a [Spinnaker](https://spinnaker.io/) service account with permissions on the current project, creates and downloads a credential key json and even prints the Halyard CLI configuration commands to use it\n  - `gcp_info.sh` - huge [Google Cloud](https://cloud.google.com/) inventory of deployed resources within the current project - Cloud SDK info plus all of the following (detects which services are enabled to query):\n    - `gcp_info_compute.sh` - [GCE](https://cloud.google.com/compute/) Virtual Machine instances, [App Engine](https://cloud.google.com/appengine) instances, [Cloud Functions](https://cloud.google.com/functions), [GKE](https://cloud.google.com/kubernetes-engine) clusters, all [Kubernetes](https://kubernetes.io/) objects across all GKE clusters (see `kubernetes_info.sh` below for more details)\n    - `gcp_info_storage.sh` - [Cloud SQL](https://cloud.google.com/sql) info below, plus: [Cloud Storage](https://cloud.google.com/storage) Buckets, [Cloud Filestore](https://cloud.google.com/filestore), [Cloud Memorystore Redis](https://cloud.google.com/memorystore), [BigTable](https://cloud.google.com/bigtable) clusters and instances, [Datastore](https://cloud.google.com/datastore) indexes\n    - `gcp_info_cloud_sql.sh` - [Cloud SQL](https://cloud.google.com/sql) instances, whether their backups are enabled, and all databases on each instance\n      - `gcp_info_cloud_sql_databases.sh` - lists databases inside each [Cloud SQL](https://cloud.google.com/sql) instance. Included in `gcp_info_cloud_sql.sh`\n      - `gcp_info_cloud_sql_backups.sh` - lists backups for each [Cloud SQL](https://cloud.google.com/sql) instance with their dates and status. Not included in `gcp_info_cloud_sql.sh` for brevity. See also `gcp_sql_export.sh` further down for more durable backups to [GCS](https://cloud.google.com/storage)\n      - `gcp_info_cloud_sql_users.sh` - lists users for each running [Cloud SQL](https://cloud.google.com/sql) instance. Not included in `gcp_info_cloud_sql.sh` for brevity but useful to audit users\n    - `gcp_info_networking.sh` - VPC Networks, Addresses, Proxies, Subnets, Routers, Routes, VPN Gateways, VPN Tunnels, Reservations, Firewall rules, Forwarding rules, [Cloud DNS](https://cloud.google.com/dns) managed zones and verified domains\n    - `gcp_info_bigdata.sh` - [Dataproc](https://cloud.google.com/dataproc) clusters and jobs in all regions, [Dataflow](https://cloud.google.com/dataflow) jobs in all regions, [PubSub](https://cloud.google.com/pubsub) messaging topics, [Cloud IOT](https://cloud.google.com/iot-core) registries in all regions\n    - `gcp_info_tools.sh` - [Cloud Source Repositories](https://cloud.google.com/source-repositories), [Cloud Builds](https://cloud.google.com/cloud-build), [Container Registry](https://cloud.google.com/container-registry) images across all major repos (`gcr.io`, `us.gcr.io`, `eu.gcr.io`, `asia.gcr.io`), [Deployment Manager](https://cloud.google.com/deployment-manager) deployments\n    - `gcp_info_auth_config.sh` - Auth Configurations, Organizations & Current Config\n    - `gcp_info_projects.sh` - Projects names and IDs\n    - `gcp_info_services.sh` - Services & APIs enabled\n      - `gcp_service_apis.sh` - lists all available [GCP](https://cloud.google.com/) Services, APIs and their states (enabled/disabled), and provides `is_service_enabled()` function used throughout the adjacent scripts to avoid errors and only show relevant enabled services\n    - `gcp_info_accounts_secrets.sh` - [IAM](https://cloud.google.com/iam) Service Accounts, [Secret Manager](https://cloud.google.com/secret-manager) secrets\n  - `gcp_info_all_projects.sh` - same as above but for all detected projects\n  - `gcp_foreach_project.sh` - executes a templated command across all GCP projects, replacing `{project_id}` and `{project_name}` in each iteration (used by `gcp_info_all_projects.sh` to call `gcp_info.sh`)\n  - `gcp_find_orphaned_disks.sh` - lists orphaned disks across one or more GCP projects (not attached to any compute instance)\n  - `gcp_secret*.sh` - Google [Secret Manager](https://cloud.google.com/secret-manager) scripts:\n    - `gcp_secret_add.sh` - reads a value from a command line argument or non-echo prompt and saves it to GCP Secrets Manager. Useful for uploading a password without exposing it on your screen\n    - `gcp_secret_add_binary.sh` - uploads a binary file to GCP Secrets Manager by base64 encoding it first. Useful for uploading QR code screenshots. Useful for uploading things like QR code screenshots for sharing MFA to recovery admin accounts\n    - `gcp_secret_update.sh` - reads a value from a command line argument or non-echo prompt and updates a given GCP Secrets Manager secret. Useful for uploading a password without exposing it on your screen\n    - `gcp_secret_get.sh` - finds the latest version of a given GCP Secret Manager secret and returns its value. Used by adjacent scripts\n    - `gcp_secret_label_k8s.sh` - labels a given existing GCP secret with the current kubectl cluster name and namespace for later use by `gcp_secrets_to_kubernetes.sh`\n    - `gcp_secrets_to_kubernetes.sh` - loads GCP secrets to Kubernetes secrets in a 1-to-1 mapping. Can specify a list of secrets or auto-loads all GCP secrets with labels `kubernetes-cluster` and `kubernetes-namespace` matching the current `kubectl` context (`kcd` to the right namespace first, see `.bash.d/kubernetes`). See also `kubernetes_get_secret_values.sh` to debug the actual values that got loaded. See also [Sealed Secrets](https://github.com/bitnami-labs/sealed-secrets) / [External Secrets](https://external-secrets.io/) in my [Kubernetes repo](https://github.com/HariSekhon/Kubernetes-configs)\n    - `gcp_secrets_to_kubernetes_multipart.sh` - creates a Kubernetes secret from multiple GCP secrets (used to put `private.pem` and `public.pem` into the same secret to appear as files on volume mounts for apps in pods to use). See also [Sealed Secrets](https://github.com/bitnami-labs/sealed-secrets) / [External Secrets](https://external-secrets.io/) in my [Kubernetes repo](https://github.com/HariSekhon/Kubernetes-configs)\n    - `gcp_secrets_labels.sh` - lists GCP Secrets and their labels, one per line suitable for quick views or shell pipelines\n    - `gcp_secrets_update_lable.sh` - updates all GCP secrets in current project matching label key=value with a new label value\n    - `gcp_service_account_credential_to_secret.sh` - creates GCP service account and exports a credential key to GCP Secret Manager (useful to stage or combine with `gcp_secrets_to_kubernetes.sh`)\n  - `gke_*.sh` - Google [Kubernetes Engine](https://cloud.google.com/kubernetes-engine) scripts\n    - `gke_kube_creds.sh` - auto-loads all GKE clusters credentials in the current / given / all projects so your kubectl is ready to rock on GCP\n    - `gke_kubectl.sh` - runs kubectl commands safely fixed to a given GKE cluster using config isolation to avoid concurrency race conditions\n    - `gke_firewall_rule_cert_manager.sh` - creates a GCP firewall rule for a given GKE cluster's masters to access [Cert Manager](https://cert-manager.io/) admission webhook (auto-determines the master cidr, network and target tags)\n    - `gke_firewall_rule_kubeseal.sh` - creates a GCP firewall rule for a given GKE cluster's masters to access [Sealed Secrets](https://github.com/bitnami-labs/sealed-secrets) controller for `kubeseal` to work (auto-determines the master cidr, network and target tags)\n    - `gke_nodepool_nodes.sh` - lists all nodes in a given nodepool on the current GKE cluster via kubectl labels (fast)\n    - `gke_nodepool_nodes2.sh` - same as above via GCloud SDK (slow, iterates instance groups)\n    - `gke_nodepool_taint.sh` - taints/untaints all nodes in a given GKE nodepool on the current cluster (see `kubectl_node_taints.sh` for a quick way to see taints)\n    - `gke_nodepool_drain.sh` - drains all nodes in a given nodepool (to decommission or rebuild the node pool, for example with different taints)\n    - `gke_persistent_volumes_disk_mappings.sh` - lists GKE kubernetes persistent volumes to GCP persistent disk names, along with PVC and namespace, useful when investigating, resizing PVs etc.\n  - `gcr_*.sh` - Google [Container Registry](https://cloud.google.com/container-registry) scripts:\n    - `gcr_list_tags.sh` - lists all the tags for a given GCR docker image\n    - `gcr_newest_image_tags.sh` - lists the tags for the given GCR docker image with the newest creation date (can use this to determine which image version to tag as `latest`)\n    - `gcr_alternate_tags.sh` - lists all the tags for a given GCR docker `image:tag` (use arg `<image>:latest` to see what version / build hashref / date tag has been tagged as `latest`)\n    - `gcr_tag_latest.sh` - tags a given GCR docker `image:tag` as `latest` without pulling or pushing the docker image\n    - `gcr_tag_branch.sh` - tags a given GCR docker `image:tag` with the current Git branch without pulling or pushing the docker image\n    - `gcr_tag_datetime.sh` - tags a given GCR docker image with its creation date and UTC timestamp (when it was uploaded or created by [Google Cloud Build](https://cloud.google.com/cloud-build)) without pulling or pushing the docker image\n    - `gcr_tag_newest_image_as_latest.sh` - finds and tags the newest build of a given GCR docker image as `latest` without pulling or pushing the docker image\n    - `gcr_tags_timestamps.sh` - lists all the tags and their timestamps for a given GCR docker image\n    - `gcr_tags_old.sh` - lists tags older than N days for a given GCR docker image\n    - `gcr_delete_old_tags.sh` - deletes tags older than N days for a given GCR docker image. Lists the image:tags to be deleted and prompts for confirmation safety\n    - see also [cloudbuild.yaml](https://github.com/HariSekhon/Templates/blob/master/cloudbuild.yaml) in the [Templates](https://github.com/HariSekhon/Templates) repo\n  - CI/CD on GCP - trigger Google Cloud Build and GKE Kubernetes deployments from orthogonal CI/CD systems like Jenkins / TeamCity:\n    - `gcp_ci_build.sh` - script template for CI/CD to trigger Google Cloud Build to build docker container image with extra datetime and latest tagging\n    - `gcp_ci_deploy_k8s.sh` - script template for CI/CD to deploy GCR docker image to GKE Kubernetes using Kustomize\n  - `gce_*.sh` - Google [Compute Engine](https://cloud.google.com/compute/) scripts:\n    - `gce_foreach_vm.sh` - run a command for each GCP VM instance matching the given name/ip regex in the current GCP project\n    - `gce_host_ips.sh` - prints the IPs and hostnames of all or a regex match of GCE VMs for use in /etc/hosts\n    - `gce_ssh.sh` - Runs `gcloud compute ssh` to a VM while auto-determining its zone first to override any inherited zone config and make it easier to script iterating through VMs\n    - `gcs_ssh_keyscan.sh` - SSH keyscans all the GCE VMs returned from the above `gce_host_ips.sh` script and adds them to `~/.ssh/known_hosts`\n    - `gce_meta.sh` - simple script to query the GCE metadata API from within Virtual Machines\n    - `gce_when_preempted.sh` - GCE VM preemption latch script - can be executed any time to set one or more commands to execute upon preemption\n    - `gce_is_preempted.sh` - GCE VM return true/false if preempted, callable from other scripts\n    - `gce_instance_service_accounts.sh` - lists GCE VM instance names and their service accounts\n  - `gcp_firewall_disable_default_rules.sh` - disables those lax GCP default network \"allow all\" firewall rules\n  - `gcp_firewall_risky_rules.sh` - lists risky GCP firewall rules that are enabled and allow traffic from 0.0.0.0/0\n  - `gcp_sql_*.sh` - [Cloud SQL](https://cloud.google.com/sql) scripts:\n    - `gcp_sql_backup.sh` - creates Cloud SQL backups\n    - `gcp_sql_export.sh` - creates Cloud SQL exports to [GCS](https://cloud.google.com/storage)\n    - `gcp_sql_enable_automated_backups.sh` - enable automated daily Cloud SQL  backups\n    - `gcp_sql_enable_point_in_time_recovery.sh` - enable point-in-time recovery with write-ahead logs\n    - `gcp_sql_proxy.sh` - boots a [Cloud SQL Proxy](https://cloud.google.com/sql/docs/postgres/sql-proxy) to all Cloud SQL instances for fast convenient direct `psql` / `mysql` access via local sockets. Installs Cloud SQL Proxy if necessary\n    - `gcp_sql_running_primaries.sh` - lists primary running Cloud SQL instances\n    - `gcp_sql_service_accounts.sh` - lists Cloud SQL instance service accounts. Useful for copying to [IAM](https://cloud.google.com/iam) to grant permissions (eg. Storage Object Creator for SQL export backups to GCS)\n    - `gcp_sql_create_readonly_service_account.sh` - creates a service account with read-only permissions to Cloud SQL eg. to run export backups to GCS\n    - `gcp_sql_grant_instances_gcs_object_creator.sh` - grants minimal GCS objectCreator permission on a bucket to primary Cloud SQL instances for exports\n  - `gcp_cloud_schedule_sql_exports.sh` - creates Google [Cloud Scheduler](https://cloud.google.com/scheduler) jobs to trigger a [Cloud Function](https://cloud.google.com/functions) via [PubSub](https://cloud.google.com/pubsub) to run [Cloud SQL](https://cloud.google.com/sql) exports to [GCS](https://cloud.google.com/storage) for all [Cloud SQL](https://cloud.google.com/sql) instances in the current GCP project\n    - the Python [GCF](https://cloud.google.com/functions) function is in the [DevOps Python tools](https://github.com/HariSekhon/DevOps-Python-tools) repo\n  - `bigquery_*.sh` - [BigQuery](https://cloud.google.com/bigquery) scripts:\n    - `bigquery_list_datasets.sh` - lists BigQuery datasets in the current GCP project\n    - `bigquery_list_tables.sh` - lists BigQuery tables in a given dataset\n    - `bigquery_list_tables_all_datasets.sh` - lists tables for all datasets in the current GCP project\n    - `bigquery_foreach_dataset.sh` - executes a templated command for each dataset\n    - `bigquery_foreach_table.sh` - executes a templated command for each table in a given dataset\n    - `bigquery_foreach_table_all_datasets.sh` - executes a templated command for each table in each dataset in the current GCP project\n    - `bigquery_table_row_count.sh` - gets the row count for a given table\n    - `bigquery_tables_row_counts.sh` - gets the row counts for all tables in a given dataset\n    - `bigquery_tables_row_counts_all_datasets.sh` - gets the row counts for all tables in all datasets in the current GCP project\n    - `bigquery_generate_query_biggest_tables_across_datasets_by_row_count.sh` - generates a BigQuery SQL query to find the top 10 biggest tables by row count\n    - `bigquery_generate_query_biggest_tables_across_datasets_by_size.sh` - generates a BigQuery SQL query to find the top 10 biggest tables by size\n    - see also the [SQL Scripts](https://github.com/HariSekhon/SQL-scripts) repo for many more straight BigQuery SQL scripts\n  - GCP [IAM](https://cloud.google.com/iam) scripts:\n    - `gcp_service_account*.sh`:\n      - `gcp_service_account_credential_to_secret.sh` - creates GCP service account and exports a credential key to GCP Secret Manager (useful to stage or combine with `gcp_secrets_to_kubernetes.sh`)\n      - `gcp_service_accounts_credential_keys.sh` - lists all service account credential keys and expiry dates, can `grep 9999-12-31T23:59:59Z` to find non-expiring keys\n      - `gcp_service_accounts_credential_keys_age.sh` - lists all service account credential keys age in days\n      - `gcp_service_accounts_credential_keys_expired.sh` - lists expired service account credential keys that should be removed and recreated if needed\n      - `gcp_service_account_members.sh` - lists all members and roles authorized to use any service accounts. Useful for finding GKE Workload Identity mappings\n    - `gcp_iam_*.sh`:\n      - `gcp_iam_roles_in_use.sh` - lists GCP IAM roles in use in the current or all projects\n      - `gcp_iam_identities_in_use.sh` - lists GCP IAM identities (users/groups/serviceAccounts) in use in the current or all projects\n      - `gcp_iam_roles_granted_to_identity.sh` - lists GCP IAM roles granted to identities matching the regex (users/groups/serviceAccounts) in the current or all projects\n      - `gcp_iam_roles_granted_too_widely.sh` - lists GCP IAM roles which have been granted to allAuthenticatedUsers or even worse allUsers (unauthenticated) in one or all projects\n      - `gcp_iam_roles_with_direct_user_grants.sh` - lists GCP IAM roles which have been granted directly to users in violation of best-practice group-based management\n      - `gcp_iam_serviceaccount_members.sh` - lists members with permissions to use each GCP service account\n      - `gcp_iam_serviceaccounts_without_permissions.sh` - finds service accounts without IAM permissionns, useful to detect obsolete service accounts after a 90 day unused permissions clean out\n      - `gcp_iam_workload_identities.sh` - lists GKE Workload Identity integrations, uses `gcp_iam_serviceaccount_members.sh`\n      - `gcp_iam_users_granted_directly.sh` - lists GCP IAM users which have been granted roles directly in violation of best-practice group-based management\n  - `gcs_bucket_project.sh` - finds the GCP project that a given bucket belongs to using the GCP Storage API\n  - `gcs_curl_file.sh` - retrieves a GCS file's contents from a given bucket and path using the GCP Storage API. Useful for starting shell pipelines or being called from other scripts\n\nSee also [Knowledge Base notes for GCP](https://github.com/HariSekhon/Knowledge-Base/blob/main/gcp.md).\n\n### Kubernetes\n\n`kubernetes/` directory:\n\n- `.envrc-kubernetes` - copy to `.envrc` for [direnv](https://direnv.net/) to auto-load the right Kubernetes `kubectl` context isolated to current shell to prevent race conditions between shells and scripts caused by otherwise naively changing the global `~/.kube/config` context\n- `aws/eksctl_cluster.sh` - quickly spins up an [AWS EKS](https://aws.amazon.com/eks/) cluster using `eksctl` with some sensible defaults\n- `kubernetes_info.sh` - huge [Kubernetes](https://kubernetes.io/) inventory listing of deployed resources across all namespaces in the current cluster / kube context:\n  - cluster-info\n  - master component statuses\n  - nodes\n  - namespaces\n  - deployments, replicasets, replication controllers, statefulsets, daemonsets, horizontal pod autoscalers\n  - storage classes, persistent volumes, persistent volume claims\n  - service accounts, resource quotas, network policies, pod security policies\n  - container images running\n  - container images running counts descending\n  - pods  (might be too much detail if you have high replica counts, so done last, comment if you're sure nobody has deployed pods outside deployments)\n- `kubectl.sh` - runs kubectl commands safely fixed to a given context using config isolation to avoid concurrency race conditions\n- `kubectl_diff_apply.sh` - generates a kubectl diff and prompts to apply\n- `kustomize_diff_apply.sh` - runs Kustomize build, precreates any namespaces, shows a kubectl diff of the proposed changes, and prompts to apply\n- `kustomize_diff_branch.sh` - runs Kustomize build against the current and target base branch for current or all given directories, then shows the diff for each directory. Useful to detect differences when refactoring, such as switching to tagged bases\n- `kubectl_create_namespaces.sh` - creates any namespaces in yaml files or stdin, a prerequisite for a diff on a blank install, used by adjacent scripts for safety\n- `kubernetes_check_objects_namespaced.sh` - checks Kubernetes yaml(s) for objects which aren't explicitly namespaced, which can easily result in deployments to the wrong namespace. Reads the API resources from your current Kubernetes cluster and if successful excludes cluster-wide objects\n- `kustomize_check_objects_namespaced.sh` - checks Kustomize build yaml output for objects which aren't explicitly namespaced (uses above script)\n- `kubectl_deployment_pods.sh` - gets the pod names with their unpredictable suffixes for a given deployment by querying the deployment's selector labels and then querying pods that match those labels\n- `kubectl_get_all.sh` - finds all namespaced Kubernetes objects and requests them for the current or given namespace. Useful because `kubectl get all` misses a lof of object types\n- `kubectl_get_annotation.sh` - find a type of object with a given annotation\n- `kubectl_restart.sh` - restarts all or filtered deployments/statefulsets in the current or given namespace. Useful when debugging or clearing application problems\n- `kubectl_logs.sh` - tails all containers in all pods or filtered pods in the current or given namespace. Useful when debugging a distributed set of pods in live testing\n- `kubectl_kv_to_secret.sh` - creates a Kuberbetes secret from `key=value` or shell export format, as args or via stdin (eg. piped from `aws_csv_creds.sh`)\n- `kubectl_secret_values.sh` - prints the keys and base64 decoded values within a given Kubernetes secret for quick debugging of Kubernetes secrets. See also: `gcp_secrets_to_kubernetes.sh`\n- `kubectl_secrets_download.sh` - downloads all secrets in current or given namespace to local files of the same name, useful as a backup before migrating to Sealed Secrets\n- `kubernetes_secrets_compare_gcp_secret_manager.sh` - compares each Kubernetes secret to the corresponding secret in GCP Secret Manager. Useful to safety check GCP Secret Manager values align before enabling [External Secrets](https://external-secrets.io/latest/) to replace them\n- `kubernetes_secret_to_external_secret.sh` - generates an [External Secret](https://external-secrets.io/latest/) from an existing Kubernetes secret\n- `kubernetes_secrets_to_external_secrets.sh` - generates [External Secrets](https://external-secrets.io/latest/) from all existing Kubernetes secrets found in the current or given namespace\n- `kubernetes_secret_to_sealed_secret.sh` - generates a [Bitnami Sealed Secret](https://github.com/bitnami-labs/sealed-secrets) from an existing Kubernetes secret\n- `kubernetes_secrets_to_sealed_secrets.sh` - generates [Bitnami Sealed Secrets](https://github.com/bitnami-labs/sealed-secrets) from all existing Kubernetes secrets found in the current or given namespace\n- `kubectl_secrets_annotate_to_be_sealed.sh` - annotates secrets in current or given namespace to allow being overwritten by Sealed Secrets (useful to sync ArgoCD health)\n- `kubectl_secrets_not_sealed.sh` - finds secrets with no SealedSecret ownerReferences\n- `kubectl_secrets_to_be_sealed.sh` - finds secrets pending overwrite by Sealed Secrets with the managed annotation\n- `kubernetes_foreach_context.sh` - executes a command across all kubectl contexts, replacing `{context}` in each iteration (skips lab contexts `docker` / `minikube` / `minishift` to avoid hangs since they're often offline)\n- `kubernetes_foreach_namespace.sh` - executes a command across all kubernetes namespaces in the current cluster context, replacing `{namespace}` in each iteration\n  - Can be chained with `kubernetes_foreach_context.sh` and useful when combined with `gcp_secrets_to_kubernetes.sh` to load all secrets from GCP to Kubernetes for the current cluster, or combined with `gke_kube_creds.sh` and `kubernetes_foreach_context.sh` for all clusters!\n- `kubernetes_api.sh` - finds Kubernetes API and runs your curl arguments against it, auto-getting authorization token and auto-populating OAuth authentication header\n- `kubernetes_autoscaler_release.sh` - finds the latest Kubernetes Autoscaler release that matches your local Kubernetes cluster version using kubectl and the GitHub API. Useful for quickly finding the image override version for `eks-cluster-autoscaler-kustomization.yaml` in the [Kubernetes configs](https://github.com/HariSekhon/Kubernetes-configs) repo\n- `kubernetes_etcd_backup.sh` - creates a timestamped backup of the Kubernetes Etcd database for a kubeadm cluster\n- `kubernetes_delete_stuck_namespace.sh` - to forcibly delete those pesky kubernetes namespaces of 3rd party apps like Knative that get stuck and hang indefinitely on the finalizers during deletion\n- `kubeadm_join_cmd.sh` - outputs `kubeadm join` command (generates new token) to join an existing Kubernetes cluster (used in [vagrant kubernetes](https://github.com/HariSekhon/DevOps-Bash-tools/tree/master/vagrant/kubernetes) provisioning scripts)\n- `kubeadm_join_cmd2.sh` - outputs `kubeadm join` command manually (calculates cert hash + generates new token) to join an existing Kubernetes cluster\n- `kubernetes_nodes_ssh_dump_logs.sh` - fetch logs from Kubernetes nodes (eg. for support debug requests by vendors)\n- `kubectl_exec.sh` - finds and execs to the first Kubernetes pod matching the given name regex, optionally specifying the container name regex to exec to, and shows the full generated `kubectl exec` command line for clarity\n- `kubectl_exec2.sh` - finds and execs to the first Kubernetes pod matching given pod filters, optionally specifying the container to exec to, and shows the full generated `kubectl exec` command line for clarity\n- `kubectl_pods_per_node.sh` - lists number of pods per node sorted descending\n- `kubectl_pods_important.sh` - lists important pods and their nodes to check on scheduling\n- `kubectl_pods_colocated.sh` - lists pods from deployments/statefulsets that are colocated on the same node\n- `kubectl_node_labels.sh` - lists nodes and their labels, one per line, easier to read visually or pipe in scripting\n- `kubectl_pods_running_with_labels.sh` - lists running pods with labels matching key=value pair arguments\n- `kubectl_node_taints.sh` - lists nodes and their taints\n- `kubectl_jobs_stuck.sh` - finds Kubernetes jobs stuck for hours or days with no completions\n- `kubectl_jobs_delete_stuck.sh` - prompts for confirmation to delete stuck Kubernetes jobs found by script above\n- `kubectl_images.sh` - lists Kubernetes container images running on the current cluster\n- `kubectl_image_counts.sh` - lists Kubernetes container images running counts sorted descending\n- `kubectl_image_deployments.sh` - lists which deployments, statefulsets or daemonsets container images belong to. Useful to find which deployment, statefulset or daemonset to upgrade to replace a container image eg. when replacing deprecated the k8s.gcr.io registry with registry.k8s.io\n- `kubectl_pod_count.sh` - lists Kubernetes pods total running count\n- `kubectl_pod_labels.sh` - lists Kubernetes pods and their labels, one label per line for easier shell script piping for further actions\n- `kubectl_pod_ips.sh` - lists Kubernetes pods and their pod IP addresses\n- `kubectl_container_count.sh` - lists Kubernetes containers total running count\n- `kubectl_container_counts.sh` - lists Kubernetes containers running counts by name sorted descending\n- `kubectl_pods_dump_*.sh` - dump stats / logs / jstacks from all pods matching a given regex and namespace to txt files for support debugging\n  - `kubectl_pods_dump_stats.sh` - dump stats\n  - `kubectl_pods_dump_logs.sh` - dump logs\n  - `kubectl_pods_dump_jstacks.sh` - dump Java jstacks\n  - `kubectl_pods_dump_all.sh` - calls the above `kubectl_pods_dump_*.sh` scripts for N iterations with a given interval\n- `kubectl_empty_namespaces.sh` - finds namespaces without any of the usual objects using `kubectl get all`\n- `kubectl_delete_empty_namespaces.sh` - removes empty namespaces, uses `kubectl_empty_namespaces.sh`\n- `kubectl_<image>.sh` - quick launch one-off pods for interactive debuggging in Kubernetes\n  - `kubectl_alpine.sh`\n  - `kubectl_busybox.sh`\n  - `kubectl_curl.sh`\n  - `kubectl_dnsutils.sh`\n  - `kubectl_gcloud_sdk.sh`\n  - `kubectl_run_sa.sh` - launch a quick pod with the given service account to test private repo pull & other permissions\n- `kubectl_port_forward.sh` - launches `kubectl port-forward` to a given pod's port with an optional label or name filter. If more than one pod is found, prompts with an interactive dialogue to choose one. Optionally automatically opens the forwarded localhost URL in the default browser\n  - `kubectl_port_forward_spark.sh` - does the above for Spark UI\n- `helm_template.sh` - templates a Helm chart for Kustomize deployments\n- `kustomize_parse_helm_charts.sh` - parses the [Helm](https://helm.sh/) charts from one or more `kustomization.yaml` files into TSV format for further shell pipe processing\n- `kustomize_install_helm_charts.sh` - installs the [Helm](https://helm.sh/) charts from one or more `kustomization.yaml` files the old fashioned Helm CLI way so that tools like [Nova](https://github.com/FairwindsOps/nova) can be used to detect outdated charts (used in [Kubernetes-configs](https://github.com/HariSekhon/Kubernetes-configs) repo's [CI](https://github.com/HariSekhon/Kubernetes-configs/actions/workflows/nova.yaml))\n- `kustomize_update_helm_chart_versions.sh` - updates one or more `kustomization.yaml` files to the latest versions of any charts they contain\n- `kustomize_materialize.sh` - recursively materializes all `kustomization.yaml` to `kustomization.materialized.yaml` in the same directories for scanning with tools like [Pluto](https://github.com/FairwindsOps/pluto) to detect deprecated API objects inherited from embedded Helm charts. Parallelized for performance\n- ArgoCD:\n  - `argocd_auto_sync.sh` - toggle Auto-sync on/off to allow repairs and maintenance operation for a given app and also disables / re-enables the App-of-Apps base apps to stop then re-enabling the app\n  - `argocd_apps_sync.sh` - sync's all [ArgoCD](https://argo-cd.readthedocs.io/en/stable/) apps matching an optional ERE regex filter on their names using the ArgoCD CLI\n  - `argocd_apps_wait_sync.sh` - sync's all [ArgoCD](https://argo-cd.readthedocs.io/en/stable/) apps matching an optional ERE regex filter on their names using the ArgoCD CLI's while also checking their health and operation\n  - `argocd_generate_resource_whitelist.sh` - generates a yaml cluster and namespace resource whitelist for ArgoCD project config. If given an existing yaml, will merge in its original whitelists, dedupe, and write them back into the file using an in-place edit. Useful because ArgoCD 2.2+ doesn't show resources that aren't explicitly allowed, such as ReplicaSets and Pods\n- Pluto:\n  - `pluto_detect_helm_materialize.sh` - recursively materializes all helm `Chart.yaml` and runs [Pluto](https://github.com/FairwindsOps/pluto) on each directory to work around [this issue](https://github.com/FairwindsOps/pluto/issues/444)\n  - `pluto_detect_kustomize_materialize.sh` - recursively materializes all `kustomization.yaml` and runs [Pluto](https://github.com/FairwindsOps/pluto) on each directory to work around [this issue](https://github.com/FairwindsOps/pluto/issues/444)\n  - `pluto_detect_kubectl_dump_objects.sh` - dumps all live Kubernetes objects to /tmp and runs [Pluto](https://github.com/FairwindsOps/pluto) to detect deprecated API objects on the cluster from any source\n- Rancher:\n  - `rancher_api.sh` - queries the Rancher API with authentication\n  - `rancher_kube_creds.sh` - downloads all Rancher clusters credentials into subdirectories matching cluster names, with `.envrc` in each, so a quick `cd` into one and your kubectl is ready to rock\n- see also Google Kubernetes Engine scripts in the [GCP - Google Cloud Platform](https://github.com/HariSekhon/DevOps-Bash-tools/#gcp---google-cloud-platform) section above\n- see also the [Kubernetes configs](https://github.com/HariSekhon/Kubernetes-configs) repo\n\nSee also [Knowledge Base notes for Kubernetes](https://github.com/HariSekhon/Knowledge-Base/blob/main/kubernetes.md).\n\n### Docker\n\n`docker/` directory:\n\n- `docker_*.sh` / `dockerhub_*.sh` - [Docker](https://www.docker.com/) / [DockerHub](https://hub.docker.com/) API scripts:\n  - `dockerhub_api.sh` - queries DockerHub API v2 with or without authentication (`$DOCKERHUB_USER` & `$DOCKERHUB_PASSWORD` / `$DOCKERHUB_TOKEN`)\n  - `docker_api.sh` - queries a Docker Registry with optional basic authentication if `$DOCKER_USER` & `$DOCKER_PASSWORD` are set\n  - `docker_build_hashref.sh` - runs `docker build` and auto-generates docker image name and tag from relative Git path and commit short SHA hashref and a dirty sha suffix if git contents are modified. Useful to compare docker image sizes between your clean and modified versions of `Dockerfile` or contents\n  - `docker_package_check.sh` - runs package installs on major versions of a docker image to check given packages are available before adding them and breaking builds across linux distro versions\n  - `docker_registry_list_images.sh` - lists images in a given private Docker Registry\n  - `docker_registry_list_tags.sh` - lists tags for a given image in a private Docker Registry\n  - `docker_registry_get_image_manifest.sh` - gets a given image:tag manifest from a private Docker Registry\n  - `docker_registry_tag_image.sh` - tags a given image with a new tag in a private Docker Registry via the API without pulling and pushing the image data (must faster and more efficient)\n  - `dockerhub_list_tags.sh` - lists tags for a given DockerHub repo. See also [dockerhub_show_tags.py](https://github.com/HariSekhon/DevOps-Python-tools/blob/master/dockerhub_show_tags.py) in the [DevOps Python tools](https://github.com/HariSekhon/DevOps-Python-tools) repo.\n  - `dockerhub_list_tags_by_last_updated.sh` - lists tags for a given DockerHub repo sorted by last updated timestamp descending\n  - `dockerhub_search.sh` - searches with a configurable number of returned items (older docker cli was limited to 25 results)\n  - `clean_caches.sh` - cleans out OS package and programming language caches, call near end of `Dockerfile` to reduce Docker image size\n  - see also the [Dockerfiles](https://github.com/HariSekhon/Dockerfiles) repo\n- `quay_api.sh` - queries the [Quay.io](https://quay.io/) API with OAuth2 authentication token `$QUAY_TOKEN`\n\nSee also [Knowledge Base notes for Docker](https://github.com/HariSekhon/Knowledge-Base/blob/main/docker.md).\n\n### Data\n\n`data/` directory:\n\n- `avro_tools.sh` - runs Avro Tools jar, downloading it if not already present (determines latest version when\n  downloading)\n- `parquet_tools.sh` - runs Parquet Tools jar, downloading it if not already present (determines latest version\n  when downloading)\n- `csv_header_indices.sh` - list CSV headers with their zero indexed numbers, useful reference when coding against\n  column positions\n- `ini_config_add_if_missing.sh` - reads INI config blocks from stdin and appends them to the specified file if the section is not found. Used by `aws_profile_config_add_if_missing.sh`\n- `ini_config_duplicate_sections.sh` - lists duplicate INI config sections that are using the same value for a given key in the given .ini file\n- `ini_config_duplicate_section_names.sh` - lists duplicate INI config section names that are using the same value for a given key in the given .ini file\n- `ini_grep_section.sh` - prints the named section from a given .ini file to stdout\n- `wordcount.sh` - counts and ranks words by their frequency in file(s) or stdin\n- Data format validation `validate_*.py` from [DevOps Python Tools repo](https://github.com/HariSekhon/DevOps-Python-tools):\n\n  - CSV\n  - JSON\n  - [Avro](https://avro.apache.org/)\n  - [Parquet](https://parquet.apache.org/)\n  - INI / Properties files (Java)\n  - LDAP LDIF\n  - XML\n  - YAML\n\n- `json2yaml.sh` - converts JSON to YAML\n- `yaml2json.sh` - converts YAML to JSON - needed for some APIs like GitLab CI linting\n  (see [Gitlab](#git---github-gitlab-bitbucket-azure-devops) section above)\n\n### Big Data & NoSQL\n\n`bigdata/` and `kafka/` directories:\n\n- `kafka_*.sh` - scripts to make [Kafka](http://kafka.apache.org/) CLI usage easier including auto-setting Kerberos to source TGT from environment and auto-populating broker and zookeeper addresses. These are auto-added to the `$PATH` when `.bashrc` is sourced. For something similar for [Solr](https://lucene.apache.org/solr/), see `solr_cli.pl` in the [DevOps Perl Tools](https://github.com/HariSekhon/DevOps-Perl-tools) repo.\n- `zookeeper*.sh` - [Apache ZooKeeper](https://zookeeper.apache.org/) scripts:\n  - `zookeeper_client.sh` - shortens `zookeeper-client` command by auto-populating the zookeeper quorum from the environment variable `$ZOOKEEPERS` or else parsing the zookeeper quorum from `/etc/**/*-site.xml` to make it faster and easier to connect\n  - `zookeeper_shell.sh` - shortens Kafka's `zookeeper-shell` command by auto-populating the zookeeper quorum from the environment variable `$KAFKA_ZOOKEEPERS` and optionally `$KAFKA_ZOOKEEPER_ROOT` to make it faster and easier to connect\n- `hive_*.sh` / `beeline*.sh` - [Apache Hive](https://hive.apache.org/) scripts:\n  - `beeline.sh` - shortens `beeline` command to connect to [HiveServer2](https://cwiki.apache.org/confluence/display/Hive/HiveServer2+Overview) by auto-populating Kerberos and SSL settings, zookeepers for HiveServer2 HA discovery if the environment variable `$HIVE_HA` is set or using the `$HIVESERVER_HOST` environment variable so you can connect with no arguments (prompts for HiveServer2 address if you haven't set `$HIVESERVER_HOST` or `$HIVE_HA`)\n    - `beeline_zk.sh` - same as above for [HiveServer2](https://cwiki.apache.org/confluence/display/Hive/HiveServer2+Overview) HA by auto-populating SSL and ZooKeeper service discovery settings (specify `$HIVE_ZOOKEEPERS` environment variable to override). Automatically called by `beeline.sh` if either `$HIVE_ZOOKEEPERS` or `$HIVE_HA` is set (the latter parses `hive-site.xml` for the ZooKeeper addresses)\n  - `hive_foreach_table.sh` - executes a SQL query against every table, replacing `{db}` and `{table}` in each iteration eg. `select count(*) from {table}`\n  - `hive_list_databases.sh` - list Hive databases, one per line, suitable for scripting pipelines\n  - `hive_list_tables.sh` - list Hive tables, one per line, suitable for scripting pipelines\n  - `hive_tables_metadata.sh` - lists a given DDL metadata field for each Hive table (to compare tables)\n  - `hive_tables_location.sh` - lists the data location per Hive table (eg. compare external table locations)\n  - `hive_tables_row_counts.sh` - lists the row count per Hive table\n  - `hive_tables_column_counts.sh` - lists the column count per Hive table\n- ` impala*.sh` - [Apache Impala](https://impala.apache.org/) scripts:\n  - `impala_shell.sh` - shortens `impala-shell` command to connect to [Impala](https://impala.apache.org/) by parsing the Hadoop topology map and selecting a random datanode to connect to its Impalad, acting as a cheap CLI load balancer. For a real load balancer see [HAProxy config for Impala](https://github.com/HariSekhon/HAProxy-configs) (and many other Big Data & NoSQL technologies). Optional environment variables `$IMPALA_HOST` (eg. point to an explicit node or an HAProxy load balancer) and `IMPALA_SSL=1` (or use regular impala-shell `--ssl` argument pass through)\n  - `impala_foreach_table.sh` - executes a SQL query against every table, replacing `{db}` and `{table}` in each iteration eg. `select count(*) from {table}`\n  - `impala_list_databases.sh` - list Impala databases, one per line, suitable for scripting pipelines\n  - `impala_list_tables.sh` - list Impala tables, one per line, suitable for scripting pipelines\n  - `impala_tables_metadata.sh` - lists a given DDL metadata field for each Impala table (to compare tables)\n  - `impala_tables_location.sh` - lists the data location per Impala table (eg. compare external table locations)\n  - `impala_tables_row_counts.sh` - lists the row count per Impala table\n  - `impala_tables_column_counts.sh` - lists the column count per Impala table\n- `hdfs_*.sh` - Hadoop [HDFS](https://en.wikipedia.org/wiki/Apache_Hadoop#Hadoop_distributed_file_system) scripts:\n  - `hdfs_checksum*.sh` - walks an HDFS directory tree and outputs HDFS native checksums (faster) or portable externally comparable CRC32, in serial or in parallel to save time\n  - `hdfs_find_replication_factor_1.sh` / `hdfs_set_replication_factor_3.sh` - finds HDFS files with replication factor 1 / sets HDFS files with replication factor <=2 to replication factor 3 to repair replication safety and avoid no replica alarms during maintenance operations (see also Python API version in the [DevOps Python Tools](https://github.com/HariSekhon/DevOps-Python-tools) repo)\n  - `hdfs_file_size.sh` / `hdfs_file_size_including_replicas.sh` - quickly differentiate HDFS files raw size vs total replicated size\n  - `hadoop_random_node.sh` - picks a random Hadoop cluster worker node, like a cheap CLI load balancer, useful in scripts when you want to connect to any worker etc. See also the read [HAProxy Load Balancer configurations](https://github.com/HariSekhon/HAProxy-configs) which focuses on master nodes\n- `cloudera_*.sh` - [Cloudera](https://www.cloudera.com/) scripts:\n  - `cloudera_manager_api.sh` - script to simplify querying [Cloudera Manager](https://www.cloudera.com/products/product-components/cloudera-manager.html) API using environment variables, prompts, authentication and sensible defaults. Built on top of `curl_auth.sh`\n  - `cloudera_manager_impala_queries*.sh` - queries [Cloudera Manager](https://www.cloudera.com/products/product-components/cloudera-manager.html) for recent [Impala](https://impala.apache.org/) queries, failed queries, exceptions, DDL statements, metadata stale errors, metadata refresh calls etc. Built on top of `cloudera_manager_api.sh`\n  - `cloudera_manager_yarn_apps.sh` - queries [Cloudera Manager](https://www.cloudera.com/products/product-components/cloudera-manager.html) for recent [Yarn](https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html) apps. Built on top of `cloudera_manager_api.sh`\n  - `cloudera_navigator_api.sh` - script to simplify querying [Cloudera Navigator](https://www.cloudera.com/products/product-components/cloudera-navigator.html) API using environment variables, prompts, authentication and sensible defaults. Built on top of `curl_auth.sh`\n  - `cloudera_navigator_audit_logs.sh` - fetches [Cloudera Navigator](https://www.cloudera.com/products/product-components/cloudera-navigator.html) audit logs for given service eg. hive/impala/hdfs via the API, simplifying date handling, authentication and common settings. Built on top of `cloudera_navigator_api.sh`\n  - `cloudera_navigator_audit_logs_download.sh` - downloads [Cloudera Navigator](https://www.cloudera.com/products/product-components/cloudera-navigator.html) audit logs for each service by year. Skips existing logs, deletes partially downloaded logs on failure, generally retry safe (while true, Control-C, not `kill -9` obviously). Built on top of `cloudera_navigator_audit_logs.sh`\n\nSee also [Knowledge Base notes for Hadoop](https://github.com/HariSekhon/Knowledge-Base/blob/main/hadoop.md).\n\n### Git - GitHub, GitLab, Bitbucket, Azure DevOps\n\n`git/`, `github/`, `gitlab/`, `bitbucket/` and `azure_devops/` directories:\n\n- `git/*.sh` - [Git](https://git-scm.com/) scripts:\n  - `precommit_run_changed_files.sh` - runs pre-commit on all files changed on the current branch vs the default branch. Useful to reproduce `pre-commit` checks that are failing in pull requests to get your PRs to pass\n  - `git_diff_commit.sh` - quickly commits added or updated files to Git, showing a diff and easy enter prompt for each file. Super convenient for fast commits on the command line, and in vim and IDEs via hotkeys\n  - `git_review_push.sh` - shows diff of what would be pushed upstream and prompts to push. Convenient for fast reviewed pushes via vim or IDEs hotkeys\n  - `git_branch_delete_squash_merged.sh` - carefully detects if a squash merged branch you want to delete has no changes with the default trunk branch before deleting it.\n     See [Squash Merges](https://github.com/HariSekhon/Knowledge-Base/blob/main/git.md#squash-merges-require-force-deleting-branches) in knowledge-base about why this is necessary.\n  - `git_tag_release.sh` - creates a Git tag, auto-incrementing a `.N` suffix on the year/month/day date format if no exact version given\n  - `git_foreach_branch.sh` - executes a command on all branches (useful in heavily version branched repos like in my [Dockerfiles](https://github.com/HariSekhon/Dockerfiles) repo)\n  - `git_foreach_repo.sh` - executes a command against all adjacent repos from a given repolist (used heavily by many adjacent scripts)\n  - `git_foreach_modified.sh` - executes a command against each file with git modified status\n  - `git_foreach_repo_replace_readme_actions.sh` - updates the `README.md` badges for GitHub Actions to match the local repo name. Useful to bulk fix copied badges quickly and easily\n  - `git_foreach_repo_update_readme.sh` - git-diff-commits the `README.md` for each Git repo checkout using adjacent `git_foreach_repo.sh` and `git_diff_commit.sh` scripts. Useful to quickly bulk update `README.md` in all your projects, such as when references need updating\n  - `git_merge_all.sh` / `git_merge_master.sh` / `git_merge_master_pull.sh` - merges updates from master branch to all other branches to avoid drift on longer lived feature branches / version branches (eg. [Dockerfiles](https://github.com/HariSekhon/Dockerfiles) repo)\n  - `git_remotes_add_origin_providers.sh` - auto-creates remotes for the 4 major public repositories ([GitHub](https://github.com/)/[GitLab](https://gitlab.com/)/[Bitbucket](https://bitbucket.org)/[Azure DevOps](https://dev.azure.com/)), useful for `git pull -all` to fetch and merge updates from all providers in one command\n  - `git_remotes_set_multi_origin.sh` - sets up multi-remote origin for unified push to automatically keep the 4 major public repositories in sync (especially useful for [Bitbucket](https://bitbucket.org) and [Azure DevOps](https://dev.azure.com/) which don't have [GitLab](https://gitlab.com/)'s auto-mirroring from [GitHub](https://github.com/) feature)\n  - `git_remotes_set_https_to_ssh.sh` - converts local repo's remote URLs from https to ssh (more convenient with SSH keys instead of https auth tokens, especially since Azure DevOps expires personal access tokens every year)\n  - `git_remotes_set_ssh_to_https.sh` - converts local repo's remote URLs from ssh to https (to get through corporate firewalls or hotels if you travel a lot)\n  - `git_remotes_set_https_creds_helpers.sh` - adds Git credential helpers configuration to the local git repo to use http API tokens dynamically from environment variables if they're set\n  - `git_repos_pull.sh` - pull multiple repos based on a source file mapping list - useful for easily sync'ing lots of Git repos among computers\n  - `git_repos_update.sh` - same as above but also runs the `make update` build to install the latest dependencies, leverages the above script\n  - `git_grep_env_vars.sh` - find environment variables in the current git repo's code base in the format `SOME_VAR` (useful to find undocumented environment variables in internal or open source projects such as ArgoCD eg. [argoproj/argocd-cd #8680](https://github.com/argoproj/argo-cd/pull/8680))\n  - `git_log_empty_commits.sh` - find empty commits in git history (eg. if a `git filter-branch` was run but `--prune-empty` was forgotten, leaking metadata like subjects containing file names or other sensitive info)\n  - `git_graph_commit_history_gnuplot.sh` - generates GNUplot graphs of Git commits per year and per month for the entire history of the local Git repo checkout\n  - `git_graph_commit_history_mermaidjs.sh` - generates MermaidJS graphs of Git commits per year and per month for the entire history of the local Git repo checkout\n  - `git_graph_commit_times_gnuplot.sh` - generates a GNUplot graph of Git commit times from the current Git repo checkout's `git log`\n  - `git_graph_commit_times_mermaidjs.sh` - generates a MermaidJS graph of Git commit times from the current Git repo checkout's `git log`\n  - `git_graph_commit_times_gnuplot_all_repos.sh` - generates GNUplot graph of the GitHub commit times from all local adjacent Git repo checkouts listed in `setup/repos.txt` using Git log in each checkout\n  - `git_graph_commit_times_mermaidjs_all_repos.sh` - generates MermaidJS graph of the GitHub commit times from all local adjacent Git repo checkouts listed in `setup/repos.txt` using Git log in each checkout\n  - `git_revert_line.sh` - reverts the first line that matches a given regex from the Git head commit's version of the same line number. Useful to revert some changes caused by over zealous sed'ing scripts, where you want to cherry-pick revert a single line change\n  - `git_files_in_history.sh` - finds all filename / file paths in the git log history, useful for prepping for `git filter-branch`\n  - `git_filter_branch_fix_author.sh` - rewrites Git history to replace author/committer name & email references (useful to replace default account commits). Powerful, read `--help` and `man git-filter-branch` carefully. Should only be used by Git Experts\n  - `git_filter_repo_replace_text.sh` - rewrites Git history to replace a given text to scrub a credential or other sensitive token from history. Refuses to operate on tokens less than 8 chars for safety\n  - `git_submodules_update_repos.sh` - updates submodules (pulls and commits latest upstream github repo submodules) - used to cascade submodule updates throughout all my repos\n  - `git_askpass.sh` - credential helper script to use environment variables for git authentication\n  - `markdown_generate_index.sh` - generates a markdown index list from the headings in a given markdown file such as README.md\n  - `markdown_replace_index.sh` - replaces a markdown index section in a given markdown file using `markdown_generate_index.sh`\n- `github/*.sh` - [GitHub](https://github.com/) API / CLI scripts:\n  - `github_api.sh` - queries the GitHub [API](https://docs.github.com/en/rest/reference). Can infer GitHub user, repo and authentication token from local checkout or environment (`$GITHUB_USER`, `$GITHUB_TOKEN`)\n  - `github_install_binary.sh` - installs a binary from GitHub releases into $HOME/bin or /usr/local/bin. Auto-determines the latest release if no version specified, detects and unpacks any tarball or zip files\n  - `github_foreach_repo.sh` - executes a templated command for each non-fork GitHub repo, replacing the `{owner}`/`{name}` or `{repo}` placeholders in each iteration\n  - `github_graph_commit_times_gnuplot.sh` - generates GNUplot graph of GitHub commit times from all public GitHub repos for a given user. Fetches the commit data via the GitHub API\n  - `github_graph_commit_times_mermaidjs.sh` - generates MermaidJS graph of the GitHub commit times from all public GitHub repos for a given user. Fetches the commit data via the GitHub API\n  - `github_clone_or_pull_all_repos.sh` - git clones or pulls all repos for a user or organization into directories of the same name under the current directory\n  - `github_download_release_file.sh` - downloads a file from GitHub Releases, optionally determining the latest version, uses `bin/download_url_file.sh`\n  - `github_download_release_jar.sh` - downloads a JAR file from GitHub Releases (used by `install/download_*_jar.sh` for things like [JDBC](https://github.com/HariSekhon/Knowledge-Base/blob/main/jdbc.md) drivers or [Java](#java) [decompilers](https://github.com/HariSekhon/Knowledge-Base/blob/main/java.md#java-decompilers)), optionally determines latest version to download, and finally validates the downloaded file's format\n  - `github_invitations.sh` - lists / accepts repo invitations. Useful to accept a large number of invites to repos generated by automation\n  - `github_mirror_repos_to_gitlab.sh` - creates/syncs GitHub repos to GitLab for migrations or to cron fast free Disaster Recovery, including all branches and tags, plus the repo descriptions. Note this doesn't include PRs/wikis/releases\n  - `github_mirror_repos_to_bitbucket.sh` - creates/syncs GitHub repos to BitBucket for migrations or to cron fast free Disaster Recovery, including all branches and tags, plus the repo descriptions. Note this doesn't include PRs/wikis/releases\n  - `github_mirror_repos_to_aws_codecommit.sh` - creates/syncs GitHub repos to AWS CodeCommit for migrations or to cron fast almost free Disaster Recovery (close to $0 compared to $100-400+ per month for [Rewind BackHub](https://rewind.com/products/backups/github/)), including all branches and tags, plus the repo descriptions. Note this doesn't include PRs/wikis/releases\n  - `github_mirror_repos_to_gcp_source_repos.sh` - creates/syncs GitHub repos to GCP Source Repos for migrations or to cron fast almost free Disaster Recovery (close to $0 compared to $100-400+ per month for [Rewind BackHub](https://rewind.com/products/backups/github/)), including all branches and tags. Note this doesn't include repo description/PRs/wikis/releases\n  - `github_pull_request_create.sh` - creates a Pull Request idempotently by first checking for an existing PR between the branches, and also checking if there are the necessary commits between the branches, to avoid common errors from blindly raising PRs. Useful to automate code promotion across environment branches. Also works across repo forks and is used by `github_repo_fork_update.sh`. Even populates github pull request template and does Jira ticket number replacement from branch prefix\n  - `github_pull_request_preview.sh` - opens a GitHub Pull Request preview page from the current local branch to the given or default branch\n  - `github_push_pr_preview.sh` - pushes to GitHub origin, sets upstream branch, then open a Pull Request preview from current branch to the given or default trunk branch in your browser\n  - `github_push_pr.sh` - pushes to GitHub origin, sets upstream branch, then idemopotently creates a Pull Request from current branch to the given or default trunk branch and opens the generated PR in your browser for review\n  - `github_merge_branch.sh` - merges one branch into another branch via a Pull Request for full audit tracking all changes. Useful to automate feature PRs, code promotion across environment branches, or backport hotfixes from Production or Staging to trunk branches such as master, main, dev or develop\n  - `github_remote_set_upstream.sh` - in a forked GitHub repo's checkout, determine the origin of the fork using GitHub CLI and configure a git remote to the upstream. Useful to be able to easily pull updates from the original source repo\n  - `github_pull_merge_trunk.sh` - pulls the origin or fork upstream repo's trunk branch and merges it into the local branch, In a forked GitHub repo's checkout, determines the origin of the fork using GitHub CLI, configures a git remote to the upstream, pulls the default branch and if on a branch other than the default then merges the default branch to the local current branch. Simplifies and automates keeping your checkout or forked repo up to date with the original source repo to quickly resolve merge conflicts locally and submit updated Pull Requests\n  - `github_forked_add_remote.sh` - quickly adds a forked repo as a remote from an interactive men list of forked repos\n  - `github_forked_checkout_branch.sh` - quickly check out a forked repo's branch from an interactive menu lists of forked repos and their branches\n  - `github_tag_hashref.sh` - Returns the GitHub commit hashref for a given GitHub Actions `owner/repo@tag` or `https://github.com/owner/repo@tag`. Useful for pinning 3rd party GitHub Actions to hashref instead of tag to follow [GitHub Actions Best Practices](https://github.com/HariSekhon/Knowledge-Base/blob/main/github-actions.md#github-actions-best-practices)\n  - `github_actions_foreach_workflow.sh` - executes a templated command for each workflow in a given GitHub repo, replacing `{name}`, `{id}` and `{state}` in each iteration\n  - `github_actions_aws_create_load_credential.sh` - creates an AWS user with group/policy, generates and downloads access keys, and uploads them to the given repo\n  - `github_actions_in_use.sh` - lists GitHub Actions directly referenced in the .github/workflows in the current local repo checkout\n  - `github_actions_in_use_repo.sh` - lists GitHub Actions for a given repo via the API, including following imported reusable workflows\n  - `github_actions_in_use_across_repos.sh` - lists GitHub Actions in use across all your repos\n  - `github_actions_repos_lockdown.sh` - secures GitHub Actions settings across all user repos to only GitHub, verified partners and selected 3rd party actions\n  - `github_actions_repo_set_secret.sh` - sets a secret in the given repo from `key=value` or shell export format, as args or via stdin (eg. piped from `aws_csv_creds.sh`)\n  - `github_actions_repo_env_set_secret.sh` - sets a secret in the given repo and environment from `key=value` or shell export format, as args or via stdin (eg. piped from `aws_csv_creds.sh`)\n  - `github_actions_repo_secrets_overriding_org.sh` - finds any secrets for a repo that are overriding organization level secrets. Useful to combine with `github_foreach_repo.sh` for auditing\n  - `github_actions_repo_restrict_actions.sh` - restricts GitHub Actions in the given repo to only running actions from GitHub and verfied partner companies (.eg AWS, Docker)\n  - `github_actions_repo_actions_allow.sh` - allows select 3rd party GitHub Actions in the given repo\n  - `github_actions_runner.sh` - generates a [GitHub Actions](https://github.com/features/actions) self-hosted runner token for a given Repo or Organization via the GitHub API and then runs a dockerized GitHub Actions runner with the appropriate configuration\n  - `github_actions_runner_local.sh` - downloads, configures and runs a local GitHub Actions Runner for Linux or Mac\n  - `github_actions_runner_token.sh` - generates a GitHub Actions runner token to register a new self-hosted runner\n  - `github_actions_runners.sh` - lists GitHub Actions self-hosted runners for a given Repo or Organization\n  - `github_actions_delete_offline_runners.sh` - deletes offline GitHub Actions self-hosted runners. Useful to clean up short-lived runners eg. Docker, Kubernetes\n  - `github_actions_workflows.sh` - lists GitHub Actions workflows for a given repo (or auto-infers local repository)\n  - `github_actions_workflow_runs.sh` - lists GitHub Actions workflow runs for a given workflow id or name\n  - `github_actions_workflows_status.sh` - lists all GitHub Actions workflows and their statuses for a given repo\n  - `github_actions_workflows_state.sh` - lists GitHub Actions workflows enabled/disabled states (GitHub now disables workflows after 6 months without a commit)\n  - `github_actions_workflows_disabled.sh` - lists GitHub Actions workflows that are disabled. Combine with `github_foreach_repo.sh` to scan all repos to find disabled workflows\n  - `github_actions_workflow_enable.sh` - enables a given GitHub Actions workflow\n  - `github_actions_workflows_enable_all.sh` - enables all GitHub Actions workflows in a given repo. Useful to undo GitHub disabling all workflows in a repo after 6 months without a commit\n  - `github_actions_workflows_trigger_all.sh` - triggers all workflows for the given repo\n  - `github_actions_workflows_cancel_all_runs.sh` - cancels all workflow runs for the given repo\n  - `github_actions_workflows_cancel_waiting_runs.sh` - cancels workflow runs that are in waiting state, eg. waiting for old deployment approvals\n  - `github_ssh_get_user_public_keys.sh` - fetches a given GitHub user's public SSH keys via the API for piping to `~/.ssh/authorized_keys` or adjacent tools\n  - `github_ssh_get_public_keys.sh` - fetches the currently authenticated GitHub user's public SSH keys via the API, similar to above but authenticated to get identifying key comments\n  - `github_ssh_add_public_keys.sh` - uploads SSH keys from local files or standard input to the currently authenticated GitHub account. Specify pubkey files (default: `~/.ssh/id_rsa.pub`) or read from standard input for piping from adjacent tools\n  - `github_ssh_delete_public_keys.sh` - deletes given SSH keys from the currently authenticated GitHub account by key id or title regex match\n  - `github_gpg_get_user_public_keys.sh` - fetches a given GitHub user's public GPG keys via the API\n  - `github_generate_status_page.sh` - generates a [STATUS.md](https://harisekhon.github.io/CI-CD/) page by merging all the README.md headers for all of a user's non-forked GitHub repos or a given list of any repos etc.\n  - `github_purge_camo_cache.sh` - send HTTP Purge requests to all camo urls (badge caches) for the current or given GitHub repo's landing/README.md page\n  - `github_ip_ranges.sh` - returns GitHub's IP ranges, either all by default or for a select given service such as hooks or actions\n  - `github_sync_repo_descriptions.sh` - syncs GitHub repo descriptions to GitLab & BitBucket repos\n  - `github_release.sh` - creates a GitHub Release, auto-incrementing a `.N` suffix on the year/month/day date format if no exact version given\n  - `github_repo_check_pat_token.sh` - checks the given PAT token can access the given GitHub repo. Useful to test a PAT token used for integrations like ArgoCD\n  - `github_repo_description.sh` - fetches the given repo's description (used by `github_sync_repo_descriptions.sh`)\n  - `github_repo_find_files.sh` - finds files matching a regex in the current or given GitHub repo via the GitHub API\n  - `github_repo_latest_release.sh` - returns the latest release tag for a given GitHub repo via the GitHub API\n  - `github_repo_latest_release_filter.sh` - returns the latest release tag matching a given regex filter for a given GitHub repo via the GitHub API. Useful for getting the latest version of things like Kustomize which has other releases for kyaml\n  - `github_repo_stars.sh` - fetches the stars, forks and watcher counts for a given repo\n  - `github_repo_teams.sh` - fetches the GitHub Enterprise teams and their role permisions for a given repo. Combine with `github_foreach_repo.sh` to audit your all your personal or GitHub organization's repos\n  - `github_repo_collaborators.sh` - fetches a repo's granted users and outside invited collaborators as well as their role permisions for a given repo. Combine with `github_foreach_repo.sh` to audit your all your personal or GitHub organization's repos\n  - `github_repo_protect_branches.sh` - enables branch protections on the given repo. Can specify one or more branches to protect, otherwise finds and applies to any of `master`, `main`, `develop`, `dev`, `staging`, `production`\n  - `github_repos_find_files.sh` - finds files matching a regex across all repos in the current GitHub organization or user account\n  - `github_repo_fork_sync.sh` - sync's current or given fork, then runs `github_repo_fork_update.sh` to cascade changes to major branches via Pull Requests for auditability\n  - `github_repo_fork_update.sh` - updates a forked repo by creating pull requests for full audit tracking and auto-merges PRs for non-production branches\n  - `github_repos_public.sh` - lists public repos for a user or organization. Useful to periodically scan and account for any public repos\n  - `github_repos_disable_wiki.sh` - disables the Wiki on one or more given repos to prevent documentation fragmentation and make people use the centralized documentation tool eg. Confluence or Slite\n  - `github_repos_with_few_users.sh` - finds repos with few or no users (default: 1), which in Enterprises is a sign that a user has created a repo without assigning team privileges\n  - `github_repos_with_few_teams.sh` - finds repos with few or no teams (default: 0), which in Enterprises is a sign that a user has created a repo without assigning team privileges\n  - `github_repos_without_branch_protections.sh` - finds repos without any branch protection rules (use `github_repo_protect_branches.sh` on such repos)\n  - `github_repos_not_in_terraform.sh` - finds all non-fork repos for current or given user/organization which are not found in `$PWD/*.tf` Terraform code\n  - `github_teams_not_in_terraform.sh` - finds all teams for given organization which are not found in `$PWD/*.tf` Terraform code\n  - `github_repos_sync_status.sh` - determines whether each GitHub repo's mirrors on GitLab / BitBucket / Azure DevOps are up to date with the latest commits, by querying all 3 APIs and comparing master branch hashrefs\n  - `github_teams_not_idp_synced.sh` - finds GitHub teams that aren't sync'd from an IdP like Azure AD. These should usually be migrated or removed\n  - `github_user_repos_stars.sh` - fetches the total number of stars for all original source public repos for a given user\n  - `github_user_repos_forks.sh` - fetches the total number of forks for all original source public repos for a given user\n  - `github_user_repos_count.sh` - fetches the total number of original source public repos for a given username\n  - `github_user_followers.sh` - fetches the number of followers for a given username\n  - `github_url_clipboard.sh` - copies a GitHub URL file's contents to the clipboard, converting the URL to a raw GitHub content URL where necessary\n- `gitlab/*.sh` - [GitLab](https://gitlab.com/) API scripts:\n  - `gitlab_api.sh` - queries the GitLab [API](https://docs.gitlab.com/ee/api/api_resources.html). Can infer GitLab user, repo and authentication token from local checkout or environment (`$GITLAB_USER`, `$GITLAB_TOKEN`)\n  - `gitlab_install_binary.sh` - installs a binary from GitLab releases into $HOME/bin or /usr/local/bin. Auto-determines the latest release if no version specified, detects and unpacks any tarball or zip files\n  - `gitlab_push_mr_preview.sh` - pushes to GitLab origin, sets upstream branch, then open a Merge Request preview from current to default branch\n  - `github_push_mr.sh` - pushes to GitLab origin, sets upstream branch, then idemopotently creates a Merge Request from current branch to the given or default trunk branch and opens the generated MR in your browser for review\n  - `gitlab_foreach_repo.sh` - executes a templated command for each GitLab project/repo, replacing the `{user}` and `{project}` in each iteration\n  - `gitlab_project_latest_release.sh` - returns the latest release tag for a given GitLab project (repo) via the GitLab API\n  - `gitlab_project_set_description.sh` - sets the description for one or more projects using the GitLab API\n  - `gitlab_project_set_env_vars.sh` - adds / updates GitLab project-level environment variable(s) via the API from `key=value` or shell export format, as args or via stdin (eg. piped from `aws_csv_creds.sh`)\n  - `gitlab_group_set_env_vars.sh` - adds / updates GitLab group-level environment variable(s) via the API from `key=value` or shell export format, as args or via stdin (eg. piped from `aws_csv_creds.sh`)\n  - `gitlab_project_create_import.sh` - creates a GitLab repo as an import from a given URL, and mirrors if on GitLab Premium (can only manually configure for public repos on free tier, API doesn't support configuring even public repos on free)\n  - `gitlab_project_protect_branches.sh` - enables branch protections on the given project. Can specify one or more branches to protect, otherwise finds and applies to any of `master`, `main`, `develop`, `dev`, `staging`, `production`\n  - `gitlab_project_mirrors.sh` - lists each GitLab repo and whether it is a mirror or not\n  - `gitlab_pull_mirror.sh` - trigger a GitLab pull mirroring for a given project's repo, or auto-infers project name from the local git repo\n  - `gitlab_ssh_get_user_public_keys.sh` - fetches a given GitLab user's public SSH keys via the API, with identifying comments, for piping to `~/.ssh/authorized_keys` or adjacent tools\n  - `gitlab_ssh_get_public_keys.sh` - fetches the currently authenticated GitLab user's public SSH keys via the API\n  - `gitlab_ssh_add_public_keys.sh` - uploads SSH keys from local files or standard input to the currently authenticated GitLab account. Specify pubkey files (default: `~/.ssh/id_rsa.pub`) or read from standard input for piping from adjacent tools\n  - `gitlab_ssh_delete_public_keys.sh` - deletes given SSH keys from the currently authenticated GitLab account by key id or title regex match\n  - `gitlab_validate_ci_yaml.sh` - validates a `.gitlab-ci.yml` file via the GitLab API\n- `bitbucket/*.sh` - [BitBucket](https://bitbucket.org/) API scripts:\n  - `bitbucket_api.sh` - queries the BitBucket [API](https://developer.atlassian.com/bitbucket/api/2/reference/resource/). Can infer BitBucket user, repo and authentication token from local checkout or environment (`$BITBUCKET_USER`, `$BITBUCKET_TOKEN`)\n  - `bitbucket_foreach_repo.sh` - executes a templated command for each BitBucket repo, replacing the `{user}` and `{repo}` in each iteration\n  - `bitbucket_workspace_set_env_vars.sh` - adds / updates Bitbucket workspace-level environment variable(s) via the API from `key=value` or shell export format, as args or via stdin (eg. piped from `aws_csv_creds.sh`)\n  - `bitbucket_repo_set_env_vars.sh` - adds / updates Bitbucket repo-level environment variable(s) via the API from `key=value` or shell export format, as args or via stdin (eg. piped from `aws_csv_creds.sh`)\n  - `bitbucket_repo_set_description.sh` - sets the description for one or more repos using the BitBucket API\n  - `bitbucket_enable_pipelines.sh` - enables the CI/CD pipelines for all repos\n  - `bitbucket_disable_pipelines.sh` - disables the CI/CD pipelines for all repos\n  - `bitbucket_repo_enable_pipeline.sh` - enables the CI/CD pipeline for a given repo\n  - `bitbucket_repo_disable_pipeline.sh` - disables the CI/CD pipeline for a given repo\n  - `bitbucket_ssh_get_public_keys.sh` - fetches the currently authenticated BitBucket user's public SSH keys via the API for piping to `~/.ssh/authorized_keys` or adjacent tools\n  - `bitbucket_ssh_add_public_keys.sh` - uploads SSH keys from local files or standard input to the currently authenticated BitBucket account. Specify pubkey files (default: `~/.ssh/id_rsa.pub`) or read from standard input for piping from adjacent tools\n  - `bitbucket_ssh_delete_public_keys.sh` - uploads SSH keys from local files or standard input to the currently authenticated BitBucket account. Specify pubkey files (default: `~/.ssh/id_rsa.pub`) or read from standard input for piping from adjacent tools\n\nSee also [Knowledge Base notes for Git](https://github.com/HariSekhon/Knowledge-Base/blob/main/git.md).\n\n### CI/CD - Continuous Integration / Continuous Deployment\n\n`jenkins/`, `terraform/`, `teamcity/`, `buildkite/`, `circlci/`, `travis/`, `azure_devops/`, ...,  `cicd/` directories:\n\n- `appveyor_api.sh` - queries [AppVeyor](https://www.appveyor.com/)'s API with authentication\n- `azure_devops/*.sh` - [Azure DevOps](https://dev.azure.com/) scripts:\n  - `azure_devops_api.sh` - queries Azure DevOps's API with authentication\n  - `azure_devops_foreach_repo.sh` - executes a templated command for each Azure DevOps repo, replacing `{user}`, `{org}`, `{project}` and `{repo}` in each iteration\n  - `azure_devops_to_github_migration.sh` - migrates one or all Azure DevOps git repos to GitHub, including all branches and sets the default branch to match via the APIs to maintain the same checkout behaviour\n  - `azure_devops_disable_repos.sh` - disables one or more given Azure DevOps repos (to prevent further pushes to them after migration to GitHub)\n- `circleci/*.sh` - [CircleCI](https://circleci.com/) scripts:\n  - `circleci_api.sh` - queries CircleCI's API with authentication\n  - `circleci_project_set_env_vars.sh` - adds / updates CircleCI project-level environment variable(s) via the API from `key=value` or shell export format, as args or via stdin (eg. piped from `aws_csv_creds.sh`)\n  - `circleci_context_set_env_vars.sh` - adds / updates CircleCI context-level environment variable(s) via the API from `key=value` or shell export format, as args or via stdin (eg. piped from `aws_csv_creds.sh`)\n  - `circleci_project_delete_env_vars.sh` - deletes CircleCI project-level environment variable(s) via the API\n  - `circleci_context_delete_env_vars.sh` - deletes CircleCI context-level environment variable(s) via the API\n  - `circleci_local_execute.sh` - installs CircleCI CLI and executes `.circleci/config.yml` locally\n  - `circleci_public_ips.sh` - lists [CircleCI](https://circleci.com) public IP addresses via dnsjson.com\n- `codeship_api.sh` - queries [CodeShip](https://codeship.com/)'s API with authentication\n- `drone_api.sh` - queries [Drone.io](https://drone.io/)'s API with authentication\n- `shippable_api.sh` - queries [Shippable](https://www.shippable.com/)'s API with authentication\n- `wercker_app_api.sh` - queries [Wercker](https://app.wercker.com/)'s Applications API with authentication\n- `gocd_api.sh` - queries [GoCD](https://www.gocd.org/)'s API\n- `gocd.sh` - one-touch [GoCD CI](https://www.gocd.org/):\n  - launches in Docker\n  - (re)creates config repo (`$PWD/setup/gocd_config_repo.json`) from which to source pipeline(s) (`.gocd.yml`)\n  - detects and enables agent(s) to start building\n  - call from any repo top level directory with a `.gocd.yml` config (all mine have it), mimicking structure of fully managed CI systems\n- `concourse.sh` - one-touch [Concourse CI](https://concourse-ci.org/):\n  - launches in Docker\n  - configures pipeline from `$PWD/.concourse.yml`\n  - triggers build\n  - tails results in terminal\n  - prints recent build statuses at end\n  - call from any repo top level directory with a `.concourse.yml` config (all mine have it), mimicking structure of fully managed CI systems\n- `fly.sh` - shortens [Concourse](https://concourse-ci.org/) `fly` command to not have to specify target all the time\n- `jenkins/*.sh` - [Jenkins CI](https://jenkins.io/) scripts:\n  - `jenkins.sh` - one-touch [Jenkins CI](https://jenkins.io/):\n    - launches Docker container\n    - installs plugins\n    - validates `Jenkinsfile`\n    - configures job from `$PWD/setup/jenkins-job.xml`\n    - sets Pipeline to git remote origin's `Jenkinsfile`\n    - triggers build\n    - tails results in terminal\n    - call from any repo top level directory with a `Jenkinsfile` pipeline and `setup/jenkins-job.xml` (all mine have it)\n  - `jenkins_api.sh` - queries the Jenkins Rest API, handles authentication, pre-fetches CSFR protection token crumb, supports many environment variables such as `$JENKINS_URL` for ease of use\n    - `jenkins_jobs.sh` - lists Jenkins jobs (pipelines)\n    - `jenkins_foreach_job.sh` - runs a templated command for each Jenkins job\n    - `jenkins_jobs_download_configs.sh` - downloads all Jenkins job configs to xml files of the same name\n    - `jenkins_job_config.sh` - gets or sets a Jenkins job's config\n    - `jenkins_job_description.sh` - gets or sets a Jenkins job's description\n    - `jenkins_job_enable.sh` - enables a Jenkins job by name\n    - `jenkins_job_disable.sh` - disables a Jenkins job by name\n    - `jenkins_job_trigger.sh` - triggers a Jenkins job by name\n    - `jenkins_job_trigger_with_params.sh` - triggers a Jenkins job with parameters which can be passed as `--data KEY=VALUE`\n    - `jenkins_jobs_enable.sh` - enables all Jenkins jobs/pipelines with names matching a given regex\n    - `jenkins_jobs_disable.sh` - disables all Jenkins jobs/pipelines with names matching a given regex\n    - `jenkins_builds.sh` - lists Jenkins latest builds for every job\n    - `jenkins_cred_add_cert.sh` - creates a Jenkins certificate credential from a PKCS#12 keystore\n    - `jenkins_cred_add_kubernetes_sa.sh` - creates a Jenkins Kubernetes service account credential\n    - `jenkins_cred_add_secret_file.sh` - creates a Jenkins secret file credential from a file\n    - `jenkins_cred_add_secret_text.sh` - creates a Jenkins secret string credential from a string or a file\n    - `jenkins_cred_add_ssh_key.sh` - creates a Jenkins SSH key credential from a string or an SSH private key file\n    - `jenkins_cred_add_user_pass.sh` - creates a Jenkins username/password credential\n    - `jenkins_cred_delete.sh` - deletes a given Jenkins credential by id\n    - `jenkins_cred_list.sh` - lists Jenkins credentials IDs and Names\n    - `jenkins_cred_update_cert.sh` - updates a Jenkins certificate credential from a PKCS#12 keystore\n    - `jenkins_cred_update_kubernetes_sa.sh` - updates a Jenkins Kubernetes service account credential\n    - `jenkins_cred_update_secret_file.sh` - updates a Jenkins secret file credential from a file\n    - `jenkins_cred_update_secret_text.sh` - updates a Jenkins secret string credential from a string or a file\n    - `jenkins_cred_update_ssh_key.sh` - updates a Jenkins SSH key credential from a string or an SSH private key file\n    - `jenkins_cred_update_user_pass.sh` - updates a Jenkins username/password credential\n    - `jenkins_cred_set_cert.sh` - creates or updates a Jenkins certificate credential from a PKCS#12 keystore\n    - `jenkins_cred_set_kubernetes_sa.sh` - creates or updates a Jenkins Kubernetes service account credential\n    - `jenkins_cred_set_secret_file.sh` - creates or updates a Jenkins secret file credential from a file\n    - `jenkins_cred_set_secret_text.sh` - creates or updates a Jenkins secret string credential from a string or a file\n    - `jenkins_cred_set_ssh_key.sh` - creates or updates a Jenkins SSH key credential from a string or an SSH private key file\n    - `jenkins_cred_set_user_pass.sh` - creates or updates a Jenkins username/password credential\n  - `jenkins_cli.sh` - shortens `jenkins-cli.jar` command by auto-inferring basic configuations, auto-downloading the CLI if absent, inferrings a bunch of Jenkins related variables like `$JENKINS_URL`, `$JENKINS_CLI_ARGS` and authentication using `$JENKINS_USER`/`$JENKINS_PASSWORD`, or finds admin password from inside local docker container. Used heavily by `jenkins.sh` one-shot setup and the following scripts:\n    - `jenkins_foreach_job_cli.sh` - runs a templated command for each Jenkins job\n    - `jenkins_create_job_parallel_test_runs.sh` - creates a freestyle parameterized test sleep job and launches N parallel runs of it to test scaling and parallelization of [Jenkins on Kubernetes](https://github.com/HariSekhon/Kubernetes-configs#jenkins-on-kubernetes) agents\n    - `jenkins_create_job_check_gcp_serviceaccount.sh` - creates a freestyle test job which runs a GCP Metadata query to determine the GCP serviceaccount the agent pod is operating under to check GKE Workload Identity integration\n    - `jenkins_jobs_download_configs_cli.sh` - downloads all Jenkins job configs to xml files of the same name\n    - `jenkins_cred_cli_add_cert.sh` - creates a Jenkins certificate credential from a PKCS#12 keystore\n    - `jenkins_cred_cli_add_kubernetes_sa.sh` - creates a Jenkins Kubernetes service account credential\n    - `jenkins_cred_cli_add_secret_file.sh` - creates a Jenkins secret file credential from a file\n    - `jenkins_cred_cli_add_secret_text.sh` - creates a Jenkins secret string credential from a string or a file\n    - `jenkins_cred_cli_add_ssh_key.sh` - creates a Jenkins SSH key credential from a string or an SSH private key file\n    - `jenkins_cred_cli_add_user_pass.sh` - creates a Jenkins username/password credential\n    - `jenkins_cred_cli_delete.sh` - deletes a given Jenkins credential by id\n    - `jenkins_cred_cli_list.sh` - lists Jenkins credentials IDs and Names\n    - `jenkins_cred_cli_update_cert.sh` - updates a Jenkins certificate credential from a PKCS#12 keystore\n    - `jenkins_cred_cli_update_kubernetes_sa.sh` - updates a Jenkins Kubernetes service account credential\n    - `jenkins_cred_cli_update_secret_file.sh` - updates a Jenkins secret file credential from a file\n    - `jenkins_cred_cli_update_secret_text.sh` - updates a Jenkins secret string credential from a string or a file\n    - `jenkins_cred_cli_update_ssh_key.sh` - updates a Jenkins SSH key credential from a string or an SSH private key file\n    - `jenkins_cred_cli_update_user_pass.sh` - updates a Jenkins username/password credential\n    - `jenkins_cred_cli_set_cert.sh` - creates or updates a Jenkins certificate credential from a PKCS#12 keystore\n    - `jenkins_cred_cli_set_kubernetes_sa.sh` - creates or updates a Jenkins Kubernetes service account credential\n    - `jenkins_cred_cli_set_secret_file.sh` - creates or updates a Jenkins secret file credential from a file\n    - `jenkins_cred_cli_set_secret_text.sh` - creates or updates a Jenkins secret string credential from a string or a file\n    - `jenkins_cred_cli_set_ssh_key.sh` - creates or updates a Jenkins SSH key credential from a string or an SSH private key file\n    - `jenkins_cred_cli_set_user_pass.sh` - creates or updates a Jenkins username/password credential\n  - `jenkins_password.sh` - gets Jenkins admin password from local docker container. Used by `jenkins_cli.sh`\n  - `jenkins_plugins_latest_versions.sh` - finds the latest versions of given Jenkins plugins. Useful to programmatically upgrade your Jenkins on Kubernetes plugins defined in [values.yaml](https://github.com/HariSekhon/Kubernetes-configs/blob/6d9e34b74d3fa8f353b0fe56e74cea3af439e01a/jenkins/base/values.yaml#L145)\n  - `check_jenkinsfiles.sh` - validates all `*Jenkinsfile*` files in the given directory trees using the online Jenkins validator\n  - See also [Knowledge Base notes for Jenkins](https://github.com/HariSekhon/Knowledge-Base/blob/main/jenkins.md).\n- `teamcity/*.sh` - [TeamCity CI](https://www.jetbrains.com/teamcity/) scripts:\n  - `teamcity.sh` - one-touch [TeamCity CI](https://www.jetbrains.com/teamcity/) cluster:\n    - launches Docker containers with server and 1 agent\n    - click proceed and accept the EULA\n    - waits for server to initialize\n    - waits for agent to register\n    - authorizes agent\n    - creates a VCS Root if `$PWD` has a `.teamcity.vcs.json` / `.teamcity.vcs.ssh.json` / `.teamcity.vcs.oauth.json` and corresponding `$TEAMCITY_SSH_KEY` or `$TEAMCITY_GITHUB_CLIENT_ID`+`$TEAMCITY_GITHUB_CLIENT_SECRET` environment variables\n    - creates a Project and imports all settings and builds from the VCS Root\n    - creates an admin user and an API token for you\n    - see also: [TeamCity CI](https://github.com/HariSekhon/TeamCity-CI) config repo for importing pipelines\n  - `teamcity_api.sh` - queries TeamCity's API, auto-handling authentication and other quirks of the API\n  - `teamcity_create_project.sh` - creates a TeamCity project using the API\n  - `teamcity_create_github_oauth_connection.sh` - creates a TeamCity GitHub OAuth VCS connection in the Root project, useful for bootstrapping projects from VCS configs\n  - `teamcity_create_vcs_root.sh` - creates a TeamCity VCS root from a save configuration (XML or JSON), as downloaded by `teamcity_export_vcs_roots.sh`\n  - `teamcity_upload_ssh_key.sh` - uploads an SSH private key to a TeamCity project (for use in VCS root connections)\n  - `teamcity_agents.sh` - lists TeamCity agents, their connected state, authorized state, whether enabled and up to date\n  - `teamcity_builds.sh` - lists the last 100 TeamCity builds along with the their state (eg. `finished`) and status (eg. `SUCCESS`/`FAILURE`)\n  - `teamcity_buildtypes.sh` - lists TeamCity buildTypes (pipelines) along with the their project and IDs\n  - `teamcity_buildtype_create.sh` - creates a TeamCity buildType from a local JSON configuration (see `teamcity_buildtypes_download.sh`)\n  - `teamcity_buildtype_set_description_from_github.sh` - sync's a TeamCity buildType's description from its Github repo description\n  - `teamcity_buildtypes_set_description_from_github.sh` - sync's all TeamCity buildType descriptions from their GitHub repos where available\n  - `teamcity_export.sh` - downloads TeamCity configs to local JSON files in per-project directories mimicking native TeamCity directory structure and file naming\n  - `teamcity_export_project_config.sh` - downloads TeamCity project config to local JSON files\n  - `teamcity_export_buildtypes.sh` - downloads TeamCity buildType config to local JSON files\n  - `teamcity_export_vcs_roots.sh` - downloads TeamCity VCS root config to local JSON files\n  - `teamcity_projects.sh` - lists TeamCity project IDs and Names\n  - `teamcity_project_set_versioned_settings.sh` - configures a project to track all changes to a VCS (eg. GitHub)\n  - `teamcity_project_vcs_versioning.sh` - quickly toggle VCS versioning on/off for a given TeamCity project (useful for testing without auto-committing)\n  - `teamcity_vcs_roots.sh` - lists TeamCity VCS root IDs and Names\n- `travis/*.sh` - [Travis CI](https://travis-ci.org/) API scripts (one of my all-time favourite CI systems):\n  - `travis_api.sh` - queries the Travis CI API with authentication using `$TRAVIS_TOKEN`\n  - `travis_repos.sh` - lists Travis CI repos\n  - `travis_foreach_repo.sh` - executes a templated command against all Travis CI repos\n  - `travis_repo_build.sh` - triggers a build for the given repo\n  - `travis_repo_caches.sh` - lists caches for a given repo\n  - `travis_repo_crons.sh` - lists crons for a given repo\n  - `travis_repo_env_vars.sh` - lists environment variables for a given repo\n  - `travis_repo_settings.sh` - lists settings for a given repo\n  - `travis_repo_create_cron.sh` - creates a cron for a given repo and branch\n  - `travis_repo_delete_crons.sh` - deletes all crons for a given repo\n  - `travis_repo_delete_caches.sh` - deletes all caches for a given repo (sometimes clears build problems)\n  - `travis_delete_cron.sh` - deletes a Travis CI cron by ID\n  - `travis_repos_settings.sh` - lists settings for all repos\n  - `travis_repos_caches.sh` - lists caches for all repos\n  - `travis_repos_crons.sh` - lists crons for all repos\n  - `travis_repos_create_cron.sh` - creates a cron for all repos\n  - `travis_repos_delete_crons.sh` - deletes all crons for all repos\n  - `travis_repos_delete_caches.sh` - deletes all caches for all repos\n  - `travis_lint.sh` - lints a given `.travis.yml` using the API\n- `buildkite/*.sh` - [BuildKite](https://buildkite.com/) API scripts:\n  - `buildkite_api.sh` - queries the BuildKite API, handling authentication using `$BUILDKITE_TOKEN`\n  - `buildkite_pipelines.sh` - list buildkite pipelines for your `$BUILDKITE_ORGANIZATION` / `$BUILDKITE_USER`\n  - `buildkite_foreach_pipeline.sh` - executes a templated command for each Buildkite pipeline, replacing the `{user}` and `{pipeline}` in each iteration\n  - `buildkite_agent.sh` - runs a buildkite agent locally on Linux or Mac, or in Docker with choice of Linux distros\n  - `buildkite_agents.sh` - lists the Buildkite agents connected along with their hostname, IP, started dated and agent details\n  - `buildkite_pipelines.sh` - lists Buildkite pipelines\n  - `buildkite_create_pipeline.sh` - create a Buildkite pipeline from a JSON configuration (like from `buildkite_get_pipeline.sh` or `buildkite_save_pipelines.sh`)\n  - `buildkite_get_pipeline.sh` - gets details for a specific Buildkite pipeline in JSON format\n  - `buildkite_update_pipeline.sh` - updates a BuildKite pipeline from a configuration provided via stdin or from a file saved via `buildkite_get_pipeline.sh`\n  - `buildkite_patch_pipeline.sh` - updates a BuildKite pipeline from a partial configuration provided as an arg, via stdin, or from a file saved via `buildkite_get_pipeline.sh`\n  - `buildkite_pipeline_skip_settings.sh` - lists the skip intermediate build settings for one or more given BuildKite pipelines\n  - `buildkite_pipeline_set_skip_settings.sh` - configures given or all BuildKite pipelines to skip intermediate builds and cancel running builds in favour of latest build\n  - `buildkite_cancel_scheduled_builds.sh` - cancels BuildKite scheduled builds (to clear a backlog due to offline agents and just focus on new builds)\n  - `buildkite_cancel_running_builds.sh` - cancels BuildKite running builds (to clear them and restart new later eg. after agent / environment change / fix)\n  - `buildkite_pipeline_disable_forked_pull_requests.sh` - disables forked pull request builds on a BuildKite pipeline to protect your build environment from arbitrary code execution security vulnerabilities\n  - `buildkite_pipelines_vulnerable_forked_pull_requests.sh` - prints the status of each pipeline, should all return false, otherwise run the above script to close the vulnerability\n  - `buildkite_rebuild_cancelled_builds.sh` - triggers rebuilds of last N cancelled builds in current pipeline\n  - `buildkite_rebuild_failed_builds.sh` - triggers rebuilds of last N failed builds in current pipeline (eg. after agent restart / environment change / fix)\n  - `buildkite_rebuild_all_pipelines_last_cancelled.sh` - triggers rebuilds of the last cancelled build in each pipeline in the organization\n  - `buildkite_rebuild_all_pipelines_last_failed.sh` - triggers rebuilds of the last failed build in each pipeline in the organization\n  - `buildkite_retry_jobs_dead_agents.sh` - triggers job retries where jobs failed due to killed agents, continuing builds from that point and replacing their false negative failed status with the real final status, slightly better than rebuilding entire jobs which happen under a new build\n  - `buildkite_recreate_pipeline.sh` - recreates a pipeline to wipe out all stats (see url and badge caveats in `--help`)\n  - `buildkite_running_builds.sh` - lists running builds and the agent they're running on\n  - `buildkite_save_pipelines.sh` - saves all BuildKite pipelines in your `$BUILDKITE_ORGANIZATION` to local JSON files in `$PWD/.buildkite-pipelines/`\n  - `buildkite_set_pipeline_description.sh` - sets the description of one or more pipelines using the BuildKite API\n  - `buildkite_set_pipeline_description_from_github.sh` - sets a Buildkite pipeline description to match its source GitHub repo\n  - `buildkite_sync_pipeline_descriptions_from_github.sh` - for all BuildKite pipelines sets each description to match its source GitHub repo\n  - `buildkite_trigger.sh` - triggers BuildKite build job for a given pipeline\n  - `buildkite_trigger_all.sh` - same as above but for all pipelines\n- `terraform_cloud_*.sh` - [Terraform Cloud](https://www.terraform.io/cloud) API scripts:\n  - `terraform_cloud_api.sh` - queries the Cloudflare API, handling authentication using `$TERRAFORM_TOKEN`\n  - `terraform_cloud_ip_ranges.sh` - returns the list of IP ranges for Terraform Cloud\n  - `terraform_cloud_organizations.sh` - lists Terraform Cloud organizations\n  - `terraform_cloud_workspaces.sh` - lists Terraform Cloud workspaces\n  - `terraform_cloud_workspace_vars.sh` - lists Terraform Cloud workspace variables\n  - `terraform_cloud_workspace_set_vars.sh` - adds / updates Terraform workspace-level sensitive environment/terraform variable(s) via the API from `key=value` or shell export format, as args or via stdin (eg. piped from `aws_csv_creds.sh`)\n  - `terraform_cloud_workspace_delete_vars.sh` - deletes one or more Terraform workspace-level variables\n  - `terraform_cloud_varsets.sh` - lists Terraform Cloud variable sets\n  - `terraform_cloud_varset_vars.sh` - lists Terraform Cloud variables in on or all variables sets for the given organization\n  - `terraform_cloud_varset_set_vars.sh` - adds / updates Terraform sensitive environment/terraform variable(s) in a given variable set via the API from `key=value` or shell export format, as args or via stdin (eg. piped from `aws_csv_creds.sh`)\n  - `terraform_cloud_varset_delete_vars.sh` - deletes one or more Terraform variables in a given variable set\n- `terraform_*.sh` - [Terraform](https://www.terraform.io/) scripts:\n  - `terraform_gcs_backend_version.sh` - determines the Terraform state version from the tfstate file in a GCS bucket found in a local given `backend.tf`\n  - `terraform_gitlab_download_backend_variable.sh` - downloads backend.tf from a GitLab CI/CD variable to be able to quickly iterate plans locally\n  - `terraform_import.sh` - finds given resource type in `./*.tf` code or Terraform plan output that are not in Terraform state and imports them\n  - `terraform_import_aws_iam_users.sh` - parses Terraform plan output to import new `aws_iam_user` additions into Terraform state\n  - `terraform_import_aws_iam_groups.sh` - parses Terraform plan output to import new `aws_iam_group` additions into Terraform state\n  - `terraform_import_aws_iam_policies.sh` - parses Terraform plan output to import new `aws_iam_policies` additions, resolves their ARNs and imports them into Terraform state\n  - `terraform_import_aws_sso_permission_sets.sh` - finds all `aws_ssoadmin_permission_set` in `./*.tf` code, resolves the ARNs and imports them to Terraform state\n  - `terraform_import_aws_sso_account_assignments.sh` - parses Terraform plan output to import new `aws_ssoadmin_account_assignment` additions into Terraform state\n  - `terraform_import_aws_sso_managed_policy_attachments.sh` - parses Terraform plan output to import new `aws_ssoadmin_account_assignment` additions into Terraform state\n  - `terraform_import_aws_sso_permission_set_inline_policies.sh` - parses Terraform plan output to import new `aws_ssoadmin_permission_set_inline_policy` additions into Terraform state\n  - `terraform_import_github_repos.sh` - finds all `github_repository` in `./*.tf` code or Terraform plan output that are not in Terraform state and imports them. See also `github_repos_not_in_terraform.sh`\n  - `terraform_import_github_team.sh` - imports a given GitHub team into a given Terraform state resource, by first querying the GitHub API for the team ID needed to import into Terraform\n  - `terraform_import_github_teams.sh` - finds all `github_team` in `./*.tf` code or Terraform plan output that are not in Terraform state, then queries the GitHub API for their IDs and imports them. See also `github_teams_not_in_terraform.sh`\n  - `terraform_import_github_team_repos.sh` - finds all `github_team_repository` in Terraform plan that would be added, then queries the GitHub API for the repos and team IDs and if they both exist then imports them to Terraform state\n  - `terraform_resources.sh` - external program to get all resource ids and attribute for a given resource type to work around Terraform splat expression limitation ([#19931](https://github.com/hashicorp/terraform/issues/19931))\n  - `terraform_managed_resource_types.sh` - quick parse of what Terraform resource types are found in `*.tf` files under the current or given directory tree. Useful to give you a quick glance of what services you are managing\n  - `terraform_registry_url_extract.sh` - extracts the Terraform Registry URL in either `tfr://` or `https://registry.terraform.io/` format from a given string, file or standard input. Useful to fast load Terraform Module documentation via editor/IDE hotkeys (see [.vimrc](configs/.vimrc)). Based on `urlextract.sh` above\n  - `terraform_registry_url_to_https.sh` - converts one or more Terraform Registry URLs from `tfr://` to `https://registry.terraform.io/` format\n  - `terraform_registry_url_open.sh` - opens the Terraform Registry URL given as a string arg, file or standard input in either `tfr://` or `https://registry.terraform.io/` format\n  - See also [Knowledge Base notes for Terraform](https://github.com/HariSekhon/Knowledge-Base/blob/main/terraform.md).\n- `checkov_resource_*.sh` - [Checkov](https://www.checkov.io/) resource counts - useful to estimate [Bridgecrew Cloud](https://www.bridgecrew.cloud/) costs which are charged per resource:\n  - `checkov_resource_count.sh` - counts the number of resources Checkov is scanning in the current or given directory\n  - `checkov_resource_count_all.sh` - counts the total number of resources Checkov is scanning across all given repo checkouts\n- `octopus_api.sh` - queries the [Octopus Deploy](https://octopus.com/) API\n\nSee also [Knowledge Base notes for CI/CD](https://github.com/HariSekhon/Knowledge-Base/blob/main/ci-cd.md).\n\n### AI & IPaaS\n\n`ai/` and `ipaas/` directories:\n\n- `openai_api.sh` - queries the [OpenAI](https://openai.com/) (ChatGPT) API with authentication\n- `make_api.sh` - queries the [Make.com](https://www.make.com) API with authentication\n\n### Internet Services\n\n`internet/`, `cloudflare/`, `pingdom/`, `terraform/` directories:\n\n- Pastebins - uploads files and copies the resulting URL to your clipboard:\n  - code / text only - prompts to approve text / code before upload for safety:\n    - `pastebin.sh` - uploads a file to <https://pastebin.com>, script auto-determines which syntax highlighting to add since API doesn't auto infer\n    - `dpaste.sh` - uploads a file to <https://dpaste.com>, script auto-determines which syntax highlighting to add since API doesn't auto infer\n    - `termbin.sh` - uploads a file to <https://termbin.com> (site has no syntax highlighting)\n  - all files, multimedia or text / code - prompts to approve text / code before upload for safety:\n    - `0x0.sh` - uploads a file to <https://0x0.st> (fast)\n    - `imgur.sh` - uploads an image file to <https://imgur.com>\n    - `file.io.sh` - uploads a file to <https://file.io> with 2 weeks, single download retention\n    - `catbox.sh` - uploads a file to <https://catbox.moe/> with permanent retention (slow)\n    - `litterbox.sh` - uploads a file to <https://litterbox.catbox.moe/> with temporary retention (slow)\n- `digital_ocean_api.sh` / `doapi.sh` - queries the [Digital Ocean](https://www.digitalocean.com/) API with authentication\n  - see also the Digital Ocean CLI `doctl` (`install/install_doctl.sh`)\n- `atlassian_ip_ranges.sh` - lists [Atlassian](https://www.atlassian.com/)'s IPv4 and/or IPv6 cidr ranges via its API\n- `circleci_public_ips.sh` - lists [CircleCI](https://circleci.com) public IP addresses via dnsjson.com\n- `cloudflare_*.sh` - [Cloudflare](https://www.cloudflare.com/) API queries and reports:\n  - `cloudflare_api.sh` - queries the Cloudflare API with authentication\n  - `cloudflare_ip_ranges.sh` - lists Cloudflare's IPv4 and/or IPv6 cidr ranges via its API\n  - `cloudflare_custom_certificates.sh` - lists any custom SSL certificates in a given Cloudflare zone along with their status and expiry date\n  - `cloudflare_dns_records.sh` - lists any Cloudflare DNS records for a zone, including the type and ttl\n  - `cloudflare_dns_records_all_zones.sh` - same as above but for all zones\n  - `cloudflare_dns_record_create.sh` - creates a DNS record in the given domain\n  - `cloudflare_dns_record_update.sh` - updates a DNS record in the given domain\n  - `cloudflare_dns_record_delete.sh` - deletes a DNS record in the given domain\n  - `cloudflare_dns_record_details.sh` - lists the details for a DNS record in the given domain in JSON format for further pipe processing\n  - `cloudflare_dnssec.sh` - lists the Cloudflare DNSSec status for all zones\n  - `cloudflare_firewall_rules.sh` - lists Cloudflare Firewall rules, optionally with filter expression\n  - `cloudflare_firewall_access_rules.sh` - lists Cloudflare Firewall Access rules, optionally with filter expression\n  - `cloudflare_foreach_account.sh` - executes a templated command for each Cloudflare account, replacing the `{account_id}` and `{account_name}` in each iteration (useful for chaining with `cloudflare_api.sh`)\n  - `cloudflare_foreach_zone.sh` - executes a templated command for each Cloudflare zone, replacing the `{zone_id}` and `{zone_name}` in each iteration (useful for chaining with `cloudflare_api.sh`, used by adjacent `cloudflare_*_all_zones.sh` scripts)\n  - `cloudflare_purge_cache.sh` - purges the entire Cloudflare cache\n  - `cloudflare_ssl_verified.sh` - gets the Cloudflare zone SSL verification status for a given zone\n  - `cloudflare_ssl_verified_all_zones.sh` - same as above for all zones\n  - `cloudflare_zones.sh` - lists Cloudflare zone names and IDs (needed for writing Terraform Cloudflare code)\n- `datadog_api.sh` - queries the [DataDog](https://www.datadoghq.com/) API with authentication\n- `dnsjson.sh` - queries dnsjson.com for DNS records\n- `gitguardian_api.sh` - queries the [GitGuardian](https://www.gitguardian.com/) API with authentication\n- `jira_api.sh` - queries [Jira](https://www.atlassian.com/software/jira) API with authentication\n- `kong_api.sh` - queries the [Kong API Gateway](https://docs.konghq.com/gateway/latest/)'s Admin API, handling authentication if enabled\n- `traefik_api.sh` - queries the [Traefik](https://traefik.io/) API, handling authentication if enabled\n- `ngrok_api.sh` - queries the [NGrok](https://ngrok.com/) API with authentication\n- `pingdom_*.sh` - [Pingdom](https://www.pingdom.com/) API queries and reports for status, latency, average response times, latency averages by hour, SMS credits, outages periods and durations over the last year etc.\n  - `pingdom_api.sh` - queries the Solarwinds [Pingdom](https://www.pingdom.com/) API with authentication\n  - `pingdom_foreach_check.sh` - executes a templated command against each Pingdom check, replacing the `{check_id}` and `{check_name}` in each iteration\n  - `pingdom_checks.sh` - show all Pingdom checks, status and latencies\n  - `pingdom_checks_outages.sh` / `pingdom_checks_outages.sh` - show one or all Pingdom checks outage histories for the last year\n  - `pingdom_checks_average_response_times.sh` - shows the average response times for all Pingdom checks for the last week\n  - `pingdom_check_latency_by_hour.sh` / `pingdom_checks_latency_by_hour.sh` - shows the average latency for one or all Pingdom checks broken down by hour of the day, over the last week\n  - `pingdom_sms_credits.sh` - gets the remaining number of Pingdom SMS credits\n- `terraform_cloud_api.sh` - queries [Terraform Cloud](https://www.terraform.io/cloud) API with authentication\n- `terraform_cloud_ip_ranges.sh` - returns the list of IP ranges for [Terraform Cloud](https://www.terraform.io/cloud) via the API, or optionally one or more of the ranges used by different functions\n- `wordpress.sh` - boots Wordpress in docker with a MySQL backend, and increases the upload_max_filesize to be able to restore a real world sized export backup\n- `wordpress_api.sh` - queries the Wordpress API with authentication\n- `wordpress_posts_without_category_tags.sh` - checks posts (articles) for categories without corresponding tags and prints the posts and their missing tags\n\n### Java\n\n`java/` directory:\n\n- `java_show_classpath.sh` - shows Java classpaths, one per line, of currently running Java programs\n- `jvm_heaps*.sh` - show all your Java heap sizes for all running Java processes, and their total MB (for performance tuning and sizing)\n- Java Decompilers:\n  - `java_decompile_jar.sh` - decompiles a Java JAR in /tmp, finds the main class and runs a Java decompiler on its main .class file using `jd_gui.sh`\n  - `jd_gui.sh` - runs Java Decompiler JD GUI, downloading its jar the first time if it's not already present\n  - `bytecode_viwer.sh` - runs Bytecode-Viewer GUI Java decompiler, downloading its jar the first time if it's not already present\n  - `cfr.sh` - runs CFR command line Java decompiler, downloading its jar the first time if it's not already present\n  - `procyon.sh` - runs Procyon command line Java decompiler, downloading its jar the first time if it's not already present\n\nSee also [Knowledge Base notes for Java](https://github.com/HariSekhon/Knowledge-Base/blob/main/java.md)\nand [JVM Performance Tuning](https://github.com/HariSekhon/Knowledge-Base/blob/main/java-jvm-performance-tuning.md).\n\n### Python\n\n`python/` directory:\n\n- `python_compile.sh` - byte-compiles Python scripts and libraries into `.pyo` optimized files\n- `python_pip_install.sh` - bulk installs PyPI modules from mix of arguments / file lists / stdin, accounting for User vs System installs, root vs user sudo, VirtualEnvs / Anaconda / GitHub Workflows/ Google Cloud Shell, Mac vs Linux library paths, and ignore failure option\n- `python_pip_install_if_absent.sh` - installs PyPI modules not already in Python libary path (OS or pip installed) for faster installations only where OS packages are already providing some of the modules, reducing time and failure rates in CI builds\n- `python_pip_install_for_script.sh` - installs PyPI modules for given script(s) if not already installed. Used for dynamic individual script dependency installation in the [DevOps Python tools](https://github.com/HariSekhon/DevOps-Python-tools) repo\n- `python_pip_reinstall_all_modules.sh` - reinstalls all PyPI modules which can fix some issues\n- `pythonpath.sh` - prints all Python libary search paths, one per line\n- `python_find_library_path.sh` - finds directory where a PyPI module is installed - without args finds the Python library base\n- `python_find_library_executable.sh` - finds directory where a PyPI module's CLI program is installed (system vs user, useful when it gets installed to a place that isn't in your `$PATH`, where `which` won't help)\n- `python_find_unused_pip_modules.sh` - finds PyPI modules that aren't used by any programs in the current directory tree\n- `python_find_duplicate_pip_requirements.sh` - finds duplicate PyPI modules listed for install under the directory tree (useful for deduping module installs in a project and across submodules)\n- `python_translate_import_module.sh` - converts Python import modules to PyPI module names, used by `python_pip_install_for_script.sh`\n- `python_translate_module_to_import.sh` - converts PyPI module names to Python import names, used by `python_pip_install_if_absent.sh` and `python_find_unused_pip_modules.sh`\n- `python_pyinstaller.sh` - creates [PyInstaller](https://pypi.org/project/pyinstaller/) self-contained Python programs with Python interpreter and all PyPI modules included\n- `python_pypi_versions.sh` - prints all available versions of a given PyPi module using the API\n\nSee also [Knowledge Base notes for Python](https://github.com/HariSekhon/Knowledge-Base/blob/main/python.md).\n\n### Perl\n\n`perl/` directory:\n\n- `perl_cpanm_install.sh` - bulk installs CPAN modules from mix of arguments / file lists / stdin, accounting for User vs System installs, root vs user sudo, [Perlbrew](https://perlbrew.pl/) / Google Cloud Shell environments, Mac vs Linux library paths, ignore failure option, auto finds and reads build failure log for quicker debugging showing root cause error in CI builds logs etc\n- `perl_cpanm_install_if_absent.sh` - installs CPAN modules not already in Perl libary path (OS or CPAN installed) for faster installations only where OS packages are already providing some of the modules, reducing time and failure rates in CI builds\n- `perl_cpanm_reinstall_all.sh` - re-installs all CPAN modules. Useful for trying to recompile XS modules on Macs after migration assistant from an Intel Mac to an ARM Silicon Mac leaves your home XS libraries broken as they're built for the wrong architecture\n- `perlpath.sh` - prints all Perl libary search paths, one per line\n- `perl_find_library_path.sh` - finds directory where a CPAN module is installed - without args finds the Perl library base\n- `perl_find_library_executable.sh` - finds directory where a CPAN module's CLI program is installed (system vs user, useful when it gets installed to a place that isn't in your `$PATH`, where `which` won't help)\n- `perl_find_unused_cpan_modules.sh` - finds CPAN modules that aren't used by any programs in the current directory tree\n- `perl_find_duplicate_cpan_requirements.sh` - finds duplicate CPAN modules listed for install more than once under the directory tree (useful for deduping module installs in a project and across submodules)\n- `perl_generate_fatpacks.sh` - creates [Fatpacks](https://metacpan.org/pod/App::FatPacker) - self-contained Perl programs with all CPAN modules built-in\n\nSee also [Knowledge Base notes for Perl](https://github.com/HariSekhon/Knowledge-Base/blob/main/perl.md).\n\n### Golang\n\n`packages/` directory:\n\n- `golang_install.sh` - bulk installs Golang modules from mix of arguments / file lists / stdin\n- `golang_install_if_absent.sh` - same as above but only if the package binary isn't already available in `$PATH`\n- `golang_rm_binaries.sh` - deletes binaries of the same name adjacent to `.go` files. Doesn't delete your `bin/` etc as these are often real deployed applications rather than development binaries\n\n### Media\n\n`media/` directory:\n\n#### Images\n\n- `image_trim_pixels.sh` - trims N pixels off one of the sides of an image. Useful to tweak screenshots before sharing them\n- `image_join_vertical.sh` - joins two images top and bottom after matching their widths so they align correctly\n- `image_join_horizontal.sh` - joins two images left and right after matching their heights so they align correctly\n- `imageopen.sh` - opens the given image file using whatever available tool is found on Linux or Mac\n- `svg_to_png.sh` - convert an SVG image to PNG to be usable on websites that don't support SVG images like LinkedIn, Medium or Reddit\n- `avif_to_png.sh` - convert an Avif image to PNG to be usable on websites that don't support Webp images like LinkedIn\n- `webp_to_png.sh` - convert a Webp image to PNG to be usable on websites that don't support Webp images like Medium\n\n#### Audio\n\n- `mp3_set_artist.sh` / `mp3_set_album.sh` - set the artist / album tag for all mp3 files under given directories. Useful for grouping artists/albums and audiobook author/books (eg. for correct importing into Mac's Books.app)\n- `mp3_set_track_name.sh` - set the track name metadata for mp3 files under given directories to follow their filenames. Useful for correctly displaying audiobook progress / chapters etc.\n- `mp3_set_track_order.sh` - set the track order metadata for mp3 files under given directories to follow the lexical file naming order. Useful for correctly ordering album songs and audiobook chapters (eg. for Mac's Books.app). Especially useful for enforcing global ordering on multi-CD audiobooks after grouping into a single audiobook using `mp3_set_album.sh` (otherwise default track numbers in each CD interleave in Mac's Books.app)\n\n#### Video\n\n- `avi_to_mp4.sh` - convert avi files to mp4 using ffmpeg. Useful to be able to play videos on devices like smart TVs that may not recognize newer codecs otherwise\n- `mkv_to_mp4.sh` - convert mkv files to mp4 using ffmpeg. Same use case as above\n- `youtube_download_channel.sh` - downloads all videos from a given YouTube channel URL\n\nSee also [Knowledge Base notes for MultiMedia](https://github.com/HariSekhon/Knowledge-Base/blob/main/multimedia.md).\n\n### Spotify\n\n40+ [Spotify](https://www.spotify.com/) API scripts (used extensively to manage my [Spotify-Playlists](https://github.com/HariSekhon/Spotify-Playlists) repo).\n\n`spotify/` directory:\n\n- `spotify_playlists*.sh` - list playlists in either `<id> <name>` or JSON format\n- `spotify_playlist_tracks*.sh` - gets playlist contents as track URIs / `Artists - Track` / CSV format - useful for backups or exports between music systems\n- `spotify_backup.sh` - backup all Spotify playlists as well as the ordered list of playlists\n- `spotify_backup_playlist*.sh` - backup Spotify playlists to local files in both human readable `Artist - Track` format and Spotify URI format for easy restores or adding to new playlists\n- `spotify_search*.sh` - search Spotify's library for tracks / albums / artists getting results in human readable format, JSON, or URI formats for easy loading to Spotify playlists\n- `spotify_release_year.sh` - searches for a given track or album and finds the original release year\n- `spotify_uri_to_name.sh` - convert Spotify track / album / artist URIs to human readable `Artist - Track` / CSV format. Takes Spotify URIs, URL links or just IDs. Reads URIs from files or standard input\n- `spotify_create_playlist.sh` - creates a Spotify playlist, either public or private\n- `spotify_rename_playlist.sh` - renames a Spotify playlist\n- `spotify_set_playlists_public.sh` / `spotify_set_playlists_private.sh` - sets one or more given Spotify playlists to public / private\n- `spotify_add_to_playlist.sh` - adds tracks to a given playlist. Takes a playlist name or ID and Spotify URIs in any form from files or standard input. Can be combined with many other tools listed here which output Spotify URIs, or appended from other playlists. Can also be used to restore a spotify playlist from backups\n- `spotify_delete_from_playlist.sh` - deletes tracks from a given playlist. Takes a playlist name or ID and Spotify URIs in any form from files or standard input, optionally prefixed with a track position to remove only specific occurrences (useful for removing duplicates from playlists)\n- `spotify_delete_from_playlist_if_in_other_playlists.sh` - deletes tracks from a given playlist if their URIs are found in the subsequently given playlists\n- `spotify_delete_from_playlist_if_track_in_other_playlists.sh` - deletes tracks from a given playlist if their 'Artist - Track' name match are found in the subsequently given playlists (less accurate than exact URI deletion above)\n- `spotify_duplicate_uri_in_playlist.sh` - finds duplicate Spotify URIs in a given playlist (these are guaranteed exact duplicate matches), returns all but the first occurrence and optionally their track positions (zero-indexed to align with the Spotify API for easy chaining with other tools)\n- `spotify_duplicate_tracks_in_playlist.sh` - finds duplicate Spotify tracks in a given playlist (these are idential `Artist - Track` name matches, which may be from different albums / singles)\n- `spotify_delete_duplicates_in_playlist.sh` - deletes duplicate Spotify URI tracks (identical) in a given playlist using `spotify_duplicate_uri_in_playlist.sh` and `spotify_delete_from_playlist.sh`\n- `spotify_delete_duplicate_tracks_in_playlist.sh` - deletes duplicate Spotify tracks (name matched) in a given playlist using `spotify_duplicate_tracks_in_playlist.sh` and `spotify_delete_from_playlist.sh`\n- `spotify_delete_any_duplicates_in_playlist.sh` - calls both of the above scripts to first get rid of duplicate URIs and then remove any other duplicates by track name matches\n- `spotify_playlist_tracks_uri_in_year.sh` - finds track URIs in a playlist where their original release date is in a given year or decade (by regex match)\n- `spotify_playlist_uri_offset.sh` - finds the offset of a given track URI in a given playlist, useful to find positions to resume processing a large playlist\n- `spotify_top_artists*.sh` - lists your top artists in URI or human readable format\n- `spotify_top_tracks*.sh` - lists top tracks in URI or human readable format\n- `spotify_liked_tracks*.sh` - lists your `Liked Songs` in URI or human readable formats\n- `spotify_liked_artists*.sh` - list artists from `Liked Songs` in URI or human readable formats\n- `spotify_artists_followed*.sh` - lists all followed artists in URI or human readable formats\n- `spotify_artist_tracks.sh` - gets all track URIs for a given artist, from both albums and single for chain loading to playlists\n- `spotify_follow_artists.sh` - follows artists for the given URIs from files or standard input\n- `spotify_follow_top_artists.sh` - follows all artists in your current Spotify top artists list\n- `spotify_follow_liked_artists.sh` - follows artists with N or more tracks in your `Liked Songs`\n- `spotify_set_tracks_uri_to_liked.sh` - sets a list of spotify track URIs to 'Liked' so they appear in the `Liked Songs` playlist. Useful for marking all the tracks in your best playlists as favourite tracks, or for porting historical `Starred` tracks to the newer `Liked Songs`\n- `spotify_foreach_playlist.sh` - executes a templated command against all playlists, replacing `{playlist}` and `{playlist_id}` in each iteration\n- `spotify_playlist_name_to_id.sh` / `spotify_playlist_id_to_name.sh` - convert playlist names <=> IDs\n- `spotify_api_token.sh` - gets a Spotify authentication token using either [Client Credentials](https://developer.spotify.com/documentation/general/guides/authorization-guide/#client-credentials-flow) or [Authorization Code](https://developer.spotify.com/documentation/general/guides/authorization-guide/#authorization-code-flow) authentication flows, the latter being able to read/modify private user data, automatically used by `spotify_api.sh`\n- `spotify_api.sh` - query any Spotify [API](https://developer.spotify.com/documentation/web-api/reference/) endpoint with authentication, used by adjacent spotify scripts\n\n### More Linux & Mac\n\n`bin/`, `install/`, `packages/`, `setup/` directories:\n\n- [Linux](https://en.wikipedia.org/wiki/Linux) / [Mac](https://en.wikipedia.org/wiki/MacOS) systems administration scripts:\n  - `install/` - installation scripts for various OS packages (RPM, Deb, Apk) for various Linux distros ([Redhat RHEL](https://www.redhat.com/en/technologies/linux-platforms/enterprise-linux) / [CentOS](https://www.centos.org/) / [Fedora](https://getfedora.org/), [Debian](https://www.debian.org/) / [Ubuntu](https://ubuntu.com/), [Alpine](https://alpinelinux.org/))\n  - install if absent scripts for Python, Perl, Ruby, NodeJS and Golang packages - good for minimizing the number of source code installs by first running the OS install scripts and then only building modules which aren't already detected as installed (provided by system packages), speeding up builds and reducing the likelihood of compile failures\n  - install scripts for tarballs, Golang binaries, random 3rd party installers, [Jython](https://www.jython.org/) and build tools like [Gradle](https://gradle.org/) and [SBT](https://www.scala-sbt.org/) for when Linux distros don't provide packaged versions or where the packaged versions are too old\n  - `packages/` - OS / Distro Package Management:\n    - `install_packages.sh` - installs package lists from arguments, files or stdin on major linux distros and Mac, detecting the package manager and invoking the right install commands, with `sudo` if not root. Works on [RHEL](https://www.redhat.com/en) / [CentOS](https://www.centos.org/) / [Fedora](https://getfedora.org/), [Debian](https://www.debian.org/) / [Ubuntu](https://ubuntu.com/), [Alpine](https://alpinelinux.org/), and [Mac Homebrew](https://brew.sh/). Leverages and supports all features of the distro / OS specific install scripts listed below\n    - `install_packages_if_absent.sh` - installs package lists if they're not already installed, saving time and minimizing install logs / CI logs, same support list as above\n    - Redhat RHEL / CentOS:\n      - `yum_install_packages.sh` / `yum_remove_packages.sh` - installs RPM lists from arguments, files or stdin. Handles Yum + Dnf behavioural differences, calls `sudo` if not root, auto-attempts variations of python/python2/python3 package names. Avoids yum slowness by checking if rpm is installed before attempting to install it, accepts `NO_FAIL=1` env var to ignore unavailable / changed package names (useful for optional packages or attempts for different package names across RHEL/CentOS/Fedora versions)\n      - `yum_install_packages_if_absent.sh` - installs RPMs only if not already installed and not a metapackage provided by other packages (eg. `vim` metapackage provided by `vim-enhanced`), saving time and minimizing install logs / CI logs, plus all the features of `yum_install_packages.sh` above\n      - `rpms_filter_installed.sh` / `rpms_filter_not_installed.sh` - pipe filter packages that are / are not installed for easy script piping\n    - Debian / Ubuntu:\n      - `apt_install_packages.sh` / `apt_remove_packages.sh` - installs Deb package lists from arguments, files or stdin. Auto calls `sudo` if not root, accepts `NO_FAIL=1` env var to ignore unavailable / changed package names (useful for optional packages or attempts for different package names across Debian/Ubuntu distros/versions)\n      - `apt_install_packages_if_absent.sh` - installs Deb packages only if not already installed, saving time and minimizing install logs / CI logs, plus all the features of `apt_install_packages.sh` above\n      - `apt_wait.sh` - blocking wait on concurrent apt locks to avoid failures and continue when available, mimicking yum's waiting behaviour rather than error'ing out\n      - `debs_filter_installed.sh` / `debs_filter_not_installed.sh` - pipe filter packages that are / are not installed for easy script piping\n    - Alpine:\n      - `apk_install_packages.sh` / `apk_remove_packages.sh` - installs Alpine apk package lists from arguments, files or stdin. Auto calls `sudo` if not root, accepts `NO_FAIL=1` env var to ignore unavailable / changed package names (useful for optional packages or attempts for different package names across Alpine versions)\n      - `apk_install_packages_if_absent.sh` - installs Alpine apk packages only if not already installed, saving time and minimizing install logs / CI logs, plus all the features of `apk_install_packages.sh` above\n      - `apk_filter_installed.sh` / `apk_filter_not_installed.sh` - pipe filter packages that are / are not installed for easy script piping\n    - Mac:\n      - `brew_install_packages.sh` / `brew_remove_packages.sh` - installs Mac Hombrew package lists from arguments, files or stdin. Accepts `NO_FAIL=1` env var to ignore unavailable / changed package names (useful for optional packages or attempts for different package names across versions)\n      - `brew_install_packages_if_absent.sh` - installs Mac Homebrew packages only if not already installed, saving time and minimizing install logs / CI logs, plus all the features of `brew_install_packages.sh` above\n      - `brew_filter_installed.sh` / `brew_filter_not_installed.sh` - pipe filter packages that are / are not installed for easy script piping\n      - `brew_package_owns.sh` - finds which brew package owns a given filename argument\n- all builds across all my GitHub repos now `make system-packages` before `make pip` / `make cpan` to shorten how many packages need installing, reducing chances of build failures\n\n### Builds, Languages & Linting\n\n`bin/`, `checks/`, `cicd/` or language specific directories:\n\n- `lint.sh` - lints one or more files, auto-determines the file types, parses lint headers and calls appropriate scripts and tools. Integrated with my custom `.vimrc`\n- `run.sh` - runs one or more files, auto-determines the file types, any run or arg headers and executes each file using the appropriate script or CLI tool. Integrated with my custom `.vimrc`\n- `check_*.sh` - extensive collection of generalized tests - these run against all my GitHub repos via [CI](https://harisekhon.github.io/CI-CD/). Some examples:\n\n  - Programming language linting:\n\n    - [Python](https://www.python.org/) (syntax, pep8, byte-compiling, reliance on asserts which can be disabled at runtime, except/pass etc.)\n    - [Perl](https://www.perl.org/)\n    - [Java](https://www.java.com/en/)\n    - [Scala](https://www.scala-lang.org/)\n    - [Ruby](https://www.ruby-lang.org/en/)\n    - [Bash](https://www.gnu.org/software/bash/) / Shell\n    - Misc (whitespace, custom code checks etc.)\n\n  - Build System, Docker & CI linting:\n\n    - [Make](https://www.gnu.org/software/make/)\n    - [Maven](https://maven.apache.org/)\n    - [SBT](https://www.scala-sbt.org/)\n    - [Gradle](https://gradle.org/)\n    - [Travis CI](https://travis-ci.org/)\n    - [Circle CI](https://circleci.com/)\n    - [GitLab CI](https://docs.gitlab.com/ee/ci/)\n    - [Concourse CI](https://concourse-ci.org/)\n    - [Codefresh CI](https://codefresh.io/)\n    - [Dockerfiles](https://docs.docker.com/engine/reference/builder/)\n    - [Docker Compose](https://docs.docker.com/compose/)\n    - [Vagrantfiles](https://www.vagrantup.com/docs/vagrantfile)\n\n## Individual Setup Parts\n\nOptional, only if you don't do the full `make install`.\n\nInstall only OS system package dependencies and [AWS CLI](https://aws.amazon.com/cli/) via Python Pip (doesn't symlink anything to `$HOME`):\n\n```shell\nmake\n```\n\nAdds sourcing to `.bashrc` and `.bash_profile` and symlinks dot config files to `$HOME` (doesn't install OS system package dependencies):\n\n```shell\nmake link\n```\n\nundo via\n\n```shell\nmake unlink\n```\n\nInstall only OS system package dependencies (doesn't include [AWS CLI](https://aws.amazon.com/cli/) or Python packages):\n\n```shell\nmake system-packages\n```\n\nInstall [AWS CLI](https://aws.amazon.com/cli/):\n\n```shell\nmake aws\n```\n\nInstall [Azure CLI](https://docs.microsoft.com/en-us/cli/azure/):\n\n```shell\nmake azure\n```\n\nInstall [GCP GCloud SDK](https://cloud.google.com/sdk) (includes CLI):\n\n```shell\nmake gcp\n```\n\nInstall [GCP GCloud Shell](https://cloud.google.com/shell) environment (sets up persistent OS packages and all home directory configs):\n\n```shell\nmake gcp-shell\n```\n\nInstall generically useful Python CLI tools and modules (includes [AWS CLI](https://aws.amazon.com/cli/), autopep8 etc):\n\n```shell\nmake python\n```\n\n### Full Help\n\n```shell\n> make help\n\n Usage:\n\n  Common Options:\n\n    make help                   show this message\n    make build                  installs all dependencies - OS packages and any language libraries via native tools eg. pip, cpanm, gem, go etc that are not available via OS packages\n    make build-retry            retries 'make build' x 3 until success to try to mitigate temporary upstream repo failures triggering false alerts in CI systems\n    make ci                     prints env, then runs 'build-retry' for more resilient CI builds with debugging\n    make printenv               prints environment variables, CPU cores, OS release, $PWD, Git branch, hashref etc. Useful for CI debugging\n    make system-packages        installs OS packages only (detects OS via whichever package manager is available)\n    make test                   run tests\n    make clean                  removes compiled / generated files, downloaded tarballs, temporary files etc.\n\n    make submodules             initialize and update submodules to the right release (done automatically by build / system-packages)\n    make init                   same as above, often useful to do in CI systems to get access to additional submodule provided targets such as 'make ci'\n\n    make cpan                   install any modules listed in any cpan-requirements.txt files if not already installed\n\n    make pip                    install any modules listed in any requirements.txt files if not already installed\n\n    make python-compile         compile any python files found in the current directory and 1 level of subdirectory\n    make pycompile\n\n    make github                 open browser at github project\n    make readme                 open browser at github's README\n    make github-url             print github url and copy to clipboard\n    make status                 open browser at Github CI Builds overview Status page for all projects\n\n    make ls                     print list of code files in project\n    make wc                     show counts of files and lines\n\n  Repo specific options:\n\n    make install                builds all script dependencies, installs AWS CLI, symlinks all config files to $HOME and adds sourcing of bash profile\n\n    make link                   symlinks all config files to $HOME and adds sourcing of bash profile\n    make unlink                 removes all symlinks pointing to this repo's config files and removes the sourcing lines from .bashrc and .bash_profile\n\n    make python-desktop         installs all Python Pip packages for desktop workstation listed in setup/pip-packages-desktop.txt\n    make perl-desktop           installs all Perl CPAN packages for desktop workstation listed in setup/cpan-packages-desktop.txt\n    make ruby-desktop           installs all Ruby Gem packages for desktop workstation listed in setup/gem-packages-desktop.txt\n    make golang-desktop         installs all Golang packages for desktop workstation listed in setup/go-packages-desktop.txt\n    make nodejs-desktop         installs all NodeJS packages for desktop workstation listed in setup/npm-packages-desktop.txt\n\n    make desktop                installs all of the above + many desktop OS packages listed in setup/\n\n    make mac-desktop            all of the above + installs a bunch of major common workstation software packages like Ansible, Terraform, MiniKube, MiniShift, SDKman, Travis CI, CCMenu, Parquet tools etc.\n    make linux-desktop\n\n    make ls-scripts             print list of scripts in this project, ignoring code libraries in lib/ and .bash.d/\n\n    make github-cli             installs GitHub CLI\n    make kubernetes             installs Kubernetes kubectl and kustomize to ~/bin/\n    make terraform              installs Terraform to ~/bin/\n    make vim                    installs Vundle and plugins\n    make tmux                   installs TMUX TPM and plugin for kubernetes context\n    make ccmenu                 installs and (re)configures CCMenu to watch this and all other major HariSekhon GitHub repos\n    make status                 open the Github Status page of all my repos build statuses across all CI platforms\n\n    make aws                    installs AWS CLI tools\n    make azure                  installs Azure CLI\n    make gcp                    installs Google Cloud SDK\n    make digital-ocean          installs Digital Ocean CLI\n\n    make aws-shell              sets up AWS Cloud Shell: installs core packages and links configs\n                                (maintains itself across future Cloud Shells via .aws_customize_environment hook)\n    make gcp-shell              sets up GCP Cloud Shell: installs core packages and links configs\n                                (maintains itself across future Cloud Shells via .customize_environment hook)\n    make azure-shell            sets up Azure Cloud Shell (limited compared to gcp-shell, doesn't install OS packages since there is no sudo)\n\nNow exiting usage help with status code 3 to explicitly prevent silent build failures from stray 'help' arguments\nmake: *** [help] Error 3\n```\n\n(`make help` exits with error code 3 like most of my programs to differentiate from build success to make sure a stray `help` argument doesn't cause silent build failure with exit code 0)\n\n## Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=HariSekhon/DevOps-Bash-tools&type=Date)](https://star-history.com/#HariSekhon/DevOps-Bash-tools&Date)\n\n[git.io/bash-tools](https://git.io/bash-tools)\n\n## More Core Repos\n\n<!-- OTHER_REPOS_START -->\n\n### Knowledge\n\n[![Readme Card](https://github-readme-stats.vercel.app/api/pin/?username=HariSekhon&repo=Knowledge-Base&theme=ambient_gradient&description_lines_count=3)](https://github.com/HariSekhon/Knowledge-Base)\n[![Readme Card](https://github-readme-stats.vercel.app/api/pin/?username=HariSekhon&repo=Diagrams-as-Code&theme=ambient_gradient&description_lines_count=3)](https://github.com/HariSekhon/Diagrams-as-Code)\n\n<!--\n\nNot support on GitHub Markdown:\n\n<iframe src=\"https://raw.githubusercontent.com/HariSekhon/HariSekhon/main/knowledge.md\" width=\"100%\" height=\"500px\"></iframe>\n\nDoes nothing:\n\n<embed src=\"https://raw.githubusercontent.com/HariSekhon/HariSekhon/main/knowledge.md\" width=\"100%\" height=\"500px\" />\n\n-->\n\n### DevOps Code\n\n[![Readme Card](https://github-readme-stats.vercel.app/api/pin/?username=HariSekhon&repo=DevOps-Bash-tools&theme=ambient_gradient&description_lines_count=3)](https://github.com/HariSekhon/DevOps-Bash-tools)\n[![Readme Card](https://github-readme-stats.vercel.app/api/pin/?username=HariSekhon&repo=DevOps-Python-tools&theme=ambient_gradient&description_lines_count=3)](https://github.com/HariSekhon/DevOps-Python-tools)\n[![Readme Card](https://github-readme-stats.vercel.app/api/pin/?username=HariSekhon&repo=DevOps-Perl-tools&theme=ambient_gradient&description_lines_count=3)](https://github.com/HariSekhon/DevOps-Perl-tools)\n[![Readme Card](https://github-readme-stats.vercel.app/api/pin/?username=HariSekhon&repo=DevOps-Golang-tools&theme=ambient_gradient&description_lines_count=3)](https://github.com/HariSekhon/DevOps-Golang-tools)\n\n<!--\n[![Gist Card](https://github-readme-stats.vercel.app/api/gist?id=f8f551332440f1ca8897ff010e363e03)](https://gist.github.com/HariSekhon/f8f551332440f1ca8897ff010e363e03)\n-->\n\n### Containerization\n\n[![Readme Card](https://github-readme-stats.vercel.app/api/pin/?username=HariSekhon&repo=Kubernetes-configs&theme=ambient_gradient&description_lines_count=3)](https://github.com/HariSekhon/Kubernetes-configs)\n[![Readme Card](https://github-readme-stats.vercel.app/api/pin/?username=HariSekhon&repo=Dockerfiles&theme=ambient_gradient&description_lines_count=3)](https://github.com/HariSekhon/Dockerfiles)\n\n### CI/CD\n\n[![Readme Card](https://github-readme-stats.vercel.app/api/pin/?username=HariSekhon&repo=GitHub-Actions&theme=ambient_gradient&description_lines_count=3)](https://github.com/HariSekhon/GitHub-Actions)\n[![Readme Card](https://github-readme-stats.vercel.app/api/pin/?username=HariSekhon&repo=Jenkins&theme=ambient_gradient&description_lines_count=3)](https://github.com/HariSekhon/Jenkins)\n\n### DBA - SQL\n\n[![Readme Card](https://github-readme-stats.vercel.app/api/pin/?username=HariSekhon&repo=SQL-scripts&theme=ambient_gradient&description_lines_count=3)](https://github.com/HariSekhon/SQL-scripts)\n\n### DevOps Reloaded\n\n[![Readme Card](https://github-readme-stats.vercel.app/api/pin/?username=HariSekhon&repo=Nagios-Plugins&theme=ambient_gradient&description_lines_count=3)](https://github.com/HariSekhon/Nagios-Plugins)\n[![Readme Card](https://github-readme-stats.vercel.app/api/pin/?username=HariSekhon&repo=HAProxy-configs&theme=ambient_gradient&description_lines_count=3)](https://github.com/HariSekhon/HAProxy-configs)\n[![Readme Card](https://github-readme-stats.vercel.app/api/pin/?username=HariSekhon&repo=Terraform&theme=ambient_gradient&description_lines_count=3)](https://github.com/HariSekhon/Terraform)\n[![Readme Card](https://github-readme-stats.vercel.app/api/pin/?username=HariSekhon&repo=Packer-templates&theme=ambient_gradient&description_lines_count=3)](https://github.com/HariSekhon/Packer-templates)\n[![Readme Card](https://github-readme-stats.vercel.app/api/pin/?username=HariSekhon&repo=Nagios-Plugin-Kafka&theme=ambient_gradient&description_lines_count=3)](https://github.com/HariSekhon/Nagios-Plugin-Kafka)\n\n### Templates\n\n[![Readme Card](https://github-readme-stats.vercel.app/api/pin/?username=HariSekhon&repo=Templates&theme=ambient_gradient&description_lines_count=3)](https://github.com/HariSekhon/Templates)\n[![Readme Card](https://github-readme-stats.vercel.app/api/pin/?username=HariSekhon&repo=Template-repo&theme=ambient_gradient&description_lines_count=3)](https://github.com/HariSekhon/Template-repo)\n\n### Misc\n\n[![Readme Card](https://github-readme-stats.vercel.app/api/pin/?username=HariSekhon&repo=Spotify-tools&theme=ambient_gradient&description_lines_count=3)](https://github.com/HariSekhon/Spotify-tools)\n[![Readme Card](https://github-readme-stats.vercel.app/api/pin/?username=HariSekhon&repo=Spotify-playlists&theme=ambient_gradient&description_lines_count=3)](https://github.com/HariSekhon/Spotify-playlists)\n\nThe rest of my original source repos are\n[here](https://github.com/HariSekhon?tab=repositories&q=&type=source&language=&sort=stargazers).\n\nPre-built Docker images are available on my [DockerHub](https://hub.docker.com/u/harisekhon/).\n\n<!-- 1x1 pixel counter to record hits -->\n![](https://hit.yhype.me/github/profile?user_id=2211051)\n\n<!-- OTHER_REPOS_END -->\n"
        },
        {
          "name": "STARCHARTS.md",
          "type": "blob",
          "size": 16.486328125,
          "content": "# GitHub StarCharts\n\n![Original Repos](https://img.shields.io/badge/Repos-20-blue?logo=github)\n![Stars](https://img.shields.io/badge/Stars-7542-blue?logo=github)\n![Forks](https://img.shields.io/badge/Forks-2603-blue?logo=github)\n![Followers](https://img.shields.io/badge/Followers-1568-blue?logo=github)\n[![Azure DevOps Profile](https://img.shields.io/badge/Azure%20DevOps-HariSekhon-0078D7?logo=azure%20devops)](https://dev.azure.com/harisekhon/GitHub)\n[![GitHub Profile](https://img.shields.io/badge/GitHub-HariSekhon-2088FF?logo=github)](https://github.com/HariSekhon)\n[![GitLab Profile](https://img.shields.io/badge/GitLab-HariSekhon-FCA121?logo=gitlab)](https://gitlab.com/HariSekhon)\n[![BitBucket Profile](https://img.shields.io/badge/BitBucket-HariSekhon-0052CC?logo=bitbucket)](https://bitbucket.org/HariSekhon)\n\n[![GitStar Ranking Profile](https://img.shields.io/badge/GitStar%20Ranking-HariSekhon-blue?logo=github)](https://gitstar-ranking.com/HariSekhon)\n\n[git.io/hari-starcharts](https://git.io/hari-starcharts) generated by `github_generate_starcharts.md.sh` in [HariSekhon/DevOps-Bash-tools](https://github.com/HariSekhon/DevOps-Bash-tools)\n\n---\n## Hari Sekhon - DevOps Bash Tools\n\n[![Repo on GitHub](https://img.shields.io/badge/GitHub-repo-blue?logo=github)](https://github.com/HariSekhon/DevOps-Bash-tools)\n[![GitHub stars](https://img.shields.io/github/stars/HariSekhon/DevOps-Bash-tools?logo=github)](https://github.com/HariSekhon/DevOps-Bash-tools/stargazers)\n[![GitHub forks](https://img.shields.io/github/forks/HariSekhon/DevOps-Bash-tools?logo=github)](https://github.com/HariSekhon/DevOps-Bash-tools/network)\n\n1000+ DevOps Bash Scripts - AWS, GCP, Kubernetes, Docker, CI/CD, APIs, SQL, PostgreSQL, MySQL, Hive, Impala, Kafka, Hadoop, Jenkins, GitHub, GitLab, BitBucket, Azure DevOps, TeamCity, Spotify, MP3, LDAP, Code/Build Linting, pkg mgmt for Linux, Mac, Python, Perl, Ruby, NodeJS, Golang, Advanced dotfiles: .bashrc, .vimrc, .gitconfig, .screenrc, tmux..\n\n[![Stargazers over time](https://starchart.cc/HariSekhon/DevOps-Bash-tools.svg)](https://starchart.cc/HariSekhon/DevOps-Bash-tools)\n\n---\n## Dockerfiles for DevOps, CI/CD, Big Data & NoSQL\n\n[![Repo on GitHub](https://img.shields.io/badge/GitHub-repo-blue?logo=github)](https://github.com/HariSekhon/Dockerfiles)\n[![GitHub stars](https://img.shields.io/github/stars/HariSekhon/Dockerfiles?logo=github)](https://github.com/HariSekhon/Dockerfiles/stargazers)\n[![GitHub forks](https://img.shields.io/github/forks/HariSekhon/Dockerfiles?logo=github)](https://github.com/HariSekhon/Dockerfiles/network)\n\n50+ DockerHub public images for Docker & Kubernetes - DevOps, CI/CD, GitHub Actions, CircleCI, Jenkins, TeamCity, Alpine, CentOS, Debian, Fedora, Ubuntu, Hadoop, Kafka, ZooKeeper, HBase, Cassandra, Solr, SolrCloud, Presto, Apache Drill, Nifi, Spark, Consul, Riak\n\n[![Stargazers over time](https://starchart.cc/HariSekhon/Dockerfiles.svg)](https://starchart.cc/HariSekhon/Dockerfiles)\n\n---\n## Advanced Nagios Plugins Collection\n\n[![Repo on GitHub](https://img.shields.io/badge/GitHub-repo-blue?logo=github)](https://github.com/HariSekhon/Nagios-Plugins)\n[![GitHub stars](https://img.shields.io/github/stars/HariSekhon/Nagios-Plugins?logo=github)](https://github.com/HariSekhon/Nagios-Plugins/stargazers)\n[![GitHub forks](https://img.shields.io/github/forks/HariSekhon/Nagios-Plugins?logo=github)](https://github.com/HariSekhon/Nagios-Plugins/network)\n\n450+ AWS, Hadoop, Cloud, Kafka, Docker, Elasticsearch, RabbitMQ, Redis, HBase, Solr, Cassandra, ZooKeeper, HDFS, Yarn, Hive, Presto, Drill, Impala, Consul, Spark, Jenkins, Travis CI, Git, MySQL, Linux, DNS, Whois, SSL Certs, Yum Security Updates, Kubernetes, Cloudera etc...\n\n[![Stargazers over time](https://starchart.cc/HariSekhon/Nagios-Plugins.svg)](https://starchart.cc/HariSekhon/Nagios-Plugins)\n\n---\n## Hari Sekhon - DevOps Python Tools\n\n[![Repo on GitHub](https://img.shields.io/badge/GitHub-repo-blue?logo=github)](https://github.com/HariSekhon/DevOps-Python-tools)\n[![GitHub stars](https://img.shields.io/github/stars/HariSekhon/DevOps-Python-tools?logo=github)](https://github.com/HariSekhon/DevOps-Python-tools/stargazers)\n[![GitHub forks](https://img.shields.io/github/forks/HariSekhon/DevOps-Python-tools?logo=github)](https://github.com/HariSekhon/DevOps-Python-tools/network)\n\n80+ DevOps & Data CLI Tools - AWS, GCP, GCF Python Cloud Functions, Log Anonymizer, Spark, Hadoop, HBase, Hive, Impala, Linux, Docker, Spark Data Converters & Validators (Avro/Parquet/JSON/CSV/INI/XML/YAML), Travis CI, AWS CloudFormation, Elasticsearch, Solr etc.\n\n[![Stargazers over time](https://starchart.cc/HariSekhon/DevOps-Python-tools.svg)](https://starchart.cc/HariSekhon/DevOps-Python-tools)\n\n---\n## Kubernetes configs\n\n[![Repo on GitHub](https://img.shields.io/badge/GitHub-repo-blue?logo=github)](https://github.com/HariSekhon/Kubernetes-configs)\n[![GitHub stars](https://img.shields.io/github/stars/HariSekhon/Kubernetes-configs?logo=github)](https://github.com/HariSekhon/Kubernetes-configs/stargazers)\n[![GitHub forks](https://img.shields.io/github/forks/HariSekhon/Kubernetes-configs?logo=github)](https://github.com/HariSekhon/Kubernetes-configs/network)\n\nAdvanced Kubernetes YAML configs - Best Practices, Tips & Tricks, Production-Ready Checklist - experience from several production environments. AWS, GCP, Azure, ArgoCD, GKE, EKS, AKS, Nginx, Traefik, Kong, Cert Manager, CI/CD, Jenkins, Artifactory, TeamCity, GitHub Actions, Cloud SQL, FluxCD, Spinnaker, Selenium Grid, Moon, Helm + Kustomize\n\n[![Stargazers over time](https://starchart.cc/HariSekhon/Kubernetes-configs.svg)](https://starchart.cc/HariSekhon/Kubernetes-configs)\n\n---\n## SQL Scripts\n\n[![Repo on GitHub](https://img.shields.io/badge/GitHub-repo-blue?logo=github)](https://github.com/HariSekhon/SQL-scripts)\n[![GitHub stars](https://img.shields.io/github/stars/HariSekhon/SQL-scripts?logo=github)](https://github.com/HariSekhon/SQL-scripts/stargazers)\n[![GitHub forks](https://img.shields.io/github/forks/HariSekhon/SQL-scripts?logo=github)](https://github.com/HariSekhon/SQL-scripts/network)\n\n100+ SQL Scripts - PostgreSQL, MySQL, Google BigQuery, MariaDB, AWS Athena. DBA, Analytics, DevOps, performance engineering. Google BigQuery ML machine learning classification.\n\n[![Stargazers over time](https://starchart.cc/HariSekhon/SQL-scripts.svg)](https://starchart.cc/HariSekhon/SQL-scripts)\n\n---\n## Advanced HAProxy Configs for Big Data, NoSQL, Web and Infrastructure technologies\n\n[![Repo on GitHub](https://img.shields.io/badge/GitHub-repo-blue?logo=github)](https://github.com/HariSekhon/HAProxy-configs)\n[![GitHub stars](https://img.shields.io/github/stars/HariSekhon/HAProxy-configs?logo=github)](https://github.com/HariSekhon/HAProxy-configs/stargazers)\n[![GitHub forks](https://img.shields.io/github/forks/HariSekhon/HAProxy-configs?logo=github)](https://github.com/HariSekhon/HAProxy-configs/network)\n\n80+ HAProxy Configs for Hadoop, Big Data, NoSQL, Docker, Kubernetes, Elasticsearch, SolrCloud, HBase, MySQL, PostgreSQL, Apache Drill, Hive, Presto, Impala, Hue, ZooKeeper, SSH, RabbitMQ, Redis, Riak, Cloudera, OpenTSDB, InfluxDB, Prometheus, Kibana, Graphite, Rancher etc.\n\n[![Stargazers over time](https://starchart.cc/HariSekhon/HAProxy-configs.svg)](https://starchart.cc/HariSekhon/HAProxy-configs)\n\n---\n## Hari Sekhon - Diagrams-as-Code\n\n[![Repo on GitHub](https://img.shields.io/badge/GitHub-repo-blue?logo=github)](https://github.com/HariSekhon/Diagrams-as-Code)\n[![GitHub stars](https://img.shields.io/github/stars/HariSekhon/Diagrams-as-Code?logo=github)](https://github.com/HariSekhon/Diagrams-as-Code/stargazers)\n[![GitHub forks](https://img.shields.io/github/forks/HariSekhon/Diagrams-as-Code?logo=github)](https://github.com/HariSekhon/Diagrams-as-Code/network)\n\nCloud & DevOps Architecture Diagrams-as-Code in Python and D2 languages\n\n[![Stargazers over time](https://starchart.cc/HariSekhon/Diagrams-as-Code.svg)](https://starchart.cc/HariSekhon/Diagrams-as-Code)\n\n---\n## Code & Config Templates\n\n[![Repo on GitHub](https://img.shields.io/badge/GitHub-repo-blue?logo=github)](https://github.com/HariSekhon/Templates)\n[![GitHub stars](https://img.shields.io/github/stars/HariSekhon/Templates?logo=github)](https://github.com/HariSekhon/Templates/stargazers)\n[![GitHub forks](https://img.shields.io/github/forks/HariSekhon/Templates?logo=github)](https://github.com/HariSekhon/Templates/network)\n\n100+ DevOps Code & Config templates for Kubernetes, AWS, GCP, Terraform, Docker, Packer, Jenkins, CircleCI, GitHub Actions, Lambda, AWS CodeBuild, GCP Cloud Build, Vagrant, Puppet, Python, Bash, Go, Perl, Java, Scala, Groovy, Maven, SBT, Gradle, Make, Jenkinsfile, Makefile, Dockerfile, docker-compose.yml, Vagrantfile, M4 etc...\n\n[![Stargazers over time](https://starchart.cc/HariSekhon/Templates.svg)](https://starchart.cc/HariSekhon/Templates)\n\n---\n## Hari Sekhon - DevOps Perl Tools\n\n[![Repo on GitHub](https://img.shields.io/badge/GitHub-repo-blue?logo=github)](https://github.com/HariSekhon/DevOps-Perl-tools)\n[![GitHub stars](https://img.shields.io/github/stars/HariSekhon/DevOps-Perl-tools?logo=github)](https://github.com/HariSekhon/DevOps-Perl-tools/stargazers)\n[![GitHub forks](https://img.shields.io/github/forks/HariSekhon/DevOps-Perl-tools?logo=github)](https://github.com/HariSekhon/DevOps-Perl-tools/network)\n\n25+ DevOps CLI Tools - Anonymizer, SQL ReCaser (MySQL, PostgreSQL, AWS Redshift, Snowflake, Apache Drill, Hive, Impala, Cassandra CQL, Microsoft SQL Server, Oracle, Couchbase N1QL, Dockerfiles), Hadoop HDFS & Hive tools, Solr/SolrCloud CLI, Nginx stats & HTTP(S) URL watchers for load-balanced web farms, Linux tools etc.\n\n[![Stargazers over time](https://starchart.cc/HariSekhon/DevOps-Perl-tools.svg)](https://starchart.cc/HariSekhon/DevOps-Perl-tools)\n\n---\n## Spotify Tools\n\n[![Repo on GitHub](https://img.shields.io/badge/GitHub-repo-blue?logo=github)](https://github.com/HariSekhon/Spotify-tools)\n[![GitHub stars](https://img.shields.io/github/stars/HariSekhon/Spotify-tools?logo=github)](https://github.com/HariSekhon/Spotify-tools/stargazers)\n[![GitHub forks](https://img.shields.io/github/forks/HariSekhon/Spotify-tools?logo=github)](https://github.com/HariSekhon/Spotify-tools/network)\n\nSpotify Tools - Playlists Backups, Spotify CLI, URI translator, duplication detection / removal, API search queries, API automation etc.\n\n[![Stargazers over time](https://starchart.cc/HariSekhon/Spotify-tools.svg)](https://starchart.cc/HariSekhon/Spotify-tools)\n\n---\n##  # Jenkins - Advanced Jenkinsfile & Groovy Shared Library\n\n[![Repo on GitHub](https://img.shields.io/badge/GitHub-repo-blue?logo=github)](https://github.com/HariSekhon/Jenkins)\n[![GitHub stars](https://img.shields.io/github/stars/HariSekhon/Jenkins?logo=github)](https://github.com/HariSekhon/Jenkins/stargazers)\n[![GitHub forks](https://img.shields.io/github/forks/HariSekhon/Jenkins?logo=github)](https://github.com/HariSekhon/Jenkins/network)\n\nJenkins - Advanced Jenkinsfile & Groovy Shared Library of reusable functions and pipelines - including for AWS, GCP, Docker, Kubernetes, ArgoCD, Slack notifications, Git Merge, Terraform, Cloudflare, Jenkins Job Backups, most major Docker registries, DockerHub, GHCR, ECR, GCR, GAR, ACR, GitLab, Quay\n\n[![Stargazers over time](https://starchart.cc/HariSekhon/Jenkins.svg)](https://starchart.cc/HariSekhon/Jenkins)\n\n---\n## Hari Sekhon - Knowledge Base from 20 years in DevOps, Linux, Cloud, Big Data, Security, AWS, GCP etc.\n\n[![Repo on GitHub](https://img.shields.io/badge/GitHub-repo-blue?logo=github)](https://github.com/HariSekhon/Knowledge-Base)\n[![GitHub stars](https://img.shields.io/github/stars/HariSekhon/Knowledge-Base?logo=github)](https://github.com/HariSekhon/Knowledge-Base/stargazers)\n[![GitHub forks](https://img.shields.io/github/forks/HariSekhon/Knowledge-Base?logo=github)](https://github.com/HariSekhon/Knowledge-Base/network)\n\nIT Knowledge Base from 20 years in DevOps, Linux, Cloud, Big Data, AWS, GCP etc - gradually porting my large private knowledge base to public\n\n[![Stargazers over time](https://starchart.cc/HariSekhon/Knowledge-Base.svg)](https://starchart.cc/HariSekhon/Knowledge-Base)\n\n---\n## Hari Sekhon - DevOps Golang Tools\n\n[![Repo on GitHub](https://img.shields.io/badge/GitHub-repo-blue?logo=github)](https://github.com/HariSekhon/DevOps-Golang-tools)\n[![GitHub stars](https://img.shields.io/github/stars/HariSekhon/DevOps-Golang-tools?logo=github)](https://github.com/HariSekhon/DevOps-Golang-tools/stargazers)\n[![GitHub forks](https://img.shields.io/github/forks/HariSekhon/DevOps-Golang-tools?logo=github)](https://github.com/HariSekhon/DevOps-Golang-tools/network)\n\nDevOps Golang tools\n\n[![Stargazers over time](https://starchart.cc/HariSekhon/DevOps-Golang-tools.svg)](https://starchart.cc/HariSekhon/DevOps-Golang-tools)\n\n---\n## Terraform Templates\n\n[![Repo on GitHub](https://img.shields.io/badge/GitHub-repo-blue?logo=github)](https://github.com/HariSekhon/Terraform)\n[![GitHub stars](https://img.shields.io/github/stars/HariSekhon/Terraform?logo=github)](https://github.com/HariSekhon/Terraform/stargazers)\n[![GitHub forks](https://img.shields.io/github/forks/HariSekhon/Terraform?logo=github)](https://github.com/HariSekhon/Terraform/network)\n\nTerraform HCL code for AWS / GCP / Azure / GitHub management\n\n[![Stargazers over time](https://starchart.cc/HariSekhon/Terraform.svg)](https://starchart.cc/HariSekhon/Terraform)\n\n---\n## GitHub Actions\n\n[![Repo on GitHub](https://img.shields.io/badge/GitHub-repo-blue?logo=github)](https://github.com/HariSekhon/GitHub-Actions)\n[![GitHub stars](https://img.shields.io/github/stars/HariSekhon/GitHub-Actions?logo=github)](https://github.com/HariSekhon/GitHub-Actions/stargazers)\n[![GitHub forks](https://img.shields.io/github/forks/HariSekhon/GitHub-Actions?logo=github)](https://github.com/HariSekhon/GitHub-Actions/network)\n\nGitHub Actions Reusable Workflows and Master Template\n\n[![Stargazers over time](https://starchart.cc/HariSekhon/GitHub-Actions.svg)](https://starchart.cc/HariSekhon/GitHub-Actions)\n\n---\n## Hari Sekhon - HashiCorp Packer templates\n\n[![Repo on GitHub](https://img.shields.io/badge/GitHub-repo-blue?logo=github)](https://github.com/HariSekhon/Packer-templates)\n[![GitHub stars](https://img.shields.io/github/stars/HariSekhon/Packer-templates?logo=github)](https://github.com/HariSekhon/Packer-templates/stargazers)\n[![GitHub forks](https://img.shields.io/github/forks/HariSekhon/Packer-templates?logo=github)](https://github.com/HariSekhon/Packer-templates/network)\n\nHashiCorp Packer templates to build portable virtual machines in OVA format for Ubuntu, Debian and Redhat based systems with automated installers Kickstart, Preseed and AutoInstaller / Cloud-Init. Useful for IoT edge sites, Kubernetes base systems and VM appliances to ship to customers\n\n[![Stargazers over time](https://starchart.cc/HariSekhon/Packer-templates.svg)](https://starchart.cc/HariSekhon/Packer-templates)\n\n---\n## Hari Sekhon - Spotify Playlists\n\n[![Repo on GitHub](https://img.shields.io/badge/GitHub-repo-blue?logo=github)](https://github.com/HariSekhon/Spotify-Playlists)\n[![GitHub stars](https://img.shields.io/github/stars/HariSekhon/Spotify-Playlists?logo=github)](https://github.com/HariSekhon/Spotify-Playlists/stargazers)\n[![GitHub forks](https://img.shields.io/github/forks/HariSekhon/Spotify-Playlists?logo=github)](https://github.com/HariSekhon/Spotify-Playlists/network)\n\n240+ playlists, 36,000+ tracks - in both Spotify URI and human-readable formats. Spotify Profile: https://open.spotify.com/user/harisekhon. Spotify API tools are submodules of this repo.\n\n[![Stargazers over time](https://starchart.cc/HariSekhon/Spotify-Playlists.svg)](https://starchart.cc/HariSekhon/Spotify-Playlists)\n\n---\n## Hari Sekhon - Perl Library\n\n[![Repo on GitHub](https://img.shields.io/badge/GitHub-repo-blue?logo=github)](https://github.com/HariSekhon/lib)\n[![GitHub stars](https://img.shields.io/github/stars/HariSekhon/lib?logo=github)](https://github.com/HariSekhon/lib/stargazers)\n[![GitHub forks](https://img.shields.io/github/forks/HariSekhon/lib?logo=github)](https://github.com/HariSekhon/lib/network)\n\nPerl Utility Library for my other repos\n\n[![Stargazers over time](https://starchart.cc/HariSekhon/lib.svg)](https://starchart.cc/HariSekhon/lib)\n\n---\n## CI/CD Status Page\n\n[![Repo on GitHub](https://img.shields.io/badge/GitHub-repo-blue?logo=github)](https://github.com/HariSekhon/CI-CD)\n[![GitHub stars](https://img.shields.io/github/stars/HariSekhon/CI-CD?logo=github)](https://github.com/HariSekhon/CI-CD/stargazers)\n[![GitHub forks](https://img.shields.io/github/forks/HariSekhon/CI-CD?logo=github)](https://github.com/HariSekhon/CI-CD/network)\n\nCI/CD Status page for Hari Sekhon's GitHub repos\n\n[![Stargazers over time](https://starchart.cc/HariSekhon/CI-CD.svg)](https://starchart.cc/HariSekhon/CI-CD)\n"
        },
        {
          "name": "STATUS.md",
          "type": "blob",
          "size": 0.064453125,
          "content": "# CI/CD Status Page\n\nMoved to https://harisekhon.github.io/CI-CD/\n"
        },
        {
          "name": "ai",
          "type": "tree",
          "content": null
        },
        {
          "name": "applescript",
          "type": "tree",
          "content": null
        },
        {
          "name": "appveyor",
          "type": "tree",
          "content": null
        },
        {
          "name": "aws",
          "type": "tree",
          "content": null
        },
        {
          "name": "azure-pipelines.yml",
          "type": "blob",
          "size": 2.0244140625,
          "content": "#  vim:ts=2:sts=2:sw=2:et\n#\n#  Author: Hari Sekhon\n#  Date: Sun Feb 23 19:02:10 2020 +0000\n#\n#  https://github.com/HariSekhon/DevOps-Bash-tools\n#\n#  License: see accompanying Hari Sekhon LICENSE file\n#\n#  If you're using my code you're welcome to connect with me on LinkedIn and optionally send me feedback\n#  to help improve or steer this or other code I publish\n#\n#  https://www.linkedin.com/in/HariSekhon\n#\n\n# ============================================================================ #\n#                   A z u r e   D e v O p s   P i p e l i n e\n# ============================================================================ #\n\n# https://aka.ms/yaml\n\ntrigger:\n  - master\n\nvariables:\n  # ubuntu version\n  os_version: '22.04'\n\npool:\n  # there is no /dev/stderr on this azure build!\n  #vmImage: 'ubuntu-latest'\n  #vmImage: 'ubuntu-22.04'\n  vmImage: 'ubuntu-$(os_version)'\n\n# unprivileged container without sudo, cannot install dependencies\n#container: ubuntu:22.04\n\nsteps:\n  - script: cat /etc/*-release\n    displayName: OS Release\n\n  # requires script as first key, otherwise parsing breaks with error message:  Unexpected value 'displayName'\n  - script: env | sort\n    displayName: Environment\n\n  # doesn't work in container due to unprivileged execution and lack of sudo\n  #- script: sudo apt-get update && sudo apt-get install -y git make\n  #  displayName: install git & make\n\n  #- script: make\n  #  displayName: build\n\n  # doesn't work in vmImage build due to lack of access to normal /dev/stderr device\n  # tee: /dev/stderr: No such device or address\n  #- script: make test\n  #  displayName: test\n\n  # hacky workaround to Azure Pipelines ubuntu environment limitations of unprivileged container and no /dev/stderr in vmImage :-(\n  - script: |\n      sudo docker run -v \"$PWD\":/code \"ubuntu:$(os_version)\" /bin/bash -c '\n        set -ex\n        cd /code\n        setup/ci_bootstrap.sh\n        if [ -x setup/ci_git_set_dir_safe.sh ]; then\n          setup/ci_git_set_dir_safe.sh\n        fi\n        make init\n        make ci test\n      '\n    displayName: docker build\n"
        },
        {
          "name": "azure_devops",
          "type": "tree",
          "content": null
        },
        {
          "name": "bigdata",
          "type": "tree",
          "content": null
        },
        {
          "name": "bin",
          "type": "tree",
          "content": null
        },
        {
          "name": "bitbucket-pipelines.yml",
          "type": "blob",
          "size": 1.0478515625,
          "content": "#\n#  Author: Hari Sekhon\n#  Date: 2020-02-24 17:08:57 +0000 (Mon, 24 Feb 2020)\n#\n#  vim:ts=2:sts=2:sw=2:et\n#\n#  https://github.com/HariSekhon/DevOps-Bash-tools\n#\n#  License: see accompanying Hari Sekhon LICENSE file\n#\n#  If you're using my code you're welcome to connect with me on LinkedIn and optionally send me feedback to help steer this or other code I publish\n#\n#  https://www.linkedin.com/in/HariSekhon\n#\n\n# ============================================================================ #\n#                B i t b u c k e t   C I / C D   P i p e l i n e\n# ============================================================================ #\n\n# Reference:\n#\n#   https://support.atlassian.com/bitbucket-cloud/docs/configure-bitbucket-pipelinesyml/\n\n# Languages:\n#\n#   https://confluence.atlassian.com/x/5Q4SMw\n\n# You can specify a custom docker image from Docker Hub as your build environment.\nimage: atlassian/default-image:2\n\npipelines:\n  default:\n    - step:\n        script:\n          - setup/ci_bootstrap.sh\n          - make init\n          - make ci\n          - make test\n"
        },
        {
          "name": "bitbucket",
          "type": "tree",
          "content": null
        },
        {
          "name": "boot",
          "type": "blob",
          "size": 0.017578125,
          "content": "setup/bootstrap.sh"
        },
        {
          "name": "buddy.yml",
          "type": "blob",
          "size": 1.4794921875,
          "content": "#\n#  Author: Hari Sekhon\n#  Date: 2020-03-16 14:02:53 +0000 (Mon, 16 Mar 2020)\n#\n#  vim:ts=2:sts=2:sw=2:et\n#\n#  https://github.com/HariSekhon/DevOps-Bash-tools\n#\n#  License: see accompanying Hari Sekhon LICENSE file\n#\n#  If you're using my code you're welcome to connect with me on LinkedIn and optionally send me feedback to help steer this or other code I publish\n#\n#  https://www.linkedin.com/in/HariSekhon\n#\n\n# ============================================================================ #\n#                                B u d d y   C I\n# ============================================================================ #\n\n# https://buddy.works/docs/yaml/yaml-schema\n\n---\n- pipeline: \"Build\"\n  trigger_mode: \"ON_EVERY_PUSH\"\n  ref_name: \"master\"\n  ref_type: \"BRANCH\"\n  target_site_url: \"https://github.com/HariSekhon/DevOps-Bash-tools\"\n  trigger_condition: \"ALWAYS\"\n  actions:\n    - action: \"Execute: make ci test\"\n      type: \"BUILD\"\n      working_directory: \"/buddy/devops-bash-tools\"\n      docker_image_name: \"library/ubuntu\"\n      docker_image_tag: \"18.04\"\n      #setup_commands:\n      # this step gets cached, which results in\n      # E: Unable to fetch some archives, maybe run apt-get update or try with --fix-missing?\n      #  - apt update\n      #  - apt install -qy git make\n      execute_commands:\n        - setup/ci_bootstrap.sh\n        - make init\n        - make ci\n        - make test\n      volume_mappings:\n        - \"/:/buddy/devops-bash-tools\"\n      shell: \"BASH\"\n      trigger_condition: \"ALWAYS\"\n"
        },
        {
          "name": "buildkite",
          "type": "tree",
          "content": null
        },
        {
          "name": "checks",
          "type": "tree",
          "content": null
        },
        {
          "name": "cicd",
          "type": "tree",
          "content": null
        },
        {
          "name": "circleci",
          "type": "tree",
          "content": null
        },
        {
          "name": "cloudflare",
          "type": "tree",
          "content": null
        },
        {
          "name": "codefresh.yml",
          "type": "blob",
          "size": 1.1455078125,
          "content": "#\n#  Author: Hari Sekhon\n#  Date: 2020-02-24 17:43:07 +0000 (Mon, 24 Feb 2020)\n#\n#  vim:ts=2:sts=2:sw=2:et\n#\n#  https://github.com/HariSekhon/DevOps-Bash-tools\n#\n#  License: see accompanying Hari Sekhon LICENSE file\n#\n#  If you're using my code you're welcome to connect with me on LinkedIn and optionally send me feedback to help steer this or other code I publish\n#\n#  https://www.linkedin.com/in/HariSekhon\n#\n\n# ============================================================================ #\n#                            C o d e f r e s h   C I\n# ============================================================================ #\n\n# https://codefresh.io/docs/docs/codefresh-yaml/\n\nversion: \"1.0\"\nstages:\n  - \"checkout\"\n  - \"build\"\nsteps:\n  checkout:\n    type: \"git-clone\"\n    description: \"Cloning main repository...\"\n    repo: '${{CF_REPO_OWNER}}/${{CF_REPO_NAME}}'\n    revision: \"${{CF_REVISION}}\"\n    stage: \"checkout\"\n  build:\n    title: Running docker image\n    type: freestyle\n    working_directory: '${{CF_REPO_NAME}}'\n    arguments:\n      image: 'ubuntu:18.04'\n      commands:\n        - setup/ci_bootstrap.sh\n        - make init\n        - make ci\n        - make test\n"
        },
        {
          "name": "codeship",
          "type": "tree",
          "content": null
        },
        {
          "name": "configs",
          "type": "tree",
          "content": null
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "docker-compose",
          "type": "tree",
          "content": null
        },
        {
          "name": "docker",
          "type": "tree",
          "content": null
        },
        {
          "name": "drone",
          "type": "tree",
          "content": null
        },
        {
          "name": "gcp",
          "type": "tree",
          "content": null
        },
        {
          "name": "git",
          "type": "tree",
          "content": null
        },
        {
          "name": "github",
          "type": "tree",
          "content": null
        },
        {
          "name": "gitlab",
          "type": "tree",
          "content": null
        },
        {
          "name": "hadolint.yaml",
          "type": "blob",
          "size": 0.896484375,
          "content": "#  vim:ts=4:sts=4:sw=4:et\n#\n#  Author: Hari Sekhon\n#  Date: 2019-10-01 16:14:15 +0100 (Tue, 01 Oct 2019)\n#\n#  https://github.com/HariSekhon/DevOps-Bash-tools\n#\n#  License: see accompanying Hari Sekhon LICENSE file\n#\n#  If you're using my code you're welcome to connect with me on LinkedIn and optionally send me feedback to help improve or steer this or other code I publish\n#\n#  https://www.linkedin.com/in/HariSekhon\n#\n\n# sourced by check_dockerfiles.sh if no \\$PWD/.hadolint.yaml is found\n\nignored:\n  # Maintainer is deprecated\n  - DL4000\n  # FROM latest - dev images build on upstream latest tag intentionally\n  - DL3007\n  # apt-get install versions need not be pinned\n  - DL3008\n  # apk add versions need not be pinned\n  - DL3018\n  #- SC1010\n  # - for dev images it's ok to use both curl and wget as they are dependencies of different scripts\n  - DL4001\n\ntrustedRegistries:\n  - docker.io\n  #- my-company.com:5000\n"
        },
        {
          "name": "install",
          "type": "tree",
          "content": null
        },
        {
          "name": "internet",
          "type": "tree",
          "content": null
        },
        {
          "name": "ipaas",
          "type": "tree",
          "content": null
        },
        {
          "name": "java",
          "type": "tree",
          "content": null
        },
        {
          "name": "jenkins",
          "type": "tree",
          "content": null
        },
        {
          "name": "kafka",
          "type": "tree",
          "content": null
        },
        {
          "name": "kics.config",
          "type": "blob",
          "size": 1.123046875,
          "content": "#\n#  Author: Hari Sekhon\n#  Date: 2023-05-05 18:05:53 +0100 (Fri, 05 May 2023)\n#\n#  vim:ts=2:sts=2:sw=2:et:filetype=yaml\n#\n#  https://github.com/HariSekhon/DevOps-Bash-tools\n#\n#  License: see accompanying Hari Sekhon LICENSE file\n#\n#  If you're using my code you're welcome to connect with me on LinkedIn and optionally send me feedback to help steer this or other code I publish\n#\n#  https://www.linkedin.com/in/HariSekhon\n#\n\n# ============================================================================ #\n#                             K i c s   C o n f i g\n# ============================================================================ #\n\n# https://github.com/Checkmarx/kics/blob/master/docs/configuration-file.md\n\n---\n#path: assets/iac_samples\nverbose: true\nlog-file: true\n#type:\n#  - Dockerfile\n#  - Kubernetes\n#queries-path: \"assets/queries\"\nexclude-paths:\n  # ignore submodules - handle them in the source repos only\n  - bash-tools/\n  - github-actions/\n  - haproxy-configs/\n  - jenkins/\n  - kubernetes-templates/\n  - lib/\n  - pylib/\n  - spotify-tools/\n  - sql/\n  - sql-keywords/\n  - templates/\n  - terraform-templates/\n#output-path: \"results\"\n"
        },
        {
          "name": "kubernetes-configs",
          "type": "commit",
          "content": null
        },
        {
          "name": "kubernetes",
          "type": "tree",
          "content": null
        },
        {
          "name": "lib",
          "type": "tree",
          "content": null
        },
        {
          "name": "media",
          "type": "tree",
          "content": null
        },
        {
          "name": "monitoring",
          "type": "tree",
          "content": null
        },
        {
          "name": "mysql",
          "type": "tree",
          "content": null
        },
        {
          "name": "packages",
          "type": "tree",
          "content": null
        },
        {
          "name": "packer",
          "type": "commit",
          "content": null
        },
        {
          "name": "perl",
          "type": "tree",
          "content": null
        },
        {
          "name": "pingdom",
          "type": "tree",
          "content": null
        },
        {
          "name": "postgres",
          "type": "tree",
          "content": null
        },
        {
          "name": "python",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.251953125,
          "content": "aws-consoler>=1.1.0\ncheckov>=2.0.618\n#git-remote-codecommit>=1.16\ngrip>=4.6.1\njsonlint>=0.1\npylint>=1.9.5\n# can only be installed on macOS - moved to setup/pip-packages-mac.txt\n#pyobjc-framework-Quartz>=9.0.1\nsemgrep>=0.78.0\nsqlfluff==3.2.2\nyamllint>=1.15.0\n"
        },
        {
          "name": "resources",
          "type": "tree",
          "content": null
        },
        {
          "name": "scalastyle_config.xml",
          "type": "blob",
          "size": 7.40234375,
          "content": "<scalastyle commentFilter=\"enabled\">\n <name>Scalastyle standard configuration</name>\n <check level=\"warning\" class=\"org.scalastyle.file.FileTabChecker\" enabled=\"true\"></check>\n <check level=\"warning\" class=\"org.scalastyle.file.FileLengthChecker\" enabled=\"true\">\n  <parameters>\n   <parameter name=\"maxFileLength\"><![CDATA[800]]></parameter>\n  </parameters>\n </check>\n <check level=\"warning\" class=\"org.scalastyle.file.HeaderMatchesChecker\" enabled=\"true\">\n  <parameters>\n   <parameter name=\"header\"><![CDATA[// Copyright (C) 2011-2012 the original author or authors.\n// See the LICENCE.txt file distributed with this work for additional\n// information regarding copyright ownership.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.]]></parameter>\n  </parameters>\n </check>\n <check level=\"warning\" class=\"org.scalastyle.scalariform.SpacesAfterPlusChecker\" enabled=\"true\"></check>\n <check level=\"warning\" class=\"org.scalastyle.file.WhitespaceEndOfLineChecker\" enabled=\"true\"></check>\n <check level=\"warning\" class=\"org.scalastyle.scalariform.SpacesBeforePlusChecker\" enabled=\"true\"></check>\n <check level=\"warning\" class=\"org.scalastyle.file.FileLineLengthChecker\" enabled=\"true\">\n  <parameters>\n   <parameter name=\"maxLineLength\"><![CDATA[160]]></parameter>\n   <parameter name=\"tabSize\"><![CDATA[4]]></parameter>\n  </parameters>\n </check>\n <check level=\"warning\" class=\"org.scalastyle.scalariform.ClassNamesChecker\" enabled=\"true\">\n  <parameters>\n   <parameter name=\"regex\"><![CDATA[[A-Z][A-Za-z]*]]></parameter>\n  </parameters>\n </check>\n <check level=\"warning\" class=\"org.scalastyle.scalariform.ObjectNamesChecker\" enabled=\"true\">\n  <parameters>\n   <parameter name=\"regex\"><![CDATA[[A-Z][A-Za-z]*]]></parameter>\n  </parameters>\n </check>\n <check level=\"warning\" class=\"org.scalastyle.scalariform.PackageObjectNamesChecker\" enabled=\"true\">\n  <parameters>\n   <parameter name=\"regex\"><![CDATA[^[a-z][A-Za-z]*$]]></parameter>\n  </parameters>\n </check>\n <check level=\"warning\" class=\"org.scalastyle.scalariform.EqualsHashCodeChecker\" enabled=\"true\"></check>\n <check level=\"warning\" class=\"org.scalastyle.scalariform.IllegalImportsChecker\" enabled=\"true\">\n  <parameters>\n   <parameter name=\"illegalImports\"><![CDATA[sun._,java.awt._]]></parameter>\n  </parameters>\n </check>\n <check level=\"warning\" class=\"org.scalastyle.scalariform.ParameterNumberChecker\" enabled=\"true\">\n  <parameters>\n   <parameter name=\"maxParameters\"><![CDATA[8]]></parameter>\n  </parameters>\n </check>\n <check level=\"warning\" class=\"org.scalastyle.scalariform.MagicNumberChecker\" enabled=\"true\">\n  <parameters>\n   <parameter name=\"ignore\"><![CDATA[-1,0,1,2,3]]></parameter>\n  </parameters>\n </check>\n <check level=\"warning\" class=\"org.scalastyle.scalariform.NoWhitespaceBeforeLeftBracketChecker\" enabled=\"true\"></check>\n <check level=\"warning\" class=\"org.scalastyle.scalariform.NoWhitespaceAfterLeftBracketChecker\" enabled=\"true\"></check>\n <check level=\"warning\" class=\"org.scalastyle.scalariform.ReturnChecker\" enabled=\"true\"></check>\n <check level=\"warning\" class=\"org.scalastyle.scalariform.NullChecker\" enabled=\"true\"></check>\n <check level=\"warning\" class=\"org.scalastyle.scalariform.NoCloneChecker\" enabled=\"true\"></check>\n <check level=\"warning\" class=\"org.scalastyle.scalariform.NoFinalizeChecker\" enabled=\"true\"></check>\n <check level=\"warning\" class=\"org.scalastyle.scalariform.CovariantEqualsChecker\" enabled=\"true\"></check>\n <check level=\"warning\" class=\"org.scalastyle.scalariform.StructuralTypeChecker\" enabled=\"true\"></check>\n <check level=\"warning\" class=\"org.scalastyle.file.RegexChecker\" enabled=\"true\">\n  <parameters>\n   <parameter name=\"regex\"><![CDATA[println]]></parameter>\n  </parameters>\n </check>\n <check level=\"warning\" class=\"org.scalastyle.scalariform.NumberOfTypesChecker\" enabled=\"true\">\n  <parameters>\n   <parameter name=\"maxTypes\"><![CDATA[30]]></parameter>\n  </parameters>\n </check>\n <check level=\"warning\" class=\"org.scalastyle.scalariform.CyclomaticComplexityChecker\" enabled=\"true\">\n  <parameters>\n   <parameter name=\"maximum\"><![CDATA[10]]></parameter>\n  </parameters>\n </check>\n <check level=\"warning\" class=\"org.scalastyle.scalariform.UppercaseLChecker\" enabled=\"true\"></check>\n <check level=\"warning\" class=\"org.scalastyle.scalariform.SimplifyBooleanExpressionChecker\" enabled=\"true\"></check>\n <check level=\"warning\" class=\"org.scalastyle.scalariform.IfBraceChecker\" enabled=\"true\">\n  <parameters>\n   <parameter name=\"singleLineAllowed\"><![CDATA[true]]></parameter>\n   <parameter name=\"doubleLineAllowed\"><![CDATA[false]]></parameter>\n  </parameters>\n </check>\n <check level=\"warning\" class=\"org.scalastyle.scalariform.MethodLengthChecker\" enabled=\"true\">\n  <parameters>\n   <parameter name=\"maxLength\"><![CDATA[50]]></parameter>\n  </parameters>\n </check>\n <check level=\"warning\" class=\"org.scalastyle.scalariform.MethodNamesChecker\" enabled=\"true\">\n  <parameters>\n   <parameter name=\"regex\"><![CDATA[^[a-z][A-Za-z0-9]*$]]></parameter>\n  </parameters>\n </check>\n <check level=\"warning\" class=\"org.scalastyle.scalariform.NumberOfMethodsInTypeChecker\" enabled=\"true\">\n  <parameters>\n   <parameter name=\"maxMethods\"><![CDATA[30]]></parameter>\n  </parameters>\n </check>\n <check level=\"warning\" class=\"org.scalastyle.scalariform.PublicMethodsHaveTypeChecker\" enabled=\"true\"></check>\n <check level=\"warning\" class=\"org.scalastyle.file.NewLineAtEofChecker\" enabled=\"true\"></check>\n <check level=\"warning\" class=\"org.scalastyle.file.NoNewLineAtEofChecker\" enabled=\"false\"></check>\n <check level=\"warning\" class=\"org.scalastyle.scalariform.WhileChecker\" enabled=\"false\"></check>\n <check level=\"warning\" class=\"org.scalastyle.scalariform.VarFieldChecker\" enabled=\"false\"></check>\n <check level=\"warning\" class=\"org.scalastyle.scalariform.VarLocalChecker\" enabled=\"false\"></check>\n <check level=\"warning\" class=\"org.scalastyle.scalariform.RedundantIfChecker\" enabled=\"false\"></check>\n <check level=\"warning\" class=\"org.scalastyle.scalariform.TokenChecker\" enabled=\"false\">\n  <parameters>\n   <parameter name=\"regex\"><![CDATA[println]]></parameter>\n  </parameters>\n </check>\n <check level=\"warning\" class=\"org.scalastyle.scalariform.DeprecatedJavaChecker\" enabled=\"true\"></check>\n <check level=\"warning\" class=\"org.scalastyle.scalariform.EmptyClassChecker\" enabled=\"true\"></check>\n <check level=\"warning\" class=\"org.scalastyle.scalariform.ClassTypeParameterChecker\" enabled=\"true\">\n  <parameters>\n   <parameter name=\"regex\"><![CDATA[^[A-Z_]$]]></parameter>\n  </parameters>\n </check>\n <check level=\"warning\" class=\"org.scalastyle.scalariform.UnderscoreImportChecker\" enabled=\"true\"></check>\n <check level=\"warning\" class=\"org.scalastyle.scalariform.LowercasePatternMatchChecker\" enabled=\"true\"></check>\n <check level=\"warning\" class=\"org.scalastyle.scalariform.MultipleStringLiteralsChecker\" enabled=\"true\">\n  <parameters>\n   <parameter name=\"allowed\"><![CDATA[2]]></parameter>\n   <parameter name=\"ignoreRegex\"><![CDATA[^\"\"$]]></parameter>\n  </parameters>\n </check>\n <check level=\"warning\" class=\"org.scalastyle.scalariform.ImportGroupingChecker\" enabled=\"true\"></check>\n</scalastyle>"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "search",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup",
          "type": "tree",
          "content": null
        },
        {
          "name": "shippable",
          "type": "tree",
          "content": null
        },
        {
          "name": "sonar-project.properties",
          "type": "blob",
          "size": 1.3173828125,
          "content": "#  vim:ts=4:sts=4:sw=4:et\n#\n#  Author: Hari Sekhon\n#  Date: 2016-07-19 18:31:17 +0100 (Tue, 19 Jul 2016)\n#\n#  https://github.com/HariSekhon/DevOps-Bash-tools\n#\n#  License: see accompanying Hari Sekhon LICENSE file\n#\n#  If you're using my code you're welcome to connect with me on LinkedIn and optionally send me feedback to help steer this or other code I publish\n#\n#  https://www.linkedin.com/in/HariSekhon\n#\n\n# ============================================================================ #\n#                               S o n a r Q u b e\n# ============================================================================ #\n\nsonar.host.url=https://sonarcloud.io\n\n# Required metadata\nsonar.organization=harisekhon\nsonar.projectName=DevOps-Bash-tools\nsonar.projectKey=HariSekhon_DevOps-Bash-tools\nsonar.projectVersion=1.0\n\nsonar.projectDescription=DevOps-Bash-tools\n\nsonar.links.homepage=https://github.com/HariSekhon/DevOps-Bash-tools\nsonar.links.scm=https://github.com/HariSekhon/DevOps-Bash-tools\nsonar.links.issue=https://github.com/HariSekhon/DevOps-Bash-tools/issues\nsonar.links.ci=https://github.com/HariSekhon/DevOps-Bash-tools/actions\n\n# directories to scan (defaults to sonar-project.properties dir otherwise)\nsonar.sources=.\n\n#sonar.language=py\n\nsonar.sourceEncoding=UTF-8\n\n#sonar.exclusions=**/tests/**\nsonar.exclusions=**/zookeeper-*/**/*\n"
        },
        {
          "name": "spotify",
          "type": "tree",
          "content": null
        },
        {
          "name": "sql",
          "type": "commit",
          "content": null
        },
        {
          "name": "teamcity",
          "type": "tree",
          "content": null
        },
        {
          "name": "templates",
          "type": "commit",
          "content": null
        },
        {
          "name": "terraform",
          "type": "tree",
          "content": null
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "travis",
          "type": "tree",
          "content": null
        },
        {
          "name": "vagrant-configs",
          "type": "commit",
          "content": null
        },
        {
          "name": "vagrant",
          "type": "tree",
          "content": null
        },
        {
          "name": "wercker",
          "type": "tree",
          "content": null
        },
        {
          "name": "yamllint",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}