{
  "metadata": {
    "timestamp": 1736568915768,
    "page": 133,
    "hasNextPage": false,
    "endCursor": "Y3Vyc29yOjEzNw==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "ben1234560/k8s_PaaS",
      "stars": 5097,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".DS_Store",
          "type": "blob",
          "size": 10.00390625,
          "content": null
        },
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0.029296875,
          "content": "*.zip linguist-language=shell\n"
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.0302734375,
          "content": "\n.DS_Store\n.DS_Store\n.DS_Store\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 0.33984375,
          "content": "## CONTRIBUTING\n\n1. [Fork on Github](https://github.com/ben1234560/k8s_PaaS/fork)\n2. Clone the forked repo: `git clone https://github.com/ben1234560/k8s_PaaS.git`\n3. Create a new branch, add some changes and commit: `git checkout -b new-branch`.\n4. Push the branch to Github: `git commit -am \"comments\"; git push`.\n5. File a Pull Request on Github."
        },
        {
          "name": "Features.md",
          "type": "blob",
          "size": 1.791015625,
          "content": "## Features\n\n![1584258151218](assets/1584258151218.png)\n\n- 对做的事情进行说明是什么（WHAT），为什么要做（WHY）\n\n![1584258268594](assets/1584258268594.png)\n\n- 对相关文件进行解析、指明哪部机器操作、配图，并在易出错点添加解决办法。\n\n![1584258427709](assets/1584258427709.png)\n\n- 使用文件皆是官方文件，且软件包有对应文件，避免被更新或其它问题导致无法下载等情况，百度云https://pan.baidu.com/s/1arE2LdtAbcR80gmIQtIELw 提取码：ouy1。\n\n  ![1584272695417](assets/1584272695417.png)\n\n- 无数前人遍历/建设代码，为代码完整性保驾护航，欢迎给我们提供你的建议、扩展、报错\n\n  ![1585471116118](assets/1585471116118.png)\n\n## Q&A\n\n- Q: 我的电脑是4核8G能做吗：\n  - A: 4核8G能做到dashboard前后，4核16G能做到Jenkins前后，8核24G能做到prometheus前后，当然对于部分同学可能一开始就买个8核32G的机器很浪费，你可以先用你的机器做到做不动再买，因为前期很多东西看不懂，学习的进度也是比较慢，而且我强烈建议各位如果可以一定要做到Jenkins后重装一遍。\n- Q: 我是新手可以吗\n  - A: 只要你会用电脑就可以，至于懂不懂linux懂不懂开发，这些都无关紧要\n- Q: 一直报错找不到解决办法怎么办\n  - A: 加群（群二维码在主页）找我们，有很多热心的群友。或者你可以重装，前提是你确定不是你操作的问题（PS：机器命名和配置尽量跟我的一样，不然很难给你找）\n- Q: 做完了一部分但是看不懂，就感觉自己在复制粘贴\n  - A: 所有的学习都是从复制粘贴开始，没有人能看一遍就什么都懂（当然是因为我没见过这种人，我遇到的大部分都是有基础所以学的快）\n\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0419921875,
          "content": "MIT License\n\nCopyright (c) 2020 ben1234560\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 28.755859375,
          "content": "# k8s_PaaS\n[![image](https://img.shields.io/badge/google-kubernetes-blue.svg)](https://kubernetes.io/) [![image](https://img.shields.io/badge/ctripcorp-apollo-gray.svg)](https://github.com/ctripcorp/apollo) [![image](https://img.shields.io/badge/CNCD-Spinnaker-skyblue.svg)](https://www.spinnaker.io/) [![image](https://img.shields.io/badge/JAVA-Jenkins-orange.svg)](https://jenkins.io/zh/) [![image](https://img.shields.io/badge/Git-Gitee-red.svg)](https://gitee.com) [![image](https://img.shields.io/badge/Git-GitLab-orange.svg)]() [![image](https://img.shields.io/badge/Apache-zookeeper-Crimson.svg)](http://zookeeper.apache.org/) [![image](https://img.shields.io/badge/used-Harbor-green.svg)](https://goharbor.io/)\n\n[![image](https://img.shields.io/badge/used-docker-blue.svg)](https://www.docker.com/) [![image](https://img.shields.io/badge/used-Prometheus-red.svg)](https://prometheus.io/) [![image](https://img.shields.io/badge/used-etcd-blue.svg)](https://etcd.io/) [![image](https://img.shields.io/badge/used-Grafana-orange.svg)](https://grafana.com)\n\n基于Kubernetes(K8S)一步步部署成PaaS/DevOps（一套完整的软件研发和部署平台）——教程/学习（实战代码/欢迎讨论/大量注释/操作配图），你将习得部署如：Kubernetes(K8S)、dashboard、Harbor、Jenkins、本地gitlab、Apollo框架、promtheus、grafana、spinnaker等。\n\n注释及配图覆盖率达80%以上，旨在帮助快速入门。\n\n并将告诉你：是什么（WHAT）、为什么这么做(WHY)、怎么做(HOW)。\n\n建议学习时长1个月+，最终将实现点点点（自动化）的形式就能部署上线并维护。\n\n## PaaS架构图\n\n![K8S_PaaS架构图](assets/K8S_PaaS架构图.png)\n\n> 橙色框内软件皆部署在K8S集群中，也就是我们可以随时扩容缩容\n\n## <a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/Features.md\">Features</a>\n\n- 对做的事情进行说明是什么（WHAT），为什么要做（WHY）。\n- 对相关文件进行解析、指明哪部机器操作、配图，并在易出错点添加解决办法。第二章由于配置内容较多，建议配合[check_tool](https://github.com/ben1234560/k8s_PaaS/tree/master/%E8%BD%AF%E4%BB%B6%E5%8C%85/check_tool)使用。\n- 使用文件皆是官方文件，相关软件包有对应文件，避免被更新或其它问题导致无法下载等情况，百度云https://pan.baidu.com/s/1arE2LdtAbcR80gmIQtIELw 提取码：ouy1。\n- 无数前人遍历/建设代码，为代码完整性保驾护航，欢迎给我们提供你的建议、扩展、报错。\n- 整理了多人问的4个问题<a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/Features.md#qa\">Q&A</a>：配置只有4核8G够吗，新手可以吗，找不到报错怎么办，做完看不懂怎么办\n- 推出公有云部署版本，如[第二章——企业部署实战_K8S【公有云版】](https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E4%BA%8C%E7%AB%A0%E2%80%94%E2%80%94%E4%BC%81%E4%B8%9A%E9%83%A8%E7%BD%B2%E5%AE%9E%E6%88%98_K8S%E3%80%90%E5%85%AC%E6%9C%89%E4%BA%91%E7%89%88%E3%80%91.md)。自己电脑资源紧张的完全可以用，而且费用也便宜\n\n## 学习章节：\n\n<ul>\n    <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E4%B8%80%E7%AB%A0%E2%80%94%E2%80%94Docker%EF%BC%88%E5%B7%B2%E7%86%9F%E6%82%89%E7%9A%84%E5%8F%AF%E4%BB%A5%E4%BB%8E%E7%AC%AC%E4%BA%8C%E7%AB%A0%E5%BC%80%E5%A7%8B%EF%BC%89.md\">第一章——Docker</a>\n    <ul>\n        <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E4%B8%80%E7%AB%A0%E2%80%94%E2%80%94Docker%EF%BC%88%E5%B7%B2%E7%86%9F%E6%82%89%E7%9A%84%E5%8F%AF%E4%BB%A5%E4%BB%8E%E7%AC%AC%E4%BA%8C%E7%AB%A0%E5%BC%80%E5%A7%8B%EF%BC%89.md#%E5%AE%89%E8%A3%85docker\">安装Docker</a>\n      <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E4%B8%80%E7%AB%A0%E2%80%94%E2%80%94Docker%EF%BC%88%E5%B7%B2%E7%86%9F%E6%82%89%E7%9A%84%E5%8F%AF%E4%BB%A5%E4%BB%8E%E7%AC%AC%E4%BA%8C%E7%AB%A0%E5%BC%80%E5%A7%8B%EF%BC%89.md#%E5%BC%80%E5%90%AF%E6%88%91%E4%BB%AC%E7%9A%84%E7%AC%AC%E4%B8%80%E4%B8%AAdocker%E5%AE%B9%E5%99%A8\">开启我们的第一个docker容器</a>\n      <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E4%B8%80%E7%AB%A0%E2%80%94%E2%80%94Docker%EF%BC%88%E5%B7%B2%E7%86%9F%E6%82%89%E7%9A%84%E5%8F%AF%E4%BB%A5%E4%BB%8E%E7%AC%AC%E4%BA%8C%E7%AB%A0%E5%BC%80%E5%A7%8B%EF%BC%89.md#dockerhub%E6%B3%A8%E5%86%8C%E8%87%AA%E5%B7%B1%E7%9A%84%E8%BF%9C%E7%A8%8B%E4%BB%93%E5%BA%93\">Dockerhub注册（自己的远程仓库）</a>\n      <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E4%B8%80%E7%AB%A0%E2%80%94%E2%80%94Docker%EF%BC%88%E5%B7%B2%E7%86%9F%E6%82%89%E7%9A%84%E5%8F%AF%E4%BB%A5%E4%BB%8E%E7%AC%AC%E4%BA%8C%E7%AB%A0%E5%BC%80%E5%A7%8B%EF%BC%89.md#docker%E9%95%9C%E5%83%8F%E7%AE%A1%E7%90%86%E5%AE%9E%E6%88%98\">Docker镜像管理实战</a>\n      <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E4%B8%80%E7%AB%A0%E2%80%94%E2%80%94Docker%EF%BC%88%E5%B7%B2%E7%86%9F%E6%82%89%E7%9A%84%E5%8F%AF%E4%BB%A5%E4%BB%8E%E7%AC%AC%E4%BA%8C%E7%AB%A0%E5%BC%80%E5%A7%8B%EF%BC%89.md#docker%E9%95%9C%E5%83%8F%E7%AE%A1%E7%90%86%E5%AE%9E%E6%88%98\">docker容器操作</a>\n      <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E4%B8%80%E7%AB%A0%E2%80%94%E2%80%94Docker%EF%BC%88%E5%B7%B2%E7%86%9F%E6%82%89%E7%9A%84%E5%8F%AF%E4%BB%A5%E4%BB%8E%E7%AC%AC%E4%BA%8C%E7%AB%A0%E5%BC%80%E5%A7%8B%EF%BC%89.md#dockerfile-%E7%BB%BC%E5%90%88%E5%AE%9E%E9%AA%8C\">dockerfile 综合实验</a>\n    </ul>\n  </li>\n    <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E4%BA%8C%E7%AB%A0%E2%80%94%E2%80%94%E4%BC%81%E4%B8%9A%E9%83%A8%E7%BD%B2%E5%AE%9E%E6%88%98_K8S.md\">第二章——企业部署实战_K8S</a>\n<ul>\n  <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E4%BA%8C%E7%AB%A0%E2%80%94%E2%80%94%E4%BC%81%E4%B8%9A%E9%83%A8%E7%BD%B2%E5%AE%9E%E6%88%98_K8S.md#k8s%E5%89%8D%E7%BD%AE%E5%87%86%E5%A4%87%E5%B7%A5%E4%BD%9Cbind9%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2dns%E6%9C%8D%E5%8A%A1\">K8S前置准备工作——bind9安装部署（DNS服务）</a>\n  <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E4%BA%8C%E7%AB%A0%E2%80%94%E2%80%94%E4%BC%81%E4%B8%9A%E9%83%A8%E7%BD%B2%E5%AE%9E%E6%88%98_K8S.md#k8s%E5%89%8D%E7%BD%AE%E5%B7%A5%E4%BD%9C%E5%87%86%E5%A4%87%E7%AD%BE%E5%8F%91%E8%AF%81%E4%B9%A6%E7%8E%AF%E5%A2%83\">K8S前置工作——准备签发证书环境</a>\n  <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E4%BA%8C%E7%AB%A0%E2%80%94%E2%80%94%E4%BC%81%E4%B8%9A%E9%83%A8%E7%BD%B2%E5%AE%9E%E6%88%98_K8S.md#k8s%E5%89%8D%E7%BD%AE%E5%B7%A5%E4%BD%9C%E9%83%A8%E7%BD%B2docker%E7%8E%AF%E5%A2%83\">K8S前置工作——部署docker环境</a>\n  <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E4%BA%8C%E7%AB%A0%E2%80%94%E2%80%94%E4%BC%81%E4%B8%9A%E9%83%A8%E7%BD%B2%E5%AE%9E%E6%88%98_K8S.md#k8s%E5%89%8D%E7%BD%AE%E5%B7%A5%E4%BD%9C%E9%83%A8%E7%BD%B2harbor%E4%BB%93%E5%BA%93\">K8S前置工作——部署harbor仓库</a>\n  <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E4%BA%8C%E7%AB%A0%E2%80%94%E2%80%94%E4%BC%81%E4%B8%9A%E9%83%A8%E7%BD%B2%E5%AE%9E%E6%88%98_K8S.md#%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2%E4%B8%BB%E6%8E%A7%E8%8A%82%E7%82%B9%E6%9C%8D%E5%8A%A1etcd\">安装部署主控节点服务etcd</a>\n  <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E4%BA%8C%E7%AB%A0%E2%80%94%E2%80%94%E4%BC%81%E4%B8%9A%E9%83%A8%E7%BD%B2%E5%AE%9E%E6%88%98_K8S.md#%E9%83%A8%E7%BD%B2api-server%E9%9B%86%E7%BE%A4\">部署API-server集群</a>\n  <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E4%BA%8C%E7%AB%A0%E2%80%94%E2%80%94%E4%BC%81%E4%B8%9A%E9%83%A8%E7%BD%B2%E5%AE%9E%E6%88%98_K8S.md#%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2%E4%B8%BB%E6%8E%A7%E8%8A%82%E7%82%B9l4%E5%8F%8D%E4%BB%A3%E6%9C%8D%E5%8A%A1\">安装部署主控节点L4反代服务</a>\n  <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E4%BA%8C%E7%AB%A0%E2%80%94%E2%80%94%E4%BC%81%E4%B8%9A%E9%83%A8%E7%BD%B2%E5%AE%9E%E6%88%98_K8S.md#%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2controller-manager%E8%8A%82%E7%82%B9%E6%8E%A7%E5%88%B6%E5%99%A8%E8%B0%83%E5%BA%A6%E5%99%A8%E6%9C%8D%E5%8A%A1\">安装部署controller-manager（节点控制器/调度器服务）</a>\n  <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E4%BA%8C%E7%AB%A0%E2%80%94%E2%80%94%E4%BC%81%E4%B8%9A%E9%83%A8%E7%BD%B2%E5%AE%9E%E6%88%98_K8S.md#%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2%E8%BF%90%E7%AE%97%E8%8A%82%E7%82%B9%E6%9C%8D%E5%8A%A1kubelet\">安装部署运算节点服务</a>\n</ul>\n    <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E4%B8%89%E7%AB%A0%E2%80%94%E2%80%94k8s%E9%9B%86%E7%BE%A4.md\">第三章——k8s集群</a>\n    <ul>\n        <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E4%B8%89%E7%AB%A0%E2%80%94%E2%80%94k8s%E9%9B%86%E7%BE%A4.md#%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2flanneld\">安装部署flanneld</a>\n      <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E4%B8%89%E7%AB%A0%E2%80%94%E2%80%94k8s%E9%9B%86%E7%BE%A4.md#flannel%E4%B9%8Bsnat%E8%A7%84%E5%88%99%E4%BC%98%E5%8C%96\">flannel之SNAT规则优化</a>\n      <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E4%B8%89%E7%AB%A0%E2%80%94%E2%80%94k8s%E9%9B%86%E7%BE%A4.md#%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2coredns%E6%9C%8D%E5%8A%A1%E5%8F%91%E7%8E%B0\">安装部署coredns（服务发现）</a>\n      <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E4%B8%89%E7%AB%A0%E2%80%94%E2%80%94k8s%E9%9B%86%E7%BE%A4.md#k8s%E7%9A%84%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2ingress\">K8S的服务暴露ingress</a>\n    </ul>\n  </li>\n  <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E5%9B%9B%E7%AB%A0%E2%80%94%E2%80%94dashboard%E6%8F%92%E4%BB%B6%E5%8F%8Ak8s%E5%AE%9E%E6%88%98%E4%BA%A4%E4%BB%98.md\">第四章——dashboard插件及k8s实战交付</a>\n    <ul>\n        <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E5%9B%9B%E7%AB%A0%E2%80%94%E2%80%94dashboard%E6%8F%92%E4%BB%B6%E5%8F%8Ak8s%E5%AE%9E%E6%88%98%E4%BA%A4%E4%BB%98.md#dashboard%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2\">dashboard安装部署</a>\n      <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E5%9B%9B%E7%AB%A0%E2%80%94%E2%80%94dashboard%E6%8F%92%E4%BB%B6%E5%8F%8Ak8s%E5%AE%9E%E6%88%98%E4%BA%A4%E4%BB%98.md#k8s%E4%BB%AA%E8%A1%A8%E7%9B%98%E9%89%B4%E6%9D%83\">K8S仪表盘鉴权</a>\n      <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E5%9B%9B%E7%AB%A0%E2%80%94%E2%80%94dashboard%E6%8F%92%E4%BB%B6%E5%8F%8Ak8s%E5%AE%9E%E6%88%98%E4%BA%A4%E4%BB%98.md#dashboardheapster%E5%8F%AF%E4%B8%8D%E5%81%9A\">dashboard——heapster</a>\n      <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E5%9B%9B%E7%AB%A0%E2%80%94%E2%80%94dashboard%E6%8F%92%E4%BB%B6%E5%8F%8Ak8s%E5%AE%9E%E6%88%98%E4%BA%A4%E4%BB%98.md#k8s%E5%B9%B3%E6%BB%91%E5%8D%87%E7%BA%A7%E6%8A%80%E5%B7%A7\">K8S平滑升级技巧</a>\n    </ul>\n  </li>\n  <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E4%BA%94%E7%AB%A0%E2%80%94%E2%80%94K8S%E7%BB%93%E5%90%88CI%26CD%E6%8C%81%E7%BB%AD%E4%BA%A4%E4%BB%98%E5%92%8C%E9%9B%86%E4%B8%AD%E7%AE%A1%E7%90%86%E9%85%8D%E7%BD%AE.md\">第五章——K8S结合CI&CD持续交付和集中管理配置</a>\n    <ul>\n        <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E4%BA%94%E7%AB%A0%E2%80%94%E2%80%94K8S%E7%BB%93%E5%90%88CI%26CD%E6%8C%81%E7%BB%AD%E4%BA%A4%E4%BB%98%E5%92%8C%E9%9B%86%E4%B8%AD%E7%AE%A1%E7%90%86%E9%85%8D%E7%BD%AE.md#%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2zookeeper\">安装部署zookeeper</a>\n        <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E4%BA%94%E7%AB%A0%E2%80%94%E2%80%94K8S%E7%BB%93%E5%90%88CI%26CD%E6%8C%81%E7%BB%AD%E4%BA%A4%E4%BB%98%E5%92%8C%E9%9B%86%E4%B8%AD%E7%AE%A1%E7%90%86%E9%85%8D%E7%BD%AE.md#%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2jenkins\">安装部署Jenkins</a>\n        <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E4%BA%94%E7%AB%A0%E2%80%94%E2%80%94K8S%E7%BB%93%E5%90%88CI%26CD%E6%8C%81%E7%BB%AD%E4%BA%A4%E4%BB%98%E5%92%8C%E9%9B%86%E4%B8%AD%E7%AE%A1%E7%90%86%E9%85%8D%E7%BD%AE.md#%E5%AE%89%E8%A3%85maven\">安装maven</a>\n        <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E4%BA%94%E7%AB%A0%E2%80%94%E2%80%94K8S%E7%BB%93%E5%90%88CI%26CD%E6%8C%81%E7%BB%AD%E4%BA%A4%E4%BB%98%E5%92%8C%E9%9B%86%E4%B8%AD%E7%AE%A1%E7%90%86%E9%85%8D%E7%BD%AE.md#%E5%88%B6%E4%BD%9Cdubbo%E5%BE%AE%E6%9C%8D%E5%8A%A1%E7%9A%84%E5%BA%95%E5%8C%85%E9%95%9C%E5%83%8F\">制作dubbo微服务的底包镜像</a>\n        <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E4%BA%94%E7%AB%A0%E2%80%94%E2%80%94K8S%E7%BB%93%E5%90%88CI%26CD%E6%8C%81%E7%BB%AD%E4%BA%A4%E4%BB%98%E5%92%8C%E9%9B%86%E4%B8%AD%E7%AE%A1%E7%90%86%E9%85%8D%E7%BD%AE.md#%E4%BD%BF%E7%94%A8jenkins%E6%8C%81%E7%BB%AD%E6%9E%84%E5%BB%BA%E4%BA%A4%E4%BB%98dubbo%E6%9C%8D%E5%8A%A1%E7%9A%84%E6%8F%90%E4%BE%9B%E8%80%85\">使用Jenkins持续构建交付dubbo服务的提供者</a>\n        <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E4%BA%94%E7%AB%A0%E2%80%94%E2%80%94K8S%E7%BB%93%E5%90%88CI%26CD%E6%8C%81%E7%BB%AD%E4%BA%A4%E4%BB%98%E5%92%8C%E9%9B%86%E4%B8%AD%E7%AE%A1%E7%90%86%E9%85%8D%E7%BD%AE.md#%E5%80%9F%E5%8A%A9blueocean%E6%8F%92%E4%BB%B6%E5%9B%9E%E9%A1%BEjenkins%E6%B5%81%E6%B0%B4%E7%BA%BF%E6%9E%84%E5%BB%BA%E5%8E%9F%E7%90%86\">借助BlueOcean插件回顾Jenkins流水线构建原理</a>\n        <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E4%BA%94%E7%AB%A0%E2%80%94%E2%80%94K8S%E7%BB%93%E5%90%88CI%26CD%E6%8C%81%E7%BB%AD%E4%BA%A4%E4%BB%98%E5%92%8C%E9%9B%86%E4%B8%AD%E7%AE%A1%E7%90%86%E9%85%8D%E7%BD%AE.md#%E4%BA%A4%E4%BB%98dubbo-monitor%E5%88%B0k8s%E9%9B%86%E7%BE%A4\">交付dubbo-monitor到k8s集群</a>\n        <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E4%BA%94%E7%AB%A0%E2%80%94%E2%80%94K8S%E7%BB%93%E5%90%88CI%26CD%E6%8C%81%E7%BB%AD%E4%BA%A4%E4%BB%98%E5%92%8C%E9%9B%86%E4%B8%AD%E7%AE%A1%E7%90%86%E9%85%8D%E7%BD%AE.md#%E5%AE%9E%E7%8E%B0dubbo%E9%9B%86%E7%BE%A4%E7%9A%84%E6%97%A5%E5%B8%B8%E7%BB%B4%E6%8A%A4\">实现dubbo集群的日常维护</a>\n        <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E4%BA%94%E7%AB%A0%E2%80%94%E2%80%94K8S%E7%BB%93%E5%90%88CI%26CD%E6%8C%81%E7%BB%AD%E4%BA%A4%E4%BB%98%E5%92%8C%E9%9B%86%E4%B8%AD%E7%AE%A1%E7%90%86%E9%85%8D%E7%BD%AE.md#%E5%AE%9E%E6%88%98k8s%E9%9B%86%E7%BE%A4%E6%AF%81%E7%81%AD%E6%80%A7%E6%B5%8B%E8%AF%95\">实战K8S集群毁灭性测试</a>\n    </ul>\n  </li>\n  <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E5%85%AD%E7%AB%A0%E2%80%94%E2%80%94%E5%9C%A8K8S%E4%B8%AD%E9%9B%86%E6%88%90Apollo%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83.md\">第六章——在K8S中集成Apollo配置中心</a>\n    <ul>\n      <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E5%85%AD%E7%AB%A0%E2%80%94%E2%80%94%E5%9C%A8K8S%E4%B8%AD%E9%9B%86%E6%88%90Apollo%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83.md#configmap%E4%BD%BF%E7%94%A8%E8%AF%A6%E8%A7%A3\">configmap使用详解</a>\n      <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E5%85%AD%E7%AB%A0%E2%80%94%E2%80%94%E5%9C%A8K8S%E4%B8%AD%E9%9B%86%E6%88%90Apollo%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83.md#%E4%BA%A4%E4%BB%98apollo-configservice%E5%88%B0k8s\">交付Apollo-ConfigService到K8S</a>\n      <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E5%85%AD%E7%AB%A0%E2%80%94%E2%80%94%E5%9C%A8K8S%E4%B8%AD%E9%9B%86%E6%88%90Apollo%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83.md#apollo-configservice%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93ip%E5%88%86%E6%9E%90\">Apollo-ConfigService连接数据库IP分析</a>\n      <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E5%85%AD%E7%AB%A0%E2%80%94%E2%80%94%E5%9C%A8K8S%E4%B8%AD%E9%9B%86%E6%88%90Apollo%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83.md#%E4%BA%A4%E4%BB%98apollo-portal%E5%89%8D%E6%95%B0%E6%8D%AE%E5%BA%93%E5%88%9D%E5%A7%8B%E5%8C%96\">交付Apollo-Portal前，数据库初始化</a>\n      <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E5%85%AD%E7%AB%A0%E2%80%94%E2%80%94%E5%9C%A8K8S%E4%B8%AD%E9%9B%86%E6%88%90Apollo%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83.md#%E5%88%B6%E4%BD%9Cportal%E7%9A%84docker%E9%95%9C%E5%83%8F%E5%B9%B6%E4%BA%A4%E4%BB%98\">制作Portal的docker镜像，并交付</a>\n      <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E5%85%AD%E7%AB%A0%E2%80%94%E2%80%94%E5%9C%A8K8S%E4%B8%AD%E9%9B%86%E6%88%90Apollo%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83.md#dubbo%E6%9C%8D%E5%8A%A1%E6%8F%90%E4%BE%9B%E8%80%85%E8%BF%9E%E6%8E%A5apollo%E5%AE%9E%E6%88%98\">dubbo服务提供者连接Apollo实战</a>\n      <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E5%85%AD%E7%AB%A0%E2%80%94%E2%80%94%E5%9C%A8K8S%E4%B8%AD%E9%9B%86%E6%88%90Apollo%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83.md#dubbo%E6%9C%8D%E5%8A%A1%E6%B6%88%E8%B4%B9%E8%80%85%E8%BF%9E%E6%8E%A5apollo%E5%AE%9E%E6%88%98\">dubbo服务消费者连接Apollo实战</a>\n      <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E5%85%AD%E7%AB%A0%E2%80%94%E2%80%94%E5%9C%A8K8S%E4%B8%AD%E9%9B%86%E6%88%90Apollo%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83.md#%E5%AE%9E%E6%88%98apollo%E5%88%86%E7%8E%AF%E5%A2%83%E7%AE%A1%E7%90%86dubbo%E6%9C%8D%E5%8A%A1-%E4%BA%A4%E4%BB%98apollo-configservice\">实战Apollo分环境管理dubbo服务-交付Apollo-configservice</a>\n      <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E5%85%AD%E7%AB%A0%E2%80%94%E2%80%94%E5%9C%A8K8S%E4%B8%AD%E9%9B%86%E6%88%90Apollo%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83.md#%E5%AE%9E%E6%88%98%E4%BD%BF%E7%94%A8apollo%E5%88%86%E7%8E%AF%E5%A2%83%E7%AE%A1%E7%90%86dubbo%E6%9C%8D%E5%8A%A1%E4%BA%A4%E4%BB%98apollo-portal%E5%92%8Cadminservice\">实战使用Apollo分环境管理dubbo服务——交付Apollo-portal和adminservice</a>\n      <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E5%85%AD%E7%AB%A0%E2%80%94%E2%80%94%E5%9C%A8K8S%E4%B8%AD%E9%9B%86%E6%88%90Apollo%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83.md#%E5%AE%9E%E6%88%98%E5%8F%91%E5%B8%83dubbo%E8%BF%9E%E6%8E%A5apollo%E5%88%B0%E4%B8%8D%E5%90%8C%E7%8E%AF%E5%A2%83\">实战发布dubbo连接Apollo到不同环境</a>\n      <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E5%85%AD%E7%AB%A0%E2%80%94%E2%80%94%E5%9C%A8K8S%E4%B8%AD%E9%9B%86%E6%88%90Apollo%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83.md#%E5%AE%9E%E6%88%98%E6%BC%94%E7%A4%BA%E9%A1%B9%E7%9B%AE%E6%8F%90%E6%B5%8B%E5%8F%91%E7%89%88%E6%B5%81%E7%A8%8B\">实战演示项目提测，发版流程</a>\n    </ul>\n  </li>\n  <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E4%B8%83%E7%AB%A0%E2%80%94%E2%80%94Promtheus%E7%9B%91%E6%8E%A7k8s%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%BA%94%E7%94%A8.md\">第七章——Promtheus监控k8s企业级应用</a>\n    <ul>\n      <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E4%B8%83%E7%AB%A0%E2%80%94%E2%80%94Promtheus%E7%9B%91%E6%8E%A7k8s%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%BA%94%E7%94%A8.md#prometheus%E7%9B%91%E6%8E%A7%E8%BD%AF%E4%BB%B6%E6%A6%82%E8%BF%B0\">Prometheus监控软件概述</a>\n      <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E4%B8%83%E7%AB%A0%E2%80%94%E2%80%94Promtheus%E7%9B%91%E6%8E%A7k8s%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%BA%94%E7%94%A8.md#%E4%BA%A4%E4%BB%98kube-state-metric\">交付kube-state-metric</a>\n      <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E4%B8%83%E7%AB%A0%E2%80%94%E2%80%94Promtheus%E7%9B%91%E6%8E%A7k8s%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%BA%94%E7%94%A8.md#%E4%BA%A4%E4%BB%98node-exporter\">交付node-exporter</a>\n      <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E4%B8%83%E7%AB%A0%E2%80%94%E2%80%94Promtheus%E7%9B%91%E6%8E%A7k8s%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%BA%94%E7%94%A8.md#%E4%BA%A4%E4%BB%98cadvisor\">交付cadvisor</a>\n      <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E4%B8%83%E7%AB%A0%E2%80%94%E2%80%94Promtheus%E7%9B%91%E6%8E%A7k8s%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%BA%94%E7%94%A8.md#%E4%BA%A4%E4%BB%98blackbox-exporter\">交付blackbox-exporter</a>\n      <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E4%B8%83%E7%AB%A0%E2%80%94%E2%80%94Promtheus%E7%9B%91%E6%8E%A7k8s%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%BA%94%E7%94%A8.md#%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2prometheus-server\">安装部署Prometheus-server</a>\n      <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E4%B8%83%E7%AB%A0%E2%80%94%E2%80%94Promtheus%E7%9B%91%E6%8E%A7k8s%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%BA%94%E7%94%A8.md#%E9%85%8D%E7%BD%AEprometheus%E7%9B%91%E6%8E%A7%E4%B8%9A%E5%8A%A1%E5%AE%B9%E5%99%A8\">配置Prometheus监控业务容器</a>\n      <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E4%B8%83%E7%AB%A0%E2%80%94%E2%80%94Promtheus%E7%9B%91%E6%8E%A7k8s%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%BA%94%E7%94%A8.md#%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2%E9%85%8D%E7%BD%AEgrafana\">安装部署配置Grafana</a>\n      <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E4%B8%83%E7%AB%A0%E2%80%94%E2%80%94Promtheus%E7%9B%91%E6%8E%A7k8s%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%BA%94%E7%94%A8.md#%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2alertmanager\">安装部署alertmanager</a>\n      <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E4%B8%83%E7%AB%A0%E2%80%94%E2%80%94Promtheus%E7%9B%91%E6%8E%A7k8s%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%BA%94%E7%94%A8.md#%E6%B5%8B%E8%AF%95alertmanager%E6%8A%A5%E8%AD%A6%E5%8A%9F%E8%83%BD\">测试alertmanager报警功能</a>\n      <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E4%B8%83%E7%AB%A0%E2%80%94%E2%80%94Promtheus%E7%9B%91%E6%8E%A7k8s%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%BA%94%E7%94%A8.md#%E9%80%9A%E8%BF%87k8s%E9%83%A8%E7%BD%B2dubbo%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%8E%A5%E5%85%A5elk%E6%9E%B6%E6%9E%84\">通过K8S部署dubbo微服务接入ELK架构</a>\n      <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E4%B8%83%E7%AB%A0%E2%80%94%E2%80%94Promtheus%E7%9B%91%E6%8E%A7k8s%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%BA%94%E7%94%A8.md#%E5%88%B6%E4%BD%9Ctomcat%E5%AE%B9%E5%99%A8%E7%9A%84%E5%BA%95%E5%8C%85%E9%95%9C%E5%83%8F\">制作tomcat容器的底包镜像</a>\n      <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E4%B8%83%E7%AB%A0%E2%80%94%E2%80%94Promtheus%E7%9B%91%E6%8E%A7k8s%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%BA%94%E7%94%A8.md#%E4%BA%A4%E4%BB%98tomcat%E5%BD%A2%E5%BC%8F%E7%9A%84dubbo%E6%9C%8D%E5%8A%A1%E6%B6%88%E8%B4%B9%E8%80%85%E5%88%B0k8s%E9%9B%86%E7%BE%A4\">交付tomcat形式的dubbo服务消费者到K8S集群</a>\n      <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E4%B8%83%E7%AB%A0%E2%80%94%E2%80%94Promtheus%E7%9B%91%E6%8E%A7k8s%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%BA%94%E7%94%A8.md#%E4%BA%8C%E8%BF%9B%E5%88%B6%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2elasticsearch\">二进制安装部署elasticsearch</a>\n      <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E4%B8%83%E7%AB%A0%E2%80%94%E2%80%94Promtheus%E7%9B%91%E6%8E%A7k8s%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%BA%94%E7%94%A8.md#%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2kafka%E5%92%8Ckafka-manager\">安装部署kafka和kafka-manager</a>\n      <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E4%B8%83%E7%AB%A0%E2%80%94%E2%80%94Promtheus%E7%9B%91%E6%8E%A7k8s%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%BA%94%E7%94%A8.md#%E5%88%B6%E4%BD%9Cfilebeat%E5%BA%95%E5%8C%85%E5%B9%B6%E6%8E%A5%E5%85%A5dubbo%E6%9C%8D%E5%8A%A1%E6%B6%88%E8%B4%B9%E8%80%85\">制作filebeat底包并接入dubbo服务消费者</a>\n      <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E4%B8%83%E7%AB%A0%E2%80%94%E2%80%94Promtheus%E7%9B%91%E6%8E%A7k8s%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%BA%94%E7%94%A8.md#%E9%83%A8%E7%BD%B2logstash%E9%95%9C%E5%83%8F\">部署logstash镜像</a>\n      <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E4%B8%83%E7%AB%A0%E2%80%94%E2%80%94Promtheus%E7%9B%91%E6%8E%A7k8s%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%BA%94%E7%94%A8.md#%E4%BA%A4%E4%BB%98kibana%E5%88%B0k8s%E9%9B%86%E7%BE%A4\">交付kibana到K8S集群</a>\n      <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E4%B8%83%E7%AB%A0%E2%80%94%E2%80%94Promtheus%E7%9B%91%E6%8E%A7k8s%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%BA%94%E7%94%A8.md#%E8%AF%A6%E8%A7%A3kibana%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5%E6%96%B9%E6%B3%95\">详解Kibana生产实践方法</a>\n    </ul>\n  </li>\n  <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E5%85%AB%E7%AB%A0%E2%80%94%E2%80%94spinnaker%E9%83%A8%E7%BD%B2%E4%B8%8E%E5%BA%94%E7%94%A8.md\">第八章——spinaker部署与应用</a>\n    <ul>\n      <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E5%85%AB%E7%AB%A0%E2%80%94%E2%80%94spinnaker%E9%83%A8%E7%BD%B2%E4%B8%8E%E5%BA%94%E7%94%A8.md#%E9%83%A8%E7%BD%B2spinnaker%E7%9A%84amory%E5%8F%91%E8%A1%8C%E7%89%88\">部署Spinnaker的Amory发行版</a>\n      <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E5%85%AB%E7%AB%A0%E2%80%94%E2%80%94spinnaker%E9%83%A8%E7%BD%B2%E4%B8%8E%E5%BA%94%E7%94%A8.md#%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2redis\">安装部署redis</a>\n      <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E5%85%AB%E7%AB%A0%E2%80%94%E2%80%94spinnaker%E9%83%A8%E7%BD%B2%E4%B8%8E%E5%BA%94%E7%94%A8.md#%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2clouddriver\">安装部署clouddriver</a>\n      <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E5%85%AB%E7%AB%A0%E2%80%94%E2%80%94spinnaker%E9%83%A8%E7%BD%B2%E4%B8%8E%E5%BA%94%E7%94%A8.md#%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2spinnaker%E5%85%B6%E4%BD%99%E7%BB%84%E4%BB%B6\">安装部署spinnaker其余组件</a>\n      <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E5%85%AB%E7%AB%A0%E2%80%94%E2%80%94spinnaker%E9%83%A8%E7%BD%B2%E4%B8%8E%E5%BA%94%E7%94%A8.md#%E4%BD%BF%E7%94%A8spinnaker%E7%BB%93%E5%90%88jenkins%E6%9E%84%E5%BB%BA%E9%95%9C%E5%83%8F\">使用spinnaker结合Jenkins构建镜像</a>\n      <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E5%85%AB%E7%AB%A0%E2%80%94%E2%80%94spinnaker%E9%83%A8%E7%BD%B2%E4%B8%8E%E5%BA%94%E7%94%A8.md#%E4%BD%BF%E7%94%A8spinnaker%E9%85%8D%E7%BD%AEdubbo%E6%9C%8D%E5%8A%A1%E6%8F%90%E4%BE%9B%E8%80%85%E5%8F%91%E5%B8%83%E8%87%B3k8s\">使用spinnaker配置dubbo服务提供者发布至K8S</a>\n      <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E5%85%AB%E7%AB%A0%E2%80%94%E2%80%94spinnaker%E9%83%A8%E7%BD%B2%E4%B8%8E%E5%BA%94%E7%94%A8.md#%E4%BD%BF%E7%94%A8spinnaker%E9%85%8D%E7%BD%AEdubbo%E6%9C%8D%E5%8A%A1%E6%B6%88%E8%B4%B9%E8%80%85%E5%88%B0k8s\">使用spinnaker配置dubbo服务消费者到K8S</a>\n      <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E5%85%AB%E7%AB%A0%E2%80%94%E2%80%94spinnaker%E9%83%A8%E7%BD%B2%E4%B8%8E%E5%BA%94%E7%94%A8.md#%E6%A8%A1%E6%8B%9F%E7%94%9F%E4%BA%A7%E4%B8%8A%E4%BB%A3%E7%A0%81%E8%BF%AD%E4%BB%A3\">模拟生产上代码迭代</a>\n    </ul>\n  </li>\n  <li><a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%BB%88%E7%AB%A0%E2%80%94%E2%80%94%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E5%8F%8A%E7%9B%B8%E5%85%B3%E6%96%B9%E6%A1%88.md\">终章——常用操作命令及相关方案</a>\n  </li>\n</ul>\n\n\n\n\n##### 资料参考：\n\n[深入剖析kubernetes](https://time.geekbang.org/column/intro/116)也可以[免费下载（在Docker章节最下面）](https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E4%B8%80%E7%AB%A0%E2%80%94%E2%80%94Docker%EF%BC%88%E5%B7%B2%E7%86%9F%E6%82%89%E7%9A%84%E5%8F%AF%E4%BB%A5%E4%BB%8E%E7%AC%AC%E4%BA%8C%E7%AB%A0%E5%BC%80%E5%A7%8B%EF%BC%89.md#docker%E9%83%A8%E5%88%86%E5%AE%8C%E7%BB%93)\n\n[老男孩教育K8S容器云架构师1期](https://www.luffycity.com/home)\n\n## 互助群\n\nQQ群号：676040917（群未回的，请一定要提Issues或者发邮件给作者）\n\n<div align='left'>\n    <img src=\"assets/1637049717005.png\" alt=\"qq_group\" style=\"max-width: 100%;\">\n</div>\n群内禁止一切广告，只为解决问题而存在。\n\n\n\n## 贡献者\n\n欢迎参与贡献和完善内容，贡献方法参考[CONTRIBUTING](https://github.com/ben1234560/k8s_PaaS/blob/master/CONTRIBUTING.md)。感谢所有的贡献者，贡献列表见[contributors](https://github.com/ben1234560/k8s_PaaS/graphs/contributors)。\n\n另外，感谢一直在群里提供建议和解答的伙伴们，感谢大家无私的开源精神👍👍👍\n\n\n\n## 说明\n\n<a href=\"https://hellogithub.com/repository/f6b4d10a91384d2ca6cd6285a9d3bc60\" target=\"_blank\"><img src=\"https://api.hellogithub.com/v1/widgets/recommend.svg?rid=f6b4d10a91384d2ca6cd6285a9d3bc60&claim_uid=2jDLrGaINM0CKfO\" alt=\"Featured｜HelloGitHub\" style=\"width: 250px; height: 54px;\" width=\"250\" height=\"54\" /></a>\n\n<p> 本专题并不用于商业用途，转载请注明本专题地址，如有侵权，请务必邮件通知作者。\n<p> 本人水平有限，文字代码难免有遗漏错误的地方，望不吝赐教，万分感谢。\n<p> Email：909336740@qq.com\n<p> PS：看到点赞很开心，谢谢😊\n\n"
        },
        {
          "name": "assets",
          "type": "tree",
          "content": null
        },
        {
          "name": "原理及源码解析",
          "type": "tree",
          "content": null
        },
        {
          "name": "第一章——Docker（已熟悉的可以从第二章开始）.md",
          "type": "blob",
          "size": 13.33203125,
          "content": "## 第一章——Docker（已熟悉的可以从第二章开始）\n\n### 安装Docker\n\n> **WHAT**：可以让开发者打包他们的应用以及依赖包到一个轻量级、可移植的容器中，然后发布到任何流行的 Linux 机器上，也可以实现虚拟化，三个核心概念：镜像、容器、仓库\n>\n> - 快速，一致地交付您的应用程序\n>   - 允许开发人员使用您提供的应用程序或服务的本地容器在标准化环境中工作，从而简化了开发的生命周期\n> - 响应式部署和扩展\n>   - 基于容器的平台，允许高度可移植的工作负载，可移植性和轻量级的特性，实时扩展或拆除应用程序和服务\n> - 在同一硬件上运行更多工作负载\n>   - 非常适合于高密度环境以及中小型部署，而您可以用更少的资源做更多的事情\n> - 这里推荐我认为讲的很不错的一篇文章https://blog.csdn.net/deng624796905/article/details/86493330\n>\n> **WHY**：docker的一个核心就是容器（沙箱），在开发环境开发的代码，到测试环境需要调整，到预生产环境也需要调整，到生产环境更加需要调整，而我们想要的是一次部署到处运行，这就是为什么使用docker。\n>\n> **原理/源码解析**：[容器是怎么隔离的](https://github.com/ben1234560/k8s_PaaS/blob/master/%E5%8E%9F%E7%90%86%E5%8F%8A%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/Docker%E5%9F%BA%E7%A1%80.md#%E5%AE%B9%E5%99%A8%E6%98%AF%E6%80%8E%E4%B9%88%E9%9A%94%E7%A6%BB%E7%9A%84)\n>\n> 推荐书籍：深入剖析kubernetes（书籍），你也可以去下载免费的https://pan.baidu.com/s/1gWAQUVsqs1AdMPvRuaEtNA 提取码：q0ht\n\n环境：centos7.6，2核2G内存（1C2G即可）\n\n> 如果你需要7.6的镜像和xshell可以去网上下载或者我提供的包https://pan.baidu.com/s/1mkIzua1XQmew240XBbvuFA 提取码：7p6h。当然如果你下载7.6镜像包比较慢，想先上手做一下，可以联系我QQ：909336740开一个**免费**的服务器（人数过多，已无）\n\n~~~\n# 查看机器信息，内核版本必须是3.8以上\n~]# uname -a\n#out： Linux VM_0_5_centos 3.10.0-957.21.3.el7.x86_64 #1 SMP Tue Jun 18 16:35:19 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux\n~]# cat /etc/redhat-release \n#out： CentOS Linux release 7.6.1810 (Core) \n# 关闭防火墙\n~]# getenforce\n#out： Disabled\n~]# systemctl stop firewalld\n# 查看内存大小\n~]# free -m\n~~~\n\n> **uname**：可显示电脑及操作系统的相关信息，-a/-all：显示全部信息\n>\n> **cat**：用于连接文件并打印内容到页面（打印有两种情况，一种是无执行内容所以就是查看，有执行内容则变成执行里面的内容）\n>\n> **getenforce**：查看防火墙信息，Disabled为关闭模式\n>\n> **systemctl**：用于管理系统，stop：停止某个服务\n>\n> **free**：用于显示内存状态，-m：以MB为单位显示内存使用情况\n\n![1578539634672](assets/1578539634672.png)\n\n~~~\n# 查看网络，并安装必要组件\n~]# ping baidu.com\n# 注意，你的本地源得是删掉的，一般是没有的\n# rm /etc/yum.repos.d/local.repo\n~]#curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo\n~]# yum install epel-release -y\n# 安装docker包\n~]# yum list docker --show-duplicates\n~]# yum install -y yum-utils\n~]# yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo\n~]# yum list docker-ce --show-duplicates\n~]# yum install -y docker-ce\n#out：...Complete!\n~~~\n\n> **ping**：执行ping指令会使用ICMP传输协议，发出要求回应的信息，若远端主机的网络功能没有问题，就会回应该信息，因而得知该主机运作正常\n>\n> **rm** ：命令用于删除一个文件或者目录。\n>\n> **curl**：支持文件的上传和下载，是综合传输工具\n>\n> **yum**：提供了查找、安装、删除某一个、一组甚至全部软件包的命令，\n>\n> - **install**：安装\n>\n> - **-y**：安装过程的提示选择全部为\"yes\"\n\n![1578540090773](assets/1578540090773.png)\n\n~~~\n# 设为开机启动，启动并修改相关配置\n~]# systemctl enable docker\n~]# systemctl start docker\n~]# vi /etc/docker/daemon.json\n{\n  \"data-root\": \"/data/docker\",\n  \"storage-driver\": \"overlay2\",\n  \"insecure-registries\": [\"registry.access.redhat.com\",\"quay.io\"],\n  \"registry-mirrors\": [\"https://q2gr04ke.mirror.aliyuncs.com\"],\n  \"bip\": \"172.7.5.1/24\",\n  \"exec-opts\": [\"native.cgroupdriver=systemd\"],\n  \"live-restore\": true\n}\n\n# 重启docker让配置生效\n~]# systemctl reset-failed docker.service\n~]# systemctl start docker.service\n~]# systemctl restart docker\n# 如果失败了，systemctl status docker查看报错信息\n~]# docker info\n~~~\n\n> **daemon.json**文件内容解析：\n>\n> - graph：docker的工作目录，docker会在下面生成一大堆文件\n> - storage-driver： 存储驱动\n> - insecure-registries：私有仓库\n> - registry-mirrors：国内加速源\n> - bip：docker容器地址（ip的中间两位和我现在的外网129.204.217.99的后两位有对照关系，方便出问题了快速定位在哪个宿主机，但是我这里没改）\n> - live-restrore：容器引擎死掉的事情，起来的docker是否继续活着\n>\n> systemctl status：查看服务信息\n>\n> docker info：查看docker信息\n\n完成\n\n\n\n### 开启我们的第一个docker容器\n\n~~~\n~]# docker run hello-world\n# docker是典型的CS架构\n~~~\n\n> **docker run**：创建一个新的容器并运行，现在本地是没有镜像的，但是它会自动拉取网上的，如下图：\n>\n> - 语法：docker run [OPTIONS] IMAGE [COMMAND] [ARG...]\n\n![1578550425609](assets/1578550425609.png)\n\n![1578550749282](assets/1578550749282.png)\n\n> 上图为docker容器和本地仓库、远程仓库的关系\n\n~~~\n镜像常规结构如下：\n${registry_name}/${repository_name}/${image_name}:${tag_name}\n例如：\ndocker.io/library/alipine:3.10.1\n~~~\n\n完成\n\n\n\n### Dockerhub注册（自己的远程仓库）\n\n![1578551639317](assets/1578551639317.png)\n\n~~~\n# 登录你的远程仓库\n~]# docker login docker.io\n# 你的登录信息在这里\n~]# cat /root/.docker/config.json\n~~~\n\n> 复习：\n>\n> - cat：用于连接文件并打印内容到页面\n\n![1578551787380](assets/1578551787380.png)\n\n完成\n\n\n\n### Docker镜像管理实战\n\n~~~\n~]# docker search alpine\n~]# docker pull alpine\n~]# docker pull alpine:3.10.3\n~]# docker pull alpine:3.10.1\n~~~\n\n> **docker pull**：从镜像仓库中拉取或者更新指定镜像\n>\n> - 语法：**docker pull [OPTIONS] NAME[:TAG|@DIGEST]**\n\n![1578551972565](assets/1578551972565.png)\n\n![1578552027321](assets/1578552027321.png)\n\n~~~\n~]# docker images\n~]# docker image ls\n~]# docker tag 965ea09ff2eb docker.io/909336740/alpine:v3.10.3\n~~~\n\n> **docker images/docker image ls :** 列出本地镜像\n>\n> **docker tag**：标记本地镜像，将其归入某一仓库\n>\n> - 语法：docker tag [OPTIONS] IMAGE[:TAG] [REGISTRYHOST/][USERNAME/]NAME[:TAG]\n\n![1578552367676](assets/1578552367676.png)\n\n~~~\n~]# docker push docker.io/909336740/alpine:v3.10.3\n~~~\n\n> **docker push**：将本地的镜像上传到镜像仓库,要先登陆到镜像仓库，带版本号\n>\n> - 语法：docker push [OPTIONS] NAME[:TAG]\n\n~~~\n~]# docker rmi 965ea09ff2eb\n~]# docker rmi -f 965ea09ff2eb\n# docker pull 909336740/alpine 拉取自己远程仓库镜像\n~~~\n\n> **docker rmi**：删除本地一个或多个镜像\n>\n> - **-f**：强制删除\n\n![1578552844322](assets/1578552844322.png)\n\n![1578553221639](assets/1578553221639.png)\n\n~~~\n# 镜像不管多大，实际线上只会改变变动的部分，并不会全部替换，所以不需要担心速度问题，只有首次比较慢\n~~~\n\n完成\n\n\n\n### docker容器基本操作\n\n~~~\n# 全部有记录的容器进程\n~]# docker ps -a\n# 存活的容器进程\n~]# docker ps\n# 启动容器（运行容器）\n~]# docker run [options] image[command]\n# 过滤出全部已经退出的容器并删掉\n~]# for i in `docker ps -a|grep -i exit|awk '{print $1}'`;do docker rm -f $i;done\n# 查看日志，-f：跟踪日志输出，即是夯住，可以按ctrl+c\n~]# docker logs -f <容器id>\n~~~\n\n> **Ctrl+c:** 强制中断程序的执行\n>\n> **Ctrl+z:** 将任务中断,但是此任务并没有结束,他仍然在进程中他只是维持挂起的状态\n\n### docker容器高级操作\n\n映射端口\n\n- docker run -p 容器外端口:容器内端口\n\n挂载数据卷\n\n- docker run -v 容器外目录:容器内目录\n\n传递环境变量\n\n- docker run -e 环境变量key:环境变量value\n\n查看内容\n\n- docker inspect <容器id>\n\n容器内安装软件（工具）\n\n- yum/apt-get/apt等\n\n~~~\n# 映射端口\n~]# docker pull nginx:1.12.2\n~]# docker images\n~]# docker tag 4037a5562b03 909336740/nginx:v1.12.2\n~]# docker push 909336740/nginx:v1.12.2\n~]# docker images\n~]# docker run --rm --name mynginx -d -p81:80 909336740/nginx:v1.12.2\n# 查看是否起来了\n~]# docker ps -a \n~]# netstat -luntp|grep 81\n~]# curl 127.0.0.1:81\n~]# docker \n~~~\n\n> **docker run**: \n>\n> - **--rm** ：用完即删\n> - **--name**：指定名字\n> - **-d**：放到后台，非交互式的\n> - **-p81:80**：映射端口，宿主机跑81端口，容器（nginx）跑80端口\n>\n> **docker push**：推送到我们的远程仓库（公网）\n>\n> **netstat -luntp**：用于显示 tcp,udp 的端口和进程等相关情况\n>\n> **|grep**：过滤管道\n\n~~~\n# 挂载数据卷\n~]# mkdir html\n~]# cd html/\nhtml]# wget www.baidu.com -O index.html\n~]# docker run -d --rm --name nginx_with_baidu -d -p82:80 -v/root/html:/usr/share/nginx/html 909336740/nginx:v1.12.2\n~]# docker exec -ti nginx_with_baidu /bin/bash\n:/# cd /usr/share/nginx/htm/\n:/htm# ls\n# out: index.html\n:/# curl\n# out:bash: curl:command not found\n:/# tee /etc/apt/sources.list << EOF\ndeb http://mirrors.163.com/debian/ jessie main non-free contrib\ndeb http://mirrors.163.com/debian/ jessie-updates main non-free contrib\nEOF\n:/# apt-get update && apt-get install curl -y\n:/# curl -k https://www.baidu.com\n:/# exit\n~]# docker ps -a\n# 把这个容器push到远程从库，后面会用到\n~]# docker commit -p 60c24fa9c6ff 909336740/nginx:curl\n~]# docker push 909336740/nginx:curl\n~~~\n\n> **-v**：挂载数据卷，/root/html为宿主机的数据卷，/usr/share...为容器的数据卷\n\n![1582086653233](assets/1582086653233.png)\n\n![1578558054990](assets/1578558054990.png)\n\n完成\n\n\n\n### dockerfile\n\n> **WHAT**：通过指令编排镜像，帮你自动化的构建镜像\n\n4组核心的Dockerfile指令\n\n- USER/WORKDIR指令\n- ADD/EXPOSE指令\n- RUN/ENV指令\n- CMD/ENTRYPOINT指令\n\n### dockerfile 综合实验\n\n> 运行一个docker容器，在浏览器打开demo.od.com能访问百度首页\n\n~~~\n\n~]# mkdir /data/dockerfile\n~]# vi /data/dockerfile/Dockerfile\nFROM 909336740/nginx:v1.12.2\nUSER root\nENV WWW /usr/share/nginx/html\nENV CONF /etc/nginx/conf.d\nRUN /bin/cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime &&\\ \n    echo 'Asia/Shanghai' >/etc/timezone\nWORKDIR $WWW\nADD index.html $WWW/index.html\nADD demo.od.com.conf $CONF/demo.od.com.conf\nEXPOSE 80\nCMD [\"nginx\",\"-g\",\"daemon off;\"]\n~~~\n\n> **vi**: 编辑文本\n>\n> **FROM**：从哪里导入\n>\n> **USER**：用什么用户起\n>\n> **ENV**：设置环境变量\n>\n> **RUN**： 修改时区成中国时区'Asia/Shanghai'\n>\n> **WORKDIR**：指定工作目录，这里指定的是之前ENV指定的WWW 目录，即是/usr/share/nginx/html\n>\n> **ADD**：添加指定的东西进去\n>\n> **EXPOSE**：暴露端口\n>\n> **CMD**：指令的首要目的在于为启动的容器指定默认要运行的程序，程序运行结束，容器也就结束\n\n~~~\ndockerfile]# vi demo.od.com.conf\nserver {\n   listen 80;\n   server_name demo.od.com;\n\n   root /usr/share/nginx/html;\n}\n\ndockerfile]# ll\ndockerfile]# wget www.baidu.com -O index.html\ndockerfile]# docker build . -t 909336740/nginx:baidu\ndockerfile]# docker run --rm -p80:80 909336740/nginx:baidu\n~~~\n\n> **ll**：显示当前目录的文件\n>\n> **wget**：下载文件工具\n>\n> - **-O**：并将文档写入后面指定的文件（这里是index.html）\n\n![1578565889942](assets/1578565889942.png)\n\n![1578564445681](assets/1578564445681.png)\n\n![1578563551095](assets/1578563551095.png)\n\n![1578563657846](assets/1578563657846.png)\n\n没有保存权限的参考这个https://jingyan.baidu.com/article/624e7459b194f134e8ba5a8e.html\n\n访问[demo.od.com](demo.od.com)\n\n![1578564376367](assets/1578564376367.png)\n\n完成\n\n\n\n### dockerfile四种网络类型\n\n- Bridge contauner（NAT）   桥接式网络模式(默认)\n- None(Close) container   封闭式网络模式，不为容器配置网络\n- Host(open) container   开放式网络模式，和宿主机共享网络\n- Container(join) container   联合挂载式网络模式，和其他容器共享网络\n\n> 用什么类型的网络要根据我们的业务去决定\n\n\n\n### Docker部分完结\n\n> **原理/源码解析：**\n>\n> - [虚拟机和容器的对比图](https://github.com/ben1234560/k8s_PaaS/blob/master/%E5%8E%9F%E7%90%86%E5%8F%8A%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/Docker%E5%9F%BA%E7%A1%80.md#%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%92%8C%E5%AE%B9%E5%99%A8%E7%9A%84%E5%AF%B9%E6%AF%94%E5%9B%BE)\n> - [深入理解容器镜像](https://github.com/ben1234560/k8s_PaaS/blob/master/%E5%8E%9F%E7%90%86%E5%8F%8A%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/Docker%E5%9F%BA%E7%A1%80.md#%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E5%AE%B9%E5%99%A8%E9%95%9C%E5%83%8F)\n\n恭喜你已经完成了docker的部分，当然，你也许还是云里雾里，不用担心，后续我们会继续用到这些内容，如果你对docker的历史、源码等解析有兴趣，推荐书籍：深入剖析kubernetes（书籍），你也可以去下载免费的https://pan.baidu.com/s/1gWAQUVsqs1AdMPvRuaEtNA 提取码：q0ht\n\n"
        },
        {
          "name": "第七章——Promtheus监控k8s企业级应用.md",
          "type": "blob",
          "size": 82.5546875,
          "content": "## 第七章——Promtheus监控k8s企业级应用\n\n##### 前言：\n\n> 配置是独立于程序的可配变量，同一份程序在不同配置下会有不同的行为。\n\n##### 云原生（Cloud Native）程序的特点：\n\n> - 程序的配置，通过设置环境变了传递到容器内部\n> - 程序的配置，通过程序启动参数配置生效\n> - 程序的配置，通过集中在配置中心进行统一换了（CRUD）\n\n##### Devops工程师应该做什么？\n\n> - 容器化公司自研的应用程序（通过Docker进行二次封装）\n> - 推动容器化应用，转变为云原生应用（一次构建，到处使用）\n> - 使用容器编排框架（kubernetes），合理、规范、专业的编排业务容器\n\n\n\n### Prometheus监控软件概述\n\n> 开源监控告警解决方案，[推荐文章](https://www.jianshu.com/p/60a50539243a)\n>\n> 当然一时半会你可能没那么快的去理解，那就跟我们先做下去你就会慢慢理解什么是时间序列数据\n>\n> [https://github.com/prometheus](https://github.com/prometheus)\n>\n> [https://prometheus.io](https://prometheus.io)\n\n#### Prometheus的特点：\n\n- 多维数据模型：由度量名称和键值对标识的时间序列数据\n- 内置时间序列数据库：TSDB\n- promQL：一种灵活的查询语言，可以利用多维数据完成复杂查询\n- 基于HTTP的pull（拉取）方式采集时间序列数据\n- 同时支持PushGateway组件收集数据\n- 通过服务发现或静态配置发现目标\n- 支持作为数据源接入Grafana\n\n##### 我们将使用的官方架构图\n\n![1582697010557](assets/1582697010557.png)\n\n> **Prometheus Server**：服务核心组件，通过pull metrics从 Exporter 拉取和存储监控数据,并提供一套灵活的查询语言（PromQL）。\n>\n> **pushgateway**：类似一个中转站，Prometheus的server端只会使用pull方式拉取数据，但是某些节点因为某些原因只能使用push方式推送数据，那么它就是用来接收push而来的数据并暴露给Prometheus的server拉取的中转站，这里我们不做它。\n>\n> **Exporters/Jobs**：负责收集目标对象（host, container…）的性能数据，并通过 HTTP 接口供 Prometheus Server 获取。\n>\n> **Service Discovery**：服务发现，Prometheus支持多种服务发现机制：文件，DNS，Consul,Kubernetes,OpenStack,EC2等等。基于服务发现的过程并不复杂，通过第三方提供的接口，Prometheus查询到需要监控的Target列表，然后轮训这些Target获取监控数据。\n>\n> **Alertmanager**：从 Prometheus server 端接收到 alerts 后，会进行去除重复数据，分组，并路由到对方的接受方式，发出报警。常见的接收方式有：电子邮件，pagerduty 等。\n>\n> **UI页面的三种方法**：\n>\n> - Prometheus web UI：自带的（不怎么好用）\n> - Grafana：美观、强大的可视化监控指标展示工具 \n> - API clients：自己开发的监控展示工具\n>\n> **工作流程**：Prometheus Server定期从配置好的Exporters/Jobs中拉metrics，或者来着pushgateway发过来的metrics，或者其它的metrics，收集完后运行定义好的alert.rules（这个文件后面会讲到），记录时间序列或者向Alertmanager推送警报。更多了解<a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E5%8E%9F%E7%90%86%E5%8F%8A%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/Kubernetes%E7%9B%B8%E5%85%B3%E7%94%9F%E6%80%81.md#prometheusmetrics-server%E4%B8%8Ekubernetes%E7%9B%91%E6%8E%A7%E4%BD%93%E7%B3%BB\">Prometheus、Metrics Server与Kubernetes监控体系</a>\n\n##### 和zabbixc对比\n\n| Prometheus                                                   | Zabbix                                                       |\n| ------------------------------------------------------------ | ------------------------------------------------------------ |\n| 后端用golang开发，K8S也是go开发                              | 后端用C开发，界面用PHP开发                                   |\n| 更适合云环境的监控，尤其是对K8S有着更好的支持                | 更适合监控物理机，虚拟机环境                                 |\n| 监控数据存储在基于时间序列的数据库内，便于对已有数据进行新的聚合 | 监控数据存储在关系型数据库内，如MySQL，很难从现有数据中扩展维度 |\n| 自身界面相对较弱，很多配置需要修改配置文件，但可以借由Grafana出图 | 图形化界面相对比较成熟                                       |\n| 支持更大的集群规模，速度也更快                               | 集群规模上线为10000个节点                                    |\n| 2015年后开始快速发展，社区活跃，使用场景越来越多             | 发展实际更长，对于很多监控场景，都有现成的解决方案           |\n\n由于资源问题，我已经把不用的服务关掉了\n\n![1583464421747](assets/1583464421747.png)\n\n![1583464533840](assets/1583464533840.png)\n\n![1583465087708](assets/1583465087708.png)\n\n\n\n### 交付kube-state-metric\n\n> **WHAT**：为prometheus采集k8s资源数据的exporter，能够采集绝大多数k8s内置资源的相关数据，例如pod、deploy、service等等。同时它也提供自己的数据，主要是资源采集个数和采集发生的异常次数统计\n\nhttps://quay.io/repository/coreos/kube-state-metrics?tab=tags\n\n~~~~\n# 200机器，下载包：\n~]# docker pull quay.io/coreos/kube-state-metrics:v1.5.0\n~]# docker images|grep kube-state\n~]# docker tag 91599517197a harbor.od.com/public/kube-state-metrics:v1.5.0\n~]# docker push harbor.od.com/public/kube-state-metrics:v1.5.0\n~~~~\n\n![1583394715770](assets/1583394715770.png)\n\n~~~~shell\n# 200机器，准备资源配置清单：\n~]# mkdir /data/k8s-yaml/kube-state-metrics\n~]# cd /data/k8s-yaml/kube-state-metrics\nkube-state-metrics]# vi rbac.yaml\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  labels:\n    addonmanager.kubernetes.io/mode: Reconcile\n    kubernetes.io/cluster-service: \"true\"\n  name: kube-state-metrics\n  namespace: kube-system\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  labels:\n    addonmanager.kubernetes.io/mode: Reconcile\n    kubernetes.io/cluster-service: \"true\"\n  name: kube-state-metrics\nrules:\n- apiGroups:\n  - \"\"\n  resources:\n  - configmaps\n  - secrets\n  - nodes\n  - pods\n  - services\n  - resourcequotas\n  - replicationcontrollers\n  - limitranges\n  - persistentvolumeclaims\n  - persistentvolumes\n  - namespaces\n  - endpoints\n  verbs:\n  - list\n  - watch\n- apiGroups:\n  - policy\n  resources:\n  - poddisruptionbudgets\n  verbs:\n  - list\n  - watch\n- apiGroups:\n  - extensions\n  resources:\n  - daemonsets\n  - deployments\n  - replicasets\n  verbs:\n  - list\n  - watch\n- apiGroups:\n  - apps\n  resources:\n  - statefulsets\n  verbs:\n  - list\n  - watch\n- apiGroups:\n  - batch\n  resources:\n  - cronjobs\n  - jobs\n  verbs:\n  - list\n  - watch\n- apiGroups:\n  - autoscaling\n  resources:\n  - horizontalpodautoscalers\n  verbs:\n  - list\n  - watch\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  labels:\n    addonmanager.kubernetes.io/mode: Reconcile\n    kubernetes.io/cluster-service: \"true\"\n  name: kube-state-metrics\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: kube-state-metrics\nsubjects:\n- kind: ServiceAccount\n  name: kube-state-metrics\n  namespace: kube-system\n\nkube-state-metrics]# vi dp.yaml\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  annotations:\n    deployment.kubernetes.io/revision: \"2\"\n  labels:\n    grafanak8sapp: \"true\"\n    app: kube-state-metrics\n  name: kube-state-metrics\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      grafanak8sapp: \"true\"\n      app: kube-state-metrics\n  strategy:\n    rollingUpdate:\n      maxSurge: 25%\n      maxUnavailable: 25%\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        grafanak8sapp: \"true\"\n        app: kube-state-metrics\n    spec:\n      containers:\n      - name: kube-state-metrics\n        image: harbor.od.com/public/kube-state-metrics:v1.5.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8080\n          name: http-metrics\n          protocol: TCP\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n      serviceAccountName: kube-state-metrics\n~~~~\n\n![1583396103539](assets/1583396103539.png)\n\n~~~~\n# 应用清单，22机器：\n~]# kubectl apply -f http://k8s-yaml.od.com/kube-state-metrics/rbac.yaml\n~]# kubectl apply -f http://k8s-yaml.od.com/kube-state-metrics/dp.yaml\n# 查询kube-metrics是否正常启动，curl哪个是在dashboard里看到的\n~]# curl 172.7.21.8:8080/healthz\n# out:ok\n# 该命令是查看取出来的信息\n~]# curl 172.7.21.8:8080/metric\n~~~~\n\n![1583396394541](assets/1583396394541.png)\n\n![1583396300678](assets/1583396300678.png)\n\n完成\n\n\n\n### 交付node-exporter\n\n> **WHAT:** 用来监控运算节点上的宿主机的资源信息，需要部署到所有运算节点\n\n[node-exporter官方dockerhub地址](https://hub.docker.com/r/prom/node-exporter)\n\n~~~\n# 200机器，下载镜像并准备资源配置清单：\n~]# docker pull prom/node-exporter:v0.15.0\n~]# docker images|grep node-exporter\n~]# docker tag 12d51ffa2b22 harbor.od.com/public/node-exporter:v0.15.0\n~]# docker push harbor.od.com/public/node-exporter:v0.15.0\n~]# mkdir /data/k8s-yaml/node-exporter/\n~]# cd /data/k8s-yaml/node-exporter/\nnode-exporter]# vi ds.yaml\nkind: DaemonSet\napiVersion: extensions/v1beta1\nmetadata:\n  name: node-exporter\n  namespace: kube-system\n  labels:\n    daemon: \"node-exporter\"\n    grafanak8sapp: \"true\"\nspec:\n  selector:\n    matchLabels:\n      daemon: \"node-exporter\"\n      grafanak8sapp: \"true\"\n  template:\n    metadata:\n      name: node-exporter\n      labels:\n        daemon: \"node-exporter\"\n        grafanak8sapp: \"true\"\n    spec:\n      volumes:\n      - name: proc\n        hostPath: \n          path: /proc\n          type: \"\"\n      - name: sys\n        hostPath:\n          path: /sys\n          type: \"\"\n      containers:\n      - name: node-exporter\n        image: harbor.od.com/public/node-exporter:v0.15.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --path.procfs=/host_proc\n        - --path.sysfs=/host_sys\n        ports:\n        - name: node-exporter\n          hostPort: 9100\n          containerPort: 9100\n          protocol: TCP\n        volumeMounts:\n        - name: sys\n          readOnly: true\n          mountPath: /host_sys\n        - name: proc\n          readOnly: true\n          mountPath: /host_proc\n      hostNetwork: true\n~~~\n\n\n\n~~~\n# 22机器，应用：\n# 先看一下宿主机有没有9100端口，发现什么都没有\n~]# netstat -luntp|grep 9100\n~]# kubectl apply -f http://k8s-yaml.od.com/node-exporter/ds.yaml\n# 创建完再看端口，可能启动的慢些，我是刷了3次才有\n~]# netstat -luntp|grep 9100\n~]# curl localhost:9100\n# 该命令是查看取出来的信息\n~]# curl localhost:9100/metrics\n~~~\n\n![1583396713104](assets/1583396713104.png)\n\n![1583396743927](assets/1583396743927.png)\n\n完成\n\n\n\n### 交付cadvisor\n\n> **WHAT**： 用来监控容器内部使用资源的信息\n\n[cadvisor官方dockerhub镜像](https://hub.docker.com/r/google/cadvisor/tags)\n\n~~~\n# 200机器，下载镜像：\n~]# docker pull google/cadvisor:v0.28.3\n~]# docker images|grep cadvisor\n~]# docker tag 75f88e3ec33 harbor.od.com/public/cadvisor:v0.28.3\n~]# docker push harbor.od.com/public/cadvisor:v0.28.3\n~]# mkdir /data/k8s-yaml/cadvisor/\n~]# cd /data/k8s-yaml/cadvisor/\ncadvisor]# vi ds.yaml\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cadvisor\n  namespace: kube-system\n  labels:\n    app: cadvisor\nspec:\n  selector:\n    matchLabels:\n      name: cadvisor\n  template:\n    metadata:\n      labels:\n        name: cadvisor\n    spec:\n      hostNetwork: true\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoExecute\n      containers:\n      - name: cadvisor\n        image: harbor.od.com/public/cadvisor:v0.28.3\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: rootfs\n          mountPath: /rootfs\n          readOnly: true\n        - name: var-run\n          mountPath: /var/run\n        - name: sys\n          mountPath: /sys\n          readOnly: true\n        - name: docker\n          mountPath: /var/lib/docker\n          readOnly: true\n        ports:\n          - name: http\n            containerPort: 4194\n            protocol: TCP\n        readinessProbe:\n          tcpSocket:\n            port: 4194\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        args:\n          - --housekeeping_interval=10s\n          - --port=4194\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - name: rootfs\n        hostPath:\n          path: /\n      - name: var-run\n        hostPath:\n          path: /var/run\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: docker\n        hostPath:\n          path: /data/docker\n~~~\n\n![1583457963534](assets/1583457963534.png)\n\n> 此时我们看到大多数节点都运行在21机器上，我们人为的让pod调度到22机器（当然即使你的大多数节点都运行在22机器上也没关系）\n\n[^tolerations]: 可人为影响调度策略的方法。为什么需要它：kube-schedule是主控节点的策略，有预选节点和优选节点的策略，但往往生活中调度策略可能不是我们想要的。\n[^tolerations-key]: 是否调度的是某节点，污点可以有多个\n[^tolerations-effect-NoExecute]: 容忍NoExecute，其它的不容忍（如：NoSchedule等），即如过节点上的污点不是NoExecute，就不调度到该节点上，如果是，就可以调度。反之，如果是NoSchedule，那么节点上的污点如果是NoSchedule则可以容器，如果不是，则不可以。\n\n> 可人为影响K8S调度策略的三种方法：\n>\n> - 污点、容忍方法：\n>   - 污点：运算节点node上的污点（先在运算节点上打标签等 kubectl taint nodes node1 key1=value1:NoSchedule），污点可以有多个\n>   - 容忍度：pod是否能够容忍污点\n>   - 参考[kubernetes官网](https:kubernetes.io/zh/docs/concepts/configuration/taint-and-toleration/)\n> - nodeName：让Pod运行再指定的node上\n> - nodeSelector：通过标签选择器，让Pod运行再指定的一类node上\n\n~~~\n# 给21机器打个污点，22机器：\n~]# kubectl taint node hdss7-21.host.com node-role.kubernetes.io/master=master:NoSchedule\n~~~\n\n![1581470696125](assets/1581470696125.png)\n\n![1583458119938](assets/1583458119938.png)\n\n~~~\n# 21/22两个机器，修改软连接：\n~]# mount -o remount,rw /sys/fs/cgroup/\n~]# ln -s /sys/fs/cgroup/cpu,cpuacct /sys/fs/cgroup/cpuacct,cpu\n~]# ls -l /sys/fs/cgroup/\n~~~\n\n> **mount -o remount, rw /sys/fs/cgroup**：重新以可读可写的方式挂载为已经挂载/sys/fs/cgroup\n>\n> **ln -s**：创建对应的软链接\n>\n> **ls -l**：显示不隐藏的文件与文件夹的详细信息\n\n![1583458181722](assets/1583458181722.png)\n\n~~~\n# 22机器，应用资源清单：\n~]# kubectl apply -f http://k8s-yaml.od.com/cadvisor/ds.yaml\n~]# kubectl get pods -n kube-system -o wide\n~~~\n\n只有22机器上有，跟我们预期一样\n\n![1583458264404](assets/1583458264404.png)\n\n~~~\n# 21机器，我们删掉污点：\n~]# kubectl taint node hdss7-21.host.com node-role.kubernetes.io/master-\n# out: node/hdss7-21.host.com untainted\n~~~\n\n看dashboard，污点已经没了\n\n![1583458299094](assets/1583458299094.png)\n\n在去Pods看，污点没了，pod就自动起来了\n\n![1583458316885](assets/1583458316885.png)\n\n完成\n\n再修改下\n\n![1583458394302](assets/1583458394302.png)\n\n\n\n\n\n### 交付blackbox-exporter\n\n> **WHAT**：监控业务容器存活性\n\n~~~\n# 200机器，下载镜像\n~]# docker pull prom/blackbox-exporter:v0.15.1\n~]# docker images|grep blackbox-exporter\n~]# docker tag 81b70b6158be harbor.od.com/public/blackbox-exporter:v0.15.1\n~]# docker push harbor.od.com/public/blackbox-exporter:v0.15.1\n~]# mkdir /data/k8s-yaml/blackbox-exporter\n~]# cd /data/k8s-yaml/blackbox-exporter\nblackbox-exporter]# vi cm.yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  labels:\n    app: blackbox-exporter\n  name: blackbox-exporter\n  namespace: kube-system\ndata:\n  blackbox.yml: |-\n    modules:\n      http_2xx:\n        prober: http\n        timeout: 2s\n        http:\n          valid_http_versions: [\"HTTP/1.1\", \"HTTP/2\"]\n          valid_status_codes: [200,301,302]\n          method: GET\n          preferred_ip_protocol: \"ip4\"\n      tcp_connect:\n        prober: tcp\n        timeout: 2s\n\nblackbox-exporter]# vi dp.yaml\nkind: Deployment\napiVersion: extensions/v1beta1\nmetadata:\n  name: blackbox-exporter\n  namespace: kube-system\n  labels:\n    app: blackbox-exporter\n  annotations:\n    deployment.kubernetes.io/revision: 1\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: blackbox-exporter\n  template:\n    metadata:\n      labels:\n        app: blackbox-exporter\n    spec:\n      volumes:\n      - name: config\n        configMap:\n          name: blackbox-exporter\n          defaultMode: 420\n      containers:\n      - name: blackbox-exporter\n        image: harbor.od.com/public/blackbox-exporter:v0.15.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --config.file=/etc/blackbox_exporter/blackbox.yml\n        - --log.level=info\n        - --web.listen-address=:9115\n        ports:\n        - name: blackbox-port\n          containerPort: 9115\n          protocol: TCP\n        resources:\n          limits:\n            cpu: 200m\n            memory: 256Mi\n          requests:\n            cpu: 100m\n            memory: 50Mi\n        volumeMounts:\n        - name: config\n          mountPath: /etc/blackbox_exporter\n        readinessProbe:\n          tcpSocket:\n            port: 9115\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          failureThreshold: 3\n\nblackbox-exporter]# vi svc.yaml\nkind: Service\napiVersion: v1\nmetadata:\n  name: blackbox-exporter\n  namespace: kube-system\nspec:\n  selector:\n    app: blackbox-exporter\n  ports:\n    - name: blackbox-port\n      protocol: TCP\n      port: 9115\n\nblackbox-exporter]# vi ingress.yaml\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: blackbox-exporter\n  namespace: kube-system\nspec:\n  rules:\n  - host: blackbox.od.com\n    http:\n      paths:\n      - path: /\n        backend:\n          serviceName: blackbox-exporter\n          servicePort: blackbox-port\n~~~\n\n![1583460099035](assets/1583460099035.png)\n\n~~~\n# 11机器，解析域名：\n~]# vi /var/named/od.com.zone\nserial 前滚一位\nblackbox           A    10.4.7.10\n\n~]# systemctl restart named\n# 22机器\n~]# dig -t A blackbox.od.com @192.168.0.2 +short\n# out: 10.4.7.10\n~~~\n\n![1583460226033](assets/1583460226033.png)\n\n~~~\n# 22机器，应用：\n~]# kubectl apply -f http://k8s-yaml.od.com/blackbox-exporter/cm.yaml\n~]# kubectl apply -f http://k8s-yaml.od.com/blackbox-exporter/dp.yaml\n~]# kubectl apply -f http://k8s-yaml.od.com/blackbox-exporter/svc.yaml\n~]# kubectl apply -f http://k8s-yaml.od.com/blackbox-exporter/ingress.yaml\n~~~\n\n![1583460413279](assets/1583460413279.png)\n\n[blackbox.od.com](blackbox.od.com)\n\n![1583460433145](assets/1583460433145.png)\n\n完成\n\n\n\n### 安装部署Prometheus-server\n\n> **WHAT**：服务核心组件，通过pull metrics从 Exporter 拉取和存储监控数据,并提供一套灵活的查询语言（PromQL）\n\n[prometheus-server官网docker地址](https://hub.docker.com/r/prom/prometheus)\n\n~~~~\n# 200机器，准备镜像、资源清单：\n~]# docker pull prom/prometheus:v2.14.0\n~]# docker images|grep prometheus\n~]# docker tag 7317640d555e harbor.od.com/infra/prometheus:v2.14.0\n~]# docker push harbor.od.com/infra/prometheus:v2.14.0\n~]# mkdir /data/k8s-yaml/prometheus\n~]# cd /data/k8s-yaml/prometheus\nprometheus]# vi rbac.yaml\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  labels:\n    addonmanager.kubernetes.io/mode: Reconcile\n    kubernetes.io/cluster-service: \"true\"\n  name: prometheus\n  namespace: infra\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  labels:\n    addonmanager.kubernetes.io/mode: Reconcile\n    kubernetes.io/cluster-service: \"true\"\n  name: prometheus\nrules:\n- apiGroups:\n  - \"\"\n  resources:\n  - nodes\n  - nodes/metrics\n  - services\n  - endpoints\n  - pods\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups:\n  - \"\"\n  resources:\n  - configmaps\n  verbs:\n  - get\n- nonResourceURLs:\n  - /metrics\n  verbs:\n  - get\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  labels:\n    addonmanager.kubernetes.io/mode: Reconcile\n    kubernetes.io/cluster-service: \"true\"\n  name: prometheus\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: prometheus\nsubjects:\n- kind: ServiceAccount\n  name: prometheus\n  namespace: infra\n\nprometheus]# vi dp.yaml\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  annotations:\n    deployment.kubernetes.io/revision: \"5\"\n  labels:\n    name: prometheus\n  name: prometheus\n  namespace: infra\nspec:\n  progressDeadlineSeconds: 600\n  replicas: 1\n  revisionHistoryLimit: 7\n  selector:\n    matchLabels:\n      app: prometheus\n  strategy:\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 1\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: prometheus\n    spec:\n      containers:\n      - name: prometheus\n        image: harbor.od.com/infra/prometheus:v2.14.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/prometheus\n        args:\n        - --config.file=/data/etc/prometheus.yml\n        - --storage.tsdb.path=/data/prom-db\n        - --storage.tsdb.min-block-duration=10m\n        - --storage.tsdb.retention=72h\n        ports:\n        - containerPort: 9090\n          protocol: TCP\n        volumeMounts:\n        - mountPath: /data\n          name: data\n        resources:\n          requests:\n            cpu: \"1000m\"\n            memory: \"1.5Gi\"\n          limits:\n            cpu: \"2000m\"\n            memory: \"3Gi\"\n      imagePullSecrets:\n      - name: harbor\n      securityContext:\n        runAsUser: 0\n      serviceAccountName: prometheus\n      volumes:\n      - name: data\n        nfs:\n          server: hdss7-200\n          path: /data/nfs-volume/prometheus\n\nprometheus]# vi svc.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: prometheus\n  namespace: infra\nspec:\n  ports:\n  - port: 9090\n    protocol: TCP\n    targetPort: 9090\n  selector:\n    app: prometheus\n\nprometheus]# vi ingress.yaml\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  annotations:\n    kubernetes.io/ingress.class: traefik\n  name: prometheus\n  namespace: infra\nspec:\n  rules:\n  - host: prometheus.od.com\n    http:\n      paths:\n      - path: /\n        backend:\n          serviceName: prometheus\n          servicePort: 9090\n\n# 准备prometheus的配置文件：\nprometheus]# mkdir /data/nfs-volume/prometheus\nprometheus]# cd /data/nfs-volume/prometheus\nprometheus]# mkdir {etc,prom-db}\nprometheus]# cd etc/\netc]# cp /opt/certs/ca.pem .\netc]# cp -a /opt/certs/client.pem .\netc]# cp -a /opt/certs/client-key.pem .\netc]# prometheus.yml\nglobal:\n  scrape_interval:     15s\n  evaluation_interval: 15s\nscrape_configs:\n- job_name: 'etcd'\n  tls_config:\n    ca_file: /data/etc/ca.pem\n    cert_file: /data/etc/client.pem\n    key_file: /data/etc/client-key.pem\n  scheme: https\n  static_configs:\n  - targets:\n    - '10.4.7.12:2379'\n    - '10.4.7.21:2379'\n    - '10.4.7.22:2379'\n- job_name: 'kubernetes-apiservers'\n  kubernetes_sd_configs:\n  - role: endpoints\n  scheme: https\n  tls_config:\n    ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n  bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token\n  relabel_configs:\n  - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]\n    action: keep\n    regex: default;kubernetes;https\n- job_name: 'kubernetes-pods'\n  kubernetes_sd_configs:\n  - role: pod\n  relabel_configs:\n  - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]\n    action: keep\n    regex: true\n  - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]\n    action: replace\n    target_label: __metrics_path__\n    regex: (.+)\n  - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]\n    action: replace\n    regex: ([^:]+)(?::\\d+)?;(\\d+)\n    replacement: $1:$2\n    target_label: __address__\n  - action: labelmap\n    regex: __meta_kubernetes_pod_label_(.+)\n  - source_labels: [__meta_kubernetes_namespace]\n    action: replace\n    target_label: kubernetes_namespace\n  - source_labels: [__meta_kubernetes_pod_name]\n    action: replace\n    target_label: kubernetes_pod_name\n- job_name: 'kubernetes-kubelet'\n  kubernetes_sd_configs:\n  - role: node\n  relabel_configs:\n  - action: labelmap\n    regex: __meta_kubernetes_node_label_(.+)\n  - source_labels: [__meta_kubernetes_node_name]\n    regex: (.+)\n    target_label: __address__\n    replacement: ${1}:10255\n- job_name: 'kubernetes-cadvisor'\n  kubernetes_sd_configs:\n  - role: node\n  relabel_configs:\n  - action: labelmap\n    regex: __meta_kubernetes_node_label_(.+)\n  - source_labels: [__meta_kubernetes_node_name]\n    regex: (.+)\n    target_label: __address__\n    replacement: ${1}:4194\n- job_name: 'kubernetes-kube-state'\n  kubernetes_sd_configs:\n  - role: pod\n  relabel_configs:\n  - action: labelmap\n    regex: __meta_kubernetes_pod_label_(.+)\n  - source_labels: [__meta_kubernetes_namespace]\n    action: replace\n    target_label: kubernetes_namespace\n  - source_labels: [__meta_kubernetes_pod_name]\n    action: replace\n    target_label: kubernetes_pod_name\n  - source_labels: [__meta_kubernetes_pod_label_grafanak8sapp]\n    regex: .*true.*\n    action: keep\n  - source_labels: ['__meta_kubernetes_pod_label_daemon', '__meta_kubernetes_pod_node_name']\n    regex: 'node-exporter;(.*)'\n    action: replace\n    target_label: nodename\n- job_name: 'blackbox_http_pod_probe'\n  metrics_path: /probe\n  kubernetes_sd_configs:\n  - role: pod\n  params:\n    module: [http_2xx]\n  relabel_configs:\n  - source_labels: [__meta_kubernetes_pod_annotation_blackbox_scheme]\n    action: keep\n    regex: http\n  - source_labels: [__address__, __meta_kubernetes_pod_annotation_blackbox_port,  __meta_kubernetes_pod_annotation_blackbox_path]\n    action: replace\n    regex: ([^:]+)(?::\\d+)?;(\\d+);(.+)\n    replacement: $1:$2$3\n    target_label: __param_target\n  - action: replace\n    target_label: __address__\n    replacement: blackbox-exporter.kube-system:9115\n  - source_labels: [__param_target]\n    target_label: instance\n  - action: labelmap\n    regex: __meta_kubernetes_pod_label_(.+)\n  - source_labels: [__meta_kubernetes_namespace]\n    action: replace\n    target_label: kubernetes_namespace\n  - source_labels: [__meta_kubernetes_pod_name]\n    action: replace\n    target_label: kubernetes_pod_name\n- job_name: 'blackbox_tcp_pod_probe'\n  metrics_path: /probe\n  kubernetes_sd_configs:\n  - role: pod\n  params:\n    module: [tcp_connect]\n  relabel_configs:\n  - source_labels: [__meta_kubernetes_pod_annotation_blackbox_scheme]\n    action: keep\n    regex: tcp\n  - source_labels: [__address__, __meta_kubernetes_pod_annotation_blackbox_port]\n    action: replace\n    regex: ([^:]+)(?::\\d+)?;(\\d+)\n    replacement: $1:$2\n    target_label: __param_target\n  - action: replace\n    target_label: __address__\n    replacement: blackbox-exporter.kube-system:9115\n  - source_labels: [__param_target]\n    target_label: instance\n  - action: labelmap\n    regex: __meta_kubernetes_pod_label_(.+)\n  - source_labels: [__meta_kubernetes_namespace]\n    action: replace\n    target_label: kubernetes_namespace\n  - source_labels: [__meta_kubernetes_pod_name]\n    action: replace\n    target_label: kubernetes_pod_name\n- job_name: 'traefik'\n  kubernetes_sd_configs:\n  - role: pod\n  relabel_configs:\n  - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scheme]\n    action: keep\n    regex: traefik\n  - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]\n    action: replace\n    target_label: __metrics_path__\n    regex: (.+)\n  - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]\n    action: replace\n    regex: ([^:]+)(?::\\d+)?;(\\d+)\n    replacement: $1:$2\n    target_label: __address__\n  - action: labelmap\n    regex: __meta_kubernetes_pod_label_(.+)\n  - source_labels: [__meta_kubernetes_namespace]\n    action: replace\n    target_label: kubernetes_namespace\n  - source_labels: [__meta_kubernetes_pod_name]\n    action: replace\n    target_label: kubernetes_pod_name\n~~~~\n\n> **cp -a**：在复制目录时使用，它保留链接、文件属性，并复制目录下的所有内容\n\n![1583461823355](assets/1583461823355.png)\n\n~~~\n# 11机器， 解析域名，有ingress就有页面就需要解析：\n~]# vi /var/named/od.com.zone\nserial 前滚一位\nprometheus         A    10.4.7.10\n\n~]# systemctl restart named\n~]# dig -t A prometheus.od.com @10.4.7.11 +short\n# out:10.4.7.10\n~~~\n\n![1582704423890](assets/1582704423890.png)\n\n~~~\n# 22机器，应用配置清单：\n~]# kubectl apply -f http://k8s-yaml.od.com/prometheus/rbac.yaml\n~]# kubectl apply -f http://k8s-yaml.od.com/prometheus/dp.yaml\n~]# kubectl apply -f http://k8s-yaml.od.com/prometheus/svc.yaml\n~]# kubectl apply -f http://k8s-yaml.od.com/prometheus/ingress.yaml\n~~~\n\n![1583461941453](assets/1583461941453.png)\n\n![1583462134250](assets/1583462134250.png)\n\n[prometheus.od.com](prometheus.od.com)\n\n> 这就是Prometheus自带的UI页面，现在你就知道为什么我们需要Grafana来替代了，如果你还不清楚，等下看Grafana的页面你就知道了\n\n![1583462164465](assets/1583462164465.png)\n\n![1583462217169](assets/1583462217169.png)\n\n完成\n\n\n\n### 配置Prometheus监控业务容器\n\n##### 先配置traefik\n\n![1583462282296](assets/1583462282296.png)\n\n~~~\n# Edit a Daemon Set，添加以下内容，记得给上面加逗号:\n\"annotations\": {\n  \"prometheus_io_scheme\": \"traefik\",\n  \"prometheus_io_path\": \"/metrics\",\n  \"prometheus_io_port\": \"8080\"\n}\n# 直接加进去update，会自动对齐 \n~~~\n\n![1583462379871](assets/1583462379871.png)\n\n删掉两个对应的pod让它重启\n\n![1583462451073](assets/1583462451073.png)\n\n~~~\n# 22机器，查看下，如果起不来就用命令行的方式强制删除：\n~]# kubectl get pods -n kube-system\n~]# kubectl delete pods traefik-ingress-g26kw -n kube-system --force --grace-period=0\n~~~\n\n![1583462566364](assets/1583462566364.png)\n\n启动成功后，去Prometheus查看\n\n刷新后，可以看到是traefik2/2，已经有了\n\n![1583462600531](assets/1583462600531.png)\n\n完成\n\n\n\n##### blackbox\n\n我们起一个dubbo-service，之前我们最后做的是Apollo的版本，现在我们的Apollo已经关了（因为消耗资源），现在需要起更早之前不是Apollo的版本。\n\n我们去harbor里面找\n\n![1583465190219](assets/1583465190219.png)\n\n> 我的Apollo的版本可能比你的多一个，不用在意，那是做实验弄的\n\n修改版本信息\n\n![1583466214230](assets/1583466214230.png)\n\n![1583466251914](assets/1583466251914.png)\n\n在把scale改成1\n\n![1583466284890](assets/1583466284890.png)\n\n查看POD的LOGS日志\n\n![1583466310189](assets/1583466310189.png)\n\n翻页查看，已经启动\n\n![1583466328146](assets/1583466328146.png)\n\n如何监控存活性，只需要修改配置\n\n![1584699708597](assets/1584699708597.png)\n\n~~~\n# Edit a Deployment（TCP），添加以下内容\n\"annotations\": {\n  \"blackbox_port\": \"20880\",\n  \"blackbox_scheme\": \"tcp\"\n}\n# 直接加进去update，会自动对齐 \n~~~\n\n![1583466938931](assets/1583466938931.png)\n\nUPDATE后，已经running起来了\n\n![1583467301614](assets/1583467301614.png)\n\n[prometheus.od.com](prometheus.od.com)刷新，自动发现业务\n\n![1583466979716](assets/1583466979716.png)\n\n[blackbox.od.com](blackbox.od.com) 刷新\n\n![1583467331128](assets/1583467331128.png)\n\n同样的，我们把dubbo-consumer也弄进来\n\n先去harbor找一个不是Apollo的版本（为什么要用不是Apollo的版本前面已经说了）\n\n![1583503611435](assets/1583503611435.png)\n\n修改版本信息，并添加annotations\n\n~~~\n# Edit a Deployment(http)，添加以下内容，记得前面的逗号\n\"annotations\":{\n  \"blackbox_path\": \"/hello?name=health\",\n  \"blackbox_port\": \"8080\",\n  \"blackbox_scheme\": \"http\"\n}\n# 直接加进去update，会自动对齐 \n~~~\n\n![1583503670313](assets/1583503670313.png)\n\n![1583504095794](assets/1583504095794.png)\n\nUPDATE后，把scale改成1\n\n![1583503796291](assets/1583503796291.png)\n\n确保起来了\n\n![1583503811855](assets/1583503811855.png)\n\n![1583503829457](assets/1583503829457.png)\n\n[prometheus.od.com](prometheus.od.com)刷新，自动发现业务\n\n![1583503935815](assets/1583503935815.png)\n\n[blackbox.od.com](blackbox.od.com) 刷新\n\n![1583504112078](assets/1583504112078.png)\n\n\n\n### 安装部署配置Grafana\n\n> **WHAT**：美观、强大的可视化监控指标展示工具 \n>\n> **WHY**：用来代替prometheus原生UI界面\n\n~~~\n# 200机器，准备镜像、资源配置清单：\n~]# docker pull grafana/grafana:5.4.2\n~]# docker images|grep grafana\n~]# docker tag 6f18ddf9e552 harbor.od.com/infra/grafana:v5.4.2\n~]# docker push harbor.od.com/infra/grafana:v5.4.2\n~]# mkdir /data/k8s-yaml/grafana/ /data/nfs-volume/grafana\n~]# cd /data/k8s-yaml/grafana/\ngrafana]# vi rbac.yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  labels:\n    addonmanager.kubernetes.io/mode: Reconcile\n    kubernetes.io/cluster-service: \"true\"\n  name: grafana\nrules:\n- apiGroups:\n  - \"*\"\n  resources:\n  - namespaces\n  - deployments\n  - pods\n  verbs:\n  - get\n  - list\n  - watch\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  labels:\n    addonmanager.kubernetes.io/mode: Reconcile\n    kubernetes.io/cluster-service: \"true\"\n  name: grafana\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: grafana\nsubjects:\n- kind: User\n  name: k8s-node\n\ngrafana]# vi dp.yaml\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  labels:\n    app: grafana\n    name: grafana\n  name: grafana\n  namespace: infra\nspec:\n  progressDeadlineSeconds: 600\n  replicas: 1\n  revisionHistoryLimit: 7\n  selector:\n    matchLabels:\n      name: grafana\n  strategy:\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 1\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: grafana\n        name: grafana\n    spec:\n      containers:\n      - name: grafana\n        image: harbor.od.com/infra/grafana:v5.4.2\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 3000\n          protocol: TCP\n        volumeMounts:\n        - mountPath: /var/lib/grafana\n          name: data\n      imagePullSecrets:\n      - name: harbor\n      securityContext:\n        runAsUser: 0\n      volumes:\n      - nfs:\n          server: hdss7-200\n          path: /data/nfs-volume/grafana\n        name: data\n\ngrafana]# vi svc.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: grafana\n  namespace: infra\nspec:\n  ports:\n  - port: 3000\n    protocol: TCP\n    targetPort: 3000\n  selector:\n    app: grafana\n\ngrafana]# vi ingress.yaml\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: grafana\n  namespace: infra\nspec:\n  rules:\n  - host: grafana.od.com\n    http:\n      paths:\n      - path: /\n        backend:\n          serviceName: grafana\n          servicePort: 3000\n\n~~~\n\n![1583504719781](assets/1583504719781.png)\n\n~~~\n# 11机器，解析域名:\n~]# vi /var/named/od.com.zone\nserial 前滚一位\n\ngrafana            A    10.4.7.10\n~]# systemctl restart named\n~]# ping grafana.od.com\n~~~\n\n![1582705048800](assets/1582705048800.png)\n\n~~~~\n# 22机器，应用配置清单：\n~]# kubectl apply -f http://k8s-yaml.od.com/grafana/rbac.yaml\n~]# kubectl apply -f http://k8s-yaml.od.com/grafana/dp.yaml\n~]# kubectl apply -f http://k8s-yaml.od.com/grafana/svc.yaml\n~]# kubectl apply -f http://k8s-yaml.od.com/grafana/ingress.yaml\n~~~~\n\n![1583504865941](assets/1583504865941.png)\n\n[grafana.od.com](grafana.od.com)\n\n默认账户和密码都是admin\n\n修改密码：admin123\n\n![1583504898029](assets/1583504898029.png)\n\n修改配置，修改如下图\n\n![1583505029816](assets/1583505029816.png)\n\n##### 装插件\n\n进入容器\n\n![1583505097409](assets/1583505097409.png)\n\n~~~\n# 第一个：kubenetes App\ngrafana# grafana-cli plugins install grafana-kubernetes-app\n# 第二个：Clock Pannel\ngrafana# grafana-cli plugins install grafana-clock-panel\n# 第三个：Pie Chart\ngrafana# grafana-cli plugins install grafana-piechart-panel\n# 第四个：D3Gauge\ngrafana# grafana-cli plugins install briangann-gauge-panel\n# 第五个：Discrete\ngrafana# grafana-cli plugins install natel-discrete-panel\n~~~\n\n![1583505305939](assets/1583505305939.png)\n\n装完后，可以在200机器查看\n\n~~~\n# 200机器：\ncd /data/nfs-volume/grafana/plugins/\nplugins]# ll\n~~~\n\n![1583505462177](assets/1583505462177.png)\n\n删掉让它重启\n\n![1583505490948](assets/1583505490948.png)\n\n重启完成后\n\n\n\n查看[grafana.od.com](grafana.od.com)，刚刚安装的5个插件都在里面了（记得检查是否在里面了）\n\n![1583505547061](assets/1583505547061.png)\n\n##### 添加数据源：Add data source\n\n![1583505581557](assets/1583505581557.png)\n\n![1583505600031](assets/1583505600031.png)\n\n~~~\n# 填入参数：\nURL:http://prometheus.od.com\nTLS Client Auth✔    With CA Cert✔\n~~~\n\n![1583505840252](assets/1583505840252.png)\n\n~~~\n# 填入参数对应的pem参数：\n# 200机器拿ca等：\n~]# cat /opt/certs/ca.pem\n~]# cat /opt/certs/client.pem\n~]# cat /opt/certs/client-key.pem\n~~~\n\n![1583505713033](assets/1583505713033.png)\n\n![1583505856093](assets/1583505856093.png)\n\n保存\n\n然后我们去配置plugins里面的kubernetes\n\n![1583505923109](assets/1583505923109.png)\n\n![1583505938700](assets/1583505938700.png)\n\n右侧就多了个按钮，点击进去\n\n![1583505969865](assets/1583505969865.png)\n\n~~~\n# 按参数填入：\nName:myk8s\nURL:https://10.4.7.10:7443\nAccess:Server\nTLS Client Auth✔    With CA Cert✔\n~~~\n\n![1583506058483](assets/1583506058483.png)\n\n~~~\n# 填入参数：\n# 200机器拿ca等：\n~]# cat /opt/certs/ca.pem\n~]# cat /opt/certs/client.pem\n~]# cat /opt/certs/client-key.pem\n~~~\n\n![1583506131529](assets/1583506131529.png)\n\nsave后再点击右侧框的图标，并点击Name\n\n![1583506163546](assets/1583506163546.png)\n\n可能抓取数据的时间会稍微慢些（两分钟左右）\n\n![1583506184293](assets/1583506184293.png)\n\n![1583506503213](assets/1583506503213.png)\n\n点击右上角的K8s Cluster，选择你要看的东西\n\n![1583506545308](assets/1583506545308.png)\n\n由于K8s Container里面数据不全，如下图\n\n![1583506559069](assets/1583506559069.png)\n\n我们改下，把Cluster删了\n\n![1583506631392](assets/1583506631392.png)\n\n![1583506645982](assets/1583506645982.png)\n\ncontainer也删了\n\n![1583506675876](assets/1583506675876.png)\n\ndeployment也删了\n\n![1583506695618](assets/1583506695618.png)\n\nnode也删了\n\n![1583506709705](assets/1583506709705.png)\n\n![1583506730713](assets/1583506730713.png)\n\n![1583506744138](assets/1583506744138.png)\n\n把我给你准备的dashboard的json文件import进来\n\n![1583543092886](assets/1583543092886.png)\n\n![1583543584476](assets/1583543584476.png)\n\n![1583543602130](assets/1583543602130.png)\n\n用同样的方法把node、deployment、cluster、container这4个分别import进来\n\n![1583543698727](assets/1583543698727.png)\n\n可以都看一下，已经正常了\n\n然后把etcd、generic、traefik也import进来\n\n![1583543809740](assets/1583543809740.png)\n\n![1583543831830](assets/1583543831830.png)\n\n还有另外一种import的方法（使用官网的）：\n\n[grafana官网](https://grafana.com/grafana/dashboards)\n\n找一个别人写好的点进去\n\n![1584241883144](assets/1584241883144.png)\n\n这个编号可以直接用\n\n![1584241903882](assets/1584241903882.png)\n\n如下图，我们装blackbox的编号是9965\n\n![1584242072703](assets/1584242072703.png)\n\n![1584242093513](assets/1584242093513.png)\n\n把名字和Prometheus修改一下\n\n![1584242164621](assets/1584242164621.png)\n\n或者，你也可以用我上传的（我用的是7587）\n\n![1583543931644](assets/1583543931644.png)\n\n你可以两个都用，自己做对比，都留着也可以，就是占一些资源\n\nJMX\n\n![1583544009027](assets/1583544009027.png)\n\n这个里面还什么都没有\n\n![1583544017606](assets/1583544017606.png)\n\n\n\n#### 把Dubbo微服务数据弄到Grafana\n\ndubbo-service\n\n![1583544062372](assets/1583544062372.png)\n\n~~~\n# Edit a Daemon Set，添加以下内容，注意给上一行加逗号\n  \"prometheus_io_scrape\": \"true\",\n  \"prometheus_io_port\": \"12346\",\n  \"prometheus_io_path\": \"/\"\n# 直接加进去update，会自动对齐，\n~~~\n\n![1583544144136](assets/1583544144136.png)\n\ndubbo-consumer\n\n![1583544157268](assets/1583544157268.png)\n\n~~~\n# Edit a Daemon Set，添加以下内容，注意给上一行加逗号\n  \"prometheus_io_scrape\": \"true\",\n  \"prometheus_io_port\": \"12346\",\n  \"prometheus_io_path\": \"/\"\n# 直接加进去update，会自动对齐，\n~~~\n\n![1583544192459](assets/1583544192459.png)\n\n刷新JMX（可能有点慢，我等了1分钟才出来service，我机器不行了）\n\n![1583544446817](assets/1583544446817.png)\n\n完成\n\n> 此时你可以感受到，Grafana明显比K8S自带的UI界面更加人性化\n\n\n\n### 安装部署alertmanager\n\n> **WHAT**： 从 Prometheus server 端接收到 alerts 后，会进行去除重复数据，分组，并路由到对方的接受方式，发出报警。常见的接收方式有：电子邮件，pagerduty 等。\n>\n> **WHY**：使得系统的警告随时让我们知道\n\n~~~\n# 200机器，准备镜像、资源清单：\n~]# mkdir /data/k8s-yaml/alertmanager\n~]# cd /data/k8s-yaml/alertmanager\nalertmanager]# docker pull docker.io/prom/alertmanager:v0.14.0\n# 注意，这里你如果不用14版本可能会报错\nalertmanager]# docker images|grep alert\nalertmanager]# docker tag 23744b2d645c harbor.od.com/infra/alertmanager:v0.14.0\nalertmanager]# docker push harbor.od.com/infra/alertmanager:v0.14.0\n# 注意下面记得修改成你自己的邮箱等信息，还有中文注释可以删掉\nalertmanager]# vi cm.yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: alertmanager-config\n  namespace: infra\ndata:\n  config.yml: |-\n    global:\n      # 在没有报警的情况下声明为已解决的时间\n      resolve_timeout: 5m\n      # 配置邮件发送信息\n      smtp_smarthost: 'smtp.163.com:25'\n      smtp_from: 'ben909336740@163.com'\n      smtp_auth_username: 'ben909336740@163.com'\n      smtp_auth_password: 'xxxxxx'\n      smtp_require_tls: false\n    # 所有报警信息进入后的根路由，用来设置报警的分发策略\n    route:\n      # 这里的标签列表是接收到报警信息后的重新分组标签，例如，接收到的报警信息里面有许多具有 cluster=A 和 alertname=LatncyHigh 这样的标签的报警信息将会批量被聚合到一个分组里面\n      group_by: ['alertname', 'cluster']\n      # 当一个新的报警分组被创建后，需要等待至少group_wait时间来初始化通知，这种方式可以确保您能有足够的时间为同一分组来获取多个警报，然后一起触发这个报警信息。\n      group_wait: 30s\n\n      # 当第一个报警发送后，等待'group_interval'时间来发送新的一组报警信息。\n      group_interval: 5m\n\n      # 如果一个报警信息已经发送成功了，等待'repeat_interval'时间来重新发送他们\n      repeat_interval: 5m\n\n      # 默认的receiver：如果一个报警没有被一个route匹配，则发送给默认的接收器\n      receiver: default\n\n    receivers:\n    - name: 'default'\n      email_configs:\n      - to: '909336740@qq.com'\n        send_resolved: true\n\nalertmanager]# vi dp.yaml\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: alertmanager\n  namespace: infra\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: alertmanager\n  template:\n    metadata:\n      labels:\n        app: alertmanager\n    spec:\n      containers:\n      - name: alertmanager\n        image: harbor.od.com/infra/alertmanager:v0.14.0\n        args:\n          - \"--config.file=/etc/alertmanager/config.yml\"\n          - \"--storage.path=/alertmanager\"\n        ports:\n        - name: alertmanager\n          containerPort: 9093\n        volumeMounts:\n        - name: alertmanager-cm\n          mountPath: /etc/alertmanager\n      volumes:\n      - name: alertmanager-cm\n        configMap:\n          name: alertmanager-config\n      imagePullSecrets:\n      - name: harbor\n\nalertmanager]# vi svc.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: alertmanager\n  namespace: infra\nspec:\n  selector: \n    app: alertmanager\n  ports:\n    - port: 80\n      targetPort: 9093\n~~~\n\n![1583547933312](assets/1583547933312.png)\n\n~~~\n# 22机器，应用清单：\n~]# kubectl apply -f http://k8s-yaml.od.com/alertmanager/cm.yaml\n~]# kubectl apply -f http://k8s-yaml.od.com/alertmanager/dp.yaml\n~]# kubectl apply -f http://k8s-yaml.od.com/alertmanager/svc.yaml\n~~~\n\n![1583545326722](assets/1583545326722.png)\n\n![1583545352951](assets/1583545352951.png)\n\n~~~\n# 200机器，配置报警规则：\n~]# vi /data/nfs-volume/prometheus/etc/rules.yml\ngroups:\n- name: hostStatsAlert\n  rules:\n  - alert: hostCpuUsageAlert\n    expr: sum(avg without (cpu)(irate(node_cpu{mode!='idle'}[5m]))) by (instance) > 0.85\n    for: 5m\n    labels:\n      severity: warning\n    annotations:\n      summary: \"{{ $labels.instance }} CPU usage above 85% (current value: {{ $value }}%)\"\n  - alert: hostMemUsageAlert\n    expr: (node_memory_MemTotal - node_memory_MemAvailable)/node_memory_MemTotal > 0.85\n    for: 5m\n    labels:\n      severity: warning\n    annotations:\n      summary: \"{{ $labels.instance }} MEM usage above 85% (current value: {{ $value }}%)\"\n  - alert: OutOfInodes\n    expr: node_filesystem_free{fstype=\"overlay\",mountpoint =\"/\"} / node_filesystem_size{fstype=\"overlay\",mountpoint =\"/\"} * 100 < 10\n    for: 5m\n    labels:\n      severity: warning\n    annotations:\n      summary: \"Out of inodes (instance {{ $labels.instance }})\"\n      description: \"Disk is almost running out of available inodes (< 10% left) (current value: {{ $value }})\"\n  - alert: OutOfDiskSpace\n    expr: node_filesystem_free{fstype=\"overlay\",mountpoint =\"/rootfs\"} / node_filesystem_size{fstype=\"overlay\",mountpoint =\"/rootfs\"} * 100 < 10\n    for: 5m\n    labels:\n      severity: warning\n    annotations:\n      summary: \"Out of disk space (instance {{ $labels.instance }})\"\n      description: \"Disk is almost full (< 10% left) (current value: {{ $value }})\"\n  - alert: UnusualNetworkThroughputIn\n    expr: sum by (instance) (irate(node_network_receive_bytes[2m])) / 1024 / 1024 > 100\n    for: 5m\n    labels:\n      severity: warning\n    annotations:\n      summary: \"Unusual network throughput in (instance {{ $labels.instance }})\"\n      description: \"Host network interfaces are probably receiving too much data (> 100 MB/s) (current value: {{ $value }})\"\n  - alert: UnusualNetworkThroughputOut\n    expr: sum by (instance) (irate(node_network_transmit_bytes[2m])) / 1024 / 1024 > 100\n    for: 5m\n    labels:\n      severity: warning\n    annotations:\n      summary: \"Unusual network throughput out (instance {{ $labels.instance }})\"\n      description: \"Host network interfaces are probably sending too much data (> 100 MB/s) (current value: {{ $value }})\"\n  - alert: UnusualDiskReadRate\n    expr: sum by (instance) (irate(node_disk_bytes_read[2m])) / 1024 / 1024 > 50\n    for: 5m\n    labels:\n      severity: warning\n    annotations:\n      summary: \"Unusual disk read rate (instance {{ $labels.instance }})\"\n      description: \"Disk is probably reading too much data (> 50 MB/s) (current value: {{ $value }})\"\n  - alert: UnusualDiskWriteRate\n    expr: sum by (instance) (irate(node_disk_bytes_written[2m])) / 1024 / 1024 > 50\n    for: 5m\n    labels:\n      severity: warning\n    annotations:\n      summary: \"Unusual disk write rate (instance {{ $labels.instance }})\"\n      description: \"Disk is probably writing too much data (> 50 MB/s) (current value: {{ $value }})\"\n  - alert: UnusualDiskReadLatency\n    expr: rate(node_disk_read_time_ms[1m]) / rate(node_disk_reads_completed[1m]) > 100\n    for: 5m\n    labels:\n      severity: warning\n    annotations:\n      summary: \"Unusual disk read latency (instance {{ $labels.instance }})\"\n      description: \"Disk latency is growing (read operations > 100ms) (current value: {{ $value }})\"\n  - alert: UnusualDiskWriteLatency\n    expr: rate(node_disk_write_time_ms[1m]) / rate(node_disk_writes_completedl[1m]) > 100\n    for: 5m\n    labels:\n      severity: warning\n    annotations:\n      summary: \"Unusual disk write latency (instance {{ $labels.instance }})\"\n      description: \"Disk latency is growing (write operations > 100ms) (current value: {{ $value }})\"\n- name: http_status\n  rules:\n  - alert: ProbeFailed\n    expr: probe_success == 0\n    for: 1m\n    labels:\n      severity: error\n    annotations:\n      summary: \"Probe failed (instance {{ $labels.instance }})\"\n      description: \"Probe failed (current value: {{ $value }})\"\n  - alert: StatusCode\n    expr: probe_http_status_code <= 199 OR probe_http_status_code >= 400\n    for: 1m\n    labels:\n      severity: error\n    annotations:\n      summary: \"Status Code (instance {{ $labels.instance }})\"\n      description: \"HTTP status code is not 200-399 (current value: {{ $value }})\"\n  - alert: SslCertificateWillExpireSoon\n    expr: probe_ssl_earliest_cert_expiry - time() < 86400 * 30\n    for: 5m\n    labels:\n      severity: warning\n    annotations:\n      summary: \"SSL certificate will expire soon (instance {{ $labels.instance }})\"\n      description: \"SSL certificate expires in 30 days (current value: {{ $value }})\"\n  - alert: SslCertificateHasExpired\n    expr: probe_ssl_earliest_cert_expiry - time()  <= 0\n    for: 5m\n    labels:\n      severity: error\n    annotations:\n      summary: \"SSL certificate has expired (instance {{ $labels.instance }})\"\n      description: \"SSL certificate has expired already (current value: {{ $value }})\"\n  - alert: BlackboxSlowPing\n    expr: probe_icmp_duration_seconds > 2\n    for: 5m\n    labels:\n      severity: warning\n    annotations:\n      summary: \"Blackbox slow ping (instance {{ $labels.instance }})\"\n      description: \"Blackbox ping took more than 2s (current value: {{ $value }})\"\n  - alert: BlackboxSlowRequests\n    expr: probe_http_duration_seconds > 2 \n    for: 5m\n    labels:\n      severity: warning\n    annotations:\n      summary: \"Blackbox slow requests (instance {{ $labels.instance }})\"\n      description: \"Blackbox request took more than 2s (current value: {{ $value }})\"\n  - alert: PodCpuUsagePercent\n    expr: sum(sum(label_replace(irate(container_cpu_usage_seconds_total[1m]),\"pod\",\"$1\",\"container_label_io_kubernetes_pod_name\", \"(.*)\"))by(pod) / on(pod) group_right kube_pod_container_resource_limits_cpu_cores *100 )by(container,namespace,node,pod,severity) > 80\n    for: 5m\n    labels:\n      severity: warning\n    annotations:\n      summary: \"Pod cpu usage percent has exceeded 80% (current value: {{ $value }}%)\"\n\n# 在最后面添加如下内容\n~]# vi /data/nfs-volume/prometheus/etc/prometheus.yml\nalerting:\n  alertmanagers:\n    - static_configs:\n        - targets: [\"alertmanager\"]\nrule_files:\n - \"/data/etc/rules.yml\"\n~~~\n\n> ![1583545590235](assets/1583545590235.png)\n>\n> **rules.yml文件**：这个文件就是报警规则\n>\n> 这时候可以重启Prometheus的pod，但生产商因为Prometheus太庞大，删掉容易拖垮集群，所以我们用另外一种方法，平滑加载（Prometheus支持）：\n\n~~~\n# 21机器，因为我们起的Prometheus是在21机器，平滑加载:\n~]# ps aux|grep prometheus\n~]# kill -SIGHUP 1488\n~~~\n\n![1583545718137](assets/1583545718137.png)\n\n![1583545762475](assets/1583545762475.png)\n\n> 这时候报警规则就都有了\n>\n\n\n\n### 测试alertmanager报警功能\n\n先把对应的两个邮箱的stmp都打开\n\n![1583721318338](assets/1583721318338.png)\n\n![1583721408460](assets/1583721408460.png)\n\n我们测试一下，把dubbo-service停了，这样consumer就会报错\n\n把service的scale改成0\n\n![1583545840349](assets/1583545840349.png)\n\n[blackbox.od.com](blackbox.od.com)查看，已经failure了\n\n![1583545937643](assets/1583545937643.png)\n\n[prometheus.od.com.alerts](prometheus.od.com.alerts)查看，两个变红了（一开始是变黄）\n\n![1583548102650](assets/1583548102650.png)\n\n![1583545983131](assets/1583545983131.png)\n\n这时候可以在163邮箱看到已发送的报警\n\n![1583721856162](assets/1583721856162.png)\n\nQQ邮箱收到报警\n\n![1583721899076](assets/1583721899076.png)\n\n完成（service的scale记得改回1）\n\n> 关于rules.yml：报警不能错报也不能漏报，在实际应用中，我们需要不断的修改rules的规则，以来贴近我们公司的实际需求。\n\n\n\n#### 资源不足时，可关闭部分非必要资源\n\n~~~\n# 22机器，也可以用dashboard操作：\n~]# kubectl scale deployment grafana --replicas=0 -n infra\n# out : deployment.extensions/grafana scaled\n~]# kubectl scale deployment alertmanager --replicas=0 -n infra\n# out : deployment.extensions/alertmanager scaled\n~]# kubectl scale deployment prometheus --replicas=0 -n infra\n# out : deployment.extensions/prometheus scaled\n~~~\n\n\n\n### 通过K8S部署dubbo微服务接入ELK架构\n\n> **WHAT**：ELK是三个开源软件的缩写，分别是：\n>\n> - E——ElasticSearch：分布式搜索引擎，提供搜集、分析、存储数据三大功能。\n> - L——LogStash：对日志的搜集、分析、过滤日志的工具，支持大量的数据获取方式。\n> - K——Kibana：为 Logstash 和 ElasticSearch 提供的日志分析友好的 Web 界面，可以帮助汇总、分析和搜索重要数据日志。\n> - 还有新增的FileBeat（流式日志收集器）：轻量级的日志收集处理工具，占用资源少，适合于在各个服务器上搜集日志后传输给Logstash，官方也推荐此工具，用来替代部分原本Logstash的工作。[收集日志的多种方式及原理](https://github.com/ben1234560/k8s_PaaS/blob/master/%E5%8E%9F%E7%90%86%E5%8F%8A%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/Kubernetes%E7%9B%B8%E5%85%B3%E7%94%9F%E6%80%81.md#%E6%97%A5%E5%BF%97%E6%94%B6%E9%9B%86%E4%B8%8E%E7%AE%A1%E7%90%86)\n>\n> **WHY**： 随着容器编排的进行，业务容器在不断的被创建、摧毁、迁移、扩容缩容等，面对如此海量的数据，又分布在各个不同的地方，我们不可能用传统的方法登录到每台机器看，所以我们需要建立一套集中的方法。我们需要这样一套日志手机、分析的系统：\n>\n> - 收集——采集多种来源的日志数据（流式日志收集器）\n> - 传输——稳定的把日志数据传输到中央系统（消息队列）\n> - 存储——将日志以结构化数据的形式存储起来（搜索引擎）\n> - 分析——支持方便的分析、检索等，有GUI管理系统（前端）\n> - 警告——提供错误报告，监控机制（监控工具）\n>\n> #### 这就是ELK\n\n#### ELK Stack概述\n\n![1581729929995](assets/1581729929995.png)\n\n> **c1/c2**：container（容器）的缩写\n>\n> **filebeat**：收集业务容器的日志，把c和filebeat放在一个pod里让他们一起跑，这样耦合就紧了\n>\n> **kafka**：高吞吐量的[分布式](https://baike.baidu.com/item/%E5%88%86%E5%B8%83%E5%BC%8F/19276232)发布订阅消息系统，它可以处理消费者在网站中的所有动作流数据。filebeat收集数据以Topic形式发布到kafka。\n>\n> **Topic**：Kafka数据写入操作的基本单元\n>\n> **logstash**：取kafka里的topic，然后再往ElasticSearch上传（异步过程，即又取又传）\n>\n> **index-pattern**：把数据按环境分（按prod和test分），并传到kibana\n>\n> **kibana**：展示数据\n\n\n\n### 制作tomcat容器的底包镜像\n\n> 尝试用tomcat的方式，因为很多公司老项目都是用tomcat跑起来，之前我们用的是springboot\n\n[tomcat官网](tomcat.apache.org)\n\n![1583558092305](assets/1583558092305.png)\n\n~~~\n# 200 机器：\ncd /opt/src/\n# 你也可以直接用我上传的，因为版本一直在变，之前的版本你是下载不下来的，如何查看新版本如上图\nsrc]# wget https://archive.apache.org/dist/tomcat/tomcat-8/v8.5.51/bin/apache-tomcat-8.5.51.tar.gz\nsrc]# mkdir /data/dockerfile/tomcat\nsrc]# tar xfv  apache-tomcat-8.5.51.tar.gz -C /data/dockerfile/tomcat\nsrc]# cd /data/dockerfile/tomcat\n# 配置tomcat-关闭AJP端口\ntomcat]# vi apache-tomcat-8.5.51/conf/server.xml\n# 找到AJP，注释掉相应的一行，结果如下图，8.5.51是已经自动注释掉的\n~~~\n\n![1583558364369](assets/1583558364369.png)\n\n~~~\n# 200机器，删掉不需要的日志：\ntomcat]# vi apache-tomcat-8.5.51/conf/logging.properties\n# 删掉3manager，4host-manager的handlers，并注释掉相关的，结果如下图\n# 日志级别改成INFO\n~~~\n\n![1583558487445](assets/1583558487445.png)\n\n![1583558525033](assets/1583558525033.png)\n\n![1583558607700](assets/1583558607700.png)\n\n~~~\n# 200机器，准备Dockerfile：\ntomcat]# vi Dockerfile\nFrom harbor.od.com/public/jre:8u112\nRUN /bin/cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime &&\\ \n    echo 'Asia/Shanghai' >/etc/timezone\nENV CATALINA_HOME /opt/tomcat\nENV LANG zh_CN.UTF-8\nADD apache-tomcat-8.5.51/ /opt/tomcat\nADD config.yml /opt/prom/config.yml\nADD jmx_javaagent-0.3.1.jar /opt/prom/jmx_javaagent-0.3.1.jar\nWORKDIR /opt/tomcat\nADD entrypoint.sh /entrypoint.sh\nCMD [\"/entrypoint.sh\"]\n\ntomcat]# vi config.yml\n---\nrules:\n  - pattern: '-*'\n\ntomcat]# wget https://repo1.maven.org/maven2/io/prometheus/jmx/jmx_prometheus_javaagent/0.3.1/jmx_prometheus_javaagent-0.3.1.jar -O jmx_javaagent-0.3.1.jar\ntomcat]# vi entrypoint.sh\n#!/bin/bash\nM_OPTS=\"-Duser.timezone=Asia/Shanghai -javaagent:/opt/prom/jmx_javaagent-0.3.1.jar=$(hostname -i):${M_PORT:-\"12346\"}:/opt/prom/config.yml\"\nC_OPTS=${C_OPTS}\nMIN_HEAP=${MIN_HEAP:-\"128m\"}\nMAX_HEAP=${MAX_HEAP:-\"128m\"}\nJAVA_OPTS=${JAVA_OPTS:-\"-Xmn384m -Xss256k -Duser.timezone=GMT+08  -XX:+DisableExplicitGC -XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:+CMSParallelRemarkEnabled -XX:+UseCMSCompactAtFullCollection -XX:CMSFullGCsBeforeCompaction=0 -XX:+CMSClassUnloadingEnabled -XX:LargePageSizeInBytes=128m -XX:+UseFastAccessorMethods -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFraction=80 -XX:SoftRefLRUPolicyMSPerMB=0 -XX:+PrintClassHistogram  -Dfile.encoding=UTF8 -Dsun.jnu.encoding=UTF8\"}\nCATALINA_OPTS=\"${CATALINA_OPTS}\"\nJAVA_OPTS=\"${M_OPTS} ${C_OPTS} -Xms${MIN_HEAP} -Xmx${MAX_HEAP} ${JAVA_OPTS}\"\nsed -i -e \"1a\\JAVA_OPTS=\\\"$JAVA_OPTS\\\"\" -e \"1a\\CATALINA_OPTS=\\\"$CATALINA_OPTS\\\"\" /opt/tomcat/bin/catalina.sh\n\ncd /opt/tomcat && /opt/tomcat/bin/catalina.sh run 2>&1 >> /opt/tomcat/logs/stdout.log\n\ntomcat]# chmod u+x entrypoint.sh\ntomcat]# ll\ntomcat]# docker build . -t harbor.od.com/base/tomcat:v8.5.51\ntomcat]# docker push harbor.od.com/base/tomcat:v8.5.51\n~~~\n\n> **Dockerfile文件解析**：\n>\n> - FROM：镜像地址\n> - RUN：修改时区\n> - ENV：设置环境变量，把tomcat软件放到opt下\n> - ENV：设置环境变量，字符集用zh_CN.UTF-8\n> - ADD：把apache-tomcat-8.5.50包放到/opt/tomcat下\n> - ADD：让prome基于文件的自动发现服务，这个可以不要，因为没在用prome\n> - ADD：把jmx_javaagent-0.3.1.jar包放到/opt/...下，用来专门收集jvm的export，能提供一个http的接口\n> - WORKDIR：工作目录\n> - ADD：移动文件\n> - CMD：运行文件\n\n![1583559245639](assets/1583559245639.png)\n\n完成\n\n\n\n### 交付tomcat形式的dubbo服务消费者到K8S集群\n\n改造下dubbo-demo-web项目\n\n由于是tomcat，我们需要多建一条Jenkins流水线\n\n![1583559296196](assets/1583559296196.png)\n\n![1583559325853](assets/1583559325853.png)\n\n![1583559348560](assets/1583559348560.png)\n\n1\n\n![1583559819045](assets/1583559819045.png)\n\n2\n\n![1583559830860](assets/1583559830860.png)\n\n3\n\n![1583559838755](assets/1583559838755.png)\n\n4\n\n![1583559908506](assets/1583559908506.png)\n\n5\n\n![1583559948318](assets/1583559948318.png)\n\n6\n\n![1583559958558](assets/1583559958558.png)\n\n7\n\n![1583559972282](assets/1583559972282.png)\n\n8\n\n![1583559985561](assets/1583559985561.png)\n\n9\n\n![1583560000469](assets/1583560000469.png)\n\n10\n\n![1583560009300](assets/1583560009300.png)\n\n11\n\n![1583560038370](assets/1583560038370.png)\n\n~~~shell\n# 将如下内容填入pipeline：\npipeline {\n  agent any \n    stages {\n    stage('pull') { //get project code from repo \n      steps {\n        sh \"git clone ${params.git_repo} ${params.app_name}/${env.BUILD_NUMBER} && cd ${params.app_name}/${env.BUILD_NUMBER} && git checkout ${params.git_ver}\"\n        }\n    }\n    stage('build') { //exec mvn cmd\n      steps {\n        sh \"cd ${params.app_name}/${env.BUILD_NUMBER}  && /var/jenkins_home/maven-${params.maven}/bin/${params.mvn_cmd}\"\n      }\n    }\n    stage('unzip') { //unzip  target/*.war -c target/project_dir\n      steps {\n        sh \"cd ${params.app_name}/${env.BUILD_NUMBER} && cd ${params.target_dir} && mkdir project_dir && unzip *.war -d ./project_dir\"\n      }\n    }\n    stage('image') { //build image and push to registry\n      steps {\n        writeFile file: \"${params.app_name}/${env.BUILD_NUMBER}/Dockerfile\", text: \"\"\"FROM harbor.od.com/${params.base_image}\nADD ${params.target_dir}/project_dir /opt/tomcat/webapps/${params.root_url}\"\"\"\n        sh \"cd  ${params.app_name}/${env.BUILD_NUMBER} && docker build -t harbor.od.com/${params.image_name}:${params.git_ver}_${params.add_tag} . && docker push harbor.od.com/${params.image_name}:${params.git_ver}_${params.add_tag}\"\n      }\n    }\n  }\n}\n~~~\n\n![1583560082441](assets/1583560082441.png)\n\nsave\n\n点击构建\n\n![1583560122560](assets/1583560122560.png)\n\n~~~\n# 填入指定参数，我的gittee是有tomcat的版本的，下面我依旧用的是gitlab\napp_name:       dubbo-demo-web\nimage_name:     app/dubbo-demo-web\ngit_repo:       http://gitlab.od.com:10000/909336740/dubbo-demo-web.git\ngit_ver:        tomcat\nadd_tag:        20200214_1300\nmvn_dir:        ./\ntarget_dir:     ./dubbo-client/target\nmvn_cmd:        mvn clean package -Dmaven.test.skip=true\nbase_image:     base/tomcat:v8.5.51\nmaven:          3.6.1-8u232\nroot_url:       ROOT\n# 点击Build进行构建，等待构建完成\n~~~\n\n![1583561544404](assets/1583561544404.png)\n\nbuild成功后\n\n\n\n修改版本信息，删掉20880，如下图，然后update\n\n![1583630633310](assets/1583630633310.png)\n\n![1583630769979](assets/1583630769979.png)\n\n浏览器输入[demo-test.od.com/hello?name=tomcat](demo-test.od.com/hello?name=tomcat)\n\n![1583630984480](assets/1583630984480.png)\n\n完成\n\n查看dashboard里的pod里\n\n![1583631035074](assets/1583631035074.png)\n\n![1583631074816](assets/1583631074816.png)\n\n> 这些就是我们要收集的日志，收到ELK\n\n\n\n### 二进制安装部署elasticsearch\n\n> 我们这里只部一个es的节点，因为我们主要是了解数据流的方式\n>\n\n[官网下载包](https://www.elastic.co/downloads/past-releases/elasticsearch-6-8-6)右键复制链接\n\n![1581667412370](assets/1581667412370.png)\n\n~~~\n# 12机器：\n~]# cd /opt/src/\nsrc]# wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.8.6.tar.gz\nsrc]# tar xfv elasticsearch-6.8.6.tar.gz -C /opt\nsrc]# ln -s /opt/elasticsearch-6.8.6/ /opt/elasticsearch\nsrc]# cd /opt/elasticsearch\n# 配置\nelasticsearch]# mkdir -p /data/elasticsearch/{data,logs}\n# 修改以下内容\nelasticsearch]# vi config/elasticsearch.yml\ncluster.name: es.od.com\nnode.name: hdss7-12.host.com\npath.data: /data/elasticsearch/data\npath.logs: /data/elasticsearch/logs\nbootstrap.memory_lock: true\nnetwork.host: 10.4.7.12\nhttp.port: 9200\n\n# 修改以下内容\nelasticsearch]# vi config/jvm.options\n-Xms512m\n-Xmx512m\n\n# 创建普通用户\nelasticsearch]# useradd -s /bin/bash -M es\nelasticsearch]# chown -R es.es /opt/elasticsearch-6.8.6/\nelasticsearch]# chown -R es.es /data/elasticsearch/\n# 文件描述符\nelasticsearch]# vi /etc/security/limits.d/es.conf\nes hard nofile 65536\nes soft fsize unlimited\nes hard memlock unlimited\nes soft memlock unlimited\n\n# 调整内核参数\nelasticsearch]# sysctl -w vm.max_map_count=262144\nelasticsearch]# echo \"vm.max_map_count=262144\" >> /etc/sysctl.conf\nelasticsearch]# sysctl -p\n# 启动\nelasticsearch]# su -c \"/opt/elasticsearch/bin/elasticsearch -d\" es\nelasticsearch]# netstat -luntp|grep 9200\n# 调整ES日志模板\nelasticsearch]# curl -H \"Content-Type:application/json\" -XPUT http://10.4.7.12:9200/_template/k8s -d '{\n  \"template\" : \"k8s*\",\n  \"index_patterns\": [\"k8s*\"],  \n  \"settings\": {\n    \"number_of_shards\": 5,\n    \"number_of_replicas\": 0\n  }\n}'\n~~~\n\n![1583631650876](assets/1583631650876.png)\n\n> 完成，你看我敲这么多遍就知道要等\n\n\n\n### 安装部署kafka和kafka-manager\n\n[官网](https://kafka.apache.org)\n\n> 做kafka的时候不建议用超过2.2.0的版本\n>\n\n~~~\n# 11机器：\ncd /opt/src/\nsrc]# wget https://archive.apache.org/dist/kafka/2.2.0/kafka_2.12-2.2.0.tgz\nsrc]# tar xfv kafka_2.12-2.2.0.tgz -C /opt/\nsrc]# ln -s /opt/kafka_2.12-2.2.0/ /opt/kafka\nsrc]# cd /opt/kafka\nkafka]# ll\n~~~\n\n![1583632474778](assets/1583632474778.png)\n\n~~~\n# 11机器，配置：\nkafka]# mkdir -pv /data/kafka/logs\n# 修改以下配置，其中zk是不变的，最下面两行则新增到尾部\n# listeners这个配置建议写成IP:9092,有些老版本的kafa不写默认是localhost,会导致filebeat识别kafka地址错误\nkafka]# vi config/server.properties\nlisteners=PLAINTEXT://10.4.7.11:9092\nlog.dirs=/data/kafka/logs\nzookeeper.connect=localhost:2181\nlog.flush.interval.messages=10000\nlog.flush.interval.ms=1000\ndelete.topic.enable=true\nhost.name=hdss7-11.host.com\n~~~\n\n![1583632595085](assets/1583632595085.png)\n\n~~~\n# 11机器，启动：\nkafka]# bin/kafka-server-start.sh -daemon config/server.properties\nkafka]# ps aux|grep kafka\nkafka]# netstat -luntp|grep 80711\n~~~\n\n![1583632747263](assets/1583632747263.png)\n\n![1583632764359](assets/1583632764359.png)\n\n##### 部署kafka-manager\n\n~~~\n# 200机器，制作docker：\n~]# mkdir /data/dockerfile/kafka-manager\n~]# cd /data/dockerfile/kafka-manager\nkafka-manager]# vi Dockerfile \nFROM hseeberger/scala-sbt\n\nENV ZK_HOSTS=10.4.7.11:2181 \\\n     KM_VERSION=2.0.0.2\n\nRUN mkdir -p /tmp && \\\n    cd /tmp && \\\n    wget https://github.com/yahoo/kafka-manager/archive/${KM_VERSION}.tar.gz && \\\n    tar xxf ${KM_VERSION}.tar.gz && \\\n    cd /tmp/kafka-manager-${KM_VERSION} && \\\n    sbt clean dist && \\\n    unzip  -d / ./target/universal/kafka-manager-${KM_VERSION}.zip && \\\n    rm -fr /tmp/${KM_VERSION} /tmp/kafka-manager-${KM_VERSION}\n\nWORKDIR /kafka-manager-${KM_VERSION}\n\nEXPOSE 9000\nENTRYPOINT [\"./bin/kafka-manager\",\"-Dconfig.file=conf/application.conf\"]\n\n# 因为大，build过程比较慢，也比较容易失败，20分钟左右，\nkafka-manager]# docker build . -t harbor.od.com/infra/kafka-manager:v2.0.0.2\n# build一直失败就用我做好的，不跟你的机器也得是10.4.7.11等，因为dockerfile里面已经写死了\n# kafka-manager]# docker pull 909336740/kafka-manager:v2.0.0.2\n# kafka-manager]# docker tag 29badab5ea08 harbor.od.com/infra/kafka-manager:v2.0.0.2\nkafka-manager]# docker images|grep kafka\nkafka-manager]# docker push harbor.od.com/infra/kafka-manager:v2.0.0.2\n~~~\n\n![1583635072663](assets/1583635072663.png)\n\n~~~\n# 200机器，配置资源清单：\nmkdir /data/k8s-yaml/kafka-manager\ncd /data/k8s-yaml/kafka-manager\nkafka-manager]# vi dp.yaml\nkind: Deployment\napiVersion: extensions/v1beta1\nmetadata:\n  name: kafka-manager\n  namespace: infra\n  labels: \n    name: kafka-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels: \n      app: kafka-manager\n  strategy:\n    type: RollingUpdate\n    rollingUpdate: \n      maxUnavailable: 1\n      maxSurge: 1\n  revisionHistoryLimit: 7\n  progressDeadlineSeconds: 600\n  template:\n    metadata:\n      labels: \n        app: kafka-manager\n    spec:\n      containers:\n      - name: kafka-manager\n        image: harbor.od.com/infra/kafka-manager:v2.0.0.2\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9000\n          protocol: TCP\n        env:\n        - name: ZK_HOSTS\n          value: zk1.od.com:2181\n        - name: APPLICATION_SECRET\n          value: letmein\n      imagePullSecrets:\n      - name: harbor\n      terminationGracePeriodSeconds: 30\n      securityContext: \n        runAsUser: 0\n\nkafka-manager]# vi svc.yaml\nkind: Service\napiVersion: v1\nmetadata: \n  name: kafka-manager\n  namespace: infra\nspec:\n  ports:\n  - protocol: TCP\n    port: 9000\n    targetPort: 9000\n  selector: \n    app: kafka-manager\n\nkafka-manager]# vi ingress.yaml\nkind: Ingress\napiVersion: extensions/v1beta1\nmetadata: \n  name: kafka-manager\n  namespace: infra\nspec:\n  rules:\n  - host: km.od.com\n    http:\n      paths:\n      - path: /\n        backend: \n          serviceName: kafka-manager\n          servicePort: 9000\n~~~\n\n![1583635130773](assets/1583635130773.png)\n\n~~~\n# 11机器，解析域名：\n~]# vi /var/named/od.com.zone\nserial 前滚一位\n\nkm                 A    10.4.7.10\n~]# systemctl restart named\n~]# dig -t A km.od.com @10.4.7.11 +short\n# out:10.4.7.10\n~~~\n\n![1583635171353](assets/1583635171353.png)\n\n~~~\n# 22机器，应用资源：\n~]# kubectl apply -f http://k8s-yaml.od.com/kafka-manager/dp.yaml\n~]# kubectl apply -f http://k8s-yaml.od.com/kafka-manager/svc.yaml\n~]# kubectl apply -f http://k8s-yaml.od.com/kafka-manager/ingress.yaml\n~~~\n\n![1583635645699](assets/1583635645699.png)\n\n文件大可能起不来，需要多拉几次（当然你的资源配置高应该是没问题的）\n\n![1581673449530](assets/1581673449530.png)\n\n启动成功后浏览器输入[km.od.com](km.od.com)\n\n![1583635710991](assets/1583635710991.png)\n\n![1583635777584](assets/1583635777584.png)\n\n填完上面三个值后就可以下拉save了\n\n点击\n\n![1583635809826](assets/1583635809826.png)\n\n![1583635828160](assets/1583635828160.png)\n\n![1583635897650](assets/1583635897650.png)\n\n完成\n\n\n\n### 制作filebeat底包并接入dubbo服务消费者\n\n[Filebeat官网](https://www.elastic.co/downloads/beats/filebeat)\n\n下载指纹\n\n![1583636131189](assets/1583636131189.png)\n\n打开后复制，后面的不需要复制\n\n![1583636252938](assets/1583636252938.png)\n\n开始前，请确保你的这些服务都是起来的\n\n![1583636282215](assets/1583636282215.png)\n\n~~~\n# 200机器，准备镜像，资源配置清单：\nmkdir /data/dockerfile/filebeat\n~]# cd /data/dockerfile/filebeat\n# 刚刚复制的指纹替代到下面的FILEBEAT_SHA1来，你用的是什么版本FILEBEAT_VERSION就用什么版本，更新的很快，我之前用的是5.1现在已经是6.1了\nfilebeat]# vi Dockerfile\nFROM debian:jessie\n\nENV FILEBEAT_VERSION=7.6.1 \\\n    FILEBEAT_SHA1=887edb2ab255084ef96dbc4c7c047bfa92dad16f263e23c0fcc80120ea5aca90a3a7a44d4783ba37b135dac76618971272a591ab4a24997d8ad40c7bc23ffabf\n\nRUN set -x && \\\n  apt-get update && \\\n  apt-get install -y wget && \\\n  wget https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-${FILEBEAT_VERSION}-linux-x86_64.tar.gz -O /opt/filebeat.tar.gz && \\\n  cd /opt && \\\n  echo \"${FILEBEAT_SHA1}  filebeat.tar.gz\" | sha512sum -c - && \\\n  tar xzvf filebeat.tar.gz && \\\n  cd filebeat-* && \\\n  cp filebeat /bin && \\\n  cd /opt && \\\n  rm -rf filebeat* && \\\n  apt-get purge -y wget && \\\n  apt-get autoremove -y && \\\n  apt-get clean && rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/*\n\nCOPY docker-entrypoint.sh /\nENTRYPOINT [\"/docker-entrypoint.sh\"]\n\nfilebeat]# vi docker-entrypoint.sh\n#!/bin/bash\n\nENV=${ENV:-\"test\"}\nPROJ_NAME=${PROJ_NAME:-\"no-define\"}\nMULTILINE=${MULTILINE:-\"^\\d{2}\"}\n\ncat > /etc/filebeat.yaml << EOF\nfilebeat.inputs:\n- type: log\n  fields_under_root: true\n  fields:\n    topic: logm-${PROJ_NAME}\n  paths:\n    - /logm/*.log\n    - /logm/*/*.log\n    - /logm/*/*/*.log\n    - /logm/*/*/*/*.log\n    - /logm/*/*/*/*/*.log\n  scan_frequency: 120s\n  max_bytes: 10485760\n  multiline.pattern: '$MULTILINE'\n  multiline.negate: true\n  multiline.match: after\n  multiline.max_lines: 100\n- type: log\n  fields_under_root: true\n  fields:\n    topic: logu-${PROJ_NAME}\n  paths:\n    - /logu/*.log\n    - /logu/*/*.log\n    - /logu/*/*/*.log\n    - /logu/*/*/*/*.log\n    - /logu/*/*/*/*/*.log\n    - /logu/*/*/*/*/*/*.log\noutput.kafka:\n  hosts: [\"10.4.7.11:9092\"]\n  topic: k8s-fb-$ENV-%{[topic]}\n  version: 2.0.0\n  required_acks: 0\n  max_message_bytes: 10485760\nEOF\n\nset -xe\n\n# If user don't provide any command\n# Run filebeat\nif [[ \"$1\" == \"\" ]]; then\n     exec filebeat  -c /etc/filebeat.yaml \nelse\n    # Else allow the user to run arbitrarily commands like bash\n    exec \"$@\"\nfi\n\nfilebeat]# chmod u+x docker-entrypoint.sh\nfilebeat]# docker build . -t harbor.od.com/infra/filebeat:v7.6.1\n# build可能会失败很多次，我最长的是7次，下面有相关报错\nfilebeat]# docker images|grep filebeat\nfilebeat]# docker push harbor.od.com/infra/filebeat:v7.6.1\n# 删掉原来的内容全部用新的，使用的两个镜像对应上你自己的镜像\nfilebeat]# vi /data/k8s-yaml/test/dubbo-demo-consumer/dp.yaml\nkind: Deployment\napiVersion: extensions/v1beta1\nmetadata:\n  name: dubbo-demo-consumer\n  namespace: test\n  labels: \n    name: dubbo-demo-consumer\nspec:\n  replicas: 1\n  selector:\n    matchLabels: \n      name: dubbo-demo-consumer\n  template:\n    metadata:\n      labels: \n        app: dubbo-demo-consumer\n        name: dubbo-demo-consumer\n    spec:\n      containers:\n      - name: dubbo-demo-consumer\n        image: harbor.od.com/app/dubbo-demo-web:tomcat_200307_1410\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n        env:\n        - name: C_OPTS\n          value: -Denv=fat -Dapollo.meta=http://apollo-configservice:8080\n        volumeMounts:\n        - mountPath: /opt/tomcat/logs\n          name: logm\n      - name: filebeat\n        image: harbor.od.com/infra/filebeat:v7.6.1\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: ENV\n          value: test\n        - name: PROJ_NAME\n          value: dubbo-demo-web\n        volumeMounts:\n        - mountPath: /logm\n          name: logm\n      volumes:\n      - emptyDir: {}\n        name: logm\n      imagePullSecrets:\n      - name: harbor\n      restartPolicy: Always\n      terminationGracePeriodSeconds: 30\n      securityContext: \n        runAsUser: 0\n      schedulerName: default-scheduler\n  strategy:\n    type: RollingUpdate\n    rollingUpdate: \n      maxUnavailable: 1\n      maxSurge: 1\n  revisionHistoryLimit: 7\n  progressDeadlineSeconds: 600\n~~~\n\n> 相关报错（其它问题基本都是网络不稳定的问题）：![1583639164435](assets/1583639164435.png)\n>\n> 因为你用的指纹不是自己的，或者版本没写对。\n>\n> **dp.yaml文件解析**：    spec-containers下有两个name，对应的两个容器，这就是边车模式（sidecar）。\n\n~~~\n# 22机器，应用资源清单：\n~]# kubectl apply -f http://k8s-yaml.od.com/test/dubbo-demo-consumer/dp.yaml\n#out: deployment.extensions/dubbo-demo-consumer configured\n~]# kubectl get pods -n test\n~~~\n\n![1583640124995](assets/1583640124995.png)\n\n机器在21机器\n\n![1583640217181](assets/1583640217181.png)\n\n~~~\n# 查看filebeat日志，21机器：\n~]# docker ps -a|grep consumer\n~]# docker exec -ti a6adcd6e83b3 bash\n:/# cd /logm\n:/#/logm# ls\n:/#/logm# cd ..\n# 这个log，是你每一次刷新demo页面都会有数据，你把它夯在这里\n:/# tail -fn 200 /logm/stdout.log\n# 日志就都在这里了\n~~~\n\n![1583640393605](assets/1583640393605.png)\n\n~~~\n# 浏览器输入：demo-test.com/hello?name=tomcat\n~~~\n\n![1583640365512](assets/1583640365512.png)\n\n刷新上面的页面，去21机器看log\n\n![1583640316947](assets/1583640316947.png)\n\n[刷新km.od.com/clusters/kafka-od/topics](km.od.com/clusters/kafka-od/topics)\n\n![1583640444719](assets/1583640444719.png)\n\n完成\n\n\n\n### 部署logstash镜像\n\n~~~\n# 200机器，准备镜像、资源清单：\n# logstash的版本需要和es的版本一样，11机器cd /opt/目录下即可查看到\n~]# docker pull logstash:6.8.6\n~]# docker images|grep logstash\n~]# docker tag d0a2dac51fcb harbor.od.com/infra/logstash:v6.8.6\n~]# docker push harbor.od.com/infra/logstash:v6.8.6\n~]# mkdir /etc/logstash\n~]# vi /etc/logstash/logstash-test.conf\ninput {\n  kafka {\n    bootstrap_servers => \"10.4.7.11:9092\"\n    client_id => \"10.4.7.200\"\n    consumer_threads => 4\n    group_id => \"k8s_test\"\n    topics_pattern => \"k8s-fb-test-.*\"\n  }\n}\n\nfilter {\n  json {\n    source => \"message\"\n  }\n}\n\noutput {\n  elasticsearch {\n    hosts => [\"10.4.7.12:9200\"]\n    index => \"k8s-test-%{+YYYY.MM.DD}\"\n  }\n}\n\n~]# vi /etc/logstash/logstash-prod.conf\ninput {\n  kafka {\n    bootstrap_servers => \"10.4.7.11:9092\"\n    client_id => \"10.4.7.200\"\n    consumer_threads => 4\n    group_id => \"k8s_prod\"\n    topics_pattern => \"k8s-fb-prod-.*\"\n  }\n}\n\nfilter {\n  json {\n    source => \"message\"\n  }\n}\n\noutput {\n  elasticsearch {\n    hosts => [\"10.4.7.12:9200\"]\n    index => \"k8s-prod-%{+YYYY.MM.DD}\"\n  }\n}\n\n# 启动\n~]# docker run -d --name logstash-test -v /etc/logstash:/etc/logstash harbor.od.com/infra/logstash:v6.8.6 -f /etc/logstash/logstash-test.conf\n~]# docker ps -a|grep logstash\n~~~\n\n[^:%s/test/prod/g]: 在vi命令行输入左边内容，即可将文本里面得test全部换成prod\n\n![1583651857160](assets/1583651857160.png)\n\n我们刷新demo页面让kafka里面更新些日志\n\n![1583651874243](assets/1583651874243.png)\n\n有日志了\n\n![1583651931546](assets/1583651931546.png)\n\n~~~\n# 200机器，验证ES索引（可能比较慢）：\n~]# curl http://10.4.7.12:9200/_cat/indices?v\n~~~\n\n![1583652168302](assets/1583652168302.png)\n\n> 这个反应有点慢，我等了快三分钟\n\n完成\n\n\n\n### 交付kibana到K8S集群\n\n> 为什么用kibana：当然运维可以直接在dashboard里exec进去，然后命令行看情况，但是开发或者测试不行，那是机密的，我们得要一个页面供他们使用，使用需要kibana。\n\n~~~\n~]# docker pull kibana:6.8.6\n~]# docker images|grep kibana\n~]# docker tag adfab5632ef4 harbor.od.com/infra/kibana:v6.8.6\n~]# docker push harbor.od.com/infra/kibana:v6.8.6\n~]# mkdir /data/k8s-yaml/kibana\n~]# cd /data/k8s-yaml/kibana/\nkibana]# vi dp.yaml\nkind: Deployment\napiVersion: extensions/v1beta1\nmetadata:\n  name: kibana\n  namespace: infra\n  labels: \n    name: kibana\nspec:\n  replicas: 1\n  selector:\n    matchLabels: \n      name: kibana\n  template:\n    metadata:\n      labels: \n        app: kibana\n        name: kibana\n    spec:\n      containers:\n      - name: kibana\n        image: harbor.od.com/infra/kibana:v6.8.6\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 5601\n          protocol: TCP\n        env:\n        - name: ELASTICSEARCH_URL\n          value: http://10.4.7.12:9200\n      imagePullSecrets:\n      - name: harbor\n      securityContext: \n        runAsUser: 0\n  strategy:\n    type: RollingUpdate\n    rollingUpdate: \n      maxUnavailable: 1\n      maxSurge: 1\n  revisionHistoryLimit: 7\n  progressDeadlineSeconds: 600\n\nkibana]# vi svc.yaml\nkind: Service\napiVersion: v1\nmetadata: \n  name: kibana\n  namespace: infra\nspec:\n  ports:\n  - protocol: TCP\n    port: 5601\n    targetPort: 5601\n  selector: \n    app: kibana\n\nkibana]# vi ingress.yaml\nkind: Ingress\napiVersion: extensions/v1beta1\nmetadata: \n  name: kibana\n  namespace: infra\nspec:\n  rules:\n  - host: kibana.od.com\n    http:\n      paths:\n      - path: /\n        backend: \n          serviceName: kibana\n          servicePort: 5601\n~~~\n\n![1583652777163](assets/1583652777163.png)\n\n~~~\n# 11机器，解析域名：\n~]# vi /var/named/od.com.zone\nserial 前滚一位\nkibana             A    10.4.7.10\n\n~]# systemctl restart named\n~]# dig -t A kibana.od.com @10.4.7.11 +short\n~~~\n\n![1583652835822](assets/1583652835822.png)\n\n~~~\n# 22机器（21机器还夯着log），应用资源清单：\n~]# kubectl apply -f http://k8s-yaml.od.com/kibana/dp.yaml\n~]# kubectl apply -f http://k8s-yaml.od.com/kibana/svc.yaml\n~]# kubectl apply -f http://k8s-yaml.od.com/kibana/ingress.yaml\n~]# kubectl get pods -n infra\n~~~\n\n![1583653095099](assets/1583653095099.png)\n\n[kibana.od.com](kibana.od.com)\n\n> 我用的低配8C32G，机器快跑不动了，还会显示service not yet\n\n![1581735655839](assets/1581735655839.png)\n\n![1583653403755](assets/1583653403755.png)\n\n> 点完可能会转圈转很久\n\n![1583654044637](assets/1583654044637.png)\n\n![1583654055784](assets/1583654055784.png)\n\n去创建\n\n![1583653710404](assets/1583653710404.png)\n\n![1583653734639](assets/1583653734639.png)\n\n![1583653767919](assets/1583653767919.png)\n\n创建后，你就能看到日志\n\n![1583654148342](assets/1583654148342.png)\n\n把prod里的configservice和admin依次起来\n\n![1583654217058](assets/1583654217058.png)\n\n~~~\n# 200机器：\ncd /data/k8s-yaml/prod/dubbo-demo-consumer/\ndubbo-demo-consumer]# cp ../../test/dubbo-demo-consumer/dp.yaml .\n# y\n# 修改namespace为prod，fat改成pro，http地址也改了\ndubbo-demo-consumer]# vi dp.yaml\n~~~\n\n![1583654303335](assets/1583654303335.png)\n\n![1583654391353](assets/1583654391353.png)\n\n[config-prod.od.com](config-prod.od.com)\n\n![1583654579273](assets/1583654579273.png)\n\n完成\n\n\n\n### 详解Kibana生产实践方法\n\n查看环境情况\n\n确认Eureka有config和admin\n\n![1583654579273](assets/1583654579273.png)\n\n确认Apollo里有两个环境\n\n![1583654655665](assets/1583654655665.png)\n\n确认完后，我们先把service起来\n\n![1583654706752](assets/1583654706752.png)\n\n然后到consumer，consumer需要接日志\n\n~~~~\n# 22机器：\n~]# kubectl apply -f http://k8s-yaml.od.com/prod/dubbo-demo-consumer/dp.yaml\n~]# kubectl get pods -n prod\n~~~~\n\n![1583654764653](assets/1583654764653.png)\n\n~~~\n# 200机器：\n# 启动\n~]# docker run -d --name logstash-prod -v /etc/logstash:/etc/logstash harbor.od.com/infra/logstash:v6.8.6 -f /etc/logstash/logstash-prod.conf\n~]# docker ps -a|grep logstash\n# curl一下，这时候还只有test\n~]# curl http://10.4.7.12:9200/_cat/indices?v\n~~~\n\n![1583654862812](assets/1583654862812.png)\n\n~~~\n# 访问浏览器demo-prod.od.com/hello?name=prod\n~~~\n\n![1583654914493](assets/1583654914493.png)\n\n我们看一下调度到哪个节点了\n\n![1583654962325](assets/1583654962325.png)\n\n~~~\n# 调度到21节点，我们去21节点看一下：\n~]# docker ps -a|grep consumer\n~]# docker exec -ti 094e68c795b0 bash\n:/# cd /logm\n:/logm# ls\n:/logm# tail -fn 200 stdout.log\n~~~\n\n![1583655033176](assets/1583655033176.png)\n\n夯住\n\n![1583655055686](assets/1583655055686.png)\n\n去[http://km.od.com](http://km.od.com/)\n\n![1583655088618](assets/1583655088618.png)\n\n![1583655114357](assets/1583655114357.png)\n\n![1583655130583](assets/1583655130583.png)\n\n已经有prod了\n\n~~~\n# 200机器，curl的时候可能要等一下才有（可以去多刷一下网页产生日志）：\n~]# curl http://10.4.7.12:9200/_cat/indices?v\n~~~\n\n![1583655264326](assets/1583655264326.png)\n\n去kibana配一下\n\n![1583655369240](assets/1583655369240.png)\n\n![1583655383868](assets/1583655383868.png)\n\n##### 如何使用kibana\n\n时间选择\n\n![1583655477179](assets/1583655477179.png)\n\n![1583655509551](assets/1583655509551.png)\n\n> test没用数据的点下这个就有了，平常用的最多的也是today，后面突然没数据了你就可以刷新或者点时间，特别是配置差的同学\n>\n> ![1583655706547](assets/1583655706547.png)\n\n环境选择器\n\n![1583655530032](assets/1583655530032.png)\n\n关键字选择器\n\n先把message顶上来，还有log.file.path、hostname\n\n![1583655759010](assets/1583655759010.png)\n\n我们先制造一些错误，把service scale成0\n\n![1583655838132](assets/1583655838132.png)\n\n然后刷新一下页面，让它报错，记得是test环境\n\n![1583655858792](assets/1583655858792.png)\n\n搜exception关键字，并可展开\n\n![1583655981692](assets/1583655981692.png)\n\n![1583656009350](assets/1583656009350.png)\n\n现在consumer日志已经完成了，记得把service的pod还原，并删掉consumer的pod让它重启\n\n\n\n#### 课外作业（不是一定要完成，但是你做了我做的这些）\n\nconsumer日志已经完成，还可以做service日志\n\n~~~\n# 200机器：\n# 修改一下内容\n~]# cd  /data/dockerfile/jre8/\n# 修改以下内容\njre8]# vi entrypoint.sh\nexec java -jar ${M_OPTS} ${C_OPTS} ${JAR_BALL} 2>&1 >> /opt/logs/stdout.log\n\njre8]# docker build . -t harbor.od.com/base/jre8:8u112_with_logs\njre8]# docker push harbor.od.com/base/jre8:8u112_with_logs\n~~~\n\n![1584067987917](assets/1584067987917.png)\n\n![1581752521087](assets/1581752521087.png)\n\n去修改一下Jenkins加一个底包\n\n![1584237876688](assets/1584237876688.png)\n\n![1584237900572](assets/1584237900572.png)\n\n![1584237947516](assets/1584237947516.png)\n\n下面就要你接着做了\n\n"
        },
        {
          "name": "第三章——k8s集群.md",
          "type": "blob",
          "size": 25.9658203125,
          "content": "## 第三章——k8s集群\n\n> 我们来回顾一下并学习一些必要知识\n\n[k8s中文社区docs.kubernetes.org.cn](http://docs.kubernetes.org.cn/)\n\n##### K8S核心资源管理方法\n\n~~~\n# 任意机器(我是在21)\n# 查看名称空间\n~]# kubectl get namespace\n~]# kubectl get ns\n~~~\n\n![1579073760060](assets/1579073760060.png)\n\n~~~\n# 任意机器(我是在21)\n~]# kubectl get all [-n default]\n~]# kubectl create ns app\n# 增\n~]# kubectl create ns app\n# 删\n~]# kubectl delete namespace app\n# 查\n~]# kubectl get ns\n# 创建deployment资源\nkubectl create deployment nginx-dp --image=harbor.od.com/public/nginx:v1.7.9 -n kube-public\n# 查指定空间\n~]# kubectl get deploy -n kube-public\n~]# kubectl get pods -o wide -n kube-public\n~]# kubectl describe deployment nginx-dp -n kube-public\n\n# 进入pod资源\n21 ~]# kubectl get pods -n kube-public\n21 ~]# kubectl exec -ti nginx-dp-5dfc689474-9zt9r /bin/bash -n kube-public\n~~~\n\n> **kubectl get deploy**：这里的deploy是容器类型，deploy也是deployment\n>\n> **kubectl exec**：进入容器\n>\n> - -t：将标准输入控制台作为容器的控制台输入\n> - -i：将控制台输入发送到容器\n> - 一般是连起来用-it，后面带的是get出来的容器名\n> - /bin/bash：终端模式\n\n![1579075596641](assets/1579075596641.png)\n\n~~~\n# 删除pod资源（重启），pod控制器预期你有一个pod，所以你删掉就会重启，后面force是强制删除\n21 ~]# kubectl delete pod nginx-dp-5dfc689474-gtfvv -n kube-public [--force --grace-period=0]\n# 删掉deploy\n21 ~]# kubectl delete deploy nginx-dp -n kube-public\n# 查看\n21 ~]# kubectl get all -n kube-public\n~~~\n\n![1579076172922](assets/1579076172922.png)\n\n##### 管理service资源\n\n~~~\n# 21机器\n# 创建\n~]# kubectl create deployment nginx-dp --image=harbor.od.com/public/nginx:v1.7.9 -n kube-public\n~]# kubectl get all -n kube-public\n# 暴露端口\n~]# kubectl expose deployment nginx-dp --port=80 -n kube-public\n~]# kubectl get all -n kube-public -o wide\n~~~\n\n> **kubectl expose**：暴露端口，后面的--port=80 指的是暴露80端口\n\n![1579077073962](assets/1579077073962.png)\n\n~~~\n# 去22机器\n~]# curl 192.168.81.37\n~]# ipvsadm -Ln\n~~~\n\n![1579077146418](assets/1579077146418.png)\n\n![1579077275447](assets/1579077275447.png)\n\n~~~\n# 做成两份代理服务器,22机器\n~]# kubectl scale deployment nginx-dp --replicas=2 -n kube-public\n~]# ipvsadm -Ln\n~~~\n\n> **kubectl scale：**    扩容或缩容 Deployment等中Pod数量\n>\n> - --replicas=2：把Pod数量改为2，即如果之前是1则扩容变成2，如果之前是3则缩容变成2\n\n可以看到下面的Pod已经变成了两个，而上图是只有一个的\n\n![1579077403598](assets/1579077403598.png)\n\n~~~\n# 获取资源配置清单，21机器\n~]# kubectl get pods -n kube-public\n~]# kubectl get pods nginx-dp-5dfc689474-788xp -o yaml -n kube-public\n# 解释怎么用\n~]# kubectl explain service.metadata\n~~~\n\n> 资源清单的内容解释由于太多，这里就不做解析了，感兴趣的朋友可以网上搜下\n\n![1579079365291](assets/1579079365291.png)\n\n~~~\n# 声明式、21机器：\n~]# vi nginx-ds-svc.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: nginx-ds\n  name: nginx-ds\n  namespace: default\nspec:\n  ports:\n  - port: 80\n    protocol: TCP\n    targetPort: 80\n  selector:\n    app: nginx-ds\n  sessionAffinity: None\n  type: ClusterIP\n  \n~]# kubectl create -f nginx-ds-svc.yaml\n# out: service/nginx-ds created\n~]# kubectl get svc -n default\n~]# kubectl get svc nginx-ds -o yaml\n~~~\n\n![1579079818058](assets/1579079818058.png)\n\n~~~\n# 修改资源，在线方式：\n~]# kubectl edit svc nginx-ds\n~]# kubectl get svc\n# 离线：删了再打开，离线修改有记录\n# 删除资源,实验，按照以下方法是无法删除的~去找一下吧\n# 陈述式\n~]# kubectl delete -f nginx-ds\n# 声明式\n~]# kubectl delete -f nginx-dp-svc.yaml \n~~~\n\n> 当然删不了也无所谓\n\n![1579085057843](assets/1579085057843.png)\n\n回顾完成\n\n\n\n### 安装部署flanneld\n\n> **WHAT**：通过给每台宿主机分配一个子网的方式为容器提供虚拟网络（覆盖网络），该网络中的结点可以看作通过虚拟或逻辑链路而连接起来的\n>\n> **WHY**：我们生产上的集群宿主机/容器之间必须是互通的，因为只有互通才能形成集群，要是集群间的宿主机和容器都不互通，那就没有做集群的必要了\n\n~~~\n# 你可以做如下尝试，21机器：\n~]# kubectl get pods -o wide\n~]# ping 172.7.21.2\n~]# ping 172.7.22.2\n~~~\n\n![1579141174992](assets/1579141174992.png)\n\n> 你可以发现，两个容器的宿主机之间是不互通的，更别说进入容器里面了。（当然ping10.4.7.22是没问题的）\n>\n> 这时候我们就需要CNI网络插件，CNI最主要的功能是实现POD资源能够跨宿主机进行通信，当然CNI网络插件有很多种，如Flannel、Calico等，而Flannel是目前市场上最为火热的\n\n~~~~\n# 21/22机器：\n~]# cd /opt/src/\nsrc]# wget https://github.com/coreos/flannel/releases/download/v0.11.0/flannel-v0.11.0-linux-amd64.tar.gz\nsrc]# mkdir /opt/flannel-v0.11.0\nsrc]# tar xf flannel-v0.11.0-linux-amd64.tar.gz -C /opt/flannel-v0.11.0/\nsrc]# ln -s /opt/flannel-v0.11.0/ /opt/flannel\nsrc]# cd /opt/flannel\nflannel]# ll\n# out:总用量 34436\nflannel]# mkdir cert\nflannel]# cd cert/\ncert]# scp hdss7-200:/opt/certs/ca.pem . \ncert]# scp hdss7-200:/opt/certs/client.pem .\ncert]# scp hdss7-200:/opt/certs/client-key.pem .\ncert]# cd ..\n# 注意机器名，需要改一处：SUBNET=172.7.21.1/24，需要改成SUBNET=172.7.22.1/24\nflannel]# vi subnet.env\nFLANNEL_NETWORK=172.7.0.0/16\nFLANNEL_SUBNET=172.7.21.1/24\nFLANNEL_MTU=1500\nFLANNEL_IPMASQ=false\n\n~~~~\n\n![1579144065680](assets/1579144065680.png)\n\n~~~\n# 21/22机器,注意，我的网络是eth0，新版的是ens33，如果是ens33，则需要改iface，其它需要改一处机器名：ip=10.4.7.21\nflannel]# vi flanneld.sh\n#!/bin/sh\n./flanneld \\\n  --public-ip=10.4.7.21 \\\n  --etcd-endpoints=https://10.4.7.12:2379,https://10.4.7.21:2379,https://10.4.7.22:2379 \\\n  --etcd-keyfile=./cert/client-key.pem \\\n  --etcd-certfile=./cert/client.pem \\\n  --etcd-cafile=./cert/ca.pem \\\n  --iface=eth0 \\\n  --subnet-file=./subnet.env \\\n  --healthz-port=2401\n  \nflannel]# chmod +x flanneld.sh\nflannel]# mkdir -p /data/logs/flanneld\nflannel]# cd /opt/etcd\n# 下面这一步在一部机器上执行即可，只需执行一次，我在21机器做的：\netcd]# ./etcdctl set /coreos.com/network/config '{\"Network\": \"172.7.0.0/16\", \"Backend\": {\"Type\": \"host-gw\"}}'\netcd]# ./etcdctl get /coreos.com/network/config\n# out:{\"Network\": \"172.7.0.0/16\", \"Backend\": {\"Type\": \"host-gw\"}}\n\n# 有一处要修改，21/22机器：flanneld-7-21]\netcd]# vi /etc/supervisord.d/flannel.ini\n[program:flanneld-7-21]\ncommand=/opt/flannel/flanneld.sh                             ; the program (relative uses PATH, can take args)\nnumprocs=1                                                   ; number of processes copies to start (def 1)\ndirectory=/opt/flannel                                       ; directory to cwd to before exec (def no cwd)\nautostart=true                                               ; start at supervisord start (default: true)\nautorestart=true                                             ; retstart at unexpected quit (default: true)\nstartsecs=30                                                 ; number of secs prog must stay running (def. 1)\nstartretries=3                                               ; max # of serial start failures (default 3)\nexitcodes=0,2                                                ; 'expected' exit codes for process (default 0,2)\nstopsignal=QUIT                                              ; signal used to kill process (default TERM)\nstopwaitsecs=10                                              ; max num secs to wait b4 SIGKILL (default 10)\nuser=root                                                    ; setuid to this UNIX account to run the program\nredirect_stderr=true                                         ; redirect proc stderr to stdout (default false)\nstdout_logfile=/data/logs/flanneld/flanneld.stdout.log       ; stderr log path, NONE for none; default AUTO\nstdout_logfile_maxbytes=64MB                                 ; max # logfile bytes b4 rotation (default 50MB)\nstdout_logfile_backups=4                                     ; # of stdout logfile backups (default 10)\nstdout_capture_maxbytes=1MB                                  ; number of bytes in 'capturemode' (default 0)\nstdout_events_enabled=false                                  ; emit events on stdout writes (default false)\n\netcd]# supervisorctl update\netcd]# supervisorctl status\n# 查看细节信息\netcd]# tail -fn 200 /data/logs/flanneld/flanneld.stdout.log \n# 两部机器完成后，在21和22机器ping对方，已经可以ping通\n~~~\n\n![1579147672612](assets/1579147672612.png)\n\n完成\n\nflannel原理：添加静态路由（前提条件，必须处在同一网关之下）\n\n利用10.4.7.x本来互通的前提，172先去找10再转到其下面的172，形成互通\n\n![1582269815607](assets/1582269815607.png)\n\n> 再次复习一遍，10的21机器对应的172的21，这样方便知道那些pod在那些机器上\n\n\n\n### flannel之SNAT规则优化\n\n> **WHAT**：使得容器之间的透明访问\n>\n> **WHY**：解决两宿主机容器之间的透明访问，如不进行优化，容器之间的访问，日志记录为宿主机的IP地址\n\n~~~\n# 把nginx:curl拉下来，21机器\n~]# docker login docker.io/909336740/nginx:curl\n~]# docker pull 909336740/nginx:curl\n~]# docker images|grep curl\n~]# docker tag 34736e20b17b harbor.od.com/public/nginx:curl\n~]# docker login harbor.od.com\n~]# docker push harbor.od.com/public/nginx:curl\n~~~\n\n![1579152363774](assets/1579152363774.png)\n\n~~~\n# 改以下内容，21机器：\ncd\n~]# vi nginx-ds.yaml\nimage: harbor.od.com/public/nginx:curl\n~]# kubectl apply -f nginx-ds.yaml\n~]# kubectl get pods\n# 删掉两个pod让它们自动重启以便应用新镜像\n~]# kubectl delete pod nginx-ds-5nhq6\n# out:pod \"nginx-ds-5nhq6\" deleted\n~]# kubectl delete pod nginx-ds-cfjvn\n#out:pod \"nginx-ds-cfjvn\" deleted\n~]# kubectl get pods -o wide\n~~~\n\n![1579152725660](assets/1579152725660.png)\n\n~~~\n# 21机器：\n~]# kubectl exec -ti nginx-ds-6nmbr /bin/bash\n6nmbr:/# curl 172.7.22.2\n# 注意这个pod是起在了22网络了，如果网段没有在22上，就curl 172.7.22.2，只要有welcome的网页回应即可，而且log日志也有\n\n# 22机器\netcd]# kubectl get pods -o wide\netcd]# kubectl logs -f nginx-ds-drrkt\n~~~\n\n> **kubectl logs -f**：查看Pod日志\n\n![1579154925882](assets/1579154925882.png)\n\n![1579155042838](assets/1579155042838.png)\n\n确认启动正常\n\n~~~\n# 21机器：\n~]# iptables-save |grep -i postrouting\n~~~\n\n![1582271259863](assets/1582271259863.png)\n\n> **iptables：**\n>\n> - `语法：iptables [-t 表名] 选项 [链名] [条件] [-j 控制类型]`\n> - **-A**：在规则链的末尾加入新规则\n> - **-s**：匹配来源地址IP/MASK，加叹号\"!\"表示除这个IP外\n> - **-o**：匹配从这块网卡流出的数据\n> - **MASQUERADE**：动态伪装，能够自动的寻找外网地址并改为当前正确的外网IP地址\n> - 上面红框内的可以理解为：如果是172.7.21.0/24段的docker的ip，网络发包不从docker0桥设备出战的，就进行SNAT转换，而我们需要的是如果出网的地址是172.7.21.0/24或者172.7.0.0/16网络（这是docker的大网络），就不要做源地址NAT转换，因为我们集群内部需要坦诚相见，自己人不需要伪装。\n\n~~~\n# 21/22机器，我们开始改：\n~]# yum install iptables-services -y\n~]# systemctl start iptables\n~]# systemctl enable iptables\n# 删掉对应的规则，以下需要对应机器，一处修改：-s 172.7.21\n~]# iptables -t nat -D POSTROUTING -s 172.7.21.0/24 ! -o docker0 -j MASQUERADE\n# 添加对应的规则，以下需要对应机器，一处修改：-s 172.7.21\n~]# iptables -t nat -I POSTROUTING -s 172.7.21.0/24 ! -d 172.7.0.0/16 ! -o docker0 -j MASQUERADE\n# 上面这条规则可以理解为：只有出网地址不是172.7.21.0/24或者172.7.0.0/16，网络发包不从docker0桥设备出战的，才做SNAT转换\n~]# iptables-save |grep -i postrouting\n~]# iptables-save > /etc/sysconfig/iptables\n# 21机器curl22，22机器curl21\n~]# kubectl exec -ti nginx-ds-6nmbr /bin/bash\n6nmbr:/# curl 172.7.22.2\n\n### 相关报错\n# 如果报错：curl: (7) Failed to connect to 172.7.22.2 port 80: No route to host\n# 则执行以下操作，在删掉两台机器21/22的iptables的reject，两边同时执行\n~]# iptables-save|grep -i reject\n~]# iptables -t filter -D [名字]\n~]# iptables-save > /etc/sysconfig/iptables\n###\n~~~\n\n> **iptables：**\n>\n> - `语法：iptables [-t 表名] 选项 [链名] [条件] [-j 控制类型]`\n> - -D：删除某一条规则\n> - -I：在规则链的头部加入新规则\n> - -s：匹配来源地址IP/MASK，加叹号\"!\"表示除这个IP外\n> - -d：匹配目标地址\n> - -o：匹配从这块网卡流出的数据\n> - MASQUERADE：动态伪装，能够自动的寻找外网地址并改为当前正确的外网IP地址\n\n![1579157014119](assets/1579157014119.png)\n\n成功图，在22上已经可以明确的看到对方是172.7.21.2了，在21上可以看到对方是172的22：\n\n![1579157196199](assets/1579157196199.png)\n\n![1579157441471](assets/1579157441471.png)\n\n完成\n\n\n\n### 安装部署coredns（服务发现）：\n\n> **WHAT**：服务（应用）之间相互定位的过程\n>\n> **WHY：**\n>\n> - 服务发现对应的场景：\n>   - 服务（应用）的动态性抢\n>   - 服务（应用）更新发布频繁\n>   - 服务（应用）支持自动伸缩\n>\n> - kuberntes中的所有pod都是基于Service域名解析后，再负载均衡分发到service后端的各个pod服务中，POD的IP是不断变化的。如何解决：\n>   - 抽象出Service资源，通过标签选择器，关联一组POD\n>   - 抽象出集群网络，通过固定的“集群IP”，使服务接入点固定\n> - 如何管理Service资源的“名称”和“集群网络IP”\n>   - 我们前面做了传统的DNS模型：hdss7-21.host.com -> 10.4.7.21\n>   - 那么我们可以在K8S里做这样的模型：nginx-ds -> 192.168.0.1\n\n~~~\n# 现在我们要开始用交付容器方式交付服务（非二进制），这也是以后最常用的方式\n# 200机器\ncerts]# cd /etc/nginx/conf.d/\nconf.d]# vi /etc/nginx/conf.d/k8s-yaml.od.com.conf\nserver {\n    listen       80;\n    server_name  k8s-yaml.od.com;\n\n    location / {\n        autoindex on;\n        default_type text/plain;\n        root /data/k8s-yaml;\n    }\n}\n\nconf.d]# mkdir /data/k8s-yaml\nconf.d]# nginx -t\nconf.d]# nginx -s reload\n~~~\n\n\n\n~~~\n# 11机器，解析域名：\n~]# vi /var/named/od.com.zone\nserial 前滚一位\n# 最下面添加这个网段，以后也都是在最下面添加，后面我就加这个注释了\nk8s-yaml           A    10.4.7.200\n\n~]# systemctl restart named\n~]# dig -t A k8s-yaml.od.com @10.4.7.11 +short\n# out：10.4.7.200\n~~~\n\n> **dig -t A**：指的是找DNS里标记为A的相关记录，@用什么机器IP访问，+short是只返回IP\n\n![1579158143760](assets/1579158143760.png)\n\n~~~\n# 200机器\nconf.d]# cd /data/k8s-yaml/\nk8s-yaml]# mkdir coredns\n~~~\n\n[k8s-yaml.od.com](k8s-yaml.od.com)\n\n![1579158360896](assets/1579158360896.png)\n\n~~~~\n# 200机器，下载coredns镜像：\ncd /data/k8s-yaml/\nk8s-yaml]# docker pull coredns/coredns:1.6.1\nk8s-yaml]# docker images|grep coredns\nk8s-yaml]# docker tag c0f6e815079e harbor.od.com/public/coredns:v1.6.1\nk8s-yaml]# docker push !$\n~~~~\n\n> 这里我们需要注意的是，任何我用到的镜像都会推到我的本地私有仓库，原因前面也说了，1、是为了用的时候速度快保证不出现网络问题，2、保证版本是同样的版本，而不是突然被别人修改了\n>\n> **docker push !$**：push上一个镜像的名字\n\n~~~\n# 200机器，准备资源配置清单：\ncd /data/k8s-yaml/coredns\ncoredns]# vi rbac.yaml\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: coredns\n  namespace: kube-system\n  labels:\n      kubernetes.io/cluster-service: \"true\"\n      addonmanager.kubernetes.io/mode: Reconcile\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  labels:\n    kubernetes.io/bootstrapping: rbac-defaults\n    addonmanager.kubernetes.io/mode: Reconcile\n  name: system:coredns\nrules:\n- apiGroups:\n  - \"\"\n  resources:\n  - endpoints\n  - services\n  - pods\n  - namespaces\n  verbs:\n  - list\n  - watch\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  annotations:\n    rbac.authorization.kubernetes.io/autoupdate: \"true\"\n  labels:\n    kubernetes.io/bootstrapping: rbac-defaults\n    addonmanager.kubernetes.io/mode: EnsureExists\n  name: system:coredns\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: system:coredns\nsubjects:\n- kind: ServiceAccount\n  name: coredns\n  namespace: kube-system\n  \ncoredns]# vi cm.yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: coredns\n  namespace: kube-system\ndata:\n  Corefile: |\n    .:53 {\n        errors\n        log\n        health\n        ready\n        kubernetes cluster.local 192.168.0.0/16\n        forward . 10.4.7.11\n        cache 30\n        loop\n        reload\n        loadbalance\n       }\n       \ncoredns]# vi dp.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: coredns\n  namespace: kube-system\n  labels:\n    k8s-app: coredns\n    kubernetes.io/name: \"CoreDNS\"\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      k8s-app: coredns\n  template:\n    metadata:\n      labels:\n        k8s-app: coredns\n    spec:\n      priorityClassName: system-cluster-critical\n      serviceAccountName: coredns\n      containers:\n      - name: coredns\n        image: harbor.od.com/public/coredns:v1.6.1\n        args:\n        - -conf\n        - /etc/coredns/Corefile\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/coredns\n        ports:\n        - containerPort: 53\n          name: dns\n          protocol: UDP\n        - containerPort: 53\n          name: dns-tcp\n          protocol: TCP\n        - containerPort: 9153\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n      dnsPolicy: Default\n      volumes:\n        - name: config-volume\n          configMap:\n            name: coredns\n            items:\n            - key: Corefile\n              path: Corefile\n              \ncoredns]# vi svc.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: coredns\n  namespace: kube-system\n  labels:\n    k8s-app: coredns\n    kubernetes.io/cluster-service: \"true\"\n    kubernetes.io/name: \"CoreDNS\"\nspec:\n  selector:\n    k8s-app: coredns\n  clusterIP: 192.168.0.2\n  ports:\n  - name: dns\n    port: 53\n    protocol: UDP\n  - name: dns-tcp\n    port: 53\n  - name: metrics\n    port: 9153\n    protocol: TCP\n~~~\n\n\n\n~~~~\n# 21机器，应用资源配置清单（陈述式）：\n~]# kubectl apply -f http://k8s-yaml.od.com/coredns/rbac.yaml\n~]# kubectl apply -f http://k8s-yaml.od.com/coredns/cm.yaml\n~]# kubectl apply -f http://k8s-yaml.od.com/coredns/dp.yaml\n~]# kubectl apply -f http://k8s-yaml.od.com/coredns/svc.yaml\n~]# kubectl get all -n kube-system\n~~~~\n\n![1579159195207](assets/1579159195207.png)\n\n> **CLUSTER-IP为什么是192.168.0.2**：因为我们之前已经写死了这是我们dns的统一接入点\n>\n> ![1582278638665](assets/1582278638665.png)\n\n~~~\n# 21机器，测试（我的已经存在了，不过不影响）：\n~]# kubectl create deployment nginx-dp --image=harbor.od.com/public/nginx:v1.7.9 -n kube-public\n~]# kubectl expose deployment nginx-dp --port=80 -n kube-public\n~]# kubectl get svc -n kube-public\n~]# dig -t A nginx-dp.kube-public.svc.cluster.local. @192.168.0.2 +short\n# out：192.168.81.37\n~~~\n\n> **dig -t A**：指的是找DNS里标记为A的相关记录，@用什么机器IP访问，+short是只返回IP\n\n![1579161063657](assets/1579161063657.png)\n\n完成集群“内”被自动发现\n\n\n\n### K8S的服务暴露ingress\n\n> **WHAT**：K8S API的标准资源类型之一，也是核心资源，它是基于域名和URL路径，把用户的请求转发至指定Service资源的规则\n>\n> - 将集群外部的请求流量，转发至集群内部，从而实现“服务暴露”\n> - nginx + go脚本\n>\n> **WHY**：上面实现了服务在集群“内”被自动发现，那么需要使得服务在集群“外”被使用和访问，常规的两种方法：\n>\n> - 使用NodePort型的service\n>   - 无法使用kube-proxy的ipvs模型，只能使用iptables模型\n> - 使用ingress资源\n>   - 只能调度并暴露7蹭应用，特指http和https协议\n\n##### 以trafiker为例\n\n> **WHAT**：为了让部署微服务更加便捷而诞生的现代HTTP反向代理、负载均衡工具。\n>\n> **WHY**：可以监听你的服务发现/基础架构组件的管理API，并且每当你的微服务被添加、移除、杀死或更新都会被感知，并且可以自动生成它们的配置文件\n\n~~~\n# 200机器，部署traefiker（ingress控制器）\ncd /data/k8s-yaml/\nk8s-yaml]# mkdir traefik\nk8s-yaml]# cd traefik/\ntraefik]# docker pull traefik:v1.7.2-alpine\ntraefik]# docker images|grep traefik\ntraefik]# docker tag add5fac61ae5 harbor.od.com/public/traefik:v1.7.2\ntraefik]# docker push harbor.od.com/public/traefik:v1.7.2\n~~~\n\n> 复习：mkdir 创建目录、cd 移动到其它目录、\n>\n> docker pull 下载镜像、docker tag 打标签、docker push 上传到仓库\n\n~~~\n# 200机器，准备资源配置清单(4个yaml)：\ntraefik]# vi rbac.yaml\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: traefik-ingress-controller\n  namespace: kube-system\n---\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: ClusterRole\nmetadata:\n  name: traefik-ingress-controller\nrules:\n  - apiGroups:\n      - \"\"\n    resources:\n      - services\n      - endpoints\n      - secrets\n    verbs:\n      - get\n      - list\n      - watch\n  - apiGroups:\n      - extensions\n    resources:\n      - ingresses\n    verbs:\n      - get\n      - list\n      - watch\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: traefik-ingress-controller\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: traefik-ingress-controller\nsubjects:\n- kind: ServiceAccount\n  name: traefik-ingress-controller\n  namespace: kube-system\n\ntraefik]# vi ds.yaml\napiVersion: extensions/v1beta1\nkind: DaemonSet\nmetadata:\n  name: traefik-ingress\n  namespace: kube-system\n  labels:\n    k8s-app: traefik-ingress\nspec:\n  template:\n    metadata:\n      labels:\n        k8s-app: traefik-ingress\n        name: traefik-ingress\n    spec:\n      serviceAccountName: traefik-ingress-controller\n      terminationGracePeriodSeconds: 60\n      containers:\n      - image: harbor.od.com/public/traefik:v1.7.2\n        name: traefik-ingress\n        ports:\n        - name: controller\n          containerPort: 80\n          hostPort: 81\n        - name: admin-web\n          containerPort: 8080\n        securityContext:\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_BIND_SERVICE\n        args:\n        - --api\n        - --kubernetes\n        - --logLevel=INFO\n        - --insecureskipverify=true\n        - --kubernetes.endpoint=https://10.4.7.10:7443\n        - --accesslog\n        - --accesslog.filepath=/var/log/traefik_access.log\n        - --traefiklog\n        - --traefiklog.filepath=/var/log/traefik.log\n        - --metrics.prometheus\n\t\t\ntraefik]# vi svc.yaml\nkind: Service\napiVersion: v1\nmetadata:\n  name: traefik-ingress-service\n  namespace: kube-system\nspec:\n  selector:\n    k8s-app: traefik-ingress\n  ports:\n    - protocol: TCP\n      port: 80\n      name: controller\n    - protocol: TCP\n      port: 8080\n      name: admin-web\n\t  \ntraefik]# vi ingress.yaml\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: traefik-web-ui\n  namespace: kube-system\n  annotations:\n    kubernetes.io/ingress.class: traefik\nspec:\n  rules:\n  - host: traefik.od.com\n    http:\n      paths:\n      - path: /\n        backend:\n          serviceName: traefik-ingress-service\n          servicePort: 8080\n~~~\n\n> 每次有ingress时，我们第一反应就是要去解析域名\n>\n> 这里为什么我们都可以把什么都丢到80端口，是因为现在已经是Pod了，已经隔离了，无所谓你用什么端口\n\n~~~\n# 21/22任意机器（我用的22），应用资源配置清单：\n~]# kubectl apply -f http://k8s-yaml.od.com/traefik/rbac.yaml\n~]# kubectl apply -f http://k8s-yaml.od.com/traefik/ds.yaml\n~]# kubectl apply -f http://k8s-yaml.od.com/traefik/svc.yaml\n~]# kubectl apply -f http://k8s-yaml.od.com/traefik/ingress.yaml\n# 下面重启docker服务要在21/22节点都执行，否则会有一个起不来\n~]# systemctl restart docker.service\n~]# kubectl get pods -n kube-system\n~]# netstat -luntp|grep 81\n~~~\n\n![1579165378812](assets/1579165378812.png)\n\n~~~\n# 11/12机器，做反代：\n~]# vi /etc/nginx/conf.d/od.com.conf\nupstream default_backend_traefik {\n    server 10.4.7.21:81    max_fails=3 fail_timeout=10s;\n    server 10.4.7.22:81    max_fails=3 fail_timeout=10s;\n}\nserver {\n    server_name *.od.com;\n  \n    location / {\n        proxy_pass http://default_backend_traefik;\n        proxy_set_header Host       $http_host;\n        proxy_set_header x-forwarded-for $proxy_add_x_forwarded_for;\n    }\n}\n\n~]# nginx -t\n~]# nginx -s reload\n\n# 11机器,解析域名：\n~]# vi /var/named/od.com.zone \n前滚serial\ntraefik            A    10.4.7.10\n\n~]# systemctl restart named\n~~~\n\n> **nginx -t**：检查nginx.conf文件有没有语法错误\n>\n> **nginx -s reload**：不需要重启nginx的热配置\n\n![1579167500955](assets/1579167500955.png)\n\n[访问traefik.od.com](traefik.od.com)\n\n![1579167546083](assets/1579167546083.png)\n\n完成\n\n##### 用户访问流程：\n\n当用户输入traefik.od.com时，被dns解析到10.4.7.10，而10则在11上，去找L7层服务，而反代配置的od.com.conf，则是将*.od.com无差别的抛给了ingress，ingress则通过noteselect找到pod\n\n![1584961721898](assets/1584961721898.png)\n\n再回顾上面的架构图，我们已经全部安装部署完。\n\n接下来，我们就要开始安装部署K8S的周边生态，使其成为一个**真正的PaaS服务**\n\n<a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E5%8E%9F%E7%90%86%E5%8F%8A%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/Kubernetes%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5.md#kubernetes%E6%8A%80%E8%83%BD%E5%9B%BE%E8%B0%B1\">kubernetes技能图谱</a>\n\n"
        },
        {
          "name": "第二章——企业部署实战_K8S.md",
          "type": "blob",
          "size": 68.826171875,
          "content": "## 第二章——企业部署实战_K8S\n\n公有云版：[第二章——企业部署实战_K8S【公有云版】](https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E4%BA%8C%E7%AB%A0%E2%80%94%E2%80%94%E4%BC%81%E4%B8%9A%E9%83%A8%E7%BD%B2%E5%AE%9E%E6%88%98_K8S%E3%80%90%E5%85%AC%E6%9C%89%E4%BA%91%E7%89%88%E3%80%91.md)\n\n**由于第二章需要配置的内容较多，不少新同学卡在这里。建议配合使用[软件包里的check_tool](https://github.com/ben1234560/k8s_PaaS/tree/master/%E8%BD%AF%E4%BB%B6%E5%8C%85/check_tool)边检查，边部署**\n\n##### 前言：如果你是新手，机器的名字及各种账户密码一定要和我的一样，先学一遍，再自己改\n\n> **WHAT**：用于管理云平台中多个主机上的容器化的应用，Kubernetes的目标是让部署容器化的应用简单并且高效（powerful）,Kubernetes提供了应用部署，规划，更新，维护的一种机制\n>\n> **WHY**：为什么使用它，因为它是管理docker容器最主流的编排工具\n\n- Pod\n  - Pod是K8S里能够被运行的最小的逻辑单元（原子单元）\n  - 1个Pod里面可以运行多个容器，它们共享UTS+NET+IPC名称空间\n  - 可以把Pod理解成豌豆荚，而同一Pod内的每个容器是一颗颗豌豆\n  - 一个Pod里运行多个容器，又叫边车（SideCar）模式\n- Pod控制器（关于更多<a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E5%8E%9F%E7%90%86%E5%8F%8A%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/Kubernetes%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5.md#%E5%88%9D%E8%AF%86pod\">初识Pod</a>）\n  - Pod控制器是Pod启动的一种模板，用来保证在K8S里启动的Pod始终按照人们的预期运行（副本数、生命周期、健康状态检查...）\n  - Pod内提供了众多的Pod控制器，常用的有以下几种：\n    - Deployment\n    - DaemonSet\n    - ReplicaSet\n    - StatefulSet\n    - Job\n    - Cronjob\n- Name\n  - 由于K8S内部，使用“资源”来定义每一种逻辑概念（功能），故每种“资源”，都应该有自己的“名称”\n  - “资源”有api版本（apiVersion）类别（kind）、元数据（metadata）、定义清单（spec）、状态（status）等配置信息\n  - “名称”通常定义在“资源”的“元数据”信息里\n- <a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E5%8E%9F%E7%90%86%E5%8F%8A%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/Docker%E5%9F%BA%E7%A1%80.md#%E5%85%B3%E4%BA%8Enamespace\">namespace</a>\n  - 随着项目增多、人员增加、集群规模的扩大，需要一种能够隔离K8S内各种“资源”的方法，这就是名称空间\n  - 名称空间可以理解为K8S内部的虚拟集群组\n  - 不同名称空间内的“资源”名称可以相同，相同名称空间内的同种“资源”、“名称”不能相同\n  - 合理的使用K8S名称空间，使得集群管理员能够更好的对交付到K8S里的服务进行分类管理和浏览\n  - K8S内默认存在的名称空间有：default、kube-system、kube-public\n  - 查询K8S里特定“资源”要带上相应的名称空间\n- Label\n  - 标签是K8S特色的管理方式，便于分类管理资源对象\n  - 一个标签可以对应多个资源，一个资源也可以有多个标签，它们是多对多的关系\n  - 一个资源拥有多个标签，可以实现不同维度的管理\n  - 标签的组成：key=value\n  - 与标签类似的，还有一种“注解”（annotations）\n- Label选择器\n  - 给资源打上标签后，可以使用标签选择器过滤指定的标签\n  - 标签选择器目前有两个：基于等值关系（等于、不等于）和基于集合关系（属于、不属于、存在）\n  - 许多资源支持内嵌标签选择器字段\n    - matchLabels\n    - matchExpressions\n- Service\n  - 在K8S的世界里，虽然每个Pod都会被分配一个单独的IP地址，但这个IP地址会随着Pod的销毁而消失\n  - Service（服务）就是用来解决这个问题的核心概念\n  - 一个Service可以看作一组提供相同服务的Pod的对外访问接口\n  - Service作用与哪些Pod是通过标签选择器来定义的\n- Ingress\n  - Ingress是K8S集群里工作在OSI网络参考模型下，第7层的应用，对外暴露的接口\n  - Service只能进行L4流量调度，表现形式是ip+port\n  - Ingress则可以调度不同业务域、不同URL访问路径的业务流量\n\n简单理解：Pod可运行的原子，name定义名字，namespace名称空间（放一堆名字），label标签（另外的名字），service提供服务，ingress通信\n\n### K8S架构图（并非传统意义上的PaaS服务，而是IaaS服务）\n\n![1582188308711](assets/1582188308711.png)\n\n> **kubectl**：Kubernetes集群的命令行接口\n>\n> **API Server**：的核心功能是对核心对象（例如：Pod，Service，RC）的增删改查操作，同时也是集群内模块之间数据交换的枢纽\n>\n> **Etcd**：包含在 APIServer 中，用来存储资源信息\n>\n> **Controller Manager **：负责维护集群的状态，比如故障检测、自动扩展、滚动更新等\n>\n> **Scheduler**：负责资源的调度，按照预定的调度策略将Pod调度到相应的机器上。可以通过这些有更深的了解：\n>\n> - <a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E5%8E%9F%E7%90%86%E5%8F%8A%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/Kubernetes%E8%B0%83%E5%BA%A6%E6%9C%BA%E5%88%B6.md\">Kubernetes调度机制</a>\n> - <a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E5%8E%9F%E7%90%86%E5%8F%8A%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/Kubernetes%E8%B0%83%E5%BA%A6%E6%9C%BA%E5%88%B6.md#kubernetes%E7%9A%84%E8%B5%84%E6%BA%90%E6%A8%A1%E5%9E%8B%E4%B8%8E%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86\">Kubernetes的资源模型与资源管理</a>\n> - <a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E5%8E%9F%E7%90%86%E5%8F%8A%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/Kubernetes%E8%B0%83%E5%BA%A6%E6%9C%BA%E5%88%B6.md#kubernetes%E9%BB%98%E8%AE%A4%E7%9A%84%E8%B0%83%E5%BA%A6%E7%AD%96%E7%95%A5\">Kubernetes默认的调度策略</a>\n> - <a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E5%8E%9F%E7%90%86%E5%8F%8A%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/Kubernetes%E8%B0%83%E5%BA%A6%E6%9C%BA%E5%88%B6.md#%E8%B0%83%E5%BA%A6%E5%99%A8%E7%9A%84%E4%BC%98%E5%85%88%E7%BA%A7%E4%B8%8E%E5%BC%BA%E5%88%B6%E6%9C%BA%E5%88%B6\">调度器的优先级与强制机制</a>\n>\n> **kube-proxy**：负责为Service提供cluster内部的服务发现和负载均衡\n>\n> **Kubelet**：在Kubernetes中，应用容器彼此是隔离的，并且与运行其的主机也是隔离的，这是对应用进行独立解耦管理的关键点。[Kubelet工作原理解析](https://github.com/ben1234560/k8s_PaaS/blob/master/%E5%8E%9F%E7%90%86%E5%8F%8A%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/Kubernetes%E8%B0%83%E5%BA%A6%E6%9C%BA%E5%88%B6.md#kubelet)\n>\n> **Node**：运行容器应用，由Master管理\n\n### 我们部署的K8S架构图\n\n![1584700891603](assets/1584700891603.png)\n\n> 可以简单理解成：\n>\n> 11机器：反向代理\n>\n> 12机器：反向代理\n>\n> 21机器：主控+运算节点（即服务群都是跑在21和22上）\n>\n> 22机器：主控+运算节点（生产上我们会把主控和运算分开）\n>\n> 200机器：运维主机（放各种文件资源）\n>\n> 这样控节点有两个，运算节点有两个，就是小型的分布式，现在你可能没办法理解这些内容，我们接着做下去，慢慢的，你就理解了\n\n\n\n### 实验机器安装详解\n\n准备一台8C64G的机器，我们将它分成5个节点，如下图\n\n> 如果你的机器资源紧张，请使用公有云版：[第二章——企业部署实战_K8S【公有云版】](https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E4%BA%8C%E7%AB%A0%E2%80%94%E2%80%94%E4%BC%81%E4%B8%9A%E9%83%A8%E7%BD%B2%E5%AE%9E%E6%88%98_K8S%E3%80%90%E5%85%AC%E6%9C%89%E4%BA%91%E7%89%88%E3%80%91.md)\n\n|      | 整体  | 11机器 | 12机器 | 21机器 | 22机器 | 200机器 |\n| ---- | ----- | ------ | ------ | ------ | ------ | ------- |\n| 低配 | 4C32G | 2C2G   | 2C2G   | 2C8G   | 2C8G   | 2C2G    |\n| 标配 | 8C64G | 2C4G   | 2C4G   | 2C16G  | 2C16G  | 2C2G    |\n\n> 如果你的电脑是4C16G的（一般笔记本都有的），你可以先用你的电脑尝试做这个PaaS服务，做到Jenkins的时候就已经很卡了，到时候你再买服务器也比较省钱，前期我还是希望各位能在理解上花些时间，慢慢的操作，否则报错都不知道怎么解决\n\n关于怎么制作虚拟机并连接NAT网并连接shell\n\n设置NAT（这样才可以直接在电脑浏览器访问到），如图：\n\n![1578637492550](assets/1578637492550.png)\n\n![1578637749475](assets/1578637749475.png)\n\n使用我的镜像包或者任意7.6以上版本的centos（这个工具是VMware Workstation Pro）\n\n> 7.6镜像网上资源少，这里提供一个：https://pan.baidu.com/s/1mkIzua1XQmew240XBbvuFA 提取码：7p6h 。注意：镜像包的network是ens33，我用的是eth0，下面你就知道在哪用了，你也可以改成跟我一样，这是百度经验https://jingyan.baidu.com/article/17bd8e524c76a285ab2bb8ff.html\n\n> virtualbox 也是可以的, 网络模式不仅仅限于nat, 本人试过桥接模式, 也是可以的, 并且宿主机可以把DNS指定为自己的虚拟机11地址,这样在宿主机访问harbor的时候就不用写hosts文件了\n\n![1583025768982](assets/1583025768982.png)\n\n![1578645849482](assets/1578645849482.png)\n\n![1582876384254](assets/1582876384254.png)\n\n位置自己存放在一个比较大的位置，我是放在了G盘\n\n![1578645877869](assets/1578645877869.png)\n\n![1578645884437](assets/1578645884437.png)\n\n![1578645890203](assets/1578645890203.png)\n\n连接进去后，到Linux安装页面（可以查网上的），安装minimal设置好root即可。\n\n然后再打开，此时ping是ping不通的，不需要管\n\n右键模板机->管理->克隆->下一步->下一步（虚拟机当前状态）->下一步（创建链接克隆）下一步->完成（改好）\n\n![1580368269935](assets/1580368269935.png)\n\n开启此虚拟机，我创建了5台机器了（用的自己电脑4C16G）\n\n![1578638975964](assets/1578638975964.png)\n\n![1578814673242](assets/1578814673242.png)\n\n~~~\n# 全部机器，设置名字，11是hdss7-11,12是hdss7-12,以此类推\n~]# hostnamectl set-hostname hdss7-11.host.com\n~]# exit\n# 下面的修改，11机器对应11，12对应12，以此类推\n~]# vi /etc/sysconfig/network-scripts/ifcfg-eth0\n...修改如下图\n\n~]# systemctl restart network\n~]# ping baidu.com\n~~~\n\n> **ifcfg-eth0**：有些人的机器可能是ifcfg-esn33，自己注意即可\n>\n> **systemctl restart**：重启某个服务\n>\n> **ping**：用于测试网络连接量的程序\n\n![1580370529295](assets/1580370529295.png)\n\n![1578648756641](assets/1578648756641.png)\n\n也有这种的。（贡献者：<https://github.com/d00522>）DNS 配置成254不行就重启机器， 配成8.8.8.8或114.114.114.114。\n\n![1582957189687](assets/1582957189687.png)\n\n然后在xshell访问id即可，如图\n\n[xshell下载](https://xshell.en.softonic.com/download)\n\n> 当然我也有提供软件包：https://pan.baidu.com/s/1mkIzua1XQmew240XBbvuFA 提取码：7p6h 。里面还有Xftp，是用来进行本地电脑和虚拟机的文件传输\n\n![1578655333674](assets/1578655333674.png)\n\n~~~\n# 查看enforce是否关闭，确保disabled状态，当然可能没有这个命令\n~]# getenforce\n# 如果不为disabled，需要vim /etc/selinux/config，将SELINUX=后改为disabled后重启即可\n# 查看内核版本，确保在3.8以上版本\n~]# uname -a\n# 关闭并禁止firewalld自启\n~]# systemctl stop firewalld\n~]# systemctl disable firewalld\n# 安装epel源及相关工具\n~]# yum install epel-release -y\n~]# yum install wget net-tools telnet tree nmap sysstat lrzsz dos2unix bind-utils -y\n~~~\n\n> **uname**:显示系统信息\n>\n> - **-a/-all**：显示全部\n>\n> **yum**：提供了查找、安装、删除某一个、一组甚至全部软件包的命令\n>\n> - **install**：安装\n> - **-y**：当安装过程提示选择全部为\"yes\"\n\n\n\n### K8S前置准备工作——bind9安装部署（DNS服务）\n\n> **WHAT**：DNS（域名系统）说白了，就是把一个域和IP地址做了一下绑定，如你在里机器里面输入 nslookup www.qq.com，出来的Address是一堆IP，IP是不容易记的，所以DNS让IP和域名做一下绑定，这样你输入域名就可以了\n>\n> **WHY**：我们要用ingress，在K8S里要做7层调度，而且无论如何都要用域名（如之前的那个百度页面的域名，那个是host的方式），但是问题是我们怎么给K8S里的容器绑host，所以我们必须做一个DNS，然后容器服从我们的DNS解析调度\n\n~~~\n# 在11机器：\n~]# yum install bind -y\n~]# rpm -qa bind\n# out: bind-9.11.4-9.P2.el7.x86_64\n# 配置主配置文件，11机器\n~]# vi /etc/named.conf\nlisten-on port 53 { 10.4.7.11; };  # 原本是127.0.0.1\n# listen-on-v6 port 53 { ::1; };  # 需要删掉\nallow-query     { any; };  # 原本是locall\nforwarders      { 10.4.7.254; };  #另外添加的\ndnssec-enable no;  # 原本是yes\ndnssec-validation no;  # 原本是yes\n\n# 检查修改情况，没有报错即可（即没有信息）\n~]# named-checkconf\n~~~\n\n> **rpm**：软件包管理器\n>\n> - **-qa**：查看已安装的所有软件包\n>\n> **rpm和yum安装的区别**：前者不检查相依性问题，后者检查（即相关依赖包）\n>\n> **named.conf文件内容解析：**\n>\n> - **listen-on**：监听端口，改为监听在内网，这样其它机器也可以用\n> - **allow-query**：哪些客户端能通过自建的DNS查\n> - **forwarders**：上级DNS是什么\n\n![1580372858987](assets/1580372858987.png)\n\n~~~\n# 11机器，经验：主机域一定得跟业务是一点关系都没有，如host.com，而业务用的是od.com，因为业务随时可能变\n# 区域配置文件，加在最下面\n~]# vi /etc/named.rfc1912.zones\nzone \"host.com\" IN {\n        type  master;\n        file  \"host.com.zone\";\n        allow-update { 10.4.7.11; };\n};\n\nzone \"od.com\" IN {\n        type  master;\n        file  \"od.com.zone\";\n        allow-update { 10.4.7.11; };\n};\n\n~~~\n\n![1578818520796](assets/1578818520796.png)\n\n~~~\n# 11机器：\n# 注意serial行的时间，代表今天的时间+第一条记录：20200112+01\n7-11 ~]# vi /var/named/host.com.zone\n$ORIGIN host.com.\n$TTL 600\t; 10 minutes\n@       IN SOA\tdns.host.com. dnsadmin.host.com. (\n\t\t\t\t2020011201 ; serial\n\t\t\t\t10800      ; refresh (3 hours)\n\t\t\t\t900        ; retry (15 minutes)\n\t\t\t\t604800     ; expire (1 week)\n\t\t\t\t86400      ; minimum (1 day)\n\t\t\t\t)\n\t\t\tNS   dns.host.com.\n$TTL 60\t; 1 minute\ndns                A    10.4.7.11\nHDSS7-11           A    10.4.7.11\nHDSS7-12           A    10.4.7.12\nHDSS7-21           A    10.4.7.21\nHDSS7-22           A    10.4.7.22\nHDSS7-200          A    10.4.7.200\n\n7-11 ~]# vi /var/named/od.com.zone\n$ORIGIN od.com.\n$TTL 600\t; 10 minutes\n@   \t\tIN SOA\tdns.od.com. dnsadmin.od.com. (\n\t\t\t\t2020011201 ; serial\n\t\t\t\t10800      ; refresh (3 hours)\n\t\t\t\t900        ; retry (15 minutes)\n\t\t\t\t604800     ; expire (1 week)\n\t\t\t\t86400      ; minimum (1 day)\n\t\t\t\t)\n\t\t\t\tNS   dns.od.com.\n$TTL 60\t; 1 minute\ndns                A    10.4.7.11\n\n# 看一下有没有报错\n7-11 ~]# named-checkconf\n7-11 ~]# systemctl start named\n7-11 ~]# systemctl enable named\n7-11 ~]# netstat -luntp|grep 53\n~~~\n\n> **TTL 600**：指定IP包被路由器丢弃之前允许通过的最大网段数量\n>\n> - **10 minutes**：过期时间10分钟\n>\n> **SOA**：一个域权威记录的相关信息，后面有5组参数分别设定了该域相关部分\n>\n> - **dnsadmin.od.com.**  一个假的邮箱\n> - **serial**：记录的时间\n>\n> **$ORIGIN**：即下列的域名自动补充od.com，如dns，外面看来是dns.od.com\n>\n> **netstat -luntp**：显示 tcp,udp 的端口和进程等相关情况\n\n![1578819148759](assets/1578819148759.png)\n\n~~~\n# 11机器，检查主机域是否解析\n7-11 ~]# dig -t A hdss7-21.host.com @10.4.7.11 +short\n# 配置linux客户端和win客户端都能使用这个服务，修改\n7-11 ~]# vi /etc/sysconfig/network-scripts/ifcfg-eth0\nDNS1=10.4.7.11\n7-11 ~]# systemctl restart network\n7-11 ~]# ping www.baidu.com\n7-11 ~]# ping hdss7-21.host.com\n~~~\n\n> **dig -t A**：指的是找DNS里标记为A的相关记录，而后面会带上相关的域，如上面的hdss7-21.host.com，为什么外面配了HDSS7-21后面还会自动接上.host.com就是因为$ORIGIN，后面则是对应的IP\n>\n> - **+short**：表示只返回IP\n\n![1578819653445](assets/1578819653445.png)\n\n~~~\n# 在所有机器添加search... ，即可使用短域名（我的是自带的）\n~]# vi /etc/resolv.conf\n~]# ping hdss7-200\n~~~\n\n![1578820746390](assets/1578820746390.png)\n\n~~~\n# 在非11机器上，全部改成11\n~]# vi /etc/sysconfig/network-scripts/ifcfg-eth0\nDNS1=10.4.7.11\n\n~]# systemctl restart network\n# 试下网络是否正常\n~]# ping baidu.com\n# 其它机器尝试ping7-11机器\n7-12 ~]# ping hdss7-11.host.com\n~~~\n\n> 让其它机器的DNS全部改成11机器的好处是，全部的机器访问外网就只有通过11端口，更好控制\n\n![1578820642420](assets/1578820642420.png)\n\n~~~\n# 修改window网络，并ping\n~~~\n\n![1578821485560](assets/1578821485560.png)\n\n![1578821522239](assets/1578821522239.png)\n\n~~~\n# ping不了的，修改以下配置\n~~~\n\n![1578821813984](assets/1578821813984.png)\n\n完成\n\n\n\n### K8S前置工作——准备签发证书环境\n\n> **WHAT**： 证书，可以用来审计也可以保障安全，k8S组件启动的时候，则需要有对应的证书，证书的详解你也可以在网上搜到，这里就不细细说明了\n>\n> **WHY**：当然是为了让我们的组件能正常运行\n\n~~~\n# cfssl方式做证书，需要三个软件，按照我们的架构图，我们部署在200机器:\n200 ~]# wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 -O /usr/bin/cfssl\n200 ~]# wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 -O /usr/bin/cfssl-json\n200 ~]# wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64 -O /usr/bin/cfssl-certinfo\n200 ~]# chmod +x /usr/bin/cfssl*\n200 ~]# which cfssl\n200 ~]# which cfssl-json\n200 ~]# which cfssl-certinfo\n~~~\n\n> **wget**：从网络上自动下载文件的自由工具\n>\n> **chmod**：给对应的文件添加权限（[菜鸟教程](https://www.runoob.com/linux/linux-comm-chmod.html)）\n>\n> - **+x**：给当前用户增加可执行该文件的权限\n>\n> **which**：查看相应的东西在哪里\n\n![1578822242531](assets/1578822242531.png)\n\n~~~\n200 ~]# cd /opt/\nopt]# mkdir certs\nopt]# cd certs/\ncerts]# vi ca-csr.json\n{\n    \"CN\": \"ben123123\",\n    \"hosts\": [\n    ],\n    \"key\": {\n        \"algo\": \"rsa\",\n        \"size\": 2048\n    },\n    \"names\": [\n        {\n            \"C\": \"CN\",\n            \"ST\": \"beijing\",\n            \"L\": \"beijing\",\n            \"O\": \"od\",\n            \"OU\": \"ops\"\n        }\n    ],\n    \"ca\": {\n        \"expiry\": \"1752000h\"\n    }\n}\ncerts]# cfssl gencert -initca ca-csr.json | cfssl-json -bare ca\ncerts]# cat ca.pem\n# 如果你这时候显示段错误或者json错误，就是之前克隆虚拟机的时候200机器地址和别的机器地址重叠冲突了，需要重建200虚拟机\n~~~\n\n> **cd **：切换当前工作目录到 dirName\n>\n> - 语法：cd [dirName]\n>\n> **mkdir**：建立名称为 dirName 之子目录\n>\n> - 语法：mkdir [-p] dirName\n> - **-p:** 确保目录存在，不存在则建一个，如mkdir empty/empty1/empty2\n>\n> **cfssl**：证书工具\n>\n> - gencert：生成的意思\n>\n> **ca-csr.json解析：**\n>\n> - CN：Common Name，浏览器使用该字段验证网址是否合法，一般写域名，非常重要\n> - ST：State，省\n> - L：Locality，地区\n> - O：Organization Name，组织名称\n> - OU：Organization Unit Name，组织单位名称\n>\n> <a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E5%8E%9F%E7%90%86%E5%8F%8A%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/Kubernetes%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5.md#pod%E4%B8%AD%E5%87%A0%E4%B8%AA%E9%87%8D%E8%A6%81%E5%AD%97%E6%AE%B5%E7%9A%84%E5%90%AB%E4%B9%89%E5%92%8C%E7%94%A8%E6%B3%95\">Pod中几个重要字段的含义和用法</a>\n\n![1578822598528](assets/1578822598528.png)\n\n\n\n### K8S前置工作——部署docker环境\n\n> **WHAT**：是一个开源的应用容器引擎，让开发者可以打包他们的应用以及依赖包到一个可移植的镜像中，然后发布到任何流行的 Linux或Windows 机器上，也可以实现虚拟化。\n>\n> **WHY**：Pod里面就是由数个docker容器组成，Pod是豌豆荚，docker容器是里面的豆子。\n\n~~~\n# 如我们架构图所示，运算节点是21/22机器（没有docker则无法运行pod），运维主机是200机器（没有docker则没办法下载docker存入私有仓库），所以在三台机器安装（21/22/200）\n~]# curl -fsSL https://get.docker.com | bash -s docker --mirror Aliyun\n# 上面的下载可能网络有问题，需要多试几次，这些部署我已经不同机器试过很多次了\n~]# mkdir -p /data/docker /etc/docker\n# # 注意，172.7.21.1，这里得21是指在hdss7-21得机器，如果是22得机器，就是172.7.22.1，共一处需要改机器名：\"bip\": \"172.7.21.1/24\"\n~]# vi /etc/docker/daemon.json\n{\n  \"data-root\": \"/data/docker\",\n  \"storage-driver\": \"overlay2\",\n  \"insecure-registries\": [\"registry.access.redhat.com\",\"quay.io\",\"harbor.od.com\"],\n  \"registry-mirrors\": [\"https://q2gr04ke.mirror.aliyuncs.com\"],\n  \"bip\": \"172.7.21.1/24\",\n  \"exec-opts\": [\"native.cgroupdriver=systemd\"],\n  \"live-restore\": true\n}\n\n~]# systemctl start docker\n~]# docker version\n~]# docker ps -a\n~~~\n\n> **mkdir -p：**前面有讲到过（上一级目录没有则创建），而这次后面带了两个目录，意思是同时创建两个目录\n>\n> **daemon.json：**为什么配aliyuncs的环境，是因为默认是连接到外网的，速度比较慢，所以我们可以直接使用国内的阿里云镜像源，当然还有腾讯云等\n>\n> **docker version：**查看docker的版本\n>\n> **daemon.json解析：**重点说一下这个为什么10.4.7.21机器对应着172.7.21.1/24，这里可以看到10的21对应得是172的21，这样做的好处就是，当你的pod出现问题时，你可以马上定位到是在哪台机器出现的问题，是21还是22还是其它的，这点在生产上非常重要，有时候你的dashboard（后面会安装）宕掉了，你没办法只能去机器找，而这时候你又找不到的时候，你老板会拿你祭天的\n\n![1578826534378](assets/1578826534378.png)\n\n\n\n### K8S前置工作——部署harbor仓库\n\n> **WHAT **：harbor仓库是可以部署到本地的私有仓库，也就是你可以把镜像推到这个仓库，然后需要用的时候再下载下来，这样的好处是：1、下载速度快，用到的时候能马上下载下来2、不用担心镜像改动或者下架等。\n>\n> **WHY**：因为我们的部署K8S涉及到很多镜像，制作相关包的时候如果网速问题会导致失败重来，而且我们在公司里也会建自己的仓库，所以必须按照harbor仓库\n\n~~~\n# 如架构图，我们安装在200机器：\n200 ~]# mkdir /opt/src  && cd /opt/src\n# 可以去这个地址下载，也可以直接用我用的软件包\nhttps://github.com/goharbor/harbor/releases/tag/v1.8.3\n200 src]# wget https://storage.googleapis.com/harbor-releases/release-1.8.0/harbor-offline-installer-v1.8.3.tgz\n7-200 src]# tar xf harbor-offline-installer-v1.8.3.tgz -C /opt/\n~~~\n\n> **tag**：可以加入，解开备份文件内的文件\n>\n> - **x**：解压\n> - **f**： 使用档案名字\n> - **-C**：切换到指定的目录\n> - 整条命令合起来就是，把tgz文件以tgz文件名为名字解压到opt目录下，并保存tgz文件原样\n>\n> **关于版本**：一般人都是喜欢用比较新的版本，我们当然也支持比较新的版本，但对于公司而已，稳定是最要紧的，v1.8.3是用的比较稳定的版本，而后续的各个组件也会有更加新的版本，你可以尝试新版本，但有些由于兼容问题不能用新的，后续那些不能用我会标记清楚。\n\n~~~\n# 200机器：\n200 src]# cd /opt/\n200 opt]# mv harbor/ harbor-v1.8.3\n200 opt]# ln -s /opt/harbor-v1.8.3/ /opt/harbor\n200 opt]# cd harbor\n200 harbor]# ll\n200 harbor]# vi harbor.yml\nhostname: harbor.od.com  # 原reg.mydomain.com\nhttp:\n  port: 180  # 原80\ndata_volume: /data/harbor\nlocation: /data/harbor/logs\n\n200 harbor]# mkdir -p /data/harbor/logs\n200 harbor]# yum install docker-compose -y\n200 harbor]# rpm -qa docker-compose\n# out: docker-compose-1.18.0-4.el7.noarch\n200 harbor]# ./install.sh\n~~~\n\n> **提示**：harbor v2.3.3版本安装也需要将https相关的配置注释掉\n>\n> 如图#https:\n>\n> ![1635775214057](assets/1635775214057.png)\n>\n> 感谢@https://github.com/xinzhuxiansheng\n>\n> **mv**：为文件或目录改名、或将文件或目录移入其它位置。\n>\n> - 这里的命令是有斜杠的，所以是移动到某个目录下\n>\n> **ln**：为某一个文件在另外一个位置建立一个同步的链接\n>\n> - `语法:ln [参数][源文件或目录][目标文件或目录]`\n> - `**-s：**软连接，可以对整个目录进行链接`\n>\n> **harbor.yml解析：**\n>\n> - port为什么改成180：因为后面我们要装nginx，nginx用的80，所以要把它们错开\n> - data_volume：数据卷，即docker镜像放在哪里\n> - location：日志文件\n> - **./install.sh**：启动shell脚本\n\n![1578830348081](assets/1578830348081.png)\n\n~~~\n# 200机器：\n200 harbor]# docker-compose ps\n200 harbor]# docker ps -a\n200 harbor]# yum install nginx -y\n\n###相关报错问题：\nyum的时候报：/var/run/yum.pid 已被锁定，PID 为 1610 的另一个程序正在运行。\n另外一个程序锁定了 yum；等待它退出……\n网上统一的解决办法：直接在终端运行 rm -f /var/run/yum.pid 将该文件删除，然后再次运行yum。\n###\n\n200 harbor]# vi /etc/nginx/conf.d/harbor.od.com.conf\nserver {\n    listen       80;\n    server_name  harbor.od.com;\n\n    client_max_body_size 1000m;\n\n    location / {\n        proxy_pass http://127.0.0.1:180;\n    }\n}\n\n200 harbor]# nginx -t\n200 harbor]# systemctl start nginx\n200 harbor]# systemctl enable nginx\n~~~\n\n> **nginx -t**：测试*nginx*.conf配置文件中是否存在语法错误\n>\n> **systemctl enable nginx**：开机自动启动\n\n~~~\n# 在11机器解析域名：\n~]# vi /var/named/od.com.zone\n# 注意serial前滚一个序号\n# 最下面添加域名\nharbor             A    10.4.7.200\n~]# systemctl restart named\n~]# dig -t A harbor.od.com +short\n# out:10.4.7.200\n~~~\n\n![1578830904937](assets/1578830904937.png)\n\n~~~\n# 200机器上curl：\nharbor]# curl harbor.od.com\n~~~\n\n> 注意：\n>\n> getenforce得是关闭状态，而不是enforcing，否则会报502\n> 暂时关闭setenforce 0\n> 永久关闭，改配置文件 vi /etc/selinux/config\n> SELINUX=disabled\n\n![1578831047079](assets/1578831047079.png)\n\n[访问harbo.od.com](harbor.od.com)\n\n![1578831134362](assets/1578831134362.png)\n\n~~~\n账号：admin\n密码：Harbor12345\n新建一个public公开项目\n~~~\n\n![1578831458247](assets/1578831458247.png)\n\n~~~\n# 200机器，尝试下是否能push成功到harbor仓库\nharbor]# docker pull nginx:1.7.9\nharbor]# docker images|grep 1.7.9\nharbor]# docker tag 84581e99d807 harbor.od.com/public/nginx:v1.7.9\nharbor]# docker login harbor.od.com\n账号：admin\n密码：Harbor12345\nharbor]# docker push harbor.od.com/public/nginx:v1.7.9\n# 报错：如果发现登录不上去了，过一阵子再登录即可，大约5分钟左右\n~~~\n\n![1578832524891](assets/1578832524891.png)\n\n成功，此时你已成功建立了自己的本地私有仓库\n\n\n\n### 安装部署主控节点服务etcd\n\n> 注意, 一定要同步每个虚拟机的时间, 时区和时间要保持一致, 可以通过以下的命令来操作, 最好所有的虚拟机都执行一次, 最好的方法是写进`/etc/rc.local`中\n\n  ```bash\n  timedatectl set-timezone Asia/Shanghai\n  timedatectl set-ntp true\n  ```\n\n> **WHAT**：一个高可用强一致性的服务发现存储仓库，关于服务发现，其本质就是知道了集群中是否有进程在监听udp和tcp端口（如上面部署的harbor就是监听180端口），并且通过名字就可以查找和连接。\n>\n> - **一个强一致性、高可用的服务存储目录**：基于Raft算法的etcd天生就是这样\n> - **一种注册服务和监控服务健康状态的机制**：在etcd中注册服务，并且对注册的服务设置`key TTL`（TTL上面有讲到），定时保持服务的心跳以达到监控健康状态的效果\n> - **一种查找和连接服务的机制**：通过在etcd指定的主题下注册的服务也能在对应的主题下查找到，为了确保连接，我们可以在每个服务机器上都部署一个Proxy模式的etcd，这样就可以确保能访问etcd集群的服务都能互相连接\n>\n> **WHY**：我们需要让服务快速透明地接入到计算集群中，让共享配置信息快速被集群中的所有机器发现\n\n看我们的结构图，可以看到我们在12/21/22机器都部署了etcd\n\n![1584701032598](assets/1584701032598.png)\n\n~~~\n# 我们开始制作证书，200机器：\ncerts]# cd /opt/certs/\ncerts]# vi /opt/certs/ca-config.json\n{\n    \"signing\": {\n        \"default\": {\n            \"expiry\": \"1752000h\"\n        },\n        \"profiles\": {\n            \"server\": {\n                \"expiry\": \"1752000h\",\n                \"usages\": [\n                    \"signing\",\n                    \"key encipherment\",\n                    \"server auth\"\n                ]\n            },\n            \"client\": {\n                \"expiry\": \"1752000h\",\n                \"usages\": [\n                    \"signing\",\n                    \"key encipherment\",\n                    \"client auth\"\n                ]\n            },\n            \"peer\": {\n                \"expiry\": \"1752000h\",\n                \"usages\": [\n                    \"signing\",\n                    \"key encipherment\",\n                    \"server auth\",\n                    \"client auth\"\n                ]\n            }\n        }\n    }\n}\n\ncerts]# vi etcd-peer-csr.json\n{\n    \"CN\": \"k8s-etcd\",\n    \"hosts\": [\n        \"10.4.7.11\",\n        \"10.4.7.12\",\n        \"10.4.7.21\",\n        \"10.4.7.22\"\n    ],\n    \"key\": {\n        \"algo\": \"rsa\",\n        \"size\": 2048\n    },\n    \"names\": [\n        {\n            \"C\": \"CN\",\n            \"ST\": \"beijing\",\n            \"L\": \"beijing\",\n            \"O\": \"od\",\n            \"OU\": \"ops\"\n        }\n    ]\n}\n\ncerts]# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=peer etcd-peer-csr.json |cfssl-json -bare etcd-peer\ncerts]# ll\n~~~\n\n> 关于这些json文件是怎么写出来的，答案：官网抄的然后修改，这些没什么重要的你也不需要太在意，后面重要的会说明是从哪里抄出来的\n>\n> **ca-config.json解析：**\n>\n> - expiry：有效期为200年\n> - profiles-server：启动server的时候需要配置证书\n> - profiles-client：client去连接server的时候需要证书\n> - profiles-peer：双向证书，服务端找客户端需要证书，客户端找服务端需要证书\n>\n> **etcd-peer-csr解析：**\n>\n> - hosts：etcd有可能部署到哪些组件的IP都要填进来\n>\n> **cfssl gencert**：生成证书\n\n![1578833436255](assets/1578833436255.png)\n\n~~~\n# 12/21/22机器，安装etcd：\n~]# mkdir /opt/src\n~]# cd /opt/src/\n# 创建用户\nsrc]# useradd -s /sbin/nologin -M etcd\nsrc]# id etcd\n\n# 到GitHub下载或者直接用我给得安装包 https://github.com/etcd-io/etcd/releases/tag/v3.1.20，百度云https://pan.baidu.com/s/1arE2LdtAbcR80gmIQtIELw 提取码：ouy1\nsrc]# wget https://github.com/etcd-io/etcd/releases/download/v3.1.20/etcd-v3.1.20-linux-amd64.tar.gz\nsrc]# tar xf etcd-v3.1.20-linux-amd64.tar.gz -C /opt\nsrc]# cd /opt\nopt]# mv etcd-v3.1.20-linux-amd64/ etcd-v3.1.20\nopt]# ln -s /opt/etcd-v3.1.20/ /opt/etcd\nopt]# cd etcd\n~~~\n\n> **tag**：可以加入，解开备份文件内的文件\n>\n> - **x**：解压\n> - **f** ：使用档案名字\n> - **-C**：切换到指定的目录\n> - 整条命令合起来就是，把tgz文件以tgz文件名为名字解压到opt目录下，并保存tgz文件原样\n>\n> **ln**：为某一个文件在另外一个位置建立一个同步的链接\n>\n> - `语法:ln [参数][源文件或目录][目标文件或目录]`\n> - **-s**：软连接，可以对整个目录进行链接\n>\n> **useradd**：建立用户帐号\n>\n> **-s**：指定用户登入后所使用的shell\n>\n> **-M**：不要自动建立用户的登入目录\n\n![1578833934325](assets/1578833934325.png)\n\n~~~\n# 12/21/22机器：\netcd]# mkdir -p /opt/etcd/certs /data/etcd /data/logs/etcd-server\netcd]# cd certs/\ncerts]# scp hdss7-200:/opt/certs/ca.pem .\n# 输入200虚机密码\ncerts]# scp hdss7-200:/opt/certs/etcd-peer.pem .\ncerts]# scp hdss7-200:/opt/certs/etcd-peer-key.pem .\ncerts]# cd ..\n# 注意，如果是21机器，这下面得12都得改成21，initial-cluster则是全部机器都有不需要改，一共5处：etcd-server-7-12、listen-peer-urls后、client-urls后、advertise-peer-urls后、advertise-client-urls后\netcd]# vi /opt/etcd/etcd-server-startup.sh\n#!/bin/sh\n./etcd --name etcd-server-7-12 \\\n       --data-dir /data/etcd/etcd-server \\\n       --listen-peer-urls https://10.4.7.12:2380 \\\n       --listen-client-urls https://10.4.7.12:2379,http://127.0.0.1:2379 \\\n       --quota-backend-bytes 8000000000 \\\n       --initial-advertise-peer-urls https://10.4.7.12:2380 \\\n       --advertise-client-urls https://10.4.7.12:2379,http://127.0.0.1:2379 \\\n       --initial-cluster  etcd-server-7-12=https://10.4.7.12:2380,etcd-server-7-21=https://10.4.7.21:2380,etcd-server-7-22=https://10.4.7.22:2380 \\\n       --ca-file ./certs/ca.pem \\\n       --cert-file ./certs/etcd-peer.pem \\\n       --key-file ./certs/etcd-peer-key.pem \\\n       --client-cert-auth  \\\n       --trusted-ca-file ./certs/ca.pem \\\n       --peer-ca-file ./certs/ca.pem \\\n       --peer-cert-file ./certs/etcd-peer.pem \\\n       --peer-key-file ./certs/etcd-peer-key.pem \\\n       --peer-client-cert-auth \\\n       --peer-trusted-ca-file ./certs/ca.pem \\\n       --log-output stdout\n\netcd]# chmod +x etcd-server-startup.sh\netcd]# chown -R etcd.etcd /opt/etcd-v3.1.20/\netcd]# chown -R etcd.etcd /data/etcd/\netcd]# chown -R etcd.etcd /data/logs/etcd-server/\netcd]# ll\n~~~\n\n> **scp**：用于 *Linux* 之间复制文件和目录\n>\n> **chmod**：添加权限\n>\n> - **+x**：给当前用户添加可执行该文件的权限权限\n>\n> **chown**：指定文件的拥有者改为指定的用户或组\n>\n> - **-R**：处理指定目录以及其子目录下的所有文件\n> - 这里即是把/opt/etcd...等的拥有者给etcd用户\n>\n> **ll**：列出权限、大小、名称等信息\n\n![1578834563843](assets/1578834563843.png)\n\n~~~\n# 12/21/22机器，我们同时需要supervisor（守护进程工具）来确保etcd是启动的，后面还会不断用到：\netcd]# yum install supervisor -y\netcd]# systemctl start supervisord\netcd]# systemctl enable supervisord\n# 注意修改下面得7-12，对应上机器，如21机器就是7-21，一共一处：[program:etcd-server-7-12]\netcd]# vi /etc/supervisord.d/etcd-server.ini\n[program:etcd-server-7-12]\ncommand=/opt/etcd/etcd-server-startup.sh                        ; the program (relative uses PATH, can take args)\nnumprocs=1                                                      ; number of processes copies to start (def 1)\ndirectory=/opt/etcd                                             ; directory to cwd to before exec (def no cwd)\nautostart=true                                                  ; start at supervisord start (default: true)\nautorestart=true                                                ; retstart at unexpected quit (default: true)\nstartsecs=30                                                    ; number of secs prog must stay running (def. 1)\nstartretries=3                                                  ; max # of serial start failures (default 3)\nexitcodes=0,2                                                   ; 'expected' exit codes for process (default 0,2)\nstopsignal=QUIT                                                 ; signal used to kill process (default TERM)\nstopwaitsecs=10                                                 ; max num secs to wait b4 SIGKILL (default 10)\nuser=etcd                                                       ; setuid to this UNIX account to run the program\nredirect_stderr=true                                            ; redirect proc stderr to stdout (default false)\nstdout_logfile=/data/logs/etcd-server/etcd.stdout.log           ; stdout log path, NONE for none; default AUTO\nstdout_logfile_maxbytes=64MB                                    ; max # logfile bytes b4 rotation (default 50MB)\nstdout_logfile_backups=4                                        ; # of stdout logfile backups (default 10)\nstdout_capture_maxbytes=1MB                                     ; number of bytes in 'capturemode' (default 0)\nstdout_events_enabled=false                                     ; emit events on stdout writes (default false)\n\netcd]# supervisorctl update\n# out：etcd-server-7-21: added process group\netcd]# supervisorctl status\n# out: etcd-server-7-12                 RUNNING   pid 16582, uptime 0:00:59\netcd]# netstat -luntp|grep etcd\n# 必须是监听了2379和2380这两个端口才算成功\n~~~\n\n> **systemctl enable**：开机启动\n>\n> **update**：更新\n>\n> **netstat -luntp：**查看端口和进程情况\n>\n> 现在你可以感觉到，supervisor守护进程也仅仅是你配好ini文件即可\n\n![1578834944148](assets/1578834944148.png)\n\n~~~\n# 任意节点（12/21/22）检测集群健康状态的两种方法\n22 etcd]# ./etcdctl cluster-health\n22 etcd]# ./etcdctl member list\n~~~\n\n![1578836465329](assets/1578836465329.png)\n\n> 这里你再哪个机器先update，哪个机器就是leader\n\n完成\n\n\n\n### 部署API-server集群\n\n[kubernetes官网](https://github.com/kubernetes/kubernetes)\n\n根据架构图，我们把运算节点部署在21和22机器\n\n![1584701070750](assets/1584701070750.png)\n\n~~~\n# 21/22机器\netcd]# cd /opt/src/\n# 可以去官网下载也可以用我的包，百度云盘https://pan.baidu.com/s/1arE2LdtAbcR80gmIQtIELw 提取码：ouy1\nsrc]# wget https://dl.k8s.io/v1.15.2/kubernetes-server-linux-amd64.tar.gz\nsrc]# mv kubernetes-server-linux-amd64.tar.gz kubernetes-server-linux-amd64-v1.15.2.tar.gz\n\nsrc]# tar xf kubernetes-server-linux-amd64-v1.15.2.tar.gz -C /opt/\nsrc]# cd /opt\nopt]# mv kubernetes/ kubernetes-v1.15.2\nopt]# ln -s /opt/kubernetes-v1.15.2/ /opt/kubernetes\nopt]# cd kubernetes\n# 删掉不需要的文件\nkubernetes]# rm -rf kubernetes-src.tar.gz\nkubernetes]# cd server/bin\nbin]# rm -f *.tar\nbin]# rm -f *_tag\nbin]# ll\n~~~\n\n> **tar xf -C**：解压到某个文件夹\n>\n> **mv**：移动到哪里\n>\n> **ln -s**：建立软连接\n>\n> **rm**：删除一个文件或者目录\n>\n> - **-r**：将目录及以下之档案亦逐一删除\n> - **-f**：直接删除，无需逐一确认（你可以试试先不加-f去删除）\n> - 加起来就是强制删除\n>\n> ***.tar**： 这里的*的意思是模糊法，即只要你的结尾是.tar的都匹配上加上rm，就是把所有.tar结尾的文件都删除\n\n![1578837147305](assets/1578837147305.png)\n\n~~~\n# 签发client证书，200机器：\n200 certs]# vi client-csr.json\n{\n    \"CN\": \"k8s-node\",\n    \"hosts\": [\n    ],\n    \"key\": {\n        \"algo\": \"rsa\",\n        \"size\": 2048\n    },\n    \"names\": [\n        {\n            \"C\": \"CN\",\n            \"ST\": \"beijing\",\n            \"L\": \"beijing\",\n            \"O\": \"od\",\n            \"OU\": \"ops\"\n        }\n    ]\n}\n\n200 certs]#  cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=client client-csr.json |cfssl-json -bare client\n200 certs]# ll\n~~~\n\n![1578837306831](assets/1578837306831.png)\n\n~~~\n# 给API-server做证书，200机器\n200 certs]# vi apiserver-csr.json\n{\n    \"CN\": \"k8s-apiserver\",\n    \"hosts\": [\n        \"127.0.0.1\",\n        \"192.168.0.1\",\n        \"kubernetes.default\",\n        \"kubernetes.default.svc\",\n        \"kubernetes.default.svc.cluster\",\n        \"kubernetes.default.svc.cluster.local\",\n        \"10.4.7.10\",\n        \"10.4.7.21\",\n        \"10.4.7.22\",\n        \"10.4.7.23\"\n    ],\n    \"key\": {\n        \"algo\": \"rsa\",\n        \"size\": 2048\n    },\n    \"names\": [\n        {\n            \"C\": \"CN\",\n            \"ST\": \"beijing\",\n            \"L\": \"beijing\",\n            \"O\": \"od\",\n            \"OU\": \"ops\"\n        }\n    ]\n}\n\n200 certs]# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=server apiserver-csr.json |cfssl-json -bare apiserver\n200 certs]# ll\n~~~\n\n![1578837423919](assets/1578837423919.png)\n\n~~~\n# 21/22机器：\ncd /opt/kubernetes/server/bin\nbin]# mkdir cert\nbin]# cd cert/\n# 把证书考过来\ncert]# scp hdss7-200:/opt/certs/ca.pem .\ncert]# scp hdss7-200:/opt/certs/ca-key.pem .\ncert]# scp hdss7-200:/opt/certs/client-key.pem .\ncert]# scp hdss7-200:/opt/certs/client.pem .\ncert]# scp hdss7-200:/opt/certs/apiserver.pem .\ncert]# scp hdss7-200:/opt/certs/apiserver-key.pem .\n~~~\n\n> **scp**：用于 *Linux* 之间复制文件和目录\n\n![1578837706422](assets/1578837706422.png)\n\n~~~\n# 21/22机器：\ncert]# ll\n# 共6个\n总用量 24\n-rw-------. 1 root root 1679 1月  12 22:01 apiserver-key.pem\n-rw-r--r--. 1 root root 1598 1月  12 22:01 apiserver.pem\n-rw-------. 1 root root 1679 1月  12 22:00 ca-key.pem\n-rw-r--r--. 1 root root 1346 1月  12 22:00 ca.pem\n-rw-------. 1 root root 1679 1月  12 22:01 client-key.pem\n-rw-r--r--. 1 root root 1363 1月  12 22:00 client.pem\n~~~\n\n~~~\n# 21/22机器：\ncd /opt/kubernetes/server/bin\nbin]# mkdir conf\nbin]# cd conf/\nconf]# vi audit.yaml\napiVersion: audit.k8s.io/v1beta1 # This is required.\nkind: Policy\n# Don't generate audit events for all requests in RequestReceived stage.\nomitStages:\n  - \"RequestReceived\"\nrules:\n  # Log pod changes at RequestResponse level\n  - level: RequestResponse\n    resources:\n    - group: \"\"\n      # Resource \"pods\" doesn't match requests to any subresource of pods,\n      # which is consistent with the RBAC policy.\n      resources: [\"pods\"]\n  # Log \"pods/log\", \"pods/status\" at Metadata level\n  - level: Metadata\n    resources:\n    - group: \"\"\n      resources: [\"pods/log\", \"pods/status\"]\n\n  # Don't log requests to a configmap called \"controller-leader\"\n  - level: None\n    resources:\n    - group: \"\"\n      resources: [\"configmaps\"]\n      resourceNames: [\"controller-leader\"]\n\n  # Don't log watch requests by the \"system:kube-proxy\" on endpoints or services\n  - level: None\n    users: [\"system:kube-proxy\"]\n    verbs: [\"watch\"]\n    resources:\n    - group: \"\" # core API group\n      resources: [\"endpoints\", \"services\"]\n\n  # Don't log authenticated requests to certain non-resource URL paths.\n  - level: None\n    userGroups: [\"system:authenticated\"]\n    nonResourceURLs:\n    - \"/api*\" # Wildcard matching.\n    - \"/version\"\n\n  # Log the request body of configmap changes in kube-system.\n  - level: Request\n    resources:\n    - group: \"\" # core API group\n      resources: [\"configmaps\"]\n    # This rule only applies to resources in the \"kube-system\" namespace.\n    # The empty string \"\" can be used to select non-namespaced resources.\n    namespaces: [\"kube-system\"]\n\n  # Log configmap and secret changes in all other namespaces at the Metadata level.\n  - level: Metadata\n    resources:\n    - group: \"\" # core API group\n      resources: [\"secrets\", \"configmaps\"]\n\n  # Log all other resources in core and extensions at the Request level.\n  - level: Request\n    resources:\n    - group: \"\" # core API group\n    - group: \"extensions\" # Version of group should NOT be included.\n\n  # A catch-all rule to log all other requests at the Metadata level.\n  - level: Metadata\n    # Long-running requests like watches that fall under this rule will not\n    # generate an audit event in RequestReceived.\n    omitStages:\n      - \"RequestReceived\"\n      \nconf]# cd ..\nbin]# vi /opt/kubernetes/server/bin/kube-apiserver.sh\n#!/bin/bash\n./kube-apiserver \\\n  --apiserver-count 2 \\\n  --audit-log-path /data/logs/kubernetes/kube-apiserver/audit-log \\\n  --audit-policy-file ./conf/audit.yaml \\\n  --authorization-mode RBAC \\\n  --client-ca-file ./cert/ca.pem \\\n  --requestheader-client-ca-file ./cert/ca.pem \\\n  --enable-admission-plugins NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota \\\n  --etcd-cafile ./cert/ca.pem \\\n  --etcd-certfile ./cert/client.pem \\\n  --etcd-keyfile ./cert/client-key.pem \\\n  --etcd-servers https://10.4.7.12:2379,https://10.4.7.21:2379,https://10.4.7.22:2379 \\\n  --service-account-key-file ./cert/ca-key.pem \\\n  --service-cluster-ip-range 192.168.0.0/16 \\\n  --service-node-port-range 3000-29999 \\\n  --target-ram-mb=1024 \\\n  --kubelet-client-certificate ./cert/client.pem \\\n  --kubelet-client-key ./cert/client-key.pem \\\n  --log-dir  /data/logs/kubernetes/kube-apiserver \\\n  --tls-cert-file ./cert/apiserver.pem \\\n  --tls-private-key-file ./cert/apiserver-key.pem \\\n  --v 2\n  \nbin]# chmod +x kube-apiserver.sh\n# 一处修改：[program:kube-apiserver-7-21]\nbin]# vi /etc/supervisord.d/kube-apiserver.ini\n[program:kube-apiserver-7-21]\ncommand=/opt/kubernetes/server/bin/kube-apiserver.sh            ; the program (relative uses PATH, can take args)\nnumprocs=1                                                      ; number of processes copies to start (def 1)\ndirectory=/opt/kubernetes/server/bin                            ; directory to cwd to before exec (def no cwd)\nautostart=true                                                  ; start at supervisord start (default: true)\nautorestart=true                                                ; retstart at unexpected quit (default: true)\nstartsecs=30                                                    ; number of secs prog must stay running (def. 1)\nstartretries=3                                                  ; max # of serial start failures (default 3)\nexitcodes=0,2                                                   ; 'expected' exit codes for process (default 0,2)\nstopsignal=QUIT                                                 ; signal used to kill process (default TERM)\nstopwaitsecs=10                                                 ; max num secs to wait b4 SIGKILL (default 10)\nuser=root                                                       ; setuid to this UNIX account to run the program\nredirect_stderr=true                                            ; redirect proc stderr to stdout (default false)\nstdout_logfile=/data/logs/kubernetes/kube-apiserver/apiserver.stdout.log        ; stderr log path, NONE for none; default AUTO\nstdout_logfile_maxbytes=64MB                                    ; max # logfile bytes b4 rotation (default 50MB)\nstdout_logfile_backups=4                                        ; # of stdout logfile backups (default 10)\nstdout_capture_maxbytes=1MB                                     ; number of bytes in 'capturemode' (default 0)\nstdout_events_enabled=false                                     ; emit events on stdout writes (default false)\n\nbin]# mkdir -p /data/logs/kubernetes/kube-apiserver\nbin]# supervisorctl update\n# 查看21/22两台机器是否跑起来了，可能比较慢在starting，等10秒\nbin]# supervisorctl status\n~~~\n\n> **mkdir -p**：创建目录，没有上一级目录则创建\n>\n> **supervisorctl update**：更新supervisorctl\n>\n> **audit.yaml解析：**\n>\n> - 可以参考这篇文章[点击跳转](https://www.baidu.com/link?url=tFECOG31lKlcqDWeAZGF1VyjhzVAN9vUKHKEKKw5G8y0AC8MKpJxSZeL647MIFdw&wd=&eqid=dafe84b80019e4a3000000065e51d2e2)，当然这里的audit.yaml可能会有些不一样，但是我们后面用到的yaml文件就很相似了\n\n![1578839161494](assets/1578839161494.png)\n\n> <a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E5%8E%9F%E7%90%86%E5%8F%8A%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/Kubernetes%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5.md#pod%E7%9A%84%E5%87%A0%E7%A7%8D%E7%8A%B6%E6%80%81\">Pod的几种状态</a>\n\n完成\n\n\n\n### 安装部署主控节点L4反代服务\n\n根据我们架构图，在11/12机器上做反代\n\n![1584701103579](assets/1584701103579.png)\n\n安装nginx时另一个注意事项 <a href=\"https://github.com/ben1234560/k8s_PaaS/issues/16\">点击链接  </a>\n\n（感谢 https://github.com/nangongchengfeng/）\n\n~~~\n# 11/12机器\n~]# yum install nginx nginx-mod-stream -y\n# 添加在最下面\n~]# vi /etc/nginx/nginx.conf\nstream {\n    upstream kube-apiserver {\n        server 10.4.7.21:6443     max_fails=3 fail_timeout=30s;\n        server 10.4.7.22:6443     max_fails=3 fail_timeout=30s;\n    }\n    server {\n        listen 7443;\n        proxy_connect_timeout 2s;\n        proxy_timeout 900s;\n        proxy_pass kube-apiserver;\n    }\n}\n\n~]# nginx -t\n~]# systemctl start nginx\n~]# systemctl enable nginx\n~]# yum install keepalived -y\n# keepalived 监控端口脚本\n~]# vi /etc/keepalived/check_port.sh\n#!/bin/bash\nCHK_PORT=$1\nif [ -n \"$CHK_PORT\" ];then\n        PORT_PROCESS=`ss -lnt|grep $CHK_PORT|wc -l`\n        if [ $PORT_PROCESS -eq 0 ];then\n                echo \"Port $CHK_PORT Is Not Used,End.\"\n                exit 1\n        fi\nelse\n        echo \"Check Port Cant Be Empty!\"\nfi\n\n~]# chmod +x /etc/keepalived/check_port.sh\n~~~\n\n> 由于7443端口未监听，Nginx 启动报 [emerg] bind() failed的可以参考[这个方法](https://blog.csdn.net/RunSnail2018/article/details/81185138)（感谢https://gitee.com/wangming91/）\n>\n> **yum install -y**：安装并自动yes\n>\n> **nginx -t**：确定nginx.conf有没有语法错误\n>\n> **systemctl start**：启动服务\n>\n> **systemctl enable**：开机自启\n\n~~~\n# 仅以下分主从操作：\n# 把原有内容都删掉，命令行快速按打出dG\n# 注意，下面的vrrp_instance下的interface，我的机器是eth0配置了网卡，有的版本是ens33配置网卡，可以用ifconfig查看，第一行就是，如果你是ens33，改这个interface ens33\n# keepalived 主（即11机器）:\n11 ~]# vi /etc/keepalived/keepalived.conf\n! Configuration File for keepalived\n\nglobal_defs {\n   router_id 10.4.7.11\n\n}\n\nvrrp_script chk_nginx {\n    script \"/etc/keepalived/check_port.sh 7443\"\n    interval 2\n    weight -20\n}\n\nvrrp_instance VI_1 {\n    state MASTER\n    interface eth0\n    virtual_router_id 251\n    priority 100\n    advert_int 1\n    mcast_src_ip 10.4.7.11\n    nopreempt\n\n    authentication {\n        auth_type PASS\n        auth_pass 11111111\n    }\n    track_script {\n         chk_nginx\n    }\n    virtual_ipaddress {\n        10.4.7.10\n    }\n}\n\nkeepalived从（即12机器）:\n12 ~]# vi /etc/keepalived/keepalived.conf\n! Configuration File for keepalived\nglobal_defs {\n\trouter_id 10.4.7.12\n}\nvrrp_script chk_nginx {\n\tscript \"/etc/keepalived/check_port.sh 7443\"\n\tinterval 2\n\tweight -20\n}\nvrrp_instance VI_1 {\n\tstate BACKUP\n\tinterface eth0\n\tvirtual_router_id 251\n\tmcast_src_ip 10.4.7.12\n\tpriority 90\n\tadvert_int 1\n\tauthentication {\n\t\tauth_type PASS\n\t\tauth_pass 11111111\n\t}\n\ttrack_script {\n\t\tchk_nginx\n\t}\n\tvirtual_ipaddress {\n\t\t10.4.7.10\n\t}\n}\n~~~\n\n\n\n~~~\n# 11/12机器\n~]# systemctl start keepalived\n~]# systemctl enable keepalived\n# 在11机器\n11 ~]# ip add\n~~~\n\n![1578840833339](assets/1578840833339.png)\n\n#### 小实验（可不做）：\n\n~~~\n# 实验(可不做)：在11机器关掉nginx\n11 ~]# nginx -s stop\n11 ~]# netstat -luntp|grep 7443\n# 代理会跑到12机器\n~~~\n\n![1578841077321](assets/1578841077321.png)\n\n~~~\n11 ~]# nginx\n11 ~]# netstat -luntp|grep 744\n# 再起来，但也不会跑回来，因为我们配置了\n~~~\n\n![1578841192980](assets/1578841192980.png)\n\n~~~\n# 11/12机器执行：\n~]# systemctl restart keepalived\n# 11机器：\n~]# ip add\n~~~\n\n> 生产中，人工确定机器没问题了，再手动回来\n\n![1578841301006](assets/1578841301006.png)\n\n完成\n\n\n\n### 安装部署controller-manager（节点控制器/调度器服务）\n\n让我们再搬出我们的架构图\n\n![1584701137068](assets/1584701137068.png)\n\n~~~\n# 21/22机器：\nbin]# vi /opt/kubernetes/server/bin/kube-controller-manager.sh\n#!/bin/sh\n./kube-controller-manager \\\n  --cluster-cidr 172.7.0.0/16 \\\n  --leader-elect true \\\n  --log-dir /data/logs/kubernetes/kube-controller-manager \\\n  --master http://127.0.0.1:8080 \\\n  --service-account-private-key-file ./cert/ca-key.pem \\\n  --service-cluster-ip-range 192.168.0.0/16 \\\n  --root-ca-file ./cert/ca.pem \\\n  --v 2\n\nbin]# chmod +x /opt/kubernetes/server/bin/kube-controller-manager.sh\nbin]# mkdir -p /data/logs/kubernetes/kube-controller-manager\n# 注意22机器，下面要改成7-22，一处修改：manager-7-21]\nbin]# vi /etc/supervisord.d/kube-conntroller-manager.ini\n[program:kube-controller-manager-7-21]\ncommand=/opt/kubernetes/server/bin/kube-controller-manager.sh                     ; the program (relative uses PATH, can take args)\nnumprocs=1                                                                        ; number of processes copies to start (def 1)\ndirectory=/opt/kubernetes/server/bin                                              ; directory to cwd to before exec (def no cwd)\nautostart=true                                                                    ; start at supervisord start (default: true)\nautorestart=true                                                                  ; retstart at unexpected quit (default: true)\nstartsecs=30                                                                      ; number of secs prog must stay running (def. 1)\nstartretries=3                                                                    ; max # of serial start failures (default 3)\nexitcodes=0,2                                                                     ; 'expected' exit codes for process (default 0,2)\nstopsignal=QUIT                                                                   ; signal used to kill process (default TERM)\nstopwaitsecs=10                                                                   ; max num secs to wait b4 SIGKILL (default 10)\nuser=root                                                                         ; setuid to this UNIX account to run the program\nredirect_stderr=true                                                              ; redirect proc stderr to stdout (default false)\nstdout_logfile=/data/logs/kubernetes/kube-controller-manager/controller.stdout.log  ; stderr log path, NONE for none; default AUTO\nstdout_logfile_maxbytes=64MB                                                      ; max # logfile bytes b4 rotation (default 50MB)\nstdout_logfile_backups=4                                                          ; # of stdout logfile backups (default 10)\nstdout_capture_maxbytes=1MB                                                       ; number of bytes in 'capturemode' (default 0)\nstdout_events_enabled=false                                                       ; emit events on stdout writes (default false)\n\nbin]# supervisorctl update\nbin]# vi /opt/kubernetes/server/bin/kube-scheduler.sh\n#!/bin/sh\n./kube-scheduler \\\n  --leader-elect  \\\n  --log-dir /data/logs/kubernetes/kube-scheduler \\\n  --master http://127.0.0.1:8080 \\\n  --v 2\n  \nbin]# chmod +x /opt/kubernetes/server/bin/kube-scheduler.sh\nbin]# mkdir -p /data/logs/kubernetes/kube-scheduler\n# 注意改机器号，一处修改：scheduler-7-21]\nbin]# vi /etc/supervisord.d/kube-scheduler.ini\n[program:kube-scheduler-7-21]\ncommand=/opt/kubernetes/server/bin/kube-scheduler.sh                     ; the program (relative uses PATH, can take args)\nnumprocs=1                                                               ; number of processes copies to start (def 1)\ndirectory=/opt/kubernetes/server/bin                                     ; directory to cwd to before exec (def no cwd)\nautostart=true                                                           ; start at supervisord start (default: true)\nautorestart=true                                                         ; retstart at unexpected quit (default: true)\nstartsecs=30                                                             ; number of secs prog must stay running (def. 1)\nstartretries=3                                                           ; max # of serial start failures (default 3)\nexitcodes=0,2                                                            ; 'expected' exit codes for process (default 0,2)\nstopsignal=QUIT                                                          ; signal used to kill process (default TERM)\nstopwaitsecs=10                                                          ; max num secs to wait b4 SIGKILL (default 10)\nuser=root                                                                ; setuid to this UNIX account to run the program\nredirect_stderr=true                                                     ; redirect proc stderr to stdout (default false)\nstdout_logfile=/data/logs/kubernetes/kube-scheduler/scheduler.stdout.log ; stderr log path, NONE for none; default AUTO\nstdout_logfile_maxbytes=64MB                                             ; max # logfile bytes b4 rotation (default 50MB)\nstdout_logfile_backups=4                                                 ; # of stdout logfile backups (default 10)\nstdout_capture_maxbytes=1MB                                              ; number of bytes in 'capturemode' (default 0)\nstdout_events_enabled=false                                              ; emit events on stdout writes (default false)\n\nbin]# supervisorctl update\nbin]# supervisorctl status\n# 起来了4个\nbin]# ln -s /opt/kubernetes/server/bin/kubectl /usr/bin/kubectl\n# 查看集群健康情况，21/22机器：\nbin]# kubectl get cs\n~~~\n\n> **ln -s**：建立软链接\n>\n> **supervisorctl status**：查看supervisor的情况\n>\n> **supervisorctl update**：更新supervisor\n>\n> **kubectl get**：获取列出一个或多个资源的信息\n>\n> - 上面一条命令的意思是：    列出所有cs信息\n\n![1578842317467](assets/1578842317467.png)\n\n完成\n\n\n\n### 安装部署运算节点服务（kubelet）\n\n~~~\n# 签发证书，200机器\n200 certs]# vi kubelet-csr.json\n{\n    \"CN\": \"k8s-kubelet\",\n    \"hosts\": [\n    \"127.0.0.1\",\n    \"10.4.7.10\",\n    \"10.4.7.21\",\n    \"10.4.7.22\",\n    \"10.4.7.23\",\n    \"10.4.7.24\",\n    \"10.4.7.25\",\n    \"10.4.7.26\",\n    \"10.4.7.27\",\n    \"10.4.7.28\"\n    ],\n    \"key\": {\n        \"algo\": \"rsa\",\n        \"size\": 2048\n    },\n    \"names\": [\n        {\n            \"C\": \"CN\",\n            \"ST\": \"beijing\",\n            \"L\": \"beijing\",\n            \"O\": \"od\",\n            \"OU\": \"ops\"\n        }\n    ]\n}\n\n200 certs]# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=server kubelet-csr.json | cfssl-json -bare kubelet\n200 certs]# ll\n~~~\n\n> **kubelet-csr-hosts**：把所有可能用到的IP都放进来\n\n![1580436431973](assets/1580436431973.png)\n\n~~~\n# 分发证书，21/22机器\ncert]# cd /opt/kubernetes/server/bin/cert/\ncert]# scp hdss7-200:/opt/certs/kubelet.pem .\ncert]# scp hdss7-200:/opt/certs/kubelet-key.pem .\n\n# 21机器：\ncert]# cd ../conf/\nconf]# kubectl config set-cluster myk8s \\\n  --certificate-authority=/opt/kubernetes/server/bin/cert/ca.pem \\\n  --embed-certs=true \\\n  --server=https://10.4.7.10:7443 \\\n  --kubeconfig=kubelet.kubeconfig\n\nconf]# kubectl config set-credentials k8s-node \\\n  --client-certificate=/opt/kubernetes/server/bin/cert/client.pem \\\n  --client-key=/opt/kubernetes/server/bin/cert/client-key.pem \\\n  --embed-certs=true \\\n  --kubeconfig=kubelet.kubeconfig \n\nconf]# kubectl config set-context myk8s-context \\\n  --cluster=myk8s \\\n  --user=k8s-node \\\n  --kubeconfig=kubelet.kubeconfig\n\nconf]# kubectl config use-context myk8s-context --kubeconfig=kubelet.kubeconfig\n#out: Switched to context \"myk8s-context\".\n# 做权限授权，推荐文章https://www.jianshu.com/p/9991f189495f\nconf]# vi k8s-node.yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: k8s-node\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: system:node\nsubjects:\n- apiGroup: rbac.authorization.k8s.io\n  kind: User\n  name: k8s-node\n  \nconf]# kubectl create -f k8s-node.yaml\nconf]# kubectl get clusterrolebinding k8s-node -o yaml\n\n# 22机器，复制21机器即可\ncert]# cd ../conf/\nconf]# scp hdss7-21:/opt/kubernetes/server/bin/conf/kubelet.kubeconfig .\n~~~\n\n> **scp**：用于 *Linux* 之间复制文件和目录\n>\n> **kubectl create -f**  ：通过配置文件名或stdin创建一个集群资源对象\n>\n> **kubectl get ... -o ...**  ：列出Pod以及运行Pod节点信息（你可以试下不加-o和加-o的区别）\n\n![1580436621354](assets/1580436621354.png)\n\n~~~\n# 准备pause基础镜像，200机器：\ncerts]# docker pull kubernetes/pause\ncerts]# docker images|grep pause\ncerts]# docker tag f9d5de079539 harbor.od.com/public/pause:latest\ncerts]# docker push harbor.od.com/public/pause:latest\n~~~\n\n> **docker pull**：下载镜像\n>\n> **docker images**：列出所有的镜像\n>\n> - 这里加上|grep 管道符是为了过滤出来pause镜像\n>\n> **docker tag**：给镜像打名字\n>\n> **docker push**：将镜像推送到指定的仓库\n\n~~~\n# 21/21机器，注意修改主机名，有一处需要改：hdss7-21\nbin]# vi /opt/kubernetes/server/bin/kubelet.sh\n#!/bin/sh\n./kubelet \\\n  --anonymous-auth=false \\\n  --cgroup-driver systemd \\\n  --cluster-dns 192.168.0.2 \\\n  --cluster-domain cluster.local \\\n  --runtime-cgroups=/systemd/system.slice \\\n  --kubelet-cgroups=/systemd/system.slice \\\n  --fail-swap-on=\"false\" \\\n  --client-ca-file ./cert/ca.pem \\\n  --tls-cert-file ./cert/kubelet.pem \\\n  --tls-private-key-file ./cert/kubelet-key.pem \\\n  --hostname-override hdss7-21.host.com \\\n  --image-gc-high-threshold 20 \\\n  --image-gc-low-threshold 10 \\\n  --kubeconfig ./conf/kubelet.kubeconfig \\\n  --log-dir /data/logs/kubernetes/kube-kubelet \\\n  --pod-infra-container-image harbor.od.com/public/pause:latest \\\n  --root-dir /data/kubelet\n  \nconf]# cd /opt/kubernetes/server/bin\nbin]# mkdir -p /data/logs/kubernetes/kube-kubelet /data/kubelet\nbin]# chmod +x kubelet.sh\n# 有一处要修改：kube-kubelet-7-21]\nbin]# vi /etc/supervisord.d/kube-kubelet.ini\n[program:kube-kubelet-7-21]\ncommand=/opt/kubernetes/server/bin/kubelet.sh     ; the program (relative uses PATH, can take args)\nnumprocs=1                                        ; number of processes copies to start (def 1)\ndirectory=/opt/kubernetes/server/bin              ; directory to cwd to before exec (def no cwd)\nautostart=true                                    ; start at supervisord start (default: true)\nautorestart=true              \t\t          ; retstart at unexpected quit (default: true)\nstartsecs=30                                      ; number of secs prog must stay running (def. 1)\nstartretries=3                                    ; max # of serial start failures (default 3)\nexitcodes=0,2                                     ; 'expected' exit codes for process (default 0,2)\nstopsignal=QUIT                                   ; signal used to kill process (default TERM)\nstopwaitsecs=10                                   ; max num secs to wait b4 SIGKILL (default 10)\nuser=root                                         ; setuid to this UNIX account to run the program\nredirect_stderr=true                              ; redirect proc stderr to stdout (default false)\nstdout_logfile=/data/logs/kubernetes/kube-kubelet/kubelet.stdout.log   ; stderr log path, NONE for none; default AUTO\nstdout_logfile_maxbytes=64MB                      ; max # logfile bytes b4 rotation (default 50MB)\nstdout_logfile_backups=4                          ; # of stdout logfile backups (default 10)\nstdout_capture_maxbytes=1MB                       ; number of bytes in 'capturemode' (default 0)\nstdout_events_enabled=false                       ; emit events on stdout writes (default false)\n\nbin]# supervisorctl update\nbin]# supervisorctl status\nbin]# kubectl get nodes\n# 给标签,21/22都给上master,node\nbin]# kubectl label node hdss7-21.host.com node-role.kubernetes.io/master=\nbin]# kubectl label node hdss7-21.host.com node-role.kubernetes.io/node=\nbin]# kubectl label node hdss7-22.host.com node-role.kubernetes.io/master=\nbin]# kubectl label node hdss7-22.host.com node-role.kubernetes.io/node=\nbin]# kubectl get nodes\n~~~\n\n![1579071142869](assets/1579071142869.png)\n\n完成\n\n\n\n### 安装部署运算节点服务（kube-proxy）\n\n~~~\n# 签发证书请求文件，200：\n200 certs]# vi kube-proxy-csr.json\n{\n    \"CN\": \"system:kube-proxy\",\n    \"key\": {\n        \"algo\": \"rsa\",\n        \"size\": 2048\n    },\n    \"names\": [\n        {\n            \"C\": \"CN\",\n            \"ST\": \"beijing\",\n            \"L\": \"beijing\",\n            \"O\": \"od\",\n            \"OU\": \"ops\"\n        }\n    ]\n}\n\n200 certs]# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=client kube-proxy-csr.json |cfssl-json -bare kube-proxy-client\n~~~\n\n![1580437266613](assets/1580437266613.png)\n\n~~~\n# 分发证书，21/22机器：\ncd /opt/kubernetes/server/bin/cert\ncert]# scp hdss7-200:/opt/certs/kube-proxy-client.pem .\ncert]# scp hdss7-200:/opt/certs/kube-proxy-client-key.pem .\ncd ../conf/\n# 21机器：\nconf]# kubectl config set-cluster myk8s \\\n  --certificate-authority=/opt/kubernetes/server/bin/cert/ca.pem \\\n  --embed-certs=true \\\n  --server=https://10.4.7.10:7443 \\\n  --kubeconfig=kube-proxy.kubeconfig\n\nconf]# kubectl config set-credentials kube-proxy \\\n  --client-certificate=/opt/kubernetes/server/bin/cert/kube-proxy-client.pem \\\n  --client-key=/opt/kubernetes/server/bin/cert/kube-proxy-client-key.pem \\\n  --embed-certs=true \\\n  --kubeconfig=kube-proxy.kubeconfig\n\nconf]# kubectl config set-context myk8s-context \\\n  --cluster=myk8s \\\n  --user=kube-proxy \\\n  --kubeconfig=kube-proxy.kubeconfig\n\nconf]# kubectl config use-context myk8s-context --kubeconfig=kube-proxy.kubeconfig\n\n# 22机器\nconf]# scp hdss7-21:/opt/kubernetes/server/bin/conf/kube-proxy.kubeconfig .\n\n\n# 21/22机器：\ncd\n~]# lsmod |grep ip_vs\n~]# vi ipvs.sh\n#!/bin/bash\nipvs_mods_dir=\"/usr/lib/modules/$(uname -r)/kernel/net/netfilter/ipvs\"\nfor i in $(ls $ipvs_mods_dir|grep -o \"^[^.]*\")\ndo\n  /sbin/modinfo -F filename $i &>/dev/null\n  if [ $? -eq 0 ];then\n    /sbin/modprobe $i\n  fi\ndone\n\n~]# chmod +x ipvs.sh\n~]# ./ipvs.sh\n~]# lsmod |grep ip_vs\n~~~\n\n> **lsmod**：显示已载入系统的模块\n>\n> - 后面带的管道符则是过滤出ip_vs来\n>\n> **chmod +x**：给文件添加执行权限\n>\n> **./ipvs.sh**：运行文件\n\n![1578846381651](assets/1578846381651.png)\n\n~~~\n# 21/22机器：\n~]#cd /opt/kubernetes/server/bin/\n# 注意修改对应的机器ip，有一处修改：hdss7-21\nbin]# vi /opt/kubernetes/server/bin/kube-proxy.sh\n#!/bin/sh\n./kube-proxy \\\n  --cluster-cidr 172.7.0.0/16 \\\n  --hostname-override hdss7-21.host.com \\\n  --proxy-mode=ipvs \\\n  --ipvs-scheduler=nq \\\n  --kubeconfig ./conf/kube-proxy.kubeconfig\n  \nbin]# chmod +x kube-proxy.sh\nbin]# mkdir -p /data/logs/kubernetes/kube-proxy\n# 注意机器IP，有一处修改：kube-proxy-7-21]\nbin]# vi /etc/supervisord.d/kube-proxy.ini\n[program:kube-proxy-7-21]\ncommand=/opt/kubernetes/server/bin/kube-proxy.sh                     ; the program (relative uses PATH, can take args)\nnumprocs=1                                                           ; number of processes copies to start (def 1)\ndirectory=/opt/kubernetes/server/bin                                 ; directory to cwd to before exec (def no cwd)\nautostart=true                                                       ; start at supervisord start (default: true)\nautorestart=true                                                     ; retstart at unexpected quit (default: true)\nstartsecs=30                                                         ; number of secs prog must stay running (def. 1)\nstartretries=3                                                       ; max # of serial start failures (default 3)\nexitcodes=0,2                                                        ; 'expected' exit codes for process (default 0,2)\nstopsignal=QUIT                                                      ; signal used to kill process (default TERM)\nstopwaitsecs=10                                                      ; max num secs to wait b4 SIGKILL (default 10)\nuser=root                                                            ; setuid to this UNIX account to run the program\nredirect_stderr=true                                                 ; redirect proc stderr to stdout (default false)\nstdout_logfile=/data/logs/kubernetes/kube-proxy/proxy.stdout.log     ; stderr log path, NONE for none; default AUTO\nstdout_logfile_maxbytes=64MB                                         ; max # logfile bytes b4 rotation (default 50MB)\nstdout_logfile_backups=4                                             ; # of stdout logfile backups (default 10)\nstdout_capture_maxbytes=1MB                                          ; number of bytes in 'capturemode' (default 0)\nstdout_events_enabled=false                                          ; emit events on stdout writes (default false)\n\nbin]# supervisorctl update\nbin]# yum install ipvsadm -y\nbin]# ipvsadm -Ln\nbin]# kubectl get svc\n~~~\n\n> **ipvsadm**：用于设置、维护和检查Linux内核中虚拟服务器列表的*命令*\n>\n> **ipvsadm -Ln** ：查看当前配置的虚拟服务和各个RS的权重\n\n![1578846997119](assets/1578846997119.png)\n\n~~~\n# 验证一下集群，21机器(在任意节点机器，我选的是21)：\ncd\n~]# vi /root/nginx-ds.yaml\napiVersion: extensions/v1beta1\nkind: DaemonSet\nmetadata:\n  name: nginx-ds\nspec:\n  template:\n    metadata:\n      labels:\n        app: nginx-ds\n    spec:\n      containers:\n      - name: my-nginx\n        image: harbor.od.com/public/nginx:v1.7.9\n        ports:\n        - containerPort: 80\n        \n~]# kubectl create -f nginx-ds.yaml\n# out：daemonset.extensions/nginx-ds created\n~]# kubectl get pods -o wide\n# 以下是成功的状态，在21/22机器都可以查到\n~~~\n\n> **注意，如果你的pod的wide22的也在21上或者类似情况，那就是你的vi /etc/docker/daemon.json没改好机器名，需要重新做过全部机器**\n>\n> **kubectl create -f**  ：通过配置文件名或stdin创建一个集群资源对象\n>\n> **kubectl get ... -o wide**：显示网络情况\n>\n> **nginx-ds.yaml解析：**\n>\n> - 可以参考这篇文章[点击跳转](https://www.baidu.com/link?url=tFECOG31lKlcqDWeAZGF1VyjhzVAN9vUKHKEKKw5G8y0AC8MKpJxSZeL647MIFdw&wd=&eqid=dafe84b80019e4a3000000065e51d2e2)，这里的nginx-ds.yaml建议自己手敲一遍，敲的同时要知道自己敲的是什么，记住，yaml语法不允许使用Tab键，只允许空格\n\n![1578847257509](assets/1578847257509.png)\n\n恭喜:tada::tada::tada:\n\n此时你已经部署好K8S集群！当然只有集群还远远不够，我们还需要更多的东西才能组成我们的PaaS服务，休息一下继续享受它:smiley:\n\n"
        },
        {
          "name": "第二章——企业部署实战_K8S【公有云版】.md",
          "type": "blob",
          "size": 84.2958984375,
          "content": "## 第二章——企业部署实战_K8S【公有云版】\n\n### 前言（推荐使用[标准版](https://github.com/ben1234560/k8s_PaaS/blob/master/%E7%AC%AC%E4%BA%8C%E7%AB%A0%E2%80%94%E2%80%94%E4%BC%81%E4%B8%9A%E9%83%A8%E7%BD%B2%E5%AE%9E%E6%88%98_K8S.md)，因为多次适配过后续内容）：\n\n注：公有云版未测试是否适配第三章往后内容，理论上是完全可行的。\n\n**考虑到部分同学机器配置并无法满足多开VMware，这里推出公有云版本（以阿里云为例）。必须有梯子（vpn/翻墙）**\n\n> **WHY：**为什么要有梯子，后面我们会创建一些网站，而国内域云服务器创建的网站需要ICP备案，每次备案都需要等审批非常麻烦。而国外域云服务器创建的网站则不需要备案，用起来非常舒服。\n>\n> <img src=\"assets/WX20230511-145214@2x.png\" alt=\"image-ICPFiling\" style=\"zoom:25%;\" />\n>\n> **HOW：**梯子推荐lestvpn(非广告，这个一个月要40多rmb，能选代理地址我因为也用chatGPT所以开了，有更便宜的一定要推荐我😭，mac/window都测过可以用)\n>\n> 下载链接（推荐使用浏览器访问）：https://bitbucket.org/letsgogo/letsgogo_19/src/master/README.md \n>\n> 备用链接（推荐使用浏览器访问）：https://github.com/LetsGo666/LetsGo_4\n>\n> 安装后打开填写我的ID：118149732 你还能多得3天会员！（可以跟朋友互相推荐互相得免费天数）\n\n\n\n**由于第二章需要配置的内容较多，不少新同学卡在这里。建议配合使用[软件包里的check_tool](https://github.com/ben1234560/k8s_PaaS/tree/master/%E8%BD%AF%E4%BB%B6%E5%8C%85/check_tool)边检查，边部署**\n\n##### 如果你是新手，机器的名字及各种账户密码一定要和我的一样，先学一遍，再自己改\n\n> **WHAT**：用于管理云平台中多个主机上的容器化的应用，Kubernetes的目标是让部署容器化的应用简单并且高效（powerful）,Kubernetes提供了应用部署，规划，更新，维护的一种机制\n>\n> **WHY**：为什么使用它，因为它是管理docker容器最主流的编排工具\n\n- Pod\n  - Pod是K8S里能够被运行的最小的逻辑单元（原子单元）\n  - 1个Pod里面可以运行多个容器，它们共享UTS+NET+IPC名称空间\n  - 可以把Pod理解成豌豆荚，而同一Pod内的每个容器是一颗颗豌豆\n  - 一个Pod里运行多个容器，又叫边车（SideCar）模式\n- Pod控制器（关于更多<a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E5%8E%9F%E7%90%86%E5%8F%8A%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/Kubernetes%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5.md#%E5%88%9D%E8%AF%86pod\">初识Pod</a>）\n  - Pod控制器是Pod启动的一种模板，用来保证在K8S里启动的Pod始终按照人们的预期运行（副本数、生命周期、健康状态检查...）\n  - Pod内提供了众多的Pod控制器，常用的有以下几种：\n    - Deployment\n    - DaemonSet\n    - ReplicaSet\n    - StatefulSet\n    - Job\n    - Cronjob\n- Name\n  - 由于K8S内部，使用“资源”来定义每一种逻辑概念（功能），故每种“资源”，都应该有自己的“名称”\n  - “资源”有api版本（apiVersion）类别（kind）、元数据（metadata）、定义清单（spec）、状态（status）等配置信息\n  - “名称”通常定义在“资源”的“元数据”信息里\n- <a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E5%8E%9F%E7%90%86%E5%8F%8A%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/Docker%E5%9F%BA%E7%A1%80.md#%E5%85%B3%E4%BA%8Enamespace\">namespace</a>\n  - 随着项目增多、人员增加、集群规模的扩大，需要一种能够隔离K8S内各种“资源”的方法，这就是名称空间\n  - 名称空间可以理解为K8S内部的虚拟集群组\n  - 不同名称空间内的“资源”名称可以相同，相同名称空间内的同种“资源”、“名称”不能相同\n  - 合理的使用K8S名称空间，使得集群管理员能够更好的对交付到K8S里的服务进行分类管理和浏览\n  - K8S内默认存在的名称空间有：default、kube-system、kube-public\n  - 查询K8S里特定“资源”要带上相应的名称空间\n- Label\n  - 标签是K8S特色的管理方式，便于分类管理资源对象\n  - 一个标签可以对应多个资源，一个资源也可以有多个标签，它们是多对多的关系\n  - 一个资源拥有多个标签，可以实现不同维度的管理\n  - 标签的组成：key=value\n  - 与标签类似的，还有一种“注解”（annotations）\n- Label选择器\n  - 给资源打上标签后，可以使用标签选择器过滤指定的标签\n  - 标签选择器目前有两个：基于等值关系（等于、不等于）和基于集合关系（属于、不属于、存在）\n  - 许多资源支持内嵌标签选择器字段\n    - matchLabels\n    - matchExpressions\n- Service\n  - 在K8S的世界里，虽然每个Pod都会被分配一个单独的IP地址，但这个IP地址会随着Pod的销毁而消失\n  - Service（服务）就是用来解决这个问题的核心概念\n  - 一个Service可以看作一组提供相同服务的Pod的对外访问接口\n  - Service作用与哪些Pod是通过标签选择器来定义的\n- Ingress\n  - Ingress是K8S集群里工作在OSI网络参考模型下，第7层的应用，对外暴露的接口\n  - Service只能进行L4流量调度，表现形式是ip+port\n  - Ingress则可以调度不同业务域、不同URL访问路径的业务流量\n\n简单理解：Pod可运行的原子，name定义名字，namespace名称空间（放一堆名字），label标签（另外的名字），service提供服务，ingress通信\n\n### K8S架构图（并非传统意义上的PaaS服务，而是IaaS服务）\n\n![1582188308711](assets/1582188308711.png)\n\n> **kubectl**：Kubernetes集群的命令行接口\n>\n> **API Server**：的核心功能是对核心对象（例如：Pod，Service，RC）的增删改查操作，同时也是集群内模块之间数据交换的枢纽\n>\n> **Etcd**：包含在 APIServer 中，用来存储资源信息\n>\n> **Controller Manager **：负责维护集群的状态，比如故障检测、自动扩展、滚动更新等\n>\n> **Scheduler**：负责资源的调度，按照预定的调度策略将Pod调度到相应的机器上。可以通过这些有更深的了解：\n>\n> - <a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E5%8E%9F%E7%90%86%E5%8F%8A%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/Kubernetes%E8%B0%83%E5%BA%A6%E6%9C%BA%E5%88%B6.md\">Kubernetes调度机制</a>\n> - <a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E5%8E%9F%E7%90%86%E5%8F%8A%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/Kubernetes%E8%B0%83%E5%BA%A6%E6%9C%BA%E5%88%B6.md#kubernetes%E7%9A%84%E8%B5%84%E6%BA%90%E6%A8%A1%E5%9E%8B%E4%B8%8E%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86\">Kubernetes的资源模型与资源管理</a>\n> - <a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E5%8E%9F%E7%90%86%E5%8F%8A%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/Kubernetes%E8%B0%83%E5%BA%A6%E6%9C%BA%E5%88%B6.md#kubernetes%E9%BB%98%E8%AE%A4%E7%9A%84%E8%B0%83%E5%BA%A6%E7%AD%96%E7%95%A5\">Kubernetes默认的调度策略</a>\n> - <a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E5%8E%9F%E7%90%86%E5%8F%8A%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/Kubernetes%E8%B0%83%E5%BA%A6%E6%9C%BA%E5%88%B6.md#%E8%B0%83%E5%BA%A6%E5%99%A8%E7%9A%84%E4%BC%98%E5%85%88%E7%BA%A7%E4%B8%8E%E5%BC%BA%E5%88%B6%E6%9C%BA%E5%88%B6\">调度器的优先级与强制机制</a>\n>\n> **kube-proxy**：负责为Service提供cluster内部的服务发现和负载均衡\n>\n> **Kubelet**：在Kubernetes中，应用容器彼此是隔离的，并且与运行其的主机也是隔离的，这是对应用进行独立解耦管理的关键点。[Kubelet工作原理解析](https://github.com/ben1234560/k8s_PaaS/blob/master/%E5%8E%9F%E7%90%86%E5%8F%8A%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/Kubernetes%E8%B0%83%E5%BA%A6%E6%9C%BA%E5%88%B6.md#kubelet)\n>\n> **Node**：运行容器应用，由Master管理\n\n### 我们部署的K8S架构图\n\n![1584700891603](assets/1584700891603.png)\n\n> 可以简单理解成：\n>\n> 11机器：反向代理\n>\n> 12机器：反向代理\n>\n> 21机器：主控+运算节点（即服务群都是跑在21和22上）\n>\n> 22机器：主控+运算节点（生产上我们会把主控和运算分开）\n>\n> 200机器：运维主机（放各种文件资源）\n>\n> 这样控节点有两个，运算节点有两个，就是小型的分布式，现在你可能没办法理解这些内容，我们接着做下去，慢慢的，你就理解了\n>\n> 这是理论上的架构，考虑到公有云对网络的限制等问题（如ipv6不能随意增加），21、22机器也会追加反向代理。\n\n### 实验机器安装详解\n\n我们将申请5个节点，如下图\n\n|      | 整体  | 11机器 | 12机器 | 21机器 | 22机器 | 200机器 |\n| ---- | ----- | ------ | ------ | ------ | ------ | ------- |\n| 低配 | 4C32G | 2C2G   | 2C2G   | 2C8G   | 2C8G   | 2C2G    |\n| 标配 | 8C64G | 2C4G   | 2C4G   | 2C16G  | 2C16G  | 2C2G    |\n\n> 低配能够满足Jenkins之前的内容，做到Jenkins的时候就已经很卡了，到时候再追加购买新的配置，当然也可以直接购买标配，前期我还是希望各位能在理解上花些时间，慢慢的操作，否则报错都不知道怎么解决\n\n### 如何购买公有云机器（以阿里云为例）\n\n> 注：未选的则是默认选项。\n\n如下图，选择“自定义购买”、“抢占式实例”（这里使用抢占式即可/省钱，担心被回收的可以选择按量付费）、“新加坡”或其它外网服务器地域（外网服务器，方便后续使用网站等时不需要备案/最好准备个梯子，作为一个程序员梯子是必备的）、“可用区”（必须选择其中一个如我选的可用区B，不能随机分配，否则多台机器的内网ip不一致）\n\n![image-购买云服务器](assets/WX20230511-144757@2x.png)\n\n> **WHY：**为什么选新加坡，因为我的梯子可以到新加坡，你选你能代理的外网地址即可\n>\n> <img src=\"assets/WX20230511-150425@2x.png\" alt=\"image-梯子代理地址\" style=\"zoom:33%;\" align=\"left\"/>\n\n选择“共享型”（也可以选择通用型，共享型会更便宜）、选择2核8G内存的配置，\n\n![image-选择机器配置](assets/WX20230511-150009@2x.png)\n\n选择CentOS，7.6版本\n\n![image-选择系统版本](assets/WX20230511-150553@2x.png)\n\n网络分配公网IPv4地址，默认选择，其它不变，这样创建下来的服务器是公用同一安全组配置，如果钱允许的话把网速开到最大\n\n选择“自定义密码“，建议设置相对复杂的密码，如包含大小写，否则容易被攻破使用\n\n![image-自定义密码](assets/WX20230511-150737@2x.png)\n\n最终订单内容，注意是5台2C8G，一小时仅需0.2\n\n![image-结算页面](assets/WX20230511-150820@2x.png)\n\n购买完成后的内容，建议给每个实例设置节点含义，如下图的k8s_11、k8s_12\n\n![image-实例列表](/Users/xueweiguo/Desktop/GitHub/k8s_PaaS/assets/WX20230511-151455@2x.png)\n\n如下是我的内网及节点对应情况\n\n> |        | 11机器         | 12机器         | 21机器         | 22机器         | 200机器        |\n> | ------ | -------------- | -------------- | -------------- | -------------- | -------------- |\n> | 内网ip | 172.27.139.122 | 172.27.139.119 | 172.27.139.118 | 172.27.139.120 | 172.27.139.121 |\n>\n> 可以看到网段在172.27.139.0/254，我的11机器内网ip是172.27.139.122，后续将会持续用到，你需要填成你自己的11机器内网ip\n\n### 机器前期准备\n\n使用xshell或者自选的shell连接工具，[xshell下载](https://xshell.en.softonic.com/download)\n\n> 当然我也有提供软件包：https://pan.baidu.com/s/1mkIzua1XQmew240XBbvuFA 提取码：7p6h 。里面还有Xftp，是用来进行本地电脑和虚拟机的文件传输\n\n~~~\n# 全部机器，查看enforce是否关闭，确保disabled状态，当然可能没有这个命令\n~]# getforce\n# 查看内核版本，确保在3.8以上版本\n~]# uname -a\n# 关闭firewalld\n~]# systemctl stop firewalld\n# 安装epel源及相关工具\n~]# yum install epel-release wget net-tools telnet tree nmap sysstat lrzsz dos2unix bind-utils yum-utils ntpd -y\n# 检查是否安装成功\n~]# rpm -q epel-release wget net-tools telnet tree nmap sysstat lrzsz dos2unix bind-utils yum-utils ntpd\n# out:\nepel-release-7-14.noarch\nwget-1.14-18.el7_6.1.x86_64\nnet-tools-2.0-0.25.20131004git.el7.x86_64\ntelnet-0.17-66.el7.x86_64\ntree-1.6.0-10.el7.x86_64\nnmap-6.40-19.el7.x86_64\nsysstat-10.1.5-20.el7_9.x86_64\nlrzsz-0.12.20-36.el7.x86_64\ndos2unix-6.0.3-7.el7.x86_64\nbind-utils-9.11.4-26.P2.el7_9.13.x86_64\nyum-utils-1.1.31-54.el7_8.noarch\n~~~\n\n> **uname**:显示系统信息\n>\n> - **-a/-all**：显示全部\n>\n> **yum**：提供了查找、安装、删除某一个、一组甚至全部软件包的命令\n>\n> - **install**：安装\n> - **-y**：当安装过程提示选择全部为\"yes\"\n\n<img src=\"assets/WX20230511-152710@2x.png\">\n\n后续代理后，yum安装会出现网络等问题，为了减少处理的时间，我们提前下载\n\n~~~\n# 全部机器，安装后续需要的服务工具，不启动则不会有影响（我就不区分每个机器所需的了，后面会看到）\n# 追加docker的repo源\n~]# yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo\n# 安装后续所需的软件包\n~]# yum install supervisor docker-compose nginx nginx-mod-stream supervisor keepalived deltarpm docker-ce ipvsadm -y\n# 检查安装情况\n~]# rpm -q supervisor docker-compose nginx nginx-mod-stream ipvsadm \n# out:\nsupervisor keepalived deltarpm docker-ce\nsupervisor-3.4.0-1.el7.noarch\ndocker-compose-1.18.0-4.el7.noarch\nnginx-1.20.1-10.el7.x86_64\nnginx-mod-stream-1.20.1-10.el7.x86_64\nsupervisor-3.4.0-1.el7.noarch\nkeepalived-1.3.5-19.el7.x86_64\ndeltarpm-3.6-3.el7.x86_64\ndocker-ce-23.0.6-1.el7.x86_64\n~~~\n\n后续章节需要用到的（可后续再装）\n\n~~~\n# 全部机器\n# 第三章\n~]# yum install -y iptables-services\n# 第四章\n~]# yum install -y openssl\n# 第五章\n~]# yum install -y java-1.8.0-openjdk*\n~]# yum install curl policycoreutils openssh-server openssh-clients policycoreutils-python -y\n\n# 全部一起下载\n~]# yum install -y iptables-services openssl policycoreutils openssh-server openssh-clients policycoreutils-python java-1.8.0-openjdk*\n# 检查安装情况\n~]# rpm -q iptables-services openssl java-1.8.0-openjdk-javadoc-zip java-1.8.0-openjdk-headless java-1.8.0-openjdk-devel java-1.8.0-openjdk-src java-1.8.0-openjdk-accessibility java-1.8.0-openjdk-demo java-1.8.0-openjdk java-1.8.0-openjdk-javadoc curl policycoreutils openssh-server openssh-clients policycoreutils-python\n~~~\n\n配置机器名及网络\n\n~~~\n# 全部机器，设置名字，11是hdss7-11,12是hdss7-12,以此类推\n~]# hostnamectl set-hostname hdss7-11.host.com\n~]# exit\n# 下面的修改，11机器对应11机器的内网ip，12对应12机器的内网ip，以此类推共两处：IPADDR、GATEWAY\n~]# vi /etc/sysconfig/network-scripts/ifcfg-eth0\nDEVICE=eth0\nBOOTPROTO=dhcp\nONBOOT=yes\nIPADDR=172.27.139.122\nGATEWAY=172.27.139.254\nNETMASK=255.255.255.0\nTYPE=Ethernet\nPROXY_METHOD=none\nBROWSER_ONLY=no\nDEFROUTE=yes\nIPV4_FAILURE_FATAL=no\nIPV6INIT=yes\nIPV6_AUTOCONF=yes\nIPV6_DEFROUTE=yes\nIPV6_FAILURE_FATAL=no\nIPV6_ADDR_GEN_MODE=stable-privacy\nNAME=eth0\nDEVICE=eth0\nIPV6_PEERNDS=yes\nIPV6_PEERROUTES=yes\nIPV6_PRIUACY=no\n\n~]# systemctl restart network\n# 结束ping命令用ctrl+c(control+c)，用于发送中断信号给当前正在运行的进程\n~]# ping baidu.com\n~~~\n\n> **ifcfg-eth0**：有些人的机器可能是ifcfg-esn33。使用ifconfig查看，命令输出中每个网卡接口都有一个名称，例如 \"eth0\"、\"ens33\" 等\n>\n> **systemctl restart**：重启某个服务\n>\n> **ping**：用于测试网络连接量的程序\n>\n> 公有云（阿里）的ifcfg-eth0原内容如下：\n>\n> DEVICE=eth0\n> BOOTPROTO=dhcp\n> ONBOOT=yes\n\n<img src=\"assets/WX20230511-152352@2x.png\" >\n\n\n\n### K8S前置准备工作——bind9安装部署（DNS服务）\n\n> **WHAT**：DNS（域名系统）说白了，就是把一个域和IP地址做了一下绑定，如你在里机器里面输入 nslookup www.qq.com，出来的Address是一堆IP，IP是不容易记的，所以DNS让IP和域名做一下绑定，这样你输入域名就可以了\n>\n> **WHY**：我们要用ingress，在K8S里要做7层调度，而且无论如何都要用域名（如之前的那个百度页面的域名，那个是host的方式），但是问题是我们怎么给K8S里的容器绑host，所以我们必须做一个DNS，然后容器服从我们的DNS解析调度\n\n~~~\n# 在11机器：\n~]# yum install bind -y\n~]# rpm -qa bind\n# out: bind-9.11.4-9.P2.el7.x86_64\n# 配置主配置文件，11机器\n~]# vi /etc/named.conf\nlisten-on port 53 { 172.27.139.122; };  # 原本是127.0.0.1，我的11内网ip是172.29.238.166，你填你的11内网ip\n# listen-on-v6 port 53 { ::1; };  # 需要删掉\nallow-query     { any; };  # 原本是localhost\nforwarders      { 172.27.139.254; };  #另外添加的\ndnssec-enable no;  # 原本是yes\ndnssec-validation no;  # 原本是yes\n\n# 检查修改情况，没有报错即可（即没有信息）\n~]# named-checkconf\n~~~\n\n> **rpm**：软件包管理器\n>\n> - **-qa**：查看已安装的所有软件包\n>\n> **rpm和yum安装的区别**：前者不检查相依性问题，后者检查（即相关依赖包）\n>\n> **named.conf文件内容解析：**\n>\n> - **listen-on**：监听端口，改为监听在内网，这样其它机器也可以用\n> - **allow-query**：哪些客户端能通过自建的DNS查\n> - **forwarders**：上级DNS是什么\n\n<img src=\"assets/WX20230511-153237@2x.png\"  />\n\n\n\n~~~\n# 11机器，经验：主机域一定得跟业务是一点关系都没有，如host.com，而业务用的是od.com，因为业务随时可能变\n# 区域配置文件，加在最下面，需要修改两处：allow-update为11机器ip\n~]# vi /etc/named.rfc1912.zones\nzone \"host.com\" IN {\n        type  master;\n        file  \"host.com.zone\";\n        allow-update { 172.27.139.122; };\n};\n\nzone \"od.com\" IN {\n        type  master;\n        file  \"od.com.zone\";\n        allow-update { 172.27.139.122; };\n};\n\n~~~\n\n<img src=\"assets/WX20230511-153512@2x.png\" >\n\n> **注意：**当配置11机器内网ip后，该机器应保存运行状态，重启后其它机器可能无法连接外网。@https://github.com/xinzhuxiansheng感谢建议！\n\n配置域名解析的相关信息\n\n~~~\n# 11机器：\n# 注意serial行的时间，代表今天的时间+第一条记录：20230511+01，需要修改serial、各个ip、\n7-11 ~]# vi /var/named/host.com.zone\n$ORIGIN host.com.\n$TTL 600\t; 10 minutes\n@       IN SOA\tdns.host.com. dnsadmin.host.com. (\n\t\t\t\t2023051101 ; serial\n\t\t\t\t10800      ; refresh (3 hours)\n\t\t\t\t900        ; retry (15 minutes)\n\t\t\t\t604800     ; expire (1 week)\n\t\t\t\t86400      ; minimum (1 day)\n\t\t\t\t)\n\t\t\tNS   dns.host.com.\n$TTL 60\t; 1 minute\ndns                A    172.27.139.122\nHDSS7-11           A    172.27.139.122\nHDSS7-12           A    172.27.139.119\nHDSS7-21           A    172.27.139.118\nHDSS7-22           A    172.27.139.120\nHDSS7-200          A    172.27.139.121\n\n7-11 ~]# vi /var/named/od.com.zone\n$ORIGIN od.com.\n$TTL 600\t; 10 minutes\n@   \t\tIN SOA\tdns.od.com. dnsadmin.od.com. (\n\t\t\t\t2023051101 ; serial\n\t\t\t\t10800      ; refresh (3 hours)\n\t\t\t\t900        ; retry (15 minutes)\n\t\t\t\t604800     ; expire (1 week)\n\t\t\t\t86400      ; minimum (1 day)\n\t\t\t\t)\n\t\t\t\tNS   dns.od.com.\n$TTL 60\t; 1 minute\ndns                A    172.27.139.122\n\n# 看一下有没有报错\n7-11 ~]# named-checkconf\n7-11 ~]# systemctl start named\n7-11 ~]# netstat -luntp|grep 53\n~~~\n\n> **TTL 600**：指定IP包被路由器丢弃之前允许通过的最大网段数量\n>\n> - **10 minutes**：过期时间10分钟\n>\n> **SOA**：一个域权威记录的相关信息，后面有5组参数分别设定了该域相关部分\n>\n> - **dnsadmin.od.com.**  一个假的邮箱\n> - **serial**：记录的时间\n>\n> **$ORIGIN**：即下列的域名自动补充od.com，如dns，外面看来是dns.od.com\n>\n> **netstat -luntp**：显示 tcp,udp 的端口和进程等相关情况\n\n<img src=\"assets/WX20230511-153817@2x.png\" />\n\n~~~\n# 11机器，检查主机域是否解析\n7-11 ~]# dig -t A hdss7-21.host.com @172.27.139.122 +short\n# out: 172.27.139.118\n# 配置服务器能使用这个配置，在最下面追加11机器的内网ip\n7-11 ~]# vi /etc/sysconfig/network-scripts/ifcfg-eth0\nDNS1=172.27.139.122\n7-11 ~]# systemctl restart network\n# 重启网络后，马上ping可能无法连接，需要等个十秒钟\n7-11 ~]# ping baidu.com\n7-11 ~]# ping hdss7-21.host.com\n~~~\n\n> **dig -t A**：指的是找DNS里标记为A的相关记录，而后面会带上相关的域，如上面的hdss7-21.host.com，为什么外面配了HDSS7-21后面还会自动接上.host.com就是因为$ORIGIN，后面则是对应的IP\n>\n> - **+short**：表示只返回IP\n\n<img src=\"assets/WX20230511-154100@2x.png\" />\n\n~~~\n# 在非11机器上，最下面追加dns1=11机器内网ip\n~]# vi /etc/sysconfig/network-scripts/ifcfg-eth0\nDNS1=172.27.139.122\n\n~]# systemctl restart network\n# 试下网络是否正常\n~]# ping baidu.com\n# 其它机器尝试ping7-11机器或200机器等任意机器，这时候短域名是可以使用了\n7-12 ~]# ping hdss7-11.host.com\n7-12 ~]# ping hdss7-200.host.com\n~~~\n\n> 让其它机器的DNS全部改成11机器的好处是，全部的机器访问外网就只有通过11端口，更好控制\n\n<img src=\"assets/WX20230511-154412@2x.png\"  />\n\n机器准备工作完成:tada:\n\n\n\n### K8S前置工作——准备签发证书环境\n\n> **WHAT**： 证书，可以用来审计也可以保障安全，k8S组件启动的时候，则需要有对应的证书，证书的详解你也可以在网上搜到，这里就不细细说明了\n>\n> **WHY**：当然是为了让我们的组件能正常运行\n\n~~~\n# cfssl方式做证书，需要三个软件（如果网络无法连通，则参考下面我已下载好的软件包），按照我们的架构图，我们部署在200机器:\n200 ~]# wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 -O /usr/bin/cfssl\n200 ~]# wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 -O /usr/bin/cfssl-json\n200 ~]# wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64 -O /usr/bin/cfssl-certinfo\n200 ~]# chmod +x /usr/bin/cfssl*\n200 ~]# which cfssl\n200 ~]# which cfssl-json\n200 ~]# which cfssl-certinfo\n~~~\n\n> **wget**：从网络上自动下载文件的自由工具\n>\n> **chmod**：给对应的文件添加权限（[菜鸟教程](https://www.runoob.com/linux/linux-comm-chmod.html)）\n>\n> - **+x**：给当前用户增加可执行该文件的权限\n>\n> **which**：查看相应的东西在哪里\n\n##### 报错：wget报错网络问题\n\n~~~\n# 针对下载cfssl网络无法连通的情况，百度云https://pan.baidu.com/s/1arE2LdtAbcR80gmIQtIELw 提取码：ouy1。找到名为”第二章涉及的软件包“中的cfssl的3个文件，上传到/root目录下\n200 ~]# cp cfssl_linux-amd64 /usr/bin/cfssl\n200 ~]# cp cfssljson_linux-amd64 /usr/bin/cfssl-json\n200 ~]# cp cfssl-certinfo_linux-amd64 /usr/bin/cfssl-certinfo\n200 ~]# chmod +x /usr/bin/cfssl*\n200 ~]# which cfssl\n200 ~]# which cfssl-json\n200 ~]# which cfssl-certinfo\n~~~\n\n<img src=\"assets/WX20230511-155452@2x.png\" />\n\n\n\n~~~\n200 ~]# mkdir /opt/certs\n200 ~]# cd /opt/certs\ncerts]# vi ca-csr.json\n{\n    \"CN\": \"ben123123\",\n    \"hosts\": [\n    ],\n    \"key\": {\n        \"algo\": \"rsa\",\n        \"size\": 2048\n    },\n    \"names\": [\n        {\n            \"C\": \"CN\",\n            \"ST\": \"beijing\",\n            \"L\": \"beijing\",\n            \"O\": \"od\",\n            \"OU\": \"ops\"\n        }\n    ],\n    \"ca\": {\n        \"expiry\": \"1752000h\"\n    }\n}\ncerts]# cfssl gencert -initca ca-csr.json | cfssl-json -bare ca\ncerts]# cat ca.pem\n# 如果你这时候显示段错误或者json错误，就是之前克隆虚拟机的时候200机器地址和别的机器地址重叠冲突了，需要重建200虚拟机\n~~~\n\n> **cd **：切换当前工作目录到 dirName\n>\n> - 语法：cd [dirName]\n>\n> **mkdir**：建立名称为 dirName 之子目录\n>\n> - 语法：mkdir [-p] dirName\n> - **-p:** 确保目录存在，不存在则建一个，如mkdir empty/empty1/empty2\n>\n> **cfssl**：证书工具\n>\n> - gencert：生成的意思\n>\n> **ca-csr.json解析：**\n>\n> - CN：Common Name，浏览器使用该字段验证网址是否合法，一般写域名，非常重要\n> - ST：State，省\n> - L：Locality，地区\n> - O：Organization Name，组织名称\n> - OU：Organization Unit Name，组织单位名称\n>\n> <a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E5%8E%9F%E7%90%86%E5%8F%8A%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/Kubernetes%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5.md#pod%E4%B8%AD%E5%87%A0%E4%B8%AA%E9%87%8D%E8%A6%81%E5%AD%97%E6%AE%B5%E7%9A%84%E5%90%AB%E4%B9%89%E5%92%8C%E7%94%A8%E6%B3%95\">Pod中几个重要字段的含义和用法</a>\n\n<img src=\"assets/WX20230511-155653@2x.png\" />\n\n\n\n### K8S前置工作——部署docker环境\n\n> **WHAT**：是一个开源的应用容器引擎，让开发者可以打包他们的应用以及依赖包到一个可移植的镜像中，然后发布到任何流行的 Linux或Windows 机器上，也可以实现虚拟化。\n>\n> **WHY**：Pod里面就是由数个docker容器组成，Pod是豌豆荚，docker容器是里面的豆子。\n\n~~~\n# 如我们架构图所示，运算节点是21/22机器（没有docker则无法运行pod），运维主机是200机器（没有docker则没办法下载docker存入私有仓库），所以在三台机器安装（21/22/200）\n# 这个前面已经安装 curl -fsSL https://get.docker.com | bash -s docker --mirror Aliyun\n# 上面的下载可能网络有问题，需要多试几次，这些部署我已经不同机器试过很多次了，实在不行的使用下面提供的报错解决方法。\n~]# mkdir -p /data/docker /etc/docker\n# # 注意，172.7.21.1，这里得21是指在hdss7-21得机器，如果是22得机器，就是172.7.22.1，共一处需要改机器名：\"bip\": \"172.7.21.1/24\"\n~]# vi /etc/docker/daemon.json\n{\n  \"data-root\": \"/data/docker\",\n  \"storage-driver\": \"overlay2\",\n  \"insecure-registries\": [\"registry.access.redhat.com\",\"quay.io\",\"harbor.od.com\"],\n  \"registry-mirrors\": [\"https://q2gr04ke.mirror.aliyuncs.com\"],\n  \"bip\": \"172.7.21.1/24\",\n  \"exec-opts\": [\"native.cgroupdriver=systemd\"],\n  \"live-restore\": true\n}\n\n~]# systemctl start docker\n~]# docker version\n~]# docker ps -a\n~~~\n\n> **mkdir -p：**前面有讲到过（上一级目录没有则创建），而这次后面带了两个目录，意思是同时创建两个目录\n>\n> **daemon.json：**为什么配aliyuncs的环境，是因为默认是连接到外网的，速度比较慢，所以我们可以直接使用国内的阿里云镜像源，当然还有腾讯云等\n>\n> **docker version：**查看docker的版本\n>\n> **daemon.json解析：**重点说一下这个为什么10.4.7.21机器对应着172.7.21.1/24，这里可以看到10的21对应得是172的21，这样做的好处就是，当你的pod出现问题时，你可以马上定位到是在哪台机器出现的问题，是21还是22还是其它的，这点在生产上非常重要，有时候你的dashboard（后面会安装）宕掉了，你没办法只能去机器找，而这时候你又找不到的时候，你老板会拿你祭天的\n\n##### 报错：curl -fsSL https://get.docker.com | bash -s docker --mirror Aliyun\n\n报错信息：\n\n```bash\nhttp://mirrors.cloud.aliyuncs.com/centos/7/updates/x86_64/Packages/libxml2-2.9.1-6.el7_9.6.x86_64.rpm: [Errno 14] curl#6 - \"Could not resolve host: mirrors.cloud.aliyuncs.com; Unknown error\"\nTrying other mirror.\n\nError downloading packages:\n  libxml2-2.9.1-6.el7_9.6.x86_64: [Errno 256] No more mirrors to try.\n  python-kitchen-1.1.1-5.el7.noarch: [Errno 256] No more mirrors to try.\n  yum-utils-1.1.31-54.el7_8.noarch: [Errno 256] No more mirrors to try.\n  python-chardet-2.2.1-3.el7.noarch: [Errno 256] No more mirrors to try.\n  libxml2-python-2.9.1-6.el7_9.6.x86_64: [Errno 256] No more mirrors to try.\n```\n\n\n\n原因：yum源出现问题\n\n解决方法：\n\n~~~\nmv /etc/yum.repos.d/epel.repo /etc/yum.repos.d/epel.repo.bak\ncurl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo  # 这一步必须成功，没成功的再执行一遍即可\nyum update -y  # 这一步耗时较长\nyum install epel-release -y\nyum install -y yum-utils\nyum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo\nyum install -y docker-ce\n~~~\n\n<img src=\"assets/WX20230511-162032@2x.png\" />\n\n\n\n### K8S前置工作——部署harbor仓库\n\n> **WHAT **：harbor仓库是可以部署到本地的私有仓库，也就是你可以把镜像推到这个仓库，然后需要用的时候再下载下来，这样的好处是：1、下载速度快，用到的时候能马上下载下来2、不用担心镜像改动或者下架等。\n>\n> **WHY**：因为我们的部署K8S涉及到很多镜像，制作相关包的时候如果网速问题会导致失败重来，而且我们在公司里也会建自己的仓库，所以必须按照harbor仓库\n\n~~~\n# 如架构图，我们安装在200机器：\n200 ~]# mkdir /opt/src  && cd /opt/src\n# 可以去这个地址下载，也可以直接用我用的软件包\nhttps://github.com/goharbor/harbor/releases/tag/v1.8.3\n200 src]# wget https://storage.googleapis.com/harbor-releases/release-1.8.0/harbor-offline-installer-v1.8.3.tgz\n7-200 src]# tar xf harbor-offline-installer-v1.8.3.tgz -C /opt/\n~~~\n\n> **tag**：可以加入，解开备份文件内的文件\n>\n> - **x**：解压\n> - **f**： 使用档案名字\n> - **-C**：切换到指定的目录\n> - 整条命令合起来就是，把tgz文件以tgz文件名为名字解压到opt目录下，并保存tgz文件原样\n>\n> **关于版本**：一般人都是喜欢用比较新的版本，我们当然也支持比较新的版本，但对于公司而已，稳定是最要紧的，v1.8.3是用的比较稳定的版本，而后续的各个组件也会有更加新的版本，你可以尝试新版本，但有些由于兼容问题不能用新的，后续那些不能用我会标记清楚。\n\n~~~\n# 200机器：\n200 src]# cd /opt/\n200 opt]# mv harbor/ harbor-v1.8.3\n200 opt]# ln -s /opt/harbor-v1.8.3/ /opt/harbor\n200 opt]# cd harbor\n200 harbor]# ll\n# 修改域名、端口、挂载卷及日志存放地址\n200 harbor]# vi harbor.yml  # 如果这里是用的2.8.0版本，则需要cp harbor.yml.tmpl harbor.yml\nhostname: harbor.od.com  # 原reg.mydomain.com\nhttp:\n  port: 180  # 原80\ndata_volume: /data/harbor\nlocation: /data/harbor/logs\n\n200 harbor]# mkdir -p /data/harbor/logs\n# 这个前面已经安装 yum install docker-compose -y\n200 harbor]# rpm -qa docker-compose\n# out: docker-compose-1.18.0-4.el7.noarch\n# yum install docker-compose 包无法下载的，可以查看下面提供的报错解决方法\n200 harbor]# ./install.sh\n~~~\n\n> **提示**：harbor v2.3.3版本安装也需要将https相关的配置注释掉\n>\n> 如图#https:\n>\n> ![1635775214057](assets/1635775214057.png)\n>\n> 感谢@https://github.com/xinzhuxiansheng\n>\n> **mv**：为文件或目录改名、或将文件或目录移入其它位置。\n>\n> - 这里的命令是有斜杠的，所以是移动到某个目录下\n>\n> **ln**：为某一个文件在另外一个位置建立一个同步的链接\n>\n> - `语法:ln [参数][源文件或目录][目标文件或目录]`\n> - `**-s：**软连接，可以对整个目录进行链接`\n>\n> **harbor.yml解析：**\n>\n> - port为什么改成180：因为后面我们要装nginx，nginx用的80，所以要把它们错开\n> - data_volume：数据卷，即docker镜像放在哪里\n> - location：日志文件\n> - **./install.sh**：启动shell脚本\n\n##### 报错：yum install docker-compose -y报错No package docker-compose available.\n\n使用我提供的官方软件包（放在百度网盘），放到harbor路径下\n\n~~~sh\n200 harbor]# sudo cp docker-compose-Linux-x86_64 /usr/local/bin/docker-compose\n200 harbor]# sudo chmod +x /usr/local/bin/docker-compose\n200 harbor]# docker-compose --version\n# out: docker-compose version 1.29.2, build 5becea4c\n200 harbor]# ./install.sh\n~~~\n\n<img src=\"assets/WX20230511-165532@2x.png\" />\n\n<img src=\"assets/WX20230511-165640@2x.png\" />\n\n\n\n~~~\n# 200机器：\n200 harbor]# docker-compose ps\n# out:\n      Name                     Command               State             Ports          \n--------------------------------------------------------------------------------------\nharbor-core         /harbor/start.sh                 Up                               \nharbor-db           /entrypoint.sh postgres          Up      5432/tcp                 \nharbor-jobservice   /harbor/start.sh                 Up                               \nharbor-log          /bin/sh -c /usr/local/bin/ ...   Up      127.0.0.1:1514->10514/tcp\nharbor-portal       nginx -g daemon off;             Up      80/tcp                   \nnginx               nginx -g daemon off;             Up      0.0.0.0:180->80/tcp      \nredis               docker-entrypoint.sh redis ...   Up      6379/tcp                 \nregistry            /entrypoint.sh /etc/regist ...   Up      5000/tcp                 \nregistryctl         /harbor/start.sh                 Up \n200 harbor]# docker ps -a\n# 这个前面已经安装 yum install nginx -y  # 报错的参考下面报错对应的解决方法\n\n###相关报错问题：\nyum的时候报：/var/run/yum.pid 已被锁定，PID 为 1610 的另一个程序正在运行。\n另外一个程序锁定了 yum；等待它退出……\n网上统一的解决办法：直接在终端运行 rm -f /var/run/yum.pid 将该文件删除，然后再次运行yum。\n###\n~~~\n\n##### 报错：yum install nginx -y没有对应包\n\n~~~\n200 ~]# yum install nginx -y\nLoaded plugins: fastestmirror\nLoading mirror speeds from cached hostfile\n * base: mirrors.aliyun.com\n * extras: mirrors.aliyun.com\n * updates: mirrors.aliyun.com\nNo package nginx available.\nError: Nothing to do\n~~~\n\n解决方法\n\n~~~\n# 编写nginx.repo\n200 harbor]# sudo vi /etc/yum.repos.d/nginx.repo\n[nginx]\nname=nginx repo\nbaseurl=http://nginx.org/packages/centos/7/$basearch/\ngpgcheck=0\nenabled=1\n\n# 保存退出后即可安装nginx了\n200 harbor]# sudo yum install nginx -y\n~~~\n\n\n\n解决完接着走\n\n~~~\n200 harbor]# vi /etc/nginx/conf.d/harbor.od.com.conf\nserver {\n    listen       80;\n    server_name  harbor.od.com;\n\n    client_max_body_size 1000m;\n\n    location / {\n        proxy_pass http://127.0.0.1:180;\n    }\n}\n\n200 harbor]# nginx -t\n200 harbor]# systemctl start nginx\n200 harbor]# systemctl enable nginx\n~~~\n\n> **nginx -t**：测试*nginx*.conf配置文件中是否存在语法错误\n>\n> **systemctl enable nginx**：开机自动启动\n\n追加域名解析\n\n~~~\n# 在11机器解析域名，IP使用200机器的IP：\n~]# vi /var/named/od.com.zone\n# 注意serial前滚一个序号\n# 最下面添加域名\nharbor             A    172.27.139.121\n~]# systemctl restart named\n~]# dig -t A harbor.od.com +short\n# out:172.29.238.162\n~~~\n\n<img src=\"assets/WX20230511-163252@2x.png\" />\n\n~~~\n# 200机器上curl：\nharbor]# curl harbor.od.com\n~~~\n\n> 注意：\n>\n> getenforce得是关闭状态，而不是enforcing，否则会报502\n> 暂时关闭setenforce 0\n> 永久关闭，改配置文件 vi /etc/selinux/config\n> SELINUX=disabled\n\n<img src=\"assets/WX20230511-163549@2x.png\"/>\n\n公有云的安全组添加tcp 180端口权限\n\n<img src=\"assets/WX20230511-164140@2x.png\" />\n\n在本机（window/mac）hosts，在文件末尾添加新的 DNS 域名解析（确保能telnet通），如：\n\n~~~\n公网ip harbo.od.com  # 公网ip对应200机器的公网ip\n~~~\n\n[访问harbor.od.com:180](harbor.od.com:180)\n\n<img src=\"assets/WX20230511-190408@2x.png\"  />\n\n<img src=\"assets/WX20230512-095227@2x.png\"  />\n\n~~~\n账号：admin\n密码：Harbor12345\n新建一个public公开项目，登陆报错请参考下面的报错解决方法，一般是网络问题，如果接口方式也无法解决，直接重装更快\n~~~\n\n![1578831458247](assets/1578831458247.png)\n\n~~~\n# 200机器，尝试下是否能push成功到harbor仓库\nharbor]# docker pull nginx:1.7.9\nharbor]# docker images|grep 1.7.9\nharbor]# docker tag 84581e99d807 harbor.od.com/public/nginx:v1.7.9\nharbor]# docker login harbor.od.com\n账号：admin\n密码：Harbor12345\nharbor]# docker push harbor.od.com/public/nginx:v1.7.9\n# 报错：如果发现登录不上去了，过一阵子再登录即可，大约5分钟左右\n~~~\n\n![1578832524891](assets/1578832524891.png)\n\n##### 报错：用正确的admin及密码仍提示密码错误\n\n排查：cat /data/harbor/logs/* |grep -i erro\n\n重新安装一次harbor或直接用命令行操作创建public\n\n~~~\n# 针对1.8.2版本，新版本需要新版本的操作指令，部署后api控制中心在这里http://harbor.od.com:180/devcenter\n# 创建public\n~]curl -u \"admin:Harbor12345\" -X POST \"http://harbor.od.com/api/projects\" -H \"Content-Type: application/json\" -d '{\"project_name\":\"public\",\"metadata\":{\"public\":\"true\"}}'\n# 确认是否创建成功\n~]curl -u \"admin:Harbor12345\" -X GET \"http://harbor.od.com/api/projects\"\n~~~\n\n<img src=\"assets/WX20230511-191809@2x.png\"  />\n\n检查推送nginx是否成功\n\n~~~\n# 必须已经下载好jq了，yum install jq -y\n# 获取public的id，一般是2\ncurl -u \"admin:Harbor12345\" -X GET \"http://harbor.od.com/api/projects?name=public\" | jq '.[0].project_id'\n# 使用id=2进行api接口的调用\ncurl -u \"admin:Harbor12345\" -X GET \"http://harbor.od.com/api/repositories?project_id=2\" | jq '.[].name'\n~~~\n\n成功，此时你已成功建立了自己的本地私有仓库:tada:，或许你会考虑建立自己的独立站，enjoy!\n\n\n\n### 安装部署主控节点服务etcd\n\n> 注意, 一定要同步每个虚拟机的时间, 时区和时间要保持一致, 可以通过以下的命令来操作, 最好所有的虚拟机都执行一次, 最好的方法是写进`/etc/rc.local`中\n\n  ```bash\ntimedatectl set-timezone Asia/Shanghai\ntimedatectl set-ntp true\n  ```\n\n> **WHAT**：一个高可用强一致性的服务发现存储仓库，关于服务发现，其本质就是知道了集群中是否有进程在监听udp和tcp端口（如上面部署的harbor就是监听180端口），并且通过名字就可以查找和连接。\n>\n> - **一个强一致性、高可用的服务存储目录**：基于Raft算法的etcd天生就是这样\n> - **一种注册服务和监控服务健康状态的机制**：在etcd中注册服务，并且对注册的服务设置`key TTL`（TTL上面有讲到），定时保持服务的心跳以达到监控健康状态的效果\n> - **一种查找和连接服务的机制**：通过在etcd指定的主题下注册的服务也能在对应的主题下查找到，为了确保连接，我们可以在每个服务机器上都部署一个Proxy模式的etcd，这样就可以确保能访问etcd集群的服务都能互相连接\n>\n> **WHY**：我们需要让服务快速透明地接入到计算集群中，让共享配置信息快速被集群中的所有机器发现\n\n看我们的结构图，可以看到我们在12/21/22机器都部署了etcd\n\n![1584701032598](assets/1584701032598.png)\n\n~~~\n# 我们开始制作证书，200机器：\ncerts]# cd /opt/certs/\ncerts]# vi /opt/certs/ca-config.json\n{\n    \"signing\": {\n        \"default\": {\n            \"expiry\": \"1752000h\"\n        },\n        \"profiles\": {\n            \"server\": {\n                \"expiry\": \"1752000h\",\n                \"usages\": [\n                    \"signing\",\n                    \"key encipherment\",\n                    \"server auth\"\n                ]\n            },\n            \"client\": {\n                \"expiry\": \"1752000h\",\n                \"usages\": [\n                    \"signing\",\n                    \"key encipherment\",\n                    \"client auth\"\n                ]\n            },\n            \"peer\": {\n                \"expiry\": \"1752000h\",\n                \"usages\": [\n                    \"signing\",\n                    \"key encipherment\",\n                    \"server auth\",\n                    \"client auth\"\n                ]\n            }\n        }\n    }\n}\n\n# 下面hosts添加11、12、21、22机器的内网ip\ncerts]# vi etcd-peer-csr.json\n{\n    \"CN\": \"k8s-etcd\",\n    \"hosts\": [\n        \"172.27.139.122\",\n        \"172.27.139.119\",\n        \"172.27.139.118\",\n        \"172.27.139.120\"\n    ],\n    \"key\": {\n        \"algo\": \"rsa\",\n        \"size\": 2048\n    },\n    \"names\": [\n        {\n            \"C\": \"CN\",\n            \"ST\": \"beijing\",\n            \"L\": \"beijing\",\n            \"O\": \"od\",\n            \"OU\": \"ops\"\n        }\n    ]\n}\n\ncerts]# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=peer etcd-peer-csr.json |cfssl-json -bare etcd-peer\ncerts]# ll\n~~~\n\n> 关于这些json文件是怎么写出来的，答案：官网抄的然后修改，这些没什么重要的你也不需要太在意，后面重要的会说明是从哪里抄出来的\n>\n> **ca-config.json解析：**\n>\n> - expiry：有效期为200年\n> - profiles-server：启动server的时候需要配置证书\n> - profiles-client：client去连接server的时候需要证书\n> - profiles-peer：双向证书，服务端找客户端需要证书，客户端找服务端需要证书\n>\n> **etcd-peer-csr解析：**\n>\n> - hosts：etcd有可能部署到哪些组件的IP都要填进来\n>\n> **cfssl gencert**：生成证书\n\n<img src=\"assets/WX20230512-100209@2x.png\"  />\n\n~~~\n# 12/21/22机器，安装etcd：\n~]# mkdir /opt/src\n~]# cd /opt/src/\n# 创建用户\nsrc]# useradd -s /sbin/nologin -M etcd\nsrc]# id etcd\n\n# 到GitHub下载或者直接用我给得安装包 https://github.com/etcd-io/etcd/releases/tag/v3.1.20，百度云https://pan.baidu.com/s/1arE2LdtAbcR80gmIQtIELw 提取码：ouy1\nsrc]# wget https://github.com/etcd-io/etcd/releases/download/v3.1.20/etcd-v3.1.20-linux-amd64.tar.gz\nsrc]# tar xf etcd-v3.1.20-linux-amd64.tar.gz -C /opt\nsrc]# cd /opt\nopt]# mv etcd-v3.1.20-linux-amd64/ etcd-v3.1.20\nopt]# ln -s /opt/etcd-v3.1.20/ /opt/etcd\nopt]# cd etcd\n~~~\n\n> **tag**：可以加入，解开备份文件内的文件\n>\n> - **x**：解压\n> - **f** ：使用档案名字\n> - **-C**：切换到指定的目录\n> - 整条命令合起来就是，把tgz文件以tgz文件名为名字解压到opt目录下，并保存tgz文件原样\n>\n> **ln**：为某一个文件在另外一个位置建立一个同步的链接\n>\n> - `语法:ln [参数][源文件或目录][目标文件或目录]`\n> - **-s**：软连接，可以对整个目录进行链接\n>\n> **useradd**：建立用户帐号\n>\n> **-s**：指定用户登入后所使用的shell\n>\n> **-M**：不要自动建立用户的登入目录\n\n<img src=\"assets/WX20230511-194515@2x.png\"  />\n\n~~~\n# 12/21/22机器：\netcd]# mkdir -p /opt/etcd/certs /data/etcd /data/logs/etcd-server\netcd]# cd certs/\ncerts]# scp hdss7-200:/opt/certs/ca.pem .\n# 输入200虚机密码\ncerts]# scp hdss7-200:/opt/certs/etcd-peer*.pem .\ncerts]# cd ..\n# 3台机器，如果是21机器，这下面得12都得改成21，对应的ip换成21的内网ip，initial-cluster则是全部机器都有不需要改，一共5处：etcd-server-7-12、listen-peer-urls后、client-urls后、advertise-peer-urls后、advertise-client-urls后，可以用:%s/旧ip/新ip/g命令进行批量更换(退出insert后粘贴)\netcd]# vi /opt/etcd/etcd-server-startup.sh\n#!/bin/sh\n./etcd --name etcd-server-7-12 \\\n       --data-dir /data/etcd/etcd-server \\\n       --listen-peer-urls https://172.27.139.119:2380 \\\n       --listen-client-urls https://172.27.139.119:2379,http://127.0.0.1:2379 \\\n       --quota-backend-bytes 8000000000 \\\n       --initial-advertise-peer-urls https://172.27.139.119:2380 \\\n       --advertise-client-urls https://172.27.139.119:2379,http://127.0.0.1:2379 \\\n       --initial-cluster  etcd-server-7-12=https://172.27.139.119:2380,etcd-server-7-21=https://172.27.139.118:2380,etcd-server-7-22=https://172.27.139.120:2380 \\\n       --ca-file ./certs/ca.pem \\\n       --cert-file ./certs/etcd-peer.pem \\\n       --key-file ./certs/etcd-peer-key.pem \\\n       --client-cert-auth  \\\n       --trusted-ca-file ./certs/ca.pem \\\n       --peer-ca-file ./certs/ca.pem \\\n       --peer-cert-file ./certs/etcd-peer.pem \\\n       --peer-key-file ./certs/etcd-peer-key.pem \\\n       --peer-client-cert-auth \\\n       --peer-trusted-ca-file ./certs/ca.pem \\\n       --log-output stdout\n\netcd]# chmod +x etcd-server-startup.sh\netcd]# chown -R etcd.etcd /opt/etcd-v3.1.20/\netcd]# chown -R etcd.etcd /data/etcd/\netcd]# chown -R etcd.etcd /data/logs/etcd-server/\netcd]# ll\n~~~\n\n> **scp**：用于 *Linux* 之间复制文件和目录\n>\n> **带***：表示模糊匹配任意内容\n>\n> **chmod**：添加权限\n>\n> - **+x**：给当前用户添加可执行该文件的权限权限\n>\n> **chown**：指定文件的拥有者改为指定的用户或组\n>\n> - **-R**：处理指定目录以及其子目录下的所有文件\n> - 这里即是把/opt/etcd...等的拥有者给etcd用户\n>\n> **ll**：列出权限、大小、名称等信息\n\n<img src=\"assets/WX20230512-105320@2x.png\"  />\n\n~~~\n# 12/21/22机器，我们同时需要supervisor（守护进程工具）来确保etcd是启动的，后面还会不断用到：\n# 这个我们前面已经装了 yum install supervisor -y\netcd]# systemctl start supervisord\netcd]# systemctl enable supervisord\n# 注意修改下面得7-12，对应上机器，如21机器就是7-21，一共一处：[program:etcd-server-7-12]\netcd]# vi /etc/supervisord.d/etcd-server.ini\n[program:etcd-server-7-12]\ncommand=/opt/etcd/etcd-server-startup.sh                        ; the program (relative uses PATH, can take args)\nnumprocs=1                                                      ; number of processes copies to start (def 1)\ndirectory=/opt/etcd                                             ; directory to cwd to before exec (def no cwd)\nautostart=true                                                  ; start at supervisord start (default: true)\nautorestart=true                                                ; retstart at unexpected quit (default: true)\nstartsecs=30                                                    ; number of secs prog must stay running (def. 1)\nstartretries=3                                                  ; max # of serial start failures (default 3)\nexitcodes=0,2                                                   ; 'expected' exit codes for process (default 0,2)\nstopsignal=QUIT                                                 ; signal used to kill process (default TERM)\nstopwaitsecs=10                                                 ; max num secs to wait b4 SIGKILL (default 10)\nuser=etcd                                                       ; setuid to this UNIX account to run the program\nredirect_stderr=true                                            ; redirect proc stderr to stdout (default false)\nstdout_logfile=/data/logs/etcd-server/etcd.stdout.log           ; stdout log path, NONE for none; default AUTO\nstdout_logfile_maxbytes=64MB                                    ; max # logfile bytes b4 rotation (default 50MB)\nstdout_logfile_backups=4                                        ; # of stdout logfile backups (default 10)\nstdout_capture_maxbytes=1MB                                     ; number of bytes in 'capturemode' (default 0)\nstdout_events_enabled=false                                     ; emit events on stdout writes (default false)\n\netcd]# supervisorctl update\n# out：etcd-server-7-21: added process group\netcd]# supervisorctl status\n# out: etcd-server-7-12                 RUNNING   pid 16582, uptime 0:00:59\netcd]# netstat -luntp|grep etcd\n# 必须是监听了2379和2380这两个端口才算成功\ntcp        0      0 172.27.139.119:2379     0.0.0.0:*               LISTEN      14903/./etcd        \ntcp        0      0 127.0.0.1:2379          0.0.0.0:*               LISTEN      14903/./etcd        \ntcp        0      0 172.27.139.119:2380     0.0.0.0:*               LISTEN      14903/./etcd \n~~~\n\n> **systemctl enable**：开机启动\n>\n> **update**：更新\n>\n> **netstat -luntp：**查看端口和进程情况\n>\n> 现在你可以感觉到，supervisor守护进程也仅仅是你配好ini文件即可\n\n~~~\n# 任意节点（12/21/22）检测集群健康状态的两种方法\n22 etcd]# ./etcdctl cluster-health\n22 etcd]# ./etcdctl member list\n~~~\n\n<img src=\"assets/WX20230512-154055@2x.png\"  />\n\n> 这里你再哪个机器先update，哪个机器就是leader\n\n完成\n\n\n\n### 部署API-server集群\n\n[kubernetes官网](https://github.com/kubernetes/kubernetes)\n\n根据架构图，我们把运算节点部署在21和22机器\n\n![1584701070750](assets/1584701070750.png)\n\n~~~\n# 21/22机器\netcd]# cd /opt/src/\n# 可以去官网下载也可以用我的包，百度云盘https://pan.baidu.com/s/1arE2LdtAbcR80gmIQtIELw 提取码：ouy1\nsrc]# wget https://dl.k8s.io/v1.15.2/kubernetes-server-linux-amd64.tar.gz\nsrc]# mv kubernetes-server-linux-amd64.tar.gz kubernetes-server-linux-amd64-v1.15.2.tar.gz\n\nsrc]# tar xf kubernetes-server-linux-amd64-v1.15.2.tar.gz -C /opt/\nsrc]# cd /opt\nopt]# mv kubernetes/ kubernetes-v1.15.2\nopt]# ln -s /opt/kubernetes-v1.15.2/ /opt/kubernetes\nopt]# cd kubernetes\n# 删掉不需要的文件\nkubernetes]# rm -rf kubernetes-src.tar.gz\nkubernetes]# cd server/bin\nbin]# rm -f *.tar\nbin]# rm -f *_tag\nbin]# ll\n~~~\n\n> **tar xf -C**：解压到某个文件夹\n>\n> **mv**：移动到哪里\n>\n> **ln -s**：建立软连接\n>\n> **rm**：删除一个文件或者目录\n>\n> - **-r**：将目录及以下之档案亦逐一删除\n> - **-f**：直接删除，无需逐一确认（你可以试试先不加-f去删除）\n> - 加起来就是强制删除\n>\n> ***.tar**： 这里的*的意思是模糊法，即只要你的结尾是.tar的都匹配上加上rm，就是把所有.tar结尾的文件都删除\n\n<img src=\"assets/WX20230512-113733@2x.png\"  />\n\n~~~\n# 200机器，签发client证书：\n200 ~]# cd /opt/certs/\n200 certs]# vi client-csr.json\n{\n    \"CN\": \"k8s-node\",\n    \"hosts\": [\n    ],\n    \"key\": {\n        \"algo\": \"rsa\",\n        \"size\": 2048\n    },\n    \"names\": [\n        {\n            \"C\": \"CN\",\n            \"ST\": \"beijing\",\n            \"L\": \"beijing\",\n            \"O\": \"od\",\n            \"OU\": \"ops\"\n        }\n    ]\n}\n\n200 certs]#  cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=client client-csr.json |cfssl-json -bare client\n200 certs]# ll\n~~~\n\n<img src=\"assets/WX20230512-133747@2x.png\" />\n\n~~~\n# 200机器，给API-server做证书，其中\"127.0.0.1\"和\"192.168.0.1\"不变，下面4个ip分别是当前网段.10做虚拟vip、21机器ip、22机器ip以及预留的23机器ip（后续如果需要则扩容，也可以不填）\n200 certs]# vi apiserver-csr.json\n{\n    \"CN\": \"k8s-apiserver\",\n    \"hosts\": [\n        \"127.0.0.1\",\n        \"192.168.0.1\",\n        \"kubernetes.default\",\n        \"kubernetes.default.svc\",\n        \"kubernetes.default.svc.cluster\",\n        \"kubernetes.default.svc.cluster.local\",\n        \"172.27.139.10\",\n        \"172.27.139.118\",\n        \"172.27.139.120\",\n        \"172.27.139.123\"\n    ],\n    \"key\": {\n        \"algo\": \"rsa\",\n        \"size\": 2048\n    },\n    \"names\": [\n        {\n            \"C\": \"CN\",\n            \"ST\": \"beijing\",\n            \"L\": \"beijing\",\n            \"O\": \"od\",\n            \"OU\": \"ops\"\n        }\n    ]\n}\n\n200 certs]# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=server apiserver-csr.json |cfssl-json -bare apiserver\n200 certs]# ll\n~~~\n\n> 请确保作为虚拟vip172.27.139.10并不真实存在，ping 172.27.139.10应该是不通的\n\n<img src=\"assets/WX20230512-141539@2x.png\"  />\n\n~~~\n# 21/22机器：\ncd /opt/kubernetes/server/bin\nbin]# mkdir cert\nbin]# cd cert/\n# 把证书考过来，一共6个\ncert]# scp hdss7-200:/opt/certs/c*.pem .\ncert]# scp hdss7-200:/opt/certs/apiserver*.pem .\n~~~\n\n> **scp**：用于 *Linux* 之间复制文件和目录\n>\n> **带***：表示模糊匹配任意内容\n\n~~~\n# 21/22机器：\ncert]# ll\ntotal 24\n-rw------- 1 root root 1675 May 12 14:22 apiserver-key.pem\n-rw-r--r-- 1 root root 1602 May 12 14:22 apiserver.pem\n-rw------- 1 root root 1679 May 12 14:24 ca-key.pem\n-rw-r--r-- 1 root root 1346 May 12 14:24 ca.pem\n-rw------- 1 root root 1675 May 12 14:24 client-key.pem\n-rw-r--r-- 1 root root 1367 May 12 14:24 client.pem\ncert]# pwd\n/opt/kubernetes/server/bin/cert\n~~~\n\n~~~\n# 21/22机器：\ncd /opt/kubernetes/server/bin\nbin]# mkdir conf\nbin]# cd conf/\nconf]# vi audit.yaml\napiVersion: audit.k8s.io/v1beta1 # This is required.\nkind: Policy\n# Don't generate audit events for all requests in RequestReceived stage.\nomitStages:\n  - \"RequestReceived\"\nrules:\n  # Log pod changes at RequestResponse level\n  - level: RequestResponse\n    resources:\n    - group: \"\"\n      # Resource \"pods\" doesn't match requests to any subresource of pods,\n      # which is consistent with the RBAC policy.\n      resources: [\"pods\"]\n  # Log \"pods/log\", \"pods/status\" at Metadata level\n  - level: Metadata\n    resources:\n    - group: \"\"\n      resources: [\"pods/log\", \"pods/status\"]\n\n  # Don't log requests to a configmap called \"controller-leader\"\n  - level: None\n    resources:\n    - group: \"\"\n      resources: [\"configmaps\"]\n      resourceNames: [\"controller-leader\"]\n\n  # Don't log watch requests by the \"system:kube-proxy\" on endpoints or services\n  - level: None\n    users: [\"system:kube-proxy\"]\n    verbs: [\"watch\"]\n    resources:\n    - group: \"\" # core API group\n      resources: [\"endpoints\", \"services\"]\n\n  # Don't log authenticated requests to certain non-resource URL paths.\n  - level: None\n    userGroups: [\"system:authenticated\"]\n    nonResourceURLs:\n    - \"/api*\" # Wildcard matching.\n    - \"/version\"\n\n  # Log the request body of configmap changes in kube-system.\n  - level: Request\n    resources:\n    - group: \"\" # core API group\n      resources: [\"configmaps\"]\n    # This rule only applies to resources in the \"kube-system\" namespace.\n    # The empty string \"\" can be used to select non-namespaced resources.\n    namespaces: [\"kube-system\"]\n\n  # Log configmap and secret changes in all other namespaces at the Metadata level.\n  - level: Metadata\n    resources:\n    - group: \"\" # core API group\n      resources: [\"secrets\", \"configmaps\"]\n\n  # Log all other resources in core and extensions at the Request level.\n  - level: Request\n    resources:\n    - group: \"\" # core API group\n    - group: \"extensions\" # Version of group should NOT be included.\n\n  # A catch-all rule to log all other requests at the Metadata level.\n  - level: Metadata\n    # Long-running requests like watches that fall under this rule will not\n    # generate an audit event in RequestReceived.\n    omitStages:\n      - \"RequestReceived\"\n      \nconf]# cd ..\n# 下面有3处ip需要改，--etcd-servers分别改成12、21、22机器的IP\nbin]# vi /opt/kubernetes/server/bin/kube-apiserver.sh\n#!/bin/bash\n./kube-apiserver \\\n  --apiserver-count 2 \\\n  --audit-log-path /data/logs/kubernetes/kube-apiserver/audit-log \\\n  --audit-policy-file ./conf/audit.yaml \\\n  --authorization-mode RBAC \\\n  --client-ca-file ./cert/ca.pem \\\n  --requestheader-client-ca-file ./cert/ca.pem \\\n  --enable-admission-plugins NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota \\\n  --etcd-cafile ./cert/ca.pem \\\n  --etcd-certfile ./cert/client.pem \\\n  --etcd-keyfile ./cert/client-key.pem \\\n  --etcd-servers https://172.27.139.119:2379,https://172.27.139.118:2379,https://172.27.139.120:2379 \\\n  --service-account-key-file ./cert/ca-key.pem \\\n  --service-cluster-ip-range 192.168.0.0/16 \\\n  --service-node-port-range 3000-29999 \\\n  --target-ram-mb=1024 \\\n  --kubelet-client-certificate ./cert/client.pem \\\n  --kubelet-client-key ./cert/client-key.pem \\\n  --log-dir  /data/logs/kubernetes/kube-apiserver \\\n  --tls-cert-file ./cert/apiserver.pem \\\n  --tls-private-key-file ./cert/apiserver-key.pem \\\n  --v 2\n  \nbin]# chmod +x kube-apiserver.sh\n# 一处修改：[program:kube-apiserver-7-21]，22机器则改成7-22\nbin]# vi /etc/supervisord.d/kube-apiserver.ini\n[program:kube-apiserver-7-21]\ncommand=/opt/kubernetes/server/bin/kube-apiserver.sh            ; the program (relative uses PATH, can take args)\nnumprocs=1                                                      ; number of processes copies to start (def 1)\ndirectory=/opt/kubernetes/server/bin                            ; directory to cwd to before exec (def no cwd)\nautostart=true                                                  ; start at supervisord start (default: true)\nautorestart=true                                                ; retstart at unexpected quit (default: true)\nstartsecs=30                                                    ; number of secs prog must stay running (def. 1)\nstartretries=3                                                  ; max # of serial start failures (default 3)\nexitcodes=0,2                                                   ; 'expected' exit codes for process (default 0,2)\nstopsignal=QUIT                                                 ; signal used to kill process (default TERM)\nstopwaitsecs=10                                                 ; max num secs to wait b4 SIGKILL (default 10)\nuser=root                                                       ; setuid to this UNIX account to run the program\nredirect_stderr=true                                            ; redirect proc stderr to stdout (default false)\nstdout_logfile=/data/logs/kubernetes/kube-apiserver/apiserver.stdout.log        ; stderr log path, NONE for none; default AUTO\nstdout_logfile_maxbytes=64MB                                    ; max # logfile bytes b4 rotation (default 50MB)\nstdout_logfile_backups=4                                        ; # of stdout logfile backups (default 10)\nstdout_capture_maxbytes=1MB                                     ; number of bytes in 'capturemode' (default 0)\nstdout_events_enabled=false                                     ; emit events on stdout writes (default false)\n\nbin]# mkdir -p /data/logs/kubernetes/kube-apiserver\nbin]# supervisorctl update\n# 查看21/22两台机器是否跑起来了，可能比较慢在STARTING，等10秒\nbin]# supervisorctl status\n~~~\n\n> **mkdir -p**：创建目录，没有上一级目录则创建\n>\n> **supervisorctl update**：更新supervisorctl\n>\n> **audit.yaml解析：**\n>\n> - 可以参考这篇文章[点击跳转](https://www.baidu.com/link?url=tFECOG31lKlcqDWeAZGF1VyjhzVAN9vUKHKEKKw5G8y0AC8MKpJxSZeL647MIFdw&wd=&eqid=dafe84b80019e4a3000000065e51d2e2)，当然这里的audit.yaml可能会有些不一样，但是我们后面用到的yaml文件就很相似了\n\n<img src=\"assets/WX20230512-143347@2x.png\"  />\n\n> <a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E5%8E%9F%E7%90%86%E5%8F%8A%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/Kubernetes%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5.md#pod%E7%9A%84%E5%87%A0%E7%A7%8D%E7%8A%B6%E6%80%81\">Pod的几种状态</a>\n\n完成\n\n\n\n### 安装部署主控节点L4反代服务\n\n根据我们架构图，在11/12机器上做反代\n\n![1584701103579](assets/1584701103579.png)\n\n安装nginx时另一个注意事项 <a href=\"https://github.com/ben1234560/k8s_PaaS/issues/16\">点击链接  </a>\n\n（感谢 https://github.com/nangongchengfeng/）\n\n~~~\n# 11/12机器及21/22机器（考虑到公有云的网络情况，我们将21/22机器同时也作为备反代节点）\n# 这个前面已经安装 yum install nginx nginx-mod-stream -y\n# 添加在最下面，server处需要改成自己的21、22机器IP\n~]# vi /etc/nginx/nginx.conf\nstream {\n    upstream kube-apiserver {\n        server 172.27.139.118:6443     max_fails=3 fail_timeout=30s;\n        server 172.27.139.120:6443     max_fails=3 fail_timeout=30s;\n    }\n    server {\n        listen 7443;\n        proxy_connect_timeout 2s;\n        proxy_timeout 900s;\n        proxy_pass kube-apiserver;\n    }\n}\n\n~]# nginx -t\n~]# systemctl start nginx\n~]# systemctl enable nginx\n\n# 这个前面已经安装 yum install keepalived -y\n# keepalived 监控端口脚本\n~]# vi /etc/keepalived/check_port.sh\n#!/bin/bash\nCHK_PORT=$1\nif [ -n \"$CHK_PORT\" ];then\n        PORT_PROCESS=`ss -lnt|grep $CHK_PORT|wc -l`\n        if [ $PORT_PROCESS -eq 0 ];then\n                echo \"Port $CHK_PORT Is Not Used,End.\"\n                exit 1\n        fi\nelse\n        echo \"Check Port Cant Be Empty!\"\nfi\n\n~]# chmod +x /etc/keepalived/check_port.sh\n~~~\n\n> 由于7443端口未监听，Nginx 启动报 [emerg] bind() failed的可以参考[这个方法](https://blog.csdn.net/RunSnail2018/article/details/81185138)（感谢https://gitee.com/wangming91/）\n>\n> 上述代码解析：`upstream` 块：定义了名为 `kube-apiserver` 的上游服务器组。在这个例子中，该组由两个服务器组成，指定了上游服务器的 IP 地址和端口。`server` 块：定义了一个代理服务器，它监听本地端口 `7443`，并将请求代理到上游服务器组 `kube-apiserver`\n>\n> 通过这个配置，当有请求发送到代理服务器的 `7443` 端口时，Nginx 将会将请求转发到 `kube-apiserver` 上游服务器组中的一个服务器上进行处理。这可以用于代理和负载均衡对 `kube-apiserver` 的请求。\n>\n> **yum install -y**：安装并自动yes\n>\n> **nginx -t**：确定nginx.conf有没有语法错误\n>\n> **systemctl start**：启动服务\n>\n> **systemctl enable**：开机自启\n\n~~~\n# 仅以下分主从操作：\n# 把原有内容都删掉，命令行快速按打出dG\n# 注意，下面的vrrp_instance下的interface，我的机器是eth0配置了网卡，有的版本是ens33配置网卡，可以用ifconfig查看，第一行就是，如果你是ens33，改这个interface ens33\n# keepalived 主（即11机器），修改router_id、mcast_src_ip两处为11机器的ip，修改virtual_ipaddress为虚拟vip 10:\n11 ~]# vi /etc/keepalived/keepalived.conf\n! Configuration File for keepalived\n\nglobal_defs {\n   router_id 172.27.139.122\n\n}\n\nvrrp_script chk_nginx {\n    script \"/etc/keepalived/check_port.sh 7443\"\n    interval 2\n    weight -20\n}\n\nvrrp_instance VI_1 {\n    state MASTER\n    interface eth0\n    virtual_router_id 251\n    priority 100\n    advert_int 1\n    mcast_src_ip 172.27.139.122\n    nopreempt\n\n    authentication {\n        auth_type PASS\n        auth_pass 11111111\n    }\n    track_script {\n         chk_nginx\n    }\n    virtual_ipaddress {\n        172.27.139.10\n    }\n}\n\n# keepalived从（即12/21/22机器），修改router_id、mcast_src_ip两处为12或21或22机器的ip，修改virtual_ipaddress为虚拟vip 10:\n12/21/22 ~]# vi /etc/keepalived/keepalived.conf\n! Configuration File for keepalived\nglobal_defs {\n\trouter_id 172.27.139.119\n}\nvrrp_script chk_nginx {\n\tscript \"/etc/keepalived/check_port.sh 7443\"\n\tinterval 2\n\tweight -20\n}\nvrrp_instance VI_1 {\n\tstate BACKUP\n\tinterface eth0\n\tvirtual_router_id 251\n\tmcast_src_ip 172.27.139.119\n\tpriority 90\n\tadvert_int 1\n\tauthentication {\n\t\tauth_type PASS\n\t\tauth_pass 11111111\n\t}\n\ttrack_script {\n\t\tchk_nginx\n\t}\n\tvirtual_ipaddress {\n\t\t172.27.139.10\n\t}\n}\n~~~\n\n启动keepalived\n\n~~~\n# 11/12/21/22机器\n~]# systemctl start keepalived\n~]# systemctl enable keepalived\n# 在11/12/21/22机器\n11 ~]# ip add\n~~~\n\n<img src=\"assets/WX20230512-144506@2x.png\" />\n\n确保21/22机器能够telnet通虚拟vip 10的7443端口\n\n~~~\n21 ~]# telnet 172.27.139.10 7443\nTrying 172.27.139.10...\nConnected to 172.27.139.10.\nEscape character is '^]'.\n\n22 ~]# telnet 172.27.139.10 7443\nTrying 172.27.139.10...\nConnected to 172.27.139.10.\nEscape character is '^]'.\n~~~\n\n完成\n\n\n\n### 安装部署controller-manager（节点控制器/调度器服务）\n\n让我们再搬出我们的架构图\n\n![1584701137068](assets/1584701137068.png)\n\n~~~\n# 21/22机器：\nbin]# vi /opt/kubernetes/server/bin/kube-controller-manager.sh\n#!/bin/sh\n./kube-controller-manager \\\n  --cluster-cidr 172.7.0.0/16 \\\n  --leader-elect true \\\n  --log-dir /data/logs/kubernetes/kube-controller-manager \\\n  --master http://127.0.0.1:8080 \\\n  --service-account-private-key-file ./cert/ca-key.pem \\\n  --service-cluster-ip-range 192.168.0.0/16 \\\n  --root-ca-file ./cert/ca.pem \\\n  --v 2\n\nbin]# chmod +x /opt/kubernetes/server/bin/kube-controller-manager.sh\nbin]# mkdir -p /data/logs/kubernetes/kube-controller-manager\n# 注意22机器，下面要改成7-22，一处修改：manager-7-21]\nbin]# vi /etc/supervisord.d/kube-conntroller-manager.ini\n[program:kube-controller-manager-7-21]\ncommand=/opt/kubernetes/server/bin/kube-controller-manager.sh                     ; the program (relative uses PATH, can take args)\nnumprocs=1                                                                        ; number of processes copies to start (def 1)\ndirectory=/opt/kubernetes/server/bin                                              ; directory to cwd to before exec (def no cwd)\nautostart=true                                                                    ; start at supervisord start (default: true)\nautorestart=true                                                                  ; retstart at unexpected quit (default: true)\nstartsecs=30                                                                      ; number of secs prog must stay running (def. 1)\nstartretries=3                                                                    ; max # of serial start failures (default 3)\nexitcodes=0,2                                                                     ; 'expected' exit codes for process (default 0,2)\nstopsignal=QUIT                                                                   ; signal used to kill process (default TERM)\nstopwaitsecs=10                                                                   ; max num secs to wait b4 SIGKILL (default 10)\nuser=root                                                                         ; setuid to this UNIX account to run the program\nredirect_stderr=true                                                              ; redirect proc stderr to stdout (default false)\nstdout_logfile=/data/logs/kubernetes/kube-controller-manager/controller.stdout.log  ; stderr log path, NONE for none; default AUTO\nstdout_logfile_maxbytes=64MB                                                      ; max # logfile bytes b4 rotation (default 50MB)\nstdout_logfile_backups=4                                                          ; # of stdout logfile backups (default 10)\nstdout_capture_maxbytes=1MB                                                       ; number of bytes in 'capturemode' (default 0)\nstdout_events_enabled=false                                                       ; emit events on stdout writes (default false)\n\nbin]# supervisorctl update\nbin]# vi /opt/kubernetes/server/bin/kube-scheduler.sh\n#!/bin/sh\n./kube-scheduler \\\n  --leader-elect  \\\n  --log-dir /data/logs/kubernetes/kube-scheduler \\\n  --master http://127.0.0.1:8080 \\\n  --v 2\n  \nbin]# chmod +x /opt/kubernetes/server/bin/kube-scheduler.sh\nbin]# mkdir -p /data/logs/kubernetes/kube-scheduler\n# 注意改机器号，一处修改：scheduler-7-21]\nbin]# vi /etc/supervisord.d/kube-scheduler.ini\n[program:kube-scheduler-7-21]\ncommand=/opt/kubernetes/server/bin/kube-scheduler.sh                     ; the program (relative uses PATH, can take args)\nnumprocs=1                                                               ; number of processes copies to start (def 1)\ndirectory=/opt/kubernetes/server/bin                                     ; directory to cwd to before exec (def no cwd)\nautostart=true                                                           ; start at supervisord start (default: true)\nautorestart=true                                                         ; retstart at unexpected quit (default: true)\nstartsecs=30                                                             ; number of secs prog must stay running (def. 1)\nstartretries=3                                                           ; max # of serial start failures (default 3)\nexitcodes=0,2                                                            ; 'expected' exit codes for process (default 0,2)\nstopsignal=QUIT                                                          ; signal used to kill process (default TERM)\nstopwaitsecs=10                                                          ; max num secs to wait b4 SIGKILL (default 10)\nuser=root                                                                ; setuid to this UNIX account to run the program\nredirect_stderr=true                                                     ; redirect proc stderr to stdout (default false)\nstdout_logfile=/data/logs/kubernetes/kube-scheduler/scheduler.stdout.log ; stderr log path, NONE for none; default AUTO\nstdout_logfile_maxbytes=64MB                                             ; max # logfile bytes b4 rotation (default 50MB)\nstdout_logfile_backups=4                                                 ; # of stdout logfile backups (default 10)\nstdout_capture_maxbytes=1MB                                              ; number of bytes in 'capturemode' (default 0)\nstdout_events_enabled=false                                              ; emit events on stdout writes (default false)\n\nbin]# supervisorctl update\nbin]# supervisorctl status\netcd-server-7-21                 RUNNING   pid 14765, uptime 4:30:28\nkube-apiserver-7-21              RUNNING   pid 16088, uptime 0:53:15\nkube-controller-manager-7-21     RUNNING   pid 18734, uptime 0:01:56\nkube-scheduler-7-21              RUNNING   pid 18958, uptime 0:00:47\n\nbin]# ln -s /opt/kubernetes/server/bin/kubectl /usr/bin/kubectl\n# 查看集群健康情况，21/22机器：\nbin]# kubectl get cs\nNAME                 STATUS    MESSAGE              ERROR\nscheduler            Healthy   ok                   \ncontroller-manager   Healthy   ok                   \netcd-2               Healthy   {\"health\": \"true\"}   \netcd-1               Healthy   {\"health\": \"true\"}   \netcd-0               Healthy   {\"health\": \"true\"} \n~~~\n\n> **ln -s**：建立软链接\n>\n> **supervisorctl status**：查看supervisor的情况\n>\n> **supervisorctl update**：更新supervisor\n>\n> **kubectl get**：获取列出一个或多个资源的信息\n>\n> - 上面一条命令的意思是：    列出所有cs信息\n\n完成\n\n\n\n### 安装部署运算节点服务（kubelet）\n\n~~~\n# 200机器，签发证书，将虚拟vip10，21、22、23等后续可能扩展的IP加入\n200 certs]# vi kubelet-csr.json\n{\n    \"CN\": \"k8s-kubelet\",\n    \"hosts\": [\n    \"127.0.0.1\",\n    \"172.27.139.10\",\n    \"172.27.139.118\",\n    \"172.27.139.120\",\n    \"172.27.139.123\",\n    \"172.27.139.124\",\n    \"172.27.139.125\",\n    \"172.27.139.126\",\n    \"172.27.139.127\",\n    \"172.27.139.128\"\n    ],\n    \"key\": {\n        \"algo\": \"rsa\",\n        \"size\": 2048\n    },\n    \"names\": [\n        {\n            \"C\": \"CN\",\n            \"ST\": \"beijing\",\n            \"L\": \"beijing\",\n            \"O\": \"od\",\n            \"OU\": \"ops\"\n        }\n    ]\n}\n\n200 certs]# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=server kubelet-csr.json | cfssl-json -bare kubelet\n200 certs]#  ll |grep kubelet\n-rw-r--r-- 1 root root 1115 May 12 15:45 kubelet.csr\n-rw-r--r-- 1 root root  496 May 12 15:45 kubelet-csr.json\n-rw------- 1 root root 1679 May 12 15:45 kubelet-key.pem\n-rw-r--r-- 1 root root 1468 May 12 15:45 kubelet.pem\n~~~\n\n> **kubelet-csr-hosts**：把所有可能用到的IP都放进来\n\n~~~\n# 分发证书，21/22机器\ncert]# cd /opt/kubernetes/server/bin/cert/\ncert]# scp hdss7-200:/opt/certs/kubelet.pem .\ncert]# scp hdss7-200:/opt/certs/kubelet-key.pem .\n\n# 21机器，修改--server的ip为虚拟vip10：\ncert]# cd ../conf/\nconf]# kubectl config set-cluster myk8s \\\n  --certificate-authority=/opt/kubernetes/server/bin/cert/ca.pem \\\n  --embed-certs=true \\\n  --server=https://172.27.139.10:7443 \\\n  --kubeconfig=kubelet.kubeconfig\n\n# out: Cluster \"myk8s\" set.\nconf]# kubectl config set-credentials k8s-node \\\n  --client-certificate=/opt/kubernetes/server/bin/cert/client.pem \\\n  --client-key=/opt/kubernetes/server/bin/cert/client-key.pem \\\n  --embed-certs=true \\\n  --kubeconfig=kubelet.kubeconfig \n\n# out: User \"k8s-node\" set.\nconf]# kubectl config set-context myk8s-context \\\n  --cluster=myk8s \\\n  --user=k8s-node \\\n  --kubeconfig=kubelet.kubeconfig\n\nout: Context \"myk8s-context\" created.\nconf]# kubectl config use-context myk8s-context --kubeconfig=kubelet.kubeconfig\n\n#out: Switched to context \"myk8s-context\".\n# 做权限授权，推荐文章https://www.jianshu.com/p/9991f189495f\nconf]# vi k8s-node.yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: k8s-node\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: system:node\nsubjects:\n- apiGroup: rbac.authorization.k8s.io\n  kind: User\n  name: k8s-node\n  \nconf]# kubectl create -f k8s-node.yaml\n# out: clusterrolebinding.rbac.authorization.k8s.io/k8s-node created\nconf]# kubectl get clusterrolebinding k8s-node -o yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  creationTimestamp: \"2023-05-12T07:50:58Z\"\n  name: k8s-node\n  resourceVersion: \"2709\"\n  selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/k8s-node\n  uid: 704e0b50-857e-48d7-a028-ff46682d2761\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: system:node\nsubjects:\n- apiGroup: rbac.authorization.k8s.io\n  kind: User\n  name: k8s-node\n\n# 22机器，复制21机器即可\ncert]# cd ../conf/\nconf]# scp hdss7-21:/opt/kubernetes/server/bin/conf/kubelet.kubeconfig .\n~~~\n\n> **scp**：用于 *Linux* 之间复制文件和目录\n>\n> **kubectl create -f**  ：通过配置文件名或stdin创建一个集群资源对象\n>\n> **kubectl get ... -o ...**  ：列出Pod以及运行Pod节点信息（你可以试下不加-o和加-o的区别）\n\n~~~\n# 准备pause基础镜像，200机器：\ncerts]# docker pull kubernetes/pause\ncerts]# docker images|grep pause\n# out: kubernetes/pause                latest                     f9d5de079539   8 years ago   240kB\ncerts]# docker tag f9d5de079539 harbor.od.com/public/pause:latest\ncerts]# docker push harbor.od.com/public/pause:latest\n~~~\n\n> **docker pull**：下载镜像\n>\n> **docker images**：列出所有的镜像\n>\n> - 这里加上|grep 管道符是为了过滤出来pause镜像\n>\n> **docker tag**：给镜像打名字\n>\n> **docker push**：将镜像推送到指定的仓库\n\n~~~\n# 21/21机器，注意修改主机名，有一处hostname-override需要改：hdss7-21\nbin]# vi /opt/kubernetes/server/bin/kubelet.sh\n#!/bin/sh\n./kubelet \\\n  --anonymous-auth=false \\\n  --cgroup-driver systemd \\\n  --cluster-dns 192.168.0.2 \\\n  --cluster-domain cluster.local \\\n  --runtime-cgroups=/systemd/system.slice \\\n  --kubelet-cgroups=/systemd/system.slice \\\n  --fail-swap-on=\"false\" \\\n  --client-ca-file ./cert/ca.pem \\\n  --tls-cert-file ./cert/kubelet.pem \\\n  --tls-private-key-file ./cert/kubelet-key.pem \\\n  --hostname-override hdss7-21.host.com \\\n  --image-gc-high-threshold 20 \\\n  --image-gc-low-threshold 10 \\\n  --kubeconfig ./conf/kubelet.kubeconfig \\\n  --log-dir /data/logs/kubernetes/kube-kubelet \\\n  --pod-infra-container-image harbor.od.com/public/pause:latest \\\n  --root-dir /data/kubelet\n  \nconf]# cd /opt/kubernetes/server/bin\nbin]# mkdir -p /data/logs/kubernetes/kube-kubelet /data/kubelet\nbin]# chmod +x kubelet.sh\n# 有一处要修改主机名：kube-kubelet-7-21\nbin]# vi /etc/supervisord.d/kube-kubelet.ini\n[program:kube-kubelet-7-21]\ncommand=/opt/kubernetes/server/bin/kubelet.sh     ; the program (relative uses PATH, can take args)\nnumprocs=1                                        ; number of processes copies to start (def 1)\ndirectory=/opt/kubernetes/server/bin              ; directory to cwd to before exec (def no cwd)\nautostart=true                                    ; start at supervisord start (default: true)\nautorestart=true              \t\t          ; retstart at unexpected quit (default: true)\nstartsecs=30                                      ; number of secs prog must stay running (def. 1)\nstartretries=3                                    ; max # of serial start failures (default 3)\nexitcodes=0,2                                     ; 'expected' exit codes for process (default 0,2)\nstopsignal=QUIT                                   ; signal used to kill process (default TERM)\nstopwaitsecs=10                                   ; max num secs to wait b4 SIGKILL (default 10)\nuser=root                                         ; setuid to this UNIX account to run the program\nredirect_stderr=true                              ; redirect proc stderr to stdout (default false)\nstdout_logfile=/data/logs/kubernetes/kube-kubelet/kubelet.stdout.log   ; stderr log path, NONE for none; default AUTO\nstdout_logfile_maxbytes=64MB                      ; max # logfile bytes b4 rotation (default 50MB)\nstdout_logfile_backups=4                          ; # of stdout logfile backups (default 10)\nstdout_capture_maxbytes=1MB                       ; number of bytes in 'capturemode' (default 0)\nstdout_events_enabled=false                       ; emit events on stdout writes (default false)\n\nbin]# supervisorctl update\nbin]# supervisorctl status\netcd-server-7-21                 RUNNING   pid 14765, uptime 5:02:11\nkube-apiserver-7-21              RUNNING   pid 16088, uptime 1:24:58\nkube-controller-manager-7-21     RUNNING   pid 18734, uptime 0:33:39\nkube-kubelet-7-21                RUNNING   pid 24802, uptime 0:00:53\nkube-scheduler-7-21              RUNNING   pid 18958, uptime 0:32:30\n\nbin]# kubectl get nodes\nNAME                STATUS   ROLES    AGE   VERSION\nhdss7-21.host.com   Ready    <none>   54s   v1.15.2\nhdss7-22.host.com   Ready    <none>   53s   v1.15.2\n\n# 给标签,21/22都给上master,node\nbin]# kubectl label node hdss7-21.host.com node-role.kubernetes.io/master=\nbin]# kubectl label node hdss7-21.host.com node-role.kubernetes.io/node=\nbin]# kubectl label node hdss7-22.host.com node-role.kubernetes.io/master=\nbin]# kubectl label node hdss7-22.host.com node-role.kubernetes.io/node=\nbin]# kubectl get nodes\n~~~\n\n<img src=\"assets/WX20230512-155833@2x.png\"  />\n\n完成\n\n\n\n### 安装部署运算节点服务（kube-proxy）\n\n~~~\n# 200机器，签发证书请求文件：\n200 certs]# vi kube-proxy-csr.json\n{\n    \"CN\": \"system:kube-proxy\",\n    \"key\": {\n        \"algo\": \"rsa\",\n        \"size\": 2048\n    },\n    \"names\": [\n        {\n            \"C\": \"CN\",\n            \"ST\": \"beijing\",\n            \"L\": \"beijing\",\n            \"O\": \"od\",\n            \"OU\": \"ops\"\n        }\n    ]\n}\n\n200 certs]# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=client kube-proxy-csr.json |cfssl-json -bare kube-proxy-client\n200 certs]# ll |grep proxy\n-rw-r--r-- 1 root root 1005 May 12 16:04 kube-proxy-client.csr\n-rw------- 1 root root 1675 May 12 16:04 kube-proxy-client-key.pem\n-rw-r--r-- 1 root root 1379 May 12 16:04 kube-proxy-client.pem\n-rw-r--r-- 1 root root  267 May 12 16:04 kube-proxy-csr.json\n~~~\n\n\n\n~~~\n# 分发证书，21/22机器：\ncd /opt/kubernetes/server/bin/cert\ncert]# scp hdss7-200:/opt/certs/kube-proxy-client.pem .\ncert]# scp hdss7-200:/opt/certs/kube-proxy-client-key.pem .\ncert]# cd ../conf/\n\n# 21机器，--serverd的IP改成虚拟vip 10：\nconf]# kubectl config set-cluster myk8s \\\n  --certificate-authority=/opt/kubernetes/server/bin/cert/ca.pem \\\n  --embed-certs=true \\\n  --server=https://172.27.139.10:7443 \\\n  --kubeconfig=kube-proxy.kubeconfig\n\n# out: Cluster \"myk8s\" set.\nconf]# kubectl config set-credentials kube-proxy \\\n  --client-certificate=/opt/kubernetes/server/bin/cert/kube-proxy-client.pem \\\n  --client-key=/opt/kubernetes/server/bin/cert/kube-proxy-client-key.pem \\\n  --embed-certs=true \\\n  --kubeconfig=kube-proxy.kubeconfig\n\n# out: User \"kube-proxy\" set.\nconf]# kubectl config set-context myk8s-context \\\n  --cluster=myk8s \\\n  --user=kube-proxy \\\n  --kubeconfig=kube-proxy.kubeconfig\n\n# out: Context \"myk8s-context\" created.\nconf]# kubectl config use-context myk8s-context --kubeconfig=kube-proxy.kubeconfig\n\n# out: Switched to context \"myk8s-context\".\n# 22机器\nconf]# scp hdss7-21:/opt/kubernetes/server/bin/conf/kube-proxy.kubeconfig .\n\n\n# 21/22机器：\ncd\n~]# lsmod |grep ip_vs\nip_vs                 145497  0 \nnf_conntrack          137239  7 ip_vs,nf_nat,nf_nat_ipv4,xt_conntrack,nf_nat_masquerade_ipv4,nf_conntrack_netlink,nf_conntrack_ipv4\nlibcrc32c              12644  3 ip_vs,nf_nat,nf_conntrack\n\n~]# vi ipvs.sh\n#!/bin/bash\nipvs_mods_dir=\"/usr/lib/modules/$(uname -r)/kernel/net/netfilter/ipvs\"\nfor i in $(ls $ipvs_mods_dir|grep -o \"^[^.]*\")\ndo\n  /sbin/modinfo -F filename $i &>/dev/null\n  if [ $? -eq 0 ];then\n    /sbin/modprobe $i\n  fi\ndone\n\n~]# chmod +x ipvs.sh\n~]# ./ipvs.sh\n~]# lsmod |grep ip_vs\n~~~\n\n> **lsmod**：显示已载入系统的模块\n>\n> - 后面带的管道符则是过滤出ip_vs来\n>\n> **chmod +x**：给文件添加执行权限\n>\n> **./ipvs.sh**：运行文件\n\n<img src=\"assets/WX20230512-161204@2x.png\" />\n\n~~~\n# 21/22机器：\n~]# cd /opt/kubernetes/server/bin/\n# 注意修改对应的机器ip，有一处hostname-override修改：hdss7-21\nbin]# vi /opt/kubernetes/server/bin/kube-proxy.sh\n#!/bin/sh\n./kube-proxy \\\n  --cluster-cidr 172.7.0.0/16 \\\n  --hostname-override hdss7-21.host.com \\\n  --proxy-mode=ipvs \\\n  --ipvs-scheduler=nq \\\n  --kubeconfig ./conf/kube-proxy.kubeconfig\n  \nbin]# chmod +x kube-proxy.sh\nbin]# mkdir -p /data/logs/kubernetes/kube-proxy\n# 注意机器IP，有一处修改：kube-proxy-7-21]\nbin]# vi /etc/supervisord.d/kube-proxy.ini\n[program:kube-proxy-7-21]\ncommand=/opt/kubernetes/server/bin/kube-proxy.sh                     ; the program (relative uses PATH, can take args)\nnumprocs=1                                                           ; number of processes copies to start (def 1)\ndirectory=/opt/kubernetes/server/bin                                 ; directory to cwd to before exec (def no cwd)\nautostart=true                                                       ; start at supervisord start (default: true)\nautorestart=true                                                     ; retstart at unexpected quit (default: true)\nstartsecs=30                                                         ; number of secs prog must stay running (def. 1)\nstartretries=3                                                       ; max # of serial start failures (default 3)\nexitcodes=0,2                                                        ; 'expected' exit codes for process (default 0,2)\nstopsignal=QUIT                                                      ; signal used to kill process (default TERM)\nstopwaitsecs=10                                                      ; max num secs to wait b4 SIGKILL (default 10)\nuser=root                                                            ; setuid to this UNIX account to run the program\nredirect_stderr=true                                                 ; redirect proc stderr to stdout (default false)\nstdout_logfile=/data/logs/kubernetes/kube-proxy/proxy.stdout.log     ; stderr log path, NONE for none; default AUTO\nstdout_logfile_maxbytes=64MB                                         ; max # logfile bytes b4 rotation (default 50MB)\nstdout_logfile_backups=4                                             ; # of stdout logfile backups (default 10)\nstdout_capture_maxbytes=1MB                                          ; number of bytes in 'capturemode' (default 0)\nstdout_events_enabled=false                                          ; emit events on stdout writes (default false)\n\nbin]# supervisorctl update\nbin]# supervisorctl status\netcd-server-7-21                 RUNNING   pid 14765, uptime 5:21:04\nkube-apiserver-7-21              RUNNING   pid 16088, uptime 1:43:51\nkube-controller-manager-7-21     RUNNING   pid 18734, uptime 0:52:32\nkube-kubelet-7-21                RUNNING   pid 24802, uptime 0:19:46\nkube-proxy-7-21                  RUNNING   pid 32354, uptime 0:00:37\nkube-scheduler-7-21              RUNNING   pid 18958, uptime 0:51:23\n\n# 这个前面已经安装 yum install ipvsadm -y\nbin]# ipvsadm -Ln\nIP Virtual Server version 1.2.1 (size=4096)\nProt LocalAddress:Port Scheduler Flags\n  -> RemoteAddress:Port           Forward Weight ActiveConn InActConn\nTCP  192.168.0.1:443 nq\n  -> 172.27.139.118:6443          Masq    1      0          0         \n  -> 172.27.139.120:6443          Masq    1      0          0  \n\nbin]# kubectl get svc\nNAME         TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)   AGE\nkubernetes   ClusterIP   192.168.0.1   <none>        443/TCP   154m\n~~~\n\n> **ipvsadm**：用于设置、维护和检查Linux内核中虚拟服务器列表的*命令*\n>\n> **ipvsadm -Ln** ：查看当前配置的虚拟服务和各个RS的权重\n\n<img src=\"assets/WX20230512-170820@2x.png\"  />\n\n~~~\n# 验证一下集群，21机器(在任意节点机器，我选的是21)：\ncd\n~]# vi /root/nginx-ds.yaml\napiVersion: extensions/v1beta1\nkind: DaemonSet\nmetadata:\n  name: nginx-ds\nspec:\n  template:\n    metadata:\n      labels:\n        app: nginx-ds\n    spec:\n      containers:\n      - name: my-nginx\n        image: harbor.od.com/public/nginx:v1.7.9\n        ports:\n        - containerPort: 80\n        \n~]# kubectl create -f nginx-ds.yaml\n# out：daemonset.extensions/nginx-ds created\n~]# kubectl get pods -o wide\nNAME             READY   STATUS    RESTARTS   AGE   IP           NODE                NOMINATED NODE   READINESS GATES\nnginx-ds-rp8j5   1/1     Running   0          13s   172.7.22.2   hdss7-22.host.com   <none>           <none>\nnginx-ds-tv6dk   1/1     Running   0          13s   172.7.21.2   hdss7-21.host.com   <none>           <none>\n# 以上是成功的状态，在21/22机器都可以查到\n~~~\n\n> **注意，如果你的pod的wide22的也在21上或者类似情况，那就是你的vi /etc/docker/daemon.json没改好机器名，需要重新做过全部机器**\n>\n> **kubectl create -f**  ：通过配置文件名或stdin创建一个集群资源对象\n>\n> **kubectl get ... -o wide**：显示网络情况\n>\n> **nginx-ds.yaml解析：**\n>\n> - 可以参考这篇文章[点击跳转](https://www.baidu.com/link?url=tFECOG31lKlcqDWeAZGF1VyjhzVAN9vUKHKEKKw5G8y0AC8MKpJxSZeL647MIFdw&wd=&eqid=dafe84b80019e4a3000000065e51d2e2)，这里的nginx-ds.yaml建议自己手敲一遍，敲的同时要知道自己敲的是什么，记住，yaml语法不允许使用Tab键，只允许空格\n\n<img src=\"assets/WX20230512-171106@2x.png\"  />\n\n恭喜:tada::tada::tada:\n\n此时你已经部署好K8S集群！当然只有集群还远远不够，我们还需要更多的东西才能组成我们的PaaS服务，休息一下继续享受它:smiley:\n\n后续暂未推出公有云版本，理论上是完全可行的，如果你是公有云版本且完全部署完成，请一定要告诉我这个好消息\n"
        },
        {
          "name": "第五章——K8S结合CI&CD持续交付和集中管理配置.md",
          "type": "blob",
          "size": 44.5361328125,
          "content": "## 第五章——K8S结合CI&CD持续交付和集中管理配置\n\n#### 交付Dubbo服务到K8S前言\n\n> **WHAT**：阿里巴巴开源的一个高性能优秀的[服务框架](https://baike.baidu.com/item/%E6%9C%8D%E5%8A%A1%E6%A1%86%E6%9E%B6)，使得应用可通过高性能的 RPC 实现服务的输出和输入功能，可以和 [1]  [Spring](https://baike.baidu.com/item/Spring)框架无缝集成。\n>\n> **WHY**：用它来当作我们现实生产中的App业务，交付到我们的PaaS里\n\n![1587274014276](assets/1587274014276.png)\n\n#### 交付架构图\n\n![1582455172381](assets/1582455172381.png)\n\n> **zk（zookeeper）**：dubbo服务的注册中心是通过zk来做的，我们用3个zk组成一个集群，就跟etcd一样，有一个leader两个从，leader死了由其它来决定谁变成leader，因为zk是有状态的服务，所以我们放它放在集群外（红框外），集群内都是无状态的。\n>\n> **dubbo微服务**：在集群内通过点点点扩容（dashboard），即当有秒杀或者什么的时候就可以扩展，过了则缩容。\n>\n> **git**：开发把代码传到git上，这里我们用gitee（码云）来做，也可以用GitHub来着，没什么区别\n>\n> **Jenkins**：用Jenkins把git的代码拉下来并编译打包成镜像，然后提送到harbor\n>\n> **OPS服务器（7-200机器）**：然后将harbor的镜像通过yaml应用到k8s里，现在我们是需要些yaml文件，后面会用spinnaker来做成点点点的方式\n>\n> **笑脸（用户）**：外部访问通过ingress转发到集群内的dubbo消费者（web服务），然后就可以访问\n>\n> **最终目标**：实现所有事情都是点点点\n\n梳理目前机器服务角色\n\n| 主机名             | 角色                      | IP         |\n| :----------------- | ------------------------- | ---------- |\n| HDSS7-11.host.com  | k8s代理节点1，zk1         | 10.4.7.11  |\n| HDSS7-12.host.com  | k8s代理节点1，zk1         | 10.4.7.12  |\n| HDSS7-21.host.com  | k8s运算节点1，zk3         | 10.4.7.21  |\n| HDSS7-22.host.com  | k8s运算节点2，jenkins     | 10.4.7.22  |\n| HDSS7-200.host.com | k8s运维节点（docker仓库） | 10.4.7.200 |\n\n\n\n### 安装部署zookeeper\n\n> **WHAT**：主要是用来解决分布式应用中经常遇到的一些数据管理问题，如：统一命名服务、状态同步服务、集群管理、分布式应用配置项的管理等。简单来说zookeeper=文件系统+监听通知机制。[推荐文章](https://blog.csdn.net/java_66666/article/details/81015302)\n>\n> **WHY**：我们的dubbo服务要注册到zk里，把配置放到zk上，一旦配置信息发生变化，zk将获取到新的配置信息应用到系统中。\n\n~~~\n# 11/12/21机器：\nmkdir /opt/src  # 这步是没有src文件夹才会这么做\ncd /opt/src\nsrc]# 由于zk依赖java环境，下载我上传的jdk-8u221-linux-x64.tar.gz放到这个目录\n# 最好用我上传的，或者yum install -y java-1.8.0-openjdk*\nsrc]# mkdir /usr/java\nsrc]# tar xf jdk-8u221-linux-x64.tar.gz -C /usr/java\nsrc]# ll /usr/java/\nsrc]# ln -s /usr/java/jdk1.8.0_221/ /usr/java/jdk\n# 粘贴到最下面\nsrc]# vi /etc/profile\nexport JAVA_HOME=/usr/java/jdk\nexport PATH=$JAVA_HOME/bin:$JAVA_HOME/bin:$PATH\nexport CLASSPATH=$CLASSPATH:$JAVA_HOME/lib:$JAVA_HOME/lib/tools.jar\n\nsrc]# source /etc/profile\nsrc]# java -version\n# out: java version \"1.8.0_221\"...\n\n# 安装zookeeper\nsrc]# wget https://archive.apache.org/dist/zookeeper/zookeeper-3.4.14/zookeeper-3.4.14.tar.gz\n# 这个下载速度是真的慢，你最好用我上传的包\nsrc]# tar xf zookeeper-3.4.14.tar.gz -C /opt\nsrc]# cd /opt\nopt]# ln -s /opt/zookeeper-3.4.14/ /opt/zookeeper\nopt]# mkdir -pv /data/zookeeper/data /data/zookeeper/logs\nopt]# vi /opt/zookeeper/conf/zoo.cfg\ntickTime=2000\ninitLimit=10\nsyncLimit=5\ndataDir=/data/zookeeper/data\ndataLogDir=/data/zookeeper/logs\nclientPort=2181\nserver.1=zk1.od.com:2888:3888\nserver.2=zk2.od.com:2888:3888\nserver.3=zk3.od.com:2888:3888\n\n~~~\n\n![1583027786123](assets/1583027786123.png)\n\n![1583028859934](assets/1583028859934.png)\n\n~~~\n# 11机器添加解析：\n~]# vi /var/named/od.com.zone\nserial 前滚一个\n# 最下面添加\nzk1                A   10.4.7.11\nzk2                A   10.4.7.12\nzk3                A   10.4.7.21\n\n~]# systemctl restart named\n~]# dig -t A zk1.od.com @10.4.7.11 +short\n# out: 10.4.7.11\n~~~\n\n\n\n~~~\n# 配置myid，11/12/21机器：\n# 注意，11机器配1，12机器配2，21机器配3,需要修改共一处：1\nopt]# vi /data/zookeeper/data/myid\n1\n\nopt]# /opt/zookeeper/bin/zkServer.sh start\nopt]# ps aux|grep zoo\nopt]# netstat -luntp|grep 2181\n~~~\n\n> **ps aux** ：查看进程情况的命令\n\n![1580178079898](assets/1580178079898.png)\n\n完成\n\n\n\n### 安装部署Jenkins\n\n> **WHAT**：[Jenkins中文网](https://www.baidu.com/link?url=W22nPpmtH_sl0ovap9ypYFgfeS67PEutnmslKb9EZvm&wd=&eqid=de484ea10012b26f000000045e534b70)，引用一句话：构建伟大，无所不能\n>\n> **WHY**：我们的之前的镜像是从网上下载下来然后push到harbor里面被应用到K8S里，那么我们自己开发的代码怎么做成镜像呢？就需要用到Jenkins\n\n~~~\n# 200机器：\n~]# docker pull jenkins/jenkins:2.190.3\n~]# docker images|grep jenkins\n~]# docker tag 22b8b9a84dbe harbor.od.com/public/jenkins:v2.190.3\n~]# docker push harbor.od.com/public/jenkins:v2.190.3\n# 下面的密钥生产你要填自己的邮箱\n~]# ssh-keygen -t rsa -b 2048 -C \"909336740@qq.com\" -N \"\" -f /root/.ssh/id_rsa\n# 拿到公钥配置到gitee里面去\n~]# cat /root/.ssh/id_rsa.pub\n# =============公钥配置到gitee的dubbo-demo-web，gitee的配置可以看下图，用我上传的代码包\n~]# mkdir /data/dockerfile\n~]# cd /data/dockerfile\ndockerfile]# mkdir jenkins\ndockerfile]# cd jenkins\njenkins]# vi Dockerfile\nFROM harbor.od.com/public/jenkins:v2.190.3\nUSER root\nRUN /bin/cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime &&\\ \n    echo 'Asia/Shanghai' >/etc/timezone\nADD id_rsa /root/.ssh/id_rsa\nADD config.json /root/.docker/config.json\nADD get-docker.sh /get-docker.sh\nRUN echo \"    StrictHostKeyChecking no\" >> /etc/ssh/ssh_config &&\\\n    /get-docker.sh\n\njenkins]# cp /root/.ssh/id_rsa .\njenkins]# cp /root/.docker/config.json .\njenkins]# curl -fsSL get.docker.com -o get-docker.sh\njenkins]# chmod +x get-docker.sh\n~~~\n\n> 配置公钥和私钥是为了让我们的机器能找到我们的git，也让我们的git代码不被别人随意使用。\n>\n> 以下是gitee操作方法\n\n![1580701689468](assets/1580701689468.png)\n\n![1580701804448](assets/1580701804448.png)\n\n![1580702353514](assets/1580702353514.png)\n\n![1580702308419](assets/1580702308419.png)\n\n![1580702278913](assets/1580702278913.png)\n\n~~~\n> git add . # 全部提交当前目录所有新增文件\n# 然后再执行下面的步骤即可到你的gitee里面看了\n> git commit -m \"first commit\"\n> git remote add origin https://gitee.com/benjas/dubbo-demo-web-test.git\n> git push -u origin master\n~~~\n\n![1582682146083](assets/1582682146083.png)\n\n目录结构必须和我的一样\n\n然后把公钥配置进去，点击添加\n\n![1583032189990](assets/1583032189990.png)\n\n![1583032257343](assets/1583032257343.png)\n\n成功\n\n新建一个infra私有仓库\n\n![1584696754306](assets/1584696754306.png)\n\n~~~\n# 200机器，开始build镜像：\njenkins]# docker build . -t harbor.od.com/infra/jenkins:v2.190.3\njenkins]# docker push harbor.od.com/infra/jenkins:v2.190.3\njenkins]# docker run --rm harbor.od.com/infra/jenkins:v2.190.3 ssh -T git@gitee.com\n~~~\n\n> 如果因为网络原因一直build失败的请往下看（我第三次部署的时候，build了3次才成功），成功也会冒红字，但是有Successfully\n\n![1580179904712](assets/1580179904712.png)\n\n成功\n\n##### build一直失败解决办法：直接用我做好的镜像\n\n~~~\n# 200机器：\ncd /data/dockerfile/jenkins\njenkins]# 把我放在里面的包下载到这里\njenkins]# docker load < jenkins-v2.190.3-with-docker.tar\njenkins]# docker images\n~~~\n\n![1584243805558](assets/1584243805558.png)\n\n~~~\n# 200机器：\njenkins]# docker tag a25e4f7b2896 harbor.od.com/public/jenkins:v2.176.2\njenkins]# docker push harbor.od.com/public/jenkins:v2.176.2\njenkins]# vi Dockerfile\nFROM harbor.od.com/public/jenkins:v2.176.2\n# 删掉 ADD get-docker.sh/get-docker.sh，如下图\n~~~\n\n![1580699790260](assets/1580699790260.png)\n\n~~~\n# 200机器:\njenkins]# docker build . -t harbor.od.com/infra/jenkins:v2.176.2\njenkins]# docker tag 编译镜像的id harbor.od.com/infra/jenkins:v2.190.3\njenkins]# docker push harbor.od.com/infra/jenkins:v2.190.3\njenkins]# docker run --rm harbor.od.com/infra/jenkins:v2.190.3 ssh -T git@gitee.com\n~~~\n\n![1580179904712](assets/1580179904712.png)\n\n成功标识\n\n~~~\n# 21机器，创建名称空间，对应私有化仓库：\n~]# kubectl create ns infra\n~]# kubectl create secret docker-registry harbor --docker-server=harbor.od.com --docker-username=admin --docker-password=Harbor12345 -n infra\n~~~\n\n> **kubectl create secret**创建私有仓库\n>\n> - 后面跟着的是对应的仓库、用户名、用户密码、仓库名称infra\n\n~~~\n# 三部机器，21/22/200，准备共享存储：\n~]# yum install nfs-utils -y\n~~~\n\n\n\n~~~\n# 200机器，做共享存储的客户端：\njenkins]# vi /etc/exports\n/data/nfs-volume 10.4.7.0/24(rw,no_root_squash)\n\njenkins]# mkdir /data/nfs-volume\njenkins]# systemctl start nfs\njenkins]# systemctl enable nfs\njenkins]# cd /data/k8s-yaml/\nk8s-yaml]# mkdir jenkins\ncd jenkins\n# 200机器，准备资源配置清单：\njenkins]# vi dp.yaml\nkind: Deployment\napiVersion: extensions/v1beta1\nmetadata:\n  name: jenkins\n  namespace: infra\n  labels: \n    name: jenkins\nspec:\n  replicas: 1\n  selector:\n    matchLabels: \n      name: jenkins\n  template:\n    metadata:\n      labels: \n        app: jenkins \n        name: jenkins\n    spec:\n      volumes:\n      - name: data\n        nfs: \n          server: hdss7-200\n          path: /data/nfs-volume/jenkins_home\n      - name: docker\n        hostPath: \n          path: /run/docker.sock\n          type: ''\n      containers:\n      - name: jenkins\n        image: harbor.od.com/infra/jenkins:v2.190.3\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n        env:\n        - name: JAVA_OPTS\n          value: -Xmx512m -Xms512m\n        volumeMounts:\n        - name: data\n          mountPath: /var/jenkins_home\n        - name: docker\n          mountPath: /run/docker.sock\n      imagePullSecrets:\n      - name: harbor\n      securityContext: \n        runAsUser: 0\n  strategy:\n    type: RollingUpdate\n    rollingUpdate: \n      maxUnavailable: 1\n      maxSurge: 1\n  revisionHistoryLimit: 7\n  progressDeadlineSeconds: 600\n  \njenkins]# vi svc.yaml\nkind: Service\napiVersion: v1\nmetadata: \n  name: jenkins\n  namespace: infra\nspec:\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 8080\n  selector:\n    app: jenkins\n\njenkins]# vi ingress.yaml\nkind: Ingress\napiVersion: extensions/v1beta1\nmetadata: \n  name: jenkins\n  namespace: infra\nspec:\n  rules:\n  - host: jenkins.od.com\n    http:\n      paths:\n      - path: /\n        backend: \n          serviceName: jenkins\n          servicePort: 80\n          \njenkins]# mkdir /data/nfs-volume/jenkins_home\n# 网页查看：\nk8s-yaml.od.com下有jenkins目录\n# 21机器，测试：\n~]# file /run/docker.sock\n# out: /run/docker.sock.socket\n~~~\n\n\n\n~~~\n# 应用资源配置清单,21机器:\n~]# kubectl apply -f http://k8s-yaml.od.com/jenkins/dp.yaml\n~]# kubectl apply -f http://k8s-yaml.od.com/jenkins/svc.yaml\n~]# kubectl apply -f http://k8s-yaml.od.com/jenkins/ingress.yaml\n~]# kubectl get pods -n infra\n~]# kubectl get all -n infra\n~~~\n\n> 如果是pending状态，一般是你的内存占用过多，你的dashboard可能开不起来了，但是不打紧，照用\n\ninfra名称空间\n\n![1584697062782](assets/1584697062782.png)\n\n启动成功\n\n~~~\n# 11机器，解析域名：\n~]# vi /var/named/od.com.zone\nserial 前滚一个,到07\n# 最下面添加\njenkins            A   10.4.7.10\n\n~]# systemctl restart named\n~]# dig -t A jenkins.od.com @10.4.7.11 +short\n#10.4.7.10\n~~~\n\n[访问jenkins.od.com](jenkins.od.com)\n\n![1583035885506](assets/1583035885506.png)\n\n~~~\n# 200机器找密码，上面的密码在initialAdminPassword里：\ncd /data/nfs-volume/jenkins_home\njenkins_home]# cat secrets/initialAdminPassword\n# cat到的密码粘到上面去\n~~~\n\nSkip Plugin Installations进来\n\n![1583036011230](assets/1583036011230.png)\n\n~~~\n# jenkins账号密码设置，一定要跟我的一样，后面要用到的：\n账号：admin\n密码：admin123\nfull name：admin\n# 然后save->save->start using Jenkins即可\n~~~\n\n![1583036074145](assets/1583036074145.png)\n\n~~~\n# 进来后要做两件事情，第一件事就是调整两安全选项：\n1、allow anonymous read acces 允许匿名用户访问\n2、Prevent cross site request forgery exploits 允许跨域\n~~~\n\n![1583036109324](assets/1583036109324.png)\n\n![1580612141388](assets/1580612141388.png)\n\n#### 安装蓝海\n\n> **WHAT**：从仪表板到各个Pipeline运行的查看分支和结果，使用可视编辑器修改Pipeline作为代码\n>\n> - 连续交付（CD）Pipeline的复杂可视化，允许快速和直观地了解Pipeline的状态（下面回顾构建镜像流程的时候有使用到）\n> - ...\n>\n> **WHY**：当然是为了让我们能更清晰明了的看到构建的情况\n\n![1583036159177](assets/1583036159177.png)\n\n![1583036337529](assets/1583036337529.png)\n\n> 这里如果没有内容的点一下check out，因为读取的慢的问题\n>\n> **ctrl+f**呼出页面搜索\n\n![1583036408736](assets/1583036408736.png)\n\n由于安装还是比较慢的，把这个勾选上就可以去做别的事情了\n\n![1583041128840](assets/1583041128840.png)\n\n重启完后会出现open blue ocean，表示安装成功\n\n> ![1583056880950](assets/1583056880950.png)\n\n完成（而且你再去搜索blue是搜不到的了）\n\n#### 一直下载不了BlueOcean怎么办\n\n方法一（退出再下载法，此方法依然依靠网络）：\n\n回到上一页\n\n![1583036159177](assets/1583036159177.png)\n\n再点击下载\n\n![1583036408736](assets/1583036408736.png)\n\n\n\n可以看到之前Failure的内容又开始下载了，而且有一部分已经成功\n\n![1583056085304](assets/1583056085304.png)\n\n方法二（离线包解压）：\n\n把我上传的jenkins_2.176_plugins.tar.gz下载解压到200机器的/data/nfs-voluem/jenkins_home/plugins/\n\n然后去删掉Jenkins的pod让它自动重启，就有了\n\n![1583056264561](assets/1583056264561.png)\n\n> #### 唠嗑：\n>\n> dashboard目前常用的两个版本：v1.8.3，v1.10.1\n>\n> Jenkins：把源码编译成可执行的二进制码\n\n\n\n### 安装maven\n\n> **WHAT**：一个项目管理工具，可以对 Java 项目进行构建、依赖管理。\n>\n> **WHY**：构建项目镜像时需要\n\n[使用官网的](https://archive.apache.org/dist/maven/maven-3/)，或者用我上传的\n\n![1580692503213](assets/1580692503213.png)\n\n![1584697256442](assets/1584697256442.png)\n\n~~~\n# 200机器：\nsrc]# 网上下载或者用我上传的，拉到这里\nsrc]# ll\nsrc]# mkdir /data/nfs-volume/jenkins_home/maven-3.6.1-8u232\n# 上面这个8u232的232是根据下图种的Jenkins版本的232来确定的\n~~~\n\n![1584697159389](assets/1584697159389.png)\n\n> 下图是EXEC到dashboard里的Jenkins，然后输入java -version\n\n![1584697213960](assets/1584697213960.png)\n\n确保你的Jenkins是没问题的\n\n~~~\n# 进入harbo\ndocker login harbor.od.com\n# 是否能连接gitee\nssh -i /root/.ssh/id_rsa -T git@gitee.com\n~~~\n\n![1583080186411](assets/1583080186411.png)\n\n~~~\n# 200机器：\nsrc]# tar xfv apache-maven-3.6.1-bin.tar.gz -C /data/nfs-volume/jenkins_home/maven-3.6.1-8u232\nsrc]# cd /data/nfs-volume/jenkins_home/maven-3.6.1-8u232\nmaven-3.6.1-8u232]# ll\n# out: apache-maven-3.6.1\nmaven-3.6.1-8u232]# mv apache-maven-3.6.1/ ../\nmaven-3.6.1-8u232]# mv ../apache-maven-3.6.1/* .\nmaven-3.6.1-8u232]# ll\n~~~\n\n![1584697314787](assets/1584697314787.png)\n\n~~~\n# 200机器,修改镜像源成阿里源，增加以下内容：\nmaven-3.6.1-8u232]# vi conf/settings.xml\n    <mirror>\n      <id>nexus-aliyun</id>\n      <mirrorOf>*</mirrorOf>\n      <name>Nexus aliyun</name>\n      <url>https://maven.aliyun.com/repository/public</url>\n      <!-- 旧地址：http://maven.allyun.com/nexus/content/groups/public --> \n    </mirror>\n~~~\n\n![1584697410112](assets/1584697410112.png)\n\n##### 其它知识（不需要操作）:\n\n~~~\n# 200机器,切换jdk的多个版本的方法：\ncd /data/nfs-volume/jenkins_home/\njenkins_home]# 把jdk的另外版本下载下来，直接用我的包\njenkins_home]# tar xf jdk-7u80-linux-x64.tar.gz -C ./\njenkins_home]# cd maven-3.6.1-8u232/\ncd bin/\nbin]# file mvn\n# out: mvn: POSIX shell script, ASCII text executable\nbin]# vi mvn\n#编辑JAVA_HOME 即可指定jdk版本\n~~~\n\n\n\n### 制作dubbo微服务的底包镜像\n\n~~~\n# 200机器：\ncd /data/nfs-volume/jenkins_home/\njenkins_home]# docker pull docker.io/909336740/jre8:8u112\njenkins_home]# docker images|grep jre \njenkins_home]# docker push harbor.od.com/public/jre:8u112\njenkins_home]# cd /data/dockerfile\ndockerfile]# mkdir jre8\njre8]# cd jre8\njre8]# vi Dockerfile\nFROM harbor.od.com/public/jre:8u112\nRUN /bin/cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime &&\\\n    echo 'Asia/Shanghai' >/etc/timezone\nADD config.yml /opt/prom/config.yml\nADD jmx_javaagent-0.3.1.jar /opt/prom/\nWORKDIR /opt/project_dir\nADD entrypoint.sh /entrypoint.sh\nCMD [\"/entrypoint.sh\"]\n\njre8]# wget https://repo1.maven.org/maven2/io/prometheus/jmx/jmx_prometheus_javaagent/0.3.1/jmx_prometheus_javaagent-0.3.1.jar -O jmx_javaagent-0.3.1.jar\njre8]# vi config.yml\n---\nrules:\n  - pattern: '.*'\n\njre8]# vi entrypoint.sh\n#!/bin/sh\nM_OPTS=\"-Duser.timezone=Asia/Shanghai -javaagent:/opt/prom/jmx_javaagent-0.3.1.jar=$(hostname -i):${M_PORT:-\"12346\"}:/opt/prom/config.yml\"\nC_OPTS=${C_OPTS}\nJAR_BALL=${JAR_BALL}\nexec java -jar ${M_OPTS} ${C_OPTS} ${JAR_BALL}\n\njre8]# chmod +x entrypoint.sh\njre8]# ll\n~~~\n\n> **Dockerfile解析**：\n>\n> - RUN 把时区改成上海时区\n> - ADD 给一个监控\n> - ADD 收集jmx的信息\n> - WORKDIR 工作目录\n> - CMD 默认执行脚本\n\n![1580718591217](assets/1580718591217.png)\n\nharbor创建base仓库\n\n![1583076206921](assets/1583076206921.png)\n\n~~~\n# 200机器，开始build镜像：\njre8]# docker build . -t harbor.od.com/base/jre8:8u112\njre8]# docker push harbor.od.com/base/jre8:8u112\n~~~\n\n> 回顾一下我们的交付架构图\n\n![1580997621696](assets/1580997621696.png)\n\n\n\n### 使用Jenkins持续构建交付dubbo服务的提供者\n\n![1583076355039](assets/1583076355039.png)\n\n![1583076388875](assets/1583076388875.png)\n\n![1583076490229](assets/1583076490229.png)\n\n构建十个参数\n\n1. ![1583076572332](assets/1583076572332.png)\n2. ![1583076620958](assets/1583076620958.png)\n3. ![1583076688952](assets/1583076688952.png)\n4. ![1583076729387](assets/1583076729387.png)\n5. ![1583076787792](assets/1583076787792.png)\n6. ![1583076845621](assets/1583076845621.png)\n7. ![1583076895447](assets/1583076895447.png)\n8. ![1583077033366](assets/1583077033366.png)\n9. ![1583077110044](assets/1583077110044.png)\n10. ![1583077171689](assets/1583077171689.png)\n11.  把以下内容填写下面的Adnanced Project Options\n\n~~~shell\npipeline {\n  agent any \n    stages {\n      stage('pull') { //get project code from repo \n        steps {\n          sh \"git clone ${params.git_repo} ${params.app_name}/${env.BUILD_NUMBER} && cd ${params.app_name}/${env.BUILD_NUMBER} && git checkout ${params.git_ver}\"\n        }\n      }\n      stage('build') { //exec mvn cmd\n        steps {\n          sh \"cd ${params.app_name}/${env.BUILD_NUMBER}  && /var/jenkins_home/maven-${params.maven}/bin/${params.mvn_cmd}\"\n        }\n      }\n      stage('package') { //move jar file into project_dir\n        steps {\n          sh \"cd ${params.app_name}/${env.BUILD_NUMBER} && cd ${params.target_dir} && mkdir project_dir && mv *.jar ./project_dir\"\n        }\n      }\n      stage('image') { //build image and push to registry\n        steps {\n          writeFile file: \"${params.app_name}/${env.BUILD_NUMBER}/Dockerfile\", text: \"\"\"FROM harbor.od.com/${params.base_image}\nADD ${params.target_dir}/project_dir /opt/project_dir\"\"\"\n          sh \"cd  ${params.app_name}/${env.BUILD_NUMBER} && docker build -t harbor.od.com/${params.image_name}:${params.git_ver}_${params.add_tag} . && docker push harbor.od.com/${params.image_name}:${params.git_ver}_${params.add_tag}\"\n        }\n      }\n    }\n}\n~~~\n\n![1580783995614](assets/1580783995614.png)\n\n> 注释（流水线脚本）：\n>\n> pull: 把项目克隆到仓库\n>\n> build: 到指定的地方创建\n>\n> package: 用完mvn后打包到project_dir\n>\n> image: 弄到我们的docker仓库\n\n~~~\n填入对应的参数：\napp_name:       dubbo-demo-service\nimage_name:     app/dubbo-demo-service\ngit_repo:       https://gitee.com/benjas/dubbo-demo-service.git\ngit_ver:        master\nadd_tag:        200301_2352\nmvn_dir:        ./\ntarget_dir:     ./dubbo-server/target\nmvn_cmd:        mvn clean package -Dmaven.test.skip=true\nbase_image:     base/jre8:8u112\nmaven:          3.6.1-8u232\n# 注意看脚注，点击Build进行构建，等待构建完成。\n~~~\n\n> **git_repo**：注意的地址是写你的地址\n>\n> **add_tag**：写现在的日期\n\n[^dubbo-service包]: dubbo-service包在我上传的文件夹里面，你下载后拉到你新建的git仓库里面（公开），然后配上你的地址（记得之前的web是配了公钥的，否则找不到，公钥的方法在上面的章节），目录结构长这个样子\n\n![1583077468832](assets/1583077468832.png)\n\nHarbor创建对应的app空间\n\n![1583082446323](assets/1583082446323.png)\n\n![1583077775076](assets/1583077775076.png)\n\n![1583129694897](assets/1583129694897.png)\n\n![1583129669192](assets/1583129669192.png)\n\n![1583129554275](assets/1583129554275.png)\n\n[查看harbor](harbor.od.com)\n\n![1583132650158](assets/1583132650158.png)\n\n#### 报错：\n\n1、\n\n![1584530943674](assets/1584530943674.png)\n\n原因：你构建的参数写错了，再去检查一遍 ,需根据git项目名来的。前面是截图默认值是：./target 不对，需要修改成./dubbo-server/target \n\n2、连接不了gitee，一直显示失败（网络波动问题），如图\n\n![1583128285936](assets/1583128285936.png)\n\n解决办法：安装本地gitlab\n\n~~~\n# 200机器：\n~]# yum install curl policycoreutils openssh-server openssh-clients policycoreutils-python -y\n~]# cd /usr/local/src\nsrc]# 去该网址把文件下载下来https://mirrors.tuna.tsinghua.edu.cn/gitlab-ce/yum/el7/\nsrc]# ls\ngitlab-ce-11.11.8-ce.0.el7.x86_64.rpm\nsrc]# rpm -ivh gitlab-ce-11.11.8-ce.0.el7.x86_64.rpm\n# 修改如下内容：\nsrc]# vim /etc/gitlab/gitlab.rb\nexternal_url \"http://gitlab.od.com:10000\"\nnginx['listen_port'] = 10000\n\n# 11机器解析：\n~]# vi /var/named/od.com.zone\nserial前滚一位\ngitlab             A    10.4.7.200\n\nopt]# systemctl restart named\nopt]# dig -t A jenkins.od.com @10.4.7.11 +short\n# out:10.4.7.200\n\n# 200机器继续，下面这步要很久，最后会有Running handlers complete的字眼，然后回车即可\nsrc]# gitlab-ctl reconfigure\nsrc]# gitlab-ctl status\nrun: alertmanager: (pid 17983) 14s; run: log: (pid 17689) 90s\nrun: gitaly: (pid 17866) 20s; run: log: (pid 16947) 236s\nrun: gitlab-monitor: (pid 17928) 18s; run: log: (pid 17591) 108s\nrun: gitlab-workhorse: (pid 17897) 19s; run: log: (pid 17451) 141s\nrun: logrotate: (pid 17500) 127s; run: log: (pid 17515) 124s\nrun: nginx: (pid 17468) 138s; run: log: (pid 17482) 134s\nrun: node-exporter: (pid 17911) 19s; run: log: (pid 17567) 114s\nrun: postgres-exporter: (pid 17998) 13s; run: log: (pid 17716) 84s\nrun: postgresql: (pid 17109) 217s; run: log: (pid 17130) 216s\nrun: prometheus: (pid 17949) 17s; run: log: (pid 17654) 96s\nrun: redis: (pid 16888) 244s; run: log: (pid 16902) 243s\nrun: redis-exporter: (pid 17936) 18s; run: log: (pid 17624) 102s\nrun: sidekiq: (pid 17395) 155s; run: log: (pid 17412) 152s\nrun: unicorn: (pid 17337) 166s; run: log: (pid 17368) 162s\n~~~\n\n然后把代码传进来\n\n![1583129295099](assets/1583129295099.png)\n\n完成\n\n> 期间如果clone下来的时候密码一直不对，就用ssh密钥的方法免密登录，最后添加200的公钥到gitlab，build镜像的时候修改git_repo即可\n>\n> ~~~\n> git_repo:      http://gitlab.od.com:10000/909336740/dubbo-demo-service.git\n> ~~~\n>\n> \n\n2、连接成功了但是一直下载不了文件然后报错（网络波动问题），如图\n\n![1583128324746](assets/1583128324746.png)\n\n解决办法：多build几次，不行就把aliyun删掉（vi conf/settings.xml），用原始的源下载\n\n> PS：我第一次一次过，第三次的时候，如图，我把27那些删掉，因为设置了只能30个，怕爆了，最终在28次的时候成功了\n>\n> ![1583129711754](assets/1583129711754.png)\n>\n> 如何删除\n>\n> ![1583129763175](assets/1583129763175.png)\n\n这件事情告诉我们，尽量不用公网的，特别是生产的时候分分钟几百万的访问，卡这么久，老板得祭天\n\n\n\n服务站镜像包已制作完成，现在开始制作资源配置清单。\n\n~~~\n# 200机器，服务者的资源清单只需要一个：\njre8]# mkdir /data/k8s-yaml/dubbo-demo-service\njre8]# vi /data/k8s-yaml/dubbo-demo-service/dp.yaml\nkind: Deployment\napiVersion: extensions/v1beta1\nmetadata:\n  name: dubbo-demo-service\n  namespace: app\n  labels: \n    name: dubbo-demo-service\nspec:\n  replicas: 1\n  selector:\n    matchLabels: \n      name: dubbo-demo-service\n  template:\n    metadata:\n      labels: \n        app: dubbo-demo-service\n        name: dubbo-demo-service\n    spec:\n      containers:\n      - name: dubbo-demo-service\n        image: harbor.od.com/app/dubbo-demo-service:master_200301_2352\n        ports:\n        - containerPort: 20880\n          protocol: TCP\n        env:\n        - name: JAR_BALL\n          value: dubbo-server.jar\n        imagePullPolicy: IfNotPresent\n      imagePullSecrets:\n      - name: harbor\n      restartPolicy: Always\n      terminationGracePeriodSeconds: 30\n      securityContext: \n        runAsUser: 0\n      schedulerName: default-scheduler\n  strategy:\n    type: RollingUpdate\n    rollingUpdate: \n      maxUnavailable: 1\n      maxSurge: 1\n  revisionHistoryLimit: 7\n  progressDeadlineSeconds: 600\n~~~\n\n> 上面spec里的image包要对上你现在harbor的名字\n>\n> 时间要改成当前的时间，这个是为了做标记，如果出现问题了可以说这个包是什么时候包，是不是自己部的，防止背锅\n\n~~~\n# 21机器，应用资源配置清单：\n~]# kubectl create ns app\n~]# kubectl create secret docker-registry harbor --docker-server=harbor.od.com --docker-username=admin --docker-password=Harbor12345 -n app\n# out: secret/harbor created\n\n# 11机器：\ncd /opt/zookeeper\nzookeeper~]# bin/zkServer.sh status\n### out:\nZooKeeper JMX enabled by default\nUsing config: /opt/zookeeper/bin/../conf/zoo.cfg\nMode: follower\n###\nzookeeper]# bin/zkCli.sh -server localhost:2181\n# 连接到zk，发现里面只有zk没有dubbo\n[zk: localhost:2181(CONNECTED) 0] ls /\n#out: [zookeeper]\n# 22机器，应用dubbo：\n~]# kubectl apply -f http://k8s-yaml.od.com/dubbo-demo-service/dp.yaml\n# out: deployment.extensions/dubbo-demo-service created\n~~~\n\n![1583133750175](assets/1583133750175.png)\n\n![1583133771985](assets/1583133771985.png)\n\n![1583133814367](assets/1583133814367.png)\n\n> 相关报错：\n>\n> ![1584603379421](assets/1584603379421.png)\n>\n> 原因：因为你的文件不见了\n>\n> ![1584603402104](assets/1584603402104.png)\n\n~~~\n# 11机器查看，这时候已经不止zk，还有dubbo：\n[zk: localhost:2181(CONNECTED) 0] ls /\n#out: [dubbo, zookeeper]\n[zk: localhost:2181(CONNECTED) 1] ls /dubbo\n[com.od.dubbotest.api.HelloService]\n~~~\n\n![1583133849488](assets/1583133849488.png)\n\n> 此时dubbo服务已经注册到JK交付中心，项目已经交付成功\n\n\n\n### 借助BlueOcean插件回顾Jenkins流水线构建原理\n\n![1583133065281](assets/1583133065281.png)\n\n![1583133094833](assets/1583133094833.png)\n\n\n\n![1583133158000](assets/1583133158000.png)\n\n![1583133204015](assets/1583133204015.png)\n\n\n\n![1583133288421](assets/1583133288421.png)\n\n\n\n![1583133397745](assets/1583133397745.png)\n\n> **一般dockerfile是由谁写**：有一些公司是运维用Jenkins写，也有些公司是开发自己写的\n\n![1583132650158](assets/1583132650158.png)\n\n> **Jenkins流水线构建就这五步**：拉取代码——>编译代码——>到指定目录打包jar——>构建镜像\n\n此时提供者已经在harbor里，我们还需要把它发到我们的k8s里（还有消费者还没操作）\n\n问题来了，如果有很多的提供者和消费者需要注册进来zk，总不能每次都用命令行连接到zk然后ls / 去查看，所以需要一个图形化界面，也就是下面的dubbo-monitor\n\n\n\n### 交付dubbo-monitor到k8s集群：\n\n> **WHAT**上面已经讲到，注册到zk里的时候不能总是打开机器进去查看，我们得有个图形化界面\n\n~~~\n# 200机器，下载包：\ncd /opt/src\nsrc]# wget https://github.com/Jeromefromcn/dubbo-monitor/archive/master.zip\nsrc]# unzip master.zip\n# 没有unzip的，yum install unzip -y\nsrc]# mv dubbo-monitor-master /opt/src/dubbo-monitor\nsrc]# ll\n~~~\n\n![1583134970446](assets/1583134970446.png)\n\n~~~\n# 修改源码，200机器：\nsrc]# vi /opt/src/dubbo-monitor/dubbo-monitor-simple/conf/dubbo_origin.properties\ndubbo.application.name=dubbo-monitor\ndubbo.application.owner=ben1234560\ndubbo.registry.address=zookeeper://zk1.od.com:2181?backup=zk2.od.com:2181,zk3.od.com:2181\ndubbo.protocol.port=20880\ndubbo.jetty.port=8080\ndubbo.jetty.directory=/dubbo-monitor-simple/monitor\ndubbo.charts.directory=/dubbo-monitor-simple/charts\ndubbo.statistics.directory=/dubbo-monitor-simple/statistics\n~~~\n\n![1583137417965](assets/1583137417965.png)\n\n~~~\n# 200机器，修改使用内存配置文件：\ncd /opt/src/dubbo-monitor/dubbo-monitor-simple/bin\n# 修改，原本是：-Xmx2g -Xms2g -Xmn256m PermSize=128m -Xms1g -Xmx1g -XX:PermSize=128m\nbin]# vi start.sh\nif [ -n \"$BITS\" ]; then\n    JAVA_MEM_OPTS=\" -server -Xmx128m -Xms128m -Xmn32m -XX:PermSize=16m -Xss256k -XX:+DisableExplicitGC -XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled -XX:+UseCMSCompactAtFullCollection -XX:LargePageSizeInBytes=128m -XX:+UseFastAccessorMethods -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFraction=70 \"\nelse\n    JAVA_MEM_OPTS=\" -server -Xms128m -Xmx128m -XX:PermSize=16m -XX:SurvivorRatio=2 -XX:+UseParallelGC \"\nfi\n\necho -e \"Starting the $SERVER_NAME ...\\c\"\nexec java $JAVA_OPTS $JAVA_MEM_OPTS $JAVA_DEBUG_OPTS $JAVA_JMX_OPTS -classpath $CONF_DIR:$LIB_JARS com.alibaba.dubbo.container.Main > $STDOUT_FILE 2>&1\n\n# 再往下的全部内容删掉，如图\n~~~\n\n![1583137508620](assets/1583137508620.png)\n\n~~~\n# 200机器，build：\ncd /opt/src\nsrc]# cp -a dubbo-monitor /data/dockerfile/\nsrc]# cd /data/dockerfile/dubbo-monitor\ndubbo-monitor]# docker build . -t harbor.od.com/infra/dubbo-monitor:latest\n# out：Successfully built ...  Successfully tagged ...\ndubbo-monitor]# docker push harbor.od.com/infra/dubbo-monitor:latest\n~~~\n\n![1583135594711](assets/1583135594711.png)\n\n~~~\n# 200机器，配置资源清单：\ndubbo-monitor]# mkdir /data/k8s-yaml/dubbo-monitor\ndubbo-monitor]# cd /data/k8s-yaml/dubbo-monitor\ndubbo-monitor]# vi dp.yaml\nkind: Deployment\napiVersion: extensions/v1beta1\nmetadata:\n  name: dubbo-monitor\n  namespace: infra\n  labels: \n    name: dubbo-monitor\nspec:\n  replicas: 1\n  selector:\n    matchLabels: \n      name: dubbo-monitor\n  template:\n    metadata:\n      labels: \n        app: dubbo-monitor\n        name: dubbo-monitor\n    spec:\n      containers:\n      - name: dubbo-monitor\n        image: harbor.od.com/infra/dubbo-monitor:latest\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n        - containerPort: 20880\n          protocol: TCP\n        imagePullPolicy: IfNotPresent\n      imagePullSecrets:\n      - name: harbor\n      restartPolicy: Always\n      terminationGracePeriodSeconds: 30\n      securityContext: \n        runAsUser: 0\n      schedulerName: default-scheduler\n  strategy:\n    type: RollingUpdate\n    rollingUpdate: \n      maxUnavailable: 1\n      maxSurge: 1\n  revisionHistoryLimit: 7\n  progressDeadlineSeconds: 600\n\ndubbo-monitor]# vi svc.yaml\nkind: Service\napiVersion: v1\nmetadata: \n  name: dubbo-monitor\n  namespace: infra\nspec:\n  ports:\n  - protocol: TCP\n    port: 8080\n    targetPort: 8080\n  selector: \n    app: dubbo-monitor\n\ndubbo-monitor]# ingress.yaml\nkind: Ingress\napiVersion: extensions/v1beta1\nmetadata: \n  name: dubbo-monitor\n  namespace: infra\nspec:\n  rules:\n  - host: dubbo-monitor.od.com\n    http:\n      paths:\n      - path: /\n        backend: \n          serviceName: dubbo-monitor\n          servicePort: 8080\n~~~\n\n> 每次创建完yaml后，你都可以去k8s-yaml.od.com网址看下有没有在里面了\n\n~~~\n# 应用资源清单前，先解析域名，11机器：\n11 ~]# vim /var/named/od.com.zone\n# serial 前滚一个\ndubbo-monitor      A    10.4.7.10\n\n11 ~]# systemctl restart named\n11 ~]# dig -t A dubbo-monitor.od.com @10.4.7.11 +short\n# out: 10.4.7.10\n~~~\n\n![1583135902145](assets/1583135902145.png)\n\n~~~\n# 22机器，应用资源配置清单：\n22 ~]# kubectl apply -f http://k8s-yaml.od.com/dubbo-monitor/dp.yaml\n22 ~]# kubectl apply -f http://k8s-yaml.od.com/dubbo-monitor/svc.yaml\n22 ~]# kubectl apply -f http://k8s-yaml.od.com/dubbo-monitor/ingress.yaml\n~~~\n\n![1580955553025](assets/1580955553025.png)\n\n> 相关报错：pod提示CrashLoopBackOff\n>\n> 排查思路：log或者kubectl get event报错信息如下\n>\n> ![1681349321605](assets/1681349321605.png)\n>\n> 进入容器内执行start.sh报错read-only file system。\n>\n> 解决方法：参考Issues（作者@stringguai）https://github.com/ben1234560/k8s_PaaS/issues/33）：\n>\n> 1、dubbo-monitor 启动报 read-only file system的错，修改bin/start.sh，注释第一行\"cp xxxxx\"，手动cp dubbo_origin.properties dubbo.properties，在进行镜像的构建\n> 2、pod起不来的问题，修改bin/start.sh, 将nohup java $JAVA_OPTS那一行最后的重定向和后台运行符& 去掉 ，即去掉“2>&1 &”\n\n[访问网站 http://dubbo-monitor.od.com](http://dubbo-monitor.od.com)\n\n![1583139242622](assets/1583139242622.png)\n\n![1583139255552](assets/1583139255552.png)\n\n界面版的交付页面完成。\n\n> 相关小问题：\n>\n> 发现pod有问题，因为之前的配置文件没修改好，镜像是起不来的\n>\n> ![1583136107703](assets/1583136107703.png)\n>\n> 解决办法：修改好配置文件，新build镜像，然后修改dp.yaml指定镜像，然后apply -f 应用，再删掉这个pod即可\n\n#### 交付dubbo服务的消费者到K8S\n\n登录Jenkins，账号：admin，密码：admin123\n\n![1583139377309](assets/1583139377309.png)\n\n~~~\n# 填入指定参数\napp_name:       dubbo-demo-consumer\nimage_name:     app/dubbo-demo-consumer\ngit_repo:        http://gitlab.od.com:10000/909336740/dubbo-demo-web.git\ngit_ver:        master\nadd_tag:        200302_1700\nmvn_dir:        ./\ntarget_dir:     ./dubbo-client/target\nmvn_cmd:        mvn clean package -e -q -Dmaven.test.skip=true\nbase_image:     base/jre8:8u112\nmaven:          3.6.1-8u232\n# 点击Build进行构建，等待构建完成，mvn_cmd 里的 -e -q是让输出输出的多点，可以看里面的内容\n~~~\n\n![1583140213653](assets/1583140213653.png)\n\n> 这里的git_repo你应该用公网的gitee或者GitHub，我因为省钱买的网络有些问题的机器，所以只能一直用gitlab\n\n![1583141273267](assets/1583141273267.png)\n\n![1583141305844](assets/1583141305844.png)\n\n> 第一次编译比较久（因为要远程下载下来），需要耐心等待，3分钟左右，最终会success，此时harbor里面已经有了\n\n![1583141341105](assets/1583141341105.png)\n\n> 可以去蓝海看构建过程，进入方法上面回顾的时候有\n\n![1583141384774](assets/1583141384774.png)\n\n~~~\n# 200机器，准备资源配置清单：\nmkdir /data/k8s-yaml/dubbo-demo-consumer\ncd /data/k8s-yaml/dubbo-demo-consumer\ndubbo-demo-consumer]# vi dp.yaml\nkind: Deployment\napiVersion: extensions/v1beta1\nmetadata:\n  name: dubbo-demo-consumer\n  namespace: app\n  labels: \n    name: dubbo-demo-consumer\nspec:\n  replicas: 1\n  selector:\n    matchLabels: \n      name: dubbo-demo-consumer\n  template:\n    metadata:\n      labels: \n        app: dubbo-demo-consumer\n        name: dubbo-demo-consumer\n    spec:\n      containers:\n      - name: dubbo-demo-consumer\n        image: harbor.od.com/app/dubbo-demo-consumer:master_200302_1700\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n        - containerPort: 20880\n          protocol: TCP\n        env:\n        - name: JAR_BALL\n          value: dubbo-client.jar\n        imagePullPolicy: IfNotPresent\n      imagePullSecrets:\n      - name: harbor\n      restartPolicy: Always\n      terminationGracePeriodSeconds: 30\n      securityContext: \n        runAsUser: 0\n      schedulerName: default-scheduler\n  strategy:\n    type: RollingUpdate\n    rollingUpdate: \n      maxUnavailable: 1\n      maxSurge: 1\n  revisionHistoryLimit: 7\n  progressDeadlineSeconds: 600\n\ndubbo-demo-consumer]# vi svc.yaml\nkind: Service\napiVersion: v1\nmetadata: \n  name: dubbo-demo-consumer\n  namespace: app\nspec:\n  ports:\n  - protocol: TCP\n    port: 8080\n    targetPort: 8080\n  selector: \n    app: dubbo-demo-consumer\n\ndubbo-demo-consumer]# vi ingress.yaml\nkind: Ingress\napiVersion: extensions/v1beta1\nmetadata: \n  name: dubbo-demo-consumer\n  namespace: app\nspec:\n  rules:\n  - host: demo.od.com\n    http:\n      paths:\n      - path: /\n        backend: \n          serviceName: dubbo-demo-consumer\n          servicePort: 8080\n~~~\n\n> 有ingress了，所以应用之前，我们得解析一下域名\n\n~~~\n# 11机器，解析域名：\n11 ~]# vi /var/named/od.com.zone\nserial 前滚一个序号\ndemo               A    10.4.7.10\n\n11 ~]# systemctl restart named\n11 ~]# dig -t A demo.od.com @10.4.7.11 +short\n#out: 10.4.7.10\n~~~\n\n![1583141600029](assets/1583141600029.png)\n\n~~~\n# 22机器（22还是21都无所谓），应用资源配置清单：\n22 ~]# kubectl apply -f http://k8s-yaml.od.com/dubbo-demo-consumer/dp.yaml\n22 ~]# kubectl apply -f http://k8s-yaml.od.com/dubbo-demo-consumer/svc.yaml\n22 ~]# kubectl apply -f http://k8s-yaml.od.com/dubbo-demo-consumer/ingress.yaml\n~~~\n\n![1580963332570](assets/1580963332570.png)\n\n[刷新dubbo-monitor-application界面](http://dubbo-monitor.od.com/applications.html)\n\n![1583141734527](assets/1583141734527.png)\n\n~~~\n在浏览器输入以下网址\nhttp://demo.od.com/hello?name=ben1234560\n~~~\n\n![1583141771920](assets/1583141771920.png)\n\n成功\n\n![1580972027995](assets/1580972027995.png)\n\n> 最重要的是软负载均衡及扩容，也就是可以随意扩容后端或前端，你可以这么试试，先把服务者pod控制器改成3个\n\n![1583141879765](assets/1583141879765.png)\n\n![1583141886649](assets/1583141886649.png)\n\n> 把消费者pod改成两个\n\n![1583141974794](assets/1583141974794.png)\n\n> 这样一共就是5个，完成扩容\n\n然后缩容，先把消费者改成1个，再把服务者也改成一个\n\n![1583142064082](assets/1583142064082.png)\n\n> 刷新页面，完成缩容\n\n![1583142098269](assets/1583142098269.png)\n\n> 这样的好处就是你可以随意无缝隙的扩容缩容，当用户访问量高的时候扩容，访问量小的时候缩容\n>\n\n\n\n### 实现dubbo集群的日常维护\n\n> **WHAT**：日常中肯定有代码迭代的情况，开发更新了代码，我们迭代App\n\n我们改了下面红框内的一些内容，模拟开发代码的迭代\n\n![1582604101870](assets/1582604101870.png)\n\n更改完后提交到仓库\n\n![1582604137705](assets/1582604137705.png)\n\n![1582604248247](assets/1582604248247.png)\n\n再用Jenkins构建\n\n~~~\n# 填入指定参数\napp_name:       dubbo-demo-consumer\nimage_name:     app/dubbo-demo-consumer\ngit_repo:       https://gitee.com/benjas/dubbo-demo-web.git\ngit_ver:        d76f474\nadd_tag:        200302_2311\nmvn_dir:        ./\ntarget_dir:     ./dubbo-client/target\nmvn_cmd:        mvn clean package -e -q -Dmaven.test.skip=true\nbase_image:     base/jre8:8u112\nmaven:          3.6.1-8u232\n# 点击Build进行构建，等待构建完成，mvn_cmd 里的 -e -q是让输出输出的多点，可以看里面的内容\n~~~\n\n![1583161598444](assets/1583161598444.png)\n\nharbor里面有了新的镜像\n\n![1583161729247](assets/1583161729247.png)\n\n> 注意，我这里的版本号是我在gitlab做的（因为网络真的不想），上面是为了演示用公网的git，所以正确的应该是这个名字d76f474_200302_2311\n\n在dashboard里面改镜像名\n\n![1583161856968](assets/1583161856968.png)\n\n![1583161906128](assets/1583161906128.png)\n\n> 同样，这个gitlab的，其实应该是d76f474_200302_2311\n\n~~~\n再去下面这个网址，刷新\nhttp://demo.od.com/hello?name=ben1234560\n~~~\n\n![1582613370939](assets/1582613370939.png)\n\n这样你就完成了版本迭代\n\n![1580997705509](assets/1580997705509.png)\n\n[^回顾]: 此时我们在看一下这张图，git我们用了，Jenkins编译打包了，ingress资源清单，dubbo微服务起了，zk注册中心做了，harbor私有仓库完成了，k8s-yaml网址有了，kubernetes连接了，所有东西都实现了点点点，就差OPS服务器的自动化。我们需要Jenkins打包后到harbor仓库里后，自动变成yaml到K8S里。\n\n> 我们的目标是实现自动化，解放生产力。\n\n\n\n### 实战K8S集群毁灭性测试\n\n> **WHAT**：生产中总会遇到突然宕机等情况，我们需要来模拟一下\n\n生产上，保证服务都是起两份以上，所有我们给consumer和service都起两份\n\n![1584239463336](assets/1584239463336.png)\n\n![1581049921071](assets/1581049921071.png)\n\n我们可以看到21和22分别都有两台，这是schedule做的资源分配\n\n接下来，我们模拟一台服务器炸了\n\n~~~\n# 21机器：\n~]# halt\n~~~\n\n![1581043155735](assets/1581043155735.png)\n\n> 再去demo网址，发现已经进不去了，dashboard已经503了\n\n![1581043277189](assets/1581043277189.png)\n\n> 宿主机爆炸，我们的第一件事情，就是把离线的主机删了（如果不删掉，那么k8s会认为是网络抖动或者什么问题，会不断的重连）\n\n~~~\n# 22机器，删除离线主机：\n~]# kubeclt delete node hdss7-21.host.com\n# out: node \"hdss7-21.host.com\" deleted\n~~~\n\n> 删除了后，k8s有自愈机制，会在22节点自己起来\n>\n> 我们在去网址查看（时间可能慢一些，当然你不断刷新的时候可能发现偶尔会有报错，然后又刷新就又好了，下面会讲到）\n\n![1582613370939](assets/1582613370939.png)\n\n> 此时可以看到dashboard已经在起来的状态，我们的配置比较差，所以起来的比较慢\n>\n> 现在看到已经起来了，但是下面还有一个backoff状态pod，应该是负载的问题，也就是引发上面的网址有时候刷新是报错状态，因为nginx的负载\n\n![1584239156636](assets/1584239156636.png)\n\n我们去改一下nginx\n\n~~~\n# 11机器，去到最下面把21节点注释掉：\nvi /etc/nginx/nginx.conf\n# server.10.4.7.21:6443\n~~~\n\n![1584239144369](assets/1584239144369.png)\n\n~~~\n# 11机器，traefik的21节点也注释掉：\n~]# Vi /etc/ningx/conf.d/od.com.conf\n# server 10.4.7.21.81     max_fails=3 fail_timeout=10s;\n\n~]# nginx -s reload\n~~~\n\n![1581044038945](assets/1581044038945.png)\n\n这时候你去刷新网址，已经不会出现偶尔报错的状况了，容器也已经起来了\n\n![1581048655265](assets/1581048655265.png)\n\n现在，我们已经完成了服务器（主机）炸了后的应急解决\n\n#### 集群恢复：\n\n~~~\n# 21机器，重新连接21机器：\n~]# supervisorctl status\n~]# kubectl get nodes\n~]# kubectl label node hdss7-21.host.com node-role.kubernetes.io/master=\n# out: node/hdss7-21.host.comt labeled\n~]# kubectl label node hdss7-21.host.com node-role.kubernetes.io/node=\n# out: node/hdss7-21.host.com labeled\n~~~\n\n![1581049071994](assets/1581049071994.png)\n\n![1584239248833](assets/1584239248833.png)\n\n> 所有已经起来了，我们在把负载的注释改回来\n\n~~~\n# 11机器，修改负载：\n~]# vi /etc/nginx/nginx.conf\nserver.10.4.7.21:6443\n\n~]# vi /etc/ningx/conf.d/od.com.conf\nserver 10.4.7.21.81     max_fails=3 fail_timeout=10s;\n\nnginx -s reload\n~~~\n\n> 这时候我们看一下dubbo服务都起在哪里\n\n~~~\n# 21机器：\n~]# kubectl get pods -n app -o wide\n~~~\n\n![1584239281131](assets/1584239281131.png)\n\n![1581049691958](assets/1581049691958.png)\n\n> 可以看到都是在22机器上运行，我们有计划的做一下调度（资源平衡）\n\n~~~\n# 删掉一个consumer和一个service，21机器：\n~]# kubectl delete pods dubbo-demo-consumer-5874d7c89d-8dmk4 -n app\n~]# kubectl delete pods dubbo-demo-service-7754d5cb8b-78bhf -n app\n~~~\n\n![1581050095300](assets/1581050095300.png)\n\n![1581050205612](assets/1581050205612.png)\n\n~~~\n# 21机器，可以看到21机器和22机器都分别是两个了：\n~]# kubectl get pods -n app -o wide\n~~~\n\n![1584239330451](assets/1584239330451.png)\n\n此时我们去dashboard看一下，发现不可用\n\n![1581050328121](assets/1581050328121.png)\n\n~~~\n# 21机器,把dashboard删掉让k8S重新调度：\n~]# kubectl get pods -n kube-system\n~]# kubectl delete pods kubernetes-dashboard-76dcdb4677-kv8mq -n kube-system\n~~~\n\n![1581050421428](assets/1581050421428.png)\n\n成功，是不是比较简单，这就是分布式的能力\n\n~~~\n# 21机器，查看一下iptables规则有没有问题\nzookeeper]# iptables-save |grep -i postrouting\n# 把有问题的规则删了\nzookeeper]# iptables -t nat -D POSTROUTING -s 172.7.21.0/24 ! -o docker0 -j MASQUERADE\n# 修改规则并启动\nzookeeper]# iptables -t nat -I POSTROUTING -s 172.7.21.0/24 ! -d 172.7.0.0/16 ! -o docker0 -j MASQUERADE\nzookeeper]# iptables-save |grep -i postrouting\n~~~\n\n![1581090020941](assets/1581090020941.png)\n\n修改完成\n\n"
        },
        {
          "name": "第八章——spinnaker部署与应用.md",
          "type": "blob",
          "size": 50.2431640625,
          "content": "## 第八章——spinnaker部署与应用\n\n> 从本章节开始，很多事情我不再截图而是一笔带过，以便你更好的动手操作，不过，一笔带过的地方我一定不会留大坑，这个你可以完全放心\n\n#### 关于IaaS、PaaS、SaaS\n\n![1581819941099](assets/1581819941099.png)\n\n> K8S不是传统意义上的Paas平台，而很多互联网公司都需要的是Paas平台，而不是单纯的K8S，K8S及其周边生态（如logstash、Prometheus等）才是Paas平台，才是公司需要的\n\n#### 获得PaaS能力的几个必要条件：\n\n- 统一应有的运行时环境（docker）\n- 有IaaS能力（K8S）\n- 有可靠的中间件集群、数据库集群（DBA的主要工作）\n- 有分布式存储集群（存储工程师的主要工作）\n- 有适配的监控、日志系统（Prometheus、ELK）\n- 有完善的CI、CD系统（Jenkins、Spinnaker）\n\n> 阿里云、腾讯云等厂商都提供了K8S为底的服务，即你买了集群就给你配备了K8S，但我们不能完全依赖于厂商，而被钳制，同时我们也需要不断的学习以备更好的理解和使用，公司越大时越需要自己创建而不是依赖于厂商。\n\n#### spinnaker简介\n\n> **WHAT**：通过灵活和可配置 Pipelines，实现可重复的自动化部署；提供所有环境的全局视图，可随时查看应用程序在其部署 Pipeline 的状态；易于配置、维护和扩展；等等；\n\n主要功能\n\n- 集群管理：主要用于管理云资源，即主要是IaaS的资源\n- 部落管理：负责将Jenkins流水线创建的镜像，部署到K8S集群中去，让服务真正运行起来。\n- 架构：[官网地址](https://www.spinnaker.io/reference/architecture/)\n\n![1581822996520](assets/1581822996520.png)\n\n> Deck：点点点页面\n>\n> Gate：网关\n>\n> Igor：用来和Jenkins通信\n>\n> Echo：信息通讯组件\n>\n> Orca：任务编排引擎\n>\n> Clouddriver：云计算基础设施\n>\n> Front50：管理持久化数据\n>\n> 其中我们用到redis、minio\n>\n> 部署顺序：Minio-->Redis-->Clouddriver-->Front50-->Orca-->Echo-->Igor-->Gate-->Deck-->Nginx(是静态页面所以需要)\n\n### 部署Spinnaker的Amory发行版\n\n~~~\n# 200机器，准备镜像、在资源清单：\n~]# docker pull minio/minio:latest\n~]# docker images|grep minio\n~]# docker tag 7ea4a619ecfc harbor.od.com/armory/minio:latest\n# 在此之前你应该创建一个私有armory\n# 否则报错：denied: requested access to the resource is denied\n~]# docker push harbor.od.com/armory/minio:latest\n~]# mkdir -p /data/k8s-yaml/armory/minio\n~]# cd /data/k8s-yaml/armory/minio/\nminio]# vi dp.yaml\nkind: Deployment\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  labels:\n    name: minio\n  name: minio\n  namespace: armory\nspec:\n  progressDeadlineSeconds: 600\n  replicas: 1\n  revisionHistoryLimit: 7\n  selector:\n    matchLabels:\n      name: minio\n  template:\n    metadata:\n      labels:\n        app: minio\n        name: minio\n    spec:\n      containers:\n      - name: minio\n        image: harbor.od.com/armory/minio:latest\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9000\n          protocol: TCP\n        args:\n        - server\n        - /data\n        env:\n        - name: MINIO_ACCESS_KEY\n          value: admin\n        - name: MINIO_SECRET_KEY\n          value: admin123\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /minio/health/ready\n            port: 9000\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        volumeMounts:\n        - mountPath: /data\n          name: data\n      imagePullSecrets:\n      - name: harbor\n      volumes:\n      - nfs:\n          server: hdss7-200\n          path: /data/nfs-volume/minio\n        name: data\n\nminio]# vi svc.yaml \napiVersion: v1\nkind: Service\nmetadata:\n  name: minio\n  namespace: armory\nspec:\n  ports:\n  - port: 80\n    protocol: TCP\n    targetPort: 9000\n  selector:\n    app: minio\n\nminio]# vi ingress.yaml \nkind: Ingress\napiVersion: extensions/v1beta1\nmetadata: \n  name: minio\n  namespace: armory\nspec:\n  rules:\n  - host: minio.od.com\n    http:\n      paths:\n      - path: /\n        backend: \n          serviceName: minio\n          servicePort: 80\n\n# 创建对应的存储\nminio]# mkdir /data/nfs-volume/minio\n~~~\n\n\n\n~~~\n# 11机器，解析域名：\nvi /var/named/od.com.zone\nserial 前滚一位\nminio              A    10.4.7.10\n\nsystemctl restart named\ndig -t A minio.od.com +short\n~~~\n\n> 为什么每次每次都不用些od.com，是因为第一行有$ORIGIN od.com. 的宏指令，会自动补\n\n~~~\n# 22机器：\n# 创建名称空间\n~]# kubectl create ns armory\n# 因为armory仓库是私有的，要创建secret，不然拉不了\n~]# kubectl create secret docker-registry harbor --docker-server=harbor.od.com --docker-username=admin --docker-password=Harbor12345 -n armory\n~~~\n\n在amory名称空间里面就能看到\n\n![1583994197390](assets/1583994197390.png)\n\n~~~\n# 22机器，应用清单:\n~]# kubectl apply -f http://k8s-yaml.od.com/armory/minio/dp.yaml\n~]# kubectl apply -f http://k8s-yaml.od.com/armory/minio/svc.yaml\n~]# kubectl apply -f http://k8s-yaml.od.com/armory/minio/ingress.yaml\n~~~\n\n![1583994262819](assets/1583994262819.png)\n\n[minio.od.com](minio.od.com)\n\n账户：admin\n\n密码：admin123\n\n![1583994292679](assets/1583994292679.png)\n\n完成\n\n\n\n### 安装部署redis\n\n~~~\n# 200机器，准备镜像、资源清单：\n~]# docker pull redis:4.0.14\n~]# docker images|grep redis\n~]# docker tag 6e221e67453d harbor.od.com/armory/redis:v4.0.14\n~]# docker push !$\n~]# mkdir /data/k8s-yaml/armory/redis\n~]# cd /data/k8s-yaml/armory/redis\nredis]# vi dp.yaml\nkind: Deployment\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  labels:\n    name: redis\n  name: redis\n  namespace: armory\nspec:\n  replicas: 1\n  revisionHistoryLimit: 7\n  selector:\n    matchLabels:\n      name: redis\n  template:\n    metadata:\n      labels:\n        app: redis\n        name: redis\n    spec:\n      containers:\n      - name: redis\n        image: harbor.od.com/armory/redis:v4.0.14\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 6379\n          protocol: TCP\n      imagePullSecrets:\n      - name: harbor\n\nredis]# vi svc.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: redis\n  namespace: armory\nspec:\n  ports:\n  - port: 6379\n    protocol: TCP\n    targetPort: 6379\n  selector:\n    app: redis\n\n~~~\n\n\n\n~~~\n# 22机器，应用资源清单：\n~]# kubectl apply -f http://k8s-yaml.od.com/armory/redis/dp.yaml\n~]# kubectl apply -f http://k8s-yaml.od.com/armory/redis/svc.yaml\n~~~\n\n> 去看pod已经起来了\n>\n\n~~~\n# 看pod的ip起在哪里了，我是172.7.22.13，22机器，确认redis的服务起来了没：\n~]# docker ps -a|grep redis\n~]# telnet 172.7.22.13 6379\n~~~\n\n![1583994842832](assets/1583994842832.png)\n\n完成\n\n\n\n### 安装部署clouddriver\n\n~~~\n# 200机器：\n~]# docker pull docker.io/armory/spinnaker-clouddriver-slim:release-1.8.x-14c9664\n~]# docker images|grep clouddriver\n~]# docker tag edb2507fdb62 harbor.od.com/armory/clouddriver:v1.8.x\n~]# docker push harbor.od.com/armory/clouddriver:v1.8.x\n~]# mkdir /data/k8s-yaml/armory/clouddriver\n~]# cd /data/k8s-yaml/armory/clouddriver/\nclouddriver] vi credentials\n[default]\naws_access_key_id=admin\naws_secret_access_key=admin123\n\n# 做证书\n~]# cd /opt/certs/\n~]# cp client-csr.json admin-csr.json\n# 修改一下内容\n~]# vi admin-csr.json\n    \"CN\": \"cluster-admin\"\n\n~]# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=client admin-csr.json | cfssl-json -bare admin\n~~~\n\n![1584005779485](assets/1584005779485.png)\n\n~~~\n# 21机器：\n~]# wget wget http://k8s-yaml.od.com/armory/clouddriver/credentials\n~]# kubectl create secret generic credentials --from-file=./credentials -n armory\n~]# scp hdss7-200:/opt/certs/ca.pem .\n~]# scp hdss7-200:/opt/certs/admin.pem .\n~]# scp hdss7-200:/opt/certs/admin-key.pem .\n\n~]# kubectl config set-cluster myk8s --certificate-authority=./ca.pem --embed-certs=true --server=https://10.4.7.10:7443 --kubeconfig=config\n~]# kubectl config set-credentials cluster-admin --client-certificate=./admin.pem --client-key=./admin-key.pem --embed-certs=true --kubeconfig=config\n~]# kubectl config set-context myk8s-context --cluster=myk8s --user=cluster-admin --kubeconfig=config\n~]# kubectl config use-context myk8s-context --kubeconfig=config\nkubectl create clusterrolebinding myk8s-admin --clusterrole=cluster-admin --user=cluster-admin\n~]# cd /root/.kube/\n.kube]# cp /root/config .\n.kube]# kubectl config view\n.kube]# kubectl get pods -n armory\n~~~\n\n![1584005992328](assets/1584005992328.png)\n\n~~~\n# 200机器：\n~]# mkdir /root/.kube\n~]# cd /root/.kube\n.kube]# scp hdss7-21:/root/config .\n.kube]# cd \n~]# scp hdss7-21:/opt/kubernetes/server/bin/kubectl .\n~]# mv kubectl /usr/bin/\n~]# kubectl config view\n~]# kubectl get pods -n infra\n~~~\n\n![1584006160054](assets/1584006160054.png)\n\n~~~\n# 21机器，给权限：\n.kube]# mv config default-kubeconfig\n.kube]# kubectl create configmap default-kubeconfig --from-file=./default-kubeconfig -n armory\n~~~\n\n![1584014708599](assets/1584014708599.png)\n\n~~~shell\n# 200机器：\n~]# cd /data/k8s-yaml/armory/clouddriver/\nclouddriver]# vi init-env.yaml\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: init-env\n  namespace: armory\ndata:\n  API_HOST: http://spinnaker.od.com/api\n  ARMORY_ID: c02f0781-92f5-4e80-86db-0ba8fe7b8544\n  ARMORYSPINNAKER_CONF_STORE_BUCKET: armory-platform\n  ARMORYSPINNAKER_CONF_STORE_PREFIX: front50\n  ARMORYSPINNAKER_GCS_ENABLED: \"false\"\n  ARMORYSPINNAKER_S3_ENABLED: \"true\"\n  AUTH_ENABLED: \"false\"\n  AWS_REGION: us-east-1\n  BASE_IP: 127.0.0.1\n  CLOUDDRIVER_OPTS: -Dspring.profiles.active=armory,configurator,local\n  CONFIGURATOR_ENABLED: \"false\"\n  DECK_HOST: http://spinnaker.od.com\n  ECHO_OPTS: -Dspring.profiles.active=armory,configurator,local\n  GATE_OPTS: -Dspring.profiles.active=armory,configurator,local\n  IGOR_OPTS: -Dspring.profiles.active=armory,configurator,local\n  PLATFORM_ARCHITECTURE: k8s\n  REDIS_HOST: redis://redis:6379\n  SERVER_ADDRESS: 0.0.0.0\n  SPINNAKER_AWS_DEFAULT_REGION: us-east-1\n  SPINNAKER_AWS_ENABLED: \"false\"\n  SPINNAKER_CONFIG_DIR: /home/spinnaker/config\n  SPINNAKER_GOOGLE_PROJECT_CREDENTIALS_PATH: \"\"\n  SPINNAKER_HOME: /home/spinnaker\n  SPRING_PROFILES_ACTIVE: armory,configurator,local\n\nclouddriver]# vi default-config.yaml\n# 这里的内容在另外放的default-config.yaml里，因为实在太大，所以没办法复制进来\n\nclouddriver]# vi custom-config.yaml\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: custom-config\n  namespace: armory\ndata:\n  clouddriver-local.yml: |\n    kubernetes:\n      enabled: true\n      accounts:\n        - name: cluster-admin\n          serviceAccount: false\n          dockerRegistries:\n            - accountName: harbor\n              namespace: []\n          namespaces:\n            - test\n            - prod\n          kubeconfigFile: /opt/spinnaker/credentials/custom/default-kubeconfig\n      primaryAccount: cluster-admin\n    dockerRegistry:\n      enabled: true\n      accounts:\n        - name: harbor\n          requiredGroupMembership: []\n          providerVersion: V1\n          insecureRegistry: true\n          address: http://harbor.od.com\n          username: admin\n          password: Harbor12345\n      primaryAccount: harbor\n    artifacts:\n      s3:\n        enabled: true\n        accounts:\n        - name: armory-config-s3-account\n          apiEndpoint: http://minio\n          apiRegion: us-east-1\n      gcs:\n        enabled: false\n        accounts:\n        - name: armory-config-gcs-account\n  custom-config.json: \"\"\n  echo-configurator.yml: |\n    diagnostics:\n      enabled: true\n  front50-local.yml: |\n    spinnaker:\n      s3:\n        endpoint: http://minio\n  igor-local.yml: |\n    jenkins:\n      enabled: true\n      masters:\n        - name: jenkins-admin\n          address: http://jenkins.od.com\n          username: admin\n          password: admin123\n      primaryAccount: jenkins-admin\n  nginx.conf: |\n    gzip on;\n    gzip_types text/plain text/css application/json application/x-javascript text/xml application/xml application/xml+rss text/javascript application/vnd.ms-fontobject application/x-font-ttf font/opentype image/svg+xml image/x-icon;\n\n    server {\n           listen 80;\n\n           location / {\n                proxy_pass http://armory-deck/;\n           }\n\n           location /api/ {\n                proxy_pass http://armory-gate:8084/;\n           }\n\n           rewrite ^/login(.*)$ /api/login$1 last;\n           rewrite ^/auth(.*)$ /api/auth$1 last;\n    }\n  spinnaker-local.yml: |\n    services:\n      igor:\n        enabled: true\n\nclouddriver]# vi dp.yaml\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  labels:\n    app: armory-clouddriver\n  name: armory-clouddriver\n  namespace: armory\nspec:\n  replicas: 1\n  revisionHistoryLimit: 7\n  selector:\n    matchLabels:\n      app: armory-clouddriver\n  template:\n    metadata:\n      annotations:\n        artifact.spinnaker.io/location: '\"armory\"'\n        artifact.spinnaker.io/name: '\"armory-clouddriver\"'\n        artifact.spinnaker.io/type: '\"kubernetes/deployment\"'\n        moniker.spinnaker.io/application: '\"armory\"'\n        moniker.spinnaker.io/cluster: '\"clouddriver\"'\n      labels:\n        app: armory-clouddriver\n    spec:\n      containers:\n      - name: armory-clouddriver\n        image: harbor.od.com/armory/clouddriver:v1.8.x\n        imagePullPolicy: IfNotPresent\n        command:\n        - bash\n        - -c\n        args:\n        - bash /opt/spinnaker/config/default/fetch.sh && cd /home/spinnaker/config\n          && /opt/clouddriver/bin/clouddriver\n        ports:\n        - containerPort: 7002\n          protocol: TCP\n        env:\n        - name: JAVA_OPTS\n          value: -Xmx512M\n        envFrom:\n        - configMapRef:\n            name: init-env\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /health\n            port: 7002\n            scheme: HTTP\n          initialDelaySeconds: 600\n          periodSeconds: 3\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /health\n            port: 7002\n            scheme: HTTP\n          initialDelaySeconds: 180\n          periodSeconds: 3\n          successThreshold: 5\n          timeoutSeconds: 1\n        securityContext: \n          runAsUser: 0\n        volumeMounts:\n        - mountPath: /etc/podinfo\n          name: podinfo\n        - mountPath: /home/spinnaker/.aws\n          name: credentials\n        - mountPath: /opt/spinnaker/credentials/custom\n          name: default-kubeconfig\n        - mountPath: /opt/spinnaker/config/default\n          name: default-config\n        - mountPath: /opt/spinnaker/config/custom\n          name: custom-config\n      imagePullSecrets:\n      - name: harbor\n      volumes:\n      - configMap:\n          defaultMode: 420\n          name: default-kubeconfig\n        name: default-kubeconfig\n      - configMap:\n          defaultMode: 420\n          name: custom-config\n        name: custom-config\n      - configMap:\n          defaultMode: 420\n          name: default-config\n        name: default-config\n      - name: credentials\n        secret:\n          defaultMode: 420\n          secretName: credentials\n      - downwardAPI:\n          defaultMode: 420\n          items:\n          - fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.labels\n            path: labels\n          - fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.annotations\n            path: annotations\n        name: podinfo\n\nclouddriver]# vi svc.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: armory-clouddriver\n  namespace: armory\nspec:\n  ports:\n  - port: 7002\n    protocol: TCP\n    targetPort: 7002\n  selector:\n    app: armory-clouddriver\n\nclouddriver]# kubectl apply -f ./init-env.yaml\nclouddriver]# kubectl apply -f ./default-config.yaml\nclouddriver]# kubectl apply -f ./custom-config.yaml\nclouddriver]# kubectl apply -f ./dp.yaml\nclouddriver]# kubectl apply -f ./svc.yaml\n~~~\n\n![1584014649148](assets/1584014649148.png)\n\n![1584015398060](assets/1584015398060.png)\n\n~~~\n# 21机器(因为我的minio在21机器)：\n.kube]# docker ps -a|grep minio\n.kube]# docker exec -it 9f5592fb2950 /bin/sh\n/ # curl armory-clouddriver:7002/health\n~~~\n\n![1584015549913](assets/1584015549913.png)\n\n完成\n\n\n\n### 安装部署spinnaker其余组件\n\n部署数据持久化组件——Front50\n\n~~~\n# 200机器：\n~]# docker pull docker.io/armory/spinnaker-front50-slim:release-1.8.x-93febf2\n~]# docker images|grep front50\n~]# docker tag 0d353788f4f2 harbor.od.com/armory/front50:v1.8.x\n~]# docker push harbor.od.com/armory/front50:v1.8.x\n~]# mkdir /data/k8s-yaml/armory/front50\n~]# cd /data/k8s-yaml/armory/front50/\nfront50]# vi dp.yaml\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  labels:\n    app: armory-front50\n  name: armory-front50\n  namespace: armory\nspec:\n  replicas: 1\n  revisionHistoryLimit: 7\n  selector:\n    matchLabels:\n      app: armory-front50\n  template:\n    metadata:\n      annotations:\n        artifact.spinnaker.io/location: '\"armory\"'\n        artifact.spinnaker.io/name: '\"armory-front50\"'\n        artifact.spinnaker.io/type: '\"kubernetes/deployment\"'\n        moniker.spinnaker.io/application: '\"armory\"'\n        moniker.spinnaker.io/cluster: '\"front50\"'\n      labels:\n        app: armory-front50\n    spec:\n      containers:\n      - name: armory-front50\n        image: harbor.od.com/armory/front50:v1.8.x\n        imagePullPolicy: IfNotPresent\n        command:\n        - bash\n        - -c\n        args:\n        - bash /opt/spinnaker/config/default/fetch.sh && cd /home/spinnaker/config\n          && /opt/front50/bin/front50\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n        env:\n        - name: JAVA_OPTS\n          value: -javaagent:/opt/front50/lib/jamm-0.2.5.jar -Xmx1000M\n        envFrom:\n        - configMapRef:\n            name: init-env\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /health\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 600\n          periodSeconds: 3\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /health\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 180\n          periodSeconds: 5\n          successThreshold: 8\n          timeoutSeconds: 1\n        volumeMounts:\n        - mountPath: /etc/podinfo\n          name: podinfo\n        - mountPath: /home/spinnaker/.aws\n          name: credentials\n        - mountPath: /opt/spinnaker/config/default\n          name: default-config\n        - mountPath: /opt/spinnaker/config/custom\n          name: custom-config\n      imagePullSecrets:\n      - name: harbor\n      volumes:\n      - configMap:\n          defaultMode: 420\n          name: custom-config\n        name: custom-config\n      - configMap:\n          defaultMode: 420\n          name: default-config\n        name: default-config\n      - name: credentials\n        secret:\n          defaultMode: 420\n          secretName: credentials\n      - downwardAPI:\n          defaultMode: 420\n          items:\n          - fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.labels\n            path: labels\n          - fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.annotations\n            path: annotations\n        name: podinfo\n\nfront50]# vi svc.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: armory-front50\n  namespace: armory\nspec:\n  ports:\n  - port: 8080\n    protocol: TCP\n    targetPort: 8080\n  selector:\n    app: armory-front50\n\nfront50]# kubectl apply -f ./dp.yaml\nfront50]# kubectl apply -f ./svc.yaml\n~~~\n\n![1584023207016](assets/1584023207016.png)\n\n[minio.od.com](minio.od.com)\n\n![1584023230603](assets/1584023230603.png)\n\n#### 部署任务编排组件——Orca\n\n~~~\n# 200机器：\n~]# docker pull docker.io/armory/spinnaker-orca-slim:release-1.8.x-de4ab55\n~]# docker images|grep orca\n~]# docker tag 5103b1f73e04 harbor.od.com/armory/orca:v1.8.x\n~]# docker push harbor.od.com/armory/orca:v1.8.x\n~]# mkdir /data/k8s-yaml/orca\n~]# cd /data/k8s-yaml/orca/\norca]# vi dp.yaml\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  labels:\n    app: armory-orca\n  name: armory-orca\n  namespace: armory\nspec:\n  replicas: 1\n  revisionHistoryLimit: 7\n  selector:\n    matchLabels:\n      app: armory-orca\n  template:\n    metadata:\n      annotations:\n        artifact.spinnaker.io/location: '\"armory\"'\n        artifact.spinnaker.io/name: '\"armory-orca\"'\n        artifact.spinnaker.io/type: '\"kubernetes/deployment\"'\n        moniker.spinnaker.io/application: '\"armory\"'\n        moniker.spinnaker.io/cluster: '\"orca\"'\n      labels:\n        app: armory-orca\n    spec:\n      containers:\n      - name: armory-orca\n        image: harbor.od.com/armory/orca:v1.8.x\n        imagePullPolicy: IfNotPresent\n        command:\n        - bash\n        - -c\n        args:\n        - bash /opt/spinnaker/config/default/fetch.sh && cd /home/spinnaker/config\n          && /opt/orca/bin/orca\n        ports:\n        - containerPort: 8083\n          protocol: TCP\n        env:\n        - name: JAVA_OPTS\n          value: -Xmx1000M\n        envFrom:\n        - configMapRef:\n            name: init-env\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /health\n            port: 8083\n            scheme: HTTP\n          initialDelaySeconds: 600\n          periodSeconds: 5\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /health\n            port: 8083\n            scheme: HTTP\n          initialDelaySeconds: 180\n          periodSeconds: 3\n          successThreshold: 5\n          timeoutSeconds: 1\n        volumeMounts:\n        - mountPath: /etc/podinfo\n          name: podinfo\n        - mountPath: /opt/spinnaker/config/default\n          name: default-config\n        - mountPath: /opt/spinnaker/config/custom\n          name: custom-config\n      imagePullSecrets:\n      - name: harbor\n      volumes:\n      - configMap:\n          defaultMode: 420\n          name: custom-config\n        name: custom-config\n      - configMap:\n          defaultMode: 420\n          name: default-config\n        name: default-config\n      - downwardAPI:\n          defaultMode: 420\n          items:\n          - fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.labels\n            path: labels\n          - fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.annotations\n            path: annotations\n        name: podinfo\n\norca]# vi svc.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: armory-orca\n  namespace: armory\nspec:\n  ports:\n  - port: 8083\n    protocol: TCP\n    targetPort: 8083\n  selector:\n    app: armory-orca\n\norca]# kubectl apply -f ./dp.yaml\norca]# kubectl apply -f ./svc.yaml\n~~~\n\n\n\n#### 部署消息总线组件——Echo\n\n~~~\n# 200机器：\n~]# docker pull docker.io/armory/echo-armory:c36d576-release-1.8.x-617c567\n~]# docker images|grep echo\n~]# docker tag 415efd46f474 harbor.od.com/armory/echo:v1.8.x\n~]# docker push harbor.od.com/armory/echo:v1.8.x\n~]# mkdir /data/k8s-yaml/echo\n~]# cd /data/k8s-yaml/echo/\necho]# vi dp.yaml\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  labels:\n    app: armory-echo\n  name: armory-echo\n  namespace: armory\nspec:\n  replicas: 1\n  revisionHistoryLimit: 7\n  selector:\n    matchLabels:\n      app: armory-echo\n  template:\n    metadata:\n      annotations:\n        artifact.spinnaker.io/location: '\"armory\"'\n        artifact.spinnaker.io/name: '\"armory-echo\"'\n        artifact.spinnaker.io/type: '\"kubernetes/deployment\"'\n        moniker.spinnaker.io/application: '\"armory\"'\n        moniker.spinnaker.io/cluster: '\"echo\"'\n      labels:\n        app: armory-echo\n    spec:\n      containers:\n      - name: armory-echo\n        image: harbor.od.com/armory/echo:v1.8.x\n        imagePullPolicy: IfNotPresent\n        command:\n        - bash\n        - -c\n        args:\n        - bash /opt/spinnaker/config/default/fetch.sh && cd /home/spinnaker/config\n          && /opt/echo/bin/echo\n        ports:\n        - containerPort: 8089\n          protocol: TCP\n        env:\n        - name: JAVA_OPTS\n          value: -javaagent:/opt/echo/lib/jamm-0.2.5.jar -Xmx1000M\n        envFrom:\n        - configMapRef:\n            name: init-env\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /health\n            port: 8089\n            scheme: HTTP\n          initialDelaySeconds: 600\n          periodSeconds: 3\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /health\n            port: 8089\n            scheme: HTTP\n          initialDelaySeconds: 180\n          periodSeconds: 3\n          successThreshold: 5\n          timeoutSeconds: 1\n        volumeMounts:\n        - mountPath: /etc/podinfo\n          name: podinfo\n        - mountPath: /opt/spinnaker/config/default\n          name: default-config\n        - mountPath: /opt/spinnaker/config/custom\n          name: custom-config\n      imagePullSecrets:\n      - name: harbor\n      volumes:\n      - configMap:\n          defaultMode: 420\n          name: custom-config\n        name: custom-config\n      - configMap:\n          defaultMode: 420\n          name: default-config\n        name: default-config\n      - downwardAPI:\n          defaultMode: 420\n          items:\n          - fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.labels\n            path: labels\n          - fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.annotations\n            path: annotations\n        name: podinfo\n\necho]# vi svc.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: armory-echo\n  namespace: armory\nspec:\n  ports:\n  - port: 8089\n    protocol: TCP\n    targetPort: 8089\n  selector:\n    app: armory-echo\n\necho]# kubectl apply -f ./dp.yaml\necho]# kubectl apply -f ./svc.yaml\n~~~\n\n#### 部署流水线交互组件——Igor\n\n~~~~\n# 200机器：\n~]# docker pull docker.io/armory/spinnaker-igor-slim:release-1.8-x-new-install-healthy-ae2b329\n~]# docker images|grep igor\n~]# docker tag 23984f5b43f6 harbor.od.com/armory/igor:v1.8.x\n~]# docker push harbor.od.com/armory/igor:v1.8.x\n~]# mkdir /data/k8s-yaml/igor\n~]# cd /data/k8s-yaml/igor/\nigor]# vi dp.yaml\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  labels:\n    app: armory-igor\n  name: armory-igor\n  namespace: armory\nspec:\n  replicas: 1\n  revisionHistoryLimit: 7\n  selector:\n    matchLabels:\n      app: armory-igor\n  template:\n    metadata:\n      annotations:\n        artifact.spinnaker.io/location: '\"armory\"'\n        artifact.spinnaker.io/name: '\"armory-igor\"'\n        artifact.spinnaker.io/type: '\"kubernetes/deployment\"'\n        moniker.spinnaker.io/application: '\"armory\"'\n        moniker.spinnaker.io/cluster: '\"igor\"'\n      labels:\n        app: armory-igor\n    spec:\n      containers:\n      - name: armory-igor\n        image: harbor.od.com/armory/igor:v1.8.x\n        imagePullPolicy: IfNotPresent\n        command:\n        - bash\n        - -c\n        args:\n        - bash /opt/spinnaker/config/default/fetch.sh && cd /home/spinnaker/config\n          && /opt/igor/bin/igor\n        ports:\n        - containerPort: 8088\n          protocol: TCP\n        env:\n        - name: IGOR_PORT_MAPPING\n          value: -8088:8088\n        - name: JAVA_OPTS\n          value: -Xmx1000M\n        envFrom:\n        - configMapRef:\n            name: init-env\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /health\n            port: 8088\n            scheme: HTTP\n          initialDelaySeconds: 600\n          periodSeconds: 3\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /health\n            port: 8088\n            scheme: HTTP\n          initialDelaySeconds: 180\n          periodSeconds: 5\n          successThreshold: 5\n          timeoutSeconds: 1\n        volumeMounts:\n        - mountPath: /etc/podinfo\n          name: podinfo\n        - mountPath: /opt/spinnaker/config/default\n          name: default-config\n        - mountPath: /opt/spinnaker/config/custom\n          name: custom-config\n      imagePullSecrets:\n      - name: harbor\n      securityContext:\n        runAsUser: 0\n      volumes:\n      - configMap:\n          defaultMode: 420\n          name: custom-config\n        name: custom-config\n      - configMap:\n          defaultMode: 420\n          name: default-config\n        name: default-config\n      - downwardAPI:\n          defaultMode: 420\n          items:\n          - fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.labels\n            path: labels\n          - fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.annotations\n            path: annotations\n        name: podinfo\n\nigor]# vi svc.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: armory-igor\n  namespace: armory\nspec:\n  ports:\n  - port: 8088\n    protocol: TCP\n    targetPort: 8088\n  selector:\n    app: armory-igor\n\nigor]# kubectl apply -f ./dp.yaml\nigor]# kubectl apply -f ./svc.yaml\n~~~~\n\n\n\n#### 部署API提供组件——Gate\n\n~~~\n# 200机器：\n~]# docker pull docker.io/armory/gate-armory:dfafe73-release-1.8.x-5d505ca\n~]# docker images|grep gate\n~]# docker tag b092d4665301 harbor.od.com/armory/gate:v1.8.x\n~]# docker push harbor.od.com/armory/gate:v1.8.x\n~]# mkdir /data/k8s-yaml/gate\n~]# cd /data/k8s-yaml/gate/\ngate]# vi dp.yaml\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  labels:\n    app: armory-gate\n  name: armory-gate\n  namespace: armory\nspec:\n  replicas: 1\n  revisionHistoryLimit: 7\n  selector:\n    matchLabels:\n      app: armory-gate\n  template:\n    metadata:\n      annotations:\n        artifact.spinnaker.io/location: '\"armory\"'\n        artifact.spinnaker.io/name: '\"armory-gate\"'\n        artifact.spinnaker.io/type: '\"kubernetes/deployment\"'\n        moniker.spinnaker.io/application: '\"armory\"'\n        moniker.spinnaker.io/cluster: '\"gate\"'\n      labels:\n        app: armory-gate\n    spec:\n      containers:\n      - name: armory-gate\n        image: harbor.od.com/armory/gate:v1.8.x\n        imagePullPolicy: IfNotPresent\n        command:\n        - bash\n        - -c\n        args:\n        - bash /opt/spinnaker/config/default/fetch.sh gate && cd /home/spinnaker/config\n          && /opt/gate/bin/gate\n        ports:\n        - containerPort: 8084\n          name: gate-port\n          protocol: TCP\n        - containerPort: 8085\n          name: gate-api-port\n          protocol: TCP\n        env:\n        - name: GATE_PORT_MAPPING\n          value: -8084:8084\n        - name: GATE_API_PORT_MAPPING\n          value: -8085:8085\n        - name: JAVA_OPTS\n          value: -Xmx1000M\n        envFrom:\n        - configMapRef:\n            name: init-env\n        livenessProbe:\n          exec:\n            command:\n            - /bin/bash\n            - -c\n            - wget -O - http://localhost:8084/health || wget -O - https://localhost:8084/health\n          failureThreshold: 5\n          initialDelaySeconds: 600\n          periodSeconds: 5\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          exec:\n            command:\n            - /bin/bash\n            - -c\n            - wget -O - http://localhost:8084/health?checkDownstreamServices=true&downstreamServices=true\n              || wget -O - https://localhost:8084/health?checkDownstreamServices=true&downstreamServices=true\n          failureThreshold: 3\n          initialDelaySeconds: 180\n          periodSeconds: 5\n          successThreshold: 10\n          timeoutSeconds: 1\n        volumeMounts:\n        - mountPath: /etc/podinfo\n          name: podinfo\n        - mountPath: /opt/spinnaker/config/default\n          name: default-config\n        - mountPath: /opt/spinnaker/config/custom\n          name: custom-config\n      imagePullSecrets:\n      - name: harbor\n      securityContext:\n        runAsUser: 0\n      volumes:\n      - configMap:\n          defaultMode: 420\n          name: custom-config\n        name: custom-config\n      - configMap:\n          defaultMode: 420\n          name: default-config\n        name: default-config\n      - downwardAPI:\n          defaultMode: 420\n          items:\n          - fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.labels\n            path: labels\n          - fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.annotations\n            path: annotations\n        name: podinfo\n\ngate]# vi svc.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: armory-gate\n  namespace: armory\nspec:\n  ports:\n  - name: gate-port\n    port: 8084\n    protocol: TCP\n    targetPort: 8084\n  - name: gate-api-port\n    port: 8085\n    protocol: TCP\n    targetPort: 8085\n  selector:\n    app: armory-gate\n\ngate]# kubectl apply -f ./dp.yaml\ngate]# kubectl apply -f ./svc.yaml\n~~~\n\n![1584023952250](assets/1584023952250.png)\n\n5个\n\n#### 部署前端网页项目——Deck\n\n~~~\n# 200机器：\n~]# docker pull docker.io/armory/deck-armory:d4bf0cf-release-1.8.x-0a33f94\n~]# docker images|grep deck\n~]# docker tag 9a87ba3b319f harbor.od.com/armory/deck:v1.8.x\n~]# docker push harbor.od.com/armory/deck:v1.8.x\n~]# mkdir /data/k8s-yaml/deck\n~]# cd /data/k8s-yaml/deck/\ndeck]# vi dp.yaml\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  labels:\n    app: armory-deck\n  name: armory-deck\n  namespace: armory\nspec:\n  replicas: 1\n  revisionHistoryLimit: 7\n  selector:\n    matchLabels:\n      app: armory-deck\n  template:\n    metadata:\n      annotations:\n        artifact.spinnaker.io/location: '\"armory\"'\n        artifact.spinnaker.io/name: '\"armory-deck\"'\n        artifact.spinnaker.io/type: '\"kubernetes/deployment\"'\n        moniker.spinnaker.io/application: '\"armory\"'\n        moniker.spinnaker.io/cluster: '\"deck\"'\n      labels:\n        app: armory-deck\n    spec:\n      containers:\n      - name: armory-deck\n        image: harbor.od.com/armory/deck:v1.8.x\n        imagePullPolicy: IfNotPresent\n        command:\n        - bash\n        - -c\n        args:\n        - bash /opt/spinnaker/config/default/fetch.sh && /entrypoint.sh\n        ports:\n        - containerPort: 9000\n          protocol: TCP\n        envFrom:\n        - configMapRef:\n            name: init-env\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /\n            port: 9000\n            scheme: HTTP\n          initialDelaySeconds: 180\n          periodSeconds: 3\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /\n            port: 9000\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 3\n          successThreshold: 5\n          timeoutSeconds: 1\n        volumeMounts:\n        - mountPath: /etc/podinfo\n          name: podinfo\n        - mountPath: /opt/spinnaker/config/default\n          name: default-config\n        - mountPath: /opt/spinnaker/config/custom\n          name: custom-config\n      imagePullSecrets:\n      - name: harbor\n      volumes:\n      - configMap:\n          defaultMode: 420\n          name: custom-config\n        name: custom-config\n      - configMap:\n          defaultMode: 420\n          name: default-config\n        name: default-config\n      - downwardAPI:\n          defaultMode: 420\n          items:\n          - fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.labels\n            path: labels\n          - fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.annotations\n            path: annotations\n        name: podinfo\n\ndeck]# vi svc.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: armory-deck\n  namespace: armory\nspec:\n  ports:\n  - port: 80\n    protocol: TCP\n    targetPort: 9000\n  selector:\n    app: armory-deck\n\ndeck]# kubectl apply -f ./dp.yaml\ndeck]# kubectl apply -f ./svc.yaml\n~~~\n\n#### 部署前端代理——Nginx\n\n~~~\n# 200机器：\n~]# docker pull nginx:1.12.2\n~]# docker images|grep nginx\n~]# docker tag 4037a5562b03 harbor.od.com/armory/nginx:v1.12.2\n~]# docker push harbor.od.com/armory/nginx:v1.12.2\n~]# mkdir /data/k8s-yaml/nginx\n~]# cd /data/k8s-yaml/nginx/\nnginx]# vi dp.yaml\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  labels:\n    app: armory-nginx\n  name: armory-nginx\n  namespace: armory\nspec:\n  replicas: 1\n  revisionHistoryLimit: 7\n  selector:\n    matchLabels:\n      app: armory-nginx\n  template:\n    metadata:\n      annotations:\n        artifact.spinnaker.io/location: '\"armory\"'\n        artifact.spinnaker.io/name: '\"armory-nginx\"'\n        artifact.spinnaker.io/type: '\"kubernetes/deployment\"'\n        moniker.spinnaker.io/application: '\"armory\"'\n        moniker.spinnaker.io/cluster: '\"nginx\"'\n      labels:\n        app: armory-nginx\n    spec:\n      containers:\n      - name: armory-nginx\n        image: harbor.od.com/armory/nginx:v1.12.2\n        imagePullPolicy: Always\n        command:\n        - bash\n        - -c\n        args:\n        - bash /opt/spinnaker/config/default/fetch.sh nginx && nginx -g 'daemon off;'\n        ports:\n        - containerPort: 80\n          name: http\n          protocol: TCP\n        - containerPort: 443\n          name: https\n          protocol: TCP\n        - containerPort: 8085\n          name: api\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /\n            port: 80\n            scheme: HTTP\n          initialDelaySeconds: 180\n          periodSeconds: 3\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /\n            port: 80\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 3\n          successThreshold: 5\n          timeoutSeconds: 1\n        volumeMounts:\n        - mountPath: /opt/spinnaker/config/default\n          name: default-config\n        - mountPath: /etc/nginx/conf.d\n          name: custom-config\n      imagePullSecrets:\n      - name: harbor\n      volumes:\n      - configMap:\n          defaultMode: 420\n          name: custom-config\n        name: custom-config\n      - configMap:\n          defaultMode: 420\n          name: default-config\n        name: default-config\n\nnginx]# vi svc.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: armory-nginx\n  namespace: armory\nspec:\n  ports:\n  - name: http\n    port: 80\n    protocol: TCP\n    targetPort: 80\n  - name: https\n    port: 443\n    protocol: TCP\n    targetPort: 443\n  - name: api\n    port: 8085\n    protocol: TCP\n    targetPort: 8085\n  selector:\n    app: armory-nginx\n\nnginx]# vi ingress.yaml\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  labels:\n    app: spinnaker\n    web: spinnaker.od.com\n  name: armory-nginx\n  namespace: armory\nspec:\n  rules:\n  - host: spinnaker.od.com\n    http:\n      paths:\n      - backend:\n          serviceName: armory-nginx\n          servicePort: 80\n~~~\n\n\n\n~~~\n# 11机器，解析域名：\n~]# vi /var/named/od.com.zone\nserial 前滚一位\nspinnaker          A    10.4.7.10\n\n~]# systemctl restart named\n~]# dig -t A spinnaker.od.com +short\n#out: 10.4.7.10 \n~~~\n\n![1581925138470](assets/1581925138470.png)\n\n~~~\n# 200机器，应用资源清单：\nnginx]# kubectl apply -f ./dp.yaml\nnginx]# kubectl apply -f ./svc.yaml\nnginx]# kubectl apply -f ./ingress.yaml\n~~~\n\n![1584024475469](assets/1584024475469.png)\n\n[spinnaker.od.com](spinnaker.od.com)\n\n![1584024561816](assets/1584024561816.png)\n\n完成\n\n\n\n### 使用spinnaker结合Jenkins构建镜像\n\n把test和prod里的deployment的dubbo删掉，结果如图\n\n![1584024722628](assets/1584024722628.png)\n\n![1584024764124](assets/1584024764124.png)\n\n删完\n\n\n\n这时候spinnaker已经没了dubbo了，我们创建\n\n![1584024817000](assets/1584024817000.png)\n\n![1584057829023](assets/1584057829023.png)\n\n刷新spinnaker，点进去，如果页面抖动的可以调成125比例\n\n查看minio\n\n![1584058052650](assets/1584058052650.png)\n\n制作流水线\n\n![1584058085990](assets/1584058085990.png)\n\n![1584058106075](assets/1584058106075.png)\n\n> 先制作service\n\n添加4个参数\n\n![1584058190590](assets/1584058190590.png)\n\n![1584058342950](assets/1584058342950.png)\n\n> 版本号、分支、commitID\n\n![1585354175145](assets/1585354175145.png)\n\n> 标签等\n\n![1584058243942](assets/1584058243942.png)\n\n> 名字\n\n![1584058287708](assets/1584058287708.png)\n\n> 仓库名\n\n![1584058477180](assets/1584058477180.png)\n\n> 保存后增加一个流水线的阶段\n\n![1584058493370](assets/1584058493370.png)\n\n![1584058537386](assets/1584058537386.png)\n\n> 可以看到Jenkins已经被加载进来了，下面的参数化构建的选项也被加载进来了\n\n~~~\n# 填入对应参数，下面不能复制粘贴\n~~~\n\n![1584058979741](assets/1584058979741.png)\n\n> 建好了，运行\n>\n\n![1584059021370](assets/1584059021370.png)\n\n![1584059050225](assets/1584059050225.png)\n\n> 点击Details，然后下拉可以看到#9，然后点进去\n>\n\n![1584063826998](assets/1584063826998.png)\n\n> 运行中显示蓝条、成功是绿条、失败是红条\n\n![1584063848587](assets/1584063848587.png)\n\n> 跳转到Jenkins了\n>\n\n![1584063864144](assets/1584063864144.png)\n\n> 完成后去看harbor\n>\n\n![1584064090529](assets/1584064090529.png)\n\n在公司，新上项目投产只需要这样点点点即可。\n\n\n\n### 使用spinnaker配置dubbo服务提供者发布至K8S\n\n![1584064188189](assets/1584064188189.png)\n\n> 开始制作deployment，之前我们是用yaml文本，现在我们只要点点点\n\n![1584064212691](assets/1584064212691.png)\n\n![1584064339967](assets/1584064339967.png)\n\n> Accont：账户（选择集群管理员）\n>\n> Namespace：所发布的名称空间\n>\n> Detail：项目名字，要和git的保持一致\n>\n> Containers：要用那个镜像，从harbor里选\n>\n> 对准问号会有注释\n\n![1584064378399](assets/1584064378399.png)\n\n> Strategy：发布策略，选择滚动升级，还有个就是重新构建\n\n![1584064490029](assets/1584064490029.png)\n\n> 挂载日志\n\n![1584065127079](assets/1584065127079.png)\n\n> 要连接到Prometheus的设置\n\n![1584065253695](assets/1584065253695.png)\n\n200机器： cat /data/k8s-yaml/test/dubbo-demo-service/dp.yaml，复制指定Value内容填入Value\n\n![1584065443152](assets/1584065443152.png)\n\n![1584065496089](assets/1584065496089.png)\n\n> 传入环境变量：一个是JAR、一个是Apollo\n\n![1584065532833](assets/1584065532833.png)\n\n> 控制资源大小\n\n![1584065559311](assets/1584065559311.png)\n\n> 挂载日志\n\n![1584065607642](assets/1584065607642.png)\n\n> 就绪性探针：\n>\n> ​\t测试端口：20880\n\n![1584065630673](assets/1584065630673.png)\n\n\n\n> 拿什么用户去执行\n\n填写第二个container，配filebeat\n\n![1584065690268](assets/1584065690268.png)\n\n![1584066057307](assets/1584066057307.png)\n\n![1584066134675](assets/1584066134675.png)\n\n修改底包\n\n![1584068096613](assets/1584068096613.png)\n\n![1584068118456](assets/1584068118456.png)\n\n更改版本，点这里\n\n![1584066764236](assets/1584066764236.png)\n\n然后再这里修改一些内容\n\n~~~\n\"imageId\": \"harbor.od.com/${parameters.image_name}:${parameters.git_ver}_${parameters.add_tag}\",\n\"registry\": \"harbor.od.com\",\n\"repository\": \"${parameters.image_name}\",\n\"tag\": \"${parameters.git_ver}_${parameters.add_tag}\"\n~~~\n\n![1584066809117](assets/1584066809117.png)\n\n![1584066841074](assets/1584066841074.png)\n\n![1584066869312](assets/1584066869312.png)\n\n然后update并save\n\n![1584066908437](assets/1584066908437.png)\n\n![1584066931500](assets/1584066931500.png)\n\n![1584067032466](assets/1584067032466.png)\n\n![1584067042410](assets/1584067042410.png)\n\n第一阶段完成后就开始第二阶段\n\n![1584067152277](assets/1584067152277.png)\n\n![1584067170255](assets/1584067170255.png)\n\n全部完成后，查看K8S\n\n![1584067199269](assets/1584067199269.png)\n\n![1584067285901](assets/1584067285901.png)\n\n~~~\n# 22机器\n~]# docker ps -a|grep dubbo-demo-service\n~]# docker exec -ti 0f46cfc60e82 bash\n#/ cd /logm\n#/ ls\n#/ cat stdout.log\n~~~\n\n![1584068666418](assets/1584068666418.png)\n\n有日志了\n\n\n\n查看kibana的日志\n\n![1584068537962](assets/1584068537962.png)\n\n![1584068699067](assets/1584068699067.png)\n\n完成（PS：log日志去到es里也需要时间，我等了大约两分钟）\n\n\n\n### 使用spinnaker配置dubbo服务消费者到K8S\n\n创建流水线\n\n![1584078407553](assets/1584078407553.png)\n\n![1584078439769](assets/1584078439769.png)\n\n![1584078491859](assets/1584078491859.png)\n\n![1584078517624](assets/1584078517624.png)\n\n![1584078573644](assets/1584078573644.png)\n\n![1584078612786](assets/1584078612786.png)\n\n![1584078672019](assets/1584078672019.png)\n\n![1584083114651](assets/1584083114651.png)\n\n先制作镜像，看有没有问题\n\n![1584079045516](assets/1584079045516.png)\n\n![1584079077761](assets/1584079077761.png)\n\n去看一下Jenkins能不能打包\n\n![1584079104250](assets/1584079104250.png)\n\n![1584079120220](assets/1584079120220.png)\n\nsuccess后\n\n现在相当于已经配了dp.yaml，这是web项目，所以还要配svc和ingress\n\n![1584079151477](assets/1584079151477.png)\n\n![1584079186625](assets/1584079186625.png)\n\n![1584079823806](assets/1584079823806.png)\n\n![1584079256815](assets/1584079256815.png)\n\n![1584079884058](assets/1584079884058.png)\n\n这时候已经K8S里已经有svc（我把原来的删掉了）\n\n![1584079910637](assets/1584079910637.png)\n\n以前的ingress我也顺便删了\n\n![1584079645933](assets/1584079645933.png)\n\n做完svc还有ingress\n\n![1584079330070](assets/1584079330070.png)\n\n![1584079347814](assets/1584079347814.png)\n\n![1584080007523](assets/1584080007523.png)\n\n去K8S里看\n\n![1584080082818](assets/1584080082818.png)\n\n然后继续\n\n![1584080109009](assets/1584080109009.png)\n\n![1584080129596](assets/1584080129596.png)\n\n![1584080169008](assets/1584080169008.png)\n\n![1584083845728](assets/1584083845728.png)\n\n> 编号可能不同，因为我重新做了一版tomcat，但是无所谓，后面我们都会用变量弄\n\n![1584080263076](assets/1584080263076.png)\n\n![1584083914202](assets/1584083914202.png)\n\n![1584080330316](assets/1584080330316.png)\n\n![1584080535614](assets/1584080535614.png)\n\n第二个container\n\n同样200机器去拿那段\n\n![1584081101719](assets/1584081101719.png)\n\n![1584081175808](assets/1584081175808.png)\n\n第三个container\n\n![1584081304018](assets/1584081304018.png)\n\n![1584081338810](assets/1584081338810.png)\n\n![1584081381128](assets/1584081381128.png)\n\n![1584081394888](assets/1584081394888.png)\n\n~~~\n\"imageId\": \"harbor.od.com/${parameters.image_name}:${parameters.git_ver}_${parameters.add_tag}\",\n\"registry\": \"harbor.od.com\",\n\"repository\": \"${parameters.image_name}\",\n\"tag\": \"${parameters.git_ver}_${parameters.add_tag}\"\n~~~\n\n![1584081434175](assets/1584081434175.png)\n\nupdate\n\n![1584081462543](assets/1584081462543.png)\n\n首先，网址是没有内容的\n\n![1584081529356](assets/1584081529356.png)\n\n![1584081486194](assets/1584081486194.png)\n\n继续构建\n\n![1584081573772](assets/1584081573772.png)\n\n![1584081698091](assets/1584081698091.png)\n\n> 可以看到只花了两分钟不到\n\n查看K8S里面起来了没有\n\n![1584081739090](assets/1584081739090.png)\n\n然后再刷新页面\n\n![1584084336732](assets/1584084336732.png)\n\n测试页面正常，去kibana（等数据过来又是漫长的过程）\n\n![1584084381158](assets/1584084381158.png)\n\n![1584084728950](assets/1584084728950.png)\n\n完成\n\n\n\n### 模拟生产上代码迭代\n\n这里我就直接用gitlab，你用自己的git即可，tomcat分支（`dubbo-client/src/main/java/com/od/dubbotest/action/HelloAction.java`）\n\n![1584085399503](assets/1584085399503.png)\n\n复制id\n\n![1584085534421](assets/1584085534421.png)\n\n![1584085463078](assets/1584085463078.png)\n\n![1584085573303](assets/1584085573303.png)\n\n![1584085726806](assets/1584085726806.png)\n\n完成后刷新页面\n\n![1584085742862](assets/1584085742862.png)\n\n> 版本更新你最需要点这么几下就可以了，全程用不到1分钟\n\n确认一致，可以上线了\n\n![1584085802694](assets/1584085802694.png)\n\n![1584085844657](assets/1584085844657.png)\n\n![1584085871848](assets/1584085871848.png)\n\n![1584085899129](assets/1584085899129.png)\n\n![1584085940886](assets/1584085940886.png)\n\n![1584085965431](assets/1584085965431.png)\n\n![1584086000580](assets/1584086000580.png)\n\n![1584086046821](assets/1584086046821.png)\n\n![1584086075157](assets/1584086075157.png)\n\n![1584086138413](assets/1584086138413.png)\n\n![1584086157826](assets/1584086157826.png)\n\n![1584086175983](assets/1584086175983.png)\n\n![1584086277429](assets/1584086277429.png)\n\n第二个container\n\n同样的方法去拿value：cat /data/k8s-yaml/prod/dubbo-demo-service/dp.yaml\n\n![1584086436864](assets/1584086436864.png)\n\n![1584086463792](assets/1584086463792.png)\n\n第三个container\n\n![1584086522123](assets/1584086522123.png)\n\n![1584086550684](assets/1584086550684.png)\n\n\n\n![1584086871253](assets/1584086871253.png)\n\n~~~\n# 添加以下内容\n\"imageId\": \"harbor.od.com/${parameters.image_name}:${parameters.git_ver}_${parameters.add_tag}\",\n\"registry\": \"harbor.od.com\",\n\"repository\": \"${parameters.image_name}\",\n\"tag\": \"${parameters.git_ver}_${parameters.add_tag}\"\n~~~\n\n![1584086852397](assets/1584086852397.png)\n\n![1584086900158](assets/1584086900158.png)\n\n看一下哪个版本通过测试了\n\n![1584086932887](assets/1584086932887.png)\n\n复制\n\n![1584086963769](assets/1584086963769.png)\n\n![1584087001949](assets/1584087001949.png)\n\n![1584087023343](assets/1584087023343.png)\n\n![1584087116902](assets/1584087116902.png)\n\n起来了，我们把prod环境里面的service和ingress的consumer干掉\n\n![1584087185801](assets/1584087185801.png)\n\n![1584087200129](assets/1584087200129.png)\n\n开始制作services\n\n![1584087239935](assets/1584087239935.png)\n\n![1584087255667](assets/1584087255667.png)\n\n![1584087279759](assets/1584087279759.png)\n\n再去做ingress\n\n![1584087808345](assets/1584087808345.png)\n\n![1584087822173](assets/1584087822173.png)\n\ningress也好了\n\n跑起来（其实就跟test一样）\n\n![1584088360303](assets/1584088360303.png)\n\n![1584088366782](assets/1584088366782.png)\n\n![1584088379944](assets/1584088379944.png)![1584088384747](assets/1584088384747.png)\n\n![1584088390258](assets/1584088390258.png)\n\n![1584088394971](assets/1584088394971.png)\n\n![1584088400134](assets/1584088400134.png)\n\n![1584088403965](assets/1584088403965.png)\n\n第二个container\n\n![1584089282070](assets/1584089282070.png)\n\n![1584089287727](assets/1584089287727.png)\n\n![1584089291709](assets/1584089291709.png)\n\n![1584089303135](assets/1584089303135.png)\n\n![1584089307606](assets/1584089307606.png)\n\n![1584089311723](assets/1584089311723.png)\n\n这下面的信息是用test的信息，因为test已经测试过没问题了\n\n![1584089319292](assets/1584089319292.png)\n\n![1584089345632](assets/1584089345632.png)\n\n![1584089352491](assets/1584089352491.png)\n\n会提示替代web，不同管，RUN\n\n![1584089265210](assets/1584089265210.png)\n\n成功\n\n当然你可能担心test的web真的被替代了，可以看一下\n\n![1584089405816](assets/1584089405816.png)\n\n也是没问题的，我们再看下kibana有没有数据\n\n![1584089444787](assets/1584089444787.png)\n\n也有\n\n### 恭喜你，顺利毕业！\n\n"
        },
        {
          "name": "第六章——在K8S中集成Apollo配置中心.md",
          "type": "blob",
          "size": 63.1181640625,
          "content": "## 第六章——在K8S中集成Apollo配置中心\n\n> **前言**：\n>\n> **运维八荣八耻**：\n>\n> - 以可配置为荣，以硬编码为耻\n> - 以互备为荣，以单点为耻\n> - 以随时重启为荣，以不能迁移为耻\n> - 以整体交付为荣，以部分交付为耻\n> - 以无状态为荣，以有状态为耻\n> - 以标准化为荣，以特殊化为耻\n> - 以自动化工具为荣，以手动和人肉为耻\n> - 以无人值守为荣，以人工介入为耻\n>\n> **目前我们交付进K8S集群的两个dubbo微服务和monitor，它们的配置都是写死在容器里的**\n>\n> **配置管理的现状**：\n>\n> - 配置散乱格式不标准（XML、ini、conf、yaml...）\n> - 主要采用本地静态配置，应用多副本集下配置修改麻烦\n> - 易引发生产事故（测试环境、生产环境配置混用）\n> - 配置缺乏安全审计和版本控制功能（config review）\n> - 不同环境的应用，配置不同，造成多次打包，测试失败\n>\n> **配置中心是什么？**：\n>\n> - 顾名思义，就是集中管理应用程序配置的“中心”\n\n\n\n> 常见的配置中心：\n>\n> - SprigCloudConfig\n> - K8S ConfigMap\n> - Apollo：基于SprigCloudConfig\n> - ...\n>\n> 目前具有争议的一般是使用Apollo还是SprigCloudConfig，看对比图，所以我们使用Apollo，而且Apollo是基于SprigCloudConfig，也就是你交付了Apollo也相当于交付了微服务\n\n![1582622382181](assets/1582622382181.png)\n\n### configmap使用详解\n\n> **WHAT**：就是为了让镜像 和 配置文件解耦，以便实现镜像的可移植性和可复用性，因为一个configMap其实就是一系列配置信息的集合，将来可直接注入到Pod中的容器使用\n>\n> **WHY**：为了配合Apollo使用\n\n使用configmap管理应用配置，需要先拆分环境，拆分未test和pro环境来模拟实际工作\n\n先把dubbo的服务（消费者/服务站/监视者）都scale成0，这样就没有容器在里面跑了\n\n![1583162321302](assets/1583162321302.png)\n\n![1583162339688](assets/1583162339688.png)\n\n拆分zk成测试环境和生产环境来模拟实际情况，如下：\n\n| 主机名            | 角色                 | ip        |\n| ----------------- | -------------------- | --------- |\n| HDSS7-11.host.com | zk1.od.com(Test环境) | 10.4.7.11 |\n| HDSS7-12.host.com | zk2.od.com(Prod环境) | 10.4.7.12 |\n\n之前是11、12、21一共3台，我们先把21拆了\n\n~~~\n# 11/12/21机器，停掉zookeeper并删掉相关data文件和log文件的内容：\n~]# cd /opt/zookeeper\nzookeeper]# bin/zkServer.sh stop\n# 停不掉就 kill -9 id 杀掉\nzookeeper]# ps aux|grep zoo\nzookeeper]# cd /data/zookeeper/data/\ndata]# rm -fr ./*\ncd ../logs/\nlogs]# rm -fr ./*\n~~~\n\n> **kill -9**：强制杀死该进程\n\n![1581059149533](assets/1581059149533.png)\n\n~~~\n# 11/12机器，修改zoo.cfg配置：\nzookeeper]# cd /opt/zookeeper/conf\nzookeeper]# vi zoo.cfg\n# 把下面的三行都删掉，不需要组成集群，如图\n~~~\n\n![1581059640346](assets/1581059640346.png)\n\n~~~\n# 11/12机器，启动zk：\ncd /opt/zookeeper/\nzookeeper]# bin/zkServer.sh start\nzookeeper]# ps aux|grep zoo\nzookeeper]# bin/zkServer.sh status\n~~~\n\n![1584697777390](assets/1584697777390.png)\n\nMode: standalone 模式，拆分完成\n\n~~~\n# 200机器，准备资源配置清单：\n~]# vi /data/k8s-yaml/dubbo-monitor/cm.yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: dubbo-monitor-cm\n  namespace: infra\ndata:\n  dubbo.properties: |\n    dubbo.container=log4j,spring,registry,jetty\n    dubbo.application.name=simple-monitor\n    dubbo.application.owner=ben1234560\n    dubbo.registry.address=zookeeper://zk1.od.com:2181\n    dubbo.protocol.port=20880\n    dubbo.jetty.port=8080\n    dubbo.jetty.directory=/dubbo-monitor-simple/monitor\n    dubbo.charts.directory=/dubbo-monitor-simple/charts\n    dubbo.statistics.directory=/dubbo-monitor-simple/statistics\n    dubbo.log4j.file=/dubbo-monitor-simple/logs/dubbo-monitor.log\n    dubbo.log4j.level=WARN\n\n~]# vi /data/k8s-yaml/dubbo-monitor/dp2.yaml\nkind: Deployment\napiVersion: extensions/v1beta1\nmetadata:\n  name: dubbo-monitor\n  namespace: infra\n  labels: \n    name: dubbo-monitor\nspec:\n  replicas: 1\n  selector:\n    matchLabels: \n      name: dubbo-monitor\n  template:\n    metadata:\n      labels: \n        app: dubbo-monitor\n        name: dubbo-monitor\n    spec:\n      containers:\n      - name: dubbo-monitor\n        image: harbor.od.com/infra/dubbo-monitor:latest\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n        - containerPort: 20880\n          protocol: TCP\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n          - name: configmap-volume\n            mountPath: /dubbo-monitor-simple/conf\n      volumes:\n        - name: configmap-volume\n          configMap:\n            name: dubbo-monitor-cm\n      imagePullSecrets:\n      - name: harbor\n      restartPolicy: Always\n      terminationGracePeriodSeconds: 30\n      securityContext: \n        runAsUser: 0\n      schedulerName: default-scheduler\n  strategy:\n    type: RollingUpdate\n    rollingUpdate: \n      maxUnavailable: 1\n      maxSurge: 1\n  revisionHistoryLimit: 7\n  progressDeadlineSeconds: 600\n~~~\n\n\n\n~~~\n# 对比两个dp有什么不同，200机器：\ncd /data/k8s-yaml/dubbo-monitor/\ndubbo-monitor]# vimdiff dp.yaml dp2.yaml\n#:qall 退出\n~~~\n\n> **vimdiff **：编辑同一文件的不同历史版本，对各文件的内容进行比对与调整\n>\n> 没有vimdiff 的下载 yum install vim -y\n\n![1583198466051](assets/1583198466051.png)\n\n> 这里两个文件不同的地方，注意，这里的image的版本因为我的操作问题所以一个v2一个没有v，你的应该是都没有的\n\n~~~\n# 200机器，替换dp：\ndubbo-monitor]# mv dp.yaml /tmp/\ndubbo-monitor]# mv dp2.yaml dp.yaml\n~~~\n\n> **mv**：如果mv后是目录，则是移动，如果是文件名，则是更改名字\n\n~~~\n# 应用资源配置清单，22机器：\n~]# kubectl apply -f http://k8s-yaml.od.com/dubbo-monitor/cm.yaml\n~]# kubectl apply -f http://k8s-yaml.od.com/dubbo-monitor/dp.yaml\n# 看一下相关容器有没有起来\n~~~\n\n![1583198817470](assets/1583198817470.png)\n\n![1583198829279](assets/1583198829279.png)\n\n![1583198887392](assets/1583198887392.png)\n\n![1583199215057](assets/1583199215057.png)\n\n我们改成zk2\n\n![1583199496585](assets/1583199496585.png)\n\n在删掉这个pod重启\n\n![1583199525277](assets/1583199525277.png)\n\n刷新页面变成zk2了\n\n![1583199653453](assets/1583199653453.png)\n\n完成\n\n\n\n#### 报错信息\n\n> flannel重启报错问题\n\n~~~\n# flannel重启需要增加以下内容，添加在最下面:\n21 ~]# vi /etc/sipervisord.d/flannel.ini\nkillasgroup=true\nstopasgroup=true\n~~~\n\n#### 如何排错：\n\n~~~\n# 查看时间是否正常\ndate\n\n# 查看kubernetes的报错\n21 ~]# tail -fn 200/data/logs/kubernetes/kube-kubelet/kubelet.stdout.log\n\n# 可以重启docker 和 kubelet\n21 ~]# systemctl restart docker \n21 ~]# ps aux|grep kubelet\n21 ~]# kill -9 id\n\n# 查看supervisor的状态\n21 ~]# supervisorctl status\n\n# 查看路由状态，是否能ping通其它机器\n21 ~]# route -n\n21 ~]# ping 172.7.22.2\n\n# 查看flannel情况\n21 ~]# tail -fn 200 /data/logs/flanneld/flanneld.stdout.log\n# 12flannel主机器上get网络，如果error则加入backend\n12 ~]# ./etcdctl get /coreos.com/network/config\n\n# 报错iptables，让机器一重启就自动使用，21/22机器\n~]# service iptables save\n# out: [ok]\n~~~\n\n#### cm对象的另一种创建方法（可不做）：\n\n~~~\n# 22机器：\ncd /opt/kubernetes/server/bin/conf\n# 当前目录下要又kubelet.kubeconfig的文件\nconf]# kubectl create cm kubelet-cm --from-file=./kubelet.kubeconfig\n#out: configmap/kubelet-cm created\n~~~\n\n可以在dashoboard的default名称空间下Config Maps看到\n\n随后记得删除\n\n\n\n#### 官方Apollo框架：\n\n> **WHAT**：请参考[官方总体设计](https://www.apolloconfig.com/#/zh/design/apollo-design)，Apollo框架的特性也已经在上面跟Spring Cloud做了对比。更多的解释可以参考[官方特性文档](https://github.com/apolloconfig/apollo#features)\n>\n> **WHY**：用该框架实现比Spring Cloud更优的能力，更多对比可参考[Nacos、Apollo、Spring Cloud Config微服务配置中心对比](https://www.jianshu.com/p/2f0ae9c7f2e1)\n\n![1587274472834](assets/1587274472834.png)\n\n#### 简化Apollo框架：\n\n> **WHY**：\n>\n> 参考[官方微服务架构](https://mp.weixin.qq.com/s/-hUaQPzfsl9Lm3IqQW3VDQ)使用Apollo架构V1\n>\n> Meta Server：\n>\n> - Portal通过域名访问MetaServer获取AdminService的地址列表\n> - Client通过域名访问MetaServer获取ConfigService的地址列表\n> - 相当于一个Eureka Proxy\n> - 逻辑角色，和ConfigService住在一起部署\n>\n> Eureka：\n>\n> - 用于服务发现和注册\n> - Config/AdminService注册实例并定期报心跳\n> - 和ConfigService住在一起部署\n>\n> 可以看到Eureka和Zookeeper是一样的功能，所以不再搭建重复的功能服务。\n>\n> 而为什么使用Zookeeper，总得来说Eureka基于AP保证高可用，遇到问题，时可以牺牲其一致性来保证系统服务的**高可用性**，既返回旧数据。Zookeeper基于CP，追求**数据一致性**。实际业务使用什么要基于业务考量。\n\n![1584698746125](assets/1584698746125.png)\n\n![1584698613252](assets/1584698613252.png)\n\n> Client通过推拉结合和ConfigService交互，然后ConfigService去拿ConfigDB里面的配置给Client端返回去。\n>\n> Portal是Apollo的一个仪表盘，通过调用AdminService去同步修改ConfigDB里面的配置\n>\n> 先交付ConfigService，然后交付AdminService，最后加portal\n\n\n\n### 交付Apollo-ConfigService到K8S\n\n[官网https://github.com/ctripcorp/apollo/](https://github.com/ctripcorp/apollo/)\n\n##### 安装部署MySQL数据库（作为`ConfigDB`/`PortalDB`）\n\n~~~~\n# 更新yum源，11机器：\n~]# vi /etc/yum.repos.d/MariaDB.repo\n[mariadb]\nname = MariaDB\nbaseurl = https://mirrors.ustc.edu.cn/mariadb/yum/10.1/centos7-amd64/\ngpgkey=https://mirrors.ustc.edu.cn/mariadb/yum/RPM-GPG-KEY-MariaDB\ngpgcheck=1\n\n# 导入GPG-KEY\n~]# rpm --import https://mirrors.ustc.edu.cn/mariadb/yum/RPM-GPG-KEY-MariaDB\n~]# yum list mariadb --show-duplicates\n# out: mariadb.x86_64\n~]# yum makecache\n# out:Metadata Cache Created\n# 更新数据库版本\n~]# yum list mariadb-server --show-duplicates\n~]# yum install mariadb-server -y\n# 编辑基础配置\n# 增加部分内容\n~] vi /etc/my.cnf.d/server.cnf\n[mysqld]\ncharacter_set_server = utf8mb4\ncollation_server = utf8mb4_general_ci\ninit_connect = \"SET NAMES 'utf8mb4'\"\n~~~~\n\n![1583202954447](assets/1583202954447.png)\n\n~~~\n# 11机器，修改基础配置,并启动：\n# 在改内容下增加，字符集\n~]# vi /etc/my.cnf.d/mysql-clients.cnf\n[mysql]\ndefault-character-set = utf8mb4\n\n~]# systemctl start mariadb\n~~~\n\n![1583202999075](assets/1583202999075.png)\n\n~~~\n# 11机器，设置,mysql密码：\n~]# mysqladmin -uroot password\n# 这里是设置密码：123456\n~]# mysql -uroot -p\n# 这里是输入密码\nnone)]> \\s\n# 确定SERVER\\DB\\CLIENT\\CONN 都是utf8\n~~~\n\n![1583203114762](assets/1583203114762.png)\n\n~~~\n# 11机器：\nnone)]> show databases;\nnone)]> drop database test;\nnone)]> exit\n# 查看数据库是否正常\n~]# ps aux|grep mysql\n~]# netstat luntp|grep 3306\n~]# wget https://raw.githubusercontent.com/ctripcorp/apollo/1.5.1/scripts/db/migration/configdb/V1.0.0__initialization.sql -O apolloconfig.sql\n~]# cat apolloconfig.sql\n~]# mysql -uroot -p < apolloconfig.sql\n# 输入密码\n~] mysql -uroot -p\n# 输密码\nnone)]> show databases;\n~~~\n\n![1581148090694](assets/1581148090694.png)\n\n~~~\n# 11机器，创建其它权限用户而不是直接给root权限：\nnone)]> use ApolloConfigDB;\nApolloConfigDB]> show tables;\nApolloConfigDB]> grant INSERT,DELETE,UPDATE,SELECT on ApolloConfigDB.* to 'apolloconfig'@'10.4.7.%' identified by \"123456\";\nApolloConfigDB]> select user,host from mysql.user;\n~~~\n\n![1583203450336](assets/1583203450336.png)\n\n~~~\n# 11机器,修改初始数据：\nApolloConfigDB]> show tables;\nApolloConfigDB]> select * from ServerConfig\\G\nApolloConfigDB]> update ApolloConfigDB.ServerConfig set ServerConfig.Value=\"http://config.od.com/eureka\" where ServerConfig.Key=\"eureka.service.url\";\nApolloConfigDB]> select * from ServerConfig\\G\n~~~\n\n原：\n\n![1583203490999](assets/1583203490999.png)\n\n改后\n\n![1583203522358](assets/1583203522358.png)\n\n~~~\n# 11机器，解析域名：\n~]# vi /var/named/od.com.zone\nserial 前滚一位\nconfig             A    10.4.7.10\n\n~]# systemctl restart named\n~~~\n\n![1583203561891](assets/1583203561891.png)\n\n~~~\n# 21机器，测试下21机器能不能查到(11机器是肯定能查到的)：\n~]# dig -t A config.od.com @192.168.0.2 +short\n# out:10.4.7.10\n~~~\n\n[包的官网地址](https://github.com/ctripcorp/apollo/releases/tag/v1.5.1)![1583204125452](assets/1583204125452.png)\n\n~~~\n# 200机器，制作docker镜像：\ncd /opt/src\nsrc]# wget https://github.com/ctripcorp/apollo/releases/download/v1.5.1/apollo-configservice-1.5.1-github.zip\n# 或者去官网下载或者用我的上传的包\nsrc]# mkdir /data/dockerfile/apollo-configservice\nsrc]# unzip -o apollo-configservice-1.5.1-github.zip -d /data/dockerfile/apollo-configservice\nsrc]# cd /data/dockerfile/apollo-configservice/\napollo-configservice]# rm -rf apollo-configservice-1.5.1-sources.jar\napollo-configservice]# ll\n~~~\n\n![1581232250106](assets/1581232250106.png)\n\n~~~\n# 11机器，解析域名：\n~]# vi /var/named/od.com.zone\nserial 前滚一位\nmysql              A    10.4.7.11\n\n~]# systemctl restart named\n~]# dig -t A mysql.od.com @10.4.7.11 +short\n# out: 10.4.7.11\n~~~\n\n![1582636827536](assets/1582636827536.png)\n\n~~~~\n# 200机器，修改账户密码：\ncd /data/dockerfile/apollo-configservice/config\nconfig]# vi application-github.properties\nspring.datasource.url = jdbc:mysql://mysql.od.com:3306/ApolloConfigDB?characterEncoding=utf8\nspring.datasource.username = apolloconfig\nspring.datasource.password = 123456\n~~~~\n\n![1581232483973](assets/1581232483973.png)\n\n~~~\n# 200机器：\ncd /data/dockerfile/apollo-configservice/scripts\nscripts]# rm -f shutdown.sh\n# 全部删掉，换成以下内容\nscripts]# vi startup.sh\n#!/bin/bash\nSERVICE_NAME=apollo-configservice\n## Adjust log dir if necessary\nLOG_DIR=/opt/logs/apollo-config-server\n## Adjust server port if necessary\nSERVER_PORT=8080\nAPOLLO_CONFIG_SERVICE_NAME=$(hostname -i)\nSERVER_URL=\"http://${APOLLO_CONFIG_SERVICE_NAME}:${SERVER_PORT}\"\n\n## Adjust memory settings if necessary\nexport JAVA_OPTS=\"-Xms128m -Xmx128m -Xss256k -XX:MetaspaceSize=128m -XX:MaxMetaspaceSize=384m -XX:NewSize=256m -XX:MaxNewSize=256m -XX:SurvivorRatio=8\"\n\n## Only uncomment the following when you are using server jvm\n#export JAVA_OPTS=\"$JAVA_OPTS -server -XX:-ReduceInitialCardMarks\"\n\n########### The following is the same for configservice, adminservice, portal ###########\nexport JAVA_OPTS=\"$JAVA_OPTS -XX:ParallelGCThreads=4 -XX:MaxTenuringThreshold=9 -XX:+DisableExplicitGC -XX:+ScavengeBeforeFullGC -XX:SoftRefLRUPolicyMSPerMB=0 -XX:+ExplicitGCInvokesConcurrent -XX:+HeapDumpOnOutOfMemoryError -XX:-OmitStackTraceInFastThrow -Duser.timezone=Asia/Shanghai -Dclient.encoding.override=UTF-8 -Dfile.encoding=UTF-8 -Djava.security.egd=file:/dev/./urandom\"\nexport JAVA_OPTS=\"$JAVA_OPTS -Dserver.port=$SERVER_PORT -Dlogging.file=$LOG_DIR/$SERVICE_NAME.log -XX:HeapDumpPath=$LOG_DIR/HeapDumpOnOutOfMemoryError/\"\n\n# Find Java\nif [[ -n \"$JAVA_HOME\" ]] && [[ -x \"$JAVA_HOME/bin/java\" ]]; then\n    javaexe=\"$JAVA_HOME/bin/java\"\nelif type -p java > /dev/null 2>&1; then\n    javaexe=$(type -p java)\nelif [[ -x \"/usr/bin/java\" ]];  then\n    javaexe=\"/usr/bin/java\"\nelse\n    echo \"Unable to find Java\"\n    exit 1\nfi\n\nif [[ \"$javaexe\" ]]; then\n    version=$(\"$javaexe\" -version 2>&1 | awk -F '\"' '/version/ {print $2}')\n    version=$(echo \"$version\" | awk -F. '{printf(\"%03d%03d\",$1,$2);}')\n    # now version is of format 009003 (9.3.x)\n    if [ $version -ge 011000 ]; then\n        JAVA_OPTS=\"$JAVA_OPTS -Xlog:gc*:$LOG_DIR/gc.log:time,level,tags -Xlog:safepoint -Xlog:gc+heap=trace\"\n    elif [ $version -ge 010000 ]; then\n        JAVA_OPTS=\"$JAVA_OPTS -Xlog:gc*:$LOG_DIR/gc.log:time,level,tags -Xlog:safepoint -Xlog:gc+heap=trace\"\n    elif [ $version -ge 009000 ]; then\n        JAVA_OPTS=\"$JAVA_OPTS -Xlog:gc*:$LOG_DIR/gc.log:time,level,tags -Xlog:safepoint -Xlog:gc+heap=trace\"\n    else\n        JAVA_OPTS=\"$JAVA_OPTS -XX:+UseParNewGC\"\n        JAVA_OPTS=\"$JAVA_OPTS -Xloggc:$LOG_DIR/gc.log -XX:+PrintGCDetails\"\n        JAVA_OPTS=\"$JAVA_OPTS -XX:+UseConcMarkSweepGC -XX:+UseCMSCompactAtFullCollection -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFraction=60 -XX:+CMSClassUnloadingEnabled -XX:+CMSParallelRemarkEnabled -XX:CMSFullGCsBeforeCompaction=9 -XX:+CMSClassUnloadingEnabled  -XX:+PrintGCDateStamps -XX:+PrintGCApplicationConcurrentTime -XX:+PrintHeapAtGC -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=5 -XX:GCLogFileSize=5M\"\n    fi\nfi\n\nprintf \"$(date) ==== Starting ==== \\n\"\n\ncd `dirname $0`/..\nchmod 755 $SERVICE_NAME\".jar\"\n./$SERVICE_NAME\".jar\" start\n\nrc=$?;\n\nif [[ $rc != 0 ]];\nthen\n    echo \"$(date) Failed to start $SERVICE_NAME.jar, return code: $rc\"\n    exit $rc;\nfi\n\ntail -f /dev/null\n~~~\n\n> [Apollo官方文档](https://github.com/ctripcorp/apollo/blob/1.5.1/scripts/apollo-on-kubernetes/apollo-config-server/scripts/startup-kubernetes.sh)\n>\n> 上述代码是从Apollo官网拉下来的，不过第7行多了一行APOLLO_CONFIG_SERVICE_NAME=$(hostname -i)\n>\n> 其中export JAVA_OPTS修改了下资源，改小了\n\n~~~\n# 200机器，给权限：\nscripts]# chmod u+x startup.sh\nscripts]# ll\n~~~\n\n![1581233647557](assets/1581233647557.png)\n\n~~~\n# 200机器，做dockerfile：\ncd /data/dockerfile/apollo-configservice\napollo-configservice]# vi Dockerfile\nFROM 909336740/jre8:8u112\n\nENV VERSION 1.5.1\n\nRUN ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime &&\\\n    echo \"Asia/Shanghai\" > /etc/timezone\n\nADD apollo-configservice-${VERSION}.jar /apollo-configservice/apollo-configservice.jar\nADD config/ /apollo-configservice/config\nADD scripts/ /apollo-configservice/scripts\n\nCMD [\"/apollo-configservice/scripts/startup.sh\"]\n~~~\n\n> [参考Apollo官网文档](https://github.com/ctripcorp/apollo/blob/1.5.1/scripts/apollo-on-kubernetes/apollo-config-server/Dockerfile)\n\n![1583219072270](assets/1583219072270.png)\n\n~~~\n# 200机器,制作容器：\napollo-configservice]# docker build . -t harbor.od.com/infra/apollo-configservice:v1.5.1\napollo-configservice]# docker push harbor.od.com/infra/apollo-configservice:v1.5.1\n~~~\n\n![1583205129961](assets/1583205129961.png)\n\n~~~\n# 制作资源配置清单，200机器：\n~]# mkdir /data/k8s-yaml/apollo-configservice\n~]# cd /data/k8s-yaml/apollo-configservice\napollo-configservice]# vi cm.yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: apollo-configservice-cm\n  namespace: infra\ndata:\n  application-github.properties: |\n    # DataSource\n    spring.datasource.url = jdbc:mysql://mysql.od.com:3306/ApolloConfigDB?characterEncoding=utf8\n    spring.datasource.username = apolloconfig\n    spring.datasource.password = 123456\n    eureka.service.url = http://config.od.com/eureka\n  app.properties: |\n    appId=100003171\n\napollo-configservice]# vi dp.yaml\nkind: Deployment\napiVersion: extensions/v1beta1\nmetadata:\n  name: apollo-configservice\n  namespace: infra\n  labels: \n    name: apollo-configservice\nspec:\n  replicas: 1\n  selector:\n    matchLabels: \n      name: apollo-configservice\n  template:\n    metadata:\n      labels: \n        app: apollo-configservice \n        name: apollo-configservice\n    spec:\n      volumes:\n      - name: configmap-volume\n        configMap:\n          name: apollo-configservice-cm\n      containers:\n      - name: apollo-configservice\n        image: harbor.od.com/infra/apollo-configservice:v1.5.1\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n        volumeMounts:\n        - name: configmap-volume\n          mountPath: /apollo-configservice/config\n        terminationMessagePath: /dev/termination-log\n        terminationMessagePolicy: File\n        imagePullPolicy: IfNotPresent\n      imagePullSecrets:\n      - name: harbor\n      restartPolicy: Always\n      terminationGracePeriodSeconds: 30\n      securityContext: \n        runAsUser: 0\n      schedulerName: default-scheduler\n  strategy:\n    type: RollingUpdate\n    rollingUpdate: \n      maxUnavailable: 1\n      maxSurge: 1\n  revisionHistoryLimit: 7\n  progressDeadlineSeconds: 600\n\napollo-configservice]# vi svc.yaml\nkind: Service\napiVersion: v1\nmetadata: \n  name: apollo-configservice\n  namespace: infra\nspec:\n  ports:\n  - protocol: TCP\n    port: 8080\n    targetPort: 8080\n  selector: \n    app: apollo-configservice\n\napollo-configservice]# vi ingress.yaml\nkind: Ingress\napiVersion: extensions/v1beta1\nmetadata: \n  name: apollo-configservice\n  namespace: infra\nspec:\n  rules:\n  - host: config.od.com\n    http:\n      paths:\n      - path: /\n        backend: \n          serviceName: apollo-configservice\n          servicePort: 8080\n~~~\n\n![1583205384294](assets/1583205384294.png)\n\n~~~\n# 应用资源配置清单，22机器：\n~]# kubectl apply -f http://k8s-yaml.od.com/apollo-configservice/cm.yaml\n~]# kubectl apply -f http://k8s-yaml.od.com/apollo-configservice/dp.yaml\n~]# kubectl apply -f http://k8s-yaml.od.com/apollo-configservice/svc.yaml\n~]# kubectl apply -f http://k8s-yaml.od.com/apollo-configservice/ingress.yaml\n~~~\n\n![1583206365045](assets/1583206365045.png)\n\n资源给的少，可能稍微慢了一些，点进去->点右上角的LOGS，日志有点多，记得点击右下角的翻页\n\n[浏览器访问config.od.com](config.od.com)\n\n鼠标对着起来的Apollo，可以看到右下角有网址\n\n![1583206733615](assets/1583206733615.png)\n\n~~~\n# 22机器，curl：\n~]# curl http://172.7.22.5:8080/info\n~~~\n\n![1583206773094](assets/1583206773094.png)\n\n成功\n\n\n\n### Apollo-ConfigService连接数据库IP分析\n\n~~~\n# 11机器：\n~]# mysql -uroot -p\nnone)]> show processlist;\n~~~\n\n![1583207560779](assets/1583207560779.png)\n\n![1583207586526](assets/1583207586526.png)\n\n> 原本是172.7.22.5，连接到数据的是10.4.7.22，因为做了NAT转换，它带了面具\n>\n\n\n\n### 交付Apollo-adminservice\n\n[官网下载https://github.com/ctripcorp/apollo/releases/tag/v1.5.1](https://github.com/ctripcorp/apollo/releases/tag/v1.5.1)\n\n![1584698854148](assets/1584698854148.png)\n\n~~~\n# 200机器：\ncd /opt/src\nsrc]# wget https://github.com/ctripcorp/apollo/releases/download/v1.5.1/apollo-adminservice-1.5.1-github.zip\nsrc]# mkdir /data/dockerfile/apollo-adminservice\nsrc]# unzip -o apollo-adminservice-1.5.1-github.zip -d /data/dockerfile/apollo-adminservice\nsrc]# cd /data/dockerfile/apollo-adminservice\napollo-adminservice]# rm -fr apollo-adminservice-1.5.1-sources.jar\napollo-adminservice]# rm -f apollo-adminservice.conf\napollo-adminservice]# cd scripts/\nscripts]# rm -f shutdown.sh\n# 删掉原来的全部内容，添加以下新的内容\nscripts]# vi startup.sh\n#!/bin/bash\nSERVICE_NAME=apollo-adminservice\n## Adjust log dir if necessary\nLOG_DIR=/opt/logs/apollo-admin-server\n## Adjust server port if necessary\nSERVER_PORT=8080\nAPOLLO_ADMIN_SERVICE_NAME=$(hostname -i)\n# SERVER_URL=\"http://localhost:${SERVER_PORT}\"\nSERVER_URL=\"http://${APOLLO_ADMIN_SERVICE_NAME}:${SERVER_PORT}\"\n\n## Adjust memory settings if necessary\n#export JAVA_OPTS=\"-Xms2560m -Xmx2560m -Xss256k -XX:MetaspaceSize=128m -XX:MaxMetaspaceSize=384m -XX:NewSize=1536m -XX:MaxNewSize=1536m -XX:SurvivorRatio=8\"\n\n## Only uncomment the following when you are using server jvm\n#export JAVA_OPTS=\"$JAVA_OPTS -server -XX:-ReduceInitialCardMarks\"\n\n########### The following is the same for configservice, adminservice, portal ###########\nexport JAVA_OPTS=\"$JAVA_OPTS -XX:ParallelGCThreads=4 -XX:MaxTenuringThreshold=9 -XX:+DisableExplicitGC -XX:+ScavengeBeforeFullGC -XX:SoftRefLRUPolicyMSPerMB=0 -XX:+ExplicitGCInvokesConcurrent -XX:+HeapDumpOnOutOfMemoryError -XX:-OmitStackTraceInFastThrow -Duser.timezone=Asia/Shanghai -Dclient.encoding.override=UTF-8 -Dfile.encoding=UTF-8 -Djava.security.egd=file:/dev/./urandom\"\nexport JAVA_OPTS=\"$JAVA_OPTS -Dserver.port=$SERVER_PORT -Dlogging.file=$LOG_DIR/$SERVICE_NAME.log -XX:HeapDumpPath=$LOG_DIR/HeapDumpOnOutOfMemoryError/\"\n\n# Find Java\nif [[ -n \"$JAVA_HOME\" ]] && [[ -x \"$JAVA_HOME/bin/java\" ]]; then\n    javaexe=\"$JAVA_HOME/bin/java\"\nelif type -p java > /dev/null 2>&1; then\n    javaexe=$(type -p java)\nelif [[ -x \"/usr/bin/java\" ]];  then\n    javaexe=\"/usr/bin/java\"\nelse\n    echo \"Unable to find Java\"\n    exit 1\nfi\n\nif [[ \"$javaexe\" ]]; then\n    version=$(\"$javaexe\" -version 2>&1 | awk -F '\"' '/version/ {print $2}')\n    version=$(echo \"$version\" | awk -F. '{printf(\"%03d%03d\",$1,$2);}')\n    # now version is of format 009003 (9.3.x)\n    if [ $version -ge 011000 ]; then\n        JAVA_OPTS=\"$JAVA_OPTS -Xlog:gc*:$LOG_DIR/gc.log:time,level,tags -Xlog:safepoint -Xlog:gc+heap=trace\"\n    elif [ $version -ge 010000 ]; then\n        JAVA_OPTS=\"$JAVA_OPTS -Xlog:gc*:$LOG_DIR/gc.log:time,level,tags -Xlog:safepoint -Xlog:gc+heap=trace\"\n    elif [ $version -ge 009000 ]; then\n        JAVA_OPTS=\"$JAVA_OPTS -Xlog:gc*:$LOG_DIR/gc.log:time,level,tags -Xlog:safepoint -Xlog:gc+heap=trace\"\n    else\n        JAVA_OPTS=\"$JAVA_OPTS -XX:+UseParNewGC\"\n        JAVA_OPTS=\"$JAVA_OPTS -Xloggc:$LOG_DIR/gc.log -XX:+PrintGCDetails\"\n        JAVA_OPTS=\"$JAVA_OPTS -XX:+UseConcMarkSweepGC -XX:+UseCMSCompactAtFullCollection -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFraction=60 -XX:+CMSClassUnloadingEnabled -XX:+CMSParallelRemarkEnabled -XX:CMSFullGCsBeforeCompaction=9 -XX:+CMSClassUnloadingEnabled  -XX:+PrintGCDateStamps -XX:+PrintGCApplicationConcurrentTime -XX:+PrintHeapAtGC -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=5 -XX:GCLogFileSize=5M\"\n    fi\nfi\n\nprintf \"$(date) ==== Starting ==== \\n\"\n\ncd `dirname $0`/..\nchmod 755 $SERVICE_NAME\".jar\"\n./$SERVICE_NAME\".jar\" start\n\nrc=$?;\n\nif [[ $rc != 0 ]];\nthen\n    echo \"$(date) Failed to start $SERVICE_NAME.jar, return code: $rc\"\n    exit $rc;\nfi\n\ntail -f /dev/null\n\n\n\n\n# 200机器，修改账户密码：\nconfig]# cd /data/dockerfile/apollo-adminservice/config\nconfig]# vi application-github.properties\nspring.datasource.url = jdbc:mysql://mysql.od.com:3306/ApolloConfigDB?characterEncoding=utf8\nspring.datasource.username = apolloconfig\nspring.datasource.password = 123456\n~~~\n\n> [startup.sh官方地址](https://github.com/ctripcorp/apollo/blob/1.5.1/scripts/apollo-on-kubernetes/apollo-admin-server/scripts/startup-kubernetes.sh)\n>\n> 修改处为：\n>\n> SERVER_PORT=8080  # 因为docker网络空间是互相隔离，用8080即可\n>\n> APOLLO_ADMIN_SERVICE_NAME=$(hostname -i)\n\n此处由 “**南宫乘风**”  补充配置修改 ：https://github.com/nangongchengfeng\n\n\n\n~~~\n# 200机器，制作dockerfile：\ncd /data/dockerfile/apollo-adminservice\napollo-adminservice]# ll\napollo-adminservice]# vi Dockerfile\nFROM 909336740/jre8:8u112\n\nENV VERSION 1.5.1\n\nRUN ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime &&\\\n    echo \"Asia/Shanghai\" > /etc/timezone\n\nADD apollo-adminservice-${VERSION}.jar /apollo-adminservice/apollo-adminservice.jar\nADD config/ /apollo-adminservice/config\nADD scripts/ /apollo-adminservice/scripts\n\nCMD [\"/apollo-adminservice/scripts/startup.sh\"]\n\napollo-adminservice]# docker build . -t harbor.od.com/infra/apollo-adminservice:v1.5.1\napollo-adminservice]# docker push harbor.od.com/infra/apollo-adminservice:v1.5.1\n~~~\n\n![1583207853169](assets/1583207853169.png)\n\n![1583219005442](assets/1583219005442.png)\n\n看以下harbor仓库有没有\n\n![1583208050305](assets/1583208050305.png)\n\n##### 重新复习一下交付过程：搞定镜像-搞定资源配置清单-应用资源配置清单\n\n~~~\n# 200机器，制作资源配置清单：\nmkdir /data/k8s-yaml/apollo-adminservice\ncd /data/k8s-yaml/apollo-adminservice\napollo-adminservice]# vi cm.yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: apollo-adminservice-cm\n  namespace: infra\ndata:\n  application-github.properties: |\n    # DataSource\n    spring.datasource.url = jdbc:mysql://mysql.od.com:3306/ApolloConfigDB?characterEncoding=utf8\n    spring.datasource.username = apolloconfig\n    spring.datasource.password = 123456\n    eureka.service.url = http://config.od.com/eureka\n  app.properties: |\n    appId=100003172\n\napollo-adminservice]# vi dp.yaml\nkind: Deployment\napiVersion: extensions/v1beta1\nmetadata:\n  name: apollo-adminservice\n  namespace: infra\n  labels: \n    name: apollo-adminservice\nspec:\n  replicas: 1\n  selector:\n    matchLabels: \n      name: apollo-adminservice\n  template:\n    metadata:\n      labels: \n        app: apollo-adminservice \n        name: apollo-adminservice\n    spec:\n      volumes:\n      - name: configmap-volume\n        configMap:\n          name: apollo-adminservice-cm\n      containers:\n      - name: apollo-adminservice\n        image: harbor.od.com/infra/apollo-adminservice:v1.5.1\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n        volumeMounts:\n        - name: configmap-volume\n          mountPath: /apollo-adminservice/config\n        terminationMessagePath: /dev/termination-log\n        terminationMessagePolicy: File\n        imagePullPolicy: IfNotPresent\n      imagePullSecrets:\n      - name: harbor\n      restartPolicy: Always\n      terminationGracePeriodSeconds: 30\n      securityContext: \n        runAsUser: 0\n      schedulerName: default-scheduler\n  strategy:\n    type: RollingUpdate\n    rollingUpdate: \n      maxUnavailable: 1\n      maxSurge: 1\n  revisionHistoryLimit: 7\n  progressDeadlineSeconds: 600\n~~~\n\n![1583208130534](assets/1583208130534.png)\n\n~~~\n# 22机器，应用资源配置清单：\n~]# kubectl apply -f http://k8s-yaml.od.com/apollo-adminservice/cm.yaml\n~]# kubectl apply -f http://k8s-yaml.od.com/apollo-adminservice/dp.yaml\n~~~\n\n![1583217458560](assets/1583217458560.png)\n\n~~~\n# 11机器，查看数据库连接IP：\n~]# mysql -uroot -p\nnone)]> show processlist;\n~~~\n\n多了更多的22连接\n\n![1583217487662](assets/1583217487662.png)\n\n[config.od.com访问地址](config.od.com)\n\n鼠标对着起来的Apollo，可以看到右下角有网址\n\n![1583217527372](assets/1583217527372.png)\n\n~~~\n# 22机器，curl：\n~]# curl http://172.7.22.7:8080/info\n~~~\n\n![1583217575749](assets/1583217575749.png)\n\n[^课外题1]: 交付完config和admin后，我们发现，交付到K8S里面的服务如此之容易，那么问题来了，交付到K8S和交付到物理机或者虚拟机里，那里有很大的区别，为什么K8S更好。\n[^课外题2]: 尝试用点点点的方式在dashboard里面扩容adminservice和configservice\n\n\n\n### 交付Apollo-Portal前，数据库初始化\n\n[官网地址](https://github.com/ctripcorp/apollo/releases/tag/v1.5.1)\n\n~~~\n# 200机器，制作镜像-下载包并整理：\ncd /opt/src/\n~]# wget https://github.com/ctripcorp/apollo/releases/download/v1.5.1/apollo-portal-1.5.1-github.zip\nsrc]# mkdir /data/dockerfile/apollo-portal\nsrc]# unzip -o apollo-portal-1.5.1-github.zip -d /data/dockerfile/apollo-portal\nsrc]# cd /data/dockerfile/apollo-portal\napollo-portal]# rm -f apollo-portal-1.5.1-sources.jar\napollo-portal]# rm -f apollo-portal.conf\napollo-portal]# rm -f scripts/shutdown.sh\n~~~\n\n[sql官网网址-需要raw](https://github.com/ctripcorp/apollo/blob/master/scripts/apollo-on-kubernetes/db/portal-db/apolloportaldb.sql)\n\n**注意：**这个数据库地址已经404\n\n可以尝试使用这个数据库地址：https://github.com/apolloconfig/apollo/blob/master/scripts/sql/apolloportaldb.sql\n\n相关资料：https://www.apolloconfig.com/#/zh/deployment/distributed-deployment-guide?id=_21-%e5%88%9b%e5%bb%ba%e6%95%b0%e6%8d%ae%e5%ba%93\n\n~~~\n# 11机器，数据库初始化：\n~]# wget https://raw.githubusercontent.com/ctripcorp/apollo/master/scripts/apollo-on-kubernetes/db/portal-db/apolloportaldb.sql -O apolloportal.sql\n~]# ll\n~~~\n\n![1583217807056](assets/1583217807056.png)\n\n~~~\n# 11机器：\n~]# mysql -uroot -p\nnone)]> source ./apolloportal.sql\nnone)]> show databases;\nnone)]> use ApolloPortalDB\nApolloportalDB]> show tables;\nApolloportalDB]> grant INSERT,DELETE,UPDATE,SELECT on ApolloPortalDB.* to \"apolloportal\"@\"10.4.7.%\" identified by \"123456\";\nApolloportalDB]> select user,host from mysql.user;\nApolloportalDB]> select * from ServerConfig\\G\nApolloportalDB]> update ServerConfig set Value='[{\"orgId\":\"ben01\",\"orgName\":\"Linux学院\"},{\"orgId\":\"ben02\",\"orgName\":\"云计算学院\"},{\"orgId\":\"ben03\",\"orgName\":\"Python学院\"}]' where Id=2;\nApolloportalDB]> select * from ServerConfig\\G\n~~~\n\n![1583217921425](assets/1583217921425.png)\n\n原![1583217963849](assets/1583217963849.png)\n\n改后\n\n![1583218104712](assets/1583218104712.png)\n\n完成\n\n\n\n### 制作Portal的docker镜像，并交付\n\n~~~\n# 200机器，更新startup:\ncd /data/dockerfile/apollo-portal/scripts/\n# 全部删掉换成下面的\nscripts]# vi startup.sh\n#!/bin/bash\nSERVICE_NAME=apollo-portal\n## Adjust log dir if necessary\nLOG_DIR=/opt/logs/apollo-portal-server\n## Adjust server port if necessary\nSERVER_PORT=8080\nAPOLLO_PORTAL_SERVICE_NAME=$(hostname -i)\n# SERVER_URL=\"http://localhost:$SERVER_PORT\"\nSERVER_URL=\"http://${APOLLO_PORTAL_SERVICE_NAME}:${SERVER_PORT}\"\n\n## Adjust memory settings if necessary\n#export JAVA_OPTS=\"-Xms2560m -Xmx2560m -Xss256k -XX:MetaspaceSize=128m -XX:MaxMetaspaceSize=384m -XX:NewSize=1536m -XX:MaxNewSize=1536m -XX:SurvivorRatio=8\"\n\n## Only uncomment the following when you are using server jvm\n#export JAVA_OPTS=\"$JAVA_OPTS -server -XX:-ReduceInitialCardMarks\"\n\n########### The following is the same for configservice, adminservice, portal ###########\nexport JAVA_OPTS=\"$JAVA_OPTS -XX:ParallelGCThreads=4 -XX:MaxTenuringThreshold=9 -XX:+DisableExplicitGC -XX:+ScavengeBeforeFullGC -XX:SoftRefLRUPolicyMSPerMB=0 -XX:+ExplicitGCInvokesConcurrent -XX:+HeapDumpOnOutOfMemoryError -XX:-OmitStackTraceInFastThrow -Duser.timezone=Asia/Shanghai -Dclient.encoding.override=UTF-8 -Dfile.encoding=UTF-8 -Djava.security.egd=file:/dev/./urandom\"\nexport JAVA_OPTS=\"$JAVA_OPTS -Dserver.port=$SERVER_PORT -Dlogging.file=$LOG_DIR/$SERVICE_NAME.log -XX:HeapDumpPath=$LOG_DIR/HeapDumpOnOutOfMemoryError/\"\n\n# Find Java\nif [[ -n \"$JAVA_HOME\" ]] && [[ -x \"$JAVA_HOME/bin/java\" ]]; then\n    javaexe=\"$JAVA_HOME/bin/java\"\nelif type -p java > /dev/null 2>&1; then\n    javaexe=$(type -p java)\nelif [[ -x \"/usr/bin/java\" ]];  then\n    javaexe=\"/usr/bin/java\"\nelse\n    echo \"Unable to find Java\"\n    exit 1\nfi\n\nif [[ \"$javaexe\" ]]; then\n    version=$(\"$javaexe\" -version 2>&1 | awk -F '\"' '/version/ {print $2}')\n    version=$(echo \"$version\" | awk -F. '{printf(\"%03d%03d\",$1,$2);}')\n    # now version is of format 009003 (9.3.x)\n    if [ $version -ge 011000 ]; then\n        JAVA_OPTS=\"$JAVA_OPTS -Xlog:gc*:$LOG_DIR/gc.log:time,level,tags -Xlog:safepoint -Xlog:gc+heap=trace\"\n    elif [ $version -ge 010000 ]; then\n        JAVA_OPTS=\"$JAVA_OPTS -Xlog:gc*:$LOG_DIR/gc.log:time,level,tags -Xlog:safepoint -Xlog:gc+heap=trace\"\n    elif [ $version -ge 009000 ]; then\n        JAVA_OPTS=\"$JAVA_OPTS -Xlog:gc*:$LOG_DIR/gc.log:time,level,tags -Xlog:safepoint -Xlog:gc+heap=trace\"\n    else\n        JAVA_OPTS=\"$JAVA_OPTS -XX:+UseParNewGC\"\n        JAVA_OPTS=\"$JAVA_OPTS -Xloggc:$LOG_DIR/gc.log -XX:+PrintGCDetails\"\n        JAVA_OPTS=\"$JAVA_OPTS -XX:+UseConcMarkSweepGC -XX:+UseCMSCompactAtFullCollection -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFraction=60 -XX:+CMSClassUnloadingEnabled -XX:+CMSParallelRemarkEnabled -XX:CMSFullGCsBeforeCompaction=9 -XX:+CMSClassUnloadingEnabled  -XX:+PrintGCDateStamps -XX:+PrintGCApplicationConcurrentTime -XX:+PrintHeapAtGC -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=5 -XX:GCLogFileSize=5M\"\n    fi\nfi\n\nprintf \"$(date) ==== Starting ==== \\n\"\n\ncd `dirname $0`/..\nchmod 755 $SERVICE_NAME\".jar\"\n./$SERVICE_NAME\".jar\" start\n\nrc=$?;\n\nif [[ $rc != 0 ]];\nthen\n    echo \"$(date) Failed to start $SERVICE_NAME.jar, return code: $rc\"\n    exit $rc;\nfi\n\ntail -f /dev/null\n\n\n# 200机器，修改账户密码：\nconfig]# cd /data/dockerfile/apollo-portal/config\nconfig]# vi application-github.properties\nspring.datasource.url = jdbc:mysql://mysql.od.com:3306/ApolloPortalDB?characterEncoding=utf8\nspring.datasource.username = apolloportal\nspring.datasource.password = 123456\n\n\n~~~\n\n> [portal_startup官网地址](https://github.com/ctripcorp/apollo/blob/1.5.1/scripts/apollo-on-kubernetes/apollo-portal-server/scripts/startup-kubernetes.sh)\n>\n> 仅有以下两处不同：\n>\n> SERVER_PORT=8080\n> APOLLO_PORTAL_SERVICE_NAME=$(hostname -i)\n\n此处由 “**南宫乘风**”  补充配置修改 ：https://github.com/nangongchengfeng\n\n\n\n~~~\n# 200机器，制作dockerfile：\ncd /data/dockerfile/apollo-portal/\napollo-portal]# vi Dockerfile\nFROM 909336740/jre8:8u112\n\nENV VERSION 1.5.1\n\nRUN ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime &&\\\n    echo \"Asia/Shanghai\" > /etc/timezone\n\nADD apollo-portal-${VERSION}.jar /apollo-portal/apollo-portal.jar\nADD config/ /apollo-portal/config\nADD scripts/ /apollo-portal/scripts\n\nCMD [\"/apollo-portal/scripts/startup.sh\"]\n\napollo-portal]# docker build . -t harbor.od.com/infra/apollo-portal:v1.5.1\napollo-portal]# docker push harbor.od.com/infra/apollo-portal:v1.5.1\n~~~\n\n![1583218938233](assets/1583218938233.png)\n\n![1583219229803](assets/1583219229803.png)\n\n~~~\n# 200机器：\nmkdir /data/k8s-yaml/apollo-portal\ncd /data/k8s-yaml/apollo-portal\napollo-portal]# vi cm.yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: apollo-portal-cm\n  namespace: infra\ndata:\n  application-github.properties: |\n    # DataSource\n    spring.datasource.url = jdbc:mysql://mysql.od.com:3306/ApolloPortalDB?characterEncoding=utf8\n    spring.datasource.username = apolloportal\n    spring.datasource.password = 123456\n  app.properties: |\n    appId=100003173\n  apollo-env.properties: |\n    dev.meta=http://config.od.com\n\napollo-portal]# vi dp.yaml\nkind: Deployment\napiVersion: extensions/v1beta1\nmetadata:\n  name: apollo-portal\n  namespace: infra\n  labels: \n    name: apollo-portal\nspec:\n  replicas: 1\n  selector:\n    matchLabels: \n      name: apollo-portal\n  template:\n    metadata:\n      labels: \n        app: apollo-portal \n        name: apollo-portal\n    spec:\n      volumes:\n      - name: configmap-volume\n        configMap:\n          name: apollo-portal-cm\n      containers:\n      - name: apollo-portal\n        image: harbor.od.com/infra/apollo-portal:v1.5.1\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n        volumeMounts:\n        - name: configmap-volume\n          mountPath: /apollo-portal/config\n        terminationMessagePath: /dev/termination-log\n        terminationMessagePolicy: File\n        imagePullPolicy: IfNotPresent\n      imagePullSecrets:\n      - name: harbor\n      restartPolicy: Always\n      terminationGracePeriodSeconds: 30\n      securityContext: \n        runAsUser: 0\n      schedulerName: default-scheduler\n  strategy:\n    type: RollingUpdate\n    rollingUpdate: \n      maxUnavailable: 1\n      maxSurge: 1\n  revisionHistoryLimit: 7\n  progressDeadlineSeconds: 600\n\napollo-portal]# vi svc.yaml\nkind: Service\napiVersion: v1\nmetadata: \n  name: apollo-portal\n  namespace: infra\nspec:\n  ports:\n  - protocol: TCP\n    port: 8080\n    targetPort: 8080\n  selector: \n    app: apollo-portal\n\napollo-portal]# vi ingress.yaml\nkind: Ingress\napiVersion: extensions/v1beta1\nmetadata: \n  name: apollo-portal\n  namespace: infra\nspec:\n  rules:\n  - host: portal.od.com\n    http:\n      paths:\n      - path: /\n        backend: \n          serviceName: apollo-portal\n          servicePort: 8080\n~~~\n\n![1583219397807](assets/1583219397807.png)\n\n~~~\n# 11机器，解析域名：\n~]# vi /var/named/od.com.zone\nserial 前滚一位\nportal             A    10.4.7.10\n\n~]# systemctl restart named\n~]# dig -t A portal.od.com @10.4.7.11 +short\n# out: 10.4.7.10\n~~~\n\n![1583219385915](assets/1583219385915.png)\n\n~~~\n# 22机器，应用资源配置清单：\n~]# kubectl apply -f http://k8s-yaml.od.com/apollo-portal/cm.yaml\n~]# kubectl apply -f http://k8s-yaml.od.com/apollo-portal/dp.yaml\n~]# kubectl apply -f http://k8s-yaml.od.com/apollo-portal/svc.yaml\n~]# kubectl apply -f http://k8s-yaml.od.com/apollo-portal/ingress.yaml\n~~~\n\n可以查看起来的portal的logs日志，可能稍微有些慢\n\n[浏览器输入portal.od.com](portal.od.com)\n\n```\nUsername: apollo\nPassword: admin\n```\n\n![1583221254589](assets/1583221254589.png)\n\n> 第一件事，修改密码，任何开源软件的第一件事，就是修改默认密码\n>\n\n![1584699514174](assets/1584699514174.png)\n\n![1583221587411](assets/1583221587411.png)\n\n![1583222001345](assets/1583222001345.png)\n\n去看一下对接数据库情况\n\n![1583222020754](assets/1583222020754.png)\n\n~~~\n\n~~~\n\n试试查询，查询key，也试试使用保存更新\n\n~~~\nkey:organizations\nvalue:[{\"orgId\":\"ben01\",\"orgName\":\"Linux学院\"},{\"orgId\":\"ben02\",\"orgName\":\"云计算学院\"},{\"orgId\":\"ben03\",\"orgName\":\"Python学院\"},{\"orgId\":\"ben03\",\"orgName\":\"大数据学院\"}]\n~~~\n\n![1583222152917](assets/1583222152917.png)\n\n~~~\n# 11机器，查询是否更新：\n~]# mysql -uroot -p\nnone)]> use ApolloPortalDB;\nApolloPortalDB]> select * from ServerConfig\\G\n~~~\n\n![1583222202986](assets/1583222202986.png)\n\n成功\n\n使用Apollo，创建项目，提交\n\n![1583222246074](assets/1583222246074.png)\n\n~~~\n# 填入对应参数，并提交：\nAppId:dubbo-demo-service\n~~~\n\n![1583222350430](assets/1583222350430.png)\n\n![1583222393771](assets/1583222393771.png)\n\n完成\n\n\n\n### dubbo服务提供者连接Apollo实战\n\n首先我们在git新建一个Apollo分支，以下是操作方法\n\n建议直接使用公网的 https://gitee.com/benjas/dubbo-demo-service.git 或者拷贝一份\n\n**git创建分支并上传代码（或者直接fork我），注意，下图配的是gitlab里面web的，现在你要操作的是自己git的service，因为service没截图，所以我用web的替代**\n\n1. 切换身份\n\n   ~~~\n   # 上传项目的文件夹打开，然后cd到附近的新建的apollo文件夹，克隆项目并切换身份：\n   $ git http://gitlab.od.com:10000/909336740/dubbo-demo-web.git\n   $ cd dubbo-demo-web/\n   $ git branch apollo\n   $ git checkout apollo\n   ~~~\n\n   ![1583287244483](assets/1583287244483.png)\n\n   将Apollo分支的文件全部拉来这个文件夹，选择替换重名文件\n\n   ![1583287180864](assets/1583287180864.png)\n\n2. 把apollo的代码全部拉过去覆盖，并上传代码\n\n   ~~~\n   $ git add .\n   $ git commit -m \"apollo commit#1\"\n   $ git push -u origin apollo\n   ~~~\n\n   ![1583287633875](assets/1583287633875.png)\n\n   ![1582680560428](assets/1582680560428.png)\n\n   完成，当然你可以直接fork我的\n\n   ![1582680588102](assets/1582680588102.png)\n\n\n\n#### 继续主线\n\n建议直接使用公网的 https://gitee.com/benjas/dubbo-demo-service.git 或者拷贝一份\n\n在Apollo portal里创建相应两个配置项（注意看清位置，是 ：resources 下的）\n\n![1582680643369](assets/1582680643369.png)\n\n新增配置项，并提交\n\n![1583223092507](assets/1583223092507.png)\n\n![1583223148433](assets/1583223148433.png)\n\n发布\n\n![1583223168662](assets/1583223168662.png)\n\n![1583223190037](assets/1583223190037.png)\n\n我们已经配置好了，然后需要用Jenkins制作镜像\n\n建议直接使用公网的 https://gitee.com/benjas/dubbo-demo-service.git 或者拷贝一份\n\n~~~\n# 填入对应参数\napp_name: dubbo-demo-service\nimage_name: app/dubbo-demo-service\ngit_repo: https://gitee.com/benjas/dubbo-demo-service.git\ngit_ver: apollo\nadd_tag: 200303_1615\ntarget_dir: ./dubbo-server/target\nbase_image: base/jre8:8u112\n~~~\n\n> 注意，我这里用的是gitlab，因为网络问题，你用自己的公网git即可\n\n![1583223413973](assets/1583223413973.png)\n\n去#38的console里查看情况\n\n![1583223612917](assets/1583223612917.png)\n\n#### 报错问题：\n\n![1581324514061](assets/1581324514061.png)\n\n该报错的原因是mirrors没有改对\n\n~~~\n# 200机器：\ncd /data/nfs-volume/jenkins_home/maven-3.6.1-8u232/conf\nconf]# vi settings.xml\n~~~\n\n修改完上面的内容后保存再去重新构建\n\n\n\n#### 继续主线\n\n![1583223754285](assets/1583223754285.png)\n\n镜像制作完成\n\n~~~\n# 200机器，修改指定的镜像，并添加部分内容：\ncd /data/k8s-yaml/dubbo-demo-service/\ndubbo-demo-service]# vi dp.yaml\n        image: harbor.od.com/app/dubbo-demo-service:apollo_200303_1615\n        ports:\n        - containerPort: 20880\n          protocol: TCP\n        env:\n        - name: JAR_BALL\n          value: dubbo-server.jar\n        - name: C_OPTS\n          value: -Denv=dev -Dapollo.meta=http://config.od.com\n        imagePullPolicy: IfNotPresent\n~~~\n\n![1583223916007](assets/1583223916007.png)\n\n~~~\n# 22机器，应用：\n~]# kubectl apply -f http://k8s-yaml.od.com/dubbo-demo-service/dp.yaml\n# out: deployment.extensions/dubbo-demo-service configured\n~~~\n\n![1583224029500](assets/1583224029500.png)\n\n去LOGS查看，可以看到连接了Apollo\n\n![1583227298634](assets/1583227298634.png)\n\n> PS：这里的pod名字你可能觉得不一样，因为我镜像弄错了，重新改了一遍\n\n![1583227711909](assets/1583227711909.png)\n\n当你把service扩容成两个后\n\n![1583227780562](assets/1583227780562.png)\n\n可以看到变成两个实例\n\n![1583227821344](assets/1583227821344.png)\n\n在monitor里可以看到端口是20880（注意必须连接的是zk1，怎么修改上面有，先改cm，然后删掉pod让它自动重启）\n\n![1583229093925](assets/1583229093925.png)\n\n![1583229141970](assets/1583229141970.png)\n\n我们去Apollo改一下端口，改成20881，并发布\n\n![1583229265526](assets/1583229265526.png)\n\n![1583229275645](assets/1583229275645.png)\n\n重启两个service的pods（就是删掉让它们自动重启）\n\n![1583229312442](assets/1583229312442.png)\n\n刷新dubbo-monitor，可以看到端口已经变成20881\n\n![1583229337601](assets/1583229337601.png)\n\n还可以改成zk2\n\n![1583229498782](assets/1583229498782.png)\n\n> 作业：改成zk2后，需要删掉对应的pod，然后让其也在zk2，然后还需要把monitor也改成zk2，方法在上面有。这个作业一定要做，不然下面会报错，具体是哪里报错我会提示\n\n最后我改回来了20880端口，但是依然用的zk2\n\n完成\n\n\n\n### dubbo服务消费者连接Apollo实战\n\n同样，我是有Apollo分支的，操作方法和上面的service服务一样\n\n![1582682538181](assets/1582682538181.png)\n\n创建项目\n\n![1583287914567](assets/1583287914567.png)\n\n新增配置，并发布\n\n![1583289334584](assets/1583289334584.png)\n\n![1583289365597](assets/1583289365597.png)\n\n用Jenkins构建dubbo消费者\n\n~~~\n# 填入对应参数\napp_name: dubbo-demo-consumer\nimage_name: app/dubbo-demo-consumer\ngit_repo: http://gitlab.od.com:10000/909336740/dubbo-demo-web.git\ngit_ver: apollo\nadd_tag: 200304_1040\ntarget_dir: ./dubbo-client/target\nbase_image: base/jre8:8u112\n~~~\n\n> 注意，我这里用的是gitlab，因为网络问题，你用自己的公网git即可\n\n![1583290002445](assets/1583290002445.png)\n\n![1583290022279](assets/1583290022279.png)\n\n![1583290134412](assets/1583290134412.png)\n\n~~~\n# 200机器，修改配置：\ncd /data/k8s-yaml/dubbo-demo-consumer/\ndubbo-demo-consumer]# vi dp.yaml\n        image: harbor.od.com/app/dubbo-demo-consumer:apollo_200304_1040\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n        - containerPort: 20880\n          protocol: TCP\n        env:\n        - name: JAR_BALL\n          value: dubbo-client.jar\n        - name: C_OPTS\n          value: -Denv=dev -Dapollo.meta=http://config.od.com\n        imagePullPolicy: IfNotPresent\n~~~\n\n![1583290251350](assets/1583290251350.png)\n\n~~~\n# 应用资源配置清单，22机器：\n~]# kubectl apply -f http://k8s-yaml.od.com/dubbo-demo-consumer/dp.yaml\n# out: deployment.extensions/dubbo-demo-consumer configured\n~~~\n\n![1583290331910](assets/1583290331910.png)\n\n![1583291244653](assets/1583291244653.png)\n\n再去Applications可以看到已经起来了\n\n~~~\n# 浏览器访问demo.od.com/hello?name=apollo\n~~~\n\n![1583291263893](assets/1583291263893.png)\n\n> 如果你登录的是这个界面，那么你没有做作业：改service的zk1成zk2，这个consumer已经是zk2的，而service还是zk1，当然不能响应\n>\n> ![1583290792208](assets/1583290792208.png)\n\n\n\n#### 实现代码迭代\n\n修改代码，然后commit\n\n![1582682749719](assets/1582682749719.png)\n\n![1582682870930](assets/1582682870930.png)\n\nJenkins构建\n\n~~~\n# 填入对应参数\napp_name: dubbo-demo-consumer\nimage_name: app/dubbo-demo-consumer\ngit_repo: http://gitlab.od.com:10000/909336740/dubbo-demo-web.git\ngit_ver: apollo\nadd_tag: 200304_1145\ntarget_dir: ./dubbo-client/target\nbase_image: base/jre8:8u112\n~~~\n\n> 注意，我这里用的是gitlab，因为网络问题，你用自己的公网git即可\n\n![1583293563142](assets/1583293563142.png)\n\n![1583293832408](assets/1583293832408.png)\n\n~~~\n# 200机器修改使用的镜像：\ncd /data/k8s-yaml/dubbo-demo-consumer/\ndubbo-demo-consumer]# vi dp.yaml\nimage: harbor.od.com/app/dubbo-demo-consumer:apollo_200304_1145\n~~~\n\n\n\n~~~\n# 22机器应用：\n~]# kubectl apply -f http://k8s-yaml.od.com/dubbo-demo-consumer/dp.yaml\n# out: deployment.extensions/dubbo-demo-consumer configured\n~~~\n\n![1583294170202](assets/1583294170202.png)\n\n~~~\n# 浏览器访问demo.od.com/hello?name=apollo\n~~~\n\n![1583294184192](assets/1583294184192.png)\n\n\n\n### 实战Apollo分环境管理dubbo服务-交付Apollo-configservice\n\n> 我们要实现测试环境和生产环境只需要打包一个镜像\n>\n\n先开始分环境\n\n~~~\n# 11机器，解析域名：\n~]# vi /var/named/od.com.zone\nserial 前滚一位\nzk-test            A    10.4.7.11\nzk-prod            A    10.4.7.12\n\n~]# systemctl restart named\n~]# dig -t A zk-test.od.com +short\n# out: 10.4.7.11\n~]# dig -t A zk-prod.od.com +short\n# out: 10.4.7.12\n~~~\n\n关闭deployment里的消费者和服务者（改成scale 0，先消费者后服务者）\n\n![1583301119606](assets/1583301119606.png)\n\n~~~\n# 创建两个名称空间， 21机器：\n~]# kubectl create ns test\n# out:namespace/test created\n~]# kubectl create secret docker-registry harbor --docker-server=harbor.od.com --docker-username=admin --docker-password=Harbor12345 -n test\n# out:secret/harbor created\n~]# kubectl create ns prod\n# out:namespace/prod created\n~]# kubectl create secret docker-registry harbor --docker-server=harbor.od.com --docker-username=admin --docker-password=Harbor12345 -n prod\n# out:secret/harbor created\n~~~\n\n![1583301237979](assets/1583301237979.png)\n\n去看一下dashboard里面的Namespaces\n\n![1583301259487](assets/1583301259487.png)\n\n把admin、portal、config都scale成0\n\n![1583301320962](assets/1583301320962.png)\n\n~~~\n# 11机器，建库：\n~]# vi apolloconfig.sql\n# 增加Test关键字\n~~~\n\n![1583301385286](assets/1583301385286.png)\n\n~~~\n# 11机器，测试环境：\n~]# mysql -uroot -p < apolloconfig.sql\n~]# mysql -uroot -p\nnone)]> show databases;\nnone)]> use ApolloConfigTestDB;\nApolloConfigTestDB]> select * from ServerConfig\\G\nApolloConfigTestDB]> update ApolloConfigTestDB.ServerConfig set ServerConfig.Value=\"http://config-test.od.com/eureka\" where ServerConfig.Key=\"eureka.service.url\";\nApolloConfigTestDB]> select * from ServerConfig\\G\nApolloConfigTestDB]> grant INSERT,DELETE,UPDATE,SELECT on ApolloConfigTestDB.* to \"apolloconfig\"@\"10.4.7.%\" identified by \"123456\";\n~~~\n\n![1583301481115](assets/1583301481115.png)\n\n~~~\n# 11机器，生产环境：\n~]# vi apolloconfig.sql\n# 修改成Prod关键字\n~]# mysql -uroot -p < apolloconfig.sql\n~]# mysql -uroot -p\nnone)]> show databases;\nnone)]> use ApolloConfigProdDB;\nApolloConfigProdDB]> select * from ServerConfig\\G\nApolloConfigProdDB]> update ApolloConfigProdDB.ServerConfig set ServerConfig.Value=\"http://config-prod.od.com/eureka\" where ServerConfig.Key=\"eureka.service.url\";\nApolloConfigProdDB]> select * from ServerConfig\\G\nApolloConfigProdDB]> grant INSERT,DELETE,UPDATE,SELECT on ApolloConfigProdDB.* to \"apolloconfig\"@\"10.4.7.%\" identified by \"123456\";\n~~~\n\n![1583301626767](assets/1583301626767.png)\n\n~~~~\n# 11机器，修改支持类型：\nnone)]> use ApolloPortalDB;\nApolloPortalDB]> show tables;\nApolloPortalDB]> select * from ServerConfig\\G\nApolloPortalDB]> update ServerConfig set Value='fat,pro' where Id=1;\nApolloPortalDB]> select * from Serverconfig\\G\n~~~~\n\n改后\n\n![1583302030474](assets/1583302030474.png)\n\n> PS按理来说你应该只有dev，但是好像Apollo更新了，一开始就有4种支持的类型，只要你确保有fat和pro即可\n\n~~~\n# 200机器：\ncd /data/k8s-yaml/apollo-portal/\n# 需改以下内容\napollo-portal]# vi cm.yaml\n  apollo-env.properties: |\n    fat.meta=http://config-test.od.com\n    pro.meta=http://config-prod.od.com\n~~~\n\n![1583302159311](assets/1583302159311.png)\n\n~~~\n# 22机器，应用：\n~]# kubectl apply -f http://k8s-yaml.od.com/apollo-portal/cm.yaml\n# out: configmap/apollo-portal configured\n~~~\n\n![1583302232037](assets/1583302232037.png)\n\n完成\n\n\n\n### 实战使用Apollo分环境管理dubbo服务——交付Apollo-portal和adminservice\n\n~~~\n# 制作资源配置清单,200机器：\ncd /data/k8s-yaml/\nk8s-yaml]# mkdir -pv test/{apollo-configservice,apollo-adminservice,dubbo-demo-service,dubbo-demo-consumer}\nk8s-yaml]# mkdir -pv prod/{apollo-configservice,apollo-adminservice,dubbo-demo-service,dubbo-demo-consumer}\nk8s-yaml]# cd test/apollo-configservice/\napollo-configservice]# cp -a /data/k8s-yaml/apollo-configservice/cm.yaml .\napollo-configservice]# cp -a /data/k8s-yaml/apollo-configservice/svc.yaml .\napollo-configservice]# cp -a /data/k8s-yaml/apollo-configservice/dp.yaml .\napollo-configservice]# cp -a /data/k8s-yaml/apollo-configservice/ingress.yaml .\n# 修改成test，三处\napollo-configservice]# vi cm.yaml\n  namespace: test\n    spring.datasource.url = jdbc:mysql://mysql.od.com:3306/ApolloConfigTestDB?characterEncoding=utf8\n    spring.service.url = http://config-test.od.com/eureka\n\n# 修改成test，一处\napollo-configservice]# vi dp.yaml\n  namespace: test\n\n# 修改成test，一处\napollo-configservice]# vi svc.yaml\n  namespace: test\n\n# 修改成test，两处\napollo-configservice]# vi ingress.yaml\n  namespace: test\n  - host: config-test.od.com\n~~~\n\n\n\n~~~\n# 11机器，解析域名：\n~]# vi /var/named/od.com.zone\nserial 前滚一位\nconfig-test        A    10.4.7.10\nconfig-prod        A    10.4.7.10\n\n~]# systemctl restart named\n~~~\n\n![1582683284142](assets/1582683284142.png)\n\n~~~\n# 应用资源配置清单，22机器：\n~]# kubectl apply -f http://k8s-yaml.od.com/test/apollo-configservice/cm.yaml\n~]# kubectl apply -f http://k8s-yaml.od.com/test/apollo-configservice/dp.yaml\n~]# kubectl apply -f http://k8s-yaml.od.com/test/apollo-configservice/svc.yaml\n~]# kubectl apply -f http://k8s-yaml.od.com/test/apollo-configservice/ingress.yaml\n~~~\n\n\n\n~~~~\n# 200机器，制作prod的：\ncd /data/k8s-yaml/prod/apollo-configservice/\napollo-configservice]# cp ../../test/apollo-configservice/*.yaml .\n# 修改成prod，三处\napollo-configservice]# vi cm.yaml\n  namespace: prod\n    spring.datasource.url = jdbc:mysql://mysql.od.com:3306/ApolloConfigProdDB?characterEncoding=utf8\n    spring.service.url = http://config-prod.od.com/eureka\n\n# 修改成prod，一处\napollo-configservice]# vi dp.yaml\n  namespace: prod\n\n# 修改成prod，一处\napollo-configservice]# vi svc.yaml\n  namespace: prod\n\n# 修改成prod，两处\napollo-configservice]# vi ingress.yaml\n  namespace: prod\n  - host: config-prod.od.com\n~~~~\n\n\n\n~~~~\n# 应用资源清单，22机器：\n~]# kubectl apply -f http://k8s-yaml.od.com/prod/apollo-configservice/cm.yaml\n~]# kubectl apply -f http://k8s-yaml.od.com/prod/apollo-configservice/dp.yaml\n~]# kubectl apply -f http://k8s-yaml.od.com/prod/apollo-configservice/svc.yaml\n~]# kubectl apply -f http://k8s-yaml.od.com/prod/apollo-configservice/ingress.yaml\n~~~~\n\n![1581347383718](assets/1581347383718.png)\n\n确认你的nslookup.exe 有没有（一般是有的）\n\n![1581347525407](assets/1581347525407.png)\n\n~~~\n# 访问config-test.od.com\nconfig.od.com已经没了\n~~~\n\n![1583304944456](assets/1583304944456.png)\n\n~~~~\n# 访问config-prod.od.com\nprod可能还没好，接着往下面做，test能起来，prod肯定也能起来\n~~~~\n\n![1583304959338](assets/1583304959338.png)\n\n> 可以看到test和prod在两个不同的容器里，这就是我们模拟的分环境\n\n~~~\n# 200机器：\ncd /data/k8s-yaml/test/apollo-adminservice/\napollo-adminservice]# cp -a /data/k8s-yaml/apollo-adminservice/*.yaml .\n# 修改成test，三处\napollo-adminservice]# vi cm.yaml\n  namespace: test\n    spring.datasource.url = jdbc:mysql://mysql.od.com:3306/ApolloConfigTestDB?\n    spring.service.url = http://config-test.od.com/eureka\n\n# 修改成test，一处\napollo-adminservice]# vi dp.yaml\n  namespace: test\n\napollo-adminservice]# cd ../../prod/apollo-adminservice\napollo-adminservice]# cp -a ../../test/apollo-adminservice/*.yaml .\n# 修改成prod，三处\napollo-adminservice]# vi cm.yaml\n  namespace: prod\n    spring.datasource.url = jdbc:mysql://mysql.od.com:3306/ApolloConfigProdDB?characterEncoding=utf8\n    spring.service.url = http://config-prod.od.com/eureka\n\n# 修改成prod，一处\napollo-adminservice]# vi dp.yaml\n  namespace: prod\n~~~\n\n\n\n~~~\n# 应用，22机器：\n~]# kubectl apply -f http://k8s-yaml.od.com/test/apollo-adminservice/cm.yaml\n~]# kubectl apply -f http://k8s-yaml.od.com/test/apollo-adminservice/dp.yaml\n~]# kubectl apply -f http://k8s-yaml.od.com/prod/apollo-adminservice/cm.yaml\n~]# kubectl apply -f http://k8s-yaml.od.com/prod/apollo-adminservice/dp.yaml\n~~~\n\n![1583304745055](assets/1583304745055.png)\n\n![1583304817198](assets/1583304817198.png)\n\n~~~\n# 11机器，清除数据：\nmysql -uroot -p\nnone)]> use ApolloPortalDB;\nApolloProtalDB]> select * from App;\nApolloProtalDB]> select * from AppNamespace;\nApolloProtalDB]> truncate table App;\nApolloProtalDB]> truncate table AppNamespace;\n~~~\n\n![1583305105464](assets/1583305105464.png)\n\n把portal scale成1\n\n![1583305172038](assets/1583305172038.png)\n\n使用查询功能查询下\n\n![1583305255615](assets/1583305255615.png)\n\n```bash\napollo.portal.envs\n```\n\n\n\n![1583305298870](assets/1583305298870.png)\n\n成功\n\n\n\n### 实战发布dubbo连接Apollo到不同环境\n\n创建项目\n\n[portal.od.com](portal.od.com)\n\n![1583305340551](assets/1583305340551.png)\n\n![1583305381813](assets/1583305381813.png)\n\n现在也可以看到环境列表有两个了\n\n![1583305431837](assets/1583305431837.png)\n\n测试环境新增配置项，并发布\n\n![1583305535156](assets/1583305535156.png)\n\n![1583305580379](assets/1583305580379.png)\n\n![1583305601716](assets/1583305601716.png)\n\n生产环境新增配置项，并发布\n\n![1583305663692](assets/1583305663692.png)\n\n![1583312777104](assets/1583312777104.png)\n\n![1583312803655](assets/1583312803655.png)\n\nservice配置好了，再配置consumer\n\n![1583305834462](assets/1583305834462.png)\n\n测试环境新增配置项，并发布\n\n![1583305939762](assets/1583305939762.png)\n\n![1583305965712](assets/1583305965712.png)\n\n生产环境新增配置项，并发布\n\n![1583306013898](assets/1583306013898.png)\n\n![1583306034551](assets/1583306034551.png)\n\n![1583306049712](assets/1583306049712.png)\n\n~~~\n# 200机器，制作测试环境service：\ncd /data/k8s-yaml/test/dubbo-demo-service/\ndubbo-demo-service]# cp -a /data/k8s-yaml/dubbo-demo-service/*.yaml .\n# 修改三处\ndubbo-demo-service]# vi dp.yaml\n  namespace: test\n        env:\n        - name: JAR_BALL\n          value: dubbo-server.jar\n        - name: C_OPTS\n          value: -Denv=fat -Dapollo.meta=http://config-test.od.com\n~~~\n\n\n~~~\n# 应用，22机器：\n~]# kubectl apply -f http://k8s-yaml.od.com/test/dubbo-demo-service/dp.yaml\n~~~\n\n![1583306650482](assets/1583306650482.png)\n\n~~~\n# 200机器，制作测试环境consumer：\ncd /data/k8s-yaml/test/dubbo-demo-consumer/\ndubbo-demo-consumer]# cp -a /data/k8s-yaml/dubbo-demo-consumer/*.yaml .\n# 修改三处\ndubbo-demo-consumer]# vi dp.yaml\n  namespace: test\n        - name: JAR_BALL\n          value: dubbo-client.jar\n        - name: C_OPTS\n          value: -Denv=fat -Dapollo.meta=http://config-test.od.com\n\n# 修改一处\ndubbo-demo-consumer]# vi svc.yaml\n  namespace: test\n\n# 修改一处\ndubbo-demo-consumer]# vi ingress.yaml\n  namespace: test\n  - host: demo-test.od.com\n~~~\n\n\n\n~~~\n# 11机器，新增解析：\n~]# vi /var/named/od.com.zone\nserial 前滚一位\ndemo-test          A    10.4.7.10\n\n~]# systemctl restart named\n~~~\n\n![1583306813921](assets/1583306813921.png)\n\n~~~\n# 应用，22机器：\n~]# kubectl apply -f http://k8s-yaml.od.com/test/dubbo-demo-consumer/dp.yaml\n~]# kubectl apply -f http://k8s-yaml.od.com/test/dubbo-demo-consumer/svc.yaml\n~]# kubectl apply -f http://k8s-yaml.od.com/test/dubbo-demo-consumer/ingress.yaml\n~~~\n\n查看dashboard的启动情况\n\n![1583307076131](assets/1583307076131.png)\n\n此时的monitor还是zk2，我们改成zk-test\n\n![1583307422408](assets/1583307422408.png)\n\n![1583307483922](assets/1583307483922.png)\n\nupdate然后删掉对应的pod让它自动重启\n\n![1583307517718](assets/1583307517718.png)\n\n![1583307567245](assets/1583307567245.png)\n\n![1583307586774](assets/1583307586774.png)\n\n~~~\n# 浏览器输入：demo-test.od.com/hello?name=test\n~~~\n\n![1583310055796](assets/1583310055796.png)\n\n完成\n\n开始做生产环境\n\n~~~\n# 11机器，新增解析：\n~]# vi /var/named/od.com.zone\nserial 前滚一位\ndemo-prod          A    10.4.7.10\n\n~]# systemctl restart named\n~~~\n\n![1581350791800](assets/1581350791800.png)\n\n~~~\n# 200机器，制作生产环境service：\ncd /data/k8s-yaml/prod/dubbo-demo-service/\ndubbo-demo-service]# cp -a ../../test/dubbo-demo-service/*.yaml .\n# 共修改三处\ndubbo-demo-service]# vi dp.yaml\n  namespace: prod\n        env:\n        - name: JAR_BALL\n          value: dubbo-server.jar\n        - name: C_OPTS\n          value: -Denv=pro -Dapollo.meta=http://apollo-configservice:8080\n~~~\n\n[^关于http://apollo-configservice:8080]: 直接用这个和用http://config-test.od.com是没有区别的，http://config-test.od.com是多了一层反代，而apollo-configservice:8080是因为它们在同一名称空间里，所以可以这么使用\n\n~~~\n# 应用，22机器：\n~]# kubectl apply -f http://k8s-yaml.od.com/prod/dubbo-demo-service/dp.yaml\n~~~\n\n![1583310378331](assets/1583310378331.png)\n\n你还可以去LOGS日志里面看看\n\n~~~\n# 200机器，制作生产环境consumer：\ncd /data/k8s-yaml/prod/dubbo-demo-consumer/\ndubbo-demo-consumer]# cp -a /data/k8s-yaml/dubbo-demo-consumer/*.yaml .\n# 共修改三处\ndubbo-demo-consumer]# vi dp.yaml\n  namespace: prod\n        env:\n        - name: JAR_BALL\n          value: dubbo-client.jar\n        - name: C_OPTS\n          value: -Denv=pro -Dapollo.meta=http://apollo-configservice:8080\n        imagePullPolicy: IfNotPresent\n\n# 共修改一处\ndubbo-demo-consumer]# vi svc.yaml\n  namespace: prod\n\n# 共修改两处\ndubbo-demo-consumer]# vi ingress.yaml\n  namespace: prod\nspec:\n  rules:\n  - host: demo-prod.od.com\n~~~\n\n\n\n~~~\n# 应用，22机器：\n~]# kubectl apply -f http://k8s-yaml.od.com/prod/dubbo-demo-consumer/dp.yaml\n~]# kubectl apply -f http://k8s-yaml.od.com/prod/dubbo-demo-consumer/svc.yaml\n~]# kubectl apply -f http://k8s-yaml.od.com/prod/dubbo-demo-consumer/ingress.yaml\n~~~\n\n~~~\n# 浏览器输入：demo-prod.od.com/hello?name=prod\n~~~\n\n![1583313070558](assets/1583313070558.png)\n\n完成\n\n> 报错：![1583312899877](assets/1583312899877.png)\n>\n> 我去LOGS里面看了一下，发现我Apollo配得port写错了，写成了dubbo-port，应该是dubbo.port，改回来就可以了，删掉service得pod重新启动，删掉consumer的pod重新启动\n\n\n\n### 实战演示项目提测，发版流程\n\n> 模拟项目提测到发布上线，这里用得gitlab，你有可以了解一下gitlab是长什么样子的，还有怎么操作的\n\n修改源代码，并commit\n\n![1583313961163](assets/1583313961163.png)\n\n![1583313975564](assets/1583313975564.png)\n\n> gitlab跟gittee在更新代码的编号有所区别\n\n![1583314181056](assets/1583314181056.png)\n\nJenkins构建\n\n~~~\n# 填入对应参数，然后build\napp_name: dubbo-demo-consumer\nimage_name: app/dubbo-demo-consumer\ngit_repo: http://gitlab.od.com:10000/909336740/dubbo-demo-web.git\ngit_ver: 535826b1239fedba0df3799b7b3b8585d56e9e18\nadd_tag: 200304_1730\ntarget_dir: ./dubbo-client/target\nbase_image: base/jre8:8u112\n~~~\n\n![1583314459402](assets/1583314459402.png)\n\n![1583314584034](assets/1583314584034.png)\n\n先到测试环境发布，修改tag\n\n![1583314731675](assets/1583314731675.png)\n\n![1583316573117](assets/1583316573117.png)\n\n重启成功，刷新测试环境的页面，查看情况\n\n![1583314816036](assets/1583314816036.png)\n\n刷新生产环境的页面，查看情况，还没改变\n\n![1583314831690](assets/1583314831690.png)\n\n我们来修改prod的，这时候已经不需要再去Jenkins打包镜像了\n\n![1583316530331](assets/1583316530331.png)\n\n![1583316573117](assets/1583316573117.png)\n\n更新完启动pod后\n\n再来刷新生产环境的页面\n\n![1583316620715](assets/1583316620715.png)\n\n已经改变，成功\n\n"
        },
        {
          "name": "第四章——dashboard插件及k8s实战交付.md",
          "type": "blob",
          "size": 17.482421875,
          "content": "## 第四章——dashboard插件及k8s实战交付\n\n> 引言：在集群的章节，我们开始使用交付服务的形式来交付，接下来我们也会持续使用这种方式来交付，流程：**准备镜像—>准备资源配置清单—>解析域名（没有ingress则不需要）—>应用配置清单—>完成**\n\n### dashboard安装部署\n\n> **WHAT**：向企业展示度量信息和关键业务指标（[KPI](https://baike.baidu.com/item/KPI)）现状的[数据虚拟化](https://baike.baidu.com/item/%E6%95%B0%E6%8D%AE%E8%99%9A%E6%8B%9F%E5%8C%96/6734581)工具\n>\n> **WHY**：我们目前都是用机器去安装部署资源，但我们以后不可能动不动就上主机，那样非常不安全，而且开发人员等也需要看到POD的情况，不可能让他们也登录主机去查看，所以我们需要一个有权限控制的界面展示和控制的工具\n\n~~~\n# 200机器，准备镜像：\ncd /data/k8s-yaml/\nk8s-yaml]# docker pull k8scn/kubernetes-dashboard-amd64:v1.8.3\nk8s-yaml]# docker images|grep dashboard\nk8s-yaml]# docker tag fcac9aa03fd6 harbor.od.com/public/dashboard:v1.8.3\nk8s-yaml]# docker push harbor.od.com/public/dashboard:v1.8.3\n~~~\n\n> 复习：push拉取、images显示所有镜像、|grep管道符，用于筛选、tag打标签、push上传\n\n~~~\n# 200机器，准备资源配置清单\nk8s-yaml]# mkdir /data/k8s-yaml/dashboard\nk8s-yaml]# cd /data/k8s-yaml/dashboard/\ndashboard]# vi rbac.yaml\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  labels:\n    k8s-app: kubernetes-dashboard\n    addonmanager.kubernetes.io/mode: Reconcile\n  name: kubernetes-dashboard-admin\n  namespace: kube-system\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: kubernetes-dashboard-admin\n  namespace: kube-system\n  labels:\n    k8s-app: kubernetes-dashboard\n    addonmanager.kubernetes.io/mode: Reconcile\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n- kind: ServiceAccount\n  name: kubernetes-dashboard-admin\n  namespace: kube-system\n  \ndashboard]# vi dp.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kubernetes-dashboard\n  namespace: kube-system\n  labels:\n    k8s-app: kubernetes-dashboard\n    kubernetes.io/cluster-service: \"true\"\n    addonmanager.kubernetes.io/mode: Reconcile\nspec:\n  selector:\n    matchLabels:\n      k8s-app: kubernetes-dashboard\n  template:\n    metadata:\n      labels:\n        k8s-app: kubernetes-dashboard\n      annotations:\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n    spec:\n      priorityClassName: system-cluster-critical\n      containers:\n      - name: kubernetes-dashboard\n        image: harbor.od.com/public/dashboard:v1.8.3\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 50m\n            memory: 100Mi\n        ports:\n        - containerPort: 8443\n          protocol: TCP\n        args:\n          # PLATFORM-SPECIFIC ARGS HERE\n          - --auto-generate-certificates\n        volumeMounts:\n        - name: tmp-volume\n          mountPath: /tmp\n        livenessProbe:\n          httpGet:\n            scheme: HTTPS\n            path: /\n            port: 8443\n          initialDelaySeconds: 30\n          timeoutSeconds: 30\n      volumes:\n      - name: tmp-volume\n        emptyDir: {}\n      serviceAccountName: kubernetes-dashboard-admin\n      tolerations:\n      - key: \"CriticalAddonsOnly\"\n        operator: \"Exists\"\n\t\t\ndashboard]# vi svc.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: kubernetes-dashboard\n  namespace: kube-system\n  labels:\n    k8s-app: kubernetes-dashboard\n    kubernetes.io/cluster-service: \"true\"\n    addonmanager.kubernetes.io/mode: Reconcile\nspec:\n  selector:\n    k8s-app: kubernetes-dashboard\n  ports:\n  - port: 443\n    targetPort: 8443\n\t\n\t\ndashboard]# vi ingress.yaml\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: kubernetes-dashboard\n  namespace: kube-system\n  annotations:\n    kubernetes.io/ingress.class: traefik\nspec:\n  rules:\n  - host: dashboard.od.com\n    http:\n      paths:\n      - backend:\n          serviceName: kubernetes-dashboard\n          servicePort: 443\n~~~\n\n> <a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E5%8E%9F%E7%90%86%E5%8F%8A%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/Kubernetes%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5.md#rbac%E5%9F%BA%E4%BA%8E%E8%A7%92%E8%89%B2%E7%9A%84%E6%9D%83%E9%99%90%E6%8E%A7%E5%88%B6\">RBAC:基于角色的权限控制</a>\n\n~~~\n# 11机器，解析域名：\n~]# vi /var/named/od.com.zone\nserial 前滚一位\ndashboard          A    10.4.7.10\n\n~]# systemctl restart named\n~]# dig -t A dashboard.od.com @10.4.7.11 +short\n~~~\n\n![1579173292893](assets/1579173292893.png)\n\n~~~\n# 选择任意运算节点，我选的是22机器，应用资源配置清单：\n# 选择应用节点，22机器\n~]# kubectl apply -f http://k8s-yaml.od.com/dashboard/rbac.yaml\n~]# kubectl apply -f http://k8s-yaml.od.com/dashboard/dp.yaml\n~]# kubectl apply -f http://k8s-yaml.od.com/dashboard/svc.yaml\n~]# kubectl apply -f http://k8s-yaml.od.com/dashboard/ingress.yaml\n~]# kubectl get pods -n kube-system\n~]# kubectl get svc -n kube-system\n~]# kubectl get ingress -n kube-system\n~~~\n\n![1579173213311](assets/1579173213311.png)\n\n[访问dashboard.od.com](dashboard.od.com)\n\n![1579173501791](assets/1579173501791.png)\n\n先跳过\n\n![1579173539350](assets/1579173539350.png)\n\n完成\n\n\n\n### K8S仪表盘鉴权\n\n> 上面这种是不需要登录就可以接入，我们可以用需要登录的版本，而且分权，即管理员用管理员的权限，普通用户用普通用户的权限\n\n~~~\n# 200机器，做证书：\ncd /opt/certs\ncerts]# (umask 077; openssl genrsa -out dashboard.od.com.key 2048)\n# 没有openssl的yum install openssl\ncerts]# openssl req -new -key dashboard.od.com.key -out dashboard.od.com.csr -subj \"/CN=dashboard.od.com/C=CN/ST=BJ/L=Beijing/O=ben1234560/OU=ops\"\ncerts]# openssl x509 -req -in dashboard.od.com.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out dashboard.od.com.crt -days 3650\ncerts]# cfssl-certinfo -cert dashboard.od.com.crt\n~~~\n\n\n\n~~~\n# 拷贝到11机器的nginx:\n~]# cd /etc/nginx/\nnginx]# mkdir certs\nnginx]# cd certs/\ncerts]# scp hdss7-200:/opt/certs/dashboard.od.com.crt .\ncerts]# scp hdss7-200:/opt/certs/dashboard.od.com.key .\ncerts]# cd ../conf.d/\nconf.d]# vi dashboard.od.com.conf\nserver {\n    listen       80;\n    server_name  dashboard.od.com;\n\n    rewrite ^(.*)$ https://${server_name}$1 permanent;\n}\nserver {\n    listen       443 ssl;\n    server_name  dashboard.od.com;\n\n    ssl_certificate \"certs/dashboard.od.com.crt\";\n    ssl_certificate_key \"certs/dashboard.od.com.key\";\n    ssl_session_cache shared:SSL:1m;\n    ssl_session_timeout  10m;\n    ssl_ciphers HIGH:!aNULL:!MD5;\n    ssl_prefer_server_ciphers on;\n\n    location / {\n        proxy_pass http://default_backend_traefik;\n        proxy_set_header Host       $http_host;\n        proxy_set_header x-forwarded-for $proxy_add_x_forwarded_for;\n    }\n}\n\nconf.d]# nginx -t\nconf.d]# nginx -s reload\n~~~\n\n> **nginx -t**：检查nginx文件语法\n>\n> **nginx -s reload**：热配置，即不用重启的更新\n\n~~~\n# 200机器，前面我们部署的是dashboard1.8，现在我们试下1.10\ncerts]# docker pull hexun/kubernetes-dashboard-amd64:v1.10.1\ncerts]# docker images|grep dash\ncerts]# docker tag f9aed6605b81 harbor.od.com/public/dashboard:v1.10.1\ncerts]# docker push harbor.od.com/public/dashboard:v1.10.1\ncerts]# cd /data/k8s-yaml/dashboard/\n# 修改以下版本信息，亦或者去dashboard修改\ndashboard]# vi dp.yaml\nimage: harbor.od.com/public/dashboard:v1.10.1\n~~~\n\n![1579228852242](assets/1579228852242.png)\n\n![1579228939094](assets/1579228939094.png)\n\n1.10.1版本是强制登录的，这时候，拿token去登录\n\n~~~\n# 21机器：\n~]# kubectl get secret -n kube-system\n# kubernetes-dashboard-admin-token-gq266\n~]# kubectl describe secret kubernetes-dashboard-admin-token-gq266 -n kube-system\n# token:eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJrdWJlcm5ldGVzLWRhc2hib2FyZC1hZG1pbi10b2tlbi1ncTI2NiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJrdWJlcm5ldGVzLWRhc2hib2FyZC1hZG1pbiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjY5MjM2OTY1LWIwOGMtNDYxNi1hOTIwLWRkMmIzNjMzOTcyZCIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTprdWJlcm5ldGVzLWRhc2hib2FyZC1hZG1pbiJ9.tTE7xZKgOm_o4S7Jq5iwudTXj66IWNz-1fv6y_ZxjgAJR8Jusa-DgDJppdATl_OZ7b1HPvKAA8T68ll2TzCpvoJe_rSyrCawrM9KxFVM7ZfyvNfIqScP5YqevV8GnvSkC50qMj3xClv9YM1Yo5ersgrB8bqYTqPIUPYwXsyBH-PA7PNVMWBHSeq6OzOUR4sM5IwSFKtNAvoM2Nxug7MY0wgUI2c3zFHeIe3d3do8zUWJClxKZG6HqABhEYICRL_zXFGQnoz8wyQsoSSg0YcctLY1BcvXYfzCvYVn953m_cz6t-WALhFW5kyqMz_JUODosl7povdJ0LW0pHzuicQYQA\n~~~\n\n> dashboard-admin-token指的是admin权限，即管理员权限\n\n![1579228067217](assets/1579228067217.png)\n\n![1579228151724](assets/1579228151724.png)\n\n[刷新dashboard.od.com](dashboard.od.com)\n\n![1579228224683](assets/1579228224683.png)\n\n![1579229142905](assets/1579229142905.png)\n\n此时管理员是需要密钥登录了，登录后有所有的权限，我们再来做普通用户\n\n[参考的官方网址](https://github.com/kubernetes/kubernetes/blob/master/cluster/addons/dashboard/)\n\n~~~\n# 200机器：\ncd /data/k8s-yaml/dashboard/\ndashboard]# vi rbac-minimal.yaml\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  labels:\n    k8s-app: kubernetes-dashboard\n    addonmanager.kubernetes.io/mode: Reconcile\n  name: kubernetes-dashboard\n  namespace: kube-system\n---\nkind: Role\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  labels:\n    k8s-app: kubernetes-dashboard\n    addonmanager.kubernetes.io/mode: Reconcile\n  name: kubernetes-dashboard-minimal\n  namespace: kube-system\nrules:\n  # Allow Dashboard to get, update and delete Dashboard exclusive secrets.\n- apiGroups: [\"\"]\n  resources: [\"secrets\"]\n  resourceNames: [\"kubernetes-dashboard-key-holder\", \"kubernetes-dashboard-certs\"]\n  verbs: [\"get\", \"update\", \"delete\"]\n  # Allow Dashboard to get and update 'kubernetes-dashboard-settings' config map.\n- apiGroups: [\"\"]\n  resources: [\"configmaps\"]\n  resourceNames: [\"kubernetes-dashboard-settings\"]\n  verbs: [\"get\", \"update\"]\n  # Allow Dashboard to get metrics from heapster.\n- apiGroups: [\"\"]\n  resources: [\"services\"]\n  resourceNames: [\"heapster\"]\n  verbs: [\"proxy\"]\n- apiGroups: [\"\"]\n  resources: [\"services/proxy\"]\n  resourceNames: [\"heapster\", \"http:heapster:\", \"https:heapster:\"]\n  verbs: [\"get\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: kubernetes-dashboard-minimal\n  namespace: kube-system\n  labels:\n    k8s-app: kubernetes-dashboard\n    addonmanager.kubernetes.io/mode: Reconcile\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: kubernetes-dashboard-minimal\nsubjects:\n- kind: ServiceAccount\n  name: kubernetes-dashboard\n  namespace: kube-system\n\n~~~\n\n>  **rbac-minimal.yaml：**\n>\n> - 这里的name变成了name: kubernetes-dashboard，已经不是admin，权限小些\n> - rules：可以看到rules给的一些比较小的权限\n> - RoleBinding：角色绑定\n> - 上半段是官方文档里面的dp.yaml，下面大班段是rbac-minimal.yaml\n\n~~~\n# 22机器，应用清单：\n~]# kubectl apply -f http://k8s-yaml.od.com/dashboard/rbac-minimal.yaml\n~~~\n\n![1582440867131](assets/1582440867131.png)\n\n~~~\n# 200机器，修改以下内容：\ndashboard]# vi dp.yaml\nserviceAccountName: kubernetes-dashboard\n~~~\n\n> 上面的意思如果没有令牌进来默认是普通用户，之前默认是admin用户\n\n![1582440913272](assets/1582440913272.png)\n\n~~~\n# 22机器，然后再回来应用dp：\n~]# kubectl apply -f http://k8s-yaml.od.com/dashboard/dp.yaml\n~]# kubectl get pods -n kube-system\n~]# kubectl get secret -n kube-system\n# 现在可以看到有两个token\n~]# kubectl describe secret kubernetes-dashboard-token-g67v7 -n kube-system\n~~~\n\n[重新登录dashboard.od.com](dashboard.od.com)\n\n![1579228224683](assets/1579228224683.png)\n\n有很多权限都没有\n\n![1582441754702](assets/1582441754702.png)\n\n> 生产上，我们应该用1.10的版本，因为登录需要token，而不是谁过来都可以skip进来用root权限，除了真正的管理员，其他人都不应该有管理员权限，而是其他人可以看/修改自己名称空间（Namespace）的权限，你只需要配置rbac-xxx.yaml的文件并应用即可\n\n~~~\n# 200机器，改回用来的1.8.3，有skip按钮比较方便学习：\n# 修改一下两段内容\ndashboard]# vi dp.yaml\nimage: harbor.od.com/public/dashboard:v1.8.3\nserviceAccountName: kubernetes-dashboard-admin\n# 22机器\n~]# kubectl apply -f http://k8s-yaml.od.com/dashboard/dp.yaml\n~]# kubectl get pods -n kube-system\n~~~\n\n![1579230932021](assets/1579230932021.png)\n\n\n\n### dashboard——heapster（可不做）\n\n> **WHAT**：让dashboard有更多图形化的小插件，不过目前由于收集的数据展示的图并不那么准确，所以可以不用部署仅作了解\n\n~~~\n# 200机器，准备镜像、资源配置清单：\ndashboard]# mkdir heapster\ndashboard]# cd heapster/\nheapster]# docker pull quay.io/bitnami/heapster:1.5.4\nheapster]# docker images|grep heapster\ndocker tag c359b95ad38b harbor.od.com/public/heapster:v1.5.4\nheapster]# docker push harbor.od.com/public/heapster:v1.5.4\nheapster]# vi rbac.yaml\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: heapster\n  namespace: kube-system\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: heapster\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: system:heapster\nsubjects:\n- kind: ServiceAccount\n  name: heapster\n  namespace: kube-system\n\nheapster]# vi dp.yaml\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: heapster\n  namespace: kube-system\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        task: monitoring\n        k8s-app: heapster\n    spec:\n      serviceAccountName: heapster\n      containers:\n      - name: heapster\n        image: harbor.od.com/public/heapster:v1.5.4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /opt/bitnami/heapster/bin/heapster\n        - --source=kubernetes:https://kubernetes.default\n\t\t\nheapster]# vi svc.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    task: monitoring\n    # For use as a Cluster add-on (https://github.com/kubernetes/kubernetes/tree/master/cluster/addons)\n    # If you are NOT using this as an addon, you should comment out this line.\n    kubernetes.io/cluster-service: 'true'\n    kubernetes.io/name: Heapster\n  name: heapster\n  namespace: kube-system\nspec:\n  ports:\n  - port: 80\n    targetPort: 8082\n  selector:\n    k8s-app: heapster\n~~~\n\n\n\n~~~\n# 22机器，应用资源配置清单：\n~]# kubectl apply -f http://k8s-yaml.od.com/dashboard/heapster/rbac.yaml\n~]# kubectl apply -f http://k8s-yaml.od.com/dashboard/heapster/dp.yaml\n~]# kubectl apply -f http://k8s-yaml.od.com/dashboard/heapster/svc.yaml\n~]# kubectl get pods -n kube-system\n~~~\n\n![1579233334649](assets/1579233334649.png)\n\n完成\n\n\n\n### K8S平滑升级技巧\n\n> **WHAT**：不需要做过多的操作，特别是关闭服务器等，而升级的方式\n>\n> **WHY**：生产中我们的服务每关一秒钟，损失的利润可能是上百万，而现在很多厂商，特别是游戏，都会写着什么时候升级但可以正常运行的公告，而且升级的时间一般是在流量低谷，流量低谷指的是用户使用量最少的时候\n\n~~~\n# 21节点\n~]# kubectl get node\n~]# kubectl get pod -n kube-system -o wide\n~~~\n\n![1579233625104](assets/1579233625104.png)\n\n~~~\n# 11机器，停掉7层网络(只需做一次)\n~]# vi /etc/nginx/nginx.conf\n# 把最下面的server21注释掉\n~]# vi /etc/nginx/conf.d/od.com.conf\n# 把server21注释掉\n~]# nginx -s reload\n~~~\n\n![1579241900984](assets/1579241900984.png)\n\n![1579241962440](assets/1579241962440.png)\n\n~~~\n# 21/22机器:\nsrc]# kubectl delete node hdss7-21.host.com\n# 以下操作可能需要不断刷新，因为调度器知道你的node关闭，才会开始迁移到其它node（会自动平衡资源），注意看IP\nsrc]# kubectl get nodes\nsrc]# kubectl get pod -n kube-system -o wide\n# 我们来看下dns有没有问题\ndig -t A kubernetes.default.svc.cluster.local @192.168.0.2 +short\n# out:192.168.0.1\n~~~\n\n![1579241869769](assets/1579241869769.png)\n\n~~~\n# 21/22机器:\ncd /opt/src\n把要升级的版本拉进来\nopt]# mkdir /opt/123\nopt]# cd src\nsrc]# tar xfv kubernetes-server-linux-amd64-v1.15.4.tar.gz -C /opt/123\nsrc]# ll\n# 把名字改下\nsrc]# cd /opt/123\n123]# mv kubernetes/ ../kubernetes-v1.15.4\nsrc]# cd ..\nopt]# ll\n# 这时候会有个1.15.2和1.15.4\ncd /kubernetes-v1.15.4\n# 删掉不需要的东西\nkubernetes-v1.15.4]# rm -f kubernetes-src.tar.gz\nkubernetes-v1.15.4]# cd server/bin\nbin]# rm -f *.tar\nbin]# rm -f *_tag\nbin]# mkdir conf\nbin]# mkdir cert\nbin]# cd cert/\ncert]# cp /opt/kubernetes/server/bin/cert/* .\ncert]# cd ../conf/\nconf]# cp /opt/kubernetes/server/bin/conf/* .\nconf]# cd ..\nbin]# cp /opt/kubernetes/server/bin/*.sh .\nbin]# cd /opt\nopt]# rm -rf kubernetes\nopt]# ln -s /opt/kubernetes-v1.15.4/ /opt/kubernetes\n# 生产上是一个一个来，我这里是一起\nopt]# supervisorctl restart all\nopt]# supervisorctl status\nopt]# kubectl get nodes\n~~~\n\n![1579246714293](assets/1579246714293.png)\n\n> 如果有出现起不来的情况：\n>\n> ​\t先用ps aux|grep [名字]\n>\n> ​\tkill -9 [id]\n>\n> ​\t再启动supervisorctl start [名字]\n\n完成\n\n下面是第三次做的结果\n\n![1582964650641](assets/1582964650641.png)\n\n<a href=\"https://github.com/ben1234560/k8s_PaaS/blob/master/%E5%8E%9F%E7%90%86%E5%8F%8A%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/Kubernetes%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5.md#%E6%B0%B4%E5%B9%B3%E6%89%A9%E5%B1%95%E5%92%8C%E6%BB%9A%E5%8A%A8%E5%8D%87%E7%BA%A7\">水平扩展和滚动升级补充</a>"
        },
        {
          "name": "终章——常用操作命令及相关方案.md",
          "type": "blob",
          "size": 6.08203125,
          "content": "## 终章——常用操作命令及相关方案\n\n> 本章节会增加一些在工作中常用的操作，以及开发交付的一些贴近场景的组件及工具代替原本教程中的内容\n\n\n\n## 命令\n\n### 查-POD级\n\nK8s查日志信息（describe即启动时的描述，在最下面的Event里。logs是deploy写好的输出日志）\n\n~~~\n# 先找到对应的pod空间和名字，第一列是空间名（NAMESPACE），第二列是pod名（PODNAME）\nkubectl get po -A | grep $PODNAME \nkubectl describe po -n$NAMESPACE $PODNAME\nkubectl logs -n$NAMESPACE $PODNAME\n# 进入到pod里\nkubectl exec -it -n$NAMESPACE $PODNAME -- /bin/bash\n~~~\n\nk8s查找pod配置内容\n\n~~~\nkubectl get $DEPLOY -n$NAMESPACE -o yaml |grep $SEARCH_SOMETHINE\n~~~\n\n> 查看哪个名称空间中那个类型配置里有这个内容，-o yaml 输出yaml格式内容\n\n查某个服务ip\n\n~~~\nkubectl get svc $DEPLOYNAME -n$NAMESPACE\n~~~\n\nk8s查找全部未运行成功的pod，-A指全部，owide可以看到是在哪个宿主下\n\n~~~\nkubectl get pod -Aowide | grep -ivE 'running|completed' \n~~~\n\n更强大的查看未运行成功的命令，因为有的pod虽然处于running但还是0/1，即未就绪状态，下面命令能把未就绪的也过滤出来\n\n~~~\nIFS_OlD=$IFS;IFS=$'\\n';for i in `kubectl get po -A -owide|grep -v NAME|grep -v Comple`;do desire=`echo $i|awk '{print $3}'|awk -F '/' '{print $1}'`;reality=`echo $i|awk '{print $3}'|awk -F '/' '{print $2}'`;if [[ \"$desire\" != \"$reality\" ]];then echo $i; fi; done;IFS=$IFS_OLD\n~~~\n\n\n\n### 查-NODE级\n\n~~~\n# 查看有多个节点\nkubectl get nodes\n# 查看集群信息\nkubectl cluster-info\n# 显示节点的资源使用情况\nkubectl top node\n~~~\n\n查看node的资源使用情况\n\n~~~\nfor node in `kubectl get node | awk '{print $1}'|grep -v NAME`;do echo \"======$node\"; echo \"==Capacity:\" ;kubectl describe  node $node | grep -A 12 \"Capacity:\" | grep -E \"cpu:|memory:|nvidia.com\\/gpu|vcuda-core|vcuda-memory:\"; echo \"=request&limit:\"; kubectl describe node $node |grep -E \"cpu | memory  |nvidia.com/gpu                             |vcuda-core               |vcuda-memory                \" ;done\n~~~\n\n\n\n\n\n### 删（重启，遇事不决重启pod）\n\n重启/删除不同空间下的不同条件的pod，-A看到全部\n\n~~~\nkubectl get pod -A | grep -E 'testA| testB' | awk '{print $1\" \"$2}' | while read line ; do kubectl delete pod -n $line ; done\n~~~\n\n> 可以先使用如下命令确认过滤出来的命名空间和pod是否一致，其中testA和testB是要过滤出来的pod名\n>\n> ~~~\n> kubectl get pod -A | grep -E 'testA| testB' | awk '{print $1\" \"$2}' \n> ~~~\n>\n> 也可以使用如下方法来重启/删除不同命名空间下非running状态的pod\n>\n> ~~~\n> kubectl get pod -A | grep -ivE 'running|completed' | awk '{print $1\" \"$2}' | while read line ; do kubectl delete pod -n $line ; done\n> ~~~\n>\n> 强制重启，在delete pod 后面跟上 --grace-period=0 --force\n>\n> ~~~\n> kubectl get pod -A | grep -ivE 'running|completed' | awk '{print $1\" \"$2}' | while read line ; do kubectl delete pod --grace-period=0 --force -n $line ; done\n> ~~~\n>\n> \n\n全部重启/删除指定空间下的pod\n\n~~~\nkubectl delete pods --all -n$NAMESPACE\nkubectl delete --all pods --namespace=$NAMESPACE\n~~~\n\n> 删除deploy之前可以先批量下载到一个文件，kubectl get deploy -n$NAMESPACE -o yaml > backup.yaml，然后再批量创建 kubectl create -f backup.yaml  -n$NAMESPACE\n\n批量重启/删除指定空间下的pod\n\n~~~\nkubectl get pods -n$NAMESPACE| grep -v Running | awk '{print $1}' | xargs kubectl delete pod -n$NAMESPACE\n~~~\n\n> grep -v：反取没有running状态的pod\n>\n>  awk '{print $1}' ：stdin出来pod名字，传递给xargs使用\n\n\n\n### 增\n\n~~~\n# 一般有现成的xxx.yaml文件，如下：\napiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  containers:\n  - name: my-container\n    image: nginx\n    ports:\n    - containerPort: 80\n# 定义了一个名为 my-pod 的 Pod，它运行一个单独的容器，该容器使用 nginx 镜像，并且开放了 80 端口。\nkubectl apply -f pod-example.yaml\n# 或者直接运行命令\nkubectl run my-pod --image=nginx --port=80\n~~~\n\n\n\n### 改\n\n~~~\n# 以上面创建的pod为例子，先导出yaml到本地。一般都有命名空间\nkubectl get deploy -n$NAMESPACE my-deployment -o yaml > my-deployment.yaml\n\n# 编辑现有的deploy，编辑后保存退出\nkubectl edit deploy -n$NAMESPACE my-deployment\n\n# 异常回滚方法一：\nkubectl rollout undo deployment my-deployment\n# 异常回滚方法二（apply之前保留的yaml文件，恢复旧配置）：\nkubectl apply -f my-deployment.yaml\n~~~\n\n\n\n## 相关方案\n\n### 关于监控\n\n第七章节中，我们用到Promtheus来做监控，随着不断更新换代，为了更轻便、快速和简洁，以及更好的兼容其他不同程序，我们会采用Jaeger、ELK、Telegraf、Grafana的组合，再加上时序数据库InfluxDB。去掉Dashboard、Promtheus，因为客户只需要开发平台，而不需要频繁的修改k8s。\n\n- **Jaeger：分布式追踪系统**（go语言），微服务系统更需要全链路跟踪，传统中，页面bug我们会开始排查前端问题，前端确认没问题说调用接口有错误日志，我们在去看后端，看完后端说底层就报错我们再去排查集群问题实在太耗费时间了，而全链路跟踪可以直接明了的看到是哪一环节的问题。\n- **ELK：ES、Logstash、Kibana**\n- **Telegraf：数据采集工具**（go语言），代替Prometheus\n- **InfluxDB：时序数据库**（go语言），代替TSDB，各个指标都高于TSDB，随着推出时间越来越久，对市面上的产品也已经很兼容了。\n- **Grafana：监控指标展示工具**（go语言）\n\n##### 关于InfluxDB在实际应用中遇到的情况\n\n在生产中由于机器数过多，使用默认配置的InfluxDB直接撑爆内存，重启内存会逐渐增大然后挂掉，也没办法进入，会报refused并提示确认是否在running，解决办法是直接把influx对应的路径下大的数据目录_retention结尾下的数字文件夹全部删掉，这样就有足够的空间，进入influx修改数据保存日期`alter retention policy \"db_name__retention\" on \"db_name\" duration 7d default`"
        },
        {
          "name": "软件包",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}