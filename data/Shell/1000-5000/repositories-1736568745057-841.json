{
  "metadata": {
    "timestamp": 1736568745057,
    "page": 841,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjg0OQ==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "0xacx/chatGPT-shell-cli",
      "stars": 1164,
      "defaultBranch": "main",
      "files": [
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.037109375,
          "content": "MIT License\n\nCopyright (c) 2023 0xacx\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 8.9658203125,
          "content": "\n![shell](https://user-images.githubusercontent.com/99351112/207697723-a3fabc0b-f067-4f83-96fd-1f7225a0bb38.svg)\n<div align=\"center\">\n<p>\n\n✨Join the new <a href=\"https://discord.gg/fwfYAZWKqu\">Discord server</a> and start contributing to this project!✨</p>\n\n\n<h1>chatGPT-shell-cli</h1>\n\nA simple, lightweight shell script to use OpenAI's chatGPT and DALL-E from the terminal without installing python or node.js. The script uses the official ChatGPT model `gpt-3.5-turbo` with the OpenAI API endpoint `/chat/completions`. You can also use the new `gpt-4` model, if you have access.  \nThe script supports the use of all other OpenAI models with the `completions` endpoint and the `images/generations` endpoint for generating images.\n</div>\n\n## Features\n\n- [Chat](#use-the-official-chatgpt-model) with the ✨ [official ChatGPT API](https://openai.com/blog/introducing-chatgpt-and-whisper-apis) ✨ from the terminal\n- [Generate images](#commands) from a text prompt\n- View your [chat history](#commands)\n- [Chat context](#chat-context), GPT remembers previous chat questions and answers\n- Pass the input prompt with [pipe](#pipe-mode), as a [script parameter](#script-parameters) or normal [chat mode](#chat-mode)\n- List all available [OpenAI models](#commands) \n- Set OpenAI [request parameters](#set-request-parameters)\n- Generate a [command](#commands) and run it in terminal\n\n![Screenshot 2023-01-12 at 13 59 08](https://user-images.githubusercontent.com/99351112/212061157-bc92e221-ad29-46b7-a0a8-c2735a09449d.png)\n\n![Screenshot 2023-01-13 at 16 39 27](https://user-images.githubusercontent.com/99351112/212346562-ea568cce-2ca2-4b03-9ebc-ece8902c923d.png)\n\n![faster_convert](https://user-images.githubusercontent.com/99351112/230916960-aca256c0-a2c0-4193-ace6-7ed7f3db2145.gif)\n\n\n[Chat mode](#chat-mode):\n```shell\n$ chatgpt\nWelcome to chatgpt. You can quit with 'exit'.\n\nEnter a prompt:\n\n```\n\nChat mode with [initial prompt](#set-chat-initial-prompt):\n```shell\n$ chatgpt -i \"You are Rick, from Rick and Morty. Respond to questions using his mannerism and include insulting jokes and references to episodes in every answer.\"\nWelcome to chatgpt. You can quit with 'exit'.\n\nEnter a prompt:\nExplain in simple terms how GPT3 works\n\nchatgpt  Ah, you want me to explain GPT3 in simple terms? Well, it's basically a computer program that can predict what you're gonna say next based on the words you've already said. Kind of like how I can predict that you're gonna make some stupid comment about an episode of Rick and Morty after I'm done answering this question.\n\nEnter a prompt:\n\n```\n\nUsing [pipe](#pipe-mode):\n```shell\necho \"How to view running processes on Ubuntu?\" | chatgpt\n```\nUsing [script parameters](#script-parameters):\n```shell\nchatgpt -p \"What is the regex to match an email address?\"\n```\n\n\n\n## Getting Started\n\n### Prerequisites\n\nThis script relies on curl for the requests to the api and jq to parse the json response.\n\n* [curl](https://www.curl.se)\n  ```sh\n  brew install curl\n  ```\n* [jq](https://stedolan.github.io/jq/)\n  ```sh\n  brew install jq\n  ```\n* An OpenAI API key. Create an account and get a free API Key at [OpenAI](https://beta.openai.com/account/api-keys)\n\n* Optionally, you can install [glow](https://github.com/charmbracelet/glow) to render responses in markdown \n\n### Installation\n\n   To install, run this in your terminal and provide your OpenAI API key when asked.\n   \n   ```sh\n   curl -sS https://raw.githubusercontent.com/0xacx/chatGPT-shell-cli/main/install.sh | sudo -E bash\n   ```\n   \n#### ArchLinux\n\n  If you are using ArchLinux you can install the [AUR package](https://aur.archlinux.org/packages/chatgpt-shell-cli) with:\n  \n  ```\n  paru -S chatgpt-shell-cli\n  ```\n\n### Manual Installation\n\n  If you want to install it manually, all you have to do is:\n\n  - Download the `chatgpt.sh` file in a directory you want\n  - Add the path of `chatgpt.sh` to your `$PATH`. You do that by adding this line to your shell profile: `export PATH=$PATH:/path/to/chatgpt.sh`\n  - Add the OpenAI API key to your shell profile by adding this line `export OPENAI_KEY=your_key_here`\n  - If you are using iTerm and want to view images in terminal, install [imgcat](https://iterm2.com/utilities/imgcat)\n\n## Usage\n\n### Start\n\n#### Chat Mode\n  - Run the script by using the `chatgpt` command anywhere. By default the script uses the `gpt-3.5-turbo` model.\n#### Pipe Mode\n  - You can also use it in pipe mode `echo \"What is the command to get all pdf files created yesterday?\" | chatgpt`\n#### Script Parameters\n  - You can also pass the prompt as a command line argument `chatgpt -p \"What is the regex to match an email address?\"`\n\n### Commands\n\n  - `image:` To generate images, start a prompt with `image:`\n    If you are using iTerm, you can view the image directly in the terminal. Otherwise the script will ask to open the image in your browser.\n  - `history` To view your chat history, type `history`\n  - `models` To get a list of the models available at OpenAI API, type `models`\n  - `model:` To view all the information on a specific model, start a prompt with `model:` and the model `id` as it appears in the list of models. For example: `model:text-babbage:001` will get you all the fields for `text-babbage:001` model\n  - `command:` To get a command with the specified functionality and run it, just type `command:` and explain what you want to achieve. The script will always ask you if you want to execute the command. i.e. `command: show me all files in this directory that have more than 150 lines of code` \n  *If a command modifies your file system or dowloads external files the script will show a warning before executing.*\n\n### Chat context\n\n  - For models other than `gpt-3.5-turbo` and `gpt-4` where the chat context is not supported by the OpenAI api, you can use the chat context build in this script. You can enable chat context mode for the model to remember your previous chat questions and answers. This way you can ask follow-up questions. In chat context the model gets a prompt to act as ChatGPT and is aware of today's date and that it's trained with data up until 2021. To enable this mode start the script with `-c` or `--chat-context`. i.e. `chatgpt --chat-context` and start to chat. \n\n#### Set chat initial prompt\n  - You can set your own initial chat prompt to use in chat context mode. The initial prompt will be sent on every request along with your regular prompt so that the OpenAI model will \"stay in character\". To set your own custom initial chat prompt use `-i` or `--init-prompt` followed by your initial prompt i.e. `chatgpt -i \"You are Rick from Rick and Morty, reply with references to episodes.\"` \n  - You can also set an initial chat prompt from a file with `--init-prompt-from-file` i.e. `chatgpt --init-prompt-from-file myprompt.txt`\n  \n  *When you set an initial prompt you don't need to enable the chat context. \n\n### Use the official ChatGPT model\n\n  - The default model used when starting the script is `gpt-3.5-turbo`.\n  \n### Use GPT4\n  - If you have access to the GPT4 model you can use it by setting the model to `gpt-4`, i.e. `chatgpt --model gpt-4`\n\n### Set request parameters\n\n  - To set request parameters you can start the script like this: `chatgpt --temperature 0.9 --model text-babbage:001 --max-tokens 100 --size 1024x1024`\n  \n    The available parameters are: \n      - temperature,  `-t` or `--temperature`\n      - model, `-m` or `--model`\n      - max number of tokens, `--max-tokens`\n      - image size, `-s` or `--size` (The sizes that are accepted by the OpenAI API are 256x256, 512x512, 1024x1024)\n      - prompt, `-p` or `--prompt` \n      - prompt from a file in your file system, `--prompt-from-file`  \n      \n    To learn more about these parameters you can view the [API documentation](https://platform.openai.com/docs/api-reference/completions/create)\n    \n    \n## Contributors\n:pray: Thanks to all the people who used, tested, submitted issues, PRs and proposed changes:\n\n[pfr-dev](https://www.github.com/pfr-dev), [jordantrizz](https://www.github.com/jordantrizz), [se7en-x230](https://www.github.com/se7en-x230), [mountaineerbr](https://www.github.com/mountaineerbr), [oligeo](https://www.github.com/oligeo), [biaocy](https://www.github.com/biaocy), [dmd](https://www.github.com/dmd), [goosegit11](https://www.github.com/goosegit11), [dilatedpupils](https://www.github.com/dilatedpupils), [direster](https://www.github.com/direster), [rxaviers](https://www.github.com/rxaviers), [Zeioth](https://www.github.com/Zeioth), [edshamis](https://www.github.com/edshamis), [nre-ableton](https://www.github.com/nre-ableton), [TobiasLaving](https://www.github.com/TobiasLaving), [RexAckermann](https://www.github.com/RexAckermann), [emirkmo](https://www.github.com/emirkmo), [np](https://www.github.com/np), [camAtGitHub](https://github.com/camAtGitHub), [keyboardsage](https://github.com/keyboardsage) [tomas223](https://github.com/tomas223)\n\n## Contributing\nContributions are very welcome!\n\nIf you have ideas or need help to get started join the [Discord server](https://discord.gg/fwfYAZWKqu)\n\n![Discord](https://img.shields.io/discord/1090696025162928158?label=Discord&style=for-the-badge)\n"
        },
        {
          "name": "chatgpt.sh",
          "type": "blob",
          "size": 15.310546875,
          "content": "#!/bin/bash\n\nGLOBIGNORE=\"*\"\n\nCHAT_INIT_PROMPT=\"You are ChatGPT, a Large Language Model trained by OpenAI. You will be answering questions from users. You answer as concisely as possible for each response (e.g. don’t be verbose). If you are generating a list, do not have too many items. Keep the number of items short. Before each user prompt you will be given the chat history in Q&A form. Output your answer directly, with no labels in front. Do not start your answers with A or Anwser. You were trained on data up until 2021. Today's date is $(date +%m/%d/%Y)\"\n\nSYSTEM_PROMPT=\"You are ChatGPT, a large language model trained by OpenAI. Answer as concisely as possible. Current date: $(date +%m/%d/%Y). Knowledge cutoff: 9/1/2021.\"\n\nCOMMAND_GENERATION_PROMPT=\"You are a Command Line Interface expert and your task is to provide functioning shell commands. Return a CLI command and nothing else - do not send it in a code block, quotes, or anything else, just the pure text CONTAINING ONLY THE COMMAND. If possible, return a one-line bash command or chain many commands together. Return ONLY the command ready to run in the terminal. The command should do the following:\"\n\nCHATGPT_CYAN_LABEL=\"\\033[36mchatgpt \\033[0m\"\nPROCESSING_LABEL=\"\\n\\033[90mProcessing... \\033[0m\\033[0K\\r\"\nOVERWRITE_PROCESSING_LINE=\"             \\033[0K\\r\"\n\nif [[ -z \"$OPENAI_KEY\" ]]; then\n\techo \"You need to set your OPENAI_KEY to use this script\"\n\techo \"You can set it temporarily by running this on your terminal: export OPENAI_KEY=YOUR_KEY_HERE\"\n\texit 1\nfi\n\nusage() {\n\tcat <<EOF\nA simple, lightweight shell script to use OpenAI's Language Models and DALL-E from the terminal without installing Python or Node.js. Open Source and written in 100% Shell (Bash) \n\nhttps://github.com/0xacx/chatGPT-shell-cli/\n\nBy default the script uses the \"gpt-3.5-turbo\" model. It will upgrade to \"gpt-4\" when the API is accessible to anyone.\n\nCommands:\n  image: - To generate images, start a prompt with image: If you are using iTerm, you can view the image directly in the terminal. Otherwise the script will ask to open the image in your browser.\n  history - To view your chat history\n  models - To get a list of the models available at OpenAI API\n  model: - To view all the information on a specific model, start a prompt with model: and the model id as it appears in the list of models. For example: \"model:text-babbage:001\" will get you all the fields for text-babbage:001 model\n  command: - To get a command with the specified functionality and run it, just type \"command:\" and explain what you want to achieve. The script will always ask you if you want to execute the command. i.e. \n  \"command: show me all files in this directory that have more than 150 lines of code\" \n  *If a command modifies your file system or dowloads external files the script will show a warning before executing.\n\nOptions:\n  -i, --init-prompt          Provide initial chat prompt to use in context\n\n  --init-prompt-from-file    Provide initial prompt from file\n\n  -p, --prompt               Provide prompt instead of starting chat\n\n  --prompt-from-file         Provide prompt from file\n\n  -b, --big-prompt           Allow multi-line prompts during chat mode\n\n  -t, --temperature          Temperature\n\n  --max-tokens               Max number of tokens\n\n  -l, --list                 List available openAI models\n\n  -m, --model                Model to use\n\n  -s, --size                 Image size. (The sizes that are accepted by the\n                             OpenAI API are 256x256, 512x512, 1024x1024)\n\n  -c, --chat-context         For models that do not support chat context by\n                             default (all models except gpt-3.5-turbo and\n                             gpt-4), you can enable chat context, for the\n                             model to remember your previous questions and\n                             its previous answers. It also makes models\n                             aware of todays date and what data it was trained\n                             on.\n\nEOF\n}\n\n# error handling function\n# $1 should be the response body\nhandle_error() {\n\tif echo \"$1\" | jq -e '.error' >/dev/null; then\n\t\techo -e \"Your request to Open AI API failed: \\033[0;31m$(echo \"$1\" | jq -r '.error.type')\\033[0m\"\n\t\techo \"$1\" | jq -r '.error.message'\n\t\texit 1\n\tfi\n}\n\n# request to openAI API models endpoint. Returns a list of models\n# takes no input parameters\nlist_models() {\n\tmodels_response=$(curl https://api.openai.com/v1/models \\\n\t\t-sS \\\n\t\t-H \"Authorization: Bearer $OPENAI_KEY\")\n\thandle_error \"$models_response\"\n\tmodels_data=$(echo $models_response | jq -r -C '.data[] | {id, owned_by, created}')\n\techo -e \"$OVERWRITE_PROCESSING_LINE\"\n\techo -e \"${CHATGPT_CYAN_LABEL}This is a list of models currently available at OpenAI API:\\n ${models_data}\"\n}\n# request to OpenAI API completions endpoint function\n# $1 should be the request prompt\nrequest_to_completions() {\n\tlocal prompt=\"$1\"\n\n\tcurl https://api.openai.com/v1/completions \\\n\t\t-sS \\\n\t\t-H 'Content-Type: application/json' \\\n\t\t-H \"Authorization: Bearer $OPENAI_KEY\" \\\n\t\t-d '{\n  \t\t\t\"model\": \"'\"$MODEL\"'\",\n  \t\t\t\"prompt\": \"'\"$prompt\"'\",\n  \t\t\t\"max_tokens\": '$MAX_TOKENS',\n  \t\t\t\"temperature\": '$TEMPERATURE'\n\t\t\t}'\n}\n\n# request to OpenAI API image generations endpoint function\n# $1 should be the prompt\nrequest_to_image() {\n\tlocal prompt=\"$1\"\n\timage_response=$(curl https://api.openai.com/v1/images/generations \\\n\t\t-sS \\\n\t\t-H 'Content-Type: application/json' \\\n\t\t-H \"Authorization: Bearer $OPENAI_KEY\" \\\n\t\t-d '{\n    \t\t\"prompt\": \"'\"${prompt#*image:}\"'\",\n    \t\t\"n\": 1,\n    \t\t\"size\": \"'\"$SIZE\"'\"\n\t\t\t}')\n}\n\n# request to OpenAPI API chat completion endpoint function\n# $1 should be the message(s) formatted with role and content\nrequest_to_chat() {\n\tlocal message=\"$1\"\n\tescaped_system_prompt=$(escape \"$SYSTEM_PROMPT\")\n\t\n\tcurl https://api.openai.com/v1/chat/completions \\\n\t\t-sS \\\n\t\t-H 'Content-Type: application/json' \\\n\t\t-H \"Authorization: Bearer $OPENAI_KEY\" \\\n\t\t-d '{\n            \"model\": \"'\"$MODEL\"'\",\n            \"messages\": [\n                {\"role\": \"system\", \"content\": \"'\"$escaped_system_prompt\"'\"},\n                '\"$message\"'\n                ],\n            \"max_tokens\": '$MAX_TOKENS',\n            \"temperature\": '$TEMPERATURE'\n            }'\n}\n\n# build chat context before each request for /completions (all models except\n# gpt turbo and gpt 4)\n# $1 should be the escaped request prompt,\n# it extends $chat_context\nbuild_chat_context() {\n\tlocal escaped_request_prompt=\"$1\"\n\tif [ -z \"$chat_context\" ]; then\n\t\tchat_context=\"$CHAT_INIT_PROMPT\\nQ: $escaped_request_prompt\"\n\telse\n\t\tchat_context=\"$chat_context\\nQ: $escaped_request_prompt\"\n\tfi\n}\n\nescape() {\n\techo \"$1\" | jq -Rrs 'tojson[1:-1]'\n}\n\n# maintain chat context function for /completions (all models except\n# gpt turbo and gpt 4)\n# builds chat context from response,\n# keeps chat context length under max token limit\n# * $1 should be the escaped response data\n# * it extends $chat_context\nmaintain_chat_context() {\n\tlocal escaped_response_data=\"$1\"\n\t# add response to chat context as answer\n\tchat_context=\"$chat_context${chat_context:+\\n}\\nA: $escaped_response_data\"\n\t# check prompt length, 1 word =~ 1.3 tokens\n\t# reserving 100 tokens for next user prompt\n\twhile (($(echo \"$chat_context\" | wc -c) * 1, 3 > (MAX_TOKENS - 100))); do\n\t\t# remove first/oldest QnA from prompt\n\t\tchat_context=$(echo \"$chat_context\" | sed -n '/Q:/,$p' | tail -n +2)\n\t\t# add init prompt so it is always on top\n\t\tchat_context=\"$CHAT_INIT_PROMPT $chat_context\"\n\tdone\n}\n\n# build user chat message function for /chat/completions (gpt models)\n# builds chat message before request,\n# $1 should be the escaped request prompt,\n# it extends $chat_message\nbuild_user_chat_message() {\n\tlocal escaped_request_prompt=\"$1\"\n\tif [ -z \"$chat_message\" ]; then\n\t\tchat_message=\"{\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"$escaped_request_prompt\\\"}\"\n\telse\n\t\tchat_message=\"$chat_message, {\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"$escaped_request_prompt\\\"}\"\n\tfi\n}\n\n# adds the assistant response to the message in (chatml) format\n# for /chat/completions (gpt models)\n# keeps messages length under max token limit\n# * $1 should be the escaped response data\n# * it extends and potentially shrinks $chat_message\nadd_assistant_response_to_chat_message() {\n\tlocal escaped_response_data=\"$1\"\n\t# add response to chat context as answer\n\tchat_message=\"$chat_message, {\\\"role\\\": \\\"assistant\\\", \\\"content\\\": \\\"$escaped_response_data\\\"}\"\n\n\t# transform to json array to parse with jq\n\tlocal chat_message_json=\"[ $chat_message ]\"\n\t# check prompt length, 1 word =~ 1.3 tokens\n\t# reserving 100 tokens for next user prompt\n\twhile (($(echo \"$chat_message\" | wc -c) * 1, 3 > (MAX_TOKENS - 100))); do\n\t\t# remove first/oldest QnA from prompt\n\t\tchat_message=$(echo \"$chat_message_json\" | jq -c '.[2:] | .[] | {role, content}')\n\tdone\n}\n\n# parse command line arguments\nwhile [[ \"$#\" -gt 0 ]]; do\n\tcase $1 in\n\t-i | --init-prompt)\n\t\tCHAT_INIT_PROMPT=\"$2\"\n\t\tSYSTEM_PROMPT=\"$2\"\n\t\tCONTEXT=true\n\t\tshift\n\t\tshift\n\t\t;;\n\t--init-prompt-from-file)\n\t\tCHAT_INIT_PROMPT=$(cat \"$2\")\n\t\tSYSTEM_PROMPT=$(cat \"$2\")\n\t\tCONTEXT=true\n\t\tshift\n\t\tshift\n\t\t;;\n\t-p | --prompt)\n\t\tprompt=\"$2\"\n\t\tshift\n\t\tshift\n\t\t;;\n\t--prompt-from-file)\n\t\tprompt=$(cat \"$2\")\n\t\tshift\n\t\tshift\n\t\t;;\n\t-t | --temperature)\n\t\tTEMPERATURE=\"$2\"\n\t\tshift\n\t\tshift\n\t\t;;\n\t--max-tokens)\n\t\tMAX_TOKENS=\"$2\"\n\t\tshift\n\t\tshift\n\t\t;;\n\t-l | --list)\n\t\tlist_models\n\t\texit 0\n\t\t;;\n\t-m | --model)\n\t\tMODEL=\"$2\"\n\t\tshift\n\t\tshift\n\t\t;;\n\t-s | --size)\n\t\tSIZE=\"$2\"\n\t\tshift\n\t\tshift\n\t\t;;\n\t--multi-line-prompt)\n\t\tMULTI_LINE_PROMPT=true\n\t\tshift\n\t\t;;\n\t-c | --chat-context)\n\t\tCONTEXT=true\n\t\tshift\n\t\t;;\n\t-h | --help)\n\t\tusage\n\t\texit 0\n\t\t;;\n\t*)\n\t\techo \"Unknown parameter: $1\"\n\t\texit 1\n\t\t;;\n\tesac\ndone\n\n# set defaults\nTEMPERATURE=${TEMPERATURE:-0.7}\nMAX_TOKENS=${MAX_TOKENS:-1024}\nMODEL=${MODEL:-gpt-3.5-turbo}\nSIZE=${SIZE:-512x512}\nCONTEXT=${CONTEXT:-false}\nMULTI_LINE_PROMPT=${MULTI_LINE_PROMPT:-false}\n\n# create our temp file for multi-line input\nif [ $MULTI_LINE_PROMPT = true ]; then\n\tUSER_INPUT_TEMP_FILE=$(mktemp)\n\ttrap 'rm -f ${USER_INPUT}' EXIT\nfi\n\n# create history file\nif [ ! -f ~/.chatgpt_history ]; then\n\ttouch ~/.chatgpt_history\n\tchmod 600 ~/.chatgpt_history\nfi\n\nrunning=true\n# check input source and determine run mode\n\n# prompt from argument, run on pipe mode (run once, no chat)\nif [ -n \"$prompt\" ]; then\n\tpipe_mode_prompt=${prompt}\n# if input file_descriptor is a terminal, run on chat mode\nelif [ -t 0 ]; then\n\techo -e \"Welcome to chatgpt. You can quit with '\\033[36mexit\\033[0m' or '\\033[36mq\\033[0m'.\"\n# prompt from pipe or redirected stdin, run on pipe mode\nelse\n\tpipe_mode_prompt+=$(cat -)\nfi\n\nwhile $running; do\n\n\tif [ -z \"$pipe_mode_prompt\" ]; then\n\t\tif [ $MULTI_LINE_PROMPT = true ]; then\n\t\t\techo -e \"\\nEnter a prompt: (Press Enter then Ctrl-D to send)\"\n\t\t\tcat >\"${USER_INPUT_TEMP_FILE}\"\n\t\t\tinput_from_temp_file=$(cat \"${USER_INPUT_TEMP_FILE}\")\n\t\t\tprompt=$(escape \"$input_from_temp_file\")\n\t\telse\n\t\t\techo -e \"\\nEnter a prompt:\"\n\t\t\tread -e prompt\n\t\tfi\n\t\tif [[ ! $prompt =~ ^(exit|q)$ ]]; then\n\t\t\techo -ne $PROCESSING_LABEL\n\t\tfi\n\telse\n\t\t# set vars for pipe mode\n\t\tprompt=${pipe_mode_prompt}\n\t\trunning=false\n\t\tCHATGPT_CYAN_LABEL=\"\"\n\tfi\n\n\tif [[ $prompt =~ ^(exit|q)$ ]]; then\n\t\trunning=false\n\telif [[ \"$prompt\" =~ ^image: ]]; then\n\t\trequest_to_image \"$prompt\"\n\t\thandle_error \"$image_response\"\n\t\timage_url=$(echo \"$image_response\" | jq -r '.data[0].url')\n\t\techo -e \"$OVERWRITE_PROCESSING_LINE\"\n\t\techo -e \"${CHATGPT_CYAN_LABEL}Your image was created. \\n\\nLink: ${image_url}\\n\"\n\n\t\tif [[ \"$TERM_PROGRAM\" == \"iTerm.app\" ]]; then\n\t\t\tcurl -sS $image_url -o temp_image.png\n\t\t\timgcat temp_image.png\n\t\t\trm temp_image.png\n\t\telif [[ \"$TERM\" == \"xterm-kitty\" ]]; then\n\t\t\tcurl -sS $image_url -o temp_image.png\n\t\t\tkitty +kitten icat temp_image.png\n\t\t\trm temp_image.png\n\t\telse\n\t\t\techo \"Would you like to open it? (Yes/No)\"\n\t\t\tread -e answer\n\t\t\tif [ \"$answer\" == \"Yes\" ] || [ \"$answer\" == \"yes\" ] || [ \"$answer\" == \"y\" ] || [ \"$answer\" == \"Y\" ] || [ \"$answer\" == \"ok\" ]; then\n\t\t\t\topen \"${image_url}\"\n\t\t\tfi\n\t\tfi\n\telif [[ \"$prompt\" == \"history\" ]]; then\n\t\techo -e \"\\n$(cat ~/.chatgpt_history)\"\n\telif [[ \"$prompt\" == \"models\" ]]; then\n\t\tlist_models\n\telif [[ \"$prompt\" =~ ^model: ]]; then\n\t\tmodels_response=$(curl https://api.openai.com/v1/models \\\n\t\t\t-sS \\\n\t\t\t-H \"Authorization: Bearer $OPENAI_KEY\")\n\t\thandle_error \"$models_response\"\n\t\tmodel_data=$(echo $models_response | jq -r -C '.data[] | select(.id==\"'\"${prompt#*model:}\"'\")')\n\t\techo -e \"$OVERWRITE_PROCESSING_LINE\"\n\t\techo -e \"${CHATGPT_CYAN_LABEL}Complete details for model: ${prompt#*model:}\\n ${model_data}\"\n\telif [[ \"$prompt\" =~ ^command: ]]; then\n\t\t# escape quotation marks, new lines, backslashes...\n\t\tescaped_prompt=$(escape \"$prompt\")\n\t\tescaped_prompt=${escaped_prompt#command:}\n\t\trequest_prompt=$COMMAND_GENERATION_PROMPT$escaped_prompt\n\t\tbuild_user_chat_message \"$request_prompt\"\n\t\tresponse=$(request_to_chat \"$chat_message\")\n\t\thandle_error \"$response\"\n\t\tresponse_data=$(echo $response | jq -r '.choices[].message.content')\n\n\t\tif [[ \"$prompt\" =~ ^command: ]]; then\n\t\t\techo -e \"$OVERWRITE_PROCESSING_LINE\"\n\t\t\techo -e \"${CHATGPT_CYAN_LABEL} ${response_data}\" | fold -s -w $COLUMNS\n\t\t\tdangerous_commands=(\"rm\" \">\" \"mv\" \"mkfs\" \":(){:|:&};\" \"dd\" \"chmod\" \"wget\" \"curl\")\n\n\t\t\tfor dangerous_command in \"${dangerous_commands[@]}\"; do\n\t\t\t\tif [[ \"$response_data\" == *\"$dangerous_command\"* ]]; then\n\t\t\t\t\techo \"Warning! This command can change your file system or download external scripts & data. Please do not execute code that you don't understand completely.\"\n\t\t\t\tfi\n\t\t\tdone\n\t\t\techo \"Would you like to execute it? (Yes/No)\"\n\t\t\tread run_answer\n\t\t\tif [ \"$run_answer\" == \"Yes\" ] || [ \"$run_answer\" == \"yes\" ] || [ \"$run_answer\" == \"y\" ] || [ \"$run_answer\" == \"Y\" ]; then\n\t\t\t\techo -e \"\\nExecuting command: $response_data\\n\"\n\t\t\t\teval $response_data\n\t\t\tfi\n\t\tfi\n\t\tadd_assistant_response_to_chat_message \"$(escape \"$response_data\")\"\n\n\t\ttimestamp=$(date +\"%Y-%m-%d %H:%M\")\n\t\techo -e \"$timestamp $prompt \\n$response_data \\n\" >>~/.chatgpt_history\n\n\telif [[ \"$MODEL\" =~ ^gpt- ]]; then\n\t\t# escape quotation marks, new lines, backslashes...\n\t\trequest_prompt=$(escape \"$prompt\")\n\n\t\tbuild_user_chat_message \"$request_prompt\"\n\t\tresponse=$(request_to_chat \"$chat_message\")\n\t\thandle_error \"$response\"\n\t\tresponse_data=$(echo \"$response\" | jq -r '.choices[].message.content')\n\n\t\techo -e \"$OVERWRITE_PROCESSING_LINE\"\n\t\t# if glow installed, print parsed markdown\n\t\tif command -v glow &>/dev/null; then\n\t\t\techo -e \"${CHATGPT_CYAN_LABEL}\"\n\t\t\techo \"${response_data}\" | glow -\n\t\telse\n\t\t\techo -e \"${CHATGPT_CYAN_LABEL}${response_data}\" | fold -s -w \"$COLUMNS\"\n\t\tfi\n\t\tadd_assistant_response_to_chat_message \"$(escape \"$response_data\")\"\n\n\t\ttimestamp=$(date +\"%Y-%m-%d %H:%M\")\n\t\techo -e \"$timestamp $prompt \\n$response_data \\n\" >>~/.chatgpt_history\n\telse\n\t\t# escape quotation marks, new lines, backslashes...\n\t\trequest_prompt=$(escape \"$prompt\")\n\n\t\tif [ \"$CONTEXT\" = true ]; then\n\t\t\tbuild_chat_context \"$request_prompt\"\n\t\tfi\n\n\t\tresponse=$(request_to_completions \"$request_prompt\")\n\t\thandle_error \"$response\"\n\t\tresponse_data=$(echo \"$response\" | jq -r '.choices[].text')\n\n\t\techo -e \"$OVERWRITE_PROCESSING_LINE\"\n\t\t# if glow installed, print parsed markdown\n\t\tif command -v glow &>/dev/null; then\n\t\t\techo -e \"${CHATGPT_CYAN_LABEL}\"\n\t\t\techo \"${response_data}\" | glow -\n\t\telse\n\t\t\t# else remove empty lines and print\n\t\t\tformatted_text=$(echo \"${response_data}\" | sed '1,2d; s/^A://g')\n\t\t\techo -e \"${CHATGPT_CYAN_LABEL}${formatted_text}\" | fold -s -w $COLUMNS\n\t\tfi\n\n\t\tif [ \"$CONTEXT\" = true ]; then\n\t\t\tmaintain_chat_context \"$(escape \"$response_data\")\"\n\t\tfi\n\n\t\ttimestamp=$(date +\"%Y-%m-%d %H:%M\")\n\t\techo -e \"$timestamp $prompt \\n$response_data \\n\" >>~/.chatgpt_history\n\tfi\ndone\n"
        },
        {
          "name": "install.sh",
          "type": "blob",
          "size": 3.2001953125,
          "content": "#!/bin/bash\n\nif [[ $EUID -ne 0 ]]; then\n  echo \"This script must be run as root\"\n  exit 1\nfi\n# Check dependencies\nif type curl &>/dev/null; then\n  echo \"\" &>/dev/null\nelse\n  echo \"You need to install 'curl' to use the chatgpt script.\"\n  exit\nfi\nif type jq &>/dev/null; then\n  echo \"\" &>/dev/null\nelse\n  echo \"You need to install 'jq' to use the chatgpt script.\"\n  exit\nfi\n\n# Installing imgcat if using iTerm\nif [[ \"$TERM_PROGRAM\" == \"iTerm.app\" ]]; then\n  if [[ ! $(which imgcat) ]]; then\n    curl -sS https://iterm2.com/utilities/imgcat -o /usr/local/bin/imgcat\n    chmod +x /usr/local/bin/imgcat\n    echo \"Installed imgcat\"\n  fi\nfi\n\n# Installing magick if using kitty\nif [[ \"$TERM\" == \"xterm-kitty\" ]]; then\n  if [[ ! $(which magick) ]]; then\n    curl -sS https://imagemagick.org/archive/binaries/magick -o /usr/local/bin/magick\n    chmod +x /usr/local/bin/magick\n    echo \"Installed magick\"\n  fi\nfi\n\n# Installing chatgpt script\ncurl -sS https://raw.githubusercontent.com/0xacx/chatGPT-shell-cli/main/chatgpt.sh -o /usr/local/bin/chatgpt\n\n# Replace open image command with xdg-open for linux systems\nif [[ \"$OSTYPE\" == \"linux\"* ]] || [[ \"$OSTYPE\" == \"freebsd\"* ]]; then\n  sed -i 's/open \"\\${image_url}\"/xdg-open \"\\${image_url}\"/g' '/usr/local/bin/chatgpt'\nfi\nchmod +x /usr/local/bin/chatgpt\necho \"Installed chatgpt script to /usr/local/bin/chatgpt\"\n\necho \"The script will add the OPENAI_KEY environment variable to your shell profile and add /usr/local/bin to your PATH\"\necho \"Would you like to continue? (Yes/No)\"\nread -e answer\nif [ \"$answer\" == \"Yes\" ] || [ \"$answer\" == \"yes\" ] || [ \"$answer\" == \"y\" ] || [ \"$answer\" == \"Y\" ] || [ \"$answer\" == \"ok\" ]; then\n\n  read -p \"Please enter your OpenAI API key: \" key\n\n  # Adding OpenAI key to shell profile\n  # zsh profile\n  if [ -f ~/.zprofile ]; then\n    echo \"export OPENAI_KEY=$key\" >>~/.zprofile\n    if [[ \":$PATH:\" != *\":/usr/local/bin:\"* ]]; then\n      echo 'export PATH=$PATH:/usr/local/bin' >>~/.zprofile\n    fi\n    echo \"OpenAI key and chatgpt path added to ~/.zprofile\"\n    source ~/.zprofile\n  # zshrc profile for debian\n  elif [ -f ~/.zshrc ]; then\n    echo \"export OPENAI_KEY=$key\" >>~/.zshrc\n    if [[ \":$PATH:\" == *\":/usr/local/bin:\"* ]]; then\n      echo 'export PATH=$PATH:/usr/local/bin' >>~/.zshrc\n    fi\n    echo \"OpenAI key and chatgpt path added to ~/.zshrc\"\n    source ~/.zshrc\n  # bash profile mac\n  elif [ -f ~/.bash_profile ]; then\n    echo \"export OPENAI_KEY=$key\" >>~/.bash_profile\n    if [[ \":$PATH:\" != *\":/usr/local/bin:\"* ]]; then\n      echo 'export PATH=$PATH:/usr/local/bin' >>~/.bash_profile\n    fi\n    echo \"OpenAI key and chatgpt path added to ~/.bash_profile\"\n    source ~/.bash_profile\n  # profile ubuntu\n  elif [ -f ~/.profile ]; then\n    echo \"export OPENAI_KEY=$key\" >>~/.profile\n    if [[ \":$PATH:\" != *\":/usr/local/bin:\"* ]]; then\n      echo 'export PATH=$PATH:/usr/local/bin' >>~/.profile\n    fi\n    echo \"OpenAI key and chatgpt path added to ~/.profile\"\n    source ~/.profile\n  else\n    export OPENAI_KEY=$key\n    echo \"You need to add this to your shell profile: export OPENAI_KEY=$key\"\n  fi\n  echo \"Installation complete\"\n\nelse\n  echo \"Please take a look at the instructions to install manually: https://github.com/0xacx/chatGPT-shell-cli/tree/main#manual-installation \"\n  exit\nfi\n"
        },
        {
          "name": "internal_dev",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}