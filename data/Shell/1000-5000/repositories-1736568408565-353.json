{
  "metadata": {
    "timestamp": 1736568408565,
    "page": 353,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjM1OQ==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "openstack/devstack",
      "stars": 2071,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.4697265625,
          "content": "*~\n.*.sw?\n*.log\n*-log\n*.log.*\n*-log.*\n*.pem\n*.pyc\n.localrc.auto\n.localrc.password\n.prereqs\n.tox\n.stackenv\naccrc\ndoc/files\ndoc/build\nfiles/*.gz\nfiles/*.vmdk\nfiles/*.rpm\nfiles/*.rpm.*\nfiles/*.deb\nfiles/*.deb.*\nfiles/*.qcow2\nfiles/*.img\nfiles/images\nfiles/pip-*\nfiles/get-pip.py*\nfiles/ir-deploy*\nfiles/ironic-inspector*\nfiles/etcd*\n/local.conf\nlocal.sh\nlocalrc\nproto\nshocco\nsrc\nstack-screenrc\nuserrc_early\nAUTHORS\nChangeLog\ntools/dbcounter/build/\ntools/dbcounter/dbcounter.egg-info/\n"
        },
        {
          "name": ".gitreview",
          "type": "blob",
          "size": 0.0732421875,
          "content": "[gerrit]\nhost=review.opendev.org\nport=29418\nproject=openstack/devstack.git\n"
        },
        {
          "name": ".mailmap",
          "type": "blob",
          "size": 0.33203125,
          "content": "# Format is:\n# <preferred e-mail> <other e-mail 1>\n# <preferred e-mail> <other e-mail 2>\nJiajun Liu <jiajun@unitedstack.com> <iamljj@gmail.com>\nJian Wen <jian.wen@canonical.com> <wenjianhn@gmail.com>\nJoe Gordon <joe.gordon0@gmail.com> <jogo@cloudscaling.com>\nSean Dague <sean.dague@samsung.com> <sdague@linux.vnet.ibm.com> <sean@dague.net>\n"
        },
        {
          "name": ".zuul.yaml",
          "type": "blob",
          "size": 30.7041015625,
          "content": "- nodeset:\n    name: openstack-single-node-jammy\n    nodes:\n      - name: controller\n        label: ubuntu-jammy\n    groups:\n      - name: tempest\n        nodes:\n          - controller\n\n- nodeset:\n    name: openstack-single-node-noble\n    nodes:\n      - name: controller\n        label: ubuntu-noble\n    groups:\n      - name: tempest\n        nodes:\n          - controller\n\n- nodeset:\n    name: openstack-single-node-focal\n    nodes:\n      - name: controller\n        label: ubuntu-focal\n    groups:\n      - name: tempest\n        nodes:\n          - controller\n\n- nodeset:\n    name: openstack-single-node-bionic\n    nodes:\n      - name: controller\n        label: ubuntu-bionic\n    groups:\n      - name: tempest\n        nodes:\n          - controller\n\n- nodeset:\n    name: devstack-single-node-centos-9-stream\n    nodes:\n      - name: controller\n        label: centos-9-stream\n    groups:\n      - name: tempest\n        nodes:\n          - controller\n\n- nodeset:\n    name: devstack-single-node-opensuse-15\n    nodes:\n      - name: controller\n        label: opensuse-15\n    groups:\n      - name: tempest\n        nodes:\n          - controller\n\n- nodeset:\n    name: devstack-single-node-debian-bookworm\n    nodes:\n      - name: controller\n        label: debian-bookworm\n    groups:\n      - name: tempest\n        nodes:\n          - controller\n\n# Note(sean-k-mooney): this is still used by horizon for\n# horizon-integration-tests, horizon-integration-pytest and\n# horizon-ui-pytest, remove when horizon is updated.\n- nodeset:\n    name: devstack-single-node-debian-bullseye\n    nodes:\n      - name: controller\n        label: debian-bullseye\n    groups:\n      - name: tempest\n        nodes:\n          - controller\n\n- nodeset:\n    name: devstack-single-node-rockylinux-9\n    nodes:\n      - name: controller\n        label: rockylinux-9\n    groups:\n      - name: tempest\n        nodes:\n          - controller\n\n- nodeset:\n    name: openstack-two-node-centos-9-stream\n    nodes:\n      - name: controller\n        label: centos-9-stream\n      - name: compute1\n        label: centos-9-stream\n    groups:\n      # Node where tests are executed and test results collected\n      - name: tempest\n        nodes:\n          - controller\n      # Nodes running the compute service\n      - name: compute\n        nodes:\n          - controller\n          - compute1\n      # Nodes that are not the controller\n      - name: subnode\n        nodes:\n          - compute1\n      # Switch node for multinode networking setup\n      - name: switch\n        nodes:\n          - controller\n      # Peer nodes for multinode networking setup\n      - name: peers\n        nodes:\n          - compute1\n\n- nodeset:\n    name: openstack-two-node-jammy\n    nodes:\n      - name: controller\n        label: ubuntu-jammy\n      - name: compute1\n        label: ubuntu-jammy\n    groups:\n      # Node where tests are executed and test results collected\n      - name: tempest\n        nodes:\n          - controller\n      # Nodes running the compute service\n      - name: compute\n        nodes:\n          - controller\n          - compute1\n      # Nodes that are not the controller\n      - name: subnode\n        nodes:\n          - compute1\n      # Switch node for multinode networking setup\n      - name: switch\n        nodes:\n          - controller\n      # Peer nodes for multinode networking setup\n      - name: peers\n        nodes:\n          - compute1\n\n- nodeset:\n    name: openstack-two-node-noble\n    nodes:\n      - name: controller\n        label: ubuntu-noble\n      - name: compute1\n        label: ubuntu-noble\n    groups:\n      # Node where tests are executed and test results collected\n      - name: tempest\n        nodes:\n          - controller\n      # Nodes running the compute service\n      - name: compute\n        nodes:\n          - controller\n          - compute1\n      # Nodes that are not the controller\n      - name: subnode\n        nodes:\n          - compute1\n      # Switch node for multinode networking setup\n      - name: switch\n        nodes:\n          - controller\n      # Peer nodes for multinode networking setup\n      - name: peers\n        nodes:\n          - compute1\n\n- nodeset:\n    name: openstack-two-node-focal\n    nodes:\n      - name: controller\n        label: ubuntu-focal\n      - name: compute1\n        label: ubuntu-focal\n    groups:\n      # Node where tests are executed and test results collected\n      - name: tempest\n        nodes:\n          - controller\n      # Nodes running the compute service\n      - name: compute\n        nodes:\n          - controller\n          - compute1\n      # Nodes that are not the controller\n      - name: subnode\n        nodes:\n          - compute1\n      # Switch node for multinode networking setup\n      - name: switch\n        nodes:\n          - controller\n      # Peer nodes for multinode networking setup\n      - name: peers\n        nodes:\n          - compute1\n\n- nodeset:\n    name: openstack-two-node-bionic\n    nodes:\n      - name: controller\n        label: ubuntu-bionic\n      - name: compute1\n        label: ubuntu-bionic\n    groups:\n      # Node where tests are executed and test results collected\n      - name: tempest\n        nodes:\n          - controller\n      # Nodes running the compute service\n      - name: compute\n        nodes:\n          - controller\n          - compute1\n      # Nodes that are not the controller\n      - name: subnode\n        nodes:\n          - compute1\n      # Switch node for multinode networking setup\n      - name: switch\n        nodes:\n          - controller\n      # Peer nodes for multinode networking setup\n      - name: peers\n        nodes:\n          - compute1\n\n- nodeset:\n    name: openstack-three-node-focal\n    nodes:\n      - name: controller\n        label: ubuntu-focal\n      - name: compute1\n        label: ubuntu-focal\n      - name: compute2\n        label: ubuntu-focal\n    groups:\n      # Node where tests are executed and test results collected\n      - name: tempest\n        nodes:\n          - controller\n      # Nodes running the compute service\n      - name: compute\n        nodes:\n          - controller\n          - compute1\n          - compute2\n      # Nodes that are not the controller\n      - name: subnode\n        nodes:\n          - compute1\n          - compute2\n      # Switch node for multinode networking setup\n      - name: switch\n        nodes:\n          - controller\n      # Peer nodes for multinode networking setup\n      - name: peers\n        nodes:\n          - compute1\n          - compute2\n\n- nodeset:\n    name: openstack-three-node-bionic\n    nodes:\n      - name: controller\n        label: ubuntu-bionic\n      - name: compute1\n        label: ubuntu-bionic\n      - name: compute2\n        label: ubuntu-bionic\n    groups:\n      # Node where tests are executed and test results collected\n      - name: tempest\n        nodes:\n          - controller\n      # Nodes running the compute service\n      - name: compute\n        nodes:\n          - controller\n          - compute1\n          - compute2\n      # Nodes that are not the controller\n      - name: subnode\n        nodes:\n          - compute1\n          - compute2\n      # Switch node for multinode networking setup\n      - name: switch\n        nodes:\n          - controller\n      # Peer nodes for multinode networking setup\n      - name: peers\n        nodes:\n          - compute1\n          - compute2\n\n- job:\n    name: devstack-base\n    parent: openstack-multinode-fips\n    abstract: true\n    description: |\n      Base abstract Devstack job.\n\n      Defines plays and base variables, but it does not include any project\n      and it does not run any service by default. This is a common base for\n      all single Devstack jobs, single or multinode.\n      Variables are defined in job.vars, which is what is then used by single\n      node jobs and by multi node jobs for the controller, as well as in\n      job.group-vars.peers, which is what is used by multi node jobs for subnode\n      nodes (everything but the controller).\n    required-projects:\n      - opendev.org/openstack/devstack\n    roles:\n      - zuul: opendev.org/openstack/openstack-zuul-jobs\n    vars:\n      devstack_localrc:\n        DATABASE_PASSWORD: secretdatabase\n        RABBIT_PASSWORD: secretrabbit\n        ADMIN_PASSWORD: secretadmin\n        SERVICE_PASSWORD: secretservice\n        NETWORK_GATEWAY: 10.1.0.1\n        FIXED_RANGE: 10.1.0.0/20\n        IPV4_ADDRS_SAFE_TO_USE: 10.1.0.0/20\n        FLOATING_RANGE: 172.24.5.0/24\n        PUBLIC_NETWORK_GATEWAY: 172.24.5.1\n        LOGFILE: /opt/stack/logs/devstacklog.txt\n        LOG_COLOR: false\n        VERBOSE: true\n        VERBOSE_NO_TIMESTAMP: true\n        NOVNC_FROM_PACKAGE: true\n        ERROR_ON_CLONE: true\n        # Gate jobs can't deal with nested virt. Disable it by default.\n        LIBVIRT_TYPE: '{{ devstack_libvirt_type | default(\"qemu\") }}'\n      devstack_services:\n        # Ignore any default set by devstack. Emit a \"disable_all_services\".\n        base: false\n      zuul_copy_output:\n        '{{ devstack_conf_dir }}/local.conf': logs\n        '{{ devstack_conf_dir }}/localrc': logs\n        '{{ devstack_conf_dir }}/.localrc.auto': logs\n        '{{ devstack_conf_dir }}/.stackenv': logs\n        '{{ devstack_log_dir }}/dstat-csv.log': logs\n        '{{ devstack_log_dir }}/devstacklog.txt': logs\n        '{{ devstack_log_dir }}/devstacklog.txt.summary': logs\n        '{{ devstack_log_dir }}/tcpdump.pcap': logs\n        '{{ devstack_log_dir }}/worlddump-latest.txt': logs\n        '{{ devstack_log_dir }}/qemu.coredump': logs\n        '{{ devstack_full_log}}': logs\n        '{{ stage_dir }}/verify_tempest_conf.log': logs\n        '{{ stage_dir }}/performance.json': logs\n        '{{ stage_dir }}/apache': logs\n        '{{ stage_dir }}/apache_config': logs\n        '{{ stage_dir }}/etc': logs\n        /var/log/rabbitmq: logs\n        /var/log/postgresql: logs\n        /var/log/mysql: logs\n        /var/log/libvirt: logs\n        /etc/libvirt: logs\n        /etc/lvm: logs\n        /etc/sudoers: logs\n        /etc/sudoers.d: logs\n        '{{ stage_dir }}/iptables.txt': logs\n        '{{ stage_dir }}/df.txt': logs\n        '{{ stage_dir }}/pip2-freeze.txt': logs\n        '{{ stage_dir }}/pip3-freeze.txt': logs\n        '{{ stage_dir }}/dpkg-l.txt': logs\n        '{{ stage_dir }}/rpm-qa.txt': logs\n        '{{ stage_dir }}/core': logs\n        '{{ stage_dir }}/listen53.txt': logs\n        '{{ stage_dir }}/services.txt': logs\n        '{{ stage_dir }}/deprecations.log': logs\n        '{{ stage_dir }}/audit.log': logs\n        /etc/ceph: logs\n        /var/log/ceph: logs\n        /var/log/openvswitch: logs\n        /var/log/glusterfs: logs\n        /etc/glusterfs/glusterd.vol: logs\n        /etc/resolv.conf: logs\n        /var/log/unbound.log: logs\n      extensions_to_txt:\n        conf: true\n        log: true\n        localrc: true\n        stackenv: true\n        auto: true\n    group-vars:\n      subnode:\n        devstack_localrc:\n          DATABASE_PASSWORD: secretdatabase\n          RABBIT_PASSWORD: secretrabbit\n          ADMIN_PASSWORD: secretadmin\n          SERVICE_PASSWORD: secretservice\n          NETWORK_GATEWAY: 10.1.0.1\n          FIXED_RANGE: 10.1.0.0/20\n          IPV4_ADDRS_SAFE_TO_USE: 10.1.0.0/20\n          FLOATING_RANGE: 172.24.5.0/24\n          PUBLIC_NETWORK_GATEWAY: 172.24.5.1\n          LOGFILE: /opt/stack/logs/devstacklog.txt\n          LOG_COLOR: false\n          VERBOSE: true\n          VERBOSE_NO_TIMESTAMP: true\n          NOVNC_FROM_PACKAGE: true\n          ERROR_ON_CLONE: true\n          LIBVIRT_TYPE: qemu\n        devstack_services:\n          base: false\n    pre-run: playbooks/pre.yaml\n    run: playbooks/devstack.yaml\n    post-run: playbooks/post.yaml\n    irrelevant-files:\n      # Documentation related\n      - ^.*\\.rst$\n      - ^api-ref/.*$\n      - ^doc/.*$\n      - ^releasenotes/.*$\n      # Translations\n      - ^.*/locale/.*po$\n      # pre-commit config\n      - ^.pre-commit-config.yaml$\n\n- job:\n    name: devstack-minimal\n    parent: devstack-base\n    description: |\n      Minimal devstack base job, intended for use by jobs that need\n      less than the normal minimum set of required-projects.\n    nodeset: openstack-single-node-noble\n    required-projects:\n      - opendev.org/openstack/requirements\n    vars:\n      devstack_localrc:\n        # Multinode specific settings\n        SERVICE_HOST: \"{{ hostvars['controller']['nodepool']['private_ipv4'] }}\"\n        HOST_IP: \"{{ hostvars['controller']['nodepool']['private_ipv4'] }}\"\n        PUBLIC_BRIDGE_MTU: '{{ external_bridge_mtu }}'\n      devstack_services:\n        # Shared services\n        dstat: false\n        etcd3: true\n        memory_tracker: true\n        file_tracker: true\n        mysql: true\n        rabbit: true\n        openstack-cli-server: true\n    group-vars:\n      subnode:\n        devstack_services:\n          # Shared services\n          dstat: false\n          memory_tracker: true\n          file_tracker: true\n          openstack-cli-server: true\n        devstack_localrc:\n          # Multinode specific settings\n          HOST_IP: \"{{ hostvars[inventory_hostname]['nodepool']['private_ipv4'] }}\"\n          SERVICE_HOST: \"{{ hostvars['controller']['nodepool']['private_ipv4'] }}\"\n          PUBLIC_BRIDGE_MTU: '{{ external_bridge_mtu }}'\n          # Subnode specific settings\n          DATABASE_TYPE: mysql\n          RABBIT_HOST: \"{{ hostvars['controller']['nodepool']['private_ipv4'] }}\"\n          DATABASE_HOST: \"{{ hostvars['controller']['nodepool']['private_ipv4'] }}\"\n\n\n- job:\n    name: devstack\n    parent: devstack-minimal\n    description: |\n      Base devstack job for integration gate.\n\n      This base job can be used for single node and multinode devstack jobs.\n\n      With a single node nodeset, this job sets up an \"all-in-one\" (aio)\n      devstack with the seven OpenStack services included in the devstack tree:\n      keystone, glance, cinder, neutron, nova, placement, and swift.\n\n      With a two node nodeset, this job sets up an aio + compute node.\n      The controller can be customised using host-vars.controller, the\n      sub-nodes can be customised using group-vars.subnode.\n\n      Descendent jobs can enable / disable services, add devstack configuration\n      options, enable devstack plugins, configure log files or directories to be\n      transferred to the log server.\n\n      The job assumes that there is only one controller node. The number of\n      subnodes can be scaled up seamlessly by setting a custom nodeset in\n      job.nodeset.\n\n      The run playbook consists of a single role, so it can be easily rewritten\n      and extended.\n    required-projects:\n      - opendev.org/openstack/cinder\n      - opendev.org/openstack/glance\n      - opendev.org/openstack/keystone\n      - opendev.org/openstack/neutron\n      - opendev.org/openstack/nova\n      - opendev.org/openstack/placement\n      - opendev.org/openstack/swift\n      - opendev.org/openstack/os-test-images\n    timeout: 7200\n    vars:\n      # based on observation of the integrated gate\n      # tempest-integrated-compute was only using ~1.7GB of swap\n      # when zswap and the host turning are enabled that increase\n      # slightly to ~2GB. we are setting the swap size to 8GB to\n      # be safe and account for more complex scenarios.\n      # we should revisit this value after some time to see if we\n      # can reduce it.\n      configure_swap_size: 8192\n      devstack_localrc:\n        # Common OpenStack services settings\n        SWIFT_REPLICAS: 1\n        SWIFT_START_ALL_SERVICES: false\n        SWIFT_HASH: 1234123412341234\n        DEBUG_LIBVIRT_COREDUMPS: true\n        NOVA_VNC_ENABLED: true\n        OVN_DBS_LOG_LEVEL: dbg\n        # tune the host to optimize memory usage and hide io latency\n        # these setting will configure the kernel to treat the host page\n        # cache and swap with equal priority, and prefer deferring writes\n        # changing the default swappiness, dirty_ratio and\n        # the vfs_cache_pressure\n        ENABLE_SYSCTL_MEM_TUNING: true\n        # the net tuning optimizes ipv4 tcp fast open and config the default\n        # qdisk policy to pfifo_fast which effectively disable all qos.\n        # this minimizes the cpu load of the host network stack\n        ENABLE_SYSCTL_NET_TUNING: true\n        # zswap allows the kernel to compress pages in memory before swapping\n        # them to disk. this can reduce the amount of swap used and improve\n        # performance. effectively this trades a small amount of cpu for an\n        # increase in swap performance by reducing the amount of data\n        # written to disk. the overall speedup is proportional to the\n        # compression ratio and the speed of the swap device.\n        # NOTE: this option is ignored when not using nova with the libvirt\n        # virt driver.\n        NOVA_LIBVIRT_TB_CACHE_SIZE: 128\n        ENABLE_ZSWAP: true\n      devstack_local_conf:\n        post-config:\n          $NEUTRON_CONF:\n            DEFAULT:\n              global_physnet_mtu: '{{ external_bridge_mtu }}'\n      devstack_services:\n        # Core services enabled for this branch.\n        # This list replaces the test-matrix.\n        # Shared services\n        dstat: false\n        etcd3: true\n        memory_tracker: true\n        file_tracker: true\n        mysql: true\n        rabbit: true\n        tls-proxy: true\n        # Keystone services\n        key: true\n        # Glance services\n        g-api: true\n        # Nova services\n        n-api: true\n        n-api-meta: true\n        n-cond: true\n        n-cpu: true\n        n-novnc: true\n        n-sch: true\n        # Placement service\n        placement-api: true\n        # OVN services\n        ovn-controller: true\n        ovn-northd: true\n        ovs-vswitchd: true\n        ovsdb-server: true\n        # Neutron services\n        q-svc: true\n        q-ovn-metadata-agent: true\n        # Swift services\n        s-account: true\n        s-container: true\n        s-object: true\n        s-proxy: true\n        # Cinder services\n        c-api: true\n        c-bak: true\n        c-sch: true\n        c-vol: true\n        # Services we don't need.\n        # This section is not really needed, it's for readability.\n        horizon: false\n        tempest: false\n        # Test matrix emits ceilometer but ceilomenter is not installed in the\n        # integrated gate, so specifying the services has not effect.\n        # ceilometer-*: false\n    group-vars:\n      subnode:\n        devstack_services:\n          # Core services enabled for this branch.\n          # This list replaces the test-matrix.\n          # Shared services\n          dstat: false\n          memory_tracker: true\n          file_tracker: true\n          tls-proxy: true\n          # Nova services\n          n-cpu: true\n          # Placement services\n          placement-client: true\n          # OVN services\n          ovn-controller: true\n          ovs-vswitchd: true\n          ovsdb-server: true\n          # Neutron services\n          q-ovn-metadata-agent: true\n          # Cinder services\n          c-bak: true\n          c-vol: true\n          # Services we don't run at all on subnode.\n          # This section is not really needed, it's for readability.\n          # keystone: false\n          # s-*: false\n          horizon: false\n          tempest: false\n          # Test matrix emits ceilometer but ceilometer is not installed in the\n          # integrated gate, so specifying the services has not effect.\n          # ceilometer-*: false\n        devstack_localrc:\n          # Subnode specific settings\n          GLANCE_HOSTPORT: \"{{ hostvars['controller']['nodepool']['private_ipv4'] }}:9292\"\n          Q_HOST: \"{{ hostvars['controller']['nodepool']['private_ipv4'] }}\"\n          NOVA_VNC_ENABLED: true\n          ENABLE_CHASSIS_AS_GW: false\n          # tune the host to optimize memory usage and hide io latency\n          # these setting will configure the kernel to treat the host page\n          # cache and swap with equal priority, and prefer deferring writes\n          # changing the default swappiness, dirty_ratio and\n          # the vfs_cache_pressure\n          ENABLE_SYSCTL_MEM_TUNING: true\n          # the net tuning optimizes ipv4 tcp fast open and config the default\n          # qdisk policy to pfifo_fast which effectively disable all qos.\n          # this minimizes the cpu load of the host network stack\n          ENABLE_SYSCTL_NET_TUNING: true\n          # zswap allows the kernel to compress pages in memory before swapping\n          # them to disk. this can reduce the amount of swap used and improve\n          # performance. effectivly this trades a small amount of cpu for an\n          # increase in swap performance by reducing the amount of data\n          # written to disk. the overall speedup is porportional to the\n          # compression ratio and the speed of the swap device.\n          ENABLE_ZSWAP: true\n          # NOTE: this option is ignored when not using nova with the libvirt\n          # virt driver.\n          NOVA_LIBVIRT_TB_CACHE_SIZE: 128\n\n- job:\n    name: devstack-ipv6\n    parent: devstack\n    description: |\n      Devstack single node job for integration gate with IPv6,\n      all services and tunnels using IPv6 addresses.\n    vars:\n      devstack_localrc:\n        SERVICE_IP_VERSION: 6\n        SERVICE_HOST: \"\"\n        TUNNEL_IP_VERSION: 6\n\n- job:\n    name: devstack-enforce-scope\n    parent: devstack\n    description: |\n      This job runs the devstack with scope checks enabled.\n    vars:\n      devstack_localrc:\n        ENFORCE_SCOPE: true\n\n- job:\n    name: devstack-multinode\n    parent: devstack\n    nodeset: openstack-two-node-noble\n    description: |\n      Simple multinode test to verify multinode functionality on devstack side.\n      This is not meant to be used as a parent job.\n\n# NOTE(ianw) Platform tests have traditionally been non-voting because\n# we often have to rush things through devstack to stabilise the gate,\n# and these platforms don't have the round-the-clock support to avoid\n# becoming blockers in that situation.\n- job:\n    name: devstack-platform-centos-9-stream\n    parent: tempest-full-py3\n    description: CentOS 9 Stream platform test\n    nodeset: devstack-single-node-centos-9-stream\n    timeout: 9000\n    voting: false\n\n- job:\n    name: devstack-platform-debian-bookworm\n    parent: tempest-full-py3\n    description: Debian Bookworm platform test\n    nodeset: devstack-single-node-debian-bookworm\n    timeout: 9000\n    vars:\n      configure_swap_size: 4096\n\n- job:\n    name: devstack-platform-rocky-blue-onyx\n    parent: tempest-full-py3\n    description: Rocky Linux 9 Blue Onyx platform test\n    nodeset: devstack-single-node-rockylinux-9\n    timeout: 9000\n    # NOTE(danms): This has been failing lately with some repository metadata\n    # errors. We're marking this as non-voting until it appears to have\n    # stabilized:\n    # https://zuul.openstack.org/builds?job_name=devstack-platform-rocky-blue-onyx&skip=0\n    voting: false\n    vars:\n      configure_swap_size: 4096\n\n- job:\n    name: devstack-platform-ubuntu-jammy\n    parent: tempest-full-py3\n    description: Ubuntu 22.04 LTS (Jammy) platform test\n    nodeset: openstack-single-node-jammy\n    timeout: 9000\n    vars:\n      configure_swap_size: 8192\n\n- job:\n    name: devstack-platform-ubuntu-noble-ovn-source\n    parent: devstack-platform-ubuntu-noble\n    description: Ubuntu 24.04 LTS (noble) platform test (OVN from source)\n    voting: false\n    vars:\n      devstack_localrc:\n        OVN_BUILD_FROM_SOURCE: True\n        OVN_BRANCH: \"v21.06.0\"\n        OVS_BRANCH: \"a4b04276ab5934d087669ff2d191a23931335c87\"\n        OVS_SYSCONFDIR: \"/usr/local/etc/openvswitch\"\n\n- job:\n    name: devstack-platform-ubuntu-noble-ovs\n    parent: tempest-full-py3\n    description: Ubuntu 24.04 LTS (noble) platform test (OVS)\n    nodeset: openstack-single-node-noble\n    voting: false\n    timeout: 9000\n    vars:\n      configure_swap_size: 8192\n      devstack_localrc:\n        Q_AGENT: openvswitch\n        Q_ML2_PLUGIN_MECHANISM_DRIVERS: openvswitch\n        Q_ML2_TENANT_NETWORK_TYPE: vxlan\n      devstack_services:\n        # Disable OVN services\n        ovn-northd: false\n        ovn-controller: false\n        ovs-vswitchd: false\n        ovsdb-server: false\n        # Disable Neutron ML2/OVN services\n        q-ovn-metadata-agent: false\n        # Enable Neutron ML2/OVS services\n        q-agt: true\n        q-dhcp: true\n        q-l3: true\n        q-meta: true\n        q-metering: true\n    group-vars:\n      subnode:\n        devstack_services:\n          # Disable OVN services\n          ovn-controller: false\n          ovs-vswitchd: false\n          ovsdb-server: false\n          # Disable Neutron ML2/OVN services\n          q-ovn-metadata-agent: false\n          # Enable Neutron ML2/OVS services\n          q-agt: true\n\n- job:\n    name: devstack-no-tls-proxy\n    parent: tempest-full-py3\n    description: |\n      Tempest job with tls-proxy off.\n\n      Some gates run devstack like this and it follows different code paths.\n    vars:\n      devstack_services:\n        tls-proxy: false\n\n- job:\n    name: devstack-tox-base\n    parent: devstack\n    description: |\n      Base job for devstack-based functional tests that use tox.\n\n      This job is not intended to be run directly. It's just here\n      for organizational purposes for devstack-tox-functional and\n      devstack-tox-functional-consumer.\n    post-run: playbooks/tox/post.yaml\n    vars:\n      tox_envlist: functional\n      tox_install_siblings: false\n\n- job:\n    name: devstack-tox-functional\n    parent: devstack-tox-base\n    description: |\n      Base job for devstack-based functional tests that use tox.\n\n      Runs devstack, then runs the tox ``functional`` environment,\n      then collects tox/testr build output like normal tox jobs.\n\n      Turns off tox sibling installation. Projects may be involved\n      in the devstack deployment and so may be in the required-projects\n      list, but may not want to test against master of the other\n      projects in their tox env. Child jobs can set tox_install_siblings\n      to True to re-enable sibling processing.\n    run: playbooks/tox/run-both.yaml\n\n- job:\n    name: devstack-tox-functional-consumer\n    parent: devstack\n    description: |\n      Base job for devstack-based functional tests for projects that\n      consume the devstack cloud.\n\n      This base job should only be used by projects that are not involved\n      in the devstack deployment step, but are instead projects that are using\n      devstack to get a cloud against which they can test things.\n\n      Runs devstack in pre-run, then runs the tox ``functional`` environment,\n      then collects tox/testr build output like normal tox jobs.\n\n      Turns off tox sibling installation. Projects may be involved\n      in the devstack deployment and so may be in the required-projects\n      list, but may not want to test against master of the other\n      projects in their tox env. Child jobs can set tox_install_siblings\n      to True to re-enable sibling processing.\n    pre-run:\n      - playbooks/devstack.yaml\n      - playbooks/tox/pre.yaml\n    run: playbooks/tox/run.yaml\n\n- job:\n    name: devstack-unit-tests\n    nodeset: ubuntu-noble\n    description: |\n      Runs unit tests on devstack project.\n\n      It runs  ``run_tests.sh``.\n    pre-run: playbooks/unit-tests/pre.yaml\n    run: playbooks/unit-tests/run.yaml\n\n- project:\n    templates:\n      - integrated-gate-py3\n      - publish-openstack-docs-pti\n    check:\n      jobs:\n        - devstack\n        - devstack-ipv6\n        - devstack-enforce-scope\n        - devstack-platform-centos-9-stream\n        - devstack-platform-debian-bookworm\n        - devstack-platform-rocky-blue-onyx\n        - devstack-platform-ubuntu-noble-ovn-source\n        - devstack-platform-ubuntu-noble-ovs\n        - devstack-platform-ubuntu-jammy\n        - devstack-multinode\n        - devstack-unit-tests\n        - openstack-tox-bashate\n        - ironic-tempest-bios-ipmi-direct-tinyipa\n        - swift-dsvm-functional\n        - grenade:\n            irrelevant-files:\n              - ^.*\\.rst$\n              - ^doc/.*$\n        - neutron-ovs-grenade-multinode:\n            irrelevant-files:\n              - ^.*\\.rst$\n              - ^doc/.*$\n        - neutron-ovn-tempest-ovs-release:\n            voting: false\n            irrelevant-files:\n              - ^.*\\.rst$\n              - ^doc/.*$\n        - tempest-multinode-full-py3:\n            voting: false\n            irrelevant-files:\n              - ^.*\\.rst$\n              - ^doc/.*$\n        - openstacksdk-functional-devstack:\n            irrelevant-files:\n              - ^.*\\.rst$\n              - ^doc/.*$\n        - tempest-ipv6-only:\n            irrelevant-files:\n              - ^.*\\.rst$\n              - ^doc/.*$\n        - nova-ceph-multistore:\n            irrelevant-files:\n              - ^.*\\.rst$\n              - ^doc/.*$\n    gate:\n      jobs:\n        - devstack\n        - devstack-ipv6\n        - devstack-platform-debian-bookworm\n        - devstack-platform-ubuntu-noble\n        # NOTE(danms): Disabled due to instability, see comment in the job\n        # definition above.\n        # - devstack-platform-rocky-blue-onyx\n        - devstack-enforce-scope\n        - devstack-multinode\n        - devstack-unit-tests\n        - openstack-tox-bashate\n        - neutron-ovs-grenade-multinode:\n            irrelevant-files:\n              - ^.*\\.rst$\n              - ^doc/.*$\n        - ironic-tempest-bios-ipmi-direct-tinyipa\n        - swift-dsvm-functional\n        - grenade:\n            irrelevant-files:\n              - ^.*\\.rst$\n              - ^doc/.*$\n        - openstacksdk-functional-devstack:\n            irrelevant-files:\n              - ^.*\\.rst$\n              - ^doc/.*$\n        - tempest-ipv6-only:\n            irrelevant-files:\n              - ^.*\\.rst$\n              - ^doc/.*$\n        - nova-ceph-multistore:\n            irrelevant-files:\n              - ^.*\\.rst$\n              - ^doc/.*$\n    # Please add a note on each job and conditions for the job not\n    # being experimental any more, so we can keep this list somewhat\n    # pruned.\n    #\n    # * nova-next: maintained by nova for unreleased/undefaulted\n    #    things, this job is not experimental but often is used to test\n    #    things that are not yet production ready or to test what will be\n    #    the new default after a deprecation period has ended.\n    # * nova-multi-cell: maintained by nova and now is voting in the\n    #    check queue for nova changes but relies on devstack configuration\n\n    experimental:\n      jobs:\n        - nova-multi-cell\n        - nova-next\n        - devstack-plugin-ceph-tempest-py3:\n            irrelevant-files:\n              - ^.*\\.rst$\n              - ^doc/.*$\n        - neutron-ovs-tempest-dvr:\n            irrelevant-files:\n              - ^.*\\.rst$\n              - ^doc/.*$\n        - neutron-ovs-tempest-dvr-ha-multinode-full:\n            irrelevant-files:\n              - ^.*\\.rst$\n              - ^doc/.*$\n        - cinder-tempest-lvm-multibackend:\n            irrelevant-files:\n              - ^.*\\.rst$\n              - ^doc/.*$\n        - tempest-pg-full:\n            irrelevant-files:\n              - ^.*\\.rst$\n              - ^doc/.*$\n        - devstack-no-tls-proxy\n    periodic:\n      jobs:\n        - devstack-no-tls-proxy\n    periodic-weekly:\n      jobs:\n        - devstack-platform-centos-9-stream\n        - devstack-platform-debian-bookworm\n        - devstack-platform-rocky-blue-onyx\n        - devstack-platform-ubuntu-noble-ovn-source\n        - devstack-platform-ubuntu-noble-ovs\n        - devstack-platform-ubuntu-jammy\n"
        },
        {
          "name": "CONTRIBUTING.rst",
          "type": "blob",
          "size": 0.5927734375,
          "content": "The source repository for this project can be found at:\n\n   https://opendev.org/openstack/devstack\n\nPull requests submitted through GitHub are not monitored.\n\nTo start contributing to OpenStack, follow the steps in the contribution guide\nto set up and use Gerrit:\n\n   https://docs.openstack.org/contributors/code-and-documentation/quick-start.html\n\nBugs should be filed on Launchpad:\n\n   https://bugs.launchpad.net/devstack\n\nFor more specific information about contributing to this repository, see the\nDevstack contributor guide:\n\n   https://docs.openstack.org/devstack/latest/contributor/contributing.html\n"
        },
        {
          "name": "FUTURE.rst",
          "type": "blob",
          "size": 3.685546875,
          "content": "=============\n Quo Vadimus\n=============\n\nWhere are we going?\n\nThis is a document in Devstack to outline where we are headed in the\nfuture. The future might be near or far, but this is where we'd like\nto be.\n\nThis is intended to help people contribute, because it will be a\nlittle clearer if a contribution takes us closer to or further away to\nour end game.\n\n==================\n Default Services\n==================\n\nDevstack is designed as a development environment first. There are a\nlot of ways to compose the OpenStack services, but we do need one\ndefault.\n\nThat should be the Compute Layer (currently Glance + Nova + Cinder +\nNeutron Core (not advanced services) + Keystone). It should be the\nbase building block going forward, and the introduction point of\npeople to OpenStack via Devstack.\n\n================\n Service Howtos\n================\n\nStarting from the base building block all services included in\nOpenStack should have an overview page in the Devstack\ndocumentation. That should include the following:\n\n- A helpful high level overview of that service\n- What it depends on (both other OpenStack services and other system\n  components)\n- What new daemons are needed to be started, including where they\n  should live\n\nThis provides a map for people doing multinode testing to understand\nwhat portions are control plane, which should live on worker nodes.\n\nService how to pages will start with an ugly \"This team has provided\nno information about this service\" until someone does.\n\n===================\n Included Services\n===================\n\nDevstack doesn't need to eat the world. Given the existence of the\nexternal devstack plugin architecture, the future direction is to move\nthe bulk of the support code out of devstack itself and into external\nplugins.\n\nThis will also promote a more clean separation between services.\n\n=============================\n Included Backends / Drivers\n=============================\n\nUpstream Devstack should only include Open Source backends / drivers,\nit's intent is for Open Source development of OpenStack. Proprietary\ndrivers should be supported via external plugins.\n\nJust being Open Source doesn't mean it should be in upstream Devstack\nif it's not required for base development of OpenStack\ncomponents. When in doubt, external plugins should be used.\n\n========================================\n OpenStack Services vs. System Services\n========================================\n\nENABLED_SERVICES is currently entirely too overloaded. We should have\na separation of actual OpenStack services that you have to run (n-cpu,\ng-api) and required backends like mysql and rabbitmq.\n\n===========================\n Splitting up of Functions\n===========================\n\nThe functions-common file has grown over time, and needs to be split\nup into smaller libraries that handle specific domains.\n\n======================\n Testing of Functions\n======================\n\nEvery function in a functions file should get tests. The devstack\ntesting framework is young, but we do have some unit tests for the\ntree, and those should be enhanced.\n\n==============================\n Not Co-Gating with the World\n==============================\n\nAs projects spin up functional test jobs, Devstack should not be\nco-gated with every single one of those. The Devstack team has one of\nthe fastest turn arounds for blocking bugs of any Open Stack\nproject.\n\nBasic service validation should be included as part of Devstack\ninstallation to mitigate this.\n\n============================\n Documenting all the things\n============================\n\nDevstack started off as an explanation as much as an install\nscript. We would love contributions to that further enhance the\ncomments and explanations about what is happening, even if it seems a\nlittle pedantic at times.\n"
        },
        {
          "name": "HACKING.rst",
          "type": "blob",
          "size": 11.5224609375,
          "content": "Contributing to DevStack\n========================\n\n\nGeneral\n-------\n\nDevStack is written in UNIX shell script.  It uses a number of bash-isms\nand so is limited to Bash (version 4 and up) and compatible shells.\nShell script was chosen because it best illustrates the steps used to\nset up and interact with OpenStack components.\n\nDevStack's official repository is located on opendev.org at\nhttps://opendev.org/openstack/devstack.  Besides the master branch that\ntracks the OpenStack trunk branches a separate branch is maintained for all\nOpenStack releases starting with Diablo (stable/diablo).\n\nContributing code to DevStack follows the usual OpenStack process as described\nin `How To Contribute`__ in the OpenStack wiki.  `DevStack's LaunchPad project`__\ncontains the usual links for blueprints, bugs, etc.\n\n__ contribute_\n.. _contribute: https://docs.openstack.org/infra/manual/developers.html\n\n__ lp_\n.. _lp: https://launchpad.net/devstack\n\nThe `Gerrit review\nqueue <https://review.opendev.org/#/q/project:openstack/devstack>`__\nis used for all commits.\n\nThe primary script in DevStack is ``stack.sh``, which performs the bulk of the\nwork for DevStack's use cases.  There is a subscript ``functions`` that contains\ngenerally useful shell functions and is used by a number of the scripts in\nDevStack.\n\nA number of additional scripts can be found in the ``tools`` directory that may\nbe useful in supporting DevStack installations.  Of particular note are ``info.sh``\nto collect and report information about the installed system, and ``install_prereqs.sh``\nthat handles installation of the prerequisite packages for DevStack.  It is\nsuitable, for example, to pre-load a system for making a snapshot.\n\nRepo Layout\n-----------\n\nThe DevStack repo generally keeps all of the primary scripts at the root\nlevel.\n\n``doc`` - Contains the Sphinx source for the documentation.\nA complete doc build can be run with ``tox -edocs``.\n\n``extras.d`` - Contains the dispatch scripts called by the hooks in\n``stack.sh``, ``unstack.sh`` and ``clean.sh``. See :doc:`the plugins\ndocs <plugins>` for more information.\n\n``files`` - Contains a variety of otherwise lost files used in\nconfiguring and operating DevStack. This includes templates for\nconfiguration files and the system dependency information. This is also\nwhere image files are downloaded and expanded if necessary.\n\n``lib`` - Contains the sub-scripts specific to each project. This is\nwhere the work of managing a project's services is located. Each\ntop-level project (Keystone, Nova, etc) has a file here. Additionally\nthere are some for system services and project plugins.  These\nvariables and functions are also used by related projects, such as\nGrenade, to manage a DevStack installation.\n\n``samples`` - Contains a sample of the local files not included in the\nDevStack repo.\n\n``tests`` - the DevStack test suite is rather sparse, mostly consisting\nof test of specific fragile functions in the ``functions`` and\n``functions-common`` files.\n\n``tools`` - Contains a collection of stand-alone scripts. While these\nmay reference the top-level DevStack configuration they can generally be\nrun alone.\n\n\nScripts\n-------\n\nDevStack scripts should generally begin by calling ``env(1)`` in the shebang line::\n\n    #!/usr/bin/env bash\n\nSometimes the script needs to know the location of the DevStack install directory.\n``TOP_DIR`` should always point there, even if the script itself is located in\na subdirectory::\n\n    # Keep track of the current DevStack directory.\n    TOP_DIR=$(cd $(dirname \"$0\") && pwd)\n\nMany scripts will utilize shared functions from the ``functions`` file.  There are\nalso rc files (``stackrc`` and ``openrc``) that are often included to set the primary\nconfiguration of the user environment::\n\n    # Keep track of the current DevStack directory.\n    TOP_DIR=$(cd $(dirname \"$0\") && pwd)\n\n    # Import common functions\n    source $TOP_DIR/functions\n\n    # Import configuration\n    source $TOP_DIR/openrc\n\n``stack.sh`` is a rather large monolithic script that flows through from beginning\nto end.  It has been broken down into project-specific subscripts (as noted above)\nlocated in ``lib`` to make ``stack.sh`` more manageable and to promote code reuse.\n\nThese library sub-scripts have a number of fixed entry points, some of which may\njust be stubs.  These entry points will be called by ``stack.sh`` in the\nfollowing order::\n\n    install_XXXX\n    configure_XXXX\n    init_XXXX\n    start_XXXX\n    stop_XXXX\n    cleanup_XXXX\n\nThere is a sub-script template in ``lib/templates`` to be used in creating new\nservice sub-scripts.  The comments in ``<>`` are meta comments describing\nhow to use the template and should be removed.\n\nIn order to show the dependencies and conditions under which project functions\nare executed the top-level conditional testing for things like ``is_service_enabled``\nshould be done in ``stack.sh``.  There may be nested conditionals that need\nto be in the sub-script, such as testing for keystone being enabled in\n``configure_swift()``.\n\n\nstackrc\n-------\n\n``stackrc`` is the global configuration file for DevStack.  It is responsible for\ncalling ``local.conf`` (or ``localrc`` if it exists) so local user configuration\nis recognized.\n\nThe criteria for what belongs in ``stackrc`` can be vaguely summarized as\nfollows:\n\n* All project repositories and branches handled directly in ``stack.sh``\n* Global configuration that may be referenced in ``local.conf``, i.e. ``DEST``, ``DATA_DIR``\n* Global service configuration like ``ENABLED_SERVICES``\n* Variables used by multiple services that do not have a clear owner, i.e.\n  ``VOLUME_BACKING_FILE_SIZE`` (nova-compute and cinder) or\n  ``PUBLIC_NETWORK_NAME`` (only neutron but formerly nova-network too)\n* Variables that can not be cleanly declared in a project file due to\n  dependency ordering, i.e. the order of sourcing the project files can\n  not be changed for other reasons but the earlier file needs to dereference a\n  variable set in the later file.  This should be rare.\n\nAlso, variable declarations in ``stackrc`` before ``local.conf`` is sourced\ndo NOT allow overriding (the form\n``FOO=${FOO:-baz}``); if they did then they can already be changed in ``local.conf``\nand can stay in the project file.\n\n\nDocumentation\n-------------\n\nThe DevStack repo now contains all of the static pages of devstack.org in\nthe ``doc/source`` directory. The OpenStack CI system rebuilds the docs after every\ncommit and updates devstack.org (now a redirect to https://docs.openstack.org/devstack/latest/).\n\nAll of the scripts are processed with shocco_ to render them with the comments\nas text describing the script below.  For this reason we tend to be a little\nverbose in the comments _ABOVE_ the code they pertain to.  Shocco also supports\nMarkdown formatting in the comments; use it sparingly.  Specifically, ``stack.sh``\nuses Markdown headers to divide the script into logical sections.\n\n.. _shocco: https://github.com/dtroyer/shocco/tree/rst_support\n\nThe script used to drive <code>shocco</code> is <code>tools/build_docs.sh</code>.\nThe complete docs build is also handled with <code>tox -edocs</code> per the\nOpenStack project standard.\n\n\nBash Style Guidelines\n~~~~~~~~~~~~~~~~~~~~~\nDevStack defines a bash set of best practices for maintaining large\ncollections of bash scripts. These should be considered as part of the\nreview process.\n\nDevStack uses the bashate_ style checker\nto enforce basic guidelines, similar to pep8 and flake8 tools for Python. The\nlist below is not complete for what bashate checks, nor is it all checked\nby bashate.  So many lines of code, so little time.\n\n.. _bashate: https://pypi.org/project/bashate/\n\nWhitespace Rules\n----------------\n\n- lines should not include trailing whitespace\n- there should be no hard tabs in the file\n- indents are 4 spaces, and all indentation should be some multiple of\n  them\n\nControl Structure Rules\n-----------------------\n\n- then should be on the same line as the if\n- do should be on the same line as the for\n\nExample::\n\n  if [[ -r $TOP_DIR/local.conf ]]; then\n      LRC=$(get_meta_section_files $TOP_DIR/local.conf local)\n      for lfile in $LRC; do\n          if [[ \"$lfile\" == \"localrc\" ]]; then\n              if [[ -r $TOP_DIR/localrc ]]; then\n                  warn $LINENO \"localrc and local.conf:[[local]] both exist, using localrc\"\n              else\n                  echo \"# Generated file, do not edit\" >$TOP_DIR/.localrc.auto\n                  get_meta_section $TOP_DIR/local.conf local $lfile >>$TOP_DIR/.localrc.auto\n              fi\n          fi\n      done\n  fi\n\nVariables and Functions\n-----------------------\n\n- functions should be used whenever possible for clarity\n- functions should use ``local`` variables as much as possible to\n  ensure they are isolated from the rest of the environment\n- local variables should be lower case, global variables should be\n  upper case\n- function names should_have_underscores, NotCamelCase.\n- functions should be declared as per the regex ^function foo {$\n  with code starting on the next line\n\n\nReview Criteria\n---------------\n\nThere are some broad criteria that will be followed when reviewing\nyour change\n\n* **Is it passing tests** -- your change will not be reviewed\n  thoroughly unless the official CI has run successfully against it.\n\n* **Does this belong in DevStack** -- DevStack reviewers have a\n  default position of \"no\" but are ready to be convinced by your\n  change.\n\n  For very large changes, you should consider :doc:`the plugins system\n  <plugins>` to see if your code is better abstracted from the main\n  repository.\n\n  For smaller changes, you should always consider if the change can be\n  encapsulated by per-user settings in ``local.conf``.  A common example\n  is adding a simple config-option to an ``ini`` file.  Specific flags\n  are not usually required for this, although adding documentation\n  about how to achieve a larger goal (which might include turning on\n  various settings, etc) is always welcome.\n\n* **Work-arounds** -- often things get broken and DevStack can be in a\n  position to fix them.  Work-arounds are fine, but should be\n  presented in the context of fixing the root-cause of the problem.\n  This means it is well-commented in the code and the change-log and\n  mostly likely includes links to changes or bugs that fix the\n  underlying problem.\n\n* **Should this be upstream** -- DevStack generally does not override\n  default choices provided by projects and attempts to not\n  unexpectedly modify behavior.\n\n* **Context in commit messages** -- DevStack touches many different\n  areas and reviewers need context around changes to make good\n  decisions.  We also always want it to be clear to someone -- perhaps\n  even years from now -- why we were motivated to make a change at the\n  time.\n\n\nMaking Changes, Testing, and CI\n-------------------------------\n\nChanges to Devstack are tested by automated continuous integration jobs\nthat run on a variety of Linux Distros using a handful of common\nconfigurations. What this means is that every change to Devstack is\nself testing. One major benefit of this is that developers do not\ntypically need to add new non voting test jobs to add features to\nDevstack. Instead the features can be added, then if testing passes\nwith the feature enabled the change is ready to merge (pending code\nreview).\n\nA concrete example of this was the switch from screen based service\nmanagement to systemd based service management. No new jobs were\ncreated for this. Instead the features were added to devstack, tested\nlocally and in CI using a change that enabled the feature, then once\nthe enabling change was passing and the new behavior communicated and\ndocumented it was merged.\n\nUsing this process has been proven to be effective and leads to\nquicker implementation of desired features.\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 9.9052734375,
          "content": "\n                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n"
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 2.431640625,
          "content": "# DevStack Makefile of Sanity\n\n# Interesting targets:\n# ds-remote - Create a Git remote for use by ds-push and ds-pull targets\n#             DS_REMOTE_URL must be set on the command line\n#\n# ds-push - Merge a list of branches taken from .ds-test and push them\n#           to the ds-remote repo in ds-test branch\n#\n# ds-pull - Pull the remote ds-test branch into a fresh local branch\n#\n# refresh - Performs a sequence of unstack, refresh and stack\n\n# Duplicated from stackrc for now\nDEST=/opt/stack\n\nall:\n\t@echo \"This just saved you from a terrible mistake!\"\n\n# Do Some Work\nstack:\n\t./stack.sh\n\nunstack:\n\t./unstack.sh\n\ndocs:\n\ttox -edocs\n\n# Just run the shocco source formatting build\ndocs-build:\n\tINSTALL_SHOCCO=True tools/build_docs.sh\n\n# Just run the Sphinx docs build\ndocs-rst:\n\tpython setup.py build_sphinx\n\n# Run the bashate test\nbashate:\n\ttox -ebashate\n\n# Run the function tests\ntest:\n\ttests/test_ini_config.sh\n\ttests/test_meta_config.sh\n\ttests/test_ip.sh\n\ttests/test_refs.sh\n\n# Spiff up the place a bit\nclean:\n\t./clean.sh\n\trm -rf accrc doc/build test*-e *.egg-info\n\n# Clean out the cache too\nrealclean: clean\n\trm -rf files/cirros*.tar.gz files/Fedora*.qcow2\n\n# Repo stuffs\n\npull:\n\tgit pull\n\n\n# These repo targets are used to maintain a branch in a remote repo that\n# consists of one or more local branches merged and pushed to the remote.\n# This is most useful for iterative testing on multiple or remote servers\n# while keeping the working repo local.\n#\n# It requires:\n# * a remote pointing to a remote repo, often GitHub is used for this\n# * a branch name to be used on the remote\n# * a local file containing the list of local branches to be merged into\n#   the remote branch\n\nGIT_REMOTE_NAME=ds-test\nGIT_REMOTE_BRANCH=ds-test\n\n# Push the current branch to a remote named ds-test\nds-push:\n\tgit checkout master\n\tgit branch -D $(GIT_REMOTE_BRANCH) || true\n\tgit checkout -b $(GIT_REMOTE_BRANCH)\n\tfor i in $(shell cat .$(GIT_REMOTE_BRANCH) | grep -v \"^#\" | grep \"[^ ]\"); do \\\n\t  git merge --no-edit $$i; \\\n\tdone\n\tgit push -f $(GIT_REMOTE_NAME) HEAD:$(GIT_REMOTE_BRANCH)\n\n# Pull the ds-test branch\nds-pull:\n\tgit checkout master\n\tgit branch -D $(GIT_REMOTE_BRANCH) || true\n\tgit pull $(GIT_REMOTE_NAME) $(GIT_REMOTE_BRANCH)\n\tgit checkout $(GIT_REMOTE_BRANCH)\n\n# Add the remote - set DS_REMOTE_URL=htps://example.com/ on the command line\nds-remote:\n\tgit remote add $(GIT_REMOTE_NAME) $(DS_REMOTE_URL)\n\n# Refresh the current DevStack checkout nd re-initialize\nrefresh: unstack ds-pull stack\n"
        },
        {
          "name": "README.rst",
          "type": "blob",
          "size": 3.837890625,
          "content": "DevStack is a set of scripts and utilities to quickly deploy an OpenStack cloud\nfrom git source trees.\n\nGoals\n=====\n\n* To quickly build dev OpenStack environments in a clean Ubuntu or RockyLinux\n  environment\n* To describe working configurations of OpenStack (which code branches\n  work together?  what do config files look like for those branches?)\n* To make it easier for developers to dive into OpenStack so that they can\n  productively contribute without having to understand every part of the\n  system at once\n* To make it easy to prototype cross-project features\n* To provide an environment for the OpenStack CI testing on every commit\n  to the projects\n\nRead more at https://docs.openstack.org/devstack/latest\n\nIMPORTANT: Be sure to carefully read `stack.sh` and any other scripts you\nexecute before you run them, as they install software and will alter your\nnetworking configuration.  We strongly recommend that you run `stack.sh`\nin a clean and disposable vm when you are first getting started.\n\nVersions\n========\n\nThe DevStack master branch generally points to trunk versions of OpenStack\ncomponents.  For older, stable versions, look for branches named\nstable/[release] in the DevStack repo.  For example, you can do the\nfollowing to create a Zed OpenStack cloud::\n\n    git checkout stable/zed\n    ./stack.sh\n\nYou can also pick specific OpenStack project releases by setting the appropriate\n`*_BRANCH` variables in the ``localrc`` section of `local.conf` (look in\n`stackrc` for the default set).  Usually just before a release there will be\nmilestone-proposed branches that need to be tested::\n\n    GLANCE_REPO=https://opendev.org/openstack/glance.git\n    GLANCE_BRANCH=milestone-proposed\n\nStart A Dev Cloud\n=================\n\nInstalling in a dedicated disposable VM is safer than installing on your\ndev machine!  Plus you can pick one of the supported Linux distros for\nyour VM.  To start a dev cloud run the following NOT AS ROOT (see\n**DevStack Execution Environment** below for more on user accounts):\n\n    ./stack.sh\n\nWhen the script finishes executing, you should be able to access OpenStack\nendpoints, like so:\n\n* Horizon: http://myhost/\n* Keystone: http://myhost/identity/v3/\n\nWe also provide an environment file that you can use to interact with your\ncloud via CLI::\n\n    # source openrc file to load your environment with OpenStack CLI creds\n    . openrc\n    # list instances\n    openstack server list\n\nDevStack Execution Environment\n==============================\n\nDevStack runs rampant over the system it runs on, installing things and\nuninstalling other things.  Running this on a system you care about is a recipe\nfor disappointment, or worse.  Alas, we're all in the virtualization business\nhere, so run it in a VM.  And take advantage of the snapshot capabilities\nof your hypervisor of choice to reduce testing cycle times.  You might even save\nenough time to write one more feature before the next feature freeze...\n\n``stack.sh`` needs to have root access for a lot of tasks, but uses\n``sudo`` for all of those tasks.  However, it needs to be not-root for\nmost of its work and for all of the OpenStack services.  ``stack.sh``\nspecifically does not run if started as root.\n\nDevStack will not automatically create the user, but provides a helper\nscript in ``tools/create-stack-user.sh``.  Run that (as root!) or just\ncheck it out to see what DevStack's expectations are for the account\nit runs under.  Many people simply use their usual login (the default\n'ubuntu' login on a UEC image for example).\n\nCustomizing\n===========\n\nDevStack can be extensively configured via the configuration file\n`local.conf`.  It is likely that you will need to provide and modify\nthis file if you want anything other than the most basic setup.  Start\nby reading the `configuration guide\n<https://docs.openstack.org/devstack/latest/configuration.html>`_\nfor details of the configuration file and the many available options.\n"
        },
        {
          "name": "clean.sh",
          "type": "blob",
          "size": 3.2294921875,
          "content": "#!/bin/bash\n\n# **clean.sh**\n\n# ``clean.sh`` does its best to eradicate traces of a Grenade\n# run except for the following:\n# - both base and target code repos are left alone\n# - packages (system and pip) are left alone\n\n# This means that all data files are removed.  More??\n\n# Keep track of the current devstack directory.\nTOP_DIR=$(cd $(dirname \"$0\") && pwd)\n\n# Import common functions\nsource $TOP_DIR/functions\n\nFILES=$TOP_DIR/files\n\n# Load local configuration\nsource $TOP_DIR/openrc\n\n# Get the variables that are set in stack.sh\nif [[ -r $TOP_DIR/.stackenv ]]; then\n    source $TOP_DIR/.stackenv\nfi\n\n# Determine what system we are running on.  This provides ``os_VENDOR``,\n# ``os_RELEASE``, ``os_PACKAGE``, ``os_CODENAME``\n# and ``DISTRO``\nGetDistro\n\n# Import apache functions\nsource $TOP_DIR/lib/apache\nsource $TOP_DIR/lib/ldap\n\n# Import database library\nsource $TOP_DIR/lib/database\nsource $TOP_DIR/lib/rpc_backend\n\nsource $TOP_DIR/lib/tls\n\nsource $TOP_DIR/lib/libraries\nsource $TOP_DIR/lib/lvm\nsource $TOP_DIR/lib/horizon\nsource $TOP_DIR/lib/keystone\nsource $TOP_DIR/lib/glance\nsource $TOP_DIR/lib/nova\nsource $TOP_DIR/lib/placement\nsource $TOP_DIR/lib/cinder\nsource $TOP_DIR/lib/swift\nsource $TOP_DIR/lib/neutron\n\nset -o xtrace\n\n# Extras Source\n# --------------\n\n# Phase: source\nif [[ -d $TOP_DIR/extras.d ]]; then\n    for i in $TOP_DIR/extras.d/*.sh; do\n        [[ -r $i ]] && source $i source\n    done\nfi\n\n# Let unstack.sh do its thing first\n$TOP_DIR/unstack.sh --all\n\n# Run extras\n# ==========\n\n# Phase: clean\nload_plugin_settings\nrun_phase clean\n\nif [[ -d $TOP_DIR/extras.d ]]; then\n    for i in $TOP_DIR/extras.d/*.sh; do\n        [[ -r $i ]] && source $i clean\n    done\nfi\n\n# Clean projects\n\n# BUG: cinder tgt doesn't exit cleanly if it's not running.\ncleanup_cinder || /bin/true\n\ncleanup_glance\ncleanup_keystone\ncleanup_nova\ncleanup_placement\ncleanup_neutron\ncleanup_swift\ncleanup_horizon\n\nif is_service_enabled ldap; then\n    cleanup_ldap\nfi\n\n# Do the hypervisor cleanup until this can be moved back into lib/nova\nif is_service_enabled nova && [[ -r $NOVA_PLUGINS/hypervisor-$VIRT_DRIVER ]]; then\n    cleanup_nova_hypervisor\nfi\n\n# Clean out /etc\nsudo rm -rf /etc/keystone /etc/glance /etc/nova /etc/cinder /etc/swift /etc/neutron /etc/openstack/\n\n# Clean out tgt\nsudo rm -f /etc/tgt/conf.d/*\n\n# Clean up the message queue\ncleanup_rpc_backend\ncleanup_database\n\n# Clean out data and status\nsudo rm -rf $DATA_DIR $DEST/status $DEST/async\n\n# Clean out the log file and log directories\nif [[ -n \"$LOGFILE\" ]] && [[ -f \"$LOGFILE\" ]]; then\n    sudo rm -f $LOGFILE\nfi\nif [[ -n \"$LOGDIR\" ]] && [[ -d \"$LOGDIR\" ]]; then\n    sudo rm -rf $LOGDIR\nfi\n\n# Clean out the systemd unit files.\nsudo find $SYSTEMD_DIR -type f -name '*devstack@*service' -delete\n# Make systemd aware of the deletion.\n$SYSTEMCTL daemon-reload\n\n# Clean up venvs\nDIRS_TO_CLEAN=\"$WHEELHOUSE ${PROJECT_VENV[@]} .config/openstack\"\nrm -rf $DIRS_TO_CLEAN\n\n# Clean up files\n\nFILES_TO_CLEAN=\".localrc.auto .localrc.password \"\nFILES_TO_CLEAN+=\"docs/files docs/html shocco/ \"\nFILES_TO_CLEAN+=\"stack-screenrc test*.conf* test.ini* \"\nFILES_TO_CLEAN+=\".stackenv .prereqs\"\n\nfor file in $FILES_TO_CLEAN; do\n    rm -rf $TOP_DIR/$file\ndone\n\nrm -rf ~/.config/openstack\n\n# Clear any fstab entries made\nsudo sed -i '/.*comment=devstack-.*/ d' /etc/fstab\n"
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "doc",
          "type": "tree",
          "content": null
        },
        {
          "name": "extras.d",
          "type": "tree",
          "content": null
        },
        {
          "name": "files",
          "type": "tree",
          "content": null
        },
        {
          "name": "functions",
          "type": "blob",
          "size": 31.333984375,
          "content": "#!/bin/bash\n#\n# functions - DevStack-specific functions\n#\n# The following variables are assumed to be defined by certain functions:\n#\n# - ``DATABASE_BACKENDS``\n# - ``ENABLED_SERVICES``\n# - ``FILES``\n# - ``GLANCE_HOSTPORT``\n#\n\n# ensure we don't re-source this in the same environment\n[[ -z \"$_DEVSTACK_FUNCTIONS\" ]] || return 0\ndeclare -r -g _DEVSTACK_FUNCTIONS=1\n\n# Include the common functions\nFUNC_DIR=$(cd $(dirname \"${BASH_SOURCE:-$0}\") && pwd)\nsource ${FUNC_DIR}/functions-common\nsource ${FUNC_DIR}/inc/ini-config\nsource ${FUNC_DIR}/inc/meta-config\nsource ${FUNC_DIR}/inc/python\nsource ${FUNC_DIR}/inc/rootwrap\nsource ${FUNC_DIR}/inc/async\n\n# Save trace setting\n_XTRACE_FUNCTIONS=$(set +o | grep xtrace)\nset +o xtrace\n\n# Check if a function already exists\nfunction function_exists {\n    declare -f -F $1 > /dev/null\n}\n\n# short_source prints out the current location of the caller in a way\n# that strips redundant directories. This is useful for PS4 usage.\nfunction short_source {\n    saveIFS=$IFS\n    IFS=\" \"\n    called=($(caller 0))\n    IFS=$saveIFS\n    file=${called[2]}\n    file=${file#$RC_DIR/}\n    printf \"%-40s \" \"$file:${called[1]}:${called[0]}\"\n}\n# PS4 is exported to child shells and uses the 'short_source' function, so\n# export it so child shells have access to the 'short_source' function also.\nexport -f short_source\n\n# Download a file from a URL\n#\n# Will check cache (in $FILES) or download given URL.\n#\n# Argument is the URL to the remote file\n#\n# Will echo the local path to the file as the output.  Will die on\n# failure to download.\n#\n# Files can be pre-cached for CI environments, see EXTRA_CACHE_URLS\n# and tools/image_list.sh\nfunction get_extra_file {\n    local file_url=$1\n\n    file_name=$(basename \"$file_url\")\n    if [[ $file_url != file* ]]; then\n        # If the file isn't cache, download it\n        if [[ ! -f $FILES/$file_name ]]; then\n            wget --progress=dot:giga -t 2 -c $file_url -O $FILES/$file_name\n            if [[ $? -ne 0 ]]; then\n                die \"$file_url could not be downloaded\"\n            fi\n        fi\n        echo \"$FILES/$file_name\"\n        return\n    else\n        # just strip the file:// bit and that's the path to the file\n        echo $file_url | sed 's/$file:\\/\\///g'\n    fi\n}\n\n# Generate image property arguments for OSC\n#\n# Arguments: properties, one per, like propname=value\n#\n# Result is --property propname1=value1 --property propname2=value2\nfunction _image_properties_to_arg {\n    local result=\"\"\n    for property in $*; do\n        result+=\" --property $property\"\n    done\n    echo $result\n}\n\n# Upload an image to glance using the configured mechanism\n#\n# Arguments:\n#  image name\n#  container format\n#  disk format\n#  path to image file\n#  optional properties (format of propname=value)\n#\nfunction _upload_image {\n    local image_name=\"$1\"\n    shift\n    local container=\"$1\"\n    shift\n    local disk=\"$1\"\n    shift\n    local image=\"$1\"\n    shift\n    local properties\n    local useimport\n\n    properties=$(_image_properties_to_arg $*)\n\n    if [[ \"$GLANCE_USE_IMPORT_WORKFLOW\" == \"True\" ]]; then\n        useimport=\"--import\"\n    fi\n\n    openstack --os-cloud=devstack-admin --os-region-name=\"$REGION_NAME\" image create \"$image_name\" --public --container-format \"$container\" --disk-format \"$disk\" $useimport $properties --file $(readlink -f \"${image}\")\n}\n\n# Retrieve an image from a URL and upload into Glance.\n# Uses the following variables:\n#\n# - ``FILES`` must be set to the cache dir\n# - ``GLANCE_HOSTPORT``\n#\n# upload_image image-url\nfunction upload_image {\n    local image_url=$1\n\n    local image image_fname image_name\n\n    local max_attempts=5\n\n    # Create a directory for the downloaded image tarballs.\n    mkdir -p $FILES/images\n    image_fname=`basename \"$image_url\"`\n    if [[ $image_url != file* ]]; then\n        # Downloads the image (uec ami+akistyle), then extracts it.\n        if [[ ! -f $FILES/$image_fname || \"$(stat -c \"%s\" $FILES/$image_fname)\" = \"0\" ]]; then\n            for attempt in `seq $max_attempts`; do\n                local rc=0\n                wget --progress=dot:giga -c $image_url -O $FILES/$image_fname || rc=$?\n                if [[ $rc -ne 0 ]]; then\n                    if [[ \"$attempt\" -eq \"$max_attempts\" ]]; then\n                        echo \"Not found: $image_url\"\n                        return\n                    fi\n                    echo \"Download failed, retrying in $attempt second, attempt: $attempt\"\n                    sleep $attempt\n                else\n                    break\n                fi\n            done\n        fi\n        image=\"$FILES/${image_fname}\"\n    else\n        # File based URL (RFC 1738): ``file://host/path``\n        # Remote files are not considered here.\n        # unix: ``file:///home/user/path/file``\n        # windows: ``file:///C:/Documents%20and%20Settings/user/path/file``\n        image=$(echo $image_url | sed \"s/^file:\\/\\///g\")\n        if [[ ! -f $image || \"$(stat -c \"%s\" $image)\" == \"0\" ]]; then\n            echo \"Not found: $image_url\"\n            return\n        fi\n    fi\n\n    # OpenVZ-format images are provided as .tar.gz, but not decompressed prior to loading\n    if [[ \"$image_url\" =~ 'openvz' ]]; then\n        image_name=\"${image_fname%.tar.gz}\"\n        _upload_image \"$image_name\" ami ami \"$image\"\n        return\n    fi\n\n    # vmdk format images\n    if [[ \"$image_url\" =~ '.vmdk' ]]; then\n        image_name=\"${image_fname%.vmdk}\"\n\n        # Before we can upload vmdk type images to glance, we need to know it's\n        # disk type, storage adapter, and networking adapter. These values are\n        # passed to glance as custom properties.\n        # We take these values from the vmdk file if populated. Otherwise, we use\n        # vmdk filename, which is expected in the following format:\n        #\n        #     <name>-<disk type>;<storage adapter>;<network adapter>\n        #\n        # If the filename does not follow the above format then the vsphere\n        # driver will supply default values.\n\n        local vmdk_disktype=\"\"\n        local vmdk_net_adapter=\"e1000\"\n        local path_len\n\n        # vmdk adapter type\n        local vmdk_adapter_type\n        vmdk_adapter_type=\"$(head -25 $image | { grep -a -F -m 1 'ddb.adapterType =' $image || true; })\"\n        vmdk_adapter_type=\"${vmdk_adapter_type#*\\\"}\"\n        vmdk_adapter_type=\"${vmdk_adapter_type%?}\"\n\n        # vmdk disk type\n        local vmdk_create_type\n        vmdk_create_type=\"$(head -25 $image | { grep -a -F -m 1 'createType=' $image || true; })\"\n        vmdk_create_type=\"${vmdk_create_type#*\\\"}\"\n        vmdk_create_type=\"${vmdk_create_type%\\\"*}\"\n\n        descriptor_data_pair_msg=\"Monolithic flat and VMFS disks \"`\n                                    `\"should use a descriptor-data pair.\"\n        if [[ \"$vmdk_create_type\" = \"monolithicSparse\" ]]; then\n            vmdk_disktype=\"sparse\"\n        elif [[ \"$vmdk_create_type\" = \"monolithicFlat\" || \"$vmdk_create_type\" = \"vmfs\" ]]; then\n            # Attempt to retrieve the ``*-flat.vmdk``\n            local flat_fname\n            flat_fname=\"$(head -25 $image | { grep -G 'RW\\|RDONLY [0-9]+ FLAT\\|VMFS' $image || true; })\"\n            flat_fname=\"${flat_fname#*\\\"}\"\n            flat_fname=\"${flat_fname%?}\"\n            if [[ -z \"$flat_fname\" ]]; then\n                flat_fname=\"$image_name-flat.vmdk\"\n            fi\n            path_len=`expr ${#image_url} - ${#image_fname}`\n            local flat_url=\"${image_url:0:$path_len}$flat_fname\"\n            warn $LINENO \"$descriptor_data_pair_msg\"`\n                            `\" Attempt to retrieve the *-flat.vmdk: $flat_url\"\n            if [[ $flat_url != file* ]]; then\n                if [[ ! -f $FILES/$flat_fname || \\\n                \"$(stat -c \"%s\" $FILES/$flat_fname)\" = \"0\" ]]; then\n                    wget --progress=dot:giga -c $flat_url -O $FILES/$flat_fname\n                fi\n                image=\"$FILES/${flat_fname}\"\n            else\n                image=$(echo $flat_url | sed \"s/^file:\\/\\///g\")\n                if [[ ! -f $image || \"$(stat -c \"%s\" $image)\" == \"0\" ]]; then\n                    echo \"Flat disk not found: $flat_url\"\n                    return 1\n                fi\n            fi\n            image_name=\"${flat_fname}\"\n            vmdk_disktype=\"preallocated\"\n        elif [[ \"$vmdk_create_type\" = \"streamOptimized\" ]]; then\n            vmdk_disktype=\"streamOptimized\"\n        elif [[ -z \"$vmdk_create_type\" ]]; then\n            # *-flat.vmdk provided: attempt to retrieve the descriptor (*.vmdk)\n            # to retrieve appropriate metadata\n            if [[ ${image_name: -5} != \"-flat\" ]]; then\n                warn $LINENO \"Expected filename suffix: '-flat'.\"`\n                            `\" Filename provided: ${image_name}\"\n            else\n                descriptor_fname=\"${image_name:0:${#image_name} - 5}.vmdk\"\n                path_len=`expr ${#image_url} - ${#image_fname}`\n                local flat_path=\"${image_url:0:$path_len}\"\n                local descriptor_url=$flat_path$descriptor_fname\n                warn $LINENO \"$descriptor_data_pair_msg\"`\n                                `\" Attempt to retrieve the descriptor *.vmdk: $descriptor_url\"\n                if [[ $flat_path != file* ]]; then\n                    if [[ ! -f $FILES/$descriptor_fname || \\\n                    \"$(stat -c \"%s\" $FILES/$descriptor_fname)\" = \"0\" ]]; then\n                        wget -c $descriptor_url -O $FILES/$descriptor_fname\n                    fi\n                    descriptor_url=\"$FILES/$descriptor_fname\"\n                else\n                    descriptor_url=$(echo $descriptor_url | sed \"s/^file:\\/\\///g\")\n                    if [[ ! -f $descriptor_url || \\\n                    \"$(stat -c \"%s\" $descriptor_url)\" == \"0\" ]]; then\n                        echo \"Descriptor not found: $descriptor_url\"\n                        return 1\n                    fi\n                fi\n                vmdk_adapter_type=\"$(head -25 $descriptor_url | { grep -a -F -m 1 'ddb.adapterType =' $descriptor_url || true; })\"\n                vmdk_adapter_type=\"${vmdk_adapter_type#*\\\"}\"\n                vmdk_adapter_type=\"${vmdk_adapter_type%?}\"\n            fi\n            vmdk_disktype=\"preallocated\"\n        else\n            vmdk_disktype=\"preallocated\"\n        fi\n\n        # NOTE: For backwards compatibility reasons, colons may be used in place\n        # of semi-colons for property delimiters but they are not permitted\n        # characters in NTFS filesystems.\n        property_string=`echo \"$image_name\" | { grep -oP '(?<=-)(?!.*-).*[:;].*[:;].*$' || true; }`\n        IFS=':;' read -a props <<< \"$property_string\"\n        vmdk_disktype=\"${props[0]:-$vmdk_disktype}\"\n        vmdk_adapter_type=\"${props[1]:-$vmdk_adapter_type}\"\n        vmdk_net_adapter=\"${props[2]:-$vmdk_net_adapter}\"\n\n        _upload_image \"$image_name\" bare vmdk \"$image\" vmware_disktype=\"$vmdk_disktype\" vmware_adaptertype=\"$vmdk_adapter_type\" hw_vif_model=\"$vmdk_net_adapter\"\n\n        return\n    fi\n\n    if [[ \"$image_url\" =~ '.hds' ]]; then\n        image_name=\"${image_fname%.hds}\"\n        vm_mode=${image_name##*-}\n        if [[ $vm_mode != 'exe' && $vm_mode != 'hvm' ]]; then\n            die $LINENO \"Unknown vm_mode=${vm_mode} for Virtuozzo image\"\n        fi\n\n        _upload_image \"$image_name\" bare ploop \"$image\" vm_mode=$vm_mode\n        return\n    fi\n\n    local kernel=\"\"\n    local ramdisk=\"\"\n    local disk_format=\"\"\n    local container_format=\"\"\n    local unpack=\"\"\n    local img_property=\"\"\n\n    # NOTE(danms): If we're on libvirt/qemu or libvirt/kvm, set the hw_rng_model\n    # to libvirt in the image properties.\n    if [[ \"$VIRT_DRIVER\" == \"libvirt\" ]]; then\n        if [[ \"$LIBVIRT_TYPE\" == \"qemu\" || \"$LIBVIRT_TYPE\" == \"kvm\" ]]; then\n            img_property=\"hw_rng_model=virtio\"\n        fi\n    fi\n\n    case \"$image_fname\" in\n        *.tar.gz|*.tgz)\n            # Extract ami and aki files\n            [ \"${image_fname%.tar.gz}\" != \"$image_fname\" ] &&\n                image_name=\"${image_fname%.tar.gz}\" ||\n                image_name=\"${image_fname%.tgz}\"\n            local xdir=\"$FILES/images/$image_name\"\n            rm -Rf \"$xdir\";\n            mkdir \"$xdir\"\n            tar -zxf $image -C \"$xdir\"\n            kernel=$(for f in \"$xdir/\"*-vmlinuz* \"$xdir/\"aki-*/image; do\n                [ -f \"$f\" ] && echo \"$f\" && break; done; true)\n            ramdisk=$(for f in \"$xdir/\"*-initrd* \"$xdir/\"ari-*/image; do\n                [ -f \"$f\" ] && echo \"$f\" && break; done; true)\n            image=$(for f in \"$xdir/\"*.img \"$xdir/\"ami-*/image; do\n                [ -f \"$f\" ] && echo \"$f\" && break; done; true)\n            if [[ -z \"$image_name\" ]]; then\n                image_name=$(basename \"$image\" \".img\")\n            fi\n            ;;\n        *.img)\n            image_name=$(basename \"$image\" \".img\")\n            local format\n            format=$(qemu-img info ${image} | awk '/^file format/ { print $3; exit }')\n            if [[ \",qcow2,raw,vdi,vmdk,vpc,\" =~ \",$format,\" ]]; then\n                disk_format=$format\n            else\n                disk_format=raw\n            fi\n            container_format=bare\n            ;;\n        *.img.gz)\n            image_name=$(basename \"$image\" \".img.gz\")\n            disk_format=raw\n            container_format=bare\n            unpack=zcat\n            ;;\n        *.img.bz2)\n            image_name=$(basename \"$image\" \".img.bz2\")\n            disk_format=qcow2\n            container_format=bare\n            unpack=bunzip2\n            ;;\n        *.qcow2)\n            image_name=$(basename \"$image\" \".qcow2\")\n            disk_format=qcow2\n            container_format=bare\n            ;;\n        *.qcow2.xz)\n            image_name=$(basename \"$image\" \".qcow2.xz\")\n            disk_format=qcow2\n            container_format=bare\n            unpack=unxz\n            ;;\n        *.raw)\n            image_name=$(basename \"$image\" \".raw\")\n            disk_format=raw\n            container_format=bare\n            ;;\n        *.iso)\n            image_name=$(basename \"$image\" \".iso\")\n            disk_format=iso\n            container_format=bare\n            ;;\n        *.vhd|*.vhdx|*.vhd.gz|*.vhdx.gz)\n            local extension=\"${image_fname#*.}\"\n            image_name=$(basename \"$image\" \".$extension\")\n            disk_format=$(echo $image_fname | grep -oP '(?<=\\.)vhdx?(?=\\.|$)')\n            container_format=bare\n            if [ \"${image_fname##*.}\" == \"gz\" ]; then\n                unpack=zcat\n            fi\n            ;;\n        *) echo \"Do not know what to do with $image_fname\"; false;;\n    esac\n\n    if is_arch \"ppc64le\" || is_arch \"ppc64\" || is_arch \"ppc\"; then\n        img_property=\"$img_property hw_cdrom_bus=scsi os_command_line=console=hvc0\"\n    fi\n\n    if is_arch \"aarch64\"; then\n        img_property=\"$img_property hw_machine_type=virt hw_cdrom_bus=scsi hw_scsi_model=virtio-scsi os_command_line='console=ttyAMA0'\"\n    fi\n\n    if [ \"$container_format\" = \"bare\" ]; then\n        if [ \"$unpack\" = \"zcat\" ]; then\n            _upload_image \"$image_name\" $container_format $disk_format <(zcat --force \"$image\") $img_property\n        elif [ \"$unpack\" = \"bunzip2\" ]; then\n            _upload_image \"$image_name\" $container_format $disk_format <(bunzip2 -cdk \"$image\") $img_property\n        elif [ \"$unpack\" = \"unxz\" ]; then\n            # NOTE(brtknr): unxz the file first and cleanup afterwards to\n            # prevent timeout while Glance tries to upload image (e.g. to Swift).\n            local tmp_dir\n            local image_path\n            tmp_dir=$(mktemp -d)\n            image_path=\"$tmp_dir/$image_name\"\n            unxz -cv \"${image}\" > \"$image_path\"\n            _upload_image \"$image_name\" $container_format $disk_format \"$image_path\" $img_property\n            rm -rf $tmp_dir\n        else\n            _upload_image \"$image_name\" $container_format $disk_format \"$image\" $img_property\n        fi\n    else\n        # Use glance client to add the kernel the root filesystem.\n        # We parse the results of the first upload to get the glance ID of the\n        # kernel for use when uploading the root filesystem.\n        local kernel_id=\"\" ramdisk_id=\"\";\n        if [ -n \"$kernel\" ]; then\n            kernel_id=$(openstack --os-cloud=devstack-admin --os-region-name=\"$REGION_NAME\" image create \"$image_name-kernel\" $(_image_properties_to_arg $img_property) --public --container-format aki --disk-format aki --file $(readlink -f \"$kernel\") -f value -c id)\n        fi\n        if [ -n \"$ramdisk\" ]; then\n            ramdisk_id=$(openstack --os-cloud=devstack-admin --os-region-name=\"$REGION_NAME\" image create \"$image_name-ramdisk\" $(_image_properties_to_arg $img_property) --public --container-format ari --disk-format ari --file $(readlink -f \"$ramdisk\") -f value -c id)\n        fi\n        _upload_image \"${image_name%.img}\" ami ami \"$image\" ${kernel_id:+ kernel_id=$kernel_id} ${ramdisk_id:+ ramdisk_id=$ramdisk_id} $img_property\n    fi\n}\n\n\n# Set the database backend to use\n# When called from stackrc/localrc DATABASE_BACKENDS has not been\n# initialized yet, just save the configuration selection and call back later\n# to validate it.\n#\n# ``$1`` - the name of the database backend to use (mysql, postgresql, ...)\nfunction use_database {\n    if [[ -z \"$DATABASE_BACKENDS\" ]]; then\n        # No backends registered means this is likely called from ``localrc``\n        # This is now deprecated usage\n        DATABASE_TYPE=$1\n        deprecated \"The database backend needs to be properly set in ENABLED_SERVICES; use_database is deprecated localrc\"\n    else\n        # This should no longer get called...here for posterity\n        use_exclusive_service DATABASE_BACKENDS DATABASE_TYPE $1\n    fi\n}\n\n#Macro for curl statements. curl requires -g option for literal IPv6 addresses.\nCURL_GET=\"${CURL_GET:-curl -g}\"\n\n# Wait for an HTTP server to start answering requests\n# wait_for_service timeout url\n#\n# If the service we want is behind a proxy, the proxy may be available\n# before the service. Compliant proxies will return a 503 in this case\n# Loop until we get something else.\n# Also check for the case where there is no proxy and the service just\n# hasn't started yet. curl returns 7 for Failed to connect to host.\nfunction wait_for_service {\n    local timeout=$1\n    local url=$2\n    local rval=0\n    time_start \"wait_for_service\"\n    timeout $timeout bash -x <<EOF || rval=$?\n        while [[ \\$( ${CURL_GET} -k --noproxy '*' -s -o /dev/null -w '%{http_code}' ${url} ) == 503 || \\$? -eq 7 ]]; do\n            sleep 1\n        done\nEOF\n    time_stop \"wait_for_service\"\n    return $rval\n}\n\nfunction wait_for_compute {\n    local timeout=$1\n    local rval=0\n    local compute_hostname\n    time_start \"wait_for_service\"\n    compute_hostname=$(iniget $NOVA_CONF DEFAULT host)\n    if [[ -z $compute_hostname ]]; then\n        compute_hostname=$(hostname)\n    fi\n    timeout $timeout bash -x <<EOF || rval=$?\n        ID=\"\"\n        while [[ \"\\$ID\" == \"\" ]]; do\n            sleep 1\n            if [[ \"$VIRT_DRIVER\" = 'fake' ]]; then\n                # When using the fake driver the compute hostnames have a suffix of 1 to NUMBER_FAKE_NOVA_COMPUTE\n                ID=\\$(openstack --os-cloud devstack-admin --os-region \"$REGION_NAME\" compute service list --host `hostname`1 --service nova-compute -c ID -f value)\n            else\n                ID=\\$(openstack --os-cloud devstack-admin --os-region \"$REGION_NAME\" compute service list --host \"$compute_hostname\" --service nova-compute -c ID -f value)\n            fi\n        done\nEOF\n    time_stop \"wait_for_service\"\n    # Figure out what's happening on platforms where this doesn't work\n    if [[ \"$rval\" != 0 ]]; then\n        echo \"Didn't find service registered by hostname after $timeout seconds\"\n        openstack --os-cloud devstack-admin --os-region \"$REGION_NAME\" compute service list\n    fi\n    return $rval\n}\n\n\n# ping check\n# Uses globals ``ENABLED_SERVICES``, ``TOP_DIR``, ``PRIVATE_NETWORK``\n# ping_check <ip> [boot-timeout] [from_net] [expected]\nfunction ping_check {\n    local ip=$1\n    local timeout=${2:-30}\n    local from_net=${3:-\"\"}\n    local expected=${4:-True}\n    local op=\"!\"\n    local failmsg=\"[Fail] Couldn't ping server\"\n    local ping_cmd=\"ping\"\n\n    # if we don't specify a from_net we're expecting things to work\n    # fine from our local box.\n    if [[ -n \"$from_net\" ]]; then\n        # TODO(stephenfin): Is there any way neutron could be disabled now?\n        if is_service_enabled neutron; then\n            ping_cmd=\"$TOP_DIR/tools/ping_neutron.sh $from_net\"\n        fi\n    fi\n\n    # inverse the logic if we're testing no connectivity\n    if [[ \"$expected\" != \"True\" ]]; then\n        op=\"\"\n        failmsg=\"[Fail] Could ping server\"\n    fi\n\n    # Because we've transformed this command so many times, print it\n    # out at the end.\n    local check_command=\"while $op $ping_cmd -c1 -w1 $ip; do sleep 1; done\"\n    echo \"Checking connectivity with $check_command\"\n\n    if ! timeout $timeout sh -c \"$check_command\"; then\n        die $LINENO $failmsg\n    fi\n}\n\n# Get ip of instance\nfunction get_instance_ip {\n    local vm_id=$1\n    local network_name=$2\n    local addresses\n    local ip\n\n    addresses=$(openstack server show -c addresses -f value \"$vm_id\")\n    ip=$(echo $addresses | sed -n \"s/^.*$network_name=\\([0-9\\.]*\\).*$/\\1/p\")\n    if [[ $ip = \"\" ]];then\n        echo \"addresses of server $vm_id : $addresses\"\n        die $LINENO \"[Fail] Couldn't get ipaddress of VM\"\n    fi\n    echo $ip\n}\n\n# ssh check\n\n# ssh_check net-name key-file floating-ip default-user active-timeout\nfunction ssh_check {\n    if is_service_enabled neutron; then\n        _ssh_check_neutron  \"$1\" $2 $3 $4 $5\n        return\n    fi\n    _ssh_check_novanet \"$1\" $2 $3 $4 $5\n}\n\nfunction _ssh_check_novanet {\n    local NET_NAME=$1\n    local KEY_FILE=$2\n    local FLOATING_IP=$3\n    local DEFAULT_INSTANCE_USER=$4\n    local ACTIVE_TIMEOUT=$5\n    local probe_cmd=\"\"\n    if ! timeout $ACTIVE_TIMEOUT sh -c \"while ! ssh -o StrictHostKeyChecking=no -i $KEY_FILE ${DEFAULT_INSTANCE_USER}@$FLOATING_IP echo success; do sleep 1; done\"; then\n        die $LINENO \"server didn't become ssh-able!\"\n    fi\n}\n\n\n# Get the location of the $module-rootwrap executables, where module is cinder\n# or nova.\n# get_rootwrap_location module\nfunction get_rootwrap_location {\n    local module=$1\n\n    echo \"$(get_python_exec_prefix)/$module-rootwrap\"\n}\n\n\n# Path permissions sanity check\n# check_path_perm_sanity path\nfunction check_path_perm_sanity {\n    # Ensure no element of the path has 0700 permissions, which is very\n    # likely to cause issues for daemons.  Inspired by default 0700\n    # homedir permissions on RHEL and common practice of making DEST in\n    # the stack user's homedir.\n\n    local real_path\n    real_path=$(readlink -f $1)\n    local rebuilt_path=\"\"\n    for i in $(echo ${real_path} | tr \"/\" \" \"); do\n        rebuilt_path=$rebuilt_path\"/\"$i\n\n        if [[ $(stat -c '%a' ${rebuilt_path}) = 700 ]]; then\n            echo \"*** DEST path element\"\n            echo \"***    ${rebuilt_path}\"\n            echo \"*** appears to have 0700 permissions.\"\n            echo \"*** This is very likely to cause fatal issues for DevStack daemons.\"\n\n            if [[ -n \"$SKIP_PATH_SANITY\" ]]; then\n                return\n            else\n                echo \"*** Set SKIP_PATH_SANITY to skip this check\"\n                die $LINENO \"Invalid path permissions\"\n            fi\n        fi\n    done\n}\n\n\n# vercmp ver1 op ver2\n#  Compare VER1 to VER2\n#   - op is one of < <= == >= >\n#   - returns true if satisified\n#  e.g.\n#  if vercmp 1.0 \"<\" 2.0; then\n#    ...\n#  fi\nfunction vercmp {\n    local v1=$1\n    local op=$2\n    local v2=$3\n    local result\n\n    # sort the two numbers with sort's \"-V\" argument.  Based on if v2\n    # swapped places with v1, we can determine ordering.\n    result=$(echo -e \"$v1\\n$v2\" | sort -V | head -1)\n\n    case $op in\n        \"==\")\n            [ \"$v1\" = \"$v2\" ]\n            return\n            ;;\n        \">\")\n            [ \"$v1\" != \"$v2\" ] && [ \"$result\" = \"$v2\" ]\n            return\n            ;;\n        \"<\")\n            [ \"$v1\" != \"$v2\" ] && [ \"$result\" = \"$v1\" ]\n            return\n            ;;\n        \">=\")\n            [ \"$result\" = \"$v2\" ]\n            return\n            ;;\n        \"<=\")\n            [ \"$result\" = \"$v1\" ]\n            return\n            ;;\n        *)\n            die $LINENO \"unrecognised op: $op\"\n            ;;\n    esac\n}\n\n# This sets up defaults we like in devstack for logging for tracking\n# down issues, and makes sure everything is done the same between\n# projects.\n# NOTE(jh): Historically this function switched between three different\n# functions: setup_systemd_logging, setup_colorized_logging and\n# setup_standard_logging_identity. Since we always run with systemd now,\n# this could be cleaned up, but the other functions may still be in use\n# by plugins. Since deprecations haven't worked in the past, we'll just\n# leave them in place.\nfunction setup_logging {\n    setup_systemd_logging $1\n}\n\n# This function sets log formatting options for colorizing log\n# output to stdout. It is meant to be called by lib modules.\nfunction setup_colorized_logging {\n    local conf_file=$1\n    # Add color to logging output\n    iniset $conf_file DEFAULT logging_context_format_string \"%(asctime)s.%(msecs)03d %(color)s%(levelname)s %(name)s [\u001b[01;36m%(request_id)s \u001b[00;36m%(project_name)s %(user_name)s%(color)s] \u001b[01;35m%(instance)s%(color)s%(message)s\u001b[00m\"\n    iniset $conf_file DEFAULT logging_default_format_string \"%(asctime)s.%(msecs)03d %(color)s%(levelname)s %(name)s [\u001b[00;36m-%(color)s] \u001b[01;35m%(instance)s%(color)s%(message)s\u001b[00m\"\n    iniset $conf_file DEFAULT logging_debug_format_suffix \"\u001b[00;33mfrom (pid=%(process)d) %(funcName)s %(pathname)s:%(lineno)d\u001b[00m\"\n    iniset $conf_file DEFAULT logging_exception_prefix \"%(color)s%(asctime)s.%(msecs)03d TRACE %(name)s \u001b[01;35m%(instance)s\u001b[00m\"\n    # Enable or disable color for oslo.log\n    iniset $conf_file DEFAULT log_color $LOG_COLOR\n}\n\nfunction setup_systemd_logging {\n    local conf_file=$1\n    # NOTE(sdague): this is a nice to have, and means we're using the\n    # native systemd path, which provides for things like search on\n    # request-id. However, there may be an eventlet interaction here,\n    # so going off for now.\n    USE_JOURNAL=$(trueorfalse False USE_JOURNAL)\n    local pidstr=\"\"\n    if [[ \"$USE_JOURNAL\" == \"True\" ]]; then\n        iniset $conf_file DEFAULT use_journal \"True\"\n        # if we are using the journal directly, our process id is already correct\n    else\n        pidstr=\"(pid=%(process)d) \"\n    fi\n    iniset $conf_file DEFAULT logging_debug_format_suffix \"\u001b[00;33m{{${pidstr}%(funcName)s %(pathname)s:%(lineno)d}}\u001b[00m\"\n\n    iniset $conf_file DEFAULT logging_context_format_string \"%(color)s%(levelname)s %(name)s [\u001b[01;36m%(global_request_id)s %(request_id)s \u001b[00;36m%(project_name)s %(user_name)s%(color)s] \u001b[01;35m%(instance)s%(color)s%(message)s\u001b[00m\"\n    iniset $conf_file DEFAULT logging_default_format_string \"%(color)s%(levelname)s %(name)s [\u001b[00;36m-%(color)s] \u001b[01;35m%(instance)s%(color)s%(message)s\u001b[00m\"\n    iniset $conf_file DEFAULT logging_exception_prefix \"ERROR %(name)s \u001b[01;35m%(instance)s\u001b[00m\"\n\n    # Enable or disable color for oslo.log\n    iniset $conf_file DEFAULT log_color $LOG_COLOR\n}\n\nfunction setup_standard_logging_identity {\n    local conf_file=$1\n    iniset $conf_file DEFAULT logging_user_identity_format \"%(project_name)s %(user_name)s\"\n}\n\n# These functions are provided for basic fall-back functionality for\n# projects that include parts of DevStack (Grenade).  stack.sh will\n# override these with more specific versions for DevStack (with fancy\n# spinners, etc).  We never override an existing version\nif ! function_exists echo_summary; then\n    function echo_summary {\n        echo $@\n    }\nfi\nif ! function_exists echo_nolog; then\n    function echo_nolog {\n        echo $@\n    }\nfi\n\n\n# create_disk - Create, configure, and mount a backing disk\nfunction create_disk {\n    local node_number\n    local disk_image=${1}\n    local storage_data_dir=${2}\n    local loopback_disk_size=${3}\n    local key\n\n    key=$(echo $disk_image | sed 's#/.##')\n    key=\"devstack-$key\"\n\n    destroy_disk $disk_image $storage_data_dir\n\n    # Create an empty file of the correct size (and ensure the\n    # directory structure up to that path exists)\n    sudo mkdir -p $(dirname ${disk_image})\n    sudo truncate -s ${loopback_disk_size} ${disk_image}\n\n    # Make a fresh XFS filesystem. Use bigger inodes so xattr can fit in\n    # a single inode. Keeping the default inode size (256) will result in multiple\n    # inodes being used to store xattr. Retrieving the xattr will be slower\n    # since we have to read multiple inodes. This statement is true for both\n    # Swift and Ceph.\n    sudo mkfs.xfs -f -i size=1024 ${disk_image}\n\n    # Install a new loopback fstab entry for this disk image, and mount it\n    echo \"$disk_image $storage_data_dir xfs loop,noatime,nodiratime,logbufs=8,comment=$key 0 0\" | sudo tee -a /etc/fstab\n    sudo mkdir -p $storage_data_dir\n    sudo mount -v $storage_data_dir\n}\n\n# Unmount, de-configure, and destroy a backing disk\nfunction destroy_disk {\n    local disk_image=$1\n    local storage_data_dir=$2\n    local key\n\n    key=$(echo $disk_image | sed 's#/.##')\n    key=\"devstack-$key\"\n\n    # Unmount the target, if mounted\n    if egrep -q $storage_data_dir /proc/mounts; then\n        sudo umount $storage_data_dir\n    fi\n\n    # Clear any fstab rules\n    sudo sed -i '/.*comment=$key.*/ d' /etc/fstab\n\n    # Delete the file\n    sudo rm -f $disk_image\n}\n\n\n# set_mtu - Set MTU on a device\nfunction set_mtu {\n    local dev=$1\n    local mtu=$2\n    sudo ip link set mtu $mtu dev $dev\n}\n\n\n# running_in_container - Returns true otherwise false\nfunction running_in_container {\n    [[ $(systemd-detect-virt --container) != 'none' ]]\n}\n\n\n# enable_kernel_bridge_firewall - Enable kernel support for bridge firewalling\nfunction enable_kernel_bridge_firewall {\n    # Load bridge module. This module provides access to firewall for bridged\n    # frames; and also on older kernels (pre-3.18) it provides sysctl knobs to\n    # enable/disable bridge firewalling\n    sudo modprobe bridge\n    # For newer kernels (3.18+), those sysctl settings are split into a separate\n    # kernel module (br_netfilter). Load it too, if present.\n    sudo modprobe br_netfilter 2>> /dev/null || :\n    # Enable bridge firewalling in case it's disabled in kernel (upstream\n    # default is enabled, but some distributions may decide to change it).\n    # This is at least needed for RHEL 7.2 and earlier releases.\n    for proto in ip ip6; do\n        sudo sysctl -w net.bridge.bridge-nf-call-${proto}tables=1\n    done\n}\n\n\n# Set a systemd system override\n#\n# This sets a system-side override in system.conf. A per-service\n# override would be /etc/systemd/system/${service}.service/override.conf\nfunction set_systemd_override {\n    local key=\"$1\"\n    local value=\"$2\"\n\n    local sysconf=\"/etc/systemd/system.conf\"\n    iniset -sudo \"${sysconf}\" \"Manager\" \"$key\" \"$value\"\n    echo \"Set systemd system override for ${key}=${value}\"\n\n    sudo systemctl daemon-reload\n}\n\n# Get a random port from the local port range\n#\n# This function returns an available port in the local port range. The search\n# order is not truly random, but should be considered a random value by the\n# user because it depends on the state of your local system.\nfunction get_random_port {\n    read lower_port upper_port < /proc/sys/net/ipv4/ip_local_port_range\n    while true; do\n        for (( port = upper_port ; port >= lower_port ; port-- )); do\n            sudo lsof -i \":$port\" &> /dev/null\n            if [[ $? > 0 ]] ; then\n                break 2\n            fi\n        done\n    done\n    echo $port\n}\n\n# Save some state information\n#\n# Write out various useful state information to /etc/devstack-version\nfunction write_devstack_version {\n    cat - <<EOF | sudo tee /etc/devstack-version >/dev/null\nDevStack Version: ${DEVSTACK_SERIES}\nChange: $(git log --format=\"%H %s %ci\" -1)\nOS Version: ${os_VENDOR} ${os_RELEASE} ${os_CODENAME}\nEOF\n}\n\n# Restore xtrace\n$_XTRACE_FUNCTIONS\n\n# Local variables:\n# mode: shell-script\n# End:\n"
        },
        {
          "name": "functions-common",
          "type": "blob",
          "size": 76.2041015625,
          "content": "#!/bin/bash\n#\n# functions-common - Common functions used by DevStack components\n#\n# The canonical copy of this file is maintained in the DevStack repo.\n# All modifications should be made there and then sync'ed to other repos\n# as required.\n#\n# This file is sorted alphabetically within the function groups.\n#\n# - Config Functions\n# - Control Functions\n# - Distro Functions\n# - Git Functions\n# - OpenStack Functions\n# - Package Functions\n# - Process Functions\n# - Service Functions\n# - System Functions\n#\n# The following variables are assumed to be defined by certain functions:\n#\n# - ``ENABLED_SERVICES``\n# - ``ERROR_ON_CLONE``\n# - ``FILES``\n# - ``OFFLINE``\n# - ``RECLONE``\n# - ``REQUIREMENTS_DIR``\n# - ``STACK_USER``\n# - ``http_proxy``, ``https_proxy``, ``no_proxy``\n#\n\n# Save trace setting\n_XTRACE_FUNCTIONS_COMMON=$(set +o | grep xtrace)\nset +o xtrace\n\n# ensure we don't re-source this in the same environment\n[[ -z \"$_DEVSTACK_FUNCTIONS_COMMON\" ]] || return 0\ndeclare -r -g _DEVSTACK_FUNCTIONS_COMMON=1\n\n# Global Config Variables\ndeclare -A -g GITREPO\ndeclare -A -g GITBRANCH\ndeclare -A -g GITDIR\n\nKILL_PATH=\"$(which kill)\"\n\n# Save these variables to .stackenv\nSTACK_ENV_VARS=\"BASE_SQL_CONN DATA_DIR DEST ENABLED_SERVICES HOST_IP \\\n    KEYSTONE_SERVICE_URI \\\n    LOGFILE OS_CACERT SERVICE_HOST STACK_USER TLS_IP \\\n    HOST_IPV6 SERVICE_IP_VERSION TUNNEL_ENDPOINT_IP TUNNEL_IP_VERSION\"\n\n\n# Saves significant environment variables to .stackenv for later use\n# Refers to a lot of globals, only TOP_DIR and STACK_ENV_VARS are required to\n# function, the rest are simply saved and do not cause problems if they are undefined.\n# save_stackenv [tag]\nfunction save_stackenv {\n    local tag=${1:-\"\"}\n    # Save some values we generated for later use\n    time_stamp=$(date \"+$TIMESTAMP_FORMAT\")\n    echo \"# $time_stamp $tag\" >$TOP_DIR/.stackenv\n    for i in $STACK_ENV_VARS; do\n        echo $i=${!i} >>$TOP_DIR/.stackenv\n    done\n}\n\n# Update/create user clouds.yaml file.\n# clouds.yaml will have\n# - A `devstack` entry for the `demo` user for the `demo` project.\n# - A `devstack-admin` entry for the `admin` user for the `admin` project.\n# write_clouds_yaml\nfunction write_clouds_yaml {\n    # The location is a variable to allow for easier refactoring later to make it\n    # overridable. There is currently no usecase where doing so makes sense, so\n    # it's not currently configurable.\n\n    CLOUDS_YAML=/etc/openstack/clouds.yaml\n\n    sudo mkdir -p $(dirname $CLOUDS_YAML)\n    sudo chown -R $STACK_USER /etc/openstack\n\n    CA_CERT_ARG=''\n    if [ -f \"$SSL_BUNDLE_FILE\" ]; then\n        CA_CERT_ARG=\"--os-cacert $SSL_BUNDLE_FILE\"\n    fi\n    # devstack: user with the member role on demo project\n    $PYTHON $TOP_DIR/tools/update_clouds_yaml.py \\\n        --file $CLOUDS_YAML \\\n        --os-cloud devstack \\\n        --os-region-name $REGION_NAME \\\n        $CA_CERT_ARG \\\n        --os-auth-url $KEYSTONE_SERVICE_URI \\\n        --os-username demo \\\n        --os-password $ADMIN_PASSWORD \\\n        --os-project-name demo\n\n    # devstack-admin: user with the admin role on the admin project\n    $PYTHON $TOP_DIR/tools/update_clouds_yaml.py \\\n        --file $CLOUDS_YAML \\\n        --os-cloud devstack-admin \\\n        --os-region-name $REGION_NAME \\\n        $CA_CERT_ARG \\\n        --os-auth-url $KEYSTONE_SERVICE_URI \\\n        --os-username admin \\\n        --os-password $ADMIN_PASSWORD \\\n        --os-project-name admin\n\n    # devstack-admin-demo: user with the admin role on the demo project\n    $PYTHON $TOP_DIR/tools/update_clouds_yaml.py \\\n        --file $CLOUDS_YAML \\\n        --os-cloud devstack-admin-demo \\\n        --os-region-name $REGION_NAME \\\n        $CA_CERT_ARG \\\n        --os-auth-url $KEYSTONE_SERVICE_URI \\\n        --os-username admin \\\n        --os-password $ADMIN_PASSWORD \\\n        --os-project-name demo\n\n    # devstack-alt: user with the member role on alt_demo project\n    $PYTHON $TOP_DIR/tools/update_clouds_yaml.py \\\n        --file $CLOUDS_YAML \\\n        --os-cloud devstack-alt \\\n        --os-region-name $REGION_NAME \\\n        $CA_CERT_ARG \\\n        --os-auth-url $KEYSTONE_SERVICE_URI \\\n        --os-username alt_demo \\\n        --os-password $ADMIN_PASSWORD \\\n        --os-project-name alt_demo\n\n    # devstack-alt-member: user with the member role on alt_demo project\n    $PYTHON $TOP_DIR/tools/update_clouds_yaml.py \\\n        --file $CLOUDS_YAML \\\n        --os-cloud devstack-alt-member \\\n        --os-region-name $REGION_NAME \\\n        $CA_CERT_ARG \\\n        --os-auth-url $KEYSTONE_SERVICE_URI \\\n        --os-username alt_demo_member \\\n        --os-password $ADMIN_PASSWORD \\\n        --os-project-name alt_demo\n\n    # devstack-alt-reader: user with the reader role on alt_demo project\n    $PYTHON $TOP_DIR/tools/update_clouds_yaml.py \\\n        --file $CLOUDS_YAML \\\n        --os-cloud devstack-alt-reader \\\n        --os-region-name $REGION_NAME \\\n        $CA_CERT_ARG \\\n        --os-auth-url $KEYSTONE_SERVICE_URI \\\n        --os-username alt_demo_reader \\\n        --os-password $ADMIN_PASSWORD \\\n        --os-project-name alt_demo\n\n    # devstack-reader: user with the reader role on demo project\n    $PYTHON $TOP_DIR/tools/update_clouds_yaml.py \\\n        --file $CLOUDS_YAML \\\n        --os-cloud devstack-reader \\\n        --os-region-name $REGION_NAME \\\n        $CA_CERT_ARG \\\n        --os-auth-url $KEYSTONE_SERVICE_URI \\\n        --os-username demo_reader \\\n        --os-password $ADMIN_PASSWORD \\\n        --os-project-name demo\n\n    # devstack-system-admin: user with the admin role on the system\n    $PYTHON $TOP_DIR/tools/update_clouds_yaml.py \\\n        --file $CLOUDS_YAML \\\n        --os-cloud devstack-system-admin \\\n        --os-region-name $REGION_NAME \\\n        $CA_CERT_ARG \\\n        --os-auth-url $KEYSTONE_SERVICE_URI \\\n        --os-username admin \\\n        --os-password $ADMIN_PASSWORD \\\n        --os-system-scope all\n\n    # devstack-system-member: user with the member role on the system\n    $PYTHON $TOP_DIR/tools/update_clouds_yaml.py \\\n        --file $CLOUDS_YAML \\\n        --os-cloud devstack-system-member \\\n        --os-region-name $REGION_NAME \\\n        $CA_CERT_ARG \\\n        --os-auth-url $KEYSTONE_SERVICE_URI \\\n        --os-username system_member \\\n        --os-password $ADMIN_PASSWORD \\\n        --os-system-scope all\n\n    # devstack-system-reader: user with the reader role on the system\n    $PYTHON $TOP_DIR/tools/update_clouds_yaml.py \\\n        --file $CLOUDS_YAML \\\n        --os-cloud devstack-system-reader \\\n        --os-region-name $REGION_NAME \\\n        $CA_CERT_ARG \\\n        --os-auth-url $KEYSTONE_SERVICE_URI \\\n        --os-username system_reader \\\n        --os-password $ADMIN_PASSWORD \\\n        --os-system-scope all\n\n    cat >> $CLOUDS_YAML <<EOF\nfunctional:\n  image_name: $DEFAULT_IMAGE_NAME\nEOF\n\n    # CLean up any old clouds.yaml files we had laying around\n    rm -f $(eval echo ~\"$STACK_USER\")/.config/openstack/clouds.yaml\n}\n\n# trueorfalse <True|False> <VAR>\n#\n# Normalize config-value provided in variable VAR to either \"True\" or\n# \"False\".  If VAR is unset (i.e. $VAR evaluates as empty), the value\n# of the second argument will be used as the default value.\n#\n#  Accepts as False: 0 no  No  NO  false False FALSE\n#  Accepts as True:  1 yes Yes YES true  True  TRUE\n#\n# usage:\n#  VAL=$(trueorfalse False VAL)\nfunction trueorfalse {\n    local xtrace\n    xtrace=$(set +o | grep xtrace)\n    set +o xtrace\n\n    local default=$1\n\n    if [ -z $2 ]; then\n        die $LINENO \"variable to normalize required\"\n    fi\n    local testval=${!2:-}\n\n    case \"$testval\" in\n        \"1\" | [yY]es | \"YES\" | [tT]rue | \"TRUE\" ) echo \"True\" ;;\n        \"0\" | [nN]o | \"NO\" | [fF]alse | \"FALSE\" ) echo \"False\" ;;\n        * )                                       echo \"$default\" ;;\n    esac\n\n    $xtrace\n}\n\n# bool_to_int <True|False>\n#\n# Convert True|False to int 1 or 0\n# This function can be used to convert the output of trueorfalse\n# to an int follow c conventions where false is 0 and 1 it true.\nfunction bool_to_int {\n    local xtrace\n    xtrace=$(set +o | grep xtrace)\n    set +o xtrace\n    if [ -z $1 ]; then\n        die $LINENO \"Bool value required\"\n    fi\n    if [[ $1 == \"True\" ]] ; then\n        echo '1'\n    else\n        echo '0'\n    fi\n    $xtrace\n}\n\n\nfunction isset {\n    [[ -v \"$1\" ]]\n}\n\n\n# Control Functions\n# =================\n\n# Prints backtrace info\n# filename:lineno:function\n# backtrace level\nfunction backtrace {\n    local level=$1\n    local deep\n    deep=$((${#BASH_SOURCE[@]} - 1))\n    echo \"[Call Trace]\"\n    while [ $level -le $deep ]; do\n        echo \"${BASH_SOURCE[$deep]}:${BASH_LINENO[$deep-1]}:${FUNCNAME[$deep-1]}\"\n        deep=$((deep - 1))\n    done\n}\n\n# Prints line number and \"message\" then exits\n# die $LINENO \"message\"\nfunction die {\n    local exitcode=$?\n    set +o xtrace\n    local line=$1; shift\n    if [ $exitcode == 0 ]; then\n        exitcode=1\n    fi\n    backtrace 2\n    err $line \"$*\"\n    # Give buffers a second to flush\n    sleep 1\n    exit $exitcode\n}\n\n# Checks an environment variable is not set or has length 0 OR if the\n# exit code is non-zero and prints \"message\" and exits\n# NOTE: env-var is the variable name without a '$'\n# die_if_not_set $LINENO env-var \"message\"\nfunction die_if_not_set {\n    local exitcode=$?\n    local xtrace\n    xtrace=$(set +o | grep xtrace)\n    set +o xtrace\n    local line=$1; shift\n    local evar=$1; shift\n    if ! is_set $evar || [ $exitcode != 0 ]; then\n        die $line \"$*\"\n    fi\n    $xtrace\n}\n\nfunction deprecated {\n    local text=$1\n    DEPRECATED_TEXT+=\"\\n$text\"\n    echo \"WARNING: $text\" >&2\n}\n\n# Prints line number and \"message\" in error format\n# err $LINENO \"message\"\nfunction err {\n    local exitcode=$?\n    local xtrace\n    xtrace=$(set +o | grep xtrace)\n    set +o xtrace\n    local msg=\"[ERROR] ${BASH_SOURCE[2]}:$1 $2\"\n    echo \"$msg\" 1>&2;\n    if [[ -n ${LOGDIR} ]]; then\n        echo \"$msg\" >> \"${LOGDIR}/error.log\"\n    fi\n    $xtrace\n    return $exitcode\n}\n\n# Checks an environment variable is not set or has length 0 OR if the\n# exit code is non-zero and prints \"message\"\n# NOTE: env-var is the variable name without a '$'\n# err_if_not_set $LINENO env-var \"message\"\nfunction err_if_not_set {\n    local exitcode=$?\n    local xtrace\n    xtrace=$(set +o | grep xtrace)\n    set +o xtrace\n    local line=$1; shift\n    local evar=$1; shift\n    if ! is_set $evar || [ $exitcode != 0 ]; then\n        err $line \"$*\"\n    fi\n    $xtrace\n    return $exitcode\n}\n\n# Exit after outputting a message about the distribution not being supported.\n# exit_distro_not_supported [optional-string-telling-what-is-missing]\nfunction exit_distro_not_supported {\n    if [[ -z \"$DISTRO\" ]]; then\n        GetDistro\n    fi\n\n    if [ $# -gt 0 ]; then\n        die $LINENO \"Support for $DISTRO is incomplete: no support for $@\"\n    else\n        die $LINENO \"Support for $DISTRO is incomplete.\"\n    fi\n}\n\n# Test if the named environment variable is set and not zero length\n# is_set env-var\nfunction is_set {\n    local var=\\$\"$1\"\n    eval \"[ -n \\\"$var\\\" ]\" # For ex.: sh -c \"[ -n \\\"$var\\\" ]\" would be better, but several exercises depends on this\n}\n\n# Prints line number and \"message\" in warning format\n# warn $LINENO \"message\"\nfunction warn {\n    local exitcode=$?\n    local xtrace\n    xtrace=$(set +o | grep xtrace)\n    set +o xtrace\n    local msg=\"[WARNING] ${BASH_SOURCE[2]}:$1 $2\"\n    echo \"$msg\"\n    $xtrace\n    return $exitcode\n}\n\n\n# Distro Functions\n# ================\n\n# Determine OS Vendor, Release and Update\n\n#\n# NOTE : For portability, you almost certainly do not want to use\n# these variables directly!  The \"is_*\" functions defined below this\n# bundle up compatible platforms under larger umbrellas that we have\n# determinted are compatible enough (e.g. is_ubuntu covers Ubuntu &\n# Debian, is_fedora covers RPM-based distros).  Higher-level functions\n# such as \"install_package\" further abstract things in better ways.\n#\n# ``os_VENDOR`` - vendor name: ``Ubuntu``, ``Fedora``, etc\n# ``os_RELEASE`` - major release: ``22.04`` (Ubuntu), ``23`` (Fedora)\n# ``os_PACKAGE`` - package type: ``deb`` or ``rpm``\n# ``os_CODENAME`` - vendor's codename for release: ``jammy``\n\ndeclare -g os_VENDOR os_RELEASE os_PACKAGE os_CODENAME\n\n# Make a *best effort* attempt to install lsb_release packages for the\n# user if not available.  Note can't use generic install_package*\n# because they depend on this!\nfunction _ensure_lsb_release {\n    if [[ -x $(command -v lsb_release 2>/dev/null) ]]; then\n        return\n    fi\n\n    if [[ -x $(command -v apt-get 2>/dev/null) ]]; then\n        sudo apt-get install -y lsb-release\n    elif [[ -x $(command -v zypper 2>/dev/null) ]]; then\n        sudo zypper -n install lsb-release\n    elif [[ -x $(command -v dnf 2>/dev/null) ]]; then\n        sudo dnf install -y redhat-lsb-core || sudo dnf install -y openeuler-lsb\n    else\n        die $LINENO \"Unable to find or auto-install lsb_release\"\n    fi\n}\n\n# GetOSVersion\n#  Set the following variables:\n#  - os_RELEASE\n#  - os_CODENAME\n#  - os_VENDOR\n#  - os_PACKAGE\nfunction GetOSVersion {\n    # CentOS Stream 9 and RHEL 9 do not provide lsb_release\n    source /etc/os-release\n    if [[ \"${ID}${VERSION}\" == \"centos9\" ]] || [[ \"${ID}${VERSION}\" =~ \"rhel9\" ]]; then\n        os_RELEASE=${VERSION_ID}\n        os_CODENAME=\"n/a\"\n        os_VENDOR=$(echo $NAME | tr -d '[:space:]')\n    elif [[ \"${ID}${VERSION}\" =~ \"rocky9\" ]]; then\n        os_VENDOR=\"Rocky\"\n        os_RELEASE=${VERSION_ID}\n    else\n        _ensure_lsb_release\n\n        os_RELEASE=$(lsb_release -r -s)\n        os_CODENAME=$(lsb_release -c -s)\n        os_VENDOR=$(lsb_release -i -s)\n    fi\n\n    if [[ $os_VENDOR =~ (Debian|Ubuntu) ]]; then\n        os_PACKAGE=\"deb\"\n    else\n        os_PACKAGE=\"rpm\"\n    fi\n\n    typeset -xr os_VENDOR\n    typeset -xr os_RELEASE\n    typeset -xr os_PACKAGE\n    typeset -xr os_CODENAME\n}\n\n# Translate the OS version values into common nomenclature\n# Sets global ``DISTRO`` from the ``os_*`` values\ndeclare -g DISTRO\n\nfunction GetDistro {\n    GetOSVersion\n    if [[ \"$os_VENDOR\" =~ (Ubuntu) || \"$os_VENDOR\" =~ (Debian) ]]; then\n        # 'Everyone' refers to Ubuntu / Debian releases by\n        # the code name adjective\n        DISTRO=$os_CODENAME\n    elif [[ \"$os_VENDOR\" =~ (Fedora) ]]; then\n        # For Fedora, just use 'f' and the release\n        DISTRO=\"f$os_RELEASE\"\n    elif [[ \"$os_VENDOR\" =~ (Red.*Hat) || \\\n        \"$os_VENDOR\" =~ (CentOS) || \\\n        \"$os_VENDOR\" =~ (AlmaLinux) || \\\n        \"$os_VENDOR\" =~ (Scientific) || \\\n        \"$os_VENDOR\" =~ (OracleServer) || \\\n        \"$os_VENDOR\" =~ (Rocky) || \\\n        \"$os_VENDOR\" =~ (Virtuozzo) ]]; then\n        # Drop the . release as we assume it's compatible\n        # XXX re-evaluate when we get RHEL10\n        DISTRO=\"rhel${os_RELEASE::1}\"\n    elif [[ \"$os_VENDOR\" =~ (openEuler) ]]; then\n        DISTRO=\"openEuler-$os_RELEASE\"\n    else\n        # We can't make a good choice here.  Setting a sensible DISTRO\n        # is part of the problem, but not the major issue -- we really\n        # only use DISTRO in the code as a fine-filter.\n        #\n        # The bigger problem is categorising the system into one of\n        # our two big categories as Ubuntu/Debian-ish or\n        # Fedora/CentOS-ish.\n        #\n        # The setting of os_PACKAGE above is only set to \"deb\" based\n        # on a hard-coded list of vendor names ... thus we will\n        # default to thinking unknown distros are RPM based\n        # (ie. is_ubuntu does not match).  But the platform will then\n        # also not match in is_fedora, because that also has a list of\n        # names.\n        #\n        # So, if you are reading this, getting your distro supported\n        # is really about making sure it matches correctly in these\n        # functions.  Then you can choose a sensible way to construct\n        # DISTRO based on your distros release approach.\n        die $LINENO \"Unable to determine DISTRO, can not continue.\"\n    fi\n    typeset -xr DISTRO\n}\n\n# Utility function for checking machine architecture\n# is_arch arch-type\nfunction is_arch {\n    [[ \"$(uname -m)\" == \"$1\" ]]\n}\n\n# Determine if current distribution is an Oracle distribution\n# is_oraclelinux\nfunction is_oraclelinux {\n    if [[ -z \"$os_VENDOR\" ]]; then\n        GetOSVersion\n    fi\n\n    [ \"$os_VENDOR\" = \"OracleServer\" ]\n}\n\n\n# Determine if current distribution is a Fedora-based distribution\n# (Fedora, RHEL, CentOS, Rocky, etc).\n# is_fedora\nfunction is_fedora {\n    if [[ -z \"$os_VENDOR\" ]]; then\n        GetOSVersion\n    fi\n\n    [ \"$os_VENDOR\" = \"Fedora\" ] || [ \"$os_VENDOR\" = \"Red Hat\" ] || \\\n        [ \"$os_VENDOR\" = \"openEuler\" ] || \\\n        [ \"$os_VENDOR\" = \"RedHatEnterpriseServer\" ] || \\\n        [ \"$os_VENDOR\" = \"RedHatEnterprise\" ] || \\\n        [ \"$os_VENDOR\" = \"RedHatEnterpriseLinux\" ] || \\\n        [ \"$os_VENDOR\" = \"Rocky\" ] || \\\n        [ \"$os_VENDOR\" = \"CentOS\" ] || [ \"$os_VENDOR\" = \"CentOSStream\" ] || \\\n        [ \"$os_VENDOR\" = \"AlmaLinux\" ] || \\\n        [ \"$os_VENDOR\" = \"OracleServer\" ] || [ \"$os_VENDOR\" = \"Virtuozzo\" ]\n}\n\n\n# Determine if current distribution is an Ubuntu-based distribution\n# It will also detect non-Ubuntu but Debian-based distros\n# is_ubuntu\nfunction is_ubuntu {\n    if [[ -z \"$os_PACKAGE\" ]]; then\n        GetOSVersion\n    fi\n    [ \"$os_PACKAGE\" = \"deb\" ]\n}\n\n# Determine if current distribution is an openEuler distribution\n# is_openeuler\nfunction is_openeuler {\n    if [[ -z \"$os_PACKAGE\" ]]; then\n        GetOSVersion\n    fi\n    [ \"$os_VENDOR\" = \"openEuler\" ]\n}\n# Git Functions\n# =============\n\n# Returns openstack release name for a given branch name\n# ``get_release_name_from_branch branch-name``\nfunction get_release_name_from_branch {\n    local branch=$1\n    if [[ $branch =~ \"stable/\" || $branch =~ \"proposed/\" ]]; then\n        echo ${branch#*/}\n    else\n        echo \"master\"\n    fi\n}\n\n# git clone only if directory doesn't exist already.  Since ``DEST`` might not\n# be owned by the installation user, we create the directory and change the\n# ownership to the proper user.\n# Set global ``RECLONE=yes`` to simulate a clone when dest-dir exists\n# Set global ``ERROR_ON_CLONE=True`` to abort execution with an error if the git repo\n# does not exist (default is False, meaning the repo will be cloned).\n# Uses globals ``ERROR_ON_CLONE``, ``OFFLINE``, ``RECLONE``\n# git_clone remote dest-dir branch\nfunction git_clone {\n    local git_remote=$1\n    local git_dest=$2\n    local git_ref=$3\n    local orig_dir\n    orig_dir=$(pwd)\n    local git_clone_flags=\"\"\n\n    RECLONE=$(trueorfalse False RECLONE)\n    if [[ \"${GIT_DEPTH}\" -gt 0 ]]; then\n        git_clone_flags=\"$git_clone_flags --depth $GIT_DEPTH\"\n    fi\n\n    if [[ \"$OFFLINE\" = \"True\" ]]; then\n        echo \"Running in offline mode, clones already exist\"\n        # print out the results so we know what change was used in the logs\n        cd $git_dest\n        git show --oneline | head -1\n        cd $orig_dir\n        return\n    fi\n\n    if echo $git_ref | egrep -q \"^refs\"; then\n        # If our branch name is a gerrit style refs/changes/...\n        if [[ ! -d $git_dest ]]; then\n            if [[ \"$ERROR_ON_CLONE\" = \"True\" ]]; then\n                echo \"The $git_dest project was not found; if this is a gate job, add\"\n                echo \"the project to 'required-projects' in the job definition.\"\n                die $LINENO \"ERROR_ON_CLONE is set to True so cloning not allowed in this configuration\"\n            fi\n            git_timed clone $git_clone_flags $git_remote $git_dest\n        fi\n        cd $git_dest\n        git_timed fetch $git_remote $git_ref && git checkout FETCH_HEAD\n    else\n        # do a full clone only if the directory doesn't exist\n        if [[ ! -d $git_dest ]]; then\n            if [[ \"$ERROR_ON_CLONE\" = \"True\" ]]; then\n                echo \"The $git_dest project was not found; if this is a gate job, add\"\n                echo \"the project to the \\$PROJECTS variable in the job definition.\"\n                die $LINENO \"ERROR_ON_CLONE is set to True so cloning not allowed in this configuration\"\n            fi\n            git_timed clone --no-checkout $git_clone_flags $git_remote $git_dest\n            cd $git_dest\n            git_timed fetch $git_clone_flags origin $git_ref\n            git_timed checkout FETCH_HEAD\n        elif [[ \"$RECLONE\" = \"True\" ]]; then\n            # if it does exist then simulate what clone does if asked to RECLONE\n            cd $git_dest\n            # set the url to pull from and fetch\n            git remote set-url origin $git_remote\n            git_timed fetch origin\n            # remove the existing ignored files (like pyc) as they cause breakage\n            # (due to the py files having older timestamps than our pyc, so python\n            # thinks the pyc files are correct using them)\n            sudo find $git_dest -name '*.pyc' -delete\n\n            # handle git_ref accordingly to type (tag, branch)\n            if [[ -n \"`git show-ref refs/tags/$git_ref`\" ]]; then\n                git_update_tag $git_ref\n            elif [[ -n \"`git show-ref refs/heads/$git_ref`\" ]]; then\n                git_update_branch $git_ref\n            elif [[ -n \"`git show-ref refs/remotes/origin/$git_ref`\" ]]; then\n                git_update_remote_branch $git_ref\n            else\n                die $LINENO \"$git_ref is neither branch nor tag\"\n            fi\n\n        fi\n    fi\n\n    # NOTE(ianw) 2022-04-13 : commit [1] has broken many assumptions\n    # about how we clone and work with repos.  Mark them safe globally\n    # as a work-around.\n    #\n    # NOTE(danms): On bionic (and likely others) git-config may write\n    # ~stackuser/.gitconfig if not run with sudo -H. Using --system\n    # writes these changes to /etc/gitconfig which is more\n    # discoverable anyway.\n    #\n    # [1] https://github.com/git/git/commit/8959555cee7ec045958f9b6dd62e541affb7e7d9\n    sudo git config --system --add safe.directory ${git_dest}\n\n    # print out the results so we know what change was used in the logs\n    cd $git_dest\n    git show --oneline | head -1\n    cd $orig_dir\n}\n\n# A variation on git clone that lets us specify a project by it's\n# actual name, like oslo.config. This is exceptionally useful in the\n# library installation case\nfunction git_clone_by_name {\n    local name=$1\n    local repo=${GITREPO[$name]}\n    local dir=${GITDIR[$name]}\n    local branch=${GITBRANCH[$name]}\n    git_clone $repo $dir $branch\n}\n\n\n# git can sometimes get itself infinitely stuck with transient network\n# errors or other issues with the remote end.  This wraps git in a\n# timeout/retry loop and is intended to watch over non-local git\n# processes that might hang.  GIT_TIMEOUT, if set, is passed directly\n# to timeout(1); otherwise the default value of 0 maintains the status\n# quo of waiting forever.\n# usage: git_timed <git-command>\nfunction git_timed {\n    local count=0\n    local timeout=0\n\n    if [[ -n \"${GIT_TIMEOUT}\" ]]; then\n        timeout=${GIT_TIMEOUT}\n    fi\n\n    time_start \"git_timed\"\n    until timeout -s SIGINT ${timeout} git \"$@\"; do\n        # 124 is timeout(1)'s special return code when it reached the\n        # timeout; otherwise assume fatal failure\n        if [[ $? -ne 124 ]]; then\n            die $LINENO \"git call failed: [git $@]\"\n        fi\n\n        count=$(($count + 1))\n        warn $LINENO \"timeout ${count} for git call: [git $@]\"\n        if [ $count -eq 3 ]; then\n            die $LINENO \"Maximum of 3 git retries reached\"\n        fi\n        sleep 5\n    done\n    time_stop \"git_timed\"\n}\n\n# git update using reference as a branch.\n# git_update_branch ref\nfunction git_update_branch {\n    local git_branch=$1\n\n    git checkout -f origin/$git_branch\n    # a local branch might not exist\n    git branch -D $git_branch || true\n    git checkout -b $git_branch\n}\n\n# git update using reference as a branch.\n# git_update_remote_branch ref\nfunction git_update_remote_branch {\n    local git_branch=$1\n\n    git checkout -b $git_branch -t origin/$git_branch\n}\n\n# git update using reference as a tag. Be careful editing source at that repo\n# as working copy will be in a detached mode\n# git_update_tag ref\nfunction git_update_tag {\n    local git_tag=$1\n\n    git tag -d $git_tag\n    # fetching given tag only\n    git_timed fetch origin tag $git_tag\n    git checkout -f $git_tag\n}\n\n\n# OpenStack Functions\n# ===================\n\n# Get the default value for HOST_IP\n# get_default_host_ip fixed_range floating_range host_ip_iface host_ip\nfunction get_default_host_ip {\n    local fixed_range=$1\n    local floating_range=$2\n    local host_ip_iface=$3\n    local host_ip=$4\n    local af=$5\n\n    # Search for an IP unless an explicit is set by ``HOST_IP`` environment variable\n    if [ -z \"$host_ip\" -o \"$host_ip\" == \"dhcp\" ]; then\n        host_ip=\"\"\n        # Find the interface used for the default route\n        host_ip_iface=${host_ip_iface:-$(ip -f $af route list match default table all | grep via | awk '/default/ {print $5}' | head -1)}\n        local host_ips\n        host_ips=$(LC_ALL=C ip -f $af addr show ${host_ip_iface} | sed /temporary/d |awk /$af'/ {split($2,parts,\"/\");  print parts[1]}')\n        local ip\n        for ip in $host_ips; do\n            # Attempt to filter out IP addresses that are part of the fixed and\n            # floating range. Note that this method only works if the ``netaddr``\n            # python library is installed. If it is not installed, an error\n            # will be printed and the first IP from the interface will be used.\n            # If that is not correct set ``HOST_IP`` in ``localrc`` to the correct\n            # address.\n            if [[ \"$af\" == \"inet6\" ]]; then\n                host_ip=$ip\n                break;\n            fi\n            if ! (address_in_net $ip $fixed_range || address_in_net $ip $floating_range); then\n                host_ip=$ip\n                break;\n            fi\n        done\n    fi\n    echo $host_ip\n}\n\n# Generates hex string from ``size`` byte of pseudo random data\n# generate_hex_string size\nfunction generate_hex_string {\n    local size=$1\n    hexdump -n \"$size\" -v -e '/1 \"%02x\"' /dev/urandom\n}\n\n# Grab a numbered field from python prettytable output\n# Fields are numbered starting with 1\n# Reverse syntax is supported: -1 is the last field, -2 is second to last, etc.\n# get_field field-number\nfunction get_field {\n    local data field\n    while read data; do\n        if [ \"$1\" -lt 0 ]; then\n            field=\"(\\$(NF$1))\"\n        else\n            field=\"\\$$(($1 + 1))\"\n        fi\n        echo \"$data\" | awk -F'[ \\t]*\\\\|[ \\t]*' \"{print $field}\"\n    done\n}\n\n# install default policy\n# copy over a default policy.json and policy.d for projects\nfunction install_default_policy {\n    local project=$1\n    local project_uc\n    project_uc=$(echo $1|tr a-z A-Z)\n    local conf_dir=\"${project_uc}_CONF_DIR\"\n    # eval conf dir to get the variable\n    conf_dir=\"${!conf_dir}\"\n    local project_dir=\"${project_uc}_DIR\"\n    # eval project dir to get the variable\n    project_dir=\"${!project_dir}\"\n    local sample_conf_dir=\"${project_dir}/etc/${project}\"\n    local sample_policy_dir=\"${project_dir}/etc/${project}/policy.d\"\n\n    # first copy any policy.json\n    cp -p $sample_conf_dir/policy.json $conf_dir\n    # then optionally copy over policy.d\n    if [[ -d $sample_policy_dir ]]; then\n        cp -r $sample_policy_dir $conf_dir/policy.d\n    fi\n}\n\n# Add a policy to a policy.json file\n# Do nothing if the policy already exists\n# ``policy_add policy_file policy_name policy_permissions``\nfunction policy_add {\n    local policy_file=$1\n    local policy_name=$2\n    local policy_perm=$3\n\n    if grep -q ${policy_name} ${policy_file}; then\n        echo \"Policy ${policy_name} already exists in ${policy_file}\"\n        return\n    fi\n\n    # Add a terminating comma to policy lines without one\n    # Remove the closing '}' and all lines following to the end-of-file\n    local tmpfile\n    tmpfile=$(mktemp)\n    uniq ${policy_file} | sed -e '\n        s/]$/],/\n        /^[}]/,$d\n    ' > ${tmpfile}\n\n    # Append policy and closing brace\n    echo \"    \\\"${policy_name}\\\": ${policy_perm}\" >>${tmpfile}\n    echo \"}\" >>${tmpfile}\n\n    mv ${tmpfile} ${policy_file}\n}\n\n# Gets or creates a domain\n# Usage: get_or_create_domain <name> <description>\nfunction get_or_create_domain {\n    local domain_id\n    domain_id=$(\n        openstack --os-cloud devstack-system-admin domain create $1 \\\n            --description \"$2\" --or-show \\\n            -f value -c id\n    )\n    echo $domain_id\n}\n\n# Gets or creates group\n# Usage: get_or_create_group <groupname> <domain> [<description>]\nfunction get_or_create_group {\n    local desc=\"${3:-}\"\n    local group_id\n    # Gets group id\n    group_id=$(\n        # Creates new group with --or-show\n        openstack --os-cloud devstack-system-admin group create $1 \\\n            --domain $2 --description \"$desc\" --or-show \\\n            -f value -c id\n    )\n    echo $group_id\n}\n\n# Gets or creates user\n# Usage: get_or_create_user <username> <password> <domain> [<email>]\nfunction get_or_create_user {\n    local user_id\n    if [[ ! -z \"$4\" ]]; then\n        local email=\"--email=$4\"\n    else\n        local email=\"\"\n    fi\n    # Gets user id\n    user_id=$(\n        # Creates new user with --or-show\n        openstack --os-cloud devstack-system-admin user create \\\n            $1 \\\n            --password \"$2\" \\\n            --domain=$3 \\\n            $email \\\n            --or-show \\\n            -f value -c id\n    )\n    echo $user_id\n}\n\n# Gets or creates project\n# Usage: get_or_create_project <name> <domain>\nfunction get_or_create_project {\n    local project_id\n    project_id=$(\n        # Creates new project with --or-show\n        openstack --os-cloud devstack-system-admin project create $1 \\\n            --domain=$2 \\\n            --or-show -f value -c id\n    )\n    echo $project_id\n}\n\n# Gets or creates role\n# Usage: get_or_create_role <name>\nfunction get_or_create_role {\n    local role_id\n    role_id=$(\n        # Creates role with --or-show\n        openstack --os-cloud devstack-system-admin role create $1 \\\n            --or-show -f value -c id\n    )\n    echo $role_id\n}\n\n# Returns the domain parts of a function call if present\n# Usage: _get_domain_args [<user_domain> <project_domain>]\nfunction _get_domain_args {\n    local domain\n    domain=\"\"\n\n    if [[ -n \"$1\" ]]; then\n        domain=\"$domain --user-domain $1\"\n    fi\n    if [[ -n \"$2\" ]]; then\n        domain=\"$domain --project-domain $2\"\n    fi\n\n    echo $domain\n}\n\n# Gets or adds user role to project\n# Usage: get_or_add_user_project_role <role> <user> <project> [<user_domain> <project_domain>]\nfunction get_or_add_user_project_role {\n    local user_role_id\n    local domain_args\n\n    domain_args=$(_get_domain_args $4 $5)\n\n    # Note this is idempotent so we are safe across multiple\n    # duplicate calls.\n    openstack --os-cloud devstack-system-admin role add $1 \\\n        --user $2 \\\n        --project $3 \\\n        $domain_args\n    user_role_id=$(openstack --os-cloud devstack-system-admin role assignment list \\\n        --role $1 \\\n        --user $2 \\\n        --project $3 \\\n        $domain_args \\\n        -c Role -f value)\n    echo $user_role_id\n}\n\n# Gets or adds user role to domain\n# Usage: get_or_add_user_domain_role <role> <user> <domain>\nfunction get_or_add_user_domain_role {\n    local user_role_id\n\n    # Note this is idempotent so we are safe across multiple\n    # duplicate calls.\n    openstack --os-cloud devstack-system-admin role add $1 \\\n        --user $2 \\\n        --domain $3\n    user_role_id=$(openstack --os-cloud devstack-system-admin role assignment list \\\n        --role $1 \\\n        --user $2 \\\n        --domain $3 \\\n        -c Role -f value)\n\n    echo $user_role_id\n}\n\n# Gets or adds user role to system\n# Usage: get_or_add_user_system_role <role> <user> <system> [<user_domain>]\nfunction get_or_add_user_system_role {\n    local user_role_id\n    local domain_args\n\n    domain_args=$(_get_domain_args $4)\n\n    # Gets user role id\n    user_role_id=$(openstack --os-cloud devstack-system-admin role assignment list \\\n        --role $1 \\\n        --user $2 \\\n        --system $3 \\\n        $domain_args \\\n        -f value -c Role)\n    if [[ -z \"$user_role_id\" ]]; then\n        # Adds role to user and get it\n        openstack --os-cloud devstack-system-admin role add $1 \\\n            --user $2 \\\n            --system $3 \\\n            $domain_args\n        user_role_id=$(openstack --os-cloud devstack-system-admin role assignment list \\\n            --role $1 \\\n            --user $2 \\\n            --system $3 \\\n            $domain_args \\\n            -f value -c Role)\n    fi\n    echo $user_role_id\n}\n\n# Gets or adds group role to project\n# Usage: get_or_add_group_project_role <role> <group> <project>\nfunction get_or_add_group_project_role {\n    local group_role_id\n\n    # Note this is idempotent so we are safe across multiple\n    # duplicate calls.\n    openstack role add $1 \\\n        --group $2 \\\n        --project $3\n    group_role_id=$(openstack --os-cloud devstack-system-admin role assignment list \\\n        --role $1 \\\n        --group $2 \\\n        --project $3 \\\n        -f value -c Role)\n\n    echo $group_role_id\n}\n\n# Gets or creates service\n# Usage: get_or_create_service <name> <type> <description>\nfunction get_or_create_service {\n    local service_id\n    # Gets service id\n    service_id=$(\n        # Gets service id\n        openstack --os-cloud devstack-system-admin service show $2 -f value -c id 2>/dev/null ||\n        # Creates new service if not exists\n        openstack --os-cloud devstack-system-admin service create \\\n            $2 \\\n            --name $1 \\\n            --description=\"$3\" \\\n            -f value -c id\n    )\n    echo $service_id\n}\n\n# Create an endpoint with a specific interface\n# Usage: _get_or_create_endpoint_with_interface <service> <interface> <url> <region>\nfunction _get_or_create_endpoint_with_interface {\n    local endpoint_id\n    endpoint_id=$(openstack --os-cloud devstack-system-admin endpoint list \\\n        --service $1 \\\n        --interface $2 \\\n        --region $4 \\\n        -c ID -f value)\n    if [[ -z \"$endpoint_id\" ]]; then\n        # Creates new endpoint\n        endpoint_id=$(openstack --os-cloud devstack-system-admin endpoint create \\\n            $1 $2 $3 --region $4 -f value -c id)\n    fi\n\n    echo $endpoint_id\n}\n\n# Gets or creates endpoint\n# Usage: get_or_create_endpoint <service> <region> <publicurl> [adminurl] [internalurl]\nfunction get_or_create_endpoint {\n    # NOTE(jamielennnox): when converting to v3 endpoint creation we go from\n    # creating one endpoint with multiple urls to multiple endpoints each with\n    # a different interface.  To maintain the existing function interface we\n    # create 3 endpoints and return the id of the public one. In reality\n    # returning the public id will not make a lot of difference as there are no\n    # scenarios currently that use the returned id. Ideally this behaviour\n    # should be pushed out to the service setups and let them create the\n    # endpoints they need.\n    local public_id\n    public_id=$(_get_or_create_endpoint_with_interface $1 public $3 $2)\n    # only create admin/internal urls if provided content for them\n    if [[ -n \"$4\" ]]; then\n        _get_or_create_endpoint_with_interface $1 admin $4 $2\n    fi\n    if [[ -n \"$5\" ]]; then\n        _get_or_create_endpoint_with_interface $1 internal $5 $2\n    fi\n    # return the public id to indicate success, and this is the endpoint most likely wanted\n    echo $public_id\n}\n\n# Get a URL from the identity service\n# Usage: get_endpoint_url <service> <interface>\nfunction get_endpoint_url {\n    echo $(openstack --os-cloud devstack-system-admin endpoint list \\\n            --service $1 --interface $2 \\\n            -c URL -f value)\n}\n\n# check if we are using ironic with hardware\n# TODO(jroll) this is a kludge left behind when ripping ironic code\n# out of tree, as it is used by nova and neutron.\n# figure out a way to refactor nova/neutron code to eliminate this\nfunction is_ironic_hardware {\n    is_service_enabled ironic && [[ \"$IRONIC_IS_HARDWARE\" == \"True\" ]] && return 0\n    return 1\n}\n\nfunction is_ironic_enforce_scope {\n    is_service_enabled ironic && [[ \"$IRONIC_ENFORCE_SCOPE\" == \"True\" || \"$ENFORCE_SCOPE\" == \"True\" ]] && return 0\n    return 1\n}\n\nfunction is_ironic_sharded {\n    # todo(JayF): Support >1 shard with multiple n-cpu instances for each\n    is_service_enabled ironic && [[ \"$IRONIC_SHARDS\" == \"1\" ]] && return 0\n    return 1\n}\n\n\n# Package Functions\n# =================\n\n# _get_package_dir\nfunction _get_package_dir {\n    local base_dir=$1\n    local pkg_dir\n\n    if [[ -z \"$base_dir\" ]]; then\n        base_dir=$FILES\n    fi\n    if is_ubuntu; then\n        pkg_dir=$base_dir/debs\n    elif is_fedora; then\n        pkg_dir=$base_dir/rpms\n    else\n        exit_distro_not_supported \"list of packages\"\n    fi\n    echo \"$pkg_dir\"\n}\n\n# Wrapper for ``apt-get update`` to try multiple times on the update\n# to address bad package mirrors (which happen all the time).\nfunction apt_get_update {\n    # only do this once per run\n    if [[ \"$REPOS_UPDATED\" == \"True\" && \"$RETRY_UPDATE\" != \"True\" ]]; then\n        return\n    fi\n\n    # bail if we are offline\n    [[ \"$OFFLINE\" = \"True\" ]] && return\n\n    local sudo=\"sudo\"\n    [[ \"$(id -u)\" = \"0\" ]] && sudo=\"env\"\n\n    # time all the apt operations\n    time_start \"apt-get-update\"\n\n    local proxies=\"http_proxy=${http_proxy:-} https_proxy=${https_proxy:-} no_proxy=${no_proxy:-} \"\n    local update_cmd=\"$sudo $proxies apt-get update\"\n    if ! timeout 300 sh -c \"while ! $update_cmd; do sleep 30; done\"; then\n        die $LINENO \"Failed to update apt repos, we're dead now\"\n    fi\n\n    REPOS_UPDATED=True\n    # stop the clock\n    time_stop \"apt-get-update\"\n}\n\n# Wrapper for ``apt-get`` to set cache and proxy environment variables\n# Uses globals ``OFFLINE``, ``*_proxy``\n# apt_get operation package [package ...]\nfunction apt_get {\n    local xtrace result\n    xtrace=$(set +o | grep xtrace)\n    set +o xtrace\n\n    [[ \"$OFFLINE\" = \"True\" || -z \"$@\" ]] && return\n    local sudo=\"sudo\"\n    [[ \"$(id -u)\" = \"0\" ]] && sudo=\"env\"\n\n    # time all the apt operations\n    time_start \"apt-get\"\n\n    $xtrace\n\n    $sudo DEBIAN_FRONTEND=noninteractive \\\n        http_proxy=${http_proxy:-} https_proxy=${https_proxy:-} \\\n        no_proxy=${no_proxy:-} \\\n        apt-get --option \"Dpkg::Options::=--force-confold\" --assume-yes \"$@\" < /dev/null\n    result=$?\n\n    # stop the clock\n    time_stop \"apt-get\"\n    return $result\n}\n\nfunction _parse_package_files {\n    local files_to_parse=$@\n\n    if [[ -z \"$DISTRO\" ]]; then\n        GetDistro\n    fi\n\n    for fname in ${files_to_parse}; do\n        local OIFS line package distros distro\n        [[ -e $fname ]] || continue\n\n        OIFS=$IFS\n        IFS=$'\\n'\n        for line in $(<${fname}); do\n            if [[ $line =~ \"NOPRIME\" ]]; then\n                continue\n            fi\n\n            # Assume we want this package; free-form\n            # comments allowed after a #\n            package=${line%%#*}\n            inst_pkg=1\n\n            # Look for # dist:xxx in comment\n            if [[ $line =~ (.*)#.*dist:([^ ]*) ]]; then\n                # We are using BASH regexp matching feature.\n                package=${BASH_REMATCH[1]}\n                distros=${BASH_REMATCH[2]}\n                # In bash ${VAR,,} will lowercase VAR\n                # Look for a match in the distro list\n                if [[ ! ${distros,,} =~ ${DISTRO,,} ]]; then\n                    # If no match then skip this package\n                    inst_pkg=0\n                fi\n            fi\n\n            # Look for # not:xxx in comment\n            if [[ $line =~ (.*)#.*not:([^ ]*) ]]; then\n                # We are using BASH regexp matching feature.\n                package=${BASH_REMATCH[1]}\n                distros=${BASH_REMATCH[2]}\n                # In bash ${VAR,,} will lowercase VAR\n                # Look for a match in the distro list\n                if [[ ${distros,,} =~ ${DISTRO,,} ]]; then\n                    # If match then skip this package\n                    inst_pkg=0\n                fi\n            fi\n\n            if [[ $inst_pkg = 1 ]]; then\n                echo $package\n            fi\n        done\n        IFS=$OIFS\n    done\n}\n\n# get_packages() collects a list of package names of any type from the\n# prerequisite files in ``files/{debs|rpms}``.  The list is intended\n# to be passed to a package installer such as apt or yum.\n#\n# Only packages required for the services in 1st argument will be\n# included.  Two bits of metadata are recognized in the prerequisite files:\n#\n# - ``# NOPRIME`` defers installation to be performed later in `stack.sh`\n# - ``# dist:DISTRO`` or ``dist:DISTRO1,DISTRO2`` limits the selection\n#   of the package to the distros listed.  The distro names are case insensitive.\n# - ``# not:DISTRO`` or ``not:DISTRO1,DISTRO2`` limits the selection\n#   of the package to the distros not listed. The distro names are case insensitive.\nfunction get_packages {\n    local xtrace\n    xtrace=$(set +o | grep xtrace)\n    set +o xtrace\n    local services=$@\n    local package_dir\n    package_dir=$(_get_package_dir)\n    local file_to_parse=\"\"\n    local service=\"\"\n\n    if [ $# -ne 1 ]; then\n        die $LINENO \"get_packages takes a single, comma-separated argument\"\n    fi\n\n    if [[ -z \"$package_dir\" ]]; then\n        echo \"No package directory supplied\"\n        return 1\n    fi\n    for service in ${services//,/ }; do\n        # Allow individual services to specify dependencies\n        if [[ -e ${package_dir}/${service} ]]; then\n            file_to_parse=\"${file_to_parse} ${package_dir}/${service}\"\n        fi\n        # NOTE(sdague) n-api needs glance for now because that's where\n        # glance client is\n        if [[ $service == n-api ]]; then\n            if [[ ! $file_to_parse =~ $package_dir/nova ]]; then\n                file_to_parse=\"${file_to_parse} ${package_dir}/nova\"\n            fi\n            if [[ ! $file_to_parse =~ $package_dir/glance ]]; then\n                file_to_parse=\"${file_to_parse} ${package_dir}/glance\"\n            fi\n            if [[ ! $file_to_parse =~ $package_dir/os-brick ]]; then\n                file_to_parse=\"${file_to_parse} ${package_dir}/os-brick\"\n            fi\n        elif [[ $service == c-* ]]; then\n            if [[ ! $file_to_parse =~ $package_dir/cinder ]]; then\n                file_to_parse=\"${file_to_parse} ${package_dir}/cinder\"\n            fi\n            if [[ ! $file_to_parse =~ $package_dir/os-brick ]]; then\n                file_to_parse=\"${file_to_parse} ${package_dir}/os-brick\"\n            fi\n        elif [[ $service == s-* ]]; then\n            if [[ ! $file_to_parse =~ $package_dir/swift ]]; then\n                file_to_parse=\"${file_to_parse} ${package_dir}/swift\"\n            fi\n        elif [[ $service == n-* ]]; then\n            if [[ ! $file_to_parse =~ $package_dir/nova ]]; then\n                file_to_parse=\"${file_to_parse} ${package_dir}/nova\"\n            fi\n            if [[ ! $file_to_parse =~ $package_dir/os-brick ]]; then\n                file_to_parse=\"${file_to_parse} ${package_dir}/os-brick\"\n            fi\n        elif [[ $service == g-* ]]; then\n            if [[ ! $file_to_parse =~ $package_dir/glance ]]; then\n                file_to_parse=\"${file_to_parse} ${package_dir}/glance\"\n            fi\n        elif [[ $service == key* ]]; then\n            if [[ ! $file_to_parse =~ $package_dir/keystone ]]; then\n                file_to_parse=\"${file_to_parse} ${package_dir}/keystone\"\n            fi\n        elif [[ $service == q-* || $service == neutron-* ]]; then\n            if [[ ! $file_to_parse =~ $package_dir/neutron-common ]]; then\n                file_to_parse=\"${file_to_parse} ${package_dir}/neutron-common\"\n            fi\n        elif [[ $service == ir-* ]]; then\n            if [[ ! $file_to_parse =~ $package_dir/ironic ]]; then\n                file_to_parse=\"${file_to_parse} ${package_dir}/ironic\"\n            fi\n        fi\n    done\n    echo \"$(_parse_package_files $file_to_parse)\"\n    $xtrace\n}\n\n# get_plugin_packages() collects a list of package names of any type from a\n# plugin's prerequisite files in ``$PLUGIN/devstack/files/{debs|rpms}``.  The\n# list is intended to be passed to a package installer such as apt or yum.\n#\n# Only packages required for enabled and collected plugins will included.\n#\n# The same metadata used in the main DevStack prerequisite files may be used\n# in these prerequisite files, see get_packages() for more info.\nfunction get_plugin_packages {\n    local xtrace\n    xtrace=$(set +o | grep xtrace)\n    set +o xtrace\n    local files_to_parse=\"\"\n    local package_dir=\"\"\n    for plugin in ${DEVSTACK_PLUGINS//,/ }; do\n        package_dir=\"$(_get_package_dir ${GITDIR[$plugin]}/devstack/files)\"\n        files_to_parse+=\" $package_dir/$plugin\"\n    done\n    echo \"$(_parse_package_files $files_to_parse)\"\n    $xtrace\n}\n\n# Search plugins for a bindep.txt file\n#\n# Uses globals ``BINDEP_CMD``, ``GITDIR``, ``DEVSTACK_PLUGINS``\n#\n# Note this is only valid after BINDEP_CMD is setup in stack.sh, and\n# is thus not really intended to be called externally.\nfunction _get_plugin_bindep_packages {\n    local xtrace\n    xtrace=$(set +o | grep xtrace)\n    set +o xtrace\n\n    local bindep_file\n    local packages\n\n    for plugin in ${DEVSTACK_PLUGINS//,/ }; do\n        bindep_file=${GITDIR[$plugin]}/devstack/files/bindep.txt\n        if [[ -f ${bindep_file} ]]; then\n            packages+=$($BINDEP_CMD -b --file ${bindep_file} || true)\n        fi\n    done\n    echo \"${packages}\"\n    $xtrace\n}\n\n# Distro-agnostic package installer\n# Uses globals ``NO_UPDATE_REPOS``, ``REPOS_UPDATED``, ``RETRY_UPDATE``\n# install_package package [package ...]\nfunction update_package_repo {\n    NO_UPDATE_REPOS=${NO_UPDATE_REPOS:-False}\n    REPOS_UPDATED=${REPOS_UPDATED:-False}\n    RETRY_UPDATE=${RETRY_UPDATE:-False}\n\n    if [[ \"$NO_UPDATE_REPOS\" = \"True\" ]]; then\n        return 0\n    fi\n\n    if is_ubuntu; then\n        apt_get_update\n    fi\n}\n\nfunction real_install_package {\n    if is_ubuntu; then\n        apt_get install \"$@\"\n    elif is_fedora; then\n        yum_install \"$@\"\n    else\n        exit_distro_not_supported \"installing packages\"\n    fi\n}\n\n# Distro-agnostic package installer\n# install_package package [package ...]\nfunction install_package {\n    update_package_repo\n    if ! real_install_package \"$@\"; then\n        RETRY_UPDATE=True update_package_repo && real_install_package \"$@\"\n    fi\n}\n\n# Distro-agnostic function to tell if a package is installed\n# is_package_installed package [package ...]\nfunction is_package_installed {\n    if [[ -z \"$@\" ]]; then\n        return 1\n    fi\n\n    if [[ -z \"$os_PACKAGE\" ]]; then\n        GetOSVersion\n    fi\n\n    if [[ \"$os_PACKAGE\" = \"deb\" ]]; then\n        dpkg -s \"$@\" > /dev/null 2> /dev/null\n    elif [[ \"$os_PACKAGE\" = \"rpm\" ]]; then\n        rpm --quiet -q \"$@\"\n    else\n        exit_distro_not_supported \"finding if a package is installed\"\n    fi\n}\n\n# Distro-agnostic package uninstaller\n# uninstall_package package [package ...]\nfunction uninstall_package {\n    if is_ubuntu; then\n        apt_get purge \"$@\"\n    elif is_fedora; then\n        sudo dnf remove -y \"$@\" ||:\n    else\n        exit_distro_not_supported \"uninstalling packages\"\n    fi\n}\n\n# Wrapper for ``dnf`` to set proxy environment variables\n# Uses globals ``OFFLINE``, ``*_proxy``\n# The name is kept for backwards compatability with external\n# callers, despite none of our supported platforms using yum\n# any more.\n# yum_install package [package ...]\nfunction yum_install {\n    local result parse_yum_result\n\n    [[ \"$OFFLINE\" = \"True\" ]] && return\n\n    time_start \"yum_install\"\n    sudo_with_proxies dnf install -y \"$@\"\n    time_stop \"yum_install\"\n}\n\n# zypper wrapper to set arguments correctly\n# Uses globals ``OFFLINE``, ``*_proxy``\n# zypper_install package [package ...]\nfunction zypper_install {\n    [[ \"$OFFLINE\" = \"True\" ]] && return\n    local sudo=\"sudo\"\n    [[ \"$(id -u)\" = \"0\" ]] && sudo=\"env\"\n    $sudo http_proxy=\"${http_proxy:-}\" https_proxy=\"${https_proxy:-}\" \\\n        no_proxy=\"${no_proxy:-}\" \\\n        zypper --non-interactive install --auto-agree-with-licenses --no-recommends \"$@\"\n}\n\n# Run bindep and install packages it outputs\n#\n# Usage:\n#  install_bindep <path-to-bindep.txt> [profile,profile]\n#\n# Note unlike the bindep command itself, profile(s) specified should\n# be a single, comma-separated string, no spaces.\nfunction install_bindep {\n    local file=$1\n    local profiles=${2:-\"\"}\n    local pkgs\n\n    if [[ ! -f $file ]]; then\n        warn $LINENO \"Can not find bindep file: $file\"\n        return\n    fi\n\n    # converting here makes it much easier to work with passing\n    # arguments\n    profiles=${profiles/,/ /}\n\n    # Note bindep returns 1 when packages need to be installed, so we\n    # have to ignore it's return for \"-e\"\n    pkgs=$($DEST/bindep-venv/bin/bindep -b --file $file $profiles || true)\n\n    if [[ -n \"${pkgs}\" ]]; then\n        install_package ${pkgs}\n    fi\n}\n\nfunction write_user_unit_file {\n    local service=$1\n    local command=\"$2\"\n    local group=$3\n    local user=$4\n    local env_vars=\"$5\"\n    local extra=\"\"\n    if [[ -n \"$group\" ]]; then\n        extra=\"Group=$group\"\n    fi\n    local unitfile=\"$SYSTEMD_DIR/$service\"\n    mkdir -p $SYSTEMD_DIR\n\n    iniset -sudo $unitfile \"Unit\" \"Description\" \"Devstack $service\"\n    iniset -sudo $unitfile \"Service\" \"Environment\" \"\\\"PATH=$PATH\\\"\"\n    iniset -sudo $unitfile \"Service\" \"User\" \"$user\"\n    iniset -sudo $unitfile \"Service\" \"ExecStart\" \"$command\"\n    iniset -sudo $unitfile \"Service\" \"KillMode\" \"process\"\n    iniset -sudo $unitfile \"Service\" \"TimeoutStopSec\" \"300\"\n    iniset -sudo $unitfile \"Service\" \"ExecReload\" \"$KILL_PATH -HUP \\$MAINPID\"\n    if [[ -n \"$env_vars\" ]] ; then\n        iniset -sudo $unitfile \"Service\" \"Environment\" \"$env_vars\"\n    fi\n    if [[ -n \"$group\" ]]; then\n        iniset -sudo $unitfile \"Service\" \"Group\" \"$group\"\n    fi\n    iniset -sudo $unitfile \"Install\" \"WantedBy\" \"multi-user.target\"\n\n    # changes to existing units sometimes need a refresh\n    $SYSTEMCTL daemon-reload\n}\n\nfunction write_uwsgi_user_unit_file {\n    local service=$1\n    local command=\"$2\"\n    local group=$3\n    local user=$4\n    local env_vars=\"$5\"\n    local unitfile=\"$SYSTEMD_DIR/$service\"\n    mkdir -p $SYSTEMD_DIR\n\n    iniset -sudo $unitfile \"Unit\" \"Description\" \"Devstack $service\"\n    iniset -sudo $unitfile \"Service\" \"Environment\" \"\\\"PATH=$PATH\\\"\"\n    iniset -sudo $unitfile \"Service\" \"SyslogIdentifier\" \"$service\"\n    iniset -sudo $unitfile \"Service\" \"User\" \"$user\"\n    iniset -sudo $unitfile \"Service\" \"ExecStart\" \"$command\"\n    iniset -sudo $unitfile \"Service\" \"ExecReload\" \"$KILL_PATH -HUP \\$MAINPID\"\n    iniset -sudo $unitfile \"Service\" \"Type\" \"notify\"\n    iniset -sudo $unitfile \"Service\" \"KillMode\" \"process\"\n    iniset -sudo $unitfile \"Service\" \"Restart\" \"always\"\n    iniset -sudo $unitfile \"Service\" \"NotifyAccess\" \"all\"\n    iniset -sudo $unitfile \"Service\" \"RestartForceExitStatus\" \"100\"\n\n    if [[ -n \"$env_vars\" ]] ; then\n        iniset -sudo $unitfile \"Service\" \"Environment\" \"$env_vars\"\n    fi\n    if [[ -n \"$group\" ]]; then\n        iniset -sudo $unitfile \"Service\" \"Group\" \"$group\"\n    fi\n    iniset -sudo $unitfile \"Install\" \"WantedBy\" \"multi-user.target\"\n\n    # changes to existing units sometimes need a refresh\n    $SYSTEMCTL daemon-reload\n}\n\nfunction _common_systemd_pitfalls {\n    local cmd=$1\n    # do some sanity checks on $cmd to see things we don't expect to work\n\n    if [[ \"$cmd\" =~ \"sudo\" ]]; then\n        read -r -d '' msg << EOF || true  # read returns 1 for EOF, but it is ok here\nYou are trying to use run_process with sudo, this is not going to work under systemd.\n\nIf you need to run a service as a user other than \\$STACK_USER call it with:\n\n   run_process \\$name \\$cmd \\$group \\$user\nEOF\n        die $LINENO \"$msg\"\n    fi\n\n    if [[ ! \"$cmd\" =~ ^/ ]]; then\n        read -r -d '' msg << EOF || true  # read returns 1 for EOF, but it is ok here\nThe cmd=\"$cmd\" does not start with an absolute path. It will fail to\nstart under systemd.\n\nPlease update your run_process stanza to have an absolute path.\nEOF\n        die $LINENO \"$msg\"\n    fi\n\n}\n\n# Helper function to build a basic unit file and run it under systemd.\nfunction _run_under_systemd {\n    local service=$1\n    local command=\"$2\"\n    local cmd=$command\n    # sanity check the command\n    _common_systemd_pitfalls \"$cmd\"\n\n    local systemd_service=\"devstack@$service.service\"\n    local group=$3\n    local user=${4:-$STACK_USER}\n    if [[ -z \"$user\" ]]; then\n        user=$STACK_USER\n    fi\n    local env_vars=\"$5\"\n    if [[ \"$command\" =~ \"uwsgi\" ]] ; then\n        if [[ \"$GLOBAL_VENV\" == \"True\" ]] ; then\n            cmd=\"$cmd --venv $DEVSTACK_VENV\"\n        fi\n        write_uwsgi_user_unit_file $systemd_service \"$cmd\" \"$group\" \"$user\" \"$env_vars\"\n    else\n        write_user_unit_file $systemd_service \"$cmd\" \"$group\" \"$user\" \"$env_vars\"\n    fi\n\n    $SYSTEMCTL enable $systemd_service\n    $SYSTEMCTL start $systemd_service\n}\n\n# Find out if a process exists by partial name.\n# is_running name\nfunction is_running {\n    local name=$1\n    ps auxw | grep -v grep | grep ${name} > /dev/null\n    local exitcode=$?\n    # some times I really hate bash reverse binary logic\n    return $exitcode\n}\n\n# Run a single service under screen or directly\n# If the command includes shell metachatacters (;<>*) it must be run using a shell\n# If an optional group is provided sg will be used to run the\n# command as that group.\n# run_process service \"command-line\" [group] [user] [env_vars]\n# env_vars must be a space separated list of variable assigments, ie: \"A=1 B=2\"\nfunction run_process {\n    local service=$1\n    local command=\"$2\"\n    local group=$3\n    local user=$4\n    local env_vars=\"$5\"\n\n    local name=$service\n\n    time_start \"run_process\"\n    if is_service_enabled $service; then\n        _run_under_systemd \"$name\" \"$command\" \"$group\" \"$user\" \"$env_vars\"\n    fi\n    time_stop \"run_process\"\n}\n\n# Stop a service process\n# If a PID is available use it, kill the whole process group via TERM\n# If screen is being used kill the screen window; this will catch processes\n# that did not leave a PID behind\n# Uses globals ``SERVICE_DIR``\n# stop_process service\nfunction stop_process {\n    local service=$1\n\n    SERVICE_DIR=${SERVICE_DIR:-${DEST}/status}\n\n    if is_service_enabled $service; then\n        # Only do this for units which appear enabled, this also\n        # catches units that don't really exist for cases like\n        # keystone without a failure.\n        if $SYSTEMCTL is-enabled devstack@$service.service; then\n            $SYSTEMCTL stop devstack@$service.service\n            $SYSTEMCTL disable devstack@$service.service\n        fi\n    fi\n}\n\n# use systemctl to check service status\nfunction service_check {\n    local service\n    for service in ${ENABLED_SERVICES//,/ }; do\n        # because some things got renamed like key => keystone\n        if $SYSTEMCTL is-enabled devstack@$service.service; then\n            # no-pager is needed because otherwise status dumps to a\n            # pager when in interactive mode, which will stop a manual\n            # devstack run.\n            $SYSTEMCTL status devstack@$service.service --no-pager\n        fi\n    done\n}\n\n\n# Plugin Functions\n# =================\n\nDEVSTACK_PLUGINS=${DEVSTACK_PLUGINS:-\"\"}\n\n# enable_plugin <name> <url> [branch]\n#\n# ``name`` is an arbitrary name - (aka: glusterfs, nova-docker, zaqar)\n# ``url`` is a git url\n# ``branch`` is a gitref. If it's not set, defaults to master\nfunction enable_plugin {\n    local name=$1\n    local url=$2\n    local branch=${3:-master}\n    if is_plugin_enabled $name; then\n        die $LINENO \"Plugin attempted to be enabled twice: ${name} ${url} ${branch}\"\n    fi\n    DEVSTACK_PLUGINS+=\",$name\"\n    GITREPO[$name]=$url\n    GITDIR[$name]=$DEST/$name\n    GITBRANCH[$name]=$branch\n}\n\n# is_plugin_enabled <name>\n#\n# Check if the plugin was enabled, e.g. using enable_plugin\n#\n# ``name`` The name with which the plugin was enabled\nfunction is_plugin_enabled {\n    local name=$1\n    if [[ \",${DEVSTACK_PLUGINS},\" =~ \",${name},\" ]]; then\n        return 0\n    fi\n    return 1\n}\n\n# fetch_plugins\n#\n# clones all plugins\nfunction fetch_plugins {\n    local plugins=\"${DEVSTACK_PLUGINS}\"\n    local plugin\n\n    # short circuit if nothing to do\n    if [[ -z $plugins ]]; then\n        return\n    fi\n\n    echo \"Fetching DevStack plugins\"\n    for plugin in ${plugins//,/ }; do\n        git_clone_by_name $plugin\n    done\n}\n\n# load_plugin_settings\n#\n# Load settings from plugins in the order that they were registered\nfunction load_plugin_settings {\n    local plugins=\"${DEVSTACK_PLUGINS}\"\n    local plugin\n\n    # short circuit if nothing to do\n    if [[ -z $plugins ]]; then\n        return\n    fi\n\n    echo \"Loading plugin settings\"\n    for plugin in ${plugins//,/ }; do\n        local dir=${GITDIR[$plugin]}\n        # source any known settings\n        if [[ -f $dir/devstack/settings ]]; then\n            source $dir/devstack/settings\n        fi\n    done\n}\n\n# plugin_override_defaults\n#\n# Run an extremely early setting phase for plugins that allows default\n# overriding of services.\nfunction plugin_override_defaults {\n    local plugins=\"${DEVSTACK_PLUGINS}\"\n    local plugin\n\n    # short circuit if nothing to do\n    if [[ -z $plugins ]]; then\n        return\n    fi\n\n    echo \"Overriding Configuration Defaults\"\n    for plugin in ${plugins//,/ }; do\n        local dir=${GITDIR[$plugin]}\n        # source any overrides\n        if [[ -f $dir/devstack/override-defaults ]]; then\n            # be really verbose that an override is happening, as it\n            # may not be obvious if things fail later.\n            echo \"$plugin has overridden the following defaults\"\n            cat $dir/devstack/override-defaults\n            source $dir/devstack/override-defaults\n        fi\n    done\n}\n\n# run_plugins\n#\n# Run the devstack/plugin.sh in all the plugin directories. These are\n# run in registration order.\nfunction run_plugins {\n    local mode=$1\n    local phase=$2\n\n    local plugins=\"${DEVSTACK_PLUGINS}\"\n    local plugin\n    for plugin in ${plugins//,/ }; do\n        local dir=${GITDIR[$plugin]}\n        if [[ -f $dir/devstack/plugin.sh ]]; then\n            source $dir/devstack/plugin.sh $mode $phase\n        fi\n    done\n}\n\nfunction run_phase {\n    local mode=$1\n    local phase=$2\n    if [[ -d $TOP_DIR/extras.d ]]; then\n        local extra_plugin_file_name\n        for extra_plugin_file_name in $TOP_DIR/extras.d/*.sh; do\n            # NOTE(sdague): only process extras.d for the 3 explicitly\n            # white listed elements in tree. We want these to move out\n            # over time as well, but they are in tree, so we need to\n            # manage that.\n            local exceptions=\"80-tempest.sh\"\n            local extra\n            extra=$(basename $extra_plugin_file_name)\n            if [[ ! ( $exceptions =~ \"$extra\" ) ]]; then\n                warn \"use of extras.d is no longer supported\"\n                warn \"processing of project $extra is skipped\"\n            else\n                [[ -r $extra_plugin_file_name ]] && source $extra_plugin_file_name $mode $phase\n            fi\n        done\n    fi\n    # the source phase corresponds to settings loading in plugins\n    if [[ \"$mode\" == \"source\" ]]; then\n        load_plugin_settings\n        verify_disabled_services\n    elif [[ \"$mode\" == \"override_defaults\" ]]; then\n        plugin_override_defaults\n    else\n        run_plugins $mode $phase\n    fi\n}\n\n# define_plugin <name>\n#\n# This function is a no-op.  It allows a plugin to define its name So\n# that other plugins may reference it by name.  It should generally be\n# the last component of the canonical git repo name.  E.g.,\n# openstack/devstack-foo should use \"devstack-foo\" as the name here.\n#\n# This function is currently a noop, but the value may still be used\n# by external tools (as in plugin_requires) and may be used by\n# devstack in the future.\n#\n# ``name`` is an arbitrary name - (aka: glusterfs, nova-docker, zaqar)\nfunction define_plugin {\n    :\n}\n\n# plugin_requires <name> <other>\n#\n# This function is a no-op.  It is currently used by external tools\n# (such as the devstack module for Ansible) to automatically generate\n# local.conf files.  It is not currently used by devstack itself to\n# resolve dependencies.\n#\n# ``name`` is an arbitrary name - (aka: glusterfs, nova-docker, zaqar)\n# ``other`` is the name of another plugin\nfunction plugin_requires {\n    :\n}\n\n\n# Service Functions\n# =================\n\n# remove extra commas from the input string (i.e. ``ENABLED_SERVICES``)\n# _cleanup_service_list service-list\nfunction _cleanup_service_list {\n    local xtrace\n    xtrace=$(set +o | grep xtrace)\n    set +o xtrace\n\n    echo \"$1\" | sed -e '\n        s/,,/,/g;\n        s/^,//;\n        s/,$//\n    '\n\n    $xtrace\n}\n\n# disable_all_services() removes all current services\n# from ``ENABLED_SERVICES`` to reset the configuration\n# before a minimal installation\n# Uses global ``ENABLED_SERVICES``\n# disable_all_services\nfunction disable_all_services {\n    ENABLED_SERVICES=\"\"\n}\n\n# Remove all services starting with '-'.  For example, to install all default\n# services except rabbit (rabbit) set in ``localrc``:\n# ENABLED_SERVICES+=\",-rabbit\"\n# Uses global ``ENABLED_SERVICES``\n# disable_negated_services\nfunction disable_negated_services {\n    local xtrace\n    xtrace=$(set +o | grep xtrace)\n    set +o xtrace\n\n    local to_remove=\"\"\n    local remaining=\"\"\n    local service\n\n    # build up list of services that should be removed; i.e. they\n    # begin with \"-\"\n    for service in ${ENABLED_SERVICES//,/ }; do\n        if [[ ${service} == -* ]]; then\n            to_remove+=\",${service#-}\"\n        else\n            remaining+=\",${service}\"\n        fi\n    done\n\n    # go through the service list.  if this service appears in the \"to\n    # be removed\" list, drop it\n    ENABLED_SERVICES=$(remove_disabled_services \"$remaining\" \"$to_remove\")\n\n    $xtrace\n}\n\n# disable_service() prepares the services passed as argument to be\n# removed from the ``ENABLED_SERVICES`` list, if they are present.\n#\n# For example:\n#   disable_service rabbit\n#\n# Uses global ``DISABLED_SERVICES``\n# disable_service service [service ...]\nfunction disable_service {\n    local xtrace\n    xtrace=$(set +o | grep xtrace)\n    set +o xtrace\n\n    local disabled_svcs=\"${DISABLED_SERVICES}\"\n    local enabled_svcs=\",${ENABLED_SERVICES},\"\n    local service\n    for service in $@; do\n        disabled_svcs+=\",$service\"\n        if is_service_enabled $service; then\n            enabled_svcs=${enabled_svcs//,$service,/,}\n        fi\n    done\n    DISABLED_SERVICES=$(_cleanup_service_list \"$disabled_svcs\")\n    ENABLED_SERVICES=$(_cleanup_service_list \"$enabled_svcs\")\n\n    $xtrace\n}\n\n# enable_service() adds the services passed as argument to the\n# ``ENABLED_SERVICES`` list, if they are not already present.\n#\n# For example:\n#   enable_service q-svc\n#\n# This function does not know about the special cases\n# for nova, glance, and neutron built into is_service_enabled().\n# Uses global ``ENABLED_SERVICES``\n# enable_service service [service ...]\nfunction enable_service {\n    local xtrace\n    xtrace=$(set +o | grep xtrace)\n    set +o xtrace\n\n    local tmpsvcs=\"${ENABLED_SERVICES}\"\n    local service\n    for service in $@; do\n        if [[ ,${DISABLED_SERVICES}, =~ ,${service}, ]]; then\n            warn $LINENO \"Attempt to enable_service ${service} when it has been disabled\"\n            continue\n        fi\n        if ! is_service_enabled $service; then\n            tmpsvcs+=\",$service\"\n        fi\n    done\n    ENABLED_SERVICES=$(_cleanup_service_list \"$tmpsvcs\")\n    disable_negated_services\n\n    $xtrace\n}\n\n# is_service_enabled() checks if the service(s) specified as arguments are\n# enabled by the user in ``ENABLED_SERVICES``.\n#\n# Multiple services specified as arguments are ``OR``'ed together; the test\n# is a short-circuit boolean, i.e it returns on the first match.\n#\n# There are special cases for some 'catch-all' services::\n#   **nova** returns true if any service enabled start with **n-**\n#   **cinder** returns true if any service enabled start with **c-**\n#   **glance** returns true if any service enabled start with **g-**\n#   **neutron** returns true if any service enabled start with **q-**\n#   **swift** returns true if any service enabled start with **s-**\n#   **trove** returns true if any service enabled start with **tr-**\n#   For backward compatibility if we have **swift** in ENABLED_SERVICES all the\n#   **s-** services will be enabled. This will be deprecated in the future.\n#\n# Uses global ``ENABLED_SERVICES``\n# is_service_enabled service [service ...]\nfunction is_service_enabled {\n    local xtrace\n    xtrace=$(set +o | grep xtrace)\n    set +o xtrace\n\n    local enabled=1\n    local services=$@\n    local service\n    for service in ${services}; do\n        [[ ,${ENABLED_SERVICES}, =~ ,${service}, ]] && enabled=0\n\n        # Look for top-level 'enabled' function for this service\n        if type is_${service}_enabled >/dev/null 2>&1; then\n            # A function exists for this service, use it\n            is_${service}_enabled && enabled=0\n        fi\n\n        # TODO(dtroyer): Remove these legacy special-cases after the is_XXX_enabled()\n        #                are implemented\n\n        [[ ${service} == n-cpu-* && ,${ENABLED_SERVICES} =~ ,\"n-cpu\" ]] && enabled=0\n        [[ ${service} == \"nova\" && ,${ENABLED_SERVICES} =~ ,\"n-\" ]] && enabled=0\n        [[ ${service} == \"glance\" && ,${ENABLED_SERVICES} =~ ,\"g-\" ]] && enabled=0\n        [[ ${service} == \"neutron\" && ,${ENABLED_SERVICES} =~ ,\"q-\" ]] && enabled=0\n        [[ ${service} == \"trove\" && ,${ENABLED_SERVICES} =~ ,\"tr-\" ]] && enabled=0\n        [[ ${service} == \"swift\" && ,${ENABLED_SERVICES} =~ ,\"s-\" ]] && enabled=0\n        [[ ${service} == s-* && ,${ENABLED_SERVICES} =~ ,\"swift\" ]] && enabled=0\n    done\n\n    $xtrace\n    return $enabled\n}\n\n# remove specified list from the input string\n# remove_disabled_services service-list remove-list\nfunction remove_disabled_services {\n    local xtrace\n    xtrace=$(set +o | grep xtrace)\n    set +o xtrace\n\n    local service_list=$1\n    local remove_list=$2\n    local service\n    local enabled=\"\"\n\n    for service in ${service_list//,/ }; do\n        local remove\n        local add=1\n        for remove in ${remove_list//,/ }; do\n            if [[ ${remove} == ${service} ]]; then\n                add=0\n                break\n            fi\n        done\n        if [[ $add == 1 ]]; then\n            enabled=\"${enabled},$service\"\n        fi\n    done\n\n    $xtrace\n\n    _cleanup_service_list \"$enabled\"\n}\n\n# Toggle enable/disable_service for services that must run exclusive of each other\n#  $1 The name of a variable containing a space-separated list of services\n#  $2 The name of a variable in which to store the enabled service's name\n#  $3 The name of the service to enable\nfunction use_exclusive_service {\n    local options=${!1}\n    local selection=$3\n    local out=$2\n    [ -z $selection ] || [[ ! \"$options\" =~ \"$selection\" ]] && return 1\n    local opt\n    for opt in $options;do\n        [[ \"$opt\" = \"$selection\" ]] && enable_service $opt || disable_service $opt\n    done\n    eval \"$out=$selection\"\n    return 0\n}\n\n# Make sure that nothing has manipulated ENABLED_SERVICES in a way\n# that conflicts with prior calls to disable_service.\n# Uses global ``ENABLED_SERVICES``\nfunction verify_disabled_services {\n    local service\n    for service in ${ENABLED_SERVICES//,/ }; do\n        if [[ ,${DISABLED_SERVICES}, =~ ,${service}, ]]; then\n            die $LINENO \"ENABLED_SERVICES directly modified to overcome 'disable_service ${service}'\"\n        fi\n    done\n}\n\n\n# System Functions\n# ================\n\n# Only run the command if the target file (the last arg) is not on an\n# NFS filesystem.\nfunction _safe_permission_operation {\n    local xtrace\n    xtrace=$(set +o | grep xtrace)\n    set +o xtrace\n    local args=( $@ )\n    local last\n    local sudo_cmd\n    local dir_to_check\n\n    let last=\"${#args[*]} - 1\"\n\n    local dir_to_check=${args[$last]}\n    if [ ! -d \"$dir_to_check\" ]; then\n        dir_to_check=`dirname \"$dir_to_check\"`\n    fi\n\n    if is_nfs_directory \"$dir_to_check\" ; then\n        $xtrace\n        return 0\n    fi\n\n    sudo_cmd=\"sudo\"\n\n    $xtrace\n    $sudo_cmd $@\n}\n\n# Exit 0 if address is in network or 1 if address is not in network\n# ip-range is in CIDR notation: 1.2.3.4/20\n# address_in_net ip-address ip-range\nfunction address_in_net {\n    local ip=$1\n    local range=$2\n    local masklen=${range#*/}\n    local network\n    network=$(maskip ${range%/*} $(cidr2netmask $masklen))\n    local subnet\n    subnet=$(maskip $ip $(cidr2netmask $masklen))\n    [[ $network == $subnet ]]\n}\n\n# Add a user to a group.\n# add_user_to_group user group\nfunction add_user_to_group {\n    local user=$1\n    local group=$2\n\n    sudo usermod -a -G \"$group\" \"$user\"\n}\n\n# Convert CIDR notation to a IPv4 netmask\n# cidr2netmask cidr-bits\nfunction cidr2netmask {\n    local maskpat=\"255 255 255 255\"\n    local maskdgt=\"254 252 248 240 224 192 128\"\n    set -- ${maskpat:0:$(( ($1 / 8) * 4 ))}${maskdgt:$(( (7 - ($1 % 8)) * 4 )):3}\n    echo ${1-0}.${2-0}.${3-0}.${4-0}\n}\n\n# Check if this is a valid ipv4 address string\nfunction is_ipv4_address {\n    local address=$1\n    local regex='([0-9]{1,3}\\.){3}[0-9]{1,3}'\n    # TODO(clarkb) make this more robust\n    if [[ \"$address\" =~ $regex ]] ; then\n        return 0\n    else\n        return 1\n    fi\n}\n\n# Remove \"[]\" around urlquoted IPv6 addresses\nfunction ipv6_unquote {\n    echo $1 | tr -d []\n}\n\n# Gracefully cp only if source file/dir exists\n# cp_it source destination\nfunction cp_it {\n    if [ -e $1 ] || [ -d $1 ]; then\n        cp -pRL $1 $2\n    fi\n}\n\n# HTTP and HTTPS proxy servers are supported via the usual environment variables [1]\n# ``http_proxy``, ``https_proxy`` and ``no_proxy``. They can be set in\n# ``localrc`` or on the command line if necessary::\n#\n# [1] http://www.w3.org/Daemon/User/Proxies/ProxyClients.html\n#\n#     http_proxy=http://proxy.example.com:3128/ no_proxy=repo.example.net ./stack.sh\n\nfunction export_proxy_variables {\n    if isset http_proxy ; then\n        export http_proxy=$http_proxy\n    fi\n    if isset https_proxy ; then\n        export https_proxy=$https_proxy\n    fi\n    if isset no_proxy ; then\n        export no_proxy=$no_proxy\n    fi\n}\n\n# Returns true if the directory is on a filesystem mounted via NFS.\nfunction is_nfs_directory {\n    local mount_type\n    mount_type=`stat -f -L -c %T $1`\n    test \"$mount_type\" == \"nfs\"\n}\n\n# Return the network portion of the given IP address using netmask\n# netmask is in the traditional dotted-quad format\n# maskip ip-address netmask\nfunction maskip {\n    local ip=$1\n    local mask=$2\n    local l=\"${ip%.*}\"; local r=\"${ip#*.}\"; local n=\"${mask%.*}\"; local m=\"${mask#*.}\"\n    local subnet\n    subnet=$((${ip%%.*}&${mask%%.*})).$((${r%%.*}&${m%%.*})).$((${l##*.}&${n##*.})).$((${ip##*.}&${mask##*.}))\n    echo $subnet\n}\n\nfunction is_provider_network {\n    if [ \"$Q_USE_PROVIDER_NETWORKING\" == \"True\" ]; then\n        return 0\n    fi\n    return 1\n}\n\n\n# Return just the <major>.<minor> for the given python interpreter\nfunction _get_python_version {\n    local interp=$1\n    local version\n    # disable erroring out here, otherwise if python 3 doesn't exist we fail hard.\n    if [[ -x $(which $interp 2> /dev/null) ]]; then\n        version=$($interp -c 'import sys; print(\"%s.%s\" % sys.version_info[0:2])')\n    fi\n    echo ${version}\n}\n\n# Return the current python as \"python<major>.<minor>\"\nfunction python_version {\n    local python_version\n    python_version=$(_get_python_version python2)\n    echo \"python${python_version}\"\n}\n\nfunction python3_version {\n    local python3_version\n    python3_version=$(_get_python_version python3)\n    echo \"python${python3_version}\"\n}\n\n\n# Service wrapper to restart services\n# restart_service service-name\nfunction restart_service {\n    if [ -x /bin/systemctl ]; then\n        sudo /bin/systemctl restart $1\n    else\n        sudo service $1 restart\n    fi\n\n}\n\n# Only change permissions of a file or directory if it is not on an\n# NFS filesystem.\nfunction safe_chmod {\n    _safe_permission_operation chmod $@\n}\n\n# Only change ownership of a file or directory if it is not on an NFS\n# filesystem.\nfunction safe_chown {\n    _safe_permission_operation chown $@\n}\n\n# Service wrapper to start services\n# start_service service-name\nfunction start_service {\n    if [ -x /bin/systemctl ]; then\n        sudo /bin/systemctl start $1\n    else\n        sudo service $1 start\n    fi\n}\n\n# Service wrapper to stop services\n# stop_service service-name\nfunction stop_service {\n    if [ -x /bin/systemctl ]; then\n        sudo /bin/systemctl stop $1\n    else\n        sudo service $1 stop\n    fi\n}\n\n# Service wrapper to reload services\n# If the service was not in running state it will start it\n# reload_service service-name\nfunction reload_service {\n    if [ -x /bin/systemctl ]; then\n        sudo /bin/systemctl reload-or-restart $1\n    else\n        sudo service $1 reload\n    fi\n}\n\n# Test with a finite retry loop.\n#\nfunction test_with_retry {\n    local testcmd=$1\n    local failmsg=$2\n    local until=${3:-10}\n    local sleep=${4:-0.5}\n\n    time_start \"test_with_retry\"\n    if ! timeout $until sh -c \"while ! $testcmd; do sleep $sleep; done\"; then\n        die $LINENO \"$failmsg\"\n    fi\n    time_stop \"test_with_retry\"\n}\n\n# Like sudo but forwarding http_proxy https_proxy no_proxy environment vars.\n# If it is run as superuser then sudo is replaced by env.\n#\nfunction sudo_with_proxies {\n    local sudo\n\n    [[ \"$(id -u)\" = \"0\" ]] && sudo=\"env\" || sudo=\"sudo\"\n\n    $sudo http_proxy=\"${http_proxy:-}\" https_proxy=\"${https_proxy:-}\"\\\n        no_proxy=\"${no_proxy:-}\" \"$@\"\n}\n\n# Timing infrastructure - figure out where large blocks of time are\n# used in DevStack\n#\n# The timing infrastructure for DevStack is about collecting buckets\n# of time that are spend in some subtask. For instance, that might be\n# 'apt', 'pip', 'osc', even database migrations. We do this by a pair\n# of functions: time_start / time_stop.\n#\n# These take a single parameter: $name - which specifies the name of\n# the bucket to be accounted against. time_totals function spits out\n# the results.\n#\n# Resolution is only in whole seconds, so should be used for long\n# running activities.\n\ndeclare -A -g _TIME_TOTAL\ndeclare -A -g _TIME_START\ndeclare -r -g _TIME_BEGIN=$(date +%s)\n\n# time_start $name\n#\n# starts the clock for a timer by name. Errors if that clock is\n# already started.\nfunction time_start {\n    local name=$1\n    local start_time=${_TIME_START[$name]}\n    if [[ -n \"$start_time\" ]]; then\n        die $LINENO \"Trying to start the clock on $name, but it's already been started\"\n    fi\n    _TIME_START[$name]=$(date +%s%3N)\n}\n\n# time_stop $name\n#\n# stops the clock for a timer by name, and accumulate that time in the\n# global counter for that name. Errors if that clock had not\n# previously been started.\nfunction time_stop {\n    local name\n    local end_time\n    local elapsed_time\n    local total\n    local start_time\n\n    name=$1\n    start_time=${_TIME_START[$name]}\n\n    if [[ -z \"$start_time\" ]]; then\n        die $LINENO \"Trying to stop the clock on $name, but it was never started\"\n    fi\n    end_time=$(date +%s%3N)\n    elapsed_time=$(($end_time - $start_time))\n    total=${_TIME_TOTAL[$name]:-0}\n    # reset the clock so we can start it in the future\n    _TIME_START[$name]=\"\"\n    _TIME_TOTAL[$name]=$(($total + $elapsed_time))\n}\n\nfunction install_openstack_cli_server {\n    export PATH=$TOP_DIR/files/openstack-cli-server:$PATH\n    run_process openstack-cli-server \"$PYTHON $TOP_DIR/files/openstack-cli-server/openstack-cli-server\"\n}\n\nfunction oscwrap {\n    local xtrace\n    xtrace=$(set +o | grep xtrace)\n    set +o xtrace\n\n    local out\n    local rc\n    local start\n    local end\n    # Cannot use timer_start and timer_stop as we run in subshells\n    # and those rely on modifying vars in the same process (which cannot\n    # happen from a subshell.\n    start=$(date +%s%3N)\n    out=$(command openstack \"$@\")\n    rc=$?\n    end=$(date +%s%3N)\n    echo $((end - start)) >> $OSCWRAP_TIMER_FILE\n\n    echo \"$out\"\n    $xtrace\n    return $rc\n}\n\nfunction install_oscwrap {\n    # File to accumulate our timing data\n    OSCWRAP_TIMER_FILE=$(mktemp)\n    # Bash by default doesn't expand aliases, allow it for the aliases\n    # we want to whitelist.\n    shopt -s expand_aliases\n    # Remove all aliases that might be expanded to preserve old unexpanded\n    # behavior\n    unalias -a\n    # Add only the alias we want for openstack\n    alias openstack=oscwrap\n}\n\nfunction cleanup_oscwrap {\n    local total=0\n    total=$(cat $OSCWRAP_TIMER_FILE | $PYTHON -c \"import sys; print(sum(int(l) for l in sys.stdin))\")\n    _TIME_TOTAL[\"osc\"]=$total\n    rm $OSCWRAP_TIMER_FILE\n}\n\n# time_totals\n#  Print out total time summary\nfunction time_totals {\n    local elapsed_time\n    local end_time\n    local len=20\n    local xtrace\n    local unaccounted_time\n\n    end_time=$(date +%s)\n    elapsed_time=$(($end_time - $_TIME_BEGIN))\n    unaccounted_time=$elapsed_time\n\n    # pad 1st column this far\n    for t in ${!_TIME_TOTAL[*]}; do\n        if [[ ${#t} -gt $len ]]; then\n            len=${#t}\n        fi\n    done\n\n    cleanup_oscwrap\n\n    xtrace=$(set +o | grep xtrace)\n    set +o xtrace\n\n    echo\n    echo \"=========================\"\n    echo \"DevStack Component Timing\"\n    echo \" (times are in seconds)  \"\n    echo \"=========================\"\n    for t in ${!_TIME_TOTAL[*]}; do\n        local v=${_TIME_TOTAL[$t]}\n        # because we're recording in milliseconds\n        v=$(($v / 1000))\n        printf \"%-${len}s %3d\\n\" \"$t\" \"$v\"\n        unaccounted_time=$(($unaccounted_time - $v))\n    done\n    echo \"-------------------------\"\n    printf \"%-${len}s %3d\\n\" \"Unaccounted time\" \"$unaccounted_time\"\n    echo \"=========================\"\n    printf \"%-${len}s %3d\\n\" \"Total runtime\" \"$elapsed_time\"\n\n    $xtrace\n}\n\nfunction clean_pyc_files {\n    # Clean up all *.pyc files\n    if [[ -n \"$DEST\" ]] && [[ -d \"$DEST\" ]]; then\n        sudo find $DEST -name \"*.pyc\" -delete\n    fi\n}\n\nfunction is_fips_enabled {\n    fips=`cat /proc/sys/crypto/fips_enabled`\n    [ \"$fips\" == \"1\" ]\n}\n\n# Restore xtrace\n$_XTRACE_FUNCTIONS_COMMON\n\n# Local variables:\n# mode: shell-script\n# End:\n"
        },
        {
          "name": "gate",
          "type": "tree",
          "content": null
        },
        {
          "name": "inc",
          "type": "tree",
          "content": null
        },
        {
          "name": "lib",
          "type": "tree",
          "content": null
        },
        {
          "name": "openrc",
          "type": "blob",
          "size": 2.63671875,
          "content": "#!/usr/bin/env bash\n#\n# source openrc [username] [projectname]\n#\n# Configure a set of credentials for $PROJECT/$USERNAME:\n#   Set OS_PROJECT_NAME to override the default project 'demo'\n#   Set OS_USERNAME to override the default user name 'demo'\n#   Set ADMIN_PASSWORD to set the password for 'admin' and 'demo'\n\nif [[ -n \"$1\" ]]; then\n    OS_USERNAME=$1\nfi\nif [[ -n \"$2\" ]]; then\n    OS_PROJECT_NAME=$2\nfi\n\n# Find the other rc files\nRC_DIR=$(cd $(dirname \"${BASH_SOURCE:-$0}\") && pwd)\n\n# Import common functions\nsource $RC_DIR/functions\n\n# Load local configuration\nsource $RC_DIR/stackrc\n\n# Load the last env variables if available\nif [[ -r $RC_DIR/.stackenv ]]; then\n    source $RC_DIR/.stackenv\n    export OS_CACERT\nfi\n\n# Get some necessary configuration\nsource $RC_DIR/lib/tls\n\n# Minimal configuration\nexport OS_AUTH_TYPE=password\nexport OS_PROJECT_NAME=${OS_PROJECT_NAME:-demo}\nexport OS_USERNAME=${OS_USERNAME:-demo}\nexport OS_PASSWORD=${ADMIN_PASSWORD:-secret}\nexport OS_REGION_NAME=${REGION_NAME:-RegionOne}\n\n# Set the host API endpoint. This will default to HOST_IP if SERVICE_IP_VERSION\n# is 4, else HOST_IPV6 if it's 6. SERVICE_HOST may also be used to specify the\n# endpoint, which is convenient for some localrc configurations. Additionally,\n# some exercises call Glance directly. On a single-node installation, Glance\n# should be listening on a local IP address, depending on the setting of\n# SERVICE_IP_VERSION. If its running elsewhere, it can be set here.\nif [[ $SERVICE_IP_VERSION == 6 ]]; then\n    HOST_IPV6=${HOST_IPV6:-::1}\n    SERVICE_HOST=${SERVICE_HOST:-[$HOST_IPV6]}\n    GLANCE_HOST=${GLANCE_HOST:-[$HOST_IPV6]}\nelse\n    HOST_IP=${HOST_IP:-127.0.0.1}\n    SERVICE_HOST=${SERVICE_HOST:-$HOST_IP}\n    GLANCE_HOST=${GLANCE_HOST:-$HOST_IP}\nfi\n\n# If you don't have a working .stackenv, this is the backup position\nKEYSTONE_BACKUP=$SERVICE_PROTOCOL://$SERVICE_HOST:5000\nKEYSTONE_SERVICE_URI=${KEYSTONE_SERVICE_URI:-$KEYSTONE_BACKUP}\n\nexport OS_AUTH_URL=${OS_AUTH_URL:-$KEYSTONE_SERVICE_URI}\n\nexport OS_USER_DOMAIN_ID=${OS_USER_DOMAIN_ID:-\"default\"}\nexport OS_PROJECT_DOMAIN_ID=${OS_PROJECT_DOMAIN_ID:-\"default\"}\n\n# Set OS_CACERT to a default CA certificate chain if it exists.\nif [[ ! -v OS_CACERT ]] ; then\n    DEFAULT_OS_CACERT=$INT_CA_DIR/ca-chain.pem\n    # If the file does not exist, this may confuse preflight sanity checks\n    if [ -e $DEFAULT_OS_CACERT ] ; then\n        export OS_CACERT=$DEFAULT_OS_CACERT\n    fi\nfi\n\n# Currently cinderclient needs you to specify the *volume api* version. This\n# needs to match the config of your catalog returned by Keystone.\nexport CINDER_VERSION=${CINDER_VERSION:-3}\nexport OS_VOLUME_API_VERSION=${OS_VOLUME_API_VERSION:-$CINDER_VERSION}\n"
        },
        {
          "name": "playbooks",
          "type": "tree",
          "content": null
        },
        {
          "name": "roles",
          "type": "tree",
          "content": null
        },
        {
          "name": "run_tests.sh",
          "type": "blob",
          "size": 1.16015625,
          "content": "#!/bin/bash\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations\n# under the License.\n\n# This runs a series of unit tests for DevStack to ensure it's functioning\n\nPASSES=\"\"\nFAILURES=\"\"\n\nfor testfile in tests/test_*.sh; do\n    $testfile\n    if [[ $? -eq 0 ]]; then\n        PASSES=\"$PASSES $testfile\"\n    else\n        FAILURES=\"$FAILURES $testfile\"\n    fi\ndone\n\n# Summary display now that all is said and done\necho \"=====================================================================\"\nfor script in $PASSES; do\n    echo PASS $script\ndone\nfor script in $FAILURES; do\n    echo FAILED $script\ndone\necho \"=====================================================================\"\n\nif [[ -n \"$FAILURES\" ]]; then\n    exit 1\nfi\n"
        },
        {
          "name": "samples",
          "type": "tree",
          "content": null
        },
        {
          "name": "stack.sh",
          "type": "blob",
          "size": 46.494140625,
          "content": "#!/usr/bin/env bash\n\n\n# ``stack.sh`` is an opinionated OpenStack developer installation.  It\n# installs and configures various combinations of **Cinder**, **Glance**,\n# **Horizon**, **Keystone**, **Nova**, **Neutron**, and **Swift**\n\n# This script's options can be changed by setting appropriate environment\n# variables.  You can configure things like which git repositories to use,\n# services to enable, OS images to use, etc.  Default values are located in the\n# ``stackrc`` file. If you are crafty you can run the script on multiple nodes\n# using shared settings for common resources (eg., mysql or rabbitmq) and build\n# a multi-node developer install.\n\n# To keep this script simple we assume you are running on a recent **Ubuntu**\n# (Bionic or newer) or **CentOS/RHEL/RockyLinux**\n# (7 or newer) machine. (It may work on other platforms but support for those\n# platforms is left to those who added them to DevStack.) It should work in\n# a VM or physical server. Additionally, we maintain a list of ``deb`` and\n# ``rpm`` dependencies and other configuration files in this repo.\n\n# Learn more and get the most recent version at http://devstack.org\n\n# Print the commands being run so that we can see the command that triggers\n# an error.  It is also useful for following along as the install occurs.\nset -o xtrace\n\n# Make sure custom grep options don't get in the way\nunset GREP_OPTIONS\n\n# NOTE(sdague): why do we explicitly set locale when running stack.sh?\n#\n# Devstack is written in bash, and many functions used throughout\n# devstack process text coming off a command (like the ip command)\n# and do transforms using grep, sed, cut, awk on the strings that are\n# returned. Many of these programs are internationalized, which is\n# great for end users, but means that the strings that devstack\n# functions depend upon might not be there in other locales. We thus\n# need to pin the world to an english basis during the runs.\n#\n# Previously we used the C locale for this, every system has it, and\n# it gives us a stable sort order. It does however mean that we\n# effectively drop unicode support.... boo!  :(\n#\n# With python3 being more unicode aware by default, that's not the\n# right option. While there is a C.utf8 locale, some distros are\n# shipping it as C.UTF8 for extra confusingness. And it's support\n# isn't super clear across distros. This is made more challenging when\n# trying to support both out of the box distros, and the gate which\n# uses diskimage builder to build disk images in a different way than\n# the distros do.\n#\n# So... en_US.utf8 it is. That's existed for a very long time. It is a\n# compromise position, but it is the least worse idea at the time of\n# this comment.\n#\n# We also have to unset other variables that might impact LC_ALL\n# taking effect.\nunset LANG\nunset LANGUAGE\nLC_ALL=en_US.utf8\nexport LC_ALL\n\n# Clear all OpenStack related envvars\nunset `env | grep -E '^OS_' | cut -d = -f 1`\n\n# Make sure umask is sane\numask 022\n\n# Not all distros have sbin in PATH for regular users.\n# osc will normally be installed at /usr/local/bin/openstack so ensure\n# /usr/local/bin is also in the path\nPATH=$PATH:/usr/local/bin:/usr/local/sbin:/usr/sbin:/sbin\n\n# Keep track of the DevStack directory\nTOP_DIR=$(cd $(dirname \"$0\") && pwd)\n\n# Check for uninitialized variables, a big cause of bugs\nNOUNSET=${NOUNSET:-}\nif [[ -n \"$NOUNSET\" ]]; then\n    set -o nounset\nfi\n\n# Set start of devstack timestamp\nDEVSTACK_START_TIME=$(date +%s)\n\n# Configuration\n# =============\n\n# Sanity Checks\n# -------------\n\n# Clean up last environment var cache\nif [[ -r $TOP_DIR/.stackenv ]]; then\n    rm $TOP_DIR/.stackenv\nfi\n\n# ``stack.sh`` keeps the list of ``deb`` and ``rpm`` dependencies, config\n# templates and other useful files in the ``files`` subdirectory\nFILES=$TOP_DIR/files\nif [ ! -d $FILES ]; then\n    set +o xtrace\n    echo \"missing devstack/files\"\n    exit 1\nfi\n\n# ``stack.sh`` keeps function libraries here\n# Make sure ``$TOP_DIR/inc`` directory is present\nif [ ! -d $TOP_DIR/inc ]; then\n    set +o xtrace\n    echo \"missing devstack/inc\"\n    exit 1\nfi\n\n# ``stack.sh`` keeps project libraries here\n# Make sure ``$TOP_DIR/lib`` directory is present\nif [ ! -d $TOP_DIR/lib ]; then\n    set +o xtrace\n    echo \"missing devstack/lib\"\n    exit 1\nfi\n\n# Check if run in POSIX shell\nif [[ \"${POSIXLY_CORRECT}\" == \"y\" ]]; then\n    set +o xtrace\n    echo \"You are running POSIX compatibility mode, DevStack requires bash 4.2 or newer.\"\n    exit 1\nfi\n\n# OpenStack is designed to be run as a non-root user; Horizon will fail to run\n# as **root** since Apache will not serve content from **root** user).\n# ``stack.sh`` must not be run as **root**.  It aborts and suggests one course of\n# action to create a suitable user account.\n\nif [[ $EUID -eq 0 ]]; then\n    set +o xtrace\n    echo \"DevStack should be run as a user with sudo permissions, \"\n    echo \"not root.\"\n    echo \"A \\\"stack\\\" user configured correctly can be created with:\"\n    echo \" $TOP_DIR/tools/create-stack-user.sh\"\n    exit 1\nfi\n\n# OpenStack is designed to run at a system level, with system level\n# installation of python packages. It does not support running under a\n# virtual env, and will fail in really odd ways if you do this. Make\n# this explicit as it has come up on the mailing list.\nif [[ -n \"$VIRTUAL_ENV\" ]]; then\n    set +o xtrace\n    echo \"You appear to be running under a python virtualenv.\"\n    echo \"DevStack does not support this, as we may break the\"\n    echo \"virtualenv you are currently in by modifying \"\n    echo \"external system-level components the virtualenv relies on.\"\n    echo \"We recommend you use a separate virtual-machine if \"\n    echo \"you are worried about DevStack taking over your system.\"\n    exit 1\nfi\n\n# Provide a safety switch for devstack. If you do a lot of devstack,\n# on a lot of different environments, you sometimes run it on the\n# wrong box. This makes there be a way to prevent that.\nif [[ -e $HOME/.no-devstack ]]; then\n    set +o xtrace\n    echo \"You've marked this host as a no-devstack host, to save yourself from\"\n    echo \"running devstack accidentally. If this is in error, please remove the\"\n    echo \"~/.no-devstack file\"\n    exit 1\nfi\n\n# Prepare the environment\n# -----------------------\n\n# Initialize variables:\nLAST_SPINNER_PID=\"\"\n\n# Import common functions\nsource $TOP_DIR/functions\n\n# Import 'public' stack.sh functions\nsource $TOP_DIR/lib/stack\n\n# Determine what system we are running on.  This provides ``os_VENDOR``,\n# ``os_RELEASE``, ``os_PACKAGE``, ``os_CODENAME``\n# and ``DISTRO``\nGetDistro\n\n\n# Global Settings\n# ---------------\n\n# Check for a ``localrc`` section embedded in ``local.conf`` and extract if\n# ``localrc`` does not already exist\n\n# Phase: local\nrm -f $TOP_DIR/.localrc.auto\nextract_localrc_section $TOP_DIR/local.conf $TOP_DIR/localrc $TOP_DIR/.localrc.auto\n\n# ``stack.sh`` is customizable by setting environment variables.  Override a\n# default setting via export:\n#\n#     export DATABASE_PASSWORD=anothersecret\n#     ./stack.sh\n#\n# or by setting the variable on the command line:\n#\n#     DATABASE_PASSWORD=simple ./stack.sh\n#\n# Persistent variables can be placed in a ``local.conf`` file:\n#\n#     [[local|localrc]]\n#     DATABASE_PASSWORD=anothersecret\n#     DATABASE_USER=hellaroot\n#\n# We try to have sensible defaults, so you should be able to run ``./stack.sh``\n# in most cases.  ``local.conf`` is not distributed with DevStack and will never\n# be overwritten by a DevStack update.\n#\n# DevStack distributes ``stackrc`` which contains locations for the OpenStack\n# repositories, branches to configure, and other configuration defaults.\n# ``stackrc`` sources the ``localrc`` section of ``local.conf`` to allow you to\n# safely override those settings.\n\nif [[ ! -r $TOP_DIR/stackrc ]]; then\n    die $LINENO \"missing $TOP_DIR/stackrc - did you grab more than just stack.sh?\"\nfi\nsource $TOP_DIR/stackrc\n\n# write /etc/devstack-version\nwrite_devstack_version\n\n# Warn users who aren't on an explicitly supported distro, but allow them to\n# override check and attempt installation with ``FORCE=yes ./stack``\nSUPPORTED_DISTROS=\"bookworm|jammy|noble|rhel9\"\n\nif [[ ! ${DISTRO} =~ $SUPPORTED_DISTROS ]]; then\n    echo \"WARNING: this script has not been tested on $DISTRO\"\n    if [[ \"$FORCE\" != \"yes\" ]]; then\n        die $LINENO \"If you wish to run this script anyway run with FORCE=yes\"\n    fi\nfi\n\n# Local Settings\n# --------------\n\n# Make sure the proxy config is visible to sub-processes\nexport_proxy_variables\n\n# Remove services which were negated in ``ENABLED_SERVICES``\n# using the \"-\" prefix (e.g., \"-rabbit\") instead of\n# calling disable_service().\ndisable_negated_services\n\n\n# Configure sudo\n# --------------\n\n# We're not as **root** so make sure ``sudo`` is available\nis_package_installed sudo || is_package_installed sudo-ldap || install_package sudo\n\n# UEC images ``/etc/sudoers`` does not have a ``#includedir``, add one\nsudo grep -q \"^#includedir.*/etc/sudoers.d\" /etc/sudoers ||\n    echo \"#includedir /etc/sudoers.d\" | sudo tee -a /etc/sudoers\n\n# Conditionally setup detailed logging for sudo\nif [[ -n \"$LOG_SUDO\" ]]; then\n    TEMPFILE=`mktemp`\n    echo \"Defaults log_output\" > $TEMPFILE\n    chmod 0440 $TEMPFILE\n    sudo chown root:root $TEMPFILE\n    sudo mv $TEMPFILE /etc/sudoers.d/00_logging\nfi\n\n# Set up DevStack sudoers\nTEMPFILE=`mktemp`\necho \"$STACK_USER ALL=(root) NOPASSWD:ALL\" >$TEMPFILE\n# Some binaries might be under ``/sbin`` or ``/usr/sbin``, so make sure sudo will\n# see them by forcing ``PATH``\necho \"Defaults:$STACK_USER secure_path=/sbin:/usr/sbin:/usr/bin:/bin:/usr/local/sbin:/usr/local/bin\" >> $TEMPFILE\necho \"Defaults:$STACK_USER !requiretty\" >> $TEMPFILE\nchmod 0440 $TEMPFILE\nsudo chown root:root $TEMPFILE\nsudo mv $TEMPFILE /etc/sudoers.d/50_stack_sh\n\n# Configure Distro Repositories\n# -----------------------------\n\n# For Debian/Ubuntu make apt attempt to retry network ops on it's own\nif is_ubuntu; then\n    echo 'APT::Acquire::Retries \"20\";' | sudo tee /etc/apt/apt.conf.d/80retry  >/dev/null\nfi\n\n# Some distros need to add repos beyond the defaults provided by the vendor\n# to pick up required packages.\n\nfunction _install_epel {\n    # epel-release is in extras repo which is enabled by default\n    install_package epel-release\n\n    # RDO repos are not tested with epel and may have incompatibilities so\n    # let's limit the packages fetched from epel to the ones not in RDO repos.\n    sudo dnf config-manager --save --setopt=includepkgs=debootstrap,dpkg epel\n}\n\nfunction _install_rdo {\n    if [[ $DISTRO == \"rhel9\" ]]; then\n        rdo_release=${TARGET_BRANCH#*/}\n        if [[ \"$TARGET_BRANCH\" == \"master\" ]]; then\n            # adding delorean-deps repo to provide current master rpms\n            sudo wget https://trunk.rdoproject.org/centos9-master/delorean-deps.repo -O /etc/yum.repos.d/delorean-deps.repo\n        else\n            if sudo dnf provides centos-release-openstack-${rdo_release} >/dev/null 2>&1; then\n                sudo dnf -y install centos-release-openstack-${rdo_release}\n            else\n                sudo wget https://trunk.rdoproject.org/centos9-${rdo_release}/delorean-deps.repo -O /etc/yum.repos.d/delorean-deps.repo\n            fi\n        fi\n    fi\n    sudo dnf -y update\n}\n\n\n# Configure Target Directories\n# ----------------------------\n\n# Destination path for installation ``DEST``\nDEST=${DEST:-/opt/stack}\n\n# Create the destination directory and ensure it is writable by the user\n# and read/executable by everybody for daemons (e.g. apache run for horizon)\n# If directory exists do not modify the permissions.\nif [[ ! -d $DEST ]]; then\n    sudo mkdir -p $DEST\n    safe_chown -R $STACK_USER $DEST\n    safe_chmod 0755 $DEST\nfi\n\n# Destination path for devstack logs\nif [[ -n ${LOGDIR:-} ]]; then\n    sudo mkdir -p $LOGDIR\n    safe_chown -R $STACK_USER $LOGDIR\n    safe_chmod 0755 $LOGDIR\nfi\n\n# Destination path for service data\nDATA_DIR=${DATA_DIR:-${DEST}/data}\nif [[ ! -d $DATA_DIR ]]; then\n    sudo mkdir -p $DATA_DIR\n    safe_chown -R $STACK_USER $DATA_DIR\n    safe_chmod 0755 $DATA_DIR\nfi\n\n# Create and/or clean the async state directory\nasync_init\n\n# Configure proper hostname\n# Certain services such as rabbitmq require that the local hostname resolves\n# correctly.  Make sure it exists in /etc/hosts so that is always true.\nLOCAL_HOSTNAME=`hostname -s`\nif ! fgrep -qwe \"$LOCAL_HOSTNAME\" /etc/hosts; then\n    sudo sed -i \"s/\\(^127.0.0.1.*\\)/\\1 $LOCAL_HOSTNAME/\" /etc/hosts\nfi\n\n# If you have all the repos installed above already setup (e.g. a CI\n# situation where they are on your image) you may choose to skip this\n# to speed things up\nSKIP_EPEL_INSTALL=$(trueorfalse False SKIP_EPEL_INSTALL)\n\nif [[ $DISTRO == \"rhel8\" ]]; then\n    # If we have /etc/ci/mirror_info.sh assume we're on a OpenStack CI\n    # node, where EPEL is installed (but disabled) and already\n    # pointing at our internal mirror\n    if [[ -f /etc/ci/mirror_info.sh ]]; then\n        SKIP_EPEL_INSTALL=True\n        sudo dnf config-manager --set-enabled epel\n    fi\n\n    # PowerTools repo provides libyaml-devel required by devstack itself and\n    # EPEL packages assume that the PowerTools repository is enable.\n    sudo dnf config-manager --set-enabled PowerTools\n\n    # CentOS 8.3 changed the repository name to lower case.\n    sudo dnf config-manager --set-enabled powertools\n\n    if [[ ${SKIP_EPEL_INSTALL} != True ]]; then\n        _install_epel\n    fi\n    # Along with EPEL, CentOS (and a-likes) require some packages only\n    # available in RDO repositories (e.g. OVS, or later versions of\n    # kvm) to run.\n    _install_rdo\n\n    # NOTE(cgoncalves): workaround RHBZ#1154272\n    # dnf fails for non-privileged users when expired_repos.json doesn't exist.\n    # RHBZ: https://bugzilla.redhat.com/show_bug.cgi?id=1154272\n    # Patch: https://github.com/rpm-software-management/dnf/pull/1448\n    echo \"[]\" | sudo tee /var/cache/dnf/expired_repos.json\nelif [[ $DISTRO == \"rhel9\" ]]; then\n    # for CentOS Stream 9 repository\n    sudo dnf config-manager --set-enabled crb\n    # for RHEL 9 repository\n    sudo dnf config-manager --set-enabled codeready-builder-for-rhel-9-x86_64-rpms\n    # rabbitmq and other packages are provided by RDO repositories.\n    _install_rdo\n\n    # Some distributions (Rocky Linux 9) provide curl-minimal instead of curl,\n    # it triggers a conflict when devstack wants to install \"curl\".\n    # Swap curl-minimal with curl.\n    if is_package_installed curl-minimal; then\n        sudo dnf swap -y curl-minimal curl\n    fi\nelif [[ $DISTRO == \"openEuler-22.03\" ]]; then\n    # There are some problem in openEuler. We should fix it first. Some required\n    # package/action runs before fixup script. So we can't fix there.\n    #\n    # 1. the hostname package is not installed by default\n    # 2. Some necessary packages are in openstack repo, for example liberasurecode-devel\n    # 3. python3-pip can be uninstalled by `get_pip.py` automaticly.\n    # 4. Ensure wget installation before use\n    install_package hostname openstack-release-wallaby wget\n    uninstall_package python3-pip\n\n    # Add yum repository for libvirt7.X\n    sudo wget https://eur.openeuler.openatom.cn/coprs/g/sig-openstack/Libvirt-7.X/repo/openeuler-22.03_LTS/group_sig-openstack-Libvirt-7.X-openeuler-22.03_LTS.repo -O /etc/yum.repos.d/libvirt7.2.0.repo\nfi\n\n# Ensure python is installed\n# --------------------------\ninstall_python\n\n\n# Configure Logging\n# -----------------\n\n# Set up logging level\nVERBOSE=$(trueorfalse True VERBOSE)\nVERBOSE_NO_TIMESTAMP=$(trueorfalse False VERBOSE)\n\n# Draw a spinner so the user knows something is happening\nfunction spinner {\n    local delay=0.75\n    local spinstr='/-\\|'\n    printf \"...\" >&3\n    while [ true ]; do\n        local temp=${spinstr#?}\n        printf \"[%c]\" \"$spinstr\" >&3\n        local spinstr=$temp${spinstr%\"$temp\"}\n        sleep $delay\n        printf \"\\b\\b\\b\" >&3\n    done\n}\n\nfunction kill_spinner {\n    if [ ! -z \"$LAST_SPINNER_PID\" ]; then\n        kill >/dev/null 2>&1 $LAST_SPINNER_PID\n        printf \"\\b\\b\\bdone\\n\" >&3\n    fi\n}\n\n# Echo text to the log file, summary log file and stdout\n# echo_summary \"something to say\"\nfunction echo_summary {\n    if [[ -t 3 && \"$VERBOSE\" != \"True\" ]]; then\n        kill_spinner\n        echo -n -e $@ >&6\n        spinner &\n        LAST_SPINNER_PID=$!\n    else\n        echo -e $@ >&6\n    fi\n}\n\n# Echo text only to stdout, no log files\n# echo_nolog \"something not for the logs\"\nfunction echo_nolog {\n    echo $@ >&3\n}\n\n# Set up logging for ``stack.sh``\n# Set ``LOGFILE`` to turn on logging\n# Append '.xxxxxxxx' to the given name to maintain history\n# where 'xxxxxxxx' is a representation of the date the file was created\nTIMESTAMP_FORMAT=${TIMESTAMP_FORMAT:-\"%F-%H%M%S\"}\nLOGDAYS=${LOGDAYS:-7}\nCURRENT_LOG_TIME=$(date \"+$TIMESTAMP_FORMAT\")\n\nif [[ -n \"$LOGFILE\" ]]; then\n    # Clean up old log files.  Append '.*' to the user-specified\n    # ``LOGFILE`` to match the date in the search template.\n    LOGFILE_DIR=\"${LOGFILE%/*}\"           # dirname\n    LOGFILE_NAME=\"${LOGFILE##*/}\"         # basename\n    mkdir -p $LOGFILE_DIR\n    find $LOGFILE_DIR -maxdepth 1 -name $LOGFILE_NAME.\\* -mtime +$LOGDAYS -exec rm {} \\;\n    LOGFILE=$LOGFILE.${CURRENT_LOG_TIME}\n    SUMFILE=$LOGFILE.summary.${CURRENT_LOG_TIME}\n\n    # Redirect output according to config\n\n    # Set fd 3 to a copy of stdout. So we can set fd 1 without losing\n    # stdout later.\n    exec 3>&1\n    if [[ \"$VERBOSE\" == \"True\" ]]; then\n        _of_args=\"-v\"\n        if [[ \"$VERBOSE_NO_TIMESTAMP\" == \"True\" ]]; then\n            _of_args=\"$_of_args --no-timestamp\"\n        fi\n        # Set fd 1 and 2 to write the log file\n        exec 1> >( $PYTHON $TOP_DIR/tools/outfilter.py $_of_args -o \"${LOGFILE}\" ) 2>&1\n        # Set fd 6 to summary log file\n        exec 6> >( $PYTHON $TOP_DIR/tools/outfilter.py -o \"${SUMFILE}\" )\n    else\n        # Set fd 1 and 2 to primary logfile\n        exec 1> >( $PYTHON $TOP_DIR/tools/outfilter.py -o \"${LOGFILE}\" ) 2>&1\n        # Set fd 6 to summary logfile and stdout\n        exec 6> >( $PYTHON $TOP_DIR/tools/outfilter.py -v -o \"${SUMFILE}\" >&3 )\n    fi\n\n    echo_summary \"stack.sh log $LOGFILE\"\n    # Specified logfile name always links to the most recent log\n    ln -sf $LOGFILE $LOGFILE_DIR/$LOGFILE_NAME\n    ln -sf $SUMFILE $LOGFILE_DIR/$LOGFILE_NAME.summary\nelse\n    # Set up output redirection without log files\n    # Set fd 3 to a copy of stdout. So we can set fd 1 without losing\n    # stdout later.\n    exec 3>&1\n    if [[ \"$VERBOSE\" != \"True\" ]]; then\n        # Throw away stdout and stderr\n        exec 1>/dev/null 2>&1\n    fi\n    # Always send summary fd to original stdout\n    exec 6> >( $PYTHON $TOP_DIR/tools/outfilter.py -v >&3 )\nfi\n\n# Basic test for ``$DEST`` path permissions (fatal on error unless skipped)\ncheck_path_perm_sanity ${DEST}\n\n# Configure Error Traps\n# ---------------------\n\n# Kill background processes on exit\ntrap exit_trap EXIT\nfunction exit_trap {\n    local r=$?\n    jobs=$(jobs -p)\n    # Only do the kill when we're logging through a process substitution,\n    # which currently is only to verbose logfile\n    if [[ -n $jobs && -n \"$LOGFILE\" && \"$VERBOSE\" == \"True\" ]]; then\n        echo \"exit_trap: cleaning up child processes\"\n        kill 2>&1 $jobs\n    fi\n\n    #Remove timing data file\n    if [ -f \"$OSCWRAP_TIMER_FILE\" ] ; then\n        rm \"$OSCWRAP_TIMER_FILE\"\n    fi\n\n    # Kill the last spinner process\n    kill_spinner\n\n    if [[ $r -ne 0 ]]; then\n        echo \"Error on exit\"\n        # If we error before we've installed os-testr, this will fail.\n        if type -p generate-subunit > /dev/null; then\n            generate-subunit $DEVSTACK_START_TIME $SECONDS 'fail' >> ${SUBUNIT_OUTPUT}\n        fi\n        if [[ -z $LOGDIR ]]; then\n            ${PYTHON} $TOP_DIR/tools/worlddump.py\n        else\n            ${PYTHON} $TOP_DIR/tools/worlddump.py -d $LOGDIR\n        fi\n    else\n        # If we error before we've installed os-testr, this will fail.\n        if type -p generate-subunit > /dev/null; then\n            generate-subunit $DEVSTACK_START_TIME $SECONDS >> ${SUBUNIT_OUTPUT}\n        fi\n    fi\n\n    exit $r\n}\n\n# Exit on any errors so that errors don't compound\ntrap err_trap ERR\nfunction err_trap {\n    local r=$?\n    set +o xtrace\n    if [[ -n \"$LOGFILE\" ]]; then\n        echo \"${0##*/} failed: full log in $LOGFILE\"\n    else\n        echo \"${0##*/} failed\"\n    fi\n    exit $r\n}\n\n# Begin trapping error exit codes\nset -o errexit\n\n# Print the kernel version\nuname -a\n\n# Reset the bundle of CA certificates\nSSL_BUNDLE_FILE=\"$DATA_DIR/ca-bundle.pem\"\nrm -f $SSL_BUNDLE_FILE\n\n# Import common services (database, message queue) configuration\nsource $TOP_DIR/lib/database\nsource $TOP_DIR/lib/rpc_backend\n\n# load host tuning functions and defaults\nsource $TOP_DIR/lib/host\n# tune host memory early to ensure zswap/ksm are configured before\n# doing memory intensive operation like cloning repos or unpacking packages.\ntune_host\n\n# Configure Projects\n# ==================\n\n# Clone all external plugins\nfetch_plugins\n\n# Plugin Phase 0: override_defaults - allow plugins to override\n# defaults before other services are run\nrun_phase override_defaults\n\n# Import Apache functions\nsource $TOP_DIR/lib/apache\n\n# Import TLS functions\nsource $TOP_DIR/lib/tls\n\n# Source project function libraries\nsource $TOP_DIR/lib/infra\nsource $TOP_DIR/lib/libraries\nsource $TOP_DIR/lib/lvm\nsource $TOP_DIR/lib/horizon\nsource $TOP_DIR/lib/keystone\nsource $TOP_DIR/lib/glance\nsource $TOP_DIR/lib/nova\nsource $TOP_DIR/lib/placement\nsource $TOP_DIR/lib/cinder\nsource $TOP_DIR/lib/swift\nsource $TOP_DIR/lib/neutron\nsource $TOP_DIR/lib/ldap\nsource $TOP_DIR/lib/dstat\nsource $TOP_DIR/lib/tcpdump\nsource $TOP_DIR/lib/etcd3\nsource $TOP_DIR/lib/os-vif\n\n# Extras Source\n# --------------\n\n# Phase: source\nrun_phase source\n\n\n# Interactive Configuration\n# -------------------------\n\n# Do all interactive config up front before the logging spew begins\n\n# Generic helper to configure passwords\nfunction read_password {\n    local xtrace\n    xtrace=$(set +o | grep xtrace)\n    set +o xtrace\n    var=$1; msg=$2\n    pw=${!var}\n\n    if [[ -f $RC_DIR/localrc ]]; then\n        localrc=$TOP_DIR/localrc\n    else\n        localrc=$TOP_DIR/.localrc.password\n    fi\n\n    # If the password is not defined yet, proceed to prompt user for a password.\n    if [ ! $pw ]; then\n        # If there is no localrc file, create one\n        if [ ! -e $localrc ]; then\n            touch $localrc\n        fi\n\n        # Presumably if we got this far it can only be that our\n        # localrc is missing the required password.  Prompt user for a\n        # password and write to localrc.\n\n        echo ''\n        echo '################################################################################'\n        echo $msg\n        echo '################################################################################'\n        echo \"This value will be written to ${localrc} file so you don't have to enter it \"\n        echo \"again.  Use only alphanumeric characters.\"\n        echo \"If you leave this blank, a random default value will be used.\"\n        pw=\" \"\n        while true; do\n            echo \"Enter a password now:\"\n            read -e $var\n            pw=${!var}\n            [[ \"$pw\" = \"`echo $pw | tr -cd [:alnum:]`\" ]] && break\n            echo \"Invalid chars in password.  Try again:\"\n        done\n        if [ ! $pw ]; then\n            pw=$(generate_hex_string 10)\n        fi\n        eval \"$var=$pw\"\n        echo \"$var=$pw\" >> $localrc\n    fi\n\n    # restore previous xtrace value\n    $xtrace\n}\n\n\n# Database Configuration\n# ----------------------\n\n# To select between database backends, add the following to ``local.conf``:\n#\n#    disable_service mysql\n#    enable_service postgresql\n#\n# The available database backends are listed in ``DATABASE_BACKENDS`` after\n# ``lib/database`` is sourced. ``mysql`` is the default.\n\nif initialize_database_backends; then\n    echo \"Using $DATABASE_TYPE database backend\"\n    # Last chance for the database password. This must be handled here\n    # because read_password is not a library function.\n    read_password DATABASE_PASSWORD \"ENTER A PASSWORD TO USE FOR THE DATABASE.\"\n\n    define_database_baseurl\nelse\n    echo \"No database enabled\"\nfi\n\n\n# Queue Configuration\n# -------------------\n\n# Rabbit connection info\n# In multi node DevStack, second node needs ``RABBIT_USERID``, but rabbit\n# isn't enabled.\nif is_service_enabled rabbit; then\n    read_password RABBIT_PASSWORD \"ENTER A PASSWORD TO USE FOR RABBIT.\"\nfi\n\n\n# Keystone\n# --------\n\nif is_service_enabled keystone; then\n    # Services authenticate to Identity with servicename/``SERVICE_PASSWORD``\n    read_password SERVICE_PASSWORD \"ENTER A SERVICE_PASSWORD TO USE FOR THE SERVICE AUTHENTICATION.\"\n    # Horizon currently truncates usernames and passwords at 20 characters\n    read_password ADMIN_PASSWORD \"ENTER A PASSWORD TO USE FOR HORIZON AND KEYSTONE (20 CHARS OR LESS).\"\n\n    # Keystone can now optionally install OpenLDAP by enabling the ``ldap``\n    # service in ``local.conf`` (e.g. ``enable_service ldap``).\n    # To clean out the Keystone contents in OpenLDAP set ``KEYSTONE_CLEAR_LDAP``\n    # to ``yes`` (e.g. ``KEYSTONE_CLEAR_LDAP=yes``) in ``local.conf``.  To enable the\n    # Keystone Identity Driver (``keystone.identity.backends.ldap.Identity``)\n    # set ``KEYSTONE_IDENTITY_BACKEND`` to ``ldap`` (e.g.\n    # ``KEYSTONE_IDENTITY_BACKEND=ldap``) in ``local.conf``.\n\n    # Only request LDAP password if the service is enabled\n    if is_service_enabled ldap; then\n        read_password LDAP_PASSWORD \"ENTER A PASSWORD TO USE FOR LDAP\"\n    fi\nfi\n\n\n# Swift\n# -----\n\nif is_service_enabled s-proxy; then\n    # We only ask for Swift Hash if we have enabled swift service.\n    # ``SWIFT_HASH`` is a random unique string for a swift cluster that\n    # can never change.\n    read_password SWIFT_HASH \"ENTER A RANDOM SWIFT HASH.\"\n\n    if [[ -z \"$SWIFT_TEMPURL_KEY\" ]] && [[ \"$SWIFT_ENABLE_TEMPURLS\" == \"True\" ]]; then\n        read_password SWIFT_TEMPURL_KEY \"ENTER A KEY FOR SWIFT TEMPURLS.\"\n    fi\nfi\n\n# Save configuration values\nsave_stackenv $LINENO\n\n\n# Install Packages\n# ================\n\n# OpenStack uses a fair number of other projects.\n\n# Bring down global requirements before any use of pip_install. This is\n# necessary to ensure that the constraints file is in place before we\n# attempt to apply any constraints to pip installs.\n# We always need the master branch in addition to any stable branch, so\n# override GIT_DEPTH here.\nGIT_DEPTH=0 git_clone $REQUIREMENTS_REPO $REQUIREMENTS_DIR $REQUIREMENTS_BRANCH\n\n# Install package requirements\n# Source it so the entire environment is available\necho_summary \"Installing package prerequisites\"\nsource $TOP_DIR/tools/install_prereqs.sh\n\n# Configure an appropriate Python environment.\n#\n# NOTE(ianw) 2021-08-11 : We install the latest pip here because pip\n# is very active and changes are not generally reflected in the LTS\n# distros.  This often involves important things like dependency or\n# conflict resolution, and has often been required because the\n# complicated constraints etc. used by openstack have tickled bugs in\n# distro versions of pip.  We want to find these problems as they\n# happen, rather than years later when we try to update our LTS\n# distro.  Whilst it is clear that global installations of upstream\n# pip are less and less common, with virtualenv's being the general\n# approach now; there are a lot of devstack plugins that assume a\n# global install environment.\nif [[ \"$OFFLINE\" != \"True\" ]]; then\n    PYPI_ALTERNATIVE_URL=${PYPI_ALTERNATIVE_URL:-\"\"} $TOP_DIR/tools/install_pip.sh\nfi\n\n# Do the ugly hacks for broken packages and distros\nsource $TOP_DIR/tools/fixup_stuff.sh\nfixup_all\n\nif [[ \"$GLOBAL_VENV\" == \"True\" ]] ; then\n    # TODO(frickler): find a better solution for this\n    sudo ln -sf /opt/stack/data/venv/bin/cinder-manage /usr/local/bin\n    sudo ln -sf /opt/stack/data/venv/bin/cinder-rtstool /usr/local/bin\n    sudo ln -sf /opt/stack/data/venv/bin/glance /usr/local/bin\n    sudo ln -sf /opt/stack/data/venv/bin/nova-manage /usr/local/bin\n    sudo ln -sf /opt/stack/data/venv/bin/openstack /usr/local/bin\n    sudo ln -sf /opt/stack/data/venv/bin/privsep-helper /usr/local/bin\n    sudo ln -sf /opt/stack/data/venv/bin/rally /usr/local/bin\n    sudo ln -sf /opt/stack/data/venv/bin/tox /usr/local/bin\n\n    setup_devstack_virtualenv\nfi\n\n# Install subunit for the subunit output stream\npip_install -U os-testr\n\n# the default rate limit of 1000 messages / 30 seconds is not\n# sufficient given how verbose our logging is.\niniset -sudo /etc/systemd/journald.conf \"Journal\" \"RateLimitBurst\" \"0\"\nsudo systemctl restart systemd-journald\n\n# Virtual Environment\n# -------------------\n\n# Install required infra support libraries\ninstall_infra\n\n# Install bindep\n$VIRTUALENV_CMD $DEST/bindep-venv\n# TODO(ianw) : optionally install from zuul checkout?\n$DEST/bindep-venv/bin/pip install bindep\nexport BINDEP_CMD=${DEST}/bindep-venv/bin/bindep\n\n# Install packages as defined in plugin bindep.txt files\npkgs=\"$( _get_plugin_bindep_packages )\"\nif [[ -n \"${pkgs}\" ]]; then\n    install_package ${pkgs}\nfi\n\n# Extras Pre-install\n# ------------------\n# Phase: pre-install\nrun_phase stack pre-install\n\n# NOTE(danms): Set global limits before installing anything\nset_systemd_override DefaultLimitNOFILE ${ULIMIT_NOFILE}\n\ninstall_rpc_backend\nrestart_rpc_backend\n\nif is_service_enabled $DATABASE_BACKENDS; then\n    install_database\nfi\nif [ -n \"$DATABASE_TYPE\" ]; then\n    install_database_python\nfi\n\nif is_service_enabled neutron; then\n    install_neutron_agent_packages\nfi\n\nif is_service_enabled etcd3; then\n    install_etcd3\nfi\n\n# Setup TLS certs\n# ---------------\n\n# Do this early, before any webservers are set up to ensure\n# we don't run into problems with missing certs when apache\n# is restarted.\nif is_service_enabled tls-proxy; then\n    configure_CA\n    init_CA\n    init_cert\nfi\n\n# Dstat\n# -----\n\n# Install dstat services prerequisites\ninstall_dstat\n\n\n# Check Out and Install Source\n# ----------------------------\n\necho_summary \"Installing OpenStack project source\"\n\n# Install additional libraries\ninstall_libs\n\n# Install uwsgi\ninstall_apache_uwsgi\n\n# Install client libraries\ninstall_keystoneauth\ninstall_keystoneclient\ninstall_glanceclient\ninstall_cinderclient\ninstall_novaclient\nif is_service_enabled swift glance horizon; then\n    install_swiftclient\nfi\nif is_service_enabled neutron nova horizon; then\n    install_neutronclient\nfi\n\n# Install middleware\ninstall_keystonemiddleware\n\nif is_service_enabled keystone; then\n    if [ \"$KEYSTONE_SERVICE_HOST\" == \"$SERVICE_HOST\" ]; then\n        stack_install_service keystone\n        configure_keystone\n    fi\nfi\n\nif is_service_enabled swift; then\n    if is_service_enabled ceilometer; then\n        install_ceilometermiddleware\n    fi\n    stack_install_service swift\n    configure_swift\n\n    # s3api middleware to provide S3 emulation to Swift\n    if is_service_enabled s3api; then\n        # Replace the nova-objectstore port by the swift port\n        S3_SERVICE_PORT=8080\n    fi\nfi\n\nif is_service_enabled g-api n-api; then\n    # Image catalog service\n    stack_install_service glance\n    configure_glance\nfi\n\nif is_service_enabled cinder; then\n    # Block volume service\n    stack_install_service cinder\n    configure_cinder\nfi\n\nif is_service_enabled neutron; then\n    # Network service\n    stack_install_service neutron\nfi\n\nif is_service_enabled nova; then\n    # Compute service\n    stack_install_service nova\n    configure_nova\nfi\n\nif is_service_enabled placement; then\n    # placement api\n    stack_install_service placement\n    configure_placement\nfi\n\n# create a placement-client fake service to know we need to configure\n# placement connectivity. We configure the placement service for nova\n# if placement-api or placement-client is active, and n-cpu on the\n# same box.\nif is_service_enabled placement placement-client; then\n    if is_service_enabled n-cpu || is_service_enabled n-sch; then\n        configure_placement_nova_compute\n    fi\nfi\n\nif is_service_enabled horizon; then\n    # dashboard\n    stack_install_service horizon\nfi\n\nif is_service_enabled tls-proxy; then\n    fix_system_ca_bundle_path\nfi\n\n# Extras Install\n# --------------\n\n# Phase: install\nrun_phase stack install\n\n# Install the OpenStack client, needed for most setup commands\nif use_library_from_git \"python-openstackclient\"; then\n    git_clone_by_name \"python-openstackclient\"\n    setup_dev_lib \"python-openstackclient\"\nelse\n    pip_install_gr python-openstackclient\n    if is_service_enabled openstack-cli-server; then\n        install_openstack_cli_server\n    fi\nfi\n\n# Installs alias for osc so that we can collect timing for all\n# osc commands. Alias dies with stack.sh.\ninstall_oscwrap\n\n# Syslog\n# ------\n\nif [[ $SYSLOG != \"False\" ]]; then\n    if [[ \"$SYSLOG_HOST\" = \"$HOST_IP\" ]]; then\n        # Configure the master host to receive\n        cat <<EOF | sudo tee /etc/rsyslog.d/90-stack-m.conf >/dev/null\n\\$ModLoad imrelp\n\\$InputRELPServerRun $SYSLOG_PORT\nEOF\n    else\n        # Set rsyslog to send to remote host\n        cat <<EOF | sudo tee /etc/rsyslog.d/90-stack-s.conf >/dev/null\n*.*\t\t:omrelp:$SYSLOG_HOST:$SYSLOG_PORT\nEOF\n    fi\n\n    RSYSLOGCONF=\"/etc/rsyslog.conf\"\n    if [ -f $RSYSLOGCONF ]; then\n        sudo cp -b $RSYSLOGCONF $RSYSLOGCONF.bak\n        if [[ $(grep '$SystemLogRateLimitBurst' $RSYSLOGCONF)  ]]; then\n            sudo sed -i 's/$SystemLogRateLimitBurst\\ .*/$SystemLogRateLimitBurst\\ 0/' $RSYSLOGCONF\n        else\n            sudo sed -i '$ i $SystemLogRateLimitBurst\\ 0' $RSYSLOGCONF\n        fi\n        if [[ $(grep '$SystemLogRateLimitInterval' $RSYSLOGCONF)  ]]; then\n            sudo sed -i 's/$SystemLogRateLimitInterval\\ .*/$SystemLogRateLimitInterval\\ 0/' $RSYSLOGCONF\n        else\n            sudo sed -i '$ i $SystemLogRateLimitInterval\\ 0' $RSYSLOGCONF\n        fi\n    fi\n\n    echo_summary \"Starting rsyslog\"\n    restart_service rsyslog\nfi\n\n\n# Export Certificate Authority Bundle\n# -----------------------------------\n\n# If certificates were used and written to the SSL bundle file then these\n# should be exported so clients can validate their connections.\n\nif [ -f $SSL_BUNDLE_FILE ]; then\n    export OS_CACERT=$SSL_BUNDLE_FILE\nfi\n\n\n# Configure database\n# ------------------\n\nif is_service_enabled $DATABASE_BACKENDS; then\n    configure_database\nfi\n\n# Save configuration values\nsave_stackenv $LINENO\n\n\n# Start Services\n# ==============\n\n# Dstat\n# -----\n\n# A better kind of sysstat, with the top process per time slice\nstart_dstat\n\n# Run a background tcpdump for debugging\n# Note: must set TCPDUMP_ARGS with the enabled service\nif is_service_enabled tcpdump; then\n    start_tcpdump\nfi\n\n# Etcd\n# -----\n\n# etcd is a distributed key value store that provides a reliable way to store data across a cluster of machines\nif is_service_enabled etcd3; then\n    start_etcd3\nfi\n\n# Keystone\n# --------\n\nif is_service_enabled tls-proxy; then\n    start_tls_proxy http-services '*' 443 $SERVICE_HOST 80\nfi\n\n# Write a clouds.yaml file and use the devstack-admin cloud\nwrite_clouds_yaml\nexport OS_CLOUD=${OS_CLOUD:-devstack-admin}\n\nif is_service_enabled keystone; then\n    echo_summary \"Starting Keystone\"\n\n    if [ \"$KEYSTONE_SERVICE_HOST\" == \"$SERVICE_HOST\" ]; then\n        init_keystone\n        start_keystone\n        bootstrap_keystone\n    fi\n\n    create_keystone_accounts\n    if is_service_enabled nova; then\n        async_runfunc create_nova_accounts\n    fi\n    if is_service_enabled glance; then\n        async_runfunc create_glance_accounts\n    fi\n    if is_service_enabled cinder; then\n        async_runfunc create_cinder_accounts\n    fi\n    if is_service_enabled neutron; then\n        async_runfunc create_neutron_accounts\n    fi\n    if is_service_enabled swift; then\n        async_runfunc create_swift_accounts\n    fi\n\nfi\n\n# Horizon\n# -------\n\nif is_service_enabled horizon; then\n    echo_summary \"Configuring Horizon\"\n    async_runfunc configure_horizon\nfi\n\nasync_wait create_nova_accounts create_glance_accounts create_cinder_accounts\nasync_wait create_neutron_accounts create_swift_accounts configure_horizon\n\n# Glance\n# ------\n\n# NOTE(yoctozepto): limited to node hosting the database which is the controller\nif is_service_enabled $DATABASE_BACKENDS && is_service_enabled glance; then\n    echo_summary \"Configuring Glance\"\n    async_runfunc init_glance\nfi\n\n\n# Neutron\n# -------\n\nif is_service_enabled neutron; then\n    echo_summary \"Configuring Neutron\"\n\n    configure_neutron\n\n    # Run init_neutron only on the node hosting the Neutron API server\n    if is_service_enabled $DATABASE_BACKENDS && is_service_enabled neutron; then\n        async_runfunc init_neutron\n    fi\nfi\n\n\n# Nova\n# ----\n\nif is_service_enabled q-dhcp; then\n    # TODO(frickler): These are remnants from n-net, check which parts are really\n    # still needed for Neutron.\n    # Do not kill any dnsmasq instance spawned by NetworkManager\n    netman_pid=$(pidof NetworkManager || true)\n    if [ -z \"$netman_pid\" ]; then\n        sudo killall dnsmasq || true\n    else\n        sudo ps h -o pid,ppid -C dnsmasq | grep -v $netman_pid | awk '{print $1}' | sudo xargs kill || true\n    fi\n\n    clean_iptables\n\n    # Force IP forwarding on, just in case\n    sudo sysctl -w net.ipv4.ip_forward=1\nfi\n\n# os-vif\n# ------\nif is_service_enabled nova neutron; then\n    configure_os_vif\nfi\n\n# Storage Service\n# ---------------\n\nif is_service_enabled swift; then\n    echo_summary \"Configuring Swift\"\n    async_runfunc init_swift\nfi\n\n\n# Volume Service\n# --------------\n\nif is_service_enabled cinder; then\n    echo_summary \"Configuring Cinder\"\n    async_runfunc init_cinder\nfi\n\n# Placement Service\n# ---------------\n\nif is_service_enabled placement; then\n    echo_summary \"Configuring placement\"\n    async_runfunc init_placement\nfi\n\n# Wait for neutron and placement before starting nova\nasync_wait init_neutron\nasync_wait init_placement\nasync_wait init_glance\nasync_wait init_swift\nasync_wait init_cinder\n\n# Compute Service\n# ---------------\n\nif is_service_enabled nova; then\n    echo_summary \"Configuring Nova\"\n    init_nova\n\n    async_runfunc configure_neutron_nova\nfi\n\n\n# Extras Configuration\n# ====================\n\n# Phase: post-config\nrun_phase stack post-config\n\n\n# Local Configuration\n# ===================\n\n# Apply configuration from ``local.conf`` if it exists for layer 2 services\n# Phase: post-config\nmerge_config_group $TOP_DIR/local.conf post-config\n\n\n# Launch Services\n# ===============\n\n# Only run the services specified in ``ENABLED_SERVICES``\n\n# Launch Swift Services\nif is_service_enabled swift; then\n    echo_summary \"Starting Swift\"\n    start_swift\nfi\n\n# NOTE(lyarwood): By default use a single hardcoded fixed_key across devstack\n# deployments.  This ensures the keys match across nova and cinder across all\n# hosts.\nFIXED_KEY=${FIXED_KEY:-bae3516cc1c0eb18b05440eba8012a4a880a2ee04d584a9c1579445e675b12defdc716ec}\nif is_service_enabled cinder; then\n    iniset $CINDER_CONF key_manager fixed_key \"$FIXED_KEY\"\nfi\n\nasync_wait configure_neutron_nova\n\n# NOTE(clarkb): This must come after async_wait configure_neutron_nova because\n# configure_neutron_nova modifies $NOVA_CONF and $NOVA_CPU_CONF as well. If\n# we don't wait then these two ini updates race either other and can result\n# in unexpected configs.\nif is_service_enabled nova; then\n    iniset $NOVA_CONF key_manager fixed_key \"$FIXED_KEY\"\n    iniset $NOVA_CPU_CONF key_manager fixed_key \"$FIXED_KEY\"\nfi\n\n# Launch the nova-api and wait for it to answer before continuing\nif is_service_enabled n-api; then\n    echo_summary \"Starting Nova API\"\n    start_nova_api\nfi\n\nif is_service_enabled ovn-controller ovn-controller-vtep; then\n    echo_summary \"Starting OVN services\"\n    start_ovn_services\nfi\n\nif is_service_enabled neutron-api; then\n    echo_summary \"Starting Neutron\"\n    start_neutron_api\nelif is_service_enabled q-svc; then\n    echo_summary \"Starting Neutron\"\n    configure_neutron_after_post_config\n    start_neutron_service_and_check\nfi\n\n# Start placement before any of the service that are likely to want\n# to use it to manage resource providers.\nif is_service_enabled placement; then\n    echo_summary \"Starting Placement\"\n    start_placement\nfi\n\nif is_service_enabled neutron; then\n    start_neutron\nfi\n# Once neutron agents are started setup initial network elements\nif is_service_enabled q-svc && [[ \"$NEUTRON_CREATE_INITIAL_NETWORKS\" == \"True\" ]]; then\n    echo_summary \"Creating initial neutron network elements\"\n    # Here's where plugins can wire up their own networks instead\n    # of the code in lib/neutron_plugins/services/l3\n    if type -p neutron_plugin_create_initial_networks > /dev/null; then\n        neutron_plugin_create_initial_networks\n    else\n        create_neutron_initial_network\n    fi\n\nfi\n\nif is_service_enabled nova; then\n    echo_summary \"Starting Nova\"\n    start_nova\n    async_runfunc create_flavors\nfi\nif is_service_enabled cinder; then\n    echo_summary \"Starting Cinder\"\n    start_cinder\n    create_volume_types\nfi\n\n# This sleep is required for cinder volume service to become active and\n# publish capabilities to cinder scheduler before creating the image-volume\nif [[ \"$USE_CINDER_FOR_GLANCE\" == \"True\" ]]; then\n    sleep 30\nfi\n\n# Launch the Glance services\n# NOTE (abhishekk): We need to start glance api service only after cinder\n# service has started as on glance startup glance-api queries cinder for\n# validating volume_type configured for cinder store of glance.\nif is_service_enabled glance; then\n    echo_summary \"Starting Glance\"\n    start_glance\nfi\n\n# Install Images\n# ==============\n\n# Upload an image to Glance.\n#\n# The default image is CirrOS, a small testing image which lets you login as **root**\n# CirrOS has a ``cloud-init`` analog supporting login via keypair and sending\n# scripts as userdata.\n# See https://help.ubuntu.com/community/CloudInit for more on ``cloud-init``\n\n# NOTE(yoctozepto): limited to node hosting the database which is the controller\nif is_service_enabled $DATABASE_BACKENDS && is_service_enabled glance; then\n    echo_summary \"Uploading images\"\n\n    for image_url in ${IMAGE_URLS//,/ }; do\n        upload_image $image_url\n    done\nfi\n\nasync_wait create_flavors\n\nif is_service_enabled horizon; then\n    echo_summary \"Starting Horizon\"\n    init_horizon\n    start_horizon\nfi\n\n\n# Create account rc files\n# =======================\n\n# Creates source able script files for easier user switching.\n# This step also creates certificates for tenants and users,\n# which is helpful in image bundle steps.\n\nif is_service_enabled nova && is_service_enabled keystone; then\n    USERRC_PARAMS=\"-PA --target-dir $TOP_DIR/accrc --os-password $ADMIN_PASSWORD\"\n\n    if [ -f $SSL_BUNDLE_FILE ]; then\n        USERRC_PARAMS=\"$USERRC_PARAMS --os-cacert $SSL_BUNDLE_FILE\"\n    fi\n\n    $TOP_DIR/tools/create_userrc.sh $USERRC_PARAMS\nfi\n\n\n# Save some values we generated for later use\nsave_stackenv\n\n\n# Wrapup configuration\n# ====================\n\n# local.conf extra\n# ----------------\n\n# Apply configuration from ``local.conf`` if it exists for layer 2 services\n# Phase: extra\nmerge_config_group $TOP_DIR/local.conf extra\n\n\n# Run extras\n# ----------\n\n# Phase: extra\nrun_phase stack extra\n\n\n# local.conf post-extra\n# ---------------------\n\n# Apply late configuration from ``local.conf`` if it exists for layer 2 services\n# Phase: post-extra\nmerge_config_group $TOP_DIR/local.conf post-extra\n\n\n# Sanity checks\n# =============\n\n# Check that computes are all ready\n#\n# TODO(sdague): there should be some generic phase here.\nif is_service_enabled n-cpu; then\n    is_nova_ready\nfi\n\n# Check the status of running services\nservice_check\n\n# Configure nova cellsv2\n# ----------------------\n\n# Do this late because it requires compute hosts to have started\nif is_service_enabled n-api; then\n    if is_service_enabled n-cpu; then\n        $TOP_DIR/tools/discover_hosts.sh\n    else\n        # Some CI systems like Hyper-V build the control plane on\n        # Linux, and join in non Linux Computes after setup. This\n        # allows them to delay the processing until after their whole\n        # environment is up.\n        echo_summary \"SKIPPING Cell setup because n-cpu is not enabled. You will have to do this manually before you have a working environment.\"\n    fi\n    # Run the nova-status upgrade check command which can also be used\n    # to verify the base install. Note that this is good enough in a\n    # single node deployment, but in a multi-node setup it won't verify\n    # any subnodes - that would have to be driven from whatever tooling\n    # is deploying the subnodes, e.g. the zuul v3 devstack-multinode job.\n    $NOVA_BIN_DIR/nova-status --config-file $NOVA_CONF upgrade check\nfi\n\n# Run local script\n# ----------------\n\n# Run ``local.sh`` if it exists to perform user-managed tasks\nif [[ -x $TOP_DIR/local.sh ]]; then\n    echo \"Running user script $TOP_DIR/local.sh\"\n    $TOP_DIR/local.sh\nfi\n\n# Bash completion\n# ===============\n\n# Prepare bash completion for OSC\n# Note we use \"command\" to avoid the timing wrapper\n# which isn't relevant here and floods logs\ncommand openstack complete \\\n    | sudo tee /etc/bash_completion.d/osc.bash_completion > /dev/null\n\n# If cinder is configured, set global_filter for PV devices\nif is_service_enabled cinder; then\n    if is_ubuntu; then\n        echo_summary \"Configuring lvm.conf global device filter\"\n        set_lvm_filter\n    else\n        echo_summary \"Skip setting lvm filters for non Ubuntu systems\"\n    fi\nfi\n\n# Run test-config\n# ---------------\n\n# Phase: test-config\nrun_phase stack test-config\n\n# Apply late configuration from ``local.conf`` if it exists for layer 2 services\n# Phase: test-config\nmerge_config_group $TOP_DIR/local.conf test-config\n\n# Fin\n# ===\n\nset +o xtrace\n\nif [[ -n \"$LOGFILE\" ]]; then\n    exec 1>&3\n    # Force all output to stdout and logs now\n    exec 1> >( tee -a \"${LOGFILE}\" ) 2>&1\nelse\n    # Force all output to stdout now\n    exec 1>&3\nfi\n\n# Make sure we didn't leak any background tasks\nasync_cleanup\n\n# Dump out the time totals\ntime_totals\nasync_print_timing\n\nif is_service_enabled mysql; then\n    if [[ \"$MYSQL_GATHER_PERFORMANCE\" == \"True\" && \"$MYSQL_HOST\" ]]; then\n        echo \"\"\n        echo \"\"\n        echo \"Post-stack database query stats:\"\n        mysql -u $DATABASE_USER -p$DATABASE_PASSWORD -h $MYSQL_HOST stats -e \\\n              'SELECT * FROM queries' -t 2>/dev/null\n        mysql -u $DATABASE_USER -p$DATABASE_PASSWORD -h $MYSQL_HOST stats -e \\\n              'DELETE FROM queries' 2>/dev/null\n    fi\nfi\n\n\n# Using the cloud\n# ===============\n\necho \"\"\necho \"\"\necho \"\"\necho \"This is your host IP address: $HOST_IP\"\nif [ \"$HOST_IPV6\" != \"\" ]; then\n    echo \"This is your host IPv6 address: $HOST_IPV6\"\nfi\n\n# If you installed Horizon on this server you should be able\n# to access the site using your browser.\nif is_service_enabled horizon; then\n    echo \"Horizon is now available at http://$SERVICE_HOST$HORIZON_APACHE_ROOT\"\nfi\n\n# If Keystone is present you can point ``nova`` cli to this server\nif is_service_enabled keystone; then\n    echo \"Keystone is serving at $KEYSTONE_SERVICE_URI/\"\n    echo \"The default users are: admin and demo\"\n    echo \"The password: $ADMIN_PASSWORD\"\nfi\n\n# Warn that a deprecated feature was used\nif [[ -n \"$DEPRECATED_TEXT\" ]]; then\n    echo\n    echo -e \"WARNING: $DEPRECATED_TEXT\"\n    echo\nfi\n\necho\necho \"Services are running under systemd unit files.\"\necho \"For more information see: \"\necho \"https://docs.openstack.org/devstack/latest/systemd.html\"\necho\n\n# Useful info on current state\ncat /etc/devstack-version\necho\n\n# Indicate how long this took to run (bash maintained variable ``SECONDS``)\necho_summary \"stack.sh completed in $SECONDS seconds.\"\n\n\n# Restore/close logging file descriptors\nexec 1>&3\nexec 2>&3\nexec 3>&-\nexec 6>&-\n"
        },
        {
          "name": "stackrc",
          "type": "blob",
          "size": 36.7841796875,
          "content": "#!/bin/bash\n#\n# stackrc\n#\n\n# ensure we don't re-source this in the same environment\n[[ -z \"$_DEVSTACK_STACKRC\" ]] || return 0\ndeclare -r -g _DEVSTACK_STACKRC=1\n\n# Find the other rc files\nRC_DIR=$(cd $(dirname \"${BASH_SOURCE:-$0}\") && pwd)\n\n# Source required DevStack functions and globals\nsource $RC_DIR/functions\n\n# Set the target branch. This is used so that stable branching\n# does not need to update each repo below.\nTARGET_BRANCH=master\n\n# Cycle trailing projects need to branch later than the others.\nTRAILING_TARGET_BRANCH=master\n\n# And some repos do not create stable branches, so this is used\n# to make it explicit and avoid accidentally setting to a stable\n# branch.\nBRANCHLESS_TARGET_BRANCH=master\n\n# Destination path for installation\nDEST=/opt/stack\n\n# Destination for working data\nDATA_DIR=${DEST}/data\n\n# Destination for status files\nSERVICE_DIR=${DEST}/status\n\n# Path for subunit output file\nSUBUNIT_OUTPUT=${DEST}/devstack.subunit\n\n# Determine stack user\nif [[ $EUID -eq 0 ]]; then\n    STACK_USER=stack\nelse\n    STACK_USER=$(whoami)\nfi\n\n# Specify region name Region\nREGION_NAME=${REGION_NAME:-RegionOne}\n\n# Specify name of region where identity service endpoint is registered.\n# When deploying multiple DevStack instances in different regions with shared\n# Keystone, set KEYSTONE_REGION_NAME to the region where Keystone is running\n# for DevStack instances which do not host Keystone.\nKEYSTONE_REGION_NAME=${KEYSTONE_REGION_NAME:-$REGION_NAME}\n\n# Specify which services to launch.  These generally correspond to\n# screen tabs. To change the default list, use the ``enable_service`` and\n# ``disable_service`` functions in ``local.conf``.\n# For example, to enable Swift as part of DevStack add the following\n# settings in ``local.conf``:\n#  [[local|localrc]]\n#  enable_service s-proxy s-object s-container s-account\n# This allows us to pass ``ENABLED_SERVICES``\nif ! isset ENABLED_SERVICES ; then\n    # Keystone - nothing works without keystone\n    ENABLED_SERVICES=key\n    # Nova - services to support libvirt based openstack clouds\n    ENABLED_SERVICES+=,n-api,n-cpu,n-cond,n-sch,n-novnc,n-api-meta\n    # Placement service needed for Nova\n    ENABLED_SERVICES+=,placement-api,placement-client\n    # Glance services needed for Nova\n    ENABLED_SERVICES+=,g-api\n    # Cinder\n    ENABLED_SERVICES+=,c-sch,c-api,c-vol\n    # OVN\n    ENABLED_SERVICES+=,ovn-controller,ovn-northd,ovs-vswitchd,ovsdb-server\n    # Neutron\n    ENABLED_SERVICES+=,q-svc,q-ovn-metadata-agent\n    # Dashboard\n    ENABLED_SERVICES+=,horizon\n    # Additional services\n    ENABLED_SERVICES+=,rabbit,tempest,mysql,etcd3,dstat\nfi\n\n# Global toggle for enabling services under mod_wsgi. If this is set to\n# ``True`` all services that use HTTPD + mod_wsgi as the preferred method of\n# deployment, will be deployed under Apache. If this is set to ``False`` all\n# services will rely on the local toggle variable.\nENABLE_HTTPD_MOD_WSGI_SERVICES=True\n\n# Set the default Nova APIs to enable\nNOVA_ENABLED_APIS=osapi_compute,metadata\n\n# allow local overrides of env variables, including repo config\nif [[ -f $RC_DIR/localrc ]]; then\n    # Old-style user-supplied config\n    source $RC_DIR/localrc\nelif [[ -f $RC_DIR/.localrc.auto ]]; then\n    # New-style user-supplied config extracted from local.conf\n    source $RC_DIR/.localrc.auto\nfi\n\n# CELLSV2_SETUP - how we should configure services with cells v2\n#\n# - superconductor - this is one conductor for the api services, and\n#   one per cell managing the compute services. This is preferred\n# - singleconductor - this is one conductor for the whole deployment,\n#   this is not recommended, and will be removed in the future.\nCELLSV2_SETUP=${CELLSV2_SETUP:-\"superconductor\"}\n\n# Set the root URL for Horizon\nHORIZON_APACHE_ROOT=\"/dashboard\"\n\n# Whether to use user specific units for running services or global ones.\nUSER_UNITS=$(trueorfalse False USER_UNITS)\nif [[ \"$USER_UNITS\" == \"True\" ]]; then\n    SYSTEMD_DIR=\"$HOME/.local/share/systemd/user\"\n    SYSTEMCTL=\"systemctl --user\"\nelse\n    SYSTEMD_DIR=\"/etc/systemd/system\"\n    SYSTEMCTL=\"sudo systemctl\"\nfi\n\n# Passwords generated by interactive devstack runs\nif [[ -r $RC_DIR/.localrc.password ]]; then\n    source $RC_DIR/.localrc.password\nfi\n\n# Adding the specific version of Python 3 to this variable will install\n# the app using that version of the interpreter instead of just 3.\n_DEFAULT_PYTHON3_VERSION=\"$(_get_python_version python3)\"\nexport PYTHON3_VERSION=${PYTHON3_VERSION:-${_DEFAULT_PYTHON3_VERSION:-3}}\n\n# Create a virtualenv with this\n# Use the built-in venv to avoid more dependencies\nexport VIRTUALENV_CMD=\"python3 -m venv\"\n\n# Default for log coloring is based on interactive-or-not.\n# Baseline assumption is that non-interactive invocations are for CI,\n# where logs are to be presented as browsable text files; hence color\n# codes should be omitted.\n# Simply override LOG_COLOR if your environment is different.\nif [ -t 1 ]; then\n    _LOG_COLOR_DEFAULT=True\nelse\n    _LOG_COLOR_DEFAULT=False\nfi\n\n# Use color for logging output (only available if syslog is not used)\nLOG_COLOR=$(trueorfalse $_LOG_COLOR_DEFAULT LOG_COLOR)\n\n# Make tracing more educational\nif [[ \"$LOG_COLOR\" == \"True\" ]]; then\n    # tput requires TERM or -T.  If neither is present, use vt100, a\n    # no-frills least common denominator supported everywhere.\n    TPUT_T=\n    if ! [ $TERM ]; then\n        TPUT_T='-T vt100'\n    fi\n    export PS4='+\\[$(tput '$TPUT_T' setaf 242)\\]$(short_source)\\[$(tput '$TPUT_T' sgr0)\\] '\nelse\n    export PS4='+ $(short_source):   '\nfi\n\n# Global option for enforcing scope. If enabled, ENFORCE_SCOPE overrides\n# each services ${SERVICE}_ENFORCE_SCOPE variables\nENFORCE_SCOPE=$(trueorfalse False ENFORCE_SCOPE)\n\n# Devstack supports the use of a global virtualenv. These variables enable\n# and disable this functionality as well as set the path to the virtualenv.\n# Note that the DATA_DIR is selected because grenade testing uses a shared\n# DATA_DIR but different DEST dirs and we don't want two sets of venvs,\n# instead we want one global set.\nDEVSTACK_VENV=${DEVSTACK_VENV:-$DATA_DIR/venv}\n\n# NOTE(kopecmartin): remove this once this is fixed\n# https://bugs.launchpad.net/devstack/+bug/2031639\n# This couldn't go to fixup_stuff as that's called after projects\n# (e.g. certain paths) are set taking GLOBAL_VENV into account\nif [[ \"$os_VENDOR\" =~ (CentOSStream|Rocky) ]]; then\n    GLOBAL_VENV=$(trueorfalse False GLOBAL_VENV)\nelse\n    GLOBAL_VENV=$(trueorfalse True GLOBAL_VENV)\nfi\n\n# Enable use of Python virtual environments.  Individual project use of\n# venvs are controlled by the PROJECT_VENV array; every project with\n# an entry in the array will be installed into the named venv.\n# By default this will put each project into its own venv.\nUSE_VENV=$(trueorfalse False USE_VENV)\n\n# Add packages that need to be installed into a venv but are not in any\n# requirements files here, in a comma-separated list.\n# Currently only used when USE_VENV is true (individual project venvs)\nADDITIONAL_VENV_PACKAGES=${ADDITIONAL_VENV_PACKAGES:-\"\"}\n\n# This can be used to turn database query logging on and off\n# (currently only implemented for MySQL backend)\nDATABASE_QUERY_LOGGING=$(trueorfalse False DATABASE_QUERY_LOGGING)\n\n# This can be used to turn on various non-default items in the\n# performance_schema that are of interest to us\nMYSQL_GATHER_PERFORMANCE=$(trueorfalse True MYSQL_GATHER_PERFORMANCE)\n\n# This can be used to reduce the amount of memory mysqld uses while running.\n# These are unscientifically determined, and could reduce performance or\n# cause other issues.\nMYSQL_REDUCE_MEMORY=$(trueorfalse True MYSQL_REDUCE_MEMORY)\n\n# Set a timeout for git operations.  If git is still running when the\n# timeout expires, the command will be retried up to 3 times.  This is\n# in the format for timeout(1);\n#\n#  DURATION is a floating point number with an optional suffix: 's'\n#  for seconds (the default), 'm' for minutes, 'h' for hours or 'd'\n#  for days.\n#\n# Zero disables timeouts\nGIT_TIMEOUT=${GIT_TIMEOUT:-0}\n\n# How should we be handling WSGI deployments. By default we're going\n# to allow for 2 modes, which is \"uwsgi\" which runs with an apache\n# proxy uwsgi in front of it, or \"mod_wsgi\", which runs in\n# apache. mod_wsgi is deprecated, don't use it.\nWSGI_MODE=${WSGI_MODE:-\"uwsgi\"}\nif [[ \"$WSGI_MODE\" != \"uwsgi\" ]]; then\n    die $LINENO \"$WSGI_MODE is no longer a supported WSGI mode. Only uwsgi is valid.\"\nfi\n\n# Repositories\n# ------------\n\n# Base GIT Repo URL\nGIT_BASE=${GIT_BASE:-https://opendev.org}\n\n# The location of REQUIREMENTS once cloned\nREQUIREMENTS_DIR=${REQUIREMENTS_DIR:-$DEST/requirements}\n\n# Which libraries should we install from git instead of using released\n# versions on pypi?\n#\n# By default DevStack is now installing libraries from pypi instead of\n# from git repositories by default. This works great if you are\n# developing server components, but if you want to develop libraries\n# and see them live in DevStack you need to tell DevStack it should\n# install them from git.\n#\n# ex: LIBS_FROM_GIT=python-keystoneclient,oslo.config\n#\n# Will install those 2 libraries from git, the rest from pypi.\n#\n# Setting the variable to 'ALL' will activate the download for all\n# libraries.\n\nDEVSTACK_SERIES=\"2025.1\"\n\n##############\n#\n#  OpenStack Server Components\n#\n##############\n\n# block storage service\nCINDER_REPO=${CINDER_REPO:-${GIT_BASE}/openstack/cinder.git}\nCINDER_BRANCH=${CINDER_BRANCH:-$TARGET_BRANCH}\n\n# image catalog service\nGLANCE_REPO=${GLANCE_REPO:-${GIT_BASE}/openstack/glance.git}\nGLANCE_BRANCH=${GLANCE_BRANCH:-$TARGET_BRANCH}\n\n# django powered web control panel for openstack\nHORIZON_REPO=${HORIZON_REPO:-${GIT_BASE}/openstack/horizon.git}\nHORIZON_BRANCH=${HORIZON_BRANCH:-$TARGET_BRANCH}\n\n# unified auth system (manages accounts/tokens)\nKEYSTONE_REPO=${KEYSTONE_REPO:-${GIT_BASE}/openstack/keystone.git}\nKEYSTONE_BRANCH=${KEYSTONE_BRANCH:-$TARGET_BRANCH}\n\n# neutron service\nNEUTRON_REPO=${NEUTRON_REPO:-${GIT_BASE}/openstack/neutron.git}\nNEUTRON_BRANCH=${NEUTRON_BRANCH:-$TARGET_BRANCH}\n\n# compute service\nNOVA_REPO=${NOVA_REPO:-${GIT_BASE}/openstack/nova.git}\nNOVA_BRANCH=${NOVA_BRANCH:-$TARGET_BRANCH}\n\n# object storage service\nSWIFT_REPO=${SWIFT_REPO:-${GIT_BASE}/openstack/swift.git}\nSWIFT_BRANCH=${SWIFT_BRANCH:-$TARGET_BRANCH}\n\n# placement service\nPLACEMENT_REPO=${PLACEMENT_REPO:-${GIT_BASE}/openstack/placement.git}\nPLACEMENT_BRANCH=${PLACEMENT_BRANCH:-$TARGET_BRANCH}\n\n##############\n#\n#  Testing Components\n#\n##############\n\n# consolidated openstack requirements\nREQUIREMENTS_REPO=${REQUIREMENTS_REPO:-${GIT_BASE}/openstack/requirements.git}\nREQUIREMENTS_BRANCH=${REQUIREMENTS_BRANCH:-$TARGET_BRANCH}\n\n# Tempest test suite\nTEMPEST_REPO=${TEMPEST_REPO:-${GIT_BASE}/openstack/tempest.git}\nTEMPEST_BRANCH=${TEMPEST_BRANCH:-$BRANCHLESS_TARGET_BRANCH}\nTEMPEST_VENV_UPPER_CONSTRAINTS=${TEMPEST_VENV_UPPER_CONSTRAINTS:-master}\n\nOSTESTIMAGES_REPO=${OSTESTIMAGES_REPO:-${GIT_BASE}/openstack/os-test-images.git}\nOSTESTIMAGES_BRANCH=${OSTESTIMAGES_BRANCH:-$BRANCHLESS_TARGET_BRANCH}\nOSTESTIMAGES_DIR=${DEST}/os-test-images\n\n##############\n#\n#  OpenStack Client Library Components\n#   Note default install is from pip, see LIBS_FROM_GIT\n#\n##############\n\n# volume client\nGITREPO[\"python-cinderclient\"]=${CINDERCLIENT_REPO:-${GIT_BASE}/openstack/python-cinderclient.git}\nGITBRANCH[\"python-cinderclient\"]=${CINDERCLIENT_BRANCH:-$TARGET_BRANCH}\n\n# os-brick client for local volume attachement\nGITREPO[\"python-brick-cinderclient-ext\"]=${BRICK_CINDERCLIENT_REPO:-${GIT_BASE}/openstack/python-brick-cinderclient-ext.git}\nGITBRANCH[\"python-brick-cinderclient-ext\"]=${BRICK_CINDERCLIENT_BRANCH:-$TARGET_BRANCH}\n\n# python barbican client library\nGITREPO[\"python-barbicanclient\"]=${BARBICANCLIENT_REPO:-${GIT_BASE}/openstack/python-barbicanclient.git}\nGITBRANCH[\"python-barbicanclient\"]=${BARBICANCLIENT_BRANCH:-$TARGET_BRANCH}\nGITDIR[\"python-barbicanclient\"]=$DEST/python-barbicanclient\n\n# python glance client library\nGITREPO[\"python-glanceclient\"]=${GLANCECLIENT_REPO:-${GIT_BASE}/openstack/python-glanceclient.git}\nGITBRANCH[\"python-glanceclient\"]=${GLANCECLIENT_BRANCH:-$TARGET_BRANCH}\n\n# ironic client\nGITREPO[\"python-ironicclient\"]=${IRONICCLIENT_REPO:-${GIT_BASE}/openstack/python-ironicclient.git}\nGITBRANCH[\"python-ironicclient\"]=${IRONICCLIENT_BRANCH:-$TARGET_BRANCH}\n# ironic plugin is out of tree, but nova uses it. set GITDIR here.\nGITDIR[\"python-ironicclient\"]=$DEST/python-ironicclient\n\n# the base authentication plugins that clients use to authenticate\nGITREPO[\"keystoneauth\"]=${KEYSTONEAUTH_REPO:-${GIT_BASE}/openstack/keystoneauth.git}\nGITBRANCH[\"keystoneauth\"]=${KEYSTONEAUTH_BRANCH:-$TARGET_BRANCH}\n\n# python keystone client library to nova that horizon uses\nGITREPO[\"python-keystoneclient\"]=${KEYSTONECLIENT_REPO:-${GIT_BASE}/openstack/python-keystoneclient.git}\nGITBRANCH[\"python-keystoneclient\"]=${KEYSTONECLIENT_BRANCH:-$TARGET_BRANCH}\n\n# neutron client\nGITREPO[\"python-neutronclient\"]=${NEUTRONCLIENT_REPO:-${GIT_BASE}/openstack/python-neutronclient.git}\nGITBRANCH[\"python-neutronclient\"]=${NEUTRONCLIENT_BRANCH:-$TARGET_BRANCH}\n\n# python client library to nova that horizon (and others) use\nGITREPO[\"python-novaclient\"]=${NOVACLIENT_REPO:-${GIT_BASE}/openstack/python-novaclient.git}\nGITBRANCH[\"python-novaclient\"]=${NOVACLIENT_BRANCH:-$TARGET_BRANCH}\n\n# python swift client library\nGITREPO[\"python-swiftclient\"]=${SWIFTCLIENT_REPO:-${GIT_BASE}/openstack/python-swiftclient.git}\nGITBRANCH[\"python-swiftclient\"]=${SWIFTCLIENT_BRANCH:-$TARGET_BRANCH}\n\n# consolidated openstack python client\nGITREPO[\"python-openstackclient\"]=${OPENSTACKCLIENT_REPO:-${GIT_BASE}/openstack/python-openstackclient.git}\nGITBRANCH[\"python-openstackclient\"]=${OPENSTACKCLIENT_BRANCH:-$TARGET_BRANCH}\n# this doesn't exist in a lib file, so set it here\nGITDIR[\"python-openstackclient\"]=$DEST/python-openstackclient\n\n# placement-api CLI\nGITREPO[\"osc-placement\"]=${OSC_PLACEMENT_REPO:-${GIT_BASE}/openstack/osc-placement.git}\nGITBRANCH[\"osc-placement\"]=${OSC_PLACEMENT_BRANCH:-$TARGET_BRANCH}\n\n\n###################\n#\n#  Oslo Libraries\n#   Note default install is from pip, see LIBS_FROM_GIT\n#\n###################\n\n# castellan key manager interface\nGITREPO[\"castellan\"]=${CASTELLAN_REPO:-${GIT_BASE}/openstack/castellan.git}\nGITBRANCH[\"castellan\"]=${CASTELLAN_BRANCH:-$TARGET_BRANCH}\n\n# cliff command line framework\nGITREPO[\"cliff\"]=${CLIFF_REPO:-${GIT_BASE}/openstack/cliff.git}\nGITBRANCH[\"cliff\"]=${CLIFF_BRANCH:-$TARGET_BRANCH}\n\n# async framework/helpers\nGITREPO[\"futurist\"]=${FUTURIST_REPO:-${GIT_BASE}/openstack/futurist.git}\nGITBRANCH[\"futurist\"]=${FUTURIST_BRANCH:-$TARGET_BRANCH}\n\n# debtcollector deprecation framework/helpers\nGITREPO[\"debtcollector\"]=${DEBTCOLLECTOR_REPO:-${GIT_BASE}/openstack/debtcollector.git}\nGITBRANCH[\"debtcollector\"]=${DEBTCOLLECTOR_BRANCH:-$TARGET_BRANCH}\n\n# helpful state machines\nGITREPO[\"automaton\"]=${AUTOMATON_REPO:-${GIT_BASE}/openstack/automaton.git}\nGITBRANCH[\"automaton\"]=${AUTOMATON_BRANCH:-$TARGET_BRANCH}\n\n# oslo.cache\nGITREPO[\"oslo.cache\"]=${OSLOCACHE_REPO:-${GIT_BASE}/openstack/oslo.cache.git}\nGITBRANCH[\"oslo.cache\"]=${OSLOCACHE_BRANCH:-$TARGET_BRANCH}\n\n# oslo.concurrency\nGITREPO[\"oslo.concurrency\"]=${OSLOCON_REPO:-${GIT_BASE}/openstack/oslo.concurrency.git}\nGITBRANCH[\"oslo.concurrency\"]=${OSLOCON_BRANCH:-$TARGET_BRANCH}\n\n# oslo.config\nGITREPO[\"oslo.config\"]=${OSLOCFG_REPO:-${GIT_BASE}/openstack/oslo.config.git}\nGITBRANCH[\"oslo.config\"]=${OSLOCFG_BRANCH:-$TARGET_BRANCH}\n\n# oslo.context\nGITREPO[\"oslo.context\"]=${OSLOCTX_REPO:-${GIT_BASE}/openstack/oslo.context.git}\nGITBRANCH[\"oslo.context\"]=${OSLOCTX_BRANCH:-$TARGET_BRANCH}\n\n# oslo.db\nGITREPO[\"oslo.db\"]=${OSLODB_REPO:-${GIT_BASE}/openstack/oslo.db.git}\nGITBRANCH[\"oslo.db\"]=${OSLODB_BRANCH:-$TARGET_BRANCH}\n\n# oslo.i18n\nGITREPO[\"oslo.i18n\"]=${OSLOI18N_REPO:-${GIT_BASE}/openstack/oslo.i18n.git}\nGITBRANCH[\"oslo.i18n\"]=${OSLOI18N_BRANCH:-$TARGET_BRANCH}\n\n# oslo.limit\nGITREPO[\"oslo.limit\"]=${OSLOLIMIT_REPO:-${GIT_BASE}/openstack/oslo.limit.git}\nGITBRANCH[\"oslo.limit\"]=${OSLOLIMIT_BRANCH:-$TARGET_BRANCH}\n\n# oslo.log\nGITREPO[\"oslo.log\"]=${OSLOLOG_REPO:-${GIT_BASE}/openstack/oslo.log.git}\nGITBRANCH[\"oslo.log\"]=${OSLOLOG_BRANCH:-$TARGET_BRANCH}\n\n# oslo.messaging\nGITREPO[\"oslo.messaging\"]=${OSLOMSG_REPO:-${GIT_BASE}/openstack/oslo.messaging.git}\nGITBRANCH[\"oslo.messaging\"]=${OSLOMSG_BRANCH:-$TARGET_BRANCH}\n\n# oslo.middleware\nGITREPO[\"oslo.middleware\"]=${OSLOMID_REPO:-${GIT_BASE}/openstack/oslo.middleware.git}\nGITBRANCH[\"oslo.middleware\"]=${OSLOMID_BRANCH:-$TARGET_BRANCH}\n\n# oslo.policy\nGITREPO[\"oslo.policy\"]=${OSLOPOLICY_REPO:-${GIT_BASE}/openstack/oslo.policy.git}\nGITBRANCH[\"oslo.policy\"]=${OSLOPOLICY_BRANCH:-$TARGET_BRANCH}\n\n# oslo.privsep\nGITREPO[\"oslo.privsep\"]=${OSLOPRIVSEP_REPO:-${GIT_BASE}/openstack/oslo.privsep.git}\nGITBRANCH[\"oslo.privsep\"]=${OSLOPRIVSEP_BRANCH:-$TARGET_BRANCH}\n\n# oslo.reports\nGITREPO[\"oslo.reports\"]=${OSLOREPORTS_REPO:-${GIT_BASE}/openstack/oslo.reports.git}\nGITBRANCH[\"oslo.reports\"]=${OSLOREPORTS_BRANCH:-$TARGET_BRANCH}\n\n# oslo.rootwrap\nGITREPO[\"oslo.rootwrap\"]=${OSLORWRAP_REPO:-${GIT_BASE}/openstack/oslo.rootwrap.git}\nGITBRANCH[\"oslo.rootwrap\"]=${OSLORWRAP_BRANCH:-$TARGET_BRANCH}\n\n# oslo.serialization\nGITREPO[\"oslo.serialization\"]=${OSLOSERIALIZATION_REPO:-${GIT_BASE}/openstack/oslo.serialization.git}\nGITBRANCH[\"oslo.serialization\"]=${OSLOSERIALIZATION_BRANCH:-$TARGET_BRANCH}\n\n# oslo.service\nGITREPO[\"oslo.service\"]=${OSLOSERVICE_REPO:-${GIT_BASE}/openstack/oslo.service.git}\nGITBRANCH[\"oslo.service\"]=${OSLOSERVICE_BRANCH:-$TARGET_BRANCH}\n\n# oslo.utils\nGITREPO[\"oslo.utils\"]=${OSLOUTILS_REPO:-${GIT_BASE}/openstack/oslo.utils.git}\nGITBRANCH[\"oslo.utils\"]=${OSLOUTILS_BRANCH:-$TARGET_BRANCH}\n\n# oslo.versionedobjects\nGITREPO[\"oslo.versionedobjects\"]=${OSLOVERSIONEDOBJECTS_REPO:-${GIT_BASE}/openstack/oslo.versionedobjects.git}\nGITBRANCH[\"oslo.versionedobjects\"]=${OSLOVERSIONEDOBJECTS_BRANCH:-$TARGET_BRANCH}\n\n# oslo.vmware\nGITREPO[\"oslo.vmware\"]=${OSLOVMWARE_REPO:-${GIT_BASE}/openstack/oslo.vmware.git}\nGITBRANCH[\"oslo.vmware\"]=${OSLOVMWARE_BRANCH:-$TARGET_BRANCH}\n\n# osprofiler\nGITREPO[\"osprofiler\"]=${OSPROFILER_REPO:-${GIT_BASE}/openstack/osprofiler.git}\nGITBRANCH[\"osprofiler\"]=${OSPROFILER_BRANCH:-$TARGET_BRANCH}\n\n# pycadf auditing library\nGITREPO[\"pycadf\"]=${PYCADF_REPO:-${GIT_BASE}/openstack/pycadf.git}\nGITBRANCH[\"pycadf\"]=${PYCADF_BRANCH:-$TARGET_BRANCH}\n\n# stevedore plugin manager\nGITREPO[\"stevedore\"]=${STEVEDORE_REPO:-${GIT_BASE}/openstack/stevedore.git}\nGITBRANCH[\"stevedore\"]=${STEVEDORE_BRANCH:-$TARGET_BRANCH}\n\n# taskflow plugin manager\nGITREPO[\"taskflow\"]=${TASKFLOW_REPO:-${GIT_BASE}/openstack/taskflow.git}\nGITBRANCH[\"taskflow\"]=${TASKFLOW_BRANCH:-$TARGET_BRANCH}\n\n# tooz plugin manager\nGITREPO[\"tooz\"]=${TOOZ_REPO:-${GIT_BASE}/openstack/tooz.git}\nGITBRANCH[\"tooz\"]=${TOOZ_BRANCH:-$TARGET_BRANCH}\n\n# pbr drives the setuptools configs\nGITREPO[\"pbr\"]=${PBR_REPO:-${GIT_BASE}/openstack/pbr.git}\nGITBRANCH[\"pbr\"]=${PBR_BRANCH:-$BRANCHLESS_TARGET_BRANCH}\n\n\n##################\n#\n#  Libraries managed by OpenStack programs (non oslo)\n#\n##################\n\n# cursive library\nGITREPO[\"cursive\"]=${CURSIVE_REPO:-${GIT_BASE}/openstack/cursive.git}\nGITBRANCH[\"cursive\"]=${CURSIVE_BRANCH:-$TARGET_BRANCH}\n\n# glance store library\nGITREPO[\"glance_store\"]=${GLANCE_STORE_REPO:-${GIT_BASE}/openstack/glance_store.git}\nGITBRANCH[\"glance_store\"]=${GLANCE_STORE_BRANCH:-$TARGET_BRANCH}\n\n# keystone middleware\nGITREPO[\"keystonemiddleware\"]=${KEYSTONEMIDDLEWARE_REPO:-${GIT_BASE}/openstack/keystonemiddleware.git}\nGITBRANCH[\"keystonemiddleware\"]=${KEYSTONEMIDDLEWARE_BRANCH:-$TARGET_BRANCH}\n\n# ceilometer middleware\nGITREPO[\"ceilometermiddleware\"]=${CEILOMETERMIDDLEWARE_REPO:-${GIT_BASE}/openstack/ceilometermiddleware.git}\nGITBRANCH[\"ceilometermiddleware\"]=${CEILOMETERMIDDLEWARE_BRANCH:-$TARGET_BRANCH}\nGITDIR[\"ceilometermiddleware\"]=$DEST/ceilometermiddleware\n\n# openstacksdk OpenStack Python SDK\nGITREPO[\"openstacksdk\"]=${OPENSTACKSDK_REPO:-${GIT_BASE}/openstack/openstacksdk.git}\nGITBRANCH[\"openstacksdk\"]=${OPENSTACKSDK_BRANCH:-$TARGET_BRANCH}\n\n# os-brick library to manage local volume attaches\nGITREPO[\"os-brick\"]=${OS_BRICK_REPO:-${GIT_BASE}/openstack/os-brick.git}\nGITBRANCH[\"os-brick\"]=${OS_BRICK_BRANCH:-$TARGET_BRANCH}\n\n# os-client-config to manage clouds.yaml and friends\nGITREPO[\"os-client-config\"]=${OS_CLIENT_CONFIG_REPO:-${GIT_BASE}/openstack/os-client-config.git}\nGITBRANCH[\"os-client-config\"]=${OS_CLIENT_CONFIG_BRANCH:-$TARGET_BRANCH}\nGITDIR[\"os-client-config\"]=$DEST/os-client-config\n\n# os-vif library to communicate between Neutron to Nova\nGITREPO[\"os-vif\"]=${OS_VIF_REPO:-${GIT_BASE}/openstack/os-vif.git}\nGITBRANCH[\"os-vif\"]=${OS_VIF_BRANCH:-$TARGET_BRANCH}\n\n# osc-lib OpenStackClient common lib\nGITREPO[\"osc-lib\"]=${OSC_LIB_REPO:-${GIT_BASE}/openstack/osc-lib.git}\nGITBRANCH[\"osc-lib\"]=${OSC_LIB_BRANCH:-$TARGET_BRANCH}\n\n# ironic common lib\nGITREPO[\"ironic-lib\"]=${IRONIC_LIB_REPO:-${GIT_BASE}/openstack/ironic-lib.git}\nGITBRANCH[\"ironic-lib\"]=${IRONIC_LIB_BRANCH:-$TARGET_BRANCH}\n# this doesn't exist in a lib file, so set it here\nGITDIR[\"ironic-lib\"]=$DEST/ironic-lib\n\n# diskimage-builder tool\nGITREPO[\"diskimage-builder\"]=${DIB_REPO:-${GIT_BASE}/openstack/diskimage-builder.git}\nGITBRANCH[\"diskimage-builder\"]=${DIB_BRANCH:-$BRANCHLESS_TARGET_BRANCH}\nGITDIR[\"diskimage-builder\"]=$DEST/diskimage-builder\n\n# neutron-lib library containing neutron stable non-REST interfaces\nGITREPO[\"neutron-lib\"]=${NEUTRON_LIB_REPO:-${GIT_BASE}/openstack/neutron-lib.git}\nGITBRANCH[\"neutron-lib\"]=${NEUTRON_LIB_BRANCH:-$TARGET_BRANCH}\nGITDIR[\"neutron-lib\"]=$DEST/neutron-lib\n\n# os-resource-classes library containing a list of standardized resource classes for OpenStack\nGITREPO[\"os-resource-classes\"]=${OS_RESOURCE_CLASSES_REPO:-${GIT_BASE}/openstack/os-resource-classes.git}\nGITBRANCH[\"os-resource-classes\"]=${OS_RESOURCE_CLASSES_BRANCH:-$TARGET_BRANCH}\n\n# os-traits library for resource provider traits in the placement service\nGITREPO[\"os-traits\"]=${OS_TRAITS_REPO:-${GIT_BASE}/openstack/os-traits.git}\nGITBRANCH[\"os-traits\"]=${OS_TRAITS_BRANCH:-$TARGET_BRANCH}\n\n# ovsdbapp used by neutron\nGITREPO[\"ovsdbapp\"]=${OVSDBAPP_REPO:-${GIT_BASE}/openstack/ovsdbapp.git}\nGITBRANCH[\"ovsdbapp\"]=${OVSDBAPP_BRANCH:-$TARGET_BRANCH}\nGITDIR[\"ovsdbapp\"]=$DEST/ovsdbapp\n\n# os-ken used by neutron\nGITREPO[\"os-ken\"]=${OS_KEN_REPO:-${GIT_BASE}/openstack/os-ken.git}\nGITBRANCH[\"os-ken\"]=${OS_KEN_BRANCH:-$TARGET_BRANCH}\nGITDIR[\"os-ken\"]=$DEST/os-ken\n\n\n#################\n#\n#  3rd Party Components (non pip installable)\n#\n#  NOTE(sdague): these should be converted to release version installs or removed\n#\n#################\n\n# ironic python agent\nIRONIC_PYTHON_AGENT_REPO=${IRONIC_PYTHON_AGENT_REPO:-${GIT_BASE}/openstack/ironic-python-agent.git}\nIRONIC_PYTHON_AGENT_BRANCH=${IRONIC_PYTHON_AGENT_BRANCH:-$TARGET_BRANCH}\n\n# a websockets/html5 or flash powered VNC console for vm instances\nNOVNC_REPO=${NOVNC_REPO:-https://github.com/novnc/novnc.git}\nNOVNC_BRANCH=${NOVNC_BRANCH:-v1.3.0}\n\n# a websockets/html5 or flash powered SPICE console for vm instances\nSPICE_REPO=${SPICE_REPO:-http://anongit.freedesktop.org/git/spice/spice-html5.git}\nSPICE_BRANCH=${SPICE_BRANCH:-$BRANCHLESS_TARGET_BRANCH}\n\n# Global flag used to configure Tempest and potentially other services if\n# volume multiattach is supported. In Queens, only the libvirt compute driver\n# and lvm volume driver support multiattach, and qemu must be less than 2.10\n# or libvirt must be greater than or equal to 3.10.\nENABLE_VOLUME_MULTIATTACH=$(trueorfalse False ENABLE_VOLUME_MULTIATTACH)\n\n# Nova hypervisor configuration.  We default to libvirt with **kvm** but will\n# drop back to **qemu** if we are unable to load the kvm module.  ``stack.sh`` can\n# also install an **LXC** or **OpenVZ** based system.\nDEFAULT_VIRT_DRIVER=libvirt\nVIRT_DRIVER=${VIRT_DRIVER:-$DEFAULT_VIRT_DRIVER}\ncase \"$VIRT_DRIVER\" in\n    ironic|libvirt)\n        LIBVIRT_TYPE=${LIBVIRT_TYPE:-kvm}\n        LIBVIRT_CPU_MODE=${LIBVIRT_CPU_MODE:-custom}\n        LIBVIRT_CPU_MODEL=${LIBVIRT_CPU_MODEL:-Nehalem}\n        if [[ \"$os_VENDOR\" =~ (Debian|Ubuntu) ]]; then\n            # The groups change with newer libvirt. Older Ubuntu used\n            # 'libvirtd', but now uses libvirt like Debian. Do a quick check\n            # to see if libvirtd group already exists to handle grenade's case.\n            LIBVIRT_GROUP=$(cut -d ':' -f 1 /etc/group | grep 'libvirtd$' || true)\n            LIBVIRT_GROUP=${LIBVIRT_GROUP:-libvirt}\n        else\n            LIBVIRT_GROUP=libvirtd\n        fi\n        ;;\n    lxd)\n        LXD_GROUP=${LXD_GROUP:-\"lxd\"}\n        ;;\n    docker|zun)\n        DOCKER_GROUP=${DOCKER_GROUP:-\"docker\"}\n        ;;\n    fake)\n        NUMBER_FAKE_NOVA_COMPUTE=${NUMBER_FAKE_NOVA_COMPUTE:-1}\n        ;;\n    *)\n        ;;\nesac\n\n# Images\n# ------\n\n# Specify a comma-separated list of images to download and install into glance.\n# Supported urls here are:\n#  * \"uec-style\" images:\n#     If the file ends in .tar.gz, uncompress the tarball and and select the first\n#     .img file inside it as the image.  If present, use \"*-vmlinuz*\" as the kernel\n#     and \"*-initrd*\" as the ramdisk\n#     example: https://cloud-images.ubuntu.com/releases/jammy/release/ubuntu-22.04-server-cloudimg-amd64.tar.gz\n#  * disk image (*.img,*.img.gz)\n#    if file ends in .img, then it will be uploaded and registered as a to\n#    glance as a disk image.  If it ends in .gz, it is uncompressed first.\n#    example:\n#      https://cloud-images.ubuntu.com/releases/jammy/release/ubuntu-22.04-server-cloudimg-amd64.img\n#      https://download.cirros-cloud.net/${CIRROS_VERSION}/cirros-${CIRROS_VERSION}-${CIRROS_ARCH}-rootfs.img.gz\n#  * OpenVZ image:\n#    OpenVZ uses its own format of image, and does not support UEC style images\n\n#IMAGE_URLS=\"https://download.cirros-cloud.net/${CIRROS_VERSION}/cirros-${CIRROS_VERSION}-${CIRROS_ARCH}-disk.img\" # cirros full disk image\n\nCIRROS_VERSION=${CIRROS_VERSION:-\"0.6.3\"}\nCIRROS_ARCH=${CIRROS_ARCH:-$(uname -m)}\n\n# Set default image based on ``VIRT_DRIVER`` and ``LIBVIRT_TYPE``, either of\n# which may be set in ``local.conf``.  Also allow ``DEFAULT_IMAGE_NAME`` and\n# ``IMAGE_URLS`` to be set in the `localrc` section of ``local.conf``.\nDOWNLOAD_DEFAULT_IMAGES=$(trueorfalse True DOWNLOAD_DEFAULT_IMAGES)\nif [[ \"$DOWNLOAD_DEFAULT_IMAGES\" == \"True\" ]]; then\n    if [[ -n \"$IMAGE_URLS\" ]]; then\n        IMAGE_URLS+=\",\"\n    fi\n    case \"$VIRT_DRIVER\" in\n        libvirt)\n            case \"$LIBVIRT_TYPE\" in\n                lxc) # the cirros root disk in the uec tarball is empty, so it will not work for lxc\n                    DEFAULT_IMAGE_NAME=${DEFAULT_IMAGE_NAME:-cirros-${CIRROS_VERSION}-${CIRROS_ARCH}-rootfs}\n                    DEFAULT_IMAGE_FILE_NAME=${DEFAULT_IMAGE_FILE_NAME:-cirros-${CIRROS_VERSION}-${CIRROS_ARCH}-rootfs.img.gz}\n                    IMAGE_URLS+=\"https://github.com/cirros-dev/cirros/releases/download/${CIRROS_VERSION}/${DEFAULT_IMAGE_FILE_NAME}\";;\n                *) # otherwise, use the qcow image\n                    DEFAULT_IMAGE_NAME=${DEFAULT_IMAGE_NAME:-cirros-${CIRROS_VERSION}-${CIRROS_ARCH}-disk}\n                    DEFAULT_IMAGE_FILE_NAME=${DEFAULT_IMAGE_FILE_NAME:-cirros-${CIRROS_VERSION}-${CIRROS_ARCH}-disk.img}\n                    IMAGE_URLS+=\"https://github.com/cirros-dev/cirros/releases/download/${CIRROS_VERSION}/${DEFAULT_IMAGE_FILE_NAME}\";;\n                esac\n            ;;\n        vsphere)\n            DEFAULT_IMAGE_NAME=${DEFAULT_IMAGE_NAME:-cirros-0.3.2-i386-disk.vmdk}\n            DEFAULT_IMAGE_FILE_NAME=${DEFAULT_IMAGE_FILE_NAME:-$DEFAULT_IMAGE_NAME}\n            IMAGE_URLS+=\"http://partnerweb.vmware.com/programs/vmdkimage/${DEFAULT_IMAGE_FILE_NAME}\";;\n        fake)\n            # Use the same as the default for libvirt\n            DEFAULT_IMAGE_NAME=${DEFAULT_IMAGE_NAME:-cirros-${CIRROS_VERSION}-${CIRROS_ARCH}-disk}\n            DEFAULT_IMAGE_FILE_NAME=${DEFAULT_IMAGE_FILE_NAME:-cirros-${CIRROS_VERSION}-${CIRROS_ARCH}-disk.img}\n            IMAGE_URLS+=\"https://github.com/cirros-dev/cirros/releases/download/${CIRROS_VERSION}/${DEFAULT_IMAGE_FILE_NAME}\";;\n    esac\n    DOWNLOAD_DEFAULT_IMAGES=False\nfi\n\n# This is a comma separated list of extra URLS to be listed for\n# download by the tools/image_list.sh script.  CI environments can\n# pre-download these URLS and place them in $FILES.  Later scripts can\n# then use \"get_extra_file <url>\" which will print out the path to the\n# file; it will either be downloaded on demand or acquired from the\n# cache if there.\nEXTRA_CACHE_URLS=\"\"\n\n# etcd3 defaults\nETCD_VERSION=${ETCD_VERSION:-v3.4.27}\nETCD_SHA256_AMD64=${ETCD_SHA256_AMD64:-\"a32d21e006252dbc3405b0645ba8468021ed41376974b573285927bf39b39eb9\"}\nETCD_SHA256_ARM64=${ETCD_SHA256_ARM64:-\"ed7e257c225b9b9545fac22246b97f4074a4b5109676e92dbaebfb9315b69cc0\"}\nETCD_SHA256_PPC64=${ETCD_SHA256_PPC64:-\"eb8825e0bc2cbaf9e55947f5ee373ebc9ca43b6a2ea5ced3b992c81855fff37e\"}\n# etcd v3.2.x and later doesn't have anything for s390x\nETCD_SHA256_S390X=${ETCD_SHA256_S390X:-\"\"}\n# Make sure etcd3 downloads the correct architecture\nif is_arch \"x86_64\"; then\n    ETCD_ARCH=\"amd64\"\n    ETCD_SHA256=${ETCD_SHA256:-$ETCD_SHA256_AMD64}\nelif is_arch \"aarch64\"; then\n    ETCD_ARCH=\"arm64\"\n    ETCD_SHA256=${ETCD_SHA256:-$ETCD_SHA256_ARM64}\nelif is_arch \"ppc64le\"; then\n    ETCD_ARCH=\"ppc64le\"\n    ETCD_SHA256=${ETCD_SHA256:-$ETCD_SHA256_PPC64}\nelif is_arch \"s390x\"; then\n    # An etcd3 binary for s390x is not available on github like it is\n    # for other arches. Only continue if a custom download URL was\n    # provided.\n    if [[ -n \"${ETCD_DOWNLOAD_URL}\" ]]; then\n        ETCD_ARCH=\"s390x\"\n        ETCD_SHA256=${ETCD_SHA256:-$ETCD_SHA256_S390X}\n    else\n        exit_distro_not_supported \"etcd3. No custom ETCD_DOWNLOAD_URL provided.\"\n    fi\nelse\n    exit_distro_not_supported \"invalid hardware type - $ETCD_ARCH\"\nfi\nETCD_PORT=${ETCD_PORT:-2379}\nETCD_PEER_PORT=${ETCD_PEER_PORT:-2380}\nETCD_DOWNLOAD_URL=${ETCD_DOWNLOAD_URL:-https://github.com/etcd-io/etcd/releases/download}\nETCD_NAME=etcd-$ETCD_VERSION-linux-$ETCD_ARCH\nETCD_DOWNLOAD_FILE=$ETCD_NAME.tar.gz\nETCD_DOWNLOAD_LOCATION=$ETCD_DOWNLOAD_URL/$ETCD_VERSION/$ETCD_DOWNLOAD_FILE\n# etcd is always required, so place it into list of pre-cached downloads\nEXTRA_CACHE_URLS+=\",$ETCD_DOWNLOAD_LOCATION\"\n\n# Cache settings\nCACHE_BACKEND=${CACHE_BACKEND:-\"dogpile.cache.memcached\"}\nMEMCACHE_SERVERS=${MEMCACHE_SERVERS:-\"localhost:11211\"}\n\n# Detect duplicate values in IMAGE_URLS\nfor image_url in ${IMAGE_URLS//,/ }; do\n    if [ $(echo \"$IMAGE_URLS\" | grep -o -F \"$image_url\" | wc -l) -gt 1 ]; then\n        die $LINENO \"$image_url is duplicate, please remove it from IMAGE_URLS.\"\n    fi\ndone\n\n# 30Gb default volume backing file size\nVOLUME_BACKING_FILE_SIZE=${VOLUME_BACKING_FILE_SIZE:-30G}\n\n# Prefixes for volume and instance names\nVOLUME_NAME_PREFIX=${VOLUME_NAME_PREFIX:-volume-}\nINSTANCE_NAME_PREFIX=${INSTANCE_NAME_PREFIX:-instance-}\n\n# Set default port for nova-objectstore\nS3_SERVICE_PORT=${S3_SERVICE_PORT:-3333}\n\n# Common network names\nPRIVATE_NETWORK_NAME=${PRIVATE_NETWORK_NAME:-\"private\"}\nPUBLIC_NETWORK_NAME=${PUBLIC_NETWORK_NAME:-\"public\"}\n\nPUBLIC_INTERFACE=${PUBLIC_INTERFACE:-\"\"}\n\n# Allow the use of an alternate protocol (such as https) for service endpoints\nSERVICE_PROTOCOL=${SERVICE_PROTOCOL:-http}\n\n# Sets the maximum number of workers for most services to reduce\n# the memory used where there are a large number of CPUs present\n# (the default number of workers for many services is the number of CPUs)\n# Also sets the minimum number of workers to 2.\nAPI_WORKERS=${API_WORKERS:=$(( ($(nproc)/4)<2 ? 2 : ($(nproc)/4) ))}\n\n# Service startup timeout\nSERVICE_TIMEOUT=${SERVICE_TIMEOUT:-60}\n\n# Timeout for compute node registration in Nova\nNOVA_READY_TIMEOUT=${NOVA_READY_TIMEOUT:-$SERVICE_TIMEOUT}\n\n# Service graceful shutdown timeout\nSERVICE_GRACEFUL_SHUTDOWN_TIMEOUT=${SERVICE_GRACEFUL_SHUTDOWN_TIMEOUT:-5}\n\n# Service graceful shutdown timeout\nWORKER_TIMEOUT=${WORKER_TIMEOUT:-80}\n\n# Common Configuration\n# --------------------\n\n# Set ``OFFLINE`` to ``True`` to configure ``stack.sh`` to run cleanly without\n# Internet access. ``stack.sh`` must have been previously run with Internet\n# access to install prerequisites and fetch repositories.\nOFFLINE=$(trueorfalse False OFFLINE)\n\n# Set ``ERROR_ON_CLONE`` to ``True`` to configure ``stack.sh`` to exit if\n# the destination git repository does not exist during the ``git_clone``\n# operation.\nERROR_ON_CLONE=$(trueorfalse False ERROR_ON_CLONE)\n\n# Whether to enable the debug log level in OpenStack services\nENABLE_DEBUG_LOG_LEVEL=$(trueorfalse True ENABLE_DEBUG_LOG_LEVEL)\n\n# Set fixed and floating range here so we can make sure not to use addresses\n# from either range when attempting to guess the IP to use for the host.\n# Note that setting ``FIXED_RANGE`` may be necessary when running DevStack\n# in an OpenStack cloud that uses either of these address ranges internally.\nFLOATING_RANGE=${FLOATING_RANGE:-172.24.4.0/24}\nIPV4_ADDRS_SAFE_TO_USE=${IPV4_ADDRS_SAFE_TO_USE:-10.0.0.0/22}\nFIXED_RANGE=${FIXED_RANGE:-$IPV4_ADDRS_SAFE_TO_USE}\nHOST_IP_IFACE=${HOST_IP_IFACE:-}\nHOST_IP=${HOST_IP:-}\nHOST_IPV6=${HOST_IPV6:-}\n\nHOST_IP=$(get_default_host_ip \"$FIXED_RANGE\" \"$FLOATING_RANGE\" \"$HOST_IP_IFACE\" \"$HOST_IP\" \"inet\")\nif [ \"$HOST_IP\" == \"\" ]; then\n    die $LINENO \"Could not determine host ip address.  See local.conf for suggestions on setting HOST_IP.\"\nfi\n\nHOST_IPV6=$(get_default_host_ip \"\" \"\" \"$HOST_IP_IFACE\" \"$HOST_IPV6\" \"inet6\")\n\n# Whether or not the port_security extension should be enabled for Neutron.\nNEUTRON_PORT_SECURITY=$(trueorfalse True NEUTRON_PORT_SECURITY)\n\n# SERVICE IP version\n# This is the IP version that services should be listening on, as well\n# as using to register their endpoints with keystone.\nSERVICE_IP_VERSION=${SERVICE_IP_VERSION:-4}\n\n# Validate SERVICE_IP_VERSION\n# It would be nice to support \"4+6\" here as well, but that will require\n# multiple calls into keystone to register endpoints, so for now let's\n# just support one or the other.\nif [[ $SERVICE_IP_VERSION != \"4\" ]] && [[ $SERVICE_IP_VERSION != \"6\" ]]; then\n    die $LINENO \"SERVICE_IP_VERSION must be either 4 or 6\"\nfi\n\nif [[ \"$SERVICE_IP_VERSION\" == 4 ]]; then\n    DEF_SERVICE_HOST=$HOST_IP\n    DEF_SERVICE_LOCAL_HOST=127.0.0.1\n    DEF_SERVICE_LISTEN_ADDRESS=0.0.0.0\nfi\n\nif [[ \"$SERVICE_IP_VERSION\" == 6 ]]; then\n    if [ \"$HOST_IPV6\" == \"\" ]; then\n        die $LINENO \"Could not determine host IPv6 address.  See local.conf for suggestions on setting HOST_IPV6.\"\n    fi\n\n    DEF_SERVICE_HOST=[$HOST_IPV6]\n    DEF_SERVICE_LOCAL_HOST=::1\n    DEF_SERVICE_LISTEN_ADDRESS=\"[::]\"\nfi\n\n# This is either 0.0.0.0 for IPv4 or [::] for IPv6\nSERVICE_LISTEN_ADDRESS=${SERVICE_LISTEN_ADDRESS:-${DEF_SERVICE_LISTEN_ADDRESS}}\n\n# Allow the use of an alternate hostname (such as localhost/127.0.0.1) for\n# service endpoints.  Default is dependent on SERVICE_IP_VERSION above.\nSERVICE_HOST=${SERVICE_HOST:-${DEF_SERVICE_HOST}}\n# This is either 127.0.0.1 for IPv4 or ::1 for IPv6\nSERVICE_LOCAL_HOST=${SERVICE_LOCAL_HOST:-${DEF_SERVICE_LOCAL_HOST}}\n\n# TUNNEL IP version\n# This is the IP version to use for tunnel endpoints\nTUNNEL_IP_VERSION=${TUNNEL_IP_VERSION:-4}\n\n# Validate TUNNEL_IP_VERSION\nif [[ $TUNNEL_IP_VERSION != \"4\" ]] && [[ $TUNNEL_IP_VERSION != \"6\" ]]; then\n    die $LINENO \"TUNNEL_IP_VERSION must be either 4 or 6\"\nfi\n\nif [[ \"$TUNNEL_IP_VERSION\" == 4 ]]; then\n    DEF_TUNNEL_ENDPOINT_IP=$HOST_IP\nfi\n\nif [[ \"$TUNNEL_IP_VERSION\" == 6 ]]; then\n    # Only die if the user has not over-ridden the endpoint IP\n    if [[ \"$HOST_IPV6\" == \"\" ]] && [[ \"$TUNNEL_ENDPOINT_IP\" == \"\" ]]; then\n        die $LINENO \"Could not determine host IPv6 address.  See local.conf for suggestions on setting HOST_IPV6.\"\n    fi\n\n    DEF_TUNNEL_ENDPOINT_IP=$HOST_IPV6\nfi\n\n# Allow the use of an alternate address for tunnel endpoints.\n# Default is dependent on TUNNEL_IP_VERSION above.\nTUNNEL_ENDPOINT_IP=${TUNNEL_ENDPOINT_IP:-${DEF_TUNNEL_ENDPOINT_IP}}\n\n# Configure services to use syslog instead of writing to individual log files\nSYSLOG=$(trueorfalse False SYSLOG)\nSYSLOG_HOST=${SYSLOG_HOST:-$HOST_IP}\nSYSLOG_PORT=${SYSLOG_PORT:-516}\n\n# Set global ``GIT_DEPTH=<number>`` to limit the history depth of the git clone\n# Set to 0 to disable shallow cloning\nGIT_DEPTH=${GIT_DEPTH:-0}\n\n# We may not need to recreate database in case 2 Keystone services\n# sharing the same database. It would be useful for multinode Grenade tests.\nRECREATE_KEYSTONE_DB=$(trueorfalse True RECREATE_KEYSTONE_DB)\n\n# Following entries need to be last items in file\n\n# New way is LOGDIR for all logs and LOGFILE for stack.sh trace log, but if not fully-qualified will be in LOGDIR\n# LOGFILE       LOGDIR              output\n# not set       not set             (new) set LOGDIR from default\n# set           not set             stack.sh log to LOGFILE, (new) set LOGDIR from LOGFILE\n# not set       set                 screen logs to LOGDIR\n# set           set                 stack.sh log to LOGFILE, screen logs to LOGDIR\n\n# Set up new logging defaults\nif [[ -z \"${LOGDIR:-}\" ]]; then\n    default_logdir=$DEST/logs\n    if [[ -z \"${LOGFILE:-}\" ]]; then\n        # Nothing is set, we need a default\n        LOGDIR=\"$default_logdir\"\n    else\n        # Set default LOGDIR\n        LOGDIR=\"${LOGFILE%/*}\"\n        logfile=\"${LOGFILE##*/}\"\n        if [[ -z \"$LOGDIR\" || \"$LOGDIR\" == \"$logfile\" ]]; then\n            # LOGFILE had no path, set a default\n            LOGDIR=\"$default_logdir\"\n        fi\n    fi\n    unset default_logdir logfile\nfi\n\n# ``LOGDIR`` is always set at this point so it is not useful as a 'enable' for service logs\n\n# System-wide ulimit file descriptors override\nULIMIT_NOFILE=${ULIMIT_NOFILE:-2048}\n\n# Local variables:\n# mode: shell-script\n# End:\n"
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "tools",
          "type": "tree",
          "content": null
        },
        {
          "name": "tox.ini",
          "type": "blob",
          "size": 1.7783203125,
          "content": "[tox]\nminversion = 3.18.0\nskipsdist = True\nenvlist = bashate\n\n[testenv]\nusedevelop = False\nbasepython = python3\n\n[testenv:bashate]\n# if you want to test out some changes you have made to bashate\n# against devstack, just set BASHATE_INSTALL_PATH=/path/... to your\n# modified bashate tree\ndeps =\n   {env:BASHATE_INSTALL_PATH:bashate}\nallowlist_externals = bash\ncommands = bash -c \"find {toxinidir}             \\\n         -not \\( -type d -name .?\\* -prune \\)    \\\n         -not \\( -type d -name doc -prune \\)     \\\n         -not \\( -type f -name localrc -prune \\) \\\n         -type f                                 \\\n         -not -name \\*~                          \\\n         -not -name \\*.md                        \\\n         -not -name stack-screenrc               \\\n         -not -name \\*.orig                      \\\n         -not -name \\*.rej                       \\\n         \\(                                      \\\n          -name \\*.sh -or                        \\\n          -name \\*rc -or                         \\\n          -name functions\\* -or                  \\\n          -wholename \\*/inc/\\* -or               \\\n          -wholename \\*/lib/\\*                   \\\n         \\)                                      \\\n         -print0 | xargs -0 bashate -v -iE006 -eE005,E042\"\n\n[testenv:docs]\ndeps =\n  -c{env:TOX_CONSTRAINTS_FILE:https://releases.openstack.org/constraints/upper/master}\n  -r{toxinidir}/doc/requirements.txt\nallowlist_externals = bash\nsetenv =\n  TOP_DIR={toxinidir}\ncommands =\n  sphinx-build -W -b html -d doc/build/doctrees doc/source doc/build/html\n\n[testenv:pdf-docs]\ndeps = {[testenv:docs]deps}\nallowlist_externals =\n   make\ncommands =\n   sphinx-build -W -b latex doc/source doc/build/pdf\n   make -C doc/build/pdf\n\n[testenv:venv]\ndeps = -r{toxinidir}/doc/requirements.txt\ncommands = {posargs}\n"
        },
        {
          "name": "unstack.sh",
          "type": "blob",
          "size": 3.9638671875,
          "content": "#!/bin/bash\n\n# **unstack.sh**\n\n# Stops that which is started by ``stack.sh`` (mostly)\n# mysql and rabbit are left running as OpenStack code refreshes\n# do not require them to be restarted.\n#\n# Stop all processes by setting ``UNSTACK_ALL`` or specifying ``-a``\n# on the command line\n\nUNSTACK_ALL=${UNSTACK_ALL:-\"\"}\n\nwhile getopts \":a\" opt; do\n    case $opt in\n        a)\n            UNSTACK_ALL=\"-1\"\n            ;;\n    esac\ndone\n\n# Keep track of the current DevStack directory.\nTOP_DIR=$(cd $(dirname \"$0\") && pwd)\nFILES=$TOP_DIR/files\n\n# Import common functions\nsource $TOP_DIR/functions\n\n# Import database library\nsource $TOP_DIR/lib/database\n\n# Load local configuration\nsource $TOP_DIR/openrc\n\n# Destination path for service data\nDATA_DIR=${DATA_DIR:-${DEST}/data}\n\nif [[ $EUID -eq 0 ]]; then\n    echo \"You are running this script as root.\"\n    echo \"It might work but you will have a better day running it as $STACK_USER\"\n    exit 1\nfi\n\n\n# Configure Projects\n# ==================\n\n# Determine what system we are running on.  This provides ``os_VENDOR``,\n# ``os_RELEASE``, ``os_PACKAGE``, ``os_CODENAME`` and ``DISTRO``\nGetDistro\n\n# Plugin Phase 0: override_defaults - allow plugins to override\n# defaults before other services are run\nrun_phase override_defaults\n\n# Import apache functions\nsource $TOP_DIR/lib/apache\n\n# Import TLS functions\nsource $TOP_DIR/lib/tls\n\n# Source project function libraries\nsource $TOP_DIR/lib/infra\nsource $TOP_DIR/lib/oslo\nsource $TOP_DIR/lib/lvm\nsource $TOP_DIR/lib/horizon\nsource $TOP_DIR/lib/keystone\nsource $TOP_DIR/lib/glance\nsource $TOP_DIR/lib/nova\nsource $TOP_DIR/lib/placement\nsource $TOP_DIR/lib/cinder\nsource $TOP_DIR/lib/swift\nsource $TOP_DIR/lib/neutron\nsource $TOP_DIR/lib/ldap\nsource $TOP_DIR/lib/dstat\nsource $TOP_DIR/lib/etcd3\n\n# Extras Source\n# --------------\n\n# Phase: source\nif [[ -d $TOP_DIR/extras.d ]]; then\n    for i in $TOP_DIR/extras.d/*.sh; do\n        [[ -r $i ]] && source $i source\n    done\nfi\n\nload_plugin_settings\n\nset -o xtrace\n\n# Run extras\n# ==========\n\n# Phase: unstack\nrun_phase unstack\n\n# Call service stop\n\nif is_service_enabled nova; then\n    stop_nova\n    cleanup_nova\nfi\n\nif is_service_enabled placement; then\n    stop_placement\nfi\n\nif is_service_enabled glance; then\n    stop_glance\nfi\n\nif is_service_enabled keystone; then\n    stop_keystone\nfi\n\n# Swift runs daemons\nif is_service_enabled s-proxy; then\n    stop_swift\n    cleanup_swift\nfi\n\n# Apache has the WSGI processes\nif is_service_enabled horizon; then\n    stop_horizon\nfi\n\n# Kill TLS proxies and cleanup certificates\nif is_service_enabled tls-proxy; then\n    stop_tls_proxy\n    cleanup_CA\nfi\n\nSCSI_PERSIST_DIR=$CINDER_STATE_PATH/volumes/*\n\n# BUG: tgt likes to exit 1 on service stop if everything isn't\n# perfect, we should clean up cinder stop paths.\n\n# Get the iSCSI volumes\nif is_service_enabled cinder; then\n    stop_cinder || /bin/true\n    cleanup_cinder || /bin/true\nfi\n\nif [[ -n \"$UNSTACK_ALL\" ]]; then\n    # Stop MySQL server\n    if is_service_enabled mysql; then\n        stop_service mysql\n    fi\n\n    if is_service_enabled postgresql; then\n        stop_service postgresql\n    fi\n\n    # Stop rabbitmq-server\n    if is_service_enabled rabbit; then\n        stop_service rabbitmq-server\n    fi\nfi\n\nif is_service_enabled neutron; then\n    stop_neutron\n    cleanup_neutron\nfi\n\nif is_service_enabled etcd3; then\n    stop_etcd3\n    cleanup_etcd3\nfi\n\nif is_service_enabled openstack-cli-server; then\n    stop_service devstack@openstack-cli-server\nfi\n\nstop_dstat\n\n# NOTE: Cinder automatically installs the lvm2 package, independently of the\n# enabled backends. So if Cinder is enabled, and installed successfully we are\n# sure lvm2 (lvremove, /etc/lvm/lvm.conf, etc.) is here.\nif is_service_enabled cinder && is_package_installed lvm2; then\n    clean_lvm_filter\nfi\n\nclean_pyc_files\nrm -Rf $DEST/async\n\n# Clean any safe.directory items we wrote into the global\n# gitconfig. We can identify the relevant ones by checking that they\n# point to somewhere in our $DEST directory.\nsudo sed -i \"\\+directory = ${DEST}+ d\" /etc/gitconfig\n"
        }
      ]
    }
  ]
}