{
  "metadata": {
    "timestamp": 1736568778450,
    "page": 904,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjkwOQ==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "learnbyexample/learn_gnuawk",
      "stars": 1095,
      "defaultBranch": "master",
      "files": [
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.046875,
          "content": "MIT License\n\nCopyright (c) 2023 Sundeep Agarwal\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 5.66796875,
          "content": "# CLI text processing with GNU awk\n\nExample based guide to mastering GNU awk. Visit https://youtu.be/KIa_EaYwGDI for a short video about the book.\n\n<p align=\"center\"><img src=\"./images/gawk_ls.png\" alt=\"CLI text processing with GNU awk ebook cover image\" /></p>\n\nThe book also includes exercises to test your understanding, which are presented together as a single file in this repo — [Exercises.md](./exercises/Exercises.md).\n\nFor solutions to the exercises, see [Exercise_solutions.md](./exercises/Exercise_solutions.md).\n\nYou can also use [this interactive TUI app](https://github.com/learnbyexample/TUI-apps/blob/main/AwkExercises) to practice some of the exercises from the book.\n\nSee [Version_changes.md](./Version_changes.md) to keep track of changes made to the book.\n\n<br>\n\n# E-book\n\n* You can purchase the pdf/epub versions of the book using these links:\n    * https://learnbyexample.gumroad.com/l/gnu_awk\n    * https://leanpub.com/gnu_awk\n* You can also get the book as part of these bundles:\n    * **All books bundle** bundle from https://learnbyexample.gumroad.com/l/all-books\n        * Includes all my programming books\n    * **Magical one-liners** bundle from https://learnbyexample.gumroad.com/l/oneliners or https://leanpub.com/b/oneliners\n    * **Awesome Regex** bundle from https://learnbyexample.gumroad.com/l/regex or https://leanpub.com/b/regex\n* See https://learnbyexample.github.io/books/ for a list of other books\n\nFor a preview of the book, see [sample chapters](./sample_chapters/gnu_awk_sample.pdf).\n\nThe book can also be [viewed as a single markdown file in this repo](./gnu_awk.md). See my blogpost on [generating pdfs from markdown using pandoc](https://learnbyexample.github.io/customizing-pandoc/) if you are interested in the ebook creation process.\n\nFor the web version of the book, visit https://learnbyexample.github.io/learn_gnuawk/\n\n<br>\n\n# Testimonials\n\n>Step up your cli fu with this fabulous intro & deep dive into awk. I learned a ton of tricks!\n>\n> — [feedback on twitter](https://twitter.com/killchain/status/1246820137455452163)\n\n>I consider myself pretty experienced at shell-fu and capable of doing most things I set out to achieve in either bash scripts or fearless one-liners. However, my awk is rudimentary at best, I think mostly because it's such an unforgiving environment to experiment in.\n>\n>These books you've written are great for a bit of first principles insight and then quickly building up to functional usage. I will have no hesitation in referring colleagues to them!\n>\n> — [feedback on Hacker News](https://news.ycombinator.com/item?id=31930840)\n\n<br>\n\n# Feedback and Contributing\n\n⚠️ ⚠️ Please DO NOT submit pull requests. Main reason being any modification requires changes in multiple places.\n\nI would highly appreciate it if you'd let me know how you felt about this book. It could be anything from a simple thank you, pointing out a typo, mistakes in code snippets, which aspects of the book worked for you (or didn't!) and so on. Reader feedback is essential and especially so for self-published authors.\n\nYou can reach me via:\n\n* Issue Manager: [https://github.com/learnbyexample/learn_gnuawk/issues](https://github.com/learnbyexample/learn_gnuawk/issues)\n* E-mail: `echo 'bGVhcm5ieWV4YW1wbGUubmV0QGdtYWlsLmNvbQo=' | base64 --decode`\n* Twitter: [https://twitter.com/learn_byexample](https://twitter.com/learn_byexample)\n\n<br>\n\n# Table of Contents\n\n1) Preface\n2) Installation and Documentation\n3) awk introduction\n4) Regular Expressions\n5) Field separators\n6) Record separators\n7) In-place file editing\n8) Using shell variables\n9) Control Structures\n10) Built-in functions\n11) Multiple file input\n12) Processing multiple records\n13) Two file processing\n14) Dealing with duplicates\n15) awk scripts\n16) Gotchas and Tips\n17) Further Reading\n\n<br>\n\n# Acknowledgements\n\n* [GNU awk documentation](https://www.gnu.org/software/gawk/manual/) — manual and examples\n* [stackoverflow](https://stackoverflow.com/) and [unix.stackexchange](https://unix.stackexchange.com/) — for getting answers to pertinent questions on `awk` and related commands\n* [tex.stackexchange](https://tex.stackexchange.com/) — for help on [pandoc](https://github.com/jgm/pandoc/) and `tex` related questions\n* [/r/commandline/](https://old.reddit.com/r/commandline), [/r/linux4noobs/](https://old.reddit.com/r/linux4noobs/), [/r/linuxquestions/](https://old.reddit.com/r/linuxquestions/) and [/r/linux/](https://old.reddit.com/r/linux/) — helpful forums\n* [canva](https://www.canva.com/) — cover image\n* [oxipng](https://github.com/shssoichiro/oxipng), [pngquant](https://pngquant.org/) and [svgcleaner](https://github.com/RazrFalcon/svgcleaner) — optimizing images\n* [Warning](https://commons.wikimedia.org/wiki/File:Warning_icon.svg) and [Info](https://commons.wikimedia.org/wiki/File:Info_icon_002.svg) icons by [Amada44](https://commons.wikimedia.org/wiki/User:Amada44) under public domain\n* [arifmahmudrana](https://github.com/arifmahmudrana) for spotting an ambiguous explanation\n* [Pound-Hash](https://github.com/Pound-Hash) for critical feedback\n* [mdBook](https://github.com/rust-lang/mdBook) — for web version of the book\n    * [mdBook-pagetoc](https://github.com/JorelAli/mdBook-pagetoc) — for adding table of contents for each chapter\n    * [minify-html](https://github.com/wilsonzlin/minify-html) — for minifying html files\n\nSpecial thanks to all my friends and online acquaintances for their help, support and encouragement, especially during these difficult times.\n\n<br>\n\n# License\n\nThe book is licensed under a [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License](https://creativecommons.org/licenses/by-nc-sa/4.0/).\n\nThe code snippets are licensed under MIT, see [LICENSE](./LICENSE) file.\n\n"
        },
        {
          "name": "Version_changes.md",
          "type": "blob",
          "size": 2.703125,
          "content": "<br>\n\n### 2.0\n\n* Command version updated to **GNU awk 5.2.2**\n* Many more exercises added, and you can practice some of them using this [interactive TUI app](https://github.com/learnbyexample/TUI-apps/blob/main/AwkExercises)\n* Long sections split into smaller ones\n* In general, many of the examples, exercises, solutions, descriptions and external links were updated/corrected\n* Updated Acknowledgements section\n* Code snippets related to info/warning sections will now appear as a single block\n* Book title changed to **CLI text processing with GNU awk**\n* New cover image\n* Images centered for EPUB format\n\n<br>\n\n### 1.4\n\n* Added example for `NF` value when input line doesn't contain the input field separator or if it is empty.\n* Added example which uses both `nextfile` and `ENDFILE`.\n* Added example for working with floating-point numbers according to locale formatting.\n* Clarified use of `\\0` with `gensub` function.\n* Updated error message for file not found.\n* Added further reading links for regexp metacharacter escaping and `NR==FNR` alternatives.\n\n<br>\n\n### 1.3\n\n* Added note regarding use of `NR==FNR` if the first file is empty\n* Added note for `split` function when the field separator is the same as default `FS`\n* Simplified one of the examples for paragraph mode\n    * Updated exercise solutions as well for this usecase\n* Updated timing data for speed comparison examples, added `mawk` results as well\n* Corrected various typos, improved descriptions, comments, external links, etc\n\n<br>\n\n### 1.2\n\n* Added link to exercise solutions\n* Corrected typo in a solution\n* Two of the buffer examples simplified\n* Corrected line anchor explanations to be referred as string anchor instead\n\n<br>\n\n### 1.1\n\n* Clarified BRE vs ERE difference for line anchor escaping\n* Added workaround for **epub** version for iBooks\n* For more detailed view of changes, see the [commit changes for the markdown source file](https://github.com/learnbyexample/learn_gnuawk/commit/7c6ffe055cf562bbd064a81f5f869e64b1692816#diff-6d6c4458a3b540abe5f09cb6af55992f)\n\n<br>\n\n### 1.0\n\n* Added exercises\n* `GNU awk` version updated to `5.1.0`\n* Role of `IGNORECASE` for `FS`, `FPAT` and `RS`\n* Alternation priority for same length matches\n* Using escape sequences for regular expression metacharacters\n* Corrected multiline code snippets and added chapter sub-headings for clarity\n* Added gotcha for using code in replacement section of substitution functions\n* Corrected various typos, improved descriptions/comments/examples/etc\n* Added epub version of the book\n* For more detailed view of changes, see the [commit changes for the markdown source file](https://github.com/learnbyexample/learn_gnuawk/commit/e7f6bcc35dc8c503c729b76aaa0aa582896516a0)\n\n<br>\n\n### 0.7\n\n* First version\n"
        },
        {
          "name": "code_snippets",
          "type": "tree",
          "content": null
        },
        {
          "name": "example_files",
          "type": "tree",
          "content": null
        },
        {
          "name": "exercises",
          "type": "tree",
          "content": null
        },
        {
          "name": "gnu_awk.md",
          "type": "blob",
          "size": 198.193359375,
          "content": "# Preface\n\nWhen it comes to command line text processing, the three major pillars are `grep` for filtering, `sed` for substitution and `awk` for field processing. These tools have overlapping features too, for example, all three of them have extensive filtering capabilities.\n\nUnlike `grep` and `sed`, `awk` is a programming language. However, this book intends to showcase `awk` one-liners that can be composed from the command line instead of focusing on larger scripts.\n\nThis book heavily leans on examples to present features one by one. Regular expressions will also be discussed in detail.\n\nIt is recommended that you manually type each example. Make an effort to understand the sample input as well as the solution presented and check if the output changes (or not!) when you alter some part of the input and the command. As an analogy, consider learning to drive a car — no matter how much you read about them or listen to explanations, you'd need practical experience to become proficient.\n\n## Prerequisites\n\nYou should be familiar with command line usage in a Unix-like environment. You should also be comfortable with concepts like file redirection and command pipelines. Knowing the basics of the `grep` and `sed` commands will be handy in understanding the filtering and substitution features of `awk`.\n\nAs `awk` is a programming language, you are also expected to be familiar with concepts like variables, printing, functions, control structures, arrays and so on.\n\nIf you are new to the world of the command line, check out my [Computing from the Command Line](https://github.com/learnbyexample/cli-computing) ebook and curated resources on [Linux CLI and Shell scripting](https://learnbyexample.github.io/curated_resources/linux_cli_scripting.html) before starting this book.\n\n## Conventions\n\n* The examples presented here have been tested with **GNU awk** version **5.2.2** and includes features not available in earlier versions.\n* Code snippets are copy pasted from the `GNU bash` shell and modified for presentation purposes. Some commands are preceded by comments to provide context and explanations. Blank lines to improve readability, only `real` time shown for speed comparisons, output skipped for commands like `wget` and so on.\n* Unless otherwise noted, all examples and explanations are meant for **ASCII** input.\n* `awk` would mean `GNU awk`, `sed` would mean `GNU sed`, `grep` would mean `GNU grep` and so on unless otherwise specified.\n* External links are provided throughout the book for you to explore certain topics in more depth.\n* The [learn_gnuawk repo](https://github.com/learnbyexample/learn_gnuawk) has all the code snippets and files used in examples, exercises and other details related to the book. If you are not familiar with the `git` command, click the **Code** button on the webpage to get the files.\n\n## Acknowledgements\n\n* [GNU awk documentation](https://www.gnu.org/software/gawk/manual/) — manual and examples\n* [stackoverflow](https://stackoverflow.com/) and [unix.stackexchange](https://unix.stackexchange.com/) — for getting answers to pertinent questions on `awk` and related commands\n* [tex.stackexchange](https://tex.stackexchange.com/) — for help on [pandoc](https://github.com/jgm/pandoc/) and `tex` related questions\n* [/r/commandline/](https://old.reddit.com/r/commandline), [/r/linux4noobs/](https://old.reddit.com/r/linux4noobs/), [/r/linuxquestions/](https://old.reddit.com/r/linuxquestions/) and [/r/linux/](https://old.reddit.com/r/linux/) — helpful forums\n* [canva](https://www.canva.com/) — cover image\n* [oxipng](https://github.com/shssoichiro/oxipng), [pngquant](https://pngquant.org/) and [svgcleaner](https://github.com/RazrFalcon/svgcleaner) — optimizing images\n* [Warning](https://commons.wikimedia.org/wiki/File:Warning_icon.svg) and [Info](https://commons.wikimedia.org/wiki/File:Info_icon_002.svg) icons by [Amada44](https://commons.wikimedia.org/wiki/User:Amada44) under public domain\n* [arifmahmudrana](https://github.com/arifmahmudrana) for spotting an ambiguous explanation\n* [Pound-Hash](https://github.com/Pound-Hash) for critical feedback\n\nSpecial thanks to all my friends and online acquaintances for their help, support and encouragement, especially during these difficult times.\n\n## Feedback and Errata\n\nI would highly appreciate it if you'd let me know how you felt about this book. It could be anything from a simple thank you, pointing out a typo, mistakes in code snippets, which aspects of the book worked for you (or didn't!) and so on. Reader feedback is essential and especially so for self-published authors.\n\nYou can reach me via:\n\n* Issue Manager: [https://github.com/learnbyexample/learn_gnuawk/issues](https://github.com/learnbyexample/learn_gnuawk/issues)\n* E-mail: learnbyexample.net@gmail.com\n* Twitter: [https://twitter.com/learn_byexample](https://twitter.com/learn_byexample)\n\n## Author info\n\nSundeep Agarwal is a lazy being who prefers to work just enough to support his modest lifestyle. He accumulated vast wealth working as a Design Engineer at Analog Devices and retired from the corporate world at the ripe age of twenty-eight. Unfortunately, he squandered his savings within a few years and had to scramble trying to earn a living. Against all odds, selling programming ebooks saved his lazy self from having to look for a job again. He can now afford all the fantasy ebooks he wants to read and spends unhealthy amount of time browsing the internet.\n\nWhen the creative muse strikes, he can be found working on yet another programming ebook (which invariably ends up having at least one example with regular expressions). Researching materials for his ebooks and everyday social media usage drowned his bookmarks, so he maintains curated resource lists for sanity sake. He is thankful for free learning resources and open source tools. His own contributions can be found at [https://github.com/learnbyexample](https://github.com/learnbyexample).\n\n**List of books:** https://learnbyexample.github.io/books/\n\n## License\n\nThis work is licensed under a [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License](https://creativecommons.org/licenses/by-nc-sa/4.0/).\n\nCode snippets are available under [MIT License](https://github.com/learnbyexample/learn_gnuawk/blob/master/LICENSE).\n\nResources mentioned in Acknowledgements section are available under original licenses.\n\n## Book version\n\n2.0\n\nSee [Version_changes.md](https://github.com/learnbyexample/learn_gnuawk/blob/master/Version_changes.md) to track changes across book versions.\n\n# Installation and Documentation\n\nThe command name `awk` is derived from its developers — Alfred V. **A**ho, Peter J. **W**einberger, and Brian W. **K**ernighan. Over the years, it has been adapted and modified by various other developers. See [gawk manual: History](https://www.gnu.org/software/gawk/manual/gawk.html#History) for more details.\n\nThis chapter will show how to install or upgrade `awk` followed by details related to documentation.\n\n## Installation\n\nIf you are on a Unix-like system, you will most likely have some version of `awk` already installed. This book is primarily about `GNU awk`. As there are syntax and feature differences between various implementations, make sure to use `GNU awk` to follow along the examples presented in this book.\n\n`GNU awk` is part of the [text creation and manipulation](https://www.gnu.org/manual/manual.html) commands and usually comes by default on GNU/Linux distributions. To install a particular version, visit [gnu: gawk software](https://www.gnu.org/software/gawk/). See also [release notes](https://lists.gnu.org/archive/cgi-bin/namazu.cgi?query=gawk+released&submit=Search%21&idxname=info-gnu&max=20&result=normal&sort=date%3Alate) for an overview of changes between versions.\n\n```bash\n$ wget https://ftp.gnu.org/gnu/gawk/gawk-5.2.2.tar.xz\n$ tar -Jxf gawk-5.2.2.tar.xz\n$ cd gawk-5.2.2/\n# see https://askubuntu.com/q/237576 if you get compiler not found error\n$ ./configure\n$ make\n$ sudo make install\n\n$ awk --version | head -n1\nGNU Awk 5.2.2, API 3.2, PMA Avon 8-g1\n```\n\nIf you are not using a Linux distribution, you may be able to access `GNU awk` using an option below:\n\n* [Git for Windows](https://git-scm.com/downloads) — provides a Bash emulation used to run Git from the command line\n* [Windows Subsystem for Linux](https://en.wikipedia.org/wiki/Windows_Subsystem_for_Linux) — compatibility layer for running Linux binary executables natively on Windows\n* [brew](https://brew.sh/) — Package Manager for macOS (or Linux)\n\n>![info](images/info.svg) See also [gawk manual: Installation](https://www.gnu.org/software/gawk/manual/html_node/Installation.html) for advanced options and instructions to install `awk` on other platforms.\n\n## Documentation\n\nIt is always good to know where to find documentation. From the command line, you can use `man awk` for a short manual and `info awk` for the full documentation. I prefer using the [online gnu awk manual](https://www.gnu.org/software/gawk/manual/), which feels much easier to use and navigate.\n\nHere's a snippet from `man awk`:\n\n```bash\n$ man awk\nGAWK(1)                        Utility Commands                        GAWK(1)  \n  \nNAME\n       gawk - pattern scanning and processing language\n\nSYNOPSIS\n       gawk [ POSIX or GNU style options ] -f program-file [ -- ] file ...\n       gawk [ POSIX or GNU style options ] [ -- ] program-text file ...\n\nDESCRIPTION\n       Gawk  is  the  GNU Project's implementation of the AWK programming lan‐\n       guage.  It conforms to the definition of  the  language  in  the  POSIX\n       1003.1  Standard.   This version in turn is based on the description in\n       The AWK Programming Language, by Aho, Kernighan, and Weinberger.   Gawk\n       provides  the additional features found in the current version of Brian\n       Kernighan's awk and numerous GNU-specific extensions.\n```\n\n## Options overview\n\nFor a quick overview of all the available options, use `awk --help` from the command line.\n\n```bash\n$ awk --help\nUsage: awk [POSIX or GNU style options] -f progfile [--] file ...\nUsage: awk [POSIX or GNU style options] [--] 'program' file ...\nPOSIX options:                  GNU long options: (standard)\n    -f progfile                 --file=progfile\n    -F fs                       --field-separator=fs\n    -v var=val                  --assign=var=val\nShort options:                  GNU long options: (extensions)\n    -b                          --characters-as-bytes\n    -c                          --traditional\n    -C                          --copyright\n    -d[file]                    --dump-variables[=file]\n    -D[file]                    --debug[=file]\n    -e 'program-text'           --source='program-text'\n    -E file                     --exec=file\n    -g                          --gen-pot\n    -h                          --help\n    -i includefile              --include=includefile\n    -I                          --trace\n    -l library                  --load=library\n    -L[fatal|invalid|no-ext]    --lint[=fatal|invalid|no-ext]\n    -M                          --bignum\n    -N                          --use-lc-numeric\n    -n                          --non-decimal-data\n    -o[file]                    --pretty-print[=file]\n    -O                          --optimize\n    -p[file]                    --profile[=file]\n    -P                          --posix\n    -r                          --re-interval\n    -s                          --no-optimize\n    -S                          --sandbox\n    -t                          --lint-old\n    -V                          --version\n```\n\n# awk introduction\n\nThis chapter will give an overview of `awk` syntax and some examples to show what kind of problems you could solve using `awk`. These features will be covered in depth in later, but you shouldn't skip this chapter.\n\n## Filtering\n\n`awk` provides filtering capabilities like those supported by the `grep` and `sed` commands. As a programming language, there are additional nifty features as well. Similar to many command line utilities, `awk` can accept input from both stdin and files.\n\n```bash\n# sample stdin data\n$ printf 'gate\\napple\\nwhat\\nkite\\n'\ngate\napple\nwhat\nkite\n\n# same as: grep 'at' and sed -n '/at/p'\n# filter lines containing 'at'\n$ printf 'gate\\napple\\nwhat\\nkite\\n' | awk '/at/'\ngate\nwhat\n\n# same as: grep -v 'e' and sed -n '/e/!p'\n# filter lines NOT containing 'e'\n$ printf 'gate\\napple\\nwhat\\nkite\\n' | awk '!/e/'\nwhat\n```\n\nBy default, `awk` automatically loops over the input content line by line. You can then use programming instructions to process those lines. As `awk` is often used from the command line, many shortcuts are available to reduce the amount of typing needed.\n\nIn the above examples, a regular expression (defined by the pattern between a pair of forward slashes) has been used to filter the input. Regular expressions (regexp) will be covered in detail in the [next chapter](#regular-expressions). String values without any special regexp characters are used in this chapter. The full syntax is `string ~ /regexp/` to check if the given string matches the regexp and `string !~ /regexp/` to check if doesn't match. When the string isn't specified, the test is performed against a special variable `$0`, which has the contents of the input line. The correct term would be input **record**, but that's a discussion for a [later chapter](#record-separators).\n\nAlso, in the above examples, only the filtering condition was given. By default, when the condition evaluates to `true`, the contents of `$0` is printed. Thus:\n\n* `awk '/regexp/'` is a shortcut for `awk '$0 ~ /regexp/{print $0}'`\n* `awk '!/regexp/'` is a shortcut for `awk '$0 !~ /regexp/{print $0}'`\n\n```bash\n# same as: awk '/at/'\n$ printf 'gate\\napple\\nwhat\\nkite\\n' | awk '$0 ~ /at/{print $0}'\ngate\nwhat\n\n# same as: awk '!/e/'\n$ printf 'gate\\napple\\nwhat\\nkite\\n' | awk '$0 !~ /e/{print $0}'\nwhat\n```\n\nIn the above examples, `{}` is used to specify a block of code to be executed when the condition that precedes the block evaluates to `true`. One or more statements can be given separated by the `;` character. You'll see such examples and learn more about `awk` syntax later.\n\n## Idiomatic use of 1\n\nIn a conditional expression, non-zero numeric values and non-empty string values are evaluated as `true`. Idiomatically, `1` is used to denote a `true` condition in one-liners as a shortcut to print the contents of `$0`.\n\n```bash\n# same as: printf 'gate\\napple\\nwhat\\nkite\\n' | cat\n# same as: awk '{print $0}'\n$ printf 'gate\\napple\\nwhat\\nkite\\n' | awk '1'\ngate\napple\nwhat\nkite\n```\n\n## Substitution\n\n`awk` has three functions to cover search and replace requirements. Two of them are shown below. The `sub` function replaces only the first match, whereas the `gsub` function replaces all the matching occurrences. By default, these functions operate on `$0` when the input string isn't provided. Both `sub` and `gsub` modifies the input source on successful substitution.\n\n```bash\n# for each input line, change only the first ':' to '-'\n# same as: sed 's/:/-/'\n$ printf '1:2:3:4\\na:b:c:d\\n' | awk '{sub(/:/, \"-\")} 1'\n1-2:3:4\na-b:c:d\n\n# for each input line, change all ':' to '-'\n# same as: sed 's/:/-/g'\n$ printf '1:2:3:4\\na:b:c:d\\n' | awk '{gsub(/:/, \"-\")} 1'\n1-2-3-4\na-b-c-d\n```\n\nThe first argument to the `sub` and `gsub` functions is the regexp to be matched against the input content. The second argument is the replacement string. String literals are specified within double quotes. In the above examples, `sub` and `gsub` are used inside a block as they aren't intended to be used as a conditional expression. The `1` after the block is treated as a conditional expression as it is used outside a block. You can also use the variations presented below to get the same results:\n\n* `awk '{sub(/:/, \"-\")} 1'` is same as `awk '{sub(/:/, \"-\"); print $0}'`\n* You can also just use `print` instead of `print $0` as `$0` is the default string\n\n>![info](images/info.svg) You might wonder why to use or learn `grep` and `sed` when you can achieve the same results with `awk`. It depends on the problem you are trying to solve. A simple line filtering will be faster with `grep` compared to `sed` or `awk` because `grep` is optimized for such cases. Similarly, `sed` will be faster than `awk` for substitution cases. Also, not all features easily translate among these tools. For example, `grep -o` requires lot more steps to code with `sed` or `awk`. Only `grep` offers recursive search. And so on. See also [unix.stackexchange: When to use grep, sed, awk, perl, etc](https://unix.stackexchange.com/q/303044/109046).\n\n## Field processing\n\nAs mentioned before, `awk` is primarily used for field based processing. Consider the sample input file shown below with fields separated by a single space character.\n\n>![info](images/info.svg) The [example_files](https://github.com/learnbyexample/learn_gnuawk/tree/master/example_files) directory has all the files used in the examples.\n\n```bash\n$ cat table.txt\nbrown bread mat hair 42\nblue cake mug shirt -7\nyellow banana window shoes 3.14\n```\n\nHere are some examples that are based on a specific field rather than the entire line. By default, `awk` splits the input line based on spaces and the field contents can be accessed using `$N` where `N` is the field number required. A special variable `NF` is updated with the total number of fields for each input line. There are many more details and nuances to cover regarding the default field splitting, but for now this is enough to proceed.\n\n```bash\n# print the second field of each input line\n$ awk '{print $2}' table.txt\nbread\ncake\nbanana\n\n# print lines only if the last field is a negative number\n# recall that the default action is to print the contents of $0\n$ awk '$NF<0' table.txt\nblue cake mug shirt -7\n\n# change 'b' to 'B' only for the first field\n$ awk '{gsub(/b/, \"B\", $1)} 1' table.txt\nBrown bread mat hair 42\nBlue cake mug shirt -7\nyellow banana window shoes 3.14\n```\n\n## awk one-liner structure\n\nThe examples in the previous sections have used a few different ways to construct a typical `awk` one-liner. If you haven't yet grasped the syntax, this generic structure might help:\n\n`awk 'cond1{action1} cond2{action2} ... condN{actionN}'`\n\nWhen a condition isn't provided, the action is always executed. Within a block, you can provide multiple statements separated by the semicolon character. If an action isn't provided, then by default, contents of `$0` variable is printed if the condition evaluates to `true`. When action isn't present, you can use a semicolon to terminate a condition and start another `condX{actionX}` snippet.\n\nNote that multiple blocks are just a syntactical sugar. It helps to avoid explicit use of `if` control structure for most one-liners. The below snippet shows the same code with and without `if` structure.\n\n```bash\n$ awk '{\n         if($NF < 0){\n            print $0\n         }\n       }' table.txt\nblue cake mug shirt -7\n\n$ awk '$NF<0' table.txt\nblue cake mug shirt -7\n```\n\nYou can use a `BEGIN{}` block when you need to execute something before the input is read and an `END{}` block to execute something after all of the input has been processed.\n\n```bash\n$ seq 2 | awk 'BEGIN{print \"---\"} 1; END{print \"%%%\"}'\n---\n1\n2\n%%%\n```\n\nThere are some more types of blocks that can be used, you'll see them in coming chapters. See [gawk manual: Operators](https://www.gnu.org/software/gawk/manual/gawk.html#All-Operators) for details about operators and [gawk manual: Truth Values and Conditions](https://www.gnu.org/software/gawk/manual/gawk.html#Truth-Values-and-Conditions) for conditional expressions.\n\n## Strings and Numbers\n\nSome examples so far have already used string and numeric literals. As mentioned earlier, `awk` tries to provide a concise way to construct a solution from the command line. The data type of a value is determined based on the syntax used. String literals are represented inside double quotes. Numbers can be integers or floating-point. Scientific notation is allowed as well. See [gawk manual: Constant Expressions](https://www.gnu.org/software/gawk/manual/gawk.html#Constants) for more details.\n\n```bash\n# BEGIN{} is also useful to write an awk program without any external input\n$ awk 'BEGIN{print \"hi\"}'\nhi\n\n$ awk 'BEGIN{print 42}'\n42\n$ awk 'BEGIN{print 3.14}'\n3.14\n$ awk 'BEGIN{print 34.23e4}'\n342300\n```\n\nYou can also save these literals in variables for later use. Some variables are predefined, `NF` for example.\n\n```bash\n$ awk 'BEGIN{a=5; b=2.5; print a+b}'\n7.5\n\n# strings placed next to each other are concatenated\n$ awk 'BEGIN{s1=\"con\"; s2=\"cat\"; print s1 s2}'\nconcat\n```\n\nIf an uninitialized variable is used, it will act as an empty string in string context and `0` in numeric context. You can force a string to behave as a number by simply using it in an expression with numeric values. You can also use unary `+` or `-` operators. If the string doesn't start with a valid number (ignoring any starting whitespaces), it will be treated as `0`. Similarly, concatenating a string to a number will automatically change the number to string. See [gawk manual: How awk Converts Between Strings and Numbers](https://www.gnu.org/software/gawk/manual/gawk.html#Strings-And-Numbers) for more details.\n\n```bash\n# same as: awk 'BEGIN{sum=0} {sum += $NF} END{print sum}'\n$ awk '{sum += $NF} END{print sum}' table.txt\n38.14\n\n$ awk 'BEGIN{n1=\"5.0\"; n2=5; if(n1==n2) print \"equal\"}'\n$ awk 'BEGIN{n1=\"5.0\"; n2=5; if(+n1==n2) print \"equal\"}'\nequal\n$ awk 'BEGIN{n1=\"5.0\"; n2=5; if(n1==n2\".0\") print \"equal\"}'\nequal\n\n$ awk 'BEGIN{print 5 + \"abc 2 xyz\"}'\n5\n$ awk 'BEGIN{print 5 + \" \\t 2 xyz\"}'\n7\n```\n\n## Arrays\n\nArrays in `awk` are associative, meaning they are key-value pairs. The keys can be numbers or strings, but numbers get converted to strings internally. They can be multi-dimensional as well. There will be plenty of array examples in later chapters in relevant context. See [gawk manual: Arrays](https://www.gnu.org/software/gawk/manual/gawk.html#Arrays) for complete details and gotchas.\n\n```bash\n# assigning an array and accessing an element based on string keys\n$ awk 'BEGIN{student[\"id\"] = 101; student[\"name\"] = \"Joe\";\n       print student[\"name\"]}'\nJoe\n\n# checking if a key exists\n$ awk 'BEGIN{student[\"id\"] = 101; student[\"name\"] = \"Joe\";\n       if(\"id\" in student) print \"Key found\"}'\nKey found\n```\n\n## Summary\n\nIn my early days of getting used to the Linux command line, I was intimidated by `sed` and `awk` examples and didn't even try to learn them. Hopefully, this gentler introduction works for you and the various syntactical magic has been explained adequately. Try to experiment with the given examples, for example change field numbers to something other than the number used. Be curious, like what happens if a field number is negative or a floating-point number. Read the manual. Practice a lot. And so on.\n\nThe next chapter is dedicated solely for regular expressions. The features introduced in this chapter would be used in the examples, so make sure you are comfortable with `awk` syntax before proceeding. Solving the exercises to follow will help test your understanding.\n\n## Interactive exercises\n\nI wrote a TUI app to help you solve some of the exercises from this book interactively. See [AwkExercises](https://github.com/learnbyexample/TUI-apps/tree/main/AwkExercises) repo for installation steps and [app_guide.md](https://github.com/learnbyexample/TUI-apps/blob/main/AwkExercises/app_guide.md) for instructions on using this app.\n\nHere's a sample screenshot:\n\n![AwkExercises example](images/awk_exercises.png)\n\n## Exercises\n\n>![info](images/info.svg) All the exercises are also collated together in one place at [Exercises.md](https://github.com/learnbyexample/learn_gnuawk/blob/master/exercises/Exercises.md). For solutions, see [Exercise_solutions.md](https://github.com/learnbyexample/learn_gnuawk/blob/master/exercises/Exercise_solutions.md).\n\n>![info](images/info.svg) The [exercises](https://github.com/learnbyexample/learn_gnuawk/tree/master/exercises) directory has all the files used in this section.\n\n**1)** For the input file `addr.txt`, display all lines containing `is`.\n\n```bash\n$ cat addr.txt\nHello World\nHow are you\nThis game is good\nToday is sunny\n12345\nYou are funny\n\n$ awk ##### add your solution here\nThis game is good\nToday is sunny\n```\n\n**2)** For the input file `addr.txt`, display the first field of lines *not* containing `y`. Consider space as the field separator for this file.\n\n```bash\n$ awk ##### add your solution here\nHello\nThis\n12345\n```\n\n**3)** For the input file `addr.txt`, display all lines containing no more than 2 fields.\n\n```bash\n$ awk ##### add your solution here\nHello World\n12345\n```\n\n**4)** For the input file `addr.txt`, display all lines containing `is` in the second field.\n\n```bash\n$ awk ##### add your solution here\nToday is sunny\n```\n\n**5)** For each line of the input file `addr.txt`, replace the first occurrence of `o` with `0`.\n\n```bash\n$ awk ##### add your solution here\nHell0 World\nH0w are you\nThis game is g0od\nT0day is sunny\n12345\nY0u are funny\n```\n\n**6)** For the input file `table.txt`, calculate and display the product of numbers in the last field of each line. Consider space as the field separator for this file.\n\n```bash\n$ cat table.txt\nbrown bread mat hair 42\nblue cake mug shirt -7\nyellow banana window shoes 3.14\n\n$ awk ##### add your solution here\n-923.16\n```\n\n**7)** Append `.` to all the input lines for the given stdin data.\n\n```bash\n$ printf 'last\\nappend\\nstop\\ntail\\n' | awk ##### add your solution here\nlast.\nappend.\nstop.\ntail.\n```\n\n**8)** Replace all occurrences of `0xA0` with `0x50` and `0xFF` with `0x7F` for the given input file.\n\n```bash\n$ cat hex.txt\nstart address: 0xA0, func1 address: 0xA0\nend address: 0xFF, func2 address: 0xB0\n\n$ awk ##### add your solution here\nstart address: 0x50, func1 address: 0x50\nend address: 0x7F, func2 address: 0xB0\n```\n\n# Regular Expressions\n\nRegular Expressions is a versatile tool for text processing. It helps to precisely define a matching criteria. For learning and understanding purposes, one can view regular expressions as a mini-programming language in itself, specialized for text processing. Parts of a regular expression can be saved for future use, analogous to variables and functions. There are ways to perform AND, OR, NOT conditionals, features to concisely define repetition to avoid manual replication and so on.\n\nHere are some common use cases:\n\n* Sanitizing a string to ensure that it satisfies a known set of rules. For example, to check if a given string matches password rules.\n* Filtering or extracting portions on an abstract level like alphabets, digits, punctuation and so on.\n* Qualified string replacement. For example, at the start or the end of a string, only whole words, based on surrounding text, etc.\n\nThis chapter will cover regular expressions as implemented in `awk`. Most of `awk`'s regular expression syntax is similar to Extended Regular Expression (ERE) supported by `grep -E` and `sed -E`. Unless otherwise indicated, examples and descriptions will assume ASCII input.\n\n>![info](images/info.svg) See also [POSIX specification](https://pubs.opengroup.org/onlinepubs/9699919799/basedefs/V1_chap09.html) for regular expressions and [unix.stackexchange: Why does my regular expression work in X but not in Y?](https://unix.stackexchange.com/q/119905/109046) See my [blog post](https://learnbyexample.github.io/gnu-bre-ere-cheatsheet/) for differences between regexp features supported by `grep`, `sed` and `awk`.\n\n>![info](images/info.svg) The [example_files](https://github.com/learnbyexample/learn_gnuawk/tree/master/example_files) directory has all the files used in the examples.\n\n## Syntax and variable assignment\n\nAs seen in the previous chapter, the syntax is `string ~ /regexp/` to check if the given string satisfies the rules specified by the regexp. And `string !~ /regexp/` to invert the condition. By default, `$0` is checked if the string isn't specified. You can also save a regexp literal in a variable by adding `@` as a prefix. This is needed because `/regexp/` by itself would mean `$0 ~ /regexp/`.\n\n```bash\n$ printf 'spared no one\\ngrasped\\nspar\\n' | awk '/ed/'\nspared no one\ngrasped\n\n$ printf 'spared no one\\ngrasped\\nspar\\n' | awk 'BEGIN{r = @/ed/} $0 ~ r'\nspared no one\ngrasped\n```\n\n## String Anchors\n\nIn the examples seen so far, the regexp was a simple string value without any special characters. Also, the regexp pattern evaluated to `true` if it was found anywhere in the string. Instead of matching anywhere in the string, restrictions can be specified. These restrictions are made possible by assigning special meaning to certain characters and escape sequences. The characters with special meaning are known as **metacharacters** in regular expressions parlance. In case you need to match those characters literally, you need to escape them with a `\\` character (discussed in the [Matching the metacharacters](#matching-the-metacharacters) section).\n\nThere are two string anchors:\n\n* `^` metacharacter restricts the matching to the start of the string\n* `$` metacharacter restricts the matching to the end of the string\n\nBy default, `awk` processes input line by line, using a newline character as the separator. This separator won't be part of the contents in `$0` but you get back the newline when printing because the default output record separator is also a newline character. Thus, these string anchors can be considered as *line* anchors when you are processing input content line by line.\n\n```bash\n$ cat anchors.txt\nsub par\nspar\napparent effort\ntwo spare computers\ncart part tart mart\n\n# lines starting with 'sp'\n$ awk '/^sp/' anchors.txt\nspar\n\n# lines ending with 'ar'\n$ awk '/ar$/' anchors.txt\nsub par\nspar\n```\n\nBy combining these two anchors, you can restrict the matching to only whole lines. Here's an example:\n\n```bash\n# change only whole line 'spar'\n# can also use: awk '/^spar$/{$0 = 123} 1'\n# can also use: awk '$0==\"spar\"{$0 = 123} 1'\n$ printf 'spared no one\\npar\\nspar\\n' | awk '{sub(/^spar$/, \"123\")} 1'\nspared no one\npar\n123\n```\n\nThe anchors can be used by themselves as a pattern too. Helps to insert text at the start/end of a string, emulating string concatenation operations. These might not feel like useful capability, but combined with other features they become quite a handy tool.\n\n```bash\n# add '* ' at the start of every input line\n$ printf 'spared no one\\ngrasped\\nspar\\n' | awk '{gsub(/^/, \"* \")} 1'\n* spared no one\n* grasped\n* spar\n\n# append '.' only if a line doesn't contain space characters\n$ printf 'spared no one\\ngrasped\\nspar\\n' | awk '!/ /{gsub(/$/, \".\")} 1'\nspared no one\ngrasped.\nspar.\n```\n\n>![info](images/info.svg) See also the [Behavior of ^ and $ when string contains newline](#behavior-of--and--when-string-contains-newline) section.\n\n## Word Anchors\n\nThe second type of restriction is word anchors. A word character is any alphabet (irrespective of case), digit and the underscore character. You might wonder why there are digits and underscores as well, why not only alphabets? This comes from variable and function naming conventions — typically alphabets, digits and underscores are allowed. So, the definition is more programming oriented than natural language.\n\nUse `\\<` to indicate the start of word anchor and `\\>` to indicate the end of word anchor. As an alternate, you can use `\\y` to indicate both the start and end of word anchors.\n\n```bash\n$ cat anchors.txt\nsub par\nspar\napparent effort\ntwo spare computers\ncart part tart mart\n\n# words starting with 'par'\n$ awk '/\\<par/' anchors.txt\nsub par\ncart part tart mart\n\n# words ending with 'par'\n$ awk '/par\\>/' anchors.txt\nsub par\nspar\n\n# replace only whole word 'par'\n# note that only lines where the substitution succeeded will be printed\n# as the return value of sub/gsub is number of substitutions made\n$ awk 'gsub(/\\<par\\>/, \"***\")' anchors.txt\nsub ***\n```\n\n>![info](images/info.svg) Typically `\\b` is used to represent the word anchor (for example, in `grep`, `sed`, `perl`, etc), but in `awk` the escape sequence `\\b` refers to the backspace character. See also the [Word boundary differences](#word-boundary-differences) section.\n\n## Opposite Word Anchor\n\nThe `\\y` escape sequence has an opposite anchor too. `\\B` matches wherever `\\y` doesn't match. This duality will be seen later with some other escape sequences too.\n\n```bash\n# match 'par' if it is surrounded by word characters\n$ awk '/\\Bpar\\B/' anchors.txt\napparent effort\ntwo spare computers\n\n# match 'par' but not at the start of a word\n$ awk '/\\Bpar/' anchors.txt\nspar\napparent effort\ntwo spare computers\n\n# match 'par' but not at the end of a word\n$ awk '/par\\B/' anchors.txt\napparent effort\ntwo spare computers\ncart part tart mart\n```\n\nHere are some examples for using word boundaries by themselves as a pattern:\n\n```bash\n$ echo 'copper' | awk '{gsub(/\\y/, \":\")} 1'\n:copper:\n\n$ echo 'copper' | awk '{gsub(/\\B/, \":\")} 1'\nc:o:p:p:e:r\n```\n\n>![warning](images/warning.svg) Negative logic is handy in many text processing situations. But use it with care, you might end up matching things you didn't intend.\n\n## Combining conditions\n\nBefore seeing the next regexp feature, it is good to note that sometimes using logical operators is easier to read and maintain compared to doing everything with regexp.\n\n```bash\n# lines starting with 'b' and not containing 'at'\n$ awk '/^b/ && !/at/' table.txt\nblue cake mug shirt -7\n\n# first field contains 'low'\n# or, the last field value is less than 0\n$ awk '$1 ~ /low/ || $NF<0' table.txt\nblue cake mug shirt -7\nyellow banana window shoes 3.14\n```\n\n## Alternation\n\nMany a times, you'd want to search for multiple terms. In a conditional expression, you can use the logical operators to combine multiple conditions (see the previous section for examples). With regular expressions, the `|` metacharacter is similar to logical OR. The regular expression will match if any of the patterns separated by `|` is satisfied.\n\nAlternation is similar to using the `||` operator between two regexps. Having a single regexp helps to write terser code and `||` cannot be used when substitution is required.\n\n```bash\n# match whole word 'par' or string ending with 's'\n# same as: awk '/\\<par\\>/ || /s$/'\n$ awk '/\\<par\\>|s$/' anchors.txt\nsub par\ntwo spare computers\n\n# replace 'cat' or 'dog' or 'fox' with '--'\n# note the use of gsub for multiple replacements\n$ echo 'cats dog bee parrot foxed' | awk '{gsub(/cat|dog|fox/, \"--\")} 1'\n--s -- bee parrot --ed\n```\n\n## Alternation precedence\n\nThere are some tricky corner cases when using alternation. If it is used for filtering a line, there is no ambiguity. However, for use cases like substitution, it depends on a few factors. Say, you want to replace `are` or `spared` — which one should get precedence? The bigger word `spared` or the substring `are` inside it or based on something else?\n\nThe alternative which matches earliest in the input gets precedence.\n\n```bash\n# here, the output will be the same irrespective of alternation order\n# note that 'sub' is used here, so only the first match gets replaced\n$ echo 'cats dog bee parrot foxed' | awk '{sub(/bee|parrot|at/, \"--\")} 1'\nc--s dog bee parrot foxed\n$ echo 'cats dog bee parrot foxed' | awk '{sub(/parrot|at|bee/, \"--\")} 1'\nc--s dog bee parrot foxed\n```\n\nIn case of matches starting from the same location, for example `spar` and `spared`, the longest matching portion gets precedence. Unlike other regular expression implementations, left-to-right priority for alternation comes into play only if the length of the matches are the same. See [Longest match wins](#longest-match-wins) and [Backreferences](#backreferences) sections for more examples. See [regular-expressions: alternation](https://www.regular-expressions.info/alternation.html) for more information on this topic.\n\n```bash\n$ echo 'spared party parent' | awk '{sub(/spa|spared/, \"**\")} 1'\n** party parent\n$ echo 'spared party parent' | awk '{sub(/spared|spa/, \"**\")} 1'\n** party parent\n\n# other regexp flavors like Perl have left-to-right priority\n$ echo 'spared party parent' | perl -pe 's/spa|spared/**/'\n**red party parent\n```\n\n## Grouping\n\nOften, there are some common things among the regular expression alternatives. It could be common characters or qualifiers like the anchors. In such cases, you can group them using a pair of parentheses metacharacters. Similar to `a(b+c)d = abd+acd` in maths, you get `a(b|c)d = abd|acd` in regular expressions.\n\n```bash\n# without grouping\n$ printf 'red\\nreform\\nread\\narrest\\n' | awk '/reform|rest/'\nreform\narrest\n# with grouping\n$ printf 'red\\nreform\\nread\\narrest\\n' | awk '/re(form|st)/'\nreform\narrest\n\n# without grouping\n$ awk '/\\<par\\>|\\<part\\>/' anchors.txt\nsub par\ncart part tart mart\n# taking out common anchors\n$ awk '/\\<(par|part)\\>/' anchors.txt\nsub par\ncart part tart mart\n# taking out common characters as well\n# you'll later learn a better technique instead of using empty alternate\n$ awk '/\\<par(|t)\\>/' anchors.txt\nsub par\ncart part tart mart\n```\n\n## Matching the metacharacters\n\nYou have already seen a few metacharacters and escape sequences that help compose a regular expression. To match the metacharacters literally, i.e. to remove their special meaning, prefix those characters with a `\\` character. To indicate a literal `\\` character, use `\\\\`.\n\nUnlike `grep` and `sed`, the string anchors have to be always escaped to match them literally as there is no BRE mode in `awk`. They do not lose their special meaning even when not used in their customary positions.\n\n```bash\n# awk '/b^2/' will not work even though ^ isn't being used as anchor\n# b^2 will work for both grep and sed if you use BRE syntax\n$ printf 'a^2 + b^2 - C*3\\nd = c^2' | awk '/b\\^2/'\na^2 + b^2 - C*3\n\n# note that ')' doesn't need to be escaped\n$ echo '(a*b) + c' | awk '{gsub(/\\(|)/, \"\")} 1'\na*b + c\n\n$ echo '\\learn\\by\\example' | awk '{gsub(/\\\\/, \"/\")} 1'\n/learn/by/example\n```\n\n>![info](images/info.svg) Handling the replacement section metacharacters will be discussed in the [Backreferences](#backreferences) section.\n\n## Using string literal as a regexp\n\nThe first argument to the `sub` and `gsub` functions can be a string as well, which will then be converted to a regexp. This is handy in a few cases. For example, if you have many `/` characters in the search pattern, it might become easier to use a string literal instead of a regexp.\n\n```bash\n$ p='/home/learnbyexample/reports'\n$ echo \"$p\" | awk '{sub(/\\/home\\/learnbyexample\\//, \"~/\")} 1'\n~/reports\n$ echo \"$p\" | awk '{sub(\"/home/learnbyexample/\", \"~/\")} 1'\n~/reports\n\n# filtering example\n$ printf '/home/joe/1\\n/home/john/1\\n' | awk '/\\/home\\/joe\\//'\n/home/joe/1\n$ printf '/home/joe/1\\n/home/john/1\\n' | awk '$0 ~ \"/home/joe/\"'\n/home/joe/1\n```\n\nIn the above examples, the string literal was supplied directly. But any other expression or variable can be used as well, examples for which will be shown later in this chapter. The reason why string isn't always used to represent regexp is that the special meaning for the `\\` character will clash. For example:\n\n```bash\n$ awk 'gsub(\"\\<par\\>\", \"X\")' anchors.txt\nawk: cmd. line:1: warning: escape sequence `\\<' treated as plain `<'\nawk: cmd. line:1: warning: escape sequence `\\>' treated as plain `>'\n\n# you'll need \\\\ to represent a single \\\n$ awk 'gsub(\"\\\\<par\\\\>\", \"X\")' anchors.txt\nsub X\n# regexp literal is better suited in these cases\n$ awk 'gsub(/\\<par\\>/, \"X\")' anchors.txt\nsub X\n\n# another example\n$ echo '\\learn\\by\\example' | awk '{gsub(\"\\\\\\\\\", \"/\")} 1'\n/learn/by/example\n$ echo '\\learn\\by\\example' | awk '{gsub(/\\\\/, \"/\")} 1'\n/learn/by/example\n```\n\n>![info](images/info.svg) See [gawk manual: Gory details](https://www.gnu.org/software/gawk/manual/gawk.html#Gory-Details) for more information than you'd want to know.\n\n## The dot meta character\n\nThe dot metacharacter serves as a placeholder to match any character (including the newline character). Later you'll learn how to define your own custom placeholder for a limited set of characters.\n\n```bash\n# 3 character sequence starting with 'c' and ending with 't'\n$ echo 'tac tin cot abc:tyz excited' | awk '{gsub(/c.t/, \"-\")} 1'\nta-in - ab-yz ex-ed\n\n# any character followed by 3 and again any character\n$ printf '42\\t3500\\n' | awk '{gsub(/.3./, \":\")} 1'\n42:00\n\n# example to show that . matches \\n as well\n# 'c' followed by any character followed by 'x'\n$ awk 'BEGIN{s=\"abc\\nxyz\"; sub(/c.x/, \" \", s); print s}'\nab yz\n```\n\n## Quantifiers\n\nAlternation helps you match one among multiple patterns. Combining the dot metacharacter with quantifiers (and alternation if needed) paves a way to perform logical AND between patterns. For example, to check if a string matches two patterns with any number of characters in between. Quantifiers can be applied to characters, groupings and some more constructs that'll be discussed later. Apart from the ability to specify exact quantity and bounded range, these can also match unbounded varying quantities.\n\nFirst up, the `?` metacharacter which quantifies a character or group to match `0` or `1` times. This helps to define optional patterns and build terser patterns.\n\n```bash\n# same as: awk '{gsub(/\\<(fe.d|fed)\\>/, \"X\")} 1'\n$ echo 'fed fold fe:d feeder' | awk '{gsub(/\\<fe.?d\\>/, \"X\")} 1'\nX fold X feeder\n\n# same as: awk '/\\<par(|t)\\>/'\n$ awk '/\\<part?\\>/' anchors.txt\nsub par\ncart part tart mart\n\n# same as: awk '{gsub(/part|parrot/, \"X\")} 1'\n$ echo 'par part parrot parent' | awk '{gsub(/par(ro)?t/, \"X\")} 1'\npar X X parent\n# same as: awk '{gsub(/part|parrot|parent/, \"X\")} 1'\n$ echo 'par part parrot parent' | awk '{gsub(/par(en|ro)?t/, \"X\")} 1'\npar X X X\n\n# matches '<' or '\\<' and they are both replaced with '\\<'\n$ echo 'apple \\< fig ice < apple cream <' | awk '{gsub(/\\\\?</, \"\\\\<\")} 1'\napple \\< fig ice \\< apple cream \\<\n```\n\nThe `*` metacharacter quantifies a character or group to match `0` or more times.\n\n```bash\n# 'f' followed by zero or more of 'e' followed by 'd'\n$ echo 'fd fed fod fe:d feeeeder' | awk '{gsub(/fe*d/, \"X\")} 1'\nX X fod fe:d Xer\n\n# zero or more of '1' followed by '2'\n$ echo '3111111111125111142' | awk '{gsub(/1*2/, \"-\")} 1'\n3-511114-\n```\n\nThe `+` metacharacter quantifies a character or group to match `1` or more times.\n\n```bash\n# 'f' followed by one or more of 'e' followed by 'd'\n$ echo 'fd fed fod fe:d feeeeder' | awk '{gsub(/fe+d/, \"X\")} 1'\nfd X fod fe:d Xer\n\n# one or more of '1' followed by optional '4' and then '2'\n$ echo '3111111111125111142' | awk '{gsub(/1+4?2/, \"-\")} 1'\n3-5-\n```\n\nYou can specify a range of integer numbers, both bounded and unbounded, using `{}` metacharacters. There are four ways to use this quantifier as listed below:\n\n| Quantifier | Description |\n| ---------- | ----------- |\n| `{m,n}`    | match `m` to `n` times |\n| `{m,}`     | match at least `m` times |\n| `{,n}`     | match up to `n` times (including `0` times) |\n| `{n}`      | match exactly `n` times |\n\n```bash\n# note that stray characters like space are not allowed anywhere within {}\n$ echo 'ac abc abbc abbbc abbbbbbbbc' | awk '{gsub(/ab{1,4}c/, \"X\")} 1'\nac X X X abbbbbbbbc\n\n$ echo 'ac abc abbc abbbc abbbbbbbbc' | awk '{gsub(/ab{3,}c/, \"X\")} 1'\nac abc abbc X X\n\n$ echo 'ac abc abbc abbbc abbbbbbbbc' | awk '{gsub(/ab{,2}c/, \"X\")} 1'\nX X X abbbc abbbbbbbbc\n\n$ echo 'ac abc abbc abbbc abbbbbbbbc' | awk '{gsub(/ab{3}c/, \"X\")} 1'\nac abc abbc X abbbbbbbbc\n```\n\n>![info](images/info.svg) The `{}` metacharacters have to be escaped to match them literally. Similar to the `()` metacharacters, escaping `{` alone is enough. If it doesn't conform strictly to any of the four forms listed above, escaping is not needed at all.\n>\n> ```bash\n> $ echo 'a{5} = 10' | awk '{sub(/a\\{5}/, \"x\")} 1'\n> x = 10\n> $ echo 'report_{a,b}.txt' | awk '{sub(/_{a,b}/, \"_c\")} 1'\n> report_c.txt\n> ```\n\n## Conditional AND\n\nNext up, how to construct conditional AND using dot metacharacter and quantifiers.\n\n```bash\n# match 'Error' followed by zero or more characters followed by 'valid'\n$ echo 'Error: not a valid input' | awk '/Error.*valid/'\nError: not a valid input\n```\n\nTo allow matching in any order, you'll have to bring in alternation as well.\n\n```bash\n# 'cat' followed by 'dog' or 'dog' followed by 'cat'\n$ echo 'two cats and a dog' | awk '{gsub(/cat.*dog|dog.*cat/, \"pets\")} 1'\ntwo pets\n$ echo 'two dogs and a cat' | awk '{gsub(/cat.*dog|dog.*cat/, \"pets\")} 1'\ntwo pets\n```\n\n## Longest match wins\n\nYou've already seen an example where the longest matching portion was chosen if the alternatives started from the same location. For example `spar|spared` will result in `spared` being chosen over `spar`. The same applies whenever there are two or more matching possibilities from the same starting location. For example, `f.?o` will match `foo` instead of `fo` if the input string to match is `foot`.\n\n```bash\n# longest match among 'foo' and 'fo' wins here\n$ echo 'foot' | awk '{sub(/f.?o/, \"X\")} 1'\nXt\n# everything will match here\n$ echo 'car bat cod map scat dot abacus' | awk '{sub(/.*/, \"X\")} 1'\nX\n\n# longest match happens when (1|2|3)+ matches up to '1233' only\n# so that '12apple' can match as well\n$ echo 'fig123312apple' | awk '{sub(/g(1|2|3)+(12apple)?/, \"X\")} 1'\nfiX\n# in other implementations like Perl, that is not the case\n# precedence is left-to-right for greedy quantifiers\n$ echo 'fig123312apple' | perl -pe 's/g(1|2|3)+(12apple)?/X/'\nfiXapple\n```\n\nWhile determining the longest match, the overall regular expression matching is also considered. That's how the `Error.*valid` example worked. If `.*` had consumed everything after `Error`, there wouldn't be any more characters to try to match `valid`. So, among the varying quantity of characters to match for `.*`, the longest portion that satisfies the overall regular expression is chosen. Something like `a.*b` will match from the first `a` in the input string to the last `b`. In other implementations, like Perl, this is achieved through a process called **backtracking**. These approaches have their own advantages and disadvantages and have cases where the pattern can result in exponential time consumption.\n\n```bash\n# from the start of line to the last 'b' in the line\n$ echo 'car bat cod map scat dot abacus' | awk '{sub(/.*b/, \"-\")} 1'\n-acus\n\n# from the first 'b' to the last 't' in the line\n$ echo 'car bat cod map scat dot abacus' | awk '{sub(/b.*t/, \"-\")} 1'\ncar - abacus\n\n# from the first 'b' to the last 'at' in the line\n$ echo 'car bat cod map scat dot abacus' | awk '{sub(/b.*at/, \"-\")} 1'\ncar - dot abacus\n\n# here 'm*' will match 'm' zero times as that gives the longest match\n$ echo 'car bat cod map scat dot abacus' | awk '{sub(/a.*m*/, \"-\")} 1'\nc-\n```\n\n## Character classes\n\nTo create a custom placeholder for limited set of characters, enclose them inside `[]` metacharacters. It is similar to using single character alternations inside a grouping, but with added flexibility and features. Character classes have their own versions of metacharacters and provide special predefined sets for common use cases. Quantifiers are also applicable to character classes.\n\n```bash\n# same as: awk '/cot|cut/' and awk '/c(o|u)t/'\n$ printf 'cute\\ncat\\ncot\\ncoat\\ncost\\nscuttle\\n' | awk '/c[ou]t/'\ncute\ncot\nscuttle\n\n# same as: awk '/.(a|e|o)t/'\n$ printf 'meeting\\ncute\\nboat\\nat\\nfoot\\n' | awk '/.[aeo]t/'\nmeeting\nboat\nfoot\n\n# same as: awk '{gsub(/\\<(s|o|t)(o|n)\\>/, \"X\")} 1'\n$ echo 'no so in to do on' | awk '{gsub(/\\<[sot][on]\\>/, \"X\")} 1'\nno X in X do X\n\n# lines made up of letters 'o' and 'n', line length at least 2\n# words.txt contains dictionary words, one word per line\n$ awk '/^[on]{2,}$/' words.txt\nno\nnon\nnoon\non\n```\n\n## Character class metacharacters\n\nCharacter classes have their own metacharacters to help define the sets succinctly. Metacharacters outside of character classes like `^`, `$`, `()` etc either don't have special meaning or have a completely different one inside the character classes.\n\nFirst up, the `-` metacharacter that helps to define a range of characters instead of having to specify them all individually.\n\n```bash\n# same as: awk '{gsub(/[0123456789]+/, \"-\")} 1'\n$ echo 'Sample123string42with777numbers' | awk '{gsub(/[0-9]+/, \"-\")} 1'\nSample-string-with-numbers\n\n# whole words made up of lowercase alphabets and digits only\n$ echo 'coat Bin food tar12 best' | awk '{gsub(/\\<[a-z0-9]+\\>/, \"X\")} 1'\nX Bin X X X\n\n# whole words made up of lowercase alphabets, starting with 'p' to 'z'\n$ echo 'road i post grip read eat pit' | awk '{gsub(/\\<[p-z][a-z]*\\>/, \"X\")} 1'\nX i X grip X eat X\n```\n\nCharacter classes can also be used to construct numeric ranges. However, it is easy to miss corner cases and some ranges are complicated to design.\n\n```bash\n# numbers between 10 to 29\n$ echo '23 154 12 26 34' | awk '{gsub(/\\<[12][0-9]\\>/, \"X\")} 1'\nX 154 X X 34\n\n# numbers >= 100 with optional leading zeros\n$ echo '0501 035 154 12 26 98234' | awk '{gsub(/\\<0*[1-9][0-9]{2,}\\>/, \"X\")} 1'\nX 035 X 12 26 X\n```\n\nNext metacharacter is `^` which has to be specified as the first character of the character class. It negates the set of characters, so all characters other than those specified will be matched. As highlighted earlier, handle negative logic with care, you might end up matching more than you wanted.\n\n```bash\n# replace all non-digit characters\n$ echo 'Sample123string42with777numbers' | awk '{gsub(/[^0-9]+/, \"-\")} 1'\n-123-42-777-\n\n# delete last two columns\n$ echo 'apple:123:banana:cherry' | awk '{sub(/(:[^:]+){2}$/, \"\")} 1'\napple:123\n\n# sequence of characters surrounded by a unique character\n$ echo 'I like \"mango\" and \"guava\"' | awk '{gsub(/\"[^\"]+\"/, \"X\")} 1'\nI like X and X\n\n# sometimes it is simpler to positively define a set than negation\n# same as: awk '/^[^aeiou]*$/'\n$ printf 'tryst\\nfun\\nglyph\\npity\\nwhy\\n' | awk '!/[aeiou]/'\ntryst\nglyph\nwhy\n```\n\nSome commonly used character sets have predefined escape sequences:\n\n* `\\w` matches all **word** characters `[a-zA-Z0-9_]` (recall the description for word boundaries)\n* `\\W` matches all non-word characters (recall duality seen earlier, like `\\y` and `\\B`)\n* `\\s` matches all **whitespace** characters: tab, newline, vertical tab, form feed, carriage return and space\n* `\\S` matches all non-whitespace characters\n\nThese escape sequences *cannot* be used inside character classes. Also, as mentioned earlier, these definitions assume ASCII input.\n\n```bash\n# match all non-word characters\n$ echo 'load;err_msg--\\/ant,r2..not' | awk '{gsub(/\\W+/, \"|\")} 1'\nload|err_msg|ant|r2|not\n\n# replace all sequences of whitespaces with a single space\n$ printf 'hi  \\v\\f  there.\\thave   \\ra nice\\t\\tday\\n' | awk '{gsub(/\\s+/, \" \")} 1'\nhi there. have a nice day\n\n# \\w would simply match w inside character classes\n$ echo 'w=y\\x+9*3' | awk '{gsub(/[\\w=]/, \"\")} 1'\ny\\x+9*3\n```\n\n>![warning](images/warning.svg) `awk` doesn't support `\\d` and `\\D`, commonly featured in other implementations as a shortcut for all the digits and non-digits.\n>\n> ```bash\n> # \\d will match just the 'd' character and produces a warning as well\n> $ echo '42\\d123' | awk '{gsub(/\\d+/, \"-\")} 1'\n> awk: cmd. line:1: warning: regexp escape sequence\n>                   '\\d' is not a known regexp operator\n> 42\\-123\n>\n> # \\d here matches all digit characters\n> $ echo '42\\d123' | perl -pe 's/\\d+/-/g'\n> -\\d-\n> ```\n\n## Named character sets\n\nA named character set is defined by a name enclosed between `[:` and `:]` and has to be used within a character class `[]`, along with other characters as needed.\n\n| Named set    | Description |\n| ------------ | ----------- |\n| `[:digit:]`  | `[0-9]` |\n| `[:lower:]`  | `[a-z]` |\n| `[:upper:]`  | `[A-Z]` |\n| `[:alpha:]`  | `[a-zA-Z]` |\n| `[:alnum:]`  | `[0-9a-zA-Z]` |\n| `[:xdigit:]` | `[0-9a-fA-F]` |\n| `[:cntrl:]`  | control characters — first 32 ASCII characters and 127th (DEL) |\n| `[:punct:]`  | all the punctuation characters |\n| `[:graph:]`  | `[:alnum:]` and `[:punct:]` |\n| `[:print:]`  | `[:alnum:]`, `[:punct:]` and space |\n| `[:blank:]`  | space and tab characters |\n| `[:space:]`  | whitespace characters, same as `\\s` |\n\nHere are some examples:\n\n```bash\n$ s='err_msg xerox ant m_2 P2 load1 eel'\n$ echo \"$s\" | awk '{gsub(/\\<[[:lower:]]+\\>/, \"X\")} 1'\nerr_msg X X m_2 P2 load1 X\n\n$ echo \"$s\" | awk '{gsub(/\\<[[:lower:]_]+\\>/, \"X\")} 1'\nX X X m_2 P2 load1 X\n\n$ echo \"$s\" | awk '{gsub(/\\<[[:alnum:]]+\\>/, \"X\")} 1'\nerr_msg X X m_2 X X X\n\n# retain only punctuation characters\n$ echo ',pie tie#ink-eat_42' | awk '{gsub(/[^[:punct:]]+/, \"\")} 1'\n,#-_\n```\n\n## Matching character class metacharacters literally\n\nSpecific placement is needed to match character class metacharacters literally. Or, they can be escaped by prefixing `\\` to avoid having to remember the different rules. As `\\` is special inside character class, use `\\\\` to represent it literally.\n\n`-` should be the first or the last character.\n\n```bash\n$ echo 'ab-cd gh-c 12-423' | awk '{gsub(/[a-z-]{2,}/, \"X\")} 1'\nX X 12-423\n\n# or escaped with \\\n$ echo 'ab-cd gh-c 12-423' | awk '{gsub(/[a-z\\-0-9]{2,}/, \"X\")} 1'\nX X X\n```\n\n`]` should be the first character.\n\n```bash\n# no match\n$ printf 'int a[5]\\nfig\\n1+1=2\\n' | awk '/[=]]/'\n\n# correct usage\n$ printf 'int a[5]\\nfig\\n1+1=2\\n' | awk '/[]=]/'\nint a[5]\n1+1=2\n```\n\n`[` can be used anywhere in the character set. Using `[][]` will match both `[` and `]`.\n\n```bash\n$ echo 'int a[5].y' | awk '{gsub(/[x[y.]/, \"\")} 1'\nint a5]\n\n$ printf 'int a[5]\\nfig\\n1+1=2\\nwho]' | awk '/[][]/'\nint a[5]\nwho]\n```\n\n`^` should be other than the first character.\n\n```bash\n$ echo 'f*(a^b) - 3*(a+b)/(a-b)' | awk '{gsub(/a[+^]b/, \"c\")} 1'\nf*(c) - 3*(c)/(a-b)\n```\n\n>![warning](images/warning.svg) Combinations like `[.` or `[:` cannot be used together to mean two individual characters, as they have special meaning within `[]`. See [gawk manual: Using Bracket Expressions](https://www.gnu.org/software/gawk/manual/gawk.html#Bracket-Expressions) for more details.\n>\n> ```bash\n> $ echo 'int a[5]' | awk '/[x[.y]/'\n> awk: cmd. line:1: error: Unmatched [, [^, [:, [., or [=: /[x[.y]/\n> $ echo 'int a[5]' | awk '/[x[y.]/'\n> int a[5]\n> ```\n\n## Escape sequences\n\nCertain ASCII characters like tab `\\t`, carriage return `\\r`, newline `\\n`, etc have escape sequences to represent them. Additionally, any character can be represented using their ASCII value in octal `\\NNN` or hexadecimal `\\xNN` formats. Unlike character set escape sequences like `\\w`, these can be used inside character classes.\n\n```bash\n# \\t represents the tab character\n$ printf 'apple\\tbanana\\tcherry\\n' | awk '{gsub(/\\t/, \" \")} 1'\napple banana cherry\n\n# these escape sequences work inside character class too\n$ printf 'a\\t\\r\\fb\\vc\\n' | awk '{gsub(/[\\t\\v\\f\\r]+/, \":\")} 1'\na:b:c\n\n# representing single quotes\n# use \\047 for octal format\n$ echo \"universe: '42'\" | awk '{gsub(/\\x27/, \"\")} 1'\nuniverse: 42\n```\n\nIf a metacharacter is specified using the ASCII value format, it will still act as the metacharacter.\n\n```bash\n# \\x5e is ^ character, acts as line anchor here\n$ printf 'acorn\\ncot\\ncat\\ncoat\\n' | awk '/\\x5eco/'\ncot\ncoat\n\n# & metacharacter in replacement will be discussed in a later section\n# it represents the entire matched portion\n$ echo 'hello world' | awk '{sub(/.*/, \"[&]\")} 1'\n[hello world]\n# \\x26 in hexadecimal is the & character\n$ echo 'hello world' | awk '{sub(/.*/, \"[\\x26]\")} 1'\n[hello world]\n```\n\nUndefined sequences will result in a warning and treated as the character it escapes.\n\n```bash\n$ echo 'read' | awk '{sub(/\\d/, \"l\")} 1'\nawk: cmd. line:1: warning: regexp escape sequence\n                  '\\d' is not a known regexp operator\nreal\n```\n\n>![info](images/info.svg) See [gawk manual: Escape Sequences](https://www.gnu.org/software/gawk/manual/gawk.html#Escape-Sequences) for full list and other details.\n\n## Replace specific occurrence\n\nThe third substitution function is `gensub` which can be used instead of both the `sub` and `gsub` functions. Syntax wise, `gensub` needs minimum three arguments. The third argument is used to indicate whether you want to replace all occurrences with `\"g\"` or a specific occurrence by passing a number. Another difference is that `gensub` returns a string value (irrespective of the substitution operation succeeding) instead of modifying the input.\n\n```bash\n$ s='apple:banana:cherry:fig:mango'\n\n# same as: sed 's/:/-/2'\n# replace only the second occurrence of ':' with '-'\n# note that the output of gensub is passed to print here\n$ echo \"$s\" | awk '{print gensub(/:/, \"-\", 2)}'\napple:banana-cherry:fig:mango\n\n# same as: sed -E 's/[^:]+/X/3'\n# replace only the third field with '123'\n$ echo \"$s\" | awk '{print gensub(/[^:]+/, \"123\", 3)}'\napple:banana:123:fig:mango\n```\n\nThe fourth argument for the `gensub` function allows you to specify a string or a variable on which the substitution has to be performed. Default is `$0`, as seen in the previous examples.\n\n```bash\n# same as: awk '{gsub(/[aeiou]/, \"X\", $4)} 1'\n$ echo '1 good 2 apples' | awk '{$4 = gensub(/[aeiou]/, \"X\", \"g\", $4)} 1'\n1 good 2 XpplXs\n```\n\n## Backreferences\n\nThe grouping metacharacters `()` are also known as **capture groups**. Similar to variables in programming languages, the portion captured by `()` can be referred later using backreferences. The syntax is `\\N` where `N` is the capture group you want. Leftmost `(` in the regular expression is `\\1`, next one is `\\2` and so on up to `\\9`. The `&` metacharacter represents entire matched string. As `\\` is already special inside double quotes, you'll have to use `\"\\\\1\"` to represent `\\1`.\n\n>![info](images/info.svg) Backreferences of the form `\\N` can only be used with the `gensub` function. `&` can be used with `sub`, `gsub` and `gensub` functions. `\\0` can also be used instead of `&` with the `gensub` function.\n\n```bash\n# reduce \\\\ to single \\ and delete if it is a single \\\n$ s='\\[\\] and \\\\w and \\[a-zA-Z0-9\\_\\]'\n$ echo \"$s\" | awk '{print gensub(/(\\\\?)\\\\/, \"\\\\1\", \"g\")}'\n[] and \\w and [a-zA-Z0-9_]\n\n# duplicate the first column value and add it as the final column\n$ echo 'one,2,3.14,42' | awk '{print gensub(/^([^,]+).*/, \"&,\\\\1\", 1)}'\none,2,3.14,42,one\n\n# add something at the start and end of string, gensub isn't needed here\n$ echo 'hello world' | awk '{sub(/.*/, \"Hi. &. Have a nice day\")} 1'\nHi. hello world. Have a nice day\n\n# here {N} refers to the last but Nth occurrence\n$ s='car,art,pot,tap,urn,ray,ear'\n$ echo \"$s\" | awk '{print gensub(/(.*),((.*,){2})/, \"\\\\1[]\\\\2\", 1)}'\ncar,art,pot,tap[]urn,ray,ear\n```\n\n>![warning](images/warning.svg) See [unix.stackexchange: Why doesn't this sed command replace the 3rd-to-last \"and\"?](https://unix.stackexchange.com/q/579889/109046) for a bug related to the use of word anchors in the `((pat){N})` generic case.\n\n>![warning](images/warning.svg) Unlike other regular expression implementations, like `grep` or `sed` or `perl`, backreferences cannot be used in the search section in `awk`. See also [unix.stackexchange: backreference in awk](https://unix.stackexchange.com/q/361427/109046).\n>\n> ```bash\n> $ s='effort flee facade oddball rat tool'\n>\n> # no change\n> $ echo \"$s\" | awk '{gsub(/\\w*(\\w)\\1\\w*/, \"X\")} 1'\n> effort flee facade oddball rat tool\n> # whole words that have at least one consecutive repeated character\n> $ echo \"$s\" | sed -E 's/\\w*(\\w)\\1\\w*/X/g'\n> X X facade X rat X\n> ```\n\nIf a quantifier is applied on a pattern grouped inside `()` metacharacters, you'll need an outer `()` group to capture the matching portion. Other flavors like Perl provide non-capturing groups to handle such cases. In `awk` you'll have to consider the extra capture groups.\n\n```bash\n# note the numbers used in the replacement section\n$ s='one,2,3.14,42'\n$ echo \"$s\" | awk '{$0=gensub(/^(([^,]+,){2})([^,]+)/, \"[\\\\1](\\\\3)\", 1)} 1'\n[one,2,](3.14),42\n```\n\nHere's an example where alternation order matters when the matching portions have the same length. Aim is to delete all whole words unless it starts with `g` or `p` and contains `y`.\n\n```bash\n$ s='tryst,fun,glyph,pity,why,group'\n\n# all words get deleted because \\<\\w+\\> gets priority here\n$ echo \"$s\" | awk '{print gensub(/\\<\\w+\\>|(\\<[gp]\\w*y\\w*\\>)/, \"\\\\1\", \"g\")}'\n,,,,,\n\n# capture group gets priority here, so words in the capture group are retained\n$ echo \"$s\" | awk '{print gensub(/(\\<[gp]\\w*y\\w*\\>)|\\<\\w+\\>/, \"\\\\1\", \"g\")}'\n,,glyph,pity,,\n```\n\nAs `\\` and `&` are special characters in the replacement section, you'll need to escape them for literal representation.\n\n```bash\n$ echo 'apple and fig' | awk '{sub(/and/, \"[&]\")} 1'\napple [and] fig\n$ echo 'apple and fig' | awk '{sub(/and/, \"[\\\\&]\")} 1'\napple [&] fig\n\n$ echo 'apple and fig' | awk '{sub(/and/, \"\\\\\")} 1'\napple \\ fig\n```\n\n## Case insensitive matching\n\nUnlike `sed` or `perl`, regular expressions in `awk` do not directly support the use of flags to change certain behaviors. For example, there is no flag to force the regexp to ignore case while matching.\n\nThe `IGNORECASE` special variable controls case sensitivity, which is `0` by default. By changing it to some other value (which would mean `true` in a conditional expression), you can match case insensitively. The `-v` command line option allows you to assign a variable before input is read. The `BEGIN` block is also often used to change such settings.\n\n```bash\n$ printf 'Cat\\ncOnCaT\\nscatter\\ncot\\n' | awk -v IGNORECASE=1 '/cat/'\nCat\ncOnCaT\nscatter\n\n# for small enough string, you can also use character class\n$ printf 'Cat\\ncOnCaT\\nscatter\\ncot\\n' | awk '{gsub(/[cC][aA][tT]/, \"(&)\")} 1'\n(Cat)\ncOn(CaT)\ns(cat)ter\ncot\n```\n\nAnother way is to use built-in string function `tolower` to change the input to lowercase first.\n\n```bash\n$ printf 'Cat\\ncOnCaT\\nscatter\\ncot\\n' | awk 'tolower($0) ~ /cat/'\nCat\ncOnCaT\nscatter\n```\n\n## Dynamic regexp\n\nAs seen earlier, string literals can be used instead of a regexp to specify the pattern to be matched. Which implies that you can use any expression or a variable as well. This is helpful if you need to compute the regexp based on some conditions or if you are getting the pattern externally, such as user input passed via the `-v` option from a `bash` variable.\n\n```bash\n$ r='cat.*dog|dog.*cat'\n$ echo 'two cats and a dog' | awk -v ip=\"$r\" '{gsub(ip, \"pets\")} 1'\ntwo pets\n\n$ awk -v s='ow' '$0 ~ s' table.txt\nbrown bread mat hair 42\nyellow banana window shoes 3.14\n\n# you'll have to make sure to use \\\\ instead of \\\n$ r='\\\\<[12][0-9]\\\\>'\n$ echo '23 154 12 26 34' | awk -v ip=\"$r\" '{gsub(ip, \"X\")} 1'\nX 154 X X 34\n```\n\n>![info](images/info.svg) See [Using shell variables](#using-shell-variables) chapter for a way to avoid having to escape backslashes.\n\nSometimes, user input has to be treated literally instead of as a regexp pattern. In such cases, you'll need to escape all the regexp metacharacters. Below example shows how to do it for the search section. For the replace section, you only have to escape the `\\` and `&` characters.\n\n```bash\n$ awk -v s='(a.b)^{c}|d' 'BEGIN{gsub(/[{[(^$*?+.|\\\\]/, \"\\\\\\\\&\", s); print s}'\n\\(a\\.b)\\^\\{c}\\|d\n\n$ echo 'f*(a^b) - 3*(a^b)' |\n     awk -v s='(a^b)' '{gsub(/[{[(^$*?+.|\\\\]/, \"\\\\\\\\&\", s); gsub(s, \"c\")} 1'\nf*c - 3*c\n\n# match given input string literally, but only at the end of string\n$ echo 'f*(a^b) - 3*(a^b)' |\n     awk -v s='(a^b)' '{gsub(/[{[(^$*?+.|\\\\]/, \"\\\\\\\\&\", s); gsub(s \"$\", \"c\")} 1'\nf*(a^b) - 3*c\n```\n\n>![info](images/info.svg) See [my blog post](https://learnbyexample.github.io/escaping-madness-awk-literal-field-separator/) for more details about escaping metacharacters.\n\n>![info](images/info.svg) If you need to just match literally instead of substitution, you can use the `index` function. See the [index](#index) section for details.\n\n## Summary\n\nRegular expressions is a feature that you'll encounter in multiple command line programs and programming languages. It is a versatile tool for text processing. Although the features in `awk` are less compared to those found in programming languages, they are sufficient for most of the tasks you'll need for command line usage. It takes a lot of time to get used to syntax and features of regular expressions, so I'll encourage you to practice a lot and maintain notes. It'd also help to consider it as a mini-programming language in itself for its flexibility and complexity.\n\n## Exercises\n\n>![info](images/info.svg) The [exercises](https://github.com/learnbyexample/learn_gnuawk/tree/master/exercises) directory has all the files used in this section.\n\n**1)** For the input file `patterns.txt`, display all lines that start with `den` or end with `ly`.\n\n```bash\n$ awk ##### add your solution here\n2 lonely\ndent\nlovely\n```\n\n**2)** For the input file `patterns.txt`, replace all occurrences of `42` with `[42]` unless it is at the edge of a word. Display only the modified lines.\n\n```bash\n$ awk ##### add your solution here\nHi[42]Bye nice1[42]3 bad42\neqn2 = pressure*3+42/5-1[42]56\ncool_[42]a 42fake\n_[42]_\n```\n\n**3)** For the input file `patterns.txt`, add `[]` around words starting with `s` and containing `e` and `t` in any order. Display only the modified lines.\n\n```bash\n$ awk ##### add your solution here\n[sets] tests Sauerkraut\n[site] cite kite bite [store_2]\n[subtle] sequoia\na [set]\n```\n\n**4)** For the input file `patterns.txt`, replace the space character that occurs after a word ending with `a` or `r` with a newline character, only if the line also contains an uppercase letter. Display only the modified lines. For example, `A car park` should get converted to `A car` and `park` separated by a newline. But `car far tar` shouldn't be matched as there's no uppercase letter in this line.\n\n```bash\n$ awk ##### add your solution here\npar\ncar\ntar\nfar\nCart\nNot a\npip DOWN\n```\n\n**5)** For the input file `patterns.txt`, replace all occurrences of `*[5]` with `2`. Display only the modified lines.\n\n```bash\n$ awk ##### add your solution here\n(9-2)2\n```\n\n**6)** `awk '/\\<[a-z](on|no)[a-z]\\>/'` is same as `awk '/\\<[a-z][on]{2}[a-z]\\>/'`. True or False? Sample input shown below might help to understand the differences, if any.\n\n```bash\n$ printf 'known\\nmood\\nknow\\npony\\ninns\\n'\nknown\nmood\nknow\npony\ninns\n```\n\n**7)** For the input file `patterns.txt`, display all lines starting with `hand` and ending immediately with `s` or `y` or `le` or no further characters. For example, `handed` shouldn't be matched even though it starts with `hand`.\n\n```bash\n$ awk ##### add your solution here\nhandle\nhandy\nhands\nhand\n```\n\n**8)** For the input file `patterns.txt`, replace `42//5` or `42/5` with `8`. Display only the modified lines.\n\n```bash\n$ awk ##### add your solution here\neqn3 = r*42-5/3+42///5-83+a\neqn1 = a+8-c\neqn2 = pressure*3+8-14256\n```\n\n**9)** For the given quantifiers, what would be the equivalent form using the `{m,n}` representation?\n\n* `?` is same as\n* `*` is same as\n* `+` is same as\n\n**10)** True or False? `(a*|b*)` is same as `(a|b)*` \n\n**11)** For the input file `patterns.txt`, construct two different regexps to get the outputs as shown below. Display only the modified lines.\n\n```bash\n# delete from '(' till the next ')'\n$ awk ##### add your solution here\na/b + c%d\n*[5]\ndef factorial\n12- *4)\nHi there. Nice day\n\n# delete from '(' till the next ')' but not if there is '(' in between\n$ awk ##### add your solution here\na/b + c%d\n*[5]\ndef factorial\n12- (e+*4)\nHi there. Nice day(a\n```\n\n**12)** For the input file `anchors.txt`, convert markdown anchors to corresponding hyperlinks as shown below.\n\n```bash\n$ cat anchors.txt\n# <a name=\"regular-expressions\"></a>Regular Expressions\n## <a name=\"subexpression-calls\"></a>Subexpression calls\n## <a name=\"the-dot-meta-character\"></a>The dot meta character\n\n$ awk ##### add your solution here\n[Regular Expressions](#regular-expressions)\n[Subexpression calls](#subexpression-calls)\n[The dot meta character](#the-dot-meta-character)\n```\n\n**13)** Display lines from `sample.txt` that satisfy both of these conditions:\n\n* `to` or `he` matched irrespective of case\n* `World` or `No` matched case sensitively\n\n```bash\n$ awk ##### add your solution here\nHello World\nNo doubt you like it too\n```\n\n**14)** Given sample strings have fields separated by `,` and field values cannot be empty. Replace the third field with `42`.\n\n```bash\n$ echo 'lion,ant,road,neon' | awk ##### add your solution here\nlion,ant,42,neon\n\n$ echo '_;3%,.,=-=,:' | awk ##### add your solution here\n_;3%,.,42,:\n```\n\n**15)** For the input file `patterns.txt`, filter lines containing three or more occurrences of `ar` and replace the last but second `ar` with `X`.\n\n```bash\n$ awk ##### add your solution here\npar car tX far Cart\npXt cart mart\n```\n\n**16)** Surround all whole words with `()`. Additionally, if the whole word is `imp` or `ant`, delete them.\n\n```bash\n$ words='tiger imp goat eagle ant important'\n$ echo \"$words\" | awk ##### add your solution here\n(tiger) () (goat) (eagle) () (important)\n```\n\n**17)** For the input file `patterns.txt`, display lines containing `car` but not as a whole word. For example, `scared-cat` and `car care` should match but not `far car park`.\n\n```bash\n$ awk ##### add your solution here\nscar\ncare\na huge discarded pile of books\nscare\npart cart mart\n```\n\n**18)** Will the pattern `^a\\w+([0-9]+:fig)?` match the same characters for the input `apple42:banana314` and `apple42:fig100`? If not, why not?\n\n**19)** For the input file `patterns.txt`, display lines starting with `4` or `-` or `u` or `sub` or `care`.\n\n```bash\n$ awk ##### add your solution here\ncare\n4*5]\n-handy\nsubtle sequoia\nunhand\n```\n\n**20)** Replace sequences made up of words separated by `:` or `.` by the first word of the sequence. Such sequences will end when `:` or `.` is not followed by a word character.\n\n```bash\n$ ip='wow:Good:2_two.five: hi-2 bye kite.777:water.'\n$ echo \"$ip\" | awk ##### add your solution here\nwow hi-2 bye kite\n```\n\n**21)** Replace sequences made up of words separated by `:` or `.` by the last word of the sequence. Such sequences will end when `:` or `.` is not followed by a word character.\n\n```bash\n$ ip='wow:Good:2_two.five: hi-2 bye kite.777:water.'\n$ echo \"$ip\" | awk ##### add your solution here\nfive hi-2 bye water\n```\n\n**22)** Replace all whole words with `X` unless it is preceded by a `(` character.\n\n```bash\n$ s='guava (apple) berry) apple (mango) (grape'\n$ echo \"$s\" | awk ##### add your solution here\nX (apple) X) X (mango) (grape\n```\n\n**23)** Surround whole words with `[]` only if they are followed by `:` or `,` or `-`.\n\n```bash\n$ ip='Poke,on=-=so_good:ink.to/is(vast)ever2-sit'\n$ echo \"$ip\" | awk ##### add your solution here\n[Poke],on=-=[so_good]:ink.to/is(vast)[ever2]-sit\n```\n\n**24)** The `fields.txt` file has fields separated by the `:` character. Delete `:` and the last field if there is a digit character anywhere before the last field.\n\n```bash\n$ cat fields.txt\n42:cat\ntwelve:a2b\nwe:be:he:0:a:b:bother\napple:banana-42:cherry:\ndragon:unicorn:centaur\n\n$ awk ##### add your solution here\n42\ntwelve:a2b\nwe:be:he:0:a:b\napple:banana-42:cherry\ndragon:unicorn:centaur\n```\n\n**25)** Can you use a character other than `/` as the regexp delimiter? If not, are there ways to construct a regexp that do not require the `/` character to be escaped for literal matching?\n\n**26)** For the input file `patterns.txt`, surround all hexadecimal sequences with a minimum of four characters with `[]`. Match `0x` as an optional prefix, but shouldn't be counted for determining the length. Match the characters case insensitively, and the sequences shouldn't be surrounded by other word characters. Display only the modified lines.\n\n```bash\n$ awk ##### add your solution here\n\"should not match [0XdeadBEEF]\"\nHi42Bye nice1423 [bad42]\ntook 0xbad 22 [0x0ff1ce]\neqn2 = pressure*3+42/5-[14256]\n```\n\n# Field separators\n\nNow that you are familiar with basic `awk` syntax and regular expressions, this chapter will dive deep into field processing. You'll learn how to set input and output field separators, how to use regexps for defining fields and how to work with fixed length fields. \n\n>![info](images/info.svg) The [example_files](https://github.com/learnbyexample/learn_gnuawk/tree/master/example_files) directory has all the files used in the examples.\n\n## Default field separation\n\nAs seen earlier, `awk` automatically splits input into fields which are accessible using `$N` where `N` is the field number you need. You can also pass an expression instead of a numeric literal to specify the field required.\n\n```bash\n$ cat table.txt\nbrown bread mat hair 42\nblue cake mug shirt -7\nyellow banana window shoes 3.14\n\n# print the fourth field if the first field starts with 'b'\n$ awk '$1 ~ /^b/{print $4}' table.txt\nhair\nshirt\n\n# print the field as specified by the value stored in the 'f' variable\n$ awk -v f=3 '{print $f}' table.txt\nmat\nmug\nwindow\n```\n\nThe `NF` special variable will give you the number of fields for each input line. This is useful when you don't know how many fields are present in the input and you need to process fields from the end.\n\n```bash\n# print the last field of each input line\n$ awk '{print $NF}' table.txt\n42\n-7\n3.14\n\n# print the last but one field\n$ awk '{print $(NF-1)}' table.txt\nhair\nshirt\nshoes\n\n# don't forget the parentheses!\n# this will subtract 1 from the last field and print it\n$ awk '{print $NF-1}' table.txt\n41\n-8\n2.14\n```\n\nBy default, `awk` does more than split the input on spaces. It splits based on one or more sequence of **space** or **tab** or **newline** characters. In addition, any of these three characters at the start or end of input gets trimmed and won't be part of the field contents. Input containing newline characters will be covered in the [Record separators](#record-separators) chapter.\n\n```bash\n$ echo '   a   b   c   ' | awk '{print NF}'\n3\n# note that the leading spaces aren't part of the field content\n$ echo '   a   b   c   ' | awk '{print $1}'\na\n# note that the trailing spaces aren't part of the field content\n$ echo '   a   b   c   ' | awk '{print $NF \".\"}'\nc.\n\n# here's another example with tab characters thrown in\n$ printf '     one \\t two\\t\\t\\tthree  ' | awk '{print NF}'\n3\n$ printf '     one \\t two\\t\\t\\tthree  ' | awk '{print $2 \".\"}'\ntwo.\n```\n\n>![warning](images/warning.svg) When passing an expression for field number, floating-point result is acceptable too. The fractional portion is ignored. However, as precision is limited, it could result in rounding instead of truncation.\n>\n> ```bash\n> $ awk 'BEGIN{printf \"%.16f\\n\", 2.999999999999999}'\n> 2.9999999999999991\n> $ awk 'BEGIN{printf \"%.16f\\n\", 2.9999999999999999}'\n> 3.0000000000000000\n>\n> # same as: awk '{print $2}' table.txt\n> $ awk '{print $2.999999999999999}' table.txt\n> bread\n> cake\n> banana\n>\n> # same as: awk '{print $3}' table.txt\n> $ awk '{print $2.9999999999999999}' table.txt\n> mat\n> mug\n> window\n> ```\n\n## Input field separator\n\nThe most common way to change the default field separator is to use the `-F` command line option. The value passed to the option will be treated as a string literal and then converted to a regexp. For now, here are some examples without any special regexp characters.\n\n```bash\n# use ':' as the input field separator\n$ echo 'goal:amazing:whistle:kwality' | awk -F: '{print $1}'\ngoal\n$ echo 'goal:amazing:whistle:kwality' | awk -F: '{print $NF}'\nkwality\n\n# use quotes to avoid clashes with shell special characters\n$ echo 'one;two;three;four' | awk -F';' '{print $3}'\nthree\n\n# first and last fields will have empty string as their values\n$ echo '=a=b=c=' | awk -F= '{print $1 \"[\" $NF \"]\"}'\n[]\n\n# difference between empty lines and lines without field separator\n$ printf '\\nhello\\napple,banana\\n' | awk -F, '{print NF}'\n0\n1\n2\n```\n\nYou can also directly set the special `FS` variable to change the input field separator. This can be done from the command line using `-v` option or within the code blocks.\n\n```bash\n$ echo 'goal:amazing:whistle:kwality' | awk -v FS=: '{print $2}'\namazing\n\n# field separator can be multiple characters too\n$ echo '1e4SPT2k6SPT3a5SPT4z0' | awk 'BEGIN{FS=\"SPT\"} {print $3}'\n3a5\n```\n\nIf you wish to split the input as individual characters, use an empty string as the field separator.\n\n```bash\n# note that the space between -F and '' is necessary here\n$ echo 'apple' | awk -F '' '{print $1}'\na\n$ echo 'apple' | awk -v FS= '{print $NF}'\ne\n\n# depending upon the locale, you can work with multibyte characters too\n$ echo 'αλεπού' | awk -v FS= '{print $3}'\nε\n```\n\nHere are some examples with regexp based field separators. The value passed to `-F` or `FS` is treated as a string and then converted to a regexp. So, you'll need `\\\\` instead of `\\` to mean a backslash character. The good news is that for single characters that are also regexp metacharacters, they'll be treated literally and you do not need to escape them.\n\n```bash\n$ echo 'Sample123string42with777numbers' | awk -F'[0-9]+' '{print $2}'\nstring\n$ echo 'Sample123string42with777numbers' | awk -F'[a-zA-Z]+' '{print $2}'\n123\n\n# note the use of \\\\W to indicate \\W\n$ echo 'load;err_msg--\\ant,r2..not' | awk -F'\\\\W+' '{print $3}'\nant\n\n# same as: awk -F'\\\\.' '{print $2}'\n$ echo 'hi.bye.hello' | awk -F. '{print $2}'\nbye\n\n# count the number of vowels for each input line\n# note that empty lines will give -1 in the output\n$ printf 'cool\\nnice car\\n' | awk -F'[aeiou]' '{print NF-1}'\n2\n3\n```\n\n>![warning](images/warning.svg) The default value of `FS` is a single space character. So, if you set the input field separator to a single space, then it will be the same as if you are using the default split discussed in the previous section. If you want to override this behavior, you can use space inside a character class.\n>\n> ```bash\n> # same as: awk '{print NF}'\n> $ echo '   a   b   c   ' | awk -F' ' '{print NF}'\n> 3\n>\n> # there are 12 space characters, thus 13 fields\n> $ echo '   a   b   c   ' | awk -F'[ ]' '{print NF}'\n> 13\n> ```\n\nIf `IGNORECASE` is set, it will affect field separation as well. Except when the field separator is a single character, which can be worked around by using a character class.\n\n```bash\n$ echo 'RECONSTRUCTED' | awk -F'[aeiou]+' -v IGNORECASE=1 '{print $NF}'\nD\n\n# when FS is a single character\n$ echo 'RECONSTRUCTED' | awk -F'e' -v IGNORECASE=1 '{print $1}'\nRECONSTRUCTED\n$ echo 'RECONSTRUCTED' | awk -F'[e]' -v IGNORECASE=1 '{print $1}'\nR\n```\n\n## Output field separator\n\nThe `OFS` special variable controls the output field separator. `OFS` is used as the string between multiple arguments passed to the `print` function. It is also used whenever `$0` has to be reconstructed as a result of field contents being modified. The default value for `OFS` is a single space character, just like `FS`. There is no equivalent command line option though, you'll have to change `OFS` directly.\n\n```bash\n# print the first and third fields, OFS is used to join these values\n# note the use of , to separate print arguments\n$ awk '{print $1, $3}' table.txt\nbrown mat\nblue mug\nyellow window\n\n# same FS and OFS\n$ echo 'goal:amazing:whistle:kwality' | awk -F: -v OFS=: '{print $2, $NF}'\namazing:kwality\n$ echo 'goal:amazing:whistle:kwality' | awk 'BEGIN{FS=OFS=\":\"} {print $2, $NF}'\namazing:kwality\n\n# different values for FS and OFS\n$ echo 'goal:amazing:whistle:kwality' | awk -F: -v OFS=- '{print $2, $NF}'\namazing-kwality\n```\n\nHere are some examples for changing field contents and then printing `$0`.\n\n```bash\n$ echo 'goal:amazing:whistle:kwality' | awk -F: -v OFS=: '{$2 = 42} 1'\ngoal:42:whistle:kwality\n$ echo 'goal:amazing:whistle:kwality' | awk -F: -v OFS=, '{$2 = 42} 1'\ngoal,42,whistle,kwality\n\n# recall that spaces at the start/end gets trimmed for default FS\n$ echo '   a   b   c   ' | awk '{$NF = \"last\"} 1'\na b last\n```\n\nSometimes you want to print the contents of `$0` with the new `OFS` value but field contents aren't being changed. In such cases, you can assign a field value to itself to force the reconstruction of `$0`.\n\n```bash\n# no change because there was no trigger to rebuild $0\n$ echo 'Sample123string42with777numbers' | awk -F'[0-9]+' -v OFS=, '1'\nSample123string42with777numbers\n\n# assign a field to itself in such cases\n$ echo 'Sample123string42with777numbers' | awk -F'[0-9]+' -v OFS=, '{$1=$1} 1'\nSample,string,with,numbers\n```\n\n>![info](images/info.svg) If you need to set the same input and output field separator, you can write a more concise one-liner using brace expansion. Here are some examples:\n>\n> ```bash\n> $ echo -v{,O}FS=:\n> -vFS=: -vOFS=:\n> \n> $ echo 'goal:amazing:whistle:kwality' | awk -v{,O}FS=: '{$2 = 42} 1'\n> goal:42:whistle:kwality\n> \n> $ echo 'goal:amazing:whistle:kwality' | awk '{$2 = 42} 1' {,O}FS=:\n> goal:42:whistle:kwality\n> ```\n>\n> However, this is not commonly used and doesn't save too many characters to be preferred over explicit assignment.\n\n## Manipulating NF\n\nChanging the value of `NF` will rebuild `$0` as well. Here are some examples:\n\n```bash\n# reducing fields\n$ echo 'goal:amazing:whistle:kwality' | awk -F: -v OFS=, '{NF=2} 1'\ngoal,amazing\n\n# increasing fields\n$ echo 'goal:amazing:whistle:kwality' | awk -F: -v OFS=: '{$(NF+1)=\"sea\"} 1'\ngoal:amazing:whistle:kwality:sea\n\n# empty fields will be created as needed\n$ echo 'goal:amazing:whistle:kwality' | awk -F: -v OFS=: '{$8=\"go\"} 1'\ngoal:amazing:whistle:kwality::::go\n```\n\n>![warning](images/warning.svg) Assigning `NF` to `0` will delete all the fields. However, a negative value will result in an error.\n\n```bash\n$ echo 'goal:amazing:whistle:kwality' | awk -F: -v OFS=: '{NF=-1} 1'\nawk: cmd. line:1: (FILENAME=- FNR=1) fatal: NF set to negative value\n```\n\n## FPAT\n\nThe `FS` variable allows you to define the input field *separator*. In contrast, `FPAT` (field pattern) allows you to define what should the fields be made up of.\n\n```bash\n$ s='Sample123string42with777numbers'\n# one or more consecutive digits\n$ echo \"$s\" | awk -v FPAT='[0-9]+' '{print $2}'\n42\n\n$ s='coat Bin food tar12 best Apple fig_42'\n# whole words made up of lowercase alphabets and digits only\n$ echo \"$s\" | awk -v FPAT='\\\\<[a-z0-9]+\\\\>' -v OFS=, '{$1=$1} 1'\ncoat,food,tar12,best\n\n$ s='items: \"apple\" and \"mango\"'\n# get the first double quoted item\n$ echo \"$s\" | awk -v FPAT='\"[^\"]+\"' '{print $1}'\n\"apple\"\n```\n\n`FPAT` is often used for CSV input where fields can contain embedded delimiter characters. For example, a field content `\"fox,42\"` when `,` is the delimiter.\n\n```bash\n$ s='eagle,\"fox,42\",bee,frog'\n\n# simply using , as separator isn't sufficient\n$ echo \"$s\" | awk -F, '{print $2}'\n\"fox\n```\n\nFor such simpler CSV input, `FPAT` helps to define fields as starting and ending with double quotes or containing non-comma characters.\n\n```bash\n# * is used instead of + to allow empty fields\n$ echo \"$s\" | awk -v FPAT='\"[^\"]*\"|[^,]*' '{print $2}'\n\"fox,42\"\n```\n\n>![warning](images/warning.svg) The above will not work for all kinds of CSV files, for example if fields contain escaped double quotes, newline characters, etc. See [stackoverflow: What's the most robust way to efficiently parse CSV using awk?](https://stackoverflow.com/q/45420535/4082052) and [csvquote](https://github.com/dbro/csvquote) for such cases. You could also use other programming languages such as Perl, Python, Ruby, etc which come with standard CSV parsing libraries or have easy access to third party solutions. There are also specialized command line tools such as [xsv](https://github.com/BurntSushi/xsv).\n\n>![info](images/info.svg) A proper CSV support is planned for a future version. You can also check out [frawk](https://github.com/ezrosent/frawk), which is mostly similar to the `awk` command but also supports CSV parsing. [goawk](https://github.com/benhoyt/goawk) is another implementation with CSV support.\n\nIf `IGNORECASE` is set, it will affect field matching as well. Unlike `FS`, there is no different behavior for a single character pattern.\n\n```bash\n# count number of 'e' in the input string\n$ echo 'Read Eat Sleep' | awk -v FPAT='e' '{print NF}'\n3\n$ echo 'Read Eat Sleep' | awk -v IGNORECASE=1 -v FPAT='e' '{print NF}'\n4\n$ echo 'Read Eat Sleep' | awk -v IGNORECASE=1 -v FPAT='[e]' '{print NF}'\n4\n```\n\n## FIELDWIDTHS\n\n`FIELDWIDTHS` is another feature where you get to define field contents. As indicated by the name, you have to specify the number of characters for each field. This method is useful to process fixed width data.\n\n```bash\n$ cat items.txt\napple   fig banana\n50      10  200\n\n# here field widths have been assigned such that\n# extra spaces are placed at the end of each field\n$ awk -v FIELDWIDTHS='8 4 6' '{print $2}' items.txt\nfig \n10  \n# note that the field contents will include the spaces as well\n$ awk -v FIELDWIDTHS='8 4 6' '{print \"[\" $2 \"]\"}' items.txt\n[fig ]\n[10  ]\n```\n\nYou can optionally prefix a field width with number of characters to be ignored.\n\n```bash\n# first field is 5 characters\n# then 3 characters are ignored and 3 characters for the second field\n# then 1 character is ignored and 6 characters for the third field\n$ awk -v FIELDWIDTHS='5 3:3 1:6' '{print \"[\" $1 \"]\"}' items.txt\n[apple]\n[50   ]\n$ awk -v FIELDWIDTHS='5 3:3 1:6' '{print \"[\" $2 \"]\"}' items.txt\n[fig]\n[10 ]\n```\n\nIf an input line length exceeds the total width specified, the extra characters will simply be ignored. If you wish to access those characters, you can use `*` to represent the last field. See [gawk manual: FIELDWIDTHS](https://www.gnu.org/software/gawk/manual/gawk.html#Fields-with-fixed-data) for more such corner cases.\n\n```bash\n$ awk -v FIELDWIDTHS='5 *' '{print \"[\" $1 \"]\"}' items.txt\n[apple]\n[50   ]\n\n$ awk -v FIELDWIDTHS='5 *' '{print \"[\" $2 \"]\"}' items.txt\n[   fig banana]\n[   10  200]\n```\n\n## Summary\n\nWorking with fields is the most popular feature of `awk`. This chapter discussed various ways in which you can split the input into fields and manipulate them. There are many more examples to be discussed related to fields in the coming chapters. I'd highly suggest to also read through [gawk manual: Fields](https://www.gnu.org/software/gawk/manual/gawk.html#Fields) for more details regarding field processing.\n\nNext chapter will discuss various ways to use record separators and related special variables.\n\n## Exercises\n\n>![info](images/info.svg) The [exercises](https://github.com/learnbyexample/learn_gnuawk/tree/master/exercises) directory has all the files used in this section.\n\n**1)** For the input file `brackets.txt`, extract only the contents between `()` or `)(` from each input line. Assume that `()` characters will be present only once every line.\n\n```bash\n$ cat brackets.txt\nfoo blah blah(ice) 123 xyz$ \n(almond-pista) choco\nyo )yoyo( yo\n\n$ awk ##### add your solution here\nice\nalmond-pista\nyoyo\n```\n\n**2)** For the input file `scores.csv`, extract `Name` and `Physics` fields in the format shown below.\n\n```bash\n$ cat scores.csv\nName,Maths,Physics,Chemistry\nBlue,67,46,99\nLin,78,83,80\nEr,56,79,92\nCy,97,98,95\nOrt,68,72,66\nIth,100,100,100\n\n$ awk ##### add your solution here\nName:Physics\nBlue:46\nLin:83\nEr:79\nCy:98\nOrt:72\nIth:100\n```\n\n**3)** For the input file `scores.csv`, display names of those who've scored above `70` in Maths.\n\n```bash\n$ awk ##### add your solution here\nLin\nCy\nIth\n```\n\n**4)** Display the number of word characters for the given inputs. Word definition here is same as used in regular expressions. Can you construct a solution with `gsub` and one without substitution functions?\n\n```bash\n$ echo 'hi there' | awk ##### add your solution here\n7\n\n$ echo 'u-no;co%.\"(do_12:as' | awk ##### add your solution here\n12\n```\n\n**5)** For the input file `quoted.txt`, extract the first and third sequence of characters surrounded by double quotes and display them in the format shown below. Solution shouldn't use substitution functions.\n\n```bash\n$ cat quoted.txt\n1 \"grape\" and \"mango\" and \"guava\"\n(\"a 1\"\"b\"\"c-2\"\"d\")\n\n$ awk ##### add your solution here\n\"grape\",\"guava\"\n\"a 1\",\"c-2\"\n```\n\n**6)** For the input file `varying_fields.txt`, construct a solution to get the output shown below. Solution shouldn't use substitution functions.\n\n```bash\n$ cat varying_fields.txt\nhi,bye,there,was,here,to\n1,2,3,4,5\n\n$ awk ##### add your solution here\nhi,bye,to\n1,2,5\n```\n\n**7)** Transform the given input file `fw.txt` to get the output as shown below. If a field is empty (i.e. contains only space characters), replace it with `NA`.\n\n```bash\n$ cat fw.txt\n1.3  rs   90  0.134563\n3.8           6\n5.2  ye       8.2387\n4.2  kt   32  45.1\n\n$ awk ##### add your solution here\n1.3,rs,0.134563\n3.8,NA,6\n5.2,ye,8.2387\n4.2,kt,45.1\n```\n\n**8)** Display only the third and fifth characters from each input line as shown below.\n\n```bash\n$ printf 'restore\\ncat one\\ncricket' | awk ##### add your solution here\nso\nto\nik\n```\n\n**9)** The `fields.txt` file has fields separated by the `:` character. Delete `:` and the last field if there is a digit character anywhere before the last field. Solution shouldn't use substitution functions.\n\n```bash\n$ cat fields.txt\n42:cat\ntwelve:a2b\nwe:be:he:0:a:b:bother\napple:banana-42:cherry:\ndragon:unicorn:centaur\n\n$ awk ##### add your solution here\n42\ntwelve:a2b\nwe:be:he:0:a:b\napple:banana-42:cherry\ndragon:unicorn:centaur\n```\n\n**10)** Retain only the first three fields for the given sample string that uses `^` as the input field separator. Use `,` as the output field separator.\n\n```bash\n$ echo 'sit^eat^very^eerie^near' | awk ##### add your solution here\nsit,eat,very\n```\n\n**11)** The sample string shown below uses `cat` as the field separator (irrespective of case). Use space as the output field separator and add `42` as the last field.\n\n```bash\n$ s='applecatfigCaT12345cAtbanana'\n$ echo \"$s\" | awk ##### add your solution here\napple fig 12345 banana 42\n```\n\n**12)** For the input file `sample.txt`, filter lines containing 6 or more lowercase vowels.\n\n```bash\n$ awk ##### add your solution here\nNo doubt you like it too\nMuch ado about nothing\n```\n\n**13)** The input file `concat.txt` has contents of various files preceded by a line starting with `###`. Replace such sequence of characters with an incrementing integer value (starting with `1`) in the format shown below.\n\n```bash\n$ awk ##### add your solution here\n1) addr.txt\nHow are you\nThis game is good\nToday is sunny\n2) broken.txt\ntop\n1234567890\nbottom\n3) sample.txt\nJust do-it\nBelieve it\n4) mixed_fs.txt\npink blue white yellow\ncar,mat,ball,basket\n```\n\n# Record separators\n\nSo far, you've seen examples where `awk` automatically splits input line by line based on the newline character. Just like you can control how those lines are further split into fields using `FS` and other features, `awk` provides a way to control what constitutes a line in the first place. In `awk` parlance, the term **record** is used to describe the contents that gets placed in the `$0` variable. And similar to `OFS`, you can control the string that gets added at the end for the `print` function. This chapter will also discuss how you can use special variables that have information related to record (line) numbers.\n\n>![info](images/info.svg) The [example_files](https://github.com/learnbyexample/learn_gnuawk/tree/master/example_files) directory has all the files used in the examples.\n\n## Input record separator\n\nThe `RS` special variable is used to control how the input content is split into records. The default is the newline character, as evident from the examples used in the previous chapters. The special variable `NR` keeps track of the current record number.\n\n```bash\n# change the input record separator to a comma character\n# note the content of the 2nd record where newline is just another character\n$ printf 'this,is\\na,sample,text' | awk -v RS=, '{print NR \")\", $0}'\n1) this\n2) is\na\n3) sample\n4) text\n```\n\nRecall that default `FS` will split input record based on spaces, tabs and newlines. Now that you've seen how `RS` can be something other than `\\n`, here's an example to show the full effect of the default record splitting.\n\n```bash\n$ s='   a\\t\\tb:1000\\n\\n\\t \\n\\n123 7777:x  y \\n \\n z  :apple banana cherry'\n$ printf '%b' \"$s\" | awk -v RS=: -v OFS=, '{$1=$1} 1'\na,b\n1000,123,7777\nx,y,z\napple,banana,cherry\n```\n\nSimilar to `FS`, the `RS` value is treated as a string literal and then converted to a regexp. For now, consider an example with multiple characters for `RS` but without needing regexp metacharacters.\n\n```bash\n$ cat report.log\nblah blah Error: second record starts\nsomething went wrong\nsome more details Error: third record\ndetails about what went wrong\n\n# use 'Error:' as the input record separator\n# print all the records that contains 'something'\n$ awk -v RS='Error:' '/something/' report.log\n second record starts\nsomething went wrong\nsome more details \n```\n\nIf `IGNORECASE` is set, it will affect record separation as well. Except when the record separator is a single character, which can be worked around by using a character class.\n\n```bash\n$ awk -v IGNORECASE=1 -v RS='error:' 'NR==1' report.log\nblah blah \n\n# when RS is a single character\n$ awk -v IGNORECASE=1 -v RS='e' 'NR==1' report.log\nblah blah Error: s\n$ awk -v IGNORECASE=1 -v RS='[e]' 'NR==1' report.log\nblah blah \n```\n\n>![warning](images/warning.svg) The default line ending for text files varies between different platforms. For example, a text file downloaded from the internet or a file originating from Windows OS would typically have lines ending with carriage return and line feed characters. So, you'll have to use `RS='\\r\\n'` for such files. See also [stackoverflow: Why does my tool output overwrite itself and how do I fix it?](https://stackoverflow.com/q/45772525/4082052) for a detailed discussion and mitigation methods.\n\n## Output record separator\n\nThe `ORS` special variable is used to customize the output record separator. `ORS` is the string that gets added to the end of every call to the `print` function. The default value for `ORS` is a single newline character, just like `RS`.\n\n```bash\n# change NUL record separator to dot and newline\n$ printf 'apple\\0banana\\0cherry\\0' | awk -v RS='\\0' -v ORS='.\\n' '1'\napple.\nbanana.\ncherry.\n\n$ cat msg.txt\nHello there.\nIt will rain to-\nday. Have a safe\nand pleasant jou-\nrney.\n# here ORS is an empty string\n$ awk -v RS='-\\n' -v ORS= '1' msg.txt\nHello there.\nIt will rain today. Have a safe\nand pleasant journey.\n```\n\n>![info](images/info.svg) Note that the `$0` variable is assigned after removing trailing characters matched by `RS`. Thus, you cannot directly manipulate those characters. With tools that don't automatically strip record separator, such as `perl`, the previous example can be solved as `perl -pe 's/-\\n//' msg.txt`.\n\nMany a times, you need to change `ORS` depending upon contents of input record or some other condition. The `cond ? expr1 : expr2` ternary operator is often used in such scenarios. The below example assumes that input is evenly divisible, you'll have to add more logic if that is not the case.\n\n```bash\n# can also use RS instead of \"\\n\" here\n$ seq 6 | awk '{ORS = NR%3 ? \"-\" : \"\\n\"} 1'\n1-2-3\n4-5-6\n```\n\n>![info](images/info.svg) If the last line of input didn't end with the input record separator, it might get added in the output if `print` is used, as `ORS` gets appended.\n>\n> ```bash\n> # here last line of the input doesn't end with a newline character\n> # but gets added via ORS when 'print' is used\n> $ printf '1\\n2' | awk '1; END{print 3}'\n> 1\n> 2\n> 3\n> ```\n\n## Regexp RS and RT\n\nAs mentioned before, the value passed to `RS` is treated as a string literal and then converted to a regexp. Here are some examples.\n\n```bash\n# set input record separator as one or more digit characters\n# print records containing both 'i' and 't'\n$ printf 'Sample123string42with777numbers' | awk -v RS='[0-9]+' '/i/ && /t/'\nstring\nwith\n\n# similar to FS, the value passed to RS is treated as a string\n# which is then converted to a regexp, so need \\\\ instead of \\ here\n$ printf 'load;err_msg--ant,r2..not' | awk -v RS='\\\\W+' '/an/'\nant\n```\n\nFirst record will be empty if `RS` matches from the start of input file. However, if `RS` matches until the very last character of the input file, there won't be an empty record as the last record. This is different from how `FS` behaves if it matches until the last character.\n\n```bash\n# first record is empty and the last record is a newline character\n# change 'echo' command to 'printf' and see what changes\n$ echo '123string42with777' | awk -v RS='[0-9]+' '{print NR \") [\" $0 \"]\"}'\n1) []\n2) [string]\n3) [with]\n4) [\n]\n\n# difference between FS and RS when they match till the end of the input\n$ printf '123string42with777' | awk -v FS='[0-9]+' '{print NF}'\n4\n$ printf '123string42with777' | awk -v RS='[0-9]+' 'END{print NR}'\n3\n```\n\nThe `RT` special variable contains the text that was matched by `RS`. This variable gets updated for every input record.\n\n```bash\n# print record number and the value of RT for that record\n# last record has empty RT because it didn't end with digits\n$ echo 'Sample123string42with777numbers' | awk -v RS='[0-9]+' '{print NR, RT}'\n1 123\n2 42\n3 777\n4 \n```\n\n## Paragraph mode\n\nAs a special case, when `RS` is set to an empty string, one or more consecutive empty lines is used as the input record separator. Consider the below sample file:\n\n```bash\n$ cat para.txt\nHello World\n\nHi there\nHow are you\n\nJust do-it\nBelieve it\n\nbanana\npapaya\nmango\n\nMuch ado about nothing\nHe he he\nAdios amigo\n```\n\nHere's an example of processing input paragraph wise:\n\n```bash\n# print all paragraphs containing 'do'\n# note that there'll be an empty line after the last record\n$ awk -v RS= -v ORS='\\n\\n' '/do/' para.txt\nJust do-it\nBelieve it\n\nMuch ado about nothing\nHe he he\nAdios amigo\n\n```\n\nThe empty line at the end is a common problem when dealing with custom record separators. You could either process the output further to remove it or add logic to handle the issue in `awk` itself. Here's one possible workaround for the previous example:\n\n```bash\n# here ORS is left as the default newline character\n# uninitialized variable 's' will be empty for the first match\n# afterwards, 's' will provide the empty line separation\n$ awk -v RS= '/do/{print s $0; s=\"\\n\"}' para.txt\nJust do-it\nBelieve it\n\nMuch ado about nothing\nHe he he\nAdios amigo\n```\n\nParagraph mode is not the same as using `RS='\\n\\n+'` because `awk` does a few more operations when `RS` is empty. See [gawk manual: multiline records](https://www.gnu.org/software/gawk/manual/html_node/Multiple-Line.html#Multiple-Line) for details. Important points are quoted below and illustrated with examples.\n\n>However, there is an important difference between `RS = \"\"` and `RS = \"\\n\\n+\"`. In the first case, leading newlines in the input data file are ignored\n\n```bash\n$ s='\\n\\n\\na\\nb\\n\\n12\\n34\\n\\nhi\\nhello\\n'\n\n# paragraph mode\n$ printf '%b' \"$s\" | awk -v RS= -v ORS='\\n---\\n' 'NR<=2'\na\nb\n---\n12\n34\n---\n\n# RS is '\\n\\n+' instead of paragraph mode\n$ printf '%b' \"$s\" | awk -v RS='\\n\\n+' -v ORS='\\n---\\n' 'NR<=2'\n\n---\na\nb\n---\n```\n\n>and if a file ends without extra blank lines after the last record, the final newline is removed from the record. In the second case, this special processing is not done.\n\n```bash\n$ s='\\n\\n\\na\\nb\\n\\n12\\n34\\n\\nhi\\nhello\\n'\n\n# paragraph mode\n$ printf '%b' \"$s\" | awk -v RS= -v ORS='\\n---\\n' 'END{print}'\nhi\nhello\n---\n\n# RS is '\\n\\n+' instead of paragraph mode\n$ printf '%b' \"$s\" | awk -v RS='\\n\\n+' -v ORS='\\n---\\n' 'END{print}'\nhi\nhello\n\n---\n```\n\n>When RS is set to the empty string and FS is set to a single character, the newline character always acts as a field separator. This is in addition to whatever field separations result from FS. When FS is the null string (`\"\"`) or a regexp, this special feature of RS does not apply. It does apply to the default field separator of a single space: `FS = \" \"`\n\n```bash\n$ s='a:b\\nc:d\\n\\n1\\n2\\n3'\n\n# FS is a single character in paragraph mode\n$ printf '%b' \"$s\" | awk -F: -v RS= -v ORS='\\n---\\n' '{$1=$1} 1'\na b c d\n---\n1 2 3\n---\n\n# FS is a regexp in paragraph mode\n$ printf '%b' \"$s\" | awk -F'[:]' -v RS= -v ORS='\\n---\\n' '{$1=$1} 1'\na b\nc d\n---\n1\n2\n3\n---\n\n# FS is a single character and RS is '\\n\\n+' instead of paragraph mode\n$ printf '%b' \"$s\" | awk -F: -v RS='\\n\\n+' -v ORS='\\n---\\n' '{$1=$1} 1'\na b\nc d\n---\n1\n2\n3\n---\n```\n\n## NR vs FNR\n\nThere are two special variables related to record numbering. You've seen `NR` earlier in the chapter, but here are some more examples.\n\n```bash\n# same as: head -n2\n$ seq 5 | awk 'NR<=2'\n1\n2\n\n# same as: tail -n1\n$ awk 'END{print}' table.txt\nyellow banana window shoes 3.14\n\n# change the first field content only for the second line\n$ awk 'NR==2{$1=\"green\"} 1' table.txt\nbrown bread mat hair 42\ngreen cake mug shirt -7\nyellow banana window shoes 3.14\n```\n\nAll the examples with `NR` so far has been with a single file input. If there are multiple file inputs, then you can choose between `NR` and the second special variable `FNR`. The difference is that `NR` contains total records read so far whereas `FNR` contains record number of only the current file being processed. Here are some examples to show them in action. You'll see more examples in later chapters as well.\n\n```bash\n$ awk -v OFS='\\t' 'BEGIN{print \"NR\", \"FNR\", \"Content\"}\n                   {print NR, FNR, $0}' report.log table.txt\nNR      FNR     Content\n1       1       blah blah Error: second record starts\n2       2       something went wrong\n3       3       some more details Error: third record\n4       4       details about what went wrong\n5       1       brown bread mat hair 42\n6       2       blue cake mug shirt -7\n7       3       yellow banana window shoes 3.14\n\n# same as: head -q -n1\n$ awk 'FNR==1' report.log table.txt\nblah blah Error: second record starts\nbrown bread mat hair 42\n```\n\nFor large input files, use `exit` to avoid unnecessary record processing.\n\n```bash\n$ seq 3542 4623452 | awk 'NR==2452{print; exit}'\n5993\n$ seq 3542 4623452 | awk 'NR==250; NR==2452{print; exit}'\n3791\n5993\n\n# here is a sample time comparison\n$ time seq 3542 4623452 | awk 'NR==2452{print; exit}' > f1\nreal    0m0.004s\n$ time seq 3542 4623452 | awk 'NR==2452' > f2\nreal    0m0.395s\n```\n\n## Summary\n\nThis chapter showed you how to change the way input content is split into records and how to set the string to be appended when `print` is used. The paragraph mode is useful for processing multiline records separated by empty lines. You also learned two special variables related to record numbers and when to use them.\n\nSo far, you've used `awk` to manipulate file content without modifying the source file. The next chapter will discuss how to write back the changes to the original input files.\n\n## Exercises\n\n>![info](images/info.svg) The [exercises](https://github.com/learnbyexample/learn_gnuawk/tree/master/exercises) directory has all the files used in this section.\n\n**1)** The input file `jumbled.txt` consists of words separated by various delimiters. Display all words that contain `an` or `at` or `in` or `it`, one per line.\n\n```bash\n$ cat jumbled.txt\novercoats;furrowing-typeface%pewter##hobby\nwavering:concession/woof\\retailer\njoint[]seer{intuition}titanic\n\n$ awk ##### add your solution here\novercoats\nfurrowing\nwavering\njoint\nintuition\ntitanic\n```\n\n**2)** Emulate `paste -sd,` with `awk`.\n\n```bash\n# this command joins all input lines with the ',' character\n$ paste -sd, addr.txt\nHello World,How are you,This game is good,Today is sunny,12345,You are funny\n# make sure there's no ',' at end of the line\n# and that there's a newline character at the end of the line\n$ awk ##### add your solution here\nHello World,How are you,This game is good,Today is sunny,12345,You are funny\n\n# if there's only one line in input, again make sure there's no trailing ','\n$ printf 'fig' | paste -sd,\nfig\n$ printf 'fig' | awk ##### add your solution here\nfig\n```\n\n**3)** For the input file `scores.csv`, add another column named **GP** which is calculated out of 100 by giving 50% weightage to Maths and 25% each for Physics and Chemistry.\n\n```bash\n$ awk ##### add your solution here\nName,Maths,Physics,Chemistry,GP\nBlue,67,46,99,69.75\nLin,78,83,80,79.75\nEr,56,79,92,70.75\nCy,97,98,95,96.75\nOrt,68,72,66,68.5\nIth,100,100,100,100\n```\n\n**4)** For the input file `sample.txt`, extract paragraphs containing `do` and exactly two lines.\n\n```bash\n$ cat sample.txt\nHello World\n\nGood day\nHow are you\n\nJust do-it\nBelieve it\n\nToday is sunny\nNot a bit funny\nNo doubt you like it too\n\nMuch ado about nothing\nHe he he\n\n# note that there's no extra empty line at the end of the output\n$ awk ##### add your solution here\nJust do-it\nBelieve it\n\nMuch ado about nothing\nHe he he\n```\n\n**5)** For the input file `sample.txt`, change each paragraph to a single line by joining lines using `.` and a space character as the separator. Also, add a final `.` to each paragraph.\n\n```bash\n# note that there's no extra empty line at the end of the output\n$ awk ##### add your solution here\nHello World.\n\nGood day. How are you.\n\nJust do-it. Believe it.\n\nToday is sunny. Not a bit funny. No doubt you like it too.\n\nMuch ado about nothing. He he he.\n```\n\n**6)** The various input/output separators can be changed dynamically and comes into effect during the next input/output operation. For the input file `mixed_fs.txt`, retain only the first two fields from each input line. The field separators should be space for the first two lines and `,` for the rest of the lines.\n\n```bash\n$ cat mixed_fs.txt\nrose lily jasmine tulip\npink blue white yellow\ncar,mat,ball,basket\ngreen,brown,black,purple\napple,banana,cherry\n\n$ awk ##### add your solution here\nrose lily\npink blue\ncar,mat\ngreen,brown\napple,banana\n```\n\n**7)** For the input file `table.txt`, print other than the second line.\n\n```bash\n$ awk ##### add your solution here\nbrown bread mat hair 42\nyellow banana window shoes 3.14\n```\n\n**8)** For the `table.txt` file, print only the line number for lines containing `air` or `win`.\n\n```bash\n$ awk ##### add your solution here\n1\n3\n```\n\n**9)** For the input file `table.txt`, calculate the sum of numbers in the last column, excluding the second line.\n\n```bash\n$ awk ##### add your solution here\n45.14\n```\n\n**10)** Print the second and fourth line for every block of five lines.\n\n```bash\n$ seq 15 | awk ##### add your solution here\n2\n4\n7\n9\n12\n14\n```\n\n**11)** For the input file `odd.txt`, surround all whole words with `{}` that start and end with the same word character. This is a contrived exercise to make you use the `RT` variable (`sed -E 's/\\b(\\w)(\\w*\\1)?\\b/{&}/g' odd.txt` would be a simpler solution).\n\n```bash\n$ cat odd.txt\n-oreo-not:a _a2_ roar<=>took%22\nRoaR to wow-\n\n$ awk ##### add your solution here\n-{oreo}-not:{a} {_a2_} {roar}<=>took%{22}\n{RoaR} to {wow}-\n```\n\n**12)** Print only the second field of the third line, if any, from these input files: `addr.txt`, `sample.txt` and `copyright.txt`. Consider space as the field separator.\n\n```bash\n$ awk ##### add your solution here\ngame\nday\nbla\n```\n\n**13)** The input file `ip.txt` has varying amount of empty lines between the records, change them to be always two empty lines. Also, remove the empty lines at the start and end of the file.\n\n```bash\n$ awk ##### add your solution here\nhello\n\n\nworld\n\n\napple\nbanana\ncherry\n\n\ntea coffee\nchocolate\n```\n\n**14)** The sample string shown below uses `cat` as the record separator (irrespective of case). Display only the even numbered records separated by a single empty line.\n\n```bash\n$ s='applecatfigCaT12345cAtbananaCATguava:caT:mangocat3'\n$ echo \"$s\" | awk ##### add your solution here\nfig\n\nbanana\n\n:mango\n```\n\n**15)** Input has the ASCII NUL character as the record separator. Change it to dot and newline characters as shown below.\n\n```bash\n$ printf 'apple\\npie\\0banana\\ncherry\\0' | awk ##### add your solution here\napple\npie.\nbanana\ncherry.\n```\n\n# In-place file editing\n\nIn the examples presented so far, the output from `awk` was displayed on the terminal. This chapter will discuss how to write back the changes to the input files using the `-i` command line option. You can also choose to create backups of the original files.\n\n>![info](images/info.svg) The [example_files](https://github.com/learnbyexample/learn_gnuawk/tree/master/example_files) directory has all the files used in the examples.\n\n## Without backup\n\nThe `-i` option allows you to load libraries (see [gawk manual: -i option](https://www.gnu.org/software/gawk/manual/gawk.html#index-_002di-option) for details). The `inplace` library comes by default with the `awk` installation. Use `-i inplace` to indicate that you want to modify the original input itself. Use this option with caution, preferably after testing that the code is working as intended.\n\n```bash\n$ cat greet.txt\nHi there\nHave a nice day\nGood bye\n\n# prefix line numbers\n$ awk -i inplace '{print NR \". \" $0}' greet.txt\n$ cat greet.txt\n1. Hi there\n2. Have a nice day\n3. Good bye\n```\n\nMultiple input files are treated separately and changes are written back to the respective files.\n\n```bash\n$ cat f1.txt\nI ate 3 apples\n$ cat f2.txt\nI bought two balls and 3 bats\n\n$ awk -i inplace '{gsub(/\\<3\\>/, \"three\")} 1' f1.txt f2.txt\n$ cat f1.txt\nI ate three apples\n$ cat f2.txt\nI bought two balls and three bats\n```\n\n## With backup\n\nYou can provide a backup extension by setting the `inplace::suffix` special variable. For example, if the input file is `ip.txt` and `inplace::suffix='.orig'` is used, the backup file will be named as `ip.txt.orig`.\n\n```bash\n$ cat f3.txt\n  Name    Physics  Maths\n Moe  76  82\nRaj  56  64\n\n$ awk -i inplace -v inplace::suffix='.bkp' -v OFS=, '{$1=$1} 1' f3.txt\n$ cat f3.txt\nName,Physics,Maths\nMoe,76,82\nRaj,56,64\n\n# original file will be preserved in 'f3.txt.bkp'\n$ cat f3.txt.bkp\n  Name    Physics  Maths\n Moe  76  82\nRaj  56  64\n```\n\n>![info](images/info.svg) In earlier versions of `awk`, the `INPLACE_SUFFIX` variable was used instead of `inplace::suffix`. Also, you can use `inplace::enable` variable to dynamically control whether files should be in-placed or not. See [gawk manual: Enabling In-Place File Editing](https://www.gnu.org/software/gawk/manual/gawk.html#Extension-Sample-Inplace) for more details.\n\n## Security implications\n\nBy default, when you use the `-i inplace` option, the `awk` command will look for a file named `inplace` or `inplace.awk` in the current working directory. If such files aren't found, then `awk` will look for them in the installation directories, which is what you'd usually want.\n\nFor secure applications, you shouldn't rely on the `-i inplace` option. Instead, you could either use the absolute path of the `inplace` file from the installation directory, or manipulate `AWKPATH` (environment variable that controls the behavior of searching for files to be loaded) to be restricted to secure paths only. See [this unix.stackexchange thread](https://unix.stackexchange.com/q/749645/109046) for more details about this issue and workarounds.\n\n## Summary\n\nThis chapter discussed about the `-i inplace` option which is useful when you need to edit a file in-place. This is particularly useful in automation scripts. But, do ensure that you have tested the `awk` command before applying changes to the actual files if you need to use this option without creating backups.\n\nThe next chapter will discuss the use of shell variables in more detail.\n\n## Exercises\n\n>![info](images/info.svg) The [exercises](https://github.com/learnbyexample/learn_gnuawk/tree/master/exercises) directory has all the files used in this section.\n\n**1)** For the input file `copyright.txt`, replace `copyright: 2018` with `copyright: 2020` and write back the changes to `copyright.txt` itself. The original contents should get saved to `copyright.txt.orig`\n\n```bash\n$ cat copyright.txt\nbla bla 2015 bla\nblah 2018 blah\nbla bla bla\ncopyright: 2018\n$ awk ##### add your solution here\n\n$ cat copyright.txt\nbla bla 2015 bla\nblah 2018 blah\nbla bla bla\ncopyright: 2020\n$ cat copyright.txt.orig\nbla bla 2015 bla\nblah 2018 blah\nbla bla bla\ncopyright: 2018\n```\n\n**2)** For the input files `nums1.txt` and `nums2.txt`, retain only the second and third lines and write back the changes to their respective files. No need to create backups.\n\n```bash\n$ cat nums1.txt\n3.14\n4201\n777\n0323012\n$ cat nums2.txt\n-45.4\n-2\n54316.12\n0x231\n\n$ awk ##### add your solution here\n$ cat nums1.txt\n4201\n777\n$ cat nums2.txt\n-2\n54316.12\n```\n\n# Using shell variables\n\nWhen it comes to automation and scripting, you'd often need to construct commands that can accept input from the user, incorporate data from a file or the output of a tool and so on.\n\nIn this chapter, you'll see how to pass information saved in shell variables to `awk` commands. As mentioned before, this book assumes `bash` as the shell being used.\n\n>![info](images/info.svg) As an example, see my repo [ch: command help](https://github.com/learnbyexample/command_help/blob/master/ch) for a practical shell script where commands are constructed dynamically.\n\n>![info](images/info.svg) The [example_files](https://github.com/learnbyexample/learn_gnuawk/tree/master/example_files) directory has all the files used in the examples.\n\n## -v option\n\nThe most common method is to use the `-v` command line option.\n\n```bash\n# assume that the 's' variable is part of some bash script\n# or perhaps a variable that stores the output of a shell command\n$ s='cake'\n$ awk -v word=\"$s\" '$2==word' table.txt\nblue cake mug shirt -7\n```\n\n## ENVIRON\n\nTo access environment variables of the shell, you can call the special array variable `ENVIRON` with the name of the environment variable as a string key.\n\n```bash\n# existing environment variable\n# output shown here is for my machine, would differ for you\n$ awk 'BEGIN{print ENVIRON[\"HOME\"]}'\n/home/learnbyexample\n$ awk 'BEGIN{print ENVIRON[\"SHELL\"]}'\n/bin/bash\n\n# defined along with the awk command\n# note that the variable is placed as a prefix to the command\n$ word='hello' awk 'BEGIN{print ENVIRON[\"word\"]}'\nhello\n```\n\n`ENVIRON` is a good way to get around `awk`'s interpretation of escape sequences. This is especially helpful for fixed string matching (see the [index](#index) section for examples).\n\n```bash\n$ s='hi\\nbye'\n\n# when passed via -v option\n$ awk -v ip=\"$s\" 'BEGIN{print ip}'\nhi\nbye\n\n# when passed as an environment variable\n$ ip=\"$s\" awk 'BEGIN{print ENVIRON[\"ip\"]}'\nhi\\nbye\n```\n\nHere's another example when a regexp is passed to an `awk` command.\n\n```bash\n# when passed via -v option\n$ r='\\Bpar\\B'\n$ awk -v rgx=\"$r\" '$0 ~ rgx' anchors.txt\nawk: warning: escape sequence '\\B' treated as plain 'B'\n$ r='\\\\Bpar\\\\B'\n$ awk -v rgx=\"$r\" '$0 ~ rgx' anchors.txt\napparent effort\ntwo spare computers\n\n# when passed as an environment variable\n$ r='\\Bpar\\B'\n$ rgx=\"$r\" awk '$0 ~ ENVIRON[\"rgx\"]' anchors.txt\napparent effort\ntwo spare computers\n```\n\n## Summary\n\nThis short chapter revisited the `-v` command line option and introduced the `ENVIRON` special array. These are particularly useful when the `awk` command is part of a shell script. Arrays will be discussed in more detail in the later chapters.\n\nThe next chapter will cover control structures.\n\n## Exercises\n\n>![info](images/info.svg) The [exercises](https://github.com/learnbyexample/learn_gnuawk/tree/master/exercises) directory has all the files used in this section.\n\n**1)** Use contents of the `s` variable to display all matching lines from the input file `sample.txt`. Assume that the `s` variable doesn't have any regexp metacharacters and construct a solution such that only whole words are matched.\n\n```bash\n$ s='do'\n##### add your solution here\nJust do-it\n```\n\n**2)** Replace all occurrences of `o` for the input file `addr.txt` with the literal contents of the `s` variable. Assume that the `s` variable has regexp metacharacters.\n\n```bash\n$ s='\\&/'\n##### add your solution here\nHell\\&/ W\\&/rld\nH\\&/w are y\\&/u\nThis game is g\\&/\\&/d\nT\\&/day is sunny\n12345\nY\\&/u are funny\n```\n\n# Control Structures\n\nYou've already seen various examples requiring conditional expressions. This chapter will revisit the `if-else` control structure and the ternary operator. Then you will see some examples with explicit loops (recall that `awk` is already looping over input records). Followed by keywords that control loop flow. Most of the syntax is very similar to the `C` language.\n\n>![info](images/info.svg) The [example_files](https://github.com/learnbyexample/learn_gnuawk/tree/master/example_files) directory has all the files used in the examples.\n\n## if-else\n\nMostly, when you need to use `if` control structure, you can get away with using the `condX{actionX}` blocks instead. But sometimes, you need additional condition checking within such action blocks. Or, you might need it inside loops. The syntax is `if(cond){action}` where the braces are optional if you need only one statement. `if` can be optionally followed by multiple `else if` conditions and a final `else` condition. These can also be nested as needed.\n\n```bash\n# print all lines starting with 'b'\n# additionally, if the last column is > 0, then print some more text\n$ awk '/^b/{print; if($NF>0) print \"------\"}' table.txt\nbrown bread mat hair 42\n------\nblue cake mug shirt -7\n\n# same as above, but uses the 'else' condition as well\n$ awk '/^b/{print; if($NF>0) print \"------\"; else print \"======\"}' table.txt\nbrown bread mat hair 42\n------\nblue cake mug shirt -7\n======\n```\n\nThe ternary operator often reduces the need for single statement `if-else` control structures.\n\n```bash\n# same as: awk '{if(NR%3) ORS=\"-\" ; else ORS=RS} 1'\n$ seq 6 | awk '{ORS = NR%3 ? \"-\" : RS} 1'\n1-2-3\n4-5-6\n\n# note that parentheses is necessary for print in this case\n$ awk '/^b/{print; print($NF>0 ? \"------\" : \"======\")}' table.txt\nbrown bread mat hair 42\n------\nblue cake mug shirt -7\n======\n```\n\n>![info](images/info.svg) See also [stackoverflow: finding min and max value of a column](https://stackoverflow.com/a/29784278) and [gawk manual: switch](https://www.gnu.org/software/gawk/manual/gawk.html#Switch-Statement).\n\n## loops\n\n`for` loops are handy when you are working with arrays. Also for processing input fields, since `$N` syntax allows passing an expression instead of just fixed values.\n\n```bash\n$ awk 'BEGIN{for(i=2; i<7; i+=2) print i}'\n2\n4\n6\n\n# looping each field\n$ awk -v OFS=, '{for(i=1; i<=NF; i++) if($i ~ /^[bm]/) $i=\"[\"$i\"]\"} 1' table.txt\n[brown],[bread],[mat],hair,42\n[blue],cake,[mug],shirt,-7\nyellow,[banana],window,shoes,3.14\n```\n\nHere's an example of looping over a dynamically constructed array.\n\n```bash\n$ cat marks.txt\nDept    Name    Marks\nECE     Raj     53\nECE     Joel    72\nEEE     Moi     68\nCSE     Surya   81\nEEE     Tia     59\nECE     Om      92\nCSE     Amy     67\n\n# average marks for each department\n$ awk 'NR>1{d[$1]+=$3; c[$1]++} END{for(k in d) print k, d[k]/c[k]}' marks.txt\nECE 72.3333\nEEE 63.5\nCSE 74\n```\n\nYou can use `break` and `continue` to alter the normal flow of loops. `break` will cause the current loop to quit immediately without processing the remaining statements and iterations. `continue` will skip the remaining statements in the loop and start the next iteration.\n\n```bash\n$ awk -v OFS=, '{for(i=1; i<=NF; i++) if($i ~ /b/){NF=i; break}} 1' table.txt\nbrown\nblue\nyellow,banana\n```\n\n>![info](images/info.svg) See also [stackoverflow: find missing numbers from sequential list](https://stackoverflow.com/q/38491676/4082052).\n\n`awk` supports the `while` and `do-while` loop mechanisms as well.\n\n```bash\n$ awk 'BEGIN{i=6; while(i>0){print i; i-=2}}'\n6\n4\n2\n\n# recursive substitution\n$ echo 'titillate' | awk '{while(gsub(/til/, \"\")) print}'\ntilate\nate\n$ echo 'titillate' | awk '{do{print} while(gsub(/til/, \"\"))}'\ntitillate\ntilate\nate\n```\n\n## next\n\n`next` is similar to the `continue` statement but it acts on the default loop that goes through the input records. It doesn't affect `BEGIN` or `END` blocks as they are outside the record looping. When `next` is executed, rest of the statements will be skipped and next input record will be fetched for processing.\n\n```bash\n$ awk '/\\<par/{print \"%% \" $0; next} {print /s/ ? \"X\" : \"Y\"}' anchors.txt\n%% sub par\nX\nY\nX\n%% cart part tart mart\n```\n\nYou'll see more examples with `next` in the coming chapters.\n\n## exit\n\nYou saw the use of `exit` earlier to quit early and avoid unnecessary processing of records. If an argument isn't passed, `awk` considers the command to have finished normally and the **exit status** will indicate success. You can pass a number argument for other cases.\n\n```bash\n$ seq 3542 4623452 | awk 'NR==2452{print; exit}'\n5993\n$ echo $?\n0\n\n$ awk '/^br/{print \"invalid data\"; exit 1}' table.txt\ninvalid data\n$ echo $?\n1\n\n# any remaining files to be processed are also skipped\n$ awk 'FNR==2{print; exit}' table.txt greeting.txt\nblue cake mug shirt -7\n```\n\nIf `exit` is used in `BEGIN` or normal blocks, any code in the `END` block will still be executed. For more details and corner cases, see [gawk manual: exit](https://www.gnu.org/software/gawk/manual/gawk.html#Exit-Statement).\n\n```bash\n# first print is executed\n# on seeing exit, rest of BEGIN and normal blocks are skipped\n# code in the END block is then executed\n$ awk 'BEGIN{print \"hi\"; exit; print \"hello\"}\n       /^b/;\n       END{print \"bye\"}' table.txt\nhi\nbye\n```\n\n## Summary\n\nThis chapter covered some of the control flow structures provided by `awk`. These features makes `awk` flexible and easier to use compared to `sed`.\n\nNext chapter will discuss some of the built-in functions.\n\n## Exercises\n\n>![info](images/info.svg) The [exercises](https://github.com/learnbyexample/learn_gnuawk/tree/master/exercises) directory has all the files used in this section.\n\n**1)** The input file `nums.txt` contains a single column of numbers. Change positive numbers to negative and vice versa. Solution should use the `sub` function and shouldn't explicitly use the `if-else` control structure or the ternary operator.\n\n```bash\n$ cat nums.txt\n42\n-2\n10101\n-3.14\n-75\n\n$ awk ##### add your solution here\n-42\n2\n-10101\n3.14\n75\n```\n\n**2)** For the input file `table.txt`, change the field separator from space to the `,` character. Also, any field not containing digit characters should be surrounded by double quotes.\n\n```bash\n$ awk ##### add your solution here\n\"brown\",\"bread\",\"mat\",\"hair\",42\n\"blue\",\"cake\",\"mug\",\"shirt\",-7\n\"yellow\",\"banana\",\"window\",\"shoes\",3.14\n```\n\n**3)** For each input line of the file `secrets.txt`, remove all characters except the last character of each field. Assume space as the input field separator.\n\n```bash\n$ cat secrets.txt\nstag area row tick\ndeaf chi rate tall glad\nBi tac toe - 42\n\n$ awk ##### add your solution here\ngawk\nfield\nice-2\n```\n\n**4)** For the input file `sample.txt`, emulate the `q` and `Q` commands of `sed` as shown below.\n\n```bash\n# sed '/are/q' sample.txt will print till the line containing 'are'\n$ awk ##### add your solution here\nHello World\n\nGood day\nHow are you\n\n# sed '/are/Q' sample.txt is similar to the 'q' command,\n# but the matching line won't be part of the output\n$ awk ##### add your solution here\nHello World\n\nGood day\n```\n\n**5)** For the input file `addr.txt`:\n\n* if a line contains `e`\n    * delete all occurrences of `e`\n    * surround all consecutive repeated characters with `{}`\n    * assume that the input will not have more than two consecutive repeats\n* if a line doesn't contain `e` but contains `u`\n    * surround all lowercase vowels in that line with `[]`\n\n```bash\n$ awk ##### add your solution here\nH{ll}o World\nHow ar you\nThis gam is g{oo}d\nT[o]d[a]y [i]s s[u]nny\n12345\nYou ar fu{nn}y\n```\n\n**6)** The goal is to print `found you` if the input file contains `you` and `not found` otherwise. However, both the `print` statements are executed in the `awk` code shown below. Change it to work as expected.\n\n```bash\n$ awk '/you/{print \"found you\"; exit} END{print \"not found\"}' addr.txt\nfound you\nnot found\n```\n\n# Built-in functions\n\nYou've already seen some built-in functions in detail, such as the `sub`, `gsub` and `gensub` functions. This chapter will discuss many more built-ins that are often used in one-liners. You'll also see more examples with arrays.\n\n>![info](images/info.svg) See [gawk manual: Functions](https://www.gnu.org/software/gawk/manual/gawk.html#Functions) for details about all the built-in functions as well as how to define your own functions.\n\n>![info](images/info.svg) The [example_files](https://github.com/learnbyexample/learn_gnuawk/tree/master/example_files) directory has all the files used in the examples.\n\n## length\n\nThe `length` function returns the number of characters for the given string argument. By default, it acts on the `$0` variable. Numeric arguments will be automatically converted to strings.\n\n```bash\n$ awk 'BEGIN{print length(\"road\"); print length(123456)}'\n4\n6\n\n# recall that the record separator isn't part of $0\n# so, line ending won't be counted here\n$ printf 'fox\\ntiger\\n' | awk '{print length()}'\n3\n5\n\n$ awk 'length($1) < 6' table.txt\nbrown bread mat hair 42\nblue cake mug shirt -7\n```\n\nThe `-b` command line option is handy if you need the number of bytes, instead of the number of characters. Locale also plays a role.\n\n```bash\n$ echo 'αλεπού' | awk '{print length()}'\n6\n$ echo 'αλεπού' | awk -b '{print length()}'\n12\n$ echo 'αλεπού' | LC_ALL=C awk '{print length()}'\n12\n```\n\n>![info](images/info.svg) For the above illustration, you can also use `match($0, /$/)-1` to get the byte count, irrespective of the locale or the use of the `-b` option. This solution was suggested in [this issue](https://github.com/learnbyexample/learn_gnuawk/issues/5).\n\n## Array sorting\n\nBy default, array looping with the `for(key in array)` format gives you elements in random order. By setting a special value to `PROCINFO[\"sorted_in\"]`, you can control the order in which you wish to retrieve the elements. See [gawk manual: Using Predefined Array Scanning Orders](https://www.gnu.org/software/gawk/manual/gawk.html#Controlling-Scanning) for other options and details.\n\n```bash\n# by default, array is traversed in random order\n$ awk 'BEGIN{a[\"z\"]=1; a[\"x\"]=12; a[\"b\"]=42; for(i in a) print i, a[i]}'\nx 12\nz 1\nb 42\n\n# index (i.e. keys) sorted in ascending order as strings\n$ awk 'BEGIN{PROCINFO[\"sorted_in\"] = \"@ind_str_asc\";\n       a[\"z\"]=1; a[\"x\"]=12; a[\"b\"]=42; for(i in a) print i, a[i]}'\nb 42\nx 12\nz 1\n\n# value sorted in ascending order as numbers\n$ awk 'BEGIN{PROCINFO[\"sorted_in\"] = \"@val_num_asc\";\n       a[\"z\"]=1; a[\"x\"]=12; a[\"b\"]=42; for(i in a) print i, a[i]}'\nz 1\nx 12\nb 42\n```\n\nHere's an example of sorting input lines in ascending order based on the second column, treating the data as strings.\n\n```bash\n$ awk 'BEGIN{PROCINFO[\"sorted_in\"] = \"@ind_str_asc\"}\n       {a[$2]=$0} END{for(k in a) print a[k]}' table.txt\nyellow banana window shoes 3.14\nbrown bread mat hair 42\nblue cake mug shirt -7\n```\n\n## split\n\nThe `split` function provides the same features as the record splitting done using `FS`. This is helpful when you need the results as an array for some reason, for example to use array sorting features. Or, when you need to further split a field content. `split` accepts four arguments, the last two being optional:\n\n* First argument is the string to be split\n* Second argument is the array variable that saves the results\n* Third argument is the separator, whose default is `FS`\n\nThe return value of the `split` function is number of fields, similar to the `NF` variable. The array gets indexed starting from `1` for the first element, `2` for the second element and so on. If the array already had some value, it gets overwritten with the new result.\n\n```bash\n# same as: awk '{print $2}'\n$ printf '     one \\t two\\t\\t\\tthree  ' | awk '{split($0, a); print a[2]}'\ntwo\n\n# example with both FS and split in action\n$ s='Joe,1996-10-25,64,78'\n$ echo \"$s\" | awk -F, '{split($2, d, \"-\"); print $1 \" was born in \" d[1]}'\nJoe was born in 1996\n\n# single row to multiple rows based on splitting the last field\n$ s='air,water,12:42:3'\n$ echo \"$s\" | awk -F, '{n=split($NF, a, \":\");\n                       for(i=1; i<=n; i++) print $1, $2, a[i]}'\nair water 12\nair water 42\nair water 3\n```\n\nSimilar to `FS`, you can use a regular expression as the separator.\n\n```bash\n$ s='Sample123string42with777numbers'\n$ echo \"$s\" | awk '{split($0, s, /[0-9]+/); print s[2], s[4]}'\nstring numbers\n```\n\nThe fourth argument provides a feature not present with `FS` splitting. It allows you to save the portions matched by the separator in an array.\n\n```bash\n$ s='Sample123string42with777numbers'\n$ echo \"$s\" | awk '{n=split($0, s, /[0-9]+/, seps);\n                   for(i=1; i<n; i++) print seps[i]}'\n123\n42\n777\n```\n\n>![info](images/info.svg) Quoting from [gawk manual: split()](https://www.gnu.org/software/gawk/manual/gawk.html#index-split_0028_0029-function-1):\n>\n>If `fieldsep` is a single space, then any leading whitespace goes into `seps[0]` and any trailing whitespace goes into `seps[n]`, where `n` is the return value of `split()` (i.e., the number of elements in `array`).\n\nHere's an example where `split` helps to initialize an array using an empty separator. Unlike `$N` syntax where an expression resulting in a floating-point number is acceptable, array index has to be an integer only. Hence, the `int` function is used to convert the floating-point result to an integer in the example below.\n\n```bash\n$ cat marks.txt\nDept    Name    Marks\nECE     Raj     53\nECE     Joel    72\nEEE     Moi     68\nCSE     Surya   81\nEEE     Tia     59\nECE     Om      92\nCSE     Amy     67\n\n# adds a new grade column based on marks in the third column\n$ awk 'BEGIN{OFS=\"\\t\"; split(\"DCBAS\", g, //)}\n       {$(NF+1) = NR==1 ? \"Grade\" : g[int($NF/10)-4]} 1' marks.txt\nDept    Name    Marks   Grade\nECE     Raj     53      D\nECE     Joel    72      B\nEEE     Moi     68      C\nCSE     Surya   81      A\nEEE     Tia     59      D\nECE     Om      92      S\nCSE     Amy     67      C\n```\n\n## patsplit\n\nThe `patsplit` function will give you the features provided by `FPAT`. The argument order and optional arguments is same as the `split` function, with `FPAT` as the default separator. The return value is number of fields obtained from the split.\n\n```bash\n$ s='eagle,\"fox,42\",bee,frog'\n\n$ echo \"$s\" | awk '{patsplit($0, a, /\"[^\"]*\"|[^,]*/); print a[2]}'\n\"fox,42\"\n```\n\n## substr\n\nThe `substr` function helps to extract a specified number of characters from an input string based on indexing. The argument order is:\n\n* First argument is the input string\n* Second argument is the starting position\n* Third argument is the number of characters to extract\n\nThe index starts from `1`. If the third argument is not specified, by default all characters until the end of the string is extracted. If the second argument is greater than the length of the string or if the third argument is less than or equal to `0`, then an empty string is returned. The second argument will be converted `1` if a number less than one is specified.\n\n\n```bash\n$ echo 'abcdefghij' | awk '{print substr($0, 1, 5)}'\nabcde\n$ echo 'abcdefghij' | awk '{print substr($0, 4, 3)}'\ndef\n\n$ echo 'abcdefghij' | awk '{print substr($0, 6)}'\nfghij\n\n$ echo 'abcdefghij' | awk -v OFS=: '{print substr($0, 2, 3), substr($0, 6, 3)}'\nbcd:fgh\n```\n\nIf only a few characters are needed from the input record, you can also use empty `FS`.\n\n```bash\n$ echo 'abcdefghij' | awk -v FS= '{print $3}'\nc\n$ echo 'abcdefghij' | awk -v FS= '{print $3, $5}'\nc e\n```\n\n## match\n\nThe `match` function is useful to extract portion of an input string matched by a regexp. There are two ways to get the matched portion:\n\n* by using the `substr` function along with special variables `RSTART` (starting position of the match) and `RLENGTH` (length of the match)\n* by passing a third argument to `match` so that the results are available from an array\n\nThe first argument to `match` is the input string and the second one is the regexp. If the match fails, then `RSTART` gets `0` and `RLENGTH` gets `-1`. Return value is same as `RSTART`.\n\n```bash\n$ s='051 035 154 12 26 98234 3'\n\n# using substr and RSTART/RLENGTH\n# match a number with >= 4 digits\n$ echo \"$s\" | awk 'match($0, /[0-9]{4,}/){print substr($0, RSTART, RLENGTH)}'\n98234\n\n# using array, note that index 0 is used here, not 1\n# match a number >= 100 (with optional leading zeros)\n$ echo \"$s\" | awk 'match($0, /0*[1-9][0-9]{2,}/, m){print m[0]}'\n154\n```\n\nBoth the above examples can also be easily solved using `FPAT` or `patsplit`. `match` has an advantage when it comes to getting portions matched only within capture groups. The first element of the array will still have the entire match. The second element will contain the portion matched by the first group, the third one will contain the portion matched by the second group and so on. See also [stackoverflow: arithmetic replacement in a text file](https://stackoverflow.com/q/62241101/4082052).\n\n```bash\n# entire matched portion\n$ echo 'apple=42, fig=314' | awk 'match($0, /fig=([0-9]+)/, m){print m[0]}'\nfig=314\n# matched portion of the first capture group\n$ echo 'apple=42, fig=314' | awk 'match($0, /fig=([0-9]+)/, m){print m[1]}'\n314\n```\n\nIf you need to get matching portions for all the matches instead of just the first match, you can use a loop and adjust the input string every iteration.\n\n```bash\n# extract numbers only if it is followed by a comma\n$ s='42 apple-5, fig3; x-83, y-20: f12'\n$ echo \"$s\" | awk '{ while( match($0, /([0-9]+),/, m) ){print m[1];\n                   $0=substr($0, RSTART+RLENGTH)} }'\n5\n83\n```\n\n## index\n\nThe `index` function is useful when you need to match a string literally. This is similar to the `grep -F` functionality of matching fixed strings. The first argument to this function is the input string and the second one is the string to be matched literally. The return value is the index of the matching location and `0` if there is no match.\n\n```bash\n$ cat eqns.txt\na=b,a-b=c,c*d\na+b,pi=3.14,5e12\ni*(t+9-g)/8,4-a+b\n\n# no output because the metacharacters aren't escaped\n$ awk '/i*(t+9-g)/' eqns.txt\n# same as: grep -F 'i*(t+9-g)' eqns.txt\n$ awk 'index($0, \"i*(t+9-g)\")' eqns.txt\ni*(t+9-g)/8,4-a+b\n\n# check only the last field\n$ awk -F, 'index($NF, \"a+b\")' eqns.txt\ni*(t+9-g)/8,4-a+b\n# index not needed if the entire field/line is being compared\n$ awk -F, '$1==\"a+b\"' eqns.txt\na+b,pi=3.14,5e12\n```\n\nThe return value is useful to ensure that the match is found at specific positions only. For example, the start or end of the string.\n\n```bash\n# start of string\n$ awk 'index($0, \"a+b\")==1' eqns.txt\na+b,pi=3.14,5e12\n\n# end of string\n$ awk -v s=\"a+b\" 'index($0, s)==length()-length(s)+1' eqns.txt\ni*(t+9-g)/8,4-a+b\n```\n\nRecall that the `-v` option gets parsed by `awk`'s string processing rules. So, if you need to pass a literal string without falling in backslash hell, use `ENVIRON` instead.\n\n```bash\n$ echo 'a\\b\\c\\d' | awk -v s='a\\b' 'index($0, s)'\n$ echo 'a\\b\\c\\d' | awk -v s='a\\\\b' 'index($0, s)'\na\\b\\c\\d\n$ echo 'a\\b\\c\\d' | s='a\\b' awk 'index($0, ENVIRON[\"s\"])'\na\\b\\c\\d\n```\n\n## system\n\nExternal commands can be issued using the `system` function. Any output generated by the external command would be as usual on `stdout` unless redirected while calling the command.\n\n```bash\n$ awk 'BEGIN{system(\"echo Hello World\")}'\nHello World\n\n$ wc table.txt\n 3 15 79 table.txt\n$ awk 'BEGIN{system(\"wc table.txt\")}'\n 3 15 79 table.txt\n\n$ awk 'BEGIN{system(\"seq 10 | paste -sd, > out.txt\")}'\n$ cat out.txt\n1,2,3,4,5,6,7,8,9,10\n\n$ cat t2.txt\nI bought two balls and 3 bats\n$ echo 'f1,t2,f3' | awk -F, '{system(\"cat \" $2 \".txt\")}'\nI bought two balls and 3 bats\n```\n\nThe return value of `system` depends on the exit status of the executed command. See [gawk manual: Input/Output Functions](https://www.gnu.org/software/gawk/manual/html_node/I_002fO-Functions.html) for details.\n\n```bash\n$ ls xyz.txt\nls: cannot access 'xyz.txt': No such file or directory\n$ echo $?\n2\n\n$ awk 'BEGIN{s=system(\"ls xyz.txt\"); print \"Exit status: \" s}'\nls: cannot access 'xyz.txt': No such file or directory\nExit status: 2\n```\n\n## printf and sprintf\n\nThe `printf` function is useful over the `print` function when you need to format the data before printing. Another difference is that `OFS` and `ORS` do not affect the `printf` function. The formatting features are similar to those found in the `C` programming language and the `printf` shell built-in command.\n\n```bash\n# OFMT controls the formatting for numbers displayed with the print function\n$ awk 'BEGIN{print OFMT}'\n%.6g\n$ awk 'BEGIN{sum = 3.1428 + 100; print sum}'\n103.143\n$ awk 'BEGIN{OFMT=\"%.5f\"; sum = 3.1428 + 100; print sum}'\n103.14280\n\n# using printf function\n# note the use of \\n as ORS isn't appended unlike print\n$ awk 'BEGIN{sum = 3.1428 + 10; printf \"%f\\n\", sum}'\n13.142800\n$ awk 'BEGIN{sum = 3.1428 + 10; printf \"%.3f\\n\", sum}'\n13.143\n```\n\nHere are some more formatting examples for floating-point numbers.\n\n```bash\n# total length is 10, filled with space if needed\n# [ and ] are used here for visualization purposes\n$ awk 'BEGIN{pi = 3.14159; printf \"[%10.3f]\\n\", pi}'\n[     3.142]\n$ awk 'BEGIN{pi = 3.14159; printf \"[%-10.3f]\\n\", pi}'\n[3.142     ]\n\n# zero filled\n$ awk 'BEGIN{pi = 3.14159; printf \"%010.3f\\n\", pi}'\n000003.142\n\n# scientific notation\n$ awk 'BEGIN{pi = 3.14159; printf \"%e\\n\", pi}'\n3.141590e+00\n```\n\nHere are some formatting examples for integers.\n\n```bash\n# note that there is no rounding\n$ awk 'BEGIN{printf \"%d\\n\", 1.99}'\n1\n\n# ensure there's always a sign prefixed for integers\n$ awk 'BEGIN{printf \"%+d\\n\", 100}'\n+100\n$ awk 'BEGIN{printf \"%+d\\n\", -100}'\n-100\n```\n\nHere are some formatting examples for strings.\n\n```bash\n# prefix remaining width with spaces\n$ awk 'BEGIN{printf \"|%10s|\\n\", \"mango\"}'\n|     mango|\n\n# suffix remaining width with spaces\n$ awk 'BEGIN{printf \"|%-10s|\\n\", \"mango\"}'\n|mango     |\n\n# truncate\n$ awk '{printf \"%.4s\\n\", $0}' table.txt\nbrow\nblue\nyell\n```\n\nYou can also refer to an argument using `N$` format, where `N` is the positional number of argument. One advantage with this method is that you can reuse an argument any number of times. You cannot mix this format with the normal way.\n\n```bash\n$ awk 'BEGIN{printf \"%1$d + %2$d * %1$d = %3$d\\n\", 3, 4, 15}'\n3 + 4 * 3 = 15\n# remove # if you do not need the prefix\n$ awk 'BEGIN{printf \"hex=%1$#x\\noct=%1$#o\\ndec=%1$d\\n\", 15}'\nhex=0xf\noct=017\ndec=15\n```\n\nYou can pass variables by specifying a `*` instead of a number in the formatting string.\n\n```bash\n# same as: awk 'BEGIN{pi = 3.14159; printf \"%010.3f\\n\",  pi}'\n$ awk 'BEGIN{d=10; p=3; pi = 3.14159; printf \"%0*.*f\\n\", d, p, pi}'\n000003.142\n```\n\n>![warning](images/warning.svg) Passing a variable directly to `printf` without using a format specifier can result in an error depending upon the contents of the variable.\n>\n> ```bash\n> $ awk 'BEGIN{s=\"solve: 5 % x = 1\"; printf s}'\n> awk: cmd. line:1: fatal: not enough arguments to satisfy format string\n>         `solve: 5 % x = 1'\n>                    ^ ran out for this one\n> ```\n\nSo, as a good practice, always use variables with an appropriate format instead of passing it directly to `printf`.\n\n```bash\n$ awk 'BEGIN{s=\"solve: 5 % x = 1\"; printf \"%s\\n\", s}'\nsolve: 5 % x = 1\n```\n\nIf `%` has to be used literally inside the format specifier, use `%%`. This is similar to using `\\\\` in regexps to represent `\\` literally.\n\n```bash\n$ awk 'BEGIN{printf \"n%%d gives the remainder\\n\"}'\nn%d gives the remainder\n```\n\nTo save the results of the formatting in a variable instead of printing, use the `sprintf` function. Unlike `printf`, parentheses are always required to use this function.\n\n```bash\n$ awk 'BEGIN{pi = 3.14159; s = sprintf(\"%010.3f\", pi); print s}'\n000003.142\n```\n\n>![info](images/info.svg) See [gawk manual: printf](https://www.gnu.org/software/gawk/manual/html_node/Printf.html) for complete list of formatting options and other details.\n\n## Redirecting print output\n\nThe results from the `print` and `printf` functions can be redirected to a shell command or a file instead of `stdout`. There's nothing special about it, you could have done it using shell redirections as well. The use case arises when you need to redirect only a specific portion or if you need multiple redirections within the same `awk` command. Here are some examples of redirecting to multiple files.\n\n```bash\n$ seq 6 | awk 'NR%2{print > \"odd.txt\"; next} {print > \"even.txt\"}'\n$ cat odd.txt\n1\n3\n5\n$ cat even.txt\n2\n4\n6\n\n# dynamically creating filenames\n$ awk -v OFS='\\t' 'NR>1{print $2, $3 > $1\".txt\"}' marks.txt\n# output for one of the departments\n$ cat ECE.txt\nRaj     53\nJoel    72\nOm      92\n```\n\nNote that the use of `>` doesn't mean that the file will get overwritten everytime. That happens only once if the file already existed prior to executing the `awk` command. Use `>>` if you wish to append to already existing files.\n\nAs seen in the above examples, the filenames are passed as string expressions. To redirect to a shell command, again you need to pass a string expression after the `|` pipe symbol. Here's an example:\n\n```bash\n$ awk '{print $2 | \"paste -sd,\"}' table.txt\nbread,cake,banana\n```\n\nAnd here are some examples with multiple redirections.\n\n```bash\n$ awk '{print $2 | \"sort | paste -sd,\"}' table.txt\nbanana,bread,cake\n\n# sort the output before writing to files\n$ awk -v OFS='\\t' 'NR>1{print $2, $3 | \"sort > \"$1\".txt\"}' marks.txt\n# output for one of the departments\n$ cat ECE.txt\nJoel    72\nOm      92\nRaj     53\n```\n\n>![info](images/info.svg) See [gawk manual: Redirecting Output of print and printf](https://www.gnu.org/software/gawk/manual/gawk.html#Redirection) for more details and operators on redirections. And see [gawk manual: Closing Input and Output Redirections](https://www.gnu.org/software/gawk/manual/gawk.html#Close-Files-And-Pipes) if you have too many redirections.\n\n## Summary\n\nThis chapter covered some of the built-in functions provided by `awk`. Do check the manual for more of them, for example math and time related functions.\n\nNext chapter will cover features related to processing multiple files passed as input to `awk`.\n\n## Exercises\n\n>![info](images/info.svg) The [exercises](https://github.com/learnbyexample/learn_gnuawk/tree/master/exercises) directory has all the files used in this section.\n\n>![info](images/info.svg) Exercises will also include functions and features not discussed in this chapter. Refer to [gawk manual: Functions](https://www.gnu.org/software/gawk/manual/gawk.html#Functions) for details.\n\n**1)** For the input file `scores.csv`, sort the rows in descending order based on the values in the Physics column. Header should be retained as the first line in the output.\n\n```bash\n$ awk ##### add your solution here\nName,Maths,Physics,Chemistry\nIth,100,100,100\nCy,97,98,95\nLin,78,83,80\nEr,56,79,92\nOrt,68,72,66\nBlue,67,46,99\n```\n\n**2)** For the input file `nums3.txt`, calculate the square root of numbers and display the results in two different formats as shown below. First, with four digits after the fractional point and then in the scientific notation, again with four digits after the fractional point. Assume that the input has only a single column of positive numbers.\n\n```bash\n$ cat nums3.txt \n3.14\n4201\n777\n0323012\n\n$ awk ##### add your solution here\n1.7720\n64.8151\n27.8747\n568.3414\n\n$ awk ##### add your solution here\n1.7720e+00\n6.4815e+01\n2.7875e+01\n5.6834e+02\n```\n\n**3)** For the input file `items.txt`, assume space as the field separator. From the second field, remove the second `:` character and the number that follows. Modify the last field by multiplying it by the number that was deleted from the second field.\n\n```bash\n$ cat items.txt\napple rxg:12:-425 og 6.2\nfig zwt:3.64:12.89e2 ljg 5\nbanana ysl:42:3.14 vle 45\n\n$ awk ##### add your solution here\napple rxg:12 og -2635\nfig zwt:3.64 ljg 6445\nbanana ysl:42 vle 141.3\n```\n\n**4)** For the input file `sum.txt`, assume space as the field separator. Replace the second field with the sum of the two numbers embedded in it. The numbers can be positive/negative integers or floating-point numbers but not scientific notation.\n\n```bash\n$ cat sum.txt\nf2:z3 kt//-42\\\\3.14//tw 5y6\nt5:x7 qr;wq<=>+10{-8764.124}yb u9\napple:fig 100:32 9j4\n\n$ awk ##### add your solution here\nf2:z3 -38.86 5y6\nt5:x7 -8754.12 u9\napple:fig 132 9j4\n```\n\n**5)** For the given input strings, extract portion of the line starting from the matching location specified by the shell variable `s` till the end of the line. If there is no match, do not print that line. The contents of `s` should be matched literally.\n\n```bash\n$ s='(a^b)'\n$ echo '3*f + (a^b) - 45' | ##### add your solution here\n(a^b) - 45\n\n$ s='\\&/'\n# should be no output for this input\n$ echo 'f\\&z\\&2.14' | ##### add your solution here\n# but this one has a match\n$ echo 'f\\&z\\&/2.14' | ##### add your solution here\n\\&/2.14\n```\n\n**6)** Extract all positive integers preceded by `-` and followed by `:` or `;`. Display the matching portions separated by a newline character.\n\n```bash\n$ s='42 apple-5; fig3; x-83, y-20:-34; f12'\n$ echo \"$s\" | awk ##### add your solution here\n5\n20\n34\n```\n\n**7)** For the input file `scores.csv`, calculate the average score for each row. Those with average greater than or equal to `80` should be saved in `pass.csv` and the rest in `fail.csv`. The output files should have the names followed by a tab character, and finally the average score (two decimal points).\n\n```bash\n$ awk ##### add your solution here\n\n$ cat fail.csv\nBlue    70.67\nEr      75.67\nOrt     68.67\n$ cat pass.csv\nLin     80.33\nCy      96.67\nIth     100.00\n```\n\n**8)** For the input file `files.txt`, replace lines starting with a space with the output of that line executed as a shell command.\n\n```bash\n$ cat files.txt\n sed -n '2p' addr.txt\n-----------\n wc -w sample.txt\n===========\n awk '{print $1}' table.txt\n-----------\n\n$ awk ##### add your solution here\nHow are you\n-----------\n31 sample.txt\n===========\nbrown\nblue\nyellow\n-----------\n```\n\n**9)** For the input file `fw.txt`, format the last column in scientific notation with two digits after the decimal point.\n\n```bash\n$ awk ##### add your solution here\n1.3  rs   90  1.35e-01\n3.8           6.00e+00\n5.2  ye       8.24e+00\n4.2  kt   32  4.51e+01\n```\n\n**10)** For the input file `addr.txt`, display all lines containing `e` or `u` but not both.\n\n>![info](images/info.svg) Hint — [gawk manual: Bit-Manipulation Functions](https://www.gnu.org/software/gawk/manual/gawk.html#Bitwise-Functions).\n\n```bash\n$ awk ##### add your solution here\nHello World\nThis game is good\nToday is sunny\n```\n\n**11)** For the input file `patterns.txt`, filter lines containing `[5]` at the start of a line. The search term should be matched literally.\n\n```bash\n$ awk ##### add your solution here\n[5]*3\n```\n\n**12)** For the input file `table.txt`, uppercase the third field.\n\n```bash\n$ awk ##### add your solution here\nbrown bread MAT hair 42\nblue cake MUG shirt -7\nyellow banana WINDOW shoes 3.14\n```\n\n**13)** For the input files `patterns.txt` and `sum.txt`, match lines containing the literal value stored in the `s` variable. Assume that the `s` variable has regexp metacharacters.\n\n```bash\n$ s='[5]'\n##### add your solution here\n(9-2)*[5]\n[5]*3\n\n$ s='\\\\'\n##### add your solution here\nf2:z3 kt//-42\\\\3.14//tw 5y6\n```\n\n# Multiple file input\n\nYou have already seen blocks like `BEGIN`, `END` and statements like `next`. This chapter will discuss features that are useful to make decisions around each file when there are multiple files passed as input.\n\n>![info](images/info.svg) The [example_files](https://github.com/learnbyexample/learn_gnuawk/tree/master/example_files) directory has all the files used in the examples.\n\n## BEGINFILE, ENDFILE and FILENAME\n\n* `BEGINFILE` — this block gets executed before the start of each input file\n* `ENDFILE` — this block gets executed after processing each input file\n* `FILENAME` — special variable having the filename of the current input file\n\nHere are some examples:\n\n```bash\n# can also use: awk 'BEGINFILE{printf \"--- %s ---\\n\", FILENAME} 1'\n$ awk 'BEGINFILE{print \"--- \" FILENAME \" ---\"} 1' greeting.txt table.txt\n--- greeting.txt ---\nHi there\nHave a nice day\nGood bye\n--- table.txt ---\nbrown bread mat hair 42\nblue cake mug shirt -7\nyellow banana window shoes 3.14\n\n# same as: tail -q -n1 greeting.txt table.txt\n$ awk 'ENDFILE{print $0}' greeting.txt table.txt\nGood bye\nyellow banana window shoes 3.14\n```\n\n## nextfile\n\nThe `nextfile` statement helps to skip the remaining records from the current file being processed and move on to the next file. Note that the `ENDFILE` block will still be executed, if present.\n\n```bash\n# print filename if it contains 'I' anywhere in the file\n# same as: grep -l 'I' f[1-3].txt greeting.txt\n$ awk '/I/{print FILENAME; nextfile}' f[1-3].txt greeting.txt\nf1.txt\nf2.txt\n\n# print filename if it contains both 'o' and 'at' anywhere in the file\n$ awk 'BEGINFILE{m1=m2=0} /o/{m1=1} /at/{m2=1}\n       m1 && m2{print FILENAME; nextfile}' f[1-3].txt greeting.txt\nf2.txt\nf3.txt\n\n# print filename if it contains 'at' but not 'o'\n$ awk 'BEGINFILE{m1=m2=0} /o/{m1=1; nextfile} /at/{m2=1}\n       ENDFILE{if(!m1 && m2) print FILENAME}' f[1-3].txt greeting.txt\nf1.txt\n```\n\n>![warning](images/warning.svg) `nextfile` cannot be used in the `BEGIN` or `END` or `ENDFILE` blocks. See [gawk manual: nextfile](https://www.gnu.org/software/gawk/manual/gawk.html#Nextfile-Statement) for more details, how it affects `ENDFILE` and other special cases.\n\n## ARGC and ARGV\n\nThe `ARGC` special variable contains the total number of arguments passed to the `awk` command, including `awk` itself as an argument. The `ARGV` special array contains the arguments themselves.\n\n```bash\n# note that the index starts with '0' here\n$ awk 'BEGIN{for(i=0; i<ARGC; i++) print ARGV[i]}' f[1-3].txt greeting.txt\nawk\nf1.txt\nf2.txt\nf3.txt\ngreeting.txt\n```\n\nSimilar to manipulating `NF` and modifying `$N` field contents, you can change the values of `ARGC` and `ARGV` to control how the arguments should be processed.\n\nHowever, not all arguments are necessarily filenames. `awk` allows assigning variable values without `-v` option if it is done in the place where you usually provide file arguments. For example:\n\n```bash\n$ awk 'BEGIN{for(i=0; i<ARGC; i++) print ARGV[i]}' table.txt n=5 greeting.txt\nawk\ntable.txt\nn=5\ngreeting.txt\n```\n\nIn the above example, the variable `n` will get a value of `5` after `awk` has finished processing the `table.txt` file. Here's an example where `FS` is changed between two files.\n\n```bash\n$ cat table.txt\nbrown bread mat hair 42\nblue cake mug shirt -7\nyellow banana window shoes 3.14\n$ cat books.csv\nHarry Potter,Mistborn,To Kill a Mocking Bird\nMatilda,Castle Hangnail,Jane Eyre\n\n# for table.txt, FS will be the default value\n# for books.csv, FS will be the comma character\n# OFS is comma for both the files\n$ awk -v OFS=, 'NF=2' table.txt FS=, books.csv\nbrown,bread\nblue,cake\nyellow,banana\nHarry Potter,Mistborn\nMatilda,Castle Hangnail\n```\n\n>![info](images/info.svg) See [stackoverflow: extract positions 2-7 from a fasta sequence](https://stackoverflow.com/a/64427745/4082052) for a practical example of changing field/record separators between the files being processed.\n\n## Summary\n\nThis chapter introduced few more special blocks and variables are that handy for processing multiple file inputs. These will show up in examples in the coming chapters as well.\n\nNext chapter will discuss use cases where you need to take decisions based on multiple input records.\n\n## Exercises\n\n>![info](images/info.svg) The [exercises](https://github.com/learnbyexample/learn_gnuawk/tree/master/exercises) directory has all the files used in this section.\n\n**1)** Print the last field of the first two lines for the input files `table.txt`, `scores.csv` and `fw.txt`. The field separators for these files are space, comma and fixed width respectively. To make the output more informative, print filenames and a separator as shown in the output below. Assume that the input files will have at least two lines.\n\n```bash\n$ awk ##### add your solution here\n>table.txt<\n42\n-7\n----------\n>scores.csv<\nChemistry\n99\n----------\n>fw.txt<\n0.134563\n6\n----------\n```\n\n**2)** For the input files `sample.txt`, `secrets.txt`, `addr.txt` and `table.txt`, display only the names of files that contain `at` or `fun` in the third field. Assume space as the field separator.\n\n```bash\n$ awk ##### add your solution here sample.txt secrets.txt addr.txt table.txt\nsecrets.txt\naddr.txt\ntable.txt\n```\n\n# Processing multiple records\n\nOften, you need to consider multiple lines at a time to make a decision, such as the paragraph mode examples seen earlier. Sometimes, you need to match a particular record and then get records surrounding the matched record. The `condX{actionX}` shortcut makes it easy to code state machines concisely, which is useful to solve such multiple record use cases. See [softwareengineering: FSM examples](https://softwareengineering.stackexchange.com/questions/47806/examples-of-finite-state-machines) if you are not familiar with state machines.\n\n>![info](images/info.svg) The [example_files](https://github.com/learnbyexample/learn_gnuawk/tree/master/example_files) directory has all the files used in the examples.\n\n## Processing consecutive records\n\nYou might need to define a condition that should satisfy something for one record and something else for the very next record. `awk` does provide a feature to get next record, but that could get complicated (see the [getline](#getline) section). Instead, you can simply save relevant records in variables/arrays and then create the required conditional expression when you have all the required records available. The default behavior of uninitialized variable to act as `0` in numerical context and empty in string context plays a role too.\n\n```bash\n# match and print two consecutive records\n# the first record should contain 'he' and the second one should contain 'you'\n$ awk 'p ~ /he/ && /you/{print p ORS $0} {p=$0}' para.txt\nHi there\nHow are you\n\n# same filtering as above, but print only the first record\n$ awk 'p ~ /he/ && /you/{print p} {p=$0}' para.txt\nHi there\n\n# same filtering as above, but print only the second record\n$ awk 'p ~ /he/ && /you/; {p=$0}' para.txt\nHow are you\n```\n\n## Context matching\n\nSometimes you want not just the matching records, but the records relative to the matches as well. For example, it could be to see the comments at the start of a function block that was matched while searching a program file. Or, it could be to see extended information from a log file while searching for a particular error message.\n\nConsider this sample input file:\n\n```bash\n$ cat context.txt\nblue\n    toy\n    flower\n    sand stone\nlight blue\n    flower\n    sky\n    water\nlanguage\n    english\n    hindi\n    spanish\n    tamil\nprogramming language\n    python\n    kotlin\n    ruby\n```\n\n**Case 1:** Here's an example that emulates the `grep --no-group-separator -A<n>` functionality. The `n && n--` trick used in the example below works like this:\n\n* If initially `n=2`, then we get\n    * `2 && 2` — evaluates to `true` and `n` becomes `1`\n    * `1 && 1` — evaluates to `true` and `n` becomes `0`\n    * `0 && ` — evaluates to `false` and `n` doesn't change\n* Note that when conditionals are connected with logical `&&`, the second expression will not be executed at all if the first one turns out to be `false` because the overall result will always be `false`. Same is the case if the first expression evaluates to `true` with the logical `||` operator. Such logical operators are also known as **short-circuit** operators. Thus, in the above case, `n--` won't be executed when `n` is `0` on the left hand side. This prevents `n` going negative and `n && n--` will never become `true` unless `n` is assigned again.\n\n```bash\n# same as: grep --no-group-separator -A1 'blue'\n# print the matching line as well as the one that follows it\n$ awk '/blue/{n=2} n && n--' context.txt\nblue\n    toy\nlight blue\n    flower\n\n# overlapping example, n gets re-assigned before reaching 0\n$ awk '/toy|flower/{n=2} n && n--{print NR, $0}' context.txt\n2     toy\n3     flower\n4     sand stone\n6     flower\n7     sky\n\n# doesn't allow overlapping cases to re-assign the counter\n$ awk '!n && /toy|flower/{n=2} n && n--{print NR, $0}' context.txt\n2     toy\n3     flower\n6     flower\n7     sky\n```\n\nOnce you've understood the above examples, the rest of the examples in this section should be easier to comprehend. They are all variations of the logic used above and re-arranged to solve the use case being discussed.\n\n**Case 2:** Print `n` records after match. This is similar to previous case, except that the matching record isn't printed.\n\n```bash\n# print 1 line after the matching line\n# for overlapping cases, n gets re-assigned before reaching 0\n$ awk 'n && n--; /language/{n=1}' context.txt\n    english\n    python\n\n# print 2 lines after the matching line\n# doesn't allow overlapping cases to re-assign the counter\n$ awk '!n && /toy|flower/{n=2; next} n && n--' context.txt\n    flower\n    sand stone\n    sky\n    water\n```\n\n**Case 3:** Here's how to print the `n`th record after the matching record.\n\n```bash\n# print only the 2nd line found after the matching line\n# the array saves the matching result for each record\n# doesn't rely on a counter, thus works for overlapping cases\n# same as: awk -v n=2 'a[NR-n]; /toy|flower/{a[NR]=1}'\n$ awk -v n=2 'NR in a; /toy|flower/{a[NR+n]}' context.txt\n    sand stone\nlight blue\n    water\n\n# print only the 3rd line found after matching line\n# n && !--n will be true only when --n yields 0\n# overlapping cases won't work as n gets re-assigned before going to 0\n$ awk 'n && !--n; /language/{n=3}' context.txt\n    spanish\n    ruby\n```\n\n**Case 4:** Print `n` records before the match. Printing the matching record as well is left as an exercise. Since the file is being read in forward direction, and the problem statement is to print something before the matching record, overlapping situation like the previous examples doesn't occur.\n\n```bash\n# i>0 is used because NR starts from 1\n$ awk -v n=2 '/toy|flower/{for(i=NR-n; i<NR; i++) if(i>0) print a[i]}\n              {a[NR]=$0}' context.txt\nblue\nblue\n    toy\n    sand stone\nlight blue\n```\n\n**Case 5:** Print `n`th record before the matching record.\n\n```bash\n# if the count is small enough, you can save them in variables\n# this one prints the 2nd line before the matching line\n# NR>2 is needed as first 2 records shouldn't be considered for a match\n$ awk 'NR>2 && /toy|flower/{print p2} {p2=p1; p1=$0}' context.txt\nblue\n    sand stone\n\n# else, use an array to save previous records\n$ awk -v n=4 'NR>n && /age/{print a[NR-n]} {a[NR]=$0}' context.txt\nlight blue\n    english\n```\n\n## Records bounded by distinct markers\n\nThis section will cover cases where the input file will always contain the same number of starting and ending patterns, arranged in an alternating fashion. For example, there cannot be two starting patterns appearing without an ending pattern between them and vice versa. Lines of text inside and between such groups are optional.\n\nThe sample file shown below will be used to illustrate examples in this section. For simplicity, assume that the starting pattern is marked by `start` and the ending pattern by `end`. They have also been given group numbers to make it easier to analyze the output.\n\n```bash\n$ cat uniform.txt\nmango\nicecream\n--start 1--\n1234\n6789\n**end 1**\nhow are you\nhave a nice day\n--start 2--\na\nb\nc\n**end 2**\npar,far,mar,tar\n```\n\n**Case 1:** Processing all the groups of records based on the distinct markers, including the records matched by markers themselves. For simplicity, the below command will just print all such records.\n\n```bash\n$ awk '/start/{f=1} f; /end/{f=0}' uniform.txt\n--start 1--\n1234\n6789\n**end 1**\n--start 2--\na\nb\nc\n**end 2**\n```\n\n>![info](images/info.svg) Similar to `sed -n '/start/,/end/p'` you can also use `awk '/start/,/end/'` but the state machine format is more suitable for the various cases to follow.\n\n**Case 2:** Processing all the groups of records but excluding the records matched by markers themselves.\n\n```bash\n$ awk '/end/{f=0} f{print \"*\", $0} /start/{f=1}' uniform.txt\n* 1234\n* 6789\n* a\n* b\n* c\n```\n\n**Case 3-4:** Processing all the groups of records but excluding one of the markers.\n\n```bash\n$ awk '/start/{f=1} /end/{f=0} f' uniform.txt\n--start 1--\n1234\n6789\n--start 2--\na\nb\nc\n\n$ awk 'f; /start/{f=1} /end/{f=0}' uniform.txt\n1234\n6789\n**end 1**\na\nb\nc\n**end 2**\n```\n\nThe next four cases are obtained by just using `!f` instead of `f` from the cases shown above.\n\n**Case 5:** Processing all input records except the groups of records bound by the markers.\n\n```bash\n$ awk '/start/{f=1} !f{print $0 \".\"} /end/{f=0}' uniform.txt\nmango.\nicecream.\nhow are you.\nhave a nice day.\npar,far,mar,tar.\n```\n\n**Case 6** Processing all input records except the groups of records between the markers.\n\n```bash\n$ awk '/end/{f=0} !f; /start/{f=1}' uniform.txt\nmango\nicecream\n--start 1--\n**end 1**\nhow are you\nhave a nice day\n--start 2--\n**end 2**\npar,far,mar,tar\n```\n\n**Case 7-8:** Similar to case 6, but include only one of the markers.\n\n```bash\n$ awk '!f; /start/{f=1} /end/{f=0}' uniform.txt\nmango\nicecream\n--start 1--\nhow are you\nhave a nice day\n--start 2--\npar,far,mar,tar\n\n$ awk '/start/{f=1} /end/{f=0} !f' uniform.txt\nmango\nicecream\n**end 1**\nhow are you\nhave a nice day\n**end 2**\npar,far,mar,tar\n```\n\n## Specific blocks\n\nInstead of working with all the groups (or blocks) bound by the markers, this section will discuss how to choose blocks based on an additional criteria.\n\nHere's how you can process only the first matching block.\n\n```bash\n$ awk '/start/{f=1} f; /end/{exit}' uniform.txt\n--start 1--\n1234\n6789\n**end 1**\n\n# use other tricks discussed in previous section as needed\n$ awk '/end/{exit} f; /start/{f=1}' uniform.txt\n1234\n6789\n```\n\nGetting last block alone involves lot more work, unless you happen to know how many blocks are present in the input file.\n\n```bash\n# reverse input linewise, change the order of comparison, reverse again\n# might not work if RS has to be something other than newline\n$ tac uniform.txt | awk '/end/{f=1} f; /start/{exit}' | tac\n--start 2--\na\nb\nc\n**end 2**\n\n# or, save the blocks in a buffer and print the last one alone\n$ awk '/start/{f=1; b=$0; next} f{b=b ORS $0} /end/{f=0}\n       END{print b}' uniform.txt\n--start 2--\na\nb\nc\n**end 2**\n```\n\nOnly the `n`th block.\n\n```bash\n# can also use: awk -v n=2 '/4/{c++} c==n{print; if(/6/) exit}'\n$ seq 30 | awk -v n=2 '/4/{c++} c==n; /6/ && c==n{exit}'\n14\n15\n16\n```\n\nAll blocks greater than `n`th block.\n\n```bash\n$ seq 30 | awk -v n=1 '/4/{f=1; c++} f && c>n; /6/{f=0}'\n14\n15\n16\n24\n25\n26\n```\n\nExcluding the `n`th block.\n\n```bash\n$ seq 30 | awk -v n=2 '/4/{f=1; c++} f && c!=n; /6/{f=0}'\n4\n5\n6\n24\n25\n26\n```\n\nAll blocks, only if the records between the markers match an additional condition.\n\n```bash\n# additional condition here is a record with entire content as '15'\n$ seq 30 | awk '/4/{f=1; buf=$0; m=0; next}\n                f{buf=buf ORS $0}\n                /6/{f=0; if(m) print buf}\n                $0==\"15\"{m=1}'\n14\n15\n16\n```\n\n## Broken blocks\n\nSometimes, you can have markers in random order and mixed in different ways. In such cases, to work with blocks without any other marker present in between them, the buffer approach comes in handy again.\n\n```bash\n$ cat broken.txt\nqqqqqqqqqqqqqqqq\nerror 1\nhi\nerror 2\n1234\n6789\nstate 1\nbye\nstate 2\nerror 3\nxyz\nerror 4\nabcd\nstate 3\nzzzzzzzzzzzzzzzz\n\n$ awk '/error/{f=1; buf=$0; next}\n       f{buf=buf ORS $0}\n       /state/{if(f) print buf; f=0}' broken.txt\nerror 2\n1234\n6789\nstate 1\nerror 4\nabcd\nstate 3\n```\n\n## Summary\n\nThis chapter covered various examples of working with multiple records. State machines play an important role in deriving solutions for such cases. Knowing various corner cases is also crucial, otherwise a solution that works for one input may fail for others.\n\nNext chapter will discuss use cases where you need to process a file input based on contents of another file.\n\n## Exercises\n\n>![info](images/info.svg) The [exercises](https://github.com/learnbyexample/learn_gnuawk/tree/master/exercises) directory has all the files used in this section.\n\n**1)** For the input file `sample.txt`, print lines containing `do` only if the previous line is empty and the line before that contains `you`.\n\n```bash\n$ awk ##### add your solution here\nJust do-it\nMuch ado about nothing\n```\n\n**2)** For the input file `sample.txt`, match lines containing `do` or `not` case insensitively. Each of these terms occur multiple times in the file. The goal is to print only the second occurrences of these terms (independent of each other).\n\n```bash\n$ awk ##### add your solution here\nNo doubt you like it too\nMuch ado about nothing\n```\n\n**3)** For the input file `sample.txt`, print the matching lines containing `are` or `bit` as well as `n` lines around the matching lines. The value for `n` is passed to the `awk` command via the `-v` option.\n\n```bash\n$ awk -v n=1 ##### add your solution here\nGood day\nHow are you\n\nToday is sunny\nNot a bit funny\nNo doubt you like it too\n\n# note that the first and last line are empty for this case\n$ awk -v n=2 ##### add your solution here\n\nGood day\nHow are you\n\nJust do-it\n\nToday is sunny\nNot a bit funny\nNo doubt you like it too\n\n```\n\n**4)** For the input file `broken.txt`, print all lines between the markers `top` and `bottom`. The first `awk` command shown below doesn't work because it is matching till the end of file as the second marker isn't found. Assume that the input file cannot have two `top` markers without a `bottom` marker appearing in between and vice-versa.\n\n```bash\n$ cat broken.txt\ntop\n3.14\nbottom\n---\ntop\n1234567890\nbottom\ntop\nHi there\nHave a nice day\nGood bye\n\n# wrong output\n$ awk '/bottom/{f=0} f; /top/{f=1}' broken.txt\n3.14\n1234567890\nHi there\nHave a nice day\nGood bye\n\n# expected output\n##### add your solution here\n3.14\n1234567890\n```\n\n**5)** For the input file `concat.txt`, extract contents from a line starting with ``### `` until but not including the next such line. The block to be extracted is indicated by the variable `n` passed via the `-v` option.\n\n```bash\n$ cat concat.txt\n### addr.txt\nHow are you\nThis game is good\nToday is sunny\n### broken.txt\ntop\n1234567890\nbottom\n### sample.txt\nJust do-it\nBelieve it\n### mixed_fs.txt\npink blue white yellow\ncar,mat,ball,basket\n\n$ awk -v n=2 ##### add your solution here\n### broken.txt\ntop\n1234567890\nbottom\n\n$ awk -v n=4 ##### add your solution here\n### mixed_fs.txt\npink blue white yellow\ncar,mat,ball,basket\n```\n\n**6)** For the input file `ruby.md`, replace all occurrences of `ruby` (irrespective of case) with `Ruby`. But, do not replace any matches between ` ```ruby ` and ` ``` ` lines (`ruby` in these markers shouldn't be replaced either). Save the output in `out.md`.\n\n```bash\n$ awk ##### add your solution here ruby.md > out.md\n$ diff -sq out.md expected.md \nFiles out.md and expected.md are identical\n```\n\n**7)** For the input file `lines.txt`, delete the line that comes after a whole line containing `---`. Assume that such lines won't occur consecutively.\n\n```bash\n$ cat lines.txt\nGo There\ncome on\ngo there\n---\n2 apples and 5 mangoes\ncome on!\n---\n2 Apples\nCOME ON\n\n$ awk ##### add your solution here\nGo There\ncome on\ngo there\n---\ncome on!\n---\nCOME ON\n```\n\n**8)** For the input file `result.csv`, use `---` to separate entries with the same name in the first column. Assume that the lines with the same first column value will always be next to each other.\n\n```bash\n$ awk ##### add your solution here\nAmy,maths,89\nAmy,physics,75\n---\nJoe,maths,79\n---\nJohn,chemistry,77\nJohn,physics,91\n---\nMoe,maths,81\n---\nRavi,physics,84\nRavi,chemistry,70\n---\nYui,maths,92\n```\n\n# Two file processing\n\nThis chapter focuses on solving problems which depend upon the contents of two or more files. These are usually based on comparing records and fields. Sometimes, record number plays a role too. You'll also learn about the `getline` built-in function.\n\n>![info](images/info.svg) The [example_files](https://github.com/learnbyexample/learn_gnuawk/tree/master/example_files) directory has all the files used in the examples.\n\n## Comparing records\n\nConsider the following input files which will be compared line wise to get the common and unique lines.\n\n```bash\n$ cat colors_1.txt\nteal\nlight blue\ngreen\nyellow\n$ cat colors_2.txt\nlight blue\nblack\ndark green\nyellow\n```\n\nThe *key* features used in the solution below:\n\n* For two files as input, `NR==FNR` will be `true` only when the first file is being processed\n* `next` will skip rest of the script and fetch the next record\n* `a[$0]` by itself is a valid statement. It will create an uninitialized element in array `a` with `$0` as the key (assuming the key doesn't exist yet)\n* `$0 in a` checks if the given string (`$0` here) exists as a key in the array `a`\n\n```bash\n# common lines\n# same as: grep -Fxf colors_1.txt colors_2.txt\n$ awk 'NR==FNR{a[$0]; next} $0 in a' colors_1.txt colors_2.txt\nlight blue\nyellow\n\n# lines from colors_2.txt not present in colors_1.txt\n# same as: grep -vFxf colors_1.txt colors_2.txt\n$ awk 'NR==FNR{a[$0]; next} !($0 in a)' colors_1.txt colors_2.txt\nblack\ndark green\n\n# reversing the order of input files gives\n# lines from colors_1.txt not present in colors_2.txt\n$ awk 'NR==FNR{a[$0]; next} !($0 in a)' colors_2.txt colors_1.txt\nteal\ngreen\n```\n\n>![warning](images/warning.svg) Note that the `NR==FNR` logic will fail if the first file is empty, since `NR` wouldn't get a chance to increment. You can set a flag after the first file has been processed to avoid this issue. See [this unix.stackexchange thread](https://unix.stackexchange.com/a/237110/109046) for more workarounds.\n>\n> ```bash\n> # no output\n> $ awk 'NR==FNR{a[$0]; next} !($0 in a)' /dev/null greeting.txt\n>\n> # gives the expected output\n> $ awk '!f{a[$0]; next} !($0 in a)' /dev/null f=1 greeting.txt\n> Hi there\n> Have a nice day\n> Good bye\n> ```\n\n## Comparing fields\n\nIn the previous section, you saw how to compare the contents of whole records between two files. This section will focus on comparing only specific fields. The below sample file will be one of the two file inputs for examples in this section.\n\n```bash\n$ cat marks.txt\nDept    Name    Marks\nECE     Raj     53\nECE     Joel    72\nEEE     Moi     68\nCSE     Surya   81\nEEE     Tia     59\nECE     Om      92\nCSE     Amy     67\n```\n\nTo start with, here's a single field comparison. The problem statement is to fetch all records from `marks.txt` if the first field matches any of the departments listed in the `dept.txt` file.\n\n```bash\n$ cat dept.txt\nCSE\nECE\n\n# note that dept.txt is used to build the array keys first\n$ awk 'NR==FNR{a[$1]; next} $1 in a' dept.txt marks.txt\nECE     Raj     53\nECE     Joel    72\nCSE     Surya   81\nECE     Om      92\nCSE     Amy     67\n\n# if the header is needed as well\n$ awk 'NR==FNR{a[$1]; next} FNR==1 || $1 in a' dept.txt marks.txt\nDept    Name    Marks\nECE     Raj     53\nECE     Joel    72\nCSE     Surya   81\nECE     Om      92\nCSE     Amy     67\n```\n\nFor multiple field comparison, you need to construct the key robustly. Simply concatenating field values can lead to false matches. For example, field values `abc` and `123` will wrongly match `ab` and `c123`. To avoid this, you may introduce some string between the field values, say `\"_\"` (if you know the field themselves cannot have this character) or `FS` (safer option). You could also allow `awk` to bail you out. If you use the `,` symbol (not `\",\"` as a string) between the field values, the value of the special variable `SUBSEP` is inserted. `SUBSEP` has a default value of the non-printing character `\\034` which is usually not used as part of text files.\n\n```bash\n$ cat dept_name.txt\nEEE Moi\nCSE Amy\nECE Raj\n\n# uses SUBSEP as a separator between the field values to construct the key\n# note the use of parentheses for key testing\n$ awk 'NR==FNR{a[$1,$2]; next} ($1,$2) in a' dept_name.txt marks.txt\nECE     Raj     53\nEEE     Moi     68\nCSE     Amy     67\n```\n\nIn this example, one of the field is used for numerical comparison.\n\n```bash\n$ cat dept_mark.txt\nECE 70\nEEE 65\nCSE 80\n\n# match Dept and minimum marks specified in dept_mark.txt\n$ awk 'NR==FNR{d[$1]=$2; next}\n       $1 in d && $3 >= d[$1]' dept_mark.txt marks.txt\nECE     Joel    72\nEEE     Moi     68\nCSE     Surya   81\nECE     Om      92\n```\n\nHere's an example of adding a new field.\n\n```bash\n$ cat role.txt\nRaj class_rep\nAmy sports_rep\nTia placement_rep\n\n$ awk -v OFS='\\t' 'NR==FNR{r[$1]=$2; next}\n         {$(NF+1) = FNR==1 ? \"Role\" : r[$2]} 1' role.txt marks.txt\nDept    Name    Marks   Role\nECE     Raj     53      class_rep\nECE     Joel    72      \nEEE     Moi     68      \nCSE     Surya   81      \nEEE     Tia     59      placement_rep\nECE     Om      92      \nCSE     Amy     67      sports_rep\n```\n\n## getline\n\nAs the name indicates, the `getline` function allows you to read a line from a file on demand. This is easiest to use when you need something based on line numbers. The following example shows how you can replace the `m`th line from a file with the `n`th line from another file. There are many syntax variations with `getline`, here the line read is saved in a variable.\n\n```bash\n# return value handling is not shown here, but should be done ideally\n$ awk -v m=3 -v n=2 'BEGIN{while(n-- > 0) getline s < \"greeting.txt\"}\n                     FNR==m{$0=s} 1' table.txt\nbrown bread mat hair 42\nblue cake mug shirt -7\nHave a nice day\n```\n\nHere's an example where two files are processed simultaneously. In this case, the return value of `getline` is also used. It will be `1` if the line was read successfully, `0` if there's no more input to be read as end of file has already been reached and `-1` if something went wrong. The `ERRNO` special variable will have the error details.\n\n```bash\n# print line from greeting.txt if the last column of the corresponding line\n# from table.txt is a positive number\n$ awk -v file='table.txt' '(getline line < file)==1{n=split(line, a);\n                           if(a[n]>0) print}' greeting.txt\nHi there\nGood bye\n```\n\nIf a file is passed as an argument to the `awk` command that cannot be opened, you get an error. For example:\n\n```bash\n$ awk '{print $2}' xyz.txt\nawk: fatal: cannot open file 'xyz.txt' for reading: No such file or directory\n```\n\nIt is recommended to always check for the return value when using `getline` or perhaps use techniques from the previous sections to avoid `getline` altogether.\n\n```bash\n# xyz.txt doesn't exist, but output doesn't show something went wrong\n$ awk '{getline line < \"xyz.txt\"; print $NF, line}' table.txt\n42 \n-7 \n3.14 \n\n$ awk -v file='xyz.txt' '{ e=(getline line < file);\n                           if(e<0){print file \": \" ERRNO; exit}\n                           print $NF, line }' table.txt\nxyz.txt: No such file or directory\n```\n\n>![info](images/info.svg) See [gawk manual: getline](https://www.gnu.org/software/gawk/manual/gawk.html#Getline) for details, especially about corner cases and errors. See also [awk.freeshell: getline caveats](http://awk.freeshell.org/AllAboutGetline).\n\n## Summary\n\nThis chapter discussed a few cases where you need to compare contents between two files. The `NR==FNR` trick is handy for such cases. You also saw a few examples with the `getline` function.\n\nNext chapter will discuss how to handle duplicate contents.\n\n## Exercises\n\n>![info](images/info.svg) The [exercises](https://github.com/learnbyexample/learn_gnuawk/tree/master/exercises) directory has all the files used in this section.\n\n**1)** Use the contents of `match_words.txt` file to display matching lines from `jumbled.txt` and `sample.txt`. The matching criteria is that the second word of lines from these files should match the third word of lines from `match_words.txt`.\n\n```bash\n$ cat match_words.txt\n%whole(Hello)--{doubt}==ado==\njust,\\joint*,concession<=nice\n\n# 'concession' is one of the third words from 'match_words.txt'\n# and second word from 'jumbled.txt'\n$ awk ##### add your solution here\nwavering:concession/woof\\retailer\nNo doubt you like it too\n```\n\n**2)** Interleave the contents of `secrets.txt` with the contents of a file passed via the `-v` option as shown below.\n\n```bash\n$ awk -v f='table.txt' ##### add your solution here\nstag area row tick\nbrown bread mat hair 42\n---\ndeaf chi rate tall glad\nblue cake mug shirt -7\n---\nBi tac toe - 42\nyellow banana window shoes 3.14\n---\n```\n\n**3)** The file `search_terms.txt` contains one search string per line, and these terms have no regexp metacharacters. Construct an `awk` command that reads this file and displays the search terms (matched case insensitively) that were found in every file passed as the arguments after `search_terms.txt`. Note that these terms should be matched anywhere in the line (so, don't use word boundaries).\n\n```bash\n$ cat search_terms.txt\nhello\nrow\nyou\nis\nat\n\n$ awk ##### add your solution here\n##file list## search_terms.txt jumbled.txt mixed_fs.txt secrets.txt table.txt\nat\nrow\n\n$ awk ##### add your solution here\n##file list## search_terms.txt addr.txt sample.txt\nis\nyou\nhello\n```\n\n**4)** Display lines from `scores.csv` by matching the first field based on a list of names from the `names.txt` file. Also, change the output field separator to a space character.\n\n```bash\n$ cat names.txt\nLin\nCy\nIth\n\n$ awk ##### add your solution here\nLin 78 83 80\nCy 97 98 95\nIth 100 100 100\n```\n\n**5)** What's the default value of the special variable `SUBSEP`? Where is it commonly used?\n\n**6)** The `result.csv` file has three columns — name, subject and mark. The `criteria.txt` file has two columns — name and subject. Match lines from `result.csv` based on the two columns from `criteria.txt` provided the mark column is greater than 80.\n\n```bash\n$ cat result.csv\nAmy,maths,89\nAmy,physics,75\nJoe,maths,79\nJohn,chemistry,77\nJohn,physics,91\nMoe,maths,81\nRavi,physics,84\nRavi,chemistry,70\nYui,maths,92\n\n$ cat criteria.txt\nAmy maths\nJohn chemistry\nJohn physics\nRavi chemistry\nYui maths\n\n$ awk ##### add your solution here\nAmy,maths,89\nJohn,physics,91\nYui,maths,92\n```\n\n# Dealing with duplicates\n\nOften, you need to eliminate duplicates from an input file. This could be based on the entire line content or based on certain fields. These are typically solved with the `sort` and `uniq` commands. Advantages with `awk` include regexp based field and record separators, input doesn't have to be sorted, and in general more flexibility because it is a programming language.\n\n>![info](images/info.svg) The [example_files](https://github.com/learnbyexample/learn_gnuawk/tree/master/example_files) directory has all the files used in the examples.\n\n## Whole line duplicates\n\n`awk '!a[$0]++'` is one of the most famous `awk` one-liners. It eliminates line based duplicates while retaining the input order. The following example shows it in action along with an illustration of how the logic works.\n\n```bash\n$ cat purchases.txt\ncoffee\ntea\nwashing powder\ncoffee\ntoothpaste\ntea\nsoap\ntea\n\n$ awk '{print +a[$0] \"\\t\" $0; a[$0]++}' purchases.txt\n0       coffee\n0       tea\n0       washing powder\n1       coffee\n0       toothpaste\n1       tea\n0       soap\n2       tea\n\n# only those entries with zero in first column will be retained\n$ awk '!a[$0]++' purchases.txt\ncoffee\ntea\nwashing powder\ntoothpaste\nsoap\n```\n\n>![info](images/info.svg) See also [huniq](https://github.com/koraa/huniq), a faster alternative for removing line based duplicates.\n\n## Column wise duplicates\n\nRemoving field based duplicates is simple for a single field comparison. Just change `$0` to the required field number after setting the appropriate field separator.\n\n```bash\n$ cat duplicates.txt\nbrown,toy,bread,42\ndark red,ruby,rose,111\nblue,ruby,water,333\ndark red,sky,rose,555\nyellow,toy,flower,333\nwhite,sky,bread,111\nlight red,purse,rose,333\n\n# based on the last field\n$ awk -F, '!seen[$NF]++' duplicates.txt\nbrown,toy,bread,42\ndark red,ruby,rose,111\nblue,ruby,water,333\ndark red,sky,rose,555\n```\n\nFor multiple fields comparison, separate the fields with `,` so that `SUBSEP` is used to combine the field values to generate the key. As mentioned before, `SUBSEP` has a default value of `\\034` non-printing character, which is typically not used in text files.\n\n```bash\n# based on the first and third fields\n$ awk -F, '!seen[$1,$3]++' duplicates.txt\nbrown,toy,bread,42\ndark red,ruby,rose,111\nblue,ruby,water,333\nyellow,toy,flower,333\nwhite,sky,bread,111\nlight red,purse,rose,333\n```\n\n## Duplicate count\n\nIn this section, how many times a duplicate record is found plays a role in determining the output.\n\nFirst up, printing only a specific numbered duplicate.\n\n```bash\n# print only the second occurrence of duplicates based on the second field\n$ awk -F, '++seen[$2]==2' duplicates.txt\nblue,ruby,water,333\nyellow,toy,flower,333\nwhite,sky,bread,111\n\n# print only the third occurrence of duplicates based on the last field\n$ awk -F, '++seen[$NF]==3' duplicates.txt\nlight red,purse,rose,333\n```\n\nNext, printing only the last copy of duplicates. Since the count isn't known, the `tac` command comes in handy again.\n\n```bash\n# reverse the input line-wise, retain first copy and then reverse again\n$ tac duplicates.txt | awk -F, '!seen[$NF]++' | tac\nbrown,toy,bread,42\ndark red,sky,rose,555\nwhite,sky,bread,111\nlight red,purse,rose,333\n```\n\nTo get all the records based on a duplicate count, you can pass the input file twice. Then use the two file processing trick to make decisions.\n\n```bash\n# all duplicates based on the last column\n$ awk -F, 'NR==FNR{a[$NF]++; next} a[$NF]>1' duplicates.txt duplicates.txt\ndark red,ruby,rose,111\nblue,ruby,water,333\nyellow,toy,flower,333\nwhite,sky,bread,111\nlight red,purse,rose,333\n\n# all duplicates based on the last column, minimum 3 duplicates\n$ awk -F, 'NR==FNR{a[$NF]++; next} a[$NF]>2' duplicates.txt duplicates.txt\nblue,ruby,water,333\nyellow,toy,flower,333\nlight red,purse,rose,333\n\n# only unique lines based on the third column\n$ awk -F, 'NR==FNR{a[$3]++; next} a[$3]==1' duplicates.txt duplicates.txt\nblue,ruby,water,333\nyellow,toy,flower,333\n```\n\n## Summary\n\nThis chapter showed how to work with duplicate contents for records and fields. If you don't need regexp based separators and if your input is too big to handle, then specialized command line tools like `sort` and `uniq` will be better suited compared to `awk`.\n\nNext chapter will show how to write `awk` scripts instead of the usual one-liners.\n\n## Exercises\n\n>![info](images/info.svg) The [exercises](https://github.com/learnbyexample/learn_gnuawk/tree/master/exercises) directory has all the files used in this section.\n\n**1)** Retain only the first copy of a line for the input file `lines.txt`. Case should be ignored while comparing the lines. For example, `hi there` and `HI TheRE` should be considered as duplicates.\n\n```bash\n$ cat lines.txt\nGo There\ncome on\ngo there\n---\n2 apples and 5 mangoes\ncome on!\n---\n2 Apples\nCOME ON\n\n$ awk ##### add your solution here\nGo There\ncome on\n---\n2 apples and 5 mangoes\ncome on!\n2 Apples\n```\n\n**2)** Retain only the first copy of a line for the input file `twos.txt`. Assume space as the field separator with exactly two fields per line. Compare the lines irrespective of the order of the fields. For example, `hehe haha` and `haha hehe` should be considered as duplicates.\n\n```bash\n$ cat twos.txt\nhehe haha\ndoor floor\nhaha hehe\n6;8 3-4\ntrue blue\nhehe bebe\nfloor door\n3-4 6;8\ntru eblue\nhaha hehe\n\n$ awk ##### add your solution here\nhehe haha\ndoor floor\n6;8 3-4\ntrue blue\nhehe bebe\ntru eblue\n```\n\n**3)** For the input file `twos.txt`, create a file `uniq.txt` with all the unique lines and `dupl.txt` with all the duplicate lines. Assume space as the field separator with exactly two fields per line. Compare the lines irrespective of the order of the fields. For example, `hehe haha` and `haha hehe` should be considered as duplicates.\n\n```bash\n$ awk ##### add your solution here\n\n$ cat uniq.txt \ntrue blue\nhehe bebe\ntru eblue\n\n$ cat dupl.txt \nhehe haha\ndoor floor\nhaha hehe\n6;8 3-4\nfloor door\n3-4 6;8\nhaha hehe\n```\n\n# awk scripts\n\nSo far, you've only seen how to provide `awk` scripts directly on the command line. In this chapter, you'll see basic examples for executing scripts saved in files.\n\n>![info](images/info.svg) The [example_files](https://github.com/learnbyexample/learn_gnuawk/tree/master/example_files) directory has all the files used in the examples.\n\n## -f option\n\nThe `-f` command line option allows you to pass the `awk` script via files instead of writing everything on the command line. Here's an one-liner seen earlier that's been converted to a multiline script. Note that `;` is no longer necessary to separate the commands, newline will do that too.\n\n```bash\n$ cat buf.awk\n/error/{\n    f = 1\n    buf = $0\n    next\n}\n\nf{\n    buf = buf ORS $0\n}\n\n/state/{\n    if(f)\n        print buf\n    f = 0\n}\n\n$ awk -f buf.awk broken.txt\nerror 2\n1234\n6789\nstate 1\nerror 4\nabcd\nstate 3\n```\n\nAnother advantage is that single quotes can be freely used.\n\n```bash\n$ echo 'cue us on this example' | awk -v q=\"'\" '{gsub(/\\w+/, q \"&\" q)} 1'\n'cue' 'us' 'on' 'this' 'example'\n\n# the above solution is simpler to write as a script\n$ cat quotes.awk\n{\n    gsub(/\\w+/, \"'&'\")\n}\n\n1\n\n$ echo 'cue us on this example' | awk -f quotes.awk\n'cue' 'us' 'on' 'this' 'example'\n```\n\n## -o option\n\nIf the code has been first tried out on the command line, you can use the `-o` option to get a pretty printed version. Output filename can be passed along as an argument to this option. By default, `awkprof.out` will be used as the filename.\n\n```bash\n# adding -o after the one-liner has been tested\n# input filenames and -v would be simply ignored\n$ awk -o -v OFS='\\t' 'NR==FNR{r[$1]=$2; next}\n         {$(NF+1) = FNR==1 ? \"Role\" : r[$2]} 1' role.txt marks.txt\n\n# pretty printed version\n$ cat awkprof.out\nNR == FNR {\n        r[$1] = $2\n        next\n}\n\n{\n        $(NF + 1) = FNR == 1 ? \"Role\" : r[$2]\n}\n\n1 {\n        print\n}\n\n# calling the script\n# note that other command line options have to be provided as usual\n$ awk -v OFS='\\t' -f awkprof.out role.txt marks.txt\nDept    Name    Marks   Role\nECE     Raj     53      class_rep\nECE     Joel    72      \nEEE     Moi     68      \nCSE     Surya   81      \nEEE     Tia     59      placement_rep\nECE     Om      92      \nCSE     Amy     67      sports_rep\n```\n\n## Summary\n\nSo, now you know how to write program files for `awk` instead of just the one-liners. And about the `-o` option, which helps to convert complicated one-liners to pretty printed program files.\n\nNext chapter will discuss a few gotchas and tricks.\n\n## Exercises\n\n>![info](images/info.svg) The [exercises](https://github.com/learnbyexample/learn_gnuawk/tree/master/exercises) directory has all the files used in this section.\n\n**1)** Before explaining the problem statement, here's an example of markdown headers and their converted link version. Note the use of `-1` for the second occurrence of the `Summary` header. Also note that this sample doesn't illustrate every rule explained below.\n\n```bash\n# Field separators\n## Summary\n# Gotchas and Tips\n## Summary\n\n* [Field separators](#field-separators)\n    * [Summary](#summary)\n* [Gotchas and Tips](#gotchas-and-tips)\n    * [Summary](#summary-1)\n```\n\nFor the input file `gawk.md`, construct a Table of Content section as per the details described below:\n\n* Identify all header lines\n    * there are two types of header lines, one starting with ``# `` and the other starting with ``## ``\n    * lines starting with `#` inside code blocks defined by ` ```bash ` and ` ``` ` markers should be ignored\n* The headers lines should then be converted as per the following rules:\n    * content is defined as the portion of the header ignoring the initial `#` or `##` characters and the space character\n    * `##` should be replaced with four spaces and a `*` character\n    * else, `#` should be replaced with `*` character\n    * create a copy of the content, change it to all lowercase, replace all space characters with the `-` character and then enclose it within `(#` and `)`\n        * if there are multiple headers with the same content, append `-1`, `-2`, etc respectively for the second header, third header, etc\n    * surround the original content with `[]` and then append the string obtained from the previous step\n* Note that the output should have only the converted headers, all other input lines should not be present\n\nThe script file should be named as `toc.awk` and save the output in `out.md`.\n\n```bash\n$ awk -f toc.awk gawk.md > out.md\n$ diff -sq out.md toc_expected.md\nFiles out.md and toc_expected.md are identical\n```\n\n**2)** For the input file `odd.txt`, surround the first two whole words of each line with `{}` that start and end with the same word character. Assume that the input file will not require case insensitive comparison. This is a contrived exercise that needs around 10 instructions and makes you use various features presented in this book.\n\n```bash\n$ cat odd.txt\n-oreo-not:a _a2_ roar<=>took%22\nRoaR to wow-\n\n$ awk -f same.awk odd.txt\n-{oreo}-not:{a} _a2_ roar<=>took%22\n{RoaR} to {wow}-\n```\n\n# Gotchas and Tips\n\nThis chapter will discuss some of the often made beginner mistakes, corner cases as well as a few tricks to improve performance.\n\n>![info](images/info.svg) The [example_files](https://github.com/learnbyexample/learn_gnuawk/tree/master/example_files) directory has all the files used in the examples.\n\n## Prefixing $ for variables\n\nSome scripting languages like `bash` require a `$` prefix when you need the value stored in a variable. For example, if you declare `name='Joe'` you'd need `echo \"$name\"` to print the value. This may result in using `$` prefix and other bashisms in `awk` as well when you are a beginner. To make it a bit worse, `awk` has the `$N` syntax for accessing field contents, which could result in false comprehension that all variables need the `$` prefix to access their values. See also [unix.stackexchange: Why does awk print the whole line when I want it to print a variable?](https://unix.stackexchange.com/q/291126/109046).\n\n```bash\n# silently fails, $word becomes $0 because of string to numeric conversion\n$ awk -v word=\"cake\" '$2==$word' table.txt\n# works when the variable is used correctly\n$ awk -v word=\"cake\" '$2==word' table.txt\nblue cake mug shirt -7\n\n# here 'field' gets replaced with '2' and hence $2 is printed\n$ awk -v field=2 '{print $field}' table.txt\nbread\ncake\nbanana\n```\n\n## DOS style line endings\n\nAs mentioned before, line endings differ from one platform to another. On Windows, it is typically a combination of carriage return and the newline character and referred as DOS style line endings. Since `GNU awk` allows multicharacter `RS`, it is easy to handle. See [stackoverflow: Why does my tool output overwrite itself and how do I fix it?](https://stackoverflow.com/q/45772525/4082052) for a detailed discussion and various mitigation methods.\n\n```bash\n# no issue with Unix style line ending\n$ printf 'mat dog\\n123 789\\n' | awk '{print $2, $1}'\ndog mat\n789 123\n\n# DOS style line ending causes trouble\n$ printf 'mat dog\\r\\n123 789\\r\\n' | awk '{print $2, $1}'\n mat\n 123\n$ printf 'mat dog\\r\\n123 789\\r\\n' | awk '{sub(/$/, \".\")} 1'\n.at dog\n.23 789\n\n# use \\r?\\n if you want to handle both Unix and DOS style with the same command\n# and use ORS=RT to preserve the line ending style\n$ printf 'mat dog\\r\\n123 789\\r\\n' | awk -v RS='\\r\\n' '{print $2, $1}'\ndog mat\n789 123\n$ printf 'mat dog\\r\\n123 789\\r\\n' | awk -v RS='\\r\\n' '{sub(/$/, \".\")} 1'\nmat dog.\n123 789.\n```\n\n## Behavior of ^ and $ when string contains newline\n\nIn some regular expression implementations, `^` matches the start of a line and `$` matches the end of a line (with newline as the line separator). In `awk`, these anchors always match the start of the entire string and end of the entire string respectively. This comes into play when `RS` is other than the newline character, or if you have a string value containing newline characters.\n\n```bash\n# 'apple\\n' doesn't match as there's a newline character\n$ printf 'apple\\n,mustard,grape,\\nmango' | awk -v RS=, '/e$/'\ngrape\n\n# '\\nmango' doesn't match as there's a newline character\n$ printf 'apple\\n,mustard,grape,\\nmango' | awk -v RS=, '/^m/'\nmustard\n```\n\n## Word boundary differences\n\nThe word boundary `\\y` matches both the start and end of word locations. Whereas, `\\<` and `\\>` will match exactly the start and end of word locations respectively. This leads to cases where you have to choose which of these word boundaries to use depending on the results desired. Consider `I have 12, he has 2!` as a sample text, shown below as an image with vertical bars marking the word boundaries. The last character `!` doesn't have the end of word boundary marker as it is not a word character.\n\n![word boundary](images/word_boundary.png)\n\n```bash\n# \\y matches both the start and end of word boundaries\n# the first match here used starting boundary of 'I' and 'have'\n$ echo 'I have 12, he has 2!' | awk '{gsub(/\\y..\\y/, \"[&]\")} 1'\n[I ]have [12][, ][he] has[ 2]!\n\n# \\< and \\> only matches the start and end word boundaries respectively\n$ echo 'I have 12, he has 2!' | awk '{gsub(/\\<..\\>/, \"[&]\")} 1'\nI have [12], [he] has 2!\n```\n\nHere's another example to show the difference between the two types of word boundaries.\n\n```bash\n# add something to both the start/end of word\n$ echo 'hi log_42 12b' | awk '{gsub(/\\y/, \":\")} 1'\n:hi: :log_42: :12b:\n\n# add something only at the start of word\n$ echo 'hi log_42 12b' | awk '{gsub(/\\</, \":\")} 1'\n:hi :log_42 :12b\n\n# add something only at the end of word\n$ echo 'hi log_42 12b' | awk '{gsub(/\\>/, \":\")} 1'\nhi: log_42: 12b:\n```\n\n## Relying on the default initial value\n\nUninitialized variables are useful, but sometimes they don't translate well if you are converting a command from single file input to multiple files. You have to workout which ones would need a reset at the beginning of each file being processed.\n\n```bash\n# step 1: works for single file\n$ awk '{sum += $NF} END{print sum}' table.txt\n38.14\n\n# step 2: prepare code to work for multiple file\n$ awk '{sum += $NF} ENDFILE{print FILENAME \":\" sum}' table.txt\ntable.txt:38.14\n\n# step 3: check with multiple file input\n# oops, default numerical value '0' for sum works only once\n$ awk '{sum += $NF} ENDFILE{print FILENAME \":\" sum}' table.txt marks.txt\ntable.txt:38.14\nmarks.txt:530.14\n\n# step 4: correctly initialize variables\n$ awk '{sum += $NF} ENDFILE{print FILENAME \":\" sum; sum=0}' table.txt marks.txt\ntable.txt:38.14\nmarks.txt:492\n```\n\n## Code in the replacement section\n\nThe replacement section in the substitution functions can accept any expression, which are converted to string whenever necessary. What happens if the regexp doesn't match the input string but the expression can change the value of a variable, such as increment/decrement operators? Well, the expression is still executed, which may or may not be what you need.\n\n```bash\n# no match for the second line, but 'c' was still modified\n$ awk '{sub(/^(br|ye)/, ++c \") &\")} 1' table.txt\n1) brown bread mat hair 42\nblue cake mug shirt -7\n3) yellow banana window shoes 3.14\n\n# check for a match before applying the substitution\n# this may also help to simplify the regexp for substitution\n# or, you could save the regexp in a variable to avoid duplication\n# can also use: awk '/^(br|ye)/{$0 = ++c \") \" $0} 1' table.txt\n$ awk '/^(br|ye)/{sub(/^/, ++c \") \")} 1' table.txt\n1) brown bread mat hair 42\nblue cake mug shirt -7\n2) yellow banana window shoes 3.14\n```\n\nAnother important point to note is that the expression is executed only once per function call, not for every match.\n\n```bash\n# the first line has two matches but 'c' is modified only once\n$ awk '{gsub(/\\<b/, ++c \") &\")} 1' table.txt\n1) brown 1) bread mat hair 42\n2) blue cake mug shirt -7\nyellow 3) banana window shoes 3.14\n```\n\n## Forcing numeric context\n\nYou can use the unary operator `+` to force numeric conversion. A variable might have numeric operations but still not get assigned a number if there's no input to read. So, when printing a variable that should be a number, use unary `+` to ensure it prints `0` instead of an empty string.\n\n```bash\n# numbers present in the last column, so no issues\n$ awk '{sum += $NF} END{print sum}' table.txt\n38.14\n# strings in the first column, gets treated as 0\n$ awk '{sum += $1} END{print sum}' table.txt\n0\n\n# no input at all, an empty string is printed\n$ awk '{sum += $1} END{print sum}' /dev/null\n\n# forced conversion to number, 0 is printed\n$ awk '{sum += $1} END{print +sum}' /dev/null\n0\n```\n\n## Locale based numbers\n\nThe `-N` option (or `--use-lc-numeric`) is useful to work with floating-point numbers based on the current locale.\n\n```bash\n# my locale uses . for the decimal point\n$ echo '3.14' | awk '{$0++} 1'\n4.14\n\n$ echo '3,14' | awk '{$0++} 1'\n4\n$ echo '3,14' | LC_NUMERIC=de_DE awk -N '{$0++} 1'\n4,14\n```\n\n## Forcing string context\n\nConcatenate an empty string to force string comparison.\n\n```bash\n# parentheses around the first argument to print used for clarity\n# fields get compared as numbers here\n$ echo '5 5.0' | awk '{print ($1==$2 ? \"same\" : \"different\"), \"number\"}'\nsame number\n\n# fields get compared as strings here\n$ echo '5 5.0' | awk '{print ($1\"\"==$2 ? \"same\" : \"different\"), \"string\"}'\ndifferent string\n```\n\n## Negative NF\n\nManipulating `NF` sometimes leads to a negative value. Fortunately, `awk` throws an error instead of failing silently.\n\n```bash\n# example file with different number of fields\n$ cat varying.txt\nparrot\ngood cool awesome\nblue sky\n12 34 56 78 90\n\n# delete the last two fields\n$ awk '{NF -= 2} 1' varying.txt\nawk: cmd. line:1: (FILENAME=varying.txt FNR=1) fatal: NF set to negative value\n\n# add a condition to check the number of fields\n# assumes that lines with less than 3 fields shouldn't be modified\n$ awk 'NF>2{NF -= 2} 1' varying.txt\nparrot\ngood\nblue sky\n12 34 56\n```\n\nHere's another example. Goal is to access the third field from the end.\n\n```bash\n$ awk '{print $(NF-2)}' varying.txt\nawk: cmd. line:1: (FILENAME=varying.txt FNR=1) fatal: attempt to access field -1\n\n# print only if there are minimum 3 fields\n$ awk 'NF>2{print $(NF-2)}' varying.txt\ngood\n56\n```\n\n## Faster execution\n\nChanging the locale to ASCII (assuming that the default is not ASCII) can give a significant speed boost. Using `mawk` is another way to speed up the execution, provided you are not using `GNU awk` specific features. There are many feature differences, for example, `mawk` doesn't support the `{}` form of quantifiers (see [unix.stackexchange: How to specify regex quantifiers with mawk?](https://unix.stackexchange.com/q/506119/109046) for details). See also [wikipedia: awk Versions and implementations](https://en.wikipedia.org/wiki/AWK_programming_language#Versions_and_implementations).\n\n```bash\n# time shown is the best result from multiple runs\n# speed benefit will vary depending on computing resources, input, etc\n# words.txt contains dictionary words, one word per line\n$ time awk '/^([a-d][r-z]){3}$/' words.txt > f1\nreal    0m0.029s\n\n$ time LC_ALL=C awk '/^([a-d][r-z]){3}$/' words.txt > f2\nreal    0m0.017s\n\n$ time mawk '/^[a-d][r-z][a-d][r-z][a-d][r-z]$/' words.txt > f3\nreal    0m0.009s\n\n# check that the results are the same\n$ diff -s f1 f2\nFiles f1 and f2 are identical\n$ diff -s f2 f3\nFiles f2 and f3 are identical\n# clean up temporary files\n$ rm f[123]\n```\n\nHere's another example.\n\n```bash\n# count words containing exactly 3 lowercase 'a' characters\n$ time awk -F'a' 'NF==4{cnt++} END{print +cnt}' words.txt\n1019\nreal    0m0.032s\n\n$ time LC_ALL=C awk -F'a' 'NF==4{cnt++} END{print +cnt}' words.txt\n1019\nreal    0m0.021s\n\n$ time mawk -F'a' 'NF==4{cnt++} END{print +cnt}' words.txt\n1019\nreal    0m0.014s\n```\n\n>![info](images/info.svg) See also [frawk](https://github.com/ezrosent/frawk), an efficient awk-like language implemented in Rust. And [huniq](https://github.com/koraa/huniq), a faster alternative for removing line based duplicates.\n\n# Further Reading\n\n* `man awk` and `info awk` and [online manual](https://www.gnu.org/software/gawk/manual/gawk.html)\n* Information about various implementations of `awk`\n    * [awk FAQ](http://www.faqs.org/faqs/computer-lang/awk/faq/) — great resource, but last modified *23 May 2002*\n    * [grymoire: awk tutorial](https://www.grymoire.com/Unix/Awk.html) — covers information about different `awk` versions as well\n    * [cheat sheet for awk/nawk/gawk](https://catonmat.net/ftp/awk.cheat.sheet.txt)\n    * [list of freely available awk implementations](https://www.gnu.org/software/gawk/manual/html_node/Other-Versions.html)\n* Q&A on stackoverflow/stackexchange are good source of learning material, good for practice exercises as well\n    * [awk Q&A on unix.stackexchange](https://unix.stackexchange.com/questions/tagged/awk?sort=votes&pageSize=15)\n    * [awk Q&A on stackoverflow](https://stackoverflow.com/questions/tagged/awk?sort=votes&pageSize=15)\n* Learn Regular Expressions (has information on flavors other than POSIX too)\n    * [regular-expressions](https://www.regular-expressions.info/) — tutorials and tools\n    * [rexegg](https://www.rexegg.com/) — tutorials, tricks and more\n    * [stackoverflow: What does this regex mean?](https://stackoverflow.com/q/22937618/4082052)\n    * [online regex tester and debugger](https://regex101.com/) — not fully suitable for CLI tools, but most of ERE syntax works\n* [My ebooks on CLI text processing tools](https://learnbyexample.github.io/books/)\n* Related tools\n    * [GNU datamash](https://www.gnu.org/software/datamash/)\n    * [bioawk](https://github.com/lh3/bioawk)\n    * [frawk](https://github.com/ezrosent/frawk) — an efficient awk-like language, implemented in Rust\n    * [goawk](https://github.com/benhoyt/goawk) — POSIX-compliant awk interpreter written in Go, with CSV support\n    * [hawk](https://github.com/gelisam/hawk) — similar to awk, but using Haskell as the text-processing language\n    * [miller](https://github.com/johnkerl/miller) — similar to awk/sed/cut/join/sort for name-indexed data such as CSV, TSV, and tabular JSON (see this [news.ycombinator discussion](https://news.ycombinator.com/item?id=10066742) for other tools like this)\n* Miscellaneous\n    * [unix.stackexchange: When to use grep, sed, awk, perl, etc](https://unix.stackexchange.com/q/303044/109046)\n    * [awk-libs](https://github.com/e36freak/awk-libs) — lots of useful functions\n    * [awkaster](https://github.com/TheMozg/awk-raycaster) — Pseudo-3D shooter written completely in awk\n    * [awk REPL](https://awk.js.org/) — live editor (browser app)\n* ASCII reference and locale usage\n    * [ASCII code table](https://ascii.cl/)\n    * [wiki.archlinux: locale](https://wiki.archlinux.org/title/locale)\n    * [shellhacks: Define Locale and Language Settings](https://www.shellhacks.com/linux-define-locale-language-settings/)\n* Examples for some of the topics not covered in this book\n    * [unix.stackexchange: rand/srand](https://unix.stackexchange.com/q/372816/109046)\n    * [unix.stackexchange: strftime](https://unix.stackexchange.com/q/224969/109046)\n    * [stackoverflow: arbitrary precision integer extension](https://stackoverflow.com/q/46904447/4082052)\n    * [stackoverflow: recognizing hexadecimal numbers](https://stackoverflow.com/q/3683110/4082052)\n    * [unix.stackexchange: sprintf and file closing](https://unix.stackexchange.com/q/223727/109046)\n    * [unix.stackexchange: user defined functions and array passing](https://unix.stackexchange.com/q/72469/109046)\n    * [unix.stackexchange: rename CSV files based on number of fields in header row](https://unix.stackexchange.com/q/408742/109046)\n\n"
        },
        {
          "name": "images",
          "type": "tree",
          "content": null
        },
        {
          "name": "sample_chapters",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}